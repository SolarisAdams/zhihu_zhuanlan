{
    "followers": [
        "https://www.zhihu.com/people/wan-yong-tao", 
        "https://www.zhihu.com/people/cui-wei-gang-64", 
        "https://www.zhihu.com/people/feng-hua-94-52", 
        "https://www.zhihu.com/people/tom-cat2", 
        "https://www.zhihu.com/people/jie-bao-38-86", 
        "https://www.zhihu.com/people/gogo-30-43", 
        "https://www.zhihu.com/people/wangdongdong89", 
        "https://www.zhihu.com/people/michael-wu-25", 
        "https://www.zhihu.com/people/willwinworld", 
        "https://www.zhihu.com/people/wangdesheng", 
        "https://www.zhihu.com/people/yang-ye-79", 
        "https://www.zhihu.com/people/herttrue", 
        "https://www.zhihu.com/people/aaaaaa-22-88", 
        "https://www.zhihu.com/people/jiang-wei-91-4", 
        "https://www.zhihu.com/people/zhang-hui-cong-76", 
        "https://www.zhihu.com/people/BruceWDZ", 
        "https://www.zhihu.com/people/zhang-dong-37-61", 
        "https://www.zhihu.com/people/hk-hu", 
        "https://www.zhihu.com/people/man-man-xue-54", 
        "https://www.zhihu.com/people/guo-xiao-long-92", 
        "https://www.zhihu.com/people/zi-mo-97-14", 
        "https://www.zhihu.com/people/jiu-bai-jiu-shi-ba-a-ge", 
        "https://www.zhihu.com/people/pu-fei-60", 
        "https://www.zhihu.com/people/xie-tao-47-50", 
        "https://www.zhihu.com/people/libingwu", 
        "https://www.zhihu.com/people/li-chi-50-43", 
        "https://www.zhihu.com/people/yunzhongke", 
        "https://www.zhihu.com/people/feng-lang-77-30", 
        "https://www.zhihu.com/people/zhang-xiao-hui-66-53", 
        "https://www.zhihu.com/people/yu-zhao-yang-87", 
        "https://www.zhihu.com/people/hum-75", 
        "https://www.zhihu.com/people/zhai-ms", 
        "https://www.zhihu.com/people/luo-hao-47-59", 
        "https://www.zhihu.com/people/da-qi-85-21", 
        "https://www.zhihu.com/people/wei-zhang-52-25", 
        "https://www.zhihu.com/people/jin-tian-29-89", 
        "https://www.zhihu.com/people/huang-jun-xin", 
        "https://www.zhihu.com/people/mr-lin-82-68", 
        "https://www.zhihu.com/people/liu-da-wei-40-78", 
        "https://www.zhihu.com/people/meng-de-bin", 
        "https://www.zhihu.com/people/xi-hong-shi-ji-dan-mian-46", 
        "https://www.zhihu.com/people/she-tian-lang-28", 
        "https://www.zhihu.com/people/greenapple-37", 
        "https://www.zhihu.com/people/ye-ling-ye-83", 
        "https://www.zhihu.com/people/luan-lin-bao", 
        "https://www.zhihu.com/people/sheldon-3-93"
    ], 
    "article": [
        {
            "url": "https://zhuanlan.zhihu.com/p/83620830", 
            "userName": "evan.wang", 
            "userLink": "https://www.zhihu.com/people/7ade3b1327452517e304cc3cc5cffa72", 
            "upvote": 0, 
            "title": "xgboost实战，一篇就好", 
            "content": "<h2>介绍：</h2><p>        之前学习了xgb的通俗介绍，和从数学的角度深入看了一下xgb的数学原理 。其实一般情况下，你知道如何运用就可以了。人们常说 人和动物的最大区别是会使用工具，而不是说了解工具的原理。 所以你用好xgb是一个最基础工作，咱得会。</p><p>学习一个算法实战，一般按照以下几步，第一步能够构建一个模型，第二步是能够优化一个模型 。</p><p>本章，我们将学习以下内容</p><ol><li>如果使用xgboost构建分类器</li><li>xgboost 的参数含义，以及如何调参</li><li>xgboost 的如何做cv </li><li>xgboost的可视化</li></ol><h2>准备数据 ：</h2><p>我们使用boston 房价数据 ，做的是一个回归任务，分类任务类似。</p><p>导入包</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"kn\">from</span>  <span class=\"nn\">sklearn</span> <span class=\"k\">import</span> <span class=\"n\">datasets</span> \n<span class=\"kn\">import</span> <span class=\"nn\">pandas</span> <span class=\"k\">as</span> <span class=\"nn\">pd</span> \n<span class=\"kn\">import</span> <span class=\"nn\">xgboost</span> <span class=\"k\">as</span> <span class=\"nn\">xgb</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.metrics</span> <span class=\"k\">import</span> <span class=\"n\">mean_squared_error</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.model_selection</span> <span class=\"k\">import</span> <span class=\"n\">train_test_split</span>\n<span class=\"kn\">import</span> <span class=\"nn\">matplotlib.pyplot</span> <span class=\"k\">as</span> <span class=\"nn\">plt</span></code></pre></div><p>读入和展示数据 </p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"n\">boston</span> <span class=\"o\">=</span> <span class=\"n\">datasets</span><span class=\"o\">.</span><span class=\"n\">load_boston</span><span class=\"p\">()</span>\n<span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">DataFrame</span><span class=\"p\">(</span><span class=\"n\">boston</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">)</span>\n<span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">columns</span> <span class=\"o\">=</span> <span class=\"n\">boston</span><span class=\"o\">.</span><span class=\"n\">feature_names</span>\n<span class=\"n\">data</span><span class=\"p\">[</span><span class=\"s1\">&#39;price&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">boston</span><span class=\"o\">.</span><span class=\"n\">target</span></code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-75a7d7b561b0a1e24d0ce2544625c2fc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1622\" data-rawheight=\"660\" class=\"origin_image zh-lightbox-thumb\" width=\"1622\" data-original=\"https://pic1.zhimg.com/v2-75a7d7b561b0a1e24d0ce2544625c2fc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1622&#39; height=&#39;660&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1622\" data-rawheight=\"660\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1622\" data-original=\"https://pic1.zhimg.com/v2-75a7d7b561b0a1e24d0ce2544625c2fc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-75a7d7b561b0a1e24d0ce2544625c2fc_b.jpg\"/></figure><p>我们拆分数据和标签</p><div class=\"highlight\"><pre><code class=\"language-text\">y = data.pop(&#39;price&#39;)</code></pre></div><p>我们也可以转为 dmatrix ，为什么要转呢 ，因为dmatrix 格式 在xgboost当中运行速度更快，性能更好。</p><div class=\"highlight\"><pre><code class=\"language-text\">data_matrix = xgb.DMatrix(data,y)</code></pre></div><h2>xgboost的参数解释 </h2><p>xgboost参数很多，咱可以查询官网，实际中我们用到的就那么一些 。接下来介绍我们用的参数。</p><h3>不需要调优的参数</h3><ul><li>booster ： [default=gbtree]   决定类型 ，gbtree   :   树模型    gblinear :线性模型。</li><li>objective : </li><ul><li>binary:logistic 用来二分类</li><li>multi:softmax 用来多分类</li><li>reg:linear 用来回归任务</li></ul><li>silent [default=0]:</li><ul><li>设为1 则不打印执行信息</li><li>设为0打印信息</li></ul><li>nthread ：控制线程数目 </li></ul><h3>可以优化的参数</h3><ul><li>max_depth: 决定树的最大深度，比较重要 常用值4-6 ，深度越深越容易过拟合。</li><li>n_estimators:  构建多少颗数 ，树越多越容易过拟合。</li><li>learning_rate/eta : 学习率 ，范围0-1 。默认值 为0.3 , 常用值0.01-0.2 </li><li>subsample: 每次迭代用多少数据集 。</li><li>colsample_bytree: 每次用多少特征 。可以控制过拟合。</li><li>min_child_weight :树的最小权重 ，越小越容易过拟合。</li><li>gamma：如果分裂能够使loss函数减小的值大于gamma，则这个节点才分裂。gamma设置了这个减小的最低阈值。如果gamma设置为0，表示只要使得loss函数减少，就分裂。</li><li>alpha：l1正则，默认为0 </li><li>lambda ：l2正则，默认为1</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><h3>baseline建模</h3><p>随机选取默认参数进行初始化建模</p><div class=\"highlight\"><pre><code class=\"language-text\">xg_reg= xgb.XGBRegressor(\n    objective=&#39;reg:linear&#39;,\n    colsample_bytree=0.3,\n    learning_rate=0.1,\n    max_depth=5,\n    n_estimators=10,\n    alpha=10\n)\nxg_reg.fit(x_train,y_train)\npred = xg_reg.predict(x_test)\nmean_squared_error(pred,y_test)</code></pre></div><p>81.1944550473363</p><h3>进行cv建模</h3><p>分成3份，10轮以后loss不下降就提前停止，最多50轮。</p><div class=\"highlight\"><pre><code class=\"language-text\">params = {&#34;objective&#34;:&#34;reg:linear&#34;,&#39;colsample_bytree&#39;: 0.3,&#39;learning_rate&#39;: 0.1,\n                &#39;max_depth&#39;: 5, &#39;alpha&#39;: 10}\ncv_results = xgb.cv(dtrain=data_matrix, params=params, nfold=3,\n                    num_boost_round=50,early_stopping_rounds=10,metrics=&#34;rmse&#34;, as_pandas=True, seed=123)\ncv_results.head()</code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-0ceb4f722e14c20c75a9e0fb5ac66e80_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1118\" data-rawheight=\"378\" class=\"origin_image zh-lightbox-thumb\" width=\"1118\" data-original=\"https://pic1.zhimg.com/v2-0ceb4f722e14c20c75a9e0fb5ac66e80_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1118&#39; height=&#39;378&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1118\" data-rawheight=\"378\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1118\" data-original=\"https://pic1.zhimg.com/v2-0ceb4f722e14c20c75a9e0fb5ac66e80_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-0ceb4f722e14c20c75a9e0fb5ac66e80_b.jpg\"/></figure><div class=\"highlight\"><pre><code class=\"language-text\">#打印最后一次结果\nprint((cv_results[&#34;test-rmse-mean&#34;]).tail(1))</code></pre></div><p>49    3.881535</p><p>Name: test-rmse-mean, dtype: float64</p><h3>可视化结果</h3><p>打印树的分裂情况</p><div class=\"highlight\"><pre><code class=\"language-text\">xg_reg = xgb.train(params=params, dtrain=data_matrix, num_boost_round=10)\nimport matplotlib.pyplot as plt\n\nxgb.plot_tree(xg_reg,num_trees=0)\nplt.rcParams[&#39;figure.figsize&#39;] = [80, 60]\nplt.show()</code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-d4210a82a6f1d76855b8cfd484ec09a1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2022\" data-rawheight=\"1014\" class=\"origin_image zh-lightbox-thumb\" width=\"2022\" data-original=\"https://pic2.zhimg.com/v2-d4210a82a6f1d76855b8cfd484ec09a1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2022&#39; height=&#39;1014&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2022\" data-rawheight=\"1014\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2022\" data-original=\"https://pic2.zhimg.com/v2-d4210a82a6f1d76855b8cfd484ec09a1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-d4210a82a6f1d76855b8cfd484ec09a1_b.jpg\"/></figure><p>打印特征权重：</p><p><b>如果特征当中有一个权重特别重要，说明有过拟合的风险，或者有穿越的现象</b>。你可以验证的方式是，把这个特征去掉，看看模型效果。</p><div class=\"highlight\"><pre><code class=\"language-text\">xgb.plot_importance(xg_reg)\nplt.rcParams[&#39;figure.figsize&#39;] = [3, 3]\nplt.show()</code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-c0a1a818b0459e7a6c9f7d8772661190_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"862\" data-rawheight=\"672\" class=\"origin_image zh-lightbox-thumb\" width=\"862\" data-original=\"https://pic1.zhimg.com/v2-c0a1a818b0459e7a6c9f7d8772661190_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;862&#39; height=&#39;672&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"862\" data-rawheight=\"672\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"862\" data-original=\"https://pic1.zhimg.com/v2-c0a1a818b0459e7a6c9f7d8772661190_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-c0a1a818b0459e7a6c9f7d8772661190_b.jpg\"/></figure><h2>关于优化的步骤：</h2><p><b>参数优化的效果一般不是特别大，更为重要的是数据清洗和特征工程。</b></p><ol><li>设置一些初始值，构建baseline模型</li><li>保持learning rate和其他booster相关的参数不变，调节estimators的参数。</li><li>调节booste相关的参数。其中影响最大的max_depth 和min_child_weight开始。逐步调节所有可能有影响的booster参数</li><li>缩小learning rate，得到最佳的learning rate值</li><li>得到一组效果还不错的参数组合</li></ol><p>代码demo ：</p><p>调节n_estimators </p><div class=\"highlight\"><pre><code class=\"language-text\">cv_params = {&#39;n_estimators&#39;: [400, 500, 600, 700, 800]}\nparams = {&#39;learning_rate&#39;: 0.1, &#39;n_estimators&#39;: 500, &#39;max_depth&#39;: 5, &#39;min_child_weight&#39;: 1, &#39;seed&#39;: 0,\n                    &#39;subsample&#39;: 0.8, &#39;colsample_bytree&#39;: 0.8, &#39;gamma&#39;: 0, &#39;reg_alpha&#39;: 0, &#39;reg_lambda&#39;: 1}\n\nmodel = xgb.XGBRegressor(**params)\noptimized_GBM = GridSearchCV(estimator=model, param_grid=cv_params, scoring=&#39;r2&#39;, cv=5, verbose=1, n_jobs=4)\noptimized_GBM.fit(X_train, y_train)</code></pre></div><p>调节和booster相关参数</p><p>先后顺序可以是 max_depth and min_child_weight </p><p>然后是gamma 参数</p><p>然后是接着是subsample以及colsample_bytree</p><p>紧接着就是：reg_alpha以及reg_lambda</p><div class=\"highlight\"><pre><code class=\"language-text\">cv_params = {&#39;max_depth&#39;: [3, 4, 5, 6, 7], &#39;min_child_weight&#39;: [1, 2, 3]}\nparams = {&#39;learning_rate&#39;: 0.1, &#39;n_estimators&#39;: 550, &#39;max_depth&#39;: 5, &#39;min_child_weight&#39;: 1, &#39;seed&#39;: 0,\n                    &#39;subsample&#39;: 0.8, &#39;colsample_bytree&#39;: 0.8, &#39;gamma&#39;: 0, &#39;reg_alpha&#39;: 0, &#39;reg_lambda&#39;: 1}\ncv_params = {&#39;gamma&#39;: [0.1, 0.2, 0.3, 0.4, 0.5, 0.6]}\nparams = {&#39;learning_rate&#39;: 0.1, &#39;n_estimators&#39;: 550, &#39;max_depth&#39;: 4, &#39;min_child_weight&#39;: 5, &#39;seed&#39;: 0,\n                    &#39;subsample&#39;: 0.8, &#39;colsample_bytree&#39;: 0.8, &#39;gamma&#39;: 0, &#39;reg_alpha&#39;: 0, &#39;reg_lambda&#39;: 1}</code></pre></div><p>最后调节learning rate</p><div class=\"highlight\"><pre><code class=\"language-text\">cv_params = {&#39;learning_rate&#39;: [0.01, 0.05, 0.07]}\nparams = {&#39;learning_rate&#39;: 0.1, &#39;n_estimators&#39;: 550, &#39;max_depth&#39;: 4, &#39;min_child_weight&#39;: 5, &#39;seed&#39;: 0,\n                    &#39;subsample&#39;: 0.7, &#39;colsample_bytree&#39;: 0.7, &#39;gamma&#39;: 0.1, &#39;reg_alpha&#39;: 1, &#39;reg_lambda&#39;: 1}</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><b>结论</b></p><p>到这篇文章结束，三篇xgboost的文章都写完了，如果觉得还不错，欢迎大家点赞，收藏。总的来说xgboost是一个很好的baseline算法，在性能和速度方面可以超过很多算法，应用起来也更加方便。 最后更多文章请看</p><a href=\"https://zhuanlan.zhihu.com/p/83563839\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\"internal\">evan.wang：小熊猫数据挖掘目录篇</a><h2>参考：</h2><p><a href=\"https://zhuanlan.zhihu.com/p/28672955\" class=\"internal\"><span class=\"invisible\">https://</span><span class=\"visible\">zhuanlan.zhihu.com/p/28</span><span class=\"invisible\">672955</span><span class=\"ellipsis\"></span></a></p><p></p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "xgboost", 
                    "tagLink": "https://api.zhihu.com/topics/20035241"
                }, 
                {
                    "tag": "实战", 
                    "tagLink": "https://api.zhihu.com/topics/19876792"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/83815919", 
            "userName": "evan.wang", 
            "userLink": "https://www.zhihu.com/people/7ade3b1327452517e304cc3cc5cffa72", 
            "upvote": 0, 
            "title": "numpy -数据挖掘之科学计算", 
            "content": "<h2>简单介绍</h2><p>本篇文章主要介绍数据挖掘里面的科学计算模块中 numpy。</p><p>通过这篇文章，咱们将一起学习</p><p>以下内容：</p><ul><li>numpy 是什么？</li><li>如何构建numpy？</li><li>numpy 与list 相比他的优点是什么？</li><li>numpy 的 操作</li></ul><h2><b>numpy的定义：</b></h2><p>numpy是一个科学计算包，能够提供强大的多为矩阵运算，内部用C写的。简单的一句话，里面的核心是矩阵运算，矩阵很容易做并行运算，这是他速度特别快的原因之一。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>构建numpy：</b></h2><p>1.构建一维度的</p><div class=\"highlight\"><pre><code class=\"language-text\">import numpy as np \na = np.array([1,2,3])\n</code></pre></div><p>2.构建多维度的</p><div class=\"highlight\"><pre><code class=\"language-text\">a = np.array([1,2,3],[4,5,6])</code></pre></div><p>3.numpy生成正态分布数据 </p><div class=\"highlight\"><pre><code class=\"language-text\">np.random.randn(2,2)</code></pre></div><p>out:  array([[-1.3720004 , -0.08225269],        [-0.9075861 ,  0.10122909]])</p><p>4. 从1，3 输出10个数字，均匀分割</p><div class=\"highlight\"><pre><code class=\"language-text\">a = np.linespace(1,3,10)</code></pre></div><p>Output : [ 1. 1.22222222 1.44444444 1.66666667 1.88888889 2.11111111 2.33333333 2.55555556 2.77777778 3. ]</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>numpy VS List </h2><p>货比三家才知道好不好，就跟我在xgboost当中和gdbt pk一样 ， 现在我们就让 numpy与 python中广为人知list 一较高下。</p><p>先说结论 </p><ol><li>占用较少的内存</li><li>速度快 ：在较大的数据集上 接近10倍的速度</li><li>方便 </li></ol><p>我们先看如何占用内存的故事</p><div class=\"highlight\"><pre><code class=\"language-text\">import numpy as np\nimport sys\nimport time\nS= range(80000)\nprint(sys.getsizeof(5)*len(S))\n \nD= np.arange(80000)\nprint(D.size*D.itemsize)</code></pre></div><p>out: </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-167eaf519e16e0509571510f106b6394_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"164\" data-rawheight=\"76\" class=\"content_image\" width=\"164\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;164&#39; height=&#39;76&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"164\" data-rawheight=\"76\" class=\"content_image lazy\" width=\"164\" data-actualsrc=\"https://pic1.zhimg.com/v2-167eaf519e16e0509571510f106b6394_b.png\"/></figure><p>如果我们输入8万个数字进去，一共相差3.5倍左右，所以下次你看别人用pandas或者list 存储的时候，你是不是可以鄙视它。当然要看情况而定，有时候有些东西不适合numpy存储。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>我们再看时间缩减和简单运算的的故事：</p><div class=\"highlight\"><pre><code class=\"language-text\">SIZE = 8000000\n \nL1= range(SIZE)\nL2= range(SIZE)\nA1= np.arange(SIZE)\nA2=np.arange(SIZE)\n \nstart= time.time()\nresult=[(x,y) for x,y in zip(L1,L2)]\nprint((time.time()-start)*1000)\n \nstart=time.time()\nresult= A1+A2\nprint((time.time()-start)*1000)</code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-1df93d607e7c38a76ef575ae5b338cfb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"354\" data-rawheight=\"116\" class=\"content_image\" width=\"354\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;354&#39; height=&#39;116&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"354\" data-rawheight=\"116\" class=\"content_image lazy\" width=\"354\" data-actualsrc=\"https://pic4.zhimg.com/v2-1df93d607e7c38a76ef575ae5b338cfb_b.jpg\"/></figure><p>同样是做加法，在numpy里面一个+ 就可以搞定 ， 同时时间上接近差了两倍的时间。</p><p>总结一下，就是大家以在遇到矩阵运算的时候尽量用numpy来。</p><h2>numpy的操作</h2><p>numpy的操作有很多，大家用的时候可以官网上看一下，我只列一下比较常用的一些</p><ul><li>dtype ：数据类型 ，这个很常用，因为数据挖掘对数字类型很敏感。</li><li>itemsize  the size of each element 每个元素字节的大小</li><li>ndim 查看多少维度</li><li>size 总的元素个数</li><li>shape ：形状</li></ul><div class=\"highlight\"><pre><code class=\"language-text\">a = np.array([(8,9,10),(11,12,13)]) \nprint(a.size,a.dtype,a.shape,a.ndim,a.itemsize)</code></pre></div><p>output : 6 int64 (2, 3) 2 8</p><ul><li>reshape ：改变形状</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-55ed33f02a7c4f54ffbce3ba0b18dac0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1064\" data-rawheight=\"444\" class=\"origin_image zh-lightbox-thumb\" width=\"1064\" data-original=\"https://pic1.zhimg.com/v2-55ed33f02a7c4f54ffbce3ba0b18dac0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1064&#39; height=&#39;444&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1064\" data-rawheight=\"444\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1064\" data-original=\"https://pic1.zhimg.com/v2-55ed33f02a7c4f54ffbce3ba0b18dac0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-55ed33f02a7c4f54ffbce3ba0b18dac0_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-text\">a = np.array([(8,9,10),(11,12,13)])\na.reshape((3,2))</code></pre></div><p>output : array([[ 8,  9],        [10, 11],        [12, 13]])</p><p>我们把两行三列的二维矩阵变成了 ，三行两列的二维矩阵。</p><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>slice  ：切片  ，重要的操作，为什么重要，因为矩阵一般很大，我们没法全部看，此外我们有时候可能只对部分数据操作</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-fe6992ff8f783bef8ed6c436d3659b1a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1092\" data-rawheight=\"522\" class=\"origin_image zh-lightbox-thumb\" width=\"1092\" data-original=\"https://pic3.zhimg.com/v2-fe6992ff8f783bef8ed6c436d3659b1a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1092&#39; height=&#39;522&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1092\" data-rawheight=\"522\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1092\" data-original=\"https://pic3.zhimg.com/v2-fe6992ff8f783bef8ed6c436d3659b1a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-fe6992ff8f783bef8ed6c436d3659b1a_b.jpg\"/></figure><div class=\"highlight\"><pre><code class=\"language-text\">#a的第一行，第3列\na[0:,2]\n</code></pre></div><p>Out ： array([10, 13])</p><ul><li>min/max/sum ，矩阵最小，最大，求和</li></ul><div class=\"highlight\"><pre><code class=\"language-text\">a = np.array([(8,9,10),(11,12,13)])\nprint(a.sum(),a.min(),a.max())</code></pre></div><p>out:63 8 13</p><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>with axis  有时候我们只求某一行或者某一列的值的和，最大，最小</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-6ae0659dc69aded0bfdb96d47555358c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"550\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic1.zhimg.com/v2-6ae0659dc69aded0bfdb96d47555358c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;640&#39; height=&#39;550&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"550\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https://pic1.zhimg.com/v2-6ae0659dc69aded0bfdb96d47555358c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-6ae0659dc69aded0bfdb96d47555358c_b.jpg\"/></figure><div class=\"highlight\"><pre><code class=\"language-text\">a.sum(axis=1)</code></pre></div><p>out:array([27, 36])</p><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>std and sqrt  方差 和 开方</li></ul><div class=\"highlight\"><pre><code class=\"language-text\">a= np.array([(1,2,3),(3,4,5)])\nnp.sqrt(a)</code></pre></div><p>out ： array([[1.        , 1.41421356, 1.73205081],        [1.73205081, 2.        , 2.23606798]])</p><div class=\"highlight\"><pre><code class=\"language-text\">np.std(a)</code></pre></div><p>out ： 1.2909944487358056</p><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>+ - * /  就是对应元素的加减乘除</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>Vertical &amp; Horizontal Stacking</li></ul><p>矩阵水平，或者垂直方向合并</p><div class=\"highlight\"><pre><code class=\"language-text\">x= np.array([(1,2,3),(3,4,5)])\ny= np.array([(1,2,3),(3,4,5)])\nnp.vstack((x,y))</code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-aa1ab308dbd1b0322913f9726b2cdb81_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"848\" data-rawheight=\"306\" class=\"origin_image zh-lightbox-thumb\" width=\"848\" data-original=\"https://pic2.zhimg.com/v2-aa1ab308dbd1b0322913f9726b2cdb81_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;848&#39; height=&#39;306&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"848\" data-rawheight=\"306\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"848\" data-original=\"https://pic2.zhimg.com/v2-aa1ab308dbd1b0322913f9726b2cdb81_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-aa1ab308dbd1b0322913f9726b2cdb81_b.jpg\"/></figure><div class=\"highlight\"><pre><code class=\"language-text\">np.hstack((x,y))</code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-716b75054149f52d174efce319b49b54_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1272\" data-rawheight=\"222\" class=\"origin_image zh-lightbox-thumb\" width=\"1272\" data-original=\"https://pic1.zhimg.com/v2-716b75054149f52d174efce319b49b54_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1272&#39; height=&#39;222&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1272\" data-rawheight=\"222\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1272\" data-original=\"https://pic1.zhimg.com/v2-716b75054149f52d174efce319b49b54_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-716b75054149f52d174efce319b49b54_b.jpg\"/></figure><div class=\"highlight\"><pre><code class=\"language-text\">#合并成一行\nx.ravel()</code></pre></div><p>out：array([1, 2, 3, 3, 4, 5])</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>怎么判断两个矩阵是否完全一样</b></p><p>(data1==data2).all()</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2>总结：</h2><p>numpy是一个经常用的东西，但是我们对它有多快和有多减少内存没有具体的量化，今天的例子估计是内存可以节约3.5倍左右，速度大概是2倍左右，当时这是简单的计算，当你的数据越大，计算越复杂，他的效用就会越高。</p><p>至于后面的操作你在实践中慢慢熟悉吧，可能我写的好一点，今天的目的就是告诉咱们能用numpy就不要用pandas，list等。</p><p>好了，如果改变了大家计算用numpy而不用list的观念，给我点个赞，给我点个赞，给我点个赞，重要事说三遍，然后可以收藏一下哦。小熊猫是专门干数据这一块的，今天numpy属于算法中的科学计算也属于数据操作模块。想了解其他可以看</p><a href=\"https://zhuanlan.zhihu.com/p/83563839\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\"internal\">evan.wang：小熊猫数据挖掘目录篇</a><p></p>", 
            "topic": [
                {
                    "tag": "数据挖掘", 
                    "tagLink": "https://api.zhihu.com/topics/19553534"
                }, 
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }, 
                {
                    "tag": "numpy", 
                    "tagLink": "https://api.zhihu.com/topics/19834165"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/83563839", 
            "userName": "evan.wang", 
            "userLink": "https://www.zhihu.com/people/7ade3b1327452517e304cc3cc5cffa72", 
            "upvote": 2, 
            "title": "小熊猫数据挖掘目录篇", 
            "content": "<p>开始写专栏，争取会写成一套比较完整的和机器学习相关的东西，帮助大家更好地发掘知识。</p><p>欢迎大家关注，你们的关注是我创作的源泉。</p><p>主要内容会设计以下内容</p><h2>数据挖掘原理：</h2><ul><li>机器学习</li><ul><li>xgboost </li></ul></ul><a href=\"https://zhuanlan.zhihu.com/p/83565042\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/equation.jpg\" data-image-width=\"0\" data-image-height=\"0\" class=\"internal\">evan.wang：数据挖掘之Xgboost数学原理，一篇就够</a><a href=\"https://zhuanlan.zhihu.com/p/83596488\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\"internal\">evan.wang：数据挖掘xgboost基本介绍,一篇就够</a><ul><ul><ul><li>xgboost实战应用 </li></ul></ul></ul><a href=\"https://zhuanlan.zhihu.com/p/83620830\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\"internal\">evan.wang：xgboost实战，一篇就好</a><ul><ul><li>lr</li><li>svm</li><li>kmeans</li></ul><li>深度学习</li><ul><li>nlp</li><ul><li>lstm</li><li>cnn在nlp应用</li><li>seq2seg</li><li>transfomer</li><li>word2vec</li></ul><li>图神经网络</li><ul><li>louvin</li><li>deepwalk</li></ul></ul><li>无监督kmeans</li><ul><ul><li>dec</li><li>vade</li><li>infogan</li><li>kmeans</li></ul></ul><li>单标签</li><ul><li>pu learning</li><li>isolate forest</li></ul></ul><h2>数据分析与统计：</h2><ol><li>hive</li><li>spark</li><li>es</li><li>numpy</li></ol><a href=\"https://zhuanlan.zhihu.com/p/83815919\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic2.zhimg.com/v2-a005ca8b5bfb0894c741e8ff251de4dd_180x120.jpg\" data-image-width=\"982\" data-image-height=\"578\" class=\"internal\">evan.wang：numpy -数据挖掘之科学计算</a><ol><li>pandas</li></ol><p class=\"ztext-empty-paragraph\"><br/></p><h2>数据清洗：</h2><ol><li>缺失率</li><li>如何发现垃圾数据</li></ol><h2>数据如何打标签：</h2><h2>特征工程：</h2><p>离散值 处理 </p><p>连续值处理 </p><p>构建特征，特征筛选 等</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>数据建模：</h2><ul><li>各种实战代码</li></ul><h2>评估与展示：</h2><ul><li>accuracy</li><li>mse</li><li>F1</li><li>psi</li><li>pandas</li><li>numpy</li><li>matplotlib</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><h2>模型部署监控：</h2><ul><li>psi</li><li>监控工具构建</li></ul><h2>大数据与机器学习：</h2><p>hive </p><p>hadoop</p><p>Es</p><p>spark</p><h2>项目：</h2><h2>面试题：</h2><p class=\"ztext-empty-paragraph\"><br/></p><h2>其他</h2><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>争取会写完一整套，欢迎各位老板，多关注，多点赞。</p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "数据", 
                    "tagLink": "https://api.zhihu.com/topics/19554449"
                }, 
                {
                    "tag": "数据挖掘", 
                    "tagLink": "https://api.zhihu.com/topics/19553534"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/83596488", 
            "userName": "evan.wang", 
            "userLink": "https://www.zhihu.com/people/7ade3b1327452517e304cc3cc5cffa72", 
            "upvote": 4, 
            "title": "数据挖掘xgboost基本介绍,一篇就够", 
            "content": "<h2>介绍：</h2><p>如果你想系统的学习机器学习，那么点赞我，关注我，打赏我就好了。我会慢慢补齐机器学习涉及的大部分内容(理论基础，工程应用，面试问题)。言归正传:</p><p>这篇文章，我将介绍以下几点，</p><ol><li>xgboost 应用在哪些场景，咱为啥要学他</li><li>xgboost 牛逼在哪些地方</li><li>xgboost 的核心思想 Boosting </li><li>xgb和 gdbt区别 </li></ol><h2>应用场景：</h2><p>xgboost 是现在机器流行的机器学习算法，虽然各种Deep Learning 层出不穷，但是在很多任务上是比不过xgboost的。 所以很多公司 all in xgboost 。这也是我用它来写第一个文章的原因。把他弄懂，搞定绝大部分模型训练不是问题，基本公司面试也都可以。</p><p>应用的地方：模型类型来说回归和分类来说，他都可以，按任务类型来说结构化数据，nlp，推荐，排序都可以。</p><p>不适合应用的地方： 时间序列的数据交给序列化模型更好，推荐里面deepfm更好，图像类的cnn等可以更好。  </p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>牛逼的地方：</b></h2><ul><li>速度快 ： 因为底层用c写，可以用GPU，且用了二阶求导， 所以速度特别快</li><li>并行：单台机器上可以并行计算，并且支持多台机器上并行，所以可以训练大量数据</li><li>效果：在很多的benchmark数据上，包括各种kaggle比赛，xgboost 都是一马当先。</li><li>实践简单 ：缺失值自己搞定，正则化自己搞定，修剪支自己搞定。</li></ul><p>很多作者写了十几个，我感觉写多了也记不住，就写了几个关键的，最重要的咱还得知道他是怎么做到的。 前面都有简单的解释，我们主要说说效果为什么好 。 </p><h2>为什么那么牛逼？</h2><p>在说这个问题之前，我们先说说啥叫Boosting？ 说完你就知道为啥牛逼了，你这么做你也牛逼。</p><p>大家都听过复仇者联盟吧 ，要想做成一件大事，我们就得联盟，团体的力量比个人的力量强大很多， 机器学习无时无刻不和我们按时人生哲学。 那么通俗的来说，Boosting是一个弱者联盟。</p><p>他是一堆若分类器的集合，t轮的结果依赖于(t - 1 )轮的结果 。那么继续举个例子好了，就是我们让n个学生来做《五年高考，三年模拟》。 A男生做对了一些，错的一些。那么B男生继续做 ，但是他会给A写错误的那群集合里面放入更多的精力学习， 判断 。 依次类推， C花更多精力学习 B判断错的，D花更多时间学习C判断错的。  </p><p>核心来了，记住他是个串行结构 。 这时候你肯定能想到另外一种，叫做bagging的东西，在这里我们不谈bagging，但是我们应该心里知道他。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-a18d16c00471de4206fc75fea1fe1238_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1306\" data-rawheight=\"900\" class=\"origin_image zh-lightbox-thumb\" width=\"1306\" data-original=\"https://pic1.zhimg.com/v2-a18d16c00471de4206fc75fea1fe1238_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1306&#39; height=&#39;900&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1306\" data-rawheight=\"900\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1306\" data-original=\"https://pic1.zhimg.com/v2-a18d16c00471de4206fc75fea1fe1238_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-a18d16c00471de4206fc75fea1fe1238_b.jpg\"/></figure><p>画了四个箱子，你肯定问这都是四个啥啊,为啥不放四大美女呢，学算法太累了</p><p>我们揭开这四个BOX的面纱</p><p>我们假设 +为胸大的，-为胸小的 ，现在我们用 boosting的方式来判断，好好学，学会了以后就知道判断了谁胸大了，😺</p><p>BOX1 ：画了一道线D1 ，然后把三个 胸大的分到了 胸小的 里面了 </p><p>BOX2：给与更多权重在错误的数据集上(即三个胸大的)，我们看到刚才那三个D2都划分对了，但是他又吧三个胸小 给划到胸大的里面了，依旧有三个错误</p><p>BOX3：给与更多的权重在 D2 划分错的 ，画出了D3 此时依然有2个 是错的 。</p><p>BOX4：结合 D1，D2，D3 ,完美的划分出来了 </p><p>如果此时你知道了，怎么用Boosting思想判断 胸大胸小 ，那么是不是点个赞 。</p><p>前面一段说了 boosing是串行结果，那么这一段就是告诉咱们他们怎么串行的。</p><p>重要的事说三遍，不能乱分胸大小，分错了就要给与更多的权重 ，给与错误的更多的权重，给与错误的更多的权重 。</p><p>最后把D1+D2+D3 就是最后的预测结果了 。实际上xgb的核心思想就是这个了。不知道你明白没，明白的点个赞，不明白加个微信继续探讨 。。。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>面试题：xgboost 和 gdbt 区别 </h2><p>谈了这么多可能你要问我gdbt 也是boosting ，那和xgboost有啥区别呢 ？</p><p>上面提到的四个优点， 除了boosting，都是xgb的独有的优点。 举例来说xgb为啥比gdbt好，xgboost 和gdbt两个人都参加高考，在考前特训 ， 虽然 xgb和gdbt 的基因一样 (都是boosting )，但是 xgb做模拟训练更快，并且因为并行，脑容量更大能做更多题 ，且不容易偏科（有正则化）。所以xgb绝大多数情况下比gdbt好。 </p><h2>总结 </h2><p>工作中我们也要和boosting一样，把精力放在自己错误的地方，对于失败给与更多的关注。然后数学原理篇，请看另外一篇 ，应用篇静等下周 。 如果有错误的地方欢迎指教。 </p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p>", 
            "topic": [
                {
                    "tag": "数据挖掘", 
                    "tagLink": "https://api.zhihu.com/topics/19553534"
                }, 
                {
                    "tag": "xgboost", 
                    "tagLink": "https://api.zhihu.com/topics/20035241"
                }, 
                {
                    "tag": "通俗解释", 
                    "tagLink": "https://api.zhihu.com/topics/19671891"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/83565042", 
            "userName": "evan.wang", 
            "userLink": "https://www.zhihu.com/people/7ade3b1327452517e304cc3cc5cffa72", 
            "upvote": 38, 
            "title": "数据挖掘之Xgboost数学原理，一篇就够", 
            "content": "<p><b>背景：</b></p><p>当时的情况是机器学习挺火，但是需要有人能够更好解决一下两个问题 </p><p>1.需要使用有效的模型来捕获复杂的数据依赖关系，</p><p>2.并使用可扩展的学习系统来从大型数据集中学习得到模型。</p><p>这个背景也是随着hadoop等分布式数据存储越来越成熟后，我们所能够得到的数据越来越多。</p><p>而机器学习还处于单机状态，所以xgboost应运而生，总结就是先有大数据存储，和分布式计算才有机器学习。</p><p>从这段背景中我们可以得到在机器学习中，如果哪天你也要发明模型结构（注意点：数据都是干净的数据），可以由以下两个方面可以入手，其中之一是喂给他更多的数据，类似于你在高考前做了大量的题目，然后去高考。另一个方式是，加深对数据的理解，举例而言就是更好地能理解你做过的题目， 之前你做题只能举一反二，现在可以举一反五。</p><p>xgb在这两方面都有改进，但是他的整体结构没有跳出统计机器学习的范畴。</p><p>接下里我们一起开始学习一下内容 :</p><p>1.xgboost通俗介绍。</p><p>2.xgboost背后的数学秘密  。</p><p>3.实战 :运用xgboost   训练篇 。</p><p>4.实战：运用xgb评估篇 。</p><p>我们这篇写的是从数学角度看xgboost。</p><p><br/><b>原理(从数学公式看xgboost)：</b></p><p>解剖一个算法，做到最好的地步肯定是和庖丁解牛一样 。那么我一定得知道他有什么结构，比如牛有的核心部件，大脑，心脏，四肢等等。算法也有自己的几个部件，他们不是独立存在，而是相互关联。算法的核心结构是</p><ol><li>构建损失函数</li><li>构建优化函数，</li><li>构建正则项。</li></ol><p>至于为什么需要这三个，我们后面再慢慢说</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>1.xgb构建正则项</b></p><p><b>公式（1）:</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-89300938e5f308749f73b86249fb44ab_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"952\" data-rawheight=\"440\" class=\"origin_image zh-lightbox-thumb\" width=\"952\" data-original=\"https://pic4.zhimg.com/v2-89300938e5f308749f73b86249fb44ab_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;952&#39; height=&#39;440&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"952\" data-rawheight=\"440\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"952\" data-original=\"https://pic4.zhimg.com/v2-89300938e5f308749f73b86249fb44ab_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-89300938e5f308749f73b86249fb44ab_b.jpg\"/></figure><p><b>理解：</b></p><p>知道公式了，这是第一步，然后我们去看看这个正则项的设计思路是什么 ？</p><p>正则项的目的是想让模型简单，从而增加泛化能力。我们现在是构建一颗树，那么我们肯定不希望这棵树枝繁叶茂，如果能力尚可，我们希望他越简单越好，所以第一点，我们要剪枝减叶。 另外一点，我们也不希望，某个树枝或者叶子有太大的权重，类似不能偏科，比如你体育学的太好，会导致你会用学体育的思想解决数学问题，导致你的数学变成体育老师教的，所以我们需要加l2正则化。</p><p>𝜸T: 是对叶子的个数加了限制，从而限制了模型的复杂度</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7B2%7D%5CVert%5Comega%5CVert\" alt=\"\\frac{1}{2}\\Vert\\omega\\Vert\" eeimg=\"1\"/> :表示的是l2 正则  ,我们希望不要有太大的权重 <img src=\"https://www.zhihu.com/equation?tex=%5Comega\" alt=\"\\omega\" eeimg=\"1\"/> 出现  </p><p>将L2正则和决策树的剪枝的思想放到了这里 ，但是能想到这么处理树的复杂度，所以作者很好的站在了前人的肩膀上。 另外一点，在之前的模型中，一般都是让操作者选择是否加入正则项，没有人将正则项默认加入进来。如果有人知道陈天奇大牛如何想到默认加入正则项，欢迎告知</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>2.XGB构建目标函数</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-cd70f088ccb54c61aece506ee04fcd3e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"940\" data-rawheight=\"220\" class=\"origin_image zh-lightbox-thumb\" width=\"940\" data-original=\"https://pic3.zhimg.com/v2-cd70f088ccb54c61aece506ee04fcd3e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;940&#39; height=&#39;220&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"940\" data-rawheight=\"220\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"940\" data-original=\"https://pic3.zhimg.com/v2-cd70f088ccb54c61aece506ee04fcd3e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-cd70f088ccb54c61aece506ee04fcd3e_b.jpg\"/></figure><p>目标函数  = 损失函数(尽量拟合) + 正则项 (减轻一点复杂度)</p><p>损失函数 ： 就是看预测值与真实值得差距</p><p>首先我们知道xgboost是将每颗树的结果和相加，得到最终的结果 ，图示如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-8fe3a66fca0ff2b38a820b5e0ea303f1_b.jpg\" data-size=\"normal\" data-rawwidth=\"876\" data-rawheight=\"438\" class=\"origin_image zh-lightbox-thumb\" width=\"876\" data-original=\"https://pic2.zhimg.com/v2-8fe3a66fca0ff2b38a820b5e0ea303f1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;876&#39; height=&#39;438&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"876\" data-rawheight=\"438\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"876\" data-original=\"https://pic2.zhimg.com/v2-8fe3a66fca0ff2b38a820b5e0ea303f1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-8fe3a66fca0ff2b38a820b5e0ea303f1_b.jpg\"/><figcaption>图片来源于陈天奇论文</figcaption></figure><p>我们看第i颗数的预测结果和  <img src=\"https://www.zhihu.com/equation?tex=%5Chat%7By%7D%5Ei\" alt=\"\\hat{y}^i\" eeimg=\"1\"/>  =  第(i-1)颗的树结果和 <img src=\"https://www.zhihu.com/equation?tex=%5Chat%7By%7D%5E%7Bi-1%7D\" alt=\"\\hat{y}^{i-1}\" eeimg=\"1\"/> +   第i颗树的结果 <img src=\"https://www.zhihu.com/equation?tex=f%28x_%7Bi%7D%29\" alt=\"f(x_{i})\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%5Chat%7By%7D%5Ei%3D%5Chat%7By%7D%5E%7Bi-1%7D+%2B+f%28x_%7Bi%7D%29\" alt=\"\\hat{y}^i=\\hat{y}^{i-1} + f(x_{i})\" eeimg=\"1\"/> </p><p>根据这个我们变换公式为(2)：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-57678b003f5833d4bdae7892a245e547_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"692\" data-rawheight=\"124\" class=\"origin_image zh-lightbox-thumb\" width=\"692\" data-original=\"https://pic4.zhimg.com/v2-57678b003f5833d4bdae7892a245e547_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;692&#39; height=&#39;124&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"692\" data-rawheight=\"124\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"692\" data-original=\"https://pic4.zhimg.com/v2-57678b003f5833d4bdae7892a245e547_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-57678b003f5833d4bdae7892a245e547_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>(3)根据泰勒扩展 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-3a4fc5e1325bcf596763ce3c3a02c690_b.png\" data-size=\"normal\" data-rawwidth=\"1156\" data-rawheight=\"124\" class=\"origin_image zh-lightbox-thumb\" width=\"1156\" data-original=\"https://pic1.zhimg.com/v2-3a4fc5e1325bcf596763ce3c3a02c690_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1156&#39; height=&#39;124&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1156\" data-rawheight=\"124\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1156\" data-original=\"https://pic1.zhimg.com/v2-3a4fc5e1325bcf596763ce3c3a02c690_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-3a4fc5e1325bcf596763ce3c3a02c690_b.png\"/><figcaption>泰勒展开式</figcaption></figure><p>我们这边用二阶泰勒扩展 ，将我们的公式变化为：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-0d4793f284ca0eb2c0847e90fbdf7d99_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"850\" data-rawheight=\"134\" class=\"origin_image zh-lightbox-thumb\" width=\"850\" data-original=\"https://pic2.zhimg.com/v2-0d4793f284ca0eb2c0847e90fbdf7d99_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;850&#39; height=&#39;134&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"850\" data-rawheight=\"134\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"850\" data-original=\"https://pic2.zhimg.com/v2-0d4793f284ca0eb2c0847e90fbdf7d99_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-0d4793f284ca0eb2c0847e90fbdf7d99_b.png\"/></figure><p>其中\t</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-997a1a4266373dd60d7888ace31e1fa4_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"942\" data-rawheight=\"82\" class=\"origin_image zh-lightbox-thumb\" width=\"942\" data-original=\"https://pic1.zhimg.com/v2-997a1a4266373dd60d7888ace31e1fa4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;942&#39; height=&#39;82&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"942\" data-rawheight=\"82\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"942\" data-original=\"https://pic1.zhimg.com/v2-997a1a4266373dd60d7888ace31e1fa4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-997a1a4266373dd60d7888ace31e1fa4_b.png\"/></figure><p>然后我们去掉常数项，前n轮的残差结果 得到公式(3)</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-e482577980c6df20503ee5d02e74c344_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"662\" data-rawheight=\"130\" class=\"origin_image zh-lightbox-thumb\" width=\"662\" data-original=\"https://pic1.zhimg.com/v2-e482577980c6df20503ee5d02e74c344_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;662&#39; height=&#39;130&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"662\" data-rawheight=\"130\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"662\" data-original=\"https://pic1.zhimg.com/v2-e482577980c6df20503ee5d02e74c344_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-e482577980c6df20503ee5d02e74c344_b.jpg\"/></figure><p>问题：为什么我们需要泰勒公式？（如果你看懂了，应该值得一个赞）</p><p>原来的目标函数转化为欧几里得域(欧几里德整环)的函数，从而能够使用传统的优化方法 。</p><p>刚才那个公式我们简单写成 <img src=\"https://www.zhihu.com/equation?tex=f%5Cleft%28+x+%5Cright%29+%3D+a+%2B+%5Ctriangle%7Bx%7D\" alt=\"f\\left( x \\right) = a + \\triangle{x}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=f%5Cleft%28+x+%5Cright%29\" alt=\"f\\left( x \\right)\" eeimg=\"1\"/> :表示损失函数</p><p><img src=\"https://www.zhihu.com/equation?tex=a\" alt=\"a\" eeimg=\"1\"/> :表示为（t-1）轮的损失</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Ctriangle%7Bx%7D\" alt=\"\\triangle{x}\" eeimg=\"1\"/> :表示当前损失</p><p>每次迭代t轮，我们都可以看成旧的(t-1)轮数 ，加上新的learner ，因此我们可以用 Euclidean space 技术去优化他。</p><p>此外我们需要注意，这是个贪婪算法，如果 <img src=\"https://www.zhihu.com/equation?tex=%5Ctriangle%7Bx%7D%3E0\" alt=\"\\triangle{x}&gt;0\" eeimg=\"1\"/> 那么损失就会增大，反之就是减小。</p><p>接下来我们定义样本总数为 n ，每个样本为i ，i的信息被分到一些叶子节点信息上，定义每个属于i的叶子权重为j 。 举个例子，我们现在要区分美女的胸大还是胸小，现在有数据n条，我们定义每条数据为i，但是这些数据不可能在每个节点上都有权重，我们把属于i的的权重叫做j。</p><p><img src=\"https://www.zhihu.com/equation?tex=I_%7Bj%7D%3D%5Cleft%5C%7B+i%7Cq%28x_%7Bi%7D%3Dj%29+%5Cright%5C%7D\" alt=\"I_{j}=\\left\\{ i|q(x_{i}=j) \\right\\}\" eeimg=\"1\"/>  表示i样本 叶子的集合。推导出公式（4）</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-8af98ce051fc4b10381b3ec7c78d335d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"792\" data-rawheight=\"250\" class=\"origin_image zh-lightbox-thumb\" width=\"792\" data-original=\"https://pic2.zhimg.com/v2-8af98ce051fc4b10381b3ec7c78d335d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;792&#39; height=&#39;250&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"792\" data-rawheight=\"250\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"792\" data-original=\"https://pic2.zhimg.com/v2-8af98ce051fc4b10381b3ec7c78d335d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-8af98ce051fc4b10381b3ec7c78d335d_b.jpg\"/></figure><p>最后我们求一下 <img src=\"https://www.zhihu.com/equation?tex=w\" alt=\"w\" eeimg=\"1\"/> 的，让当前函数对 <img src=\"https://www.zhihu.com/equation?tex=w\" alt=\"w\" eeimg=\"1\"/> 求导为0，得到公式（5）:</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ddc7f5b971982341fd07a56ffc96087f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"412\" data-rawheight=\"130\" class=\"content_image\" width=\"412\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;412&#39; height=&#39;130&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"412\" data-rawheight=\"130\" class=\"content_image lazy\" width=\"412\" data-actualsrc=\"https://pic4.zhimg.com/v2-ddc7f5b971982341fd07a56ffc96087f_b.jpg\"/></figure><p>将 <img src=\"https://www.zhihu.com/equation?tex=w\" alt=\"w\" eeimg=\"1\"/> 再带入原来的函数 (4),得到最后的损失函数的分数 （6）</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-5e1fefe00b09e6ca533ed8a7764caae5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"622\" data-rawheight=\"152\" class=\"origin_image zh-lightbox-thumb\" width=\"622\" data-original=\"https://pic2.zhimg.com/v2-5e1fefe00b09e6ca533ed8a7764caae5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;622&#39; height=&#39;152&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"622\" data-rawheight=\"152\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"622\" data-original=\"https://pic2.zhimg.com/v2-5e1fefe00b09e6ca533ed8a7764caae5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-5e1fefe00b09e6ca533ed8a7764caae5_b.jpg\"/></figure><p>这个函数越小，结构越好。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>最后提一下树的分支怎么判断的，很简单的，就是我们把左边的分支 + 右边的分支 - 不分支的的结果。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-7131ac7b3b503098bf59060d89b2a7ef_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"926\" data-rawheight=\"120\" class=\"origin_image zh-lightbox-thumb\" width=\"926\" data-original=\"https://pic4.zhimg.com/v2-7131ac7b3b503098bf59060d89b2a7ef_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;926&#39; height=&#39;120&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"926\" data-rawheight=\"120\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"926\" data-original=\"https://pic4.zhimg.com/v2-7131ac7b3b503098bf59060d89b2a7ef_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-7131ac7b3b503098bf59060d89b2a7ef_b.png\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>总结：</h2><p>好了，暂时告一段落，有些的不好的地方欢迎大家批评改正，我会把xgboost分成数学原理篇，通俗理论篇，实战建模baseline，评估与优化 四个部分来讲述。最后觉得写的还可以的点个赞，收藏一下，打赏打赏。</p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "数据挖掘", 
                    "tagLink": "https://api.zhihu.com/topics/19553534"
                }, 
                {
                    "tag": "xgboost", 
                    "tagLink": "https://api.zhihu.com/topics/20035241"
                }
            ], 
            "comments": [
                {
                    "userName": "qu89pe", 
                    "userLink": "https://www.zhihu.com/people/47e8458d05d39d5fd97ff63762ccd509", 
                    "content": "讲陈天奇好几年前的ppt还行。", 
                    "likes": 1, 
                    "childComments": [
                        {
                            "userName": "evan.wang", 
                            "userLink": "https://www.zhihu.com/people/7ade3b1327452517e304cc3cc5cffa72", 
                            "content": "<p>谢谢</p>", 
                            "likes": 1, 
                            "replyToAuthor": "qu89pe"
                        }
                    ]
                }, 
                {
                    "userName": "机器学习爱好者", 
                    "userLink": "https://www.zhihu.com/people/3dd7a909aef15b7acac760df6bb49a80", 
                    "content": "xgb训练集和测试集auc差多少才能不算过拟合呢？", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "張惠聰", 
                    "userLink": "https://www.zhihu.com/people/97bdd3edea727223fad74b9227461619", 
                    "content": "讲的很好 谢谢！", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }
    ], 
    "url": "https://zhuanlan.zhihu.com/c_1150418749071978496"
}
