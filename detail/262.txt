{
    "title": "鸭鸭的NLP日常", 
    "description": "", 
    "followers": [
        "https://www.zhihu.com/people/li-sun-9", 
        "https://www.zhihu.com/people/yang-yu-58042", 
        "https://www.zhihu.com/people/liang-xiao-22-35", 
        "https://www.zhihu.com/people/xu-jian-zhi", 
        "https://www.zhihu.com/people/shijinglei", 
        "https://www.zhihu.com/people/hee-yoyu-zhi-xuan", 
        "https://www.zhihu.com/people/xu-liang-27-53", 
        "https://www.zhihu.com/people/drinkinblood", 
        "https://www.zhihu.com/people/lin-zhen-kun-4", 
        "https://www.zhihu.com/people/muyaba", 
        "https://www.zhihu.com/people/qian-ren-jian-45", 
        "https://www.zhihu.com/people/zuo-zhao-rui-90", 
        "https://www.zhihu.com/people/qi-le-68-80", 
        "https://www.zhihu.com/people/2wanhao", 
        "https://www.zhihu.com/people/hao-zhang-44-23", 
        "https://www.zhihu.com/people/tao-gu-du-huan-zhe", 
        "https://www.zhihu.com/people/xin-xin-zhang-46", 
        "https://www.zhihu.com/people/wen-qing-69", 
        "https://www.zhihu.com/people/zheng-cheng-43-41", 
        "https://www.zhihu.com/people/liang-pan-fei", 
        "https://www.zhihu.com/people/wo-jiao-xiao-xiao-fei-fei-xia", 
        "https://www.zhihu.com/people/yuzhao-22", 
        "https://www.zhihu.com/people/MicrostrongAI", 
        "https://www.zhihu.com/people/zhao-jun-83-21-53", 
        "https://www.zhihu.com/people/liu-hai-cheng-88", 
        "https://www.zhihu.com/people/tian-xiao-zhai-54", 
        "https://www.zhihu.com/people/bling-29", 
        "https://www.zhihu.com/people/niu-mo-mo-78", 
        "https://www.zhihu.com/people/yiyuanyu17", 
        "https://www.zhihu.com/people/wu-wen-xi-dong-58-58", 
        "https://www.zhihu.com/people/chen-yu-11-4", 
        "https://www.zhihu.com/people/ideas-yd", 
        "https://www.zhihu.com/people/qianghuazhnag", 
        "https://www.zhihu.com/people/big-big-stone", 
        "https://www.zhihu.com/people/1111112222", 
        "https://www.zhihu.com/people/kim-74-51", 
        "https://www.zhihu.com/people/fei-yu-38-68", 
        "https://www.zhihu.com/people/yang-yang-4-70-26", 
        "https://www.zhihu.com/people/frank0329", 
        "https://www.zhihu.com/people/ll-uncle", 
        "https://www.zhihu.com/people/han-lin-feng-36", 
        "https://www.zhihu.com/people/li-ding-15-51", 
        "https://www.zhihu.com/people/zong-dao-ming", 
        "https://www.zhihu.com/people/coreseek", 
        "https://www.zhihu.com/people/cheng-ming-9-60", 
        "https://www.zhihu.com/people/xie-tie", 
        "https://www.zhihu.com/people/zhangyuting", 
        "https://www.zhihu.com/people/mitake-11", 
        "https://www.zhihu.com/people/kevin-jiang-36", 
        "https://www.zhihu.com/people/xu-hong-bin-89", 
        "https://www.zhihu.com/people/wang-dong-wei-91-54", 
        "https://www.zhihu.com/people/liu-zhi-feng-64", 
        "https://www.zhihu.com/people/nipi", 
        "https://www.zhihu.com/people/xia-wu-yu-77", 
        "https://www.zhihu.com/people/zenRRan", 
        "https://www.zhihu.com/people/tillman-30", 
        "https://www.zhihu.com/people/scottdc-31", 
        "https://www.zhihu.com/people/yuleichin", 
        "https://www.zhihu.com/people/langlangago-30", 
        "https://www.zhihu.com/people/zzx-47-7", 
        "https://www.zhihu.com/people/tiao-jian-usb", 
        "https://www.zhihu.com/people/guoqunabc", 
        "https://www.zhihu.com/people/wang-diao-na-ge-ren", 
        "https://www.zhihu.com/people/ysgc", 
        "https://www.zhihu.com/people/erwei-lu", 
        "https://www.zhihu.com/people/lin-hao-xing-57", 
        "https://www.zhihu.com/people/ch0588", 
        "https://www.zhihu.com/people/wang-yu-xuan-46-64", 
        "https://www.zhihu.com/people/xing-yu-39-16", 
        "https://www.zhihu.com/people/aleck_zhang13", 
        "https://www.zhihu.com/people/antler-14-78", 
        "https://www.zhihu.com/people/kira-11-6", 
        "https://www.zhihu.com/people/imcl-49", 
        "https://www.zhihu.com/people/xiong-qiu-77", 
        "https://www.zhihu.com/people/zhou-hang-5", 
        "https://www.zhihu.com/people/niceworld-49-6", 
        "https://www.zhihu.com/people/DeepLearnerLT", 
        "https://www.zhihu.com/people/zhang-andi", 
        "https://www.zhihu.com/people/constan-tine", 
        "https://www.zhihu.com/people/xdd-2", 
        "https://www.zhihu.com/people/ni-wei-tai-yang", 
        "https://www.zhihu.com/people/wang-wang-61-41-86", 
        "https://www.zhihu.com/people/xiaojidan", 
        "https://www.zhihu.com/people/mamy", 
        "https://www.zhihu.com/people/quxiaofeng", 
        "https://www.zhihu.com/people/IsingZhang", 
        "https://www.zhihu.com/people/jie-geng-74-4-4", 
        "https://www.zhihu.com/people/li-zhu-92", 
        "https://www.zhihu.com/people/hao-feng-hao-65", 
        "https://www.zhihu.com/people/guo-yi-yun-2", 
        "https://www.zhihu.com/people/xu-wan-jin-6", 
        "https://www.zhihu.com/people/lu-jie-10-70", 
        "https://www.zhihu.com/people/ykp-41", 
        "https://www.zhihu.com/people/wang-wang-wang-34-47", 
        "https://www.zhihu.com/people/michael-wu-25", 
        "https://www.zhihu.com/people/Micro-Kun", 
        "https://www.zhihu.com/people/liu-lin-qi-78", 
        "https://www.zhihu.com/people/ticktock-47", 
        "https://www.zhihu.com/people/cover_ShangGuan", 
        "https://www.zhihu.com/people/xuhaiyang-86", 
        "https://www.zhihu.com/people/wu-ye-nan-30", 
        "https://www.zhihu.com/people/liang-yuan-88", 
        "https://www.zhihu.com/people/cao-ji-49-42", 
        "https://www.zhihu.com/people/edward-77-20", 
        "https://www.zhihu.com/people/guomuguomunuo", 
        "https://www.zhihu.com/people/flaster", 
        "https://www.zhihu.com/people/aaa-79-26", 
        "https://www.zhihu.com/people/ck_welder", 
        "https://www.zhihu.com/people/Johnqoe", 
        "https://www.zhihu.com/people/abelard", 
        "https://www.zhihu.com/people/xu-jin-78-90", 
        "https://www.zhihu.com/people/yildhd-wang", 
        "https://www.zhihu.com/people/dong-feng-zao-ji", 
        "https://www.zhihu.com/people/tang-li-wei-52", 
        "https://www.zhihu.com/people/hijackjave", 
        "https://www.zhihu.com/people/tengwei9", 
        "https://www.zhihu.com/people/pengzx-11", 
        "https://www.zhihu.com/people/666233-95-78", 
        "https://www.zhihu.com/people/chen-xi-3-93-81", 
        "https://www.zhihu.com/people/luo-xiao-qi-39", 
        "https://www.zhihu.com/people/duan-xing-99", 
        "https://www.zhihu.com/people/tinker-34-37", 
        "https://www.zhihu.com/people/hong-alex", 
        "https://www.zhihu.com/people/rainbow_jjh", 
        "https://www.zhihu.com/people/lxgend", 
        "https://www.zhihu.com/people/zhao-jia-hao-67", 
        "https://www.zhihu.com/people/ai-mao-25-5-31", 
        "https://www.zhihu.com/people/commiyou", 
        "https://www.zhihu.com/people/lei-xiao-yu-49", 
        "https://www.zhihu.com/people/qqqq-12-71", 
        "https://www.zhihu.com/people/kosmasdiogenes", 
        "https://www.zhihu.com/people/xierry-41", 
        "https://www.zhihu.com/people/cat-lucky-5", 
        "https://www.zhihu.com/people/chen-chen-chen-31-39", 
        "https://www.zhihu.com/people/j-ames", 
        "https://www.zhihu.com/people/ybtk", 
        "https://www.zhihu.com/people/buptguo", 
        "https://www.zhihu.com/people/a-piece-of-bread", 
        "https://www.zhihu.com/people/hu-yidao-96-83", 
        "https://www.zhihu.com/people/willwinworld", 
        "https://www.zhihu.com/people/jin-se-liang-dian-ban", 
        "https://www.zhihu.com/people/zztx-70", 
        "https://www.zhihu.com/people/wang-hg", 
        "https://www.zhihu.com/people/chen-xing-han-41", 
        "https://www.zhihu.com/people/chen-zhen-hao-19", 
        "https://www.zhihu.com/people/sun-yan-90-29", 
        "https://www.zhihu.com/people/wu-hao-15-22", 
        "https://www.zhihu.com/people/sun-xiao-fei-75-94", 
        "https://www.zhihu.com/people/li-yi-3-54", 
        "https://www.zhihu.com/people/jinshenhehuan", 
        "https://www.zhihu.com/people/wang-tong-78-19", 
        "https://www.zhihu.com/people/wang-fei-46-4", 
        "https://www.zhihu.com/people/jiude-luo", 
        "https://www.zhihu.com/people/lang-man-jing-ye-si", 
        "https://www.zhihu.com/people/ma-yu-xiang-4", 
        "https://www.zhihu.com/people/zhaopeng-2", 
        "https://www.zhihu.com/people/harric", 
        "https://www.zhihu.com/people/shen-jing-jie-de-gu-du", 
        "https://www.zhihu.com/people/huwei-50-7", 
        "https://www.zhihu.com/people/yang-jing-lin-16", 
        "https://www.zhihu.com/people/mountain-blue-64", 
        "https://www.zhihu.com/people/jin-yao-hui-86", 
        "https://www.zhihu.com/people/locker87", 
        "https://www.zhihu.com/people/gebixiaowang110", 
        "https://www.zhihu.com/people/jia-xue-feng", 
        "https://www.zhihu.com/people/mattzheng7", 
        "https://www.zhihu.com/people/zhang-da-xian-1973", 
        "https://www.zhihu.com/people/she-liang", 
        "https://www.zhihu.com/people/yu-luo-22-49", 
        "https://www.zhihu.com/people/zhang-hong-feng-45", 
        "https://www.zhihu.com/people/yan-si-de-yu-ysdy44-96", 
        "https://www.zhihu.com/people/bow-rain-95", 
        "https://www.zhihu.com/people/zhong-er-cheng-xu-yuan", 
        "https://www.zhihu.com/people/liu-xiao-yong-80-42", 
        "https://www.zhihu.com/people/wangbicong", 
        "https://www.zhihu.com/people/liu-fei-94-95", 
        "https://www.zhihu.com/people/wanghaihe", 
        "https://www.zhihu.com/people/ceng-yu-wei-36-33", 
        "https://www.zhihu.com/people/li-zhao-yue-74", 
        "https://www.zhihu.com/people/shen-xing-ke-14", 
        "https://www.zhihu.com/people/chen-xiang-qing-47", 
        "https://www.zhihu.com/people/he-en-dong", 
        "https://www.zhihu.com/people/xu-luo-song", 
        "https://www.zhihu.com/people/matrix-48", 
        "https://www.zhihu.com/people/fen-dou-de-qing-chun-66-72", 
        "https://www.zhihu.com/people/li-liang-de-96", 
        "https://www.zhihu.com/people/axibulifu", 
        "https://www.zhihu.com/people/jian-kang-kuai-le-69-67", 
        "https://www.zhihu.com/people/mu-yang-28-39", 
        "https://www.zhihu.com/people/zi-an-15-68", 
        "https://www.zhihu.com/people/theo-97-5", 
        "https://www.zhihu.com/people/guapi1024", 
        "https://www.zhihu.com/people/long-xin-wei-3", 
        "https://www.zhihu.com/people/sa-lu-nian-si-sa-lu", 
        "https://www.zhihu.com/people/liu-xian-sen-98-32", 
        "https://www.zhihu.com/people/min-da-39", 
        "https://www.zhihu.com/people/zafedom", 
        "https://www.zhihu.com/people/le-tian-41-71", 
        "https://www.zhihu.com/people/li-yifei-48-25", 
        "https://www.zhihu.com/people/qqlzfmn", 
        "https://www.zhihu.com/people/xiong-gavin", 
        "https://www.zhihu.com/people/pan-xiang-9-14", 
        "https://www.zhihu.com/people/an-an-31-90", 
        "https://www.zhihu.com/people/hao-zhang-46-93", 
        "https://www.zhihu.com/people/dragonfly", 
        "https://www.zhihu.com/people/lydna-51", 
        "https://www.zhihu.com/people/lingyv", 
        "https://www.zhihu.com/people/wen-hong-chen", 
        "https://www.zhihu.com/people/stanleyhu-59", 
        "https://www.zhihu.com/people/shenyinian", 
        "https://www.zhihu.com/people/HWGuderian1994", 
        "https://www.zhihu.com/people/liu-yi-25-9", 
        "https://www.zhihu.com/people/yan-jun-80-45", 
        "https://www.zhihu.com/people/ni-hui-51-23", 
        "https://www.zhihu.com/people/gao-zhi-yong-21-3", 
        "https://www.zhihu.com/people/ku-gou-82", 
        "https://www.zhihu.com/people/wang-jing-bo-27-88", 
        "https://www.zhihu.com/people/feng-xing-long-5", 
        "https://www.zhihu.com/people/lan-lan-lan-lan-96-82", 
        "https://www.zhihu.com/people/feng-xia-chong-90", 
        "https://www.zhihu.com/people/zhao-fu-bang-95", 
        "https://www.zhihu.com/people/yuan-li-39-78", 
        "https://www.zhihu.com/people/li-yue-38-12-49", 
        "https://www.zhihu.com/people/qinlibo_nlp", 
        "https://www.zhihu.com/people/liu-bo-33-1-2"
    ], 
    "article": [
        {
            "url": "https://zhuanlan.zhihu.com/p/56186865", 
            "userName": "田卿", 
            "userLink": "https://www.zhihu.com/people/c5b07c6b53cc7f9e2edd185fdc4a0d45", 
            "upvote": 11, 
            "title": "用Gym学习强化学习 —— Policy Gradient", 
            "content": "<h2><b>目录</b></h2><ul><li>什么是强化学习</li><li>强化学习的问题要素</li><li>Gym简介</li><li>Policy Gradient实战</li><li>总结</li><li>参考</li></ul><h2><b>1 什么是强化学习</b></h2><p><b>强化学习</b>在机器学习的应用分类里常常和<b>监督学习</b>和<b>非监督学习</b>并列。</p><p>在<b>监督学习</b>和<b>非监督学习</b>中，我们可以获得固定的数据集，将数据集喂给特定的模型，拟合数学公式，学习其特征规律。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3687e5af60853b8eca8967e2b8f72761_b.jpg\" data-size=\"normal\" data-rawwidth=\"862\" data-rawheight=\"393\" class=\"origin_image zh-lightbox-thumb\" width=\"862\" data-original=\"https://pic2.zhimg.com/v2-3687e5af60853b8eca8967e2b8f72761_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;862&#39; height=&#39;393&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"862\" data-rawheight=\"393\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"862\" data-original=\"https://pic2.zhimg.com/v2-3687e5af60853b8eca8967e2b8f72761_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-3687e5af60853b8eca8967e2b8f72761_b.jpg\"/><figcaption>监督学习 VS 非监督学习</figcaption></figure><p>在上述问题中，我们需要考虑的问题是如何将已有数据集的知识迁移到新的数据集中。</p><p>然而，<b>强化学习</b>并不属于这类问题。在强化学习里，我们没有现成的数据集可以学习，而是需要在环境中“试错”，根据环境给我们的反馈进行模型参数的调整。</p><p>所以，<b>反馈</b>即是强化学习独特的问题特征。</p><h2><b>2 强化学习的问题要素</b></h2><p>在强化学习中，三个重要的要素是：<b>Actor(Agent)、Environment、Reward</b>。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-8c6215c0214dc8129c590dbe5d5173c8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1478\" data-rawheight=\"974\" class=\"origin_image zh-lightbox-thumb\" width=\"1478\" data-original=\"https://pic1.zhimg.com/v2-8c6215c0214dc8129c590dbe5d5173c8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1478&#39; height=&#39;974&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1478\" data-rawheight=\"974\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1478\" data-original=\"https://pic1.zhimg.com/v2-8c6215c0214dc8129c590dbe5d5173c8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-8c6215c0214dc8129c590dbe5d5173c8_b.jpg\"/></figure><p>这三者的关系可以表示为：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-88ee8fc4fa19aa32a634a3079c1680ae_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"177\" data-rawheight=\"111\" class=\"content_image\" width=\"177\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;177&#39; height=&#39;111&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"177\" data-rawheight=\"111\" class=\"content_image lazy\" width=\"177\" data-actualsrc=\"https://pic3.zhimg.com/v2-88ee8fc4fa19aa32a634a3079c1680ae_b.jpg\"/></figure><p><b>Agent</b>是我们学习的对象，Agent可以做出自己的行为action，然后获取<b>Environment</b>反馈的信息observation(state)。根据observation(state)附带的激励<b>reward</b>，Agent可以判断本次action的好坏，进而调整自己的参数。同时，本次observation(state)将继续作为Agent的输入进行下一次行为action的判断。</p><p>这种根据环境反馈来进行模型学习的方式就是强化学习。</p><h2><b>3 Gym简介</b></h2><p>听完上面的描述，读者可能对强化学习还存在较为模糊的认知，这里选择Gym平台来让大家直观感受强化学习的训练过程。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-334cbdb9c8e47ebe99d3c82e4438b82f_b.jpg\" data-size=\"normal\" data-rawwidth=\"2558\" data-rawheight=\"1236\" class=\"origin_image zh-lightbox-thumb\" width=\"2558\" data-original=\"https://pic4.zhimg.com/v2-334cbdb9c8e47ebe99d3c82e4438b82f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2558&#39; height=&#39;1236&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"2558\" data-rawheight=\"1236\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2558\" data-original=\"https://pic4.zhimg.com/v2-334cbdb9c8e47ebe99d3c82e4438b82f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-334cbdb9c8e47ebe99d3c82e4438b82f_b.jpg\"/><figcaption>Gym 环境预览</figcaption></figure><p><a href=\"https://link.zhihu.com/?target=https%3A//gym.openai.com/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Gym</a>是Openai开发的一个用来测试强化学习算法的工具。当我们使用Gym的时候，我们只需要重点关注Agent的训练过程，而Environment、reward这些信息都将由Gym平台提供，无需我们进行额外的设计和开发。</p><p>这里我们选择较为简单的<a href=\"https://link.zhihu.com/?target=https%3A//gym.openai.com/envs/CartPole-v1/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">CartPole-v1</a>进行强化学习算法Policy Gradient的实现：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-08550134f5ad2379315623293e959ea2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2558\" data-rawheight=\"1198\" class=\"origin_image zh-lightbox-thumb\" width=\"2558\" data-original=\"https://pic3.zhimg.com/v2-08550134f5ad2379315623293e959ea2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2558&#39; height=&#39;1198&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2558\" data-rawheight=\"1198\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2558\" data-original=\"https://pic3.zhimg.com/v2-08550134f5ad2379315623293e959ea2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-08550134f5ad2379315623293e959ea2_b.jpg\"/></figure><p>在这个任务中，小车Cart通过左移和右移来保证竿子Pole的平衡，时间越久奖励reward越高。</p><p>在使用前，请根据<a href=\"https://link.zhihu.com/?target=https%3A//gym.openai.com/docs/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">官方文档</a>安装其Gym，本文选择的工具为<b>Python 3.6，Pytorch 0.4</b>。</p><h2><b>4 Policy Gradient实战</b></h2><p>在Policy Gradient中，我们需要训练一个Agent。这个Agent相当于一个<b>分类器</b>，其输入是观测到环境的信息observation(state)，输出为行为action的概率分布。我们可以用简单的多层感知机去实现这个分类器：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-d24b7c5686243f9aa97db8ff31a7790d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1054\" data-rawheight=\"450\" class=\"origin_image zh-lightbox-thumb\" width=\"1054\" data-original=\"https://pic2.zhimg.com/v2-d24b7c5686243f9aa97db8ff31a7790d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1054&#39; height=&#39;450&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1054\" data-rawheight=\"450\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1054\" data-original=\"https://pic2.zhimg.com/v2-d24b7c5686243f9aa97db8ff31a7790d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-d24b7c5686243f9aa97db8ff31a7790d_b.jpg\"/></figure><p>在该任务中，可以查看输入observation(state)和输出action的维度：</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"kn\">import</span> <span class=\"nn\">gym</span>\n<span class=\"n\">env</span> <span class=\"o\">=</span> <span class=\"n\">gym</span><span class=\"o\">.</span><span class=\"n\">make</span><span class=\"p\">(</span><span class=\"s1\">&#39;CartPole-v1&#39;</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">action_space</span><span class=\"p\">)</span>\n<span class=\"c1\"># Discrete(2)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">observation_space</span><span class=\"p\">)</span>\n<span class=\"c1\"># Box(4,)</span></code></pre></div><p>我们可以构建一个简单的多层感知机充当二分类器：</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"k\">class</span> <span class=\"nc\">PGN</span><span class=\"p\">(</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Module</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"nf\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"nb\">super</span><span class=\"p\">(</span><span class=\"n\">PGN</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">()</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">linear1</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">24</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">linear2</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">24</span><span class=\"p\">,</span> <span class=\"mi\">36</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">linear3</span> <span class=\"o\">=</span> <span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">Linear</span><span class=\"p\">(</span><span class=\"mi\">36</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">forward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">relu</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">linear1</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">))</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">F</span><span class=\"o\">.</span><span class=\"n\">relu</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">linear2</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">))</span>\n        <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">sigmoid</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">linear3</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">))</span>\n        <span class=\"k\">return</span> <span class=\"n\">x</span></code></pre></div><p>我们的需要训练一个CartAgent，应当至少具备以下接口：</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"k\">class</span> <span class=\"nc\">CartAgent</span><span class=\"p\">(</span><span class=\"nb\">object</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"nf\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span><span class=\"p\">,</span> <span class=\"n\">gamma</span><span class=\"p\">):</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">pgn</span> <span class=\"o\">=</span> <span class=\"n\">PGN</span><span class=\"p\">()</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">gamma</span> <span class=\"o\">=</span> <span class=\"n\">gamma</span>\n\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">_init_memory</span><span class=\"p\">()</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">optim</span><span class=\"o\">.</span><span class=\"n\">RMSprop</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">pgn</span><span class=\"o\">.</span><span class=\"n\">parameters</span><span class=\"p\">(),</span> <span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"n\">learning_rate</span><span class=\"p\">)</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">memorize</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">state</span><span class=\"p\">,</span> <span class=\"n\">action</span><span class=\"p\">,</span> <span class=\"n\">reward</span><span class=\"p\">):</span>\n        <span class=\"c1\"># save to memory for mini-batch gradient descent</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">state_pool</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">state</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">action_pool</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">action</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">reward_pool</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">reward</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">steps</span> <span class=\"o\">+=</span> <span class=\"mi\">1</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">learn</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"k\">pass</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">act</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">state</span><span class=\"p\">):</span>\n        <span class=\"k\">return</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">pgn</span><span class=\"p\">(</span><span class=\"n\">state</span><span class=\"p\">)</span></code></pre></div><p>前面提过，强化学习不同于监督学习（能够直接使用训练语料中的X和Y拟合该模型）。在Policy Gradient中，我们需要使用特殊的损失函数：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-5edb984ac2f3536223a0121dc5a3e472_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1436\" data-rawheight=\"1016\" class=\"origin_image zh-lightbox-thumb\" width=\"1436\" data-original=\"https://pic3.zhimg.com/v2-5edb984ac2f3536223a0121dc5a3e472_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1436&#39; height=&#39;1016&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1436\" data-rawheight=\"1016\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1436\" data-original=\"https://pic3.zhimg.com/v2-5edb984ac2f3536223a0121dc5a3e472_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-5edb984ac2f3536223a0121dc5a3e472_b.jpg\"/></figure><p>其中 <img src=\"https://www.zhihu.com/equation?tex=p%28a%7Cs%29\" alt=\"p(a|s)\" eeimg=\"1\"/> 由Agent模型计算得出，<img src=\"https://www.zhihu.com/equation?tex=R\" alt=\"R\" eeimg=\"1\"/> 即Reward，由环境给出。<b>这个损失函数能够提高reward值大的action出现的概率</b>。loss的核心实现为：</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"k\">def</span> <span class=\"nf\">learn</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n    <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">_adjust_reward</span><span class=\"p\">()</span>\n\n    <span class=\"c1\"># policy gradient</span>\n    <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">zero_grad</span><span class=\"p\">()</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">steps</span><span class=\"p\">):</span>\n        <span class=\"c1\"># all steps in multi games </span>\n        <span class=\"n\">state</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">state_pool</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span>\n        <span class=\"n\">action</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">FloatTensor</span><span class=\"p\">([</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">action_pool</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]])</span>\n        <span class=\"n\">reward</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">reward_pool</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span>\n\n        <span class=\"n\">probs</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">act</span><span class=\"p\">(</span><span class=\"n\">state</span><span class=\"p\">)</span>\n        <span class=\"n\">m</span> <span class=\"o\">=</span> <span class=\"n\">Bernoulli</span><span class=\"p\">(</span><span class=\"n\">probs</span><span class=\"p\">)</span>\n        <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"o\">-</span><span class=\"n\">m</span><span class=\"o\">.</span><span class=\"n\">log_prob</span><span class=\"p\">(</span><span class=\"n\">action</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">reward</span>\n        <span class=\"n\">loss</span><span class=\"o\">.</span><span class=\"n\">backward</span><span class=\"p\">()</span>\n    <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">()</span>\n    \n    <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">_init_memory</span><span class=\"p\">()</span></code></pre></div><p>想要让该损失函数得到训练，我们就必须获得多组state，action，reward。在实际应用中，这样的组合需要我们在游戏过程中随机抽样出来。</p><p>对于某次游戏，我们可以抽样出多组state、action、reward：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-51ecf77b520d47af50c358fe5f573ae8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1398\" data-rawheight=\"1064\" class=\"origin_image zh-lightbox-thumb\" width=\"1398\" data-original=\"https://pic1.zhimg.com/v2-51ecf77b520d47af50c358fe5f573ae8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1398&#39; height=&#39;1064&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1398\" data-rawheight=\"1064\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1398\" data-original=\"https://pic1.zhimg.com/v2-51ecf77b520d47af50c358fe5f573ae8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-51ecf77b520d47af50c358fe5f573ae8_b.jpg\"/></figure><p>具体的实现为：</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"c1\"># hyper parameter</span>\n<span class=\"n\">BATCH_SIZE</span> <span class=\"o\">=</span> <span class=\"mi\">5</span>\n<span class=\"n\">LEARNING_RATE</span> <span class=\"o\">=</span> <span class=\"mf\">0.01</span>\n<span class=\"n\">GAMMA</span> <span class=\"o\">=</span> <span class=\"mf\">0.99</span>\n<span class=\"n\">NUM_EPISODES</span> <span class=\"o\">=</span> <span class=\"mi\">500</span>\n\n<span class=\"n\">env</span> <span class=\"o\">=</span> <span class=\"n\">gym</span><span class=\"o\">.</span><span class=\"n\">make</span><span class=\"p\">(</span><span class=\"s1\">&#39;CartPole-v1&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">cart_agent</span> <span class=\"o\">=</span> <span class=\"n\">CartAgent</span><span class=\"p\">(</span><span class=\"n\">learning_rate</span><span class=\"o\">=</span><span class=\"n\">LEARNING_RATE</span><span class=\"p\">,</span> <span class=\"n\">gamma</span><span class=\"o\">=</span><span class=\"n\">GAMMA</span><span class=\"p\">)</span>\n\n<span class=\"k\">for</span> <span class=\"n\">i_episode</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">NUM_EPISODES</span><span class=\"p\">):</span>\n    <span class=\"n\">next_state</span> <span class=\"o\">=</span> <span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">reset</span><span class=\"p\">()</span>\n    <span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">render</span><span class=\"p\">(</span><span class=\"n\">mode</span><span class=\"o\">=</span><span class=\"s1\">&#39;rgb_array&#39;</span><span class=\"p\">)</span>\n\n    <span class=\"k\">for</span> <span class=\"n\">t</span> <span class=\"ow\">in</span> <span class=\"n\">count</span><span class=\"p\">():</span>\n        <span class=\"n\">state</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">from_numpy</span><span class=\"p\">(</span><span class=\"n\">next_state</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"nb\">float</span><span class=\"p\">()</span>\n\n        <span class=\"n\">probs</span> <span class=\"o\">=</span> <span class=\"n\">cart_agent</span><span class=\"o\">.</span><span class=\"n\">act</span><span class=\"p\">(</span><span class=\"n\">state</span><span class=\"p\">)</span>\n        <span class=\"n\">m</span> <span class=\"o\">=</span> <span class=\"n\">Bernoulli</span><span class=\"p\">(</span><span class=\"n\">probs</span><span class=\"p\">)</span>\n        <span class=\"n\">action</span> <span class=\"o\">=</span> <span class=\"n\">m</span><span class=\"o\">.</span><span class=\"n\">sample</span><span class=\"p\">()</span>\n\n        <span class=\"n\">action</span> <span class=\"o\">=</span> <span class=\"n\">action</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">numpy</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">astype</span><span class=\"p\">(</span><span class=\"nb\">int</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">item</span><span class=\"p\">()</span>\n        <span class=\"n\">next_state</span><span class=\"p\">,</span> <span class=\"n\">reward</span><span class=\"p\">,</span> <span class=\"n\">done</span><span class=\"p\">,</span> <span class=\"n\">_</span> <span class=\"o\">=</span> <span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">(</span><span class=\"n\">action</span><span class=\"p\">)</span>\n        <span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">render</span><span class=\"p\">(</span><span class=\"n\">mode</span><span class=\"o\">=</span><span class=\"s1\">&#39;rgb_array&#39;</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># end action&#39;s reward equals 0</span>\n        <span class=\"k\">if</span> <span class=\"n\">done</span><span class=\"p\">:</span>\n            <span class=\"n\">reward</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n\n        <span class=\"n\">cart_agent</span><span class=\"o\">.</span><span class=\"n\">memorize</span><span class=\"p\">(</span><span class=\"n\">state</span><span class=\"p\">,</span> <span class=\"n\">action</span><span class=\"p\">,</span> <span class=\"n\">reward</span><span class=\"p\">)</span>\n\n        <span class=\"k\">if</span> <span class=\"n\">done</span><span class=\"p\">:</span>\n            <span class=\"n\">logger</span><span class=\"o\">.</span><span class=\"n\">info</span><span class=\"p\">({</span><span class=\"s1\">&#39;Episode </span><span class=\"si\">{}</span><span class=\"s1\">: durations </span><span class=\"si\">{}</span><span class=\"s1\">&#39;</span><span class=\"o\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"n\">i_episode</span><span class=\"p\">,</span> <span class=\"n\">t</span><span class=\"p\">)})</span>\n            <span class=\"k\">break</span>\n\n    <span class=\"c1\"># update parameter every batch size</span>\n    <span class=\"k\">if</span> <span class=\"n\">i_episode</span> <span class=\"o\">&gt;</span> <span class=\"mi\">0</span> <span class=\"ow\">and</span> <span class=\"n\">i_episode</span> <span class=\"o\">%</span> <span class=\"n\">BATCH_SIZE</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n        <span class=\"n\">cart_agent</span><span class=\"o\">.</span><span class=\"n\">learn</span><span class=\"p\">()</span></code></pre></div><p>上述state、action、reward组合存在一定的问题。在这个游戏中所有的reward均为0或1，但是某次游戏实际进行了多轮，某次action产生的效益可能远远大于其他相同reward的action。因此，我们需要在某次游戏中结合整个马尔科夫链去考虑某次action的<b>潜在reward</b>。</p><p>一种很容易想到的思路是，如果某次action之后的reward很大，说明这次action更好，我们可以将这种潜在的reward添加进来：</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"k\">def</span> <span class=\"nf\">_adjust_reward</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n    <span class=\"c1\"># backward weight</span>\n    <span class=\"n\">running_add</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">reversed</span><span class=\"p\">(</span><span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">steps</span><span class=\"p\">)):</span>\n        <span class=\"k\">if</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">reward_pool</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n            <span class=\"n\">running_add</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n        <span class=\"k\">else</span><span class=\"p\">:</span>\n            <span class=\"n\">running_add</span> <span class=\"o\">=</span> <span class=\"n\">running_add</span> <span class=\"o\">*</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">gamma</span> <span class=\"o\">+</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">reward_pool</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span>\n            <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">reward_pool</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">running_add</span></code></pre></div><p>此外，我们需要reward进行适当的<b>均一化</b>。如果reward均为正，则在抽样过程中可能会让一些本应当降低概率的action获得更大的概率提升；如果reward没有进行平均，则游戏行为更多的action组倾向于获得更大的概率提升。</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"c1\"># normalize reward</span>\n    <span class=\"n\">reward_mean</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">reward_pool</span><span class=\"p\">)</span>\n    <span class=\"n\">reward_std</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">std</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">reward_pool</span><span class=\"p\">)</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">steps</span><span class=\"p\">):</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">reward_pool</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">reward_pool</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span> <span class=\"o\">-</span> <span class=\"n\">reward_mean</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">reward_std</span></code></pre></div><p>完成代码后，可以看到实验结果。在起初的时候，小车只能坚持较短的时间：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-3df4e6b6bbfbbf94ba26f8864be1bcef_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1176\" data-rawheight=\"718\" class=\"origin_image zh-lightbox-thumb\" width=\"1176\" data-original=\"https://pic4.zhimg.com/v2-3df4e6b6bbfbbf94ba26f8864be1bcef_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1176&#39; height=&#39;718&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1176\" data-rawheight=\"718\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1176\" data-original=\"https://pic4.zhimg.com/v2-3df4e6b6bbfbbf94ba26f8864be1bcef_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-3df4e6b6bbfbbf94ba26f8864be1bcef_b.jpg\"/></figure><p>进行150次游戏后，小车坚持的时间已经有了显著的提高。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-f764dc71c1e3ba1bdc3aa20e95390313_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1258\" data-rawheight=\"774\" class=\"origin_image zh-lightbox-thumb\" width=\"1258\" data-original=\"https://pic4.zhimg.com/v2-f764dc71c1e3ba1bdc3aa20e95390313_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1258&#39; height=&#39;774&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1258\" data-rawheight=\"774\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1258\" data-original=\"https://pic4.zhimg.com/v2-f764dc71c1e3ba1bdc3aa20e95390313_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-f764dc71c1e3ba1bdc3aa20e95390313_b.jpg\"/></figure><h2><b>5 总结</b></h2><p>从实践结果看，实现一个Demo级别的Policy Gradient算法就可以取得较好的效果。如果你想学习强化学习算法，Gym将会是一个测试这些算法的好帮手。</p><h2><b>6 参考</b></h2><ol><li><a href=\"https://link.zhihu.com/?target=https%3A//gym.openai.com/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Gym: A toolkit for developing and comparing reinforcement learning algorithms</a></li><li><a href=\"https://link.zhihu.com/?target=http%3A//speech.ee.ntu.edu.tw/~tlkagk/courses/MLDS_2018/Lecture/PPO%2520%28v3%29.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">台湾大学-李宏毅《Deep Reinforcement Learning》</a></li><li><a href=\"https://link.zhihu.com/?target=https%3A//pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Reinforcement Learning (DQN) tutorial</a></li></ol>", 
            "topic": [
                {
                    "tag": "强化学习 (Reinforcement Learning)", 
                    "tagLink": "https://api.zhihu.com/topics/20039099"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "人工智能", 
                    "tagLink": "https://api.zhihu.com/topics/19551275"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/55981383", 
            "userName": "田卿", 
            "userLink": "https://www.zhihu.com/people/c5b07c6b53cc7f9e2edd185fdc4a0d45", 
            "upvote": 57, 
            "title": "中文同义词挖掘工作的踩坑之路", 
            "content": "<h2>目录</h2><ol><li>中文同义词挖掘简介</li><li>词对的数据来源</li><li>词对数据的解决思路</li><li>同义语句的数据来源</li><li>同义语句数据的解决思路</li><li>其他</li></ol><h2>1 中文同义词挖掘简介</h2><p>同义词挖掘是自然语言处理中一个较为基础的问题，但是现有的同义词词表不足以应对许多场景下的工作，所以我们需要在新的词对中寻找同义词。</p><p>但事实上，当你开始检索相关资料，就能发现这个问题的困难之处，在我们数据量和数据信息都偏少的情况下，很难保证这个问题能够被很好解决，只能在精确率和召回率这样的指标之间尽量折中。</p><p>经过一番论文阅读，我总结出同义词挖掘的数据构造思路主要有以下几种：</p><ol><li>给定若干中文同义词对，寻找新的中文同义词对。</li><li>给定外文（主要是英文）同义词词典，翻译为对齐的中文同义词词典。</li><li>给定若干句子，抽取出句子中特定词（比如实体）的同义关系。</li></ol><p>其中，数据 1 和 2 的数据形式相似，都是利用给定的词对寻找新的词对。但这些数据可利用的上下文信息较少，效果会很受预训练词向量质量的影响。而数据 3 在句子中挖掘同义关系，可以利用到句子中的上下文信息，但同时也可能引入较多噪声。</p><h2>2 词对的数据来源</h2><p>数据 1 可以从常见的<b>中文同义词词典</b>中构建，如哈工大社会计算与信息检索研究中心整理的《<b>同义词词林</b>》。</p><p>数据 2 可以使用<b>英文同义词词典</b>，比如 <b>WordNet、VerbNet </b>这样的词网。</p><p>预训练词向量可以使用 Word2vec 训练的 Wikipedia 数据。但是此处建议大家尝试腾讯 AI Lab 开源的 <a href=\"https://link.zhihu.com/?target=https%3A//ai.tencent.com/ailab/nlp/embedding.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">800W Word Embedding</a>，用过都说好。</p><h2>3 词对数据的解决思路</h2><p>较为直观的思路是：</p><ol><li>直接利用<b>预训练的词向量</b>计算词语间的<b>相似度</b>（如<b>余弦相似度</b>）。</li><li>将该问题转化为二分类问题，<b>输入</b>为<b>两个词的词向量</b>，<b>输出</b>为<b>是否是同义词 </b>{0, 1} 。</li></ol><p>上述思路效果一般，主要因为这样预训练的词向量不能准确捕捉同义关系，事实上诸如 Word2vec 这样类型的词向量捕捉到的是同位关系（包括同义、同类等）。这就导致利用词向量相似来判断的结果只能保证<b>较高的召回率</b>，很难保证<b>精确率</b>。</p><p>而用简单的分类思路去做，也会因为数据量的不足导致<b>分类效果不够好</b>。比如对于数据 2 ，中文是没有出现在英文词典上的。</p><p>在<b>词表</b>相对<b>封闭集</b>的情况下（训练集和测试集的词表接近），有一些工作能够较好地解决这个问题：</p><blockquote>EMNLP 2017 《Cross-Lingual Induction and Transfer of Verb Classes Based on Word Vector Space Specialisation》</blockquote><p>这篇工作主要用于<b>跨语言动词词表对齐</b>， 英文词表如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-35c200921d0a90da36235b7655e31530_b.jpg\" data-rawwidth=\"1234\" data-rawheight=\"512\" data-size=\"normal\" class=\"origin_image zh-lightbox-thumb\" width=\"1234\" data-original=\"https://pic1.zhimg.com/v2-35c200921d0a90da36235b7655e31530_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1234&#39; height=&#39;512&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1234\" data-rawheight=\"512\" data-size=\"normal\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1234\" data-original=\"https://pic1.zhimg.com/v2-35c200921d0a90da36235b7655e31530_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-35c200921d0a90da36235b7655e31530_b.jpg\"/><figcaption>VerbNet 图示</figcaption></figure><p>基本思路是<b>微调预训练的词向量</b>，进而<b>拉近同义词对的距离</b>，<b>拉远临近非同义词对的距离</b>，同时加上<b>正则项保证调整的幅度</b>。这个思路体现在损失函数的设计中：</p><p><img src=\"https://www.zhihu.com/equation?tex=O+%5Cleft%28+%5Cmathcal+%7B+B+%7D+_+%7B+C+%7D+%5Cright%29+%3D+O+_+%7B+C+%7D+%5Cleft%28+%5Cmathcal+%7B+B+%7D+_+%7B+C+%7D+%5Cright%29+%2B+R+%5Cleft%28+%5Cmathcal+%7B+B+%7D+_+%7B+C+%7D+%5Cright%29\" alt=\"O \\left( \\mathcal { B } _ { C } \\right) = O _ { C } \\left( \\mathcal { B } _ { C } \\right) + R \\left( \\mathcal { B } _ { C } \\right)\" eeimg=\"1\"/> </p><p>其中 <img src=\"https://www.zhihu.com/equation?tex=O_c\" alt=\"O_c\" eeimg=\"1\"/> 表示词向量的调整：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+O+_+%7B+C+%7D+%5Cleft%28+%5Cmathcal+%7B+B+%7D+_+%7B+C+%7D+%5Cright%29+%3D+%26+%5Csum+_+%7B+%5Cleft%28+x+_+%7B+l+%7D+%2C+x+_+%7B+r+%7D+%5Cright%29+%5Cin+%5Cmathcal+%7B+B+%7D+_+%7B+C+%7D+%7D+%5Cleft%5B+%5Ctau+%5Cleft%28+%5Cdelta+_+%7B+a+t+t+%7D+%2B+%5Cmathbf+%7B+x+%7D+_+%7B+l+%7D+%5Cmathbf+%7B+t+%7D+_+%7B+l+%7D+-+%5Cmathbf+%7B+x+%7D+_+%7B+l+%7D+%5Cmathbf+%7B+x+%7D+_+%7B+r+%7D+%5Cright%29+%5Cright.++%2B+%5Ctau+%5Cleft%28+%5Cdelta+_+%7B+a+t+t+%7D+%2B+%5Cmathbf+%7B+x+%7D+_+%7B+r+%7D+%5Cmathbf+%7B+t+%7D+_+%7B+r+%7D+-+%5Cmathbf+%7B+x+%7D+_+%7B+l+%7D+%5Cmathbf+%7B+x+%7D+_+%7B+r+%7D+%5Cright%29+%5D+%5Cend%7Baligned%7D\" alt=\"\\begin{aligned} O _ { C } \\left( \\mathcal { B } _ { C } \\right) = &amp; \\sum _ { \\left( x _ { l } , x _ { r } \\right) \\in \\mathcal { B } _ { C } } \\left[ \\tau \\left( \\delta _ { a t t } + \\mathbf { x } _ { l } \\mathbf { t } _ { l } - \\mathbf { x } _ { l } \\mathbf { x } _ { r } \\right) \\right.  + \\tau \\left( \\delta _ { a t t } + \\mathbf { x } _ { r } \\mathbf { t } _ { r } - \\mathbf { x } _ { l } \\mathbf { x } _ { r } \\right) ] \\end{aligned}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=R\" alt=\"R\" eeimg=\"1\"/> 表示正则项：</p><p><img src=\"https://www.zhihu.com/equation?tex=R+%5Cleft%28+%5Cmathcal+%7B+B+%7D+_+%7B+C+%7D+%5Cright%29+%3D+%5Csum+_+%7B+%5Cmathbf+%7B+x+%7D+_+%7B+i+%7D+%5Cin+%5Cmathcal+%7B+V+%7D+%5Cleft%28+%5Cmathcal+%7B+B+%7D+_+%7B+C+%7D+%5Cright%29+%7D+%5Clambda+_+%7B+r+e+g+%7D+%5Cleft%5C%7C+%5Cmathbf+%7B+x+%7D+_+%7B+i+%7D+%5E+%7B+i+n+i+t+%7D+-+%5Cmathbf+%7B+x+%7D+_+%7B+i+%7D+%5Cright%5C%7C+_+%7B+2+%7D\" alt=\"R \\left( \\mathcal { B } _ { C } \\right) = \\sum _ { \\mathbf { x } _ { i } \\in \\mathcal { V } \\left( \\mathcal { B } _ { C } \\right) } \\lambda _ { r e g } \\left\\| \\mathbf { x } _ { i } ^ { i n i t } - \\mathbf { x } _ { i } \\right\\| _ { 2 }\" eeimg=\"1\"/> </p><p>具体参数的含义可以参见<a href=\"https://link.zhihu.com/?target=https%3A//www.aclweb.org/anthology/D17-1270\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">原论文</a>。</p><blockquote>ACL 2018《Using pseudo-senses for improving the extraction of synonyms from word embeddings》</blockquote><p>这篇工作损失函数和上篇工作相同，但是直接提出这个损失函数的设计不仅能用于跨语言词表对齐，还能在不使用外部知识的前提下，<b>引入词向量中的同义关系</b>。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>但是，当在开放域中抽取同义词时，如果仅有少量的同义词对数据，还是基本不可能达到较好的效果的。</p><h2>4 同义语句的数据来源</h2><p>类似于关系抽取中的<b>距离监督</b>思路，我们可以对句子中希望提取同义词的词语（此处可以认为是<b>实体</b>）进行同义关系标注。标注方式可以<b>直接使用同义词词典</b>进行标注，也可以利用<b>实体链接</b>和<b>知识图谱</b>的同义词进行标注。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-309e8fb682cc235abc1a360bbddbbbd6_b.jpg\" data-rawwidth=\"714\" data-rawheight=\"360\" data-size=\"normal\" class=\"origin_image zh-lightbox-thumb\" width=\"714\" data-original=\"https://pic3.zhimg.com/v2-309e8fb682cc235abc1a360bbddbbbd6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;714&#39; height=&#39;360&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"714\" data-rawheight=\"360\" data-size=\"normal\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"714\" data-original=\"https://pic3.zhimg.com/v2-309e8fb682cc235abc1a360bbddbbbd6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-309e8fb682cc235abc1a360bbddbbbd6_b.jpg\"/><figcaption>同义关系抽取与距离监督</figcaption></figure><h2>5 同义语句数据的解决思路</h2><p>我们直接利用关系抽取的模型去处理这样的数据，在该数据中，关系只有两种：同义和 NA 。而关系抽取任务中常见的 Baseline 是 PCNN，PCNN 及其上面的拓展都可以直接用于该数据集。</p><blockquote>EMNLP 2015 《Neural Relation Extraction with Selective Attention over Instances》</blockquote><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-cc50c15f01ab0a384f1fda252ffd1995_b.jpg\" data-rawwidth=\"566\" data-rawheight=\"432\" data-size=\"normal\" class=\"origin_image zh-lightbox-thumb\" width=\"566\" data-original=\"https://pic2.zhimg.com/v2-cc50c15f01ab0a384f1fda252ffd1995_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;566&#39; height=&#39;432&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"566\" data-rawheight=\"432\" data-size=\"normal\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"566\" data-original=\"https://pic2.zhimg.com/v2-cc50c15f01ab0a384f1fda252ffd1995_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-cc50c15f01ab0a384f1fda252ffd1995_b.jpg\"/><figcaption>PCNN 模型图</figcaption></figure><p>此外，也可以利用常见的句法分析工具，抽取出每个句子的 Pattern，将 Pattern 分类后当做针对 Pattern 的二分类任务。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-51190e9cad119942ef7a5bf67359308b_b.jpg\" data-rawwidth=\"694\" data-rawheight=\"332\" data-size=\"normal\" class=\"origin_image zh-lightbox-thumb\" width=\"694\" data-original=\"https://pic4.zhimg.com/v2-51190e9cad119942ef7a5bf67359308b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;694&#39; height=&#39;332&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"694\" data-rawheight=\"332\" data-size=\"normal\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"694\" data-original=\"https://pic4.zhimg.com/v2-51190e9cad119942ef7a5bf67359308b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-51190e9cad119942ef7a5bf67359308b_b.jpg\"/><figcaption>语句的语法分析等</figcaption></figure><h2><br/>其他</h2><p>韩家炜老师组在 KDD 2017 上发表的《Automatic Synonym Discovery with Knowledge Bases Meng》是一篇对我启发很大的文章。我在检索解决思路之间首先读了这篇文章，因此才萌生了将抽取方式区分为词对数据和同义语句数据两种形式的想法。建议做同义词抽取工作的同学首先阅读。</p>", 
            "topic": [
                {
                    "tag": "自然语言处理", 
                    "tagLink": "https://api.zhihu.com/topics/19560026"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "同义词", 
                    "tagLink": "https://api.zhihu.com/topics/19763012"
                }
            ], 
            "comments": [
                {
                    "userName": "胡一刀", 
                    "userLink": "https://www.zhihu.com/people/719e4296a1c0d1376a174d1456552a4a", 
                    "content": "想请教一下大神挖掘中文同义词有什么具体的应用场景么？", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "田卿", 
                            "userLink": "https://www.zhihu.com/people/c5b07c6b53cc7f9e2edd185fdc4a0d45", 
                            "content": "<p>不是大神哈哈，我说一下自己粗浅的理解。</p><p><br></p><p>同义词挖掘本身是个基础的自然语言处理&amp;数据挖掘的任务，目的之一是构造一个更加完善或者更加符合本领域需求的同义词词典(可以简单理解为词对)。</p><p>这个同义词词典可以改善很多下游具体任务的质量。比如关键词抽取时我们可以把”NLP\"和“自然语言处理”当做同一个关键词处理，文本分类时可以把同义词视为同一个分类结果，信息检索时也可以做到更加完备的检索，当然也包括其他许多任务。</p><p>所以说同义词抽取作为基础任务确实具有很大的实用性，但是真正应用起来也具有一定难度，比如很多词只在特定语境下同义，还有我们也难以较为完备地抽取出大量同义词。</p>", 
                            "likes": 3, 
                            "replyToAuthor": "胡一刀"
                        }, 
                        {
                            "userName": "远行人", 
                            "userLink": "https://www.zhihu.com/people/6829a5859315ad0fe2ac232d0add29d3", 
                            "content": "<p>我补充一个具体的，我现在遇到的问题是，智能问答中匹配问答对，需要拓展更多的question，就需要用到同义词。</p><p><br></p><a class=\"comment_sticker\" href=\"https://pic4.zhimg.com/v2-b789d8e597d920061dcd4efb585cd343.gif\" data-width=\"\" data-height=\"\">[思考]</a>", 
                            "likes": 0, 
                            "replyToAuthor": "胡一刀"
                        }
                    ]
                }, 
                {
                    "userName": "胡一刀", 
                    "userLink": "https://www.zhihu.com/people/719e4296a1c0d1376a174d1456552a4a", 
                    "content": "感谢分享，比如KBQA的工程项目中是否也可以引入同义词知识库，来提高近似问题-答案对的准确性呢？", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "田卿", 
                            "userLink": "https://www.zhihu.com/people/c5b07c6b53cc7f9e2edd185fdc4a0d45", 
                            "content": "<p>搜到这篇文章里有系统提到KBQA的思路：<a href=\"http://link.zhihu.com/?target=http%3A//www.vldb.org/pvldb/vol10/p565-cui.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">vldb.org/pvldb/vol10/p5</span><span class=\"invisible\">65-cui.pdf</span><span class=\"ellipsis\"></span></a></p><p>其中一大类就是基于同义词的方法。</p>", 
                            "likes": 2, 
                            "replyToAuthor": "胡一刀"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/54057012", 
            "userName": "田卿", 
            "userLink": "https://www.zhihu.com/people/c5b07c6b53cc7f9e2edd185fdc4a0d45", 
            "upvote": 75, 
            "title": "综述：注意力机制在自然语言处理中的应用", 
            "content": "<h2>摘要</h2><p>近些年来，注意力（Attention）机制已经成为深度学习中的一个热点。在自然语言处理领域中，特别是在Seq2Seq模型中，注意力机制更是成为一种“标配”，出现在各种各样的任务里。本文将按照时间和分类顺序，对自然语言处理中出现过的注意力机制、常见变种以及评价指标等进行介绍。</p><h2>目录</h2><ol><li>简介</li><li>基本概念</li><li>变种</li><ol><li>Multi-dimensional Attention</li><li>Soft Attention 和 Hard Attention</li><li>Global Attention 和 Local Attention</li><li>Hierarchical Attention</li><li>Attention Over Attention</li><li>Memory-based Attention</li><li>Self-Attention\t</li></ol><li>评价指标</li><ol><li>定量指标</li><li>定性指标</li></ol><li>结论</li><li>引用</li></ol><h2>简介</h2><p>注意力机制的思想很早就存在，第一次应用在深度学习是在计算机视觉中。我们在观察一幅图时，我们的注意力会自然地集中于图像的某些地方，例如，当我们看到下图时，我们的注意力会集中在小智、皮卡丘、喷火龙等角色上，而非图片背景上。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-68f0069df2789cfa7cbdf3467aa032af_b.jpg\" data-size=\"normal\" data-rawwidth=\"1146\" data-rawheight=\"798\" class=\"origin_image zh-lightbox-thumb\" width=\"1146\" data-original=\"https://pic4.zhimg.com/v2-68f0069df2789cfa7cbdf3467aa032af_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1146&#39; height=&#39;798&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1146\" data-rawheight=\"798\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1146\" data-original=\"https://pic4.zhimg.com/v2-68f0069df2789cfa7cbdf3467aa032af_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-68f0069df2789cfa7cbdf3467aa032af_b.jpg\"/><figcaption>精灵宝可梦示意图</figcaption></figure><p>而在自然语言处理中，注意力机制的一大应用在Seq2Seq模型中。Seq2Seq模型由<b>编码器Encoder</b>和<b>解码器Decoder</b>组成，两者由Encoder的<b>上下文编码Context</b>连接。如图：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-30b3cff155c4b728d491d6e20171ebbf_b.jpg\" data-size=\"normal\" data-rawwidth=\"1212\" data-rawheight=\"422\" class=\"origin_image zh-lightbox-thumb\" width=\"1212\" data-original=\"https://pic4.zhimg.com/v2-30b3cff155c4b728d491d6e20171ebbf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1212&#39; height=&#39;422&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1212\" data-rawheight=\"422\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1212\" data-original=\"https://pic4.zhimg.com/v2-30b3cff155c4b728d491d6e20171ebbf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-30b3cff155c4b728d491d6e20171ebbf_b.jpg\"/><figcaption>Seq2Seq</figcaption></figure><p>事实上，在我们阅读句子的时候，也存在注意力机制。对于句子中某些重要的词，往往会抓住我们的注意力。然而，在基础的Seq2Seq模型中，在解码的过程中，并没有获得“注意力”，对于每个解码时刻，使用的编码器信息均相同。而引入了注意力机制的Seq2Seq模型，结构如下图：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-dd2fe71e4a9ecb5940a0c86d8b2c813a_b.jpg\" data-size=\"normal\" data-rawwidth=\"658\" data-rawheight=\"362\" class=\"origin_image zh-lightbox-thumb\" width=\"658\" data-original=\"https://pic3.zhimg.com/v2-dd2fe71e4a9ecb5940a0c86d8b2c813a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;658&#39; height=&#39;362&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"658\" data-rawheight=\"362\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"658\" data-original=\"https://pic3.zhimg.com/v2-dd2fe71e4a9ecb5940a0c86d8b2c813a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-dd2fe71e4a9ecb5940a0c86d8b2c813a_b.jpg\"/><figcaption>Seq2Seq with Different Context</figcaption></figure><p>在不同解码时刻，都存在一个不同的隐藏层状态。于是，我们使用<b>某时刻的解码器的隐藏层状态</b>和<b>编码器所有时刻的状态</b>进行匹配计算，得出各自的<b>权重值</b>，进而对编码器的状态进行加权求和作为额外的<b>注意力知识</b>。如下图所示：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-2977cc23ad8d97af47148c115aeffef6_b.jpg\" data-size=\"normal\" data-rawwidth=\"710\" data-rawheight=\"518\" class=\"origin_image zh-lightbox-thumb\" width=\"710\" data-original=\"https://pic3.zhimg.com/v2-2977cc23ad8d97af47148c115aeffef6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;710&#39; height=&#39;518&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"710\" data-rawheight=\"518\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"710\" data-original=\"https://pic3.zhimg.com/v2-2977cc23ad8d97af47148c115aeffef6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-2977cc23ad8d97af47148c115aeffef6_b.jpg\"/><figcaption>Attention</figcaption></figure><p>注意力机制在2014年首次正式使用在计算机视觉领域，而后在2014-2015年出现在神经机器翻译中，这是注意力机制首次出现在自然语言处理的时间点。而后，在2015-2016年，注意力机制及其在循环神经网络、卷积神经网络中的变种也陆陆续续出现在自然语言处理中的各个任务里。在2017年，注意力机制再一次被推上热点，Google提出的Transformer模型非常好地使用了Self-Attention模块，获得了学术界和工业界的重点关注。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-71f1838a941661a9d3ed6c372473baab_b.jpg\" data-size=\"normal\" data-rawwidth=\"1556\" data-rawheight=\"820\" class=\"origin_image zh-lightbox-thumb\" width=\"1556\" data-original=\"https://pic4.zhimg.com/v2-71f1838a941661a9d3ed6c372473baab_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1556&#39; height=&#39;820&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1556\" data-rawheight=\"820\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1556\" data-original=\"https://pic4.zhimg.com/v2-71f1838a941661a9d3ed6c372473baab_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-71f1838a941661a9d3ed6c372473baab_b.jpg\"/><figcaption>Attention发展过程</figcaption></figure><h2>基本概念</h2><p>在前面提到的Seq2Seq模型中，我们需要计算解码器状态和编码器状态的注意力分数，此处我们可以把计算分数的对象抽象为<b>请求Query</b>和<b>键Key</b>。<b>请求Query为解码器状态，键Key是编码器状态</b>。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-9510b041513ef7178c89ee889ec1ed02_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"452\" data-rawheight=\"120\" class=\"origin_image zh-lightbox-thumb\" width=\"452\" data-original=\"https://pic3.zhimg.com/v2-9510b041513ef7178c89ee889ec1ed02_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;452&#39; height=&#39;120&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"452\" data-rawheight=\"120\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"452\" data-original=\"https://pic3.zhimg.com/v2-9510b041513ef7178c89ee889ec1ed02_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-9510b041513ef7178c89ee889ec1ed02_b.jpg\"/></figure><p>其中函数a为<b>对齐函数</b>，用来计算请求Query和键Key的相似度。常见的函数有：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-6af7f48491fe45f646344bfdddf3d24a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"538\" data-rawheight=\"332\" class=\"origin_image zh-lightbox-thumb\" width=\"538\" data-original=\"https://pic3.zhimg.com/v2-6af7f48491fe45f646344bfdddf3d24a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;538&#39; height=&#39;332&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"538\" data-rawheight=\"332\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"538\" data-original=\"https://pic3.zhimg.com/v2-6af7f48491fe45f646344bfdddf3d24a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-6af7f48491fe45f646344bfdddf3d24a_b.jpg\"/></figure><p>进而，对每个请求Query产生的一组注意力分数进行标准化。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-904a029abb8e2f235582ff8cd82f9ae6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"312\" data-rawheight=\"160\" class=\"content_image\" width=\"312\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;312&#39; height=&#39;160&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"312\" data-rawheight=\"160\" class=\"content_image lazy\" width=\"312\" data-actualsrc=\"https://pic3.zhimg.com/v2-904a029abb8e2f235582ff8cd82f9ae6_b.jpg\"/></figure><p>最终，计算值Value和上述结果的加权求和。<b>此处值Value和键Key一样，均为编码器状态</b>。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-d8a1b500150bbfea12648403989c65fe_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"392\" data-rawheight=\"162\" class=\"content_image\" width=\"392\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;392&#39; height=&#39;162&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"392\" data-rawheight=\"162\" class=\"content_image lazy\" width=\"392\" data-actualsrc=\"https://pic3.zhimg.com/v2-d8a1b500150bbfea12648403989c65fe_b.jpg\"/></figure><p>在上述介绍中，容易让人产生疑问的是为什么会有Key和Value这样的概念，既然两者相同为什么要拆分开？事实上，在最基础的注意力机制中，确实只有两个元素，而非三个元素，之所以要拆分成Key和Value，是因为在某些需求中，计算相似度的对象和最终的注意力对象并非是相同的。例如，在阅读理解问题中，我们可以把三者这样对应：Query表示提出的问题，Key表示知识库中的问题，Value表示知识库中的答案。</p><p>综上所述，我们可以把注意力机制分为三个步骤：</p><ol><li>计算请求Query和键Key的注意力分数。</li><li>标准化每组注意力分数。</li><li>计算标准化后的注意力分数和值Value的加权求和。</li></ol><h2>变种</h2><h2>﻿Multi-dimensional Attention</h2><p>Multi-dimensional Attention是一种较为容易理解的Attention变种。原始的Attention中，每个Query对应一组Attention分数。然而，这样的表示在很多时候会认为是信息不足的，当Key本身包括多维度信息时，显然我们需要多组Attention分数共同表示其结果。于是，Multi-dimensional Attention使用<b>2维Attention</b>而不是1维Attention，如下表：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ae732c117ee3f3ebbd7ada5b80baf7cf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"630\" data-rawheight=\"128\" class=\"origin_image zh-lightbox-thumb\" width=\"630\" data-original=\"https://pic4.zhimg.com/v2-ae732c117ee3f3ebbd7ada5b80baf7cf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;630&#39; height=&#39;128&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"630\" data-rawheight=\"128\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"630\" data-original=\"https://pic4.zhimg.com/v2-ae732c117ee3f3ebbd7ada5b80baf7cf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-ae732c117ee3f3ebbd7ada5b80baf7cf_b.jpg\"/></figure><p>最终，对于每个Query我们可以计算出多个加权求和的结果，因此如果想用向量表示结果，就需要将多个Attention结果<b>拼接</b>起来。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-cd01a8b69185252385953a7a71675883_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"624\" data-rawheight=\"58\" class=\"origin_image zh-lightbox-thumb\" width=\"624\" data-original=\"https://pic4.zhimg.com/v2-cd01a8b69185252385953a7a71675883_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;624&#39; height=&#39;58&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"624\" data-rawheight=\"58\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"624\" data-original=\"https://pic4.zhimg.com/v2-cd01a8b69185252385953a7a71675883_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-cd01a8b69185252385953a7a71675883_b.jpg\"/></figure><p>此外，我们可能需要加入一些强约束来保证各组Attention能学到更加充分的信息，如果各组Attention的结果相同，那么将失去其意义。尽管随机初始化也可能隐式地让各组Attention能够到达自己的局部最优点，但显式的强约束将让该条件变得更加有力：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-fea76b837a9414df0b967a6a10db607c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"202\" data-rawheight=\"58\" class=\"content_image\" width=\"202\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;202&#39; height=&#39;58&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"202\" data-rawheight=\"58\" class=\"content_image lazy\" width=\"202\" data-actualsrc=\"https://pic1.zhimg.com/v2-fea76b837a9414df0b967a6a10db607c_b.jpg\"/></figure><p>该式子中A表示权重矩阵，I表示单位矩阵，F表示Frobenius范数，这是一种应用于矩阵上的范数，类似于向量上的L2范数。利用上述约束，我们可以让权重矩阵A倾向于是正交的，进而保证各个权重维度都能尽量垂直。</p><h2>Soft Attention 和 Hard Attention</h2><p>原始的Attention即为Soft Attention，每个权重取值在[0, 1]。</p><p>对于Hard Attention而言，每个Key的注意力只会取0或者1。也就是说，对于特定的Query，我们会强限制其只会对某些Key存在注意力，且权重相同，均为1。</p><p>如下图所示，Hard Attention在Soft Attention的第二步和第三步中间加入了<b>采样层</b>，这里使用的是蒙特卡洛采样，可以保证整个模型的端到端特性。在采样层中，以标准化后的权重为概率值，随机抽样出有限个权重值，抽样中的结果权重设置为1，其余设置为0。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-8750f8c1228aef4b176b992bb6067755_b.jpg\" data-size=\"normal\" data-rawwidth=\"1230\" data-rawheight=\"962\" class=\"origin_image zh-lightbox-thumb\" width=\"1230\" data-original=\"https://pic2.zhimg.com/v2-8750f8c1228aef4b176b992bb6067755_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1230&#39; height=&#39;962&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1230\" data-rawheight=\"962\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1230\" data-original=\"https://pic2.zhimg.com/v2-8750f8c1228aef4b176b992bb6067755_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-8750f8c1228aef4b176b992bb6067755_b.jpg\"/><figcaption>Hard Attention</figcaption></figure><h2>Global Attention 和 Local Attention</h2><p>原始的Attention即为Global Attention。根据原始的Attention机制，每个解码时刻，并不限制编码器状态的个数，而是能够动态适配编码器长度，进而和所有的编码器状态匹配。如下图：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-f3f4e856df56214807cbac2a022c2c75_b.jpg\" data-size=\"normal\" data-rawwidth=\"624\" data-rawheight=\"526\" class=\"origin_image zh-lightbox-thumb\" width=\"624\" data-original=\"https://pic2.zhimg.com/v2-f3f4e856df56214807cbac2a022c2c75_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;624&#39; height=&#39;526&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"624\" data-rawheight=\"526\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"624\" data-original=\"https://pic2.zhimg.com/v2-f3f4e856df56214807cbac2a022c2c75_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-f3f4e856df56214807cbac2a022c2c75_b.jpg\"/><figcaption>Global Attention</figcaption></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>在长文本中，我们对整个编码器长度进行对齐匹配，可能会导致注意力不集中的问题，因此我们可以通过限制注意力机制的范围，令注意力机制更加有效。</p><p>在Local Attention中，<b>每个解码器的ht对应一个编码器位置pt</b>，根据经验值选定区间大小D，进而只在<b>编码器的[pt-D, pt+D]位置</b>使用注意力机制。根据不同的pt选择方式，存在两种常见的Local Attention分类：Local-m和Local-p。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-3fcd4781b103e0eb05a2d1e4e83cb8e0_b.jpg\" data-size=\"normal\" data-rawwidth=\"610\" data-rawheight=\"508\" class=\"origin_image zh-lightbox-thumb\" width=\"610\" data-original=\"https://pic1.zhimg.com/v2-3fcd4781b103e0eb05a2d1e4e83cb8e0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;610&#39; height=&#39;508&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"610\" data-rawheight=\"508\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"610\" data-original=\"https://pic1.zhimg.com/v2-3fcd4781b103e0eb05a2d1e4e83cb8e0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-3fcd4781b103e0eb05a2d1e4e83cb8e0_b.jpg\"/><figcaption>Local Attention</figcaption></figure><p><b>Local-m</b>：简单设置pt为ht对应位置。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-3b8c80d0d9d29fac8fae9bda741f12be_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"200\" data-rawheight=\"116\" class=\"content_image\" width=\"200\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;200&#39; height=&#39;116&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"200\" data-rawheight=\"116\" class=\"content_image lazy\" width=\"200\" data-actualsrc=\"https://pic3.zhimg.com/v2-3b8c80d0d9d29fac8fae9bda741f12be_b.jpg\"/></figure><p><b>Local-p</b>：利用ht预测pt，进而使用高斯分布令Local Attention的权重以pt呈现出峰值形状。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-968a7589a653ba1047e5b3c1c248c8d9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"660\" data-rawheight=\"120\" class=\"origin_image zh-lightbox-thumb\" width=\"660\" data-original=\"https://pic2.zhimg.com/v2-968a7589a653ba1047e5b3c1c248c8d9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;660&#39; height=&#39;120&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"660\" data-rawheight=\"120\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"660\" data-original=\"https://pic2.zhimg.com/v2-968a7589a653ba1047e5b3c1c248c8d9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-968a7589a653ba1047e5b3c1c248c8d9_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-f226b8cfa594d19e6065565c72ed0501_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"518\" data-rawheight=\"86\" class=\"origin_image zh-lightbox-thumb\" width=\"518\" data-original=\"https://pic2.zhimg.com/v2-f226b8cfa594d19e6065565c72ed0501_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;518&#39; height=&#39;86&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"518\" data-rawheight=\"86\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"518\" data-original=\"https://pic2.zhimg.com/v2-f226b8cfa594d19e6065565c72ed0501_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-f226b8cfa594d19e6065565c72ed0501_b.jpg\"/></figure><p>在高斯分布中，我们设置：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-6ce4f8524ea2ce484d3e5c0bd8bd57a5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"224\" data-rawheight=\"152\" class=\"content_image\" width=\"224\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;224&#39; height=&#39;152&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"224\" data-rawheight=\"152\" class=\"content_image lazy\" width=\"224\" data-actualsrc=\"https://pic2.zhimg.com/v2-6ce4f8524ea2ce484d3e5c0bd8bd57a5_b.jpg\"/></figure><h2>﻿Hierarchical Attention</h2><p>Hierarchical Attention也可以用来解决长文本注意力不集中的问题，与Local Attention不同的是，Local Attention强行限制了注意力机制的范围，忽略剩余位置。而Hierarchical Attention则使用分层的思想，在所有的状态上都利用了注意力机制，如下图：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-df1124b94b1e4d0a5c15504b829dfb5c_b.jpg\" data-size=\"normal\" data-rawwidth=\"662\" data-rawheight=\"708\" class=\"origin_image zh-lightbox-thumb\" width=\"662\" data-original=\"https://pic1.zhimg.com/v2-df1124b94b1e4d0a5c15504b829dfb5c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;662&#39; height=&#39;708&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"662\" data-rawheight=\"708\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"662\" data-original=\"https://pic1.zhimg.com/v2-df1124b94b1e4d0a5c15504b829dfb5c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-df1124b94b1e4d0a5c15504b829dfb5c_b.jpg\"/><figcaption>Hierarchical Attention</figcaption></figure><p>在篇章级文本分类中，文本由多个句子组成，句子由多个词语组成。在这样的思路中，首先分别在各个句子中使用注意力机制，提取出每个句子的关键信息，进而对每个句子的关键信息使用注意力机制，提取出文本的关键信息，最终利用文本的关键信息进行篇章及文本分类，公式如下：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-42d284d358d7d7f6f9f8d40378c0f6a5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"504\" data-rawheight=\"268\" class=\"origin_image zh-lightbox-thumb\" width=\"504\" data-original=\"https://pic2.zhimg.com/v2-42d284d358d7d7f6f9f8d40378c0f6a5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;504&#39; height=&#39;268&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"504\" data-rawheight=\"268\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"504\" data-original=\"https://pic2.zhimg.com/v2-42d284d358d7d7f6f9f8d40378c0f6a5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-42d284d358d7d7f6f9f8d40378c0f6a5_b.jpg\"/></figure><h2>Attention Over Attention</h2><p>Attention Over Attention在阅读理解的完形填空任务中提出，基本思想是对Attention之后的结果再进行Attention，但具体步骤区别于Hierarchical Attention。</p><p>在Attention Over Attention中，第一次Attention的对象是<b>请求中的词向量</b>和阅读中<b>整个文本构成的词向量</b>。</p><p>在第一次Attention中，我们可以获取到一个权重矩阵，两个维度分别是请求长度和文本长度，横轴和纵轴分别代表一方对另一方的注意力分布。在文本长度上进行取<b>均值并归一化</b>，则能得出和请求长度相同的<b>注意力平均分布向量</b>，可以用来表示引入了注意力机制的请求Query。在请求长度上<b>归一化</b>，则可以表示文本中各个词关于请求的<b>注意力分布矩阵</b>。</p><p>在第二次Attention中，我们用第一次Attention的两个结果再次求Attention权重，可以得到一个请求关于阅读文本的注意力分布向量，进而可以用来进行完形填空，例如求出P(“Mary”|D, Q)。模型架构图如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-6847d900a8bbaf3c267f506ada6f6457_b.jpg\" data-size=\"normal\" data-rawwidth=\"1268\" data-rawheight=\"878\" class=\"origin_image zh-lightbox-thumb\" width=\"1268\" data-original=\"https://pic4.zhimg.com/v2-6847d900a8bbaf3c267f506ada6f6457_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1268&#39; height=&#39;878&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1268\" data-rawheight=\"878\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1268\" data-original=\"https://pic4.zhimg.com/v2-6847d900a8bbaf3c267f506ada6f6457_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-6847d900a8bbaf3c267f506ada6f6457_b.jpg\"/><figcaption>Attention Over Attention</figcaption></figure><h2>﻿Memory-based Attention</h2><p>Memory Network也可以视为是一种Attention的变种，这里列出End-to-End Memory Network来解释Memory-based Attention，如下图(a)：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-e5c7867c0f204c35094cfd29407e74cb_b.jpg\" data-size=\"normal\" data-rawwidth=\"2048\" data-rawheight=\"880\" class=\"origin_image zh-lightbox-thumb\" width=\"2048\" data-original=\"https://pic4.zhimg.com/v2-e5c7867c0f204c35094cfd29407e74cb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2048&#39; height=&#39;880&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"2048\" data-rawheight=\"880\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2048\" data-original=\"https://pic4.zhimg.com/v2-e5c7867c0f204c35094cfd29407e74cb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-e5c7867c0f204c35094cfd29407e74cb_b.jpg\"/><figcaption>End-to-End Memory Network</figcaption></figure><p>图中是阅读理解任务，阅读理解任务中，给定一个阅读，阅读由多个语句构成，然后给定一个问题，返回一个答案。图中对阅读中的多个句子进行两套Embedding，蓝色部分表示抽离出的<b>问题Embedding</b>，对应于<b>注意力机制中的Key</b>，黄色部分表示抽离出的<b>答案Embedding</b>，对应于<b>注意力机制中的Value</b>。对于整个过程中，对应于<b>注意力中Query的输入问题</b>，与问题Embedding进行对齐匹配，得到权重后，与答案Embedding进行加权求和。整个过程中都可以和注意力机制中的步骤对应，如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-e5ff03d223bf846c7442b2ac4a876cc1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"562\" data-rawheight=\"286\" class=\"origin_image zh-lightbox-thumb\" width=\"562\" data-original=\"https://pic2.zhimg.com/v2-e5ff03d223bf846c7442b2ac4a876cc1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;562&#39; height=&#39;286&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"562\" data-rawheight=\"286\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"562\" data-original=\"https://pic2.zhimg.com/v2-e5ff03d223bf846c7442b2ac4a876cc1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-e5ff03d223bf846c7442b2ac4a876cc1_b.jpg\"/></figure><p>Memory Network中，把上述的问题Embedding和答案Embedding当做记忆，表示对记忆的显式建模。Memory-based Attention实际上是输入问题和记忆之间的Attention。</p><h2>Self-Attention</h2><p>《Attention is all you need》中的Transformer模型几乎把Self-Attention应用到了极致。简单理解Self-Attention，即为Attention基础概念中的<b>Query，Key和Value均来自于相同的对象</b>，例如可以为同一个句子，句子中每个词语都和整个句子的中的词语进行对齐匹配，得到权重后再合整个句子的词向量进行加权求和，结果为融入<b>整个句子注意力信息的词向量</b>。下图为Transformer模型中使用的Self-Attention模块：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-6efa03eaa8ac7a16c591bacaa735a0a4_b.jpg\" data-size=\"normal\" data-rawwidth=\"454\" data-rawheight=\"514\" class=\"origin_image zh-lightbox-thumb\" width=\"454\" data-original=\"https://pic1.zhimg.com/v2-6efa03eaa8ac7a16c591bacaa735a0a4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;454&#39; height=&#39;514&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"454\" data-rawheight=\"514\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"454\" data-original=\"https://pic1.zhimg.com/v2-6efa03eaa8ac7a16c591bacaa735a0a4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-6efa03eaa8ac7a16c591bacaa735a0a4_b.jpg\"/><figcaption>Self-Attention</figcaption></figure><p>整个计算公式为：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-bff267bb822f0f6a25a5510ed147684a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"538\" data-rawheight=\"96\" class=\"origin_image zh-lightbox-thumb\" width=\"538\" data-original=\"https://pic3.zhimg.com/v2-bff267bb822f0f6a25a5510ed147684a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;538&#39; height=&#39;96&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"538\" data-rawheight=\"96\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"538\" data-original=\"https://pic3.zhimg.com/v2-bff267bb822f0f6a25a5510ed147684a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-bff267bb822f0f6a25a5510ed147684a_b.jpg\"/></figure><p>在Transformer模型中，区别于传统的Seq2Seq模型，不再使用RNN或者CNN充当编码器和解码器，而是<b>仅使用Attention机制来完成全部的编码和解码</b>。如下图：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-9f4bff929908a2dc826fcc9a07e21ca4_b.jpg\" data-size=\"normal\" data-rawwidth=\"698\" data-rawheight=\"982\" class=\"origin_image zh-lightbox-thumb\" width=\"698\" data-original=\"https://pic1.zhimg.com/v2-9f4bff929908a2dc826fcc9a07e21ca4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;698&#39; height=&#39;982&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"698\" data-rawheight=\"982\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"698\" data-original=\"https://pic1.zhimg.com/v2-9f4bff929908a2dc826fcc9a07e21ca4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-9f4bff929908a2dc826fcc9a07e21ca4_b.jpg\"/><figcaption>Transformer</figcaption></figure><p>Transformer模型让Self-Attention获得了更多的重视，并在多个任务上屠榜。从2017年开始，许多论文都成功将Transformer和Self-Attention应用到了各种任务里。2018年的BERT更是一个再次成功的典范。</p><h2>评价指标</h2><p>Attention的评价方式主要分为两类：<b>定量指标</b>和<b>定性指标</b>。</p><h2>定量指标</h2><p>一般而言，Attention机制是模型的一部分，而非独立为一个模型。因此，想要定量评价Attention的效果一般都需要<b>下游任务指标</b>的支撑。例如，我们会使用BLEU值去评价机器翻译任务，那么可以在机器翻译模型中加入Attention机制，根据最终机器翻译的BLEU值的升降来评价Attention机制的好坏。</p><p>除此之外，也存在直接评价Attention机制的方法，但存在一定的困难。例如，某些数据存在实现标注好的对齐值，可以当做Attention的目标值，那么可以计算出实际值和目标值的差异来计算出<b>对齐错误率（AER）</b>。但这样的方式非常受限，标注对齐值的成本过大。</p><h2>定性指标</h2><p>定性指标也是Attention机制常用的一种评价标准，我们可以通过可视化的方式来定性评价Attention机制的好坏。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-096fd953f8be2764bb700c5438f39509_b.jpg\" data-size=\"normal\" data-rawwidth=\"612\" data-rawheight=\"170\" class=\"origin_image zh-lightbox-thumb\" width=\"612\" data-original=\"https://pic2.zhimg.com/v2-096fd953f8be2764bb700c5438f39509_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;612&#39; height=&#39;170&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"612\" data-rawheight=\"170\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"612\" data-original=\"https://pic2.zhimg.com/v2-096fd953f8be2764bb700c5438f39509_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-096fd953f8be2764bb700c5438f39509_b.jpg\"/><figcaption>定性评价示意图-1</figcaption></figure><p>例如，在某个Seq2Seq任务中，我们可以用颜色表示Attention的权重值，深红色表示权重值接近于1，白色表示接近于0，那么我们可以通过颜色深浅的直观可视化表示来判断Attention机制的好坏。</p><p>此外，也存在其他类似的可视化表示：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-0f24d6bd3d269273f53bf95f2bbb8569_b.jpg\" data-size=\"normal\" data-rawwidth=\"1180\" data-rawheight=\"592\" class=\"origin_image zh-lightbox-thumb\" width=\"1180\" data-original=\"https://pic2.zhimg.com/v2-0f24d6bd3d269273f53bf95f2bbb8569_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1180&#39; height=&#39;592&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1180\" data-rawheight=\"592\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1180\" data-original=\"https://pic2.zhimg.com/v2-0f24d6bd3d269273f53bf95f2bbb8569_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-0f24d6bd3d269273f53bf95f2bbb8569_b.jpg\"/><figcaption>定性评价示意图-2</figcaption></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-7d5619a6a0922dfcd77f1dc409ffb936_b.jpg\" data-size=\"normal\" data-rawwidth=\"586\" data-rawheight=\"570\" class=\"origin_image zh-lightbox-thumb\" width=\"586\" data-original=\"https://pic3.zhimg.com/v2-7d5619a6a0922dfcd77f1dc409ffb936_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;586&#39; height=&#39;570&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"586\" data-rawheight=\"570\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"586\" data-original=\"https://pic3.zhimg.com/v2-7d5619a6a0922dfcd77f1dc409ffb936_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-7d5619a6a0922dfcd77f1dc409ffb936_b.jpg\"/><figcaption>定性评价示意图-3</figcaption></figure><h2>结论</h2><p>目前，Attention机制已经成为深度学习和自然语言处理中重要的一部分，出现在各种各样的模型和任务里。事实证明，Attention机制在许多场景下是非常有效的，并且由于其形式简洁，通常不会为模型带来更多的复杂度。因此，Attention机制应当是我们在做自然语言处理尤其是Seq2Seq任务中几乎必不可少的帮手。</p><h2>引用</h2><p>1. Hu D. An Introductory Survey on Attention Mechanisms in NLP Problems[J]. arXiv preprint arXiv:1811.05544, 2018.</p><p>2. Bahdanau D, Cho K, Bengio Y. Neural machine translation by jointly learning to align and translate[J]. arXiv preprint arXiv:1409.0473, 2014.</p><p>3. Luong M T, Pham H, Manning C D. Effective approaches to attention-based neural machine translation[J]. arXiv preprint arXiv:1508.04025, 2015.</p><p>4. Yang Z, Yang D, Dyer C, et al. Hierarchical attention networks for document classification[C]//Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2016: 1480-1489.</p><p>5. Sukhbaatar S, Weston J, Fergus R. End-to-end memory networks[C]//Advances in neural information processing systems. 2015: 2440-2448.</p><p>6. Cui Y, Chen Z, Wei S, et al. Attention-over-attention neural networks for reading comprehension[J]. arXiv preprint arXiv:1607.04423, 2016.</p><p>7. Vaswani A, Shazeer N, Parmar N, et al. Attention is all you need[C]//Advances in Neural Information Processing Systems. 2017: 5998-6008.</p><p>8. <a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/qq_40027052/article/details/78421155\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">深度学习中的注意力机制</a></p>", 
            "topic": [
                {
                    "tag": "自然语言处理", 
                    "tagLink": "https://api.zhihu.com/topics/19560026"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "注意力机制", 
                    "tagLink": "https://api.zhihu.com/topics/20682987"
                }
            ], 
            "comments": []
        }
    ], 
    "url": "https://zhuanlan.zhihu.com/c_1064520346647973888"
}
