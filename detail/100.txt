{
    "title": "机器拾趣", 
    "description": "机器学习，深度学习，AI，大数据\n个人博客：aifun.xyz", 
    "followers": [
        "https://www.zhihu.com/people/teawh", 
        "https://www.zhihu.com/people/wang-jun-1-6", 
        "https://www.zhihu.com/people/max-bay-14-15", 
        "https://www.zhihu.com/people/happy-hwang", 
        "https://www.zhihu.com/people/zi-bu-yu-38-84", 
        "https://www.zhihu.com/people/molinyunhai", 
        "https://www.zhihu.com/people/huang-wei-jian-74-35", 
        "https://www.zhihu.com/people/joy586210", 
        "https://www.zhihu.com/people/hao-qiong-zhen-huan", 
        "https://www.zhihu.com/people/zheng-le-jun", 
        "https://www.zhihu.com/people/ran-zhang-50", 
        "https://www.zhihu.com/people/nathen-42-36", 
        "https://www.zhihu.com/people/ooo-13-23", 
        "https://www.zhihu.com/people/ykp-41", 
        "https://www.zhihu.com/people/newbin-15", 
        "https://www.zhihu.com/people/stackbox", 
        "https://www.zhihu.com/people/nodeljd", 
        "https://www.zhihu.com/people/xiao-hei-wu-li-guan-tian-xiang", 
        "https://www.zhihu.com/people/zhang-jia-yi-6-82", 
        "https://www.zhihu.com/people/yang-dong-xiao-61", 
        "https://www.zhihu.com/people/zhuang-ng-han", 
        "https://www.zhihu.com/people/roachsinai", 
        "https://www.zhihu.com/people/xu-hua-qing-6", 
        "https://www.zhihu.com/people/cao-yuan-18", 
        "https://www.zhihu.com/people/yi-zhi-a-mu-mu-16", 
        "https://www.zhihu.com/people/wang-jian-89-85", 
        "https://www.zhihu.com/people/mr-lin-82-68", 
        "https://www.zhihu.com/people/xiao-xia-mi-65-43", 
        "https://www.zhihu.com/people/yttlj", 
        "https://www.zhihu.com/people/wang-xin-90-39", 
        "https://www.zhihu.com/people/johnny-63-54", 
        "https://www.zhihu.com/people/jia-xue-feng", 
        "https://www.zhihu.com/people/liu-fei-94-95"
    ], 
    "article": [
        {
            "url": "https://zhuanlan.zhihu.com/p/88745411", 
            "userName": "异尘", 
            "userLink": "https://www.zhihu.com/people/4e3a0f7cb9013a6fd523eecb5730987d", 
            "upvote": 3, 
            "title": "手把手教你快速入门知识图谱 - Neo4J教程", 
            "content": "<h2>前言</h2><p>今天，我们来聊一聊知识图谱中的Neo4J。首先，什么是知识图谱？先摘一段百度百科：</p><blockquote> 知识图谱（Knowledge Graph），在图书情报界称为知识域可视化或知识领域映射地图，是显示知识发展进程与结构关系的一系列各种不同的图形，用 可视化技术描述知识资源及其载体，挖掘、分析、 构建、绘制和显示知识及它们之间的相互联系。  知识图谱是通过将应用数学、 图形学、信息可视化技术、 信息科学等学科的理论与方法与计量学引文分析、共现分析等方法结合，并利用可视化的图谱形象地展示学科的核心结构、发展历史、 前沿领域以及整体知识架构达到多学科融合目的的现代理论。它能为学科研究提供切实的、有价值的参考。<br/> </blockquote><p>简单说来，知识图谱就是通过不同知识的关联性形成一个网状的知识结构，而这个知识结构，恰好就是人工智能AI的基石。当前AI领域热门的计算机图像、语音识别甚至是NLP，其实都是AI的<code>感知</code>能力，真正AI的<code>认知</code>能力，就要靠知识图谱。 </p><p>知识图谱目前的应用主要在搜索、智能问答、推荐系统等方面。知识图谱的建设，一般包括数据获取、实体识别和关系抽取、数据存储、图谱应用都几个方面。本文着眼于数据存储这块，给大家一个Neo4J的快速教程。</p><hr/><h2>Neo4J简介</h2><p>知识图谱由于其数据包含实体、属性、关系等，常见的关系型数据库诸如MySQL之类不能很好的体现数据的这些特点，因此知识图谱数据的存储一般是采用图数据库（Graph Databases）。而<a href=\"https://link.zhihu.com/?target=https%3A//neo4j.com/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Neo4j</a>是其中最为常见的图数据库。</p><h2>Neo4J安装</h2><p>首先在 <a href=\"https://link.zhihu.com/?target=https%3A//neo4j.com/download/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">neo4j.com/download/</span><span class=\"invisible\"></span></a> 下载Neo4J。Neo4J分为社区版和企业版，企业版在横向扩展、权限控制、运行性能、HA等方面都比社区版好，适合正式的生产环境，普通的学习和开发采用免费社区版就好。 </p><p>在Mac或者Linux中，安装好jdk后，直接解压下载好的Neo4J包，运行<code>bin/neo4j start</code>即可</p><h2>Neo4J使用</h2><p>Neo4J提供了一个用户友好的web界面，可以进行各项配置、写入、查询等操作，并且提供了可视化功能。类似ElasticSearch一样，我个人非常喜欢这种开箱即用的设计。 </p><p>打开浏览器，输入<code>http://127.0.0.1:7474/browser/</code>，如下图所示，界面最上方就是交互的输入框。 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-943bff526d4b0fdfe80d3fca706a7b64_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2156\" data-rawheight=\"1214\" class=\"origin_image zh-lightbox-thumb\" width=\"2156\" data-original=\"https://pic1.zhimg.com/v2-943bff526d4b0fdfe80d3fca706a7b64_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2156&#39; height=&#39;1214&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2156\" data-rawheight=\"1214\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2156\" data-original=\"https://pic1.zhimg.com/v2-943bff526d4b0fdfe80d3fca706a7b64_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-943bff526d4b0fdfe80d3fca706a7b64_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>Cypher查询语言</h2><p>Cypher是Neo4J的声明式图形查询语言，允许用户不必编写图形结构的遍历代码，就可以对图形数据进行高效的查询。Cypher的设计目的类似SQL，适合于开发者以及在数据库上做点对点模式（ad-hoc）查询的专业操作人员。其具备的能力包括：  - 创建、更新、删除节点和关系  - 通过模式匹配来查询和修改节点和关系  - 管理索引和约束等</p><hr/><h2>Neo4J实战教程</h2><p>直接讲解Cypher的语法会非常枯燥，本文通过一个实际的案例来一步一步教你使用Cypher来操作Neo4J。 </p><p>这个案例的节点主要包括人物和城市两类，人物和人物之间有朋友、夫妻等关系，人物和城市之间有出生地的关系。 </p><p>1. 首先，我们删除数据库中以往的图，确保一个空白的环境进行操作：</p><div class=\"highlight\"><pre><code class=\"language-text\">MATCH (n) DETACH DELETE n</code></pre></div><p>这里，<code>MATCH</code>是<b>匹配</b>操作，而小括号()代表一个<b>节点</b>node（可理解为括号类似一个圆形），括号里面的n为<b>标识符</b>。</p><p>2. 接着，我们创建一个人物节点：</p><div class=\"highlight\"><pre><code class=\"language-text\">CREATE (n:Person {name:&#39;John&#39;}) RETURN n</code></pre></div><p><code>CREATE</code>是<b>创建</b>操作，<code>Person</code>是<b>标签</b>，代表节点的类型。花括号{}代表节点的<b>属性</b>，属性类似Python的字典。这条语句的含义就是创建一个标签为Person的节点，该节点具有一个name属性，属性值是John。 </p><p>如图所示，在Neo4J的界面上可以看到创建成功的节点。  </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-4ebdfd370af2f4145fb9142760da66e1_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"103\" data-rawheight=\"105\" class=\"content_image\" width=\"103\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;103&#39; height=&#39;105&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"103\" data-rawheight=\"105\" class=\"content_image lazy\" width=\"103\" data-actualsrc=\"https://pic2.zhimg.com/v2-4ebdfd370af2f4145fb9142760da66e1_b.png\"/></figure><p>  3. 我们继续来创建更多的人物节点，并分别命名：</p><div class=\"highlight\"><pre><code class=\"language-text\">CREATE (n:Person {name:&#39;Sally&#39;}) RETURN n\nCREATE (n:Person {name:&#39;Steve&#39;}) RETURN n\nCREATE (n:Person {name:&#39;Mike&#39;}) RETURN n\nCREATE (n:Person {name:&#39;Liz&#39;}) RETURN n\nCREATE (n:Person {name:&#39;Shawn&#39;}) RETURN n</code></pre></div><p>如图所示，6个人物节点创建成功  </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-0dfa4f82a0f1b710f814d653bad85568_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"375\" data-rawheight=\"390\" class=\"content_image\" width=\"375\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;375&#39; height=&#39;390&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"375\" data-rawheight=\"390\" class=\"content_image lazy\" width=\"375\" data-actualsrc=\"https://pic1.zhimg.com/v2-0dfa4f82a0f1b710f814d653bad85568_b.jpg\"/></figure><p>  4. 接下来创建地区节点</p><div class=\"highlight\"><pre><code class=\"language-text\">CREATE (n:Location {city:&#39;Miami&#39;, state:&#39;FL&#39;})\nCREATE (n:Location {city:&#39;Boston&#39;, state:&#39;MA&#39;})\nCREATE (n:Location {city:&#39;Lynn&#39;, state:&#39;MA&#39;})\nCREATE (n:Location {city:&#39;Portland&#39;, state:&#39;ME&#39;})\nCREATE (n:Location {city:&#39;San Francisco&#39;, state:&#39;CA&#39;})</code></pre></div><p>可以看到，节点类型为Location，属性包括city和state。 </p><p>如图所示，共有6个人物节点、5个地区节点，Neo4J贴心地使用不用的颜色来表示不同类型的节点。  </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-39890877ca54748f24ab4c3211d11510_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"476\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic1.zhimg.com/v2-39890877ca54748f24ab4c3211d11510_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;640&#39; height=&#39;476&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"476\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https://pic1.zhimg.com/v2-39890877ca54748f24ab4c3211d11510_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-39890877ca54748f24ab4c3211d11510_b.jpg\"/></figure><p>5. 接下来创建关系</p><div class=\"highlight\"><pre><code class=\"language-text\">MATCH (a:Person {name:&#39;Liz&#39;}), \n      (b:Person {name:&#39;Mike&#39;}) \nMERGE (a)-[:FRIENDS]-&gt;(b)</code></pre></div><p>这里的方括号<code>[]</code>即为关系，<code>FRIENDS</code>为关系的类型。注意这里的箭头<code>--&gt;</code>是有方向的，表示是从a到b的关系。 如图，Liz和Mike之间建立了<code>FRIENDS</code>关系，通过Neo4J的可视化很明显的可以看出：  </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-dcc171700a1fd012878d52606188f89a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"228\" data-rawheight=\"132\" class=\"content_image\" width=\"228\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;228&#39; height=&#39;132&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"228\" data-rawheight=\"132\" class=\"content_image lazy\" width=\"228\" data-actualsrc=\"https://pic3.zhimg.com/v2-dcc171700a1fd012878d52606188f89a_b.jpg\"/></figure><p>6. 关系也可以增加属性</p><div class=\"highlight\"><pre><code class=\"language-text\">MATCH (a:Person {name:&#39;Shawn&#39;}), \n      (b:Person {name:&#39;Sally&#39;}) \nMERGE (a)-[:FRIENDS {since:2001}]-&gt;(b)</code></pre></div><p>在关系中，同样的使用花括号{}来增加关系的属性，也是类似Python的字典，这里给FRIENDS关系增加了since属性，属性值为2001，表示他们建立朋友关系的时间。 </p><p>7. 接下来增加更多的关系</p><div class=\"highlight\"><pre><code class=\"language-text\">MATCH (a:Person {name:&#39;Shawn&#39;}), (b:Person {name:&#39;John&#39;}) MERGE (a)-[:FRIENDS {since:2012}]-&gt;(b)\nMATCH (a:Person {name:&#39;Mike&#39;}), (b:Person {name:&#39;Shawn&#39;}) MERGE (a)-[:FRIENDS {since:2006}]-&gt;(b)\nMATCH (a:Person {name:&#39;Sally&#39;}), (b:Person {name:&#39;Steve&#39;}) MERGE (a)-[:FRIENDS {since:2006}]-&gt;(b)\nMATCH (a:Person {name:&#39;Liz&#39;}), (b:Person {name:&#39;John&#39;}) MERGE (a)-[:MARRIED {since:1998}]-&gt;(b)</code></pre></div><p>如图，人物关系图已建立好，有点图谱的意思了吧？  </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-955b7f17c2c3e4be847726228006a005_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"429\" data-rawheight=\"259\" class=\"origin_image zh-lightbox-thumb\" width=\"429\" data-original=\"https://pic2.zhimg.com/v2-955b7f17c2c3e4be847726228006a005_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;429&#39; height=&#39;259&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"429\" data-rawheight=\"259\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"429\" data-original=\"https://pic2.zhimg.com/v2-955b7f17c2c3e4be847726228006a005_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-955b7f17c2c3e4be847726228006a005_b.jpg\"/></figure><p>8. 然后，我们需要建立不同类型节点之间的关系-人物和地点的关系</p><div class=\"highlight\"><pre><code class=\"language-text\">MATCH (a:Person {name:&#39;John&#39;}), (b:Location {city:&#39;Boston&#39;}) MERGE (a)-[:BORN_IN {year:1978}]-&gt;(b)</code></pre></div><p>这里的关系是BORN_IN，表示出生地，同样有一个属性，表示出生年份。 </p><p>如图，在人物节点和地区节点之间，人物出生地关系已建立好。  </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b968613a93fdcddceb860e386a953699_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"216\" data-rawheight=\"114\" class=\"content_image\" width=\"216\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;216&#39; height=&#39;114&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"216\" data-rawheight=\"114\" class=\"content_image lazy\" width=\"216\" data-actualsrc=\"https://pic2.zhimg.com/v2-b968613a93fdcddceb860e386a953699_b.jpg\"/></figure><p>9. 同样建立更多人的出生地</p><div class=\"highlight\"><pre><code class=\"language-text\">MATCH (a:Person {name:&#39;Liz&#39;}), (b:Location {city:&#39;Boston&#39;}) MERGE (a)-[:BORN_IN {year:1981}]-&gt;(b)\nMATCH (a:Person {name:&#39;Mike&#39;}), (b:Location {city:&#39;San Francisco&#39;}) MERGE (a)-[:BORN_IN {year:1960}]-&gt;(b)\nMATCH (a:Person {name:&#39;Shawn&#39;}), (b:Location {city:&#39;Miami&#39;}) MERGE (a)-[:BORN_IN {year:1960}]-&gt;(b)\nMATCH (a:Person {name:&#39;Steve&#39;}), (b:Location {city:&#39;Lynn&#39;}) MERGE (a)-[:BORN_IN {year:1970}]-&gt;(b)</code></pre></div><p>建好以后，整个图如下  </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ecbdeac21cc6dff47fb1b824757b7d17_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"483\" data-rawheight=\"328\" class=\"origin_image zh-lightbox-thumb\" width=\"483\" data-original=\"https://pic4.zhimg.com/v2-ecbdeac21cc6dff47fb1b824757b7d17_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;483&#39; height=&#39;328&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"483\" data-rawheight=\"328\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"483\" data-original=\"https://pic4.zhimg.com/v2-ecbdeac21cc6dff47fb1b824757b7d17_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-ecbdeac21cc6dff47fb1b824757b7d17_b.jpg\"/></figure><p>10. 至此，知识图谱的数据已经插入完毕，可以开始做查询了。我们查询下所有在Boston出生的人物</p><div class=\"highlight\"><pre><code class=\"language-text\">MATCH (a:Person)-[:BORN_IN]-&gt;(b:Location {city:&#39;Boston&#39;}) RETURN a,b</code></pre></div><p>结果如图  </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-fa8ab8074aaa811f7c1b95f7bf96f1dd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"203\" data-rawheight=\"189\" class=\"content_image\" width=\"203\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;203&#39; height=&#39;189&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"203\" data-rawheight=\"189\" class=\"content_image lazy\" width=\"203\" data-actualsrc=\"https://pic2.zhimg.com/v2-fa8ab8074aaa811f7c1b95f7bf96f1dd_b.jpg\"/></figure><p>11. 查询所有对外有关系的节点</p><div class=\"highlight\"><pre><code class=\"language-text\">MATCH (a)--&gt;() RETURN a</code></pre></div><p>注意这里箭头的方向，返回结果不含任何地区节点，因为地区并没有指向其他节点（只是被指向）  </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-d82d57aa81aecc42612d8c464368b75b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"421\" data-rawheight=\"217\" class=\"origin_image zh-lightbox-thumb\" width=\"421\" data-original=\"https://pic4.zhimg.com/v2-d82d57aa81aecc42612d8c464368b75b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;421&#39; height=&#39;217&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"421\" data-rawheight=\"217\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"421\" data-original=\"https://pic4.zhimg.com/v2-d82d57aa81aecc42612d8c464368b75b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-d82d57aa81aecc42612d8c464368b75b_b.jpg\"/></figure><p>12. 查询所有有关系的节点</p><div class=\"highlight\"><pre><code class=\"language-text\">MATCH (a)--() RETURN a</code></pre></div><p>结果如图  </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-e34f0e9adb7cefd59e26c16eec88e422_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"537\" data-rawheight=\"344\" class=\"origin_image zh-lightbox-thumb\" width=\"537\" data-original=\"https://pic3.zhimg.com/v2-e34f0e9adb7cefd59e26c16eec88e422_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;537&#39; height=&#39;344&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"537\" data-rawheight=\"344\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"537\" data-original=\"https://pic3.zhimg.com/v2-e34f0e9adb7cefd59e26c16eec88e422_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-e34f0e9adb7cefd59e26c16eec88e422_b.jpg\"/></figure><p>13. 查询所有对外有关系的节点，以及关系类型</p><div class=\"highlight\"><pre><code class=\"language-text\">MATCH (a)-[r]-&gt;() RETURN a.name, type(r)</code></pre></div><p>结果如图  </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ac2a3c82ddfef52ee487b7efb59c9548_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"346\" data-rawheight=\"339\" class=\"content_image\" width=\"346\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;346&#39; height=&#39;339&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"346\" data-rawheight=\"339\" class=\"content_image lazy\" width=\"346\" data-actualsrc=\"https://pic1.zhimg.com/v2-ac2a3c82ddfef52ee487b7efb59c9548_b.jpg\"/></figure><p>14. 查询所有有结婚关系的节点</p><div class=\"highlight\"><pre><code class=\"language-text\">MATCH (n)-[:MARRIED]-() RETURN n</code></pre></div><p>结果如图  </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-e7850b5800fe28e628eda23b25ded3c6_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"110\" data-rawheight=\"185\" class=\"content_image\" width=\"110\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;110&#39; height=&#39;185&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"110\" data-rawheight=\"185\" class=\"content_image lazy\" width=\"110\" data-actualsrc=\"https://pic3.zhimg.com/v2-e7850b5800fe28e628eda23b25ded3c6_b.png\"/></figure><p>15. 创建节点的时候就建好关系</p><div class=\"highlight\"><pre><code class=\"language-text\">CREATE (a:Person {name:&#39;Todd&#39;})-[r:FRIENDS]-&gt;(b:Person {name:&#39;Carlos&#39;})</code></pre></div><p>结果如图  </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-e0222d0cc457d186ba5a1355f1b3990f_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"168\" data-rawheight=\"165\" class=\"content_image\" width=\"168\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;168&#39; height=&#39;165&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"168\" data-rawheight=\"165\" class=\"content_image lazy\" width=\"168\" data-actualsrc=\"https://pic4.zhimg.com/v2-e0222d0cc457d186ba5a1355f1b3990f_b.png\"/></figure><p>16. 查找某人的朋友的朋友</p><div class=\"highlight\"><pre><code class=\"language-text\">MATCH (a:Person {name:&#39;Mike&#39;})-[r1:FRIENDS]-()-[r2:FRIENDS]-(friend_of_a_friend) RETURN friend_of_a_friend.name AS fofName</code></pre></div><p>返回Mike的朋友的朋友：  </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-8630e2f27bb7bc7551a9699a4a4f6d73_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"95\" data-rawheight=\"94\" class=\"content_image\" width=\"95\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;95&#39; height=&#39;94&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"95\" data-rawheight=\"94\" class=\"content_image lazy\" width=\"95\" data-actualsrc=\"https://pic4.zhimg.com/v2-8630e2f27bb7bc7551a9699a4a4f6d73_b.png\"/></figure><p>  从图上也可以看出，Mike的朋友是Shawn，Shawn的朋友是John和Sally  </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-76ac9c4df2a24228f7d74be0b6bd2ace_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"375\" data-rawheight=\"260\" class=\"content_image\" width=\"375\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;375&#39; height=&#39;260&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"375\" data-rawheight=\"260\" class=\"content_image lazy\" width=\"375\" data-actualsrc=\"https://pic3.zhimg.com/v2-76ac9c4df2a24228f7d74be0b6bd2ace_b.jpg\"/></figure><p>17. 增加/修改节点的属性</p><div class=\"highlight\"><pre><code class=\"language-text\">MATCH (a:Person {name:&#39;Liz&#39;}) SET a.age=34\nMATCH (a:Person {name:&#39;Shawn&#39;}) SET a.age=32\nMATCH (a:Person {name:&#39;John&#39;}) SET a.age=44\nMATCH (a:Person {name:&#39;Mike&#39;}) SET a.age=25</code></pre></div><p>这里，SET表示<code>修改</code>操作 </p><p>18. 删除节点的属性</p><div class=\"highlight\"><pre><code class=\"language-text\">MATCH (a:Person {name:&#39;Mike&#39;}) SET a.test=&#39;test&#39;\nMATCH (a:Person {name:&#39;Mike&#39;}) REMOVE a.test</code></pre></div><p>删除属性操作主要通过<code>REMOVE</code> </p><p>19. 删除节点</p><div class=\"highlight\"><pre><code class=\"language-text\">MATCH (a:Location {city:&#39;Portland&#39;}) DELETE a</code></pre></div><p>删除节点操作是<code>DELETE</code></p><p>20. 删除有关系的节点</p><div class=\"highlight\"><pre><code class=\"language-text\">MATCH (a:Person {name:&#39;Todd&#39;})-[rel]-(b:Person) DELETE a,b,rel</code></pre></div><hr/><h2>总结</h2><p>本文重点针对常见的知识图谱图数据库Neo4J进行了介绍，并且采用一个实际的案例来说明Neo4J的查询语言Cypher的使用方法。 </p><p>当然，类似MySQL一样，在实际的生产应用中，除了简单的查询操作会在Neo4J的web页面进行外，一般还是使用Python、Java等的driver来在程序中实现。后续会继续介绍编程语言如何操作Neo4J。</p>", 
            "topic": [
                {
                    "tag": "知识图谱", 
                    "tagLink": "https://api.zhihu.com/topics/19838204"
                }, 
                {
                    "tag": "Neo4j", 
                    "tagLink": "https://api.zhihu.com/topics/19672349"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/85910281", 
            "userName": "异尘", 
            "userLink": "https://www.zhihu.com/people/4e3a0f7cb9013a6fd523eecb5730987d", 
            "upvote": 0, 
            "title": "Keras中return_sequences和return_state有什么用？", 
            "content": "<p> Photo by Jon Tyson on Unsplash</p><h2>前言</h2><p>CNN和RNN，作为深度学习的两大护法，促进了深度学习近几年在Computer Vision、NLP等领域席卷全世界。相比CNN，RNN其实更为“骨骼精奇”，它开创性的递归网络结构，让模型具有了“记忆”，使得我们向着“AI”更近了一步。虽然最近各种Transformer结构有了超越RNN之势，但是我依然觉得RNN是非常值得学习和发展的。</p><p>今天，我们以LSTM为例，来谈一个RNN中的一个具体的问题。我们知道，在Keras的LSTM实现中，有两个参数<code>return_sequences</code>和<code>return_state</code>。这两个参数的实际意义是什么？在什么场景下会用到呢？</p><p>PS：<code>Keras</code>是我最喜爱的深度学习框架了，其API的设计非常精妙和优雅，François Chollet是不愧是大师中的大师。相比传统的<code>Tensorflow</code>和<code>PyTorch</code>，Keras的API才是真正的“Deep Learning for Human”。另外，看到<code>Tensorflow 2.0</code>也开始以<code>tf.keras</code>作为第一公民，我非常欣慰。关于我对这几个框架的理解，后面再以专题文章和大家分享。</p><hr/><h2>LSTM介绍</h2><blockquote> Long Short Term Memory networks – usually just called “LSTMs” – are a special kind of RNN, capable of learning long-term dependencies. They were introduced by <a href=\"https://link.zhihu.com/?target=http%3A//www.bioinf.jku.at/publications/older/2604.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Hochreiter &amp; Schmidhuber (1997)</a> , and were refined and popularized by many people in following work.They work tremendously well on a large variety of problems, and are now widely used.<br/> </blockquote><p>LSTM是为了解决普通RNN网络在实际实践中出现的“梯度消失”等问题而出现的。这里我们略过里面的细节，重点看看单个LSTM cell的输入输出情况。  </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-a659e57ffd4dbfc88f34da4beb1dde37_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"8297\" data-rawheight=\"5634\" class=\"origin_image zh-lightbox-thumb\" width=\"8297\" data-original=\"https://pic4.zhimg.com/v2-a659e57ffd4dbfc88f34da4beb1dde37_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;8297&#39; height=&#39;5634&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"8297\" data-rawheight=\"5634\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"8297\" data-original=\"https://pic4.zhimg.com/v2-a659e57ffd4dbfc88f34da4beb1dde37_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-a659e57ffd4dbfc88f34da4beb1dde37_b.jpg\"/></figure><p>  从上图可以看出，单个LSTM cell其实有2个输出的，一个是h(t)，一个是c(t)。这里的h(t)称为hidden state，c(t)称为cell state。这个命名其实我认为是不太好的。熟悉全连接神经网络的同学，一定会把h(t)跟hidden layer相混淆。其实，这个h(t)才是LSTM的真正output，c(t)才是LSTM的内部”隐藏”状态。 </p><p>我们进一步把LSTM网络展开来看。每一个时间节点timestep，输入一个x(t)，cell里面的c(t)做一次更新，输出h(t)。紧接着下一个timestep，x(t+1)、h(t)和c(t)继续输入到cell，输出为h(t+1)和c(t+1)，如下图。  </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-30154905ca45df688e0f44b36520a5ac_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"590\" data-rawheight=\"177\" class=\"origin_image zh-lightbox-thumb\" width=\"590\" data-original=\"https://pic1.zhimg.com/v2-30154905ca45df688e0f44b36520a5ac_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;590&#39; height=&#39;177&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"590\" data-rawheight=\"177\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"590\" data-original=\"https://pic1.zhimg.com/v2-30154905ca45df688e0f44b36520a5ac_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-30154905ca45df688e0f44b36520a5ac_b.jpg\"/></figure><p>  因此，Keras中的<code>return_sequences</code>和<code>return_state</code>，就是跟h(t)和c(t)相关。</p><hr/><h2>Return Sequences</h2><p>接下来我们来点hands-on的代码，来具体看看这两个参数的作用。 </p><h3>实验一</h3><p>试验代码中，<code>return_sequences</code>和<code>return_state</code>默认都是false，输入shape为(1,3,1)，表示1个batch，3个timestep，1个feature</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">from</span> <span class=\"nn\">keras.models</span> <span class=\"kn\">import</span> <span class=\"n\">Model</span>\n<span class=\"kn\">from</span> <span class=\"nn\">keras.layers</span> <span class=\"kn\">import</span> <span class=\"n\">Input</span>\n<span class=\"kn\">from</span> <span class=\"nn\">keras.layers</span> <span class=\"kn\">import</span> <span class=\"n\">LSTM</span>\n<span class=\"kn\">from</span> <span class=\"nn\">numpy</span> <span class=\"kn\">import</span> <span class=\"n\">array</span>\n<span class=\"c1\"># define model</span>\n<span class=\"n\">inputs1</span> <span class=\"o\">=</span> <span class=\"n\">Input</span><span class=\"p\">(</span><span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n<span class=\"n\">lstm1</span> <span class=\"o\">=</span> <span class=\"n\">LSTM</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)(</span><span class=\"n\">inputs1</span><span class=\"p\">)</span>\n<span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">Model</span><span class=\"p\">(</span><span class=\"n\">inputs</span><span class=\"o\">=</span><span class=\"n\">inputs1</span><span class=\"p\">,</span> <span class=\"n\">outputs</span><span class=\"o\">=</span><span class=\"n\">lstm1</span><span class=\"p\">)</span>\n<span class=\"c1\"># define input data</span>\n<span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"n\">array</span><span class=\"p\">([</span><span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"mf\">0.2</span><span class=\"p\">,</span> <span class=\"mf\">0.3</span><span class=\"p\">])</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">((</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">3</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">))</span>\n<span class=\"c1\"># make and show prediction</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">))</span></code></pre></div><p>输出结果为</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"p\">[[</span><span class=\"o\">-</span><span class=\"mf\">0.0953151</span><span class=\"p\">]]</span></code></pre></div><p>表示在经历了3个time step的输入后，LSTM返回的hidden state，也就是上文中的h(t)。由于输出的是网络最后一个timestep的值，因此结果是一个标量。 </p><h3>实验二</h3><p>我们加上参数<code>return_sequences=True</code></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">lstm1</span> <span class=\"o\">=</span> <span class=\"n\">LSTM</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">return_sequences</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)(</span><span class=\"n\">inputs1</span><span class=\"p\">)</span></code></pre></div><p>输出结果为</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"p\">[[[</span><span class=\"o\">-</span><span class=\"mf\">0.02243521</span><span class=\"p\">]</span>\n<span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mf\">0.06210149</span><span class=\"p\">]</span>\n<span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mf\">0.11457888</span><span class=\"p\">]]]</span></code></pre></div><p>我们看到，输出了一个array，长度等于timestep，表示网络输出了每个timestep的h(t)。</p><p>总结一下，<code>return_sequences</code>即表示，LSTM的输出h(t)，是输出最后一个timestep的h(t)，还是把所有timestep的h(t)都输出出来。在实际应用中，关系到网络的应用场景是many-to-one还是many-to-many，非常重要。</p><hr/><h2>Return State</h2><h3>实验三</h3><p>接下来我们继续实验<code>return_state</code></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">lstm1</span><span class=\"p\">,</span> <span class=\"n\">state_h</span><span class=\"p\">,</span> <span class=\"n\">state_c</span> <span class=\"o\">=</span> <span class=\"n\">LSTM</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">return_state</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)(</span><span class=\"n\">inputs1</span><span class=\"p\">)</span></code></pre></div><p>输出结果为</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"p\">[</span><span class=\"n\">array</span><span class=\"p\">([[</span> <span class=\"mf\">0.10951342</span><span class=\"p\">]],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">float32</span><span class=\"p\">),</span>\n <span class=\"n\">array</span><span class=\"p\">([[</span> <span class=\"mf\">0.10951342</span><span class=\"p\">]],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">float32</span><span class=\"p\">),</span>\n <span class=\"n\">array</span><span class=\"p\">([[</span> <span class=\"mf\">0.24143776</span><span class=\"p\">]],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">float32</span><span class=\"p\">)]</span></code></pre></div><p>注意，输出是一个列表list，分别表示 - 最后一个time step的hidden state - 最后一个time step的hidden state（跟上面一样) - 最后一个time step的cell state（注意就是上文中的c(t)） </p><p>可以看出，<code>return_state</code>就是控制LSTM中的c(t)输出与否。 </p><h3>实验四</h3><p>我们最后看看<code>return_sequences</code>和<code>return_state</code>全开的情况。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">lstm1</span><span class=\"p\">,</span> <span class=\"n\">state_h</span><span class=\"p\">,</span> <span class=\"n\">state_c</span> <span class=\"o\">=</span> <span class=\"n\">LSTM</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">return_sequences</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span> <span class=\"n\">return_state</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span></code></pre></div><p>输出结果为</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"p\">[</span><span class=\"n\">array</span><span class=\"p\">([[[</span><span class=\"o\">-</span><span class=\"mf\">0.02145359</span><span class=\"p\">],</span>\n         <span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mf\">0.0540871</span> <span class=\"p\">],</span>\n         <span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mf\">0.09228823</span><span class=\"p\">]]],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">float32</span><span class=\"p\">),</span>\n <span class=\"n\">array</span><span class=\"p\">([[</span><span class=\"o\">-</span><span class=\"mf\">0.09228823</span><span class=\"p\">]],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">float32</span><span class=\"p\">),</span>\n <span class=\"n\">array</span><span class=\"p\">([[</span><span class=\"o\">-</span><span class=\"mf\">0.19803026</span><span class=\"p\">]],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">float32</span><span class=\"p\">)]</span></code></pre></div><p>输出列表的意义其实跟上面实验三一致，只是第一个hidden state h(t)变成了所有timestep的，因此也是长度等于timestep的array。</p><hr/><h2>Time Distributed</h2><p>最后再讲一讲<code>Keras</code>中的<code>TimeDistributed</code>。这个也是在RNN中非常常用但比较难理解的概念，原作者解释说</p><blockquote> TimeDistributedDense applies a same Dense (fully-connected) operation to every timestep of a 3D tensor. <br/> </blockquote><p>其实它的主要用途在于Many-to-Many： 比如输入shape为(1, 5, 1)，输出shape为(1, 5, 1)</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">Sequential</span><span class=\"p\">()</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">add</span><span class=\"p\">(</span><span class=\"n\">LSTM</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"n\">input_shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">length</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">return_sequences</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">))</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">add</span><span class=\"p\">(</span><span class=\"n\">TimeDistributed</span><span class=\"p\">(</span><span class=\"n\">Dense</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)))</span></code></pre></div><p>根据上面解读，<code>return_sequences=True</code>，使得LSTM的输出为每个timestep的hidden state，shape为(1, 5, 3) </p><p>现在需要将这个(1 ,5, 3)的3D tensor变换为(1, 5, 1)的结果，需要3个Dense layer，分别作用于每个time step的输出。而使用了<code>TimeDistributed</code>后，则把一个相同的Dense layer去分别作用，可以使得网络更为紧凑，参数更少的作用。</p><p>如果是在many-to-one的情况，<code>return_sequence=False</code>，则LSTM的输出为最后一个time step的hidden state，shape为(1, 3)。此时加上一个Dense layer, 不用使用<code>TimeDistributed</code>，就可以将(1, 3)变换为(1, 1)。</p><hr/><h2>总结</h2><p>本文主要通过一些实际的代码案例，解释了<code>Keras</code>的LSTM API中常见的两个参数<code>return_sequence</code>和<code>return_state</code>的原理及作用，在<code>Tensorflow</code>及<code>PyTorch</code>，也有相通的，希望能够帮助大家加深对RNN的理解。</p>", 
            "topic": [
                {
                    "tag": "Keras", 
                    "tagLink": "https://api.zhihu.com/topics/20052139"
                }, 
                {
                    "tag": "神经网络", 
                    "tagLink": "https://api.zhihu.com/topics/19607065"
                }, 
                {
                    "tag": "LSTM", 
                    "tagLink": "https://api.zhihu.com/topics/20023220"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/85701151", 
            "userName": "异尘", 
            "userLink": "https://www.zhihu.com/people/4e3a0f7cb9013a6fd523eecb5730987d", 
            "upvote": 45, 
            "title": "使用numpy来理解PCA和SVD", 
            "content": "<h2>前言</h2><p>线性代数是高等数学里面非常重要的一门课程，可惜在学校的时候是一种自底向上的学习方式，并不知道学出来有什么用，以致彻底沦为应试教育。后来在工作中接触了机器学习，才真正看到了“数学之美”，才发现线性代数是多么的优雅多么的有用。</p><p>今天我们来看看下线性代数中非常重要的一个知识点<b>奇异值分解</b>SVD <a href=\"https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Singular_value_decomposition\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Singular value decomposition</a>。SVD在数据科学当中非常有用，其常见的应用包括：  - 自然语言处理中的Latent Semantic Analysis  - 推荐系统中的Collaborative Filtering  - 降维常用套路Principal Component Analysis</p><p>LSA已经在前文中有所讲解，CF的话后面在推荐系统的专题中来写，今天主要聊聊PCA，以及SVD在PCA中的重要作用。同样延续我们“手撕”的传统，使用numpy来理解其中的原理。</p><hr/><h2>PCA</h2><p><a href=\"https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Principal_component_analysis\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Principal component analysis</a>即<b>主成分分析</b>，是机器学习中一种非常常用的降维方式。其发源也是源自于早期的计算机处理能力有限，当数据样本的维度很高时，预先去除掉数据中的一些冗余信息和噪声（降维），使得数据变得更加简单高效，节省时间和成本。 </p><p>在深度学习时代，更强调的是原始数据的直接输入，再通过神经网络来做降维工作，最典型是场景就是计算机视觉，直接输入原始图片像素信息，通过CNN<b>卷积层</b>、<b>MaxPooling层</b>来进行降维。因此PCA逐渐开始淡出人们的视线，通常是作为一种数据可视化的手段（二维图表无法展示多维的数据样本）。 </p><p>其实，在深度学习目前尚未全面攻克的结构化数据领域，PCA仍然有较多的用，其数据处理的思路依然值得我们去学习揣摩。 </p><h3>PCA正常解法</h3><p>PCA算法的本质，其实就是找到一些投影方向，使得数据在这些投影方向上的方差最大，且这些投影方向是相互正交的。找到新的<b>正交基</b>后，计算原始数据在这些正交基上<b>投影</b>的方差，方差越大，就说明对应正交基上包含了更多的信息量。 </p><p>关于原始数据的方差，最好的一个工具就是<b>协方差矩阵</b>了。协方差矩阵的<b>特征值</b>越大，对应的方差也就越大，在对应的<b>特征向量</b>上投影的信息量就越大。因此，我们如果将小特征值对应方向的数据删除，就可以达到降维的目的。因此，在数学上，我们可以把问题转化为求原始数据的协方差矩阵，然后计算协方差矩阵的特征值与特征向量。 </p><p>对于广大程序员来说，学习机器学习最重要的一个坎还是数学。很多实际的代码其实是公式推导后的结果的代码实现，如果没有理清公式推导的过程，那么最后肯定是一头雾水。所以，克服心中的恐惧，翻出压在箱底的《线性代数》，我们上。 </p><p>首先，求原始数据X的协方差矩阵C，将原始矩阵中心化后，做如下操作</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-5a16bceaf936a2c1923484020f041b82_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"216\" data-rawheight=\"124\" class=\"content_image\" width=\"216\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;216&#39; height=&#39;124&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"216\" data-rawheight=\"124\" class=\"content_image lazy\" width=\"216\" data-actualsrc=\"https://pic3.zhimg.com/v2-5a16bceaf936a2c1923484020f041b82_b.jpg\"/></figure><p>接着，由于协方差矩阵C是方阵，就可以通过特征分解的方式来求C的特征值和特征向量。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-a9fcf53f04161fed65932d743f105aa1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"290\" data-rawheight=\"66\" class=\"content_image\" width=\"290\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;290&#39; height=&#39;66&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"290\" data-rawheight=\"66\" class=\"content_image lazy\" width=\"290\" data-actualsrc=\"https://pic2.zhimg.com/v2-a9fcf53f04161fed65932d743f105aa1_b.jpg\"/></figure><p>最后，选择最大的k个特征值进行保留，求X的k阶PCA（<b>X右乘k阶特征向量</b>）</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-c98fd46ebcee1e4a5daad76b879d0054_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"244\" data-rawheight=\"74\" class=\"content_image\" width=\"244\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;244&#39; height=&#39;74&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"244\" data-rawheight=\"74\" class=\"content_image lazy\" width=\"244\" data-actualsrc=\"https://pic1.zhimg.com/v2-c98fd46ebcee1e4a5daad76b879d0054_b.jpg\"/></figure><h3>用SVD来解PCA</h3><p>根据上面的推导，我们已经可以对矩阵X做PCA了。同学们可能要问了，这跟SVD有什么关系呢？ </p><p>工程化思维强的同学应该已经想到了，这种纯数学的解法，在实际工程实践中有以下问题：  - 在数据量很大时，把原始矩阵进行转置求协方差矩阵，然后再进行特征值分解是一个非常慢的过程。  - 稳定性问题。可以看到X转置乘以X，如果矩阵有非常小的数，很容易在平方中丢失。 </p><p>工业界中，还是“<b>唯快不破</b>，<b>唯稳不破</b>”。我们知道，奇异值分解相对特征分解，有个很大的优势就是不要求原始矩阵是方阵。这非常符合现实生活中的数据。因此，有大神想到，是否可以用svd来解PCA？推导如下： </p><p>我们根据协方差矩阵的公式，把X按照奇异值分解展开，注意后面应用到了一个<b>酋矩阵</b>(unitary)的特性：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-6c9f98ec2007ea5730aec3f7ccd40cd2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"648\" data-rawheight=\"422\" class=\"origin_image zh-lightbox-thumb\" width=\"648\" data-original=\"https://pic3.zhimg.com/v2-6c9f98ec2007ea5730aec3f7ccd40cd2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;648&#39; height=&#39;422&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"648\" data-rawheight=\"422\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"648\" data-original=\"https://pic3.zhimg.com/v2-6c9f98ec2007ea5730aec3f7ccd40cd2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-6c9f98ec2007ea5730aec3f7ccd40cd2_b.jpg\"/></figure><p>看到最后的结果，是否跟上面的</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-a9fcf53f04161fed65932d743f105aa1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"290\" data-rawheight=\"66\" class=\"content_image\" width=\"290\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;290&#39; height=&#39;66&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"290\" data-rawheight=\"66\" class=\"content_image lazy\" width=\"290\" data-actualsrc=\"https://pic2.zhimg.com/v2-a9fcf53f04161fed65932d743f105aa1_b.jpg\"/></figure><p>很像？没错。协方差矩阵C的特征值和X的奇异值有以下关系</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-d299fa74199ee6022372a90455dcf853_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"220\" data-rawheight=\"118\" class=\"content_image\" width=\"220\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;220&#39; height=&#39;118&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"220\" data-rawheight=\"118\" class=\"content_image lazy\" width=\"220\" data-actualsrc=\"https://pic4.zhimg.com/v2-d299fa74199ee6022372a90455dcf853_b.jpg\"/></figure><p>而<b>C的特征向量即为X的SVD分解后的V向量</b>, 则参考PCA正常解法，X的k阶PCA即为_X右乘k阶V向量_。因此这种方式求PCA，只需要把原始矩阵做一次SVD分解即可，不用转置，不用求协方差矩阵。事实上，在<code>Scikit Learn</code>等机器学习框架中，就是用的SVD来做PCA。</p><hr/><h2>用numpy来验证</h2><h3>numpy原始解法求PCA</h3><p>接下来，我们用numpy来验证这种思路。首先是PCA的标准解法： 随机模拟一个原始数据矩阵，5个样本，3个特征：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"kn\">as</span> <span class=\"nn\">np</span>\n<span class=\"n\">X</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"mi\">5</span><span class=\"p\">,</span><span class=\"mi\">3</span><span class=\"p\">)</span>\n<span class=\"s1\">&#39;&#39;&#39;\n</span><span class=\"s1\">array([[0.86568791, 0.73022945, 0.17982869],\n</span><span class=\"s1\">       [0.07201287, 0.99358411, 0.84389196],\n</span><span class=\"s1\">       [0.61267696, 0.08867997, 0.11770573],\n</span><span class=\"s1\">       [0.16898969, 0.3093472 , 0.9010064 ],\n</span><span class=\"s1\">       [0.43840269, 0.97250927, 0.64897872]])\n</span><span class=\"s1\">&#39;&#39;&#39;</span></code></pre></div><p>将矩阵中心化，即减去均值：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">X_new</span> <span class=\"o\">=</span> <span class=\"n\">X</span> <span class=\"o\">-</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n<span class=\"s1\">&#39;&#39;&#39;\n</span><span class=\"s1\">array([[ 0.43413389,  0.11135945, -0.35845361],\n</span><span class=\"s1\">       [-0.35954115,  0.37471411,  0.30560966],\n</span><span class=\"s1\">       [ 0.18112294, -0.53019003, -0.42057657],\n</span><span class=\"s1\">       [-0.26256433, -0.3095228 ,  0.3627241 ],\n</span><span class=\"s1\">       [ 0.00684866,  0.35363927,  0.11069642]])\n</span><span class=\"s1\">&#39;&#39;&#39;</span>\n<span class=\"c1\"># 确保结果正确，即转换后均值为0</span>\n<span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">allclose</span><span class=\"p\">(</span><span class=\"n\">X_new</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"n\">X_new</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]))</span></code></pre></div><p>求X_new的协方差矩阵C</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">C</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">X_new</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">,</span> <span class=\"n\">X_new</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"n\">X_new</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">-</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"s1\">&#39;&#39;&#39;\n</span><span class=\"s1\">C\n</span><span class=\"s1\">array([[ 0.10488363, -0.02467955, -0.10903811],\n</span><span class=\"s1\">       [-0.02467955,  0.16369454,  0.05611495],\n</span><span class=\"s1\">       [-0.10903811,  0.05611495,  0.13564834]])\n</span><span class=\"s1\">&#39;&#39;&#39;</span></code></pre></div><p>求C的特征值和特征向量，这里用的是numpy的特征分解函数</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">eig_vals</span><span class=\"p\">,</span> <span class=\"n\">eig_vecs</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linalg</span><span class=\"o\">.</span><span class=\"n\">eig</span><span class=\"p\">(</span><span class=\"n\">C</span><span class=\"p\">)</span>\n<span class=\"s1\">&#39;&#39;&#39;\n</span><span class=\"s1\">eig_vals\n</span><span class=\"s1\">array([0.26474535, 0.00779743, 0.13168373])\n</span><span class=\"s1\">eig_vecs\n</span><span class=\"s1\">array([[-0.53801107,  0.72610049, -0.42816139],\n</span><span class=\"s1\">       [ 0.50584138, -0.12820944, -0.85304562],\n</span><span class=\"s1\">       [ 0.67429117,  0.67552974,  0.29831358]])\n</span><span class=\"s1\">&#39;&#39;&#39;</span></code></pre></div><p>求X的PCA结果，就是X右乘k阶特征向量。这里k还是取的3。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">X_pca</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">X_new</span><span class=\"p\">,</span> <span class=\"n\">eig_vecs</span><span class=\"p\">)</span>\n<span class=\"s1\">&#39;&#39;&#39;\n</span><span class=\"s1\">array([[-0.41894072,  0.05880142, -0.38780564],\n</span><span class=\"s1\">       [ 0.58905292, -0.10265648, -0.07453908],\n</span><span class=\"s1\">       [-0.64922927, -0.08462316,  0.24926273],\n</span><span class=\"s1\">       [ 0.22927473,  0.09406657,  0.4846625 ],\n</span><span class=\"s1\">       [ 0.24984234,  0.03441165, -0.27158052]])\n</span><span class=\"s1\">&#39;&#39;&#39;</span></code></pre></div><h3>numpy的SVD求PCA</h3><p>首先，直接求X_new的SVD，同样使用numpy的函数</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># 注意这里的Vh其实是公式中的VT</span>\n<span class=\"n\">U</span><span class=\"p\">,</span> <span class=\"n\">Sigma</span><span class=\"p\">,</span> <span class=\"n\">Vh</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linalg</span><span class=\"o\">.</span><span class=\"n\">svd</span><span class=\"p\">(</span><span class=\"n\">X_new</span><span class=\"p\">,</span> <span class=\"n\">full_matrices</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">,</span> <span class=\"n\">compute_uv</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n<span class=\"s1\">&#39;&#39;&#39;\n</span><span class=\"s1\">U\n</span><span class=\"s1\">array([[-0.40710685,  0.53434046,  0.33295236,  0.59843699, -0.28241844],\n</span><span class=\"s1\">       [ 0.57241386,  0.10270414, -0.58127362,  0.5471169 , -0.15677466],\n</span><span class=\"s1\">       [-0.63089041, -0.34344824, -0.47916326,  0.32579597,  0.38507162],\n</span><span class=\"s1\">       [ 0.22279838, -0.66779531,  0.53263483,  0.46842625,  0.03587876],\n</span><span class=\"s1\">       [ 0.24278501,  0.37419895,  0.19484969,  0.13026931,  0.86376738]])\n</span><span class=\"s1\">Sigma\n</span><span class=\"s1\">array([1.02906823, 0.72576506, 0.17660612])\n</span><span class=\"s1\">Vh\n</span><span class=\"s1\">array([[-0.53801107,  0.50584138,  0.67429117],\n</span><span class=\"s1\">       [ 0.42816139,  0.85304562, -0.29831358],\n</span><span class=\"s1\">       [ 0.72610049, -0.12820944,  0.67552974]])\n</span><span class=\"s1\">&#39;&#39;&#39;</span></code></pre></div><p>我们来根据上面的公式，确认下eig_vals和S的关系，注意在numpy的实现中，特征值和奇异值的排序是不同的</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">allclose</span><span class=\"p\">(</span><span class=\"n\">eig_vals</span><span class=\"p\">,</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">square</span><span class=\"p\">(</span><span class=\"n\">S</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"n\">X_new</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">-</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n<span class=\"s1\">&#39;&#39;&#39;\n</span><span class=\"s1\">eig_vals\n</span><span class=\"s1\">array([0.26474535, 0.00779743, 0.13168373])\n</span><span class=\"s1\">np.square(S) / (X_new.shape[0] - 1)\n</span><span class=\"s1\">array([0.26474535, 0.13168373, 0.00779743])\n</span><span class=\"s1\">&#39;&#39;&#39;</span></code></pre></div><p>从结果看出，确实跟公式是一致的。 接下来用SVD求PCA就简单了，直接右乘V即可。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># 注意Vh是公式中的VT，因此V=Vh.T</span>\n<span class=\"n\">X_pca_svd</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">X_new</span><span class=\"p\">,</span> <span class=\"n\">Vh</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">)</span>\n<span class=\"c1\"># X_pca_svd = np.dot(U, np.diag(Sigma))</span>\n<span class=\"s1\">&#39;&#39;&#39;\n</span><span class=\"s1\">X_pca_svd\n</span><span class=\"s1\">array([[-0.41894072,  0.38780564,  0.05880142],\n</span><span class=\"s1\">       [ 0.58905292,  0.07453908, -0.10265648],\n</span><span class=\"s1\">       [-0.64922927, -0.24926273, -0.08462316],\n</span><span class=\"s1\">       [ 0.22927473, -0.4846625 ,  0.09406657],\n</span><span class=\"s1\">       [ 0.24984234,  0.27158052,  0.03441165]])\n</span><span class=\"s1\">&#39;&#39;&#39;</span></code></pre></div><p>求出结果后，正当我们信心满满的对比一下<code>X_pca</code>和<code>X_pca_svd</code>,以为大功告成打完收工时，却发现二者是不一致的。WTF？ </p><h3>结果分析</h3><p>仔细研究下<code>X_pca</code>和<code>X_pca_svd</code>的结果，可以看出，排除特征值和奇异值的排序导致的列向量顺序不同外，部分列向量的绝对值相同但正负不同。 </p><p>问题出在哪里？我们搬出<code>Scikit Learn</code>，再来算一次PCA：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">from</span> <span class=\"nn\">sklearn.decomposition</span> <span class=\"kn\">import</span> <span class=\"n\">PCA</span>\n\n<span class=\"n\">pca</span> <span class=\"o\">=</span> <span class=\"n\">PCA</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">)</span>\n<span class=\"n\">pca</span><span class=\"o\">.</span><span class=\"n\">fit_transform</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span>   <span class=\"c1\"># sklearn自动处理去均值化</span>\n<span class=\"s1\">&#39;&#39;&#39;\n</span><span class=\"s1\">array([[ 0.41894072, -0.38780564, -0.05880142],\n</span><span class=\"s1\">       [-0.58905292, -0.07453908,  0.10265648],\n</span><span class=\"s1\">       [ 0.64922927,  0.24926273,  0.08462316],\n</span><span class=\"s1\">       [-0.22927473,  0.4846625 , -0.09406657],\n</span><span class=\"s1\">       [-0.24984234, -0.27158052, -0.03441165]])\n</span><span class=\"s1\">&#39;&#39;&#39;</span></code></pre></div><p>嗯，也是绝对值相同但正负不同。都说<code>Scikit Learn</code>的PCA就是SVD做的，难道是骗人的？ 好在代码不会骗人，我们直接翻出源码。</p><p>通过研究<code>Scikit Learn</code>的源码<a href=\"https://link.zhihu.com/?target=https%3A//github.com/scikit-learn/scikit-learn/blob/4c65d8e615c9331d37cbb6225c5b67c445a5c959/sklearn/utils/extmath.py%23L609\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">svd_flip@scikit-learn/extmath.py</a>找到了答案： SVD奇异值分解的结果是唯一的，但是分解出来的U矩阵和V矩阵的正负可以不是唯一，只要保证它们乘起来是一致的就行。因此，sklearn为了保证svd分解结果的一致性，它们的方案是：保证U矩阵的每一行(<code>u_i</code>)中，绝对值最大的元素一定是正数，否则将<code>u_i</code>转成<code>-u_i</code>,并将相应的<code>v_i</code>转成<code>-v_i</code>已保证结果的一致。 </p><p>这又是数学与工程的问题了。在数学上，几种结果都是正确的。但是在工程上，有个很重要的特性叫<b>幂等性</b>(Idempotence)。</p><blockquote> Methods can also have the property of “idempotence” in that (aside from error or expiration issues) the side-effects of N &gt; 0 identical requests is the same as for a single request.<br/> </blockquote><p>这是源自于HTTP规范中的一个概念，可以引申至各种分布式服务的设计当中，即：高质量的服务，一次请求和多次请求，其副作用（结果）应当是一致的。<code>Scikit Learn</code>正是通过<code>svd_flip</code>这个函数，把一个数学上并不幂等的操作，转化成了幂等的服务，其设计之讲究可见一斑。</p><hr/><h2>总结</h2><p>本文通过公式推导和numpy代码实战，展示了PCA的正常解法，以及工业界常用的SVD解法，并最后引申至数学和实现的一些探讨。“part-science, part-art”，这就是我最喜爱的机器学习之道</p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "numpy", 
                    "tagLink": "https://api.zhihu.com/topics/19834165"
                }, 
                {
                    "tag": "Principal Component Analysis", 
                    "tagLink": "https://api.zhihu.com/topics/19806816"
                }
            ], 
            "comments": [
                {
                    "userName": "知乎用户", 
                    "userLink": "https://www.zhihu.com/people/0", 
                    "content": "<p>不论什么数据量协方差矩阵都是方阵233</p>", 
                    "likes": 1, 
                    "childComments": []
                }, 
                {
                    "userName": "旋蓬", 
                    "userLink": "https://www.zhihu.com/people/91de0df0f287ae4b64a7b9cf23c14cb8", 
                    "content": "<p>按照文中说的X乘以Wk，假设X是n*n<b>的，那么W也是n*n，那么Wk应该是n*k,最后的结果不就是n*n mul n*k-&gt;n*k了吗？PCA并不改变矩阵维度把</b></p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/85542973", 
            "userName": "异尘", 
            "userLink": "https://www.zhihu.com/people/4e3a0f7cb9013a6fd523eecb5730987d", 
            "upvote": 4, 
            "title": "不用for循环 - 教你用numpy“手撕”朴素贝叶斯", 
            "content": "<p>常见的python朴素贝叶斯算法的方式，都是使用<code>for循环</code>来统计各个<code>p(特征|类型)</code>的值。其实机器学习除了常规算法思路外，很关键且很优雅的地方在于矩阵化（向量化），即<b>vectorization</b>。通过各种矩阵运算来去除<code>for循环</code>，是目前机器学习、深度学习中非常关键的技巧。文本不使用各种高阶机器学习库，单纯使用numpy来<b>手撕</b>朴素贝叶斯算法，带你领略机器学习中的矩阵运算之道。</p><hr/><h2>数据准备</h2><p>为了demo方便，我们先伪造一些数据。这里伪造的是NLP情感分类的数据。分为两部门<code>train</code>和<code>valid</code>，标签分为1和0，1代表侮辱性文字，0代表正常言论。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">train</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s1\">&#39;my dog had flea problems help help please&#39;</span><span class=\"p\">,</span>\n        <span class=\"s1\">&#39;mybe not take him to dog park stupid dog&#39;</span><span class=\"p\">,</span>\n        <span class=\"s1\">&#39;my dalmation is so cute I love him&#39;</span><span class=\"p\">,</span>\n        <span class=\"s1\">&#39;stop posting stupid worthless garbage&#39;</span><span class=\"p\">,</span>\n        <span class=\"s1\">&#39;mr licks ate my steak how to stop him&#39;</span><span class=\"p\">,]</span>\n<span class=\"n\">valid</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s1\">&#39;my stupid stupid worthless dog&#39;</span><span class=\"p\">]</span>\n<span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n<span class=\"n\">valid_y</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span></code></pre></div><hr/><h2>数据预处理</h2><p>NLP的数据预处理主要就是词典的创建和文本的向量化。这里由于是英语文档，不存在分词的问题。 </p><p>这里由于是<b>手撕</b>教程，不使用<code>spacy</code>等高阶NLP库，词典创建的也即使用Python的<code>set</code></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">vocab</span> <span class=\"o\">=</span> <span class=\"nb\">set</span><span class=\"p\">([])</span>\n<span class=\"k\">for</span> <span class=\"n\">doc</span> <span class=\"ow\">in</span> <span class=\"n\">train</span><span class=\"p\">:</span>\n    <span class=\"n\">vocab</span> <span class=\"o\">=</span> <span class=\"n\">vocab</span> <span class=\"o\">|</span> <span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"n\">doc</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">())</span>\n<span class=\"n\">vocabList</span> <span class=\"o\">=</span> <span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"n\">vocab</span><span class=\"p\">)</span>\n<span class=\"c1\"># 根据词反查index</span>\n<span class=\"n\">vocabList</span><span class=\"o\">.</span><span class=\"n\">index</span><span class=\"p\">(</span><span class=\"s1\">&#39;please&#39;</span><span class=\"p\">)</span></code></pre></div><hr/><h2>文档向量化</h2><p>向量话的原理，首先将文档中句子的每一个单词按照字典转换为<code>index</code></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">train_vec</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n<span class=\"n\">valid_vec</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n<span class=\"k\">for</span> <span class=\"n\">doc</span> <span class=\"ow\">in</span> <span class=\"n\">train</span><span class=\"p\">:</span>\n    <span class=\"n\">train_vec</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">([</span><span class=\"n\">vocabList</span><span class=\"o\">.</span><span class=\"n\">index</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">x</span> <span class=\"ow\">in</span> <span class=\"n\">doc</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">()])</span>\n<span class=\"k\">for</span> <span class=\"n\">doc</span> <span class=\"ow\">in</span> <span class=\"n\">valid</span><span class=\"p\">:</span>\n    <span class=\"n\">valid_vec</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">([</span><span class=\"n\">vocabList</span><span class=\"o\">.</span><span class=\"n\">index</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">x</span> <span class=\"ow\">in</span> <span class=\"n\">doc</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">()])</span></code></pre></div><p>可以看看向量化后的结果</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">train_vec</span><span class=\"p\">,</span> <span class=\"n\">valid_vec</span>\n<span class=\"s1\">&#39;&#39;&#39;\n</span><span class=\"s1\">[[28, 11, 16, 7, 26, 19, 19, 8],\n</span><span class=\"s1\">  [20, 24, 21, 14, 6, 11, 12, 22, 11],\n</span><span class=\"s1\">  [28, 5, 9, 25, 0, 3, 2, 14],\n</span><span class=\"s1\">  [1, 17, 22, 15, 13],\n</span><span class=\"s1\">  [18, 4, 27, 28, 10, 23, 6, 1, 14]],\n</span><span class=\"s1\"> [[28, 22, 22, 15, 11]]\n</span><span class=\"s1\">&#39;&#39;&#39;</span></code></pre></div><p>接着是将词典的每一个单词作为一个feature维度处理，将所有文档处理成相同维度的矩阵，维度大小即为词典的长度，而每个维度的值有多种处理方式，比如按照句子中每个单词的<code>Count</code>计数，或者是每个单词的<code>TFIDF</code>值。这里我们采用了<code>Count</code>，为了体现手撕，使用了Python的<code>Counter</code>，为了体现numpy矩阵运算的思路，使用了<code>scipy</code>的稀疏矩阵<code>sparse.csr_matrix</code>。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">from</span> <span class=\"nn\">collections</span> <span class=\"kn\">import</span> <span class=\"n\">Counter</span>\n<span class=\"kn\">from</span> <span class=\"nn\">scipy.sparse</span> <span class=\"kn\">import</span> <span class=\"n\">csr_matrix</span>\n<span class=\"k\">def</span> <span class=\"nf\">get_term_doc_matrix</span><span class=\"p\">(</span><span class=\"n\">label_list</span><span class=\"p\">,</span> <span class=\"n\">vocab_len</span><span class=\"p\">):</span>\n    <span class=\"n\">j_indices</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n    <span class=\"n\">indptr</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n    <span class=\"n\">values</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n    <span class=\"n\">indptr</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">doc</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">label_list</span><span class=\"p\">):</span>\n        <span class=\"n\">feature_counter</span> <span class=\"o\">=</span> <span class=\"n\">Counter</span><span class=\"p\">(</span><span class=\"n\">doc</span><span class=\"p\">)</span>\n        <span class=\"n\">j_indices</span><span class=\"o\">.</span><span class=\"n\">extend</span><span class=\"p\">(</span><span class=\"n\">feature_counter</span><span class=\"o\">.</span><span class=\"n\">keys</span><span class=\"p\">())</span>\n        <span class=\"n\">values</span><span class=\"o\">.</span><span class=\"n\">extend</span><span class=\"p\">(</span><span class=\"n\">feature_counter</span><span class=\"o\">.</span><span class=\"n\">values</span><span class=\"p\">())</span>\n        <span class=\"n\">indptr</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">j_indices</span><span class=\"p\">))</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">csr_matrix</span><span class=\"p\">((</span><span class=\"n\">values</span><span class=\"p\">,</span> <span class=\"n\">j_indices</span><span class=\"p\">,</span> <span class=\"n\">indptr</span><span class=\"p\">),</span>\n                      <span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">indptr</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">vocab_len</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"nb\">int</span><span class=\"p\">)</span>\n<span class=\"n\">train_matrix</span> <span class=\"o\">=</span> <span class=\"n\">get_term_doc_matrix</span><span class=\"p\">(</span><span class=\"n\">train_vec</span><span class=\"p\">,</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">vocabList</span><span class=\"p\">)</span>\n<span class=\"n\">valid_matrix</span> <span class=\"o\">=</span> <span class=\"n\">get_term_doc_matrix</span><span class=\"p\">(</span><span class=\"n\">valid_vec</span><span class=\"p\">,</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">vocabList</span><span class=\"p\">)</span></code></pre></div><p>可以看看矩阵化后的train和valid，分别转换成了samples*feature，29即为字典的维度。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">train_matrix</span><span class=\"p\">,</span> <span class=\"n\">valid_matrix</span>\n<span class=\"s1\">&#39;&#39;&#39;\n</span><span class=\"s1\">&lt;5x29 sparse matrix of type &#39;&#39;\n</span><span class=\"s1\">    with 37 stored elements in Compressed Sparse Row format&gt;,\n</span><span class=\"s1\"> &lt;1x29 sparse matrix of type &#39;&#39;\n</span><span class=\"s1\">    with 4 stored elements in Compressed Sparse Row format&gt;\n</span><span class=\"s1\">&#39;&#39;&#39;</span></code></pre></div><p>我们将生成的<b>train_matrix</b>作图如下（为方便观看省去了部分维度） </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-24150646b87785b995d8b019dbb75573_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"745\" data-rawheight=\"127\" class=\"origin_image zh-lightbox-thumb\" width=\"745\" data-original=\"https://pic4.zhimg.com/v2-24150646b87785b995d8b019dbb75573_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;745&#39; height=&#39;127&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"745\" data-rawheight=\"127\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"745\" data-original=\"https://pic4.zhimg.com/v2-24150646b87785b995d8b019dbb75573_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-24150646b87785b995d8b019dbb75573_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><hr/><h2>朴素贝叶斯算法原理</h2><p>谈到朴素贝叶斯算法，首先想到的必定是著名的贝叶斯公式：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-439c540a190f82e363a6a57f8c2d3104_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"470\" data-rawheight=\"112\" class=\"origin_image zh-lightbox-thumb\" width=\"470\" data-original=\"https://pic1.zhimg.com/v2-439c540a190f82e363a6a57f8c2d3104_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;470&#39; height=&#39;112&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"470\" data-rawheight=\"112\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"470\" data-original=\"https://pic1.zhimg.com/v2-439c540a190f82e363a6a57f8c2d3104_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-439c540a190f82e363a6a57f8c2d3104_b.jpg\"/></figure><p>不过这个定理如何体现到我们机器学习的场景呢？我们把其中的A和B换成实际的物理意义如下： </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-d871ed1dc6ff7de941d6fe87c802ad54_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"724\" data-rawheight=\"112\" class=\"origin_image zh-lightbox-thumb\" width=\"724\" data-original=\"https://pic1.zhimg.com/v2-d871ed1dc6ff7de941d6fe87c802ad54_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;724&#39; height=&#39;112&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"724\" data-rawheight=\"112\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"724\" data-original=\"https://pic1.zhimg.com/v2-d871ed1dc6ff7de941d6fe87c802ad54_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-d871ed1dc6ff7de941d6fe87c802ad54_b.png\"/></figure><p> 在我们的分类监督学习中，其实就是求在一定特征的样本情况下，某个具体类别的概率。根据贝叶斯公式，可以转换成分别求某个类别下这些特征的概率、某类别的概率、特征的概率。 </p><p>实际场景中，特征一般是多维的，而多维特征的联合概率是比较复杂的。这时候，就体现出<b>朴素</b>的概念了。我们对样本作出假设：样本中的各个特征是相互独立的。这样，可以将联合概率转换成以下的独立概率乘积：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-346c09dddc30b0b7f46df325dcc63f73_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1326\" data-rawheight=\"112\" class=\"origin_image zh-lightbox-thumb\" width=\"1326\" data-original=\"https://pic4.zhimg.com/v2-346c09dddc30b0b7f46df325dcc63f73_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1326&#39; height=&#39;112&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1326\" data-rawheight=\"112\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1326\" data-original=\"https://pic4.zhimg.com/v2-346c09dddc30b0b7f46df325dcc63f73_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-346c09dddc30b0b7f46df325dcc63f73_b.png\"/></figure><p> 当然，这种假设肯定是粗暴的。但是实践过程中，计算量减少带来的益处是远大于粗暴假设带来的失真。这也是最考验算法工程师的地方，既要考虑数学的严谨，同时也要考虑工程实现。“part-science, part-art”，这或许也是机器学习之道。 </p><p>当然，对于模型最终的应用来说，求出具体的概率绝对值是意义不大的。我们只需要知道不同特征之间概率的比值就行。也就是说，对于二分类问题，我们只需要知道：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b455413175091d2065aa1bc3a8f9e625_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"582\" data-rawheight=\"62\" class=\"origin_image zh-lightbox-thumb\" width=\"582\" data-original=\"https://pic2.zhimg.com/v2-b455413175091d2065aa1bc3a8f9e625_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;582&#39; height=&#39;62&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"582\" data-rawheight=\"62\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"582\" data-original=\"https://pic2.zhimg.com/v2-b455413175091d2065aa1bc3a8f9e625_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-b455413175091d2065aa1bc3a8f9e625_b.png\"/></figure><p> 就可以判断出该样本是属于类别1。于是，我们可以接着进行公式变换，将<code>P(类别2|特征)</code>移到等式左边，并取<code>log</code>：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-7b497f2d28a2e62b90e4733ab5cb24f8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"410\" data-rawheight=\"112\" class=\"content_image\" width=\"410\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;410&#39; height=&#39;112&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"410\" data-rawheight=\"112\" class=\"content_image lazy\" width=\"410\" data-actualsrc=\"https://pic1.zhimg.com/v2-7b497f2d28a2e62b90e4733ab5cb24f8_b.jpg\"/></figure><p> 同样，根据<b>朴素</b>的假设，可以得到：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-883e6462cbdf079c02a239510e01b8e6_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1282\" data-rawheight=\"112\" class=\"origin_image zh-lightbox-thumb\" width=\"1282\" data-original=\"https://pic3.zhimg.com/v2-883e6462cbdf079c02a239510e01b8e6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1282&#39; height=&#39;112&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1282\" data-rawheight=\"112\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1282\" data-original=\"https://pic3.zhimg.com/v2-883e6462cbdf079c02a239510e01b8e6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-883e6462cbdf079c02a239510e01b8e6_b.png\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-c0304d772f36f6726e7491c53a0b2ffb_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1550\" data-rawheight=\"112\" class=\"origin_image zh-lightbox-thumb\" width=\"1550\" data-original=\"https://pic4.zhimg.com/v2-c0304d772f36f6726e7491c53a0b2ffb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1550&#39; height=&#39;112&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1550\" data-rawheight=\"112\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1550\" data-original=\"https://pic4.zhimg.com/v2-c0304d772f36f6726e7491c53a0b2ffb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-c0304d772f36f6726e7491c53a0b2ffb_b.png\"/></figure><p> 即，我们需要求出每个特征对应的</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-933249fdaee888e637add55ee0ed8a3f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"370\" data-rawheight=\"112\" class=\"content_image\" width=\"370\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;370&#39; height=&#39;112&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"370\" data-rawheight=\"112\" class=\"content_image lazy\" width=\"370\" data-actualsrc=\"https://pic4.zhimg.com/v2-933249fdaee888e637add55ee0ed8a3f_b.jpg\"/></figure><p> 以及每个类别对应的</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-a5dd2a2fd6da486b3b79ceff46eda2d9_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"180\" data-rawheight=\"112\" class=\"content_image\" width=\"180\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;180&#39; height=&#39;112&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"180\" data-rawheight=\"112\" class=\"content_image lazy\" width=\"180\" data-actualsrc=\"https://pic2.zhimg.com/v2-a5dd2a2fd6da486b3b79ceff46eda2d9_b.png\"/></figure><p> 然后全部相加，看结果是否大于0即可判断出样本的类别。而这两块，我们都是可以从训练样本中求出（先验概率），这也就是朴素贝叶斯算法训练的部分。</p><hr/><h2>朴素贝叶斯矩阵运算</h2><p>前言中说到，一般的教程，直接使用<code>for循环</code>数数一般就可以求出上述的两个log值，完成“训练”。我们这样，要充分应用numpy的矩阵运算特性，不使用<code>for循环</code></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"kn\">as</span> <span class=\"nn\">np</span>\n<span class=\"n\">p1</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">squeeze</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">asarray</span><span class=\"p\">(</span><span class=\"n\">train_matrix</span><span class=\"p\">[</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">asarray</span><span class=\"p\">(</span><span class=\"n\">label</span><span class=\"o\">==</span><span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)))</span>\n<span class=\"n\">p0</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">squeeze</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">asarray</span><span class=\"p\">(</span><span class=\"n\">train_matrix</span><span class=\"p\">[</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">asarray</span><span class=\"p\">(</span><span class=\"n\">label</span><span class=\"o\">==</span><span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)))</span>\n<span class=\"n\">pr1</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">p1</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"p\">((</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">asarray</span><span class=\"p\">(</span><span class=\"n\">label</span><span class=\"p\">)</span><span class=\"o\">==</span><span class=\"mi\">1</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">()</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"n\">pr0</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">p0</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"p\">((</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">asarray</span><span class=\"p\">(</span><span class=\"n\">label</span><span class=\"p\">)</span><span class=\"o\">==</span><span class=\"mi\">0</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">()</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"n\">r</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">log</span><span class=\"p\">(</span><span class=\"n\">pr1</span><span class=\"o\">/</span><span class=\"n\">pr0</span><span class=\"p\">)</span></code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-7bf40d6495d3f3839f4d486344f1034a_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"740\" data-rawheight=\"121\" class=\"origin_image zh-lightbox-thumb\" width=\"740\" data-original=\"https://pic3.zhimg.com/v2-7bf40d6495d3f3839f4d486344f1034a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;740&#39; height=&#39;121&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"740\" data-rawheight=\"121\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"740\" data-original=\"https://pic3.zhimg.com/v2-7bf40d6495d3f3839f4d486344f1034a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-7bf40d6495d3f3839f4d486344f1034a_b.png\"/></figure><p>还是以这个图为例，<code>pr1</code>即为类别为1的样本中特征x的概率。在NLP中，每个特征都是一个词，其物理意义就是某个词在某个类别中出现的概率，而最后求出的<code>r</code>即为所有特征的概率在2个类别的比值，注意，这里的<code>r</code>是个向量，是通过样本之间的矩阵运算一次性求出。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">r</span>\n<span class=\"s1\">&#39;&#39;&#39;\n</span><span class=\"s1\">array([-0.40546511,  0.28768207, -0.40546511, -0.40546511,-0.40546511,\n</span><span class=\"s1\">       -0.40546511,  0.28768207, -0.40546511, -0.40546511, -0.40546511,\n</span><span class=\"s1\">       -0.40546511,  0.69314718,  0.98082925,  0.98082925, -0.11778304,\n</span><span class=\"s1\">        0.98082925, -0.40546511,  0.98082925, -0.40546511, -0.81093022,\n</span><span class=\"s1\">        0.98082925,  0.98082925,  1.38629436, -0.40546511,  0.98082925,\n</span><span class=\"s1\">       -0.40546511, -0.40546511, -0.40546511, -1.09861229])\n</span><span class=\"s1\">&#39;&#39;&#39;</span></code></pre></div><p>接下来求第二个log，这个直接就是不同类别的比例。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">b</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">log</span><span class=\"p\">((</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">asarray</span><span class=\"p\">(</span><span class=\"n\">label</span><span class=\"p\">)</span><span class=\"o\">==</span><span class=\"mi\">1</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">()</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">asarra</span><span class=\"p\">(</span><span class=\"n\">label</span><span class=\"p\">)</span><span class=\"o\">==</span><span class=\"mi\">0</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">())</span>\n<span class=\"s1\">&#39;&#39;&#39;\n</span><span class=\"s1\">-0.4054651081081643\n</span><span class=\"s1\">&#39;&#39;&#39;</span></code></pre></div><p>这里的<code>r</code>和<code>b</code>即为我们训练出的模型的参数。可以将其序列化到磁盘，供后续预测的时候使用（也就是深度学习中的inference）。</p><hr/><h2>朴素贝叶斯推理</h2><p>由于<code>r</code>代表了训练样本中每一个特征词的在不同类别的概率比值，比如’stupid’等词就很高，而’love’等词就较低。在实际应用中，我们同样取出预测样本中的所有词，然后考虑这些词的概率比值，最后全部相加，即可判断最终样本的类别。同样，我们使用numpy的矩阵运算。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">train_matrix</span><span class=\"nd\">@r</span> <span class=\"o\">+</span> <span class=\"n\">b</span> <span class=\"o\">&gt;</span> <span class=\"mi\">0</span>\n<span class=\"c1\"># array([False,  True, False,  True, False])</span>\n<span class=\"n\">valid_matrix</span><span class=\"nd\">@r</span> <span class=\"o\">+</span> <span class=\"n\">b</span> <span class=\"o\">&gt;</span> <span class=\"mi\">0</span>\n<span class=\"c1\"># array([ True])</span></code></pre></div><p>这里<code>@</code>是numpy的矩阵乘操作，举例如图  </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-adfc3dffb205c424f4f7795a16ed6005_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"284\" data-rawheight=\"164\" class=\"content_image\" width=\"284\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;284&#39; height=&#39;164&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"284\" data-rawheight=\"164\" class=\"content_image lazy\" width=\"284\" data-actualsrc=\"https://pic2.zhimg.com/v2-adfc3dffb205c424f4f7795a16ed6005_b.jpg\"/></figure><p>而<code>b</code>作为一个标量，会直接触发numpy中的<code>broadcast</code>操作，直接把<code>+</code>应用到所有样本，同样<code>&gt;</code>也会<code>broadcast</code>，最终得到一个array，表示每一个测试样本是否预测为类别1。</p><hr/><h2>扩展</h2><p>由于上面推理过程中，<code>train_matrix</code>和<code>valid_matrix</code>都是由词频组成，相当于在求具体概率比值的时候应用词频做了加权。在实际应用中，也可考虑不使用词频，直接使用是否含有某个特征词，如下：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">train_matrix</span><span class=\"o\">.</span><span class=\"n\">sign</span><span class=\"p\">()</span><span class=\"nd\">@r</span> <span class=\"o\">+</span> <span class=\"n\">b</span> <span class=\"o\">&gt;</span> <span class=\"mi\">0</span>\n<span class=\"n\">valid_matrix</span><span class=\"o\">.</span><span class=\"n\">sign</span><span class=\"p\">()</span><span class=\"nd\">@r</span> <span class=\"o\">+</span> <span class=\"n\">b</span> <span class=\"o\">&gt;</span> <span class=\"mi\">0</span></code></pre></div><p>具体哪种方式好，还是要根据实际的数据样本情况来具体测试。</p><hr/><h2>总结</h2><p>本文以NLP中的句子情感分析为例，使用numpy的矩阵运算来<b>手撕</b>朴素贝叶斯算法，可以加深对与朴素贝叶斯算法的理解，并体会机器学习中矩阵运算之道。 </p><p>朴素贝叶斯简单快捷，特别适合较大规模的稀疏矩阵，在情感分析、垃圾邮件分类等场景中有很普遍的应用。在深度学习大行其道的今天，我们仍然可以通过这个简单的算法来快速搭建整个流程的<b>pipeline</b>。在我眼里，在机器学习项目的初期，流程pipeline的重要性大于任何算法。</p>", 
            "topic": [
                {
                    "tag": "numpy", 
                    "tagLink": "https://api.zhihu.com/topics/19834165"
                }, 
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }, 
                {
                    "tag": "朴素贝叶斯", 
                    "tagLink": "https://api.zhihu.com/topics/20682927"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/84788824", 
            "userName": "异尘", 
            "userLink": "https://www.zhihu.com/people/4e3a0f7cb9013a6fd523eecb5730987d", 
            "upvote": 4, 
            "title": "numpy“手撕”文本主题模型之LSA", 
            "content": "<h2>前言</h2><p>在前文<a href=\"https://zhuanlan.zhihu.com/p/84732006\" class=\"internal\">任意网页正文内容主题词提取</a>中，我们采用了一个tf-idf来获取本文的关键词（主题）。由于tf-idf算法仅仅是一个统计模型，简单快速，适合作为baseline。它最大的问题在于单单考虑了词频，而没有考虑语义，并且我们取topK的词，这个K值如何选取，也是一个问题。 </p><p>本文介绍一种真正的主题模型，也是最早出现的主题模型LSA（Latent Semantic Analysis）：潜在语义分析，它主要是利用SVD降维的方式，将词与本文映射到一个新的空间，而这个空间正是以主题作为维度。它的原理非常漂亮，一次奇异值分解就可以得到主题模型，同时也解决了词义的问题。 </p><p>本文继续发挥hands-on的传统，以一个实例来说明LSA的用法。在生产环境中，一般会使用gensim等框架来快速进行开发，本文从scipy和numpy入手，可以更清楚的了解其中的原理。</p><hr/><h2>数据预处理</h2><p>为了演示方便，我们直接采用了scikit-learn中的Newsgroups数据集。这是用于文本分类、文本挖据和信息检索研究的国际标准数据集之一。数据集收集了大约20,000左右的新闻组文档，均匀分为20个不同主题的新闻组集合。 </p><p>我们截取了其中4个主题的数据，并采用scikit-learn中的API来装载。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">from</span> <span class=\"nn\">sklearn.datasets</span> <span class=\"kn\">import</span> <span class=\"n\">fetch_20newsgroups</span>\n<span class=\"n\">categories</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s1\">&#39;alt.atheism&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;talk.religion.misc&#39;</span><span class=\"p\">,</span><span class=\"s1\">&#39;comp.graphics&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;sci.space&#39;</span><span class=\"p\">]</span>\n<span class=\"n\">remove</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"s1\">&#39;headers&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;footers&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;quotes&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">newsgroups_train</span> <span class=\"o\">=</span> <span class=\"n\">fetch_20newsgroups</span><span class=\"p\">(</span><span class=\"n\">subset</span><span class=\"o\">=</span><span class=\"s1\">&#39;train&#39;</span><span class=\"p\">,</span><span class=\"n\">categories</span><span class=\"o\">=</span><span class=\"n\">categories</span><span class=\"p\">,</span> <span class=\"n\">remove</span><span class=\"o\">=</span><span class=\"n\">remove</span><span class=\"p\">)</span>\n<span class=\"n\">newsgroups_test</span> <span class=\"o\">=</span> <span class=\"n\">fetch_20newsgroups</span><span class=\"p\">(</span><span class=\"n\">subset</span><span class=\"o\">=</span><span class=\"s1\">&#39;test&#39;</span><span class=\"p\">,</span><span class=\"n\">categories</span><span class=\"o\">=</span><span class=\"n\">categories</span><span class=\"p\">,</span> <span class=\"n\">remove</span><span class=\"o\">=</span><span class=\"n\">remove</span><span class=\"p\">)</span></code></pre></div><p>数据下载好后，我们看看里面的文本长啥样</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">newsgroups_train</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">[:</span><span class=\"mi\">5</span><span class=\"p\">]</span>\n<span class=\"s1\">&#39;&#39;&#39;\n</span><span class=\"s1\">[&#34;Hi,</span><span class=\"se\">\\n\\n</span><span class=\"s1\">\n</span><span class=\"s1\">  I&#39;ve noticed that if you only save a model (withall your mapping planes</span><span class=\"se\">\\n</span><span class=\"s1\">\n</span><span class=\"s1\">  positioned carefully) to a .3DSfile that when you reload it after restarting</span><span class=\"se\">\\n</span><span class=\"s1\">\n</span><span class=\"s1\">  3DS, theyare given a default position and orientation.  But if yousave</span><span class=\"se\">\\n</span><span class=\"s1\">to a .PRJ file their positions/orientation arepreserved.  Does anyone</span><span class=\"se\">\\n</span><span class=\"s1\">\n</span><span class=\"s1\">  know why this information is notstored in the .3DS file?  Nothing is</span><span class=\"se\">\\n</span><span class=\"s1\">explicitly said inthe manual about saving texture rules in the .PRJ file.</span><span class=\"se\">\\n</span><span class=\"s1\">\n</span><span class=\"s1\">  I&#39;d like to be able to read the texture rule information,does anyone have </span><span class=\"se\">\\n</span><span class=\"s1\">\n</span><span class=\"s1\">  the format for the .PRJ file?</span><span class=\"se\">\\n\\n</span><span class=\"s1\">\n</span><span class=\"s1\">  Is the.CEL file format available from somewhere?</span><span class=\"se\">\\n\\n</span><span class=\"s1\">\n</span><span class=\"s1\">  Rych&#34;,\n</span><span class=\"s1\"> &#39;</span><span class=\"se\">\\n\\n</span><span class=\"s1\">\n</span><span class=\"s1\"> Seems to be, barring evidence to the contrary, that Koresh was simply</span><span class=\"se\">\\n</span><span class=\"s1\">\n</span><span class=\"s1\"> another deranged fanatic who thought it neccessary to take a whole bunch of</span><span class=\"se\">\\n</span><span class=\"s1\">\n</span><span class=\"s1\"> folks with him, children and all, to satisfy his delusional mania. Jim</span><span class=\"se\">\\n</span><span class=\"s1\">\n</span><span class=\"s1\"> Jones, circa 1993.</span><span class=\"se\">\\n\\n\\n</span><span class=\"s1\">\n</span><span class=\"s1\"> Nope - fruitcakes like Koresh have been demonstrating such evil corruption</span><span class=\"se\">\\n</span><span class=\"s1\">\n</span><span class=\"s1\"> for centuries.&#39;]\n</span><span class=\"s1\">&#39;&#39;&#39;</span></code></pre></div><p>拿到文本后，第一件事当然是tokenizer，然后采用bag-of-words词袋模型将其向量化。这里使用scikit-learn中的<code>CountVectorizer</code>，以词频计数来作为向量值，当然更精细化可以采用<code>TfidfVectorizer</code>。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"kn\">as</span> <span class=\"nn\">np</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.feature_extraction.text</span> <span class=\"kn\">import</span> <span class=\"n\">CountVectorizer</span>\n<span class=\"n\">vectorizer</span> <span class=\"o\">=</span> <span class=\"n\">CountVectorizer</span><span class=\"p\">(</span><span class=\"n\">stop_words</span><span class=\"o\">=</span><span class=\"s1\">&#39;english&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">vectors</span> <span class=\"o\">=</span> <span class=\"n\">vectorizer</span><span class=\"o\">.</span><span class=\"n\">fit_transform</span><span class=\"p\">(</span><span class=\"n\">newsgroups_train</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">todense</span><span class=\"p\">()</span>\n<span class=\"n\">vocab</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">(</span><span class=\"n\">vectorizer</span><span class=\"o\">.</span><span class=\"n\">get_feature_names</span><span class=\"p\">())</span></code></pre></div><p>可以看看向量化后的矩阵</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">vectors</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n<span class=\"s1\">&#39;&#39;&#39;\n</span><span class=\"s1\">(2034, 26576)\n</span><span class=\"s1\">&#39;&#39;&#39;</span></code></pre></div><p>这里表示共有2034篇文档，词的vocab_size为26576，将所有文档转成了26576维的向量，每个向量的值为该维表示词的词频。</p><hr/><h2>SVD分解</h2><p>向量化完成后，我们就可以开始奇异值分解了。 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b71f8c02b7cee7afbcd2e61d127cdc1d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"582\" data-rawheight=\"510\" class=\"origin_image zh-lightbox-thumb\" width=\"582\" data-original=\"https://pic2.zhimg.com/v2-b71f8c02b7cee7afbcd2e61d127cdc1d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;582&#39; height=&#39;510&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"582\" data-rawheight=\"510\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"582\" data-original=\"https://pic2.zhimg.com/v2-b71f8c02b7cee7afbcd2e61d127cdc1d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-b71f8c02b7cee7afbcd2e61d127cdc1d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">import</span> <span class=\"nn\">scipy</span>\n<span class=\"n\">U</span><span class=\"p\">,</span> <span class=\"n\">s</span><span class=\"p\">,</span> <span class=\"n\">Vh</span> <span class=\"o\">=</span> <span class=\"n\">scipy</span><span class=\"o\">.</span><span class=\"n\">linalg</span><span class=\"o\">.</span><span class=\"n\">svd</span><span class=\"p\">(</span><span class=\"n\">vectors</span><span class=\"p\">,</span> <span class=\"n\">full_matrices</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">)</span></code></pre></div><p>根据上图，我们可以看到分解出来的矩阵：  - U: (2034, 2034) 表示2034个样本，对应2034个topic  - s: (2034, ) 表示2034个奇异值，即topic的重要性分数  - Vh: (2034, 26576) 表示2034个topic，对应26576个vocab，注意这个Vh是公式中的VT，即转置后的V </p><p>SVD奇异值分解，具有以下性质：  - 奇异值分解为精确分解，即分解后的矩阵可以完全还原原矩阵，信息不丢失  - U和Vh是正交矩阵 我们可以在numpy中验证一下：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># 注意将s从一维向量转换成对角矩阵(diagonal matrix)</span>\n<span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">allclose</span><span class=\"p\">(</span><span class=\"n\">U</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">diag</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">Vh</span><span class=\"p\">),</span> <span class=\"n\">vectors</span><span class=\"p\">)</span>\n<span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">allclose</span><span class=\"p\">(</span><span class=\"n\">U</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">U</span><span class=\"p\">),</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">eye</span><span class=\"p\">(</span><span class=\"n\">U</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]))</span>\n<span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">allclose</span><span class=\"p\">(</span><span class=\"n\">Vh</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">Vh</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">),</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">eye</span><span class=\"p\">(</span><span class=\"n\">Vh</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]))</span></code></pre></div><hr/><h2>Topic解读</h2><p>LSA的优雅之处，就是把之前的高维文档向量，降维到低维，且这个维度代表了文档的隐含语义，即这个文档的主题topic。svd分解出来的Vh矩阵，即是每个主题的矩阵，维度是每个单词，维度值可以看成是这个主题中每个单词的的重要性。那么，我们可以选取重要性最高的词，来解读某个隐含主题。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">num_top_words</span> <span class=\"o\">=</span> <span class=\"mi\">8</span>\n<span class=\"k\">def</span> <span class=\"nf\">show_topics</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">):</span>\n    <span class=\"n\">top_words</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span> <span class=\"n\">t</span><span class=\"p\">:</span> <span class=\"p\">[</span><span class=\"n\">vocab</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span> <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">argsort</span><span class=\"p\">(</span><span class=\"n\">t</span><span class=\"p\">)[:</span><span class=\"o\">-</span><span class=\"n\">num_top_words</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">:</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]]</span>\n    <span class=\"n\">topic_words</span> <span class=\"o\">=</span> <span class=\"p\">([</span><span class=\"n\">top_words</span><span class=\"p\">(</span><span class=\"n\">t</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">t</span> <span class=\"ow\">in</span> <span class=\"n\">a</span><span class=\"p\">])</span>\n<span class=\"k\">return</span> <span class=\"p\">[</span><span class=\"s1\">&#39; &#39;</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">t</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">t</span> <span class=\"ow\">in</span> <span class=\"n\">topic_words</span><span class=\"p\">]</span>\n<span class=\"n\">show_topics</span><span class=\"p\">(</span><span class=\"n\">Vh</span><span class=\"p\">[:</span><span class=\"mi\">10</span><span class=\"p\">])</span>\n<span class=\"s1\">&#39;&#39;&#39;\n</span><span class=\"s1\">[&#39;ditto critus propagandist surname galacticentrickindergarten surreal imaginative&#39;,\n</span><span class=\"s1\"> &#39;jpeg gif file color quality image jfif format&#39;,\n</span><span class=\"s1\"> &#39;graphics edu pub mail 128 3d ray ftp&#39;,\n</span><span class=\"s1\"> &#39;jesus god matthew people atheists atheism does graphics&#39;,\n</span><span class=\"s1\"> &#39;image data processing analysis software available tools display&#39;,\n</span><span class=\"s1\"> &#39;god atheists atheism religious believe religion argument true&#39;,\n</span><span class=\"s1\"> &#39;space nasa lunar mars probe moon missions probes&#39;,\n</span><span class=\"s1\"> &#39;image probe surface lunar mars probes moon orbit&#39;,\n</span><span class=\"s1\"> &#39;argument fallacy conclusion example true ad argumentum premises&#39;,\n</span><span class=\"s1\"> &#39;space larson image theory universe physical nasa material&#39;]\n</span><span class=\"s1\">&#39;&#39;&#39;</span></code></pre></div><p>可以看出，有些主题是关于图片格式的，有的是关于邮件协议的，有的是关于太空的。 而svd分解出来的U矩阵，就是每个文档对应的主题矩阵，维度是每个主题，维度值也可以看成是每个主题的重要性。</p><hr/><h2>Truncated SVD</h2><p>在生产实践中，普通svd由于要exact decomposition，计算量会非常大，且最后的应用往往会只看前n个topic，因此Truncated SVD的优势在于，预先设置好n的值，可以在牺牲一定精度的条件下，大大减少计算量。这里提供两种常见的svd实现以作参考。 </p><h3>sklearn实现</h3><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">from</span> <span class=\"nn\">sklearn</span> <span class=\"kn\">import</span> <span class=\"n\">decomposition</span>\n<span class=\"n\">u</span><span class=\"p\">,</span> <span class=\"n\">s</span><span class=\"p\">,</span> <span class=\"n\">v</span> <span class=\"o\">=</span> <span class=\"n\">decomposition</span><span class=\"o\">.</span><span class=\"n\">randomized_svd</span><span class=\"p\">(</span><span class=\"n\">vectors</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">)</span></code></pre></div><h3>Facebook实现</h3><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">import</span> <span class=\"nn\">fbpca</span>\n<span class=\"n\">u</span><span class=\"p\">,</span> <span class=\"n\">s</span><span class=\"p\">,</span> <span class=\"n\">v</span> <span class=\"o\">=</span> <span class=\"n\">fbpca</span><span class=\"o\">.</span><span class=\"n\">pca</span><span class=\"p\">(</span><span class=\"n\">vectors</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">)</span></code></pre></div><hr/><h2>总结</h2><p>LSA作为最早出现的真正主题模型，非常优雅，同时也存在很多不足，比如它得到的不是一个概率模型，同时得到的向量中含有负值，难以直观解释。针对这些问题，后续就有NMF（非负矩阵分解），LDA（隐含狄利克雷分布）等模型出现。</p>", 
            "topic": [
                {
                    "tag": "自然语言处理", 
                    "tagLink": "https://api.zhihu.com/topics/19560026"
                }, 
                {
                    "tag": "LSA", 
                    "tagLink": "https://api.zhihu.com/topics/20075781"
                }, 
                {
                    "tag": "numpy", 
                    "tagLink": "https://api.zhihu.com/topics/19834165"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/84732006", 
            "userName": "异尘", 
            "userLink": "https://www.zhihu.com/people/4e3a0f7cb9013a6fd523eecb5730987d", 
            "upvote": 1, 
            "title": "任意网页正文内容主题词提取", 
            "content": "<h2>前言</h2><p>网页正文内容主题提取，即任意给一个网页url，通过爬取网页内容和文本分析，得出该网页内容的关键词，作为网页的标签。这些关键词和标签在做流量分析_内容推荐方面有非常重要的意义。比如说我们做数字营销，用过页面来做用户引流，我们就可以知道吸引用户过来的点是什么，用户的潜在需求是什么；另外，针对内容社区的用户画像/推荐系统，关键点也是文章/页面的主题和标签。 </p><p>这个任务涉及的技术点主要有以下几个：  </p><ol><li><b>网页爬虫</b>。做网页内容分析，首先得根据url把网页内容扒下来吧。 </li><li><b>正文提取</b>。现在的web页面是非常复杂的，除了正文外，包含了大量的广告、导航、信息流等，我们需要去除干扰，只提取网页的正文信息。  </li><li><b>主题模型</b> 。拿到正文文本后，就需要做NLP来提取主题关键字了。</li></ol><hr/><h2>网页爬虫</h2><p>这里的网页爬虫和一般的爬虫还不太一样，会简单许多，主要是把原始网页的HTML抓下来即可，主要是为后续的分析挖掘打下基础，属于数据采集的阶段。 </p><p>这里我们采用了Python的<code>requests</code>包。<code>requests</code>相对于Python自带的<code>urllib</code>来说，API更为人性化，鲁棒性也更好。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">import</span> <span class=\"nn\">requests</span>\n<span class=\"n\">r</span> <span class=\"o\">=</span> <span class=\"n\">request</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">)</span>\n<span class=\"n\">r</span><span class=\"o\">.</span><span class=\"n\">encoding</span><span class=\"o\">=</span><span class=\"s1\">&#39;utf-8&#39;</span>\n<span class=\"n\">html</span> <span class=\"o\">=</span> <span class=\"n\">r</span><span class=\"o\">.</span><span class=\"n\">text</span></code></pre></div><hr/><h2>正文提取</h2><p>通过研究爬取下来的原始HTML，我们可以看到是非常负责而且杂乱无章的，充斥着大量的js代码等。我们首先需要解析HTML，尽量过滤掉js代码，留下文本内容。 </p><p>这里我们采用了Python的<code>BeautifulSoup</code>包。这个包堪称Python一大神器，解析HTML效果非常好</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">from</span> <span class=\"nn\">bs4</span> <span class=\"kn\">import</span> <span class=\"n\">BeautifulSoup</span>\n<span class=\"n\">soup</span> <span class=\"o\">=</span> <span class=\"n\">BeautifulSoup</span><span class=\"p\">(</span><span class=\"n\">html</span><span class=\"p\">,</span> <span class=\"n\">features</span><span class=\"o\">=</span><span class=\"s2\">&#34;html.parser&#34;</span><span class=\"p\">)</span>\n<span class=\"k\">for</span> <span class=\"n\">script</span> <span class=\"ow\">in</span> <span class=\"n\">soup</span><span class=\"p\">([</span><span class=\"s2\">&#34;script&#34;</span><span class=\"p\">,</span> <span class=\"s2\">&#34;style&#34;</span><span class=\"p\">]):</span>\n    <span class=\"n\">script</span><span class=\"o\">.</span><span class=\"n\">decompose</span><span class=\"p\">()</span>\n<span class=\"n\">text</span> <span class=\"o\">=</span> <span class=\"n\">soup</span><span class=\"o\">.</span><span class=\"n\">get_text</span><span class=\"p\">()</span></code></pre></div><p>我们想要的是网页的正文内容，其他的诸如广告或者导航栏等干扰内容需要尽可能的过滤掉。通过<code>BeautifulSoup</code>可以解析出整个HTML的DOM树结构，但是每个网页HTML的写法各不相同，单纯靠HTML解析无法做到通用，因此我们需要跳出HTML的思维，使用其他的方法来提取网页的正文。这里有个很优雅的方式是“基于行块分布函数”的算法<code>cx-extractor</code>。</p><blockquote> 基于行块分布函数的通用网页正文抽取：线性时间、不建DOM树、与HTML标签无关<br/> 对于Web信息检索来说，网页正文抽取是后续处理的关键。虽然使用正则表达式可以准确的抽取某一固定格式的页面，但面对形形色色的HTML，使用规则处理难免捉襟见肘。能不能高效、准确的将一个页面的正文抽取出来，并做到在大规模网页范围内通用，这是一个直接关系上层应用的难题。<br/> <a href=\"https://link.zhihu.com/?target=http%3A//weibo.com/cx3180\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">作者</a> 提出了 <a href=\"https://link.zhihu.com/?target=http%3A//cx-extractor.googlecode.com/files/%25E5%259F%25BA%25E4%25BA%258E%25E8%25A1%258C%25E5%259D%2597%25E5%2588%2586%25E5%25B8%2583%25E5%2587%25BD%25E6%2595%25B0%25E7%259A%2584%25E9%2580%259A%25E7%2594%25A8%25E7%25BD%2591%25E9%25A1%25B5%25E6%25AD%25A3%25E6%2596%2587%25E6%258A%25BD%25E5%258F%2596%25E7%25AE%2597%25E6%25B3%2595.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">《基于行块分布函数的通用网页正文抽取算法》</a> ，首次将网页正文抽取问题转化为求页面的行块分布函数，这种方法不用建立Dom树，不被病态HTML所累（事实上与HTML标签完全无关）。通过在线性时间内建立的行块分布函数图，直接准确定位网页正文。同时采用了统计与规则相结合的方法来处理通用性问题。作者相信简单的事情总应该用最简单的办法来解决这一亘古不变的道理。整个算法实现代码不足百行。但量不在多，在法。<br/> </blockquote><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-a3cf2ec259f0383ff94d01dbe2e25e40_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"712\" data-rawheight=\"456\" class=\"origin_image zh-lightbox-thumb\" width=\"712\" data-original=\"https://pic1.zhimg.com/v2-a3cf2ec259f0383ff94d01dbe2e25e40_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;712&#39; height=&#39;456&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"712\" data-rawheight=\"456\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"712\" data-original=\"https://pic1.zhimg.com/v2-a3cf2ec259f0383ff94d01dbe2e25e40_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-a3cf2ec259f0383ff94d01dbe2e25e40_b.jpg\"/></figure><p>上图就是某个页面求出的行块分布函数曲线。该网页的正文区域为145行至182行，即分布函数图上含有最值且连续的一个区域，这个区域往往含有一个骤升点和一个骤降点，因此，网页正文抽取问题转化为了求行块分布函数上的骤升点和骤降点两个边节点。 这里我们采用了这个算法的Python实现<a href=\"https://link.zhihu.com/?target=https%3A//github.com/chrislinan/cx-extractor-python\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">GitHub - chrislinan/cx-extractor-python</a>：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">from</span> <span class=\"nn\">CxExtractor</span> <span class=\"kn\">import</span> <span class=\"n\">CxExtractor</span>\n<span class=\"n\">cx</span> <span class=\"o\">=</span> <span class=\"n\">CxExtractor</span><span class=\"p\">(</span><span class=\"n\">threshold</span><span class=\"o\">=</span><span class=\"mi\">40</span><span class=\"p\">)</span>\n<span class=\"n\">text</span> <span class=\"o\">=</span> <span class=\"n\">cx</span><span class=\"o\">.</span><span class=\"n\">getText</span><span class=\"p\">(</span><span class=\"n\">text</span><span class=\"p\">)</span>\n<span class=\"n\">texts</span> <span class=\"o\">=</span> <span class=\"n\">text</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">(</span><span class=\"s1\">&#39;</span><span class=\"se\">\\n</span><span class=\"s1\">&#39;</span><span class=\"p\">)</span></code></pre></div><hr/><h2>主题模型</h2><p>拿到网页正文内容文本后，就需要提取正文主题关键词了。常见做法有以下几种：  </p><ol><li>TFIDF  </li><li>Text-Rank  </li><li>LSI/LDA </li></ol><p>这里我们先采用TFIDF的方式来做。</p><blockquote> TFIDF(Term Frequency Inverse Document Frequency)是一种用于信息检索与数据挖掘的常用加权技术。 词频（TF）=某个词在文本中出现的次数/该文本中总词数 逆向文档频（IDF）=log(语料库中所有文档总数/(包含某词的文档数+1)) 我们通过TF，也就是某个词在文本中出现的频度，来提升这个词在主题中的权重，然后我们通过IDF值，即逆向文档频来降低公共词的主题权重。TF*IDF也就得到了我们要的主题词权重。<br/> </blockquote><p>做TFIDF，首先步骤是分词。分词的效果取决于词典的构建，且对后续关键词提取影响巨大。首先要基于分析的行业主题建立专用词典，然后还需要维护停用词的词典。有了词典后，我们就可以采用Python分词的神器<code>jieba</code>来处理分词。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">import</span> <span class=\"nn\">jieba</span>\n<span class=\"n\">jieba</span><span class=\"o\">.</span><span class=\"n\">load_userdict</span><span class=\"p\">(</span><span class=\"s1\">&#39;./dict.txt&#39;</span><span class=\"p\">)</span>    <span class=\"c1\">#自定义词典</span>\n<span class=\"n\">stopwords</span> <span class=\"o\">=</span> <span class=\"nb\">set</span><span class=\"p\">([</span><span class=\"n\">line</span><span class=\"o\">.</span><span class=\"n\">strip</span><span class=\"p\">()</span> <span class=\"k\">for</span> <span class=\"n\">line</span> <span class=\"ow\">in</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"s1\">&#39;stopwords.txt&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;r&#39;</span><span class=\"p\">,</span> <span class=\"n\">encoding</span><span class=\"o\">=</span><span class=\"s1\">&#39;utf-8&#39;</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">readlines</span><span class=\"p\">()])</span>   <span class=\"c1\">#停用词典</span>\n\n<span class=\"n\">word_lists</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n<span class=\"k\">for</span> <span class=\"n\">text</span> <span class=\"ow\">in</span> <span class=\"n\">texts</span><span class=\"p\">:</span>\n    <span class=\"n\">word_lists</span> <span class=\"o\">+=</span> <span class=\"p\">(</span><span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"n\">jieba</span><span class=\"o\">.</span><span class=\"n\">cut</span><span class=\"p\">(</span><span class=\"n\">text</span><span class=\"p\">,</span> <span class=\"n\">cut_all</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">)))</span>\n<span class=\"n\">word_lists</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">w</span> <span class=\"k\">for</span> <span class=\"n\">w</span> <span class=\"ow\">in</span> <span class=\"n\">word_lists</span> <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">is_stop_word</span><span class=\"p\">(</span><span class=\"n\">w</span><span class=\"p\">)]</span></code></pre></div><p>分词完毕后，我们就可以计算TFIDF了。可以通过<code>gensim</code>，<code>scikit-learn</code>等机器学习专用包来做，<code>jieba</code>本身也提供这个功能，这里我们直接用<code>jieba</code>。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">import</span> <span class=\"nn\">jieba.analyse</span>\n<span class=\"n\">keywords</span> <span class=\"o\">=</span> <span class=\"n\">jieba</span><span class=\"o\">.</span><span class=\"n\">analyse</span><span class=\"o\">.</span><span class=\"n\">extract_tags</span><span class=\"p\">(</span><span class=\"s1\">&#39; &#39;</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">word_lists</span><span class=\"p\">),</span> \n                                      <span class=\"n\">topK</span><span class=\"o\">=</span><span class=\"mi\">20</span><span class=\"p\">,</span> \n                                      <span class=\"n\">withWeight</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span> \n                                      <span class=\"n\">allowPOS</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s1\">&#39;n&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;ns&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;nr&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;nt&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;nz&#39;</span><span class=\"p\">])</span></code></pre></div><p>注意这里有个参数是<code>allowPOS</code>，按照词性过滤。这个需要根据实际的业务需求来设置。</p><blockquote> 词性标注(Part-Of-Speech Tagging, POS tagging)，是语料库语言学中将语料库内单词的词性按照其含义和上下文内容进行标记的文本数据处理技术。常见标注示例：<br/> n 名词<br/> nr 人名<br/> ns 地名<br/> nt 机构团体<br/> nz 其他专名<br/> a 形容词<br/> v 动词<br/> </blockquote><hr/><h2>服务</h2><p>到这里，我们的关键词提取就结束了，为了方便其他同学来使用，我们可以用<code>Flask</code>做一个restful api，输入为网址url，输出为提取出的关键词并排序。</p><hr/><h2>总结</h2><p>在这篇文章里，我们完成了从任意网页url提取正文主题关键词的功能。在主题模型这块采用了常见的TFIDF的算法来解决，可以快速出一个原型提供给业务方使用。后续我们会继续优化，采用更多的算法来进一步提升效果。</p>", 
            "topic": [
                {
                    "tag": "网页", 
                    "tagLink": "https://api.zhihu.com/topics/19571150"
                }, 
                {
                    "tag": "自然语言处理", 
                    "tagLink": "https://api.zhihu.com/topics/19560026"
                }
            ], 
            "comments": []
        }
    ], 
    "url": "https://zhuanlan.zhihu.com/aifun"
}
