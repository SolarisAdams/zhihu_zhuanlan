{
    "title": "机器学习 ML", 
    "description": "记录平时学习与科研的点滴", 
    "followers": [
        "https://www.zhihu.com/people/lin-guang-han", 
        "https://www.zhihu.com/people/chu-xue-zhe-67-21", 
        "https://www.zhihu.com/people/gu-lu-gu-lu-xiao-mo-xian-12", 
        "https://www.zhihu.com/people/zouyb-zolo", 
        "https://www.zhihu.com/people/yao-hao-hao-xue-xi-de-xiao-sen-lin", 
        "https://www.zhihu.com/people/tian-chang-di-jiu-63-41", 
        "https://www.zhihu.com/people/Steven_Jokes", 
        "https://www.zhihu.com/people/laiyongqin-4", 
        "https://www.zhihu.com/people/er2e4r-46", 
        "https://www.zhihu.com/people/xidiangulasijia", 
        "https://www.zhihu.com/people/gao-zhang-yang-95", 
        "https://www.zhihu.com/people/zhao-da-94-57", 
        "https://www.zhihu.com/people/xu-qing-yao-86", 
        "https://www.zhihu.com/people/chen-qiang-13-63", 
        "https://www.zhihu.com/people/fengboxie", 
        "https://www.zhihu.com/people/qiu-wei-min-11", 
        "https://www.zhihu.com/people/shine-93-33-59", 
        "https://www.zhihu.com/people/da-hai-22-91", 
        "https://www.zhihu.com/people/liaohongyu", 
        "https://www.zhihu.com/people/li-shi-kun-34", 
        "https://www.zhihu.com/people/xixi-68-41", 
        "https://www.zhihu.com/people/calvinjku", 
        "https://www.zhihu.com/people/pan-pan-8-26-63", 
        "https://www.zhihu.com/people/iterator-26", 
        "https://www.zhihu.com/people/zhmman-9", 
        "https://www.zhihu.com/people/fan-di-kai-82", 
        "https://www.zhihu.com/people/yang-jing-lin-16", 
        "https://www.zhihu.com/people/zxx-32-31", 
        "https://www.zhihu.com/people/zha-ma-si-da-ren", 
        "https://www.zhihu.com/people/xi-wei-ling-21", 
        "https://www.zhihu.com/people/alfredzhang", 
        "https://www.zhihu.com/people/xiao-guai-46-78", 
        "https://www.zhihu.com/people/he-hong-liang-80", 
        "https://www.zhihu.com/people/ldh1848590134", 
        "https://www.zhihu.com/people/ye-li-tiao-deng-kan-jian-99", 
        "https://www.zhihu.com/people/tuo-la-ji-shou-5", 
        "https://www.zhihu.com/people/guo-jia-chang-49", 
        "https://www.zhihu.com/people/zhong-er-cheng-xu-yuan", 
        "https://www.zhihu.com/people/lu-yuan-yuan-39", 
        "https://www.zhihu.com/people/peng-xue-feng-34", 
        "https://www.zhihu.com/people/echo-75-50-36", 
        "https://www.zhihu.com/people/wang-jing-95-88-3", 
        "https://www.zhihu.com/people/HaveTwoBrush", 
        "https://www.zhihu.com/people/li-xiang-73-34", 
        "https://www.zhihu.com/people/cao-kai-98-70", 
        "https://www.zhihu.com/people/hu-yan-di-18", 
        "https://www.zhihu.com/people/wen-jie-hua-35", 
        "https://www.zhihu.com/people/dong-feng-zao-ji", 
        "https://www.zhihu.com/people/zhao-yi-ting-22-82", 
        "https://www.zhihu.com/people/yu-zhe-1", 
        "https://www.zhihu.com/people/cerena-8", 
        "https://www.zhihu.com/people/wang-jie-jie-46", 
        "https://www.zhihu.com/people/xing-yun-lucky-70", 
        "https://www.zhihu.com/people/zhao-rong-wen", 
        "https://www.zhihu.com/people/zhang-jin-jie-20-44", 
        "https://www.zhihu.com/people/anton-94", 
        "https://www.zhihu.com/people/qiu-cheng-jian-54", 
        "https://www.zhihu.com/people/huan-shi-ting-shuo-rene", 
        "https://www.zhihu.com/people/guang-ming-gmg", 
        "https://www.zhihu.com/people/amusi1994", 
        "https://www.zhihu.com/people/fei-de-geng-gao-2", 
        "https://www.zhihu.com/people/ren-sheng-12-5", 
        "https://www.zhihu.com/people/wang-jing-bo-27-88", 
        "https://www.zhihu.com/people/quxiaofeng", 
        "https://www.zhihu.com/people/kemin-cheng", 
        "https://www.zhihu.com/people/zheng-jie-68-47", 
        "https://www.zhihu.com/people/1111112222", 
        "https://www.zhihu.com/people/dian-fan-guo-chi-de-lao-guo", 
        "https://www.zhihu.com/people/xi-feng-fei-ma", 
        "https://www.zhihu.com/people/xi-que-16-92-9", 
        "https://www.zhihu.com/people/hitljx", 
        "https://www.zhihu.com/people/yu-mo-ye-81", 
        "https://www.zhihu.com/people/lou-wan-wan-11", 
        "https://www.zhihu.com/people/pure-33", 
        "https://www.zhihu.com/people/wj2014-59", 
        "https://www.zhihu.com/people/jacob-40-92", 
        "https://www.zhihu.com/people/lllsh-44", 
        "https://www.zhihu.com/people/shui-shi-yun-fei", 
        "https://www.zhihu.com/people/spongebob-53-24", 
        "https://www.zhihu.com/people/wang-chen-91-90", 
        "https://www.zhihu.com/people/hao-zi-63-16", 
        "https://www.zhihu.com/people/enzo-22-30", 
        "https://www.zhihu.com/people/stone-mr-89", 
        "https://www.zhihu.com/people/mtino", 
        "https://www.zhihu.com/people/zzx-47-7", 
        "https://www.zhihu.com/people/wzz-74-1", 
        "https://www.zhihu.com/people/bobby-90-13", 
        "https://www.zhihu.com/people/lin-___-chen", 
        "https://www.zhihu.com/people/mulanshine", 
        "https://www.zhihu.com/people/qiu-bo-11-82", 
        "https://www.zhihu.com/people/panovr", 
        "https://www.zhihu.com/people/hesun312", 
        "https://www.zhihu.com/people/liu-zheng-zhao-17", 
        "https://www.zhihu.com/people/hai-yang-67-68", 
        "https://www.zhihu.com/people/led-77", 
        "https://www.zhihu.com/people/mie-zi-qi-62", 
        "https://www.zhihu.com/people/myluo", 
        "https://www.zhihu.com/people/xiesh0427", 
        "https://www.zhihu.com/people/ke-ke-23-12", 
        "https://www.zhihu.com/people/wu-qi-mo-50", 
        "https://www.zhihu.com/people/ye-cha-4", 
        "https://www.zhihu.com/people/ching-swy", 
        "https://www.zhihu.com/people/meetofly", 
        "https://www.zhihu.com/people/wen-hong-chen", 
        "https://www.zhihu.com/people/lan-lan-lan-lan-96-82", 
        "https://www.zhihu.com/people/alahlll", 
        "https://www.zhihu.com/people/skylakex64", 
        "https://www.zhihu.com/people/tom-pareto", 
        "https://www.zhihu.com/people/zhang-xiao-hui-66-53", 
        "https://www.zhihu.com/people/liu-fei-94-95", 
        "https://www.zhihu.com/people/ping.love", 
        "https://www.zhihu.com/people/reseted1565083768867", 
        "https://www.zhihu.com/people/zhang-peng-11-2", 
        "https://www.zhihu.com/people/li-qin-peng-29", 
        "https://www.zhihu.com/people/kevin-hill", 
        "https://www.zhihu.com/people/sunyqg", 
        "https://www.zhihu.com/people/hai-lan-xin", 
        "https://www.zhihu.com/people/kankanshu", 
        "https://www.zhihu.com/people/ke-wu-88", 
        "https://www.zhihu.com/people/su-wei-gen-83", 
        "https://www.zhihu.com/people/zzd-65-78"
    ], 
    "article": [
        {
            "url": "https://zhuanlan.zhihu.com/p/50217835", 
            "userName": "KIRA", 
            "userLink": "https://www.zhihu.com/people/e9aa8c6a879e9c02311dc5f5111cbdcd", 
            "upvote": 20, 
            "title": "草稿纸上的Transformer", 
            "content": "<p>【写在前面】</p><p>本篇是自然语言处理系列总结的第二篇，也是继上一篇<a href=\"https://zhuanlan.zhihu.com/p/46040939\" class=\"internal\">seq2seq</a>后再次深入讨论文本生成的文章。本文基于2017年谷歌团队推出的《Attention is all you need》[1]，将尽可能详细的阐述论文中transformer的思想和细节。由于这篇论文涉及的点很多，不一定能全面涵盖到，还请谅解，我也将不断的补充在一刷二刷论文时未发现的内容。</p><p>【正文开始】</p><p>上一篇专栏文章中已经探讨了经典的Encoder-Decoder框架，一般情况下采用RNN系列的网络（LSTM/GRU）来作为Encoder和Decoder部分的主要工具，利用Attention机制来保证两部分的文本对齐等问题。看似已经较好的解决了大多数序列到序列（seq2seq）的问题，然而RNN系列的一个天生局限就是其时序性，以文本序列为例，我们必须要先计算出t时刻的隐状态信息，才能够继续计算下一时刻的相应信息，<b>这就导致大规模数据下模型整体效率比较低，难以实现计算的并行化。</b>在这个背景下，Transformer应运而生，就像其论文的名字一样，将Encoder和Decoder部分的RNN系列全部替换为Attention，使得模型变成了Attention is <b>all </b>you need。</p><h2><b>一、Attention机制的一般形式及self-Attention</b></h2><ul><li><b>Attention的一般形式</b></li></ul><p>为了更统一的描述各个部分的Attention机制，我们首先对过去各种各样的Attention做一个形式上的抽象。回想之前提到过的seq2seq中的Attention机制是如何进行的呢？也就是对于待解码的词向量 <img src=\"https://www.zhihu.com/equation?tex=E%28x_%7Bi%7D%29\" alt=\"E(x_{i})\" eeimg=\"1\"/> ,需要考虑它上一时刻的隐状态 <img src=\"https://www.zhihu.com/equation?tex=s_%7Bi-1%7D\" alt=\"s_{i-1}\" eeimg=\"1\"/> ,与Encoder部分的每一个隐状态 <img src=\"https://www.zhihu.com/equation?tex=h_%7Bj%7D\" alt=\"h_{j}\" eeimg=\"1\"/> 进行比较，通过某个打分函数 <img src=\"https://www.zhihu.com/equation?tex=a%28%5Ccdot%29\" alt=\"a(\\cdot)\" eeimg=\"1\"/> 得到权重 <img src=\"https://www.zhihu.com/equation?tex=a_%7Bij%7D%3Da%28s_%7Bi-1%2Ch_%7Bj%7D%7D%29\" alt=\"a_{ij}=a(s_{i-1,h_{j}})\" eeimg=\"1\"/> ,再将这j个权重赋给j个Encoder的隐状态 <img src=\"https://www.zhihu.com/equation?tex=h_%7Bj%7D\" alt=\"h_{j}\" eeimg=\"1\"/> 得到加权求和的向量 <img src=\"https://www.zhihu.com/equation?tex=c_%7Bi%7D+%3D+%5Csum_%7Bj%7D%7Ba_%7Bij%7Dh_%7Bj%7D%7D\" alt=\"c_{i} = \\sum_{j}{a_{ij}h_{j}}\" eeimg=\"1\"/> 。</p><p>在上面这个过程中，不难发现实际上操作的有三类变量，第一类是<b>需要与一个变量集合逐一比较的，我们称为query</b>，简记为q。而那个集合里每一个变量作为第二类变量，是<b>要与q进行比较的，称为key</b>，简记为k。而最后一类变量是<b>被权重 <img src=\"https://www.zhihu.com/equation?tex=a_%7Bij%7D\" alt=\"a_{ij}\" eeimg=\"1\"/> 对应j位置赋权的，称为value</b>，简记为v<b>(这样定义，很容易发现key和value是一一对应的，</b>因为第j个需要和query比较的key，得到的权重要赋予给第j个value)。这样描述起来很抽象，与上述具体的过程对应起来，就是 <img src=\"https://www.zhihu.com/equation?tex=q%3Ds_%7Bi-1%7D\" alt=\"q=s_{i-1}\" eeimg=\"1\"/> , <img src=\"https://www.zhihu.com/equation?tex=k%3Dv%3Dh_%7Bj%7D\" alt=\"k=v=h_{j}\" eeimg=\"1\"/> ，这样理解起来就形象多了。</p><p>有了这种抽象的表述，将每一个q,k,v以矩阵形式表述，可以表示为：</p><p><img src=\"https://www.zhihu.com/equation?tex=Q%3D%28q_1%2Cq_2%2C...%2Cq_m%29%5E%7BT%7D\" alt=\"Q=(q_1,q_2,...,q_m)^{T}\" eeimg=\"1\"/> ， <img src=\"https://www.zhihu.com/equation?tex=K%3D%28k_1%2Ck_2%2C..%2Ck_n%29%5E%7BT%7D\" alt=\"K=(k_1,k_2,..,k_n)^{T}\" eeimg=\"1\"/> , <img src=\"https://www.zhihu.com/equation?tex=V%3D%28v_1%2Cv_2%2C..%2Cv_n%29%5ET\" alt=\"V=(v_1,v_2,..,v_n)^T\" eeimg=\"1\"/> </p><p>（m，n为序列的长度）</p><p>那Attention的一般形式可以被表述为【注1】：</p><p><img src=\"https://www.zhihu.com/equation?tex=Attention%28Q%2CK%2CV%29%3D%28a%28Q_1%2CK%29V%2Ca%28Q_2%2CK%29V%2C..a%28Q_m%2CK%29V%29\" alt=\"Attention(Q,K,V)=(a(Q_1,K)V,a(Q_2,K)V,..a(Q_m,K)V)\" eeimg=\"1\"/>    (1)</p><p>是不是巨乱，是不是蒙圈了，那是因为我把打分函数 <img src=\"https://www.zhihu.com/equation?tex=a%28%5Ccdot%29\" alt=\"a(\\cdot)\" eeimg=\"1\"/> 给抽象化了。下面就来说最常用的打分函数，在上一篇专栏文章里面的打分函数用的是加性函数（ <img src=\"https://www.zhihu.com/equation?tex=a%28q_i%2Ck_j%29%3Dsoftmax%28u_a%5E%7BT%7Dtanh%28w_qq_i%2Bw_kk_j%29%29\" alt=\"a(q_i,k_j)=softmax(u_a^{T}tanh(w_qq_i+w_kk_j))\" eeimg=\"1\"/> ），而在Transformer的论文中采用了乘性打分函数：</p><p><img src=\"https://www.zhihu.com/equation?tex=a%28q_i%2Ck_j%29%3Dsoftmax%28q_i%5Ccdot+k_j%29\" alt=\"a(q_i,k_j)=softmax(q_i\\cdot k_j)\" eeimg=\"1\"/> ,也就是 对<img src=\"https://www.zhihu.com/equation?tex=q_i\" alt=\"q_i\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=k_j\" alt=\"k_j\" eeimg=\"1\"/> 的内积计算softmax值。</p><p>此时（1）式可改写为：</p><p><img src=\"https://www.zhihu.com/equation?tex=Attention%28Q%2CK%2CV%29%3Dsoftmax%28QK%5E%7BT%7D%29V\" alt=\"Attention(Q,K,V)=softmax(QK^{T})V\" eeimg=\"1\"/>    (2)</p><p>同时文中做了一个小的<b>缩放操作</b>，原因是单纯的做q与k的内积会导致 <img src=\"https://www.zhihu.com/equation?tex=QK%5E%7BT%7D\" alt=\"QK^{T}\" eeimg=\"1\"/> 内的元素方差较大，以至于将softmax送到梯度较小的区域之内（我将试图在【注2】中对这件事进行解释），不利于模型参数的更新，所以将（2）式进行缩放操作，及在 <img src=\"https://www.zhihu.com/equation?tex=QK%5E%7BT%7D\" alt=\"QK^{T}\" eeimg=\"1\"/> 的基础上除以 <img src=\"https://www.zhihu.com/equation?tex=%5Csqrt%7Bd_k%7D\" alt=\"\\sqrt{d_k}\" eeimg=\"1\"/> ,（<img src=\"https://www.zhihu.com/equation?tex=d_k\" alt=\"d_k\" eeimg=\"1\"/>为q，k向量的维度 ）。最终形成论文提及的Attention形式：</p><p><img src=\"https://www.zhihu.com/equation?tex=Attention%28Q%2CK%2CV%29%3Dsoftmax%28%5Cfrac%7BQK%5E%7BT%7D%7D%7B%5Csqrt%7Bd_k%7D%7D%29V\" alt=\"Attention(Q,K,V)=softmax(\\frac{QK^{T}}{\\sqrt{d_k}})V\" eeimg=\"1\"/>    (3)</p><p>以下关于Attention的操作将根据（3）式来进行。</p><p class=\"ztext-empty-paragraph\"><br/></p><ul><li><b>self-Attentiion</b></li></ul><p>Transformer的核心思想就是用Attention来替代原来Encoder和Decoder部分的RNN，先前RNN的主要作用就是将当前时刻输入的词向量信息与前一时刻隐状态进行整合，形成该时刻的隐状态信息，换句话说，就是将词向量信息抽象为可供模型使用的高层次信息。那么如何<b>只利用Attention</b>来完成这个过程呢？</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-718ff9c4cb39f0e1e89171dd347c8a2b_b.jpg\" data-size=\"normal\" data-rawwidth=\"647\" data-rawheight=\"364\" class=\"origin_image zh-lightbox-thumb\" width=\"647\" data-original=\"https://pic4.zhimg.com/v2-718ff9c4cb39f0e1e89171dd347c8a2b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;647&#39; height=&#39;364&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"647\" data-rawheight=\"364\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"647\" data-original=\"https://pic4.zhimg.com/v2-718ff9c4cb39f0e1e89171dd347c8a2b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-718ff9c4cb39f0e1e89171dd347c8a2b_b.jpg\"/><figcaption>图1 self-Attention图示</figcaption></figure><p>还拿上篇文章的“我”“很”“帅”为例，由图1所示，对于“我”这个词来说，将其词向量作为query向量q,将&#34;我很帅&#34;及句子的起始和终止符号每个词的词向量都作为待对比的key，value向量取与key相同。设通过Attention操作后的向量为h,那么根据(3)式:</p><p><img src=\"https://www.zhihu.com/equation?tex=h%28%E6%88%91%29%3D+softmax%28%5Cfrac%7BE%28%E6%88%91%29++%5Cbegin%7Bpmatrix%7D+E%28%3Cs%3E%29%2C%26++E%28%3C%E6%88%91%3E%29%2C%26+E%28%3C%E5%BE%88%3E%29%2C%26+E%28%3C%E5%B8%85%3E%29%2C%26+E%28%3C%2Fs%3E%29%5Cend%7Bpmatrix%7D%7D%7B%5Csqrt%7Bd_k%7D%7D%29%5Cbegin%7Bpmatrix%7D+E%28%3Cs%3E%29%5C%5C+E%28%3C%E6%88%91%3E%29%5C%5C+E%28%3C%E5%BE%88%3E%29%5C%5CE%28%3C%E5%B8%85%3E%29%5C%5CE%28%3C%2Fs%3E%29%26%5Cend%7Bpmatrix%7D+%5C%5C%3Dsoftmax%28%5Cfrac%7BE%28%E6%88%91%29E%28%3Cs%3E%29%7D%7B%5Csqrt%7Bd_k%7D%7D%29E%28%3Cs%3E%29%2Bsoftmax%28%5Cfrac%7BE%28%E6%88%91%29E%28%3C%E6%88%91%3E%29%7D%7B%5Csqrt%7Bd_k%7D%7D%29E%28%3C%E6%88%91%3E%29%2Bsoftmax%28%5Cfrac%7BE%28%E6%88%91%29E%28%3C%E5%BE%88%3E%29%7D%7B%5Csqrt%7Bd_k%7D%7D%29E%28%3C%E5%BE%88%3E%29%5C%5C%2Bsoftmax%28%5Cfrac%7BE%28%E6%88%91%29E%28%3C%E5%B8%85%3E%29%7D%7B%5Csqrt%7Bd_k%7D%7D%29E%28%3C%E5%B8%85%3E%29%2Bsoftmax%28%5Cfrac%7BE%28%E6%88%91%29E%28%3C%2Fs%3E%29%7D%7B%5Csqrt%7Bd_k%7D%7D%29E%28%3C%2Fs%3E%29\" alt=\"h(我)= softmax(\\frac{E(我)  \\begin{pmatrix} E(&lt;s&gt;),&amp;  E(&lt;我&gt;),&amp; E(&lt;很&gt;),&amp; E(&lt;帅&gt;),&amp; E(&lt;/s&gt;)\\end{pmatrix}}{\\sqrt{d_k}})\\begin{pmatrix} E(&lt;s&gt;)\\\\ E(&lt;我&gt;)\\\\ E(&lt;很&gt;)\\\\E(&lt;帅&gt;)\\\\E(&lt;/s&gt;)&amp;\\end{pmatrix} \\\\=softmax(\\frac{E(我)E(&lt;s&gt;)}{\\sqrt{d_k}})E(&lt;s&gt;)+softmax(\\frac{E(我)E(&lt;我&gt;)}{\\sqrt{d_k}})E(&lt;我&gt;)+softmax(\\frac{E(我)E(&lt;很&gt;)}{\\sqrt{d_k}})E(&lt;很&gt;)\\\\+softmax(\\frac{E(我)E(&lt;帅&gt;)}{\\sqrt{d_k}})E(&lt;帅&gt;)+softmax(\\frac{E(我)E(&lt;/s&gt;)}{\\sqrt{d_k}})E(&lt;/s&gt;)\" eeimg=\"1\"/> </p><p>结合上式和图1可以看出，对“我”这个词整合成隐层信息的时候，是将“我”这个词与其所在句子中的每一个词进行比较，考查其在句子中的表达更注重哪些位置的信息，从而分配不同的权重给对应位置的词进行加权求和，这也符合Attention的基本思想。由于所有的词汇的Attention操作都是在其句子自身进行的，因此这种Attention机制被称为self-Attention(自注意力机制)。</p><p>Transformer将Encoder和Decoder的主要部分都替换成了这样的self-Attention，优点很明显，所有待处理的向量在矩阵运算下可以很好的并行处理，加快了以前Encoder和Decoder的效率。但还有一个问题待解决，那就是在RNN当中可以很好的保留词与词之间的序列顺序，现在通过self-Attention并行化以后，这个信息难以被描述出来，在Transformer论文原文中，引入了位置向量来标记每个词所在的位置信息。</p><h2><b>二、位置向量和多头注意力机制</b></h2><ul><li><b>位置向量</b></li></ul><p>在论文原文中，作者引入了位置向量PE(pos)，并与输入的词向量进行求和，之后输入对应的下一层，这就要求位置向量应该与词向量具有相同的维度 <img src=\"https://www.zhihu.com/equation?tex=d_%7Bmodel%7D\" alt=\"d_{model}\" eeimg=\"1\"/> ,并在奇偶性不同的分量位置上设计了不同的取值，如下：</p><p>在偶数位分量，有 <img src=\"https://www.zhihu.com/equation?tex=PE%28pos%2C2i%29%3Dsin%28%5Cfrac%7Bpos%7D%7B10000%5E%7B%5Cfrac%7B2i%7D%7Bd_%7Bmodel%7D%7D%7D%7D%29\" alt=\"PE(pos,2i)=sin(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})\" eeimg=\"1\"/> </p><p>在奇数位分量，有 <img src=\"https://www.zhihu.com/equation?tex=PE%28pos%2C2i%2B1%29%3Dcos%28%5Cfrac%7Bpos%7D%7B10000%5E%7B%5Cfrac%7B2i%7D%7Bd_%7Bmodel%7D%7D%7D%7D%29\" alt=\"PE(pos,2i+1)=cos(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})\" eeimg=\"1\"/></p><p>其中，pos为当前词语在句子中的位置(1,2,3,4....)，i为位置向量的分量位置</p><p>可能这看起来有点匪夷所思，但如果从位置的偏移角度来看，解释起来就相对合理了。假设我们要计算pos位置移动k位置的位置向量，也就是 <img src=\"https://www.zhihu.com/equation?tex=PE%28pos%2Bk%29\" alt=\"PE(pos+k)\" eeimg=\"1\"/> ,以偶数位分量为例，有：</p><p><img src=\"https://www.zhihu.com/equation?tex=PE%28pos%2Bk%2C2i%29%3Dsin%28%5Cfrac%7Bpos%2Bk%7D%7B10000%5E%7B%5Cfrac%7B2i%7D%7Bd_%7Bmodel%7D%7D%7D%7D%29\" alt=\"PE(pos+k,2i)=sin(\\frac{pos+k}{10000^{\\frac{2i}{d_{model}}}})\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%3Dsin%28%5Cfrac%7Bpos%7D%7B10000%5E%7B%5Cfrac%7B2i%7D%7Bd_%7Bmodel%7D%7D%7D%7D%29cos%28%5Cfrac%7Bk%7D%7B10000%5E%7B%5Cfrac%7B2i%7D%7Bd_%7Bmodel%7D%7D%7D%7D%29%2Bcos%28%5Cfrac%7Bpos%7D%7B10000%5E%7B%5Cfrac%7B2i%7D%7Bd_%7Bmodel%7D%7D%7D%7D%29sin%28%5Cfrac%7Bk%7D%7B10000%5E%7B%5Cfrac%7B2i%7D%7Bd_%7Bmodel%7D%7D%7D%7D%29+\" alt=\"=sin(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})cos(\\frac{k}{10000^{\\frac{2i}{d_{model}}}})+cos(\\frac{pos}{10000^{\\frac{2i}{d_{model}}}})sin(\\frac{k}{10000^{\\frac{2i}{d_{model}}}}) \" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%3DPE%28pos%2C2i%29PE%28k%2C2i%2B1%29%2BPE%28pos%2C2i%2B1%29PE%28k%2C2i%29\" alt=\"=PE(pos,2i)PE(k,2i+1)+PE(pos,2i+1)PE(k,2i)\" eeimg=\"1\"/> </p><p>可见，在pos位置偏移k个位置以后的位置向量可以表示为pos位置向量的线性组合，使得位置的相对信息有了一定体现。</p><ul><li><b>多头注意力机制</b></li></ul><p>在引入了位置向量PE(pos)以后,我们将位置向量与词向量进行求和，输入到self-Attention中进行抽象信息的整合。多数情况下，单一的self-Attention难以捕获序列信息的多样性。因此考虑多个相同操作的self-Attention<b>平行</b>的提取每个词语的信息，然后再将多个Attention的结果拼接起来，用于后续层次的操作。这点<b>类似于卷积神经网络中多个卷积核同时作用一个矩阵对象的想法</b>，都是试图取实现信息不同角度的多样化采集。</p><p><b>三、其余模型细节</b></p><p>在介绍完Transformer的主要模型构件以后，就可以祭出各种博客里粘了多少次的图，由图2所示。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-8fc1dc24ef7273f50805ff4d3ee6d20a_b.jpg\" data-size=\"normal\" data-rawwidth=\"1009\" data-rawheight=\"787\" class=\"origin_image zh-lightbox-thumb\" width=\"1009\" data-original=\"https://pic3.zhimg.com/v2-8fc1dc24ef7273f50805ff4d3ee6d20a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1009&#39; height=&#39;787&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1009\" data-rawheight=\"787\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1009\" data-original=\"https://pic3.zhimg.com/v2-8fc1dc24ef7273f50805ff4d3ee6d20a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-8fc1dc24ef7273f50805ff4d3ee6d20a_b.jpg\"/><figcaption>图2 Transformer示意</figcaption></figure><p>图2中除了我们熟悉的全连接层和softmax层以外，红色箭头指出的部分我们都已经有了相对详细的介绍。而蓝色箭头的部分则是模型的一些细节处理，使得<b>模型加速收敛及保持序列的有序性。</b></p><ul><li><b>残差边与Layer-Normalization</b></li></ul><p>残差边（short-cut）在图像的resnet中已经被广泛应用了，它的出现是为了防止网络层数的加深导致网络内的参数退化。其想法是将低层的特征跃过一些网络层直接送进高层网络，这就保证了网络的最差情况也能学到那个低层特征本身。原因是过去的网络是计算 <img src=\"https://www.zhihu.com/equation?tex=l\" alt=\"l\" eeimg=\"1\"/> 层输入 <img src=\"https://www.zhihu.com/equation?tex=x%28l%29\" alt=\"x(l)\" eeimg=\"1\"/> 的线性与非线性的变换 <img src=\"https://www.zhihu.com/equation?tex=z%5E%7B%28l%2B1%29%7D+%3D+%5Csigma%28wx%5E%7B%28l%29%7D%2Bb%29\" alt=\"z^{(l+1)} = \\sigma(wx^{(l)}+b)\" eeimg=\"1\"/> ,而残差边的思想是将低层的（比如 <img src=\"https://www.zhihu.com/equation?tex=l-1\" alt=\"l-1\" eeimg=\"1\"/> 层 <img src=\"https://www.zhihu.com/equation?tex=x%5E%7B%28l-1%29%7D\" alt=\"x^{(l-1)}\" eeimg=\"1\"/> )）特征直接送入 <img src=\"https://www.zhihu.com/equation?tex=l\" alt=\"l\" eeimg=\"1\"/> 层参与运算<img src=\"https://www.zhihu.com/equation?tex=z%5E%7B%28l%2B1%29%7D+%3D+%5Csigma%28wx%5E%7B%28l%29%7D%2Bb%2Bx%5E%7B%28l-1%29%7D%29\" alt=\"z^{(l+1)} = \\sigma(wx^{(l)}+b+x^{(l-1)})\" eeimg=\"1\"/> ,这样一来，即使参数w，b都退化为0，也还能留下低层特征的信息送到高层，不至于让信息在深层网络传递的过程中丢失过多。</p><p>另一个操作Layer-Normalization（LN）则是起到与Batch-Normalization（BN）类似的作用，BN是指在批次这个维度来标准化数据，使得其落在梯度适中的区域。LN则是在某一层输出时对所有的神经元的值做标准化，好处是不用依赖Batch大小的设置，也能起到一定的加速收敛的作用（其实LN的理解我还不是很到位，以后理解的更好时会更新这一块内容）。</p><ul><li><b>Masking操作</b></li></ul><p>masking操作在论文原文中介绍的相对简洁，作者认为序列在解码的过程中，对于 <img src=\"https://www.zhihu.com/equation?tex=i\" alt=\"i\" eeimg=\"1\"/> 时刻的解码，只能依赖到 <img src=\"https://www.zhihu.com/equation?tex=i\" alt=\"i\" eeimg=\"1\"/> 之前时刻的词语信息，以保证解码过程中的有序性。因此要对 <img src=\"https://www.zhihu.com/equation?tex=i\" alt=\"i\" eeimg=\"1\"/> 时刻后的信息进行masking操作。关于masking的具体做法，主要是设置一个与序列长度相同维度的mask向量，将其第 <img src=\"https://www.zhihu.com/equation?tex=i\" alt=\"i\" eeimg=\"1\"/> 个分量以后的分量全部置为0，第 <img src=\"https://www.zhihu.com/equation?tex=i\" alt=\"i\" eeimg=\"1\"/> 个分量以前的分量全部置为1。用于标记解码序列哪些在网络中的计算是有效的[2]。</p><p>至此，Transformer的基本内容就全都介绍完了。这个模型框架只用了Attention的思想，在执行效率以及一些任务效果上较以前的RNN都有了一定的提高。目前在自然语言处理领域已经有了相对广泛的应用，比如最新的BERT模型，听说就是基于Transformer提出的，后续我也将会补充总结。</p><p>那么说有了“又快又好”的Transformer以后，RNN系列的模型是不是可以从此摒弃？说不好，只能看后续的发展了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>感谢你的阅读。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>【注1】这个形式在论文原文中并未提到，是我在加性Attention和乘性Attention的基础上归纳出来的，式(1)其中的内涵是Q矩阵内的每个query向量都要与K矩阵去做一个比较打分，因为对于单独的query向量来说，要遍历K的每一个key去比较。比较后得到一个权重的分布，以 <img src=\"https://www.zhihu.com/equation?tex=Q_1\" alt=\"Q_1\" eeimg=\"1\"/> 为例，也就是 :<img src=\"https://www.zhihu.com/equation?tex=a%28Q_1%2CK%29%3D%28softmax%28a%28q_1%2Ck_1%29%29%2Csoftmax%28a%28q_1%2Ck_2%29%29%2C...%2Csoftmax%28a%28q_1%2Ck_n%29%29%29\" alt=\"a(Q_1,K)=(softmax(a(q_1,k_1)),softmax(a(q_1,k_2)),...,softmax(a(q_1,k_n)))\" eeimg=\"1\"/> </p><p>是个n维的行向量，与后面的n维列向量相乘代表着对V中的每个value赋予权重进而加权求和。</p><p>【注2】对于缩放因子这个事，论文原文给了一些解释。假设Q，K中的每一个 <img src=\"https://www.zhihu.com/equation?tex=d_%7Bk%7D\" alt=\"d_{k}\" eeimg=\"1\"/> 维向量是q，k。q，k中的每个分量独立同分布于均值为0，方差为1的随机变量。那么 <img src=\"https://www.zhihu.com/equation?tex=q%5Ccdot+k\" alt=\"q\\cdot k\" eeimg=\"1\"/> 就将服从于均值为0，方差为 <img src=\"https://www.zhihu.com/equation?tex=d_k\" alt=\"d_k\" eeimg=\"1\"/> 的随机变量。这就在作为 <img src=\"https://www.zhihu.com/equation?tex=d_%7Bk%7D\" alt=\"d_{k}\" eeimg=\"1\"/> 的方差很大的时候，导致对于固定的 <img src=\"https://www.zhihu.com/equation?tex=q_%7Bi%7D\" alt=\"q_{i}\" eeimg=\"1\"/> 来说，其与每个 <img src=\"https://www.zhihu.com/equation?tex=k_%7Bj%7D\" alt=\"k_{j}\" eeimg=\"1\"/> 的内积将会差异很大。那么在进行softmax运算的时候，就会呈现要么趋向于0，要么趋向于1的情况（这根据softmax的计算式可以验证）。这个时候如果再更新Attention中某个待估参数w，其梯度计算大概是:</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+L%7D%7B%5Cpartial+w%7D+%3D+%5Cfrac%7B%5Cpartial+L%7D%7B%5Cpartial+m_1%7D%7C_%7B%28q%2Ck%29%3D%28q_%7Bi%7D%2Ck_%7Bj%7D%29%7D%5Ccdot%5Cfrac%7B%5Cpartial+m_1%7D%7B%5Cpartial+softmax%7D%7C_%7B%28q%2Ck%29%3D%28q_%7Bi%7D%2Ck_%7Bj%7D%29%7D%5Ccdot%5Ccdot%5Ccdot%5Cfrac%7B%5Cpartial+softmax%7D%7B%5Cpartial+w%7D%7C_%7B%28q%2Ck%29%3D%28q_%7Bi%7D%2Ck_%7Bj%7D%29%7D\" alt=\"\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial m_1}|_{(q,k)=(q_{i},k_{j})}\\cdot\\frac{\\partial m_1}{\\partial softmax}|_{(q,k)=(q_{i},k_{j})}\\cdot\\cdot\\cdot\\frac{\\partial softmax}{\\partial w}|_{(q,k)=(q_{i},k_{j})}\" eeimg=\"1\"/> </p><p>其中 <img src=\"https://www.zhihu.com/equation?tex=m_%7B1%7D\" alt=\"m_{1}\" eeimg=\"1\"/>是反向传播中的某个中间变量， <img src=\"https://www.zhihu.com/equation?tex=L\" alt=\"L\" eeimg=\"1\"/> 为模型优化的目标函数。</p><p>由于 <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+softmax%7D%7B%5Cpartial+w%7D%7C_%7B%28q%2Ck%29%3D%28q_%7Bi%7D%2Ck_%7Bj%7D%29%7D\" alt=\"\\frac{\\partial softmax}{\\partial w}|_{(q,k)=(q_{i},k_{j})}\" eeimg=\"1\"/> 这一项在这种情况下基本趋于0(类比sigmoid函数比较水平的那段曲线)，因此导致整体梯度的更新处于一个较小的区域，这才引入了缩放因子以消除大方差的影响。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>参考文献</p><p>[1] <a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1706.03762.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Vaswani A , Shazeer N , Parmar N , et al. Attention Is All You Need[J]. 2017.</a></p><p>[2] <a href=\"https://link.zhihu.com/?target=https%3A//www.quora.com/What-is-masking-in-a-recurrent-neural-network-RNN\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">What is masking in a recurrent neural network (RNN)?</a></p><p></p><p></p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "自然语言处理", 
                    "tagLink": "https://api.zhihu.com/topics/19560026"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": [
                {
                    "userName": "silver bullet", 
                    "userLink": "https://www.zhihu.com/people/d3ec52cfeef2e4a171131d9f84847dec", 
                    "content": "但有一个疑问是偶数位分量如何得到PE(pos,2i+1)呢？感觉这个是奇数位置才有的元素？向量线性变换的话不同奇偶位置感觉加不到一块？还是我理解的有些偏差呢？谢谢😊", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "KIRA", 
                            "userLink": "https://www.zhihu.com/people/e9aa8c6a879e9c02311dc5f5111cbdcd", 
                            "content": "<p>在计算PE(pos+k,2i)的时候，可以认为PE(pos)各个分量是已知的，因为此时我们关注的是pos+k的位置向量，即未知的是pos+k的奇数位分量</p>", 
                            "likes": 0, 
                            "replyToAuthor": "silver bullet"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/59521772", 
            "userName": "leo666", 
            "userLink": "https://www.zhihu.com/people/a22bfdaa2fdedff01a8e865b81a8e221", 
            "upvote": 138, 
            "title": "医学图像领域的GANs", 
            "content": "<p><b>这是一篇关于GANs在医学图像领域的总结，如果各位大佬也在做医学图像分割/分类，可以mark交流一下。</b></p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>Introduction</b></p><p>Goodfellow等人引入了<b>生成对抗网络(GANs)来模拟数据分布</b>。GANs能合成真实图像，其原因与两个基本属性有关。</p><ol><li> GANs是一种无监督的训练方法，可以<b>通过数据获取信息，其方式类似于人类学习图像特征的方式</b>。</li></ol><p>2. GANs通过<b>发现数据潜在的高维分布</b>，在特征提取方面有很好的表现。</p><p>本文综述了医学图像处理应用中提出的基于GAN的结构，包括<b>去噪、重构、分割、检测、分类和图像合成</b>。论文分布情况如图1所示。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-e7549fd2b3d08cfd6284505a9351c508_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"824\" data-rawheight=\"531\" class=\"origin_image zh-lightbox-thumb\" width=\"824\" data-original=\"https://pic1.zhimg.com/v2-e7549fd2b3d08cfd6284505a9351c508_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;824&#39; height=&#39;531&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"824\" data-rawheight=\"531\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"824\" data-original=\"https://pic1.zhimg.com/v2-e7549fd2b3d08cfd6284505a9351c508_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-e7549fd2b3d08cfd6284505a9351c508_b.jpg\"/></figure><p>本文最终得到了63篇论文，涵盖了各种GANs。在第3节中，介绍了用于医学图像应用的GAN及其子类的体系结构。第4节描述了GANs在医学图像处理应用中的不同贡献(去噪、重建、分割、检测、分类和合成)，第5节给出了将GANs用于医学图像处理的研究方法、挑战和未来方向。</p><p><b>监督深度学习</b>是目前许多计算机视觉和医学图像分析任务中最先进的技术。然而，其主要限制因素，是它<b>依赖于大量带注释的训练数据</b>。在医学领域，这一点尤为重要，因为<b>医学图像的获取和标注需要专家，导致标签训练数据严重缺乏</b>。</p><p>另一个机器学习的问题是，对于一般任务(如超分辨率、分割或图像到图像转换)，<b>必须手工设计相似性度量</b>。传统的相似性包含像素级的损失，如L1和L2距离，两者都会模糊结果，缺乏上下文的整合。GANs的对抗式训练通过<b>学习丰富的相似性度量来区分真假数据</b>从理论上消除了对显式像素级目标函数建模的需要。这一特性最近被用于改进医学<b>图像分割、图像增强</b>(如去噪)以及使用基于GANs的图像到图像转换技术<b>解决医学图像domain shift的问题</b>。</p><p><b>domain shift现象</b>实际上是目前限制深度学习模型泛化能力的另一个主要问题。假设训练数据(training data)和推理数据（inference data）来自相同的分布，因此训练的模型也应该在不可见的数据(unseen data)上正常工作，这种假设往往不成立，并限制了模型的适用性。Domain Adaptation是指使模型对这种d<b>omain shift</b>具有鲁棒性，而对抗性训练具有很大的潜力。</p><p><b>概述</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-e1b2e0aeb1bc76ab15fecd7f8853c27e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"555\" data-rawheight=\"116\" class=\"origin_image zh-lightbox-thumb\" width=\"555\" data-original=\"https://pic3.zhimg.com/v2-e1b2e0aeb1bc76ab15fecd7f8853c27e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;555&#39; height=&#39;116&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"555\" data-rawheight=\"116\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"555\" data-original=\"https://pic3.zhimg.com/v2-e1b2e0aeb1bc76ab15fecd7f8853c27e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-e1b2e0aeb1bc76ab15fecd7f8853c27e_b.jpg\"/></figure><p>GAN框架由生成器(G)、鉴别器(D)以及真实数据X的训练数据集组成。G生成器,是一个多层网络参数θG,旨在找到一种映射x = G(z,θG)。通过映射,G生成假数据。另一方面,鉴别器D(x;θD)旨在把假样本和真实数据区分开来。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-eb42fd05afac7cc66aa3c8718056c5b8_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"768\" data-rawheight=\"53\" class=\"origin_image zh-lightbox-thumb\" width=\"768\" data-original=\"https://pic1.zhimg.com/v2-eb42fd05afac7cc66aa3c8718056c5b8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;768&#39; height=&#39;53&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"768\" data-rawheight=\"53\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"768\" data-original=\"https://pic1.zhimg.com/v2-eb42fd05afac7cc66aa3c8718056c5b8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-eb42fd05afac7cc66aa3c8718056c5b8_b.png\"/></figure><p>GAN的主要优点是通过关注<b>数据的潜在概率密度，找到模型的数据分布</b>。虽然GANs与CNNs相比具有这样的内在优势，但也存在一些挑战:</p><ul><li>1) 崩溃问题（mode collapse）:当G崩溃时，将<b>所有不同的输入映射到相同的数据;</b>  </li><li>2)不稳定性:导致<b>相同输入产生不同的输出</b>。这些现象的主要原因与优化过程中<b>梯度消失有关</b>。</li></ul><p>虽然批处理归一化(<b>batch normalization</b>)是解决GAN不稳定性的一种方法，但它不足以使GAN的性能达到最优稳定性。因此，已经引入了许多GANs的子类来解决这些缺陷。部分框架如图所示：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-7e8650c8a23f4907668b13cf1def2a6a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"610\" data-rawheight=\"629\" class=\"origin_image zh-lightbox-thumb\" width=\"610\" data-original=\"https://pic3.zhimg.com/v2-7e8650c8a23f4907668b13cf1def2a6a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;610&#39; height=&#39;629&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"610\" data-rawheight=\"629\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"610\" data-original=\"https://pic3.zhimg.com/v2-7e8650c8a23f4907668b13cf1def2a6a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-7e8650c8a23f4907668b13cf1def2a6a_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-aecc8d4ef2b103dac1732ba5e4f11f59_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"809\" data-rawheight=\"655\" class=\"origin_image zh-lightbox-thumb\" width=\"809\" data-original=\"https://pic2.zhimg.com/v2-aecc8d4ef2b103dac1732ba5e4f11f59_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;809&#39; height=&#39;655&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"809\" data-rawheight=\"655\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"809\" data-original=\"https://pic2.zhimg.com/v2-aecc8d4ef2b103dac1732ba5e4f11f59_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-aecc8d4ef2b103dac1732ba5e4f11f59_b.jpg\"/></figure><h2><b>在医学图像处理中的应用</b></h2><p><b>1 去噪</b></p><p>由于过度辐射危害健康，<b>降低辐射剂量</b>已被作为一种有效的解决方案。然而，剂量减少<b>会<u>增加医学图像的噪声水平</u></b>，这会导致一些信息的丢失。目前基于<b><u>CNN</u></b>的去噪方法的主要问题是在优化中<b><u>使用均方误差,导致预测图像模糊</u></b>，无法提供常规剂量图像的纹理质量。<b><u>GAN可以通过检测噪声图像和去噪图像之间的映射来消除这个问题，并生成图像。</u></b></p><p>表1总结了主要的基于GAN的去噪方法。通过控制loss function来考虑更多的纹理特征，实现了良好的医学图像降噪性能。然而，寻找一个快速、准确、更稳定的体系结构是未来工作的一个开放方向。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-e090d19d1a35dcd1633c24ae4a4ad9b1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"865\" data-rawheight=\"535\" class=\"origin_image zh-lightbox-thumb\" width=\"865\" data-original=\"https://pic2.zhimg.com/v2-e090d19d1a35dcd1633c24ae4a4ad9b1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;865&#39; height=&#39;535&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"865\" data-rawheight=\"535\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"865\" data-original=\"https://pic2.zhimg.com/v2-e090d19d1a35dcd1633c24ae4a4ad9b1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-e090d19d1a35dcd1633c24ae4a4ad9b1_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>2重建</b></p><p><b>对丢失的图像数据进行重建</b>，可以在诊断过程中起到有效的作用。由于GAN<b>在数据合成方面的良好性能</b>，使其具有相当大的潜力。在一些医学影像中，如<b>磁共振成像(MRI)，需要较长的获取时间，患者的无意识(即由于呼吸)和自主(即由于不舒适的情况)运动是非常常见的。这些运动导致图像中器官的一些关键信息丢失。基于GAN的方法则试图在不完全(零填充)和完全采样的MR图像之间找到映射。</b></p><p>表2和表3总结了一些GANs的特性和性能。在医学图像的重建中，GANs似乎可以提供很好的性能，在损失函数中加入一些操作，突出纹理细节和特殊特征。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-78d662a0f1e8b24d75a90f8d8eff614d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"853\" data-rawheight=\"536\" class=\"origin_image zh-lightbox-thumb\" width=\"853\" data-original=\"https://pic2.zhimg.com/v2-78d662a0f1e8b24d75a90f8d8eff614d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;853&#39; height=&#39;536&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"853\" data-rawheight=\"536\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"853\" data-original=\"https://pic2.zhimg.com/v2-78d662a0f1e8b24d75a90f8d8eff614d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-78d662a0f1e8b24d75a90f8d8eff614d_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-39421931dd7433d91d54a1d7305aef4f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"887\" data-rawheight=\"648\" class=\"origin_image zh-lightbox-thumb\" width=\"887\" data-original=\"https://pic4.zhimg.com/v2-39421931dd7433d91d54a1d7305aef4f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;887&#39; height=&#39;648&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"887\" data-rawheight=\"648\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"887\" data-original=\"https://pic4.zhimg.com/v2-39421931dd7433d91d54a1d7305aef4f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-39421931dd7433d91d54a1d7305aef4f_b.jpg\"/></figure><p><b>3 分割</b></p><p>医学图像处理中<b>物体和器官的标注在<u>异常检测和形状识别</u>中起着重要作用</b>。此外，分割被定义为许多其他任务的预处理步骤，如检测和分类。因此，自动分割引起了大量研究者的关注，近几十年来，自动分割是医学图像处理中应用深度学习最常见的课题。</p><p>一般来说，<b>基于CNN的分割方法利用像素丢失来学习像素之间的局部和全局关系是不够的。</b>所以需要统计建模方法，如<b>条件随机场</b>或<b>统计形状模型</b>来修正他们的结果。虽然已经提出了一些基于patch的CNN方法来解决这个问题，但是这些方法需要在准确性和patch大小之间进行权衡。人们又提出了<b>一种基于U-Net的基于加权交叉熵损失的体系结构，但这些方法都面临着weights优化问题。</b>所以除了加权损失外，还需要一般性损失来解决这个问题。GANs在医学图像分割主要在 大脑,胸部,眼睛,腹部, 显微图像, 心动, 脊柱。表5至10总结了基于GAN的分割方法。从已知的DNN架构来看，U-Net和ResNet 由于提供通用的识别功能，是最常用的网络，可用作基于GAN的分段模型中的生成器。</p><p>大脑</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-a56f41387ecb600a0b839d4327d43cd1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"860\" data-rawheight=\"736\" class=\"origin_image zh-lightbox-thumb\" width=\"860\" data-original=\"https://pic2.zhimg.com/v2-a56f41387ecb600a0b839d4327d43cd1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;860&#39; height=&#39;736&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"860\" data-rawheight=\"736\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"860\" data-original=\"https://pic2.zhimg.com/v2-a56f41387ecb600a0b839d4327d43cd1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-a56f41387ecb600a0b839d4327d43cd1_b.jpg\"/></figure><p><b>胸部</b></p><p>胸部x线图像分割的主要障碍是图像质量差、局部伪影和心肺重叠。Dai等人提出了一种基于GAN的解决方案(SCAN)，增强分割的全局一致性，提取心脏和左/右肺的轮廓。这项工作的主要贡献是使用一个完全连接的网络与VGG下采样路径使用更少的特征映射。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-c962022756235005185f3326ac577968_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"750\" data-rawheight=\"256\" class=\"origin_image zh-lightbox-thumb\" width=\"750\" data-original=\"https://pic1.zhimg.com/v2-c962022756235005185f3326ac577968_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;750&#39; height=&#39;256&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"750\" data-rawheight=\"256\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"750\" data-original=\"https://pic1.zhimg.com/v2-c962022756235005185f3326ac577968_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-c962022756235005185f3326ac577968_b.jpg\"/></figure><p><b>眼睛</b></p><p>在视网膜血管分割中，许多基于CNN的方法甚至比人类专家的表现更好。Son等人将CNN替换为遵循生成器的U-Net架构的GAN。在两个数据集上的实验结果表明，利用传统的鉴别器可以获得最佳的性能，甚至优于人类专家的注释。</p><p>Lahiri et al.提出了一种基于DC-GAN的分割方法，将RoI patch从背景中分割出来。类似的CNN需要大量的训练数据才能很好地表现，而提出的结构使用九分之一的训练数据就可以达到类似的性能。</p><p>Shankaranarayana等人提出利用cGAN网络对二维彩色眼底图像进行分割。生成器是一个由对抗损耗和L1损耗构成的网络。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-22d76192f563ca4bc4677c28c5b28122_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"837\" data-rawheight=\"513\" class=\"origin_image zh-lightbox-thumb\" width=\"837\" data-original=\"https://pic3.zhimg.com/v2-22d76192f563ca4bc4677c28c5b28122_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;837&#39; height=&#39;513&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"837\" data-rawheight=\"513\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"837\" data-original=\"https://pic3.zhimg.com/v2-22d76192f563ca4bc4677c28c5b28122_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-22d76192f563ca4bc4677c28c5b28122_b.jpg\"/></figure><p><b>腹部</b></p><p>腹部MRI图像中脾脏大小和形状的不同，导致了CNN深度分割方法的错误标记。GANs模型可以解决这一问题。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-adc7aaf0b73bbe876f9192c17c54a8ec_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"844\" data-rawheight=\"434\" class=\"origin_image zh-lightbox-thumb\" width=\"844\" data-original=\"https://pic1.zhimg.com/v2-adc7aaf0b73bbe876f9192c17c54a8ec_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;844&#39; height=&#39;434&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"844\" data-rawheight=\"434\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"844\" data-original=\"https://pic1.zhimg.com/v2-adc7aaf0b73bbe876f9192c17c54a8ec_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-adc7aaf0b73bbe876f9192c17c54a8ec_b.jpg\"/></figure><p><b>显微图像</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-eda99ba7a7154376fc00836b2b7e5ed3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"857\" data-rawheight=\"548\" class=\"origin_image zh-lightbox-thumb\" width=\"857\" data-original=\"https://pic4.zhimg.com/v2-eda99ba7a7154376fc00836b2b7e5ed3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;857&#39; height=&#39;548&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"857\" data-rawheight=\"548\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"857\" data-original=\"https://pic4.zhimg.com/v2-eda99ba7a7154376fc00836b2b7e5ed3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-eda99ba7a7154376fc00836b2b7e5ed3_b.jpg\"/></figure><p><b>脊柱</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-f903b0b61a9ce732fcb0b49f270c32c1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"737\" data-rawheight=\"203\" class=\"origin_image zh-lightbox-thumb\" width=\"737\" data-original=\"https://pic2.zhimg.com/v2-f903b0b61a9ce732fcb0b49f270c32c1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;737&#39; height=&#39;203&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"737\" data-rawheight=\"203\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"737\" data-original=\"https://pic2.zhimg.com/v2-f903b0b61a9ce732fcb0b49f270c32c1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-f903b0b61a9ce732fcb0b49f270c32c1_b.jpg\"/></figure><p><b>4 检测</b></p><p>在医学诊断中，许多疾病标记物被称为异常。然而，来自图像的<b>异常的计算检测需要大量的监督训练数据</b>。即使有如此庞大的数据，也无法保证学习型网络能够检测到看不见的情况。</p><p>与先前的应用相比，GAN在异常检测中提出的论文具有更多的结构复杂性，因为它们受益于GAN的不同方面。事实上，鉴别器的作用在实践中更为突出。此外，提取的map定义了识别健康和异常图像的潜在方面，以更感性的方式使用。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-bbff5de8baf083b9e9b7cd8890d38963_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"827\" data-rawheight=\"731\" class=\"origin_image zh-lightbox-thumb\" width=\"827\" data-original=\"https://pic4.zhimg.com/v2-bbff5de8baf083b9e9b7cd8890d38963_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;827&#39; height=&#39;731&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"827\" data-rawheight=\"731\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"827\" data-original=\"https://pic4.zhimg.com/v2-bbff5de8baf083b9e9b7cd8890d38963_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-bbff5de8baf083b9e9b7cd8890d38963_b.jpg\"/></figure><p><b>5分类</b></p><p>由于在<b>心脏超声（US）成像期间发生心脏和呼吸运动，所得到的图像可能显示不完整的信息</b>，如心脏的基底和心尖切片，这是识别左心室（LV）解剖结构的关键特征。因此，需要一个自动系统来完成缺失的部分或丢弃信息不完整的图像，这可能会误导分类过程。</p><p>张等人提出半耦合GAN（SCGAN）来分类有缺失基底切片的有用心脏图像，如下图所示。结果表明，与CNN方法相比，该方法具有更高的精度，降低了计算成本。此外，SCGAN还提高了对抗训练的稳健性。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-daaef7fbe782be35028a0bfeea9298e5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"625\" data-rawheight=\"257\" class=\"origin_image zh-lightbox-thumb\" width=\"625\" data-original=\"https://pic2.zhimg.com/v2-daaef7fbe782be35028a0bfeea9298e5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;625&#39; height=&#39;257&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"625\" data-rawheight=\"257\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"625\" data-original=\"https://pic2.zhimg.com/v2-daaef7fbe782be35028a0bfeea9298e5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-daaef7fbe782be35028a0bfeea9298e5_b.jpg\"/></figure><p><b>6 合成</b></p><p>最初，GAN被提议作为<b>完全无监督的生成框架，其目标是在训练数据分布之后将随机噪声映射到真实的图像。</b>利用conditional GAN，成功地转变为监督生成框架。本文将原始GAN框架称为无条件或无监督GAN，与conditional GAN相反。要强调的是，区分这些不同的概念对文献进行相应的分类是非常重要的。</p><p>两种框架的属性被利用来合成某些类型的医学图像，这些图像既可以来自单独的噪声，也可以来自先前的知识（参见条件图像合成），例如元数据甚至是用于映射的图像数据。从一种形态到另一种形态的图像。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>Discussion</b></p><p>1  GAN在医疗领域的优势</p><p>基于GAN的深度生成模型能够产生逼真的图像，在医疗图像独有的两个挑战中，Gan具有得天独厚的优势：</p><ul><li><b>标注的稀缺性</b>：通常，标注在医学图像中too expensive且难以获得。针对此类问题的基于监督学习的深度神经网络具有挑战性。正如合成和转化中的多项研究所证明的那样，GAN可以利用这两个即将到来的框架</li><li><b>不成对的数据：</b>找到正确的数据（按像素或按区域）是极具挑战性的。GAN框架十分强大，例如cycle GAN从不成对的训练图像中学习独特的模式并产生逼真的输出。</li></ul><p><b>2 缺点</b></p><p>本文确定当前形式的GAN中可能阻碍其在医学界发展的三个主要缺点：</p><ul><li><b>合成数据的可信度：</b>基本网络 - 发生器和鉴别器仍然是深度神经网络，其机制尚未得到很好的研究。在医学图像中，强度通常与某些含义相关，例如，可以基于CT数据的HU大致分类组织类型。目前GAN重建中缺少这种关联和映射，这一缺点足以让临床医生不信任GAN合成的图像。</li><li><b>不稳定的训练：许多文献指出GAN训练的数值是不稳定的</b>。这会导致mode cllaspe等情况。state of the art的工作在真实图像的GAN训练中着重于解决这些数值不稳定问题。然而，在医学成像中，图像模式本身不清晰，如何识别这样的问题尚不清楚。</li><li><b>评估指标：</b>评估reconstruction结果的最佳方法仍不清楚。在医学成像中，研究人员主要依靠传统指标（<b>如PSNR或MSE</b>）来评估GAN重建质量。然而这种指标的缺点是人们选择GAN的主要原因。</li></ul><p><b>3 未来展望</b></p><p>   本文认为GAN需要解决上面提到的重大缺陷，然后才能成为医疗保健领域值得信赖的技术。</p><p>   还需要解决训练不稳定性问题，这意味着需要严格的实验来理解GAN在医学成像环境中的收敛。关于度量的问题非常棘手，临床医生理解临床医师在CAD中合成GAN图像的性能是必要的第一步。简而言之GAN在未来几年内开辟了许多可能的研究问题。正确理解和回答那些是在真实临床情景中成功部署GAN的关键。</p><p>参考文献：<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1809.06222\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">GANs for Medical Image Analysis</a></p><p></p><p></p><p></p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "生成对抗网络（GAN）", 
                    "tagLink": "https://api.zhihu.com/topics/20070859"
                }, 
                {
                    "tag": "计算机视觉", 
                    "tagLink": "https://api.zhihu.com/topics/19590195"
                }
            ], 
            "comments": [
                {
                    "userName": "匿各用户", 
                    "userLink": "https://www.zhihu.com/people/d67ee185c975baea76cf8d401876f9d3", 
                    "content": "<p>感觉AI进入MIA之后，这个领域的同质化更严重了，原来在特征工程大旗下，大家还有各自的工具偏好，每一个组玩一个自己习惯或喜欢的东西，大家的差异虽然不大，但还是有风格不同，现在大家玩的工具都一样了，风格也趋同了。不知道这算大家共识找到好方法了呢，还是创造性多样性降低了？</p>", 
                    "likes": 1, 
                    "childComments": [
                        {
                            "userName": "leo666", 
                            "userLink": "https://www.zhihu.com/people/a22bfdaa2fdedff01a8e865b81a8e221", 
                            "content": "大佬言之有理，抓住了本跟质，望尘莫及", 
                            "likes": 0, 
                            "replyToAuthor": "匿各用户"
                        }
                    ]
                }, 
                {
                    "userName": "打针", 
                    "userLink": "https://www.zhihu.com/people/c275bcd56cca3d1f9e474b704a33a6b3", 
                    "content": "<p>感觉接下来医学图像AI，几个方向，弱监督/半监督，GAN，迁移，都是要围绕着解决标注不足的问题。</p>", 
                    "likes": 1, 
                    "childComments": [
                        {
                            "userName": "leo666", 
                            "userLink": "https://www.zhihu.com/people/a22bfdaa2fdedff01a8e865b81a8e221", 
                            "content": "标注确实是医学图像一个大problem", 
                            "likes": 0, 
                            "replyToAuthor": "打针"
                        }, 
                        {
                            "userName": "匿各用户", 
                            "userLink": "https://www.zhihu.com/people/d67ee185c975baea76cf8d401876f9d3", 
                            "content": "<p>还有一个是精度和可靠性问题，哪怕是传统图像处理，MIA的精度可靠度要求也比普通IA要求高，现在依然如此，有时候需要精确到subpixel，这个在数据不足的条件下矛盾更突出了。</p>", 
                            "likes": 2, 
                            "replyToAuthor": "打针"
                        }
                    ]
                }, 
                {
                    "userName": "Meepo", 
                    "userLink": "https://www.zhihu.com/people/45ae9f04164b39b9a8c0201f00b7405f", 
                    "content": "赞，以前看了好久分割网络为啥加一个判别器。，都不能找到一个恰当的解释。", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "leo666", 
                            "userLink": "https://www.zhihu.com/people/a22bfdaa2fdedff01a8e865b81a8e221", 
                            "content": "哈哈，我也看到类似分割网络了", 
                            "likes": 0, 
                            "replyToAuthor": "Meepo"
                        }
                    ]
                }, 
                {
                    "userName": "董秋迪", 
                    "userLink": "https://www.zhihu.com/people/32fb51fd9223b5cf0e14db7c0e081f27", 
                    "content": "再提一个。医学图像的重建也可以用到。", 
                    "likes": 1, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/59232702", 
            "userName": "leo666", 
            "userLink": "https://www.zhihu.com/people/a22bfdaa2fdedff01a8e865b81a8e221", 
            "upvote": 8, 
            "title": "特征值分解 和 SVD分解", 
            "content": "<h2><b>特征值</b></h2><ol><li>特征向量的代数上含义是：将矩阵乘法转换为数乘操作；</li><li>特征向量的几何含义是：特征向量通过方阵A变换只进行伸缩，而保持特征向量的方向不变。</li></ol><p>特征值表示的是<b>这个特征到底有多重要，类似于权重</b>，而<b>特征向量在几何上就是一个点，从原点到该点的方向表示向量的方向</b>。</p><p>一个变换方阵的<b>所有特征向量组成了这个变换矩阵的一组基</b>。所谓基，可以理解为坐标系的轴。</p><p>我们平常用到的大多是直角坐标系，在<b>线性代数中可以把这个坐标系扭曲、拉伸、旋转，称为基变换</b>。</p><p>我们可以按需求去设定基，但是<b>基的轴之间必须是线性无关的</b>，也就是保证坐标系的不同轴不要指向同一个方向或可以被别的轴组合而成，否则的话原来的空间就“撑”不起来了。</p><p>从线性空间的角度看，在一个定义了内积的线性空间里，对一个N阶对称方阵进行特征分解，就是产生了该空间的N个标准正交基，然后把矩阵投影到这N个基上。<b>N个特征向量就是N个标准正交基，而特征值的模则代表矩阵在每个基上的投影长度</b>。</p><p>特征值越大，说明矩阵在对应的特征向量上的方差越大，功率越大，信息量越多。</p><p><b>特征值分解可以得到特征值与特征向量，特征值表示的是这个特征到底有多重要，而特征向量表示这个特征是什么</b></p><p>在机器学习特征提取中，意思就是<b>最大特征值对应的特征向量方向上包含最多的信息量</b>，如果某几个特征值很小，说明这几个方向信息量很小，可以用来降维，也就是删除小特征值对应方向的数据，只保留大特征值方向对应的数据，这样做以后数据量减小，但有用信息量变化不大，PCA降维就是基于这种思路。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>奇异值</b></h2><p>特征值及<b>特征值分解都是针对方阵而言</b>，现实世界中，我们看到的<b>大部分矩阵不是方阵，比如每道数据有M个点，一共采集了N道数据，这样就形成了一个N*M的矩阵</b>，那么怎样才能像方阵一样提取出它的特征，以及特征的重要性。</p><p>奇异值分解就是来干这个事情的。奇异值相当于方阵中的特征值，奇异值分解相当于方阵中的特征值分解。</p><h2><b>奇异值分解（SVD）</b></h2><p>奇异值分解是一种适用于任意矩阵的分解方法。</p><p><img src=\"https://www.zhihu.com/equation?tex=M+%3DU%CE%A3V%5ET\" alt=\"M =UΣV^T\" eeimg=\"1\"/> </p><p>U 矩阵（左奇异矩阵）的列向量分别是u1,u2（ <img src=\"https://www.zhihu.com/equation?tex=MM%5ET\" alt=\"MM^T\" eeimg=\"1\"/> 的特征向量）；</p><p>Σ是一个对角矩阵，对角元素分别是对应的σ1 和 σ2；</p><p>V矩阵（右奇异矩阵）的列向量分别是v1,v2（ <img src=\"https://www.zhihu.com/equation?tex=M%5ETM\" alt=\"M^TM\" eeimg=\"1\"/> 的特征向量）。</p><p>V表示了原始域的标准正交基，U表示经过M 变换后的co-domain的标准正交基，Σ表示了V 中的向量与u中相对应向量之间的关系。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>特征值和奇异值关系</b></h2><p>特征值和奇异值既然各自描述了矩阵中特征（特征向量和奇异值向量）的重要性，那么必然存在一定的关系。</p><p>一般矩阵A，将A与其转置相乘 <img src=\"https://www.zhihu.com/equation?tex=A%5ETA\" alt=\"A^TA\" eeimg=\"1\"/> 将会得到一个方阵，便可求得方阵 <img src=\"https://www.zhihu.com/equation?tex=A%5ETA\" alt=\"A^TA\" eeimg=\"1\"/> 的特征值（ <img src=\"https://www.zhihu.com/equation?tex=A%5ETAv_i%3D%CE%BB_iv_i\" alt=\"A^TAv_i=λ_iv_i\" eeimg=\"1\"/> ）。</p><p>这里的得到的特征向量vi上面的<b>右奇异向量</b>，<b>所有特征向量一起组成了右奇异矩阵</b>。此外我们还可以得到奇异值和左奇异向量（矩阵）</p><p>奇异值σ跟特征值类似，<b>在矩阵Σ中也是从大到小排列，而且σ的减少特别的快</b>，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上了。</p><p>也就是说，我们也可以用前r大的奇异值来近似描述矩阵，这里定义一下部分奇异值分解：</p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "矩阵", 
                    "tagLink": "https://api.zhihu.com/topics/19650614"
                }, 
                {
                    "tag": "线性代数", 
                    "tagLink": "https://api.zhihu.com/topics/19577698"
                }
            ], 
            "comments": [
                {
                    "userName": "知乎用户", 
                    "userLink": "https://www.zhihu.com/people/0", 
                    "content": "<p>写的真好，呱唧呱唧</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "wzzzzz", 
                    "userLink": "https://www.zhihu.com/people/c99a6b8f0d1fc2a7d1daaf3e937f332d", 
                    "content": "下面没了啊", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/46936935", 
            "userName": "leo666", 
            "userLink": "https://www.zhihu.com/people/a22bfdaa2fdedff01a8e865b81a8e221", 
            "upvote": 5, 
            "title": "What Do We Understand About CNNs翻译（四）", 
            "content": "<p></p><hr/><h2>2.2 空间卷积网络</h2><p>理论上，卷积网络可以应用于任意维数的数据。它们的二维实例化非常适合于单个图像的结构，因此在计算机视觉领域受到了广泛关注。随着大规模数据集和强大计算机的可用性，vision社区最近在各种应用使用ConvNets的数量激增。本节描述最突出的2D ConvNet网络结构，该网络结构将相对新颖的结构引入到原始LeNet中。</p><h2>2.2.1 最近重要的ConvNets</h2><p>重新激起人们对ConvNet架构兴趣的是Krishevsky的AlexNet。AlexNet能够在ImageNet数据集上实现最优的物体识别结果。它总共由8个层组成，5个卷积层和3个完全连接的层，如图2.8所示。AlexNet介绍了几种架构设计，允许使用标准随机梯度下降进行有效的网络训练。特别地，四个重要的贡献是AlexNet成功的关键：</p><p>*首先，AlexNet考虑<b>使用ReLU</b>而不是饱和非线性激活函数（saturating nonlinearites），如sigmoids。ReLU的使用<b>减少了梯度消失的问题，并加快了训练速度</b>。</p><p>*网络中最后一个完全连接层包含的参数数量最多，因此AlexNet<b>将dropout首次引入神经网络，以抑制过拟合问题</b>。Dropout即随机删除一个层给定百分比的参数，使得在每次传递时训练一个略微不同的网络结构，并人为地减少每次传递需要学习的参数数量，这有助于减少单元之间的相关性，从而抑制过度拟合。</p><p>*第三，AlexNet依靠<b>数据增强来提高网络学习不变表征的能力</b>。例如，网络不仅训练了训练集中的原始图像，还训练了随机移动和反射训练图像所产生的变化。</p><p>*最后，AlexNet还使用了一些手段来使训练过程收敛得更快，例如<b>使用momument和有计划地减低学习率，即每次学习停滞时学习率都会降低</b>。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-897443ec75e8e80fd69b6d29928c64f3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"917\" data-rawheight=\"421\" class=\"origin_image zh-lightbox-thumb\" width=\"917\" data-original=\"https://pic4.zhimg.com/v2-897443ec75e8e80fd69b6d29928c64f3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;917&#39; height=&#39;421&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"917\" data-rawheight=\"421\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"917\" data-original=\"https://pic4.zhimg.com/v2-897443ec75e8e80fd69b6d29928c64f3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-897443ec75e8e80fd69b6d29928c64f3_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>AlexNet的出现导致了大量的论文试图通过可视化来理解网络正在学习什么，如在DeConvNet所做的那样，或者通过对各种体系结构的系统探索来理解网络。这些探索的直接结果之一是认识到更深层的网络可以获得更好的结果，正如第一次在19层深vggg - net中演示的那样。VGG-Net通过简单地堆叠更多的层来达到它的深度，同时遵循AlexNet引入的标准实践(例如依赖ReLU非线性和数据增强技术来进行更好的训练)。在VGG-Net中呈现的主要创新点在于<b>使用较小的滤波器</b>（即整个网络中使用3×3滤波器，而不是例如AlexNet中使用的11×11滤波器），这样可以<b>在不显著增加参数的情况下增加深度</b>。值得注意的是，虽然使用更小的滤波器，但是VGG-Net每层需要更多的滤波器。</p><p>VGG-Net是继AlexNet之后的许多深层ConvNet架构中的第一个也是最简单的一个。后来又提出了一种更深层的架构，通常称为GoogLeNet，有22层[138]。由于使用了所谓的inception模块(如图2.9(a)所示)作为构建块，<b>虽然它比VGG-Net更深，但GoogLeNet需要的参数要少得多</b>。在初始模块中，<b>各种尺度的卷积运算和空间池并行发生。 该模块还增加了1×1卷积</b>，其用于降低维数以避免冗余滤波器，同时保持网络的大小可控。 这种想法是由先前的一项研究发现所启发的，该研究被称为网络中的网络(NiN)，它消除了学习网络中的大量冗余。堆叠许多初始模块形成现在广泛使用的GoogLeNet架构,如图2.9（b）所示： </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-a3dde933c4951ac5c9f0bd7d33a16eef_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"974\" data-rawheight=\"682\" class=\"origin_image zh-lightbox-thumb\" width=\"974\" data-original=\"https://pic4.zhimg.com/v2-a3dde933c4951ac5c9f0bd7d33a16eef_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;974&#39; height=&#39;682&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"974\" data-rawheight=\"682\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"974\" data-original=\"https://pic4.zhimg.com/v2-a3dde933c4951ac5c9f0bd7d33a16eef_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-a3dde933c4951ac5c9f0bd7d33a16eef_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>GoogLeNet是<b>第一个脱离简单堆叠卷积层和池化层策略的网络</b>，很快<b>超过150层的新架构ResNet被提出</b>。ResNet表示残差网络，其主要贡献在于对残差学习的依赖。ResNet的构建使每个层在输入x的基础上学习增量转换F(x)，根据： </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-de1ed1c51e1e244019810b354e5044a0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"624\" data-rawheight=\"89\" class=\"origin_image zh-lightbox-thumb\" width=\"624\" data-original=\"https://pic1.zhimg.com/v2-de1ed1c51e1e244019810b354e5044a0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;624&#39; height=&#39;89&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"624\" data-rawheight=\"89\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"624\" data-original=\"https://pic1.zhimg.com/v2-de1ed1c51e1e244019810b354e5044a0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-de1ed1c51e1e244019810b354e5044a0_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>而不像其他标准ConvNet架构那样直接学习转换H（x）。 这种残差学习是通过使用跳跃连接来实现的，如图2.10（a）所示，该连接使用标识映射连接不同层的组件。信号x的直接传播在反向传播期间对抗消失的梯度问题，从而能够训练非常深的网络。 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-4164c4956b030c92fe9fc20c2ad5b9e0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"951\" data-rawheight=\"612\" class=\"origin_image zh-lightbox-thumb\" width=\"951\" data-original=\"https://pic1.zhimg.com/v2-4164c4956b030c92fe9fc20c2ad5b9e0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;951&#39; height=&#39;612&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"951\" data-rawheight=\"612\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"951\" data-original=\"https://pic1.zhimg.com/v2-4164c4956b030c92fe9fc20c2ad5b9e0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-4164c4956b030c92fe9fc20c2ad5b9e0_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>最近，一个关于ResNet成功的密切相关的网络是<b>DenseNet，它进一步推动了残差连接的概念</b>。 在DenseNet中，每个层通过跳跃连接连接到dense block的所有后续层，如图2.11所示。 具体地，dense block将所有层与相同大小的特征图（即空间池化层之间的块）连接起来。 与ResNet不同，DenseNet不会添加前一层的特征映射（2.15），而是级联特征映射，以便网络学习新的特征： </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-d68cbf7dffee03d2ce5765cac7f3d3f1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"658\" data-rawheight=\"75\" class=\"origin_image zh-lightbox-thumb\" width=\"658\" data-original=\"https://pic2.zhimg.com/v2-d68cbf7dffee03d2ce5765cac7f3d3f1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;658&#39; height=&#39;75&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"658\" data-rawheight=\"75\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"658\" data-original=\"https://pic2.zhimg.com/v2-d68cbf7dffee03d2ce5765cac7f3d3f1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-d68cbf7dffee03d2ce5765cac7f3d3f1_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>作者声称，这种策略允许DenseNet<b>在每一层使用更少的滤波器，因为将在一层提取的特征推送到层次结构中的更高层，可以避免可能的冗余信息</b>。 重要的是，这些深度跳跃连接考虑了更好的梯度流，因为较低层可以更直接地访问损失函数。 使用这个简单的想法，DenseNet可以与其他深层架构（如ResNet）媲美，同时参数更少并能防止过度拟合。 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-8da2b736188737660cdc9565f2e38b7e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"712\" data-rawheight=\"95\" class=\"origin_image zh-lightbox-thumb\" width=\"712\" data-original=\"https://pic3.zhimg.com/v2-8da2b736188737660cdc9565f2e38b7e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;712&#39; height=&#39;95&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"712\" data-rawheight=\"95\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"712\" data-original=\"https://pic3.zhimg.com/v2-8da2b736188737660cdc9565f2e38b7e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-8da2b736188737660cdc9565f2e38b7e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>2.2.2 ConvNet不变性</h2><p>使用ConvNets的挑战之一是<b>需要非常大的数据集来学习所有底层参数</b>。即使像ImageNet这样拥有超过一百万张图像的大型数据集也被认为太小而无法训练某些深层架构。处理大数据集的一种方法是<b>通过随机翻转，旋转和抖动来改变图像来人为地增加数据集</b>。这些增强的主要优点是所得到的网络<b>对各种变换变得更加稳定</b>。事实上，这项技术是AlexNet取得巨大成功背后的主要原因之一。因此，除了改变网络架构以便于训练之外，其他工作旨在引入能够产生更好训练的新型网络模块。具体而言，本节讨论的网络引入了新的块，这些块直接包含来自原始数据的不变特征。</p><p>一个解决不变性的ConvNet是<b>空间变换网络（Spatial Transformer Network）</b>。特别地，该网络利用了一种新颖的学习模块，该模块增加了对不重要的空间变换的不变性，例如，在物体识别过程中由不同视点产生的场景。该模块由三个子模块组成： localisation network， grid generator和sampler。如图2.12（a）所示。执行阶段可以分为三个步骤。首先，localisation network（通常是一个小的2层神经网络）将一个input map U作为输入，并从该输入学习空间变换的参数 θ。例如，变换Tθ可以被定义为对网络缩放，旋转和剪切的一般仿射变换。其次，给定变换参数和预定义大小的输出网格H×W，grid generator计算每个输出坐标 (x_i^t,y_i^t)。对应的坐标(x_i^s,y_i^s)应该从input map U采样，根据： </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-f64fe308fd000c7e2d7ecbb56383ffe6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"682\" data-rawheight=\"109\" class=\"origin_image zh-lightbox-thumb\" width=\"682\" data-original=\"https://pic3.zhimg.com/v2-f64fe308fd000c7e2d7ecbb56383ffe6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;682&#39; height=&#39;109&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"682\" data-rawheight=\"109\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"682\" data-original=\"https://pic3.zhimg.com/v2-f64fe308fd000c7e2d7ecbb56383ffe6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-f64fe308fd000c7e2d7ecbb56383ffe6_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>最后， sampler根据input map U和 对应关系 Tθ，生成最终的output map.. 如图2.12（b）所示。在任何ConvNet架构的每一层添加此类模块允许它从输入自适应地学习各种变换，以增加其不变性，从而提高其准确性。 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-802000b0124b5896cc3539b2bc11a9ca_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"937\" data-rawheight=\"429\" class=\"origin_image zh-lightbox-thumb\" width=\"937\" data-original=\"https://pic3.zhimg.com/v2-802000b0124b5896cc3539b2bc11a9ca_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;937&#39; height=&#39;429&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"937\" data-rawheight=\"429\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"937\" data-original=\"https://pic3.zhimg.com/v2-802000b0124b5896cc3539b2bc11a9ca_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-802000b0124b5896cc3539b2bc11a9ca_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>为了增强ConvNets的几何变换建模能力，现在有两种方法，即Deformable ConvNet和Active ConvNet，引入了灵活的卷积块。 这些方法的基本思想是避免在卷积期间使用固定窗口，其有利于学习卷积的感兴趣区域（RoI）。 这个想法类似于空间变换器模块的localization network和grid generator所做的事情。 为了确定每层的RoI，修改卷积块，使其从初始固定卷积窗口学习偏移。 具体来说，从在给定的固定窗口上的卷积运算的标准定义开始 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-f64fe308fd000c7e2d7ecbb56383ffe6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"682\" data-rawheight=\"109\" class=\"origin_image zh-lightbox-thumb\" width=\"682\" data-original=\"https://pic3.zhimg.com/v2-f64fe308fd000c7e2d7ecbb56383ffe6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;682&#39; height=&#39;109&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"682\" data-rawheight=\"109\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"682\" data-original=\"https://pic3.zhimg.com/v2-f64fe308fd000c7e2d7ecbb56383ffe6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-f64fe308fd000c7e2d7ecbb56383ffe6_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>其中R是卷积的区域，pn是区域R内的像素位置，w（pn）是相应的滤波器权重，添加新项以包括根据的偏移量。 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-ff7fb2858dc98df97f6b61539c90a3e9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"746\" data-rawheight=\"115\" class=\"origin_image zh-lightbox-thumb\" width=\"746\" data-original=\"https://pic2.zhimg.com/v2-ff7fb2858dc98df97f6b61539c90a3e9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;746&#39; height=&#39;115&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"746\" data-rawheight=\"115\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"746\" data-original=\"https://pic2.zhimg.com/v2-ff7fb2858dc98df97f6b61539c90a3e9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-ff7fb2858dc98df97f6b61539c90a3e9_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>其中4pn是偏移量，现在最终的卷积步骤将在变形窗口上执行，而不是传统的固定n×n窗口。为了学习偏移量4pn，修改了可变形ConvNets的卷积块，使其包含一个新的子模块，其作用是学习偏移量，如图2.13所示。与可交替学习子模块参数和网络权重的空间变换网络不同，Deformable ConvNets可同时学习权重和偏移，从而使其在各种体系结构中的部署速度更快，更容易。 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-cfafd1c4f6f6df2a5c639ed6a0e59c5f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"965\" data-rawheight=\"571\" class=\"origin_image zh-lightbox-thumb\" width=\"965\" data-original=\"https://pic4.zhimg.com/v2-cfafd1c4f6f6df2a5c639ed6a0e59c5f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;965&#39; height=&#39;571&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"965\" data-rawheight=\"571\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"965\" data-original=\"https://pic4.zhimg.com/v2-cfafd1c4f6f6df2a5c639ed6a0e59c5f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-cfafd1c4f6f6df2a5c639ed6a0e59c5f_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>2.2.3 ConvNet定位</h2><p>除了简单的分类任务，例如对象识别，最近ConvNets在需要精确定位的任务中也表现出色，例如语义分割和对象检测。用于语义分割的最成功的网络是所谓的<b>完全卷积网络（FCN）</b>。顾名思义，FCN<b>没有明确地使用完全连接层，而是将它们转换为卷积层，其接收域覆盖整个底层特征映射</b>。重要的是，用上采样或反卷积滤波器，恢复最后一层图像的完整分辨率，如图2.14所示。在FCN中，通过将问题转换为密集像素分类来实现分割。换句话说，softmax层附着到每个像素，并且通过对属于相同类的像素进行分组来实现分割。值得注意的是，在网络中，使用来自架构的较低层的特征在上采样步骤中起着重要作用。考虑到较低层特征倾向于捕获更细粒度的细节，它允许更准确的分割，这对于分割任务而言更为重要。反卷积滤波器训练方案是使用atrou或扩张卷积，即上采样稀疏滤波器，其有助于恢复更高分辨率的特征图，同时可以保持学习的参数的数量可管理。 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-4d49f3537f873176c827fc153cca340e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"924\" data-rawheight=\"380\" class=\"origin_image zh-lightbox-thumb\" width=\"924\" data-original=\"https://pic3.zhimg.com/v2-4d49f3537f873176c827fc153cca340e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;924&#39; height=&#39;380&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"924\" data-rawheight=\"380\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"924\" data-original=\"https://pic3.zhimg.com/v2-4d49f3537f873176c827fc153cca340e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-4d49f3537f873176c827fc153cca340e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>在物体定位方面，ConvNet框架中最早的方法之一被称为Region CNN或R-CNN。该网络将Regional Proposal方法与ConvNet架构相结合[53]。虽然R-CNN是建立在简单的想法上，但它产生了最好的物体检测结果。特别地，R-CNN首先使用现成的算法用于Regional Proposal（例如，选择性搜索）以检测可能包含对象的潜在区域。然后对这些区域进行扭曲以匹配所使用的ConvNet的默认输入大小，并将其送到ConvNet以进行特征提取。最后，每个区域的特征用SVM分类，并在后处理步骤中通过非最大抑制进行细化。 </p><p>在其不完善的版本中，R-CNN仅仅简单地使用ConvNets作为特征提取器。然而，它的突破性成果带来了改进，从而更好地利用了ConvNets强大特征提取功能。例如，Fast R-CNN ， Faster R-CNN和Mask R-CNN。Fast R-CNN建议通过网络传播独立计算的Regional Proposal以在最后的特征图层中提取它们的对应区域。 该技术避免了从图像中提取的每个Region通过网络的昂贵传递。另外，Fast R-CNN通过改变网络的最后一层来避免繁重的后期处理步骤，这样它就可以同时学习对象类别和精细的Bounding Box坐标。重要的是，在R-CNN和Fast R-CNN中，检测瓶颈都位于ConvNet范式之外的Regional Proposal步骤中。</p><p>Faster R-CNN通过在ConvNet的最后一个卷积层之后添加称为Region Proposal Network（RPN）的子模块（或子网络）来进一步推动使用ConvNets。 RPN模块使网络能够学习作为网络优化一部分的Region Proposal。具体地，RPN是由卷积层和小的完全连接层组成的小型ConvNet，它可以返回潜在的对象位置和分类得分（即，属于对象类的概率）两个输出。整个网络的训练通过两步迭代实现。首先，使用RPN网络以进行Regional Proposal提取。其次，保持提取的Regional Proposal固定，微调用于对象分类和最终对象Bounding Box位置。最近， 引入Mask R-CNN增强Faster R-CNN分割detected region的能力，从而在检测到的物体周围产生紧密掩模。为此，mask R-CNN将切割分支添加到Faster R-CNN的分类和Bounding Box回归分支。新分支为一个小FCN，它可用于将任何边界框中的像素分类为两个类中的一个;前景或背景。图2.15说明了从简单的R-CNN到mask R-CNN的差异和进展。 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-349ddc48d45c5d47a01dc029b45e6647_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1024\" data-rawheight=\"571\" class=\"origin_image zh-lightbox-thumb\" width=\"1024\" data-original=\"https://pic4.zhimg.com/v2-349ddc48d45c5d47a01dc029b45e6647_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1024&#39; height=&#39;571&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1024\" data-rawheight=\"571\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1024\" data-original=\"https://pic4.zhimg.com/v2-349ddc48d45c5d47a01dc029b45e6647_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-349ddc48d45c5d47a01dc029b45e6647_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "卷积神经网络（CNN）", 
                    "tagLink": "https://api.zhihu.com/topics/20043586"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "计算机视觉", 
                    "tagLink": "https://api.zhihu.com/topics/19590195"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/46712627", 
            "userName": "leo666", 
            "userLink": "https://www.zhihu.com/people/a22bfdaa2fdedff01a8e865b81a8e221", 
            "upvote": 2, 
            "title": "what do we understand about CNNs 翻译(二)", 
            "content": "<p><b>本文翻译不当之处希望大家指出</b></p><hr/><h2>第二章 多层网络</h2><p>本章简要概述了计算机视觉中最突出的多层网络。值得注意的是，虽然本章涵盖了文献中最重要的贡献，但它不会对这些论文进行综合地回顾，因为这些可在通过精度论文获得。相反，本章的<b>目的是为文献的其余部分设置阶段，并详细介绍和讨论目前应用于视觉处理的卷积网络</b>。</p><h2>2.1多层网络</h2><p>在成功开发基于深度学习的网络之前，用于识别的最先进的计算机视觉系统依赖于<b>两个独立但互补的步骤</b>。首先，通过一组手工设计的操作（例如，具有基础数据，局部或全局编码方法的卷积）将输入数据转换为合适的形式。该变换通常需要找到输入的具体或抽象的特征，同时根据手头的task加入几个不变性。这种转换的目标是为了更容易被分类器分类。其次，变换后的数据用于训练某种分类器（例如支持向量机）以识别输入信号的内容。因此<b>任何分类器的性能通常都严重受到转换的影响</b>。</p><p>多层结构网络<b>不仅要学习分类器，而且还要直接从数据中学习所需的转换操作</b>，从而可以从不同视角看问题。这种学习形式通常被称为<b>特征学习</b>，当在多层网络的上下文中使用时称为<b>深度学习</b>。</p><p>多层结构可以定义为<b>从输入中提取多个抽象的有用信息的计算模型</b>。通常，多层网络在较高网络层强调输入中的重要信息，同时对较不显著的变化越来越robust。大多数多层网络<b>使用交替的线性和非线性函数堆叠简单的构建块模块</b>。多年来，提出了大量的各种多层网络，本节将介绍计算机视觉应用中这些最突出的网络。特别是，由于其突出性，人工神经网络将成为焦点。为了简洁起见，下面将更简单地将这种网络称为神经网络。</p><h2>2.1.1神经网络</h2><p>典型的神经网络结构由输入层x，输出层y和多个隐藏层h组成，其中每个层由多个单元组成，如图2.1所示。 通常，每个隐藏单元hj接收来自前一层的所有单元的输入，并且被定义为输入的加权组合，随后是非线性，根据 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-130a514b8e048dc4538dc0f65de4ce8f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"423\" data-rawheight=\"65\" class=\"origin_image zh-lightbox-thumb\" width=\"423\" data-original=\"https://pic4.zhimg.com/v2-130a514b8e048dc4538dc0f65de4ce8f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;423&#39; height=&#39;65&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"423\" data-rawheight=\"65\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"423\" data-original=\"https://pic4.zhimg.com/v2-130a514b8e048dc4538dc0f65de4ce8f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-130a514b8e048dc4538dc0f65de4ce8f_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>其中，Wij是控制输入单元和隐藏单元之间连接强度的权重，bj是隐藏单元的小偏差，F（）是一些非线性函数，例如sigmoid。</p><p>深度神经网络可以被视为Rosenblatt的感知器和多层感知器的现代实例。 尽管神经网络模型已存在多年（即自1960年代以来），但直到最近才被大量使用。 这种延迟有很多原因。 最初的负面结果显示<b>感知器无法对XOR这样的简单操作进行建模</b>，阻碍了对感知器进行一段时间的进一步研究，直到它们被推广到多层。 此外，<b>缺乏适当的训练算法会减慢进展，直到反向传播算法的普及</b>。 然而，阻碍多层神经网络发展的<b>更大障碍是它们依赖于非常大量的参数</b>，这反过来意味着需要<b>大量的训练数据和计算资源</b>来支持参数的学习。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-1e9fdd0579da1ad17f836b428d608559_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"410\" data-rawheight=\"290\" class=\"content_image\" width=\"410\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;410&#39; height=&#39;290&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"410\" data-rawheight=\"290\" class=\"content_image lazy\" width=\"410\" data-actualsrc=\"https://pic2.zhimg.com/v2-1e9fdd0579da1ad17f836b428d608559_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>使用<b>受限玻尔兹曼机器（RBM）</b>，在深度神经网络领域取得重大进展的主要贡献是<b>分层无监督预训练</b>。 受限制的Boltzman机器可以看作是两层神经网络，在其受限制的形式中，只允许前馈连接。</p><p>在图像识别的背景下，用于训练RBM的无监督学习方法可以<b>归纳为三个步骤</b>。 首先，对于每个像素xi，并且以一组随机权重wij和偏差bj开始，每个单元的隐藏状态hj被设置为1，概率为pj。 概率定义为 ！</p><p>其中，σ（y）= 1 /（1 + exp（-y））。 第二，一旦基于等式2.2随机地设置了所有隐藏状态，通过概率pi，将每个像素xi设置为1来执行重建图像的尝试。 第三，通过基于由给出的重建误差更新权重和偏差来校正隐藏单元。 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-0ce366b97f497295f67854bfc33fccfe_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"470\" data-rawheight=\"42\" class=\"origin_image zh-lightbox-thumb\" width=\"470\" data-original=\"https://pic3.zhimg.com/v2-0ce366b97f497295f67854bfc33fccfe_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;470&#39; height=&#39;42&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"470\" data-rawheight=\"42\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"470\" data-original=\"https://pic3.zhimg.com/v2-0ce366b97f497295f67854bfc33fccfe_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-0ce366b97f497295f67854bfc33fccfe_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>其中α是学习率， 是像素Xi和隐藏单元hj在相关联的次数。整个过程重复N次或直到误差下降到预设阈值τ。在训练一个层之后，其输出被用作层次结构中下一层的输入，该层又按照相同的过程进行训练。通常，在预训练所有网络层之后，通过使用梯度下降的误差反向传播，它们进一步通过标记数据进行微调。使用该<b>分层无监督预训练允许训练深度神经网络而不需要大量标记数据</b>，因为无监督RBM预训练提供了用于经验上有用的初始化各种网络参数的方式。堆叠RBM的神经网络首先成功地用作降维，并应用于人脸识别，其中它们被看为一种自动编码器。简而言之，自动编码器可以定义为由两个主要部分组成的多层神经网络：首先，<b>编码器将输入数据转换为特征向量</b>;第二，<b>解码器将生成的特征向量映射回输入空间</b>;见图2.2。通过最小化输入与其重建版本之间的重建误差来学习自动编码器的参数。 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-65d8897e367daa312f9e703d531c13ab_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1024\" data-rawheight=\"375\" class=\"origin_image zh-lightbox-thumb\" width=\"1024\" data-original=\"https://pic4.zhimg.com/v2-65d8897e367daa312f9e703d531c13ab_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1024&#39; height=&#39;375&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1024\" data-rawheight=\"375\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1024\" data-original=\"https://pic4.zhimg.com/v2-65d8897e367daa312f9e703d531c13ab_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-65d8897e367daa312f9e703d531c13ab_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>除了基于RBM的自动编码器之外，后来提出了几种类型的自动编码器。每个自动编码器都<b>引入了一种不同的正则化方法</b>，即使在执行不同的不变性时也能阻止网络过拟合。示例包括<b>稀疏自动编码器（SAE），去噪自动编码器（DAE）和压缩自动编码器（CAE）</b>。稀疏自动编码器允许中间特征的大小（即由编码器部分生成）大于输入的大小，同时通过错误输出来强制稀疏。相比之下，去噪自动编码器通过人为破坏重建清晰输入来达成重建的目标，目的是学习强robust的representation。类似地，压缩自动编码器构建通过进一步惩罚对噪声最敏感的单元来对自动编码器进行去噪处理。各种类型的自动编码器的更详细的介绍可以在别处找到。</p><h2>2.1.2递归神经网络</h2><p>在考虑依赖于顺序输入的任务时，最成功的多层架构之一是递归神经网络（RNN）。 如图2.3所示，RNN可以看作是一种特殊类型的神经网络，其中每个隐藏单元从当前时间输入的数据以及前一时间的状态获取输入。 RNN的输出定义为 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-55536627ca46e92d089222ebacd0c116_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"675\" data-rawheight=\"74\" class=\"origin_image zh-lightbox-thumb\" width=\"675\" data-original=\"https://pic3.zhimg.com/v2-55536627ca46e92d089222ebacd0c116_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;675&#39; height=&#39;74&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"675\" data-rawheight=\"74\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"675\" data-original=\"https://pic3.zhimg.com/v2-55536627ca46e92d089222ebacd0c116_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-55536627ca46e92d089222ebacd0c116_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-548798cad08f782900adc0e4050fcfc7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1024\" data-rawheight=\"483\" class=\"origin_image zh-lightbox-thumb\" width=\"1024\" data-original=\"https://pic4.zhimg.com/v2-548798cad08f782900adc0e4050fcfc7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1024&#39; height=&#39;483&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1024\" data-rawheight=\"483\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1024\" data-original=\"https://pic4.zhimg.com/v2-548798cad08f782900adc0e4050fcfc7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-548798cad08f782900adc0e4050fcfc7_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>其中σ是非线性函数，wi和ui是控制当前和过去信息的相对重要性的参数。尽管RNN看起来是强大的网络结构，但它们的<b>主要问题之一是对周期长的建模能力有限</b>。这种限制<b>归因于在通过多个时间步骤传播误差时可能发生的爆炸或消失梯度导致的训练差异</b>。特别是，在训练期间，反向传播的梯度乘以从当前时间一直到初始时间的网络权重。因此，由于<b>这种乘法积累，权重可以对传播的梯度具有非平凡的影响。如果权重很小，则梯度消失，而较大的权重导致梯度爆炸</b>。为了纠正这种困难，引入了LSTM。</p><p>LSTMs是循环网络，它进一步<b>配备了存储或内存组件</b>，如图2.4所示，随着时间的推移积累信息。LSTM的<b>内存单元是门控的</b>，这样就可以从它读取或写入信息。值得注意的是，LSTMs还包含一个<b>遗忘门，允许网络在不再需要信息时删除信息</b>。LSTMs由<b>三个不同的门控制(输入门、遗忘门和输出门)</b>，以及内存单元状态ct。输入门由最新的输入xt和前一个状态ht−1控制,它被定义为 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-06a17d1bc9b7a233f246b6bc926fb50b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"748\" data-rawheight=\"65\" class=\"origin_image zh-lightbox-thumb\" width=\"748\" data-original=\"https://pic4.zhimg.com/v2-06a17d1bc9b7a233f246b6bc926fb50b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;748&#39; height=&#39;65&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"748\" data-rawheight=\"65\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"748\" data-original=\"https://pic4.zhimg.com/v2-06a17d1bc9b7a233f246b6bc926fb50b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-06a17d1bc9b7a233f246b6bc926fb50b_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>其中，wi，ui，bi表示控制输入门的权重和偏差，σ通常是Sigmoid函数。 遗忘门同样被定义为 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-1c70bdd81bbd40d493b2d88c135aadc0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"739\" data-rawheight=\"91\" class=\"origin_image zh-lightbox-thumb\" width=\"739\" data-original=\"https://pic1.zhimg.com/v2-1c70bdd81bbd40d493b2d88c135aadc0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;739&#39; height=&#39;91&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"739\" data-rawheight=\"91\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"739\" data-original=\"https://pic1.zhimg.com/v2-1c70bdd81bbd40d493b2d88c135aadc0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-1c70bdd81bbd40d493b2d88c135aadc0_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>它由相应的权重和偏差控制，wf，uf，bf。 可以说，LSTM<b>最重要的方面是它可以应对梯度消失和梯度爆炸</b>。 在确定存储器单元的状态时，通过<b>遗忘和输入门状态的相加组合来实现该能力</b>，该状态又控制信息是否经由输出门传递到另一个单元。 具体而言分两步计算。 首先，根据估计候选单元状态 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-5c247b368295adabfc3b3466b7c82069_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"759\" data-rawheight=\"91\" class=\"origin_image zh-lightbox-thumb\" width=\"759\" data-original=\"https://pic2.zhimg.com/v2-5c247b368295adabfc3b3466b7c82069_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;759&#39; height=&#39;91&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"759\" data-rawheight=\"91\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"759\" data-original=\"https://pic2.zhimg.com/v2-5c247b368295adabfc3b3466b7c82069_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-5c247b368295adabfc3b3466b7c82069_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>其中φ通常是双曲正切线（激活函数）。 其次，最终的单元状态最终由当前估计的状态gt和前一个状态ct-1控制，由输入和忘记门调制。 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-2088ac4f4da5704f35507e235366b6b4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"706\" data-rawheight=\"109\" class=\"origin_image zh-lightbox-thumb\" width=\"706\" data-original=\"https://pic1.zhimg.com/v2-2088ac4f4da5704f35507e235366b6b4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;706&#39; height=&#39;109&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"706\" data-rawheight=\"109\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"706\" data-original=\"https://pic1.zhimg.com/v2-2088ac4f4da5704f35507e235366b6b4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-2088ac4f4da5704f35507e235366b6b4_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>最后，使用单元的状态以及当前和先前的输入，输出门的值和LSTM单元的输出可以根据 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-9f9a765882d54a9cc8eb8c12bfbc4af2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"982\" data-rawheight=\"282\" class=\"origin_image zh-lightbox-thumb\" width=\"982\" data-original=\"https://pic3.zhimg.com/v2-9f9a765882d54a9cc8eb8c12bfbc4af2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;982&#39; height=&#39;282&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"982\" data-rawheight=\"282\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"982\" data-original=\"https://pic3.zhimg.com/v2-9f9a765882d54a9cc8eb8c12bfbc4af2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-9f9a765882d54a9cc8eb8c12bfbc4af2_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-34ce6ba38f49ed33848b6c48cc9855e6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1024\" data-rawheight=\"582\" class=\"origin_image zh-lightbox-thumb\" width=\"1024\" data-original=\"https://pic3.zhimg.com/v2-34ce6ba38f49ed33848b6c48cc9855e6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1024&#39; height=&#39;582&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1024\" data-rawheight=\"582\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1024\" data-original=\"https://pic3.zhimg.com/v2-34ce6ba38f49ed33848b6c48cc9855e6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-34ce6ba38f49ed33848b6c48cc9855e6_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p><a href=\"https://zhuanlan.zhihu.com/p/46839142\" class=\"internal\">what do we understand about CNNs 翻译(三)</a></p>", 
            "topic": [
                {
                    "tag": "卷积神经网络（CNN）", 
                    "tagLink": "https://api.zhihu.com/topics/20043586"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "LSTM", 
                    "tagLink": "https://api.zhihu.com/topics/20023220"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/46638107", 
            "userName": "leo666", 
            "userLink": "https://www.zhihu.com/people/a22bfdaa2fdedff01a8e865b81a8e221", 
            "upvote": 11, 
            "title": "What Do We Understand About CNNs翻译（一）", 
            "content": "<p></p><hr/><p><b><code>最近读了篇深度学习综述类的论文，读完对整个深度学习网络会有个新的了解，该论文是在2018年3月23号发表在了arXiv，论文正文部分长达80页，我会将翻译分成几块，陆续发出来跟大家讨论。</code></b> </p><p><b>论文封面</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-a996aa72629b9f31ca32869d9ee713da_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"712\" data-rawheight=\"492\" class=\"origin_image zh-lightbox-thumb\" width=\"712\" data-original=\"https://pic3.zhimg.com/v2-a996aa72629b9f31ca32869d9ee713da_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;712&#39; height=&#39;492&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"712\" data-rawheight=\"492\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"712\" data-original=\"https://pic3.zhimg.com/v2-a996aa72629b9f31ca32869d9ee713da_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-a996aa72629b9f31ca32869d9ee713da_b.jpg\"/></figure><h2>Introduction</h2><h2>1.1 动机</h2><p>在过去几年中，主要的计算机视觉研究都集中在卷积神经网络上，通常称为ConvNets或CNN。这些效果在广泛的分类和回归任务中产生了新的最先进的性能。相比之下，虽然这些方法的历史可以追溯到若干年，但对这些系统如何实现其卓越结果的理论理解滞后。事实上，目前计算机视觉领域的许多贡献都使用ConvNets作为一个黑盒子，它对于它的工作原理有一个非常模糊的想法，从科学的角度来看这是非常不令人满意的。特别是，有两个主要的互补问题： <b>（1）对于学习方面（例如卷积核），它究竟学了什么？  （2）对于网络设计方面（例如层数，内核数，池化策略，非线性选择），为什么某些选择比其他选择更好？</b> 这些问题的答案不仅可以提高对ConvNets的科学理解，还可以提高它们的实际适用性。</p><p>此外，目前ConvNets的实现需要大量的数据用于训练，设计决策对性能有很大影响。 更深入的理论理解应该减少对数据驱动设计的依赖。 虽然实证研究调查了实施网络的运作，但迄今为止，他们的结果主要局限于内部处理的可视化，以了解ConvNet不同层面正在发生的事情。</p><h2>1.2 目标</h2><p>本文献将回顾多种卷积网络。重要的是，我们将通过用多种方法来讨论典型的卷积网络的各个组成部分，这些方法的设计决策基于生物学发现和合理的理论基础。此外，还将通过可视化和实证研究来理解ConvNets。最终目标是阐明ConvNet网络中涉及的每一层的作用，提炼我们目前对ConvNets的理解，并突出不足之处。</p><h2>1.3 论文大纲</h2><p>本报告的结构如下：本章的动机是需要回顾我们对卷积网络的理解。第2章将介绍各种多层网络，并介绍计算机视觉应用中最典型的网络。第3章将更具体地关注典型卷积网络的每个结构块，并从生物学和理论角度讨论不同结构的设计。最后，第4章将描述ConvNet设计的当前趋势以及对ConvNet理解的有效性，并强调仍然存在的一些突出不足之处。</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>下面是翻译二的链接：</p><p><a href=\"https://zhuanlan.zhihu.com/p/46712627\" class=\"internal\">what do we understand about CNNs 翻译(二)</a></p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "卷积神经网络（CNN）", 
                    "tagLink": "https://api.zhihu.com/topics/20043586"
                }, 
                {
                    "tag": "文献综述", 
                    "tagLink": "https://api.zhihu.com/topics/19565727"
                }
            ], 
            "comments": [
                {
                    "userName": "leo666", 
                    "userLink": "https://www.zhihu.com/people/a22bfdaa2fdedff01a8e865b81a8e221", 
                    "content": "<p>我已添加翻译二，初到知乎，不知有没有提醒。</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "JustDoIT", 
                    "userLink": "https://www.zhihu.com/people/d69c9f6a22c7e503f4fe519cd77a3e7d", 
                    "content": "很强", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "leo666", 
                    "userLink": "https://www.zhihu.com/people/a22bfdaa2fdedff01a8e865b81a8e221", 
                    "content": "<p>开了个专栏，以后相关文章会发到该专栏里面，大家可以关注关注</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "知乎用户", 
                    "userLink": "https://www.zhihu.com/people/0", 
                    "content": "Arxiv和scholar都搜不到这篇论文  能分享一下么？", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/46839142", 
            "userName": "leo666", 
            "userLink": "https://www.zhihu.com/people/a22bfdaa2fdedff01a8e865b81a8e221", 
            "upvote": 3, 
            "title": "what do we understand about CNNs 翻译(三)", 
            "content": "<p></p><hr/><h2>2.1.3卷积网络</h2><p>卷积网络（ConvNets）是一种特殊类型的神经网络，特别适用于计算机视觉应用，因为它们能够通过局部连接（local connection）使representations分层抽象化。有两个关键的理念推动计算机视觉中卷积网络结构的成功。首先，ConvNets利用<b>图像的2D结构以及邻域内的像素通常高度相关的特性</b>。因此，ConvNets<b>避免在所有像素之间使用一对一连接（即大多数神经网络的情况），而是使用分组连接</b>。此外，ConvNet架构<b>依赖于特征共享</b>，因此每个通道（或输出特征图）由在所有位置使用相同滤波器的卷积生成，如图2.5所示。与标准神经网络相比，ConvNets的这一重要特性导致了一种<b>更少参数的架构</b>。其次，ConvNets还<b>引入了池化层，该步骤提供了一定程度的平移不变性，使得架构不受位置微小变化的影响</b>。值得注意的是，由于网络接收域的大小增加，池化还允许网络逐渐看到输入的较大部分。接收域大小的增加（加上输入分辨率的降低）使得在<b>网络深度增加时表示更抽象的特征</b>。例如，对于物体识别，ConvNets layers从关注对象的边缘开始然后在较高层覆盖整个对象的特征。 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-dd79997ae463cc9410ea80feeeb8913c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1024\" data-rawheight=\"412\" class=\"origin_image zh-lightbox-thumb\" width=\"1024\" data-original=\"https://pic1.zhimg.com/v2-dd79997ae463cc9410ea80feeeb8913c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1024&#39; height=&#39;412&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1024\" data-rawheight=\"412\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1024\" data-original=\"https://pic1.zhimg.com/v2-dd79997ae463cc9410ea80feeeb8913c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-dd79997ae463cc9410ea80feeeb8913c_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>卷积网络的结构很大程度上受到了视觉皮层的启发，如Hubel和Wiesel的开创性工作所述（在第3章中进一步讨论）。实际上，最早的卷积网络实例似乎是Fukushima的Neocognitron，它也依赖于局部连接，其中每个特征图最大限度地响应特定的特征类型。 Neocognitron由一系列K层组成，其中每层交替显示S细胞单元Usl和复杂细胞单元Ucl，它们分别模拟生物简单和复杂细胞中发生的处理，如图所示2.6。简单单元单元执行类似于局部卷积的操作，然后执行Rectified线性单元（ReLU）非线性，而复杂单元执行类似于均值滤波器。该模型还包括一个非线性函数，以实现类似于当代ConvNets中的正则化的功能。 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-0ceb30eefc95abf3fa17985afbb2368c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"814\" data-rawheight=\"391\" class=\"origin_image zh-lightbox-thumb\" width=\"814\" data-original=\"https://pic1.zhimg.com/v2-0ceb30eefc95abf3fa17985afbb2368c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;814&#39; height=&#39;391&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"814\" data-rawheight=\"391\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"814\" data-original=\"https://pic1.zhimg.com/v2-0ceb30eefc95abf3fa17985afbb2368c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-0ceb30eefc95abf3fa17985afbb2368c_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>与大多数标准ConvNet架构相反，Neocognitron<b>不需要标记数据进行学习，因为它是基于自组织图设计的</b>，它可以学习通过重复呈现一组刺激图像（stimulate images）来了解连续层之间的局部连接。特别地，Neocognitron训练了输入特征图和简单细胞层之间的连接（简单细胞层和复杂细胞层之间的连接是预先确定的），并且学习过程可以概括为两个步骤。首先，每当<b>在输入处呈现新的刺激时，选择最大响应它的简单细胞作为该刺激类型的表征(representation)</b>。其次，<b>每次响应相同的输入类型时，输入和表征单元之间的连接就会得到加强</b>。值得注意的是，简单的单元层被组织在不同的组或平面中，使得<b>每个平面仅响应一种刺激类型（即，类似于现在的ConvNet架构中的特征图）</b>。对Neocognitron的后续扩展包括监督学习以及自上而下的注意机制。</p><p>在最近的计算机视觉应用中部署的大多数ConvNets架构受到LeCun在1998年提出的成功架构的启发，现在称为LeNet，用于手写识别。如文献所述，经典卷积网络由四个基本处理层组成：<b>（i）卷积层，（ii）非线性或修正层，（iii）归一化层和（iv）池化层</b>。如上所述，这些成分在Neocognitron中也是重要部分。 <b>LeNet的一个亮点是反向传播</b>，以便相对<b>高效地学习卷积参数</b>。ConvNets是较优化的架构，与完全连接的神经网络相比，它需要的<b>参数要少得多</b>，但它们的<b>主要缺点仍然是它们严重依赖训练数据和标记数据</b>。这种数据依赖性可能是2012年前ConvNets未被广泛使用的主要原因之一，因为<b>大型ImageNet数据集和相应的计算资源的可用性</b>使得ConvNets的重新兴起。 ConvNets在ImageNet上的成功引发了各种ConvNet结构的突飞猛进，该领域的大多数贡献仅仅基于ConvNets的基本结构块的不同变化，这将在后面的2.2节中讨论。</p><h2>2.1.4生成对抗网络</h2><p>生成性对抗网络（GAN）是相对较新的模型。 GAN首先在2014年引入，虽然它们本身没有提供不同的架构（例如，就新颖的网络结构块而言），但它们具有一些特殊性，这使得它们成为一种稍微不同的多层网络。 </p><p>GAN响应的一个关键挑战是采用<b>无监督学习的方法，不需要标记数据</b>。 典型的GAN由两个竞争块或子网组成，如图2.7所示;<b>生成器网络G（z;θg）（generator network）和鉴别器网络D（x;θd）（discriminator network）</b>，其中z是输入的随机噪声，x是实际输入数据（例如图像），θg和θd是两者的参数。每个块可以由任何先前定义的多层网络结构制成。在最初的论文中，生成器和鉴别器都是多层全连接网络。<b>训练鉴别器D以识别来自生成器的数据并且以概率pd分配标签“假”，同时以概率1-pd将标签“真实”分配给真实输入数据</b>。另外，不断优化生成器网络使它能够生成欺骗鉴别器的伪输出。这两个块在几个步骤中交替训练，其中训练过程的理想结果是<b>鉴别器对真实数据和假数据各分配50％的概率</b>。换句话说，在收敛之后，<b>生成器能够从随机输入生成实际数据</b>。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-ab1424afc56606bd6453dc0f9248f8dd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"809\" data-rawheight=\"387\" class=\"origin_image zh-lightbox-thumb\" width=\"809\" data-original=\"https://pic2.zhimg.com/v2-ab1424afc56606bd6453dc0f9248f8dd_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;809&#39; height=&#39;387&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"809\" data-rawheight=\"387\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"809\" data-original=\"https://pic2.zhimg.com/v2-ab1424afc56606bd6453dc0f9248f8dd_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-ab1424afc56606bd6453dc0f9248f8dd_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>自最初论文以来，许多贡献通过使用更强大的多层架构作为网络的主干参与增强GAN的功能（例如，用于鉴别器和反卷积网络的预训练卷积网络，学习生成器的上采样滤波器） 。GAN的一些成功应用包括： 文本到图像合成：网络的输入是文本描述，输出是要渲染的图像 图像超分辨率：从较低的生成逼真的高分辨率图像分辨率输入） 图像修复：从输入图像中填充缺失信息的点/块 纹理合成：从输入噪声合成真实纹理。</p><h2>2.1.5多层网络</h2><p>正如前面所说，各种多层架构的成功<b>很大程度上取决于他们训练过程</b>。 虽然神经网络通常<b>依赖于无监督的预训练</b>，如2.1.1节所述，但它们通常遵循多层网路的基本训练策略，并且是完全监督的。 训练过程通常<b>基于梯度下降法进行反向传播</b>。 梯度下降因其简单性而广泛用于训练多层架构。 它依赖于最小化平滑误差函数E（w），迭代过程如下 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-3bce4291bc98320cb277d3f023a1fbd0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"710\" data-rawheight=\"113\" class=\"origin_image zh-lightbox-thumb\" width=\"710\" data-original=\"https://pic1.zhimg.com/v2-3bce4291bc98320cb277d3f023a1fbd0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;710&#39; height=&#39;113&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"710\" data-rawheight=\"113\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"710\" data-original=\"https://pic1.zhimg.com/v2-3bce4291bc98320cb277d3f023a1fbd0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-3bce4291bc98320cb277d3f023a1fbd0_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>其中w代表网络的参数，α是可以控制收敛速度的学习速率，∂E（w）/∂w是在训练集上计算的误差梯度。这种简单的梯度下降方法<b>特别适用于训练多层网络，这是因为使用链式法则进行反向传播并计算相对于不同层的各种网络参数的误差导数</b>。虽然反向传播可追溯到多年前，但它在<b>多层网络的背景下才得到了普及</b>。在实践中，使用<b>随机梯度下降（SGD），其从连续的相对小的子集中近似取训练集的误差梯度</b>。</p><p>梯度下降算法的主要问题之一是<b>学习率α的选择</b>。学习率太小会导致收敛缓慢，而较大的学习率会导致发散。因此，提出了几种方法来进一步改进简单的随机梯度下降优化方法。最简单的方法称为<b>momentum随机梯度下降</b>。它<b>记录从一次迭代到另一次迭代的更新量，如果梯度指向同一方向，就进一步推动更新</b>，如下： </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-21fecfefa618eee8a096a26568769d4b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"733\" data-rawheight=\"87\" class=\"origin_image zh-lightbox-thumb\" width=\"733\" data-original=\"https://pic4.zhimg.com/v2-21fecfefa618eee8a096a26568769d4b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;733&#39; height=&#39;87&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"733\" data-rawheight=\"87\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"733\" data-original=\"https://pic4.zhimg.com/v2-21fecfefa618eee8a096a26568769d4b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-21fecfefa618eee8a096a26568769d4b_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>用γ控制momentum。另一种简单的方法涉及<b>根据固定的时间表以递减的方式设置学习速率</b>，但这远非理想，因为该时间表必须在训练过程之前预先设定并且完全独立于数据。其他更复杂的方法（例如Adagrad，Adadelta，Adam）建议通过<b>对频繁变化的参数执行较小的更新以及对不频繁变化参数进行更大更新</b>，使训练期间的学习速率适应每个参数wi。在其他地方可以找到这些算法的不同版本之间的详细比较。</p><p>使用梯度下降及其变体进行训练的主要缺点是需要大量标记数据。解决这一困难的一种方法是采用<b>无监督学习</b>。用于训练一些浅层ConvNet架构的无监督方法是基于预测稀疏分解（Predictive Sparse Decomposition）。 Predictive Sparse Decomposition学习一组滤波器的超完备集，其可用于图像重建。该方法特别适用于学习卷积体系结构的参数，因为该算法用于学习重建图像的基函数。具体地，预测稀疏分解（PSD）建立在稀疏编码算法的基础上，该算法试图通过基本子集的线性组合来找到输入信号X的有效特征Y。形式上，稀疏编码的问题转化为最小化问题，如下: </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-6416deb356d5b7bb8b23ba62d274161d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"654\" data-rawheight=\"81\" class=\"origin_image zh-lightbox-thumb\" width=\"654\" data-original=\"https://pic2.zhimg.com/v2-6416deb356d5b7bb8b23ba62d274161d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;654&#39; height=&#39;81&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"654\" data-rawheight=\"81\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"654\" data-original=\"https://pic2.zhimg.com/v2-6416deb356d5b7bb8b23ba62d274161d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-6416deb356d5b7bb8b23ba62d274161d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>PSD采取稀疏编码在卷积框架下最小化重建误差的思想: </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-c568464368eb4eddcd508e3675490140_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"872\" data-rawheight=\"91\" class=\"origin_image zh-lightbox-thumb\" width=\"872\" data-original=\"https://pic1.zhimg.com/v2-c568464368eb4eddcd508e3675490140_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;872&#39; height=&#39;91&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"872\" data-rawheight=\"91\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"872\" data-original=\"https://pic1.zhimg.com/v2-c568464368eb4eddcd508e3675490140_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-c568464368eb4eddcd508e3675490140_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>其中F（X; G，W，D）= G tanh（W X + D），W，D和G分别是网络的权重，偏差和增益（或归一化因子）。 通过最小化方程2.14中定义的损失函数，算法学习表征Y，其重建输入的patch X，类似于预测表征F。由于方程的第二项，学习的表征也将是稀疏的。 在实践中，误差在两个交替步骤中被最小化，其中<b>参数（B，G，W，D）是固定的并且在Y上执行最小化</b>。然后，<b>表征Y被固定，同时最小化其他参数</b>。值得注意的是，PSD应用于一个分段过程中，每个参数集(G, W, D)都是通过从输入图像中重构不同的patch来学习的。 换句话说，通过将重建聚焦在输入图像的不同部分上来学习不同的内核集。</p><h2>2.1.6 迁移学习</h2><p>训练多层网络架构的一个意想不到的好处是，<b>学习到的特征在不同的数据集甚至不同的任务中具有惊人的适应性</b>。例子包括使用网络训练与ImageNet识别:加州理工学院- 101等其他<b>物体识别</b>数据集。其他识别任务,比如<b>纹理识别</b>。其他应用,如<b>物体检测</b>,甚至基于视频的任务,比如<b>视频动作识别</b>。</p><p>多层架构在不同的数据集和任务中提取出来的特征的适应性，可以<b>归因于它们的层次性，从简单的、局部的到抽象的、全局的</b>。因此，在<b>较低层次提取的特征往往在不同任务之间很常见，从而使多层网络结构更易于转移学习</b>。 系统地探索了不同网络和任务中特征的可转移性，揭示了在迁移学习时需要考虑的几个好实践。首先，研究表明，与对整个网络进行微调相比，<b>只对更高层次进行微调可以获得系统更好的性能</b>。其次，本研究表明，<b>任务类型越不同，迁移学习效率越低</b>。第三，更令人惊讶的是，研究人员发现，<b>即使在对网络在初始任务下的性能进行微调之后，网络的性能也不会受到特别的阻碍</b>。</p><p>最近，一些实验试图通过将学习作为一个连续的两步过程来进一步加强网络的迁移学习能力。首先，<b>执行快速学习步骤，其中网络是针对特定任务进行优化的</b>。其次，在<b>全局学习步骤中，参数会进一步更新，试图在不同任务之间最小化错误。</b></p>", 
            "topic": [
                {
                    "tag": "卷积神经网络（CNN）", 
                    "tagLink": "https://api.zhihu.com/topics/20043586"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "计算机视觉", 
                    "tagLink": "https://api.zhihu.com/topics/19590195"
                }
            ], 
            "comments": []
        }
    ], 
    "url": "https://zhuanlan.zhihu.com/c_1035124912373096448"
}
