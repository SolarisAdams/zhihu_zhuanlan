{
    "title": "Cruiser的日常搬砖", 
    "description": "", 
    "followers": [
        "https://www.zhihu.com/people/zhang-lang-cai-jin-sq", 
        "https://www.zhihu.com/people/mu-xin-48-10", 
        "https://www.zhihu.com/people/dawnsx", 
        "https://www.zhihu.com/people/cabbage-im", 
        "https://www.zhihu.com/people/ke-kou-ke-le-61", 
        "https://www.zhihu.com/people/du-san-31", 
        "https://www.zhihu.com/people/shui-mu-qing-hua-88-16", 
        "https://www.zhihu.com/people/wang-kuan-57-47", 
        "https://www.zhihu.com/people/hu-peng-43-17", 
        "https://www.zhihu.com/people/tang-long-30-1", 
        "https://www.zhihu.com/people/marshal-shi-14", 
        "https://www.zhihu.com/people/liu-qi-52-45", 
        "https://www.zhihu.com/people/qiao-xu-26", 
        "https://www.zhihu.com/people/huang-yong-wen-wu", 
        "https://www.zhihu.com/people/aslj", 
        "https://www.zhihu.com/people/yang-xin-hang-26", 
        "https://www.zhihu.com/people/zhang-le-3-67-28", 
        "https://www.zhihu.com/people/wu-wang-ben-better", 
        "https://www.zhihu.com/people/lwltony", 
        "https://www.zhihu.com/people/metapsycho", 
        "https://www.zhihu.com/people/lgc-44-20", 
        "https://www.zhihu.com/people/vulcan-9", 
        "https://www.zhihu.com/people/wwhgood", 
        "https://www.zhihu.com/people/li-guan-yi-17", 
        "https://www.zhihu.com/people/cherrysh", 
        "https://www.zhihu.com/people/den-war", 
        "https://www.zhihu.com/people/liu-lu-34-98", 
        "https://www.zhihu.com/people/mel-tor", 
        "https://www.zhihu.com/people/gozo-43", 
        "https://www.zhihu.com/people/urra-seriq", 
        "https://www.zhihu.com/people/wu-yun-kun", 
        "https://www.zhihu.com/people/zhang-chang-song-55", 
        "https://www.zhihu.com/people/lightdrops", 
        "https://www.zhihu.com/people/ye-cha-4", 
        "https://www.zhihu.com/people/blueman", 
        "https://www.zhihu.com/people/fzjmike", 
        "https://www.zhihu.com/people/shao-lin-xiao-zi-24", 
        "https://www.zhihu.com/people/wang-shuai-jun-66", 
        "https://www.zhihu.com/people/long-jian-1", 
        "https://www.zhihu.com/people/re-qing-de-ren-gong-zhi-zhang", 
        "https://www.zhihu.com/people/17754337872", 
        "https://www.zhihu.com/people/xiao-guai-shou-57-89-47", 
        "https://www.zhihu.com/people/xing-zou-zai-lu-shang-2", 
        "https://www.zhihu.com/people/hu-ru-yuan-xing-ke-23", 
        "https://www.zhihu.com/people/dao-geng-huo-chong-2", 
        "https://www.zhihu.com/people/sky-31-60-98", 
        "https://www.zhihu.com/people/xu-x-83", 
        "https://www.zhihu.com/people/xiao-ai-43-45-82", 
        "https://www.zhihu.com/people/leon-57-66", 
        "https://www.zhihu.com/people/zhao-ben-97", 
        "https://www.zhihu.com/people/wan9009", 
        "https://www.zhihu.com/people/penergy", 
        "https://www.zhihu.com/people/Sunzz528", 
        "https://www.zhihu.com/people/miao-wu-82-61", 
        "https://www.zhihu.com/people/li-dong-hai-34-70", 
        "https://www.zhihu.com/people/liu-xiao-sheng-27", 
        "https://www.zhihu.com/people/frankwang428", 
        "https://www.zhihu.com/people/quan-67-82", 
        "https://www.zhihu.com/people/xiao-yy-45", 
        "https://www.zhihu.com/people/gu-qing-lei-19", 
        "https://www.zhihu.com/people/xiao-fa-94-94", 
        "https://www.zhihu.com/people/jin-zhe-kai", 
        "https://www.zhihu.com/people/tong-kai-lin", 
        "https://www.zhihu.com/people/luozhongbin", 
        "https://www.zhihu.com/people/zhao-da-peng-89", 
        "https://www.zhihu.com/people/a-gui-7", 
        "https://www.zhihu.com/people/qi-che-ren-82", 
        "https://www.zhihu.com/people/tang-yi-fan-87-42", 
        "https://www.zhihu.com/people/zhang-yi-feng-84-78", 
        "https://www.zhihu.com/people/kozzy", 
        "https://www.zhihu.com/people/zheng-kai-lin-5", 
        "https://www.zhihu.com/people/Gtesla-10-49-76", 
        "https://www.zhihu.com/people/wlwsjl", 
        "https://www.zhihu.com/people/bonhomiestriker", 
        "https://www.zhihu.com/people/zheng-rong-8-71", 
        "https://www.zhihu.com/people/happy_siyuzhou", 
        "https://www.zhihu.com/people/cheng-kai-75", 
        "https://www.zhihu.com/people/jin-ding-59", 
        "https://www.zhihu.com/people/wu-long-69", 
        "https://www.zhihu.com/people/xu-yin-da-58", 
        "https://www.zhihu.com/people/fang-chuan-40-19", 
        "https://www.zhihu.com/people/huang-shuai-4", 
        "https://www.zhihu.com/people/crayz2", 
        "https://www.zhihu.com/people/unicorn007981", 
        "https://www.zhihu.com/people/q576333", 
        "https://www.zhihu.com/people/xcxccxxc", 
        "https://www.zhihu.com/people/zhao-jun-peng-44", 
        "https://www.zhihu.com/people/xiao-yu-22-71-20", 
        "https://www.zhihu.com/people/leslie-33-96", 
        "https://www.zhihu.com/people/jester-30", 
        "https://www.zhihu.com/people/xia-mu-38", 
        "https://www.zhihu.com/people/pipi_genius", 
        "https://www.zhihu.com/people/suo-long-16-97", 
        "https://www.zhihu.com/people/a-ti-10", 
        "https://www.zhihu.com/people/mike-miller-34", 
        "https://www.zhihu.com/people/gui-ke-82", 
        "https://www.zhihu.com/people/frank999-1", 
        "https://www.zhihu.com/people/feng-xie-25-44", 
        "https://www.zhihu.com/people/ma-yu-xiang-4", 
        "https://www.zhihu.com/people/caorui1995", 
        "https://www.zhihu.com/people/liu-tao-65-52", 
        "https://www.zhihu.com/people/cun-kou-zhang-sifu", 
        "https://www.zhihu.com/people/serser", 
        "https://www.zhihu.com/people/hahaha-92-99", 
        "https://www.zhihu.com/people/xsbdb-17", 
        "https://www.zhihu.com/people/stongliang", 
        "https://www.zhihu.com/people/liu-yu-qi-5-70", 
        "https://www.zhihu.com/people/ai-ren-5", 
        "https://www.zhihu.com/people/larrymusk", 
        "https://www.zhihu.com/people/yang-zhuo-ran-33-39", 
        "https://www.zhihu.com/people/william-zhu-82", 
        "https://www.zhihu.com/people/wu-ye-nan-30", 
        "https://www.zhihu.com/people/yildhd-wang", 
        "https://www.zhihu.com/people/kerry-wu-96", 
        "https://www.zhihu.com/people/zhong-yong-58-68", 
        "https://www.zhihu.com/people/DerekZhao", 
        "https://www.zhihu.com/people/haihaiye", 
        "https://www.zhihu.com/people/wang-cong-1-22", 
        "https://www.zhihu.com/people/wang-tian-yuan-77", 
        "https://www.zhihu.com/people/yaogang2020", 
        "https://www.zhihu.com/people/zhang-qi-chang-78", 
        "https://www.zhihu.com/people/chen-chen-chen-chen-22-91", 
        "https://www.zhihu.com/people/yan-bing-qing-90", 
        "https://www.zhihu.com/people/yeu-yang", 
        "https://www.zhihu.com/people/jax-59", 
        "https://www.zhihu.com/people/hijackjave", 
        "https://www.zhihu.com/people/Ni_Guo_Chen", 
        "https://www.zhihu.com/people/yiiwood", 
        "https://www.zhihu.com/people/hai-lan-xin", 
        "https://www.zhihu.com/people/smartcat", 
        "https://www.zhihu.com/people/huitan", 
        "https://www.zhihu.com/people/ni-cai-63-87", 
        "https://www.zhihu.com/people/xu-wen-hua-10", 
        "https://www.zhihu.com/people/martin.chou", 
        "https://www.zhihu.com/people/zhihuer2018", 
        "https://www.zhihu.com/people/cheng-yu-jun-17-4", 
        "https://www.zhihu.com/people/cerena-8", 
        "https://www.zhihu.com/people/ju-shang-38", 
        "https://www.zhihu.com/people/zhang-peng-76-23", 
        "https://www.zhihu.com/people/zheng-hao-yuan", 
        "https://www.zhihu.com/people/neal-liu-31", 
        "https://www.zhihu.com/people/xing-ming-55-85", 
        "https://www.zhihu.com/people/Radon86", 
        "https://www.zhihu.com/people/wang-meng-ya-68", 
        "https://www.zhihu.com/people/xiong-xiao-94", 
        "https://www.zhihu.com/people/moni-gg", 
        "https://www.zhihu.com/people/ysgc", 
        "https://www.zhihu.com/people/geng-dong-dong-80", 
        "https://www.zhihu.com/people/itwob", 
        "https://www.zhihu.com/people/DongShengYang", 
        "https://www.zhihu.com/people/zhaojianzhung", 
        "https://www.zhihu.com/people/cui-colby", 
        "https://www.zhihu.com/people/shao-ze-28", 
        "https://www.zhihu.com/people/skdajdka", 
        "https://www.zhihu.com/people/xiao-jian-zhi-63", 
        "https://www.zhihu.com/people/hoffman-29", 
        "https://www.zhihu.com/people/delimma-91-18", 
        "https://www.zhihu.com/people/zhang-qin-sheng-sheng", 
        "https://www.zhihu.com/people/jia-wu-33", 
        "https://www.zhihu.com/people/libin-sui", 
        "https://www.zhihu.com/people/huang-chao-lin-6", 
        "https://www.zhihu.com/people/shui-ma-xi-mu-dong", 
        "https://www.zhihu.com/people/li-jia-hong-95", 
        "https://www.zhihu.com/people/li-miaobo", 
        "https://www.zhihu.com/people/ha-ha-85-81", 
        "https://www.zhihu.com/people/hailong-jiang", 
        "https://www.zhihu.com/people/lu-xian-sheng-61-6", 
        "https://www.zhihu.com/people/nu-li-que-chang-chang-shi-bai", 
        "https://www.zhihu.com/people/li-xin-95-42-23", 
        "https://www.zhihu.com/people/hellhere", 
        "https://www.zhihu.com/people/charmyoung", 
        "https://www.zhihu.com/people/tx-fan"
    ], 
    "article": [
        {
            "url": "https://zhuanlan.zhihu.com/p/56631451", 
            "userName": "Cruiser", 
            "userLink": "https://www.zhihu.com/people/c9a437c07539d39980fd01ed38fc7e10", 
            "upvote": 9, 
            "title": "又双叒一篇Sim2Real力作", 
            "content": "<blockquote><i>Please stand firm and hold the paper review. Mind the gap between the Stage[1] and real world. —— 沃兹基硕德</i></blockquote><p><a href=\"https://zhuanlan.zhihu.com/p/30544417\" class=\"internal\">之前的文章</a>[2]中介绍过Sim-to-Real相关领域的工作，目前主流的方法主要分为两类，即领域自适应方法（domain adaptation）和领域随机化方法（domain randomization）[3]，其中前者是将从仿真环境中学到的模型迁移到真实场景，即跨模型迁移（transfer across dynamics），而后者是针对特征进行随机化，即跨观测数据迁移（transfer across observations）。</p><p>UC Berkeley的Abbeel大神和Levine大神最近放出的Sim2Real论文[4]中提出了一种结合仿真数据和真实环境采样得到的数据来训练无人机进行避障的DRL算法，使用真实环境采样数据对模型本身进行学习，包括机器人的物理特性和环境交互模型等，而利用仿真数据来学习如何将感知模型进行泛化。该研究并没有使用真实环境采样数据对网络进行fine-tuning，而是将模型分为感知和控制两部分，感知模块负责迁移仿真中得到的视觉特征，而控制模块负责训练采样得到的数据。不同于以往方法通过减少仿真和真实之间的策略过拟合来改善迁移效果，该方法试图利用少量真实数据来将从仿真中学习得到的模型自适应到真实环境。</p><p>首先我们回顾下真实采样数据和仿真数据的分布特点，1）真实采样数据能较准确地反应环境交互特征，但是由于采样困难，导致无法覆盖所有情况；2）仿真数据简单易得，也更安全，但是往往由于模型不够精确或者参数误差，导致其不能反应真实情况。因此，该研究采用将采样数据和仿真数据结合的方法。</p><h2>Real-World策略学习</h2><p>考虑到真实环境采样困难，该研究采用样本效率高的策略学习算法，泛化计算图算法，并将该计算图构建为基于动作的奖励预测器。该预测器 <img src=\"https://www.zhihu.com/equation?tex=G_%5Ctheta%28s_t%2CA_t%5EH%29\" alt=\"G_\\theta(s_t,A_t^H)\" eeimg=\"1\"/> 将当前状态 <img src=\"https://www.zhihu.com/equation?tex=s_t\" alt=\"s_t\" eeimg=\"1\"/> 和规划动作序列 <img src=\"https://www.zhihu.com/equation?tex=A_t%5EH%3D%5C%7Ba_t%2C...%2Ca_%7Bt%2BH-1%7D%5C%7D\" alt=\"A_t^H=\\{a_t,...,a_{t+H-1}\\}\" eeimg=\"1\"/> 作为输入，将未来每个时刻的奖励预测序列 <img src=\"https://www.zhihu.com/equation?tex=%5Chat%7BR%7D_t%5EH%3D%5C%7B%5Chat%7Br%7D_t%2C...%2C%5Chat%7Br%7D_%7Bt%2BH-1%7D%5C%7D\" alt=\"\\hat{R}_t^H=\\{\\hat{r}_t,...,\\hat{r}_{t+H-1}\\}\" eeimg=\"1\"/> 作为预测输出。</p><p>在训练阶段，该预测器利用真实采样数据 <img src=\"https://www.zhihu.com/equation?tex=D%5E%7BRW%7D\" alt=\"D^{RW}\" eeimg=\"1\"/> 、按照监督学习的方式进行学习，其目标是最小化预测输出与真实采样奖励序列的差，</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Ctheta%5E%2A%3D%5Carg%5Cmin_%5Ctheta%5Csum_%7B%28s_t%2CA_t%5EH%2CR_t%5EH%29%5Cin+D%5E%7BRW%7D%7D%7C%7CG_%5Ctheta%28s_t%2CA_t%5EH%29-R_t%5EH%7C%7C%5E2\" alt=\"\\theta^*=\\arg\\min_\\theta\\sum_{(s_t,A_t^H,R_t^H)\\in D^{RW}}||G_\\theta(s_t,A_t^H)-R_t^H||^2\" eeimg=\"1\"/> </p><p>测试阶段利用该预测器来实现有限时域最优控制器，根据当前状态选择动作序列，以最大化未来奖励的预测值，即</p><p><img src=\"https://www.zhihu.com/equation?tex=A%5E%2A%3D%5Carg%5Cmax_A%5Csum_%7Bh%3D0%7D%5E%7BH-1%7D%5Cgamma%5Eh%5Chat%7Br%7D_%7Bt%2Bh%7D\" alt=\"A^*=\\arg\\max_A\\sum_{h=0}^{H-1}\\gamma^h\\hat{r}_{t+h}\" eeimg=\"1\"/> </p><p>该过程实际上是一个基于模型（model-based）的算法：</p><ol><li>训练阶段类似于Q-learning，都是对值函数进行估计来预测未来奖励，不同之处在于Q-learning采用表格（tabular Q-learning）或网络（DQN），通过Bellman方程来进行迭代更新，而该方法退化成了标准的监督学习，样本效率高；</li><li>测试阶段类似模型预测控制方法（MPC），同样是对 <img src=\"https://www.zhihu.com/equation?tex=H\" alt=\"H\" eeimg=\"1\"/> 步内的奖励值进行预测，只不过该模型是通过状态转移数据来学习得到的，且状态维度很高。</li></ol><h2>仿真特征迁移</h2><p>考虑到小样本监督学习带来的过拟合，以及无人机图像由于光照和角度等影响带来的多样性因素，上述控制模块提供的预测器必然无法泛化到其他场景，该研究采用在仿真器中学习针对任务的模型，并将模型中的感知特征迁移到真实环境策略的方法。</p><p>针对任务的模型采用Q-learning来进行训练，值网络模型与上述预测器类似，如下图所示，不同之处在于其包含三个不同的网络模块：</p><ul><li>感知模块，包括一个用于处理输入图像的卷积网络</li><li>动作模块，包括一个处理动作的全连接网络</li><li>值函数模块，包括一个结合经过预处理的图像和动作的全连接网络，输出最终的Q值</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-2a426e47dcd62d980abe46436b4bf1a3_b.jpg\" data-size=\"normal\" data-rawwidth=\"1021\" data-rawheight=\"435\" class=\"origin_image zh-lightbox-thumb\" width=\"1021\" data-original=\"https://pic4.zhimg.com/v2-2a426e47dcd62d980abe46436b4bf1a3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1021&#39; height=&#39;435&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1021\" data-rawheight=\"435\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1021\" data-original=\"https://pic4.zhimg.com/v2-2a426e47dcd62d980abe46436b4bf1a3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-2a426e47dcd62d980abe46436b4bf1a3_b.jpg\"/><figcaption>预测器和值函数网络模型</figcaption></figure><p>迁移部分是典型的迁移学习方法，利用上述值网络训练好的感知层参数来初始化预测器的感知层，并且在进行策略学习的过程中保持其参数不变（也可以利用真实采样数据进行fine-tuning），以此来进行感知特征的迁移。</p><h2>训练过程</h2><p>训练过程分为三个部分，即</p><ul><li>利用DRL在仿真环境中训练值函数网络</li><li>构建基于动作的值预测模型，使用仿真训练模型的感知层来处理输入图像</li><li>利用真实采样数据训练值预测模型</li></ul><p>模型和仿真参数不再详细介绍，感兴趣的童鞋可参考论文和<a href=\"https://link.zhihu.com/?target=https%3A//github.com/gkahn13/GtS\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">代码</a>。</p><p>整篇论文的思想不复杂，更像是一个大杂烩，然鹅其为Sim2Real研究提供了一个有趣的视角，实现起来也不麻烦，个人觉得比单纯的利用fine-tuning来做迁移学习要简单得多，针对较为简单的环境来说效果也是很不错的，具体效果可骑墙观看<a href=\"https://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3DRb2a6lSQSas\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">油管视频</a>。但是既然采用了有限样本的监督学习来做预测器，能否真的泛化到室外或者较复杂的环境，以及能否应用到更复杂的控制系统，那就不好说了，拭目以待咯。</p><blockquote>[1] A famous robot simulation tool. <a href=\"https://link.zhihu.com/?target=https%3A//player-stage-manual.readthedocs.io/en/latest/INTRO/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Player/Stage Intro</a><br/>[2] 从仿真学习到实战演练：两篇Robot Learning论文的阅读笔记. <a href=\"https://www.zhihu.com/people/dragoncuiser/posts\" class=\"internal\">Cruiser专栏</a><br/>[3]  Deep Reinforcement Learning and Control: <a href=\"https://link.zhihu.com/?target=https%3A//katefvision.github.io/katefSlides/sim2real.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Sim2Real</a><br/>[4] Generalization through Simulation: Integrating Simulated and Real Data into Deep Reinforcement Learning for Vision-Based Autonomous Flight. <a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1902.03701\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">arXiv</a></blockquote>", 
            "topic": [
                {
                    "tag": "机器人", 
                    "tagLink": "https://api.zhihu.com/topics/19551273"
                }, 
                {
                    "tag": "强化学习 (Reinforcement Learning)", 
                    "tagLink": "https://api.zhihu.com/topics/20039099"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/52327137", 
            "userName": "Cruiser", 
            "userLink": "https://www.zhihu.com/people/c9a437c07539d39980fd01ed38fc7e10", 
            "upvote": 16, 
            "title": "Deep RL关键论文一览（转自OpenAI）", 
            "content": "<p>OpenAI近期在其<a href=\"https://link.zhihu.com/?target=https%3A//github.com/openai/spinningup\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">spinningup</a>教学项目中列出了深度强化学习领域中的关键论文，以方便想要了解和进入该领域及其发展历程的研究者。该清单不仅包含了经典的DQN，DDPG，A3C，PPO等算法，同时也有与Meta Learning，Imitation Learning等结合的RL^2、GAIL和DeepMimic等前沿研究，还有考虑DRL安全性和可重复性的一些论文。感兴趣的研究者也可以参考其教学项目中关于一些经典算法的实现和介绍。</p><p>以下是列表的目录，详情请参考<b><a href=\"https://link.zhihu.com/?target=https%3A//spinningup.openai.com/en/latest/spinningup/keypapers.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">原文列表</a>。</b></p><h2>目录</h2><ul><li><a href=\"https://link.zhihu.com/?target=https%3A//spinningup.openai.com/en/latest/spinningup/keypapers.html%23model-free-rl\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">1. Model-Free RL</a></li><li><a href=\"https://link.zhihu.com/?target=https%3A//spinningup.openai.com/en/latest/spinningup/keypapers.html%23exploration\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">2. Exploration</a></li><li><a href=\"https://link.zhihu.com/?target=https%3A//spinningup.openai.com/en/latest/spinningup/keypapers.html%23transfer-and-multitask-rl\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">3. Transfer and Multitask RL</a></li><li><a href=\"https://link.zhihu.com/?target=https%3A//spinningup.openai.com/en/latest/spinningup/keypapers.html%23hierarchy\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">4. Hierarchy</a></li><li><a href=\"https://link.zhihu.com/?target=https%3A//spinningup.openai.com/en/latest/spinningup/keypapers.html%23memory\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">5. Memory</a></li><li><a href=\"https://link.zhihu.com/?target=https%3A//spinningup.openai.com/en/latest/spinningup/keypapers.html%23model-based-rl\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">6. Model-Based RL</a></li><li><a href=\"https://link.zhihu.com/?target=https%3A//spinningup.openai.com/en/latest/spinningup/keypapers.html%23meta-rl\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">7. Meta-RL</a></li><li><a href=\"https://link.zhihu.com/?target=https%3A//spinningup.openai.com/en/latest/spinningup/keypapers.html%23scaling-rl\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">8. Scaling RL</a></li><li><a href=\"https://link.zhihu.com/?target=https%3A//spinningup.openai.com/en/latest/spinningup/keypapers.html%23rl-in-the-real-world\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">9. RL in the Real World</a></li><li><a href=\"https://link.zhihu.com/?target=https%3A//spinningup.openai.com/en/latest/spinningup/keypapers.html%23safety\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">10. Safety</a></li><li><a href=\"https://link.zhihu.com/?target=https%3A//spinningup.openai.com/en/latest/spinningup/keypapers.html%23imitation-learning-and-inverse-reinforcement-learning\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">11. Imitation Learning and Inverse Reinforcement Learning</a></li><li><a href=\"https://link.zhihu.com/?target=https%3A//spinningup.openai.com/en/latest/spinningup/keypapers.html%23reproducibility-analysis-and-critique\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">12. Reproducibility, Analysis, and Critique</a></li><li><a href=\"https://link.zhihu.com/?target=https%3A//spinningup.openai.com/en/latest/spinningup/keypapers.html%23bonus-classic-papers-in-rl-theory-or-review\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">13. Bonus: Classic Papers in RL Theory or Review</a></li></ul>", 
            "topic": [
                {
                    "tag": "强化学习 (Reinforcement Learning)", 
                    "tagLink": "https://api.zhihu.com/topics/20039099"
                }, 
                {
                    "tag": "机器人", 
                    "tagLink": "https://api.zhihu.com/topics/19551273"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/51816784", 
            "userName": "Cruiser", 
            "userLink": "https://www.zhihu.com/people/c9a437c07539d39980fd01ed38fc7e10", 
            "upvote": 25, 
            "title": "结合DL和Sampling-based的运动规划", 
            "content": "<p>新鲜出炉的运动规划论文，<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1812.01127.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Learning to Predict Ego-Vehicle Poses for Sampling-Based Nonholonomic Motion Planning</a>，来自博世下属研究机构。</p><p>该文在技术上并没有太大创新，但是为解决non-holonomic机器人/汽车的运动规划问题提供了一个相当有趣的角度。针对对环境适应性和安全性要求极高的自动驾驶问题，基于采样的运动规划很难保证得到最优解。该文将静态障碍物、位置环境、历史运动轨迹、当前姿态和目标姿态编码成5个256x256的图像，经过语义分割网络SegNet进行编解码，输出四个相同尺寸的图像，并将其分别用于预测路径点位置以及航向角。经过采样的预测位姿利用曲率连续函数连接，并进一步用于BiRRT*的初始化。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-fbdcd4bc3a701a98446fc5551abf6d28_b.jpg\" data-size=\"normal\" data-rawwidth=\"1051\" data-rawheight=\"458\" class=\"origin_image zh-lightbox-thumb\" width=\"1051\" data-original=\"https://pic1.zhimg.com/v2-fbdcd4bc3a701a98446fc5551abf6d28_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1051&#39; height=&#39;458&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1051\" data-rawheight=\"458\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1051\" data-original=\"https://pic1.zhimg.com/v2-fbdcd4bc3a701a98446fc5551abf6d28_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-fbdcd4bc3a701a98446fc5551abf6d28_b.jpg\"/><figcaption>SegNet预测网络</figcaption></figure><p>作者认为相比于基于均匀采样的BiRRT<i>*和最新的A*</i>算法，该预测网络可以加速算法收敛，提高规划成功率，更精确地预测车辆轨迹，同时具有良好的泛化能力，在全新的场景中不需要任何调参也可以达到不错的效果。该文的实现过程同样很有意思（可能是我关注此方面不多，哈哈哈）。</p><h2>数据生成</h2><p>仿真数据利用ROS+Gazebo来完成，主要关注低速和障碍物较多的环境，共采样了13418条轨迹。首先，利用基于A<i>*启发的BiRRT*</i>规划器来产生一条从起始点到目标点的曲率连续、无障碍路径；然后在控制器执行路径期间，对各个时间点上的位置、朝向、曲率和速度进行采样；最后利用激光雷达数据来生成60mx60m、精度为10cm的栅格图作为地图。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>网络构建</h2><p>下图中的(a)-(e)作为网络输入，(f)-(h)作为标签，共同用于网络训练。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-ff43beb2a780046d3cc55ade9083e525_b.jpg\" data-size=\"normal\" data-rawwidth=\"1512\" data-rawheight=\"515\" class=\"origin_image zh-lightbox-thumb\" width=\"1512\" data-original=\"https://pic2.zhimg.com/v2-ff43beb2a780046d3cc55ade9083e525_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1512&#39; height=&#39;515&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1512\" data-rawheight=\"515\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1512\" data-original=\"https://pic2.zhimg.com/v2-ff43beb2a780046d3cc55ade9083e525_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-ff43beb2a780046d3cc55ade9083e525_b.jpg\"/><figcaption>输入和输出</figcaption></figure><p>网络预测输出格式如下图所示：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-cbbdd29a3cb6ea023d3f24579a9ca3a1_b.jpg\" data-size=\"normal\" data-rawwidth=\"232\" data-rawheight=\"228\" class=\"content_image\" width=\"232\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;232&#39; height=&#39;228&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"232\" data-rawheight=\"228\" class=\"content_image lazy\" width=\"232\" data-actualsrc=\"https://pic2.zhimg.com/v2-cbbdd29a3cb6ea023d3f24579a9ca3a1_b.jpg\"/><figcaption>预测输出</figcaption></figure><p>图中点的密度表示该网格属于预测路径上一点的概率；上图中(g)和(h)分别作为航向角的sine和cosine部分。这里航向角为什么要拆成sine和cosine两部分呢？作者提出了三个好处：</p><ul><li>sine和cosine均为0，表示该点信息无效，标注为无航向角信息；</li><li>sine和cosine部分的正则可作为预测值的置信概率，即 <img src=\"https://www.zhihu.com/equation?tex=%5Csin%5E2%28%5Calpha%29%2B%5Ccos%5E2%28%5Calpha%29%5Crightarrow+1\" alt=\"\\sin^2(\\alpha)+\\cos^2(\\alpha)\\rightarrow 1\" eeimg=\"1\"/> 时越可信；</li><li>航向角的预测由分类问题转化为回归问题，减小了输出的维度。</li></ul><h2>位姿采样</h2><p>上文已经提到，网络输出将用于采样，并作为BiRRT*规划器的初始值。采样工作同样分为几个步骤：</p><ul><li>利用低方差采样，从轨迹预测中随机采样 <img src=\"https://www.zhihu.com/equation?tex=N\" alt=\"N\" eeimg=\"1\"/> 个位姿；</li><li>从上一步采样得到的位姿中均匀采样，得到连续的位姿预测值 <img src=\"https://www.zhihu.com/equation?tex=%28x_%7Bpred%7D%5E%7B%5Bi%5D%7D%2C+y_%7Bpred%7D%5E%7B%5Bi%5D%7D%2C+%5Ctheta_%7Bpred%7D%5E%7B%5Bi%5D%7D%29\" alt=\"(x_{pred}^{[i]}, y_{pred}^{[i]}, \\theta_{pred}^{[i]})\" eeimg=\"1\"/> ；</li><li>注意，采样时仅采样置信概率大于0.5的栅格。</li></ul><h2>模型训练</h2><p>接下来就是训练过程了。在此作者采用了data augmentation，在采样的轨迹上随机选择不同的起始状态，生成新的轨迹，共得到807273条不同的轨迹。网络的损失函数主要分为两部分，其一是预测轨迹与ground truth轨迹的偏差，</p><p><img src=\"https://www.zhihu.com/equation?tex=d%28x_t%2Cx_%7Bpred%7D%5E%7B%5Bi%5D%7D%29%3Dw_%7Bpos%7D%7C%7Cx_%7Bpred%7D%5E%7B%5Bi%5D%7D-x_t%7C%7C_2%2Bw_%7B%5Ctheta%7D%7C%5Ctheta_%7Bpred%7D%5E%7B%5Bi%5D%7D-%5Ctheta_t%7C\" alt=\"d(x_t,x_{pred}^{[i]})=w_{pos}||x_{pred}^{[i]}-x_t||_2+w_{\\theta}|\\theta_{pred}^{[i]}-\\theta_t|\" eeimg=\"1\"/> </p><p>第二部分是衡量预测位姿的均匀分布程度，即预测位姿之间的最大距离。最终的损失函数为：</p><p><img src=\"https://www.zhihu.com/equation?tex=L%3D%5Csum_%7Bc%5Cin+C%7D%28f_%7BCE%7D%28c%29L_%7BCE%7D%28c%29%2Bf_%7BMSE%7D%28c%29L_%7BMSE%7D%28c%29%29%2B%5Csum_%7Bw%5Cin+W%7D%5Cfrac%7B%5Clambda+w%5E2%7D%7B2%7D\" alt=\"L=\\sum_{c\\in C}(f_{CE}(c)L_{CE}(c)+f_{MSE}(c)L_{MSE}(c))+\\sum_{w\\in W}\\frac{\\lambda w^2}{2}\" eeimg=\"1\"/> </p><h2>效果</h2><p>作者在新场景下将该算法分别与基于均匀采样和基于A<i>*的BiRRT*</i>（OSE）算法作对比，得出的结论是该算法可以大幅度提高规划效率，同时保证了不俗的规划成功率。详细的效果可以科学上网自行观看<a href=\"https://link.zhihu.com/?target=https%3A//youtu.be/FZPn3OHdQxk\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">油管</a>视频。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-fe215fce60f7c962aedf73b08d6fdea0_b.jpg\" data-size=\"normal\" data-rawwidth=\"1583\" data-rawheight=\"463\" class=\"origin_image zh-lightbox-thumb\" width=\"1583\" data-original=\"https://pic1.zhimg.com/v2-fe215fce60f7c962aedf73b08d6fdea0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1583&#39; height=&#39;463&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1583\" data-rawheight=\"463\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1583\" data-original=\"https://pic1.zhimg.com/v2-fe215fce60f7c962aedf73b08d6fdea0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-fe215fce60f7c962aedf73b08d6fdea0_b.jpg\"/><figcaption>效果对比</figcaption></figure><p>那么，问题就来了，我们是否可以利用DRL来做这项工作呢？那自然是可以的，不知道哪位小伙伴感兴趣呢？</p>", 
            "topic": [
                {
                    "tag": "运动规划（Motion Planning）", 
                    "tagLink": "https://api.zhihu.com/topics/20042609"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "强化学习 (Reinforcement Learning)", 
                    "tagLink": "https://api.zhihu.com/topics/20039099"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50214427", 
            "userName": "Cruiser", 
            "userLink": "https://www.zhihu.com/people/c9a437c07539d39980fd01ed38fc7e10", 
            "upvote": 85, 
            "title": "DRL in Robot Navigation", 
            "content": "<p>虽然端到端的无人驾驶在前两年备受瞩目，网友基于GTA的无人驾驶仿真&amp;在线训练直播却也随着大潮退去逐渐无人关注。本文旨在对近几年DRL在机器人导航中的研究进行简单的梳理，并不涉及端到端的无人驾驶研究。</p><p>大概是乘了DRL和无人驾驶热潮的东风，基于DRL的机器人导航在近两年开始变得大火。笔者对该领域的兴趣始于MIT控制大佬Jonathan P. How的博士生Yufan Chen的两篇论文：</p><ul><li>《<a href=\"https://link.zhihu.com/?target=http%3A//cn.arxiv.org/abs/1609.07845\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Decentralized Non-communicating Multiagent Collision Avoidance with Deep Reinforcement Learning</a>》</li><li>《<a href=\"https://link.zhihu.com/?target=http%3A//cn.arxiv.org/abs/1703.08862\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Socially Aware Motion Planning with Deep Reinforcement Learning</a>》</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-7f983941f288c142d955d11e42a055ee_b.gif\" data-size=\"normal\" data-rawwidth=\"444\" data-rawheight=\"250\" data-thumbnail=\"https://pic3.zhimg.com/v2-7f983941f288c142d955d11e42a055ee_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"444\" data-original=\"https://pic3.zhimg.com/v2-7f983941f288c142d955d11e42a055ee_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;444&#39; height=&#39;250&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"444\" data-rawheight=\"250\" data-thumbnail=\"https://pic3.zhimg.com/v2-7f983941f288c142d955d11e42a055ee_b.jpg\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"444\" data-original=\"https://pic3.zhimg.com/v2-7f983941f288c142d955d11e42a055ee_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-7f983941f288c142d955d11e42a055ee_b.gif\"/><figcaption>Socially Aware Navigation with DRL</figcaption></figure><p>这两篇文章将所有的状态和输入都转换到机器人本体坐标系中，将自身状态和临近个体的估计状态（包括位置、速度和尺寸等信息）作为输入，考虑了其他个体运动的不确定性，构建了一个针对时间估计的值网络，同时后者还根据机器人的行为准则（如靠右行驶）制定了奖赏函数。利用离线训练来做机器人避障和运动规划确实是个挺有意思的想法，而且不得不说效果出其意料的好，特别是针对复杂的动态环境，如人流量大的校园、展览会等。值得注意的是，其采用了传统的基于VO的多机器人避障算法ORCA来生成初始的轨迹采样，对网络参数进行了预训练。从某个角度上来说，这几篇论文是真正意义上实现了复杂动态环境下，从传感器数据、聚类分析，到预训练和仿真训练，再到终端输出的一整套的多机器人动态避障方案。</p><p>紧接着，同实验室的Michael Everett在其论文《<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1805.01956.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Motion Planning Among Dynamic, Decision-Making Agents with Deep Reinforcement Learning</a>》中做了部分改进，不再假设其他个体的行为准则，换句话说就是假设对方并不合作（不采取相同的避让策略；同时加入了LSTM对其他任意个体的状态进行预测，还抛弃了昂贵的多线激光雷达，转而使用单线。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-43b609a853c1916dc0eb2fcd702be367_b.jpg\" data-size=\"normal\" data-rawwidth=\"743\" data-rawheight=\"349\" class=\"origin_image zh-lightbox-thumb\" width=\"743\" data-original=\"https://pic4.zhimg.com/v2-43b609a853c1916dc0eb2fcd702be367_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;743&#39; height=&#39;349&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"743\" data-rawheight=\"349\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"743\" data-original=\"https://pic4.zhimg.com/v2-43b609a853c1916dc0eb2fcd702be367_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-43b609a853c1916dc0eb2fcd702be367_b.jpg\"/><figcaption>网络架构</figcaption></figure><p>更贴心的是，Michael（让我们为大兄弟欢呼）在其硕士论文《<a href=\"https://link.zhihu.com/?target=https%3A//dspace.mit.edu/handle/1721.1/111698\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Robot Designed for Socially Acceptable Navigation</a>》中详细描述了从硬件架构到算法开发的一整套流程，同时开源了部分代码<a href=\"https://link.zhihu.com/?target=https%3A//github.com/mfe7/cadrl_ros\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">cadrl_ros</a>。然而，单线雷达的问题在于仅仅将其作为动态聚类的输入很难得到不错的聚类效果，导致对其他个体的状态预测不是很准。</p><p>接下来是来自港科大的刘明教授，他们实验室与业内大佬Wolfram Burgard教授也有不少相关方向的合作论文，并且从2016年起就开始关注深度学习在机器人避障中的应用。《<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1703.00420.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Virtual-to-real deep reinforcement learning: Continuous control of mobile robots for mapless navigation</a>》中为了弥合仿真和现实的差距（sim-to-real gap），同时降低状态空间的维度，构造了一套端到端的、不依赖地图的运动控制方法。其中作为唯一感知数据的单线雷达数据只有固定的10个维度，因而其在仿真训练后不需要fine-tuning也可以在实际场景中达到不错的效果。但是这也造成了其不适用于较复杂的场景，且在实际应用过程中对雷达数据变化极其敏感，容易产生震荡。今年的一篇《<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1804.00456.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Curiosity-driven Exploration for Mapless Navigation with Deep Reinforcement Learning</a>》显然更有意思。以往的DRL更多地关注局部避障，或者依赖全局规划来提供subgoal/primitives，而该研究结合了Intrinsic Curiosity Module，通过构建模型、模型预测和反馈（是不是很眼熟？），将预测状态与ground truth的差值作为内在激励。因此，当前网络 <img src=\"https://www.zhihu.com/equation?tex=%5Cphi\" alt=\"\\phi\" eeimg=\"1\"/> 利用已学到的知识对下一步状态进行预测时，其预测误差越大，则激励越强。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-a2800370b6e491c4b1d31ba4fd595994_b.jpg\" data-size=\"normal\" data-rawwidth=\"734\" data-rawheight=\"311\" class=\"origin_image zh-lightbox-thumb\" width=\"734\" data-original=\"https://pic1.zhimg.com/v2-a2800370b6e491c4b1d31ba4fd595994_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;734&#39; height=&#39;311&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"734\" data-rawheight=\"311\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"734\" data-original=\"https://pic1.zhimg.com/v2-a2800370b6e491c4b1d31ba4fd595994_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-a2800370b6e491c4b1d31ba4fd595994_b.jpg\"/><figcaption>Intrinsic Curiosity Module</figcaption></figure><p>当然，整体的导航还依赖外部激励，诸如到达目标点、避障、距离目标点远近等等，最终达到的效果还是不错的。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-a388e9902780fdbda5a31d063f6495b5_b.gif\" data-size=\"normal\" data-rawwidth=\"444\" data-rawheight=\"250\" data-thumbnail=\"https://pic2.zhimg.com/v2-a388e9902780fdbda5a31d063f6495b5_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"444\" data-original=\"https://pic2.zhimg.com/v2-a388e9902780fdbda5a31d063f6495b5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;444&#39; height=&#39;250&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"444\" data-rawheight=\"250\" data-thumbnail=\"https://pic2.zhimg.com/v2-a388e9902780fdbda5a31d063f6495b5_b.jpg\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"444\" data-original=\"https://pic2.zhimg.com/v2-a388e9902780fdbda5a31d063f6495b5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-a388e9902780fdbda5a31d063f6495b5_b.gif\"/><figcaption>Curiosity-driven Navigation</figcaption></figure><p>今年他们也出了一篇该领域内的综述文章《<a href=\"https://link.zhihu.com/?target=https%3A//pdfs.semanticscholar.org/8d21/8715b292f423799bb574e24bd03a6fb861dd.pdf%3F_ga%3D2.188046382.729664455.1541688024-1254579388.1539689560\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">A Survey of Deep Network Solutions for Learning Control in Robotics: From Reinforcement to Imitation</a>》，对近两年DL/DRL在机器人控制，特别是移动机器人导航和机械臂控制中的理论创新和应用，进行了相当水准的总结，同时对Imitation Learning进行了探索。算是对数年前发表的《<a href=\"https://link.zhihu.com/?target=https%3A//www.ias.informatik.tu-darmstadt.de/uploads/Publications/Kober_IJRR_2013.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Reinforcement Learning in Robotics: A Survey</a>》的一个技术更新吧。</p><p>同时，表现亮眼的还有港中文的潘佳教授团队，他们今年在ICRA上发表了《<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1709.10082.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Towards Optimally Decentralized Multi-Robot Collision Avoidance via Deep Reinforcement Learning</a>》一文，该作在前文的基础上做了很多工程化的改进，根据环境信息进行场景分类，从而采取不同的运动策略，以提高效率和保证安全，其视频中的展示效果也是很不错的。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-c2c6aa775f5d309a304afa945150c5ff_b.gif\" data-size=\"normal\" data-rawwidth=\"444\" data-rawheight=\"250\" data-thumbnail=\"https://pic4.zhimg.com/v2-c2c6aa775f5d309a304afa945150c5ff_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"444\" data-original=\"https://pic4.zhimg.com/v2-c2c6aa775f5d309a304afa945150c5ff_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;444&#39; height=&#39;250&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"444\" data-rawheight=\"250\" data-thumbnail=\"https://pic4.zhimg.com/v2-c2c6aa775f5d309a304afa945150c5ff_b.jpg\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"444\" data-original=\"https://pic4.zhimg.com/v2-c2c6aa775f5d309a304afa945150c5ff_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-c2c6aa775f5d309a304afa945150c5ff_b.gif\"/><figcaption>Hybird Control</figcaption></figure><p>渐渐地，随着Model-based RL的崛起，除了上述的Model-free方法外，结合模型预测控制（MPC）的DRL也开始崭露头角。相较于端到端的控制方法，MPC对于加速模型收敛和预防输出震荡（这一点对于提高机器人速度至关重要）有着良好的表现。针对Model-free的一系列问题，结合Imatition Learning和Few shot learning的Meta RL也被认为是未来的出路之一。</p>", 
            "topic": [
                {
                    "tag": "机器人", 
                    "tagLink": "https://api.zhihu.com/topics/19551273"
                }, 
                {
                    "tag": "强化学习 (Reinforcement Learning)", 
                    "tagLink": "https://api.zhihu.com/topics/20039099"
                }
            ], 
            "comments": [
                {
                    "userName": "delimma", 
                    "userLink": "https://www.zhihu.com/people/9656f79265d73224f2539d73b88c847e", 
                    "content": "赞一个！", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "知乎用户", 
                    "userLink": "https://www.zhihu.com/people/0", 
                    "content": "MIT那个速度快不少", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "叫我童童童", 
                    "userLink": "https://www.zhihu.com/people/ce3b59afe00cc31212d0fe6ab8c69bc1", 
                    "content": "<p>请问《<a href=\"http://link.zhihu.com/?target=https%3A//arxiv.org/abs/1703.00420.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Virtual-to-real</a>》这篇论文，怎么理解“实际应用过程中对雷达数据变化极其敏感，容易产生震荡”？</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "娴儿娴儿你快跑", 
                    "userLink": "https://www.zhihu.com/people/15198cf890c82b72ea5ef07bd445a54c", 
                    "content": "<p>请问，这些动图我怎么才能保存成gif格式呢？</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/30544417", 
            "userName": "Cruiser", 
            "userLink": "https://www.zhihu.com/people/c9a437c07539d39980fd01ed38fc7e10", 
            "upvote": 5, 
            "title": "从仿真学习到实战演练：两篇Robot Learning论文的阅读笔记", 
            "content": "<blockquote>本文仅供交流，如有理解不到位的地方欢迎讨论。</blockquote><p>今天要介绍的两篇论文均来自OpenAI大神Pieter Abbeel项目组。其一是《<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1710.06537\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Sim-to-Real Transfer of Robotic Control with Dynamics Randomization</a>》[1]，其二是《<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1710.06542\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Asymmetric Actor Critic for Image-Based Robot Learning</a>》[2]。两篇论文在同一天上传到arXiv上，且采用了近乎相同的手法来处理从仿真到实际的问题，因而在此一并学习。</p><h2>Sim-to-Real Transfer of Robotic Control with Dynamics Randomization</h2><p>先说第一篇Sim-to-Real Transfer of Robotic Control with Dynamics Randomization。</p><p>Google在其论文《<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1603.02199.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection</a>》[3]中通过大手笔向我们证明了他们的暴（cai）力（da）美（qi）学（cu），人们已经意识到在机器人领域中采用仿真这一重要手段的意义。然而考虑到仿真误差和仿真器的特性，在仿真中得到的训练结果在应用到实际操作环境中常常会不尽如人意。文章在仿真训练时考虑了随机化的动态特性，使得其训练得到的策略能够直接应用在物理环境中而不需要额外的训练。文章最后通过将物体推到指定位置这一例子对方法进行说明。实际试验中在物体下放了一个袋子，从而改变了物体的动力学特性，结果机器人仍可以完成任务。</p><figure><noscript><img src=\"https://pic4.zhimg.com/v2-80b245fd4291e62cffa8f252d4d72fcf_b.jpg\" data-rawwidth=\"525\" data-rawheight=\"169\" class=\"origin_image zh-lightbox-thumb\" width=\"525\" data-original=\"https://pic4.zhimg.com/v2-80b245fd4291e62cffa8f252d4d72fcf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;525&#39; height=&#39;169&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"525\" data-rawheight=\"169\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"525\" data-original=\"https://pic4.zhimg.com/v2-80b245fd4291e62cffa8f252d4d72fcf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-80b245fd4291e62cffa8f252d4d72fcf_b.jpg\"/><figcaption>实际环境演示</figcaption></figure><p>文章采用的主要手段有：</p><ol><li><b>对仿真机器人和物体的动力学参数进行随机</b>，包括：</li></ol><ul><li>机器人每个连杆的质量</li><li>关节的阻尼系数</li><li>物体的质量、摩擦系数和阻尼系数</li><li>桌子的高度</li><li>位置控制器的增益</li><li>动作的时间步长</li><li>观测噪声</li></ul><p>共95个随机参数。不得不说这是一个粗暴但是很管用的trick，对于解决仿真结果在实际环境中的泛化问题很有意义。文章引入一个动力学参数集 <img src=\"https://www.zhihu.com/equation?tex=%5Cmu\" alt=\"\\mu\" eeimg=\"1\"/> 来参数化仿真的状态转移模型 <img src=\"https://www.zhihu.com/equation?tex=%5Chat%7Bp%7D%28s_%7Bt%2B1%7D%7Cs_t%2Ca_t%2C%5Cmu%29\" alt=\"\\hat{p}(s_{t+1}|s_t,a_t,\\mu)\" eeimg=\"1\"/> ，其中 <img src=\"https://www.zhihu.com/equation?tex=%5Chat%7Bp%7D\" alt=\"\\hat{p}\" eeimg=\"1\"/> 是对真实环境 <img src=\"https://www.zhihu.com/equation?tex=p%5E%2A\" alt=\"p^*\" eeimg=\"1\"/> 的近似。因而最终学习的目标变成了：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cmathbb%7BE%7D_%7B%5Cmu%5Csim%5Crho_%7B%5Cmu%7D%7D%5B%5Cmathbb%7BE%7D_%7B%5Ctau%5Csim+p%28%5Ctau%7C%5Cpi%2C%5Cmu%29%7D%5B%5Csum_%7Bt%3D0%7D%5E%7BT-1%7D%7Br%28s_t%2Ca_t%29%7D%5D%5D\" alt=\"\\mathbb{E}_{\\mu\\sim\\rho_{\\mu}}[\\mathbb{E}_{\\tau\\sim p(\\tau|\\pi,\\mu)}[\\sum_{t=0}^{T-1}{r(s_t,a_t)}]]\" eeimg=\"1\"/> </p><p>  2. <b>事后经验回放（Hindsight Experience Replay，HER）</b>[4]。该方法主要是解决奖励稀疏性问题，即在训练过程中，针对机器人推物体和抓物体这一任务，其大多数情况下是得不到任何奖励的，且其奖励是个二值数，即0和-1。因而在一段经历<img src=\"https://www.zhihu.com/equation?tex=%5Ctau%5Cin%28s_0%2Ca_0%2C...%2Ca_%7BT-1%7D%2Cs_T%29\" alt=\"\\tau\\in(s_0,a_0,...,a_{T-1},s_T)\" eeimg=\"1\"/> 中，若最终未能达成目标 <img src=\"https://www.zhihu.com/equation?tex=g\" alt=\"g\" eeimg=\"1\"/> ，则其每个时间点的奖励都会是-1，对如何学习到更好的策略几乎没有任何帮助。HER的核心思想是在经验回放中，对于每一段经历都采样一个不同的目标 <img src=\"https://www.zhihu.com/equation?tex=g%27\" alt=\"g&#39;\" eeimg=\"1\"/> 来进行学习，即不仅是其想要实现的目标 <img src=\"https://www.zhihu.com/equation?tex=g\" alt=\"g\" eeimg=\"1\"/> ，还可以是中间目标或者在该段经历中已经实现的目标。因此，尽管在该段经历中没有实现最终目标 <img src=\"https://www.zhihu.com/equation?tex=g\" alt=\"g\" eeimg=\"1\"/> ，但是我们将最终结果 <img src=\"https://www.zhihu.com/equation?tex=g%27\" alt=\"g&#39;\" eeimg=\"1\"/> 当成我们最初想要达成的目标，该段经历就洗白变成了成功的典范，这样我们就可以从该段经历中学到一些，而不是粗暴地将每个迭代的奖励设为-1。</p><h2>Asymmetric Actor Critic for Image-Based Robot Learning</h2><p>So what&#39;s new in the second one? 前一篇文章将关节位置和速度、夹具位置、物块的位姿和速度等作为52D的状态数据，而将7D的关节角度作为动作输出。本文与其最大的不同即在其是端到端的学习，即<b>直接将仿真的RGB-D图像（即部分观测状态）作为actor的训练输入，同时利用仿真环境的完全状态可观来作为critic的输入</b>，以此来进行训练，如下图所示。</p><figure><noscript><img src=\"https://pic1.zhimg.com/v2-bd5cf90a33203ec9daede876a4765ea0_b.jpg\" data-rawwidth=\"962\" data-rawheight=\"956\" class=\"origin_image zh-lightbox-thumb\" width=\"962\" data-original=\"https://pic1.zhimg.com/v2-bd5cf90a33203ec9daede876a4765ea0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;962&#39; height=&#39;956&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"962\" data-rawheight=\"956\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"962\" data-original=\"https://pic1.zhimg.com/v2-bd5cf90a33203ec9daede876a4765ea0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-bd5cf90a33203ec9daede876a4765ea0_b.jpg\"/><figcaption>仿真RGB-D图像，actor模型输入</figcaption></figure><figure><noscript><img src=\"https://pic4.zhimg.com/v2-3ddafea51e1b679e26604c70225b2b77_b.jpg\" data-rawwidth=\"938\" data-rawheight=\"728\" class=\"origin_image zh-lightbox-thumb\" width=\"938\" data-original=\"https://pic4.zhimg.com/v2-3ddafea51e1b679e26604c70225b2b77_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;938&#39; height=&#39;728&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"938\" data-rawheight=\"728\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"938\" data-original=\"https://pic4.zhimg.com/v2-3ddafea51e1b679e26604c70225b2b77_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-3ddafea51e1b679e26604c70225b2b77_b.jpg\"/><figcaption>A3C学习框架</figcaption></figure><p>文章最后对采用领域随机化（domain randomization）的效果进行了分析，如下表所示，即采用参数随机化进行训练可以大大提升仿真训练在实际环境中的鲁棒性（该对比在youtube的视频里更直观和明显，有兴趣可以看一下）。</p><figure><noscript><img src=\"https://pic4.zhimg.com/v2-0d46ad4d6142995459a00189d880010b_b.jpg\" data-rawwidth=\"1826\" data-rawheight=\"246\" class=\"origin_image zh-lightbox-thumb\" width=\"1826\" data-original=\"https://pic4.zhimg.com/v2-0d46ad4d6142995459a00189d880010b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1826&#39; height=&#39;246&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1826\" data-rawheight=\"246\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1826\" data-original=\"https://pic4.zhimg.com/v2-0d46ad4d6142995459a00189d880010b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-0d46ad4d6142995459a00189d880010b_b.jpg\"/><figcaption>随机化策略效果对比</figcaption></figure><p class=\"ztext-empty-paragraph\"><br/></p><blockquote>参考资料：<br/>[1] Peng, Xue Bin, et al. &#34;Sim-to-Real Transfer of Robotic Control with Dynamics Randomization.&#34; arXiv preprint arXiv:1710.06537 (2017).<br/>[2] Pinto, Lerrel, et al. &#34;Asymmetric Actor Critic for Image-Based Robot Learning.&#34; arXiv preprint arXiv:1710.06542 (2017).<br/>[3] Levine, Sergey, et al. &#34;Learning hand-eye coordination for robotic grasping with deep learning and large-scale data collection.&#34; The International Journal of Robotics Research (2016): 0278364917710318.<br/>[4] Andrychowicz, Marcin, et al. &#34;Hindsight experience replay.&#34; arXiv preprint arXiv:1707.01495 (2017).</blockquote>", 
            "topic": [
                {
                    "tag": "强化学习 (Reinforcement Learning)", 
                    "tagLink": "https://api.zhihu.com/topics/20039099"
                }, 
                {
                    "tag": "机器人", 
                    "tagLink": "https://api.zhihu.com/topics/19551273"
                }
            ], 
            "comments": [
                {
                    "userName": "Chrisole", 
                    "userLink": "https://www.zhihu.com/people/3e2189704ffdeaf3e446a0320d1b4edf", 
                    "content": "您好 请问论文二有复现的代码吗", 
                    "likes": 1, 
                    "childComments": [
                        {
                            "userName": "知乎用户", 
                            "userLink": "https://www.zhihu.com/people/0", 
                            "content": "<p>您好，我没有具体做这方面的工作。貌似有人复现过，你可以关注<a href=\"http://link.zhihu.com/?target=https%3A//github.com/bhairavmehta95/asymmetric-actor-critic\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">bhairavmehta95/asymmetric-actor-critic</a></p>", 
                            "likes": 0, 
                            "replyToAuthor": "Chrisole"
                        }, 
                        {
                            "userName": "Chrisole", 
                            "userLink": "https://www.zhihu.com/people/3e2189704ffdeaf3e446a0320d1b4edf", 
                            "content": "谢谢您的回复 这个我有看到过 做的particle的 没办法迁移到真实环境", 
                            "likes": 0, 
                            "replyToAuthor": "知乎用户"
                        }
                    ]
                }
            ]
        }
    ], 
    "url": "https://zhuanlan.zhihu.com/c_137883028"
}
