{
    "title": "北极圈智能", 
    "description": "CMVS 民间学习小分队", 
    "followers": [
        "https://www.zhihu.com/people/Xmin", 
        "https://www.zhihu.com/people/wang-cheng-rui-24", 
        "https://www.zhihu.com/people/hcc-12-16", 
        "https://www.zhihu.com/people/huang-xiao-hu-35-27", 
        "https://www.zhihu.com/people/kim-74-51", 
        "https://www.zhihu.com/people/scou-t", 
        "https://www.zhihu.com/people/hei-ye-ming-mou", 
        "https://www.zhihu.com/people/xiao-xin-zha-bi-75", 
        "https://www.zhihu.com/people/yi-lv-qing-xiang", 
        "https://www.zhihu.com/people/Steven_Jokes", 
        "https://www.zhihu.com/people/tongzhan", 
        "https://www.zhihu.com/people/xiao-yao-73-56", 
        "https://www.zhihu.com/people/jiang-yan-27-95", 
        "https://www.zhihu.com/people/sheng-huo-le-wu-bian-40", 
        "https://www.zhihu.com/people/ben-pao-de-gua-niu-79", 
        "https://www.zhihu.com/people/ye-yu-7-30-47", 
        "https://www.zhihu.com/people/m-m-94", 
        "https://www.zhihu.com/people/chen-bo-hua-62", 
        "https://www.zhihu.com/people/xiaoai91", 
        "https://www.zhihu.com/people/lin-bin-8-95", 
        "https://www.zhihu.com/people/da-hai-34-58", 
        "https://www.zhihu.com/people/world-bairui", 
        "https://www.zhihu.com/people/mu-zi-9-46-33", 
        "https://www.zhihu.com/people/sebastian-gao", 
        "https://www.zhihu.com/people/deng-yi-biao-25", 
        "https://www.zhihu.com/people/wen-song-80-63", 
        "https://www.zhihu.com/people/kim-yang", 
        "https://www.zhihu.com/people/yong-heng-de-xing-he", 
        "https://www.zhihu.com/people/roger-gou", 
        "https://www.zhihu.com/people/littlejjewel", 
        "https://www.zhihu.com/people/quxiaofeng", 
        "https://www.zhihu.com/people/chong-hua-jia-de-xiao-xiao-meng-40", 
        "https://www.zhihu.com/people/lu-qing-27-96", 
        "https://www.zhihu.com/people/chu-da-28", 
        "https://www.zhihu.com/people/li-sheng-61-4", 
        "https://www.zhihu.com/people/lu-ying-fa-87", 
        "https://www.zhihu.com/people/rui-zong-yun", 
        "https://www.zhihu.com/people/zhizhu-94-32", 
        "https://www.zhihu.com/people/tang-bai-mo", 
        "https://www.zhihu.com/people/chen-zhi-yuan-6-76-58", 
        "https://www.zhihu.com/people/zienon", 
        "https://www.zhihu.com/people/borago_weizhen", 
        "https://www.zhihu.com/people/tzattack", 
        "https://www.zhihu.com/people/wudilp", 
        "https://www.zhihu.com/people/deeplearninger-58", 
        "https://www.zhihu.com/people/cao-ji-49-42", 
        "https://www.zhihu.com/people/zhang-lu-29-21", 
        "https://www.zhihu.com/people/wulw1990", 
        "https://www.zhihu.com/people/JDIJason", 
        "https://www.zhihu.com/people/an-yong-34-26", 
        "https://www.zhihu.com/people/tyger-77", 
        "https://www.zhihu.com/people/serena-yuan-41", 
        "https://www.zhihu.com/people/wan-xing-13", 
        "https://www.zhihu.com/people/tiancaiye", 
        "https://www.zhihu.com/people/Lucy_Lu_Lu", 
        "https://www.zhihu.com/people/cerena-8", 
        "https://www.zhihu.com/people/er-jin-chen", 
        "https://www.zhihu.com/people/ceng-qiang-16-57", 
        "https://www.zhihu.com/people/dou-zi-93-8", 
        "https://www.zhihu.com/people/lizhengx", 
        "https://www.zhihu.com/people/liu-jun-34-76", 
        "https://www.zhihu.com/people/leo1901", 
        "https://www.zhihu.com/people/jiaxing-ye", 
        "https://www.zhihu.com/people/Ni_Guo_Chen", 
        "https://www.zhihu.com/people/ni-kuai-han-wo", 
        "https://www.zhihu.com/people/Gtesla-10-49-76", 
        "https://www.zhihu.com/people/ao-lu-de-chen-hao-yu", 
        "https://www.zhihu.com/people/ma-ru-bin-88", 
        "https://www.zhihu.com/people/hou-kai-76", 
        "https://www.zhihu.com/people/dai-mei-lin", 
        "https://www.zhihu.com/people/yang-zhi-gang-70-1", 
        "https://www.zhihu.com/people/ma-yu-xiang-4", 
        "https://www.zhihu.com/people/caijia", 
        "https://www.zhihu.com/people/Huang-Hungsing", 
        "https://www.zhihu.com/people/lan-xian-sheng-60", 
        "https://www.zhihu.com/people/dong-shu-93-79", 
        "https://www.zhihu.com/people/mjfly-17", 
        "https://www.zhihu.com/people/tyron-67", 
        "https://www.zhihu.com/people/biteye-47", 
        "https://www.zhihu.com/people/iboneyoung", 
        "https://www.zhihu.com/people/xsongxnudt", 
        "https://www.zhihu.com/people/thikingfly", 
        "https://www.zhihu.com/people/bengang-zhuo", 
        "https://www.zhihu.com/people/dancy-96-7", 
        "https://www.zhihu.com/people/fei-de-geng-gao-2", 
        "https://www.zhihu.com/people/cui-yong-ming-88", 
        "https://www.zhihu.com/people/yousaid0927", 
        "https://www.zhihu.com/people/tjamc80", 
        "https://www.zhihu.com/people/leo-lee-58-57", 
        "https://www.zhihu.com/people/binghuo-43", 
        "https://www.zhihu.com/people/scholltan", 
        "https://www.zhihu.com/people/li-song-tao-53", 
        "https://www.zhihu.com/people/lizhc", 
        "https://www.zhihu.com/people/song-gou-zi-83", 
        "https://www.zhihu.com/people/vulcan-9", 
        "https://www.zhihu.com/people/xiao-ma-jia-55-39", 
        "https://www.zhihu.com/people/gao-kai-2-83", 
        "https://www.zhihu.com/people/steve-15-41", 
        "https://www.zhihu.com/people/zhangyinqi317", 
        "https://www.zhihu.com/people/pang-dou-56", 
        "https://www.zhihu.com/people/feng-ling-gang", 
        "https://www.zhihu.com/people/zheng-jian-yang-56", 
        "https://www.zhihu.com/people/lei-yu-93-93-80", 
        "https://www.zhihu.com/people/you-pei-84-44", 
        "https://www.zhihu.com/people/xxspurs-57", 
        "https://www.zhihu.com/people/aihaoge1314", 
        "https://www.zhihu.com/people/knowledgeblah", 
        "https://www.zhihu.com/people/wang-wei-78-6", 
        "https://www.zhihu.com/people/lin-liu-85", 
        "https://www.zhihu.com/people/bu-si-kao-de-mao", 
        "https://www.zhihu.com/people/jac123", 
        "https://www.zhihu.com/people/guang-ming-gmg", 
        "https://www.zhihu.com/people/wang-xiao-qi-44-65", 
        "https://www.zhihu.com/people/chl-97-25", 
        "https://www.zhihu.com/people/wang-jing-bo-27-88", 
        "https://www.zhihu.com/people/wei-xin-81-7", 
        "https://www.zhihu.com/people/zhu-forrest", 
        "https://www.zhihu.com/people/a-xia-29-74", 
        "https://www.zhihu.com/people/wayne-98-87", 
        "https://www.zhihu.com/people/zhihuer2018", 
        "https://www.zhihu.com/people/xiao-leng-88-5", 
        "https://www.zhihu.com/people/liu-zhen-hua-27-42", 
        "https://www.zhihu.com/people/fei-yang-51-50", 
        "https://www.zhihu.com/people/hai-bin-17-85", 
        "https://www.zhihu.com/people/xi-bo-li-ya-han-liu", 
        "https://www.zhihu.com/people/jeffzhou-98", 
        "https://www.zhihu.com/people/wt-shen-87", 
        "https://www.zhihu.com/people/dong-hai-43", 
        "https://www.zhihu.com/people/shi-bu-guo-san-91", 
        "https://www.zhihu.com/people/feng-zhong-de-yi-han-83", 
        "https://www.zhihu.com/people/kalashnigov", 
        "https://www.zhihu.com/people/ppny", 
        "https://www.zhihu.com/people/liu-dun-qiang-11", 
        "https://www.zhihu.com/people/yildhd-wang", 
        "https://www.zhihu.com/people/cang-hai-heng-liu-78", 
        "https://www.zhihu.com/people/che-la-la-98", 
        "https://www.zhihu.com/people/gongyunbo", 
        "https://www.zhihu.com/people/jiu-i-77-31", 
        "https://www.zhihu.com/people/li-kun-2-96", 
        "https://www.zhihu.com/people/da-yu-90-14-31", 
        "https://www.zhihu.com/people/ykp-41", 
        "https://www.zhihu.com/people/choiyeren", 
        "https://www.zhihu.com/people/yang-feng-11-49", 
        "https://www.zhihu.com/people/art-wang", 
        "https://www.zhihu.com/people/yan-hua-chang-an", 
        "https://www.zhihu.com/people/aitracker", 
        "https://www.zhihu.com/people/wuge-mr", 
        "https://www.zhihu.com/people/jeff-soong", 
        "https://www.zhihu.com/people/stone-rock-30", 
        "https://www.zhihu.com/people/weng-ding", 
        "https://www.zhihu.com/people/ideaplat", 
        "https://www.zhihu.com/people/bai-jun-50-17", 
        "https://www.zhihu.com/people/tian-yi-59-33", 
        "https://www.zhihu.com/people/Vansiee", 
        "https://www.zhihu.com/people/zjh-38-94", 
        "https://www.zhihu.com/people/fan-11-46", 
        "https://www.zhihu.com/people/xuzhengbo", 
        "https://www.zhihu.com/people/liu-fei-94-95", 
        "https://www.zhihu.com/people/1111112222", 
        "https://www.zhihu.com/people/ybtk", 
        "https://www.zhihu.com/people/kmotion", 
        "https://www.zhihu.com/people/zhaowzxp", 
        "https://www.zhihu.com/people/jacky-75-50", 
        "https://www.zhihu.com/people/su-zhuo-86", 
        "https://www.zhihu.com/people/moni-gg", 
        "https://www.zhihu.com/people/see-am", 
        "https://www.zhihu.com/people/guifengstyle", 
        "https://www.zhihu.com/people/yu-zi-tong-13", 
        "https://www.zhihu.com/people/chen-hao-yu-52-2", 
        "https://www.zhihu.com/people/lao-xiao-65-24", 
        "https://www.zhihu.com/people/liang-zhi-song-shu-66", 
        "https://www.zhihu.com/people/xiang-tian-4", 
        "https://www.zhihu.com/people/xi-li-hu-tu-3-28", 
        "https://www.zhihu.com/people/zhui-feng-zheng-de-ren-70-19", 
        "https://www.zhihu.com/people/fu-mei-rong-31-69", 
        "https://www.zhihu.com/people/asoe", 
        "https://www.zhihu.com/people/xiaoq-41", 
        "https://www.zhihu.com/people/hhda-tim", 
        "https://www.zhihu.com/people/long-ri-yao", 
        "https://www.zhihu.com/people/tu-zi-ing", 
        "https://www.zhihu.com/people/huang-yi-83-43", 
        "https://www.zhihu.com/people/cornpop", 
        "https://www.zhihu.com/people/xian-sen-90", 
        "https://www.zhihu.com/people/GreatMaxwell", 
        "https://www.zhihu.com/people/feng-xing-long-5", 
        "https://www.zhihu.com/people/xiao-qiang-bu-si-39", 
        "https://www.zhihu.com/people/whis-19-80", 
        "https://www.zhihu.com/people/tian-hong-liang-59", 
        "https://www.zhihu.com/people/young-13-96", 
        "https://www.zhihu.com/people/Lonegineer", 
        "https://www.zhihu.com/people/long-mao-shou-hu-shen-28", 
        "https://www.zhihu.com/people/mo-xiao-guai-1993-3", 
        "https://www.zhihu.com/people/shen-zi-jian-mo-ru-yun-piao-bo-88", 
        "https://www.zhihu.com/people/xiao-qi-e-77-51"
    ], 
    "article": [
        {
            "url": "https://zhuanlan.zhihu.com/p/73180893", 
            "userName": "奥卢的陈皓宇", 
            "userLink": "https://www.zhihu.com/people/6e25ca19a73ea5a2d81f195cf4c1851f", 
            "upvote": 1, 
            "title": "2018年 ACTION RECOGNITION 的汇总 （AAAI）", 
            "content": "<p>把ECCV和CVPR还有AAAI 2018年的action recognition汇了个总，放在这里。</p><p>全部文章请看第一期：</p><p><a href=\"https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzI4MTAyMzQ2OA%3D%3D%26mid%3D2650679415%26idx%3D2%26sn%3Dde4ebc9192da9d7de9c909ffb12a043a%26chksm%3Df3a50ef4c4d287e2d4abceadcca3172f1d529dae975ee00aa015848c6b9c0e69cbcc6938d0c8%26scene%3D21%23wechat_redirect\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">2018 动作识别的顶会文献总结</a><br/></p><p>这一期总结了AAAI 的八篇文章，在此。</p><hr/><h2>AAAI 2018</h2><p>Action Recognition from Skeleton Data via Analogical Generalization over Qualitative Representations</p><p>Unsupervised Representation Learning with Long-Term Dynamics for Skeleton Based Action Recognition</p><p>Cooperative Training of Deep Aggregation Networks for RGB-D Action Recognition</p><p>Hierarchical Nonlinear Orthogonal Adaptive-Subspace Self-Organizing Map based Feature Extraction for Human Action Recognition</p><p>Action Recognition with Coarse-to-Fine Deep Feature Integration and Asynchronous Fusion</p><p>Unsupervised Deep Learning of Mid-Level Video Representation for Action Recognition</p><p>T-C3D: Temporal Convolutional 3D Network for Real-time Action Recognition</p><p>Spatial-Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition</p><p class=\"ztext-empty-paragraph\"><br/></p><p>以下正文开始。</p><hr/><h2>Action Recognition from Skeleton Data via Analogical Generalization over Qualitative Representations</h2><h3>Kezhen Chen, Kenneth D. Forbus</h3><h3>Northwestern University<br/></h3><p>这是美国西北大学的一个工作，总的来说帮助不大。</p><p>本文的贡献是在其已有的2011年推出的 CogSketch的基础上，对action recognition做的一个应用的拓展。</p><p>这篇工作是基于他们之前的一个叫做CogSketch的工作：</p><p><a href=\"https://link.zhihu.com/?target=http%3A//www.qrg.northwestern.edu/software/cogsketch/index.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">qrg.northwestern.edu/so</span><span class=\"invisible\">ftware/cogsketch/index.html</span><span class=\"ellipsis\"></span></a></p><p>来做的。</p><p>搜索了一下这个CogSketch，是一个应用。其作用是对给定的一个Sketch提供可解释性的描述。</p><p>然后他们就把skeleton作为天然的sketch输入到他们的应用，然后得到一些可解释性的描述。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-3140bf86d73f5ad493c5282f98195c5e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"502\" data-rawheight=\"555\" class=\"origin_image zh-lightbox-thumb\" width=\"502\" data-original=\"https://pic3.zhimg.com/v2-3140bf86d73f5ad493c5282f98195c5e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;502&#39; height=&#39;555&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"502\" data-rawheight=\"555\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"502\" data-original=\"https://pic3.zhimg.com/v2-3140bf86d73f5ad493c5282f98195c5e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-3140bf86d73f5ad493c5282f98195c5e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>但是他们识别精度远不如state of the art，其创新点在于第一次使用基于定性关系的类比生成（ analogical generalization on qualitative relations ）而不是机器学习的方法来做这个任务。</p><p>结果如下：</p><p>UTKinect-Action3D Dataset</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-290e220b877d1678a74b30f73ec6d810_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"488\" data-rawheight=\"213\" class=\"origin_image zh-lightbox-thumb\" width=\"488\" data-original=\"https://pic1.zhimg.com/v2-290e220b877d1678a74b30f73ec6d810_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;488&#39; height=&#39;213&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"488\" data-rawheight=\"213\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"488\" data-original=\"https://pic1.zhimg.com/v2-290e220b877d1678a74b30f73ec6d810_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-290e220b877d1678a74b30f73ec6d810_b.jpg\"/></figure><p>the Berkeley Multi-modal Human Action dataset (MHAD)</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ebf2cd1aace7f4eaeaecaa2e4e5ba5b0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"490\" data-rawheight=\"332\" class=\"origin_image zh-lightbox-thumb\" width=\"490\" data-original=\"https://pic1.zhimg.com/v2-ebf2cd1aace7f4eaeaecaa2e4e5ba5b0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;490&#39; height=&#39;332&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"490\" data-rawheight=\"332\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"490\" data-original=\"https://pic1.zhimg.com/v2-ebf2cd1aace7f4eaeaecaa2e4e5ba5b0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-ebf2cd1aace7f4eaeaecaa2e4e5ba5b0_b.jpg\"/></figure><p>关于可解释性的解释让人更confused了：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-eb40141efd274aae11f7a0e4a4ca3ba9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"506\" data-rawheight=\"552\" class=\"origin_image zh-lightbox-thumb\" width=\"506\" data-original=\"https://pic2.zhimg.com/v2-eb40141efd274aae11f7a0e4a4ca3ba9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;506&#39; height=&#39;552&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"506\" data-rawheight=\"552\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"506\" data-original=\"https://pic2.zhimg.com/v2-eb40141efd274aae11f7a0e4a4ca3ba9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-eb40141efd274aae11f7a0e4a4ca3ba9_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>难道不是高级版本的hand-craft？</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-700d0c9254a3c69f024764499b0edbd8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"492\" data-rawheight=\"586\" class=\"origin_image zh-lightbox-thumb\" width=\"492\" data-original=\"https://pic1.zhimg.com/v2-700d0c9254a3c69f024764499b0edbd8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;492&#39; height=&#39;586&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"492\" data-rawheight=\"586\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"492\" data-original=\"https://pic1.zhimg.com/v2-700d0c9254a3c69f024764499b0edbd8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-700d0c9254a3c69f024764499b0edbd8_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>关于对动作的可解释性这一点来说，算是一大闪光点，在精度实在干不过人家的情况下。</p><h2>Unsupervised Representation Learning with Long-Term Dynamics for Skeleton Based Action Recognition</h2><h3>Nenggan Zheng,1 Jun Wen,2 Risheng Liu,3∗ Liangqu Long,2 Jianhua Dai,4 Zhefeng Gong5</h3><h3>1 Qiushi Academy for Advanced Studies, Zhejiang University, Hangzhou, Zhejiang, China</h3><h3>2 College of Computer Science and Techology, Zhejiang University, Hangzhou, Zhejiang, China</h3><h3>3 DUT-RU International School of Information Science &amp; Engineering, Dalian University of Technology, Liaoning, China</h3><h3>4 College of Information Science and Engineering, Hunan Normal University, Changsha, Hunan, China</h3><h3>5 Department of Neurobiology, Zhejiang University School of Medicine, Hangzhou, Zhejiang, China</h3><p>浙大的工作主导的工作，感觉不错，github上已经有了链接，但是代码还没挂出来。</p><p>简述</p><p>这篇文章的贡献是提出了一种无监督的第一次对长时全局动作的表征方法， conditional inpainting。它是基于骨骼来做的。</p><p>他学了一种条件式的骨骼绘制结构，用来学习一个固定维度的表征。结果表明它可以大大减少序列绘制错误。</p><p>介绍</p><p>动作识别分为2D 的RGB 和3D 的skeleton，skeleton的关键是学习动作的姿态和动态。</p><p>之前的时序都是基于HMM 或者时序金字塔。这些都是需要用滑窗的方式来对时序建模。近些年，端到端的就来了，比如RNN取得了很好的成绩。这些模型的insights是提取判别特征来表示不同动作的时间演变。但是，这些工作都是在监督训练的情况下做的，需要大量标定数据。</p><p>近些时间来，一些无监督的方法被提出来，他们要么强制让表征时序平滑并学习缓慢变化的表征（类似于DTW之类的）要么就进行帧的重建（插值之类的）。但是都不够灵活到足以对长段的时序特征进行操作。学习到的特征判别性不足。</p><p>他们的工作由三个子网络构成，一个encoder，把输入序列变成紧凑的固定维度的表征，一个decoder把条件遮挡了的表征重建成学习的表征。最后一个discriminator学会区分原序列和重建序列。</p><p>用传统的元素级别的损失函数不行，重建的序列会失真，所以要用条件约束。</p><p>主要贡献有三个，一条件对抗网络来捕捉长时序列特征，二增强的学习策略，三在已有数据集上的详细实验。</p><p>相关工作</p><p>序列级别的表征学习，以前的RNN等都要接受等长的序列，而且不能很好地捕捉长时信息。基于骨骼的动作识别，大部分都是TP和HMM来时序建模，RNN进来也有，骨骼节点之间的cooccurrence也是非常有好的判别特征，gated 的LSTM 等表现比较爆炸的模型又受限于label的限制。</p><p>具体</p><p>网络结构，有三个子网络</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-af1700604048a69ef0b5b5ed2d2e9785_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"738\" data-rawheight=\"547\" class=\"origin_image zh-lightbox-thumb\" width=\"738\" data-original=\"https://pic2.zhimg.com/v2-af1700604048a69ef0b5b5ed2d2e9785_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;738&#39; height=&#39;547&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"738\" data-rawheight=\"547\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"738\" data-original=\"https://pic2.zhimg.com/v2-af1700604048a69ef0b5b5ed2d2e9785_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-af1700604048a69ef0b5b5ed2d2e9785_b.jpg\"/></figure><p>他们提出了一个联合损失函数：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-1d3f6651ac1869e07da96d407216b0d9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"500\" data-rawheight=\"90\" class=\"origin_image zh-lightbox-thumb\" width=\"500\" data-original=\"https://pic2.zhimg.com/v2-1d3f6651ac1869e07da96d407216b0d9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;500&#39; height=&#39;90&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"500\" data-rawheight=\"90\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"500\" data-original=\"https://pic2.zhimg.com/v2-1d3f6651ac1869e07da96d407216b0d9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-1d3f6651ac1869e07da96d407216b0d9_b.jpg\"/></figure><p>这个loss就是加了一个conditional 的GAN</p><p>训练的算法，见下图。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-6af0701fdee1b6d3b6b9ee70464b38fd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"511\" data-rawheight=\"491\" class=\"origin_image zh-lightbox-thumb\" width=\"511\" data-original=\"https://pic2.zhimg.com/v2-6af0701fdee1b6d3b6b9ee70464b38fd_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;511&#39; height=&#39;491&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"511\" data-rawheight=\"491\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"511\" data-original=\"https://pic2.zhimg.com/v2-6af0701fdee1b6d3b6b9ee70464b38fd_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-6af0701fdee1b6d3b6b9ee70464b38fd_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>GAN 的真样本是原始数据，假样本是加入扰动z的x，以及随机掩盖的x。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-fc72a688bfd78ae978ebaf3af6ba40ce_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"421\" data-rawheight=\"53\" class=\"origin_image zh-lightbox-thumb\" width=\"421\" data-original=\"https://pic3.zhimg.com/v2-fc72a688bfd78ae978ebaf3af6ba40ce_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;421&#39; height=&#39;53&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"421\" data-rawheight=\"53\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"421\" data-original=\"https://pic3.zhimg.com/v2-fc72a688bfd78ae978ebaf3af6ba40ce_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-fc72a688bfd78ae978ebaf3af6ba40ce_b.png\"/></figure><p>他在解释loss 的时候，是这么说的：</p><p>利用公式1中的联合损失函数，我们训练encoder-decoder和GAN</p><p>训练的时候，Dis仅最小化对抗性损失，而Dec则从元素级损失和对抗性损失中接收误差损失。我们观察到Enc不应该试图最小化对抗性损失，否则Enc倾向于编码对于产生视觉上逼真的序列而不是编码用于序列修复的运动动力学的信息。</p><p>对于这个核心问题，它是这么解释的，元素级的损失，用来对内容进行保留，而对抗性损失，用来对风格进行保留。</p><p>实验结果</p><p>设置了三种模式</p><p>一，无监督的训练encoder-decoder模型，只微调最后一层来做识别</p><p>二，无监督训练，再对整个网络做微调</p><p>三，直接训练整个网络</p><p>CMU的库</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3d218bd9e4a4a07abee4b2e45225d68d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"495\" data-rawheight=\"337\" class=\"origin_image zh-lightbox-thumb\" width=\"495\" data-original=\"https://pic2.zhimg.com/v2-3d218bd9e4a4a07abee4b2e45225d68d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;495&#39; height=&#39;337&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"495\" data-rawheight=\"337\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"495\" data-original=\"https://pic2.zhimg.com/v2-3d218bd9e4a4a07abee4b2e45225d68d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-3d218bd9e4a4a07abee4b2e45225d68d_b.jpg\"/></figure><p>无监督的学习非常有效，但是看了对比算法，发现就是对LSTM 的自编码版本。</p><p>HDM05和伯克利的库</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-1049d4f8ae6c9fb45f8c93b878c3c154_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"495\" data-rawheight=\"585\" class=\"origin_image zh-lightbox-thumb\" width=\"495\" data-original=\"https://pic1.zhimg.com/v2-1049d4f8ae6c9fb45f8c93b878c3c154_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;495&#39; height=&#39;585&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"495\" data-rawheight=\"585\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"495\" data-original=\"https://pic1.zhimg.com/v2-1049d4f8ae6c9fb45f8c93b878c3c154_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-1049d4f8ae6c9fb45f8c93b878c3c154_b.jpg\"/></figure><p>他们还验证了不同样本数量的影响:</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-125dbe9ec02d3fdf0e48463790250195_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1042\" data-rawheight=\"317\" class=\"origin_image zh-lightbox-thumb\" width=\"1042\" data-original=\"https://pic2.zhimg.com/v2-125dbe9ec02d3fdf0e48463790250195_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1042&#39; height=&#39;317&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1042\" data-rawheight=\"317\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1042\" data-original=\"https://pic2.zhimg.com/v2-125dbe9ec02d3fdf0e48463790250195_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-125dbe9ec02d3fdf0e48463790250195_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>Cooperative Training of Deep Aggregation Networks for RGB-D Action Recognition</p><h3>Pichao Wang,1,2 Wanqing Li,1 Jun Wan,3∗ Philip Ogunbona,1 Xinwang Liu,4</h3><h3>1 Advanced Multimedia Research Lab, University of Wollongong, Australia</h3><h3>2 Motovis Inc</h3><h3>3 Center for Biometrics and Security Research &amp; National Laboratory of Pattern Recognition<br/>Institute of Automation, Chinese Academy of Sciences</h3><h3>4 School of Computer Science, National University of Defense Technology, Changsha 410073, China</h3><p>卧龙岗大学，魔视，中科院，国防科大合作的工作。</p><p>简述</p><p>这篇文章提出了一个叫做c-ConvNet的新的范式，用来进行多模态的RGB-D动作识别。以前的都是每个模态用不同的网络分别训练再融合，而这个工作则是提出了一个可以联合训练的方案。</p><p>个人认为是比较偏工程的方案，整个思路比较好理解，而效果很好。</p><p>code：<a href=\"https://link.zhihu.com/?target=https%3A//sites.google.com/site/pichaossites/resources/codesAAAI.zip%3Fattredirects%3D0%26d%3D1\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">sites.google.com/site/p</span><span class=\"invisible\">ichaossites/resources/codesAAAI.zip?attredirects=0&amp;d=1</span><span class=\"ellipsis\"></span></a></p><p>具体</p><p>特征提取</p><p>不同于一般的RGB D 用光流来处理运动的特性，他直接提出了一个新的特征模式：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-f53958b9dbd800fba3638955f4d0c829_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"424\" data-rawheight=\"577\" class=\"origin_image zh-lightbox-thumb\" width=\"424\" data-original=\"https://pic2.zhimg.com/v2-f53958b9dbd800fba3638955f4d0c829_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;424&#39; height=&#39;577&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"424\" data-rawheight=\"577\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"424\" data-original=\"https://pic2.zhimg.com/v2-f53958b9dbd800fba3638955f4d0c829_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-f53958b9dbd800fba3638955f4d0c829_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>通过上面的描述，我们可以知道，他是用了前后几帧来构造一个新的特征图，有按时间序列的，也有逆时间序列的，VDI 表示video的dynamic，DVI 表示depth的dynamic，特征长这个样子：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-61a25f8e3ab2545bfd56840a2c597bbc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"425\" data-rawheight=\"381\" class=\"origin_image zh-lightbox-thumb\" width=\"425\" data-original=\"https://pic1.zhimg.com/v2-61a25f8e3ab2545bfd56840a2c597bbc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;425&#39; height=&#39;381&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"425\" data-rawheight=\"381\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"425\" data-original=\"https://pic1.zhimg.com/v2-61a25f8e3ab2545bfd56840a2c597bbc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-61a25f8e3ab2545bfd56840a2c597bbc_b.jpg\"/></figure><p>网络结构</p><p>然后用这种特征输到网络中，提出了ranking loss 和三个loss selection的训练方法：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-736a4c4353d50df7ac0f650011040d2a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"872\" data-rawheight=\"400\" class=\"origin_image zh-lightbox-thumb\" width=\"872\" data-original=\"https://pic3.zhimg.com/v2-736a4c4353d50df7ac0f650011040d2a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;872&#39; height=&#39;400&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"872\" data-rawheight=\"400\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"872\" data-original=\"https://pic3.zhimg.com/v2-736a4c4353d50df7ac0f650011040d2a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-736a4c4353d50df7ac0f650011040d2a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>训练时的损失函数如下</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-fa38b48e86746663860fee536cc14ca3_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"415\" data-rawheight=\"57\" class=\"content_image\" width=\"415\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;415&#39; height=&#39;57&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"415\" data-rawheight=\"57\" class=\"content_image lazy\" width=\"415\" data-actualsrc=\"https://pic4.zhimg.com/v2-fa38b48e86746663860fee536cc14ca3_b.png\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-bcef9c3903fcdf368cc148d01edb1f81_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"424\" data-rawheight=\"50\" class=\"origin_image zh-lightbox-thumb\" width=\"424\" data-original=\"https://pic2.zhimg.com/v2-bcef9c3903fcdf368cc148d01edb1f81_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;424&#39; height=&#39;50&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"424\" data-rawheight=\"50\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"424\" data-original=\"https://pic2.zhimg.com/v2-bcef9c3903fcdf368cc148d01edb1f81_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-bcef9c3903fcdf368cc148d01edb1f81_b.png\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>最后的结果：<br/></p><p>在NTU 上表现还凑合：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-866b2fd7814b454f227211138aebf56d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"403\" data-rawheight=\"372\" class=\"content_image\" width=\"403\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;403&#39; height=&#39;372&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"403\" data-rawheight=\"372\" class=\"content_image lazy\" width=\"403\" data-actualsrc=\"https://pic2.zhimg.com/v2-866b2fd7814b454f227211138aebf56d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>在中山大学的这个库上表现爆炸了：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-0ff42d1c1618f7d95a44aca6dbd9c04a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"422\" data-rawheight=\"166\" class=\"origin_image zh-lightbox-thumb\" width=\"422\" data-original=\"https://pic3.zhimg.com/v2-0ff42d1c1618f7d95a44aca6dbd9c04a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;422&#39; height=&#39;166&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"422\" data-rawheight=\"166\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"422\" data-original=\"https://pic3.zhimg.com/v2-0ff42d1c1618f7d95a44aca6dbd9c04a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-0ff42d1c1618f7d95a44aca6dbd9c04a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>Hierarchical Nonlinear Orthogonal Adaptive-Subspace Self-Organizing Map based Feature Extraction for Human Action Recognition</p><h3>Yang Du,1,2,3 Chunfeng Yuan,1∗ Weiming Hu,1 Hao Yang1,2,3</h3><h3>1 CAS Center for Excellence in Brain Science and Intelligence Technology, National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences;</h3><h3>2 University of Chinese Academy of Sciences</h3><h3>3 MTdata, Meitu</h3><p>中科大，美图合作的工作。</p><p>简述</p><p>这篇文章比较另类，做的是Self-Organizing Map 自组织映射。查了一下，是另一种类型的神经网络，与其说是神经网络不如说是一种类似kmean的聚类。通常意义上的自组织映射都是线性的，他们提出了一种非线性的多层映射（那不就是向经典的神经网络靠拢了吗？？）</p><p>个人认为文章能中的原因是比较新，数学多，加上能对无标签能进行处理。实际效果其实并不能到state of the art。</p><p>具体</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-ed41bb84d11014d02d5e5244451b0b2e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"462\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-ed41bb84d11014d02d5e5244451b0b2e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;462&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"462\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-ed41bb84d11014d02d5e5244451b0b2e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-ed41bb84d11014d02d5e5244451b0b2e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>他们加了一个非线性正交的层，来学习非线性的数据。在他们已有的工作基础上，ASSOM。</p><p>他们提取特征的方式是走了两个通道，空间和时间的融合。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-8333ed204d075acb49cd248ddda178ce_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"350\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-8333ed204d075acb49cd248ddda178ce_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;350&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"350\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-8333ed204d075acb49cd248ddda178ce_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-8333ed204d075acb49cd248ddda178ce_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>由于SOM 更像是一种聚类，所以其数学模型比较复杂，看得我脑壳昏：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-83b3038dc92a8819bea59c4e39128965_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"528\" data-rawheight=\"599\" class=\"origin_image zh-lightbox-thumb\" width=\"528\" data-original=\"https://pic2.zhimg.com/v2-83b3038dc92a8819bea59c4e39128965_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;528&#39; height=&#39;599&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"528\" data-rawheight=\"599\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"528\" data-original=\"https://pic2.zhimg.com/v2-83b3038dc92a8819bea59c4e39128965_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-83b3038dc92a8819bea59c4e39128965_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>最后的结果：</p><p>两个数据集都不能到state of the art：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-8f46d8a3ba74a37ddfe09bbda077a2fc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1026\" data-rawheight=\"438\" class=\"origin_image zh-lightbox-thumb\" width=\"1026\" data-original=\"https://pic1.zhimg.com/v2-8f46d8a3ba74a37ddfe09bbda077a2fc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1026&#39; height=&#39;438&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1026\" data-rawheight=\"438\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1026\" data-original=\"https://pic1.zhimg.com/v2-8f46d8a3ba74a37ddfe09bbda077a2fc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-8f46d8a3ba74a37ddfe09bbda077a2fc_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>KTH这个数据集特别老，早就没人做了。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-27adcccf09c113b7918f9a87b7c337c4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"463\" data-rawheight=\"215\" class=\"origin_image zh-lightbox-thumb\" width=\"463\" data-original=\"https://pic1.zhimg.com/v2-27adcccf09c113b7918f9a87b7c337c4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;463&#39; height=&#39;215&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"463\" data-rawheight=\"215\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"463\" data-original=\"https://pic1.zhimg.com/v2-27adcccf09c113b7918f9a87b7c337c4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-27adcccf09c113b7918f9a87b7c337c4_b.jpg\"/></figure><h2>Action Recognition with Coarse-to-Fine Deep Feature Integration and Asynchronous Fusion</h2><h3>Weiyao Lin,1∗ Yang Mi,1 Jianxin Wu,2 Ke Lu,3 Hongkai Xiong1</h3><h3>1 Department of Electronic Engineering, Shanghai Jiao Tong University, China</h3><h3>2 National Key Laboratory for Novel Software Technology, Nanjing University, China</h3><h3>3 University of Chinese Academy of Sciences, China</h3><p>中科大，南京大学和上海交大的工作。</p><p>简述</p><p>这篇文章提出一个新的动作识别的架构。通过更精确的特征和减少异步的信息来提高识别精度。具体来说就是，用了从粗到细的网络融合，以及一种异步的融合。</p><p>其闪光点在于，利用top5，到top3 再到top1的方式，让网络渐进地学习动作类别。</p><p>引言</p><p>大部分的网络都是用特征来直接表征每个动作类别，但是每个类别其实有很多类间相似点，所以会对判别有限制。</p><p>他们的直觉是，动作类别之间的区别有大有小，所以用统一的特征去判别那几个类别，判别的能力会弱化。所以先用一些大体的特征来做粗放的识别top5，再用细腻的特征来做top1的识别。</p><p>然后在不同流的融合的时候，光流和RGB，都是在同一阶段融合，应该在不同阶段融合，做个异步的融合。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-6c76de862be01dc0389d0fa264499932_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"1001\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-6c76de862be01dc0389d0fa264499932_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;1001&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"1001\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-6c76de862be01dc0389d0fa264499932_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-6c76de862be01dc0389d0fa264499932_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>比如</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-3a7d8494e0b355c626a36a8235911c04_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"487\" data-rawheight=\"635\" class=\"origin_image zh-lightbox-thumb\" width=\"487\" data-original=\"https://pic1.zhimg.com/v2-3a7d8494e0b355c626a36a8235911c04_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;487&#39; height=&#39;635&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"487\" data-rawheight=\"635\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"487\" data-original=\"https://pic1.zhimg.com/v2-3a7d8494e0b355c626a36a8235911c04_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-3a7d8494e0b355c626a36a8235911c04_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>他们提取特征的方式是走了两个通道，空间和时间的融合。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-25cf5cd8f80fc0868651d97f55e7dd3e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"507\" data-rawheight=\"460\" class=\"origin_image zh-lightbox-thumb\" width=\"507\" data-original=\"https://pic3.zhimg.com/v2-25cf5cd8f80fc0868651d97f55e7dd3e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;507&#39; height=&#39;460&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"507\" data-rawheight=\"460\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"507\" data-original=\"https://pic3.zhimg.com/v2-25cf5cd8f80fc0868651d97f55e7dd3e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-25cf5cd8f80fc0868651d97f55e7dd3e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>他们用一个CNN_M_2018 的模型先做一个不同细腻度的分类：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-696ee8c8ee2c6c23787c5206ff5e3b7a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"490\" data-rawheight=\"395\" class=\"origin_image zh-lightbox-thumb\" width=\"490\" data-original=\"https://pic3.zhimg.com/v2-696ee8c8ee2c6c23787c5206ff5e3b7a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;490&#39; height=&#39;395&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"490\" data-rawheight=\"395\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"490\" data-original=\"https://pic3.zhimg.com/v2-696ee8c8ee2c6c23787c5206ff5e3b7a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-696ee8c8ee2c6c23787c5206ff5e3b7a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>然后这是异步的融合方式：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-f8f0275f8c64b5f8399c6fc18fe27082_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"490\" data-rawheight=\"377\" class=\"origin_image zh-lightbox-thumb\" width=\"490\" data-original=\"https://pic3.zhimg.com/v2-f8f0275f8c64b5f8399c6fc18fe27082_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;490&#39; height=&#39;377&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"490\" data-rawheight=\"377\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"490\" data-original=\"https://pic3.zhimg.com/v2-f8f0275f8c64b5f8399c6fc18fe27082_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-f8f0275f8c64b5f8399c6fc18fe27082_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>最后的结果：</p><p>UCF和HMDB上，他们粗细网络在双流上的表现：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-cb1959f74dca74bebd7096329fd5c2ab_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"496\" data-rawheight=\"242\" class=\"origin_image zh-lightbox-thumb\" width=\"496\" data-original=\"https://pic4.zhimg.com/v2-cb1959f74dca74bebd7096329fd5c2ab_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;496&#39; height=&#39;242&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"496\" data-rawheight=\"242\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"496\" data-original=\"https://pic4.zhimg.com/v2-cb1959f74dca74bebd7096329fd5c2ab_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-cb1959f74dca74bebd7096329fd5c2ab_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>加入了异步融合的表现：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-50e1161bf78f5afcd6cdb6b11b5bf14e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"467\" data-rawheight=\"203\" class=\"origin_image zh-lightbox-thumb\" width=\"467\" data-original=\"https://pic3.zhimg.com/v2-50e1161bf78f5afcd6cdb6b11b5bf14e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;467&#39; height=&#39;203&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"467\" data-rawheight=\"203\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"467\" data-original=\"https://pic3.zhimg.com/v2-50e1161bf78f5afcd6cdb6b11b5bf14e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-50e1161bf78f5afcd6cdb6b11b5bf14e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>总的表现，不用IDT 不行：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-df40a51063ceee1917d6391decffa5b4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"483\" data-rawheight=\"442\" class=\"origin_image zh-lightbox-thumb\" width=\"483\" data-original=\"https://pic1.zhimg.com/v2-df40a51063ceee1917d6391decffa5b4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;483&#39; height=&#39;442&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"483\" data-rawheight=\"442\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"483\" data-original=\"https://pic1.zhimg.com/v2-df40a51063ceee1917d6391decffa5b4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-df40a51063ceee1917d6391decffa5b4_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>一定要看看这个IDT 是个啥子。</p><h2>Unsupervised Deep Learning of Mid-Level Video Representation for Action Recognition</h2><h3>Jingyi Hou,1 Xinxiao Wu,1 Jin Chen,1 Jiebo Luo,2 Yunde Jia1</h3><h3>1. Beijing Laboratory of Intelligent Information Technology, School of Computer Science, Beijing Institute of Technology, Beijing 100081, China</h3><h3>2. Department of Computer Science, University of Rochester, Rochester NY 14627, USA</h3><p>北京理工大学和罗切斯特大学的工作。</p><p>简述</p><p>现在的动作识别的方法都是有监督的，基于大量的标注样本。这样标注的样本不仅昂贵，而且有人工偏差。</p><p>这篇文章的闪光点在于，用无监督的方法，从同一个动作类别中提取相关的中层的特征，比如打篮球的类，就提取篮球，球鞋这些中层的特征，这样就能实现无监督的分类。效果也会更好，理论上。</p><p>具体做法</p><p>他们先用一个预训练好的自编码器去提取特征，用低维度的特征在类内做聚类，得到比较优化的cluster，然后在test 的时候用这些聚类的cluster 来做中层的表征，用来实现分类问题。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-34f708bb951b15b77fa8fbab868d015b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1070\" data-rawheight=\"439\" class=\"origin_image zh-lightbox-thumb\" width=\"1070\" data-original=\"https://pic4.zhimg.com/v2-34f708bb951b15b77fa8fbab868d015b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1070&#39; height=&#39;439&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1070\" data-rawheight=\"439\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1070\" data-original=\"https://pic4.zhimg.com/v2-34f708bb951b15b77fa8fbab868d015b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-34f708bb951b15b77fa8fbab868d015b_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>聚类的时候，对encoder进行反向传播，finetune：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-8823bc05055b93ee2c7be22851c9499c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"489\" data-rawheight=\"490\" class=\"origin_image zh-lightbox-thumb\" width=\"489\" data-original=\"https://pic1.zhimg.com/v2-8823bc05055b93ee2c7be22851c9499c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;489&#39; height=&#39;490&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"489\" data-rawheight=\"490\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"489\" data-original=\"https://pic1.zhimg.com/v2-8823bc05055b93ee2c7be22851c9499c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-8823bc05055b93ee2c7be22851c9499c_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>最后的结果：</p><p>UCF和HMDB上，他们的表现：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-bad06cdae0b637b2cc975b7cf254c304_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"509\" data-rawheight=\"483\" class=\"origin_image zh-lightbox-thumb\" width=\"509\" data-original=\"https://pic1.zhimg.com/v2-bad06cdae0b637b2cc975b7cf254c304_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;509&#39; height=&#39;483&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"509\" data-rawheight=\"483\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"509\" data-original=\"https://pic1.zhimg.com/v2-bad06cdae0b637b2cc975b7cf254c304_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-bad06cdae0b637b2cc975b7cf254c304_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>用的是前人用的IT features，improved trajectory。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>T-C3D: Temporal Convolutional 3D Network for Real-time Action Recognition</h2><h3>Kun Liu,1 Wu Liu,1 Chuang Gan,2 Mingkui Tan,3 Huadong Ma1</h3><h3>1Beijing Key Lab of Intelligent Telecommunication Software and Multimedia, Beijing University of Posts and Telecommunications, Beijing 100876, China;</h3><h3>2Tsinghua University, Beijing, China; 3South China University of Technology, Guangzhou, China</h3><p>北邮和南科大的工作。</p><p>简述</p><p>现在的动作识别的方法都是有监督的，基于大量的标注样本。这样标注的样本不仅昂贵，而且有人工偏差。</p><p>这篇文章的创新点在于，不拿一个大的3DCNN 去做整段视频的卷积，而是分割成不同的片段，每个片段都提供一个probobility，然后来做汇总。</p><p>这样速度快，而且避免了有的片段和其他动作很混淆，在汇总的时候它的contribution可以平均掉。</p><p>其实是个很工程的东西，不过有代码：</p><p>code ：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/tc3d/tc3d\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/tc3d/tc3d</span><span class=\"invisible\"></span></a></p><p>用caffe做的</p><p>具体做法</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-b401af2b5a9f46599b6bba740d478d7f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"895\" data-rawheight=\"447\" class=\"origin_image zh-lightbox-thumb\" width=\"895\" data-original=\"https://pic4.zhimg.com/v2-b401af2b5a9f46599b6bba740d478d7f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;895&#39; height=&#39;447&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"895\" data-rawheight=\"447\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"895\" data-original=\"https://pic4.zhimg.com/v2-b401af2b5a9f46599b6bba740d478d7f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-b401af2b5a9f46599b6bba740d478d7f_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>我就很纳闷，这怎么能实时，</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-513e66812ebdb676e8a7f2662880211c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"510\" data-rawheight=\"210\" class=\"origin_image zh-lightbox-thumb\" width=\"510\" data-original=\"https://pic1.zhimg.com/v2-513e66812ebdb676e8a7f2662880211c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;510&#39; height=&#39;210&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"510\" data-rawheight=\"210\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"510\" data-original=\"https://pic1.zhimg.com/v2-513e66812ebdb676e8a7f2662880211c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-513e66812ebdb676e8a7f2662880211c_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>在最后的融合的时候，用attention pooling 最好。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-e0d0bcbeb4c211c82a673e06f1e209a7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"506\" data-rawheight=\"523\" class=\"origin_image zh-lightbox-thumb\" width=\"506\" data-original=\"https://pic4.zhimg.com/v2-e0d0bcbeb4c211c82a673e06f1e209a7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;506&#39; height=&#39;523&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"506\" data-rawheight=\"523\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"506\" data-original=\"https://pic4.zhimg.com/v2-e0d0bcbeb4c211c82a673e06f1e209a7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-e0d0bcbeb4c211c82a673e06f1e209a7_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>结果很一般，不过速度很快。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>Spatial-Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition</p><p class=\"ztext-empty-paragraph\"><br/></p><h3>Sijie Yan, Yuanjun Xiong, Dahua Lin</h3><h3><br/>Department of Information Engineering, The Chinese University of Hong Kong</h3><p>港中文的工作。yan sijie这个人代码写得非常漂亮，用起来很方便。</p><p>code：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/yysijie/st-gcn\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/yysijie/st-g</span><span class=\"invisible\">cn</span><span class=\"ellipsis\"></span></a></p><p>简述</p><p>现在基于骨骼的都是骨骼节点的卷积，但是骨骼本身其实就是一个图网络。于是他们就提出了这个概念。</p><p>我把这个工作复现了，其实就是普通的卷积网络，再用conv做时序的卷积得到最后的结果。</p><p>只能说，性能的提升来自于算力的强大，他用了八个TITAN，迭代了80个epoch 。。我如果用自己电脑整整要跑八天，用芬科院的4个P100跑，也要一两天。</p><p>具体做法</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-314bf0cd669761e04feccb10b738506a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"714\" data-rawheight=\"232\" class=\"origin_image zh-lightbox-thumb\" width=\"714\" data-original=\"https://pic3.zhimg.com/v2-314bf0cd669761e04feccb10b738506a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;714&#39; height=&#39;232&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"714\" data-rawheight=\"232\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"714\" data-original=\"https://pic3.zhimg.com/v2-314bf0cd669761e04feccb10b738506a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-314bf0cd669761e04feccb10b738506a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>此外，他还提出，直接把整个图抡进去，效果不好，要自己设计一下骨骼的连接方式。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-ea61555f836f71676d0cb21dfacb8a8d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"726\" data-rawheight=\"318\" class=\"origin_image zh-lightbox-thumb\" width=\"726\" data-original=\"https://pic2.zhimg.com/v2-ea61555f836f71676d0cb21dfacb8a8d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;726&#39; height=&#39;318&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"726\" data-rawheight=\"318\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"726\" data-original=\"https://pic2.zhimg.com/v2-ea61555f836f71676d0cb21dfacb8a8d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-ea61555f836f71676d0cb21dfacb8a8d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>测试了两个数据集，NTU 和Kinetic。</p><p>Kinetic没有skeleton，他就用OpenPose强行撸了一个skeleton来做比较。</p><p>可能他自己也知道，只有大的数据集才能发挥它的优势。</p><p>总的来说，这个工作就是一个暴力美学：</p><p>结果</p><p>消融实验 ：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-9417c8f01d1d042496c43e8eab68b055_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"360\" data-rawheight=\"395\" class=\"content_image\" width=\"360\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;360&#39; height=&#39;395&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"360\" data-rawheight=\"395\" class=\"content_image lazy\" width=\"360\" data-actualsrc=\"https://pic2.zhimg.com/v2-9417c8f01d1d042496c43e8eab68b055_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>我蛮佩服作者的，在Kinetic上的对比试验都是自己直接撸的，NTU上的效果当年还行，现在2019年就不行了：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-9417c8f01d1d042496c43e8eab68b055_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"360\" data-rawheight=\"395\" class=\"content_image\" width=\"360\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;360&#39; height=&#39;395&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"360\" data-rawheight=\"395\" class=\"content_image lazy\" width=\"360\" data-actualsrc=\"https://pic2.zhimg.com/v2-9417c8f01d1d042496c43e8eab68b055_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p>总结：<br/></p><p>几个大家都在做而且对性能很有帮助的点：</p><ol><li>对视频分段做会有大提升</li><li>无监督是趋势，大量数据集的标注太昂贵了，必须想办法</li><li>个人最喜欢的是coarse to fine 的那一篇，点子新颖，而且很自然。</li><li>石哥说得对，现在真的是工程为王。</li></ol><p>往期总结：</p><p><a href=\"https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzI4MTAyMzQ2OA%3D%3D%26mid%3D2650679415%26idx%3D2%26sn%3Dde4ebc9192da9d7de9c909ffb12a043a%26chksm%3Df3a50ef4c4d287e2d4abceadcca3172f1d529dae975ee00aa015848c6b9c0e69cbcc6938d0c8%26scene%3D21%23wechat_redirect\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">2018 动作识别的顶会文献总结</a></p><p><a href=\"https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzI4MTAyMzQ2OA%3D%3D%26mid%3D2650679493%26idx%3D2%26sn%3De69ac0e495f2514ced305f24e0dd898c%26chksm%3Df3a50e46c4d28750cb3a94d1fd358e482cc6506c917d8985ada5f3cc21c4bc128796d8575f25%26scene%3D21%23wechat_redirect\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">ECCV CVPR AAAI 2018年 Action recognition 的汇总 （ECCV 第二期）</a></p><p><a href=\"https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzI4MTAyMzQ2OA%3D%3D%26mid%3D2650679563%26idx%3D2%26sn%3D41a25b5c3acb24c4a04b1716085b4821%26chksm%3Df3a50f88c4d2869ee738d7e5c61dbd7432fbfbd250cd326bcb8cc97f22f73af0e3f2571ec2c1%26scene%3D21%23wechat_redirect\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">2018年 Action recognition 的汇总 （CVPR第一期）</a></p><p><a href=\"https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzI4MTAyMzQ2OA%3D%3D%26mid%3D2650679625%26idx%3D2%26sn%3Dde5d11b27f867ce573d065cc4828e46b%26chksm%3Df3a50fcac4d286dc2487f706d2d8abe575324008e694c530932df78e48411d5c132312bdb67d%26scene%3D21%23wechat_redirect\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">2018年 ACTION RECOGNITION 的汇总 （CVPR下期）</a> </p><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p>往期相关，推荐你看<br/></p><p>用我的专业缅怀金庸 </p><p><a href=\"https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzI4MTAyMzQ2OA%3D%3D%26mid%3D2650679338%26idx%3D1%26sn%3Dd1f8684d2046db9dae20ce4f1921ce23%26chksm%3Df3a50ea9c4d287bf6d46248d326cbe315e4a0c4c2a17c0bd20d422226d20ec1e89c0586705d8%26scene%3D21%23wechat_redirect\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">我试着训练了一下神经网络来生成金庸小说</a><br/></p><p>跟风最近换脸的技术</p><p><a href=\"https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzI4MTAyMzQ2OA%3D%3D%26mid%3D2650679493%26idx%3D1%26sn%3D8b34239595dd02c75479fd621bfce5f3%26chksm%3Df3a50e46c4d287502016b1022921507991838412ad69c97545afb76e1a5e55258e1690ac4cc0%26scene%3D21%23wechat_redirect\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">用 AI技术进行换脸（洪山陈冠希的诞生）</a><br/></p><p>浅谈今年图灵奖得主</p><p><a href=\"https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzI4MTAyMzQ2OA%3D%3D%26mid%3D2650679563%26idx%3D1%26sn%3De6a6a6a5012514447f79ab19a09d6493%26chksm%3Df3a50f88c4d2869eee30d1f0584190f4037eba33543a7697ace76ad60495b8d75a30407882c4%26scene%3D21%23wechat_redirect\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">月亮与六便士</a></p><p class=\"ztext-empty-paragraph\"><br/></p><p>这么多年线代白学了</p><p><a href=\"https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzI4MTAyMzQ2OA%3D%3D%26mid%3D2650679315%26idx%3D2%26sn%3Ddaf4874e54fbd3dfa13e8c61703b6d01%26chksm%3Df3a50e90c4d287865c427e3f2a8d69dd9c8f7394cb48b0219b8d5974f0f802ba95993683a2b3%26scene%3D21%23wechat_redirect\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">十分钟理解线性代数的本质</a></p><p class=\"ztext-empty-paragraph\"><br/></p><p>关注我的公众号，获取更多有趣好玩的内容</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-f0c8de24388943709d2f92cef42fc45b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"258\" data-rawheight=\"258\" class=\"content_image\" width=\"258\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;258&#39; height=&#39;258&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"258\" data-rawheight=\"258\" class=\"content_image lazy\" width=\"258\" data-actualsrc=\"https://pic4.zhimg.com/v2-f0c8de24388943709d2f92cef42fc45b_b.jpg\"/></figure><p></p><p></p>", 
            "topic": [
                {
                    "tag": "机器视觉技术", 
                    "tagLink": "https://api.zhihu.com/topics/20373729"
                }, 
                {
                    "tag": "图像识别", 
                    "tagLink": "https://api.zhihu.com/topics/19588774"
                }, 
                {
                    "tag": "模式识别", 
                    "tagLink": "https://api.zhihu.com/topics/19564812"
                }
            ], 
            "comments": [
                {
                    "userName": "WD小良子", 
                    "userLink": "https://www.zhihu.com/people/c4cffce5b8e34b378d4e1ad764939304", 
                    "content": "<p>对于骨骼数据集中每个动作帧数不一样，那怎么处理成同维的训练数据呢？</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "奥卢的陈皓宇", 
                            "userLink": "https://www.zhihu.com/people/6e25ca19a73ea5a2d81f195cf4c1851f", 
                            "content": "<p>常见的就是插值或者重复成相同帧数</p>", 
                            "likes": 0, 
                            "replyToAuthor": "WD小良子"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/65424635", 
            "userName": "奥卢的陈皓宇", 
            "userLink": "https://www.zhihu.com/people/6e25ca19a73ea5a2d81f195cf4c1851f", 
            "upvote": 6, 
            "title": "2018年 ACTION RECOGNITION 的汇总 （CVPR下期）", 
            "content": "<p>把ECCV和CVPR还有AAAI 2018年的action recognition汇了个总，放在这里。<br/>这里是第四次更新，上次更新总结了CVPR的前半部分相关文章。<br/>这一次总结CVPR后半部分的文章。<br/>所有文章汇总请看第一期。</p><hr/><p>CVPR 2018<br/>上期：<br/><u><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_cvpr_2018/html/Tang_Deep_Progressive_Reinforcement_CVPR_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Deep Progressive Reinforcement Learning for Skeleton-Based Action Recognition</a></u><br/><u><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_cvpr_2018/html/Zhou_MiCT_Mixed_3D2D_CVPR_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">MiCT: Mixed 3D/2D Convolutional Tube for Human Action Recognition</a></u><br/><u><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_cvpr_2018/html/Sun_Optical_Flow_Guided_CVPR_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Optical Flow Guided Feature: A Fast and Robust Motion Representation for Video Action Recognition</a></u><br/><u><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_cvpr_2018/html/Luvizon_2D3D_Pose_Estimation_CVPR_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">2D/3D Pose Estimation and Action Recognition Using Multitask Deep Learning</a></u><br/>本期：</p><ul><li><b><u><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_cvpr_2018/html/Wu_Compressed_Video_Action_CVPR_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Compressed Video Action Recognition</a></u></b></li><li><b><u><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_cvpr_2018/html/Tran_A_Closer_Look_CVPR_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">A Closer Look at Spatiotemporal Convolutions for Action Recognition</a></u></b></li><li><b><u><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_cvpr_2018/html/Lei_Temporal_Deformable_Residual_CVPR_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Temporal Deformable Residual Networks for Action Segmentation in Videos</a></u></b></li><li><b><u><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_cvpr_2018/html/Choutas_PoTion_Pose_MoTion_CVPR_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">PoTion: Pose MoTion Representation for Action Recognition</a></u></b></li><li><b><u><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_cvpr_2018/html/Feichtenhofer_What_Have_We_CVPR_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">What Have We Learned From Deep Representations for Action Recognition?</a></u></b></li><li><b><u><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_cvpr_2018/html/Zhu_Towards_Universal_Representation_CVPR_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Towards Universal Representation for Unseen Action Recognition</a></u></b><br/>以下是正文</li></ul><hr/><p><b><u><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_cvpr_2018/html/Wu_Compressed_Video_Action_CVPR_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">COMPRESSED VIDEO ACTION RECOGNITION</a></u></b><br/>CHAO-YUAN WU1,5 MANZIL ZAHEER2,5∗ HEXIANG HU3,5∗ R. MANMATHA4, ALEXANDER J. SMOLA5<br/>PHILIPP KRAHENBUHL1<br/>1THE UNIVERSITY OF TEXAS AT AUSTIN,<br/>2CARNEGIE MELLON UNIVERSITY,<br/>3UNIVERSITY OF SOUTHERN CALIFORNIA, 4A9, 5AMAZON<br/>德州奥斯丁大学，CMU 大学以及亚马逊的合作工作。<br/>Github 代码 （Pytorch）：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/chaoyuaw/pytorch-coviar\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/chaoyuaw/pyt</span><span class=\"invisible\">orch-coviar</span><span class=\"ellipsis\"></span></a><br/>摘要：事实证明，训练强大的深度视频表示比学习深度图像表示更具挑战性。 这部分是由于原始视频流的巨大规模和高时间冗余; 真实有趣的信号经常被无用的数据淹没。 由于通过视频压缩（使用H.264，HEVC等）可以将多余信息减少多达两个数量级，我们建议直接在压缩视频上训练深度网络。 这种表示具有更高的信息密度，我们发现训练更容易。 此外，压缩视频中的信号提供免费的，尽管是噪声的运动信息。 我们提出了有效使用它们的新技术。 我们的方法比Res3D快4.6倍，比ResNet-152快2.7倍。 关于动作识别的任务，我们的方法优于UCF-101，HMDB-51和Charades数据集上的所有其他方法<br/>一句话来说就是，压缩了的视频其实对很有用，他们改善了现有的压缩算法用在action上。<br/>主要考虑两个重要的因素：<br/>（1）冗余信息太多.视频的信息密度非常低。例如一个长度为1小时的720p视频可以通过压缩技术从222G压缩到1G；换句话说，视频中的大多数信息是冗余的，而真正有用的信息就很容被淹没其中，对于CNNs这样的深度网络来说，也很难从大量的冗余信息中获取有价值的信息。<br/>（2）只有RGB 而没有很多运动信息。单从RGB图像中是很难学到时序结构。有大量的文献是通过RGB图像序列来处理视频，如2D CNNs，3D CNNs，或RNNs。这一部分后来人们发现可以通过optical flow来提升效果。<br/>概要：<br/>视频识别研究在深度学习中是具有挑战性的领域，因为视频中包含了巨大的信息，而重要的信息就很容易被大量的冗余信息所淹没。<br/>同时，作者发现通过视频压缩技术（H.264, HEVC等）可以将冗余信息减少高达两个量级，这样的压缩视频拥有更高的信息密度。<br/>为此，作者通过实验，设计了一种新颖的技术，可以直接将压缩后的视频作为深度神经网络的输入。<br/>作者提出的这种方法要比普通的Res3D快4.6倍，比普通的ResNet-152快2.7倍；且在UCF101，HMDB-51等数据集上取得了不错的效果。<br/>CoViAR模型与传统模型的输入差异比较：<br/>模型设计：<br/>为了处理上述问题，作者使用视频的压缩格式（MPEG4等）作为输入。<br/>这种压缩技术可以很好的平衡连续相似帧：压缩数据中会保留很少的完整图像，并且会根据称为motion vector和residual error的偏移量来重构其他帧。<br/>作者为此设计了multiple CNNs，可以直接将压缩数据中的RGB images，motion vectors 和residuals作为输入。<br/>Multiple CNNs模型结构：<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-5b7113c5939e862db1a76ab929606692_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"340\" data-rawheight=\"341\" class=\"content_image\" width=\"340\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;340&#39; height=&#39;341&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"340\" data-rawheight=\"341\" class=\"content_image lazy\" width=\"340\" data-actualsrc=\"https://pic3.zhimg.com/v2-5b7113c5939e862db1a76ab929606692_b.jpg\"/></figure><p><br/>优点：<br/>1. 这种压缩技术可以降低两个数量级的视频冗余信息，这样可以使重要信息更加突出。（two orders of magnitude of superfluous information）<br/>2. Motion vector 可以提供单RGB图像所提供不了的motion信息。<br/>3. 通过视频压缩，我们可以获得视频中帧与帧之间的相关性（Residuals）。例如视频中当前的spatial view 是在前一时刻的基础上加一少部分的变化，而不是将每帧视作独立同分布。<br/>这样可以大大降低输入的dimensions。<br/>4. 消除了冗余信息，且避免了视频的解压缩（视频通常以压缩形式存储），让模型的效率得到提升。<br/>视频压缩技术：<br/>目前大部分的视频压缩算法（MPEG-4，H.264，HEVC等）是基于连续帧具有高度相似的特征。<br/>这样可以只存储一帧和与其他帧的差异值，就可以根据该帧和对应差异值来得到其他帧。<br/>当前大部分的编码器将视频分为I-frames (intra-coded frames), P-frames (predictive frames) 和 zero or B-frames (bi-directional frames)<br/>I-frames是正规图像；P-frames根据前一帧，只编码变化部分，变化的这一部分可以是为 motion vector；<br/>若计算t时刻的图像，只有t-1时刻的图像I，和t时刻的P还是不够的，它们组成的图像与t时刻的真实图像还是有误差存在的，这个误差就是residuals；<br/>B-frames可是视为特殊的P-frames，motion vector of B-frames computed by bi-directionally。<br/>在本论文中，作者主要用到了视频压缩数据中I-frames和P-frames<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-c99c0d671bf89138701df1d2d9f61c33_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"343\" data-rawheight=\"216\" class=\"content_image\" width=\"343\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;343&#39; height=&#39;216&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"343\" data-rawheight=\"216\" class=\"content_image lazy\" width=\"343\" data-actualsrc=\"https://pic4.zhimg.com/v2-c99c0d671bf89138701df1d2d9f61c33_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-52b51f408db7b1a8e8276922ef3f0908_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"377\" data-rawheight=\"226\" class=\"content_image\" width=\"377\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;377&#39; height=&#39;226&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"377\" data-rawheight=\"226\" class=\"content_image lazy\" width=\"377\" data-actualsrc=\"https://pic1.zhimg.com/v2-52b51f408db7b1a8e8276922ef3f0908_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-1ba45f75b8adad6ca45e03e4413da595_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"356\" data-rawheight=\"195\" class=\"content_image\" width=\"356\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;356&#39; height=&#39;195&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"356\" data-rawheight=\"195\" class=\"content_image lazy\" width=\"356\" data-actualsrc=\"https://pic2.zhimg.com/v2-1ba45f75b8adad6ca45e03e4413da595_b.jpg\"/></figure><p><br/><b><u><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_cvpr_2018/html/Tran_A_Closer_Look_CVPR_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">A CLOSER LOOK AT SPATIOTEMPORAL CONVOLUTIONS FOR ACTION RECOGNITION</a></u></b><br/>DU TRAN1, HENG WANG1, LORENZO TORRESANI1,2, JAMIE RAY1, YANN LECUN1, MANOHAR PALURI1<br/>1FACEBOOK RESEARCH<br/>2DARTMOUTH COLLEGE<br/>主要讨论了时空卷积的几种网络结构，在Action Recognition 的几个标准数据集上也取得了媲美最好方法的效果。作者是FAIR的工作人员，其中包括Du Tran(C3D)作者，Heng Wang(iDT)作者和Yann LecCun等。<br/>在本文中，我们讨论了视频分析的几种形式的时空卷积，并研究它们对动作识别的影响。 我们的动机源于观察到应用于视频的各个帧的2D CNN在动作识别中仍然是稳固的表演者。 在这项工作中，我们在残差学习的框架内凭经验证明了3D CNN相对于2D CNN的准确性优势。 此外，我们表明，将3D卷积滤波器分解为单独的空间和时间分量可以显着提高准确度。 我们的实证研究导致设计了一个新的时空卷积块“R（2 + 1）D”，它产生的CNN可以达到与Sports-1M，Kinetics，UCF101和HMDB51上最先进技术相当或更优的结果。<br/>和在上一期讨论的微软做的MiCT有异曲同工之妙。<br/>几种网络的对比<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-9d4e68ea46b17754fa70cf13c03be526_b.jpg\" data-caption=\"\" data-size=\"normal\" class=\"content_image\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;0&#39; height=&#39;0&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" class=\"content_image lazy\" data-actualsrc=\"https://pic3.zhimg.com/v2-9d4e68ea46b17754fa70cf13c03be526_b.jpg\"/></figure><p>网络结构如图Figure 1所示，具体每种网络陈述如下。<br/>R2D: 整个CLIP上的2D卷积网络<br/>R代表ResNet， 即残差网络。 R2D将L帧，宽高分别为W，H的一个视频clip当成3LxWxH的3D tensor输入网络，得到的还是3D的tensor。虽然是3D tensor，实际的卷积是2D卷积，因此时间信息是全部丢失了的。<br/>F-R2D: 帧层面的2D卷积网络<br/>跟R2D不同，f-R2D中没有将整个clip的L帧当作不同的channel，而是每个frame单独的作用卷积 （原文： The same filters are applied to all L frames）。 和R2D一样，这种方法也没有保留时间维度的信息。但是所有的帧都公用一套filter<br/>R3D: 3D的RESNET<br/>这个就是标准的3D ResNet结构，即将输入看作Ni * L * W * H 的4D tensor, 卷积核也是4D的。<br/>时间维度是有卷积的，因此时序信息能够保留下来。<br/>MCX和RMCX: 混合2D和3D卷积的结构<br/>有一种观点认为卷积网络较低层对motion的建模比较好，而高层由于特征已经很抽象了，motion和时序信息建模是不需要的，因此作者提出了MCx网络，即将第x以及后面的3D卷积group换为2D的卷积group，而R3D总共有5个卷积group（具体参数见Table 1），因此像MC4表示将group 4和group 5中的卷积和都换为2D卷积，而前面的group 1-3则保留为3D卷积。 注意此时MC1等效于f-R2D，即所有的层都是2D卷积。<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-922378d845fd3436f2bdd6713b107b2f_b.jpg\" data-caption=\"\" data-size=\"normal\" class=\"content_image\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;0&#39; height=&#39;0&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" class=\"content_image lazy\" data-actualsrc=\"https://pic4.zhimg.com/v2-922378d845fd3436f2bdd6713b107b2f_b.jpg\"/></figure><p>同时还有一种假设认为高层的信息需要用3D卷积来建模，而底层的信息通过2D卷积就可以获取，因此作者提出了rMCx结构，前面的<code>r</code>代表reverse，即反向的意思。rMCx表示前面的5-x层为2D卷积，后面的x层为3D卷积。<br/>R(2+1)D: 拆分3D卷积为2D卷积+1D卷积<br/>在这个模型中第i层的Ni个尺寸为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-9db7b777fac4374ffa875ad0def68cff_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"145\" data-rawheight=\"20\" class=\"content_image\" width=\"145\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;145&#39; height=&#39;20&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"145\" data-rawheight=\"20\" class=\"content_image lazy\" width=\"145\" data-actualsrc=\"https://pic4.zhimg.com/v2-9db7b777fac4374ffa875ad0def68cff_b.png\"/></figure><p>的3D卷积核被替代为一个(2+1)D block，包含Mi个尺寸为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-1f3c2348e3a9ba13153960298081c309_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"152\" data-rawheight=\"20\" class=\"content_image\" width=\"152\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;152&#39; height=&#39;20&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"152\" data-rawheight=\"20\" class=\"content_image lazy\" width=\"152\" data-actualsrc=\"https://pic2.zhimg.com/v2-1f3c2348e3a9ba13153960298081c309_b.png\"/></figure><p>的2D卷积核和Ni个尺寸为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-264f8643367ffc651b8153647e5db17b_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"129\" data-rawheight=\"18\" class=\"content_image\" width=\"129\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;129&#39; height=&#39;18&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"129\" data-rawheight=\"18\" class=\"content_image lazy\" width=\"129\" data-actualsrc=\"https://pic4.zhimg.com/v2-264f8643367ffc651b8153647e5db17b_b.png\"/></figure><p>的1D（temporal维度的）卷积核，也就是说先用Mi个2D卷积核用输入数据生成channel数为Mi的tensor，之后再用temporal维度上的卷积将channel数变为Ni，也即是第i层输出channel数，这里超参数Mi是连接时间和空间卷积的intermediate subspace的维度（也就是channel数），这个参数的数量由下式确定<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-4367472e9a77c75ca159bbba8b0dfcc5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"207\" data-rawheight=\"44\" class=\"content_image\" width=\"207\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;207&#39; height=&#39;44&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"207\" data-rawheight=\"44\" class=\"content_image lazy\" width=\"207\" data-actualsrc=\"https://pic2.zhimg.com/v2-4367472e9a77c75ca159bbba8b0dfcc5_b.jpg\"/></figure><p><br/>使用这个式子是想让R(2+1)D block的参数数量大致和完整的3D卷积block参数数量相等。<br/>本文的实验部分就是对这些网络结构进行了试验，实验的结果是R(2+1)D网络在数据集Sports-1M、Kinetics、UCF101和HMDB51上达到或者是超越了state-of-the-art的水准，除此之外，实验结果还表明，在本文所用的数据集上，本文所使用的3D模型比2D模型效果要好，作者借此说明motion modeling的重要性，但是我觉得，在特定的数据集上，基于特定的网络结构，对比3D和2D卷积，而且只能从实验结果来分析，其实不一定能说明太多问题，没准不在resnet的框架下，或者换个数据集，整个实验结果就会改变。<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-97ca15743a303e8b2c51c99aec99d1d4_b.jpg\" data-caption=\"\" data-size=\"normal\" class=\"content_image\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;0&#39; height=&#39;0&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" class=\"content_image lazy\" data-actualsrc=\"https://pic1.zhimg.com/v2-97ca15743a303e8b2c51c99aec99d1d4_b.jpg\"/></figure><p>Figure 2<br/>这种从3D到(2+1)D的拆分有下面两个好处：<br/></p><ol><li>增加了非线性的层数，因为从图2可以看到，原先的1个卷积变成2个卷积，而2个卷积之间多了非线性层（通过ReLU来得到）， 因此总体的非线性层增加了。 用同样的参数来得到增加非线性的目的。</li><li>使得网络优化更容易，这个可以参考Figure 4中的结果，可以看到R(2+1)D的训练错误率比R3D更低，说明网络更易于训练。</li></ol><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ec178dcb0a64d12fa546d55280b4d9b0_b.jpg\" data-caption=\"\" data-size=\"normal\" class=\"content_image\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;0&#39; height=&#39;0&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" class=\"content_image lazy\" data-actualsrc=\"https://pic1.zhimg.com/v2-ec178dcb0a64d12fa546d55280b4d9b0_b.jpg\"/></figure><p>另外作者还和P3D进行了比较，因为两者结构确实比较类似。<br/>2. 实验设置<br/>作者在视频动作识别的中型和大型数据集上都做了实验，包括HMDB51, UCF101， Sport-1M 和 Kinetics这几个数据集。<br/>由于前面讨论的都是残差网络，因此实验中的网络都采用了残差网络。对R3D网络，作者采取了2种结构，包括18层的和34层的，图片输入采用了8帧的clip，图像大小为112×112。在3D网络的基础上，进行修改来得到R2D, MCx和rMCx，R(2+1)D等结构。 需要注意的是，由于不同网络结构时间维度的卷积和stride操作和个数不同，因此输出的feature map的时间维度是不一致的，为了方便统一比较，作者在卷积层最后的feature map后跟了一个时间空间的average pooling，然后晋国一个维度为K的fc层，$K$为数据集对应的类别，如对UCF101数据集，$K$=101。<br/>视频帧数据首先被缩放到128×171，然后通过随机crop112x112的区域得到clip。训练时还应用了时域上的抖动。每个卷积层后面还使用到了BN。训练是batch size设置为32个clip，初始学习率设置为0.01，然后每过10个周期下降为原来的1/10，总共训练45个周期。video-level的准确率是在clip-level的准确率上得到的，即随机在视频中选择10个clip，然后对每个clip做center crop得到最后的clip，将这10个clip单独训练，结果进行一个平均，即为video-level的准确率。实验中采用caffe2在GPU cluster进行训练。<br/>3. 实验分析<br/>不同网络结构性能分析<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-e15c4229aacfb46690cfc6fdf6af4ba8_b.jpg\" data-caption=\"\" data-size=\"normal\" class=\"content_image\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;0&#39; height=&#39;0&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" class=\"content_image lazy\" data-actualsrc=\"https://pic1.zhimg.com/v2-e15c4229aacfb46690cfc6fdf6af4ba8_b.jpg\"/></figure><p>由于这部分实验比较的是不同网络结构的性能，因此作者只在Kinetics上用18层的ResNet进行了实验，具体结果见Table 2。这里主要的结论有下面几点：<br/></p><ol><li>纯2D网络（包括R2D和f-R2D）比含3D的网络（包括R3D, MCx,rMCx, R(2+1)D）性能要差</li><li>R(2+1)D性能最好</li><li>MCx性能优于rMCr，因此说明在网络底层的3D卷积层更有用，而后面用2D卷积更合理。</li></ol><p>不同clip长度分析<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-71b59cd73a7e94af9a7b761c1661280f_b.jpg\" data-caption=\"\" data-size=\"normal\" class=\"content_image\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;0&#39; height=&#39;0&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" class=\"content_image lazy\" data-actualsrc=\"https://pic4.zhimg.com/v2-71b59cd73a7e94af9a7b761c1661280f_b.jpg\"/></figure><p>作者采用了8，16，24，32，40和48帧的clip进行实验，对clip-level的结果和video-level的结果进行分析，得到的准确率如Figure 5所示。可以看到，clip-level的准确率随着clip的长度增长在持续上升，而video-level的准确率则在24帧的时候达到最高，后面反倒有所下降，作者分析随着clip长度的增加，不同clip之间的相关性增加（甚至可能会产生重叠），所以video-level的准确率增益越来越小。 为了分析video-level准确率下降的原因，作者又做了两个实验：<br/></p><ol><ol><li>采用8帧的clip训练网络，然后在32帧的的clip上测试，发现结果相比用8帧的clip做测试，clip-level的准确率下降2.6%</li><li>在8帧的clip上训练的网络的基础上，采用32帧的clip进行fine tune，得到的clip-level的准确率与32帧从头训练的结果相差不多（56.8% vs 58.5%），而比8帧的clip的clip-level结果高4.4%。因此用长的clip结果更高说明学到了long-term的时间域上的信息</li></ol></ol><p class=\"ztext-empty-paragraph\"><br/></p><p>不同图片分辨率的分析<br/>作者采用了224×224的输入训练网路，发现和112×112的输入结果只有微小的差距。<br/>和现有方法在4个动作识别数据集上的性能分析<br/>为了和目前最好的方法进行PK，作者采用了34层的ResNet网络，结构采用R(2+1)D。在Sports-1M上，取得了目前最好的性能，而在Kinetics上，RGB单路性能比I3D高4.5%，而RGB和光流融合后性能比I3D的融合结果稍微差些。在UCF101和HMDB51上，使用Sports-1M和Kinetics上预训练的模型，fine tune后性能有较大提升。<br/>分析<br/>今年做网络结构优化的工作很多，可能是I3D网络讨论引起的新的风潮。我们当时觉得I3D在UCF101和HMDB51上做这么高，需要换数据集了，因此看了看Charades数据集，但是好像今年做Charades数据集的工作还是比较少。接下来还是得在Kinetics上做了，但是在国内网络情况下，数据下载还是挺捉急的。<br/>总体来说论文较多篇幅介绍了各种不同的网络，最后实验证明了MCx比rMCx好，但是其中的原理没怎么分析，而且最后采用了R(2+1)D，而且其效果最好，因此MCx实际没有使用的价值了。根据本文的结论，以后应该采用R(2+1)D的结构，能达到最好的性能。<br/><b><u><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_cvpr_2018/html/Lei_Temporal_Deformable_Residual_CVPR_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">TEMPORAL DEFORMABLE RESIDUAL NETWORKS FOR ACTION SEGMENTATION IN VIDEOS</a></u></b><br/>PENG LEI AND SINISA TODOROVIC<br/>OREGON STATE UNIVERSITY<br/>简述<br/>本文是关于视频中人类行为的时间分割。我们引入了一种新的模型 – 时间可变形残差网络（TDRN） – 旨在分析多个时间尺度的视频间隔，以标记视频帧。我们的TDRN计算两个并行时间流：i）以完整时间分辨率分析视频信息的剩余流，以及ii）以不同比例捕获远程视频信息的池/解组流。<br/>前者促进局部的精细尺度动作分割，后者使用多尺度上下文来提高帧分类的准确性。这两个流由具有可变形卷积的一组时间残余模块计算，并且在完整视频分辨率下由时间残差融合。我们对50 Salads, Georgia Tech Egocentric Activities,和JHU-ISI手势和技能评估工作集的评估表明，TDRN在 frame-wise segmentation accuracy, segmental edit score和F1 score方面优于现有技术水平。<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-375631bcc135d1674486374fb0a1d70e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"332\" data-rawheight=\"491\" class=\"content_image\" width=\"332\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;332&#39; height=&#39;491&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"332\" data-rawheight=\"491\" class=\"content_image lazy\" width=\"332\" data-actualsrc=\"https://pic3.zhimg.com/v2-375631bcc135d1674486374fb0a1d70e_b.jpg\"/></figure><p><br/>TDRN 的主要思想就是，用一个残差网络来做frame level的action recognition，然后用多个可以在frame尺度上形变的TCN 来实现high level的切割，pooling可以用来提取high level的特征和appearance。<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-1d9d42858a1217873dbebc989ab5b1ba_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"344\" data-rawheight=\"336\" class=\"content_image\" width=\"344\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;344&#39; height=&#39;336&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"344\" data-rawheight=\"336\" class=\"content_image lazy\" width=\"344\" data-actualsrc=\"https://pic3.zhimg.com/v2-1d9d42858a1217873dbebc989ab5b1ba_b.jpg\"/></figure><p><br/>对比了一下类似的三个网络的区别。自编码器，U网络，和残差网络。但是都没有用到形变。<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-2f62f3734038c50eee5de633989f7e1e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"341\" data-rawheight=\"331\" class=\"content_image\" width=\"341\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;341&#39; height=&#39;331&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"341\" data-rawheight=\"331\" class=\"content_image lazy\" width=\"341\" data-actualsrc=\"https://pic3.zhimg.com/v2-2f62f3734038c50eee5de633989f7e1e_b.jpg\"/></figure><p><br/>结构如上图所示，思路并不复杂，但是工程量不小。<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-8ff8ff47cadfda5a287aba86317cf348_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"343\" data-rawheight=\"241\" class=\"content_image\" width=\"343\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;343&#39; height=&#39;241&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"343\" data-rawheight=\"241\" class=\"content_image lazy\" width=\"343\" data-actualsrc=\"https://pic1.zhimg.com/v2-8ff8ff47cadfda5a287aba86317cf348_b.jpg\"/></figure><p><br/>在三个数据集的表现结果如下：<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-9e7c4158389fb49596bbfbdf84949f8c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"300\" data-rawheight=\"234\" class=\"content_image\" width=\"300\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;300&#39; height=&#39;234&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"300\" data-rawheight=\"234\" class=\"content_image lazy\" width=\"300\" data-actualsrc=\"https://pic1.zhimg.com/v2-9e7c4158389fb49596bbfbdf84949f8c_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-a9fde82e60e82c067334dbcabf2f003e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"286\" data-rawheight=\"210\" class=\"content_image\" width=\"286\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;286&#39; height=&#39;210&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"286\" data-rawheight=\"210\" class=\"content_image lazy\" width=\"286\" data-actualsrc=\"https://pic3.zhimg.com/v2-a9fde82e60e82c067334dbcabf2f003e_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-752cd7c7a5d9d06f542e8ceae3d9741a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"277\" data-rawheight=\"210\" class=\"content_image\" width=\"277\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;277&#39; height=&#39;210&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"277\" data-rawheight=\"210\" class=\"content_image lazy\" width=\"277\" data-actualsrc=\"https://pic3.zhimg.com/v2-752cd7c7a5d9d06f542e8ceae3d9741a_b.jpg\"/></figure><p><br/>可以发现，BiLSTM真的不怎么样，STCNN和TCN还是最基础的。时序的残差网络会带来一个质的提升。<br/>和第一篇文章压缩网络有点相似。<br/><b><u><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_cvpr_2018/html/Choutas_PoTion_Pose_MoTion_CVPR_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">POTION: POSE MOTION REPRESENTATION FOR ACTION RECOGNITION</a></u></b><br/>VASILEIOS CHOUTAS1,2 PHILIPPE WEINZAEPFEL2 JER´ OME REVAUD 2 CORDELIA SCHMID1<br/>1INRIA∗<br/>2NAVER LABS EUROPE<br/>简述<br/>这个工作的核心思想就是制作一个每个关节点的热图，然后形成一个轨迹。成为新的特征表示。<br/>大多数最先进的动作识别方法都依赖于独立处理外观和运动的双流架构。在本文中，我们声称共同考虑它们提供了丰富的行动识别信息。我们引入了一种新颖的表示，它优雅地编码了一些语义关键点的运动。我们使用人体关节作为这些关键点，并将术语定义为Pose moTion。具体来说，我们首先运行一个最先进的人体姿势估计器[4]并提取每个帧中人体关节的热图。我们通过时序聚合这些概率图来获得我们的PoTion表示。这是通过根据视频剪辑中帧的相对时间对它们进行“着色”并对它们求和来实现的。整个视频剪辑的这种固定大小的表示适合于使用浅卷积神经网络对动作进行分类。我们的实验评估表明，PoTion优于其他最先进的姿势表示[6,48]。此外，它与标准外观和运动流互补。当将PoTion与最近的双流I3D方法相结合[5]时，我们在JHMDB，HMDB和UCF101数据集上获得了最先进的性能。<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ac822e28dc2bbfae9fd262b5fd3e573c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"559\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic1.zhimg.com/v2-ac822e28dc2bbfae9fd262b5fd3e573c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;640&#39; height=&#39;559&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"559\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https://pic1.zhimg.com/v2-ac822e28dc2bbfae9fd262b5fd3e573c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-ac822e28dc2bbfae9fd262b5fd3e573c_b.jpg\"/></figure><p><br/>这个和上一期的最后一篇，骨骼检测和动作识别的多任务很像。<br/>首先在每个帧中运行目前最先进的人体姿态估计器，并为每个人体关节获取热图。<b>这些热图对每个像素的概率进行编码以包含特定的关节</b>。<b>我们使用取决于视频片段帧的相对时间的颜色对这些热度图进行着色</b>。如下图所示的为不同通道下的随时间的上色机制：<br/>对于每个关节，我们对所有帧上的彩色热图进行求和，以获得整个视频片段的PoTion表示。如下图所示为某一关节点聚合之后的色彩图，使用了不同的聚合方式：<br/>给定这种表示形式，我们训练一个浅层CNN架构，包含6个卷积层和一个完全连接的层来执行动作分类，CNN结构如下：<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-c3228915982d1985d68f0cd49dcd9ddc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"424\" data-rawheight=\"235\" class=\"origin_image zh-lightbox-thumb\" width=\"424\" data-original=\"https://pic1.zhimg.com/v2-c3228915982d1985d68f0cd49dcd9ddc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;424&#39; height=&#39;235&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"424\" data-rawheight=\"235\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"424\" data-original=\"https://pic1.zhimg.com/v2-c3228915982d1985d68f0cd49dcd9ddc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-c3228915982d1985d68f0cd49dcd9ddc_b.jpg\"/></figure><p><br/>整个这个网络可以从头开始训练，并胜过其他姿势表示。而且，由于网络很浅并且以整个视频clip的紧凑表示为输入，因此训练例如非常快速。在一台用于HMDB的GPU上只需要4个小时，而标准的双流方法则需要几天的培训和仔细的初始化。另外，PoTion可以看做是标准外观和运动流的补充。与RGB和光学流程的I3D 结合使用时，我们在JHMDB，HMDB，UCF101上获得了最先进的性能。<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-45723a82f77165f9570432a042d56dec_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"440\" data-rawheight=\"418\" class=\"origin_image zh-lightbox-thumb\" width=\"440\" data-original=\"https://pic1.zhimg.com/v2-45723a82f77165f9570432a042d56dec_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;440&#39; height=&#39;418&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"440\" data-rawheight=\"418\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"440\" data-original=\"https://pic1.zhimg.com/v2-45723a82f77165f9570432a042d56dec_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-45723a82f77165f9570432a042d56dec_b.jpg\"/></figure><p><br/>单独使用结果很差，但是结合了I3D就能达到state of the art。<br/><b><u><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_cvpr_2018/html/Feichtenhofer_What_Have_We_CVPR_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">WHAT HAVE WE LEARNED FROM DEEP REPRESENTATIONS FOR ACTION RECOGNITION?</a></u></b><br/>Christoph Feichtenhofer*, Axel Pinz, Richard P. Wildes, Andrew Zisserman<br/>TU Graz<br/>York University<br/>University of Oxford<br/>VGG发明者大牛蔡司曼指导的工作，一作是和he kaiming 搞出SlowFast Networks的人。<br/>摘要<br/>深度模型在计算机视觉的每个领域都有部署，因此，理解这些深度模型得到的representation到底是怎么工作的，以及这些representation到底抓去了什么信息就变得越来越重要。接着说本文的工作，本文通过可视化two-stream模型在进行动作识别任务的时候学到了什么来探索这个问题。得到的观察结果主要有以下几点，首先，cross-stream fusion使得学习的过程能够真正的学习到spatiotemporal feature，而不是仅仅分开两支，一支只获得appearance feature，一支只获得motion feature；第二点是网络得到的local representation可以非常专一，非常针对性的表示某一class的特征（class specific），也可能会更加一般一些，能够包含多个类的特征，我觉得这个意思就是，有一些local representation可以直接指明这个特征是属于某一个class的，有些可能就是缩小了范围，指明这个特征可能对应着某几个class；第三点观察结果是，通过网络结构的层级结构，feature变得越来越抽象，并且展示出越来越高的稳定性，对于数据中一些无关紧要的变化（例如不同速度的motion pattern）有着越来越高的invariance；第四点是这种可视化手段不仅能对学到的representation使用，还能对training data使用，揭示出数据的独特性，可以用来解释为什么有些时候模型不能正确的进行预测。<br/>本文采取的是activation maximization的方式进行可视化，这个activation指的是某一个hidden layer的激活值，目的在于寻找恰当的输入，可以使得某一个感兴趣的激活值最大化。本文采取的方式示意图如下<br/>这篇CVPR2018的文章主要解释了在动作识别中学习到的特征，得到了一些重要结论:<br/>1.流融合可以真的学习到可区分的Spatitemporal特征<br/>2.网络可以学习得到高区分度的表示信息<br/>3.通过分层机制，特征变得更加抽象并且增加了对于方向的无偏性<br/>4.可视化可以用来查看分类出错的原因<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-59ac56d308473d0240e73636dc033606_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"242\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic3.zhimg.com/v2-59ac56d308473d0240e73636dc033606_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;640&#39; height=&#39;242&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"242\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https://pic3.zhimg.com/v2-59ac56d308473d0240e73636dc033606_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-59ac56d308473d0240e73636dc033606_b.jpg\"/></figure><p><br/>此外，在看这篇文章的时候，看到了一个专门通过可视化来对时空信息进行研究的网站：<br/><u><a href=\"https://link.zhihu.com/?target=http%3A//cs231n.github.io/understanding-cnn/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">cs231n.github.io/unders</span><span class=\"invisible\">tanding-cnn/</span><span class=\"ellipsis\"></span></a></u><br/>做的不错，对时空信息做出了思考。<br/>这篇文章中，把可视化常见的研究方法分为了三类。<br/>1.Visualization for given inputs<br/>2.Activation maximization<br/>3.Generative Adversarial Networks(GANs)<br/>Visualization for given inputs<br/>1.选用大数据集，得到最大化感兴趣区域的数据集，用来做可视化。类似于前面介绍的Retrieving images that maximally activate a neuron<br/>2.使用BP高亮隐藏单元中关键位置<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-d42270ada944b8a8ae027f38415f861f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"596\" data-rawheight=\"441\" class=\"origin_image zh-lightbox-thumb\" width=\"596\" data-original=\"https://pic4.zhimg.com/v2-d42270ada944b8a8ae027f38415f861f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;596&#39; height=&#39;441&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"596\" data-rawheight=\"441\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"596\" data-original=\"https://pic4.zhimg.com/v2-d42270ada944b8a8ae027f38415f861f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-d42270ada944b8a8ae027f38415f861f_b.jpg\"/></figure><p><br/>GANS<br/>通过生成对抗网络可视化输出<br/>这篇文章比较大的一个创新在于第一次在行为识别中引入了可视化，之前没有其它人做过。<br/>此外提出了一个Activation maximization 和两个正则化方法。比较大的贡献就是证明了时空融合的特征确实学习到了可区分的特征。<br/>文中揭示了几个比较容易混淆的例子，比如playingViolin和playingCello。使得产生分类混淆的时候可以通过可视化学习到的特征来理解为何会识别错误。<br/><b><u><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_cvpr_2018/html/Zhu_Towards_Universal_Representation_CVPR_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">TOWARDS UNIVERSAL REPRESENTATION FOR UNSEEN ACTION RECOGNITION</a></u></b><br/>Yi Zhu1, Yang Long∗2, Yu Guan2, Shawn Newsam1, and Ling Shao3<br/>1 University of California, Merced<br/>2 Open Lab, School of Computing, Newcastle University, UK.<br/>3 Inception Institute of Artificial Intelligence (IIAI), Abu Dhabi, UAE<br/>shao ling 大牛指导的工作。<br/>未知行动识别（UAR）的目的是在没有训练样例的情况下识别新的行动类别。 虽然以前的方法侧重于内部数据集可见/未知的集，但本文提出了一种使用大规模训练源来实现通用表示（UR）的管道，该通用表示可以推广到更真实的跨数据集UAR（CDUAR）场景。 我们首先将UAR作为广义多实例学习（GMIL）问题来解决，并使用分布内核从大型ActivityNet数据集中发现“构建块”。 基本的视觉和语义组件保留在共享空间中，以实现可以有效推广到新数据集的UR。 通过简单的语义自适应可以改善预测的UR样本，然后在测试期间可以使用UR直接识别未知动作。 在没有进一步培训的情况下，广泛的实验显示出对UCF101和HMDB51基准的显着改进。<br/>一句话来说就是，把所有动作集中的动作看成是multiple instance来学习。<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-c7b2dfb1e8c7a8e84a0074c69b6a7461_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"300\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic2.zhimg.com/v2-c7b2dfb1e8c7a8e84a0074c69b6a7461_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;640&#39; height=&#39;300&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"300\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https://pic2.zhimg.com/v2-c7b2dfb1e8c7a8e84a0074c69b6a7461_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-c7b2dfb1e8c7a8e84a0074c69b6a7461_b.jpg\"/></figure><p>数学表达<br/>用已知数据集训练， 推断的时候，再把未知的动作联系到训练的动作集上。<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ba1d9c1ce1e50075d8718d144e08b21b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"491\" data-rawheight=\"458\" class=\"origin_image zh-lightbox-thumb\" width=\"491\" data-original=\"https://pic4.zhimg.com/v2-ba1d9c1ce1e50075d8718d144e08b21b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;491&#39; height=&#39;458&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"491\" data-rawheight=\"458\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"491\" data-original=\"https://pic4.zhimg.com/v2-ba1d9c1ce1e50075d8718d144e08b21b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-ba1d9c1ce1e50075d8718d144e08b21b_b.jpg\"/></figure><p>再用K mean来做聚类，每个类有H个bag。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-e4d1597a20f44c81b66dd56d38bd8a0d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"503\" data-rawheight=\"296\" class=\"origin_image zh-lightbox-thumb\" width=\"503\" data-original=\"https://pic2.zhimg.com/v2-e4d1597a20f44c81b66dd56d38bd8a0d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;503&#39; height=&#39;296&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"503\" data-rawheight=\"296\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"503\" data-original=\"https://pic2.zhimg.com/v2-e4d1597a20f44c81b66dd56d38bd8a0d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-e4d1597a20f44c81b66dd56d38bd8a0d_b.jpg\"/></figure><p>再用一种全局统一的表征来表达</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-0ce8e10b5f818928131043f6a839de8f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"499\" data-rawheight=\"796\" class=\"origin_image zh-lightbox-thumb\" width=\"499\" data-original=\"https://pic4.zhimg.com/v2-0ce8e10b5f818928131043f6a839de8f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;499&#39; height=&#39;796&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"499\" data-rawheight=\"796\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"499\" data-original=\"https://pic4.zhimg.com/v2-0ce8e10b5f818928131043f6a839de8f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-0ce8e10b5f818928131043f6a839de8f_b.jpg\"/></figure><p>再加上一些post processing，比如优化，正交化等等。<br/>最后的结果<br/>和 state of the art 的比较</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-a607f039f271d35b4c7b5a8e40fcfff4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"496\" data-rawheight=\"374\" class=\"origin_image zh-lightbox-thumb\" width=\"496\" data-original=\"https://pic1.zhimg.com/v2-a607f039f271d35b4c7b5a8e40fcfff4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;496&#39; height=&#39;374&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"496\" data-rawheight=\"374\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"496\" data-original=\"https://pic1.zhimg.com/v2-a607f039f271d35b4c7b5a8e40fcfff4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-a607f039f271d35b4c7b5a8e40fcfff4_b.jpg\"/></figure><p>跨库的深入分析</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ae33f3b7dac3168618ff63f36e1c3b48_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"200\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic1.zhimg.com/v2-ae33f3b7dac3168618ff63f36e1c3b48_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;640&#39; height=&#39;200&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"200\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https://pic1.zhimg.com/v2-ae33f3b7dac3168618ff63f36e1c3b48_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-ae33f3b7dac3168618ff63f36e1c3b48_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p>总结：<br/>这次解读的几篇文章，一个是通过压缩文件来提取有效运动信息，有两个是研究时序流的融合，进行深入探讨，一个是提出了一个新的特征表示PoTion，一个是对于新颖动作的识别的探索，一些感想：<br/></p><ol><li>时序流的拓荒时代已经过去，现在已经开始进入对各种时序信息，各种流之间关系的细致探索，比如他们之间的融合，以及作用原理。</li><li>找新的方向，比如跨库来实现动作识别的，shao ling算是带头的了。</li></ol><p></p>", 
            "topic": [
                {
                    "tag": "计算机视觉", 
                    "tagLink": "https://api.zhihu.com/topics/19590195"
                }, 
                {
                    "tag": "图像识别", 
                    "tagLink": "https://api.zhihu.com/topics/19588774"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/54211238", 
            "userName": "奥卢的陈皓宇", 
            "userLink": "https://www.zhihu.com/people/6e25ca19a73ea5a2d81f195cf4c1851f", 
            "upvote": 3, 
            "title": "Chalearn 2014 手势识别比赛的技术总结", 
            "content": "<p>这篇文章主要介绍几个在Chalearn2014数据库上排名比较靠前的几个方法。</p><p>这个比赛的链接在此：<a href=\"https://link.zhihu.com/?target=http%3A//gesture.chalearn.org/2014-looking-at-people-challenge\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">比赛链接</a></p><p class=\"ztext-empty-paragraph\"><br/></p><p>这个比赛有三个Track， 分别是</p><p>Track 1:Human Pose Recovery， </p><p>Track 2: Action/Interaction Recognition，以及</p><p>Track 3: Gesture Recognition</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在这里我们要介绍的是Track 3: Gesture Recognition的一些方法。</p><p>由于个人主要是做skeleton的，所以在这里只着重分析了skeleton的一些方法。</p><p>排名在此：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-995a72d63998a07999c879701386dacf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"784\" data-rawheight=\"470\" class=\"origin_image zh-lightbox-thumb\" width=\"784\" data-original=\"https://pic4.zhimg.com/v2-995a72d63998a07999c879701386dacf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;784&#39; height=&#39;470&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"784\" data-rawheight=\"470\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"784\" data-original=\"https://pic4.zhimg.com/v2-995a72d63998a07999c879701386dacf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-995a72d63998a07999c879701386dacf_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>方法介绍</b></p><p><b>1. Moddrop[1]：</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ad7b6291e07289d45cd5b51d88d0390f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"773\" data-rawheight=\"324\" class=\"origin_image zh-lightbox-thumb\" width=\"773\" data-original=\"https://pic4.zhimg.com/v2-ad7b6291e07289d45cd5b51d88d0390f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;773&#39; height=&#39;324&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"773\" data-rawheight=\"324\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"773\" data-original=\"https://pic4.zhimg.com/v2-ad7b6291e07289d45cd5b51d88d0390f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-ad7b6291e07289d45cd5b51d88d0390f_b.jpg\"/></figure><p>这篇工作的主要框架是，通过三个（也可以更多）不同步长的时域上的感受野，来采集多模态的特征（RGB，skeleton，audio等等），其创新点是在融合多个模态特征的时候，使用了不同的权重去融合（现在看来已经不能算是创新），并且使用了dropout。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>个人通过实验，发现这篇文章其主要的能够提升性能的几个点：</p><p>不同步长的时序上的感受野能够增加识别性能的提升。类似的作用在寿政大神CVPR2016的工作<b>Temporal action localization in untrimmed videos via multi-stage cnns</b>上[2]以及朱佳刚大哥ICPR2018的工作<b>End-to-end Video-level Representation Learning for Action Recognition</b>上[3]都有所体现。都是用时域上的不同尺度来更好地表达长段时序信息。</p><p>特征提取上，其使用的Moving pose[4] 的skeleton descriptor，只用了183维，比eigenjoint[5] 的891维要少的多。而且Moving pose性能的更好，识别精度更高，单单使用去全连接网络，moving pose 的特征能到0.8002，而eigenjoint只能到0.779。 然而这也不是绝对的，这会在排名第6的方法中介绍。</p><p>最后，也是提升其性能最大的一个就是，作者单独加入了一个专门用来检测有没有动作的detector。这个能带来多达0.04的性能提升。</p><p>还有就是，在最后的决策融合阶段，他们使用了一个extremely random forest，能够带来0.008的提升。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>2. Boosted Detector</b>[6] <b>：</b></p><p>Monnier et al.等人提出的排名第二的方法和第一名几乎一样，只是classifier不是神经网络，而是1000个 depth-2 的决策树。</p><p>特征也都大同小异，底下是结果：</p><p>Normalized skeleton joints and velocities (SK); </p><p>Joint angles and angular velocities (JA); </p><p>Joint-pair distances and velocities (JP); </p><p>Hand HOG descriptors (HH).</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-4adba70917eb577defc003349ab5ef08_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"171\" data-rawheight=\"109\" class=\"content_image\" width=\"171\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;171&#39; height=&#39;109&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"171\" data-rawheight=\"109\" class=\"content_image lazy\" width=\"171\" data-actualsrc=\"https://pic1.zhimg.com/v2-4adba70917eb577defc003349ab5ef08_b.png\"/></figure><p>这和我的实验结果基本一致，没有用到RGB提供的HOG的话，那么纯的分类器（不加后处理）只能做到0.80上下一点(0.791)。</p><p>值得一提的是，他也用到了多步长来提供更充分的时序信息。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>3. Nonparametric Gesture Labeling</b>[7] <b>：</b></p><p>Ju Yong Chang 提出的排名第三的方法是一个基于高斯kernel 和最近邻的classifier，这个韩国老兄提出来的最大贡献是在后处理的时候，加入了CRF条件随机场来融合各个模态。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-1c333cd69c72538ffa084f91baf813af_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"496\" data-rawheight=\"104\" class=\"origin_image zh-lightbox-thumb\" width=\"496\" data-original=\"https://pic4.zhimg.com/v2-1c333cd69c72538ffa084f91baf813af_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;496&#39; height=&#39;104&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"496\" data-rawheight=\"104\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"496\" data-original=\"https://pic4.zhimg.com/v2-1c333cd69c72538ffa084f91baf813af_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-1c333cd69c72538ffa084f91baf813af_b.jpg\"/></figure><p>特征也都大同小异，底下是结果：</p><p>P, D, L, R, and A denote the joint position, joint distance, left hand, right hand, and active hand based features,</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ebe01807dbd5bd3effb1853180905190_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"233\" data-rawheight=\"127\" class=\"content_image\" width=\"233\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;233&#39; height=&#39;127&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"233\" data-rawheight=\"127\" class=\"content_image lazy\" width=\"233\" data-actualsrc=\"https://pic1.zhimg.com/v2-ebe01807dbd5bd3effb1853180905190_b.jpg\"/></figure><p>这实验结果也是和上述基本一致，没有用到RGB的话，只能做到0.80上下一点（0.7948）,CRF 带来了0.03-0.04的提升。</p><p>active hand的trick能带来0.013的提升。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>4. Super Vector Representation</b>[8] <b>：</b></p><p>接下来就是我们的wang limin 和peng xiaojiang大佬提出的方法，他们的处理分为两个部分，分割整个sequence为几个clips，然后再对每个clips做dense 。</p><p>分割的时候，他们用的是一个非常简单的方法：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-a9284bfb0475e9c6404aff95d61b7ee4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"771\" data-rawheight=\"386\" class=\"origin_image zh-lightbox-thumb\" width=\"771\" data-original=\"https://pic1.zhimg.com/v2-a9284bfb0475e9c6404aff95d61b7ee4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;771&#39; height=&#39;386&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"771\" data-rawheight=\"386\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"771\" data-original=\"https://pic1.zhimg.com/v2-a9284bfb0475e9c6404aff95d61b7ee4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-a9284bfb0475e9c6404aff95d61b7ee4_b.jpg\"/></figure><p>利用给定的姿势信息，他们使用与像素网格重叠的100×100个单元的2D直方图来估计整个视频中手部位置的空间分布。 然后我们选择具有最高频率的单元作为静态指针位置。 静态指针位置，对于每个帧，计算当前指针位置到静态位置的距离。 最后，根据该距离，使用单个阈值τ来确定是否正在执行手势。</p><p>我在写硕士论文的时候也想到了这个方法，当时觉得很棒，现在想想确实太naive了。</p><p>在分类阶段，选择IDT作为RGB的low level descriptor。 将轨迹长度设置为9并提取四种描述符，即HOG，HOF，MBHx和MBHy。</p><p>再用Fisher vector 加上GMM 的方式来表达，最后用SVM来分类。</p><p>为了避免不相关的动作的错误检测，他们设计了个后处理。 首先，在动作和手势识别的训练阶段，他们专门找了一些静态背景或嘈杂运动。 然后使用这些实例来训练表示背景类的分类器。  在评估中，发现此后处理步骤对于消除那些误报检测并提高动作和手势定位的性能非常有效。</p><p><br/><b>5. CNN</b>[9] <b>：</b></p><p>接下来Lionel Pigou等人的方法，就很直接了，用CNN对给定的那些手势训练，然后使用滑动窗口技术，间隔为32。 具有相同类别和足够高的分类概率（阈值）的连续间隔被认为是最后预测的手势。 此外，一个额外的类，以帮助识别没有手势的视频间隔。</p><p>现在这种技术基本已经被3DCNN取代了。</p><p>由于没有做skeleton的方法，所以就不多介绍了。</p><p><br/><b>6. DBN+HMM</b>[9] <b>：</b></p><p>最后是wu di的方法，他是先用深度信念网络Deep Belief Network来训练骨骼模型，得到对应的pose，然后用HMM 来排列这些Pose，得到最后的结果。</p><p>由于需要做时序的分割，所以加入了新的类别，间隔的Pose，用来给HMM 转换。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-87498a8ad00a174cf49ad2332e932e8a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"339\" data-rawheight=\"191\" class=\"content_image\" width=\"339\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;339&#39; height=&#39;191&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"339\" data-rawheight=\"191\" class=\"content_image lazy\" width=\"339\" data-actualsrc=\"https://pic3.zhimg.com/v2-87498a8ad00a174cf49ad2332e932e8a_b.jpg\"/></figure><p>我在实验的时候，发现他用的骨骼特征eigenjoint 加上DBN会得到最好的结果，在HMM 排序后大概在0.78，如果DBN 加上Moving Pose 反而结果会下降，排序后在0.76左右。</p><p>所以特征和网络是相辅相成的，不能一概而论哪个绝对好或者绝对不好。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>技术总结：</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-0c77b90c6409040828613cf06a2be087_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"823\" data-rawheight=\"386\" class=\"origin_image zh-lightbox-thumb\" width=\"823\" data-original=\"https://pic4.zhimg.com/v2-0c77b90c6409040828613cf06a2be087_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;823&#39; height=&#39;386&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"823\" data-rawheight=\"386\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"823\" data-original=\"https://pic4.zhimg.com/v2-0c77b90c6409040828613cf06a2be087_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-0c77b90c6409040828613cf06a2be087_b.jpg\"/></figure><p>总体来看，好的性能大部分取决于feature的选择。</p><p>其次就是分割好坏。</p><p>事实上，以上所有方法都能达到很好的识别精度了，（94%往上）</p><p class=\"ztext-empty-paragraph\"><br/></p><p>[1] Neverova N, Wolf C, Taylor G, et al. Moddrop: adaptive multi-modal gesture recognition[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2016, 38(8): 1692-1706.</p><p>[2] Shou Z, Wang D, Chang S F. Temporal action localization in untrimmed videos via multi-stage cnns[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016: 1049-1058.</p><p>[3] Jiagang Zhu, Wei Z, Zheng Z. End-to-end Video-level Representation Learning for Action Recognition//ICPR. 2018. </p><p>[4] Mihai Zanfir, Marius L, Cristian S. The Moving Pose: An Efficient 3D Kinematics Descriptor for Low-Latency Action Recognition and Detection //ICCV. 2013. </p><p>[5] <a href=\"https://link.zhihu.com/?target=https%3A//ieeexplore.ieee.org/author/37534296100\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Xiaodong Yang</a>. EigenJoints-based action recognition using Naïve-Bayes-Nearest-Neighbor //CVPRW. 2013. </p><p>[6] Monnier C, German S, Ost A. A multi-scale boosted detector for efficient and robust gesture recognition[C]//Workshop at the European Conference on Computer Vision. Springer, Cham, 2014: 491-502.</p><p>[7] J. Y. Chang, “Nonparametric gesture labeling from multi-modal data,” in Proc. Eur. Conf. Comput. Vision Pattern Recog. Workshops, 2014, pp. 503–517</p><p>[8]L. Pigou, S. Dieleman, P.-J. Kindermans, and B. Schrauwen, “Sign language recognition using convolutional neural networks,” in Proc. Eur. Conf. Comput. Vis. Pattern Recog. Workshops, 2014, pp. 1–6.</p><p>[9]D. Wu, L. Pigou, P.-J. Kindermans, N. Le, L. Shao, J. Dambre, and J.-M. Odobez. Deep dynamic neural networks for multimodal gesture segmentation and recognition. IEEE Transactions on Pattern Analysis &amp; Machine Intelligence, 38(8):1583–1597, 2016.</p><p></p>", 
            "topic": [
                {
                    "tag": "计算机视觉", 
                    "tagLink": "https://api.zhihu.com/topics/19590195"
                }, 
                {
                    "tag": "手势识别", 
                    "tagLink": "https://api.zhihu.com/topics/19801921"
                }
            ], 
            "comments": [
                {
                    "userName": "Fisher Yu余梓彤", 
                    "userLink": "https://www.zhihu.com/people/60d41039d82337a73fa807d3494319c9", 
                    "content": "19年有没有新比赛搞搞？", 
                    "likes": 1, 
                    "childComments": [
                        {
                            "userName": "chen haoyu", 
                            "userLink": "https://www.zhihu.com/people/4cdb135a66d6b9c275d1c265cef14d5b", 
                            "content": "想搞啊，组起队来呀", 
                            "likes": 0, 
                            "replyToAuthor": "Fisher Yu余梓彤"
                        }, 
                        {
                            "userName": "Fisher Yu余梓彤", 
                            "userLink": "https://www.zhihu.com/people/60d41039d82337a73fa807d3494319c9", 
                            "content": "<p>忙完三月份，叫上彭大师一起搞起</p>", 
                            "likes": 0, 
                            "replyToAuthor": "chen haoyu"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/56061717", 
            "userName": "奥卢的陈皓宇", 
            "userLink": "https://www.zhihu.com/people/6e25ca19a73ea5a2d81f195cf4c1851f", 
            "upvote": 94, 
            "title": "2018年 Action recognition 的汇总（ECCV CVPR AAAI）", 
            "content": "<p>把ECCV和CVPR还有AAAI 2018年的action recognition汇了个总，放在这里。</p><hr/><p>ECCV 2018</p><p><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_ECCV_2018/html/Chenyang_Si_Skeleton-Based_Action_Recognition_ECCV_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Skeleton-Based Action Recognition with Spatial Reasoning and Temporal Stack Learning</a></p><p><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_ECCV_2018/html/Dongang_Wang_Dividing_and_Aggregating_ECCV_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Dividing and Aggregating Network for Multi-view Action Recognition</a></p><p><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_ECCV_2018/html/HU_Jian-Fang_Deep_Bilinear_Learning_ECCV_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Deep Bilinear Learning for RGB-D Action Recognition</a></p><p><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_ECCV_2018/html/Nuno_Garcia_Modality_Distillation_with_ECCV_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Modality Distillation with Multiple Stream Networks for Action Recognition</a></p><p><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_ECCV_2018/html/Yang_Du_Interaction-aware_Spatio-temporal_Pyramid_ECCV_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Interaction-aware Spatio-temporal Pyramid Attention Networks for Action Classification</a></p><p><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_ECCV_2018/html/Myunggi_Lee_Motion_Feature_Network_ECCV_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Motion Feature Network: Fixed Motion Filter for Action Recognition</a></p><p><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_ECCV_2018/html/Ali_Diba_Spatio-Temporal_Channel_Correlation_ECCV_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Spatio-Temporal Channel Correlation Networks for Action Classification</a></p><p><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_ECCV_2018/html/Dong_Li_Recurrent_Tubelet_Proposal_ECCV_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Recurrent Tubelet Proposal and Recognition Networks for Action Detection</a></p><p><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_ECCV_2018/html/Lan_Wang_PM-GANs_Discriminative_Representation_ECCV_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">PM-GANs: Discriminative Representation Learning for Action Recognition Using Partial-modalities</a></p><p><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_ECCV_2018/html/Yingwei_Li_RESOUND_Towards_Action_ECCV_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">RESOUND: Towards Action Recognition without Representation Bias</a></p><h2>CVPR 2018</h2><p><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_cvpr_2018/html/Zhou_MiCT_Mixed_3D2D_CVPR_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">MiCT: Mixed 3D/2D Convolutional Tube for Human Action Recognition</a></p><p><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_cvpr_2018/html/Sun_Optical_Flow_Guided_CVPR_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Optical Flow Guided Feature: A Fast and Robust Motion Representation for Video Action Recognition</a></p><p><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_cvpr_2018/html/Luvizon_2D3D_Pose_Estimation_CVPR_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">2D/3D Pose Estimation and Action Recognition Using Multitask Deep Learning</a></p><p><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_cvpr_2018/html/Wang_Temporal_Hallucinating_for_CVPR_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Temporal Hallucinating for Action Recognition With Few Still Images</a></p><p><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_cvpr_2018/html/Tang_Deep_Progressive_Reinforcement_CVPR_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Deep Progressive Reinforcement Learning for Skeleton-Based Action Recognition</a></p><p><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_cvpr_2018/html/Gao_Im2Flow_Motion_Hallucination_CVPR_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Im2Flow: Motion Hallucination From Static Images for Action Recognition</a></p><p><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_cvpr_2018/html/Wu_Compressed_Video_Action_CVPR_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Compressed Video Action Recognition</a></p><p><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_cvpr_2018/html/Tran_A_Closer_Look_CVPR_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">A Closer Look at Spatiotemporal Convolutions for Action Recognition</a></p><p><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_cvpr_2018/html/Lei_Temporal_Deformable_Residual_CVPR_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Temporal Deformable Residual Networks for Action Segmentation in Videos</a></p><p><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_cvpr_2018/html/Choutas_PoTion_Pose_MoTion_CVPR_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">PoTion: Pose MoTion Representation for Action Recognition</a></p><p><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_cvpr_2018/html/Feichtenhofer_What_Have_We_CVPR_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">What Have We Learned From Deep Representations for Action Recognition?</a></p><p><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_cvpr_2018/html/Zhu_Towards_Universal_Representation_CVPR_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Towards Universal Representation for Unseen Action Recognition</a></p><h2>AAAI 2018</h2><p><a href=\"https://link.zhihu.com/?target=http%3A//qrg.northwestern.edu/qr2017/papers/QR2017_paper_1.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Action Recognition from Skeleton Data via Analogical Generalization over Qualitative Representations</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1711.07430\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Action Recognition with Coarse-to-Fine Deep Feature Integration and Asynchronous Fusion</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1801.01080\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Cooperative Training of Deep Aggregation Networks for RGB-D Action Recognitio</a>n</p><p><a href=\"https://link.zhihu.com/?target=https%3A//aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16332\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Hierarchical Nonlinear Orthogonal Adaptive-Subspace Self-Organizing Map based Feature Extraction for Human Action Recognition</a> </p><p>中文的报道：<a href=\"https://link.zhihu.com/?target=http%3A//www.sohu.com/a/221351172_756411\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【重磅】自适应无监督学习的特征提取方法</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1801.07455\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Spatial Temporal Graph Convolutional Networks for Skeleton-Based Action Recognition</a></p><p>中文的报道：<a href=\"https://link.zhihu.com/?target=https%3A//www.leiphone.com/news/201802/sSRbW4I2M4HheARj.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">港中文AAAI录用论文详解：ST-GCN 时空图卷积网络模型 | AAAI 2018</a></p><p>code: <a href=\"https://link.zhihu.com/?target=https%3A//github.com/yysijie/st-gcn\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/yysijie/st-g</span><span class=\"invisible\">cn</span><span class=\"ellipsis\"></span></a></p><p>亲测，非常好用</p><p><a href=\"https://link.zhihu.com/?target=https%3A//aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17205\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">T-C3D: Temporal Convolutional 3D Network for Real-time Action Recognition</a></p><p>code:<a href=\"https://link.zhihu.com/?target=https%3A//github.com/tc3d/tc3d\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">tc3d/tc3d</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16794\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Unsupervised Deep Learning of Mid-Level Video Representation for Action Recognition</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//www.semanticscholar.org/paper/Unsupervised-Representation-Learning-With-Long-Term-Zheng-Wen/648fe9873634d4ec07eaf130c067ffe628ed3d2b\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Unsupervised Representation Learning with Long-Term Dynamics for Skeleton Based Action Recognition</a></p><h2><b><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_ECCV_2018/papers/Junwu_Weng_Deformable_Pose_Traversal_ECCV_2018_paper.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Deformable Pose Traversal Convolution for 3D Action and Gesture Recognition</a></b></h2><p>Junwu Weng, Mengyuan Liu, Xudong Jiang, and Junsong Yuan</p><p>School of EEE, Nanyang Technological University</p><p>Department of CSE, The State University of New York, Buffalo jsyuan@buffalo.edu</p><p>这又是一个NTU 的作品。在NTU的库上做的东西。</p><p>本文的贡献是3D姿势的表征。他们提出了一种可变形姿势遍历卷积网络 Deformable Pose Traversal Convolution，它采用一维卷积来遍历3D姿势以表示其姿态，而不是直接通过其关节位置来表示3D姿势。 在执行遍历卷积时感受野也不是固定的，而是通过考虑具有不同权重的上下文关节来优化每个关节的卷积核。 这种可变形卷积更好地利用上下文关节进行动作和手势识别，并且对于嘈杂的关节更加鲁棒。 此外，通过将学习的姿势特征馈送到LSTM，我们执行端对端训练，其共同优化姿势表征和时间序列识别。</p><p>测试了三个基准数据集的实验验证了我们提出的方法的竞争性能，以及它处理姿态噪声关节的效率和鲁棒性。</p><p>这篇工作是基于NTU之前的一个基本的LSTM的工作：</p><p><a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1607.07043.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Liu, J., Shahroudy, A., Xu, D., Wang, G.: Spatio-temporal lstm with trust gates for 3d human action recognition. In: ECCV, Springer (2016) 816–833 </a></p><p>code（lua）：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/kinect59/Spatio-Temporal-LSTM\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/kinect59/Spa</span><span class=\"invisible\">tio-Temporal-LSTM</span><span class=\"ellipsis\"></span></a></p><p>来做的。</p><p>他们也是用了一个树遍历的方法来表示相关节点的空间临近关系。然后用一个一维的卷积去卷。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-efe83f56913cf2ef51c9cb2e1c385fe7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"460\" data-rawheight=\"147\" class=\"origin_image zh-lightbox-thumb\" width=\"460\" data-original=\"https://pic4.zhimg.com/v2-efe83f56913cf2ef51c9cb2e1c385fe7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;460&#39; height=&#39;147&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"460\" data-rawheight=\"147\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"460\" data-original=\"https://pic4.zhimg.com/v2-efe83f56913cf2ef51c9cb2e1c385fe7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-efe83f56913cf2ef51c9cb2e1c385fe7_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>基本思想就是卷积不是固定的，而是可以 deformable 的。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-b943b9714919b0c7890e6676ee73ba4b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"430\" data-rawheight=\"282\" class=\"origin_image zh-lightbox-thumb\" width=\"430\" data-original=\"https://pic4.zhimg.com/v2-b943b9714919b0c7890e6676ee73ba4b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;430&#39; height=&#39;282&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"430\" data-rawheight=\"282\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"430\" data-original=\"https://pic4.zhimg.com/v2-b943b9714919b0c7890e6676ee73ba4b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-b943b9714919b0c7890e6676ee73ba4b_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>卷积的形变是通过一个offset的向量来学习的。</p><p>贡献可归纳如下：<br/>•引入了一维卷积神经网络，Deformable Pose Traversal Convolution，来表示3D姿势。 它可以通过识别关节的关键组合来提取姿势特征。<br/>•应用ConvLSTM 来学习卷积的变形偏移。 它模拟关节关键组合的时间动态。</p><p>convLSTM在此：</p><p>Xingjian, S., Chen, Z., Wang, H., Yeung, D.Y., Wong, W.K., Woo, W.c.: Convolutional lstm network: A machine learning approach for precipitation nowcasting. In: NIPS. (2015) 802–810</p><p>code（Theano）：<a href=\"https://link.zhihu.com/?target=https%3A//home.cse.ust.hk/~xshiab/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">home.cse.ust.hk/~xshiab</span><span class=\"invisible\">/</span><span class=\"ellipsis\"></span></a></p><p>Dynamic Hand Gesture 14/28 dataset (DHG)</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-1f1b1dff4c99e532560626f90d7f7f4d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"393\" data-rawheight=\"132\" class=\"content_image\" width=\"393\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;393&#39; height=&#39;132&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"393\" data-rawheight=\"132\" class=\"content_image lazy\" width=\"393\" data-actualsrc=\"https://pic2.zhimg.com/v2-1f1b1dff4c99e532560626f90d7f7f4d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>the NTU-RGB+D dataset (NTU)</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-347a7946b9563ae7b6f27d9fc602091f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"222\" data-rawheight=\"187\" class=\"content_image\" width=\"222\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;222&#39; height=&#39;187&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"222\" data-rawheight=\"187\" class=\"content_image lazy\" width=\"222\" data-actualsrc=\"https://pic4.zhimg.com/v2-347a7946b9563ae7b6f27d9fc602091f_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>the Berkeley Multi-modal Human Action dataset (MHAD)</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-15d189ad8a98ff192e0d3f47e7b4e974_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"240\" data-rawheight=\"181\" class=\"content_image\" width=\"240\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;240&#39; height=&#39;181&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"240\" data-rawheight=\"181\" class=\"content_image lazy\" width=\"240\" data-actualsrc=\"https://pic1.zhimg.com/v2-15d189ad8a98ff192e0d3f47e7b4e974_b.jpg\"/></figure><h2><b><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_ECCV_2018/papers/HU_Jian-Fang_Deep_Bilinear_Learning_ECCV_2018_paper.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Deep Bilinear Learning for RGB-D Action Recognition</a></b></h2><p><i><b>Jian-Fang Hu</b></i>, Wei-Shi Zheng, Jiahui Pan, Jianhuang Lai, and Jianguo Zhang</p><p>Sun Yat-sen University, China</p><p>University of Dundee, United Kingdom</p><p>Key Laboratory of Machine Intelligence and Advanced Computing, MOE 4 Inception Institute of Artificial Intelligence, United Arab Emirates</p><p>中山大学 <b>Jian-Fang Hu </b>的工作。</p><p>简述</p><p>这篇文章提出了一个RGB-D动作识别的多模态和时序信息交互的学习方法。 为了共同学习时变信息和多模态特征，他们提出了一种新颖的深层双线性学习框架。</p><p>在该框架中，我们提出了由两个线性池化层 two linear pooling layers组成的双线性模块 bilinear blocks，用于分别从模态和时间方向池化输入的特征。</p><p>为了捕获丰富的模态 - 时间信息并促进深层双线性学习，他们给网络的特征进行了创新，提出了一种称为模态 - 时间立方体的新动作特征，用于从全面的角度表征RGB-D动作。</p><p>具体</p><p>骨骼的特征</p><p>把 一个动作分成D 个segment， 每个segment的开始d个小segment组成一组Action history sequence 共有D个AHS</p><p>再用RNN 来提skeleton特征，用了这个工作，也是这个人的</p><p>Hu, J.F., Zheng, W.S., Lai, J., Zhang, J.: Jointly learning heterogeneous features for rgb-d activity recognition. IEEE transactions on pattern analysis and machine intelligence</p><p>的描述子</p><p>code: <a href=\"https://link.zhihu.com/?target=http%3A//isee.sysu.edu.cn/~hujianfang/ProjectJOULE.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">isee.sysu.edu.cn/~hujia</span><span class=\"invisible\">nfang/ProjectJOULE.html</span><span class=\"ellipsis\"></span></a></p><p>RGB 的特征</p><p>GIST frames ， 用基于骨骼节点附近的RGB patch来平铺成一个新的长段轨迹，所以这篇工作也属于基于轨迹的动作识别家族的文章。</p><p>基于轨迹的动作识别的汇总：Wang, H., Klaser, A., Schmid, C., Liu, C.L.: Dense trajectories and motion boundary descriptors for action recognition. International Journal of Computer Vision</p><p>再针对这个训练了两个，K-channel 的CNN descriptors，K 是depth 貌似是16.</p><p>为了训练K channel 的CNN， 选了K 个frame的 GIST ，</p><p>选择的方法是 max(1, 1 + (u − 1)ls/ K + δ)，就是在后面加了一个随机扰动。</p><p>用了两组depth， K = 1 是为了学习静态的外观，K = 16 学习动态的外观。</p><p>最后得到的特征</p><p>包含五个具有时序信息的特征cube，其中两个来自RGB AHS（1通道CNN和16通道CNN），两个来自深度AHS（（1通道CNN和16通道CNN），以及一个来自骨架AHS（RNN），其中每一个都表征了特定模态下不同AHS长度的动作。它们的组合可以形成一个综合的动作表示。</p><p>然后，到此为止，特征已经做完，重点来了。</p><p>先开始我还没看懂他说的第三维就是类别什么意思，后来发现，他融合的不是特征，而是上一层神经网络给出的probability。</p><p>所以第三维是类别的数量。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-0d49b434ab1d4b30e758216b10ec9889_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"933\" data-rawheight=\"339\" class=\"origin_image zh-lightbox-thumb\" width=\"933\" data-original=\"https://pic2.zhimg.com/v2-0d49b434ab1d4b30e758216b10ec9889_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;933&#39; height=&#39;339&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"933\" data-rawheight=\"339\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"933\" data-original=\"https://pic2.zhimg.com/v2-0d49b434ab1d4b30e758216b10ec9889_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-0d49b434ab1d4b30e758216b10ec9889_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>深度双线性学习</p><p>由于不同模块直接融合不好，如上图所示，他们提出了新的融合方法。</p><p>在数学中，双线性映射是组合两个向量空间的元素以产生第三向量空间的元素的函数。</p><p>深双线性结构。给定一组M×T×C大小的模态 - 时间立方体，我们的目标是学习底层映射f，其将所有立方体元素合并为鲁棒的表征。换句话说，目标是找到一个映射，将输入多维数据集的模态维度和时间维度汇总到一维。在本文中，我们将映射f定义为双线性块，Relu和softmax运算符的堆栈，即f =g1◦g2◦... gn ...（•），其中gn表示到上述操作之一或双线性区块。</p><p><br/>深层双线性架构的形式是灵活的。本文中的实验涉及具有三个双线性块，三个Relu层和softmax层的深层架构，而更多层也是可以的。在该体系结构中，每个双线性块后面都有一个Relu层，以非线性方式映射块的输出。图5中可以找到所采用的深层体系结构的图示。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-6f2c7f54bff178a40b970dfb4cd4168d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"853\" data-rawheight=\"388\" class=\"origin_image zh-lightbox-thumb\" width=\"853\" data-original=\"https://pic2.zhimg.com/v2-6f2c7f54bff178a40b970dfb4cd4168d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;853&#39; height=&#39;388&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"853\" data-rawheight=\"388\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"853\" data-original=\"https://pic2.zhimg.com/v2-6f2c7f54bff178a40b970dfb4cd4168d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-6f2c7f54bff178a40b970dfb4cd4168d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>结果</p><p>NTU</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-37512e8c4bc0dfbba68e853b353f7e86_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"479\" data-rawheight=\"229\" class=\"origin_image zh-lightbox-thumb\" width=\"479\" data-original=\"https://pic3.zhimg.com/v2-37512e8c4bc0dfbba68e853b353f7e86_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;479&#39; height=&#39;229&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"479\" data-rawheight=\"229\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"479\" data-original=\"https://pic3.zhimg.com/v2-37512e8c4bc0dfbba68e853b353f7e86_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-37512e8c4bc0dfbba68e853b353f7e86_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>SYSU 3D HOI set 他们自己的库</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-37cbebb81805013f29f6d50a90f92f42_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"434\" data-rawheight=\"214\" class=\"origin_image zh-lightbox-thumb\" width=\"434\" data-original=\"https://pic3.zhimg.com/v2-37cbebb81805013f29f6d50a90f92f42_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;434&#39; height=&#39;214&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"434\" data-rawheight=\"214\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"434\" data-original=\"https://pic3.zhimg.com/v2-37cbebb81805013f29f6d50a90f92f42_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-37cbebb81805013f29f6d50a90f92f42_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>AHS的作用</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-49391b4e1708c056a43f2e054f3b73ad_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"503\" data-rawheight=\"239\" class=\"origin_image zh-lightbox-thumb\" width=\"503\" data-original=\"https://pic2.zhimg.com/v2-49391b4e1708c056a43f2e054f3b73ad_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;503&#39; height=&#39;239&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"503\" data-rawheight=\"239\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"503\" data-original=\"https://pic2.zhimg.com/v2-49391b4e1708c056a43f2e054f3b73ad_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-49391b4e1708c056a43f2e054f3b73ad_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>跟其他的融合方法相比</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-fe4b4546ba72dc2898e8d19eca280b30_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"480\" data-rawheight=\"69\" class=\"origin_image zh-lightbox-thumb\" width=\"480\" data-original=\"https://pic1.zhimg.com/v2-fe4b4546ba72dc2898e8d19eca280b30_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;480&#39; height=&#39;69&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"480\" data-rawheight=\"69\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"480\" data-original=\"https://pic1.zhimg.com/v2-fe4b4546ba72dc2898e8d19eca280b30_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-fe4b4546ba72dc2898e8d19eca280b30_b.png\"/></figure><h2><b><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_ECCV_2018/papers/Chenyang_Si_Skeleton-Based_Action_Recognition_ECCV_2018_paper.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Skeleton-Based Action Recognition with Spatial Reasoning and Temporal Stack Learning</a></b></h2><p>Chenyang Si, Ya Jing, Wei Wang, Liang Wang, and Tieniu Tan</p><p>Center for Research on Intelligent Perception and Computing (CRIPAC), National Laboratory of Pattern Recognition (NLPR)</p><p>Center for Excellence in Brain Science and Intelligence Technology (CEBSIT), Institute of Automation, Chinese Academy of Sciences (CASIA)</p><p>University of Chinese Academy of Sciences (UCAS)</p><p>中科院的工作。</p><p>简述</p><p>大多数之前骨架序列的表征缺少空间结构信息和详细的时间动态特征。</p><p>这篇文章提出了一种基于空间推理和时间栈学习 a novel model with spatial reasoning and temporal stack learning（SR-TSL）的基于骨架的动作识别的新模型，它由空间推理网络（SRN）和时间栈学习网络（TSLN）组成。</p><p>SRN可以通过残差图神经网络捕获每个帧内的高级空间结构信息，而TSLN可以通过多个跳过剪辑LSTM的组合来模拟骨架序列的详细时间动态。</p><p>在训练中，提出新的clip-based incremental loss。</p><p>在SYSU 3D数据集和NTU RGB + D数据集验证了。</p><p>抛出的两个问题</p><p>首先，人类行为是协调完成的。例如，走路需要腿走路，也需要摆动手臂以协调身体平衡。如果直接将所有身体关节喂到神经网络，抓住高级特征是很难的是。</p><p>其次，这些方法利用RNN直接模拟骨架序列的整体时间动态。最终RNN的隐藏表示用于识别动作。对于长期序列，最后隐藏的表示不能完全包含序列的详细时间动态。</p><p>方法的描述</p><p>一，提出了一个空间推理网络来捕捉每个帧内的高级空间结构特征。身体可以分解成不同的部分，例如两条胳膊，两条腿和一条树干。每个部分的连接的连接被转换成具有线性层的单独空间特征。身体部位的这些个体空间特征被馈送到残差图神经网络（RGNN）以捕获不同身体部位之间的高级结构特征，其中每个节点对应于身体部位。</p><p>二，在时序上，提出了一个temporal stack learning network（TSLN）去建模序列的detailed temporal dynamics。</p><p>其由三个skip clip 的LSTM组成</p><p>经过空间推理网络后，可以得到了一个特征序列。在时序空间上，这里的时序堆叠学习网络，首先是将长时序列划分成多个连续的短时clip，每个clip通过LSTM进行时序建模，不同clip之间的LSTM是参数共享的。每个短时序clip的最后一个隐含层的状态最为这个clip的表示，然后将该clip以及之前的所有clip的表示进行累加，列所包含的所有详细的动态特征。为了更好地保持表示从开始到该clip的为止的长时序序clip之间的时序关系 ，我们将这个详细的动态特征去初始化下一个clip的LSTM。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-1f9767fba6bf517a7278709b79a52440_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"886\" data-rawheight=\"487\" class=\"origin_image zh-lightbox-thumb\" width=\"886\" data-original=\"https://pic1.zhimg.com/v2-1f9767fba6bf517a7278709b79a52440_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;886&#39; height=&#39;487&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"886\" data-rawheight=\"487\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"886\" data-original=\"https://pic1.zhimg.com/v2-1f9767fba6bf517a7278709b79a52440_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-1f9767fba6bf517a7278709b79a52440_b.jpg\"/></figure><p>三，为了进一步学习详细的时序特征，又提出了一个Clip-based Incremental Loss去优化网络。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-7b65acd391feb6ac0b58f3e6a32f9364_b.jpg\" data-caption=\"\" data-size=\"normal\" class=\"content_image\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;0&#39; height=&#39;0&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" class=\"content_image lazy\" data-actualsrc=\"https://pic1.zhimg.com/v2-7b65acd391feb6ac0b58f3e6a32f9364_b.jpg\"/></figure><p>结果</p><p>NTU</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-a9fb1209797bc3c48a0a6105bf16e7d1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"512\" data-rawheight=\"232\" class=\"origin_image zh-lightbox-thumb\" width=\"512\" data-original=\"https://pic2.zhimg.com/v2-a9fb1209797bc3c48a0a6105bf16e7d1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;512&#39; height=&#39;232&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"512\" data-rawheight=\"232\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"512\" data-original=\"https://pic2.zhimg.com/v2-a9fb1209797bc3c48a0a6105bf16e7d1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-a9fb1209797bc3c48a0a6105bf16e7d1_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>SYSU 3D HOI set</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-c5fc20ff18493f1e68f53bd9476cc7b1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"427\" data-rawheight=\"126\" class=\"origin_image zh-lightbox-thumb\" width=\"427\" data-original=\"https://pic2.zhimg.com/v2-c5fc20ff18493f1e68f53bd9476cc7b1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;427&#39; height=&#39;126&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"427\" data-rawheight=\"126\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"427\" data-original=\"https://pic2.zhimg.com/v2-c5fc20ff18493f1e68f53bd9476cc7b1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-c5fc20ff18493f1e68f53bd9476cc7b1_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>不同step的影响</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-2fc866ecbdd94616246b49d97fd6e5d1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"661\" data-rawheight=\"306\" class=\"origin_image zh-lightbox-thumb\" width=\"661\" data-original=\"https://pic2.zhimg.com/v2-2fc866ecbdd94616246b49d97fd6e5d1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;661&#39; height=&#39;306&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"661\" data-rawheight=\"306\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"661\" data-original=\"https://pic2.zhimg.com/v2-2fc866ecbdd94616246b49d97fd6e5d1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-2fc866ecbdd94616246b49d97fd6e5d1_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2><b><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_ECCV_2018/papers/Myunggi_Lee_Motion_Feature_Network_ECCV_2018_paper.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Motion Feature Network: Fixed Motion Filter for Action Recognition</a></b></h2><p>Myunggi Lee, Seungeui Lee, Sungjoon Son , Gyutae Park, and Nojun Kwak</p><p>Seoul National University, Seoul, South Korea</p><p>V.DO Inc., Suwon, Korea</p><p>首尔大学韩国兄弟的工作。</p><p>简述</p><p>之前将光流作为时间信息与包含空间信息的一组RGB图像组合使用的方法已经在动作识别任务中显示出极大的性能增强（个人觉得说的是双流呀， optical flow guided呀 ）。 然而，它具有昂贵的计算成本并且需要双流（RGB和光流）框架。 本文提出了包含运动块的MFNet（运动特征网络(Motion Feature Network) ），该运动块使得可以在可以端到端训练的统一网络中的相邻帧之间编码时空信息。 运动块可以附加到任何现有的基于CNN的动作识别框架，只需要很少的额外成本。</p><p>在两个动作识别数据集（Jester和Something-Something）上评估了。</p><p>抛出的问题</p><p>CNN好是好，但是没有时序信息。用光流又太耗时间了。</p><p>其次，很多数据集都是在对对象做分类，而不是action本身（仅仅通过一帧的场景，对象就能识别）。</p><p>所以他们的点子就做了一个专门识别motion feature的network MFN，再用这个block来做RGB 的处理。</p><p>方法的描述</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-2298f901dae25061204fcbb3be55f37b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1006\" data-rawheight=\"712\" class=\"origin_image zh-lightbox-thumb\" width=\"1006\" data-original=\"https://pic4.zhimg.com/v2-2298f901dae25061204fcbb3be55f37b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1006&#39; height=&#39;712&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1006\" data-rawheight=\"712\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1006\" data-original=\"https://pic4.zhimg.com/v2-2298f901dae25061204fcbb3be55f37b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-2298f901dae25061204fcbb3be55f37b_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>MFNet的结构如图所示。他们的工作是基于wang liming的时间片段网络（TSN），该体系结构用于从整个视频中采样的K个片段序列。</p><p>code：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/yjxiong/temporal-segment-networks\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/yjxiong/temp</span><span class=\"invisible\">oral-segment-networks</span><span class=\"ellipsis\"></span></a></p><p>他们的网络由两个主要组成部分组成。</p><p>一个是对空间信息进行编码的外观块。这可以是图像分类任务中使用的任何体系结构。在我们的实验中，他们用的ResNet [10]作为外观块的骨干网络。</p><p>另一个组件是运动块，它对时间信息进行编码。为了对运动表示进行建模，它将来自相同层次3的相应连续帧的两个连续特征映射作为输入，然后使用一组固定运动滤波器来提取时间信息。应将每个层次结构中提取的空间和时间特征适当地传播到下一个层次结构。</p><p>为了捕获运动表示，动作识别中常用的方法之一是使用光流作为CNN的输入。 尽管在动作识别任务中它具有重要作用，但光学流程在实践中计算成本很高。 为了取代光流的作用并提取时间特征，我们提出了与光流密切相关的运动滤波器。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-28ec8161bb36f7022b00f5b178334522_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"604\" data-rawheight=\"322\" class=\"origin_image zh-lightbox-thumb\" width=\"604\" data-original=\"https://pic3.zhimg.com/v2-28ec8161bb36f7022b00f5b178334522_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;604&#39; height=&#39;322&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"604\" data-rawheight=\"322\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"604\" data-original=\"https://pic3.zhimg.com/v2-28ec8161bb36f7022b00f5b178334522_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-28ec8161bb36f7022b00f5b178334522_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>这个就是滤波模块。</p><p>滤波的模块和RGB的特征有两种融合方法，一是相加，二是concat。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-41bf69762bf76d2ed3e8411774f7ca40_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"785\" data-rawheight=\"309\" class=\"origin_image zh-lightbox-thumb\" width=\"785\" data-original=\"https://pic1.zhimg.com/v2-41bf69762bf76d2ed3e8411774f7ca40_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;785&#39; height=&#39;309&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"785\" data-rawheight=\"309\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"785\" data-original=\"https://pic1.zhimg.com/v2-41bf69762bf76d2ed3e8411774f7ca40_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-41bf69762bf76d2ed3e8411774f7ca40_b.jpg\"/></figure><p>最后的结果，他在两个别的库上跑的结果。</p><p>K是分割的个数。 </p><h2><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_ECCV_2018/papers/Dongang_Wang_Dividing_and_Aggregating_ECCV_2018_paper.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Dividing and Aggregating Network for Multi-view Action Recognition</a></h2><p>Dongang Wang1, Wanli Ouyang1,2 , Wen Li3, and Dong Xu1</p><p>1 The University of Sydney, School of Electrical and Information Engineering</p><p>2 The University of Sydney, SenseTime Computer Vision Research Group</p><p>3 ETH Zurich, Computer Vision Laboratory</p><p>欧阳万里组的工作。</p><p>简述</p><p>本文提出了一种新的划分和聚合用于多视图动作识别的网络（DA-Net）。在DA-Net中，学习了较低层的所有视角共享的表示，而在较高层学习了每个视角的特定表示。然后，基于每个视角的视角特定表示和基于较低层的共享表示的视角分类器来训练特定于视角的动作分类器。视图分类器用于预测每个视频属于每个视角的可能性。最后，当融合视角特定动作分类器的预测分数时，来自多个视角的预测视角概率被用作权重。还提出了一种基于条件随机场（CRF）公式的新方法，以在来自不同分支的视角特定表示之间传递消息以相互帮助。</p><p>方法的描述</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-7ef2da42bf802ed64b42dd8b2784b581_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"685\" data-rawheight=\"396\" class=\"origin_image zh-lightbox-thumb\" width=\"685\" data-original=\"https://pic2.zhimg.com/v2-7ef2da42bf802ed64b42dd8b2784b581_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;685&#39; height=&#39;396&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"685\" data-rawheight=\"396\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"685\" data-original=\"https://pic2.zhimg.com/v2-7ef2da42bf802ed64b42dd8b2784b581_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-7ef2da42bf802ed64b42dd8b2784b581_b.jpg\"/></figure><p>最后的结果，他在NTU库上跑的结果。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-111ec1afdab011de0d0b7060e4241275_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"682\" data-rawheight=\"216\" class=\"origin_image zh-lightbox-thumb\" width=\"682\" data-original=\"https://pic2.zhimg.com/v2-111ec1afdab011de0d0b7060e4241275_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;682&#39; height=&#39;216&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"682\" data-rawheight=\"216\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"682\" data-original=\"https://pic2.zhimg.com/v2-111ec1afdab011de0d0b7060e4241275_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-111ec1afdab011de0d0b7060e4241275_b.jpg\"/></figure><h2><b><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_ECCV_2018/papers/Nuno_Garcia_Modality_Distillation_with_ECCV_2018_paper.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Modality Distillation with Multiple Stream Networks for Action Recognition</a></b></h2><p>Nuno C. Garcia, Pietro Morerio, and Vittorio Murino</p><p>Istituto Italiano di Tecnologia</p><p>Universita’ degli Studi di Genova</p><p>Universita‘ di Verona</p><p>简述</p><p>如何在训练阶段学习利用多模态数据的稳健表示的挑战，同时考虑在测试时的限制，例如噪声或丢失模态。本文提出了一种新的多模态视频动作识别方法，该方法是在蒸馏和特权信息的统一框架内开发的，称为广义蒸馏。特别是，虽然在训练的时候考虑从深度和RGB视频学习，但能仅在测试时依赖RGB数据。我们提出了一种新的方法来训练幻化网络，该网络通过时空表示的乘法连接，利用软标签和硬标签以及特征图之间的距离来学习提取深度特征。</p><p>方法的描述</p><p>这种学习范例，即使用额外信息训练模型时，通常被称为学习特权信息[30]或学附带信息[11]。</p><p>[30] Vapnik, V., Vashist, A.: A new learning paradigm: Learning using privileged information. Neural networks 22(5), 544–557 (2009)</p><p>[11] Hoffman, J., Gupta, S., Darrell, T.: Learning with side information through modality hallucination. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 826–834 (2016)</p><p>在这种情况下，我们引入了一种新的学习范式，如图1所示，将深度传递的信息提取到幻化网络中，这意味着在测试时“模仿”缺失的流。蒸馏[10] [1]是指任何培训程序，其中知识从先前训练的复杂模型转移到更简单的模型。我们的学习过程引入了一种新的损失函数，它受到广义精馏框架的启发[15]，它正式统一了蒸馏和特权信息学习理论。</p><p>我们的模型受到了Simonyan和Zisserman [25]引入的双流网络的启发，该网络在视频动作识别任务的传统设置中取得了显着的成功[2] [5]。与以前的工作不同，我们使用多模态数据，为每种模态部署一个流（在我们的例子中为RGB和深度），并在特权信息的框架中使用它。另一个鼓舞人心的工作是[11]，它提出了一个幻化网络来学习辅助信息。我们建立在这个想法的基础上，通过设计一个新的概念来扩展它通过更一般的损失函数和流间连接来学习和使用这种幻觉流的机制。</p><p>总之，本文的主要贡献如下：<br/>- 提出了一种能够利用的新的多模式流网络架构训练时采用多种数据模式，同时在测试时仅使用一种模式;</p><p><br/>- 引入了一种新的范例来学习一个幻化网络新颖的双流模型;</p><p><br/>- 在这种情况下，实现了一种流间连接机制改善幻化网络的学习过程，并设计了一个<br/>更广泛的损失函数，基于广义蒸馏框架;</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-0e0b9a644db901d8b53986d72ead11ce_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"569\" class=\"origin_image zh-lightbox-thumb\" width=\"960\" data-original=\"https://pic3.zhimg.com/v2-0e0b9a644db901d8b53986d72ead11ce_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;960&#39; height=&#39;569&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"569\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"960\" data-original=\"https://pic3.zhimg.com/v2-0e0b9a644db901d8b53986d72ead11ce_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-0e0b9a644db901d8b53986d72ead11ce_b.jpg\"/></figure><p> 最后的结果，他在NUT库上跑的结果。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-1ccca3c04b482828948e9be7298f125d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"951\" data-rawheight=\"592\" class=\"origin_image zh-lightbox-thumb\" width=\"951\" data-original=\"https://pic2.zhimg.com/v2-1ccca3c04b482828948e9be7298f125d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;951&#39; height=&#39;592&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"951\" data-rawheight=\"592\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"951\" data-original=\"https://pic2.zhimg.com/v2-1ccca3c04b482828948e9be7298f125d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-1ccca3c04b482828948e9be7298f125d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>持续更新中。。。</p><hr/><p>总结：</p><p>几个大家都在做而且对性能很有帮助的点：</p><ol><li>RGB 和skeleton，depth 的<b>有效</b>融合。比如hu jianfang的双线性融合，Verona大学的幻化网络来处理Depth模块。</li><li>skeleton的更好的表征的探索。比如NTU liu jun的形变卷积去做，或者中科院的 spatial reasoning network去做。</li><li>时序信息和RGB的有效结合。Hu jianfang的时空feature，韩国首尔大学的motion filter替代光流。</li><li>多阶时序信息的使用。Action history sequence， skip -clips， 等等，都是在做这个东西。</li></ol><p>我在接下来的工作中，也是准备用HMM和多阶时序信息结合来更好地表征action的动作。</p>", 
            "topic": [
                {
                    "tag": "计算机视觉", 
                    "tagLink": "https://api.zhihu.com/topics/19590195"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "模式识别", 
                    "tagLink": "https://api.zhihu.com/topics/19564812"
                }
            ], 
            "comments": [
                {
                    "userName": "每天提高一点儿", 
                    "userLink": "https://www.zhihu.com/people/824f9f0991f10056500bdf178bcc91f6", 
                    "content": "<p>大神，Deep Bilinear Learning for RGB-D Action Recognition这个中山大学的网站打不开啊!能分享一下资源吗</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/57981385", 
            "userName": "奥卢的陈皓宇", 
            "userLink": "https://www.zhihu.com/people/6e25ca19a73ea5a2d81f195cf4c1851f", 
            "upvote": 20, 
            "title": "ECCV 2018年 ACTION RECOGNITION 的汇总 （ECCV 第二期）", 
            "content": "<p>把ECCV和CVPR还有AAAI 2018年的action recognition汇了个总，放在这里。</p><p>这里是第二次更新，上一次更新总结了ECCV的六篇文章。</p><p>这一次总结剩下来的五篇ECCV的文章。</p><p>所有文章汇总请看上一期。<br/></p><hr/><p><br/>ECCV 2018<br/>第一期<br/></p><ul><li><u><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_ECCV_2018/papers/Junwu_Weng_Deformable_Pose_Traversal_ECCV_2018_paper.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Deformable Pose Traversal Convolution for 3D Action and Gesture Recognition</a></u></li><li><u><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_ECCV_2018/html/HU_Jian-Fang_Deep_Bilinear_Learning_ECCV_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Deep Bilinear Learning for RGB-D Action Recognition</a></u></li><li><u><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_ECCV_2018/html/Chenyang_Si_Skeleton-Based_Action_Recognition_ECCV_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Skeleton-Based Action Recognition with Spatial Reasoning and Temporal Stack Learning</a></u></li><li><u><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_ECCV_2018/html/Myunggi_Lee_Motion_Feature_Network_ECCV_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Motion Feature Network: Fixed Motion Filter for Action Recognition</a></u></li><li><u><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_ECCV_2018/html/Dongang_Wang_Dividing_and_Aggregating_ECCV_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Dividing and Aggregating Network for Multi-view Action Recognition</a></u></li><li><u><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_ECCV_2018/html/Nuno_Garcia_Modality_Distillation_with_ECCV_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Modality Distillation with Multiple Stream Networks for Action Recognition</a></u></li></ul><p>第二期（本期）<br/></p><ul><li><b><u><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_ECCV_2018/html/Yang_Du_Interaction-aware_Spatio-temporal_Pyramid_ECCV_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Interaction-aware Spatio-temporal Pyramid Attention Networks for Action Classification</a></u></b></li><li><b><u><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_ECCV_2018/html/Ali_Diba_Spatio-Temporal_Channel_Correlation_ECCV_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Spatio-Temporal Channel Correlation Networks for Action Classification</a></u></b></li><li><b><u><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_ECCV_2018/html/Dong_Li_Recurrent_Tubelet_Proposal_ECCV_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Recurrent Tubelet Proposal and Recognition Networks for Action Detection</a></u></b></li><li><b><u><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_ECCV_2018/html/Lan_Wang_PM-GANs_Discriminative_Representation_ECCV_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">PM-GANs: Discriminative Representation Learning for Action Recognition Using Partial-modalities</a></u></b></li><li><b><u><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_ECCV_2018/html/Yingwei_Li_RESOUND_Towards_Action_ECCV_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">RESOUND: Towards Action Recognition without Representation Bias</a></u></b></li></ul><p><br/>以下是正文</p><hr/><p><b><u><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_ECCV_2018/html/Yang_Du_Interaction-aware_Spatio-temporal_Pyramid_ECCV_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">INTERACTION-AWARE SPATIO-TEMPORAL PYRAMID ATTENTION NETWORKS FOR ACTION CLASSIFICATION</a></u></b><br/>YANG DU1,2,3, CHUNFENG YUAN2⋆, BING LI2, LILI ZHAO3, YANGXI LI4, AND WEIMING HU2</p><p><br/>1 UNIVERSITY OF CHINESE ACADEMY OF SCIENCES<br/>2 CAS CENTER FOR EXCELLENCE IN BRAIN SCIENCE AND INTELLIGENCE TECHNOLOGY, NATIONAL<br/>LABORATORY OF PATTERN RECOGNITION, INSTITUTE OF AUTOMATION, CAS<br/>3 MEITU, 4 NATIONAL COMPUTER NETWORK EMERGENCY RESPONSE TECHNICAL<br/>TEAM/COORDINATION CENTER OF CHINA</p><p><br/>这是中科院的工作。大名鼎鼎的Weiming Hu指导的工作，发现来来去去都是那些人。<br/>本文的贡献是利用了局部特征图具有的交互性质而提出的注意力机制。以及一个金字塔的特征图。<br/>特征图里面的局部特征具有高相关性，因为它们的感知域经常重叠。<br/>自我注意力机制通常使用加权和（或其他函数）使用每个局部特征的元素来获得其权重分数，这忽略了局部特征之间的相互作用。<br/>为了解决这个问题，他们提出了一个有效的交互感知自我关注模型，它受到PCA的启发，可以学习注意力图。<br/>此外，由于深层网络中的不同层捕获不同尺度的特征图，他们使用这些特征图构建空间金字塔，然后利用多尺度信息获得更准确的注意力分数，用于对所有局部特征进行加权。特征图的空间位置以计算注意力图。此外，空间金字塔注意力不受其输入要素图数量的限制，因此很容易扩展到时空版本。最后，模型可以嵌入到一般的CNN中，以形成用于行动分类的端到端关注网络。<br/>在UCF101，HMDB51和Charades上实现了最先进的结果。<br/> <br/>主要贡献：1）仿照PCA定义了新的Loss；2）利用多尺度信息；3）输入可以为任意帧数，并融合进主流CNN架构<br/>关于PCA的讨论<br/>PCA 可以提取全局特征主要维度的主成分信息，而这些主成分信息可以看作是提取的局部特征，最后降维后的全局特征即是关键局部特征的集合。注意机制的目的是从局部特征集合中提取关键部分，也就是 PCA 中的局部特征。不同的是注意力机制使用每个局部特征对应的加权得分来计算最终的全局特征。PCA 利用协方差矩阵来获得降维（或加权权重）的基向量，从而减少特征间的信息冗余和噪声。基于以上背景，该团队使用 PCA 来指导提出的注意力模型，并通过将 PCA 算法转换成损失设计实现。此外，由于深度网络中的不同层可以捕获不同尺度的特征图，算法使用这些特征图来构造空间金字塔，利用多尺度信息来计算每个局部通道特征更精确的注意力分数，这些权重得分用于在所有空间位置中对局部特征进行加权。<br/>网络结构如下：<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-e97f29ec3b69fa2933c4a0436df1e822_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"245\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic3.zhimg.com/v2-e97f29ec3b69fa2933c4a0436df1e822_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;640&#39; height=&#39;245&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"245\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https://pic3.zhimg.com/v2-e97f29ec3b69fa2933c4a0436df1e822_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-e97f29ec3b69fa2933c4a0436df1e822_b.jpg\"/></figure><p><br/> <br/>本论文定义了一个新的交互感知时空金字塔注意力层，以此实现输入在深度卷积神经网络中各个层的不同尺度局部特征的交互感知和时空特征融合的功能。它的架构如上图所示，算法首先定义了一个下采样函数 R, 将不同层的特征图统一到一个尺度。接着对不同尺度的特征图的局部通道特征使用注意力机制进行关键特征提取，通过使用融合函数对不同尺度的特征进行融合，并计算每个局部特征的注意力得分，用于加权特征。<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-f6c65173d9f82475033741cd3632cd63_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"740\" data-rawheight=\"125\" class=\"origin_image zh-lightbox-thumb\" width=\"740\" data-original=\"https://pic4.zhimg.com/v2-f6c65173d9f82475033741cd3632cd63_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;740&#39; height=&#39;125&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"740\" data-rawheight=\"125\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"740\" data-original=\"https://pic4.zhimg.com/v2-f6c65173d9f82475033741cd3632cd63_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-f6c65173d9f82475033741cd3632cd63_b.jpg\"/></figure><p><br/>在 PCA 中使用协方差矩阵计算投影向量并依此进行降维，即提取关键的局部特征，本论文将其转化损失函数的设计加入到最终的模型中：<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-3a9fc38dbf24758a57cb9e2b81aea33c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"740\" data-rawheight=\"76\" class=\"origin_image zh-lightbox-thumb\" width=\"740\" data-original=\"https://pic1.zhimg.com/v2-3a9fc38dbf24758a57cb9e2b81aea33c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;740&#39; height=&#39;76&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"740\" data-rawheight=\"76\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"740\" data-original=\"https://pic1.zhimg.com/v2-3a9fc38dbf24758a57cb9e2b81aea33c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-3a9fc38dbf24758a57cb9e2b81aea33c_b.jpg\"/></figure><p><br/>再对提出的空间金字塔注意力模型进行约束，使其不同尺度层的特征图尽量关注到不同的信息，加入分类损失得出最终的损失函数：<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-7ad5ee9ee4b3d9ca76e2fca4928f514a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"740\" data-rawheight=\"92\" class=\"origin_image zh-lightbox-thumb\" width=\"740\" data-original=\"https://pic3.zhimg.com/v2-7ad5ee9ee4b3d9ca76e2fca4928f514a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;740&#39; height=&#39;92&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"740\" data-rawheight=\"92\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"740\" data-original=\"https://pic3.zhimg.com/v2-7ad5ee9ee4b3d9ca76e2fca4928f514a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-7ad5ee9ee4b3d9ca76e2fca4928f514a_b.jpg\"/></figure><p><br/>论文提出的模型参数与输入特征图的数目无关，因此，自然地将其拓展到视频级端到端训练的时空网络。<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3d560496f4e74e7cc07c69ed0d9f6d79_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"332\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic2.zhimg.com/v2-3d560496f4e74e7cc07c69ed0d9f6d79_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;640&#39; height=&#39;332&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"332\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https://pic2.zhimg.com/v2-3d560496f4e74e7cc07c69ed0d9f6d79_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-3d560496f4e74e7cc07c69ed0d9f6d79_b.jpg\"/></figure><p><br/>由结构图中可以看到，金字塔的融合是直接cancatenate的。<br/>他也讨论了集中融合的方式：<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-159b86f28c6fd8166bdf1bb024251050_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"900\" data-rawheight=\"722\" class=\"origin_image zh-lightbox-thumb\" width=\"900\" data-original=\"https://pic1.zhimg.com/v2-159b86f28c6fd8166bdf1bb024251050_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;900&#39; height=&#39;722&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"900\" data-rawheight=\"722\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"900\" data-original=\"https://pic1.zhimg.com/v2-159b86f28c6fd8166bdf1bb024251050_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-159b86f28c6fd8166bdf1bb024251050_b.jpg\"/></figure><p><br/>在三个数据集上的表现：<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-6029591138bbad66bd45ab1bf5374ef4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"516\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic1.zhimg.com/v2-6029591138bbad66bd45ab1bf5374ef4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;640&#39; height=&#39;516&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"516\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https://pic1.zhimg.com/v2-6029591138bbad66bd45ab1bf5374ef4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-6029591138bbad66bd45ab1bf5374ef4_b.jpg\"/></figure><p><br/><b><u><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_ECCV_2018/html/Ali_Diba_Spatio-Temporal_Channel_Correlation_ECCV_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">SPATIO-TEMPORAL CHANNEL CORRELATION NETWORKS FOR ACTION CLASSIFICATION</a></u></b></p><p><br/>ALI DIBA1,4,⋆, MOHSEN FAYYAZ2,⋆, VIVEK SHARMA3, M.MAHDI ARZANI4, RAHMANYOUSEFZADEH4, JUERGEN GALL2, LUC VAN GOOL1,4</p><p><br/>1 ESAT-PSI, KU LEUVEN,<br/>2 UNIVERSITY OF BONN,<br/>3 CV:HCI, KIT, KARLSRUHE,<br/>4 SENSIFAI</p><p><br/>Luc Van Gool 大佬带领下做的工作。<br/>简述<br/>这篇文章主要是提出了一种利用时空特征中channel correlation来训练3D CNN的方式，以及用2D CNN来辅助3D CNN训练的方式。</p><p><br/>主要贡献： 1 ）针对3D卷积网络设计了Attention机制(TCB, SCB)；2）基于2D预训练网络的参数初始化方式。</p><p><br/>STC（SPATIO-TEMPORAL CHANNEL CORRELATION）BLOCK<br/>由两个branch组成：SCB（spatial correlation branch）和TCB（temporal correlation branch）<br/>目的是学习在时空特征上的inter channels correlations的信息。</p><p><br/>TCB：<br/>X是3D卷积的输出feature-map，维度是H x W x T x C 。在这个brance对spatial和temporal进行pooling来抽取channel特征，之后进行两层FC变换。W1的维度是C/r x C, W2的维度是C x C/r， 最后用s对X进行缩放。<br/></p><u><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-50513ab4a4d2b47a9f7f9fca510552be_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"662\" data-rawheight=\"174\" class=\"origin_image zh-lightbox-thumb\" width=\"662\" data-original=\"https://pic3.zhimg.com/v2-50513ab4a4d2b47a9f7f9fca510552be_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;662&#39; height=&#39;174&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"662\" data-rawheight=\"174\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"662\" data-original=\"https://pic3.zhimg.com/v2-50513ab4a4d2b47a9f7f9fca510552be_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-50513ab4a4d2b47a9f7f9fca510552be_b.jpg\"/></figure></u><p class=\"ztext-empty-paragraph\"><br/></p><u><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-98d00e3de1cb5e987038dd0ea1728d04_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"656\" data-rawheight=\"198\" class=\"origin_image zh-lightbox-thumb\" width=\"656\" data-original=\"https://pic1.zhimg.com/v2-98d00e3de1cb5e987038dd0ea1728d04_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;656&#39; height=&#39;198&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"656\" data-rawheight=\"198\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"656\" data-original=\"https://pic1.zhimg.com/v2-98d00e3de1cb5e987038dd0ea1728d04_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-98d00e3de1cb5e987038dd0ea1728d04_b.jpg\"/></figure></u><p><br/>SCB：<br/>与TCB类似，但是只对channel-wise的信息进行压缩。这一个branch考虑了temporal-channel信息。W1的维度是(T * C)/r x (T * C), W2维度是 C x (T*C)/r<br/></p><u><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-1a52bc9e41bbcfb6ac62e324b738145d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"530\" data-rawheight=\"170\" class=\"origin_image zh-lightbox-thumb\" width=\"530\" data-original=\"https://pic2.zhimg.com/v2-1a52bc9e41bbcfb6ac62e324b738145d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;530&#39; height=&#39;170&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"530\" data-rawheight=\"170\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"530\" data-original=\"https://pic2.zhimg.com/v2-1a52bc9e41bbcfb6ac62e324b738145d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-1a52bc9e41bbcfb6ac62e324b738145d_b.jpg\"/></figure></u><p class=\"ztext-empty-paragraph\"><br/></p><u><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-80a352956c95efd7926c099befb572eb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"660\" data-rawheight=\"258\" class=\"origin_image zh-lightbox-thumb\" width=\"660\" data-original=\"https://pic4.zhimg.com/v2-80a352956c95efd7926c099befb572eb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;660&#39; height=&#39;258&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"660\" data-rawheight=\"258\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"660\" data-original=\"https://pic4.zhimg.com/v2-80a352956c95efd7926c099befb572eb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-80a352956c95efd7926c099befb572eb_b.jpg\"/></figure></u><p><br/>最后总体的结构：<br/></p><u><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-f88fa8cbe3e3786d184b80144f3df783_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"962\" data-rawheight=\"756\" class=\"origin_image zh-lightbox-thumb\" width=\"962\" data-original=\"https://pic4.zhimg.com/v2-f88fa8cbe3e3786d184b80144f3df783_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;962&#39; height=&#39;756&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"962\" data-rawheight=\"756\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"962\" data-original=\"https://pic4.zhimg.com/v2-f88fa8cbe3e3786d184b80144f3df783_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-f88fa8cbe3e3786d184b80144f3df783_b.jpg\"/></figure></u><p><br/>TRANSFER LEARNING<br/></p><u><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-c72f6a9d8148741d6d0bc582b24fb725_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1428\" data-rawheight=\"468\" class=\"origin_image zh-lightbox-thumb\" width=\"1428\" data-original=\"https://pic2.zhimg.com/v2-c72f6a9d8148741d6d0bc582b24fb725_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1428&#39; height=&#39;468&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1428\" data-rawheight=\"468\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1428\" data-original=\"https://pic2.zhimg.com/v2-c72f6a9d8148741d6d0bc582b24fb725_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-c72f6a9d8148741d6d0bc582b24fb725_b.jpg\"/></figure></u><p><br/>这个迁移学习是完全无监督的，从同一个视频的时间戳提取出来的两个pairs是正样本，属于不同视频的是负样本。在训练时把2D CNN的参数冻结，2D CNN的结果是对X帧进行一个pooling得到。<br/>这里的transfer learning有几个难点，一个是底层的处理，一个是正负样本到底是怎么做的还不得而知。</p><p><br/>结果<br/>UCF101<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-d579923b48a39eae5604d8f011d6ba96_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"713\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic3.zhimg.com/v2-d579923b48a39eae5604d8f011d6ba96_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;640&#39; height=&#39;713&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"713\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https://pic3.zhimg.com/v2-d579923b48a39eae5604d8f011d6ba96_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-d579923b48a39eae5604d8f011d6ba96_b.jpg\"/></figure><p><br/><b><u><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_ECCV_2018/html/Dong_Li_Recurrent_Tubelet_Proposal_ECCV_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">RECURRENT TUBELET PROPOSAL AND RECOGNITION NETWORKS FOR ACTION DETECTION</a></u></b></p><p><br/>DONG LI1, ZHAOFAN QIU1, QI DAI2, TING YAO3, AND TAO MEI3</p><p><br/>1 UNIVERSITY OF SCIENCE AND TECHNOLOGY OF CHINA, HEFEI, CHINA<br/>2 MICROSOFT RESEARCH, BEIJING, CHINA<br/>3 JD AI RESEARCH, BEIJING, CHINA</p><p><br/>微软研究院，京东，以及中科大的合作工作。<br/>简述<br/>Tubelet最近特别火，从这一片物体检测的文章开始的：<br/>TPN: Tubelet Proposal Network<br/>Introduction<br/><code>TPN</code>, short for <code>Tubelet Proposal Network</code>, is a deep learning framework for video object detection, originally designed for ImageNet VID dataset.<br/>This framework mainly contains two components shown below: </p><u><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-b20cb940a3c10d5db219042d4d0b4648_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"692\" data-rawheight=\"356\" class=\"origin_image zh-lightbox-thumb\" width=\"692\" data-original=\"https://pic1.zhimg.com/v2-b20cb940a3c10d5db219042d4d0b4648_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;692&#39; height=&#39;356&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"692\" data-rawheight=\"356\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"692\" data-original=\"https://pic1.zhimg.com/v2-b20cb940a3c10d5db219042d4d0b4648_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-b20cb940a3c10d5db219042d4d0b4648_b.jpg\"/></figure></u><p><br/>The <code>Tubelet Proposal Network</code> generates hundreds of tubelet proposals based on static image box proposals, and the <code>Encoder-decoder LSTM</code> encodes the tubelet visual features into memory and decodes the information to classify each tubelet box proposal into different classes. More details in the <u><a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1702.06355\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">paper</a></u> in <code>CVPR 2017</code>.</p><p><br/>这里这篇文章的Tubelet又有点不一样。</p><p><br/>检测动作，现有方法主要为每个人提出proposal，同时考虑它们的时间背景。</p><p><br/>检测作为一种动作本质上是一系列动作。这促使我们在之前的框架中利用局部的动作proposal<br/>具体来说，他们提出了一种新颖的深层结构，称为Recurrent Tubelet Proposal和识别（RTPR）网络以结合时间上下文用于动作检测。</p><p><br/>拟议的RTPR由两个相关网络组成，即，Recurrent Tubelet Proposal（RTP）网络和Recurrent Tubelet识别（RTR）网络。</p><p><br/>RTP给出一个局部的兴趣区域，然后给出一个proposal，然后进行估算下一帧中的proposal。</p><p><br/>RTR利用多渠道架构，其中包括每个通道，一个小管proposal被送入CNN加LSTM，以反复识别小管中的动作。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-cbfaa7647c2646599ae7bb3419ee8527_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"386\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic4.zhimg.com/v2-cbfaa7647c2646599ae7bb3419ee8527_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;640&#39; height=&#39;386&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"386\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https://pic4.zhimg.com/v2-cbfaa7647c2646599ae7bb3419ee8527_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-cbfaa7647c2646599ae7bb3419ee8527_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-96e6dc8f5f350419f607cc4b7ef2db71_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"205\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic2.zhimg.com/v2-96e6dc8f5f350419f607cc4b7ef2db71_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;640&#39; height=&#39;205&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"205\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https://pic2.zhimg.com/v2-96e6dc8f5f350419f607cc4b7ef2db71_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-96e6dc8f5f350419f607cc4b7ef2db71_b.jpg\"/></figure><p><br/>他们进行了大量实验在四个基准数据集上，并展示了优于最先进方法的卓越成果。更值得注意的是，获得了分别为98.6％，81.3％，77.9％和22.3％的mAP，涨幅分别为2.9％，4.3％，0.7％和3.9％，在对应的 UCF-Sports，J-HMDB，UCF-101和AVA数据集上。</p><p><br/><b><u><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_ECCV_2018/html/Lan_Wang_PM-GANs_Discriminative_Representation_ECCV_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">PM-GANS: DISCRIMINATIVE REPRESENTATION LEARNING FOR ACTION RECOGNITION USING PARTIAL-MODALITIES</a></u></b></p><p><br/>LAN WANG1,2[0000−0002−7341−4904], CHENQIANG GAO1,2, LUYU YANG3, YUE ZHAO1,2, WANGMENG ZUO4, AND DEYU MENG5</p><p><br/>1 SCHOOL OF COMMUNICATION AND INFORMATION ENGINEERING, CHONGQING UNIVERSITY OF POSTS AND TELECOMMUNICATIONS, CHONGQING 400065, CHINA<br/>2 CHONGQING KEY LABORATORY OF SIGNAL AND INFORMATION PROCESSING, CHONGQING</p><p><br/>重庆邮电大学的工作（但是不知道为啥我在西交大的网页上搜到这个工作）。</p><p><br/>简述</p><p><br/>作者认为不同模态的数据应有互补的信息（如RGB+红外），本文提出的PM-GAN能够通过部分模态的数据来学习全模态的表示，从而提升动作识别任务的性能。完整的表示是通过生成的表示代替丢失的数据通道来实现的。除此之外提供了一个新公开的行动识别红外数据集。<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-c5772ad289cc48029c8d93c16f19130c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"392\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic1.zhimg.com/v2-c5772ad289cc48029c8d93c16f19130c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;640&#39; height=&#39;392&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"392\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https://pic1.zhimg.com/v2-c5772ad289cc48029c8d93c16f19130c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-c5772ad289cc48029c8d93c16f19130c_b.jpg\"/></figure><p><br/>就是拿红外的特征对RGB的特征用GAN做了一个增强。</p><p><br/>和上一期的模块蒸馏的文章非常的像：<br/><u><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_ECCV_2018/html/Nuno_Garcia_Modality_Distillation_with_ECCV_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Modality Distillation with Multiple Stream Networks for Action Recognition</a></u><br/><br/><u><b><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_ECCV_2018/html/Yingwei_Li_RESOUND_Towards_Action_ECCV_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">RESOUND: TOWARDS ACTION RECOGNITION WITHOUT REPRESENTATION BIAS</a></b></u><br/></p><p>YINGWEI LI, YI LI, AND NUNO VASCONCELOS<br/>UC SAN DIEGO<br/></p><p>我非常喜欢的一个学校的工作。在Github上发现有人已经写了详细的解读，于是直接偷过来用了。哈哈。</p><p><br/>偷窃的链接：<br/><u><a href=\"https://link.zhihu.com/?target=https%3A//github.com/xiadingZ/Paper_reading/blob/master/RESOUND%2520Towards%2520Action%2520Recognition%2520without%2520Representation%2520Bias.md\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/xiadingZ/Pap</span><span class=\"invisible\">er_reading/blob/master/RESOUND%20Towards%20Action%20Recognition%20without%20Representation%20Bias.md</span><span class=\"ellipsis\"></span></a></u><br/></p><p>简述</p><p><br/>这篇文章主要是提出了一种程序RESOUND来量化和最小化表示偏差，用来解决动作识别数据集偏差的问题。</p><p><br/>研究了两种版本的RESOUND。</p><p><br/>提出了一种显式RESOUND过程，通过对现有数据集进行采样来组合新数据集。</p><p><br/>隐式的RESOUND程序用于指导创建一个新的数据集Diving48，该数据集包含超过18,000个竞争性潜水行动的视频剪辑，涵盖48个细粒度的潜水课程。</p><p><br/>许多数据集中用静态信息就能取得很好的效果，这就是有了静态偏差。如下是三种静态偏差：<br/></p><ul><li>object bias：例如“play piano”在ActivitNet与UCF101中是唯一一个类别涉及到钢琴的，那么识别出钢琴就能进行动作分类。</li><li>scene bias：basketball dunk和soccer juggling有不同的时间特征，然而只用通过背景就行进行分类。</li><li>person bias：brushing hair有很多脸部特写，military marching通常包含一大群穿着军事制服的人</li></ul><p>总体来说有两类bias</p><ul><li>Dataset Bias与算法相关，在A数据集上效果好但是在B上差，可以通过评价算法的bias和var得出</li><li>Representation Bias与数据集相关。例如一个数据集是抛硬币，如果硬币重量不均匀，那么数据集就是有偏的</li></ul><p>定义：<br/>Representation: a mathematical characterization of some property of the visual work。例如光流就是一种representation。<br/>下面是representation $\\phi$在数据集上的性能的定义，$\\gamma_\\phi$表示使用这种representation的一种算法。<br/></p><u><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-ed2a1c453589383a1c16a1a68e2e26e2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"510\" data-rawheight=\"124\" class=\"origin_image zh-lightbox-thumb\" width=\"510\" data-original=\"https://pic3.zhimg.com/v2-ed2a1c453589383a1c16a1a68e2e26e2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;510&#39; height=&#39;124&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"510\" data-rawheight=\"124\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"510\" data-original=\"https://pic3.zhimg.com/v2-ed2a1c453589383a1c16a1a68e2e26e2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-ed2a1c453589383a1c16a1a68e2e26e2_b.jpg\"/></figure></u><p><br/>下面是chance-level-performance，例如分类任务中的随机分类<br/></p><u><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-3bc6d558dbcca60ad96bbadcda1c6d2c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"424\" data-rawheight=\"108\" class=\"origin_image zh-lightbox-thumb\" width=\"424\" data-original=\"https://pic1.zhimg.com/v2-3bc6d558dbcca60ad96bbadcda1c6d2c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;424&#39; height=&#39;108&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"424\" data-rawheight=\"108\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"424\" data-original=\"https://pic1.zhimg.com/v2-3bc6d558dbcca60ad96bbadcda1c6d2c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-3bc6d558dbcca60ad96bbadcda1c6d2c_b.jpg\"/></figure></u><p><br/>数据集对representation的bias以及class level的bias则表示为<br/></p><u><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-9703151228103498488b23a44108ade6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"456\" data-rawheight=\"132\" class=\"origin_image zh-lightbox-thumb\" width=\"456\" data-original=\"https://pic3.zhimg.com/v2-9703151228103498488b23a44108ade6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;456&#39; height=&#39;132&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"456\" data-rawheight=\"132\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"456\" data-original=\"https://pic3.zhimg.com/v2-9703151228103498488b23a44108ade6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-9703151228103498488b23a44108ade6_b.jpg\"/></figure></u><p class=\"ztext-empty-paragraph\"><br/></p><u><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-9cd9e44cdb6e58be2e0c15e3383a5998_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"476\" data-rawheight=\"154\" class=\"origin_image zh-lightbox-thumb\" width=\"476\" data-original=\"https://pic1.zhimg.com/v2-9cd9e44cdb6e58be2e0c15e3383a5998_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;476&#39; height=&#39;154&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"476\" data-rawheight=\"154\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"476\" data-original=\"https://pic1.zhimg.com/v2-9cd9e44cdb6e58be2e0c15e3383a5998_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-9cd9e44cdb6e58be2e0c15e3383a5998_b.jpg\"/></figure></u><p><br/>那么目的就是设计一个对representation family Ryou最小的偏差的数据集<br/></p><u><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3d6a34048c7844bb2109e48fed44de69_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"564\" data-rawheight=\"126\" class=\"origin_image zh-lightbox-thumb\" width=\"564\" data-original=\"https://pic2.zhimg.com/v2-3d6a34048c7844bb2109e48fed44de69_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;564&#39; height=&#39;126&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"564\" data-rawheight=\"126\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"564\" data-original=\"https://pic2.zhimg.com/v2-3d6a34048c7844bb2109e48fed44de69_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-3d6a34048c7844bb2109e48fed44de69_b.jpg\"/></figure></u><p class=\"ztext-empty-paragraph\"><br/></p><u><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-c5205fd87179f45896efa2a848ef8410_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1430\" data-rawheight=\"730\" class=\"origin_image zh-lightbox-thumb\" width=\"1430\" data-original=\"https://pic1.zhimg.com/v2-c5205fd87179f45896efa2a848ef8410_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1430&#39; height=&#39;730&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1430\" data-rawheight=\"730\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1430\" data-original=\"https://pic1.zhimg.com/v2-c5205fd87179f45896efa2a848ef8410_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-c5205fd87179f45896efa2a848ef8410_b.jpg\"/></figure></u><p><br/>有两种实现RESOUND的方式<br/></p><ol><li>以一个数据集D为起点，添加或删除来进行优化</li><li>找到对representation family R最无偏的类别来设计。例如不能仅仅通过判断背景就能把object进行分类。</li></ol><p><br/>至此，ECCV 2018的action recognition相关文章更新完毕。<br/>下一期持续更新 CVPR 2018的相关文章。<br/></p><hr/><p><br/>总结：<br/>发现ECCV 剩下的几篇文章Action recognition 都是关于RGB的：</p><ol><li>skeleton，depth， infrared 等模块对RGB的辅助作用。</li><li>时序信息和2D的RGB的有效融合。Hu weiming 的文章，Luc Van的文章都在做这个探索。</li><li>多阶时序信息的使用。</li></ol><p></p>", 
            "topic": [
                {
                    "tag": "计算机视觉", 
                    "tagLink": "https://api.zhihu.com/topics/19590195"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/61112376", 
            "userName": "奥卢的陈皓宇", 
            "userLink": "https://www.zhihu.com/people/6e25ca19a73ea5a2d81f195cf4c1851f", 
            "upvote": 16, 
            "title": "2018年 ACTION RECOGNITION 的汇总 （CVPR 上期）", 
            "content": "<p>把ECCV和CVPR还有AAAI 2018年的action recognition汇了个总，放在这里。</p><p>这里是第三次更新，前两次更新总结了ECCV的全部相关文章。</p><p>这一次总结CVPR的文章。</p><p>所有文章汇总请看第一期。</p><hr/><h2>CVPR 2018</h2><p>本期：</p><ul><li><b><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_cvpr_2018/html/Tang_Deep_Progressive_Reinforcement_CVPR_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Deep Progressive Reinforcement Learning for Skeleton-Based Action Recognition</a></b></li><li><b><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_cvpr_2018/html/Zhou_MiCT_Mixed_3D2D_CVPR_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">MiCT: Mixed 3D/2D Convolutional Tube for Human Action Recognition</a></b></li><li><b><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_cvpr_2018/html/Sun_Optical_Flow_Guided_CVPR_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Optical Flow Guided Feature: A Fast and Robust Motion Representation for Video Action Recognition</a></b></li><li><b><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_cvpr_2018/html/Luvizon_2D3D_Pose_Estimation_CVPR_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">2D/3D Pose Estimation and Action Recognition Using Multitask Deep Learning</a></b></li></ul><p>下期：</p><p><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_cvpr_2018/html/Wu_Compressed_Video_Action_CVPR_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Compressed Video Action Recognition</a></p><p><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_cvpr_2018/html/Tran_A_Closer_Look_CVPR_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">A Closer Look at Spatiotemporal Convolutions for Action Recognition</a></p><p><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_cvpr_2018/html/Lei_Temporal_Deformable_Residual_CVPR_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Temporal Deformable Residual Networks for Action Segmentation in Videos</a></p><p><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_cvpr_2018/html/Choutas_PoTion_Pose_MoTion_CVPR_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">PoTion: Pose MoTion Representation for Action Recognition</a></p><p><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_cvpr_2018/html/Feichtenhofer_What_Have_We_CVPR_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">What Have We Learned From Deep Representations for Action Recognition?</a></p><p><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_cvpr_2018/html/Zhu_Towards_Universal_Representation_CVPR_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Towards Universal Representation for Unseen Action Recognition</a></p><p>以下是正文</p><hr/><h2><b><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_cvpr_2018/html/Tang_Deep_Progressive_Reinforcement_CVPR_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Deep Progressive Reinforcement Learning for Skeleton-Based Action Recognition</a></b></h2><p>Yansong Tang1,2,3,∗ Yi Tian1, ∗Jiwen Lu1,2,3 Peiyang Li1, Jie Zhou1,2,3</p><p>1Department of Automation, Tsinghua University, China</p><p>2State Key Lab of Intelligent Technologies and Systems, Tsinghua University, China</p><p>3Beijing National Research Center for Information Science and Technology, China</p><p>清华大学的工作，据我所知是第一个用基于深度的增强学习的方法来做骨骼的动作识别的。</p><p>去年看了这个文章好多次，这次系统地看看。</p><p>本文的核心思想就是先用RL的算法来选择关键帧， 再把选取到的关键帧输入到神经网络，得到这个序列的标签：深度渐进式强化学习方法（DPRL）。</p><p>贡献三点：RL算法，FDNet，GCNN图模型，以及渐进式的学习过程。</p><p>目的是为了识别动作而提取最具信息量的帧，然后去除不明确的帧。</p><p>因为为每一个视频选择最具代表性得帧是多种多样的，他们就构建了帧选择模型是一个渐进的过程通过深度强化学习。</p><p>逐渐调整选择帧的时候主要考虑两个重要的因素：</p><p>（1）被选择帧的质量</p><p>（2）是被选择的帧和整个视频之间的关系。</p><p>而且，考虑到人类身体固有的拓扑结构是基于图的架构，这些顶点和边缘分别代表了铰接头和硬骨，应用了基于图的卷积神经网络来刻画动作识别时这些关节的依赖。</p><p>方法在三个广泛使用的数据集上实现了很好的效果。<br/>1. FDNet</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-4f8d2ec3770d01975ef251686bfd9e3e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"693\" data-rawheight=\"371\" class=\"origin_image zh-lightbox-thumb\" width=\"693\" data-original=\"https://pic3.zhimg.com/v2-4f8d2ec3770d01975ef251686bfd9e3e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;693&#39; height=&#39;371&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"693\" data-rawheight=\"371\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"693\" data-original=\"https://pic3.zhimg.com/v2-4f8d2ec3770d01975ef251686bfd9e3e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-4f8d2ec3770d01975ef251686bfd9e3e_b.jpg\"/></figure><p>输入的是含有多个帧的一段视频，但我们不要全部用到全部帧数</p><p>所以我们想提取关键帧，设定提取的关键帧为m，把一个长视频分成m个小视频。对于每个视频分开处理。</p><p>每一个action表示这些关键帧是要向左移动，向右移动或是保持当前位置。</p><p>设定迭代步数，当程序迭代步数达到设定值时，就停止。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-7934d99d75b81d282925b81a223acc55_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"330\" data-rawheight=\"183\" class=\"content_image\" width=\"330\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;330&#39; height=&#39;183&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"330\" data-rawheight=\"183\" class=\"content_image lazy\" width=\"330\" data-actualsrc=\"https://pic2.zhimg.com/v2-7934d99d75b81d282925b81a223acc55_b.jpg\"/></figure><p>2. GCNN</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b4772d08891d054265fa40037fb0111d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"414\" data-rawheight=\"411\" class=\"content_image\" width=\"414\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;414&#39; height=&#39;411&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"414\" data-rawheight=\"411\" class=\"content_image lazy\" width=\"414\" data-actualsrc=\"https://pic2.zhimg.com/v2-b4772d08891d054265fa40037fb0111d_b.jpg\"/></figure><p>把人体结构变成一个图结构。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-c7f1049804c8a471f0f6451f1234debb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"341\" data-rawheight=\"79\" class=\"content_image\" width=\"341\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;341&#39; height=&#39;79&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"341\" data-rawheight=\"79\" class=\"content_image lazy\" width=\"341\" data-actualsrc=\"https://pic4.zhimg.com/v2-c7f1049804c8a471f0f6451f1234debb_b.jpg\"/></figure><p>3 GCNN和FDNet的结合，渐进式的学习过程</p><p><br/>对于所有在训练集中基于骨骼的视频，我们首先均匀取样帧来获得在固定尺寸的序列。这些序列被用来训练GCNN来刻画在空间域中的关节依赖。之后，我们固定GCNN的参数来训练FDNet还有更新每一个在时间域上对每一个视频所选择的帧，这被用来改善GCNN。这两个模型互相促进对方，GCNN提供为FDNet提供奖赏，FDNet选择关键帧用来改善GCNN。GCNN越好，更准确的奖赏将会被提供。选择的帧质量越高，GCNN就可以更好的被改善。在测试时，每一个视频经过FDNet来产生它的具有信息帧的对应序列，然后最终将发送到GCNN来提供动作标签。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-0e8ca97abb88228b88afb1868a592897_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"354\" data-rawheight=\"451\" class=\"content_image\" width=\"354\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;354&#39; height=&#39;451&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"354\" data-rawheight=\"451\" class=\"content_image lazy\" width=\"354\" data-actualsrc=\"https://pic4.zhimg.com/v2-0e8ca97abb88228b88afb1868a592897_b.jpg\"/></figure><p>结果</p><p>SYSU</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-eab24b44f065c574f5a7444513ed7639_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"346\" data-rawheight=\"207\" class=\"content_image\" width=\"346\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;346&#39; height=&#39;207&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"346\" data-rawheight=\"207\" class=\"content_image lazy\" width=\"346\" data-actualsrc=\"https://pic2.zhimg.com/v2-eab24b44f065c574f5a7444513ed7639_b.jpg\"/></figure><p>NTU</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-2f40a0e599940daa67784e0fcf11e18a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"340\" data-rawheight=\"380\" class=\"content_image\" width=\"340\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;340&#39; height=&#39;380&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"340\" data-rawheight=\"380\" class=\"content_image lazy\" width=\"340\" data-actualsrc=\"https://pic3.zhimg.com/v2-2f40a0e599940daa67784e0fcf11e18a_b.jpg\"/></figure><p>UT</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-e2204b92883c0f0f87ca22d2ec5597e1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"333\" data-rawheight=\"259\" class=\"content_image\" width=\"333\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;333&#39; height=&#39;259&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"333\" data-rawheight=\"259\" class=\"content_image lazy\" width=\"333\" data-actualsrc=\"https://pic2.zhimg.com/v2-e2204b92883c0f0f87ca22d2ec5597e1_b.jpg\"/></figure><h2><b><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_cvpr_2018/html/Zhou_MiCT_Mixed_3D2D_CVPR_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">MiCT: Mixed 3D/2D Convolutional Tube for Human Action Recognition</a></b></h2><p>Yizhou Zhou∗1 Xiaoyan Sun2 Zheng-Jun Zha1 Wenjun Zeng2</p><p><br/>1University of Science and Technology of China<br/><br/>2Microsoft Research Asia<br/></p><p>这是中科大和微软亚洲研究院的工作。</p><p>本文的贡献是提出了一个混合2D 与3D 的卷积来做动作识别。</p><p>视频中的人为动作是三维（3D）信号。<br/>最近的尝试使用3D卷积来做动作识别。虽然很有前景，但是3D 的表现甚至还不如2D的对应用于静止图像中的视觉识别。</p><p>原因就是高训练时空融合的复杂性和巨大的3D卷积的内存成本阻碍了当前的3D CNN，</p><p>但是通过输出逐层堆叠3D卷积更深层次的特征映射，对于高级任务又是很重要的。</p><p>他们因此提出了一种集成的混合卷积管（MiCT）具有3D卷积模块的2D CNN生成更深入，更丰富的功能图，同时减少训练每轮时空融合的复杂性。是一个新的端到端可训练深度3D网络，MiCTNet。</p><p>用了三个着名的基准数据集UCF101，Sport1M和HMDB-51。其中两个UCF101和HMDB-51能达到state of the art。</p><p>混合的结构如下：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-16f314ab891d02313a86258d0edd21b1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"580\" data-rawheight=\"286\" class=\"origin_image zh-lightbox-thumb\" width=\"580\" data-original=\"https://pic2.zhimg.com/v2-16f314ab891d02313a86258d0edd21b1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;580&#39; height=&#39;286&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"580\" data-rawheight=\"286\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"580\" data-original=\"https://pic2.zhimg.com/v2-16f314ab891d02313a86258d0edd21b1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-16f314ab891d02313a86258d0edd21b1_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>因此，他们首先提出3D/2D串联混合模块 （Concatenate Connection）。如图所示，在3D/2D串联模块中，在每个3D卷积之后串联一个深度2D CNN。通过3D/2D串联模块的使用，可以有效地增加3D CNN的深度，加强2D空域的学习能力，从而生成更深更强的3D特征，并使得3D CNN<b>可以充分利用在图像数据上预先训练的2D CNN模型</b>。在相关的网络设计中，提出相应地减少3D卷积的数量，从而进一步减小模型的大小，提升模型的效率。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-bb3ebada553c35f681f0cf6b5daad62e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1105\" data-rawheight=\"609\" class=\"origin_image zh-lightbox-thumb\" width=\"1105\" data-original=\"https://pic3.zhimg.com/v2-bb3ebada553c35f681f0cf6b5daad62e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1105&#39; height=&#39;609&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1105\" data-rawheight=\"609\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1105\" data-original=\"https://pic3.zhimg.com/v2-bb3ebada553c35f681f0cf6b5daad62e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-bb3ebada553c35f681f0cf6b5daad62e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>通过3D/2D串联模块的使用，会得到一个更深的3D卷积神经网络。然而，更深的CNN通常意味着更难的训练过程。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-7bcd1bda589233f03f3968e81757529c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"954\" data-rawheight=\"481\" class=\"origin_image zh-lightbox-thumb\" width=\"954\" data-original=\"https://pic1.zhimg.com/v2-7bcd1bda589233f03f3968e81757529c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;954&#39; height=&#39;481&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"954\" data-rawheight=\"481\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"954\" data-original=\"https://pic1.zhimg.com/v2-7bcd1bda589233f03f3968e81757529c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-7bcd1bda589233f03f3968e81757529c_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>简单的层数堆叠来加深网络可能造成梯度消失并导致更大的训练错误</b>。为了解决这个问题，我们提出<b>利用3D和2D特征图之间的相关性，让3D和2D卷积共享空间信息</b>，如图所示。 由于2D空间特征相对容易学习，我们可以利用2D卷积并通过残差学习的方式来促进3D特征的学习。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-8c63c93a6f9701c72d6fa3e370b1f6e9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2426\" data-rawheight=\"1275\" class=\"origin_image zh-lightbox-thumb\" width=\"2426\" data-original=\"https://pic2.zhimg.com/v2-8c63c93a6f9701c72d6fa3e370b1f6e9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2426&#39; height=&#39;1275&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2426\" data-rawheight=\"1275\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2426\" data-original=\"https://pic2.zhimg.com/v2-8c63c93a6f9701c72d6fa3e370b1f6e9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-8c63c93a6f9701c72d6fa3e370b1f6e9_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>用数学语言来描述的话，在t时刻的feature map表示为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-9e64a61ae28c04fad7669f8de9846dac_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"29\" data-rawheight=\"21\" class=\"content_image\" width=\"29\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;29&#39; height=&#39;21&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"29\" data-rawheight=\"21\" class=\"content_image lazy\" width=\"29\" data-actualsrc=\"https://pic1.zhimg.com/v2-9e64a61ae28c04fad7669f8de9846dac_b.png\"/></figure><p>，那么将</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-9e64a61ae28c04fad7669f8de9846dac_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"29\" data-rawheight=\"21\" class=\"content_image\" width=\"29\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;29&#39; height=&#39;21&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"29\" data-rawheight=\"21\" class=\"content_image lazy\" width=\"29\" data-actualsrc=\"https://pic1.zhimg.com/v2-9e64a61ae28c04fad7669f8de9846dac_b.png\"/></figure><p>可以表述为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-db2b2de6881783fdf9e96c23a202a529_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"111\" data-rawheight=\"56\" class=\"content_image\" width=\"111\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;111&#39; height=&#39;56&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"111\" data-rawheight=\"56\" class=\"content_image lazy\" width=\"111\" data-actualsrc=\"https://pic2.zhimg.com/v2-db2b2de6881783fdf9e96c23a202a529_b.png\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>其中</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-023f3286ece8cd21cfc3c77ac28b880c_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"126\" data-rawheight=\"20\" class=\"content_image\" width=\"126\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;126&#39; height=&#39;20&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"126\" data-rawheight=\"20\" class=\"content_image lazy\" width=\"126\" data-actualsrc=\"https://pic1.zhimg.com/v2-023f3286ece8cd21cfc3c77ac28b880c_b.png\"/></figure><p>是从t时刻到</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-cb4152efd890fca0f4e88dea95075d62_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"50\" data-rawheight=\"17\" class=\"content_image\" width=\"50\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;50&#39; height=&#39;17&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"50\" data-rawheight=\"17\" class=\"content_image lazy\" width=\"50\" data-actualsrc=\"https://pic3.zhimg.com/v2-cb4152efd890fca0f4e88dea95075d62_b.png\"/></figure><p>时刻的sliced tensor，上式不是最终版本，M其实只是spatio-temporal feature map的linear fused操作，最终版本是另外提出了一个函数</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-3b8fbe529a1ce5232fc3d7c4eb66a0e0_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"36\" data-rawheight=\"22\" class=\"content_image\" width=\"36\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;36&#39; height=&#39;22&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"36\" data-rawheight=\"22\" class=\"content_image lazy\" width=\"36\" data-actualsrc=\"https://pic1.zhimg.com/v2-3b8fbe529a1ce5232fc3d7c4eb66a0e0_b.png\"/></figure><p>，这个</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-4020f2d60d9ee623fa74dfda12dfc2b4_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"134\" data-rawheight=\"29\" class=\"content_image\" width=\"134\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;134&#39; height=&#39;29&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"134\" data-rawheight=\"29\" class=\"content_image lazy\" width=\"134\" data-actualsrc=\"https://pic1.zhimg.com/v2-4020f2d60d9ee623fa74dfda12dfc2b4_b.png\"/></figure><p>，而这里的H其实就是2D卷积，也就是说这个数学描述描述的就是上图所示的过程，先进行3D卷积，接着对feature map进行2D卷积。这个过程可以理解为：3D卷积聚合了时空两个维度的信息，之后如果像是3D CNN那样继续叠加3D卷积层的话，会极大增加计算复杂度，因此退而求其次，使用2D卷积继续提升feature map的abstract level，同时又不会增加太大的计算负担。</p><p>整体结构</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-58dafe0b2da34db6752bc295ef9322cf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1016\" data-rawheight=\"426\" class=\"origin_image zh-lightbox-thumb\" width=\"1016\" data-original=\"https://pic4.zhimg.com/v2-58dafe0b2da34db6752bc295ef9322cf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1016&#39; height=&#39;426&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1016\" data-rawheight=\"426\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1016\" data-original=\"https://pic4.zhimg.com/v2-58dafe0b2da34db6752bc295ef9322cf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-58dafe0b2da34db6752bc295ef9322cf_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>结果</p><p>鲁棒性好：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-35ab51013959eb56c8c859354ecdb43a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1989\" data-rawheight=\"817\" class=\"origin_image zh-lightbox-thumb\" width=\"1989\" data-original=\"https://pic3.zhimg.com/v2-35ab51013959eb56c8c859354ecdb43a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1989&#39; height=&#39;817&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1989\" data-rawheight=\"817\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1989\" data-original=\"https://pic3.zhimg.com/v2-35ab51013959eb56c8c859354ecdb43a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-35ab51013959eb56c8c859354ecdb43a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>对于一帧一帧的RGB，能最好</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-f4e8b7b9008d49ad969faea250fa59a3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"498\" data-rawheight=\"573\" class=\"origin_image zh-lightbox-thumb\" width=\"498\" data-original=\"https://pic4.zhimg.com/v2-f4e8b7b9008d49ad969faea250fa59a3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;498&#39; height=&#39;573&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"498\" data-rawheight=\"573\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"498\" data-original=\"https://pic4.zhimg.com/v2-f4e8b7b9008d49ad969faea250fa59a3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-f4e8b7b9008d49ad969faea250fa59a3_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>用video，不如TLE：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-c26bf9fc8c15121e997aff794db8a1eb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"493\" data-rawheight=\"425\" class=\"origin_image zh-lightbox-thumb\" width=\"493\" data-original=\"https://pic4.zhimg.com/v2-c26bf9fc8c15121e997aff794db8a1eb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;493&#39; height=&#39;425&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"493\" data-rawheight=\"425\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"493\" data-original=\"https://pic4.zhimg.com/v2-c26bf9fc8c15121e997aff794db8a1eb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-c26bf9fc8c15121e997aff794db8a1eb_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2><b><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_cvpr_2018/html/Sun_Optical_Flow_Guided_CVPR_2018_paper.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Optical Flow Guided Feature: A Fast and Robust Motion Representation for Video Action Recognition</a></b></h2><h3>Shuyang Sun1,2, Zhanghui Kuang2, Lu Sheng3, Wanli Ouyang1, Wei Zhang2</h3><p><br/>1The University of Sydney</p><p>2SenseTime Research</p><p>3The Chinese University of Hong Kong</p><p>欧阳万里等人的工作，很早就准备读这个文章了。</p><p>有代码：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/kevin-ssy/Optical-Flow-Guided-Feature\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/kevin-ssy/Op</span><span class=\"invisible\">tical-Flow-Guided-Feature</span><span class=\"ellipsis\"></span></a></p><p>caffe做的</p><p>简述</p><p>这篇文章最核心的思想是，用光流的特征来引导对RGB特征的注意和学习。Optical Flow guided Feature，OFF。</p><p>目前的双流网络Two-Stream在训练时其实还是比较麻烦，因为需要单独对视频提取光流图，然后送到网络的另一至进行训练；而且如果数据集很大的话，光流图和RGB图像合起来得有原视频数据大小的好几倍，也十分消耗硬盘空间。</p><p>实验显示，在UCF101上仅以RGB输入到OFF网络得到的acc达到93.3%，和two stream （RGB+optial flow）的acc相当，并且快15倍。</p><p>1. Optical Flow Guided Feature的来历<br/>OFF是从光流的定义得到的。光流的基本假设有3个： <br/>（1）相邻帧之间的亮度恒定 <br/>（2）相邻视频帧的取帧时间连续，即，相邻帧之间物体的运动比较“微小”； <br/>（3）保持空间一致性，即，同一子图像的像素点具有相同的运动 <br/>根据光流的微小运动和亮度恒定的假设，我们可以得到：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-94f4da7c67073babf9f990480529e4ea_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"354\" data-rawheight=\"50\" class=\"content_image\" width=\"354\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;354&#39; height=&#39;50&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"354\" data-rawheight=\"50\" class=\"content_image lazy\" width=\"354\" data-actualsrc=\"https://pic3.zhimg.com/v2-94f4da7c67073babf9f990480529e4ea_b.png\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>其中I(x,y,t)表示的是t时刻的一帧图像I上位置(x,y)的像素值。</p><p>将上述假设推广到特征层，即：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-f1361ee4fd5e725750d90eedfa8dfd27_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"355\" data-rawheight=\"43\" class=\"content_image\" width=\"355\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;355&#39; height=&#39;43&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"355\" data-rawheight=\"43\" class=\"content_image lazy\" width=\"355\" data-actualsrc=\"https://pic4.zhimg.com/v2-f1361ee4fd5e725750d90eedfa8dfd27_b.png\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>其中f(I;w)表示的是一帧图像I的特征提取function，w为参数。function可以是任何可微函数，在本文中我们使用的是CNN。根据光流的定义，上式中令p=(x,y,t)，得到（3）：</p><p>等式左右两边同时除以ΔtΔt，得到（4）：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-dc427922314dcb90428410b03f2b7597_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"353\" data-rawheight=\"146\" class=\"content_image\" width=\"353\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;353&#39; height=&#39;146&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"353\" data-rawheight=\"146\" class=\"content_image lazy\" width=\"353\" data-actualsrc=\"https://pic4.zhimg.com/v2-dc427922314dcb90428410b03f2b7597_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>这里的vx, vy相当于在feature点p上的二维速度矢量。</p><p>从上式中我们看到光流和feature flow成正交关系的，OFF将空间时间的信息编码成和特征光流(vx, vy)正交且互补的向量。<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-d9fdc7cc13e7ca7b3394dbb3727a9ff5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1149\" data-rawheight=\"934\" class=\"origin_image zh-lightbox-thumb\" width=\"1149\" data-original=\"https://pic2.zhimg.com/v2-d9fdc7cc13e7ca7b3394dbb3727a9ff5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1149&#39; height=&#39;934&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1149\" data-rawheight=\"934\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1149\" data-original=\"https://pic2.zhimg.com/v2-d9fdc7cc13e7ca7b3394dbb3727a9ff5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-d9fdc7cc13e7ca7b3394dbb3727a9ff5_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>网络<br/>由三个子网络组成，用于不同目的：: feature generation sub-network, OFF sub-network and classification sub-network。feature generation sub-network 使用常见的CNN结构生成基本特征。 在OFF sub-network 中，使用的是上一层的特征生成子网络得到的特征，以及然后堆叠几个残余块以获得更精细的特征。 前两个子网的特征由最后的分类子网使用用于获得动作识别结果。 如图3所示。 整个网络有3个OFF单位不同的尺度。</p><p>结果</p><p>速度体现</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-e7c957478e9e117d1afb6bc75d2f4423_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"372\" data-rawheight=\"314\" class=\"content_image\" width=\"372\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;372&#39; height=&#39;314&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"372\" data-rawheight=\"314\" class=\"content_image lazy\" width=\"372\" data-actualsrc=\"https://pic4.zhimg.com/v2-e7c957478e9e117d1afb6bc75d2f4423_b.jpg\"/></figure><p>两个数据集上的表现</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-77f6f6ba4c6587c017149c172302cb88_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"370\" data-rawheight=\"369\" class=\"content_image\" width=\"370\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;370&#39; height=&#39;369&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"370\" data-rawheight=\"369\" class=\"content_image lazy\" width=\"370\" data-actualsrc=\"https://pic1.zhimg.com/v2-77f6f6ba4c6587c017149c172302cb88_b.jpg\"/></figure><h2><b><a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_cvpr_2018/papers/Luvizon_2D3D_Pose_Estimation_CVPR_2018_paper.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">2D/3D Pose Estimation and Action Recognition using Multitask Deep Learning</a></b></h2><p>Diogo C. Luvizon1, David Picard1,2, Hedi Tabia1</p><p>1ETIS UMR 8051, Paris Seine University, ENSEA, CNRS, F-95000, Cergy, France</p><p>2Sorbonne Universite, CNRS, Laboratoire d’Informatique de Paris 6, LIP6, F-75005 Paris, France</p><p>巴黎塞纳大学的工作。</p><p>简述</p><p>这个工作的核心思想就是把姿态估计和人体识别联合起来做，会比分阶段来做好。</p><p>动作识别和人体姿势估计密切相关，但这两个问题通常作为文献中的不同任务来处理。</p><p>这项工作就提出了一个多任务框架，用于从静止图像和视频序列的人类动作识别中联合进行2D和3D姿态估计。</p><p>实验表明，可以使用单一架构以有效的方式解决这两个问题，并且仍然可以实现最先进的结果。</p><p>此外，他们证明从端到端的优化导致比分离学习更高的准确性。</p><p>验证了四个数据集（MPII，Human3.6M，PennAction和NTU）。</p><p>姿态估计的目标函数:</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-2a7db5373e7e26a452c9689aaea741a7_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"321\" data-rawheight=\"46\" class=\"content_image\" width=\"321\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;321&#39; height=&#39;46&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"321\" data-rawheight=\"46\" class=\"content_image lazy\" width=\"321\" data-actualsrc=\"https://pic4.zhimg.com/v2-2a7db5373e7e26a452c9689aaea741a7_b.png\"/></figure><p>用的是热图的方式来估计：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-18896523bb0a1ba63a3466090a7e1ffb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"341\" data-rawheight=\"298\" class=\"content_image\" width=\"341\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;341&#39; height=&#39;298&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"341\" data-rawheight=\"298\" class=\"content_image lazy\" width=\"341\" data-actualsrc=\"https://pic4.zhimg.com/v2-18896523bb0a1ba63a3466090a7e1ffb_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>整体的时序表征没有用到LSTM，而是CNN，结果也不错。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-34590146f9067f1067b34ef9ca448a83_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"329\" data-rawheight=\"211\" class=\"content_image\" width=\"329\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;329&#39; height=&#39;211&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"329\" data-rawheight=\"211\" class=\"content_image lazy\" width=\"329\" data-actualsrc=\"https://pic4.zhimg.com/v2-34590146f9067f1067b34ef9ca448a83_b.jpg\"/></figure><p>他们进行了大量实验在四个基准数据集上，结果很不错，这里主要看看NTU的表现：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-5aa7fbcc88358a54d8311bbc396e22b8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"344\" data-rawheight=\"442\" class=\"content_image\" width=\"344\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;344&#39; height=&#39;442&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"344\" data-rawheight=\"442\" class=\"content_image lazy\" width=\"344\" data-actualsrc=\"https://pic1.zhimg.com/v2-5aa7fbcc88358a54d8311bbc396e22b8_b.jpg\"/></figure><p>不同modality之间的比较：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-15be537e31af70e30d472e33785900a7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"346\" data-rawheight=\"514\" class=\"content_image\" width=\"346\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;346&#39; height=&#39;514&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"346\" data-rawheight=\"514\" class=\"content_image lazy\" width=\"346\" data-actualsrc=\"https://pic4.zhimg.com/v2-15be537e31af70e30d472e33785900a7_b.jpg\"/></figure><p>下一期持续更新 CVPR 2018的相关文章。</p><hr/><p>总结：</p><p>这次解读的几篇文章，一个是增强学习实现skeleton识别，重点是关键帧的提取，一个是3D 与2D 卷积的混合，一个是用光流来指导RGB特征的学习，一个是骨骼估计和预测的联合学习，所以动作识别的关键问题：</p><ol><li>关键帧的提取。</li><li>时序信息和2D的RGB的有效融合，几个大的组都在做这个，比如Luc van 的以及微软研究院的这个。</li></ol>", 
            "topic": [
                {
                    "tag": "计算机视觉", 
                    "tagLink": "https://api.zhihu.com/topics/19590195"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/43537456", 
            "userName": "iker peng", 
            "userLink": "https://www.zhihu.com/people/d2c038d7a07f7970c419f9ac781c332b", 
            "upvote": 0, 
            "title": "CMVS(1)--图像风格传输", 
            "content": "<p>文章代码连接如下：</p><p>Photorealistic image stylization：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/NVIDIA/FastPhotoStyle\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/NVIDIA/FastP</span><span class=\"invisible\">hotoStyle</span><span class=\"ellipsis\"></span></a></p><p class=\"ztext-empty-paragraph\"><br/></p><p>风格传输任务如下：输入一张内容图片以及一张风格图片，输出一张实现了风格迁移的图片。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-c782d8c2e8cb30dbae9e029cfe362c26_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"854\" data-rawheight=\"263\" class=\"origin_image zh-lightbox-thumb\" width=\"854\" data-original=\"https://pic3.zhimg.com/v2-c782d8c2e8cb30dbae9e029cfe362c26_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;854&#39; height=&#39;263&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"854\" data-rawheight=\"263\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"854\" data-original=\"https://pic3.zhimg.com/v2-c782d8c2e8cb30dbae9e029cfe362c26_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-c782d8c2e8cb30dbae9e029cfe362c26_b.jpg\"/></figure><p>如果你早期关注图像的风格传输这个领域，那你对于下面的这个结果一定不陌生。图中的</p><p>A表示的是输入的原始图像，其余图像的左下角表示的是想要学习到的风格，BCD当中的大图表示的就是最终的效果。这就是两三年前的效果，在当时还是引起了不小的轰动的。但是如果我们今天再来看这些结果感觉还是差的比较多的。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-b4b5a0a0b8b9fe3f528698bac561aae3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"749\" data-rawheight=\"552\" class=\"origin_image zh-lightbox-thumb\" width=\"749\" data-original=\"https://pic4.zhimg.com/v2-b4b5a0a0b8b9fe3f528698bac561aae3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;749&#39; height=&#39;552&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"749\" data-rawheight=\"552\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"749\" data-original=\"https://pic4.zhimg.com/v2-b4b5a0a0b8b9fe3f528698bac561aae3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-b4b5a0a0b8b9fe3f528698bac561aae3_b.jpg\"/></figure><p>如果我们用作者甚至前面几个任务当中的概念来解释这个问题，那就是并没有将内容和风格在隐空间当中分离开来，导致风格的传输不仅传递了风格，甚至将一些内容传递了过去。并且部分的掩盖了原来的内容。上面的效果感觉就像是一幅模糊不清的油画。当然，也不乏有很强的艺术气息。而这篇文章当中的风格传输被称作是：Photorealistic 的风格传输。也就是输出的效果更加的逼真，接近于真实的相片。那我们来看一些生成的结果：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-71c975a1f1b1be729da5856c98b5ed1b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"726\" data-rawheight=\"573\" class=\"origin_image zh-lightbox-thumb\" width=\"726\" data-original=\"https://pic4.zhimg.com/v2-71c975a1f1b1be729da5856c98b5ed1b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;726&#39; height=&#39;573&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"726\" data-rawheight=\"573\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"726\" data-original=\"https://pic4.zhimg.com/v2-71c975a1f1b1be729da5856c98b5ed1b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-71c975a1f1b1be729da5856c98b5ed1b_b.jpg\"/></figure><p>你被吓倒了吗？ 我是真的被吓到了。这里的第一列表示的是想要得到的风格，第二列表示的是内容图片。而最右边的那一栏就是风格传输的结果。真是被惊艳到了。感慨风格传输居然到了这种程度了。那我们还是来看一下是怎么实现的。</p><p>最开始的算法采用的是Gram矩阵来代替风格的方式，这种方式显然是比较粗暴的。而后来出现了一些通过匹配输出和输入的梯度来微调输出的结果。而这一篇文章是这样做的。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-c6a884fd59cfd43cc6eeba070a409b3b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"824\" data-rawheight=\"307\" class=\"origin_image zh-lightbox-thumb\" width=\"824\" data-original=\"https://pic4.zhimg.com/v2-c6a884fd59cfd43cc6eeba070a409b3b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;824&#39; height=&#39;307&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"824\" data-rawheight=\"307\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"824\" data-original=\"https://pic4.zhimg.com/v2-c6a884fd59cfd43cc6eeba070a409b3b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-c6a884fd59cfd43cc6eeba070a409b3b_b.jpg\"/></figure><p>文章主要的思想是这样的：首先，训练一个自编码器（Auto-Encoder，AE）实现图像的重构；其次，插入两个投影方程到网络的中间（bottleneck）来完成这个风格转换的任务；最终，再对输出的结果进行平滑处理。其中，AE的编码器E采用VGG-19网络，并且固定网络参数；解码器D为E网络的对称结构，网络参数待训练。在这里，整个Auto-Encoder网络实现图像layer级别的重构；投影方程的插入约束图像的风格和输入的风格图像接近。实际上本文就是从NIPS2017的文章WCT上面改进的。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ed30cedd60dd9dcdb153e5346f6a7bb7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"551\" data-rawheight=\"76\" class=\"origin_image zh-lightbox-thumb\" width=\"551\" data-original=\"https://pic4.zhimg.com/v2-ed30cedd60dd9dcdb153e5346f6a7bb7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;551&#39; height=&#39;76&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"551\" data-rawheight=\"76\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"551\" data-original=\"https://pic4.zhimg.com/v2-ed30cedd60dd9dcdb153e5346f6a7bb7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-ed30cedd60dd9dcdb153e5346f6a7bb7_b.jpg\"/></figure><p><b>具体先来看一下WCT算法。</b></p><p><b>网络重构部分：</b>首先，保证重构的图像接近于输入图像；其次，在各个网络层也约束特征重构的质量。<br/><b>插入投影方程：</b>投影方程的插入被分解为两个部分：白化（Whitening）以及着色（Coloring）。白化操作主要是抽出图片的主题内容部分，着色部分将风格部分的特点转移给图片。举例如下：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-7c1ad41b9b82758aade63c12c119160c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"855\" data-rawheight=\"217\" class=\"origin_image zh-lightbox-thumb\" width=\"855\" data-original=\"https://pic1.zhimg.com/v2-7c1ad41b9b82758aade63c12c119160c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;855&#39; height=&#39;217&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"855\" data-rawheight=\"217\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"855\" data-original=\"https://pic1.zhimg.com/v2-7c1ad41b9b82758aade63c12c119160c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-7c1ad41b9b82758aade63c12c119160c_b.jpg\"/></figure><p>具体这两个部分是通过图像计算协方差矩阵，然后进行特征分解得到的。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-ce0031173dcdc4a54ffa0be02e69324a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"807\" data-rawheight=\"514\" class=\"origin_image zh-lightbox-thumb\" width=\"807\" data-original=\"https://pic3.zhimg.com/v2-ce0031173dcdc4a54ffa0be02e69324a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;807&#39; height=&#39;514&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"807\" data-rawheight=\"514\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"807\" data-original=\"https://pic3.zhimg.com/v2-ce0031173dcdc4a54ffa0be02e69324a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-ce0031173dcdc4a54ffa0be02e69324a_b.jpg\"/></figure><p>在WCT中，先后输入风格图片Is以及内容图片Ic，通过编码器E得到对应的特征Hs和Hc。然后对该特征图分别都求解协方差矩阵；然后，将它们进行矩阵分解，构造两个分别和内容以及风格相关的矩阵Ps 和Pc，</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-16bfdaf0bb31db676c52072a5b3c025b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"369\" data-rawheight=\"50\" class=\"content_image\" width=\"369\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;369&#39; height=&#39;50&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"369\" data-rawheight=\"50\" class=\"content_image lazy\" width=\"369\" data-actualsrc=\"https://pic4.zhimg.com/v2-16bfdaf0bb31db676c52072a5b3c025b_b.jpg\"/></figure><p>，其中的E 表示的就是分别通过</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-5e6f4c9bebc61be731b56d92e3255f26_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"211\" data-rawheight=\"45\" class=\"content_image\" width=\"211\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;211&#39; height=&#39;45&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"211\" data-rawheight=\"45\" class=\"content_image lazy\" width=\"211\" data-actualsrc=\"https://pic3.zhimg.com/v2-5e6f4c9bebc61be731b56d92e3255f26_b.jpg\"/></figure><p>得到的特征向量构成的矩阵；分别对应上述的白化以及着色；然后对内容进行转换。转换后的结果Hcs，</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-cf3a0d347205f7a52a91499bc86504f3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"153\" data-rawheight=\"38\" class=\"content_image\" width=\"153\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;153&#39; height=&#39;38&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"153\" data-rawheight=\"38\" class=\"content_image lazy\" width=\"153\" data-actualsrc=\"https://pic4.zhimg.com/v2-cf3a0d347205f7a52a91499bc86504f3_b.jpg\"/></figure><p>最后，通过解码器D对Hcs 进行重构。</p><p>如上的操作一方面保留了输入图像的内容，另一方面保证了图像的的协方差有如下的关系。因为，Hc先和Hc转置作用得到单位向量。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-4ea5efbac29ab939c872504fca3ea36b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"350\" data-rawheight=\"60\" class=\"content_image\" width=\"350\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;350&#39; height=&#39;60&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"350\" data-rawheight=\"60\" class=\"content_image lazy\" width=\"350\" data-actualsrc=\"https://pic4.zhimg.com/v2-4ea5efbac29ab939c872504fca3ea36b_b.jpg\"/></figure><p>在继承了WCT的算法的基础上，本文还对风格的转换的输出结果<b>进行了平滑</b>，使得生成的图片更加的真实。平滑采取的策略主要是以下的两个方面：1. 保证领域内具有相同内容的像素点保持同样的风格；2.  平滑后的结果和风格转化后的结果不能相差太大。</p><p>具体看看是如何从这两个方面进行平滑。首先，将输出图像的每一个像素看作是Graph当中的一个node，然后构造一个相似度矩阵。假设每一张图存在N个像素点，那么这个矩阵W的大小就是 N*N，而其中的每一个元素为：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-93e59d8291a486a7595a8bb0b6dee405_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"430\" data-rawheight=\"94\" class=\"origin_image zh-lightbox-thumb\" width=\"430\" data-original=\"https://pic2.zhimg.com/v2-93e59d8291a486a7595a8bb0b6dee405_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;430&#39; height=&#39;94&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"430\" data-rawheight=\"94\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"430\" data-original=\"https://pic2.zhimg.com/v2-93e59d8291a486a7595a8bb0b6dee405_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-93e59d8291a486a7595a8bb0b6dee405_b.jpg\"/></figure><p>其中的Ij 表示的j像素点的RGB值。那么为了约束领域内的像素点具有同等的风格，采用如下优化函数：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-66c0df89139bff56afc8ed25e82c4f9d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"654\" data-rawheight=\"97\" class=\"origin_image zh-lightbox-thumb\" width=\"654\" data-original=\"https://pic2.zhimg.com/v2-66c0df89139bff56afc8ed25e82c4f9d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;654&#39; height=&#39;97&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"654\" data-rawheight=\"97\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"654\" data-original=\"https://pic2.zhimg.com/v2-66c0df89139bff56afc8ed25e82c4f9d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-66c0df89139bff56afc8ed25e82c4f9d_b.jpg\"/></figure><p>其中 r表示最终的输出像素值，y表示的是上述得到的风格转换的结果。</p><p>这样做的目的是为了保证最终的平滑结果和上个阶段的输出风格结果接近，同时也保证相邻的像素点能够接近。那么直接通过graph-based ranking 的算法对这个结果进行求解：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-8cd6ea94e5e0eaddc9cf426c4fc2bd0b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"509\" data-rawheight=\"65\" class=\"origin_image zh-lightbox-thumb\" width=\"509\" data-original=\"https://pic4.zhimg.com/v2-8cd6ea94e5e0eaddc9cf426c4fc2bd0b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;509&#39; height=&#39;65&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"509\" data-rawheight=\"65\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"509\" data-original=\"https://pic4.zhimg.com/v2-8cd6ea94e5e0eaddc9cf426c4fc2bd0b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-8cd6ea94e5e0eaddc9cf426c4fc2bd0b_b.jpg\"/></figure><p>其中的 S : </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-b4743b931edb2fde32ce221994081bcf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"217\" data-rawheight=\"40\" class=\"content_image\" width=\"217\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;217&#39; height=&#39;40&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"217\" data-rawheight=\"40\" class=\"content_image lazy\" width=\"217\" data-actualsrc=\"https://pic4.zhimg.com/v2-b4743b931edb2fde32ce221994081bcf_b.jpg\"/></figure><p>. 其中R就是最终输出的图像的像素。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>以上，供大家复习~</p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "计算机视觉", 
                    "tagLink": "https://api.zhihu.com/topics/19590195"
                }, 
                {
                    "tag": "人工智能", 
                    "tagLink": "https://api.zhihu.com/topics/19551275"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/43480539", 
            "userName": "Fisher Yu余梓彤", 
            "userLink": "https://www.zhihu.com/people/60d41039d82337a73fa807d3494319c9", 
            "upvote": 186, 
            "title": "活体检测Face Anti-spoofing综述", 
            "content": "<p><b>1. 什么是活体检测？</b></p><p>        --&gt;  判断捕捉到的人脸是真实人脸，还是伪造的人脸攻击（如：彩色纸张打印人脸图，电子设备屏幕中的人脸数字图像 以及 面具 等）</p><p><b>2. 为什么需要活体检测？</b></p><p>        --&gt; 在金融支付，门禁等应用场景，活体检测一般是嵌套在人脸检测与人脸识别or验证中的模块，用来验证是否用户真实本人</p><p><b>3. 活体检测对应的计算机视觉问题：</b></p><p>        --&gt; 就是分类问题，可看成二分类（真 or 假）；也可看成多分类（真人，纸张攻击，屏幕攻击，面具攻击）</p><hr/><h2><b>Anti-spoofing 1.0 时代</b></h2><p>从早期 handcrafted 特征的传统方法说起，目标很明确，就是找到活体与非活体攻击的difference，然后根据这些差异来设计特征，最后送给分类器去决策。</p><p>那么问题来了，活体与非活体有哪些差异？</p><ul><li>颜色纹理</li><li>非刚性运动变形</li><li>材料 （皮肤，纸质，镜面）</li><li>图像or视频质量 </li></ul><p>所以这段时期的文章都是很有针对性地设计特征，列举几篇比较重要的：</p><p><b>[1] Image Distortion Analysis, 2015</b></p><p>如下图，单帧输入的方法，设计了 镜面反射+图像质量失真+颜色 等统计量特征，合并后直接送SVM进行二分类。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-6be35e04329b55c2039674978292bd1d_b.jpg\" data-size=\"normal\" data-rawwidth=\"1272\" data-rawheight=\"519\" class=\"origin_image zh-lightbox-thumb\" width=\"1272\" data-original=\"https://pic2.zhimg.com/v2-6be35e04329b55c2039674978292bd1d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1272&#39; height=&#39;519&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1272\" data-rawheight=\"519\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1272\" data-original=\"https://pic2.zhimg.com/v2-6be35e04329b55c2039674978292bd1d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-6be35e04329b55c2039674978292bd1d_b.jpg\"/><figcaption>Image Distortion Analysis, 2015</figcaption></figure><p>Cons: 对于高清彩色打印的纸张 or 高清录制视频，质量失真不严重时，难区分开</p><p><b>[2] Colour Texture, 2016</b> </p><p>Oulu CMVS组的产物，算是传统方法中的战斗机，特别简洁实用，Matlab代码（课题组官网有），很适合搞成C++部署到门禁系统。</p><p>原理：活体与非活体，在RGB空间里比较难区分，但在其他颜色空间里的纹理有明显差异</p><p>算法：HSV空间人脸多级LBP特征 + YCbCr空间人脸LPQ特征  （后在17年的paper拓展成SURF特征）</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-fc257d3fdfaa437b6320667ba3ee0c6d_b.jpg\" data-size=\"normal\" data-rawwidth=\"1414\" data-rawheight=\"489\" class=\"origin_image zh-lightbox-thumb\" width=\"1414\" data-original=\"https://pic2.zhimg.com/v2-fc257d3fdfaa437b6320667ba3ee0c6d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1414&#39; height=&#39;489&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1414\" data-rawheight=\"489\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1414\" data-original=\"https://pic2.zhimg.com/v2-fc257d3fdfaa437b6320667ba3ee0c6d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-fc257d3fdfaa437b6320667ba3ee0c6d_b.jpg\"/><figcaption>Colour Texture, 2016</figcaption></figure><p>Pros: 算法简洁高效易部署；也证明了活体与非活体在 HSV等其他空间也是 discriminative，故后续深度学习方法有将HSV等channel也作为输入来提升性能。</p><p><b>[3]  Motion mag.-HOOF + LBP-TOP, 2014</b></p><p><b>[4]  DMD + LBP, 2015</b></p><p>前面说的都是单帧方法，这两篇文章输入的是连续多帧人脸图；</p><p>主要通过捕获活体与非活体微动作之间的差异来设计特征。</p><p>一个是先通过运动放大来增强脸部微动作， 然后提取方向光流直方图HOOF + 动态纹理LBP-TOP 特征；一个是通过动态模式分解DMD，得到最大运动能量的子空间图，再分析纹理。</p><p>PS：这个 motion magnification 的预处理很差劲，加入了很多其他频段噪声（18年新出了一篇用 Deep learning 来搞 Motion mag. 看起来效果挺好，可以尝试用那个来做运动增强，再来光流orDMD）</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-c765652dfa0b23577cfc48f7f84aaa93_b.jpg\" data-size=\"normal\" data-rawwidth=\"1365\" data-rawheight=\"521\" class=\"origin_image zh-lightbox-thumb\" width=\"1365\" data-original=\"https://pic4.zhimg.com/v2-c765652dfa0b23577cfc48f7f84aaa93_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1365&#39; height=&#39;521&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1365\" data-rawheight=\"521\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1365\" data-original=\"https://pic4.zhimg.com/v2-c765652dfa0b23577cfc48f7f84aaa93_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-c765652dfa0b23577cfc48f7f84aaa93_b.jpg\"/><figcaption>Motion mag.-HOOF + LBP-TOP, 2014</figcaption></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-1fc6c5b9e1252585022b65880956dc3a_b.jpg\" data-size=\"normal\" data-rawwidth=\"625\" data-rawheight=\"386\" class=\"origin_image zh-lightbox-thumb\" width=\"625\" data-original=\"https://pic3.zhimg.com/v2-1fc6c5b9e1252585022b65880956dc3a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;625&#39; height=&#39;386&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"625\" data-rawheight=\"386\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"625\" data-original=\"https://pic3.zhimg.com/v2-1fc6c5b9e1252585022b65880956dc3a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-1fc6c5b9e1252585022b65880956dc3a_b.jpg\"/><figcaption>DMD + LBP, 2015</figcaption></figure><p>Cons: 基于Motion的方法，对于 仿人脸wrapped纸张抖动 和 视频攻击，效果不好；因为它假定了活体与非活体之间的非刚性运动有明显的区别，但其实这种微动作挺难描述与学习~</p><p><b>[5]  Pulse + texture, 2016</b></p><p>第一个将 remote pluse 应用到活体检测中，多帧输入</p><p>（交代下背景：在CVPR2014，当时 Xiaobai Li 已经提出了从人脸视频里测量心率的方法）</p><p>算法流程： </p><p>       1. 通过 pluse 在频域上分布不同先区分 活体 or 照片攻击 （因为照片中的人脸提取的心率分布不同）</p><p>       2. 若判别1结果是活体，再 cascade 一个 纹理LBP 分类器，来区分 活体 or 屏幕攻击（因为屏幕视频中人脸心率分布与活体相近）</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-59ea4618559c07bdc97260c2eba7a48c_b.jpg\" data-size=\"normal\" data-rawwidth=\"650\" data-rawheight=\"203\" class=\"origin_image zh-lightbox-thumb\" width=\"650\" data-original=\"https://pic1.zhimg.com/v2-59ea4618559c07bdc97260c2eba7a48c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;650&#39; height=&#39;203&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"650\" data-rawheight=\"203\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"650\" data-original=\"https://pic1.zhimg.com/v2-59ea4618559c07bdc97260c2eba7a48c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-59ea4618559c07bdc97260c2eba7a48c_b.jpg\"/><figcaption>Pulse + texture, 2016</figcaption></figure><p>Pros: 从学术界来说，引入了 心理信号 这个新模态，很是进步；从工业界来看，如果不能一步到位，针对每种类型攻击，也可进行 Cascade 对应的特征及分类器的部署方式</p><p>Cons: 由于 remote heart rate 的算法本来鲁棒性也一般，故出来的 pulse-feature 的判别性能力很不能保证；再者屏幕video里的人脸视频出来的 pulse-feature 是否也有微小区别，还待验证~ </p><hr/><h2><b>Anti-spoofing 2.0 时代</b></h2><p>其实用 Deep learning 来做活体检测，从15年陆陆续续就有人在研究，但由于公开数据集样本太少，一直性能也超越不了传统方法：</p><p><b>[6]  CNN-LSTM, 2015</b></p><p>多帧方法，想通过 CNN-LSTM 来模拟传统方法 LBP-TOP，性能堪忧~</p><p><b>[7]  PatchNet pretrain，CNN finetune, 2017</b></p><p>单帧方法，通过人脸分块，pre-train 网络；然后再在 global 整个人脸图 fine-tune，作用不大</p><p><b>[8]  Patch and Depth-Based CNNs, 2017</b></p><p>第一个考虑把 人脸深度图 作为活体与非活体的差异特征，因为像屏幕中的人脸一般是平的，而纸张中的人脸就算扭曲，和真人人脸的立体分布也有差异；</p><p>就算用了很多 tricks 去 fusion，性能还是超越不了传统方法。。。 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-98186d06a1ebcd60b36cefbcea26499b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1244\" data-rawheight=\"319\" class=\"origin_image zh-lightbox-thumb\" width=\"1244\" data-original=\"https://pic4.zhimg.com/v2-98186d06a1ebcd60b36cefbcea26499b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1244&#39; height=&#39;319&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1244\" data-rawheight=\"319\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1244\" data-original=\"https://pic4.zhimg.com/v2-98186d06a1ebcd60b36cefbcea26499b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-98186d06a1ebcd60b36cefbcea26499b_b.jpg\"/></figure><p>------------------------------- 华丽的分割线 -----------------------------</p><p><b>[9]  Deep Pulse and Depth, 2018</b></p><p>发表在 CVPR2018 的文章，终于超越了传统方法性能。</p><p>文章[8]的同一组人，设计了深度框架 准端到端 地去预测 Pulse统计量 及 Depth map （这里说的“准”，就是最后没接分类器，直接通过样本 feature 的相似距离，阈值决策）</p><p>在文章中明确指明：</p><ul><li>过去方法把活体检测看成二分类问题，直接让DNN去学习，这样学出来的cues不够general 和 discriminative</li><li>将二分类问题换成带目标性地特征监督问题，即 回归出 <i>pulse 统计量 + 回归出 Depth map</i>，保证网络学习的就是这两种特征（哈哈，不排除假设学到了 color texture 在里面，黑箱网络这么聪明）</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-8e6d0311f08eddb996530021370a07e3_b.jpg\" data-size=\"normal\" data-rawwidth=\"1324\" data-rawheight=\"272\" class=\"origin_image zh-lightbox-thumb\" width=\"1324\" data-original=\"https://pic4.zhimg.com/v2-8e6d0311f08eddb996530021370a07e3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1324&#39; height=&#39;272&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1324\" data-rawheight=\"272\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1324\" data-original=\"https://pic4.zhimg.com/v2-8e6d0311f08eddb996530021370a07e3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-8e6d0311f08eddb996530021370a07e3_b.jpg\"/><figcaption>Deep Pulse and Depth, 2018</figcaption></figure><p>回归 Depth map，跟文章[8]中一致，就是通过 Landmark 然后 3DMMfitting 得到 人脸3D shape，然后再阈值化去背景，得到 depth map 的 groundtruth，最后和网络预测的 estimated depth map 有 L2 loss。</p><p>而文章亮点在于设计了 Non-rigid Registration Layer 来对齐各帧人脸的非刚性运动（如姿态，表情等），然后通过RNN更好地学到 temporal pulse 信息。</p><figure data-size=\"small\"><noscript><img src=\"https://pic2.zhimg.com/v2-83ac87895d7ee50434c3637e7c223add_b.jpg\" data-size=\"small\" data-rawwidth=\"612\" data-rawheight=\"384\" class=\"origin_image zh-lightbox-thumb\" width=\"612\" data-original=\"https://pic2.zhimg.com/v2-83ac87895d7ee50434c3637e7c223add_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;612&#39; height=&#39;384&#39;&gt;&lt;/svg&gt;\" data-size=\"small\" data-rawwidth=\"612\" data-rawheight=\"384\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"612\" data-original=\"https://pic2.zhimg.com/v2-83ac87895d7ee50434c3637e7c223add_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-83ac87895d7ee50434c3637e7c223add_b.jpg\"/><figcaption>Non-rigid Registration Layer</figcaption></figure><p>为什么需要这个对齐网络呢？我们来想想，在做运动识别任务时，只需简单把 sampling或者连续帧 合并起来喂进网络就行了，是假定相机是不动的，对象在运动；而文中需要对连续人脸帧进行pulse特征提取，主要对象是人脸上对应ROI在 temporal 上的 Intensity 变化，所以就需要把人脸当成是相机固定不动。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>[10]  Micro-texture + SSD or binocular depth , 2018</b></p><p>ArXiv 刚挂出不久的文章，最大的贡献是把 活体检测 直接放到 人脸检测（SSD，MTCNN等） 模块里作为一个类，即人脸检测出来的 bbox 里有 背景，真人人脸，假人脸 三类的置信度，这样可以在早期就过滤掉一部分非活体。</p><p>所以整个系统速度非常地快，很适合工业界部署~</p><p>至于后续手工设计的  SPMT feature 和 TFBD feature 比较复杂繁琐，分别是表征 micro-texture 和 stereo structure of face，有兴趣的同学可以去细看。 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-b4f7a276a3477dad64723bd546ec7813_b.jpg\" data-size=\"normal\" data-rawwidth=\"1041\" data-rawheight=\"431\" class=\"origin_image zh-lightbox-thumb\" width=\"1041\" data-original=\"https://pic4.zhimg.com/v2-b4f7a276a3477dad64723bd546ec7813_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1041&#39; height=&#39;431&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1041\" data-rawheight=\"431\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1041\" data-original=\"https://pic4.zhimg.com/v2-b4f7a276a3477dad64723bd546ec7813_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-b4f7a276a3477dad64723bd546ec7813_b.jpg\"/><figcaption>ture + SSD or binocular depth , 2018</figcaption></figure><p><b>[11]  De-Spoofing, ECCV2018</b></p><p>单帧方法，与Paper[8]和[9]一样，是MSU同一个课题组做的。</p><p>文章的idea很有趣，启发于图像去噪de-noise 和 图像去抖动 de-blur。无论是噪声图还是模糊图，都可看成是在原图上加噪声运算或者模糊运算（即下面的公式），而去噪和去抖动，就是估计噪声分布和模糊核，从而重构回原图。</p><p><img src=\"https://www.zhihu.com/equation?tex=x+%3D+%5Ctilde%7Bx%7D+%2B+N%28%5Ctilde%7Bx%7D%29\" alt=\"x = \\tilde{x} + N(\\tilde{x})\" eeimg=\"1\"/> </p><p>文中把活体人脸图看成是原图 <img src=\"https://www.zhihu.com/equation?tex=%5Ctilde%7Bx%7D\" alt=\"\\tilde{x}\" eeimg=\"1\"/> ，而非活体人脸图看成是加了噪声后失真的 <img src=\"https://www.zhihu.com/equation?tex=x\" alt=\"x\" eeimg=\"1\"/> ，故 task 就变成估计 Spoof noise <img src=\"https://www.zhihu.com/equation?tex=N%28%5Ctilde%7Bx%7D%29\" alt=\"N(\\tilde{x})\" eeimg=\"1\"/> ，然后用这个 Noise pattern feature 去分类决策。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-188a29e827236d636e6fbe5431475df4_b.jpg\" data-size=\"normal\" data-rawwidth=\"1005\" data-rawheight=\"285\" class=\"origin_image zh-lightbox-thumb\" width=\"1005\" data-original=\"https://pic1.zhimg.com/v2-188a29e827236d636e6fbe5431475df4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1005&#39; height=&#39;285&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1005\" data-rawheight=\"285\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1005\" data-original=\"https://pic1.zhimg.com/v2-188a29e827236d636e6fbe5431475df4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-188a29e827236d636e6fbe5431475df4_b.jpg\"/><figcaption>De-spoofing process</figcaption></figure><p>那问题来了，数据集没有像素级别一一对应的 groundtruth，也没有Spoof Noise模型的先验知识（如果有知道Noise模型，可以用Live Face来生成Spoofing Face），那拿什么来当groundtruth，怎么设计网络去估计 Spoofing noise 呢？</p><p>如一般Low-level image 任务一样，文中利用Encoder-decoder来得到 Spoof noise N，然后通过残差重构出 <img src=\"https://www.zhihu.com/equation?tex=%5Ctilde%7BI%7D+%3D+I+-+N%28%5Ctilde%7BI%7D%29\" alt=\"\\tilde{I} = I - N(\\tilde{I})\" eeimg=\"1\"/> ，这就是下图的DS Net。为了保证网络对于不同输入，学出来的Noise是有效的，根据先验知识设计了三个Loss来constrain：</p><p>Magnitude loss(当输入是Live face时，N尽量逼近0)；</p><p>Repetitive loss(Spooing face的Noise图在高频段有较大的峰值)；</p><p>0\\1Map Loss(让Real Face 的 deep feature map分布尽量逼近全0，而Spoofing face的 deep feature map 尽量逼近全1)</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-28d56381988f239f577d74fb751396d1_b.jpg\" data-size=\"normal\" data-rawwidth=\"795\" data-rawheight=\"386\" class=\"origin_image zh-lightbox-thumb\" width=\"795\" data-original=\"https://pic2.zhimg.com/v2-28d56381988f239f577d74fb751396d1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;795&#39; height=&#39;386&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"795\" data-rawheight=\"386\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"795\" data-original=\"https://pic2.zhimg.com/v2-28d56381988f239f577d74fb751396d1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-28d56381988f239f577d74fb751396d1_b.jpg\"/><figcaption>De-spoofing网络架构</figcaption></figure><p> 那网络右边的 VQ-Net 和 DQ-Net 又有什么作用呢？因为没有 Live face 的 Groundtruth，要保证重构出来的分布接近 Live face，作者用了对抗生成网络GAN (即 VQ-Net )去约束重构生成的 <img src=\"https://www.zhihu.com/equation?tex=%5Ctilde%7BI%7D\" alt=\"\\tilde{I}\" eeimg=\"1\"/> 与Live face分布尽量一致；而用了文章[8]中的 pre-trained Depth model 来保证 <img src=\"https://www.zhihu.com/equation?tex=%5Ctilde%7BI%7D\" alt=\"\\tilde{I}\" eeimg=\"1\"/> 的深度图与Live face的深度图尽量一致。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>Pros: 通过可视化最终让大众知道了 Spoofing Noise 是长什么样子的~</p><p>Cons: 在实际场景中难部署（该模型假定Spoofing Noise是 strongly 存在的，当实际场景中活体的人脸图质量并不是很高，而非活体攻击的质量相对高时，Spoofing noise走不通）</p><hr/><h2><b>后记：不同模态的相机输入对于活体检测的作用</b></h2><p><b>1.</b> <b>近红外NIR</b></p><p>由于NIR的光谱波段与可见光VIS不同，故真实人脸及非活体载体对于近红外波段的吸收和反射强度也不同，即也可通过近红外相机出来的图像来活体检测。从出来的图像来说，近红外图像对屏幕攻击的区分度较大，对高清彩色纸张打印的区分度较小。</p><p>从特征工程角度来说，方法无非也是提取NIR图中的光照纹理特征[12] 或者 远程人脸心率特征[13] 来进行。下图可见，上面两行是真实人脸图中人脸区域与背景区域的直方图分布，明显与下面两行的非活体图的分布不一致；而通过与文章[5]中一样的rPPG提取方法，在文章[]中说明其在NIR图像中出来的特征更加鲁棒~</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-dd9711e485d07d515c476caefc68c2d1_b.jpg\" data-size=\"normal\" data-rawwidth=\"508\" data-rawheight=\"332\" class=\"origin_image zh-lightbox-thumb\" width=\"508\" data-original=\"https://pic2.zhimg.com/v2-dd9711e485d07d515c476caefc68c2d1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;508&#39; height=&#39;332&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"508\" data-rawheight=\"332\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"508\" data-original=\"https://pic2.zhimg.com/v2-dd9711e485d07d515c476caefc68c2d1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-dd9711e485d07d515c476caefc68c2d1_b.jpg\"/><figcaption>NIR人脸区域与背景区域直方图[12]</figcaption></figure><p><b>2.</b> <b>结构光/ToF</b></p><p>由于结构光及ToF能在近距离里相对准确地进行3D人脸重构，即可得到人脸及背景的点云图及深度图，可作为精准活体检测（而不像单目RGB或双目RGB中仍需估计深度）。不过就是成本较高，看具体应用场景决定。</p><p><b>3.</b> <b>光场 Light field</b></p><p>光场相机具有光学显微镜头阵列，且由于光场能描述空间中任意一点向任意方向的光线强度，出来的raw光场照片及不同重聚焦的照片，都能用于活体检测：</p><p>3.1 raw光场照片及对应的子孔径照片[14] </p><p>如下图所示，对于真实人脸的脸颊边缘的微镜图像，其像素应该是带边缘梯度分布；而对应纸张打印或屏幕攻击，其边缘像素是随机均匀分布：</p><figure data-size=\"small\"><noscript><img src=\"https://pic3.zhimg.com/v2-cf1be55a8e7541b5ef9f09364ac7fb8e_b.jpg\" data-size=\"small\" data-rawwidth=\"611\" data-rawheight=\"440\" class=\"origin_image zh-lightbox-thumb\" width=\"611\" data-original=\"https://pic3.zhimg.com/v2-cf1be55a8e7541b5ef9f09364ac7fb8e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;611&#39; height=&#39;440&#39;&gt;&lt;/svg&gt;\" data-size=\"small\" data-rawwidth=\"611\" data-rawheight=\"440\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"611\" data-original=\"https://pic3.zhimg.com/v2-cf1be55a8e7541b5ef9f09364ac7fb8e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-cf1be55a8e7541b5ef9f09364ac7fb8e_b.jpg\"/><figcaption>光场相机图[14]</figcaption></figure><p>3.2 使用一次拍照的重聚焦图像[15]</p><p>原理是可以从两张重聚焦图像的差异中，估计出深度信息；从特征提取来说，真实人脸与非活体人脸的3D人脸模型不同，可提取差异图像中的 亮度分布特征+聚焦区域锐利程度特征+频谱直方图特征。</p><hr/><p class=\"ztext-empty-paragraph\"><br/></p><p>至此，Face anti-spoofing 的简单Survey已完毕~</p><p>毫无疑问，对于<b>学术界</b>，后续方向应该是用DL学习更精细的 人脸3D特征 和 人脸微变化微动作(Motion Spoofing Noise?) 表征；而也可探索活体检测与人脸检测及人脸识别之间更紧密的关系。 </p><p>对于<b>工业界</b>，最后倒数第二篇文章的启发很不错；更可借助近红外，结构光/ToF等硬件做到更精准。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>Reference:</b></p><p>[1] Di Wen, Hu Han, Anil K. Jain. Face Spoof Detection with Image Distortion Analysis. IEEE Transactions on Information Forensics and Security, 2015</p><p>[2] Zinelabidine Boulkenafet, Jukka Komulainen,  Abdenour Hadid. Face Spoofing Detection Using Colour Texture Analysis.  IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY,  2016</p><p>[3] Samarth Bharadwaj. Face Anti-spoofing via Motion Magnification and</p><p>Multifeature Videolet Aggregation,  2014</p><p>[4] Santosh Tirunagari, Norman Poh. Detection of Face Spoofing Using Visual Dynamics.  IEEE TRANS. ON INFORMATION FORENSICS AND SECURIT,  2015</p><p>[5] Xiaobai Li, , Guoying Zhao. Generalized face anti-spoofing by detecting pulse</p><p>from face videos,  2016 23rd ICPR</p><p>[6] Zhenqi Xu. Learning Temporal Features Using LSTM-CNN Architecture for Face Anti-spoofing,  2015 3rd IAPR</p><p>[7] Gustavo Botelho de Souza, On the Learning of Deep Local Features for</p><p>Robust Face Spoofing Detection,  2017</p><p>[8] Yousef Atoum, Xiaoming Liu. Face Anti-Spoofing Using Patch and Depth-Based CNNs,  2017</p><p>[9] Yaojie Liu, Amin Jourabloo, Xiaoming Liu, Learning Deep Models for Face Anti-Spoofing: Binary or Auxiliary Supervision ，CVPR2018</p><p>[10] Discriminative Representation Combinations for Accurate Face Spoofing Detection ”，2018 PR，上海交大</p><p>[11] Face De-Spoofing: Anti-Spoofing via Noise Modeling, ECCV2018</p><p>[12]Xudong Sun, Context Based Face Spoofing Detection Using Active Near-Infrared Images, ICPR 2016</p><p>[13]Javier Hernandez-Ortega, Time Analysis of Pulse-based Face Anti-Spoofing in Visible and NIR, CVPR2018 workshop</p><p>[14]Sooyeon Kim, Face Liveness Detection Using a Light Field Camera, 2014</p><p>[15]Xiaohua Xie, One-snapshot Face Anti-spoofing Using a Light Field Camera, 2017</p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "计算机视觉", 
                    "tagLink": "https://api.zhihu.com/topics/19590195"
                }, 
                {
                    "tag": "活体检测", 
                    "tagLink": "https://api.zhihu.com/topics/20095248"
                }
            ], 
            "comments": [
                {
                    "userName": "林成世", 
                    "userLink": "https://www.zhihu.com/people/7b674ce2e23d10e881d091d19586becb", 
                    "content": "不过现在依托红外线图像的识别就可以达到较好的识别率了吧……如果两个结合起来效果是不是更好", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "Fisher Yu余梓彤", 
                            "userLink": "https://www.zhihu.com/people/60d41039d82337a73fa807d3494319c9", 
                            "content": "是的，多模态更稳定……近红外图出来，基本可以区分屏幕攻击，但彩色纸张跟真人人脸差异不太大……", 
                            "likes": 0, 
                            "replyToAuthor": "林成世"
                        }
                    ]
                }, 
                {
                    "userName": "壁虎erxxxxx", 
                    "userLink": "https://www.zhihu.com/people/6fd77e7de8483e072c4ac0d371c6ac93", 
                    "content": "<p>学术文章的评论的点赞越来越少了整天都是明星动漫游戏的评论点赞高居不下</p>", 
                    "likes": 1, 
                    "childComments": []
                }, 
                {
                    "userName": "lucas", 
                    "userLink": "https://www.zhihu.com/people/4b07a726ea12f1271e15cef2cfaf620c", 
                    "content": "志伟要火了", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "clks-wzz", 
                    "userLink": "https://www.zhihu.com/people/1e4bc1374580b3c3201d56f479c62101", 
                    "content": "eccv还有一篇新的，都是msu 刘他们做的", 
                    "likes": 1, 
                    "childComments": [
                        {
                            "userName": "Fisher Yu余梓彤", 
                            "userLink": "https://www.zhihu.com/people/60d41039d82337a73fa807d3494319c9", 
                            "content": "<p>学习了老铁，\"Face De-Spoofing: Anti-Spoofing via Noise Modeling\", ECCV2018 ~ 有空我看看对比下蛤，谢谢</p>", 
                            "likes": 0, 
                            "replyToAuthor": "clks-wzz"
                        }, 
                        {
                            "userName": "小飞鱼也有大翅膀", 
                            "userLink": "https://www.zhihu.com/people/af75de6d28ec42c9ed6b0e1c5972d09a", 
                            "content": "<p>Eccv 那个Liu的文章有点诡异，总感觉是强项得出的结论</p>", 
                            "likes": 0, 
                            "replyToAuthor": "clks-wzz"
                        }
                    ]
                }, 
                {
                    "userName": "深度学习小白", 
                    "userLink": "https://www.zhihu.com/people/340379a5be7ed37fcd397baee0895349", 
                    "content": "<p>题主感觉利用optimal flow的方法怎么样呀？感觉这种方法对于应对非3D攻击还是很有效果的</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "Fisher Yu余梓彤", 
                            "userLink": "https://www.zhihu.com/people/60d41039d82337a73fa807d3494319c9", 
                            "content": "<p>你说的是多帧的光流输入？其实在文献[3]中的HOOF已经有类似思想在里面，效果一般般~~不过可以考虑当成一个新模态，放进深度学习框架里，可能提升性能</p>", 
                            "likes": 0, 
                            "replyToAuthor": "深度学习小白"
                        }, 
                        {
                            "userName": "林远", 
                            "userLink": "https://www.zhihu.com/people/9e352d6ad98d498338702d4ced572bdd", 
                            "content": "<p>真没什么用，抖两下纸张就过关了</p>", 
                            "likes": 0, 
                            "replyToAuthor": "深度学习小白"
                        }
                    ]
                }, 
                {
                    "userName": "Fei Long", 
                    "userLink": "https://www.zhihu.com/people/c7ab8509ee741a0a4de49e852d861752", 
                    "content": "<p>最近小米6上了MIUI10，支持人脸解锁，支持活体检测，可以防范照片和视频攻击。这应该用的是face++的防范。请问，他们是怎么做的？</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "Fisher Yu余梓彤", 
                            "userLink": "https://www.zhihu.com/people/60d41039d82337a73fa807d3494319c9", 
                            "content": "<p>个人觉得是单帧的传统方法与深度学习方法的融合出score~当然里面也可以搞很多trick，比如检测到边框啥的</p>", 
                            "likes": 0, 
                            "replyToAuthor": "Fei Long"
                        }
                    ]
                }, 
                {
                    "userName": "于丽", 
                    "userLink": "https://www.zhihu.com/people/a1237ecdbf47108ac30f7b493197715c", 
                    "content": "<p>您好，我是刚接触这一领域，想实现一下[2] Colour Texture, 2016的代码，请问在哪里可以获取相关代码呢？谢谢了</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "Fisher Yu余梓彤", 
                            "userLink": "https://www.zhihu.com/people/60d41039d82337a73fa807d3494319c9", 
                            "content": "在oulu大学cmvs主页有matlab代码", 
                            "likes": 0, 
                            "replyToAuthor": "于丽"
                        }, 
                        {
                            "userName": "于丽", 
                            "userLink": "https://www.zhihu.com/people/a1237ecdbf47108ac30f7b493197715c", 
                            "content": "<p>非常感谢，请问是这个<a href=\"http://link.zhihu.com/?target=http%3A//www.oulu.fi/cmvs/node/33019\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">oulu.fi/cmvs/node/33019</span><span class=\"invisible\"></span></a>链接吗？是没有完整代码，需要自己整合的？</p>", 
                            "likes": 0, 
                            "replyToAuthor": "Fisher Yu余梓彤"
                        }
                    ]
                }, 
                {
                    "userName": "西伯利亚寒流", 
                    "userLink": "https://www.zhihu.com/people/1a311d176a75dab57ece8bb534fc48cf", 
                    "content": "<p>收藏了，谢谢作者！</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "hc0704", 
                    "userLink": "https://www.zhihu.com/people/db7a30817a58b907df0e9661692e7b67", 
                    "content": "感谢大神分享，看完对活体检测方法的发展脉络清晰了不少，现在刚开始做单目活体这块，也尝试过里面文章中的一些方法，但是实际环境下，或者屏幕翻拍和打印照片一混合效果就很差了，现在考虑混合特征和级联方式检测", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "Fisher Yu余梓彤", 
                            "userLink": "https://www.zhihu.com/people/60d41039d82337a73fa807d3494319c9", 
                            "content": "如果不是要求很实时的话，可以考虑几样特征融合去部署……比如传统+DL，或者单帧加多帧", 
                            "likes": 0, 
                            "replyToAuthor": "hc0704"
                        }
                    ]
                }, 
                {
                    "userName": "Alfred", 
                    "userLink": "https://www.zhihu.com/people/6ec65421264216c9cf25ff34b3334e7d", 
                    "content": "<p>现在对于图片，视频翻拍能做到99%的防御吗？</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "Fisher Yu余梓彤", 
                            "userLink": "https://www.zhihu.com/people/60d41039d82337a73fa807d3494319c9", 
                            "content": "直接用RGB camera比较困难", 
                            "likes": 0, 
                            "replyToAuthor": "Alfred"
                        }
                    ]
                }, 
                {
                    "userName": "T-饼", 
                    "userLink": "https://www.zhihu.com/people/ba3b7243df8f724b22ea20b3ab349ccf", 
                    "content": "<p>基于近红外的活体检测论文还有吗，比如用深度学习的？</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "Fisher Yu余梓彤", 
                            "userLink": "https://www.zhihu.com/people/60d41039d82337a73fa807d3494319c9", 
                            "content": "去谷歌搜搜咯~红外的话难度低很多，收集部分数据，直接自己跑个模型试试就好", 
                            "likes": 0, 
                            "replyToAuthor": "T-饼"
                        }, 
                        {
                            "userName": "T-饼", 
                            "userLink": "https://www.zhihu.com/people/ba3b7243df8f724b22ea20b3ab349ccf", 
                            "content": "<p>楼主《Face Spoofing Detection Using Colour Texture Analysis》这篇文章你有吗?有的话能不能给我发一份？</p>", 
                            "likes": 0, 
                            "replyToAuthor": "Fisher Yu余梓彤"
                        }
                    ]
                }, 
                {
                    "userName": "深度学习小白", 
                    "userLink": "https://www.zhihu.com/people/340379a5be7ed37fcd397baee0895349", 
                    "content": "不知道楼主注意了没有，rPPG那篇文章的backbone中提到有一个resizing layers，这个是怎么实现的呀", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "Fisher Yu余梓彤", 
                            "userLink": "https://www.zhihu.com/people/60d41039d82337a73fa807d3494319c9", 
                            "content": "自由发挥阿，stride下采样也行，pooling也行，反正饭同维度concat而已", 
                            "likes": 0, 
                            "replyToAuthor": "深度学习小白"
                        }
                    ]
                }, 
                {
                    "userName": "豫章王子", 
                    "userLink": "https://www.zhihu.com/people/d5e8b39d198faf64ff5c444c1347140c", 
                    "content": "<p>你好，我正好对这方面感兴趣，请问有哪些第三方开源库包吗，跪求。QQ26128199541</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "取名字好麻烦", 
                    "userLink": "https://www.zhihu.com/people/840f0a02e03e56f79dfc1a4d412c2537", 
                    "content": "<p>请问一下目前还有对于3D的antispoofing请问大佬你觉得2D和3D在antispoofing上有什么区别吗（刚接触这个范围不是很懂怎么进行实验的）</p><a class=\"comment_sticker\" href=\"https://pic1.zhimg.com/v2-12562ad40366818a1ea39bcecb2599a0.gif\" data-width=\"\" data-height=\"\">[害羞]</a>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "王光成", 
                    "userLink": "https://www.zhihu.com/people/b962ef479d97a11530991c0e5d119e91", 
                    "content": "<p>您好，请问您有[1] Di Wen, Hu Han, Anil K. Jain. Face Spoof Detection with Image Distortion Analysis. IEEE Transactions on Information Forensics and Security, 2015。[2] Xiaobai Li, , Guoying Zhao. Generalized face anti-spoofing by detecting pulse from face videos, 2016 23rd ICPR. 这两篇文章的代码吗？</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "知乎用户", 
                    "userLink": "https://www.zhihu.com/people/0", 
                    "content": "<p>都是face anti-spoofing领域的专家, 不如微信建个群讨论技术细节吧！kim_young</p><a class=\"comment_sticker\" href=\"https://pic4.zhimg.com/v2-fa3cb6bc9ec57da84ab53a60f48d0c6f.gif\" data-width=\"\" data-height=\"\">[棒]</a>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "嘎嘎嘎", 
                            "userLink": "https://www.zhihu.com/people/7c7ffb4c75e472c310bd0c8cc25214d5", 
                            "content": "有没有群呢", 
                            "likes": 0, 
                            "replyToAuthor": "知乎用户"
                        }
                    ]
                }
            ]
        }
    ], 
    "url": "https://zhuanlan.zhihu.com/AI-cmvs"
}
