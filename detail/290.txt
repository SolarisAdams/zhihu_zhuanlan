{
    "title": "RDS on Kubernetes", 
    "description": "同步博客：http://littledriver.net/", 
    "followers": [
        "https://www.zhihu.com/people/leng-se-diao-de-xia-tian", 
        "https://www.zhihu.com/people/9527-polar-36", 
        "https://www.zhihu.com/people/mu-mu-31-13-6", 
        "https://www.zhihu.com/people/yc-liu-97", 
        "https://www.zhihu.com/people/maxwunj", 
        "https://www.zhihu.com/people/jiang-da-ge-18", 
        "https://www.zhihu.com/people/wu-kai-sheng", 
        "https://www.zhihu.com/people/ibrother", 
        "https://www.zhihu.com/people/ssshhh-58", 
        "https://www.zhihu.com/people/wsjhk", 
        "https://www.zhihu.com/people/ShawnZhong", 
        "https://www.zhihu.com/people/wang-li-qun-94", 
        "https://www.zhihu.com/people/wang-yin-jie", 
        "https://www.zhihu.com/people/hong-feng-85", 
        "https://www.zhihu.com/people/wu-qing-man", 
        "https://www.zhihu.com/people/fei007", 
        "https://www.zhihu.com/people/beer2100", 
        "https://www.zhihu.com/people/aman13", 
        "https://www.zhihu.com/people/jiahut", 
        "https://www.zhihu.com/people/kinglong-29", 
        "https://www.zhihu.com/people/crhelong", 
        "https://www.zhihu.com/people/lao-ke-xiao-hao-1", 
        "https://www.zhihu.com/people/yang-yong-peng-25", 
        "https://www.zhihu.com/people/li-guo-45-9", 
        "https://www.zhihu.com/people/ning-rain", 
        "https://www.zhihu.com/people/overflowtian", 
        "https://www.zhihu.com/people/yan-wei-33-81", 
        "https://www.zhihu.com/people/lu-bin-77-68", 
        "https://www.zhihu.com/people/jaycelau", 
        "https://www.zhihu.com/people/ganlu-jiang", 
        "https://www.zhihu.com/people/cui-xiao-chen-82", 
        "https://www.zhihu.com/people/ampxe", 
        "https://www.zhihu.com/people/mathdoge", 
        "https://www.zhihu.com/people/zhao-kai-82-4", 
        "https://www.zhihu.com/people/iosdevl", 
        "https://www.zhihu.com/people/magictour", 
        "https://www.zhihu.com/people/EvinTsai", 
        "https://www.zhihu.com/people/deardrops", 
        "https://www.zhihu.com/people/ju-shang-38", 
        "https://www.zhihu.com/people/benying", 
        "https://www.zhihu.com/people/li-shi-zhen-76", 
        "https://www.zhihu.com/people/meng-ke-43-99", 
        "https://www.zhihu.com/people/zhang-jia-wang-38-94", 
        "https://www.zhihu.com/people/zhang-bo-34", 
        "https://www.zhihu.com/people/xu-xiao-long-96-16", 
        "https://www.zhihu.com/people/du-yi-88", 
        "https://www.zhihu.com/people/zhao-jun-8-93", 
        "https://www.zhihu.com/people/zheng-you-quan-78", 
        "https://www.zhihu.com/people/wan-wan-25-54", 
        "https://www.zhihu.com/people/ventsing", 
        "https://www.zhihu.com/people/yao-xiao-wen-84", 
        "https://www.zhihu.com/people/buctx982zz21", 
        "https://www.zhihu.com/people/xiao-kai-bu-pa-bu-pa-89", 
        "https://www.zhihu.com/people/huang-bin-51-48", 
        "https://www.zhihu.com/people/wu-cheng-tao-76", 
        "https://www.zhihu.com/people/mi-tu-zhi-fan-de-itgou", 
        "https://www.zhihu.com/people/wang-teng-yun-45", 
        "https://www.zhihu.com/people/zhu-gang-95-68", 
        "https://www.zhihu.com/people/wang-mike-4", 
        "https://www.zhihu.com/people/zhang-xiao-ming-49-55", 
        "https://www.zhihu.com/people/jack-56-50-67", 
        "https://www.zhihu.com/people/xliangcn", 
        "https://www.zhihu.com/people/hu-chang-qi", 
        "https://www.zhihu.com/people/liu-shi-69-85", 
        "https://www.zhihu.com/people/woofyzhao", 
        "https://www.zhihu.com/people/xu-ling-xiao-63-98", 
        "https://www.zhihu.com/people/hote-17", 
        "https://www.zhihu.com/people/ping-qi-xing", 
        "https://www.zhihu.com/people/xiao-cong-ban-dou-fu-36", 
        "https://www.zhihu.com/people/li-song-yuan-25", 
        "https://www.zhihu.com/people/kunpeng-geng", 
        "https://www.zhihu.com/people/kissweb", 
        "https://www.zhihu.com/people/la-la-la-wa-wa-wa", 
        "https://www.zhihu.com/people/qi-zhu-qu-kan-hai-31", 
        "https://www.zhihu.com/people/chu-hai-liang-41", 
        "https://www.zhihu.com/people/xue-yu-yong-zhe-192", 
        "https://www.zhihu.com/people/bebc", 
        "https://www.zhihu.com/people/awslong", 
        "https://www.zhihu.com/people/sufar", 
        "https://www.zhihu.com/people/lane-97", 
        "https://www.zhihu.com/people/newcfg", 
        "https://www.zhihu.com/people/pengfeixue", 
        "https://www.zhihu.com/people/ha-ha-85-92-87", 
        "https://www.zhihu.com/people/devilsm", 
        "https://www.zhihu.com/people/guan-tang-bao-56", 
        "https://www.zhihu.com/people/kai-bruce", 
        "https://www.zhihu.com/people/cfc4n", 
        "https://www.zhihu.com/people/zhang-jun-58-99", 
        "https://www.zhihu.com/people/su-xiao-run", 
        "https://www.zhihu.com/people/mo-ke-shu-chong-46", 
        "https://www.zhihu.com/people/colin-48-22", 
        "https://www.zhihu.com/people/chen-rui-31-53", 
        "https://www.zhihu.com/people/steel-13-83", 
        "https://www.zhihu.com/people/ru-xue", 
        "https://www.zhihu.com/people/zhang-yiming-42-22", 
        "https://www.zhihu.com/people/xiejunlong", 
        "https://www.zhihu.com/people/hurricane1260", 
        "https://www.zhihu.com/people/han-shao-ye-21", 
        "https://www.zhihu.com/people/henter", 
        "https://www.zhihu.com/people/xing-long-67", 
        "https://www.zhihu.com/people/feng003", 
        "https://www.zhihu.com/people/barry-40-63", 
        "https://www.zhihu.com/people/wu-miao-jun-40", 
        "https://www.zhihu.com/people/li-shao-52-36", 
        "https://www.zhihu.com/people/yidou-wang-yang", 
        "https://www.zhihu.com/people/willwinworld", 
        "https://www.zhihu.com/people/xuanxuan-zhu", 
        "https://www.zhihu.com/people/liu-feng-yi-93", 
        "https://www.zhihu.com/people/hong-lu-yu-72", 
        "https://www.zhihu.com/people/jiangzc86", 
        "https://www.zhihu.com/people/jienan.zjn", 
        "https://www.zhihu.com/people/feng1o", 
        "https://www.zhihu.com/people/wang-hai-bo-49-82", 
        "https://www.zhihu.com/people/chu-yuan-57", 
        "https://www.zhihu.com/people/sun-38", 
        "https://www.zhihu.com/people/chunxin.ycx", 
        "https://www.zhihu.com/people/noner-77", 
        "https://www.zhihu.com/people/a-fei-xi-ya-65", 
        "https://www.zhihu.com/people/coocla", 
        "https://www.zhihu.com/people/sadhen", 
        "https://www.zhihu.com/people/wang-lian-hui", 
        "https://www.zhihu.com/people/zhang-ming-feng-91", 
        "https://www.zhihu.com/people/xiao-han-13-60", 
        "https://www.zhihu.com/people/chuangbo", 
        "https://www.zhihu.com/people/codejw", 
        "https://www.zhihu.com/people/riskers", 
        "https://www.zhihu.com/people/xie-11-9", 
        "https://www.zhihu.com/people/liu-shao-hui-79", 
        "https://www.zhihu.com/people/hector-13-3", 
        "https://www.zhihu.com/people/xu-zhi-fa-17", 
        "https://www.zhihu.com/people/seven-cool", 
        "https://www.zhihu.com/people/eta-100a", 
        "https://www.zhihu.com/people/huang-yi-99", 
        "https://www.zhihu.com/people/xu-xiao-sheng-18", 
        "https://www.zhihu.com/people/firstcao", 
        "https://www.zhihu.com/people/xiang-xu-39", 
        "https://www.zhihu.com/people/edidada", 
        "https://www.zhihu.com/people/er-hai-40-26-11", 
        "https://www.zhihu.com/people/danielv", 
        "https://www.zhihu.com/people/genuinejyn", 
        "https://www.zhihu.com/people/smartcat", 
        "https://www.zhihu.com/people/tao-tao-tao-tao-wen", 
        "https://www.zhihu.com/people/firetaker", 
        "https://www.zhihu.com/people/isllei", 
        "https://www.zhihu.com/people/liu-nan-9-79", 
        "https://www.zhihu.com/people/veryzhun", 
        "https://www.zhihu.com/people/liang-xiao-wei-92", 
        "https://www.zhihu.com/people/wang-chao-84-29", 
        "https://www.zhihu.com/people/hryang", 
        "https://www.zhihu.com/people/tu-xiao-xia-8", 
        "https://www.zhihu.com/people/kakusilong", 
        "https://www.zhihu.com/people/halfer53", 
        "https://www.zhihu.com/people/nan-ge-56", 
        "https://www.zhihu.com/people/guoweikuang-77", 
        "https://www.zhihu.com/people/_andy2046", 
        "https://www.zhihu.com/people/he-yi-45-2", 
        "https://www.zhihu.com/people/shi-shu-sheng-", 
        "https://www.zhihu.com/people/xiaokaibupabupa", 
        "https://www.zhihu.com/people/mi-hao-tu", 
        "https://www.zhihu.com/people/jgeng", 
        "https://www.zhihu.com/people/qisong-wei", 
        "https://www.zhihu.com/people/detailyang", 
        "https://www.zhihu.com/people/sapliguang", 
        "https://www.zhihu.com/people/mq4096", 
        "https://www.zhihu.com/people/mr-z-38", 
        "https://www.zhihu.com/people/shi-yu-11-95", 
        "https://www.zhihu.com/people/michael-65-12", 
        "https://www.zhihu.com/people/SecondaryMarquis", 
        "https://www.zhihu.com/people/ma-geng-bin", 
        "https://www.zhihu.com/people/castzhong", 
        "https://www.zhihu.com/people/songzhili", 
        "https://www.zhihu.com/people/rsysw", 
        "https://www.zhihu.com/people/yang-gong-67-15", 
        "https://www.zhihu.com/people/iskywalker", 
        "https://www.zhihu.com/people/hijackjave", 
        "https://www.zhihu.com/people/wang-xiao-ming-6-5", 
        "https://www.zhihu.com/people/zhang-hai-tian-33-59", 
        "https://www.zhihu.com/people/yang-xue-song-89", 
        "https://www.zhihu.com/people/lan-lan-lan-lan-96-82", 
        "https://www.zhihu.com/people/she-liang", 
        "https://www.zhihu.com/people/jinxinxin", 
        "https://www.zhihu.com/people/fu-dong-wei", 
        "https://www.zhihu.com/people/peng-jin-yi", 
        "https://www.zhihu.com/people/tonytan", 
        "https://www.zhihu.com/people/kevin-hill", 
        "https://www.zhihu.com/people/xiao-jie-40-97", 
        "https://www.zhihu.com/people/baomingjei", 
        "https://www.zhihu.com/people/mafei777", 
        "https://www.zhihu.com/people/ling-ling-2", 
        "https://www.zhihu.com/people/zhu-chang-lin-27", 
        "https://www.zhihu.com/people/yongge-89", 
        "https://www.zhihu.com/people/chris", 
        "https://www.zhihu.com/people/haotianhaq", 
        "https://www.zhihu.com/people/le-le-2-16", 
        "https://www.zhihu.com/people/hewanxiang", 
        "https://www.zhihu.com/people/xie-tao-47-50", 
        "https://www.zhihu.com/people/quietcool", 
        "https://www.zhihu.com/people/linuxcpp", 
        "https://www.zhihu.com/people/yue-guo-shuai-68", 
        "https://www.zhihu.com/people/princenju", 
        "https://www.zhihu.com/people/sun-ya-10-51", 
        "https://www.zhihu.com/people/li-xiao-hui-26-36", 
        "https://www.zhihu.com/people/lupoo", 
        "https://www.zhihu.com/people/lei-zi-hai", 
        "https://www.zhihu.com/people/hellokangning", 
        "https://www.zhihu.com/people/zealoussnow", 
        "https://www.zhihu.com/people/lu-jian-xin-25", 
        "https://www.zhihu.com/people/zhao-zhong-wei-82", 
        "https://www.zhihu.com/people/hu-xin-lei-53", 
        "https://www.zhihu.com/people/hugo-57-60", 
        "https://www.zhihu.com/people/Juntaran", 
        "https://www.zhihu.com/people/yuxuan-liu-99", 
        "https://www.zhihu.com/people/liu-yue-chi-ge", 
        "https://www.zhihu.com/people/xxt-44", 
        "https://www.zhihu.com/people/crazyactor", 
        "https://www.zhihu.com/people/jeff-yang-37", 
        "https://www.zhihu.com/people/wang-cheng-xuan", 
        "https://www.zhihu.com/people/tao-yu-16-44", 
        "https://www.zhihu.com/people/fosmjo", 
        "https://www.zhihu.com/people/dh-chen-53", 
        "https://www.zhihu.com/people/fourhu-68", 
        "https://www.zhihu.com/people/xie-yin-song", 
        "https://www.zhihu.com/people/cai-yu-qiang-65", 
        "https://www.zhihu.com/people/bbtan", 
        "https://www.zhihu.com/people/devil-bsd", 
        "https://www.zhihu.com/people/wen-zhi-hong", 
        "https://www.zhihu.com/people/wang-ji-ggg", 
        "https://www.zhihu.com/people/gtalk", 
        "https://www.zhihu.com/people/shawn.zhou", 
        "https://www.zhihu.com/people/ming-ri-qing-yun-qu", 
        "https://www.zhihu.com/people/fuzhengzong", 
        "https://www.zhihu.com/people/zhong-jia-xin-15", 
        "https://www.zhihu.com/people/jlangplusplus", 
        "https://www.zhihu.com/people/shi-ze-kui", 
        "https://www.zhihu.com/people/sulvto", 
        "https://www.zhihu.com/people/shanwill", 
        "https://www.zhihu.com/people/fleurer"
    ], 
    "article": [
        {
            "url": "https://zhuanlan.zhihu.com/p/82659299", 
            "userName": "要没时间了", 
            "userLink": "https://www.zhihu.com/people/bdbe7f88af9a021c5c770649d266f143", 
            "upvote": 9, 
            "title": "Kubernetes 的基石 — 容器技术", 
            "content": "<blockquote>文章同步于个人博客：<a href=\"https://link.zhihu.com/?target=https%3A//littledriver.net/posts/understand-container-technology/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">littledriver</a>.net</blockquote><h2>Overview</h2><p>随着容器技术的诞生和发展，它不再仅仅是一个发布你所开发软件的新姿势，更是在后端开发生态中，慢慢的成为了工程师开发模式的一部分。</p><p>容器技术最核心的一个优势，就是抹平了由于开发环境和部署环境的差异导致的部署线上服务困难的问题。任何一个以“容器”，都会被独立发布和部署。并且在“构建”的过程中，可以同时将服务所依赖的“环境”进行打包，从而保证了线上和本地的环境一致，减少部署过程中出现的问题。</p><p>除此之外，容器技术的实现借助了 Linux 操作系统中两个比较重要的技术 Namespace 和 Cgroup。前者让某一个容器运行起来之后可以在环境或者说是视图上保持良好的隔离性，后者则让一个容器在使用操作系统提供的资源的时候，与其他容器甚至是宿主机产生良好的隔离。</p><p>另外，对于容器技术值得提及的一点就是。虽然它所实现的『隔离性』不及虚拟机提供的那样完善，但它在性能上以及资源占用上表现的要比虚拟机更加优秀。这里说的资源占用是指非容器内部的业务应用所消耗的部分。因为一个容器，本质上就是一个操作系统的进程。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-7fb2e2d54e310dc2a31029616c8e2d2a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"685\" data-rawheight=\"336\" class=\"origin_image zh-lightbox-thumb\" width=\"685\" data-original=\"https://pic3.zhimg.com/v2-7fb2e2d54e310dc2a31029616c8e2d2a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;685&#39; height=&#39;336&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"685\" data-rawheight=\"336\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"685\" data-original=\"https://pic3.zhimg.com/v2-7fb2e2d54e310dc2a31029616c8e2d2a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-7fb2e2d54e310dc2a31029616c8e2d2a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>不过，在计算机世界中没有绝对的『银弹』。容器技术虽然在性能和资源上更好一些，但是在隔离性的保证上面往往表现的不如虚拟机。这是因为，软件层面上的隔离终究不是彻底的。某些难以隔离的资源，比如 Linux 内核是对所有的容器共享的。所以，使用容器技术带来的一个明显的缺陷，就是我们要额外的考虑很多『安全性』的问题。</p><h2>隔离技术</h2><p>容器技术在实现『隔离性』的时候，借助了两项技术，一个为 Namespace，另外一个为 Cgroup。</p><h2>Namespace</h2><p>简单来说，Linux Namespace 是一项 Linux 系统提供的进程间的隔离技术。它能够在『视图』层面上对进程的一些信息进行隔离。</p><p>在理解 Namespace 的时候，我们可以把整个操作系统默认的空间认为是一个全局的 Namespace。如果不加过多的处理，那么所有的进程都是在这一个 Namespace 下的。他们共享网络设备，共享同一组进程 ID 等等。</p><p>根据要隔离的东西的不同，Linux 对 Namespace 进行了分类：Mount Namespace， UTS Namespace，PID Namespace，IPC Namespace，Network Namespace， User Namespace。</p><p>例如，在某一个容器中，我们希望第一个启动的进程 PID 就应该是 1。这相当于在给进程编号这件事上，我们要和原本的操作系统的全局空间隔离开来。所以，Linux 在实现这个功能的时候，同时保留了两个空间内的 PID。全局空间下，容器作为一个进程仍然按照当前的规则继续进行命名，但是到了容器内部，在开启了 PID Namespace 之后，容器外的 PID 命名情况就完全对容器内部不可见了。也就是在容器内部，对于 PID 命名这件事来说，一切都是从头开始了。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-9343809dda2b09129984ce7405f2a77e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"3352\" data-rawheight=\"834\" class=\"origin_image zh-lightbox-thumb\" width=\"3352\" data-original=\"https://pic3.zhimg.com/v2-9343809dda2b09129984ce7405f2a77e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;3352&#39; height=&#39;834&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"3352\" data-rawheight=\"834\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"3352\" data-original=\"https://pic3.zhimg.com/v2-9343809dda2b09129984ce7405f2a77e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-9343809dda2b09129984ce7405f2a77e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>看过上面对于 PID Namespace 的使用，我想你应该能够明白，“Namespace 提供的仅仅是『视图』上隔离“ 这句话的含义了。</p><p>到此为止，虽然你可能了解了『Namespace 是负责为容器实现环境隔离』的事实。但是你有没有再深入的想一下：为什么在容器中一定要把第一个进程的 PID 作为 1 呢？以及 Mount Namespace 到底是隔离了什么呢？</p><h3>PID = 1 的进程</h3><p>在 Linux 操作系统启动的过程中，内核经过 Boot Loader 的加载并初始化之后，会启动一个名为 Init 的进程，它的进程号为 1。Init 进程还有几个别名：超级父进程，根进程。</p><p>从字面意思上我们就可以看出，在 Linux 系统启动后创建的进程都是这个超级父进程的子进程，孙子进程等等。换句话说，如果某个（孙）子进程的父进程异常退出了，那么 Init 进程将会接管这个『孤儿』进程。这种『接管』操作，是超级父进程众多功能之一。</p><p>为了让某个容器启动之后，在视图级别上与其他进程隔离开来。除了使用 chroot 命令更改进程的根目录之外，我们还将在容器内部通过创建一个新的 PID Namespace，来重置 PID 命名规则。这使得容器内第一个启动的进程为超级父进程。</p><p>为一个封闭的环境指定超级父进程的好处在于，我们可以更加精细化的管理这个环境内部衍生出来的其他子进程。Pause 程序以及 Systemd 都可以借助这个特性应用到我们的容器应用开发中来。这两个特性，我会在之后的文章中，单独拿出来讲。</p><p>（关于超级父进程，可以读一下耗子叔这篇文章，写的很有趣：<a href=\"https://link.zhihu.com/?target=https%3A//coolshell.cn/articles/17998.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Linux PID 1 和 Systemd | | 酷 壳 - CoolShell</a>）</p><h3>Mount Namespace</h3><p>本着 Namespace 是为容器进程提供『视图』上的隔离这一想法。我们来看下 Mount Namespace 究竟做了些什么。</p><p>Mount Namespace 主要是为容器进程隔离了”挂载点“相关信息。其实提到挂载点，我们很容易把它和在该挂载点挂载的文件搞混。例如，在容器内部我们执行了一次 Mount 操作，并且更改了这个被挂载文件的内容。很多人以为这个操作是被隔离的，但其实，当你退出这个容器进程之后，你之前所做的更改是会出现在Host 的原始文件上的。</p><p>如果你对这个现象感到奇怪，那么证明你对两个概念理解的不够深刻：</p><ol><li>Namespace 只是提供了『视图』层面上的隔离。换句话说，Namespace 就是一种障眼法。</li><li>容器实质上是 OS 上的一个特殊的进程。既然是进程，它的一些修改操作，尤其是在数据层面上，势必会影响到 Host 上的内容。</li></ol><p>所以，在容器进程启动后，且执行 Mount 操作以前，你会发现在容器内部看到的挂载点信息和在 Host 上看到的是一样的。这是因为在开启 Mount Namespace 隔离之后，容器进程会先继承父进程所看到的挂载点信息。之后在容器内部执行 Mount 操作的记录，不会被外部看见。</p><p>仔细理解好这里所说的关于 Mount Namespace 的内容，因为之后再讨论容器 Volume 机制的实现的时候，还会提到它。</p><h2>CGroup</h2><p>有了前面对 Namespace 技术的了解，我相信你应该能够意识到 Namespace 技术只是把容器进程 Jail 到了一个特定的环境中。但是这个进程所使用的资源，所做的修改类的操作，还是会影响到 Host 上的其他进程。</p><p>如果对资源的使用不加限制，那么其他进程很可能会因为『吃不饱』的原因来获取更多甚至抢占其他进程的资源。这是在容器技术中我们所不愿意看到的情况。</p><p>Linux CGroup 实际上是内核提供的一个功能，由多个不同类型的子系统组成，通过文件系统的形式暴露给用户使用。它不仅仅可以对进程使用资源进行限制，还可以动态的对进程的状态进行调整，比如将某个进程挂起或者恢复。所以，在容器技术中，我们使用 Linux CGroup 来实现『资源隔离』。</p><p>CGroup  中有几个重要的概念需要了解：</p><ol><li>Task：即受到 CGroup 限制的进程</li><li>Processes Group： 即进程组，多个进程按照一定的层级关系组合起来的整体</li><li>Hierarchy: CGroup 中进程组的组织模式，效仿了 Linux 目录的树形结构。子进程默认继承父进程的限制条件。</li></ol><p>容器技术对于 CGroup 以及操作系统来说，并没有什么特殊之处。他们会把每一个容器都当做是普通的进程来对待。通过设置进程组或者某个进程的资源限制条件，即可达到将容器进程与其他进程在资源使用上隔离的目的。</p><p>不过，「CGroup」 也不是万能的，有些操作系统的资源是不能够被完全隔离或者限制的。比如：操作系统的时间是不能在容器中随意更改的，<code>/proc</code> 文件系统因为没有感知到「CGroup」对于容器进程做的事情，所以在容器内部执行 top 等命令仍然看到的是宿主机的情况。</p><p>对于「系统时间」问题，这可能不单单涉及到对一个容器进行某些修改行为的限制，还涉及到一定的安全性问题。而对于 <code>/proc</code> 文件系统来说，可以使用 lxcfs + 挂载的方式进行解决。如果是在 Kubernetes 当中，可以将 lxcfs 以 DaemonSet 的方式部署到集群的各个节点上。</p><h2>容器镜像</h2><p>如果你把一个容器想成一个圆柱体，那么我们刚才所讲的 Namespace 就相当于是这个圆柱体的『侧面』，站在这个圆柱体内，我们对外面发生的事情一无所知。这就是所谓的『环境隔离』。</p><p>但是，当视角从上向下俯视的时候，你就会发现，你所看的目录，文件都是属于 Host 的并且和其他进程是共享的。也就是说，虽然这个容器的『前后左右』四个方向被隔离了，但是『上下』两个方向并没有。『上下』方向其实就是我们常说『文件系统』，你可以将它简单的理解为，容器内部的目录与文件。</p><p>既然想做到文件系统层面上的隔离。那我们就必须要满足两个条件：</p><ol><li>干净并且完整的文件系统</li><li>可以重新挂载容器进程根目录且此行为不会影响其他进程</li></ol><p>第一个是比较好准备的，一个普通的 OS Image 就会包含完整的文件系统所需要的目录结构和文件。第二个工具就是容器技术当中常说的 Mount Namespace 隔离。</p><p>如果你对 Linux 系统比较熟悉的话，应该可以很快想到一个名为『chroot』的命令。它可以改变一个进程的根目录。在某个进程内部调用这个命令，相当于将这个进程 jail 到了一个特定的目录下。在这个进程内向上下两个方向看的时候，就会给它造成一种，自己处于一个单独且隔离的文件系统中的错觉。</p><p>所以，Mount Namespace 和 chroot 命令以及文件系统，三者为容器进程补齐了『隔离』层面上最后的缺口。而我们上面提到的，为容器进程准备的『纯净的文件系统』还有另外一个名字，就是根文件系统。正因为有了根文件系统和容器镜像的概念，才从根本上解决了本地开发环境和远端部署环境不一致的问题。</p><p>这里需要注意一点，根文件系统并不等于操作系统内核。『内核』其实说白了也只是一段程序，是软件。在操作系统启动的时候会加载内核的镜像。内核是只有一个的，但是『根文件系统』却可以有多个。这个特性，也从侧面证明了容器的隔离性并没有虚拟机那么好，因为所有的容器进程都会共享一个内核。</p><h2>复用</h2><p>既然提到了文件系统，那么在进程运行期间是不可避免的会修改其中的文件的。但是，如果每一个容器进程在启动之后，都需要一份完成的文件系统 copy 的话，这就是对磁盘资源极大的浪费。并且，在容器终止之后，如果不需要持久化它修改过的内容，我们还需要清理掉这些文件。</p><p>基于上述问题，看起来在制作容器镜像的时候，光有一个文件系统还不够。我们得需要借助一些特殊的手段来构建镜像，希望它能够达到以下几个目标：</p><ol><li>修改过的内容在容器终止之后默认被丢弃，如果需要持久化的话，一种方案是借助 Volume 机制，另外一种是通过 docker commit 保存修改</li><li>按照一定的维度将整个文件系统进行细粒度的切分，能够尽可能的复用其中的内容</li></ol><p>在容器技术中，我们借助『联合文件系统』以及『镜像分层』两项技术来满足上面的要求。『联合文件系统』最普遍的实现方式就是 AUFS。首先，我们将一个容器镜像划分为三层：只读层，Init 层和读写层。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-fa38c40d5ce8866c797b931d788d4f9b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1350\" data-rawheight=\"1002\" class=\"origin_image zh-lightbox-thumb\" width=\"1350\" data-original=\"https://pic4.zhimg.com/v2-fa38c40d5ce8866c797b931d788d4f9b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1350&#39; height=&#39;1002&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1350\" data-rawheight=\"1002\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1350\" data-original=\"https://pic4.zhimg.com/v2-fa38c40d5ce8866c797b931d788d4f9b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-fa38c40d5ce8866c797b931d788d4f9b_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>其中只读层指的就是一个纯净的文件系统全部的内容，为了保证复用，该层的内容不允许被修改。Init 的层指的是那些在容器运行过程中，需要被修改，但是不需要传递给其他容器的部分，比如 host 文件。而读写层指的就是可以被修改的部分。</p><p>看起来这个分层的措施虽然满足了『复用』的目的，但是『读写层』和『只读层』的划分明显是矛盾的。此时，就轮到 AUFS 发挥用处了。AUFS 将原生文件系统中的多个目录联合在一起，挂载到一个特定的目录下。容器进程的根目录，其实就是在这个挂载点上获取的。读操作一般来说没有问题。当容器进程想执行写操作的时候，AUFS 会自上而下的寻找到第一个符合要求的文件，并把它 copy 到读写层所在的目录，然后再进行修改。这就是我们常说的 copy-on-write 机制。不但写操作的时候是按照从上至下的顺序，读操作也是一样。所以上层的文件会覆盖掉下层同名的文件。</p><p>写操作和删除操作的实现方式略有不同。虽然都以『上层覆盖下层』的原则为基础，但是删除操作是在读写层创建一个以『.wh.』+ 原文件名的组合为新文件名的特殊文件。在容器内部读取文件信息的时候，AUFS 会自动的屏蔽掉原文件。造成一种这个文件已经被删除的假象。</p><p>在讨论容器镜像复用的时候，必须要提到的一点就是。无论容器如何隔离，镜像如何分层，其修改的内容，都是体现在宿主机上的。只不过非持久化的部分在你不知情的情况下被清理掉了。</p><h2>持久化</h2><p>容器镜像的持久化，是依靠 docker commit 命令来实现的。当你想调用 docker commit 的时候就证明你想保存下你当前在容器中所做的修改到一个新的镜像内部，然后开放给其他人使用。从某个角度来说，这也是一种复用，只不过复用的基础改变了。</p><p>docker commit 的实现原理也很简单，它在之前提到的镜像分层以及复用的基础上，为所修改的内容单独加上了一层。『上层屏蔽下层』的原则，在构建新镜像的时候也有所体现。</p><p>我在自己的本地环境中，进行了一次构建新镜像的实验。以 busybox 镜像为例，在原生镜像中，使用 docker inspect 命令可以看到镜像只有一层</p><div class=\"highlight\"><pre><code class=\"language-json\"><span class=\"s2\">&#34;RootFS&#34;</span><span class=\"err\">:</span> <span class=\"p\">{</span>\n            <span class=\"nt\">&#34;Type&#34;</span><span class=\"p\">:</span> <span class=\"s2\">&#34;layers&#34;</span><span class=\"p\">,</span>\n            <span class=\"nt\">&#34;Layers&#34;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n                <span class=\"s2\">&#34;sha256:6c0ea40aef9d2795f922f4e8642f0cd9ffb9404e6f3214693a1fd45489f38b44&#34;</span>\n            <span class=\"p\">]</span>\n        <span class=\"p\">}</span></code></pre></div><p>使用 busybox 镜像启动容器，并且尝试做一些修改。</p><div class=\"highlight\"><pre><code class=\"language-bash\"><span class=\"o\">[</span>root ~<span class=\"o\">]</span>$ sudo docker run -it busybox sh\n/ <span class=\"c1\"># ls</span>\nbin   dev   etc   home  proc  root  sys   tmp   usr   var\n/ <span class=\"c1\"># mkdir test</span>\n/ <span class=\"c1\"># cd test/</span>\n/test <span class=\"c1\"># ls</span>\n/test <span class=\"c1\"># echo &#34;hello world&#34; &gt;&gt; a.log</span>\n/test <span class=\"c1\"># cat a.log</span>\nhello world</code></pre></div><p>然后，在宿主机上执行 docker commit 命令，生成一个名为 newbusybox 的新镜像：</p><div class=\"highlight\"><pre><code class=\"language-bash\">sudo docker commit 66f0c49b6948 newbusybox:1.0</code></pre></div><p>此时，再通过 docker inspect 查看这个新镜像的结构，可以发现，新的镜像多出来一个layer。而且，原有的 layer 的 hash 值也没有变。这证明，新镜像的构造是一次纯粹的『叠加』操作。</p><div class=\"highlight\"><pre><code class=\"language-json\"><span class=\"s2\">&#34;RootFS&#34;</span><span class=\"err\">:</span> <span class=\"p\">{</span>\n            <span class=\"nt\">&#34;Type&#34;</span><span class=\"p\">:</span> <span class=\"s2\">&#34;layers&#34;</span><span class=\"p\">,</span>\n            <span class=\"nt\">&#34;Layers&#34;</span><span class=\"p\">:</span> <span class=\"p\">[</span>\n                <span class=\"err\">#</span><span class=\"s2\">&#34;sha256:6c0ea40aef9d2795f922f4e8642f0cd9ffb9404e6f3214693a1fd45489f38b44&#34;</span><span class=\"p\">,</span>\n                <span class=\"s2\">&#34;sha256:918cb44f6e232c43550ba9405c565f6a5dcb69a8b323d53d07f9cddc1c710d66&#34;</span>\n            <span class=\"p\">]</span>\n        <span class=\"p\">}</span></code></pre></div><p>最终，我们使用新的镜像启动一个新的容器，其内部果然能够看到我们之前作出的修改：</p><div class=\"highlight\"><pre><code class=\"language-bash\"><span class=\"o\">[</span>root ~<span class=\"o\">]</span>$ sudo docker run -it newbusybox:1.0 sh\n/ <span class=\"c1\"># ls</span>\nbin   dev   etc   home  proc  root  sys   <span class=\"nb\">test</span>  tmp   usr   var\n/ <span class=\"c1\"># cd test/</span>\n/test <span class=\"c1\"># cat a.log</span>\nhello world\n/test #</code></pre></div><h2>容器持久化</h2><p>虽然容器和宿主机之间是有一定的隔离机制的。但是，在容器运行的过程中，两者不可避免的要进行一些数据层面上的交互。这就涉及到了如下两个问题：</p><ol><li>容器如何将内部的数据共享给宿主机上的程序</li><li>宿主机上的程序如何将 local 目录的东西共享给容器内部</li></ol><p>顺着『容器也是一个特殊的进程』和『容器所使用的文件系统也是处于宿主机上』两个思路来思考。上述两个问题的解决方案，看起来是要找到一种方式来打通容器和宿主机的数据共享通道。这种方式就是我们所熟悉的 Volume 机制。（Docker 的 Volume 机制和 Kubernetes 中的 Volume 机制是有所不同的，之后再介绍 Kubernetes Volume 概念的时候会着重阐述一下）</p><p>Volume 机制的实现也是比较简单的。借助 Linux 操作系统中的 Bind-Mount 技术，将需要共享的目录挂载至容器内部的某个目录下。但是转念一想，这个『挂载』的操作很可能会影响到宿主机上的『挂载点』信息，即使它是在容器进程内部执行的。那么，为了在容器进程内部隐藏起这个操作，我们需要借助 Mount Namespace 的帮助。即在容器进程（docker init 进程）创建后且mount namespace 开启后，chroot 或者类似改变进程根目录的操作执行之前，进行 Mount 操作，将共享的目录挂载至容器内。此时 Mount 操作的记录对宿主机是不可见的，所以不用担心污染挂载点信息的问题。最后，当容器进程（应用进程）运行的时候，就会在内部看到这个共享的目录，从而实现的数据的互通。</p><p>但是，在使用这种 Volume 机制的时候也要注意一点。如果我想通过 docker commit 的形式来构建一个新的镜像的话。在宿主机上执行的 commit 的命令并不会把共享目录内的数据打包至新的镜像。原因就是：docker commit 命令是在宿主机内执行的。而在宿主机上我们是无法感知到共享目录的挂载信息的，因为 mount namespace 隔离的原因。但是，在容器内部因为共享目录挂载产生的挂载点目录，会被保存到新的镜像中，只不过这个目录内没有内容。</p><p>因为这个挂载点目录的信息，已经属于镜像的更改了。它存在于镜像的读写层，在构建新镜像的时候，是会被包含进来的。</p><h2>小结</h2><p>经过上面的叙述，相信你在头脑中对于容器技术会有如下的认识：</p><p>一个容器就是一个操作系统中比较特殊的进程，它由动态和静态两部分组件构成。</p><p>静态部分指的就是容器的 rootfs，也就是人们常说的镜像。它为一个容器内部的应用进程打包了所有必要的依赖，使得远程部署环境和本地开发环境得到了统一。并且，在设计容器镜像的时候，使用了分层的方式来组织镜像的结构，最大限度的提升了容器镜像的复用性。</p><p>而动态部分可以分为两个方面：Namespace 和 CGroup。前者负责容器视角和环境的隔离后者负责资源使用的隔离。</p><p>最后，也请你记住，无论容器技术设计的多么巧妙。它最终还是运行在宿主机上的。所以在容器内部进行的一些数据层面上的更改，尤其是隔离不彻底会被共享的部分，还是会影响到宿主机的。这也给使用者提了个醒：容器并不是完全隔离的。</p><h2>容器与 Kubernetes 的联系</h2><p>如果你还记得我们讨论容器技术出现意义时所提到的『本地和远端部署环境不一致』的问题，就应该明白，任何一项技术的出现都是为了解决一类特定的问题，或者优化已有的方案。</p><p>那么 Kubernetes 既然是在容器技术的基础（准确的说，应该是在 CGroup 和 Namespace 等更加底层的隔离技术之上。因为 Borg 系统在创立之初是没有容器这个概念的）之上产生的，就证明两者之前肯定有一定的联系。</p><p>首先，开发容器应用的小伙伴的精力变得更加集中，他们专注于开发自己的应用以及如何构建一个合格的容器镜像。至于这个镜像如何运行，以及如何更好的运行，甚至是如何和一起第三方工具（如日志，监控等组件）一起良好的运行，都不应该再让开发者来操心。这些问题，应该由维护容器部署环境的人来解决。而这个环境的维护者，就是 Kubernetes。</p><p>Kubernetes 在不同的时期，针对不同的任务解决很多不同类型的问题。比如，集群管理，任务调度，服务编排等等。但是它们都可以抽象成为一种能力：如何打造一个良好的环境让容器能够更高效的运行。</p><p>如果单单是需要容器的运行，编排等等功能，那么普通的 PaaS 平台是可以在一定程度上满足开发者的，即使 Kubernetes 做得更好一些，但也不足以让 Kubernetes 在服务编排领域获得这么高的地位。</p><p>Kubernetes 的优势在我看来，主要体现在以下几点：</p><ol><li>Kubernetes 在与底层的操作系统和上层的应用交互的时候，都采取了耦合度较低的方式。即 API Server 和 各类底层接口标准的诞生（OCI，CRI， CSI…）。它让 Kubernetes 变得更加的包容和开放，扩展性更强。</li><li>Kubernetes 更加注重对于分布式集群中所运行任务的分类以及这些任务之间关系的处理。Kubernetes 根据任务的特点以及关系抽象出了很多 Workload，比如 Pod，Deployment，StatefulSet 等。它让各类任务的划分更加清晰，在部署的时候也能更加精细的被调度。</li><li>Kubernetes 没有使用『命令式』的方式来处理这个平台内的任务，而是采用了『声明式』。这种方案的优势，其一是提升了任务执行的成功率以及整个平台的容错性。因为处理逻辑是依靠状态信息来进行处理的，这些处理过程即使遇到异常被终止，重启之后仍然能够正常运行。状态信息也被持久化到内部的 etcd 数据库中。其二是 Kubernetes 再一次把 Kubernetes 的使用者和 Kubernetes 的开发者的角色分开了。前者更注重使用，只需要关心选择合适的资源对象来描述自己的任务并且创建相应的『服务对象』（Service 是为了 Pod 的负载均衡而存在的）加以配合。而后者更侧重于『如何将当前状态转变为期望状态』的逻辑开发。</li></ol><p>综上所述，Kubernetes 借助了容器技术为广大开发者打造了一个平台。这个平台不仅可以完成最基本 PaaS 平台的职责：调度应用到合适的节点运行，而且还根据不同任务的特点为我们抽象出了多种 Workload，以便更好的描述我们自己的应用的运行特点。最重要的是，Kubernetes 全自动的为我们处理了这一切，并且更加注重对多个任务之间关系的处理。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>Reference</h2><ol><li>张磊老师在极客时间开设的 Kubernetes 技术专栏，推荐大家订阅</li><li>陈皓老师的博客，尤其是和容器底层技术相关的几篇文章 <a href=\"https://link.zhihu.com/?target=http%3A//coolshell.cn/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">coolshell.cn/</a></li></ol>", 
            "topic": [
                {
                    "tag": "Kubernetes", 
                    "tagLink": "https://api.zhihu.com/topics/20018384"
                }, 
                {
                    "tag": "Docker", 
                    "tagLink": "https://api.zhihu.com/topics/19950993"
                }, 
                {
                    "tag": "容器", 
                    "tagLink": "https://api.zhihu.com/topics/19615961"
                }
            ], 
            "comments": [
                {
                    "userName": "danchaofan", 
                    "userLink": "https://www.zhihu.com/people/e1f271e5770c23ec565837b4ff758cc4", 
                    "content": "pid = 1 有个好处在于 这个pid = 1进程挂了，内核会把这个namespace下的进程清理掉，防止泄漏了～", 
                    "likes": 1, 
                    "childComments": [
                        {
                            "userName": "要没时间了", 
                            "userLink": "https://www.zhihu.com/people/bdbe7f88af9a021c5c770649d266f143", 
                            "content": "学到了，多谢补充", 
                            "likes": 0, 
                            "replyToAuthor": "danchaofan"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/79826072", 
            "userName": "要没时间了", 
            "userLink": "https://www.zhihu.com/people/bdbe7f88af9a021c5c770649d266f143", 
            "upvote": 2, 
            "title": "[RDS On Kubernetes 专栏重要声明]", 
            "content": "<p>大家好，我是这个专栏的作者，我叫徐冉，一名软件工程师。创立这个专栏的初衷是为了把自己学习k8s的过程以及实际开发中积累的与k8s打交道的经验分享出来，希望能和大家共同讨论学习。</p><p>如果您留意的话，这个专栏有很久没有更新了，但是让我好奇的是关注数一直在涨。这让我感觉还是有很多关注者想要继续看到这个专栏的文章。这里有必要解释一下专栏没有更新的原因：我本人这一段时间没有工作，休息了很久，生活也发生了一些有趣的变化。所以，对于k8s的学习，暂时搁置了一段时间。</p><p>这两天有想法把这个专栏继续维护起来，初步的一个思路是开一个系列性的文章，按照学习k8s的思路，由浅入深给大家讲一些你可能没有关注到的一些小细节。比如说pause容器是什么？容器volume的几种实现机制有何区别？类似的问题可能无论在工作debug还是在面试过程中都有可能会用到。这些知识一部分来自于我对一些问题的深入思考，另外一部分来自我在工作当中的实践。希望能对得起大家的关注。</p><p>另外，如果你也对学习k8s和分享有兴趣，可以随时联系我。欢迎一起讨论和投稿。最后，如果大家有想看的主题也可以告诉我，我会尽量满足。</p><p>Email：hnustphoenix@gmail.com</p><p></p><p></p>", 
            "topic": [], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/51966018", 
            "userName": "要没时间了", 
            "userLink": "https://www.zhihu.com/people/bdbe7f88af9a021c5c770649d266f143", 
            "upvote": 3, 
            "title": "Head First Solution of Expired Keys in Redis", 
            "content": "<h2>Redis 的多数据库模式</h2><h2>Redis Server</h2><p>Redis 是一个将数据保存在内存中的 key-value 数据库。很多使用 Redis 的人或许不知道，Redis 在单机数据库的实现上将总容量切分成多个「数据库」，就好像在规定的内存容量中建立多个「Storage Bucket」。它给每一个 Bucket 都标号，默认情况下有16个 Bucket。不同 Bucket 是相互隔离的，在一个 Bucket 内的操作不会影响另一个。</p><p>一个单机的 Redis 数据库，即一个 Redis Server 的实例的数据结构如下：</p><div class=\"highlight\"><pre><code class=\"language-c\"><span class=\"k\">struct</span> <span class=\"n\">redisServer</span> <span class=\"p\">{</span>\n    <span class=\"n\">redisDb</span> <span class=\"o\">*</span><span class=\"n\">db</span><span class=\"p\">;</span>\n    <span class=\"kt\">int</span> <span class=\"n\">dbnum</span><span class=\"p\">;</span>                      <span class=\"cm\">/* Total number of configured DBs */</span>\n<span class=\"p\">}</span></code></pre></div><p>其中 db 是一个指针类型的成员，它会指向一个存有 redisDb 类型对象的数组。根据前面描述的「单机数据库分成多个 Bucket 来存储数据」的事实，我们猜测这个 redisDb 对象就相当于每一个 Bucket。至于要构造多少个 Bucket，是根据 dbnum  这个成员的值来确定的。</p><h2>Redis Client</h2><p>既然 Redis Server 是依靠在内存中构造多个 Storage Bucket 来实现一个数据库实例的，相应的 Redis Client 在访问 Redis Server 的时候也可以选择要访问哪一个 Storage Bucket。这也正是 Redis 中 Select 命令做的事情。</p><div class=\"highlight\"><pre><code class=\"language-c\"><span class=\"k\">struct</span> <span class=\"n\">redisClient</span> <span class=\"p\">{</span>\n    <span class=\"n\">redisDb</span> <span class=\"o\">*</span><span class=\"n\">db</span><span class=\"p\">;</span>\n<span class=\"p\">}</span></code></pre></div><p>在 Redis Client 的数据结构中，我们也发现了同样的类型为 redisDb 的成员。它指向的是该 Client 实例访问的 Bucket。通过 Select 命令，我们可以切换这个成员指向的 Bucket。默认情况下，客户端访问的是编号为 0 的 Bucket。</p><h2>redisDb</h2><p>既然 Redis Server 是分为多个 redisDb 来实现的，且 Redis Client 实际操作的也是某一个具体的 redisDb。那么看起来这个类型为 redisDb 的成员应该就是实际存储数据的地方。</p><div class=\"highlight\"><pre><code class=\"language-c\"><span class=\"k\">typedef</span> <span class=\"k\">struct</span> <span class=\"n\">redisDb</span> <span class=\"p\">{</span>\n    <span class=\"n\">dict</span> <span class=\"o\">*</span><span class=\"n\">dict</span><span class=\"p\">;</span>                 <span class=\"cm\">/* The keyspace for this DB */</span>\n    <span class=\"kt\">int</span> <span class=\"n\">id</span><span class=\"p\">;</span>                     <span class=\"cm\">/* Database ID */</span>\n      <span class=\"p\">...</span>\n<span class=\"p\">}</span> <span class=\"n\">redisDb</span><span class=\"p\">;</span></code></pre></div><p>除了 id 成员标识了它是第几个 Bucket 之外，dict 成员看起来是引导了一个字典对象，存放 Client 写入的 key-value 数据。</p><p>我们通过客户端对 Redis Server 执行的写入，更新，删除，其实都是对目标 redisDb 的 dict 成员进行操作。你可以想象一下，在一个用 golang 语言编写 demo 中，你构造了一个 Map。你是怎么对这个 Map 进行增删改查的，Redis Server 在内部就是怎样对这个 redisDb .dict 操作的。唯一不同的地方在于，Redis Server 构造的 redisDb 的 dict 成员，其 key 必须是字符串对象，value 需要是 Redis 承认的合法的数据类型的对象。</p><p>除了可以正常的读写 redisDb 对象，在这些操作执行期间，Redis Server 还会做一些标记和维护性的工作：</p><ol><li>若成功读取某个 key，那么给这个 key 的命中次数+1。否则对这个 key 的 miss 次数+1</li><li>更新这个 key 最新被访问的时间，以便用于之后的淘汰策略（如 LRU 等）</li></ol><h2>Key 的生命周期的管理</h2><p>默认情况下， 如果我们通过 Redis Client 向 Server 写入一个 Key-Value，这个 Key 及其对应的 Value 是不会过期的，因为没有为其设置过期时间。通过<code>ttl keyname</code> 即可查看这个 Key 有效的剩余时间。由于大多数用户在使用 Redis 的时候都是将它作为缓存，所以一般来说，我们都会给写入 Redis 的 Key 设置一个过期时间，并将它保存起来。如果这个 Key 已经达到了它的过期时间，那么Redis 就会将它删除。</p><p>上面叙述的过程可以认为是 Redis 自身对 Key 的生命周期的管理，它基本可以分为以下几个部分：</p><ul><li>设置 Key 的过期时间</li><li>保存 Key 的过期时间</li><li>判定键已经过期</li><li>自动删除过期的 Key</li></ul><h2>设置 Key 的过期时间</h2><p>设置某个 Key 的过期时间，可以通过 PEXPIREAT 命令来实现，该命令接受两个参数，第一个为 keyName，第二个为过期的时间点，用 Unix 毫秒时间戳来表示。</p><h2>保存 Key 的过期时间</h2><div class=\"highlight\"><pre><code class=\"language-c\"><span class=\"k\">typedef</span> <span class=\"k\">struct</span> <span class=\"n\">redisDb</span> <span class=\"p\">{</span>\n    <span class=\"n\">dict</span> <span class=\"o\">*</span><span class=\"n\">dict</span><span class=\"p\">;</span>                 <span class=\"cm\">/* The keyspace for this DB */</span>\n      <span class=\"n\">dict</span> <span class=\"o\">*</span><span class=\"n\">expires</span><span class=\"p\">;</span>              <span class=\"cm\">/* Timeout of keys with a timeout set */</span>\n    <span class=\"kt\">int</span> <span class=\"n\">id</span><span class=\"p\">;</span>                     <span class=\"cm\">/* Database ID */</span>\n    <span class=\"p\">....</span>\n<span class=\"p\">}</span> <span class=\"n\">redisDb</span><span class=\"p\">;</span></code></pre></div><p>在标识每一个数据库（即每一个 Bucket） 的 redisDb 类型的对象中，有一个名为 expires 的对象，它也是 dict 类型的。其中 Key 为 数据库中 key 的名称，value 则为该 Key 的过期时间，即一个毫秒的 Unix 时间戳。</p><h2>判定键的过期</h2><p>判定一个 Key 是否已经过期，是计算了 expires 内该 Key 的过期时间和当前时间的差值。这也就是 TTL 和 PTTL 命令返回的结果。两个命令唯一的区别就是输出前做了单位上面的转化。</p><h2>自动删除过期的 Key</h2><h2>定时删除</h2><p>定时删除的实现方式，是在对一个 Key 设置过期时间的同时为这个 Key 创建一个定时器。等到定时器计时结束的时候删除这个 Key。定时器的好处就是删除及时，不会浪费内存空间。缺点也很明显：创建定时器，就需要额外的消耗。并且，当有大量写入和读取请求的时候，Redis 到底是优先处理外来的任务呢，还是先删除这些过期的 Key 呢？</p><h2>惰性删除</h2><p>惰性删除的原理也很简单：虽然我们在 Key 被写入到 Redis 的时候为其设置了超时时间，但是我们并不去主动检查它。而是等它要被读取的时候，计算一下当前时间和其设置的过期时间的差值，来决定是否要删除这个 Key。</p><p>惰性删除的好处是，不会让删除操作影响用户正常的读写操作，而且不用忍受创建定时器带来的额外的消耗。 惰性删除的坏处是，如果一直没有对某个过期的 Key 执行读取操作，那么这个过期的 Key 就会一直占用内存的空间。</p><h2>定期删除</h2><p>既然定时删除的缺点是会占用 CPU 的时间妨碍正常写入和读取操作的执行，而惰性删除操作是会引起「内存泄漏」。那么，一个折中的方案就是，定期主动的去删除过期的 Key。这样一来，既避免了长期不清理导致的内存浪费，也通过减少删除操作的执行频率和执行时间，降低了对 Redis 正常操作的影响。</p><p>Redis 在实现的时候，实际上是配合使用定期删除和惰性删除两种策略。前者可以认为是一种「缓慢版」的定时删除策略。</p><h2>删除过期 Key 的策略的实现</h2><h2>惰性删除</h2><p>由于我们已经在每个 redisDb 的对象内存储了当前数据库的 Key 过期时间的 dict。所以，每当我们在对某个 Key 执行一个命令的时候，都会先通过一个函数来检查一下该 Key 的过期情况，如果过期了或者 Key 不存在，就按照不存在的情况统一处理。否则正常执行相应的操作。因为这些数据都是存在 Map 数据结构内的，所以读取操作相对来说是非常迅速的，不会对 Redis 造成什么性能上的压力。</p><h2>定期删除</h2><p>定期删除过期 Key 的逻辑，放在 Redis 一个名为 serverCron 的函数中执行。这个函数中放的都是 Redis 想周期执行的逻辑。定期删除的策略如下：</p><ol><li>为本次删除操作先设定一个计时器，a 秒</li><li>遍历所有的数据库，即我们前面说的 redisServer.redisDb 指向的数组内保存的所有的 redisDb 的实例</li><li>对每个数据库检查一定数量的 Key，如每个数据库挑20个。每次随机挑选一个 Key 检查它的过期时间，如果过期就直接删除</li></ol><p>默认情况下，Redis 会在 a 秒内不断重复第三步。要么等到计时器终止退出此次删除操作，要么对每个数据库都检查了一遍，正常退出此次操作。如果是在计时器中止的情况下退出的，Redis 还会记录下上次处理到了第几个数据库。下次定期删除操作再执行的时候，会接着上次的现场继续处理。</p><h2>过期的 Key 对于 RDB 和 AOF 以及 Replication 功能的影响</h2><h2>RDB</h2><h2>主动备份</h2><p>当我们通过 Redis Client 执行  Save 或者 BGSave 命令的时候，Redis Server 会根据当前内存的内容，生成一个 Snapshot，它就是 RDB 文件。但是在生成这个 RDB 文件内容的时候，Redis 会对写入的 Key 进行检查，如果某个 Key 已经过期的话，那么它不会被写入到 RDB 文件中。</p><h2>RDB 文件载入</h2><p>若一个 Redis Server 开启了 RDB 的备份功能，那么在 Redis Server 启动的时候，它会在指定的目录下寻找 RDB 文件，并将文件内容载入到内存中。但是，此时 RDB 文件内部的某一部分 Key 可能已经过期了。在对这些过期的 Key 的处理上，主从服务器会采取不同的措施：</p><h2>Master</h2><p>在读取 RDB 文件的时候，它会主动检测是否有过期的 Key。如果有，那么不会被载入到内存中。</p><h2>Slave</h2><p>在读取 RDB 文件的时候，它不会检测 Key 是否过期，而是都加载到内存中。Slave 服务器中的过期的 Key 会通过 Master 和  Slave 之间的数据同步操作被覆盖掉。从而达到清理的效果。</p><h2>AOF</h2><p>AOF 文件内记录的是我们对数据库的写入命令。当我们上面提到的「惰性删除」和「定期删除」执行的时候，Redis 会向 AOF 文件内对过期的 Key 写入一个 DEL 操作。而这个 DEL 操作也会在 AOF 文件发生重写的时候，与同一个 Key 的写入操作进行抵消。最终 AOF 文件内可能都找不到和这个过期的 Key 有关的写入记录。</p><p>即使没有执行「清理」操作，AOF 文件在重写的时候也会检查每一个 Key 的过期时间，如果发现有过期的 Key 的话，那么这个 Key 的写入操作不会被记录到新的 AOF 文件中。</p><h2>Replication</h2><p>Replication 操作发生在 Master 节点和  Slave 节点之间。无论是以何种方式部署 Redis 集群，写入操作通常来讲都是对 Master 节点执行的，而读取操作，可以在 Master 节点执行，也可以通过 Slave 节点执行。但是由于主从节点之间是依靠 Replication 操作做数据同步的，所以，某一时间段内，主从数据可能会有一些不一致。比如，我们在 Master 节点上触发了惰性删除操作，清理了一个过期的 Key。此时 Master 节点会向其 Slave 节点发送 DEL 命令，通知 Slave 节点删除这个 Key。但是，在 Slave 节点上我们是无法触发惰性删除操作的，也不会执行定期清理的操作。Redis 为了保持数据的一致性，所有对数据的修改操作，都必须由 Master 来完成。修改的部分，要依靠其他机制传递给 Slave 节点。</p>", 
            "topic": [
                {
                    "tag": "Redis", 
                    "tagLink": "https://api.zhihu.com/topics/19557280"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/51777369", 
            "userName": "要没时间了", 
            "userLink": "https://www.zhihu.com/people/bdbe7f88af9a021c5c770649d266f143", 
            "upvote": 11, 
            "title": "Head First Of Golang Channel", 
            "content": "<h2>Channel 的种类</h2><p>在 Golang 中，Channel 作为多个 goroutine 之间通信的桥梁，大致可以分为两类：</p><ul><li>缓冲</li><li>非缓冲</li></ul><p>Channel 在使用之间，必须要通过<code>make</code>来进行创建。</p><h2>非缓冲 Channel</h2><p>若在两个 gorountine 中使用了非缓冲 Channel，就说明在通信双方的中间，留给它们可以存放「通信消息」的位置只有一个。此时，如果该 Channel 中没有消息，那么「读取」操作将会被阻塞。同样，如果该 Channel 中有消息但是没有被取走，那么「写入」操作将会被阻塞。</p><p>在使用了 Channel 之后，通信双方和 Channel 一起可以构成一个相当标准的「生产者—消费者」模型。</p><h2>缓冲 Channel</h2><p>缓冲 Channel 与非缓冲 Channel 唯一不同的地方就在于存放「通信消息」的位置可以有多个。「写入」和「读取」操作被阻塞的条件也随之变成了：</p><ul><li>当缓冲区满的时候，「写入」操作被阻塞</li><li>当缓冲区空的时候，「读取」操作被阻塞</li></ul><h2>Channel 的关闭操作</h2><p>Channel 的操作和队列一样，无非就三个：写入，读取，删除（关闭）。其中写入和读取操作相对来说比较简单。接下来我们看下，如果某个 Channel 不想使用，准备关闭它，需要通过调用一个名为<code>close</code> 的函数。若一个 Channel 已经被关闭，再对它进行一些操作可能会引起不同的结果：</p><ul><li>重复关闭：程序会 panic</li><li>向关闭的 Channel 继续写入数据：程序会 panic</li><li>从关闭的 Channel 中读取数据：程序会正常读取 Channel 中余下的消息。当 Channel 中不再有消息的时候，它会返回一个 Channel 存储消息类型的 0 值。假设我们现在有一个存储 int 类型消息的 Channel。当 Channel 已经关闭之后，我们继续读取它内部的消息。由于 int 的默认值是0，且 Channel 里面也完全有可能有 0 这个数据。所以我们要通过额外的一个变量来判断这个 0 到底是标识 Channel 内没有消息了还是数据就是 0。</li></ul><div class=\"highlight\"><pre><code class=\"language-go\"><span class=\"nx\">msg</span><span class=\"p\">,</span> <span class=\"nx\">ok</span> <span class=\"o\">:=</span> <span class=\"o\">&lt;-</span> <span class=\"nx\">ch</span>\n<span class=\"k\">if</span> <span class=\"p\">!</span><span class=\"nx\">ok</span> <span class=\"p\">{</span>\n    <span class=\"nx\">log</span><span class=\"p\">.</span><span class=\"nf\">Info</span><span class=\"p\">(</span><span class=\"s\">&#34;There is no msg&#34;</span><span class=\"p\">)</span>\n<span class=\"p\">}</span>\n<span class=\"nx\">log</span><span class=\"p\">.</span><span class=\"nf\">Info</span><span class=\"p\">(</span><span class=\"s\">&#34;You get a new msg from channel&#34;</span><span class=\"p\">)</span></code></pre></div><p><code>ok</code> 是一个 bool 类型的变量，它会告诉我们此次读取出来的消息是默认的0值，还是缓冲区内真正保存的内容。</p><h2>Channel 的常用姿势</h2><h2>多个 goroutine 间通信</h2><p>完全可以在 Golang 中将 Channel 当做消息队列来使用。通过 Channel 在多个 goroutine 间共享数据是非常方便的，减少了很多使用者的心智负担，不需要再担心锁的问题。</p><h2>select</h2><p>Golang 中有一个名为<code>select</code>的原语，它的作用类似 Linux 系统调用中的<code>select</code>。我们可以将多个 Channel 的读取或者写入操作放在 <code>select</code> 的 Case 中进行使用。通常情况下，我们用非缓冲的 Channel 和 <code>select</code> 一起实现一个「超时处理机制」：</p><div class=\"highlight\"><pre><code class=\"language-go\"><span class=\"k\">select</span> <span class=\"p\">{</span>\n  <span class=\"k\">case</span> <span class=\"nx\">value</span> <span class=\"o\">:=</span> <span class=\"o\">&lt;-</span> <span class=\"nx\">ch</span><span class=\"p\">:</span>\n        <span class=\"nf\">handle</span><span class=\"p\">(</span><span class=\"nx\">value</span><span class=\"p\">)</span>\n  <span class=\"k\">case</span> <span class=\"nx\">time</span><span class=\"p\">.</span><span class=\"nf\">After</span><span class=\"p\">(</span><span class=\"mi\">5</span> <span class=\"o\">*</span> <span class=\"nx\">time</span><span class=\"p\">.</span><span class=\"nx\">Second</span><span class=\"p\">):</span>\n      <span class=\"nf\">handleTimeout</span><span class=\"p\">()</span>\n<span class=\"p\">}</span></code></pre></div><h2>range</h2><p>range 通常用于遍历一个集合内的元素，它同样可以用于 Channel，无论是缓冲版本还是非缓冲版本。（这里埋了一个小伏笔）。若 Channel 中有数据，那么 range 其实就相当于执行一个「读取」操作。关键是，当 channel 中没有数据的时候，range 它会发生什么情况呢？</p><div class=\"highlight\"><pre><code class=\"language-go\"><span class=\"kn\">package</span> <span class=\"nx\">main</span>\n\n<span class=\"kn\">import</span> <span class=\"p\">(</span>\n    <span class=\"s\">&#34;fmt&#34;</span>\n<span class=\"p\">)</span>\n\n<span class=\"kd\">func</span> <span class=\"nf\">main</span><span class=\"p\">()</span> <span class=\"p\">{</span>\n    <span class=\"nx\">fmt</span><span class=\"p\">.</span><span class=\"nf\">Println</span><span class=\"p\">(</span><span class=\"s\">&#34;Hello, playground&#34;</span><span class=\"p\">)</span>\n    <span class=\"nx\">ch</span> <span class=\"o\">:=</span> <span class=\"nb\">make</span><span class=\"p\">(</span><span class=\"kd\">chan</span> <span class=\"kt\">int</span><span class=\"p\">)</span>\n    <span class=\"k\">go</span> <span class=\"kd\">func</span><span class=\"p\">()</span> <span class=\"p\">{</span>\n        <span class=\"nx\">ch</span> <span class=\"o\">&lt;-</span> <span class=\"mi\">1</span>\n    <span class=\"p\">}()</span>\n\n    <span class=\"nx\">count</span> <span class=\"o\">:=</span> <span class=\"mi\">0</span>\n    <span class=\"k\">for</span> <span class=\"nx\">v</span> <span class=\"o\">:=</span> <span class=\"k\">range</span> <span class=\"nx\">ch</span> <span class=\"p\">{</span>\n        <span class=\"nx\">fmt</span><span class=\"p\">.</span><span class=\"nf\">Print</span><span class=\"p\">(</span><span class=\"nx\">v</span><span class=\"p\">)</span>\n        <span class=\"nx\">count</span><span class=\"o\">++</span>\n        <span class=\"k\">if</span> <span class=\"nx\">count</span> <span class=\"o\">==</span> <span class=\"mi\">1</span> <span class=\"p\">{</span>\n            <span class=\"nb\">close</span><span class=\"p\">(</span><span class=\"nx\">ch</span><span class=\"p\">)</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">}</span></code></pre></div><p>上述实例程序最终会顺利退出。最主要的原因是我们在读取了 Channel 中的消息之后，主动的关闭了这个 Channel。而 range 一个关闭了 Channel 会自动退出循环。如果不执行 close 操作的话，那么程序会报错：dead lock。因为 range 执行的读取操作会一直阻塞在那。</p><h2>Channel 的内部实现</h2><h2>数据结构</h2><p>Channel 的本质其实是一个指针，它指向了一个名为<code>hchan</code>的结构体，所以我们在传递 Channel 变量的时候不需要对其进行取地址操作：</p><div class=\"highlight\"><pre><code class=\"language-go\"><span class=\"kd\">type</span> <span class=\"nx\">hchan</span> <span class=\"kd\">struct</span> <span class=\"p\">{</span>\n    <span class=\"nx\">qcount</span>   <span class=\"kt\">uint</span>           <span class=\"c1\">// total data in the queue\n</span><span class=\"c1\"></span>    <span class=\"nx\">dataqsiz</span> <span class=\"kt\">uint</span>           <span class=\"c1\">// size of the circular queue\n</span><span class=\"c1\"></span>    <span class=\"nx\">buf</span>      <span class=\"nx\">unsafe</span><span class=\"p\">.</span><span class=\"nx\">Pointer</span> <span class=\"c1\">// points to an array of dataqsiz elements\n</span><span class=\"c1\"></span>    <span class=\"nx\">elemsize</span> <span class=\"kt\">uint16</span>\n    <span class=\"nx\">closed</span>   <span class=\"kt\">uint32</span>\n    <span class=\"nx\">elemtype</span> <span class=\"o\">*</span><span class=\"nx\">_type</span> <span class=\"c1\">// element type\n</span><span class=\"c1\"></span>    <span class=\"nx\">sendx</span>    <span class=\"kt\">uint</span>   <span class=\"c1\">// send index\n</span><span class=\"c1\"></span>    <span class=\"nx\">recvx</span>    <span class=\"kt\">uint</span>   <span class=\"c1\">// receive index\n</span><span class=\"c1\"></span>    <span class=\"nx\">recvq</span>    <span class=\"nx\">waitq</span>  <span class=\"c1\">// list of recv waiters\n</span><span class=\"c1\"></span>    <span class=\"nx\">sendq</span>    <span class=\"nx\">waitq</span>  <span class=\"c1\">// list of send waiters\n</span><span class=\"c1\"></span>\n    <span class=\"c1\">// lock protects all fields in hchan, as well as several\n</span><span class=\"c1\"></span>    <span class=\"c1\">// fields in sudogs blocked on this channel.\n</span><span class=\"c1\"></span>    <span class=\"c1\">//\n</span><span class=\"c1\"></span>    <span class=\"c1\">// Do not change another G&#39;s status while holding this lock\n</span><span class=\"c1\"></span>    <span class=\"c1\">// (in particular, do not ready a G), as this can deadlock\n</span><span class=\"c1\"></span>    <span class=\"c1\">// with stack shrinking.\n</span><span class=\"c1\"></span>    <span class=\"nx\">lock</span> <span class=\"nx\">mutex</span>\n<span class=\"p\">}</span></code></pre></div><p>通过注释其实我们就可以大概的了解 Channel 是如何来实现的：</p><p>首先，qcount 和 dataqsiz 分别代表了缓冲区内元素的个数以及缓冲区本身的大小，注释中是通过 queue 来标识的。那就说明，Channel 底层的缓冲区可能是通过一个队列实现的。紧接着，我们看到了三个比较关键的成员：</p><ul><li>buf</li><li>sendx</li><li>recvx</li></ul><p>其中 buf 所指向的内存区域应该就是我们前面说到的 queue 了。而且，我们又发现了<code>sendx</code> 和<code>sendx</code>两个成员。他们似乎标识了缓冲区中读取和写入要操作的索引信息。这三个成员在一起，不得不让我们怀疑，Channel 底层的缓冲区是通过一个「循环队列」的数据结构来实现的。因为 Channel 中既有读取操作也有写入操作，如果一直使用线性增长的缓冲区，势必会造成资源的浪费，并且在元素顺序的整理上也会付出相当大的代价。</p><p>既然是对内存中的数据操作，并且还会涉及到多个 goroutine，所以不可避免的要通过锁来避免「竞争」：lock。另外，之前提到过，如果一个 Channel 的缓冲区没有元素或者已经满了，那么执行读取操作或者写入操作的 goroutine 会被组塞住。而 recvq 和 sendq 就是用来保存它们的。以便在方便的时候将它们唤醒。</p><h2>写入和读取操作</h2><p>Golang 使用 groutine 和 Channel 实现了CSP(Communicating Sequential Processes)模型。在 CSP 模型中有一个非常重要的理念：</p><div class=\"highlight\"><pre><code class=\"language-go\"><span class=\"nx\">不要通过共享内存的方式进行通信</span><span class=\"err\">，</span><span class=\"nx\">要通过通信的方式共享内存</span></code></pre></div><p>这个理念在 Channel 的写入和读取操作上得到了充分展示。</p><p>无论是写入还是读取操作，涉及到 hchan 数据结构中的成员有如下几个：</p><ul><li>lock</li><li>sendx/recvx</li><li>buf</li><li>recvq/sendq</li></ul><h2>非阻塞场景</h2><p>我们先把阻塞写入和读取操作的场景放在一边，只讨论下 Channel 写入和读取数据的过程。因为这是最基本的，也是最主要的部分。</p><h2>写入</h2><p>当我们向一个 Channel 发送一个数据的时候，写入操作会首先尝试获取 lock，将 buf 锁住，防止出现竞争。然后根据 sendx 给出的索引值，将消息 Copy 一份，塞入其中。为什么一定要 Copy 一份呢？直接使用原始数据不行么？很显然，如果我们将原始数据即生产者写入的数据放到 buf 内，这相当于让消费者和生产者共享内存了。这是违背了 CSP 模型提出的那个核心理念的。最后，在写入操作完成的时候，它会释放 lock。</p><h2>读取</h2><p>读取操作是写入操作的逆过程。在获取 Channel 元素的时候，也同样会执行一个 Copy 操作。</p><h2>阻塞场景</h2><p>当 buf 已经存满消息的时候，触发写入操作的 goroutine 将会被阻塞住。此时这个阻塞的 goroutine 将会被存储到我们前面说的 <code>hchan</code>对象的 sendq 成员内。直到 buf 内再次有空位即有读取操作发生之后，该 goroutine 才会被唤醒，执行完之前的写入操作。</p><h2>goroutine—用户态实现的线程模型</h2><p>既然说到了 goroutine 的唤醒与阻塞，我们就不得不回忆起操作系统中线程和进程的相关概念。实际上，goroutine 就是一种 Golang 在用户态下实现的轻量级线程，它由用户态中的 go runtime 进行管理。但是，goroutine 最终还是跑在一个线程上的。因为可被 CPU 调度执行的最小单位是进程创建的线程。goroutine 和 线程的关系类似于「多路复用」，一个线程可能会携带多个 goroutine。Golang Scheduler 将会依照一定的调度策略，将线程和 goroutine 进行绑定，以便线程和 goroutine 一起被 CPU 调度运行。</p><p>线程，goroutine，调度上下文三者之间的关系如下图所示：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-8a870be151b3c4e3c8024a680847b667_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1034\" data-rawheight=\"783\" class=\"origin_image zh-lightbox-thumb\" width=\"1034\" data-original=\"https://pic4.zhimg.com/v2-8a870be151b3c4e3c8024a680847b667_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1034&#39; height=&#39;783&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1034\" data-rawheight=\"783\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1034\" data-original=\"https://pic4.zhimg.com/v2-8a870be151b3c4e3c8024a680847b667_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-8a870be151b3c4e3c8024a680847b667_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>回到刚才那个触发了「阻塞」操作的 goroutine，我们将它标记为 G1。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-2126184b3d24269fe5cc15821677acdf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"697\" data-rawheight=\"408\" class=\"origin_image zh-lightbox-thumb\" width=\"697\" data-original=\"https://pic4.zhimg.com/v2-2126184b3d24269fe5cc15821677acdf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;697&#39; height=&#39;408&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"697\" data-rawheight=\"408\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"697\" data-original=\"https://pic4.zhimg.com/v2-2126184b3d24269fe5cc15821677acdf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-2126184b3d24269fe5cc15821677acdf_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>此时它将从 M 上被调度下来，放入一个等待队列中，即我们上面提到过的 sendq 内。Golang Scheduler 将会从 runable queue 中取出一个可执行的 goroutine，将其调度到线程上执行。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-61f82ffd1245ac009bc492ec829d66a4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1036\" data-rawheight=\"785\" class=\"origin_image zh-lightbox-thumb\" width=\"1036\" data-original=\"https://pic1.zhimg.com/v2-61f82ffd1245ac009bc492ec829d66a4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1036&#39; height=&#39;785&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1036\" data-rawheight=\"785\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1036\" data-original=\"https://pic1.zhimg.com/v2-61f82ffd1245ac009bc492ec829d66a4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-61f82ffd1245ac009bc492ec829d66a4_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>如果有另外一个 goroutine 从 Channel 中读取一个消息，当消息成功的被读取之后，这次「读取」操作仍然没有结束。它还会执行接下来的两个步骤：</p><ol><li>从 sendq 中将 G1 要写入的元素直接塞入 queue 中</li><li>通过使用 Golang Scheduler 提供的一些接口，将 G1 塞入我们上面提到的 runnable queue 中</li></ol><p>上面我们提到的是写入操作被阻塞，接下来我们来看看当读取操作被阻塞的时候会发生什么。</p><p>读取操作被阻塞过程的前半部分和写入操作一样</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-f1de4c73c7e71eb0488b892d6ffff309_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1018\" data-rawheight=\"755\" class=\"origin_image zh-lightbox-thumb\" width=\"1018\" data-original=\"https://pic2.zhimg.com/v2-f1de4c73c7e71eb0488b892d6ffff309_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1018&#39; height=&#39;755&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1018\" data-rawheight=\"755\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1018\" data-original=\"https://pic2.zhimg.com/v2-f1de4c73c7e71eb0488b892d6ffff309_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-f1de4c73c7e71eb0488b892d6ffff309_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>一共发生了三件事：</p><ol><li>创建一个 sudog 对象，保存 G2</li><li>将 sudog 对象塞入 recvq 中</li><li>Golang Scheduler 将 G2 调离线程，从 runnable queue 中找一个新的且可运行的 goroutine 调度执行</li></ol><p>按照上面「读取操作唤醒写入操作」过程的描述，写入操作 G1 应该是先获取锁，然后将消息写入 buf 中，然后释放锁。最终将 G2 唤醒。G2 下次再被调度执行的时候就可以拿到 buf 中的消息了。但是，Golang 对这一过程进行了优化：</p><ol><li>去掉 G1 对锁的操作</li><li>直接将 G1 当中要写入的数据塞入 G2 sudog 中的 elem 成员</li></ol><p>第一个操作不用说了，减少对锁的操作肯定会提升程序的执行效率。第二个操作避免了 G2 再一次访问 buf，也提升了程序运行的效率。</p>", 
            "topic": [
                {
                    "tag": "Go 语言", 
                    "tagLink": "https://api.zhihu.com/topics/19625982"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/51665648", 
            "userName": "要没时间了", 
            "userLink": "https://www.zhihu.com/people/bdbe7f88af9a021c5c770649d266f143", 
            "upvote": 2, 
            "title": "Detect Source Code of Statefulset Controller", 
            "content": "<blockquote>由于文章字数超过专栏的限制，大家可以移步我的 blog 观看。<br/><br/>原文链接：</blockquote><a href=\"https://link.zhihu.com/?target=http%3A//littledriver.net/post/2018/12/05/detect-source-code-of-statefulset-controller/\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic2.zhimg.com/v2-67bcb8cfba758fa046758626163e53b5_ipico.jpg\" data-image-width=\"950\" data-image-height=\"810\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Detect Source Code of Statefulset Controller</a><p><br/> </p>", 
            "topic": [
                {
                    "tag": "Kubernetes", 
                    "tagLink": "https://api.zhihu.com/topics/20018384"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/51665604", 
            "userName": "要没时间了", 
            "userLink": "https://www.zhihu.com/people/bdbe7f88af9a021c5c770649d266f143", 
            "upvote": 1, 
            "title": "Detect StatefulSet", 
            "content": "<p></p><h2>什么是 StatefulSet？</h2><p>StatefulSet 是 Kubernetes 提供一种资源对象。它适合用来管理我们的用状态服务。管理的内容包括部署，升级，扩容等。</p><h2>StatefulSet 和 Deployment 有什么不同？</h2><p>其实 StatefulSet 和 Deployment 都是 Kubernetes 提供的资源对象。但是，Deployment 更适合帮我们管理无状态服务。所以，两者的区别主要是在于管理 Pod 的方式上。说的更加明确一点，两者行为上的不同主要是因为 Controller 对同一个操作实现逻辑的不同（部署，升级，扩容等）。</p><p>另外一个实现细节上面的不同之处是，StatefulSet 直接管辖的是 Pod，而 Deployment 和 Pod 之间还会有一个 ReplicaSet。</p><h2>StatefulSet 为什么适合管理有状态服务？</h2><p>我们先来看看有状态服务和无状态服务有什么区别。</p><h2>有状态服务</h2><p>「状态」指的是一些应用在对客户提供服务的过程中必要的一些信息，并且这些信息是需要被保存的。可以将它简单理解为一个计算的「中间结果」。有状态服务指的就是依赖这些信息对外提供服务的应用。</p><h2>无状态服务</h2><p>无状态的应用在对外提供服务的时候，将不会依赖任何持久化的数据。绝大部分它需要的信息都来自于网络请求，或者说内存中的一些信息。即使应用在对外服务的过程当中有重启行为，内存或者请求的信息丢失，也不需要担心。</p><h2>如何管理</h2><p>StatefulSet 为有状态服务提供了以下几个机制来保证它们部署，扩容和升级的正常进行：</p><ol><li>稳定的存储</li><li>集群唯一的标识符</li><li>操作的顺序性</li></ol><p>其中第一个措施主要是用来解决「状态」的存储问题的。在 StatefulSet 中，若一个 Pod 创建一个存储设备。即使之后 Pod 和 Statefulset 被删除，这个存储设备已经它里面保存的「状态」信息仍旧会被保存下来。除非我们手动的将这些存储设备删除（存储设备包含 PVC，PV，以及底层的存储实例）。</p><p>第二个措施是和 Deployment 区别比较大的一点。对于一个无状态服务的集合来说，哪一个实例对外提供服务都是 ok 的，因为他们提供的服务不依赖任何状态。但是有状态服务就不一样了：比如 A 实例依赖 B 实例的输出，B 实例又依赖 C 实例的输出。所以很多时候，有状态服务接收的流量可能仅来自于其中一个实例。既然有这种需求，那么我们就必须要有一种明确的方式来标识 Statefulset 中管理的每一个实例（Pod）。这种标识符的生成有一套特殊的规则。</p><p>第三个措施主要是针对有状态服务部署，升级，销毁三个操作的。有状态服务一般都对实例部署的顺序和销毁的顺序有一些要求。比如，A 如果不启动，B 就无法启动，因为 B 在启动时依赖了 A 写入的一些状态信息。同理，升级和销毁操作也是这样。</p><h2>StatefulSet Pod 标识符的生成规则</h2><p>假设现在有一个构造了3副本的 StatefulSet 对象，该对象的名字为 web。那么StatefulSet 首先会为这三个副本都标一个序号，从0~N-1。</p><ul><li>web-0</li><li>web-1</li><li>web-2</li></ul><p>Kubernetes 将其称为 Ordinal Index。</p><p>根据我们上面说过的，一个 StatefulSet 中不同的副本可能同一时间只能有一个实例对外提供服务，其余的两个都作为冗余，旨在为服务提供高可用的能力。所以，在网络通信的场景下， 这三个 Pod 也需要一个唯一的网络标识，这个唯一的网络标识，就是就是 Kubernetes 集群内部的  Pod DNS Name。它的生成规则如下：</p><div class=\"highlight\"><pre><code class=\"language-text\">web{0-N-1}.$(service name).$(namespace).svc.cluster.local</code></pre></div><p>其中 service name 来自于 StatefulSet 的  Service，namespace 也是根据 StatefulSet 这个对象所在的 ns 来确定的。</p><p>Kubernetes 将其称为 Stable Network ID。</p><p>除此之外，如果在 Pod.Spec 中还写入了 PVC 创建存储设备。那么 StatefulSet 还会为每一个 Pod 的 PVC 以及 PV 也生成一个唯一的标识。生成的规则如下:</p><div class=\"highlight\"><pre><code class=\"language-text\">pvcName-web{0-N-1}</code></pre></div><h2>部署与升级</h2><p>StatefulSet 的部署顺序将严格遵循以下几个原则：</p><ol><li>部署顺序和 Pod Ordinal Index 的顺序一致，从 0~N-1</li><li>删除顺序和 Pod Ordinal Index 的顺序相反，从 N-1~0</li><li>一个 Pod 开始部署之前，它前面的 Pod 必须处于 Running 和 Ready 状态。相反，一个 Pod 中止之前，它前面的 Pod 必须已经被中止</li></ol><p>对于第三点来说，这种执行的顺序性不仅限于 Ordinal Index 相邻的两个 Pod。如一个三副本的 StatefulSet 部署，pod-0，pod-1 已经部署成功。但是在 pod-2  部署的时候，pod-0 挂了，此时 Pod-2 不会开始部署，因为它的前置 Pod 有异常。</p><p>StatefulSet 自身提供了两种升级策略：OnDelete， RollingUpdate</p><h2>OnDelete</h2><p>当我们给 StatefulSet 配置了这种升级策略的时候，即使我们改动了 StatefulSet 的 template 中的信息，它也不会自动进行升级操作。我们需要手动的删除 Pod。这些 Pod 在重建之后，会自动应用最新的配置。这种方式的灵活性是比较大的，按照什么样的顺序升级，哪个 Pod 升级，哪个 Pod 不升级都可以由我们自己来控制。</p><h2>RollingUpdate</h2><p>StatefulSet 将会自动的按照 Pod 终止的顺序进行升级（从 N-1~0）。另外，在 RollingUpdate 这个策略上，StatefulSet 还额外的提供了一个 Partition 的功能，用于分阶段的升级。Partition 是一个 Pod Ordinal Index，它是 StatefulSet.Spec.updateStrategy.rollingUpdate 下的一个字段。</p><p>假设我们现在有一个三副本的 StatefulSet，当 partition 指定为1的时候，那么只有 pod-1 和 pod-2 才会因为 template 中的信息的更新而进行升级，升级顺序还是按照  Ordinal Index 的逆序。而 pod-0 仍然会以旧的版本运行，即使发生重启。如果你的 partition 设置的值已经超过了 Ordinal Index 的范围，那么将没有 Pod 会升级。</p>", 
            "topic": [
                {
                    "tag": "Kubernetes", 
                    "tagLink": "https://api.zhihu.com/topics/20018384"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/51376044", 
            "userName": "要没时间了", 
            "userLink": "https://www.zhihu.com/people/bdbe7f88af9a021c5c770649d266f143", 
            "upvote": 14, 
            "title": "Docker Container 的日志收集/管理", 
            "content": "<blockquote>更好的阅读体验请移步我的 blog：</blockquote><a href=\"https://link.zhihu.com/?target=http%3A//littledriver.net/post/2018/12/02/detect-logging-of-docker/\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">littledriver.net/post/2</span><span class=\"invisible\">018/12/02/detect-logging-of-docker/</span><span class=\"ellipsis\"></span></a><h2>写在前面</h2><p>运行在容器内部的应用会在运行期间产生大量的日志，这些日志将作为我们 Debug，分析应用行为的重要依据。本文将带大家了解一下 Docker 面对的和「容器日志」有关的问题以及它的解决方案。</p><h2>日志种类</h2><p>通常来讲，容器内运行的应用会以两种方式输出日志：</p><ol><li>标准输出</li><li>日志文件</li></ol><h2>日志需求</h2><p>我们对于容器内应用输出的日志的需求比较简单，基本上可以归结为以下几点：</p><ol><li>易使用：可以方便的通过 docker logs 或者 kubectl logs 等命令行查看容器内部的日志信息</li><li>易收集：可以很容易的找到容器内部日志最终持久化的位置，如某个文件或者说某个存储服务</li><li>自处理：当日志容量达到一定额度的时候，可以通过一些「滚动操作」将旧的日志清除掉一部分，维持一个固定大小的日志文件</li><li>可分析：可以方便的将所有的日志（包括被滚动掉的）内容送入第三方的日志分析服务，以便能够以更加有效的方式通过对日志的分析而了解服务的运行情况</li></ol><h2>Docker 的容器日志处理方案</h2><h2>标准输出日志</h2><h2>实质</h2><p>回想一下 Docker 的实现原理，无非是「多进程」+「隔离机制」。其中所有容器（子进程）都是由 dockerdaemon（父进程）来创建的。父进程可以收集子进程 PID Namespace 内 PID=1 进程的和标准输出的内容。因为只有 PID = 1的进程才是父进程创建的。如果该 PID = 1的子进程再创建孙子进程的话，父进程是无法收集到孙子进程内的标准输出的内容的。</p><p>父进程是通过 Pipe 来获取子进程的和标准输出的。所以，在容器进程被创建的时候，会通过一个 Pipe 将输出到标准输出的日志信息传递给父进程。而这一切对容器内部应用都是透明的。</p><h2>Docker Log Driver</h2><p>当 dockerdaemon 进程从 Pipe 中拿到日志信息之后，会将它交给一个特殊的模块来进行处理—— Docker Log Driver。Log Driver 的职责也很简单，既然收集到了日志信息，那肯定需要将它写入到一个位置，这个位置可以是一个 JSON 格式的文件（默认行为），也可以是一个有特殊含义的文件，如 syslog，甚至是一个第三方日志的收集服务。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-8644f39f3a3a6b61f0ac96cbe777c6cd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"816\" data-rawheight=\"298\" class=\"origin_image zh-lightbox-thumb\" width=\"816\" data-original=\"https://pic2.zhimg.com/v2-8644f39f3a3a6b61f0ac96cbe777c6cd_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;816&#39; height=&#39;298&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"816\" data-rawheight=\"298\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"816\" data-original=\"https://pic2.zhimg.com/v2-8644f39f3a3a6b61f0ac96cbe777c6cd_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-8644f39f3a3a6b61f0ac96cbe777c6cd_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>Log Driver 写入的位置，可以通过很灵活的方式进行配置。可以在 dockerdaemon 启动的时候通过 —log-driver 参数配置，也可以在每个容器启动的时候用同样的参数配置。容器级别的 LogDriver 配置会覆盖全局的。</p><h2>生产环境中的问题——真正打日志的服务不是 dockerdaemon 的子进程</h2><h2>问题</h2><p>在基于 Kubernetes 平台开发数据库应用的时候，为了能够实现一些「自运维」的功能，通常在一个容器内，我们会以一个baseImage 为基础，再启动我们的「业务进程」（负责自运维）和数据库进程（redisd，mysqld 等等）。此时，以 dockerdaemon 进程的视角来看的话，它的子进程就不是数据库进程或者说是我们的业务进程了。因为 baseImage 的存在，它通常在容器启动的时候会顺便启动一些辅助进程，帮我们在容器内部搭建一个较好的运行环境。但是，我们想要观察的日志大部分都来自于业务进程和数据库进程。</p><h2>解决思路</h2><p>针对上面的问题，并且结合 Docker 收集容器日志的原理。我们想出了以下的解决方案：</p><ol><li>将容器内部我们想收集的来自标准输出的日志都先塞入到一个容器内部的文件中，如 syslog</li><li>利用容器内部的其他服务，将 syslog 文件内容再重定向到标准输出</li></ol><h2>解决方案</h2><ol><li>使用 <a href=\"https://link.zhihu.com/?target=https%3A//github.com/phusion/baseimage-docker\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">GitHub - phusion/baseimage-docker: A minimal Ubuntu base image modified for Docker-friendliness</a> baseImage 中内置的一个名为 syslog-ng 的服务，将 syslog 的内容重定向到标准输出</li><li>通过 logrus 第三方日志库，加入日志输出的 hook。将业务进程内部输出的日志重定向到 syslog</li><li>开启 Redis Server 自身提供的，将服务日志输出到 syslog 的功能</li></ol><p>通过对上面关于「标准输出日志」遇到问题的解决方案的了解，我们可以发现，它其实有如下的几个风险：</p><ol><li>syslog 文件的内容可能会无限增大，占用容器的存储资源（此问题在 log driver 将日志保存在宿主机文件的情况下，也同样存在）</li></ol><h2>日志滚动</h2><p>在宿主机上，Docker 将会帮我们解决问题1。若你使用 json-file 这种 logdriver 的时候，会让你配置一个日志文件的大小。一旦达到这个阈值，Docker 会通过一些日志滚动服务来删除掉一部分旧的日志，从而保持日志文件一个稳定的大小。</p><p>而在容器内部我们就得自己想办法了。之前提到过在生产环境中，我们在容器内部使用了 baseImage。其内部内置的一个服务叫：logrotate。他会帮我们针对某个特定的文件做「滚动」操作。</p><h2>对上述解决方案的一点思考</h2><p>其实上述方案的本质是：用文件日志作为服务日志和容器标准输出之间的桥梁，虽然容器内部是文件日志，但是是通过标准输出向外传递的。而容器内的文件日志可以通过 baseImage 提供的 syslog-ng 和 logrotate 服务来进行日志的重定向和文件日志的滚动。这是在我们没有成熟的收集容器内部文件日志的机制的前提下，想出来的一种解决方案。它复用了 Docker 提供的一些现有的机制。</p><h2>再抽象一点——Docker 的日志管理</h2><p>容器日志管理的本质是「如何处理 stdout」 和「如何将我关心的日志都打入 stdout」。这两个问题的出现，其实是受限于 Docker 对于容器日志的管理方式（通过父子进程 pipe 和 log driver）。</p><h2>能否对上述方案做一些优化？</h2><p>上面的方案其实已经能解决容器日志所面临的大部分问题了。但是仍旧有个做的不是很好的点：baseImage 集成了太多了服务（如 logrotate 和 syslog-ng）。其实，对于 baseImage，你可以说这是优点，也可是说这是缺点。优点是方便，缺点就是可能会和业务进程抢占资源。</p><p>既然现有方案上有缺点，我们就需要解决它。思路很简单：什么进程可能会抢占业务进程的资源，就把谁拿出这个容器。</p><ol><li>将 logrotate 和 syslog-ng 等服务放入另外一个容器（但是在一个 Pod 内，相当于一个 sidecar container）</li><li>业务容器通过第三方日志库或者其他方式的支持，将 业务进程的日志通过网络通信（UDP） 传递给「日志服务容器」内的服务</li><li>在日志服务容器内部，可重复上述：日志打入 syslog 然后再从定向到标准输出的过程</li><li>集群级别部署一个高可靠的第三方日志收集组件，收集来自 logdriver（即标准输出）的日志内容</li></ol><p>这种方案，虽然在资源用量上面没有什么优化。但是却将日志服务和业务进程从一个容器中分开来，两者申请的资源也可以分别控制，互不影响。</p><h2>如果没有 syslog-ng 和 logrotate</h2><p>其实大部分容器用户，它们对 baseImage 的要求不是很高。甚至有的时候他们希望 baseImage 是尽量干净，尽量小的。如果 baseImage 中没有我们上面说到的 syslog-ng 和 logrotate 服务的话，容器内的日志收集问题就比较麻烦（在业务进程仍然不是 pid=1的情况下）。</p><p>通常情况下，容器内部的业务进程会将日志打入容器内部的一个文件中。所以，我们将面临两个问题：</p><ol><li>日志文件内容的滚动</li><li>如何确认日志文件在宿主机的具体路径</li></ol><p>引发这两个问题的根源是相同的：我们由处理标准输出的内容变成了处理文件内容。对于「标准输出」，有 logdriver 负责日志的滚动和最终日志文件在宿主机上的存储位置。而对于「文件」，就必须要我们自己来处理了。</p><h2>文件日志</h2><h2>解决方案</h2><h2>尝试1——每个容器内部配备日志收集进程</h2><p>最直接的想法就是在每个业务容器中配备一个用于收集日志的进程，它负责将容器内部的日志压缩文件上传到某个地方。但是这样做的缺点也很明显：</p><ol><li>每个容器都必须要起一个日志收集进程，1w 个容器就是1w 个收集进程，对资源会有浪费</li><li>「上传」操作难以控制，一旦有网络的参与，尤其是内外网的交互，传输可靠性风险极大</li><li>收集日志的进程可能需要做一些自定义的配置，如间隔多久收集一次等等。这些都需要额外的开发工作</li></ol><h2>尝试2——每个容器集群配备一个日志收集服务（进程）</h2><p>既然每个容器一个收集程序有诸多缺点，那我们就借助开源社区的力量，找一个强大的日志收集组件，将它部署到集群中。这样一来整个集群只有一个日志收集服务。但是这个方案，也同样需要解决几个和日志路径相关的问题：</p><ol><li>Docker 依赖的底层存储方案不同，导致容器运行镜像时候的 top layer 的映射规则五花八门。你不知道容器的 top layer 实际映射到了物理机上的哪个目录。比如 AUFS 这种实现方案，映射关系很复杂，我在了解 AUFS 原理的时候，在这部分花了大量的时间。这种映射关系及时有，估计文档化也比较差，我一直没有找到一个很官方的解释。不了解映射关系就直接导致了：你的日志收集服务不知道到哪里去收集日志</li><li>即使日志路径的映射关系确定了，这部分信息如何通知给集群级别的日志收集服务？</li></ol><p>针对于问题1，Docker 为我们提供了原生的解决方案：在容器启动的时候，将 top layer 通过显示声明的方式挂载到一个宿主机目录下。通过<code>-v</code> 参数即可实现。</p><p>针对于问题2，Docker 同样也为我们提供了帮助：集群级别的日志收集服务可以通过和宿主机 docker 通信的方式，监听容器的各类事件（创建，删除等）。这些事件内部将会包含我们上面提到的容器的挂载信息。</p><p>如果能找到一个比较好的日志收集组件的话，那么此方案会被很好的支持。并且，有了容器的 label以及对容器各类事件的监听，日志收集服务可以很容易的对功能进行扩展。</p><p>另外，对于日志文件滚动的的问题。既然是一个单独的日志收集服务来负责容器内文件日志的处理，那么这部分功能理应交由它来实现。该功能要至少满足以下两个需求：</p><ol><li>回滚的策略可配置</li><li>「判定是否满足回滚策略」的信息可以很方便的获取到（如监听容器的各类事件）</li></ol><h2>总结</h2><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ae58b43269850eded5b678ff0e6a9133_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1658\" data-rawheight=\"928\" class=\"origin_image zh-lightbox-thumb\" width=\"1658\" data-original=\"https://pic4.zhimg.com/v2-ae58b43269850eded5b678ff0e6a9133_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1658&#39; height=&#39;928&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1658\" data-rawheight=\"928\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1658\" data-original=\"https://pic4.zhimg.com/v2-ae58b43269850eded5b678ff0e6a9133_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-ae58b43269850eded5b678ff0e6a9133_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "Docker", 
                    "tagLink": "https://api.zhihu.com/topics/19950993"
                }, 
                {
                    "tag": "日志分析", 
                    "tagLink": "https://api.zhihu.com/topics/19615655"
                }
            ], 
            "comments": [
                {
                    "userName": "ltye", 
                    "userLink": "https://www.zhihu.com/people/34a505ffe832f069a52b96d36b1206f7", 
                    "content": "其实logrotate已经是不再被推荐的东西，而且也不够轻量，并不适合容器的场景，特别是现在越来越多的以alpine封装base image的情况。这些问题和复杂性也没必要自己去设计和处理，用社区成熟的应用即可，比如s6 log，可以实现标准输出重定向至文件、日志切割，性能好且足够轻量，因为是组件所以配置起来也简单。", 
                    "likes": 3, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/51183153", 
            "userName": "要没时间了", 
            "userLink": "https://www.zhihu.com/people/bdbe7f88af9a021c5c770649d266f143", 
            "upvote": 1, 
            "title": "Detect the source code of Scheduler", 
            "content": "<p>由于文章字数过多，专栏无法上传，可以移步我的 blog 观看：</p><a href=\"https://link.zhihu.com/?target=http%3A//littledriver.net/post/2018/11/29/detect-source-code-of-scheduler/\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-712308ae0a6112062c1ed230b2c22248_180x120.jpg\" data-image-width=\"928\" data-image-height=\"236\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Detect Source Code of Scheduler</a><p></p>", 
            "topic": [
                {
                    "tag": "Kubernetes", 
                    "tagLink": "https://api.zhihu.com/topics/20018384"
                }, 
                {
                    "tag": "调度算法", 
                    "tagLink": "https://api.zhihu.com/topics/19795728"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50766026", 
            "userName": "要没时间了", 
            "userLink": "https://www.zhihu.com/people/bdbe7f88af9a021c5c770649d266f143", 
            "upvote": 1, 
            "title": "Deep Into Recursion", 
            "content": "<p></p><h2>浅显的理解</h2><p>递归是一个在数据结构中非常基本的思想，也是一个非常重要的编程技巧。很多复杂的数据结构的操作其核心思想都是递归。递归的特点之一就是简洁，看起来非常的清晰。另外一个特点就是「绕」，因为它在一直的进行「嵌套」。</p><h2>递归与递推</h2><p>和递归思想相对应的就是递推。这么对比着来思考这两个东西是非常有趣的。递归贯彻的是一种「先切割，后合并」的思想，而递推更强调「积少成多」。两种思想是截然相反的，它们是彼此的「逆过程」。所以，很多时候，递归和递归的代码是可以相互转化的。</p><h2>递归的特点</h2><p>通过上面和递推的思想进行对比，我们可以知道，递归有两个要素：</p><ol><li>待求解的问题可以被分割为多个小问题</li><li>多个小问题的结果经过合并就是带求解问题的结果</li></ol><p>在了解了上述两个要素之外，这里还有一个隐含的因素：待求解问题必须是可分割的，而且是有边界的，不能一直分割下去。所以，递归的三个特点也就呼之欲出了：</p><ol><li>待求解问题可分为个数量有限的子问题</li><li>多个子问题的求解思路一致，可重复</li><li>递归过程一定要有一个终止条件，即最小子问题</li></ol><h2>如何编写递归代码</h2><p>无论是递归，还是动态规划，都隐含了「由小见大」的思想。所以，对这类问题，重中之重就是推导出求解公式。除此之外，就是写出一个可靠的边界条件，以便在有限的步骤内求出问题的解。最后才是代码上面的实现。</p><p>以斐波那契数列为例：</p><h2>公式</h2><p>Fn=Fn-1+Fn-2（n&gt;=2，n∈N*）</p><h2>边界条件</h2><p>F0=0，F1=1</p><h2>代码</h2><div class=\"highlight\"><pre><code class=\"language-go\"><span class=\"kd\">func</span> <span class=\"nf\">Fibonacci</span><span class=\"p\">(</span><span class=\"nx\">n</span> <span class=\"kt\">int</span><span class=\"p\">)</span> <span class=\"kt\">int</span> <span class=\"p\">{</span>\n    <span class=\"k\">if</span> <span class=\"p\">(</span> <span class=\"nx\">n</span> <span class=\"o\">==</span> <span class=\"mi\">1</span> <span class=\"p\">)</span> <span class=\"k\">return</span> <span class=\"mi\">1</span>\n  <span class=\"k\">if</span> <span class=\"p\">(</span> <span class=\"nx\">n</span> <span class=\"o\">==</span> <span class=\"mi\">0</span> <span class=\"p\">)</span> <span class=\"k\">return</span> <span class=\"mi\">0</span>\n\n    <span class=\"k\">return</span> <span class=\"nf\">f</span><span class=\"p\">(</span><span class=\"nx\">n</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"nf\">f</span><span class=\"p\">(</span><span class=\"nx\">n</span><span class=\"o\">-</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n<span class=\"p\">}</span></code></pre></div><h2>递归的风险</h2><p>虽然我们可以通过「认真」来规避递归由于代码编写上带来的问题。但是，有些问题，可能是递归与生俱来的。</p><h2>爆栈</h2><p>在使用递归的时候，一般都是通过函数嵌套调用的形式。函数的嵌套调用，根据之前我们对堆栈的了解，每一次调用都会分配一定空间的函数栈，以便能够存储一些临时变量。如果此时嵌套层数过多，那么就可能会引起堆栈溢出的情况。</p><h2>重复计算</h2><p>编写递归逻辑非常容易出错，甚至有的时候效率会出奇的低。因为在递归计算的过程中，我们的计算步骤有的时候可能会重合。以上面说到的斐波那契数列为例：假设现在 n = 4。</p><ol><li>F(4)= F(3) + F(2)</li><li>F(3) = F(2) + F(1)</li><li>…</li></ol><p>我们发现F(2)其实是被重复计算了，这其实是没必要的。但是如果你将整个递归过程写完，就可以发现 F(1) F(0) 虽然也被重复用到，但是他们仅仅只是一个赋值操作，不会有额外的消耗。借鉴这种思想，我们应该将已经计算出来的结构保存起来，如果之前计算过，那么就用之前的结果。我习惯把这种技巧叫做「记忆化搜索」</p><div class=\"highlight\"><pre><code class=\"language-go\"><span class=\"nx\">memory</span> <span class=\"o\">:=</span> <span class=\"nb\">make</span><span class=\"p\">(</span><span class=\"kd\">map</span><span class=\"p\">[</span><span class=\"kt\">int</span><span class=\"p\">]</span><span class=\"kt\">int</span><span class=\"p\">)</span>\n<span class=\"kd\">func</span> <span class=\"nf\">Fibonacci</span><span class=\"p\">(</span><span class=\"nx\">n</span> <span class=\"kt\">int</span><span class=\"p\">)</span> <span class=\"kt\">int</span> <span class=\"p\">{</span>\n    <span class=\"k\">if</span> <span class=\"p\">(</span> <span class=\"nx\">n</span> <span class=\"o\">==</span> <span class=\"mi\">1</span> <span class=\"p\">)</span> <span class=\"k\">return</span> <span class=\"mi\">1</span>\n  <span class=\"k\">if</span> <span class=\"p\">(</span> <span class=\"nx\">n</span> <span class=\"o\">==</span> <span class=\"mi\">0</span> <span class=\"p\">)</span> <span class=\"k\">return</span> <span class=\"mi\">0</span>\n\n  <span class=\"nx\">oldValue</span><span class=\"p\">,</span> <span class=\"nx\">ok</span> <span class=\"o\">:=</span> <span class=\"nx\">memory</span><span class=\"p\">[</span><span class=\"nx\">n</span><span class=\"p\">]</span>\n  <span class=\"k\">if</span> <span class=\"nx\">ok</span> <span class=\"p\">{</span>\n        <span class=\"k\">return</span> <span class=\"nx\">value</span>\n  <span class=\"p\">}</span>\n\n    <span class=\"nx\">newValue</span> <span class=\"o\">:=</span> <span class=\"nf\">f</span><span class=\"p\">(</span><span class=\"nx\">n</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"nf\">f</span><span class=\"p\">(</span><span class=\"nx\">n</span><span class=\"o\">-</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n  <span class=\"nx\">memory</span><span class=\"p\">[</span><span class=\"nx\">n</span><span class=\"p\">]</span> <span class=\"p\">=</span> <span class=\"nx\">newValue</span>\n  <span class=\"k\">return</span> <span class=\"nx\">newValue</span>\n<span class=\"p\">}</span></code></pre></div><h2>由递归至递推</h2><p>上面已经说到了递归和递推的区别，并且已经知道它们的逻辑可以相互转换。还是以斐波那契数列为例：</p><div class=\"highlight\"><pre><code class=\"language-go\"><span class=\"kd\">func</span> <span class=\"nf\">Fibonacci</span><span class=\"p\">(</span><span class=\"nx\">n</span> <span class=\"kt\">int</span><span class=\"p\">)</span> <span class=\"kt\">int</span> <span class=\"p\">{</span>\n   <span class=\"nx\">prepre</span><span class=\"p\">,</span> <span class=\"nx\">pre</span> <span class=\"o\">:=</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span>\n   <span class=\"kd\">var</span> <span class=\"nx\">result</span> <span class=\"kt\">int</span>\n   <span class=\"k\">for</span> <span class=\"nx\">i</span> <span class=\"o\">:=</span> <span class=\"mi\">2</span><span class=\"p\">;</span> <span class=\"nx\">i</span> <span class=\"o\">&lt;=</span> <span class=\"nx\">n</span><span class=\"p\">;</span> <span class=\"nx\">i</span><span class=\"o\">++</span> <span class=\"p\">{</span>\n        <span class=\"nx\">result</span> <span class=\"p\">=</span> <span class=\"nx\">pre</span> <span class=\"o\">+</span> <span class=\"nx\">prepre</span>\n      <span class=\"nx\">prepre</span> <span class=\"p\">=</span> <span class=\"nx\">pre</span>\n      <span class=\"nx\">pre</span> <span class=\"p\">=</span> <span class=\"nx\">result</span>\n     <span class=\"p\">}</span>\n<span class=\"p\">}</span></code></pre></div><p>对边上面递归的过程就可以知道，递归是「由大到小」，递推是「由小到大」</p><h2>重中之重—递归的思想</h2><p>其实，在之前学习递归的过程中，我有一个很大的误区，就是喜欢手推递归的过程，尤其是那种很冗长的，比如汉诺塔。因为我觉得只有我了解了它每一个步骤，我才真正理解了递归的过程。但其实，这是一个非常大的思维误区。打个比方吧：乔布斯好不容易把IPhone 做的那么轻薄，你偏要给它带个厚重的手机壳。递归之所以看起来简洁，执行起来复杂，是因为人的大脑是不适合一直重复的做一个动作的，如果硬是要这样，几次就会把你绕晕。所以我们才将这种工作交由计算机来处理。递归本身是帮助我们减轻思维负担的，结果因为一些「程序员思维」的坏处，反而没有享受到这种好处。</p><p>想要更有效的使用递归，你必须建立一种思想：在求解一个大问题的时候，想象它分割成的小问题已经解决了。在其小问题已经解决情况下，大问题能不能解决才是我们需要考虑的。如果能够解决，那直接按照上面的步骤，写出递归逻辑即可，而不用再给自己找麻烦了。</p><p>这种思维看起来是在自己骗自己，但其实是一种更高阶的抽象思维。站在全局的角度看问题，而不是死磕一个细节，可能也正是大部分程序员需要改正的地方吧。</p>", 
            "topic": [
                {
                    "tag": "递归", 
                    "tagLink": "https://api.zhihu.com/topics/19631498"
                }, 
                {
                    "tag": "数据结构", 
                    "tagLink": "https://api.zhihu.com/topics/19591797"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50766007", 
            "userName": "要没时间了", 
            "userLink": "https://www.zhihu.com/people/bdbe7f88af9a021c5c770649d266f143", 
            "upvote": 1, 
            "title": "Deep Into Array and Linklist", 
            "content": "<p></p><p></p><h2>数组</h2><p>数组是线性数据结构的一种实现方式。当提到数组的时候，我可能会想到以下几个点：</p><ol><li>顺序存储</li><li>连续的内存空间</li><li>O（1）时间复杂度快速访问元素，O（N） 时间复杂度删除元素</li><li>固定大小</li><li>能更好的利用 CPU 的高速缓存机制（CPU 加载内存当中的内容至高速缓存会利用「就近原则」）</li><li>存储数据的类型必须相同</li></ol><h2>如何实现高效的随机访问</h2><p>其中第三点提到的 O（1）时间复杂度访问元素，是数组这个数据结构特有的一个优势。之所以能达到这个效果，来自于第一点和第六点的两个限制。因为数组的元素说到底都是存储在内存中的，既然在内存中就会有地址来标识数据所处的位置。而数组中又限制只能存储相同数据类型的元素，所以我们很轻松的就可以通过一个公式计算出数组中某个元素的内存地址，进而直接通过这个地址去访问它。</p><blockquote> element_address = base_address + data_type_size * i    计算第 i+1个元素的内存地址  <br/> </blockquote><p>在这里需要特别注意一点的是，数组高效的操作是「访问」而不是「查找」。万不可将两个操作混淆。访问包括计算内存地址+取值两个小操作。而查找则是需要通过一定的策略去遍历数组中的元素的。</p><h2>为什么数组的「插入」和「删除」操作是低效的？</h2><p>数组的插入操作大致可分为两种：</p><ol><li>尾部</li><li>非尾部</li></ol><p>其中从尾部插入元素的时间复杂度为 O（1），只需要一个简单的赋值操作即可，本质上来讲是一个「访问」操作的变种。而如果在数组头部和中间部分插入一个元素的话，可能就会涉及到一个非常昂贵的操作：「移动」。假设你要在数组第 k 个元素的位置插入一个新的元素，那边第 k 个元素至第 n（数组大小为 n）个元素就都需要向后移动一位，以便给新的元素让出位置。最坏的情况下，时间复杂度为 O（n）。</p><p>若读者仔细琢磨一下上面说的插入步骤，在结合文章最开始说的数组的第四个特点，我们就可以发现这里面还有一个隐含的风险：数组空间不够了怎么办？</p><p>如果出现了这种问题，肯定是需要从新开辟一块更大的内存空间，然后将所有的元素都 copy 过去。这样一来，时间复杂度妥妥的是 O（N），并且还增加了内存分配上的性能消耗。甚至是对内存空间也有一定的浪费。不过，对于这种昂贵的操作，很多编程语言都对其做了优化，比如设置一个更合适的数组内存空间分配策略，减少内存空间分配回收的次数等。</p><p>数组的删除操作基本上和插入操作面对的问题是一样的。虽然他不需要移动旧的元素给新的元素让出插入的位置，但是它要保证前面说到的数组的第一个特点：连续存储。所以，删除操作需要清除掉元素之间的「间隙」。不同之处在于，删除操作可以使用一些技巧来减少移动元素的次数：</p><p>对一个数组的元素执行了删除操作之后，先不移动元素，而是将此元素做一个标记。当数组的剩余空间已经没有的时候，可以统一对数组中的元素进行一次「整理」。</p><p>但是这个技巧的使用是有一个前提的：使用数组的人不需要数组「连续存储」的特性。</p><h2>优点和缺点</h2><h2>优点</h2><ol><li>通过下标可以对元素进行快速随机访问，时间复杂度为 O（1）</li><li>可以更好的利用 CPU 的高速缓存</li></ol><h2>缺点</h2><ol><li>必须要一段连续的内存空间存储。若内存剩余容量够但是空间不连续，则为数组分配内存会失败</li><li>昂贵的「插入」和「删除」操作</li></ol><h2>链表</h2><p>链表是按照一定的顺序通过指针将多个内存块串联起来的一种线性数据结构。当提到链表的时候，我通常会想到以下几个点：</p><ol><li>对内存空间的连续性没有要求</li><li>变种很多：单链表，双向链表，循环链表，双向循环链表</li><li>操作复杂，尤其是对头结点，尾节点，甚至是空链表</li><li>无法做到快速的随机访问，只能从链表头部进行遍历。但是插入和删除操作的时间复杂度为 O（1）</li><li>没有固定大小，可以轻易的扩缩容</li></ol><h2>为什么链表的插入和删除操作可以达到 O（1）的时间复杂度？</h2><p>其实，对于数组来说，它的插入和删除操作之所以昂贵，是因为数组的一个特性：连续存储。连续存储带来的好处就是可以通过公式计算出某个元素的内存地址，进而达到「随机访问」的效果。如果要保证这种「连续」的特性，那么是避免不了要频繁移动数组中的元素的。</p><h2>链表都有哪些种类？</h2><h2>单链表</h2><p>单链表是最简单的链表数据结构。它的每一节点通常由两个 field 构成：</p><ul><li>value：存储该节点的数据值</li><li>ptr_next：存储下一个节点的内存地址</li></ul><h2>循环链表</h2><p>循环链表和单链表唯一的区别就是在链表尾节点上。单链表尾节点 ptr_next 的值为 NULL，而循环链表为头结点的内存地址。通过将尾节点的 next 指针又指向了头结点，形成了一个环形的数据结构</p><h2>双向链表</h2><p>双向链表相较于单链表来说，其每一个节点不单单有后继指针，还有前驱指针。所以，双向链表可以实现两种相反方向的顺序访问。除了向使用者提供了另外一种访问链表的顺序之外，在特定场景下，他对于链表的删除和插入操作有一定的优化效果。</p><p>以删除来举例。如果我们想删除链表中的一个节点的时候，通常有两种方式可以对待删除节点进行描述：</p><ol><li>删除内存地址为 P 的节点</li><li>删除值为 X 的节点</li></ol><p>对于单链表来说，这两种方式没有任何区别。因为我们都需要从链表的头部开始遍历，直到找到待删除节点的前一个节点，再进行相应的操作。而对于双向链表来说，如果待删除节点是以内存地址来描述的话，我们就可以直接通过 p-&gt;ptr_pre 指针保存的内存地址定位到待删除节点的前一个节点。同理，对于插入操作来说也是一样的效果。 </p><p>除了删除和插入操作之外，对于一个元素有序的链表而言。双向链表查询操作的效率也要高于单链表。因为双向列表通过记录上一次查找的位置，可以根据大小选择到底是查找前半部分还是后半部分的元素。</p><h2>双向链表和单项链表的区别</h2><p>很容易就可以看出，双向链表要比单项链表多出一个「前驱指针」，这意味着双向链表要消耗更多的内存空间。但是，消耗更多空间的同时也为特定场景下链表的插入，删除，查找等操作节省了一定的时间。这是一种典型的「以空间换时间」的思想。相反，在计算机世界中也有很多「以时间换空间」的例子：比如对于一些消耗内存较多的服务，我们可以限制它的内存配额，将部分数据存储在硬盘上。虽然读写硬盘可能会造成一定时间上的延迟，但是，「以时间换空间」的目的却达到了。</p><h2>链表使用的注意事项</h2><p>在编写关于链表相关的代码的时候，是非常容易出错的。尤其是在以下几个点上：</p><ol><li>链表为空，是否可以工作</li><li>链表只有一个节点，是否可以工作</li><li>链表只有两个节点，是否可以工作</li><li>处理头结点和尾节点，是否可以工作</li></ol><h2>链表的常用操作</h2><ul><li>单链表反转</li><li>链表成环检测</li><li>有序链表合并</li><li>删除链表倒数第 N 个节点</li><li>求链表的中间节点</li></ul>", 
            "topic": [
                {
                    "tag": "链表", 
                    "tagLink": "https://api.zhihu.com/topics/19649942"
                }, 
                {
                    "tag": "数据结构", 
                    "tagLink": "https://api.zhihu.com/topics/19591797"
                }, 
                {
                    "tag": "数组", 
                    "tagLink": "https://api.zhihu.com/topics/19725906"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50765972", 
            "userName": "要没时间了", 
            "userLink": "https://www.zhihu.com/people/bdbe7f88af9a021c5c770649d266f143", 
            "upvote": 1, 
            "title": "Deep Into Stack and Queue", 
            "content": "<p></p><h2>写在前面</h2><p>栈和队列是两种首先的线性数据结构。前者只能从一段插入和弹出元素，后者只能从一端插入元素，另外一端弹出元素。它们底层都可以分别用数组和链表来实现，主要的操作有 push 和 pop 两个。尤其是对于队列来说，它在基础形式上有很多的变形，如循环队列，阻塞队列，优先级队列等。之所以在数组和链表上实现者这两种数据结构，是因为它们的操作都符合一些特定的场景，也符合人类思考事物的一些思维。当我们使用它来描述一些较为复杂问题的时候，有很大的帮助，如操作系统中的生产者和消费者模型，从计算机的角度看待如何求不定式的值以及如何判断括号是匹配的。</p><h2>栈和「栈」</h2><p>提起栈，我们想到的可能不光是一种数据结构。在操作系统的内存中也存在着「栈」，它是一段真实的物理存储区。主要用来存储函数调用时的一些局部变量。栈的内存地址是从高向低扩展的，原因就是它借助了数据结构中栈的思想：后进先出。与「栈」相对的另外两种类型的存储区是堆和静态存储区。前者可由用户通过一些库函数在堆内存中分配一定的空间来进行使用，典型的就是 malloc 和 new。后者则用来存储一些全局变量，静态变量，常量等。</p><p>之所以使用栈来存储函数调用的局部变量，是因为函数的调用存在着「递归」关系，这个递归的层数可大可小。多层嵌套且后面的层先计算出结果的特点是非常符合栈这种数据结构的特点的：后进先出。而且，栈的操作：入栈，出栈也非常的方便。即使是想清空栈，也直接调整一下栈顶指针即可。</p><h2>扩容栈</h2><p>我们无法在使用一个数据结构之前就能够预判它之后所需要的空间。所以说，我们需要实现一定的扩容机制。对于链表的实现来说，直接在队尾插入新节点即可。 但是对于数组来说，可能涉及到要移动元素。因为数组的内存空间在定义时就已经确定了。不过由于移动元素的操作并不是每一次入栈和出栈都会发生，所以通过「摊还时间分析法」可以简单的分析出，即使是有扩容操作，入栈和出栈的时间复杂度仍然为 O（1）。</p><p>无论使用链表实现栈还是数组，都需要考虑以下几个问题：</p><ol><li>空间复杂度：主要是针对链表而言，因为它需要额外存一个指针</li><li>内存溢出：无限制的扩张，最终可能导致存储空间被占用光</li><li>内存泄漏：不适用的空间没有及时回收，导致内存泄漏</li></ol><p>其中对第一点来说，如果我们对空间复杂度要求比较高，那么我们应该选择数组来实现。而对 2，3点来说，就得在实现中具体做一些限制，防止这种情况的发生。</p><h2>栈的应用方式</h2><p>对于栈的应用来说，一般是通过两种形式：</p><ol><li>两个栈配合：实现队列，浏览器页面的回退功能，表达式求值，括号匹配</li><li>单个栈：逆序输出一个单项序列</li></ol><h2>队列</h2><p>对于队列来说，唯一比栈复杂的地方就是它需要维护两个游标：一个指向队列头部，一个指向队列尾部。随着出队和入队操作的执行，这两个游标的位置要随之改变。更为复杂的是如何判断两个临界的条件：</p><ol><li>队列满</li><ol><li>真满</li><li>假满</li></ol><br/><li>队列空</li></ol><p>我们将指向队列头部的游标命名为 head, 指向队列尾部待插入元素位置的游标为 tail。</p><h2>队列满</h2><h2>真满</h2><p>一个真正将申请的空间全都填满的队列有如下特点：</p><ol><li>tail 已经到了之前申请空间的尾部了，无法再继续向后移动</li><li>head 仍然指向整个数据结构中的首地址的位置</li></ol><p>之所以这么判断，是和 tail，head 的操作有关的。入队操作会引起 tail 向后移动，出队操作会引起 head 向后移动。转换到代码层面上，可以如下表示：</p><div class=\"highlight\"><pre><code class=\"language-c\"><span class=\"n\">head</span> <span class=\"o\">==</span> <span class=\"mi\">0</span> <span class=\"o\">&amp;&amp;</span> <span class=\"n\">tail</span> <span class=\"o\">==</span> <span class=\"n\">n</span> <span class=\"o\">//</span> <span class=\"err\">队列满</span></code></pre></div><h2>假满</h2><p>一个假满的队列，是在有限的内存空间中，tail 已经移动到尾部了，但是 head 没有在首地址的位置。因为入队操作一直以来操作的都是 tail，当发现 tail == n 的时候，就认为队列满了，但是其实由于有出队操作的执行，还是有空余空间的。所以，这种就是假满的情况。</p><div class=\"highlight\"><pre><code class=\"language-c\"><span class=\"n\">head</span> <span class=\"o\">!=</span> <span class=\"mi\">0</span> <span class=\"o\">&amp;&amp;</span> <span class=\"n\">tail</span> <span class=\"o\">==</span> <span class=\"n\">n</span> <span class=\"o\">//</span> <span class=\"err\">队列假满</span></code></pre></div><p>假满的状态一般都要伴随着「数据迁移」。就如同之前我们聊数据这种数据结构的时候，为了保证它的顺序存储和连续存储的特性，我们时不时的要「整理碎片」。</p><h2>队列空</h2><p>在创建队列且未被使用的初始状态，head 和 tail 通常都赋值为数据结构的首地址。这证明在首尾指针之间没有任何的元素。所以，这也是判定队列为空的条件。</p><div class=\"highlight\"><pre><code class=\"language-c\"><span class=\"n\">head</span> <span class=\"o\">==</span> <span class=\"n\">tail</span> <span class=\"o\">//</span> <span class=\"err\">队列空</span></code></pre></div><h2>循环队列</h2><p>循环队列的出现，解决了一个很重要的问题：如何在不进行「数据移动」操作的前提下最大限度的利用队列的内存空间。下面我们通过数组实现循环队列的例子来举例。</p><p>通过上面对普通队列的描述，我们可以知道，实现队列最关键也是最容易出错的地方就是两个临界条件：队满和队空。其中对空这个临界条件在循环队列中仍然可以按照之前的标准进行判断。但是队满就不一样了。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-56a94fc89dbe38faeb59532ebd9d6690_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1142\" data-rawheight=\"640\" class=\"origin_image zh-lightbox-thumb\" width=\"1142\" data-original=\"https://pic1.zhimg.com/v2-56a94fc89dbe38faeb59532ebd9d6690_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1142&#39; height=&#39;640&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1142\" data-rawheight=\"640\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1142\" data-original=\"https://pic1.zhimg.com/v2-56a94fc89dbe38faeb59532ebd9d6690_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-56a94fc89dbe38faeb59532ebd9d6690_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>上述图片是一个循环队列队满的状态。细心的你一定会发现，这命名还有一个空余空间没有用，为什么说他已经满了？此时，你可以结合对空的临界条件，就可以知道，如果我们不预留这个一个元素空间的话，有一个地方是矛盾的：判断队满和对空的条件重合了： head == tail。而且，由于在这种环形的数据结构中，tail 可以无限制的移动，所以之前的队满临界条件就不生效了。</p><p>既然上面的图描述的就是队满的情况，那么如何将它固化成一个临界条件呢：</p><div class=\"highlight\"><pre><code class=\"language-c\"><span class=\"n\">tail</span> <span class=\"o\">+</span> <span class=\"mi\">1</span> <span class=\"o\">==</span> <span class=\"n\">head</span></code></pre></div><p>看起来这样的就大功告成了。但是我们还漏掉了一点，就是 tail 如果一直+1的移动，可能会超出数组的边界。那么如何让他在到达边界的地方回到首部位置呢？答案是取余数。</p><div class=\"highlight\"><pre><code class=\"language-c\"><span class=\"p\">(</span><span class=\"n\">tail</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">%</span> <span class=\"n\">n</span> <span class=\"o\">==</span> <span class=\"n\">head</span></code></pre></div><p></p>", 
            "topic": [
                {
                    "tag": "队列（数据结构）", 
                    "tagLink": "https://api.zhihu.com/topics/20204697"
                }, 
                {
                    "tag": "栈（数据结构）", 
                    "tagLink": "https://api.zhihu.com/topics/20183311"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50638018", 
            "userName": "要没时间了", 
            "userLink": "https://www.zhihu.com/people/bdbe7f88af9a021c5c770649d266f143", 
            "upvote": 5, 
            "title": "Detect the Mechanism of Scheduling in Kubernetes", 
            "content": "<blockquote>更好的阅读体验请访问我的 blog：</blockquote><a href=\"https://link.zhihu.com/?target=http%3A//littledriver.net/post/2018/11/23/detect-the-mechanism-of-scheduling-in-kubernetes/\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-712308ae0a6112062c1ed230b2c22248_180x120.jpg\" data-image-width=\"928\" data-image-height=\"236\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Detect the Mechanism of Scheduling in Kubernetes</a><h2>什么是 Kubernetes scheduling？</h2><p><code>Kubernetes scheduling</code>是 Kubernetes 集群中的调度机制，它负责将集群内部的资源按照一定的策略将其调度到不同的 Node 上运行。更准确一点，「集群内部的资源」说的其实就是 Pod----Kubernetes 中最小的工作实体和可调度实体。</p><h2>如何实现 Kubernetes scheduling？</h2><p><code>Kubernetes scheduling</code>一共由四个组件配合完成：</p><ol><li>API Server： 负责给出调度过程中需要的资源对象的信息</li><li>kube-scheduler：Kubernetes 内置的一个 Controller，核心的调度逻辑在这个组件内实现</li><li>Kubelet：Kubelet 存在于每一个 Node 上。它会根据 Pod.Spec 中的信息为其创建相应的容器，并负责维护这些容器的生命周期</li><li>ControllerManager：虽然<code>Kubernetes scheduling</code>主要是对于 Pod 而言的。但是除了你直接创建一个 Pod 之外，大多数情况下我们还是通过创建一个具体的资源，如 Deployment，Statefulset 等，然后这些资源的控制器（Controller） 将会为我们调用 API Server 的 API 创建相应的 Pod。所以，和调度相关的工作一定有 ControllerManager 的参与</li></ol><p>有了上述的四个组件，就可以实现一个完备的<code>Kubernetes scheduling</code>了。但是这里有一个小的细节需要读者考虑一下：各个组件看起来是通过「资源对象的信息」即 Event 来驱动整个调度过程运行的。那这些 Event 是如何在各个组件内进行传递的呢？答案是<code>List-Watch</code> 机制。</p><h2>快速了解 Kubernetes scheduling 运行机制</h2><div class=\"highlight\"><pre><code class=\"language-go\"><span class=\"k\">for</span> <span class=\"p\">{</span>\n    <span class=\"nx\">pod</span> <span class=\"o\">:=</span> <span class=\"nf\">getNextPod</span><span class=\"p\">()</span>\n    <span class=\"nx\">node</span><span class=\"p\">,</span> <span class=\"nx\">err</span> <span class=\"o\">:=</span> <span class=\"nf\">findFitNode</span><span class=\"p\">(</span><span class=\"nx\">pod</span><span class=\"p\">)</span>\n    <span class=\"k\">if</span> <span class=\"nx\">err</span> <span class=\"o\">==</span> <span class=\"kc\">nil</span> <span class=\"p\">{</span>\n        <span class=\"nx\">pod</span><span class=\"p\">.</span><span class=\"nx\">Spec</span><span class=\"p\">.</span><span class=\"nx\">nodeName</span> <span class=\"p\">=</span> <span class=\"nx\">node</span><span class=\"p\">.</span><span class=\"nx\">Name</span>\n        <span class=\"nf\">updatePod</span><span class=\"p\">(</span><span class=\"nx\">pod</span><span class=\"p\">)</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">}</span></code></pre></div><p>上面一个较为简单的实例可以大致描述一个 Pod 被调度的过程：</p><ol><li>从 Queue 中取出一个待调度的 Pod</li><li>按照一定的策略，为其找到合适的 Node</li><li>将 Pod 调度至相应的 Node</li></ol><p>其中第三步是依靠更新 Pod 的信息来做到的。因为此时 Kubelet 会通过<code>List-Watch</code>机制监听 Pod 的相关Event。当它发现有预备调度的 Pod 时，就可以在相应的 Node 上按照 pod.Spec 上描述的信息为其创建 Containers。</p><h2>扩展一下</h2><p>真正的调度过程，要远比我们上面给出的那个例子要复杂的多。本节我们将在此基础之上继续了解调度过程内部的一些更细节的地方。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-0936b93de7d5e23fa18d428b9d30b5de_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"928\" data-rawheight=\"236\" class=\"origin_image zh-lightbox-thumb\" width=\"928\" data-original=\"https://pic3.zhimg.com/v2-0936b93de7d5e23fa18d428b9d30b5de_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;928&#39; height=&#39;236&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"928\" data-rawheight=\"236\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"928\" data-original=\"https://pic3.zhimg.com/v2-0936b93de7d5e23fa18d428b9d30b5de_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-0936b93de7d5e23fa18d428b9d30b5de_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>上图主要描述了<code>findFitNode</code>方法内部的一些细节。我们可以看到，它会根据一定的策略和优先级来为 Pod 挑选 Node，如果没有挑选到合适的呢？那只能将这个 Pod 继续塞回队尾，等待下一次处理。如果已经找到符合条件的，那自然就可以进行后面的调度操作了。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-ae6adc1e6231a9058f67d19a8b56b589_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1021\" data-rawheight=\"204\" class=\"origin_image zh-lightbox-thumb\" width=\"1021\" data-original=\"https://pic2.zhimg.com/v2-ae6adc1e6231a9058f67d19a8b56b589_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1021&#39; height=&#39;204&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1021\" data-rawheight=\"204\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1021\" data-original=\"https://pic2.zhimg.com/v2-ae6adc1e6231a9058f67d19a8b56b589_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-ae6adc1e6231a9058f67d19a8b56b589_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>有些 Pod 在创建的时候，可能需要挂载一定量的存储资源，所以不可避免的需要和 PVC 和 PV 打交道。上图中给出的实例是针对「静态 PVC」 场景的，即 PV 和相应的物理资源已经处理好了，若此时有符合条件的 PVC 出现，那么直接将 PVC 和 PV 绑定即可。与之相反的是「动态 PVC」 场景，即集群内部没有与 PVC 匹配的 PV 资源，此时 PV Controller 将会通过<code>List-Watch</code>机制监听到 PVC 相关的信息，为其动态的创建一个符合要求的 PV 和相应的物理资源。</p><p>可能有的读者比较迷惑，为什么在完成了 Bind PVC 的工作之后又将 Pod 送回到了 Queue 中呢？这是因为 Scheduler 虽然已经了解了 Pod 对于 Node 的要求，并且也已经确认能够满足它。但是它想再将目前的场景送入「策略系统」，看是否能将 Pod 调度到里物理资源更近的机器上（也可能是有其他可以优化的地方）。</p><blockquote> 说到这里就不得不提及一下 Scheduler 这个组件的本质。无论是在 Kubernetes 中还是在 Mesos 中，Scheduler 组件的一个重要的职责就是通过一定的调度策略充分利用集群的资源，并且让使用者更高效的利用集群内的资源。当你对 Scheduler 这个组件有了这样一个认识之后，上面它所做的操作相信理解起来也不难了。  <br/> </blockquote><h2>聊聊调度策略（从用户可控的角度）</h2><p>Kubernetes 一个非常值得称赞的特性就是，它为用户在很多方面都提供了高度的扩展性，如 CRD，自定义 Controller，CNI，CRI 等。所以，对于「调度策略」这么贴近业务的一个功能也一定会有所支持。</p><p>在 Kubernetes 中，通过以下几种方式为用户提供稍微灵活一些的调度策略</p><ol><li>NodeSelector</li><li>Anti-Affinity/Affinity</li><li>Tolerations/Taints</li></ol><p>它们有一个共同的特点：都是通过 Kubernetes 的 Label 和 LabelSelector 机制来实现的。更详细的描述可以通过这篇文章深入了解一下：<a href=\"https://link.zhihu.com/?target=http%3A//littledriver.net/post/2018/09/11/contrain-pod-scheduling/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Contrain Pod Scheduling - LittleDriver</a></p><p>除了上面我们说到的 Kubernetes 内置的可以影响调度策略的方式，还有一个更高阶的玩法：自己实现一个 Scheduler，替换掉 Kubernetes 中默认的。</p><h2>对于替换 Scheduler 的一点思考</h2><p>不得不说，Kubernetes 中可以替换 Scheduler 这种重要组件的行为着实让我吃了一惊。但是仔细回味一下我前几天看过的<code>List-Watch</code>机制原理以及它在调度过程中的应用，我一下子明白了 Kubernetes 设计者的良苦用心：由于 Kubernetes 集群内部各个组件基本都是依靠<code>List-Watch</code>这种机制来进行消息传递的，且 Kubernetes 本身各个核心组件也都是依靠 Event 驱动机制来执行一些操作，所以 Scheduler 组件并没有和集群内其他任何组件产生过度的耦合。他们之间的交互要么依赖 Event，要么依赖一些 HTTP 或者 gRPC 的接口。那么，如果我们自己将 Scheduler 的框架实现一下，然后在其中填充和自己业务紧密相关的逻辑，完全可以无缝替换掉默认的 Scheduler。</p>", 
            "topic": [
                {
                    "tag": "Kubernetes", 
                    "tagLink": "https://api.zhihu.com/topics/20018384"
                }, 
                {
                    "tag": "调度算法", 
                    "tagLink": "https://api.zhihu.com/topics/19795728"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50637979", 
            "userName": "要没时间了", 
            "userLink": "https://www.zhihu.com/people/bdbe7f88af9a021c5c770649d266f143", 
            "upvote": 1, 
            "title": "Detect the source code of list-watch 2", 
            "content": "<blockquote>更好的阅读体验请访问我的 blog: </blockquote><a href=\"https://link.zhihu.com/?target=http%3A//littledriver.net/post/2018/11/22/detect-the-source-code-of-list-watch-between-api-server-and-controller/\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">littledriver.net/post/2</span><span class=\"invisible\">018/11/22/detect-the-source-code-of-list-watch-between-api-server-and-controller/</span><span class=\"ellipsis\"></span></a><p class=\"ztext-empty-paragraph\"><br/></p><h2>写在前面</h2><p>在上一篇blog 中，我们从源码的角度来分析了 Kubernetes 中的<code>List-Watch</code>机制的部分内容。它更注重于 API Server 和 etcd 之间的交互。通过下面的这幅 Kubernetes 的架构图和<code>List-Watch</code>机制的时序图我们可以知道，API Server 通过<code>List-Watch</code>机制获取到的资源对象的信息还将用于 Kubernetes 内置的 Controller 和我们自定义的 Controller，甚至是 Scheduler 和 Kubelet。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-acb94786dd7e8634c478d8ac9c1411ac_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1280\" data-rawheight=\"720\" class=\"origin_image zh-lightbox-thumb\" width=\"1280\" data-original=\"https://pic1.zhimg.com/v2-acb94786dd7e8634c478d8ac9c1411ac_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1280&#39; height=&#39;720&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1280\" data-rawheight=\"720\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1280\" data-original=\"https://pic1.zhimg.com/v2-acb94786dd7e8634c478d8ac9c1411ac_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-acb94786dd7e8634c478d8ac9c1411ac_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-cc706ed7459aebaa597c333b8669fc15_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1858\" data-rawheight=\"1126\" class=\"origin_image zh-lightbox-thumb\" width=\"1858\" data-original=\"https://pic2.zhimg.com/v2-cc706ed7459aebaa597c333b8669fc15_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1858&#39; height=&#39;1126&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1858\" data-rawheight=\"1126\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1858\" data-original=\"https://pic2.zhimg.com/v2-cc706ed7459aebaa597c333b8669fc15_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-cc706ed7459aebaa597c333b8669fc15_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>本文我们将通过对<code>ReplicaSetController</code>源码的分析来了解，API Server 保存的和资源对象有关的数据到底是怎么流动到 Controller（目的组件）中的，以及最终这些数据是如何被处理的。</p><h2>ControllerManager</h2><p>Kubernetes 中一个非常重要的组件就是：ControllerManager。它负责管理集群内部各个资源控制器（Controller） 的启停。通过阅读 ControManager 的代码可以很容易的发现，它在运行起来的时候会启动 Kubernetes 内置的各个 Controller。（下面的实例我截取了部分关键的逻辑，完整版的链接：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/kubernetes/kubernetes/blob/f48e18faa4dc035cc927c6a2b34c83c8475b55fa/cmd/kube-controller-manager/app/controllermanager.go%23L207\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">kubernetes/controllermanager.go at f48e18faa4dc035cc927c6a2b34c83c8475b55fa · kubernetes/kubernetes · GitHub</a> ）</p><div class=\"highlight\"><pre><code class=\"language-go\"><span class=\"nx\">run</span> <span class=\"o\">:=</span> <span class=\"kd\">func</span><span class=\"p\">(</span><span class=\"nx\">ctx</span> <span class=\"nx\">context</span><span class=\"p\">.</span><span class=\"nx\">Context</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n        <span class=\"o\">...</span>\n        <span class=\"nx\">controllerContext</span><span class=\"p\">,</span> <span class=\"nx\">err</span> <span class=\"o\">:=</span> <span class=\"nf\">CreateControllerContext</span><span class=\"p\">(</span><span class=\"nx\">c</span><span class=\"p\">,</span> <span class=\"nx\">rootClientBuilder</span><span class=\"p\">,</span> <span class=\"nx\">clientBuilder</span><span class=\"p\">,</span> <span class=\"nx\">ctx</span><span class=\"p\">.</span><span class=\"nf\">Done</span><span class=\"p\">())</span>\n        <span class=\"k\">if</span> <span class=\"nx\">err</span> <span class=\"o\">!=</span> <span class=\"kc\">nil</span> <span class=\"p\">{</span>\n            <span class=\"nx\">klog</span><span class=\"p\">.</span><span class=\"nf\">Fatalf</span><span class=\"p\">(</span><span class=\"s\">&#34;error building controller context: %v&#34;</span><span class=\"p\">,</span> <span class=\"nx\">err</span><span class=\"p\">)</span>\n        <span class=\"p\">}</span>\n        <span class=\"o\">...</span>\n        <span class=\"k\">if</span> <span class=\"nx\">err</span> <span class=\"o\">:=</span> <span class=\"nf\">StartControllers</span><span class=\"p\">(</span><span class=\"nx\">controllerContext</span><span class=\"p\">,</span> <span class=\"nx\">saTokenControllerInitFunc</span><span class=\"p\">,</span> <span class=\"nf\">NewControllerInitializers</span><span class=\"p\">(</span><span class=\"nx\">controllerContext</span><span class=\"p\">.</span><span class=\"nx\">LoopMode</span><span class=\"p\">),</span> <span class=\"nx\">unsecuredMux</span><span class=\"p\">);</span> <span class=\"nx\">err</span> <span class=\"o\">!=</span> <span class=\"kc\">nil</span> <span class=\"p\">{</span>\n            <span class=\"nx\">klog</span><span class=\"p\">.</span><span class=\"nf\">Fatalf</span><span class=\"p\">(</span><span class=\"s\">&#34;error starting controllers: %v&#34;</span><span class=\"p\">,</span> <span class=\"nx\">err</span><span class=\"p\">)</span>\n        <span class=\"p\">}</span>\n\n        <span class=\"nx\">controllerContext</span><span class=\"p\">.</span><span class=\"nx\">InformerFactory</span><span class=\"p\">.</span><span class=\"nf\">Start</span><span class=\"p\">(</span><span class=\"nx\">controllerContext</span><span class=\"p\">.</span><span class=\"nx\">Stop</span><span class=\"p\">)</span>\n        <span class=\"nb\">close</span><span class=\"p\">(</span><span class=\"nx\">controllerContext</span><span class=\"p\">.</span><span class=\"nx\">InformersStarted</span><span class=\"p\">)</span></code></pre></div><p>上述逻辑有几个值得注意的点：</p><ol><li>StartControllers</li><li>NewControllerInitializers</li><li>controllerContext</li><ol><li>controllerContext.InformerFactory.Start</li></ol></ol><h2>StartControllers</h2><div class=\"highlight\"><pre><code class=\"language-go\"><span class=\"kd\">func</span> <span class=\"nf\">StartControllers</span><span class=\"p\">(</span><span class=\"nx\">ctx</span> <span class=\"nx\">ControllerContext</span><span class=\"p\">,</span> <span class=\"nx\">startSATokenController</span> <span class=\"nx\">InitFunc</span><span class=\"p\">,</span> <span class=\"nx\">controllers</span> <span class=\"kd\">map</span><span class=\"p\">[</span><span class=\"kt\">string</span><span class=\"p\">]</span><span class=\"nx\">InitFunc</span><span class=\"p\">,</span> <span class=\"nx\">unsecuredMux</span> <span class=\"o\">*</span><span class=\"nx\">mux</span><span class=\"p\">.</span><span class=\"nx\">PathRecorderMux</span><span class=\"p\">)</span> <span class=\"kt\">error</span> <span class=\"p\">{</span>\n<span class=\"o\">...</span>\n    <span class=\"k\">for</span> <span class=\"nx\">controllerName</span><span class=\"p\">,</span> <span class=\"nx\">initFn</span> <span class=\"o\">:=</span> <span class=\"k\">range</span> <span class=\"nx\">controllers</span> <span class=\"p\">{</span>\n        <span class=\"k\">if</span> <span class=\"p\">!</span><span class=\"nx\">ctx</span><span class=\"p\">.</span><span class=\"nf\">IsControllerEnabled</span><span class=\"p\">(</span><span class=\"nx\">controllerName</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n            <span class=\"nx\">glog</span><span class=\"p\">.</span><span class=\"nf\">Warningf</span><span class=\"p\">(</span><span class=\"s\">&#34;%q is disabled&#34;</span><span class=\"p\">,</span> <span class=\"nx\">controllerName</span><span class=\"p\">)</span>\n            <span class=\"k\">continue</span>\n        <span class=\"p\">}</span>\n\n<span class=\"nx\">time</span><span class=\"p\">.</span><span class=\"nf\">Sleep</span><span class=\"p\">(</span><span class=\"nx\">wait</span><span class=\"p\">.</span><span class=\"nf\">Jitter</span><span class=\"p\">(</span><span class=\"nx\">ctx</span><span class=\"p\">.</span><span class=\"nx\">ComponentConfig</span><span class=\"p\">.</span><span class=\"nx\">Generic</span><span class=\"p\">.</span><span class=\"nx\">ControllerStartInterval</span><span class=\"p\">.</span><span class=\"nx\">Duration</span><span class=\"p\">,</span> <span class=\"nx\">ControllerStartJitter</span><span class=\"p\">))</span>\n\n        <span class=\"nx\">glog</span><span class=\"p\">.</span><span class=\"nf\">V</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">).</span><span class=\"nf\">Infof</span><span class=\"p\">(</span><span class=\"s\">&#34;Starting %q&#34;</span><span class=\"p\">,</span> <span class=\"nx\">controllerName</span><span class=\"p\">)</span>\n        <span class=\"nx\">debugHandler</span><span class=\"p\">,</span> <span class=\"nx\">started</span><span class=\"p\">,</span> <span class=\"nx\">err</span> <span class=\"o\">:=</span> <span class=\"nf\">initFn</span><span class=\"p\">(</span><span class=\"nx\">ctx</span><span class=\"p\">)</span></code></pre></div><p>StartControllers 的逻辑比较简单，它遍历了一个存有 Controller 的 Map，依次调用 Controller 的启动函数<code>initFn</code>。这个 Map 是通过函数参数传递进来的，对这个参数进行赋值的位置也正是<code>NewControllerInitializers</code>函数被调用的位置。</p><h2>NewControllerInitializers</h2><div class=\"highlight\"><pre><code class=\"language-go\"><span class=\"kd\">func</span> <span class=\"nf\">NewControllerInitializers</span><span class=\"p\">(</span><span class=\"nx\">loopMode</span> <span class=\"nx\">ControllerLoopMode</span><span class=\"p\">)</span> <span class=\"kd\">map</span><span class=\"p\">[</span><span class=\"kt\">string</span><span class=\"p\">]</span><span class=\"nx\">InitFunc</span> <span class=\"p\">{</span>\n    <span class=\"nx\">controllers</span> <span class=\"o\">:=</span> <span class=\"kd\">map</span><span class=\"p\">[</span><span class=\"kt\">string</span><span class=\"p\">]</span><span class=\"nx\">InitFunc</span><span class=\"p\">{}</span>\n    <span class=\"nx\">controllers</span><span class=\"p\">[</span><span class=\"s\">&#34;endpoint&#34;</span><span class=\"p\">]</span> <span class=\"p\">=</span> <span class=\"nx\">startEndpointController</span>\n    <span class=\"nx\">controllers</span><span class=\"p\">[</span><span class=\"s\">&#34;replicationcontroller&#34;</span><span class=\"p\">]</span> <span class=\"p\">=</span> <span class=\"nx\">startReplicationController</span>\n    <span class=\"nx\">controllers</span><span class=\"p\">[</span><span class=\"s\">&#34;podgc&#34;</span><span class=\"p\">]</span> <span class=\"p\">=</span> <span class=\"nx\">startPodGCController</span>\n    <span class=\"nx\">controllers</span><span class=\"p\">[</span><span class=\"s\">&#34;resourcequota&#34;</span><span class=\"p\">]</span> <span class=\"p\">=</span> <span class=\"nx\">startResourceQuotaController</span>\n    <span class=\"nx\">controllers</span><span class=\"p\">[</span><span class=\"s\">&#34;namespace&#34;</span><span class=\"p\">]</span> <span class=\"p\">=</span> <span class=\"nx\">startNamespaceController</span>\n    <span class=\"nx\">controllers</span><span class=\"p\">[</span><span class=\"s\">&#34;serviceaccount&#34;</span><span class=\"p\">]</span> <span class=\"p\">=</span> <span class=\"nx\">startServiceAccountController</span>\n    <span class=\"nx\">controllers</span><span class=\"p\">[</span><span class=\"s\">&#34;garbagecollector&#34;</span><span class=\"p\">]</span> <span class=\"p\">=</span> <span class=\"nx\">startGarbageCollectorController</span>\n    <span class=\"nx\">controllers</span><span class=\"p\">[</span><span class=\"s\">&#34;daemonset&#34;</span><span class=\"p\">]</span> <span class=\"p\">=</span> <span class=\"nx\">startDaemonSetController</span>\n    <span class=\"nx\">controllers</span><span class=\"p\">[</span><span class=\"s\">&#34;job&#34;</span><span class=\"p\">]</span> <span class=\"p\">=</span> <span class=\"nx\">startJobController</span>\n    <span class=\"nx\">controllers</span><span class=\"p\">[</span><span class=\"s\">&#34;deployment&#34;</span><span class=\"p\">]</span> <span class=\"p\">=</span> <span class=\"nx\">startDeploymentController</span>\n    <span class=\"nx\">controllers</span><span class=\"p\">[</span><span class=\"s\">&#34;replicaset&#34;</span><span class=\"p\">]</span> <span class=\"p\">=</span> <span class=\"nx\">startReplicaSetController</span>\n    <span class=\"nx\">controllers</span><span class=\"p\">[</span><span class=\"s\">&#34;horizontalpodautoscaling&#34;</span><span class=\"p\">]</span> <span class=\"p\">=</span> <span class=\"nx\">startHPAController</span>\n    <span class=\"nx\">controllers</span><span class=\"p\">[</span><span class=\"s\">&#34;disruption&#34;</span><span class=\"p\">]</span> <span class=\"p\">=</span> <span class=\"nx\">startDisruptionController</span>\n    <span class=\"nx\">controllers</span><span class=\"p\">[</span><span class=\"s\">&#34;statefulset&#34;</span><span class=\"p\">]</span> <span class=\"p\">=</span> <span class=\"nx\">startStatefulSetController</span></code></pre></div><p>截取该函数的部分逻辑就可以看出，它构造了一个 key 为 Controller 名字，value 为 Controller 创建函数的 Map。</p><h2>controllerContext</h2><p>有了 <code>NewControllerInitializers</code>和<code>StartControllers</code>的出现，按理说 Controller 就可以启动了。但是我们发现在<code>StartControllers</code>函数调用的位置，传递进去了一个比较特殊的参数：<code>controllerContext</code>，它是通过<code>CreateControllerContext</code>函数创建的。</p><p>查看<code>CreateControllerContext</code>函数逻辑后发现并没有什么特别的地方，唯一值得注意的就是一个名为<code>sharedInformers</code>的变量。因为他在 ControllerManager 中的 Run 函数内调用了自己的 Start 方法。</p><h2>controllerContext.InformerFactory</h2><p><code>controllerContext.InformerFactory</code>被我们在上面说到<code>sharedInformers</code>赋值。根据创建<code>shardInformers</code> 的流程跟进下去，我们发现它的核心逻辑在这里：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/kubernetes/kubernetes/blob/7f23a743e8c23ac6489340bbb34fa6f1d392db9d/staging/src/k8s.io/apiextensions-apiserver/pkg/client/informers/externalversions/factory.go%23L91\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">kubernetes/factory.go at 7f23a743e8c23ac6489340bbb34fa6f1d392db9d · kubernetes/kubernetes · GitHub</a>。并且同时发现，它所实现的 Start 函数，其实是通过遍历其内部一个名为<code>informers</code>的Map，调用了每一个<code>informer</code>实现的 Run 函数。</p><div class=\"highlight\"><pre><code class=\"language-go\"><span class=\"c1\">// NewSharedInformerFactoryWithOptions constructs a new instance of a SharedInformerFactory with additional options.\n</span><span class=\"c1\"></span><span class=\"kd\">func</span> <span class=\"nf\">NewSharedInformerFactoryWithOptions</span><span class=\"p\">(</span><span class=\"nx\">client</span> <span class=\"nx\">kubernetes</span><span class=\"p\">.</span><span class=\"nx\">Interface</span><span class=\"p\">,</span> <span class=\"nx\">defaultResync</span> <span class=\"nx\">time</span><span class=\"p\">.</span><span class=\"nx\">Duration</span><span class=\"p\">,</span> <span class=\"nx\">options</span> <span class=\"o\">...</span><span class=\"nx\">SharedInformerOption</span><span class=\"p\">)</span> <span class=\"nx\">SharedInformerFactory</span> <span class=\"p\">{</span>\n    <span class=\"nx\">factory</span> <span class=\"o\">:=</span> <span class=\"o\">&amp;</span><span class=\"nx\">sharedInformerFactory</span><span class=\"p\">{</span>\n        <span class=\"nx\">client</span><span class=\"p\">:</span>           <span class=\"nx\">client</span><span class=\"p\">,</span>\n        <span class=\"nx\">namespace</span><span class=\"p\">:</span>        <span class=\"nx\">v1</span><span class=\"p\">.</span><span class=\"nx\">NamespaceAll</span><span class=\"p\">,</span>\n        <span class=\"nx\">defaultResync</span><span class=\"p\">:</span>    <span class=\"nx\">defaultResync</span><span class=\"p\">,</span>\n        <span class=\"nx\">informers</span><span class=\"p\">:</span>        <span class=\"nb\">make</span><span class=\"p\">(</span><span class=\"kd\">map</span><span class=\"p\">[</span><span class=\"nx\">reflect</span><span class=\"p\">.</span><span class=\"nx\">Type</span><span class=\"p\">]</span><span class=\"nx\">cache</span><span class=\"p\">.</span><span class=\"nx\">SharedIndexInformer</span><span class=\"p\">),</span>\n        <span class=\"nx\">startedInformers</span><span class=\"p\">:</span> <span class=\"nb\">make</span><span class=\"p\">(</span><span class=\"kd\">map</span><span class=\"p\">[</span><span class=\"nx\">reflect</span><span class=\"p\">.</span><span class=\"nx\">Type</span><span class=\"p\">]</span><span class=\"kt\">bool</span><span class=\"p\">),</span>\n        <span class=\"nx\">customResync</span><span class=\"p\">:</span>     <span class=\"nb\">make</span><span class=\"p\">(</span><span class=\"kd\">map</span><span class=\"p\">[</span><span class=\"nx\">reflect</span><span class=\"p\">.</span><span class=\"nx\">Type</span><span class=\"p\">]</span><span class=\"nx\">time</span><span class=\"p\">.</span><span class=\"nx\">Duration</span><span class=\"p\">),</span>\n    <span class=\"p\">}</span>\n\n    <span class=\"c1\">// Apply all options\n</span><span class=\"c1\"></span>    <span class=\"k\">for</span> <span class=\"nx\">_</span><span class=\"p\">,</span> <span class=\"nx\">opt</span> <span class=\"o\">:=</span> <span class=\"k\">range</span> <span class=\"nx\">options</span> <span class=\"p\">{</span>\n        <span class=\"nx\">factory</span> <span class=\"p\">=</span> <span class=\"nf\">opt</span><span class=\"p\">(</span><span class=\"nx\">factory</span><span class=\"p\">)</span>\n    <span class=\"p\">}</span>\n\n    <span class=\"k\">return</span> <span class=\"nx\">factory</span>\n<span class=\"p\">}</span>\n\n<span class=\"c1\">// Start initializes all requested informers.\n</span><span class=\"c1\"></span><span class=\"kd\">func</span> <span class=\"p\">(</span><span class=\"nx\">f</span> <span class=\"o\">*</span><span class=\"nx\">sharedInformerFactory</span><span class=\"p\">)</span> <span class=\"nf\">Start</span><span class=\"p\">(</span><span class=\"nx\">stopCh</span> <span class=\"o\">&lt;-</span><span class=\"kd\">chan</span> <span class=\"kd\">struct</span><span class=\"p\">{})</span> <span class=\"p\">{</span>\n    <span class=\"nx\">f</span><span class=\"p\">.</span><span class=\"nx\">lock</span><span class=\"p\">.</span><span class=\"nf\">Lock</span><span class=\"p\">()</span>\n    <span class=\"k\">defer</span> <span class=\"nx\">f</span><span class=\"p\">.</span><span class=\"nx\">lock</span><span class=\"p\">.</span><span class=\"nf\">Unlock</span><span class=\"p\">()</span>\n\n    <span class=\"k\">for</span> <span class=\"nx\">informerType</span><span class=\"p\">,</span> <span class=\"nx\">informer</span> <span class=\"o\">:=</span> <span class=\"k\">range</span> <span class=\"nx\">f</span><span class=\"p\">.</span><span class=\"nx\">informers</span> <span class=\"p\">{</span>\n        <span class=\"k\">if</span> <span class=\"p\">!</span><span class=\"nx\">f</span><span class=\"p\">.</span><span class=\"nx\">startedInformers</span><span class=\"p\">[</span><span class=\"nx\">informerType</span><span class=\"p\">]</span> <span class=\"p\">{</span>\n            <span class=\"k\">go</span> <span class=\"nx\">informer</span><span class=\"p\">.</span><span class=\"nf\">Run</span><span class=\"p\">(</span><span class=\"nx\">stopCh</span><span class=\"p\">)</span>\n            <span class=\"nx\">f</span><span class=\"p\">.</span><span class=\"nx\">startedInformers</span><span class=\"p\">[</span><span class=\"nx\">informerType</span><span class=\"p\">]</span> <span class=\"p\">=</span> <span class=\"kc\">true</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">}</span></code></pre></div><p>那么现在看起来，这个<code>informer</code>和其 Run 函数的实现就值得我们再去深入的了解一下了。</p><h2>informer</h2><p>Informer 的数据类型是<code>cache.SharedIndexInformer</code>：</p><div class=\"highlight\"><pre><code class=\"language-go\"><span class=\"c1\">// SharedInformer has a shared data cache and is capable of distributing notifications for changes\n</span><span class=\"c1\">// to the cache to multiple listeners who registered via AddEventHandler. If you use this, there is\n</span><span class=\"c1\">// one behavior change compared to a standard Informer.  When you receive a notification, the cache\n</span><span class=\"c1\">// will be AT LEAST as fresh as the notification, but it MAY be more fresh.  You should NOT depend\n</span><span class=\"c1\">// on the contents of the cache exactly matching the notification you&#39;ve received in handler\n</span><span class=\"c1\">// functions.  If there was a create, followed by a delete, the cache may NOT have your item.  This\n</span><span class=\"c1\">// has advantages over the broadcaster since it allows us to share a common cache across many\n</span><span class=\"c1\">// controllers. Extending the broadcaster would have required us keep duplicate caches for each\n</span><span class=\"c1\">// watch.\n</span><span class=\"c1\"></span><span class=\"kd\">type</span> <span class=\"nx\">SharedInformer</span> <span class=\"kd\">interface</span> <span class=\"p\">{</span>\n    <span class=\"c1\">// AddEventHandler adds an event handler to the shared informer using the shared informer&#39;s resync\n</span><span class=\"c1\"></span>    <span class=\"c1\">// period.  Events to a single handler are delivered sequentially, but there is no coordination\n</span><span class=\"c1\"></span>    <span class=\"c1\">// between different handlers.\n</span><span class=\"c1\"></span>    <span class=\"nf\">AddEventHandler</span><span class=\"p\">(</span><span class=\"nx\">handler</span> <span class=\"nx\">ResourceEventHandler</span><span class=\"p\">)</span>\n    <span class=\"c1\">// AddEventHandlerWithResyncPeriod adds an event handler to the shared informer using the\n</span><span class=\"c1\"></span>    <span class=\"c1\">// specified resync period.  Events to a single handler are delivered sequentially, but there is\n</span><span class=\"c1\"></span>    <span class=\"c1\">// no coordination between different handlers.\n</span><span class=\"c1\"></span>    <span class=\"nf\">AddEventHandlerWithResyncPeriod</span><span class=\"p\">(</span><span class=\"nx\">handler</span> <span class=\"nx\">ResourceEventHandler</span><span class=\"p\">,</span> <span class=\"nx\">resyncPeriod</span> <span class=\"nx\">time</span><span class=\"p\">.</span><span class=\"nx\">Duration</span><span class=\"p\">)</span>\n    <span class=\"c1\">// GetStore returns the Store.\n</span><span class=\"c1\"></span>    <span class=\"nf\">GetStore</span><span class=\"p\">()</span> <span class=\"nx\">Store</span>\n    <span class=\"c1\">// GetController gives back a synthetic interface that &#34;votes&#34; to start the informer\n</span><span class=\"c1\"></span>    <span class=\"nf\">GetController</span><span class=\"p\">()</span> <span class=\"nx\">Controller</span>\n    <span class=\"c1\">// Run starts the shared informer, which will be stopped when stopCh is closed.\n</span><span class=\"c1\"></span>    <span class=\"nf\">Run</span><span class=\"p\">(</span><span class=\"nx\">stopCh</span> <span class=\"o\">&lt;-</span><span class=\"kd\">chan</span> <span class=\"kd\">struct</span><span class=\"p\">{})</span>\n    <span class=\"c1\">// HasSynced returns true if the shared informer&#39;s store has synced.\n</span><span class=\"c1\"></span>    <span class=\"nf\">HasSynced</span><span class=\"p\">()</span> <span class=\"kt\">bool</span>\n    <span class=\"c1\">// LastSyncResourceVersion is the resource version observed when last synced with the underlying\n</span><span class=\"c1\"></span>    <span class=\"c1\">// store. The value returned is not synchronized with access to the underlying store and is not\n</span><span class=\"c1\"></span>    <span class=\"c1\">// thread-safe.\n</span><span class=\"c1\"></span>    <span class=\"nf\">LastSyncResourceVersion</span><span class=\"p\">()</span> <span class=\"kt\">string</span>\n<span class=\"p\">}</span>\n\n<span class=\"kd\">type</span> <span class=\"nx\">SharedIndexInformer</span> <span class=\"kd\">interface</span> <span class=\"p\">{</span>\n    <span class=\"nx\">SharedInformer</span>\n    <span class=\"c1\">// AddIndexers add indexers to the informer before it starts.\n</span><span class=\"c1\"></span>    <span class=\"nf\">AddIndexers</span><span class=\"p\">(</span><span class=\"nx\">indexers</span> <span class=\"nx\">Indexers</span><span class=\"p\">)</span> <span class=\"kt\">error</span>\n    <span class=\"nf\">GetIndexer</span><span class=\"p\">()</span> <span class=\"nx\">Indexer</span>\n<span class=\"p\">}</span></code></pre></div><p>通过它的数据结构的定义以及注释我们兴奋的发现，<code>SharedIndexInformer</code>类型的对象可能正是多个 Controller 从 API Server 接收资源对象信息的一个入口。总结下来，<code>SharedIndexInformer</code>有如下几个特点：</p><ol><li>它和 API Server 一样，在内部会维护一个缓存。一旦缓存的内容有变化，它将通知给关心这些消息的对象。「关心」的方式很简单，就是自己实现这些消息的处理函数，并将它们注册到<code>SharedIndexInformer</code>类型的实例中</li></ol><div class=\"highlight\"><pre><code class=\"language-go\"><span class=\"c1\">// ResourceEventHandlerFuncs is an adaptor to let you easily specify as many or\n</span><span class=\"c1\">// as few of the notification functions as you want while still implementing\n</span><span class=\"c1\">// ResourceEventHandler.\n</span><span class=\"c1\"></span><span class=\"kd\">type</span> <span class=\"nx\">ResourceEventHandlerFuncs</span> <span class=\"kd\">struct</span> <span class=\"p\">{</span>\n    <span class=\"nx\">AddFunc</span>    <span class=\"kd\">func</span><span class=\"p\">(</span><span class=\"nx\">obj</span> <span class=\"kd\">interface</span><span class=\"p\">{})</span>\n    <span class=\"nx\">UpdateFunc</span> <span class=\"kd\">func</span><span class=\"p\">(</span><span class=\"nx\">oldObj</span><span class=\"p\">,</span> <span class=\"nx\">newObj</span> <span class=\"kd\">interface</span><span class=\"p\">{})</span>\n    <span class=\"nx\">DeleteFunc</span> <span class=\"kd\">func</span><span class=\"p\">(</span><span class=\"nx\">obj</span> <span class=\"kd\">interface</span><span class=\"p\">{})</span>\n<span class=\"p\">}</span></code></pre></div><ol><li><code>SharedIndexInformer</code> 所维护的缓存将尽量保证其中的内容和实时的资源对象的信息一致。但是我们不能够依赖通过 event 形式发来的资源对象的信息和缓存内容之间的匹配关系。如过一个资源对象的创建和删除操作发生的间隔时间比较短，那么缓存中最终是没有这个资源对象的信息的。这非常符合 Kubernetes 的开发理念：保持数据的最终一致性。</li></ol><h2>Run</h2><p>大概了解了<code>informer</code> 的实现原理后，我们来看下它的 Run 函数的逻辑： <a href=\"https://link.zhihu.com/?target=https%3A//github.com/kubernetes/client-go/blob/ee7a1ba5cdf1292b67a1fdf1fa28f90d2a7b0084/tools/cache/shared_informer.go%23L189\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">client-go/shared_informer.go at ee7a1ba5cdf1292b67a1fdf1fa28f90d2a7b0084 · kubernetes/client-go · GitHub</a></p><p>在去掉了一些干扰性的信息之后，我们发现，Run 函数做的核心工作有两点：</p><ol><li>通过配置信息创建一个 controller 对象</li><li>调用这个 controller 对象的 Run 函数</li></ol><h2>controller.Run</h2><p>在 controller 对象的 Run 函数中我们发现，它创建了一个让我们非常熟悉的对象：Reflector。 这个类型为 Reflector 的对象之前在 API Server 中也被创建过:<a href=\"https://link.zhihu.com/?target=http%3A//littledriver.net/post/2018/11/21/detect-the-source-code-of-list-watch-between-api-server-and-etcd/%23reflector-cache-reflector\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Detect the Source Code of List Watch Between API Server and Etcd - LittleDriver</a>。它有两个比较重要的功能：</p><ol><li>封装了 List/Watch 等方法在 listerWatcher 成员内部，用于后期调用获取资源对象的数据</li><li>维护了一个缓存（以队列的形式实现），存储获取到的资源对象的数据</li></ol><p>controller.Run 函数在末尾调用了 Reflector 对象的 Run 函数， 其内部的核心逻辑就是调用listerWatcher 成员的 ListWatch 方法，开始等待接受资源对象的信息。这部分和 API Server 等待接收来自 ectd 的消息原理是一样的。</p><p>除此之外，它还调用了一个名为<code>processLoop</code>的函数。这个函数的工作也很简单：不断的从 Reflector对象维护的资源对象信息的缓存中取出消息进行处理。</p><h2>回到 Controller Manager</h2><p>通过递归的观察 Controller Manager 的 Run 函数的逻辑，我们基本上可以确定下一阶段要关注的重点：informer 的创建者。因为这整段的逻辑看下来，最终通过<code>List-Watch</code>机制获取资源对象信息的是 informer。而 informer 的创建逻辑并没有在<code>controllerContext.InformerFactory</code>对象初始化的时候完成。那么也就是说，它是在之后的逻辑中对其内部的 informer 成员赋值的。</p><p>再次查看<code>StartControllers</code>的逻辑可知，持有<code>controllerContext.InformerFactory</code>对象的<code>controllerContext</code>，作为参数被传递到了每一个 Controller 的初始化函数。所以，看起来 informer 肯定是在各个 Controller 的逻辑中被赋值的。</p><h2>ReplicaSetController</h2><p>ReplicaSetController 的初始化函数如下所示：</p><div class=\"highlight\"><pre><code class=\"language-go\"><span class=\"kd\">func</span> <span class=\"nf\">startReplicaSetController</span><span class=\"p\">(</span><span class=\"nx\">ctx</span> <span class=\"nx\">ControllerContext</span><span class=\"p\">)</span> <span class=\"p\">(</span><span class=\"nx\">http</span><span class=\"p\">.</span><span class=\"nx\">Handler</span><span class=\"p\">,</span> <span class=\"kt\">bool</span><span class=\"p\">,</span> <span class=\"kt\">error</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"k\">if</span> <span class=\"p\">!</span><span class=\"nx\">ctx</span><span class=\"p\">.</span><span class=\"nx\">AvailableResources</span><span class=\"p\">[</span><span class=\"nx\">schema</span><span class=\"p\">.</span><span class=\"nx\">GroupVersionResource</span><span class=\"p\">{</span><span class=\"nx\">Group</span><span class=\"p\">:</span> <span class=\"s\">&#34;apps&#34;</span><span class=\"p\">,</span> <span class=\"nx\">Version</span><span class=\"p\">:</span> <span class=\"s\">&#34;v1&#34;</span><span class=\"p\">,</span> <span class=\"nx\">Resource</span><span class=\"p\">:</span> <span class=\"s\">&#34;replicasets&#34;</span><span class=\"p\">}]</span> <span class=\"p\">{</span>\n        <span class=\"k\">return</span> <span class=\"kc\">nil</span><span class=\"p\">,</span> <span class=\"kc\">false</span><span class=\"p\">,</span> <span class=\"kc\">nil</span>\n    <span class=\"p\">}</span>\n    <span class=\"k\">go</span> <span class=\"nx\">replicaset</span><span class=\"p\">.</span><span class=\"nf\">NewReplicaSetController</span><span class=\"p\">(</span>\n        <span class=\"nx\">ctx</span><span class=\"p\">.</span><span class=\"nx\">InformerFactory</span><span class=\"p\">.</span><span class=\"nf\">Apps</span><span class=\"p\">().</span><span class=\"nf\">V1</span><span class=\"p\">().</span><span class=\"nf\">ReplicaSets</span><span class=\"p\">(),</span>\n        <span class=\"nx\">ctx</span><span class=\"p\">.</span><span class=\"nx\">InformerFactory</span><span class=\"p\">.</span><span class=\"nf\">Core</span><span class=\"p\">().</span><span class=\"nf\">V1</span><span class=\"p\">().</span><span class=\"nf\">Pods</span><span class=\"p\">(),</span>\n        <span class=\"nx\">ctx</span><span class=\"p\">.</span><span class=\"nx\">ClientBuilder</span><span class=\"p\">.</span><span class=\"nf\">ClientOrDie</span><span class=\"p\">(</span><span class=\"s\">&#34;replicaset-controller&#34;</span><span class=\"p\">),</span>\n        <span class=\"nx\">replicaset</span><span class=\"p\">.</span><span class=\"nx\">BurstReplicas</span><span class=\"p\">,</span>\n    <span class=\"p\">).</span><span class=\"nf\">Run</span><span class=\"p\">(</span><span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"nx\">ctx</span><span class=\"p\">.</span><span class=\"nx\">ComponentConfig</span><span class=\"p\">.</span><span class=\"nx\">ReplicaSetController</span><span class=\"p\">.</span><span class=\"nx\">ConcurrentRSSyncs</span><span class=\"p\">),</span> <span class=\"nx\">ctx</span><span class=\"p\">.</span><span class=\"nx\">Stop</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"kc\">nil</span><span class=\"p\">,</span> <span class=\"kc\">true</span><span class=\"p\">,</span> <span class=\"kc\">nil</span>\n<span class=\"p\">}</span></code></pre></div><p>可以看出在一开始调用 ReplicaSetController 的创建函数的时候，我们就传递进去了两个 Informer。选择<code>ReplicaSets</code>函数跟进去发现，它返回是一个类型为<code>replicaSetInformer</code>的对象。而<code>replicaSetInformer</code>这个类实现了一个名为<code>Informer</code>的方法，它返回的正式一个我们前面说到的<code>SharedIndexInformer</code>类型的对象。我们之前要找的和各个 Controller 相关的 informer 对象也是<code>SharedIndexInformer</code>类型。通过查看<code>Informer</code>函数的逻辑，它一共涉及到以下三个函数：</p><div class=\"highlight\"><pre><code class=\"language-go\"><span class=\"c1\">// NewFilteredReplicaSetInformer constructs a new informer for ReplicaSet type.\n</span><span class=\"c1\">// Always prefer using an informer factory to get a shared informer instead of getting an independent\n</span><span class=\"c1\">// one. This reduces memory footprint and number of connections to the server.\n</span><span class=\"c1\"></span><span class=\"kd\">func</span> <span class=\"nf\">NewFilteredReplicaSetInformer</span><span class=\"p\">(</span><span class=\"nx\">client</span> <span class=\"nx\">kubernetes</span><span class=\"p\">.</span><span class=\"nx\">Interface</span><span class=\"p\">,</span> <span class=\"nx\">namespace</span> <span class=\"kt\">string</span><span class=\"p\">,</span> <span class=\"nx\">resyncPeriod</span> <span class=\"nx\">time</span><span class=\"p\">.</span><span class=\"nx\">Duration</span><span class=\"p\">,</span> <span class=\"nx\">indexers</span> <span class=\"nx\">cache</span><span class=\"p\">.</span><span class=\"nx\">Indexers</span><span class=\"p\">,</span> <span class=\"nx\">tweakListOptions</span> <span class=\"nx\">internalinterfaces</span><span class=\"p\">.</span><span class=\"nx\">TweakListOptionsFunc</span><span class=\"p\">)</span> <span class=\"nx\">cache</span><span class=\"p\">.</span><span class=\"nx\">SharedIndexInformer</span> <span class=\"p\">{</span>\n    <span class=\"k\">return</span> <span class=\"nx\">cache</span><span class=\"p\">.</span><span class=\"nf\">NewSharedIndexInformer</span><span class=\"p\">(</span>\n        <span class=\"o\">&amp;</span><span class=\"nx\">cache</span><span class=\"p\">.</span><span class=\"nx\">ListWatch</span><span class=\"p\">{</span>\n            <span class=\"nx\">ListFunc</span><span class=\"p\">:</span> <span class=\"kd\">func</span><span class=\"p\">(</span><span class=\"nx\">options</span> <span class=\"nx\">metav1</span><span class=\"p\">.</span><span class=\"nx\">ListOptions</span><span class=\"p\">)</span> <span class=\"p\">(</span><span class=\"nx\">runtime</span><span class=\"p\">.</span><span class=\"nx\">Object</span><span class=\"p\">,</span> <span class=\"kt\">error</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n                <span class=\"k\">if</span> <span class=\"nx\">tweakListOptions</span> <span class=\"o\">!=</span> <span class=\"kc\">nil</span> <span class=\"p\">{</span>\n                    <span class=\"nf\">tweakListOptions</span><span class=\"p\">(</span><span class=\"o\">&amp;</span><span class=\"nx\">options</span><span class=\"p\">)</span>\n                <span class=\"p\">}</span>\n                <span class=\"k\">return</span> <span class=\"nx\">client</span><span class=\"p\">.</span><span class=\"nf\">AppsV1</span><span class=\"p\">().</span><span class=\"nf\">ReplicaSets</span><span class=\"p\">(</span><span class=\"nx\">namespace</span><span class=\"p\">).</span><span class=\"nf\">List</span><span class=\"p\">(</span><span class=\"nx\">options</span><span class=\"p\">)</span>\n            <span class=\"p\">},</span>\n            <span class=\"nx\">WatchFunc</span><span class=\"p\">:</span> <span class=\"kd\">func</span><span class=\"p\">(</span><span class=\"nx\">options</span> <span class=\"nx\">metav1</span><span class=\"p\">.</span><span class=\"nx\">ListOptions</span><span class=\"p\">)</span> <span class=\"p\">(</span><span class=\"nx\">watch</span><span class=\"p\">.</span><span class=\"nx\">Interface</span><span class=\"p\">,</span> <span class=\"kt\">error</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n                <span class=\"k\">if</span> <span class=\"nx\">tweakListOptions</span> <span class=\"o\">!=</span> <span class=\"kc\">nil</span> <span class=\"p\">{</span>\n                    <span class=\"nf\">tweakListOptions</span><span class=\"p\">(</span><span class=\"o\">&amp;</span><span class=\"nx\">options</span><span class=\"p\">)</span>\n                <span class=\"p\">}</span>\n                <span class=\"k\">return</span> <span class=\"nx\">client</span><span class=\"p\">.</span><span class=\"nf\">AppsV1</span><span class=\"p\">().</span><span class=\"nf\">ReplicaSets</span><span class=\"p\">(</span><span class=\"nx\">namespace</span><span class=\"p\">).</span><span class=\"nf\">Watch</span><span class=\"p\">(</span><span class=\"nx\">options</span><span class=\"p\">)</span>\n            <span class=\"p\">},</span>\n        <span class=\"p\">},</span>\n        <span class=\"o\">&amp;</span><span class=\"nx\">appsv1</span><span class=\"p\">.</span><span class=\"nx\">ReplicaSet</span><span class=\"p\">{},</span>\n        <span class=\"nx\">resyncPeriod</span><span class=\"p\">,</span>\n        <span class=\"nx\">indexers</span><span class=\"p\">,</span>\n    <span class=\"p\">)</span>\n<span class=\"p\">}</span>\n\n<span class=\"kd\">func</span> <span class=\"p\">(</span><span class=\"nx\">f</span> <span class=\"o\">*</span><span class=\"nx\">replicaSetInformer</span><span class=\"p\">)</span> <span class=\"nf\">defaultInformer</span><span class=\"p\">(</span><span class=\"nx\">client</span> <span class=\"nx\">kubernetes</span><span class=\"p\">.</span><span class=\"nx\">Interface</span><span class=\"p\">,</span> <span class=\"nx\">resyncPeriod</span> <span class=\"nx\">time</span><span class=\"p\">.</span><span class=\"nx\">Duration</span><span class=\"p\">)</span> <span class=\"nx\">cache</span><span class=\"p\">.</span><span class=\"nx\">SharedIndexInformer</span> <span class=\"p\">{</span>\n    <span class=\"k\">return</span> <span class=\"nf\">NewFilteredReplicaSetInformer</span><span class=\"p\">(</span><span class=\"nx\">client</span><span class=\"p\">,</span> <span class=\"nx\">f</span><span class=\"p\">.</span><span class=\"nx\">namespace</span><span class=\"p\">,</span> <span class=\"nx\">resyncPeriod</span><span class=\"p\">,</span> <span class=\"nx\">cache</span><span class=\"p\">.</span><span class=\"nx\">Indexers</span><span class=\"p\">{</span><span class=\"nx\">cache</span><span class=\"p\">.</span><span class=\"nx\">NamespaceIndex</span><span class=\"p\">:</span> <span class=\"nx\">cache</span><span class=\"p\">.</span><span class=\"nx\">MetaNamespaceIndexFunc</span><span class=\"p\">},</span> <span class=\"nx\">f</span><span class=\"p\">.</span><span class=\"nx\">tweakListOptions</span><span class=\"p\">)</span>\n<span class=\"p\">}</span>\n\n<span class=\"kd\">func</span> <span class=\"p\">(</span><span class=\"nx\">f</span> <span class=\"o\">*</span><span class=\"nx\">replicaSetInformer</span><span class=\"p\">)</span> <span class=\"nf\">Informer</span><span class=\"p\">()</span> <span class=\"nx\">cache</span><span class=\"p\">.</span><span class=\"nx\">SharedIndexInformer</span> <span class=\"p\">{</span>\n    <span class=\"k\">return</span> <span class=\"nx\">f</span><span class=\"p\">.</span><span class=\"nx\">factory</span><span class=\"p\">.</span><span class=\"nf\">InformerFor</span><span class=\"p\">(</span><span class=\"o\">&amp;</span><span class=\"nx\">appsv1</span><span class=\"p\">.</span><span class=\"nx\">ReplicaSet</span><span class=\"p\">{},</span> <span class=\"nx\">f</span><span class=\"p\">.</span><span class=\"nx\">defaultInformer</span><span class=\"p\">)</span>\n<span class=\"p\">}</span></code></pre></div><p>看起来，关键的部分大概有两个：</p><ol><li>实际创建这个 Informer 对象的函数应该是<code>NewFilteredReplicaSetInformer</code>方法</li><li>而<code>NewFilteredReplicaSetInformer</code>方法作为参数传到了我们前面提到的<code>controllerContext.InformerFactory</code>对象的<code>InformerFor</code>方法中</li></ol><p>最终，我们通过查看<code>InformerFor</code>方法的逻辑可知，<code>NewFilteredReplicaSetInformer</code> 最终在这里被调用，生成一个 informer 对象返回给外部。</p><div class=\"highlight\"><pre><code class=\"language-go\"><span class=\"c1\">// InternalInformerFor returns the SharedIndexInformer for obj using an internal\n</span><span class=\"c1\">// client.\n</span><span class=\"c1\"></span><span class=\"kd\">func</span> <span class=\"p\">(</span><span class=\"nx\">f</span> <span class=\"o\">*</span><span class=\"nx\">sharedInformerFactory</span><span class=\"p\">)</span> <span class=\"nf\">InformerFor</span><span class=\"p\">(</span><span class=\"nx\">obj</span> <span class=\"nx\">runtime</span><span class=\"p\">.</span><span class=\"nx\">Object</span><span class=\"p\">,</span> <span class=\"nx\">newFunc</span> <span class=\"nx\">internalinterfaces</span><span class=\"p\">.</span><span class=\"nx\">NewInformerFunc</span><span class=\"p\">)</span> <span class=\"nx\">cache</span><span class=\"p\">.</span><span class=\"nx\">SharedIndexInformer</span> <span class=\"p\">{</span>\n    <span class=\"nx\">f</span><span class=\"p\">.</span><span class=\"nx\">lock</span><span class=\"p\">.</span><span class=\"nf\">Lock</span><span class=\"p\">()</span>\n    <span class=\"k\">defer</span> <span class=\"nx\">f</span><span class=\"p\">.</span><span class=\"nx\">lock</span><span class=\"p\">.</span><span class=\"nf\">Unlock</span><span class=\"p\">()</span>\n\n    <span class=\"nx\">informerType</span> <span class=\"o\">:=</span> <span class=\"nx\">reflect</span><span class=\"p\">.</span><span class=\"nf\">TypeOf</span><span class=\"p\">(</span><span class=\"nx\">obj</span><span class=\"p\">)</span>\n    <span class=\"nx\">informer</span><span class=\"p\">,</span> <span class=\"nx\">exists</span> <span class=\"o\">:=</span> <span class=\"nx\">f</span><span class=\"p\">.</span><span class=\"nx\">informers</span><span class=\"p\">[</span><span class=\"nx\">informerType</span><span class=\"p\">]</span>\n    <span class=\"k\">if</span> <span class=\"nx\">exists</span> <span class=\"p\">{</span>\n        <span class=\"k\">return</span> <span class=\"nx\">informer</span>\n    <span class=\"p\">}</span>\n\n    <span class=\"nx\">resyncPeriod</span><span class=\"p\">,</span> <span class=\"nx\">exists</span> <span class=\"o\">:=</span> <span class=\"nx\">f</span><span class=\"p\">.</span><span class=\"nx\">customResync</span><span class=\"p\">[</span><span class=\"nx\">informerType</span><span class=\"p\">]</span>\n    <span class=\"k\">if</span> <span class=\"p\">!</span><span class=\"nx\">exists</span> <span class=\"p\">{</span>\n        <span class=\"nx\">resyncPeriod</span> <span class=\"p\">=</span> <span class=\"nx\">f</span><span class=\"p\">.</span><span class=\"nx\">defaultResync</span>\n    <span class=\"p\">}</span>\n\n    <span class=\"nx\">informer</span> <span class=\"p\">=</span> <span class=\"nf\">newFunc</span><span class=\"p\">(</span><span class=\"nx\">f</span><span class=\"p\">.</span><span class=\"nx\">client</span><span class=\"p\">,</span> <span class=\"nx\">resyncPeriod</span><span class=\"p\">)</span>\n    <span class=\"nx\">f</span><span class=\"p\">.</span><span class=\"nx\">informers</span><span class=\"p\">[</span><span class=\"nx\">informerType</span><span class=\"p\">]</span> <span class=\"p\">=</span> <span class=\"nx\">informer</span>\n\n    <span class=\"k\">return</span> <span class=\"nx\">informer</span>\n<span class=\"p\">}</span></code></pre></div><p>而<code>Informer</code>方法调用的位置是在ReplicaSetController 创建函数中：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/kubernetes/kubernetes/blob/a3ccea9d8743f2ff82e41b6c2af6dc2c41dc7b10/pkg/controller/replicaset/replica_set.go%23L141\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">kubernetes/replica_set.go at a3ccea9d8743f2ff82e41b6c2af6dc2c41dc7b10 · kubernetes/kubernetes · GitHub</a>。并且，同时还通过<code>informer</code>对象向<code>controllerContext.InformerFactory</code>注册了相应的 Handler 函数，以便之后处理接收到的资源对象的信息。</p><h2>链接 ReplicaSetController 和 controllerContext.InformerFactory</h2><p>之所以要看下 <code>ReplicaSetController</code> 和<code>controllerContext.InformerFactory</code>是如何建立链接开始交互的，是因为 ReplicaSetController 最终还是要靠之前提到的<code>Reflector</code>对象调用 List/Watch方法从 API Server 接受资源对象的信息。而这个 <code>Reflector</code>对象是被实现在<code>controllerContext.InformerFactory</code>中的。</p><p>返回上面提到<code>informer</code> 对象调用<code>Run</code>的地方，我们发现，<code>Reflector</code>对象是借助了<code>informer</code> 对象的<code>listWatcher</code>成员创建的。而这个<code>listWatcher</code>成员是在<code>informer</code>对象被创建的时候就赋值进去了，赋值的位置就是<code>NewFilteredReplicaSetInformer</code>函数。</p><p>截止到目前为止，我们找到了资源对象信息的生产者:<code>Reflector</code>，也找到了这些信息的消费者:<code>ReplicaSetController</code>为其<code>informer</code> 注册的处理函数。那么这个消息是如何从生产者送到消费者的呢？</p><p>返回<code>Reflector</code>对象被创建的位置：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/kubernetes/client-go/blob/03bfb9bdcfe5482795b999f39ca3ed9ad42ce5bb/tools/cache/controller.go%23L100\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">client-go/controller.go at 03bfb9bdcfe5482795b999f39ca3ed9ad42ce5bb · kubernetes/client-go · GitHub</a>。我们发现它除了执行生产消息塞入缓存的逻辑<code>wg.StartWithChannel(stopCh, r.Run)</code>，还调用了一个名为<code>processLoop</code>的函数。跟进去之后发现，该函数的功能是不断的从缓存中取出有效的内容（资源对象的信息）并通过一个函数<code>c.config.Process</code>对其进行处理：</p><div class=\"highlight\"><pre><code class=\"language-go\"><span class=\"kd\">func</span> <span class=\"p\">(</span><span class=\"nx\">c</span> <span class=\"o\">*</span><span class=\"nx\">controller</span><span class=\"p\">)</span> <span class=\"nf\">processLoop</span><span class=\"p\">()</span> <span class=\"p\">{</span>\n    <span class=\"k\">for</span> <span class=\"p\">{</span>\n        <span class=\"nx\">obj</span><span class=\"p\">,</span> <span class=\"nx\">err</span> <span class=\"o\">:=</span> <span class=\"nx\">c</span><span class=\"p\">.</span><span class=\"nx\">config</span><span class=\"p\">.</span><span class=\"nx\">Queue</span><span class=\"p\">.</span><span class=\"nf\">Pop</span><span class=\"p\">(</span><span class=\"nf\">PopProcessFunc</span><span class=\"p\">(</span><span class=\"nx\">c</span><span class=\"p\">.</span><span class=\"nx\">config</span><span class=\"p\">.</span><span class=\"nx\">Process</span><span class=\"p\">))</span>\n        <span class=\"k\">if</span> <span class=\"nx\">err</span> <span class=\"o\">!=</span> <span class=\"kc\">nil</span> <span class=\"p\">{</span>\n            <span class=\"k\">if</span> <span class=\"nx\">err</span> <span class=\"o\">==</span> <span class=\"nx\">FIFOClosedError</span> <span class=\"p\">{</span>\n                <span class=\"k\">return</span>\n            <span class=\"p\">}</span>\n            <span class=\"k\">if</span> <span class=\"nx\">c</span><span class=\"p\">.</span><span class=\"nx\">config</span><span class=\"p\">.</span><span class=\"nx\">RetryOnError</span> <span class=\"p\">{</span>\n                <span class=\"c1\">// This is the safe way to re-enqueue.\n</span><span class=\"c1\"></span>                <span class=\"nx\">c</span><span class=\"p\">.</span><span class=\"nx\">config</span><span class=\"p\">.</span><span class=\"nx\">Queue</span><span class=\"p\">.</span><span class=\"nf\">AddIfNotPresent</span><span class=\"p\">(</span><span class=\"nx\">obj</span><span class=\"p\">)</span>\n            <span class=\"p\">}</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">}</span></code></pre></div><p>函数<code>c.config.Process</code>是在 informer 对象 Run 起来的时候被创建的，也就是上面提到的 controller 对象被创建的地方：</p><div class=\"highlight\"><pre><code class=\"language-go\"><span class=\"kd\">func</span> <span class=\"p\">(</span><span class=\"nx\">s</span> <span class=\"o\">*</span><span class=\"nx\">sharedIndexInformer</span><span class=\"p\">)</span> <span class=\"nf\">Run</span><span class=\"p\">(</span><span class=\"nx\">stopCh</span> <span class=\"o\">&lt;-</span><span class=\"kd\">chan</span> <span class=\"kd\">struct</span><span class=\"p\">{})</span> <span class=\"p\">{</span>\n    <span class=\"k\">defer</span> <span class=\"nx\">utilruntime</span><span class=\"p\">.</span><span class=\"nf\">HandleCrash</span><span class=\"p\">()</span>\n\n    <span class=\"nx\">fifo</span> <span class=\"o\">:=</span> <span class=\"nf\">NewDeltaFIFO</span><span class=\"p\">(</span><span class=\"nx\">MetaNamespaceKeyFunc</span><span class=\"p\">,</span> <span class=\"nx\">s</span><span class=\"p\">.</span><span class=\"nx\">indexer</span><span class=\"p\">)</span>\n\n    <span class=\"nx\">cfg</span> <span class=\"o\">:=</span> <span class=\"o\">&amp;</span><span class=\"nx\">Config</span><span class=\"p\">{</span>\n        <span class=\"nx\">Queue</span><span class=\"p\">:</span>            <span class=\"nx\">fifo</span><span class=\"p\">,</span>\n        <span class=\"nx\">ListerWatcher</span><span class=\"p\">:</span>    <span class=\"nx\">s</span><span class=\"p\">.</span><span class=\"nx\">listerWatcher</span><span class=\"p\">,</span>\n        <span class=\"nx\">ObjectType</span><span class=\"p\">:</span>       <span class=\"nx\">s</span><span class=\"p\">.</span><span class=\"nx\">objectType</span><span class=\"p\">,</span>\n        <span class=\"nx\">FullResyncPeriod</span><span class=\"p\">:</span> <span class=\"nx\">s</span><span class=\"p\">.</span><span class=\"nx\">resyncCheckPeriod</span><span class=\"p\">,</span>\n        <span class=\"nx\">RetryOnError</span><span class=\"p\">:</span>     <span class=\"kc\">false</span><span class=\"p\">,</span>\n        <span class=\"nx\">ShouldResync</span><span class=\"p\">:</span>     <span class=\"nx\">s</span><span class=\"p\">.</span><span class=\"nx\">processor</span><span class=\"p\">.</span><span class=\"nx\">shouldResync</span><span class=\"p\">,</span>\n\n        <span class=\"nx\">Process</span><span class=\"p\">:</span> <span class=\"nx\">s</span><span class=\"p\">.</span><span class=\"nx\">HandleDeltas</span><span class=\"p\">,</span>\n    <span class=\"p\">}</span>\n\n    <span class=\"kd\">func</span><span class=\"p\">()</span> <span class=\"p\">{</span>\n        <span class=\"nx\">s</span><span class=\"p\">.</span><span class=\"nx\">startedLock</span><span class=\"p\">.</span><span class=\"nf\">Lock</span><span class=\"p\">()</span>\n        <span class=\"k\">defer</span> <span class=\"nx\">s</span><span class=\"p\">.</span><span class=\"nx\">startedLock</span><span class=\"p\">.</span><span class=\"nf\">Unlock</span><span class=\"p\">()</span>\n\n        <span class=\"nx\">s</span><span class=\"p\">.</span><span class=\"nx\">controller</span> <span class=\"p\">=</span> <span class=\"nf\">New</span><span class=\"p\">(</span><span class=\"nx\">cfg</span><span class=\"p\">)</span>\n        <span class=\"nx\">s</span><span class=\"p\">.</span><span class=\"nx\">controller</span><span class=\"p\">.(</span><span class=\"o\">*</span><span class=\"nx\">controller</span><span class=\"p\">).</span><span class=\"nx\">clock</span> <span class=\"p\">=</span> <span class=\"nx\">s</span><span class=\"p\">.</span><span class=\"nx\">clock</span>\n        <span class=\"nx\">s</span><span class=\"p\">.</span><span class=\"nx\">started</span> <span class=\"p\">=</span> <span class=\"kc\">true</span>\n    <span class=\"p\">}()</span></code></pre></div><p>其中<code>HandleDeltas</code>便是函数<code>c.config.Process</code>的实际值。接下来，我们可以跟进到<code>HandleDeltas</code>函数的内部一探究竟：</p><div class=\"highlight\"><pre><code class=\"language-go\"><span class=\"kd\">func</span> <span class=\"p\">(</span><span class=\"nx\">s</span> <span class=\"o\">*</span><span class=\"nx\">sharedIndexInformer</span><span class=\"p\">)</span> <span class=\"nf\">HandleDeltas</span><span class=\"p\">(</span><span class=\"nx\">obj</span> <span class=\"kd\">interface</span><span class=\"p\">{})</span> <span class=\"kt\">error</span> <span class=\"p\">{</span>\n    <span class=\"nx\">s</span><span class=\"p\">.</span><span class=\"nx\">blockDeltas</span><span class=\"p\">.</span><span class=\"nf\">Lock</span><span class=\"p\">()</span>\n    <span class=\"k\">defer</span> <span class=\"nx\">s</span><span class=\"p\">.</span><span class=\"nx\">blockDeltas</span><span class=\"p\">.</span><span class=\"nf\">Unlock</span><span class=\"p\">()</span>\n\n    <span class=\"c1\">// from oldest to newest\n</span><span class=\"c1\"></span>    <span class=\"k\">for</span> <span class=\"nx\">_</span><span class=\"p\">,</span> <span class=\"nx\">d</span> <span class=\"o\">:=</span> <span class=\"k\">range</span> <span class=\"nx\">obj</span><span class=\"p\">.(</span><span class=\"nx\">Deltas</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n        <span class=\"k\">switch</span> <span class=\"nx\">d</span><span class=\"p\">.</span><span class=\"nx\">Type</span> <span class=\"p\">{</span>\n        <span class=\"k\">case</span> <span class=\"nx\">Sync</span><span class=\"p\">,</span> <span class=\"nx\">Added</span><span class=\"p\">,</span> <span class=\"nx\">Updated</span><span class=\"p\">:</span>\n            <span class=\"nx\">isSync</span> <span class=\"o\">:=</span> <span class=\"nx\">d</span><span class=\"p\">.</span><span class=\"nx\">Type</span> <span class=\"o\">==</span> <span class=\"nx\">Sync</span>\n            <span class=\"nx\">s</span><span class=\"p\">.</span><span class=\"nx\">cacheMutationDetector</span><span class=\"p\">.</span><span class=\"nf\">AddObject</span><span class=\"p\">(</span><span class=\"nx\">d</span><span class=\"p\">.</span><span class=\"nx\">Object</span><span class=\"p\">)</span>\n            <span class=\"k\">if</span> <span class=\"nx\">old</span><span class=\"p\">,</span> <span class=\"nx\">exists</span><span class=\"p\">,</span> <span class=\"nx\">err</span> <span class=\"o\">:=</span> <span class=\"nx\">s</span><span class=\"p\">.</span><span class=\"nx\">indexer</span><span class=\"p\">.</span><span class=\"nf\">Get</span><span class=\"p\">(</span><span class=\"nx\">d</span><span class=\"p\">.</span><span class=\"nx\">Object</span><span class=\"p\">);</span> <span class=\"nx\">err</span> <span class=\"o\">==</span> <span class=\"kc\">nil</span> <span class=\"o\">&amp;&amp;</span> <span class=\"nx\">exists</span> <span class=\"p\">{</span>\n                <span class=\"k\">if</span> <span class=\"nx\">err</span> <span class=\"o\">:=</span> <span class=\"nx\">s</span><span class=\"p\">.</span><span class=\"nx\">indexer</span><span class=\"p\">.</span><span class=\"nf\">Update</span><span class=\"p\">(</span><span class=\"nx\">d</span><span class=\"p\">.</span><span class=\"nx\">Object</span><span class=\"p\">);</span> <span class=\"nx\">err</span> <span class=\"o\">!=</span> <span class=\"kc\">nil</span> <span class=\"p\">{</span>\n                    <span class=\"k\">return</span> <span class=\"nx\">err</span>\n                <span class=\"p\">}</span>\n                <span class=\"nx\">s</span><span class=\"p\">.</span><span class=\"nx\">processor</span><span class=\"p\">.</span><span class=\"nf\">distribute</span><span class=\"p\">(</span><span class=\"nx\">updateNotification</span><span class=\"p\">{</span><span class=\"nx\">oldObj</span><span class=\"p\">:</span> <span class=\"nx\">old</span><span class=\"p\">,</span> <span class=\"nx\">newObj</span><span class=\"p\">:</span> <span class=\"nx\">d</span><span class=\"p\">.</span><span class=\"nx\">Object</span><span class=\"p\">},</span> <span class=\"nx\">isSync</span><span class=\"p\">)</span>\n            <span class=\"p\">}</span> <span class=\"k\">else</span> <span class=\"p\">{</span>\n                <span class=\"k\">if</span> <span class=\"nx\">err</span> <span class=\"o\">:=</span> <span class=\"nx\">s</span><span class=\"p\">.</span><span class=\"nx\">indexer</span><span class=\"p\">.</span><span class=\"nf\">Add</span><span class=\"p\">(</span><span class=\"nx\">d</span><span class=\"p\">.</span><span class=\"nx\">Object</span><span class=\"p\">);</span> <span class=\"nx\">err</span> <span class=\"o\">!=</span> <span class=\"kc\">nil</span> <span class=\"p\">{</span>\n                    <span class=\"k\">return</span> <span class=\"nx\">err</span>\n                <span class=\"p\">}</span>\n                <span class=\"nx\">s</span><span class=\"p\">.</span><span class=\"nx\">processor</span><span class=\"p\">.</span><span class=\"nf\">distribute</span><span class=\"p\">(</span><span class=\"nx\">addNotification</span><span class=\"p\">{</span><span class=\"nx\">newObj</span><span class=\"p\">:</span> <span class=\"nx\">d</span><span class=\"p\">.</span><span class=\"nx\">Object</span><span class=\"p\">},</span> <span class=\"nx\">isSync</span><span class=\"p\">)</span>\n            <span class=\"p\">}</span>\n        <span class=\"k\">case</span> <span class=\"nx\">Deleted</span><span class=\"p\">:</span>\n            <span class=\"k\">if</span> <span class=\"nx\">err</span> <span class=\"o\">:=</span> <span class=\"nx\">s</span><span class=\"p\">.</span><span class=\"nx\">indexer</span><span class=\"p\">.</span><span class=\"nf\">Delete</span><span class=\"p\">(</span><span class=\"nx\">d</span><span class=\"p\">.</span><span class=\"nx\">Object</span><span class=\"p\">);</span> <span class=\"nx\">err</span> <span class=\"o\">!=</span> <span class=\"kc\">nil</span> <span class=\"p\">{</span>\n                <span class=\"k\">return</span> <span class=\"nx\">err</span>\n            <span class=\"p\">}</span>\n            <span class=\"nx\">s</span><span class=\"p\">.</span><span class=\"nx\">processor</span><span class=\"p\">.</span><span class=\"nf\">distribute</span><span class=\"p\">(</span><span class=\"nx\">deleteNotification</span><span class=\"p\">{</span><span class=\"nx\">oldObj</span><span class=\"p\">:</span> <span class=\"nx\">d</span><span class=\"p\">.</span><span class=\"nx\">Object</span><span class=\"p\">},</span> <span class=\"kc\">false</span><span class=\"p\">)</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">}</span>\n    <span class=\"k\">return</span> <span class=\"kc\">nil</span>\n<span class=\"p\">}</span></code></pre></div><p>对于从缓存中取出的每一个 event（资源对象的信息），在<code>HandleDeltas</code>中都会被判定eventType，然后交由<code>processor.distribute</code>方法继续处理。而在<code>processor.distribute</code>方法的内部，我们发现 event 最终给了 processor 的 listener 成员：</p><div class=\"highlight\"><pre><code class=\"language-go\"><span class=\"kd\">func</span> <span class=\"p\">(</span><span class=\"nx\">p</span> <span class=\"o\">*</span><span class=\"nx\">sharedProcessor</span><span class=\"p\">)</span> <span class=\"nf\">distribute</span><span class=\"p\">(</span><span class=\"nx\">obj</span> <span class=\"kd\">interface</span><span class=\"p\">{},</span> <span class=\"nx\">sync</span> <span class=\"kt\">bool</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"nx\">p</span><span class=\"p\">.</span><span class=\"nx\">listenersLock</span><span class=\"p\">.</span><span class=\"nf\">RLock</span><span class=\"p\">()</span>\n    <span class=\"k\">defer</span> <span class=\"nx\">p</span><span class=\"p\">.</span><span class=\"nx\">listenersLock</span><span class=\"p\">.</span><span class=\"nf\">RUnlock</span><span class=\"p\">()</span>\n\n    <span class=\"k\">if</span> <span class=\"nx\">sync</span> <span class=\"p\">{</span>\n        <span class=\"k\">for</span> <span class=\"nx\">_</span><span class=\"p\">,</span> <span class=\"nx\">listener</span> <span class=\"o\">:=</span> <span class=\"k\">range</span> <span class=\"nx\">p</span><span class=\"p\">.</span><span class=\"nx\">syncingListeners</span> <span class=\"p\">{</span>\n            <span class=\"nx\">listener</span><span class=\"p\">.</span><span class=\"nf\">add</span><span class=\"p\">(</span><span class=\"nx\">obj</span><span class=\"p\">)</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">}</span> <span class=\"k\">else</span> <span class=\"p\">{</span>\n        <span class=\"k\">for</span> <span class=\"nx\">_</span><span class=\"p\">,</span> <span class=\"nx\">listener</span> <span class=\"o\">:=</span> <span class=\"k\">range</span> <span class=\"nx\">p</span><span class=\"p\">.</span><span class=\"nx\">listeners</span> <span class=\"p\">{</span>\n            <span class=\"nx\">listener</span><span class=\"p\">.</span><span class=\"nf\">add</span><span class=\"p\">(</span><span class=\"nx\">obj</span><span class=\"p\">)</span>\n        <span class=\"p\">}</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">}</span></code></pre></div><p>由于这个 Processor 对象是 informer 对象的一个内部成员，但是我在查找 ReplicaSet 的 informer 被创建的逻辑的时候并没有发现 Processor.Listeners 或 p.syncingListeners 被赋值。但是我找到了Processor 实现的一个名为 <code>addListener</code>的方法。它看起来是专门为了添加 listener 成员给存在的。</p><div class=\"highlight\"><pre><code class=\"language-go\"><span class=\"kd\">func</span> <span class=\"p\">(</span><span class=\"nx\">p</span> <span class=\"o\">*</span><span class=\"nx\">sharedProcessor</span><span class=\"p\">)</span> <span class=\"nf\">addListener</span><span class=\"p\">(</span><span class=\"nx\">listener</span> <span class=\"o\">*</span><span class=\"nx\">processorListener</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"nx\">p</span><span class=\"p\">.</span><span class=\"nx\">listenersLock</span><span class=\"p\">.</span><span class=\"nf\">Lock</span><span class=\"p\">()</span>\n    <span class=\"k\">defer</span> <span class=\"nx\">p</span><span class=\"p\">.</span><span class=\"nx\">listenersLock</span><span class=\"p\">.</span><span class=\"nf\">Unlock</span><span class=\"p\">()</span>\n\n    <span class=\"nx\">p</span><span class=\"p\">.</span><span class=\"nf\">addListenerLocked</span><span class=\"p\">(</span><span class=\"nx\">listener</span><span class=\"p\">)</span>\n    <span class=\"k\">if</span> <span class=\"nx\">p</span><span class=\"p\">.</span><span class=\"nx\">listenersStarted</span> <span class=\"p\">{</span>\n        <span class=\"nx\">p</span><span class=\"p\">.</span><span class=\"nx\">wg</span><span class=\"p\">.</span><span class=\"nf\">Start</span><span class=\"p\">(</span><span class=\"nx\">listener</span><span class=\"p\">.</span><span class=\"nx\">run</span><span class=\"p\">)</span>\n        <span class=\"nx\">p</span><span class=\"p\">.</span><span class=\"nx\">wg</span><span class=\"p\">.</span><span class=\"nf\">Start</span><span class=\"p\">(</span><span class=\"nx\">listener</span><span class=\"p\">.</span><span class=\"nx\">pop</span><span class=\"p\">)</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">}</span></code></pre></div><p>查看该函数的调用位置可知，仅有一处，在<code>sharedIndexInformer</code>类的<code>AddEventHandlerWithResyncPeriod</code>方法中。看着这个函数名我觉得非常的熟悉，并且它又是属于<code>sharedIndexInformer</code>类，很可能和 informer 对象有关。那也就是说，它可能和 ReplicaSetController 也有关系，毕竟 <code>informer</code> 对象就是在 ReplicaSetController 的 New 函数中被创建的。</p><p>此时，我返回ReplicaSetController的 New 函数。我发现它在创建了<code>informer</code> 对象之后还执行了一个名为 AddHandler 的函数，将处理 event 的函数通过<code>informer</code>对象向<code>controllerContext.InformerFactory</code>注册了相应的 Handler 函数。跟进 AddHandler 这个函数之后我们发现：</p><div class=\"highlight\"><pre><code class=\"language-go\"><span class=\"kd\">func</span> <span class=\"p\">(</span><span class=\"nx\">s</span> <span class=\"o\">*</span><span class=\"nx\">sharedIndexInformer</span><span class=\"p\">)</span> <span class=\"nf\">AddEventHandler</span><span class=\"p\">(</span><span class=\"nx\">handler</span> <span class=\"nx\">ResourceEventHandler</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"nx\">s</span><span class=\"p\">.</span><span class=\"nf\">AddEventHandlerWithResyncPeriod</span><span class=\"p\">(</span><span class=\"nx\">handler</span><span class=\"p\">,</span> <span class=\"nx\">s</span><span class=\"p\">.</span><span class=\"nx\">defaultEventHandlerResyncPeriod</span><span class=\"p\">)</span>\n<span class=\"p\">}</span></code></pre></div><p>果然，我们上面提到的处理 event 的 listener 就是 ReplicaSetController 注册的 Handler 函数。</p><h2>结束语</h2><p>至此，从 ControllerManager 到 API Server 之间通过 List-Watch 机制处理资源对象信息的过程也就梳理完成了。如果你此时返回到本文头部给出的 List-Watch 机制时序图，就可以发现，其实我们这整个一篇 blog，都在说 List-Watch-1这个虚线框内，标号为 0， 4的过程。而标号为2，3的两个过程，真是我们昨天讲的，API Server 和 ectd 交互的过程。</p>", 
            "topic": [
                {
                    "tag": "Kubernetes", 
                    "tagLink": "https://api.zhihu.com/topics/20018384"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50489723", 
            "userName": "要没时间了", 
            "userLink": "https://www.zhihu.com/people/bdbe7f88af9a021c5c770649d266f143", 
            "upvote": 4, 
            "title": "Detect the source code of list-watch 1", 
            "content": "<blockquote>更好的阅读体验请移步至我的 blog：</blockquote><a href=\"https://link.zhihu.com/?target=http%3A//littledriver.net/post/2018/11/21/detect-the-source-code-of-list-watch-between-api-server-and-etcd/\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-1c7297c94cf66e7cc451ad0cdd20a4f8_180x120.jpg\" data-image-width=\"1280\" data-image-height=\"720\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Detect the Source Code of List Watch Between API Server and Etcd</a><p class=\"ztext-empty-paragraph\"><br/></p><h2>写在前面</h2><p>在上一篇文章中，我们通过 Kubernetes 的架构图以及一个 Deployment 资源对象创建的过程大致了解了<code>List-Watch</code>机制在一个 Kubernetes 集群中所起的作用以及它所面临的问题。本文我们将继续深入<code>List-Watch</code>机制的实现原理，从源码的角度再次探索它其中的奥秘。</p><h2>List-Watch 机制时序图</h2><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-acb94786dd7e8634c478d8ac9c1411ac_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1280\" data-rawheight=\"720\" class=\"origin_image zh-lightbox-thumb\" width=\"1280\" data-original=\"https://pic1.zhimg.com/v2-acb94786dd7e8634c478d8ac9c1411ac_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1280&#39; height=&#39;720&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1280\" data-rawheight=\"720\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1280\" data-original=\"https://pic1.zhimg.com/v2-acb94786dd7e8634c478d8ac9c1411ac_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-acb94786dd7e8634c478d8ac9c1411ac_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>通过上面的时序图我们可以看到，在 Controller 和 API Server 交互之前，API Server 和 Kubectl 以及 etcd 还有一段交互的过程。这个过程对于整个 List-Watch 机制是非常重要的，因为它是 List-Watch 机制对外提供的数据的生产过程。所以，本文将对这一过程做出详细的分析。</p><h2>API Server</h2><p>一个资源创建的起点是从 API Server 提供的 HTTP API 开始的。这里之所以没有提到时序图中的 kubelet 是因为，除了使用 kubelet，我们还可以通过 client-go 或者直接发送 HTTP 请求的方式给 API Server 来创建资源。既然<code>List-Watch</code>机制中消息的发送端为 API Server，那么它肯定就提供了相应的 List 和 Watch 的 HTTP。API。通过观察 API Server 中注册 HTTP API 的代码逻辑：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/kubernetes/apiserver/blob/master/pkg/endpoints/installer.go%23L166\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">apiserver/installer.go at master · kubernetes/apiserver · GitHub</a>， 我们可以发现它通过「类型转换」构造了一个 Lister 对象还有一个 Watcher 对象：</p><div class=\"highlight\"><pre><code class=\"language-go\"><span class=\"c1\">// what verbs are supported by the storage, used to know what verbs we support per path\n</span><span class=\"c1\"></span>    <span class=\"nx\">creater</span><span class=\"p\">,</span> <span class=\"nx\">isCreater</span> <span class=\"o\">:=</span> <span class=\"nx\">storage</span><span class=\"p\">.(</span><span class=\"nx\">rest</span><span class=\"p\">.</span><span class=\"nx\">Creater</span><span class=\"p\">)</span>\n    <span class=\"nx\">namedCreater</span><span class=\"p\">,</span> <span class=\"nx\">isNamedCreater</span> <span class=\"o\">:=</span> <span class=\"nx\">storage</span><span class=\"p\">.(</span><span class=\"nx\">rest</span><span class=\"p\">.</span><span class=\"nx\">NamedCreater</span><span class=\"p\">)</span>\n    <span class=\"nx\">lister</span><span class=\"p\">,</span> <span class=\"nx\">isLister</span> <span class=\"o\">:=</span> <span class=\"nx\">storage</span><span class=\"p\">.(</span><span class=\"nx\">rest</span><span class=\"p\">.</span><span class=\"nx\">Lister</span><span class=\"p\">)</span>\n    <span class=\"nx\">getter</span><span class=\"p\">,</span> <span class=\"nx\">isGetter</span> <span class=\"o\">:=</span> <span class=\"nx\">storage</span><span class=\"p\">.(</span><span class=\"nx\">rest</span><span class=\"p\">.</span><span class=\"nx\">Getter</span><span class=\"p\">)</span>\n    <span class=\"nx\">getterWithOptions</span><span class=\"p\">,</span> <span class=\"nx\">isGetterWithOptions</span> <span class=\"o\">:=</span> <span class=\"nx\">storage</span><span class=\"p\">.(</span><span class=\"nx\">rest</span><span class=\"p\">.</span><span class=\"nx\">GetterWithOptions</span><span class=\"p\">)</span>\n    <span class=\"nx\">gracefulDeleter</span><span class=\"p\">,</span> <span class=\"nx\">isGracefulDeleter</span> <span class=\"o\">:=</span> <span class=\"nx\">storage</span><span class=\"p\">.(</span><span class=\"nx\">rest</span><span class=\"p\">.</span><span class=\"nx\">GracefulDeleter</span><span class=\"p\">)</span>\n    <span class=\"nx\">collectionDeleter</span><span class=\"p\">,</span> <span class=\"nx\">isCollectionDeleter</span> <span class=\"o\">:=</span> <span class=\"nx\">storage</span><span class=\"p\">.(</span><span class=\"nx\">rest</span><span class=\"p\">.</span><span class=\"nx\">CollectionDeleter</span><span class=\"p\">)</span>\n    <span class=\"nx\">updater</span><span class=\"p\">,</span> <span class=\"nx\">isUpdater</span> <span class=\"o\">:=</span> <span class=\"nx\">storage</span><span class=\"p\">.(</span><span class=\"nx\">rest</span><span class=\"p\">.</span><span class=\"nx\">Updater</span><span class=\"p\">)</span>\n    <span class=\"nx\">patcher</span><span class=\"p\">,</span> <span class=\"nx\">isPatcher</span> <span class=\"o\">:=</span> <span class=\"nx\">storage</span><span class=\"p\">.(</span><span class=\"nx\">rest</span><span class=\"p\">.</span><span class=\"nx\">Patcher</span><span class=\"p\">)</span>\n    <span class=\"nx\">watcher</span><span class=\"p\">,</span> <span class=\"nx\">isWatcher</span> <span class=\"o\">:=</span> <span class=\"nx\">storage</span><span class=\"p\">.(</span><span class=\"nx\">rest</span><span class=\"p\">.</span><span class=\"nx\">Watcher</span><span class=\"p\">)</span>\n    <span class=\"nx\">connecter</span><span class=\"p\">,</span> <span class=\"nx\">isConnecter</span> <span class=\"o\">:=</span> <span class=\"nx\">storage</span><span class=\"p\">.(</span><span class=\"nx\">rest</span><span class=\"p\">.</span><span class=\"nx\">Connecter</span><span class=\"p\">)</span>\n    <span class=\"nx\">storageMeta</span><span class=\"p\">,</span> <span class=\"nx\">isMetadata</span> <span class=\"o\">:=</span> <span class=\"nx\">storage</span><span class=\"p\">.(</span><span class=\"nx\">rest</span><span class=\"p\">.</span><span class=\"nx\">StorageMetadata</span><span class=\"p\">)</span></code></pre></div><p>顺着 <code>registerResourceHandlers</code>函数的逻辑往下看我们可以知道，无论是 Lister 还是 Watcher，都是通过一个叫做<code>restfulListResource</code>的方法封装了一下暴露给外部使用的：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/kubernetes/apiserver/blob/master/pkg/endpoints/installer.go%23L610\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">apiserver/installer.go at master · kubernetes/apiserver · GitHub</a>。而通过进一步观察这个函数的<a href=\"https://link.zhihu.com/?target=https%3A//github.com/kubernetes/apiserver/blob/master/pkg/endpoints/installer.go%23L1022\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">内部逻辑</a>我们也可以看到，watcher 和 lister 最终在名为<code>ListResource</code>的方法内执行其内部真正的逻辑。具体当一个 GET 请求过来调用的是 Watch 还是 List 接口，是通过请求当中的一个参数来确定的：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/kubernetes/apiserver/blob/b8915a5609e4d7553d92f0d431ba04ecf9b52777/pkg/endpoints/handlers/get.go%23L234\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">apiserver/get.go at b8915a5609e4d7553d92f0d431ba04ecf9b52777 · kubernetes/apiserver · GitHub</a>。</p><p>假设目前的 HTTP 请求是 Watch。那么在<code>ListResource</code> 的逻辑中就会走到<a href=\"https://link.zhihu.com/?target=https%3A//github.com/kubernetes/apiserver/blob/b8915a5609e4d7553d92f0d431ba04ecf9b52777/pkg/endpoints/handlers/get.go%23L249\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">apiserver/get.go at b8915a5609e4d7553d92f0d431ba04ecf9b52777 · kubernetes/apiserver · GitHub</a> 这一步。它调用 Watcher 的 Watch 方法，创建了一个<code>Watch-Interface</code>类型的对象。然后将其传递至 <code>serveWatch</code>方法：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/kubernetes/apiserver/blob/8a1312795085bced3bc5d4553b97c450a79fc420/pkg/endpoints/handlers/watch.go%23L66\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">apiserver/watch.go at 8a1312795085bced3bc5d4553b97c450a79fc420 · kubernetes/apiserver · GitHub</a>。通过观察 <code>serveWatch</code>方法的内部逻辑可知，之前创建的<code>Watch-Interface</code>的对象被塞入了 ServeHTTP中，然后利用这个对象内部一个用于传递「资源对象信息的 channel」中的消息来对外部的「Watch」 请求提供服务：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/kubernetes/apiserver/blob/8a1312795085bced3bc5d4553b97c450a79fc420/pkg/endpoints/handlers/watch.go%23L195\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">apiserver/watch.go at 8a1312795085bced3bc5d4553b97c450a79fc420 · kubernetes/apiserver · GitHub</a></p><p>到目前为止，通过对 API Server 关于<code>List-Watch</code> 机制的源码梳理，我们基本可以确定，API Server 获取资源对象信息的逻辑主要是实现于<code>Watch-Interface</code>类型的对象以及向其内部的channel 传递消息的发送端。而在上面的描述中，我们还可以梳理出一条<code>Watch-Interface</code>类型的对象的创建链路：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-8c6601a9611efa8d575cad0163811557_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1282\" data-rawheight=\"722\" class=\"origin_image zh-lightbox-thumb\" width=\"1282\" data-original=\"https://pic4.zhimg.com/v2-8c6601a9611efa8d575cad0163811557_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1282&#39; height=&#39;722&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1282\" data-rawheight=\"722\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1282\" data-original=\"https://pic4.zhimg.com/v2-8c6601a9611efa8d575cad0163811557_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-8c6601a9611efa8d575cad0163811557_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>既然 Watcher 对象是通过 Storage 对象进行转换而来的，那么就说明<code>watch.Interface</code>中的方法大概率也是在 Storage 类型的对象中实现的。在<a href=\"https://link.zhihu.com/?target=https%3A//github.com/kubernetes/apiserver/blob/b8915a5609e4d7553d92f0d431ba04ecf9b52777/pkg/registry/generic/registry/store.go%23L1159\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">apiserver/store.go at b8915a5609e4d7553d92f0d431ba04ecf9b52777 · kubernetes/apiserver · GitHub</a> 文件中，我们看到了 Storage类型实现了List 和 Watch 方法，继续递归的跟进下面的逻辑发现，最终，在名为<code>WatchPredicate</code>的函数中，调用了名为 Storage 成员（与上面说的 Storage 类型的对象不是一个，它只是 struct 中其中一个 filed）的 Watch 方法，返回了类型为<code>watch.Interface</code> 的对象：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/kubernetes/apiserver/blob/b8915a5609e4d7553d92f0d431ba04ecf9b52777/pkg/registry/generic/registry/store.go%23L1182\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">apiserver/store.go at b8915a5609e4d7553d92f0d431ba04ecf9b52777 · kubernetes/apiserver · GitHub</a>。而这个名为 Storage 成员的类型为<code>DryRunnableStorage</code>。通过查看 <code>DryRunnableStorage</code>这个类型的定义可知，其内部包含了一个类型为<code>storage.Interface</code>的对象，该 Interface 内部涵盖了 Watch 和 List 方法，这两个方法应该会被具体的某个资源实现，如 Pod， Deployment 等。</p><blockquote> WARNING: 读者阅读到这里想必有点头晕，因为逻辑嵌套的层数太多，并且很多方法和成员的名字都是相同的。所以这里建议大家根据我贴出的源码的链接，按照 blog 中叙述的顺序画一个流程图，会看的更清楚。  <br/> </blockquote><p>全局搜索一下创建<code>Store</code>类型对象的地方可知，几乎存在于每一个资源的目录下：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-e9987af0c16e65a65358f01bc788aa44_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"647\" data-rawheight=\"457\" class=\"origin_image zh-lightbox-thumb\" width=\"647\" data-original=\"https://pic1.zhimg.com/v2-e9987af0c16e65a65358f01bc788aa44_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;647&#39; height=&#39;457&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"647\" data-rawheight=\"457\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"647\" data-original=\"https://pic1.zhimg.com/v2-e9987af0c16e65a65358f01bc788aa44_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-e9987af0c16e65a65358f01bc788aa44_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>我们挑选 Deployment 资源对象的创建<code>Store</code>类型对象的逻辑：</p><div class=\"highlight\"><pre><code class=\"language-go\"><span class=\"c1\">// NewREST returns a RESTStorage object that will work against deployments.\n</span><span class=\"c1\"></span><span class=\"kd\">func</span> <span class=\"nf\">NewREST</span><span class=\"p\">(</span><span class=\"nx\">optsGetter</span> <span class=\"nx\">generic</span><span class=\"p\">.</span><span class=\"nx\">RESTOptionsGetter</span><span class=\"p\">)</span> <span class=\"p\">(</span><span class=\"o\">*</span><span class=\"nx\">REST</span><span class=\"p\">,</span> <span class=\"o\">*</span><span class=\"nx\">StatusREST</span><span class=\"p\">,</span> <span class=\"o\">*</span><span class=\"nx\">RollbackREST</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"nx\">store</span> <span class=\"o\">:=</span> <span class=\"o\">&amp;</span><span class=\"nx\">genericregistry</span><span class=\"p\">.</span><span class=\"nx\">Store</span><span class=\"p\">{</span>\n        <span class=\"nx\">NewFunc</span><span class=\"p\">:</span>                  <span class=\"kd\">func</span><span class=\"p\">()</span> <span class=\"nx\">runtime</span><span class=\"p\">.</span><span class=\"nx\">Object</span> <span class=\"p\">{</span> <span class=\"k\">return</span> <span class=\"o\">&amp;</span><span class=\"nx\">extensions</span><span class=\"p\">.</span><span class=\"nx\">Deployment</span><span class=\"p\">{}</span> <span class=\"p\">},</span>\n        <span class=\"nx\">NewListFunc</span><span class=\"p\">:</span>              <span class=\"kd\">func</span><span class=\"p\">()</span> <span class=\"nx\">runtime</span><span class=\"p\">.</span><span class=\"nx\">Object</span> <span class=\"p\">{</span> <span class=\"k\">return</span> <span class=\"o\">&amp;</span><span class=\"nx\">extensions</span><span class=\"p\">.</span><span class=\"nx\">DeploymentList</span><span class=\"p\">{}</span> <span class=\"p\">},</span>\n        <span class=\"nx\">DefaultQualifiedResource</span><span class=\"p\">:</span> <span class=\"nx\">extensions</span><span class=\"p\">.</span><span class=\"nf\">Resource</span><span class=\"p\">(</span><span class=\"s\">&#34;deployments&#34;</span><span class=\"p\">),</span>\n\n        <span class=\"nx\">CreateStrategy</span><span class=\"p\">:</span> <span class=\"nx\">deployment</span><span class=\"p\">.</span><span class=\"nx\">Strategy</span><span class=\"p\">,</span>\n        <span class=\"nx\">UpdateStrategy</span><span class=\"p\">:</span> <span class=\"nx\">deployment</span><span class=\"p\">.</span><span class=\"nx\">Strategy</span><span class=\"p\">,</span>\n        <span class=\"nx\">DeleteStrategy</span><span class=\"p\">:</span> <span class=\"nx\">deployment</span><span class=\"p\">.</span><span class=\"nx\">Strategy</span><span class=\"p\">,</span>\n\n        <span class=\"nx\">TableConvertor</span><span class=\"p\">:</span> <span class=\"nx\">printerstorage</span><span class=\"p\">.</span><span class=\"nx\">TableConvertor</span><span class=\"p\">{</span><span class=\"nx\">TablePrinter</span><span class=\"p\">:</span> <span class=\"nx\">printers</span><span class=\"p\">.</span><span class=\"nf\">NewTablePrinter</span><span class=\"p\">().</span><span class=\"nf\">With</span><span class=\"p\">(</span><span class=\"nx\">printersinternal</span><span class=\"p\">.</span><span class=\"nx\">AddHandlers</span><span class=\"p\">)},</span>\n    <span class=\"p\">}</span>\n    <span class=\"nx\">options</span> <span class=\"o\">:=</span> <span class=\"o\">&amp;</span><span class=\"nx\">generic</span><span class=\"p\">.</span><span class=\"nx\">StoreOptions</span><span class=\"p\">{</span><span class=\"nx\">RESTOptions</span><span class=\"p\">:</span> <span class=\"nx\">optsGetter</span><span class=\"p\">}</span>\n    <span class=\"k\">if</span> <span class=\"nx\">err</span> <span class=\"o\">:=</span> <span class=\"nx\">store</span><span class=\"p\">.</span><span class=\"nf\">CompleteWithOptions</span><span class=\"p\">(</span><span class=\"nx\">options</span><span class=\"p\">);</span> <span class=\"nx\">err</span> <span class=\"o\">!=</span> <span class=\"kc\">nil</span> <span class=\"p\">{</span>\n        <span class=\"nb\">panic</span><span class=\"p\">(</span><span class=\"nx\">err</span><span class=\"p\">)</span> <span class=\"c1\">// TODO: Propagate error up\n</span><span class=\"c1\"></span>    <span class=\"p\">}</span>\n\n    <span class=\"nx\">statusStore</span> <span class=\"o\">:=</span> <span class=\"o\">*</span><span class=\"nx\">store</span>\n    <span class=\"nx\">statusStore</span><span class=\"p\">.</span><span class=\"nx\">UpdateStrategy</span> <span class=\"p\">=</span> <span class=\"nx\">deployment</span><span class=\"p\">.</span><span class=\"nx\">StatusStrategy</span>\n    <span class=\"k\">return</span> <span class=\"o\">&amp;</span><span class=\"nx\">REST</span><span class=\"p\">{</span><span class=\"nx\">store</span><span class=\"p\">,</span> <span class=\"p\">[]</span><span class=\"kt\">string</span><span class=\"p\">{</span><span class=\"s\">&#34;all&#34;</span><span class=\"p\">}},</span> <span class=\"o\">&amp;</span><span class=\"nx\">StatusREST</span><span class=\"p\">{</span><span class=\"nx\">store</span><span class=\"p\">:</span> <span class=\"o\">&amp;</span><span class=\"nx\">statusStore</span><span class=\"p\">},</span> <span class=\"o\">&amp;</span><span class=\"nx\">RollbackREST</span><span class=\"p\">{</span><span class=\"nx\">store</span><span class=\"p\">:</span> <span class=\"nx\">store</span><span class=\"p\">}</span>\n<span class=\"p\">}</span></code></pre></div><p>可以看出在构造<code>genericregistry.Store</code>对象的时候，没有有指定一个名为 Storage 的成员。在执行 return 语句之前仅仅只调用了一个<code>store.CompleteWithOptions</code>方法。跟进去之后，豁然开朗：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/kubernetes/apiserver/blob/b8915a5609e4d7553d92f0d431ba04ecf9b52777/pkg/registry/generic/registry/store.go%23L1383\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">apiserver/store.go at b8915a5609e4d7553d92f0d431ba04ecf9b52777 · kubernetes/apiserver · GitHub</a>。名为 Storage 成员内部包含的类型为<code>storage.Interface</code>的对象最终是被一个名为<code>Decorator</code>的方法创建的。而这个方法和<code>opts</code>这个变量有关。顺着这条线向上查找，我们最终发现，在 Deployment 代码逻辑中有一个名为 NewStorage 的函数：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/kubernetes/kubernetes/blob/master/pkg/registry/apps/deployment/storage/storage.go%23L56\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">kubernetes/storage.go at master · kubernetes/kubernetes · GitHub</a>。<code>opts</code> 这个变量的值来自于这个函数的参数<code>optsGetter</code>。</p><p>在向上寻找 opts 参数的过程中我们发现这条链路是比较长的，也是比较绕的，很难清晰的去定位到它第一次被创建的地方。所以，我们换一种思路：因为看到 opts 这个参数的类型为<code>generic.RESTOptionsGetter</code>，所以我们在全局可以搜一下，哪里有对这个类型变量的赋值操作并且和 Storage 有关的。最后，我们定位到了这里：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/kubernetes/apiserver/blob/c53cd379d4b8e8acbe23a7a3b40c949687ba9926/pkg/server/options/etcd.go%23L186\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">apiserver/etcd.go at c53cd379d4b8e8acbe23a7a3b40c949687ba9926 · kubernetes/apiserver · GitHub</a>。这是一段和 etcd 配置有关的逻辑。这段逻辑在 API Server 启动构造其使用的配置的逻辑中有调用过：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/kubernetes/kubernetes/blob/b1a52a38e9e3651680655416cc7afbec5e119854/cmd/kube-apiserver/app/server.go%23L424\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">kubernetes/server.go at b1a52a38e9e3651680655416cc7afbec5e119854 · kubernetes/kubernetes · GitHub</a>。<code>buildGenericConfig</code>函数构造的通用配置，最终赋值给了启动 Master 节点所需要的配置集合：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/kubernetes/kubernetes/blob/b1a52a38e9e3651680655416cc7afbec5e119854/cmd/kube-apiserver/app/server.go%23L318\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">kubernetes/server.go at b1a52a38e9e3651680655416cc7afbec5e119854 · kubernetes/kubernetes · GitHub</a>。而这部分配置最终被用于创建 API Server： <a href=\"https://link.zhihu.com/?target=https%3A//github.com/kubernetes/kubernetes/blob/b1a52a38e9e3651680655416cc7afbec5e119854/cmd/kube-apiserver/app/server.go%23L175\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">kubernetes/server.go at b1a52a38e9e3651680655416cc7afbec5e119854 · kubernetes/kubernetes · GitHub</a>。在 <code>CreateKubeAPIServer</code>函数内，我们可以看到，通过参数传递进来的 master.Config 最终调用了一个名为<code>Complete</code>的函数处理了一下相关配置，并且通过它的返回值调用了一个 New 函数。而在 New 函数的内部，也通过调用一个名为<code>Install</code>的函数，将<code>restOptionsGetter</code>参数传了进去。</p><p>此时，我们再次返回到Deployment 代码逻辑中的 NewStorage 函数被调用的地方：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/kubernetes/kubernetes/blob/7f23a743e8c23ac6489340bbb34fa6f1d392db9d/pkg/registry/apps/rest/storage_apps.go%23L60\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">kubernetes/storage_apps.go at 7f23a743e8c23ac6489340bbb34fa6f1d392db9d · kubernetes/kubernetes · GitHub</a>，随机选取一个版本的函数<code>v1beta1Storage</code>，它在当前文件的一个名为<code>NewRestStorage</code>函数中被调用：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/kubernetes/kubernetes/blob/7f23a743e8c23ac6489340bbb34fa6f1d392db9d/pkg/registry/apps/rest/storage_apps.go%23L38\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/kubernetes/k</span><span class=\"invisible\">ubernetes/blob/7f23a743e8c23ac6489340bbb34fa6f1d392db9d/pkg/registry/apps/rest/storage_apps.go#L38</span><span class=\"ellipsis\"></span></a>。随后，我们按照这条调用链路再继续向上寻找<code>restOptionsGetter</code>参数被赋值位置。随即定位到了<a href=\"https://link.zhihu.com/?target=https%3A//github.com/kubernetes/kubernetes/blob/ec2e767e59395376fa191d7c56a74f53936b7653/pkg/master/master.go%23L401\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">kubernetes/master.go at ec2e767e59395376fa191d7c56a74f53936b7653 · kubernetes/kubernetes · GitHub</a>中的 <code>InstallAPIs</code>函数。</p><p>Bingo，此时我们可以将调用<code>InstallAPIs</code>函数的逻辑作为桥梁，将我们上面整个的查找流程链接起来。所以，当前我们已经可以确认的是，我们之前说的<code>opts</code>变量已经找到了出处。但实际上，在创建 Deployment storage 对象时，名为 Storage 成员内部包含的类型为<code>storage.Interface</code>的对象最终是被一个名为<code>Decorator</code>的方法创建的，这个<code>Decorator</code>方法来自于<code>opts</code>变量。再次查看<code>Decorator</code>方法的定义可知，它被包含在一个 Interface 内部的函数的返回值中，而这个函数正是创建<code>opts</code>这个变量所在的类必须要实现的：<code>GetRESTOptions</code>。所以，我们需要再次回到和 etcd 配置有关的逻辑：</p><div class=\"highlight\"><pre><code class=\"language-go\"><span class=\"kd\">func</span> <span class=\"p\">(</span><span class=\"nx\">s</span> <span class=\"o\">*</span><span class=\"nx\">EtcdOptions</span><span class=\"p\">)</span> <span class=\"nf\">ApplyWithStorageFactoryTo</span><span class=\"p\">(</span><span class=\"nx\">factory</span> <span class=\"nx\">serverstorage</span><span class=\"p\">.</span><span class=\"nx\">StorageFactory</span><span class=\"p\">,</span> <span class=\"nx\">c</span> <span class=\"o\">*</span><span class=\"nx\">server</span><span class=\"p\">.</span><span class=\"nx\">Config</span><span class=\"p\">)</span> <span class=\"kt\">error</span> <span class=\"p\">{</span>\n    <span class=\"k\">if</span> <span class=\"nx\">err</span> <span class=\"o\">:=</span> <span class=\"nx\">s</span><span class=\"p\">.</span><span class=\"nf\">addEtcdHealthEndpoint</span><span class=\"p\">(</span><span class=\"nx\">c</span><span class=\"p\">);</span> <span class=\"nx\">err</span> <span class=\"o\">!=</span> <span class=\"kc\">nil</span> <span class=\"p\">{</span>\n        <span class=\"k\">return</span> <span class=\"nx\">err</span>\n    <span class=\"p\">}</span>\n    <span class=\"nx\">c</span><span class=\"p\">.</span><span class=\"nx\">RESTOptionsGetter</span> <span class=\"p\">=</span> <span class=\"o\">&amp;</span><span class=\"nx\">StorageFactoryRestOptionsFactory</span><span class=\"p\">{</span><span class=\"nx\">Options</span><span class=\"p\">:</span> <span class=\"o\">*</span><span class=\"nx\">s</span><span class=\"p\">,</span> <span class=\"nx\">StorageFactory</span><span class=\"p\">:</span> <span class=\"nx\">factory</span><span class=\"p\">}</span>\n    <span class=\"k\">return</span> <span class=\"kc\">nil</span>\n<span class=\"p\">}</span>\n\n<span class=\"kd\">type</span> <span class=\"nx\">StorageFactoryRestOptionsFactory</span> <span class=\"kd\">struct</span> <span class=\"p\">{</span>\n    <span class=\"nx\">Options</span>        <span class=\"nx\">EtcdOptions</span>\n    <span class=\"nx\">StorageFactory</span> <span class=\"nx\">serverstorage</span><span class=\"p\">.</span><span class=\"nx\">StorageFactory</span>\n<span class=\"p\">}</span>\n\n<span class=\"kd\">func</span> <span class=\"p\">(</span><span class=\"nx\">f</span> <span class=\"o\">*</span><span class=\"nx\">StorageFactoryRestOptionsFactory</span><span class=\"p\">)</span> <span class=\"nf\">GetRESTOptions</span><span class=\"p\">(</span><span class=\"nx\">resource</span> <span class=\"nx\">schema</span><span class=\"p\">.</span><span class=\"nx\">GroupResource</span><span class=\"p\">)</span> <span class=\"p\">(</span><span class=\"nx\">generic</span><span class=\"p\">.</span><span class=\"nx\">RESTOptions</span><span class=\"p\">,</span> <span class=\"kt\">error</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"nx\">storageConfig</span><span class=\"p\">,</span> <span class=\"nx\">err</span> <span class=\"o\">:=</span> <span class=\"nx\">f</span><span class=\"p\">.</span><span class=\"nx\">StorageFactory</span><span class=\"p\">.</span><span class=\"nf\">NewConfig</span><span class=\"p\">(</span><span class=\"nx\">resource</span><span class=\"p\">)</span>\n    <span class=\"k\">if</span> <span class=\"nx\">err</span> <span class=\"o\">!=</span> <span class=\"kc\">nil</span> <span class=\"p\">{</span>\n        <span class=\"k\">return</span> <span class=\"nx\">generic</span><span class=\"p\">.</span><span class=\"nx\">RESTOptions</span><span class=\"p\">{},</span> <span class=\"nx\">fmt</span><span class=\"p\">.</span><span class=\"nf\">Errorf</span><span class=\"p\">(</span><span class=\"s\">&#34;unable to find storage destination for %v, due to %v&#34;</span><span class=\"p\">,</span> <span class=\"nx\">resource</span><span class=\"p\">,</span> <span class=\"nx\">err</span><span class=\"p\">.</span><span class=\"nf\">Error</span><span class=\"p\">())</span>\n    <span class=\"p\">}</span>\n\n    <span class=\"nx\">ret</span> <span class=\"o\">:=</span> <span class=\"nx\">generic</span><span class=\"p\">.</span><span class=\"nx\">RESTOptions</span><span class=\"p\">{</span>\n        <span class=\"nx\">StorageConfig</span><span class=\"p\">:</span>           <span class=\"nx\">storageConfig</span><span class=\"p\">,</span>\n        <span class=\"nx\">Decorator</span><span class=\"p\">:</span>               <span class=\"nx\">generic</span><span class=\"p\">.</span><span class=\"nx\">UndecoratedStorage</span><span class=\"p\">,</span>\n        <span class=\"nx\">DeleteCollectionWorkers</span><span class=\"p\">:</span> <span class=\"nx\">f</span><span class=\"p\">.</span><span class=\"nx\">Options</span><span class=\"p\">.</span><span class=\"nx\">DeleteCollectionWorkers</span><span class=\"p\">,</span>\n        <span class=\"nx\">EnableGarbageCollection</span><span class=\"p\">:</span> <span class=\"nx\">f</span><span class=\"p\">.</span><span class=\"nx\">Options</span><span class=\"p\">.</span><span class=\"nx\">EnableGarbageCollection</span><span class=\"p\">,</span>\n        <span class=\"nx\">ResourcePrefix</span><span class=\"p\">:</span>          <span class=\"nx\">f</span><span class=\"p\">.</span><span class=\"nx\">StorageFactory</span><span class=\"p\">.</span><span class=\"nf\">ResourcePrefix</span><span class=\"p\">(</span><span class=\"nx\">resource</span><span class=\"p\">),</span>\n        <span class=\"nx\">CountMetricPollPeriod</span><span class=\"p\">:</span>   <span class=\"nx\">f</span><span class=\"p\">.</span><span class=\"nx\">Options</span><span class=\"p\">.</span><span class=\"nx\">StorageConfig</span><span class=\"p\">.</span><span class=\"nx\">CountMetricPollPeriod</span><span class=\"p\">,</span>\n    <span class=\"p\">}</span>\n    <span class=\"k\">if</span> <span class=\"nx\">f</span><span class=\"p\">.</span><span class=\"nx\">Options</span><span class=\"p\">.</span><span class=\"nx\">EnableWatchCache</span> <span class=\"p\">{</span>\n        <span class=\"nx\">sizes</span><span class=\"p\">,</span> <span class=\"nx\">err</span> <span class=\"o\">:=</span> <span class=\"nf\">ParseWatchCacheSizes</span><span class=\"p\">(</span><span class=\"nx\">f</span><span class=\"p\">.</span><span class=\"nx\">Options</span><span class=\"p\">.</span><span class=\"nx\">WatchCacheSizes</span><span class=\"p\">)</span>\n        <span class=\"k\">if</span> <span class=\"nx\">err</span> <span class=\"o\">!=</span> <span class=\"kc\">nil</span> <span class=\"p\">{</span>\n            <span class=\"k\">return</span> <span class=\"nx\">generic</span><span class=\"p\">.</span><span class=\"nx\">RESTOptions</span><span class=\"p\">{},</span> <span class=\"nx\">err</span>\n        <span class=\"p\">}</span>\n        <span class=\"nx\">cacheSize</span><span class=\"p\">,</span> <span class=\"nx\">ok</span> <span class=\"o\">:=</span> <span class=\"nx\">sizes</span><span class=\"p\">[</span><span class=\"nx\">resource</span><span class=\"p\">]</span>\n        <span class=\"k\">if</span> <span class=\"p\">!</span><span class=\"nx\">ok</span> <span class=\"p\">{</span>\n            <span class=\"nx\">cacheSize</span> <span class=\"p\">=</span> <span class=\"nx\">f</span><span class=\"p\">.</span><span class=\"nx\">Options</span><span class=\"p\">.</span><span class=\"nx\">DefaultWatchCacheSize</span>\n        <span class=\"p\">}</span>\n        <span class=\"nx\">ret</span><span class=\"p\">.</span><span class=\"nx\">Decorator</span> <span class=\"p\">=</span> <span class=\"nx\">genericregistry</span><span class=\"p\">.</span><span class=\"nf\">StorageWithCacher</span><span class=\"p\">(</span><span class=\"nx\">cacheSize</span><span class=\"p\">)</span>\n    <span class=\"p\">}</span>\n\n    <span class=\"k\">return</span> <span class=\"nx\">ret</span><span class=\"p\">,</span> <span class=\"kc\">nil</span>\n<span class=\"p\">}</span></code></pre></div><p>可以看到<code>GetRESTOptions</code>最终被<code>StorageFactoryRestOptionsFactory</code>类实现。查看<code>genericregistry.StorageWithCacher</code>的定义，一路跟下去，就会发现，我们最终是创建了一个名为<code>cacher</code>类型为<code>storage.Interface</code>的变量，它将作为 Decorator 的值：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/kubernetes/apiserver/blob/b8915a5609e4d7553d92f0d431ba04ecf9b52777/pkg/registry/generic/registry/storage_factory.go%23L65\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">apiserver/storage_factory.go at b8915a5609e4d7553d92f0d431ba04ecf9b52777 · kubernetes/apiserver · GitHub</a>。如果再深入至<code>cacher</code>创建的逻辑，可以看到，它是实现了<code>storage.Interface</code>的全部接口的：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/kubernetes/apiserver/blob/cf5eff4f5e8f6019796cb18c69918b9f2f09e6db/pkg/storage/cacher/cacher.go%23L160\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">apiserver/cacher.go at cf5eff4f5e8f6019796cb18c69918b9f2f09e6db · kubernetes/apiserver · GitHub</a>。</p><p>还记得我们当时在阅读API Server中关于<code>ListResource</code>方法的时候，发现最终通过 ServeHTTP 函数对外提供服务的是一个<code>storage.Interface</code>类型对象调用了其 Watch 接口的返回值，即一个<code>Watch-Interface</code>类型的对象。在<code>cacher</code>的实现部分，我们同样可以找到一个这样的函数：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/kubernetes/apiserver/blob/cf5eff4f5e8f6019796cb18c69918b9f2f09e6db/pkg/storage/cacher/cacher.go%23L292\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">apiserver/cacher.go at cf5eff4f5e8f6019796cb18c69918b9f2f09e6db · kubernetes/apiserver · GitHub</a>，其内部将会为我们创建一个类型为<code>cacheWatcher</code>的对象<a href=\"https://link.zhihu.com/?target=https%3A//github.com/kubernetes/apiserver/blob/cf5eff4f5e8f6019796cb18c69918b9f2f09e6db/pkg/storage/cacher/cacher.go%23L794\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">apiserver/cacher.go at cf5eff4f5e8f6019796cb18c69918b9f2f09e6db · kubernetes/apiserver · GitHub</a>。这个对象内部有一个非常重要的成员：<code>result:    make(chan watch.Event, chanSize)</code>，它是被一个名为<code>ResultChan</code>的函数暴露给外部使用的：</p><div class=\"highlight\"><pre><code class=\"language-go\"><span class=\"c1\">// Implements watch.Interface.\n</span><span class=\"c1\"></span><span class=\"kd\">func</span> <span class=\"p\">(</span><span class=\"nx\">c</span> <span class=\"o\">*</span><span class=\"nx\">cacheWatcher</span><span class=\"p\">)</span> <span class=\"nf\">ResultChan</span><span class=\"p\">()</span> <span class=\"o\">&lt;-</span><span class=\"kd\">chan</span> <span class=\"nx\">watch</span><span class=\"p\">.</span><span class=\"nx\">Event</span> <span class=\"p\">{</span>\n    <span class=\"k\">return</span> <span class=\"nx\">c</span><span class=\"p\">.</span><span class=\"nx\">result</span>\n<span class=\"p\">}</span></code></pre></div><p>这个函数相信你看到后会非常非常的熟悉，因为我们在ServeHTTP 函数中看到过它曾经被<code>watcher.Watch()</code>的返回值调用：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/kubernetes/apiserver/blob/8a1312795085bced3bc5d4553b97c450a79fc420/pkg/endpoints/handlers/watch.go%23L195\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">apiserver/watch.go at 8a1312795085bced3bc5d4553b97c450a79fc420 · kubernetes/apiserver · GitHub</a>。<code>newCacheWatcher</code>除了返回一个<code>cacheWatcher</code>类型的对象之外，还会启动一个goroutine, 执行一个名为<code>process</code>的函数：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/kubernetes/apiserver/blob/cf5eff4f5e8f6019796cb18c69918b9f2f09e6db/pkg/storage/cacher/cacher.go%23L923\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">apiserver/cacher.go at cf5eff4f5e8f6019796cb18c69918b9f2f09e6db · kubernetes/apiserver · GitHub</a>。当它从一个名为<code>input</code>的 channel 中监听到有资源对象的信息发送过来的时候，就会通过<code>sendWatchCacheEvent</code>函数，将最新的 event 通过刚才提到的 Result Channel 发送给客户端。并且由于客户端和 API Server 之间是一个长连接，所以这个循环会一直执行。</p><blockquote> 如果读者阅读过前一篇过于 List-Watch 机制原理性的文章就可以知道，List-Watch 机制会通过 ResourceVersion 来保证发送报文的顺序性。而这部分逻辑就是在 <code>process</code>内实现的。如果当前客户端对 API Server 的 Watch 请求带来的 ResourceVersion 为1，那么 process 函数内的逻辑保证会返回给客户端一个序号大于1的报文。  <br/> </blockquote><p>但是，截止到目前为止，我们只是看到了<code>cacheWatcher</code>为每一个请求启动一个 goroutine 不断的监听资源对象信息的变化，如果有新的消息过来就返回给客户端。那么这个变化的消息是谁向<code>input</code>channel 传递过来的呢？回头看下 cacher 的数据结构：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/kubernetes/apiserver/blob/cf5eff4f5e8f6019796cb18c69918b9f2f09e6db/pkg/storage/cacher/cacher.go%23L141\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">apiserver/cacher.go at cf5eff4f5e8f6019796cb18c69918b9f2f09e6db · kubernetes/apiserver · GitHub</a>，我们发现有如下几个成员是值得注意的：</p><div class=\"highlight\"><pre><code class=\"language-go\"><span class=\"c1\">// Underlying storage.Interface.\n</span><span class=\"c1\"></span>    <span class=\"nx\">storage</span> <span class=\"nx\">storage</span><span class=\"p\">.</span><span class=\"nx\">Interface</span>\n    <span class=\"c1\">// &#34;sliding window&#34; of recent changes of objects and the current state.\n</span><span class=\"c1\"></span>    <span class=\"nx\">watchCache</span> <span class=\"o\">*</span><span class=\"nx\">watchCache</span>\n    <span class=\"nx\">reflector</span>  <span class=\"o\">*</span><span class=\"nx\">cache</span><span class=\"p\">.</span><span class=\"nx\">Reflector</span></code></pre></div><h2>storage storage.Interface（资源对象数据真正的来源）</h2><p>storage 成员是在创建 Cacher 的时候就被传递进来了。根据这个顺序向回查找，查看<code>genericregistry.StorageWithCacher</code>的定义<a href=\"https://link.zhihu.com/?target=https%3A//github.com/kubernetes/apiserver/blob/b8915a5609e4d7553d92f0d431ba04ecf9b52777/pkg/registry/generic/registry/storage_factory.go%23L44\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">apiserver/storage_factory.go at b8915a5609e4d7553d92f0d431ba04ecf9b52777 · kubernetes/apiserver · GitHub</a>，可知 storage 是通过 <code>NewRawStorage</code>创建的。一路跟下去后。我们最后到了创建 etcdStorage 的函数里：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/kubernetes/kubernetes/blob/release-1.12/staging/src/k8s.io/apiserver/pkg/storage/etcd/etcd_helper.go%23L65\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">kubernetes/etcd_helper.go at release-1.12 · kubernetes/kubernetes · GitHub</a>。此时，你可以看下 <code>NewEtcdStorage</code>的返回值还有<code>etcdHelper</code>类实现的的 List 和 Watch 方法，就可以明白，整个 ListWatch 机制中，API Server 从 etcd 获取资源对象信息所使用的 Watch 和 List 方法就是在这里真正的被实现。而这也符合我们之前看的 Kubernetes 的架构图中的一个细节：API Server 对外提供的一切信息都是从 etcd 而来的。如果你进入 etcd 实现的 Watch 方法中，稍微扫一眼就可以看到，它核心的逻辑就是启动一个死循环，不断的等待从 etcd 而来的有关资源对象的信息。</p><div class=\"highlight\"><pre><code class=\"language-go\"><span class=\"k\">for</span> <span class=\"p\">{</span>\n        <span class=\"nx\">resp</span><span class=\"p\">,</span> <span class=\"nx\">err</span> <span class=\"o\">:=</span> <span class=\"nx\">watcher</span><span class=\"p\">.</span><span class=\"nf\">Next</span><span class=\"p\">(</span><span class=\"nx\">w</span><span class=\"p\">.</span><span class=\"nx\">ctx</span><span class=\"p\">)</span>\n        <span class=\"k\">if</span> <span class=\"nx\">err</span> <span class=\"o\">!=</span> <span class=\"kc\">nil</span> <span class=\"p\">{</span>\n            <span class=\"nx\">w</span><span class=\"p\">.</span><span class=\"nx\">etcdError</span> <span class=\"o\">&lt;-</span> <span class=\"nx\">err</span>\n            <span class=\"k\">return</span>\n        <span class=\"p\">}</span>\n        <span class=\"nx\">w</span><span class=\"p\">.</span><span class=\"nx\">etcdIncoming</span> <span class=\"o\">&lt;-</span> <span class=\"nx\">resp</span>\n    <span class=\"p\">}</span></code></pre></div><p>除此之外，<code>etcdHelper</code>类还实现了很多和 etcd 相关的方法。那么我们姑且可以认为，storage 成员是操作 etcd 的一个封装。</p><h2>watchCache *watchCache（资源对象信息的缓存）</h2><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/kubernetes/apiserver/blob/b080aefffce393d0aa75a2d3c62442b5515c8963/pkg/storage/cacher/watch_cache.go%23L147\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">apiserver/watch_cache.go at b080aefffce393d0aa75a2d3c62442b5515c8963 · kubernetes/apiserver · GitHub</a> 通过观察 watchCache 和数据结构以及它实现的方法，我猜测它应该是实现了一个资源对象信息的缓存。将通过和 etcd 通信而获取到的资源对象的信息缓存在内存中。当资源对象长时间未发生变化的时候，如果再有 List 或者 Watch 请求该资源对象的信息，可以直接返回给它缓存中的内容，而不再去和 etcd 通信。</p><h2>reflector  *cache.Reflector</h2><p>reflector 的创建一共需要两个重要的对象作为参数：</p><ol><li>listerWatcher</li><li>watchCache</li></ol><p>其中 watchCache 我们上面已经提到过，那 listerWatcher 是什么呢？在 cacher.go 文件中可以找到创建这个对象的方法的定义：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/kubernetes/apiserver/blob/cf5eff4f5e8f6019796cb18c69918b9f2f09e6db/pkg/storage/cacher/cacher.go%23L719\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">apiserver/cacher.go at cf5eff4f5e8f6019796cb18c69918b9f2f09e6db · kubernetes/apiserver · GitHub</a>。浏览一下它实现的方法集合可知，它其实就是对上面我们说到的 storage 对象的一个封装。实际上调用的还是 storage 内实现的一些方法。</p><p>通过观察 Cacher 数据结构中几个比较重要的成员的逻辑，我们现在确定了资源对象真正的数据来源，也了解了catchWatcher 启动了一个goroutine 运行死循环等待着从<code>input</code>channel 发来的资源对象的信息。目前唯一缺少的就是数据生产者是如何将数据传递到<code>input</code>这个 channel 中的。</p><p>在创建了 Cacher 对象时候，我们紧接着运行了一个名为<code>StartCaching</code> 的方法：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/kubernetes/apiserver/blob/cf5eff4f5e8f6019796cb18c69918b9f2f09e6db/pkg/storage/cacher/cacher.go%23L248\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">apiserver/cacher.go at cf5eff4f5e8f6019796cb18c69918b9f2f09e6db · kubernetes/apiserver · GitHub</a>。在它的逻辑内部，我们调用了 reflector 的 ListWatch 方法<a href=\"https://link.zhihu.com/?target=https%3A//github.com/kubernetes/client-go/blob/ee7a1ba5cdf1292b67a1fdf1fa28f90d2a7b0084/tools/cache/reflector.go%23L168\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">client-go/reflector.go at ee7a1ba5cdf1292b67a1fdf1fa28f90d2a7b0084 · kubernetes/client-go · GitHub</a>。在这个方法中，它先通过 listerWatcher 封装的 List 方法全量的获取了一下 Kubernetes 集群中的资源，然后起了一个死循环，调用了 ListerWatcher 封装的 Watch 方法，然后在 watchHandler 方法中，通过访问一个阻塞的 channel，等待资源对象信息从 etcd 发过来。如果此时确实接收到了一个资源对象的信息，它会调用 watchCache.Add 方法，将其塞入缓存中：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/kubernetes/apiserver/blob/b080aefffce393d0aa75a2d3c62442b5515c8963/pkg/storage/cacher/watch_cache.go%23L170\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">apiserver/watch_cache.go at b080aefffce393d0aa75a2d3c62442b5515c8963 · kubernetes/apiserver · GitHub</a>。在 Add 的函数的逻辑中，会继续调用 <code>processEvent</code>函数，在其内部我们发现，除了正常的更新 watchCache 的缓存之外，还执行了行逻辑：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/kubernetes/apiserver/blob/b080aefffce393d0aa75a2d3c62442b5515c8963/pkg/storage/cacher/watch_cache.go%23L257\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">apiserver/watch_cache.go at b080aefffce393d0aa75a2d3c62442b5515c8963 · kubernetes/apiserver · GitHub</a>。</p><p>那么这个 onEvent 函数究竟是什么呢？返回 watchCache 被创建的逻辑的位置会发现，watchCache.onEvent 是被cacher.processEvent 赋值的。cacher.processEvent 函数的定义如下：</p><div class=\"highlight\"><pre><code class=\"language-go\"><span class=\"kd\">func</span> <span class=\"p\">(</span><span class=\"nx\">c</span> <span class=\"o\">*</span><span class=\"nx\">Cacher</span><span class=\"p\">)</span> <span class=\"nf\">processEvent</span><span class=\"p\">(</span><span class=\"nx\">event</span> <span class=\"o\">*</span><span class=\"nx\">watchCacheEvent</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"k\">if</span> <span class=\"nx\">curLen</span> <span class=\"o\">:=</span> <span class=\"nb\">int64</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"nx\">c</span><span class=\"p\">.</span><span class=\"nx\">incoming</span><span class=\"p\">));</span> <span class=\"nx\">c</span><span class=\"p\">.</span><span class=\"nx\">incomingHWM</span><span class=\"p\">.</span><span class=\"nf\">Update</span><span class=\"p\">(</span><span class=\"nx\">curLen</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n        <span class=\"c1\">// Monitor if this gets backed up, and how much.\n</span><span class=\"c1\"></span>        <span class=\"nx\">glog</span><span class=\"p\">.</span><span class=\"nf\">V</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">).</span><span class=\"nf\">Infof</span><span class=\"p\">(</span><span class=\"s\">&#34;cacher (%v): %v objects queued in incoming channel.&#34;</span><span class=\"p\">,</span> <span class=\"nx\">c</span><span class=\"p\">.</span><span class=\"nx\">objectType</span><span class=\"p\">.</span><span class=\"nf\">String</span><span class=\"p\">(),</span> <span class=\"nx\">curLen</span><span class=\"p\">)</span>\n    <span class=\"p\">}</span>\n    <span class=\"nx\">c</span><span class=\"p\">.</span><span class=\"nx\">incoming</span> <span class=\"o\">&lt;-</span> <span class=\"o\">*</span><span class=\"nx\">event</span>\n<span class=\"p\">}</span></code></pre></div><p>再结合对比一下 watchCache.Add 函数的逻辑可知，从 etcd 发来的 event 将会通过 processEvent 函数传递至<code>incoming</code> 这个 channel。而 <code>incoming</code> 这个 channel 是在 cacher 对象的dispatchEvents 函数内被读取的：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/kubernetes/apiserver/blob/cf5eff4f5e8f6019796cb18c69918b9f2f09e6db/pkg/storage/cacher/cacher.go%23L603\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">apiserver/cacher.go at cf5eff4f5e8f6019796cb18c69918b9f2f09e6db · kubernetes/apiserver · GitHub</a>。跟着这个数据流动的逻辑继续向下，我们发现这个 event 最终作为参数传递到了之前我们创建的<code>watcher</code>的 Add 方法内：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/kubernetes/apiserver/blob/cf5eff4f5e8f6019796cb18c69918b9f2f09e6db/pkg/storage/cacher/cacher.go%23L624\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">apiserver/cacher.go at cf5eff4f5e8f6019796cb18c69918b9f2f09e6db · kubernetes/apiserver · GitHub</a>。进入这个 Add 函数的内部，你会发现一下子豁然开朗，因为这个 event 事件经过漫长的流程终于传递到了<code>input</code>channel: <a href=\"https://link.zhihu.com/?target=https%3A//github.com/kubernetes/apiserver/blob/cf5eff4f5e8f6019796cb18c69918b9f2f09e6db/pkg/storage/cacher/cacher.go%23L854\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">apiserver/cacher.go at cf5eff4f5e8f6019796cb18c69918b9f2f09e6db · kubernetes/apiserver · GitHub</a>。而上面这些 watcher 是怎么被收集进来的呢？通过 cacher 中 Watch 方法的逻辑可以发现，它来自于对 Watch 方法的调用：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/kubernetes/apiserver/blob/cf5eff4f5e8f6019796cb18c69918b9f2f09e6db/pkg/storage/cacher/cacher.go%23L343\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">apiserver/cacher.go at cf5eff4f5e8f6019796cb18c69918b9f2f09e6db · kubernetes/apiserver · GitHub</a>。而这个方法，正是在我们前面说到的，API Server 在响应 Watch/List 相关的 HTTP 请求的时候，生成类型为<code>watch.Interface</code>且名为<code>catcherWatcher</code>对象时调用的。</p><p>至此，整个<code>List-Watch</code>机制中，资源对象的数据从 etcd 到 API Server HTTP API 之间的数据流动过程就都走通了。</p>", 
            "topic": [
                {
                    "tag": "Kubernetes", 
                    "tagLink": "https://api.zhihu.com/topics/20018384"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50363516", 
            "userName": "要没时间了", 
            "userLink": "https://www.zhihu.com/people/bdbe7f88af9a021c5c770649d266f143", 
            "upvote": 5, 
            "title": "Detect the mechanism of list-watch in Kubernetes", 
            "content": "<blockquote>更好的阅读体验请移步至我的 blog： <a href=\"https://link.zhihu.com/?target=http%3A//littledriver.net/post/2018/11/19/detect-the-mechanism-of-list-watch-in-kubernetes/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Detect the mechanism of list-watch in Kubernetes</a></blockquote><h2>先来看一个小例子</h2><p>一个 Kubernetes 的集群的架构通常如下图所示：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-cc706ed7459aebaa597c333b8669fc15_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1858\" data-rawheight=\"1126\" class=\"origin_image zh-lightbox-thumb\" width=\"1858\" data-original=\"https://pic2.zhimg.com/v2-cc706ed7459aebaa597c333b8669fc15_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1858&#39; height=&#39;1126&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1858\" data-rawheight=\"1126\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1858\" data-original=\"https://pic2.zhimg.com/v2-cc706ed7459aebaa597c333b8669fc15_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-cc706ed7459aebaa597c333b8669fc15_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>当一个用户通过<code>kubectl</code>命令行想要创建一个 Pod 数量为3的 Deployment 的资源对象的时候。此请求会先通过 HTTP 协议发送到 API Server。随后，API Server 会将此资源对象的相关信息持久化至 etcd 中。紧接着，ControllerManager 中负责管理  Deployment 的 Controller 将会通过<code>List-Watch</code> 机制感知到目前有一个 Deployment 的资源需要被创建，它会通过一段逻辑来处理这个「新增」的资源：创建一个 ReplicaSet。ReplicaSet 资源的创建请求在 Kubernetes 中被处理的方式和 Deployment 是一样的。最终，在 ReplicaSet Controller 中将会创建符合用户预期数量的 Pod。而 Pod 被创建的行为将会被 Scheduler 通过<code>List-Watch</code> 感知到。在通过一定的调度策略选取好 Pod 要调度的 Node 之后，Kubelet 也将通过 <code>ListWatch</code>机制感知到这一结果。最终，Kubelet 将会在相应的 Node 上为 Pod 创建符合预期数量的容器，并且把 Pod 最新的状态通过 API Server 写回至 etcd。</p><p>在了解了一个 Deployment 资源被创建的过程之后，再对比上面 k8s 的架构图。我们可以知道，粉色线标识的，基本就是<code>List-Watch</code> 机制传递消息的过程。</p><h2>对上述例子的思考</h2><ol><li>细心观察一下就会发现，整个 Kubernetes 集群中各个组件执行一些逻辑的依据都来源于<code>List-Watch</code>机制传递过来的资源对象的信息。而这些信息基本上都存放在 etcd 中，依靠 API Server 所暴露的 HTTP 接口对外提供。</li><li>整个 Kubernetes 集群中的组件除了 etcd 之外均为无状态服务。这就预示着除了 etcd 之外，其他服务如果因为故障或者升级等出现短暂断开是完全不影响整个集群的正常工作的。</li><li>Kubernetes 的设计理念决定了它的各种行为依赖的是具体资源对象的状态信息，而不是某个用户传递至 API Server 的命令。</li><li>Kubernetes 作为一个由多个组件联合构成的一个分布式系统，对一致性的要求是「最终一致」。因为和资源对象相关的信息在不断被获取的同时也在不断被修改。整个过程中可能要有一些非预期的行为，但是只要最终某个操作的结果和正确的，对于 Kubernetes 来说就是可以接受的。</li></ol><h2>Kubernetes 的架构以及核心组件的职责</h2><p><a href=\"https://link.zhihu.com/?target=https%3A//jimmysong.io/kubernetes-handbook/images/architecture.png\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">核心组件架构图</a></p><ul><li>etcd：集群中唯一有状态的服务，负责存储资源对象的信息</li><li>API Server：集群资源的访问入口。任何对资源对象的操作（内置，自定义）都需要通过它暴露出来的HTTP 接口</li><li>Scheduler：集群的调度器。通过一定的调度策略，为 Pod 选取符合条件的调度机器并将其调度到相应的机器上</li><li>Controller Manager： 集群资源控制器的集合。其内部包含了多个处理不同类型资源的 Controller。它们负责通过一定的逻辑将各个资源对象维护至用户期望的状态（desire state）。</li><li>Kubelet：集群中各节点的守护进程。负责管理其 Node 上对应的 Container 的生命周期</li><li>Container Runtime：集群底层依赖的容器运行时。负责管理容器镜像以及容器的运行</li><li>Kube-proxy：  负责管理集群内部的服务发现和负载均衡，给各个服务提供网络通信层面的支持</li></ul><h2>什么是 List-Watch？</h2><p>List-Watch 准确的说是 Kubernetes 集群内部的一种「异步消息传递机制」。任何集群内部资源对象的状态变化，都将通过这种机制传递给关心它的组件。如上面描述的关于创建 Deployment 资源对象的过程，scheduler，kubelet，controller 等组件都会通过 List-Watch 机制来监听整个创建过程中的变化。</p><h2>一个好的 List-Watch 机制需要解决哪些问题？</h2><p>一个好的「异步消息传递机制」，应该保证如下几个关键的性质：</p><ol><li>实时性：尤其是对于 Kubernetes 来说，集群中的各项操作都依赖于「资源对象的消息」。所以消息传递的实时性就显得尤为重要</li><li>顺序性：同一个资源对象的操作，很多时候都是互斥的且要遵守一定的顺序执行的。如 Pod 的删除和创建操作，明显不能先响应删除，再响应创建</li><li>可靠性：一旦某个组件在接收到「消息 」之后发生故障进行重启，那么这个消息需要按照一定的机制重传</li></ol><h2>如何保证高实时性？</h2><p>既然信息是通过 API Server 向外输出的，那么想更加实时的获取这部分信息就得减少除了真正通讯之外的操作的消耗。对于网络通信而言，我们很容易就想到一个办法：尽量减少链接的创建和销毁操作，保持一个长连接，一直等待着服务端数据。所以，在 Kubernetes 1.7 版本之后，使用 HTTP Streaming 协议与 API Server 建立链接，不断等待从 API Server 到来的数据。</p><h2>如何保证顺序性？</h2><p>在分布式系统中，若想保证某一个操作或者资源的顺序性，最常见的办法就是加入「版本号」机制。即给那些需要保证顺序的资源添加一个 Version 字段。Version 从小到大依次递增，每当这个资源的信息被更新时，它的 Version 也就相应的+1。通常，一个系统中对于同一个资源可能有多个不同的版本。当目的组件想要 Watch 一个资源对象的信息的时候，可以带上之前本地缓存的资源对象的版本号。API Server 会按照递增的顺序进行传递。默认这个版本号从0开始递增。</p><p>在 Kubernetes 中，这个 Version 字段加在了资源对象的 meta 信息部分：</p><div class=\"highlight\"><pre><code class=\"language-text\">// An opaque value that represents the internal version of this object that can\n    // be used by clients to determine when objects have changed. May be used for optimistic\n    // concurrency, change detection, and the watch operation on a resource or set of resources.\n    // Clients must treat these values as opaque and passed unmodified back to the server.\n    // They may only be valid for a particular resource or set of resources.\n    //\n    // Populated by the system.\n    // Read-only.\n    // Value must be treated as opaque by clients and .\n    // More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#concurrency-control-and-consistency\n    // +optional\n    ResourceVersion string `json:&#34;resourceVersion,omitempty&#34; protobuf:&#34;bytes,6,opt,name=resourceVersion&#34;`</code></pre></div><p>这种「版本号机制」的正常运行，依赖于更新版本号的操作必须是原子的。而这部分操作应该由 etcd 来保证。</p><h2>关于顺序性的一点小思考</h2><p>通过对「实时性」的了解，我们知道，在目的组件和 api server 之间是有一条长链接用于传输资源对象信息的。虽然我们通过 ResourceVersion 可以保证  API Server 向目的组件发送的消息是按照递增顺序的。但其实对于目的组件而言，我们貌似是无法保证消息的到达顺序的。并且，在我观察了 client-go 中关于`List-Watch` 机制的代码逻辑之后，我发现 client-go 对于向缓存中更新这个最新的 ResourceVersion 之前并没有将新旧 Version 进行对比。那么，这里就有一个隐含的风险：长链接中有 1，2两个资源按照递增的顺序发出。但是当到达目的组件的时候，2先到达，1后到达。当这种情况发生的时候，ResourceVersion 是不是就有可能被错误的更新呢？</p><p>逆向的想一下这个问题， k8s 的实现者肯定是考虑到了并且解决了这个问题。那我们也来思考一下：</p><p>目前看来，API Server 只能保证发送消息的顺序，而目的组件在处理消息的时候又没有对新旧 ResourceVersion 做对比。那么处理上述所说风险的唯一可行的位置就是中间的通信链路。由于 HTTP Streaming 在传输层依赖的是 TCP 协议。而 TCP 协议本身就保证的报文传递的顺序性：发送方会按照报文的顺序等待相应的 ACK 报文，否则会启动超时重传机制。而接收方也会将接收到的报文先存于缓冲区中，按顺序进行消费。如果发现接下来要消费的报文没有按顺序到达，可能会启用快重传机制，不断的给发送方发送上一个顺序到达报文的 ACK 报文。所以，我们之前假想的问题是不存在的。</p><h2>如何保证可靠性？</h2><p>Kubernetes 不但提供了持续 Watch 资源对象信息这种增量获取的机制，还提供了一种 List 操作，即一次性获取集群中所有的符合条件的资源对象信息。这样一来，当目的组件第一次启动或者重启，甚至是 Watch 操作出错的时候，可以通过 List 操作统一的获取集群中相关的资源对象，刷新掉可能已经被污染的数据。</p><h2>现有的 List-Watch 机制有哪些弊端？</h2><p>通过对 List-Watch 机制实现「消息传递的可靠性」的思路可知，当发生异常情况或客户端重启的时候，会调用 List 接口对 Kubernetes 集群内部所有的资源对象都扫描一次。这个操作是比较昂贵的，尤其是当集群内资源对象的数量达到一定量级的时候。所以，我们应当在日常的开发中尽量减少触发 List 接口。</p><h2>关于 List-Watch 机制中长连接的一点思考</h2><p>在 HTTP 1.1 版本中，每一个长连接都会占用一个单独的 TCP 链接。所以，当这个长连接断掉的时候，客户端和服务端是感知不到的。仅仅只是发现长时间没有数据。如果此时 TCP 打开了 keep-alive 功能，那么客户端或者服务端会间隔一定的时间向对方发送 keep-alive 报文来检测链接是否正常。如果对方多次都没有返回一个 ACK 报文的话，发送端会主动断开这个链接，并且重新建立一个新的链接。</p><p>在 HTTP2.0版本中，多个 HTTP 链接会共用一个 TCP 链接。这个链接上可能无时不刻都有数据在传递。此时，keep-alive 功能就不起作用了。但是，在连接异常的情况下，发送端（可能是客户端也可能是服务端）发送的报文可能一直得不到接收端的 ACK 报文。这就会持续引发发送端的「超时重传」机制。由于链接断开，即使发生重传也依然会失败。最终，发送端会因为达到最大的超时重传次数而重置掉当前的 TCP 链接。</p><p>List-Watch 机制为了减少链接创建和销毁的开销，使用了 HTTP Streaming 协议。客户端是不能确定一个准确的关闭连接的时间点的，因为它也不知道什么时候服务端会有数据过来。所以，关闭连接的任务一般都由服务端来发起。</p>", 
            "topic": [
                {
                    "tag": "Kubernetes", 
                    "tagLink": "https://api.zhihu.com/topics/20018384"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50189681", 
            "userName": "要没时间了", 
            "userLink": "https://www.zhihu.com/people/bdbe7f88af9a021c5c770649d266f143", 
            "upvote": 2, 
            "title": "Docker 依赖的 Linux 基础技术总结", 
            "content": "<p>下面的脑图大致梳理了 Docker 使用的 Linux 的一些基础技术以及它们所解决的问题。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-7ab0dc0fb8880845daf9c892ed0ad703_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1944\" data-rawheight=\"2290\" class=\"origin_image zh-lightbox-thumb\" width=\"1944\" data-original=\"https://pic4.zhimg.com/v2-7ab0dc0fb8880845daf9c892ed0ad703_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1944&#39; height=&#39;2290&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1944\" data-rawheight=\"2290\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1944\" data-original=\"https://pic4.zhimg.com/v2-7ab0dc0fb8880845daf9c892ed0ad703_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-7ab0dc0fb8880845daf9c892ed0ad703_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "Docker", 
                    "tagLink": "https://api.zhihu.com/topics/19950993"
                }, 
                {
                    "tag": "Linux", 
                    "tagLink": "https://api.zhihu.com/topics/19554300"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50101602", 
            "userName": "要没时间了", 
            "userLink": "https://www.zhihu.com/people/bdbe7f88af9a021c5c770649d266f143", 
            "upvote": 2, 
            "title": "Detect Redis Config File", 
            "content": "<p></p><h2>Include Part</h2><h2>Include</h2><p><code>include</code> 可以允许用户在 Redis 的 Conf 文件中引用一份「已经准备好」的配置。一般来说，我们都会把一些通用的且很少变化的配置放在一个「配置模板」中，然后在真正的 Redis-Server 启动的配置文件中使用<code>include</code> 命令包含它。这样一来，Redis-Server 在启动的时候就会使用配置模板内的配置项。</p><p>你也可以在<code>include</code> 命令后按照正常的方法写入其他配置。若新添加的配置项不在「配置模板」内容之中，那么这就相当于做了一个「追加」操作，否则，那将是一个「覆盖」操作。更为重要的是，<code>include</code> 的内容不会被<code>CONFIG REWRITE</code>命令的执行结果覆盖。</p><div class=\"highlight\"><pre><code class=\"language-yaml\"><span class=\"c\"># Example</span><span class=\"w\">\n</span><span class=\"w\"></span><span class=\"c\"># include /path/to/local.conf</span><span class=\"w\">\n</span><span class=\"w\"></span><span class=\"c\"># include /path/to/other.conf</span></code></pre></div><h2>Load Part</h2><h2>loadmodule</h2><p><code>loadmodule</code> 可以允许用户在 Redis 的 Conf 文件中指定在 Redis-Server 启动时要加载的「外部模块」。因为 Redis 本身也是通过 C 语言进行实现的，所以它在<code>redismodule.h</code>文件中提供了一些 C API， 可供用户在使用 C 语言实现「外部模块」的时候使用。</p><div class=\"highlight\"><pre><code class=\"language-yaml\"><span class=\"c\"># Example</span><span class=\"w\">\n</span><span class=\"w\"></span><span class=\"c\"># loadmodule /path/to/my_module.so</span><span class=\"w\">\n</span><span class=\"w\"></span><span class=\"c\"># loadmodule /path/to/other_module.so</span></code></pre></div><h2>Network Part</h2><h2>bind</h2><p><code>bind</code>配置项，允许用户指定一个或多个特定网卡的地址，并且将 Redis-Server 绑定在这个地址上。设置好了之后，若想通过 Redis-Cli 等工具访问 Redis-Server 的时候，<code>host</code> 参数必须指定<code>bind</code> 所绑定的地址。否则无法访问 Redis-Server。</p><p>默认情况下，Redis 将此配置项的缺省值指定为：<code>127.0.0.1</code>。这代表仅有和 Redis-Server 处于同一机器上的客户端可以访问。如果不指定<code>bind</code>配置项，那么 Redis-Server 将接受来自机器上所有网卡的链接。</p><p>这里一定需要注意的是，<code>bind</code> 参数是给 Redis-Server 绑定一个本地机器网卡的地址，而不是一个外部访问的 IP 地址。</p><div class=\"highlight\"><pre><code class=\"language-yaml\"><span class=\"c\"># Example</span><span class=\"w\">\n</span><span class=\"w\"></span><span class=\"c\"># bind 192.168.1.100 10.0.0.1</span><span class=\"w\">\n</span><span class=\"w\"></span><span class=\"c\"># bind 127.0.0.1 ::1</span></code></pre></div><h2>protected-mode</h2><p><code>protected-mode</code>是 Redis 除了访问密码之外为增加其安全性所给出的一个配置参数。该配置项默认开启，仅接受和 Redis-Server 处于同一机器上的客户端的链接。并且可以支持无密码访问。否则，会禁用掉这项保护措施。</p><div class=\"highlight\"><pre><code class=\"language-yaml\"><span class=\"c\"># Example</span><span class=\"w\">\n</span><span class=\"w\"></span>protected-mode<span class=\"w\"> </span>yes</code></pre></div><h2>port</h2><p><code>port</code> 配置允许用户指定 Redis-Server 运行所监听的端口，默认情况下为 6379。</p><div class=\"highlight\"><pre><code class=\"language-yaml\"><span class=\"c\"># Example</span><span class=\"w\">\n</span><span class=\"w\"></span>port<span class=\"w\"> </span><span class=\"m\">6379</span></code></pre></div><h2>tcp-backlog</h2><p><code>tcp-backlog</code>配置项是和 TCP 建立链接的过程有关的。对于使用者来说，可以简单的理解为，在完成了 TCP 的三次握手之后，客户端和 Redis—Server 建立链接的请求被放入了一个 Accept Queue 中，相当于进程的就绪队列，等待被 Redis-Server 处理。这个队列容量的大小就是受<code>tcp-backlog</code>配置项控制的。</p><p>理论上来说，如果使用 Redis 的服务有「高吞吐量」的特性，那么就需要在配置项中调大这个值。但是，这个值同时受到操作系统的相关参数的限制。最终 backlog 的值将取 <code>tcp-backlog</code> 和  /proc/sys/net/core/somaxconn 两个值中较小的那一个。默认情况下，这个值的参数为 511。</p><div class=\"highlight\"><pre><code class=\"language-yaml\"><span class=\"c\"># Example</span><span class=\"w\">\n</span><span class=\"w\"></span>tcp-backlog<span class=\"w\"> </span><span class=\"m\">511</span></code></pre></div><h2>tcp-keepalive</h2><p><code>tcp-keepalive</code> 是和 TCP 链接保活机制有关的一个参数。若该配置项的值大于0，则 Redis-Server 将每隔 <code>tcp-keepalive</code>秒向客户端发送一个保活探测报文，根据回应判断客户端当前是否正常。如果超过一定次数，发现对方仍然没有回应，则 Redis-Server 会主动断开他们之间的链接。若该配置项被设置为0，则 Redis—Server 不会再进行这个检测。否则将每隔 <code>tcp-keepalive</code>秒发送一次保活探测报文。</p><div class=\"highlight\"><pre><code class=\"language-yaml\"><span class=\"c\"># Example</span><span class=\"w\">\n</span><span class=\"w\"></span>tcp-keepalive<span class=\"w\"> </span><span class=\"m\">500</span></code></pre></div><h2>unixsocket &amp;&amp; unixsocketperm</h2><p><code>unixsocket</code> 和 <code>unixsocketperm</code> 是 Unix socket 的配置项。如果用户想通过 Unix socket 的方式来链接 Redis-Server 的话，需要开启这两个配置，使得 Redis-Server 可以监听从 Unix socket 进来的链接。默认情况下这两个配置是不开启的。</p><div class=\"highlight\"><pre><code class=\"language-yaml\"><span class=\"c\"># Example</span><span class=\"w\">\n</span><span class=\"w\"></span><span class=\"c\"># unixsocket /tmp/redis.sock</span><span class=\"w\">\n</span><span class=\"w\"></span><span class=\"c\"># unixsocketperm 700</span></code></pre></div><h2>timeout</h2><p><code>timeout</code> 配置项可以让用户自由指定一个客户端到 Redis-Server 的链接在空闲多久之后断开，单位是秒。当配置为 0 的时候，意味着链接永远不会断开，直到通信双方有一方进程停掉。</p><div class=\"highlight\"><pre><code class=\"language-yaml\"><span class=\"c\"># Example</span><span class=\"w\">\n</span><span class=\"w\"></span>timeout<span class=\"w\"> </span><span class=\"m\">0</span></code></pre></div><h2>General Part</h2><h2>daemonize</h2><p><code>daemonize</code>配置项可以由用户来决定，是否以「守护进程」的方式来运行 Redis-Server。默认情况下这个配置项是禁用的。</p><div class=\"highlight\"><pre><code class=\"language-yaml\"><span class=\"c\"># Example</span><span class=\"w\">\n</span><span class=\"w\"></span>daemonize<span class=\"w\"> </span>no</code></pre></div><h2>pidfile</h2><p><code>pidfile</code> 配置项允许用户指定一个文件的绝对路径。该文件的内容为 Redis-Server 进程的 PID。若已经开启了 <code>daemonize</code>配置且<code>pidfile</code>指定了一个路径，那么默认情况下会将  Redis-Server 的 PID 写入到<code>pidfile</code>指定的文件中。否则，写入到<code>/var/run/redis.pid</code>中。</p><div class=\"highlight\"><pre><code class=\"language-yaml\"><span class=\"c\"># Example</span><span class=\"w\">\n</span><span class=\"w\"></span>pidfile<span class=\"w\"> </span>/var/run/redis_6379.pid</code></pre></div><h2>loglevel &amp;&amp; logfile</h2><p><code>loglevel</code> 配置项允许用户指定 Redis-Server 日志输出的等级。等级越高，输出的信息也就越丰富。该配置项的候选值有如下几个：</p><ul><li>warning： 仅一些必要的错误信息和重要的信息</li><li>notice: 除一些必要的错误信息和重要的信息之外，有一部分的冗余信息。推荐在生产环境中开启</li><li>verbose：信息更加丰富，揭示 Redis-Server 的运行过程</li><li>debug：Redis-Server 能够输出的所有的信息</li></ul><p><code>logfile</code>允许用户指定 Redis-Server 服务日志的存放路径。默认情况下如果不指定这个参数，Redis—Server 会将日志输出到标准输入。而且，如果是以守护进程的模式运行的话，日志会被打入「黑洞」：<code>/dev/null</code></p><div class=\"highlight\"><pre><code class=\"language-yaml\"><span class=\"c\"># Example</span><span class=\"w\">\n</span><span class=\"w\"></span>loglevel<span class=\"w\"> </span>notice<span class=\"w\">\n</span><span class=\"w\"></span>logfile<span class=\"w\"> </span><span class=\"s2\">&#34;/var/redis/log&#34;</span></code></pre></div><h2>syslog-enabled &amp;&amp; syslog-ident</h2><p><code>syslog-enabled</code> 配置项开启之后，会将 Redis-Server 的服务日志输出到操作系统日志所在的文件中。也就是<code>/var/log/syslog</code>。默认情况下这个配置是关闭的。</p><p><code>syslog-ident</code> 配置项主要是用来给输出到系统日志的 Redis-Server 的服务日志内容添加一些特殊的标记，方便通过一些工具进行筛选。</p><div class=\"highlight\"><pre><code class=\"language-yaml\"><span class=\"c\"># Example</span><span class=\"w\">\n</span><span class=\"w\"></span>syslog-enabled<span class=\"w\"> </span>yes<span class=\"w\">\n</span><span class=\"w\"></span>syslog-ident<span class=\"w\"> </span>native-redis</code></pre></div><h2>databases</h2><p>在一个 Redis 的实例中，通常会将它申请的内存按照 <code>databases</code>配置项的值切分成不同的部分 (即「分组」操作)。每一个子 database 通过整数进行索引，编号从 0~ databases。在对 Redis 操作之前，可以先通过 select 命令选取其中一个子数据库进行读取和写入。若你在两个不同的子数据库中写入相同的 key 但是不同的值之后就会发现，两者是不发生冲突的。默认情况下 <code>databases</code>配置项为16。</p><div class=\"highlight\"><pre><code class=\"language-yaml\"><span class=\"c\"># Example</span><span class=\"w\">\n</span><span class=\"w\"></span>databases<span class=\"w\"> </span><span class=\"m\">16</span></code></pre></div><p>Redis 之所以提供了这样的功能，最主要的原因是，很多重要的功能无法在多个 Redis 实例上一起配合来实现。如涉及到多个实例上的 Key 的事务功能是不可以用的。Redis 提供的分区功能，是在同一个进程的内存空间中实现了内存使用的隔离。至于每一个 db 被分配多少内存，一个 db 写满了之后会发生什么，是和 Redis 的内存分配策略有关的，这里暂时不详细介绍了。</p><h2>RDB Backup</h2><h2>save</h2><p>RDB 是 Redis 提供的一种「快照」形式的备份策略。它不能够保证高实时性，但是可以对 Redis 数据的「容灾性」提供一个比较好的帮助。</p><p>用户可以通过 <code>save</code>配置来设定 RDB 备份的策略，如 <code>save 900 1</code>，意味着当900秒的时间内，如果有1个 key 的内容发生了改变，那么在900秒的时间结束后，就会进行一次 RDB 的备份操作。当然，如果你想禁用掉这个 RDB 的备份功能，可以在配置文件中不指定 <code>save</code>配置项。</p><div class=\"highlight\"><pre><code class=\"language-yaml\"><span class=\"c\"># Example</span><span class=\"w\">\n</span><span class=\"w\"></span>save<span class=\"w\"> </span><span class=\"m\">900</span><span class=\"w\"> </span><span class=\"m\">1</span><span class=\"w\">  </span><span class=\"c\"># 900s 内至少有一个 Key 的内容发生修改就备份</span><span class=\"w\">\n</span><span class=\"w\"></span>save<span class=\"w\"> </span><span class=\"m\">1000</span><span class=\"w\"> </span><span class=\"m\">2</span><span class=\"w\"> </span><span class=\"c\"># 1000s 内至少有两个 Key 的内容发生修改就备份</span></code></pre></div><h2>stop-writes-on-bgsave-error</h2><p>默认开启 RDB 备份功能的情况下，备份操作可能会出现失败的情况。此时，如果用户和运维都没有察觉的话，最终的结果可能是灾难性的。<code>stop-writes-on-bgsave-error</code>配置项允许用户借助 Redis 自身提供的一个功能：RDB 备份失败后，写入操作会报错。直到下一次 RDB 备份开始，才允许客户端进行写入。</p><p>不过，Redis 自身提供的这个「防范机制」有点太暴力的。如果想自己去感知这个问题并且做出一些处理，可以对 Redis 的一些指标进行监控，然后将这个配置关掉。因为它默认是开启的。</p><div class=\"highlight\"><pre><code class=\"language-yaml\">stop-writes-on-bgsave-error<span class=\"w\"> </span>yes</code></pre></div><h2>rdbcompression</h2><p><code>rdbcompression</code> 配置项开启之后，Redis 在进行 RDB 备份的时候，会对 String 类型的内容按照一定的压缩算法进行压缩，以节省空间。此配置项默认开启。</p><div class=\"highlight\"><pre><code class=\"language-yaml\"><span class=\"c\"># Exapmle </span><span class=\"w\">\n</span><span class=\"w\"></span>rdbcompression<span class=\"w\"> </span>yes</code></pre></div><h2>rdbchecksum</h2><p>默认情况下，RDB 备份操作之后，会在备份文件的末尾插入一些校验位，以保证数据的完整性。若<code>rdbchecksum</code> 配置项开启，则在 Redis 导入备份数据的时候，会消耗一定的性能对备份数据进行校验，防止导入脏数据甚至是导入数据失败。此配置项默认开启。</p><div class=\"highlight\"><pre><code class=\"language-yaml\"><span class=\"c\"># Exapmle </span><span class=\"w\">\n</span><span class=\"w\"></span>rdbchecksum<span class=\"w\"> </span>yes</code></pre></div><h2>dbfilename</h2><p><code>dbfilename</code>可以指定 Redis 创建的 RDB 备份文件的名称。</p><div class=\"highlight\"><pre><code class=\"language-yaml\"><span class=\"c\"># Example</span><span class=\"w\">\n</span><span class=\"w\"></span>dbfilename<span class=\"w\"> </span>dump.rdb</code></pre></div><h2>Dir</h2><p><code>dir</code> 配置项可以由用户指定 RDB 备份文件的存放路径。</p><div class=\"highlight\"><pre><code class=\"language-yaml\"><span class=\"c\"># Example </span><span class=\"w\">\n</span><span class=\"w\"></span>dir<span class=\"w\"> </span>./</code></pre></div><h2>AOF Backup</h2><h2>appendonly</h2><p>AOF 是 Redis 为了保证数据可靠性提供的第二种数据持久化方案。它记录的是 Redis 的写入操作而非数据。AOF 的实时性更强，因为 RDB 快照的属性，总是有相当大的风险丢失两次 RDB 备份之间的数据的。<code>appendonly</code>配置项可以开启和关闭 AOF 的功能。该配置项默认开启。</p><div class=\"highlight\"><pre><code class=\"language-yaml\"><span class=\"c\"># Example </span><span class=\"w\">\n</span><span class=\"w\"></span>appendonly<span class=\"w\"> </span>yes</code></pre></div><h2>appendfilename</h2><p><code>appendfilename</code>可以指定 Redis 创建的 aof 备份文件的名称。</p><div class=\"highlight\"><pre><code class=\"language-yaml\"><span class=\"c\"># Example </span><span class=\"w\">\n</span><span class=\"w\"></span>appendfilename<span class=\"w\"> </span>a.aof</code></pre></div><h2>appendfsync</h2><p>Redis 在开启了 AOF 之后会将操作记录写在内存当中。但是这仍然是不安全的，因为进程 down 掉或者机器掉电，这些数据都没了。而操作系统自动将内存数据 flush 到硬盘的频率是比较长的，不一定能够满足用户对于 AOF 实时性的需求。所以提供了一个名为 <code>appendfsync</code> 的配置来调整 flush 写入记录到硬盘的频率。默认情况下是每秒保存一次。</p><ul><li>appendfsync always： 每一个操作都会 flush 到硬盘</li><li>appendfsync everysec：每秒做一次 flush 操作</li><li>appendfsync no：不主动做 flush，依靠操作系统的机制</li></ul><div class=\"highlight\"><pre><code class=\"language-yaml\"><span class=\"c\"># Example </span><span class=\"w\">\n</span><span class=\"w\"></span>appendfsync<span class=\"w\"> </span>everysec</code></pre></div><h2>no-appendfsync-on-rewrite</h2><p>AOF 文件重写，RDB 快照备份，都是需要进行磁盘I/O 的操作。如果再加上 AOF 以每秒或者每个操作的写入，势必会发生一些 I/O 阻塞的问题。Redis 在这个问题上，采取了「规避」的方式来处理。当 <code>no-appendfsync-on-rewrite</code>配置项开启之后，如果统一时间段里有其他的磁盘操作在进行，AOF 的 flush 操作可能会延迟一会执行。当然，提升性能的同时带来的问题就是用户需要忍受 AOF 的实时性会有降低。该配置默认关闭。</p><div class=\"highlight\"><pre><code class=\"language-yaml\"><span class=\"c\"># Example </span><span class=\"w\">\n</span><span class=\"w\"></span>no-appendfsync-on-rewrite<span class=\"w\"> </span>no</code></pre></div><h2>auto-aof-rewrite-percentage &amp;&amp; auto-aof-rewrite-min-size</h2><p>AOF 文件随着对 Redis 操作的增多，其文件大小也会上升。但是 Redis 中有很多写操作是可以进行合并的，比如你对一个 Key，先 set 一个 A，又 set 了一个 B。最终 B 是有效的，此时就可以把两个 Set 操作合二为一。Redis 使用 <code>auto-aof-rewrite-percentage</code> 和 <code>auto-aof-rewrite-min-size</code> 两个配置为重写操作提供了一些定制化的功能：</p><ul><li>auto-aof-rewrite-percentage： 在第一次  AOF 文件重写之后，会记录一个 base size，Redis 会定义将目前 AOF 文件的大小和这个 base size 比较。如果current size 已经达到了base size 的某个比例的大小，就会进行重写操作。base size 在每一次 Rewrite 操作之后都会更新。该配置项取值 0~100.</li><li>auto-aof-rewrite-min-size：为了防止 AOF 文件在重写的过程中变的越来越小，可以指定一个该文件最小的大小。如果 current size 还没有达到它所设置的阈值，则不会发生AOF 文件重写。</li></ul><div class=\"highlight\"><pre><code class=\"language-yaml\"><span class=\"c\"># Example </span><span class=\"w\">\n</span><span class=\"w\"></span>auto-aof-rewrite-percentage<span class=\"w\"> </span><span class=\"m\">100</span><span class=\"w\">\n</span><span class=\"w\"></span>auto-aof-rewrite-min-size<span class=\"w\"> </span>64mb</code></pre></div><h2>aof-load-truncated</h2><p>Redis 在重新启动的时候，会通过加载 AOF 和 RDB 文件两种方式来恢复内存中的数据。其中 AOF 文件会优先使用。如果在 Load AOF 文件中所写入的数据时候，发现在 AOF 文件的尾部有内容上的损坏，则用户可以通过指定 <code>aof-load-truncated</code>配置项来指示 Redis 到底是应该继续恢复并且尽可能多的恢复数据，还是立刻报错。默认情况下，Redis 会尽可能多的读入数据，<code>aof-load-truncated</code>的值为 Yes。</p><p>但是，这里要注意的是，被损坏的部分仅限于 AOF 文件的尾端。若中间部分有损坏，Redis 会无视这个配置项，直接报错。</p><div class=\"highlight\"><pre><code class=\"language-yaml\"><span class=\"c\"># Example </span><span class=\"w\">\n</span><span class=\"w\"></span>aof-load-truncated<span class=\"w\"> </span>yes</code></pre></div><h2>Replica</h2><h2>masterauth</h2><p>如果 Master 已经配置了访问密码，那么需要在从节点的配置中加入<code>masterauth</code>配置项，并将访问密码作为该配置项的值。因为在从节点要和主节点建立数据同步链接的时候，需要通过访问密码来鉴权。</p><div class=\"highlight\"><pre><code class=\"language-yaml\">masterauth<span class=\"w\"> </span>rootroot</code></pre></div><h2>replica-serve-stale-data</h2><p><code>replica-serve-stale-data</code>配置项允许用户设定，当主从链接断开的时候，从节点如何响应客户端的请求。默认情况下，该配置项的值为 yes， 即允许从节点正常响应用户的请求，但是可能会发生数据是过期的，甚至是根本没有数据的情况。若将其赋值为 no，则从节点将会对客户端的请求返回一个错误，拒绝提供服务。</p><div class=\"highlight\"><pre><code class=\"language-yaml\"><span class=\"c\"># Example </span><span class=\"w\">\n</span><span class=\"w\"></span>replica-serve-stale-data<span class=\"w\"> </span>yes</code></pre></div><h2>replica-read-only</h2><p><code>replica-read-only</code> 配置项允许用户将从节点设置为只读模式。这是一种保护措施。因为当主从关系建立之后，主节点会一直向从节点同步数据。如果用户向从节点写数据的话，最终会被覆盖。该配置项默认为yes。</p><div class=\"highlight\"><pre><code class=\"language-yaml\"><span class=\"c\"># Example </span><span class=\"w\">\n</span><span class=\"w\"></span>replica-read-only<span class=\"w\"> </span>yes</code></pre></div><h2>repl-ping-replica-period</h2><p>该配置项默认值为10，表示从节点每隔 10s 将向主节点执行 PING 命令，以确认两者之间的关系正常。</p><div class=\"highlight\"><pre><code class=\"language-yaml\"><span class=\"c\"># Example </span><span class=\"w\">\n</span><span class=\"w\"></span>repl-ping-replica-period<span class=\"w\"> </span>yes</code></pre></div><h2>repl-timeout</h2><p><code>repl-timeout</code>配置项为主从节点之间设置了一个通信的超时时间。若该配置项指定的时间内发现两者没有通信，则认为主从节点间的链接异常。这个配置项的值在设置的时候需要大于 <code>replica-read-only</code>配置项的值，否则在主从节点之间数据量较低的情况下，可能会一直检测到「超时」。</p><div class=\"highlight\"><pre><code class=\"language-yaml\"><span class=\"c\"># Example </span><span class=\"w\">\n</span><span class=\"w\"></span>repl-timeout<span class=\"w\"> </span>yes</code></pre></div><h2>repl-disable-tcp-nodelay</h2><p><code>repl-disable-tcp-nodelay</code>配置项主要用于控制主从节点同步数据的速率。若该配置项的值为 yes，则 Redis 将使用更小尺寸的 TCP 报文和更小的带宽来进行数据同步操作。好处比较明显，能够控制 Redis 数据同步的流量，坏处就是增加了数据同步的时延。反之，如果设置为 no，则会去掉这个限制。提升数据同步的效率。</p><div class=\"highlight\"><pre><code class=\"language-yaml\"><span class=\"c\"># Example </span><span class=\"w\">\n</span><span class=\"w\"></span>repl-disable-tcp-nodelay<span class=\"w\"> </span>yes</code></pre></div><h2>repl-backlog-size &amp;&amp; repl-backlog-ttl</h2><p>Master 节点会为 Slave 节点配置一个复制缓冲区，用来保存最新的需要同步的数据。当 Slave 节点和 Master 节点断开连接之后又重新建立链接的时候，很可能只需要一次「局部」的同步操作即可实现主从数据的一致性。这就是复制缓冲区的作用。而 <code>repl-backlog-size</code> 配置项，指定了这个缓冲区的大小。缓冲区越大，那么保存的数据也就越多，对 Slave 节点离线时间的长度容忍度也就越高。</p><p>但是，如果主从链接是我们有意断开的，这个复制缓冲区总不能一直保存在那里。所以 Redis 提供了<code>repl-backlog-ttl</code>配置项，当主从链接断开的时间超过该配置项的值的时候，Master 节点将会清空这个复制缓冲区的内容。</p><p><code>repl-backlog-size</code> 默认值为 16mb，<code>repl-backlog-ttl</code> 为3600。</p><h2>replica-priority</h2><p><code>replica-priority</code> 配置项主要用于 Redis Slave 节点，标识了这个节点的优先级，取值范围从0~100。数字越低，优先级越高。在进行主从切换的时候越有可能被 Sentinel 节点提升为 Master 节点。</p><div class=\"highlight\"><pre><code class=\"language-yaml\"><span class=\"c\"># Example </span><span class=\"w\">\n</span><span class=\"w\"></span>replica-priority<span class=\"w\"> </span><span class=\"m\">100</span></code></pre></div><h2>min-replicas-to-write &amp;&amp; min-replicas-max-lag</h2><p><code>min-replicas-to-write</code> 和 <code>min-replicas-max-lag</code> 两个配置项分别指定了最小链接到主节点的活跃的从节点的个数以及最大主从节点通信的延迟。如果在主从节点通信的过程中，这个两个选项的条件有哪一个没有满足，主节点将拒绝客户端的写入操作。前者默认值为3，或者为10，都是秒为单位。</p><div class=\"highlight\"><pre><code class=\"language-yaml\"><span class=\"c\"># Example</span><span class=\"w\">\n</span><span class=\"w\"></span><span class=\"c\">#</span><span class=\"w\">\n</span><span class=\"w\"></span><span class=\"c\"># min-replicas-to-write 3</span><span class=\"w\">\n</span><span class=\"w\"></span><span class=\"c\"># min-replicas-max-lag 10</span></code></pre></div><h2>slave-announce-ip &amp;&amp; slave-announce-port</h2><p>一般来说，通过对 Master 节点执行 Info Replication 命令，即可通过 Master 节点列出所有的 Slave 节点的相关信息（IP，port 等）。Master 节点本来是可以通过两者之间的链接来发现 Slave 节点的 IP 和端口的。但是，如果在环境中启动了 NAT 或者端口转发机制（尤其是在 Docker 和 k8s 中），或者说，两者通信链路之间还通过了一些「代理」，在 Master 上显示的 Slave 节点的信息可能就不准确。所以，用户可以通过 <code>slave-announce-ip</code>和 <code>slave-announce-port</code> 配置项，强制指定 Slave 节点的 IP 和 Port，并将此信息通知给 Master 节点。</p><div class=\"highlight\"><pre><code class=\"language-yaml\"><span class=\"c\"># Example</span><span class=\"w\">\n</span><span class=\"w\"></span><span class=\"c\"># replica-announce-ip 5.5.5.5</span><span class=\"w\">\n</span><span class=\"w\"></span><span class=\"c\"># replica-announce-port 1234</span></code></pre></div><h2>Security</h2><h2>requirepass</h2><p>用户可以通过<code>requirepass</code>配置项为 Redis Server 设置密码。客户端在连接了服务端之后，需要使用密码鉴权才能够执行命令。</p><h2>rename-command</h2><p><code>rename-command</code> 是 Redis 为提升其安全性加入的配置项。它允许用户将一些高危命令进行重命名，以防止用户不小心或者恶意对数据库做出一些危险的操作。</p><p>这个配置项虽然有用，可以提升安全性。但是最好用在 Redis 实例只有一个用户使用的场景下。因为不同的用户需求不同，强行禁用掉一些命令也并不是一个好事。</p><div class=\"highlight\"><pre><code class=\"language-yaml\"><span class=\"c\"># Example</span><span class=\"w\">\n</span><span class=\"w\"></span><span class=\"c\"># rename-command CONFIG &#34;&#34;</span><span class=\"w\">\n</span><span class=\"w\"></span><span class=\"c\"># rename-command flushall &#34;&#34;</span></code></pre></div><h2>Memory</h2><p>内存对于 Redis 是一个非常重要的资源，它是 Redis 高性能的支撑点，同时也是最容易出问题的地方。所以，Redis 也开放了一些配置项，使得用户可以根据自己不同的使用姿势来对 Redis 使用内存的行为作出一些限制，以此来提高效率和防止一些问题的发生。</p><h2>maxmemory &amp;&amp; maxmemory-policy</h2><p>该配置项允许用户为 Redis 配置一个「最大内存使用值」。如果用户在设置了<code>maxmemory</code>的同时还通过 <code>maxmemory-policy</code>设置了一定的缓存淘汰策略。那么当Redis 实例的内存使用量达到阈值之后，会通过「淘汰规则」清理掉一部分的数据，以留出足够的空间来正常响应后续的写入操作。否则，若没有设置淘汰策略(默认为noeviction)，当 Redis 实例的内存达到阈值之后，会拒绝写入操作，但是正常响应读取操作。</p><div class=\"highlight\"><pre><code class=\"language-yaml\"><span class=\"c\"># Example</span><span class=\"w\">\n</span><span class=\"w\"></span><span class=\"c\"># maxmemory &lt;bytes&gt;</span><span class=\"w\">\n</span><span class=\"w\"></span><span class=\"c\"># maxmemory-policy noeviction</span></code></pre></div><h2>maxmemory-samples</h2><p>Redis 在执行内存淘汰策略的时候，会按照一定的规则选取「候选样本」。比如，当用户将<code>maxmemory-policy</code>设置为<code>volatile-lru</code>的时候，如果 Redis 实例的内存用量达到了<code>maxmemory</code>指定的阈值，Redis 就会每次取<code>maxmemory-samples</code>个 key 为样本，删掉一个最近未使用时间最长的 key。<code>maxmemory-samples</code>设置的值越大，淘汰策略越接近真实的 LRU 算法，但是同时给 CPU 造成的压力也会增加。否则，虽然会运行的比较快，但是淘汰的效果不会很好。</p><div class=\"highlight\"><pre><code class=\"language-yaml\"><span class=\"c\"># Example</span><span class=\"w\">\n</span><span class=\"w\"></span><span class=\"c\"># maxmemory-samples 5</span></code></pre></div><p></p>", 
            "topic": [
                {
                    "tag": "Redis", 
                    "tagLink": "https://api.zhihu.com/topics/19557280"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50101300", 
            "userName": "要没时间了", 
            "userLink": "https://www.zhihu.com/people/bdbe7f88af9a021c5c770649d266f143", 
            "upvote": 15, 
            "title": "K8s GC Design Principle", 
            "content": "<p></p><blockquote> Ref: <a href=\"https://link.zhihu.com/?target=https%3A//github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/garbage-collection.md%23orphaning-the-descendants-with-orphan-finalizer\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/kubernetes/c</span><span class=\"invisible\">ommunity/blob/master/contributors/design-proposals/api-machinery/garbage-collection.md#orphaning-the-descendants-with-orphan-finalizer</span><span class=\"ellipsis\"></span></a> Warning：设计文档的对应的 k8s 版本为1.7<br/><br/>PS: 知乎专栏对代码高亮支持的不是很好，想尝试更好的阅读体验请移步我的博客：</blockquote><a href=\"https://link.zhihu.com/?target=http%3A//littledriver.net/\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Home</a><p class=\"ztext-empty-paragraph\"><br/></p><p><i>Q: What is GC of Kuernetes ?</i></p><p><i>A:</i></p><p>GC 是 Garbage Collector 的简称。从功能层面上来说，它和编程语言当中的「GC」 基本上是一样的。它清理 Kubernetes 中「符合特定条件」的 Resource Object。（在 k8s 中，你可以认为万物皆资源，很多逻辑的操作对象都是 Resource Object。）</p><p><i>Q: What are dependent mechanisms to clear needless resource objects?</i></p><p><i>A:</i></p><p>Kubernetes 在不同的 Resource Objects 中维护一定的「从属关系」。内置的 Resource Objects 一般会默认在一个 Resource Object 和它的创建者之间建立一个「从属关系」。当然，你也可以利用<code>ObjectMeta.OwnerReferences</code>自由的去给两个 Resource Object 建立关系，前提是被建立关系的两个对象必须在一个 Namespace 下。</p><div class=\"highlight\"><pre><code class=\"language-text\">// OwnerReference contains enough information to let you identify an owning\n// object. Currently, an owning object must be in the same namespace, so there\n// is no namespace field.\ntype OwnerReference struct {\n    // API version of the referent.\n    APIVersion string `json:&#34;apiVersion&#34; protobuf:&#34;bytes,5,opt,name=apiVersion&#34;`\n    // Kind of the referent.\n    // More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#types-kinds\n    Kind string `json:&#34;kind&#34; protobuf:&#34;bytes,1,opt,name=kind&#34;`\n    // Name of the referent.\n    // More info: http://kubernetes.io/docs/user-guide/identifiers#names\n    Name string `json:&#34;name&#34; protobuf:&#34;bytes,3,opt,name=name&#34;`\n    // UID of the referent.\n    // More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n    UID types.UID `json:&#34;uid&#34; protobuf:&#34;bytes,4,opt,name=uid,casttype=k8s.io/apimachinery/pkg/types.UID&#34;`\n    // If true, this reference points to the managing controller.\n    // +optional\n    Controller *bool `json:&#34;controller,omitempty&#34; protobuf:&#34;varint,6,opt,name=controller&#34;`\n    // If true, AND if the owner has the &#34;foregroundDeletion&#34; finalizer, then\n    // the owner cannot be deleted from the key-value store until this\n    // reference is removed.\n    // Defaults to false.\n    // To set this field, a user needs &#34;delete&#34; permission of the owner,\n    // otherwise 422 (Unprocessable Entity) will be returned.\n    // +optional\n    BlockOwnerDeletion *bool `json:&#34;blockOwnerDeletion,omitempty&#34; protobuf:&#34;varint,7,opt,name=blockOwnerDeletion&#34;`\n}</code></pre></div><p><code>OwnerReference</code>一般存在于某一个 Resource Object  信息中的 metadata 部分。<code>OwnerReference</code>中的字段可以唯一的确定 k8s 中的一个 Resource Object。两个 Object 可以通过这种方式建立一个 <code>owner-dependent</code>的关系。</p><p>K8s 实现了一种「Cascading deletion」（级联删除）的机制，它利用已经建立的「从属关系」进行资源对象的清理工作。例如，当一个 dependent 资源的 owner 已经被删除或者不存在的时候，从某种角度就可以判定，这个 dependent 的对象已经是异常（无人管辖）的了，需要进行清理。而 「cascading deletion」则是被 k8s 中的一个 controller 组件实现的：<code>Garbage Collector</code></p><p>所以，k8s 是通过 <code>Garbage Collector</code> 和 <code>ownerReference</code> 一起配合实现了「垃圾回收」的功能。</p><p><b>Q: What is the relationship like ?(owner-dependent)</b></p><p><b>A:</b></p><p>我们可以通过一个实际的例子来了解这个「从属关系」：</p><div class=\"highlight\"><pre><code class=\"language-yaml\">apiVersion<span class=\"p\">:</span><span class=\"w\"> </span>extensions/v1beta1<span class=\"w\">\n</span><span class=\"w\"></span>kind<span class=\"p\">:</span><span class=\"w\"> </span>ReplicaSet<span class=\"w\">\n</span><span class=\"w\"></span>metadata<span class=\"p\">:</span><span class=\"w\">\n</span><span class=\"w\">  </span>annotations<span class=\"p\">:</span><span class=\"w\">\n</span><span class=\"w\">    </span>deployment.kubernetes.io/desired-replicas<span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&#34;2&#34;</span><span class=\"w\">\n</span><span class=\"w\">    </span>deployment.kubernetes.io/max-replicas<span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&#34;3&#34;</span><span class=\"w\">\n</span><span class=\"w\">    </span>deployment.kubernetes.io/revision<span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&#34;1&#34;</span><span class=\"w\">\n</span><span class=\"w\">  </span>creationTimestamp<span class=\"p\">:</span><span class=\"w\"> </span><span class=\"ld\">2018-09-07T07:11:52Z</span><span class=\"w\">\n</span><span class=\"w\">  </span>generation<span class=\"p\">:</span><span class=\"w\"> </span><span class=\"m\">1</span><span class=\"w\">\n</span><span class=\"w\">  </span>labels<span class=\"p\">:</span><span class=\"w\">\n</span><span class=\"w\">    </span>app<span class=\"p\">:</span><span class=\"w\"> </span>coffee<span class=\"w\">\n</span><span class=\"w\">    </span>pod-template-hash<span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&#34;3866135192&#34;</span><span class=\"w\">\n</span><span class=\"w\">  </span>name<span class=\"p\">:</span><span class=\"w\"> </span>coffee-7dbb5795f6<span class=\"w\">\n</span><span class=\"w\">  </span>namespace<span class=\"p\">:</span><span class=\"w\"> </span>default<span class=\"w\">\n</span><span class=\"w\">  </span>ownerReferences<span class=\"p\">:</span><span class=\"w\">\n</span><span class=\"w\">  </span>-<span class=\"w\"> </span>apiVersion<span class=\"p\">:</span><span class=\"w\"> </span>apps/v1<span class=\"w\">\n</span><span class=\"w\">    </span>blockOwnerDeletion<span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span><span class=\"w\">\n</span><span class=\"w\">    </span>controller<span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span><span class=\"w\">\n</span><span class=\"w\">    </span>kind<span class=\"p\">:</span><span class=\"w\"> </span>Deployment<span class=\"w\">\n</span><span class=\"w\">    </span>name<span class=\"p\">:</span><span class=\"w\"> </span>coffee<span class=\"w\">\n</span><span class=\"w\">    </span>uid<span class=\"p\">:</span><span class=\"w\"> </span>4b807ee6-b26d-<span class=\"m\">11e8</span>-b891-fa163eebca40<span class=\"w\">\n</span><span class=\"w\">  </span>resourceVersion<span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&#34;476159&#34;</span><span class=\"w\">\n</span><span class=\"w\">  </span>selfLink<span class=\"p\">:</span><span class=\"w\"> </span>/apis/extensions/v1beta1/namespaces/default/replicasets/coffee-7dbb5795f6<span class=\"w\">\n</span><span class=\"w\">  </span>uid<span class=\"p\">:</span><span class=\"w\"> </span>4b81e76c-b26d-<span class=\"m\">11e8</span>-b891-fa163eebca40<span class=\"w\">\n</span><span class=\"w\"></span>spec<span class=\"p\">:</span><span class=\"w\">\n</span><span class=\"w\">  </span>replicas<span class=\"p\">:</span><span class=\"w\"> </span><span class=\"m\">2</span><span class=\"w\">\n</span><span class=\"w\"></span>....</code></pre></div><p>上面截取了一个 ReplicaSet Object 中的 metadata 的部分信息。我们可以注意到，它的 <code>ownerReferences</code>字段标识了一个 Deployment Object。我们都清楚的是，ReplicaSet 会创建一系列的 Pod。通过<code>spec.replicas:2</code>可以知道，他会创建两个pod。</p><div class=\"highlight\"><pre><code class=\"language-text\">root@xr-service-mesh-lab:~/istio-1.0.2# kubectl get pods  | grep coffee\ncoffee-7dbb5795f6-6crxz          1/1       Running   0          9d\ncoffee-7dbb5795f6-hv7tr          1/1       Running   0          5d\nroot@xr-service-mesh-lab:~/istio-1.0.2#</code></pre></div><p>让我们来观察其中一个 Pod：</p><div class=\"highlight\"><pre><code class=\"language-yaml\">apiVersion<span class=\"p\">:</span><span class=\"w\"> </span>v1<span class=\"w\">\n</span><span class=\"w\"></span>kind<span class=\"p\">:</span><span class=\"w\"> </span>Pod<span class=\"w\">\n</span><span class=\"w\"></span>metadata<span class=\"p\">:</span><span class=\"w\">\n</span><span class=\"w\">  </span>annotations<span class=\"p\">:</span><span class=\"w\">\n</span><span class=\"w\">    </span>cni.projectcalico.org/podIP<span class=\"p\">:</span><span class=\"w\"> </span><span class=\"m\">192.168</span>.<span class=\"m\">0.14</span>/<span class=\"m\">32</span><span class=\"w\">\n</span><span class=\"w\">  </span>creationTimestamp<span class=\"p\">:</span><span class=\"w\"> </span><span class=\"ld\">2018-09-07T07:11:52Z</span><span class=\"w\">\n</span><span class=\"w\">  </span>generateName<span class=\"p\">:</span><span class=\"w\"> </span>coffee-7dbb5795f6<span class=\"sd\">-\n</span><span class=\"sd\">  labels:</span><span class=\"w\">\n</span><span class=\"w\">    </span>app<span class=\"p\">:</span><span class=\"w\"> </span>coffee<span class=\"w\">\n</span><span class=\"w\">    </span>pod-template-hash<span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&#34;3866135192&#34;</span><span class=\"w\">\n</span><span class=\"w\">  </span>name<span class=\"p\">:</span><span class=\"w\"> </span>coffee-7dbb5795f6-6crxz<span class=\"w\">\n</span><span class=\"w\">  </span>namespace<span class=\"p\">:</span><span class=\"w\"> </span>default<span class=\"w\">\n</span><span class=\"w\">  </span>ownerReferences<span class=\"p\">:</span><span class=\"w\">\n</span><span class=\"w\">  </span>-<span class=\"w\"> </span>apiVersion<span class=\"p\">:</span><span class=\"w\"> </span>apps/v1<span class=\"w\">\n</span><span class=\"w\">    </span>blockOwnerDeletion<span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span><span class=\"w\">\n</span><span class=\"w\">    </span>controller<span class=\"p\">:</span><span class=\"w\"> </span><span class=\"kc\">true</span><span class=\"w\">\n</span><span class=\"w\">    </span>kind<span class=\"p\">:</span><span class=\"w\"> </span>ReplicaSet<span class=\"w\">\n</span><span class=\"w\">    </span>name<span class=\"p\">:</span><span class=\"w\"> </span>coffee-7dbb5795f6<span class=\"w\">\n</span><span class=\"w\">    </span>uid<span class=\"p\">:</span><span class=\"w\"> </span>4b81e76c-b26d-<span class=\"m\">11e8</span>-b891-fa163eebca40<span class=\"w\">\n</span><span class=\"w\">  </span>resourceVersion<span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&#34;76727&#34;</span><span class=\"w\">\n</span><span class=\"w\">  </span>selfLink<span class=\"p\">:</span><span class=\"w\"> </span>/api/v1/namespaces/default/pods/coffee-7dbb5795f6-6crxz<span class=\"w\">\n</span><span class=\"w\">  </span>uid<span class=\"p\">:</span><span class=\"w\"> </span>4b863e4d-b26d-<span class=\"m\">11e8</span>-b891-fa163eebca40</code></pre></div><p>我们可以看出，pod 中的<code>ownerReferences</code>所标识的 Object 正式我们上面看到过的 ReplicaSet。最后让我们来检查一下 ReplicaSet 所对应的 Deployment 的情况：</p><div class=\"highlight\"><pre><code class=\"language-yaml\">apiVersion<span class=\"p\">:</span><span class=\"w\"> </span>extensions/v1beta1<span class=\"w\">\n</span><span class=\"w\"></span>kind<span class=\"p\">:</span><span class=\"w\"> </span>Deployment<span class=\"w\">\n</span><span class=\"w\"></span>metadata<span class=\"p\">:</span><span class=\"w\">\n</span><span class=\"w\">  </span>annotations<span class=\"p\">:</span><span class=\"w\">\n</span><span class=\"w\">    </span>deployment.kubernetes.io/revision<span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&#34;1&#34;</span><span class=\"w\">\n</span><span class=\"w\">  </span>creationTimestamp<span class=\"p\">:</span><span class=\"w\"> </span><span class=\"ld\">2018-09-07T07:11:52Z</span><span class=\"w\">\n</span><span class=\"w\">  </span>generation<span class=\"p\">:</span><span class=\"w\"> </span><span class=\"m\">1</span><span class=\"w\">\n</span><span class=\"w\">  </span>labels<span class=\"p\">:</span><span class=\"w\">\n</span><span class=\"w\">    </span>app<span class=\"p\">:</span><span class=\"w\"> </span>coffee<span class=\"w\">\n</span><span class=\"w\">  </span>name<span class=\"p\">:</span><span class=\"w\"> </span>coffee<span class=\"w\">\n</span><span class=\"w\">  </span>namespace<span class=\"p\">:</span><span class=\"w\"> </span>default<span class=\"w\">\n</span><span class=\"w\">  </span>resourceVersion<span class=\"p\">:</span><span class=\"w\"> </span><span class=\"s2\">&#34;476161&#34;</span><span class=\"w\">\n</span><span class=\"w\">  </span>selfLink<span class=\"p\">:</span><span class=\"w\"> </span>/apis/extensions/v1beta1/namespaces/default/deployments/coffee<span class=\"w\">\n</span><span class=\"w\">  </span>uid<span class=\"p\">:</span><span class=\"w\"> </span>4b807ee6-b26d-<span class=\"m\">11e8</span>-b891-fa163eebca40</code></pre></div><p>对比一下 ReplicaSet Object 中 ownerReference 标识的 Object 可知，这个 Deployment 是 ReplicaSet 的 owner。至此，我们通过观察三个 Object 中的 ownerReference 的信息，可以建立起如下的「从属关系」：</p><ul><li>Deployment（owner）—&gt; ReplicaSet (dependent)</li><li>ReplicaSet (owner) —&gt; Pod (dependent)</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-aa3cf3451a5d190501df2d6cf7b9bd57_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"811\" data-rawheight=\"586\" class=\"origin_image zh-lightbox-thumb\" width=\"811\" data-original=\"https://pic4.zhimg.com/v2-aa3cf3451a5d190501df2d6cf7b9bd57_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;811&#39; height=&#39;586&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"811\" data-rawheight=\"586\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"811\" data-original=\"https://pic4.zhimg.com/v2-aa3cf3451a5d190501df2d6cf7b9bd57_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-aa3cf3451a5d190501df2d6cf7b9bd57_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><i>Q: What is the working mechanism of Garbage Collector?</i></p><p><i>A:</i></p><p>一个 Garbage Collector 通常由三部分实现：</p><ul><li>Scanner： 它负责收集目前系统中已存在的 Resource，并且周期性的将这些资源对象放入一个队列中，等待处理（检测是否要对某一个Resource Object 进行 GC 操作）</li><li>Garbage Processor: Garbage Processor 由两部分组成</li><ul><li>Dirty Queue： Scanner 会将周期性扫描到的 Resource Object 放入这个队列中等待处理</li><li>Worker：worker 负责从这个队列中取出元素进行处理</li><ul><li>检查 Object 的 metaData 部分，查看<code>ownerReference</code>字段是否为空</li><ul><li>如果为空，则本次处理结束</li><li>如果不为空，检测<code>ownerReference</code>字段内标识的 Owner Resource Object是否存在</li><ul><li>存在：则本次处理结束</li><li>不存在：删除这个 Object</li></ul></ul></ul></ul></ul><p>其实，在有了 Scanner 和 Garbage Processor 之后，Garbage Collector 就已经能够实现「垃圾回收」的功能了。但是有一个明显的问题：Scanner 的扫描频率设置多少好呢？太长了，k8s 内部就会积累过多的「废弃资源」；太短了，尤其是在集群内部资源对象较多的时候，频繁的拉取信息对 API-Server 也是一个不小的压力。</p><p>k8s 作为一个分布式的服务编排系统，其内部执行任何一项逻辑或者行为，都依赖一种机制：「事件驱动」。说的简单点，k8s 中一些看起来「自动」的行为，其实都是由一些神秘的「力量」在驱动着。而这个「力量」就是我们所说的「Event」。任意一个 Resource Object 发生变动的时候（新建，更新，删除），都会触发一个 k8s 的事件（Event），这个事件在 k8s 的内部是公开的，也就是说，我们可以在任意一个地方监听这些事件。</p><p>总的来说，无论是「事件的监听机制」还是「周期性访问 API-Server 批量获取 Resource Object 信息」，其目的都是为了能够掌握 Resource Object 的最新信息。两者是各有优势的：</p><ol><li>批量拉取：一次性拉取所有的 Resource Object，全面</li><li>监听 Resource 的 Event：实时性强， 且对 API—SERVER 不会造成太大的压力</li></ol><p>综上所述，在实现 Garbage Collector 的过程中，k8s 向其添加了一个「增强型」的组件：Propagator</p><ul><li>Propagator： Propagator 由三个部分构成</li><ul><li>EventQueue：负责存储 k8s 中资源对象的事件（Eg：ADD，UPDATE，DELETE）</li><li>DAG(有向无环图)：负责存储 k8s 中所有资源对象的「owner-dependent」 关系</li><li>Worker：从 EventQueue 中，取出资源对象的事件，根据事件的类型会采取以下两种操作</li><ul><li>ADD/UPDATE: 将该事件对应的资源对象加入 DAG，且如果该对象有 owner 且 owner 不在 DAG 中，将它同时加入 Garbage Processor 的 Dirty Queue 中</li><li>DELETE：将该事件对应的资源对象从 DAG 中删除，并且将其「管辖」的对象（只向下寻找一级，如删除 Deployment，那么只操作 ReplicaSet ）加入 Garbage Processor 的 Dirty Queue 中</li></ul></ul></ul><p>在有了 Propagator 的加入之后，我们完全可以仅在 GC 开始运行的时候，让 Scanner 扫描一下系统中所有的 Object，然后将这些信息传递给 Propagator 和 Dirty Queue。只要 DAG 一建立起来之后，那么 Scanner 其实就没有再工作的必要了。「事件驱动」的机制提供了一种增量的方式让 GC 来监控 k8s 集群内部的资源对象变化情况。</p><p><i>Q: How can I delete the owner and reserve dependents ?</i></p><p><i>A:</i></p><blockquote> ╮(╯▽╰)╭没错，需求就是这么奇怪，k8s 还兼容一种情况：删除 owner，留下 dependents。剩余的 dependents 被称为是「orphan」<br/> </blockquote><h2>你想怎么实现?</h2><p>如果暂时先不看设计文档中关于这部分的内容，根据之前对 k8s GC 的了解，让你来实现这个功能，你会怎么做呢？这里给出一下笔者的想法：</p><p>首先，我们先来根据上面对于 GC 的了解，给出一幅大致架构图：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-c590e54eca8035f4b70144299a9487e5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1068\" data-rawheight=\"797\" class=\"origin_image zh-lightbox-thumb\" width=\"1068\" data-original=\"https://pic2.zhimg.com/v2-c590e54eca8035f4b70144299a9487e5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1068&#39; height=&#39;797&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1068\" data-rawheight=\"797\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1068\" data-original=\"https://pic2.zhimg.com/v2-c590e54eca8035f4b70144299a9487e5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-c590e54eca8035f4b70144299a9487e5_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>在上图中，我用三种颜色分别标记了三条较为重要的处理过程：</p><ul><li>红色：worker 从 dirtyQueue 中取出资源对象，检查其是否带有 owner ，如果没带，则不处理。否则检测其 owner是否存在，存在，则处理下一个资源对象，不存在，删除这个 object。</li><li>绿色： scanner 从 api-server 中扫描存在于 k8s 集群中的资源对象并加入至 dirtyQueue</li><li>粉色：propagator.worker 从 eventQueue 中取出相应的事件并且获得对应的资源对象，根据事件的类型以及相应资源对象所属 owner 对象的情况来进行判定，是否要进行两个操作：</li><ul><li>从 DAG 中删除相应节点（多为响应 DELETE 事件的逻辑）</li><li>将有级联关系但是 owner 不存在的对象送入 diryQueue 中</li></ul></ul><p>其中红色是「数据处理」过程，而绿色和粉色是「数据收集」的过程。在「数据处理」的过程中（即我们上面分析过的 GC 的 Worker 的工作过程），worker 做的较为重要的工作有两步：</p><ul><li>检查资源对象信息的「ownerReference」字段，判断其是否处在一个级联关系中</li><li>若资源对象有所属 owner 且不存在，则删除这个对象</li></ul><p>此时，回头看下我们的需求:「owner 删除，dependents 留下」。如果想在「数据处理」这条链路上做些修改达到我们目的的话，唯一可行的办法就是：在删除了 dependents 对应的 owner 对象之后，同时删除 dependents 信息中 「ownerReference」字段和对应的值。这样一来，在检测资源对象是否应该被删除的过程就会因为其没有「ownerReference」字段而放过它，最终实现了 dependents 对象的“孤立”。</p><h2>k8s 是怎么实现的?</h2><blockquote> 如果你了解 gRPC-intercepter 的工作机制，那么会加快你理解下面的内容<br/> </blockquote><p>k8s 在系统内部实现了一种类似「删除拦截器链」的机制：即在删除某个资源对象的「删除链路」上，执行一个或多个「拦截逻辑」。并且这种「拦截逻辑」可以自主实现，然后像插件一样注入到这个删除链路上。这种机制在 k8s 当中统称为： <code>Finalizers</code>。<code>Finalizers</code>的声明非常简单，就是一个<code>[]string</code>。这个 Slice 的内部填充的是要执行拦截器的名称。它存在于任何一个资源对象的 Meta 信息中: <a href=\"https://link.zhihu.com/?target=https%3A//github.com/kubernetes/apimachinery/blob/master/pkg/apis/meta/v1/types.go%23L246\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">apimachinery/types.go at master · kubernetes/apimachinery · GitHub</a>。<code>Finalizers</code>中的拦截器在其宿主资源对象触发删除操作之后顺序执行（资源对象的deletionTimestamp不为 nil），每执行完一个，就会从<code>Finalizers</code>中移除一个，直到<code>Finalizers</code>为空的 Slice，其宿主资源对象才可以被真正的删除。</p><p>对于「删除 owner 但是不删除 dependents」 的需求，k8s 则是实现了一个：orphan finalizer。一般情况下，正常利用 GC 级连删除一个资源对象是不会涉及到 orphan finalizer 的。它执行的是我们之前提到的 GC 的工作逻辑。如果你想启用这个特性，就需要在删除资源对象的时候，根据 K8s 版本的不同，将名为<code>DeleteOption.OrphanDependents</code>的参数赋值为 True（1.7版本以前）, 或者将<code>DeleteOption.PropagationPolicy</code>参数赋值为 <code>metav1.DeletePropagationOrphan</code>：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/kubernetes/apimachinery/blob/9dc1de72c0f3996657ffc88895f89f3844d8cf01/pkg/apis/meta/v1/types.go%23L457\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">apimachinery/types.go at 9dc1de72c0f3996657ffc88895f89f3844d8cf01 · kubernetes/apimachinery · GitHub</a>。通过这个参数的注释也可以看出：如果设置好之后，将会在它的 Finalizers 中加入 orphan finalizer。而加入 orphan finalizer 这部分的逻辑是在 api-server 的 package 中：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/kubernetes/apiserver/blob/master/pkg/registry/generic/registry/store.go%23L719\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">apiserver/store.go at master · kubernetes/apiserver · GitHub</a></p><p>加入了 orphan finalizer 之后，在 GC 的 worker 从 dirtyQueue 中取出 owner 资源对象进行处理的时候，就会执行它的逻辑：删除 dependents 的 OwnerReference 部分： <a href=\"https://link.zhihu.com/?target=https%3A//github.com/kubernetes/kubernetes/blob/0972ce1accf859b73abb5a68c0adf4174245d4bf/pkg/controller/garbagecollector/garbagecollector.go%23L543\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">kubernetes/garbagecollector.go at 0972ce1accf859b73abb5a68c0adf4174245d4bf · kubernetes/kubernetes · GitHub</a>。最终，在「保留 dependents」 的逻辑完成之后，orphan finalizer 也会从相应资源对象的<code>Finalizers</code>中删除。</p><h2>一个隐含的 Race 问题</h2><p>对于 Controller 来说，它会周期性的通过 Selector 来寻找它所创建的资源。如果在筛选到了符合自己 label 的 资源，但是发现它的 Meta.OwnerReference 字段中没有自己相关的信息的时候，就会执行一个 <code>Adoption</code>的操作，也就是将和自己有关的 OwnerReference 信息注入到这个 Pod 的 Meta 部分。这种逻辑虽然看起来是比较「保险」，但是实际上它和 orphan finalizer 的逻辑是有冲突的。前者是对 dependents 增加 OwnerReference 信息， 后者则是删除它。两个逻辑在执行的时候，如果不保证「互斥」的话，很可能就会出现一个很严重的竞争问题：指定了 orphan finalizer 的 对象，其 dependents 最终也会被删除。</p><p>借鉴操作系统对于「竞争」问题的处理方式，对 OwnerReference操作的的逻辑（即临界区），应该被「互斥」机制保护起来。而在 k8s 中，实现这种互斥保护机制的方式也很简单：Controller 在想执行 Adoption 操作之前，会检查一下当前资源对象的meta.DeletionTimestamp。如果这个字段的值为非 nil，那么就证明这个资源对象已经在被删除中了。所以就不会再继续执行 Adoption 操作。</p><p>但是仔细想一下，这种「互斥」保护机制的实现方式，看起来是借助了一个「锁变量」(meta.DeletionTimestamp)的帮助。不过，我们并不需要担心这个字段的「竞争」问题，因为能修改它的操作，只有「删除」操作，而删除操作是肯定会发生在 orphan finalizer 执行之前的。也就是说，当 orphan finalizer 执行的时候，这个值早就被设置进去了。 <a href=\"https://link.zhihu.com/?target=https%3A//github.com/kubernetes/kubernetes/blob/7f23a743e8c23ac6489340bbb34fa6f1d392db9d/pkg/controller/replicaset/replica_set.go%23L644\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">kubernetes/replica_set.go at 7f23a743e8c23ac6489340bbb34fa6f1d392db9d · kubernetes/kubernetes · GitHub</a></p><p><i>Q: How Kubernetes defines delete operation of resource object?</i></p><p><i>A:</i> </p><p>K8s 在对资源对象「删除」操作的定义上，思考了一个较为重要的问题：「删除」操作真正完成的标志是什么？（达到什么样的条件才可以通知用户「删除」操作成功）。这个问题出现的源头是在用户侧，当用户在使用 k8s 提供的资源对象的「删除」操作时，有个问题会影响到他们：</p><ol><li>「删除」操作成功多久后才可以在同一个 ns 下创建同名资源对象？</li></ol><p>如果你了解过构建一个 k8s 集群所需要的服务组件，就可以很清楚的知道：k8s 中的资源对象的信息都是存于一个key-value 的数据库当中的（etcd），且是以名字来做索引的。通过命令行<code>kubectl get xxx</code>查询的资源对象的信息都来自于那。而且，当<code>kubelet</code>组件删除掉其所在节点上的一些资源的时候，会调用 API-Server 提供的接口删除掉key-value 数据库中相应的记录。所以，在 k8s 中，给「删除」操作下了这样一个定义：</p><blockquote> 在没有 orphanFinalizer 参与的前提下，直到被删除对象及其「管辖」对象的信息在 key-value 数据库中都被清除，才认为该对象真正的被 GC 回收。即达到了返回给用户「删除成功」的标准。<br/> </blockquote><p>本质上来说，上述所表示的删除操作是「同步」的。因为有「级联关系」（owner-dependent） 关系的存在，删除一个资源对象往往影响的不是他自己，还有他的 dependents。只有将因它出现的所有资源都删除，才可以认为这个对象被删除了。</p><p>若想指定这种同步的删除模式，需要在两个不同的位置设置两个参数：</p><ol><li>dependents 对象 meta 信息中 OwnerReference.BlockOwnerDeletion</li><li>在发送删除对象请求时，设置 DeleteOptions.PropagationPolicy </li></ol><p><code>OwnerReference.BlockOwnerDeletion</code>参数大多数情况下在相应的 dependents 对象创建的时候就设置进去了。如果想在 dependents 对象创建之后更新这个参数的值，可能需要使用<code>admission controller</code> (1.7及以上版本)提供的一些权限相关的功能。</p><p><code>DeleteOptions.PropagationPolicy</code>一共有3个候选值：</p><div class=\"highlight\"><pre><code class=\"language-go\"><span class=\"c1\">// DeletionPropagation decides if a deletion will propagate to the dependents of\n</span><span class=\"c1\">// the object, and how the garbage collector will handle the propagation.\n</span><span class=\"c1\"></span><span class=\"kd\">type</span> <span class=\"nx\">DeletionPropagation</span> <span class=\"kt\">string</span>\n\n<span class=\"kd\">const</span> <span class=\"p\">(</span>\n    <span class=\"c1\">// Orphans the dependents.\n</span><span class=\"c1\"></span>    <span class=\"nx\">DeletePropagationOrphan</span> <span class=\"nx\">DeletionPropagation</span> <span class=\"p\">=</span> <span class=\"s\">&#34;Orphan&#34;</span>\n    <span class=\"c1\">// Deletes the object from the key-value store, the garbage collector will\n</span><span class=\"c1\"></span>    <span class=\"c1\">// delete the dependents in the background.\n</span><span class=\"c1\"></span>    <span class=\"nx\">DeletePropagationBackground</span> <span class=\"nx\">DeletionPropagation</span> <span class=\"p\">=</span> <span class=\"s\">&#34;Background&#34;</span>\n    <span class=\"c1\">// The object exists in the key-value store until the garbage collector\n</span><span class=\"c1\"></span>    <span class=\"c1\">// deletes all the dependents whose ownerReference.blockOwnerDeletion=true\n</span><span class=\"c1\"></span>    <span class=\"c1\">// from the key-value store.  API sever will put the &#34;foregroundDeletion&#34;\n</span><span class=\"c1\"></span>    <span class=\"c1\">// finalizer on the object, and sets its deletionTimestamp.  This policy is\n</span><span class=\"c1\"></span>    <span class=\"c1\">// cascading, i.e., the dependents will be deleted with Foreground.\n</span><span class=\"c1\"></span>    <span class=\"nx\">DeletePropagationForeground</span> <span class=\"nx\">DeletionPropagation</span> <span class=\"p\">=</span> <span class=\"s\">&#34;Foreground&#34;</span>\n<span class=\"p\">)</span></code></pre></div><p>再结合<code>OwnerReference.BlockOwnerDeletion</code> 参数的注释</p><div class=\"highlight\"><pre><code class=\"language-go\"><span class=\"c1\">// If true, AND if the owner has the &#34;foregroundDeletion&#34; finalizer, then\n</span><span class=\"c1\"></span>    <span class=\"c1\">// the owner cannot be deleted from the key-value store until this\n</span><span class=\"c1\"></span>    <span class=\"c1\">// reference is removed.\n</span><span class=\"c1\"></span>    <span class=\"c1\">// Defaults to false.\n</span><span class=\"c1\"></span>    <span class=\"c1\">// To set this field, a user needs &#34;delete&#34; permission of the owner,\n</span><span class=\"c1\"></span>    <span class=\"c1\">// otherwise 422 (Unprocessable Entity) will be returned.\n</span><span class=\"c1\"></span>    <span class=\"c1\">// +optional\n</span><span class=\"c1\"></span>    <span class=\"nx\">BlockOwnerDeletion</span> <span class=\"o\">*</span><span class=\"kt\">bool</span> <span class=\"s\">`json:&#34;blockOwnerDeletion,omitempty&#34; protobuf:&#34;varint,7,opt,name=blockOwnerDeletion&#34;`</span></code></pre></div><p>我们可以了解到。同步删除的开启方式如下：</p><ol><li>DeleteOptions.PropagationPolicy  =  DeletePropagationForeground</li><li>OwnerReference. BlockOwnerDeletion = True</li></ol><p>开启之后，在删除身份为 owner 的资源对象的时候，就会先将 denpendents 对象中 <code>OwnerReference.BlockOwnerDeletion</code>为 true 的资源对象先删除，然后再删除 owner 身份的对象。这里的「删除」就指的是我们前面说过的「真正的删除」：从 k8s 存储资源对象信息的 key-value 数据库中删除所有与其相关的信息。需要注意的是，<code>OwnerReference.BlockOwnerDeletion</code>为 false 的dependent 对象不会阻碍 owner 对象的删除操作。</p><p>Foreground 是 k8s 提供的两种级联删除方式其中之一，另外一种为 Background。通过上面相关的注释可以看到 Foreground 级联删除也是通过 Finalizer 来实现的，查看 Finalizer 相关的定义可知，标准的 Finalizer，一个是 orphan 的，另一个就是Foreground的：</p><div class=\"highlight\"><pre><code class=\"language-text\">// These are internal finalizer values for Kubernetes-like APIs, must be qualified name unless defined here\nconst (\n    FinalizerOrphanDependents string = &#34;orphan&#34;\n    FinalizerDeleteDependents string = &#34;foregroundDeletion&#34;\n)</code></pre></div><p>API-Server 的<code>Delete</code>函数，在接受到删除请求的时候，会检查 <code>DeleteOptions.PropagationPolicy</code>参数，若其值为<code>DeletePropagationForeground</code>, API-Server 随即会对该资源对象进行 Update 操作：</p><ol><li>插入<code>FinalizerDeleteDependents</code> Finalizer</li><li>设置<code>ObjectMeta.DeletionTimestamp</code>为当前值</li></ol><p>然后，在 GC 处理 owner 对象的 Update 事件的逻辑中，还会给 owner 对象打上一个「正在删除 dependents」 对象的标签。之后，我们会将 owner 对象管辖的 dependent 对象和他自己都加入到 dirtyQueue。dirtyQueue 的 worker 在处理 owner 对象的时候，会检查 owner 对象 「正在删除 dependents」的标签是否存在，如果仍有 dependent 对象没有被删掉，owner 会被轮询处理。而 dependent 对象将会被正常删除。当 dependent 对象相应的删除事件被 Propagator 感知到后，会将其从 DAG 和其 owner 的 dependents 信息中删除。几个循环之后，dependents 机会被删光，而 owner 对象中的 finalizer 和自身也会随之被删掉。</p><p>Background 模式的级联删除不会因 dependent 对象而影响 owner 对象的删除操作。当我们发送给 API-Server 删除一个 owner 身份的对象的请求之后，这个资源对象会立即被删除。它「管辖」的 dependent 对象会以「静默」的方式删除。</p><p><i>Q: What problems GC handles in k8s?</i></p><p><i>A:</i></p><p>通过对 k8s GC 设计文档的阅读，可以大致的概括一下：GC 主要是按照用户的需求来清理系统中「异常」的资源，用户可以自定义「清理方式」和「清理策略」。不难发现，在 GC 中，到底是保留一个资源还是删除一个资源都参照了资源之间的「从属关系」。资源的「从属关系」可以大致分为几个形态：</p><ol><li>无从属关系：这部分资源基本不会被 GC 做处理</li><li>有从属关系</li><ol><li>不符合用户预期：删除异常资源</li><li>符合用户预期：解绑异常资源之间的级联关系</li></ol></ol><p>在有从属关系的资源之间，即使被探测到关系异常，也并不代表一定要将他们都清除。如果有 Orphan Finalizer 的存在，可能某种「异常」正是用户想要的。所以，这就回到了我们一开始所说到的「清理策略」问题。GC 有一定的默认的清理策略，但是用户可以通过加入 Finalizer 的形式来修改「清理策略」，从而保持一个「符合用户期望」的资源之间的从属关系。</p><p>同时，用户可以还可以通过参数来制定特定的「清理方式」，如 Foreground 或者 Background。总体上来说，GC 的行为会受到如下几个因素的影响：</p><ul><li>默认：</li><ul><li>依据：资源之间默认的从属关系</li><li>行为：删除级联关系异常的资源</li><li>方式：Foreground 或者 Background</li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>可定制</li><ul><li>依据：用户定义的从属关系（通过 Finalizer）</li><li>行为：删除级联关系异常的资源</li><li>方式：Foreground 或者 Background</li></ul></ul><p>目前看来，GC 主要是解决了「资源清理」 的问题。那么再抽象一点来看的话，GC 解决的是「资源管理」这个大问题中的一个关于「清理」的小问题。既然说到「资源管理」，那么肯定就不止「清理」一个问题需要处理：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-98ec16ca74c686aca3525f471616006e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1998\" data-rawheight=\"1532\" class=\"origin_image zh-lightbox-thumb\" width=\"1998\" data-original=\"https://pic3.zhimg.com/v2-98ec16ca74c686aca3525f471616006e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1998&#39; height=&#39;1532&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1998\" data-rawheight=\"1532\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1998\" data-original=\"https://pic3.zhimg.com/v2-98ec16ca74c686aca3525f471616006e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-98ec16ca74c686aca3525f471616006e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>其中，对于资源的创建部分，除了正常的新建操作之外，controller 还有定期执行一个「Adoption」 的操作，用来维护其创建的资源之间那些「本应该建立但是却断开的从属关系」。而对于更新操作来说，controller_manager 需要处理用户对于资源的扩缩容请求，如将 deployment.replicaset.replicacount 减少或者增大，相应资源对应的 controller 需要对可见资源的数量进行调整。至于「资源超卖」的问题，一定会涉及到 scheduler。因为物理资源是固定的，「超卖」本质上来说就是按照实时的需求，动态的调整服务所在的 Node，以便恰好满足服务对于资源的需求。</p><p>如果不把资源管理问题讨论的范围局限在 k8s 中的话，那么「审计」和「复用」同样也是「资源管理」问题中不得不考虑的两个点。前者可以增加整个系统资源的「可控性」，后者则可以最大限度的提升资源的利用率，从而降低成本。其实「复用」和「超卖」的目的是一样的，都是想最大限度的利用物理资源。不过这两个功能笔者暂时还没有去查看它们是否在 k8s 已经实现。</p><h2>总结</h2><p>之所以去了解 k8s 的 GC，是因为在将 k8s 集群从1.7版本升级至1.9版本的过程中，因为我错误的设置了资源之间的从属关系，导致该资源被 GC 给回收掉了。问题在1.7版本没有出现的原因是那时 k8s 还没有支持对自定义资源（CRD）的级联删除。通过对 GC 设计理念的了解，我们可以初步的感受到 k8s 对于「资源管理」这个问题域中「资源清理」这个小问题的解决思路。以此为起点，我们可以顺藤摸瓜，去观察 k8s 对于「资源管理」问题域中的其他难题是如何处理的。后续，我也将会根据设计文档中的思路，在代码级别上去了解 GC 的实现细节，从而贡献出更加详细的 blog。</p>", 
            "topic": [
                {
                    "tag": "Kubernetes", 
                    "tagLink": "https://api.zhihu.com/topics/20018384"
                }, 
                {
                    "tag": "GC垃圾回收（计算机科学）", 
                    "tagLink": "https://api.zhihu.com/topics/19614999"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/47683490", 
            "userName": "要没时间了", 
            "userLink": "https://www.zhihu.com/people/bdbe7f88af9a021c5c770649d266f143", 
            "upvote": 14, 
            "title": "Docker 镜像分层机制与 AUFS（Advanced Union File System）", 
            "content": "<h2>Docker Image 和 AUFS 是什么关系？</h2><p>Image 是 Docker 部署的基本单位，一个 Image 运行在一个 Docker Container 上面。这个 Image 包含了我们的程序文件，以及这个程序依赖的资源的环境。Docker Image 对外是以一个文件的形式展示的（更准确的说是一个 mount 点）。</p><p>在 Linux 内核 4.0以及之前的版本上（主要是 Ubuntu 和 Debian），Docker 使用 AUFS 来管理 Docker Image 的存储。虽然，在一些新的 Docker 版本中，已经使用了其他不同的方案来管理镜像，如 DeviceMapper，overlay2。但是 AUFS 是一个比较标准且简单的实现方式，通过 AUFS 来了解 Docker Image 的原理是一个不错的选择。</p><h2>什么是 AUFS？</h2><p>AUFS 是 Union File System 众多实现方式的一种。Union File System 从字面意思上来理解就是「联合文件系统」。它将多个物理位置不同的文件目录「联合」起来，挂载到某一个目录下，形成一个抽象的文件系统。</p><p>概念理解起来比较枯燥，最好是有一个真实的例子来帮助我们理解：</p><p>首先，我们建立 company 和 home 两个目录，并且分别为他们创造两个文件</p><div class=\"highlight\"><pre><code class=\"language-c\"><span class=\"n\">root</span><span class=\"err\">@</span><span class=\"n\">rds</span><span class=\"o\">-</span><span class=\"n\">k8s</span><span class=\"o\">-</span><span class=\"mi\">18</span><span class=\"o\">-</span><span class=\"nl\">svr0</span><span class=\"p\">:</span><span class=\"o\">~/</span><span class=\"n\">xuran</span><span class=\"o\">/</span><span class=\"n\">aufs</span><span class=\"cp\"># tree .\n</span><span class=\"cp\"></span><span class=\"p\">.</span>\n<span class=\"o\">|--</span> <span class=\"n\">company</span>\n<span class=\"o\">|</span>   <span class=\"o\">|--</span> <span class=\"n\">code</span>\n<span class=\"o\">|</span>   <span class=\"err\">`</span><span class=\"o\">--</span> <span class=\"n\">meeting</span>\n<span class=\"err\">`</span><span class=\"o\">--</span> <span class=\"n\">home</span>\n    <span class=\"o\">|--</span> <span class=\"n\">eat</span>\n    <span class=\"err\">`</span><span class=\"o\">--</span> <span class=\"n\">sleep</span></code></pre></div><p>然后我们将通过 mount 命令把 company 和 home 两个目录「联合」起来，建立一个 AUFS 的文件系统，并挂载到当前目录下的 mnt 目录下：</p><div class=\"highlight\"><pre><code class=\"language-c\"><span class=\"n\">oot</span><span class=\"err\">@</span><span class=\"n\">rds</span><span class=\"o\">-</span><span class=\"n\">k8s</span><span class=\"o\">-</span><span class=\"mi\">18</span><span class=\"o\">-</span><span class=\"nl\">svr0</span><span class=\"p\">:</span><span class=\"o\">~/</span><span class=\"n\">xuran</span><span class=\"o\">/</span><span class=\"n\">aufs</span><span class=\"cp\"># mkdir mnt\n</span><span class=\"cp\"></span><span class=\"n\">root</span><span class=\"err\">@</span><span class=\"n\">rds</span><span class=\"o\">-</span><span class=\"n\">k8s</span><span class=\"o\">-</span><span class=\"mi\">18</span><span class=\"o\">-</span><span class=\"nl\">svr0</span><span class=\"p\">:</span><span class=\"o\">~/</span><span class=\"n\">xuran</span><span class=\"o\">/</span><span class=\"n\">aufs</span><span class=\"cp\"># ll\n</span><span class=\"cp\"></span><span class=\"n\">total</span> <span class=\"mi\">20</span>\n<span class=\"n\">drwxr</span><span class=\"o\">-</span><span class=\"n\">xr</span><span class=\"o\">-</span><span class=\"n\">x</span> <span class=\"mi\">5</span> <span class=\"n\">root</span> <span class=\"n\">root</span> <span class=\"mi\">4096</span> <span class=\"n\">Oct</span> <span class=\"mi\">25</span> <span class=\"mi\">16</span><span class=\"o\">:</span><span class=\"mi\">10</span> <span class=\"p\">.</span><span class=\"o\">/</span>\n<span class=\"n\">drwxr</span><span class=\"o\">-</span><span class=\"n\">xr</span><span class=\"o\">-</span><span class=\"n\">x</span> <span class=\"mi\">5</span> <span class=\"n\">root</span> <span class=\"n\">root</span> <span class=\"mi\">4096</span> <span class=\"n\">Oct</span> <span class=\"mi\">25</span> <span class=\"mi\">16</span><span class=\"o\">:</span><span class=\"mo\">06</span> <span class=\"p\">..</span><span class=\"o\">/</span>\n<span class=\"n\">drwxr</span><span class=\"o\">-</span><span class=\"n\">xr</span><span class=\"o\">-</span><span class=\"n\">x</span> <span class=\"mi\">4</span> <span class=\"n\">root</span> <span class=\"n\">root</span> <span class=\"mi\">4096</span> <span class=\"n\">Oct</span> <span class=\"mi\">25</span> <span class=\"mi\">16</span><span class=\"o\">:</span><span class=\"mo\">06</span> <span class=\"n\">company</span><span class=\"o\">/</span>\n<span class=\"n\">drwxr</span><span class=\"o\">-</span><span class=\"n\">xr</span><span class=\"o\">-</span><span class=\"n\">x</span> <span class=\"mi\">4</span> <span class=\"n\">root</span> <span class=\"n\">root</span> <span class=\"mi\">4096</span> <span class=\"n\">Oct</span> <span class=\"mi\">25</span> <span class=\"mi\">16</span><span class=\"o\">:</span><span class=\"mo\">05</span> <span class=\"n\">home</span><span class=\"o\">/</span>\n<span class=\"n\">drwxr</span><span class=\"o\">-</span><span class=\"n\">xr</span><span class=\"o\">-</span><span class=\"n\">x</span> <span class=\"mi\">2</span> <span class=\"n\">root</span> <span class=\"n\">root</span> <span class=\"mi\">4096</span> <span class=\"n\">Oct</span> <span class=\"mi\">25</span> <span class=\"mi\">16</span><span class=\"o\">:</span><span class=\"mi\">10</span> <span class=\"n\">mnt</span><span class=\"o\">/</span>\n<span class=\"n\">root</span><span class=\"err\">@</span><span class=\"n\">rds</span><span class=\"o\">-</span><span class=\"n\">k8s</span><span class=\"o\">-</span><span class=\"mi\">18</span><span class=\"o\">-</span><span class=\"nl\">svr0</span><span class=\"p\">:</span><span class=\"o\">~/</span><span class=\"n\">xuran</span><span class=\"o\">/</span><span class=\"n\">aufs</span><span class=\"cp\"># mount -t aufs -o dirs=./home:./company none ./mnt\n</span><span class=\"cp\"></span><span class=\"n\">root</span><span class=\"err\">@</span><span class=\"n\">rds</span><span class=\"o\">-</span><span class=\"n\">k8s</span><span class=\"o\">-</span><span class=\"mi\">18</span><span class=\"o\">-</span><span class=\"nl\">svr0</span><span class=\"p\">:</span><span class=\"o\">~/</span><span class=\"n\">xuran</span><span class=\"o\">/</span><span class=\"n\">aufs</span><span class=\"cp\"># ll\n</span><span class=\"cp\"></span><span class=\"n\">total</span> <span class=\"mi\">20</span>\n<span class=\"n\">drwxr</span><span class=\"o\">-</span><span class=\"n\">xr</span><span class=\"o\">-</span><span class=\"n\">x</span> <span class=\"mi\">5</span> <span class=\"n\">root</span> <span class=\"n\">root</span> <span class=\"mi\">4096</span> <span class=\"n\">Oct</span> <span class=\"mi\">25</span> <span class=\"mi\">16</span><span class=\"o\">:</span><span class=\"mi\">10</span> <span class=\"p\">.</span><span class=\"o\">/</span>\n<span class=\"n\">drwxr</span><span class=\"o\">-</span><span class=\"n\">xr</span><span class=\"o\">-</span><span class=\"n\">x</span> <span class=\"mi\">5</span> <span class=\"n\">root</span> <span class=\"n\">root</span> <span class=\"mi\">4096</span> <span class=\"n\">Oct</span> <span class=\"mi\">25</span> <span class=\"mi\">16</span><span class=\"o\">:</span><span class=\"mo\">06</span> <span class=\"p\">..</span><span class=\"o\">/</span>\n<span class=\"n\">drwxr</span><span class=\"o\">-</span><span class=\"n\">xr</span><span class=\"o\">-</span><span class=\"n\">x</span> <span class=\"mi\">4</span> <span class=\"n\">root</span> <span class=\"n\">root</span> <span class=\"mi\">4096</span> <span class=\"n\">Oct</span> <span class=\"mi\">25</span> <span class=\"mi\">16</span><span class=\"o\">:</span><span class=\"mo\">06</span> <span class=\"n\">company</span><span class=\"o\">/</span>\n<span class=\"n\">drwxr</span><span class=\"o\">-</span><span class=\"n\">xr</span><span class=\"o\">-</span><span class=\"n\">x</span> <span class=\"mi\">6</span> <span class=\"n\">root</span> <span class=\"n\">root</span> <span class=\"mi\">4096</span> <span class=\"n\">Oct</span> <span class=\"mi\">25</span> <span class=\"mi\">16</span><span class=\"o\">:</span><span class=\"mi\">10</span> <span class=\"n\">home</span><span class=\"o\">/</span>\n<span class=\"n\">drwxr</span><span class=\"o\">-</span><span class=\"n\">xr</span><span class=\"o\">-</span><span class=\"n\">x</span> <span class=\"mi\">8</span> <span class=\"n\">root</span> <span class=\"n\">root</span> <span class=\"mi\">4096</span> <span class=\"n\">Oct</span> <span class=\"mi\">25</span> <span class=\"mi\">16</span><span class=\"o\">:</span><span class=\"mi\">10</span> <span class=\"n\">mnt</span><span class=\"o\">/</span>\n<span class=\"n\">root</span><span class=\"err\">@</span><span class=\"n\">rds</span><span class=\"o\">-</span><span class=\"n\">k8s</span><span class=\"o\">-</span><span class=\"mi\">18</span><span class=\"o\">-</span><span class=\"nl\">svr0</span><span class=\"p\">:</span><span class=\"o\">~/</span><span class=\"n\">xuran</span><span class=\"o\">/</span><span class=\"n\">aufs</span><span class=\"cp\"># tree ./mnt/\n</span><span class=\"cp\"></span><span class=\"p\">.</span><span class=\"o\">/</span><span class=\"n\">mnt</span><span class=\"o\">/</span>\n<span class=\"o\">|--</span> <span class=\"n\">code</span>\n<span class=\"o\">|--</span> <span class=\"n\">eat</span>\n<span class=\"o\">|--</span> <span class=\"n\">meeting</span>\n<span class=\"err\">`</span><span class=\"o\">--</span> <span class=\"n\">sleep</span>\n\n<span class=\"mi\">4</span> <span class=\"n\">directories</span><span class=\"p\">,</span> <span class=\"mi\">0</span> <span class=\"n\">files</span></code></pre></div><p>通过 ./mnt 目录结构的输出结果，可以看到原来两个目录下的内容都被合并到了一个 mnt 的目录下。</p><p>默认情况下，如果我们不对「联合」的目录指定权限，内核将根据从左至右的顺序将第一个目录指定为可读可写的，其余的都为只读。那么，当我们向只读的目录做一些写入操作的话，会发生什么呢？</p><div class=\"highlight\"><pre><code class=\"language-c\"><span class=\"n\">root</span><span class=\"err\">@</span><span class=\"n\">rds</span><span class=\"o\">-</span><span class=\"n\">k8s</span><span class=\"o\">-</span><span class=\"mi\">18</span><span class=\"o\">-</span><span class=\"nl\">svr0</span><span class=\"p\">:</span><span class=\"o\">~/</span><span class=\"n\">xuran</span><span class=\"o\">/</span><span class=\"n\">aufs</span><span class=\"cp\"># echo apple &gt; ./mnt/code\n</span><span class=\"cp\"></span><span class=\"n\">root</span><span class=\"err\">@</span><span class=\"n\">rds</span><span class=\"o\">-</span><span class=\"n\">k8s</span><span class=\"o\">-</span><span class=\"mi\">18</span><span class=\"o\">-</span><span class=\"nl\">svr0</span><span class=\"p\">:</span><span class=\"o\">~/</span><span class=\"n\">xuran</span><span class=\"o\">/</span><span class=\"n\">aufs</span><span class=\"cp\"># cat company/code\n</span><span class=\"cp\"></span><span class=\"n\">root</span><span class=\"err\">@</span><span class=\"n\">rds</span><span class=\"o\">-</span><span class=\"n\">k8s</span><span class=\"o\">-</span><span class=\"mi\">18</span><span class=\"o\">-</span><span class=\"nl\">svr0</span><span class=\"p\">:</span><span class=\"o\">~/</span><span class=\"n\">xuran</span><span class=\"o\">/</span><span class=\"n\">aufs</span><span class=\"cp\"># cat home/code\n</span><span class=\"cp\"></span><span class=\"n\">apple</span>\n<span class=\"n\">root</span><span class=\"err\">@</span><span class=\"n\">rds</span><span class=\"o\">-</span><span class=\"n\">k8s</span><span class=\"o\">-</span><span class=\"mi\">18</span><span class=\"o\">-</span><span class=\"nl\">svr0</span><span class=\"p\">:</span><span class=\"o\">~/</span><span class=\"n\">xuran</span><span class=\"o\">/</span><span class=\"n\">aufs</span><span class=\"cp\">#</span></code></pre></div><p>通过对上面代码段的观察，我们可以看出，当写入操作发生在 company/code 文件时， 对应的修改并没有反映到原始的目录中。而是在 home 目录下又创建了一个名为 code 的文件，并将 apple 写入了进去。</p><p>看起来很奇怪的现象，其实这正是 Union File System 的厉害之处：Union File System 联合了多个不同的目录，并且把他们挂载到一个统一的目录上。在这些「联合」的子目录中， 有一部分是可读可写的，但是有一部分只是可读的。当你对可读的目录内容做出修改的时候，其结果只会保存到可写的目录下，不会影响只读的目录。比如，我们可以把我们的服务的源代码目录和一个存放代码修改记录的目录「联合」起来构成一个 AUFS。前者设置只读权限，后者设置读写权限。那么，一切对源代码目录下文件的修改都只会影响那个存放修改的目录，不会污染原始的代码。</p><p>对于被 AUFS 「联合」的目录来说，在「联合」之后，它们在挂载点下面的目录权限可能有以下几种：</p><ul><li>rw：可读可写，用户能直接修改这个 branch 的文件内容</li><li>ro：只读，用户不能通过 aufs 的接口对文件进行写操作，只能读取里面的内容</li><li>rr：real read only，底层的文件本来就是只读的（这种情况比较少见），这种情况下，aufs 就不用担心文件不通过它的接口被修改的情况</li></ul><p>但是，上面这些权限都仅仅是对于「联合」的目录来说的。实际上，我们仍然可以绕过这些目录而去直接更改「源目录」。当这种情况发生的时候，AUFS 所管理的联合目录会对一些修改有相应的反馈么？</p><p>对于这个问题，AUFS 是通过在「联合」目录的时候传递一个特殊的参数来控制的：udba。 udba 一共有三种可能的取值：</p><ul><li>none：aufs 不会进行任何数据同步的检查，所以性能会比其他两种方式要高，但是可能会出现数据不一致的情况。</li><li>reval：aufs 会检查底层的文件有没有改动，如果有的话，把改动的内容更新到挂载点。这个性能会导致 aufs 产生额外的性能损耗</li><li>notify：通过 inotify 监听底层的文件变化，基于事件驱动，能够减少第二种方式的性能损耗</li></ul><p>除了上面说到的这些，在 AUFS 中还有一个特殊的概念需要提及一下：</p><ol><li>branch – 就是各个要被union起来的目录。它会根据 Union 的顺序形成一个 Stack 的结构，从下至上，最上面的目录是可读写的，其余都是可读的。如果按照我们刚刚执行 aufs 挂载的命令来说，最左侧的目录就对应 Stack 最顶层的 branch。</li></ol><h2>什么是 Docker 镜像分层机制？</h2><p>首先，让我们来看下 Docker Image 中的 Layer 的概念：</p><div class=\"highlight\"><pre><code class=\"language-text\">Definition of: layer\nIn an image, a layer is modification to the image, represented by an instruction in the Dockerfile. Layers are applied in sequence to the base image to create the final image. When an image is updated or rebuilt, only layers that change need to be updated, and unchanged layers are cached locally. This is part of why Docker images are so fast and lightweight. The sizes of each layer add up to equal the size of the final image.</code></pre></div><p>简单来说，一个 Image 是通过一个 DockerFile 定义的，然后使用 docker build 命令构建它。DockerFile 中的每一条命令的执行结果都会成为 Image 中的一个 Layer。Docker Image 是有一个层级结构的，最底层的 Layer 为 BaseImage（一般为一个操作系统的 ISO 镜像），然后顺序执行每一条指令，生成的 Layer 按照入栈的顺序逐渐累加，最终形成一个 Image。如果 DockerFile 中的内容没有变动，那么相应的镜像在 build 的时候会复用之前的 layer，以便提升构建效率。并且，即使文件内容有修改，那也只会重新 build 修改的 layer，其他未修改的也仍然会复用。</p><p>通过了解了 Docker Image 的分层机制，我们多多少少能够感觉到，Layer 和 Image 的关系与 AUFS 中的联合目录和挂载点的关系比较相似。而 Docker 也正是通过 AUFS 来管理 Images 的。</p><p>这里，我们通过 Build 一个镜像，来观察 Image 的分层机制：</p><p>Dockerfile:</p><div class=\"highlight\"><pre><code class=\"language-text\"># Use an official Python runtime as a parent image\nFROM python:2.7-slim\n\n# Set the working directory to /app\nWORKDIR /app\n\n# Copy the current directory contents into the container at /app\nCOPY . /app\n\n# Install any needed packages specified in requirements.txt\nRUN pip install --trusted-host pypi.python.org -r requirements.txt\n\n# Make port 80 available to the world outside this container\nEXPOSE 80\n\n# Define environment variable\nENV NAME World\n\n# Run app.py when the container launches\nCMD [&#34;python&#34;, &#34;app.py&#34;]</code></pre></div><p>构建结果：</p><div class=\"highlight\"><pre><code class=\"language-c\"><span class=\"n\">root</span><span class=\"err\">@</span><span class=\"n\">rds</span><span class=\"o\">-</span><span class=\"n\">k8s</span><span class=\"o\">-</span><span class=\"mi\">18</span><span class=\"o\">-</span><span class=\"nl\">svr0</span><span class=\"p\">:</span><span class=\"o\">~/</span><span class=\"n\">xuran</span><span class=\"o\">/</span><span class=\"n\">exampleimage</span><span class=\"cp\"># docker build -t hello ./\n</span><span class=\"cp\"></span><span class=\"n\">Sending</span> <span class=\"n\">build</span> <span class=\"n\">context</span> <span class=\"n\">to</span> <span class=\"n\">Docker</span> <span class=\"n\">daemon</span>  <span class=\"mf\">5.12</span> <span class=\"n\">kB</span>\n<span class=\"n\">Step</span> <span class=\"mi\">1</span><span class=\"o\">/</span><span class=\"mi\">7</span> <span class=\"o\">:</span> <span class=\"n\">FROM</span> <span class=\"nl\">python</span><span class=\"p\">:</span><span class=\"mf\">2.7</span><span class=\"o\">-</span><span class=\"n\">slim</span>\n <span class=\"o\">---&gt;</span> <span class=\"mi\">804</span><span class=\"n\">b0a01ea83</span>\n<span class=\"n\">Step</span> <span class=\"mi\">2</span><span class=\"o\">/</span><span class=\"mi\">7</span> <span class=\"o\">:</span> <span class=\"n\">WORKDIR</span> <span class=\"o\">/</span><span class=\"n\">app</span>\n <span class=\"o\">---&gt;</span> <span class=\"n\">Using</span> <span class=\"n\">cache</span>\n <span class=\"o\">---&gt;</span> <span class=\"mi\">6</span><span class=\"n\">d93c5b91703</span>\n<span class=\"n\">Step</span> <span class=\"mi\">3</span><span class=\"o\">/</span><span class=\"mi\">7</span> <span class=\"o\">:</span> <span class=\"n\">COPY</span> <span class=\"p\">.</span> <span class=\"o\">/</span><span class=\"n\">app</span>\n <span class=\"o\">---&gt;</span> <span class=\"n\">Using</span> <span class=\"n\">cache</span>\n <span class=\"o\">---&gt;</span> <span class=\"n\">feddc82d321b</span>\n<span class=\"n\">Step</span> <span class=\"mi\">4</span><span class=\"o\">/</span><span class=\"mi\">7</span> <span class=\"o\">:</span> <span class=\"n\">RUN</span> <span class=\"n\">pip</span> <span class=\"n\">install</span> <span class=\"o\">--</span><span class=\"n\">trusted</span><span class=\"o\">-</span><span class=\"n\">host</span> <span class=\"n\">pypi</span><span class=\"p\">.</span><span class=\"n\">python</span><span class=\"p\">.</span><span class=\"n\">org</span> <span class=\"o\">-</span><span class=\"n\">r</span> <span class=\"n\">requirements</span><span class=\"p\">.</span><span class=\"n\">txt</span>\n <span class=\"o\">---&gt;</span> <span class=\"n\">Using</span> <span class=\"n\">cache</span>\n <span class=\"o\">---&gt;</span> <span class=\"mi\">94695</span><span class=\"n\">df5e14d</span>\n<span class=\"n\">Step</span> <span class=\"mi\">5</span><span class=\"o\">/</span><span class=\"mi\">7</span> <span class=\"o\">:</span> <span class=\"n\">EXPOSE</span> <span class=\"mi\">81</span>\n <span class=\"o\">---&gt;</span> <span class=\"n\">Using</span> <span class=\"n\">cache</span>\n <span class=\"o\">---&gt;</span> <span class=\"mi\">43</span><span class=\"n\">c392d51dff</span>\n<span class=\"n\">Step</span> <span class=\"mi\">6</span><span class=\"o\">/</span><span class=\"mi\">7</span> <span class=\"o\">:</span> <span class=\"n\">ENV</span> <span class=\"n\">NAME</span> <span class=\"n\">World</span>\n <span class=\"o\">---&gt;</span> <span class=\"n\">Using</span> <span class=\"n\">cache</span>\n <span class=\"o\">---&gt;</span> <span class=\"mi\">78</span><span class=\"n\">c9a60237c8</span>\n<span class=\"n\">Step</span> <span class=\"mi\">7</span><span class=\"o\">/</span><span class=\"mi\">7</span> <span class=\"o\">:</span> <span class=\"n\">CMD</span> <span class=\"n\">python</span> <span class=\"n\">app</span><span class=\"p\">.</span><span class=\"n\">py</span>\n <span class=\"o\">---&gt;</span> <span class=\"n\">Using</span> <span class=\"n\">cache</span>\n <span class=\"o\">---&gt;</span> <span class=\"n\">a5ccd4e1b15d</span>\n<span class=\"n\">Successfully</span> <span class=\"n\">built</span> <span class=\"n\">a5ccd4e1b15d</span></code></pre></div><p>通过构建结果可以看出，构建的过程就是执行 Dockerfile 文件中我们写入的命令。构建一共进行了7个步骤，每个步骤进行完都会生成一个随机的 ID，来标识这一 layer 中的内容。 最后一行的 a5ccd4e1b15d 为镜像的 ID。由于我贴上来的构建过程已经是构建了第二次的结果了，所以可以看出，对于没有任何修改的内容，Docker 会复用之前的结果。</p><h2>Docker 和 AUFS 是如何联系起来的？</h2><p>和 Docker 有关的 AUFS 的内容，都在<code>/var/lib/docker/aufs</code>目录下。其实，AUFS 不仅会帮忙管理 Docker Image，还会帮我们管理运行中的容器。aufs的目录的结构一般如下所示：</p><div class=\"highlight\"><pre><code class=\"language-text\">root@rds-k8s-18-svr0:/var/lib/docker/aufs# tree -L 1\n.\n├── diff\n├── layers\n└── mnt</code></pre></div><p>其中 diff 存放的是每一个 Image 中 Layers 的内容， 而 layer 目录则是保存的每一个镜像的 layer 的结构信息。mnt 目录下的内容，其实就是一个镜像或者容器运行起来之后，使用的 AUFS 的挂载点。可以认为它就是你在容器内部所看到的文件系统。通过这三个子目录，docker 就能实现镜像的分层存储、容器的 Copy-On-Write 启动。</p><p>通过 <code>docker inspect hello</code>命令，我们可以看到之前构建的镜像的 Layer 信息：</p><div class=\"highlight\"><pre><code class=\"language-c\"><span class=\"err\">…</span>\n        <span class=\"s\">&#34;RootFS&#34;</span><span class=\"o\">:</span> <span class=\"p\">{</span>\n            <span class=\"s\">&#34;Type&#34;</span><span class=\"o\">:</span> <span class=\"s\">&#34;layers&#34;</span><span class=\"p\">,</span>\n            <span class=\"s\">&#34;Layers&#34;</span><span class=\"o\">:</span> <span class=\"p\">[</span>\n                <span class=\"s\">&#34;sha256:237472299760d6726d376385edd9e79c310fe91d794bc9870d038417d448c2d5&#34;</span><span class=\"p\">,</span>\n                <span class=\"s\">&#34;sha256:9ff579683928293838edf785162e11d7362bdeadd1b91a913d0d777f07a0c14b&#34;</span><span class=\"p\">,</span>\n                <span class=\"s\">&#34;sha256:473e9a98e4dd51cc459336e2e411eef27ceeb35c4698b2906d1473c155fbb620&#34;</span><span class=\"p\">,</span>\n                <span class=\"s\">&#34;sha256:639216b11d4ff9961788a9198e663ae74db047fb720049fe1d84f23b5b5dc9d6&#34;</span><span class=\"p\">,</span>\n                <span class=\"s\">&#34;sha256:c91ccb170b062fc1de577b156c9f16c6923d5a8ab177861e3a4b67aec53e8f13&#34;</span><span class=\"p\">,</span>\n                <span class=\"s\">&#34;sha256:d2a243c9257f3caa11f31057472b86eeb91ebd4de14c61293cde71823395a187&#34;</span><span class=\"p\">,</span>\n                <span class=\"s\">&#34;sha256:0c696b8e58cb0ce8528268fdb480f95ec3f005edc39caef6a31c76a8381e6825&#34;</span>\n            <span class=\"p\">]</span>\n        <span class=\"p\">}</span>\n<span class=\"err\">…</span></code></pre></div><p>Dockerfile 中一共有7个命令，所以layer 信息的数量也同样有7个。这个7个 layer，每一个 layer 都对应到 /var/lib/docker/aufs/diff 目录下的一个目录。但是上述代码段中的 sha256 ID 不是直接映射到 /var/lib/docker/aufs/diff 目录下的目录名的，这之间有一个比较复杂的映射关系。（由于这个映射关系比较复杂，且难以描述，所以这里暂不详细叙述。读者也不用过于纠结这里的映射关系，重点在于理解 AUFS 和镜像分层的工作机制）。</p><p>接下来，我们运行一下刚刚构建好的镜像：</p><div class=\"highlight\"><pre><code class=\"language-text\">root@rds-k8s-18-svr0:~/xuran/exampleimage# docker run -it hello bash\nroot@rds-k8s-18-svr0:~# docker ps | grep hello\n9d040e5b9999        hello                                                                                                                       &#34;bash&#34;                   7 minutes ago       Up 7 minutes        81/tcp              laughing_northcutt</code></pre></div><p>通过 <code>docker ps</code>命令，可以看到当前运行的容器进程 ID 为 9d040e5b9999。按照上面所描述的，一个容器启动之后，会在 aufs/mnt 下挂载一个目录 /var/lib/docker/aufs/mnt/755ee5904e25f223a5393cc9202e4daf0ffc9770f67715c966e939eeedf358d4，这个目录就是容器当中看到的文件系统。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-7efafa887a733534d38eaf9ca14f0e4a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"983\" data-rawheight=\"478\" class=\"origin_image zh-lightbox-thumb\" width=\"983\" data-original=\"https://pic3.zhimg.com/v2-7efafa887a733534d38eaf9ca14f0e4a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;983&#39; height=&#39;478&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"983\" data-rawheight=\"478\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"983\" data-original=\"https://pic3.zhimg.com/v2-7efafa887a733534d38eaf9ca14f0e4a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-7efafa887a733534d38eaf9ca14f0e4a_b.jpg\"/></figure><p>在通过 mount 命令观察系统中的 AUFS 文件系统挂载情况时可以发现，每一条挂载记录后面都跟了一个sid:</p><div class=\"highlight\"><pre><code class=\"language-c\"><span class=\"n\">none</span> <span class=\"o\">/</span><span class=\"n\">var</span><span class=\"o\">/</span><span class=\"n\">lib</span><span class=\"o\">/</span><span class=\"n\">docker</span><span class=\"o\">/</span><span class=\"n\">aufs</span><span class=\"o\">/</span><span class=\"n\">mnt</span><span class=\"o\">/</span><span class=\"mi\">755</span><span class=\"n\">ee5904e25f223a5393cc9202e4daf0ffc9770f67715c966e939eeedf358d4</span> <span class=\"n\">aufs</span> <span class=\"n\">rw</span><span class=\"p\">,</span><span class=\"n\">relatime</span><span class=\"p\">,</span><span class=\"n\">si</span><span class=\"o\">=</span><span class=\"n\">fcff2ee8dd687806</span><span class=\"p\">,</span><span class=\"n\">dio</span><span class=\"p\">,</span><span class=\"n\">dirperm1</span> <span class=\"mi\">0</span> <span class=\"mi\">0</span></code></pre></div><p>hello 这个容器挂载的 AUFS 的 SID 为 fcff2ee8dd687806。我们将通过这个 ID，可以在/sys/fs/aufs 的目录下找到对应的 AUFS  的详细信息：</p><div class=\"highlight\"><pre><code class=\"language-c\"><span class=\"n\">root</span><span class=\"err\">@</span><span class=\"n\">rds</span><span class=\"o\">-</span><span class=\"n\">k8s</span><span class=\"o\">-</span><span class=\"mi\">18</span><span class=\"o\">-</span><span class=\"nl\">svr0</span><span class=\"p\">:</span><span class=\"o\">/</span><span class=\"n\">sys</span><span class=\"o\">/</span><span class=\"n\">fs</span><span class=\"o\">/</span><span class=\"n\">aufs</span><span class=\"o\">/</span><span class=\"n\">si_fcff2ee8dd687806</span><span class=\"cp\"># cat br[0-9]*\n</span><span class=\"cp\"></span><span class=\"o\">/</span><span class=\"n\">var</span><span class=\"o\">/</span><span class=\"n\">lib</span><span class=\"o\">/</span><span class=\"n\">docker</span><span class=\"o\">/</span><span class=\"n\">aufs</span><span class=\"o\">/</span><span class=\"n\">diff</span><span class=\"o\">/</span><span class=\"mi\">755</span><span class=\"n\">ee5904e25f223a5393cc9202e4daf0ffc9770f67715c966e939eeedf358d4</span><span class=\"o\">=</span><span class=\"n\">rw</span>\n<span class=\"o\">/</span><span class=\"n\">var</span><span class=\"o\">/</span><span class=\"n\">lib</span><span class=\"o\">/</span><span class=\"n\">docker</span><span class=\"o\">/</span><span class=\"n\">aufs</span><span class=\"o\">/</span><span class=\"n\">diff</span><span class=\"o\">/</span><span class=\"mi\">755</span><span class=\"n\">ee5904e25f223a5393cc9202e4daf0ffc9770f67715c966e939eeedf358d4</span><span class=\"o\">-</span><span class=\"n\">init</span><span class=\"o\">=</span><span class=\"n\">ro</span><span class=\"o\">+</span><span class=\"n\">wh</span>\n<span class=\"o\">/</span><span class=\"n\">var</span><span class=\"o\">/</span><span class=\"n\">lib</span><span class=\"o\">/</span><span class=\"n\">docker</span><span class=\"o\">/</span><span class=\"n\">aufs</span><span class=\"o\">/</span><span class=\"n\">diff</span><span class=\"o\">/</span><span class=\"mf\">678e5401</span><span class=\"n\">f758378d36dbc6f410c9324f9c5599db498aea5909b83d6a974af446</span><span class=\"o\">=</span><span class=\"n\">ro</span><span class=\"o\">+</span><span class=\"n\">wh</span>\n<span class=\"o\">/</span><span class=\"n\">var</span><span class=\"o\">/</span><span class=\"n\">lib</span><span class=\"o\">/</span><span class=\"n\">docker</span><span class=\"o\">/</span><span class=\"n\">aufs</span><span class=\"o\">/</span><span class=\"n\">diff</span><span class=\"o\">/</span><span class=\"mf\">81e4703</span><span class=\"n\">c9d06f502774f8fb9af8734ac222af2c4c3aa65813e7bd73ad23f33cb</span><span class=\"o\">=</span><span class=\"n\">ro</span><span class=\"o\">+</span><span class=\"n\">wh</span>\n<span class=\"o\">/</span><span class=\"n\">var</span><span class=\"o\">/</span><span class=\"n\">lib</span><span class=\"o\">/</span><span class=\"n\">docker</span><span class=\"o\">/</span><span class=\"n\">aufs</span><span class=\"o\">/</span><span class=\"n\">diff</span><span class=\"o\">/</span><span class=\"mi\">877</span><span class=\"n\">a44a6fdba1f633b7051b583ee3cc1953dac242cbf8ff2482b59e782fb2c32</span><span class=\"o\">=</span><span class=\"n\">ro</span><span class=\"o\">+</span><span class=\"n\">wh</span>\n<span class=\"o\">/</span><span class=\"n\">var</span><span class=\"o\">/</span><span class=\"n\">lib</span><span class=\"o\">/</span><span class=\"n\">docker</span><span class=\"o\">/</span><span class=\"n\">aufs</span><span class=\"o\">/</span><span class=\"n\">diff</span><span class=\"o\">/</span><span class=\"mi\">309</span><span class=\"n\">ae284b55e1b4d5238ede32dc6fa197982a9f65c34b78c2091b846d9d089d6</span><span class=\"o\">=</span><span class=\"n\">ro</span><span class=\"o\">+</span><span class=\"n\">wh</span>\n<span class=\"o\">/</span><span class=\"n\">var</span><span class=\"o\">/</span><span class=\"n\">lib</span><span class=\"o\">/</span><span class=\"n\">docker</span><span class=\"o\">/</span><span class=\"n\">aufs</span><span class=\"o\">/</span><span class=\"n\">diff</span><span class=\"o\">/</span><span class=\"mf\">83507e141977</span><span class=\"n\">e72a072d7530cb54ee32abff1ade3a8453183a39f3ea48fb4cfe</span><span class=\"o\">=</span><span class=\"n\">ro</span><span class=\"o\">+</span><span class=\"n\">wh</span>\n<span class=\"o\">/</span><span class=\"n\">var</span><span class=\"o\">/</span><span class=\"n\">lib</span><span class=\"o\">/</span><span class=\"n\">docker</span><span class=\"o\">/</span><span class=\"n\">aufs</span><span class=\"o\">/</span><span class=\"n\">diff</span><span class=\"o\">/</span><span class=\"n\">c2e85f6f2e0d67d38f41adafe94c2d82a41247f5dcca690fedad554611c4dc08</span><span class=\"o\">=</span><span class=\"n\">ro</span><span class=\"o\">+</span><span class=\"n\">wh</span>\n<span class=\"o\">/</span><span class=\"n\">var</span><span class=\"o\">/</span><span class=\"n\">lib</span><span class=\"o\">/</span><span class=\"n\">docker</span><span class=\"o\">/</span><span class=\"n\">aufs</span><span class=\"o\">/</span><span class=\"n\">diff</span><span class=\"o\">/</span><span class=\"mi\">787977346</span><span class=\"n\">a1c8653b2c535abd156ddf368f7839e385b237d7325177e6ef335a6</span><span class=\"o\">=</span><span class=\"n\">ro</span><span class=\"o\">+</span><span class=\"n\">wh</span></code></pre></div><p>可以看到，不光是 Docker Image 是通过 AUFS 分层来管理的。因为 Image 是容器内文件的提供者，所以自然而然，容器内部的文件系统也需要通过 AUFS 的方式来进行管理。其中最上面的 branch 是允许修改的，它对应着这个容器内部的文件系统目录：<code>/var/lib/docker/aufs/mnt/755ee5904e25f223a5393cc9202e4daf0ffc9770f67715c966e939eeedf358d4</code>。其余的 branch 都是只读。这就说明，我们在容器当中做的一些修改操作，最后都会反馈到最上面这个目录下：<code>/var/lib/docker/aufs/diff/755ee5904e25f223a5393cc9202e4daf0ffc9770f67715c966e939eeedf358d4</code>。</p><p>一个能够反映出容器和镜像分层之间关系的图示如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-23561151caaff6514047868a178547fa_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"431\" class=\"origin_image zh-lightbox-thumb\" width=\"450\" data-original=\"https://pic3.zhimg.com/v2-23561151caaff6514047868a178547fa_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;450&#39; height=&#39;431&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"431\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"450\" data-original=\"https://pic3.zhimg.com/v2-23561151caaff6514047868a178547fa_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-23561151caaff6514047868a178547fa_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>其中下半部分都是镜像的只读 layer，而顶层才是允许在容器内部修改的。</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2>Ref</h2><ol><li><a href=\"https://link.zhihu.com/?target=https%3A//coolshell.cn/articles/17061.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Docker基础技术：AUFS | | 酷 壳 - CoolShell</a></li></ol>", 
            "topic": [
                {
                    "tag": "Docker", 
                    "tagLink": "https://api.zhihu.com/topics/19950993"
                }, 
                {
                    "tag": "aufs", 
                    "tagLink": "https://api.zhihu.com/topics/20090938"
                }
            ], 
            "comments": [
                {
                    "userName": "灌汤包", 
                    "userLink": "https://www.zhihu.com/people/38ff0b3245450d42eaa785672b2f8fb4", 
                    "content": "<p>终于有一个写docker和aufs了。楼主还打算写btrfs和overlay2不，时刻关注啊</p><a class=\"comment_sticker\" href=\"https://pic4.zhimg.com/v2-fa3cb6bc9ec57da84ab53a60f48d0c6f.gif\" data-width=\"\" data-height=\"\">[棒]</a>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "lane", 
                    "userLink": "https://www.zhihu.com/people/ffee5194958072496c187d7bcc624bdb", 
                    "content": "<p>当有多个容器存在时，使用aufs的情况下怎么判断容器和aufs/mnt下目录的对应关系呢</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "程空万里", 
                    "userLink": "https://www.zhihu.com/people/d351517444a97bd55fc80966047dbe0f", 
                    "content": "<a class=\"comment_sticker\" href=\"https://pic4.zhimg.com/v2-fa3cb6bc9ec57da84ab53a60f48d0c6f.gif\" data-width=\"\" data-height=\"\">[棒]</a>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/47590418", 
            "userName": "要没时间了", 
            "userLink": "https://www.zhihu.com/people/bdbe7f88af9a021c5c770649d266f143", 
            "upvote": 14, 
            "title": "Linux 资源隔离机制 -- CGroup", 
            "content": "<h2>为什么需要 CGroup</h2><p>Linux Namespace 为容器（进程）提供了环境上的隔离，它的行为类似 chroot 这个命令，将某个用户jail 到一个特定的环境下，与外界隔离。但是在之前介绍 Namespace 的文章中我们也提到过，虽然 Namespace 提供的隔离机制有很多，但实际上我们操作的一些资源仍然是全局的，并且基本上是没有什么限制的，如：内存，CPU，硬盘等。一些在已经「隔离」了的进程中做的操作还是会影响到其他进程的。</p><p>所以，Linux 在内核中以文件系统的形式为我们实现了一种资源隔离的机制：Linux CGroup，位于 /sys/fs/cgroup 目录 。它用来限制，控制一个进程群组的资源。工作方式类似于：先对计算机的某个资源设置了一些限制规则，如只能使用 CPU 的20%。然后，如果我们想一些进程去遵守这个使用 CPU 资源的限制的话，就将它加入到这个规则所绑定的进程组中，之后，相应的限制就会对其生效。</p><p>总的来说，使用 CGroup，可以以控制组为单位，对其使用的操作系统的资源做更精细的控制。</p><h2>使用 CGroup</h2><p>其实 CGroup 说到底就是内核实现的一个资源隔离的功能，既然是一个功能，那直接去使用它将会有一个更加直观的了解：</p><p>先来看下 cgroup 的文件系统下都提供了对那些资源的隔离：</p><div class=\"highlight\"><pre><code class=\"language-text\">xr@xr-lab:/sys/fs/cgroup$ ll\ntotal 0\ndrwxr-xr-x 15 root root 380 10月 24 14:35 ./\ndrwxr-xr-x 11 root root   0 10月 24 19:33 ../\ndr-xr-xr-x  4 root root   0 10月 24 19:33 blkio/\nlrwxrwxrwx  1 root root  11 10月 24 14:35 cpu -&gt; cpu,cpuacct/\nlrwxrwxrwx  1 root root  11 10月 24 14:35 cpuacct -&gt; cpu,cpuacct/\ndr-xr-xr-x  5 root root   0 10月 24 19:39 cpu,cpuacct/\ndr-xr-xr-x  2 root root   0 10月 24 19:33 cpuset/\ndr-xr-xr-x  4 root root   0 10月 24 19:33 devices/\ndr-xr-xr-x  2 root root   0 10月 24 19:33 freezer/\ndr-xr-xr-x  2 root root   0 10月 24 19:33 hugetlb/\ndr-xr-xr-x  4 root root   0 10月 24 19:33 memory/\nlrwxrwxrwx  1 root root  16 10月 24 14:35 net_cls -&gt; net_cls,net_prio/\ndr-xr-xr-x  2 root root   0 10月 24 19:33 net_cls,net_prio/\nlrwxrwxrwx  1 root root  16 10月 24 14:35 net_prio -&gt; net_cls,net_prio/\ndr-xr-xr-x  2 root root   0 10月 24 19:33 perf_event/\ndr-xr-xr-x  4 root root   0 10月 24 19:33 pids/\ndr-xr-xr-x  2 root root   0 10月 24 19:33 rdma/\ndr-xr-xr-x  5 root root   0 10月 24 19:33 systemd/\ndr-xr-xr-x  5 root root   0 10月 24 19:33 unified/</code></pre></div><p>其中 cpu 和 memory 我们都是比较熟悉的，而 blkio 代表了用于 I/O 的块设备，姑且可以将它当做是硬盘资源吧。假设我们现在有一个核心逻辑为「死循环」的程序:</p><div class=\"highlight\"><pre><code class=\"language-c\"><span class=\"c1\">// 该例子参考了陈皓老师的博客：https://coolshell.cn/articles/17049.html，欢迎大家去原文观看\n</span><span class=\"c1\"></span><span class=\"kt\">int</span> <span class=\"nf\">main</span><span class=\"p\">(</span><span class=\"kt\">void</span><span class=\"p\">)</span>\n<span class=\"p\">{</span>\n    <span class=\"kt\">int</span> <span class=\"n\">i</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">;</span>\n    <span class=\"k\">for</span><span class=\"p\">(;;)</span> <span class=\"n\">i</span><span class=\"o\">++</span><span class=\"p\">;</span>\n    <span class=\"k\">return</span> <span class=\"mi\">0</span><span class=\"p\">;</span>\n<span class=\"p\">}</span></code></pre></div><p>启动了该程序后，可以通过 <code>top</code>命令看到其 CPU 占用率已经到达了100%</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-0e1e269fcbe1b05d0d6e583b5f3202b0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"893\" data-rawheight=\"217\" class=\"origin_image zh-lightbox-thumb\" width=\"893\" data-original=\"https://pic1.zhimg.com/v2-0e1e269fcbe1b05d0d6e583b5f3202b0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;893&#39; height=&#39;217&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"893\" data-rawheight=\"217\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"893\" data-original=\"https://pic1.zhimg.com/v2-0e1e269fcbe1b05d0d6e583b5f3202b0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-0e1e269fcbe1b05d0d6e583b5f3202b0_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>现在，我们准备继续改造一下这个小程序：</p><ol><li>父进程启动后且创建子进程之前在 /sys/fs/cgroup/cpu 目录下再新建一个目录，作为一个我们自定义的进程组。并且对这个进程组使用的 CPU 资源写入一个限制规则：只能使用 CPU 的50%</li><li>创建一个子进程并将其加入到我们已经创建好的进程组中，然后执行「死循环」逻辑</li></ol><div class=\"highlight\"><pre><code class=\"language-c\"><span class=\"cp\">#include</span> <span class=\"cpf\">&lt;sys/types.h&gt;</span><span class=\"cp\">\n</span><span class=\"cp\">#include</span> <span class=\"cpf\">&lt;sys/wait.h&gt;</span><span class=\"cp\">\n</span><span class=\"cp\">#include</span> <span class=\"cpf\">&lt;sys/stat.h&gt;</span><span class=\"cp\">\n</span><span class=\"cp\">#include</span> <span class=\"cpf\">&lt;sys/types.h&gt;</span><span class=\"cp\">\n</span><span class=\"cp\">#include</span> <span class=\"cpf\">&lt;stdio.h&gt;</span><span class=\"cp\">\n</span><span class=\"cp\">#include</span> <span class=\"cpf\">&lt;sched.h&gt;</span><span class=\"cp\">\n</span><span class=\"cp\">#include</span> <span class=\"cpf\">&lt;signal.h&gt;</span><span class=\"cp\">\n</span><span class=\"cp\">#include</span> <span class=\"cpf\">&lt;unistd.h&gt;</span><span class=\"cp\">\n</span><span class=\"cp\">#include</span> <span class=\"cpf\">&lt;stdlib.h&gt;</span><span class=\"cp\">\n</span><span class=\"cp\"></span>\n<span class=\"cp\">#define STACK_SIZE (1024 * 1024)\n</span><span class=\"cp\"></span>\n<span class=\"kt\">int</span> <span class=\"n\">pipefd</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">];</span>\n<span class=\"k\">static</span> <span class=\"kt\">char</span> <span class=\"n\">container_stack</span><span class=\"p\">[</span><span class=\"n\">STACK_SIZE</span><span class=\"p\">];</span>\n<span class=\"kt\">int</span> <span class=\"nf\">container_main</span><span class=\"p\">(</span><span class=\"kt\">void</span><span class=\"o\">*</span> <span class=\"n\">arg</span><span class=\"p\">)</span>\n<span class=\"p\">{</span>\n        <span class=\"kt\">char</span> <span class=\"n\">ch</span><span class=\"p\">;</span>\n        <span class=\"kt\">int</span> <span class=\"n\">i</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">;</span>\n        <span class=\"n\">close</span><span class=\"p\">(</span><span class=\"n\">pipefd</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]);</span>\n        <span class=\"n\">read</span><span class=\"p\">(</span><span class=\"n\">pipefd</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"o\">&amp;</span><span class=\"n\">ch</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">);</span>\n        <span class=\"n\">printf</span><span class=\"p\">(</span><span class=\"s\">&#34;start</span><span class=\"se\">\\n</span><span class=\"s\">&#34;</span><span class=\"p\">);</span>\n        <span class=\"k\">for</span><span class=\"p\">(;;)</span><span class=\"n\">i</span><span class=\"o\">++</span><span class=\"p\">;</span>\n        <span class=\"k\">return</span> <span class=\"mi\">1</span><span class=\"p\">;</span>\n<span class=\"p\">}</span>\n\n<span class=\"kt\">int</span> <span class=\"nf\">main</span><span class=\"p\">()</span>\n<span class=\"p\">{</span>\n            <span class=\"n\">printf</span><span class=\"p\">(</span><span class=\"s\">&#34;Parent - start a container!</span><span class=\"se\">\\n</span><span class=\"s\">&#34;</span><span class=\"p\">);</span>\n            <span class=\"cm\">/* 设置CPU利用率为50% */</span>\n            <span class=\"n\">mkdir</span><span class=\"p\">(</span><span class=\"s\">&#34;/sys/fs/cgroup/cpu/deadloop&#34;</span><span class=\"p\">,</span> <span class=\"mi\">755</span><span class=\"p\">);</span>\n            <span class=\"n\">system</span><span class=\"p\">(</span><span class=\"s\">&#34;echo 50000 &gt; /sys/fs/cgroup/cpu/deadloop/cpu.cfs_quota_us&#34;</span><span class=\"p\">);</span>\n            <span class=\"n\">pipe</span><span class=\"p\">(</span><span class=\"n\">pipefd</span><span class=\"p\">);</span>\n             <span class=\"cm\">/* 调用clone函数，其中传出一个函数，还有一个栈空间的（为什么传尾指针，因为栈是反着的） */</span>\n            <span class=\"kt\">int</span> <span class=\"n\">container_pid</span> <span class=\"o\">=</span> <span class=\"n\">clone</span><span class=\"p\">(</span><span class=\"n\">container_main</span><span class=\"p\">,</span> <span class=\"n\">container_stack</span><span class=\"o\">+</span><span class=\"n\">STACK_SIZE</span><span class=\"p\">,</span> <span class=\"n\">SIGCHLD</span> <span class=\"p\">,</span><span class=\"nb\">NULL</span><span class=\"p\">);</span>\n            <span class=\"kt\">char</span> <span class=\"n\">cmd</span><span class=\"p\">[</span><span class=\"mi\">128</span><span class=\"p\">];</span>\n            <span class=\"n\">sprintf</span><span class=\"p\">(</span><span class=\"n\">cmd</span><span class=\"p\">,</span> <span class=\"s\">&#34;echo %d &gt;&gt; /sys/fs/cgroup/cpu/deadloop/tasks&#34;</span><span class=\"p\">,</span><span class=\"n\">container_pid</span><span class=\"p\">);</span>\n            <span class=\"n\">system</span><span class=\"p\">(</span><span class=\"n\">cmd</span><span class=\"p\">);</span>\n            <span class=\"n\">close</span><span class=\"p\">(</span><span class=\"n\">pipefd</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]);</span>\n            <span class=\"n\">waitpid</span><span class=\"p\">(</span><span class=\"n\">container_pid</span><span class=\"p\">,</span> <span class=\"nb\">NULL</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">);</span>\n            <span class=\"n\">printf</span><span class=\"p\">(</span><span class=\"s\">&#34;Parent - container stopped!</span><span class=\"se\">\\n</span><span class=\"s\">&#34;</span><span class=\"p\">);</span>\n            <span class=\"k\">return</span> <span class=\"mi\">0</span><span class=\"p\">;</span>\n<span class=\"p\">}</span></code></pre></div><p>在上面的例子中，我使用了一个 pipe 做父子进程间的同步，确保父进程把子进程 id 写入到名为 deadloop 的进程组之后再唤醒子进程执行死循环的逻辑。编译执行后，可以通过 top 命令看到，子进程的 CPU 利用率已经被限制到了50%。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-b5296e535b00dd6bc3e9dc9d4c0a4a67_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1002\" data-rawheight=\"255\" class=\"origin_image zh-lightbox-thumb\" width=\"1002\" data-original=\"https://pic4.zhimg.com/v2-b5296e535b00dd6bc3e9dc9d4c0a4a67_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1002&#39; height=&#39;255&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1002\" data-rawheight=\"255\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1002\" data-original=\"https://pic4.zhimg.com/v2-b5296e535b00dd6bc3e9dc9d4c0a4a67_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-b5296e535b00dd6bc3e9dc9d4c0a4a67_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>除了对 CPU 限制之外，对 MEM，硬盘容量都可以做限制，甚至对某个块设备的读写速率也是可以限制的。</p><h2>一些重要的概念</h2><h2>子系统</h2><p>在 CGroup 中，有很多子系统。一个子系统就代表一个资源控制器。<i>sys/fs/cgroup</i> 目录下的项目就是目前操作系统提供的全部子系统。</p><h2>控制组 (Control Group)</h2><p>一个控制组包含多个进程，而资源的限制也是定义在控制组上的。若一个进程加入到某一个控制组，则自动会受到定义在这个控制组上面的限制规则的影响。</p><h2>层级</h2><p>一个子系统下面的控制组，可以进行嵌套，最终形成一个树形的结构。子节点控制组会继承父节点控制组上对于资源的限制规则。若在子节点的控制组重定义了和父节点中相同资源的规则，则会发生覆盖（子覆盖父）</p><h2>总结</h2><p>在 Docker 中，其实也是通过 CGroup 来实现对容器使用资源的限制的。如果你在 k8s 的环境下工作过，可能会对资源的「 Request」 和「Limit」 的概念比较熟悉。这是 k8s 给用户提供的一个可以指定容器使用资源限制的一个入口。最终到了操作系统这里，还是通过 CGroup 实现的。</p>", 
            "topic": [
                {
                    "tag": "操作系统", 
                    "tagLink": "https://api.zhihu.com/topics/19552686"
                }, 
                {
                    "tag": "Linux", 
                    "tagLink": "https://api.zhihu.com/topics/19554300"
                }, 
                {
                    "tag": "Docker", 
                    "tagLink": "https://api.zhihu.com/topics/19950993"
                }
            ], 
            "comments": [
                {
                    "userName": "D丝肥宅秃老博士", 
                    "userLink": "https://www.zhihu.com/people/84923df7d40ea34c4a4a7bd67b2c4397", 
                    "content": "徐总早点休息啊，注意身体", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "要没时间了", 
                            "userLink": "https://www.zhihu.com/people/bdbe7f88af9a021c5c770649d266f143", 
                            "content": "<p>再不用功就只能回赤峰挖煤了</p>", 
                            "likes": 0, 
                            "replyToAuthor": "D丝肥宅秃老博士"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/47571649", 
            "userName": "要没时间了", 
            "userLink": "https://www.zhihu.com/people/bdbe7f88af9a021c5c770649d266f143", 
            "upvote": 24, 
            "title": "Linux 环境隔离机制 -- Linux Namespace", 
            "content": "<h2>什么是 Linux Namespace？它解决了什么问题？</h2><p>简单来说，Linux Namespace 是操作系统内核在不同进程间实现的一种「环境隔离机制」。</p><p>举例来说：现在有两个进程A，B。他们处于两个不同的 PID Namespace 下：ns1, ns2。在ns1下，A 进程的 PID 可以被设置为1，在 ns2 下，B 进程的 PID 也可以设置为1。但是它们两个并不会冲突，因为 Linux PID Namespace 对 PID 这个资源在进程 A，B 之间做了隔离。A 进程在 ns1下是不知道 B 进程在 ns2 下面的 PID 的。</p><p>这种环境隔离机制是实现容器技术的基础。因为在整个操作系统的视角下，一个容器表现出来的就是一个进程。</p><p>Linux 一共构建了 6 种不同的 Namespace，用于不同场景下的隔离：</p><ol><li>Mount - isolate filesystem mount points</li><li>UTS - isolate hostname and domainname</li><li>IPC - isolate interprocess communication (IPC) resources</li><li>PID - isolate the PID number space</li><li>Network - isolate network interfaces</li><li>User - isolate UID/GID number spaces</li></ol><h2>Docker 的网络隔离机制——Linux Network Namespace</h2><p>Docker 使用的网络模型是 CNM（Container Network Model），根据官方的设计文档，它的结构大致如下：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-ba14d2a7b21763c6746f403585aa4ed1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"632\" data-rawheight=\"248\" class=\"origin_image zh-lightbox-thumb\" width=\"632\" data-original=\"https://pic2.zhimg.com/v2-ba14d2a7b21763c6746f403585aa4ed1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;632&#39; height=&#39;248&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"632\" data-rawheight=\"248\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"632\" data-original=\"https://pic2.zhimg.com/v2-ba14d2a7b21763c6746f403585aa4ed1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-ba14d2a7b21763c6746f403585aa4ed1_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>CNM 模型一共需要三个组件：</p><ul><li>NetworkSandbox： 在 docker 中的实现对应 Linux Network Namespace</li><li>Endpoint： 在 docker 中的实现对应 VETH （一种虚拟网卡设备）</li><li>Network： 在 docker 中的实现对应 Linux Bridge</li></ul><h2>什么是 VETH ？ 什么是 Linux Bridge ？什么是 Linux Network Namespace</h2><p>Linux Bridge 是 Linux 提供的一种虚拟网络设备，它可以实现多个不同容器在一个以太网内进行通信。</p><p>Bridge 默认情况下工作在二层网络，可以在同一网络根据一定的规则过滤和转发以太网包。若给一个 Linux Bridge 设备分配一个 IP 地址，就会开启它的三层工作模式。</p><p>若你在一台安装了 docker 的 Linux 主机上执行 <code>ip addr</code>命令，就可以看到一个名为 docker0的Linux Bridge。默认情况下在这台宿主机上启动的容器都会链接到这个 Bridge 上。因为是在同一个网络下，且通过 Bridge 链接在一起，所以不同的容器之间可以进行网络通信。否则，不同的容器之间会因为链接的 Bridge 不同而产生网络隔离。</p><p>VETH 也是 Linux 提供的一种网络设备，它在行为上类似操作系统的 Pipe。因为 VETH 总是成对出现，一端为输入端，一端为输出端。每一个 VETH 设备都可以被赋予一个 IP 地址，然后参与三层网络通信的过程。</p><p>Linux Network Namespace 是 Linux 提供的在不同进程之间的一种网络环境隔离机制。这里可以简单的理解为，每一个进程在自己的 NS 下，都独享了一套完整的网络环境（与宿主机对比）。特定 NS 内的网络环境对外部来说是不可见的，并且在其中对一些网络设置做修改也不会影响到外部（如路由规则）。</p><p>若只考虑两个容器在宿主机上面的网络模型，它的结构如下图所示：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-2a00521c59658d3b44eabb0152e000e5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"512\" data-rawheight=\"356\" class=\"origin_image zh-lightbox-thumb\" width=\"512\" data-original=\"https://pic2.zhimg.com/v2-2a00521c59658d3b44eabb0152e000e5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;512&#39; height=&#39;356&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"512\" data-rawheight=\"356\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"512\" data-original=\"https://pic2.zhimg.com/v2-2a00521c59658d3b44eabb0152e000e5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-2a00521c59658d3b44eabb0152e000e5_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>Docker 的 Hostname 隔离机制——Linux UTS Namespace</h2><p>简单来说，这是 Linux 提供的一种针对多个进程间的 Hostname 的隔离机制。它允许一个进程在其内部设置自己的 hostname。让我们通过一个例子来了解：</p><div class=\"highlight\"><pre><code class=\"language-c\"><span class=\"cm\">/*\n</span><span class=\"cm\">下面的实例来自于陈皓老师的博客（https://coolshell.cn/articles/17010.html），感谢陈皓老师的文章\n</span><span class=\"cm\">*/</span>\n<span class=\"cp\">#define _GNU_SOURCE\n</span><span class=\"cp\">#include</span> <span class=\"cpf\">&lt;sys/types.h&gt;</span><span class=\"cp\">\n</span><span class=\"cp\">#include</span> <span class=\"cpf\">&lt;sys/wait.h&gt;</span><span class=\"cp\">\n</span><span class=\"cp\">#include</span> <span class=\"cpf\">&lt;stdio.h&gt;</span><span class=\"cp\">\n</span><span class=\"cp\">#include</span> <span class=\"cpf\">&lt;sched.h&gt;</span><span class=\"cp\">\n</span><span class=\"cp\">#include</span> <span class=\"cpf\">&lt;signal.h&gt;</span><span class=\"cp\">\n</span><span class=\"cp\">#include</span> <span class=\"cpf\">&lt;unistd.h&gt;</span><span class=\"cp\">\n</span><span class=\"cp\"></span>\n<span class=\"cm\">/* 定义一个给 clone 用的栈，栈大小1M */</span>\n<span class=\"cp\">#define STACK_SIZE (1024 * 1024)\n</span><span class=\"cp\"></span><span class=\"k\">static</span> <span class=\"kt\">char</span> <span class=\"n\">container_stack</span><span class=\"p\">[</span><span class=\"n\">STACK_SIZE</span><span class=\"p\">];</span>\n\n<span class=\"kt\">char</span><span class=\"o\">*</span> <span class=\"k\">const</span> <span class=\"n\">container_args</span><span class=\"p\">[]</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s\">&#34;/bin/bash&#34;</span><span class=\"p\">,</span>\n    <span class=\"nb\">NULL</span>\n<span class=\"p\">};</span>\n\n<span class=\"kt\">int</span> <span class=\"nf\">container_main</span><span class=\"p\">(</span><span class=\"kt\">void</span><span class=\"o\">*</span> <span class=\"n\">arg</span><span class=\"p\">)</span>\n<span class=\"p\">{</span>\n    <span class=\"n\">printf</span><span class=\"p\">(</span><span class=\"s\">&#34;Container - inside the container!</span><span class=\"se\">\\n</span><span class=\"s\">&#34;</span><span class=\"p\">);</span>\n    <span class=\"o\">**</span><span class=\"n\">sethostname</span><span class=\"p\">(</span><span class=\"s\">&#34;container&#34;</span><span class=\"p\">,</span><span class=\"mi\">10</span><span class=\"p\">);</span> <span class=\"cm\">/* 设置hostname */</span><span class=\"o\">**</span>\n    <span class=\"n\">execv</span><span class=\"p\">(</span><span class=\"n\">container_args</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">container_args</span><span class=\"p\">);</span>\n    <span class=\"n\">printf</span><span class=\"p\">(</span><span class=\"s\">&#34;Something&#39;s wrong!</span><span class=\"se\">\\n</span><span class=\"s\">&#34;</span><span class=\"p\">);</span>\n    <span class=\"k\">return</span> <span class=\"mi\">1</span><span class=\"p\">;</span>\n<span class=\"p\">}</span>\n\n<span class=\"kt\">int</span> <span class=\"nf\">main</span><span class=\"p\">()</span>\n<span class=\"p\">{</span>\n    <span class=\"n\">printf</span><span class=\"p\">(</span><span class=\"s\">&#34;Parent - start a container!</span><span class=\"se\">\\n</span><span class=\"s\">&#34;</span><span class=\"p\">);</span>\n    <span class=\"kt\">int</span> <span class=\"n\">container_pid</span> <span class=\"o\">=</span> <span class=\"n\">clone</span><span class=\"p\">(</span><span class=\"n\">container_main</span><span class=\"p\">,</span> <span class=\"n\">container_stack</span><span class=\"o\">+</span><span class=\"n\">STACK_SIZE</span><span class=\"p\">,</span> \n            <span class=\"o\">**</span><span class=\"n\">CLONE_NEWUTS</span> <span class=\"o\">|</span> <span class=\"n\">SIGCHLD</span><span class=\"p\">,</span> <span class=\"nb\">NULL</span><span class=\"p\">);</span> <span class=\"cm\">/*启用CLONE_NEWUTS Namespace隔离 */</span><span class=\"o\">**</span>\n    <span class=\"n\">waitpid</span><span class=\"p\">(</span><span class=\"n\">container_pid</span><span class=\"p\">,</span> <span class=\"nb\">NULL</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">);</span>\n    <span class=\"n\">printf</span><span class=\"p\">(</span><span class=\"s\">&#34;Parent - container stopped!</span><span class=\"se\">\\n</span><span class=\"s\">&#34;</span><span class=\"p\">);</span>\n    <span class=\"k\">return</span> <span class=\"mi\">0</span><span class=\"p\">;</span>\n<span class=\"p\">}</span></code></pre></div><p>上面的例程是通过创建一个子进程的方式来测试 Linux UTS Namespace 提供的隔离机制。其中被双星号标记的两行代码是比较关键的部分:  <code>CLONE_NEWUTS</code> 是启动 Hostname 隔离机制的一个系统调用参数，当以这个参数创建进程的时候，就会开启隔离。<code>sethostname</code>同样是一个系统的调用，它在自己进程内部设置了单独的 Hostname，且不会影响到宿主机。反之，如果没有上面的两行代码的话，子进程中的 Hostname 和宿主机应该是一致的。</p><h2>Docker 的 IPC 隔离机制——Linux IPC Namespace</h2><p>Linux 在实现进程间通信时用了以下几种方法：</p><ol><li>管道</li><li>共享内存</li><li>信号量</li><li>消息队列</li></ol><p>这些结构在被创建出来的时候，都会在全局范围内有一个唯一的 ID。所以，如果想要在单独的进程空间中，有一套自己的 IPC 标识并且对宿主机环境屏蔽，这就是 Linux 的  IPC 隔离机制。 IPC Namespace 的实现其实和 UTS 是差不多的：在创建进程的时候加入<code>CLONE_NEWIPC</code>标志即可。</p><p>当子进程和父进程都被创建之后，在子进程中通过<code>ipcmk -Q</code>命令创建的消息队列不会在宿主机上被发现，而在宿主机上创建的也不会被子进程发现。</p><h2>Docker 的 Mount 隔离机制——Linux Mount Namespace</h2><p>Linux Mount Namespace 实现了在不同进程间对于文件系统「挂载点」的隔离机制。每一个进程所持有的挂载点信息都 可以在 /proc/mounts 和 /proc/mountinfo 和 /proc/mountstats 中找到。/proc 是 Linux 提供的一种虚拟文件系统。此目录下保存的文件和目录信息描述了该操作系统一些运行时的信息。我们既可以通过改变目录下的一些内容来影响操作系统运行的结果，也可以查询该目录下的信息以便获得当前操作系统的运行情况。/proc 目录下的东西并不是真的文件和目录，它实际上是存在于内存中的。</p><p>如果想开启这种隔离机制，需要在创建子进程的时候使用<code>CLONE_NEWNS</code>参数。默认情况下，子进程的挂载点信息一般都是从父进程的 mount namespace 下拷贝的。但是在子进程创建完成之后，两者之间的 mount namespace 以及相应的挂载点信息就没有任何关系了。在子进程中对挂载点信息的操作是不会影响到父进程的。</p><blockquote> PS: 这里一定要提醒读者的是，Linux Mount Namespace 提供的仅仅是对「挂载点」的隔离，并不是对文件系统的隔离。事实上，即使是在已经建立了Mount Namespace 隔离的两个进程中执行 mount/umount 操作也同样会影响到宿主机的文件系统。 <br/> </blockquote><h2>Docker 的 PID 隔离机制——Linux PID Namespace</h2><p>在众多 Linux 中的进程中，有一个进程是比较特殊的：init 进程（PID 为1）。它是操作系统内核初始化后第一个启动的进程，也是整个操作系统范围内的父进程，即祖先进程。之后所有的进程都是从它派生而来。最终形成一个具有层级结构的进程树。Init 进程有很多特殊的权限，如屏蔽一些信号或检查它派生的进程的状态。Init 进程在检查到一些孤儿进程的时候，会对他们进行回收。</p><p>如果能做到 PID 在容器内外部也是隔离的，那么在容器内部看起来进程就好像运行在了一个单独的操作系统中。特定容器内部或者说特定进程空间下的 PID 是可以和宿主机的 PID 取值相同的，并且不会发生冲突。以此类推，若进程和容器内部也有一个PID 为1的进程，它将会独立的管理其创建出的子进程。而这些特性，也正是 Linux PID Namespace 实现 PID 隔离所需要的。</p><h2>Docker 的 User 隔离机制——Linux User Namespace</h2><p>Linux User Namespace 提供的隔离机制允许多个不同的进程间各有自己独立的一套 UID/GID 体系，并且可以将进程内部的 UID/GID 与宿主机的 UID/GID 进行映射。开启这个隔离机制的方法也很简单：在创建子进程的时候传入<code>CLONE_NEWUSER</code>参数即可。至于 UID/GID 的映射，可以在<code>/proc/&lt;pid&gt;/uid_map</code> 和 <code>/proc/&lt;pid&gt;/gid_map</code> 两个文件中，按照 <code>ID-inside-ns ID-outside-ns length</code>的形式写入映射记录。</p><p>这里有一个实现进程间「安全机制」的 Case，是通过 Linux User Namespace 来实现的:</p><p>在创建子进程的时候，父进程通过对<code>/proc/&lt;子进程pid&gt;/uid_map</code> 和 <code>/proc/&lt;子进程pid&gt;/gid_map</code> 两个文件的写入，将子进程的PID 映射为子进程内部值为0的 uid 和 gid。子进程启动的时候，会因为我们设置了 uid 为0，从而自动切换到 root 用户。这样一来，我们就实现了使用一般用户创建子进程，但是在子进程的内部确是以 root 用户的身份来运行的效果。</p><h2>总结</h2><p>到此为止，Linux Namespace 的隔离机制就全部介绍完了。它是容器技术中「隔离机制」的基础。其实对于这些隔离机制来说，如果想理解透彻，还是要仔细琢磨 Namespace 的概念。这个概念在很多编程语言中都有出现。如果从最简单的字面意思上来理解的话，它就是一个名字空间。不同空间中可以有同一个标识，但是同一个空间中不能出现两个同样的标识。而上面所提到的 PID，Hostname，UID/GID 等等其实本质上都是一种名字的隔离，只有 Network 的部分比较特殊。尤其是在理解 Mount 隔离机制的时候，一定不要忘记一点：我们所做的一切操作都是在宿主机的文件系统上的，隔离的仅仅只是挂载点的记录而已。</p><p>Linux Namespace 的隔离，说到底还是一个逻辑上的概念，它不能切断任何进程和操作系统的链接，所以再怎么隔离是也不彻底的。不同容器或者说进程依赖的都是操作系统的资源，稍有不慎，一些操作还是会影响宿主机系统的。</p>", 
            "topic": [
                {
                    "tag": "Linux", 
                    "tagLink": "https://api.zhihu.com/topics/19554300"
                }, 
                {
                    "tag": "容器云", 
                    "tagLink": "https://api.zhihu.com/topics/20042550"
                }, 
                {
                    "tag": "Docker", 
                    "tagLink": "https://api.zhihu.com/topics/19950993"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/47301830", 
            "userName": "要没时间了", 
            "userLink": "https://www.zhihu.com/people/bdbe7f88af9a021c5c770649d266f143", 
            "upvote": 1, 
            "title": "深入理解进程和线程-2", 
            "content": "<p><b>Q： 为什么都说「进程切换」是个比较昂贵的操作，它昂贵在哪呢？</b></p><p><b>A:</b> </p><p>首先，就是由用户态向内核态的切换。因为我们需要保存旧的进程的状态。其次，我们可能需要执行一个比较复杂的「调度算法」，挑选出一个合适的候选进程。除此之外，每一次进程的切换都会伴随着 CPU 高速缓存的失效。在新的进程被切换到 CPU 上开始运行之后，高速缓存需要从内存中动态装入一些和新的进程运行有关的信息。</p><p><b>Q:  在什么情况下需要进行进程的调度（切换）？</b></p><p><b>A:</b> </p><p>进程切换发生的时候是必然会进行进程调度的，因为此时 CPU 空闲，需要让新的进程在上面运行。那么，什么场景下会发生进程间的切换呢？</p><ol><li>CPU 时间片消耗完：这种场景是较为普通和正常的，操作系统为了让所有的进程都能够得到 CPU 的资源，只分配每个进程一定的 CPU 时间片，当时间片消耗完后，进程正常退出，就需要调度新的进程上来。</li><li>I/O 中断触发：当一个进程因触发 I / O 活动而阻塞之后，若相应的 I / O 设备完成了所需的任务，会向 CPU 发送 I / O 中断。此时，需要决定到底调度那类进程运行：阻塞之后满足条件的，随机的就绪进程，刚刚在运行的进程（被中断打断）</li><li>触发阻塞条件：一个进程可能会执行一些会阻塞自身的操作：如阻塞的系统调用，等待某一个资源的释放。此时，操作系统会将另外一个就绪的进程切换上来。</li></ol><p>一般来说，一个进程的在运行期间过多的是在 CPU 上进行计算，那么它被认为「计算密集型」的。反之，如果大多数时间都消耗在了 I_O 等待上，那么它被认为是「I / O密集型」的。对于当前的 CPU 和 I_O 设备来说，我们可能需要的问题可能更多的和 I / O密集型的进程有关。因为 CPU 的发展速度是远大于 I / O 设备的。</p><p><b>Q: 什么是调度算法？都分为哪几种？</b></p><p><b>A:</b> </p><p>调度算法是一种规则，调度程序依据它来调度进程到 CPU 上运行。调度算法可以大致分为两大类：1. 抢占式调度  2. 非抢占式调度。其实我们平常所说的「CPU 时间片消耗光，进程从 CPU 被切换下来」，指的就是抢占式调度。抢占式调度一般会规定一个时钟周期，当一个时钟周期结束之后，触发时钟中断，将控制权交给调度程序（scheduler），它会根据一定的规则挑选一个可运行的进程。而非抢占式调度则不会主动进行进程间的切换，除非进程自己进入阻塞状态或者它自己让出 CPU。否则，即使一个进程因中断导致暂时停止运行，在中断被处理掉之后，它仍然会在 CPU 上继续运行，而不会被切换。</p><p>所以，对于非抢占调度和抢占调度来说，它们唯一的区别，就是对中断的处理（时间中断或者其他事件类的中断）。除了通过「处理中断的方式」将调度算法分为两类之外，我们还可以根据调度算法所适配的系统将其分为三类：</p><ol><li>用于批处理系统的</li><li>用于交互式系统的</li><li>用于实时系统的</li></ol><p>对于不同的系统，在设计调度算法的时候会有不同的考量。比如批处理系统，基本不会在意终端用户的感受，只要保证批处理任务顺利进行即可。所以，非抢占或者说长时间才抢占的两种调度算法它都能接受。而对于交互式系统来说，抢占是必须要有的，因为它更加的通用，且要保证终端用户的感受。</p><p><b>Q: 调度算法的设计目标是什么？</b></p><p><b>A:</b></p><p>若不考虑特定类型的系统，有以下三个较为通用的目标：</p><ol><li>公平性：保证可调度的进程都有机会在 CPU 上执行</li><li>抢占性：必须要有一定的措施可以响应「抢占性」的调度请求</li><li>平衡性：能够在现有的条件下，最大程度的提升 CPU 的使用率</li></ol><p><b>Q: 对于批处理系统，有哪些调度算法？</b></p><p><b>A:</b></p><h2>先来先服务</h2><p>先来先服务是最好理解的一种调度算法。因为它实在是太符合人们「顺序处理」事情的习惯了，以至于我怀疑它能称得上是一种算法么。先来先服务将待执行的进程放入一个类似单链表的数据结构中，按照从头至尾的顺序运行。若一个进程被阻塞，则切换至另外一个进程。若阻塞的进程被重新唤醒，那么它将被加入到链表尾部，等待运行。</p><h2>最短作业优先 &amp;&amp; 最短剩余时间优先</h2><p>将这两种调度算法放在一起说的原因是：两个算法的顺利执行都依赖于我们必须要事先知道每个进程的运行时间。</p><p>对于最短作业优先算法来说，我们可以考虑这样一个例子：现在有 A， B， C， D 四个进程，四个进程的运行时间分别为 5, 4, 4, 3。 按照正常的顺序，四个进程顺序执行花费的总时间为 5+（4+5）+ （4+5+4）+（3+4+4+5）= 43， 平均时间为 10。如果将例子中具体的数值换为变量，四个进程运行的时间分别为 a, b, c, d, 则总时间为 a + (a+b) + (a+b+c) + (a +b+c+d) = 4a + 3b + 2c +d, 平均时间为 a+ 3/4b + 1/2c + 1/4d。 仔细观察就会发现，在运行总时间不变的情况下，按照对平均时间最终取值的影响程度从大到小排序， a 是影响最大的一个，bcd 依次降低。那么如果我们将 a, b, c, d 的取值按照从小到大来安排，最终的平均运行时间就是最优的。</p><p>但是最短作业优先算法的一个致命缺陷就是它不能动态的调整进程执行的顺序，因为它是一个非抢占式调度的算法。所以，最短剩余时间优先算法就在它的基础上做了改进：若有新的进程进入就绪队列的时候，对比它的执行时间和当前进程剩余的时间大小，选择较小的一个调度至 CPU 运行。</p><p><b>Q: 对于交互式系统，有哪些调度算法？</b></p><p><b>A:</b> </p><h2>轮转调度</h2><p>轮转调度就是我们常说的 round robin 的形式，它维护一个就绪进程的队列，分配给每个进程一个时间片，若某个进程在时间片内被阻塞，则 CPU 会进行进程切换，从就绪队列选取下一个顺位的进程调度上来。此外，若在规定的时间片内，某个进程一直在运行。当时间片被消耗完的时候，CPU 也会进行上述的切换动作</p><blockquote> 对于这里的抢占行为，笔者觉得应该是通过中断来完成的。如周期性的时钟中断实现的抢占可以保证一个进程不会占用 CPU 太久，而一些事件类的中断如 I / O 中断，将通过给 CPU 发送中断信号的形式来通知它进行处理，此时调度程序也将获得机会，根据已有的策略来运行一个就绪的进程。  <br/> </blockquote><p>轮转调度的实现是比较简单的，但是它也有一个不好控制的地方：时间片的长度。如果时间片长度设置的过长，可能对于短时任务的进程就不是很友好，甚至会出现饥饿的现象。如果时间片设置的太短，那么进程间切换的开销也会消耗相当多的 CPU 资源（保存和恢复进程状态，更新各种表格，清除并重新载入高速缓存）。</p><h2>优先级调度</h2><p>轮转调度的设计是处在一个特定的前提下的：就绪队列中的每个进程优先级都是相同的。但是很多时候，在一个操作系统中有一部分进程是比较重要的，需要优先来运行，有一部分进程是次要的，可以缓一些时间再运行。基于这种考虑，操作系统给出了名为「优先级调度」的调度算法。</p><p>既然是以优先级为调度，那么自然要将进程分类。为此，一种可能的实现是：按照优先级的划分设置多个就绪进程的队列。将不同优先级的进程放入到相应的队列中。比如， 对于 I / O 密集型的进程我们应该尽快分配给他 CPU，以便它能够尽快的进入 I / O 等待的状态，从而能够和其他进程并行执行。所以它的优先级就会被指定的比较高。那么在同一个优先级的队列中我们应该按照什么样的算法去弹出进程呢？答案当然是「轮转调度」，因为轮转调度本身就是为了优先级相同的进程提供的一种较为公平的调度算法。</p><p>在优先级调度算法中，我们通常会按照优先级从高到低的顺序处理就绪队列中的进程，如先把高优先级队列中的进程调度完，然后再处理次优先级的。但是有一点需要注意，进程的优先级不仅仅需要在创建时就指定，在之后的运行过程中要能有动态调整的能力，不然的话，有些低优先级的进程可能会被饿死。</p><h2>最短进程优先</h2><p>这里的最短进程优先算法是借鉴了批处理系统中最短作业优先算法。通过上面的描述我们可以知道，最短进程优先优先在交互式系统中比较难以把控的就是进程的运行时间。因为终端用户有权利运行多样的进程，操作系统不可能像批处理系统一样，在任务没执行之前就准确的估计了任务的执行时间。但是，在交互式系统中，我们可以动态的估计一个进程剩余的运行时间，从而将这个「预估时间」作为依据进行进程调度，这种技术叫做「老化」。如一个进程第一次执行时间为t0,第二次为t1, 通过计算加权和的方式可以估计出它下一次的运行时间为 a*t0 + (1-a) * t1。而加权系数的选择决定了在计算进程运行时间的过程中，想要多快忘记它之前的运行时间。</p>", 
            "topic": [
                {
                    "tag": "线程", 
                    "tagLink": "https://api.zhihu.com/topics/19619468"
                }, 
                {
                    "tag": "操作系统", 
                    "tagLink": "https://api.zhihu.com/topics/19552686"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/47203807", 
            "userName": "要没时间了", 
            "userLink": "https://www.zhihu.com/people/bdbe7f88af9a021c5c770649d266f143", 
            "upvote": 6, 
            "title": "深入理解进程和线程-1", 
            "content": "<h2>深入理解进程和线程</h2><p><b>Q: 什么是进程？</b></p><p><b>A:</b></p><p>进程其实是一个比较抽象的概念，它是用来描述多道程序设计系统中的一个工作单元。单纯的给进程下一个定义是没有任何意义的。比如现在所谓的标准答案：进程是操作系统中运行的程序。对于进程，我们更多的要理解它是一个「复合体」，它是一系列活动的组合。它是一个逻辑上的概念，并不是一个现实世界中具体的事物。这一点和 k8s 中的 pod很像。所以，我更倾向于将进程理解为操作系统中的一个复杂且基本的工作单元。</p><p><b>Q: 子进程被创建之后和父进程是如何隔离的？</b></p><p><b>A:</b></p><p>通常情况下，在 Linux 系统当中，一旦子进程被创建，那么子进程和父进程就会分别占有两块独立的地址空间。相互之间是隔离的，并且可以通过一些方式来进行通信或者共享某些资源。但是，在之后操作系统发展的过程当中，对于父子进程的创建过程可能会有一些优化，而不仅仅是粗暴的将父进程地址空间中所有的东西都 copy 一份给子进程。这里也是有一个比较重要的机制：COW（写时复制机制）。</p><p><b>Q: Linux 中的进程和 Windows 中有哪些不同？</b></p><p><b>A:</b></p><p>Linux 系统中的进程是有严格的「父子关系」的，并且所有的进程会以树形的层次结构组织起来。其中祖先进程可认为是 Init，它是进程树中的根。而 Windows 中的进程，无论父子，都是靠一个叫做「句柄」的概念对一个进程进行标识的，并且这个句柄是可以传递的。所以在 Windows 中，进程间没有严格的父子关系。</p><p><b>Q: 什么是线程？</b></p><p><b>A:</b> </p><p>线程是轻量级的进程。进程由操作系统来管理而线程由进程来管理。不同进程之间的地址空间是隔离的，但是不同线程之间的地址空间是共享的。一般来说，一个进程通常会有一个主线程，进程负责向内核申请线程运行所需要的资源和环境，而线程才是真正执行程序的单位。</p><p><b>Q: 有了进程为什么还需要线程？</b></p><p><b>A:</b></p><p>从程序性能的角度来说，很多程序在一个进程中都会做很多任务。这些任务可以大致的被划分为两类，一类是 I_O, 一类是计算。I_O 通常消耗的时间会比较长，对于只有主线程的进程来说，它会一直处于等待状态，内核分配给他的 CPU 时间片也会被白白的消耗。计算类的任务则会直接消耗 CPU 资源，最大限度的利用了已分配的时间片。所以，如果一个程序中同时包含这两类任务的话，计算类的任务很可能被 I_O 类的任务阻塞，最终导致整个程序的效率下降。因为线程是存在于进程的地址空间中的，如果可以在进程地址空间中创建多个线程，并且让这些线程重叠执行，分别去运行不同类型的任务，就可以在一定的 CPU 时间片内，将程序的效率尽可能的提高。通过上面的一些思考，我们甚至可以延伸出另外一个问题：多线程技术一定会对我们的程序产生积极的影响么？其实也不尽然。如果我们的程序中既包含大量的 I_O 操作，也包含大量的计算操作，那么多线程技术是可以提升我们程序的效率的。因为此时由于多个线程重叠的进行，最大限度的利用了 CPU 的时间片。如果我们的程序基本都是计算类的任务，很少有 I/O 操作，那么多线程的引入可能不会对提升程序的效率有太大的帮助。因为即使线程间的切换消耗再小，也还是有的。同样，这个问题的思考方式还可以延伸到：多进程技术一定会对我们的程序有积极的影响么？</p><p>从资源共享的角度来说，不同进程间的地址是不同的，所以它们在共享一些资源的时候就会比较麻烦，可能需要借助第三方的东西，比如文件。然而对于同一个进程中的不同的线程来说，这种内存上的隔离是不存在的，它们可以很方便的去共享一些资源。看到这里你可能会说，在地址空间不隔离的条件下，多个线程对同一个资源可能会出现竞争的想象。对于这个问题，我们要明确两点：首先，线程间共享资源的初衷是让多个线程合作，而不是让它们竞争。其次，如果不可避免的发生了竞争，也可以通过一些互斥的机制来解决。</p><p>最后还要提及一点的就是，大多数操作系统对于多线程的实现都是在「用户态」下，且线程中维护的必要信息会较进程少很多。这就造成了线程是比进程更轻量级的。如果不可避免的发生频繁和切换操作，那么很明显线程在这种场景下会更具优势。</p><p><b>Q: 进程和线程之间的关系是什么？</b></p><p><b>A:</b></p><p>进程更倾向于从操作系统申请资源，并对这些资源进行统一的管理，提供一个良好的运行环境。线程则更注重利用已经分配好的资源运行程序。也就是说，实际上在 CPU 上调度执行的并不是进程而是线程。</p><p><b>Q: 如何实现线程？</b></p><p><b>A:</b></p><p>实现线程有两种思路：在用户态实现 or 在内核态实现。</p><p>当我们想在用户态实现「线程」的时候，就意味着「线程」或者说是「多线程」对于内核来讲应该是透明的。内核与具有单个控制线程的主进程还是按照原来的模式运行（进程模型）。所以，我们很自然的就能够想到，在用户态下需要一系列「过程」的集合来实现和线程有关的操作以及「多线程」技术。这个「过程」的集合可以被称作为是一种 Runtime 系统。</p><p>用户态 Runtime 系统数和进程数成正比。每一个进程中都有一个 Runtime 去管理进程中的多个线程。它负责线程的创建，销毁。同时也要负责维护一张「线程表」，用于保存进程内部线程的运行状态。更重要的是，这个 Runtime 系统需要借助「线程表」进行线程间的切换，因为同一时刻只有一个线程可以获得 CPU 的时间片。</p><p>其实，这样看起来，Runtime 运行的方式很像一个有限状态机。它将进程内的线程的状态保存至「线程表」中，当一个线程被调度到 CPU 上执行的时候，Runtime 就需要在线程表中读取和这个线程有关的信息；当一个线程要被调度离开 CPU 的时候，同样需要 Runtime 将此时的状态保存到线程表中，以便下一次复原运行的上下文环境。如果再类比一下进程和操作系统内核的关系就可以得知，用轻量级的进程来描述线程，真的是再合适不过了。</p><p>对于线程间的切换来说，它和进程间的切换的实现有所不同。进程间的切换，要不就是依靠中断机制，强行将进程从 CPU 上拿下来；要不就是等到该进程的 CPU 时间片被消耗完，调度系统切换新的进程上来。由于我们现在是在用户态实现线程，操作系统内核无法干预线程的相关操作。所以，我们需要在 Runtime 中实现一个过程，这个过程在调用之后可以主动的将 CPU 时间片让给其他处于就绪态的线程。这也就是 POSIX 线程标准中定义的 Pthread_yield 所要实现的功能。</p><p>在用户态实现 Runtime 的好处其实很明显：1. 之前看起来比较复杂的操作，如线程间的切换，都是在用户态下完成的，不需要内核的参与，所以肯定要比内核实现的版本效率要高 2. 由于这个 Runtime 是我们自己来实现的，所以它的可定制性是非常强的。我们甚至可以开发出自己的一套「线程调度策略」来保证我们的程序效率最大化。</p><p>在用户态实现 Runtime 的坏处其实都可以归结到一个问题上：阻塞。阻塞是指：当一个线程执行了一些阻塞系统调用的时候，不仅仅是其他的线程没有运行的机会了，整个进程都会因为进入阻塞态而被调离 CPU。这是一个非常严重的事情。而触发这种问题的 Case 也很常见：缺页中断（线程所需要的数据或者代码没有在内存页中找到而是在硬盘中）。此外，由于多线程间只能够通过主动调用 Pthread_yield 过程来实现切换操作，如果你的代码写的有 bug 的话，其他的线程就会处于「饥饿」或者「饿死」的状态。</p><p>内核态实现的 Runtime 系统的数量不再随着进程数的变化而变化。事实上，如果真的把线程拿到内核态来实现的话，线程和进程基本就没什么区别了。线程会和进程一样，在内核中有一个线程表，用来维护线程的运行情况。通过对比之前在用户态实现线程的缺点可以知道，如果将所有阻塞线程的调用全都以系统调用的形式来实现的话，线程间的切换就统一由内核来进行管理，它会选取一个合适的线程继续使用剩余的 CPU 时间片。这种阻塞调用既包括线程之间的阻塞也包括阻塞的系统调用。</p><p>虽然说，内核态实现 Runtime 开销比较大的问题是不可避免的。但是仍然可以做出一些优化，比如在线程的销毁操作上，如果一个线程需要被销毁，内核可以不进行真正的销毁操作，而是打上一个空闲线程的标记，并且由它统一管理。这样如果有线程创建需求的时候，有可以直接复用之前已经分配的资源。</p><p>很显然，内核态实现 Runtime 也是有很多缺点的。其中最被大家诟病的就是「开销」变大了。这个开销不仅仅是指时间上面的，还包括空间上面的。如：线程的数量一般都是要比进程多的，所以线程表的规模的增长速度会远远大于进程表。当规模大起来之后如何保证一个高效的读取和写入操作呢？毕竟引入线程和多线程相关概念的初衷是在合适的场景下能够提升程序的效率而不是拉低。</p><p>既然两者各有优劣，那么根据操作系统的一贯思想，就是最大化的将这两个方案的优点结合起来，产出一个普适性更强的方案：调度程序激活机制。它借助了用户态 Runtime 系统的优势：高效的进行线程间的切换。同时，在用户态下模拟「内核」线程的功能，防止因线程使用阻塞的系统调用而发生进程的切换。</p><p>调度程序激活机制启用后，内核会为每一个进程分配一个或多个的虚拟 CPU，用户态 Runtime 系统可以将线程分配到这些虚拟的 CPU 上。虚拟 CPU 代表这个进程可以使用的 CPU 核心数。当一个线程被同进程的另外一个线程所阻塞，它会被用户态 Runtime 系统处理，并调度新的进程运行。此时，不会发生用户态和内核态的切换，对于内核来说，这些操作都是透明的。当一个线程被进程之外的因素阻塞住时（阻塞的系统调用，缺页中断等），内核会感知到这个问题，它会通知用户态 Runtime 系统，需要重新调度一个就绪的线程运行。而当阻塞的事件被完成的时候，内核也会将这个事件通知给用户态 Runtime 系统，让它自己来决定，下一步应该调度哪个线程运行。</p><p>这种内核调用用户态 Runtime 系统的机制被称作为「上行调用」。在CPU中用户空间为上层，内核为下层层，常规调用应该是上层调用下层，下层不应该调用上层，上行调用就是指内核调用用户空间的 Runtime 系统。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-9712f5555042db4af5772b221524761d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1264\" data-rawheight=\"384\" class=\"origin_image zh-lightbox-thumb\" width=\"1264\" data-original=\"https://pic2.zhimg.com/v2-9712f5555042db4af5772b221524761d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1264&#39; height=&#39;384&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1264\" data-rawheight=\"384\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1264\" data-original=\"https://pic2.zhimg.com/v2-9712f5555042db4af5772b221524761d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-9712f5555042db4af5772b221524761d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>Q: 如何实现进程间的通信？</b></p><p><b>A:</b></p><p>两个进程或者线程间如果要进行通信，可能涉及到以下三个问题：</p><ol><li>如何传递信息？</li><li>如何防止「竞争」？</li><li>如何保证进/线程间执行的顺序？</li></ol><p><b>Q: 如何防止「竞争」？</b></p><p><b>A:</b></p><p>我们首先来看下如何在进程间通信过程中避免「竞争」的问题。「竞争」通常出现在两个进程或者线程同时要访问/修改一个共享资源的时候，除了同时读取可能还不会有问题之外，比如一读一写，两个都是写这种组合是肯定会引起资源的「竞争」的。这种「同时操作共享资源」的结果，完全取决于两个竞争者之间执行的时序。但是我们都清楚，在编程的世界中，是不能够对「时序」做任何假设的，因为它的随机性较大，很多「竞争」问题之所以难以 track，就是因为它出现的几率不固定，很难定位。</p><p>那现在问题就变成了：如何在使用共享资源的时候，同一时间只允许一个进程对其操作。这种行为有一个比较统一和抽象的称呼：「互斥」。而产生「竞争」问题的地方或者说要进行「互斥」改造的地方，我们统一把它成为「临界区」。「临界区」是一段操作共享资源的代码，如果能保证同一时间只有一个进程进入「临界区」，那么「竞争」的问题也就随之解决了。</p><p>第一种可实现互斥机制的技术是：屏蔽中断。这应该是最暴力的一种方式了，而且更多的是对于进程间「竞争」问题的解决方案。当一个进程进入临界区后，可以屏蔽所有的中断。此时， CPU 不会再切换进程，已经处在临界区的进程也不会受到影响，它可以放心的操作「共享资源」。但是这种方案的缺点也很明显：1. 进程一旦运行异常很可能从临界区中退不出来，一直不能切换其他进程  2.  在多 CPU 的情形下，除非把所有 CPU 都 disable 掉，否则还是有其他的 CPU 可以调度运行与其「竞争」的进程。</p><p>第二种是在软件层面的方案： 锁变量。进临界区之前查看是否可以获得锁，离开临界区之后释放锁。「获得」和「释放」的操作通过修改某一个变量实现。但是，这其实并没有什么卵用，在 CPU 可以对进程进行任意切换的前提下，锁变量就变成了另外一个「共享资源」。如：在 A 进程进入临界区获得锁之后想更改锁状态时发生了进程间切换，那么 B 进程此时仍然可以获得锁进入临界区，最终的结果就是，A 和 B 都认为自己拿到了这个锁，都进入了临界区。究其原因，还是因为没有保证进程之间对于共享资源操作的顺序性。</p><p>第三种是利用「忙等待」的原理：利用一个全局变量实现两个进程间的同步，保证执行顺序。如，设置一个共享变量 turn，初始值为0。A 进程将通过一个 while 循环检查turn，当其值为0的时候进入临界区，出临界区的时候将其改为1。B 进程将通过一个 while 循环检查 turn，当其值为1的时候进入临界区，出临界区的时候将其改为0。利用这种机制就实现了两个进程严格的「同步」，轮换进入临界区。但是，这种实现有一个Edege Case 没有考虑到，如果两个进程执行的速度相差过大，就会导致速度较快的进程在离开临界区之后，一直在等待速度较慢的进程先进入临界区。此时，速度较快的进程在 CPU 时间片内执行一个死循环，浪费了 CPU 资源，而且最终执行的效率看起来已经完全取决于速度较慢的进程到底有多慢。</p><p>第四种则是在「忙等待」的基础上进行了一些改进，由原来的不断访问锁变量，查看是否可以进入临界区的方式变为：当访问锁变量不能进入临界区的时候就进入睡眠状态，将自身阻塞在临界区外。直到从临界区出来的进程唤醒它。这种方式的本质是实现了一对「同步原语」：Sleep/Wake。不过它的缺陷和使用锁变量是类似的：使用同步原语之前还是需要去访问一个共享变量来决定是否执行 Sleep or Wake。如果在「访问共享变量」和「执行 Sleep 原语」之间发生了进程间的切换，那么很有可能在另外一个进程从临界区出来之后执行 Wake，但是之前被切换的进程因没有执行 Sleep ，所以最终将导致这个信号丢失。等到下一次它重新获得 CPU 时间片的时候，会将自己Sleep，最终它将无法进入临界区。</p><p>讨论到现在为止，其实有一部分问题都已经解决了，比如：如何实现互斥，如何避免忙等待给系统带来的消耗。唯一一个还没有办法解决的其实就是对共享变量的「互斥访问」问题。而互斥访问这个东西，基本上在用户态下是不太可能做到的，因为内核才是大 Boss，他想把你调离 CPU 那你就是没机会再进行下去了。所以，借鉴上面第一种方案的思路，我们需要借助「屏蔽中断」这一特性来实现共享变量的「互斥访问」。这就必须要提及「信号量」的概念了。</p><p><b>Q: 什么是信号量？</b></p><p><b>A:</b></p><p>信号量是一个新的名字，它本质上其实就是我们之前所说的在睡眠和唤醒进程前访问的共享变量。之所以改了一个新的名字，是因为信号量附加的相关操作是通过系统调用+屏蔽中断来实现的，它可以保证将之前讨论的一些可能发生「竞争」的操作实现为一个原子操作（要么不执行，要么就都执行完）。</p><p>信号量是一个整型的变量，它的取值范围为[0, +无穷]，在当前的场景下，它的数值代表了还有多少进程处于睡眠状态中并等待被唤醒。它以系统调用的方式实现了两个操作：Up， Down。Up 操作为唤醒操作，Down 操作即为 Sleep 操作。它的工作机制大致是这样的：</p><p>对于 Down 操作来说，它在操作一个信号量之前会检查它是否大于0，如果大于0，则对信号量进行-1，然后进行剩余的操作；如果等于0，那么将进程睡眠。检查，修改，以及后续的操作（睡眠 or 继续）三者是作为一个原子性的操作来处理的。</p><p>对于 Up 操作来说，由于信号量的取值范围是到正无穷的，所以它在对信号量进行+1操作之前是不需要检查相应的值的。信号量一旦执行了Up 操作，就说明此时可以唤醒一个睡眠的进程执行了。而睡眠的进程在被唤醒的时候会使用 Down 操作对信号量-1，这样一增一减也就平衡了。所以，增加信号量的值，唤醒一个进程，两个步骤若被实现为一个原子操作，即可称作是一个 Up 操作。</p><p>上面所说的，基本上是依靠信号量实现了「互斥」的功能，从而可以解决进程之间的「竞争」问题。实际上，由于信号量是一个整型变量，它的取值范围比较大，所以可以利用它的计数功能实现进程间的「同步」，从而保证多进程的执行顺序。所以，对于生产者和消费者模型来说，如何在保证互斥的同时又保证了两者的执行顺序，信号量的使用起到了至关重要的作用。</p><p>信号量的出现极大的丰富了我们处理「互斥」和「同步」问题的方式。若你想解决「竞争」，则可以使用一个仅有「解锁，锁住」两种语义的信号量，我们通常称他为「互斥量」。「互斥量」的作用仅限于避免「临界区」同时多有个进程或者线程进入。若你想解决「同步问题」，则可以使用一个仅有「计数」语义的信号量。「计数量」的作用是在具有依赖关系的两个进程或线程中传递「计数信号」，当「计数量」的值未达到某个进程运行条件时，该进程就会被阻塞，反之会顺利的进行。</p><p><b>Q: 「互斥」+ 「条件」的另一种实现方式是什么？</b></p><p><b>A:</b></p><p>了解到目前为止，我们大致可以归纳出操作系统在处理「互斥」和「同步」的问题上究竟依赖的是什么思想了：</p><ol><li>互斥：通过某种实现，锁住临界区，防止临界区内同一时间被多个进线程访问。而且必须要借助内核的力量，防止进程间切换导致的「竞争」</li><li>同步：通过某种实现，在进线程间建立一种「通知」机制，可以按照一定的条件「睡眠」和「唤醒」某个进线程。「通知」机制的运行需要互斥的保护</li></ol><p>其实，实现「同步」一种比较简单的方式就是利用我们上面所说的信号量，而且这个信号量还有存储信号的功能，不会怕信号丢失。同步和互斥一般都是在一起使用的：互斥用于锁住临界区，同步用于保证执行顺序。但是在运用他们的时候，请一定要掌握好它们之间语义的差别：互斥作用于进程已经可以按照顺序执行但是执行过程中受阻，同步作用于进程是否可以顺序执行。这么说可能比较迷惑，我们来看下面一个例子（利用信号量解决生产者、消费者问题）：</p><div class=\"highlight\"><pre><code class=\"language-c\"><span class=\"n\">m_mux</span> <span class=\"c1\">// 互斥信号量\n</span><span class=\"c1\"></span><span class=\"n\">m_num_empty</span> <span class=\"c1\">// 缓冲区空闲位置个数\n</span><span class=\"c1\"></span><span class=\"n\">m_num_full</span> <span class=\"c1\">// 缓冲区有数据的位置个数\n</span><span class=\"c1\"></span><span class=\"n\">buf</span> <span class=\"c1\">// 缓冲区\n</span><span class=\"c1\"></span>\n<span class=\"kt\">void</span> <span class=\"n\">producer</span><span class=\"p\">(){</span>\n     <span class=\"n\">down</span><span class=\"p\">(</span><span class=\"n\">m_num_empty</span><span class=\"p\">)</span> <span class=\"c1\">// 缓冲区是否已满，可以生产消息\n</span><span class=\"c1\"></span>     <span class=\"n\">down</span><span class=\"p\">(</span><span class=\"n\">m_mux</span><span class=\"p\">)</span> <span class=\"c1\">// 是否可进入临界区\n</span><span class=\"c1\"></span>     <span class=\"n\">buf</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"c1\">// 生产消息\n</span><span class=\"c1\"></span>     <span class=\"n\">up</span><span class=\"p\">(</span><span class=\"n\">m_mux</span><span class=\"p\">)</span> <span class=\"c1\">// 离开临界区\n</span><span class=\"c1\"></span>     <span class=\"n\">up</span><span class=\"p\">(</span><span class=\"n\">m_num_full</span><span class=\"p\">)</span> <span class=\"c1\">// 增加有数据的位置个数\n</span><span class=\"c1\"></span><span class=\"p\">}</span> \n\n<span class=\"kt\">void</span> <span class=\"n\">comsumer</span><span class=\"p\">(){</span>\n     <span class=\"n\">down</span><span class=\"p\">(</span><span class=\"n\">m_num_full</span><span class=\"p\">)</span> <span class=\"c1\">// 缓冲区是否还有数据，可以消费\n</span><span class=\"c1\"></span>     <span class=\"n\">down</span><span class=\"p\">(</span><span class=\"n\">m_mux</span><span class=\"p\">)</span> <span class=\"c1\">// 是否可进入临界区\n</span><span class=\"c1\"></span>     <span class=\"n\">buf</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"c1\">// 生产消息\n</span><span class=\"c1\"></span>     <span class=\"n\">up</span><span class=\"p\">(</span><span class=\"n\">m_mux</span><span class=\"p\">)</span> <span class=\"c1\">// 离开临界区\n</span><span class=\"c1\"></span>     <span class=\"n\">up</span><span class=\"p\">(</span><span class=\"n\">m_num_empty</span><span class=\"p\">)</span> <span class=\"c1\">// 增加空闲的位置个数\n</span><span class=\"c1\"></span><span class=\"p\">}</span></code></pre></div><p>以目前的状态，上面这个例子运行起来是没有什么问题的。但是如果我们把同步和互斥的语义搞错，粗心一点将down 操作全部颠倒顺序就可能会发生死锁：当 m_mux 在减小 m_num_empty 信号前就被-1且缓冲区已经满了的时候，生产者将会被阻塞，但是此时生产者已经没有机会在释放互斥量了。而消费者会因为被互斥量阻塞的原因无法进入临界区消费，从而不能对m_num_empty信号量执行 up 操作释放生产者。这是一个标准的死锁的例子，它像我们说明了一个事实：在使用同步和互斥的时候要注意对信号量的操作顺序，否则会引起灾难。</p><p><b>Q: 什么是管程？什么是条件变量？</b></p><p><b>A:</b> </p><p>管程是一种高级的同步原语，它是编程语言的组成部分。管程是「互斥」和「同步」的结合体。其中「互斥」部分仍然由「互斥量」实现，但是它由编译器进行操作。使用管程的人只需要将临界区的代码注入到一个管程中，而不用关注它是怎么实现的。至于「同步」的部分，则是通过一个叫做「条件变量」的东西来实现的。条件变量是一种功能单一的信号量。它只负责实现「同步」。条件变量通常还伴随着一个过程的集合（Wait，Signal），其中它的 Wait 操作的实现是比较有意思的：Wait 操作在发现当前进程因某些条件不满足不能继续执行下去的时候，除了将当前的进程阻塞，还会将另外一个合适的进程调入到管程中来。并且在执行完 Wait 操作之前，不会被任何中断打断，从而引起竞争，因为管程帮我们做了「互斥」的保护。</p><blockquote> 其实笔者对于「条件变量」和「信号量」的区别也不是特别清楚，总觉得条件变量是信号量的一个 Special Case。因为条件变量能做的东西，信号量也一样可以做。所以，在这篇文章中，我们姑且就按照这样来理解。  <br/> </blockquote><p><b>Q: 通过信号量实现的「互斥」和「同步」在哪些场景下会有缺陷呢？</b></p><p><b>A:</b></p><p>使用信号量实现「互斥」和「同步」有一个比较大的限制：通过将共享变量放在共享内存，并且通过 TSL 等指令来保护这些变量的操作，以避免竞争。但是，当在一个由多个操作系统组成的分布式系统中，每个 CPU 都有自己的私有内存，且还可以通过网络互连，那么信号量的保护机制就将失效了。看起来，信号量并不能解决处于不同机器之间的进程的通信问题。</p><p>所以，操作系统又实现了两个新的原语：Send &amp;&amp; Receive。 通过字面的意思就可以看出，这是一种通过消息传递的方式来实现「互斥」和「同步」的。共享变量在通信双方的机器上都需要被「互斥」机制保护，这一点在单机上实现起来应该是比较简单的。发送进程可以调用 Send 原语发送信息，而接受进程可以调用 Receive 源于来接受消息。至于「阻塞」和「唤醒」则可以通过网络在双端传输共享变量来实现：如生产者进程在启动时调用 Receive  等待消费者向他传递缓冲区的空闲情况，因为它还不知道现在的情况是否可以传递消息，所以会进入阻塞状态。消费者在启动后，先调用 Send 将缓冲区的空闲情况发送给生产者，然后再调用 Receive 等待接受消息。生产者收到消息后会查看缓冲区的情况，如果确认可以发送，则调用 Send 向消费者发送消息。 在执行了一个循环之后，整个通信流程就变成了以下几个步骤重复执行：</p><ol><li>生产者发送完消息被阻塞</li><li>消费者接受并消费消息</li><li>消费者发送缓冲区空闲情况</li><li>消费者等待消息被阻塞</li><li>生产者接受到和缓冲区空闲情况有关的消息</li><li>生产者生产消息</li></ol><p><b>Q: 如果需要「同步」机制的不是一个进程而是一个进程组，我们需要怎么办呢？</b></p><p><b>A:</b> </p><p>信号量，管程都是针对于两个进程间通讯所遇到的问题的解决方案。但是，当同步机制作用于进程组的时候，问题似乎更加抽象了。如，现在一共有八个进程为一组。这里的同步是指，无论执行的速率，只有等到进程组内所有的进程都执行完了某一阶段逻辑，它们才能够继续向下运行。</p><p>所以，操作系统专门为进程组的同步创建了一个原语：屏障（barrier）。它也是通过一个系统调用来实现的，且最终操作的肯定还是一个进程组内多个进程共享的变量。以上面的进程组为例来描述一下屏障的工作机制：进程组中的每一个进程在执行完它自己的第一阶段逻辑之后都会调用 barrier 原语，此时，如果该进程不是进程组中最后一个执行完毕的进程，那么它会被挂起，直到最后一个进程执行完且调用了 barrier 原语之后，所有的进程才会被释放去执行第二阶段的逻辑。</p><p><b>Q: 什么是进程？</b></p><p><b>A:</b></p><p>进程其实是一个比较抽象的概念，它是用来描述多道程序设计系统中的一个工作单元。单纯的给进程下一个定义是没有任何意义的。比如现在所谓的标准答案：进程是操作系统中运行的程序。对于进程，我们更多的要理解它是一个「复合体」，它是一系列活动的组合。它是一个逻辑上的概念，并不是一个现实世界中具体的事物。这一点和 k8s 中的 pod很像。所以，我更倾向于将进程理解为操作系统中的一个复杂且基本的工作单元。</p><p><b>Q: 子进程被创建之后和父进程是如何隔离的？</b></p><p><b>A:</b></p><p>通常情况下，在 Linux 系统当中，一旦子进程被创建，那么子进程和父进程就会分别占有两块独立的地址空间。相互之间是隔离的，并且可以通过一些方式来进行通信或者共享某些资源。但是，在之后操作系统发展的过程当中，对于父子进程的创建过程可能会有一些优化，而不仅仅是粗暴的将父进程地址空间中所有的东西都 copy 一份给子进程。这里也是有一个比较重要的机制：COW（写时复制机制）。</p><p><b>Q: Linux 中的进程和 Windows 中有哪些不同？</b></p><p><b>A:</b></p><p>Linux 系统中的进程是有严格的「父子关系」的，并且所有的进程会以树形的层次结构组织起来。其中祖先进程可认为是 Init，它是进程树中的根。而 Windows 中的进程，无论父子，都是靠一个叫做「句柄」的概念对一个进程进行标识的，并且这个句柄是可以传递的。所以在 Windows 中，进程间没有严格的父子关系。</p><p><b>Q: 什么是线程？</b></p><p><b>A:</b> </p><p>线程是轻量级的进程。进程由操作系统来管理而线程由进程来管理。不同进程之间的地址空间是隔离的，但是不同线程之间的地址空间是共享的。一般来说，一个进程通常会有一个主线程，进程负责向内核申请线程运行所需要的资源和环境，而线程才是真正执行程序的单位。</p><p><b>Q: 有了进程为什么还需要线程？</b></p><p><b>A:</b></p><p>从程序性能的角度来说，很多程序在一个进程中都会做很多任务。这些任务可以大致的被划分为两类，一类是 I_O, 一类是计算。I_O 通常消耗的时间会比较长，对于只有主线程的进程来说，它会一直处于等待状态，内核分配给他的 CPU 时间片也会被白白的消耗。计算类的任务则会直接消耗 CPU 资源，最大限度的利用了已分配的时间片。所以，如果一个程序中同时包含这两类任务的话，计算类的任务很可能被 I_O 类的任务阻塞，最终导致整个程序的效率下降。因为线程是存在于进程的地址空间中的，如果可以在进程地址空间中创建多个线程，并且让这些线程重叠执行，分别去运行不同类型的任务，就可以在一定的 CPU 时间片内，将程序的效率尽可能的提高。通过上面的一些思考，我们甚至可以延伸出另外一个问题：多线程技术一定会对我们的程序产生积极的影响么？其实也不尽然。如果我们的程序中既包含大量的 I_O 操作，也包含大量的计算操作，那么多线程技术是可以提升我们程序的效率的。因为此时由于多个线程重叠的进行，最大限度的利用了 CPU 的时间片。如果我们的程序基本都是计算类的任务，很少有 I/O 操作，那么多线程的引入可能不会对提升程序的效率有太大的帮助。因为即使线程间的切换消耗再小，还是有 CPU 时间片上面的损耗的。同样，这个问题的思考方式还可以延伸到：多进程技术一定会对我们的程序有积极的影响么？</p><p>从资源共享的角度来说，不同进程间的地址是不同的，所以它们在共享一些资源的时候就会比较麻烦，可能需要借助第三方的东西，比如文件。然而对于同一个进程中的不同的线程来说，这种内存上的隔离是不存在的，它们可以很方便的去共享一些资源。看到这里你可能会说，在地址空间不隔离的条件下，多个线程对同一个资源可能会出现竞争的想象。对于这个问题，我们要明确两点：首先，线程间共享资源的初衷是让多个线程合作，而不是让它们竞争。其次，如果不可避免的发生了竞争，也可以通过一些互斥的机制来解决。</p><p>最后还要提及一点的就是，大多数操作系统对于多线程的实现都是在「用户态」下，且线程中维护的必要信息会较进程少很多。这就造成了线程是比进程更轻量级的。如果不可避免的发生频繁和切换操作，那么很明显线程在这种场景下会更具优势。</p><p><b>Q: 进程和线程之间的关系是什么？</b></p><p><b>A:</b></p><p>进程更倾向于从操作系统申请资源，并对这些资源进行统一的管理，提供一个良好的运行环境。线程则更注重利用已经分配好的资源运行程序。也就是说，实际上在 CPU 上调度执行的并不是进程而是线程。</p><p><b>Q: 如何实现线程？</b></p><p><b>A:</b></p><p>实现线程有两种思路：在用户态实现 or 在内核态实现。</p><p>当我们想在用户态实现「线程」的时候，就意味着「线程」或者说是「多线程」对于内核来讲应该是透明的。内核与具有单个控制线程的主进程还是按照原来的模式运行（进程模型）。所以，我们很自然的就能够想到，在用户态下需要一系列「过程」的集合来实现和线程有关的操作以及「多线程」技术。这个「过程」的集合可以被称作为是一种 Runtime 系统。</p><p>用户态 Runtime 系统的数和进程数成正比。每一个进程中都有一个 Runtime 去管理进程中的多个线程。它负责线程的创建，销毁。同时也要负责维护一张「线程表」，用于保存进程内部线程的运行状态。更重要的是，这个 Runtime 系统需要借助「线程表」进行线程间的切换，因为同一时刻只有一个线程可以获得 CPU 的时间片。其实，这样看起来，Runtime 运行的方式很像一个有限状态机。它将进程内的线程的状态保存至「线程表」中，当一个线程被调度到 CPU 上执行的时候，Runtime 就需要在线程表中读取和这个线程有关的信息；当一个线程要被调度离开 CPU 的时候，同样需要 Runtime 将此时的状态保存到线程表中，以便下一次复原运行的上下文环境。如果再类比一下进程和操作系统内核的关系就可以得知，用轻量级的进程来描述线程，真的是再合适不过了。</p><p>对于线程间的切换来说，它和进程间的切换的实现有所不同。进程间的切换，要不就是依靠中断机制，强行将进程从 CPU 上拿下来；要不就是等到该进程的 CPU 时间片被消耗完，调度系统切换新的进程上来。由于我们现在是在用户态实现线程，操作系统内核无法干预线程的相关操作。所以，我们需要在 Runtime 中实现一个过程，这个过程在调用之后可以主动的将 CPU 时间片让给其他处于就绪态的线程。这也就是 POSIX 线程标准中定义的 Pthread_yield 所要实现的功能。</p><p>在用户态实现 Runtime 的好处其实很明显：1. 之前看起来比较复杂的操作，如线程间的切换，都是在用户态下完成的，不需要内核的参与，所以肯定要比内核实现的版本效率要高 2. 由于这个 Runtime 是我们自己来实现的，所以它的可定制性是非常强的。我们甚至可以开发出自己的一套「线程调度策略」来保证我们的程序效率最大化。</p><p>在用户态实现 Runtime 的坏处其实都可以归结到一个问题上：阻塞，它既包括线程之间的阻塞也包括线程所在进程的阻塞。线程间的阻塞是指：当一个线程想要进行一些阻塞操作的时候，比如从键盘读取输入信息。如果让这个阻塞操作进行了且它所需要的条件一直没有被满足，那么该进程中其他可运行的线程在这一个 CPU 时间片上就没有机会再被运行了。这其实是不符合「多线程」技术发明的初衷的。进程的阻塞是指：当一个线程执行了一些阻塞系统调用的时候，不仅仅是其他的线程没有运行的机会了，整个进程都会因为进入阻塞态而被调离 CPU。这是一个非常严重的事情。而触发这种问题的 Case 也很常见：缺页中断（线程所需要的数据或者代码没有在内存页中找到而是在硬盘中）。此外，由于多线程间只能够通过主动调用 Pthread_yield 过程来实现切换操作，如果你的代码写的有 bug 的话，其他的线程就会处于「饥饿」或者「饿死」的状态。</p><p>内核态实现的 Runtime 系统的数量不再随着进程数的变化而变化。事实上，如果真的把线程拿到内核态来实现的话，线程额进程基本就没什么区别了。线程会和进程一样，在内核中有一个线程表，用来维护线程的运行情况。通过对比之前在用户态实现线程的缺点可以知道，如果将所有阻塞线程的调用全都以系统调用的形式来实现的话，线程间的切换就统一由内核来进行管理，它会选取一个合适的线程继续使用剩余的 CPU 时间片。这种阻塞调用既包括线程之间的阻塞也包括阻塞的系统调用。</p><p>虽然说，内核态实现 Runtime 开销比较大的问题是不可避免的。但是仍然可以做出一些优化，比如在线程的销毁操作上，如果一个线程需要被销毁，内核可以不进行真正的销毁操作，而是打上一个空闲线程的标记，并且由它统一管理。这样如果有线程创建需求的时候，有可以直接复用之前已经分配的资源。</p><p>很显然，内核态实现 Runtime 也是有很多缺点的。其中最被大家诟病的就是「开销」变大了。这个开销不仅仅是指时间上面的，还包括空间上面的。如：线程的数量一般都是要比进程多的，所以线程表的规模的增长速度会远远大于进程表。当规模大起来之后如何保证一个高效的读取和写入操作呢？毕竟引入线程和多线程相关概念的初衷是在合适的场景下能够提升程序的效率而不是拉低。</p><p>既然两者各有优劣，那么根据操作系统的一贯思想，就是最大化的将这两个方案的优点结合起来，产出一个普适性更强的方案：调度程序激活机制。它借助了用户态 Runtime 系统的优势：高效的进行线程间的切换。同时，在用户态下模拟「内核」线程的功能，防止因线程使用阻塞的系统调用而发生进程的切换。</p><p>调度程序激活机制启用后，内核会为每一个进程分配一个或多个的虚拟 CPU，用户态 Runtime 系统可以将线程分配到这些虚拟的 CPU 上。虚拟 CPU 代表这个进程可以使用的 CPU 核心数。当一个线程被同进程的另外一个线程所阻塞，它会被用户态 Runtime 系统处理，并调度新的进程运行。此时，不会发生用户态和内核态的切换，对于内核来说，这些操作都是透明的。当一个线程被进程之外的因素阻塞住时（阻塞的系统调用，缺页终端），内核会感知到这个问题，它会通知用户态 Runtime 系统，需要重新调度一个就绪的线程运行。而当阻塞的事件被完成的时候，内核也会将这个事件通知给用户态 Runtime 系统，让它自己来决定，下一步应该调度哪个线程运行。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-78a72959db3281d6888b5ec560ac1cd9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1264\" data-rawheight=\"384\" class=\"origin_image zh-lightbox-thumb\" width=\"1264\" data-original=\"https://pic2.zhimg.com/v2-78a72959db3281d6888b5ec560ac1cd9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1264&#39; height=&#39;384&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1264\" data-rawheight=\"384\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1264\" data-original=\"https://pic2.zhimg.com/v2-78a72959db3281d6888b5ec560ac1cd9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-78a72959db3281d6888b5ec560ac1cd9_b.jpg\"/></figure><p>这种内核调用用户态 Runtime 系统的机制被称作为「上行调用」。在CPU中用户空间为上层，内核为下层层，常规调用应该是上层调用下层，下层不应该调用上层，上行调用就是指内核调用用户空间的 Runtime 系统。</p><p><b>Q: 如何实现进程间的通信？</b></p><p><b>A:</b></p><p>两个进程或者线程间如果要进行通信，可能涉及到以下三个问题：</p><ol><li>如何传递信息？</li><li>如何防止「竞争」？</li><li>如何保证进/线程间执行的顺序？</li></ol><p><b>Q: 如何防止「竞争」？</b></p><p><b>A:</b></p><p>我们首先来看下如何在进程间通信过程中避免「竞争」的问题。「竞争」通常出现在两个进程或者线程同时要访问/修改一个共享资源的时候，除了同时读取可能还不会有问题之外，比如一读一写，两个都是写这种组合是肯定会引起资源的「竞争」的。这种「同时操作共享资源」的结果，完全取决于两个竞争者之间执行的时序。但是我们都清楚，在编程的世界中，是不能够对「时序」做任何假设的，因为它的随机性较大，很多「竞争」问题之所以难以 track，就是因为它出现的几率不固定，很难定位。</p><p>那现在问题就变成了：如何在使用共享资源的时候，同一时间只允许一个进程对其操作。这种行为有一个比较统一和抽象的称呼：「互斥」。而产生「竞争」问题的地方或者说要进行「互斥」改造的地方，我们统一把它成为「临界区」。「临界区」是一段操作共享资源的代码，如果能保证同一时间只有一个进程进入「临界区」，那么「竞争」的问题也就随之解决了。</p><p>第一种可实现互斥机制的技术是：屏蔽中断。这应该是最暴力的一种方式了，而且更多的是对于进程间「竞争」问题的解决方案。当一个进程进入临界区后，可以屏蔽所有的中断。此时， CPU 不会再切换进程，已经处在临界区的进程也不会受到影响，它可以放心的操作「共享资源」。但是这种方案的缺点也很明显：1. 进程一旦运行异常很可能从临界区中退不出来，一直不能切换其他进程  2.  在多 CPU 的情形下，除非把所有 CPU 都 disable 掉，否则还是有其他的 CPU 可以调度运行与其「竞争」的进程。</p><p>第二种是在软件层面的方案： 锁变量。进临界区之前查看是否可以获得锁，离开临界区之后释放锁。「获得」和「释放」的操作通过修改某一个变量实现。但是，这其实并没有什么卵用，在 CPU 可以对进程进行任意切换的前提下，锁变量就变成了另外一个「共享资源」。如：在 A 进程进入临界区获得锁之后想更改锁状态时发生了进程间切换，那么 B 进程此时仍然可以获得锁进入临界区，最终的结果就是，A 和 B 都认为自己拿到了这个锁，都进入了临界区。究其原因，还是因为没有保证进程之间对于共享资源操作的顺序性。</p><p>第三种是利用「忙等待」的原理：利用一个全局变量实现两个进程间的同步，保证执行顺序。如，设置一个共享变量 turn，初始值为0。A 进程将通过一个 while 循环检查turn，当其值为0的时候进入临界区，出临界区的时候将其改为1。B 进程将通过一个 while 循环检查 turn，当其值为1的时候进入临界区，出临界区的时候将其改为0。利用这种机制就实现了两个进程严格的「同步」，轮换进入临界区。但是，这种实现有一个Edege Case 没有考虑到，如果两个进程执行的速度相差过大，就会导致速度较快的进程在离开临界区之后，一直在等待速度较慢的进程先进入临界区。此时，速度较快的进程在 CPU 时间片内执行一个死循环，浪费了 CPU 资源，而且最终执行的效率看起来已经完全取决于速度较慢的进程到底有多慢。</p><p>第四种则是在「忙等待」的基础上进行了一些改进，由原来的不断访问锁变量，查看是否可以进入临界区的方式变为：当访问锁变量不能进入临界区的时候就进入睡眠状态，将自身阻塞在临界区外。直到从临界区出来的进程唤醒它。这种方式的本质是实现了一对「同步原语」：Sleep/Wake。不过它的缺陷和使用锁变量是类似的：使用同步原语之前还是需要去访问一个共享变量来决定是否执行 Sleep or Wake。如果在「访问共享变量」和「执行 Sleep 原语」之间发生了进程间的切换，那么很有可能在另外一个进程从临界区出来之后执行 Wake，但是之前被切换的进程因没有执行 Sleep，并没有收到这个信号，从而导致了 Wake 消息丢失的问题。等到下一次它重新获得 CPU 时间片的时候，会将自己Sleep，最终它将无法进入临界区。</p><p>讨论到现在为止，其实有一部分问题都已经解决了，比如：如何实现互斥，如何避免忙等待给系统带来的消耗。唯一一个还没有办法解决的其实就是对共享变量的「互斥访问」问题。而互斥访问这个东西，基本上在用户态下是不太可能做到的，因为内核才是大 Boss，他想把你调离 CPU 那你就是没机会再进行下去了。所以，借鉴上面第一种方案的思路，我们需要借助「屏蔽中断」这一特性来实现共享变量的「互斥访问」。这就必须要提及「信号量」的概念了。</p><p><b>Q: 什么是信号量？</b></p><p><b>A:</b></p><p>信号量是一个新的名字，它本质上其实就是我们之前所说的在睡眠和唤醒进程前访问的共享变量。之所以改了一个新的名字，是因为信号量附加的相关操作是通过系统调用+屏蔽中断来实现的，它可以保证将之前讨论的一些可能发生「竞争」的操作实现为一个原子操作。</p><p>信号量是一个整型的变量，它的取值范围为[0, +无穷]，在当前的场景下，它的数值代表了还有多少进程处于睡眠状态中并等待被唤醒。它以系统调用的方式实现了两个操作：Up， Down。Up 操作为唤醒操作，Down 操作即为 Sleep 操作。它的工作机制大致是这样的：</p><p>对于 Down 操作来说，它在操作一个信号量之前会检查它是否大于0，如果大于0，则对信号量进行-1，然后进行剩余的操作；如果等于0，那么将进程睡眠，但是此时 Down 操作还并没有结束，因为它还没有将被它阻塞的进程顺利的送出去。检查，修改，以及后续的操作（睡眠 or 继续）三者是作为一个原子性的操作来处理的。</p><p>对于 Up 操作来说，由于信号量的取值范围是到正无穷的，所以它在对信号量进行+1操作之前是不需要检查相应的值的。信号量一旦执行了Up 操作，就说明此时可以唤醒一个睡眠的进程执行了，而睡眠的进程在被唤醒的时候会使用 Down 操作对信号量-1，这样一增一减也就平衡了。所以，增加信号量的值，唤醒一个进程，两个步骤若被实现为一个原子操作，即可称作是一个 Up 操作。</p><p>上面所说的，基本上是依靠信号量实现了「互斥」的功能，从而可以解决进程之间的「竞争」问题。实际上，由于信号量是一个整型变量，它的取值范围比较大，所以可以利用它的计数功能实现进程间的「同步」，从而保证多进程的执行顺序。所以，对于生产者和消费者模型来说，如何在保证互斥的同时又保证了两者的执行顺序，信号量的使用起到了至关重要的作用。</p><p>信号量的出现极大的丰富了我们处理「互斥」和「同步」问题的方式。若你想解决「竞争」，则可以使用一个仅有「解锁，锁住」两种语义的信号量，我们通常称他为「互斥量」。「互斥量」的作用仅限于避免「临界区」同时多有个进程或者线程进入。若你想解决「同步问题」，则可以使用一个仅有「计数」语义的信号量。「计数量」的作用是在具有依赖关系的两个进程或线程中传递「计数信号」，当「计数量」的值未达到某个进程运行条件时，该进程就会被阻塞，反之会顺利的进行。</p><p><b>Q: 「互斥」+ 「条件」的另一种实现方式是什么？</b></p><p><b>A:</b></p><p>了解到目前为止，我们大致可以归纳出操作系统在处理「互斥」和「同步」的问题上究竟依赖的是什么思想了：</p><ol><li>互斥：通过某种实现，锁住临界区，防止临界区内同一时间被多个进线程访问</li><li>同步：通过某种实现，在进线程间建立一种「通知」机制，可以按照一定的条件「睡眠」和「唤醒」某个进线程。「通知」机制的运行需要互斥的保护</li></ol><p>其实，实现「同步」一种比较简单的方式就是利用我们上面所说的信号量，而且这个信号量还有存储信号的功能，不会怕信号丢失。同步和互斥一般都是在一起使用的：互斥用于锁住临界区，同步用于保证执行顺序。但是在运用他们的时候，请一定要掌握好它们之间语义的差别：互斥作用于进程已经可以执行但是执行过程中受阻，同步作用于进程是否可以执行。这么说可能比较迷惑，我们来看下面一个例子（利用信号量解决生产者、消费者问题）：</p><div class=\"highlight\"><pre><code class=\"language-c\"><span class=\"n\">m_mux</span> <span class=\"c1\">// 互斥信号量\n</span><span class=\"c1\"></span><span class=\"n\">m_num_empty</span> <span class=\"c1\">// 缓冲区空闲位置个数\n</span><span class=\"c1\"></span><span class=\"n\">m_num_full</span> <span class=\"c1\">// 缓冲区有数据的位置个数\n</span><span class=\"c1\"></span><span class=\"n\">buf</span> <span class=\"c1\">// 缓冲区\n</span><span class=\"c1\"></span>\n<span class=\"kt\">void</span> <span class=\"n\">producer</span><span class=\"p\">(){</span>\n     <span class=\"n\">down</span><span class=\"p\">(</span><span class=\"n\">m_num_empty</span><span class=\"p\">)</span> <span class=\"c1\">// 缓冲区是否已满，可以生产消息\n</span><span class=\"c1\"></span>     <span class=\"n\">down</span><span class=\"p\">(</span><span class=\"n\">m_mux</span><span class=\"p\">)</span> <span class=\"c1\">// 是否可进入临界区\n</span><span class=\"c1\"></span>   <span class=\"n\">buf</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"c1\">// 生产消息\n</span><span class=\"c1\"></span>   <span class=\"n\">up</span><span class=\"p\">(</span><span class=\"n\">m_mux</span><span class=\"p\">)</span> <span class=\"c1\">// 离开临界区\n</span><span class=\"c1\"></span>     <span class=\"n\">up</span><span class=\"p\">(</span><span class=\"n\">m_num_full</span><span class=\"p\">)</span> <span class=\"c1\">// 增加有数据的位置个数\n</span><span class=\"c1\"></span><span class=\"p\">}</span> \n\n<span class=\"kt\">void</span> <span class=\"n\">comsumer</span><span class=\"p\">(){</span>\n     <span class=\"n\">down</span><span class=\"p\">(</span><span class=\"n\">m_num_full</span><span class=\"p\">)</span> <span class=\"c1\">// 缓冲区是否还有数据，可以消费\n</span><span class=\"c1\"></span>     <span class=\"n\">down</span><span class=\"p\">(</span><span class=\"n\">m_mux</span><span class=\"p\">)</span> <span class=\"c1\">// 是否可进入临界区\n</span><span class=\"c1\"></span>   <span class=\"n\">buf</span><span class=\"p\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"c1\">// 生产消息\n</span><span class=\"c1\"></span>   <span class=\"n\">up</span><span class=\"p\">(</span><span class=\"n\">m_mux</span><span class=\"p\">)</span> <span class=\"c1\">// 离开临界区\n</span><span class=\"c1\"></span>     <span class=\"n\">up</span><span class=\"p\">(</span><span class=\"n\">m_num_empty</span><span class=\"p\">)</span> <span class=\"c1\">// 增加空闲的位置个数\n</span><span class=\"c1\"></span><span class=\"p\">}</span></code></pre></div><p>以目前的状态，上面这个例子运行起来是没有什么问题的。但是如果我们把同步和互斥的语义搞错，粗心一点将down 操作全部颠倒顺序就可能会发生死锁：当 m_mux 在减小 m_num_empty 信号前就被-1且缓冲区已经满了的时候，生产者将会被阻塞，但是此时生产者已经没有机会在释放互斥量了。而消费者会因为被互斥量阻塞的原因无法进入临界区消费，从而不能对m_num_empty信号量执行 up 操作环境生产者。这是一个标准的死锁的例子，它像我们说明了一个事实：在使用同步和互斥的时候要注意对信号量的操作顺序，否则会引起灾难。</p><p><b>Q: 什么是管程？什么是条件变量？</b></p><p><b>A:</b> </p><p>管程是一种高级的同步原语，它是编程语言的组成部分。管程是「互斥」和「同步」的结合体。其中「互斥」部分仍然由「互斥量」实现，但是它由编译器进行操作。使用管程的人只需要将临界区的代码注入到一个管程中，而不用关注它是怎么实现的。而「同步」的部分，则是通过一个叫做「条件变量」的东西来实现的。条件变量是一种功能单一的信号量。它只负责实现「同步」。条件变量通常还伴随着一个过程的集合，其中它的 Wait 操作的实现是比较有意思的：Wait 操作在发现当前进程因某些条件不满足不能继续执行下去的时候，除了将当前的进程阻塞，还会将另外一个合适的进程调入到管程中来。并且在执行完 Wait 操作之前，不会被任何中断打断，从而引起竞争，因为管程帮我们做了「互斥」的保护。</p><blockquote> 其实笔者对于「条件变量」和「信号量」的区别也不是特别清楚，总觉得条件变量是信号量的一个 Special Case。因为条件变量能做的东西，信号量也一样可以做。所以，在这篇文章中，我们姑且就按照这样来理解。  <br/> </blockquote><p><b>Q: 通过信号量实现的「互斥」和「同步」在哪些场景下会有缺陷呢？</b></p><p><b>A:</b></p><p>使用信号量实现「互斥」和「同步」有一个比较大的限制：通过将共享变量放在共享内存，并且通过 TSL 等指令来保护这些变量的操作，以避免竞争。但是，当在一个由多个操作系统组成的分布式系统中，每个 CPU 都有自己的私有内存，且还可以通过网络互连，那么信号量的保护机制就将失效了。看起来，信号量并不能解决处于不同机器之间的进程的通信问题。</p><p>所以，操作系统又实现了两个新的原语：Send &amp;&amp; Receive。 通过字面的意思就可以看出，这是一种通过消息传递的方式来实现「互斥」和「同步」的。共享变量在通信双方的机器上都需要被「互斥」机制保护，这一点在单机上实现起来应该是比较简单的。发送进程可以调用 Send 原语发送信息，而接受进程可以调用 Receive 源于来接受消息。至于「阻塞」和「唤醒」则可以通过网络在双端传输共享变量来实现：如生产者进程在启动时调用 Receive  等待消费者向他传递缓冲区的空闲情况，因为它还不知道现在的情况是否可以传递消息，所以会进入阻塞状态。消费者在启动后，先调用 Send 将缓冲区的空闲情况发送给生产者，然后再调用 Receive 等待接受消息。生产者收到消息后会查看缓冲区的情况，如果确认可以发送，则调用 Send 向消费者发送消息。 在执行了一个循环之后，整个通信流程就变成了以下几个步骤重复执行：</p><ol><li>生产者发送完消息被阻塞</li><li>消费者接受并消费消息</li><li>消费者发送缓冲区空闲情况</li><li>消费者等待消息被阻塞</li><li>生产者接受到和缓冲区空闲情况有关的消息</li><li>生产者生产消息</li></ol><p><b>Q: 如果需要「同步」机制的不是一个进程而是一个进程组，我们需要怎么办呢？</b></p><p><b>A:</b> </p><p>信号量，管程都是针对于两个进程间通讯所遇到的问题的解决方案。但是，当同步机制作用于进程组的时候，问题似乎更加抽象了。如，现在一共有八个进程为一组。这里的同步是指，无论执行的速率，只有等到进程组内所有的进程都执行完了某一阶段逻辑，它们才能够继续向下运行。</p><p>所以，操作系统专门为进程组的同步创建了一个原语：屏障（barrier）。它也是通过一个系统调用来实现的，且最终操作的肯定还是一个进程组内多个进程共享的变量。以上面的进程组为例来描述一下屏障的工作机制：进程组中的每一个进程在执行完它自己的第一阶段逻辑之后都会调用 barrier 原语，此时，如果该进程不是进程组中最后一个执行完毕的进程，那么它会被挂起，直到最后一个进程执行完且调用了 barrier 原语之后，所有的进程才会被释放去执行第二阶段的逻辑。</p>", 
            "topic": [
                {
                    "tag": "线程", 
                    "tagLink": "https://api.zhihu.com/topics/19619468"
                }, 
                {
                    "tag": "进程", 
                    "tagLink": "https://api.zhihu.com/topics/19634510"
                }, 
                {
                    "tag": "操作系统", 
                    "tagLink": "https://api.zhihu.com/topics/19552686"
                }
            ], 
            "comments": [
                {
                    "userName": "干豆腐", 
                    "userLink": "https://www.zhihu.com/people/5900eb686dd3f894dc2bd948d6687b5d", 
                    "content": "厉害了，想不到能在哔呼看到这么硬核的知识，膜拜<br>如果可以的话，还望大佬分享下Linux原理方面的书籍", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "时空隧道", 
                    "userLink": "https://www.zhihu.com/people/d005030bc0021a0e39fa17d7fdb1b855", 
                    "content": "<p>一篇文章的内容重复两次为什么？？</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }
    ], 
    "url": "https://zhuanlan.zhihu.com/developrds"
}
