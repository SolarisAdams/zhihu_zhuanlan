{
    "title": "机器学习笔记", 
    "description": "机器学习笔记", 
    "followers": [
        "https://www.zhihu.com/people/ma-hu-67-97", 
        "https://www.zhihu.com/people/echo-27-86", 
        "https://www.zhihu.com/people/wang-bo-36-30-42", 
        "https://www.zhihu.com/people/zzx-47-7", 
        "https://www.zhihu.com/people/zhang-zi-yi-78-17", 
        "https://www.zhihu.com/people/guo-gong-yin", 
        "https://www.zhihu.com/people/ganko", 
        "https://www.zhihu.com/people/liu-guang-qun-72", 
        "https://www.zhihu.com/people/xu-jia-yun-36", 
        "https://www.zhihu.com/people/zrant", 
        "https://www.zhihu.com/people/rui-jia-33-6", 
        "https://www.zhihu.com/people/wu-qi-mo-50", 
        "https://www.zhihu.com/people/jiang-you-46-54", 
        "https://www.zhihu.com/people/zhao-hu-41-13", 
        "https://www.zhihu.com/people/fen-nen-shao-nu-xin-de-mou-xiao-liu", 
        "https://www.zhihu.com/people/eureka-yang", 
        "https://www.zhihu.com/people/hao-tian-cai", 
        "https://www.zhihu.com/people/yu-guo-guo-35", 
        "https://www.zhihu.com/people/qian-qiu-xue-16", 
        "https://www.zhihu.com/people/rabasses2016", 
        "https://www.zhihu.com/people/xiao-tao-93-25", 
        "https://www.zhihu.com/people/zhang-bi-hui-50", 
        "https://www.zhihu.com/people/mao-lao-shi-94-66", 
        "https://www.zhihu.com/people/cory-76-5", 
        "https://www.zhihu.com/people/wa-ka-ka-27-54", 
        "https://www.zhihu.com/people/benying", 
        "https://www.zhihu.com/people/jnutangyin", 
        "https://www.zhihu.com/people/shen-gao-deng-yu-ti-zhong", 
        "https://www.zhihu.com/people/yuan-jhen", 
        "https://www.zhihu.com/people/xue-hua-piao-51"
    ], 
    "article": [
        {
            "url": "https://zhuanlan.zhihu.com/p/51010145", 
            "userName": "陈德龙", 
            "userLink": "https://www.zhihu.com/people/fb342217a29a19462616c21ace65a810", 
            "upvote": 1, 
            "title": "用于图像的卷积神经网络架构的发展", 
            "content": "<p>注：本文为本人在信息科技前沿课堂的随堂论文。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>用于图像的卷积神经网络发展的综述</p><p>A review of the development of convolutional neural networks for image recognition</p><p>陈德龙 河海大学 计算机与信息学院 </p><p>2017级计算机一班 1706010120</p><p class=\"ztext-empty-paragraph\"><br/></p><p>摘要：随着互联网时代的到来，网络中获取大量图片作为训练数据集成为可能。计算机运算能力不断增长，使得大规模的深度神经网络训练成为了可能，这两个因素促使了卷积神经网络在近年来重新活跃了起来。在各种类型的深度学习模型当中，卷积神经网络是最先得到最深入研究的。早期由于缺乏训练数据和计算能力，要在不产生过拟合的情况下训练出高性能卷积神经网络是很困难的。ImageNet这样的大规模标记数据的出现和GPU计算性能的快速提高，使得对卷积神经网络的研究迅速井喷。卷积神经网络的功能日渐丰富和强大，从图像分类到人脸识别、风格迁移、图像标注等等。本文将纵览卷积神经网络的发展，同时介绍卷积神经网络在图像方面的应用。</p><p>关键词：深度学习、卷积神经网络</p><p class=\"ztext-empty-paragraph\"><br/></p><p>Abstract: With the advent of the Internet era, it becomes possible to acquire a large number of pictures in the network as training data integration. The increasing computing power of the computer makes large-scale deep neural network training possible. These two factors have prompted the convolutional neural network to re-energize in recent years. Among various types of deep learning models, convolutional neural networks are the first to get the most in-depth study. In the early days, due to the lack of training data and computational power, it was difficult to train a high-performance convolutional neural network without over-fitting. The emergence of large-scale tag data such as ImageNet and the rapid improvement of GPU computing performance have led to rapid research on convolutional neural networks. Applications of convolutional neural networks are increasingly rich and powerful, ranging from image classification to face recognition, style migration, image annotation, and more. This article will provide an overview of the development of convolutional neural networks and introduce the application of convolutional neural networks in image processing.</p><p>Keywords: Deep learning, Convolutional neural network</p><p class=\"ztext-empty-paragraph\"><br/></p><p>1. 引言</p><p>\t卷积神经网络（Convolutional Neural Network, CNN）是一种常见的深度学习网络架构，受生物自然视觉认知机制启发而来。1959年，Hubel &amp; Wiesel发现了大脑视觉系统的、信息处理的分级架构。在20世纪末[1]设计了卷积网络并将其应用于手写数字识别中后，卷积神经网络技术并没有取得研究人员的足够重视。受限于数据量与计算能力，卷积神经网络在2012年[2]后，卷积神经网络才成为学者的研究热点。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>2. 深度学习与卷积神经网络</p><p>2.1 深度学习</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-e1e5ab3de83a808b24d5886932ce1666_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"550\" data-rawheight=\"297\" class=\"origin_image zh-lightbox-thumb\" width=\"550\" data-original=\"https://pic3.zhimg.com/v2-e1e5ab3de83a808b24d5886932ce1666_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;550&#39; height=&#39;297&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"550\" data-rawheight=\"297\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"550\" data-original=\"https://pic3.zhimg.com/v2-e1e5ab3de83a808b24d5886932ce1666_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-e1e5ab3de83a808b24d5886932ce1666_b.jpg\"/></figure><p>深度学习（Deep Learning）是由机器学习（Machine Learning）发展而来的，应用深度神经网络的一类算法的总称。神经网络是一种能对任意函数进行近似学习与拟合的机器学习模型，由于所学习的函数的复杂性（非凸函数），神经网络使用梯度下降对于损失函数进行优化。实际上，神经网络算法在20世纪80年代就已经被Hinton 和 LeCun提出与发展，但受限于当时的计算机计算能力与训练数据量，多层深度神经网络在近年才被重新提出并快速发展。</p><p>图1</p><p class=\"ztext-empty-paragraph\"><br/></p><p>神经网络的实质是对于复杂多元函数的局部线性拟合，拟合过程分为前馈与反馈。如，前馈中四个输入作为输入神经元，与右侧隐藏层的每一个神经元全连接，将其值传入隐藏层的神经元中。神经元在对来自输入层的细胞进行处理后，类似地将其值继续向下一层传递，直到最后得到输出层。每个神经元对于来自上一层各个神经元的数值进行的处理分为两步：第一步将各个上层值线性组合，第二部将第一步得到的数值传入激活函数。激活函数的输入便为这个神经元的输出。激活函数根据不同的使用情形而定，历史上曾经流行使用Tanh函数，当今一般使用ReLU以及sigmoid函数。反馈时，使用损失函数来计算神经网络输出与真实值的差距，并计算各层神经元线性组合的参数对于该损失函数的偏导数（线性组合与激活函数均可导），根据偏导数对线性组合的各个参数进行调整，一次前馈-反馈训练结束。实际应用中，神经网络通常需要经过几万次训练才能达到可以接受的性能。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>2.2 卷积神经网络</p><p>一般地，一个卷积神经网络包括三个部分：卷积层、池化层和全连接层。</p><p>卷积层（Convolutions layer）：用于学习输入数据的特征表示，卷积层由很多的卷积核（convolutional kernel）组成，每个卷积层对于上层输出的矩阵进行卷积运算（不同于数学中严格的卷积，深度学习中一般不先将卷积核倒置），得到的结果传入激活函数处理，即得该卷积核的输出。</p><p>池化层（Pooling layer）：用于降低卷积层输出的特征向量，同时改善结果，使结构不容易出现过拟合。典型的操作包括平均池化和最大化池化。通过卷积层与池化层，我可以获得更多的抽象特征。</p><p>全连接层（Full connected layer）：将卷积层和池化层堆叠起来以后，就能够形成一层或多层全连接层，这样就能够实现高阶的推理能力，在整个卷积神经网络中起到“分类器”的作用。如果说卷积层、池化层和激活函数层等操作是将原始数据映射到隐层特征空间的话，全连接层则起到将学到的“分布式特征表示”映射到样本标记空间的作用。常用的非线性激活函数有sigmoid、tanh、relu等等，前两者sigmoid/tanh比较常见于全链接层，后者relu常见于卷积层。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>3. 卷积神经网络架构的发展</p><p>3.1 LeNet-5</p><p>LeNet-5是现代卷积神经网络架构的奠基者，其结构如。LeNet-5被应用于手写数字识别，取得了很大的成功。其训练集MNIST（图3）成为了当今深度学习算法的一个试验标准。</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-d69f9e0b5775c1e69971ab471a7374d1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"540\" data-rawheight=\"530\" class=\"origin_image zh-lightbox-thumb\" width=\"540\" data-original=\"https://pic2.zhimg.com/v2-d69f9e0b5775c1e69971ab471a7374d1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;540&#39; height=&#39;530&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"540\" data-rawheight=\"530\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"540\" data-original=\"https://pic2.zhimg.com/v2-d69f9e0b5775c1e69971ab471a7374d1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-d69f9e0b5775c1e69971ab471a7374d1_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-fce6a9de938386429e9fbe77f4bb7aaf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1298\" data-rawheight=\"372\" class=\"origin_image zh-lightbox-thumb\" width=\"1298\" data-original=\"https://pic4.zhimg.com/v2-fce6a9de938386429e9fbe77f4bb7aaf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1298&#39; height=&#39;372&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1298\" data-rawheight=\"372\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1298\" data-original=\"https://pic4.zhimg.com/v2-fce6a9de938386429e9fbe77f4bb7aaf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-fce6a9de938386429e9fbe77f4bb7aaf_b.jpg\"/></figure><p>图2</p><p>图3</p><p>3.2 AlexNet</p><p>AlexNet[2]是以其文章一作姓氏命名的一种神经网络架构。尽管现在看来AlexNet的架构（）稍显粗糙与简陋，但该模型在2012年李飞飞创办的ImageNet图像识别大赛中获得了较先前最高水平大幅提高的成绩，为学者所瞩目。AlexNet的出现也标志着卷积神经网络在图像处理领域的复兴。</p><p>AlexNet使用了许多优化技巧来保证卷积神经网络的正常训练，包括使用节省计算资源的ReLU作为激活函数，使用多个GPU进行训练，局部响应归一化（后被认为无必要），Dropout以及重叠池化方法。</p><p>图4</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-2b34f5f28b5c9e6b10adec958c025ff6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1234\" data-rawheight=\"380\" class=\"origin_image zh-lightbox-thumb\" width=\"1234\" data-original=\"https://pic3.zhimg.com/v2-2b34f5f28b5c9e6b10adec958c025ff6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1234&#39; height=&#39;380&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1234\" data-rawheight=\"380\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1234\" data-original=\"https://pic3.zhimg.com/v2-2b34f5f28b5c9e6b10adec958c025ff6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-2b34f5f28b5c9e6b10adec958c025ff6_b.jpg\"/></figure><p>3.3 VGG</p><p>VGGNet之所以闻名于世，是因为其赢得了2014年的ImageNet大赛。VGGNet 将网络的深度扩展到了19层，并且在每个卷积层使用了3x3这种小尺寸的卷积核。文章的实验证明，缩小卷积核尺寸，增多层数可以大大提升卷积神经网络的性能。</p><p>3.4 GoogLenet与inception</p><p>GoogLenet的名字来自于谷歌公司以及卷积神经网络的鼻祖LeNet。其另一个名字为inception网络。在该文章的参考文献中，作者引用了电影盗梦空间中的图片，生动形象地说明了深度对于卷积神经网络的重要性。收到VGGNet的启发，inception网络将卷积神经网络的层数继续增加，并设计了许多精致的细节来确保这一庞大的神经网络的性能达到最优(图6)。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-869f770d3fd83f645f3ecede043b4f7f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"400\" data-rawheight=\"226\" class=\"content_image\" width=\"400\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;400&#39; height=&#39;226&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"400\" data-rawheight=\"226\" class=\"content_image lazy\" width=\"400\" data-actualsrc=\"https://pic4.zhimg.com/v2-869f770d3fd83f645f3ecede043b4f7f_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>图5</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>图6</p><p>图6</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-9c24bea9b5bcc9b4c71fa8b199724839_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1997\" data-rawheight=\"2497\" class=\"origin_image zh-lightbox-thumb\" width=\"1997\" data-original=\"https://pic2.zhimg.com/v2-9c24bea9b5bcc9b4c71fa8b199724839_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1997&#39; height=&#39;2497&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1997\" data-rawheight=\"2497\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1997\" data-original=\"https://pic2.zhimg.com/v2-9c24bea9b5bcc9b4c71fa8b199724839_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-9c24bea9b5bcc9b4c71fa8b199724839_b.jpg\"/></figure><p>3.5 ResNet</p><p>随着网络的加深，出现了训练集准确率下降的现象，我们可以确定这不是由于Overfit过拟合造成的(过拟合的情况训练集应该准确率很高)；所以[5]的作者针对这个问题提出了一种新的网络，名为深度残差网络，它允许网络尽可能的加深，其中引入了全新的结构如。残差网络在卷积网络的深度扩增上登峰造极，试验了带有残差模块的多达152层的卷积神经网络。同时文章也利用相似的残差模块构建34、50、101层的网络，对比发现152层的卷积神经网络的表现最佳。（）</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-d3da6946b41bf6db866760cbf2521278_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1311\" data-rawheight=\"745\" class=\"origin_image zh-lightbox-thumb\" width=\"1311\" data-original=\"https://pic1.zhimg.com/v2-d3da6946b41bf6db866760cbf2521278_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1311&#39; height=&#39;745&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1311\" data-rawheight=\"745\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1311\" data-original=\"https://pic1.zhimg.com/v2-d3da6946b41bf6db866760cbf2521278_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-d3da6946b41bf6db866760cbf2521278_b.jpg\"/></figure><p>图7</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>图8</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-32e06c659826c55a50ecf45950320841_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"386\" data-rawheight=\"2416\" class=\"content_image\" width=\"386\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;386&#39; height=&#39;2416&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"386\" data-rawheight=\"2416\" class=\"content_image lazy\" width=\"386\" data-actualsrc=\"https://pic2.zhimg.com/v2-32e06c659826c55a50ecf45950320841_b.jpg\"/></figure><p>3.6 Capsule</p><p>Hinton提出的Capsule网络[6]对于卷积神经网络进行了大胆的革新。作者使用capsule代替传统的神经元，在capsule中的张量，方向表不同特性，长度代表存在的概率（通过squash函数将张量长度进行放缩，使其最大值为1）。作者使用capsule网络对于重叠的MNIST数据集进行训练，结果表明，十分轻量级的CapsuleNet即可达到传统的卷积网络的效果，且该模型更擅长于处理重叠的字符，展示了该模型巨大的潜力。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>4.总结</p><p>近年来，卷积神经网络 的权值共享、可训练参数少、鲁棒性强等优良特性使其受到了许多研究者的关注。卷积神经网络 通过权值共享减少了需要训练的权值个数、降低了网络的计算复杂度，同时通过池化操作使得网络对输入的局部变换具有一定的不变性如平移不变性、缩放不变性等，提升了网络的泛化能力。卷积神经网络将原始数据直接输入到网络中，然后隐性地从训练数据中进行网络学习，避免了手工提取特征、从而导致误差累积，其整个分类过程是自动的。虽然卷积神经网络 所具有的这些特点使其已被广泛应用于各种领域中。特别是模式识别与人工智能领域。</p><p>但是 卷积神经网络仍有许多工作需要进一步研究。例如，尽管 卷积神经网络 在许多领域如计算机视觉上已经取得了令人满意的成果，但是仍然不能够很好地理解其基本理论。对于一个具体的任务，仍很难确定哪种网络结构，使用多少层，每一层使用多少个神经元等才是合适的。仍然需要详细的知识来选择合理的值如学习率、正则化的强度等。尽管依赖于计算机制的 卷积神经网络 模型是否与灵长类视觉系统相似仍待确定，但是通过模仿和纳入灵长类视觉系统也能使 卷积神经网络 模型具有进一步提高性能的潜力。</p><p>总的来说，卷积神经网络 虽然还有许多有待解决的问题，但是这不影响今后它在模式识别与人工智能等领域中进一步的发展与应用，它在未来很长的一段时间内仍然会是人们研究的一个热点。新的理论和技术的纳入以及新成果的不断出现也会使它能够应用于更多新的领域中。</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>5. 参考文献</p><ol><li>Lecun Y, Bottou L, Bengio Y, et al. Gradient-based learning applied to document recognition[J]. Proceedings of the IEEE, 1998, 86(11):2278-2324.</li><li>Krizhevsky A, Sutskever I, Hinton G E. ImageNet classification with deep convolutional neural networks[C]// International Conference on Neural Information Processing Systems. Curran Associates Inc. 2012:1097-1105.</li><li>K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale image recognition,” in ICLR, 2015.</li><li>C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich, “Going deeper with convolutions,” CoRR, vol. abs/1409.4842, 2014.</li><li>He K, Zhang X, Ren S, et al. Deep Residual Learning for Image Recognition[J]. 2015:770-778.</li><li>Sabour S, Frosst N, Hinton G E. Dynamic Routing Between Capsules[J]. 2017.</li><li>Matrix capsules with EM routing</li><li>Guo Yan-Ming, Liu Yu, Ard Oerlemans, et al. Deep learning for visual nderstanding: a review. Neurocomputing, 2016,187(Special Issue):27-48</li></ol>", 
            "topic": [
                {
                    "tag": "卷积神经网络（CNN）", 
                    "tagLink": "https://api.zhihu.com/topics/20043586"
                }
            ], 
            "comments": [
                {
                    "userName": "Tony", 
                    "userLink": "https://www.zhihu.com/people/4121c3973a09f5daa83a0117854fa738", 
                    "content": "不错 不错 辛苦啦 下来是densenet、senet", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/44492582", 
            "userName": "陈德龙", 
            "userLink": "https://www.zhihu.com/people/fb342217a29a19462616c21ace65a810", 
            "upvote": 2, 
            "title": "机器学习笔记（七）k-Means", 
            "content": "<h2>﻿零、写在前面</h2><p>参考资料：</p><ul><li>《机器学习》周志华</li><li>《机器学习实战》Peter Harrington</li><li>斯坦福 CS 229 吴恩达</li><li><a href=\"https://link.zhihu.com/?target=http%3A//blog.pluskid.org/%3Fp%3D17\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">漫谈 Clustering (1): k-means</a></li></ul><h2>一、算法原理</h2><p>k-Means是一种十分简单的算法，一张图就可以解释清楚。 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-13129ec94678e2adef15761a8cd6ea2b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2459\" data-rawheight=\"1654\" class=\"origin_image zh-lightbox-thumb\" width=\"2459\" data-original=\"https://pic4.zhimg.com/v2-13129ec94678e2adef15761a8cd6ea2b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2459&#39; height=&#39;1654&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2459\" data-rawheight=\"1654\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2459\" data-original=\"https://pic4.zhimg.com/v2-13129ec94678e2adef15761a8cd6ea2b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-13129ec94678e2adef15761a8cd6ea2b_b.jpg\"/></figure><p>算法流程（上图k=2）：</p><div class=\"highlight\"><pre><code class=\"language-text\">图a 原始数据\n图b 随机选取k个点作为类别中心\n图c 对于每个原始数据的点，把它归为最近的类别中心的那一类\n图d 计算每簇点的平均值，将类别中心移至均值点\n图e 对于每个原始数据的点，把它归为最近的类别中心的那一类\n图f 计算每簇点点的平均值，将类别中心移至均值点\n循环至收敛</code></pre></div><h2>二、收敛性</h2><p>考虑平方误差： <img src=\"https://www.zhihu.com/equation?tex=E+%3D+%5Csum%5Ek_i+%5Csum_%7Bx%5Cin+c_i%7D+%7C%7Cx+-+%5Cmu_i%7C%7C%5E2_2+%5C%5C+%E5%85%B6%E4%B8%AD+%5Cmu_i+%E6%98%AF%E7%AC%ACi%E7%B0%87%E7%9A%84%E5%9D%87%E5%80%BC%E7%82%B9\" alt=\"E = \\sum^k_i \\sum_{x\\in c_i} ||x - \\mu_i||^2_2 \\\\ 其中 \\mu_i 是第i簇的均值点\" eeimg=\"1\"/> 上面的误差可以衡量聚类结果的好坏。</p><p>实际上，k-Means算法的循环中，先固定均值向量不动，改变x的类别，再固定类别不动，改变均值向量。而每一步‘固定——改变’步骤，改变都是在做argmin（归为最近的类别中心的那一类、将类别中心移至均值点，都是尽可能地降低平方误差E）。也就是说，这一算法实际上是对平方误差E，用做坐标下降法求极值。每一步都是argmin可以保证在聚类过程中平方误差E是单调递减的，这也就确保了收敛性。</p><h2>三、局部极小值</h2><p>虽然保证了收敛性，但是所优化的误差函数E并不是凸函数，也就是说如果初始化不佳，k-Means有陷入局部极小值的风险。</p><p>初始化： </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-27e13203fdb3d6e7c6966fb86d4e6778_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"560\" data-rawheight=\"420\" class=\"origin_image zh-lightbox-thumb\" width=\"560\" data-original=\"https://pic1.zhimg.com/v2-27e13203fdb3d6e7c6966fb86d4e6778_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;560&#39; height=&#39;420&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"560\" data-rawheight=\"420\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"560\" data-original=\"https://pic1.zhimg.com/v2-27e13203fdb3d6e7c6966fb86d4e6778_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-27e13203fdb3d6e7c6966fb86d4e6778_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>陷入局部极小： </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-389f68197f283956c5a6d1cd78d5744a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"560\" data-rawheight=\"420\" class=\"origin_image zh-lightbox-thumb\" width=\"560\" data-original=\"https://pic3.zhimg.com/v2-389f68197f283956c5a6d1cd78d5744a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;560&#39; height=&#39;420&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"560\" data-rawheight=\"420\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"560\" data-original=\"https://pic3.zhimg.com/v2-389f68197f283956c5a6d1cd78d5744a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-389f68197f283956c5a6d1cd78d5744a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>如何解决？因为是随机初始化，再运行几次就好了。</p><h2>四、实现</h2><p>以下是ready-to-use的二维k-Means的python代码，使用时只需更改前两行。</p><div class=\"highlight\"><pre><code class=\"language-text\">from numpy import *\nimport matplotlib.pyplot as plt\n\nk=2\ndataMat = [[1,1],[1.5,2],[3,3],[3.5,3]]\n\ndef distEclud(vecA, vecB):\n    return sqrt(sum(power(vecA - vecB, 2))) #la.norm(vecA-vecB)\n\ndef randCent(dataSet, k):\n    n = shape(dataSet)[1]\n    centroids = mat(zeros((k,n)))#create centroid mat\n    for j in range(n):#create random cluster centers, within bounds of each dimension\n        minJ = min(dataSet[:,j])\n        rangeJ = float(max(dataSet[:,j]) - minJ)\n        centroids[:,j] = mat(minJ + rangeJ * random.rand(k,1))\n    return centroids\n\ndef kMeans(dataSet, k, distMeas=distEclud, createCent=randCent):\n    m = shape(dataSet)[0]\n    clusterAssment = mat(zeros((m,2)))#create mat to assign data points \n                                      #to a centroid, also holds SE of each point\n    centroids = createCent(dataSet, k)\n    clusterChanged = True\n    while clusterChanged:\n        clusterChanged = False\n        for i in range(m):#for each data point assign it to the closest centroid\n            minDist = inf; minIndex = -1\n            for j in range(k):\n                distJI = distMeas(centroids[j,:],dataSet[i,:])\n                if distJI &lt; minDist:\n                    minDist = distJI; minIndex = j\n            if clusterAssment[i,0] != minIndex: clusterChanged = True\n            clusterAssment[i,:] = minIndex,minDist**2\n        for cent in range(k):#recalculate centroids\n            ptsInClust = dataSet[nonzero(clusterAssment[:,0].A==cent)[0]]#get all the point in this cluster\n            centroids[cent,:] = mean(ptsInClust, axis=0) #assign centroid to mean \n    return centroids, clusterAssment\n\ndataMat = mat(dataMat)\nmyCentroids, clustAssing = kMeans(dataMat,k)\n\nx_c, y_c = zeros(k), zeros(k)\nx, y = zeros(len(dataMat)), zeros(len(dataMat))\n\nfor i in range(k):\n    x_c[i] = myCentroids[i,0]\n    y_c[i] = myCentroids[i,1]\n\nfor i in range(len(dataMat)):\n    x[i] = dataMat[i,0]\n    y[i] = dataMat[i,1]\n\nc = zeros(len(dataMat))\nfor i in range(len(clustAssing)): c[i] = int(clustAssing[i,0])\n\nplt.scatter(x_c, y_c,marker=&#39;x&#39;,c = &#39;black&#39;)\nplt.scatter(x,y,c = c)\nplt.show()</code></pre></div><p>运行结果： </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-2835a6cb5f0378e8e4d3f21add3cb12b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"480\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic4.zhimg.com/v2-2835a6cb5f0378e8e4d3f21add3cb12b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;640&#39; height=&#39;480&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"480\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https://pic4.zhimg.com/v2-2835a6cb5f0378e8e4d3f21add3cb12b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-2835a6cb5f0378e8e4d3f21add3cb12b_b.jpg\"/></figure><p> x为聚类中心，不同颜色为不同的簇。</p>", 
            "topic": [
                {
                    "tag": "聚类", 
                    "tagLink": "https://api.zhihu.com/topics/19590190"
                }, 
                {
                    "tag": "无监督学习", 
                    "tagLink": "https://api.zhihu.com/topics/19590194"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/42418216", 
            "userName": "陈德龙", 
            "userLink": "https://www.zhihu.com/people/fb342217a29a19462616c21ace65a810", 
            "upvote": 0, 
            "title": "机器学习笔记（六） Adaboost", 
            "content": "<h2><b>负一、写在最前面﻿</b></h2><p>知乎对于md表格还有latex公式的支持真的是垃圾</p><p>懒得改了</p><p>麻烦移步我的CSDN博客：</p><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/weixin_41405111/article/details/81839112\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">机器学习笔记（六） Adaboost</a><p>来阅读本文。</p><h2><b>零、写在前面</b></h2><p>参考资料：</p><ul><li>《机器学习》周志华 </li></ul><p>本文记录了Adabooost算法的推导过程，不是Adaboost的教程。如果之前没有接触过Adaboost，我保证读者看完本文后一定学不会Adaboost算法。</p><h2><b>一、推导</b></h2><h2>1. 为什么指数损失函数可以作为优化目标</h2><p>首先说明我们使用的符号： </p><p>| 符号| 含义| </p><p>| ----- | ---- | </p><p>|T | 基学习器个数| </p><p>|h|基学习器| </p><p>|$\\alpha$|不同基学习器的权重| </p><p>|H|集成学习器| </p><p>|f(·)|真实函数（算法要学习的函数）|</p><p> |D|训练样本的分布| |E|期望| </p><p>|$l$|损失函数|</p><p>Adaboost算法是要将T个弱学习器h(x)（决策树桩或神经网络等）加权结合结合成一个集成学习器H(x)：</p><p>$$H(x) = \\sum ^T_t \\alpha_t h_t (x)$$</p><p>这是指数损失函数： </p><p>$$l_{exp} (H|D) = E_{x\\sim D} [e^{-f(x)H(x)}]$$ 我们要证明，令这一损失函数最小化，能使得集成学习的效果最优。</p><p>求函数极小值点，最简单的是另其偏导数为零。指数损失函数中，f(x)是真实函数，不是变量，所以我们仅需令其对H(x)的偏导为零： </p><p>$$\\frac {\\partial l_{exp} (H|D) }{ \\partial H(x)}=  -e^{-H(x)}P(f(x)=1|x)+ e^{-H(x)}P(f(x)=-1|x) = 0$$</p><p> 解得： $$H(x) = \\frac{1}{2}ln \\frac{P(f(x) = 1|x) }{ P(f(x) = -1)|x)}$$ 等式两边加sign函数（正值得1，负值为-1） $$sign(H(x)) = sign(\\frac{1}{2}ln \\frac{P(f(x) = 1|x) }{ P(f(x) = -1)|x)})$$ 当 P( f(x) = 1 |x) &gt;  P( f(x) = -1 |x)，即H(x)预测值为1时，上式值为1，反之预测值为-1时，上式值为-1</p><p>故 $$sign(H(x)) = argmax\\ P(f(x) = y|x)$$ 上式由指数损失函数最小化推导而来，即指数损失函数最小化等价于$argmax\\ P(f(x) = y|x)$，显然指数损失函数可以作为等价的优化目标。而之所以要使用指数损失函数，是因为其良好的数学性质。</p><p>推导过程：</p><div class=\"highlight\"><pre><code class=\"language-text\">指数损失函数-&gt;求偏导数-&gt;偏导数置为零-&gt;化简-&gt;加sign函数</code></pre></div><h2>2.权重$\\alpha$的更新公式</h2><p>t时刻，当基分类器$h_t$基于样本分布$D_t$学习产生后，我们要为这个学习器分配一个权重$\\alpha_t$，分配的目的是：配合$h_t$的特性，要最小化指数损失函数： </p><p>$$ \\begin{align} l_{exp} (H_t|D_t)&amp;= l_{exp} (\\alpha_t h_t|D_t)\\ &amp;=E_{x\\sim D} [e^{-f(x)H(x)}]\\ &amp;=E_{x\\sim D} [e^{-f(x)\\alpha_t h_t(x)}]\\ &amp;=E_{x\\sim D} [e^{\\alpha t }||(f(x) = h_t(x))+e^{\\alpha t }||(f(x)\\neq h_t(x))]\\ &amp;=e^{-\\alpha_t }P_{x \\sim D_t} (f(x) = h_t(x))  +e^{\\alpha_t }P_{x \\sim D_t} (f(x) \\neq h_t(x)) \\ &amp;=e^{-\\alpha_t}(1-\\epsilon_t)+e^{\\alpha_t}\\epsilon_t \\end{align} $$ 其中：  - $H_t$表示这一步的集成学习器  -  ||(·)为指示函数，自变量为真时值为1，假时为0 <br/>  - 第四个等号是考虑到预测值为真时，f(x)和h(x)同号  - 错误率$\\epsilon_t = P_{x\\sim D_t}(h_t(x) \\neq f(x))$</p><p>对于这时的指数损失函数求偏导数： $$\\frac{\\partial l_{exp} (\\alpha_t h_t)|D_t}{\\partial \\alpha_t} =-e^{-\\alpha_t}(1-\\epsilon_t)+e^{\\alpha_t}\\epsilon_t $$ 令其为零，解得 $$\\alpha_t =\\frac{1}{2} ln (\\frac{1-\\epsilon_t}{\\epsilon_t})$$</p><p>这就是Adaboost每一步权重的更新公式。</p><h2>3. 样本分布更新公式</h2><p>Adaboost学习的t时刻，我们已有了$H_{t-1}$，要产生$h_t$。我们希望这一时间步之后的学习器$H_t = H_{t-1}+h_t$能够修正之前的错误，即最小化指数损失函数： $$\\begin{align} l_{exp}( H_{t-1}+h_t)  &amp;=E_{x\\sim D} [e^{-f(x)H_t(x)}]\\ &amp;=E_{x\\sim D} [e^{-f(x) (H_{t-1}+h_t))}]\\ &amp;=E_{x\\sim D} [e^{-f(x) H_{t-1}}e^{-f(x)h_t}]\\ &amp;= E_{x\\sim D} [e^{-f(x) H_{t-1}}(1-f(x)h_t(x)+\\frac{f^2(x)h^2_t(x)}{2})]\\ &amp;= E_{x\\sim D} [e^{-f(x) H_{t-1}}(1-f(x)h_t(x)+\\frac{1}{2})]\\ \\end{align} $$ 其中：     第四个等号对$e^{-f(x)h_t}$做了二阶泰勒展开     第五个等号是由于f(x)和h(x)都只能取$\\pm1$</p><p>回过头来，我们要$h_t(x)$最小化指数损失函数，我们可以去掉上式中的一些常量： $$ \\begin{align} h_t(x)  &amp; = argmin\\ l_exp(H_{t-1}+h_t |D)\\ &amp; = argmin E_{x \\sim D}[e^{-f(x) H_{t-1}}(1-f(x)h_t(x)+\\frac{1}{2})]\\ &amp; = argmin E_{x \\sim D}[e^{-f(x) H_{t-1}}(-1)f(x)h_t(x)]\\ &amp; = argmax E_{x \\sim D}[e^{-f(x) H_{t-1}}f(x)h_t(x)]\\ &amp; = argmax E_{x \\sim D}[\\frac{e^{-f(x) H_{t-1}}f(x)h_t(x)} {E_{x \\sim D}[e^{-f(x)H_{t-1}}]}]\\ &amp; = argmax E_{x \\sim D}[\\frac{e^{-f(x) H_{t-1}}} {E_{x \\sim D}[e^{-f(x)H_{t-1}}]}f(x)h_t(x)]\\ \\end{align} $$ 其中：     第四个等号去掉一个负号，并将argmin更为argmax     第五个等号加上一个分母$E_{x \\sim D}[e^{-f(x)H_{t-1}}]$，是上一时间步的损失函数，是常量、定值。</p><p>下面这一步我没搞懂=.=，可能是概率论还没有学？ 令上式中$\\frac{e^{-f(x) H_{t-1}}} {E_{x \\sim D}[e^{-f(x)H_{t-1}}]} = \\frac{D_t(x)}{D(x)}$ (D为分布) 将上式代入上上式，“由数学期望的定义”， $$\\begin{align} h_t(x) &amp;=  argmax E_{x \\sim D}[\\frac{e^{-f(x) H_{t-1}}} {E_{x \\sim D}[e^{-f(x)H_{t-1}}]}f(x)h_t(x)]\\ &amp;= argmax E_{x \\sim D}[\\frac{D_t(x)}{D(x)}f(x)h_t(x)]\\  &amp;=argmax E_{x \\sim D_t}[f(x)h_t(x)]  \\end{align}$$</p><p>再一次，f(x)和h(x)都只能取$\\pm1$，故可以有 $f(x)h(x)=1-2||（f(x)\\neq h(x)）$ 将上式代入上上式， $$\\begin{align} h_t(x)   &amp;=argmax E_{x \\sim D_t}[f(x)h_t(x)]\\  &amp;=argmax E_{x \\sim D_t}[1-2||（f(x)\\neq h(x)）]\\  &amp;=argmax E_{x \\sim D_t}[（-1）||（f(x)\\neq h(x)）]\\    &amp;=argmin E_{x \\sim D_t}[||（f(x)\\neq h(x)）]   \\end{align}$$  很明显，最后一个等式达到了我们想要的效果。现在我们再回过头看神秘的$\\frac{e^{-f(x) H_{t-1}}} {E_{x \\sim D}[e^{-f(x)H_{t-1}}]} = \\frac{D_t(x)}{D(x)}$： $$\\begin{align} D_{t+1}(x) &amp;=\\frac{D(x)e^{-f(x) H_{t}}} {E_{x \\sim D}[e^{-f(x)H_{t}}]}\\ &amp;=\\frac{D(x)e^{-f(x) (H_{t-1}+\\alpha_t h_t)}} {E_{x \\sim D}[e^{-f(x)H_{t}}]}\\ &amp;=\\frac{D(x)e^{-f(x) H_{t-1}}e^{-f(x)\\alpha_t h_t}} {E_{x \\sim D}[e^{-f(x)H_{t}}]}\\ &amp;=D(x)e^{-f(x) H_{t-1}}\\frac{e^{-f(x)\\alpha_t h_t}} {E_{x \\sim D}[e^{-f(x)H_{t}}]}\\ &amp;=D_t(x)E_{x\\sim D}[e^{-f(x) H_{t-1}}]\\frac{e^{-f(x)\\alpha_t h_t}} {E_{x \\sim D}[e^{-f(x)H_{t}}]}\\ &amp;=D_t(x)e^{-f(x)\\alpha_t h_t}\\frac{E_{x\\sim D}[e^{-f(x) H_{t-1}}]} {E_{x \\sim D}[e^{-f(x)H_{t}}]}\\   \\end{align}$$ 其中，第五个等号是将与 $\\frac{e^{-f(x) H_{t-1}}} {E_{x \\sim D}[e^{-f(x)H_{t-1}}]} = \\frac{D_t(x)}{D(x)}$ 等价的 $D(x)e^{-f(x) H_{t-1}} = D_t (x)E_{x \\sim D}[e^{-f(x)H_{t-1}}]$代入得到。</p><p>至此，我们完成了Adaboost算法的所有推导。</p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "adaboost", 
                    "tagLink": "https://api.zhihu.com/topics/19719221"
                }, 
                {
                    "tag": "知乎产品改进", 
                    "tagLink": "https://api.zhihu.com/topics/19550512"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/35275280", 
            "userName": "陈德龙", 
            "userLink": "https://www.zhihu.com/people/fb342217a29a19462616c21ace65a810", 
            "upvote": 2, 
            "title": "机器学习笔记（五） 朴素贝叶斯分类器", 
            "content": "<h2><b>零、写在前面</b></h2><p>参考资料：</p><ul><li>《机器学习》周志华 </li><li>《机器学习实战》Peter Harrington</li><li>斯坦福 CS 229 吴恩达</li></ul><h2><b>一、属性条件独立性假设</b></h2><p>由<a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/weixin_41405111/article/details/79737421\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">上篇文章</a>中介绍的贝叶斯决策论知道，我们想做这样的事：<br/>给出一个样本各个属性xi的值（记为向量大X），求出这个样本属于各个类别c的概率，输出这些概率中最大的那个类别。由贝叶斯定理，这概率（等号左侧）可以这样来求：<br/> <img src=\"https://www.zhihu.com/equation?tex=%5C%5CP%28c%7CX%29+%3D+%5Cfrac+%7BP%28c%29P%28X%7Cc%29%7D%7BP%28X%29%7D\" alt=\"\\\\P(c|X) = \\frac {P(c)P(X|c)}{P(X)}\" eeimg=\"1\"/> </p><p>问题的关键就在于P(X|c)的获得方法，在极大似然估计中，我们假设它服从某一概率分布，然后按照这个概率分布的轮廓来拟合需要的概率。而今天我们要换一种方法来估算它。</p><p>事实上，我们觊觎的这个概率是可以<b>直接展开</b>的：<br/> <img src=\"https://www.zhihu.com/equation?tex=%5C%5CP%28X%7Cc%29+%3D+P%28x1%7Cc%29%5C+P%28x2%7Cc%2Cx1%29%5C+P%28x3%7Cc%2C+x1%2C+x2%29.....P%28xi%7Cc%2C+x1%2Cx2%2Cx3%2C......%2Cx%7Bi-1%7D%29\" alt=\"\\\\P(X|c) = P(x1|c)\\ P(x2|c,x1)\\ P(x3|c, x1, x2).....P(xi|c, x1,x2,x3,......,x{i-1})\" eeimg=\"1\"/> </p><p>但是对于这个<b>所有属性上的联合概率</b>，等号右侧的概率是越来越难以计算的，或者说，要获得等号右侧的所有项，需要很大很大很大的数据集。但其实，形如$P(xi|c)$的概率是很容易计算的，它代表着固定类别中，固定属性各个取值的概率。那我们就不妨强行地令：<br/> <img src=\"https://www.zhihu.com/equation?tex=%5C%5CP%28X%7Cc%29+%3DP%28x1%7Cc%29P%28x2%7Cc%29P%28x3%7Cc%29.....P%28xi%7Cc%29\" alt=\"\\\\P(X|c) =P(x1|c)P(x2|c)P(x3|c).....P(xi|c)\" eeimg=\"1\"/> </p><p>, 这样就好算了。</p><p>接着，计算好了P(X|c)，代入 <img src=\"https://www.zhihu.com/equation?tex=P%28c%7CX%29+%3D+%5Cfrac+%7BP%28c%29P%28X%7Cc%29%7D%7BP%28X%29%7D\" alt=\"P(c|X) = \\frac {P(c)P(X|c)}{P(X)}\" eeimg=\"1\"/> （其余两项很容易解决），就大功告成啦。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>这就是朴素贝叶斯。</b></p><p class=\"ztext-empty-paragraph\"><br/></p><p>好的我们现在再回过头来细致地讲述一遍。事实上，要完成刚才做的 “不妨强行地令” 这一操作是需要一个假设的，即<b>属性条件独立性假设</b>，也称作<b>贝叶斯假设</b>。这一假设说的就是：</p><p class=\"ztext-empty-paragraph\"><br/></p><blockquote>各个属性x之间是没有相互关系的。</blockquote><p class=\"ztext-empty-paragraph\"><br/></p><p>公式化地表达就是： </p><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5CP%28xi%7Cc%2C+x1%2Cx2%2Cx3%2C......%2Cx%7Bi-1%7D%29%3DP%28xi%7Cc%29\" alt=\"\\\\P(xi|c, x1,x2,x3,......,x{i-1})=P(xi|c)\" eeimg=\"1\"/> </p><p>很好理解：条件概率中竖线 | 右侧的内容表示 :“在......的条件下” ， 那么，“在 <img src=\"https://www.zhihu.com/equation?tex=c%2C+x1%2Cx2%2Cx3%2C......%2Cx%7Bi-1%7D\" alt=\"c, x1,x2,x3,......,x{i-1}\" eeimg=\"1\"/> 的条件下”  等价于  “在c的条件下”， 也就意味着这些xi之间是<b>不存在相互影响的关系</b>的。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>但是我们要记住，所谓<b>不存在相互影响的关系</b>并不准确，这只是我们为了计算方便而作的一个假设（很强的一个假设），并且这一个方法在实际应用的时候效果还不错。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>有了上面的公式，我们就要想办法计算等号右侧的那个概率了。它表示的是，在给定类别c的情形，第i个属性取特定值xi的概率。即（用频率估计概率）：</p><p><br/> <img src=\"https://www.zhihu.com/equation?tex=%5C%5CP%28xi%7Cc%29%3D%5Cfrac%7B%E6%9F%90%E7%B1%BB%E6%A0%B7%E6%9C%AC%E4%B8%AD%EF%BC%8C%E7%AC%ACi%E4%B8%AA%E5%B1%9E%E6%80%A7%E5%8F%96%E5%80%BCxi%E7%9A%84%E4%B8%AA%E6%95%B0%7D%7B%E8%BF%99%E7%B1%BB%E6%A0%B7%E6%9C%AC%E6%80%BB%E4%B8%AA%E6%95%B0%7D\" alt=\"\\\\P(xi|c)=\\frac{某类样本中，第i个属性取值xi的个数}{这类样本总个数}\" eeimg=\"1\"/> </p><p>至此我们在公式 <img src=\"https://www.zhihu.com/equation?tex=P%28c%7CX%29+%3D+%5Cfrac+%7BP%28c%29P%28X%7Cc%29%7D%7BP%28X%29%7D\" alt=\"P(c|X) = \\frac {P(c)P(X|c)}{P(X)}\" eeimg=\"1\"/> 中还剩下两个需要求的量了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>现在我们说P(c)，它表示的是样本类别为c的概率。很自然地，它可以这样算出来：<br/> <img src=\"https://www.zhihu.com/equation?tex=%5C%5CP%28c%29%3D%5Cfrac%7B%E6%9F%90%E7%B1%BB%E6%A0%B7%E6%9C%AC%E4%B8%AA%E6%95%B0%7D%7B%E6%80%BB%E5%85%B1%E6%A0%B7%E6%9C%AC%E4%B8%AA%E6%95%B0%7D\" alt=\"\\\\P(c)=\\frac{某类样本个数}{总共样本个数}\" eeimg=\"1\"/> </p><p>其实还剩下一个P(X)，但是我们一会就会发现，其实这个概率并不需要计算。</p><p>好的，现在我们回到贝叶斯决策论。我们需要对于给出一个样本各个属性xi的值（记为向量大X），求出这个样本属于各个类别c的概率，输出这些概率中最大的那个类别。综合上面得到的结论，把这句话翻译成公式，则有：<br/> <img src=\"https://www.zhihu.com/equation?tex=%5C%5Coutput+class%3Dargmax+P%28c%7CX%29+%3D+argmax+%5Cfrac+%7BP%28c%29P%28X%7Cc%29%7D%7BP%28X%29%7D+%5C%5C%3D+argmax+%5Cfrac+%7BP%28c%29P%28x1%7Cc%29P%28x2%7Cc%29P%28x3%7Cc%29.....P%28xi%7Cc%29%7D%7BP%28X%29%7D%5C%5C%3D+argmax+P%28c%29P%28x1%7Cc%29P%28x2%7Cc%29P%28x3%7Cc%29.....P%28xi%7Cc%29\" alt=\"\\\\output class=argmax P(c|X) = argmax \\frac {P(c)P(X|c)}{P(X)} \\\\= argmax \\frac {P(c)P(x1|c)P(x2|c)P(x3|c).....P(xi|c)}{P(X)}\\\\= argmax P(c)P(x1|c)P(x2|c)P(x3|c).....P(xi|c)\" eeimg=\"1\"/> <br/></p><p>对于所有类别来说，P(X)是相同的，所以这一因子在最后一个等号去掉了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>这就是朴素贝叶斯。</b></p><p class=\"ztext-empty-paragraph\"><br/></p><p>将上面的各个式子放到一起：</p><blockquote><img src=\"https://www.zhihu.com/equation?tex=P%28c%7CX%29+%3D+%5Cfrac+%7BP%28c%29P%28X%7Cc%29%7D%7BP%28X%29%7D%5C%5C+%5C%5C+P%28c%29%3D%5Cfrac%7B%E6%9F%90%E7%B1%BB%E6%A0%B7%E6%9C%AC%E4%B8%AA%E6%95%B0%7D%7B%E6%80%BB%E5%85%B1%E6%A0%B7%E6%9C%AC%E4%B8%AA%E6%95%B0%7D+%5C%5C+%5C%5C+P%28X%7Cc%29+%3DP%28x1%7Cc%29P%28x2%7Cc%29P%28x3%7Cc%29.....P%28xi%7Cc%29%5C%5C%5C%5C+P%28xi%7Cc%29%3D%5Cfrac%7B%E6%9F%90%E7%B1%BB%E6%A0%B7%E6%9C%AC%E4%B8%AD%EF%BC%8C%E7%AC%ACi%E4%B8%AA%E5%B1%9E%E6%80%A7%E5%8F%96%E5%80%BCxi%E7%9A%84%E4%B8%AA%E6%95%B0%7D%7B%E8%BF%99%E7%B1%BB%E6%A0%B7%E6%9C%AC%E6%80%BB%E4%B8%AA%E6%95%B0%7D\" alt=\"P(c|X) = \\frac {P(c)P(X|c)}{P(X)}\\\\ \\\\ P(c)=\\frac{某类样本个数}{总共样本个数} \\\\ \\\\ P(X|c) =P(x1|c)P(x2|c)P(x3|c).....P(xi|c)\\\\\\\\ P(xi|c)=\\frac{某类样本中，第i个属性取值xi的个数}{这类样本总个数}\" eeimg=\"1\"/> </blockquote><h2><b>二、拉普拉斯修正</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p>我们接下来要讨论的是这两个式子的升级版：<br/> <img src=\"https://www.zhihu.com/equation?tex=P%28c%29%3D%5Cfrac%7B%E6%9F%90%E7%B1%BB%E6%A0%B7%E6%9C%AC%E4%B8%AA%E6%95%B0%7D%7B%E6%80%BB%E5%85%B1%E6%A0%B7%E6%9C%AC%E4%B8%AA%E6%95%B0%7D%5C%5CP%28xi%7Cc%29%3D%5Cfrac%7B%E6%9F%90%E7%B1%BB%E6%A0%B7%E6%9C%AC%E4%B8%AD%EF%BC%8C%E7%AC%ACi%E4%B8%AA%E5%B1%9E%E6%80%A7%E5%8F%96%E5%80%BCxi%E7%9A%84%E4%B8%AA%E6%95%B0%7D%7B%E8%BF%99%E7%B1%BB%E6%A0%B7%E6%9C%AC%E6%80%BB%E4%B8%AA%E6%95%B0%7D\" alt=\"P(c)=\\frac{某类样本个数}{总共样本个数}\\\\P(xi|c)=\\frac{某类样本中，第i个属性取值xi的个数}{这类样本总个数}\" eeimg=\"1\"/> <br/></p><p>考虑这样的情形，我们手头上的训练集将将够大，但是存在着对于某一个类别，<b>没有某一个属性取特定值的样本</b>。那如果我们直接用之前的方法计算，我们会发现，在上面第二个式子中，分子为零，计算出的 <img src=\"https://www.zhihu.com/equation?tex=P%28x_i%7Cc%29\" alt=\"P(x_i|c)\" eeimg=\"1\"/> 也为零。接着回溯到 <img src=\"https://www.zhihu.com/equation?tex=P%28X%7Cc%29+%3DP%28x1%7Cc%29P%28x2%7Cc%29P%28x3%7Cc%29.....P%28x_i%7Cc%29\" alt=\"P(X|c) =P(x1|c)P(x2|c)P(x3|c).....P(x_i|c)\" eeimg=\"1\"/> ，乘数因子有一个是零，那么整个的$P(X|c)$也为零。再往前推， <img src=\"https://www.zhihu.com/equation?tex=P%28c%7CX%29+%3D+%5Cfrac+%7BP%28c%29P%28X%7Cc%29%7D%7BP%28X%29%7D\" alt=\"P(c|X) = \\frac {P(c)P(X|c)}{P(X)}\" eeimg=\"1\"/> 也变成了零。</p><p>也就是说，因为一个样本的缺失，造成了<b>这个样本所属的类不能被预测到</b>（概率恒为零）。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>我们要解决这个问题，可以用<b>拉普拉斯修正</b>的方法：</p><p><img src=\"https://www.zhihu.com/equation?tex=P%28c%29%3D%5Cfrac%7B%E6%9F%90%E7%B1%BB%E6%A0%B7%E6%9C%AC%E4%B8%AA%E6%95%B0+%2B1%7D%7B%E6%80%BB%E5%85%B1%E6%A0%B7%E6%9C%AC%E4%B8%AA%E6%95%B0%2B%E6%80%BB%E7%B1%BB%E5%88%AB%E6%95%B0%7D%5C%5CP%28xi%7Cc%29%3D%5Cfrac%7B%E6%9F%90%E7%B1%BB%E6%A0%B7%E6%9C%AC%E4%B8%AD%EF%BC%8C%E7%AC%ACi%E4%B8%AA%E5%B1%9E%E6%80%A7%E5%8F%96%E5%80%BCxi%E7%9A%84%E4%B8%AA%E6%95%B0%2B1%7D%7B%E8%BF%99%E7%B1%BB%E6%A0%B7%E6%9C%AC%E6%80%BB%E4%B8%AA%E6%95%B0%2B%E7%AC%ACi%E4%B8%AA%E5%B1%9E%E6%80%A7%E7%9A%84%E5%8F%AF%E8%83%BD%E5%8F%96%E5%80%BC%E4%B8%AA%E6%95%B0%7D\" alt=\"P(c)=\\frac{某类样本个数 +1}{总共样本个数+总类别数}\\\\P(xi|c)=\\frac{某类样本中，第i个属性取值xi的个数+1}{这类样本总个数+第i个属性的可能取值个数}\" eeimg=\"1\"/> </p><p class=\"ztext-empty-paragraph\"><br/></p><p>这样，对于某一个缺失样本的类别来说，它的概率就变成了一个很小的数（这也是符合逻辑的），而非零了。</p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "贝叶斯分类", 
                    "tagLink": "https://api.zhihu.com/topics/19576062"
                }, 
                {
                    "tag": "拉普拉斯", 
                    "tagLink": "https://api.zhihu.com/topics/19907931"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/35066894", 
            "userName": "陈德龙", 
            "userLink": "https://www.zhihu.com/people/fb342217a29a19462616c21ace65a810", 
            "upvote": 4, 
            "title": "机器学习笔记（四） 极大似然估计", 
            "content": "<h2><b>零、写在前面</b></h2><p>参考资料：</p><ul><li>《机器学习》周志华</li><li>斯坦福 CS 229 吴恩达</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>一、贝叶斯决策论</b></h2><p>贝叶斯分类器显然是用于<b>分类</b>问题的，是一种<b>监督学习</b>的模型。最核心的过程是这样的：</p><blockquote>在训练过程中，分类器要根据训练集中的好多好多组x（各个特征）和y（类别）学会做这样一件事：对于没见过的样本，能根据它的各个特征计算出他属于各个类别的<b>概率</b>。<br/>  继而在应用时，选择<b>概率最高</b>的那个样本作为输出结果。</blockquote><p>其中 “根据它的各个特征（x）计算出他属于各个类别（c）的概率” 理解为<b>条件概率</b>：在具有特征x的条件下，属于类别c的概率。这概率我们记为 <img src=\"https://www.zhihu.com/equation?tex=P%28c%7Cx%29\" alt=\"P(c|x)\" eeimg=\"1\"/> 。有时他也被称为<b>似然</b>（likelihood）。</p><p>只要得到了这个概率，剩下的事情就只有比较大小了，所以我们要关注怎样能比较准确地得到这个概率。大体来说有两种策略：</p><p class=\"ztext-empty-paragraph\"><br/></p><ol><li>直接建模 <img src=\"https://www.zhihu.com/equation?tex=P%28c%7Cx%29\" alt=\"P(c|x)\" eeimg=\"1\"/> ，即这个模型要学会，输入给他一组特征x，能够输出它属于各个类别c的概率，即模型学到了特征x到类别c的<b>映射</b>。</li><li>另一种方法是反过来，要对 <img src=\"https://www.zhihu.com/equation?tex=P%28x%7Cc%29\" alt=\"P(x|c)\" eeimg=\"1\"/> 建模，即对于每个类别，模型要试着了解，这个类别的样本的各个特征大概是什么样的。即对于联合概率 <img src=\"https://www.zhihu.com/equation?tex=P%28x%2Cc%29\" alt=\"P(x,c)\" eeimg=\"1\"/> 建模，并以此来获得需要的 <img src=\"https://www.zhihu.com/equation?tex=P%28c%7Cx%29\" alt=\"P(c|x)\" eeimg=\"1\"/> 。</li></ol><p>第一种策略我们称之为<b>判别式（discriminative）模型</b>，像决策树，支持向量机，以及近几年大红大紫的神经网络都属于这样的类型。第二种曲线救国的策略我们称为生成式（generative）模型，本文要讨论的贝叶斯分类器就属于这一类型。</p><p>先说上面 “并以此来获得需要的 <img src=\"https://www.zhihu.com/equation?tex=P%28c%7Cx%29\" alt=\"P(c|x)\" eeimg=\"1\"/> ”的方法——贝叶斯定理：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5CP%28c%7Cx%29%3DP%28c%29P%28x%7Cc%29%2FP%28x%29\" alt=\"\\\\P(c|x)=P(c)P(x|c)/P(x)\" eeimg=\"1\"/> </p><p>只要得到了等号右侧的每一项，我们就能顺利获得 <img src=\"https://www.zhihu.com/equation?tex=P%28c%7Cx%29\" alt=\"P(c|x)\" eeimg=\"1\"/> 了。当然，其中P(c)和P(x)并不费力，重点是在 <img src=\"https://www.zhihu.com/equation?tex=P%28x%7Cc%29\" alt=\"P(x|c)\" eeimg=\"1\"/> 上。</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>二、极大似然估计</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p>极大似然估计就是获得P(x|c)的一种方法，它的主要思想是：先假设它服从某种概率分布形式，这形式可以用一组参数表示出来，然后，由训练样本计算出这组参数。</p><p>举个例子，一个人要通过描述让你把他脑子里想的东西画出来，考虑这两种方法：</p><p class=\"ztext-empty-paragraph\"><br/></p><ol><li>在纸上随便找一个点开始，接着往随便一个方向画，然后逆时针旋转，旋转，旋转，旋转。。。直到回到原来的那个开始的点，注意画出的轨迹一定要平滑而优美。</li><li>画一个圆，半径为 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 。</li></ol><p>极大似然估计就相当于第二种方法，即先假设要你画的东西是一个圆，这个圆能通过参数 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 表示出来。</p><p>回到我们的极大似然估计，我们假设$P(x|c)$服从高斯（正态）分布，即<br/> <img src=\"https://www.zhihu.com/equation?tex=%5C%5CP%28x%7Cc%29+%5Csim+%5Cmathcal+N%28%5Cmu%2C+%5Csigma%29+%5C+\" alt=\"\\\\P(x|c) \\sim \\mathcal N(\\mu, \\sigma) \\ \" eeimg=\"1\"/> <br/>但如果高斯分布中的参数 <img src=\"https://www.zhihu.com/equation?tex=%5Cmu%2C+%5Csigma\" alt=\"\\mu, \\sigma\" eeimg=\"1\"/> 都只是实数，那这个概率分布就不能表示<b>多个特征</b>到类别的映射。因此我们使用<b>多元正态分布</b>，两个参数分别调整为—— <img src=\"https://www.zhihu.com/equation?tex=%5Cmu\" alt=\"\\mu\" eeimg=\"1\"/> 为n维的<b>平均向量</b>， <img src=\"https://www.zhihu.com/equation?tex=%5Csigma\" alt=\"\\sigma\" eeimg=\"1\"/> 为n * n维的<b>方差矩阵</b>。与普通高斯分布相似，平均向量控制概率取最大值的位置，方差矩阵控制图像“扁”的程度。我们把这两组参数一起记为 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> <img src=\"https://www.zhihu.com/equation?tex=%5C%5CP%28x%3B%5Cmu%2C+%5Csigma%29%3Dexp%28-%28x-%5Cmu%29%5ET%5Csigma+%5E%7B-1%7D%28x-%5Cmu%29%29+%2F+%282%5Cpi%29%5E%7B2%2Fn%7D+%7Csigma%7C%5E%7B2%2Fn%7D\" alt=\"\\\\P(x;\\mu, \\sigma)=exp(-(x-\\mu)^T\\sigma ^{-1}(x-\\mu)) / (2\\pi)^{2/n} |sigma|^{2/n}\" eeimg=\"1\"/> <br/>另，如果假设P(x|c)服从高斯（正态）分布，这算法也可以被称为<b>高斯判别分析</b>（Gaussian Discriminant Analysis model ）<br/>令Dc表示训练集D中属于类别c的样本们，假设独立同分布，则参数 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 对数据集的条件概率（即似然）为<br/> <img src=\"https://www.zhihu.com/equation?tex=%5C%5CP%28Dc+%7C+%5Ctheta%29%3D%5Cprod+P%28x%7C%5Ctheta%29\" alt=\"\\\\P(Dc | \\theta)=\\prod P(x|\\theta)\" eeimg=\"1\"/> </p><p>在计算机上，连乘很多小于1的数很可能造成<b>数值下溢</b>，所以我们取对数，改为累加：<br/> <img src=\"https://www.zhihu.com/equation?tex=%5C%5CLL%28%5Ctheta%29%3Dlog+P%28Dc+%7C+%5Ctheta%29%3D%5Csum+log+P%28x%7C%5Ctheta%29\" alt=\"\\\\LL(\\theta)=log P(Dc | \\theta)=\\sum log P(x|\\theta)\" eeimg=\"1\"/> </p><p>我们自然是希望这<b>对数似然</b>越大越好。所以我们要求：<br/> <img src=\"https://www.zhihu.com/equation?tex=%5C%5C%5Ctheta+%3D+arg+max_%7B%5Ctheta%7D+LL%28%5Ctheta%29\" alt=\"\\\\\\theta = arg max_{\\theta} LL(\\theta)\" eeimg=\"1\"/> </p><p>在正态分布中，我们对公式求导，将对于各个参数的偏导数置为零，就可以得到各个参数的极大似然估计：<br/> <img src=\"https://www.zhihu.com/equation?tex=%5Cmu+%3D+1%2F%7CDc%7C+%5Csum+x%5C+%5C+%28x%5Cin+Dc%29+%5C+%5C%5C%5Csigma+%5E2+%3D+1%2F%7CDc%7C+%5Csum+%28x-%5Cmu%29%28x-%5Cmu%29%5ET%5C+%5C+%28x%5Cin+Dc%29\" alt=\"\\mu = 1/|Dc| \\sum x\\ \\ (x\\in Dc) \\ \\\\\\sigma ^2 = 1/|Dc| \\sum (x-\\mu)(x-\\mu)^T\\ \\ (x\\in Dc)\" eeimg=\"1\"/> </p><p>这样，我们获得了一组参数，用这组参数确定的模型可以计算P(x|c)，代入贝叶斯公式（其他所需的两项很容易获得），我们就能得到给定属性时属于各个类别的概率，比较这些概率，选出最大的那一个，就得到了预测结果。</p><p>此外，不难看出，因为一直在使用概率，这一算法有很好的<b>可解释性</b>。</p><p>贝叶斯决策论，除了极大似然估计之外，还有很多其他的应用，比如大名鼎鼎的朴素贝叶斯等等。我们将在下篇文章里介绍这些算法。</p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "贝叶斯分类", 
                    "tagLink": "https://api.zhihu.com/topics/19576062"
                }, 
                {
                    "tag": "贝叶斯理论", 
                    "tagLink": "https://api.zhihu.com/topics/19632222"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/34716966", 
            "userName": "陈德龙", 
            "userLink": "https://www.zhihu.com/people/fb342217a29a19462616c21ace65a810", 
            "upvote": 2, 
            "title": "机器学习笔记（三） 支持向量机 原型、对偶问题", 
            "content": "<h2><b>零、摘要</b></h2><p>本篇文章讲述支持向量机的<b>原型</b>与他的<b>拉格朗日对偶问题</b>。</p><p>主要参考资料：</p><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>斯坦福大学 CS229 笔记 吴恩达</li><li>《机器学习》周志华</li><li>《机器学习实战》peter Harrington</li><li>《高等数学》同济大学</li><li>《微积分学教程》【俄】菲赫金格尔茨</li><li><a href=\"https://link.zhihu.com/?target=https%3A//zh.wikipedia.org/wiki/%25E6%2594%25AF%25E6%258C%2581%25E5%2590%2591%25E9%2587%258F%25E6%259C%25BA\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">维基百科 支持向量机</a></li></ul><h2><b>一、原型</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p>支持向量机（support vector machine）处理的是<b>分类</b>问题。首先，我们考虑这样一个问题，<b>二维</b>平面上有两个点集，要画一条<b>一维</b>直线把他们分开。 </p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-29d6291b2f9278b92349f159d734027b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"600\" data-rawheight=\"445\" class=\"origin_image zh-lightbox-thumb\" width=\"600\" data-original=\"https://pic4.zhimg.com/v2-29d6291b2f9278b92349f159d734027b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;600&#39; height=&#39;445&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"600\" data-rawheight=\"445\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"600\" data-original=\"https://pic4.zhimg.com/v2-29d6291b2f9278b92349f159d734027b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-29d6291b2f9278b92349f159d734027b_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>如图，A是给定的点集，B，C，D分别尝试用一条直线分开。直观来看，D中的直线比B，C中的要更合理一些。<b>支持向量机</b>算法就是要求得这一最为合理的直线。点集中距离这条直线最近的那些点被称作<b>支持向量</b></p><p class=\"ztext-empty-paragraph\"><br/></p><p>这里，我们的输入属性值共有两种（<b>二维</b>点集），我们可以将分割的概念推广到高维空间中去。已知二维的点集可以用一维的直线分开（假定点集是<b>线性可分</b>的），那么在同样的假定下，三维空间中的点集可以用二维的平面分开，n维空间中的点集可以用n-1维的“平面”分开。由于人类无法直观感受到三维以上的空间，我么这些情形，统一把这“平面”称作<b>超平面</b>。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在n维空间中，要描述这样一个超平面，我们可以写： <img src=\"https://www.zhihu.com/equation?tex=w%5ET+x+%2B+b%3D0+\" alt=\"w^T x + b=0 \" eeimg=\"1\"/> ,其中法向量 <img src=\"https://www.zhihu.com/equation?tex=w%3D%28w_1%3Bw_2%3B.......%3Aw_n%29\" alt=\"w=(w_1;w_2;.......:w_n)\" eeimg=\"1\"/> 。我们对于一个样本点x，当 <img src=\"https://www.zhihu.com/equation?tex=w%5ET+x+%2B+b%3E0\" alt=\"w^T x + b&gt;0\" eeimg=\"1\"/> 时，就预测他为正类， <img src=\"https://www.zhihu.com/equation?tex=w%5ET+x+%2B+b%3C0\" alt=\"w^T x + b&lt;0\" eeimg=\"1\"/> 时，就预测他为反类（这里只讨论二元分类）。 同时，我们有m个训练样本，对应于上图中的m个点。m个点中第i个点有属性 <img src=\"https://www.zhihu.com/equation?tex=y_i\" alt=\"y_i\" eeimg=\"1\"/> ，即该点是方块还是圆圈。</p><p>有了这些符号的定义，我们可以写出支持向量机的原型/基本型： <img src=\"https://www.zhihu.com/equation?tex=min+_w%5C%2C_b+1%2F2%7C%7Cw%7C%7C%5E2%5C+%5C%5Cs.t.%5C+y_i%28w%5ET+x_i%2Bb%29+%5Cge+1%EF%BC%8Ci%3D1%2C2%2C...%2Cm\" alt=\"min _w\\,_b 1/2||w||^2\\ \\\\s.t.\\ y_i(w^T x_i+b) \\ge 1，i=1,2,...,m\" eeimg=\"1\"/> </p><p>即在对于每一个样本点满足条件 <img src=\"https://www.zhihu.com/equation?tex=y%28w%5ET+x%2Bb%29%5Cge+1\" alt=\"y(w^T x+b)\\ge 1\" eeimg=\"1\"/> 的情况下，通过最小化<img src=\"https://www.zhihu.com/equation?tex=1%2F2%7C%7Cw%7C%7C%5E2\" alt=\"1/2||w||^2\" eeimg=\"1\"/> 来求得参数w,b。 下面我们将从零开始推导这两行的问题原型。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>首先，我们考虑什么样的超平面分割效果最好？ </p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-f615cec53561fad97522b3a7bc44763a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1635\" data-rawheight=\"1212\" class=\"origin_image zh-lightbox-thumb\" width=\"1635\" data-original=\"https://pic3.zhimg.com/v2-f615cec53561fad97522b3a7bc44763a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1635&#39; height=&#39;1212&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1635\" data-rawheight=\"1212\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1635\" data-original=\"https://pic3.zhimg.com/v2-f615cec53561fad97522b3a7bc44763a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-f615cec53561fad97522b3a7bc44763a_b.jpg\"/></figure><p>我们看到，D中直线和两个类别的点集的轮廓接近平行，换句话说，它离各个属性点集的<b>最小距离最大</b>。推广到高维空间就是最佳超平面与各个属性点集的最小距离最大。这样的超平面产生的分类结果是最为<b>鲁棒</b>的，对未见样本的<b>泛化</b>性能更强。那么现在，事情就转化成了，先求处最小距离表达式，再求这式取最大值时w,b的值。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>我们先求任意点到超平面距离的表达式。 </p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-b70e6a0d58b038f6a1c953578baf7bd0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"671\" data-rawheight=\"626\" class=\"origin_image zh-lightbox-thumb\" width=\"671\" data-original=\"https://pic1.zhimg.com/v2-b70e6a0d58b038f6a1c953578baf7bd0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;671&#39; height=&#39;626&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"671\" data-rawheight=\"626\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"671\" data-original=\"https://pic1.zhimg.com/v2-b70e6a0d58b038f6a1c953578baf7bd0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-b70e6a0d58b038f6a1c953578baf7bd0_b.jpg\"/></figure><p>我们有这张示意图，即在二维空间中的情形（我么可以毫不费力地将下面的结果推广到高维空间）。注意两坐标轴都标为x，这是因为竖轴也是用来表示样本点坐标，而在这个问题中我们用y来表示样本点的属性（上图中就有y=红色或y=绿色）。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>同样在上图中，我们用A来标记某一个（任意一个）样本点 <img src=\"https://www.zhihu.com/equation?tex=x_i\" alt=\"x_i\" eeimg=\"1\"/> ，而B点是通过A点向超平面作垂线与超平面的交点。我们还有超平面的法向量w，因此，w/||w||就是法向量方向上的单位向量。 我们令， <img src=\"https://www.zhihu.com/equation?tex=%7CAB%7C+%3D+%5Cgamma+%5Ei\" alt=\"|AB| = \\gamma ^i\" eeimg=\"1\"/> ，可以得到B点坐标为 <img src=\"https://www.zhihu.com/equation?tex=x_i+-+%5Cgamma%5Ei+w%2F%7C%7Cw%7C%7C\" alt=\"x_i - \\gamma^i w/||w||\" eeimg=\"1\"/> ，同时，点B在超平面上，满足超平面方程 <img src=\"https://www.zhihu.com/equation?tex=w%5ET+x+%2B+b%3D0\" alt=\"w^T x + b=0\" eeimg=\"1\"/> ，将点B坐标带入超平面方程，移项，解得 <img src=\"https://www.zhihu.com/equation?tex=%7CAB%7C%3D%5Cgamma%5Ei%3D+%7Cw%5ET+x_i+%2B+b%7C%2F%7C%7Cw%7C%7C\" alt=\"|AB|=\\gamma^i= |w^T x_i + b|/||w||\" eeimg=\"1\"/> 。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>但存在一个问题，对于上图中的A我们得到的距离是正值，然而对于绿色的点，距离就变成了负值。回想我们的目的是要对这些距离的值进行计算，这符号相异的情形令人十分烦躁。于是我们将两个属性的点的y值分别标记为1和-1，然后在进行距离计算之后，所得距离乘以这个y值，即 <img src=\"https://www.zhihu.com/equation?tex=%5Cgamma%5Ei%3D+yi+%7Cw%5ET+xi+%2B+b%7C%2F%7C%7Cw%7C%7C\" alt=\"\\gamma^i= yi |w^T xi + b|/||w||\" eeimg=\"1\"/> 。这时，我们就有了任意样本点与超平面<b>几何距离</b>的一般表达式。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>回想我们是想要最小距离最大化，那么现在我们可以直接写: <img src=\"https://www.zhihu.com/equation?tex=%5C%5Carg%5C+max+_w%5C%2C_b+%7Bmin_n+%5Cgamma%5Ei%7D+%3D+arg%5C+max_w%5C%2C_b+%7Bmin_n+y_i+%7Cw%5ET+x_i+%2B+b%7C%2F%7C%7Cw%7C%7C%7D+\" alt=\"\\\\arg\\ max _w\\,_b {min_n \\gamma^i} = arg\\ max_w\\,_b {min_n y_i |w^T x_i + b|/||w||} \" eeimg=\"1\"/> 或者，等价地： <img src=\"https://www.zhihu.com/equation?tex=max_%5Cgamma%5C%2C_w%5C%2C_b+%28%5Cgamma%29+%5C%5Cs.t.%5C+y_i%28%7Cw%5ET+x_i+%2B+b%7C%2F%7C%7Cw%7C%7C%29+%5Cge+%5Cgamma%EF%BC%8Ci%3D1%2C2%2C...%2Cm+\" alt=\"max_\\gamma\\,_w\\,_b (\\gamma) \\\\s.t.\\ y_i(|w^T x_i + b|/||w||) \\ge \\gamma，i=1,2,...,m \" eeimg=\"1\"/> 即在确定\\gamma是最小值的情况下（第二行），求他的最大值。我们看到，这时我们得到的就与原型至少在结构上很接近了： <img src=\"https://www.zhihu.com/equation?tex=min_w%5C%2C_b+1%2F2%7C%7Cw%7C%7C%5E2%5C%5Cs.t.%5C+y_i%28w%5ET+x_i%2Bb%29%5Cge1%EF%BC%8Ci%3D1%2C2%2C...%2Cm\" alt=\"min_w\\,_b 1/2||w||^2\\\\s.t.\\ y_i(w^T x_i+b)\\ge1，i=1,2,...,m\" eeimg=\"1\"/> </p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>但是直接求解我们得到的问题相当困难（非凸优化），我们需要对这个问题进行最后的等价转换，使他称为相对更容易求解的原型。</p><p>回想，对于一个样本点x，当 <img src=\"https://www.zhihu.com/equation?tex=+min_w%5C%2C_b+1%2F2%7C%7Cw%7C%7C%5E2%5C%5Cs.t.%5C+y_i%28w%5ET+x_i%2Bb%29%5Cge1%EF%BC%8Ci%3D1%2C2%2C...%2Cm\" alt=\" min_w\\,_b 1/2||w||^2\\\\s.t.\\ y_i(w^T x_i+b)\\ge1，i=1,2,...,m\" eeimg=\"1\"/><img src=\"https://www.zhihu.com/equation?tex=w%5ET+x+%2B+b%3E0\" alt=\"w^T x + b&gt;0\" eeimg=\"1\"/> 时，就预测他为+1， <img src=\"https://www.zhihu.com/equation?tex=w%5ET+x+%2B+b%3C0\" alt=\"w^T x + b&lt;0\" eeimg=\"1\"/> 时，就预测他为-1(跃迁函数)。那么当w和b同时乘以一个大于零的乘数因子时，预测结果不会改变。所以说我们有自由缩放w，b的权利，因此我们不妨限制整个样本集中与超平面距离最小的距离为 <img src=\"https://www.zhihu.com/equation?tex=%5Cgamma%3D1\" alt=\"\\gamma=1\" eeimg=\"1\"/> 。我们由此得到： <img src=\"https://www.zhihu.com/equation?tex=max_%5Cgamma%5C%2C_w%5C%2C_b+%28%5Cgamma%29+%5C%5Cs.t.%5C+y_i%28w%5ET+x_i+%2B+b%29+%5Cge+1%EF%BC%8Ci%3D1%2C2%2C...%2Cm\" alt=\"max_\\gamma\\,_w\\,_b (\\gamma) \\\\s.t.\\ y_i(w^T x_i + b) \\ge 1，i=1,2,...,m\" eeimg=\"1\"/> </p><p>第二行已经于原型完全相同，我们再来关注第一行。回想我们对 <img src=\"https://www.zhihu.com/equation?tex=%5Cgamma\" alt=\"\\gamma\" eeimg=\"1\"/> 的定义\\<img src=\"https://www.zhihu.com/equation?tex=%5Cgamma%3D+y+%7Cw%5ET+xi+%2B+b%7C%2F%7C%7Cw%7C%7C\" alt=\"\\gamma= y |w^T xi + b|/||w||\" eeimg=\"1\"/> ，最大化 <img src=\"https://www.zhihu.com/equation?tex=%5Cgamma\" alt=\"\\gamma\" eeimg=\"1\"/> 等价于最小化||w||等价于最小化<img src=\"https://www.zhihu.com/equation?tex=%7C%7Cw%7C%7C%5E2\" alt=\"||w||^2\" eeimg=\"1\"/> 。为了求导方便，我们增加1/2的乘积因子。所以我们最终得到了支持向量机的原型：</p><p>第二行求出所有距离中的最小值，第一行求他的最大值，我们想获得取最大值时参数w和b的值。</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>二、条件极值与拉格朗日对偶问题</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><h2>1.二元函数的情形</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>我们发现，我们刚刚得到的支持向量机的原型问题是一个<b>凸二次规划</b>问题，能直接用现成的优化计算包来求解，但我们可以导出<b>更加高效</b>的方法。即将原型转化为拉格朗日对偶问题求解。</p><p>我们看到，我们的原型属于一个条件极值问题，即在 <img src=\"https://www.zhihu.com/equation?tex=y_i%28w%5ET+x_i%2Bb%29%5Cge1\" alt=\"y_i(w^T x_i+b)\\ge1\" eeimg=\"1\"/> 的条件下，求 <img src=\"https://www.zhihu.com/equation?tex=1%2F2%7C%7Cw%7C%7C%5E2\" alt=\"1/2||w||^2\" eeimg=\"1\"/> 的极值，现在我们来研究这种问题的解决方法。</p><p>我们考虑这样一个问题：寻求函数 <img src=\"https://www.zhihu.com/equation?tex=z%3Df%28x1%2Cx2%29%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%281.1%29%5C%5C\" alt=\"z=f(x1,x2)\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (1.1)\\\\\" eeimg=\"1\"/> 在满足条件 <img src=\"https://www.zhihu.com/equation?tex=%5Cphi%28x1%2Cx2%29%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%281.2%29%5C%5C\" alt=\"\\phi(x1,x2)\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (1.2)\\\\\" eeimg=\"1\"/> 时的极值点。</p><p>首先，假设函数(1.1)在(x1^0,x2^0)取得极值，那么显然这点需要满足条件(1.2)，即有 <img src=\"https://www.zhihu.com/equation?tex=%5C%5C%5Cphi%28x1%5E0%2Cx2%5E0%29%3D0%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%281.3%29\" alt=\"\\\\\\phi(x1^0,x2^0)=0\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (1.3)\" eeimg=\"1\"/> </p><p>接下来，为了使用隐函数存在定理，我们做如下假定：</p><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>在 <img src=\"https://www.zhihu.com/equation?tex=%28x_1%5E0%2Cx_2%5E0%29\" alt=\"(x_1^0,x_2^0)\" eeimg=\"1\"/> 的某一邻域内 <img src=\"https://www.zhihu.com/equation?tex=f%28x_1%2Cx_2%29%E3%80%81%5Cphi%28x_1%2Cx_2%29\" alt=\"f(x_1,x_2)、\\phi(x_1,x_2)\" eeimg=\"1\"/> 都有连续的一阶偏导数</li><li><img src=\"https://www.zhihu.com/equation?tex=%5Cphi%7Bx_2%7D%28x_1%5E0%2Cx_2%5E0%29\" alt=\"\\phi{x_2}(x_1^0,x_2^0)\" eeimg=\"1\"/> 即该函数关于x_2的偏导数不为零</li></ul><p>则由<b>隐函数存在定理</b>：</p><blockquote>设函数 <img src=\"https://www.zhihu.com/equation?tex=%5Cphi+%28x_1%2Cx_2%29\" alt=\"\\phi (x_1,x_2)\" eeimg=\"1\"/> 在点 <img src=\"https://www.zhihu.com/equation?tex=%28x_1%5E0%2Cx_2%5E0%29\" alt=\"(x_1^0,x_2^0)\" eeimg=\"1\"/> 的某一邻域内具有连续的偏导数，<br/>且 <img src=\"https://www.zhihu.com/equation?tex=%5Cphi+%28x1%5E0%2Cx2%5E0%29%3D0%EF%BC%8C%5Cphi+%7Bx2%7D%28x1%5E0%2Cx2%5E0%29%5Cneq+0%EF%BC%8C\" alt=\"\\phi (x1^0,x2^0)=0，\\phi {x2}(x1^0,x2^0)\\neq 0，\" eeimg=\"1\"/> <br/>则方程 <img src=\"https://www.zhihu.com/equation?tex=%5Cphi+%28x_1%2Cx_2%29%3D0\" alt=\"\\phi (x_1,x_2)=0\" eeimg=\"1\"/> 在点 <img src=\"https://www.zhihu.com/equation?tex=%28x_1%5E0%2Cx_2%5E0%29\" alt=\"(x_1^0,x_2^0)\" eeimg=\"1\"/> 的某一邻域内恒能唯一确定一个连续且具有连续导数的函数 <img src=\"https://www.zhihu.com/equation?tex=x_2%3Dg%28x_1%29\" alt=\"x_2=g(x_1)\" eeimg=\"1\"/> ，它满足条件 <img src=\"https://www.zhihu.com/equation?tex=x_2%5E0+%3D+g%28x_1%5E0%29\" alt=\"x_2^0 = g(x_1^0)\" eeimg=\"1\"/> ，<br/>并有 <img src=\"https://www.zhihu.com/equation?tex=d+x_2%2Fd+x_1+%3D+-%5Cphi+%7Bx_1%7D%2F%5Cphi+%7Bx_2%7D\" alt=\"d x_2/d x_1 = -\\phi {x_1}/\\phi {x_2}\" eeimg=\"1\"/> 。</blockquote><p>可知，方程(1.2)确定了一个连续且具有连续导数的函数 <img src=\"https://www.zhihu.com/equation?tex=x_2%3Dg%28x_1%29\" alt=\"x_2=g(x_1)\" eeimg=\"1\"/> （注意，我们只利用了隐函数的存在性，而并不强求他能被显化），把它带入(1.1)式，结果得到了一个只关于 <img src=\"https://www.zhihu.com/equation?tex=x_1\" alt=\"x_1\" eeimg=\"1\"/> 的函数 <img src=\"https://www.zhihu.com/equation?tex=%5C%5Cz%3Df%28x_1%2C+g%28x_1%29%29%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%281.4%29\" alt=\"\\\\z=f(x_1, g(x_1))\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (1.4)\" eeimg=\"1\"/> </p><p>于是所求极值转化为了(1.4)在 <img src=\"https://www.zhihu.com/equation?tex=x_1%3Dx_1%5E0\" alt=\"x_1=x_1^0\" eeimg=\"1\"/> 时的值（因为我们之前曾假定函数(1)在 <img src=\"https://www.zhihu.com/equation?tex=%28x_1%5E0%2Cx_2%5E0%29\" alt=\"(x_1^0,x_2^0)\" eeimg=\"1\"/> 取得极值）。这一元函数取得极值的条件便是关于自变量的导数为零，即当 <img src=\"https://www.zhihu.com/equation?tex=x_1+%3D+x_1%5E0\" alt=\"x_1 = x_1^0\" eeimg=\"1\"/> 时，有： <img src=\"https://www.zhihu.com/equation?tex=%5C%5Cdz%2Fd+x_1+%3D+f%7Bx_1%7D%28x_1%5E0%2Cx_2%5E0%29%2Bf%7Bx_2%7D%28x_1%5E0%2Cx_2%5E0%29d+x_2%2Fd+x_1+%3D+0\" alt=\"\\\\dz/d x_1 = f{x_1}(x_1^0,x_2^0)+f{x_2}(x_1^0,x_2^0)d x_2/d x_1 = 0\" eeimg=\"1\"/> 注意其中使用了复合函数求导的<b>链式法则</b>。</p><p>由之前隐函数存在性定理中提到的求导公式，可以知道当 <img src=\"https://www.zhihu.com/equation?tex=x_1+%3D+x_1%5E0\" alt=\"x_1 = x_1^0\" eeimg=\"1\"/> 时， <img src=\"https://www.zhihu.com/equation?tex=d+x_2%2Fd+x_1+%3D+-%5Cphi+%7Bx_1%7D%2F%5Cphi+%7Bx_2%7D\" alt=\"d x_2/d x_1 = -\\phi {x_1}/\\phi {x_2}\" eeimg=\"1\"/> ，代入上式，得到 <img src=\"https://www.zhihu.com/equation?tex=%5C%5Cf%7Bx_1%7D%28x_1%5E0%2Cx_2%5E0%29-f%7Bx_2%7D%28x_1%5E0%2Cx_2%5E0%29%5Cphi+%7Bx_1%7D%2F%5Cphi+%7Bx_2%7D+%3D+0\" alt=\"\\\\f{x_1}(x_1^0,x_2^0)-f{x_2}(x_1^0,x_2^0)\\phi {x_1}/\\phi {x_2} = 0\" eeimg=\"1\"/> </p><p>于是，这式与之前的(1.3)式 <img src=\"https://www.zhihu.com/equation?tex=%5Cphi%28x_1%5E0%2Cx_2%5E0%29%3D0\" alt=\"\\phi(x_1^0,x_2^0)=0\" eeimg=\"1\"/> 就是所求极值的条件。</p><p>其实到此为止我们已经完成目的了，但为了方便记忆，我们设 <img src=\"https://www.zhihu.com/equation?tex=f%7Bx_2%7D%28x_1%5E0%2Cx_2%5E0%29%2F%5Cphi+%7Bx_2%7D%28x_1%5E0%2Cx_2%5E0%29%3D-%5Clambda\" alt=\"f{x_2}(x_1^0,x_2^0)/\\phi {x_2}(x_1^0,x_2^0)=-\\lambda\" eeimg=\"1\"/> ，则上述条件可以等价地改写为</p><p>· <img src=\"https://www.zhihu.com/equation?tex=f_%7Bx_1%7D%28x_1%5E0%2Cx_2%5E0%29+%2B+%5Clambda+%5Cphi+_%7Bx_1%7D%28x_1%5E0%2Cx_2%5E0%29%3D0\" alt=\"f_{x_1}(x_1^0,x_2^0) + \\lambda \\phi _{x_1}(x_1^0,x_2^0)=0\" eeimg=\"1\"/> </p><p>· <img src=\"https://www.zhihu.com/equation?tex=+f_%7Bx_2%7D%28x_1%5E0%2Cx_2%5E0%29+%2B+%5Clambda+%5Cphi+_%7Bx_2%7D%28x_1%5E0%2Cx_2%5E0%29%3D0\" alt=\" f_{x_2}(x_1^0,x_2^0) + \\lambda \\phi _{x_2}(x_1^0,x_2^0)=0\" eeimg=\"1\"/> </p><p>· <img src=\"https://www.zhihu.com/equation?tex=%5Cphi%28x_1%5E0%2Cx_2%5E0%29%3D0\" alt=\"\\phi(x_1^0,x_2^0)=0\" eeimg=\"1\"/> </p><p>引进辅助函数 <img src=\"https://www.zhihu.com/equation?tex=L%28x_1%2Cx_2%29%3Df%28x_1%2Cx_2%29+%2B%5Clambda+%5Cphi%28x_1%2Cx_2%29\" alt=\"L(x_1,x_2)=f(x_1,x_2) +\\lambda \\phi(x_1,x_2)\" eeimg=\"1\"/> ，则对这函数分别关于 <img src=\"https://www.zhihu.com/equation?tex=x_1%EF%BC%8Cx_2\" alt=\"x_1，x_2\" eeimg=\"1\"/> 求偏导数，就得到了上面第一、二个式子。</p><p>在这个辅助函数里面， <img src=\"https://www.zhihu.com/equation?tex=L%28x_1%2Cx_2%29\" alt=\"L(x_1,x_2)\" eeimg=\"1\"/> 被称作<b>拉格朗日函数</b>， <img src=\"https://www.zhihu.com/equation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"/> 被称作<b>拉格朗日乘子</b>，这一方法也被称作<b>拉格朗日乘数法</b>。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>2.多元函数的情形</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>我们只讨论了二元函数的条件极值，但回想我们要转化的原型： <img src=\"https://www.zhihu.com/equation?tex=%5C%5Cmin_w%5C%2C_b+1%2F2%7C%7Cw%7C%7C%5E2%5C%5Cs.t.%5C+y_i%28w%5ET+x_i%2Bb%29%5Cge1%EF%BC%8Ci%3D1%2C2%2C...%2Cm\" alt=\"\\\\min_w\\,_b 1/2||w||^2\\\\s.t.\\ y_i(w^T x_i+b)\\ge1，i=1,2,...,m\" eeimg=\"1\"/> 这里要求的极值是关于w的函数，而回想关于超平面的定义： <img src=\"https://www.zhihu.com/equation?tex=w%5ET+x+%2B+b%3D0\" alt=\"w^T x + b=0\" eeimg=\"1\"/> ， 其中法向量 <img src=\"https://www.zhihu.com/equation?tex=w%3D%28w_1%3Bw_2%3B.......%3Aw_n%29\" alt=\"w=(w_1;w_2;.......:w_n)\" eeimg=\"1\"/> 是n维的，我们还需要研究当n大于二时如何导出拉格朗日乘数法。</p><p>为了推广拉格朗日乘数法，我们考虑这样一个问题：求n+m个变元的函数 <img src=\"https://www.zhihu.com/equation?tex=%5C%5Cz%3Df%28x_1%2Cx_2%2Cx3...%2Cx%7Bn%2Bm%7D%29%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%282.1%29\" alt=\"\\\\z=f(x_1,x_2,x3...,x{n+m})\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (2.1)\" eeimg=\"1\"/> 假定这些变元满足m个条件： <img src=\"https://www.zhihu.com/equation?tex=%5C%5C%5Cphi_i%28x_1%2Cx_2%2Cx3...%2Cx_%7Bx%2Bm%7D%29%3D0%5C+%5C+%5C+%5C+%28i%3D1%2C2%2C...%2Cm%29%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%282.2%29\" alt=\"\\\\\\phi_i(x_1,x_2,x3...,x_{x+m})=0\\ \\ \\ \\ (i=1,2,...,m)\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (2.2)\" eeimg=\"1\"/> 时的极值点。</p><p>首先，假设函数(2.1)在 <img src=\"https://www.zhihu.com/equation?tex=%28x_1%5E0%2Cx_2%5E0%2C.....%2Cx_%7Bn%2Bm%7D%5E0%29\" alt=\"(x_1^0,x_2^0,.....,x_{n+m}^0)\" eeimg=\"1\"/> 取得极值，那么显然这点需要满足条件(2.2)，即有 <img src=\"https://www.zhihu.com/equation?tex=%5C%5C%5Cphi_i%28x_1%5E0%2Cx_2%5E0....%2Cx_%7Bn%2Bm%7D%5E0%29%3D0%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%282.3%29\" alt=\"\\\\\\phi_i(x_1^0,x_2^0....,x_{n+m}^0)=0\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (2.3)\" eeimg=\"1\"/> </p><p>接下来，类似于为了使用隐函数存在定理，我们做如下假定：</p><p>· 函数 f 即 <img src=\"https://www.zhihu.com/equation?tex=%5Cphi_i\" alt=\"\\phi_i\" eeimg=\"1\"/> 在点 <img src=\"https://www.zhihu.com/equation?tex=%28x_1%5E0%2Cx_2%5E0%2C.....%2Cx_%7Bn%2Bm%7D%5E0%29\" alt=\"(x_1^0,x_2^0,.....,x_{n+m}^0)\" eeimg=\"1\"/> 的某一邻域内存在关于所有变元的连续偏导数</p><p>· 从偏导数组成的雅可比矩阵内取出的m阶行列式中至少有一个在点 <img src=\"https://www.zhihu.com/equation?tex=%28x_1%5E0%2Cx_2%5E0%2C.....%2Cx_%7Bn%2Bm%7D%5E0%29\" alt=\"(x_1^0,x_2^0,.....,x_{n+m}^0)\" eeimg=\"1\"/> 处不为零</p><p>则由定理：</p><p class=\"ztext-empty-paragraph\"><br/></p><blockquote>假定： <br/>1)一切函数F1,F2,...Fm在以点 <img src=\"https://www.zhihu.com/equation?tex=%28x_1%5E0%2C+x_2%5E0...x_n%5E0%2Cy_1%5E0%2Cy_2%5E0%2C...y_m%5E0%29\" alt=\"(x_1^0, x_2^0...x_n^0,y_1^0,y_2^0,...y_m^0)\" eeimg=\"1\"/> 为中心的某邻域内有定义且连续 <br/>2)在定义域中这些函数关于一切变元的偏导数存在且连续 <br/>3)点 <img src=\"https://www.zhihu.com/equation?tex=%28x_1%5E0%2C+x_2%5E0...x_n%5E0%2Cy_1%5E0%2Cy_2%5E0%2C...y_m%5E0%29\" alt=\"(x_1^0, x_2^0...x_n^0,y_1^0,y_2^0,...y_m^0)\" eeimg=\"1\"/> 满足方程组<img src=\"https://www.zhihu.com/equation?tex=F_i%28x_1%5E0%2C+x_2%5E0...x_n%5E0%2Cy_1%5E0%2Cy_2%5E0%2C...y_m%5E0%29%3D0%5C+%5C+%5C+i%3D1%2C2...%2Cm\" alt=\"F_i(x_1^0, x_2^0...x_n^0,y_1^0,y_2^0,...y_m^0)=0\\ \\ \\ i=1,2...,m\" eeimg=\"1\"/> <br/>4)雅可比式在此处异于零 <br/><br/>则： <br/>1)在点 <img src=\"https://www.zhihu.com/equation?tex=%28x_1%5E0%2C+x_2%5E0...x_n%5E0%2Cy_1%5E0%2Cy_2%5E0%2C...y_m%5E0%29\" alt=\"(x_1^0, x_2^0...x_n^0,y_1^0,y_2^0,...y_m^0)\" eeimg=\"1\"/> 的某邻域内确定 <img src=\"https://www.zhihu.com/equation?tex=y_1%2Cy_2%2C...y_m\" alt=\"y_1,y_2,...y_m\" eeimg=\"1\"/> 为 <img src=\"https://www.zhihu.com/equation?tex=x_1%2Cx_2...%2Cx_n\" alt=\"x_1,x_2...,x_n\" eeimg=\"1\"/> 的单值函数 <br/>2)当 <img src=\"https://www.zhihu.com/equation?tex=x_1%3Dx_1%5E0%2C+...x_n%3Dx_n%5E0\" alt=\"x_1=x_1^0, ...x_n=x_n^0\" eeimg=\"1\"/> 时，这些函数的函数值为 <img src=\"https://www.zhihu.com/equation?tex=y_1%3Dy_1%5E0%2C....y_m+%3D+y_m%5E0\" alt=\"y_1=y_1^0,....y_m = y_m^0\" eeimg=\"1\"/> 这些函数连续且有关于一切变元的偏导数</blockquote><p class=\"ztext-empty-paragraph\"><br/></p><p>可知，（2.2）中 <img src=\"https://www.zhihu.com/equation?tex=x_%7Bn%2B1%7D%2Cx_%7Bn%2B2%7D%2C.....%2Cx_%7Bn%2Bm%7D\" alt=\"x_{n+1},x_{n+2},.....,x_{n+m}\" eeimg=\"1\"/> 就可以写成关于 <img src=\"https://www.zhihu.com/equation?tex=x_1%2Cx_2%2C....x_n\" alt=\"x_1,x_2,....x_n\" eeimg=\"1\"/> 的函数了，即 <img src=\"https://www.zhihu.com/equation?tex=%5C%5Cx_%7Bn%2Bi%7D%3Dg_i%28x_1%2Cx_2%2C...x_n%29%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+i%3D1%2C2%2C.......%2Cm%5C+%5C+%5C+%5C+%282.5%29+\" alt=\"\\\\x_{n+i}=g_i(x_1,x_2,...x_n)\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ i=1,2,.......,m\\ \\ \\ \\ (2.5) \" eeimg=\"1\"/> （注意，我们只利用了这些函数的存在性，而并不强求他能被显化），把它带入(2.1)式，结果得到了一个只关于 <img src=\"https://www.zhihu.com/equation?tex=x_1%2Cx_2%2C...%2Cx_n\" alt=\"x_1,x_2,...,x_n\" eeimg=\"1\"/> 的函数 <img src=\"https://www.zhihu.com/equation?tex=%5C%5Cz%3Df%28x_1%2C+x_2%2C....x_n%2Cg_1%28x_1%2Cx_2%2C...%2Cx_n%29%2Cg_2%28x_1%2Cx_2%2C...x_n%29%2C....g_m%28x_1%2Cx_2%2C...%2Cx_n%29%29%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%282.4%29+\" alt=\"\\\\z=f(x_1, x_2,....x_n,g_1(x_1,x_2,...,x_n),g_2(x_1,x_2,...x_n),....g_m(x_1,x_2,...,x_n))\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (2.4) \" eeimg=\"1\"/> 于是这就转化为了普通极值（而非条件极值）的问题了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>注意到我们得到的函数（2.4）中后面的变量与前面的变量有关，并不是所有变量都能自由地变动的，所以并不能用一般的方法来求他的极值。于是我们还要进行如下一波骚操作：</p><p>不要忘记，我们之前已经有过假设：函数(2.1)在 <img src=\"https://www.zhihu.com/equation?tex=%28x_1%5E0%2Cx2%5E0%2C.....%2Cx_%7Bn%2Bm%7D%5E0%29\" alt=\"(x_1^0,x2^0,.....,x_{n+m}^0)\" eeimg=\"1\"/> <b>取得极值</b>。<br/>那么，在该点的<b>全微分：</b> <img src=\"https://www.zhihu.com/equation?tex=%5C%5Cd+f%28x_1%5E0%2Cx_2%5E0%2C.....%2Cx_%7Bn%2Bm%7D%5E0%29%3Df%27%7Bx_1%7D+d+x_1%2Bf%27%7Bx_2%7D+dx_2%2B...%2Bf%27%7Bx_%7Bn%2Bm%7D%7D+d+x_%7Bn%2Bm%7D%3D0\" alt=\"\\\\d f(x_1^0,x_2^0,.....,x_{n+m}^0)=f&#39;{x_1} d x_1+f&#39;{x_2} dx_2+...+f&#39;{x_{n+m}} d x_{n+m}=0\" eeimg=\"1\"/></p><p>先了解一下下面这个定理：</p><p class=\"ztext-empty-paragraph\"><br/></p><blockquote>如果其中各个微分 <img src=\"https://www.zhihu.com/equation?tex=dx_1%2Cdx_2%2C...%2Cdx_%7Bn%2Bm%7D\" alt=\"dx_1,dx_2,...,dx_{n+m}\" eeimg=\"1\"/> 是任意的，则各个偏导数 <img src=\"https://www.zhihu.com/equation?tex=f%27%7Bx_1%7D%EF%BC%8Cf%27%7Bx_2%7D+%2C.....%2Cf%27%7Bx_%7Bn%2Bm%7D%7D+\" alt=\"f&#39;{x_1}，f&#39;{x_2} ,.....,f&#39;{x_{n+m}} \" eeimg=\"1\"/> 都等于零。</blockquote><p class=\"ztext-empty-paragraph\"><br/></p><p>又由<b>一阶微分形式的不变性</b>，这条件可以写成 <img src=\"https://www.zhihu.com/equation?tex=%5C%5C%5Csum+%EF%BC%88%5Cpartial+f%2F%5Cpartial+x_j%EF%BC%89+d+x_j%3D0%5C+%5C+%5C+%5C+%5C+j%3D1%2C2%2C...n%2Bm%5C+%5C+%5C+%5C+%5C+%5C+%282.6%29+\" alt=\"\\\\\\sum （\\partial f/\\partial x_j） d x_j=0\\ \\ \\ \\ \\ j=1,2,...n+m\\ \\ \\ \\ \\ \\ (2.6) \" eeimg=\"1\"/> 其中 <img src=\"https://www.zhihu.com/equation?tex=d+x_%7Bn%2B1%7D%2C...d+x_%7Bn%2Bm%7D\" alt=\"d x_{n+1},...d x_{n+m}\" eeimg=\"1\"/> 要理解为（2.5）的微分。</p><p>但是我们不能断定各个偏导数 <img src=\"https://www.zhihu.com/equation?tex=f%27%7Bx_1%7D%EF%BC%8Cf%27%7Bx_2%7D+%2C.....%2Cf%27%7Bx_%7Bn%2Bm%7D%7D+\" alt=\"f&#39;{x_1}，f&#39;{x_2} ,.....,f&#39;{x_{n+m}} \" eeimg=\"1\"/> 都等于零。因为各个微分 <img src=\"https://www.zhihu.com/equation?tex=dx_1%2Cdx_2%2C...%2Cdx_%7Bn%2Bm%7D\" alt=\"dx_1,dx_2,...,dx_{n+m}\" eeimg=\"1\"/> 不是任意的（回想刚才了解的定理和之前说过的：并不是所有变量都能自由地变动的）。</p><p>这样的情形同样令人十分烦躁。所以我们要试图使问题只涉及可以任意选取的微分，即前n个微分。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>回想我们有m个条件方程（2.2） <img src=\"https://www.zhihu.com/equation?tex=%5C%5C%5Cphi_i%28x_1%2Cx_2%2Cx_3...%2Cx_%7Bx%2Bm%7D%29%3D0%5C+%5C+%5C+%5C+%28i%3D1%2C2%2C...%2Cm%29\" alt=\"\\\\\\phi_i(x_1,x_2,x_3...,x_{x+m})=0\\ \\ \\ \\ (i=1,2,...,m)\" eeimg=\"1\"/> ：显然这些<b>恒等于零</b>的函数的全微分也恒等于零。我们取这m个方程的全微分，并令他们为零： <img src=\"https://www.zhihu.com/equation?tex=%5C%5C%5Csum+%EF%BC%88%5Cpartial+%5Cphi_i%2F%5Cpartial+x_j%EF%BC%89+d+x_j%3D0%5C+%5C+%5C+%5C+%5C+%EF%BC%88i%3D1%2C2%2C....m%EF%BC%9Bj%3D1%2C2%2C...n%2Bm%EF%BC%89%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%282.7%29\" alt=\"\\\\\\sum （\\partial \\phi_i/\\partial x_j） d x_j=0\\ \\ \\ \\ \\ （i=1,2,....m；j=1,2,...n+m）\\ \\ \\ \\ \\ \\ \\ \\ (2.7)\" eeimg=\"1\"/> </p><p>回想很久之前我们做过这样一个假定： 从偏导数组成的雅可比矩阵内取出的m阶行列式中至少有一个在点 <img src=\"https://www.zhihu.com/equation?tex=%28x_1%5E0%2Cx_2%5E0%2C.....%2Cx_%7Bn%2Bm%7D%5E0%29\" alt=\"(x_1^0,x_2^0,.....,x_{n+m}^0)\" eeimg=\"1\"/> 处不为零。因此，从（2.7）中我们可以将 <img src=\"https://www.zhihu.com/equation?tex=d+x_%7Bn%2B1%7D%2C...d+x_%7Bn%2Bm%7D\" alt=\"d x_{n+1},...d x_{n+m}\" eeimg=\"1\"/> 用 <img src=\"https://www.zhihu.com/equation?tex=dx_1%2Cdx_2%2C...%2Cdx_n\" alt=\"dx_1,dx_2,...,dx_n\" eeimg=\"1\"/> 表达。把这些表达式代入(2.6)： <img src=\"https://www.zhihu.com/equation?tex=%5C%5C%5Csum+%EF%BC%88%5Cpartial+f%2F%5Cpartial+x_j%EF%BC%89+d+x_j%3D0%5C+%5C+%5C+%5C+%5C+j%3D1%2C2%2C...n%2Bm+\" alt=\"\\\\\\sum （\\partial f/\\partial x_j） d x_j=0\\ \\ \\ \\ \\ j=1,2,...n+m \" eeimg=\"1\"/> 得到了等式形如： <img src=\"https://www.zhihu.com/equation?tex=%5C%5CA_1+d+x_1%2B...%2BA_n+d+x_n+%3D+0+\" alt=\"\\\\A_1 d x_1+...+A_n d x_n = 0 \" eeimg=\"1\"/> 我们知道这里的n个微分是任意的，回想之前了解的但不能使用的那个定理，他在这时终于启用处了。我们得到： <img src=\"https://www.zhihu.com/equation?tex=A_i+%3D+0%5C+%5C+%5C+%5C+i%3D1%2C2%2C...n+\" alt=\"A_i = 0\\ \\ \\ \\ i=1,2,...n \" eeimg=\"1\"/> 这n个方程加上m个条件方程一共有n+m个方程，可以解出n+m个未知量。 <img src=\"https://www.zhihu.com/equation?tex=x_1%2Cx_2%2Cx_3...%2Cx_%7Bn%2Bm%7D\" alt=\"x_1,x_2,x_3...,x_{n+m}\" eeimg=\"1\"/> 。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>但非常容易能够知道，上述方法会导致非常非常复杂的计算。伟大的拉格朗日提出了解决方法：把（2.7） 中的各个等式 <img src=\"https://www.zhihu.com/equation?tex=%5C%5C%5Csum+%EF%BC%88%5Cpartial+%5Cphi_i%2F%5Cpartial+x_j%EF%BC%89+d+x_j%3D0%5C+%5C+%5C+%5C+%5C+%EF%BC%88i%3D1%2C2%2C....m%EF%BC%9Bj%3D1%2C2%2C...n%2Bm%EF%BC%89\" alt=\"\\\\\\sum （\\partial \\phi_i/\\partial x_j） d x_j=0\\ \\ \\ \\ \\ （i=1,2,....m；j=1,2,...n+m）\" eeimg=\"1\"/> 依次乘以（不定的）乘数 <img src=\"https://www.zhihu.com/equation?tex=%5Clambda_i+%5C+%28i%3D1%2C2%2C...%2Cm%29\" alt=\"\\lambda_i \\ (i=1,2,...,m)\" eeimg=\"1\"/> ，然后将所得结果与（2.6） <img src=\"https://www.zhihu.com/equation?tex=%5C%5C%5Csum+%EF%BC%88%5Cpartial+f%2F%5Cpartial+x_j%EF%BC%89+d+x_j%3D0%5C+%5C+%5C+%5C+%5C+%28j%3D1%2C2%2C...n%2Bm%29\" alt=\"\\\\\\sum （\\partial f/\\partial x_j） d x_j=0\\ \\ \\ \\ \\ (j=1,2,...n+m)\" eeimg=\"1\"/> 相加。得到等式： <img src=\"https://www.zhihu.com/equation?tex=%5C%5C%5Csum+%28%5Cpartial+f%2F%5Cpartial+x_j+%2B%5Csum%5Clambda_i%5Cpartial+%5Cphi_i%2F%5Cpartial+x_j+%29+d+x_j%3D0%5C+%5C+%5C+%5C+%28i%3D1%2C2%2C...m%EF%BC%9Bj%3D1%2C2%2C....%2Cn%2Bm%29%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%282.8%29\" alt=\"\\\\\\sum (\\partial f/\\partial x_j +\\sum\\lambda_i\\partial \\phi_i/\\partial x_j ) d x_j=0\\ \\ \\ \\ (i=1,2,...m；j=1,2,....,n+m)\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (2.8)\" eeimg=\"1\"/> </p><p>回想我们之前使用过的那个假定： 从偏导数组成的雅可比矩阵内取出的m阶行列式中至少有一个在点 <img src=\"https://www.zhihu.com/equation?tex=%28x_1%5E0%2Cx_2%5E0%2C.....%2Cx_%7Bn%2Bm%7D%5E0%29\" alt=\"(x_1^0,x_2^0,.....,x_{n+m}^0)\" eeimg=\"1\"/> 处不为零。<br/>这假定使得下面的操作成为可能 ：</p><p>这样选取乘数 <img src=\"https://www.zhihu.com/equation?tex=%5Clambda_i+%5C+%28i%3D1%2C2%2C...m%29\" alt=\"\\lambda_i \\ (i=1,2,...m)\" eeimg=\"1\"/> 的值，使得（2.8）中微分 <img src=\"https://www.zhihu.com/equation?tex=d+x_%7Bn%2B1%7D%2C...d+x_%7Bn%2Bm%7D\" alt=\"d x_{n+1},...d x_{n+m}\" eeimg=\"1\"/> 前得系数刚好等于零： <img src=\"https://www.zhihu.com/equation?tex=%5C%5C%5Cpartial+f%2F%5Cpartial+x_j+%2B%5Csum%5Clambda_i%5Cpartial+%5Cphi_i%2F%5Cpartial+x_j+%3D0+%5C+%5C+%28j%3Dn%2B1%2C+n%2B2%2C....%2Cn%2Bm%29%5C+%5C+%5C+%5C+%5C+%5C+%282.9%29\" alt=\"\\\\\\partial f/\\partial x_j +\\sum\\lambda_i\\partial \\phi_i/\\partial x_j =0 \\ \\ (j=n+1, n+2,....,n+m)\\ \\ \\ \\ \\ \\ (2.9)\" eeimg=\"1\"/> </p><p>这样确定了乘数 <img src=\"https://www.zhihu.com/equation?tex=%5Clambda+_i+%28i%3D1%2C2%2C...m%29\" alt=\"\\lambda _i (i=1,2,...m)\" eeimg=\"1\"/> 的值之后，之前的等式（2.8）就成为了 <img src=\"https://www.zhihu.com/equation?tex=%5C%5C%5Csum+%28%5Cpartial+f%2F%5Cpartial+x_j+%2B%5Csum%5Clambda_i%28%5Cpartial+%5Cphi_i%2F%5Cpartial+x_j+%29+d+x_j%3D0%5C+%5C+%5C+%5C+%28i%3D1%2C2%2C...m%EF%BC%9Bj%3Dn%2B1%2C....%2Cn%2Bm%29+\" alt=\"\\\\\\sum (\\partial f/\\partial x_j +\\sum\\lambda_i(\\partial \\phi_i/\\partial x_j ) d x_j=0\\ \\ \\ \\ (i=1,2,...m；j=n+1,....,n+m) \" eeimg=\"1\"/> </p><p class=\"ztext-empty-paragraph\"><br/></p><p>我们愉快地发现，这里面只剩下前n个任意变换得微分了，这样，他们之前得系数应都为零。这是因为之前的定理：</p><p>如果其中各个微分 <img src=\"https://www.zhihu.com/equation?tex=dx_1%2Cdx_2%2C...%2Cdx_%7Bn%2Bm%7D\" alt=\"dx_1,dx_2,...,dx_{n+m}\" eeimg=\"1\"/> 是任意的，则各个偏导数 <img src=\"https://www.zhihu.com/equation?tex=f%27%7Bx_1%7D%EF%BC%8Cf%27%7Bx_2%7D+%2C.....%2Cf%27%7Bx_%7Bn%2Bm%7D%7D+\" alt=\"f&#39;{x_1}，f&#39;{x_2} ,.....,f&#39;{x_{n+m}} \" eeimg=\"1\"/> 都等于零。</p><p>于是我们有了： <img src=\"https://www.zhihu.com/equation?tex=%5C%5C%5Csum+%28%5Cpartial+f%2F%5Cpartial+x_j+%2B%5Csum%5Clambda_i%28%5Cpartial+%5Cphi_i%2F%5Cpartial+x_j+%29+d+x_j%3D0%5C+%5C+%5C+%5C+%28i%3D1%2C2%2C...m%EF%BC%9Bj%3D1%2C2%2C....%2Cn%29\" alt=\"\\\\\\sum (\\partial f/\\partial x_j +\\sum\\lambda_i(\\partial \\phi_i/\\partial x_j ) d x_j=0\\ \\ \\ \\ (i=1,2,...m；j=1,2,....,n)\" eeimg=\"1\"/> 加上之前的，我们有了： <img src=\"https://www.zhihu.com/equation?tex=%5C%5C%5Csum+%28%5Cpartial+f%2F%5Cpartial+x_j+%2B%5Csum%5Clambda_i%28%5Cpartial+%5Cphi_i%2F%5Cpartial+x_j+%29+d+x_j%3D0%5C+%5C+%5C+%5C+%28i%3D1%2C2%2C...m%EF%BC%9Bj%3D1%2C2%2C....%2Cn%2Bm%29\" alt=\"\\\\\\sum (\\partial f/\\partial x_j +\\sum\\lambda_i(\\partial \\phi_i/\\partial x_j ) d x_j=0\\ \\ \\ \\ (i=1,2,...m；j=1,2,....,n+m)\" eeimg=\"1\"/> （关注 j 的取值）</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>其实到此为止我们已经完成目的了，但为了方便记忆，与二元函数的情形类似地，我们引入辅助函数 <img src=\"https://www.zhihu.com/equation?tex=F%3Df%2B%5Clambda_1+%5Cphi+_1%2B...%2B%5Clambda_m+%5Cphi_m\" alt=\"F=f+\\lambda_1 \\phi _1+...+\\lambda_m \\phi_m\" eeimg=\"1\"/> ，这样，上面的方程就可以写成 <img src=\"https://www.zhihu.com/equation?tex=%5C%5C%5Cpartial+F%2F+%5Cpartial+x_j+%3D+0%5C+%5C+%5C+%5C+%28j%3D1%2C2%2C...%2Cn%2Bm%29\" alt=\"\\\\\\partial F/ \\partial x_j = 0\\ \\ \\ \\ (j=1,2,...,n+m)\" eeimg=\"1\"/> <b>至此，我们终于完成了拉格朗日乘数法由二元向多元的推广。</b></p><p>重述一遍多元函数的拉格朗日乘数法：</p><p class=\"ztext-empty-paragraph\"><br/></p><blockquote>求n+m个变元的函数 <img src=\"https://www.zhihu.com/equation?tex=z%3Df%28x_1%2Cx_2%2Cx_3...%2Cx_%7Bn%2Bm%7D%29\" alt=\"z=f(x_1,x_2,x_3...,x_{n+m})\" eeimg=\"1\"/> 假定这些变元满足m个条件： <img src=\"https://www.zhihu.com/equation?tex=%5Cphi_i%28x_1%2Cx_2%2Cx_3...%2Cx_%7Bn%2Bm%7D%29%3D0%5C+%5C+%5C+%5C+%28i%3D1%2C2%2C...%2Cm%29\" alt=\"\\phi_i(x_1,x_2,x_3...,x_{n+m})=0\\ \\ \\ \\ (i=1,2,...,m)\" eeimg=\"1\"/> 时的极值点。</blockquote><p class=\"ztext-empty-paragraph\"><br/></p><p>我们的做法是：</p><p class=\"ztext-empty-paragraph\"><br/></p><blockquote>引入拉格朗日函数 <img src=\"https://www.zhihu.com/equation?tex=F%3Df%2B%5Clambda_1+%5Cphi+_1%2B...%2B%5Clambda_m%5Cphi_m\" alt=\"F=f+\\lambda_1 \\phi _1+...+\\lambda_m\\phi_m\" eeimg=\"1\"/> 并令 <img src=\"https://www.zhihu.com/equation?tex=%5C%5C%5Cpartial+F%2F+%5Cpartial+x_j+%3D+0%5C+%5C+%5C+%5C+%28j%3D1%2C2%2C...%2Cn%2Bm%29\" alt=\"\\\\\\partial F/ \\partial x_j = 0\\ \\ \\ \\ (j=1,2,...,n+m)\" eeimg=\"1\"/> 从而解出所有的自变量 <img src=\"https://www.zhihu.com/equation?tex=x_1%2Cx_2%2C...x_%7Bn%2Bm%7D\" alt=\"x_1,x_2,...x_{n+m}\" eeimg=\"1\"/> </blockquote><h2>3.广义拉格朗日</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>再来回想我们的支持向量机原型： <img src=\"https://www.zhihu.com/equation?tex=%5C%5Cmin_w%5C%2C_b+1%2F2%7C%7Cw%7C%7C%5E2%5C%5C%5C+s.t.%5C+y_i%28w%5ET+x_i%2Bb%29%5Cge1%EF%BC%8Ci%3D1%2C2%2C...%2Cm\" alt=\"\\\\min_w\\,_b 1/2||w||^2\\\\\\ s.t.\\ y_i(w^T x_i+b)\\ge1，i=1,2,...,m\" eeimg=\"1\"/> <br/>注意到，这里的条件是<b>不等式</b>。为了能将拉格朗日乘数法应用到这个问题上，我们需要再对它进行推广。 接下来，我们不再使用 <img src=\"https://www.zhihu.com/equation?tex=x_1%2Cx_2%2Cx_3...%2Cx_%7Bn%2Bm%7D\" alt=\"x_1,x_2,x_3...,x_{n+m}\" eeimg=\"1\"/> 做为自变量，转而使用 <img src=\"https://www.zhihu.com/equation?tex=w%3D%28w_1%3Bw_2%3B.......%3Bw_n%29\" alt=\"w=(w_1;w_2;.......;w_n)\" eeimg=\"1\"/> 。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>我们把要讨论的<b>主问题</b>记为：<img src=\"https://www.zhihu.com/equation?tex=min_w+f%28w%29%5C%5C%5C+s.t.%5C+g_i%28w%29%5Cle+0%EF%BC%8Ci%3D1%2C2%2C...%2Ck+%5C%5C%5C+h_i%28w%29%3D0%EF%BC%8C+i%3D1%2C2%2C...l+\" alt=\"min_w f(w)\\\\\\ s.t.\\ g_i(w)\\le 0，i=1,2,...,k \\\\\\ h_i(w)=0， i=1,2,...l \" eeimg=\"1\"/> 即我们要最小化关于w的函数f（w），约束条件包括关于w的k个不等式和l个等式。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>我们定义<b>广义拉格朗日函数</b>： <img src=\"https://www.zhihu.com/equation?tex=L%28w%2C+a%2C+b%29%3Df%28w%29%2B%5Csum+a_i+g_i%28w%29%2B%5Csum+b_i+h_i%28w%29\" alt=\"L(w, a, b)=f(w)+\\sum a_i g_i(w)+\\sum b_i h_i(w)\" eeimg=\"1\"/> 其中各个a,b都是拉格朗日乘数。 我们要求这一函数在 <img src=\"https://www.zhihu.com/equation?tex=a_i+%5Cge+0\" alt=\"a_i \\ge 0\" eeimg=\"1\"/> 时的最大值 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta+p%28w%29%3Dmax_%7Ba_i%3E0%7DL%28w%2C+a%2C+b%29\" alt=\"\\theta p(w)=max_{a_i&gt;0}L(w, a, b)\" eeimg=\"1\"/> 其中脚标p代表主问题（primal）。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>我们来思考为什么这个取最大值的操作有意义呢？我们的限制条件是 <img src=\"https://www.zhihu.com/equation?tex=g_i%28w%29%5Cle+0\" alt=\"g_i(w)\\le 0\" eeimg=\"1\"/> ，假如有某一个 <img src=\"https://www.zhihu.com/equation?tex=+g_i%28w%29%5Cge+0\" alt=\" g_i(w)\\ge 0\" eeimg=\"1\"/> ，我们又是要将整个函数最大化，这就迫使乘数 <img src=\"https://www.zhihu.com/equation?tex=a_i+%5Cge+0\" alt=\"a_i \\ge 0\" eeimg=\"1\"/> 越大越好，于是整个函数就趋向于无穷。反而如果所有条件都满足 <img src=\"https://www.zhihu.com/equation?tex=g_i%28w%29%5Cle+0\" alt=\"g_i(w)\\le 0\" eeimg=\"1\"/> ，那么无论乘数多大，这一项 <img src=\"https://www.zhihu.com/equation?tex=a_i+g_i%28w%29\" alt=\"a_i g_i(w)\" eeimg=\"1\"/> 都不会出现在拉格朗日函数之中（对于 <img src=\"https://www.zhihu.com/equation?tex=h_i%28w%29%5Cneq+0\" alt=\"h_i(w)\\neq 0\" eeimg=\"1\"/> 也是同样的道理）。也就是说， <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta+_p\" alt=\"\\theta _p\" eeimg=\"1\"/> 有这样的性质：<br/>如果违背了任一条件，则它趋向于正无穷；如果所有条件都满足，则它的值等于L(w,a,b)。</p><p>我们不希望违背条件， 所以我们不希望 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta+p\" alt=\"\\theta p\" eeimg=\"1\"/> 趋向于正无穷， 所以我们希望 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_p\" alt=\"\\theta_p\" eeimg=\"1\"/> 趋向于零， 所以我们希望最小化 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta+_p\" alt=\"\\theta _p\" eeimg=\"1\"/> 。</p><p>设p为主问题的解，即 <img src=\"https://www.zhihu.com/equation?tex=p%3Dmin_w%5Ctheta+_p+%28w%29%3Dmin_w%5C+max_%7Ba_i%3E0%7DL%28w%2C+a%2C+b%29\" alt=\"p=min_w\\theta _p (w)=min_w\\ max_{a_i&gt;0}L(w, a, b)\" eeimg=\"1\"/> </p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>高潮来了，定义<b>对偶问题</b>： <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_d%3Dmin_w+L%28w%2Ca%2Cb%29\" alt=\"\\theta_d=min_w L(w,a,b)\" eeimg=\"1\"/> ，其中 d 代表着“对偶”（dual） 我们设d为对偶问题的解，下面这个式子： <img src=\"https://www.zhihu.com/equation?tex=d%3Dmax_%7Ba_i%3E0%7D%5C+min_w+L%28w%2Ca%2Cb%29+%5Cle+min_w%5C+max_%7Ba_i%3E0%7DL%28w%2C+a%2C+b%29+%3D+p%2A+\" alt=\"d=max_{a_i&gt;0}\\ min_w L(w,a,b) \\le min_w\\ max_{a_i&gt;0}L(w, a, b) = p* \" eeimg=\"1\"/> </p><p><b>当满足KKT条件时等号成立。</b></p><p>我们一会再来说这莫名其妙的KKT条件。先观察上面的式子，即一个函数<b>最小值中的最大值</b>\\le<b>最大值中的最小值</b>。</p><p>我们再列出KKT（Karush-Kuhn-Tucker）条件：</p><p>· <img src=\"https://www.zhihu.com/equation?tex=%5Cpartial+L%2F%5Cpartial+w_i+%3D+0%EF%BC%8Ci%3D1%2C2%2C...n\" alt=\"\\partial L/\\partial w_i = 0，i=1,2,...n\" eeimg=\"1\"/> </p><p>· <img src=\"https://www.zhihu.com/equation?tex=a_i+g_i%28w%29+%3D+0%EF%BC%8Ci%3D1%2C2%2C...k\" alt=\"a_i g_i(w) = 0，i=1,2,...k\" eeimg=\"1\"/> </p><p>· <img src=\"https://www.zhihu.com/equation?tex=g_i%28w%29%5Cle+0%EF%BC%8Ci%3D1%2C2%2C...%2Ck\" alt=\"g_i(w)\\le 0，i=1,2,...,k\" eeimg=\"1\"/> </p><p>· <img src=\"https://www.zhihu.com/equation?tex=a_i%5Cge+0%EF%BC%8Ci%3D1%2C2%2C...k\" alt=\"a_i\\ge 0，i=1,2,...k\" eeimg=\"1\"/> </p><p>好的，再次回忆我们的原型（简单地移项）： <img src=\"https://www.zhihu.com/equation?tex=min_w%5C%2C_b+1%2F2%7C%7Cw%7C%7C%5E2%5C%5C%5C+s.t.%5C+%5C+1-y_i%28w%5ET+x_i%2Bb%29%5Cle0%EF%BC%8Ci%3D1%2C2%2C...%2Cm\" alt=\"min_w\\,_b 1/2||w||^2\\\\\\ s.t.\\ \\ 1-y_i(w^T x_i+b)\\le0，i=1,2,...,m\" eeimg=\"1\"/> </p><p>将 <img src=\"https://www.zhihu.com/equation?tex=1-y_i%28w%5ETx_i%2Bb%29\" alt=\"1-y_i(w^Tx_i+b)\" eeimg=\"1\"/> 看作 <img src=\"https://www.zhihu.com/equation?tex=g_i%28w%29\" alt=\"g_i(w)\" eeimg=\"1\"/> ，我们就有拉格朗日函数 <img src=\"https://www.zhihu.com/equation?tex=L%28w%2Ca%2Cb%29%3D1%2F2%7C%7Cw%7C%7C%5E2%2B%5Csum+a_i%281-y_i%28w%5ET+x_i%2Bb%29%29%5C+%5C+%5C+%5C+%5C+i%3D1%2C2%2C...m\" alt=\"L(w,a,b)=1/2||w||^2+\\sum a_i(1-y_i(w^T x_i+b))\\ \\ \\ \\ \\ i=1,2,...m\" eeimg=\"1\"/> </p><p>要注意，这里L（w,a,b）中的b和之前的b是不同的概念。</p><p>我们令L（w,a,b）对w和b的偏导数为零得到</p><p><img src=\"https://www.zhihu.com/equation?tex=w%3D%5Csum+a_i+y_i+x_i\" alt=\"w=\\sum a_i y_i x_i\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=0%3D%5Csum+a_i+y_i\" alt=\"0=\\sum a_i y_i\" eeimg=\"1\"/> </p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>综合上面几个式子，我们终于得到关于支持向量机原型的对偶问题：</b> <img src=\"https://www.zhihu.com/equation?tex=max_a+%5Csum+a_i+-+1%2F2+%5Csum+%5Csum+a_i+a_j+x_i%5ET+x_j+y_i+y_j+%5C%5C%5C+s.t.%5C+%5C+%5C+%5Csum+a_i+y_i%3D0%5C%5C%5C+a_i+%5Cle+0%2C+%5C+%5C+%5C+%5C+%5C%5Ci%3D1%2C2%2C...m%EF%BC%9Bj%3D1%2C2%2C..m\" alt=\"max_a \\sum a_i - 1/2 \\sum \\sum a_i a_j x_i^T x_j y_i y_j \\\\\\ s.t.\\ \\ \\ \\sum a_i y_i=0\\\\\\ a_i \\le 0, \\ \\ \\ \\ \\\\i=1,2,...m；j=1,2,..m\" eeimg=\"1\"/> </p><p>在满足KKT条件</p><p>· <img src=\"https://www.zhihu.com/equation?tex=a_i%5Cle+0\" alt=\"a_i\\le 0\" eeimg=\"1\"/> </p><p>· <img src=\"https://www.zhihu.com/equation?tex=1-y_i%28w%5ET+x_i%2Bb%29+%5Cge+0\" alt=\"1-y_i(w^T x_i+b) \\ge 0\" eeimg=\"1\"/> </p><p>· <img src=\"https://www.zhihu.com/equation?tex=a_i%281-y_i%28w%5ET+x_i%2Bb%29%29+%3D+0\" alt=\"a_i(1-y_i(w^T x_i+b)) = 0\" eeimg=\"1\"/> </p><p>的情形下，我们从上式中解出a，然后就能求出w和b，这样就得到了所求的超平面 <img src=\"https://www.zhihu.com/equation?tex=w%5ET+x+%2B+b\" alt=\"w^T x + b\" eeimg=\"1\"/>。</p><p>但是，同样地，这是一个<b>二次规划</b>问题，<b>问题的规模正比于训练样本数</b>。于是，人们提出了很多高效算法，包括著名的<b>SMO</b>算法。</p><h2><b>三、总结与讨论</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p>本文花费了大量的篇幅讲述支持向量机的原型与其拉格朗日对偶问题的<b>推导过程</b>。虽然说，明白了算法的推导过程并不意味着能够使用好，但是，如果只知道结论而对于结论的来历完全不懂的话，不仅会在接下来的拓展学习中遭遇天花板，更会减少创新的能力。希望有科研志向的读者能够<b>对于每一个算法都不拘泥于仅仅学会使用</b>。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>本文关于拉格朗日对偶问题的推导，二元部分来自高等数学，多元推广来自俄罗斯数学分析的教材。为了便于理解，我尽力将这些部分写得尽可能的友善。但是由于这狗血问题本身的复杂性，我也是在反复阅读之后才明白作者的确切意思（当然现在我才大一，数学功底有限），如果读者没能在读完第一遍的时候完全明白，也千万不要灰心：)</p><p class=\"ztext-empty-paragraph\"><br/></p><p>至此本文更是超过一万一千字，但也仅仅介绍了支持向量机算法最基础部分的一部分。在下一篇文章里，我们将继续讨论基于拉格朗日对偶问题的SMO算法，核函数与核技巧等内容。</p><p></p><p></p>", 
            "topic": [
                {
                    "tag": "人工智能", 
                    "tagLink": "https://api.zhihu.com/topics/19551275"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "SVM", 
                    "tagLink": "https://api.zhihu.com/topics/19583524"
                }
            ], 
            "comments": [
                {
                    "userName": "晒着阳光喝豆浆", 
                    "userLink": "https://www.zhihu.com/people/9dd2a18b36d551ea373880730c099074", 
                    "content": "请问 B点坐标是怎么求的", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/34624474", 
            "userName": "陈德龙", 
            "userLink": "https://www.zhihu.com/people/fb342217a29a19462616c21ace65a810", 
            "upvote": 3, 
            "title": "机器学习笔记（二） 决策树", 
            "content": "<h2><b>零、摘要</b></h2><p>本文讨论决策树的构建、最佳划分的选择、剪枝处理以及缺失值处理。</p><p>主要参考资料：</p><ul><li>《机器学习》周志华</li><li>《机器学习实战》 Peter Harrington</li><li><a href=\"https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/C4.5_algorithm\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">维基百科 C4.5_algorithm</a></li><li><a href=\"https://link.zhihu.com/?target=https%3A//zh.wikipedia.org/wiki/%25E7%2586%25B5_%2528%25E4%25BF%25A1%25E6%2581%25AF%25E8%25AE%25BA%2529\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">维基百科 熵（信息论）</a></li></ul><h2><b>一、构建决策树</b></h2><p>本文引用周志华《机器学习》中西瓜的例子。<br/>假设你是河海大学大四老阿姨，有整整三的在教育超市的买西瓜经验，阅瓜无数，积累了一个数据集，包括买过的西瓜的各种特性（色泽、根蒂、敲声、纹理等等）以及吃瓜后这个瓜是否是好瓜的判断。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-e90a4a6bfdf9c4fd1814b69341afbf1b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"959\" data-rawheight=\"502\" class=\"origin_image zh-lightbox-thumb\" width=\"959\" data-original=\"https://pic4.zhimg.com/v2-e90a4a6bfdf9c4fd1814b69341afbf1b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;959&#39; height=&#39;502&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"959\" data-rawheight=\"502\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"959\" data-original=\"https://pic4.zhimg.com/v2-e90a4a6bfdf9c4fd1814b69341afbf1b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-e90a4a6bfdf9c4fd1814b69341afbf1b_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>现在，你要教急缺买瓜经验的大一萌新陈德龙如何在超市选出好瓜。<br/>作为前辈的你，是一个经验丰富的买瓜老手，挑瓜时有独到的套路：<br/>比如说，首先看瓜的色泽，如果是青绿色的，就观察它的根蒂，如果根蒂是蜷缩着的，就轻敲西瓜听声音，如果是浊响，非常好，这是一个好瓜。<br/></p><figure data-size=\"small\"><noscript><img src=\"https://pic4.zhimg.com/v2-c0dfa878eb12cfaebb2397e109621467_b.jpg\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"1268\" data-rawheight=\"1015\" class=\"origin_image zh-lightbox-thumb\" width=\"1268\" data-original=\"https://pic4.zhimg.com/v2-c0dfa878eb12cfaebb2397e109621467_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1268&#39; height=&#39;1015&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"1268\" data-rawheight=\"1015\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1268\" data-original=\"https://pic4.zhimg.com/v2-c0dfa878eb12cfaebb2397e109621467_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-c0dfa878eb12cfaebb2397e109621467_b.jpg\"/></figure><p>这就是决策树的基本思想。<br/>但是显然在刚才的判断过程中，色泽不只有青绿色一种，其他颜色的瓜中也有好瓜的存在。另外一方面，对于一个瓜，怎样知道先判断色泽更好还是先判断根蒂更好呢？这就是<b>寻找最优划分属性</b>的问题。这一问题我们将在下一部分里讲述，而接下来，我们来试图用伪代码构建一棵决策树。</p><div class=\"highlight\"><pre><code class=\"language-text\">函数 生成决策树（训练集、属性集）：\n\n    如果（训练集中的样本都属于同一类别）：#比如：都是好瓜\n        则return 这一类别的标签；\n\n    如果（训练集中样本在属性集上的取值相同）：#比如：属性集包含色泽和根蒂，训练集中的瓜色泽和根蒂都相同\n        则return 最多类别的标签； #比如：这些色泽和根蒂都相同的瓜里面好瓜占多数，则返回好瓜。\n\n    上面两个条件都不满足，则寻找最优化分属性a；# 比如：找到属性——敲声\n    根据属性a划分训练集，得到几个训练子集。\n\n    循环，对于a中每一个值ai： #比如：a1=浊响， a2=沉闷， a3=清脆\n        生成决策树（ai的训练子集、属性集/a）</code></pre></div><p>上面，训练集相当于高维坐标中的点（x，y）。其中x是一个向量，分量xi是这个瓜在第i个属性上的取值，y代表是不是好瓜。属性集包括买瓜时要考虑的各个因素：色泽、根蒂、敲声、纹理等等。</p><p>我们看到，这是一个<b>递归函数</b>，他在前两个“如果”中各有一个停止条件，在最后一行递归地调用自己。</p><p>接下来，我们就要讨论，代码中部“寻找最优化分属性”这一步是如何进行的。</p><h2><b>二、寻找最优划分属性</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p>1.信息、信息熵和信息增益</p><p>首先，如果一件事情有i种可能的结果，每种结果x<i>i发生的概率为</i> <img src=\"https://www.zhihu.com/equation?tex=p%28x_i%29\" alt=\"p(x_i)\" eeimg=\"1\"/> ，则要确定这件事的结果为 <img src=\"https://www.zhihu.com/equation?tex=x_i\" alt=\"x_i\" eeimg=\"1\"/> 所需的信息为 <img src=\"https://www.zhihu.com/equation?tex=I%28x_i%29+%3D+-+log_2+p%28x_i%29\" alt=\"I(x_i) = - log_2 p(x_i)\" eeimg=\"1\"/> 。<br/>    我们来理解一下这个公式。针对抛硬币这个事件，一共有两个结果 <img src=\"https://www.zhihu.com/equation?tex=x_1\" alt=\"x_1\" eeimg=\"1\"/> =正面 和<img src=\"https://www.zhihu.com/equation?tex=x_2\" alt=\"x_2\" eeimg=\"1\"/> =反面 。他们各自的概率 <img src=\"https://www.zhihu.com/equation?tex=p%28x_1%29+%3D+p%28x_2%29+%3D+0.5\" alt=\"p(x_1) = p(x_2) = 0.5\" eeimg=\"1\"/> 。带入上面的公式中，得到：</p><p>确定抛硬币结果所需的信息 <img src=\"https://www.zhihu.com/equation?tex=%3D+I%28x_i%29+%3D+-+log_2+p%28x_i%29+%3D+-log_2%280.5%29+%3D+1%28bit%29\" alt=\"= I(x_i) = - log_2 p(x_i) = -log_2(0.5) = 1(bit)\" eeimg=\"1\"/> ， 即，我们需要1比特的信息来知道抛硬币得到的是正面还是反面。我们知道，1比特代表1或0，所以这信息的定义也是符合我们的直观感受的。<br/>之所以这里的信息单位为比特，是因为我们在上面的式子里用了以2为底取对数。如果是 <img src=\"https://www.zhihu.com/equation?tex=log_e+p%28x_i%29\" alt=\"log_e p(x_i)\" eeimg=\"1\"/> 即 <img src=\"https://www.zhihu.com/equation?tex=ln+p%28x_i%29\" alt=\"ln p(x_i)\" eeimg=\"1\"/> ,那信息的单位就是<b>nat</b>；如果是以10为底的 <img src=\"https://www.zhihu.com/equation?tex=lg+p%28x_i%29\" alt=\"lg p(x_i)\" eeimg=\"1\"/> ，那单位就是<b>Hart</b>。在本文中，我们使用以2为底取对数。</p><p>接下来我们要讨论信息熵的概念。在1948年，香农将热力学的熵（entropy）引入到信息论，因此信息熵又被称为香农熵。有趣的是，最开始当香农写完信息论时，实际上是冯诺依曼对香农建议用“熵”这个术语，<b>因为大家都不知道香农说的是什么意思</b>。</p><p>我们可以这样理解：信息熵是度量一个集合中，样本<b>混乱度</b>的一种指标。越复杂，信息熵的值越大。<br/>信息熵的定义为 <img src=\"https://www.zhihu.com/equation?tex=Ent%28D%29+%3D+%5Csum+p%28x_i%29I%28x_i%29%3D+-+%5Csum+p%28x_i%29log_2+p%28x_i%29+\" alt=\"Ent(D) = \\sum p(x_i)I(x_i)= - \\sum p(x_i)log_2 p(x_i) \" eeimg=\"1\"/> ，即对于样本集合D，他的信息熵（混乱度）等于所有结果的概率乘上信息再求和。</p><p>直观地理解信息熵：<br/>我们知道 <img src=\"https://www.zhihu.com/equation?tex=p%28x_i%29\" alt=\"p(x_i)\" eeimg=\"1\"/> 的值在0和1之间，那么信息 <img src=\"https://www.zhihu.com/equation?tex=I%28xi%29+%3D+-log_2+p%28x_i%29\" alt=\"I(xi) = -log_2 p(x_i)\" eeimg=\"1\"/> 取非负值，故信息熵的最小值为0。这时即意味着混乱度为零——样本集合很纯粹。比如说一枚硬币两面都是正面，求“抛硬币结果为正面的信息熵”，那 <img src=\"https://www.zhihu.com/equation?tex=p%28x_i%29\" alt=\"p(x_i)\" eeimg=\"1\"/> 恒等于1， 带入后得到信息熵为0。这符合我们的直观感受——结果只可能是正面，一点也不混乱。再关注信息熵的最大值。已知所有概率之和为1，求关于他们的函数的极值。这是一个<b>多元函数条件极值</b>问题，可以借助<b>拉格朗日乘数法</b>解决。限于篇幅这里省略详细过程。结果是当所有符号有同等机会出现的情况下，熵达到最大值（所有可能的事件同等概率时不确定性最高）。所以说，如果一个事件一共有n种结果，那么他的信息熵介于0和 <img src=\"https://www.zhihu.com/equation?tex=log_2%28n%29\" alt=\"log_2(n)\" eeimg=\"1\"/> 之间。</p><p>最后我们引入<b>信息增益</b>。在有了<b>信息</b>和<b>信息熵</b>的概念之后，信息增益的概念就很好理解了。为了说明，我们用D表示已有的样本集合，他的信息熵为Ent(D)，现在我们通过某一个属性a对他进行划分（色泽），其中取第i个特性的所有样本组成的集合（所有青绿色的瓜）记作Di。此时，用属性a对样本集合D进行划分所获得的<b>信息增益（information gain）</b>为：<br/> <img src=\"https://www.zhihu.com/equation?tex=Gain%28D%2Ca%29+%3D+Ent%28D%29+-+%5Csum+%28%7CD_i%7C%2F%7CD%7C%29Ent%28D_i%29\" alt=\"Gain(D,a) = Ent(D) - \\sum (|D_i|/|D|)Ent(D_i)\" eeimg=\"1\"/> <br/>我们说，信息增益越大，则意味着使用属性a来进行划分所获得的<b>纯度提升</b>越大。</p><h2><b>2.ID3</b></h2><p>ID3算法的原理很简单，即迭代地选择<b>信息增益最大的属性</b>进行划分。其中ID是Iterative Dichotomiser（迭代二分器）的简称。这一算法是由澳大利亚计算机科学家昆兰在1979年发表的。1986年machine learning 杂志创刊，昆兰应邀在创刊号重新发表了ID3算法，掀起了决策树研究的热潮。</p><p>下面给出的代码是ID3构建决策树的简单实现（python）。</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"k\">def</span> <span class=\"nf\">calcShannonEnt</span><span class=\"p\">(</span><span class=\"n\">dataSet</span><span class=\"p\">):</span> <span class=\"c1\">#计算信息熵</span>\n    <span class=\"n\">numEntries</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">dataSet</span><span class=\"p\">)</span> <span class=\"c1\">#获得样本总数</span>\n    <span class=\"n\">labelCounts</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>\n    <span class=\"k\">for</span> <span class=\"n\">featVec</span> <span class=\"ow\">in</span> <span class=\"n\">dataSet</span><span class=\"p\">:</span> <span class=\"c1\">#计算不同标签的个数，dataset的每一行是（x,y）,最后一个数据是y，即标签</span>\n        <span class=\"n\">currentLabel</span> <span class=\"o\">=</span> <span class=\"n\">featVec</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"c1\">#Label——标签（好瓜/坏瓜）</span>\n        <span class=\"k\">if</span> <span class=\"n\">currentLabel</span> <span class=\"ow\">not</span> <span class=\"ow\">in</span> <span class=\"n\">labelCounts</span><span class=\"o\">.</span><span class=\"n\">keys</span><span class=\"p\">():</span> <span class=\"n\">labelCounts</span><span class=\"p\">[</span><span class=\"n\">currentLabel</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n        <span class=\"n\">labelCounts</span><span class=\"p\">[</span><span class=\"n\">currentLabel</span><span class=\"p\">]</span> <span class=\"o\">+=</span> <span class=\"mi\">1</span> <span class=\"c1\">#如果是未见标签，则计数器加一</span>\n    <span class=\"n\">shannonEnt</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span>\n    <span class=\"k\">for</span> <span class=\"n\">key</span> <span class=\"ow\">in</span> <span class=\"n\">labelCounts</span><span class=\"p\">:</span>\n        <span class=\"n\">prob</span> <span class=\"o\">=</span> <span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"n\">labelCounts</span><span class=\"p\">[</span><span class=\"n\">key</span><span class=\"p\">])</span><span class=\"o\">/</span><span class=\"n\">numEntries</span> <span class=\"c1\">#计算p(xi)</span>\n        <span class=\"n\">shannonEnt</span> <span class=\"o\">-=</span> <span class=\"n\">prob</span> <span class=\"o\">*</span> <span class=\"n\">log</span><span class=\"p\">(</span><span class=\"n\">prob</span><span class=\"p\">,</span><span class=\"mi\">2</span><span class=\"p\">)</span> <span class=\"c1\">#计算-p(xi)*log_2(p(xi))</span>\n    <span class=\"k\">return</span> <span class=\"n\">shannonEnt</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">splitDataSet</span><span class=\"p\">(</span><span class=\"n\">dataSet</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"p\">,</span> <span class=\"n\">value</span><span class=\"p\">):</span> <span class=\"c1\">#用于分割数据集， axis是用来分割的属性（回忆dataset的每一行是（x,y）），value是属性的取值（如色泽=青绿）</span>\n    <span class=\"n\">retDataSet</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n    <span class=\"k\">for</span> <span class=\"n\">featVec</span> <span class=\"ow\">in</span> <span class=\"n\">dataSet</span><span class=\"p\">:</span>\n        <span class=\"k\">if</span> <span class=\"n\">featVec</span><span class=\"p\">[</span><span class=\"n\">axis</span><span class=\"p\">]</span> <span class=\"o\">==</span> <span class=\"n\">value</span><span class=\"p\">:</span> \n            <span class=\"n\">reducedFeatVec</span> <span class=\"o\">=</span> <span class=\"n\">featVec</span><span class=\"p\">[:</span><span class=\"n\">axis</span><span class=\"p\">]</span>     \n            <span class=\"n\">reducedFeatVec</span><span class=\"o\">.</span><span class=\"n\">extend</span><span class=\"p\">(</span><span class=\"n\">featVec</span><span class=\"p\">[</span><span class=\"n\">axis</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">:])</span>\n            <span class=\"n\">retDataSet</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">reducedFeatVec</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">retDataSet</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">chooseBestFeatureToSplit</span><span class=\"p\">(</span><span class=\"n\">dataSet</span><span class=\"p\">):</span> <span class=\"c1\">#用ID3算法来选择最优分划属性</span>\n    <span class=\"n\">numFeatures</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">dataSet</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">])</span> <span class=\"o\">-</span> <span class=\"mi\">1</span>      <span class=\"c1\">#获得属性总个数（dataset最后一列是标签，前面是属性）</span>\n    <span class=\"n\">baseEntropy</span> <span class=\"o\">=</span> <span class=\"n\">calcShannonEnt</span><span class=\"p\">(</span><span class=\"n\">dataSet</span><span class=\"p\">)</span> <span class=\"c1\">#计算Ent（D）</span>\n    <span class=\"n\">bestInfoGain</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span><span class=\"p\">;</span> <span class=\"n\">bestFeature</span> <span class=\"o\">=</span> <span class=\"o\">-</span><span class=\"mi\">1</span>  <span class=\"c1\">#初始化</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">numFeatures</span><span class=\"p\">):</span>        <span class=\"c1\">#遍历所有属性</span>\n        <span class=\"n\">featList</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">example</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span> <span class=\"k\">for</span> <span class=\"n\">example</span> <span class=\"ow\">in</span> <span class=\"n\">dataSet</span><span class=\"p\">]</span> <span class=\"c1\">#取出第i列的所有属性值</span>\n        <span class=\"n\">uniqueVals</span> <span class=\"o\">=</span> <span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"n\">featList</span><span class=\"p\">)</span>       <span class=\"c1\">#计算这列有几个不同的可能取值</span>\n        <span class=\"n\">newEntropy</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span>\n        <span class=\"k\">for</span> <span class=\"n\">value</span> <span class=\"ow\">in</span> <span class=\"n\">uniqueVals</span><span class=\"p\">:</span> <span class=\"c1\">#分别计算他们的信息增益</span>\n            <span class=\"n\">subDataSet</span> <span class=\"o\">=</span> <span class=\"n\">splitDataSet</span><span class=\"p\">(</span><span class=\"n\">dataSet</span><span class=\"p\">,</span> <span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">value</span><span class=\"p\">)</span>\n            <span class=\"n\">prob</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">subDataSet</span><span class=\"p\">)</span><span class=\"o\">/</span><span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">dataSet</span><span class=\"p\">))</span>\n            <span class=\"n\">newEntropy</span> <span class=\"o\">+=</span> <span class=\"n\">prob</span> <span class=\"o\">*</span> <span class=\"n\">calcShannonEnt</span><span class=\"p\">(</span><span class=\"n\">subDataSet</span><span class=\"p\">)</span>     \n        <span class=\"n\">infoGain</span> <span class=\"o\">=</span> <span class=\"n\">baseEntropy</span> <span class=\"o\">-</span> <span class=\"n\">newEntropy</span>    \n        <span class=\"k\">if</span> <span class=\"p\">(</span><span class=\"n\">infoGain</span> <span class=\"o\">&gt;</span> <span class=\"n\">bestInfoGain</span><span class=\"p\">):</span>       <span class=\"c1\">#选出信息增益最大的属性</span>\n            <span class=\"n\">bestInfoGain</span> <span class=\"o\">=</span> <span class=\"n\">infoGain</span>         \n            <span class=\"n\">bestFeature</span> <span class=\"o\">=</span> <span class=\"n\">i</span>\n    <span class=\"k\">return</span> <span class=\"n\">bestFeature</span>        \n\n\n<span class=\"k\">def</span> <span class=\"nf\">createTree</span><span class=\"p\">(</span><span class=\"n\">dataSet</span><span class=\"p\">,</span><span class=\"n\">labels</span><span class=\"p\">):</span> <span class=\"c1\">#生成决策树</span>\n    <span class=\"n\">classList</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">example</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"k\">for</span> <span class=\"n\">example</span> <span class=\"ow\">in</span> <span class=\"n\">dataSet</span><span class=\"p\">]</span>\n    <span class=\"k\">if</span> <span class=\"n\">classList</span><span class=\"o\">.</span><span class=\"n\">count</span><span class=\"p\">(</span><span class=\"n\">classList</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">])</span> <span class=\"o\">==</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">classList</span><span class=\"p\">):</span> <span class=\"c1\">#如果（训练集中的样本都属于同一类别），则return 这一类别的标签；</span>\n        <span class=\"k\">return</span> <span class=\"n\">classList</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n    <span class=\"k\">if</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">dataSet</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">])</span> <span class=\"o\">==</span> <span class=\"mi\">1</span><span class=\"p\">:</span> <span class=\"c1\">#如果（训练集中样本在属性集上的取值相同），则return 最多类别的标签； </span>\n        <span class=\"k\">return</span> <span class=\"n\">majorityCnt</span><span class=\"p\">(</span><span class=\"n\">classList</span><span class=\"p\">)</span>\n    <span class=\"n\">bestFeat</span> <span class=\"o\">=</span> <span class=\"n\">chooseBestFeatureToSplit</span><span class=\"p\">(</span><span class=\"n\">dataSet</span><span class=\"p\">)</span> <span class=\"c1\">#上面两个条件都不满足，则寻找最优化分属性a</span>\n    <span class=\"n\">bestFeatLabel</span> <span class=\"o\">=</span> <span class=\"n\">labels</span><span class=\"p\">[</span><span class=\"n\">bestFeat</span><span class=\"p\">]</span>\n    <span class=\"n\">myTree</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"n\">bestFeatLabel</span><span class=\"p\">:{}}</span>\n    <span class=\"k\">del</span><span class=\"p\">(</span><span class=\"n\">labels</span><span class=\"p\">[</span><span class=\"n\">bestFeat</span><span class=\"p\">])</span>\n    <span class=\"n\">featValues</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">example</span><span class=\"p\">[</span><span class=\"n\">bestFeat</span><span class=\"p\">]</span> <span class=\"k\">for</span> <span class=\"n\">example</span> <span class=\"ow\">in</span> <span class=\"n\">dataSet</span><span class=\"p\">]</span>\n    <span class=\"n\">uniqueVals</span> <span class=\"o\">=</span> <span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"n\">featValues</span><span class=\"p\">)</span>\n    <span class=\"k\">for</span> <span class=\"n\">value</span> <span class=\"ow\">in</span> <span class=\"n\">uniqueVals</span><span class=\"p\">:</span><span class=\"c1\">#循环，对于a中每一个值ai，递归地调用 生成决策树（ai的训练子集、属性集/a）</span>\n        <span class=\"n\">subLabels</span> <span class=\"o\">=</span> <span class=\"n\">labels</span><span class=\"p\">[:]</span>       \n        <span class=\"n\">myTree</span><span class=\"p\">[</span><span class=\"n\">bestFeatLabel</span><span class=\"p\">][</span><span class=\"n\">value</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">createTree</span><span class=\"p\">(</span><span class=\"n\">splitDataSet</span><span class=\"p\">(</span><span class=\"n\">dataSet</span><span class=\"p\">,</span> <span class=\"n\">bestFeat</span><span class=\"p\">,</span> <span class=\"n\">value</span><span class=\"p\">),</span><span class=\"n\">subLabels</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">myTree</span></code></pre></div><h2><b>3.C4.5</b></h2><p>C4.5算法也是昆兰提出的。在他machine learning上发表的ID3算法掀起决策树研究的热潮之后，短短几年时间，众多决策树算法问世。ID4，ID5等名字迅速被其他研究者提出的算法占用。昆兰只好将自己的ID3后继算法命名为C4.0（C代表Classifier）。在此基础上，昆兰又提出了C4.5。（昆兰自称C4.5仅是对C4.0做了些小改进，因此不是C5.0，而真正的C5.0是后续的商业化版本）<br/>C4.5算法使用“<b>增益率</b>”而不是之前的信息增益来选择最优划分。要指出的是，C4.5的改进不只有这一处，实际上，关于C4.5算法，昆兰写了整整一本书：<a href=\"https://link.zhihu.com/?target=https%3A//books.google.com/books%3Fhl%3Dzh-CN%26lr%3D%26id%3Db3ujBQAAQBAJ%26oi%3Dfnd%26pg%3DPP1%26ots%3DsQ3nSOIsG3%26sig%3D1d3U1VLdh9V0TzTq266L-xZpOfU%23v%3Donepage%26q%26f%3Dfalse\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">C4.5: Programs for Machine Learning</a>。</p><p>回想之前的数据集：<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-c75703dcd285b12177d2dda7b6f3586f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"959\" data-rawheight=\"502\" class=\"origin_image zh-lightbox-thumb\" width=\"959\" data-original=\"https://pic4.zhimg.com/v2-c75703dcd285b12177d2dda7b6f3586f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;959&#39; height=&#39;502&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"959\" data-rawheight=\"502\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"959\" data-original=\"https://pic4.zhimg.com/v2-c75703dcd285b12177d2dda7b6f3586f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-c75703dcd285b12177d2dda7b6f3586f_b.jpg\"/></figure><p>如果我们把第一列编号也当作一种属性来看的话，回想之前对信息熵最大值的介绍，用编号这一属性来划分数据集，得到的信息增益是最大的。但是显然，这一个属性没有<b>泛化能力</b>。事实上，ID3算法对于<b>可能取值数目较多的属性</b>有偏好。为了减少这种偏好，我们不妨尝试对于可能取值数目较多的属性进行<b>惩罚</b>。最简单的惩罚方法就是将信息增益除以一个函数，这个函数关于属性的可能取值数目<b>单调递增</b>。昆兰选择了这样一个单调递增函数，他把这函数称作属性a的固有值（IV——intrinsic value）：<br/> <img src=\"https://www.zhihu.com/equation?tex=IV%28a%29+%3D+-%5Csum%28%7CD_i%7C%2F%7CD%7C%29log_2%28%7CD_i%7C%2F%7CD%7C%29\" alt=\"IV(a) = -\\sum(|D_i|/|D|)log_2(|D_i|/|D|)\" eeimg=\"1\"/> <br/>惩罚后的信息增益称作信息增益率（gain ratio）:<br/> <img src=\"https://www.zhihu.com/equation?tex=Gain%5C_ratio%28D%2Ca%29+%3D+Gain%28D%2Ca%29%2FIV%28a%29\" alt=\"Gain\\_ratio(D,a) = Gain(D,a)/IV(a)\" eeimg=\"1\"/> </p><p>但是信息增益率有矫枉过正的问题，即他对于可能取值较少的属性有偏好。因此C4.5的解决方法是这样的：先选出<b>信息增益</b>高于平均水平的属性，再从这些属性里面选出<b>信息增益率</b>最高的那个作为最优划分属性。</p><p>值得一提的是，C4.5每次只做二元切分。</p><h2><b>4.CART决策树</b></h2><p>Breiman等人在1984年提出的CART（Classification and Regression Tree）算法引入了<b>基尼指数（Gini index）</b>作为确定划分的依据。同样，要指出的是，CART算法的内容绝不仅仅为基尼指数的引入。回忆ID3在确定分划后将该属性的所有值都分开来，这就可能会造成过于“贪心”而切分操之过急的问题。CART算法则与C4.5同样每一次只做二元切分，即分为左子树及又子树。同时，从他的名字中就可以看出，CART决策树还可以用于回归任务。</p><p>基尼指数是要反映从样本集合D中随机地抽取两个样本，他们标签不一样的概率。概率越小就意味着纯度越高。我们希望划分后每个样本子集的纯度越高越好，那么我们就选择划分后那概率最小的属性作为最优划分属性。</p><p><img src=\"https://www.zhihu.com/equation?tex=Gini%28D%29+%3D+%5Csum_k%5Csum_%7Bk%27%7D+p%28k%29p%28k%27%29\" alt=\"Gini(D) = \\sum_k\\sum_{k&#39;} p(k)p(k&#39;)\" eeimg=\"1\"/>      其中k不等于k‘<br/> <img src=\"https://www.zhihu.com/equation?tex=Gini%5C_index%28D%2Ca%29+%3D+%5Csum%28%7CD_i%7C%2F%7CD%7C%29Gini%28D%29\" alt=\"Gini\\_index(D,a) = \\sum(|D_i|/|D|)Gini(D)\" eeimg=\"1\"/> </p><p>这里我们可以将基尼指数于之前的信息熵与信息增益的公式做对比，很容易发现他们之间的相似性。</p><p><img src=\"https://www.zhihu.com/equation?tex=Ent%28D%29+%3D+%5Csum+p%28x_i%29I%28x_i%29%3D+-+%5Csum+p%28x_i%29log2+p%28x_i%29+\" alt=\"Ent(D) = \\sum p(x_i)I(x_i)= - \\sum p(x_i)log2 p(x_i) \" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=+Gain%28D%2Ca%29+%3D+Ent%28D%29+-+%5Csum+%28%7CD_i%7C%2F%7CD%7C%29Ent%28D_i%29\" alt=\" Gain(D,a) = Ent(D) - \\sum (|D_i|/|D|)Ent(D_i)\" eeimg=\"1\"/> </p><p>注意，这两组公式在使用时，前者选取基尼指数最小的，后者选取信息增益最大的。<br/>至此，我们介绍完了三种选取最优划分属性的方法。</p><h2>三、剪枝</h2><p>对于之前的决策树构建方法，存在这样一个问题：对噪音十分敏感，即容易发生<b>过拟合</b>。举例来说，如果有一个基因突变的好瓜，他不符合一般好瓜的各种性质，即他按理来说应该是坏瓜。那对于这个瓜，决策树也会为他单独伸出一根树干。显然，这根树干没有泛化能力。这时，我们就要对我们的决策树进行<b>剪枝</b>了。</p><h2>1.预剪枝</h2><p>预剪枝的思想就是：</p><blockquote>在构建决策树过程中，如果划分后决策树性能没有提升（或提升不大）则不进行划分。<br/></blockquote><p>回想之前构建决策树的代码，要实现预剪枝，只需要在选出最优划分之后，试探性地划分一下，拿划分之后的决策树在测试集（验证集）上跑一下，算出错误率。如果错误率没有下降或下降的幅度比较小（小于某个事先给定的超参数），则放弃此次划分，将这些样本标记为最多的那一种标签。</p><p>但是预剪枝也存在着矫枉过正的问题，即解决了过拟合，却带来了<b>欠拟合</b>的风险。预剪枝基于他“贪心”的本质禁止了很多分支的展开，有可能导致泛化性能的下降。</p><h2>2.后剪枝</h2><p>为了叙述的方便，我们把所有决策树末端能确定标签的节点称作<b>叶节点</b>，其余的节点称作<b>分支节点</b>。<br/>同时，我们还要把训练数据的一部分移出来作为<b>验证集</b>。</p><p>后剪枝有两种，一种是将两个末端叶节点合并，一种是令末端叶节点的上一级分支节点不展开。<br/>后剪枝的过程是这样的：</p><blockquote>尝试进行上述两种后剪枝之中的一种，如果剪枝后的决策树在验证集上的精度有提升，则确认剪枝，否则不剪枝。<br/></blockquote><p>我们发现，后剪枝的第二种和预剪枝比较相似，本质都是禁止展开。但不要混淆：两个方法的方向相反：预剪枝是构建决策树时从<b>树根</b>开始进行的，而后剪枝时构建完成决策树之后从<b>树叶</b>开始进行的，这也使得后剪枝能保留更多有用的分支，从而回避了欠拟合的风险。</p><h2>四、缺失值处理</h2><p>作为大四老阿姨的你，随着年纪不断增加，关于买瓜的记忆逐渐开始遗忘，很多时候记不清曾经买过的瓜所有的属性值。造成的后果就是，首先，当我们进行“选择最优划分属性”的时候，有缺失值的样本不好处理，还有一种情况，如果确定了某个划分属性，而有样本在这个属性上的值缺失。换成人话来讲，就是1）有的西瓜的性状忘记了，买瓜时鉴别属性的顺序不好确定。2）当你知道要按敲声往下分类时，有个瓜的敲声你忘记了，那这个样本怎么处理。直接放弃这些样本是对数据的浪费，我们不妨尝试把这些带有<b>缺失值</b>的数据也能利用起来。</p><p>下面我们介绍C4.5中的处理方法，为了便于理解，我们举例来说。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-59c9a654ea4abde3b98e0f3cd05ccdda_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"943\" data-rawheight=\"502\" class=\"origin_image zh-lightbox-thumb\" width=\"943\" data-original=\"https://pic3.zhimg.com/v2-59c9a654ea4abde3b98e0f3cd05ccdda_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;943&#39; height=&#39;502&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"943\" data-rawheight=\"502\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"943\" data-original=\"https://pic3.zhimg.com/v2-59c9a654ea4abde3b98e0f3cd05ccdda_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-59c9a654ea4abde3b98e0f3cd05ccdda_b.jpg\"/></figure><p>我们发现，你忘记了第一个瓜的敲声了，但还记得这个瓜的其他属性值。<br/>对于第一个<b>划分属性时</b>的问题，我们这么来解决：<br/>整个数据集这17个瓜记为训练集D，我们要讨论的属性“敲声”记为a，令D&#39;表示在属性a上没有缺失值的样本子集，在这里，就是没有忘记敲声的那些瓜，即2-17个瓜记为D&#39;。我们要做的是用这16个瓜来判断属性a是不是最优划分属性。</p><p>属性a有V个可取值{a1,a2......aV}——敲声共有V=3种可能：{浊响，沉闷，清脆}<br/>D&#39;v表示属性a取值为av（注意小写）的样本子集——D&#39;v=1：所有敲声浊响的瓜，D&#39;v=2：所有沉闷的瓜，D&#39;v=3：所有清脆的瓜。<br/>D&#39;k表示D&#39;种标签为第k个的样本组成的样本子集——D&#39;k=1：所有好瓜，D&#39;k=2：所有坏瓜。 <br/>再给每个样本一个赋予一个权重 <img src=\"https://www.zhihu.com/equation?tex=w_x\" alt=\"w_x\" eeimg=\"1\"/> <i>。<br/>定义:<br/></i> <img src=\"https://www.zhihu.com/equation?tex=%5Crho%3D%28%5Csum_%7BD%27%7D+w_x%29%2F%28%5Csum_D+w_x%29+\" alt=\"\\rho=(\\sum_{D&#39;} w_x)/(\\sum_D w_x) \" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=p%27k%3D%28%5Csum_%7BD%27k%7D+w_x%29%2F%28%5Csum+_%7BD%27%7D+w_x%29+\" alt=\"p&#39;k=(\\sum_{D&#39;k} w_x)/(\\sum _{D&#39;} w_x) \" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=r%27v%3D%28%5Csum_%7BD%27v%7D+w_x%29%2F%28%5Csum_%7BD%27%7D+w_x%29\" alt=\"r&#39;v=(\\sum_{D&#39;v} w_x)/(\\sum_{D&#39;} w_x)\" eeimg=\"1\"/> </p><p>直观地理解，对于属性a<br/> <img src=\"https://www.zhihu.com/equation?tex=%5Crho\" alt=\"\\rho\" eeimg=\"1\"/> 表示没有缺失值的样本占的比例——敲声没忘的后16个瓜占比多少（16/17）<br/> <img src=\"https://www.zhihu.com/equation?tex=p%E2%80%99_k\" alt=\"p’_k\" eeimg=\"1\"/> 表示无缺失值样本中第k类占的比例——记住敲声的后16个瓜里面，好瓜坏瓜各占比多少<i><br/></i> <img src=\"https://www.zhihu.com/equation?tex=r%27_v\" alt=\"r&#39;_v\" eeimg=\"1\"/> 表示无缺失值样本中在属性a上值为av的样本所占比例——记住敲声的后16个瓜里面，浊响的，沉闷的，清脆的各占比多少</p><p>基于上述的一大堆繁琐的定义，我们有了信息增益的推广式：<br/> <img src=\"https://www.zhihu.com/equation?tex=Gain%28D%2Ca%29%3D%5Crho+Gain%28D%27%2Ca%29%3D%5Crho%28Ent%28D%27%29-%5Csum_r%27v+Ent%28D%27v%29%29\" alt=\"Gain(D,a)=\\rho Gain(D&#39;,a)=\\rho(Ent(D&#39;)-\\sum_r&#39;v Ent(D&#39;v))\" eeimg=\"1\"/> <br/>其中 <img src=\"https://www.zhihu.com/equation?tex=Ent%28D%27%29%3D-%5Csum+p%27_k+log_2%28p%27_k%29\" alt=\"Ent(D&#39;)=-\\sum p&#39;_k log_2(p&#39;_k)\" eeimg=\"1\"/> </p><p>对比原来的定义式</p><p><img src=\"https://www.zhihu.com/equation?tex=Gain%28D%2Ca%29+%3D+Ent%28D%29+-+%5Csum+%28%7CD_i%7C%2F%7CD%7C%29Ent%28D_i%29\" alt=\"Gain(D,a) = Ent(D) - \\sum (|D_i|/|D|)Ent(D_i)\" eeimg=\"1\"/> <br/>其中 <img src=\"https://www.zhihu.com/equation?tex=Ent%28D%29+%3D+%5Csum+p%28x_i%29I%28x_i%29%3D+-+%5Csum+p%28x_i%29log_2+p%28x_i%29+\" alt=\"Ent(D) = \\sum p(x_i)I(x_i)= - \\sum p(x_i)log_2 p(x_i) \" eeimg=\"1\"/> </p><p>这就解决了刚才提出的第1）个问题，接下来讨论第2）个：如果确定了某个划分属性，而有样本在这个属性上的值缺失，如何处理。<br/>解决方法就是将这个忘记敲声的西瓜分到所有分支中去，但把他在每个属性值里面的的权值 <img src=\"https://www.zhihu.com/equation?tex=w_x\" alt=\"w_x\" eeimg=\"1\"/> <i>更新为</i> <img src=\"https://www.zhihu.com/equation?tex=w_x+%C2%B7+r_v\" alt=\"w_x · r_v\" eeimg=\"1\"/> ，而其他正常没忘记敲声的瓜，权值则保持不变。</p><h2>五、总结与讨论</h2><p>恭喜读者完成了本篇笔记的阅读！写到这里，这篇文章已经超过了8000字，但要囊括决策树的各个方面还远远不够。与笔记（一）中的KNN相似，决策树也是一个历史悠久但十分著名且性能尚佳的经典机器学习算法。我们将继续学习这些有些“古老”的算法，而对这些经典算法的研究将会是很有好处的。</p><p>本文始终以挑西瓜作为例子，首先讲述了决策树的构造，之后讨论了三种寻找最优划分属性的方法。然后针对过拟合问题引入了两种剪枝的方法，最后讲述了缺失值的处理方法。</p>", 
            "topic": [
                {
                    "tag": "人工智能", 
                    "tagLink": "https://api.zhihu.com/topics/19551275"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "笔记", 
                    "tagLink": "https://api.zhihu.com/topics/19554982"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/34624320", 
            "userName": "陈德龙", 
            "userLink": "https://www.zhihu.com/people/fb342217a29a19462616c21ace65a810", 
            "upvote": 14, 
            "title": "机器学习笔记（一） KNN K-最近邻", 
            "content": "<h2><b>零、摘要</b></h2><p>本篇文章主要讲述KNN算法（K-nearest neighbor）的原理与技术细节，并简单提及了数据预处理的方法。</p><p>主要参考资料:</p><p class=\"ztext-empty-paragraph\"><br/></p><ul><li><a href=\"https://link.zhihu.com/?target=http%3A//cs231n.github.io/classification/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">斯坦福CS231n课程笔记：分类</a></li><li>《机器学习》周志华</li><li>《机器学习实战》Peter Harrington</li><li><a href=\"https://zhuanlan.zhihu.com/p/34594997/https%3Ci%3E://en.wik%3C/i%3Eipedia.org/wiki/K-nearest_neighbors_algorithm\" class=\"internal\">维基百科：K-nearestneighborsalgorithm</a></li></ul><h2><b>一、从1NN到KNN</b></h2><blockquote><b><i>近朱者赤，近墨者黑</i></b></blockquote><p class=\"ztext-empty-paragraph\"><br/></p><p>考虑这样一个问题：给定平面上十个点，其中五个红色的点，五个黑色的点。现在要求预测虚线描出的点属于哪一个类别。当然对于我们人类来说，显然能断定这一点更有可能是红色的，但是我们应该怎样设计一个算法让机器也这样认为呢？</p><p>我们可以这样做：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-19879c3d9367d412c829167526ba4c9e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1158\" data-rawheight=\"857\" class=\"origin_image zh-lightbox-thumb\" width=\"1158\" data-original=\"https://pic3.zhimg.com/v2-19879c3d9367d412c829167526ba4c9e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1158&#39; height=&#39;857&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1158\" data-rawheight=\"857\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1158\" data-original=\"https://pic3.zhimg.com/v2-19879c3d9367d412c829167526ba4c9e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-19879c3d9367d412c829167526ba4c9e_b.jpg\"/></figure><ul><li>计算虚线圈出的点与给定的所有点之间的<b>距离</b></li><li>找出与他距离最小的那个点</li><li>距离最小的点是什么颜色，则预测所求点是什么颜色<br/><br/></li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-40038eff2317cb859cfa37b0324e3b69_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1718\" data-rawheight=\"1184\" class=\"origin_image zh-lightbox-thumb\" width=\"1718\" data-original=\"https://pic2.zhimg.com/v2-40038eff2317cb859cfa37b0324e3b69_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1718&#39; height=&#39;1184&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1718\" data-rawheight=\"1184\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1718\" data-original=\"https://pic2.zhimg.com/v2-40038eff2317cb859cfa37b0324e3b69_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-40038eff2317cb859cfa37b0324e3b69_b.jpg\"/></figure><p>A点距离最短，是红色的，断定所求点应该是红色的。这就使<b>1NN</b>即<b>最近邻算法。</b><br/>为了更好地理解最近邻算法，我们做这样一件事情：在这个平面的所有位置都摆放一次要预测的点，然后用1NN算法对他们进行预测。预测的点布满整个平面后，我们就能得到两个区域：<br/><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-d810fde7b2d0c0f78bf0f58b13965138_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1892\" data-rawheight=\"1410\" class=\"origin_image zh-lightbox-thumb\" width=\"1892\" data-original=\"https://pic1.zhimg.com/v2-d810fde7b2d0c0f78bf0f58b13965138_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1892&#39; height=&#39;1410&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1892\" data-rawheight=\"1410\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1892\" data-original=\"https://pic1.zhimg.com/v2-d810fde7b2d0c0f78bf0f58b13965138_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-d810fde7b2d0c0f78bf0f58b13965138_b.jpg\"/></figure><p>如果要预测的点出现在红色边界线左边的黑色区域里面，那么他就会被预测为黑色，对于右侧区域同理。从这张图来看，我们的1NN分类器表现得还不错。</p><p>但是如果我们换一个点集，再做一次同样的事情呢？我们得到下面图里的结果：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-b46e0cb93156dda97361d273ab78b893_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1502\" data-rawheight=\"1123\" class=\"origin_image zh-lightbox-thumb\" width=\"1502\" data-original=\"https://pic4.zhimg.com/v2-b46e0cb93156dda97361d273ab78b893_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1502&#39; height=&#39;1123&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1502\" data-rawheight=\"1123\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1502\" data-original=\"https://pic4.zhimg.com/v2-b46e0cb93156dda97361d273ab78b893_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-b46e0cb93156dda97361d273ab78b893_b.jpg\"/></figure><p> 显然，点B有可能是统计数据时因为马虎出现的错误，我们通常称他为<b>噪声</b>。点B附近的那一小块区域看起来更应该属于黑色才对。那么我们怎么让机器也这样认为呢？</p><p>考虑这样一个方法：<br/> - 计算要预测的点与给定的所有点之间的<b>距离</b><br/> - 找出与他距离最小的K个点<br/> - 这K个点中那种颜色的点出现次数最多，则预测所求点是什么颜色<br/>（通常，我们把第三步称作<b>投票</b>）</p><p>好的，我们想一下这样的话会得出什么样的结果：<br/>    当K=1时， 我们的这个方法与之前的方法没有任何不同。当K=3时，对于上图B点附近红色区域中的点（注意我们是要讨论这一区域里面的点而不是B点），我们找出离他最近的三个点，会得出这样的结果：两黑一红。选出出现次数最多的颜色——黑色，预测这一点的颜色是黑色的。搞定。 </p><p>下图时真正程序跑出来的分类结果。左边起第一张是原始数据，第二张是我们的1NN，第三张是K=5时的情形。（白色区域表示有至少两个颜色得票相同）<br/><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-cfe83546164e8d2895fe8dee5309421c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1052\" data-rawheight=\"264\" class=\"origin_image zh-lightbox-thumb\" width=\"1052\" data-original=\"https://pic1.zhimg.com/v2-cfe83546164e8d2895fe8dee5309421c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1052&#39; height=&#39;264&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1052\" data-rawheight=\"264\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1052\" data-original=\"https://pic1.zhimg.com/v2-cfe83546164e8d2895fe8dee5309421c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-cfe83546164e8d2895fe8dee5309421c_b.jpg\"/></figure><p>我们看到，相对于1NN，KNN算法对于噪声更加不敏感，我们称这一特性为<b>鲁棒性</b>。<br/>至于<b>超参数</b>K值的确定，我们通常是通过<b>交叉验证</b>来获得。即在验证集上尝试不同的K值，使用错误率最低的那个。</p><h2><b>二、加权KNN</b></h2><p>考虑下图中的预测问题：<br/><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-c9e364997d86a2e365a1e3352d8592dc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1274\" data-rawheight=\"962\" class=\"origin_image zh-lightbox-thumb\" width=\"1274\" data-original=\"https://pic1.zhimg.com/v2-c9e364997d86a2e365a1e3352d8592dc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1274&#39; height=&#39;962&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1274\" data-rawheight=\"962\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1274\" data-original=\"https://pic1.zhimg.com/v2-c9e364997d86a2e365a1e3352d8592dc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-c9e364997d86a2e365a1e3352d8592dc_b.jpg\"/></figure><p>显然这点应该属于红色，但是比如说当K=6时，预测结果就容易出错。这是因为KNN考虑了很多距离更远的黑点。要解决这个问题，我们可以让距离越近的点对于决策越重要。于是我们就有了加权KNN。</p><p>我们原来的KNN是在比较所求点与所有给定点之间的距离d，如果说加权KNN要比较的是关于d的函数f(d)，那么只要这个函数关于d单调递减，就能满足要求。</p><p>最简单地，我们有f(d)=1/d。但是，想到当d趋向于零时，该函数值 f(d)=d 趋向于正无穷。这相当于为距离很近的点分配了无限大的权重，假如恰巧有一个噪声的点在离要预测的点非常近的位置，那么这一噪音对于预测结果的干扰就非常大了。</p><p>考虑高斯函数 <img src=\"https://www.zhihu.com/equation?tex=f%28d%29+%3D+exp%28-d%5E2%2F2%29\" alt=\"f(d) = exp(-d^2/2)\" eeimg=\"1\"/> </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-95badc26b71be6910d0a395a207f3de2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1755\" data-rawheight=\"900\" class=\"origin_image zh-lightbox-thumb\" width=\"1755\" data-original=\"https://pic3.zhimg.com/v2-95badc26b71be6910d0a395a207f3de2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1755&#39; height=&#39;900&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1755\" data-rawheight=\"900\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1755\" data-original=\"https://pic3.zhimg.com/v2-95badc26b71be6910d0a395a207f3de2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-95badc26b71be6910d0a395a207f3de2_b.jpg\"/></figure><p>这个函数就很好地弥补了反比例函数的缺点，当d趋向于零时，函数值趋向于1。当d&gt;0时，f(d)单调递减（显然距离d取正值）。同时高斯函数还有一个优点就是，当距离d很大时，函数值f(d)始终会保持正值，而不会跌入负值。</p><blockquote><b><i>加权KNN做的事情就是，比较 最近的K个点的距离d 的单调递减函数f(d)的函数值</i></b></blockquote><h2><b>三、距离的选择</b></h2><p>我们之前一直在说，选择<b>距离</b>最近的K个点，那么这个距离是怎么计算的呢？</p><p>我们知道，对于平面上的两点的距离，是x, y轴坐标各自的平方差之和再开根号。一般地，对于n维空间中的点，有<br/> <img src=\"https://www.zhihu.com/equation?tex=d+%28I1%2C+I2%29+%3D+%5Csqrt%7B%5Csum%7Bp%7D+%5Cleft%28+I%5Ep1+-+I%5Ep2+%5Cright%29%5E2%7D\" alt=\"d (I1, I2) = \\sqrt{\\sum{p} \\left( I^p1 - I^p2 \\right)^2}\" eeimg=\"1\"/> <i>，<br/>其中</i> <img src=\"https://www.zhihu.com/equation?tex=I_1%2CI_2\" alt=\"I_1,I_2\" eeimg=\"1\"/> 是两个点的坐标，p表示维度。我们通常称他维<b>L2距离</b></p><p>当然，还有<b>L1距离</b> <img src=\"https://www.zhihu.com/equation?tex=d%28I1%2C+I2%29+%3D+%5Csum%7Bp%7D+%5Cleft%7C+I%5Ep1+-+I%5Ep_2+%5Cright%7C\" alt=\"d(I1, I2) = \\sum{p} \\left| I^p1 - I^p_2 \\right|\" eeimg=\"1\"/> </p><p>L1和L2距离都是常用的距离度量方法。他们之间的区别可以这样来思考：在某一维度上，如果两点相距较远，则L2距离比L1距离要更大，而若两点相距很近，则反之。这是由于平方的存在，可以参考 <img src=\"https://www.zhihu.com/equation?tex=y%3Dx\" alt=\"y=x\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=y%3Dx%5E2\" alt=\"y=x^2\" eeimg=\"1\"/> 在第一象限内的图像。</p><h2><b>四、归一化</b></h2><p>在处理实际问题时，事情可能变得与处理平面时有些区别。<br/>比如说我们要用KNN处理这样一个问题：<br/>我们手中有这样一个数据集，河海大学100个学生的<br/> - 每学期天在图书馆的总小时数<br/> - 每学期使用电脑的总小时数<br/> - 每学期交作业总次数<br/> - 所属院系<br/>前三个是数值，每个学生有三个对应数值，相当于每一个学生是三维空间中的一个点，三个对应数值是这个点的坐标，而所属院系则是类似于红或黑的属性。<br/>我们要做的事情是根据数据集里没有的一名学生的前三个数据，来判断他的院系。</p><p>如果直接使用KNN的话，我们会发现，三个维度上的数据相差过大，即前两个维度对于“距离”的影响要比第三个维度大得多。为了解决这个问题，我们要对数据进行预处理：归一化。</p><p>如果我们能将三个维度上的数值都压缩到同样一个区间上（比如说0到1或者-1到1），这样三个维度的数据对与最后决策的影响差距就会基本相等。</p><p>我们有： 归一化后数值 = （原数值 - 平均值）/ （最大值 - 平均值）<br/>上式能将原数据压缩至0到1的区间。</p><h2><b>五、数据精简</b></h2><p><b>剔除噪声</b><br/>我们的KNN算法，并没有显示的训练过程（但我们仍把给定点集称作“训练集”），这是一种比较“懒惰”的机器学习方法。这也导致了在实际预测过程中，需要对训练集的每一个点都进行距离计算，这是十分耗时的。如果我们能减少一部分<b>冗余</b>的样本，自然就能减少使用时计算的负担。</p><p>首先，之前提到过的<b>噪声</b>点是最应该被去掉的。我们来思考如何能判断训练集中的样本（点）是否属于噪声。通常，噪声样本都有“孤军深入”这样的特点。那我们就可以这样设计算法：</p><blockquote>只要某一个样本的周围都是与它属性不同的点，那我们就判断这个样本属于噪声。</blockquote><p>比较严谨地：</p><blockquote>给定超参数K，取小于K的正整数R，若某样本的“R近邻”都是属性不同的样本，则删除该样本<br/>  这样，我们就可以删除一部分噪声了，同时也精简了数据集。</blockquote><p><b>选择原型</b><br/>我们说，数据精简的目的是：用留下的最少的部分数据，完成与完整训练集差不多的分类性能。这一部分里，我们把那留下的最少的那部分数据称作“<b>原型</b>”，接下来，我们来讨论应该如何选择这一训练集的子集。</p><p>试想这样一个情景，你坐在河海大学文体中心小剧场听新生入学教育，剧场观众席的学生是按班级分布的。你觉得台上的演讲十分无聊，便想要确定观众席中每个班级的具体位置以及边界。<br/>你可以这样做：首先对于你附近的人，询问他们的班级。如果他属于一个没见过的的班级，就那小本本把他的位置记下来。一小会之后，你的小本本上就会有几个不同班的学生了。</p><p>然后对于在场的每一位学生：<br/>        询问他的班级，再询问离他最近的那个人的班级。<br/>        如果两个不一样，则拿出小本本把这位学生记下来；如果一样，那么我们说，你这位学生对于我确定边界没什么用，不记下你了。<br/>在遍历一遍在场的所有同学之后，你就可以用你手中的小本本来做KNN分类啦。我们把这个小本本上的同学就称作原型。</p><p>比较严谨地，我们有：</p><div class=\"highlight\"><pre><code class=\"language-text\">有原型（样本集合）U\n遍历所有样本，对于当前样本x：{\n    如果他和他的最近邻属性不同：\n        就将x加入U；\n    如果他和他的最近邻属性相同：\n        继续循环，即舍弃该样本；\n}</code></pre></div><p>这样，当我们完成遍历，就得到了原型U，这时我们就可以用精简后的U做<b>1NN</b>分类了。这一方法被称为CNN(Condensed nearest neighbor)，有趣的是，他与卷积神经网络——convolutional neural networks, CNN重名。</p><p><b>边界率</b><br/>还有一个技术细节，就是我们要指定遍历样本的顺序，来获得更好的原型U。<br/>定义边界率 a(x)=||x&#39;-y||/||x-y||，我们要对于训练集中的每一个样本x，挨个计算边界率a。<br/>其中，x表示当前样本；y表示离x最近的，不同属性的样本；x&#39;表示离y最近的，与x同属性的样本。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-f71fc72d5e95fdba1755ea55401ea59d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"159\" data-rawheight=\"127\" class=\"content_image\" width=\"159\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;159&#39; height=&#39;127&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"159\" data-rawheight=\"127\" class=\"content_image lazy\" width=\"159\" data-actualsrc=\"https://pic2.zhimg.com/v2-f71fc72d5e95fdba1755ea55401ea59d_b.jpg\"/></figure><p>如图所示，当前样本是红色的，y是与当前样本不同的绿色，x&#39;是与当前样本相同的红色。<br/>要指出，由于||x&#39;-y|| &lt;= ||x-y||， 边界率a的值在0和1之间。</p><p>计算完所有边界率a之后，我们对训练集中的样本，按边界率a从大到小的顺序进行排序，再按这个顺序来进行遍历。<br/>这样的顺序有助于我们获得更经济且性能更好的原型U。</p><p>对于之前的例子：<br/><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-cfe83546164e8d2895fe8dee5309421c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1052\" data-rawheight=\"264\" class=\"origin_image zh-lightbox-thumb\" width=\"1052\" data-original=\"https://pic1.zhimg.com/v2-cfe83546164e8d2895fe8dee5309421c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1052&#39; height=&#39;264&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1052\" data-rawheight=\"264\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1052\" data-original=\"https://pic1.zhimg.com/v2-cfe83546164e8d2895fe8dee5309421c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-cfe83546164e8d2895fe8dee5309421c_b.jpg\"/></figure><p>进行了数据精简后，我们得到<br/><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-3b9a4a2b33f8a52afb01989582b381fe_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"272\" data-rawheight=\"180\" class=\"content_image\" width=\"272\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;272&#39; height=&#39;180&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"272\" data-rawheight=\"180\" class=\"content_image lazy\" width=\"272\" data-actualsrc=\"https://pic3.zhimg.com/v2-3b9a4a2b33f8a52afb01989582b381fe_b.jpg\"/></figure><p>其中，标有x的点是去掉的噪音，标方框的点是原型U中的样本，标圆圈的则是丢弃的冗余样本。<br/>用这一原型U做<b>1NN</b>分类，我们得到如下结果：<br/><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ce34d7c5aeaf5901b6b35816bcadfed0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"275\" data-rawheight=\"180\" class=\"content_image\" width=\"275\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;275&#39; height=&#39;180&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"275\" data-rawheight=\"180\" class=\"content_image lazy\" width=\"275\" data-actualsrc=\"https://pic1.zhimg.com/v2-ce34d7c5aeaf5901b6b35816bcadfed0_b.jpg\"/></figure><p>与上面使用完整训练集得1NN相比，原型U的分类性能并不差。同时，原型U中的样本数只占完整训练集的15%-20%</p><h2><b>六、简单实现（python）</b></h2><div class=\"highlight\"><pre><code class=\"language-text\">def KNN(input, dataSet, labels, k):\n\n# 计算距离\n    dataSetSize = dataSet.shape[0]\n    # 计算每个维度上的坐标差 difference matrix\n    diffMat = tile(input, (dataSetSize,1)) - dataSet\n    # 这里用L2距离, 计算平方后的各维度坐标差 squared difference matrix\n    sqDiffMat = diffMat**2\n    #求和, squared distance \n    sqDistances = sqDiffMat.sum(axis=1)\n    #开根号, distance\n    distances = sqDistances**0.5\n # 排序\n    sortedDistIndicies = distances.argsort()     \n    classCount={} \n    # 投票         \n    for i in range(k):\n        voteIlabel = labels[sortedDistIndicies[i]]\n        classCount[voteIlabel] = classCount.get(voteIlabel,0) + 1\n    sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True)\n\n    return sortedClassCount[0][0]</code></pre></div><p>这段代码使用L2距离<br/> <img src=\"https://www.zhihu.com/equation?tex=d+%28I1%2C+I2%29+%3D+%5Csqrt%7B%5Csum%7Bp%7D+%5Cleft%28+I%5Ep1+-+I%5Ep_2+%5Cright%29%5E2%7D\" alt=\"d (I1, I2) = \\sqrt{\\sum{p} \\left( I^p1 - I^p_2 \\right)^2}\" eeimg=\"1\"/> <br/>输入为要判断的点input、给定点dataSet， 给定属性labels， 以及<b>超参数</b>K<br/>输出为经过投票后得票最高的属性。</p><h2>七、总结与讨论</h2><p>在本文中，我们讨论了从最近邻到K-最近邻的升级、介绍了为了减少密集敌对样本影响的加权KNN、浅谈了距离度量方法的选择，之后是数据预处理，包括归一化和精简数据，最后，我们给出了实现最简单KNN的一段代码。</p><p>KNN是机器学习中相对简单的一个入门算法，他原理相对简单，但使用时需要存储大量样本（即使有数据精简），而且计算成本很高。他没有显式的训练过程，是一种“懒惰学习”。这个算法最初由Cover和Hart于1968年提出，历史久远，但是仍能应用在当今的很多实际问题上，并且性能不差：他的泛化错误率不超过贝叶斯最优分类器错误率的两倍。</p>", 
            "topic": [
                {
                    "tag": "人工智能", 
                    "tagLink": "https://api.zhihu.com/topics/19551275"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "笔记", 
                    "tagLink": "https://api.zhihu.com/topics/19554982"
                }
            ], 
            "comments": [
                {
                    "userName": "粉嫩少女心的某小刘", 
                    "userLink": "https://www.zhihu.com/people/a38ff912169fd138a20907365cff10eb", 
                    "content": "<p>很通俗易懂的一篇文章，谢谢~</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }
    ], 
    "url": "https://zhuanlan.zhihu.com/c_171086503"
}
