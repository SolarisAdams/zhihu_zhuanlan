{
    "title": "深度学习与自然语言处理", 
    "description": "", 
    "followers": [
        "https://www.zhihu.com/people/tju-lls", 
        "https://www.zhihu.com/people/pao-jiao-xi-hong-shi", 
        "https://www.zhihu.com/people/zhang-duo-long-61", 
        "https://www.zhihu.com/people/ck_welder", 
        "https://www.zhihu.com/people/programmer_song", 
        "https://www.zhihu.com/people/chen-bo-86-42", 
        "https://www.zhihu.com/people/bailingnan", 
        "https://www.zhihu.com/people/zhang-ni-mei-70-37", 
        "https://www.zhihu.com/people/gao-ying-jie-42-78", 
        "https://www.zhihu.com/people/wang-yu-xuan-46-64", 
        "https://www.zhihu.com/people/zhangrong888", 
        "https://www.zhihu.com/people/qia-fu-qia-de-shen-yin", 
        "https://www.zhihu.com/people/tan-wei-43", 
        "https://www.zhihu.com/people/buptguo", 
        "https://www.zhihu.com/people/wang-tian-yuan-77", 
        "https://www.zhihu.com/people/jin-se-liang-dian-ban", 
        "https://www.zhihu.com/people/sun-xiao-fei-75-94", 
        "https://www.zhihu.com/people/chevson", 
        "https://www.zhihu.com/people/ji-wen-fei-17", 
        "https://www.zhihu.com/people/wobian-liao-womei-bian", 
        "https://www.zhihu.com/people/jing-ju-jing-duan-ss9g0046", 
        "https://www.zhihu.com/people/liu-guo-jing-62", 
        "https://www.zhihu.com/people/wang-xiao-fei-72-22", 
        "https://www.zhihu.com/people/adblock123", 
        "https://www.zhihu.com/people/wu-long-69", 
        "https://www.zhihu.com/people/venus024", 
        "https://www.zhihu.com/people/le-tian-41-71", 
        "https://www.zhihu.com/people/wotan-z", 
        "https://www.zhihu.com/people/tinker-34-37", 
        "https://www.zhihu.com/people/pang-jia-lai-li-man", 
        "https://www.zhihu.com/people/lifengshen", 
        "https://www.zhihu.com/people/warren-ding", 
        "https://www.zhihu.com/people/di-hong-jia-4", 
        "https://www.zhihu.com/people/tang-yin-chun-25", 
        "https://www.zhihu.com/people/bigfish-66", 
        "https://www.zhihu.com/people/wang-zi-feng-48-58", 
        "https://www.zhihu.com/people/peng-bin-61-37", 
        "https://www.zhihu.com/people/yi-cheng-ta-shu", 
        "https://www.zhihu.com/people/hai-mian-57-57", 
        "https://www.zhihu.com/people/gu-he-you-xi", 
        "https://www.zhihu.com/people/thisisbill", 
        "https://www.zhihu.com/people/kang-xiao-su-11", 
        "https://www.zhihu.com/people/llp-1", 
        "https://www.zhihu.com/people/ha-ha-ha-1-98-22", 
        "https://www.zhihu.com/people/paulPig", 
        "https://www.zhihu.com/people/ni-hui-51-23", 
        "https://www.zhihu.com/people/freeman-96-88", 
        "https://www.zhihu.com/people/lin-can-xiong-42", 
        "https://www.zhihu.com/people/sunpeng1996", 
        "https://www.zhihu.com/people/xiao-fei-23-70-59", 
        "https://www.zhihu.com/people/wang-sen-18-60", 
        "https://www.zhihu.com/people/cangqizhou", 
        "https://www.zhihu.com/people/jinshenhehuan", 
        "https://www.zhihu.com/people/he-hai-yang", 
        "https://www.zhihu.com/people/muzi618", 
        "https://www.zhihu.com/people/loving-eating-soil", 
        "https://www.zhihu.com/people/shen-xiao-xiao-36-13", 
        "https://www.zhihu.com/people/li-hui-70-6", 
        "https://www.zhihu.com/people/rfk-13", 
        "https://www.zhihu.com/people/su-tong-32-78", 
        "https://www.zhihu.com/people/liu-bo-33-1-2", 
        "https://www.zhihu.com/people/alimuhamad-50-59", 
        "https://www.zhihu.com/people/valar_chen", 
        "https://www.zhihu.com/people/eternity-73-15", 
        "https://www.zhihu.com/people/jackstark-87", 
        "https://www.zhihu.com/people/wo-men-du-shi-cheng-nian-ren-48", 
        "https://www.zhihu.com/people/dou-dou-jun-64-36", 
        "https://www.zhihu.com/people/lei-en-jia-er-23", 
        "https://www.zhihu.com/people/eric-3-92", 
        "https://www.zhihu.com/people/wang-jing-bo-27-88", 
        "https://www.zhihu.com/people/jie-yin-65", 
        "https://www.zhihu.com/people/wind-70-84", 
        "https://www.zhihu.com/people/shakenlz", 
        "https://www.zhihu.com/people/susan-30-33", 
        "https://www.zhihu.com/people/he-mu-74-56", 
        "https://www.zhihu.com/people/mizhawushi", 
        "https://www.zhihu.com/people/li-chen-22-1", 
        "https://www.zhihu.com/people/ren-wo-xing-59-44", 
        "https://www.zhihu.com/people/li-shuang-32-32", 
        "https://www.zhihu.com/people/pi-zi-zhu-51", 
        "https://www.zhihu.com/people/yi-bu-yi-bu-zou-72", 
        "https://www.zhihu.com/people/yuan-andy-5", 
        "https://www.zhihu.com/people/zheng-zi-ou", 
        "https://www.zhihu.com/people/hao-qi-xin-82", 
        "https://www.zhihu.com/people/vocan", 
        "https://www.zhihu.com/people/kang-si-tan-ding-21-39", 
        "https://www.zhihu.com/people/NotAndOr", 
        "https://www.zhihu.com/people/ou-mi-jia-62-86", 
        "https://www.zhihu.com/people/albert-33", 
        "https://www.zhihu.com/people/eated_potato", 
        "https://www.zhihu.com/people/zhong-er-cheng-xu-yuan", 
        "https://www.zhihu.com/people/ka-ka-ke-6", 
        "https://www.zhihu.com/people/chen-lei-69-28", 
        "https://www.zhihu.com/people/huang-cheng-yang-78", 
        "https://www.zhihu.com/people/lurending0", 
        "https://www.zhihu.com/people/bu-ming-yuan", 
        "https://www.zhihu.com/people/wu-ye-nan-30", 
        "https://www.zhihu.com/people/du-pei-wei", 
        "https://www.zhihu.com/people/zhi-shi-ke-ji", 
        "https://www.zhihu.com/people/chen-1-54-49", 
        "https://www.zhihu.com/people/wang-dong-1-21", 
        "https://www.zhihu.com/people/yzb-83-67", 
        "https://www.zhihu.com/people/starship-85", 
        "https://www.zhihu.com/people/jiang-hai-yun-92", 
        "https://www.zhihu.com/people/zhaocy-68", 
        "https://www.zhihu.com/people/leo-1611", 
        "https://www.zhihu.com/people/mabinma", 
        "https://www.zhihu.com/people/chang-feng-ting-99", 
        "https://www.zhihu.com/people/yang-jing-lin-16", 
        "https://www.zhihu.com/people/xing-min", 
        "https://www.zhihu.com/people/yunyanglee", 
        "https://www.zhihu.com/people/zheng-xiong-feng", 
        "https://www.zhihu.com/people/wustyx2zym", 
        "https://www.zhihu.com/people/xyyhcl", 
        "https://www.zhihu.com/people/xie-wei-89", 
        "https://www.zhihu.com/people/luo-bi-de-75", 
        "https://www.zhihu.com/people/yanghongkai", 
        "https://www.zhihu.com/people/shui-mu-dao-50", 
        "https://www.zhihu.com/people/xingyuan-bu", 
        "https://www.zhihu.com/people/tenny1109", 
        "https://www.zhihu.com/people/xiao-lei-67-5", 
        "https://www.zhihu.com/people/zhao-zhi-bai-hu", 
        "https://www.zhihu.com/people/jluautolab", 
        "https://www.zhihu.com/people/oliver-kahn-60", 
        "https://www.zhihu.com/people/na-lan-56-89", 
        "https://www.zhihu.com/people/jiang-peng-51-20", 
        "https://www.zhihu.com/people/zhu-forrest", 
        "https://www.zhihu.com/people/xiao-shi-yilang-19-67", 
        "https://www.zhihu.com/people/liang-xu-59-15", 
        "https://www.zhihu.com/people/old_wine", 
        "https://www.zhihu.com/people/dang-liu-xing-hua-guo-ni-de-bi-an", 
        "https://www.zhihu.com/people/cheng-long-pku", 
        "https://www.zhihu.com/people/chomk-kroaity", 
        "https://www.zhihu.com/people/long-gzwen-ran", 
        "https://www.zhihu.com/people/du-daniel-47", 
        "https://www.zhihu.com/people/zhang-jia-xin-1203", 
        "https://www.zhihu.com/people/tifosi", 
        "https://www.zhihu.com/people/closer-76-45", 
        "https://www.zhihu.com/people/xia-peng-67", 
        "https://www.zhihu.com/people/xndxyf", 
        "https://www.zhihu.com/people/zhang-jun-lin-76", 
        "https://www.zhihu.com/people/clement-dong", 
        "https://www.zhihu.com/people/yige-wu-liao-de-ren-85", 
        "https://www.zhihu.com/people/ku-ji-xing-kong", 
        "https://www.zhihu.com/people/lan-lan-lan-lan-96-82", 
        "https://www.zhihu.com/people/wangmurong", 
        "https://www.zhihu.com/people/zhang-zi-yi-31-40", 
        "https://www.zhihu.com/people/kksh", 
        "https://www.zhihu.com/people/yu-qing-59-6", 
        "https://www.zhihu.com/people/tong-jing-yu", 
        "https://www.zhihu.com/people/steven-21-7-25", 
        "https://www.zhihu.com/people/gao-zhao-xing", 
        "https://www.zhihu.com/people/kfliubo", 
        "https://www.zhihu.com/people/hua-qing-68-67", 
        "https://www.zhihu.com/people/ComeOnJian", 
        "https://www.zhihu.com/people/hijackjave", 
        "https://www.zhihu.com/people/spin-74", 
        "https://www.zhihu.com/people/leengsmile", 
        "https://www.zhihu.com/people/antler-14-78", 
        "https://www.zhihu.com/people/jackhuang", 
        "https://www.zhihu.com/people/zhang-ji-peng-42", 
        "https://www.zhihu.com/people/snakealpha", 
        "https://www.zhihu.com/people/li-xi-sheng-24", 
        "https://www.zhihu.com/people/mao-tou-xiao-zi-55-9", 
        "https://www.zhihu.com/people/kiwee", 
        "https://www.zhihu.com/people/ren-zhen-sheng-huo-62", 
        "https://www.zhihu.com/people/monidanai", 
        "https://www.zhihu.com/people/spirit-22-32", 
        "https://www.zhihu.com/people/bi-ruo-sheng-feng-85", 
        "https://www.zhihu.com/people/yeu-yang", 
        "https://www.zhihu.com/people/iloveks", 
        "https://www.zhihu.com/people/moni-gg", 
        "https://www.zhihu.com/people/fan-11-46", 
        "https://www.zhihu.com/people/li-hua-xia-69-95", 
        "https://www.zhihu.com/people/la-geek", 
        "https://www.zhihu.com/people/piao-miao-87-73", 
        "https://www.zhihu.com/people/leo-lee-58-57", 
        "https://www.zhihu.com/people/suo-long-16-97", 
        "https://www.zhihu.com/people/chen-cheng-en-32", 
        "https://www.zhihu.com/people/pathriclee", 
        "https://www.zhihu.com/people/hs-chen-40", 
        "https://www.zhihu.com/people/tang-xiao-liang-72", 
        "https://www.zhihu.com/people/shou-tan-you-le-54", 
        "https://www.zhihu.com/people/heyang-36", 
        "https://www.zhihu.com/people/hu-la-la-83-14", 
        "https://www.zhihu.com/people/hui-xuan-zhuan-de-ba-dong", 
        "https://www.zhihu.com/people/xiao-bin-38-29", 
        "https://www.zhihu.com/people/ku-gou-82", 
        "https://www.zhihu.com/people/jiang-wei-42-52", 
        "https://www.zhihu.com/people/zhu-xiao-min-46-21", 
        "https://www.zhihu.com/people/shenlant", 
        "https://www.zhihu.com/people/xiao-zhan-37-17", 
        "https://www.zhihu.com/people/rainbow_jjh", 
        "https://www.zhihu.com/people/quan-quan-76-58", 
        "https://www.zhihu.com/people/chen-tian-jiao-30", 
        "https://www.zhihu.com/people/ma-jian-guo-74-27", 
        "https://www.zhihu.com/people/ji-feng-88", 
        "https://www.zhihu.com/people/yizhi-mao-mao-jiang", 
        "https://www.zhihu.com/people/lwmiracle", 
        "https://www.zhihu.com/people/louB-yong-zhihu", 
        "https://www.zhihu.com/people/young-mao-72", 
        "https://www.zhihu.com/people/yin-chun-kai", 
        "https://www.zhihu.com/people/max-bay-14-15", 
        "https://www.zhihu.com/people/hoctoboy", 
        "https://www.zhihu.com/people/plzdeletethisaccount", 
        "https://www.zhihu.com/people/liu-fan-xi-32", 
        "https://www.zhihu.com/people/a-diao-92-27", 
        "https://www.zhihu.com/people/Harune", 
        "https://www.zhihu.com/people/yildhd-wang", 
        "https://www.zhihu.com/people/gao-peng-yu-13", 
        "https://www.zhihu.com/people/li-wen-qi-14-93", 
        "https://www.zhihu.com/people/ping.love", 
        "https://www.zhihu.com/people/xiao-wang-zi-50-97", 
        "https://www.zhihu.com/people/kai-xu-37-40", 
        "https://www.zhihu.com/people/zhou-hong-sen-96-67", 
        "https://www.zhihu.com/people/shengjian-9", 
        "https://www.zhihu.com/people/iwdps", 
        "https://www.zhihu.com/people/yang-xiao-meng-37", 
        "https://www.zhihu.com/people/cang-cang-cang-yue", 
        "https://www.zhihu.com/people/wu-xiao-er-32-70", 
        "https://www.zhihu.com/people/yu-rui-jie-93", 
        "https://www.zhihu.com/people/fu-sheng-lei-90", 
        "https://www.zhihu.com/people/mo-da-qiang", 
        "https://www.zhihu.com/people/Teriho", 
        "https://www.zhihu.com/people/qi-ye-59-20", 
        "https://www.zhihu.com/people/chen-yi-30-6", 
        "https://www.zhihu.com/people/opendayoff", 
        "https://www.zhihu.com/people/ling-hu-e-da", 
        "https://www.zhihu.com/people/xiaoq-41", 
        "https://www.zhihu.com/people/bai-zhi-wei", 
        "https://www.zhihu.com/people/liu-yi-25-9", 
        "https://www.zhihu.com/people/leo-liu-68-37", 
        "https://www.zhihu.com/people/min-da-39", 
        "https://www.zhihu.com/people/ju-shang-38", 
        "https://www.zhihu.com/people/wu-fang-4-85", 
        "https://www.zhihu.com/people/duan-qiao-bian-80", 
        "https://www.zhihu.com/people/qinkang-69", 
        "https://www.zhihu.com/people/xie-pan-33-24", 
        "https://www.zhihu.com/people/cai-cai-cai-22-68", 
        "https://www.zhihu.com/people/kaierlong", 
        "https://www.zhihu.com/people/dong-feng-zao-ji", 
        "https://www.zhihu.com/people/feng-wei-3-32", 
        "https://www.zhihu.com/people/zhang-chen-hao-30", 
        "https://www.zhihu.com/people/haidawyl", 
        "https://www.zhihu.com/people/hogwartsrico", 
        "https://www.zhihu.com/people/Micro-Kun", 
        "https://www.zhihu.com/people/shenyinian", 
        "https://www.zhihu.com/people/hong-alex", 
        "https://www.zhihu.com/people/chen-yong-17-33", 
        "https://www.zhihu.com/people/qichao-tang", 
        "https://www.zhihu.com/people/xiao-ming-57-51", 
        "https://www.zhihu.com/people/wang-yi-IG", 
        "https://www.zhihu.com/people/ge-wu-3", 
        "https://www.zhihu.com/people/ni-yong-hu-97-91", 
        "https://www.zhihu.com/people/liang-hang-ming", 
        "https://www.zhihu.com/people/li-hui-71-94", 
        "https://www.zhihu.com/people/wang-zhi-jun-5-1", 
        "https://www.zhihu.com/people/zheng-yi-10-82", 
        "https://www.zhihu.com/people/transmittolarry", 
        "https://www.zhihu.com/people/wu-yi-yang-92", 
        "https://www.zhihu.com/people/wei-yuan-88-25", 
        "https://www.zhihu.com/people/an-di-dufresne-24", 
        "https://www.zhihu.com/people/xie-guo-chen-11", 
        "https://www.zhihu.com/people/mopperwhite", 
        "https://www.zhihu.com/people/qinlibo_nlp", 
        "https://www.zhihu.com/people/xiang-pin-29", 
        "https://www.zhihu.com/people/feng-xia-chong-90", 
        "https://www.zhihu.com/people/xie-er-ji-er", 
        "https://www.zhihu.com/people/lian-yi-de-69", 
        "https://www.zhihu.com/people/sydnever", 
        "https://www.zhihu.com/people/liu-yue-4-31", 
        "https://www.zhihu.com/people/li-xiao-wu-28-43-76"
    ], 
    "article": [
        {
            "url": "https://zhuanlan.zhihu.com/p/37483073", 
            "userName": "Octocat", 
            "userLink": "https://www.zhihu.com/people/db5a40777722f345e64b9c173860d605", 
            "upvote": 4, 
            "title": "ACL2017《Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme 》Summary", 
            "content": "<p>论文地址：<a href=\"https://link.zhihu.com/?target=http%3A//aclweb.org/anthology/P17-1113\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">《Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme 》</a>。</p><h2>1 解决的问题 &amp; 亮点</h2><p>本论文主要解决的问题是<b>实体和关系的联合抽取</b>。</p><p>许多现有的方法解决实体和关系抽取的思路是：(i) 对文本中的实体进行提取 (ii) 根据预定义的关系集和提取出的实体对文本进行关系提取。</p><p>本论文的不同点在于：</p><ul><li>(i) 设计了一种<b>特别的标签类型</b>，将实体和关系提取转化为<b>单个序列标注问题</b>；</li><li>(ii) 在(i)的基础上，提出一个<b>端到端</b>模型，对实体和关系进行<b>联合抽取</b>；</li><li>(iii) 区别于BiLSTM-CRF模型，本论文提出的模型设计是BiLSTM+LSTM-<b>bias</b>；</li></ul><h2>2 标签类型</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-72f33038e877456373a4cc74d2b94f06_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1354\" data-rawheight=\"226\" class=\"origin_image zh-lightbox-thumb\" width=\"1354\" data-original=\"https://pic3.zhimg.com/v2-72f33038e877456373a4cc74d2b94f06_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1354&#39; height=&#39;226&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1354\" data-rawheight=\"226\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1354\" data-original=\"https://pic3.zhimg.com/v2-72f33038e877456373a4cc74d2b94f06_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-72f33038e877456373a4cc74d2b94f06_b.jpg\"/></figure><p>本文提出的标签类型主要由三部分组成：<b>实体标签-关系标签-三元组标签</b>。</p><ul><li><b>实体标签</b>用经典的<code>“BIES” (Begin, Inside, End, Single)</code>指代实体；</li><li><b>关系标签</b>用有限个关系种类指代关系；</li><li><b>三元组标签</b>把相同关系标签的实体联系起来，<code>1</code>代表第一个实体，<code>2</code>代表第二个实体；</li></ul><h2>3 端到端模型</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-bdb0d0ea8b333b791025480d7c2f74ce_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1346\" data-rawheight=\"478\" class=\"origin_image zh-lightbox-thumb\" width=\"1346\" data-original=\"https://pic3.zhimg.com/v2-bdb0d0ea8b333b791025480d7c2f74ce_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1346&#39; height=&#39;478&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1346\" data-rawheight=\"478\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1346\" data-original=\"https://pic3.zhimg.com/v2-bdb0d0ea8b333b791025480d7c2f74ce_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-bdb0d0ea8b333b791025480d7c2f74ce_b.jpg\"/></figure><p>使用的Bi-LSTM和LSTM模型的公式论文中已经给出，这里不多余赘述。</p><h2>4 偏差权重</h2><p>模型的偏差函数为：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-df7dbee21ae67784fca0266038de53e3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"682\" data-rawheight=\"194\" class=\"origin_image zh-lightbox-thumb\" width=\"682\" data-original=\"https://pic4.zhimg.com/v2-df7dbee21ae67784fca0266038de53e3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;682&#39; height=&#39;194&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"682\" data-rawheight=\"194\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"682\" data-original=\"https://pic4.zhimg.com/v2-df7dbee21ae67784fca0266038de53e3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-df7dbee21ae67784fca0266038de53e3_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-f8a9cbd8f68377bc00e8cb857adec3d9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"438\" data-rawheight=\"104\" class=\"origin_image zh-lightbox-thumb\" width=\"438\" data-original=\"https://pic2.zhimg.com/v2-f8a9cbd8f68377bc00e8cb857adec3d9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;438&#39; height=&#39;104&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"438\" data-rawheight=\"104\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"438\" data-original=\"https://pic2.zhimg.com/v2-f8a9cbd8f68377bc00e8cb857adec3d9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-f8a9cbd8f68377bc00e8cb857adec3d9_b.jpg\"/></figure><p>标签 <img src=\"https://www.zhihu.com/equation?tex=O\" alt=\"O\" eeimg=\"1\"/> 指的是训练数据外的关系标签，引入 <img src=\"https://www.zhihu.com/equation?tex=%5Calpha\" alt=\"\\alpha\" eeimg=\"1\"/> 是为了增强已有关系的权重。 </p><h2>5 实验结果</h2><p>在用距离监督方法标注的开源数据集NYT上进行测试：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-f142e1c92d36155d8716311556d056c2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1434\" data-rawheight=\"642\" class=\"origin_image zh-lightbox-thumb\" width=\"1434\" data-original=\"https://pic3.zhimg.com/v2-f142e1c92d36155d8716311556d056c2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1434&#39; height=&#39;642&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1434\" data-rawheight=\"642\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1434\" data-original=\"https://pic3.zhimg.com/v2-f142e1c92d36155d8716311556d056c2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-f142e1c92d36155d8716311556d056c2_b.jpg\"/></figure><p>此外，也进行了其他测试：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-7e73ae78c81a54052b6aff5d72103222_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1292\" data-rawheight=\"302\" class=\"origin_image zh-lightbox-thumb\" width=\"1292\" data-original=\"https://pic3.zhimg.com/v2-7e73ae78c81a54052b6aff5d72103222_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1292&#39; height=&#39;302&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1292\" data-rawheight=\"302\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1292\" data-original=\"https://pic3.zhimg.com/v2-7e73ae78c81a54052b6aff5d72103222_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-7e73ae78c81a54052b6aff5d72103222_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b097f9e6616febdf1506d36a9bd18e7d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"674\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb\" width=\"674\" data-original=\"https://pic2.zhimg.com/v2-b097f9e6616febdf1506d36a9bd18e7d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;674&#39; height=&#39;600&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"674\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"674\" data-original=\"https://pic2.zhimg.com/v2-b097f9e6616febdf1506d36a9bd18e7d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-b097f9e6616febdf1506d36a9bd18e7d_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-3887a90140fdb7142a1859969b2ebb94_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"638\" data-rawheight=\"570\" class=\"origin_image zh-lightbox-thumb\" width=\"638\" data-original=\"https://pic1.zhimg.com/v2-3887a90140fdb7142a1859969b2ebb94_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;638&#39; height=&#39;570&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"638\" data-rawheight=\"570\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"638\" data-original=\"https://pic1.zhimg.com/v2-3887a90140fdb7142a1859969b2ebb94_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-3887a90140fdb7142a1859969b2ebb94_b.jpg\"/></figure><h2>6 已知缺陷</h2><p>论文中明确表明本模型未能解决<b>关系重叠</b>的情况，打算在未来把模型最后一层的<b>softmax函数</b>替换为<b>多标签分类器</b>。</p><p>同时，作者还打算在<b>相关实体的联系</b>上进行深入研究。</p>", 
            "topic": [
                {
                    "tag": "命名实体识别", 
                    "tagLink": "https://api.zhihu.com/topics/19648557"
                }, 
                {
                    "tag": "自然语言处理", 
                    "tagLink": "https://api.zhihu.com/topics/19560026"
                }, 
                {
                    "tag": "论文", 
                    "tagLink": "https://api.zhihu.com/topics/19572442"
                }
            ], 
            "comments": [
                {
                    "userName": "雪雪雪啊", 
                    "userLink": "https://www.zhihu.com/people/999d5a0b38526a110579b9d2880329b1", 
                    "content": "<p>请问 I(O)指的是预测出来的标签还是真实的标签</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/37389506", 
            "userName": "Octocat", 
            "userLink": "https://www.zhihu.com/people/db5a40777722f345e64b9c173860d605", 
            "upvote": 1, 
            "title": "ACL2017《A Local Detection Approach for Named Entity Recognition and Mention Detection》Summary", 
            "content": "<p>论文地址：<a href=\"https://link.zhihu.com/?target=http%3A//aclweb.org/anthology/P17-1114\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">《A Local Detection Approach for Named Entity Recognition and Mention Detection》</a>。</p><p>Gayhub地址：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/xmb-cipher/fofe-ner\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">xmb-cipher/fofe-ner</a></p><h2>摘要</h2><p>通常大家会把<b>命名实体识别</b>当做<b>序列标注问题</b>去解决，本文提出一种<b>局部探测</b>的方法，通过<b>固定长度</b>的<b>句子片段及其前后文</b>来对命名实体进行识别。在CoNLL 2003 NER、TAC-KBP2015和TAC-KBP2016任务上表现很好。</p><h2>1 简介</h2><ul><li><b>命名实体识别(Named Entity Recognition, NER)</b> ：识别出句子中的命名实体。</li><li><b>指代探测(Mention Detection, MD)</b>：识别出句子中的人名指代。</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ca181739a7083026c02ecb3d681925c3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"530\" data-rawheight=\"86\" class=\"origin_image zh-lightbox-thumb\" width=\"530\" data-original=\"https://pic4.zhimg.com/v2-ca181739a7083026c02ecb3d681925c3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;530&#39; height=&#39;86&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"530\" data-rawheight=\"86\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"530\" data-original=\"https://pic4.zhimg.com/v2-ca181739a7083026c02ecb3d681925c3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-ca181739a7083026c02ecb3d681925c3_b.jpg\"/></figure><p>序列标注问题是把整个句子进行训练与测试，进而识别出句子中的实体。而<b>局部探测</b>方法的对象是句子中的每个片段，<b>达成精准的局部探测的要点是把句子片段以及其充足的上下文信息识别出来。</b></p><p>许多特征工程技术都难免会造成上下文信息的丢失，这里选择了一种<b>定长编码</b>技术：<b>fixed-size ordinally forgetting encoding(FOFE)</b>。</p><p>整个模型的基本流程是这样的：</p><ol><li>用FOFE把<b>变长文本</b>编码成<b>定长文本</b>。</li><li>用普通的<b>前馈神经网络(FFNN)</b>进行<b>NER</b>和<b>MD</b>。</li></ol><h2>3 基本知识</h2><p><b>前馈神经网络</b>就是最常见的<b>多层感知机</b>，基于<b>单词</b>和基于<b>字母</b>的模型根据字面意思也都容易理解，这里简要讲下<b>FOFE</b>。</p><p>NLP中文本都是<b>变长</b>的，RNNs/LSTMs模型擅长处理这类数据，而FFNN并不擅长。但是，FOFE可以把变长数据编码为定长数据，这就很好克服了FFNN自身的限制，毕竟FFNN这种简单模型的训练和预测效率非常高。</p><p>FOFE的编码思想类似于<b>词带模型(BoW)</b>+<b>遗忘因子</b>：</p><ol><li>首先把定义集合 <img src=\"https://www.zhihu.com/equation?tex=S+%3D+w_1%2C+w_2%2C+w_3%2C+%C2%B7%C2%B7%C2%B7+w_T\" alt=\"S = w_1, w_2, w_3, ··· w_T\" eeimg=\"1\"/> ，其中 <img src=\"https://www.zhihu.com/equation?tex=w\" alt=\"w\" eeimg=\"1\"/> 代表一个单词。起初的时候，每个单词用<b>one-hot向量编码</b>表示，例如对于 <img src=\"https://www.zhihu.com/equation?tex=S%3D+A%2C+B+%2CC\" alt=\"S= A, B ,C\" eeimg=\"1\"/> ，可以用 <img src=\"https://www.zhihu.com/equation?tex=%5B1%2C0%2C0%5D%2C+%5B0%2C1%2C0%5D%2C+%5B0%2C0%2C1%5D\" alt=\"[1,0,0], [0,1,0], [0,0,1]\" eeimg=\"1\"/> 表示。</li></ol><p>2. 设定一个遗忘因子 <img src=\"https://www.zhihu.com/equation?tex=%5Calpha%5Cin%280%2C1%29\" alt=\"\\alpha\\in(0,1)\" eeimg=\"1\"/> ，使用以下公式计算出每个单词的最终编码 <img src=\"https://www.zhihu.com/equation?tex=z_t\" alt=\"z_t\" eeimg=\"1\"/> ：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-3cbd13f1916e86dac7982b05bfa2f456_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"472\" data-rawheight=\"114\" class=\"origin_image zh-lightbox-thumb\" width=\"472\" data-original=\"https://pic3.zhimg.com/v2-3cbd13f1916e86dac7982b05bfa2f456_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;472&#39; height=&#39;114&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"472\" data-rawheight=\"114\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"472\" data-original=\"https://pic3.zhimg.com/v2-3cbd13f1916e86dac7982b05bfa2f456_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-3cbd13f1916e86dac7982b05bfa2f456_b.jpg\"/></figure><p>3. 例如对于序列 <img src=\"https://www.zhihu.com/equation?tex=ABC\" alt=\"ABC\" eeimg=\"1\"/> ，其结果为 <img src=\"https://www.zhihu.com/equation?tex=%5B%CE%B1%5E2%2C%CE%B1%2C1%5D\" alt=\"[α^2,α,1]\" eeimg=\"1\"/> ；对于序列 <img src=\"https://www.zhihu.com/equation?tex=ABCBC\" alt=\"ABCBC\" eeimg=\"1\"/> ，其结果为 <img src=\"https://www.zhihu.com/equation?tex=%5B%CE%B1%5E4%2C%CE%B1%2B%CE%B1%5E3%2C1%2B+%CE%B1%5E2+%5D\" alt=\"[α^4,α+α^3,1+ α^2 ]\" eeimg=\"1\"/> 。这就把任意<b>变长序列编码</b>为长度为 <img src=\"https://www.zhihu.com/equation?tex=T\" alt=\"T\" eeimg=\"1\"/> 的<b>定长编码</b>。</p><h2>4 基于FOFE的局部探测模型</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-2d8171a8086f96359a74caf4ab91fd96_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1286\" data-rawheight=\"434\" class=\"origin_image zh-lightbox-thumb\" width=\"1286\" data-original=\"https://pic3.zhimg.com/v2-2d8171a8086f96359a74caf4ab91fd96_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1286&#39; height=&#39;434&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1286\" data-rawheight=\"434\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1286\" data-original=\"https://pic3.zhimg.com/v2-2d8171a8086f96359a74caf4ab91fd96_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-2d8171a8086f96359a74caf4ab91fd96_b.jpg\"/></figure><p><b>FOFE-NER</b>最主要的特征如上图，每个序列被拆分为<b>三部分(left+segment+right)</b>，这三部分先经过<b>FOFE算法</b>进行一轮<b>编码</b>，然后进入<b>FFNN</b>中进行<b>NER</b>。</p><p>此外，还可以引入其他基于单词的特征和基于字母的特征，文章中讲的比较清楚。</p><h2>5 训练和解码算法</h2><p>在训练过程中，实体主要有以下情况：</p><ol><li>segment正好是实体</li><li>segment中部分word构成实体</li><li>segment完全不参与实体构成</li></ol><p>对于上述情况，在分类上采取这样的措施：</p><ol><li>如果是上述情况1，则对实体进行分类，如PER、ORG</li><li>如果是上述情况2、3，则将实体分类为NONE</li></ol><p>对于预测结果，如果存在冗余结果，可以根据以下策略进行剔除：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-0dbc2e90e57602fa842bc19f446b69e5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"646\" data-rawheight=\"384\" class=\"origin_image zh-lightbox-thumb\" width=\"646\" data-original=\"https://pic2.zhimg.com/v2-0dbc2e90e57602fa842bc19f446b69e5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;646&#39; height=&#39;384&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"646\" data-rawheight=\"384\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"646\" data-original=\"https://pic2.zhimg.com/v2-0dbc2e90e57602fa842bc19f446b69e5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-0dbc2e90e57602fa842bc19f446b69e5_b.jpg\"/></figure><h2>6 两层增强</h2><p>通过借鉴CRF的思想，我们构造两轮<b>FOFE-NER模型</b>，在第一轮中目标是把</p><blockquote>Google has also recruited Fei-Fei Li, director of the AI lab at Stanford University</blockquote><p>预测为</p><blockquote>&lt;ORG&gt; has also recruited Fei-Fei Li, director of the AI lab at &lt;ORG&gt;.</blockquote><p>用实体类型替代实体后，进一步寻找实体<code>Fei-Fei Li</code> </p><h2>7 实验结果</h2><p>我们用CoNLL2003 NER数据集测试结果来理解<b>特征选取</b>和<b>模型对比</b>：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-a4508622eced32dcd4f9680429048945_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1484\" data-rawheight=\"1232\" class=\"origin_image zh-lightbox-thumb\" width=\"1484\" data-original=\"https://pic2.zhimg.com/v2-a4508622eced32dcd4f9680429048945_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1484&#39; height=&#39;1232&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1484\" data-rawheight=\"1232\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1484\" data-original=\"https://pic2.zhimg.com/v2-a4508622eced32dcd4f9680429048945_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-a4508622eced32dcd4f9680429048945_b.jpg\"/></figure><h2>个人疑问</h2><ol><li><b>FOFE编码的合理性</b>？这个问题应该要深入阅读FOFE的论文和其相关的应用论文去寻找答案。</li><li>文中提出的用<b>序列标注</b>解决NBR存在的问题（训练数据<b>实体标注残缺</b>和<b>实体嵌套</b>时效果差）是否客观？多阅读一些NER的论文后可以进行深入比较。</li></ol><h2>总结</h2><p>NER问题目前比较常规的解决方法是围绕LSTM和CRF展开的，相比于直接把整个序列作为输入的序列标注模型，本论文提出的局部探测法更加强调了局部上下文信息在NER问题中的重要性。而且，本论文提出的模型结构简单，性能上应该更优（待进一步验证）。</p>", 
            "topic": [
                {
                    "tag": "自然语言处理", 
                    "tagLink": "https://api.zhihu.com/topics/19560026"
                }, 
                {
                    "tag": "命名实体识别", 
                    "tagLink": "https://api.zhihu.com/topics/19648557"
                }, 
                {
                    "tag": "论文", 
                    "tagLink": "https://api.zhihu.com/topics/19572442"
                }
            ], 
            "comments": [
                {
                    "userName": "霍华德", 
                    "userLink": "https://www.zhihu.com/people/4a0d3a504b9859139f2c003005230717", 
                    "content": "<p>没意思，花式灌水</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "Octocat", 
                            "userLink": "https://www.zhihu.com/people/db5a40777722f345e64b9c173860d605", 
                            "content": "<p>刚开始看论文，在顶会里对着Entity Recognition筛的，见谅 😑</p>", 
                            "likes": 0, 
                            "replyToAuthor": "霍华德"
                        }, 
                        {
                            "userName": "霍华德", 
                            "userLink": "https://www.zhihu.com/people/4a0d3a504b9859139f2c003005230717", 
                            "content": "主要是ner这个任务其实是做的差不多了 大家都90%多", 
                            "likes": 0, 
                            "replyToAuthor": "Octocat"
                        }
                    ]
                }, 
                {
                    "userName": "loveissofar", 
                    "userLink": "https://www.zhihu.com/people/59e69e7de9fd68411600e45f012c3d10", 
                    "content": "<p>看了一下2015年那篇提出的FOFE编码的论文，感觉他那个两个性质的证明有点不太理解不能啊？求教</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/37247221", 
            "userName": "Octocat", 
            "userLink": "https://www.zhihu.com/people/db5a40777722f345e64b9c173860d605", 
            "upvote": 3, 
            "title": "《深度学习》读书笔记2~5 第一部分 小结", 
            "content": "<h2>第一部分 小结</h2><p>第2~5章为本书的第一部分，主题是<b>应用数学与机器学习基础</b>。</p><ul><li>2 <b>线性代数</b>：<a href=\"https://zhuanlan.zhihu.com/p/36406690\" class=\"internal\">《深度学习》读书笔记2——线性代数</a></li><li>3 <b>概率与信息论</b>：<a href=\"https://zhuanlan.zhihu.com/p/36619765\" class=\"internal\">《深度学习》读书笔记3——概率与信息论</a></li><li>4 <b>数值计算</b>：<a href=\"https://zhuanlan.zhihu.com/p/36681774\" class=\"internal\">《深度学习》读书笔记4——数值计算</a></li><li>5 <b>机器学习基础</b>：<a href=\"http://zhuanlan.zhihu.com/p/36740601\" class=\"internal\">《深度学习》读书笔记5——机器学习基础</a></li></ul><p>在阅读第2~4章关于<b>数学</b>内容的时候，可以感受到其中少量内容超出本科时数学课所学内容。个人建议是遇到过难的内容不要过分纠缠，难免会打击积极性，但遇到简单的内容一定要拿回数学书认真复习。</p><p>阅读第5章关于<b>机器学习</b>内容的时候，可以感受到跟常见的机器学习书籍（如周志华老师的西瓜书《机器学习》）画风不一样，如果要学习机器学习模型部分建议换上述两本书籍学习。</p><p>总而言之，这本《深度学习》确实不是一本新手友好的书。数学基础偏差、毫无机器学习or深度学习经验的同学，一定读的非常痛苦。比如我 ：）</p><p></p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "读书笔记", 
                    "tagLink": "https://api.zhihu.com/topics/19590861"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/36740601", 
            "userName": "Octocat", 
            "userLink": "https://www.zhihu.com/people/db5a40777722f345e64b9c173860d605", 
            "upvote": 2, 
            "title": "《深度学习》读书笔记5——机器学习基础", 
            "content": "<h2>第五章 机器学习基础</h2><h2>5.1 学习算法</h2><p>机器学习是什么？Mitchell提供了一个简洁的定义：</p><p>对于某类任务 <img src=\"https://www.zhihu.com/equation?tex=T\" alt=\"T\" eeimg=\"1\"/> 和性能度量 <img src=\"https://www.zhihu.com/equation?tex=P\" alt=\"P\" eeimg=\"1\"/> ，一个计算机程序可以从经验 <img src=\"https://www.zhihu.com/equation?tex=E\" alt=\"E\" eeimg=\"1\"/> 中进行学习。通过经验 <img src=\"https://www.zhihu.com/equation?tex=E\" alt=\"E\" eeimg=\"1\"/> 改进后，任务 <img src=\"https://www.zhihu.com/equation?tex=T\" alt=\"T\" eeimg=\"1\"/> 由性能度量 <img src=\"https://www.zhihu.com/equation?tex=P\" alt=\"P\" eeimg=\"1\"/> 衡量的性能有所提升。</p><blockquote>任务 <img src=\"https://www.zhihu.com/equation?tex=T\" alt=\"T\" eeimg=\"1\"/> </blockquote><p>通常机器学习任务定义为机器学习系统如何处理<b>样本</b>。<b>样本</b>指的是我们希望机器学习系统处理的对象或事件中收集到的已经量化的<b>特征</b>。每个样本可以表示为一个向量： <img src=\"https://www.zhihu.com/equation?tex=x+%5Cin+R%5En\" alt=\"x \\in R^n\" eeimg=\"1\"/> ，向量 <img src=\"https://www.zhihu.com/equation?tex=x\" alt=\"x\" eeimg=\"1\"/> 的每个元素 <img src=\"https://www.zhihu.com/equation?tex=x%5Ei\" alt=\"x^i\" eeimg=\"1\"/> 是一个特征。常见的机器学习任务有：</p><ul><li>分类</li><li>输入缺失分类</li><li>回归</li><li>转录</li><li>机器翻译</li><li>结构化输出</li><li>异常检测</li><li>合成和采样</li><li>缺失值填补</li><li>去噪</li><li>密度估计</li></ul><blockquote>性能度量 <img src=\"https://www.zhihu.com/equation?tex=P\" alt=\"P\" eeimg=\"1\"/> </blockquote><p>为了评估机器学习算法的能力，我们需要设计其性能的定量度量，这通常是针对特定的任务 <img src=\"https://www.zhihu.com/equation?tex=T\" alt=\"T\" eeimg=\"1\"/> 而言的。</p><p>例如，我们可以使用<b>准确率</b>、<b>错误率</b>来度量分类任务等。</p><blockquote>经验 <img src=\"https://www.zhihu.com/equation?tex=E\" alt=\"E\" eeimg=\"1\"/> </blockquote><p>根据学习过程中的不同经验，机器学习算法可以分为<b>监督学习</b>和<b>无监督学习</b>。</p><p>事实上，这两类学习都是从样本数据集中去获取经验，其不同点是：</p><ul><li><b>无监督学习</b>学习出整个数据集上有用的结构特征，也就是估计出特征 <img src=\"https://www.zhihu.com/equation?tex=x\" alt=\"x\" eeimg=\"1\"/> 的概率分布 <img src=\"https://www.zhihu.com/equation?tex=p%28x%29\" alt=\"p(x)\" eeimg=\"1\"/> 。</li><li><b>监督学习</b>中样本有标签 <img src=\"https://www.zhihu.com/equation?tex=y\" alt=\"y\" eeimg=\"1\"/> ，我们需要学习出剩余特征 <img src=\"https://www.zhihu.com/equation?tex=x\" alt=\"x\" eeimg=\"1\"/> 与标签 <img src=\"https://www.zhihu.com/equation?tex=y\" alt=\"y\" eeimg=\"1\"/> 的关系，也就是估计出 <img src=\"https://www.zhihu.com/equation?tex=p%28y%7Cx%29\" alt=\"p(y|x)\" eeimg=\"1\"/> 。</li></ul><p>事实上<b>无监督学习</b>和<b>监督学习</b>之间也没有明确的界线，我们可以使用概率公式对两者进行转换：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-f7f387d2db04460f7a7cf26a4cec1276_b.jpg\" data-size=\"normal\" data-rawwidth=\"372\" data-rawheight=\"98\" class=\"content_image\" width=\"372\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;372&#39; height=&#39;98&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"372\" data-rawheight=\"98\" class=\"content_image lazy\" width=\"372\" data-actualsrc=\"https://pic3.zhimg.com/v2-f7f387d2db04460f7a7cf26a4cec1276_b.jpg\"/><figcaption>p(y|x) -&gt; p(x)</figcaption></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-56cb8981f52590e05659ec6f3bd92b01_b.jpg\" data-size=\"normal\" data-rawwidth=\"304\" data-rawheight=\"82\" class=\"content_image\" width=\"304\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;304&#39; height=&#39;82&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"304\" data-rawheight=\"82\" class=\"content_image lazy\" width=\"304\" data-actualsrc=\"https://pic2.zhimg.com/v2-56cb8981f52590e05659ec6f3bd92b01_b.jpg\"/><figcaption>p(x) -&gt; p(y|x)</figcaption></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>也存在两者的变种：<b>半监督学习</b>。</p><p>此外，还有些机器学习算法不是训练在一个固定的数据集上，而是学习系统和训练过程会有反馈回路，这类学习算法叫<b>强化学习</b>。</p><blockquote>示例：线性回归</blockquote><p>对于线性回归问题，我们可以定义任务 <img src=\"https://www.zhihu.com/equation?tex=T\" alt=\"T\" eeimg=\"1\"/> ：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-814f30ea3122cdefbb8cbee41430c9aa_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"128\" data-rawheight=\"34\" class=\"content_image\" width=\"128\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;128&#39; height=&#39;34&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"128\" data-rawheight=\"34\" class=\"content_image lazy\" width=\"128\" data-actualsrc=\"https://pic3.zhimg.com/v2-814f30ea3122cdefbb8cbee41430c9aa_b.jpg\"/></figure><p>对于测试集，我们定义性能度量 <img src=\"https://www.zhihu.com/equation?tex=P\" alt=\"P\" eeimg=\"1\"/> ：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-119c96f8b94cc511befa9a4fa398ee67_b.jpg\" data-size=\"normal\" data-rawwidth=\"448\" data-rawheight=\"84\" class=\"origin_image zh-lightbox-thumb\" width=\"448\" data-original=\"https://pic4.zhimg.com/v2-119c96f8b94cc511befa9a4fa398ee67_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;448&#39; height=&#39;84&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"448\" data-rawheight=\"84\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"448\" data-original=\"https://pic4.zhimg.com/v2-119c96f8b94cc511befa9a4fa398ee67_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-119c96f8b94cc511befa9a4fa398ee67_b.jpg\"/><figcaption>MES：均方误差</figcaption></figure><p>上述式子等价于：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-6a744b0533a9d5b62af740f7a5fc38fa_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"412\" data-rawheight=\"76\" class=\"content_image\" width=\"412\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;412&#39; height=&#39;76&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"412\" data-rawheight=\"76\" class=\"content_image lazy\" width=\"412\" data-actualsrc=\"https://pic3.zhimg.com/v2-6a744b0533a9d5b62af740f7a5fc38fa_b.jpg\"/></figure><p>在训练过程中，我们实际上要最小化的是 <img src=\"https://www.zhihu.com/equation?tex=MSE_%7Btrain%7D\" alt=\"MSE_{train}\" eeimg=\"1\"/> ，简单求解其导数等于0的情况：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-a5500e516cd0bcc67fb81bedb6f4ee07_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"920\" data-rawheight=\"136\" class=\"origin_image zh-lightbox-thumb\" width=\"920\" data-original=\"https://pic4.zhimg.com/v2-a5500e516cd0bcc67fb81bedb6f4ee07_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;920&#39; height=&#39;136&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"920\" data-rawheight=\"136\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"920\" data-original=\"https://pic4.zhimg.com/v2-a5500e516cd0bcc67fb81bedb6f4ee07_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-a5500e516cd0bcc67fb81bedb6f4ee07_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-964915554707eed6755fdc4ff41992c8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"944\" data-rawheight=\"338\" class=\"origin_image zh-lightbox-thumb\" width=\"944\" data-original=\"https://pic1.zhimg.com/v2-964915554707eed6755fdc4ff41992c8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;944&#39; height=&#39;338&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"944\" data-rawheight=\"338\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"944\" data-original=\"https://pic1.zhimg.com/v2-964915554707eed6755fdc4ff41992c8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-964915554707eed6755fdc4ff41992c8_b.jpg\"/></figure><p>上述最终结果的方程叫做<b>正规方程</b>。</p><p>当然，线性回归中往往会有偏置项 <img src=\"https://www.zhihu.com/equation?tex=b\" alt=\"b\" eeimg=\"1\"/> ：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-9e04d53990c50570323bd8dfed2632a3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"188\" data-rawheight=\"48\" class=\"content_image\" width=\"188\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;188&#39; height=&#39;48&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"188\" data-rawheight=\"48\" class=\"content_image lazy\" width=\"188\" data-actualsrc=\"https://pic4.zhimg.com/v2-9e04d53990c50570323bd8dfed2632a3_b.jpg\"/></figure><p>最终的优化结果如图所示：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-8529f3d69dca5ee3437b5303cf45e66f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"878\" data-rawheight=\"350\" class=\"origin_image zh-lightbox-thumb\" width=\"878\" data-original=\"https://pic4.zhimg.com/v2-8529f3d69dca5ee3437b5303cf45e66f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;878&#39; height=&#39;350&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"878\" data-rawheight=\"350\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"878\" data-original=\"https://pic4.zhimg.com/v2-8529f3d69dca5ee3437b5303cf45e66f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-8529f3d69dca5ee3437b5303cf45e66f_b.jpg\"/></figure><h2>5.2 容量、过拟合和欠拟合</h2><p>我们训练机器学习模型的目的是让模型能够在未观测到的新输入上表现良好，这叫做<b>泛化</b>能力。</p><p>机器学习与优化问题不同的地方在于，机器学习不仅要考虑<b>训练误差</b>，还要考虑<b>泛化误差</b>。通常我们用<b>测试集</b>上的误差来衡量泛化误差。</p><p>也就是说，训练的时候我们需要最小化<b>训练误差</b>：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-9a72e2a81ed60721d4a71c5b31fb5c2d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"398\" data-rawheight=\"78\" class=\"content_image\" width=\"398\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;398&#39; height=&#39;78&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"398\" data-rawheight=\"78\" class=\"content_image lazy\" width=\"398\" data-actualsrc=\"https://pic2.zhimg.com/v2-9a72e2a81ed60721d4a71c5b31fb5c2d_b.jpg\"/></figure><p>但是我们真正需要关注的是<b>测试误差</b>：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-08c6009a5aca232f91bb11c2402d45f0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"322\" data-rawheight=\"52\" class=\"content_image\" width=\"322\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;322&#39; height=&#39;52&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"322\" data-rawheight=\"52\" class=\"content_image lazy\" width=\"322\" data-actualsrc=\"https://pic1.zhimg.com/v2-08c6009a5aca232f91bb11c2402d45f0_b.jpg\"/></figure><p>如果训练集和测试集完全没有关系，从训练集上训练出来的模型自然也不可能很好迁移到测试集上。<b>统计学习理论</b>给了我们一些指导：通常，我们会做一系列被统称为<b>独立同分布假设</b>的假设，这保证了每个数据集中的样本都是<b>相互独立的</b>，并且训练集和测试集是<b>同分布的</b>。我们利用这样的假设去生成训练集和测试集。</p><p>在实际应用的时候，我们可以保证对于某个固定的 <img src=\"https://www.zhihu.com/equation?tex=w\" alt=\"w\" eeimg=\"1\"/> ，独立同分布的训练集和测试集误差的期望是一样的。但是实际应用时，权值 <img src=\"https://www.zhihu.com/equation?tex=w\" alt=\"w\" eeimg=\"1\"/> 在学习过程中一直在改变，所以真实的测试误差期望会大于等于训练误差期望，因此在训练过程中我们需要做到以下两点：</p><ol><li>降低训练误差</li><li>缩小训练误差和测试误差的差距</li></ol><p>这两个因素分别对应机器学习中两个重要的挑战：</p><ol><li><b>欠拟合</b>：模型在训练集上不能得到足够低的训练误差</li><li><b>过拟合</b>：训练误差和测试误差差距太大</li></ol><p>我们可以通过调整模型的<b>容量</b>，来控制模型是否偏向于欠拟合或者过拟合。模型的<b>容量</b>指的是模型拟合函数的能力。</p><p>一种控制模型容量的方法是选择<b>假设空间</b>。例如对于典型的线性回归：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-e421700d54e40fbd1ecaf8ea05215db9_b.jpg\" data-size=\"normal\" data-rawwidth=\"186\" data-rawheight=\"48\" class=\"content_image\" width=\"186\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;186&#39; height=&#39;48&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"186\" data-rawheight=\"48\" class=\"content_image lazy\" width=\"186\" data-actualsrc=\"https://pic2.zhimg.com/v2-e421700d54e40fbd1ecaf8ea05215db9_b.jpg\"/><figcaption>一次多项式</figcaption></figure><p>我们可以引入特征 <img src=\"https://www.zhihu.com/equation?tex=x%5E2\" alt=\"x^2\" eeimg=\"1\"/> 来增大假设空间：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-bb09e1aeebc1c7ff5017006128e4ce53_b.jpg\" data-size=\"normal\" data-rawwidth=\"294\" data-rawheight=\"52\" class=\"content_image\" width=\"294\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;294&#39; height=&#39;52&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"294\" data-rawheight=\"52\" class=\"content_image lazy\" width=\"294\" data-actualsrc=\"https://pic4.zhimg.com/v2-bb09e1aeebc1c7ff5017006128e4ce53_b.jpg\"/><figcaption>二次多项式</figcaption></figure><p>这样模型的容量就会增大。</p><p>通常选择适当的模型容量可以尽量避免过拟合或者欠拟合，如下图：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-9373cfec9478252934bfa690fcd92110_b.jpg\" data-size=\"normal\" data-rawwidth=\"760\" data-rawheight=\"364\" class=\"origin_image zh-lightbox-thumb\" width=\"760\" data-original=\"https://pic1.zhimg.com/v2-9373cfec9478252934bfa690fcd92110_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;760&#39; height=&#39;364&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"760\" data-rawheight=\"364\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"760\" data-original=\"https://pic1.zhimg.com/v2-9373cfec9478252934bfa690fcd92110_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-9373cfec9478252934bfa690fcd92110_b.jpg\"/><figcaption>线性 vs 二次 vs 9次</figcaption></figure><p>模型本身所覆盖的函数族决定了模型的<b>表示容量</b>，但由于很多情况下很难得到最优解，所以学习算法的<b>有效容量</b>可能会小于模型的<b>表示容量</b>。</p><p>根据<b>奥卡姆剃刀</b>原则，在同样能够解释已知观测现象的假设中，我们应该选择最简单的那一个。</p><p>在量化模型容量的不同方法中，最有名的是<b>VC维</b>。VC维是二元分类器的容量，定义为该分类器能够分类的训练样本的最大数目。</p><p>此外，还存在一类<b>非参数模型</b>，如<b>最近邻回归</b>等。</p><p>容量和误差的关系：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-08e46f4e33df0039824dfccb5840f8bf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"754\" data-rawheight=\"362\" class=\"origin_image zh-lightbox-thumb\" width=\"754\" data-original=\"https://pic4.zhimg.com/v2-08e46f4e33df0039824dfccb5840f8bf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;754&#39; height=&#39;362&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"754\" data-rawheight=\"362\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"754\" data-original=\"https://pic4.zhimg.com/v2-08e46f4e33df0039824dfccb5840f8bf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-08e46f4e33df0039824dfccb5840f8bf_b.jpg\"/></figure><p>此外，从预先知道的真实分布 <img src=\"https://www.zhihu.com/equation?tex=p%28x%2Cy%29\" alt=\"p(x,y)\" eeimg=\"1\"/> 预测而出现的误差被称为<b>贝叶斯误差</b>。</p><p>训练样本数目和误差与最优容量的关系如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-4f710098b4467a2e0f591acca02e595d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"814\" data-rawheight=\"736\" class=\"origin_image zh-lightbox-thumb\" width=\"814\" data-original=\"https://pic2.zhimg.com/v2-4f710098b4467a2e0f591acca02e595d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;814&#39; height=&#39;736&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"814\" data-rawheight=\"736\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"814\" data-original=\"https://pic2.zhimg.com/v2-4f710098b4467a2e0f591acca02e595d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-4f710098b4467a2e0f591acca02e595d_b.jpg\"/></figure><p>在机器学习中，我们的研究目标不是找到一个通用的或者绝对好的学习算法，而是找到与“真实世界”或者我们关注的数据分布上效果最好。</p><blockquote>正则化</blockquote><p>在之前的讨论中，我们过度简化了模型容量的增减方法。事实上，我们也可以保持假设空间不变，通过添加<b>权重衰减</b>来降低模型容量：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-f44397da6850abb527434b5a0d2139cb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"350\" data-rawheight=\"64\" class=\"content_image\" width=\"350\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;350&#39; height=&#39;64&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"350\" data-rawheight=\"64\" class=\"content_image lazy\" width=\"350\" data-actualsrc=\"https://pic4.zhimg.com/v2-f44397da6850abb527434b5a0d2139cb_b.jpg\"/></figure><p>对于这样的权重衰减，效果如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-d1c292d137f463aff9782c0b3c176f0a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"798\" data-rawheight=\"366\" class=\"origin_image zh-lightbox-thumb\" width=\"798\" data-original=\"https://pic3.zhimg.com/v2-d1c292d137f463aff9782c0b3c176f0a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;798&#39; height=&#39;366&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"798\" data-rawheight=\"366\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"798\" data-original=\"https://pic3.zhimg.com/v2-d1c292d137f463aff9782c0b3c176f0a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-d1c292d137f463aff9782c0b3c176f0a_b.jpg\"/></figure><p><b>正则化是指修改学习算法，使其降低泛化误差而非训练误差</b>。上述的<b>权重衰减</b>方法是<b>正则化</b>的一种。</p><h2>5.3 超参数和验证集</h2><p>大多机器学习算法都有<b>超参数</b>，这些参数被预先设定的，而非通过学习出来的，例如线性回归中多项式的次数是<b>容量</b>超参数， <img src=\"https://www.zhihu.com/equation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"/> 是<b>权重衰减</b>的超参数。</p><p>这些超参数不通过学习得出的原因是学习这些参数会使模型趋向于过拟合。</p><p>我们也可以在训练集中拿出少量数据子集用于挑选超参数，这类数据集叫做<b>验证集</b>。</p><blockquote>交叉验证</blockquote><p>如果只将数据集分为固定的训练集和测试集，可能会导致误差统计的不确定性，所以我们经常需要<b>交叉验证</b>来统计误差。</p><p>对于<b>k-折交叉验证算法</b>而言：将数据集分成k个不重合的子集，每次选择其中一份作为测试集，其他作为训练集，重复k次，最终求出平均误差。</p><h2>5.4 估计、偏差和方差</h2><blockquote>点估计</blockquote><p><b>点估计</b>：根据m个数据点估计 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> ：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-777790437c04429baa291faf18deb5ac_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"286\" data-rawheight=\"54\" class=\"content_image\" width=\"286\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;286&#39; height=&#39;54&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"286\" data-rawheight=\"54\" class=\"content_image lazy\" width=\"286\" data-actualsrc=\"https://pic1.zhimg.com/v2-777790437c04429baa291faf18deb5ac_b.jpg\"/></figure><blockquote>偏差</blockquote><p>估计的<b>偏差</b>定义为：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-e5c2e73fca822b6fa23f89e98a2d0b07_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"292\" data-rawheight=\"42\" class=\"content_image\" width=\"292\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;292&#39; height=&#39;42&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"292\" data-rawheight=\"42\" class=\"content_image lazy\" width=\"292\" data-actualsrc=\"https://pic4.zhimg.com/v2-e5c2e73fca822b6fa23f89e98a2d0b07_b.jpg\"/></figure><p>如果满足：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-f1f3c7ed2c0951d96da86dd511342a7b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"162\" data-rawheight=\"34\" class=\"content_image\" width=\"162\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;162&#39; height=&#39;34&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"162\" data-rawheight=\"34\" class=\"content_image lazy\" width=\"162\" data-actualsrc=\"https://pic4.zhimg.com/v2-f1f3c7ed2c0951d96da86dd511342a7b_b.jpg\"/></figure><p>则估计量 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 被称为<b>无偏</b>的。</p><p>如果满足</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-e07089f170d3308865d265869ce88088_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"258\" data-rawheight=\"34\" class=\"content_image\" width=\"258\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;258&#39; height=&#39;34&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"258\" data-rawheight=\"34\" class=\"content_image lazy\" width=\"258\" data-actualsrc=\"https://pic1.zhimg.com/v2-e07089f170d3308865d265869ce88088_b.jpg\"/></figure><p>则估计量 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 被称为<b>渐近无偏</b>的。</p><blockquote>方差和标准差</blockquote><p><b>方差</b>：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-fd0e0d7d64626adf706b7b7893aacd69_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"142\" data-rawheight=\"54\" class=\"content_image\" width=\"142\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;142&#39; height=&#39;54&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"142\" data-rawheight=\"54\" class=\"content_image lazy\" width=\"142\" data-actualsrc=\"https://pic2.zhimg.com/v2-fd0e0d7d64626adf706b7b7893aacd69_b.jpg\"/></figure><p><b>标准差</b>是<b>方差</b>的平方根。例如，<b>均值</b>的<b>标准差</b>为：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3785d44a8a7799bcfb30c76e8f0a97f1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"482\" data-rawheight=\"124\" class=\"origin_image zh-lightbox-thumb\" width=\"482\" data-original=\"https://pic2.zhimg.com/v2-3785d44a8a7799bcfb30c76e8f0a97f1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;482&#39; height=&#39;124&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"482\" data-rawheight=\"124\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"482\" data-original=\"https://pic2.zhimg.com/v2-3785d44a8a7799bcfb30c76e8f0a97f1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-3785d44a8a7799bcfb30c76e8f0a97f1_b.jpg\"/></figure><blockquote>权衡偏差和方差以最小化均方误差</blockquote><p><b>偏差</b>和<b>方差</b>是度量估计量的两个不同误差来源，而事实上我们也可以用<b>均方误差</b>来同时估计<b>偏差</b>和<b>方差</b>：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-6793bb786ed918abf6281817ae65ee8e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"438\" data-rawheight=\"108\" class=\"origin_image zh-lightbox-thumb\" width=\"438\" data-original=\"https://pic3.zhimg.com/v2-6793bb786ed918abf6281817ae65ee8e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;438&#39; height=&#39;108&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"438\" data-rawheight=\"108\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"438\" data-original=\"https://pic3.zhimg.com/v2-6793bb786ed918abf6281817ae65ee8e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-6793bb786ed918abf6281817ae65ee8e_b.jpg\"/></figure><p><b>偏差</b>和<b>方差</b>与<b>欠拟合</b>与<b>过拟合</b>的关系紧密相连。通常，偏差较大时，认为模型欠拟合；而方差较大时，认为模型过拟合。如图：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-7c620325c6f851c382168fb69c64e5f2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"802\" data-rawheight=\"414\" class=\"origin_image zh-lightbox-thumb\" width=\"802\" data-original=\"https://pic3.zhimg.com/v2-7c620325c6f851c382168fb69c64e5f2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;802&#39; height=&#39;414&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"802\" data-rawheight=\"414\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"802\" data-original=\"https://pic3.zhimg.com/v2-7c620325c6f851c382168fb69c64e5f2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-7c620325c6f851c382168fb69c64e5f2_b.jpg\"/></figure><blockquote>一致性</blockquote><p>(弱)<b>一致性</b>：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-945d2db4cff8cd11402cefa657e93399_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"260\" data-rawheight=\"46\" class=\"content_image\" width=\"260\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;260&#39; height=&#39;46&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"260\" data-rawheight=\"46\" class=\"content_image lazy\" width=\"260\" data-actualsrc=\"https://pic2.zhimg.com/v2-945d2db4cff8cd11402cefa657e93399_b.jpg\"/></figure><p>此外，还存在<b>强一致性</b>（<b>几乎必然收敛</b>），指的是当</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-121211ee48d3c16c7e04f2deafc1c8c9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"296\" data-rawheight=\"40\" class=\"content_image\" width=\"296\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;296&#39; height=&#39;40&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"296\" data-rawheight=\"40\" class=\"content_image lazy\" width=\"296\" data-actualsrc=\"https://pic2.zhimg.com/v2-121211ee48d3c16c7e04f2deafc1c8c9_b.jpg\"/></figure><p>随机变量序列 <img src=\"https://www.zhihu.com/equation?tex=x%5E%7B%281%29%7D%2Cx%5E%7B%282%29%7D%2C...\" alt=\"x^{(1)},x^{(2)},...\" eeimg=\"1\"/> 收敛到 <img src=\"https://www.zhihu.com/equation?tex=x\" alt=\"x\" eeimg=\"1\"/> 。</p><h2>5.5 最大似然估计</h2><p>对于一组含有 <img src=\"https://www.zhihu.com/equation?tex=m\" alt=\"m\" eeimg=\"1\"/> 个样本的数据集 <img src=\"https://www.zhihu.com/equation?tex=X%3D%7B+x%5E%7B%281%29%7D%2C%5Cdots%2C+x%5E%7B%28m%29%7D+%7D\" alt=\"X={ x^{(1)},\\dots, x^{(m)} }\" eeimg=\"1\"/> ，独立地由未知的真实数据生成分布 <img src=\"https://www.zhihu.com/equation?tex=p_%7Bdata%7D%28x%29\" alt=\"p_{data}(x)\" eeimg=\"1\"/> 生成。 <img src=\"https://www.zhihu.com/equation?tex=p_%7Bmodel%7D%28x%2C+%5Ctheta%29\" alt=\"p_{model}(x, \\theta)\" eeimg=\"1\"/> 将任意输入 <img src=\"https://www.zhihu.com/equation?tex=x\" alt=\"x\" eeimg=\"1\"/> 映射到实数上来估计 <img src=\"https://www.zhihu.com/equation?tex=p_%7Bdata%7D%28x%29\" alt=\"p_{data}(x)\" eeimg=\"1\"/> 。</p><p>因此，对 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 的最大似然估计被定义为：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-a4332bb36106f2157265e512036a5341_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"470\" data-rawheight=\"162\" class=\"origin_image zh-lightbox-thumb\" width=\"470\" data-original=\"https://pic2.zhimg.com/v2-a4332bb36106f2157265e512036a5341_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;470&#39; height=&#39;162&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"470\" data-rawheight=\"162\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"470\" data-original=\"https://pic2.zhimg.com/v2-a4332bb36106f2157265e512036a5341_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-a4332bb36106f2157265e512036a5341_b.jpg\"/></figure><p>为了避免计算中可能出现的数值下溢，一般会对其做对数处理：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-c21482065eeeaca8f6cc36550a585f80_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"490\" data-rawheight=\"92\" class=\"origin_image zh-lightbox-thumb\" width=\"490\" data-original=\"https://pic1.zhimg.com/v2-c21482065eeeaca8f6cc36550a585f80_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;490&#39; height=&#39;92&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"490\" data-rawheight=\"92\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"490\" data-original=\"https://pic1.zhimg.com/v2-c21482065eeeaca8f6cc36550a585f80_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-c21482065eeeaca8f6cc36550a585f80_b.jpg\"/></figure><p>进而转化为：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ef3fa2f0d4a404c83a89e68e71092190_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"532\" data-rawheight=\"74\" class=\"origin_image zh-lightbox-thumb\" width=\"532\" data-original=\"https://pic1.zhimg.com/v2-ef3fa2f0d4a404c83a89e68e71092190_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;532&#39; height=&#39;74&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"532\" data-rawheight=\"74\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"532\" data-original=\"https://pic1.zhimg.com/v2-ef3fa2f0d4a404c83a89e68e71092190_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-ef3fa2f0d4a404c83a89e68e71092190_b.jpg\"/></figure><blockquote>条件对数似然估计</blockquote><p>最大似然估计可以拓展到条件概率的情况中，这个构成了大多数监督学习的基础：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-89935f4de374dfacb08fe3916ed42f20_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"422\" data-rawheight=\"68\" class=\"origin_image zh-lightbox-thumb\" width=\"422\" data-original=\"https://pic1.zhimg.com/v2-89935f4de374dfacb08fe3916ed42f20_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;422&#39; height=&#39;68&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"422\" data-rawheight=\"68\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"422\" data-original=\"https://pic1.zhimg.com/v2-89935f4de374dfacb08fe3916ed42f20_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-89935f4de374dfacb08fe3916ed42f20_b.jpg\"/></figure><blockquote>示例：线性回归作为最大似然</blockquote><p>现在，我们定义线性回归的 <img src=\"https://www.zhihu.com/equation?tex=p%28y%7Cx%29\" alt=\"p(y|x)\" eeimg=\"1\"/> 为：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-f4dd4c1e06fecbf982b7e4d3f396f2aa_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"326\" data-rawheight=\"34\" class=\"content_image\" width=\"326\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;326&#39; height=&#39;34&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"326\" data-rawheight=\"34\" class=\"content_image lazy\" width=\"326\" data-actualsrc=\"https://pic3.zhimg.com/v2-f4dd4c1e06fecbf982b7e4d3f396f2aa_b.jpg\"/></figure><p>其中方差 <img src=\"https://www.zhihu.com/equation?tex=%5Csigma%5E2\" alt=\"\\sigma^2\" eeimg=\"1\"/> 是某个常量，而函数 <img src=\"https://www.zhihu.com/equation?tex=%5Chat+y%28x%3Bw%29\" alt=\"\\hat y(x;w)\" eeimg=\"1\"/> 预测高斯分布的均值。</p><p>我们的目标是最大化对数似然，因此可以得到如下等式：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-58b90f571fc494c11764ef4344b9cca9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"598\" data-rawheight=\"176\" class=\"origin_image zh-lightbox-thumb\" width=\"598\" data-original=\"https://pic2.zhimg.com/v2-58b90f571fc494c11764ef4344b9cca9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;598&#39; height=&#39;176&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"598\" data-rawheight=\"176\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"598\" data-original=\"https://pic2.zhimg.com/v2-58b90f571fc494c11764ef4344b9cca9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-58b90f571fc494c11764ef4344b9cca9_b.jpg\"/></figure><p>对比最小化均方误差分布：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-11a6a04d326a71faaf90492736f4ac7f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"444\" data-rawheight=\"82\" class=\"origin_image zh-lightbox-thumb\" width=\"444\" data-original=\"https://pic4.zhimg.com/v2-11a6a04d326a71faaf90492736f4ac7f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;444&#39; height=&#39;82&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"444\" data-rawheight=\"82\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"444\" data-original=\"https://pic4.zhimg.com/v2-11a6a04d326a71faaf90492736f4ac7f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-11a6a04d326a71faaf90492736f4ac7f_b.jpg\"/></figure><p>可以得出两个式子可以得出相同的参数估计 <img src=\"https://www.zhihu.com/equation?tex=w\" alt=\"w\" eeimg=\"1\"/> 。</p><h2>5.6 贝叶斯统计</h2><p>待补充</p><h2>5.7 监督学习算法</h2><blockquote>线性回归和逻辑回归</blockquote><p><b>线性回归</b>的函数族：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-70e0e8df9c27575526d4f2d8005b0fa3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"334\" data-rawheight=\"46\" class=\"content_image\" width=\"334\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;334&#39; height=&#39;46&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"334\" data-rawheight=\"46\" class=\"content_image lazy\" width=\"334\" data-actualsrc=\"https://pic4.zhimg.com/v2-70e0e8df9c27575526d4f2d8005b0fa3_b.jpg\"/></figure><p>对于二元变量上的分布，可以使用 <img src=\"https://www.zhihu.com/equation?tex=sigmoid\" alt=\"sigmoid\" eeimg=\"1\"/> 函数将至于限制在 <img src=\"https://www.zhihu.com/equation?tex=%280%2C1%29\" alt=\"(0,1)\" eeimg=\"1\"/> ：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-c8b8782ffe9dfed4984445df60498d84_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"354\" data-rawheight=\"58\" class=\"content_image\" width=\"354\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;354&#39; height=&#39;58&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"354\" data-rawheight=\"58\" class=\"content_image lazy\" width=\"354\" data-actualsrc=\"https://pic1.zhimg.com/v2-c8b8782ffe9dfed4984445df60498d84_b.jpg\"/></figure><p>这叫做<b>逻辑回归</b>，但事实上这个模型用于<b>分类</b>而非回归。</p><blockquote>支持向量机</blockquote><p>支持向量机同样基于线性函数 <img src=\"https://www.zhihu.com/equation?tex=w%5ETx+%2B+b\" alt=\"w^Tx + b\" eeimg=\"1\"/> ，但是支持向量机是个分类模型。</p><p>其最重要的创新点是<b>核技巧</b>，机器学习算法可以写成样本点积的形式，例如：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-cf07fa065e58543bc1ac1138a892fc33_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"372\" data-rawheight=\"86\" class=\"content_image\" width=\"372\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;372&#39; height=&#39;86&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"372\" data-rawheight=\"86\" class=\"content_image lazy\" width=\"372\" data-actualsrc=\"https://pic4.zhimg.com/v2-cf07fa065e58543bc1ac1138a892fc33_b.jpg\"/></figure><p>其中，样本点积也被替换为核函数：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-5f4080a10f829e946d9eb9c9fc3fa846_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"290\" data-rawheight=\"32\" class=\"content_image\" width=\"290\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;290&#39; height=&#39;32&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"290\" data-rawheight=\"32\" class=\"content_image lazy\" width=\"290\" data-actualsrc=\"https://pic3.zhimg.com/v2-5f4080a10f829e946d9eb9c9fc3fa846_b.jpg\"/></figure><p>可以得出：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-108e63e5c37a805dc4aa21af90976655_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"386\" data-rawheight=\"90\" class=\"content_image\" width=\"386\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;386&#39; height=&#39;90&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"386\" data-rawheight=\"90\" class=\"content_image lazy\" width=\"386\" data-actualsrc=\"https://pic2.zhimg.com/v2-108e63e5c37a805dc4aa21af90976655_b.jpg\"/></figure><p>常见的核函数是<b>高斯核</b>：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-550d86d00e4478d3b562848ec1c926d9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"368\" data-rawheight=\"66\" class=\"content_image\" width=\"368\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;368&#39; height=&#39;66&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"368\" data-rawheight=\"66\" class=\"content_image lazy\" width=\"368\" data-actualsrc=\"https://pic2.zhimg.com/v2-550d86d00e4478d3b562848ec1c926d9_b.jpg\"/></figure><p>也叫做<b>径向基函数(radial basis function, RBF)</b>核。</p><blockquote>其他简单的监督学习方法</blockquote><ul><li>最近邻回归</li><li>决策树</li><li>··· </li></ul><h2>5.8 无监督学习方法</h2><p>通常来讲，无监督学习方法用于不需要人为标注的样本，常见的无监督学习方法：</p><ul><li>主成分分析</li><li>k-均值聚类</li><li>···</li></ul><h2>5.9 随机梯度下降</h2><p>对于标准的梯度下降，我们需要计算出<b>损失函数</b>的梯度，进而进行梯度下降。如对于负条件对数似然：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3b9caee78ddd59d0790ccefeb26e3141_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"638\" data-rawheight=\"84\" class=\"origin_image zh-lightbox-thumb\" width=\"638\" data-original=\"https://pic2.zhimg.com/v2-3b9caee78ddd59d0790ccefeb26e3141_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;638&#39; height=&#39;84&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"638\" data-rawheight=\"84\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"638\" data-original=\"https://pic2.zhimg.com/v2-3b9caee78ddd59d0790ccefeb26e3141_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-3b9caee78ddd59d0790ccefeb26e3141_b.jpg\"/></figure><p>其中每个样本的损失为：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-3b57672d1d0ef378984dd7d8fea24a94_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"338\" data-rawheight=\"40\" class=\"content_image\" width=\"338\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;338&#39; height=&#39;40&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"338\" data-rawheight=\"40\" class=\"content_image lazy\" width=\"338\" data-actualsrc=\"https://pic1.zhimg.com/v2-3b57672d1d0ef378984dd7d8fea24a94_b.jpg\"/></figure><p>计算 <img src=\"https://www.zhihu.com/equation?tex=J%28%5Ctheta%29\" alt=\"J(\\theta)\" eeimg=\"1\"/> 的梯度：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-c02863ec7118e03cfe8b813fca378eb6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"454\" data-rawheight=\"98\" class=\"origin_image zh-lightbox-thumb\" width=\"454\" data-original=\"https://pic3.zhimg.com/v2-c02863ec7118e03cfe8b813fca378eb6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;454&#39; height=&#39;98&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"454\" data-rawheight=\"98\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"454\" data-original=\"https://pic3.zhimg.com/v2-c02863ec7118e03cfe8b813fca378eb6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-c02863ec7118e03cfe8b813fca378eb6_b.jpg\"/></figure><p>在每次计算梯度时，均需要使用整个数据集 <img src=\"https://www.zhihu.com/equation?tex=X\" alt=\"X\" eeimg=\"1\"/> 来计算，时间复杂度为 <img src=\"https://www.zhihu.com/equation?tex=o%28m%29\" alt=\"o(m)\" eeimg=\"1\"/> 。当数据量迅速增长时，这样的方式不可取。</p><p>在<b>随机梯度下降</b>中，用小规模的样本近似估计梯度。梯度变成：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-635a0a4212490f40bd85186a30f515f3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"352\" data-rawheight=\"86\" class=\"content_image\" width=\"352\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;352&#39; height=&#39;86&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"352\" data-rawheight=\"86\" class=\"content_image lazy\" width=\"352\" data-actualsrc=\"https://pic4.zhimg.com/v2-635a0a4212490f40bd85186a30f515f3_b.jpg\"/></figure><p>其中 <img src=\"https://www.zhihu.com/equation?tex=m%5E%7B%27%7D+\" alt=\"m^{&#39;} \" eeimg=\"1\"/> 通常为固定的常数。</p><h2>5.10 构建机器学习算法</h2><p>在构建机器学习算法时，通常需要以下配方：</p><ul><li><b>特定的数据集</b></li><li><b>代价函数</b>：最常见的代价函数为负对数似然，同时代价函数也可以有正则项。</li><li><b>优化过程</b>：对于非线性模型，往往无法直接求解优化出代价函数最小值，需要使用梯度下降。</li><li><b>模型</b></li></ul><h2>5.11 促进深度学习发展的挑战</h2><p>传统机器学习方法在很多问题上都表现良好，但无法解决人工智能中的核心问题，如语音识别和对象识别。</p><blockquote>维度灾难</blockquote><p>当数据的维度增高后，数据的描述空间呈指数级增长，传统机器学习模型很难学习出有意义的知识。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-e21db9c40e2f3f5c544def7c054cd189_b.jpg\" data-size=\"normal\" data-rawwidth=\"826\" data-rawheight=\"316\" class=\"origin_image zh-lightbox-thumb\" width=\"826\" data-original=\"https://pic2.zhimg.com/v2-e21db9c40e2f3f5c544def7c054cd189_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;826&#39; height=&#39;316&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"826\" data-rawheight=\"316\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"826\" data-original=\"https://pic2.zhimg.com/v2-e21db9c40e2f3f5c544def7c054cd189_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-e21db9c40e2f3f5c544def7c054cd189_b.jpg\"/><figcaption>一维 vs 二维 vs 三维</figcaption></figure><blockquote>局部不变性和平滑正则化</blockquote><p>在机器学习中，我们通常把经验引入作为先验条件。其中很重要的一条就是<b>平滑先验</b>，也叫做<b>局部不变先验</b>。其含义是我们选择的机器学习模型里的函数不应在小区间内发生很大的变化，需要满足：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3de52e915c3c49e2422a4f63d1b76d9d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"228\" data-rawheight=\"54\" class=\"content_image\" width=\"228\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;228&#39; height=&#39;54&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"228\" data-rawheight=\"54\" class=\"content_image lazy\" width=\"228\" data-actualsrc=\"https://pic2.zhimg.com/v2-3de52e915c3c49e2422a4f63d1b76d9d_b.jpg\"/></figure><p>在高维模型中，区间数目很大，传统机器学习模型通常无法达到很好的泛化效果。例如，如果仅有 <img src=\"https://www.zhihu.com/equation?tex=o%28k%29\" alt=\"o(k)\" eeimg=\"1\"/> 个样本，是否能表达出 <img src=\"https://www.zhihu.com/equation?tex=o%282%5Ek%29\" alt=\"o(2^k)\" eeimg=\"1\"/> 大小的区间。</p><blockquote>流式学习</blockquote><p>我们首先对”<b>流形</b>“下一个模糊的定义：在每个点均存在变换能够使其从一个位置移动到另一个位置。例如地球就是<b>流形</b>。</p><p>当然 <img src=\"https://www.zhihu.com/equation?tex=R%5En\" alt=\"R^n\" eeimg=\"1\"/> 空间也是<b>流形</b>，但是我们无法期望我们能在整个空间内都学习到知识。因此<b>流式学习</b>算法提出这样一个假说：</p><p><b><img src=\"https://www.zhihu.com/equation?tex=R%5En\" alt=\"R^n\" eeimg=\"1\"/>中大部分区域都是无效的输入，有意义的输入值分布在包含少数点的流形中</b>。如图：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-3ea0c4ed6e5d60121a1b85186f30977b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"694\" data-rawheight=\"352\" class=\"origin_image zh-lightbox-thumb\" width=\"694\" data-original=\"https://pic4.zhimg.com/v2-3ea0c4ed6e5d60121a1b85186f30977b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;694&#39; height=&#39;352&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"694\" data-rawheight=\"352\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"694\" data-original=\"https://pic4.zhimg.com/v2-3ea0c4ed6e5d60121a1b85186f30977b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-3ea0c4ed6e5d60121a1b85186f30977b_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "自然语言处理", 
                    "tagLink": "https://api.zhihu.com/topics/19560026"
                }
            ], 
            "comments": [
                {
                    "userName": "「已注销」", 
                    "userLink": "https://www.zhihu.com/people/83214594bfd44cb3b3d120bedbf07fbb", 
                    "content": "赞赞！੧ᐛ੭", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/36681774", 
            "userName": "Octocat", 
            "userLink": "https://www.zhihu.com/people/db5a40777722f345e64b9c173860d605", 
            "upvote": 4, 
            "title": "《深度学习》读书笔记4——数值计算", 
            "content": "<h2>第四章 数值计算</h2><p>机器学习算法需要大量数值计算，但通常是通过<b>迭代过程更新解的估计值</b>来解决算法问题，而非直接推导出公式求解。</p><h2>4.1 上溢和下溢</h2><p>计算机里需要用有限位精度表示实数，所以跟数学理论相比总存在<b>误差</b>。两种常见的误差是：</p><ul><li><b>下溢</b>：较小的数字被舍入为零，在分母和 <img src=\"https://www.zhihu.com/equation?tex=log\" alt=\"log\" eeimg=\"1\"/> 下被视为 <img src=\"https://www.zhihu.com/equation?tex=NaN\" alt=\"NaN\" eeimg=\"1\"/> (Not a Number)。</li><li><b>上溢</b>：较大的数字无法被数据类型处理，通常被视为 <img src=\"https://www.zhihu.com/equation?tex=%5Cinfty\" alt=\"\\infty\" eeimg=\"1\"/> 。</li></ul><p>在算法实现的时候需要考虑这些误差，例如我们可以实现 <img src=\"https://www.zhihu.com/equation?tex=log%28softmax%28x%29%29\" alt=\"log(softmax(x))\" eeimg=\"1\"/> 来替代 <img src=\"https://www.zhihu.com/equation?tex=softmax%28x%29\" alt=\"softmax(x)\" eeimg=\"1\"/> 。</p><h2>4.2 病态条件</h2><p><b>条件数</b>指的是函数相对于输入的微小变化而变化的快慢程度。</p><p>对于条件数过大的函数，输入产生微小变化就会极大影响输出结果，因此需要着重考虑输入中的误差。</p><h2>4.3 基于梯度的优化方法</h2><p>在深度学习中，我们经常涉及优化任务，即通过改变 <img src=\"https://www.zhihu.com/equation?tex=x\" alt=\"x\" eeimg=\"1\"/> 来最小化或者最大化 <img src=\"https://www.zhihu.com/equation?tex=f%28x%29\" alt=\"f(x)\" eeimg=\"1\"/> 。最大化 <img src=\"https://www.zhihu.com/equation?tex=f%28x%29\" alt=\"f(x)\" eeimg=\"1\"/> 可以通过最小化 <img src=\"https://www.zhihu.com/equation?tex=-f%28x%29\" alt=\"-f(x)\" eeimg=\"1\"/> 来实现，所以我们经常讨论最小化任务：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-34a9f9974f0b46dec06d784976b629da_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"226\" data-rawheight=\"34\" class=\"content_image\" width=\"226\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;226&#39; height=&#39;34&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"226\" data-rawheight=\"34\" class=\"content_image lazy\" width=\"226\" data-actualsrc=\"https://pic3.zhimg.com/v2-34a9f9974f0b46dec06d784976b629da_b.jpg\"/></figure><p>通常把需要最小化的函数称为<b>代价(cost)函数</b>、<b>损失(loss)函数</b>或<b>误差(error)函数</b>。</p><p>根据所学的微积分知识，知道<b>导数</b>的意义是衡量了函数的变化趋势。容易推出，当 <img src=\"https://www.zhihu.com/equation?tex=%5Cvarepsilon\" alt=\"\\varepsilon\" eeimg=\"1\"/> 足够小的时候：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-449dcf342fcc7cff9849803482410885_b.jpg\" data-size=\"normal\" data-rawwidth=\"230\" data-rawheight=\"36\" class=\"content_image\" width=\"230\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;230&#39; height=&#39;36&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"230\" data-rawheight=\"36\" class=\"content_image lazy\" width=\"230\" data-actualsrc=\"https://pic2.zhimg.com/v2-449dcf342fcc7cff9849803482410885_b.jpg\"/><figcaption>sign(x) = {1, if x &gt; 0; 0, if x = 0; -1 if x &lt; 0}</figcaption></figure><p>上述式子是小于 <img src=\"https://www.zhihu.com/equation?tex=f%28x%29\" alt=\"f(x)\" eeimg=\"1\"/> 的。因为当导数大于0时，足够近的左侧小于当前位置；当导数小于0时，足够近的右侧小于当前位置。</p><p>我们可以通过不断向导数反方向移动一小步来减小 <img src=\"https://www.zhihu.com/equation?tex=f%28x%29\" alt=\"f(x)\" eeimg=\"1\"/> ，这样的方法称为<b>梯度下降</b>。</p><blockquote>梯度下降</blockquote><p>如图所示：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-5f504623be6c39555c802c9a3847c2f6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"944\" data-rawheight=\"566\" class=\"origin_image zh-lightbox-thumb\" width=\"944\" data-original=\"https://pic3.zhimg.com/v2-5f504623be6c39555c802c9a3847c2f6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;944&#39; height=&#39;566&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"944\" data-rawheight=\"566\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"944\" data-original=\"https://pic3.zhimg.com/v2-5f504623be6c39555c802c9a3847c2f6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-5f504623be6c39555c802c9a3847c2f6_b.jpg\"/></figure><p>这样的优化思路会在 <img src=\"https://www.zhihu.com/equation?tex=f%28x%29+%3D+0\" alt=\"f(x) = 0\" eeimg=\"1\"/> 处终止。事实上满足<img src=\"https://www.zhihu.com/equation?tex=f%28x%29+%3D+0\" alt=\"f(x) = 0\" eeimg=\"1\"/>的点共有两种：<b>极值</b>（极大值和极小值）和<b>鞍点</b>。如下图。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-10fad8217086a742eeb4711d9b84945d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"808\" data-rawheight=\"220\" class=\"origin_image zh-lightbox-thumb\" width=\"808\" data-original=\"https://pic2.zhimg.com/v2-10fad8217086a742eeb4711d9b84945d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;808&#39; height=&#39;220&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"808\" data-rawheight=\"220\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"808\" data-original=\"https://pic2.zhimg.com/v2-10fad8217086a742eeb4711d9b84945d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-10fad8217086a742eeb4711d9b84945d_b.jpg\"/></figure><p>因此，<b>梯度下降</b>并不能保证我们一定能找到全局最小值。</p><p>对于多维输入，我们需要用多元求导中<b>偏导数</b>的知识。<b>梯度</b>是函数 <img src=\"https://www.zhihu.com/equation?tex=f\" alt=\"f\" eeimg=\"1\"/> 相对于一个向量求导时所有偏导数的向量，记作：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-aa86fec6ab178edd8cef6b3b4359e736_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"94\" data-rawheight=\"32\" class=\"content_image\" width=\"94\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;94&#39; height=&#39;32&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"94\" data-rawheight=\"32\" class=\"content_image lazy\" width=\"94\" data-actualsrc=\"https://pic3.zhimg.com/v2-aa86fec6ab178edd8cef6b3b4359e736_b.jpg\"/></figure><p>梯度中第 <img src=\"https://www.zhihu.com/equation?tex=i\" alt=\"i\" eeimg=\"1\"/> 个元素就是 <img src=\"https://www.zhihu.com/equation?tex=f\" alt=\"f\" eeimg=\"1\"/> 关于 <img src=\"https://www.zhihu.com/equation?tex=x%5Ei\" alt=\"x^i\" eeimg=\"1\"/> 的偏导数。</p><p>求得梯度后，我们希望找到使 <img src=\"https://www.zhihu.com/equation?tex=f\" alt=\"f\" eeimg=\"1\"/> 下降最快的方向进行梯度下降。对于单位向量 <img src=\"https://www.zhihu.com/equation?tex=u\" alt=\"u\" eeimg=\"1\"/> 上的<b>方向导数</b>（ <img src=\"https://www.zhihu.com/equation?tex=f\" alt=\"f\" eeimg=\"1\"/> 的在 <img src=\"https://www.zhihu.com/equation?tex=u\" alt=\"u\" eeimg=\"1\"/> 方向上的斜率），有</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-573904b76417736de9fa74cecd264a8b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"420\" data-rawheight=\"134\" class=\"content_image\" width=\"420\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;420&#39; height=&#39;134&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"420\" data-rawheight=\"134\" class=\"content_image lazy\" width=\"420\" data-actualsrc=\"https://pic4.zhimg.com/v2-573904b76417736de9fa74cecd264a8b_b.jpg\"/></figure><p>简化后为：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-b76ea3840a84fe5f3ff519999022f936_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"126\" data-rawheight=\"34\" class=\"content_image\" width=\"126\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;126&#39; height=&#39;34&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"126\" data-rawheight=\"34\" class=\"content_image lazy\" width=\"126\" data-actualsrc=\"https://pic3.zhimg.com/v2-b76ea3840a84fe5f3ff519999022f936_b.jpg\"/></figure><p>可以得到与在梯度相反方向进行梯度下降，速度最快。这也照应了一元函数的梯度下降方法。</p><p>对于<b>梯度下降</b>算法，迭代过程公式为：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-962bee782e0e9950a3d2ea9b135fa877_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"238\" data-rawheight=\"48\" class=\"content_image\" width=\"238\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;238&#39; height=&#39;48&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"238\" data-rawheight=\"48\" class=\"content_image lazy\" width=\"238\" data-actualsrc=\"https://pic4.zhimg.com/v2-962bee782e0e9950a3d2ea9b135fa877_b.jpg\"/></figure><p>其中 <img src=\"https://www.zhihu.com/equation?tex=%5Cvarepsilon\" alt=\"\\varepsilon\" eeimg=\"1\"/> 称为<b>学习率</b>。</p><p>事实上，对于某些特定情况，完全可以求解</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-5af86dae640bfdc22a612bcda474f252_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"142\" data-rawheight=\"30\" class=\"content_image\" width=\"142\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;142&#39; height=&#39;30&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"142\" data-rawheight=\"30\" class=\"content_image lazy\" width=\"142\" data-actualsrc=\"https://pic3.zhimg.com/v2-5af86dae640bfdc22a612bcda474f252_b.jpg\"/></figure><p>来获得极值点，进而求出最小点。这种情况并不需要使用迭代计算的<b>梯度下降法</b>。</p><blockquote>Jacobian和Hessian矩阵</blockquote><p>有时候我们需要计算输入输出均为向量的函数，因此全部的<b>偏导数</b>可以组成一个矩阵，叫做<b>Jacobian矩阵</b>。其定义为：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-a5a5b7f73ae8a328747330d255ef85a6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"186\" data-rawheight=\"40\" class=\"content_image\" width=\"186\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;186&#39; height=&#39;40&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"186\" data-rawheight=\"40\" class=\"content_image lazy\" width=\"186\" data-actualsrc=\"https://pic3.zhimg.com/v2-a5a5b7f73ae8a328747330d255ef85a6_b.jpg\"/></figure><p>此外，我们经常会用到<b>二阶导数</b>。二阶函数描述了导数的变化趋势：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-e76dbbdb7fb70dd39b182b76fee32a51_b.jpg\" data-size=\"normal\" data-rawwidth=\"712\" data-rawheight=\"392\" class=\"origin_image zh-lightbox-thumb\" width=\"712\" data-original=\"https://pic2.zhimg.com/v2-e76dbbdb7fb70dd39b182b76fee32a51_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;712&#39; height=&#39;392&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"712\" data-rawheight=\"392\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"712\" data-original=\"https://pic2.zhimg.com/v2-e76dbbdb7fb70dd39b182b76fee32a51_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-e76dbbdb7fb70dd39b182b76fee32a51_b.jpg\"/><figcaption>不同曲率下实际函数值和梯度预测值的差异</figcaption></figure><p>当函数拥有多维输入时，二阶导数也有很多，也可以组成一个矩阵，叫做<b>Hessian矩阵</b>。其定义为：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-a9f8c4ba8aec6b068e50e7f3d1a03eb9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"328\" data-rawheight=\"76\" class=\"content_image\" width=\"328\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;328&#39; height=&#39;76&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"328\" data-rawheight=\"76\" class=\"content_image lazy\" width=\"328\" data-actualsrc=\"https://pic2.zhimg.com/v2-a9f8c4ba8aec6b068e50e7f3d1a03eb9_b.jpg\"/></figure><p>微分在任何二阶偏导连续的点可交换，所以</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-1c628219ba4f62f2e0a699fe8f82ca2d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"342\" data-rawheight=\"78\" class=\"content_image\" width=\"342\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;342&#39; height=&#39;78&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"342\" data-rawheight=\"78\" class=\"content_image lazy\" width=\"342\" data-actualsrc=\"https://pic2.zhimg.com/v2-1c628219ba4f62f2e0a699fe8f82ca2d_b.jpg\"/></figure><p>可以得出矩阵 <img src=\"https://www.zhihu.com/equation?tex=H\" alt=\"H\" eeimg=\"1\"/> 是实对称矩阵。因此其可以分解为一组实特征向量和特征值。</p><p>特定方向 <img src=\"https://www.zhihu.com/equation?tex=d\" alt=\"d\" eeimg=\"1\"/> 上的二阶导数可以写作 <img src=\"https://www.zhihu.com/equation?tex=d%5ETHd\" alt=\"d^THd\" eeimg=\"1\"/> ，如果 <img src=\"https://www.zhihu.com/equation?tex=d\" alt=\"d\" eeimg=\"1\"/> 是特征向量，则特征值就是相应的二次导数。</p><p>我们可以通过二阶导数来衡量<b>梯度下降</b>的表现，首先在 <img src=\"https://www.zhihu.com/equation?tex=x%5E%7B%280%29%7D\" alt=\"x^{(0)}\" eeimg=\"1\"/> 处对函数 <img src=\"https://www.zhihu.com/equation?tex=f%28x%29\" alt=\"f(x)\" eeimg=\"1\"/> 泰勒二阶展开：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-300ce0db155b6be2f2302b37ef0e534d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"704\" data-rawheight=\"70\" class=\"origin_image zh-lightbox-thumb\" width=\"704\" data-original=\"https://pic2.zhimg.com/v2-300ce0db155b6be2f2302b37ef0e534d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;704&#39; height=&#39;70&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"704\" data-rawheight=\"70\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"704\" data-original=\"https://pic2.zhimg.com/v2-300ce0db155b6be2f2302b37ef0e534d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-300ce0db155b6be2f2302b37ef0e534d_b.jpg\"/></figure><p>其中 <img src=\"https://www.zhihu.com/equation?tex=g\" alt=\"g\" eeimg=\"1\"/> 表示梯度， <img src=\"https://www.zhihu.com/equation?tex=H\" alt=\"H\" eeimg=\"1\"/> 是Hessian矩阵。</p><p>当使用学习率 <img src=\"https://www.zhihu.com/equation?tex=%5Cvarepsilon\" alt=\"\\varepsilon\" eeimg=\"1\"/> 进行梯度下降时，可以得到：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-148419d5d5a0ffb970514185f6d24d05_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"556\" data-rawheight=\"70\" class=\"origin_image zh-lightbox-thumb\" width=\"556\" data-original=\"https://pic2.zhimg.com/v2-148419d5d5a0ffb970514185f6d24d05_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;556&#39; height=&#39;70&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"556\" data-rawheight=\"70\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"556\" data-original=\"https://pic2.zhimg.com/v2-148419d5d5a0ffb970514185f6d24d05_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-148419d5d5a0ffb970514185f6d24d05_b.jpg\"/></figure><p>我们需要梯度下降后的函数值小于原函数值，就要保证上式的最后一项不能过大。通过计算可以得到，学习率的最优解是：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-c6706602b78fe74c3078c4009ea64070_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"194\" data-rawheight=\"74\" class=\"content_image\" width=\"194\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;194&#39; height=&#39;74&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"194\" data-rawheight=\"74\" class=\"content_image lazy\" width=\"194\" data-actualsrc=\"https://pic1.zhimg.com/v2-c6706602b78fe74c3078c4009ea64070_b.jpg\"/></figure><p>二阶导数还可以用来确定 <img src=\"https://www.zhihu.com/equation?tex=f%5E%7B%27%7D%28x%29+%3D+0\" alt=\"f^{&#39;}(x) = 0\" eeimg=\"1\"/> 的点是<b>极大点</b>、<b>极小点</b>或者<b>鞍点</b>。根据其几何意义很容易判断，这叫做<b>二阶函数测试</b>。</p><blockquote>牛顿法</blockquote><p>在多维情况下，梯度下降法很难找到合适的步长来达到最优解，因此可能会走很多弯路：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-cfa12a00c8dd0992a16469211db4f980_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"490\" data-rawheight=\"370\" class=\"origin_image zh-lightbox-thumb\" width=\"490\" data-original=\"https://pic1.zhimg.com/v2-cfa12a00c8dd0992a16469211db4f980_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;490&#39; height=&#39;370&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"490\" data-rawheight=\"370\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"490\" data-original=\"https://pic1.zhimg.com/v2-cfa12a00c8dd0992a16469211db4f980_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-cfa12a00c8dd0992a16469211db4f980_b.jpg\"/></figure><p>我们可以使用<b>Hessian矩阵</b>来指导优化过程。在<b>牛顿法</b>中，首先在 <img src=\"https://www.zhihu.com/equation?tex=x%5E%7B%280%29%7D\" alt=\"x^{(0)}\" eeimg=\"1\"/> 处进行二阶泰勒展开：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-c69d0fe2591cdf3f10c5af578acb086a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"844\" data-rawheight=\"54\" class=\"origin_image zh-lightbox-thumb\" width=\"844\" data-original=\"https://pic3.zhimg.com/v2-c69d0fe2591cdf3f10c5af578acb086a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;844&#39; height=&#39;54&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"844\" data-rawheight=\"54\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"844\" data-original=\"https://pic3.zhimg.com/v2-c69d0fe2591cdf3f10c5af578acb086a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-c69d0fe2591cdf3f10c5af578acb086a_b.jpg\"/></figure><p>通过计算，可以求出函数的临界点：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-47098264eeb7f164d5dd9a1052541ae8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"500\" data-rawheight=\"52\" class=\"origin_image zh-lightbox-thumb\" width=\"500\" data-original=\"https://pic1.zhimg.com/v2-47098264eeb7f164d5dd9a1052541ae8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;500&#39; height=&#39;52&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"500\" data-rawheight=\"52\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"500\" data-original=\"https://pic1.zhimg.com/v2-47098264eeb7f164d5dd9a1052541ae8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-47098264eeb7f164d5dd9a1052541ae8_b.jpg\"/></figure><p>多次迭代上述式子，直到收敛。</p><p>牛顿法仅在附近的临界点是最小点的情况下才适用，否则可能会导致不收敛。</p><blockquote>优化算法总结</blockquote><p>仅使用梯度信息的优化算法称为一阶优化算法，如<b>梯度下降</b>。使用Hessian矩阵的优化算法称为二阶优化算法，如<b>牛顿法</b>。</p><p><b>凸优化</b>是最成功的优化领域问题，指的是对凸函数进行最小值求解。但是深度学习中目标函数往往不是凸函数，所以很难把优化思路迁移过来，但在分析算法收敛性时有一定指导意义。</p><h2>4.4 约束优化</h2><p>在解决优化问题的时候，经常需要加入定义域的约束条件。这里提出了一种非常通用的解决方案：<b>Karush-Kuhn-Tucker(KKT)方法</b>。</p><p>假如 <img src=\"https://www.zhihu.com/equation?tex=x\" alt=\"x\" eeimg=\"1\"/> 的约束条件集合 <img src=\"https://www.zhihu.com/equation?tex=S\" alt=\"S\" eeimg=\"1\"/> 定义为：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-5c51cee99b4f59842e56526d66b4b67a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"496\" data-rawheight=\"38\" class=\"origin_image zh-lightbox-thumb\" width=\"496\" data-original=\"https://pic3.zhimg.com/v2-5c51cee99b4f59842e56526d66b4b67a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;496&#39; height=&#39;38&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"496\" data-rawheight=\"38\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"496\" data-original=\"https://pic3.zhimg.com/v2-5c51cee99b4f59842e56526d66b4b67a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-5c51cee99b4f59842e56526d66b4b67a_b.jpg\"/></figure><p>其中 <img src=\"https://www.zhihu.com/equation?tex=g%28x%29\" alt=\"g(x)\" eeimg=\"1\"/> 称为<b>等式约束</b>， <img src=\"https://www.zhihu.com/equation?tex=h%28x%29\" alt=\"h(x)\" eeimg=\"1\"/> 称为<b>不等式约束</b>。</p><p>根据上述定义，我们可以写出以下函数：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-8069dd3b327bd8433a9f6e8380c24a10_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"586\" data-rawheight=\"84\" class=\"origin_image zh-lightbox-thumb\" width=\"586\" data-original=\"https://pic1.zhimg.com/v2-8069dd3b327bd8433a9f6e8380c24a10_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;586&#39; height=&#39;84&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"586\" data-rawheight=\"84\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"586\" data-original=\"https://pic1.zhimg.com/v2-8069dd3b327bd8433a9f6e8380c24a10_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-8069dd3b327bd8433a9f6e8380c24a10_b.jpg\"/></figure><p>然后我们可以将集合 <img src=\"https://www.zhihu.com/equation?tex=S\" alt=\"S\" eeimg=\"1\"/> 上最小化 <img src=\"https://www.zhihu.com/equation?tex=f%28x%29\" alt=\"f(x)\" eeimg=\"1\"/> 的目标：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-5dcac67c26d06ec3a38d4704ed96012e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"122\" data-rawheight=\"54\" class=\"content_image\" width=\"122\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;122&#39; height=&#39;54&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"122\" data-rawheight=\"54\" class=\"content_image lazy\" width=\"122\" data-actualsrc=\"https://pic3.zhimg.com/v2-5dcac67c26d06ec3a38d4704ed96012e_b.jpg\"/></figure><p>转化为：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-d1f814db1b226dd1e2c39d8fc7f70061_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"314\" data-rawheight=\"56\" class=\"content_image\" width=\"314\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;314&#39; height=&#39;56&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"314\" data-rawheight=\"56\" class=\"content_image lazy\" width=\"314\" data-actualsrc=\"https://pic2.zhimg.com/v2-d1f814db1b226dd1e2c39d8fc7f70061_b.jpg\"/></figure><p>这是因为如果违反集合 <img src=\"https://www.zhihu.com/equation?tex=S\" alt=\"S\" eeimg=\"1\"/> 的约束，就会导致：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-7c0a72f07f6d1810fcff116968040d8a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"316\" data-rawheight=\"50\" class=\"content_image\" width=\"316\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;316&#39; height=&#39;50&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"316\" data-rawheight=\"50\" class=\"content_image lazy\" width=\"316\" data-actualsrc=\"https://pic3.zhimg.com/v2-7c0a72f07f6d1810fcff116968040d8a_b.jpg\"/></figure><p>但是，需要注意的是，对于<b>不等式约束 <img src=\"https://www.zhihu.com/equation?tex=h%28x%29\" alt=\"h(x)\" eeimg=\"1\"/></b> ，可能存在负值，导致在某些特定点上影响了 <img src=\"https://www.zhihu.com/equation?tex=f%28x%29\" alt=\"f(x)\" eeimg=\"1\"/> 的优化。但是，一定可以求出<b>驻点</b>(满足 <img src=\"https://www.zhihu.com/equation?tex=f%5E%7B%27%7D%28x%29%3D0\" alt=\"f^{&#39;}(x)=0\" eeimg=\"1\"/> 的点)<b>。</b></p><h2>4.5 实例：线性最小二乘</h2><blockquote>最小二乘法与梯度下降</blockquote><p>用梯度下降求解最小二乘法的优化问题：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-a2f4e29cd9c3356dcc58030910610af3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"284\" data-rawheight=\"82\" class=\"content_image\" width=\"284\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;284&#39; height=&#39;82&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"284\" data-rawheight=\"82\" class=\"content_image lazy\" width=\"284\" data-actualsrc=\"https://pic4.zhimg.com/v2-a2f4e29cd9c3356dcc58030910610af3_b.jpg\"/></figure><p>求解其导数：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-94ff4245d598e680ee51b8134d20f3d4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"524\" data-rawheight=\"50\" class=\"origin_image zh-lightbox-thumb\" width=\"524\" data-original=\"https://pic1.zhimg.com/v2-94ff4245d598e680ee51b8134d20f3d4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;524&#39; height=&#39;50&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"524\" data-rawheight=\"50\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"524\" data-original=\"https://pic1.zhimg.com/v2-94ff4245d598e680ee51b8134d20f3d4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-94ff4245d598e680ee51b8134d20f3d4_b.jpg\"/></figure><p>执行<b>梯度下降</b>直到导数小于某很小的正数 <img src=\"https://www.zhihu.com/equation?tex=%5Cdelta\" alt=\"\\delta\" eeimg=\"1\"/> ：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-5d66c6f4ff891104d1c8f09a6c6202e1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"964\" data-rawheight=\"236\" class=\"origin_image zh-lightbox-thumb\" width=\"964\" data-original=\"https://pic2.zhimg.com/v2-5d66c6f4ff891104d1c8f09a6c6202e1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;964&#39; height=&#39;236&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"964\" data-rawheight=\"236\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"964\" data-original=\"https://pic2.zhimg.com/v2-5d66c6f4ff891104d1c8f09a6c6202e1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-5d66c6f4ff891104d1c8f09a6c6202e1_b.jpg\"/></figure><blockquote>最小二乘法与约束优化</blockquote><p>如果对上述式子引入约束</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-6a5e7575891a9c8005b932decd957bae_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"114\" data-rawheight=\"36\" class=\"content_image\" width=\"114\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;114&#39; height=&#39;36&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"114\" data-rawheight=\"36\" class=\"content_image lazy\" width=\"114\" data-actualsrc=\"https://pic3.zhimg.com/v2-6a5e7575891a9c8005b932decd957bae_b.jpg\"/></figure><p>那么可以得到目标式子：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-190f08ca0bc7efab638b24e066d1e896_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"420\" data-rawheight=\"54\" class=\"content_image\" width=\"420\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;420&#39; height=&#39;54&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"420\" data-rawheight=\"54\" class=\"content_image lazy\" width=\"420\" data-actualsrc=\"https://pic3.zhimg.com/v2-190f08ca0bc7efab638b24e066d1e896_b.jpg\"/></figure><p>我们的优化目标是：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-2d6f9eb9d9ed9da9c34c0296b65ceaf0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"258\" data-rawheight=\"66\" class=\"content_image\" width=\"258\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;258&#39; height=&#39;66&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"258\" data-rawheight=\"66\" class=\"content_image lazy\" width=\"258\" data-actualsrc=\"https://pic1.zhimg.com/v2-2d6f9eb9d9ed9da9c34c0296b65ceaf0_b.jpg\"/></figure><p>将 <img src=\"https://www.zhihu.com/equation?tex=L%28x%2C%5Clambda%29\" alt=\"L(x,\\lambda)\" eeimg=\"1\"/> 对 <img src=\"https://www.zhihu.com/equation?tex=x\" alt=\"x\" eeimg=\"1\"/> 求导，令其等于 <img src=\"https://www.zhihu.com/equation?tex=0\" alt=\"0\" eeimg=\"1\"/> ：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ad0fad81fc313a3cd017a10500a449c8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"386\" data-rawheight=\"62\" class=\"content_image\" width=\"386\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;386&#39; height=&#39;62&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"386\" data-rawheight=\"62\" class=\"content_image lazy\" width=\"386\" data-actualsrc=\"https://pic1.zhimg.com/v2-ad0fad81fc313a3cd017a10500a449c8_b.jpg\"/></figure><p>可以得到解的形式是：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-7fb303b0ac50fe87aaf0704a3895ad39_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"360\" data-rawheight=\"54\" class=\"content_image\" width=\"360\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;360&#39; height=&#39;54&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"360\" data-rawheight=\"54\" class=\"content_image lazy\" width=\"360\" data-actualsrc=\"https://pic2.zhimg.com/v2-7fb303b0ac50fe87aaf0704a3895ad39_b.jpg\"/></figure><p>将 <img src=\"https://www.zhihu.com/equation?tex=x\" alt=\"x\" eeimg=\"1\"/> 对 <img src=\"https://www.zhihu.com/equation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"/> 求导：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b98346ea128d8b754a5a00ce0c5b17ed_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"344\" data-rawheight=\"82\" class=\"content_image\" width=\"344\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;344&#39; height=&#39;82&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"344\" data-rawheight=\"82\" class=\"content_image lazy\" width=\"344\" data-actualsrc=\"https://pic2.zhimg.com/v2-b98346ea128d8b754a5a00ce0c5b17ed_b.jpg\"/></figure><p>对 <img src=\"https://www.zhihu.com/equation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"/> 进行梯度上升，在这个过程中 <img src=\"https://www.zhihu.com/equation?tex=x\" alt=\"x\" eeimg=\"1\"/> 也随之优化，直到得到最终优化结果。</p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "读书笔记", 
                    "tagLink": "https://api.zhihu.com/topics/19590861"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/36619765", 
            "userName": "Octocat", 
            "userLink": "https://www.zhihu.com/people/db5a40777722f345e64b9c173860d605", 
            "upvote": 1, 
            "title": "《深度学习》读书笔记3——概率与信息论", 
            "content": "<h2>第三章 概率与信息论</h2><h2>3.1 为什么要使用概率</h2><p>机器学习从数据中学习到知识，因此通常必须处理不确定性，这就使用到概率论的知识。</p><blockquote>频率派概率和贝叶斯概率</blockquote><p>在概率论中，我们可以看到以下两类例子：</p><ol><li>扑克牌中抽出一手特定牌的概率</li><li>医生诊断病人患病的概率</li></ol><p>在这两个例子中，前者事件是可以重复的，我们可以通过大量反复实验统计事件的<b>频率</b>，这种概率和频率相联系，称为<b>频率派概率</b>；后者事件并不能重复，这样的概率表示了一种<b>信任度</b>，被称为<b>贝叶斯概率</b>。</p><p>在我们使用不确定性时，可以把频率派概率和贝叶斯概率视为等同的。</p><h2>3.2 随机变量</h2><p><b>随机变量</b>是可以随机地取不同值的变量。</p><p>通常用 <img src=\"https://www.zhihu.com/equation?tex=x_1%2C+x_2\" alt=\"x_1, x_2\" eeimg=\"1\"/>表示标量， <img src=\"https://www.zhihu.com/equation?tex=x\" alt=\"x\" eeimg=\"1\"/> 表示向量。</p><h2>3.3 概率分布</h2><p><b>概率分布</b>描述了随机变量在每一个可能取到的状态的可能性的大小。</p><blockquote>离散型变量和概率质量函数</blockquote><p>离散型变量的概率分布可以用<b>概率质量分布</b>表示， <img src=\"https://www.zhihu.com/equation?tex=P%28x%29\" alt=\"P(x)\" eeimg=\"1\"/> 表示随机变量的值等于 <img src=\"https://www.zhihu.com/equation?tex=x\" alt=\"x\" eeimg=\"1\"/> 的概率。</p><p><img src=\"https://www.zhihu.com/equation?tex=P%28x%29\" alt=\"P(x)\" eeimg=\"1\"/> 的性质有：</p><ul><li>P的定义域是x所有可能状态的集合</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-fb346d2497d9168925bca57bdeeac6a3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"256\" data-rawheight=\"34\" class=\"content_image\" width=\"256\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;256&#39; height=&#39;34&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"256\" data-rawheight=\"34\" class=\"content_image lazy\" width=\"256\" data-actualsrc=\"https://pic4.zhimg.com/v2-fb346d2497d9168925bca57bdeeac6a3_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-bf25d32840ba000dba189019425ff2b9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"192\" data-rawheight=\"44\" class=\"content_image\" width=\"192\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;192&#39; height=&#39;44&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"192\" data-rawheight=\"44\" class=\"content_image lazy\" width=\"192\" data-actualsrc=\"https://pic2.zhimg.com/v2-bf25d32840ba000dba189019425ff2b9_b.jpg\"/></figure><blockquote>连续型变量和概率密度函数</blockquote><p>连续型变量的概率分布可以用<b>概率密度分布</b>表示。离散型变量可以度量某个点的概率，但是连续型变量只能度量某个区间上的概率（因为每个点上的概率均为0），所以不能用概率质量分布表示。区间 <img src=\"https://www.zhihu.com/equation?tex=%5Ba%2C+b%5D\" alt=\"[a, b]\" eeimg=\"1\"/> 上的概率可以用下面公式计算：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-5f4a60f29327c9eb2d28fb6a4e2a9e55_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"136\" data-rawheight=\"34\" class=\"content_image\" width=\"136\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;136&#39; height=&#39;34&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"136\" data-rawheight=\"34\" class=\"content_image lazy\" width=\"136\" data-actualsrc=\"https://pic2.zhimg.com/v2-5f4a60f29327c9eb2d28fb6a4e2a9e55_b.jpg\"/></figure><p><img src=\"https://www.zhihu.com/equation?tex=p%28x%29\" alt=\"p(x)\" eeimg=\"1\"/> 的性质有：</p><ul><li>P的定义域是x所有可能状态的集合</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-11427ea31728f513b2d8055dd124264a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"198\" data-rawheight=\"48\" class=\"content_image\" width=\"198\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;198&#39; height=&#39;48&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"198\" data-rawheight=\"48\" class=\"content_image lazy\" width=\"198\" data-actualsrc=\"https://pic3.zhimg.com/v2-11427ea31728f513b2d8055dd124264a_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-e89c155e2db3273ccd2a52408894c12a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"170\" data-rawheight=\"52\" class=\"content_image\" width=\"170\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;170&#39; height=&#39;52&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"170\" data-rawheight=\"52\" class=\"content_image lazy\" width=\"170\" data-actualsrc=\"https://pic3.zhimg.com/v2-e89c155e2db3273ccd2a52408894c12a_b.jpg\"/></figure><h2>3.4 联合概率和边缘概率</h2><p>对于多个随机变量，可以用 <img src=\"https://www.zhihu.com/equation?tex=P%28x%2C+y%29\" alt=\"P(x, y)\" eeimg=\"1\"/> 表示<b>联合概率</b>。同时，也可以求出<b>联合概率</b>下的子集概率 <img src=\"https://www.zhihu.com/equation?tex=P%28x%29\" alt=\"P(x)\" eeimg=\"1\"/> ，这样的概率分布称为<b>边缘概率</b>。</p><p>概率质量函数的边缘概率公式为：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-68025e73a4ce19a0643b15c7e62eb37f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"498\" data-rawheight=\"82\" class=\"origin_image zh-lightbox-thumb\" width=\"498\" data-original=\"https://pic4.zhimg.com/v2-68025e73a4ce19a0643b15c7e62eb37f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;498&#39; height=&#39;82&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"498\" data-rawheight=\"82\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"498\" data-original=\"https://pic4.zhimg.com/v2-68025e73a4ce19a0643b15c7e62eb37f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-68025e73a4ce19a0643b15c7e62eb37f_b.jpg\"/></figure><p>概率密度函数的边缘概率公式为：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3900fedf3e0e3b181e7c412670e967f5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"268\" data-rawheight=\"80\" class=\"content_image\" width=\"268\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;268&#39; height=&#39;80&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"268\" data-rawheight=\"80\" class=\"content_image lazy\" width=\"268\" data-actualsrc=\"https://pic2.zhimg.com/v2-3900fedf3e0e3b181e7c412670e967f5_b.jpg\"/></figure><h2>3.5 条件概率</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-e3fb8283738995e3288ae1ac8644cdf2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"466\" data-rawheight=\"96\" class=\"origin_image zh-lightbox-thumb\" width=\"466\" data-original=\"https://pic3.zhimg.com/v2-e3fb8283738995e3288ae1ac8644cdf2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;466&#39; height=&#39;96&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"466\" data-rawheight=\"96\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"466\" data-original=\"https://pic3.zhimg.com/v2-e3fb8283738995e3288ae1ac8644cdf2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-e3fb8283738995e3288ae1ac8644cdf2_b.jpg\"/></figure><h2>3.6 条件概率的链式法则</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-9f3e436fc3137e3b958b1ecf09deee6a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"642\" data-rawheight=\"64\" class=\"origin_image zh-lightbox-thumb\" width=\"642\" data-original=\"https://pic3.zhimg.com/v2-9f3e436fc3137e3b958b1ecf09deee6a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;642&#39; height=&#39;64&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"642\" data-rawheight=\"64\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"642\" data-original=\"https://pic3.zhimg.com/v2-9f3e436fc3137e3b958b1ecf09deee6a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-9f3e436fc3137e3b958b1ecf09deee6a_b.jpg\"/></figure><p>对于 <img src=\"https://www.zhihu.com/equation?tex=P%28a%2Cb%2Cc%29\" alt=\"P(a,b,c)\" eeimg=\"1\"/> ，可以用以下式子计算：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-7454445af87c51817a117d525f8c42e2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"500\" data-rawheight=\"126\" class=\"origin_image zh-lightbox-thumb\" width=\"500\" data-original=\"https://pic3.zhimg.com/v2-7454445af87c51817a117d525f8c42e2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;500&#39; height=&#39;126&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"500\" data-rawheight=\"126\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"500\" data-original=\"https://pic3.zhimg.com/v2-7454445af87c51817a117d525f8c42e2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-7454445af87c51817a117d525f8c42e2_b.jpg\"/></figure><h2>3.7 独立性和条件独立性</h2><p>对于随机变量x和y，如果满足：<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-0c837bcb949a86e78a4debd53a6357c3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"616\" data-rawheight=\"48\" class=\"origin_image zh-lightbox-thumb\" width=\"616\" data-original=\"https://pic4.zhimg.com/v2-0c837bcb949a86e78a4debd53a6357c3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;616&#39; height=&#39;48&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"616\" data-rawheight=\"48\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"616\" data-original=\"https://pic4.zhimg.com/v2-0c837bcb949a86e78a4debd53a6357c3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-0c837bcb949a86e78a4debd53a6357c3_b.jpg\"/></figure><p>那么称这两个随机变量是<b>相互独立</b>的，记作 <img src=\"https://www.zhihu.com/equation?tex=x%5Cbot+y\" alt=\"x\\bot y\" eeimg=\"1\"/> 。</p><p>如果满足：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-f6ef3de67bce483ae13ba3436fa84174_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"928\" data-rawheight=\"48\" class=\"origin_image zh-lightbox-thumb\" width=\"928\" data-original=\"https://pic1.zhimg.com/v2-f6ef3de67bce483ae13ba3436fa84174_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;928&#39; height=&#39;48&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"928\" data-rawheight=\"48\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"928\" data-original=\"https://pic1.zhimg.com/v2-f6ef3de67bce483ae13ba3436fa84174_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-f6ef3de67bce483ae13ba3436fa84174_b.jpg\"/></figure><p>那么称这两个随机变量在给定随机变量z时是<b>条件独立</b>的，记作 <img src=\"https://www.zhihu.com/equation?tex=x%5Cbot+y+%7C+z\" alt=\"x\\bot y | z\" eeimg=\"1\"/> 。</p><h2>3.8 期望、方差和协方差</h2><p><b>期望</b>指的是 <img src=\"https://www.zhihu.com/equation?tex=f%28x%29\" alt=\"f(x)\" eeimg=\"1\"/> 的加权平均值，其两种计算方法：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-a0e952f5cb8586f6f889fe51159f6eb7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"354\" data-rawheight=\"82\" class=\"content_image\" width=\"354\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;354&#39; height=&#39;82&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"354\" data-rawheight=\"82\" class=\"content_image lazy\" width=\"354\" data-actualsrc=\"https://pic4.zhimg.com/v2-a0e952f5cb8586f6f889fe51159f6eb7_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-0e6031a1d104d752131db226bf1d8fcd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"376\" data-rawheight=\"94\" class=\"content_image\" width=\"376\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;376&#39; height=&#39;94&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"376\" data-rawheight=\"94\" class=\"content_image lazy\" width=\"376\" data-actualsrc=\"https://pic2.zhimg.com/v2-0e6031a1d104d752131db226bf1d8fcd_b.jpg\"/></figure><p>期望是<b>线性</b>的，满足：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-e63f7abecc073dbf42fffe691bdc6138_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"542\" data-rawheight=\"56\" class=\"origin_image zh-lightbox-thumb\" width=\"542\" data-original=\"https://pic1.zhimg.com/v2-e63f7abecc073dbf42fffe691bdc6138_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;542&#39; height=&#39;56&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"542\" data-rawheight=\"56\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"542\" data-original=\"https://pic1.zhimg.com/v2-e63f7abecc073dbf42fffe691bdc6138_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-e63f7abecc073dbf42fffe691bdc6138_b.jpg\"/></figure><p><b>方差</b>衡量了数据相对于期望差异的大小，计算方法为：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-d9814f5c0abd88e024158d9d17914a84_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"430\" data-rawheight=\"64\" class=\"origin_image zh-lightbox-thumb\" width=\"430\" data-original=\"https://pic1.zhimg.com/v2-d9814f5c0abd88e024158d9d17914a84_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;430&#39; height=&#39;64&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"430\" data-rawheight=\"64\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"430\" data-original=\"https://pic1.zhimg.com/v2-d9814f5c0abd88e024158d9d17914a84_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-d9814f5c0abd88e024158d9d17914a84_b.jpg\"/></figure><p>方差的平方根成为<b>标准差</b>。</p><p><b>协方差</b>描述了两个变量线性相关的程度：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-58c39bd0c69a16f89bce64cf76b0621d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"668\" data-rawheight=\"48\" class=\"origin_image zh-lightbox-thumb\" width=\"668\" data-original=\"https://pic2.zhimg.com/v2-58c39bd0c69a16f89bce64cf76b0621d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;668&#39; height=&#39;48&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"668\" data-rawheight=\"48\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"668\" data-original=\"https://pic2.zhimg.com/v2-58c39bd0c69a16f89bce64cf76b0621d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-58c39bd0c69a16f89bce64cf76b0621d_b.jpg\"/></figure><p>协方差的正负对应的是两个变量的正相关与负相关。同时为了均一化协方差的值，定义了指标<b>相关系数</b>：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-d233f3a28fc4689c4855d6aeb6a12ee2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"128\" data-rawheight=\"50\" class=\"content_image\" width=\"128\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;128&#39; height=&#39;50&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"128\" data-rawheight=\"50\" class=\"content_image lazy\" width=\"128\" data-actualsrc=\"https://pic3.zhimg.com/v2-d233f3a28fc4689c4855d6aeb6a12ee2_b.jpg\"/></figure><p><b>相关系数</b>就是用X、Y的协方差除以X的标准差和Y的标准差。可以证明，相关系数的取值被均一化为 <img src=\"https://www.zhihu.com/equation?tex=%5B-1%2C+1%5D\" alt=\"[-1, 1]\" eeimg=\"1\"/> 。</p><p>另外，<b>方差实际上只是协方差的一种特殊情况</b>。</p><h2>3.9 常用概率分布</h2><blockquote>Bernoulli分布和Multinoulli分布</blockquote><p><b>伯努利(Bernoulli)分布</b>是单个随机变量只取 <img src=\"https://www.zhihu.com/equation?tex=0%2C+1\" alt=\"0, 1\" eeimg=\"1\"/> 两个值的分布，满足如下性质：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-3abaca123ef140277b8f89472cf56147_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"358\" data-rawheight=\"234\" class=\"content_image\" width=\"358\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;358&#39; height=&#39;234&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"358\" data-rawheight=\"234\" class=\"content_image lazy\" width=\"358\" data-actualsrc=\"https://pic4.zhimg.com/v2-3abaca123ef140277b8f89472cf56147_b.jpg\"/></figure><p><b>Multinoulli分布</b>是Bernoulli分布的延伸，指的是单个随机变量可以取k个不同的离散值。</p><blockquote>高斯分布</blockquote><p><b>高斯分布（正态分布）</b>的公式为：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-33080926da8c5c3d6164fb739611a4b5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"532\" data-rawheight=\"92\" class=\"origin_image zh-lightbox-thumb\" width=\"532\" data-original=\"https://pic2.zhimg.com/v2-33080926da8c5c3d6164fb739611a4b5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;532&#39; height=&#39;92&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"532\" data-rawheight=\"92\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"532\" data-original=\"https://pic2.zhimg.com/v2-33080926da8c5c3d6164fb739611a4b5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-33080926da8c5c3d6164fb739611a4b5_b.jpg\"/></figure><p>其中 <img src=\"https://www.zhihu.com/equation?tex=%5Cmu\" alt=\"\\mu\" eeimg=\"1\"/> 表示正态分布的<b>期望</b>， <img src=\"https://www.zhihu.com/equation?tex=%5Csigma\" alt=\"\\sigma\" eeimg=\"1\"/> 表示正态分布的<b>标准差</b>。</p><p>标准正态分布中<img src=\"https://www.zhihu.com/equation?tex=%5Cmu+%3D+0%EF%BC%8C+%5Csigma+%3D+1\" alt=\"\\mu = 0， \\sigma = 1\" eeimg=\"1\"/>，如图：<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-26b8978fed96bf47cded8acda7cfbc6a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"890\" data-rawheight=\"454\" class=\"origin_image zh-lightbox-thumb\" width=\"890\" data-original=\"https://pic3.zhimg.com/v2-26b8978fed96bf47cded8acda7cfbc6a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;890&#39; height=&#39;454&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"890\" data-rawheight=\"454\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"890\" data-original=\"https://pic3.zhimg.com/v2-26b8978fed96bf47cded8acda7cfbc6a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-26b8978fed96bf47cded8acda7cfbc6a_b.jpg\"/></figure><blockquote>指数分布和Laplace分布</blockquote><p><b>指数分布</b>指的是事件以恒定平均速率连续且独立地发生，公式为：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-e2f5e6e60554c6d09bf87a8b47bf3a06_b.jpg\" data-size=\"normal\" data-rawwidth=\"324\" data-rawheight=\"42\" class=\"content_image\" width=\"324\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;324&#39; height=&#39;42&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"324\" data-rawheight=\"42\" class=\"content_image lazy\" width=\"324\" data-actualsrc=\"https://pic3.zhimg.com/v2-e2f5e6e60554c6d09bf87a8b47bf3a06_b.jpg\"/><figcaption>1 condition 表示条件为真时取1，否则取0</figcaption></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-330f90e32eadd7439708bd8bdea163bd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"500\" data-rawheight=\"400\" class=\"origin_image zh-lightbox-thumb\" width=\"500\" data-original=\"https://pic2.zhimg.com/v2-330f90e32eadd7439708bd8bdea163bd_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;500&#39; height=&#39;400&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"500\" data-rawheight=\"400\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"500\" data-original=\"https://pic2.zhimg.com/v2-330f90e32eadd7439708bd8bdea163bd_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-330f90e32eadd7439708bd8bdea163bd_b.jpg\"/></figure><p>指数分布在 <img src=\"https://www.zhihu.com/equation?tex=x%3D0\" alt=\"x=0\" eeimg=\"1\"/> 处取得边界值，此外<b>Laplace分布</b>允许在任意一点 <img src=\"https://www.zhihu.com/equation?tex=%5Cmu\" alt=\"\\mu\" eeimg=\"1\"/> 处设置概率质量的峰值：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-5876743d1a65c91419fdb9d661713e5d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"488\" data-rawheight=\"96\" class=\"origin_image zh-lightbox-thumb\" width=\"488\" data-original=\"https://pic2.zhimg.com/v2-5876743d1a65c91419fdb9d661713e5d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;488&#39; height=&#39;96&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"488\" data-rawheight=\"96\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"488\" data-original=\"https://pic2.zhimg.com/v2-5876743d1a65c91419fdb9d661713e5d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-5876743d1a65c91419fdb9d661713e5d_b.jpg\"/></figure><blockquote>Dirac分布和经验分布</blockquote><p><b>Dirac分布</b>表示了我们把概率分布中所有质量都集中在一个点。不同于普通函数一样对每个函数值x都有实数值输出的定义，Dirac分布定义了一个在 <img src=\"https://www.zhihu.com/equation?tex=%5Cmu\" alt=\"\\mu\" eeimg=\"1\"/> 处无限窄但是无限高的概率质量：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-4f3d425d82c2de61f5b0adb6801e50fe_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"212\" data-rawheight=\"36\" class=\"content_image\" width=\"212\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;212&#39; height=&#39;36&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"212\" data-rawheight=\"36\" class=\"content_image lazy\" width=\"212\" data-actualsrc=\"https://pic3.zhimg.com/v2-4f3d425d82c2de61f5b0adb6801e50fe_b.jpg\"/></figure><p>我们用极限的思想在连续型随机变量上定义了一个近似于离散型随机变量的分布。同样的，我们可以拓展其为连续型随机变量上的<b>经验分布</b>：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-c5e9263362e2fdfdb06d12c4a91929bf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"322\" data-rawheight=\"86\" class=\"content_image\" width=\"322\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;322&#39; height=&#39;86&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"322\" data-rawheight=\"86\" class=\"content_image lazy\" width=\"322\" data-actualsrc=\"https://pic4.zhimg.com/v2-c5e9263362e2fdfdb06d12c4a91929bf_b.jpg\"/></figure><blockquote>分布的混合</blockquote><p>我们可以通过一些简单的分布来组合构造出<b>混合分布</b>。混合分布由组件分布构成，公式为：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-157a87d021d19a6db50f74c9ddddca5a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"392\" data-rawheight=\"72\" class=\"content_image\" width=\"392\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;392&#39; height=&#39;72&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"392\" data-rawheight=\"72\" class=\"content_image lazy\" width=\"392\" data-actualsrc=\"https://pic3.zhimg.com/v2-157a87d021d19a6db50f74c9ddddca5a_b.jpg\"/></figure><p>这里的 <img src=\"https://www.zhihu.com/equation?tex=P%28c%29\" alt=\"P(c)\" eeimg=\"1\"/> 实际上是各组件的一个Multinoulli分布。在混合模型中，存在我们无法直接观测到的随机变量：<b>潜变量</b>。上述公式中的 <img src=\"https://www.zhihu.com/equation?tex=c\" alt=\"c\" eeimg=\"1\"/> 就是一个潜变量。</p><h2>3.10 常用函数的有用性</h2><blockquote>sigmoid函数和softplus函数</blockquote><p><b>sigmoid函数</b>的公式：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-9fc64807249e706f267530131b22e19b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"270\" data-rawheight=\"80\" class=\"content_image\" width=\"270\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;270&#39; height=&#39;80&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"270\" data-rawheight=\"80\" class=\"content_image lazy\" width=\"270\" data-actualsrc=\"https://pic4.zhimg.com/v2-9fc64807249e706f267530131b22e19b_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-54c209441edf40762ca781525529549d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"758\" data-rawheight=\"444\" class=\"origin_image zh-lightbox-thumb\" width=\"758\" data-original=\"https://pic2.zhimg.com/v2-54c209441edf40762ca781525529549d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;758&#39; height=&#39;444&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"758\" data-rawheight=\"444\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"758\" data-original=\"https://pic2.zhimg.com/v2-54c209441edf40762ca781525529549d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-54c209441edf40762ca781525529549d_b.jpg\"/></figure><p>sigmoid函数的取值为 <img src=\"https://www.zhihu.com/equation?tex=%5B0%2C+1%5D\" alt=\"[0, 1]\" eeimg=\"1\"/> ，在 <img src=\"https://www.zhihu.com/equation?tex=x\" alt=\"x\" eeimg=\"1\"/> 绝对值较大的时候，斜率趋近于0，出现<b>饱和</b>现象。</p><p><b>softplus函数</b>的公式：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-399139240720581b5e4bec3de460809f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"308\" data-rawheight=\"52\" class=\"content_image\" width=\"308\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;308&#39; height=&#39;52&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"308\" data-rawheight=\"52\" class=\"content_image lazy\" width=\"308\" data-actualsrc=\"https://pic4.zhimg.com/v2-399139240720581b5e4bec3de460809f_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-d6826dbb03c800e88433f7fb9e3cf6aa_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"722\" data-rawheight=\"464\" class=\"origin_image zh-lightbox-thumb\" width=\"722\" data-original=\"https://pic3.zhimg.com/v2-d6826dbb03c800e88433f7fb9e3cf6aa_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;722&#39; height=&#39;464&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"722\" data-rawheight=\"464\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"722\" data-original=\"https://pic3.zhimg.com/v2-d6826dbb03c800e88433f7fb9e3cf6aa_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-d6826dbb03c800e88433f7fb9e3cf6aa_b.jpg\"/></figure><p>这个函数实际上是<b>正部函数</b>的平滑：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-400c8a41b220e9e248682689285db713_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"276\" data-rawheight=\"50\" class=\"content_image\" width=\"276\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;276&#39; height=&#39;50&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"276\" data-rawheight=\"50\" class=\"content_image lazy\" width=\"276\" data-actualsrc=\"https://pic4.zhimg.com/v2-400c8a41b220e9e248682689285db713_b.jpg\"/></figure><blockquote>函数的性质</blockquote><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-364ee7ddb241ecf98a27a5e0d027092d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"444\" data-rawheight=\"570\" class=\"origin_image zh-lightbox-thumb\" width=\"444\" data-original=\"https://pic2.zhimg.com/v2-364ee7ddb241ecf98a27a5e0d027092d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;444&#39; height=&#39;570&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"444\" data-rawheight=\"570\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"444\" data-original=\"https://pic2.zhimg.com/v2-364ee7ddb241ecf98a27a5e0d027092d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-364ee7ddb241ecf98a27a5e0d027092d_b.jpg\"/></figure><h2>3.11 贝叶斯规则</h2><p>如果需要已知 <img src=\"https://www.zhihu.com/equation?tex=p%28y%7Cx%29\" alt=\"p(y|x)\" eeimg=\"1\"/> ，想要计算 <img src=\"https://www.zhihu.com/equation?tex=p%28x%7Cy%29\" alt=\"p(x|y)\" eeimg=\"1\"/> ，可以使用<b>贝叶斯规则</b>，该规则可以使用条件概率的定义推导出来：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-6a14858475dfd2d345f9bd31d208f804_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"348\" data-rawheight=\"80\" class=\"content_image\" width=\"348\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;348&#39; height=&#39;80&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"348\" data-rawheight=\"80\" class=\"content_image lazy\" width=\"348\" data-actualsrc=\"https://pic1.zhimg.com/v2-6a14858475dfd2d345f9bd31d208f804_b.jpg\"/></figure><p>其中</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-5526196d3d1901ef964f696ff6fdae81_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"280\" data-rawheight=\"34\" class=\"content_image\" width=\"280\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;280&#39; height=&#39;34&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"280\" data-rawheight=\"34\" class=\"content_image lazy\" width=\"280\" data-actualsrc=\"https://pic2.zhimg.com/v2-5526196d3d1901ef964f696ff6fdae81_b.jpg\"/></figure><h2>3.12 连续型变量的技术细节</h2><p>深入理解连续型随机变量需要<b>测度论</b>的知识，在这里只简要介绍两个概念：</p><ul><li><b>零测度</b>：在度量空间里不占用任何体积，如直线上的点、空间上的直线。</li><li><b>几乎处处</b>：如果某个性质除零测度的集合外均成立，可以认为是几乎处处成立的。</li></ul><p>此外，纠正一个可能出现的常识错误。如果 <img src=\"https://www.zhihu.com/equation?tex=y+%3D+g%28x%29\" alt=\"y = g(x)\" eeimg=\"1\"/> ，且 <img src=\"https://www.zhihu.com/equation?tex=g\" alt=\"g\" eeimg=\"1\"/> 是可逆的、连续可微的函数，那么：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-409bd0131cb6e219ed90d074cac38163_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"228\" data-rawheight=\"38\" class=\"content_image\" width=\"228\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;228&#39; height=&#39;38&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"228\" data-rawheight=\"38\" class=\"content_image lazy\" width=\"228\" data-actualsrc=\"https://pic4.zhimg.com/v2-409bd0131cb6e219ed90d074cac38163_b.jpg\"/></figure><p>实际上这个结论是错误的，因为引入了 <img src=\"https://www.zhihu.com/equation?tex=g\" alt=\"g\" eeimg=\"1\"/> 空间会有变形。正确的计算如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-6daa09e686aa9c220d034c77abae8536_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"318\" data-rawheight=\"54\" class=\"content_image\" width=\"318\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;318&#39; height=&#39;54&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"318\" data-rawheight=\"54\" class=\"content_image lazy\" width=\"318\" data-actualsrc=\"https://pic3.zhimg.com/v2-6daa09e686aa9c220d034c77abae8536_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-0c74c9dfe537ce38559bf1de6b1bcf54_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"328\" data-rawheight=\"92\" class=\"content_image\" width=\"328\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;328&#39; height=&#39;92&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"328\" data-rawheight=\"92\" class=\"content_image lazy\" width=\"328\" data-actualsrc=\"https://pic1.zhimg.com/v2-0c74c9dfe537ce38559bf1de6b1bcf54_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-34666d4309b418c712124633aa9689aa_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"308\" data-rawheight=\"80\" class=\"content_image\" width=\"308\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;308&#39; height=&#39;80&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"308\" data-rawheight=\"80\" class=\"content_image lazy\" width=\"308\" data-actualsrc=\"https://pic3.zhimg.com/v2-34666d4309b418c712124633aa9689aa_b.jpg\"/></figure><h2>3.13 信息论</h2><p><b>信息论</b>的基本思想是概率小的事情发生比概率大的事情发生提供了更多的信息。比如，”今天早上太阳从东边升起“这件事几乎是1，并没有提供过多信息。而”今天早上有日食“提供了很多信息。</p><blockquote>自信息</blockquote><p>因此定义事件x = <img src=\"https://www.zhihu.com/equation?tex=x\" alt=\"x\" eeimg=\"1\"/> 的<b>自信息</b>为：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-25813ffc8b42e0f867679ed99d2f4453_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"230\" data-rawheight=\"52\" class=\"content_image\" width=\"230\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;230&#39; height=&#39;52&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"230\" data-rawheight=\"52\" class=\"content_image lazy\" width=\"230\" data-actualsrc=\"https://pic4.zhimg.com/v2-25813ffc8b42e0f867679ed99d2f4453_b.jpg\"/></figure><blockquote>香农熵和自信息</blockquote><p>自信息只描述了单个事件的信息量，对于某个随机变量，定义<b>香农熵</b>：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-8ea545f76ec676620946148f71aaf264_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"484\" data-rawheight=\"48\" class=\"origin_image zh-lightbox-thumb\" width=\"484\" data-original=\"https://pic1.zhimg.com/v2-8ea545f76ec676620946148f71aaf264_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;484&#39; height=&#39;48&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"484\" data-rawheight=\"48\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"484\" data-original=\"https://pic1.zhimg.com/v2-8ea545f76ec676620946148f71aaf264_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-8ea545f76ec676620946148f71aaf264_b.jpg\"/></figure><p>事实上<b>香农熵</b>是随机变量信息量的期望值。计算时定义 <img src=\"https://www.zhihu.com/equation?tex=0log0+%3D+0\" alt=\"0log0 = 0\" eeimg=\"1\"/> 。</p><blockquote>KL散度</blockquote><p>我们可以使用<b>KL散度</b>衡量两个分布的差异：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-9ccef0868fb3ad1a60cd6f55f3fd93a2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"726\" data-rawheight=\"94\" class=\"origin_image zh-lightbox-thumb\" width=\"726\" data-original=\"https://pic3.zhimg.com/v2-9ccef0868fb3ad1a60cd6f55f3fd93a2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;726&#39; height=&#39;94&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"726\" data-rawheight=\"94\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"726\" data-original=\"https://pic3.zhimg.com/v2-9ccef0868fb3ad1a60cd6f55f3fd93a2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-9ccef0868fb3ad1a60cd6f55f3fd93a2_b.jpg\"/></figure><p>可以证明KL散度的值是大于等于0的。</p><blockquote>交叉熵</blockquote><p><b>交叉熵</b>也用于衡量两个分布的差异，与KL散度略有不同：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-f8c7f93566e23b3f6a8a32bf017fa4c7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"360\" data-rawheight=\"30\" class=\"content_image\" width=\"360\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;360&#39; height=&#39;30&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"360\" data-rawheight=\"30\" class=\"content_image lazy\" width=\"360\" data-actualsrc=\"https://pic4.zhimg.com/v2-f8c7f93566e23b3f6a8a32bf017fa4c7_b.jpg\"/></figure><p>结合KL散度的公式，可以得到：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-94bf959dc61ba22dc72b280c434dafcb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"362\" data-rawheight=\"58\" class=\"content_image\" width=\"362\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;362&#39; height=&#39;58&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"362\" data-rawheight=\"58\" class=\"content_image lazy\" width=\"362\" data-actualsrc=\"https://pic4.zhimg.com/v2-94bf959dc61ba22dc72b280c434dafcb_b.jpg\"/></figure><h2>3.14 结构化概率模型</h2><p>在机器学习中可能会涉及到非常多的随机变量，但是这些随机变量的相互作用都是介于非常少的变量之间的，因此用单个函数描述整个联合变量分布是不合适的，所以我们会将其拆解成用图表示的分解结构，称为<b>结构化概率模型</b>或者<b>图模型</b>。</p><p>图模型也分为<b>有向图模型</b>和<b>无向图模型</b>。</p><blockquote>有向图模型</blockquote><p><b>有向图模型</b>的公式为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-f7a81359c97fcd917b7c1ba492bd7f26_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"326\" data-rawheight=\"74\" class=\"content_image\" width=\"326\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;326&#39; height=&#39;74&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"326\" data-rawheight=\"74\" class=\"content_image lazy\" width=\"326\" data-actualsrc=\"https://pic3.zhimg.com/v2-f7a81359c97fcd917b7c1ba492bd7f26_b.jpg\"/></figure><p><img src=\"https://www.zhihu.com/equation?tex=P_ag\" alt=\"P_ag\" eeimg=\"1\"/> 函数表示 <img src=\"https://www.zhihu.com/equation?tex=x_i\" alt=\"x_i\" eeimg=\"1\"/> 的父节点。</p><p>例如，对于下图：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-773d6d7635385c542abab8befa4df291_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"310\" data-rawheight=\"324\" class=\"content_image\" width=\"310\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;310&#39; height=&#39;324&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"310\" data-rawheight=\"324\" class=\"content_image lazy\" width=\"310\" data-actualsrc=\"https://pic2.zhimg.com/v2-773d6d7635385c542abab8befa4df291_b.jpg\"/></figure><p>可以表示为：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-a2890ef2a321771c15b4e34cb84466d4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"554\" data-rawheight=\"42\" class=\"origin_image zh-lightbox-thumb\" width=\"554\" data-original=\"https://pic1.zhimg.com/v2-a2890ef2a321771c15b4e34cb84466d4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;554&#39; height=&#39;42&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"554\" data-rawheight=\"42\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"554\" data-original=\"https://pic1.zhimg.com/v2-a2890ef2a321771c15b4e34cb84466d4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-a2890ef2a321771c15b4e34cb84466d4_b.jpg\"/></figure><blockquote>无向图模型</blockquote><p><b>无向图</b>模型中把图中任何满足两两之间有边连接的顶点的集合成为<b>团</b>，记作 <img src=\"https://www.zhihu.com/equation?tex=C\" alt=\"C\" eeimg=\"1\"/> ，每个团都对应一个因子 <img src=\"https://www.zhihu.com/equation?tex=%5Cphi\" alt=\"\\phi\" eeimg=\"1\"/> ，公式记为：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-c262ed6ba09d506467f4cbb4bfc6598e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"318\" data-rawheight=\"78\" class=\"content_image\" width=\"318\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;318&#39; height=&#39;78&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"318\" data-rawheight=\"78\" class=\"content_image lazy\" width=\"318\" data-actualsrc=\"https://pic3.zhimg.com/v2-c262ed6ba09d506467f4cbb4bfc6598e_b.jpg\"/></figure><p>其中 <img src=\"https://www.zhihu.com/equation?tex=Z\" alt=\"Z\" eeimg=\"1\"/> 表示归一化常数。</p><p>例如，对于下图：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-e26b5863ce3a0f6791289d6b82d71224_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"300\" data-rawheight=\"322\" class=\"content_image\" width=\"300\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;300&#39; height=&#39;322&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"300\" data-rawheight=\"322\" class=\"content_image lazy\" width=\"300\" data-actualsrc=\"https://pic1.zhimg.com/v2-e26b5863ce3a0f6791289d6b82d71224_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>可以表示为：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-4ac8b43cbb59b8981fce13fcec52ef07_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"552\" data-rawheight=\"68\" class=\"origin_image zh-lightbox-thumb\" width=\"552\" data-original=\"https://pic4.zhimg.com/v2-4ac8b43cbb59b8981fce13fcec52ef07_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;552&#39; height=&#39;68&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"552\" data-rawheight=\"68\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"552\" data-original=\"https://pic4.zhimg.com/v2-4ac8b43cbb59b8981fce13fcec52ef07_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-4ac8b43cbb59b8981fce13fcec52ef07_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "读书笔记", 
                    "tagLink": "https://api.zhihu.com/topics/19590861"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/36406690", 
            "userName": "Octocat", 
            "userLink": "https://www.zhihu.com/people/db5a40777722f345e64b9c173860d605", 
            "upvote": 2, 
            "title": "《深度学习》读书笔记2——线性代数", 
            "content": "<h2>第二章 线性代数</h2><p>本章主要讲述<b>线性代数</b>中较为基础的知识，对于基本概念不做过多记录。</p><h2>2.1 标量、向量、矩阵和张量</h2><blockquote>标量 vs 向量 vs 矩阵 vs 张量</blockquote><ul><li><b>标量(scalar) </b>—— 单个数</li><li><b>向量(vector) </b>—— 一维数组</li><li><b>矩阵(metrix)</b> —— 二维数组</li><li><b>张量(tensor)</b> —— 矩阵的拓展，N维数组</li></ul><blockquote>代数运算时的广播(broadcasting)</blockquote><p>通常情况下，矩阵加法只能在同阶之间进行，也就是说矩阵是无法和向量相加的。但是在深度学习里，为了应用方便我们经常允许矩阵和向量相加。例如，(3, 4)阶的矩阵可以和(3, 1)阶的向量相加，其运算过程是该列向量和矩阵中的每一列相加。</p><p>以上仅是<b>广播</b>是一种形式，<code>python</code>的<code>numpy</code>库里有其对<a href=\"https://link.zhihu.com/?target=https%3A//docs.scipy.org/doc/numpy/user/basics.broadcasting.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">广播实现</a>的详细说明。 </p><h2>2.2 矩阵和向量相乘</h2><ul><li><b>矩阵乘法</b>和<b>元素对应乘积(Hadamard乘积)</b>的区别</li><li>向量<b>点积</b>和矩阵乘法的转换</li><li>矩阵乘法满足乘法结合律和分配律，不满足交换律</li><li>矩阵<b>转置</b>的运算性质</li></ul><h2>2.3 单位矩阵和逆矩阵</h2><ul><li><b>单位矩阵</b>和<b>逆矩阵</b>的定义</li><li>求解线性方程组</li></ul><blockquote>线性方程组与矩阵运算</blockquote><p>对于线性方程组：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-1cea8dd1d5a69f5f4084c26676c9ee06_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"462\" data-rawheight=\"42\" class=\"origin_image zh-lightbox-thumb\" width=\"462\" data-original=\"https://pic3.zhimg.com/v2-1cea8dd1d5a69f5f4084c26676c9ee06_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;462&#39; height=&#39;42&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"462\" data-rawheight=\"42\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"462\" data-original=\"https://pic3.zhimg.com/v2-1cea8dd1d5a69f5f4084c26676c9ee06_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-1cea8dd1d5a69f5f4084c26676c9ee06_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-22680d613b686d270a69ef6f84bfc035_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"494\" data-rawheight=\"146\" class=\"origin_image zh-lightbox-thumb\" width=\"494\" data-original=\"https://pic2.zhimg.com/v2-22680d613b686d270a69ef6f84bfc035_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;494&#39; height=&#39;146&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"494\" data-rawheight=\"146\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"494\" data-original=\"https://pic2.zhimg.com/v2-22680d613b686d270a69ef6f84bfc035_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-22680d613b686d270a69ef6f84bfc035_b.jpg\"/></figure><p>可以表示为矩阵乘法的形式并进行求解：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-63deedbc63c6f9ec72a24dc7cd61e8a0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"310\" data-rawheight=\"140\" class=\"content_image\" width=\"310\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;310&#39; height=&#39;140&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"310\" data-rawheight=\"140\" class=\"content_image lazy\" width=\"310\" data-actualsrc=\"https://pic1.zhimg.com/v2-63deedbc63c6f9ec72a24dc7cd61e8a0_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-ea87d9afd9cc77208a42492f336a346a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"202\" data-rawheight=\"58\" class=\"content_image\" width=\"202\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;202&#39; height=&#39;58&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"202\" data-rawheight=\"58\" class=\"content_image lazy\" width=\"202\" data-actualsrc=\"https://pic3.zhimg.com/v2-ea87d9afd9cc77208a42492f336a346a_b.jpg\"/></figure><h2>2.4 线性相关和生成子空间</h2><blockquote>线性方程组解的个数</blockquote><p>对于上述线性方程组，其解的个数只有三种情况：<b>无解</b>、<b>一个解</b>和<b>无穷多个解</b>。</p><p><b>一个解</b>的情况2.3已经说明，就是当且仅当矩阵A存在逆矩阵的时候。</p><p>当矩阵A不存在逆矩阵的时候，其解只能是<b>零个</b>或者<b>无穷多个</b>，可以证明不存在有穷个大于1个解的情况：</p><p>假如存在两个解x和y，那么必然存在z也是该线性方程组的解：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-681c9ee5af935bc9de42301e09f867f9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"268\" data-rawheight=\"46\" class=\"content_image\" width=\"268\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;268&#39; height=&#39;46&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"268\" data-rawheight=\"46\" class=\"content_image lazy\" width=\"268\" data-actualsrc=\"https://pic2.zhimg.com/v2-681c9ee5af935bc9de42301e09f867f9_b.jpg\"/></figure><p>其中α的取值可以是任意实数，所以z有无穷多个。因此只要线性方程组存在两个解，就可以推出其存在无穷多个解。</p><blockquote>线性组合和生成子空间</blockquote><p>对于拆解后的线性方程组，其实等价于矩阵A的每个列向量乘以某个常数，然后进行求和：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-06d1b4d6716d04492568ed884e645cb3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"276\" data-rawheight=\"104\" class=\"content_image\" width=\"276\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;276&#39; height=&#39;104&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"276\" data-rawheight=\"104\" class=\"content_image lazy\" width=\"276\" data-actualsrc=\"https://pic4.zhimg.com/v2-06d1b4d6716d04492568ed884e645cb3_b.jpg\"/></figure><p>这样的求和称为<b>线性组合。</b>在N维空间里，每个向量可以看作是从原点到某个特定点的有向线段，所以一组向量的线性组合就是若干个方向上的有向线段首尾相连进行向量加法，进而到达某个点，所有这些能够到达的点称为这组向量的<b>生成子空间</b>。</p><p>在特定问题线性方程组里，对于</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-6187019571c0f1978dd767b2358a6571_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"106\" data-rawheight=\"40\" class=\"content_image\" width=\"106\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;106&#39; height=&#39;40&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"106\" data-rawheight=\"40\" class=\"content_image lazy\" width=\"106\" data-actualsrc=\"https://pic2.zhimg.com/v2-6187019571c0f1978dd767b2358a6571_b.jpg\"/></figure><p>我们可以通过判断b是否在A列向量的生成子空间里，来判断是否存在解x。在这个问题里，把A列向量的生成子空间称为A的<b>列空间</b>或<b>值域</b>。</p><blockquote>线性相关与线性无关</blockquote><p>对于一组向量，如果存在一个向量是其他向量的线性组合，那么称这组向量<b>线性相关</b>，反之为<b>线性无关</b>。</p><p>而对于一组m阶的向量，如果想让其生成子空间充满整个m阶空间，就需要这组向量的个数大于等于m，且至少包含m个线性无关的向量组。也就说，<b>m个线性无关的m阶向量刚好可以填充整个m阶空间</b>。</p><blockquote>矩阵A的性质与Ax=b解个数的具体关系</blockquote><ul><li>无解的情况：b不在A的列空间里。</li><li>有解的情况：b在A的列空间里。</li><ul><li>一个解的情况：A的列向量线性无关。</li><li>无穷个解的情况：A的列向量线性相关。</li></ul></ul><p>所以当A是m阶方阵，且列向量线性无关时，对于任意的b，均只存在一个解x。</p><blockquote>矩阵的奇异性和可逆性</blockquote><p>一个列向量线性相关的方针称为<b>奇异方阵</b>。</p><hr/><ul><li><b>非奇异方阵就是可逆方阵，奇异方阵就是不可逆方阵。</b></li><li><b>非奇异方阵就是可逆方阵，奇异方阵就是不可逆方阵。</b></li><li><b>非奇异方阵就是可逆方阵，奇异方阵就是不可逆方阵。</b></li></ul><hr/><h2>2.5 范数</h2><blockquote>衡量向量的大小</blockquote><p><b>范数</b>用于衡量向量的大小，其定义是：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-dca0bf6dbc3f675ae669c53f2accdeeb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"310\" data-rawheight=\"146\" class=\"content_image\" width=\"310\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;310&#39; height=&#39;146&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"310\" data-rawheight=\"146\" class=\"content_image lazy\" width=\"310\" data-actualsrc=\"https://pic4.zhimg.com/v2-dca0bf6dbc3f675ae669c53f2accdeeb_b.jpg\"/></figure><p>p=1时，叫做L1范数；p=2时，叫做L2范数（也叫<b>欧几里得范数</b>）；以此类推。</p><p>范数的性质：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-4b0867c165dc61dcfd69dd13019a864b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"668\" data-rawheight=\"164\" class=\"origin_image zh-lightbox-thumb\" width=\"668\" data-original=\"https://pic4.zhimg.com/v2-4b0867c165dc61dcfd69dd13019a864b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;668&#39; height=&#39;164&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"668\" data-rawheight=\"164\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"668\" data-original=\"https://pic4.zhimg.com/v2-4b0867c165dc61dcfd69dd13019a864b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-4b0867c165dc61dcfd69dd13019a864b_b.jpg\"/></figure><p>另外，还存在<b>最大范数</b>，定义为：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-9fe9f9447d754cbebe2b980e0a4849e8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"268\" data-rawheight=\"78\" class=\"content_image\" width=\"268\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;268&#39; height=&#39;78&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"268\" data-rawheight=\"78\" class=\"content_image lazy\" width=\"268\" data-actualsrc=\"https://pic1.zhimg.com/v2-9fe9f9447d754cbebe2b980e0a4849e8_b.jpg\"/></figure><p>向量的点积也可以用范数来表示：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-293723a27b7ac0573ad537529f24f732_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"334\" data-rawheight=\"86\" class=\"content_image\" width=\"334\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;334&#39; height=&#39;86&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"334\" data-rawheight=\"86\" class=\"content_image lazy\" width=\"334\" data-actualsrc=\"https://pic3.zhimg.com/v2-293723a27b7ac0573ad537529f24f732_b.jpg\"/></figure><blockquote>衡量矩阵的大小</blockquote><p>类似于向量的L2范数，也有一种衡量矩阵的<b>Frobenius范数</b>：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-45913b0439309cf774389ff3fb501e12_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"270\" data-rawheight=\"92\" class=\"content_image\" width=\"270\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;270&#39; height=&#39;92&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"270\" data-rawheight=\"92\" class=\"content_image lazy\" width=\"270\" data-actualsrc=\"https://pic3.zhimg.com/v2-45913b0439309cf774389ff3fb501e12_b.jpg\"/></figure><h2>2.6 特殊类型的矩阵和向量</h2><ul><li><b>对角矩阵</b>：对角线外的元素均为0的矩阵</li><li><b>对称矩阵</b>：转置等于自身的矩阵</li><li><b>单位向量</b>：L2范数等于1的矩阵</li><li><b>正交矩阵</b>：</li></ul><p>如果向量x、y满足：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-34ac24911c5c503bd7f6e8a3270f299a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"102\" data-rawheight=\"36\" class=\"content_image\" width=\"102\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;102&#39; height=&#39;36&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"102\" data-rawheight=\"36\" class=\"content_image lazy\" width=\"102\" data-actualsrc=\"https://pic3.zhimg.com/v2-34ac24911c5c503bd7f6e8a3270f299a_b.jpg\"/></figure><p>那么称x、y<b>正交</b>。如果x、y均为单位向量，称x、y<b>标准正交</b>。对于n维空间，最多有n个非零向量正交。</p><p><b>正交矩阵</b>是行向量和列向量均标准正交的方阵，满足：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-13e90f6c663f65d9b4ecc9c4bd41d701_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"220\" data-rawheight=\"34\" class=\"content_image\" width=\"220\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;220&#39; height=&#39;34&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"220\" data-rawheight=\"34\" class=\"content_image lazy\" width=\"220\" data-actualsrc=\"https://pic2.zhimg.com/v2-13e90f6c663f65d9b4ecc9c4bd41d701_b.jpg\"/></figure><p>也就是说：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-8e713ab39f7181642cd37dd73eb04a4f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"124\" data-rawheight=\"32\" class=\"content_image\" width=\"124\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;124&#39; height=&#39;32&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"124\" data-rawheight=\"32\" class=\"content_image lazy\" width=\"124\" data-actualsrc=\"https://pic4.zhimg.com/v2-8e713ab39f7181642cd37dd73eb04a4f_b.jpg\"/></figure><h2>2.7 特征分解</h2><p>许多数学对象可以通过分解成多个组成部分来更好地理解。例如整数可以分解为多个质因数。</p><p>矩阵里常用的分解方式是<b>特征分解</b>，矩阵可以分解为一组<b>特征向量</b>和<b>特征值</b>。</p><blockquote>特征向量和特征值</blockquote><p>矩阵A的<b>特征向量</b>指的是与A相乘相当于对该向量进行缩放的非零向量 <img src=\"https://www.zhihu.com/equation?tex=v\" alt=\"v\" eeimg=\"1\"/> ：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-0aeea195d1836819c8363a971646f773_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"238\" data-rawheight=\"62\" class=\"content_image\" width=\"238\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;238&#39; height=&#39;62&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"238\" data-rawheight=\"62\" class=\"content_image lazy\" width=\"238\" data-actualsrc=\"https://pic4.zhimg.com/v2-0aeea195d1836819c8363a971646f773_b.jpg\"/></figure><p>其中 <img src=\"https://www.zhihu.com/equation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"/> 叫做 <img src=\"https://www.zhihu.com/equation?tex=v\" alt=\"v\" eeimg=\"1\"/> 的<b>特征值</b>。</p><blockquote>特征分解</blockquote><p>对于矩阵A，其<b>特征分解</b>可以记作：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-9330d262b8bd96b08e58c862f03c7dff_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"260\" data-rawheight=\"54\" class=\"content_image\" width=\"260\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;260&#39; height=&#39;54&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"260\" data-rawheight=\"54\" class=\"content_image lazy\" width=\"260\" data-actualsrc=\"https://pic4.zhimg.com/v2-9330d262b8bd96b08e58c862f03c7dff_b.jpg\"/></figure><p>其中矩阵 <img src=\"https://www.zhihu.com/equation?tex=V\" alt=\"V\" eeimg=\"1\"/> 的列向量是矩阵<img src=\"https://www.zhihu.com/equation?tex=A\" alt=\"A\" eeimg=\"1\"/> 的一组<b>特征向量</b>。 <img src=\"https://www.zhihu.com/equation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"/> 是矩阵A的一组<b>特征值</b>。</p><p>每个实对称矩阵都可以分解为实特征向量和实特征值：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-1d4d87ea8e30efcb6d3abbb704a66639_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"138\" data-rawheight=\"34\" class=\"content_image\" width=\"138\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;138&#39; height=&#39;34&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"138\" data-rawheight=\"34\" class=\"content_image lazy\" width=\"138\" data-actualsrc=\"https://pic2.zhimg.com/v2-1d4d87ea8e30efcb6d3abbb704a66639_b.jpg\"/></figure><p>其中矩阵 <img src=\"https://www.zhihu.com/equation?tex=Q\" alt=\"Q\" eeimg=\"1\"/> 的含义等同于上述的矩阵 <img src=\"https://www.zhihu.com/equation?tex=V\" alt=\"V\" eeimg=\"1\"/> ，但是矩阵 <img src=\"https://www.zhihu.com/equation?tex=Q\" alt=\"Q\" eeimg=\"1\"/> 是正交矩阵，因此 <img src=\"https://www.zhihu.com/equation?tex=Q%5E%7B-1%7D+%3D+Q%5ET\" alt=\"Q^{-1} = Q^T\" eeimg=\"1\"/> 。</p><p>对于矩阵乘法的几何意义，我们可以结合特征分解去理解：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-8792f4e1aa4d69f50ae0d52479d06282_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"930\" data-rawheight=\"476\" class=\"origin_image zh-lightbox-thumb\" width=\"930\" data-original=\"https://pic3.zhimg.com/v2-8792f4e1aa4d69f50ae0d52479d06282_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;930&#39; height=&#39;476&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"930\" data-rawheight=\"476\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"930\" data-original=\"https://pic3.zhimg.com/v2-8792f4e1aa4d69f50ae0d52479d06282_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-8792f4e1aa4d69f50ae0d52479d06282_b.jpg\"/></figure><p>矩阵的特征分解结果不唯一，但是我们可以通过以下规定保证特征分解的唯一性：</p><ol><li><b>特征向量</b>构成的矩阵是正定矩阵</li><li>按照降序排列<b>特征值</b></li></ol><blockquote>特征分解与二元方程</blockquote><p>对于函数：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-f68430e3864ed23cca0199f68662e021_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"164\" data-rawheight=\"34\" class=\"content_image\" width=\"164\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;164&#39; height=&#39;34&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"164\" data-rawheight=\"34\" class=\"content_image lazy\" width=\"164\" data-actualsrc=\"https://pic2.zhimg.com/v2-f68430e3864ed23cca0199f68662e021_b.jpg\"/></figure><p>如果 <img src=\"https://www.zhihu.com/equation?tex=x\" alt=\"x\" eeimg=\"1\"/> 是矩阵 <img src=\"https://www.zhihu.com/equation?tex=A\" alt=\"A\" eeimg=\"1\"/> 的特征向量，且规定：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-ee1e375f3f56b32014683333e877e2a6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"112\" data-rawheight=\"36\" class=\"content_image\" width=\"112\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;112&#39; height=&#39;36&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"112\" data-rawheight=\"36\" class=\"content_image lazy\" width=\"112\" data-actualsrc=\"https://pic3.zhimg.com/v2-ee1e375f3f56b32014683333e877e2a6_b.jpg\"/></figure><p>那么 <img src=\"https://www.zhihu.com/equation?tex=f%28x%29\" alt=\"f(x)\" eeimg=\"1\"/> 的结果就是 <img src=\"https://www.zhihu.com/equation?tex=x\" alt=\"x\" eeimg=\"1\"/> 对应的特征值。</p><blockquote>正定矩阵</blockquote><p>根据特征值的符号，我们定义以下概念：</p><ul><li><b>正定矩阵</b>：特征值均大于零</li><li><b>半正定矩阵</b>：特征值均大于等于零</li><li><b>负定矩阵</b>：特征值均小于零</li><li><b>半负定矩阵</b>：特征值均小于等于零</li></ul><h2>2.8 奇异值分解</h2><p><b>奇异值分解（SVD）</b>是矩阵的另一种分解方式。</p><p>奇异值分解的公式为三个矩阵的乘积：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-bd98d883139ce5cf4be76a194d87448b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"152\" data-rawheight=\"44\" class=\"content_image\" width=\"152\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;152&#39; height=&#39;44&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"152\" data-rawheight=\"44\" class=\"content_image lazy\" width=\"152\" data-actualsrc=\"https://pic4.zhimg.com/v2-bd98d883139ce5cf4be76a194d87448b_b.jpg\"/></figure><p>其中 <img src=\"https://www.zhihu.com/equation?tex=U\" alt=\"U\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=V\" alt=\"V\" eeimg=\"1\"/> 的列向量是矩阵 <img src=\"https://www.zhihu.com/equation?tex=A\" alt=\"A\" eeimg=\"1\"/> 的<b>左奇异向量</b>和<b>右奇异向量</b>， <img src=\"https://www.zhihu.com/equation?tex=D\" alt=\"D\" eeimg=\"1\"/> 称为矩阵 <img src=\"https://www.zhihu.com/equation?tex=A\" alt=\"A\" eeimg=\"1\"/> 的<b>奇异值</b>。</p><p>可以用特征值和特征向量去解释SVD：</p><ul><li><img src=\"https://www.zhihu.com/equation?tex=AA%5ET\" alt=\"AA^T\" eeimg=\"1\"/> 的特征向量是 <img src=\"https://www.zhihu.com/equation?tex=A\" alt=\"A\" eeimg=\"1\"/> 的左奇异向量</li><li><img src=\"https://www.zhihu.com/equation?tex=A%5ETA\" alt=\"A^TA\" eeimg=\"1\"/> 的特征向量是 <img src=\"https://www.zhihu.com/equation?tex=A\" alt=\"A\" eeimg=\"1\"/> 的右奇异向量</li><li><img src=\"https://www.zhihu.com/equation?tex=A\" alt=\"A\" eeimg=\"1\"/> 的非零奇异值是 <img src=\"https://www.zhihu.com/equation?tex=AA%5ET\" alt=\"AA^T\" eeimg=\"1\"/> （也是 <img src=\"https://www.zhihu.com/equation?tex=A%5ETA\" alt=\"A^TA\" eeimg=\"1\"/> ）特征值的平方根</li></ul><h2>2.9 Moore-Penrose 伪逆</h2><p>前面在求解线性方程组 <img src=\"https://www.zhihu.com/equation?tex=Ax+%3D+y\" alt=\"Ax = y\" eeimg=\"1\"/> 的时候，通过左乘矩阵 <img src=\"https://www.zhihu.com/equation?tex=A\" alt=\"A\" eeimg=\"1\"/> 的左逆 <img src=\"https://www.zhihu.com/equation?tex=B\" alt=\"B\" eeimg=\"1\"/> ，可以得到 <img src=\"https://www.zhihu.com/equation?tex=x+%3D+By\" alt=\"x = By\" eeimg=\"1\"/> 。</p><p>对于 <img src=\"https://www.zhihu.com/equation?tex=A\" alt=\"A\" eeimg=\"1\"/> 没有左逆的情况，可以用Moore-Penrose 伪逆去解决，公式为：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-6e03aed6c90b46f16aa67796c9598c8d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"366\" data-rawheight=\"70\" class=\"content_image\" width=\"366\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;366&#39; height=&#39;70&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"366\" data-rawheight=\"70\" class=\"content_image lazy\" width=\"366\" data-actualsrc=\"https://pic2.zhimg.com/v2-6e03aed6c90b46f16aa67796c9598c8d_b.jpg\"/></figure><p>上述公式较为复杂，实际上会使用SVD去计算：<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-6bd3d24ead65566e9cd4ae17aa2dbd35_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"210\" data-rawheight=\"48\" class=\"content_image\" width=\"210\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;210&#39; height=&#39;48&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"210\" data-rawheight=\"48\" class=\"content_image lazy\" width=\"210\" data-actualsrc=\"https://pic2.zhimg.com/v2-6bd3d24ead65566e9cd4ae17aa2dbd35_b.jpg\"/></figure><h2>2.10 迹运算</h2><p><b>迹运算</b>是矩阵对角元素的和：<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-075261415d5dbf543865d530a8a9df27_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"234\" data-rawheight=\"80\" class=\"content_image\" width=\"234\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;234&#39; height=&#39;80&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"234\" data-rawheight=\"80\" class=\"content_image lazy\" width=\"234\" data-actualsrc=\"https://pic4.zhimg.com/v2-075261415d5dbf543865d530a8a9df27_b.jpg\"/></figure><p>迹运算提供Frobenius范数：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-c56f29b0a4d682ce0bea67204c6141fd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"274\" data-rawheight=\"78\" class=\"content_image\" width=\"274\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;274&#39; height=&#39;78&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"274\" data-rawheight=\"78\" class=\"content_image lazy\" width=\"274\" data-actualsrc=\"https://pic2.zhimg.com/v2-c56f29b0a4d682ce0bea67204c6141fd_b.jpg\"/></figure><p>迹运算的性质：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-c3de2bd6e2855afb2020286df67d019c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"242\" data-rawheight=\"56\" class=\"content_image\" width=\"242\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;242&#39; height=&#39;56&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"242\" data-rawheight=\"56\" class=\"content_image lazy\" width=\"242\" data-actualsrc=\"https://pic1.zhimg.com/v2-c3de2bd6e2855afb2020286df67d019c_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-5b9f80ea77be4dbe95cb72fda3191648_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"414\" data-rawheight=\"102\" class=\"content_image\" width=\"414\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;414&#39; height=&#39;102&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"414\" data-rawheight=\"102\" class=\"content_image lazy\" width=\"414\" data-actualsrc=\"https://pic1.zhimg.com/v2-5b9f80ea77be4dbe95cb72fda3191648_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-0c548b528fe6f1c1538bb361fb17f414_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"146\" data-rawheight=\"56\" class=\"content_image\" width=\"146\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;146&#39; height=&#39;56&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"146\" data-rawheight=\"56\" class=\"content_image lazy\" width=\"146\" data-actualsrc=\"https://pic1.zhimg.com/v2-0c548b528fe6f1c1538bb361fb17f414_b.jpg\"/></figure><h2>2.11 行列式</h2><p>行列式有多种计算方法，其中一种是<b>行列式等于矩阵特征值的乘积</b>。根据前面提到的矩阵特征分解的几何意义，可以得知矩阵的行列式可以衡量矩阵参与乘法后空间扩大或缩小了多少。</p><h2>2.12 实例：主成分分析</h2><p><b>主成分分析</b>是一个常见的机器学习算法。</p><p>对于 <img src=\"https://www.zhihu.com/equation?tex=R%5En\" alt=\"R^n\" eeimg=\"1\"/> 空间里 <img src=\"https://www.zhihu.com/equation?tex=m\" alt=\"m\" eeimg=\"1\"/> 个点，我们有时候希望对这些点进行有损压缩，压缩的方式是降维，同时压缩时也希望损失的精度尽量小。</p><p>压缩的过程实际上是编码，把向量的维度降低。<b>主成分分析</b>实际上是解码的过程，需要在低维向量里提取出尽量完整的高维向量信息。</p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "读书笔记", 
                    "tagLink": "https://api.zhihu.com/topics/19590861"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/36381274", 
            "userName": "Octocat", 
            "userLink": "https://www.zhihu.com/people/db5a40777722f345e64b9c173860d605", 
            "upvote": 7, 
            "title": "《深度学习》读书笔记1——引言", 
            "content": "<h2>第一章 引言</h2><blockquote>人工智能的两种思路——<b>规则模板</b>与<b>机器学习</b></blockquote><p><b>人工智能</b>从很久之前就被人们所研究，人们希望计算机能拥有“智力”，但计算机擅长的是大量重复的按照一定规则的运算，而非人们所擅长的识别说的话或图像中的脸。</p><p>起初，人们实现人工智能的方式是构建<b>知识库</b>，这个知识库包含了大量形式化的语言，用来让计算机根据逻辑推理规则来理解知识。但是，这样的规则定义完全人为输入，真实世界的知识很多且很复杂，用有限数量的推理语句无法很好让计算机达到智能。</p><p>用上述<b>硬编码</b>来让计算机构建知识体系的方法十分困难，因此有人希望计算机可以自己通过原始数据来提取特定的模式（也就是知识），这样的方式就是<b>机器学习。</b></p><blockquote>机器学习与深度学习</blockquote><p>在机器学习中，我们需要对数据的进行<b>特征提取</b>，这些特征才是数据真正的<b>表示</b>。比如，我们用逻辑回归判断产妇是否适合剖腹产，“产妇本身”无法作为数据的输入，我们需要提取其特征（产妇的年龄、产妇是否存在子宫疤痕）作为真实输入，这样就把“产妇”变成了一组可量化的数值。</p><p>然而，在真实世界里我们人类观察数据时往往并不会显式地进行特征提取，我们看到猫时会用自己天然的“第六感”感知到这是一只猫，而非进行一系列特征判断（是否活物、是否有胡子、是否抓老鼠）。所以我们在某些场景下需要设计一种方法自动地把特征提取出来，而非总是人为设计特征抽取规则，这种方法就是<b>表示学习</b>。</p><p><b>深度学习</b>相比于其他机器学习模型（如逻辑回归、支持向量机等），解决了表示学习的核心问题。</p><blockquote>深度学习是什么</blockquote><p>深度学习是机器学习中神经网络模型的现代化实践，这里的深度通常指的是神经网络的层数很多。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-8d1ae207c1b23044226a0fb85db2e128_b.jpg\" data-size=\"normal\" data-rawwidth=\"896\" data-rawheight=\"652\" class=\"origin_image zh-lightbox-thumb\" width=\"896\" data-original=\"https://pic1.zhimg.com/v2-8d1ae207c1b23044226a0fb85db2e128_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;896&#39; height=&#39;652&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"896\" data-rawheight=\"652\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"896\" data-original=\"https://pic1.zhimg.com/v2-8d1ae207c1b23044226a0fb85db2e128_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-8d1ae207c1b23044226a0fb85db2e128_b.jpg\"/><figcaption>多层感知机</figcaption></figure><p>上图即为一个较浅的神经网络。不同于传统机器学习模型，在深度学习里，特征通过一层层神经网络进行抽象。在输入层（可见层）里，我们将原始数据进行输入，经过一层层隐含层后，特征一步步被提取出来，特征提取的规则在隐含层里进行了学习，完成了<b>表示学习</b>的任务。</p><p>这样的模型，在复杂问题中帮我们减轻了特征提取的任务。</p><blockquote>概念包含关系：深度学习 &lt; 表示学习 &lt; 机器学习 &lt; 人工智能</blockquote><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-f83f5db6a436a69b6d76e15666623dfc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"866\" data-rawheight=\"770\" class=\"origin_image zh-lightbox-thumb\" width=\"866\" data-original=\"https://pic1.zhimg.com/v2-f83f5db6a436a69b6d76e15666623dfc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;866&#39; height=&#39;770&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"866\" data-rawheight=\"770\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"866\" data-original=\"https://pic1.zhimg.com/v2-f83f5db6a436a69b6d76e15666623dfc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-f83f5db6a436a69b6d76e15666623dfc_b.jpg\"/></figure><blockquote>不同AI方法的流程对比</blockquote><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-197a31cf6445eb384b2bb56069f71798_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"790\" data-rawheight=\"1022\" class=\"origin_image zh-lightbox-thumb\" width=\"790\" data-original=\"https://pic1.zhimg.com/v2-197a31cf6445eb384b2bb56069f71798_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;790&#39; height=&#39;1022&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"790\" data-rawheight=\"1022\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"790\" data-original=\"https://pic1.zhimg.com/v2-197a31cf6445eb384b2bb56069f71798_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-197a31cf6445eb384b2bb56069f71798_b.jpg\"/></figure><p>上述图对比了规则系统、传统机器学习、传统表示学习和深度学习流程图的差异。</p><h2>1.1 本书面向的读者</h2><p>本书的组织结构图：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-bf0ca65541fee76540a04d64ff6c7f43_b.jpg\" data-size=\"normal\" data-rawwidth=\"872\" data-rawheight=\"1224\" class=\"origin_image zh-lightbox-thumb\" width=\"872\" data-original=\"https://pic4.zhimg.com/v2-bf0ca65541fee76540a04d64ff6c7f43_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;872&#39; height=&#39;1224&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"872\" data-rawheight=\"1224\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"872\" data-original=\"https://pic4.zhimg.com/v2-bf0ca65541fee76540a04d64ff6c7f43_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-bf0ca65541fee76540a04d64ff6c7f43_b.jpg\"/><figcaption>《深度学习》组织结构图</figcaption></figure><h2>1.2 深度学习的历史趋势</h2><p>深度学习近些年特别火热，但神经网络的出现其实可以追溯到20世纪40年代，中间也经历过多次兴衰。如今神经网络的第三次兴起主要是由于以下几个原因：</p><ol><li><b>更强大的计算机</b></li><li><b>更大量的数据集</b></li><li><b>更优的训练技术</b></li></ol><p>在这样的条件背景下，层数更多的神经网络模型才能够真正应用到实际场景中，这就是我们如今看到的<b>深度学习。</b></p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "读书笔记", 
                    "tagLink": "https://api.zhihu.com/topics/19590861"
                }
            ], 
            "comments": [
                {
                    "userName": "「已注销」", 
                    "userLink": "https://www.zhihu.com/people/83214594bfd44cb3b3d120bedbf07fbb", 
                    "content": "啊哇⊙∀⊙！", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "忆臻", 
                    "userLink": "https://www.zhihu.com/people/1b72d70b702b3920638f0235d380ebd8", 
                    "content": "<p>赞，应该是必聪吧</p>", 
                    "likes": 1, 
                    "childComments": [
                        {
                            "userName": "Octocat", 
                            "userLink": "https://www.zhihu.com/people/db5a40777722f345e64b9c173860d605", 
                            "content": "嗯嗯，师兄好 😁", 
                            "likes": 1, 
                            "replyToAuthor": "忆臻"
                        }
                    ]
                }, 
                {
                    "userName": "loveissofar", 
                    "userLink": "https://www.zhihu.com/people/59e69e7de9fd68411600e45f012c3d10", 
                    "content": "赞赞赞聪神带我", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }
    ], 
    "url": "https://zhuanlan.zhihu.com/dlnlp"
}
