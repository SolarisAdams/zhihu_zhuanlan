{
    "title": "NLP问题生成论文笔记", 
    "description": "paper notes about question generation", 
    "followers": [
        "https://www.zhihu.com/people/xiao-mu-85-21", 
        "https://www.zhihu.com/people/dojw", 
        "https://www.zhihu.com/people/yin-su-xi", 
        "https://www.zhihu.com/people/jonxia", 
        "https://www.zhihu.com/people/xu-ye-75-17", 
        "https://www.zhihu.com/people/xiong-xin-lei-15", 
        "https://www.zhihu.com/people/lin-xi-64-46", 
        "https://www.zhihu.com/people/la-la-la-89-28", 
        "https://www.zhihu.com/people/stone-82-84", 
        "https://www.zhihu.com/people/yan-yan-qing-84-74", 
        "https://www.zhihu.com/people/innerpeace-24-25-99", 
        "https://www.zhihu.com/people/san-nian-15-69-28", 
        "https://www.zhihu.com/people/zhong-hao-26-43", 
        "https://www.zhihu.com/people/yu-hou-shi-cha-hai", 
        "https://www.zhihu.com/people/yijiang-48-91", 
        "https://www.zhihu.com/people/sqchi1991", 
        "https://www.zhihu.com/people/mr-lin-82-68", 
        "https://www.zhihu.com/people/cheng-arenas", 
        "https://www.zhihu.com/people/kim-74-51", 
        "https://www.zhihu.com/people/shui-zhong-de-yu-35-79", 
        "https://www.zhihu.com/people/yi-xian-wei-50", 
        "https://www.zhihu.com/people/wang-wen-hao-33-75", 
        "https://www.zhihu.com/people/cyrusyang-81", 
        "https://www.zhihu.com/people/mohanweihan", 
        "https://www.zhihu.com/people/zzx-47-7", 
        "https://www.zhihu.com/people/ricahrd_cai", 
        "https://www.zhihu.com/people/masculn", 
        "https://www.zhihu.com/people/skycastle", 
        "https://www.zhihu.com/people/ma-yu-xiang-4", 
        "https://www.zhihu.com/people/hwl-9-46", 
        "https://www.zhihu.com/people/yuan-bai-bai-63", 
        "https://www.zhihu.com/people/qing-mu-10-60", 
        "https://www.zhihu.com/people/gupeng.nju", 
        "https://www.zhihu.com/people/lin-li-19-82", 
        "https://www.zhihu.com/people/xiao-tian-zi-33-7", 
        "https://www.zhihu.com/people/shan-hui-47", 
        "https://www.zhihu.com/people/zhou-xiao-jun-79-80", 
        "https://www.zhihu.com/people/Walker_Apple", 
        "https://www.zhihu.com/people/su-ge-la-di-de-di", 
        "https://www.zhihu.com/people/cherish-66-4", 
        "https://www.zhihu.com/people/hao-tian-87", 
        "https://www.zhihu.com/people/luo-wei-bin-35", 
        "https://www.zhihu.com/people/wu-xuan-sheng-69", 
        "https://www.zhihu.com/people/shi-peng-70-54", 
        "https://www.zhihu.com/people/ceng-guan-rong-72", 
        "https://www.zhihu.com/people/zhang-ai-hu", 
        "https://www.zhihu.com/people/hong-alex", 
        "https://www.zhihu.com/people/mo-yan-37-77-85", 
        "https://www.zhihu.com/people/xuwei0221", 
        "https://www.zhihu.com/people/ding-3shi", 
        "https://www.zhihu.com/people/vans-13-63", 
        "https://www.zhihu.com/people/michaelchen13", 
        "https://www.zhihu.com/people/lan-lan-lan-lan-96-82", 
        "https://www.zhihu.com/people/an-ran-57", 
        "https://www.zhihu.com/people/wang-jing-bo-27-88", 
        "https://www.zhihu.com/people/quxiaofeng", 
        "https://www.zhihu.com/people/medivhc", 
        "https://www.zhihu.com/people/cang-tian-lan-yao", 
        "https://www.zhihu.com/people/dgjk1010", 
        "https://www.zhihu.com/people/sun-yan-90-29", 
        "https://www.zhihu.com/people/lang-jie-yi-xi", 
        "https://www.zhihu.com/people/gongyunbo", 
        "https://www.zhihu.com/people/hum-75", 
        "https://www.zhihu.com/people/xingluxi", 
        "https://www.zhihu.com/people/eric-sun-6", 
        "https://www.zhihu.com/people/ren-wo-xing-59-44", 
        "https://www.zhihu.com/people/wu-xiao-bai-92", 
        "https://www.zhihu.com/people/ning-rain", 
        "https://www.zhihu.com/people/zhao-jun-83-21-53", 
        "https://www.zhihu.com/people/liu-jin-long-17-97"
    ], 
    "article": [
        {
            "url": "https://zhuanlan.zhihu.com/p/81100245", 
            "userName": "Chevalier", 
            "userLink": "https://www.zhihu.com/people/eba7ab78a0341c3ba393073fe68782d0", 
            "upvote": 14, 
            "title": "Kaggle 竞赛总结", 
            "content": "<h2>Jigsaw Unintended Bias in Toxicity Classification</h2><p>文本分类问题，写这篇文章的目的主要是想学习一下前排方案，记录一下自己认为的亮点部分。</p><ul><li><b><a href=\"https://link.zhihu.com/?target=https%3A//github.com/huggingface/pytorch-transformers/tree/master/examples/lm_finetuning\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Simple fine-tuning</a></b></li><li><b>Snapshot Ensemble：</b><a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1704.00109\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">论文</a>、<a href=\"https://link.zhihu.com/?target=https%3A//github.com/gaohuang/SnapshotEnsemble\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">代码</a>，原理主要是网络训练时，可能会收敛到不同的局部最优点，通过集成这些局部最优的模型进行预测。当时在参加某个比赛时也有类似的做法，就是保存最后几个epoch的模型权重，因为前几个epoch模型可能没学到什么东西，然后根据验证集上的分数作为权重，对测试集进行加权求和；</li><li><b>stochastic weighted average：</b>通过结合相同网络结构不同训练阶段的权重获得集成模型，然后进行预测，该方法优于Snapshot Ensemble。<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1803.05407\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">论文</a>、<a href=\"https://link.zhihu.com/?target=https%3A//github.com/timgaripov/swa\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">代码</a></li><li><b>取最后四层[CLS]代表的特征：</b>平均融合、加权融合、pooling</li><li><b>Multi-Sample Dropout：</b>提高模型的泛化能力</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-e94ce2f722587b8414f0d8189036a5e9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"794\" data-rawheight=\"321\" class=\"origin_image zh-lightbox-thumb\" width=\"794\" data-original=\"https://pic2.zhimg.com/v2-e94ce2f722587b8414f0d8189036a5e9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;794&#39; height=&#39;321&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"794\" data-rawheight=\"321\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"794\" data-original=\"https://pic2.zhimg.com/v2-e94ce2f722587b8414f0d8189036a5e9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-e94ce2f722587b8414f0d8189036a5e9_b.jpg\"/></figure><ul><li><b>Add LSTM char-level model</b></li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>BERT Finetune 相关论文：</b></p><p><a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1905.05583\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">How to Fine-Tune BERT for Text Classification?</a></p>", 
            "topic": [
                {
                    "tag": "自然语言处理", 
                    "tagLink": "https://api.zhihu.com/topics/19560026"
                }, 
                {
                    "tag": "Kaggle", 
                    "tagLink": "https://api.zhihu.com/topics/20003862"
                }, 
                {
                    "tag": "文本分类", 
                    "tagLink": "https://api.zhihu.com/topics/19576060"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/81126870", 
            "userName": "Chevalier", 
            "userLink": "https://www.zhihu.com/people/eba7ab78a0341c3ba393073fe68782d0", 
            "upvote": 6, 
            "title": "机器阅读理解综述（三）", 
            "content": "<h2>五、New Trends</h2><p><b>5.1 Knowledge-Based Machine Reading Comprehension </b></p><p>有时候，我们只根据context是无法回答问题的，需要借助外部知识。因此，基于外部知识的MRC应运而生。KBMRC和MRC的不同主要在输入部分，MRC的输入是context和question，而KBMRC的输入是context、question、knowledge。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-7639ff4b878691078a327a083329f728_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1224\" data-rawheight=\"349\" class=\"origin_image zh-lightbox-thumb\" width=\"1224\" data-original=\"https://pic1.zhimg.com/v2-7639ff4b878691078a327a083329f728_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1224&#39; height=&#39;349&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1224\" data-rawheight=\"349\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1224\" data-original=\"https://pic1.zhimg.com/v2-7639ff4b878691078a327a083329f728_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-7639ff4b878691078a327a083329f728_b.jpg\"/></figure><p>目前KBMRC的主要挑战在于：</p><ul><li>Relevant External Knowledge Retrieval</li><li>External Knowledge Integration</li></ul><p><b>5.2 Unanswerable Questions </b></p><p>有一个潜在的假设就是MRC任务中正确答案总是存在于给定的上下文中。显然这是不现实的，上下文覆盖的知识是有限的，存在一些问题是无法只根据上下文就可以回答的。因此，MRC系统应该区分这些无法回答的问题。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-1809b9f8f2d72626c2b98d0a5f0e68d6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1231\" data-rawheight=\"338\" class=\"origin_image zh-lightbox-thumb\" width=\"1231\" data-original=\"https://pic3.zhimg.com/v2-1809b9f8f2d72626c2b98d0a5f0e68d6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1231&#39; height=&#39;338&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1231\" data-rawheight=\"338\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1231\" data-original=\"https://pic3.zhimg.com/v2-1809b9f8f2d72626c2b98d0a5f0e68d6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-1809b9f8f2d72626c2b98d0a5f0e68d6_b.jpg\"/></figure><p>关于不可回答的问题，相比传统的MRC，在该新任务上又有新的挑战：</p><ul><li>Unanswerable Question Detection</li><li>Plausible Answer Discrimination </li></ul><p><b>5.3 Multi-Passage Machine Reading Comprehension </b></p><p>在MRC任务中，相关的段落是预定义好的，这与人类的问答流程矛盾。因为人们通常先提出一个问题，然后再去找所有相关的段落，最后在这些段落中找答案。因此研究学者提出了multi-passage machine reading comprehension，相关数据集有MS MARCO、TriviaQA、SearchQA、Dureader、QUASAR。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-440cd6d5068a467954c83340a209880b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1208\" data-rawheight=\"204\" class=\"origin_image zh-lightbox-thumb\" width=\"1208\" data-original=\"https://pic4.zhimg.com/v2-440cd6d5068a467954c83340a209880b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1208&#39; height=&#39;204&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1208\" data-rawheight=\"204\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1208\" data-original=\"https://pic4.zhimg.com/v2-440cd6d5068a467954c83340a209880b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-440cd6d5068a467954c83340a209880b_b.jpg\"/></figure><p>相比传统的MRC，MP MRC的挑战在于：</p><ul><li>Massive Document Corpus</li><li>Noisy Document Retrieval</li><li>No Answer</li><li>Multiple Answers </li><li>Evidence Aggregation </li></ul><p><b>5.4 Conversational Question Answering </b></p><p>MRC系统理解了给定段落的语义后回答问题，问题之间是相互独立的。然而，人们获取知识的最自然方式是通过一系列相互关联的问答过程。比如，给定一个问答，A提问题，B回复答案，然后A根据答案继续提问题。这个方式有点类似多轮对话。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-0f7d74919ef2a9dce1013ddbbc3e8965_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1225\" data-rawheight=\"211\" class=\"origin_image zh-lightbox-thumb\" width=\"1225\" data-original=\"https://pic2.zhimg.com/v2-0f7d74919ef2a9dce1013ddbbc3e8965_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1225&#39; height=&#39;211&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1225\" data-rawheight=\"211\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1225\" data-original=\"https://pic2.zhimg.com/v2-0f7d74919ef2a9dce1013ddbbc3e8965_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-0f7d74919ef2a9dce1013ddbbc3e8965_b.jpg\"/></figure><p>相关数据集：CoQA、QuAC</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-82993d4dd455512282e6576f41939b94_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"890\" data-rawheight=\"459\" class=\"origin_image zh-lightbox-thumb\" width=\"890\" data-original=\"https://pic1.zhimg.com/v2-82993d4dd455512282e6576f41939b94_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;890&#39; height=&#39;459&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"890\" data-rawheight=\"459\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"890\" data-original=\"https://pic1.zhimg.com/v2-82993d4dd455512282e6576f41939b94_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-82993d4dd455512282e6576f41939b94_b.jpg\"/></figure><p>与MRC相比，CQA带来了新的挑战：</p><ul><li>Conversational History</li><li>Coreference Resolution</li></ul><h2>6 Open Issues</h2><ul><li>Incorporation of External Knowledge</li><li>Robustness of MRC Systems </li><li>Limitation of Given Context</li><li>Lack of Inference Ability</li></ul>", 
            "topic": [
                {
                    "tag": "自然语言处理", 
                    "tagLink": "https://api.zhihu.com/topics/19560026"
                }, 
                {
                    "tag": "机器阅读", 
                    "tagLink": "https://api.zhihu.com/topics/20203649"
                }
            ], 
            "comments": [
                {
                    "userName": "Tim-s", 
                    "userLink": "https://www.zhihu.com/people/a510b88b686e58f37eba458f2421705f", 
                    "content": "老哥也太上进了[赞]", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "Chevalier", 
                            "userLink": "https://www.zhihu.com/people/eba7ab78a0341c3ba393073fe68782d0", 
                            "content": "<p>压力山大</p>", 
                            "likes": 0, 
                            "replyToAuthor": "Tim-s"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/80980403", 
            "userName": "Chevalier", 
            "userLink": "https://www.zhihu.com/people/eba7ab78a0341c3ba393073fe68782d0", 
            "upvote": 4, 
            "title": "机器阅读理解综述（二）", 
            "content": "<h2>四、Methods</h2><p>有各种各样的方法应用到MRC系统中，如下图所示，接下来会一一介绍。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-f3427b1b1a004102422218690b6fc35a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"605\" data-rawheight=\"797\" class=\"origin_image zh-lightbox-thumb\" width=\"605\" data-original=\"https://pic3.zhimg.com/v2-f3427b1b1a004102422218690b6fc35a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;605&#39; height=&#39;797&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"605\" data-rawheight=\"797\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"605\" data-original=\"https://pic3.zhimg.com/v2-f3427b1b1a004102422218690b6fc35a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-f3427b1b1a004102422218690b6fc35a_b.jpg\"/></figure><p><b>4.1 Embeddings </b></p><p>Embeddings模块将单词转换为对应的向量表示。如何充分编码context和question是本模块中的关键任务。在目前的MRC模型中，词表示方法可以分为传统的词表示和预训练上下文表示。为了编码更丰富的语义信息，MRC系统在原来的词级别表示的基础上，还会融合字向量、POS、NER、词频、问题类别等信息。</p><p>(1) Conventional Word Representation </p><ul><li>One-Hot：向量长度为词表大小，只有单词的位置为1，其它全0，这种表示方法无法表示两个单词之间的关系；</li><li>Distributed Word Representation：将单词编码为连续的低维向量，如word2vec、glove；</li></ul><p>(2) Pre-Trained Contextualized Word Representation </p><p>尽管分布式词表示可以在编码低维空间中编码单词，并且反映了不同单词之间的相关性，但是它们不能有效地挖掘上下文信息。具体来说，就是词的分布式表示在不同上下文中都是一个常量。为了解决这个问题，研究学者提出了上下文的词表示，在大规模数据集预训练，直接当做传统的词表示来使用或者在特定任务finetune。</p><ul><li>CoVE：利用大规模语料训练Seq2Seq模型，将Encoder的输出拿出来作为CoVE。</li><li>ELMo：在大规模文本语料预训练双向语言模型，特征抽取模块为LSTM。</li><li>GPT：采用单向的Transformer在大规模语料上预训练；</li><li>BERT：采用双向的Transformer在大规模语料上预训练，目标任务为masked language model (MLM)和next sentence prediction（NSP）。</li></ul><p>(3) Multiple Granularity </p><ul><li>Character Embeddings</li><li>Part-of-Speech Tags</li><li>Name-Entity Tags</li><li>Binary Feature of Exact Match (EM)：如果context中的一个单词在query中存在，那么值为1，否则为0；</li><li>Query-Category</li></ul><p><b>4.2 Feature Extraction </b></p><ul><li>Recurrent Neural Networks：LSTM、GRU</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-a9b2655f7e7c4889caa126f14d6e02a8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"598\" data-rawheight=\"370\" class=\"origin_image zh-lightbox-thumb\" width=\"598\" data-original=\"https://pic1.zhimg.com/v2-a9b2655f7e7c4889caa126f14d6e02a8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;598&#39; height=&#39;370&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"598\" data-rawheight=\"370\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"598\" data-original=\"https://pic1.zhimg.com/v2-a9b2655f7e7c4889caa126f14d6e02a8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-a9b2655f7e7c4889caa126f14d6e02a8_b.jpg\"/></figure><ul><li>Convolution Neural Networks</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-258109c3abbf740b65ff11a3d56ac18d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"755\" data-rawheight=\"589\" class=\"origin_image zh-lightbox-thumb\" width=\"755\" data-original=\"https://pic2.zhimg.com/v2-258109c3abbf740b65ff11a3d56ac18d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;755&#39; height=&#39;589&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"755\" data-rawheight=\"589\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"755\" data-original=\"https://pic2.zhimg.com/v2-258109c3abbf740b65ff11a3d56ac18d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-258109c3abbf740b65ff11a3d56ac18d_b.jpg\"/></figure><ul><li>Transformer</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-c51e818d6d8e98d5883fa3cf471d0ad9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"554\" data-rawheight=\"521\" class=\"origin_image zh-lightbox-thumb\" width=\"554\" data-original=\"https://pic2.zhimg.com/v2-c51e818d6d8e98d5883fa3cf471d0ad9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;554&#39; height=&#39;521&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"554\" data-rawheight=\"521\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"554\" data-original=\"https://pic2.zhimg.com/v2-c51e818d6d8e98d5883fa3cf471d0ad9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-c51e818d6d8e98d5883fa3cf471d0ad9_b.jpg\"/></figure><p><b>4.3 Context-Question Interaction </b></p><p>通过提取context和question之间的相关性，模型能够找到答案预测的证据。根据模型是如何抽取相关性的方式，目前的工作可以分为两类，一跳交互和多条交互。无论哪种交互方式，在MRC模型中，attention机制在强调context哪部分信息在回答问题方面更重要发挥着关键作用。在机器阅读理解中，attention机制可以分为无向和双向的。</p><p><b>(1a) Unidirectional Attention</b></p><p>单向的attention主要是根据问题关注context中最相关的部分。如果context中的单词与问题更相似，那么该单词更可能是答案。通过计算公式 <img src=\"https://www.zhihu.com/equation?tex=S_i%3Df%28P_i%2CQ%29\" alt=\"S_i=f(P_i,Q)\" eeimg=\"1\"/> 得到context中的单词与question的相似度，其中 <img src=\"https://www.zhihu.com/equation?tex=P_i\" alt=\"P_i\" eeimg=\"1\"/> 是context中的单词的embedding，Q是question的句子表示，最后通过softmax进行权重归一化，如下公式所示。</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Calpha_i%3D%5Cfrac%7BexpS_i%7D%7B%5Csum_%7Bj%7D%7BexpS_j%7D%7D\" alt=\"\\alpha_i=\\frac{expS_i}{\\sum_{j}{expS_j}}\" eeimg=\"1\"/> </p><p>其中 <img src=\"https://www.zhihu.com/equation?tex=S_i\" alt=\"S_i\" eeimg=\"1\"/> 的计算方式有很多种，如下所示。</p><p><img src=\"https://www.zhihu.com/equation?tex=S_i%3Dtanh%28W_PP_i%2BW_QQ%29\" alt=\"S_i=tanh(W_PP_i+W_QQ)\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=S_i%3DQ%5ETW_sP_i\" alt=\"S_i=Q^TW_sP_i\" eeimg=\"1\"/> </p><p>单向的attention可以关注context中最重要的词来回答问题，但是该方法无法关注对答案预测也至关重要的question的词。因此，单向的attention不足以抽取context和query之间的交互信息。</p><p><b>(1b) Bidirectional Attention</b></p><p>同时计算query-to-context attention和context-to-query attention。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-360629828100eb8cd2337bd86c05be93_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"444\" data-rawheight=\"565\" class=\"origin_image zh-lightbox-thumb\" width=\"444\" data-original=\"https://pic4.zhimg.com/v2-360629828100eb8cd2337bd86c05be93_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;444&#39; height=&#39;565&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"444\" data-rawheight=\"565\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"444\" data-original=\"https://pic4.zhimg.com/v2-360629828100eb8cd2337bd86c05be93_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-360629828100eb8cd2337bd86c05be93_b.jpg\"/></figure><p><b>(2) One-Hop Interaction &amp; Multi-Hop Interaction </b></p><p>One-Hop Interaction可能无法全面理解相互question-context信息，与此相反，Multi-Hop Interaction可以记住之前的context和question信息，能够深度提取相关性并聚合答案预测的证据。</p><p><b>4.4 Answer Prediction </b></p><p>该模块与任务高度相关，之前我们将MRC分为四类，分别是完形填空、多项选择、片段抽取、自由回答，那么对应的答案预测方法也有四种，分别是word predictor，option selector，span extractor，answer generator。</p><p><b>(1) Word Predictor</b></p><p>完形填空要求模型预测单词或实体进行填空，该单词或实体来自给定的context。这方面的工作有Attentive Reader、Attention Sum Reader</p><p><b>(2) Option Selector</b></p><p>对于多项选择任务，模型从候选答案列表中选择一个正确答案。很普遍的做法是衡量attentive context representations和候选答案表示之间的相似度，选择相似度最高的作为预测答案。</p><p><b>(3) Span Extractor </b></p><p>片段抽取任务是完形填空任务的扩展，要求模型从context中抽取一个子串，而不是一个单词。目前的工作有Sequence Model、Boundary Model。</p><p><b>(4) Answer Generator</b></p><p>自由回答任务中答案不再局限于context中的一个片段，而是需要根据context和question合成答案。目前的工作有S-Net。</p><p><b>4.5 Other Tricks</b></p><p>(1) Reinforcement Learning </p><p>(2) Answer Ranker </p><p>(3) Sentence Selector</p><p>实际上，如果给MRC模型一个很长的文档，那么理解全部上下文来回答问题是很费时的。但是，事先找到问题中最相关的句子是加速后续训练过程的一种可能方法。有研究学者提出了sentence selector来选择回答问题需要的最小句子集合。</p><p></p><p></p>", 
            "topic": [
                {
                    "tag": "自然语言处理", 
                    "tagLink": "https://api.zhihu.com/topics/19560026"
                }, 
                {
                    "tag": "机器阅读", 
                    "tagLink": "https://api.zhihu.com/topics/20203649"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/80905984", 
            "userName": "Chevalier", 
            "userLink": "https://www.zhihu.com/people/eba7ab78a0341c3ba393073fe68782d0", 
            "upvote": 6, 
            "title": "机器阅读理解综述（一）", 
            "content": "<h2>一、Introduction</h2><p>机器阅读理解（MRC）是一项任务，用于测试机器通过要求机器根据给定的上下文回答问题来理解自然语言的程度。早期的MRC系统是基于规则的，性能非常差。随着深度学习和大规模数据集的兴起，基于深度学习的MRC显著优于基于规则的MRC。常见的MRC任务可以分为四种类型：完形填空、多项选择、片段抽取、自由回答。一般的MRC架构由以下几个模块组成：Embedding、Feature Extraction、Context-Question Interaction、Answer Prediction。另外，考虑到目前方法的局限性，MRC出现了新的任务，比如，knowledge-based MRC, MRC with unanswerable questions, multi-passage MRC，conversational question answering。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-df27a586b962a28c9bd212cf8db14d52_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"563\" data-rawheight=\"426\" class=\"origin_image zh-lightbox-thumb\" width=\"563\" data-original=\"https://pic3.zhimg.com/v2-df27a586b962a28c9bd212cf8db14d52_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;563&#39; height=&#39;426&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"563\" data-rawheight=\"426\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"563\" data-original=\"https://pic3.zhimg.com/v2-df27a586b962a28c9bd212cf8db14d52_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-df27a586b962a28c9bd212cf8db14d52_b.jpg\"/></figure><h2>二、Tasks &amp; Evaluation Metrics</h2><h2><b>2.1 Tasks </b></h2><p><b>2.1.1 Cloze Test </b></p><p>给定上下文 <img src=\"https://www.zhihu.com/equation?tex=C\" alt=\"C\" eeimg=\"1\"/> ，一个词或实体 <img src=\"https://www.zhihu.com/equation?tex=a%28a%5Cin+C%29\" alt=\"a(a\\in C)\" eeimg=\"1\"/> 被移除，完形填空任务要求模型使用正确的词或实体进行填空，最大化条件概率 <img src=\"https://www.zhihu.com/equation?tex=P%28a%7CC+%E2%88%92+%5C%7Ba%5C%7D%29\" alt=\"P(a|C − \\{a\\})\" eeimg=\"1\"/> 。</p><p>数据集：CNN &amp; Daily Mail 、CBT、LAMBADA、Who-did-What、CLOTH、CliCR</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-427fab0f389e2af7aca82a7dd00b3ab1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1195\" data-rawheight=\"392\" class=\"origin_image zh-lightbox-thumb\" width=\"1195\" data-original=\"https://pic2.zhimg.com/v2-427fab0f389e2af7aca82a7dd00b3ab1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1195&#39; height=&#39;392&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1195\" data-rawheight=\"392\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1195\" data-original=\"https://pic2.zhimg.com/v2-427fab0f389e2af7aca82a7dd00b3ab1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-427fab0f389e2af7aca82a7dd00b3ab1_b.jpg\"/></figure><p><b>2.1.2 Multiple Choice </b></p><p>给定上下文 <img src=\"https://www.zhihu.com/equation?tex=C\" alt=\"C\" eeimg=\"1\"/> ，问题 <img src=\"https://www.zhihu.com/equation?tex=Q\" alt=\"Q\" eeimg=\"1\"/> ，候选答案列表 <img src=\"https://www.zhihu.com/equation?tex=A%3D%5C%7Ba_1%2Ca_2%2C...%2Ca_n%5C%7D\" alt=\"A=\\{a_1,a_2,...,a_n\\}\" eeimg=\"1\"/> ，多项选择任务要求模型从A中选择正确的答案 <img src=\"https://www.zhihu.com/equation?tex=a_i\" alt=\"a_i\" eeimg=\"1\"/> ，最大化条件概率 <img src=\"https://www.zhihu.com/equation?tex=P%28a_i%7CC%2CQ%2CA%29\" alt=\"P(a_i|C,Q,A)\" eeimg=\"1\"/> 。与完形填空任务的区别就是答案不再局限于单词或实体，并且候选答案列表是必须要提供的。</p><p>数据集：MCTest、RACE</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ae829c393644acd934125e1d92a740f7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1198\" data-rawheight=\"337\" class=\"origin_image zh-lightbox-thumb\" width=\"1198\" data-original=\"https://pic4.zhimg.com/v2-ae829c393644acd934125e1d92a740f7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1198&#39; height=&#39;337&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1198\" data-rawheight=\"337\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1198\" data-original=\"https://pic4.zhimg.com/v2-ae829c393644acd934125e1d92a740f7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-ae829c393644acd934125e1d92a740f7_b.jpg\"/></figure><p><b>2.1.3 Span Extraction </b></p><p>尽管完形填空和多项选择一定程度上可以机器阅读理解的能力，但是这两个任务有一定的局限性。首先，单词或实体可能不足以回答问题，需要完整的句子进行回答；其次，在很多情形是没有提供候选答案的。所以片段抽取任务应运而生。</p><p>给定上下文 <img src=\"https://www.zhihu.com/equation?tex=C\" alt=\"C\" eeimg=\"1\"/> 和问题 <img src=\"https://www.zhihu.com/equation?tex=Q\" alt=\"Q\" eeimg=\"1\"/> ，其中 <img src=\"https://www.zhihu.com/equation?tex=C%3D%5C%7Bt_1%2Ct_2%2C...%2Ct_n%5C%7D\" alt=\"C=\\{t_1,t_2,...,t_n\\}\" eeimg=\"1\"/> ，片段抽取任务要求模型从C中抽取连续的子序列 <img src=\"https://www.zhihu.com/equation?tex=a%3D%5C%7Bt_i%2Ct_%7Bi%2B1%7D%2C...%2Ct_%7Bi%2Bk%7D%5C%7D%281%5Cleq+i%5Cleq+i%2Bk%5Cleq+n%29\" alt=\"a=\\{t_i,t_{i+1},...,t_{i+k}\\}(1\\leq i\\leq i+k\\leq n)\" eeimg=\"1\"/> 作为正确答案，最大化条件概率<img src=\"https://www.zhihu.com/equation?tex=P%28a%7CC%2CQ%29\" alt=\"P(a|C,Q)\" eeimg=\"1\"/> 。</p><p>数据集：SQuAD、NewsQA、TriviaQA、DuoRC</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-0bd4ad87a3a13cb6191223872ecbb598_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1201\" data-rawheight=\"392\" class=\"origin_image zh-lightbox-thumb\" width=\"1201\" data-original=\"https://pic1.zhimg.com/v2-0bd4ad87a3a13cb6191223872ecbb598_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1201&#39; height=&#39;392&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1201\" data-rawheight=\"392\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1201\" data-original=\"https://pic1.zhimg.com/v2-0bd4ad87a3a13cb6191223872ecbb598_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-0bd4ad87a3a13cb6191223872ecbb598_b.jpg\"/></figure><p><b>2.1.4 Free Answering </b></p><p>对于答案局限于一段上下文是不现实的，为了回答问题，机器需要在多个上下文中进行推理并总结答案。自由回答任务是四个任务中最复杂的，也更适合现实的应用场景。</p><p>给定上下文C和问题Q，在自由回答任务中正确答案可能不是C中的一个子序列， <img src=\"https://www.zhihu.com/equation?tex=a%5Csubseteq+C\" alt=\"a\\subseteq C\" eeimg=\"1\"/> 或 <img src=\"https://www.zhihu.com/equation?tex=a%5Cnsubseteq+C\" alt=\"a\\nsubseteq C\" eeimg=\"1\"/> ，自由回答任务需要预测正确答案a，并且最大化条件概率<img src=\"https://www.zhihu.com/equation?tex=P%28a%7CC%2CQ%29\" alt=\"P(a|C,Q)\" eeimg=\"1\"/> 。</p><p>数据集：bAbI、MS MARCO 、SearchQA、NarrativeQA、DuReader</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-dd065ba9a3f54f2a1b66e25faa233a63_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1198\" data-rawheight=\"597\" class=\"origin_image zh-lightbox-thumb\" width=\"1198\" data-original=\"https://pic4.zhimg.com/v2-dd065ba9a3f54f2a1b66e25faa233a63_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1198&#39; height=&#39;597&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1198\" data-rawheight=\"597\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1198\" data-original=\"https://pic4.zhimg.com/v2-dd065ba9a3f54f2a1b66e25faa233a63_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-dd065ba9a3f54f2a1b66e25faa233a63_b.jpg\"/></figure><p><b>2.2 Evaluation Metrics </b></p><ul><li>Accuracy：完形填空、多项选择</li><li>F1：</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-e1ddea5b7a0b3b9aca95aee59afd9894_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"847\" data-rawheight=\"169\" class=\"origin_image zh-lightbox-thumb\" width=\"847\" data-original=\"https://pic1.zhimg.com/v2-e1ddea5b7a0b3b9aca95aee59afd9894_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;847&#39; height=&#39;169&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"847\" data-rawheight=\"169\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"847\" data-original=\"https://pic1.zhimg.com/v2-e1ddea5b7a0b3b9aca95aee59afd9894_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-e1ddea5b7a0b3b9aca95aee59afd9894_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-4b29c5ab75ad1209a865b8f644e2a018_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"263\" data-rawheight=\"172\" class=\"content_image\" width=\"263\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;263&#39; height=&#39;172&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"263\" data-rawheight=\"172\" class=\"content_image lazy\" width=\"263\" data-actualsrc=\"https://pic1.zhimg.com/v2-4b29c5ab75ad1209a865b8f644e2a018_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-35e2e49aa311e01aff041e3bbc961cf9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"225\" data-rawheight=\"75\" class=\"content_image\" width=\"225\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;225&#39; height=&#39;75&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"225\" data-rawheight=\"75\" class=\"content_image lazy\" width=\"225\" data-actualsrc=\"https://pic2.zhimg.com/v2-35e2e49aa311e01aff041e3bbc961cf9_b.jpg\"/></figure><ul><li>ROUGE-L</li><li>BLEU</li></ul><p><b>三、General Architecture</b></p><p>典型的MRC系统以上下文和问题为输入，然后输入答案，系统包含四个关键模块：Embeddings, Feature Extraction, Context-Question Interaction，Answer Prediction。</p><ul><li><b>Embeddings：</b>将单词映射为对应的词向量，可能还会加上POS、NER、question category等信息；</li><li><b>Feature Extraction：</b>抽取question和context的上下文信息，可以通过CNN、RNN等模型结构；</li><li><b>Context-Question Interaction：</b>context和question之间的相关性在预测答案中起着重要作用。有了这些信息，机器就能够找出context中哪些部分对回答question更为重要。为了实现该目标，在该模块中广泛使用attention机制，单向或双向，以强调与query相关的context的部分。为了充分提取它们的相关性，context和question之间的相互作用有时会执行多跳，这模拟了人类理解的重读过程。</li><li><b>Answer Prediction：</b>基于上述模块获得的信息输出最终答案。因为MRC任务根据答案形式分为了很多种，所以该模块与不同任务相关。对于完形填空，该模块输出context中的一个单词或一个实体；对于多项选择，该模块从候选答案中选择正确答案。</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-239d4adc691bad54ef41c68a4d4aef62_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"949\" data-rawheight=\"450\" class=\"origin_image zh-lightbox-thumb\" width=\"949\" data-original=\"https://pic3.zhimg.com/v2-239d4adc691bad54ef41c68a4d4aef62_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;949&#39; height=&#39;450&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"949\" data-rawheight=\"450\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"949\" data-original=\"https://pic3.zhimg.com/v2-239d4adc691bad54ef41c68a4d4aef62_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-239d4adc691bad54ef41c68a4d4aef62_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "自然语言处理", 
                    "tagLink": "https://api.zhihu.com/topics/19560026"
                }, 
                {
                    "tag": "机器阅读", 
                    "tagLink": "https://api.zhihu.com/topics/20203649"
                }, 
                {
                    "tag": "阅读理解", 
                    "tagLink": "https://api.zhihu.com/topics/20015446"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/80658515", 
            "userName": "Chevalier", 
            "userLink": "https://www.zhihu.com/people/eba7ab78a0341c3ba393073fe68782d0", 
            "upvote": 4, 
            "title": "《通过答案分离来提升自然问题的生成》论文笔记", 
            "content": "<h2>本篇论文发表于2019 AAAI会议</h2><h2>一、Introduction</h2><p>问题生成任务的定义是给定一个段落 <img src=\"https://www.zhihu.com/equation?tex=X%5Ep\" alt=\"X^p\" eeimg=\"1\"/> 和答案 <img src=\"https://www.zhihu.com/equation?tex=X%5Ea\" alt=\"X^a\" eeimg=\"1\"/> 作为模型的输入，输出一个可以被该答案回答的问题 <img src=\"https://www.zhihu.com/equation?tex=Y\" alt=\"Y\" eeimg=\"1\"/> 。数学表达式如下：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbar%7BY%7D%3D+%5Cargmax_%7BY%7DP%28Y%7CX%5Ep%2CX%5Ea%29\" alt=\"\\bar{Y}= \\argmax_{Y}P(Y|X^p,X^a)\" eeimg=\"1\"/> </p><p>目前最主流的模型是Encoder-Decoder模型，Encoder将变长的输入转化为定长的向量表示，该向量表示包含了输入序列的上下文信息，Decoder根据Encoder的输出和attention机制进行解码。</p><p>对于上述的Encoder-Decoder模型，输入有两种：一种是只输入段落，另一种是输入段落和答案。只输入段落生成的问题比较有随机性，没有明确的目标，并且一个段落只能生成同一个问题，实际上一个段落可能对应有多个问题；为了解决这个问题，有研究学者就将段落和答案同时输入到模型中，另外，为了缓解UNK问题，在模型中引入了copy机制，将输入序列的单词拷贝到输出序列中。</p><h2>二、Motivation</h2><ul><li>目前的Encoder-Decoder模型将整个段落输入到模型中，编码器就会将段落的所有信息（包括答案）传到解码器，这就会导致生成的问题包含了答案，如下图所示；</li><li>另外，copy机制的引入加重了上述问题，因为copy机制是直接将输入序列的单词拷贝到输出序列中。</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-38bd9ff735ad17f929285f1e48ceef7e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"763\" data-rawheight=\"139\" class=\"origin_image zh-lightbox-thumb\" width=\"763\" data-original=\"https://pic3.zhimg.com/v2-38bd9ff735ad17f929285f1e48ceef7e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;763&#39; height=&#39;139&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"763\" data-rawheight=\"139\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"763\" data-original=\"https://pic3.zhimg.com/v2-38bd9ff735ad17f929285f1e48ceef7e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-38bd9ff735ad17f929285f1e48ceef7e_b.jpg\"/></figure><h2><b>三、Model</b></h2><p>本文的模型可以分为以下几个部分：</p><ul><li>Encoder</li><ul><li>Answer-Separated Passage Encoder</li><li>Answer Encoder</li></ul><li>Decoder</li><ul><li>Keyword-Net</li><li>Retrieval Style Word Generator</li></ul></ul><ol><li><b>Answer-Separated Passage Encoder</b></li></ol><p>与Seq2Seq模型的Encoder没什么不同，只是把答案用标记&#39;&lt;a&gt;&#39;代替，如下图所示，然后通过BiLSTM对输入序列进行编码，捕捉上下文信息，这种替换策略可以降低生成问题中包含答案的概率。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-faf504f7bc009b4573022310134259ae_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"429\" data-rawheight=\"296\" class=\"origin_image zh-lightbox-thumb\" width=\"429\" data-original=\"https://pic3.zhimg.com/v2-faf504f7bc009b4573022310134259ae_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;429&#39; height=&#39;296&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"429\" data-rawheight=\"296\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"429\" data-original=\"https://pic3.zhimg.com/v2-faf504f7bc009b4573022310134259ae_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-faf504f7bc009b4573022310134259ae_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-76ecfc873411da305329b45e2b06174f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"576\" data-rawheight=\"351\" class=\"origin_image zh-lightbox-thumb\" width=\"576\" data-original=\"https://pic4.zhimg.com/v2-76ecfc873411da305329b45e2b06174f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;576&#39; height=&#39;351&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"576\" data-rawheight=\"351\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"576\" data-original=\"https://pic4.zhimg.com/v2-76ecfc873411da305329b45e2b06174f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-76ecfc873411da305329b45e2b06174f_b.jpg\"/></figure><p><b><br/> 2. Answer Encoder</b></p><p>将答案单独拎出来，输入到另一个编码器中，得到答案的表示。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-365d6d05569230e116bd224993c9ee83_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"379\" data-rawheight=\"202\" class=\"content_image\" width=\"379\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;379&#39; height=&#39;202&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"379\" data-rawheight=\"202\" class=\"content_image lazy\" width=\"379\" data-actualsrc=\"https://pic4.zhimg.com/v2-365d6d05569230e116bd224993c9ee83_b.jpg\"/></figure><p><b>3. Keyword-Net</b></p><p>在生成问题的时候，答案中每个词对问题的贡献是不一样的。举个例子，给定一个段落“Steve Jobs is the founder of Apple”和答案“founder of Apple”，我们想生成问题“Who is Steve Jobs?”，那么“founder”作为答案的关键词其实就可以表示答案的整个语义。因此，作者提出了Keyword-Net来捕捉答案中的关键信息，其实就是attention。</p><p>首先用当前时刻的上下文向量 <img src=\"https://www.zhihu.com/equation?tex=c_t\" alt=\"c_t\" eeimg=\"1\"/> （ <img src=\"https://www.zhihu.com/equation?tex=c_t+\" alt=\"c_t \" eeimg=\"1\"/> 的计算与传统的基于attention的Seq2Seq模型一样， 这里不再赘述）来初始化第一层的Keyword-Net <img src=\"https://www.zhihu.com/equation?tex=o_t%5E0\" alt=\"o_t^0\" eeimg=\"1\"/> ，该向量包含了段落的信息。</p><p><img src=\"https://www.zhihu.com/equation?tex=o_t%5E0%3Dc_t\" alt=\"o_t^0=c_t\" eeimg=\"1\"/> </p><p>接着就是点乘得到答案中每个词的attention权重，然后再加权求和得到当前时刻下答案的相关程度的一个表征。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-75e2133fa0027c1a79a9efad6ab7b504_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"421\" data-rawheight=\"154\" class=\"origin_image zh-lightbox-thumb\" width=\"421\" data-original=\"https://pic1.zhimg.com/v2-75e2133fa0027c1a79a9efad6ab7b504_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;421&#39; height=&#39;154&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"421\" data-rawheight=\"154\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"421\" data-original=\"https://pic1.zhimg.com/v2-75e2133fa0027c1a79a9efad6ab7b504_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-75e2133fa0027c1a79a9efad6ab7b504_b.jpg\"/></figure><p>这个Keyword-Net可以堆叠多层，最后将 <img src=\"https://www.zhihu.com/equation?tex=o_t%5El\" alt=\"o_t^l\" eeimg=\"1\"/> 输入到Decoder进行解码。</p><p><img src=\"https://www.zhihu.com/equation?tex=s_t%3DLSTM%28y_%7Bt-1%7D%2Cs_%7Bt-1%7D%2Cc_t%2Co_t%5EL%29\" alt=\"s_t=LSTM(y_{t-1},s_{t-1},c_t,o_t^L)\" eeimg=\"1\"/> </p><p><b>4. Retrieval Style Word Generator</b></p><p>一般情况下，经过上述步骤我们得到了t时刻的隐藏层状态，然后就直接softmax得到词的概率分布，但是这种做法并没有理解词的语义，只是记住了训练集的词和模式。因此，作者参考了释义生成任务中的一个工作，将t时刻的向量表示与词向量做一个点乘，计算两个向量的语义相似度，然后再进行softmax。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-dd429934f0b80648c10f08e29f358f8b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"467\" data-rawheight=\"158\" class=\"origin_image zh-lightbox-thumb\" width=\"467\" data-original=\"https://pic4.zhimg.com/v2-dd429934f0b80648c10f08e29f358f8b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;467&#39; height=&#39;158&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"467\" data-rawheight=\"158\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"467\" data-original=\"https://pic4.zhimg.com/v2-dd429934f0b80648c10f08e29f358f8b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-dd429934f0b80648c10f08e29f358f8b_b.jpg\"/></figure><h2>四、Experiment</h2><ol><li><b>Dataset</b></li></ol><p>SQuAD：两种数据划分方式。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b14abe5c65ab2e35f97cdbefa47274c5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"821\" data-rawheight=\"150\" class=\"origin_image zh-lightbox-thumb\" width=\"821\" data-original=\"https://pic2.zhimg.com/v2-b14abe5c65ab2e35f97cdbefa47274c5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;821&#39; height=&#39;150&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"821\" data-rawheight=\"150\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"821\" data-original=\"https://pic2.zhimg.com/v2-b14abe5c65ab2e35f97cdbefa47274c5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-b14abe5c65ab2e35f97cdbefa47274c5_b.jpg\"/></figure><p><b>2. Evaluation Methods </b></p><p><b>BLEU-4、Meteor、</b> <img src=\"https://www.zhihu.com/equation?tex=Rough_L\" alt=\"Rough_L\" eeimg=\"1\"/> </p><p><b>3. Named Entity Replacement</b></p><p>考虑到大多数命名实体不在词汇库中，生成问题的时候就会出现很多UNK，这里采取了替换策略，用命名实体的标签来代替该实体，比如“Bob”用“Person”代替，如果一个段落中出现多个相同类型的实体，那么用“Person1”、“Person2”代替，最后再通过字典替换回来。</p><p><b>4. Performance Comparison </b></p><p>可以看到作者提出的ASs2s模型取得了最优的性能</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-9f455d72844a35ba9e749df96bf6a199_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"292\" class=\"origin_image zh-lightbox-thumb\" width=\"960\" data-original=\"https://pic2.zhimg.com/v2-9f455d72844a35ba9e749df96bf6a199_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;960&#39; height=&#39;292&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"292\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"960\" data-original=\"https://pic2.zhimg.com/v2-9f455d72844a35ba9e749df96bf6a199_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-9f455d72844a35ba9e749df96bf6a199_b.jpg\"/></figure><p><b>五、Conclusion</b></p><p>本文提出了分离答案的Seq2Seq模型，通过两个编码器同时编码段落和答案的信息，在解码器部分，通过keyword-net来捕捉答案中的关键信息。这种分离答案的Seq2Seq模型有效地缓解了生成问题包含答案的情况。</p>", 
            "topic": [
                {
                    "tag": "自然语言处理", 
                    "tagLink": "https://api.zhihu.com/topics/19560026"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/78668130", 
            "userName": "Chevalier", 
            "userLink": "https://www.zhihu.com/people/eba7ab78a0341c3ba393073fe68782d0", 
            "upvote": 1, 
            "title": "SMP所见所闻——第一天（2）", 
            "content": "<p>本文主要是SMP各个比赛中的top队伍方案，只简单介绍获奖方案中吸引到我的部分。SMP 2019主要包括四个评测比赛，具体比赛介绍链接：<a href=\"https://link.zhihu.com/?target=http%3A//conference.cipsc.org.cn/smp2019/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">传送门</a></p><ul><li>  中文人机对话技术评测；</li><li>“拓尔思”中文隐式情感分析评测；</li><li>“语通杯”文本溯源技术测评；</li><li>“中国法研杯”中文法律阅读理解比赛</li></ul><p><b>敲重点！BERT、ERNIE等预训练模型作为baseline模型出现在各个top方案中。</b></p><h2><b>一、</b>“拓尔思”中文隐式情感分析评测</h2><ul><li>基于pytorch的开源的预训练模型框架：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/dbiir/UER-py\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">UER</a></li><li>对于微调策略，传统的二阶段流程为：大规模语料预训练-&gt;下游任务微调；三阶段流程：大规模语料预训练-&gt;<b>下游任务预训练</b>-&gt;下游任务微调。这样做的好处就是可以使模型学习到下游任务的数据分布，更好地拟合下游任务，当然如果数据太少也就没必要了。</li><li>《<b>Understanding the Behaviors of BERT in Ranking</b>》提出集成不同层Transformer的响应值的方案，从字面的意思来看就是融合各个层提取的特征（有兴趣的可以去看原文），这里有三种策略：</li><ul><li>只取最后一层Transformer[CLS]对应的响应值；</li><li>平均集成后三层Transformer的响应值；</li><li>加权求和后三层Transformer的响应值；</li></ul><li>《<b>Unsupervised Data Augmentation for Consistency Training</b>》<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1904.12848.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">论文</a>、<a href=\"https://link.zhihu.com/?target=https%3A//github.com/google-research/uda\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">github</a>：</li><ul><li>virtual adversarial training：在训练的每次迭代中，对句子输入做随机字mask，然后算输出的概率分布与没有mask的概率分布的KL散度，加入到原本的CrossEntropyLoss中，使得模型在样本点附近预测更平滑，增加泛化性。</li><li>training signal annealing：在设定一个随时间变化的阈值 <img src=\"https://www.zhihu.com/equation?tex=%5Ceta\" alt=\"\\eta\" eeimg=\"1\"/> ，当一个样本在正确类别的概率大于<img src=\"https://www.zhihu.com/equation?tex=%5Ceta\" alt=\"\\eta\" eeimg=\"1\"/>时，屏蔽它的损失，目的是逐步释放训练信号，防止模型对一些已经正确的样本过拟合，<img src=\"https://www.zhihu.com/equation?tex=%5Ceta\" alt=\"\\eta\" eeimg=\"1\"/>的计算使用了指数增长的方式，并且设定阈值从0.5开始增长。</li></ul></ul><h2>二、“中国法研杯”中文法律阅读理解比赛</h2><ul><li>BERT+CNN/LSTM/GRU：一般情况下，我们通过bert得到相应的特征后，直接接全连接层做分类任务。至于后面是否需要再接CNN/LSTM/GRU，这得看炼丹和机器了</li></ul><h2><b>三、</b> 中文人机对话技术评测</h2><p>这个比赛有两个赛道，分别是自然语言理解和个性化对话。</p><ol><li>自然语言理解</li></ol><p>自然语言理解分为三个子任务：领域分类、意图分类、槽填充。</p><ul><li>领域意图约束：不同领域下的意图和槽的类型是不一定相同的，所以通过one-hot向量表示该领域下有哪些意图和槽，计算损失和输出类别时乘以相应的one-hot向量，将不在该领域下的意图、槽mask掉；</li><li>多解码启发式规则：存在一些领域的数据具有模糊性，模型很容易分错。多解码表示用多个领域的槽向量去解码同一段输出，得到不同领域下不同的槽，再对比不同解码得到的槽来修正领域识别的结果；</li><li>这里提到一个feature work挺有意思：目前多任务学习只是简单地将三个任务的loss相加，微软的MT-DNN对于多任务每次只随机计算一个任务的loss，多任务底层共享参数。</li><li>数据增强：训练数据只有2k条，因为这个比赛可以引入外部数据，所以根据百度知道和自主造句扩充了2w条数据；还有一个队伍的公司本身做NLU这块，利用公司的数据扩充了20w条数据。</li><li>引入知识图谱</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-cf04eab77f7b310e5a65ffab063afcff_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1047\" data-rawheight=\"658\" class=\"origin_image zh-lightbox-thumb\" width=\"1047\" data-original=\"https://pic4.zhimg.com/v2-cf04eab77f7b310e5a65ffab063afcff_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1047&#39; height=&#39;658&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1047\" data-rawheight=\"658\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1047\" data-original=\"https://pic4.zhimg.com/v2-cf04eab77f7b310e5a65ffab063afcff_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-cf04eab77f7b310e5a65ffab063afcff_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>2. 个性化对话</p><ul><li>中文小说预训练的GPT模型-&gt;全量对话数据预训练GPT模型-&gt;个性化对话数据微调GPT模型</li></ul><h2>四、“语通杯”文本溯源技术测评</h2><p>后面听的有点累了，没怎么记录= =</p><h2>五、总结</h2><p>预训练模型已经是各个NLP比赛的baseline模型了，通过集成不同数据（百度百科、中文维基百科、新浪微博、贴吧等）训练的预训练模型和不同预训练模型（BERT、GPT、ELMO、ERNIE等）就能得到一个不错的分数，至于获奖的话，还需要一些方法，包括但不限于：数据增强、微调策略、task-specific feature、融合策略、数据预处理等等。</p><p></p>", 
            "topic": [
                {
                    "tag": "自然语言处理", 
                    "tagLink": "https://api.zhihu.com/topics/19560026"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/78625877", 
            "userName": "Chevalier", 
            "userLink": "https://www.zhihu.com/people/eba7ab78a0341c3ba393073fe68782d0", 
            "upvote": 0, 
            "title": "SMP所见所闻——第一天（1）", 
            "content": "<h2>SMP主题</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-6383f6415c53eba0be51aaccac6670d4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"4032\" data-rawheight=\"3024\" class=\"origin_image zh-lightbox-thumb\" width=\"4032\" data-original=\"https://pic1.zhimg.com/v2-6383f6415c53eba0be51aaccac6670d4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;4032&#39; height=&#39;3024&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"4032\" data-rawheight=\"3024\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"4032\" data-original=\"https://pic1.zhimg.com/v2-6383f6415c53eba0be51aaccac6670d4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-6383f6415c53eba0be51aaccac6670d4_b.jpg\"/></figure><h2>一、特邀报告</h2><ol><li><b>城市大脑与边缘计算</b></li></ol><p>第一次听这个名词，还挺有意思，城市大脑是智能城市的决策指挥系统，其决策信息来自于包括摄像头等视频传感器在内的各种感知系统。城市大脑在警务、刑侦、交通安防中扮演着重要角色。我在想城市大脑能否对突发事件快速有效地制定决策，辅助人们。就拿最近的一个热点来说——利奇马台风在浙江登陆。城市大脑能否制定逃离路线、在哪里安置人群等。还有一个就是HK，最近经常上热搜，城市大脑如何制定决策去控制舆情的发展，这是一个值得思考的问题。</p><p><b>2. 基于端到端模型的知识获取与知识问答</b></p><p>本人有浓厚兴趣的报告之一，这是中科院自动化所刘康老师做的报告。首先介绍了流水线的方法在实体关系三元组的抽取上的一个局限性，紧接着介绍了目前实体关系抽取的一个难点：关系重叠。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-26176d99708679a5efdd6a1e3761eab0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"4032\" data-rawheight=\"3024\" class=\"origin_image zh-lightbox-thumb\" width=\"4032\" data-original=\"https://pic1.zhimg.com/v2-26176d99708679a5efdd6a1e3761eab0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;4032&#39; height=&#39;3024&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"4032\" data-rawheight=\"3024\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"4032\" data-original=\"https://pic1.zhimg.com/v2-26176d99708679a5efdd6a1e3761eab0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-26176d99708679a5efdd6a1e3761eab0_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-5066df119a3c0c99bd6e5051c7a061e8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"4032\" data-rawheight=\"3024\" class=\"origin_image zh-lightbox-thumb\" width=\"4032\" data-original=\"https://pic1.zhimg.com/v2-5066df119a3c0c99bd6e5051c7a061e8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;4032&#39; height=&#39;3024&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"4032\" data-rawheight=\"3024\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"4032\" data-original=\"https://pic1.zhimg.com/v2-5066df119a3c0c99bd6e5051c7a061e8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-5066df119a3c0c99bd6e5051c7a061e8_b.jpg\"/></figure><p>刘康老师实验室在18年ACL的一篇工作提出基于Seq2Seq的方法来生成实体关系三元组，作者的脑洞太大了，orz（关于这篇论文的解析：<a href=\"https://zhuanlan.zhihu.com/p/75226723\" class=\"internal\">基于copy机制的端到端实体关系联合抽取</a>）</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ca61a4b9c571564847a0680a5c4c77d8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"4032\" data-rawheight=\"3024\" class=\"origin_image zh-lightbox-thumb\" width=\"4032\" data-original=\"https://pic1.zhimg.com/v2-ca61a4b9c571564847a0680a5c4c77d8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;4032&#39; height=&#39;3024&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"4032\" data-rawheight=\"3024\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"4032\" data-original=\"https://pic1.zhimg.com/v2-ca61a4b9c571564847a0680a5c4c77d8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-ca61a4b9c571564847a0680a5c4c77d8_b.jpg\"/></figure><p>当时看完这篇论文的时候就发现该方法在生成实体关系三元组时是有顺序的，实际上实体关系三元组是顺序无关的，比如{(关系1，实体1，实体2)，(关系2，实体3，实体4)}与{(关系2，实体1，实体2)，(关系1，实体3，实体4)}都是对的，但是基于Seq2Seq的方法生成实体关系三元组时存在一定的顺序性，即使是对的情况下但顺序与groundtruth不对导致计算的loss变大了。刚好实验室组会介绍了一篇发表在2019 ACL的论文：《A Deep Reinforced Sequence-to-Set Model for Multi-Label Text Classification》。这篇论文也是用Seq2Seq的方法解决文本多标签分类问题，这种方法也是存在上述的局限性，然后本篇论文利用了policy gradient 的方法缓解了这个问题，本想利用这个思想解决实体关系抽取的问题，万万没想到的是，刘康老师的实验室在2019 EMNLP上通过RL解决了这个问题。具体论文见下图。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-2abe6e3dcf48f8e7ab73656c460515ac_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"4032\" data-rawheight=\"3024\" class=\"origin_image zh-lightbox-thumb\" width=\"4032\" data-original=\"https://pic1.zhimg.com/v2-2abe6e3dcf48f8e7ab73656c460515ac_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;4032&#39; height=&#39;3024&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"4032\" data-rawheight=\"3024\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"4032\" data-original=\"https://pic1.zhimg.com/v2-2abe6e3dcf48f8e7ab73656c460515ac_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-2abe6e3dcf48f8e7ab73656c460515ac_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "自然语言处理", 
                    "tagLink": "https://api.zhihu.com/topics/19560026"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/77151352", 
            "userName": "Chevalier", 
            "userLink": "https://www.zhihu.com/people/eba7ab78a0341c3ba393073fe68782d0", 
            "upvote": 9, 
            "title": "问题生成综述", 
            "content": "<h2><b>一、Introduction</b></h2><p>问题生成任务是指从各种各样的输入（如原始文本，数据库，语义表示）自动生成问题。问题生成有很多应用：在教育领域生成问题评估学生；对话系统、阅读理解、问答等等。传统的问题生成主要专注于从一个句子或段落中生成事实性问题。</p><h2><b>二、Fundamental Aspects of NQG</b></h2><p>本文通过对NQG近期发展进行系统调查，重点关注深度学习为QG带来的三个新兴趋势:学习范式的变化、输入的拓展、深层问题的生成。</p><p><b>2.1 Learning Paradigm </b></p><p>传统的问题生成主要考虑两方面：what to ask 和 how to ask。过去的研究通过content selection和question construction来考虑这两方面。给定一个句子或段落作为输入，内容选择是选择一个值得提问的主题并且决定问题类型(what，how，when等)，基本的方法都是通过语义解析获得中间的语义表示。问题构造是将这个中间表示转换为一个自然语言问题，可以分为基于转换的方法和基于模板的方法。这种QG架构是有缺陷的，因为它们的表示局限于各种中间表示、转换规则或模板。</p><p>随着深度学习的兴起，不少研究学者提出了端到端的架构。比如Seq2Seq模型联合学习内容选择（encoder）和问题构造（decoder）。在这个模型中，传统的基于语义的内容选择方法被更灵活的方法代替，比如attention、copy机制。问题构造完全变为数据驱动，无须耗费大量人力来构造转换规则。</p><p><b>2.2 Input Modality</b></p><p>问题生成的输入形式是多样的，取决于应用。传统的QG主要是文本输入。随着VQA和KBQA的兴起，NQG的输入扩大为知识库和图像。</p><p><b>2.3 Cognitive Levels</b></p><p>来自Bloom分类法的框架试图对生成问题进行认知水平的分类，目前有六个认知水平：Remembering, Understanding, Applying, Analyzing, Evaluating，Creating。传统方法生成的问题的认知水平还停留在浅层级别，生成的问题大部分是基于句子的事实性问题（Who, What, Where等），对应的答案是出现在输入句子中的。然而，实现人类认知水平的QG系统应该能够产生有意义的问题（比如why，what if，how）。传统的方法通过手工模板生成问题，这些方法并没有真正意义上理解文本。</p><p>尽管提出深层次问题很复杂，但NQG能够对大量数据进行推广，这使得最近的研究能够探索QG的理解和推理层面。</p><h2><b>三、Corpora</b></h2><p>QG任务可以视为QA的对偶任务，所以QA的数据集都适用于QG任务。但是，至少有两个与语料相关的因素会影响生成问题的难度。第一个是回答问题需要的认知水平，目前的NQG主要在浅层的事实型问题上取得了不错的成果，比如SQuAD、MSMARCO。然而在深度问题数据集上的效果很差，比如LearningQ。第二个因素是答案类型，也就是答案格式，可以分为四种：（1）答案是段落中的文本片段，通常针对于事实型问题；（2）人工生成的、抽象的答案，可能不在段落中直接出现，通常针对于深度问题；（3）多项选择，问题和干扰项是一起生成的；（4）没有答案，需要模型自动学习是否值得提问。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-30fbe632f1468b54f24bc736d14f5187_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1531\" data-rawheight=\"431\" class=\"origin_image zh-lightbox-thumb\" width=\"1531\" data-original=\"https://pic4.zhimg.com/v2-30fbe632f1468b54f24bc736d14f5187_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1531&#39; height=&#39;431&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1531\" data-rawheight=\"431\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1531\" data-original=\"https://pic4.zhimg.com/v2-30fbe632f1468b54f24bc736d14f5187_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-30fbe632f1468b54f24bc736d14f5187_b.jpg\"/></figure><h2><b>四、Evaluation Metrics</b></h2><p>尽管QG和QA共享数据集，但是评价标准是不一样的。定义一个标准来衡量生成问题很有挑战性。有意义，语法正确，语义合理和自然都是有用的标准，但它们很难量化。大多数QG系统采用人工评测，但是人工评测很耗时间。因此借鉴了机器翻译的评测标准，BLEU、METEOR、ROUGE等广泛应用于QG中。然而，有研究指出这些指标与流畅性，充分性，连贯性没有很好的相关性，因为它们基本上计算了groundtruth与生成问题之间的n-gram相似性。因此，Nema and Khapra提出了一个新的度量标准，通过计算几个特定问题因素的分数来评估问题的“可回答性”，包括问题类型，内容词，功能词和命名实体。但这种方法刚提出不久，还没应用到所有的NQG系统中。</p><h2><b>五、Methodology</b></h2><p>目前大多数NQG模型都采用了Seq2Seq架构。输入一个句子或一个段落X和目标答案A，生成一个问题Y。Du等人是第一个使用基于attention的Seq2Seq模型来说生成问题，但是没有将答案作为输入[1]。</p><p>尽管大多数模型采用了Seq2Seq架构，但是它们之间也有很大不同——（1）QG-specific factors（比如，答案编码、问题词生成，段落级上下文）；（2）普遍的NLG技术（copy机制，语言特征，强化学习）</p><p><b>5.1 Encoding Answers </b></p><p>答案作为额外的输入用来引导模型生成问题的时候应该关注哪部分信息。模型要么输入答案的位置或者通过RNN来编码答案，得到答案信息。</p><p>第一种方法就是在输入的词向量中额外添加一维特征，用来表示该词是否属于答案片段。Zhou等人使用BIO表示该词是答案边界、内、外[2]。Harrison and Walker等人直接用01表示是否是答案的一部分[3]。Sun等人认为答案周围的单词应该更重要，因为它们与答案是更相关的。所以计算attention的时候输入了位置向量[4]。</p><p>为了生成答案相关的问题，输入了答案显示地强调了答案的重要性，这也会导致生成的问题可能包含答案，这种问题显示是不可用的。也有工作解决这个问题，这里不再赘述。</p><p>未来的趋势可能是将答案与段落分开建模，因为答案可能不是直接出现在段落中的。当然，这也增加了模型的复杂性以及训练的难度。</p><p><b>5.2 Question Word Generation </b></p><p>问题词（why、who、when）在QG中扮演着重要角色。有研究学者观察到生成的问题词与答案类型不匹配，比如，对于答案“the end of the Mexican War”的问题词应该是when，但生成的问题词却是why。所以有工作在模型设计中考虑了问题词的生成。</p><p>Duan等人提出的模型中首先生成包含问题词的模板（“how to #”），接着生成问题的剩余部分[5]。Sun等人通过引入额外的解码模式来生成问题词。</p><p><b>5.3 Paragraph-level Contexts </b></p><p>段落级别的信息对于问题生成是有帮助的。在SQuAD数据集中，大概有20%的问题需要段落级别的信息才能正确回答。但是输入文本越长，Seq2Seq模型有效地利用相关上下文的时间更加艰难，同时要避免不相关的信息。</p><p>因此，为了解决这个难题，Zhao提出了gated self-attention编码器通过将重要信息与上下文的自我表示正确融合来优化编码上下文[6]。与上述工作利用整个上下文不同，Du and Cardie通过在上下文段落中运行coreference resolution system来进行预过滤，以获得输入句子和答案的coreference clusters。</p><p><b>5.4 Answer-unaware QG</b></p><p>上述模型都需要答案作为输入，根据答案 去生成问题。如果数据集中不存在答案，只有段落，那么模型应该自动识别段落中值得提问的部分。目前只有两篇工作(Du and Cardie, 2017; Subramanian et al., 2018)，将传统的QG任务分解为内容选择和问题构造，但是将这两个任务独立开来了。</p><p><b>5.5 Technical Considerations </b></p><p>自然语言生成常用的技术也应用到问题生成中。</p><p>（1）<b>Copying Mechanism：</b>在解码阶段，直接复制原句的相关单词到问题中。这是因为生成事实型问题时，RNN解码器难以生成稀有词，所以直接从原句中复制相关的短语和实体。</p><p>（2）<b>Linguistic Features：</b>额外的语言特征：POS、NER标签、依存信息。将这些特征与词向量拼接在一起。</p><p>（3）<b>Policy Gradient：</b>仅仅优化对数似然损失可能忽略了提问问题的许多等效方式。有一部分QG相关的工作采用策略梯度的方法在原来的目标函数上加了任务特定的reward。这有助于问题生成的多样性，因为模型在等效方式中而不是单个基本事实问题中学习分布概率质量。</p><p><b>5.6 The State of the Art</b></p><p>表二列出了各种模型在SQuAD数据集上的实验结果。有两个地方需要注意，尽管copy机制已被证明可以提升模型的性能，但也存在不足。Kim et al. (2019)观察到许多错误的问题，答案出现在这些问题中，这些问题的生成就是因为使用了copy机制[9]。其次，段落级别的上下文信息已被证明是有效的，不仅提高了模型的性能，而且还可以生成深层次的问题。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-79b4957f6faa5bfbbddbb26ddb38671e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1219\" data-rawheight=\"505\" class=\"origin_image zh-lightbox-thumb\" width=\"1219\" data-original=\"https://pic3.zhimg.com/v2-79b4957f6faa5bfbbddbb26ddb38671e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1219&#39; height=&#39;505&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1219\" data-rawheight=\"505\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1219\" data-original=\"https://pic3.zhimg.com/v2-79b4957f6faa5bfbbddbb26ddb38671e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-79b4957f6faa5bfbbddbb26ddb38671e_b.jpg\"/></figure><h2><b>六、Emerging Trends </b></h2><p>接下来会讨论NQG的三个研究趋势：Multi-task Learning, Wider Input Modalities and Deep Question Generation.</p><p><b>6.1 Multi-task Learning</b></p><p>结合其它任务来辅助QG，或者用QG来辅助其它任务。比如语义解析、QA。语义解析任务是将一个自然语言问题转化为一个SQL查询语句，那么可以通过QG模型生成（SQL, question）对来增加训练数据。在QA中，Sachan and Xing (2018)通过自训练的方法来联合学习QA和QG[10]。然而，虽然联合训练已显示出一定的效果，但由于混合的目标函数，其在QG上的表现低于state-of-the-art，这为未来的探索留下了空间。</p><p><b>6.2 Wider Input Modalities </b></p><p>QG工作现在已经整合了知识库（KBQG）和图像（VQG）的输入。输入三元组，生成相应的问题；输入一张图片，生成相应的问题。VQG根据认知水平可以分为grounded 和 open-ended VQG。grounded VQG是指答案的所有相关信息都可以在图片中直接找到。而open-ended VQG是指生成的问题需要通过推理才能回答。</p><p><b>6.3 Generation of Deep Questions </b></p><p>赋予QG系统能够提出深层问题的能力将帮助我们构建能够以更好的方式与人类互动的机器。然而，对人类自己来说，提出一个深层次的问题都很有挑战性。有学者在LearningQ数据集上跑了基于attention的Seq2Seq模型，但是实验结果非常差。尽管需要进一步深入分析才能探究背后的原因，但是我们认为有两种解释：（1）Seq2Seq模型无法有效地处理长输入；（2）Seq2Seq模型缺乏对多条信息进行推理的能力。</p><p>根据上述调研，我们认为深层次NQG的方向将会是：通过考虑多个源句之间的关系来增强NGQ模型；显式地建模典型的推理模式；理解并模拟人类问题提出背后的机制。</p><h2><b>七、Conclusion – What’s the Outlook? </b></h2><p>我们对NQG做了详细的调研，根据QG-specific和相应的技术划分了目前的NQG模型，并且指出了未来的研究方向：多任务学习、更广泛的输入、深层次问题的生成。</p><p><b>When to Ask：</b>模型除了要学习问什么、怎么问，还要学习何时问。</p><p><b>Personalized QG：</b>问题提问是存在个性化的，拥有不同知识背景的人会问不同的问题。然而，对话管理或推荐系统中将QG与用户建模相结合的工作还比较少。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>引用</b></h2><blockquote>[1]Xinya Du, Junru Shao, and Claire Cardie. 2017. Learning to ask: Neural question generation for reading comprehension. In Annual Meeting of the Association for Computational Linguistics (ACL), pages 1342–1352.<br/>[2] Qingyu Zhou, Nan Yang, Furu Wei, Chuanqi Tan, Hangbo Bao, and Ming Zhou. 2017. Neural question generation from text: A preliminary study. In CCF International Conference of Natural Language Processing and Chinese Computing (NLPCC), pages 662–671.<br/>[3] Vrindavan Harrison and Marilyn A. Walker. 2018. Neural generation of diverse questions using answer focus, contextual and linguistic features. In International Conference on Natural Language Generation (INLG), pages 296–306.<br/>[4] Xingwu Sun, Jing Liu, Yajuan Lyu, Wei He, Yanjun Ma, and Shi Wang. 2018. Answer-focused and position-aware neural question generation. In Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3930–3939.<br/>[5] Nan Duan, Duyu Tang, Peng Chen, and Ming Zhou. 2017. Question generation for question answering. In Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 866–874. <br/>[6] Yao Zhao, Xiaochuan Ni, Yuanyuan Ding, and Qifa Ke. 2018. Paragraph-level neural question generation with maxout pointer and gated self-attention networks. In Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3901– 3910.<br/>[7] Xinya Du and Claire Cardie. 2018. Harvesting paragraph-level question-answer pairs from wikipedia. In Annual Meeting of the Association for Computational Linguistics (ACL), pages 1907– 1917.<br/>[8] Sandeep Subramanian, Tong Wang, Xingdi Yuan, Saizheng Zhang, Adam Trischler, and Yoshua Bengio. 2018. Neural models for key phrase extraction and question generation. In Workshop on Machine Reading for Question Answering@ACL, pages 78– 88.<br/>[9]Yanghoon Kim, Hwanhee Lee, Joongbo Shin, and Kyomin Jung. 2019. Improving neural question generation using answer separation. In AAAI Conference on Artificial Intelligence (AAAI).<br/>[10] Mrinmaya Sachan and Eric P. Xing. 2018. Selftraining for jointly learning to ask and answer questions. In Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT), pages 629–640.</blockquote><p></p>", 
            "topic": [
                {
                    "tag": "自然语言处理", 
                    "tagLink": "https://api.zhihu.com/topics/19560026"
                }
            ], 
            "comments": [
                {
                    "userName": "曾明", 
                    "userLink": "https://www.zhihu.com/people/2494673a2fbd632a07dcb65566c10690", 
                    "content": "<p>请教一下 table2 中为什么说 song (2018) 的方法没有用到 Paragraph-level context</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "Chevalier", 
                            "userLink": "https://www.zhihu.com/people/eba7ab78a0341c3ba393073fe68782d0", 
                            "content": "<p>这估计是作者忘填了</p>", 
                            "likes": 0, 
                            "replyToAuthor": "曾明"
                        }, 
                        {
                            "userName": "曾明", 
                            "userLink": "https://www.zhihu.com/people/2494673a2fbd632a07dcb65566c10690", 
                            "content": "<p>害怕是我Paragraph-level context的理解有误差= =</p>", 
                            "likes": 0, 
                            "replyToAuthor": "Chevalier"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/76908626", 
            "userName": "Chevalier", 
            "userLink": "https://www.zhihu.com/people/eba7ab78a0341c3ba393073fe68782d0", 
            "upvote": 2, 
            "title": "《学会提问：用于阅读理解的自然问题生成》PPT", 
            "content": "<p><b>由于研究项目需要，最近主要阅读问题生成这一方向的论文，欢迎一起交流学习~</b></p><p><b>本篇论文主要是将机器翻译Seq2Seq那一套用到问题生成这个新任务上，比较简单，就列一下做的PPT</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-b70ae2812652cd134364a05e7cbe1f44_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb\" width=\"960\" data-original=\"https://pic1.zhimg.com/v2-b70ae2812652cd134364a05e7cbe1f44_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;960&#39; height=&#39;600&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"960\" data-original=\"https://pic1.zhimg.com/v2-b70ae2812652cd134364a05e7cbe1f44_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-b70ae2812652cd134364a05e7cbe1f44_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-326fd5e1fe73fb90658e9be1225aa643_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb\" width=\"960\" data-original=\"https://pic4.zhimg.com/v2-326fd5e1fe73fb90658e9be1225aa643_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;960&#39; height=&#39;600&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"960\" data-original=\"https://pic4.zhimg.com/v2-326fd5e1fe73fb90658e9be1225aa643_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-326fd5e1fe73fb90658e9be1225aa643_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-d456e48bd7c6f7485031133bad324710_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb\" width=\"960\" data-original=\"https://pic1.zhimg.com/v2-d456e48bd7c6f7485031133bad324710_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;960&#39; height=&#39;600&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"960\" data-original=\"https://pic1.zhimg.com/v2-d456e48bd7c6f7485031133bad324710_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-d456e48bd7c6f7485031133bad324710_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-11fe9c2b4fd2d8c3c26064e5fffb4d71_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb\" width=\"960\" data-original=\"https://pic2.zhimg.com/v2-11fe9c2b4fd2d8c3c26064e5fffb4d71_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;960&#39; height=&#39;600&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"960\" data-original=\"https://pic2.zhimg.com/v2-11fe9c2b4fd2d8c3c26064e5fffb4d71_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-11fe9c2b4fd2d8c3c26064e5fffb4d71_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-0baa5364afc5e34246ac843ec538d497_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb\" width=\"960\" data-original=\"https://pic4.zhimg.com/v2-0baa5364afc5e34246ac843ec538d497_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;960&#39; height=&#39;600&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"960\" data-original=\"https://pic4.zhimg.com/v2-0baa5364afc5e34246ac843ec538d497_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-0baa5364afc5e34246ac843ec538d497_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-a45f0841b2b513656a64ecd91ee16b7d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb\" width=\"960\" data-original=\"https://pic2.zhimg.com/v2-a45f0841b2b513656a64ecd91ee16b7d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;960&#39; height=&#39;600&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"960\" data-original=\"https://pic2.zhimg.com/v2-a45f0841b2b513656a64ecd91ee16b7d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-a45f0841b2b513656a64ecd91ee16b7d_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-2bb550a09d3763faf7f2e61aa3f33d9e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb\" width=\"960\" data-original=\"https://pic3.zhimg.com/v2-2bb550a09d3763faf7f2e61aa3f33d9e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;960&#39; height=&#39;600&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"960\" data-original=\"https://pic3.zhimg.com/v2-2bb550a09d3763faf7f2e61aa3f33d9e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-2bb550a09d3763faf7f2e61aa3f33d9e_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-440404a1c2daa66df867a022049f4995_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb\" width=\"960\" data-original=\"https://pic2.zhimg.com/v2-440404a1c2daa66df867a022049f4995_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;960&#39; height=&#39;600&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"960\" data-original=\"https://pic2.zhimg.com/v2-440404a1c2daa66df867a022049f4995_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-440404a1c2daa66df867a022049f4995_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-eb9de9b5280013a04745cade9e9b602f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb\" width=\"960\" data-original=\"https://pic4.zhimg.com/v2-eb9de9b5280013a04745cade9e9b602f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;960&#39; height=&#39;600&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"960\" data-original=\"https://pic4.zhimg.com/v2-eb9de9b5280013a04745cade9e9b602f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-eb9de9b5280013a04745cade9e9b602f_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-e03310da5add1cd4620a780b50d56f8f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb\" width=\"960\" data-original=\"https://pic4.zhimg.com/v2-e03310da5add1cd4620a780b50d56f8f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;960&#39; height=&#39;600&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"960\" data-original=\"https://pic4.zhimg.com/v2-e03310da5add1cd4620a780b50d56f8f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-e03310da5add1cd4620a780b50d56f8f_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-f32deeed0c6c2cdce349070cae5be907_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb\" width=\"960\" data-original=\"https://pic4.zhimg.com/v2-f32deeed0c6c2cdce349070cae5be907_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;960&#39; height=&#39;600&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"960\" data-original=\"https://pic4.zhimg.com/v2-f32deeed0c6c2cdce349070cae5be907_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-f32deeed0c6c2cdce349070cae5be907_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-5da3db3a8cf386e34f5921310edf0194_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb\" width=\"960\" data-original=\"https://pic1.zhimg.com/v2-5da3db3a8cf386e34f5921310edf0194_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;960&#39; height=&#39;600&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"960\" data-original=\"https://pic1.zhimg.com/v2-5da3db3a8cf386e34f5921310edf0194_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-5da3db3a8cf386e34f5921310edf0194_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-1884b8014143681fe946ec58a17ffac5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb\" width=\"960\" data-original=\"https://pic2.zhimg.com/v2-1884b8014143681fe946ec58a17ffac5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;960&#39; height=&#39;600&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"960\" data-original=\"https://pic2.zhimg.com/v2-1884b8014143681fe946ec58a17ffac5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-1884b8014143681fe946ec58a17ffac5_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-9476829af5aae77f26e36bc2d64bf2de_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb\" width=\"960\" data-original=\"https://pic3.zhimg.com/v2-9476829af5aae77f26e36bc2d64bf2de_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;960&#39; height=&#39;600&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"960\" data-original=\"https://pic3.zhimg.com/v2-9476829af5aae77f26e36bc2d64bf2de_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-9476829af5aae77f26e36bc2d64bf2de_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-df3cf862cfa0e8ff90c4e02e16264bd9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb\" width=\"960\" data-original=\"https://pic2.zhimg.com/v2-df3cf862cfa0e8ff90c4e02e16264bd9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;960&#39; height=&#39;600&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"960\" data-original=\"https://pic2.zhimg.com/v2-df3cf862cfa0e8ff90c4e02e16264bd9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-df3cf862cfa0e8ff90c4e02e16264bd9_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-6f2688d97b41622e32e90ba8ab039362_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb\" width=\"960\" data-original=\"https://pic3.zhimg.com/v2-6f2688d97b41622e32e90ba8ab039362_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;960&#39; height=&#39;600&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"960\" data-original=\"https://pic3.zhimg.com/v2-6f2688d97b41622e32e90ba8ab039362_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-6f2688d97b41622e32e90ba8ab039362_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-742e7b4f5ac4147024fdefcf920d9a31_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb\" width=\"960\" data-original=\"https://pic2.zhimg.com/v2-742e7b4f5ac4147024fdefcf920d9a31_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;960&#39; height=&#39;600&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"960\" data-original=\"https://pic2.zhimg.com/v2-742e7b4f5ac4147024fdefcf920d9a31_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-742e7b4f5ac4147024fdefcf920d9a31_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-7105bd4df1a1b3bf0922b9612da38a8a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb\" width=\"960\" data-original=\"https://pic3.zhimg.com/v2-7105bd4df1a1b3bf0922b9612da38a8a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;960&#39; height=&#39;600&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"960\" data-original=\"https://pic3.zhimg.com/v2-7105bd4df1a1b3bf0922b9612da38a8a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-7105bd4df1a1b3bf0922b9612da38a8a_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-e58030e8fcbef25df622c510cc35871b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb\" width=\"960\" data-original=\"https://pic4.zhimg.com/v2-e58030e8fcbef25df622c510cc35871b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;960&#39; height=&#39;600&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"960\" data-original=\"https://pic4.zhimg.com/v2-e58030e8fcbef25df622c510cc35871b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-e58030e8fcbef25df622c510cc35871b_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-51ea62c8f420b496c5fa271c0d9ec936_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb\" width=\"960\" data-original=\"https://pic3.zhimg.com/v2-51ea62c8f420b496c5fa271c0d9ec936_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;960&#39; height=&#39;600&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"960\" data-original=\"https://pic3.zhimg.com/v2-51ea62c8f420b496c5fa271c0d9ec936_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-51ea62c8f420b496c5fa271c0d9ec936_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-fccc4ce31eb5a24b0ed47e02141b0ba2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb\" width=\"960\" data-original=\"https://pic3.zhimg.com/v2-fccc4ce31eb5a24b0ed47e02141b0ba2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;960&#39; height=&#39;600&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"960\" data-original=\"https://pic3.zhimg.com/v2-fccc4ce31eb5a24b0ed47e02141b0ba2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-fccc4ce31eb5a24b0ed47e02141b0ba2_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-996682adcb6adf318d69c37b46cbf468_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb\" width=\"960\" data-original=\"https://pic1.zhimg.com/v2-996682adcb6adf318d69c37b46cbf468_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;960&#39; height=&#39;600&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"960\" data-original=\"https://pic1.zhimg.com/v2-996682adcb6adf318d69c37b46cbf468_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-996682adcb6adf318d69c37b46cbf468_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-eec4fef7c3b6480a2785b48df10b2525_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb\" width=\"960\" data-original=\"https://pic2.zhimg.com/v2-eec4fef7c3b6480a2785b48df10b2525_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;960&#39; height=&#39;600&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"960\" data-original=\"https://pic2.zhimg.com/v2-eec4fef7c3b6480a2785b48df10b2525_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-eec4fef7c3b6480a2785b48df10b2525_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-39c8aa9712257f816deab910e48e69aa_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb\" width=\"960\" data-original=\"https://pic3.zhimg.com/v2-39c8aa9712257f816deab910e48e69aa_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;960&#39; height=&#39;600&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"960\" data-original=\"https://pic3.zhimg.com/v2-39c8aa9712257f816deab910e48e69aa_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-39c8aa9712257f816deab910e48e69aa_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "自然语言处理", 
                    "tagLink": "https://api.zhihu.com/topics/19560026"
                }, 
                {
                    "tag": "阅读理解", 
                    "tagLink": "https://api.zhihu.com/topics/20015446"
                }, 
                {
                    "tag": "问答系统", 
                    "tagLink": "https://api.zhihu.com/topics/19571693"
                }
            ], 
            "comments": []
        }
    ], 
    "url": "https://zhuanlan.zhihu.com/c_1142122691518693376"
}
