{
    "title": "Machine Learning", 
    "description": "", 
    "followers": [
        "https://www.zhihu.com/people/tu-dou-chao-ma-ling-shu-32", 
        "https://www.zhihu.com/people/xun-gu-13", 
        "https://www.zhihu.com/people/humaolin", 
        "https://www.zhihu.com/people/lianyuhu", 
        "https://www.zhihu.com/people/wilson22-22", 
        "https://www.zhihu.com/people/zhu-forrest", 
        "https://www.zhihu.com/people/ah1991", 
        "https://www.zhihu.com/people/liu-di-xian-85", 
        "https://www.zhihu.com/people/sasuke-wh", 
        "https://www.zhihu.com/people/long-fei-25-1", 
        "https://www.zhihu.com/people/qin-mu-yu-fei-5", 
        "https://www.zhihu.com/people/teng-91-41", 
        "https://www.zhihu.com/people/ybsolar", 
        "https://www.zhihu.com/people/su-wei-feng-47", 
        "https://www.zhihu.com/people/li-ying-34-84", 
        "https://www.zhihu.com/people/zhang-gao-jian-89", 
        "https://www.zhihu.com/people/yi-da-88-9", 
        "https://www.zhihu.com/people/Electrical_Brandy", 
        "https://www.zhihu.com/people/lu-xin-lei-44-91", 
        "https://www.zhihu.com/people/liu-wei-feng-35", 
        "https://www.zhihu.com/people/ikara-tasi", 
        "https://www.zhihu.com/people/li-ming-qiang-32-67", 
        "https://www.zhihu.com/people/xu-zi-xiao-56-45", 
        "https://www.zhihu.com/people/yu-mo-ye-81", 
        "https://www.zhihu.com/people/whutxutao", 
        "https://www.zhihu.com/people/yu-wan-67-69", 
        "https://www.zhihu.com/people/gu-wei-32-82", 
        "https://www.zhihu.com/people/zhang-nan-nan-19-28", 
        "https://www.zhihu.com/people/phenix-19", 
        "https://www.zhihu.com/people/das-universum", 
        "https://www.zhihu.com/people/zhu-you-wei-9", 
        "https://www.zhihu.com/people/zhao-xing-bo-85", 
        "https://www.zhihu.com/people/jiangxu-98", 
        "https://www.zhihu.com/people/derjungemann", 
        "https://www.zhihu.com/people/pang-shou-qun", 
        "https://www.zhihu.com/people/yu-liu-19-66-93", 
        "https://www.zhihu.com/people/shi-kong-chuan-suo-64", 
        "https://www.zhihu.com/people/xiao-han-13-60", 
        "https://www.zhihu.com/people/dong-feng-66-72", 
        "https://www.zhihu.com/people/pursuit330", 
        "https://www.zhihu.com/people/wenk-18", 
        "https://www.zhihu.com/people/hc2018-85", 
        "https://www.zhihu.com/people/mel-tor", 
        "https://www.zhihu.com/people/fly-zhai", 
        "https://www.zhihu.com/people/he-tong-88-36", 
        "https://www.zhihu.com/people/what-74-83-23", 
        "https://www.zhihu.com/people/xiong-fei-1-73", 
        "https://www.zhihu.com/people/calvinjku", 
        "https://www.zhihu.com/people/tang-shao-6-46", 
        "https://www.zhihu.com/people/uzetatu", 
        "https://www.zhihu.com/people/mingming-31-52", 
        "https://www.zhihu.com/people/lao-guo-2-44", 
        "https://www.zhihu.com/people/liu-jian-87-5-7", 
        "https://www.zhihu.com/people/ya-yue-50", 
        "https://www.zhihu.com/people/han-rong-12-50", 
        "https://www.zhihu.com/people/yokiyu", 
        "https://www.zhihu.com/people/chenlv-78", 
        "https://www.zhihu.com/people/di-shui-qin-quan", 
        "https://www.zhihu.com/people/jordan-belfort", 
        "https://www.zhihu.com/people/ren-yu-xian-da-shen", 
        "https://www.zhihu.com/people/yiling-zhimu", 
        "https://www.zhihu.com/people/liu-music", 
        "https://www.zhihu.com/people/alicili", 
        "https://www.zhihu.com/people/mumu-54-31", 
        "https://www.zhihu.com/people/lu-xiao-78-88", 
        "https://www.zhihu.com/people/pang-zi-56-70", 
        "https://www.zhihu.com/people/bei-zi-ya-ya", 
        "https://www.zhihu.com/people/sha-bu-38", 
        "https://www.zhihu.com/people/zhao-wei-11594", 
        "https://www.zhihu.com/people/lch-78-85", 
        "https://www.zhihu.com/people/chengshilieren", 
        "https://www.zhihu.com/people/panovr", 
        "https://www.zhihu.com/people/ke-wu-88", 
        "https://www.zhihu.com/people/dgjk1010", 
        "https://www.zhihu.com/people/chen-zhao-wei-16-2", 
        "https://www.zhihu.com/people/kai-xin-qi-shui-ren-ling-chu", 
        "https://www.zhihu.com/people/ma-yu-xiang-4", 
        "https://www.zhihu.com/people/li-shi-lin-93-31", 
        "https://www.zhihu.com/people/liu-feng-96-66-33", 
        "https://www.zhihu.com/people/DINGKAIMENG", 
        "https://www.zhihu.com/people/hui-fei-de-zhu-45-27", 
        "https://www.zhihu.com/people/liu-fei-94-95", 
        "https://www.zhihu.com/people/liu-zong-35-60", 
        "https://www.zhihu.com/people/xyc-84-81", 
        "https://www.zhihu.com/people/moni-gg", 
        "https://www.zhihu.com/people/zhe-da-gai-jiu-shi-ren-sheng-ba", 
        "https://www.zhihu.com/people/yang-qing-58-91", 
        "https://www.zhihu.com/people/yi-zhi-61-29", 
        "https://www.zhihu.com/people/butiange", 
        "https://www.zhihu.com/people/duan-xing-99", 
        "https://www.zhihu.com/people/ka-ka-8-86", 
        "https://www.zhihu.com/people/huang-xiang-38", 
        "https://www.zhihu.com/people/william-zhu-82", 
        "https://www.zhihu.com/people/fu-gang-74-17", 
        "https://www.zhihu.com/people/a-dong-71-27", 
        "https://www.zhihu.com/people/rockcheung", 
        "https://www.zhihu.com/people/xiao-e-mo-38-29", 
        "https://www.zhihu.com/people/li-xiang-73-34", 
        "https://www.zhihu.com/people/she-liang", 
        "https://www.zhihu.com/people/jiahao-lee-73", 
        "https://www.zhihu.com/people/superpermutation", 
        "https://www.zhihu.com/people/heyang-36", 
        "https://www.zhihu.com/people/wings1972", 
        "https://www.zhihu.com/people/xin-27-26", 
        "https://www.zhihu.com/people/Turiny", 
        "https://www.zhihu.com/people/he-tao-50-23", 
        "https://www.zhihu.com/people/wei-jin-jie-32", 
        "https://www.zhihu.com/people/xu-yin-da-58", 
        "https://www.zhihu.com/people/yu-meng-yu-39", 
        "https://www.zhihu.com/people/sun-yan-90-29", 
        "https://www.zhihu.com/people/kevin-hill", 
        "https://www.zhihu.com/people/pken", 
        "https://www.zhihu.com/people/liu-hua-73-44", 
        "https://www.zhihu.com/people/lan-lan-lan-lan-96-82", 
        "https://www.zhihu.com/people/wizardforcel", 
        "https://www.zhihu.com/people/zhou-xin-50-24-50", 
        "https://www.zhihu.com/people/chu-sheng-niu-du-46", 
        "https://www.zhihu.com/people/qia-qia-qia-qia-qia-qia-27", 
        "https://www.zhihu.com/people/zhou-an-hao-78", 
        "https://www.zhihu.com/people/cathy722", 
        "https://www.zhihu.com/people/YangJangBo", 
        "https://www.zhihu.com/people/applover-5", 
        "https://www.zhihu.com/people/zheng-guo-dong-82", 
        "https://www.zhihu.com/people/yildhd-wang", 
        "https://www.zhihu.com/people/li-ji-13", 
        "https://www.zhihu.com/people/sw3185", 
        "https://www.zhihu.com/people/yue-sun-7-88", 
        "https://www.zhihu.com/people/fang-duo-2", 
        "https://www.zhihu.com/people/lst-92-24", 
        "https://www.zhihu.com/people/yzp1108", 
        "https://www.zhihu.com/people/dou-bao-10-68", 
        "https://www.zhihu.com/people/li-da-da-30-58", 
        "https://www.zhihu.com/people/qi-sun-50"
    ], 
    "article": [
        {
            "url": "https://zhuanlan.zhihu.com/p/73025638", 
            "userName": "胡今朝", 
            "userLink": "https://www.zhihu.com/people/fb128b56fdfed86d1a2c8237571d133f", 
            "upvote": 1, 
            "title": "常用激活函数的比较", 
            "content": "<p>转载自:</p><p><a href=\"https://zhuanlan.zhihu.com/p/32610035\" class=\"internal\"><span class=\"invisible\">https://</span><span class=\"visible\">zhuanlan.zhihu.com/p/32</span><span class=\"invisible\">610035</span><span class=\"ellipsis\"></span></a></p><p>每个激活函数的输入都是一个数字，然后对其进行某种固定的数学操作。激活函数给神经元引入了非线性因素，如果不用激活函数的话，无论神经网络有多少层，输出都是输入的线性组合。</p><p>激活函数的发展经历了Sigmoid -&gt; Tanh -&gt; ReLU -&gt; Leaky ReLU -&gt; Maxout这样的过程，还有一个特殊的激活函数Softmax，因为它只会被用在网络中的最后一层，用来进行最后的分类和归一化。本文简单来梳理这些激活函数是如何一步一步演变而来的。</p><p>总结如下，先贴图：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-665f3304d409b17471dd0b7258818e0a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"769\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic3.zhimg.com/v2-665f3304d409b17471dd0b7258818e0a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;640&#39; height=&#39;769&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"769\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https://pic3.zhimg.com/v2-665f3304d409b17471dd0b7258818e0a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-665f3304d409b17471dd0b7258818e0a_b.jpg\"/></figure><hr/><p><b>Sigmoid</b></p><p><b>数学公式：</b></p><p>sigmoid非线性函数的数学公式是</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Csigma%28x%29%3D%5Cfrac%7B1%7D%7B1%2Be%5E%7B-x%7D%7D%5C%5C\" alt=\"\\sigma(x)=\\frac{1}{1+e^{-x}}\\\\\" eeimg=\"1\"/></p><p>函数图像如下图所示。它输入实数值并将其“挤压”到0到1范围内，适合输出为概率的情况，但是现在已经很少有人在构建神经网络的过程中使用sigmoid。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-4b26e9fdd710e3621467de2fa935d63f_b.jpg\" data-size=\"normal\" data-rawwidth=\"1298\" data-rawheight=\"864\" class=\"origin_image zh-lightbox-thumb\" width=\"1298\" data-original=\"https://pic4.zhimg.com/v2-4b26e9fdd710e3621467de2fa935d63f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1298&#39; height=&#39;864&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1298\" data-rawheight=\"864\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1298\" data-original=\"https://pic4.zhimg.com/v2-4b26e9fdd710e3621467de2fa935d63f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-4b26e9fdd710e3621467de2fa935d63f_b.jpg\"/><figcaption>Sigmoid函数图像</figcaption></figure><p><b>存在问题：</b></p><ul><li><i>Sigmoid函数饱和使梯度消失</i>。当神经元的激活在接近0或1处时会饱和，在这些区域梯度几乎为0，这就会导致梯度消失，几乎就有没有信号通过神经传回上一层。</li><li><i>Sigmoid函数的输出不是零中心的</i>。因为如果输入神经元的数据总是正数，那么关于<img src=\"https://www.zhihu.com/equation?tex=w\" alt=\"w\" eeimg=\"1\"/>的梯度在反向传播的过程中，将会要么全部是正数，要么全部是负数，这将会导致梯度下降权重更新时出现z字型的下降。</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p><b>Tanh</b></p><p><b>数学公式：</b></p><p>Tanh非线性函数的数学公式是</p><p><img src=\"https://www.zhihu.com/equation?tex=tanh%28x%29%3D2%5Csigma%282x%29-1%5C%5C\" alt=\"tanh(x)=2\\sigma(2x)-1\\\\\" eeimg=\"1\"/></p><p>Tanh非线性函数图像如下图所示，它将实数值压缩到[-1,1]之间。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-c14a20c9ccd9c724e603aafdc11dfdb8_b.jpg\" data-size=\"normal\" data-rawwidth=\"1288\" data-rawheight=\"858\" class=\"origin_image zh-lightbox-thumb\" width=\"1288\" data-original=\"https://pic1.zhimg.com/v2-c14a20c9ccd9c724e603aafdc11dfdb8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1288&#39; height=&#39;858&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1288\" data-rawheight=\"858\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1288\" data-original=\"https://pic1.zhimg.com/v2-c14a20c9ccd9c724e603aafdc11dfdb8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-c14a20c9ccd9c724e603aafdc11dfdb8_b.jpg\"/><figcaption>Tanh函数图像</figcaption></figure><p><b>存在问题：</b></p><p>Tanh解决了Sigmoid的输出是不是零中心的问题，但仍然存在饱和问题。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>为了防止饱和，现在主流的做法会在激活函数前多做一步<i>batch normalization</i>，尽可能保证每一层网络的输入具有均值较小的、零中心的分布。</b></p><hr/><p><b>ReLU</b></p><p><b>数学公式：</b></p><p>函数公式是<img src=\"https://www.zhihu.com/equation?tex=f%28x%29%3Dmax%280%2Cx%29%5C%5C\" alt=\"f(x)=max(0,x)\\\\\" eeimg=\"1\"/></p><p>ReLU非线性函数图像如下图所示。相较于sigmoid和tanh函数，ReLU对于随机梯度下降的收敛有巨大的加速作用；sigmoid和tanh在求导时含有指数运算，而ReLU求导几乎不存在任何计算量。</p><p>对比sigmoid类函数主要变化是：</p><p>1）单侧抑制；</p><p>2）相对宽阔的兴奋边界；</p><p>3）稀疏激活性。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-e05dd5aa9a1674395a279da6645c060f_b.jpg\" data-size=\"normal\" data-rawwidth=\"1372\" data-rawheight=\"992\" class=\"origin_image zh-lightbox-thumb\" width=\"1372\" data-original=\"https://pic4.zhimg.com/v2-e05dd5aa9a1674395a279da6645c060f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1372&#39; height=&#39;992&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1372\" data-rawheight=\"992\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1372\" data-original=\"https://pic4.zhimg.com/v2-e05dd5aa9a1674395a279da6645c060f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-e05dd5aa9a1674395a279da6645c060f_b.jpg\"/><figcaption>ReLU函数图像</figcaption></figure><p><b>存在问题：</b></p><p>ReLU单元比较脆弱并且可能“死掉”，而且是不可逆的，因此导致了数据多样化的丢失。通过合理设置学习率，会降低神经元“死掉”的概率。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>Leaky ReLU</b></p><p><b>数学公式：</b></p><p>函数公式是</p><p><img src=\"https://www.zhihu.com/equation?tex=f%28y%29%3Dmax%28%5Cvarepsilon+y%2Cy%29%5C%5C\" alt=\"f(y)=max(\\varepsilon y,y)\\\\\" eeimg=\"1\"/></p><p>其中 <img src=\"https://www.zhihu.com/equation?tex=%5Cvarepsilon\" alt=\"\\varepsilon\" eeimg=\"1\"/> 是很小的负数梯度值，比如0.01，Leaky ReLU非线性函数图像如下图所示。这样做目的是使负轴信息不会全部丢失，解决了ReLU神经元“死掉”的问题。更进一步的方法是PReLU，即把 <img src=\"https://www.zhihu.com/equation?tex=%5Cvarepsilon\" alt=\"\\varepsilon\" eeimg=\"1\"/> 当做每个神经元中的一个参数，是可以通过梯度下降求解的。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-0f6aa9f364b302da5826a4108cb899cb_b.jpg\" data-size=\"normal\" data-rawwidth=\"500\" data-rawheight=\"437\" class=\"origin_image zh-lightbox-thumb\" width=\"500\" data-original=\"https://pic4.zhimg.com/v2-0f6aa9f364b302da5826a4108cb899cb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;500&#39; height=&#39;437&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"500\" data-rawheight=\"437\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"500\" data-original=\"https://pic4.zhimg.com/v2-0f6aa9f364b302da5826a4108cb899cb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-0f6aa9f364b302da5826a4108cb899cb_b.jpg\"/><figcaption>Leaky ReLU函数图像</figcaption></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>Maxout</b></p><p><b>数学公式：</b></p><p>Maxout是对ReLU和leaky ReLU的一般化归纳，函数公式是</p><p><img src=\"https://www.zhihu.com/equation?tex=f%28x%29+%3D+max%28w%5ET_1x%2Bb_1%2Cw%5ET_2x%2Bb_2%29%5C%5C\" alt=\"f(x) = max(w^T_1x+b_1,w^T_2x+b_2)\\\\\" eeimg=\"1\"/></p><p>Maxout非线性函数图像如下图所示。Maxout具有ReLU的优点，如计算简单，不会 saturation，同时又没有ReLU的一些缺点，如容易go die。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-690b93748aa2ad63f3939760f7207aa0_b.jpg\" data-size=\"normal\" data-rawwidth=\"889\" data-rawheight=\"217\" class=\"origin_image zh-lightbox-thumb\" width=\"889\" data-original=\"https://pic1.zhimg.com/v2-690b93748aa2ad63f3939760f7207aa0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;889&#39; height=&#39;217&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"889\" data-rawheight=\"217\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"889\" data-original=\"https://pic1.zhimg.com/v2-690b93748aa2ad63f3939760f7207aa0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-690b93748aa2ad63f3939760f7207aa0_b.jpg\"/><figcaption>Maxout函数图像</figcaption></figure><p><b>存在问题：</b></p><p>每个神经元的参数double，这就导致整体参数的数量激增。</p><hr/><p><b>Softmax</b></p><p><b>数学公式：</b></p><p>Softmax用于多分类神经网络输出，目的是让大的更大。函数公式是</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Csigma%28z%29_%7Bj%7D%3D%5Cfrac%7Be%5E%7Bz_%7Bj%7D%7D%7D%7B%5Csum_%7Bk%3D1%7D%5E%7BK%7D%7Be%5E%7Bz_%7Bk%7D%7D%7D%7D%5C%5C\" alt=\"\\sigma(z)_{j}=\\frac{e^{z_{j}}}{\\sum_{k=1}^{K}{e^{z_{k}}}}\\\\\" eeimg=\"1\"/></p><p>示意图如下。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-68a7dfdf613d8cd43f0569184b206c5c_b.jpg\" data-size=\"normal\" data-rawwidth=\"643\" data-rawheight=\"387\" class=\"origin_image zh-lightbox-thumb\" width=\"643\" data-original=\"https://pic1.zhimg.com/v2-68a7dfdf613d8cd43f0569184b206c5c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;643&#39; height=&#39;387&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"643\" data-rawheight=\"387\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"643\" data-original=\"https://pic1.zhimg.com/v2-68a7dfdf613d8cd43f0569184b206c5c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-68a7dfdf613d8cd43f0569184b206c5c_b.jpg\"/><figcaption>Softmax示意图</figcaption></figure><p>Softmax是Sigmoid的扩展，当类别数k＝2时，Softmax回归退化为Logistic回归。</p><p></p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50637853", 
            "userName": "胡今朝", 
            "userLink": "https://www.zhihu.com/people/fb128b56fdfed86d1a2c8237571d133f", 
            "upvote": 2, 
            "title": "深度学习框架选择", 
            "content": "<p>从最开始学习tensorflow到现在基本熟练了，但是经常看论文会遇到各种各样的语言或者框架写的代码，所以仅仅一个tensorflow是不够用的，最近看一篇paper是用的torch写的，自己赶紧去学习lua语言和torch框架，却发现torch不在更新了，所以为了一篇paper的源代码学习一门语言和一个框架是不值得的，这篇文章记录一下这两天浏览的各种各样框架的介绍，以便于自己最终选择扩展学习的深度学习框架；</p><hr/><p>深度学习的框架有哪些？</p><p><b>👍🏿</b>👍🏿<b> TensorFlow</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-82550df4d26954d5170272a8d65e682d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"674\" data-rawheight=\"401\" class=\"origin_image zh-lightbox-thumb\" width=\"674\" data-original=\"https://pic2.zhimg.com/v2-82550df4d26954d5170272a8d65e682d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;674&#39; height=&#39;401&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"674\" data-rawheight=\"401\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"674\" data-original=\"https://pic2.zhimg.com/v2-82550df4d26954d5170272a8d65e682d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-82550df4d26954d5170272a8d65e682d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>Theano （</b>停止更新了，所以不必转向这个框架）</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-2976a2f810339fb61cb38886dc4a8dce_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"669\" data-rawheight=\"580\" class=\"origin_image zh-lightbox-thumb\" width=\"669\" data-original=\"https://pic3.zhimg.com/v2-2976a2f810339fb61cb38886dc4a8dce_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;669&#39; height=&#39;580&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"669\" data-rawheight=\"580\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"669\" data-original=\"https://pic3.zhimg.com/v2-2976a2f810339fb61cb38886dc4a8dce_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-2976a2f810339fb61cb38886dc4a8dce_b.jpg\"/></figure><p><b>Keras（包装太多了，</b>所以不必转向这个框架<b>）</b></p><p><b>Lasagne</b>（基于Theano，它停止更新了，所以不必转向这个框架）</p><p><b>👍🏿 Caffe2（候选学习框架）</b></p><p><b>DSSTNE（太小众了，总之很少在网上看到，</b>所以不必转向这个框架<b>）</b></p><p><b>Torch（已经移植到pytorch上了，</b>所以不必转向这个框架<b>）</b></p><p class=\"ztext-empty-paragraph\"><br/></p><p>👍🏿👍🏿<b> Pytorch（各种大牛都建议学习这个，因为支持动态图，就是在定义后可以修改图，这点在自己使用tensorflow的时候感受很明显，比如输入的尺寸就没有办法改变）</b></p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>👍🏿 MXNet（候选学习框架）</b></p><p>MXNet 是亚马逊（Amazon）选择的深度学习库，并且也许是最优秀的库之一</p><p><b>DL4J（用在java上的，暂时不必学习）</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-50273db54373b4ca8e83501f50465fdd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"515\" data-rawheight=\"156\" class=\"origin_image zh-lightbox-thumb\" width=\"515\" data-original=\"https://pic2.zhimg.com/v2-50273db54373b4ca8e83501f50465fdd_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;515&#39; height=&#39;156&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"515\" data-rawheight=\"156\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"515\" data-original=\"https://pic2.zhimg.com/v2-50273db54373b4ca8e83501f50465fdd_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-50273db54373b4ca8e83501f50465fdd_b.jpg\"/></figure><p><b>Cognitive Toolkit（CNTK，同为大厂，微软的这个后起之秀的框架，并不是大多数的选择，还是不转向这个，毕竟能力有限）</b></p><hr/><p>总结下来</p><p>精进tensorflow，着手学习pytorch，大致学习了一下pytorch的官方tutorial感觉很简单，放弃正在学习的lua和torch，虽然很简单，但是毕竟是在浪费精力；</p><p>如果以后有多余的精力的话可以继续学习caffe2和Mxnet；</p><p>选择的目的是为了能够快速上手论文所给的源代码，而不是看移植的二手代码</p><hr/><p>纯属个人笔记，小白屌丝一个，不构成任何其他人选择的建议。</p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50497470", 
            "userName": "胡今朝", 
            "userLink": "https://www.zhihu.com/people/fb128b56fdfed86d1a2c8237571d133f", 
            "upvote": 2, 
            "title": "LS-GAN最小二乘生成对抗网络", 
            "content": "<p></p><a href=\"https://zhuanlan.zhihu.com/p/46418234\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic4.zhimg.com/v2-7ebe2c7ae55eeb63fd1afb3fb4725c97_180x120.jpg\" data-image-width=\"900\" data-image-height=\"500\" class=\"internal\">张俊：经典论文复现 | LSGAN：最小二乘生成对抗网络</a><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-0e744996e00460346ddf1f5ef09fdac6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"715\" data-rawheight=\"293\" class=\"origin_image zh-lightbox-thumb\" width=\"715\" data-original=\"https://pic3.zhimg.com/v2-0e744996e00460346ddf1f5ef09fdac6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;715&#39; height=&#39;293&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"715\" data-rawheight=\"293\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"715\" data-original=\"https://pic3.zhimg.com/v2-0e744996e00460346ddf1f5ef09fdac6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-0e744996e00460346ddf1f5ef09fdac6_b.jpg\"/></figure><p>传统的生成对抗网络面临的问题是：sigmoid传递梯度非常容易进入饱和区</p><p>我们知道传统 GAN 的训练过程十分不稳定，这很大程度上是因为它的目标函数，尤其是在最小化目标函数时可能发生梯度弥散，使其很难再去更新生成器。而论文指出 LSGANs 可以解决这个问题，因为 LSGANs 会惩罚那些远离决策边界的样本，这些样本的梯度是梯度下降的决定方向。</p><p>论文指出因为传统 GAN 辨别器 D 使用的是 sigmoid 函数，并且由于 sigmoid 函数饱和得十分迅速，所以即使是十分小的数据点 x，该函数也会迅速忽略样本 x 到决策边界 w 的距离。这就意味着 sigmoid 函数本质上不会惩罚远离决策边界的样本，并且也说明我们满足于将 x 标注正确，因此辨别器 D 的梯度就会很快地下降到 0。</p><p>我们可以认为，交叉熵并不关心距离，而是仅仅关注于是否正确分类。正如论文作者在下图中所指出的那样，交叉熵损失很容易就达到饱和状态，最小二乘损失只在一点达到饱和，作者认为这样训练会更加稳定。</p><a class=\"video-box\" href=\"https://link.zhihu.com/?target=https%3A//www.zhihu.com/video/1048589473755041792\" target=\"_blank\" data-video-id=\"\" data-video-playable=\"true\" data-name=\"\" data-poster=\"https://pic3.zhimg.com/v2-053f0341d4ea552017e45d6433337f30.jpg\" data-lens-id=\"1048589473755041792\"><img class=\"thumbnail\" src=\"https://pic3.zhimg.com/v2-053f0341d4ea552017e45d6433337f30.jpg\"/><span class=\"content\"><span class=\"title\"><span class=\"z-ico-extern-gray\"></span><span class=\"z-ico-extern-blue\"></span></span><span class=\"url\"><span class=\"z-ico-video\"></span>https://www.zhihu.com/video/1048589473755041792</span></span></a><hr/><p>这个就有点儿像给权重加惩罚的那种方式，最小化某个值-1的平方，这样某个值就会靠近1</p><hr/><a href=\"https://link.zhihu.com/?target=https%3A//github.com/meliketoy/LSGAN.pytorch\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/meliketoy/LS</span><span class=\"invisible\">GAN.pytorch</span><span class=\"ellipsis\"></span></a><p></p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50492258", 
            "userName": "胡今朝", 
            "userLink": "https://www.zhihu.com/people/fb128b56fdfed86d1a2c8237571d133f", 
            "upvote": 1, 
            "title": "距离和相似度度量方法", 
            "content": "<h2><b>距离和相似度度量方法的引出</b></h2><p>在机器学习和数据挖掘中，我们经常需要知道个体间差异的大小，进而评价个体的相似性和类别。最常见的是数据分析中的相关分析，数据挖掘中的分类和聚类算法，如 K 最近邻（KNN）和 K 均值（K-Means）等等。</p><hr/><h2><b>距离度量公理Axioms of Distance Measures</b></h2><p>一般而言，定义一个距离函数 d(x,y), 需要满足下面几个准则：</p><p>1) d(x,y) = 0  if x = y                  // 到自己的距离为0</p><p>2) d(x,y) &gt;= 0                  // 距离非负</p><p>3) d(x,y) = d(y,x)                   // 对称性: 如果 A 到 B 距离是 a，那么 B 到 A 的距离也应该是 a</p><p>4) d(x,k)+ d(k,y) &gt;= d(x,y)    // 三角形法则triangle inequality: (两边之和大于第三边)</p><p>Note: if =  if and only if（当且仅当）</p><hr/><h2><b>欧氏距离度量</b></h2><p>欧拉距离，来自于欧式几何，在数学上也可以成为范数。</p><p>r范数就是x,y每个维度差距上取r次方加和后再开r次方根。</p><p>L∞范数：向量各个元素求绝对值，最大那个元素的绝对值。也就是x，y在任意维度上差距最大的那个值。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-fd784312bc92c1886407de3ab9303bab_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"543\" data-rawheight=\"87\" class=\"origin_image zh-lightbox-thumb\" width=\"543\" data-original=\"https://pic4.zhimg.com/v2-fd784312bc92c1886407de3ab9303bab_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;543&#39; height=&#39;87&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"543\" data-rawheight=\"87\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"543\" data-original=\"https://pic4.zhimg.com/v2-fd784312bc92c1886407de3ab9303bab_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-fd784312bc92c1886407de3ab9303bab_b.jpg\"/></figure><hr/><h2><b>Cosine余弦相似度/向量内积</b></h2><p>适合高维度向量vectors的相似度计算</p><p>两个向量间的余弦值可以很容易地通过使用欧几里得点积和量级公式推导</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-4cb508f4577ba7fe856c4792ddcf221a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"172\" data-rawheight=\"20\" class=\"content_image\" width=\"172\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;172&#39; height=&#39;20&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"172\" data-rawheight=\"20\" class=\"content_image lazy\" width=\"172\" data-actualsrc=\"https://pic3.zhimg.com/v2-4cb508f4577ba7fe856c4792ddcf221a_b.jpg\"/></figure><hr/><h2><b>皮尔逊相关系数（Pearson correlation）</b></h2><p>数值型数据的相关性分析Correlation Analysis (Numerical Data)，从下面这个公式看出，它其实就是将数据归一化（数据减去其对应均值）后进行cosine相似度计算，所以叫centered cosine</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-7d64966cde88bb76096e010a099c9688_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"720\" data-rawheight=\"64\" class=\"origin_image zh-lightbox-thumb\" width=\"720\" data-original=\"https://pic1.zhimg.com/v2-7d64966cde88bb76096e010a099c9688_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;720&#39; height=&#39;64&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"720\" data-rawheight=\"64\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"720\" data-original=\"https://pic1.zhimg.com/v2-7d64966cde88bb76096e010a099c9688_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-7d64966cde88bb76096e010a099c9688_b.jpg\"/></figure><hr/><h2><b>kl散度/相对熵/kl距离</b></h2><p>KL散度是两个概率分布P和Q差别的非对称性的度量（designed to measure the difference between probability distributions）KL散度是用来度量使用基于Q的编码来编码来自P的样本平均所需的额外的位元数。 典型情况下，P表示数据的真实分布，Q表示数据的理论分布，模型分布，或P的近似分布</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-05ac225e9e47ec406a9e6df57d84918b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"313\" data-rawheight=\"87\" class=\"content_image\" width=\"313\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;313&#39; height=&#39;87&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"313\" data-rawheight=\"87\" class=\"content_image lazy\" width=\"313\" data-actualsrc=\"https://pic4.zhimg.com/v2-05ac225e9e47ec406a9e6df57d84918b_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "数据挖掘", 
                    "tagLink": "https://api.zhihu.com/topics/19553534"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50484278", 
            "userName": "胡今朝", 
            "userLink": "https://www.zhihu.com/people/fb128b56fdfed86d1a2c8237571d133f", 
            "upvote": 2, 
            "title": "GAN目标函数的本质（log-MLE）", 
            "content": "<h2><b>目标函数：</b></h2><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-13eb1fc6b25110c40f8bceed01c15d49_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"653\" data-rawheight=\"78\" class=\"origin_image zh-lightbox-thumb\" width=\"653\" data-original=\"https://pic2.zhimg.com/v2-13eb1fc6b25110c40f8bceed01c15d49_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;653&#39; height=&#39;78&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"653\" data-rawheight=\"78\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"653\" data-original=\"https://pic2.zhimg.com/v2-13eb1fc6b25110c40f8bceed01c15d49_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-13eb1fc6b25110c40f8bceed01c15d49_b.jpg\"/></figure><p>可以从两个角度理解：</p><p>1、交叉熵</p><p>2、最大似然估计</p><p>给定了x和G(z)求参数 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 使得x为真且G(z)为假发生的概率最大</p><p class=\"ztext-empty-paragraph\"><br/></p><p>最大似然估计的视频：</p><a class=\"video-box\" href=\"https://link.zhihu.com/?target=https%3A//www.zhihu.com/video/1048546217839218688\" target=\"_blank\" data-video-id=\"\" data-video-playable=\"true\" data-name=\"\" data-poster=\"https://pic2.zhimg.com/v2-9c373dcecc31b7fb9c4e833d1664cfc5.jpg\" data-lens-id=\"1048546217839218688\"><img class=\"thumbnail\" src=\"https://pic2.zhimg.com/v2-9c373dcecc31b7fb9c4e833d1664cfc5.jpg\"/><span class=\"content\"><span class=\"title\"><span class=\"z-ico-extern-gray\"></span><span class=\"z-ico-extern-blue\"></span></span><span class=\"url\"><span class=\"z-ico-video\"></span>https://www.zhihu.com/video/1048546217839218688</span></span></a><p></p>", 
            "topic": [
                {
                    "tag": "神经网络", 
                    "tagLink": "https://api.zhihu.com/topics/19607065"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50448422", 
            "userName": "胡今朝", 
            "userLink": "https://www.zhihu.com/people/fb128b56fdfed86d1a2c8237571d133f", 
            "upvote": 1, 
            "title": "微分几何学习笔记", 
            "content": "<p></p><a href=\"https://link.zhihu.com/?target=https%3A//www.bilibili.com/video/av1157024%3Ffrom%3Dsearch%26seid%3D13535179591724147659\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">bilibili.com/video/av11</span><span class=\"invisible\">57024?from=search&amp;seid=13535179591724147659</span><span class=\"ellipsis\"></span></a><h2><b>第一课 集合</b></h2><p>1、集合和元素的关系，集合和集合的关系：这个很简单</p><h2><b><i>2、什么是卡氏积？</i></b></h2><p>Cartesian product</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-bc1112918a5eca4d753a731b7a86a2c3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"751\" data-rawheight=\"49\" class=\"origin_image zh-lightbox-thumb\" width=\"751\" data-original=\"https://pic4.zhimg.com/v2-bc1112918a5eca4d753a731b7a86a2c3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;751&#39; height=&#39;49&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"751\" data-rawheight=\"49\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"751\" data-original=\"https://pic4.zhimg.com/v2-bc1112918a5eca4d753a731b7a86a2c3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-bc1112918a5eca4d753a731b7a86a2c3_b.jpg\"/></figure><p>3、Rn表示n维空间</p><p>4、R2 自然坐标</p><p>5、距离</p><p>6、map</p><p>7、f(x)  : f of x</p><h2>8、<b>两个集合的map关系</b></h2><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-cae94b1270e8ce56d9d5b3c45b585ead_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"160\" data-rawheight=\"53\" class=\"content_image\" width=\"160\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;160&#39; height=&#39;53&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"160\" data-rawheight=\"53\" class=\"content_image lazy\" width=\"160\" data-actualsrc=\"https://pic2.zhimg.com/v2-cae94b1270e8ce56d9d5b3c45b585ead_b.jpg\"/></figure><p>one to one map：image的inverse image至多有一个，可以没有</p><p>onto map：image必定有inverse iamge</p><h2><b>9、函数映射</b></h2><p>R-&gt;R</p><p>R2-&gt;R</p><p>Rn-&gt;Rm:m个n元函数</p><h2><b>10、连续C not</b></h2><p>11、</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-e6c46dd00aa207f146216cf88ee8b380_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"72\" data-rawheight=\"69\" class=\"content_image\" width=\"72\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;72&#39; height=&#39;69&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"72\" data-rawheight=\"69\" class=\"content_image lazy\" width=\"72\" data-actualsrc=\"https://pic1.zhimg.com/v2-e6c46dd00aa207f146216cf88ee8b380_b.jpg\"/></figure><p>12、开区间之并</p><p></p><p></p><p></p><p></p><p></p><p></p><p></p>", 
            "topic": [
                {
                    "tag": "高等数学", 
                    "tagLink": "https://api.zhihu.com/topics/19612131"
                }, 
                {
                    "tag": "微积分", 
                    "tagLink": "https://api.zhihu.com/topics/19558728"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": [
                {
                    "userName": "Rookie.Yao", 
                    "userLink": "https://www.zhihu.com/people/a2e600c986d8d16acf8484508586b842", 
                    "content": "就这也敢叫微分几何?", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "胡今朝", 
                            "userLink": "https://www.zhihu.com/people/fb128b56fdfed86d1a2c8237571d133f", 
                            "content": "一点点点集拓扑的概念而已，本来打算系统复习一遍，看的没兴趣，就放弃了，高手自动出门左转，B站有视频源", 
                            "likes": 0, 
                            "replyToAuthor": "Rookie.Yao"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50168473", 
            "userName": "胡今朝", 
            "userLink": "https://www.zhihu.com/people/fb128b56fdfed86d1a2c8237571d133f", 
            "upvote": 9, 
            "title": "关于WGAN-GP中的遗留问题？", 
            "content": "<p></p><a href=\"https://zhuanlan.zhihu.com/p/49827134\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\"internal\">胡今朝：关于WGAN-GP的理解</a><p>上一篇文章中从intuitional的角度理解了WGAN-GP，基本上已经可以了,除了一个遗留问题</p><h2><b>Review一下GAN</b></h2><p>GAN本身用的是JS散度作为损失函数，当判别器最优的时候</p><p>WGAN用的是在Wassertein distance的基础上修改的损失函数，需要D需要是满足李普西斯条件的的函数，在WGAN中用的是gradient clipping</p><p>因为WGAN中的D的参数都集中在了+-0.01上，所以WGAN-GP中改进了满足李普西斯条件的约束，改成了：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-9510174b0f6d0c5a8bb7184a34c85bb4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"260\" data-rawheight=\"33\" class=\"content_image\" width=\"260\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;260&#39; height=&#39;33&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"260\" data-rawheight=\"33\" class=\"content_image lazy\" width=\"260\" data-actualsrc=\"https://pic1.zhimg.com/v2-9510174b0f6d0c5a8bb7184a34c85bb4_b.jpg\"/></figure><p>什么意思？对参数加了惩罚，什么是加了惩罚上一篇有详细的比较intuitional的解释，都ok都能理解，唯一不能理解的东西是</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-44c8dd1592ce3f171453e8aea1e53bda_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"184\" data-rawheight=\"41\" class=\"content_image\" width=\"184\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;184&#39; height=&#39;41&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"184\" data-rawheight=\"41\" class=\"content_image lazy\" width=\"184\" data-actualsrc=\"https://pic3.zhimg.com/v2-44c8dd1592ce3f171453e8aea1e53bda_b.jpg\"/></figure><p>怎么理解？举个例子看看</p><p>比如我取(不好打出来上面公式的notation，讲究着看)</p><p>x = [1,2,3]</p><p>x~=[4,5,6]</p><p>ℇ=1/3</p><p>x^=[1/3,2/3,3/3]+[8/3,10/3,12/3]=[3,4,5]</p><p>好，这个x^经过一个网络D</p><p>3*w1 + 4*w2 +5*w3</p><p>如果是x经过网络D</p><p>1*w1 + 2*w2 +3*w3</p><p>D对x偏导了之后确实就是w ，对x求偏导数就是将x看作变量，x既然是变量，他是几，对他求偏到之后的值有什么影响？这一点深深困扰我</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-e34bad548b0691a72b1b9e88048bc2ab_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"114\" data-rawheight=\"53\" class=\"content_image\" width=\"114\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;114&#39; height=&#39;53&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"114\" data-rawheight=\"53\" class=\"content_image lazy\" width=\"114\" data-actualsrc=\"https://pic4.zhimg.com/v2-e34bad548b0691a72b1b9e88048bc2ab_b.jpg\"/></figure><p>那直接用x或x^不就可以了吗？为何</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-44c8dd1592ce3f171453e8aea1e53bda_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"184\" data-rawheight=\"41\" class=\"content_image\" width=\"184\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;184&#39; height=&#39;41&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"184\" data-rawheight=\"41\" class=\"content_image lazy\" width=\"184\" data-actualsrc=\"https://pic3.zhimg.com/v2-44c8dd1592ce3f171453e8aea1e53bda_b.jpg\"/></figure><h2><b>理解难点</b></h2><p>这一点我相信很多专家是懂的，但是专家他娘的不屑写博客笔记，翻遍了所有的野生博客，没人回答，除了</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-e544cc62b5c3d2695e0d8f5c3e0ea079_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"982\" data-rawheight=\"232\" class=\"origin_image zh-lightbox-thumb\" width=\"982\" data-original=\"https://pic2.zhimg.com/v2-e544cc62b5c3d2695e0d8f5c3e0ea079_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;982&#39; height=&#39;232&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"982\" data-rawheight=\"232\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"982\" data-original=\"https://pic2.zhimg.com/v2-e544cc62b5c3d2695e0d8f5c3e0ea079_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-e544cc62b5c3d2695e0d8f5c3e0ea079_b.jpg\"/></figure><p>地址：<a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/qq_25737169/article/details/78857788\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">DCGAN、WGAN、WGAN-GP、LSGAN、BEGAN原理总结及对比 - Double_V的博客 - CSDN博客</a></p><p>涉及了一点，但是只是原文的翻译一样，还是不够直观和本质，他自己应该是懂了；</p><p>如果x是变量而w是常数，那么w也是变动的</p><p>如果x是变量而w是常数，那么w也是变动的</p><p>如果x是变量而w是常数，那么w也是变动的</p><p>why？</p><p>why？</p><p>why？</p><p>对于一个特定的训练好的神经网络他的x是变量，w是常数；</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-82d9347d47858010f5f9270449a9070e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"270\" data-rawheight=\"290\" class=\"content_image\" width=\"270\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;270&#39; height=&#39;290&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"270\" data-rawheight=\"290\" class=\"content_image lazy\" width=\"270\" data-actualsrc=\"https://pic3.zhimg.com/v2-82d9347d47858010f5f9270449a9070e_b.jpg\"/></figure><p>但是对于一个没有训练好的网络，x和w都是变量，有一个像上帝一样的更大的函数</p><p>f(x,w);什么意思，对于所有的x要找出来一组w，能使f(x,w)满足特定的功能，这里所有的x对于WGAN来说就是真实数据和假数据，因为D既要判断真的也判断假的，这个是多么高的一个维度呢？这篇论文告诉我们我们不考虑所有的x，活在当下，当下的x中的中间态的x做个代表，代表这个batch的全体，上帝视角的函数简单处理成f(x^, w)找出一组x是的f(x,w)在x是x^的时候完成f(x,w)的目标；</p><p>至此，这一点就可以理解了；现在就可以抽象成术语了，data和G(z)共同组成了一个很大的高纬空间x，我们要做的事情是f(x,w)在这个x的条件下找到一组最优的w使得f(x,w)完成特定的任务，当我们给w加惩罚的时候应该加在所有的x对应w上，but impossible，所以我们就在x和x^的中间值上取一个来代表全体的x和x^,在x^上的w加惩罚</p><p>注意判别器本身判别的还是batch的全体，如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-5106e82b4c415b147374d262bd8e64c2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"557\" data-rawheight=\"104\" class=\"origin_image zh-lightbox-thumb\" width=\"557\" data-original=\"https://pic3.zhimg.com/v2-5106e82b4c415b147374d262bd8e64c2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;557&#39; height=&#39;104&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"557\" data-rawheight=\"104\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"557\" data-original=\"https://pic3.zhimg.com/v2-5106e82b4c415b147374d262bd8e64c2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-5106e82b4c415b147374d262bd8e64c2_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": [
                {
                    "userName": "朱跃华", 
                    "userLink": "https://www.zhihu.com/people/e1833a15fc2297b13bde63973a5f8fb7", 
                    "content": "<p>牛掰，跪服！！！！！！！！！！！！！！！！！！！！！！</p><p></p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "朱跃华", 
                    "userLink": "https://www.zhihu.com/people/e1833a15fc2297b13bde63973a5f8fb7", 
                    "content": "<p>牛掰，跪服！！！！！！！！！！！！！！！！！！！！！！</p><p></p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "朱跃华", 
                    "userLink": "https://www.zhihu.com/people/e1833a15fc2297b13bde63973a5f8fb7", 
                    "content": "<p>牛掰，跪服！！！！！！！！！！！！！！！！！！！！！！</p><p></p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "码泥", 
                    "userLink": "https://www.zhihu.com/people/f443cb21d1c3d2aa7d7ec6662e1079db", 
                    "content": "<p>显然不是数学专业的</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50166514", 
            "userName": "胡今朝", 
            "userLink": "https://www.zhihu.com/people/fb128b56fdfed86d1a2c8237571d133f", 
            "upvote": 0, 
            "title": "原始GAN中两种G的损失函数的名称", 
            "content": "<p></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-47c9ad2db4fd161d25061d5d1efc0aa0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1440\" data-rawheight=\"900\" class=\"origin_image zh-lightbox-thumb\" width=\"1440\" data-original=\"https://pic1.zhimg.com/v2-47c9ad2db4fd161d25061d5d1efc0aa0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1440&#39; height=&#39;900&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1440\" data-rawheight=\"900\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1440\" data-original=\"https://pic1.zhimg.com/v2-47c9ad2db4fd161d25061d5d1efc0aa0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-47c9ad2db4fd161d25061d5d1efc0aa0_b.jpg\"/></figure><p>第一种G的损失函数，来源于最初的minmax Game，所以叫做MMGAN</p><p>第二种损失函数，为了使参数有更新的梯度，所以叫做non-saturating GAN 简称NSGAN</p><p></p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50157063", 
            "userName": "胡今朝", 
            "userLink": "https://www.zhihu.com/people/fb128b56fdfed86d1a2c8237571d133f", 
            "upvote": 0, 
            "title": "原始GAN中G的损失函数为什么需要替代", 
            "content": "<p></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-c069a6e6ae371fd917112abc04e37e17_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1440\" data-rawheight=\"900\" class=\"origin_image zh-lightbox-thumb\" width=\"1440\" data-original=\"https://pic4.zhimg.com/v2-c069a6e6ae371fd917112abc04e37e17_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1440&#39; height=&#39;900&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1440\" data-rawheight=\"900\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1440\" data-original=\"https://pic4.zhimg.com/v2-c069a6e6ae371fd917112abc04e37e17_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-c069a6e6ae371fd917112abc04e37e17_b.jpg\"/></figure><p>在训练之初，G(z)很容易被D判别出来，也就是说D(G(z))很小，也就是在上图中D(x)很小，如果用log(1-D(x))作为损失函数的话，在初始阶段曲线的斜率很小也就是梯度很小，这样的就不能为G的参数更新提供足够的梯度，从heuristic的角度将最小化G(z)被抓出来的几率转换成最大化G(z)被D抓不出来的概率，这样的话对应的就是上面这条曲线，这样在训练之初是可以提供足够的梯度的；虽然G的替代版损失函数还有其他的问题但是这是后话了</p><hr/><p>另外D的网络是根据1-logD(x)来训练的，log(D(x))并不等价于1-logD(x)，这样的话D并不是那么容易在log(D(x))也很好的指示真假</p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "PyTorch", 
                    "tagLink": "https://api.zhihu.com/topics/20075993"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50139603", 
            "userName": "胡今朝", 
            "userLink": "https://www.zhihu.com/people/fb128b56fdfed86d1a2c8237571d133f", 
            "upvote": 3, 
            "title": "t-SNE", 
            "content": "<h2><b>介绍</b></h2><p>t-SNE：t-Distributed Stochastic Neighbor Embedding</p><p>作用：用来做高维数据降维然后可视化的</p><p>学习目的：扩大见识</p><p>Reference：</p><p><a href=\"https://link.zhihu.com/?target=https%3A//bindog.github.io/blog/2016/06/04/from-sne-to-tsne-to-largevis/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">从SNE到t-SNE再到LargeVis</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3DGBUEjkpoxXc%26list%3DPLJV_el3uVTsPy9oCRY30oBPNLCo89yu49%26index%3D24\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">youtube.com/watch?</span><span class=\"invisible\">v=GBUEjkpoxXc&amp;list=PLJV_el3uVTsPy9oCRY30oBPNLCo89yu49&amp;index=24</span><span class=\"ellipsis\"></span></a></p><p class=\"ztext-empty-paragraph\"><br/></p><hr/><h2><b>SNE原理</b></h2><p>在高维空间中，算出来任意两个点之间的距离(二范数)，然后再将这两点间的距离用如下公式转换成概率：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-b9b4b6088d88742cd0b29f07e9fcccf4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"316\" data-rawheight=\"79\" class=\"content_image\" width=\"316\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;316&#39; height=&#39;79&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"316\" data-rawheight=\"79\" class=\"content_image lazy\" width=\"316\" data-actualsrc=\"https://pic1.zhimg.com/v2-b9b4b6088d88742cd0b29f07e9fcccf4_b.jpg\"/></figure><p>||xi-xj||表示的是xi和xj两点间的欧式距离</p><p>||xi-xk||表示的是xi和其他点间的欧式距离，注意不是所有点间两两距离而是xi和其他点间的欧式距离</p><p>其中σi表示以xi为中心点的高斯分布的标准差，是个超参数。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在低维空间，当我们把数据映射到低维空间后，高维数据点之间的相似性也应该在低维空间的数据点上体现出来。这里同样用条件概率的形式描述，假设高维数据点xi和xj在低维空间的映射点分别为yi和yj。类似的，低维空间中的条件概率用qj∣i表示，并将所有高斯分布的方差均设定为根号二分之一，所以有：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-d025671e64676ccac7c08815f6e80d1a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"257\" data-rawheight=\"77\" class=\"content_image\" width=\"257\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;257&#39; height=&#39;77&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"257\" data-rawheight=\"77\" class=\"content_image lazy\" width=\"257\" data-actualsrc=\"https://pic3.zhimg.com/v2-d025671e64676ccac7c08815f6e80d1a_b.jpg\"/></figure><p>若yi和yj真实反映了高维数据点xi和xj之间的关系，那么条件概率pj∣i与qj∣i应该完全相等。这里我们只考虑了xi与xj之间的条件概率，若考虑xi与其他所有点之间的条件概率，则可构成一个条件概率分布Pi，同理在低维空间存在一个条件概率分布Qi且应该与Pi一致。如何衡量两个分布之间的相似性？当然是用经典的KL距离(Kullback-Leibler Divergence)，SNE最终目标就是对所有数据点最小化这个KL距离，我们可以使用梯度下降算法最小化如下代价函数：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-24999129ce37bbdec1567823aca27593_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"346\" data-rawheight=\"69\" class=\"content_image\" width=\"346\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;346&#39; height=&#39;69&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"346\" data-rawheight=\"69\" class=\"content_image lazy\" width=\"346\" data-actualsrc=\"https://pic4.zhimg.com/v2-24999129ce37bbdec1567823aca27593_b.jpg\"/></figure><p>似乎到这里问题就漂亮的解决了，你看我们代价函数都写出来了，剩下的事情就是利用梯度下降算法进行训练了。但事情远没有那么简单，因为KL距离是一个非对称的度量。最小化代价函数的目的是让pj∣i和qj∣i的值尽可能的接近，即低维空间中点的相似性应当与高维空间中点的相似性一致。但是从代价函数的形式就可以看出，当pj∣i较大，qj∣i较小时，代价较高；而pj∣i较小，qj∣i较大时，代价较低。什么意思呢？很显然，高维空间中两个数据点距离较近时，若映射到低维空间后距离较远，那么将得到一个很高的惩罚，这当然没问题。反之，高维空间中两个数据点距离较远时，若映射到低维空间距离较近，将得到一个很低的惩罚值，这就有问题了，理应得到一个较高的惩罚才对。换句话说，SNE的代价函数更关注局部结构，而忽视了全局结构。</p><p>SNE代价函数对yi求梯度后的形式如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-123198f8cded44e7c9f9152b2ffb8a5f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"328\" data-rawheight=\"61\" class=\"content_image\" width=\"328\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;328&#39; height=&#39;61&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"328\" data-rawheight=\"61\" class=\"content_image lazy\" width=\"328\" data-actualsrc=\"https://pic4.zhimg.com/v2-123198f8cded44e7c9f9152b2ffb8a5f_b.jpg\"/></figure><h2><b>对称SNE</b></h2><p>高维空间，这里采用一种更简单直观的方式定义pij：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-e4af7e399f2f36125f81998f8118b54b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"160\" data-rawheight=\"61\" class=\"content_image\" width=\"160\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;160&#39; height=&#39;61&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"160\" data-rawheight=\"61\" class=\"content_image lazy\" width=\"160\" data-actualsrc=\"https://pic4.zhimg.com/v2-e4af7e399f2f36125f81998f8118b54b_b.jpg\"/></figure><p>其中n为数据点的总数，这样定义即满足了对称性，又保证了xi的惩罚值不会过小。此时可以利用KL距离写出如下代价函数：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ad5ef45ea5de9f2002d44c53ee180f98_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"299\" data-rawheight=\"76\" class=\"content_image\" width=\"299\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;299&#39; height=&#39;76&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"299\" data-rawheight=\"76\" class=\"content_image lazy\" width=\"299\" data-actualsrc=\"https://pic1.zhimg.com/v2-ad5ef45ea5de9f2002d44c53ee180f98_b.jpg\"/></figure><p>对称SNE的效果只是略微优于原始SNE的效果，依然没有从根本上解决问题。</p><h2><b>拥挤问题(The Crowding Problem)</b></h2><p>所谓拥挤问题，顾名思义，看看SNE的可视化效果，不同类别的簇挤在一起，无法区分开来，这就是拥挤问题。有的同学说，是不是因为SNE更关注局部结构，而忽略了全局结构造成的？这的确有一定影响，但是别忘了使用对称SNE时同样存在拥挤问题。实际上，拥挤问题的出现与某个特定算法无关，而是由于高维空间距离分布和低维空间距离分布的差异造成的。</p><p>我们生活在一个低维的世界里，所以有些时候思维方式容易受到制约。比如在讨论流形学习问题的时候，总喜欢拿一个经典的“Swiss roll”作为例子，这只不过是把一个简单的二维流形嵌入到三维空间里而已。实际上真实世界的数据形态远比“Swiss roll”复杂，比如一个10维的流形嵌入到更高维度的空间中，现在我们的问题是把这个10维的流形找出来，并且映射到二维空间上可视化。在进行可视化时，问题就来了，在10维流形上可以存在11个点且两两之间距离相等。在二维空间中呢？我们最多只能使三个点两两之间距离相等，想将高维空间中的距离关系完整保留到低维空间是不可能的。</p><p>如何解决呢？这个时候就需要请出t分布了。</p><p><b>那么对于高维空间中相距较近的点，为了满足pij=qij，低维空间中的距离需要稍小一点；而对于高维空间中相距较远的点，为了满足pij=qij，低维空间中的距离需要更远。</b>这恰好满足了我们的需求，即同一簇内的点(距离较近)聚合的更紧密，不同簇之间的点(距离较远)更加疏远。我们使用自由度为1的tt分布重新定义qij：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-1725bb9173a1a121cfb259d484082dcc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"295\" data-rawheight=\"73\" class=\"content_image\" width=\"295\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;295&#39; height=&#39;73&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"295\" data-rawheight=\"73\" class=\"content_image lazy\" width=\"295\" data-actualsrc=\"https://pic1.zhimg.com/v2-1725bb9173a1a121cfb259d484082dcc_b.jpg\"/></figure><p>依然用KL距离衡量两个分布之间的相似性，此时梯度变为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-02af4bc629e19f25397d03ae21f45b1e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"360\" data-rawheight=\"68\" class=\"content_image\" width=\"360\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;360&#39; height=&#39;68&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"360\" data-rawheight=\"68\" class=\"content_image lazy\" width=\"360\" data-actualsrc=\"https://pic3.zhimg.com/v2-02af4bc629e19f25397d03ae21f45b1e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>t分布</b></p><p>大家在概率与统计课程中都接触过t分布的概念，从正态总体中抽取容量为N的随机样本，若该正态总体的均值为μ，方差为σ2，。随机样本均值为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-05e3c027410e92145387c5253dae3290_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"9\" data-rawheight=\"20\" class=\"content_image\" width=\"9\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;9&#39; height=&#39;20&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"9\" data-rawheight=\"20\" class=\"content_image lazy\" width=\"9\" data-actualsrc=\"https://pic1.zhimg.com/v2-05e3c027410e92145387c5253dae3290_b.jpg\"/></figure><p>方差为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-4acf12d939e6a2c2a934e3069d50c8ef_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"171\" data-rawheight=\"40\" class=\"content_image\" width=\"171\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;171&#39; height=&#39;40&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"171\" data-rawheight=\"40\" class=\"content_image lazy\" width=\"171\" data-actualsrc=\"https://pic4.zhimg.com/v2-4acf12d939e6a2c2a934e3069d50c8ef_b.jpg\"/></figure><p>随机变量t可表示为：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-0f46d30eae593d7aa3e289b3b11ec243_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"103\" data-rawheight=\"51\" class=\"content_image\" width=\"103\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;103&#39; height=&#39;51&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"103\" data-rawheight=\"51\" class=\"content_image lazy\" width=\"103\" data-actualsrc=\"https://pic4.zhimg.com/v2-0f46d30eae593d7aa3e289b3b11ec243_b.jpg\"/></figure><p>此时我们称t服从自由度为n−1的t分布，即t∼t(n−1)</p><h2><b>代码：</b></h2><a href=\"https://link.zhihu.com/?target=https%3A//github.com/cemoody/topicsne\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">cemoody/topicsne</a><p></p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50126209", 
            "userName": "胡今朝", 
            "userLink": "https://www.zhihu.com/people/fb128b56fdfed86d1a2c8237571d133f", 
            "upvote": 2, 
            "title": "自编码器：AutoEncoder", 
            "content": "<p></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-d372c170e3cff683fb9b022e8e6aa252_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"600\" data-rawheight=\"190\" class=\"origin_image zh-lightbox-thumb\" width=\"600\" data-original=\"https://pic3.zhimg.com/v2-d372c170e3cff683fb9b022e8e6aa252_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;600&#39; height=&#39;190&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"600\" data-rawheight=\"190\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"600\" data-original=\"https://pic3.zhimg.com/v2-d372c170e3cff683fb9b022e8e6aa252_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-d372c170e3cff683fb9b022e8e6aa252_b.jpg\"/></figure><p>自动编码器是一种数据的压缩算法，其中数据的压缩和解压缩函数是数据相关的、有损的、从样本中自动学习的。在大部分提到自动编码器的场合，压缩和解压缩的函数是通过神经网络实现的。</p><p>1）自动编码器是数据相关的（data-specific 或 data-dependent），这意味着自动编码器只能压缩那些与训练数据类似的数据。比如，使用人脸训练出来的自动编码器在压缩别的图片，在压缩树木时性能很差，因为它学习到的特征是与人脸相关的。</p><p>2）自动编码器是有损的，意思是解压缩的输出与原来的输入相比是退化的，MP3，JPEG等压缩算法也是如此。这与无损压缩算法不同。</p><p>3）自动编码器是从数据样本中自动学习的，这意味着很容易对指定类的输入训练出一种特定的编码器，而不需要完成任何新工作。</p><p>搭建一个自动编码器需要完成下面三样工作：搭建编码器，搭建解码器，设定一个损失函数，用以衡量由于压缩而损失掉的信息，通常是MSE。编码器和解码器一般都是参数化的方程，并关于损失函数可导，典型情况是使用神经网络。编码器和解码器的参数可以通过最小化损失函数而优化，例如SGD。</p><p>自编码器是一个自监督的算法，并不是一个无监督算法。自监督学习是监督学习的一个实例，其标签产生自输入数据。要获得一个自监督的模型，你需要一个靠谱的目标跟一个损失函数，仅仅把目标设定为重构输入可能不是正确的选项。基本上，要求模型在像素级上精确重构输入不是机器学习的兴趣所在，学习到高级的抽象特征才是。事实上，当主要任务是分类、定位之类的任务时，那些对这类任务而言的最好的特征基本上都是重构输入时的最差的那种特征。</p><h2><b>应用</b></h2><p>目前自编码器的应用主要有两个方面，第一是数据去噪，第二是为进行可视化而降维。配合适当的维度和稀疏约束，自编码器可以学习到比PCA等技术更有意思的数据投影。</p><p>对于2D的数据可视化，t-SNE或许是目前最好的算法，但通常还是需要原数据的维度相对低一些。所以，可视化高维数据的一个好办法是首先使用自编码器将维度降低到较低的水平（如32维），然后再使用t-SNE将其投影在2D平面上。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3e10a6324176b15eee440d447ca61639_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"677\" data-rawheight=\"506\" class=\"origin_image zh-lightbox-thumb\" width=\"677\" data-original=\"https://pic2.zhimg.com/v2-3e10a6324176b15eee440d447ca61639_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;677&#39; height=&#39;506&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"677\" data-rawheight=\"506\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"677\" data-original=\"https://pic2.zhimg.com/v2-3e10a6324176b15eee440d447ca61639_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-3e10a6324176b15eee440d447ca61639_b.jpg\"/></figure><h2><b>几种自编码器</b></h2><p>自编码器的想法一直是神经网络历史景象的一部分 (LeCun, 1987; Bourlard and Kamp, 1988; Hinton and Zemel, 1994)。传统自编码器被用于降维或特征学习。近年来，自编码器与潜变量模型理论的联系将自编码器带到了生成式建模的前沿。自编码器可以被看作是前馈网络的一个特例，并且可以使用完全相同的技术进行训练，通常使用小批量梯度下降法（其中梯度基于反向传播计算） 。不同于一般的前馈网络，自编码器也可以使用再循环（recirculation）训练 (Hinton and McClelland, 1988)，这种学习算法基于比较原始输入的激活和重构输入的激活。</p><p>欠完备自编码器：从自编码器获得有用特征的一种方法是限制h的维度比x小，这种编码维度小于输入维度的自编码器称为欠完备（undercomplete）自编码器。学习欠完备的表示将强制自编码器捕捉训练数据中最显著的特征。</p><p>正则自编码器：编码维数小于输入维数的欠完备自编码器可以学习数据分布最显著的特征。我们已经知道，如果赋予这类自编码器过大的容量，它就不能学到任何有用的信息。如果隐藏编码的维数允许与输入相等，或隐藏编码维数大于输入的过完备（overcomplete）情况下，会发生类似的问题。在这些情况下，即使是线性编码器和线性解码器也可以学会将输入复制到输出，而学不到任何有关数据分布的有用信息。理想情况下，根据要建模的数据分布的复杂性，选择合适的编码维数和编码器、解码器容量，就可以成功训练任意架构的自编码器。正则自编码器提供这样的能力。正则自编码器使用的损失函数可以鼓励模型学习其他特性（除了将输入复制到输出），而不必限制使用浅层的编码器和解码器以及小的编码维数来限制模型的容量。这些特性包括稀疏表示、表示的小导数、以及对噪声或输入缺失的鲁棒性。即使模型容量大到足以学习一个无意义的恒等函数，非线性且过完备的正则自编码器仍然能够从数据中学到一些关于数据分布的有用信息。</p><p>稀疏自编码器一般用来学习特征，以便用于像分类这样的任务。稀疏正则化的自编码器必须反映训练数据集的独特统计特征，而不是简单地充当恒等函数。以这种方式训练，执行附带稀疏惩罚的复制任务可以得到能学习有用特征的模型。由于自编码器的潜在表示y是对于输入x的一种有损压缩。优化和训练只能让它对于训练集合来说是很好的压缩表示，但并不是对于所有的输入都是这样。为了增加隐藏层的特征表示的鲁棒性和泛化能力，引入去噪自编码器。</p><p>去噪自编码器在自编码器的基础上，在输入中加入随机噪声再传递给自编码器，通过自编码器来重建出无噪声的输入。加入随机噪声的方式有很多种。该过程随机的把输入的一些位（最多一半位）设置为0，这样去噪自编码器就需要通过没有被污染的位来猜测被置为零的位。能够从数据的抽样部分预测整体数据的任何子集是在该抽样中能够找到变量联合分布的充分条件(Gibbs抽样的理论依据)，这说明去噪自编码器能够从理论上证明潜在表示能够获取到输入的所有有效特征。</p><h2><b>代码实现</b></h2><p>相对来说不是很难，就不写了</p><p></p>", 
            "topic": [
                {
                    "tag": "汽车", 
                    "tagLink": "https://api.zhihu.com/topics/19551915"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/49833496", 
            "userName": "胡今朝", 
            "userLink": "https://www.zhihu.com/people/fb128b56fdfed86d1a2c8237571d133f", 
            "upvote": 0, 
            "title": "神经网络中的维度", 
            "content": "<p>这里的神经网络特指普通神经网络，不包含卷积神经网络中的维度问题。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-7b33e46c88089f8dd004f14854b4d920_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"624\" data-rawheight=\"575\" class=\"origin_image zh-lightbox-thumb\" width=\"624\" data-original=\"https://pic1.zhimg.com/v2-7b33e46c88089f8dd004f14854b4d920_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;624&#39; height=&#39;575&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"624\" data-rawheight=\"575\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"624\" data-original=\"https://pic1.zhimg.com/v2-7b33e46c88089f8dd004f14854b4d920_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-7b33e46c88089f8dd004f14854b4d920_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "维度", 
                    "tagLink": "https://api.zhihu.com/topics/19567336"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "神经网络", 
                    "tagLink": "https://api.zhihu.com/topics/19607065"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/49833261", 
            "userName": "胡今朝", 
            "userLink": "https://www.zhihu.com/people/fb128b56fdfed86d1a2c8237571d133f", 
            "upvote": 0, 
            "title": "关于优化算法最好的一张图", 
            "content": "<p>adam算法：</p><p>momentum算法：</p><p>adagrad算法：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-7d29228778f5340ebe0635a72776d5af_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1366\" data-rawheight=\"768\" class=\"origin_image zh-lightbox-thumb\" width=\"1366\" data-original=\"https://pic4.zhimg.com/v2-7d29228778f5340ebe0635a72776d5af_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1366&#39; height=&#39;768&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1366\" data-rawheight=\"768\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1366\" data-original=\"https://pic4.zhimg.com/v2-7d29228778f5340ebe0635a72776d5af_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-7d29228778f5340ebe0635a72776d5af_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "优化", 
                    "tagLink": "https://api.zhihu.com/topics/19570512"
                }, 
                {
                    "tag": "算法", 
                    "tagLink": "https://api.zhihu.com/topics/19553510"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/49833089", 
            "userName": "胡今朝", 
            "userLink": "https://www.zhihu.com/people/fb128b56fdfed86d1a2c8237571d133f", 
            "upvote": 0, 
            "title": "混淆矩阵", 
            "content": "<p>两个类的混淆矩阵就是将以下四种情况放在同一个表格中：</p><p>（T表示预测对了；F表示预测错了；T、P表示真实情况）</p><p>TP(True Positive):真实值为真，预测值也为真；</p><p>FN(False Negative):真实值为假，预测值为真；</p><p>FP(False Positive):真实值为真，预测值为假；</p><p>TN(True Negative):真实值为假，预测值为假；</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-8f82b307449da074e0f75dd5a8aab11b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"217\" data-rawheight=\"75\" class=\"content_image\" width=\"217\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;217&#39; height=&#39;75&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"217\" data-rawheight=\"75\" class=\"content_image lazy\" width=\"217\" data-actualsrc=\"https://pic4.zhimg.com/v2-8f82b307449da074e0f75dd5a8aab11b_b.jpg\"/></figure><p>多个类的混淆矩阵举例：</p><p>如有150个样本数据，这些数据分成3类，每类50个。分类结束后得到的混淆矩阵为：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-115f4929670877a42744a336ad10aee2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"230\" data-rawheight=\"173\" class=\"content_image\" width=\"230\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;230&#39; height=&#39;173&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"230\" data-rawheight=\"173\" class=\"content_image lazy\" width=\"230\" data-actualsrc=\"https://pic3.zhimg.com/v2-115f4929670877a42744a336ad10aee2_b.jpg\"/></figure><p>实际：类1有50个</p><p>实际：类2有50个</p><p>实际：类3有50个</p><p>预测：类1有43+2个，错将2个类2的样本分类为类1</p><p>预测：类2有5+45+1个，错将5个类1的样本分类为类2，错将1个类3的样本分为类2</p><p>预测：类3有2+3+49个，错将2个类1的样本分类为类3，错将3个类2的样本分为类3</p>", 
            "topic": [
                {
                    "tag": "矩阵", 
                    "tagLink": "https://api.zhihu.com/topics/19650614"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/49832888", 
            "userName": "胡今朝", 
            "userLink": "https://www.zhihu.com/people/fb128b56fdfed86d1a2c8237571d133f", 
            "upvote": 21, 
            "title": "彻底搞懂双线性插值", 
            "content": "<p>在FCN中upsampling的时候会用到双线性插值，双线性插值实际上是一个非常基础的图像resize的手段，但是网上没有任何一篇博客真正说清楚了每一个细节，什么叫说清楚？就是当我看完对这个问题的解释之后能够用代码实现出来，这就是说清楚了，也是理解清楚了，但是很抱歉，我暂时没有发现任何一篇文章真正做到这一点了（我不是针对谁，我是说所有文章的都是复制一下Wikipedia的垃圾,holly crap），在我理解这个知识点的过程中我发现很多人都并没有真正搞懂，只是知道套用公式而已，所以这片文章两个目的：个人笔记</p><p>step1:什么是插值？</p><p>自行百度</p><p>step2:什么是线性插值？</p><p>自行百度</p><p>step3:为什么要对图像插值？</p><p>resize</p><p>step4:什么是双线性插值？</p><p>自行百度</p><p class=\"ztext-empty-paragraph\"><br/></p><p>step5:双线性插值</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-74149bf0ce0bdb66a3246d797c362f18_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"272\" data-rawheight=\"223\" class=\"content_image\" width=\"272\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;272&#39; height=&#39;223&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"272\" data-rawheight=\"223\" class=\"content_image lazy\" width=\"272\" data-actualsrc=\"https://pic1.zhimg.com/v2-74149bf0ce0bdb66a3246d797c362f18_b.jpg\"/></figure><p>根据点1⃣️2⃣️算出来m处的像素值：</p><p>(p2-p1)/(x2-x1) = (p2-pm)/(x2-xm)</p><p>x2-x1 = 1</p><p>p2-p1 = (p2-pm)/(x2-xm) =&gt; pm = p2 - (p2-p1)*(x2-xm)</p><p>根据点3⃣️ 4⃣️算出来n处的像素值：</p><p>(p4-p3)/(x4-x3) = (p4-pn)/(x4-xn)</p><p>x4-x3 = 1</p><p>p4-p3 = (p4-pn)/(x4-xn) =&gt; pn = p4 - (p4-p3)*(x4-xn)</p><p>根据点m、n算出来5处的像素值：</p><p>(pn-pm)/(yn-ym) = (pn-p5)/(yn-y5)</p><p>yn-ym = 1</p><p>pn-pm = (pn-p5)/(yn-y5) =&gt; p5 = pn - (pn-pm)*(yn-y5)</p><p class=\"ztext-empty-paragraph\"><br/></p><p>p5 = p4 - (p4-p3)*(x4-xn) - (p4 - (p4-p3)*(x4-xn) - p2 - (p2-p1)(x2-xm))*(yn-y5)</p><p>xn = xm = x5</p><p>yn = y3 = y4</p><p>通过1，3算a；2，4算b再算5点的结果是相同的</p><p>=========================================================</p><p>要通过双线性插值的方法算出dst中每一个像素点的像素值，是通过dst像素点的坐标对应到src图像当中的坐标；然后通过双线性插值的方法算出src中相应坐标的像素值</p><p>坐标对应关系：</p><p>➢按比例对应：</p><p>SrcX=(dstX)* (srcWidth/dstWidth)</p><p>SrcY=(dstY) * (srcHeight/dstHeight)</p><p>➢按比例对应最后一列没有办法参与计算，所以按几何中心对应：</p><p>SrcX+0.5=(dstX+0.5)* (srcWidth/dstWidth)</p><p>SrcY+0.5=(dstY+0.5) * (srcHeight/dstHeight)</p><p>几何中心对应中，如果索引是负值，实际上是从一行像素值的末端取值</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-1c3551f61030230600394ba95c834b35_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"248\" data-rawheight=\"176\" class=\"content_image\" width=\"248\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;248&#39; height=&#39;176&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"248\" data-rawheight=\"176\" class=\"content_image lazy\" width=\"248\" data-actualsrc=\"https://pic2.zhimg.com/v2-1c3551f61030230600394ba95c834b35_b.jpg\"/></figure><p>现在我们需要对上面这个3x3的图片resize为5x5的图片：</p><p>首先np.zeros([5, 5])生成一个空的shape = [5, 5]的图片，如下图</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-526c76bd4d8ead7aea21b3668ad3a57c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"328\" data-rawheight=\"240\" class=\"content_image\" width=\"328\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;328&#39; height=&#39;240&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"328\" data-rawheight=\"240\" class=\"content_image lazy\" width=\"328\" data-actualsrc=\"https://pic1.zhimg.com/v2-526c76bd4d8ead7aea21b3668ad3a57c_b.jpg\"/></figure><p>============================================================</p><p>找到对应的坐标SrcX和SrcY之后如何找到相邻的四个坐标呢？</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-110960de8ca992d56abece7c18273993_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"340\" data-rawheight=\"287\" class=\"content_image\" width=\"340\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;340&#39; height=&#39;287&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"340\" data-rawheight=\"287\" class=\"content_image lazy\" width=\"340\" data-actualsrc=\"https://pic4.zhimg.com/v2-110960de8ca992d56abece7c18273993_b.jpg\"/></figure><p>(np.floor(srcX), np.floor(srcY))就是点1⃣️的坐标</p><p>(np.floor(srcX)+1, np.floor(srcY)+1)就是点2⃣️的坐标</p><p>但是不能超过src的边界同时坐标必须是整型，综合起来有：</p><p>src_x_0 = int(np.floor(srcX))<br/>src_y_0 = int(np.floor(srcY))<br/>src_x_1 = min(src_x_0 + 1, src_w - 1)<br/>src_y_1 = min(src_y_0 + 1, src_h - 1)</p><p>============================================================</p><p>按channel循环遍历每一个坐标点：</p><div class=\"highlight\"><pre><code class=\"language-text\">for n in range(3): # 对channel循环\n        for dst_y in range(dst_h): # 对height循环\n            for dst_x in range(dst_w): # 对width循环\n                # 目标在源上的坐标\n                src_x = (dst_x + 0.5) * scale_x - 0.5\n                src_y = (dst_y + 0.5) * scale_y - 0.5\n                # 计算在源图上四个近邻点的位置\n                src_x_0 = int(np.floor(src_x))\n                src_y_0 = int(np.floor(src_y))\n                src_x_1 = min(src_x_0 + 1, src_w - 1)\n                src_y_1 = min(src_y_0 + 1, src_h - 1)\n\n                # 双线性插值\n                value0 = (src_x_1 - src_x) * src[src_y_0, src_x_0, n] + (src_x - src_x_0) * src[src_y_0, src_x_1, n]\n                value1 = (src_x_1 - src_x) * src[src_y_1, src_x_0, n] + (src_x - src_x_0) * src[src_y_1, src_x_1, n]\n                dst[dst_y, dst_x, n] = int((src_y_1 - src_y) * value0 + (src_y - src_y_0) * value1)\n    return dst</code></pre></div><p></p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": [
                {
                    "userName": "萌萌糖", 
                    "userLink": "https://www.zhihu.com/people/e9f6b97468485c55226a4b0e54c8b25b", 
                    "content": "(np.floor(srcX)+1, np.floor(srcY)+1)就是点2⃣️的坐标，这个是点4的坐标吧", 
                    "likes": 1, 
                    "childComments": [
                        {
                            "userName": "嗯哼哼", 
                            "userLink": "https://www.zhihu.com/people/fafec6e8279085ae65e858df77fadc24", 
                            "content": "<p>是的，确实是点4  对角线嘛</p><p>作者应该写错了</p>", 
                            "likes": 0, 
                            "replyToAuthor": "萌萌糖"
                        }
                    ]
                }, 
                {
                    "userName": "摇旗呐喊", 
                    "userLink": "https://www.zhihu.com/people/b384b01d66f5e6a579b0745e9b59a90a", 
                    "content": "<p>什么叫几何中心对应？指的是哪里</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "UYTRE", 
                    "userLink": "https://www.zhihu.com/people/3eb689a7b8cc297c84c9b80130806aa6", 
                    "content": "<p>麻烦你把变量 指明清楚</p><p></p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/49832504", 
            "userName": "胡今朝", 
            "userLink": "https://www.zhihu.com/people/fb128b56fdfed86d1a2c8237571d133f", 
            "upvote": 0, 
            "title": "关于优化算法最好的一张图", 
            "content": "<p>adam算法：</p><p>momentum算法：</p><p>adagrad算法：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-2480ec1eba4474270d4a0f1e3313c837_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"720\" data-rawheight=\"405\" class=\"origin_image zh-lightbox-thumb\" width=\"720\" data-original=\"https://pic4.zhimg.com/v2-2480ec1eba4474270d4a0f1e3313c837_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;720&#39; height=&#39;405&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"720\" data-rawheight=\"405\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"720\" data-original=\"https://pic4.zhimg.com/v2-2480ec1eba4474270d4a0f1e3313c837_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-2480ec1eba4474270d4a0f1e3313c837_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "算法", 
                    "tagLink": "https://api.zhihu.com/topics/19553510"
                }, 
                {
                    "tag": "优化", 
                    "tagLink": "https://api.zhihu.com/topics/19570512"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/49832412", 
            "userName": "胡今朝", 
            "userLink": "https://www.zhihu.com/people/fb128b56fdfed86d1a2c8237571d133f", 
            "upvote": 1, 
            "title": "1*1卷积核的作用", 
            "content": "<h2>1x1的卷积核由于大小只有1x1，所以并不需要考虑像素跟周边像素的关系，它主要用于调节通道数，对不同的通道上的像素点进行线性组合，然后进行非线性化操作，可以完成升维和降维的功能，如下图所示，选择2个1x1大小的卷积核，那么特征图的深度将会从3变成2，如果使用4个1x1的卷积核，特征图的深度将会由3变成4</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-1680908e8bbcfff3748a4dbb172dd617_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"600\" data-rawheight=\"482\" class=\"origin_image zh-lightbox-thumb\" width=\"600\" data-original=\"https://pic4.zhimg.com/v2-1680908e8bbcfff3748a4dbb172dd617_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;600&#39; height=&#39;482&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"600\" data-rawheight=\"482\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"600\" data-original=\"https://pic4.zhimg.com/v2-1680908e8bbcfff3748a4dbb172dd617_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-1680908e8bbcfff3748a4dbb172dd617_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/49832048", 
            "userName": "胡今朝", 
            "userLink": "https://www.zhihu.com/people/fb128b56fdfed86d1a2c8237571d133f", 
            "upvote": 7, 
            "title": "双线性插值详解", 
            "content": "<h2><b>1.线性插值：</b></h2><p>知道两个点，然后根据这个两个点的值去估计其他点的值；我们假设函数关系是线性的，这就是线性插值，比如：</p><p>已知（3， 4） （4，8）这两个点的坐标来推测3.5这个点的坐标：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ed1653f30733871fe1cac01699eb5507_b.jpg\" data-size=\"normal\" data-rawwidth=\"627\" data-rawheight=\"359\" class=\"origin_image zh-lightbox-thumb\" width=\"627\" data-original=\"https://pic4.zhimg.com/v2-ed1653f30733871fe1cac01699eb5507_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;627&#39; height=&#39;359&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"627\" data-rawheight=\"359\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"627\" data-original=\"https://pic4.zhimg.com/v2-ed1653f30733871fe1cac01699eb5507_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-ed1653f30733871fe1cac01699eb5507_b.jpg\"/><figcaption>线性插值</figcaption></figure><h2><b>2.最邻近插值</b></h2><p>我们就举个简单的图像：3X3 的256级灰度图，假如图像的象素矩阵如下图所示（这个原始图把它叫做源图，Source）：</p><p>234 38 22</p><p>67 44 12</p><p>89 65 63</p><p>这个矩阵中，元素坐标(x,y)是这样确定的，x从左到右，从0开始，y从上到下，也是从零开始，这是图象处理中最常用的坐标系，就是这样一个坐标：</p><p>----------------------＞X<br/>|<br/>|<br/>|<br/>|<br/>|<br/>Y</p><p>把这副图放大为4X4大小的图像，第一步是先把4X4的矩阵先画出来，如下所示，当然，矩阵的每个像素都是未知数，这个将要被填充的图的叫做目标图,Destination：<br/>? ? ? ?<br/>? ? ? ?<br/>? ? ? ?<br/>? ? ? ?</p><p>然后要往这个空的矩阵里面填值了，要填的值从哪里来来呢？是从源图中来，先填写目标图最左上角的象素，坐标为（0，0），那么该坐标对应源图中的坐标可以由如下公式得出： <br/><b>srcX=dstX* (srcWidth/dstWidth) , srcY = dstY * (srcHeight/dstHeight)</b></p><p><b>目标图像对应到原图像的坐标位置</b></p><p>套用公式，就可以找到对应的原图的坐标了(0*(3/4),0*(3/4))=&gt;(0*0.75,0*0.75)=&gt;(0,0)<br/>,找到了源图的对应坐标,就可以把源图中坐标为(0,0)处的234象素值填进去目标图的(0,0)这个位置了。</p><p>接下来,如法炮制,寻找目标图中坐标为(1,0)的象素对应源图中的坐标,套用公式:<br/>(1*0.75,0*0.75)=&gt;(0.75,0)<br/>结果发现,得到的坐标里面竟然有小数,这可怎么办?计算机里的图像可是数字图像,象素就是最小单位了,<b>象素的坐标都是整数,从来没有小数坐标。这时候采用的一种策略就是采用四舍五入</b>的方法（也可以采用直接舍掉小数位的方法），把非整数坐标转换成整数，好，那么按照四舍五入的方法就得到坐标（1，0），完整的运算过程就是这样的：<br/>(1*0.75,0*0.75)=&gt;(0.75,0)=&gt;(1,0)<br/>那么就可以再填一个象素到目标矩阵中了，同样是把源图中坐标为(1,0)处的像素值38填入目标图中的坐标。<br/><br/>依次填完每个象素，一幅放大后的图像就诞生了，像素矩阵如下所示：<br/>234 38 22 22 <br/>67 44 12 12 <br/>89 65 63 63 <br/>89 65 63 63</p><h2><b>3.线性插值</b></h2><p>上面的最邻近插值显然是有问题的，图像不够平滑，所如果需要过渡的更加平滑的话需要用双线性插值；</p><p>在数学上，双线性插值是有两个变量的插值函数的线性插值扩展，其核心思想是在两个方向分别进行一次线性插值</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-78464133a42643701c4899ff46900079_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"180\" data-rawheight=\"172\" class=\"content_image\" width=\"180\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;180&#39; height=&#39;172&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"180\" data-rawheight=\"172\" class=\"content_image lazy\" width=\"180\" data-actualsrc=\"https://pic2.zhimg.com/v2-78464133a42643701c4899ff46900079_b.jpg\"/></figure><p>假如我们想得到未知函数 f 在点 P = (x, y) 的值，假设我们已知函数 f 在 Q11 = (x1, y1)、Q12 = (x1, y2), Q21 = (x2, y1) 以及 Q22 = (x2, y2) 四个点的值。<b>最常见的情况，f就是一个像素点的像素值</b>。首先在 x 方向进行线性插值，得到</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-a4248134a5f91a8f42de732bfeafaa35_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"541\" data-rawheight=\"40\" class=\"origin_image zh-lightbox-thumb\" width=\"541\" data-original=\"https://pic2.zhimg.com/v2-a4248134a5f91a8f42de732bfeafaa35_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;541&#39; height=&#39;40&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"541\" data-rawheight=\"40\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"541\" data-original=\"https://pic2.zhimg.com/v2-a4248134a5f91a8f42de732bfeafaa35_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-a4248134a5f91a8f42de732bfeafaa35_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-7eef5d715e6cde10a01967dd5bbdf0c3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"541\" data-rawheight=\"39\" class=\"origin_image zh-lightbox-thumb\" width=\"541\" data-original=\"https://pic4.zhimg.com/v2-7eef5d715e6cde10a01967dd5bbdf0c3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;541&#39; height=&#39;39&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"541\" data-rawheight=\"39\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"541\" data-original=\"https://pic4.zhimg.com/v2-7eef5d715e6cde10a01967dd5bbdf0c3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-7eef5d715e6cde10a01967dd5bbdf0c3_b.jpg\"/></figure><p>然后在 y 方向进行线性插值，得到</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-4a8fc2b901eae2dc532afd2a42f5bc88_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"321\" data-rawheight=\"40\" class=\"content_image\" width=\"321\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;321&#39; height=&#39;40&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"321\" data-rawheight=\"40\" class=\"content_image lazy\" width=\"321\" data-actualsrc=\"https://pic1.zhimg.com/v2-4a8fc2b901eae2dc532afd2a42f5bc88_b.jpg\"/></figure><p>综合起来就是双线性插值最后的结果：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-d727991431ebd8c2b01335b0603e62b8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"646\" data-rawheight=\"47\" class=\"origin_image zh-lightbox-thumb\" width=\"646\" data-original=\"https://pic1.zhimg.com/v2-d727991431ebd8c2b01335b0603e62b8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;646&#39; height=&#39;47&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"646\" data-rawheight=\"47\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"646\" data-original=\"https://pic1.zhimg.com/v2-d727991431ebd8c2b01335b0603e62b8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-d727991431ebd8c2b01335b0603e62b8_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-ee2ee7ad2dfa69035e52db2e4979f515_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"627\" data-rawheight=\"47\" class=\"origin_image zh-lightbox-thumb\" width=\"627\" data-original=\"https://pic2.zhimg.com/v2-ee2ee7ad2dfa69035e52db2e4979f515_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;627&#39; height=&#39;47&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"627\" data-rawheight=\"47\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"627\" data-original=\"https://pic2.zhimg.com/v2-ee2ee7ad2dfa69035e52db2e4979f515_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-ee2ee7ad2dfa69035e52db2e4979f515_b.jpg\"/></figure><p>由于图像双线性插值只会用相邻的4个点，因此上述公式的分母都是1</p><p><b>注意：目标图像中的每个值都是通过双线性插值的方法得到的，先生成一个空的目标图像大小的矩阵，然后往里面填值</b></p><h2><b>计算在源图上四个近邻点的位置</b></h2><h2><b>src_x = (dst_x + 0.5) * scale_x - 0.5</b></h2><h2><b>src_y = (dst_y + 0.5) * scale_y - 0.5</b></h2><h2><b># 计算在源图上四个近邻点的位置</b></h2><h2><b>src_x_0 = int(np.floor(src_x))</b></h2><h2><b>src_y_0 = int(np.floor(src_y))</b></h2><h2><b>src_x_1 = min(src_x_0 + 1, src_w - 1)</b></h2><h2><b>src_y_1 = min(src_y_0 + 1, src_h - 1)</b></h2><p>以及源图像和目标图像几何中心的对齐</p><p><b>SrcX=(dstX+0.5)* (srcWidth/dstWidth) -0.5</b></p><p><b>SrcY=(dstY+0.5) * (srcHeight/dstHeight)-0.5</b></p><p>这个要重点说一下，源图像和目标图像的原点（0，0）均选择左上角，然后根据插值公式计算目标图像每点像素，假设你需要将一幅5x5的图像缩小成3x3，那么源图像和目标图像各个像素之间的对应关系如下。如果没有这个中心对齐，根据基本公式去算，就会得到左边这样的结果；而用了对齐，就会得到右边的结果：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-345bc23c201378640af76b039d4fa7b2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"795\" data-rawheight=\"378\" class=\"origin_image zh-lightbox-thumb\" width=\"795\" data-original=\"https://pic3.zhimg.com/v2-345bc23c201378640af76b039d4fa7b2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;795&#39; height=&#39;378&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"795\" data-rawheight=\"378\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"795\" data-original=\"https://pic3.zhimg.com/v2-345bc23c201378640af76b039d4fa7b2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-345bc23c201378640af76b039d4fa7b2_b.jpg\"/></figure><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"c1\"># coding=utf-8</span>\n<span class=\"kn\">import</span> <span class=\"nn\">cv2</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n<span class=\"kn\">import</span> <span class=\"nn\">time</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">resize</span><span class=\"p\">(</span><span class=\"n\">src</span><span class=\"p\">,</span> <span class=\"n\">new_size</span><span class=\"p\">):</span>\n    <span class=\"n\">dst_w</span><span class=\"p\">,</span> <span class=\"n\">dst_h</span> <span class=\"o\">=</span> <span class=\"n\">new_size</span> <span class=\"c1\"># 目标图像宽高</span>\n    <span class=\"n\">src_h</span><span class=\"p\">,</span> <span class=\"n\">src_w</span> <span class=\"o\">=</span> <span class=\"n\">src</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[:</span><span class=\"mi\">2</span><span class=\"p\">]</span> <span class=\"c1\"># 源图像宽高</span>\n    <span class=\"k\">if</span> <span class=\"n\">src_h</span> <span class=\"o\">==</span> <span class=\"n\">dst_h</span> <span class=\"ow\">and</span> <span class=\"n\">src_w</span> <span class=\"o\">==</span> <span class=\"n\">dst_w</span><span class=\"p\">:</span>\n        <span class=\"k\">return</span> <span class=\"n\">src</span><span class=\"o\">.</span><span class=\"n\">copy</span><span class=\"p\">()</span>\n    <span class=\"n\">scale_x</span> <span class=\"o\">=</span> <span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"n\">src_w</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">dst_w</span> <span class=\"c1\"># x缩放比例</span>\n    <span class=\"n\">scale_y</span> <span class=\"o\">=</span> <span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"n\">src_h</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">dst_h</span> <span class=\"c1\"># y缩放比例</span>\n\n    <span class=\"c1\"># 遍历目标图像，插值</span>\n    <span class=\"n\">dst</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">dst_h</span><span class=\"p\">,</span> <span class=\"n\">dst_w</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">uint8</span><span class=\"p\">)</span>\n    <span class=\"k\">for</span> <span class=\"n\">n</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">):</span> <span class=\"c1\"># 对channel循环</span>\n        <span class=\"k\">for</span> <span class=\"n\">dst_y</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">dst_h</span><span class=\"p\">):</span> <span class=\"c1\"># 对height循环</span>\n            <span class=\"k\">for</span> <span class=\"n\">dst_x</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">dst_w</span><span class=\"p\">):</span> <span class=\"c1\"># 对width循环</span>\n                <span class=\"c1\"># 目标在源上的坐标</span>\n                <span class=\"n\">src_x</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">dst_x</span> <span class=\"o\">+</span> <span class=\"mf\">0.5</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">scale_x</span> <span class=\"o\">-</span> <span class=\"mf\">0.5</span>\n                <span class=\"n\">src_y</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">dst_y</span> <span class=\"o\">+</span> <span class=\"mf\">0.5</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">scale_y</span> <span class=\"o\">-</span> <span class=\"mf\">0.5</span>\n                <span class=\"c1\"># 计算在源图上四个近邻点的位置</span>\n                <span class=\"n\">src_x_0</span> <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">floor</span><span class=\"p\">(</span><span class=\"n\">src_x</span><span class=\"p\">))</span>\n                <span class=\"n\">src_y_0</span> <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">floor</span><span class=\"p\">(</span><span class=\"n\">src_y</span><span class=\"p\">))</span>\n                <span class=\"n\">src_x_1</span> <span class=\"o\">=</span> <span class=\"nb\">min</span><span class=\"p\">(</span><span class=\"n\">src_x_0</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">src_w</span> <span class=\"o\">-</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n                <span class=\"n\">src_y_1</span> <span class=\"o\">=</span> <span class=\"nb\">min</span><span class=\"p\">(</span><span class=\"n\">src_y_0</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">src_h</span> <span class=\"o\">-</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n\n                <span class=\"c1\"># 双线性插值</span>\n                <span class=\"n\">value0</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">src_x_1</span> <span class=\"o\">-</span> <span class=\"n\">src_x</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">src</span><span class=\"p\">[</span><span class=\"n\">src_y_0</span><span class=\"p\">,</span> <span class=\"n\">src_x_0</span><span class=\"p\">,</span> <span class=\"n\">n</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">src_x</span> <span class=\"o\">-</span> <span class=\"n\">src_x_0</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">src</span><span class=\"p\">[</span><span class=\"n\">src_y_0</span><span class=\"p\">,</span> <span class=\"n\">src_x_1</span><span class=\"p\">,</span> <span class=\"n\">n</span><span class=\"p\">]</span>\n                <span class=\"n\">value1</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">src_x_1</span> <span class=\"o\">-</span> <span class=\"n\">src_x</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">src</span><span class=\"p\">[</span><span class=\"n\">src_y_1</span><span class=\"p\">,</span> <span class=\"n\">src_x_0</span><span class=\"p\">,</span> <span class=\"n\">n</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">src_x</span> <span class=\"o\">-</span> <span class=\"n\">src_x_0</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">src</span><span class=\"p\">[</span><span class=\"n\">src_y_1</span><span class=\"p\">,</span> <span class=\"n\">src_x_1</span><span class=\"p\">,</span> <span class=\"n\">n</span><span class=\"p\">]</span>\n                <span class=\"n\">dst</span><span class=\"p\">[</span><span class=\"n\">dst_y</span><span class=\"p\">,</span> <span class=\"n\">dst_x</span><span class=\"p\">,</span> <span class=\"n\">n</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">((</span><span class=\"n\">src_y_1</span> <span class=\"o\">-</span> <span class=\"n\">src_y</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">value0</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">src_y</span> <span class=\"o\">-</span> <span class=\"n\">src_y_0</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">value1</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">dst</span>\n\n<span class=\"k\">if</span> <span class=\"vm\">__name__</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;__main__&#39;</span><span class=\"p\">:</span>\n    <span class=\"n\">img_in</span> <span class=\"o\">=</span> <span class=\"n\">cv2</span><span class=\"o\">.</span><span class=\"n\">imread</span><span class=\"p\">(</span><span class=\"s1\">&#39;/Users/andy/Desktop/paper code/1.jpg&#39;</span><span class=\"p\">)</span>\n    <span class=\"n\">start</span> <span class=\"o\">=</span> <span class=\"n\">time</span><span class=\"o\">.</span><span class=\"n\">time</span><span class=\"p\">()</span>\n    <span class=\"n\">img_out</span> <span class=\"o\">=</span> <span class=\"n\">cv2</span><span class=\"o\">.</span><span class=\"n\">resize</span><span class=\"p\">(</span><span class=\"n\">img_in</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"mi\">600</span><span class=\"p\">,</span><span class=\"mi\">600</span><span class=\"p\">))</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;cost </span><span class=\"si\">%f</span><span class=\"s1\"> seconds&#39;</span> <span class=\"o\">%</span> <span class=\"p\">(</span><span class=\"n\">time</span><span class=\"o\">.</span><span class=\"n\">time</span><span class=\"p\">()</span> <span class=\"o\">-</span> <span class=\"n\">start</span><span class=\"p\">))</span>\n\n    <span class=\"n\">cv2</span><span class=\"o\">.</span><span class=\"n\">imshow</span><span class=\"p\">(</span><span class=\"s1\">&#39;src_image&#39;</span><span class=\"p\">,</span> <span class=\"n\">img_in</span><span class=\"p\">)</span>\n    <span class=\"n\">cv2</span><span class=\"o\">.</span><span class=\"n\">imshow</span><span class=\"p\">(</span><span class=\"s1\">&#39;dst_image&#39;</span><span class=\"p\">,</span> <span class=\"n\">img_out</span><span class=\"p\">)</span>\n    <span class=\"n\">cv2</span><span class=\"o\">.</span><span class=\"n\">waitKey</span><span class=\"p\">()</span></code></pre></div><p></p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": [
                {
                    "userName": "魏之远", 
                    "userLink": "https://www.zhihu.com/people/fdd8765abb0b59cfcac3f8eaaad86e7e", 
                    "content": "<p>很清晰，感谢</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "摇旗呐喊", 
                    "userLink": "https://www.zhihu.com/people/b384b01d66f5e6a579b0745e9b59a90a", 
                    "content": "<p>清晰吗？为什么双线性插值不能举一个具体的例子呢 后面的计算四个点的公式乱七八糟的</p>", 
                    "likes": 1, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/49831949", 
            "userName": "胡今朝", 
            "userLink": "https://www.zhihu.com/people/fb128b56fdfed86d1a2c8237571d133f", 
            "upvote": 0, 
            "title": "卷积层如何转化为矩阵运算", 
            "content": "<p>卷积层可以转化为矩阵运算：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-19876f7d9c85eb061bd469ac501e37f2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"720\" data-rawheight=\"540\" class=\"origin_image zh-lightbox-thumb\" width=\"720\" data-original=\"https://pic3.zhimg.com/v2-19876f7d9c85eb061bd469ac501e37f2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;720&#39; height=&#39;540&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"720\" data-rawheight=\"540\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"720\" data-original=\"https://pic3.zhimg.com/v2-19876f7d9c85eb061bd469ac501e37f2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-19876f7d9c85eb061bd469ac501e37f2_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "卷积", 
                    "tagLink": "https://api.zhihu.com/topics/19678959"
                }, 
                {
                    "tag": "矩阵运算", 
                    "tagLink": "https://api.zhihu.com/topics/19598145"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/49831761", 
            "userName": "胡今朝", 
            "userLink": "https://www.zhihu.com/people/fb128b56fdfed86d1a2c8237571d133f", 
            "upvote": 0, 
            "title": "神经网络中的维度", 
            "content": "<p>这里的神经网络特指普通神经网络，不包含卷积神经网络中的维度问题。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-7b33e46c88089f8dd004f14854b4d920_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"624\" data-rawheight=\"575\" class=\"origin_image zh-lightbox-thumb\" width=\"624\" data-original=\"https://pic1.zhimg.com/v2-7b33e46c88089f8dd004f14854b4d920_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;624&#39; height=&#39;575&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"624\" data-rawheight=\"575\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"624\" data-original=\"https://pic1.zhimg.com/v2-7b33e46c88089f8dd004f14854b4d920_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-7b33e46c88089f8dd004f14854b4d920_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "神经网络", 
                    "tagLink": "https://api.zhihu.com/topics/19607065"
                }, 
                {
                    "tag": "维度", 
                    "tagLink": "https://api.zhihu.com/topics/19567336"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/49829909", 
            "userName": "胡今朝", 
            "userLink": "https://www.zhihu.com/people/fb128b56fdfed86d1a2c8237571d133f", 
            "upvote": 0, 
            "title": "熵-交叉熵", 
            "content": "<h2><b>概念1：信息量</b></h2><p>I(x) = -log p(x) = log [1/p(x)]</p><p>直观理解：一个信息包含的信息量与这件事情发生的概率成反比，所以公式中有1/p(x)，两个相互独立的信息的信息量等于各自信息量的和，所以有log；log{[1/p(x1)]*[1/p(x2)]} = log [1/p(x1)] + log [1/p(x2)]。</p><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>概念2：熵</b></h2><p>H(x) = -∑p(x)log p(x)</p><p>信息量的均值</p><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>概念3:平均编码长度</b></h2><p>举例：</p><p>信息x1的发生概率：p(x1) = 1/2</p><p>信息x2的发生概率：p(x2) = 1/2</p><p>2进制编码的编码长度为：1</p><p>分析：</p><p>2的n次方代表需要编码的个数，n表示编码长度；1/p(x)表示需要编码的个数：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-7a9afbf698e615d5cf2f3f27a2a46daf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"292\" data-rawheight=\"60\" class=\"content_image\" width=\"292\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;292&#39; height=&#39;60&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"292\" data-rawheight=\"60\" class=\"content_image lazy\" width=\"292\" data-actualsrc=\"https://pic4.zhimg.com/v2-7a9afbf698e615d5cf2f3f27a2a46daf_b.jpg\"/></figure><p>平均编码长度：就是熵H(x) = -∑p(x)log p(x)</p><h2><b>概念4：交叉熵</b></h2><p>H(x) = -∑p(x)log q(x)</p><p>用估计编码q(x)近似真实编码p(x)需要的平均编码长度</p><p>二分类交叉熵：</p><p>loss=−ylog(ŷ )−(1−y)log(1−ŷ )</p><h2><b>交叉熵与MSE：</b></h2><p>交叉熵代价函数能够在相同条件下，学习的速率较二次代价函数要快，因为在求输出层和中间层的delta的时候，会有一个乘子，就是激活函数关于输入的导数。当采用sigmoid函数作为激活函数的时候，sigmoid的输出接近0或者1的时候，sigmoid变得很平缓，求导后的值就变得很小。再乘上学习率，就成了一个很小的值，也就是迈开的步子很小，学习的很慢。而交叉熵代价函数能够把这个乘子给约掉。很好的避免了学习速度下降的原因。</p><h2></h2><p></p><p></p><p></p><p></p><p></p><p></p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/49828063", 
            "userName": "胡今朝", 
            "userLink": "https://www.zhihu.com/people/fb128b56fdfed86d1a2c8237571d133f", 
            "upvote": 0, 
            "title": "elu&selu", 
            "content": "<p>elu的表达式，如下图所示:</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-ac4983279547a03368fd51166846b732_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"269\" data-rawheight=\"304\" class=\"content_image\" width=\"269\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;269&#39; height=&#39;304&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"269\" data-rawheight=\"304\" class=\"content_image lazy\" width=\"269\" data-actualsrc=\"https://pic3.zhimg.com/v2-ac4983279547a03368fd51166846b732_b.jpg\"/></figure><p>selu的表达式，如下式所示：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-9564bee66ea059b27c42ae32f72261c6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"344\" data-rawheight=\"139\" class=\"content_image\" width=\"344\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;344&#39; height=&#39;139&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"344\" data-rawheight=\"139\" class=\"content_image lazy\" width=\"344\" data-actualsrc=\"https://pic3.zhimg.com/v2-9564bee66ea059b27c42ae32f72261c6_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/49827733", 
            "userName": "胡今朝", 
            "userLink": "https://www.zhihu.com/people/fb128b56fdfed86d1a2c8237571d133f", 
            "upvote": 2, 
            "title": "批标准化，如何在面试的时候口头表达清楚？", 
            "content": "<p>面试官：批标准化的原理是什么？</p><p>答：</p><p>1、先说说为什么要做批标准化？（internal covariate shift）</p><p>输入的各个特征值的大小可能差异很大，比如一个人的特征有身高180cm，年龄20岁，月收入5000元；这就导致对应到各个特征的参数值差异很大，这样不同的参数的学习率就不同，不能在同一个学习率下收敛，这是第一个问题</p><p>第二个问题是，就算所有的特征值都差不多大小，每个维度的值大约都在400-500之间，还有一个问题是，这样的话会让wx+b的值处在激活函数（以 sigmoid举例）的饱和区域，并不能很好的传递梯度</p><p>2、说说怎么办？</p><p>不同的特征值差异很大那就都平移放缩到同一个尺度，scale*x + offset</p><p>放缩到同一个尺度之后的值有可能还是在激活函数的饱和区域，搞成正太分布了再平移放缩，scale*[( x - μ ) / σ] + offset</p><p class=\"ztext-empty-paragraph\"><br/></p><p>3、其中scale和offset是trainable的，μ 和σ是每个batch算一个，等到测试的时候用滑动平均</p><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p class=\"ztext-empty-paragraph\"><br/></p><p>Z=( X - μ ) / σ会得到一个标准正态分布 ，σ这个是标准差</p><p>internal covariate shift这个词要记住</p><p>scale*[( x - μ ) / σ] + offset这个公式要记得，面试官会问到</p><p>参考视频：这个老师讲的应该是关于这个东西最清楚的一个了</p><p class=\"ztext-empty-paragraph\"><br/></p><a href=\"https://link.zhihu.com/?target=https%3A//www.bilibili.com/video/av16540598%3Ffrom%3Dsearch%26seid%3D8859536659160672507\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">bilibili.com/video/av16</span><span class=\"invisible\">540598?from=search&amp;seid=8859536659160672507</span><span class=\"ellipsis\"></span></a><p></p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/49827134", 
            "userName": "胡今朝", 
            "userLink": "https://www.zhihu.com/people/fb128b56fdfed86d1a2c8237571d133f", 
            "upvote": 46, 
            "title": "关于WGAN-GP的理解", 
            "content": "<p>关于原始GAN的理解网上的野生博客漫天飞，从GAN到WGAN都还好，非常能起到导学的作用，但是到了WGAN-GP这篇paper了没有几个能掰扯清楚的了，绕了一大圈还是要自己慢慢啃，在谷歌上搜了一下WGAN-GP，也在quora上提问了一下，如果你看到因为WGAN-GP看到这篇笔记，恭喜你哦，运气不错，带上你的智商和注意力，我对自己的表达能力有自信（前提是你懂了WGAN及之前的相关paper）！</p><p>reference：</p><a href=\"https://link.zhihu.com/?target=https%3A//medium.com/%40jonathan_hui/gan-wasserstein-gan-wgan-gp-6a1a2aa1b490\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">medium.com/@jonathan_hu</span><span class=\"invisible\">i/gan-wasserstein-gan-wgan-gp-6a1a2aa1b490</span><span class=\"ellipsis\"></span></a><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p class=\"ztext-empty-paragraph\"><br/></p><p>WGAN-GP纯粹是为了满足WGAN中的李普西斯条件，WGAN自己的满足方式是gradient clipping，但是这样的话WGAN的大部分weight会是正负0.01，需要新的满足李普西斯条件的方法，这就是motivation；</p><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>Earth-Mover (EM) distance/ Wasserstein Metric</b></h2><p>用一个小的实验来解释一下什么是推土机距离？我们有6个盒子并且我们想将他们从左边的位置移动到右边虚线表示的位置；对于1号box，我们从左边1号盒子所在的位置移动到右边1号盒子所在的位置，移动的距离为6，只看横坐标的差值；</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b68ba2f10e65fb944a3bad25e3aaf641_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"418\" data-rawheight=\"208\" class=\"content_image\" width=\"418\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;418&#39; height=&#39;208&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"418\" data-rawheight=\"208\" class=\"content_image lazy\" width=\"418\" data-actualsrc=\"https://pic2.zhimg.com/v2-b68ba2f10e65fb944a3bad25e3aaf641_b.jpg\"/></figure><p>下面的表格呈现了两种不同的移动方案r1和r2，右边的表格解释了盒子是怎么移动的，比如：第一种方案中我们把2号和3号盒子这两个盒子从位置1移动到位置10，那么在右边的表格中(1，10)的位置填上2（这个数表示从1处移到10处盒子的个数为2）；其他位置的数同理</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-d7cd42892281bbd0b43d5e27ca4ba7ab_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"669\" data-rawheight=\"362\" class=\"origin_image zh-lightbox-thumb\" width=\"669\" data-original=\"https://pic4.zhimg.com/v2-d7cd42892281bbd0b43d5e27ca4ba7ab_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;669&#39; height=&#39;362&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"669\" data-rawheight=\"362\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"669\" data-original=\"https://pic4.zhimg.com/v2-d7cd42892281bbd0b43d5e27ca4ba7ab_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-d7cd42892281bbd0b43d5e27ca4ba7ab_b.jpg\"/></figure><p>虽然这两个方案的总移动距离相同但是并不是所有的移动方案的距离都是相同的；<b>Wasserstein distance</b>是所有方案中距离最小一个；用下面这个小例子来表示：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-103b4fedb2309927b3fde81268ba8837_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"545\" data-rawheight=\"256\" class=\"origin_image zh-lightbox-thumb\" width=\"545\" data-original=\"https://pic4.zhimg.com/v2-103b4fedb2309927b3fde81268ba8837_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;545&#39; height=&#39;256&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"545\" data-rawheight=\"256\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"545\" data-original=\"https://pic4.zhimg.com/v2-103b4fedb2309927b3fde81268ba8837_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-103b4fedb2309927b3fde81268ba8837_b.jpg\"/></figure><p>有两种移动方案：</p><p>1⃣️ 1号盒子从位置3移动到位置4，2号盒子从7移动到6；</p><p class=\"ztext-empty-paragraph\"><br/></p><p>2⃣️ 2号盒子从位置3移动到位置6，1号盒子从位置4移动到位置7</p><p class=\"ztext-empty-paragraph\"><br/></p><p>对两个分布Pr和Pg的Wasserstein距离的公式：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-2f69d5174e8c95af2c98bf267bbd021a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"690\" data-rawheight=\"291\" class=\"origin_image zh-lightbox-thumb\" width=\"690\" data-original=\"https://pic3.zhimg.com/v2-2f69d5174e8c95af2c98bf267bbd021a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;690&#39; height=&#39;291&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"690\" data-rawheight=\"291\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"690\" data-original=\"https://pic3.zhimg.com/v2-2f69d5174e8c95af2c98bf267bbd021a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-2f69d5174e8c95af2c98bf267bbd021a_b.jpg\"/></figure><p>不要被上面的公式吓到了；左边的W（Pr，Pg）表示的Pr，Pg的Wasserstein距离；</p><p>∏(Pr, Pg)表示的意思是所有的r(x, y)这种距离的联合分布的集合，他们的边缘分布相应是Pr, Pg</p><p>我们将变量x，y结合起来形成联合分布r(x,y)</p><p><b>至此，搞清楚了什么是Wasserstein距离！！！</b></p><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>Wasserstein GAN</b></h2><p>由于wasserstein distance非常的复杂，要先算出来所有的推土方式的距离，再找出来最小值；</p><p>Using the Kantorovich-Rubinstein duality, we can simplify the calculation to</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-f97816c56982b9ca570a0a2f4a55d2d6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"800\" data-rawheight=\"62\" class=\"origin_image zh-lightbox-thumb\" width=\"800\" data-original=\"https://pic3.zhimg.com/v2-f97816c56982b9ca570a0a2f4a55d2d6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;800&#39; height=&#39;62&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"800\" data-rawheight=\"62\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"800\" data-original=\"https://pic3.zhimg.com/v2-f97816c56982b9ca570a0a2f4a55d2d6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-f97816c56982b9ca570a0a2f4a55d2d6_b.jpg\"/></figure><p>真是操蛋，又有一个什么Kantorovich-Rubinstein duality，这一部分重点说一下这个，智力富余者继续，reference：</p><a href=\"https://link.zhihu.com/?target=https%3A//vincentherrmann.github.io/blog/wasserstein/\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic2.zhimg.com/v2-2bb0d85058230d91d41ddb3b0b62bcd9_180x120.jpg\" data-image-width=\"1120\" data-image-height=\"720\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Wasserstein GAN and the Kantorovich-Rubinstein Duality</a><p>Kantorovich-Rubinstein duality这个东西在原论文中一笔带过，直接鄙视了一批人，设置了很高的理解门槛，这个东西不是新东西了，这里用的好，这篇paper引用了书《Optimal Transport - Old and New》，菲尔兹奖获得者Cedric Villani写的，这里是链接地址：</p><a href=\"https://link.zhihu.com/?target=https%3A//www.cedricvillani.org/wp-content/uploads/2012/08/preprint-1.pdf\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">cedricvillani.org/wp-co</span><span class=\"invisible\">ntent/uploads/2012/08/preprint-1.pdf</span><span class=\"ellipsis\"></span></a><p>这书是给数学系的phd的，998页，如果你想读，不拦你，have fun and good luck！！！</p><p>这是Cedric Villani在华为的演讲，enjoy please：</p><a href=\"https://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3Dzo46TEp6FB8\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">youtube.com/watch?</span><span class=\"invisible\">v=zo46TEp6FB8</span><span class=\"ellipsis\"></span></a><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-66f8272f145d3dfb490293dbfc24526d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"648\" data-rawheight=\"365\" class=\"origin_image zh-lightbox-thumb\" width=\"648\" data-original=\"https://pic2.zhimg.com/v2-66f8272f145d3dfb490293dbfc24526d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;648&#39; height=&#39;365&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"648\" data-rawheight=\"365\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"648\" data-original=\"https://pic2.zhimg.com/v2-66f8272f145d3dfb490293dbfc24526d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-66f8272f145d3dfb490293dbfc24526d_b.jpg\"/></figure><p>这是演讲的画风，please enjoy～！</p><p>这条路去理解，我放弃了，下面从一个比较intuitional的角度来理解：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-3b61cdc4ec59fe1c6d88db427ac28adf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"666\" data-rawheight=\"302\" class=\"origin_image zh-lightbox-thumb\" width=\"666\" data-original=\"https://pic4.zhimg.com/v2-3b61cdc4ec59fe1c6d88db427ac28adf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;666&#39; height=&#39;302&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"666\" data-rawheight=\"302\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"666\" data-original=\"https://pic4.zhimg.com/v2-3b61cdc4ec59fe1c6d88db427ac28adf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-3b61cdc4ec59fe1c6d88db427ac28adf_b.jpg\"/></figure><p>如果用JS divergence来度量Pg1，Pg2到Pg3的距离的话，对于判别器的损失都是都是-log2</p><p>所以要用推土机距离，上面已经解释了什么是推土机距离；这个距离可以衡量两个毫无交集的分布的距离对于Pg1，Pg2到Pg3的推土机距离是不同的</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-0cf3a6c53fc81d93c7b232dcb1cabc92_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"867\" data-rawheight=\"501\" class=\"origin_image zh-lightbox-thumb\" width=\"867\" data-original=\"https://pic3.zhimg.com/v2-0cf3a6c53fc81d93c7b232dcb1cabc92_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;867&#39; height=&#39;501&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"867\" data-rawheight=\"501\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"867\" data-original=\"https://pic3.zhimg.com/v2-0cf3a6c53fc81d93c7b232dcb1cabc92_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-0cf3a6c53fc81d93c7b232dcb1cabc92_b.jpg\"/></figure><p>我们看看算法里具体是如何implement的</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-890894b12c7878db42d2b0fb62d9b4a0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"719\" data-rawheight=\"442\" class=\"origin_image zh-lightbox-thumb\" width=\"719\" data-original=\"https://pic1.zhimg.com/v2-890894b12c7878db42d2b0fb62d9b4a0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;719&#39; height=&#39;442&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"719\" data-rawheight=\"442\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"719\" data-original=\"https://pic1.zhimg.com/v2-890894b12c7878db42d2b0fb62d9b4a0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-890894b12c7878db42d2b0fb62d9b4a0_b.jpg\"/></figure><p>然后这实际是在干嘛呢？</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-69f90f7a02c866bc805c3323f3551dec_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"292\" data-rawheight=\"28\" class=\"content_image\" width=\"292\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;292&#39; height=&#39;28&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"292\" data-rawheight=\"28\" class=\"content_image lazy\" width=\"292\" data-actualsrc=\"https://pic1.zhimg.com/v2-69f90f7a02c866bc805c3323f3551dec_b.jpg\"/></figure><p>实际上是求D(x)的均值和D(G(z))的均值之差；也就是让D(x)的均值和D(G(z))的数字特征尽量靠近，但是需要保证的是D(x)要大于D(G(z)),我们可以保证的是x&gt;G(z),因为初始化权重和给定的噪声都可以是很小的值，但是怎么保证x一直大于G(z)呢？来G(z)要慢慢的不断靠近x，在靠近的过程中x，D(G(z))不能超过D(x)---&gt;这句话非常重要,你在其他地方保证看不到；因为我们是要最小化上面黄色截图的公式，如果变成了负数，那不断的变小实际是让两个分布的距离增大了，所以G(z)靠近x的过程中，D(G(z))不能迈过D(x);也就是传说中的李普西斯条件条件；虽然我这么说很low，没有高大上的公式，但是够不够intuitive，佩服自己一把，用小学数学解释了抽像代数；好,现在就知道为什么要用weights clipping[-0.01, 0.01]了吧，非常直观，控制D(G(z))的大小,D 的梯度如果太奇葩会怎样？会让G(z)还没有追上x，就让D(G(z))超过了D(x),还有这事儿？如下图，有点儿丑，但是还是能说明问题</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-a8668e9f4c60ea82e0a69d34dfd3a50c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"308\" data-rawheight=\"382\" class=\"content_image\" width=\"308\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;308&#39; height=&#39;382&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"308\" data-rawheight=\"382\" class=\"content_image lazy\" width=\"308\" data-actualsrc=\"https://pic1.zhimg.com/v2-a8668e9f4c60ea82e0a69d34dfd3a50c_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>接下来说说主角WGAN-GP</b></h2><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-3c960aef78ba47927dadedfa39a33654_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"561\" data-rawheight=\"103\" class=\"origin_image zh-lightbox-thumb\" width=\"561\" data-original=\"https://pic1.zhimg.com/v2-3c960aef78ba47927dadedfa39a33654_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;561&#39; height=&#39;103&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"561\" data-rawheight=\"103\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"561\" data-original=\"https://pic1.zhimg.com/v2-3c960aef78ba47927dadedfa39a33654_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-3c960aef78ba47927dadedfa39a33654_b.jpg\"/></figure><p>那就是理解这货，前面的一项很好说，就是上面讲的，本质上是用随机变量的数字特征（均值）的距离来表征和两个分布的距离，这是题眼，很重要；后面做的事儿是什么呢？也intuitive的讲一下，我们要保证G(z)靠近x,且靠近的过程D(G(z))不超过D(x)，对D的权重加惩罚，什么意思</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-f9364206bd6b19589140361455c41870_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"95\" data-rawheight=\"36\" class=\"content_image\" width=\"95\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;95&#39; height=&#39;36&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"95\" data-rawheight=\"36\" class=\"content_image lazy\" width=\"95\" data-actualsrc=\"https://pic1.zhimg.com/v2-f9364206bd6b19589140361455c41870_b.jpg\"/></figure><p>这一坨就是D的权重，待会儿解释为什么，先说什么是加惩罚，</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-f30f9efa7b1cc77dd9c9d303996bd49d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"170\" data-rawheight=\"43\" class=\"content_image\" width=\"170\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;170&#39; height=&#39;43&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"170\" data-rawheight=\"43\" class=\"content_image lazy\" width=\"170\" data-actualsrc=\"https://pic2.zhimg.com/v2-f30f9efa7b1cc77dd9c9d303996bd49d_b.jpg\"/></figure><p>这一项是一个非负数，我们是要最小化L，如果这个非负数太大的话L就太大，所以在最小化L的时候这个非负数是不会太大的也就是</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ae1a500ed9cf732bc02df9e20d38e8d3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"103\" data-rawheight=\"41\" class=\"content_image\" width=\"103\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;103&#39; height=&#39;41&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"103\" data-rawheight=\"41\" class=\"content_image lazy\" width=\"103\" data-actualsrc=\"https://pic4.zhimg.com/v2-ae1a500ed9cf732bc02df9e20d38e8d3_b.jpg\"/></figure><p>这货会在1的附近；这就是加惩罚，那就稳定住了梯度，也就是保证了G(z)靠近x,且靠近的过程D(G(z))不超过D(x)</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-6b38fa439b52b09a444b8c285b468e09_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"966\" data-rawheight=\"505\" class=\"origin_image zh-lightbox-thumb\" width=\"966\" data-original=\"https://pic2.zhimg.com/v2-6b38fa439b52b09a444b8c285b468e09_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;966&#39; height=&#39;505&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"966\" data-rawheight=\"505\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"966\" data-original=\"https://pic2.zhimg.com/v2-6b38fa439b52b09a444b8c285b468e09_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-6b38fa439b52b09a444b8c285b468e09_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-1ceeaebfc6cb586e4efb5958e8d09734_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"70\" data-rawheight=\"28\" class=\"content_image\" width=\"70\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;70&#39; height=&#39;28&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"70\" data-rawheight=\"28\" class=\"content_image lazy\" width=\"70\" data-actualsrc=\"https://pic1.zhimg.com/v2-1ceeaebfc6cb586e4efb5958e8d09734_b.jpg\"/></figure><p>表示的是Dw(x^)对x^求导；而x^的系数就是权重w啊</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>不懂的点：为什么这么处理一下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-99372ca490a36cdbc768d4530530c7eb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"138\" data-rawheight=\"20\" class=\"content_image\" width=\"138\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;138&#39; height=&#39;20&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"138\" data-rawheight=\"20\" class=\"content_image lazy\" width=\"138\" data-actualsrc=\"https://pic4.zhimg.com/v2-99372ca490a36cdbc768d4530530c7eb_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>2018-11-17 更新：在这里已经解决了：不懂的点：为什么这么处理一下：</p><a href=\"https://zhuanlan.zhihu.com/p/50168473\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\"internal\">胡今朝：关于WGAN-GP中的遗留问题？</a><p>另外这个算法的trick是用Adam优化器</p>", 
            "topic": [
                {
                    "tag": "生成对抗网络（GAN）", 
                    "tagLink": "https://api.zhihu.com/topics/20070859"
                }
            ], 
            "comments": [
                {
                    "userName": "知乎用户", 
                    "userLink": "https://www.zhihu.com/people/0", 
                    "content": "<p>初始的wgan建议不要用动量算法，为什么到wgan-gp又要使用adam作为trick呢？</p>", 
                    "likes": 1, 
                    "childComments": []
                }, 
                {
                    "userName": "神利·代目", 
                    "userLink": "https://www.zhihu.com/people/84df8488a0ebaeb7389bedef76576a30", 
                    "content": "<p>牛啤！！！感谢博主！学到啦！！非常感谢！</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "nlp27149", 
                    "userLink": "https://www.zhihu.com/people/3aea264b7b684994bab8d187af241427", 
                    "content": "<p>感谢博主，那个红色折线图能解释一下吗”会让G(z)还没有追上x，就让D(G(z))超过了D(x),还有这事儿？“</p>", 
                    "likes": 1, 
                    "childComments": []
                }, 
                {
                    "userName": "叽里咕噜咕噜", 
                    "userLink": "https://www.zhihu.com/people/a6db53b8c5322655a518a9c5a5d424f4", 
                    "content": "<p>博主你好，文中黄色截图公式那里，为什么说是最小化呢？ 不是最大化吗？ 用的是梯度上升。</p>", 
                    "likes": 1, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/47010738", 
            "userName": "胡今朝", 
            "userLink": "https://www.zhihu.com/people/fb128b56fdfed86d1a2c8237571d133f", 
            "upvote": 1, 
            "title": "语义分割-1", 
            "content": "<h2><b>1、什么是语义分割？</b></h2><p>分类问题：一张图片里面有一个物体，识别出来这个物体是什么，这类问题是分类问题；</p><p>bounding box：一张图片里面有一个或若干个物体，识别出来这个物体是什么，并用框子框出来；</p><p>语义分割：一张图片里面有若干个物体，对于图片中的每个像素，判断其属于哪个类别；</p><p>实例分割：在语义分割的基础上，判断每个像素属于对应类别的哪个个体；</p><h2><b>2、什么是全卷积神经网络？</b></h2><p>FCN将网络全连接层用卷积取代，因此使任意图像大小的输入都变成可能，尽管移除了全连接层，但是CNN模型用于语义分割还存在一个问题，就是下采样操作（比如，pooling）。pooling操作可以扩大感受野因而能够很好地整合context信息，对high-level的任务（比如分类），这是很有效的。但是对于分割任务来说需要进行上采样操作；</p><h2><b>3、encoder-decoder架构</b></h2><p>encoder-decoder是基于FCN的架构。encoder由于pooling逐渐减少空间维度，而decoder逐渐恢复空间维度和细节信息。通常从encoder到decoder还有shortcut connetction（捷径连接，也就是跨层连接）。其中U-net就是这种架构很流行的一种，如下图：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-6ef05b0d8349ff806c83790f7462c30e_b.jpg\" data-caption=\"\" data-size=\"normal\" class=\"content_image\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;0&#39; height=&#39;0&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" class=\"content_image lazy\" data-actualsrc=\"https://pic3.zhimg.com/v2-6ef05b0d8349ff806c83790f7462c30e_b.jpg\"/></figure><h2><b>4.空洞卷积</b></h2><p>dilated/atrous （空洞卷积）架构，这种结构代替了pooling，一方面它可以保持空间分辨率，另外一方面它由于可以扩大感受野因而可以很好地整合上下文信息。如下图：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-b1ff163f7a014186d69fdc9cdf74f10c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"750\" data-rawheight=\"351\" class=\"origin_image zh-lightbox-thumb\" width=\"750\" data-original=\"https://pic1.zhimg.com/v2-b1ff163f7a014186d69fdc9cdf74f10c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;750&#39; height=&#39;351&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"750\" data-rawheight=\"351\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"750\" data-original=\"https://pic1.zhimg.com/v2-b1ff163f7a014186d69fdc9cdf74f10c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-b1ff163f7a014186d69fdc9cdf74f10c_b.jpg\"/></figure><h2><b>5.条件随机场</b></h2><p>除了以上思路，还有一种对分割结果进行后处理的方法，那就是条件随机场(Conditional Random Fields (CRFs))后处理用来改善分割效果。DeepLab系列文章基本都采用这种后处理方法，可以较好地改善分割结果，如下图：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-e4e863acf8aff610b89598f3e8f7d28a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1241\" data-rawheight=\"584\" class=\"origin_image zh-lightbox-thumb\" width=\"1241\" data-original=\"https://pic3.zhimg.com/v2-e4e863acf8aff610b89598f3e8f7d28a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1241&#39; height=&#39;584&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1241\" data-rawheight=\"584\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1241\" data-original=\"https://pic3.zhimg.com/v2-e4e863acf8aff610b89598f3e8f7d28a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-e4e863acf8aff610b89598f3e8f7d28a_b.jpg\"/></figure><p></p><p></p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "卷积神经网络（CNN）", 
                    "tagLink": "https://api.zhihu.com/topics/20043586"
                }, 
                {
                    "tag": "神经网络", 
                    "tagLink": "https://api.zhihu.com/topics/19607065"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/45638983", 
            "userName": "胡今朝", 
            "userLink": "https://www.zhihu.com/people/fb128b56fdfed86d1a2c8237571d133f", 
            "upvote": 3, 
            "title": "图像处理结果的度量 —— SNR、PSNR、SSIM", 
            "content": "<p>衡量两幅图像的相似度：</p><p>MSE/SNR/PSNR</p><p>================================</p><ul><li>f(x,y)：表示原始的信号/图像；</li><li>f̂ (x,y)：则表示处理后的信号/图像；</li></ul><p>================================</p><p>MSE：mean squared error</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-df3fe1cec914227b8c99809c73dbe76d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"254\" data-rawheight=\"123\" class=\"content_image\" width=\"254\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;254&#39; height=&#39;123&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"254\" data-rawheight=\"123\" class=\"content_image lazy\" width=\"254\" data-actualsrc=\"https://pic2.zhimg.com/v2-df3fe1cec914227b8c99809c73dbe76d_b.jpg\"/></figure><p>================================</p><p>SNR（dB）：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-0010e68e08934a252e7646cb47b39c1a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"413\" data-rawheight=\"168\" class=\"content_image\" width=\"413\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;413&#39; height=&#39;168&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"413\" data-rawheight=\"168\" class=\"content_image lazy\" width=\"413\" data-actualsrc=\"https://pic3.zhimg.com/v2-0010e68e08934a252e7646cb47b39c1a_b.jpg\"/></figure><p>================================</p><p>PSNR（dB）：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-cea3806dc54ed8e7a5d160698c84e57d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"431\" data-rawheight=\"134\" class=\"origin_image zh-lightbox-thumb\" width=\"431\" data-original=\"https://pic2.zhimg.com/v2-cea3806dc54ed8e7a5d160698c84e57d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;431&#39; height=&#39;134&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"431\" data-rawheight=\"134\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"431\" data-original=\"https://pic2.zhimg.com/v2-cea3806dc54ed8e7a5d160698c84e57d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-cea3806dc54ed8e7a5d160698c84e57d_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "图像处理", 
                    "tagLink": "https://api.zhihu.com/topics/19556376"
                }, 
                {
                    "tag": "图像识别", 
                    "tagLink": "https://api.zhihu.com/topics/19588774"
                }, 
                {
                    "tag": "计算机视觉", 
                    "tagLink": "https://api.zhihu.com/topics/19590195"
                }
            ], 
            "comments": [
                {
                    "userName": "flickzhou", 
                    "userLink": "https://www.zhihu.com/people/9a28bb9e88ba1d662aab810457f147cf", 
                    "content": "最后这个PSNR公式的右边不敢苟同", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/45063103", 
            "userName": "胡今朝", 
            "userLink": "https://www.zhihu.com/people/fb128b56fdfed86d1a2c8237571d133f", 
            "upvote": 1, 
            "title": "线性回归视频讲解", 
            "content": "<p>线性回归视频讲解（有口误）</p><a class=\"video-box\" href=\"https://link.zhihu.com/?target=https%3A//www.zhihu.com/video/1026416474948239360\" target=\"_blank\" data-video-id=\"\" data-video-playable=\"true\" data-name=\"线性回归1\" data-poster=\"https://pic2.zhimg.com/v2-7bf3935d74946eb12b98f48aaba7c2ce.jpg\" data-lens-id=\"1026416474948239360\"><img class=\"thumbnail\" src=\"https://pic2.zhimg.com/v2-7bf3935d74946eb12b98f48aaba7c2ce.jpg\"/><span class=\"content\"><span class=\"title\">线性回归1<span class=\"z-ico-extern-gray\"></span><span class=\"z-ico-extern-blue\"></span></span><span class=\"url\"><span class=\"z-ico-video\"></span>https://www.zhihu.com/video/1026416474948239360</span></span></a><p class=\"ztext-empty-paragraph\"><br/></p><a class=\"video-box\" href=\"https://link.zhihu.com/?target=https%3A//www.zhihu.com/video/1026419677974646784\" target=\"_blank\" data-video-id=\"\" data-video-playable=\"true\" data-name=\"线性回归2\" data-poster=\"https://pic2.zhimg.com/v2-7bf3935d74946eb12b98f48aaba7c2ce.jpg\" data-lens-id=\"1026419677974646784\"><img class=\"thumbnail\" src=\"https://pic2.zhimg.com/v2-7bf3935d74946eb12b98f48aaba7c2ce.jpg\"/><span class=\"content\"><span class=\"title\">线性回归2<span class=\"z-ico-extern-gray\"></span><span class=\"z-ico-extern-blue\"></span></span><span class=\"url\"><span class=\"z-ico-video\"></span>https://www.zhihu.com/video/1026419677974646784</span></span></a><p></p>", 
            "topic": [
                {
                    "tag": "线性回归", 
                    "tagLink": "https://api.zhihu.com/topics/19650500"
                }, 
                {
                    "tag": "回归分析", 
                    "tagLink": "https://api.zhihu.com/topics/19577456"
                }, 
                {
                    "tag": "回归模型", 
                    "tagLink": "https://api.zhihu.com/topics/19600081"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/44947141", 
            "userName": "胡今朝", 
            "userLink": "https://www.zhihu.com/people/fb128b56fdfed86d1a2c8237571d133f", 
            "upvote": 6, 
            "title": "L1正则化求导方法（智商不够，暂未理解，先CTRL+C、V）", 
            "content": "<p></p><p>参考文献：</p><a href=\"https://link.zhihu.com/?target=http%3A//www.luolei.info/2016/09/27/proximalAlgo/\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">luolei.info/2016/09/27/</span><span class=\"invisible\">proximalAlgo/</span><span class=\"ellipsis\"></span></a><h2><b>Proximal Algorithm 入门</b></h2><p>正则化是机器学习方法实践中用于避免overfitting的主要方法，给优化目标加上基于L1、L2的正则项是常用的正则化方法。之前自己在实现一些机器学习方法时一直是使用L2的方法，因为L2正则项有连续可微的性质，在求导时特别方便，而基于L1的正则项（lasso）并不是处处连续的，因此在优化时有一定的难度。</p><p>虽然L1解起来有一定的难度，但是它的好处也比较明显，L1能够产生稀疏解（sparsity），而通常稀疏解的泛化能力会比较好，之前也听说过Proximal Algorithm是求解L1的很好的方法，粗看了一次也没能搞懂，后面就偷懒一直没有去学习这个方法，前段时间有机会参加CCF-ADL70的学习班，听了James Kwok的报告，讲的非常清楚，收获颇丰，Proximal Algorithm方法也在报告的内容之中，查阅一些文献总结一些粗浅认识，成了此文。</p><h2><b>为什么L1能够产生稀疏解</b></h2><p>为了追求更好的模型效果，往往使用更复杂的模型，模型的维度都是非常大的，非常容易造成过拟合（overfitting）的现象，实践中发现使用L1可以产生稀疏解，而稀疏解的模型不容易过拟合，泛化能力更好。</p><p>那么为什么使用L1就可以产生稀疏的解，而使用L2就不会呢，可以看下面一个小例子：</p><p class=\"ztext-empty-paragraph\"><br/></p><p><i>minz</i>∈<i>RL</i>=<i>λ</i>|<i>z</i>|+<i>γ</i>2(<i>z</i>−<i>x</i>)2minz∈RL=λ|z|+γ2(z−x)2</p><p class=\"ztext-empty-paragraph\"><br/></p><p>当<i>z</i>&gt;0z&gt;0时有：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>∂<i>L</i>∂<i>z</i>=<i>λ</i>+<i>γ</i>(<i>z</i>−<i>x</i>)=0∂L∂z=λ+γ(z−x)=0</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p><i>z</i>=<i>x</i>−<i>λγ</i>(<i>z</i>&gt;0)z=x−λγ(z&gt;0)</p><p class=\"ztext-empty-paragraph\"><br/></p><p>当<i>z</i>&lt;0z&lt;0时有：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>∂<i>L</i>∂<i>z</i>=−<i>λ</i>+<i>γ</i>(<i>z</i>−<i>x</i>)∂L∂z=−λ+γ(z−x)</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p><i>z</i>=<i>x</i>+<i>λγ</i>(<i>z</i>&lt;0)z=x+λγ(z&lt;0)</p><p class=\"ztext-empty-paragraph\"><br/></p><p>综上，当<i>x</i>&gt;<i>λγ</i>x&gt;λγ时，<i>z</i>=<i>x</i>−<i>λγ</i>z=x−λγ；当<i>x</i>&lt;−<i>λγ</i>x&lt;−λγ时，<i>z</i>=<i>x</i>+<i>λγ</i>z=x+λγ；当−<i>λγ</i>&lt;=<i>x</i>&lt;=<i>λγ</i>−λγ&lt;=x&lt;=λγ时，<i>z</i>=0z=0，可见L1容易产生稀疏解。</p><p>那么，如果这里的优化目标不采用L1的正则项，而是采用L2正则项会怎么样呢？</p><p class=\"ztext-empty-paragraph\"><br/></p><p><i>minz</i>∈<i>RL</i>=12<i>λz</i>2+<i>γ</i>2(<i>z</i>−<i>x</i>)2minz∈RL=12λz2+γ2(z−x)2</p><p class=\"ztext-empty-paragraph\"><br/></p><p>可直接求导：</p><p>∂<i>L</i>∂<i>z</i>=<i>λz</i>+<i>γ</i>(<i>z</i>−<i>x</i>)=0∂L∂z=λz+γ(z−x)=0</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p><i>z</i>=<i>γλ</i>+<i>γx</i>z=γλ+γx</p><p class=\"ztext-empty-paragraph\"><br/></p><p>可见L2的正则项，即使x是很接近0的数，z也只是变成了比x更接近0而已，并不能变成0，而L1的正则项可以得到0。</p><h2><b>理解Proximal Algorithm</b></h2><h2><b>为什么使用Proximal Algorithm</b></h2><p>对于目标函数不是处处连续可微的情况，通常是使用<a href=\"https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Subderivative%23The_subgradient\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">次梯度（subgradient）</a>来进行优化，由于次梯度自身的原因会导致两方面问题：</p><ul><li>求解慢</li><li>通常不会产生稀疏解</li></ul><p>Proximal Algorithm 自然肩负了要解决这两个问题的使命。</p><h2><b>Proximal Algorithm</b></h2><p>是时候揭开Proximal Algorithm的什么面纱了，首先先定义算法的核心部分proximal operator：</p><p class=\"ztext-empty-paragraph\"><br/></p><p><i>proxλf</i>(<i>v</i>)=<i>argminx</i>(<i>f</i>(<i>x</i>)+12<i>λ</i>||<i>x</i>−<i>v</i>||2)proxλf(v)=argminx(f(x)+12λ||x−v||2)</p><p class=\"ztext-empty-paragraph\"><br/></p><p>从上面这个式子可以看出，上式是在寻找一个距离v点不要太远的一个x，使得f(x)尽可能小，显然<i>f</i>(<i>x</i>)&lt;=<i>f</i>(<i>v</i>)f(x)&lt;=f(v)。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-9bd6167a8911d9c0305c72659888d16a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"594\" data-rawheight=\"466\" class=\"origin_image zh-lightbox-thumb\" width=\"594\" data-original=\"https://pic3.zhimg.com/v2-9bd6167a8911d9c0305c72659888d16a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;594&#39; height=&#39;466&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"594\" data-rawheight=\"466\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"594\" data-original=\"https://pic3.zhimg.com/v2-9bd6167a8911d9c0305c72659888d16a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-9bd6167a8911d9c0305c72659888d16a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>图片来自《Proximal Algorithms in Statistics and Machine Learning》2，这张图形象的表示了上面式子的几何意义，其中加粗的黑线表示作用域，浅色的黑线表示函数f的等高线，蓝色的点对应上面式子的v点，红色点表示最终求得的x点。</p><p>接下来介绍使用Proximal Gradient Method优化，上面提到的prox式子仿佛在优化算法里常用的迭代优化的步骤，从v点出发，找到一个更好的点x，使得<i>f</i>(<i>x</i>)&lt;=<i>f</i>(<i>v</i>)f(x)&lt;=f(v)。</p><p>设待优化目标函数为<i>F</i>(<i>x</i>)=<i>l</i>(<i>x</i>)+<i>ϕ</i>(<i>x</i>)F(x)=l(x)+ϕ(x)，其中<i>l</i>(<i>x</i>)l(x)是连续可微的，<i>ϕ</i>(<i>x</i>)ϕ(x)不是处处连续的，这类优化目标在机器学习中比较常见，如<i>l</i>(<i>x</i>)l(x)表示最小二乘的拟合误差，<i>ϕ</i>(<i>x</i>)ϕ(x)表示L1正则化因子用于产生稀疏解。</p><blockquote>Proximal Gradient Algorithm<br/>for t = 1,2…n<br/>1) Gradient Step，定义<i>vt</i>vt是沿着<i>l</i>(<i>x</i>)l(x)梯度方向找到的一个点：<br/><br/><i>vt</i>=<i>xt</i>−<i>γ</i>▽<i>l</i>(<i>xt</i>)vt=xt−γ▽l(xt)<br/><br/>2) Proximal Operator Step，使用prox式子优化<i>phi</i>(<i>x</i>)phi(x)<br/><br/><i>xt</i>+1=<i>proxλϕ</i>(<i>vt</i>)xt+1=proxλϕ(vt)<br/><br/>直到收敛或达到最大迭代次数</blockquote><p>这里有一个没有提到的是参数<i>λ</i>λ的选择，proximal算法中要求▽<i>l</i>(<i>x</i>)▽l(x)满足<a href=\"https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Lipschitz_continuity\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Lipschitz continuity</a> 系数为L，那么只需让<i>λ</i>∈(0,1<i>L</i>)λ∈(0,1L) 即可，若L的取值未知，可以使用line search的方法去找：</p><blockquote>repeat<br/></blockquote><ol><li><i>z</i>=<i>proxλϕ</i>(<i>vt</i>)z=proxλϕ(vt)</li><li>break if <i>f</i>(<i>z</i>)&lt;=<i>f</i>(<i>vt</i>)+▽<i>fT</i>(<i>vt</i>)(<i>v</i>2−<i>z</i>)+12<i>λ</i>||<i>vt</i>−<i>z</i>||2f(z)&lt;=f(vt)+▽fT(vt)(v2−z)+12λ||vt−z||2</li><li><i>λ</i>=12<i>λ</i>λ=12λ</li></ol><p><br/>return <i>xt</i>+1=<i>z</i>xt+1=z</p><h2><b>Proximal Algorithm和SGD</b></h2><p>可能会有人和我一样觉得上面算法第二步直接应用proximal operator觉得有些生硬，通常情况下大家已经习惯使用Stochastic Gradient Descent（sgd）直接优化满足处处可微的目标函数，这二者之间有哪些关系呢？</p><p>SGD是把目标函数进行一阶泰勒展开，Proximal Algorithm也是同样的，只不过Proximal Aglorithm更为严格，要求目标函数<i>F</i>(<i>x</i>)=<i>l</i>(<i>x</i>)+<i>ϕ</i>(<i>x</i>)F(x)=l(x)+ϕ(x)，其中▽<i>l</i>(<i>x</i>)▽l(x)满足Lipschitz continuity，有：</p><p class=\"ztext-empty-paragraph\"><br/></p><p><i>F</i>(<i>x</i>)=<i>l</i>(<i>x</i>)+<i>ϕ</i>(<i>x</i>)⩽<i>l</i>(<i>x</i>0)+(<i>x</i>−<i>x</i>0)<i>T</i>▽<i>l</i>(<i>x</i>0)+12<i>γ</i>||<i>x</i>−<i>x</i>0||2+<i>ϕ</i>(<i>x</i>)F(x)=l(x)+ϕ(x)⩽l(x0)+(x−x0)T▽l(x0)+12γ||x−x0||2+ϕ(x)</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p><i>whereγ</i>∈(0,1<i>L</i>]whereγ∈(0,1L]</p><p class=\"ztext-empty-paragraph\"><br/></p><p>寻找可以使F(x)最小化的x，因为直接求解F(x)不容易求解，所以转为求使得F(x)上确界的最小的x，即</p><p class=\"ztext-empty-paragraph\"><br/></p><p><i>x</i>=<i>argminxl</i>(<i>x</i>0)+(<i>x</i>−<i>x</i>0)<i>T</i>▽<i>l</i>(<i>x</i>0)+12<i>γ</i>||<i>x</i>−<i>x</i>0||2+<i>ϕ</i>(<i>x</i>)x=argminxl(x0)+(x−x0)T▽l(x0)+12γ||x−x0||2+ϕ(x)</p><p class=\"ztext-empty-paragraph\"><br/></p><p>凑方并增减常数项，得：</p><p class=\"ztext-empty-paragraph\"><br/></p><p><i>x</i>=<i>argminx</i>(<i>f</i>(<i>x</i>)+12<i>λ</i>||<i>x</i>−<i>u</i>||2)x=argminx(f(x)+12λ||x−u||2)</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p><i>whereu</i>=<i>x</i>0−<i>γ</i>▽<i>l</i>(<i>x</i>0)whereu=x0−γ▽l(x0)</p><p class=\"ztext-empty-paragraph\"><br/></p><p>由此可见，Proximal Aglorithm是在目标函数F不满足处处可微条件时，可以转而去优化目标函数的上界的自然结果。</p><h2><b>Proximal Algorithm和Trust Region</b></h2><p>最小化目标函数的优化方法中，SGD的思路是，先找到目标函数的梯度方向，然后沿着梯度方向去寻找一个步长，使得在新的坐标点上目标函数值降低。除了SGD，还有一种做最优化的基本方法，是Trust Region方法，因为在实战中泰勒公式通常只展开到一阶或二阶，高阶项被丢弃，要使得被丢弃的高阶项不至于对优化造成太大影响，下一个坐标点必须不能离原坐标点距离太大，因此Trust Region先在当前坐标点附近寻找一个小的信赖区域（类比SGD中的步长），然后在这个区域内寻找使目标函数最小的坐标点。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><i>proxλf</i>(<i>v</i>)=<i>argminx</i>(<i>f</i>(<i>x</i>)+12<i>λ</i>||<i>x</i>−<i>v</i>||2)proxλf(v)=argminx(f(x)+12λ||x−v||2)</p><p class=\"ztext-empty-paragraph\"><br/></p><p>Proximal Algorithm的式子里也体现着这种思想，最小化f(x)且要求新求得的x点不能和上一轮迭代得到的v点距离太远。</p><h2><b>加速Proximal Algorithm和ADMM</b></h2><p>最后这两个话题超过这篇小文想要介绍的范围了，以后或许会再写文章介绍这两个话题，这里只提一下。</p><p>有不少研究是想让Proximal Algorithm更快，提高收敛速度，最简单的引入神经网络中常用的“冲量”即可以加速这个算法，其他更多改进算法需要去查阅更多资料了（PS,<a href=\"https://link.zhihu.com/?target=http%3A//www.stat.rutgers.edu/home/tzhang/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">张潼</a>老师也做相关研究）。</p><p>使用Proximal Algorithm 求解一些lasso的问题的根本原因是Proximal Algorithm用起来很方便，求解很快。但Proximal Algorithm也不是对所有问题都是很方便的，比如它对L1这种，<i>ϕ</i>(<i>x</i>)=∑|<i>xi</i>|ϕ(x)=∑|xi| non-overlapping的很容易求解，对于一些其他的正则项如Group lasso就没有这么方便求解了，因此又提出了Alternating Direction Method of Multipliers(ADMM)算法用来求解这种问题。</p><h2><b>Ref</b></h2><p>1 《Big Machine Learning》 James Kwok, <a href=\"https://link.zhihu.com/?target=http%3A//www.ccf.org.cn/sites/ccf/adldongtai.jsp%3FcontentId%3D2935854532676\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">CCF ADL70</a></p><p>2 <a href=\"https://link.zhihu.com/?target=http%3A//arxiv.org/abs/1502.03175\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">《Proximal Algorithms in Statistics and Machine Learning》</a> Nicholas G. Polson, James G. Scott, Brandon T. Willard</p><p>3 <a href=\"https://link.zhihu.com/?target=http%3A//web.stanford.edu/~boyd/papers/pdf/prox_algs.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">《Proximal Algorithms》</a> Neal Parikh,Stephen Boyd</p><p></p>", 
            "topic": [
                {
                    "tag": "自然科学", 
                    "tagLink": "https://api.zhihu.com/topics/19553298"
                }, 
                {
                    "tag": "数学", 
                    "tagLink": "https://api.zhihu.com/topics/19554091"
                }
            ], 
            "comments": [
                {
                    "userName": "阿西不理喵", 
                    "userLink": "https://www.zhihu.com/people/366b36183bb4b94d4831454ed9aeb127", 
                    "content": "哇哈，吸收了不少东西", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/44945036", 
            "userName": "胡今朝", 
            "userLink": "https://www.zhihu.com/people/fb128b56fdfed86d1a2c8237571d133f", 
            "upvote": 7, 
            "title": "光流法", 
            "content": "<blockquote>Greedy method and smart method are two classes of algorithm in the world. Greedy method is the most common one!</blockquote><p>光流为optical flow，它通过连续的图像序列来检测物体微小的动作变化。光流在图像中的含义就是动作向量（motion vector）（u,v），分别表示位移在x和y方向上的变化率。</p><p>光流法可以用来做：</p><ul><li>motion based segmentation。因为它可以检测会动的物体。</li><li>structure from motion(3D shape and motion)。</li><li>alignment(global motion compensation)</li><li>video compression(一旦知道了规律的动作模式，就可以通过一张图片生成其它图片)</li></ul><h2><b>光流法的通用假设：亮度限制</b></h2><p>在理想情况下，你做了一个手势动作，摄像机记录了这个过程。你的手从第一张图片的某个位置移动到第二张图片的另一个位置，但是你的手是不变的，也就是你的手在图片的像素点的亮度值是不变的。即：</p><p><img src=\"https://www.zhihu.com/equation?tex=f%28x%2Cy%2Ct%29+%3D+f%28x%2Bdx%2Cy%2Bdy%2Ct%2Bdt%29\" alt=\"f(x,y,t) = f(x+dx,y+dy,t+dt)\" eeimg=\"1\"/></p><p>其中f是亮度值，dx，dy表示位移，dt表示过去的时间。</p><p>根据等式右边的泰勒展开公式，可以消去f(x,y,t)，从而得到一个方程：</p><p><img src=\"https://www.zhihu.com/equation?tex=f_xdx%2Bf_ydy%2Bf_tdt+%3D+0\" alt=\"f_xdx+f_ydy+f_tdt = 0\" eeimg=\"1\"/></p><p>两边除以dt得到：</p><p><img src=\"https://www.zhihu.com/equation?tex=f_x%5Cfrac%7Bdx%7D%7Bdt%7D%2Bf_y%5Cfrac%7Bdy%7D%7Bdt%7D%2Bf_t%3D0%5CRightarrow+f_xu%2Bf_yv%2Bf_t%3D0\" alt=\"f_x\\frac{dx}{dt}+f_y\\frac{dy}{dt}+f_t=0\\Rightarrow f_xu+f_yv+f_t=0\" eeimg=\"1\"/></p><p>这就是一般的亮度限制方程。一般我们把dx/dt写成u，dy/dt写成v，（u,v）就是我们要计算的光流，也就是像素点的运动速度和方向。</p><p>我们的目标就是要求解（u，v）。</p><p>可是问题来了，一个方程两个未知数是不能求解的，怎么寻找额外的方程来求得（u，v）呢？</p><p>下面两篇论文给出了自己的方法。</p><h2><b>Horn&amp;Schunck method</b></h2><blockquote>One way to express the additional constraint is to limit the difference between the flow velocity at a point and the average velocity over a small neighborhood containing the point。</blockquote><p>意思就是说我跟我邻域的像素点的速度不能相差太大。作者使用了最小化u、v的二阶导数来实现这一限制。这个限制又称为平滑性限制（smoothness constraint）。</p><p>于是，寻找光流法的问题变成了一个最优化的问题。其目标函数是：</p><p><img src=\"https://www.zhihu.com/equation?tex=min%5Cint%5Cint%28f_xu%2Bf_yv%2Bft%29%5E2%2B%5Clambda%28u_x%5E2%2Bu_y%5E2%2Bv_x%5E2%2Bv_y%5E2%29dxdy\" alt=\"min\\int\\int(f_xu+f_yv+ft)^2+\\lambda(u_x^2+u_y^2+v_x^2+v_y^2)dxdy\" eeimg=\"1\"/></p><p>意思就是遍历图像的所有像素点，找到某个（u，v），使得目标函数最小化。其中第一项是brightness constraint，第二项是smoothness constraint。</p><p>在变分法里面，对目标函数求偏导得到两个方程：</p><p><img src=\"https://www.zhihu.com/equation?tex=%28f_xu%2Bf_yv%2Bf_t%29f_x%2B%5Clambda%28%5CDelta%5E2u%29%3D0\" alt=\"(f_xu+f_yv+f_t)f_x+\\lambda(\\Delta^2u)=0\" eeimg=\"1\"/></p><p><img src=\"https://www.zhihu.com/equation?tex=%28f_xu%2Bf_yv%2Bf_t%29f_y%2B%5Clambda%28%5CDelta%5E2v%29%3D0\" alt=\"(f_xu+f_yv+f_t)f_y+\\lambda(\\Delta^2v)=0\" eeimg=\"1\"/></p><p>很好，这样我们就得到了两个方程，两个未知数，在理论上我们是可以求解得到（u，v）的。</p><p>在具体实现上，需要用离散的值来逼近上述的已知数。</p><p>最后通过牛顿法迭代求解得到光流（u，v）。</p><h2><b>Lucas&amp;Kanade method</b></h2><p>这一方法假设像素点附近（局部）都具有相同的光流。于是根据brightness constraint equation有：</p><p><img src=\"https://www.zhihu.com/equation?tex=f_%7Bx1%7Du%2Bf_%7By1%7Dv+%3D+-f_t\" alt=\"f_{x1}u+f_{y1}v = -f_t\" eeimg=\"1\"/></p><p><img src=\"https://www.zhihu.com/equation?tex=f_%7Bx2%7Du%2Bf_%7By2%7Dv+%3D+-f_t\" alt=\"f_{x2}u+f_{y2}v = -f_t\" eeimg=\"1\"/></p><p>...</p><p><img src=\"https://www.zhihu.com/equation?tex=f_%7Bxn%7Du%2Bf_%7Byn%7Dv+%3D+-f_t\" alt=\"f_{xn}u+f_{yn}v = -f_t\" eeimg=\"1\"/></p><p>其中1,2，，，n是局部窗口内的像素。那么我们就得到了n个方程两个未知数，也是可以求解的。</p><p>把上述等式写成矩阵形式：AU=b</p><p>上述方程可以使用最小平方法获得一个近似解。</p><p>我们可以左乘A的转置得到2*2的方程组：</p><p><img src=\"https://www.zhihu.com/equation?tex=A%5ETAU%3DA%5ETb\" alt=\"A^TAU=A^Tb\" eeimg=\"1\"/></p><p>所以如果 <img src=\"https://www.zhihu.com/equation?tex=A%5ETA\" alt=\"A^TA\" eeimg=\"1\"/> 存在逆的话，</p><p>则光流U= <img src=\"https://www.zhihu.com/equation?tex=%28A%5ETA%29%5E%7B-1%7DA%5ETb\" alt=\"(A^TA)^{-1}A^Tb\" eeimg=\"1\"/> 。</p><h2><b>Conclusion</b></h2><p>又一次，梯度、laplacian、泰勒展开、数值分析。这些基础是我们做问题建模的必要前提。很多时候，计算机视觉里面的任务最终都变成了一个最优化问题，因此我应该多去上上课，补补基础。</p><h2><b>Reference</b></h2><a href=\"https://link.zhihu.com/?target=https%3A//zh.wikipedia.org/wiki/%25E5%258D%25A2%25E5%258D%25A1%25E6%2596%25AF-%25E5%258D%25A1%25E7%25BA%25B3%25E5%25BE%25B7%25E6%2596%25B9%25E6%25B3%2595\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">lucas-kanade method</a><a href=\"https://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3DkJouUVZ0QqU%26index%3D7%26list%3DPLd3hlSJsX_ImKP68wfKZJVIPTd8Ie5u-9\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Optical flow</a><a href=\"https://link.zhihu.com/?target=https%3A//dspace.mit.edu/bitstream/handle/1721.1/6337/%25EE%2580%2580AIM%25EE%2580%2581-572.pdf%3Fsequence%3D2\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Determining optical flow</a><a href=\"https://link.zhihu.com/?target=https%3A//pdfs.semanticscholar.org/6c68/ebb14d2d53806386abfad7a454ce9f6360b1.pdf\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">An iterative image registration technique with an application to stereo vision</a><p></p>", 
            "topic": [
                {
                    "tag": "数学", 
                    "tagLink": "https://api.zhihu.com/topics/19554091"
                }
            ], 
            "comments": [
                {
                    "userName": "CV小生", 
                    "userLink": "https://www.zhihu.com/people/0c05a21750fab59f95b2edc1132b473e", 
                    "content": "您好，如果可视化水平和垂直轴的光流图，这样做的话，像素点移动的方向信息岂不是会丢失？", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/41053095", 
            "userName": "胡今朝", 
            "userLink": "https://www.zhihu.com/people/fb128b56fdfed86d1a2c8237571d133f", 
            "upvote": 1, 
            "title": "✡︎initializer与batch_norm", 
            "content": "<p></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-79ae8922d99dce8d5cd1cbcdedcf2765_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"826\" data-rawheight=\"525\" class=\"origin_image zh-lightbox-thumb\" width=\"826\" data-original=\"https://pic2.zhimg.com/v2-79ae8922d99dce8d5cd1cbcdedcf2765_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;826&#39; height=&#39;525&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"826\" data-rawheight=\"525\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"826\" data-original=\"https://pic2.zhimg.com/v2-79ae8922d99dce8d5cd1cbcdedcf2765_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-79ae8922d99dce8d5cd1cbcdedcf2765_b.jpg\"/></figure><p>➢在神经网络中为了增加非线性，我们会使用激活函数如sigmoid</p><p>➢但是，如果wx+b过大或者过小，在经过激活函数后的值的变化幅度都会很小，正向传播时 sigmoid(10)和sigmoid(100)几乎没有差别，sigmoid(-10)和sigmoid(-100)几乎没有差别，出现饱和现象（激活值为1）或者激活值为0的现象，反向传播时的梯度会趋近于0，出现梯度消失；</p><p>➢所以，我们希望wx+b落在激活函数的活跃区域</p><p>✡︎为此，我们一方面对x进行batch_normalization的处理，另一方面对w进行科学的初始化</p><hr/><h2><b>Batch_Normalization</b></h2><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-d74234555981dced8f32d80fdc7f64e3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"720\" data-rawheight=\"488\" class=\"origin_image zh-lightbox-thumb\" width=\"720\" data-original=\"https://pic4.zhimg.com/v2-d74234555981dced8f32d80fdc7f64e3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;720&#39; height=&#39;488&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"720\" data-rawheight=\"488\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"720\" data-original=\"https://pic4.zhimg.com/v2-d74234555981dced8f32d80fdc7f64e3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-d74234555981dced8f32d80fdc7f64e3_b.jpg\"/></figure><p>这里的scale和shift是可以学习的，也就是说在哪个区间能使得loss最小就选择哪个区间，并不是一定在[0, 1]上</p><h2><b>参数初始化</b></h2><p>参数初始化后的效果也是要求wx+b落在激活函数的活跃区域，那么本质上也要要求w每一层都服从相同的分布，不能这一层很大，下一层很小，这样每一层的数据的范围才能大致相同</p><p>根据假设条件：各层的服从相同的分布，均值为0</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-8f16352d7e15893599703120f6ab5f0e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"871\" data-rawheight=\"1166\" class=\"origin_image zh-lightbox-thumb\" width=\"871\" data-original=\"https://pic3.zhimg.com/v2-8f16352d7e15893599703120f6ab5f0e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;871&#39; height=&#39;1166&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"871\" data-rawheight=\"1166\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"871\" data-original=\"https://pic3.zhimg.com/v2-8f16352d7e15893599703120f6ab5f0e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-8f16352d7e15893599703120f6ab5f0e_b.jpg\"/></figure><p>说明:</p><p>1.如果是图像的话stddev=tf.sqrt[2/(k*<b>k</b>*dl-1)]   w的个数相当于所有filter的所有点的和</p><p>2.结合反向传播，服从的分布是2/(n_<i>in+n_</i>out)</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>😂反向传播中，怎么推导的暂时还没有理解透彻</p><p></p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "TensorFlow", 
                    "tagLink": "https://api.zhihu.com/topics/20032249"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/39872003", 
            "userName": "胡今朝", 
            "userLink": "https://www.zhihu.com/people/fb128b56fdfed86d1a2c8237571d133f", 
            "upvote": 1, 
            "title": "人工神经网络的底层缺陷", 
            "content": "<p>人工神经网络是对生物神经网络的数学抽象，但这种抽象的结构和生物神经网络的结构有很大的差别，这就造成了人工神经网络在设计底层上必然会存在缺陷。</p><h2><b>生物神经网络介绍</b></h2><p><a href=\"https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%25E7%25A5%259E%25E7%25BB%258F%25E5%2585%2583/674777\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">神经元</a>的数目如此巨大，它们之间的联系也必然非常复杂，现介绍几种主要的联系方式：</p><p><b>辐散</b></p><p>一个<a href=\"https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%25E7%25A5%259E%25E7%25BB%258F%25E5%2585%2583/674777\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">神经元</a>的<a href=\"https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%25E8%25BD%25B4%25E7%25AA%2581/1561671\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">轴突</a>可以通过分支与许多神经元建立<a href=\"https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%25E7%25AA%2581%25E8%25A7%25A6/449914\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">突触</a>联系，称为辐散。例如在脊髓，<a href=\"https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%25E4%25BC%25A0%25E5%2585%25A5%25E7%25A5%259E%25E7%25BB%258F%25E5%2585%2583/2070598\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">传入神经元</a>的纤维进入中枢后，除以分支与本节段脊髓的<a href=\"https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%25E4%25B8%25AD%25E9%2597%25B4%25E7%25A5%259E%25E7%25BB%258F%25E5%2585%2583/11032824\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">中间神经元</a>及<a href=\"https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%25E4%25BC%25A0%25E5%2587%25BA%25E7%25A5%259E%25E7%25BB%258F%25E5%2585%2583/2070705\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">传出神经元</a>发生突触联系外，还有上升和下降分支与相邻节段脊髓的中间神经元发生突触联系。因此，传入神经元与其他神经元的联系方式主要是辐散。这种联系方式可使一个<a href=\"https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%25E7%25A5%259E%25E7%25BB%258F%25E5%2585%2583/674777\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">神经元</a>的兴奋引起许多神经元同时兴奋或抑制，形成兴奋或抑制的扩散。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-873904e109ec76547ae542ea316e7b43_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"220\" data-rawheight=\"187\" class=\"content_image\" width=\"220\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;220&#39; height=&#39;187&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"220\" data-rawheight=\"187\" class=\"content_image lazy\" width=\"220\" data-actualsrc=\"https://pic4.zhimg.com/v2-873904e109ec76547ae542ea316e7b43_b.jpg\"/></figure><p><b>聚合</b></p><p>一个<a href=\"https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%25E7%25A5%259E%25E7%25BB%258F%25E5%2585%2583/674777\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">神经元</a>的<a href=\"https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%25E8%2583%259E%25E4%25BD%2593/2069776\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">胞体</a>与树突表面可接受许多来自不同神经元的<a href=\"https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%25E7%25AA%2581%25E8%25A7%25A6/449914\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">突触</a>联系，</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-fc11fb2b4a26cbbc24952134f28604db_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"220\" data-rawheight=\"185\" class=\"content_image\" width=\"220\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;220&#39; height=&#39;185&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"220\" data-rawheight=\"185\" class=\"content_image lazy\" width=\"220\" data-actualsrc=\"https://pic4.zhimg.com/v2-fc11fb2b4a26cbbc24952134f28604db_b.jpg\"/></figure><p>称为聚合。这种联系方式可使许多神经元的兴奋作用聚合在一个神经元上，引起后者的兴奋；也可使来自许多不同神经元的兴奋和抑制作用在同一神经元上而发生<a href=\"https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%25E6%258B%25AE%25E6%258A%2597/8432336\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">拮抗</a>。在中枢神经系统内，<a href=\"https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%25E4%25BC%25A0%25E5%2587%25BA%25E7%25A5%259E%25E7%25BB%258F%25E5%2585%2583/2070705\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">传出神经元</a>接受其他神经元的突触联系，主要是聚合方式；例如脊髓前角<a href=\"https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%25E8%25BF%2590%25E5%258A%25A8%25E7%25A5%259E%25E7%25BB%258F%25E5%2585%2583/5148402\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">运动神经元</a>，它接受许多不同来源的<a href=\"https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%25E7%25AA%2581%25E8%25A7%25A6/449914\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">突触</a>联系，其中有兴奋性的，也有抑制性的。因此，脊髓前角运动神经元最终表现为兴奋还是抑制，以及兴奋或抑制的程度有多大，则取决于不同来源的兴奋和抑制作用相互拮抗的结果。这一规律称为最后公路原则。传出神经元是各种来源的突触联系的<a href=\"https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%25E6%259C%2580%25E5%2590%258E%25E5%2585%25AC%25E8%25B7%25AF/7606175\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">最后公路</a>，由它传出冲动产生反射活动效应，通过各种来源作用的<a href=\"https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%25E6%258B%25AE%25E6%258A%2597/8432336\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">拮抗</a>使效应在强度上更为协调。</p><p><b>联系</b></p><p><a href=\"https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%25E4%25B8%25AD%25E9%2597%25B4%25E7%25A5%259E%25E7%25BB%258F%25E5%2585%2583/11032824\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">中间神经元</a>之间的联系更为复杂，形式多样，有的形成链锁状，有的链锁状与环状联系呈环状。在这些联系中，辐散和聚合同时存在。兴奋通过链锁状联系，在空间上扩大其作用范围。兴奋通过环状联系时，由于环路中<a href=\"https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%25E7%25A5%259E%25E7%25BB%258F%25E5%2585%2583/674777\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">神经元</a>的性质不同而表现出不同的效应。如果环路中各种神经元的生理效应相同，则兴奋由于反复在环路中传导，导致兴奋活动时间延长。如果环路中存在抑制性<a href=\"https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%25E4%25B8%25AD%25E9%2597%25B4%25E7%25A5%259E%25E7%25BB%258F%25E5%2585%2583/11032824\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">中间神经元</a>，则兴奋经过环状联系将使原来的神经元活动减弱或及时终止。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ae3c1205a82c27635114651d5657418c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"899\" data-rawheight=\"509\" class=\"origin_image zh-lightbox-thumb\" width=\"899\" data-original=\"https://pic1.zhimg.com/v2-ae3c1205a82c27635114651d5657418c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;899&#39; height=&#39;509&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"899\" data-rawheight=\"509\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"899\" data-original=\"https://pic1.zhimg.com/v2-ae3c1205a82c27635114651d5657418c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-ae3c1205a82c27635114651d5657418c_b.jpg\"/></figure><h2><b>结论</b></h2><p>不光是神经元的连接方式中人工神经网络对生物神经网络进行了简化，当然还有神经元的个数以及神经元到底有多少层，这些对应到人工神经网络中是相应的超参，这些都是影响人工神经网络是否能够很好模拟生物神经网络的因素，所以从底层上来说，神经网络是有内生性缺陷的。</p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "神经网络", 
                    "tagLink": "https://api.zhihu.com/topics/19607065"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/39261779", 
            "userName": "胡今朝", 
            "userLink": "https://www.zhihu.com/people/fb128b56fdfed86d1a2c8237571d133f", 
            "upvote": 1, 
            "title": "SELU   ELU", 
            "content": "<p></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-7071fc6a63a89bd95ea7d5d4660acbee_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"298\" data-rawheight=\"61\" class=\"content_image\" width=\"298\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;298&#39; height=&#39;61&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"298\" data-rawheight=\"61\" class=\"content_image lazy\" width=\"298\" data-actualsrc=\"https://pic3.zhimg.com/v2-7071fc6a63a89bd95ea7d5d4660acbee_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-839c1923a426af5a03308fba3ee30ac5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"570\" data-rawheight=\"85\" class=\"origin_image zh-lightbox-thumb\" width=\"570\" data-original=\"https://pic2.zhimg.com/v2-839c1923a426af5a03308fba3ee30ac5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;570&#39; height=&#39;85&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"570\" data-rawheight=\"85\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"570\" data-original=\"https://pic2.zhimg.com/v2-839c1923a426af5a03308fba3ee30ac5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-839c1923a426af5a03308fba3ee30ac5_b.jpg\"/></figure><p>selu与relu 的唯一区别就是x有没有乘以/alpha</p>", 
            "topic": [
                {
                    "tag": "科技", 
                    "tagLink": "https://api.zhihu.com/topics/19556664"
                }, 
                {
                    "tag": "数学", 
                    "tagLink": "https://api.zhihu.com/topics/19554091"
                }
            ], 
            "comments": [
                {
                    "userName": "vBaiCai", 
                    "userLink": "https://www.zhihu.com/people/482c01a6ed6f9d679ac4adfeaf5bc894", 
                    "content": "<p>/lambda</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/39030338", 
            "userName": "胡今朝", 
            "userLink": "https://www.zhihu.com/people/fb128b56fdfed86d1a2c8237571d133f", 
            "upvote": 4, 
            "title": "通用近似定理", 
            "content": "<p>        在人工神经网络领域的数学观点中，「<b>通用近似定理</b> (Universal approximation theorem，一译万能逼近定理)」指的是：如果一个前馈神经网络具有线性输出层和至少一层隐藏层，只要给予网络足够数量的神经元，便可以实现以足够高精度来逼近任意一个在 ℝn 的紧子集 (Compact subset) 上的连续函数。</p><p>        这一定理表明，只要给予了适当的参数，我们便可以通过简单的神经网络架构去拟合一些现实中非常有趣、复杂的函数。这一拟合能力也是神经网络架构能够完成现实世界中复杂任务的原因。尽管如此，此定理并没有涉及到这些参数的算法可学性 (Algorithmic learnablity)。</p><p>        通用近似定理用数学语言描述如下：</p><p>令 φ 为一单调递增、有界的非常数连续函数。记 m 维单元超立方体 (Unit hypercube) [0,1]m为 Im，并记在 Im 上的连续函数的值域为 C(Im)。则对任意实数 ϵ&gt;0 与函数 f∈C(Im)，存在整数 N、常数 vi,bi∈ℝ 与向量 wi∈ℝm(i=1,…,n)，使得我们可以定义：F(x)=∑i=1Nviφ(wTix+bi)</p><p>为 f 的目标拟合实现。在这里， f 与 φ 无关，亦即对任意 x∈Im，有：|F(x)–f(x)|&lt;ϵ</p><p>因此，形为 F(x) 这样的函数在 C(Im) 里是<b>稠密</b>的。替换上述 Im 为 ℝm 的任意紧子集，结论依然成立。</p><p>在 1989 年，George Cybenko 最早提出并证明了这一定理在激活函数为 Sigmoid 函数时的特殊情况。那时，这一定理被看作是 Sigmoid 函数的特殊性质。但两年之后，Kurt Hornik 研究发现，造就「通用拟合」这一特性的根源并非 Sigmoid 函数，而是<b>多层前馈神经网络这一架构本身</b>。当然，所用的激活函数仍然必须满足一定的弱条件假设，常数函数便是显然无法实现的。</p>", 
            "topic": [
                {
                    "tag": "自然科学", 
                    "tagLink": "https://api.zhihu.com/topics/19553298"
                }, 
                {
                    "tag": "数学", 
                    "tagLink": "https://api.zhihu.com/topics/19554091"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/38964806", 
            "userName": "胡今朝", 
            "userLink": "https://www.zhihu.com/people/fb128b56fdfed86d1a2c8237571d133f", 
            "upvote": 5, 
            "title": "双线性插值的实现方法：filter+deconv", 
            "content": "<p>卷积神经网络经常要对图像进行上采样，假设现在要将3×6的图象转变为6×12的图象。双线性插值传统实现方法为：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-aa4cfbd625e022eb0babe55823a60fcf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"800\" data-rawheight=\"500\" class=\"origin_image zh-lightbox-thumb\" width=\"800\" data-original=\"https://pic4.zhimg.com/v2-aa4cfbd625e022eb0babe55823a60fcf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;800&#39; height=&#39;500&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"800\" data-rawheight=\"500\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"800\" data-original=\"https://pic4.zhimg.com/v2-aa4cfbd625e022eb0babe55823a60fcf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-aa4cfbd625e022eb0babe55823a60fcf_b.jpg\"/></figure><p>上图中， <img src=\"https://www.zhihu.com/equation?tex=y_%7Bm%2Cn%7D%3D%281-a%29%281-b%29x_%7Bi%2Cj%7D%2Ba%281-b%29x_%7Bi%2B1%2Cj%7D%2B%281-a%29bx_%7Bi%2Cj%2B1%7D%2Babx_%7Bi%2B1%2Cj%2B1%7D+%280%5Cleq+a%2Cb%5Cleq1%29\" alt=\"y_{m,n}=(1-a)(1-b)x_{i,j}+a(1-b)x_{i+1,j}+(1-a)bx_{i,j+1}+abx_{i+1,j+1} (0\\leq a,b\\leq1)\" eeimg=\"1\"/><br/>这种方法相当于在低分辨率图象上进行一系列卷积核变化的卷积操作。<br/>双线性插值的另外一种实现方法是用deconvolution方法实现（以下参考自<a href=\"https://link.zhihu.com/?target=http%3A//cv-tricks.com/image-segmentation/transpose-convolution-in-tensorflow/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Image Segmentation using deconvolution layer in Tensorflow</a>）</p><div class=\"highlight\"><pre><code class=\"language-text\">步骤1：计算卷积核。\nimport numpy as np\nimport tensorflow as tf\ndef get_bilinear_filter(filter_shape, upscale_factor):\n ##filter_shape is [width, height, num_in_channels, num_out_channels]\n kernel_size = 2*upscale_factor - upscale_factor%2\n ### Centre location of the filter for which value is calculated\n if kernel_size % 2 == 1:\n centre_location = upscale_factor - 1\n else:\n centre_location = upscale_factor - 0.5\n bilinear = np.zeros([filter_shape[0], filter_shape[1]])\n for x in range(filter_shape[0]):\n for y in range(filter_shape[1]):\n ##Interpolation Calculation\n value = (1 - abs((x - centre_location)/ upscale_factor)) * (1 - abs((y - centre_location)/ upscale_factor))\n bilinear[x, y] = value\nreturn bilinear\n\n现在我们要将3×6的图象转变为6×12的图象，即已知upscale_factor=2。filter_shape是卷积核的形状，由kernel_size = 2*upscale_factor - upscale_factor%2计算得到，filter_shape=[4,4,1,1]。\nbilinear = get_bilinear_filter([4,4,1,1,],2)\nprint(bilinear)\narray([[ 0.0625, 0.1875, 0.1875, 0.0625],\n [ 0.1875, 0.5625, 0.5625, 0.1875],\n [ 0.1875, 0.5625, 0.5625, 0.1875],\n [ 0.0625, 0.1875, 0.1875, 0.0625]])\n\n步骤2：deconvolution计算得到6×12的目标图象。\n#3*6图像\nbottom = tf.constant([[1,2,3,4,5,6],[7,8,9,10,11,12],[13,14,15,16,17,18]], dtype=&#39;float32&#39;)\nbottom = tf.reshape(bottom, [1, 3, 6, 1])\nweights = tf.reshape(bilinear, [4, 4, 1, 1])\nstride = upscale_factor\nstrides = [1, stride, stride, 1] \ndeconv = tf.nn.conv2d_transpose(bottom, weights, [1,6,12,1],strides, padding=&#39;SAME&#39;)\nwith tf.Session() as sess:\n print(&#34;deconv: &#34;, deconv.eval())\n</code></pre></div><p><br/>下面给出以上过程的示意图：<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-cbc55b4b234227cdbe53212d71f7550c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"500\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic1.zhimg.com/v2-cbc55b4b234227cdbe53212d71f7550c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;500&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"500\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic1.zhimg.com/v2-cbc55b4b234227cdbe53212d71f7550c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-cbc55b4b234227cdbe53212d71f7550c_b.jpg\"/></figure><p>我们知道deconvolution操作首先要在3×6的图像上（对应上图中第1张图像）不同像素间进行插0的操作，即得到5×11的图象（第2张图像的中间部分）。<br/>该5×11的图象要与已知是4×4的卷积核（第2个箭头上的卷积核）进行步长为1卷积，得到目标6×12的图象（第3张图像），计算得到padding为2。用 0 padding后，得到第2张图像。<br/>卷积核的取值上面已经由代码得到，下面用示意图给出卷积核中4个位置的数值的具体计算方法，其他12个位置的计算类似：<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-0faaee75f4402e9901c99ea41573e741_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"800\" data-rawheight=\"500\" class=\"origin_image zh-lightbox-thumb\" width=\"800\" data-original=\"https://pic2.zhimg.com/v2-0faaee75f4402e9901c99ea41573e741_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;800&#39; height=&#39;500&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"800\" data-rawheight=\"500\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"800\" data-original=\"https://pic2.zhimg.com/v2-0faaee75f4402e9901c99ea41573e741_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-0faaee75f4402e9901c99ea41573e741_b.jpg\"/></figure><p>右图卷积核作用在左图中虚线框出的4×4区域时，真正有效的卷积核参数是填充色为红色的4个位置的参数。采样点为centre_location = upscale_factor - 0.5=3/2，即为下图中的红色点：<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-96413bbaa558ebaf2f4dc2a921c37aea_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"300\" data-rawheight=\"300\" class=\"content_image\" width=\"300\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;300&#39; height=&#39;300&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"300\" data-rawheight=\"300\" class=\"content_image lazy\" width=\"300\" data-actualsrc=\"https://pic3.zhimg.com/v2-96413bbaa558ebaf2f4dc2a921c37aea_b.jpg\"/></figure><p>其中， <img src=\"https://www.zhihu.com/equation?tex=1-a%3D%5Cfrac%7B%5Cfrac%7B3%7D%7B2%7D%7D%7B2%7D%3D%5Cfrac%7B3%7D%7B4%7D%2Ca%3D%5Cfrac%7B%5Cfrac%7B1%7D%7B2%7D%7D%7B2%7D%3D%5Cfrac%7B1%7D%7B4%7D\" alt=\"1-a=\\frac{\\frac{3}{2}}{2}=\\frac{3}{4},a=\\frac{\\frac{1}{2}}{2}=\\frac{1}{4}\" eeimg=\"1\"/><br/>同样的， <img src=\"https://www.zhihu.com/equation?tex=1-b%3D%5Cfrac%7B3%7D%7B4%7D%2Cb%3D%5Cfrac%7B1%7D%7B4%7D\" alt=\"1-b=\\frac{3}{4},b=\\frac{1}{4}\" eeimg=\"1\"/><br/>根据传统双线性插值知， <img src=\"https://www.zhihu.com/equation?tex=y_%7Bm%2Cn%7D%3D%5Cfrac%7B1%7D%7B16%7Dx_%7B00%7D%2B%5Cfrac%7B3%7D%7B16%7Dx_%7B01%7D%2B%5Cfrac%7B3%7D%7B16%7Dx_%7B10%7D%2B%5Cfrac%7B9%7D%7B16%7Dx_%7B11%7D\" alt=\"y_{m,n}=\\frac{1}{16}x_{00}+\\frac{3}{16}x_{01}+\\frac{3}{16}x_{10}+\\frac{9}{16}x_{11}\" eeimg=\"1\"/><br/>所以填充色为红色的4个位置的卷积核参数依次为 <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7B16%7D%EF%BC%8C%5Cfrac%7B3%7D%7B16%7D%EF%BC%8C%5Cfrac%7B3%7D%7B16%7D%EF%BC%8C%5Cfrac%7B9%7D%7B16%7D%E3%80%82\" alt=\"\\frac{1}{16}，\\frac{3}{16}，\\frac{3}{16}，\\frac{9}{16}。\" eeimg=\"1\"/><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-d9cf1332e14de327eaa4533502e39a4d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"800\" data-rawheight=\"500\" class=\"origin_image zh-lightbox-thumb\" width=\"800\" data-original=\"https://pic2.zhimg.com/v2-d9cf1332e14de327eaa4533502e39a4d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;800&#39; height=&#39;500&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"800\" data-rawheight=\"500\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"800\" data-original=\"https://pic2.zhimg.com/v2-d9cf1332e14de327eaa4533502e39a4d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-d9cf1332e14de327eaa4533502e39a4d_b.jpg\"/></figure><p>整个卷积核参数的计算过程如下：<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-c49bb01834083d31a8ee21c5bc2bda2e_b.gif\" data-caption=\"\" data-size=\"normal\" data-thumbnail=\"https://pic3.zhimg.com/v2-c49bb01834083d31a8ee21c5bc2bda2e_b.jpg\" class=\"content_image\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;0&#39; height=&#39;0&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-thumbnail=\"https://pic3.zhimg.com/v2-c49bb01834083d31a8ee21c5bc2bda2e_b.jpg\" class=\"content_image lazy\" data-actualsrc=\"https://pic3.zhimg.com/v2-c49bb01834083d31a8ee21c5bc2bda2e_b.gif\"/></figure><p>以上就是在高分辨率图像上做卷积核不变的卷积运算得到双线性插值后图像的方法。这种方法采样点的取法为：<br/><img src=\"https://www.zhihu.com/equation?tex=%E5%AE%BD%E5%BA%A6%E6%96%B9%E5%90%91%EF%BC%9A%5Cfrac%7By%2B0.5%7D%7B11.5%2B0.5%7D%3D%5Cfrac%7Bx%2B0.5%7D%7B5.5%2B0.5%7D%5CRightarrow+x%3D%5Cfrac%7By%7D%7B2%7D-%5Cfrac%7B1%7D%7B4%7D+%28y%E7%9A%84%E5%8F%96%E5%80%BC%E4%B8%BA0%2C1%2C...%2C11%29\" alt=\"宽度方向：\\frac{y+0.5}{11.5+0.5}=\\frac{x+0.5}{5.5+0.5}\\Rightarrow x=\\frac{y}{2}-\\frac{1}{4} (y的取值为0,1,...,11)\" eeimg=\"1\"/><br/><img src=\"https://www.zhihu.com/equation?tex=%E9%AB%98%E5%BA%A6%E6%96%B9%E5%90%91%EF%BC%9A%5Cfrac%7By%2B0.5%7D%7B5.5%2B0.5%7D%3D%5Cfrac%7Bx%2B0.5%7D%7B2.5%2B0.5%7D%5CRightarrow+x%3D%5Cfrac%7By%7D%7B2%7D-%5Cfrac%7B1%7D%7B4%7D+%28y%E7%9A%84%E5%8F%96%E5%80%BC%E4%B8%BA0%2C1%2C...%2C5%29\" alt=\"高度方向：\\frac{y+0.5}{5.5+0.5}=\\frac{x+0.5}{2.5+0.5}\\Rightarrow x=\\frac{y}{2}-\\frac{1}{4} (y的取值为0,1,...,5)\" eeimg=\"1\"/><br/>可以验证opencv中的imresize方法采样点也是这种取法，但是它不是直接用 0 padding，而是由边界的像素点决定padding数值。所以cv2.resize的计算结果和tf.nn.conv2d_transpose的计算结果不同。</p><p></p>", 
            "topic": [
                {
                    "tag": "计算机视觉", 
                    "tagLink": "https://api.zhihu.com/topics/19590195"
                }
            ], 
            "comments": [
                {
                    "userName": "发条男孩", 
                    "userLink": "https://www.zhihu.com/people/ecd9d4b87b73a15971dd0ebfaf670500", 
                    "content": "您好，最近在学习fcn时涉及到了deconvolution，看了您这篇文章感觉明白了很多，就是在最后的部分，按照您的例子，采样点的坐标是(1.5，1.5)么？如果这样的话，为什么1-a为0.75呢？为什么除以2？", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "梦的狩猎者", 
                    "userLink": "https://www.zhihu.com/people/875a4f5c0057333cb89fa36d3a3873f1", 
                    "content": "<p>看了很多博客，这一篇是唯一一篇把fcn的deconvolution原理讲清楚的</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "leeh0m", 
                    "userLink": "https://www.zhihu.com/people/b9a8ff93fb51c6ad8ff4c7bff9785693", 
                    "content": "牛逼", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "知乎用户", 
                    "userLink": "https://www.zhihu.com/people/0", 
                    "content": "<p>公式里xi,j的系数是不是应该是ab</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "yui", 
                    "userLink": "https://www.zhihu.com/people/1eb005c3a383f4c6acb479e2cdd64fab", 
                    "content": "<p>很好的文章。但貌似开头ymn公式系数写反了，距离应该和影响成反比。</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "嗯哼哼", 
                    "userLink": "https://www.zhihu.com/people/fafec6e8279085ae65e858df77fadc24", 
                    "content": "<p><a href=\"https://zhuanlan.zhihu.com/p/32414293\" class=\"internal\"><span class=\"invisible\">https://</span><span class=\"visible\">zhuanlan.zhihu.com/p/32</span><span class=\"invisible\">414293</span><span class=\"ellipsis\"></span></a></p><p>一模一样？？</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/38962672", 
            "userName": "胡今朝", 
            "userLink": "https://www.zhihu.com/people/fb128b56fdfed86d1a2c8237571d133f", 
            "upvote": 0, 
            "title": "双线性插值", 
            "content": "<p>双线性插值</p><p>➢用来将图像放大的一种算法</p><p>➢假设源图像大小为mxn，目标图像为axb。那么两幅图像的边长比分别为：m/a和n/b，注意，通常这个比例不是整数，编程存储的时候要用浮点型。</p><p>➢目标图像的第（i,j）个像素点（i行j列）可以通过边长比对应图像。其对应坐标为（i*m/a,j*n/b），最左上角的像素为第零个像素点。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-2e0aa6c45e1533f0b20396a8a11565e9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1708\" data-rawheight=\"125\" class=\"origin_image zh-lightbox-thumb\" width=\"1708\" data-original=\"https://pic2.zhimg.com/v2-2e0aa6c45e1533f0b20396a8a11565e9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1708&#39; height=&#39;125&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1708\" data-rawheight=\"125\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1708\" data-original=\"https://pic2.zhimg.com/v2-2e0aa6c45e1533f0b20396a8a11565e9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-2e0aa6c45e1533f0b20396a8a11565e9_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-0631898a09797e847b09395b53bfc0cf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1708\" data-rawheight=\"125\" class=\"origin_image zh-lightbox-thumb\" width=\"1708\" data-original=\"https://pic4.zhimg.com/v2-0631898a09797e847b09395b53bfc0cf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1708&#39; height=&#39;125&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1708\" data-rawheight=\"125\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1708\" data-original=\"https://pic4.zhimg.com/v2-0631898a09797e847b09395b53bfc0cf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-0631898a09797e847b09395b53bfc0cf_b.jpg\"/></figure><p>然后在 y 方向进行线性插值，得到</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-a42f25595b4a99878315cfa6555ef627_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1004\" data-rawheight=\"125\" class=\"origin_image zh-lightbox-thumb\" width=\"1004\" data-original=\"https://pic4.zhimg.com/v2-a42f25595b4a99878315cfa6555ef627_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1004&#39; height=&#39;125&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1004\" data-rawheight=\"125\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1004\" data-original=\"https://pic4.zhimg.com/v2-a42f25595b4a99878315cfa6555ef627_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-a42f25595b4a99878315cfa6555ef627_b.jpg\"/></figure><p>这样就得到所要的结果f(x,y)</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-cd450199842a28f2ea9c45b3e0ddb50c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2017\" data-rawheight=\"146\" class=\"origin_image zh-lightbox-thumb\" width=\"2017\" data-original=\"https://pic1.zhimg.com/v2-cd450199842a28f2ea9c45b3e0ddb50c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2017&#39; height=&#39;146&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2017\" data-rawheight=\"146\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2017\" data-original=\"https://pic1.zhimg.com/v2-cd450199842a28f2ea9c45b3e0ddb50c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-cd450199842a28f2ea9c45b3e0ddb50c_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-123449d283c9600f2949c69878071590_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1958\" data-rawheight=\"146\" class=\"origin_image zh-lightbox-thumb\" width=\"1958\" data-original=\"https://pic1.zhimg.com/v2-123449d283c9600f2949c69878071590_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1958&#39; height=&#39;146&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1958\" data-rawheight=\"146\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1958\" data-original=\"https://pic1.zhimg.com/v2-123449d283c9600f2949c69878071590_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-123449d283c9600f2949c69878071590_b.jpg\"/></figure><p>如果选择一个坐标系统使得的四个已知点坐标分别为 (0, 0)、(0, 1)、(1, 0)和 (1, 1)，那么插值公式就可以化简为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-880bbdb2dd5405559a6d899e71387a9e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2017\" data-rawheight=\"67\" class=\"origin_image zh-lightbox-thumb\" width=\"2017\" data-original=\"https://pic3.zhimg.com/v2-880bbdb2dd5405559a6d899e71387a9e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2017&#39; height=&#39;67&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2017\" data-rawheight=\"67\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2017\" data-original=\"https://pic3.zhimg.com/v2-880bbdb2dd5405559a6d899e71387a9e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-880bbdb2dd5405559a6d899e71387a9e_b.jpg\"/></figure><p>如果选择一个坐标系统使得f的四个已知点坐标分别为 (0, 0)、(0, 1)、(1, 0)和 (1, 1)，那么插值公式就可以化简为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-880bbdb2dd5405559a6d899e71387a9e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2017\" data-rawheight=\"67\" class=\"origin_image zh-lightbox-thumb\" width=\"2017\" data-original=\"https://pic3.zhimg.com/v2-880bbdb2dd5405559a6d899e71387a9e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2017&#39; height=&#39;67&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2017\" data-rawheight=\"67\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2017\" data-original=\"https://pic3.zhimg.com/v2-880bbdb2dd5405559a6d899e71387a9e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-880bbdb2dd5405559a6d899e71387a9e_b.jpg\"/></figure><p>或者用矩阵运算表示为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-c56cd5bef9a055738d6c6bc4139211b1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1192\" data-rawheight=\"150\" class=\"origin_image zh-lightbox-thumb\" width=\"1192\" data-original=\"https://pic2.zhimg.com/v2-c56cd5bef9a055738d6c6bc4139211b1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1192&#39; height=&#39;150&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1192\" data-rawheight=\"150\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1192\" data-original=\"https://pic2.zhimg.com/v2-c56cd5bef9a055738d6c6bc4139211b1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-c56cd5bef9a055738d6c6bc4139211b1_b.jpg\"/></figure><p>这种插值方法的结果通常不是线性的，线性插值的结果与插值的顺序无关。首先进行 y 方向的插值，然后进行 x 方向的插值，所得到的结果是一样的。</p><p></p><p></p>", 
            "topic": [
                {
                    "tag": "科技", 
                    "tagLink": "https://api.zhihu.com/topics/19556664"
                }, 
                {
                    "tag": "数学", 
                    "tagLink": "https://api.zhihu.com/topics/19554091"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/38572686", 
            "userName": "胡今朝", 
            "userLink": "https://www.zhihu.com/people/fb128b56fdfed86d1a2c8237571d133f", 
            "upvote": 2, 
            "title": "滑动平均", 
            "content": "<h2><b>概念的引出</b></h2><p>普通计算平均值的方法是将所有项加和再除以项数：（a1+a2+...+an）➗n；</p><p>但是在某些情形中，如果股票中，越是远离当前的之前股价对预测以后的股价越没有参考价值，比如说100天以前的股价就没有三天前的股价更有参考价值；</p><p>这样的话，我们就不去取所有的交易日的股价来计算平均值，而是取距今某个特定的时间段内的股价平均值作为参考，比如5日均值---&gt;对应到股票术语中的5日均线，10日均值---&gt;对应到股票术语中的10日均线；</p><h2><b>普通滑动平均计算示例</b></h2><p>以3年滑动平均值举例：</p><p>有1、2、3、4、5共5个数，计算过程为：</p><p>（1+2+3）/3=2</p><p>（2+3+4）/3=3</p><p>（3+4+5）/3=4</p><h2><b>加权滑动平均</b></h2><p>普通的滑动平均大致可以做到计算出相对准确的近期的平均值，但是这还不够，比如计算10日均值中十天前的影响就要小于五天前的影响，如何更精细地刻画每一天的值对均值的影响呢？---&gt;加权滑动平均：w1*At-1＋w2*At-2＋w3*At-3＋…＋wn*At-n；wn为经验系数</p><h2><b>指数滑动平均</b></h2><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-fdd2120633587e3aa98279a053b5be80_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"297\" data-rawheight=\"62\" class=\"content_image\" width=\"297\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;297&#39; height=&#39;62&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"297\" data-rawheight=\"62\" class=\"content_image lazy\" width=\"297\" data-actualsrc=\"https://pic1.zhimg.com/v2-fdd2120633587e3aa98279a053b5be80_b.jpg\"/></figure><p>shadow_variable = decay * shadow_variable + (1 - decay) * variable</p><p>注意decay = 1-⍺</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-bc4d9ffdeec96504285927311a38fa67_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"372\" data-rawheight=\"122\" class=\"content_image\" width=\"372\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;372&#39; height=&#39;122&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"372\" data-rawheight=\"122\" class=\"content_image lazy\" width=\"372\" data-actualsrc=\"https://pic4.zhimg.com/v2-bc4d9ffdeec96504285927311a38fa67_b.jpg\"/></figure><p>decay是变动的：</p><p>min(decay, (1 + num_updates) / (10 + num_updates))</p><h2><b>代码示例</b></h2><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">import</span> <span class=\"nn\">tensorflow</span> <span class=\"kn\">as</span> <span class=\"nn\">tf</span>\n\n<span class=\"n\">v1</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">Variable</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>   <span class=\"c1\"># 定义一个变量，初始值为0</span>\n<span class=\"n\">step</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">Variable</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">trainable</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">)</span>  <span class=\"c1\"># step为迭代轮数变量，控制衰减率</span>\n\n<span class=\"n\">ema</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"o\">.</span><span class=\"n\">ExponentialMovingAverage</span><span class=\"p\">(</span><span class=\"mf\">0.99</span><span class=\"p\">,</span><span class=\"n\">step</span><span class=\"p\">)</span>  <span class=\"c1\"># 初始设定衰减率为0.99</span>\n<span class=\"n\">maintain_averages_op</span> <span class=\"o\">=</span> <span class=\"n\">ema</span><span class=\"o\">.</span><span class=\"nb\">apply</span><span class=\"p\">([</span><span class=\"n\">v1</span><span class=\"p\">])</span>                 <span class=\"c1\"># 更新列表中的变量</span>\n<span class=\"k\">with</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">Session</span><span class=\"p\">()</span> <span class=\"k\">as</span> <span class=\"n\">sess</span><span class=\"p\">:</span>\n    <span class=\"n\">init_op</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">global_variables_initializer</span><span class=\"p\">()</span>        <span class=\"c1\"># 初始化所有变量</span>\n<span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">init_op</span><span class=\"p\">)</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">([</span><span class=\"n\">v1</span><span class=\"p\">,</span> <span class=\"n\">ema</span><span class=\"o\">.</span><span class=\"n\">average</span><span class=\"p\">(</span><span class=\"n\">v1</span><span class=\"p\">)]))</span>                <span class=\"c1\"># 输出初始化后变量v1的值和v1的滑动平均值</span>\n<span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">assign</span><span class=\"p\">(</span><span class=\"n\">v1</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">))</span>                            <span class=\"c1\"># 更新v1的值</span>\n<span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">maintain_averages_op</span><span class=\"p\">)</span>                        <span class=\"c1\"># 更新v1的滑动平均值</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">([</span><span class=\"n\">v1</span><span class=\"p\">,</span> <span class=\"n\">ema</span><span class=\"o\">.</span><span class=\"n\">average</span><span class=\"p\">(</span><span class=\"n\">v1</span><span class=\"p\">)]))</span>\n<span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">assign</span><span class=\"p\">(</span><span class=\"n\">step</span><span class=\"p\">,</span> <span class=\"mi\">10000</span><span class=\"p\">))</span>                      <span class=\"c1\"># 更新迭代轮转数step</span>\n<span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">assign</span><span class=\"p\">(</span><span class=\"n\">v1</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">))</span>\n<span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">maintain_averages_op</span><span class=\"p\">)</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">([</span><span class=\"n\">v1</span><span class=\"p\">,</span> <span class=\"n\">ema</span><span class=\"o\">.</span><span class=\"n\">average</span><span class=\"p\">(</span><span class=\"n\">v1</span><span class=\"p\">)]))</span>\n                                                      <span class=\"c1\"># 再次更新滑动平均值，</span>\n<span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">maintain_averages_op</span><span class=\"p\">)</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">([</span><span class=\"n\">v1</span><span class=\"p\">,</span> <span class=\"n\">ema</span><span class=\"o\">.</span><span class=\"n\">average</span><span class=\"p\">(</span><span class=\"n\">v1</span><span class=\"p\">)]))</span>\n                                                      <span class=\"c1\"># 更新v1的值为15</span>\n<span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">assign</span><span class=\"p\">(</span><span class=\"n\">v1</span><span class=\"p\">,</span> <span class=\"mi\">15</span><span class=\"p\">))</span>\n\n<span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">maintain_averages_op</span><span class=\"p\">)</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">([</span><span class=\"n\">v1</span><span class=\"p\">,</span> <span class=\"n\">ema</span><span class=\"o\">.</span><span class=\"n\">average</span><span class=\"p\">(</span><span class=\"n\">v1</span><span class=\"p\">)]))</span></code></pre></div><p></p>", 
            "topic": [
                {
                    "tag": "数学", 
                    "tagLink": "https://api.zhihu.com/topics/19554091"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/34523003", 
            "userName": "胡今朝", 
            "userLink": "https://www.zhihu.com/people/fb128b56fdfed86d1a2c8237571d133f", 
            "upvote": 0, 
            "title": "LSTM", 
            "content": "<h2>公式</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-0a59a171dc7a7886b0eb3f7485fb53a4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1190\" data-rawheight=\"658\" class=\"origin_image zh-lightbox-thumb\" width=\"1190\" data-original=\"https://pic1.zhimg.com/v2-0a59a171dc7a7886b0eb3f7485fb53a4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1190&#39; height=&#39;658&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1190\" data-rawheight=\"658\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1190\" data-original=\"https://pic1.zhimg.com/v2-0a59a171dc7a7886b0eb3f7485fb53a4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-0a59a171dc7a7886b0eb3f7485fb53a4_b.jpg\"/></figure><h2>图解</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-6abd8c4b040b3b709c9094199f4707b9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1180\" data-rawheight=\"784\" class=\"origin_image zh-lightbox-thumb\" width=\"1180\" data-original=\"https://pic2.zhimg.com/v2-6abd8c4b040b3b709c9094199f4707b9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1180&#39; height=&#39;784&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1180\" data-rawheight=\"784\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1180\" data-original=\"https://pic2.zhimg.com/v2-6abd8c4b040b3b709c9094199f4707b9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-6abd8c4b040b3b709c9094199f4707b9_b.jpg\"/></figure><h2>说明</h2><p>何为一个LSTM的cell？下面公式即是对cell的表达：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-d0f94ac8521644f7b7c13ec0373d449f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"312\" data-rawheight=\"50\" class=\"content_image\" width=\"312\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;312&#39; height=&#39;50&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"312\" data-rawheight=\"50\" class=\"content_image lazy\" width=\"312\" data-actualsrc=\"https://pic4.zhimg.com/v2-d0f94ac8521644f7b7c13ec0373d449f_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "LSTM", 
                    "tagLink": "https://api.zhihu.com/topics/20023220"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/34504680", 
            "userName": "胡今朝", 
            "userLink": "https://www.zhihu.com/people/fb128b56fdfed86d1a2c8237571d133f", 
            "upvote": 1, 
            "title": "反向传播算法", 
            "content": "<h2>Reference</h2><a href=\"https://link.zhihu.com/?target=https%3A//www.bilibili.com/video/av19161412/%3Ffrom%3Dsearch%26seid%3D1639340351066839582\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic2.zhimg.com/v2-9a9bd1ee237ce74f0cc33c890902f7e1_180x120.jpg\" data-image-width=\"1728\" data-image-height=\"1080\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">CS231n反向传播</a><ul><li>01:45   beginning time</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>计算梯度（偏导数）的方法：数值方法，解析方法</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>04:00   梯度下降法计算梯度</li></ul><h2>例一</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-de2ab26fc1e05f58e0f1b69f971b61ed_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"931\" data-rawheight=\"550\" class=\"origin_image zh-lightbox-thumb\" width=\"931\" data-original=\"https://pic2.zhimg.com/v2-de2ab26fc1e05f58e0f1b69f971b61ed_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;931&#39; height=&#39;550&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"931\" data-rawheight=\"550\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"931\" data-original=\"https://pic2.zhimg.com/v2-de2ab26fc1e05f58e0f1b69f971b61ed_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-de2ab26fc1e05f58e0f1b69f971b61ed_b.jpg\"/></figure><h2>例二</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-7b17b394499a22b26101e52dd64d9115_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"690\" data-rawheight=\"377\" class=\"origin_image zh-lightbox-thumb\" width=\"690\" data-original=\"https://pic2.zhimg.com/v2-7b17b394499a22b26101e52dd64d9115_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;690&#39; height=&#39;377&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"690\" data-rawheight=\"377\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"690\" data-original=\"https://pic2.zhimg.com/v2-7b17b394499a22b26101e52dd64d9115_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-7b17b394499a22b26101e52dd64d9115_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-514501def109d962676dc4016a2f0eb7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"875\" data-rawheight=\"416\" class=\"origin_image zh-lightbox-thumb\" width=\"875\" data-original=\"https://pic4.zhimg.com/v2-514501def109d962676dc4016a2f0eb7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;875&#39; height=&#39;416&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"875\" data-rawheight=\"416\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"875\" data-original=\"https://pic4.zhimg.com/v2-514501def109d962676dc4016a2f0eb7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-514501def109d962676dc4016a2f0eb7_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-b506944ca956ee62ceb3e7309dff44fb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"930\" data-rawheight=\"556\" class=\"origin_image zh-lightbox-thumb\" width=\"930\" data-original=\"https://pic4.zhimg.com/v2-b506944ca956ee62ceb3e7309dff44fb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;930&#39; height=&#39;556&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"930\" data-rawheight=\"556\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"930\" data-original=\"https://pic4.zhimg.com/v2-b506944ca956ee62ceb3e7309dff44fb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-b506944ca956ee62ceb3e7309dff44fb_b.jpg\"/></figure><h2>理解</h2><p>反向传播的本质是用一种更加便捷的方式计算Loss函数的梯度，这种方式就是链式法则，这会带来很多的好处；第一，简化了直接求导的复杂度，第二，当有新的数据加入的时候整个Loss函数对某个参数的偏导数只用按照链式法则增加一项即可，不用完全推倒重来，前向传播的整个Loss函数完全变了参数的偏导数需要重新计算，第三，每一层的参数从后往前逐层更新</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-2a754a6b8e4445e2c2c4ff9f8d698448_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"963\" data-rawheight=\"275\" class=\"origin_image zh-lightbox-thumb\" width=\"963\" data-original=\"https://pic1.zhimg.com/v2-2a754a6b8e4445e2c2c4ff9f8d698448_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;963&#39; height=&#39;275&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"963\" data-rawheight=\"275\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"963\" data-original=\"https://pic1.zhimg.com/v2-2a754a6b8e4445e2c2c4ff9f8d698448_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-2a754a6b8e4445e2c2c4ff9f8d698448_b.jpg\"/></figure><p>反向传播传播所做的一切工作最终都是服务于梯度下降算法的梯度计算：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-a47c4d2d51111330ba0cf63db365a041_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"321\" data-rawheight=\"87\" class=\"content_image\" width=\"321\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;321&#39; height=&#39;87&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"321\" data-rawheight=\"87\" class=\"content_image lazy\" width=\"321\" data-actualsrc=\"https://pic2.zhimg.com/v2-a47c4d2d51111330ba0cf63db365a041_b.jpg\"/></figure><p>梯度下降是为找到Loss的最小值</p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/34361537", 
            "userName": "胡今朝", 
            "userLink": "https://www.zhihu.com/people/fb128b56fdfed86d1a2c8237571d133f", 
            "upvote": 2, 
            "title": "梯度下降法", 
            "content": "<h2>牛顿法</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-af0f87154ec21228546fe8144f479fda_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"984\" data-rawheight=\"522\" class=\"origin_image zh-lightbox-thumb\" width=\"984\" data-original=\"https://pic3.zhimg.com/v2-af0f87154ec21228546fe8144f479fda_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;984&#39; height=&#39;522&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"984\" data-rawheight=\"522\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"984\" data-original=\"https://pic3.zhimg.com/v2-af0f87154ec21228546fe8144f479fda_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-af0f87154ec21228546fe8144f479fda_b.jpg\"/></figure><p>牛顿迭代法是用来求非线性方程近似解的（不是求最小值），是把非线性方程f(x)=0 线性化的一种近似方法，把f(x)在点x0的某邻域内展开成泰勒级数：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-03955a6c152afacc759ddec41c7d5345_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"559\" data-rawheight=\"40\" class=\"origin_image zh-lightbox-thumb\" width=\"559\" data-original=\"https://pic2.zhimg.com/v2-03955a6c152afacc759ddec41c7d5345_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;559&#39; height=&#39;40&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"559\" data-rawheight=\"40\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"559\" data-original=\"https://pic2.zhimg.com/v2-03955a6c152afacc759ddec41c7d5345_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-03955a6c152afacc759ddec41c7d5345_b.jpg\"/></figure><p>取其线性部分（即泰勒展开式的前两项），并令其等于0，即：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-3fa45fa99d6e9e3cf05b69f3b7fcca50_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"181\" data-rawheight=\"18\" class=\"content_image\" width=\"181\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;181&#39; height=&#39;18&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"181\" data-rawheight=\"18\" class=\"content_image lazy\" width=\"181\" data-actualsrc=\"https://pic1.zhimg.com/v2-3fa45fa99d6e9e3cf05b69f3b7fcca50_b.jpg\"/></figure><p>，以此作为非线性方程f(x)=0的近似方程，若：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-fac3c124c0e7bebf32ddda28eb92eb30_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"71\" data-rawheight=\"18\" class=\"content_image\" width=\"71\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;71&#39; height=&#39;18&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"71\" data-rawheight=\"18\" class=\"content_image lazy\" width=\"71\" data-actualsrc=\"https://pic1.zhimg.com/v2-fac3c124c0e7bebf32ddda28eb92eb30_b.jpg\"/></figure><p>则其解为：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-f438a7c09ef88f529a0a9de65dc5c818_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"113\" data-rawheight=\"40\" class=\"content_image\" width=\"113\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;113&#39; height=&#39;40&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"113\" data-rawheight=\"40\" class=\"content_image lazy\" width=\"113\" data-actualsrc=\"https://pic1.zhimg.com/v2-f438a7c09ef88f529a0a9de65dc5c818_b.jpg\"/></figure><p>这样，得到牛顿迭代法的一个迭代关系式：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-4ba20a4f5876a9c9b66cde46e917382f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"129\" data-rawheight=\"40\" class=\"content_image\" width=\"129\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;129&#39; height=&#39;40&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"129\" data-rawheight=\"40\" class=\"content_image lazy\" width=\"129\" data-actualsrc=\"https://pic4.zhimg.com/v2-4ba20a4f5876a9c9b66cde46e917382f_b.jpg\"/></figure><p>牛顿迭代法与梯度下降法没有实质的联系，但是可以通过以上直观的迭代过程理解一下迭代</p><h2>梯度下降法</h2><p><b>一、视频教程</b></p><a class=\"video-box\" href=\"https://link.zhihu.com/?target=https%3A//www.zhihu.com/video/955214011583741952\" target=\"_blank\" data-video-id=\"\" data-video-playable=\"true\" data-name=\"\" data-poster=\"https://pic3.zhimg.com/80/v2-46a745dcf350e8477afef7a5b357b96a_b.jpg\" data-lens-id=\"955214011583741952\"><img class=\"thumbnail\" src=\"https://pic3.zhimg.com/80/v2-46a745dcf350e8477afef7a5b357b96a_b.jpg\"/><span class=\"content\"><span class=\"title\"><span class=\"z-ico-extern-gray\"></span><span class=\"z-ico-extern-blue\"></span></span><span class=\"url\"><span class=\"z-ico-video\"></span>https://www.zhihu.com/video/955214011583741952</span></span></a><p><b>二、求导求最小值</b></p><p><b>函数f(x)=x*x-2x+1的导数为f(x)=2x-2，其图像如下图所示：</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-e4cabb367119114f6560fba5ddde5c50_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"447\" data-rawheight=\"360\" class=\"origin_image zh-lightbox-thumb\" width=\"447\" data-original=\"https://pic1.zhimg.com/v2-e4cabb367119114f6560fba5ddde5c50_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;447&#39; height=&#39;360&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"447\" data-rawheight=\"360\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"447\" data-original=\"https://pic1.zhimg.com/v2-e4cabb367119114f6560fba5ddde5c50_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-e4cabb367119114f6560fba5ddde5c50_b.jpg\"/></figure><p>切线1---&gt;切线2---&gt;切线3，切线的斜率越来越小，最终切线的斜率为零，函数f(x)在x=1处取得最小值，在这个过程中点x1, x2, x3不断左移，直到点x3处，那么显然我们需要一种迭代方法使得点x能不断靠近斜率为零的点，可以用下面的迭代公式：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-e0ec75b8525c487868a23beed592e1d8_b.jpg\" data-size=\"normal\" data-rawwidth=\"1496\" data-rawheight=\"131\" class=\"origin_image zh-lightbox-thumb\" width=\"1496\" data-original=\"https://pic1.zhimg.com/v2-e0ec75b8525c487868a23beed592e1d8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1496&#39; height=&#39;131&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1496\" data-rawheight=\"131\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1496\" data-original=\"https://pic1.zhimg.com/v2-e0ec75b8525c487868a23beed592e1d8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-e0ec75b8525c487868a23beed592e1d8_b.jpg\"/><figcaption>梯度下降法迭代公式</figcaption></figure><p>如上图点xn在曲线对称轴右侧时曲线斜率为正值，即f&#39;(x)&gt;0，学习率也大于零，所以xn+1相对于点xn往左移，在这个过程中曲线的斜率逐步减小直至趋近于零，xn+1逐步趋近于xn从而逼近最低点，在这过程中算法收敛的前提是学习率是一个比较小的数值，迭代的步长会越来越小最终收敛，至于学习率设置为多少并没有统一的定论，这就是所谓的调参（炼丹）过程</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-5bb419a06edaf873e6f534bd483656df_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"691\" data-rawheight=\"229\" class=\"origin_image zh-lightbox-thumb\" width=\"691\" data-original=\"https://pic4.zhimg.com/v2-5bb419a06edaf873e6f534bd483656df_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;691&#39; height=&#39;229&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"691\" data-rawheight=\"229\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"691\" data-original=\"https://pic4.zhimg.com/v2-5bb419a06edaf873e6f534bd483656df_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-5bb419a06edaf873e6f534bd483656df_b.jpg\"/></figure><p>代码实现：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">import</span> <span class=\"nn\">matplotlib.pyplot</span> <span class=\"kn\">as</span> <span class=\"nn\">plt</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"kn\">as</span> <span class=\"nn\">np</span>\n\n<span class=\"c1\">#构造函数f(x)</span>\n<span class=\"k\">def</span> <span class=\"nf\">fun</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">):</span>\n    <span class=\"k\">return</span> <span class=\"p\">(</span><span class=\"n\">x</span><span class=\"o\">**</span><span class=\"mi\">2</span><span class=\"p\">)</span><span class=\"o\">-</span><span class=\"mi\">2</span><span class=\"o\">*</span><span class=\"n\">x</span><span class=\"o\">+</span><span class=\"mi\">1</span>\n\n\n<span class=\"c1\">#画出f(x)图像</span>\n<span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linspace</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">)</span>\n<span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">fun</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"s1\">&#39;b&#39;</span><span class=\"p\">,</span> <span class=\"n\">linewidth</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>\n\n<span class=\"c1\">#递归公式：x(n+1) = x(n) - learning_rate*(2*x(n) - 2)</span>\n<span class=\"n\">learning_rate</span> <span class=\"o\">=</span> <span class=\"mf\">0.1</span>\n<span class=\"k\">def</span> <span class=\"nf\">x</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"p\">):</span>\n    <span class=\"k\">if</span> <span class=\"n\">n</span> <span class=\"o\">==</span><span class=\"mi\">1</span><span class=\"p\">:</span>\n        <span class=\"k\">return</span> <span class=\"mi\">3</span>\n    <span class=\"k\">else</span><span class=\"p\">:</span> \n        <span class=\"k\">return</span> <span class=\"n\">x</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"n\">learning_rate</span><span class=\"o\">*</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"o\">*</span><span class=\"n\">x</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"mi\">2</span><span class=\"p\">)</span>\n\n<span class=\"c1\">#打印结果</span>\n<span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">25</span><span class=\"p\">):</span>\n    <span class=\"n\">x_i</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">)</span>\n    <span class=\"n\">res</span> <span class=\"o\">=</span> <span class=\"n\">fun</span><span class=\"p\">(</span><span class=\"n\">x_i</span><span class=\"p\">)</span>\n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">x_i</span><span class=\"p\">,</span> <span class=\"n\">res</span><span class=\"p\">)</span>\n\n<span class=\"s1\">&#39;&#39;&#39;\n</span><span class=\"s1\">Question:如何处理递归的开销，当将迭代次数设置为30的时候计算就跑不动了\n</span><span class=\"s1\">&#39;&#39;&#39;</span> </code></pre></div><h2>多变量函数的梯度下降法</h2><p>梯度方向：偏导数组成的向量的方向</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-e0d6ab6c37a1d61ed87fccb2064619d9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"391\" data-rawheight=\"70\" class=\"content_image\" width=\"391\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;391&#39; height=&#39;70&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"391\" data-rawheight=\"70\" class=\"content_image lazy\" width=\"391\" data-actualsrc=\"https://pic2.zhimg.com/v2-e0d6ab6c37a1d61ed87fccb2064619d9_b.jpg\"/></figure><p>性质：函数沿梯度反方向下降最快，如下图所示：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-bae05b325592d42690b07a07d19c297a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"660\" data-rawheight=\"323\" class=\"origin_image zh-lightbox-thumb\" width=\"660\" data-original=\"https://pic3.zhimg.com/v2-bae05b325592d42690b07a07d19c297a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;660&#39; height=&#39;323&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"660\" data-rawheight=\"323\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"660\" data-original=\"https://pic3.zhimg.com/v2-bae05b325592d42690b07a07d19c297a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-bae05b325592d42690b07a07d19c297a_b.jpg\"/></figure><p>迭代公式：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-0895aea81222b4bfe0f9af9fc0da7af9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"523\" data-rawheight=\"138\" class=\"origin_image zh-lightbox-thumb\" width=\"523\" data-original=\"https://pic2.zhimg.com/v2-0895aea81222b4bfe0f9af9fc0da7af9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;523&#39; height=&#39;138&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"523\" data-rawheight=\"138\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"523\" data-original=\"https://pic2.zhimg.com/v2-0895aea81222b4bfe0f9af9fc0da7af9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-0895aea81222b4bfe0f9af9fc0da7af9_b.jpg\"/></figure><p>代码实现：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"kn\">as</span> <span class=\"nn\">np</span>\n<span class=\"kn\">import</span> <span class=\"nn\">matplotlib.pyplot</span> <span class=\"kn\">as</span> <span class=\"nn\">plt</span>\n<span class=\"kn\">from</span> <span class=\"nn\">mpl_toolkits.mplot3d</span> <span class=\"kn\">import</span> <span class=\"n\">Axes3D</span>\n\n<span class=\"c1\">#求函数f(x, y) = (x-2)**2 + (y+3)**2 + 3的最小值</span>\n<span class=\"k\">def</span> <span class=\"nf\">func</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">):</span>\n    <span class=\"k\">return</span> <span class=\"p\">(</span><span class=\"n\">x</span><span class=\"o\">-</span><span class=\"mi\">2</span><span class=\"p\">)</span><span class=\"o\">**</span><span class=\"mi\">2</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">y</span><span class=\"o\">+</span><span class=\"mi\">3</span><span class=\"p\">)</span><span class=\"o\">**</span><span class=\"mi\">2</span> <span class=\"o\">+</span> <span class=\"mi\">3</span>\n\n\n<span class=\"c1\">#画图</span>\n<span class=\"n\">fig</span> <span class=\"o\">=</span> <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">figure</span><span class=\"p\">()</span>\n<span class=\"n\">ax</span> <span class=\"o\">=</span> <span class=\"n\">Axes3D</span><span class=\"p\">(</span><span class=\"n\">fig</span><span class=\"p\">)</span>\n<span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mf\">0.25</span><span class=\"p\">)</span>\n<span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mf\">0.25</span><span class=\"p\">)</span>\n<span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">meshgrid</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span>\n<span class=\"n\">z</span> <span class=\"o\">=</span> <span class=\"n\">func</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">plot_surface</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">z</span><span class=\"p\">,</span> <span class=\"n\">rstride</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">cstride</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">cmap</span><span class=\"o\">=</span><span class=\"s1\">&#39;rainbow&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>\n\n\n<span class=\"c1\">#设置自变量的迭代公式</span>\n<span class=\"n\">alpha</span> <span class=\"o\">=</span> <span class=\"mf\">0.05</span><span class=\"c1\">#这里的学习率调整到0.5可以直接得到最优解</span>\n<span class=\"c1\">#这里的递归算法计算量呈指数级增长，怎么处理？</span>\n<span class=\"k\">def</span> <span class=\"nf\">x</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"p\">):</span>\n    <span class=\"k\">if</span> <span class=\"n\">n</span><span class=\"o\">==</span><span class=\"mi\">1</span><span class=\"p\">:</span>\n        <span class=\"k\">return</span> <span class=\"mi\">5</span>\n    <span class=\"k\">else</span><span class=\"p\">:</span>\n        <span class=\"k\">return</span> <span class=\"n\">x</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"n\">alpha</span><span class=\"o\">*</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"o\">*</span><span class=\"n\">x</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">-</span><span class=\"mi\">4</span><span class=\"p\">)</span>\n<span class=\"k\">def</span> <span class=\"nf\">y</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"p\">):</span>\n    <span class=\"k\">if</span> <span class=\"n\">n</span><span class=\"o\">==</span><span class=\"mi\">1</span><span class=\"p\">:</span>\n        <span class=\"k\">return</span> <span class=\"mi\">5</span>\n    <span class=\"k\">else</span><span class=\"p\">:</span>\n        <span class=\"k\">return</span> <span class=\"n\">y</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"n\">alpha</span><span class=\"o\">*</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"o\">*</span><span class=\"n\">y</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">)</span><span class=\"o\">+</span><span class=\"mi\">6</span><span class=\"p\">)</span>\n\n<span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">30</span><span class=\"p\">):</span><span class=\"c1\">#range里面要把0排除掉，n的取值是从1开始的</span>\n    <span class=\"n\">x_i</span> <span class=\"o\">=</span> <span class=\"n\">x</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">)</span>\n    <span class=\"n\">y_i</span> <span class=\"o\">=</span> <span class=\"n\">y</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">)</span>\n    <span class=\"n\">res</span> <span class=\"o\">=</span> <span class=\"n\">func</span><span class=\"p\">(</span><span class=\"n\">x_i</span><span class=\"p\">,</span> <span class=\"n\">y_i</span><span class=\"p\">)</span>\n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">x_i</span><span class=\"p\">,</span> <span class=\"n\">y_i</span><span class=\"p\">,</span> <span class=\"n\">res</span><span class=\"p\">)</span></code></pre></div><h2>批量梯度下降Batch Gradient Descent</h2><p>上面的梯度下降是求某一个一元函数或者多元函数的极值问题，接下来要讲的是对某个损失函数求极值的问题损失函数的项数与训练集的大小相关，以线性回归为例进行说明：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-41a65faf2609c4b9a6a2351b53addf02_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"930\" data-rawheight=\"1005\" class=\"origin_image zh-lightbox-thumb\" width=\"930\" data-original=\"https://pic3.zhimg.com/v2-41a65faf2609c4b9a6a2351b53addf02_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;930&#39; height=&#39;1005&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"930\" data-rawheight=\"1005\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"930\" data-original=\"https://pic3.zhimg.com/v2-41a65faf2609c4b9a6a2351b53addf02_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-41a65faf2609c4b9a6a2351b53addf02_b.jpg\"/></figure><p>将每一步的参数值带入损失函数，观察结果是否收敛</p><p>代码实现（直接用tensorflow的API实现）：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># -*- coding: utf-8 -*-</span>\n<span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">Created on Fri Mar  9 19:59:32 2018\n</span><span class=\"s2\">\n</span><span class=\"s2\">@author: Andy Hu\n</span><span class=\"s2\">&#34;&#34;&#34;</span>\n<span class=\"c1\">#生成数据</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"kn\">as</span> <span class=\"nn\">np</span>\n<span class=\"kn\">import</span> <span class=\"nn\">tensorflow</span> <span class=\"kn\">as</span> <span class=\"nn\">tf</span>\n<span class=\"n\">x_1</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">astype</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n<span class=\"n\">x_2</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">astype</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n<span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"mf\">0.1</span> <span class=\"o\">+</span> <span class=\"mf\">0.2</span><span class=\"o\">*</span><span class=\"n\">x_1</span> <span class=\"o\">+</span> <span class=\"mf\">0.3</span><span class=\"o\">*</span><span class=\"n\">x_2</span>\n\n<span class=\"c1\">#定义损失函数</span>\n<span class=\"n\">Weights_1</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">Variable</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">random_uniform</span><span class=\"p\">([</span><span class=\"mi\">1</span><span class=\"p\">],</span><span class=\"o\">-</span><span class=\"mf\">1.0</span><span class=\"p\">,</span> <span class=\"mf\">1.0</span><span class=\"p\">))</span>\n<span class=\"n\">Weights_2</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">Variable</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">random_uniform</span><span class=\"p\">([</span><span class=\"mi\">1</span><span class=\"p\">],</span><span class=\"o\">-</span><span class=\"mf\">1.0</span><span class=\"p\">,</span> <span class=\"mf\">1.0</span><span class=\"p\">))</span>\n<span class=\"n\">biases</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">Variable</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">([</span><span class=\"mi\">1</span><span class=\"p\">]))</span>\n\n<span class=\"n\">h</span> <span class=\"o\">=</span> <span class=\"n\">Weights_1</span><span class=\"o\">*</span><span class=\"n\">x_1</span> <span class=\"o\">+</span> <span class=\"n\">Weights_2</span><span class=\"o\">*</span><span class=\"n\">x_2</span> <span class=\"o\">+</span> <span class=\"n\">biases</span>\n\n<span class=\"n\">Loss</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">reduce_mean</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">square</span><span class=\"p\">(</span><span class=\"n\">h</span> <span class=\"o\">-</span> <span class=\"n\">y</span><span class=\"p\">))</span>\n<span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"o\">.</span><span class=\"n\">GradientDescentOptimizer</span><span class=\"p\">(</span><span class=\"mf\">0.5</span><span class=\"p\">)</span>\n<span class=\"n\">train</span> <span class=\"o\">=</span> <span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">minimize</span><span class=\"p\">(</span><span class=\"n\">Loss</span><span class=\"p\">)</span>\n<span class=\"n\">init</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">global_variables_initializer</span><span class=\"p\">()</span>\n<span class=\"n\">sess</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">Session</span><span class=\"p\">()</span>\n<span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">init</span><span class=\"p\">)</span>\n<span class=\"k\">for</span> <span class=\"n\">step</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">201</span><span class=\"p\">):</span>\n    <span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">train</span><span class=\"p\">)</span>\n    <span class=\"k\">if</span> <span class=\"n\">step</span><span class=\"o\">%</span><span class=\"mi\">20</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n        <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">step</span><span class=\"p\">,</span><span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">biases</span><span class=\"p\">),</span> <span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">Weights_1</span><span class=\"p\">),</span> <span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">Weights_2</span><span class=\"p\">))</span>\n<span class=\"s1\">&#39;&#39;&#39;\n</span><span class=\"s1\">0 [0.45400265] [-0.14213887] [0.46785587]\n</span><span class=\"s1\">20 [0.15688866] [0.10223585] [0.29202002]\n</span><span class=\"s1\">40 [0.11751703] [0.17623536] [0.29121837]\n</span><span class=\"s1\">60 [0.10547507] [0.19380628] [0.2960241]\n</span><span class=\"s1\">80 [0.10172707] [0.1982829] [0.29850975]\n</span><span class=\"s1\">100 [0.10054782] [0.19950032] [0.29948243]\n</span><span class=\"s1\">120 [0.10017434] [0.1998495] [0.29982683]\n</span><span class=\"s1\">140 [0.1000556] [0.19995362] [0.29994318]\n</span><span class=\"s1\">160 [0.10001776] [0.19998552] [0.29998153]\n</span><span class=\"s1\">180 [0.10000567] [0.19999544] [0.29999405]\n</span><span class=\"s1\">200 [0.10000182] [0.19999854] [0.2999981]</span></code></pre></div><h2>随机梯度下降法</h2><p>略</p><h2>FUTURE WORK</h2><p>Momentum</p><p>Adamgrad</p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/34286125", 
            "userName": "胡今朝", 
            "userLink": "https://www.zhihu.com/people/fb128b56fdfed86d1a2c8237571d133f", 
            "upvote": 0, 
            "title": "RNN", 
            "content": "<h2>零、参考资料</h2><a href=\"https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Recurrent_neural_network\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic2.zhimg.com/v2-e2f9e0969518c2896e07d8f123ce3981_180x120.jpg\" data-image-width=\"1200\" data-image-height=\"545\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Recurrent neural network</a><hr/><h2>一、直观理解RNN </h2><a href=\"https://link.zhihu.com/?target=https%3A//www.bilibili.com/video/av13565327/%3Ffrom%3Dsearch%26seid%3D16420320447874914426\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-b00f63b773a85746846e1035fe3dd172_180x120.jpg\" data-image-width=\"1280\" data-image-height=\"720\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">通俗理解RNN</a><p>传统的神经网络解决的问题是拟合一个静态的函数，比如要判断一张照片是狗还是猫，我们将一张图片的数据信息给到神经网络，神经网络给出一个判断结果，它的输出只与输入的图片有关系，示意如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-e404053f0919463916ccef610d3b8fb5_b.jpg\" data-size=\"normal\" data-rawwidth=\"1240\" data-rawheight=\"293\" class=\"origin_image zh-lightbox-thumb\" width=\"1240\" data-original=\"https://pic2.zhimg.com/v2-e404053f0919463916ccef610d3b8fb5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1240&#39; height=&#39;293&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1240\" data-rawheight=\"293\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1240\" data-original=\"https://pic2.zhimg.com/v2-e404053f0919463916ccef610d3b8fb5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-e404053f0919463916ccef610d3b8fb5_b.jpg\"/><figcaption>CNN</figcaption></figure><p>但是往往有一些问题，不仅与当前时刻的输入有关，还与上一时刻的输出有关，如股票市场中，上一交易日因为经理人发布了将要执行的新的经营理念大幅上涨（上一时刻的输出），且今天的有消息称经理人患上了绝症（当前时刻的输入），那么今天的股票的走势与昨天的上涨和今天的消息都有关，可能是跌掉昨天上涨的部分（新政无法实施），再跌掉一部分（经理人生病造成的动荡），这就引出了新的解决方案RNN，示意如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-e4db36bb71e536e9a398ba7ed4887b38_b.jpg\" data-size=\"normal\" data-rawwidth=\"953\" data-rawheight=\"531\" class=\"origin_image zh-lightbox-thumb\" width=\"953\" data-original=\"https://pic1.zhimg.com/v2-e4db36bb71e536e9a398ba7ed4887b38_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;953&#39; height=&#39;531&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"953\" data-rawheight=\"531\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"953\" data-original=\"https://pic1.zhimg.com/v2-e4db36bb71e536e9a398ba7ed4887b38_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-e4db36bb71e536e9a398ba7ed4887b38_b.jpg\"/><figcaption>RNN</figcaption></figure><hr/><h2>二、公式理解</h2><p>1、公式</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-a836c1d0e9cc993c097dd03dc8c4d361_b.jpg\" data-size=\"normal\" data-rawwidth=\"601\" data-rawheight=\"529\" class=\"origin_image zh-lightbox-thumb\" width=\"601\" data-original=\"https://pic2.zhimg.com/v2-a836c1d0e9cc993c097dd03dc8c4d361_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;601&#39; height=&#39;529&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"601\" data-rawheight=\"529\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"601\" data-original=\"https://pic2.zhimg.com/v2-a836c1d0e9cc993c097dd03dc8c4d361_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-a836c1d0e9cc993c097dd03dc8c4d361_b.jpg\"/><figcaption>两种简单RNN</figcaption></figure><p>两种RNN都属于简单RNN，两者的区别是一个隐层输出是用上一时刻隐层的输出，一个用上一时刻的输出，目前广泛使用的是前者</p><p class=\"ztext-empty-paragraph\"><br/></p><p>2、图解</p><p>单层感知机：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-41484e9d2a8d26aa6584083f69efeb72_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"410\" data-rawheight=\"554\" class=\"content_image\" width=\"410\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;410&#39; height=&#39;554&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"410\" data-rawheight=\"554\" class=\"content_image lazy\" width=\"410\" data-actualsrc=\"https://pic3.zhimg.com/v2-41484e9d2a8d26aa6584083f69efeb72_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>RNN：</p><p>总体示意图</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-f8cf940781dc92b9f6d9316ac9a874b8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1268\" data-rawheight=\"699\" class=\"origin_image zh-lightbox-thumb\" width=\"1268\" data-original=\"https://pic1.zhimg.com/v2-f8cf940781dc92b9f6d9316ac9a874b8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1268&#39; height=&#39;699&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1268\" data-rawheight=\"699\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1268\" data-original=\"https://pic1.zhimg.com/v2-f8cf940781dc92b9f6d9316ac9a874b8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-f8cf940781dc92b9f6d9316ac9a874b8_b.jpg\"/></figure><p>上一时刻隐层的输出ht-1和当前时刻的输入xt共同作为当前时刻隐层的输入</p><p class=\"ztext-empty-paragraph\"><br/></p><p>展开示意图</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-1bf893cf91ee408bbbd0a437e91386eb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1175\" data-rawheight=\"481\" class=\"origin_image zh-lightbox-thumb\" width=\"1175\" data-original=\"https://pic4.zhimg.com/v2-1bf893cf91ee408bbbd0a437e91386eb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1175&#39; height=&#39;481&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1175\" data-rawheight=\"481\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1175\" data-original=\"https://pic4.zhimg.com/v2-1bf893cf91ee408bbbd0a437e91386eb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-1bf893cf91ee408bbbd0a437e91386eb_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>解析示意图</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-18ec60acfe3a1cec33675ac1fcbd298d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1010\" data-rawheight=\"583\" class=\"origin_image zh-lightbox-thumb\" width=\"1010\" data-original=\"https://pic2.zhimg.com/v2-18ec60acfe3a1cec33675ac1fcbd298d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1010&#39; height=&#39;583&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1010\" data-rawheight=\"583\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1010\" data-original=\"https://pic2.zhimg.com/v2-18ec60acfe3a1cec33675ac1fcbd298d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-18ec60acfe3a1cec33675ac1fcbd298d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>其中激活函数为tanh</p><p>运算过程图</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-760f5b2e87d49a2d46871236067fa2cd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1221\" data-rawheight=\"623\" class=\"origin_image zh-lightbox-thumb\" width=\"1221\" data-original=\"https://pic2.zhimg.com/v2-760f5b2e87d49a2d46871236067fa2cd_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1221&#39; height=&#39;623&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1221\" data-rawheight=\"623\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1221\" data-original=\"https://pic2.zhimg.com/v2-760f5b2e87d49a2d46871236067fa2cd_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-760f5b2e87d49a2d46871236067fa2cd_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><a href=\"https://www.zhihu.com/question/61265076/answer/186347780\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic2.zhimg.com/80/v2-2d19e8c9b39f40044853ea65f6edfb31_180x120.jpg\" data-image-width=\"1345\" data-image-height=\"277\" class=\"internal\">RNN中为什么要采用tanh而不是ReLu作为激活函数？</a><h2>三、前向传播RNN</h2><a href=\"https://link.zhihu.com/?target=https%3A//www.cnblogs.com/xweiblogs/p/5914622.html\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-32fa712d7a5de8172ba3b96deb837010_180x120.jpg\" data-image-width=\"1178\" data-image-height=\"527\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">循环神经网络RNN的基本介绍 - 夜半仰望者 - 博客园</a><a href=\"https://link.zhihu.com/?target=https%3A//www.cnblogs.com/pinard/p/6509630.html\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-891ce7c62ffcfbc761ca4d8465412582_180x120.jpg\" data-image-width=\"644\" data-image-height=\"416\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">循环神经网络(RNN)模型与前向反向传播算法 - 刘建平Pinard - 博客园</a><h2>四、后向传播RNN</h2><h2>五、代码实现</h2><p>模拟运算过程图</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">import</span> <span class=\"nn\">tensorflow</span> <span class=\"kn\">as</span> <span class=\"nn\">tf</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"kn\">as</span> <span class=\"nn\">np</span>\n<span class=\"c1\"># Mini-batch: instance 0,instance 1,instance 2,instance 3</span>\n<span class=\"c1\">#注意下面是一个Batch的输入数据，而不是一个输入数据</span>\n<span class=\"n\">X0_batch</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">6</span><span class=\"p\">,</span> <span class=\"mi\">7</span><span class=\"p\">,</span> <span class=\"mi\">8</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">9</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]])</span> <span class=\"c1\"># t = 0</span>\n<span class=\"n\">X1_batch</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([[</span><span class=\"mi\">9</span><span class=\"p\">,</span> <span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"mi\">7</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">6</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]])</span> <span class=\"c1\"># t = 1</span>\n\n\n<span class=\"c1\">#定义各种参数</span>\n<span class=\"n\">n_inputs</span> <span class=\"o\">=</span> <span class=\"mi\">3</span><span class=\"c1\">#例如[0, 1, 2]有三个数，这个变量指的是这个意思</span>\n<span class=\"n\">n_neurons</span> <span class=\"o\">=</span> <span class=\"mi\">5</span>\n<span class=\"c1\">#placeholder存输入数据，Variable存变量</span>\n<span class=\"n\">X0</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">placeholder</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"bp\">None</span><span class=\"p\">,</span> <span class=\"n\">n_inputs</span><span class=\"p\">])</span>\n<span class=\"n\">X1</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">placeholder</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"bp\">None</span><span class=\"p\">,</span> <span class=\"n\">n_inputs</span><span class=\"p\">])</span>\n<span class=\"c1\">#单个输入，重点关注后面如何从X0/1_batch中取值</span>\n<span class=\"n\">Wx</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">Variable</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">random_normal</span><span class=\"p\">(</span><span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">n_inputs</span><span class=\"p\">,</span> <span class=\"n\">n_neurons</span><span class=\"p\">],</span><span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">))</span>\n<span class=\"n\">Wy</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">Variable</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">random_normal</span><span class=\"p\">(</span><span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">n_neurons</span><span class=\"p\">,</span><span class=\"n\">n_neurons</span><span class=\"p\">],</span><span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">))</span>\n<span class=\"n\">b</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">Variable</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">([</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">n_neurons</span><span class=\"p\">],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">))</span>\n<span class=\"c1\">#该例子并没有给出标签，所以就没有所谓的训练，这里给Wx Wy b随机赋值，整理例子构造计算时间序列输出值得过程</span>\n\n\n<span class=\"n\">Y0</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">tanh</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">X0</span><span class=\"p\">,</span> <span class=\"n\">Wx</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">b</span><span class=\"p\">)</span><span class=\"c1\">#0时刻没有上一时刻，所以Y0=tanh(wx+b)</span>\n<span class=\"n\">Y1</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">tanh</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">Y0</span><span class=\"p\">,</span> <span class=\"n\">Wy</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">X1</span><span class=\"p\">,</span> <span class=\"n\">Wx</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">b</span><span class=\"p\">)</span><span class=\"c1\">#1时刻的h1=y0，上一时刻没有隐状态h0</span>\n<span class=\"n\">init</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">global_variables_initializer</span><span class=\"p\">()</span><span class=\"c1\">#初始化所有变量</span>\n<span class=\"k\">with</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">Session</span><span class=\"p\">()</span> <span class=\"k\">as</span> <span class=\"n\">sess</span><span class=\"p\">:</span>\n    <span class=\"n\">init</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">()</span>\n    <span class=\"n\">Y0_val</span><span class=\"p\">,</span> <span class=\"n\">Y1_val</span> <span class=\"o\">=</span> <span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">([</span><span class=\"n\">Y0</span><span class=\"p\">,</span> <span class=\"n\">Y1</span><span class=\"p\">],</span> <span class=\"n\">feed_dict</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"n\">X0</span><span class=\"p\">:</span> <span class=\"n\">X0_batch</span><span class=\"p\">,</span> <span class=\"n\">X1</span><span class=\"p\">:</span> <span class=\"n\">X1_batch</span><span class=\"p\">})</span>\n    <span class=\"c1\">#feed数据的时候是feed进去一个矩阵(numpy来实现)，实际用的时候每一行算一条数据，这个可以用print(help(tf.placeholder))找到依据</span>\n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">Y0_val</span><span class=\"p\">)</span>\n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">Y0_val</span><span class=\"p\">)</span></code></pre></div><a href=\"https://link.zhihu.com/?target=http%3A//blog.csdn.net/liuchonge/article/details/70809288\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-c8487a387341be89175176cfeb5ecd92_180x120.jpg\" data-image-width=\"303\" data-image-height=\"242\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">使用TensorFlow实现RNN模型入门篇1 - CSDN博客</a><p class=\"ztext-empty-paragraph\"><br/></p><h2>六、RNN神经网络输入输出究竟是怎样的？</h2><a href=\"https://www.zhihu.com/question/41949741?sort=created\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/80/feb56c7a4db812b04172781af9660f52_ipico.jpg\" data-image-width=\"616\" data-image-height=\"518\" class=\"internal\">最清晰的解释</a><a href=\"https://www.zhihu.com/question/32275069\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\"internal\">对上面链接的word embedding的解释</a><p></p>", 
            "topic": [
                {
                    "tag": "RNN", 
                    "tagLink": "https://api.zhihu.com/topics/20086967"
                }
            ], 
            "comments": []
        }
    ], 
    "url": "https://zhuanlan.zhihu.com/c_168715383"
}
