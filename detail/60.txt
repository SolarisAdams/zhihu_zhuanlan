{
    "title": "谓之小一", 
    "description": "谓之小一希望提供给读者别处看不到的内容，关于深度学习、自然语言处理、知识图谱、问答系统、书籍、生活...", 
    "followers": [
        "https://www.zhihu.com/people/evan013", 
        "https://www.zhihu.com/people/le-si-rong", 
        "https://www.zhihu.com/people/fei-er-dao", 
        "https://www.zhihu.com/people/wendy-46-32-82", 
        "https://www.zhihu.com/people/ttleon", 
        "https://www.zhihu.com/people/ya-gu-san-san-30", 
        "https://www.zhihu.com/people/wang-xian-sheng-69-78", 
        "https://www.zhihu.com/people/dai-wei-38-53", 
        "https://www.zhihu.com/people/p1rate", 
        "https://www.zhihu.com/people/xie-xiao-wen-44-89", 
        "https://www.zhihu.com/people/chen-zi-83-32", 
        "https://www.zhihu.com/people/suger-90-53", 
        "https://www.zhihu.com/people/kong-ling-jing-21", 
        "https://www.zhihu.com/people/shu-pian-bu-chi-shu-pian", 
        "https://www.zhihu.com/people/nova-38-35", 
        "https://www.zhihu.com/people/mlxuan", 
        "https://www.zhihu.com/people/su-wen-34-94", 
        "https://www.zhihu.com/people/camille-79-46-7", 
        "https://www.zhihu.com/people/ma-meng-26", 
        "https://www.zhihu.com/people/acproject", 
        "https://www.zhihu.com/people/yi-zhi-da-lan-mao-39", 
        "https://www.zhihu.com/people/innerpeace-24-25-99", 
        "https://www.zhihu.com/people/wu-heng-49-69", 
        "https://www.zhihu.com/people/fei-yu-38-68", 
        "https://www.zhihu.com/people/wang-ya-zhou-1", 
        "https://www.zhihu.com/people/liu-fei-94-95", 
        "https://www.zhihu.com/people/angrycat-73", 
        "https://www.zhihu.com/people/wang-lei-19-60-89", 
        "https://www.zhihu.com/people/jin-tian-you-shi-hou", 
        "https://www.zhihu.com/people/pan-pan-8-26-63", 
        "https://www.zhihu.com/people/123456ddd-42", 
        "https://www.zhihu.com/people/guanhui07", 
        "https://www.zhihu.com/people/yang-ye-79", 
        "https://www.zhihu.com/people/csxz", 
        "https://www.zhihu.com/people/alston-3-13-37", 
        "https://www.zhihu.com/people/tian-ming-da-ge-dun", 
        "https://www.zhihu.com/people/yan-chao-mei-53", 
        "https://www.zhihu.com/people/deng-liao-ge-lei", 
        "https://www.zhihu.com/people/nan-feng-77-48", 
        "https://www.zhihu.com/people/yi-zao-ru-dong-jing-hua", 
        "https://www.zhihu.com/people/zheng-jian-yang-56", 
        "https://www.zhihu.com/people/ma-dong-hui-62", 
        "https://www.zhihu.com/people/johnny-63-54", 
        "https://www.zhihu.com/people/feng-yun-ren-wu-94-29", 
        "https://www.zhihu.com/people/jackey-56-69", 
        "https://www.zhihu.com/people/sun-da-shuai-10", 
        "https://www.zhihu.com/people/puzzle-19-51", 
        "https://www.zhihu.com/people/luvmoan", 
        "https://www.zhihu.com/people/dufei-lee", 
        "https://www.zhihu.com/people/4recommend", 
        "https://www.zhihu.com/people/nian-de-lu-xing-xiang", 
        "https://www.zhihu.com/people/meng-zhen-97-65", 
        "https://www.zhihu.com/people/vapourlee", 
        "https://www.zhihu.com/people/re-yan-pang-guan-69", 
        "https://www.zhihu.com/people/cai-xia-45-46", 
        "https://www.zhihu.com/people/pu-pu-pu-pu-pu-56-88", 
        "https://www.zhihu.com/people/wj2014-59", 
        "https://www.zhihu.com/people/zhangshengdong29", 
        "https://www.zhihu.com/people/xu-yimeng-25", 
        "https://www.zhihu.com/people/xie-chen-80-40", 
        "https://www.zhihu.com/people/cai-jin-chao", 
        "https://www.zhihu.com/people/tian-ye-5-60", 
        "https://www.zhihu.com/people/xiao-shan-94-8", 
        "https://www.zhihu.com/people/trieno", 
        "https://www.zhihu.com/people/qichao-tang", 
        "https://www.zhihu.com/people/mattzheng7", 
        "https://www.zhihu.com/people/wu-ye-nan-30", 
        "https://www.zhihu.com/people/kuang-qi-30-11", 
        "https://www.zhihu.com/people/moslandwez", 
        "https://www.zhihu.com/people/piao-shi-de-feng", 
        "https://www.zhihu.com/people/jiangziqian", 
        "https://www.zhihu.com/people/xin-yi-59-66", 
        "https://www.zhihu.com/people/zhang-yu-52-24", 
        "https://www.zhihu.com/people/paulersu", 
        "https://www.zhihu.com/people/a-qi-65-77", 
        "https://www.zhihu.com/people/he-xiao-yu-24", 
        "https://www.zhihu.com/people/chu-da-shu-80", 
        "https://www.zhihu.com/people/ling-yk", 
        "https://www.zhihu.com/people/BelieveOP5", 
        "https://www.zhihu.com/people/qi-yan-ran-4", 
        "https://www.zhihu.com/people/zhangzbxyz", 
        "https://www.zhihu.com/people/cheng-fu-50-79", 
        "https://www.zhihu.com/people/stephen-34-51", 
        "https://www.zhihu.com/people/zhang-medivn", 
        "https://www.zhihu.com/people/oldli-25", 
        "https://www.zhihu.com/people/alex-8-93", 
        "https://www.zhihu.com/people/jie-xiao-rui-66", 
        "https://www.zhihu.com/people/sharon-32-41", 
        "https://www.zhihu.com/people/wang-ze-wei-28-32", 
        "https://www.zhihu.com/people/alex-45-44-1", 
        "https://www.zhihu.com/people/ayaka1551", 
        "https://www.zhihu.com/people/roy-luo-22", 
        "https://www.zhihu.com/people/zhu-xiu", 
        "https://www.zhihu.com/people/xu-xu-dong-37-36", 
        "https://www.zhihu.com/people/Leezhi403", 
        "https://www.zhihu.com/people/623601391", 
        "https://www.zhihu.com/people/qiu-zhi-yao-chen-zao", 
        "https://www.zhihu.com/people/jackslowfucck", 
        "https://www.zhihu.com/people/zheng-li-96-31", 
        "https://www.zhihu.com/people/lyao-3-35", 
        "https://www.zhihu.com/people/aegency", 
        "https://www.zhihu.com/people/wxqfree", 
        "https://www.zhihu.com/people/30zheng-qing-chun", 
        "https://www.zhihu.com/people/alex-79-81-31", 
        "https://www.zhihu.com/people/qian-ye-64-7", 
        "https://www.zhihu.com/people/simongui-lai", 
        "https://www.zhihu.com/people/lian-zhi-wen", 
        "https://www.zhihu.com/people/kant-yue", 
        "https://www.zhihu.com/people/louB-yong-zhihu", 
        "https://www.zhihu.com/people/ceng-kai-xiang", 
        "https://www.zhihu.com/people/flying-fish-73", 
        "https://www.zhihu.com/people/yu-zhi-bo-a-xiu-luo", 
        "https://www.zhihu.com/people/lykos-80", 
        "https://www.zhihu.com/people/yao-ji-yizhi-xue", 
        "https://www.zhihu.com/people/nao-nao-nao-35", 
        "https://www.zhihu.com/people/zhang-zi-cheng-81", 
        "https://www.zhihu.com/people/jin-ji-de-he-cheng-shi", 
        "https://www.zhihu.com/people/18131375507", 
        "https://www.zhihu.com/people/tao-ji-jiang", 
        "https://www.zhihu.com/people/chuan-chuan-xiang-52", 
        "https://www.zhihu.com/people/yao-yao-yao-yao-qiang", 
        "https://www.zhihu.com/people/lauren-wu", 
        "https://www.zhihu.com/people/cong-ling-kai-shi-29-58", 
        "https://www.zhihu.com/people/zai-yuan-fang-52-5", 
        "https://www.zhihu.com/people/typo-98", 
        "https://www.zhihu.com/people/planbreath", 
        "https://www.zhihu.com/people/du-jiang-li-72-80", 
        "https://www.zhihu.com/people/13689021436", 
        "https://www.zhihu.com/people/reus-2", 
        "https://www.zhihu.com/people/wang-kai-tai-42", 
        "https://www.zhihu.com/people/james-chan-70", 
        "https://www.zhihu.com/people/ironpaul", 
        "https://www.zhihu.com/people/ge-zi-xiao-wo-94", 
        "https://www.zhihu.com/people/zhangzhe", 
        "https://www.zhihu.com/people/hui-gu-jiu-cheng", 
        "https://www.zhihu.com/people/si-ji-he", 
        "https://www.zhihu.com/people/lu-ti-xia", 
        "https://www.zhihu.com/people/susssssssss", 
        "https://www.zhihu.com/people/liu-qi-61-8", 
        "https://www.zhihu.com/people/in0-75", 
        "https://www.zhihu.com/people/bihaiyifan", 
        "https://www.zhihu.com/people/yanqiu-zhang", 
        "https://www.zhihu.com/people/iamlgg", 
        "https://www.zhihu.com/people/aalu-49", 
        "https://www.zhihu.com/people/luo-yi-27-8-38", 
        "https://www.zhihu.com/people/liu-song-yan-11", 
        "https://www.zhihu.com/people/fan-tian-ba", 
        "https://www.zhihu.com/people/xqtbox", 
        "https://www.zhihu.com/people/spirit-soul", 
        "https://www.zhihu.com/people/liu-can-36", 
        "https://www.zhihu.com/people/martinson", 
        "https://www.zhihu.com/people/xu-fan-8", 
        "https://www.zhihu.com/people/moolighty", 
        "https://www.zhihu.com/people/stone-27-11", 
        "https://www.zhihu.com/people/jia-xue-feng", 
        "https://www.zhihu.com/people/one-79-42", 
        "https://www.zhihu.com/people/tan-xiao-yang-95", 
        "https://www.zhihu.com/people/zhou-pu-tong-66", 
        "https://www.zhihu.com/people/wei-zhi-77-59", 
        "https://www.zhihu.com/people/yan-jing-yu-70-53", 
        "https://www.zhihu.com/people/xi-suo-84", 
        "https://www.zhihu.com/people/huangxiaolist", 
        "https://www.zhihu.com/people/ma-yu-xiang-4", 
        "https://www.zhihu.com/people/boyua", 
        "https://www.zhihu.com/people/wu-ming-15-39-11", 
        "https://www.zhihu.com/people/ryan-74-59", 
        "https://www.zhihu.com/people/wangcheny91", 
        "https://www.zhihu.com/people/tian-lei-lei-14", 
        "https://www.zhihu.com/people/yang-ding-kang-1", 
        "https://www.zhihu.com/people/fang-dou", 
        "https://www.zhihu.com/people/wang-wang-wang-34-47", 
        "https://www.zhihu.com/people/scottdc-31", 
        "https://www.zhihu.com/people/qianjun235179", 
        "https://www.zhihu.com/people/zhangyuting", 
        "https://www.zhihu.com/people/hdd0411", 
        "https://www.zhihu.com/people/guangbao", 
        "https://www.zhihu.com/people/array.fu", 
        "https://www.zhihu.com/people/gong-chang-99-68", 
        "https://www.zhihu.com/people/shaw-66-86", 
        "https://www.zhihu.com/people/tlyu0419", 
        "https://www.zhihu.com/people/you-zi-pi-35", 
        "https://www.zhihu.com/people/dickekatze", 
        "https://www.zhihu.com/people/leng-feng-14", 
        "https://www.zhihu.com/people/yi-da-38", 
        "https://www.zhihu.com/people/wang-ming-66-71", 
        "https://www.zhihu.com/people/liu-lang-64", 
        "https://www.zhihu.com/people/lu-gui-fu", 
        "https://www.zhihu.com/people/zanyonghuang", 
        "https://www.zhihu.com/people/chang-kong-da-xia-68", 
        "https://www.zhihu.com/people/wang-tao-95-71", 
        "https://www.zhihu.com/people/chi-la-wang", 
        "https://www.zhihu.com/people/tu-bu-lu-xing-40", 
        "https://www.zhihu.com/people/shan-liang-de-shui-zhi-da-di", 
        "https://www.zhihu.com/people/lou-qi-lin-37", 
        "https://www.zhihu.com/people/axe-41", 
        "https://www.zhihu.com/people/pan-xin-yi", 
        "https://www.zhihu.com/people/alphabetagammadelta", 
        "https://www.zhihu.com/people/li-hai-bo-71", 
        "https://www.zhihu.com/people/da-zhao-ge-51", 
        "https://www.zhihu.com/people/zz-zhang-58-52", 
        "https://www.zhihu.com/people/huang-he-he-43-44", 
        "https://www.zhihu.com/people/hu-jin-91-91-22", 
        "https://www.zhihu.com/people/qing-shan-gu-ren-11", 
        "https://www.zhihu.com/people/15626262327", 
        "https://www.zhihu.com/people/wang-wang-wang-68-46", 
        "https://www.zhihu.com/people/walker-sleep-75", 
        "https://www.zhihu.com/people/mang-guo-mango-70", 
        "https://www.zhihu.com/people/hhwx", 
        "https://www.zhihu.com/people/lue-lue-lue-73-69-26", 
        "https://www.zhihu.com/people/jiang-wei-47-76-47", 
        "https://www.zhihu.com/people/lu-i-83", 
        "https://www.zhihu.com/people/blackgu", 
        "https://www.zhihu.com/people/feiying-93", 
        "https://www.zhihu.com/people/musen-8", 
        "https://www.zhihu.com/people/liu-yichen-81", 
        "https://www.zhihu.com/people/papipipa", 
        "https://www.zhihu.com/people/xu-fei-fei-12-56", 
        "https://www.zhihu.com/people/xie-shao-55", 
        "https://www.zhihu.com/people/cheng-nan-hua-yi-kai-4-89", 
        "https://www.zhihu.com/people/zhang-li-32-82", 
        "https://www.zhihu.com/people/ideas-yd", 
        "https://www.zhihu.com/people/jack-54-23-19", 
        "https://www.zhihu.com/people/bi-mo-si-hou-83", 
        "https://www.zhihu.com/people/xiao-zhao-xian-sheng-36-84", 
        "https://www.zhihu.com/people/dengwenwu", 
        "https://www.zhihu.com/people/aretlas", 
        "https://www.zhihu.com/people/allen-zhou-48", 
        "https://www.zhihu.com/people/limiqs", 
        "https://www.zhihu.com/people/tian-tian-31-90", 
        "https://www.zhihu.com/people/lu-ke-lun", 
        "https://www.zhihu.com/people/roger-34-31-21", 
        "https://www.zhihu.com/people/jingkangy91", 
        "https://www.zhihu.com/people/li_xiaowen", 
        "https://www.zhihu.com/people/Matildajing", 
        "https://www.zhihu.com/people/wudayo", 
        "https://www.zhihu.com/people/ma-lin-57-97", 
        "https://www.zhihu.com/people/qiu-zhi-98-9", 
        "https://www.zhihu.com/people/ju-zi-87-6-6", 
        "https://www.zhihu.com/people/yangkun727", 
        "https://www.zhihu.com/people/yuge-12", 
        "https://www.zhihu.com/people/du-bian-miao", 
        "https://www.zhihu.com/people/pi-dan-97-68", 
        "https://www.zhihu.com/people/wufachuli", 
        "https://www.zhihu.com/people/sun-ying-yun-77", 
        "https://www.zhihu.com/people/caae-31", 
        "https://www.zhihu.com/people/mo-mo-94-56-60", 
        "https://www.zhihu.com/people/lan-lan-lan-lan-96-82", 
        "https://www.zhihu.com/people/bu-xiao-xin-jia-dao-le-kuai-zi", 
        "https://www.zhihu.com/people/wang-li-zhou-35", 
        "https://www.zhihu.com/people/stone12138", 
        "https://www.zhihu.com/people/que-xi-xiao-jie", 
        "https://www.zhihu.com/people/lao-feng-lei", 
        "https://www.zhihu.com/people/chocolate-89", 
        "https://www.zhihu.com/people/ma-cen", 
        "https://www.zhihu.com/people/meng-xiang-xiang-27-74", 
        "https://www.zhihu.com/people/chang-Rainbow79", 
        "https://www.zhihu.com/people/im-mantis", 
        "https://www.zhihu.com/people/Mr.DongDong", 
        "https://www.zhihu.com/people/sherlock-holmes-75-64", 
        "https://www.zhihu.com/people/k1e0", 
        "https://www.zhihu.com/people/mi-lan-de-yao-yao", 
        "https://www.zhihu.com/people/10xian-sheng", 
        "https://www.zhihu.com/people/dan-dandan-71", 
        "https://www.zhihu.com/people/sheng-ss-yuan", 
        "https://www.zhihu.com/people/wang-yi-71-69-9", 
        "https://www.zhihu.com/people/wuhanml", 
        "https://www.zhihu.com/people/aa12", 
        "https://www.zhihu.com/people/dong-feng-zao-ji", 
        "https://www.zhihu.com/people/AI_Technology", 
        "https://www.zhihu.com/people/zheng-shuo-18", 
        "https://www.zhihu.com/people/shi-xun-44-6", 
        "https://www.zhihu.com/people/zhang-shuai-shuai-39-34", 
        "https://www.zhihu.com/people/yoojoo-36", 
        "https://www.zhihu.com/people/ke-ju-44", 
        "https://www.zhihu.com/people/lan-meng-ji-di"
    ], 
    "article": [
        {
            "url": "https://zhuanlan.zhihu.com/p/77594908", 
            "userName": "谓之小一", 
            "userLink": "https://www.zhihu.com/people/761dda159c5cec5fd926f761ab5f4ccf", 
            "upvote": 8, 
            "title": "电影知识图谱问答（五）| 基于微信公众号的电影知识图谱问答Demo", 
            "content": "<p>通过【<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3MjA2NTQzMw%3D%3D%26mid%3D2247484097%26idx%3D1%26sn%3Dfa84465206038fc11bdbe1b564cea9e9%26chksm%3Dfcd7d137cba0582153319759dbd472e609084c92662553205f03cbf8672504ec886bd7462663%26token%3D1313854198%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">电影知识图谱问答（一）|爬取豆瓣电影与书籍详细信息</a>、<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3MjA2NTQzMw%3D%3D%26mid%3D2247484098%26idx%3D1%26sn%3D61cebf54365a2ff1ded72635ad251cc7%26chksm%3Dfcd7d134cba058221c034f1770af6a2aa95c6dc906fdda8e3ddbfc9b4108c0d55e05f6bf8c08%26token%3D1313854198%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">电影知识图谱问答（二）|生成298万条RDF三元组数据</a>、<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3MjA2NTQzMw%3D%3D%26mid%3D2247484099%26idx%3D1%26sn%3D1017d850bfa07a149706434bec255f07%26chksm%3Dfcd7d135cba05823e612b67a67eeaf9d6277d0e404af078ad7d9a08d49e44be71cd6247923c6%26token%3D1313854198%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">电影知识图谱问答（三）|Apache Jena知识存储及SPARQL知识检索</a>、<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3MjA2NTQzMw%3D%3D%26mid%3D2247484106%26idx%3D1%26sn%3D8db42df72eb37e5deea9940f9a0b4a74%26chksm%3Dfcd7d13ccba0582a91e9fb59973b0eb7037524b25c7c5ac8f6e6cbe078584a26cfcec01e301d%26token%3D1439023852%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">电影知识图谱问答（四）| 问句理解及答案推理</a>】四篇文章的介绍，我们已经了解如何从豆瓣官网中爬取数据；如何将爬取的数据转换得到可用的三元组数据，并存储至Apache Jena之中；如何利用SPARQL查询语言进行知识检索和答案推理；如何理解问句所表达的深层语义信息，即获取问句实体和目标属性信息；如何利用问句的深层语义信息，结合规则和表示学习方法，推理得到问题答案。结合上面几篇文章，已经能够从零开始构建一个电影知识图谱问答系统，有兴趣的朋友可以尝试搭建。本篇文章，将介绍如何将<b>电影知识图谱问答系统部署至微信公众平台</b>，部署完成后能够通过微信公众号进行知识问答。</p><h3>本项目相关代码已经发布至 GitHub，项目地址为<a href=\"https://link.zhihu.com/?target=https%3A//github.com/weizhixiaoyi/DouBan-KGQA\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">weizhixiaoyi/DouBan-KGQA</a>，欢迎Star。</h3><hr/><h3>1. 服务器</h3><p>将代码部署至微信公众平台需要一个服务器，如果只是用于Demo展示的话，推荐购买<a href=\"https://link.zhihu.com/?target=https%3A//cloud.tencent.com/act/campus\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">腾讯云学生服务器</a>或者<a href=\"https://link.zhihu.com/?target=https%3A//promotion.aliyun.com/ntms/act/campus2018.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">阿里云学生服务器</a>，价格十分优惠。个人购买的是腾讯云服务器，配置为1核CPU、2G内存、1Mbps带宽、50GB高性能云盘，价格6个月60元。</p><p>购买好服务器之后，需要安装系统环境，个人安装的是Ubuntu16.04系统。系统安装成功之后，安装<a href=\"https://link.zhihu.com/?target=https%3A//www.anaconda.com/distribution/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Anaconda3</a>，配置<a href=\"https://link.zhihu.com/?target=https%3A//mirror.tuna.tsinghua.edu.cn/help/anaconda/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">清华源</a>。然后配置微信公众号，将服务器和微信公众号进行关联。</p><h3>2. 微信公众号开发</h3><p>根据<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/wiki%3Ft%3Dresource/res_main%26id%3Dmp1421135319\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">微信公众平台开发文档接入指南</a>，将服务器接入至微信公众号。微信官方文档已经比较详细，此处不再进行介绍，接入过程中有相关问题直接在文章下方评论即可。下图是个人微信公众号配置截图，有兴趣的话，也可以关注下个人微信公众号<a href=\"https://link.zhihu.com/?target=https%3A//weizhixiaoyi.com/usr/uploads/img/wechat_public.png\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">谓之小一</a>。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-5c4e0349f037519894e588927286d063_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1874\" data-rawheight=\"684\" class=\"origin_image zh-lightbox-thumb\" width=\"1874\" data-original=\"https://pic4.zhimg.com/v2-5c4e0349f037519894e588927286d063_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1874&#39; height=&#39;684&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1874\" data-rawheight=\"684\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1874\" data-original=\"https://pic4.zhimg.com/v2-5c4e0349f037519894e588927286d063_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-5c4e0349f037519894e588927286d063_b.jpg\"/></figure><p>目前只是将服务器成功接入到微信公众号，但是还不能处理用户发送过来的请求，因此需编写用户请求处理方面的代码。处理方法可参考微信官方<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/wiki%3Ft%3Dresource/res_main%26id%3Dmp1472017492_58YV5\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Demo</a>，包括微信公众号处理用户请求的流程，如何接收消息，如何发送消息等问题。</p><p>熟悉官方Demo之后，开始编写豆瓣电影知识图谱(BM-KGQA)对话处理的代码。首先创建query<i>server.py文件，用于响应用户经微信公众号端发送过来的请求，并返回相应答案。代码如下所示，其中</i><b><i>from query</i>main import Query</b>是单条问句处理的接口<b>，import receive</b>, <b>reply</b>是定义的微信消息接收和返回函数。</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"c1\"># query_server.py</span>\n<span class=\"c1\"># -*- coding:utf-8 -*-</span>\n\n<span class=\"kn\">import</span> <span class=\"nn\">web</span>\n<span class=\"kn\">import</span> <span class=\"nn\">hashlib</span>\n<span class=\"kn\">from</span> <span class=\"nn\">query_main</span> <span class=\"k\">import</span> <span class=\"n\">Query</span>\n<span class=\"kn\">import</span> <span class=\"nn\">receive</span><span class=\"o\">,</span> <span class=\"nn\">reply</span>\n\n<span class=\"n\">query</span> <span class=\"o\">=</span> <span class=\"n\">Query</span><span class=\"p\">()</span>\n\n<span class=\"k\">class</span> <span class=\"nc\">Handle</span><span class=\"p\">(</span><span class=\"nb\">object</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"nf\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"c1\"># 初始化query</span>\n        <span class=\"c1\">#  self.query = Query()</span>\n        <span class=\"k\">pass</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">GET</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"k\">try</span><span class=\"p\">:</span>\n            <span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"n\">web</span><span class=\"o\">.</span><span class=\"nb\">input</span><span class=\"p\">()</span>\n            <span class=\"k\">if</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">)</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n                <span class=\"k\">return</span> <span class=\"s2\">&#34;hello, this is handle view&#34;</span>\n            <span class=\"n\">signature</span> <span class=\"o\">=</span> <span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">signature</span>\n            <span class=\"n\">timestamp</span> <span class=\"o\">=</span> <span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">timestamp</span>\n            <span class=\"n\">nonce</span> <span class=\"o\">=</span> <span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">nonce</span>\n            <span class=\"n\">echostr</span> <span class=\"o\">=</span> <span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">echostr</span>\n            <span class=\"c1\"># 和公众平台官网--&gt;基本配置中信息填写相同</span>\n            <span class=\"n\">token</span> <span class=\"o\">=</span> <span class=\"s2\">&#34;douban_kgqa&#34;</span>\n\n            <span class=\"nb\">list</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">token</span><span class=\"p\">,</span> <span class=\"n\">timestamp</span><span class=\"p\">,</span> <span class=\"n\">nonce</span><span class=\"p\">]</span>\n            <span class=\"nb\">list</span><span class=\"o\">.</span><span class=\"n\">sort</span><span class=\"p\">()</span>\n            <span class=\"n\">sha1</span> <span class=\"o\">=</span> <span class=\"n\">hashlib</span><span class=\"o\">.</span><span class=\"n\">sha1</span><span class=\"p\">()</span>\n            <span class=\"nb\">map</span><span class=\"p\">(</span><span class=\"n\">sha1</span><span class=\"o\">.</span><span class=\"n\">update</span><span class=\"p\">,</span> <span class=\"nb\">list</span><span class=\"p\">)</span>\n            <span class=\"n\">hashcode</span> <span class=\"o\">=</span> <span class=\"n\">sha1</span><span class=\"o\">.</span><span class=\"n\">hexdigest</span><span class=\"p\">()</span>\n            <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&#34;handle/GET func: hashcode, signature: &#34;</span><span class=\"p\">,</span> <span class=\"n\">hashcode</span><span class=\"p\">,</span> <span class=\"n\">signature</span><span class=\"p\">)</span>\n            <span class=\"k\">if</span> <span class=\"n\">hashcode</span> <span class=\"o\">==</span> <span class=\"n\">signature</span><span class=\"p\">:</span>\n                <span class=\"k\">return</span> <span class=\"n\">echostr</span>\n            <span class=\"k\">else</span><span class=\"p\">:</span>\n                <span class=\"k\">return</span> <span class=\"s2\">&#34;I don&#39;t Know&#34;</span>\n        <span class=\"k\">except</span> <span class=\"ne\">Exception</span> <span class=\"k\">as</span> <span class=\"n\">err</span><span class=\"p\">:</span>\n            <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;ERROR: &#39;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">err</span><span class=\"p\">))</span>\n            <span class=\"k\">return</span> <span class=\"n\">err</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">POST</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"k\">try</span><span class=\"p\">:</span>\n            <span class=\"n\">webData</span> <span class=\"o\">=</span> <span class=\"n\">web</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">()</span>\n            <span class=\"c1\"># 后台打印日志</span>\n            <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;Handle Post webdata is &#39;</span><span class=\"p\">,</span> <span class=\"n\">webData</span><span class=\"p\">)</span>\n            <span class=\"n\">recMsg</span> <span class=\"o\">=</span> <span class=\"n\">receive</span><span class=\"o\">.</span><span class=\"n\">parse_xml</span><span class=\"p\">(</span><span class=\"n\">webData</span><span class=\"p\">)</span>\n            <span class=\"k\">if</span> <span class=\"nb\">isinstance</span><span class=\"p\">(</span><span class=\"n\">recMsg</span><span class=\"p\">,</span> <span class=\"n\">receive</span><span class=\"o\">.</span><span class=\"n\">Msg</span><span class=\"p\">):</span>\n                <span class=\"n\">toUser</span> <span class=\"o\">=</span> <span class=\"n\">recMsg</span><span class=\"o\">.</span><span class=\"n\">FromUserName</span>\n                <span class=\"n\">fromUser</span> <span class=\"o\">=</span> <span class=\"n\">recMsg</span><span class=\"o\">.</span><span class=\"n\">ToUserName</span>\n                <span class=\"k\">if</span> <span class=\"n\">recMsg</span><span class=\"o\">.</span><span class=\"n\">MsgType</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;text&#39;</span><span class=\"p\">:</span>\n                    <span class=\"c1\"># result = &#34;彩虹屁屁&#34;</span>\n                    <span class=\"n\">question</span> <span class=\"o\">=</span> <span class=\"n\">recMsg</span><span class=\"o\">.</span><span class=\"n\">Content</span>\n                    <span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">query</span><span class=\"o\">.</span><span class=\"n\">parse</span><span class=\"p\">(</span><span class=\"n\">question</span><span class=\"p\">)</span>\n                    <span class=\"n\">replyMsg</span> <span class=\"o\">=</span> <span class=\"n\">reply</span><span class=\"o\">.</span><span class=\"n\">TextMsg</span><span class=\"p\">(</span><span class=\"n\">toUser</span><span class=\"p\">,</span> <span class=\"n\">fromUser</span><span class=\"p\">,</span> <span class=\"n\">result</span><span class=\"p\">)</span>\n                    <span class=\"k\">return</span> <span class=\"n\">replyMsg</span><span class=\"o\">.</span><span class=\"n\">send</span><span class=\"p\">()</span>\n                <span class=\"k\">if</span> <span class=\"n\">recMsg</span><span class=\"o\">.</span><span class=\"n\">MsgType</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;image&#39;</span><span class=\"p\">:</span>\n                    <span class=\"n\">mediaId</span> <span class=\"o\">=</span> <span class=\"n\">recMsg</span><span class=\"o\">.</span><span class=\"n\">MsgId</span>\n                    <span class=\"n\">replyMsg</span> <span class=\"o\">=</span> <span class=\"n\">reply</span><span class=\"o\">.</span><span class=\"n\">ImageMsg</span><span class=\"p\">(</span><span class=\"n\">toUser</span><span class=\"p\">,</span> <span class=\"n\">fromUser</span><span class=\"p\">,</span> <span class=\"n\">mediaId</span><span class=\"p\">)</span>\n                    <span class=\"k\">return</span> <span class=\"n\">replyMsg</span><span class=\"o\">.</span><span class=\"n\">send</span><span class=\"p\">()</span>\n                <span class=\"k\">else</span><span class=\"p\">:</span>\n                    <span class=\"k\">return</span> <span class=\"n\">reply</span><span class=\"o\">.</span><span class=\"n\">Msg</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">send</span><span class=\"p\">()</span>\n            <span class=\"k\">else</span><span class=\"p\">:</span>\n                <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;暂且不处理&#39;</span><span class=\"p\">)</span>\n            <span class=\"k\">return</span> <span class=\"n\">reply</span><span class=\"o\">.</span><span class=\"n\">Msg</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">send</span><span class=\"p\">()</span>\n        <span class=\"k\">except</span> <span class=\"ne\">Exception</span> <span class=\"k\">as</span> <span class=\"n\">err</span><span class=\"p\">:</span>\n            <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;ERROR: &#39;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">err</span><span class=\"p\">))</span>\n            <span class=\"k\">return</span> <span class=\"n\">err</span>\n\n\n<span class=\"n\">urls</span> <span class=\"o\">=</span> <span class=\"p\">(</span>\n    <span class=\"s1\">&#39;/douban_kgqa&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;Handle&#39;</span>\n<span class=\"p\">)</span>\n\n<span class=\"k\">if</span> <span class=\"vm\">__name__</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;__main__&#39;</span><span class=\"p\">:</span>\n    <span class=\"n\">douban_kgqa_web</span> <span class=\"o\">=</span> <span class=\"n\">web</span><span class=\"o\">.</span><span class=\"n\">application</span><span class=\"p\">(</span><span class=\"n\">urls</span><span class=\"p\">,</span> <span class=\"nb\">globals</span><span class=\"p\">())</span>\n    <span class=\"n\">douban_kgqa_web</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">()</span></code></pre></div><p>receive.py是接收用户经微信公众号发送过来的请求，解析后获取问句详细信息。</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"c1\"># receive.py</span>\n<span class=\"c1\"># -*- coding:utf-8 -*-</span>\n<span class=\"kn\">import</span> <span class=\"nn\">xml.etree.ElementTree</span> <span class=\"k\">as</span> <span class=\"nn\">ET</span>\n\n\n<span class=\"k\">def</span> <span class=\"nf\">parse_xml</span><span class=\"p\">(</span><span class=\"n\">web_data</span><span class=\"p\">):</span>\n    <span class=\"k\">if</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">web_data</span><span class=\"p\">)</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n        <span class=\"k\">return</span> <span class=\"kc\">None</span>\n    <span class=\"n\">xmlData</span> <span class=\"o\">=</span> <span class=\"n\">ET</span><span class=\"o\">.</span><span class=\"n\">fromstring</span><span class=\"p\">(</span><span class=\"n\">web_data</span><span class=\"p\">)</span>\n    <span class=\"n\">msg_type</span> <span class=\"o\">=</span> <span class=\"n\">xmlData</span><span class=\"o\">.</span><span class=\"n\">find</span><span class=\"p\">(</span><span class=\"s1\">&#39;MsgType&#39;</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">text</span>\n    <span class=\"k\">if</span> <span class=\"n\">msg_type</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;text&#39;</span><span class=\"p\">:</span>\n        <span class=\"k\">return</span> <span class=\"n\">TextMsg</span><span class=\"p\">(</span><span class=\"n\">xmlData</span><span class=\"p\">)</span>\n    <span class=\"k\">elif</span> <span class=\"n\">msg_type</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;image&#39;</span><span class=\"p\">:</span>\n        <span class=\"k\">return</span> <span class=\"n\">ImageMsg</span><span class=\"p\">(</span><span class=\"n\">xmlData</span><span class=\"p\">)</span>\n\n\n<span class=\"k\">class</span> <span class=\"nc\">Msg</span><span class=\"p\">(</span><span class=\"nb\">object</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"nf\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">xmlData</span><span class=\"p\">):</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">ToUserName</span> <span class=\"o\">=</span> <span class=\"n\">xmlData</span><span class=\"o\">.</span><span class=\"n\">find</span><span class=\"p\">(</span><span class=\"s1\">&#39;ToUserName&#39;</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">text</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">FromUserName</span> <span class=\"o\">=</span> <span class=\"n\">xmlData</span><span class=\"o\">.</span><span class=\"n\">find</span><span class=\"p\">(</span><span class=\"s1\">&#39;FromUserName&#39;</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">text</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">CreateTime</span> <span class=\"o\">=</span> <span class=\"n\">xmlData</span><span class=\"o\">.</span><span class=\"n\">find</span><span class=\"p\">(</span><span class=\"s1\">&#39;CreateTime&#39;</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">text</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">MsgType</span> <span class=\"o\">=</span> <span class=\"n\">xmlData</span><span class=\"o\">.</span><span class=\"n\">find</span><span class=\"p\">(</span><span class=\"s1\">&#39;MsgType&#39;</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">text</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">MsgId</span> <span class=\"o\">=</span> <span class=\"n\">xmlData</span><span class=\"o\">.</span><span class=\"n\">find</span><span class=\"p\">(</span><span class=\"s1\">&#39;MsgId&#39;</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">text</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">Content</span> <span class=\"o\">=</span> <span class=\"n\">xmlData</span><span class=\"o\">.</span><span class=\"n\">find</span><span class=\"p\">(</span><span class=\"s1\">&#39;Content&#39;</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">text</span>\n\n\n<span class=\"k\">class</span> <span class=\"nc\">TextMsg</span><span class=\"p\">(</span><span class=\"n\">Msg</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"nf\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">xmlData</span><span class=\"p\">):</span>\n        <span class=\"n\">Msg</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">xmlData</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">Content</span> <span class=\"o\">=</span> <span class=\"n\">xmlData</span><span class=\"o\">.</span><span class=\"n\">find</span><span class=\"p\">(</span><span class=\"s1\">&#39;Content&#39;</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">text</span><span class=\"o\">.</span><span class=\"n\">encode</span><span class=\"p\">(</span><span class=\"s2\">&#34;utf-8&#34;</span><span class=\"p\">)</span>\n\n\n<span class=\"k\">class</span> <span class=\"nc\">ImageMsg</span><span class=\"p\">(</span><span class=\"n\">Msg</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"nf\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">xmlData</span><span class=\"p\">):</span>\n        <span class=\"n\">Msg</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">xmlData</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">PicUrl</span> <span class=\"o\">=</span> <span class=\"n\">xmlData</span><span class=\"o\">.</span><span class=\"n\">find</span><span class=\"p\">(</span><span class=\"s1\">&#39;PicUrl&#39;</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">text</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">MediaId</span> <span class=\"o\">=</span> <span class=\"n\">xmlData</span><span class=\"o\">.</span><span class=\"n\">find</span><span class=\"p\">(</span><span class=\"s1\">&#39;MediaId&#39;</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">text</span></code></pre></div><p>reply.py是将需要返回的答案封装成微信要求的数据类型。</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"c1\"># reply</span>\n<span class=\"c1\"># -*- coding:utf-8 -*-</span>\n\n<span class=\"kn\">import</span> <span class=\"nn\">time</span>\n\n\n<span class=\"k\">class</span> <span class=\"nc\">Msg</span><span class=\"p\">(</span><span class=\"nb\">object</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"nf\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"k\">pass</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">send</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"k\">return</span> <span class=\"s2\">&#34;success&#34;</span>\n\n\n<span class=\"k\">class</span> <span class=\"nc\">TextMsg</span><span class=\"p\">(</span><span class=\"n\">Msg</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"nf\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">toUserName</span><span class=\"p\">,</span> <span class=\"n\">fromUserName</span><span class=\"p\">,</span> <span class=\"n\">content</span><span class=\"p\">):</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">__dict</span> <span class=\"o\">=</span> <span class=\"nb\">dict</span><span class=\"p\">()</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">__dict</span><span class=\"p\">[</span><span class=\"s1\">&#39;ToUserName&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">toUserName</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">__dict</span><span class=\"p\">[</span><span class=\"s1\">&#39;FromUserName&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">fromUserName</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">__dict</span><span class=\"p\">[</span><span class=\"s1\">&#39;CreateTime&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">time</span><span class=\"o\">.</span><span class=\"n\">time</span><span class=\"p\">())</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">__dict</span><span class=\"p\">[</span><span class=\"s1\">&#39;Content&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">content</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">send</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"n\">XmlForm</span> <span class=\"o\">=</span> <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">        &lt;xml&gt;\n</span><span class=\"s2\">        &lt;ToUserName&gt;&lt;![CDATA[</span><span class=\"si\">{ToUserName}</span><span class=\"s2\">]]&gt;&lt;/ToUserName&gt;\n</span><span class=\"s2\">        &lt;FromUserName&gt;&lt;![CDATA[</span><span class=\"si\">{FromUserName}</span><span class=\"s2\">]]&gt;&lt;/FromUserName&gt;\n</span><span class=\"s2\">        &lt;CreateTime&gt;</span><span class=\"si\">{CreateTime}</span><span class=\"s2\">&lt;/CreateTime&gt;\n</span><span class=\"s2\">        &lt;MsgType&gt;&lt;![CDATA[text]]&gt;&lt;/MsgType&gt;\n</span><span class=\"s2\">        &lt;Content&gt;&lt;![CDATA[</span><span class=\"si\">{Content}</span><span class=\"s2\">]]&gt;&lt;/Content&gt;\n</span><span class=\"s2\">        &lt;/xml&gt;\n</span><span class=\"s2\">        &#34;&#34;&#34;</span>\n        <span class=\"k\">return</span> <span class=\"n\">XmlForm</span><span class=\"o\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"o\">**</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">__dict</span><span class=\"p\">)</span>\n\n\n<span class=\"k\">class</span> <span class=\"nc\">ImageMsg</span><span class=\"p\">(</span><span class=\"n\">Msg</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"nf\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">toUserName</span><span class=\"p\">,</span> <span class=\"n\">fromUserName</span><span class=\"p\">,</span> <span class=\"n\">mediaId</span><span class=\"p\">):</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">__dict</span> <span class=\"o\">=</span> <span class=\"nb\">dict</span><span class=\"p\">()</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">__dict</span><span class=\"p\">[</span><span class=\"s1\">&#39;ToUserName&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">toUserName</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">__dict</span><span class=\"p\">[</span><span class=\"s1\">&#39;FromUserName&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">fromUserName</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">__dict</span><span class=\"p\">[</span><span class=\"s1\">&#39;CreateTime&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">time</span><span class=\"o\">.</span><span class=\"n\">time</span><span class=\"p\">())</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">__dict</span><span class=\"p\">[</span><span class=\"s1\">&#39;MediaId&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">mediaId</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">send</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"n\">XmlForm</span> <span class=\"o\">=</span> <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">        &lt;xml&gt;\n</span><span class=\"s2\">        &lt;ToUserName&gt;&lt;![CDATA[</span><span class=\"si\">{ToUserName}</span><span class=\"s2\">]]&gt;&lt;/ToUserName&gt;\n</span><span class=\"s2\">        &lt;FromUserName&gt;&lt;![CDATA[</span><span class=\"si\">{FromUserName}</span><span class=\"s2\">]]&gt;&lt;/FromUserName&gt;\n</span><span class=\"s2\">        &lt;CreateTime&gt;</span><span class=\"si\">{CreateTime}</span><span class=\"s2\">&lt;/CreateTime&gt;\n</span><span class=\"s2\">        &lt;MsgType&gt;&lt;![CDATA[image]]&gt;&lt;/MsgType&gt;\n</span><span class=\"s2\">        &lt;Image&gt;\n</span><span class=\"s2\">        &lt;MediaId&gt;&lt;![CDATA[</span><span class=\"si\">{MediaId}</span><span class=\"s2\">]]&gt;&lt;/MediaId&gt;\n</span><span class=\"s2\">        &lt;/Image&gt;\n</span><span class=\"s2\">        &lt;/xml&gt;\n</span><span class=\"s2\">        &#34;&#34;&#34;</span>\n        <span class=\"k\">return</span> <span class=\"n\">XmlForm</span><span class=\"o\">.</span><span class=\"nb\">format</span><span class=\"p\">(</span><span class=\"o\">**</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">__dict</span><span class=\"p\">)</span></code></pre></div><p>代码编写完成之后，将所有代码移到服务器端，安装项目所需要的python依赖包，最后利用python query_server.py 80开启服务。开启成功之后，便能够利用微信公众号进行问答。</p><h3>3.微信问答Demo</h3><p>通过上面的配置，已经能够通过微信公众号处理电影,书籍方面的问题，下面来看看<b>书籍-电影知识图谱问答系统(BM-KGQA)</b>的最终效果，下图为BM-KGQA能够处理的问句类型。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-089b36773c85af011a94a23ec0a93300_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"297\" data-rawheight=\"529\" class=\"content_image\" width=\"297\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;297&#39; height=&#39;529&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"297\" data-rawheight=\"529\" class=\"content_image lazy\" width=\"297\" data-actualsrc=\"https://pic1.zhimg.com/v2-089b36773c85af011a94a23ec0a93300_b.jpg\"/></figure><p>针对电影类信息，可提问其主演、导演、编剧、海报、上映地区、上映时间、时长、其他名字、简介、详细信息、评分、评分人数等内容。比如提问“流浪地球的主演是谁？”、“流浪地球的上映时间是什么时候？”、“流浪地球的评分是多少？”、“流浪地球的详细信息是什么”等。需要注意的是，针对问句“流浪地球的评分是多少？”，因“流浪地球”既有书籍也有电影，所以返回两种答案，并对其进行标注区分。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-b9f22d2216d15fe72313a0de601d3a10_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1004\" data-rawheight=\"878\" class=\"origin_image zh-lightbox-thumb\" width=\"1004\" data-original=\"https://pic1.zhimg.com/v2-b9f22d2216d15fe72313a0de601d3a10_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1004&#39; height=&#39;878&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1004\" data-rawheight=\"878\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1004\" data-original=\"https://pic1.zhimg.com/v2-b9f22d2216d15fe72313a0de601d3a10_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-b9f22d2216d15fe72313a0de601d3a10_b.jpg\"/></figure><p>针对电影人物类信息，可提问其照片、性别、星座、生日、出生地、职业、其他名字、详细信息、介绍等内容。比如提问“吴京的生日是什么时候？”、“吴京的出生地是在哪儿？”、“吴京的职业是什么？”、“吴京的星座是什么？”、“吴京个人的详细信息和我说一下”、“吴京主演了哪些电影”、“吴京指导了哪些电影”等。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-3accc39c1721ef92badc0b243f523fd3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"994\" data-rawheight=\"886\" class=\"origin_image zh-lightbox-thumb\" width=\"994\" data-original=\"https://pic4.zhimg.com/v2-3accc39c1721ef92badc0b243f523fd3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;994&#39; height=&#39;886&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"994\" data-rawheight=\"886\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"994\" data-original=\"https://pic4.zhimg.com/v2-3accc39c1721ef92badc0b243f523fd3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-3accc39c1721ef92badc0b243f523fd3_b.jpg\"/></figure><p>针对书籍类信息，可提问其图片、出版社、出版日期、页数、目录、简介、评分、评价人数、详细信息等内容。比如提问“《追风筝的人》的出版社是哪儿？”、“《追风筝的人》的出版日期是什么时候？”、“《追风筝的人》总共多少页？”、“《追风筝的人》的作者是谁呢”、“《追风筝的人》的详细信息？”。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-4faf194c402272e4bcf80f1d8b9eead7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"988\" data-rawheight=\"884\" class=\"origin_image zh-lightbox-thumb\" width=\"988\" data-original=\"https://pic4.zhimg.com/v2-4faf194c402272e4bcf80f1d8b9eead7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;988&#39; height=&#39;884&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"988\" data-rawheight=\"884\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"988\" data-original=\"https://pic4.zhimg.com/v2-4faf194c402272e4bcf80f1d8b9eead7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-4faf194c402272e4bcf80f1d8b9eead7_b.jpg\"/></figure><p>针对书籍类人物信息，可提问其图片、性别、生日、出生地、其他名称、介绍、详细信息等内容。比如提问“杨绛的图片内容？”、“杨绛写作了哪些书？”、“杨绛的出生地是哪儿？”、“杨绛的其他名字叫做什么？”、“杨绛的详细信息和我说一下？”。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-e652c75ee5a491a3d8943db2240292e4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"990\" data-rawheight=\"878\" class=\"origin_image zh-lightbox-thumb\" width=\"990\" data-original=\"https://pic1.zhimg.com/v2-e652c75ee5a491a3d8943db2240292e4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;990&#39; height=&#39;878&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"990\" data-rawheight=\"878\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"990\" data-original=\"https://pic1.zhimg.com/v2-e652c75ee5a491a3d8943db2240292e4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-e652c75ee5a491a3d8943db2240292e4_b.jpg\"/></figure><p>以上，便是<b>书籍-电影知识图谱智能问答系统(BM-KGQA)</b>的最终效果，能够通过微信公众号来精确回答用户关于书籍-电影方面的问题。当然，目前该知识图谱问答系统仅能够处理书籍,电影领域的问题，处理过程也需要依赖大量规则模版，功能还不是很完善。但通过此项目，能够给初学者提供一个解决问题的完整流程，即如何利用知识图谱来进行特定领域知识问答。后续，将继续进行完善，包括但不限于将特定领域内知识问答推广至百科类知识问答；构建端到端的问句理解模块，直接从中抽取出问句三元组；能够提供复杂问句理解和复杂答案推理功能。</p><p>欢迎关注公众号<b>谓之小一</b>阅读更多内容。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-f65a22cd2e3a45ffa2109a5dc948e167_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"990\" data-rawheight=\"1642\" class=\"origin_image zh-lightbox-thumb\" width=\"990\" data-original=\"https://pic4.zhimg.com/v2-f65a22cd2e3a45ffa2109a5dc948e167_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;990&#39; height=&#39;1642&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"990\" data-rawheight=\"1642\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"990\" data-original=\"https://pic4.zhimg.com/v2-f65a22cd2e3a45ffa2109a5dc948e167_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-f65a22cd2e3a45ffa2109a5dc948e167_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "知识图谱", 
                    "tagLink": "https://api.zhihu.com/topics/19838204"
                }, 
                {
                    "tag": "问答系统", 
                    "tagLink": "https://api.zhihu.com/topics/19571693"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/77594347", 
            "userName": "谓之小一", 
            "userLink": "https://www.zhihu.com/people/761dda159c5cec5fd926f761ab5f4ccf", 
            "upvote": 1, 
            "title": "电影知识图谱问答（四）| 问句理解及答案推理", 
            "content": "<p>上篇文章《<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3MjA2NTQzMw%3D%3D%26mid%3D2247484099%26idx%3D1%26sn%3D1017d850bfa07a149706434bec255f07%26chksm%3Dfcd7d135cba05823e612b67a67eeaf9d6277d0e404af078ad7d9a08d49e44be71cd6247923c6%26token%3D1313854198%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">电影知识图谱问答（三）|Apache Jena知识存储及SPARQL知识检索</a>》中讲到如何将处理后的RDF数据存储至Apache Jena数据库之中、如何利用SPARQL语句从Apache Jena之中进行知识检索和答案推理。本篇文章将主要介绍如何<b>理解问句所表达的深层语义含义</b>、如何<b>将自然语言问句转换成SPARQL查询语句</b>、如何进行<b>答案推理</b>。</p><hr/><p>上篇文章讲到利用SPARQL语句能够从Apache Jena数据库之中检索得到问题答案，那么如果想要构建电影知识图谱问答系统，亟需解决的问题就是如何将<b>自然语言问句</b>转换成SPARQL查询语句。比如问句<b>“流浪地球的主演有哪些？”</b>，转换成如下SPARQL查询语句需要经过哪些步骤呢？</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"n\">PREFIX</span> <span class=\"p\">:</span> <span class=\"o\">&lt;</span><span class=\"n\">http</span><span class=\"p\">:</span><span class=\"o\">//</span><span class=\"n\">www</span><span class=\"o\">.</span><span class=\"n\">douban_kgqa</span><span class=\"o\">.</span><span class=\"n\">com</span><span class=\"c1\">#&gt;</span>\n<span class=\"n\">PREFIX</span> <span class=\"n\">rdf</span><span class=\"p\">:</span> <span class=\"o\">&lt;</span><span class=\"n\">http</span><span class=\"p\">:</span><span class=\"o\">//</span><span class=\"n\">www</span><span class=\"o\">.</span><span class=\"n\">w3</span><span class=\"o\">.</span><span class=\"n\">org</span><span class=\"o\">/</span><span class=\"mi\">1999</span><span class=\"o\">/</span><span class=\"mi\">02</span><span class=\"o\">/</span><span class=\"mi\">22</span><span class=\"o\">-</span><span class=\"n\">rdf</span><span class=\"o\">-</span><span class=\"n\">syntax</span><span class=\"o\">-</span><span class=\"n\">ns</span><span class=\"c1\">#&gt;</span>\n<span class=\"n\">PREFIX</span> <span class=\"n\">owl</span><span class=\"p\">:</span> <span class=\"o\">&lt;</span><span class=\"n\">http</span><span class=\"p\">:</span><span class=\"o\">//</span><span class=\"n\">www</span><span class=\"o\">.</span><span class=\"n\">w3</span><span class=\"o\">.</span><span class=\"n\">org</span><span class=\"o\">/</span><span class=\"mi\">2002</span><span class=\"o\">/</span><span class=\"mi\">07</span><span class=\"o\">/</span><span class=\"n\">owl</span><span class=\"c1\">#&gt;</span>\n<span class=\"n\">PREFIX</span> <span class=\"n\">rdfs</span><span class=\"p\">:</span> <span class=\"o\">&lt;</span><span class=\"n\">http</span><span class=\"p\">:</span><span class=\"o\">//</span><span class=\"n\">www</span><span class=\"o\">.</span><span class=\"n\">w3</span><span class=\"o\">.</span><span class=\"n\">org</span><span class=\"o\">/</span><span class=\"mi\">2000</span><span class=\"o\">/</span><span class=\"mi\">01</span><span class=\"o\">/</span><span class=\"n\">rdf</span><span class=\"o\">-</span><span class=\"n\">schema</span><span class=\"c1\">#&gt;</span>\n\n<span class=\"n\">SELECT</span> <span class=\"err\">?</span><span class=\"n\">x</span> <span class=\"n\">WHERE</span> <span class=\"p\">{</span>\n  <span class=\"err\">?</span><span class=\"n\">s</span> <span class=\"p\">:</span><span class=\"n\">movie_info_name</span> <span class=\"s1\">&#39;流浪地球&#39;</span><span class=\"o\">.</span>\n  <span class=\"err\">?</span><span class=\"n\">s</span> <span class=\"p\">:</span><span class=\"n\">has_actor</span> <span class=\"err\">?</span><span class=\"n\">a</span><span class=\"o\">.</span>\n  <span class=\"err\">?</span><span class=\"n\">a</span> <span class=\"p\">:</span><span class=\"n\">movie_person_name</span> <span class=\"err\">?</span><span class=\"n\">x</span><span class=\"o\">.</span>\n<span class=\"p\">}</span>\n<span class=\"n\">LIMIT</span> <span class=\"mi\">25</span></code></pre></div><h3>1. 问句理解</h3><p>针对用户提问的自然语言问句，首先需要理解其中的深层次语义信息，即获取问句实体和目标属性信息。以问句<b>“流浪地球的导演是谁？”</b>为例，其问句实体是<b>流浪地球</b>、目标属性是<b>导演</b>，所采用的方法分别是实体识别和属性链接。</p><h3>1.1 实体识别</h3><p>从问句中提取出实体可以采用以下两种方法：1）构建诸如<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1508.01991.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">BiLSTM-CRF</a>等深度学习模型，然后利用训练好的深度学习模型预测出问句实体。2）构建实体词表，从问句中提取词表中所包含的实体。</p><p>第一种深度学习方法，能够预测得到训练数据中未出现过的电影名称，预测准确率保持在90%以上。缺点是需要构建训练数据，从头开始训练深度学习模型，耗费时间长；第二种词表方法，构建快捷方便，缺点是只能够发现词表中包含的电影实体名称，无法发现新的电影实体。</p><p>比较推荐的方法是词表+BiLSTM-CRF深度学习模型，但此处为了构建方便，只采用词表方法。词表构建方法是从爬取的数据之中，选出其中的<b>电影和书籍名称</b>、<b>人物名称</b>加入到词表之中。另外，有兴趣的朋友，可加上深度学习预测模型。BiLSTM-CRF模型在GitHub上有很多，可自主寻找。</p><h3>1.2 属性链接</h3><p>属性链接可以采用以下两种方法：1）构建诸如CNN等多分类深度学习模型，然后利用训练好的深度学习模型预测问句的目标属性。2）构建关键词集合，把问句中所包含的关键词当作问句的目标属性。</p><p>同样，此处问了方便，直接采用关键词方法。有兴趣的朋友，可自主加上CNN等多分类预测模型。CNN等多分类模型GitHub上有很多，此处不再介绍。</p><p>需要注意的是，同一目标属性可以表达成多种含义。比如流浪地球的<b>评分</b>是多少？、也可以表达成流浪地球在豆瓣有多少<b>分数</b>？，那么此时我们就需要同时考虑<b>评分</b>和<b>分数</b>关键词。</p><blockquote>rating = (W(&#39;评分&#39;) | W(&#39;分数&#39;))  # 评分</blockquote><h3>2. 答案推理</h3><h3>2.1基于规则的答案推理</h3><p>获取问句的实体和目标属性之后，便可根据规则模版将传统<b>自然语言问句</b>转换得到<b>SPARQL</b>查询语句，进而从Apache Jena数据库之中推理得到问题答案。构建规则模型可利用Python Refo库进行构建，比如构建<b>某某电影的导演是谁？</b>模糊匹配规则，方法如下所示。</p><blockquote>movie_info_rules = [<br/>    Rule(condition_num=1,condition=(book_or_movie_entity + Star(Any(), greedy=False) + director + Star(Any(), greedy=False)) | (writer + Star(Any(), greedy=False) + book_or_movie_entity) + Star(Any(), greedy=False), action=QuestionSet.has_director)<br/>]</blockquote><p>模糊匹配规则中<b>has_director</b>为SPARQL检索语句转换函数，函数定义方法如下所示。</p><blockquote>SPARQL_PREFIX = u&#34;&#34;&#34;PREFIX : &lt;<a href=\"https://link.zhihu.com/?target=http%3A//www.douban_kgqa.com\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">douban_kgqa.com</span><span class=\"invisible\"></span></a>#&gt;<br/>PREFIX rdf: &lt;<a href=\"https://link.zhihu.com/?target=http%3A//www.w3.org/1999/02/22-rdf-syntax-ns%23\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">w3.org/1999/02/22-rdf-s</span><span class=\"invisible\">yntax-ns#</span><span class=\"ellipsis\"></span></a>&gt;<br/>PREFIX owl: &lt;<a href=\"https://link.zhihu.com/?target=http%3A//www.w3.org/2002/07/owl%23\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">w3.org/2002/07/owl</span><span class=\"invisible\">#</span></a>&gt; #某电影有哪些导演<br/>&#34;&#34;&#34;<br/><br/>SPARQL_SELECT_TEM = u&#34;{prefix}\\n&#34; + \\<br/>                    u&#34;SELECT DISTINCT {select} WHERE {{\\n&#34; + \\<br/>                    u&#34;{expression}\\n&#34; + \\<br/>                    u&#34;}}\\n&#34;<br/><br/>def has_director(word_objects):<br/>        &#34;&#34;&#34;<br/>        某电影有哪些导演<br/>        :param word_objects:<br/>        :return:<br/>        &#34;&#34;&#34;<br/>        select = u&#34;?x&#34;<br/>        sparql = None<br/>        for w in word_objects:<br/>            if w.pos == pos_book_or_movie:<br/>                e = u&#34;?m :movie_info_name &#39;{movie}&#39;.\\n&#34; \\<br/>                    u&#34;?m :has_director ?a.\\n&#34; \\<br/>                    u&#34;?a :movie_person_name ?x&#34;.format(movie=w.token)<br/><br/>                sparql = SPARQL_SELECT_TEM.format(prefix=SPARQL_PREFIX,<br/>                                                  select=select,<br/>                                                  expression=e)<br/>                break<br/><br/>        return sparql</blockquote><p>通过上述规则模版，能够处理以下类型的简单问句。</p><blockquote># 某电影的图片/上映地区/语言/上映时间/时长/其他名称/介绍/评分/ 评价人数<br/># 某电影的类型<br/># 某电影有哪些演员<br/># 某电影有哪些编剧<br/># 某电影有哪些导演<br/># 某电影的详细信息<br/><br/># 某人的图片/性别/星座/生日/出生地/职业/其他名称/介绍/<br/># 某人演了哪些电影<br/># 某人写了哪些电影<br/># 某人指导了哪些电影<br/># 某人的详细信息</blockquote><p>当然，也可以处理以下类型的复杂问句，但规则模版构建比较复杂。</p><blockquote># 某电影的评分是否大于8<br/># 哪些喜剧电影的评分小于4<br/># ...<br/><br/># 某人出演了多少部电影<br/># 某演员参演的评分大于X的电影有哪些<br/># 某演员出演过哪些类型的电影<br/># 演员A和演员B合作出演了哪些电影<br/># ...</blockquote><p>将问句转换成SPARQL查询语句之后，便可从Apache Jena之中检索得到问句答案，查询代码如下所示。另外，为提高推理的准确率，还可以对<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3MjA2NTQzMw%3D%3D%26mid%3D2247484099%26idx%3D1%26sn%3D1017d850bfa07a149706434bec255f07%26chksm%3Dfcd7d135cba05823e612b67a67eeaf9d6277d0e404af078ad7d9a08d49e44be71cd6247923c6%26token%3D1313854198%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">电影知识图谱问答（三）|Apache Jena知识存储及SPARQL知识检索</a>中所介绍的自定义推理规则进行补充。</p><div class=\"highlight\"><pre><code class=\"language-text\"># -*- coding:utf-8 -*-\n\n&#34;&#34;&#34;\njena fuseki查询\n&#34;&#34;&#34;\n\nfrom collections import OrderedDict\nfrom SPARQLWrapper import SPARQLWrapper, JSON\n\n\nclass SparqlQuery:\n    &#34;&#34;&#34;\n    SPARQL 查询\n    &#34;&#34;&#34;\n\n    def __init__(self, endpoint_url=&#39;http://localhost:3030/douban_kgqa/query&#39;):\n        &#34;&#34;&#34;\n        初始化链接\n        :param endpoint_url:\n        &#34;&#34;&#34;\n        self.sparql_conn = SPARQLWrapper(endpoint_url)\n\n    def get_sparql_result(self, query):\n        &#34;&#34;&#34;\n        根据查询条件,得到查询结果\n        :param query:\n        :return:\n        &#34;&#34;&#34;\n        self.sparql_conn.setQuery(query)\n        self.sparql_conn.setReturnFormat(JSON)\n        return self.sparql_conn.query().convert()\n\n    @staticmethod\n    def parse_result(query_result):\n        &#34;&#34;&#34;\n        解析返回的结果\n        :param query_result:\n        :return:\n        &#34;&#34;&#34;\n        try:\n            query_head = query_result[&#39;head&#39;][&#39;vars&#39;]\n            query_results = []\n            for r in query_result[&#39;results&#39;][&#39;bindings&#39;]:\n                temp_dict = OrderedDict()\n                for h in query_head:\n                    temp_dict[h] = r[h][&#39;value&#39;]\n                query_results.append(temp_dict)\n            return query_head, query_results\n        except Exception as err:\n            print(&#39;解析结果错误&#39; + str(err))\n\n    def get_sparql_result_value(self, query_result):\n        &#34;&#34;&#34;\n        列表存储结果值\n        :param query_result:\n        :return:\n        &#34;&#34;&#34;\n        query_head, query_result = self.parse_result(query_result)\n        if query_head is None:\n            return query_result\n        else:\n            values = []\n            for qr in query_result:\n                for _, value in qr.items():\n                    values.append(value)\n            return values</code></pre></div><h3>2.2 基于表示学习的答案推理</h3><p>通过问句理解模块，能够得到问句的实体和目标属性信息。然后结合基于模版的答案推理方法，能够将问句转换成SPARQL查询语句，进而在Apache Jena数据库之中推理得到问题答案。但基于规则的答案推理仅能够处理已定义的规则，不能覆盖问句的所有情况。而我们又不能定义所有规则，这应该怎么处理呢？</p><p>这时，可以采用<b>基于表示学习的答案推理</b>方法，比如知识图谱嵌入中经典的Trans系列方法。这里我们以<a href=\"https://link.zhihu.com/?target=https%3A//www.utc.fr/~bordesan/dokuwiki/_media/en/transe_nips13.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">TransE</a>为例进行解释，知识图谱中三元组向量化后可以表示为&lt;h, r, t&gt;，其中头实体为h、关系为r、尾实体为t。TransE假设实体和关系之间存在，即头实体h加上关系r的向量信息近似等于尾实体，那么我们便能够通过头实体和关系预测得到尾实体。也就是说，能够通过问句中实体和目标属性信息，预测得到问句答案。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-5118cfed71ff15b32f3a735f87b1ad59_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"460\" data-rawheight=\"391\" class=\"origin_image zh-lightbox-thumb\" width=\"460\" data-original=\"https://pic2.zhimg.com/v2-5118cfed71ff15b32f3a735f87b1ad59_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;460&#39; height=&#39;391&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"460\" data-rawheight=\"391\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"460\" data-original=\"https://pic2.zhimg.com/v2-5118cfed71ff15b32f3a735f87b1ad59_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-5118cfed71ff15b32f3a735f87b1ad59_b.jpg\"/></figure><p>此处对TransE原理内容不进行过多介绍，有兴趣的朋友可以去看<a href=\"https://link.zhihu.com/?target=https%3A//www.utc.fr/~bordesan/dokuwiki/_media/en/transe_nips13.pd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">论文</a>。TransE训练代码可以从<a href=\"https://link.zhihu.com/?target=https%3A//github.com/thunlp/OpenKE\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">thunlp/OpenKE</a>获取，训练数据可以从已爬取的豆瓣数据中抽取，训练完成后便可结合问句理解模块进行答案预测。</p><h3>3. 总结</h3><p>本篇文章介绍了如何<b>理解问句深层次语义信息</b>、如何将<b>自然语言问句转换成SPARQL查询语句</b>、如何利用<b>TransE表示学习</b>进行答案预测。至此，通过【<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3MjA2NTQzMw%3D%3D%26mid%3D2247484097%26idx%3D1%26sn%3Dfa84465206038fc11bdbe1b564cea9e9%26chksm%3Dfcd7d137cba0582153319759dbd472e609084c92662553205f03cbf8672504ec886bd7462663%26token%3D1313854198%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">一</a>、<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3MjA2NTQzMw%3D%3D%26mid%3D2247484098%26idx%3D1%26sn%3D61cebf54365a2ff1ded72635ad251cc7%26chksm%3Dfcd7d134cba058221c034f1770af6a2aa95c6dc906fdda8e3ddbfc9b4108c0d55e05f6bf8c08%26token%3D1313854198%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">二</a>、<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3MjA2NTQzMw%3D%3D%26mid%3D2247484099%26idx%3D1%26sn%3D1017d850bfa07a149706434bec255f07%26chksm%3Dfcd7d135cba05823e612b67a67eeaf9d6277d0e404af078ad7d9a08d49e44be71cd6247923c6%26token%3D1313854198%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">三</a>、<a href=\"https://link.zhihu.com/?target=https%3A//weizhixiaoyi.com/archives/368.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">四</a>】几篇文章的介绍，我们已经了解如何从豆瓣官网中爬取数据；如何将爬取的数据转换得到可用的三元组数据，并存储至Apache Jena之中；如何利用SPARQL查询语言进行知识检索和答案推理；如何理解问句所表达的深层语义信息，即获取问句实体和目标属性信息；如何利用问句的深层语义信息，结合规则和表示学习方法，推理得到问题答案。结合上面几篇文章，已经能够从零开始构建一个电影知识图谱问答系统，有兴趣的朋友可以尝试构建。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-06bdb2ced47860e01569aedc5a2dc190_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"787\" data-rawheight=\"491\" class=\"origin_image zh-lightbox-thumb\" width=\"787\" data-original=\"https://pic1.zhimg.com/v2-06bdb2ced47860e01569aedc5a2dc190_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;787&#39; height=&#39;491&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"787\" data-rawheight=\"491\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"787\" data-original=\"https://pic1.zhimg.com/v2-06bdb2ced47860e01569aedc5a2dc190_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-06bdb2ced47860e01569aedc5a2dc190_b.jpg\"/></figure><p>下篇文章，将介绍如何将电影知识图谱问答系统部署至微信公众平台，并利用微信公众号进行知识问答，构建一个完整的知识图谱问答系统Demo。</p><p>欢迎关注公众号谓之小一阅读更多内容。 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-f65a22cd2e3a45ffa2109a5dc948e167_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"990\" data-rawheight=\"1642\" class=\"origin_image zh-lightbox-thumb\" width=\"990\" data-original=\"https://pic4.zhimg.com/v2-f65a22cd2e3a45ffa2109a5dc948e167_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;990&#39; height=&#39;1642&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"990\" data-rawheight=\"1642\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"990\" data-original=\"https://pic4.zhimg.com/v2-f65a22cd2e3a45ffa2109a5dc948e167_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-f65a22cd2e3a45ffa2109a5dc948e167_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "知识图谱", 
                    "tagLink": "https://api.zhihu.com/topics/19838204"
                }, 
                {
                    "tag": "问答系统", 
                    "tagLink": "https://api.zhihu.com/topics/19571693"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/77497246", 
            "userName": "谓之小一", 
            "userLink": "https://www.zhihu.com/people/761dda159c5cec5fd926f761ab5f4ccf", 
            "upvote": 0, 
            "title": "电影知识图谱问答（三）|Apache Jena知识存储及SPARQL知识检索", 
            "content": "<p>上篇文章《<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3MjA2NTQzMw%3D%3D%26mid%3D2247484098%26idx%3D1%26sn%3D61cebf54365a2ff1ded72635ad251cc7%26chksm%3Dfcd7d134cba058221c034f1770af6a2aa95c6dc906fdda8e3ddbfc9b4108c0d55e05f6bf8c08%26token%3D1227257816%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">电影知识图谱问答（二）|生成298万条RDF三元组数据</a>》中讲到如何将爬取得到的豆瓣电影和书籍数据转换成知识图谱所需的RDF类型数据，本篇文章将介绍如何<b>将得到的298万条RDF类型数据存储到知识图谱数据库</b>之中，并介绍如何<b>利用SPARQL进行知识检索</b>。实践之前，请自主学习<b>Apache Jena</b>, <b>Apache Fuseki</b>, <b>SPARQL</b>相关知识。</p><hr/><h3>1. 知识图谱数据库</h3><p>既然是要存储三元组数据，那选择什么样的数据库呢？你可能见过很多类型的数据库软件，比如MySql、MongoDB等，那么能不能采用这些传统数据库呢？答案是不能，因此传统关系型数据库不能够体现知识间的层次关系，更不能进行知识推理和知识检索。因此，需要选择特定的图数据库，目前常用的图数据库包括Neo4j和Apache Jena。</p><p>Neo4j是高性能、NoSQL类型的图数据库，存储过程中将数据表示为节点，数据之间的关系表示为边，节点和边的类型可以是字符串、数字等。Neo4j能够存储百亿节点，形成巨大的图网络结构，即大规模知识图谱。Neo4j能够非常方便的将数据可视化，看出数据之间的关联关系，可视化效果如下所示。 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-b409511d63b3d57ce4fd0ed6fa60005e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1600\" data-rawheight=\"734\" class=\"origin_image zh-lightbox-thumb\" width=\"1600\" data-original=\"https://pic3.zhimg.com/v2-b409511d63b3d57ce4fd0ed6fa60005e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1600&#39; height=&#39;734&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1600\" data-rawheight=\"734\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1600\" data-original=\"https://pic3.zhimg.com/v2-b409511d63b3d57ce4fd0ed6fa60005e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-b409511d63b3d57ce4fd0ed6fa60005e_b.jpg\"/></figure><p>Apache Jena是开源的Java语义网框架，用于链接数据和构建语义网，可存储RDF、RDFS类型数据。Apache Jena提供TDB、Rule Reasoner、Fuseki组件，其中TDB是Jena用于存储RDF类型数据的组件，属于存储层面的技术；Rule Reasoner可进行简单规则推理，支持用户进行自定义推理规则；Fuseki是Jena提供的SPARQL服务器，支持SPARQL语言进行检索，可在单机和服务器端高效运行。 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-5e43d598a0a5e3b6c2b086fe901cbc43_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"651\" data-rawheight=\"629\" class=\"origin_image zh-lightbox-thumb\" width=\"651\" data-original=\"https://pic4.zhimg.com/v2-5e43d598a0a5e3b6c2b086fe901cbc43_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;651&#39; height=&#39;629&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"651\" data-rawheight=\"629\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"651\" data-original=\"https://pic4.zhimg.com/v2-5e43d598a0a5e3b6c2b086fe901cbc43_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-5e43d598a0a5e3b6c2b086fe901cbc43_b.jpg\"/></figure><p>因为知识图谱问答需定义很多推理规则，对可视化没有太多要求，所以我们选择Apache Jena来存储RDF数据。有兴趣的朋友可尝试Neo4j图数据库，另外北大自研的gStore也可以尝试一下，有成果后欢迎分享。</p><h3>2. Apache Jena知识存储</h3><p>选择好存储方法(Apache Jena)之后，便需要了解如何进行知识存储和知识检索，具体流程包括将RDF类型数据转换成TDB类型数据、配置及启动Apache Fuseki、利用SPARQL从Apache Jena中进行知识检索。</p><h3>2.1 RDF2TDB</h3><p>Apache Jena需要tdb类型的数据，所以需要将已得到的RDF类型数据转换成tdb类型数据，转换方法可通过Apache Jena提供的工具进行实现。</p><p>首先创建tdb文件夹，后续用于存储生成的tdb类型数据。接下来下载<a href=\"https://link.zhihu.com/?target=https%3A//mirrors.tuna.tsinghua.edu.cn/apache/jena/binaries/apache-jena-3.12.0.tar.gz\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Apache Jena</a>，下载完成之后进入到apache-jena-3.12.0/bin目录，利用下列命令将RDF类型数据转换成TDB类型数据，其中/GitHub/DouBan-KGQA/data/tdb是tdb文件夹路径，/GitHub/DouBan-KGQA/data/rdf/douban_kgqa.nt是生成的RDF数据地址。</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"o\">./</span><span class=\"n\">tdbloader</span> <span class=\"o\">--</span><span class=\"n\">loc</span><span class=\"o\">=</span><span class=\"s2\">&#34;/GitHub/DouBan-KGQA/data/tdb&#34;</span> <span class=\"s2\">&#34;/GitHub/DouBan-KGQA/data/rdf/douban_kgqa.nt&#34;</span></code></pre></div><h3>2.2 Fuseki配置</h3><p>转换成TDB类型数据完成之后，如果想要在网页端进行查看和检索，还需要配置Apache Fuseki。首先进行下载<a href=\"https://link.zhihu.com/?target=https%3A//mirrors.tuna.tsinghua.edu.cn/apache/jena/binaries/apache-jena-fuseki-3.12.0.tar.gz\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Apache Fuseki</a>，下载完成之后，进入到apache-jena-fuseki-3.12.0/文件夹内，运行下列命令。</p><div class=\"highlight\"><pre><code class=\"language-text\">./fuseki-server</code></pre></div><p>运行完之后，退出上述命令，Apache Fuseki会自动在apache-jena-fuseki-3.12.0/文件夹内创建run/文件。进入到/apache-jena-fuseki-3.12.0/run/database/文件夹，创建douban<i>kgqa</i>inference.ttl文件，配置自定义推理规则，示例如下所示，比如自反规则(p导演了电影m也可以表示为m电影的导演是p)。</p><div class=\"highlight\"><pre><code class=\"language-text\">@prefix : &lt;http://www.douban_kgqa.com#&gt; .\n@prefix owl: &lt;http://www.w3.org/2002/07/owl#&gt; .\n@prefix rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; .\n@prefix xsd: &lt;XML Schema&gt; .\n@prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt; .\n\n\n[ruleInverse: (?p :has_acted_in ?m) -&gt; (?m :has_actor ?p)]\n[ruleInverse: (?p :has_writed_in ?m) -&gt; (?m :has_writer ?p)]\n[ruleInverse: (?p :has_directed_in ?m) -&gt; (?m :has_director ?p)]\n[ruleInverse: (?p :has_authored_in ?m) -&gt; (?m :has_author ?p)]\n[ruleInverse: (?p :has_translated_in ?m) -&gt; (?m :has_translator ?p)]</code></pre></div><p>自定义规则配置完成之后，需要将生成的tdb类型数据和Apache Fuseki进行关联，配置文件路径为/apache-jena-fuseki-3.12.0/run/configuration/fuseki_conf.ttl。配置文件如下所示，其中需要修改的是<b>fuseki:name</b>、<b>ja:rulesFrom</b>、<b>tdb:location</b>。fuseki:name替换成前面定义的数据库名称，ja:rulesFrom为自定义推理机路径，tdb:location为生成的tdb文件夹路径。详细配置文件可参考官方教程<a href=\"https://link.zhihu.com/?target=https%3A//jena.apache.org/documentation/fuseki2/fuseki-configuration.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">jena.apache.org/documen</span><span class=\"invisible\">tation/fuseki2/fuseki-configuration.html</span><span class=\"ellipsis\"></span></a>。</p><div class=\"highlight\"><pre><code class=\"language-text\">@prefix fuseki:  &lt;http://jena.apache.org/fuseki#&gt; .\n@prefix rdf:     &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt; .\n@prefix rdfs:    &lt;http://www.w3.org/2000/01/rdf-schema#&gt; .\n@prefix tdb:     &lt;http://jena.hpl.hp.com/2008/tdb#&gt; .\n@prefix ja:      &lt;http://jena.hpl.hp.com/2005/11/Assembler#&gt; .\n@prefix :        &lt;#&gt; .\n\n&lt;#service1&gt;  rdf:type fuseki:Service ;\n    fuseki:name                       &#34;douban_kgqa&#34; ;       # http://host:port/tdb\n    fuseki:serviceQuery               &#34;sparql&#34; ;    # SPARQL query service\n    fuseki:serviceQuery               &#34;query&#34; ;    # SPARQL query service (alt name)\n    fuseki:serviceUpdate              &#34;update&#34; ;   # SPARQL update service\n    fuseki:serviceUpload              &#34;upload&#34; ;   # Non-SPARQL upload service\n    fuseki:serviceReadWriteGraphStore &#34;data&#34; ;     # SPARQL Graph store protocol (read and write)\n    # A separate read-only graph store endpoint:\n    fuseki:serviceReadGraphStore      &#34;get&#34; ;      # SPARQL Graph store protocol (read only)\n    fuseki:dataset           &lt;#dataset&gt; ;\n    .\n\n&lt;#dataset&gt; rdf:type ja:RDFDataset ;\n    ja:defaultGraph &lt;#modelInf&gt; ;\n    .\n\n&lt;#modelInf&gt; rdf:type ja:InfModel ;\n    ja:reasoner [\n        ja:reasonerURL &lt;http://jena.hpl.hp.com/2003/GenericRuleReasoner&gt;;\n        # ubuntu\n        # ja:rulesFrom &lt;file:///home/ubuntu/DouBan-KGQA/json2jena/rdf2jena/apache-jena-fuseki-3.10.0.2/run/databases/douban_kgqa_inference.ttl&gt;;\n    ];\n    ja:baseModel &lt;#g&gt; ;\n    .\n\n&lt;#g&gt; rdf:type tdb:GraphTDB ;\n    # ubuntu\n    # tdb:location &#34;/home/ubuntu/DouBan-KGQA/data/tdb/&#34; ;\n    tdb:unionDefaultGraph true ;</code></pre></div><p>fuseki_conf.ttl配置完成之后，再次运行下列命令，便能够启动Apache Jena和Apache Fuseki服务。</p><div class=\"highlight\"><pre><code class=\"language-text\">./fuseki-server</code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ce84b9ff5ebf0b59ca2f898b3b7b2313_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2558\" data-rawheight=\"462\" class=\"origin_image zh-lightbox-thumb\" width=\"2558\" data-original=\"https://pic4.zhimg.com/v2-ce84b9ff5ebf0b59ca2f898b3b7b2313_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2558&#39; height=&#39;462&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2558\" data-rawheight=\"462\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2558\" data-original=\"https://pic4.zhimg.com/v2-ce84b9ff5ebf0b59ca2f898b3b7b2313_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-ce84b9ff5ebf0b59ca2f898b3b7b2313_b.jpg\"/></figure><p>启动成功之后，打开<a href=\"https://link.zhihu.com/?target=http%3A//localhost%3A3030/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">localhost:3030/</span><span class=\"invisible\"></span></a>网页便能够进行查看和知识检索。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-8d326a2fd70ef17390f4ff393d51d0ef_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2558\" data-rawheight=\"1380\" class=\"origin_image zh-lightbox-thumb\" width=\"2558\" data-original=\"https://pic4.zhimg.com/v2-8d326a2fd70ef17390f4ff393d51d0ef_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2558&#39; height=&#39;1380&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2558\" data-rawheight=\"1380\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2558\" data-original=\"https://pic4.zhimg.com/v2-8d326a2fd70ef17390f4ff393d51d0ef_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-8d326a2fd70ef17390f4ff393d51d0ef_b.jpg\"/></figure><h3>3. SPARQL知识检索</h3><p>数据存储成功之后，便能够通过SPARQL检索语言从Apache Jena数据库之中进行检索答案。比如查询<b>流浪地球的主演有哪些？</b>，翻译成SPARQL检索语言如下所示。</p><div class=\"highlight\"><pre><code class=\"language-text\">PREFIX : &lt;http://www.douban_kgqa.com#&gt;\nPREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-syntax-ns#&gt;\nPREFIX owl: &lt;http://www.w3.org/2002/07/owl#&gt;\nPREFIX rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;\n\nSELECT ?x WHERE {\n  ?s :movie_info_name &#39;流浪地球&#39;.\n  ?s :has_actor ?a.\n  ?a :movie_person_name ?x.\n}\nLIMIT 25</code></pre></div><p>利用上述SPARQL查询语言，在Apache Fuseki网页中便能够检索得到答案，如下图所示，能够得到如下吴京、赵今麦等等答案。 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-cf6552b09e17cfba5df1ec22d59d1ac2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2560\" data-rawheight=\"1382\" class=\"origin_image zh-lightbox-thumb\" width=\"2560\" data-original=\"https://pic3.zhimg.com/v2-cf6552b09e17cfba5df1ec22d59d1ac2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2560&#39; height=&#39;1382&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2560\" data-rawheight=\"1382\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2560\" data-original=\"https://pic3.zhimg.com/v2-cf6552b09e17cfba5df1ec22d59d1ac2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-cf6552b09e17cfba5df1ec22d59d1ac2_b.jpg\"/></figure><p>当然，通过SPARQL查询语言也能够查询得到<b>流浪地球的上映时间是什么时候？</b>、<b>流浪地球的导演是谁？</b>、<b>吴京的出生地是在哪儿？</b>、<b>围城的作者是谁？</b>等等问题答案。但难点问题是如何将自然语言问句转换得到SPARQL查询语句？</p><h3>4.总结</h3><p>本篇文章介绍了常用两种图数据库的特点，并选用Apache Jena数据库作为知识存储。同时，介绍了如何将RDF类型数据转换成Apache Jena所需的tdb类型数据，如何配置Apache Fuseki引擎，如何利用SPARQL查询语句进行知识检索。但同时我们发现，利用SPARQL能够进行知识检索，但如何将自然语言问句转换成SPARQL查询语句成为难点问题，下篇文章我们进行详细分析。</p><p>欢迎关注公众号<b>谓之小一</b>阅读更多内容。 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-f65a22cd2e3a45ffa2109a5dc948e167_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"990\" data-rawheight=\"1642\" class=\"origin_image zh-lightbox-thumb\" width=\"990\" data-original=\"https://pic4.zhimg.com/v2-f65a22cd2e3a45ffa2109a5dc948e167_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;990&#39; height=&#39;1642&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"990\" data-rawheight=\"1642\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"990\" data-original=\"https://pic4.zhimg.com/v2-f65a22cd2e3a45ffa2109a5dc948e167_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-f65a22cd2e3a45ffa2109a5dc948e167_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "知识图谱", 
                    "tagLink": "https://api.zhihu.com/topics/19838204"
                }, 
                {
                    "tag": "问答系统", 
                    "tagLink": "https://api.zhihu.com/topics/19571693"
                }
            ], 
            "comments": [
                {
                    "userName": "一只大懒猫", 
                    "userLink": "https://www.zhihu.com/people/bcead3e5d37a5f2e42f110008bff45b1", 
                    "content": "<p>github上面的tbd数据链接失效了，请问能再发一次吗</p><a class=\"comment_sticker\" href=\"https://pic2.zhimg.com/v2-f941117b9911dd9af1a6b637fc22ee9d.gif\" data-width=\"\" data-height=\"\">[安慰]</a>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "谓之小一", 
                            "userLink": "https://www.zhihu.com/people/761dda159c5cec5fd926f761ab5f4ccf", 
                            "content": "<p>github上链接已更新 </p>", 
                            "likes": 0, 
                            "replyToAuthor": "一只大懒猫"
                        }
                    ]
                }, 
                {
                    "userName": "金蟾", 
                    "userLink": "https://www.zhihu.com/people/8e94658477557db1ce66faabdfd710c0", 
                    "content": "这个本体文件跟生成rdf貌似没有关系啊", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/58840251", 
            "userName": "谓之小一", 
            "userLink": "https://www.zhihu.com/people/761dda159c5cec5fd926f761ab5f4ccf", 
            "upvote": 21, 
            "title": "电影知识图谱问答（二）|生成298万条RDF三元组数据", 
            "content": "<p>本篇文章接<a href=\"https://zhuanlan.zhihu.com/p/58126191\" class=\"internal\">《电影知识图谱问答（一）|爬取豆瓣电影与书籍详细信息》</a>，学习如何利用爬取的数据，构建知识图谱所需的三元组。主要内容包括如何从Json类型的数据，转换成RDF数据，并最终存储到Jena之中，然后利用SPARQL进行查询。数据链接: <a href=\"https://link.zhihu.com/?target=https%3A//pan.baidu.com/s/1cLdsAXLGH2akJqMIsGdoig\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">pan.baidu.com/s/1cLdsAX</span><span class=\"invisible\">LGH2akJqMIsGdoig</span><span class=\"ellipsis\"></span></a> 提取码: n97y。</p><p>实践之前，请自主学习相关背景知识。</p><ul><li> 语义网络, 语义网, 链接数据, 知识图谱是什么。<br/> </li><li> RDF, RDFS, OWL, Protege, 本体构建。<br/> </li><li> MySQL数据库, pymysql。<br/> </li><li> D2rq, Jena, fuseki, SPARQL。<br/> </li></ul><h2>1.数据清洗</h2><ul><li><b>电影信息</b>包括电影id、图片链接、名称、导演名称、编剧名称、主演名称、类型、制片国家、语言、上映日期、片长、季数、其他名称、剧情简介、评分、评分人数，共<b>67245条</b>数据信息。虽说是电影信息，但其中也包括电视剧、综艺、动漫、纪录片、短片。</li><li><b>电影演员信息</b>包括演员id、姓名、图片链接、性别、星座、出生日期、出生地、职业、更多中文名、更多外文名、家庭成员、简介，共<b>89592条</b>数据信息。这里所指的演员包括电影演员、编剧、导演。</li><li><b>书籍信息</b>包括书籍id、图片链接、姓名、子标题、原作名称、作者、译者、出版社、出版年份、页数、价格、内容简介、目录简介、评分、评分人数，共<b>64321条</b>数据信息。</li><li><b>书籍作者信息</b>包括作者id，姓名、图片链接、性别、出生日期、国家、更多中文名、更多外文名、简介，共<b>6231条</b>数据信息。这里作者包括书籍作者和译者。</li></ul><p>上述为我们爬取的数据类别，但数据有很多噪音，比如中文电影名称会外接英文电影名称、某些类型数据严重缺失、数据格式不统一等等，这就需要我们根据具体数据进行具体分析。此处需要多搬搬砖，没什么技术，不多讲。</p><h2>2. Json2MySQL</h2><p>首先我们将json类型的数据存储到MySQL之中，这里共构建了13个表，包含</p><ul><li><b>movie_genre:</b> 包含movie_genre_id, movie_genre_name属性，表示movie类别信息。 </li><li><b>movie_info:</b> 包含movie_info_id, movie_info_name, movie_info_image_url, movie_info_country, movie_info_language, movie_info_pubdate, movie_info_duration, movie_info_other_name, movie_info_summary, movie_info_rating, movie_info_review_count属性，表示movie信息。</li><li><b>movie_person:</b> 包含movie_person_id, movie_person_name, movie_person_image_url, movie_person_gender, movie_person_constellation, movie_person_birthday, movie_person_birthplace, movie_person_profession, movie_person_other_name, movie_person_introduction属性，表示movie_person信息。</li><li><b>movie_to_gender:</b> 包含movie_info_id, movie_genre_id属性，设置两个外键，分别关联到movie_info表和movie_genre表，表示movie到genre的关联。</li><li><b>actor_to_movie:</b> 包含movie_info_id, movie_actor_id属性，设置两个外键，分别关联到movie_info表和movie_person表，表示movie到actor的关联。</li><li><b>writer_to_movie:</b> 包含movie_info_id, movie_writer_id，设置两个外键，分别关联到movie_info表和movie_person表，表示movie到writer的关联。</li><li><b>director_to_movie:</b> 包含movie_info_id, movie_director_id，设置两个外键，分别关联到movie_info表和movie_person表，表示movie到director的关联。</li><li>同理，根据图书信息构建<b>book_genre, book_info, book_person_info, book_to_genre, author_to_book, translator_to_book</b>表。</li></ul><p>表构建好之后，利用pymysql将Json类型数据导入到MySQL之中。</p><h2>3. RDB2RDF</h2><p>我们已经将Json类型的数据导入到关系型数据库RDB之中，现在问题是怎么将RDB Data转换成RDF。转换之前，我们先根据数据构建本体。</p><h2>3.1 本体构建</h2><p>什么是本体？本体有点哲学的含义，在计算机领域，可以理解为一种模型，用于描述由一套对象类型（概念或者说类）属性以及关系类型所构成的世界。此处我们使用Protege进行本体建模。</p><p>首先下载protege，下载链接为<a href=\"https://link.zhihu.com/?target=https%3A//protege.stanford.edu/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">protege.stanford.edu/</span><span class=\"invisible\"></span></a>。安装完成之后，新建class，如果没有的话，在window-&gt;Tabs-&gt;Classes寻找。根据MySQL之中构建的表，此处构建相应的类，如下所示。红色箭头表示的是构建子类，右边图标指的是构建兄弟类，最右边指的是删除当前类。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-d3a3b166ea2d2fc946b4f1eafb15c543_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2554\" data-rawheight=\"1438\" class=\"origin_image zh-lightbox-thumb\" width=\"2554\" data-original=\"https://pic4.zhimg.com/v2-d3a3b166ea2d2fc946b4f1eafb15c543_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2554&#39; height=&#39;1438&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2554\" data-rawheight=\"1438\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2554\" data-original=\"https://pic4.zhimg.com/v2-d3a3b166ea2d2fc946b4f1eafb15c543_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-d3a3b166ea2d2fc946b4f1eafb15c543_b.jpg\"/></figure><p>类构建完成之后，进行构建对象属性，共包含</p><ul><li><b>has_movie_genre:</b> domains为movie_info, ranges为movie_genre，表示某电影有某类别。</li><li><b>has_book_genre:</b> domains为book_info, ranges为book_genre，表示某书籍有某类别。</li><li><b>has_actor:</b> domains为movie_info, ranges为movie_actor，表示某电影有某演员。和has_acted_in为互逆关系。</li><li><b>has_acted_in:</b> domains为movie_actor, ranges为movie_info，表示某演员出演了某电影。和has_actor为互逆关系。</li><li>同理<b>has_writer, has_writed_in, has_director, has_directed_in, has_author, has_authored_in, has_translator, has_translator_in</b>构建方法相同。</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b78a4d8551eccdda489374443fc757e5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2556\" data-rawheight=\"1444\" class=\"origin_image zh-lightbox-thumb\" width=\"2556\" data-original=\"https://pic2.zhimg.com/v2-b78a4d8551eccdda489374443fc757e5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2556&#39; height=&#39;1444&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2556\" data-rawheight=\"1444\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2556\" data-original=\"https://pic2.zhimg.com/v2-b78a4d8551eccdda489374443fc757e5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-b78a4d8551eccdda489374443fc757e5_b.jpg\"/></figure><p>对象属性构建完成之后，进行构建数据属性。数据属性构建比较简单，指明数据类别和值类别即可。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-c040718ece20c2327a5c8c7871196bac_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2560\" data-rawheight=\"1442\" class=\"origin_image zh-lightbox-thumb\" width=\"2560\" data-original=\"https://pic1.zhimg.com/v2-c040718ece20c2327a5c8c7871196bac_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2560&#39; height=&#39;1442&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2560\" data-rawheight=\"1442\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2560\" data-original=\"https://pic1.zhimg.com/v2-c040718ece20c2327a5c8c7871196bac_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-c040718ece20c2327a5c8c7871196bac_b.jpg\"/></figure><p>构建完成之后，可以通过OntoGrap看到关系图。可以去window-&gt;Tabs-&gt;OntoGrap寻找OntoGrap。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-4a4b91c4406bc3fd525971c4964d00d9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1670\" data-rawheight=\"938\" class=\"origin_image zh-lightbox-thumb\" width=\"1670\" data-original=\"https://pic2.zhimg.com/v2-4a4b91c4406bc3fd525971c4964d00d9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1670&#39; height=&#39;938&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1670\" data-rawheight=\"938\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1670\" data-original=\"https://pic2.zhimg.com/v2-4a4b91c4406bc3fd525971c4964d00d9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-4a4b91c4406bc3fd525971c4964d00d9_b.jpg\"/></figure><p>最后通过File-&gt;Save as保存成Turtle Syntax形式，命名为<b>douban_kgqa_ontology.owl</b>。</p><h2>3.2 D2RQ</h2><p>RDB转换成RDF有两种方式，一是direct mapping，即直接映射。规则为</p><ul><li>数据库的表作为本体中的类（Class）。</li><li>表的列作为属性（Property）。</li><li>表的行作为实例/资源。</li><li>表的单元格值为字面量。</li><li>如果单元格所在的列是外键，那么其值为IRI，或者说实体/资源。</li></ul><p>但实际中，我们很少使用这种方法，因为不能把RDB中数据映射到我们定义的本体上面。因此我们采用另外一种方法，R2RDF(RDB to RDF Mapping Language)，链接为<a href=\"https://link.zhihu.com/?target=https%3A//www.w3.org/TR/r2rml/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">w3.org/TR/r2rml/</span><span class=\"invisible\"></span></a>。下面我们使用D2RQ工具将RDB数据转换到RDF形式。</p><p>D2RQ提供了自己的mapping language，其形式和R2RML类似，具体语法链接为<a href=\"https://link.zhihu.com/?target=https%3A//www.w3.org/TR/2004/REC-owl-features-20040210/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">w3.org/TR/2004/REC-owl-</span><span class=\"invisible\">features-20040210/</span><span class=\"ellipsis\"></span></a>。D2RQ有一个比较方便的地方，可以根据已定义的数据库自动生成预定义的mapping文件，用户可以在mapping文件上修改，把数据映射到自己的本体上。</p><p>首先下载D2RQ文件，链接为<a href=\"https://link.zhihu.com/?target=http%3A//d2rq.org/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">d2rq.org/</span><span class=\"invisible\"></span></a>，进入到目录之中，利用下列命令生成<b>douban_kgqa_mapping.ttl</b>文件。</p><p>mac, linux系统命令为</p><blockquote>./generate-mapping -u root -p 123456 -o douban_kgqa_mapping.ttl jdbc:mysql:///douban_kgqa </blockquote><p>windows系统命令为</p><blockquote>generate-mapping.bat -u root -o douban_kgqa_mapping.ttl jdbc:mysql:///douban_kgqa</blockquote><p>参数解读：root是mysql用户名，123456是root密码，douban_kgqa_mapping.ttl是输出文件名称，douban_kgqa是MySQL数据库名称。注：如果Mac用户如果提示permission denied, 可以用chmod改变访问权限，chmod 777 generate-mapping。</p><p>现在根据我们的MySQL数据库已经生成了默认的<b>douban_kgqa_mapping.ttl</b>文件，然后根据<b>douban_kgqa_ontology.owl</b>中定义的本体修改<b>douban_kgqa_mapping.ttl</b>文件。修改规则如下</p><ul><li>将<b>id</b>和<b>label</b>属性删除，因为我们不需要这两个属性。</li><li>修改类型值，将vocab:xxxx修改为我们owl文件中定义的类。例如将<b>d2rq:class vocab: movie_genre;</b>修改为<b>d2rq:class :movie_genre;</b></li></ul><h2>3.3 D2RQ RDF</h2><p>利用下列命令将数据转换成我们需要的RDF数据。</p><p>mac, linux命令为</p><blockquote> ./dump-rdf -o douban_kgqa.nt ./douban_kgqa_mapping.ttl</blockquote><p>windows命令为</p><blockquote> .\\dump-rdf -o douban_kgqa.nt .\\douban_kgqa_mapping.ttl</blockquote><p>参数解读：douban_kgqa_mapping.ttl是我们修改后的mapping文件，其支持导出的RDF格式有TURTLE, RDF/XML, RDF/XML-ABBREV, N3, N-TRIPLE，N-TRIPLE是默认的输出格式。</p><p>利用下列命令，我们能够在http://localhost:2020/ 上进行SPARQL数据查询，有兴趣的读者可以尝试一下。</p><blockquote> ./d2r-server ./douban_kgqa_mapping.ttl</blockquote><p>最后查看一下我们生成的RDF数据，可以看到共298万行，前10行的数据格式。其实我们爬虫只运行了两天，数据还是太少，以后有空闲时间再更新更多数据。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-e0de7a01f917f9f18b4bf31c6a2f2ac5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2558\" data-rawheight=\"1000\" class=\"origin_image zh-lightbox-thumb\" width=\"2558\" data-original=\"https://pic2.zhimg.com/v2-e0de7a01f917f9f18b4bf31c6a2f2ac5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2558&#39; height=&#39;1000&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2558\" data-rawheight=\"1000\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2558\" data-original=\"https://pic2.zhimg.com/v2-e0de7a01f917f9f18b4bf31c6a2f2ac5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-e0de7a01f917f9f18b4bf31c6a2f2ac5_b.jpg\"/></figure><h2>4.总结</h2><p>本篇文章主讲了如何将Json数据存储到关系型数据库之中，Protege构建本体方法，如何将RDB数据转换成RDF类型数据。篇幅有限，下篇文章再讲如何将RDF数据转换成TDB数据并存储到Jena之中，如何利用Fuseki, SPARQL进行查询，如何自定义推理规则。欢迎关注公众号<b>谓之小一</b>阅读更多内容。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-f65a22cd2e3a45ffa2109a5dc948e167_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"990\" data-rawheight=\"1642\" class=\"origin_image zh-lightbox-thumb\" width=\"990\" data-original=\"https://pic4.zhimg.com/v2-f65a22cd2e3a45ffa2109a5dc948e167_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;990&#39; height=&#39;1642&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"990\" data-rawheight=\"1642\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"990\" data-original=\"https://pic4.zhimg.com/v2-f65a22cd2e3a45ffa2109a5dc948e167_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-f65a22cd2e3a45ffa2109a5dc948e167_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "知识图谱", 
                    "tagLink": "https://api.zhihu.com/topics/19838204"
                }, 
                {
                    "tag": "MySQL", 
                    "tagLink": "https://api.zhihu.com/topics/19554128"
                }
            ], 
            "comments": [
                {
                    "userName": "伏熵减", 
                    "userLink": "https://www.zhihu.com/people/67742b88e66a76a9667abe28f0721b6f", 
                    "content": "大佬能留个微信咨询一下问题嘛", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "一只大懒猫", 
                    "userLink": "https://www.zhihu.com/people/bcead3e5d37a5f2e42f110008bff45b1", 
                    "content": "<p>请问一下，如果你的rdf数据导入到neo4j图数据库中大概是什么思路，我下载了你的.nt文件，每个三元组是这样的&lt;file:///Users/zhenhaiwang/GitHub/DouBan-KGQA/data/rdf/douban_kgqa.nt#book_genre/1&gt; &lt;<a href=\"http://link.zhihu.com/?target=http%3A//www.douban_kgqa.com\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">douban_kgqa.com</span><span class=\"invisible\"></span></a>#has_book_genre&gt; &lt;file:///Users/zhenhaiwang/GitHub/DouBan-KGQA/data/rdf/douban_kgqa.nt#book_info/1000754&gt; 。照着网上几个帖子导入都没有成功<a href=\"http://link.zhihu.com/?target=https%3A//www.jianshu.com/p/6ed57d5f9747\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">jianshu.com/p/6ed57d5f9</span><span class=\"invisible\">747</span><span class=\"ellipsis\"></span></a>。</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/58126191", 
            "userName": "谓之小一", 
            "userLink": "https://www.zhihu.com/people/761dda159c5cec5fd926f761ab5f4ccf", 
            "upvote": 5, 
            "title": "电影知识图谱问答（一）|爬取豆瓣电影与书籍详细信息", 
            "content": "<p></p><p>最近在做关于知识图谱方面的实验，需要一些数据，于是爬取了豆瓣上关于电影和书籍的信息。两天时间内共爬取<b>20W+</b>条数据，包括电影信息、电影演员信息、书籍信息、书籍作者信息，GitHub链接为<a href=\"https://link.zhihu.com/?target=https%3A//github.com/weizhixiaoyi/DouBan-Spider\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/weizhixiaoyi</span><span class=\"invisible\">/DouBan-Spider</span><span class=\"ellipsis\"></span></a>。</p><hr/><h2>1. 数据说明</h2><ul><li><b>电影信息</b>包括电影id、图片链接、名称、导演名称、编剧名称、主演名称、类型、制片国家、语言、上映日期、片长、季数、集数、其他名称、剧情简介、评分、评分人数，共<b>67245条</b>数据信息。虽说是电影信息，但其中也包括电视剧、综艺、动漫、纪录片、短片。<br/> </li><li><b>电影演员信息</b>包括演员id、姓名、图片链接、性别、星座、出生日期、出生地、职业、更多中文名、更多外文名、家庭成员、简介，共<b>89592条</b>数据信息。这里所指的演员包括电影演员、编剧、导演。<br/> </li><li><b>书籍信息</b>包括书籍id、图片链接、姓名、子标题、原作名称、作者、译者、出版社、出版年份、页数、价格、内容简介、目录简介、评分、评分人数，共<b>64321条</b>数据信息。<br/> </li><li><b>书籍作者信息</b>包括作者id，姓名、图片链接、性别、出生日期、国家、更多中文名、更多外文名、简介，共<b>6231条</b>数据信息。这里作者包括书籍作者和译者。<br/> </li></ul><h2>2. 配制环境</h2><ul><li> 系统环境：ubuntu 18.04<br/> </li><li> python环境：python3.6<br/> </li><li> python依赖包：requests, bs4, redis, yaml, multiprocessing<br/> </li></ul><h2>3. 爬虫思路</h2><h2>3.1电影爬虫</h2><p>进入电影分类界面后，找到开发者工具，找到NetWork-&gt;XHR，我们能够看到Request URL为<a href=\"https://link.zhihu.com/?target=https%3A//movie.douban.com/j/new_search_subjects%3Fsort%3DU%26range%3D0%2C10%26tags%3D%26start%3D0%26genres%3D%25E5%2589%25A7%25E6%2583%2585\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">https://movie.douban.com/j/new_search_subjects?sort=U&amp;range=0,10&amp;tags=&amp;start=0&amp;genres=剧情</a>。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b81a251534d7d39551f68be776f05d69_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2560\" data-rawheight=\"1600\" class=\"origin_image zh-lightbox-thumb\" width=\"2560\" data-original=\"https://pic2.zhimg.com/v2-b81a251534d7d39551f68be776f05d69_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2560&#39; height=&#39;1600&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2560\" data-rawheight=\"1600\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2560\" data-original=\"https://pic2.zhimg.com/v2-b81a251534d7d39551f68be776f05d69_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-b81a251534d7d39551f68be776f05d69_b.jpg\"/></figure><p>打开上述Request URL，能够看到一系列电影信息，我们只需要拿到其中的id即可。为了确保不重复爬取相同的电影，每拿到一个id之后，都存到redis已爬取队列之中。如果下次再遇到相同的id，则跳过不进行爬取。</p><p>另外，再次观察上面URL，发现只要改变start和genres，便能够拿到所有电影id。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-c0389dae76b9a4628da3ffc2fee441cf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2560\" data-rawheight=\"1600\" class=\"origin_image zh-lightbox-thumb\" width=\"2560\" data-original=\"https://pic4.zhimg.com/v2-c0389dae76b9a4628da3ffc2fee441cf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2560&#39; height=&#39;1600&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2560\" data-rawheight=\"1600\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2560\" data-original=\"https://pic4.zhimg.com/v2-c0389dae76b9a4628da3ffc2fee441cf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-c0389dae76b9a4628da3ffc2fee441cf_b.jpg\"/></figure><p>以新喜剧之王id 4840388为例，拼接<a href=\"https://link.zhihu.com/?target=https%3A//movie.douban.com/subject\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">movie.douban.com/subjec</span><span class=\"invisible\">t</span><span class=\"ellipsis\"></span></a>后得到Movie URL为<a href=\"https://link.zhihu.com/?target=https%3A//movie.douban.com/subject/4840388\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">movie.douban.com/subjec</span><span class=\"invisible\">t/4840388</span><span class=\"ellipsis\"></span></a>。请求Movie URL，便能得到电影信息。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-03dd6912a9432b11c81979a99565198b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2560\" data-rawheight=\"1600\" class=\"origin_image zh-lightbox-thumb\" width=\"2560\" data-original=\"https://pic4.zhimg.com/v2-03dd6912a9432b11c81979a99565198b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2560&#39; height=&#39;1600&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2560\" data-rawheight=\"1600\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2560\" data-original=\"https://pic4.zhimg.com/v2-03dd6912a9432b11c81979a99565198b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-03dd6912a9432b11c81979a99565198b_b.jpg\"/></figure><p>通过BeautifulSoup选取相应标签，便能够拿到电影id、图片链接、名称、导演名称、编剧名称、主演名称、类型、制片国家、语言、上映日期、片长、季数、集数、其他名称、剧情简介、评分、评分人数信息。</p><p>爬取电影信息结束之后，将演员id单独进行提取出来。同样为了保证不重复爬取，每得到一个演员id，都存放到redis已爬取队列之中。</p><p>以张全蛋id /celberity/1350283为例，拼接<a href=\"https://link.zhihu.com/?target=https%3A//movie.douban.com/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">https://movie.douban.com</a>，便能得到演员URF为<a href=\"https://link.zhihu.com/?target=https%3A//movie.douban.com/celebrity/1350283/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">movie.douban.com/celebr</span><span class=\"invisible\">ity/1350283/</span><span class=\"ellipsis\"></span></a>。然后请求演员URL，利用BeautifulSoup选取相应标签，便能拿到演员id、姓名、图片链接、性别、星座、出生日期、出生地、职业、更多中文名、更多外文名、家庭成员、简介信息。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-fcc12c66c391bc5517e968ada4ea03ea_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2560\" data-rawheight=\"1600\" class=\"origin_image zh-lightbox-thumb\" width=\"2560\" data-original=\"https://pic3.zhimg.com/v2-fcc12c66c391bc5517e968ada4ea03ea_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2560&#39; height=&#39;1600&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2560\" data-rawheight=\"1600\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2560\" data-original=\"https://pic3.zhimg.com/v2-fcc12c66c391bc5517e968ada4ea03ea_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-fcc12c66c391bc5517e968ada4ea03ea_b.jpg\"/></figure><p>总结一下，获取电影信息和电影演员信息流程为</p><ol><li> 获取<a href=\"https://link.zhihu.com/?target=https%3A//movie.douban.com/tag/%23/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">movie.douban.com/tag/#</span><span class=\"invisible\">/</span><span class=\"ellipsis\"></span></a>界面所有电影类别genres，循环电影类别genres。<br/> </li><li> 请求<a href=\"https://link.zhihu.com/?target=https%3A//movie.douban.com/j/new_search_subjects%3Fsort%3DU%26range%3D0%2C10%26tags%3D%26start%3D0%26genres%3D%25E5%2589%25A7%25E6%2583%2585\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">https://movie.douban.com/j/new_search_subjects?sort=U&amp;range=0,10&amp;tags=&amp;start=0&amp;genres=剧情</a>网址，每次返回20个电影id。如果返回为空，更换电影类别genres。<br/> </li><li> 对返回的20个电影id存放到redis已爬取队列之中，返回去重后的电影id list。<br/> </li><li> 多线程爬取电影id list之中的电影信息。<br/> </li><li> 获取电影演员id，存到到redis已爬取队列之中，返回去重后的演员id list。<br/> </li><li> 多线程爬取演员id list之中的电影信息。<br/> </li><li> start加20循环2-7步骤。<br/> </li></ol><h2>3.2 书籍爬虫</h2><p>进入豆瓣图书标签页面，请求<a href=\"https://link.zhihu.com/?target=https%3A//book.douban.com/tag/%3Fview%3Dtype%26icn%3Dindex-sorttags-all\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">book.douban.com/tag/?</span><span class=\"invisible\">view=type&amp;icn=index-sorttags-all</span><span class=\"ellipsis\"></span></a>，利用BeautifulSoup得到所有图书标签。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-9b7871a4653690e0a78793258ffc98d5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2560\" data-rawheight=\"1600\" class=\"origin_image zh-lightbox-thumb\" width=\"2560\" data-original=\"https://pic2.zhimg.com/v2-9b7871a4653690e0a78793258ffc98d5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2560&#39; height=&#39;1600&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2560\" data-rawheight=\"1600\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2560\" data-original=\"https://pic2.zhimg.com/v2-9b7871a4653690e0a78793258ffc98d5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-9b7871a4653690e0a78793258ffc98d5_b.jpg\"/></figure><p>以小说标签为例，URL为<a href=\"https://link.zhihu.com/?target=https%3A//book.douban.com/tag/%25E5%25B0%258F%25E8%25AF%25B4%3Fstart%3D0%26type%3DT\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">https://book.douban.com/tag/小说?start=0&amp;type=T</a>，请求URL之后，利用BeautifulSoup选取相应标签，便能够拿到当前页面所有书籍id。为了确保不重复爬取相同的书籍，每拿到一个id之后，都存到redis已爬取队列之中。如果下次再遇到相同的id，则跳过不进行爬取。</p><p>同样，观察上述URL，我们只需要通过遍历start和tag便能够拿到所有书籍id。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-05f4ed38181208e697e6c5ea1abf2ffb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2560\" data-rawheight=\"1600\" class=\"origin_image zh-lightbox-thumb\" width=\"2560\" data-original=\"https://pic4.zhimg.com/v2-05f4ed38181208e697e6c5ea1abf2ffb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2560&#39; height=&#39;1600&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2560\" data-rawheight=\"1600\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2560\" data-original=\"https://pic4.zhimg.com/v2-05f4ed38181208e697e6c5ea1abf2ffb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-05f4ed38181208e697e6c5ea1abf2ffb_b.jpg\"/></figure><p>以解忧杂货店id 25862578为例，拼接<a href=\"https://link.zhihu.com/?target=https%3A//book.douban.com/subject/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">book.douban.com/subject</span><span class=\"invisible\">/</span><span class=\"ellipsis\"></span></a>后便能得到书籍URL为<a href=\"https://link.zhihu.com/?target=https%3A//book.douban.com/subject/25862578/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">book.douban.com/subject</span><span class=\"invisible\">/25862578/</span><span class=\"ellipsis\"></span></a>。然后请求书籍URL页面，通过BeautifulSoup选取相应标签，便能够拿到书籍id、图片链接、姓名、子标题、原作名称、作者、译者、出版社、出版年份、页数、价格、内容简介、目录简介、评分、评分人数信息。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-44cc89b291a658a78cd21f22ac85e6e6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2560\" data-rawheight=\"1600\" class=\"origin_image zh-lightbox-thumb\" width=\"2560\" data-original=\"https://pic3.zhimg.com/v2-44cc89b291a658a78cd21f22ac85e6e6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2560&#39; height=&#39;1600&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2560\" data-rawheight=\"1600\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2560\" data-original=\"https://pic3.zhimg.com/v2-44cc89b291a658a78cd21f22ac85e6e6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-44cc89b291a658a78cd21f22ac85e6e6_b.jpg\"/></figure><p>爬取书籍信息结束之中，将作者id单独提取出来。同样为了保证不重复爬取，每得到一个作者id，都存放到redis已爬取队列之中。</p><p>以东野圭吾为例，获取作者id 4537266，拼接<a href=\"https://link.zhihu.com/?target=https%3A//book.douban.com/author/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">book.douban.com/author/</span><span class=\"invisible\"></span></a>之后，便能得到作者URL为<a href=\"https://link.zhihu.com/?target=https%3A//book.douban.com/author/4537266/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">book.douban.com/author/</span><span class=\"invisible\">4537266/</span><span class=\"ellipsis\"></span></a>。然后请求作者URL，利用BeautifulSoup选取相应标签，便能拿到作者id，姓名、图片链接、性别、出生日期、国家、更多中文名、更多外文名、简介信息。</p><p>总结一下，获取书籍信息和书籍作者流程为</p><ol><li> 请求<a href=\"https://link.zhihu.com/?target=https%3A//book.douban.com/tag/%3Fview%3Dtype%26icn%3Dindex-sorttags-all\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">book.douban.com/tag/?</span><span class=\"invisible\">view=type&amp;icn=index-sorttags-all</span><span class=\"ellipsis\"></span></a>界面，利用BeautifulSoup获取图书所有标签。<br/> </li><li> 请求<a href=\"https://link.zhihu.com/?target=https%3A//book.douban.com/tag/%25E5%25B0%258F%25E8%25AF%25B4%3Fstart%3D0%26type%3DT\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">https://book.douban.com/tag/小说?start=0&amp;type=T</a>，利用BeautifulSoup获取20个书籍ID。如果为空，则更换书籍标签tag。<br/> </li><li> 对返回的20个书籍id存放到redis已爬取队列之中，返回去重后的书籍id list。<br/> </li><li> 多线程爬取书籍id list之中的书籍信息。<br/> </li><li> 获取书籍作者id，存放到redis已爬取队列之中，返回去重后的作者id list。<br/> </li><li> 多线程爬取演员id list之中的作者信息。<br/> </li><li> start加20循环2-7步骤。</li></ol><h2>4. 使用教程</h2><div class=\"highlight\"><pre><code class=\"language-text\">├── book\n│   ├── __init__.py\n│   ├── book_crawl.py\n│   ├── book_page_parse.py\n│   ├── book_person_page_parse.py\n│   └── book_spider_config.yaml\n├── log\n│   ├── book_log_config.yaml\n│   └── movie_log_config.yaml\n├── movie\n│   ├── __init__.py\n│   ├── movie_crawl.py\n│   ├── movie_page_parse.py\n│   ├── movie_person_page_parse.py\n│   └── movie_spider_config.yaml\n└── proxy\n    ├── get_ip.py\n    ├── ip_list.txt\n    └── ua_list.txt</code></pre></div><p>爬取之前，需要先启动redis server，然后再配置proxy中的get_ip。爬取过程中为了省事，我用的是收费的ip代理池，蘑菇代理，每三分钟请求10个ip。如果你要使用的话，可以找一些免费的ip代理工具，成功之后，将有效ip写入到ip_list之中即可。方便一点的话，可以直接购买ip代理池，同样成功后将ip写入到ip_list即可。</p><p>movie_spider_config.yaml和book_spider_config用于配制一些爬虫信息，比如超时时间和redis信息。</p><p>movie_log_config.yaml和book_log_config.yaml用于配制log信息，比如log地址和写文件格式信息。</p><p>如果你想爬取一些电影或书籍的其他信息，比如电影评论等，可以根据需求更改movie_page_parse, movie_person_page_parse, book_page_parse, book_person_page_parse的代码。</p><p>最后，配置好redis和ip代理池之后，分别启动movie_crawl和book_crawl即可。</p><h2>5.数据获取</h2><p>如果需要豆瓣电影和书籍的数据，欢迎关注公众号<b>谓之小一</b>，回复<b>豆瓣数据</b>获取下载链接。配置过程中有任何问题，欢迎在公众号后台提问，随时回答，内容转载请注明出处。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-f65a22cd2e3a45ffa2109a5dc948e167_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"990\" data-rawheight=\"1642\" class=\"origin_image zh-lightbox-thumb\" width=\"990\" data-original=\"https://pic4.zhimg.com/v2-f65a22cd2e3a45ffa2109a5dc948e167_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;990&#39; height=&#39;1642&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"990\" data-rawheight=\"1642\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"990\" data-original=\"https://pic4.zhimg.com/v2-f65a22cd2e3a45ffa2109a5dc948e167_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-f65a22cd2e3a45ffa2109a5dc948e167_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "python爬虫", 
                    "tagLink": "https://api.zhihu.com/topics/20086364"
                }, 
                {
                    "tag": "豆瓣电影", 
                    "tagLink": "https://api.zhihu.com/topics/19563237"
                }, 
                {
                    "tag": "豆瓣", 
                    "tagLink": "https://api.zhihu.com/topics/19579266"
                }
            ], 
            "comments": [
                {
                    "userName": "Z先森", 
                    "userLink": "https://www.zhihu.com/people/f08ef0d698f918e66b58b417aaeb0c03", 
                    "content": "感觉用selenium + css方便一点", 
                    "likes": 1, 
                    "childComments": []
                }, 
                {
                    "userName": "知乎用户", 
                    "userLink": "https://www.zhihu.com/people/0", 
                    "content": "你好，生成的txt数据太乱了。能不能转换成excel的？？没有那个水平。。", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "谓之小一", 
                            "userLink": "https://www.zhihu.com/people/761dda159c5cec5fd926f761ab5f4ccf", 
                            "content": "<p>你好，用python的json，xlwt库提取一下就行。</p>", 
                            "likes": 0, 
                            "replyToAuthor": "知乎用户"
                        }, 
                        {
                            "userName": "知乎用户", 
                            "userLink": "https://www.zhihu.com/people/0", 
                            "content": "不是学计算机的。。完全听不明白@-@", 
                            "likes": 0, 
                            "replyToAuthor": "谓之小一"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/57088958", 
            "userName": "谓之小一", 
            "userLink": "https://www.zhihu.com/people/761dda159c5cec5fd926f761ab5f4ccf", 
            "upvote": 0, 
            "title": "Ununtu16.04搭建GitLab服务器教程", 
            "content": "<h2>GitLab官方搭建教程地址: <a href=\"https://link.zhihu.com/?target=https%3A//about.gitlab.com/install/%23ubuntu\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">about.gitlab.com/instal</span><span class=\"invisible\">l/#ubuntu</span><span class=\"ellipsis\"></span></a></h2><h2>1.依赖包安装</h2><div class=\"highlight\"><pre><code class=\"language-text\">sudo apt-get update\nsudo apt-get install -y curl openssh-server ca-certificates</code></pre></div><p>执行完成后,邮件配置时选择Internet即可.</p><h2>2.GitLab安装</h2><h2>2.1官方教程</h2><p>如果按照官方安装方法, 直接运行下列命令即可. </p><div class=\"highlight\"><pre><code class=\"language-text\">curl https://packages.gitlab.com/install/repositories/gitlab/gitlab-ee/script.deb.sh | sudo bash\nsudo apt-get install gitlab-ce</code></pre></div><p>但按照官方教程安装会非常慢, 推荐使用下列方式, 利用清华源安装. </p><h2>2.2清华源</h2><p>首先信任GitLab的GPG公钥</p><div class=\"highlight\"><pre><code class=\"language-text\">curl https://packages.gitlab.com/gpg.key 2&gt; /dev/null | sudo apt-key add - &amp;&gt;/dev/null</code></pre></div><p>接下来打开<code>gitlab-ce.list</code>文本</p><div class=\"highlight\"><pre><code class=\"language-text\">sudo vim /etc/apt/sources.list.d/gitlab-ce.list</code></pre></div><p>然后写入如下内容</p><div class=\"highlight\"><pre><code class=\"language-text\">deb https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/ubuntu xenial main</code></pre></div><p>最后<code>:wq</code>保存退出. 文本写入成功之后, 执行下列命令安装GitLab.</p><div class=\"highlight\"><pre><code class=\"language-text\">sudo apt-get update\nsudo apt-get install gitlab-ce</code></pre></div><h2>3.GitLab启动</h2><p>安装完成之后启动GitLab</p><div class=\"highlight\"><pre><code class=\"language-text\">sudo gitlab-ctl reconfigure</code></pre></div><p>打开sshd和postfix服务</p><div class=\"highlight\"><pre><code class=\"language-text\">service sshd start\nservice postfix start</code></pre></div><p>最后输入下列命令, 检查GitLab是否成功运行. </p><div class=\"highlight\"><pre><code class=\"language-text\">sudo gitlab-ctl status</code></pre></div><p>如果成功运行, 在浏览器中输入http://127.0.0.1便可访问到GitLab界面. 首次使用时, GitLab会提示设置密码, 设置完成之后便可成功使用.</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-0a1d10809e81e2feab4df70f55a6572a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1904\" data-rawheight=\"1080\" class=\"origin_image zh-lightbox-thumb\" width=\"1904\" data-original=\"https://pic3.zhimg.com/v2-0a1d10809e81e2feab4df70f55a6572a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1904&#39; height=&#39;1080&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1904\" data-rawheight=\"1080\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1904\" data-original=\"https://pic3.zhimg.com/v2-0a1d10809e81e2feab4df70f55a6572a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-0a1d10809e81e2feab4df70f55a6572a_b.jpg\"/></figure><h2>4.GitLab配置</h2><h2>4.1更改服务器IP地址</h2><p>打开gitlab.yml文件</p><div class=\"highlight\"><pre><code class=\"language-text\">cd /opt/gitlab/embedded/service/gitlab-rails/config\nsudo vim gitlab.yml</code></pre></div><p>找到下列内容</p><div class=\"highlight\"><pre><code class=\"language-text\">gitlab:\n    ## Web server settings (note: host is the FQDN, do not include http://)\n    host: localhost\n    port: 80\n    https: false</code></pre></div><p>将localhost修改为本机IP, 例如修改为192.168.1.25.</p><div class=\"highlight\"><pre><code class=\"language-text\">gitlab:\n    ## Web server settings (note: host is the FQDN, do not include http://)\n    host: 192.168.1.25\n    port: 80\n    https: false</code></pre></div><p>修改完成并保存之后, 重启GitLab服务器. </p><div class=\"highlight\"><pre><code class=\"language-text\">sudo gitlab-ctl restart</code></pre></div><p>重启成功之后, 便可通过<a href=\"https://link.zhihu.com/?target=http%3A//192.168.1.25\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">192.168.1.25</span><span class=\"invisible\"></span></a>访问GitLab服务器.</p><h2>4.2更改服务器端口</h2><p>如果80和8080端口被其他应用占用, 比如apache, 此时就要更改GitLab服务器端口, 此处将端口改为8081.</p><p>首先打开gitlab.rb文件</p><div class=\"highlight\"><pre><code class=\"language-text\">cd /etc/gitlab\nvim gitlab.rb</code></pre></div><p>找到下列内容</p><div class=\"highlight\"><pre><code class=\"language-text\">#nginx[&#39;listen_port&#39;]=nil</code></pre></div><p>修改为</p><div class=\"highlight\"><pre><code class=\"language-text\">nginx[&#39;listen_port&#39;]=8081</code></pre></div><p>接下来重启GitLab配置</p><div class=\"highlight\"><pre><code class=\"language-text\">sudo gitlab-ctl reconfigure</code></pre></div><p>然后重启GitLab服务器</p><div class=\"highlight\"><pre><code class=\"language-text\">sudo gitlab-ctl restart</code></pre></div><p>最后便可用<a href=\"https://link.zhihu.com/?target=http%3A//192.168.1.25%3A8081\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">192.168.1.25:8081</span><span class=\"invisible\"></span></a>访问gitlab服务器.</p><h2>4.3设置GitLab开机自启动</h2><p>设置GitLab开机自启动命令为</p><div class=\"highlight\"><pre><code class=\"language-text\">sudo systemctl enable gitlab-runsvdir.service</code></pre></div><p>禁止GitLab开机自启动命令为</p><div class=\"highlight\"><pre><code class=\"language-text\">sudo systemctl disable gitlab-runsvdir.service</code></pre></div><h2>5.GitLab备份</h2><h2>5.1创建备份文件</h2><p>使用下列命令创建GitLab备份</p><div class=\"highlight\"><pre><code class=\"language-text\">sudo gitlab-rake gitlab:backup:create</code></pre></div><p>然后便会在<code>/var/opt/gitlab/backups</code>目录下创建一个类似于<b>1550415608_2019_02_17_11.5.1_gitlab_backup.tar</b>的文件. 其中开头部分是创建的日期.</p><h2>5.2修改备份目录</h2><p>首先打开gitlab.rb文件</p><div class=\"highlight\"><pre><code class=\"language-text\">cd /etc/gitlab\nvim gitlab.rb</code></pre></div><p>找到下列命令</p><div class=\"highlight\"><pre><code class=\"language-text\">gitlab_rails[&#39;backup_path&#39;] = &#34;/var/opt/gitlab/backups&#34;</code></pre></div><p>然后修改后面的地址即可. 修改完成之后重启配置文件生效.</p><div class=\"highlight\"><pre><code class=\"language-text\">sudo gitlab-ctl reconfigure</code></pre></div><h2>5.3设置自动备份机制</h2><p>手动备份过于麻烦, 所以通过crontab设置自动备份. crontab文件之中, 每一行表示一项任务, 每行的每个字段表示一项设置. crontab共6个字段, 其中前5个字段设置执行的时间段, 第6个字段设置命令.</p><div class=\"highlight\"><pre><code class=\"language-text\">m h dom mon dow user user command</code></pre></div><p>其中</p><blockquote> m： 表示分钟，可以是从0到59之间的任何整数。<br/> h：表示小时，可以是从0到23之间的任何整数。<br/> dom：表示日期，可以是从1到31之间的任何整数。<br/> mon：表示月份，可以是从1到12之间的任何整数。<br/> dow：表示星期几，可以是从0到7之间的任何整数，这里的0或7代表星期日。<br/> user : 表示执行的用户。<br/> command：要执行的命令，可以是系统命令，也可以是自己编写的脚本文件(如shell文件)。<br/> </blockquote><p>现在我们来实现每天23点自动备份GitLab文件, crontab命令如下</p><div class=\"highlight\"><pre><code class=\"language-text\">0 23 * * * /opt/gitlab/bin/gitlab-rake gitlab:backup:create CRON=1</code></pre></div><p>为保证安全, 使用<b>双备份机制</b>. 所以再加一个crontab任务, 设置每天23点1分, 将生成的gitlab文件放到外置硬盘之中, crontab命令如下</p><div class=\"highlight\"><pre><code class=\"language-text\">1 23 * * * cp -rf /var/opt/gitlab/backups/* /media/cciip/新加卷1/gitlab_backup/</code></pre></div><p>编辑完/etc/crontab文件之后, 需要重新启动crontab服务, 命令如下</p><div class=\"highlight\"><pre><code class=\"language-text\"># 重新加载cron配置文件\nsudo /usr/sbin/service cron reload\n# 重启cron服务\nsudo /usr/sbin/service cron restart</code></pre></div><p>至此, 便能进行自动备份, 而且是双备份机制.</p><h2>5.4设置备份过期时间</h2><p>GitLab每天在备份, 文件会一直增大, 所以最好设置个过期时间, 比如7天.</p><p>首先打开/etc/gitlab/gitlab.rb文件</p><div class=\"highlight\"><pre><code class=\"language-text\">cd /etc/gitlab\nsudo vim gitlab.rb</code></pre></div><p>找到下列命令</p><div class=\"highlight\"><pre><code class=\"language-text\"># gitlab_rails[&#39;backup_keep_time&#39;] = 604800</code></pre></div><p>修改为</p><div class=\"highlight\"><pre><code class=\"language-text\"># 604800 = 60*60*24*7\ngitlab_rails[&#39;backup_keep_time&#39;] = 604800</code></pre></div><p>最后重启GitLab配置文件即可.</p><div class=\"highlight\"><pre><code class=\"language-text\">sudo gitlab-ctl reconfigure</code></pre></div><h2>5.5恢复备份文件</h2><p>如果想要将GitLab服务器迁移到其他主机上, 首先确保新服务器GitLab版本和老服务器GitLab版本相同.</p><p>然后copy备份文件到新服务器上. 比如此时我把192.168.1.25服务器上的备份文件拷贝到192.168.1.24上面, 可以通过如下命令进行.</p><div class=\"highlight\"><pre><code class=\"language-text\">scp /var/opt/gitlab/backups/1550415608_2019_02_17_11.5.1_gitlab_backup.tar root@192.168.1.24:/var/opt/gitlab/backups</code></pre></div><p>然后在192.168.1.24服务器上进行如下操作</p><p>1.将备份文件权限改为777</p><div class=\"highlight\"><pre><code class=\"language-text\">chmod 777 1550415608_2019_02_17_11.5.1_gitlab_backup.tar</code></pre></div><p>2.执行命令停止相关数据连接服务</p><div class=\"highlight\"><pre><code class=\"language-text\">gitlab-ctl stop unicorn\ngitlab-ctl stop sidekiq</code></pre></div><p>3.执行命令从备份文件中恢复GitLab</p><div class=\"highlight\"><pre><code class=\"language-text\">gitlab-rake gitlab:backup:restore BACKUP=1550415608_2019_02_17_11.5.1_gitlab_backup.tar</code></pre></div><p>最后启动GitLab服务器即可</p><div class=\"highlight\"><pre><code class=\"language-text\">sudo gitlab-ctl start</code></pre></div><p>现在便可通过<a href=\"https://link.zhihu.com/?target=http%3A//192.168.1.24%3A8081\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">192.168.1.24:8081</span><span class=\"invisible\"></span></a>访问GitLab服务器.</p>", 
            "topic": [
                {
                    "tag": "Git", 
                    "tagLink": "https://api.zhihu.com/topics/19557710"
                }, 
                {
                    "tag": "Ubuntu", 
                    "tagLink": "https://api.zhihu.com/topics/19557067"
                }, 
                {
                    "tag": "服务器", 
                    "tagLink": "https://api.zhihu.com/topics/19554575"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/56109450", 
            "userName": "谓之小一", 
            "userLink": "https://www.zhihu.com/people/761dda159c5cec5fd926f761ab5f4ccf", 
            "upvote": 7, 
            "title": "详解准确率、精确率、召回率、F1值等评价指标的含义", 
            "content": "<p>机器学习问题之中，通常需要建立模型来解决具体问题，但对于模型的好坏，也就是模型的泛化能力，如何进行评估呢？</p><p>很简单，我们可以定一些评价指标，来度量模型的优劣。比如准确率、精确率、召回率、F1值、ROC、AUC等指标，但是你清楚这些指标的具体含义吗？下面我们一起来看看吧。</p><h2>1.混淆矩阵</h2><p>介绍各个指标之前，我们先来了解一下混淆矩阵。假如现在有一个二分类问题，那么预测结果和实际结果两两结合会出现如下四种情况。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-15f4924b8125fae5ee91cb4d5d2ff642_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"762\" data-rawheight=\"196\" class=\"origin_image zh-lightbox-thumb\" width=\"762\" data-original=\"https://pic3.zhimg.com/v2-15f4924b8125fae5ee91cb4d5d2ff642_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;762&#39; height=&#39;196&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"762\" data-rawheight=\"196\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"762\" data-original=\"https://pic3.zhimg.com/v2-15f4924b8125fae5ee91cb4d5d2ff642_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-15f4924b8125fae5ee91cb4d5d2ff642_b.jpg\"/></figure><p>由于用数字1、0表示不太方便阅读，我们转换一下，用<b>T(True)代表正确</b>、<b>F(False)代表错误</b>、<b>P(Positive)代表1</b>、<b>N(Negative)代表0</b>。<b>先看预测结果(P|N)，然后再针对实际结果对比预测结果，给出判断结果(T|F)</b>。按照上面逻辑，重新分配后为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-3cc6707951f93d72bcf1918594598898_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"762\" data-rawheight=\"190\" class=\"origin_image zh-lightbox-thumb\" width=\"762\" data-original=\"https://pic1.zhimg.com/v2-3cc6707951f93d72bcf1918594598898_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;762&#39; height=&#39;190&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"762\" data-rawheight=\"190\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"762\" data-original=\"https://pic1.zhimg.com/v2-3cc6707951f93d72bcf1918594598898_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-3cc6707951f93d72bcf1918594598898_b.jpg\"/></figure><p>TP、FP、FN、TN可以理解为</p><ul><li>TP：预测为1，实际为1，预测正确。</li><li>FP：预测为1，实际为0，预测错误。</li><li>FN：预测为0，实际为1，预测错误。</li><li>TN：预测为0，实际为0，预测正确。</li></ul><h2>2.准确率</h2><p>首先给出<b>准确率(Accuracy)</b>的定义，即<b>预测正确的结果占总样本的百分比</b>，表达式为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-44fe8ef399b877ed14e1b8717bb7cf04_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1556\" data-rawheight=\"130\" class=\"origin_image zh-lightbox-thumb\" width=\"1556\" data-original=\"https://pic1.zhimg.com/v2-44fe8ef399b877ed14e1b8717bb7cf04_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1556&#39; height=&#39;130&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1556\" data-rawheight=\"130\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1556\" data-original=\"https://pic1.zhimg.com/v2-44fe8ef399b877ed14e1b8717bb7cf04_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-44fe8ef399b877ed14e1b8717bb7cf04_b.png\"/></figure><p>虽然准确率能够判断总的正确率，但是在<b>样本不均衡</b>的情况下，并不能作为很好的指标来衡量结果。</p><p>比如在样本集中，正样本有90个，负样本有10个，样本是严重的不均衡。对于这种情况，我们只需要将全部样本预测为正样本，就能得到90%的准确率，但是完全没有意义。对于新数据，完全体现不出准确率。因此，在样本不平衡的情况下，得到的高准确率没有任何意义，此时准确率就会失效。所以，我们需要寻找新的指标来评价模型的优劣。</p><h2>3.精确率</h2><p><b>精确率(Precision)</b>是针对预测结果而言的，其含义是<b>在被所有预测为正的样本中实际为正样本的概率</b>，表达式为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-801a7f9a01000e75e6c050167f3119b1_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1570\" data-rawheight=\"134\" class=\"origin_image zh-lightbox-thumb\" width=\"1570\" data-original=\"https://pic2.zhimg.com/v2-801a7f9a01000e75e6c050167f3119b1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1570&#39; height=&#39;134&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1570\" data-rawheight=\"134\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1570\" data-original=\"https://pic2.zhimg.com/v2-801a7f9a01000e75e6c050167f3119b1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-801a7f9a01000e75e6c050167f3119b1_b.png\"/></figure><p>精确率和准确率看上去有些类似，但是是两个完全不同的概念。精确率代表对正样本结果中的预测准确程度，准确率则代表整体的预测准确程度，包括正样本和负样本。</p><h2>4.召回率</h2><p><b>召回率(Recall)</b>是针对原样本而言的，其含义是<b>在实际为正的样本中被预测为正样本的概率</b>，表达式为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-a58d554ab1bd94d22a5e7da0c53f9af0_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1574\" data-rawheight=\"124\" class=\"origin_image zh-lightbox-thumb\" width=\"1574\" data-original=\"https://pic1.zhimg.com/v2-a58d554ab1bd94d22a5e7da0c53f9af0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1574&#39; height=&#39;124&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1574\" data-rawheight=\"124\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1574\" data-original=\"https://pic1.zhimg.com/v2-a58d554ab1bd94d22a5e7da0c53f9af0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-a58d554ab1bd94d22a5e7da0c53f9af0_b.png\"/></figure><p>下面我们通过一个简单例子来看看精确率和召回率。假设一共有10篇文章，里面4篇是你要找的。根据你的算法模型，你找到了5篇，但实际上在这5篇之中，只有3篇是你真正要找的。</p><p>那么算法的精确率是3/5=60%，也就是你找的这5篇，有3篇是真正对的。算法的召回率是3/4=75%，也就是需要找的4篇文章，你找到了其中三篇。以精确率还是以召回率作为评价指标，需要根据具体问题而定。</p><h2>5.F1分数</h2><p>精确率和召回率又被叫做查准率和查全率，可以通过P-R图进行表示</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-183ff7425a0dfd08870fe337116d47f6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"842\" data-rawheight=\"511\" class=\"origin_image zh-lightbox-thumb\" width=\"842\" data-original=\"https://pic3.zhimg.com/v2-183ff7425a0dfd08870fe337116d47f6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;842&#39; height=&#39;511&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"842\" data-rawheight=\"511\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"842\" data-original=\"https://pic3.zhimg.com/v2-183ff7425a0dfd08870fe337116d47f6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-183ff7425a0dfd08870fe337116d47f6_b.jpg\"/></figure><p>如何理解<b>P-R(精确率-召回率)曲线</b>呢？或者说这些曲线是根据什么变化呢？</p><p>以逻辑回归举例，其输出值是0-1之间的数字。因此，如果我们想要判断用户的好坏，那么就必须定一个阈值。比如大于0.5指定为好用户，小于0.5指定为坏用户，然后就可以得到相应的精确率和召回率。但问题是，这个阈值是我们随便定义的，并不知道这个阈值是否符合我们的要求。因此为了寻找一个合适的阈值，我们就需要<b>遍历0-1之间所有的阈值</b>，而每个阈值都对应一个精确率和召回率，从而就能够得到上述曲线。</p><p>根据上述的P-R曲线，怎么判断最好的阈值点呢？首先我们先明确目标，我们<b>希望精确率和召回率都很高</b>，但实际上是矛盾的，上述两个指标是矛盾体，无法做到双高。因此，选择合适的阈值点，就需要根据实际问题需求，比如我们想要很高的精确率，就要牺牲掉一些召回率。想要得到很高的召回率，就要牺牲掉一些精准率。但通常情况下，我们可以根据他们之间的平衡点，定义一个新的指标：<b>F1分数(F1-Score)</b>。F1分数同时考虑精确率和召回率，让两者同时达到最高，取得平衡。F1分数表达式为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-2267879a9c4a8f38cceee7eff938c64d_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1562\" data-rawheight=\"134\" class=\"origin_image zh-lightbox-thumb\" width=\"1562\" data-original=\"https://pic2.zhimg.com/v2-2267879a9c4a8f38cceee7eff938c64d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1562&#39; height=&#39;134&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1562\" data-rawheight=\"134\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1562\" data-original=\"https://pic2.zhimg.com/v2-2267879a9c4a8f38cceee7eff938c64d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-2267879a9c4a8f38cceee7eff938c64d_b.png\"/></figure><p>上图P-R曲线中，平衡点就是F1值的分数。</p><h2>6.Roc、AUC曲线</h2><p>正式介绍ROC和AUC之前，还需要再介绍两个指标，<b>真正率(TPR)和假正率(FPR)</b>。</p><ul><li>真正率(TPR) = 灵敏度(Sensitivity) = <b>TP/(TP+FN)</b></li><li>假正率(FPR) = 1-特异度(Specificity) = <b>FP/(FP+TN)</b></li></ul><p>TPR和FPR分别是基于实际表现1、0出发的，也就是说在实际的正样本和负样本中来观察相关概率问题。因此，无论样本是否均衡，都不会被影响。</p><p>继续用上面例子，总样本中有90%的正样本，10%的负样本。TPR能够得到90%正样本中有多少是被真正覆盖的，而与那10%无关。同理FPR能够得到10%负样本中有多少是被覆盖的，而与那90%无关。因此我们从实际表现的各个结果出发，就能避免样本不平衡的问题，这就是为什么用TPR和FPR作为ROC、AUC指标的原因。</p><h2>6.1 ROC</h2><p>ROC曲线图如下所示，其中横坐标为假正率(FPR)，纵坐标为真正率(TPR)。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-3f23a2d379e0b740dea3c181b201e7f4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"674\" data-rawheight=\"528\" class=\"origin_image zh-lightbox-thumb\" width=\"674\" data-original=\"https://pic1.zhimg.com/v2-3f23a2d379e0b740dea3c181b201e7f4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;674&#39; height=&#39;528&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"674\" data-rawheight=\"528\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"674\" data-original=\"https://pic1.zhimg.com/v2-3f23a2d379e0b740dea3c181b201e7f4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-3f23a2d379e0b740dea3c181b201e7f4_b.jpg\"/></figure><p>与前面的P-R曲线类似，ROC曲线也是通过遍历所有阈值来绘制曲线的。如果我们不断的遍历所有阈值，预测的正样本和负样本是在不断变化的，相应的ROC曲线TPR和FPR也会沿着曲线滑动。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-7ea3e127c421f354cad3ba4d66203de8_b.gif\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"600\" data-rawheight=\"194\" data-thumbnail=\"https://pic1.zhimg.com/v2-7ea3e127c421f354cad3ba4d66203de8_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"600\" data-original=\"https://pic1.zhimg.com/v2-7ea3e127c421f354cad3ba4d66203de8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;600&#39; height=&#39;194&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"600\" data-rawheight=\"194\" data-thumbnail=\"https://pic1.zhimg.com/v2-7ea3e127c421f354cad3ba4d66203de8_b.jpg\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"600\" data-original=\"https://pic1.zhimg.com/v2-7ea3e127c421f354cad3ba4d66203de8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-7ea3e127c421f354cad3ba4d66203de8_b.gif\"/></figure><p>同时，我们也会思考，如何判断ROC曲线的好坏呢？我们来看，FPR表示模型虚报的程度，TPR表示模型预测覆盖的程度。理所当然的，我们希望虚报的越少越好，覆盖的越多越好。所以TPR越高，同时FPR越低，也就是ROC曲线越陡，那么模型的性能也就越好。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-20f8fdf8942f86de6acd119673fab2af_b.gif\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"600\" data-rawheight=\"194\" data-thumbnail=\"https://pic4.zhimg.com/v2-20f8fdf8942f86de6acd119673fab2af_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"600\" data-original=\"https://pic4.zhimg.com/v2-20f8fdf8942f86de6acd119673fab2af_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;600&#39; height=&#39;194&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"600\" data-rawheight=\"194\" data-thumbnail=\"https://pic4.zhimg.com/v2-20f8fdf8942f86de6acd119673fab2af_b.jpg\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"600\" data-original=\"https://pic4.zhimg.com/v2-20f8fdf8942f86de6acd119673fab2af_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-20f8fdf8942f86de6acd119673fab2af_b.gif\"/></figure><p>最后，我们来看一下，不论样本比例如何改变，ROC曲线都没有影响，也就是ROC曲线无视样本间的不平衡问题。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-173a442fc895c0d3b24146333549c22b_b.gif\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"600\" data-rawheight=\"392\" data-thumbnail=\"https://pic4.zhimg.com/v2-173a442fc895c0d3b24146333549c22b_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"600\" data-original=\"https://pic4.zhimg.com/v2-173a442fc895c0d3b24146333549c22b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;600&#39; height=&#39;392&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"600\" data-rawheight=\"392\" data-thumbnail=\"https://pic4.zhimg.com/v2-173a442fc895c0d3b24146333549c22b_b.jpg\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"600\" data-original=\"https://pic4.zhimg.com/v2-173a442fc895c0d3b24146333549c22b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-173a442fc895c0d3b24146333549c22b_b.gif\"/></figure><h2>6.2 AUC</h2><p><b>AUC(Area Under Curve)</b>表示ROC中曲线下的面积，用于判断模型的优劣。如ROC曲线所示，连接对角线的面积刚好是0.5，对角线的含义也就是随机判断预测结果，正负样本覆盖应该都是50%。另外，ROC曲线越陡越好，所以理想值是1，即正方形。所以AUC的值一般是介于0.5和1之间的。AUC评判标准可参考如下</p><ul><li>0.5-0.7：效果较低。</li><li>0.7-0.85：效果一般。</li><li>0.85-0.95：效果很好。</li><li>0.95-1：效果非常好。</li></ul><h2>7.推广</h2><p>更多内容请关注公众号<b>谓之小一</b>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-f65a22cd2e3a45ffa2109a5dc948e167_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"990\" data-rawheight=\"1642\" class=\"origin_image zh-lightbox-thumb\" width=\"990\" data-original=\"https://pic4.zhimg.com/v2-f65a22cd2e3a45ffa2109a5dc948e167_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;990&#39; height=&#39;1642&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"990\" data-rawheight=\"1642\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"990\" data-original=\"https://pic4.zhimg.com/v2-f65a22cd2e3a45ffa2109a5dc948e167_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-f65a22cd2e3a45ffa2109a5dc948e167_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "召回率", 
                    "tagLink": "https://api.zhihu.com/topics/20683691"
                }, 
                {
                    "tag": "Roc 曲线下面积", 
                    "tagLink": "https://api.zhihu.com/topics/20687816"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/56035667", 
            "userName": "谓之小一", 
            "userLink": "https://www.zhihu.com/people/761dda159c5cec5fd926f761ab5f4ccf", 
            "upvote": 6, 
            "title": "机器学习之交叉验证", 
            "content": "<h2>1.交叉验证简介</h2><p><b>交叉验证(Cross Validation)</b>是在机器学习建立模型和验证模型参数时常用的方法。顾名思义，就是重复的使用数据，把得到的样本数据进行切分，组合为不同的训练集和测试集。用训练集来训练模型，测试集来评估模型的好坏。在此基础上可以得到多组不同的训练集和测试集，某次训练集中的样本，在下次可能成为测试集中的样本，也就是所谓的<b>交叉</b>。</p><h2>2.为什么用交叉验证？</h2><ul><li>交叉验证用在数据量不是很充足的情况(比如数据量小于一万条)，能够从有限的数据中获取尽可能多的有效信息。</li><li>交叉验证用于评估模型的预测性能，尤其是训练好的模型在新数据上的表现，能够一定程度上减小过拟合。</li></ul><h2>3.交叉验证方法</h2><h2>3.1 留出法交叉验证</h2><p>留出法<b>(Hold-Out Cross Validation)</b>是一种简单交叉验证，即针对原始数据集，通常分为<b>训练集、测试集</b>。训练集用于训练模型、测试集对于模型来说是未知数据，用于评估模型的泛化能力。</p><p>比如我们随机的将样本数据分为两部分(70%的训练集，30%的测试集)，然后用训练集来训练模型，测试集上验证模型及参数，最后选择损失函数评估最优的模型和参数。　</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">from</span> <span class=\"nn\">sklearn</span> <span class=\"kn\">import</span> <span class=\"n\">datasets</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn</span> <span class=\"kn\">import</span> <span class=\"n\">model_selection</span>\n\n<span class=\"c1\">#引入sklearn库中手写数字的数据集</span>\n<span class=\"n\">digits</span> <span class=\"o\">=</span> <span class=\"n\">datasets</span><span class=\"o\">.</span><span class=\"n\">load_digits</span><span class=\"p\">()</span>\n\n<span class=\"c1\">#留出法</span>\n<span class=\"n\">X_train</span><span class=\"p\">,</span> <span class=\"n\">X_test</span><span class=\"p\">,</span> <span class=\"n\">y_train</span><span class=\"p\">,</span> <span class=\"n\">y_test</span> <span class=\"o\">=</span> <span class=\"n\">model_selection</span><span class=\"o\">.</span><span class=\"n\">train_test_split</span><span class=\"p\">(</span><span class=\"n\">digits</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">digits</span><span class=\"o\">.</span><span class=\"n\">target</span><span class=\"p\">,</span> <span class=\"n\">test_size</span> <span class=\"o\">=</span> <span class=\"mf\">0.3</span><span class=\"p\">,</span> <span class=\"n\">shuffle</span> <span class=\"o\">=</span> <span class=\"bp\">True</span><span class=\"p\">)</span></code></pre></div><h2>3.2 k折交叉验证</h2><p>k折交叉验证<b>(k-fold Cross Validation)</b>过程如下所示：</p><ol><li> 不重复抽样将原始数据随机分成k份。<br/> </li><li> 每次挑选其中1份作为测试集，剩余k-1份作为训练集用于训练模型。<br/> </li><li> 重复第2步k次，在每个训练集上训练后得到一个模型。用这个模型在相应的测试集上测试，计算并保存模型的评估指标。<br/> </li><li>计算k组测试结果的平均值作为模型准确度的估计，并作为当前k折交叉验证下模型的性能指标。</li></ol><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-1130f8ac73ae43ec07d6a29d5d43e3cf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"650\" data-rawheight=\"313\" class=\"origin_image zh-lightbox-thumb\" width=\"650\" data-original=\"https://pic4.zhimg.com/v2-1130f8ac73ae43ec07d6a29d5d43e3cf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;650&#39; height=&#39;313&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"650\" data-rawheight=\"313\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"650\" data-original=\"https://pic4.zhimg.com/v2-1130f8ac73ae43ec07d6a29d5d43e3cf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-1130f8ac73ae43ec07d6a29d5d43e3cf_b.jpg\"/></figure><p>k一般取10，数据量大的时候，k可以设置的小一些。数据量小的时候，k可以设置的大一些，这样训练集占整体数据的比例就比较大，不过同时训练的模型个数也就相应增加。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">from</span> <span class=\"nn\">sklearn</span> <span class=\"kn\">import</span> <span class=\"n\">datasets</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn</span> <span class=\"kn\">import</span> <span class=\"n\">model_selection</span>\n\n<span class=\"c1\">#引入sklearn库中手写数字的数据集</span>\n<span class=\"n\">digits</span> <span class=\"o\">=</span> <span class=\"n\">datasets</span><span class=\"o\">.</span><span class=\"n\">load_digits</span><span class=\"p\">()</span>\n\n<span class=\"c1\">#K折交叉验证</span>\n<span class=\"c1\">#设置K为5</span>\n<span class=\"n\">kf</span> <span class=\"o\">=</span> <span class=\"n\">model_selection</span><span class=\"o\">.</span><span class=\"n\">KFold</span><span class=\"p\">(</span><span class=\"n\">n_splits</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">)</span>\n<span class=\"c1\">#使用5折交叉验验证划分数据集，返回一个生成器对象（即索引）</span>\n<span class=\"n\">digits_gen</span> <span class=\"o\">=</span> <span class=\"n\">kf</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">(</span><span class=\"n\">digits</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">)</span>\n<span class=\"k\">for</span> <span class=\"n\">train_idx</span><span class=\"p\">,</span> <span class=\"n\">test_idx</span> <span class=\"ow\">in</span> <span class=\"n\">digits_gen</span><span class=\"p\">:</span>\n        <span class=\"n\">X_train</span> <span class=\"o\">=</span> <span class=\"n\">digits</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">[</span><span class=\"n\">train_idx</span><span class=\"p\">]</span> <span class=\"c1\">#训练集</span>\n        <span class=\"n\">X_test</span> <span class=\"o\">=</span> <span class=\"n\">digits</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">[</span><span class=\"n\">test_idx</span><span class=\"p\">]</span> <span class=\"c1\">#测试集</span>\n        <span class=\"n\">y_train</span> <span class=\"o\">=</span> <span class=\"n\">digits</span><span class=\"o\">.</span><span class=\"n\">target</span><span class=\"p\">[</span><span class=\"n\">train_idx</span><span class=\"p\">]</span> <span class=\"c1\">#训练集标签</span>\n        <span class=\"n\">y_test</span> <span class=\"o\">=</span> <span class=\"n\">digits</span><span class=\"o\">.</span><span class=\"n\">target</span><span class=\"p\">[</span><span class=\"n\">test_idx</span><span class=\"p\">]</span> <span class=\"c1\">#测试及标签</span></code></pre></div><h2>3.3 留一法交叉验证</h2><p>留一法交叉验证<b>(Leave-one-out Cross Validation)</b>是k折交叉验证的特例，此时的k等于样本数N。因此，对于N个样本，每次选择N-1个样本来训练数据，留一个样本来验证模型的好坏。此方法主要适用于数据量非常小的情况，比如N小于50的时候，推荐采用留一交叉验证。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">from</span> <span class=\"nn\">sklearn</span> <span class=\"kn\">import</span> <span class=\"n\">datasets</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn</span> <span class=\"kn\">import</span> <span class=\"n\">model_selection</span>\n\n<span class=\"c1\">#引入sklearn库中手写数字的数据集</span>\n<span class=\"n\">digits</span> <span class=\"o\">=</span> <span class=\"n\">datasets</span><span class=\"o\">.</span><span class=\"n\">load_digits</span><span class=\"p\">()</span>\n\n<span class=\"c1\">#留一法交叉验证</span>\n<span class=\"n\">loo</span> <span class=\"o\">=</span> <span class=\"n\">model_selection</span><span class=\"o\">.</span><span class=\"n\">LeaveOneOut</span><span class=\"p\">()</span>\n<span class=\"n\">digits_gen</span> <span class=\"o\">=</span> <span class=\"n\">loo</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">(</span><span class=\"n\">digits</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">)</span>\n<span class=\"k\">for</span> <span class=\"n\">train_idx</span><span class=\"p\">,</span> <span class=\"n\">test_idx</span> <span class=\"ow\">in</span> <span class=\"n\">digits_gen</span><span class=\"p\">:</span>\n        <span class=\"n\">X_train</span> <span class=\"o\">=</span> <span class=\"n\">digits</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">[</span><span class=\"n\">train_idx</span><span class=\"p\">]</span>\n        <span class=\"n\">X_test</span> <span class=\"o\">=</span> <span class=\"n\">digits</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">[</span><span class=\"n\">test_idx</span><span class=\"p\">]</span>\n        <span class=\"n\">y_train</span> <span class=\"o\">=</span> <span class=\"n\">digits</span><span class=\"o\">.</span><span class=\"n\">target</span><span class=\"p\">[</span><span class=\"n\">train_idx</span><span class=\"p\">]</span>\n        <span class=\"n\">y_test</span> <span class=\"o\">=</span> <span class=\"n\">digits</span><span class=\"o\">.</span><span class=\"n\">target</span><span class=\"p\">[</span><span class=\"n\">test_idx</span><span class=\"p\">]</span></code></pre></div><h2>3.4 交叉验证方法选择</h2><p>那这三种情况，到底应该选择哪一种方法呢？其实很简单，如果我们只是对数据做一个初步的模型建立，不是要做深入分析的话，简单交叉验证就可以。否则就用k折交叉验证。在样本量少的时候，使用留一交叉验证。</p><h2>4.推广</h2><p>更多内容请关注公众号<b>谓之小一</b>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-f65a22cd2e3a45ffa2109a5dc948e167_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"990\" data-rawheight=\"1642\" class=\"origin_image zh-lightbox-thumb\" width=\"990\" data-original=\"https://pic4.zhimg.com/v2-f65a22cd2e3a45ffa2109a5dc948e167_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;990&#39; height=&#39;1642&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"990\" data-rawheight=\"1642\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"990\" data-original=\"https://pic4.zhimg.com/v2-f65a22cd2e3a45ffa2109a5dc948e167_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-f65a22cd2e3a45ffa2109a5dc948e167_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "交叉验证", 
                    "tagLink": "https://api.zhihu.com/topics/20683689"
                }, 
                {
                    "tag": "k 折交叉验证", 
                    "tagLink": "https://api.zhihu.com/topics/20688061"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/55967317", 
            "userName": "谓之小一", 
            "userLink": "https://www.zhihu.com/people/761dda159c5cec5fd926f761ab5f4ccf", 
            "upvote": 0, 
            "title": "机器学习降维之线性判别模型(LDA)", 
            "content": "<blockquote>由于较多公式，所以我将部分内容转为图片进行上传，请见谅。清晰版请访问<a href=\"https://link.zhihu.com/?target=https%3A//weizhixiaoyi.com\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">weizhixiaoyi.com</span><span class=\"invisible\"></span></a>查看，你也可以关注我公众号<b>谓之小一</b>，后台直接向我要pdf版本，如有相关问题可公众号后台询问，随时回答。</blockquote><h2>1.LDA简介</h2><p><b>线性判别分析(Linear Discriminant Analysis, LDA)</b>是一种监督学习的降维方法，也就是说数据集的每个样本是有类别输出。和之前介绍的<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3MjA2NTQzMw%3D%3D%26mid%3D2247484027%26idx%3D1%26sn%3D245a237eed7fa6ec4d6db08168c4e889%26chksm%3Dfcd7d18dcba0589b2ab1289bd7b9cec3a2b265f1d17cb2fb1f109437a1f53285cdab75e997b4%26token%3D1198916416%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">机器学习降维之主成分分析(PCA)</a>方法不同，PCA是不考虑样本类别输出的无监督学习方法。LDA的原理简单来说就是将带上标签的数据（点），通过投影的方法，投影到维度更低的空间中，使得投影后的点会形成按类别区分。而我们的目标就是使得投影后的数据，<b>类间方差最大，类内方差最小</b>。</p><p>以下图为例，假设有两类数据，分别为红色和蓝色。现在我们希望，将这些数据投影到一维的直线上，让每一种类别数据的投影点尽可能的接近，而红色和蓝色数据中心之间的距离尽可能的大。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-5e0825c6d993b830928805b81a132852_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"963\" data-rawheight=\"372\" class=\"origin_image zh-lightbox-thumb\" width=\"963\" data-original=\"https://pic3.zhimg.com/v2-5e0825c6d993b830928805b81a132852_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;963&#39; height=&#39;372&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"963\" data-rawheight=\"372\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"963\" data-original=\"https://pic3.zhimg.com/v2-5e0825c6d993b830928805b81a132852_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-5e0825c6d993b830928805b81a132852_b.jpg\"/></figure><p>从上图的两种投影方式能够看出，右图能够更好的满足我们的目标，即<b>类间方差最大，类内方差最小</b>。下面我们来看看LDA内部原理，如何达到我们所希望的目标。</p><h2>2.瑞利商和广义瑞利商</h2><p>介绍LDA原理之前，我们先了解一些数学知识，即<b>瑞利商(Rayleigh quotient)</b>与<b>广义瑞利商(genralized Rayleigh quotient)</b>。首先来看看瑞利商的函数R(A,x)</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-d2ffe12345bc0032058f58ac4ffd378f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1584\" data-rawheight=\"264\" class=\"origin_image zh-lightbox-thumb\" width=\"1584\" data-original=\"https://pic4.zhimg.com/v2-d2ffe12345bc0032058f58ac4ffd378f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1584&#39; height=&#39;264&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1584\" data-rawheight=\"264\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1584\" data-original=\"https://pic4.zhimg.com/v2-d2ffe12345bc0032058f58ac4ffd378f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-d2ffe12345bc0032058f58ac4ffd378f_b.jpg\"/></figure><p>瑞利商R(A,x)有一个非常重要的性质，即它的最大值等于矩阵A的最大特征值，而最小值等于矩阵A的最小特征值，即满足</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-39397604398b06f6962216124089ac46_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1596\" data-rawheight=\"148\" class=\"origin_image zh-lightbox-thumb\" width=\"1596\" data-original=\"https://pic3.zhimg.com/v2-39397604398b06f6962216124089ac46_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1596&#39; height=&#39;148&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1596\" data-rawheight=\"148\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1596\" data-original=\"https://pic3.zhimg.com/v2-39397604398b06f6962216124089ac46_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-39397604398b06f6962216124089ac46_b.png\"/></figure><p>以上就是瑞利商的内容，现在看看广义瑞利商内容，广义瑞利商函数R(A,B,x)</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-9921f4a026dfff122d81929a0f641fc8_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1584\" data-rawheight=\"126\" class=\"origin_image zh-lightbox-thumb\" width=\"1584\" data-original=\"https://pic1.zhimg.com/v2-9921f4a026dfff122d81929a0f641fc8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1584&#39; height=&#39;126&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1584\" data-rawheight=\"126\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1584\" data-original=\"https://pic1.zhimg.com/v2-9921f4a026dfff122d81929a0f641fc8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-9921f4a026dfff122d81929a0f641fc8_b.png\"/></figure><p>其中x为非零向量，而A,B为n*n的Hermitan矩阵，B是正定矩阵。那么R(A,B,x)的最大值和最小值是什么呢？</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-83418070e4766359c4bb2af7de9899c2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1624\" data-rawheight=\"740\" class=\"origin_image zh-lightbox-thumb\" width=\"1624\" data-original=\"https://pic3.zhimg.com/v2-83418070e4766359c4bb2af7de9899c2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1624&#39; height=&#39;740&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1624\" data-rawheight=\"740\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1624\" data-original=\"https://pic3.zhimg.com/v2-83418070e4766359c4bb2af7de9899c2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-83418070e4766359c4bb2af7de9899c2_b.jpg\"/></figure><h2>3.二类LDA原理</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-6d85af6d0a4fb9795fd5a62f3b3ecb0e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1610\" data-rawheight=\"430\" class=\"origin_image zh-lightbox-thumb\" width=\"1610\" data-original=\"https://pic3.zhimg.com/v2-6d85af6d0a4fb9795fd5a62f3b3ecb0e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1610&#39; height=&#39;430&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1610\" data-rawheight=\"430\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1610\" data-original=\"https://pic3.zhimg.com/v2-6d85af6d0a4fb9795fd5a62f3b3ecb0e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-6d85af6d0a4fb9795fd5a62f3b3ecb0e_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-4dae62f74a70ad6ffaabe35e27238be2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1636\" data-rawheight=\"992\" class=\"origin_image zh-lightbox-thumb\" width=\"1636\" data-original=\"https://pic3.zhimg.com/v2-4dae62f74a70ad6ffaabe35e27238be2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1636&#39; height=&#39;992&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1636\" data-rawheight=\"992\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1636\" data-original=\"https://pic3.zhimg.com/v2-4dae62f74a70ad6ffaabe35e27238be2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-4dae62f74a70ad6ffaabe35e27238be2_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-cc226704cb7449f40b0964acc5cc9dc6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1610\" data-rawheight=\"652\" class=\"origin_image zh-lightbox-thumb\" width=\"1610\" data-original=\"https://pic3.zhimg.com/v2-cc226704cb7449f40b0964acc5cc9dc6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1610&#39; height=&#39;652&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1610\" data-rawheight=\"652\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1610\" data-original=\"https://pic3.zhimg.com/v2-cc226704cb7449f40b0964acc5cc9dc6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-cc226704cb7449f40b0964acc5cc9dc6_b.jpg\"/></figure><h2>4.多类LDA原理</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ab7f59057510e4b1926453ae07e05e8b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1624\" data-rawheight=\"622\" class=\"origin_image zh-lightbox-thumb\" width=\"1624\" data-original=\"https://pic4.zhimg.com/v2-ab7f59057510e4b1926453ae07e05e8b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1624&#39; height=&#39;622&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1624\" data-rawheight=\"622\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1624\" data-original=\"https://pic4.zhimg.com/v2-ab7f59057510e4b1926453ae07e05e8b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-ab7f59057510e4b1926453ae07e05e8b_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-9e3791f8dd6bc70544ce6ddf1f172565_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1630\" data-rawheight=\"840\" class=\"origin_image zh-lightbox-thumb\" width=\"1630\" data-original=\"https://pic2.zhimg.com/v2-9e3791f8dd6bc70544ce6ddf1f172565_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1630&#39; height=&#39;840&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1630\" data-rawheight=\"840\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1630\" data-original=\"https://pic2.zhimg.com/v2-9e3791f8dd6bc70544ce6ddf1f172565_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-9e3791f8dd6bc70544ce6ddf1f172565_b.jpg\"/></figure><h2>5.LDA算法流程</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-80bc55600bf084489e4f4f852f1e3cb7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1642\" data-rawheight=\"682\" class=\"origin_image zh-lightbox-thumb\" width=\"1642\" data-original=\"https://pic4.zhimg.com/v2-80bc55600bf084489e4f4f852f1e3cb7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1642&#39; height=&#39;682&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1642\" data-rawheight=\"682\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1642\" data-original=\"https://pic4.zhimg.com/v2-80bc55600bf084489e4f4f852f1e3cb7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-80bc55600bf084489e4f4f852f1e3cb7_b.jpg\"/></figure><h2>6.LDA vs PCA</h2><p>LDA和PCA有很多相同点和不同点，我们来对比看看两者的区别。</p><p><b>相同点</b></p><ul><li>两者均可对数据进行降维。</li><li>两者在降维时均使用了矩阵特征分解的思想。</li><li>两者都假设数据符合高斯分布。</li></ul><p><b>不同点</b></p><ul><li>LDA是有监督的降维方法，而PCA是无监督的降维方法。</li><li>LDA降维最多降到类别数k-1的维数，而PCA无此限制。</li><li>LDA选择分类性能最好的投影方向，而PCA选择样本点投影具有最大方差的方向。</li></ul><p>不同数据情况下，LDA和PCA降维方法各有优劣。例如某些数据情况下LDA比PCA方法更好</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-58c385b422d28b203ff8ae4d165c1bb3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"554\" data-rawheight=\"421\" class=\"origin_image zh-lightbox-thumb\" width=\"554\" data-original=\"https://pic4.zhimg.com/v2-58c385b422d28b203ff8ae4d165c1bb3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;554&#39; height=&#39;421&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"554\" data-rawheight=\"421\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"554\" data-original=\"https://pic4.zhimg.com/v2-58c385b422d28b203ff8ae4d165c1bb3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-58c385b422d28b203ff8ae4d165c1bb3_b.jpg\"/></figure><p>某些数据情况下PCA比LDA方法降维更好</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-6c3e942d4e63559a9029bb9df6dc5185_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"407\" data-rawheight=\"331\" class=\"content_image\" width=\"407\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;407&#39; height=&#39;331&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"407\" data-rawheight=\"331\" class=\"content_image lazy\" width=\"407\" data-actualsrc=\"https://pic2.zhimg.com/v2-6c3e942d4e63559a9029bb9df6dc5185_b.jpg\"/></figure><h2>7.LDA算法总结</h2><p><b>LDA优点</b></p><ul><li>在降维过程中可以使用类别的先验知识经验，而PCA这种无监督学习则无法使用类别先验知识。</li></ul><p><b>LDA缺点</b></p><ul><li> LDA可能过度拟合数据。<br/> </li><li> LDA不适合对非高斯分布样本进行降维，PCA也有这个问题。<br/> </li><li>LDA降维最多降到类别数k-1的维数，如果我们降维的维数大于k-1，则不能使用LDA。</li></ul><h2>8.推广</h2><p>更多内容请关注公众号<b>谓之小一</b>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-f65a22cd2e3a45ffa2109a5dc948e167_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"990\" data-rawheight=\"1642\" class=\"origin_image zh-lightbox-thumb\" width=\"990\" data-original=\"https://pic4.zhimg.com/v2-f65a22cd2e3a45ffa2109a5dc948e167_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;990&#39; height=&#39;1642&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"990\" data-rawheight=\"1642\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"990\" data-original=\"https://pic4.zhimg.com/v2-f65a22cd2e3a45ffa2109a5dc948e167_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-f65a22cd2e3a45ffa2109a5dc948e167_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "LDA", 
                    "tagLink": "https://api.zhihu.com/topics/19565464"
                }, 
                {
                    "tag": "降维算法", 
                    "tagLink": "https://api.zhihu.com/topics/20687563"
                }, 
                {
                    "tag": "线性判别分析", 
                    "tagLink": "https://api.zhihu.com/topics/20682964"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/53792488", 
            "userName": "谓之小一", 
            "userLink": "https://www.zhihu.com/people/761dda159c5cec5fd926f761ab5f4ccf", 
            "upvote": 2, 
            "title": "机器学习降维之奇异值分解(SVD)", 
            "content": "<blockquote>由于较多公式，所以我将部分内容转为图片进行上传，请见谅。清晰版请访问<a href=\"https://link.zhihu.com/?target=https%3A//weizhixiaoyi.com\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">weizhixiaoyi.com</span><span class=\"invisible\"></span></a>查看，你也可以关注我公众号<b>谓之小一</b>，后台直接向我要pdf版本，如有相关问题可公众号后台询问，随时回答。</blockquote><p><b>奇异值分解(Singular Value Decompostion, SVD)</b>是在机器学习领域广泛应用的算法，不光可以用于降维算法中的特征分解，还可以用于推荐系统，以及自然语言处理等领域，是很多机器学习算法的基石。本篇文章对SVD原理做主要讲解，在学习之前，确保你已经熟悉线性代数中的基本知识，包括特征值、特征向量、相似矩阵相关知识点。如果不太熟悉的话，推荐阅读如下两篇文章，<a href=\"https://www.zhihu.com/question/21874816\" class=\"internal\">如何理解矩阵特征值？知乎马同学的回答</a>和<a href=\"https://link.zhihu.com/?target=https%3A//www.matongxue.com/madocs/491.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">如何理解相似矩阵？马同学高等数学</a>，读完之后再看本篇文章会有很大帮助。</p><h2>1. 回顾特征值和特征向量</h2><p>我们首先回顾下特征值和特征向量的定义，如下所示。其中A是一个n×n的矩阵，x是一个n维向量，则我们说λ是矩阵A的一个特征值，x是矩阵A的特征值λ所对应的特征向量。但是求出特征值和特征向量有什么好处呢?</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-ce772af5a4af1b4bcde4dc86b3f544ca_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1558\" data-rawheight=\"108\" class=\"origin_image zh-lightbox-thumb\" width=\"1558\" data-original=\"https://pic3.zhimg.com/v2-ce772af5a4af1b4bcde4dc86b3f544ca_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1558&#39; height=&#39;108&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1558\" data-rawheight=\"108\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1558\" data-original=\"https://pic3.zhimg.com/v2-ce772af5a4af1b4bcde4dc86b3f544ca_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-ce772af5a4af1b4bcde4dc86b3f544ca_b.png\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-8e782dfaf14879fef601d205566c3d70_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1626\" data-rawheight=\"210\" class=\"origin_image zh-lightbox-thumb\" width=\"1626\" data-original=\"https://pic1.zhimg.com/v2-8e782dfaf14879fef601d205566c3d70_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1626&#39; height=&#39;210&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1626\" data-rawheight=\"210\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1626\" data-original=\"https://pic1.zhimg.com/v2-8e782dfaf14879fef601d205566c3d70_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-8e782dfaf14879fef601d205566c3d70_b.png\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-acaa30cd3e016c3b5c5389c6f1184c90_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1632\" data-rawheight=\"262\" class=\"origin_image zh-lightbox-thumb\" width=\"1632\" data-original=\"https://pic1.zhimg.com/v2-acaa30cd3e016c3b5c5389c6f1184c90_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1632&#39; height=&#39;262&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1632\" data-rawheight=\"262\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1632\" data-original=\"https://pic1.zhimg.com/v2-acaa30cd3e016c3b5c5389c6f1184c90_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-acaa30cd3e016c3b5c5389c6f1184c90_b.png\"/></figure><p>上面矩阵能够进行特征分解，需要满足矩阵A必须为方阵。那么如果A不是方阵，即行和列不相同时，我们还可以进行矩阵分解吗？</p><h2>2. 奇异值分解(SVD)</h2><p>当矩阵A不是方阵时，可以用奇异值进行分解，假设我们的的矩阵A时一个m×n的矩阵，那么我们定义矩阵A的SVD为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-ebe29cced4af3318f0c1888a51b9bdb1_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1586\" data-rawheight=\"88\" class=\"origin_image zh-lightbox-thumb\" width=\"1586\" data-original=\"https://pic2.zhimg.com/v2-ebe29cced4af3318f0c1888a51b9bdb1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1586&#39; height=&#39;88&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1586\" data-rawheight=\"88\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1586\" data-original=\"https://pic2.zhimg.com/v2-ebe29cced4af3318f0c1888a51b9bdb1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-ebe29cced4af3318f0c1888a51b9bdb1_b.png\"/></figure><p>其中U时一个m×m的矩阵，Σ是一个m×n的矩阵，除了主对角线上的元素以外全为0，主对角线上的每个元素都称为奇异值，V时一个n×n的矩阵。U和V都是酉矩阵，即满足</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-1647d70da9fd5101ea95270ea4b2fb5f_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1568\" data-rawheight=\"152\" class=\"origin_image zh-lightbox-thumb\" width=\"1568\" data-original=\"https://pic4.zhimg.com/v2-1647d70da9fd5101ea95270ea4b2fb5f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1568&#39; height=&#39;152&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1568\" data-rawheight=\"152\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1568\" data-original=\"https://pic4.zhimg.com/v2-1647d70da9fd5101ea95270ea4b2fb5f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-1647d70da9fd5101ea95270ea4b2fb5f_b.png\"/></figure><p>下图可以形象的表示出上述SVD的定义，但我们如何求出SVD分解后的U,Σ,V这三个矩阵呢?</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-a5d6b92118b45dd8f56daf1a0038d077_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"768\" data-rawheight=\"254\" class=\"origin_image zh-lightbox-thumb\" width=\"768\" data-original=\"https://pic4.zhimg.com/v2-a5d6b92118b45dd8f56daf1a0038d077_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;768&#39; height=&#39;254&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"768\" data-rawheight=\"254\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"768\" data-original=\"https://pic4.zhimg.com/v2-a5d6b92118b45dd8f56daf1a0038d077_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-a5d6b92118b45dd8f56daf1a0038d077_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-68cb29a0cf21f97c398f25913646a9fc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1630\" data-rawheight=\"328\" class=\"origin_image zh-lightbox-thumb\" width=\"1630\" data-original=\"https://pic1.zhimg.com/v2-68cb29a0cf21f97c398f25913646a9fc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1630&#39; height=&#39;328&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1630\" data-rawheight=\"328\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1630\" data-original=\"https://pic1.zhimg.com/v2-68cb29a0cf21f97c398f25913646a9fc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-68cb29a0cf21f97c398f25913646a9fc_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-a3236c53cf0f9380b7254e1aa00e1d9c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1634\" data-rawheight=\"330\" class=\"origin_image zh-lightbox-thumb\" width=\"1634\" data-original=\"https://pic1.zhimg.com/v2-a3236c53cf0f9380b7254e1aa00e1d9c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1634&#39; height=&#39;330&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1634\" data-rawheight=\"330\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1634\" data-original=\"https://pic1.zhimg.com/v2-a3236c53cf0f9380b7254e1aa00e1d9c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-a3236c53cf0f9380b7254e1aa00e1d9c_b.jpg\"/></figure><p>U和V都已经求出，现在只有奇异值矩阵Σ没有求出。由于Σ除了对角线上是奇异值，其他位置都是0，因此我们只需要求出每个奇异值σ就可以了。我们注意到</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-6fa049585ecb7007b639d12a7b4b5304_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1568\" data-rawheight=\"324\" class=\"origin_image zh-lightbox-thumb\" width=\"1568\" data-original=\"https://pic1.zhimg.com/v2-6fa049585ecb7007b639d12a7b4b5304_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1568&#39; height=&#39;324&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1568\" data-rawheight=\"324\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1568\" data-original=\"https://pic1.zhimg.com/v2-6fa049585ecb7007b639d12a7b4b5304_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-6fa049585ecb7007b639d12a7b4b5304_b.jpg\"/></figure><p>通过上式，我们便可以求出每个奇异值，进而求出奇异值矩阵Σ。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-af582ac38b432d48f7c876c21c15a4d9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1618\" data-rawheight=\"690\" class=\"origin_image zh-lightbox-thumb\" width=\"1618\" data-original=\"https://pic2.zhimg.com/v2-af582ac38b432d48f7c876c21c15a4d9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1618&#39; height=&#39;690&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1618\" data-rawheight=\"690\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1618\" data-original=\"https://pic2.zhimg.com/v2-af582ac38b432d48f7c876c21c15a4d9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-af582ac38b432d48f7c876c21c15a4d9_b.jpg\"/></figure><h2>3. SVD示例</h2><p>下面我们通过一个简单例子来说明矩阵式如何进行奇异值分解的，假设矩阵A为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-f03b8512d7fc669bc7a3c18eb88cac57_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1600\" data-rawheight=\"190\" class=\"origin_image zh-lightbox-thumb\" width=\"1600\" data-original=\"https://pic4.zhimg.com/v2-f03b8512d7fc669bc7a3c18eb88cac57_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1600&#39; height=&#39;190&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1600\" data-rawheight=\"190\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1600\" data-original=\"https://pic4.zhimg.com/v2-f03b8512d7fc669bc7a3c18eb88cac57_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-f03b8512d7fc669bc7a3c18eb88cac57_b.png\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-c3c6a81ca8f1225cf623aedc66db8e97_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1606\" data-rawheight=\"412\" class=\"origin_image zh-lightbox-thumb\" width=\"1606\" data-original=\"https://pic4.zhimg.com/v2-c3c6a81ca8f1225cf623aedc66db8e97_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1606&#39; height=&#39;412&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1606\" data-rawheight=\"412\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1606\" data-original=\"https://pic4.zhimg.com/v2-c3c6a81ca8f1225cf623aedc66db8e97_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-c3c6a81ca8f1225cf623aedc66db8e97_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-5159019dc28cce18b16bc590b78707e7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1578\" data-rawheight=\"388\" class=\"origin_image zh-lightbox-thumb\" width=\"1578\" data-original=\"https://pic4.zhimg.com/v2-5159019dc28cce18b16bc590b78707e7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1578&#39; height=&#39;388&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1578\" data-rawheight=\"388\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1578\" data-original=\"https://pic4.zhimg.com/v2-5159019dc28cce18b16bc590b78707e7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-5159019dc28cce18b16bc590b78707e7_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-6197515b43db4630c9c7e163bbe7fc17_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1624\" data-rawheight=\"740\" class=\"origin_image zh-lightbox-thumb\" width=\"1624\" data-original=\"https://pic4.zhimg.com/v2-6197515b43db4630c9c7e163bbe7fc17_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1624&#39; height=&#39;740&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1624\" data-rawheight=\"740\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1624\" data-original=\"https://pic4.zhimg.com/v2-6197515b43db4630c9c7e163bbe7fc17_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-6197515b43db4630c9c7e163bbe7fc17_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-eb78229a579a78e492e76028ed308c2a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1610\" data-rawheight=\"510\" class=\"origin_image zh-lightbox-thumb\" width=\"1610\" data-original=\"https://pic3.zhimg.com/v2-eb78229a579a78e492e76028ed308c2a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1610&#39; height=&#39;510&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1610\" data-rawheight=\"510\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1610\" data-original=\"https://pic3.zhimg.com/v2-eb78229a579a78e492e76028ed308c2a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-eb78229a579a78e492e76028ed308c2a_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-24cdcba5994ee11871f3c70f81d1f103_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1612\" data-rawheight=\"302\" class=\"origin_image zh-lightbox-thumb\" width=\"1612\" data-original=\"https://pic4.zhimg.com/v2-24cdcba5994ee11871f3c70f81d1f103_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1612&#39; height=&#39;302&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1612\" data-rawheight=\"302\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1612\" data-original=\"https://pic4.zhimg.com/v2-24cdcba5994ee11871f3c70f81d1f103_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-24cdcba5994ee11871f3c70f81d1f103_b.jpg\"/></figure><h2>4. SVD性质</h2><p>对于SVD有哪些重要的性质值得我们注意呢? 对于奇异值，它跟特征分解中的特征值类似，在奇异值矩阵中也是按照从大到小排列，而且奇异值的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部奇异值之和的99%以上的比例。也就是说，我们可以用最大的k个奇异值和对应的左右奇异向量来近似描述矩阵，即</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-e12d9a429e94ead1561949a3dfe1fac0_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1604\" data-rawheight=\"114\" class=\"origin_image zh-lightbox-thumb\" width=\"1604\" data-original=\"https://pic1.zhimg.com/v2-e12d9a429e94ead1561949a3dfe1fac0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1604&#39; height=&#39;114&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1604\" data-rawheight=\"114\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1604\" data-original=\"https://pic1.zhimg.com/v2-e12d9a429e94ead1561949a3dfe1fac0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-e12d9a429e94ead1561949a3dfe1fac0_b.png\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-d03d4f05cf94c3daff72508247e67bf0_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1626\" data-rawheight=\"122\" class=\"origin_image zh-lightbox-thumb\" width=\"1626\" data-original=\"https://pic1.zhimg.com/v2-d03d4f05cf94c3daff72508247e67bf0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1626&#39; height=&#39;122&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1626\" data-rawheight=\"122\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1626\" data-original=\"https://pic1.zhimg.com/v2-d03d4f05cf94c3daff72508247e67bf0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-d03d4f05cf94c3daff72508247e67bf0_b.png\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-b7f72ac63b95e87d3333ec494a8a118c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"744\" data-rawheight=\"294\" class=\"origin_image zh-lightbox-thumb\" width=\"744\" data-original=\"https://pic1.zhimg.com/v2-b7f72ac63b95e87d3333ec494a8a118c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;744&#39; height=&#39;294&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"744\" data-rawheight=\"294\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"744\" data-original=\"https://pic1.zhimg.com/v2-b7f72ac63b95e87d3333ec494a8a118c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-b7f72ac63b95e87d3333ec494a8a118c_b.jpg\"/></figure><p>由于这个重要的性质，因此SVD可以用于PCA降维，用来做数据压缩和去噪。也可以用于推荐算法，将用户和喜好对应的矩阵做特征分解，进而得到隐含的用户需求来做推荐。同时也可以用于NLP之中的算法，比如潜在语义索引(LSI)。</p><h2>5. SVD在PCA之中的应用</h2><p>在<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3MjA2NTQzMw%3D%3D%26mid%3D2247484027%26idx%3D1%26sn%3D245a237eed7fa6ec4d6db08168c4e889%26chksm%3Dfcd7d18dcba0589b2ab1289bd7b9cec3a2b265f1d17cb2fb1f109437a1f53285cdab75e997b4%26token%3D980498466%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">机器学习降维之主成分分析(PCA)</a>之中，我们讲到PCA降维时，需要找到样本协方差矩阵 <img src=\"https://www.zhihu.com/equation?tex=X%5ETX\" alt=\"X^TX\" eeimg=\"1\"/> 最大的d个特征向量，然后用着最大的d个特征向量组成的矩阵来做低维投影降维。可以看出，在这个过程中需要先求出协方差矩阵 <img src=\"https://www.zhihu.com/equation?tex=X%5ETX\" alt=\"X^TX\" eeimg=\"1\"/> ，当样本数多、样本特征数也多的时候，比如10000*10000的矩阵，这个计算量是很大的。</p><p>注意到SVD也可以求出协方差矩阵 <img src=\"https://www.zhihu.com/equation?tex=X%5ETX\" alt=\"X^TX\" eeimg=\"1\"/> 最大的d个特征向量组成的矩阵，但是SVD有个好处，就是可以不求出协方差矩阵 <img src=\"https://www.zhihu.com/equation?tex=X%5ETX\" alt=\"X^TX\" eeimg=\"1\"/> ，也能通过某些算法求出右奇异矩阵 <img src=\"https://www.zhihu.com/equation?tex=V\" alt=\"V\" eeimg=\"1\"/> ，比如<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/0909.4061\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/abs/0909.4061</span><span class=\"invisible\"></span></a>。也就是说，PCA算法可以不用做特征分解，而是用SVD来进行完成。</p><p>另一方面，PCA仅仅使用了SVD的右奇异矩阵，没有使用左奇异矩阵，那么左奇异矩阵有什么用呢？假如我们的样本是m×n的矩阵X，如果通过SVD找到矩阵 <img src=\"https://www.zhihu.com/equation?tex=XX%5ET\" alt=\"XX^T\" eeimg=\"1\"/> 最大的d个特征向量组成的m×d的矩阵U，则我们进行如下处理</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-9a85d1a755c8d30a7e88f64cc32fdaf5_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1580\" data-rawheight=\"112\" class=\"origin_image zh-lightbox-thumb\" width=\"1580\" data-original=\"https://pic2.zhimg.com/v2-9a85d1a755c8d30a7e88f64cc32fdaf5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1580&#39; height=&#39;112&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1580\" data-rawheight=\"112\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1580\" data-original=\"https://pic2.zhimg.com/v2-9a85d1a755c8d30a7e88f64cc32fdaf5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-9a85d1a755c8d30a7e88f64cc32fdaf5_b.png\"/></figure><p>可以得到一个d×n的矩阵X&#39;，这个矩阵和我们原来的m×n维样本矩阵X相比，行数从m减到了d，可见对行数进行了压缩。也就是说，左奇异矩阵可以用于行数的压缩，右奇异矩阵可以用于列数压缩，即特征降维。</p><h2>6. SVD算法总结</h2><p>SVD作为一个很基本的算法，在很多机器学习算法中都有它的身影，特别是在现在的大数据时代，由于SVD可以实现并行化，因此更是大展身手。当然，SVD的缺点是分解出的矩阵解释性往往不强，不过这不影响它的使用。</p><h2>7. 推广</h2><p>更多内容请关注公众号<b>谓之小一</b>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-f65a22cd2e3a45ffa2109a5dc948e167_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"990\" data-rawheight=\"1642\" class=\"origin_image zh-lightbox-thumb\" width=\"990\" data-original=\"https://pic4.zhimg.com/v2-f65a22cd2e3a45ffa2109a5dc948e167_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;990&#39; height=&#39;1642&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"990\" data-rawheight=\"1642\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"990\" data-original=\"https://pic4.zhimg.com/v2-f65a22cd2e3a45ffa2109a5dc948e167_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-f65a22cd2e3a45ffa2109a5dc948e167_b.jpg\"/></figure><p>参考 </p><a href=\"https://link.zhihu.com/?target=https%3A//www.cnblogs.com/pinard/p/6251584.html\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic4.zhimg.com/v2-a5d6b92118b45dd8f56daf1a0038d077_180x120.jpg\" data-image-width=\"768\" data-image-height=\"254\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">奇异值分解(SVD)原理与在降维中的应用 - 刘建平Pinard - 博客园</a><p></p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "数据降维", 
                    "tagLink": "https://api.zhihu.com/topics/20010182"
                }, 
                {
                    "tag": "奇异值分解", 
                    "tagLink": "https://api.zhihu.com/topics/19940797"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/53696634", 
            "userName": "谓之小一", 
            "userLink": "https://www.zhihu.com/people/761dda159c5cec5fd926f761ab5f4ccf", 
            "upvote": 2, 
            "title": "机器学习降维之主成分分析(PCA", 
            "content": "<blockquote>由于较多公式，所以我将部分内容转为图片进行上传，请见谅。清晰版请访问<a href=\"https://link.zhihu.com/?target=https%3A//weizhixiaoyi.com\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">weizhixiaoyi.com</span><span class=\"invisible\"></span></a>查看，你也可以关注我公众号<b>谓之小一</b>，后台直接向我要pdf版本，如有相关问题可公众号后台询问，随时回答。</blockquote><p><b>主成分分析(Principal components analysis, PCA)</b>是最重要的降维方法之一，在数据压缩、消除冗余和数据噪音消除等方面有广泛的应用。通常我们提到降维算法，最先想到的就是PCA，下面我们对PCA原理进行介绍。</p><h2>1. PCA思想</h2><p>PCA就是找出数据中最主要的方面，用数据中最重要的方面来代替原始数据。假如我们的数据集是n维的，共有m个数据(x1,x2,...,xm)，我们将这m个数据从n维降到r维，希望这m个r的数据集尽可能的代表原始数据集。</p><p>我们知道从n维降到r维肯定会有损失，但是希望损失尽可能的小，那么如何让这r维的数据尽可能表示原来的数据呢？首先来看最简单的情况，即将二维数据降到一维，也就是n=2,r=1。数据如下图所示，我们希望找到某一个维度方向，它可以代表这两个维度的数据。图中列了两个向量，也就是u1和u2，那么哪个向量可以更好的代表原始数据集呢？</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-042889503003d7e56a88db5ba918f9e1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"600\" data-rawheight=\"450\" class=\"origin_image zh-lightbox-thumb\" width=\"600\" data-original=\"https://pic2.zhimg.com/v2-042889503003d7e56a88db5ba918f9e1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;600&#39; height=&#39;450&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"600\" data-rawheight=\"450\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"600\" data-original=\"https://pic2.zhimg.com/v2-042889503003d7e56a88db5ba918f9e1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-042889503003d7e56a88db5ba918f9e1_b.jpg\"/></figure><p>直观上看u1比u2更好，为什么呢？可以有两种解释，第一种解释是样本点在这个直线上的投影尽可能的分开，第二种解释是样本点到这个直线的距离足够近。假如我们把r从1维推广到任意维，则我们希望降维的标准为样本点在这个超平面上的投影尽可能分开，或者说样本点到这个超平面的距离足够近。基于上面的两种标准，我们可以得到PCA的两种等价推导。</p><h2>2. PCA推导:基于最大投影方差</h2><h2>2.1 基变换</h2><p>一般来说，想要获得原始数据的表示空间，最简单的方式是对原始数据进行线性变换(基变换)，即Y=PX。其中Y是样本在新空间的表达，P是基向量，X是原始样本。我们可知选择不同的基能够对一组数据给出不同的表示，同时当基的数量少于原始样本本身的维数时，则可以达到降维的效果，矩阵表示如下</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3c6370bce443ce1209cd3cdc9a4757f1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1656\" data-rawheight=\"554\" class=\"origin_image zh-lightbox-thumb\" width=\"1656\" data-original=\"https://pic2.zhimg.com/v2-3c6370bce443ce1209cd3cdc9a4757f1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1656&#39; height=&#39;554&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1656\" data-rawheight=\"554\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1656\" data-original=\"https://pic2.zhimg.com/v2-3c6370bce443ce1209cd3cdc9a4757f1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-3c6370bce443ce1209cd3cdc9a4757f1_b.jpg\"/></figure><h2>2.2 方差</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-7732feda6b33541000452602efdfaf0d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"539\" data-rawheight=\"410\" class=\"origin_image zh-lightbox-thumb\" width=\"539\" data-original=\"https://pic2.zhimg.com/v2-7732feda6b33541000452602efdfaf0d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;539&#39; height=&#39;410&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"539\" data-rawheight=\"410\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"539\" data-original=\"https://pic2.zhimg.com/v2-7732feda6b33541000452602efdfaf0d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-7732feda6b33541000452602efdfaf0d_b.jpg\"/></figure><p>那么考虑，如何选择一个方向或者基才是最优的呢? 观察上图，我们将所有的点分别向两条直线做投影，基于前面PCA最大可分的思想，我们要找的方向是降维后损失最小，可以理解为投影后的数据尽可能的分开。那么这种分散程度可以用数学上的方差进行表示，方差越大数据越分散，方差公式如下所示</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b09a97d8856b34cf815e47c3ce7b2a89_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1602\" data-rawheight=\"150\" class=\"origin_image zh-lightbox-thumb\" width=\"1602\" data-original=\"https://pic2.zhimg.com/v2-b09a97d8856b34cf815e47c3ce7b2a89_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1602&#39; height=&#39;150&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1602\" data-rawheight=\"150\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1602\" data-original=\"https://pic2.zhimg.com/v2-b09a97d8856b34cf815e47c3ce7b2a89_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-b09a97d8856b34cf815e47c3ce7b2a89_b.png\"/></figure><p>对数据进行中心化后得到</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-d8e11a4d784ca6d4a616d3f2196bc858_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1610\" data-rawheight=\"146\" class=\"origin_image zh-lightbox-thumb\" width=\"1610\" data-original=\"https://pic1.zhimg.com/v2-d8e11a4d784ca6d4a616d3f2196bc858_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1610&#39; height=&#39;146&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1610\" data-rawheight=\"146\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1610\" data-original=\"https://pic1.zhimg.com/v2-d8e11a4d784ca6d4a616d3f2196bc858_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-d8e11a4d784ca6d4a616d3f2196bc858_b.png\"/></figure><p>现在我们知道以下几点</p><ul><li>对原始数据进行基变换可以对原始数据给出不同表示。</li><li>基的维度小于数据的维度可以起到降维的效果。</li><li>对基变换后新的样本进行求方差，选择使其方差最大的基。</li></ul><h2>2.3 协方差</h2><p>基于上面提到的几点，我们来探讨如何寻找计算方案。从上面可以得到，二维降到一维可以使用方差最大，来选出能使基变换后数据分散最大的方向(基)，但如果遇到高维的变换，怎么办呢? </p><p>针对高维情况，数学上采用协方差来表示</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-78b17f6940af3c7c18768c781be419a4_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1606\" data-rawheight=\"114\" class=\"origin_image zh-lightbox-thumb\" width=\"1606\" data-original=\"https://pic1.zhimg.com/v2-78b17f6940af3c7c18768c781be419a4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1606&#39; height=&#39;114&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1606\" data-rawheight=\"114\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1606\" data-original=\"https://pic1.zhimg.com/v2-78b17f6940af3c7c18768c781be419a4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-78b17f6940af3c7c18768c781be419a4_b.png\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-29d5d756359c1ee36667173bf05c64fc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1638\" data-rawheight=\"292\" class=\"origin_image zh-lightbox-thumb\" width=\"1638\" data-original=\"https://pic1.zhimg.com/v2-29d5d756359c1ee36667173bf05c64fc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1638&#39; height=&#39;292&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1638\" data-rawheight=\"292\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1638\" data-original=\"https://pic1.zhimg.com/v2-29d5d756359c1ee36667173bf05c64fc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-29d5d756359c1ee36667173bf05c64fc_b.jpg\"/></figure><h2>2.4 协方差矩阵</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-3197ea152e21c700f201995d4bed28de_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1612\" data-rawheight=\"512\" class=\"origin_image zh-lightbox-thumb\" width=\"1612\" data-original=\"https://pic3.zhimg.com/v2-3197ea152e21c700f201995d4bed28de_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1612&#39; height=&#39;512&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1612\" data-rawheight=\"512\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1612\" data-original=\"https://pic3.zhimg.com/v2-3197ea152e21c700f201995d4bed28de_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-3197ea152e21c700f201995d4bed28de_b.jpg\"/></figure><h2>2.5 协方差矩阵对角化</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-68fc084cc780febe2bae7f9fdeaf3a45_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1624\" data-rawheight=\"170\" class=\"origin_image zh-lightbox-thumb\" width=\"1624\" data-original=\"https://pic2.zhimg.com/v2-68fc084cc780febe2bae7f9fdeaf3a45_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1624&#39; height=&#39;170&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1624\" data-rawheight=\"170\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1624\" data-original=\"https://pic2.zhimg.com/v2-68fc084cc780febe2bae7f9fdeaf3a45_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-68fc084cc780febe2bae7f9fdeaf3a45_b.png\"/></figure><p>我们来看看原数据协方差矩阵和通过基变换后的协方差矩阵之间的关系。设原数据协方差矩阵为C，P是一组基按行组成的矩阵，设Y=PX，则Y为X对P做基变换后的数据。设Y的协方差矩阵为D，我们来推导一下D和C的关系</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-7819a199df51469593c21356d0bec62d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1580\" data-rawheight=\"564\" class=\"origin_image zh-lightbox-thumb\" width=\"1580\" data-original=\"https://pic2.zhimg.com/v2-7819a199df51469593c21356d0bec62d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1580&#39; height=&#39;564&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1580\" data-rawheight=\"564\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1580\" data-original=\"https://pic2.zhimg.com/v2-7819a199df51469593c21356d0bec62d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-7819a199df51469593c21356d0bec62d_b.jpg\"/></figure><p>可以看出，我们的目标是寻找能够让原始协方差矩阵对角化的P。换句话说，优化目标变成了寻找一个矩阵P，满足PCP^T是一个对角矩阵。并且对角元素按照从大到小依次排列，那么P的前k行就是要寻找的基，用P的前k行组成的矩阵乘以X就使得X从n维降到了r维。</p><p>我们希望投影后的方差最大化，于是优化目标为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-b004ebda296cdf3144caebd62b25d7ea_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1574\" data-rawheight=\"148\" class=\"origin_image zh-lightbox-thumb\" width=\"1574\" data-original=\"https://pic3.zhimg.com/v2-b004ebda296cdf3144caebd62b25d7ea_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1574&#39; height=&#39;148&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1574\" data-rawheight=\"148\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1574\" data-original=\"https://pic3.zhimg.com/v2-b004ebda296cdf3144caebd62b25d7ea_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-b004ebda296cdf3144caebd62b25d7ea_b.png\"/></figure><p>其中tr表示矩阵的迹，利用拉格朗日函数可以得到</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-b1fa6cfa67f3ced136b9a371133e66e7_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1594\" data-rawheight=\"114\" class=\"origin_image zh-lightbox-thumb\" width=\"1594\" data-original=\"https://pic4.zhimg.com/v2-b1fa6cfa67f3ced136b9a371133e66e7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1594&#39; height=&#39;114&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1594\" data-rawheight=\"114\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1594\" data-original=\"https://pic4.zhimg.com/v2-b1fa6cfa67f3ced136b9a371133e66e7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-b1fa6cfa67f3ced136b9a371133e66e7_b.png\"/></figure><p>对P进行求导，整理得到</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-b1ba5869bad5b8d3a9a3d3cd655229da_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1626\" data-rawheight=\"276\" class=\"origin_image zh-lightbox-thumb\" width=\"1626\" data-original=\"https://pic3.zhimg.com/v2-b1ba5869bad5b8d3a9a3d3cd655229da_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1626&#39; height=&#39;276&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1626\" data-rawheight=\"276\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1626\" data-original=\"https://pic3.zhimg.com/v2-b1ba5869bad5b8d3a9a3d3cd655229da_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-b1ba5869bad5b8d3a9a3d3cd655229da_b.jpg\"/></figure><h2>3. PCA推导:基于最小投影距离</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-6f144078905517bd1ed53c8425e59be0_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1610\" data-rawheight=\"174\" class=\"origin_image zh-lightbox-thumb\" width=\"1610\" data-original=\"https://pic1.zhimg.com/v2-6f144078905517bd1ed53c8425e59be0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1610&#39; height=&#39;174&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1610\" data-rawheight=\"174\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1610\" data-original=\"https://pic1.zhimg.com/v2-6f144078905517bd1ed53c8425e59be0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-6f144078905517bd1ed53c8425e59be0_b.png\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-a0b4e923d93e68f890bd80a0b448522f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1640\" data-rawheight=\"1034\" class=\"origin_image zh-lightbox-thumb\" width=\"1640\" data-original=\"https://pic4.zhimg.com/v2-a0b4e923d93e68f890bd80a0b448522f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1640&#39; height=&#39;1034&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1640\" data-rawheight=\"1034\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1640\" data-original=\"https://pic4.zhimg.com/v2-a0b4e923d93e68f890bd80a0b448522f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-a0b4e923d93e68f890bd80a0b448522f_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-909cdd13ec67e1f00a1fd93e329c84a0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1640\" data-rawheight=\"314\" class=\"origin_image zh-lightbox-thumb\" width=\"1640\" data-original=\"https://pic1.zhimg.com/v2-909cdd13ec67e1f00a1fd93e329c84a0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1640&#39; height=&#39;314&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1640\" data-rawheight=\"314\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1640\" data-original=\"https://pic1.zhimg.com/v2-909cdd13ec67e1f00a1fd93e329c84a0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-909cdd13ec67e1f00a1fd93e329c84a0_b.jpg\"/></figure><p>可以发现，和第二节基于最大投影方差的优化目标完全一样。只是上述计算的是加负号的最小化，现在计算的是无负号最大化。然后利用拉格朗日函数可以得到</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-515117bccbe5fae600bf2ecd70835cdc_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1614\" data-rawheight=\"112\" class=\"origin_image zh-lightbox-thumb\" width=\"1614\" data-original=\"https://pic1.zhimg.com/v2-515117bccbe5fae600bf2ecd70835cdc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1614&#39; height=&#39;112&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1614\" data-rawheight=\"112\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1614\" data-original=\"https://pic1.zhimg.com/v2-515117bccbe5fae600bf2ecd70835cdc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-515117bccbe5fae600bf2ecd70835cdc_b.png\"/></figure><p>对P求导有</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-1b7f2e1d35886c90f7a7b6ad5243662f_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1636\" data-rawheight=\"254\" class=\"origin_image zh-lightbox-thumb\" width=\"1636\" data-original=\"https://pic4.zhimg.com/v2-1b7f2e1d35886c90f7a7b6ad5243662f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1636&#39; height=&#39;254&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1636\" data-rawheight=\"254\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1636\" data-original=\"https://pic4.zhimg.com/v2-1b7f2e1d35886c90f7a7b6ad5243662f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-1b7f2e1d35886c90f7a7b6ad5243662f_b.png\"/></figure><h2>4. PCA算法流程</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-9a29e6dacc53ab832c6227118a9c30e4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1660\" data-rawheight=\"876\" class=\"origin_image zh-lightbox-thumb\" width=\"1660\" data-original=\"https://pic1.zhimg.com/v2-9a29e6dacc53ab832c6227118a9c30e4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1660&#39; height=&#39;876&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1660\" data-rawheight=\"876\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1660\" data-original=\"https://pic1.zhimg.com/v2-9a29e6dacc53ab832c6227118a9c30e4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-9a29e6dacc53ab832c6227118a9c30e4_b.jpg\"/></figure><h2>5. 核主成分分析KPCA</h2><p>在上面的PCA算法中，我们假设存在一个线性的超平面，可以让我们对数据进行投影。但是有些时候，数据不是线性的，不能直接进行PCA降维。这里便需要利用和支持向量机一样的核函数思想，先把数据集从n维映射到线性可分的高维N，其中N&gt;n，然后再从N维降维到一个低维度r，这里维度之间满足r&lt;n&lt;N。</p><p>使用核函数的主成分分析称为核主成分分析(Kernelized PCA, KPCA)。假设高维空间的数据由n维空间的数据通过映射ϕ产生。则对于n维空间的特征分解</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b52b98e47f38ba6c4a3d4ea20286c2dd_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1574\" data-rawheight=\"156\" class=\"origin_image zh-lightbox-thumb\" width=\"1574\" data-original=\"https://pic2.zhimg.com/v2-b52b98e47f38ba6c4a3d4ea20286c2dd_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1574&#39; height=&#39;156&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1574\" data-rawheight=\"156\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1574\" data-original=\"https://pic2.zhimg.com/v2-b52b98e47f38ba6c4a3d4ea20286c2dd_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-b52b98e47f38ba6c4a3d4ea20286c2dd_b.png\"/></figure><p>映射为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-72ac25eca86e8c6cfc28b5de5626f675_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1604\" data-rawheight=\"152\" class=\"origin_image zh-lightbox-thumb\" width=\"1604\" data-original=\"https://pic2.zhimg.com/v2-72ac25eca86e8c6cfc28b5de5626f675_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1604&#39; height=&#39;152&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1604\" data-rawheight=\"152\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1604\" data-original=\"https://pic2.zhimg.com/v2-72ac25eca86e8c6cfc28b5de5626f675_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-72ac25eca86e8c6cfc28b5de5626f675_b.png\"/></figure><p>通过在高维空间进行协方差的特征值分解，然后用和PCA一样的方法进行降维。一般来说，映射ϕ不用显式的计算，而是在需要计算的时候通过核函数完成。由于KPCA需要核函数的运算，因此它的计算量要比PCA大很多。</p><h2>6. PCA算法总结</h2><p>作为一个非监督学习的降维方法，PCA只需要特征值分解，就可以对数据进行压缩，去噪。因此在实际场景应用很广泛。为了克服PCA的一些缺点，出现了很多PCA的变种，比如解决非线性降维的KPCA，还有解决内存限制的增量PCA方法Incremental PCA，以及解决稀疏数据降维的PCA方法Sparse PCA等。</p><h2>7. 推广</h2><p>更多内容请关注公众号<b>谓之小一</b>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-f65a22cd2e3a45ffa2109a5dc948e167_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"990\" data-rawheight=\"1642\" class=\"origin_image zh-lightbox-thumb\" width=\"990\" data-original=\"https://pic4.zhimg.com/v2-f65a22cd2e3a45ffa2109a5dc948e167_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;990&#39; height=&#39;1642&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"990\" data-rawheight=\"1642\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"990\" data-original=\"https://pic4.zhimg.com/v2-f65a22cd2e3a45ffa2109a5dc948e167_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-f65a22cd2e3a45ffa2109a5dc948e167_b.jpg\"/></figure><p>引用</p><a href=\"https://link.zhihu.com/?target=https%3A//www.cnblogs.com/pinard/p/6239403.html\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic2.zhimg.com/v2-042889503003d7e56a88db5ba918f9e1_180x120.jpg\" data-image-width=\"600\" data-image-height=\"450\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">主成分分析（PCA）原理总结 - 刘建平Pinard - 博客园</a><a href=\"https://zhuanlan.zhihu.com/p/32412043\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic4.zhimg.com/v2-db0c770d8dd55e5a433b6306d7085bf3_180x120.jpg\" data-image-width=\"556\" data-image-height=\"234\" class=\"internal\">鱼遇雨欲语与余：PCA主成分分析学习总结</a><p></p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "数据降维", 
                    "tagLink": "https://api.zhihu.com/topics/20010182"
                }, 
                {
                    "tag": "降维算法", 
                    "tagLink": "https://api.zhihu.com/topics/20687563"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/52509253", 
            "userName": "谓之小一", 
            "userLink": "https://www.zhihu.com/people/761dda159c5cec5fd926f761ab5f4ccf", 
            "upvote": 0, 
            "title": "Gibbs采样", 
            "content": "<blockquote>由于较多公式，所以我将部分内容转为图片进行上传，请见谅。清晰版请访问<a href=\"https://link.zhihu.com/?target=https%3A//weizhixiaoyi.com\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">weizhixiaoyi.com</span><span class=\"invisible\"></span></a>查看，你也可以关注我公众号<b>谓之小一</b>，后台直接向我要pdf版本，如有相关问题可公众号后台询问，随时回答。</blockquote><p>在<a href=\"https://link.zhihu.com/?target=https%3A//weizhixiaoyi.com/2018/12/15/MCMC%25E9%2587%2587%25E6%25A0%25B7%25E5%2592%258CM-H%25E9%2587%2587%25E6%25A0%25B7/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">MCMC采样和M-H采样</a>中，我们讲到M-H采样已经可以很好的解决蒙特卡罗方法需要的任意概率分布的样本集问题。但是M-H采样有两个缺点：一是需要计算接受率，在高维情况下计算量非常大，同时由于接受率的原因导致算法收敛时间变长。二是有些高维数据，特征的条件概率分布方便求解，但特征的联合分布很难求解。因此需要改进M-H算法，来解决上面提到的两个问题，下面我们详细介绍Gibbs采样方法。</p><h2>1.细致平衡条件</h2><p><a href=\"https://link.zhihu.com/?target=https%3A//weizhixiaoyi.com/2018/12/15/MCMC%25E9%2587%2587%25E6%25A0%25B7%25E5%2592%258CM-H%25E9%2587%2587%25E6%25A0%25B7/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">MCMC采样和M-H采样</a>中我们讲到细致平衡条件，即如果非周期马尔可夫链状态转移矩阵P和概率分布π(x)对于所有的i,j满足下列方程，则称概率分布π(x)是状态转移矩阵P的平稳分布。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-cb8cc040a6c675fb2bf0da7ab5980879_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1558\" data-rawheight=\"112\" class=\"origin_image zh-lightbox-thumb\" width=\"1558\" data-original=\"https://pic2.zhimg.com/v2-cb8cc040a6c675fb2bf0da7ab5980879_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1558&#39; height=&#39;112&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1558\" data-rawheight=\"112\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1558\" data-original=\"https://pic2.zhimg.com/v2-cb8cc040a6c675fb2bf0da7ab5980879_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-cb8cc040a6c675fb2bf0da7ab5980879_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-01d59781e338d43412d994eca2e31e4e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1650\" data-rawheight=\"342\" class=\"origin_image zh-lightbox-thumb\" width=\"1650\" data-original=\"https://pic3.zhimg.com/v2-01d59781e338d43412d994eca2e31e4e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1650&#39; height=&#39;342&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1650\" data-rawheight=\"342\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1650\" data-original=\"https://pic3.zhimg.com/v2-01d59781e338d43412d994eca2e31e4e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-01d59781e338d43412d994eca2e31e4e_b.jpg\"/></figure><p>可以发现，上面两式的右边相等，因此我们有</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-66d9de2d9d47c35def5aa3da74c251fa_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1574\" data-rawheight=\"110\" class=\"origin_image zh-lightbox-thumb\" width=\"1574\" data-original=\"https://pic3.zhimg.com/v2-66d9de2d9d47c35def5aa3da74c251fa_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1574&#39; height=&#39;110&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1574\" data-rawheight=\"110\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1574\" data-original=\"https://pic3.zhimg.com/v2-66d9de2d9d47c35def5aa3da74c251fa_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-66d9de2d9d47c35def5aa3da74c251fa_b.jpg\"/></figure><p>也就是</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-d11839e782ed41a67f8be1371eecb7af_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1570\" data-rawheight=\"104\" class=\"origin_image zh-lightbox-thumb\" width=\"1570\" data-original=\"https://pic4.zhimg.com/v2-d11839e782ed41a67f8be1371eecb7af_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1570&#39; height=&#39;104&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1570\" data-rawheight=\"104\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1570\" data-original=\"https://pic4.zhimg.com/v2-d11839e782ed41a67f8be1371eecb7af_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-d11839e782ed41a67f8be1371eecb7af_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-069e1a3da3a036fbc9a47f509ef06e9f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1624\" data-rawheight=\"482\" class=\"origin_image zh-lightbox-thumb\" width=\"1624\" data-original=\"https://pic4.zhimg.com/v2-069e1a3da3a036fbc9a47f509ef06e9f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1624&#39; height=&#39;482&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1624\" data-rawheight=\"482\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1624\" data-original=\"https://pic4.zhimg.com/v2-069e1a3da3a036fbc9a47f509ef06e9f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-069e1a3da3a036fbc9a47f509ef06e9f_b.jpg\"/></figure><p>有了上面状态转移矩阵，我们很容易验证平面上任意两点E,F可以满足细致平稳条件</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-a33a11be4c6f6f507b2a6e5a8bde3fbb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1574\" data-rawheight=\"112\" class=\"origin_image zh-lightbox-thumb\" width=\"1574\" data-original=\"https://pic4.zhimg.com/v2-a33a11be4c6f6f507b2a6e5a8bde3fbb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1574&#39; height=&#39;112&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1574\" data-rawheight=\"112\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1574\" data-original=\"https://pic4.zhimg.com/v2-a33a11be4c6f6f507b2a6e5a8bde3fbb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-a33a11be4c6f6f507b2a6e5a8bde3fbb_b.jpg\"/></figure><h2>2.二维Gibbs采样</h2><p>根据上面提到的状态转移矩阵，我们就可以得到二维Gibbs采样，这个采样需要两维度之间的条件概率，具体过程如下</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-ab3bf3831ffa829300d94ad8c55edbf6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1606\" data-rawheight=\"566\" class=\"origin_image zh-lightbox-thumb\" width=\"1606\" data-original=\"https://pic3.zhimg.com/v2-ab3bf3831ffa829300d94ad8c55edbf6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1606&#39; height=&#39;566&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1606\" data-rawheight=\"566\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1606\" data-original=\"https://pic3.zhimg.com/v2-ab3bf3831ffa829300d94ad8c55edbf6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-ab3bf3831ffa829300d94ad8c55edbf6_b.jpg\"/></figure><p>用下图可以直观的看出，采样是在两个坐标轴上不断变换的。当然，坐标轴轮换不是必须的，也可以每次随意选择一个坐标轴进行采样。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-12d5b2aac386c91e47605bd073027b1a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"512\" data-rawheight=\"384\" class=\"origin_image zh-lightbox-thumb\" width=\"512\" data-original=\"https://pic3.zhimg.com/v2-12d5b2aac386c91e47605bd073027b1a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;512&#39; height=&#39;384&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"512\" data-rawheight=\"384\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"512\" data-original=\"https://pic3.zhimg.com/v2-12d5b2aac386c91e47605bd073027b1a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-12d5b2aac386c91e47605bd073027b1a_b.jpg\"/></figure><h2>3.多维Gibbs采样</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-429dbc35f771af593c120bc11e3a46e2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1616\" data-rawheight=\"870\" class=\"origin_image zh-lightbox-thumb\" width=\"1616\" data-original=\"https://pic3.zhimg.com/v2-429dbc35f771af593c120bc11e3a46e2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1616&#39; height=&#39;870&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1616\" data-rawheight=\"870\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1616\" data-original=\"https://pic3.zhimg.com/v2-429dbc35f771af593c120bc11e3a46e2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-429dbc35f771af593c120bc11e3a46e2_b.jpg\"/></figure><h2>4.Gibbs采样总结</h2><p>由于Gibbs采样在高维特征时的优势，目前通常意义上的MCMC采样都是用Gibbs采样。Gibbs采样要求数据至少有两个维度，一维概率分布的采样无法用Gibbs采样实现，这时可以用M-H方法采样。通过Gibbs采样来获取概率分布的样本集，通过蒙特卡罗方法来用样本集求和，两者一起奠定了MCMC算法在高维数据模拟求和时的作用。</p><h2>5.推广</h2><p>更多内容请关注公众号<b>谓之小一</b>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p><p><a href=\"https://link.zhihu.com/?target=http%3A//weixin.qq.com/r/Ty_4oDPE2W6mrXfD93pd\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">weixin.qq.com/r/Ty_4oDP</span><span class=\"invisible\">E2W6mrXfD93pd</span><span class=\"ellipsis\"></span></a> (二维码自动识别)</p><p></p>", 
            "topic": [
                {
                    "tag": "MCMC采样", 
                    "tagLink": "https://api.zhihu.com/topics/20080781"
                }, 
                {
                    "tag": "吉布斯采样", 
                    "tagLink": "https://api.zhihu.com/topics/20683707"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/52507478", 
            "userName": "谓之小一", 
            "userLink": "https://www.zhihu.com/people/761dda159c5cec5fd926f761ab5f4ccf", 
            "upvote": 1, 
            "title": "MCMC采样和M-H采样", 
            "content": "<blockquote>由于较多公式，所以我将部分内容转为图片进行上传，请见谅。清晰版请访问<a href=\"https://link.zhihu.com/?target=https%3A//weizhixiaoyi.com\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">weizhixiaoyi.com</span><span class=\"invisible\"></span></a>查看，你也可以关注我公众号<b>谓之小一</b>，后台直接向我要pdf版本，如有相关问题可公众号后台询问，随时回答。</blockquote><p>在<a href=\"https://link.zhihu.com/?target=https%3A//weizhixiaoyi.com/2018/12/01/MCMC%25E4%25B9%258B%25E9%25A9%25AC%25E5%25B0%2594%25E5%258F%25AF%25E5%25A4%25AB%25E9%2593%25BE/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">MCMC之马尔可夫链</a>之中我们介绍到，给定一个概率分布π，很难直接找到对应的马尔可夫链状态转移矩阵P。只要解决这个问题，我们便可以找到一种通用的概率分布采样方法，进而用于蒙特卡罗模拟。下面我们来介绍如何找到马尔可夫链所对应的状态转移矩阵P。</p><h2>1.马尔可夫链细致平稳条件</h2><p>解决平稳分布π所对应的马尔可夫链状态转移矩阵P之前，我们先看一下马尔可夫链的细致平稳条件。其定义为：如果非周期马尔可夫链的状态转移矩阵P和概率分布π(x)对于所有的i,j满足下列方程，则概率分布π(x)是状态转移矩阵P的平稳分布。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-0472a573957f1999f170b4dcca0e11a4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1564\" data-rawheight=\"114\" class=\"origin_image zh-lightbox-thumb\" width=\"1564\" data-original=\"https://pic1.zhimg.com/v2-0472a573957f1999f170b4dcca0e11a4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1564&#39; height=&#39;114&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1564\" data-rawheight=\"114\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1564\" data-original=\"https://pic1.zhimg.com/v2-0472a573957f1999f170b4dcca0e11a4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-0472a573957f1999f170b4dcca0e11a4_b.jpg\"/></figure><p>证明如下，由细致平稳条件有</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-a5ffebbb7c7be7c9bf28cb458aeb786a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1572\" data-rawheight=\"160\" class=\"origin_image zh-lightbox-thumb\" width=\"1572\" data-original=\"https://pic3.zhimg.com/v2-a5ffebbb7c7be7c9bf28cb458aeb786a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1572&#39; height=&#39;160&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1572\" data-rawheight=\"160\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1572\" data-original=\"https://pic3.zhimg.com/v2-a5ffebbb7c7be7c9bf28cb458aeb786a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-a5ffebbb7c7be7c9bf28cb458aeb786a_b.jpg\"/></figure><p>将上式用矩阵表示为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-d2067bf46bf839fdf05dc437687088b6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1538\" data-rawheight=\"114\" class=\"origin_image zh-lightbox-thumb\" width=\"1538\" data-original=\"https://pic3.zhimg.com/v2-d2067bf46bf839fdf05dc437687088b6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1538&#39; height=&#39;114&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1538\" data-rawheight=\"114\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1538\" data-original=\"https://pic3.zhimg.com/v2-d2067bf46bf839fdf05dc437687088b6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-d2067bf46bf839fdf05dc437687088b6_b.jpg\"/></figure><p>上式满足马尔可夫链的收敛性质，也就是说，只要我们找到可以使概率分布π(x)满足细致平稳分布的矩阵P即可。不过仅仅从细致平稳条件还是很难找到合适的矩阵P，比如我们的目标平稳分布使π(x)，随机找一个马尔可夫链状态转移矩阵Q，他是很难满足细致平稳条件的，即</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-f2b597a0fe05bbe9590f339ea465e504_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1566\" data-rawheight=\"110\" class=\"origin_image zh-lightbox-thumb\" width=\"1566\" data-original=\"https://pic1.zhimg.com/v2-f2b597a0fe05bbe9590f339ea465e504_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1566&#39; height=&#39;110&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1566\" data-rawheight=\"110\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1566\" data-original=\"https://pic1.zhimg.com/v2-f2b597a0fe05bbe9590f339ea465e504_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-f2b597a0fe05bbe9590f339ea465e504_b.jpg\"/></figure><p>那么有什么办法可以使这个等式相等呢？</p><h2>2.MCMC采样</h2><p>由于一般情况下，目标平稳分布π(x)和某一马尔可夫链状态转移矩阵Q不满足细致平稳条件，即</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-b875806012dd4dbfb76dc9d841478c73_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1536\" data-rawheight=\"106\" class=\"origin_image zh-lightbox-thumb\" width=\"1536\" data-original=\"https://pic4.zhimg.com/v2-b875806012dd4dbfb76dc9d841478c73_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1536&#39; height=&#39;106&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1536\" data-rawheight=\"106\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1536\" data-original=\"https://pic4.zhimg.com/v2-b875806012dd4dbfb76dc9d841478c73_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-b875806012dd4dbfb76dc9d841478c73_b.jpg\"/></figure><p>我们对上式进行一些变换，使细致平稳条件成立。方法是引入一个α(i,j)，使得上式等式能够成立，即</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-cbc37d4aece90a15633820da3f9ff882_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1548\" data-rawheight=\"112\" class=\"origin_image zh-lightbox-thumb\" width=\"1548\" data-original=\"https://pic3.zhimg.com/v2-cbc37d4aece90a15633820da3f9ff882_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1548&#39; height=&#39;112&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1548\" data-rawheight=\"112\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1548\" data-original=\"https://pic3.zhimg.com/v2-cbc37d4aece90a15633820da3f9ff882_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-cbc37d4aece90a15633820da3f9ff882_b.jpg\"/></figure><p>问题是什么样的α可以使上式成立？其实很简单，只要满足</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-593c3784c81d7c07542ec4e44ec9b9be_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1556\" data-rawheight=\"104\" class=\"origin_image zh-lightbox-thumb\" width=\"1556\" data-original=\"https://pic3.zhimg.com/v2-593c3784c81d7c07542ec4e44ec9b9be_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1556&#39; height=&#39;104&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1556\" data-rawheight=\"104\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1556\" data-original=\"https://pic3.zhimg.com/v2-593c3784c81d7c07542ec4e44ec9b9be_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-593c3784c81d7c07542ec4e44ec9b9be_b.jpg\"/></figure><p>这样，我们便找到使分布π(x)对应的马尔可夫链状态转移矩阵P，满足</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-2efd505beb0ec92a15a661eddaab941f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1576\" data-rawheight=\"116\" class=\"origin_image zh-lightbox-thumb\" width=\"1576\" data-original=\"https://pic4.zhimg.com/v2-2efd505beb0ec92a15a661eddaab941f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1576&#39; height=&#39;116&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1576\" data-rawheight=\"116\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1576\" data-original=\"https://pic4.zhimg.com/v2-2efd505beb0ec92a15a661eddaab941f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-2efd505beb0ec92a15a661eddaab941f_b.jpg\"/></figure><p>从上面可以得到，目标矩阵P可以通过任意一个马尔可夫链状态转移矩阵Q乘以α(i,j)得到。α(i,j)我们一般称之为接受率，取值在[0,1]之间，可以理解为一个概率值。也就是说，目标矩阵P可以通过任意一个马尔可夫链状态转移矩阵Q以一定的接受率得到。</p><p>其实很像我们在<a href=\"https://link.zhihu.com/?target=https%3A//weizhixiaoyi.com/2018/11/24/MCMC%25E4%25B9%258B%25E8%2592%2599%25E7%2589%25B9%25E5%258D%25A1%25E7%25BD%2597%25E6%2596%25B9%25E6%25B3%2595/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">MCMC之蒙特卡罗方法</a>中提到的接受-拒绝采样，那里是以常用分布通过一定的接受-拒绝概率得到一个非常见分布。这里是通过常见的马尔可夫链状态转移矩阵Q通过一定的接受-拒绝概率得到目标转移矩阵P，两者解决问题的思路是相同的。下面，我们来总结下MCMC的采样过程</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-b8dce1dd8413dfa9a50f374b73303027_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1634\" data-rawheight=\"542\" class=\"origin_image zh-lightbox-thumb\" width=\"1634\" data-original=\"https://pic4.zhimg.com/v2-b8dce1dd8413dfa9a50f374b73303027_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1634&#39; height=&#39;542&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1634\" data-rawheight=\"542\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1634\" data-original=\"https://pic4.zhimg.com/v2-b8dce1dd8413dfa9a50f374b73303027_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-b8dce1dd8413dfa9a50f374b73303027_b.jpg\"/></figure><p>上述过程便是MCMC采样理论，但很难在实际应用，为什么呢? 因为α可能非常小，比如0.1，导致大部分采样值都被拒绝转移，采样效率很低。可能我们采样可上百万次，马尔科夫链还没有收敛。实际应用中，我们可以通过M-H采样方法进行采样。</p><h2>3.M-H采样</h2><p>M-H采样解决了MCMC采样接受率过低的问题，我们首先回到MCMC采样的细致平稳条件</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-4ccb05c7547674c839a8af9d9941c258_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1608\" data-rawheight=\"116\" class=\"origin_image zh-lightbox-thumb\" width=\"1608\" data-original=\"https://pic1.zhimg.com/v2-4ccb05c7547674c839a8af9d9941c258_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1608&#39; height=&#39;116&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1608\" data-rawheight=\"116\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1608\" data-original=\"https://pic1.zhimg.com/v2-4ccb05c7547674c839a8af9d9941c258_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-4ccb05c7547674c839a8af9d9941c258_b.jpg\"/></figure><p>采样效率过低的原因是α(i,j)太小，比如0.1，α(j,i)为0.2，即</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-f918f7f908ed7df8fd97a701b1916794_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1552\" data-rawheight=\"108\" class=\"origin_image zh-lightbox-thumb\" width=\"1552\" data-original=\"https://pic1.zhimg.com/v2-f918f7f908ed7df8fd97a701b1916794_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1552&#39; height=&#39;108&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1552\" data-rawheight=\"108\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1552\" data-original=\"https://pic1.zhimg.com/v2-f918f7f908ed7df8fd97a701b1916794_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-f918f7f908ed7df8fd97a701b1916794_b.jpg\"/></figure><p>如果两边同时扩大5倍，细致平稳条件仍然是满足的，即</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-6dbd65e4691f5043ffee03e8ee16875d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1582\" data-rawheight=\"102\" class=\"origin_image zh-lightbox-thumb\" width=\"1582\" data-original=\"https://pic2.zhimg.com/v2-6dbd65e4691f5043ffee03e8ee16875d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1582&#39; height=&#39;102&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1582\" data-rawheight=\"102\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1582\" data-original=\"https://pic2.zhimg.com/v2-6dbd65e4691f5043ffee03e8ee16875d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-6dbd65e4691f5043ffee03e8ee16875d_b.jpg\"/></figure><p>这样我们可以对接受率做如下改进，即</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-19bf3fe29283f590cb7eccf5b9b39b43_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1570\" data-rawheight=\"144\" class=\"origin_image zh-lightbox-thumb\" width=\"1570\" data-original=\"https://pic4.zhimg.com/v2-19bf3fe29283f590cb7eccf5b9b39b43_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1570&#39; height=&#39;144&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1570\" data-rawheight=\"144\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1570\" data-original=\"https://pic4.zhimg.com/v2-19bf3fe29283f590cb7eccf5b9b39b43_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-19bf3fe29283f590cb7eccf5b9b39b43_b.jpg\"/></figure><p>通过上述的转换，我们便可在实际应用中使用M-H算法进行采样，M-H采样算法过程如下所示</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-1a6888a1c07667665f118d97be609f0a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1598\" data-rawheight=\"558\" class=\"origin_image zh-lightbox-thumb\" width=\"1598\" data-original=\"https://pic3.zhimg.com/v2-1a6888a1c07667665f118d97be609f0a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1598&#39; height=&#39;558&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1598\" data-rawheight=\"558\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1598\" data-original=\"https://pic3.zhimg.com/v2-1a6888a1c07667665f118d97be609f0a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-1a6888a1c07667665f118d97be609f0a_b.jpg\"/></figure><p>很多时候，我们选择的马尔科夫链状态转移矩阵Q如果是对称的，即满足Q(i,j)=Q(j,i)，这时我们的接受率可以进一步简化为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-df7da4a596d17da41dc8548b57c36b8a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1602\" data-rawheight=\"150\" class=\"origin_image zh-lightbox-thumb\" width=\"1602\" data-original=\"https://pic3.zhimg.com/v2-df7da4a596d17da41dc8548b57c36b8a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1602&#39; height=&#39;150&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1602\" data-rawheight=\"150\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1602\" data-original=\"https://pic3.zhimg.com/v2-df7da4a596d17da41dc8548b57c36b8a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-df7da4a596d17da41dc8548b57c36b8a_b.jpg\"/></figure><h2>4.M-H采样总结</h2><p>M-H采样解决了使用蒙特卡罗方法需要的任意概率分布样本集的问题，因此在实际生产环境中得到广泛应用。但在大数据情况下，M-H面临如下问题</p><ul><li><b>数据特征非常多：</b>因为M-H采样由于接受率的存在，在高维计算时需要很长的计算时间，算法效率很低。同时α(i,j)一般小于1，有时候辛苦计算出来的结果却被拒绝，能不能做到不拒绝转移呢？</li><li><b>特征维度比较大：</b>很多时候我们很难求出目标的各特征维度联合分布，但是可以方便求出各个特征之间的条件概率分布。这时能不能只用各维度之间的条件概率分布去方便的采样呢?</li></ul><h2>5.推广</h2><p>更多内容请关注公众号<b>谓之小一</b>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p><p><a href=\"https://link.zhihu.com/?target=http%3A//weixin.qq.com/r/Ty_4oDPE2W6mrXfD93pd\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">weixin.qq.com/r/Ty_4oDP</span><span class=\"invisible\">E2W6mrXfD93pd</span><span class=\"ellipsis\"></span></a> (二维码自动识别)</p><p></p>", 
            "topic": [
                {
                    "tag": "MCMC采样", 
                    "tagLink": "https://api.zhihu.com/topics/20080781"
                }, 
                {
                    "tag": "采样", 
                    "tagLink": "https://api.zhihu.com/topics/19691257"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/52475662", 
            "userName": "谓之小一", 
            "userLink": "https://www.zhihu.com/people/761dda159c5cec5fd926f761ab5f4ccf", 
            "upvote": 4, 
            "title": "MCMC之马尔可夫链", 
            "content": "<blockquote>由于较多公式，所以我将部分内容转为图片进行上传，请见谅。清晰版请访问<a href=\"https://link.zhihu.com/?target=https%3A//weizhixiaoyi.com\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">weizhixiaoyi.com</span><span class=\"invisible\"></span></a>查看，你也可以关注我公众号<b>谓之小一</b>，后台直接向我要pdf版本，如有相关问题可公众号后台询问，随时回答。</blockquote><p>在<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3MjA2NTQzMw%3D%3D%26mid%3D2247483993%26idx%3D1%26sn%3D811450f6af76945a5808f225d2566d17%26chksm%3Dfcd7d1afcba058b98b0acaad9775fc6a9c3791af4a5abf71bfc4a16a1c1b29edcb1d17f49e8d%26token%3D1376908250%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">MCMC之蒙特卡罗方法之中</a>，讲到如何利用蒙特卡罗方法来随机模拟求解一些复杂的连续积分或者离散求和方法。但蒙特卡罗方法需要得到对应的概率分布的样本集，而对于某些概率分布，得到这样的样本集很困难，因此本篇我们将介绍马尔可夫链来解决这种问题。</p><h2>1.马尔可夫链简介</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-2d33914b74502464b62bde911525572f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1630\" data-rawheight=\"260\" class=\"origin_image zh-lightbox-thumb\" width=\"1630\" data-original=\"https://pic4.zhimg.com/v2-2d33914b74502464b62bde911525572f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1630&#39; height=&#39;260&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1630\" data-rawheight=\"260\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1630\" data-original=\"https://pic4.zhimg.com/v2-2d33914b74502464b62bde911525572f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-2d33914b74502464b62bde911525572f_b.jpg\"/></figure><p>因为某一时刻状态转移只依赖于它的前一个状态，那么我们只要能求出系统中任意两个状态之间的转移概率，进而得到状态转移概率矩阵，那么马尔科夫链的模型便定了。以下图股市模型为例，共有三个状态，分别为牛市(Bull market)、熊市(Bear market)、横盘(Stagnant market)。每一个状态都能够以一定概率转移到下一状态，比如牛市以0.075的概率转移到横盘的概率，这些状态转移概率图可以转换为矩阵的形式进行表示。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-a2a05df1e2e45f253e87402157e9bbdf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"400\" data-rawheight=\"296\" class=\"content_image\" width=\"400\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;400&#39; height=&#39;296&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"400\" data-rawheight=\"296\" class=\"content_image lazy\" width=\"400\" data-actualsrc=\"https://pic4.zhimg.com/v2-a2a05df1e2e45f253e87402157e9bbdf_b.jpg\"/></figure><p>如果我们定义矩阵$P$某一位置P(i,j)的值为P(j|i)，即从状态i转移到状态j的概率，并定义牛市的状态为0、熊市状态为1、横盘状态为2，这样便得到马尔可夫链模型的状态转移矩阵。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b3c50b354464eab5593cc00df3db43a9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1556\" data-rawheight=\"198\" class=\"origin_image zh-lightbox-thumb\" width=\"1556\" data-original=\"https://pic2.zhimg.com/v2-b3c50b354464eab5593cc00df3db43a9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1556&#39; height=&#39;198&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1556\" data-rawheight=\"198\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1556\" data-original=\"https://pic2.zhimg.com/v2-b3c50b354464eab5593cc00df3db43a9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-b3c50b354464eab5593cc00df3db43a9_b.jpg\"/></figure><p>那么马尔科夫链模型的状态转移矩阵和蒙特卡罗方法所需要的概率分布样本集有什么关系呢？</p><h2>2.马尔可夫链状态转移矩阵性质</h2><p>得到马尔可夫链状态转移矩阵，我们看看马尔可夫链模型状态转移矩阵的性质。仍以上面的状态转移矩阵为例，假设当前股市的概率分布为[0.3, 0.4, 0.3]，即30%概率的牛市、40%概率的熊市、30%概率的横盘。如果以这个状态作为序列概率分布的初始状态t0，与状态转移矩阵计算得到t1,t2,t3,…时刻的概率，相应代码和结果如下。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"kn\">as</span> <span class=\"nn\">np</span>\n\n\n<span class=\"k\">def</span> <span class=\"nf\">markov_chain</span><span class=\"p\">():</span>\n    <span class=\"n\">matrix</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">matrix</span><span class=\"p\">([[</span><span class=\"mf\">0.9</span><span class=\"p\">,</span> <span class=\"mf\">0.075</span><span class=\"p\">,</span> <span class=\"mf\">0.025</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mf\">0.15</span><span class=\"p\">,</span> <span class=\"mf\">0.8</span><span class=\"p\">,</span> <span class=\"mf\">0.05</span><span class=\"p\">],</span>\n                        <span class=\"p\">[</span><span class=\"mf\">0.25</span><span class=\"p\">,</span> <span class=\"mf\">0.25</span><span class=\"p\">,</span> <span class=\"mf\">0.5</span><span class=\"p\">]],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"nb\">float</span><span class=\"p\">)</span>\n    <span class=\"n\">current</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">matrix</span><span class=\"p\">([[</span><span class=\"mf\">0.3</span><span class=\"p\">,</span> <span class=\"mf\">0.4</span><span class=\"p\">,</span> <span class=\"mf\">0.3</span><span class=\"p\">]],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"nb\">float</span><span class=\"p\">)</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">):</span>\n        <span class=\"n\">current</span> <span class=\"o\">=</span> <span class=\"n\">current</span> <span class=\"o\">*</span> <span class=\"n\">matrix</span>\n        <span class=\"k\">print</span> <span class=\"s2\">&#34;Current round:&#34;</span><span class=\"p\">,</span> <span class=\"n\">i</span> <span class=\"o\">+</span> <span class=\"mi\">1</span>\n        <span class=\"k\">print</span> <span class=\"n\">current</span>\n\n\n<span class=\"k\">if</span> <span class=\"vm\">__name__</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;__main__&#39;</span><span class=\"p\">:</span>\n    <span class=\"n\">markov_chain</span><span class=\"p\">()</span>\n<span class=\"n\">Current</span> <span class=\"nb\">round</span><span class=\"p\">:</span> <span class=\"mi\">1</span>\n<span class=\"p\">[[</span><span class=\"mf\">0.405</span>  <span class=\"mf\">0.4175</span> <span class=\"mf\">0.1775</span><span class=\"p\">]]</span>\n<span class=\"n\">Current</span> <span class=\"nb\">round</span><span class=\"p\">:</span> <span class=\"mi\">2</span>\n<span class=\"p\">[[</span><span class=\"mf\">0.4715</span>  <span class=\"mf\">0.40875</span> <span class=\"mf\">0.11975</span><span class=\"p\">]]</span>\n<span class=\"n\">Current</span> <span class=\"nb\">round</span><span class=\"p\">:</span> <span class=\"mi\">3</span>\n<span class=\"p\">[[</span><span class=\"mf\">0.5156</span> <span class=\"mf\">0.3923</span> <span class=\"mf\">0.0921</span><span class=\"p\">]]</span>\n<span class=\"n\">Current</span> <span class=\"nb\">round</span><span class=\"p\">:</span> <span class=\"mi\">4</span>\n<span class=\"p\">[[</span><span class=\"mf\">0.54591</span>  <span class=\"mf\">0.375535</span> <span class=\"mf\">0.078555</span><span class=\"p\">]]</span>\n<span class=\"o\">...</span>\n<span class=\"o\">...</span>\n<span class=\"n\">Current</span> <span class=\"nb\">round</span><span class=\"p\">:</span> <span class=\"mi\">58</span>\n<span class=\"p\">[[</span><span class=\"mf\">0.62499999</span> <span class=\"mf\">0.31250001</span> <span class=\"mf\">0.0625</span>    <span class=\"p\">]]</span>\n<span class=\"n\">Current</span> <span class=\"nb\">round</span><span class=\"p\">:</span> <span class=\"mi\">59</span>\n<span class=\"p\">[[</span><span class=\"mf\">0.62499999</span> <span class=\"mf\">0.3125</span>     <span class=\"mf\">0.0625</span>    <span class=\"p\">]]</span>\n<span class=\"n\">Current</span> <span class=\"nb\">round</span><span class=\"p\">:</span> <span class=\"mi\">60</span>\n<span class=\"p\">[[</span><span class=\"mf\">0.625</span>  <span class=\"mf\">0.3125</span> <span class=\"mf\">0.0625</span><span class=\"p\">]]</span>\n<span class=\"n\">Current</span> <span class=\"nb\">round</span><span class=\"p\">:</span> <span class=\"mi\">61</span>\n<span class=\"p\">[[</span><span class=\"mf\">0.625</span>  <span class=\"mf\">0.3125</span> <span class=\"mf\">0.0625</span><span class=\"p\">]]</span>\n<span class=\"n\">Current</span> <span class=\"nb\">round</span><span class=\"p\">:</span> <span class=\"mi\">62</span>\n<span class=\"p\">[[</span><span class=\"mf\">0.625</span>  <span class=\"mf\">0.3125</span> <span class=\"mf\">0.0625</span><span class=\"p\">]]</span>\n<span class=\"o\">...</span>\n<span class=\"o\">...</span>\n<span class=\"n\">Current</span> <span class=\"nb\">round</span><span class=\"p\">:</span> <span class=\"mi\">98</span>\n<span class=\"p\">[[</span><span class=\"mf\">0.625</span>  <span class=\"mf\">0.3125</span> <span class=\"mf\">0.0625</span><span class=\"p\">]]</span>\n<span class=\"n\">Current</span> <span class=\"nb\">round</span><span class=\"p\">:</span> <span class=\"mi\">99</span>\n<span class=\"p\">[[</span><span class=\"mf\">0.625</span>  <span class=\"mf\">0.3125</span> <span class=\"mf\">0.0625</span><span class=\"p\">]]</span>\n<span class=\"n\">Current</span> <span class=\"nb\">round</span><span class=\"p\">:</span> <span class=\"mi\">100</span>\n<span class=\"p\">[[</span><span class=\"mf\">0.625</span>  <span class=\"mf\">0.3125</span> <span class=\"mf\">0.0625</span><span class=\"p\">]]</span></code></pre></div><p>可以发现从第60轮开始，状态转移矩阵的概率分布就不变了，一直保持在[0.625, 0.3125, 0.0625]，即62.5%的概率的牛市、31.25%概率的熊市、6.25%概率的横盘，那么这个是巧合吗？</p><p>答案并不是巧合，如果我们换一个初始概率分布t0，比如以[0.7, 0.1, 0.2]作为初始概率分布，然后带入状态转移矩阵得到t1,t2,t3,…时刻的概率，会发现得到同样的结果。即最终的状态概率分布会趋于同一个稳定概率分布[0.625, 0.3125, 0.0625]，也就是说，马尔可夫链的状态转移矩阵收敛到稳定概率分布与初始状态概率分布无关。</p><p>上述结果是一个非常好的形式，比如我们得到了稳定概率分布所对应的马尔可夫链模型的状态转移矩阵，那么可以用任意的概率分布样本开始，带入马尔可夫链状态转移矩阵，然后就可以得到符合对应稳定概率分布的样本。这个性质不光对于上面的状态转移矩阵有效，对于绝大多数的其他马尔可夫链模型的状态转移矩阵也有效。同时不光是离散状态，连续状态情况下也成立。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-84b58e74c3be5818756b9be94984462d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1632\" data-rawheight=\"164\" class=\"origin_image zh-lightbox-thumb\" width=\"1632\" data-original=\"https://pic2.zhimg.com/v2-84b58e74c3be5818756b9be94984462d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1632&#39; height=&#39;164&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1632\" data-rawheight=\"164\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1632\" data-original=\"https://pic2.zhimg.com/v2-84b58e74c3be5818756b9be94984462d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-84b58e74c3be5818756b9be94984462d_b.jpg\"/></figure><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"kn\">as</span> <span class=\"nn\">np</span>\n\n\n<span class=\"k\">def</span> <span class=\"nf\">markov_chain</span><span class=\"p\">():</span>\n    <span class=\"n\">matrix</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">matrix</span><span class=\"p\">([[</span><span class=\"mf\">0.9</span><span class=\"p\">,</span> <span class=\"mf\">0.075</span><span class=\"p\">,</span> <span class=\"mf\">0.025</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mf\">0.15</span><span class=\"p\">,</span> <span class=\"mf\">0.8</span><span class=\"p\">,</span> <span class=\"mf\">0.05</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mf\">0.25</span><span class=\"p\">,</span> <span class=\"mf\">0.25</span><span class=\"p\">,</span> <span class=\"mf\">0.5</span><span class=\"p\">]],</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"nb\">float</span><span class=\"p\">)</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">):</span>\n        <span class=\"n\">matrix</span> <span class=\"o\">=</span> <span class=\"n\">matrix</span> <span class=\"o\">*</span> <span class=\"n\">matrix</span>\n        <span class=\"k\">print</span> <span class=\"s2\">&#34;Current round:&#34;</span><span class=\"p\">,</span> <span class=\"n\">i</span> <span class=\"o\">+</span> <span class=\"mi\">1</span>\n        <span class=\"k\">print</span> <span class=\"n\">matrix</span>\n\n\n<span class=\"k\">if</span> <span class=\"vm\">__name__</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;__main__&#39;</span><span class=\"p\">:</span>\n    <span class=\"n\">markov_chain</span><span class=\"p\">()</span>\n<span class=\"n\">Current</span> <span class=\"nb\">round</span><span class=\"p\">:</span> <span class=\"mi\">1</span>\n<span class=\"p\">[[</span><span class=\"mf\">0.8275</span>  <span class=\"mf\">0.13375</span> <span class=\"mf\">0.03875</span><span class=\"p\">]</span>\n <span class=\"p\">[</span><span class=\"mf\">0.2675</span>  <span class=\"mf\">0.66375</span> <span class=\"mf\">0.06875</span><span class=\"p\">]</span>\n <span class=\"p\">[</span><span class=\"mf\">0.3875</span>  <span class=\"mf\">0.34375</span> <span class=\"mf\">0.26875</span><span class=\"p\">]]</span>\n<span class=\"n\">Current</span> <span class=\"nb\">round</span><span class=\"p\">:</span> <span class=\"mi\">2</span>\n<span class=\"p\">[[</span><span class=\"mf\">0.73555</span>  <span class=\"mf\">0.212775</span> <span class=\"mf\">0.051675</span><span class=\"p\">]</span>\n <span class=\"p\">[</span><span class=\"mf\">0.42555</span>  <span class=\"mf\">0.499975</span> <span class=\"mf\">0.074475</span><span class=\"p\">]</span>\n <span class=\"p\">[</span><span class=\"mf\">0.51675</span>  <span class=\"mf\">0.372375</span> <span class=\"mf\">0.110875</span><span class=\"p\">]]</span>\n<span class=\"o\">...</span>\n<span class=\"n\">Current</span> <span class=\"nb\">round</span><span class=\"p\">:</span> <span class=\"mi\">5</span>\n<span class=\"p\">[[</span><span class=\"mf\">0.62502532</span> <span class=\"mf\">0.31247685</span> <span class=\"mf\">0.06249783</span><span class=\"p\">]</span>\n <span class=\"p\">[</span><span class=\"mf\">0.6249537</span>  <span class=\"mf\">0.31254233</span> <span class=\"mf\">0.06250397</span><span class=\"p\">]</span>\n <span class=\"p\">[</span><span class=\"mf\">0.62497828</span> <span class=\"mf\">0.31251986</span> <span class=\"mf\">0.06250186</span><span class=\"p\">]]</span>\n<span class=\"n\">Current</span> <span class=\"nb\">round</span><span class=\"p\">:</span> <span class=\"mi\">6</span>\n<span class=\"p\">[[</span><span class=\"mf\">0.625</span>  <span class=\"mf\">0.3125</span> <span class=\"mf\">0.0625</span><span class=\"p\">]</span>\n <span class=\"p\">[</span><span class=\"mf\">0.625</span>  <span class=\"mf\">0.3125</span> <span class=\"mf\">0.0625</span><span class=\"p\">]</span>\n <span class=\"p\">[</span><span class=\"mf\">0.625</span>  <span class=\"mf\">0.3125</span> <span class=\"mf\">0.0625</span><span class=\"p\">]]</span>\n<span class=\"o\">...</span>\n<span class=\"n\">Current</span> <span class=\"nb\">round</span><span class=\"p\">:</span> <span class=\"mi\">10</span>\n<span class=\"p\">[[</span><span class=\"mf\">0.625</span>  <span class=\"mf\">0.3125</span> <span class=\"mf\">0.0625</span><span class=\"p\">]</span>\n <span class=\"p\">[</span><span class=\"mf\">0.625</span>  <span class=\"mf\">0.3125</span> <span class=\"mf\">0.0625</span><span class=\"p\">]</span>\n <span class=\"p\">[</span><span class=\"mf\">0.625</span>  <span class=\"mf\">0.3125</span> <span class=\"mf\">0.0625</span><span class=\"p\">]]</span></code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-8eb46a135e487969de6f4b11e11c4f6f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1614\" data-rawheight=\"1142\" class=\"origin_image zh-lightbox-thumb\" width=\"1614\" data-original=\"https://pic4.zhimg.com/v2-8eb46a135e487969de6f4b11e11c4f6f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1614&#39; height=&#39;1142&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1614\" data-rawheight=\"1142\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1614\" data-original=\"https://pic4.zhimg.com/v2-8eb46a135e487969de6f4b11e11c4f6f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-8eb46a135e487969de6f4b11e11c4f6f_b.jpg\"/></figure><p>上面性质中，有几点需要特意说明</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-ce288aab255dd1afad703d8871c68e11_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1630\" data-rawheight=\"422\" class=\"origin_image zh-lightbox-thumb\" width=\"1630\" data-original=\"https://pic2.zhimg.com/v2-ce288aab255dd1afad703d8871c68e11_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1630&#39; height=&#39;422&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1630\" data-rawheight=\"422\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1630\" data-original=\"https://pic2.zhimg.com/v2-ce288aab255dd1afad703d8871c68e11_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-ce288aab255dd1afad703d8871c68e11_b.jpg\"/></figure><h2>3.基于马尔可夫链采样</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-53c6fb26911414e243bdde0be240d3b4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1656\" data-rawheight=\"504\" class=\"origin_image zh-lightbox-thumb\" width=\"1656\" data-original=\"https://pic1.zhimg.com/v2-53c6fb26911414e243bdde0be240d3b4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1656&#39; height=&#39;504&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1656\" data-rawheight=\"504\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1656\" data-original=\"https://pic1.zhimg.com/v2-53c6fb26911414e243bdde0be240d3b4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-53c6fb26911414e243bdde0be240d3b4_b.jpg\"/></figure><h2>4.马尔可夫链总结</h2><p>如果假定我们可以得到所需要采样样本的平稳分布所对应的马尔可夫链状态转移矩阵，那么我们就可以用马尔可夫链采样得到我们需要的样本集，进而进行蒙特卡罗模拟。但是现在还有个很重要的问题，随意给定一个平稳分布$\\pi$，如何得到它所对应的马尔可夫链状态转移矩阵P呢？下篇文章，我们将重点介绍MCMC采用通过与会的方式解决上述问题，以及改进版的M-H采样和Gibbs采样。</p><h2>5.推广</h2><p>更多内容请关注公众号<b>谓之小一</b>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p><p><a href=\"https://link.zhihu.com/?target=http%3A//weixin.qq.com/r/Ty_4oDPE2W6mrXfD93pd\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">weixin.qq.com/r/Ty_4oDP</span><span class=\"invisible\">E2W6mrXfD93pd</span><span class=\"ellipsis\"></span></a> (二维码自动识别)</p><p></p>", 
            "topic": [
                {
                    "tag": "统计学", 
                    "tagLink": "https://api.zhihu.com/topics/19558740"
                }, 
                {
                    "tag": "马尔可夫链蒙特卡罗方法", 
                    "tagLink": "https://api.zhihu.com/topics/20687817"
                }, 
                {
                    "tag": "MCMC采样", 
                    "tagLink": "https://api.zhihu.com/topics/20080781"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/52474066", 
            "userName": "谓之小一", 
            "userLink": "https://www.zhihu.com/people/761dda159c5cec5fd926f761ab5f4ccf", 
            "upvote": 1, 
            "title": "MCMC之蒙特卡罗方法", 
            "content": "<blockquote>由于较多公式，所以我将部分内容转为图片进行上传，请见谅。清晰版请访问<a href=\"https://link.zhihu.com/?target=https%3A//weizhixiaoyi.com\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">weizhixiaoyi.com</span><span class=\"invisible\"></span></a>查看，你也可以关注我公众号<b>谓之小一</b>，后台直接向我要pdf版本，如有相关问题可公众号后台询问，随时回答。</blockquote><h2>1.MCMC简介</h2><p><b>马尔可夫链蒙克卡罗(Markov Chain Monte Carlo,MCMC)</b>是一种随机采样方法，在机器学习、深度学习及自然语言处理等领域都有广泛的应用，是很多复杂算法求解的基础，例如受限玻尔兹曼机(RBM)便是用MCMC来做一些复杂算法的近似求解。在具体讲解什么是MCMC之前，我们先看看MCMC可以解决什么样的问题，为什么需要MCMC方法。</p><h2>2. 为什么需要MCMC?</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-9afce82d82fffb973d3a578d61cc9b86_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1626\" data-rawheight=\"218\" class=\"origin_image zh-lightbox-thumb\" width=\"1626\" data-original=\"https://pic3.zhimg.com/v2-9afce82d82fffb973d3a578d61cc9b86_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1626&#39; height=&#39;218&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1626\" data-rawheight=\"218\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1626\" data-original=\"https://pic3.zhimg.com/v2-9afce82d82fffb973d3a578d61cc9b86_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-9afce82d82fffb973d3a578d61cc9b86_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-63f9dec8d71be9a0ac1eb5a53c7637aa_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1606\" data-rawheight=\"308\" class=\"origin_image zh-lightbox-thumb\" width=\"1606\" data-original=\"https://pic3.zhimg.com/v2-63f9dec8d71be9a0ac1eb5a53c7637aa_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1606&#39; height=&#39;308&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1606\" data-rawheight=\"308\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1606\" data-original=\"https://pic3.zhimg.com/v2-63f9dec8d71be9a0ac1eb5a53c7637aa_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-63f9dec8d71be9a0ac1eb5a53c7637aa_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-1c37005226a38f3b3558572d9e000937_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"561\" data-rawheight=\"420\" class=\"origin_image zh-lightbox-thumb\" width=\"561\" data-original=\"https://pic4.zhimg.com/v2-1c37005226a38f3b3558572d9e000937_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;561&#39; height=&#39;420&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"561\" data-rawheight=\"420\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"561\" data-original=\"https://pic4.zhimg.com/v2-1c37005226a38f3b3558572d9e000937_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-1c37005226a38f3b3558572d9e000937_b.jpg\"/></figure><p>累积分布函数如下所示</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-579d402db8bf3a6d4b4ba03282faa85e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1566\" data-rawheight=\"190\" class=\"origin_image zh-lightbox-thumb\" width=\"1566\" data-original=\"https://pic3.zhimg.com/v2-579d402db8bf3a6d4b4ba03282faa85e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1566&#39; height=&#39;190&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1566\" data-rawheight=\"190\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1566\" data-original=\"https://pic3.zhimg.com/v2-579d402db8bf3a6d4b4ba03282faa85e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-579d402db8bf3a6d4b4ba03282faa85e_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-22578e3f5f06da8e0338be8230d58bd6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"561\" data-rawheight=\"420\" class=\"origin_image zh-lightbox-thumb\" width=\"561\" data-original=\"https://pic3.zhimg.com/v2-22578e3f5f06da8e0338be8230d58bd6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;561&#39; height=&#39;420&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"561\" data-rawheight=\"420\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"561\" data-original=\"https://pic3.zhimg.com/v2-22578e3f5f06da8e0338be8230d58bd6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-22578e3f5f06da8e0338be8230d58bd6_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-4d31faab1b74f407558bc47570ced1e1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1622\" data-rawheight=\"124\" class=\"origin_image zh-lightbox-thumb\" width=\"1622\" data-original=\"https://pic2.zhimg.com/v2-4d31faab1b74f407558bc47570ced1e1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1622&#39; height=&#39;124&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1622\" data-rawheight=\"124\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1622\" data-original=\"https://pic2.zhimg.com/v2-4d31faab1b74f407558bc47570ced1e1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-4d31faab1b74f407558bc47570ced1e1_b.jpg\"/></figure><p>上面针对连续分布采样有两个假设前提：一是概率密度函数可积；二是累积分布函数有反函数。但是上述假设前提不成立怎么办？此时便需要用到下面介绍的MCMC。</p><h2>3.蒙特卡罗方法</h2><p>我们首先介绍MCMC中的蒙特卡罗(Monte Carlo)方法，蒙特卡罗是一种随机模拟的方法，最初的蒙特卡罗方法是用来求解积分问题，比如</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-c51607fb519550775091c699f1414a3b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1590\" data-rawheight=\"152\" class=\"origin_image zh-lightbox-thumb\" width=\"1590\" data-original=\"https://pic4.zhimg.com/v2-c51607fb519550775091c699f1414a3b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1590&#39; height=&#39;152&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1590\" data-rawheight=\"152\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1590\" data-original=\"https://pic4.zhimg.com/v2-c51607fb519550775091c699f1414a3b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-c51607fb519550775091c699f1414a3b_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-46e58f66642bcf5485b6a44e00513ff2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"732\" data-rawheight=\"482\" class=\"origin_image zh-lightbox-thumb\" width=\"732\" data-original=\"https://pic3.zhimg.com/v2-46e58f66642bcf5485b6a44e00513ff2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;732&#39; height=&#39;482&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"732\" data-rawheight=\"482\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"732\" data-original=\"https://pic3.zhimg.com/v2-46e58f66642bcf5485b6a44e00513ff2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-46e58f66642bcf5485b6a44e00513ff2_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-fc49dd0944d2fea888beafaf15a050c6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1614\" data-rawheight=\"524\" class=\"origin_image zh-lightbox-thumb\" width=\"1614\" data-original=\"https://pic3.zhimg.com/v2-fc49dd0944d2fea888beafaf15a050c6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1614&#39; height=&#39;524&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1614\" data-rawheight=\"524\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1614\" data-original=\"https://pic3.zhimg.com/v2-fc49dd0944d2fea888beafaf15a050c6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-fc49dd0944d2fea888beafaf15a050c6_b.jpg\"/></figure><p>虽然上面的方法可以求解近似值，但它隐含一个假设，即x在[a,b]之间是均匀分布的。而大多数情况下，x在[a,b]之间不是均匀分布的，如果继续用上面的方法，模拟求出的结果很可能和结果相差甚远，那怎么解决这个问题呢？如果我们可以得到x在[a,b]的概率分布函数p(x)，那么我们的定积分求和可以转换为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-fc1ef3d2e37981f42ed7307016446d4b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1602\" data-rawheight=\"178\" class=\"origin_image zh-lightbox-thumb\" width=\"1602\" data-original=\"https://pic4.zhimg.com/v2-fc1ef3d2e37981f42ed7307016446d4b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1602&#39; height=&#39;178&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1602\" data-rawheight=\"178\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1602\" data-original=\"https://pic4.zhimg.com/v2-fc1ef3d2e37981f42ed7307016446d4b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-fc1ef3d2e37981f42ed7307016446d4b_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-f319be2bf5764bdc43863237c2344c61_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1622\" data-rawheight=\"256\" class=\"origin_image zh-lightbox-thumb\" width=\"1622\" data-original=\"https://pic2.zhimg.com/v2-f319be2bf5764bdc43863237c2344c61_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1622&#39; height=&#39;256&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1622\" data-rawheight=\"256\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1622\" data-original=\"https://pic2.zhimg.com/v2-f319be2bf5764bdc43863237c2344c61_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-f319be2bf5764bdc43863237c2344c61_b.jpg\"/></figure><p>也就是说，最上面的均匀分布可以作为一般概率分布函数p(x)在均匀分布时候的特例，那么现在问题便转换为<b>如何求出x的分布p(x)对应的若干个样本上来</b>。</p><h2>4.概率分布采样</h2><p>上面讲到蒙特卡罗方法的关键是得到x的概率分布p(x)，如果求出了x的概率分布，便可以基于这个概率分布去采样n个x的样本集，然后带入蒙特卡罗求和的方程式便可以求解。当然，上面的关键问题还没有解决，即如何基于概率分布去采样n个x的样本集。</p><p>对于常见的均匀分布uniform(0,1)是非常容易采集样本的，一般通过线性同余发生器便可以很方便的生成(0,1)之间的伪随机数样本。对于常见的概率分布，无论是离散的分布还是连续的分布，都可以通过uniform(0,1)的样本转换得到。比如二维正态分布的样本(Z1,Z2)，可以通过独立采样得到uniform(0,1)样本对(X1,X2)进行转换得到，转换方程式如下所示</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-f55ecfb56345ec33a7dbeebd37eb05f2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1608\" data-rawheight=\"212\" class=\"origin_image zh-lightbox-thumb\" width=\"1608\" data-original=\"https://pic3.zhimg.com/v2-f55ecfb56345ec33a7dbeebd37eb05f2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1608&#39; height=&#39;212&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1608\" data-rawheight=\"212\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1608\" data-original=\"https://pic3.zhimg.com/v2-f55ecfb56345ec33a7dbeebd37eb05f2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-f55ecfb56345ec33a7dbeebd37eb05f2_b.jpg\"/></figure><p>其他的一些常见的连续分布，比如t分布,F分布,Beta分布,Gamma分布等，都可以通过类似的方式从uniform(0,1)得到的样本转换得到。不过很多时候，我们<b>x的概率分布不是常见的分布</b>，这意味着我们无法方便的得到这些概率分布的样本集，那这个问题怎么解决呢？</p><h2>5.接受-拒绝采样</h2><p>对于概率分布不是常见的分布，一个可行的方法是采用接受-拒绝采样来得到该分布的样本。既然p(x)太复杂在程序中没法直接采样，那么我们设定一个可采样的分布q(x)，比如高斯分布，然后按照一定的方法拒绝某些样本，以达到接近p(x)分布的目的，其中q(x)叫做建议分布(proposal distribution)。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-7a35d07e329239597e8437b9e1410d2f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"566\" data-rawheight=\"291\" class=\"origin_image zh-lightbox-thumb\" width=\"566\" data-original=\"https://pic4.zhimg.com/v2-7a35d07e329239597e8437b9e1410d2f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;566&#39; height=&#39;291&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"566\" data-rawheight=\"291\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"566\" data-original=\"https://pic4.zhimg.com/v2-7a35d07e329239597e8437b9e1410d2f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-7a35d07e329239597e8437b9e1410d2f_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-f894df1536ca97237b3e1f788219198f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1608\" data-rawheight=\"304\" class=\"origin_image zh-lightbox-thumb\" width=\"1608\" data-original=\"https://pic4.zhimg.com/v2-f894df1536ca97237b3e1f788219198f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1608&#39; height=&#39;304&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1608\" data-rawheight=\"304\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1608\" data-original=\"https://pic4.zhimg.com/v2-f894df1536ca97237b3e1f788219198f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-f894df1536ca97237b3e1f788219198f_b.jpg\"/></figure><h2>6.蒙特卡罗方法总结</h2><p>使用接受-拒绝采样，可以解决一些概率分布不是常见分布的情况，然后得到采样集，最后用蒙特卡罗方法求和。但是接受-拒绝采样只能部分满足我们的情况，在很多时候我们还是很难得到概率分布的样本集，比如</p><ul><li>对于一些二维分布(x,y)，有些时候只能得到条件概率分布p(x|y)和p(y|x)，却很难得到二维分布p(x,y)，这时便无法采用接受拒绝采样得到其样本集。</li><li>对于一些高维的复杂分布p(x1,x2,x3,...,xn)，得到合适的q(x)和k非常困难。</li></ul><p>从上面可以看出，要将蒙特卡罗方法作为通用的采样模拟求和方法，必须解决如何方便得到<b>各种复杂概率分布的对应采样样本</b>的问题。下篇文章，我们将通过介绍马尔可夫链，来帮助我们找到这些复杂概率分布所对应的采样样本集。</p><h2>7.推广</h2><p>更多内容请关注公众号<b>谓之小一</b>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p><p><a href=\"https://link.zhihu.com/?target=http%3A//weixin.qq.com/r/Ty_4oDPE2W6mrXfD93pd\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">weixin.qq.com/r/Ty_4oDP</span><span class=\"invisible\">E2W6mrXfD93pd</span><span class=\"ellipsis\"></span></a> (二维码自动识别)</p><p></p>", 
            "topic": [
                {
                    "tag": "蒙特卡洛方法", 
                    "tagLink": "https://api.zhihu.com/topics/19800937"
                }, 
                {
                    "tag": "统计学", 
                    "tagLink": "https://api.zhihu.com/topics/19558740"
                }, 
                {
                    "tag": "算法", 
                    "tagLink": "https://api.zhihu.com/topics/19553510"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/49995824", 
            "userName": "谓之小一", 
            "userLink": "https://www.zhihu.com/people/761dda159c5cec5fd926f761ab5f4ccf", 
            "upvote": 5, 
            "title": "LSTM神经网络之前向反向传播算法", 
            "content": "<blockquote>由于较多公式，所以我将部分内容转为图片进行上传，请见谅。清晰版请访问<a href=\"https://link.zhihu.com/?target=https%3A//weizhixiaoyi.com\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">weizhixiaoyi.com</span><span class=\"invisible\"></span></a>查看，你也可以关注我公众号<b>谓之小一</b>，后台直接向我要pdf版本，如有相关问题可公众号后台询问，随时回答。</blockquote><p>上篇文章我们已经学习了<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3MjA2NTQzMw%3D%3D%26mid%3D2247483980%26idx%3D1%26sn%3D55041808b3f6cd25a6bb40a9586e121f%26chksm%3Dfcd7d1bacba058ac6e9f077f5a8e606eaa032b0fd5d86964c26b7ff83433c179ebc6f3d5f439%26token%3D749555039%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">循环神经网络</a>的原理，并指出RNN存在严重的梯度爆炸和梯度消失问题，因此很难处理长序列的数据。本篇文章，我们将学习长短期记忆网络(<b>LSTM,Long Short Term Memory)</b>，看LSTM解决RNN所带来的梯度消失和梯度爆炸问题。</p><h2>1.从RNN到LSTM</h2><p><a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3MjA2NTQzMw%3D%3D%26mid%3D2247483980%26idx%3D1%26sn%3D55041808b3f6cd25a6bb40a9586e121f%26chksm%3Dfcd7d1bacba058ac6e9f077f5a8e606eaa032b0fd5d86964c26b7ff83433c179ebc6f3d5f439%26token%3D749555039%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">RNN</a>模型具有如下所示的结构，其中每个索引位置t都有一个隐藏状态h(t)。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-891ce7c62ffcfbc761ca4d8465412582_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"644\" data-rawheight=\"416\" class=\"origin_image zh-lightbox-thumb\" width=\"644\" data-original=\"https://pic3.zhimg.com/v2-891ce7c62ffcfbc761ca4d8465412582_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;644&#39; height=&#39;416&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"644\" data-rawheight=\"416\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"644\" data-original=\"https://pic3.zhimg.com/v2-891ce7c62ffcfbc761ca4d8465412582_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-891ce7c62ffcfbc761ca4d8465412582_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-a3b16457484920d4bcde7c32b86aea87_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1594\" data-rawheight=\"110\" class=\"origin_image zh-lightbox-thumb\" width=\"1594\" data-original=\"https://pic4.zhimg.com/v2-a3b16457484920d4bcde7c32b86aea87_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1594&#39; height=&#39;110&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1594\" data-rawheight=\"110\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1594\" data-original=\"https://pic4.zhimg.com/v2-a3b16457484920d4bcde7c32b86aea87_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-a3b16457484920d4bcde7c32b86aea87_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-e05d7f04207ef4fcd70fe662e519e95f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"769\" data-rawheight=\"290\" class=\"origin_image zh-lightbox-thumb\" width=\"769\" data-original=\"https://pic4.zhimg.com/v2-e05d7f04207ef4fcd70fe662e519e95f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;769&#39; height=&#39;290&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"769\" data-rawheight=\"290\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"769\" data-original=\"https://pic4.zhimg.com/v2-e05d7f04207ef4fcd70fe662e519e95f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-e05d7f04207ef4fcd70fe662e519e95f_b.jpg\"/></figure><p>为解决梯度消失的问题，大牛们针对RNN序列索引位置<b>t</b>的隐藏结构作出相应改进，进而提出LSTM模型。其中LSTM模型有多种形式，下面我们以最常见的LSTM模型为例进行讲解。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-bd1f88ed4748a7f5f347287f07e9292a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"756\" data-rawheight=\"290\" class=\"origin_image zh-lightbox-thumb\" width=\"756\" data-original=\"https://pic3.zhimg.com/v2-bd1f88ed4748a7f5f347287f07e9292a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;756&#39; height=&#39;290&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"756\" data-rawheight=\"290\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"756\" data-original=\"https://pic3.zhimg.com/v2-bd1f88ed4748a7f5f347287f07e9292a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-bd1f88ed4748a7f5f347287f07e9292a_b.jpg\"/></figure><h2>2.LSTM模型结构</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-4f9af04f3290b3067bfd0497c941b1df_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1616\" data-rawheight=\"108\" class=\"origin_image zh-lightbox-thumb\" width=\"1616\" data-original=\"https://pic4.zhimg.com/v2-4f9af04f3290b3067bfd0497c941b1df_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1616&#39; height=&#39;108&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1616\" data-rawheight=\"108\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1616\" data-original=\"https://pic4.zhimg.com/v2-4f9af04f3290b3067bfd0497c941b1df_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-4f9af04f3290b3067bfd0497c941b1df_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-d7cc6ac9b9683f456fd9234909be8207_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"379\" data-rawheight=\"230\" class=\"content_image\" width=\"379\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;379&#39; height=&#39;230&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"379\" data-rawheight=\"230\" class=\"content_image lazy\" width=\"379\" data-actualsrc=\"https://pic4.zhimg.com/v2-d7cc6ac9b9683f456fd9234909be8207_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>除了细胞状态外，LSTM中还多了很多奇怪的结构，称之为门控结构(Gate)。针对每个序列索引位置<b>t</b>，门控结构一般包含遗忘门、输入门和输出门，下面来看看门控结构和细胞状态的结构。</p><h2>2.1 LSTM之遗忘门</h2><p><b>遗忘门(forget gate)</b>是以一定的概率控制是否遗忘上一层的隐藏细胞状态，遗忘门的结构如下所示。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-81e14c41f2adef9d45e8a417429edbec_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"379\" data-rawheight=\"232\" class=\"content_image\" width=\"379\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;379&#39; height=&#39;232&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"379\" data-rawheight=\"232\" class=\"content_image lazy\" width=\"379\" data-actualsrc=\"https://pic1.zhimg.com/v2-81e14c41f2adef9d45e8a417429edbec_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-14aeaaefbd3fdb64ec8e3291547edcc3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1594\" data-rawheight=\"260\" class=\"origin_image zh-lightbox-thumb\" width=\"1594\" data-original=\"https://pic4.zhimg.com/v2-14aeaaefbd3fdb64ec8e3291547edcc3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1594&#39; height=&#39;260&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1594\" data-rawheight=\"260\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1594\" data-original=\"https://pic4.zhimg.com/v2-14aeaaefbd3fdb64ec8e3291547edcc3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-14aeaaefbd3fdb64ec8e3291547edcc3_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>2.2 LSTM之输入门</h2><p><b>输入门(input gate)</b>负责处理当前序列位置的输入，输入门的结构如下所示。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-6f8fe3f114623773de2a79e62e43bf98_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"393\" data-rawheight=\"230\" class=\"content_image\" width=\"393\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;393&#39; height=&#39;230&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"393\" data-rawheight=\"230\" class=\"content_image lazy\" width=\"393\" data-actualsrc=\"https://pic1.zhimg.com/v2-6f8fe3f114623773de2a79e62e43bf98_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-2fa1d3b2ebfa05011bdb36adff6ff5ba_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1618\" data-rawheight=\"334\" class=\"origin_image zh-lightbox-thumb\" width=\"1618\" data-original=\"https://pic3.zhimg.com/v2-2fa1d3b2ebfa05011bdb36adff6ff5ba_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1618&#39; height=&#39;334&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1618\" data-rawheight=\"334\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1618\" data-original=\"https://pic3.zhimg.com/v2-2fa1d3b2ebfa05011bdb36adff6ff5ba_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-2fa1d3b2ebfa05011bdb36adff6ff5ba_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>2.3 LSTM之细胞状态更新</h2><p>研究LSTM输出门之前，我们先看一下LSTM细胞状态的更新，其中遗忘门和输入门的结果都作用于细胞状态C(t)。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-ad2a005f0c57667908de316bb1a0c091_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"382\" data-rawheight=\"228\" class=\"content_image\" width=\"382\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;382&#39; height=&#39;228&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"382\" data-rawheight=\"228\" class=\"content_image lazy\" width=\"382\" data-actualsrc=\"https://pic2.zhimg.com/v2-ad2a005f0c57667908de316bb1a0c091_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-8c8fcd2a4d1412a9ba8ff1173a9cccfa_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1604\" data-rawheight=\"226\" class=\"origin_image zh-lightbox-thumb\" width=\"1604\" data-original=\"https://pic3.zhimg.com/v2-8c8fcd2a4d1412a9ba8ff1173a9cccfa_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1604&#39; height=&#39;226&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1604\" data-rawheight=\"226\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1604\" data-original=\"https://pic3.zhimg.com/v2-8c8fcd2a4d1412a9ba8ff1173a9cccfa_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-8c8fcd2a4d1412a9ba8ff1173a9cccfa_b.jpg\"/></figure><h2>2.4 LSTM之输出门</h2><p>有了新的隐藏细胞状态C(t)，便可以来看输出门，其结构如下所示。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-3b5dba9fac974feb43db74813133b4df_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"384\" data-rawheight=\"242\" class=\"content_image\" width=\"384\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;384&#39; height=&#39;242&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"384\" data-rawheight=\"242\" class=\"content_image lazy\" width=\"384\" data-actualsrc=\"https://pic4.zhimg.com/v2-3b5dba9fac974feb43db74813133b4df_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-afac8fa0f106f3c7091bee5d693c059a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1616\" data-rawheight=\"300\" class=\"origin_image zh-lightbox-thumb\" width=\"1616\" data-original=\"https://pic3.zhimg.com/v2-afac8fa0f106f3c7091bee5d693c059a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1616&#39; height=&#39;300&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1616\" data-rawheight=\"300\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1616\" data-original=\"https://pic3.zhimg.com/v2-afac8fa0f106f3c7091bee5d693c059a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-afac8fa0f106f3c7091bee5d693c059a_b.jpg\"/></figure><h2>3.LSTM之前向传播算法</h2><p>通过上面的介绍，已经能够得到LSTM前向传播算法主要包括更新遗忘门输出、更新输入门、更新细胞状态、更新输出门、更新当前序列索引预测输出，各传播过程如下所示。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-38440357e528f878aa2da893379c238b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1606\" data-rawheight=\"1010\" class=\"origin_image zh-lightbox-thumb\" width=\"1606\" data-original=\"https://pic4.zhimg.com/v2-38440357e528f878aa2da893379c238b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1606&#39; height=&#39;1010&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1606\" data-rawheight=\"1010\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1606\" data-original=\"https://pic4.zhimg.com/v2-38440357e528f878aa2da893379c238b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-38440357e528f878aa2da893379c238b_b.jpg\"/></figure><h2>4.LSTM之反向传播算法</h2><p>了解前向传播算法流程之后，对于反向传播算法就非常简单了。我们采用和RNN相同的反向传播算法思路，即通过梯度下降法迭代更新所有的参数。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-4b596f1b9bfcb387ff204c4f5ba80136_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1600\" data-rawheight=\"460\" class=\"origin_image zh-lightbox-thumb\" width=\"1600\" data-original=\"https://pic3.zhimg.com/v2-4b596f1b9bfcb387ff204c4f5ba80136_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1600&#39; height=&#39;460&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1600\" data-rawheight=\"460\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1600\" data-original=\"https://pic3.zhimg.com/v2-4b596f1b9bfcb387ff204c4f5ba80136_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-4b596f1b9bfcb387ff204c4f5ba80136_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-fcd010834034b603102d3d7b866c3237_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1432\" data-rawheight=\"1568\" class=\"origin_image zh-lightbox-thumb\" width=\"1432\" data-original=\"https://pic4.zhimg.com/v2-fcd010834034b603102d3d7b866c3237_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1432&#39; height=&#39;1568&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1432\" data-rawheight=\"1568\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1432\" data-original=\"https://pic4.zhimg.com/v2-fcd010834034b603102d3d7b866c3237_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-fcd010834034b603102d3d7b866c3237_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-8c19439bd1121758242dbcf6a6e61619_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1628\" data-rawheight=\"934\" class=\"origin_image zh-lightbox-thumb\" width=\"1628\" data-original=\"https://pic2.zhimg.com/v2-8c19439bd1121758242dbcf6a6e61619_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1628&#39; height=&#39;934&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1628\" data-rawheight=\"934\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1628\" data-original=\"https://pic2.zhimg.com/v2-8c19439bd1121758242dbcf6a6e61619_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-8c19439bd1121758242dbcf6a6e61619_b.jpg\"/></figure><h2>5.LSTM怎么解决梯度消失和梯度爆炸</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-1ad14deb04dfb48f5722a1d447743e97_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1630\" data-rawheight=\"1012\" class=\"origin_image zh-lightbox-thumb\" width=\"1630\" data-original=\"https://pic4.zhimg.com/v2-1ad14deb04dfb48f5722a1d447743e97_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1630&#39; height=&#39;1012&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1630\" data-rawheight=\"1012\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1630\" data-original=\"https://pic4.zhimg.com/v2-1ad14deb04dfb48f5722a1d447743e97_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-1ad14deb04dfb48f5722a1d447743e97_b.jpg\"/></figure><h2>6.LSTM总结</h2><p>LSTM虽然复杂，但能够很好的解决梯度消失和梯度爆炸的问题，只要我们理清各部分之间的关系，进而理解前向和反向传播算法还是不难的。针对RNN和LSTM之中的梯度消失和梯度爆炸的描述，如果有相应错误，欢迎指出。</p><h2>7.推广</h2><p>更多内容请关注公众号<b>谓之小一</b>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p><p><a href=\"https://link.zhihu.com/?target=http%3A//weixin.qq.com/r/Ty_4oDPE2W6mrXfD93pd\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">weixin.qq.com/r/Ty_4oDP</span><span class=\"invisible\">E2W6mrXfD93pd</span><span class=\"ellipsis\"></span></a> (二维码自动识别)</p><p></p>", 
            "topic": [
                {
                    "tag": "神经网络", 
                    "tagLink": "https://api.zhihu.com/topics/19607065"
                }, 
                {
                    "tag": "LSTM", 
                    "tagLink": "https://api.zhihu.com/topics/20023220"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/48519740", 
            "userName": "谓之小一", 
            "userLink": "https://www.zhihu.com/people/761dda159c5cec5fd926f761ab5f4ccf", 
            "upvote": 3, 
            "title": "循环神经网络之前向反向传播算法", 
            "content": "<blockquote>由于较多公式，所以我将部分内容转为图片进行上传，请见谅。清晰版请访问<a href=\"https://link.zhihu.com/?target=https%3A//weizhixiaoyi.com\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">weizhixiaoyi.com</span><span class=\"invisible\"></span></a>查看，你也可以关注我公众号<b>谓之小一</b>，后台直接向我要pdf版本，如有相关问题可公众号后台询问，随时回答。</blockquote><p>前面我们已经介绍了<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3MjA2NTQzMw%3D%3D%26mid%3D2247483903%26idx%3D1%26sn%3D4e3f92578399013eba9f203d35afe972%26chksm%3Dfcd7d209cba05b1ffc66494ea8008c669e40f3045398695b479aba14e1c425f85b7c8f033c4f%26token%3D1135020617%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">深度神经网络</a>和<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3MjA2NTQzMw%3D%3D%26mid%3D2247483958%26idx%3D1%26sn%3D4527cba9c60fe8d634c5132398d17827%26chksm%3Dfcd7d1c0cba058d6c7d331affcaa3bf1a9bb532959a857763c3c31f069f1fcb343c67c89652a%26token%3D1135020617%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">卷积神经网络</a>，这些算法都是前向反馈，模型的输出和模型本身没有关联关系。今天我们学习输出和模型间有反馈的神经网络，循环神经网络(Recurrent Neual Networks)，其广泛应用于自然语言处理中的语音识别，书写识别和机器翻译等领域。</p><h2>1.RNN简介</h2><p>前面介绍的DNN和CNN之中，训练样本的输入和输出都是确定的。但对于训练样本输入是连续的序列，训练样本长度不同的样本，比如一段连续的语音和手写文字，DNN和CNN是比较难处理的。而对于上述问题，RNN则是比较擅长，那么RNN是怎么做到的呢？</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-8d76a4eb376a639307f2e8288ea3f00a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1272\" data-rawheight=\"292\" class=\"origin_image zh-lightbox-thumb\" width=\"1272\" data-original=\"https://pic3.zhimg.com/v2-8d76a4eb376a639307f2e8288ea3f00a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1272&#39; height=&#39;292&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1272\" data-rawheight=\"292\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1272\" data-original=\"https://pic3.zhimg.com/v2-8d76a4eb376a639307f2e8288ea3f00a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-8d76a4eb376a639307f2e8288ea3f00a_b.jpg\"/></figure><h2>2.RNN模型</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-891ce7c62ffcfbc761ca4d8465412582_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"644\" data-rawheight=\"416\" class=\"origin_image zh-lightbox-thumb\" width=\"644\" data-original=\"https://pic3.zhimg.com/v2-891ce7c62ffcfbc761ca4d8465412582_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;644&#39; height=&#39;416&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"644\" data-rawheight=\"416\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"644\" data-original=\"https://pic3.zhimg.com/v2-891ce7c62ffcfbc761ca4d8465412582_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-891ce7c62ffcfbc761ca4d8465412582_b.jpg\"/></figure><p>循环神经网络有多种模型结构，这里我们介绍最主流的模型结构。上图中左边是没有按时间序列展开的图，右边是按照时间序列展开的结构，我们重点看右边的模型结构。这里描述了在序列索引号t附近的RNN模型，下面针对一些参数做具体说明。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-59387d44e8ca91180751bbd31002acdb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1268\" data-rawheight=\"486\" class=\"origin_image zh-lightbox-thumb\" width=\"1268\" data-original=\"https://pic4.zhimg.com/v2-59387d44e8ca91180751bbd31002acdb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1268&#39; height=&#39;486&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1268\" data-rawheight=\"486\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1268\" data-original=\"https://pic4.zhimg.com/v2-59387d44e8ca91180751bbd31002acdb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-59387d44e8ca91180751bbd31002acdb_b.jpg\"/></figure><h2>3.RNN前向传播算法</h2><p>根据上面介绍的模型，我们来看一下RNN前向传播算法，对于任意时刻序列索引号t，能够得到当前的隐藏状态。其中σ为RNN的激活函数，一般是tanh，b为偏倚系数。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-96dc8efbd7c782dbc10c51afe9296ec3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1240\" data-rawheight=\"112\" class=\"origin_image zh-lightbox-thumb\" width=\"1240\" data-original=\"https://pic4.zhimg.com/v2-96dc8efbd7c782dbc10c51afe9296ec3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1240&#39; height=&#39;112&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1240\" data-rawheight=\"112\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1240\" data-original=\"https://pic4.zhimg.com/v2-96dc8efbd7c782dbc10c51afe9296ec3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-96dc8efbd7c782dbc10c51afe9296ec3_b.jpg\"/></figure><p>序列索引号t时模型的输出为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-0e7265beb90ea0f6c967090f60ab228a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1240\" data-rawheight=\"104\" class=\"origin_image zh-lightbox-thumb\" width=\"1240\" data-original=\"https://pic3.zhimg.com/v2-0e7265beb90ea0f6c967090f60ab228a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1240&#39; height=&#39;104&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1240\" data-rawheight=\"104\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1240\" data-original=\"https://pic3.zhimg.com/v2-0e7265beb90ea0f6c967090f60ab228a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-0e7265beb90ea0f6c967090f60ab228a_b.jpg\"/></figure><p>最终能够得到模型的预测输出，由于RNN是识别类的分类模型，所以下式激活函数一般是softmax函数。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-ead2e8123b5dc7a2dd404df83b15b896_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1208\" data-rawheight=\"108\" class=\"origin_image zh-lightbox-thumb\" width=\"1208\" data-original=\"https://pic3.zhimg.com/v2-ead2e8123b5dc7a2dd404df83b15b896_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1208&#39; height=&#39;108&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1208\" data-rawheight=\"108\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1208\" data-original=\"https://pic3.zhimg.com/v2-ead2e8123b5dc7a2dd404df83b15b896_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-ead2e8123b5dc7a2dd404df83b15b896_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-7c31a49597bff8b86165603b1f0ece4d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1260\" data-rawheight=\"124\" class=\"origin_image zh-lightbox-thumb\" width=\"1260\" data-original=\"https://pic2.zhimg.com/v2-7c31a49597bff8b86165603b1f0ece4d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1260&#39; height=&#39;124&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1260\" data-rawheight=\"124\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1260\" data-original=\"https://pic2.zhimg.com/v2-7c31a49597bff8b86165603b1f0ece4d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-7c31a49597bff8b86165603b1f0ece4d_b.jpg\"/></figure><h2>4.RNN反向传播算法</h2><p>RNN反向传播算法和DNN思路相同，即通过梯度下降法进行迭代，得到合适的RNN模型参数U,W,V,b,c，传播过程中所有的参数在序列中各个位置是共享的，即反向传播中我们更新的是相同的参数。为了简化描述，反向传播时损失函数采用对数损失函数，隐藏层的激活函数为tanh函数，输出的激活函数为softmax函数。</p><p>对于RNN，由于我们在序列各位置都有损失函数，因此最终的损失函数L为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-999de71ae4f88b5fa43f9995bfa5ec54_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1240\" data-rawheight=\"158\" class=\"origin_image zh-lightbox-thumb\" width=\"1240\" data-original=\"https://pic1.zhimg.com/v2-999de71ae4f88b5fa43f9995bfa5ec54_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1240&#39; height=&#39;158&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1240\" data-rawheight=\"158\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1240\" data-original=\"https://pic1.zhimg.com/v2-999de71ae4f88b5fa43f9995bfa5ec54_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-999de71ae4f88b5fa43f9995bfa5ec54_b.jpg\"/></figure><p>其中V,c的梯度计算比较简单，如下所示</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-f0a8cac1cc8ea25342b143c112a4f9c6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1236\" data-rawheight=\"284\" class=\"origin_image zh-lightbox-thumb\" width=\"1236\" data-original=\"https://pic3.zhimg.com/v2-f0a8cac1cc8ea25342b143c112a4f9c6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1236&#39; height=&#39;284&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1236\" data-rawheight=\"284\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1236\" data-original=\"https://pic3.zhimg.com/v2-f0a8cac1cc8ea25342b143c112a4f9c6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-f0a8cac1cc8ea25342b143c112a4f9c6_b.jpg\"/></figure><p>针对W,U,b的梯度计算比较复杂，从RNN模型可以看出，在反向传播时，在某一序列位置t的梯度损失，由当前位置的输出对应的梯度损失和序列索引位置t+1时的梯度损失两部分共同决定。对于W在某一序列位置t的梯度损失需要反向传播一步步来进行计算，此处定义序列索引t位置的隐藏状态梯度为</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-8d8e4c8eafe6b2682238b30d6dddc3b7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1254\" data-rawheight=\"1024\" class=\"origin_image zh-lightbox-thumb\" width=\"1254\" data-original=\"https://pic4.zhimg.com/v2-8d8e4c8eafe6b2682238b30d6dddc3b7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1254&#39; height=&#39;1024&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1254\" data-rawheight=\"1024\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1254\" data-original=\"https://pic4.zhimg.com/v2-8d8e4c8eafe6b2682238b30d6dddc3b7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-8d8e4c8eafe6b2682238b30d6dddc3b7_b.jpg\"/></figure><h2>4.RNN梯度爆炸和梯度消失</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-06d30f641f769f69b7453ef373f8db01_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1636\" data-rawheight=\"658\" class=\"origin_image zh-lightbox-thumb\" width=\"1636\" data-original=\"https://pic2.zhimg.com/v2-06d30f641f769f69b7453ef373f8db01_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1636&#39; height=&#39;658&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1636\" data-rawheight=\"658\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1636\" data-original=\"https://pic2.zhimg.com/v2-06d30f641f769f69b7453ef373f8db01_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-06d30f641f769f69b7453ef373f8db01_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-b5a095ee782db9379cf4ecda24a595bc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"619\" data-rawheight=\"479\" class=\"origin_image zh-lightbox-thumb\" width=\"619\" data-original=\"https://pic1.zhimg.com/v2-b5a095ee782db9379cf4ecda24a595bc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;619&#39; height=&#39;479&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"619\" data-rawheight=\"479\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"619\" data-original=\"https://pic1.zhimg.com/v2-b5a095ee782db9379cf4ecda24a595bc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-b5a095ee782db9379cf4ecda24a595bc_b.jpg\"/></figure><h2>5.其他</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3ac1c744ed603ec9e75ecd09c7f630dd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1834\" data-rawheight=\"1036\" class=\"origin_image zh-lightbox-thumb\" width=\"1834\" data-original=\"https://pic2.zhimg.com/v2-3ac1c744ed603ec9e75ecd09c7f630dd_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1834&#39; height=&#39;1036&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1834\" data-rawheight=\"1036\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1834\" data-original=\"https://pic2.zhimg.com/v2-3ac1c744ed603ec9e75ecd09c7f630dd_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-3ac1c744ed603ec9e75ecd09c7f630dd_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-e909cf512bffb7d058f45bfcfda2dbf9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"764\" data-rawheight=\"270\" class=\"origin_image zh-lightbox-thumb\" width=\"764\" data-original=\"https://pic2.zhimg.com/v2-e909cf512bffb7d058f45bfcfda2dbf9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;764&#39; height=&#39;270&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"764\" data-rawheight=\"270\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"764\" data-original=\"https://pic2.zhimg.com/v2-e909cf512bffb7d058f45bfcfda2dbf9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-e909cf512bffb7d058f45bfcfda2dbf9_b.jpg\"/></figure><p>上面总结了通用的RNN模型的前向传播算法和反向传播算法，当然RNN还有很多其他的模型，比如多层RNN、双向循环RNN（如上图所示），在前向和反向传播时公式自然也会不同，但基本原理类似，有兴趣可查询其他资料继续学习。</p><p>RNN存在梯度爆炸和梯度消失问题，但怎么解决呢，下篇文章我们来介绍LSTM算法，看如何解决传播时出现的梯度爆炸和梯度消失问题。</p><h2>6.推广</h2><p>更多内容请关注公众号<b>谓之小一</b>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p><p><a href=\"https://link.zhihu.com/?target=http%3A//weixin.qq.com/r/Ty_4oDPE2W6mrXfD93pd\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">weixin.qq.com/r/Ty_4oDP</span><span class=\"invisible\">E2W6mrXfD93pd</span><span class=\"ellipsis\"></span></a> (二维码自动识别)</p><p></p>", 
            "topic": [
                {
                    "tag": "卷积神经网络（CNN）", 
                    "tagLink": "https://api.zhihu.com/topics/20043586"
                }, 
                {
                    "tag": "算法", 
                    "tagLink": "https://api.zhihu.com/topics/19553510"
                }, 
                {
                    "tag": "神经网络", 
                    "tagLink": "https://api.zhihu.com/topics/19607065"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/47319324", 
            "userName": "谓之小一", 
            "userLink": "https://www.zhihu.com/people/761dda159c5cec5fd926f761ab5f4ccf", 
            "upvote": 5, 
            "title": "卷积神经网络之反向传播算法", 
            "content": "<blockquote>由于较多公式，所以我将部分内容转为图片进行上传，请见谅。清晰版请访问<a href=\"https://link.zhihu.com/?target=https%3A//weizhixiaoyi.com\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">weizhixiaoyi.com</span><span class=\"invisible\"></span></a>查看，你也可以关注我公众号<b>谓之小一</b>，后台直接向我要pdf版本，如有相关问题可公众号后台询问，随时回答。</blockquote><p>前面已经推导学习了<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3MjA2NTQzMw%3D%3D%26mid%3D2247483958%26idx%3D1%26sn%3D4527cba9c60fe8d634c5132398d17827%26chksm%3Dfcd7d1c0cba058d6c7d331affcaa3bf1a9bb532959a857763c3c31f069f1fcb343c67c89652a%26token%3D1009592119%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">卷积神经网络之前向传播算法</a>，本篇文章将推导<b>卷积神经网络之反向传播算法</b>。在学习卷积神经网络算法之前，希望你对深度神经网络有一定程度的了解，我在之前也有写过相关的文章，包括<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3MjA2NTQzMw%3D%3D%26mid%3D2247483903%26idx%3D1%26sn%3D4e3f92578399013eba9f203d35afe972%26chksm%3Dfcd7d209cba05b1ffc66494ea8008c669e40f3045398695b479aba14e1c425f85b7c8f033c4f%26token%3D937283080%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">深度神经网络之前向传播算法</a>、<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3MjA2NTQzMw%3D%3D%26mid%3D2247483911%26idx%3D1%26sn%3Dbcc0fe6a4a0c20a422f254b3264a5fb8%26chksm%3Dfcd7d1f1cba058e7bddefb3d47ba6f87879663c15a5cea5c6653138d0fb1aa9d3454e2ea605a%26token%3D937283080%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">深度神经网络之反向传播算法</a>、<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3MjA2NTQzMw%3D%3D%26mid%3D2247483920%26idx%3D1%26sn%3De72c2ce47cffdf80aefcf441f3660099%26chksm%3Dfcd7d1e6cba058f0d4ebb65997f1649f2fd203fb22d6e166dc2b4cc1373fcdf696ca1b9db83c%26token%3D937283080%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">深度神经网络之损失函数和激活函数</a>、<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3MjA2NTQzMw%3D%3D%26mid%3D2247483926%26idx%3D1%26sn%3D53dc36cd5bcc4fa31ce48b76f2d3e105%26chksm%3Dfcd7d1e0cba058f6a4a55104bb57fc5976c4cbbd3f96a575074689d1e97dd1d4e2f245074757%26token%3D937283080%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">深度神经网络之正则化</a>，可以先看一下再学习卷积神经网络。</p><h2>1.DNN反向传播算法</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-62ff764597eb6d82994db0267991bf84_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1616\" data-rawheight=\"892\" class=\"origin_image zh-lightbox-thumb\" width=\"1616\" data-original=\"https://pic1.zhimg.com/v2-62ff764597eb6d82994db0267991bf84_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1616&#39; height=&#39;892&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1616\" data-rawheight=\"892\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1616\" data-original=\"https://pic1.zhimg.com/v2-62ff764597eb6d82994db0267991bf84_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-62ff764597eb6d82994db0267991bf84_b.jpg\"/></figure><h2>2.CNN反向传播算法</h2><p>对比深度神经网络反向传播算法，卷积神经网络反向传播算法需要解决以下几个问题。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-e9f07392d062e20c651c76a3b2cdc8b7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1612\" data-rawheight=\"328\" class=\"origin_image zh-lightbox-thumb\" width=\"1612\" data-original=\"https://pic4.zhimg.com/v2-e9f07392d062e20c651c76a3b2cdc8b7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1612&#39; height=&#39;328&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1612\" data-rawheight=\"328\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1612\" data-original=\"https://pic4.zhimg.com/v2-e9f07392d062e20c651c76a3b2cdc8b7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-e9f07392d062e20c651c76a3b2cdc8b7_b.jpg\"/></figure><p>由于卷积层可以有多个卷积核，各个卷积核之间的处理方式是完全相同的，为了简化算法公式的复杂度，下面推导时只针对卷积层中若干卷积核中的一个。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-7427fdb44937c362e892e5115be5af5d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1624\" data-rawheight=\"1196\" class=\"origin_image zh-lightbox-thumb\" width=\"1624\" data-original=\"https://pic2.zhimg.com/v2-7427fdb44937c362e892e5115be5af5d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1624&#39; height=&#39;1196&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1624\" data-rawheight=\"1196\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1624\" data-original=\"https://pic2.zhimg.com/v2-7427fdb44937c362e892e5115be5af5d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-7427fdb44937c362e892e5115be5af5d_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-a4cebcd4293b4d8293622f64ddc90dd5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1616\" data-rawheight=\"752\" class=\"origin_image zh-lightbox-thumb\" width=\"1616\" data-original=\"https://pic2.zhimg.com/v2-a4cebcd4293b4d8293622f64ddc90dd5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1616&#39; height=&#39;752&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1616\" data-rawheight=\"752\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1616\" data-original=\"https://pic2.zhimg.com/v2-a4cebcd4293b4d8293622f64ddc90dd5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-a4cebcd4293b4d8293622f64ddc90dd5_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-792e69fb41fc03b4240d48fac0f98f40_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1640\" data-rawheight=\"1066\" class=\"origin_image zh-lightbox-thumb\" width=\"1640\" data-original=\"https://pic1.zhimg.com/v2-792e69fb41fc03b4240d48fac0f98f40_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1640&#39; height=&#39;1066&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1640\" data-rawheight=\"1066\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1640\" data-original=\"https://pic1.zhimg.com/v2-792e69fb41fc03b4240d48fac0f98f40_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-792e69fb41fc03b4240d48fac0f98f40_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-78aa609cd55600a85642421181a558be_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1634\" data-rawheight=\"1060\" class=\"origin_image zh-lightbox-thumb\" width=\"1634\" data-original=\"https://pic3.zhimg.com/v2-78aa609cd55600a85642421181a558be_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1634&#39; height=&#39;1060&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1634\" data-rawheight=\"1060\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1634\" data-original=\"https://pic3.zhimg.com/v2-78aa609cd55600a85642421181a558be_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-78aa609cd55600a85642421181a558be_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-c6b40675ce86a860c551d2d744ef829e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1626\" data-rawheight=\"1290\" class=\"origin_image zh-lightbox-thumb\" width=\"1626\" data-original=\"https://pic3.zhimg.com/v2-c6b40675ce86a860c551d2d744ef829e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1626&#39; height=&#39;1290&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1626\" data-rawheight=\"1290\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1626\" data-original=\"https://pic3.zhimg.com/v2-c6b40675ce86a860c551d2d744ef829e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-c6b40675ce86a860c551d2d744ef829e_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-82cd4b0e15195276c6c8e4a7d2315a5c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1622\" data-rawheight=\"912\" class=\"origin_image zh-lightbox-thumb\" width=\"1622\" data-original=\"https://pic1.zhimg.com/v2-82cd4b0e15195276c6c8e4a7d2315a5c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1622&#39; height=&#39;912&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1622\" data-rawheight=\"912\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1622\" data-original=\"https://pic1.zhimg.com/v2-82cd4b0e15195276c6c8e4a7d2315a5c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-82cd4b0e15195276c6c8e4a7d2315a5c_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-62ff764597eb6d82994db0267991bf84_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1616\" data-rawheight=\"892\" class=\"origin_image zh-lightbox-thumb\" width=\"1616\" data-original=\"https://pic1.zhimg.com/v2-62ff764597eb6d82994db0267991bf84_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1616&#39; height=&#39;892&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1616\" data-rawheight=\"892\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1616\" data-original=\"https://pic1.zhimg.com/v2-62ff764597eb6d82994db0267991bf84_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-62ff764597eb6d82994db0267991bf84_b.jpg\"/></figure><h2>5.CNN反向传播算法总结</h2><p>输入：m个图片样本，CNN模型的层数L和所有隐藏层的类型。对于卷积层，要定义卷积核的大小K，卷积核子矩阵的维度F，填充大小P，步幅S。对于池化层，要定义池化层区域大小k和池化标准(Max或Average)。对于全连接层，定义全连接层的激活函数(输出层除外)和各层神经元的个数。梯度迭代步长α，最大迭代次数Max和停止迭代阈值ϵ。</p><p>输出：CNN模型各隐藏层与输出层的W,b。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-c4e35de784b69014641a9fffb357f3a2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1604\" data-rawheight=\"1256\" class=\"origin_image zh-lightbox-thumb\" width=\"1604\" data-original=\"https://pic3.zhimg.com/v2-c4e35de784b69014641a9fffb357f3a2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1604&#39; height=&#39;1256&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1604\" data-rawheight=\"1256\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1604\" data-original=\"https://pic3.zhimg.com/v2-c4e35de784b69014641a9fffb357f3a2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-c4e35de784b69014641a9fffb357f3a2_b.jpg\"/></figure><h2>6.推广</h2><p>更多内容请关注公众号<b>谓之小一</b>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p><p><a href=\"https://link.zhihu.com/?target=http%3A//weixin.qq.com/r/Ty_4oDPE2W6mrXfD93pd\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">weixin.qq.com/r/Ty_4oDP</span><span class=\"invisible\">E2W6mrXfD93pd</span><span class=\"ellipsis\"></span></a> (二维码自动识别)</p><p></p>", 
            "topic": [
                {
                    "tag": "神经网络", 
                    "tagLink": "https://api.zhihu.com/topics/19607065"
                }, 
                {
                    "tag": "算法", 
                    "tagLink": "https://api.zhihu.com/topics/19553510"
                }, 
                {
                    "tag": "卷积神经网络（CNN）", 
                    "tagLink": "https://api.zhihu.com/topics/20043586"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/47260328", 
            "userName": "谓之小一", 
            "userLink": "https://www.zhihu.com/people/761dda159c5cec5fd926f761ab5f4ccf", 
            "upvote": 1, 
            "title": "卷积神经网络之前向传播算法", 
            "content": "<blockquote>由于较多公式，所以我将部分内容转为图片进行上传，请见谅。清晰版请访问<a href=\"https://link.zhihu.com/?target=https%3A//weizhixiaoyi.com\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">weizhixiaoyi.com</span><span class=\"invisible\"></span></a>查看，你也可以关注我公众号<b>谓之小一</b>，后台直接向我要pdf版本，如有相关问题可公众号后台询问，随时回答。</blockquote><h2>0.杂谈</h2><p>本来个人是准备毕业直接工作的，但前段时间学校保研大名单出来之后，发现本人有保研机会，于是就和主管请了几天假，回学校准备保研的事情。经过两天的准备，也是非常幸运，成功拿到本院的保研名额。明确得到保研名额的时候已经是9月18号，然而国家推免系统开放时间是9月28号，也就是说我只还有10天时间准备保研，而且这个时间点很多学校夏令营、预报名活动早已结束，不再接受学生申请。所以能够申请的学校也就很少，同时这10天之间，还要赶回北京实习，所以时间还是很赶的。</p><p>短短10天，发生了很多有趣的事情，但不论怎样，最后的结果还是很不错的，成功保研到<b>华中科技大学</b>，全国排名前10左右的高校，而且计算机很强 。但华科要求，本科非985高校不允许读学硕，只能读专硕。我倒觉得无所谓，反正不准备继续读博，学硕、专硕没有太大区别。</p><p>被华科预录取之后，导师要求提前到实验室学习，于是10月10号便来到实验室搬砖。怎么说呢，感觉还是挺累的，比工作还累，早9晚10。但确实能够学习到很多东西，实验室环境很不错，每人一台电脑，等等福利都很好。至此，一直准备工作的我，成功走上读研之路，加油啦。前段时间一直想写篇关于<b>从双非到985高校的飞跃之路</b>，但感觉有点哗众取宠，以后看情况再写吧。</p><p>最后，个人准备从推荐方向慢慢转到NLP方向，但推荐方向不会放弃，继续学习。OK，开始学习算法，今天准备讲解<b>CNN(卷积神经网络)</b>。学习卷积神经网络之前，建议学习下深度神经网络，没学习过的可以看我之前写的文章，<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3MjA2NTQzMw%3D%3D%26mid%3D2247483903%26idx%3D1%26sn%3D4e3f92578399013eba9f203d35afe972%26chksm%3Dfcd7d209cba05b1ffc66494ea8008c669e40f3045398695b479aba14e1c425f85b7c8f033c4f%26token%3D937283080%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">深度神经网络之前向传播算法</a>、<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3MjA2NTQzMw%3D%3D%26mid%3D2247483911%26idx%3D1%26sn%3Dbcc0fe6a4a0c20a422f254b3264a5fb8%26chksm%3Dfcd7d1f1cba058e7bddefb3d47ba6f87879663c15a5cea5c6653138d0fb1aa9d3454e2ea605a%26token%3D937283080%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">深度神经网络之反向传播算法</a>、<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3MjA2NTQzMw%3D%3D%26mid%3D2247483920%26idx%3D1%26sn%3De72c2ce47cffdf80aefcf441f3660099%26chksm%3Dfcd7d1e6cba058f0d4ebb65997f1649f2fd203fb22d6e166dc2b4cc1373fcdf696ca1b9db83c%26token%3D937283080%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">深度神经网络之损失函数和激活函数</a>、<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3MjA2NTQzMw%3D%3D%26mid%3D2247483926%26idx%3D1%26sn%3D53dc36cd5bcc4fa31ce48b76f2d3e105%26chksm%3Dfcd7d1e0cba058f6a4a55104bb57fc5976c4cbbd3f96a575074689d1e97dd1d4e2f245074757%26token%3D937283080%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">深度神经网络之正则化</a>。</p><h2>1.CNN基本结构</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-9b70d0829d8357d0d16f9db279d50012_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"748\" data-rawheight=\"178\" class=\"origin_image zh-lightbox-thumb\" width=\"748\" data-original=\"https://pic3.zhimg.com/v2-9b70d0829d8357d0d16f9db279d50012_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;748&#39; height=&#39;178&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"748\" data-rawheight=\"178\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"748\" data-original=\"https://pic3.zhimg.com/v2-9b70d0829d8357d0d16f9db279d50012_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-9b70d0829d8357d0d16f9db279d50012_b.jpg\"/></figure><p>首先我们来看看卷积神经网络(CNN)的基本结构。如上图所示，可以看出最左边的图片就是我们的<b>输入层</b>，计算机理解为输入若干个矩阵。接着是<b>卷积层(Convolution Layer)</b>，卷积层是CNN所特有的，卷积层使用的激活函数是ReLU，之前在DNN之中介绍过ReLU的激活函数，形式如ReLU=max(0,x)​。卷积层后面的是<b>池化层</b>，池化层也是CNN所特有的，池化层没有激活函数。</p><p>卷积层+池化层的组合可以在CNN隐藏层中出现多次，实际使用中根据模型需要而定。同时我们也可以灵活使用卷积层+卷积层，或者卷积层+卷积层+池化层的组合，卷积层+池化层的组合在构建模型时没有限制，但最常见的CNN都是若干卷积层+池化层的组合。</p><p>在若干卷积层+池化层的组合后面是<b>全连接层(Fully Connected Layer)</b>，全连接层就是之前讲到的DNN结构，只是<b>输出层</b>使用了Softmax激活函数来做图像识别的分类。从上面模型可以看出，CNN相对于DNN，比较特殊的是卷积层和池化层。如果之前熟悉DNN的话，只要把卷积层和池化层的原理理解清楚，那么CNN就简单啦。</p><h2>2.卷积</h2><p>既然是学习卷积神经网络，那自然需要了解什么是卷积。在学习高等数学的时候，微积分中卷积表达式和其离散形式如下所示。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-c931009cf1e28671678d5067a03839fe_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1596\" data-rawheight=\"270\" class=\"origin_image zh-lightbox-thumb\" width=\"1596\" data-original=\"https://pic3.zhimg.com/v2-c931009cf1e28671678d5067a03839fe_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1596&#39; height=&#39;270&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1596\" data-rawheight=\"270\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1596\" data-original=\"https://pic3.zhimg.com/v2-c931009cf1e28671678d5067a03839fe_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-c931009cf1e28671678d5067a03839fe_b.jpg\"/></figure><p>当然也可以用矩阵进行表达，其中<b>*</b>表示卷积。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-f17cf9200c7e8f12ee8f2fcb4999bdf5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1594\" data-rawheight=\"124\" class=\"origin_image zh-lightbox-thumb\" width=\"1594\" data-original=\"https://pic2.zhimg.com/v2-f17cf9200c7e8f12ee8f2fcb4999bdf5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1594&#39; height=&#39;124&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1594\" data-rawheight=\"124\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1594\" data-original=\"https://pic2.zhimg.com/v2-f17cf9200c7e8f12ee8f2fcb4999bdf5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-f17cf9200c7e8f12ee8f2fcb4999bdf5_b.jpg\"/></figure><p>如果是二维的卷积，则其表达式如下所示。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-27f4ae673a65c64a2265459af62c1e92_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1590\" data-rawheight=\"134\" class=\"origin_image zh-lightbox-thumb\" width=\"1590\" data-original=\"https://pic3.zhimg.com/v2-27f4ae673a65c64a2265459af62c1e92_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1590&#39; height=&#39;134&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1590\" data-rawheight=\"134\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1590\" data-original=\"https://pic3.zhimg.com/v2-27f4ae673a65c64a2265459af62c1e92_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-27f4ae673a65c64a2265459af62c1e92_b.jpg\"/></figure><p>在CNN中，虽然我们也是说卷积，但严格意义上和数学上所定义的卷积稍有不同，比如对于二维的卷积，其定义如下所示，其中X为输入，W为卷积核。如果X是二维输入，那么W也是二维矩阵，如果X是多维张量，那么W也是多维张量。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-1ad2f07f8521489c0d221a9f3850a900_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1582\" data-rawheight=\"132\" class=\"origin_image zh-lightbox-thumb\" width=\"1582\" data-original=\"https://pic1.zhimg.com/v2-1ad2f07f8521489c0d221a9f3850a900_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1582&#39; height=&#39;132&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1582\" data-rawheight=\"132\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1582\" data-original=\"https://pic1.zhimg.com/v2-1ad2f07f8521489c0d221a9f3850a900_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-1ad2f07f8521489c0d221a9f3850a900_b.jpg\"/></figure><p>卷积有什么简单利用呢？我们举个例子，假如有两枚骰子，然后把骰子扔出去，求两枚骰子点数之和加起来为4的概率是多少。</p><p>上述例子的关键点便是两个骰子点数之和加起来要等于4，这正是卷积的应用场景。假设利用f表示第一枚骰子，g表示第二枚骰子。f(1)表示点数为1的概率，f(2)表示点数为2的概率。那么两枚骰子点数加起来为4的情况有f(1)g(3)、f(2)g(2)、f(3)g(1)，因此两枚骰子点数加起来为4的概率为f(1)g(3)+f(2)g(2)+f(3)g(1)。符合卷积的定义，那么转换成卷积的标准形式便是</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-4843064e8f90c321b769b1b44a1e3d17_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1588\" data-rawheight=\"176\" class=\"origin_image zh-lightbox-thumb\" width=\"1588\" data-original=\"https://pic4.zhimg.com/v2-4843064e8f90c321b769b1b44a1e3d17_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1588&#39; height=&#39;176&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1588\" data-rawheight=\"176\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1588\" data-original=\"https://pic4.zhimg.com/v2-4843064e8f90c321b769b1b44a1e3d17_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-4843064e8f90c321b769b1b44a1e3d17_b.jpg\"/></figure><h2>3.CNN卷积层</h2><p>现在我们来深入CNN中的卷积层，如下图所示，针对图像进行卷积计算。图中的输入是二维的3*4的矩阵，卷积核是2*2的矩阵。这里我们假设卷积是每次移动一个像素来进行卷积，首先对左上角2*2局部和卷积核进行卷积计算，即各个位置的元素相乘再相加，得到的输出值S00为aw+bx+ey+fz。接着我们将输入的局部向右平移一个像素，现在是(b,c,f,g)四个元素构成的矩阵和卷积核进行卷积，这样便能够得到值为S01的元素。同样的方法能够得到S02,S10,S11,S12的元素，最后能够得到为2*3的输出矩阵S。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-f970f6c74f2e9ef98822cb65bb9ba2d6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"551\" data-rawheight=\"498\" class=\"origin_image zh-lightbox-thumb\" width=\"551\" data-original=\"https://pic3.zhimg.com/v2-f970f6c74f2e9ef98822cb65bb9ba2d6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;551&#39; height=&#39;498&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"551\" data-rawheight=\"498\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"551\" data-original=\"https://pic3.zhimg.com/v2-f970f6c74f2e9ef98822cb65bb9ba2d6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-f970f6c74f2e9ef98822cb65bb9ba2d6_b.jpg\"/></figure><p>下面我们再看一个动态的卷积过程，其中输入是5*5的矩阵，卷积核是3*3的矩阵，卷积的步幅是一个像素，卷积的结果是3*3的矩阵。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-a67dbfb0e31fc4b32aeb6debba882197_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"127\" data-rawheight=\"115\" class=\"content_image\" width=\"127\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;127&#39; height=&#39;115&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"127\" data-rawheight=\"115\" class=\"content_image lazy\" width=\"127\" data-actualsrc=\"https://pic4.zhimg.com/v2-a67dbfb0e31fc4b32aeb6debba882197_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-4afb1f5a464fd499dd1b60e8a30f219c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"74\" data-rawheight=\"63\" class=\"content_image\" width=\"74\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;74&#39; height=&#39;63&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"74\" data-rawheight=\"63\" class=\"content_image lazy\" width=\"74\" data-actualsrc=\"https://pic1.zhimg.com/v2-4afb1f5a464fd499dd1b60e8a30f219c_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-32b19250b52d9177343aed3b428b95f0_b.gif\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"268\" data-rawheight=\"196\" data-thumbnail=\"https://pic1.zhimg.com/v2-32b19250b52d9177343aed3b428b95f0_b.jpg\" class=\"content_image\" width=\"268\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;268&#39; height=&#39;196&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"268\" data-rawheight=\"196\" data-thumbnail=\"https://pic1.zhimg.com/v2-32b19250b52d9177343aed3b428b95f0_b.jpg\" class=\"content_image lazy\" width=\"268\" data-actualsrc=\"https://pic1.zhimg.com/v2-32b19250b52d9177343aed3b428b95f0_b.gif\"/></figure><p>上面举例都是二维的输入，卷积过程比较简单，那么如果输入是多维怎么计算呢？比如输入对应的是彩色图像，每个矩阵分别对应R、G、B矩阵。在斯坦福大学cs231n课程上，有个动态例子，链接为<a href=\"https://link.zhihu.com/?target=http%3A//cs231n.github.io/assets/conv-demo/index.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">cs231n.github.io/assets</span><span class=\"invisible\">/conv-demo/index.html</span><span class=\"ellipsis\"></span></a>，可以看着动图理解。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-36d7956824e51be8e2c38da53cee2f17_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1560\" data-rawheight=\"1362\" class=\"origin_image zh-lightbox-thumb\" width=\"1560\" data-original=\"https://pic4.zhimg.com/v2-36d7956824e51be8e2c38da53cee2f17_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1560&#39; height=&#39;1362&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1560\" data-rawheight=\"1362\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1560\" data-original=\"https://pic4.zhimg.com/v2-36d7956824e51be8e2c38da53cee2f17_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-36d7956824e51be8e2c38da53cee2f17_b.jpg\"/></figure><p>其中原输入矩阵为3个5*5的矩阵，在输入周围加上了1的padding，变成了3个7*7的矩阵。另外使用了两个卷积核，我们只关注于卷积核W0。由于输入是3个7*7的矩阵，或者说是7*7*3的张量，那么对应的卷积核W0应该是3*3*3的张量。另外这里每次卷积计算移动2个像素，也就是步幅为2。</p><p>最终的卷积过程和上面的2维矩阵计算类似，这里采用的是两个张量的三个子矩阵计算后，再把卷积的结果相加，最后再加上偏移量b。因此7*7*3的张量和3*3*3的卷积核张量W0计算后，结果是一个3*3的矩阵，同时由于我们有两个卷积核，那么最终结果便是3*3*2的张量。如果把上面的卷积过程用数学表达出来的话，那么表达式如下所示，其中n_in为输入矩阵的个数，或者说是最后一维的维数。Xk代表第k个输入矩阵，Wk代表第k个子卷积核矩阵，s(i,j)即卷积核W对应的输出矩阵元素的值。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-14740b4bd948d52b1e4a873b100484f9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1596\" data-rawheight=\"148\" class=\"origin_image zh-lightbox-thumb\" width=\"1596\" data-original=\"https://pic2.zhimg.com/v2-14740b4bd948d52b1e4a873b100484f9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1596&#39; height=&#39;148&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1596\" data-rawheight=\"148\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1596\" data-original=\"https://pic2.zhimg.com/v2-14740b4bd948d52b1e4a873b100484f9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-14740b4bd948d52b1e4a873b100484f9_b.jpg\"/></figure><h2>4.CNN池化层</h2><p>CNN池化层就是对输入张量的各个子矩阵进行压缩，假如是2*2的池化，那么就是将子矩阵的每2*2的元素变成一个元素，如果是3*3的池化，便是将子矩阵每3*3的元素变成一个元素，这样输入矩阵的维度也就降低。常见的池化标准是Max或者Average，即取对应区域的最大值或者平均值。如下图所示，4*4的矩阵在池化后变成2*2的矩阵，矩阵维度进行了压缩。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-bdcb436f061e5b819e1fdda4f855669d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"494\" data-rawheight=\"421\" class=\"origin_image zh-lightbox-thumb\" width=\"494\" data-original=\"https://pic2.zhimg.com/v2-bdcb436f061e5b819e1fdda4f855669d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;494&#39; height=&#39;421&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"494\" data-rawheight=\"421\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"494\" data-original=\"https://pic2.zhimg.com/v2-bdcb436f061e5b819e1fdda4f855669d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-bdcb436f061e5b819e1fdda4f855669d_b.jpg\"/></figure><h2>5.CNN前向传播</h2><p>前面已经了解卷积神经网络的基本结构，包括输入层、若干卷积层+ReLU激活函数、若干的池化层、DNN全连接层，以及最后用Softmax激活函数的输出层。以彩色的汽车样本图像为例，图中的CONV即为卷积层、POLL即为池化层，FC即为DNN的全连接层。要理解CNN的前向传播算法，重点是输入层的前向传播、卷积层的前向传播、池化层的前向传播。另外全连接层传播和用Softmax激活函数的输出层的前向传播已经在DNN中讲解，这里不再赘述。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-d9259be829b1cdb3d98a399ebc56defa_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1255\" data-rawheight=\"601\" class=\"origin_image zh-lightbox-thumb\" width=\"1255\" data-original=\"https://pic3.zhimg.com/v2-d9259be829b1cdb3d98a399ebc56defa_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1255&#39; height=&#39;601&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1255\" data-rawheight=\"601\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1255\" data-original=\"https://pic3.zhimg.com/v2-d9259be829b1cdb3d98a399ebc56defa_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-d9259be829b1cdb3d98a399ebc56defa_b.jpg\"/></figure><h2>6.CNN输入层前向传播到卷积层</h2><p>以图像为例，如果样本是二维的黑白图片，那么输入层X便是一个矩阵，矩阵的值等于图片的各个像素的值，这时和卷积层相连的卷积核W也就是一个矩阵。如果样本是RGB彩色图片，那么输入X便是3个矩阵，即每个对应R、G、B的矩阵，或者说是一个张量，这时和卷积层相连的卷积核W也是张量，每个卷积核都由3个子矩阵组成。同样的方法，对于3D的彩色图片，输入X可以是4维、5维等的张量，那么对应的卷积核W也是个高维的张量。不管维度多少，对于我们的输入，前向传播算法可以表示为如下所示，其中上标表示层数，<b>*</b>表示卷积，b表示偏倚，σ表示激活函数，一般都是ReLU激活函数。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-9a0a54ca631ed854c9fe9d8929599568_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1588\" data-rawheight=\"114\" class=\"origin_image zh-lightbox-thumb\" width=\"1588\" data-original=\"https://pic1.zhimg.com/v2-9a0a54ca631ed854c9fe9d8929599568_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1588&#39; height=&#39;114&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1588\" data-rawheight=\"114\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1588\" data-original=\"https://pic1.zhimg.com/v2-9a0a54ca631ed854c9fe9d8929599568_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-9a0a54ca631ed854c9fe9d8929599568_b.jpg\"/></figure><p>和DNN的前向传播比较一下，两者形式非常像，只是CNN这儿是张量的卷积，而不是矩阵的乘法。最后，我们需要定义一些CNN模型参数，即为</p><ul><li>卷积核个数K。假设我们有K个卷积核，那么我们输入层的输出就有K个，即第二层卷积层的输入有K个。</li><li>卷积核中每个子矩阵的大小。一般我们都用子矩阵为方振的卷积核，比如F*F的子矩阵。</li><li>填充Padding(简称P)。卷积的时候，为了可以更好的识别边缘，一般会在输入矩阵周围加上若干圈的0再进行卷积，加多少圈则P为多少。</li><li>步幅Stride（简称S）。即在卷积过程中每次移动的像素距离大小。</li></ul><h2>7.隐藏层前向传播到卷积层</h2><p>假设隐藏层的输出是M个矩阵对应的三维张量，则输出到卷积层的卷积核也是M个子矩阵对应的三维张量，这时表达式如下所示。其中上标代表层数，<b>*</b>表示卷积，b表示偏倚量，σ表示激活函数，这里一般用ReLU。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-580d3d2fea17762f7b1633ecbe8b6ae5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1580\" data-rawheight=\"112\" class=\"origin_image zh-lightbox-thumb\" width=\"1580\" data-original=\"https://pic2.zhimg.com/v2-580d3d2fea17762f7b1633ecbe8b6ae5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1580&#39; height=&#39;112&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1580\" data-rawheight=\"112\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1580\" data-original=\"https://pic2.zhimg.com/v2-580d3d2fea17762f7b1633ecbe8b6ae5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-580d3d2fea17762f7b1633ecbe8b6ae5_b.jpg\"/></figure><p>以可以写成M个子矩阵卷积后对应位置想加的形式，即为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-0fd738d6dc802e0094a20caba1d4106f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1604\" data-rawheight=\"160\" class=\"origin_image zh-lightbox-thumb\" width=\"1604\" data-original=\"https://pic4.zhimg.com/v2-0fd738d6dc802e0094a20caba1d4106f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1604&#39; height=&#39;160&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1604\" data-rawheight=\"160\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1604\" data-original=\"https://pic4.zhimg.com/v2-0fd738d6dc802e0094a20caba1d4106f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-0fd738d6dc802e0094a20caba1d4106f_b.jpg\"/></figure><p>这里和上节的区别在于，这里的输入是隐藏层来的，而不是我们输入原始图片所形成的矩阵。同样，这里我们也需要定义卷积核的个数K，卷积核子矩阵的维度F，填充大小P以及步幅S。</p><h2>8.隐藏层前向传播到池化层</h2><p>池化层的处理逻辑比较简单，目的便是对输入矩阵进行缩小概括。比如输入的矩阵是N*N维的，而需要的池化大小区域是k*k维的，那么输出的矩阵都是(N/k)*(N/k)维度。这里需要定义的CNN模型参数为</p><ul><li>池化区域的大小k。</li><li>池化的标准，一般是Max或者Average。</li></ul><h2>9.隐藏层前向传播到全连接层</h2><p>由于全连接层就是普通的DNN模型结构，因此我们可以直接使用DNN的前向传播算法逻辑，表达式如下所示，这里的激活函数一般用sigmoid或者tanh函数。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-638c84417ae299312ef69295d836d939_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1590\" data-rawheight=\"118\" class=\"origin_image zh-lightbox-thumb\" width=\"1590\" data-original=\"https://pic2.zhimg.com/v2-638c84417ae299312ef69295d836d939_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1590&#39; height=&#39;118&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1590\" data-rawheight=\"118\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1590\" data-original=\"https://pic2.zhimg.com/v2-638c84417ae299312ef69295d836d939_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-638c84417ae299312ef69295d836d939_b.jpg\"/></figure><p>经过若干全连接层之后，最后一层为Softmax输出层。此时输出层和普通的全连接层唯一的区别是，激活函数是Softmax函数。这里需要定义的CNN模型参数为</p><ul><li>全连接层的激活函数。</li><li>全连接层各层神经元的个数。</li></ul><h2>10.CNN前向传播算法总结</h2><p>输入：1个图片样本，CNN模型的层数L和所有隐藏层的类型。对于卷积层，要定义卷积核的大小K，卷积核子矩阵的维度F，填充大小P，步幅S。对于池化层，要定义池化层区域大小k和池化标准(Max或Average)。对于全连接层，定义全连接层的激活函数(输出层除外)和各层神经元的个数。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-792f023558299ee091403bcbcb8f7d3b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1594\" data-rawheight=\"864\" class=\"origin_image zh-lightbox-thumb\" width=\"1594\" data-original=\"https://pic4.zhimg.com/v2-792f023558299ee091403bcbcb8f7d3b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1594&#39; height=&#39;864&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1594\" data-rawheight=\"864\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1594\" data-original=\"https://pic4.zhimg.com/v2-792f023558299ee091403bcbcb8f7d3b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-792f023558299ee091403bcbcb8f7d3b_b.jpg\"/></figure><p>上面就是CNN前向传播算法全过程，下篇来讨论CNN的反向传播算法。</p><h2>11.推广</h2><p>更多内容请关注公众号<b>谓之小一</b>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p><p><a href=\"https://link.zhihu.com/?target=http%3A//weixin.qq.com/r/Ty_4oDPE2W6mrXfD93pd\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">weixin.qq.com/r/Ty_4oDP</span><span class=\"invisible\">E2W6mrXfD93pd</span><span class=\"ellipsis\"></span></a> (二维码自动识别)</p><p></p>", 
            "topic": [
                {
                    "tag": "卷积神经网络（CNN）", 
                    "tagLink": "https://api.zhihu.com/topics/20043586"
                }, 
                {
                    "tag": "算法", 
                    "tagLink": "https://api.zhihu.com/topics/19553510"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/43189533", 
            "userName": "谓之小一", 
            "userLink": "https://www.zhihu.com/people/761dda159c5cec5fd926f761ab5f4ccf", 
            "upvote": 23, 
            "title": "进程、线程、锁的概念", 
            "content": "<p>以前在学校的时候，推导过挺多算法，也总结到个人公众号上面去啦。在实习工作之中，也能用到部分算法，使用起来也是很得心应手。但对于刚实习或工作的同学来说(就是我)，<b>吓人的技术</b>可能略懂，但工程方面的知识却是很浅薄。就拿最简单的进程、线程问题来说，代码实现过程中也会遇到很多问题，所以在这儿总结一下，加深自己理解。<b>重点：基础真的很重要。操作系统、网络原理、数据结构，这些知识要认真学习呢。</b></p><p>首先通俗例子解释下什么是进程和线程的关系。比如你开启一个QQ，就开启了一个进程。开启了微信，就开启了另外一个进程。在QQ这个进程里，传输文字是一个线程、传输语音是一个线程、弹出对话框是一个线程。也就是说，进程可以包含多个线程。</p><h2>1.进程</h2><ul><li>进程：进程是正在执行程序的实例，是资源分配最小的单位，每个进程都有自己单独的资源区域。进程在一定的环境下，把静态的程序代码运行起来，通过使用不同的资源，来完成一定的任务。进程的环境包括环境变量，进程所掌控的资源，有中央处理器，有内存，打开的文件，映射的网络端口等。</li><li>守护进程：运行在后台的进程，用于执行特定的系统任务。</li><li>进程的状态：只介绍进程基本状态。</li><ul><li>就绪态：进程已分配到除CPU以外的所有必要资源后，只要再获得CPU，便可立即执行，进程这时的状态称为就绪状态。在一个系统中处于就绪状态的进程可能有多个，通常将它们排成一个队列，称为就绪队列。</li><li>运行态：进程已获得CPU，其程序正在执行。</li><li>阻塞态：正在执行的进程由于发生某事件而暂时无法继续执行时，便放弃处理机而处于暂停状态。致使进程阻塞的典型事件有：请求I/O，申请缓冲空间等。通常将这种处于阻塞状态的进程也排成一个队列。阻塞状态的进程，除非某种外部时间发生，否则进程不能运行。</li></ul><li>进程状态的转换：进程在运行期间，不断地从一种状态转换到另一种状态，它可以多次处于就绪状态和运行状态，也可以多次处于阻塞状态。</li><ul><li>就绪→运行：处于就绪状态的进程，当进程调度程序为之分配了处理机后，该进程便由就绪状态转变成运行状态。</li><li>运行→就绪：处于运行状态的进程在其运行过程中，因分配给它的一个时间片已用完而不得不让出处理机，于是进程从运行状态转变成就绪状态。</li><li>运行→阻塞：正在运行的进程因等待某种事件发生而无法继续运行时，便从运行状态变成阻塞状态。</li><li>阻塞→就绪：处于阻塞状态的进程，若其等待的事件已经发生，于是进程由阻塞状态转变为就绪状态。</li></ul><li>进程表：为实现进程模型，每个进程占用一个进程表项，该进程表项包含了进程状态的重要信息，包括程序计数器、堆栈指针、内存分配状况、所打开文件的状态、帐号和调度信息等。</li></ul><h2>2.线程</h2><ul><li>线程：cpu调度的最小单位。线程共享进程的资源，多个线程可以共享同一地址空间和其他资源，比如共享全局变量。线程作为进程的一部分，扮演的角色就是怎么利用中央处理器去运行代码。线程关注的是中央处理器的运行，而不是内存等资源的管理。同一时刻只有一个线程占用cpu，但高速切换给人带来并行的假象。</li><li>线程状态及转换：只介绍线程基本状态。</li><ul><li>就绪状态：线程对象被创建后，其它线程调用了该对象的start()方法，从而来启动该线程。处于就绪状态的线程，随时可能被CPU调度执行。</li><li>运行状态: 线程已获得CPU，正在运行。</li><li>阻塞状态： 阻塞状态是线程因为某种原因放弃CPU使用权，暂时停止运行。直到线程进入就绪状态，才有机会转到运行状态。</li></ul><li>为什么多线程？</li><ul><li>线程比进程更加轻量级，线程更容易、快捷的创建和销毁。</li><li>多CPU系统中，使用线程提高CPU利用率。</li><li>耗时的操作使用线程，提高应用程序响应。拥有多个线程允许活动彼此重叠进行，从而会加快应用程序执行速度。</li><li>并行操作时使用线程，如C/S架构的服务器端并发线程响应用户的请求。</li><li>改善程序结构。一个既长又复杂的进程可以考虑分为多个线程，成为几个独立或半独立的运行部分，这样的程序会利于理解和修改。</li><li>并行实体共享同一个地址空间和所有可用数据的能力。</li></ul></ul><h2>3.线程和进程的对比</h2><ul><li> 调度：在引入线程的操作系统中，线程是调度和分配的基本单位 ，进程是资源拥有的基本单位 。把传统进程的两个属性分开，线程便能轻装运行，从而可显著地提高系统的并发程度。 在同一进程中，线程的切换不会引起进程的切换，在由一个进程中的线程切换到另一个进程中的线程时，才会引起进程的切换。<br/> </li><li> 并发性：在引入线程的操作系统中，不仅进程之间可以并发执行，而且在一个进程中的多个线程之间亦可并发执行，因而使操作系统具有更好的并发性，从而能更有效地使用系统资源和提高系统吞吐量。<br/> </li><li> 拥有资源：进程是资源分配的最小单位，线程是cpu调度的最小单位。进程更倾向于内存管理的概念，进程在自己的区域掌控自己的资源，也不越界。线程更倾向于cpu的运行。进程在执行过程中拥有独立的内存单元，而多个线程共享内存， 从而极大的提高了程序的运行效率。<br/> </li><li> 系统开销：由于在创建或撤消进程时，系统都要为之分配或回收资源，因此，操作系统所付出的开销将显著地大于在创建或撤消线程时的开销。 进程切换的开销也远大于线程切换的开销。<br/> </li><li> 进程更为健壮。一个进程之间的某个线程死掉，整个进程就死掉了。一个进程死掉对其他进程没有影响。另外一个线程可以创建和撤销另一个线程。同一个进程中的多个线程之间可以并发执行。</li></ul><h2>4. 锁</h2><h2>4.1 锁机制</h2><p>通过锁机制，能够保证在多核多线程环境中，在某一个时间点上，只能有一个线程进入临界区代码，从而保证临界区中操作数据的一致性。</p><p>所谓的锁，可以理解为内存中的一个整型数，拥有两种状态：空闲状态和上锁状态。加锁时，判断锁是否空闲，如果空闲，修改为上锁状态，返回成功。如果已经上锁，则返回失败。解锁时，则把锁状态修改为空闲状态。</p><h2>4.2 死锁的概念</h2><p>死锁是指两个或两个以上的进程在执行过程中，因争夺资源而造成的一种互相等待的现象，若无外力作用，它们都将无法推进下去。此时称系统处于死锁状态或系统产生了死锁，这些永远在互相等待的进程称为死锁进程。死锁的原因包括系统资源不足、进程运行推进顺序不合适、资源分配不当等。</p><p>比如两只羊过独木桥。进程比作羊，资源比作桥。若两只羊互不相让，争着过桥，就产生死锁。</p><h2>4.3 死锁的必要条件</h2><p>产生死锁的四个必要条件：  + 互斥条件：一个资源每次只能被一个进程使用。  + 请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放。  + 不剥夺条件： 进程已获得的资源，在末使用完之前，不能强行剥夺。  + 循环等待条件：若干进程之间形成一种头尾相接的循环等待资源关系。存在一个进程等待序列{P1，P2，…，Pn}，其中P1等待P2所占有的某一资源，P2等待P3所占有的某一资源，……，而Pn等待P1所占有的的某一资源，形成一个进程循环等待环。 </p><h2>4.4 解决死锁的四个方式</h2><ul><li>鸵鸟算法。(直接忽略该问题)</li><li>检测死锁并且恢复。（检测与解除策略） </li><li>仔细地对资源进行动态分配，以避免死锁。（避免策略） </li><li>通过破除死锁四个必要条件之一，来防止死锁产生。（预防策略）</li></ul><h2>4.5 实例</h2><p>多线程开发过程中，任何一个线程都可对变量进行修改，如果关键代码部分没有进行加锁，那么因此而产生bug，可能是我们不能理解的。比如我们定义了一个共享变量 balance，初始值为 0，并且启动两个线程，先存后取，理论上结果应该为 0。但是，由于线程的调度是由操作系统决定的，当 t1、t2 交替执行时，只要循环次数足够多，balance 的结果就不一定是 0 了。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">import</span> <span class=\"nn\">time</span><span class=\"o\">,</span> <span class=\"nn\">threading</span>\n\n<span class=\"n\">balance</span> <span class=\"o\">=</span> <span class=\"mi\">0</span> <span class=\"c1\"># 假定这是你的银行存款:</span>\n\n<span class=\"c1\"># 先存后取，结果应该为0</span>\n<span class=\"k\">def</span> <span class=\"nf\">change_it</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"p\">):</span>\n    <span class=\"k\">global</span> <span class=\"n\">balance</span>\n    <span class=\"n\">balance</span> <span class=\"o\">=</span> <span class=\"n\">balance</span> <span class=\"o\">+</span> <span class=\"n\">n</span>\n    <span class=\"n\">balance</span> <span class=\"o\">=</span> <span class=\"n\">balance</span> <span class=\"o\">-</span> <span class=\"n\">n</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">run_thread</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"p\">):</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">100000</span><span class=\"p\">):</span>\n        <span class=\"n\">change_it</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"p\">)</span>\n\n<span class=\"n\">t1</span> <span class=\"o\">=</span> <span class=\"n\">threading</span><span class=\"o\">.</span><span class=\"n\">Thread</span><span class=\"p\">(</span><span class=\"n\">target</span><span class=\"o\">=</span><span class=\"n\">run_thread</span><span class=\"p\">,</span> <span class=\"n\">args</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">5</span><span class=\"p\">,))</span>\n<span class=\"n\">t2</span> <span class=\"o\">=</span> <span class=\"n\">threading</span><span class=\"o\">.</span><span class=\"n\">Thread</span><span class=\"p\">(</span><span class=\"n\">target</span><span class=\"o\">=</span><span class=\"n\">run_thread</span><span class=\"p\">,</span> <span class=\"n\">args</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">8</span><span class=\"p\">,))</span>\n<span class=\"n\">t1</span><span class=\"o\">.</span><span class=\"n\">start</span><span class=\"p\">()</span>\n<span class=\"n\">t2</span><span class=\"o\">.</span><span class=\"n\">start</span><span class=\"p\">()</span>\n<span class=\"n\">t1</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">()</span>\n<span class=\"n\">t2</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">()</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">balance</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># 26</span></code></pre></div><p>另外，针对加锁的代码，我们也要考虑锁的开销，尽可能只对关键代码进行加锁。写代码之前，可以先大概构思好，如何实现，考虑好数据结构等的应用，然后再去coding。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">import</span> <span class=\"nn\">time</span><span class=\"o\">,</span> <span class=\"nn\">threading</span>\n<span class=\"n\">balance</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n<span class=\"n\">lock</span> <span class=\"o\">=</span> <span class=\"n\">threading</span><span class=\"o\">.</span><span class=\"n\">Lock</span><span class=\"p\">()</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">change_it</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"p\">):</span>\n    <span class=\"k\">global</span> <span class=\"n\">balance</span>\n    <span class=\"n\">balance</span> <span class=\"o\">=</span> <span class=\"n\">balance</span> <span class=\"o\">+</span> <span class=\"n\">n</span>\n    <span class=\"n\">balance</span> <span class=\"o\">=</span> <span class=\"n\">balance</span> <span class=\"o\">-</span> <span class=\"n\">n</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">run_thread</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"p\">):</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">100000</span><span class=\"p\">):</span>\n        <span class=\"c1\"># 先要获取锁:</span>\n        <span class=\"n\">lock</span><span class=\"o\">.</span><span class=\"n\">acquire</span><span class=\"p\">()</span>\n        <span class=\"k\">try</span><span class=\"p\">:</span>\n            <span class=\"c1\"># 放心地改吧:</span>\n            <span class=\"n\">change_it</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"p\">)</span>\n        <span class=\"k\">finally</span><span class=\"p\">:</span>\n            <span class=\"c1\"># 改完了一定要释放锁:</span>\n            <span class=\"n\">lock</span><span class=\"o\">.</span><span class=\"n\">release</span><span class=\"p\">()</span>\n\n<span class=\"n\">t1</span> <span class=\"o\">=</span> <span class=\"n\">threading</span><span class=\"o\">.</span><span class=\"n\">Thread</span><span class=\"p\">(</span><span class=\"n\">target</span><span class=\"o\">=</span><span class=\"n\">run_thread</span><span class=\"p\">,</span> <span class=\"n\">args</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">5</span><span class=\"p\">,))</span>\n<span class=\"n\">t2</span> <span class=\"o\">=</span> <span class=\"n\">threading</span><span class=\"o\">.</span><span class=\"n\">Thread</span><span class=\"p\">(</span><span class=\"n\">target</span><span class=\"o\">=</span><span class=\"n\">run_thread</span><span class=\"p\">,</span> <span class=\"n\">args</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">8</span><span class=\"p\">,))</span>\n<span class=\"n\">t1</span><span class=\"o\">.</span><span class=\"n\">start</span><span class=\"p\">()</span>\n<span class=\"n\">t2</span><span class=\"o\">.</span><span class=\"n\">start</span><span class=\"p\">()</span>\n<span class=\"n\">t1</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">()</span>\n<span class=\"n\">t2</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">()</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">balance</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># 0</span></code></pre></div><p>Python 的线程虽然是真正的线程，但解释器执行代码时，有一个 GIL 锁(Global Interpreter Lock)，任何 Python 线程执行前，必须先获得 GIL 锁。每执行 100 条字节码，解释器就自动释放 GIL 锁，让别的线程有机会执行。这个 GIL 全局锁实际上把所有线程的执行代码都给上了锁。所以，多线程在 Python 中只能交替执行，即使 100 个线程跑在 100 核 CPU 上，也只能用到 1 个核。</p><p>GIL 是 Python 解释器设计的历史遗留问题，通常我们用的解释器是官方实现的 CPython，要真正利用多核，除非重写一个不带 GIL 的解释器。所以，在 Python 如果一定要通过多线程利用多核，那只能通过 C 扩展来实现。因而，多线程的并发在 Python 中就是一个美梦，如果想真正实现多核任务，还是通过多进程来实现吧。</p><p>篇幅有限，本文不再针对进程(线程)之间的通信进行介绍，有兴趣的可以直接去这儿<a href=\"https://link.zhihu.com/?target=https%3A//www.jianshu.com/p/c1015f5ffa74\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">jianshu.com/p/c1015f5ff</span><span class=\"invisible\">a74</span><span class=\"ellipsis\"></span></a>。</p><h2>5.推广</h2><p>更多内容请关注公众号<b>谓之小一</b>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p><p><a href=\"https://link.zhihu.com/?target=http%3A//weixin.qq.com/r/Ty_4oDPE2W6mrXfD93pd\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">weixin.qq.com/r/Ty_4oDP</span><span class=\"invisible\">E2W6mrXfD93pd</span><span class=\"ellipsis\"></span></a> (二维码自动识别)</p><p></p>", 
            "topic": [
                {
                    "tag": "线程", 
                    "tagLink": "https://api.zhihu.com/topics/19619468"
                }, 
                {
                    "tag": "进程", 
                    "tagLink": "https://api.zhihu.com/topics/19634510"
                }, 
                {
                    "tag": "多线程", 
                    "tagLink": "https://api.zhihu.com/topics/19619463"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/42059170", 
            "userName": "谓之小一", 
            "userLink": "https://www.zhihu.com/people/761dda159c5cec5fd926f761ab5f4ccf", 
            "upvote": 2, 
            "title": "基于google protobuf的gRPC实现", 
            "content": "<h2>1.Protobuf简介</h2><p><b>Protobuf(Google Protocol Buffers)</b>提供一种灵活、高效、自动化的机制，用于序列化结构数据。Protobuf仅需自定义一次所需要的数据格式，然后我们就可以使用Protobuf编译器自动生成各种语言的源码，方便我们读写自定义的格式化数据。另外Protobuf的使用与平台和语言无关，可以在不破坏原数据格式的基础上，扩展新的数据。</p><p>我们可以将Protobuf与XML进行对比，但Protobuf更小、更快、更加简单。总结来说具有一下特点： </p><ul><li>性能好、效率高。Protobuf作用与XML、json类似，但它是二进制格式，所以性能更好。但同时因为是二进制格式，所以缺点也就是可读性差。 </li><li>代码生成机制，易于使用。 </li><li>解析速度快。 </li><li>支持多种语言，例C++、C#、Go、Java、Python等。 </li><li>向前兼容，向后兼容。</li></ul><h2>2.Protobuf安装</h2><p>Mac用户可以使用brew进行安装，命令如下所示。</p><blockquote> brew install protobuf</blockquote><p>如需要安装特定版本，可以先进行搜索有哪些版本，命令如下所示。搜索完成之后，采用上述brew安装方法，安装特定版本即可。</p><blockquote> brew search protobuf</blockquote><p>安装完成之后，可以通过protoc --version查看是否安装成功。</p><blockquote> protoc --version libprotoc 3.6.0</blockquote><p>另外可以通过which protoc命令查看protoc安装所在的位置。</p><blockquote> which protoc /usr/local/bin/protoc</blockquote><h2>3.Protobuf实例</h2><h2>3.1编译.proto文件</h2><p>首先我们需要创建一个以<b>.proto</b>结尾的文件，可以在其中定义<b>message</b>来指定所需要序列化的数据格式。每一个message都是一个小的信息逻辑单元，包含一系列的name-value值对。以官网上的示例，我们创建一个addressbook.proto文件，内容如下所示。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">syntax</span> <span class=\"o\">=</span> <span class=\"s2\">&#34;proto2&#34;</span><span class=\"p\">;</span>\n\n<span class=\"n\">package</span> <span class=\"n\">tutorial</span><span class=\"p\">;</span>\n\n<span class=\"n\">message</span> <span class=\"n\">Person</span> <span class=\"p\">{</span>\n  <span class=\"n\">required</span> <span class=\"n\">string</span> <span class=\"n\">name</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">;</span>\n  <span class=\"n\">required</span> <span class=\"n\">int32</span> <span class=\"nb\">id</span> <span class=\"o\">=</span> <span class=\"mi\">2</span><span class=\"p\">;</span>\n  <span class=\"n\">optional</span> <span class=\"n\">string</span> <span class=\"n\">email</span> <span class=\"o\">=</span> <span class=\"mi\">3</span><span class=\"p\">;</span>\n\n  <span class=\"n\">enum</span> <span class=\"n\">PhoneType</span> <span class=\"p\">{</span>\n    <span class=\"n\">MOBILE</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">;</span>\n    <span class=\"n\">HOME</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">;</span>\n    <span class=\"n\">WORK</span> <span class=\"o\">=</span> <span class=\"mi\">2</span><span class=\"p\">;</span>\n  <span class=\"p\">}</span>\n\n  <span class=\"n\">message</span> <span class=\"n\">PhoneNumber</span> <span class=\"p\">{</span>\n    <span class=\"n\">required</span> <span class=\"n\">string</span> <span class=\"n\">number</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">;</span>\n    <span class=\"n\">optional</span> <span class=\"n\">PhoneType</span> <span class=\"nb\">type</span> <span class=\"o\">=</span> <span class=\"mi\">2</span> <span class=\"p\">[</span><span class=\"n\">default</span> <span class=\"o\">=</span> <span class=\"n\">HOME</span><span class=\"p\">];</span>\n  <span class=\"p\">}</span>\n\n  <span class=\"n\">repeated</span> <span class=\"n\">PhoneNumber</span> <span class=\"n\">phones</span> <span class=\"o\">=</span> <span class=\"mi\">4</span><span class=\"p\">;</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">message</span> <span class=\"n\">AddressBook</span> <span class=\"p\">{</span>\n  <span class=\"n\">repeated</span> <span class=\"n\">Person</span> <span class=\"n\">people</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">;</span>\n<span class=\"p\">}</span></code></pre></div><ul><li><b>syntax=”proto2”</b>代表版本，目前支持proto2和proto3，不写默认proto2。</li><li><b>package</b>类似于C++中的namespace概念。</li><li><b>message</b>是包含了各种类型字段的聚集，相当于struct，并且可以嵌套。</li><li>proto3版本去掉了required和optional类型，保留了repeated(数组)。其中“＝1”，“＝2”表示每个元素的标识号，它会用在二进制编码中对域的标识，[1,15]之内的标志符在使用时占用一个字节，[16,2047]之内的标识号则占用2个字节，所以从最优化角度考虑，可以将[1,15]使用在一些较常用或repeated的元素上。同时为了考虑将来可能会增加新的标志符，我们要事先预留一些标志符。</li></ul><p>构建好addressbook.proto文件后，运行Protobuf编译器编译.proto文件，运行方法如下所示。其中-I表示.protoc所在的路径，--python_out表示指定生成的目标文件存在的路径，最后的参数表示要编译的.proto文件。</p><blockquote> protoc -I=\\$SRC_DIR --python_out=\\$DST_DIR \\$SRC_DIR/addressbook.proto </blockquote><p>其中SRC_DIR为目录，如果处于当前目录的话，可通过如下所示命令来编译.proto文件。</p><blockquote> protoc -I=. --python_out=. addressbook.proto</blockquote><p>编译完成之后会生成addressbook_pb2.py文件，里面包含序列化和反序列化等方法。</p><h2>3.2序列化</h2><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">import</span> <span class=\"nn\">addressbook_pb2</span>\n<span class=\"kn\">import</span> <span class=\"nn\">sys</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">PromptForAddress</span><span class=\"p\">(</span><span class=\"n\">person</span><span class=\"p\">):</span>\n  <span class=\"n\">person</span><span class=\"o\">.</span><span class=\"nb\">id</span> <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"nb\">raw_input</span><span class=\"p\">(</span><span class=\"s2\">&#34;Enter person ID number: &#34;</span><span class=\"p\">))</span>\n  <span class=\"n\">person</span><span class=\"o\">.</span><span class=\"n\">name</span> <span class=\"o\">=</span> <span class=\"nb\">raw_input</span><span class=\"p\">(</span><span class=\"s2\">&#34;Enter name: &#34;</span><span class=\"p\">)</span>\n\n  <span class=\"n\">email</span> <span class=\"o\">=</span> <span class=\"nb\">raw_input</span><span class=\"p\">(</span><span class=\"s2\">&#34;Enter email address (blank for none): &#34;</span><span class=\"p\">)</span>\n  <span class=\"k\">if</span> <span class=\"n\">email</span> <span class=\"o\">!=</span> <span class=\"s2\">&#34;&#34;</span><span class=\"p\">:</span>\n    <span class=\"n\">person</span><span class=\"o\">.</span><span class=\"n\">email</span> <span class=\"o\">=</span> <span class=\"n\">email</span>\n\n  <span class=\"k\">while</span> <span class=\"bp\">True</span><span class=\"p\">:</span>\n    <span class=\"n\">number</span> <span class=\"o\">=</span> <span class=\"nb\">raw_input</span><span class=\"p\">(</span><span class=\"s2\">&#34;Enter a phone number (or leave blank to finish): &#34;</span><span class=\"p\">)</span>\n    <span class=\"k\">if</span> <span class=\"n\">number</span> <span class=\"o\">==</span> <span class=\"s2\">&#34;&#34;</span><span class=\"p\">:</span>\n        <span class=\"k\">break</span>\n\n    <span class=\"n\">phone_number</span> <span class=\"o\">=</span> <span class=\"n\">person</span><span class=\"o\">.</span><span class=\"n\">phones</span><span class=\"o\">.</span><span class=\"n\">add</span><span class=\"p\">()</span>\n    <span class=\"n\">phone_number</span><span class=\"o\">.</span><span class=\"n\">number</span> <span class=\"o\">=</span> <span class=\"n\">number</span>\n\n    <span class=\"nb\">type</span> <span class=\"o\">=</span> <span class=\"nb\">raw_input</span><span class=\"p\">(</span><span class=\"s2\">&#34;Is this a mobile, home, or work phone? &#34;</span><span class=\"p\">)</span>\n    <span class=\"k\">if</span> <span class=\"nb\">type</span> <span class=\"o\">==</span> <span class=\"s2\">&#34;mobile&#34;</span><span class=\"p\">:</span>\n        <span class=\"n\">phone_number</span><span class=\"o\">.</span><span class=\"nb\">type</span> <span class=\"o\">=</span> <span class=\"n\">addressbook_pb2</span><span class=\"o\">.</span><span class=\"n\">Person</span><span class=\"o\">.</span><span class=\"n\">MOBILE</span>\n    <span class=\"k\">elif</span> <span class=\"nb\">type</span> <span class=\"o\">==</span> <span class=\"s2\">&#34;home&#34;</span><span class=\"p\">:</span>\n        <span class=\"n\">phone_number</span><span class=\"o\">.</span><span class=\"nb\">type</span> <span class=\"o\">=</span> <span class=\"n\">addressbook_pb2</span><span class=\"o\">.</span><span class=\"n\">Person</span><span class=\"o\">.</span><span class=\"n\">HOME</span>\n    <span class=\"k\">elif</span> <span class=\"nb\">type</span> <span class=\"o\">==</span> <span class=\"s2\">&#34;work&#34;</span><span class=\"p\">:</span>\n        <span class=\"n\">phone_number</span><span class=\"o\">.</span><span class=\"nb\">type</span> <span class=\"o\">=</span> <span class=\"n\">addressbook_pb2</span><span class=\"o\">.</span><span class=\"n\">Person</span><span class=\"o\">.</span><span class=\"n\">WORK</span>\n    <span class=\"k\">else</span><span class=\"p\">:</span>\n        <span class=\"k\">print</span> <span class=\"s2\">&#34;Unknown phone type; leaving as default value.&#34;</span>\n\n\n<span class=\"k\">if</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">sys</span><span class=\"o\">.</span><span class=\"n\">argv</span><span class=\"p\">)</span> <span class=\"o\">!=</span> <span class=\"mi\">2</span><span class=\"p\">:</span>\n  <span class=\"k\">print</span> <span class=\"s2\">&#34;Usage:&#34;</span><span class=\"p\">,</span> <span class=\"n\">sys</span><span class=\"o\">.</span><span class=\"n\">argv</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"s2\">&#34;ADDRESS_BOOK_FILE&#34;</span>\n  <span class=\"n\">sys</span><span class=\"o\">.</span><span class=\"nb\">exit</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n\n<span class=\"n\">address_book</span> <span class=\"o\">=</span> <span class=\"n\">addressbook_pb2</span><span class=\"o\">.</span><span class=\"n\">AddressBook</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># Read the existing address book.</span>\n<span class=\"k\">try</span><span class=\"p\">:</span>\n  <span class=\"n\">f</span> <span class=\"o\">=</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"n\">sys</span><span class=\"o\">.</span><span class=\"n\">argv</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"s2\">&#34;rb&#34;</span><span class=\"p\">)</span>\n  <span class=\"n\">address_book</span><span class=\"o\">.</span><span class=\"n\">ParseFromString</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"o\">.</span><span class=\"n\">read</span><span class=\"p\">())</span>\n  <span class=\"n\">f</span><span class=\"o\">.</span><span class=\"n\">close</span><span class=\"p\">()</span>\n<span class=\"k\">except</span> <span class=\"ne\">IOError</span><span class=\"p\">:</span>\n  <span class=\"k\">print</span> <span class=\"n\">sys</span><span class=\"o\">.</span><span class=\"n\">argv</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"s2\">&#34;: Could not open file.  Creating a new one.&#34;</span>\n\n<span class=\"c1\"># Add an address.</span>\n<span class=\"n\">PromptForAddress</span><span class=\"p\">(</span><span class=\"n\">address_book</span><span class=\"o\">.</span><span class=\"n\">people</span><span class=\"o\">.</span><span class=\"n\">add</span><span class=\"p\">())</span>\n\n<span class=\"c1\"># Write the new address book back to disk.</span>\n<span class=\"n\">f</span> <span class=\"o\">=</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"n\">sys</span><span class=\"o\">.</span><span class=\"n\">argv</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"s2\">&#34;wb&#34;</span><span class=\"p\">)</span>\n<span class=\"n\">f</span><span class=\"o\">.</span><span class=\"n\">write</span><span class=\"p\">(</span><span class=\"n\">address_book</span><span class=\"o\">.</span><span class=\"n\">SerializeToString</span><span class=\"p\">())</span>\n<span class=\"n\">f</span><span class=\"o\">.</span><span class=\"n\">close</span><span class=\"p\">()</span></code></pre></div><p>创建add_person.py文件，代码如上所示，然后通过SerializeToString()方法来进行序列化addressbook.proto中所定义的信息。如果想要运行上述代码的话，我们首先需要创建一个输入文件，例如命名为input.txt，不需输入值。然后采用<code>python add_person input.txt</code>，便可进行序列化所输入的数据。如果运行<code>python add_person</code>的话，不指定输入文件，则会报错。</p><blockquote>Enter person ID number: 1001<br/>Enter name: 1001 <br/>Enter email address (blank for none): <a href=\"mailto:hello@email.com\">hello@email.com</a><br/>Enter a phone number (or leave blank to finish): 10010 <br/>Is this a mobile, home, or work phone? work <br/>Enter a phone number (or leave blank to finish): </blockquote><h2>3.3反序列化</h2><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"ch\">#! /usr/bin/python</span>\n<span class=\"kn\">import</span> <span class=\"nn\">addressbook_pb2</span>\n<span class=\"kn\">import</span> <span class=\"nn\">sys</span>\n\n<span class=\"c1\"># Iterates though all people in the AddressBook and prints info about them.</span>\n<span class=\"k\">def</span> <span class=\"nf\">ListPeople</span><span class=\"p\">(</span><span class=\"n\">address_book</span><span class=\"p\">):</span>\n  <span class=\"k\">for</span> <span class=\"n\">person</span> <span class=\"ow\">in</span> <span class=\"n\">address_book</span><span class=\"o\">.</span><span class=\"n\">people</span><span class=\"p\">:</span>\n    <span class=\"k\">print</span> <span class=\"s2\">&#34;Person ID:&#34;</span><span class=\"p\">,</span> <span class=\"n\">person</span><span class=\"o\">.</span><span class=\"nb\">id</span>\n    <span class=\"k\">print</span> <span class=\"s2\">&#34;  Name:&#34;</span><span class=\"p\">,</span> <span class=\"n\">person</span><span class=\"o\">.</span><span class=\"n\">name</span>\n    <span class=\"k\">if</span> <span class=\"n\">person</span><span class=\"o\">.</span><span class=\"n\">HasField</span><span class=\"p\">(</span><span class=\"s1\">&#39;email&#39;</span><span class=\"p\">):</span>\n      <span class=\"k\">print</span> <span class=\"s2\">&#34;  E-mail address:&#34;</span><span class=\"p\">,</span> <span class=\"n\">person</span><span class=\"o\">.</span><span class=\"n\">email</span>\n\n    <span class=\"k\">for</span> <span class=\"n\">phone_number</span> <span class=\"ow\">in</span> <span class=\"n\">person</span><span class=\"o\">.</span><span class=\"n\">phones</span><span class=\"p\">:</span>\n      <span class=\"k\">if</span> <span class=\"n\">phone_number</span><span class=\"o\">.</span><span class=\"nb\">type</span> <span class=\"o\">==</span> <span class=\"n\">addressbook_pb2</span><span class=\"o\">.</span><span class=\"n\">Person</span><span class=\"o\">.</span><span class=\"n\">MOBILE</span><span class=\"p\">:</span>\n        <span class=\"k\">print</span> <span class=\"s2\">&#34;  Mobile phone #: &#34;</span><span class=\"p\">,</span>\n      <span class=\"k\">elif</span> <span class=\"n\">phone_number</span><span class=\"o\">.</span><span class=\"nb\">type</span> <span class=\"o\">==</span> <span class=\"n\">addressbook_pb2</span><span class=\"o\">.</span><span class=\"n\">Person</span><span class=\"o\">.</span><span class=\"n\">HOME</span><span class=\"p\">:</span>\n        <span class=\"k\">print</span> <span class=\"s2\">&#34;  Home phone #: &#34;</span><span class=\"p\">,</span>\n      <span class=\"k\">elif</span> <span class=\"n\">phone_number</span><span class=\"o\">.</span><span class=\"nb\">type</span> <span class=\"o\">==</span> <span class=\"n\">addressbook_pb2</span><span class=\"o\">.</span><span class=\"n\">Person</span><span class=\"o\">.</span><span class=\"n\">WORK</span><span class=\"p\">:</span>\n        <span class=\"k\">print</span> <span class=\"s2\">&#34;  Work phone #: &#34;</span><span class=\"p\">,</span>\n      <span class=\"k\">print</span> <span class=\"n\">phone_number</span><span class=\"o\">.</span><span class=\"n\">number</span>\n\n<span class=\"c1\"># Main procedure:  Reads the entire address book from a file and prints all</span>\n<span class=\"c1\">#   the information inside.</span>\n\n<span class=\"k\">if</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">sys</span><span class=\"o\">.</span><span class=\"n\">argv</span><span class=\"p\">)</span> <span class=\"o\">!=</span> <span class=\"mi\">2</span><span class=\"p\">:</span>\n  <span class=\"k\">print</span> <span class=\"s2\">&#34;Usage:&#34;</span><span class=\"p\">,</span> <span class=\"n\">sys</span><span class=\"o\">.</span><span class=\"n\">argv</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"s2\">&#34;ADDRESS_BOOK_FILE&#34;</span>\n  <span class=\"n\">sys</span><span class=\"o\">.</span><span class=\"nb\">exit</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n\n<span class=\"n\">address_book</span> <span class=\"o\">=</span> <span class=\"n\">addressbook_pb2</span><span class=\"o\">.</span><span class=\"n\">AddressBook</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># Read the existing address book.</span>\n<span class=\"n\">f</span> <span class=\"o\">=</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"n\">sys</span><span class=\"o\">.</span><span class=\"n\">argv</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"s2\">&#34;rb&#34;</span><span class=\"p\">)</span>\n<span class=\"n\">address_book</span><span class=\"o\">.</span><span class=\"n\">ParseFromString</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"o\">.</span><span class=\"n\">read</span><span class=\"p\">())</span>\n<span class=\"n\">f</span><span class=\"o\">.</span><span class=\"n\">close</span><span class=\"p\">()</span>\n\n<span class=\"n\">ListPeople</span><span class=\"p\">(</span><span class=\"n\">address_book</span><span class=\"p\">)</span></code></pre></div><p>创建list_person.py文件来进行反序列化，代码如上所示。通过<code>python list_person.py input.txt</code>命令来执行上述代码，输出结果如下所示。</p><blockquote>Person ID: 1001<br/>Name: 1001<br/>E-mail address: <a href=\"mailto:hello@email.com\">hello@email.com</a><br/>Work phone #:  10010</blockquote><h2>4.RPC简介</h2><p>这里引用知乎用户<b>用心阁</b>关于<b>谁能用通俗的语言解释一下什么是 RPC 框架？</b>的问题答案来解释什么是RPC。<b>RPC(Remote Procedure Call)</b>是指远程过程调用，也就是说两台服务器A、B，一个应用部署在A服务器上，想要调用B服务器上应用提供的函数/方法，由于不在一个内存空间上，不能直接调用，需要通过网络来表达调用的语义和传达调用的数据。如果需要实现RPC，那么需要解决如下几个问题。</p><ul><li>通讯：主要是通过在客户端和服务器之间建立TCP连接，远程过程调用的所有交换的数据都在这个连接里传输。连接可以是按需连接，调用结束后就断掉，也可以是长连接，多个远程过程调用共享同一个连接。 </li><li>寻址：A服务器上的应用怎么告诉底层的RPC框架，如何连接到B服务器（如主机或IP地址）以及特定的端口，方法的名称名称是什么。</li><li>序列化：当A服务器上的应用发起远程过程调用时，方法的参数需要通过底层的网络协议，如TCP传递到B服务器。由于网络协议是基于二进制的，内存中的参数值要序列化成二进制的形式，也就是序列化（Serialize）或编组（marshal），通过寻址和传输将序列化的二进制发送给B服务器。 B服务器收到请求后，需要对参数进行反序列化，恢复为内存中的表达方式，然后找到对应的方法进行本地调用，然后得到返回值。 返回值还要发送回服务器A上的应用，也要经过序列化的方式发送，服务器A接到后，再反序列化，恢复为内存中的表达方式，交给A服务器上的应用 。</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-67b8d1eef9e87a53b8325de9a70b3047_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"952\" data-rawheight=\"522\" class=\"origin_image zh-lightbox-thumb\" width=\"952\" data-original=\"https://pic4.zhimg.com/v2-67b8d1eef9e87a53b8325de9a70b3047_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;952&#39; height=&#39;522&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"952\" data-rawheight=\"522\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"952\" data-original=\"https://pic4.zhimg.com/v2-67b8d1eef9e87a53b8325de9a70b3047_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-67b8d1eef9e87a53b8325de9a70b3047_b.jpg\"/></figure><p>总结来说，RPC提供一种透明调用机制让使用者不必显示区分本地调用还是远程调用。如上图所示，客户方像调用本地方法一样去调用远程接口方法，RPC 框架提供接口的代理实现，实际的调用将委托给代理<code>RpcProxy</code> 。代理封装调用信息并将调用转交给<code>RpcInvoker</code> 去实际执行。在客户端的<code>RpcInvoker</code> 通过连接器<code>RpcConnector</code> 去维持与服务端的通道<code>RpcChannel</code>，并使用<code>RpcProtocol</code> 执行协议编码（encode）并将编码后的请求消息通过通道发送给服务方。RPC 服务端接收器 <code>RpcAcceptor</code> 接收客户端的调用请求，同样使用<code>RpcProtocol</code> 执行协议解码（decode）。解码后的调用信息传递给<code>RpcProcessor</code> 去控制处理调用过程，最后再委托调用给<code>RpcInvoker</code> 去实际执行并返回调用结果。</p><h2>5.基于google protobuf的gRPC实现</h2><p>我们可以利用protobuf实现序列化和反序列化，但如何实现RPC通信呢。为简单起见，我们先介绍gRPC，gRPC是google构建的RPC框架，这样我们就不再考虑如何写通信方法。</p><h2>5.1gRPC安装</h2><p>首先安装gRPC，安装命令如下所示。</p><blockquote> pip install grpcio</blockquote><p>然后安装protobuf相关的依赖库。</p><blockquote> pip install protobuf</blockquote><p>然后安装python gRPC相关的protobuf相关文件。</p><blockquote> pip install grpcio-tools</blockquote><h2>5.2gRPC实例</h2><p>创建三个文件夹，名称为example、server、client，里面内容如下所示，具体含义在后面解释。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-64b1cf1b9e6816ca961b9e3786392698_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1104\" data-rawheight=\"632\" class=\"origin_image zh-lightbox-thumb\" width=\"1104\" data-original=\"https://pic1.zhimg.com/v2-64b1cf1b9e6816ca961b9e3786392698_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1104&#39; height=&#39;632&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1104\" data-rawheight=\"632\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1104\" data-original=\"https://pic1.zhimg.com/v2-64b1cf1b9e6816ca961b9e3786392698_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-64b1cf1b9e6816ca961b9e3786392698_b.jpg\"/></figure><p> 5.2.1 example</p><p>example主要用于编写.proto文件并生成data接口，其中__init__.py的作用是方便其他文件夹引用example文件夹中文件，data.proto文件内容如下所示。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">syntax</span><span class=\"o\">=</span><span class=\"s2\">&#34;proto3&#34;</span><span class=\"p\">;</span>\n<span class=\"n\">package</span> <span class=\"n\">example</span><span class=\"p\">;</span>\n\n<span class=\"n\">message</span> <span class=\"n\">Data</span><span class=\"p\">{</span>\n    <span class=\"n\">string</span> <span class=\"n\">text</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">;</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">service</span> <span class=\"n\">FormatData</span><span class=\"p\">{</span>\n    <span class=\"n\">rpc</span> <span class=\"n\">DoFormat</span><span class=\"p\">(</span><span class=\"n\">Data</span><span class=\"p\">)</span> <span class=\"n\">returns</span> <span class=\"p\">(</span><span class=\"n\">Data</span><span class=\"p\">)</span> <span class=\"p\">{}</span>\n<span class=\"p\">}</span></code></pre></div><p>然后在example目录下利用下述命令生成data_pb2.py和data_pb2_grpc.py文件。data_pb2.py用于序列化信息，data_pb2_grpc.py用于通信。</p><blockquote> python -m grpc_tools.protoc -I. --python_out=. --grpc_python_out=. ./data.proto</blockquote><h2>5.2.2 server</h2><p>server为服务器端，server.py实现接受客户端发送的数据，并对数据进行处理后返回给客户端。FormatData的作用是将服务器端传过来的数据转换为大写，具体含义见相关代码和注释。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"ch\">#! /usr/bin/env python</span>\n<span class=\"c1\"># -*- coding: utf-8 -*-</span>\n<span class=\"kn\">import</span> <span class=\"nn\">grpc</span>\n<span class=\"kn\">import</span> <span class=\"nn\">time</span>\n<span class=\"kn\">from</span> <span class=\"nn\">concurrent</span> <span class=\"kn\">import</span> <span class=\"n\">futures</span> <span class=\"c1\">#具有线程池和进程池、管理并行编程任务、处理非确定性的执行流程、进程/线程同步等功能</span>\n<span class=\"kn\">from</span> <span class=\"nn\">example</span> <span class=\"kn\">import</span> <span class=\"n\">data_pb2</span>\n<span class=\"kn\">from</span> <span class=\"nn\">example</span> <span class=\"kn\">import</span> <span class=\"n\">data_pb2_grpc</span>\n\n<span class=\"n\">_ONE_DAY_IN_SECONDS</span> <span class=\"o\">=</span> <span class=\"mi\">60</span><span class=\"o\">*</span><span class=\"mi\">60</span><span class=\"o\">*</span><span class=\"mi\">24</span>\n<span class=\"n\">_HOST</span><span class=\"o\">=</span><span class=\"s1\">&#39;localhost&#39;</span>\n<span class=\"n\">_PORT</span><span class=\"o\">=</span><span class=\"s1\">&#39;8080&#39;</span>\n\n<span class=\"k\">class</span> <span class=\"nc\">FormatData</span><span class=\"p\">(</span><span class=\"n\">data_pb2_grpc</span><span class=\"o\">.</span><span class=\"n\">FormatDataServicer</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"nf\">DoFormat</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span><span class=\"n\">request</span><span class=\"p\">,</span><span class=\"n\">context</span><span class=\"p\">):</span>\n        <span class=\"nb\">str</span><span class=\"o\">=</span><span class=\"n\">request</span><span class=\"o\">.</span><span class=\"n\">text</span>\n        <span class=\"k\">return</span> <span class=\"n\">data_pb2</span><span class=\"o\">.</span><span class=\"n\">Data</span><span class=\"p\">(</span><span class=\"n\">text</span><span class=\"o\">=</span><span class=\"nb\">str</span><span class=\"o\">.</span><span class=\"n\">upper</span><span class=\"p\">())</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">serve</span><span class=\"p\">():</span>\n    <span class=\"n\">grpcServer</span><span class=\"o\">=</span><span class=\"n\">grpc</span><span class=\"o\">.</span><span class=\"n\">server</span><span class=\"p\">(</span><span class=\"n\">futures</span><span class=\"o\">.</span><span class=\"n\">ThreadPoolExecutor</span><span class=\"p\">(</span><span class=\"n\">max_workers</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">))</span><span class=\"c1\">#最多有多少work并行执行任务</span>\n    <span class=\"n\">data_pb2_grpc</span><span class=\"o\">.</span><span class=\"n\">add_FormatDataServicer_to_server</span><span class=\"p\">(</span><span class=\"n\">FormatData</span><span class=\"p\">(),</span><span class=\"n\">grpcServer</span><span class=\"p\">)</span><span class=\"c1\"># 添加函数方法和服务器，服务器端会进行反序列化。</span>\n    <span class=\"n\">grpcServer</span><span class=\"o\">.</span><span class=\"n\">add_insecure_port</span><span class=\"p\">(</span><span class=\"n\">_HOST</span><span class=\"o\">+</span><span class=\"s1\">&#39;:&#39;</span><span class=\"o\">+</span><span class=\"n\">_PORT</span><span class=\"p\">)</span> <span class=\"c1\">#建立服务器和端口</span>\n    <span class=\"n\">grpcServer</span><span class=\"o\">.</span><span class=\"n\">start</span><span class=\"p\">()</span><span class=\"c1\"># 启动服务端</span>\n    <span class=\"k\">try</span><span class=\"p\">:</span>\n        <span class=\"k\">while</span> <span class=\"bp\">True</span><span class=\"p\">:</span>\n            <span class=\"n\">time</span><span class=\"o\">.</span><span class=\"n\">sleep</span><span class=\"p\">(</span><span class=\"n\">_ONE_DAY_IN_SECONDS</span><span class=\"p\">)</span>\n    <span class=\"k\">except</span> <span class=\"ne\">KeyboardInterrupt</span><span class=\"p\">:</span>\n        <span class=\"n\">grpcServer</span><span class=\"o\">.</span><span class=\"n\">stop</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n\n\n<span class=\"k\">if</span> <span class=\"vm\">__name__</span><span class=\"o\">==</span><span class=\"s1\">&#39;__main__&#39;</span><span class=\"p\">:</span>\n    <span class=\"n\">serve</span><span class=\"p\">()</span></code></pre></div><h2>5.2.3 client</h2><p>clinet为客户端，client.py实现客户端发送数据，并接受server处理后返回的数据，具体含义见相关代码和注释。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"ch\">#! /usr/bin/env python</span>\n<span class=\"c1\"># -*- coding: utf-8 -*-</span>\n<span class=\"kn\">import</span> <span class=\"nn\">grpc</span>\n<span class=\"kn\">from</span> <span class=\"nn\">example</span> <span class=\"kn\">import</span> <span class=\"n\">data_pb2</span><span class=\"p\">,</span><span class=\"n\">data_pb2_grpc</span>\n\n<span class=\"n\">_HOST</span><span class=\"o\">=</span><span class=\"s1\">&#39;localhost&#39;</span>\n<span class=\"n\">_PORT</span><span class=\"o\">=</span><span class=\"s1\">&#39;8080&#39;</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">run</span><span class=\"p\">():</span>\n    <span class=\"n\">conn</span><span class=\"o\">=</span><span class=\"n\">grpc</span><span class=\"o\">.</span><span class=\"n\">insecure_channel</span><span class=\"p\">(</span><span class=\"n\">_HOST</span><span class=\"o\">+</span><span class=\"s1\">&#39;:&#39;</span><span class=\"o\">+</span><span class=\"n\">_PORT</span><span class=\"p\">)</span><span class=\"c1\"># 服务器信息</span>\n    <span class=\"n\">client</span><span class=\"o\">=</span><span class=\"n\">data_pb2_grpc</span><span class=\"o\">.</span><span class=\"n\">FormatDataStub</span><span class=\"p\">(</span><span class=\"n\">channel</span><span class=\"o\">=</span><span class=\"n\">conn</span><span class=\"p\">)</span> <span class=\"c1\">#客户端建立连接</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">5</span><span class=\"p\">):</span>\n        <span class=\"n\">respnse</span> <span class=\"o\">=</span> <span class=\"n\">client</span><span class=\"o\">.</span><span class=\"n\">DoFormat</span><span class=\"p\">(</span><span class=\"n\">data_pb2</span><span class=\"o\">.</span><span class=\"n\">Data</span><span class=\"p\">(</span><span class=\"n\">text</span><span class=\"o\">=</span><span class=\"s1\">&#39;hello,world!&#39;</span><span class=\"p\">))</span>  <span class=\"c1\"># 序列化数据传递过去</span>\n        <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s2\">&#34;received: &#34;</span> <span class=\"o\">+</span> <span class=\"n\">respnse</span><span class=\"o\">.</span><span class=\"n\">text</span><span class=\"p\">)</span>\n\n<span class=\"k\">if</span> <span class=\"vm\">__name__</span><span class=\"o\">==</span><span class=\"s1\">&#39;__main__&#39;</span><span class=\"p\">:</span>\n    <span class=\"n\">run</span><span class=\"p\">()</span></code></pre></div><p>接下来运行server.py来启动服务器，然后运行client.py便可以得到结果，可以看到所有数据均已大写。最后需要关闭服务器端，否则一直会处于运行状态。</p><blockquote>received: HELLO,WORLD! <br/>received: HELLO,WORLD! <br/>received: HELLO,WORLD! <br/>received: HELLO,WORLD! <br/>received: HELLO,WORLD!</blockquote><h2>6.基于google protobuf的RPC实现</h2><p>因为RPC需要我们实现通信，所以会有一定难度，代码量很大程度上也有增加，不方便在文中展现出来。所以我把代码放到了github上面，地址在<a href=\"https://link.zhihu.com/?target=https%3A//github.com/weizhixiaoyi/google-protobuf-service\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/weizhixiaoyi</span><span class=\"invisible\">/google-protobuf-service</span><span class=\"ellipsis\"></span></a>，有兴趣的可以看下。</p><p>总的来说，protobuf RPC定义了一个抽象的RPC框架，RpcServiceStub和RpcService类是protobuf编译器根据proto定义生成的类，RpcService定义了服务端暴露给客户端的函数接口，具体实现需要用户自己继承这个类来实现。RpcServiceStub定义了服务端暴露函数的描述，并将客户端对RpcServiceStub中函数的调用统一转换到调用RpcChannel中的CallMethod方法，CallMethod通过RpcServiceStub传过来的函数描述符和函数参数对该次rpc调用进行encode，最终通过RpcConnecor发送给服务方。对方以客户端相反的过程最终调用RpcSerivice中定义的函数。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-e8efe5b1ef1e9da86255af60fb9ddc20_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"972\" data-rawheight=\"404\" class=\"origin_image zh-lightbox-thumb\" width=\"972\" data-original=\"https://pic1.zhimg.com/v2-e8efe5b1ef1e9da86255af60fb9ddc20_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;972&#39; height=&#39;404&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"972\" data-rawheight=\"404\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"972\" data-original=\"https://pic1.zhimg.com/v2-e8efe5b1ef1e9da86255af60fb9ddc20_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-e8efe5b1ef1e9da86255af60fb9ddc20_b.jpg\"/></figure><p>事实上，protobuf rpc的框架只是RpcChannel中定义了空的CallMethod，所以具体怎样进行encode和调用RpcConnector都要自己实现。RpcConnector在protobuf中没有定义，所以这个完成由用户自己实现，它的作用就是收发rpc消息包。在服务端，RpcChannel通过调用RpcService中的CallMethod来具体调用RpcService中暴露给客户端的函数。</p><p><b>参考</b></p><blockquote><a href=\"https://www.zhihu.com/question/25536695\" class=\"internal\">用心阁-谁能用通俗的语言解释一下什么是 RPC 框架？</a><br/> <a href=\"https://link.zhihu.com/?target=https%3A//www.cnblogs.com/chengxuyuancc/p/5245749.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">在于思考-python通过protobuf实现rpc</a><br/> </blockquote><h2>7.推广</h2><p>更多内容请关注公众号<b>谓之小一</b>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p><p><a href=\"https://link.zhihu.com/?target=http%3A//weixin.qq.com/r/Ty_4oDPE2W6mrXfD93pd\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">weixin.qq.com/r/Ty_4oDP</span><span class=\"invisible\">E2W6mrXfD93pd</span><span class=\"ellipsis\"></span></a> (二维码自动识别)</p><p></p>", 
            "topic": [
                {
                    "tag": "protobuf", 
                    "tagLink": "https://api.zhihu.com/topics/19564722"
                }, 
                {
                    "tag": "RPC框架", 
                    "tagLink": "https://api.zhihu.com/topics/20086162"
                }, 
                {
                    "tag": "gRPC", 
                    "tagLink": "https://api.zhihu.com/topics/20066354"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/38972380", 
            "userName": "谓之小一", 
            "userLink": "https://www.zhihu.com/people/761dda159c5cec5fd926f761ab5f4ccf", 
            "upvote": 4, 
            "title": "深度神经网络之正则化", 
            "content": "<blockquote>由于较多公式，所以我将部分内容转为图片进行上传，请见谅。清晰版请访问<a href=\"https://link.zhihu.com/?target=https%3A//weizhixiaoyi.com\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">weizhixiaoyi.com</span><span class=\"invisible\"></span></a>查看，你也可以关注我公众号<b>谓之小一</b>，后台直接向我要pdf版本，如有相关问题可公众号后台询问，随时回答。</blockquote><h2>1.正则化</h2><p>之前介绍的<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/UeEL1rOdjMpaKqTFpclS3A\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">文章</a>之中，我们已多次接触到正则化方法，但没有详细的解释为什么要正则化，什么是正则化，以及L1正则化和L2正则化的区别。本次文章之中，我们将详解机器学习中正则化的概念和深度神经网络中的正则化方法。</p><h2>1.1 为什么要正则化？</h2><p>讲到为什么需要正则化，就需要了解什么是过拟合问题。以下面图片为例，我们能够看到有两个类别，其中以X代表男生，O代表女生。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-22cc4b651156fe5dc0e5d44281d3f689_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2020\" data-rawheight=\"640\" class=\"origin_image zh-lightbox-thumb\" width=\"2020\" data-original=\"https://pic2.zhimg.com/v2-22cc4b651156fe5dc0e5d44281d3f689_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2020&#39; height=&#39;640&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2020\" data-rawheight=\"640\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2020\" data-original=\"https://pic2.zhimg.com/v2-22cc4b651156fe5dc0e5d44281d3f689_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-22cc4b651156fe5dc0e5d44281d3f689_b.jpg\"/></figure><p>我们想要通过学习来得到分类曲线，其中分类曲线能够有效区分男生和女生，现在来分析下上面的三种分类结果。</p><ul><li><b>欠拟合：</b>图1分类明显欠缺，有些男生被分为女生，有些女生被分为男生。</li><li><b>正拟合：</b>图2虽然有两个男生被分类为女生，但能够理解，毕竟我们人类自己也有分类错误的情况，比如通过化妆，女装等方法。</li><li><b>过拟合：</b>图3虽然能够全部分类正确，但结果全部正确就一定好吗？不一定，我们能够看到分类曲线明显过于复杂，模型学习的时候学习了过多的参数项，但其中某些参数项是无用的特征，比如<b>眼睛大小</b>。当我们进行识别测试集数据时，就需要提供更多的特征，如果测试集包含海量的数据，模型的时间复杂度可想而知。</li></ul><h2>1.2 什么是正则化？</h2><p>既然我们已经知道什么是过拟合，那么怎么解决过拟合问题呢？上面有介绍到，模型出现过拟合，是在模型<b>特征</b>上过于复杂。而特征又包含在我们的目标函数f(x)之中，那么只能从目标函数f(x)中寻找解决问题的方法。假设目标函数f(x)和损失函数J0为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-0fdef2c042cfeaf3bc2d4cd7399b5b80_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1286\" data-rawheight=\"380\" class=\"origin_image zh-lightbox-thumb\" width=\"1286\" data-original=\"https://pic1.zhimg.com/v2-0fdef2c042cfeaf3bc2d4cd7399b5b80_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1286&#39; height=&#39;380&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1286\" data-rawheight=\"380\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1286\" data-original=\"https://pic1.zhimg.com/v2-0fdef2c042cfeaf3bc2d4cd7399b5b80_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-0fdef2c042cfeaf3bc2d4cd7399b5b80_b.jpg\"/></figure><p>如果从<b>X</b>入手解决问题，但训练过程中我们不知道下一个样本X是什么，会怎样的影响目标函数，所以此路不通。那么<b>W</b>如何呢？我们知道W系数是训练过程中通过学习历史数据得到的，和历史数据有关，所以应该可以。现在再回到我们原来的问题，希望减少N的数目，而让N最小化，其实就是让X向量或W向量中项的个数最小化，既然X不行，那么我们可以尝试让W向量中项的个数最小化。如何求解才能让W向量中项的个数最小，我们先简单介绍下0、1、2范数的概念。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-a7c7cf95260600d376ec096b31eea1a2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1254\" data-rawheight=\"224\" class=\"origin_image zh-lightbox-thumb\" width=\"1254\" data-original=\"https://pic3.zhimg.com/v2-a7c7cf95260600d376ec096b31eea1a2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1254&#39; height=&#39;224&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1254\" data-rawheight=\"224\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1254\" data-original=\"https://pic3.zhimg.com/v2-a7c7cf95260600d376ec096b31eea1a2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-a7c7cf95260600d376ec096b31eea1a2_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-e3bfade6fdfac3106423faea9ecf5430_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1272\" data-rawheight=\"400\" class=\"origin_image zh-lightbox-thumb\" width=\"1272\" data-original=\"https://pic1.zhimg.com/v2-e3bfade6fdfac3106423faea9ecf5430_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1272&#39; height=&#39;400&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1272\" data-rawheight=\"400\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1272\" data-original=\"https://pic1.zhimg.com/v2-e3bfade6fdfac3106423faea9ecf5430_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-e3bfade6fdfac3106423faea9ecf5430_b.jpg\"/></figure><p>说完L0范数和L1范数，就不得不提L2范数。L2范数是指先求向量各元素的平方和，然后再进行求平方根，也就是通常意义上的模。同样，对于正则化问题，我们的目标是让W向量中的每个元素都很小，也就是让L2范数最小。L1范数和L2范数的不同点在于，L1范数会让其中某些元素等于0，而L2范数只是让其中元素接近0，这里有很大不同，我们在后面会进行详细讲解。最后损失函数后面添加的额外项||W||2，也就是我们称作的L2正则化。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-b2196d18b96fea6eb874a52f40ef2982_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1252\" data-rawheight=\"144\" class=\"origin_image zh-lightbox-thumb\" width=\"1252\" data-original=\"https://pic3.zhimg.com/v2-b2196d18b96fea6eb874a52f40ef2982_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1252&#39; height=&#39;144&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1252\" data-rawheight=\"144\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1252\" data-original=\"https://pic3.zhimg.com/v2-b2196d18b96fea6eb874a52f40ef2982_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-b2196d18b96fea6eb874a52f40ef2982_b.jpg\"/></figure><h2>1.3 L1正则化和L2正则化</h2><p><b>L1正则化</b>可以产生稀疏值矩阵，即产生一个稀疏模型，可以用于特征选择和解决过拟合。那什么是稀疏值矩阵呢？稀疏矩阵是矩阵中很多元素为0，只有少数元素是非零值的矩阵，稀疏矩阵的好处就是能够帮助模型找到重要特征，而去掉无用特征或影响甚小的特征。</p><p>比如在分类或预测时，很多特征难以选择，如果代入稀疏矩阵，能够筛选出少数对目标函数有贡献的特征，去掉绝大部分贡献很小或没有贡献的特征(因为稀疏矩阵很多值是0或是很小值)。因此我们只需要关注系数是非零值的特征，从而达到特征选择和解决过拟合的问题。那么为什么L1正则化可以产生稀疏模型呢？</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-f409cbef9c1b2e828058010d365c6d8d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1266\" data-rawheight=\"314\" class=\"origin_image zh-lightbox-thumb\" width=\"1266\" data-original=\"https://pic2.zhimg.com/v2-f409cbef9c1b2e828058010d365c6d8d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1266&#39; height=&#39;314&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1266\" data-rawheight=\"314\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1266\" data-original=\"https://pic2.zhimg.com/v2-f409cbef9c1b2e828058010d365c6d8d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-f409cbef9c1b2e828058010d365c6d8d_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-a837bff5e31eb0764630c38880d1e339_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1268\" data-rawheight=\"194\" class=\"origin_image zh-lightbox-thumb\" width=\"1268\" data-original=\"https://pic2.zhimg.com/v2-a837bff5e31eb0764630c38880d1e339_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1268&#39; height=&#39;194&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1268\" data-rawheight=\"194\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1268\" data-original=\"https://pic2.zhimg.com/v2-a837bff5e31eb0764630c38880d1e339_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-a837bff5e31eb0764630c38880d1e339_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-019f90836ee5966b87419c0f859fc5c8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"318\" data-rawheight=\"297\" class=\"content_image\" width=\"318\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;318&#39; height=&#39;297&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"318\" data-rawheight=\"297\" class=\"content_image lazy\" width=\"318\" data-actualsrc=\"https://pic1.zhimg.com/v2-019f90836ee5966b87419c0f859fc5c8_b.jpg\"/></figure><p>从上图可以看出，当J0等值线与L1图形首次相交的点就是最优解，也就是上图中的(0,w)。而对于L1函数有许多突出的点(二维情况下是4个)，J0函数与这些顶点接触的概率远大于与L1其他部分接触的概率，恰好在这些顶点上会有很多权值等于0，这就是为什么L1正则化可以产生稀疏模型，进而可以用于特征选择。最后针对L1正则化再介绍下系数α，其目的是控制L1图形的大小。当α越小，L1的图形越大，α越大，L1图形也就越小。L1图形可以小到在原点附近，这也就是为什么w可以取到很小的原因。</p><p>另外<b>L2正则化</b>也可以很好的解决过拟合问题。从上面得知，拟合过程中通常都倾向于让权值尽可能小，最后构造出一个让所有参数都比较小的模型。因为一般认为参数值小的模型比较简单，能够适应于不同的数据集，比如对于目标方程，若参数很大，那么数据只要偏倚一点点，那么对结果的影响就很大。如果参数很小的话，即使数据变化范围比较大，对结果影响也不是很大。相对来说，参数较小的话，对模型的抗扰动能力强。那么为什么L2正则化可以获得很小的参数值呢？我们假设带有L2正则化的损失函数方程如下所示，并对损失函数进行求导。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-cb96550dd05118d2cec3157863010f46_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1250\" data-rawheight=\"352\" class=\"origin_image zh-lightbox-thumb\" width=\"1250\" data-original=\"https://pic3.zhimg.com/v2-cb96550dd05118d2cec3157863010f46_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1250&#39; height=&#39;352&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1250\" data-rawheight=\"352\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1250\" data-original=\"https://pic3.zhimg.com/v2-cb96550dd05118d2cec3157863010f46_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-cb96550dd05118d2cec3157863010f46_b.jpg\"/></figure><p>当利用梯度下降算法进行更新w时，w变化如下所示，其中α是学习速率。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-6b5471211707079fb0340b980f94ddc6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1234\" data-rawheight=\"236\" class=\"origin_image zh-lightbox-thumb\" width=\"1234\" data-original=\"https://pic3.zhimg.com/v2-6b5471211707079fb0340b980f94ddc6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1234&#39; height=&#39;236&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1234\" data-rawheight=\"236\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1234\" data-original=\"https://pic3.zhimg.com/v2-6b5471211707079fb0340b980f94ddc6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-6b5471211707079fb0340b980f94ddc6_b.jpg\"/></figure><p>可以看到在梯度下降算法过程中，w是不断进行减小的，也就是权重衰减，这样也就能得到一个让所有参数都比较小的模型，也就能解决过拟合问题。最后再解释下为什么L2正则化不具有稀疏性的原因，如下图所示，二维平面下L2正则化的函数图形是圆，与L1图形相比，没有了菱角。因此J0与L2接触时，使w1或w2等于0的机率就小了很多，所以L2正则化不具有稀疏性。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-23d3b165e8dba9159049932b4e83c86f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"318\" data-rawheight=\"297\" class=\"content_image\" width=\"318\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;318&#39; height=&#39;297&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"318\" data-rawheight=\"297\" class=\"content_image lazy\" width=\"318\" data-actualsrc=\"https://pic4.zhimg.com/v2-23d3b165e8dba9159049932b4e83c86f_b.jpg\"/></figure><h2>2.DNN之L1和L2正则化</h2><p>和普通机器学习算法一样，DNN也会遇到过拟合的问题，因此需要考虑泛化。结合我们上面讲到的L1和L2正则化，这里对深度神经网络中的正则化做个总结，其中L1正则化和L2正则化原理类似，这里主要介绍L2正则化方法。通过<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3MjA2NTQzMw%3D%3D%26mid%3D2247483903%26idx%3D1%26sn%3D4e3f92578399013eba9f203d35afe972%26chksm%3Dfcd7d209cba05b1ffc66494ea8008c669e40f3045398695b479aba14e1c425f85b7c8f033c4f%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">深度神经网络之前向传播算法</a>的学习，我们知道前向传播过程中损失函数为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-1dc02de81579cfbac675efd81e15c2c1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1248\" data-rawheight=\"164\" class=\"origin_image zh-lightbox-thumb\" width=\"1248\" data-original=\"https://pic2.zhimg.com/v2-1dc02de81579cfbac675efd81e15c2c1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1248&#39; height=&#39;164&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1248\" data-rawheight=\"164\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1248\" data-original=\"https://pic2.zhimg.com/v2-1dc02de81579cfbac675efd81e15c2c1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-1dc02de81579cfbac675efd81e15c2c1_b.jpg\"/></figure><p>加入L2正则化后，损失函数如下所示。其中λ是正则化参数，实际使用时需要我们进行调参。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-1b68b9eb6136e5f5f1201d9246adf628_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1244\" data-rawheight=\"174\" class=\"origin_image zh-lightbox-thumb\" width=\"1244\" data-original=\"https://pic1.zhimg.com/v2-1b68b9eb6136e5f5f1201d9246adf628_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1244&#39; height=&#39;174&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1244\" data-rawheight=\"174\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1244\" data-original=\"https://pic1.zhimg.com/v2-1b68b9eb6136e5f5f1201d9246adf628_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-1b68b9eb6136e5f5f1201d9246adf628_b.jpg\"/></figure><p>如果使用上式的损失函数，进行反向传播算法时，流程和没有正则化时的反向传播算法相同。区别在于进行梯度下降时，W更新公式会进行改变。在<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3MjA2NTQzMw%3D%3D%26mid%3D2247483911%26idx%3D1%26sn%3Dbcc0fe6a4a0c20a422f254b3264a5fb8%26chksm%3Dfcd7d1f1cba058e7bddefb3d47ba6f87879663c15a5cea5c6653138d0fb1aa9d3454e2ea605a%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">深度神经网络之反向传播算法</a>中，W的梯度下降更新公式为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-41501a482fde37de9f671d2573fbec40_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1246\" data-rawheight=\"164\" class=\"origin_image zh-lightbox-thumb\" width=\"1246\" data-original=\"https://pic1.zhimg.com/v2-41501a482fde37de9f671d2573fbec40_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1246&#39; height=&#39;164&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1246\" data-rawheight=\"164\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1246\" data-original=\"https://pic1.zhimg.com/v2-41501a482fde37de9f671d2573fbec40_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-41501a482fde37de9f671d2573fbec40_b.jpg\"/></figure><p>加入L2正则化后，W迭代更新公式如下所示</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-6e828e660d3efc6991dd51e717322d21_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1240\" data-rawheight=\"164\" class=\"origin_image zh-lightbox-thumb\" width=\"1240\" data-original=\"https://pic2.zhimg.com/v2-6e828e660d3efc6991dd51e717322d21_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1240&#39; height=&#39;164&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1240\" data-rawheight=\"164\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1240\" data-original=\"https://pic2.zhimg.com/v2-6e828e660d3efc6991dd51e717322d21_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-6e828e660d3efc6991dd51e717322d21_b.jpg\"/></figure><p>类似的正则化方法，同样可以用于其他损失函数，在这里不再介绍。</p><h2>3.DNN之Dropout正则化</h2><p>Dropout指的是在用前向传播算法和反向传播算法训练模型时，随机的从全连接DNN网络中去掉一部分隐含层的神经元。比如我们完整的DNN模型如下所示</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-9fe15dd7d44c5c55cfe919edfdbacf63_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"310\" data-rawheight=\"324\" class=\"content_image\" width=\"310\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;310&#39; height=&#39;324&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"310\" data-rawheight=\"324\" class=\"content_image lazy\" width=\"310\" data-actualsrc=\"https://pic4.zhimg.com/v2-9fe15dd7d44c5c55cfe919edfdbacf63_b.jpg\"/></figure><p>然后随机的去掉部分隐含层的神经元，利用数据进行训练模型，更新所有的W,b。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b4b6cd7f30d3ddb1814039205b9c3d15_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"310\" data-rawheight=\"324\" class=\"content_image\" width=\"310\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;310&#39; height=&#39;324&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"310\" data-rawheight=\"324\" class=\"content_image lazy\" width=\"310\" data-actualsrc=\"https://pic2.zhimg.com/v2-b4b6cd7f30d3ddb1814039205b9c3d15_b.jpg\"/></figure><p>总结下Dropout方法就是，每轮梯度下降迭代时，将训练数据分成若干批，然后分批进行迭代。每批数据迭代时，将原始的DNN模型随机去掉部分隐含层的神经元，然后用残缺的DNN模型来迭代更新W,b。每批数据迭代完成之后，将残缺的DNN模型恢复成原始的DNN模型，接着去训练模型，更新W,b。当然，运用Dropout正则化方法，需要有较大数据量支持，否则可能会出现欠拟合的情况。</p><h2>4.DNN之集成学习正则化</h2><p>在<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3MjA2NTQzMw%3D%3D%26mid%3D2247483845%26idx%3D1%26sn%3D5484385408d694ba03a8bdc3a03c2263%26chksm%3Dfcd7d233cba05b25b0f65289f8416466df9124b019b60704b4e3875d9864ff90e3388c666f66%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">机器学习之随机森林</a>和<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3MjA2NTQzMw%3D%3D%26mid%3D2247483849%26idx%3D1%26sn%3D3cd3d40d26e600901cf3d72c43f7c696%26chksm%3Dfcd7d23fcba05b2923651f1b0808861b4e559de1b004f37a2adcec7fae9002dd9f11d99c7791%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">机器学习之梯度提升决策树</a>之中，我们已经学习集成学习中的Bagging和Boosting方法，而DNN可以用Bagging方法来正则化。随机森林中，Bagging方法通过随机采样构建若干个相互独立的弱决策树学习器，最后通过采用加权平均法或者投票法决定集成的输出。</p><p>DNN中我们采用的是若干个DNN的网络，首先对原始的训练样本进行有放回的随机采样，构建N组m个样本的数据集，然后分别用这N组数据集去训练我们的DNN。通过利用前向传播算法和反向传播算法得到N个DNN模型的W,b参数组合，然后对N个DNN模型的输出用加权平均法或者投票法决定最后的输出。最后，因为DNN模型比较复杂，通过Bagging后模型参数会增加N倍，从而导致训练模型需要花费较长的时间，因此一般N的取值不能太大，5-10个即可。</p><h2>5.DNN之增强数据集正则化</h2><p>增强模型泛化能力最好的方法，是有更多更好的训练数据，但实际情况之中，对于某些数据，我们很难能够得到。那么，我们不如去构造一些数据，来让模型得到更强的泛化能力。对于传统的机器学习算法，比如上面提到的<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3MjA2NTQzMw%3D%3D%26mid%3D2247483845%26idx%3D1%26sn%3D5484385408d694ba03a8bdc3a03c2263%26chksm%3Dfcd7d233cba05b25b0f65289f8416466df9124b019b60704b4e3875d9864ff90e3388c666f66%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">机器学习之随机森林</a>和<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3MjA2NTQzMw%3D%3D%26mid%3D2247483849%26idx%3D1%26sn%3D3cd3d40d26e600901cf3d72c43f7c696%26chksm%3Dfcd7d23fcba05b2923651f1b0808861b4e559de1b004f37a2adcec7fae9002dd9f11d99c7791%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">机器学习之梯度提升决策树</a>算法，想要构造数据的话，能够很方便的构造输入数据，但是很难构造出对应的输出数据。</p><p>但对于深度神经网络来说，比如图像识别领域，对于原始数据集的图像，我们可以偏倚或者旋转图像之后，得到新的数据集。显然原始数据和新构造的数据输入是不同的图像，但输出是相同的，因此通过训练后，模型的泛化便能够增强。对应的例子，比如利用DNN识别手写数字，数字5旋转15度之后，识别之后还是5。</p><p>参考</p><blockquote><a href=\"https://link.zhihu.com/?target=https%3A//www.cnblogs.com/pinard/p/6472666.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">深度神经网络(DNN)的正则化-刘建平Pinard</a><br/><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/jinping_shi/article/details/52433975\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">机器学习中正则化项L1和L2的直观理解-阿拉丁吃米粉</a><br/><a href=\"https://www.zhihu.com/question/20924039\" class=\"internal\">机器学习中常常提到的正则化到底是什么意思？-陶轻松</a></blockquote><h2>6.推广</h2><p>更多内容请关注公众号<b>谓之小一</b>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p><p><a href=\"https://link.zhihu.com/?target=http%3A//weixin.qq.com/r/Ty_4oDPE2W6mrXfD93pd\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">weixin.qq.com/r/Ty_4oDP</span><span class=\"invisible\">E2W6mrXfD93pd</span><span class=\"ellipsis\"></span></a> (二维码自动识别)</p><p></p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "神经网络", 
                    "tagLink": "https://api.zhihu.com/topics/19607065"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": [
                {
                    "userName": "吾恒", 
                    "userLink": "https://www.zhihu.com/people/72fa20f743d663bd2b8a89f8b7bf4021", 
                    "content": "看了好几篇正则化，就你讲得最通俗易懂[赞][赞]", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/38733907", 
            "userName": "谓之小一", 
            "userLink": "https://www.zhihu.com/people/761dda159c5cec5fd926f761ab5f4ccf", 
            "upvote": 3, 
            "title": "深度神经网络之损失函数和激活函数", 
            "content": "<blockquote>由于较多公式，所以我将部分内容转为图片进行上传，请见谅。清晰版请访问<a href=\"https://link.zhihu.com/?target=https%3A//weizhixiaoyi.com\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">weizhixiaoyi.com</span><span class=\"invisible\"></span></a>查看，你也可以关注我公众号<b>谓之小一</b>，后台直接向我要pdf版本，如有相关问题可公众号后台询问，随时回答。</blockquote><h2>1.损失函数和激活函数简介</h2><p>通过前面<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3MjA2NTQzMw%3D%3D%26mid%3D2247483903%26idx%3D1%26sn%3D4e3f92578399013eba9f203d35afe972%26chksm%3Dfcd7d209cba05b1ffc66494ea8008c669e40f3045398695b479aba14e1c425f85b7c8f033c4f%26scene%3D38%23wechat_redirect\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">深度神经网络之前向传播算法</a>和<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3MjA2NTQzMw%3D%3D%26mid%3D2247483911%26idx%3D1%26sn%3Dbcc0fe6a4a0c20a422f254b3264a5fb8%26chksm%3Dfcd7d1f1cba058e7bddefb3d47ba6f87879663c15a5cea5c6653138d0fb1aa9d3454e2ea605a%26scene%3D38%23wechat_redirect\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">深度神经网络之反向传播算法</a>的学习，我们能够了解到损失函数是用来评估模型的预测值与真实值之间的差异程度。另外损失函数也是神经网络中优化的目标函数，神经网络训练或者优化的过程就是最小化损失函数的过程，损失函数越小，说明模型的预测值就越接近真实值，模型的准确性也就越好。前面我们已经学习过<b>平方损失函数</b>，<b>对数损失函数</b>、<b>交叉熵损失函数</b>等不同形式的损失函数，这里也就不做太多介绍。</p><p>那么在深度神经网络之中，激活函数的作用又是什么呢？首先我们来看单层感知机模型，如下图所示，感知机可以利用分割线将平面分割开来。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-714096c5cf3e6f6ee7bd0c5a6611b06d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"720\" data-rawheight=\"375\" class=\"origin_image zh-lightbox-thumb\" width=\"720\" data-original=\"https://pic2.zhimg.com/v2-714096c5cf3e6f6ee7bd0c5a6611b06d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;720&#39; height=&#39;375&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"720\" data-rawheight=\"375\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"720\" data-original=\"https://pic2.zhimg.com/v2-714096c5cf3e6f6ee7bd0c5a6611b06d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-714096c5cf3e6f6ee7bd0c5a6611b06d_b.jpg\"/></figure><p>现在我们利用多个感知机进行组合，获得更强的分类能力，模型分类效果如下图所示。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ead0c09370f9b052ace7e8d69aa6bb9b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"720\" data-rawheight=\"376\" class=\"origin_image zh-lightbox-thumb\" width=\"720\" data-original=\"https://pic4.zhimg.com/v2-ead0c09370f9b052ace7e8d69aa6bb9b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;720&#39; height=&#39;376&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"720\" data-rawheight=\"376\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"720\" data-original=\"https://pic4.zhimg.com/v2-ead0c09370f9b052ace7e8d69aa6bb9b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-ead0c09370f9b052ace7e8d69aa6bb9b_b.jpg\"/></figure><p>但无论怎样组合，模型输出的时候都只是线性模型，如何解决非线性分类呢？好吧，上面是我们没有增加激活函数的情况。那么现在我们在每一层迭代完之后，增加一个激活函数，如下图的y=σ(a)所示，这样模型的输出便能解决非线性情况。将多个有激活函数的神经元组合起来，我们就可以得到一个相当复杂的函数。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-44adaf6e40c56c0945e2f1685b059cba_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"720\" data-rawheight=\"395\" class=\"origin_image zh-lightbox-thumb\" width=\"720\" data-original=\"https://pic3.zhimg.com/v2-44adaf6e40c56c0945e2f1685b059cba_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;720&#39; height=&#39;395&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"720\" data-rawheight=\"395\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"720\" data-original=\"https://pic3.zhimg.com/v2-44adaf6e40c56c0945e2f1685b059cba_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-44adaf6e40c56c0945e2f1685b059cba_b.jpg\"/></figure><p>引入非线性激活函数之中，模型的表达能力增强，能够有效解决非线性情况。通过不同形式的激活函数，模型也就能够学习到不同形式的分类方式，比如<b>平滑分类平面</b>，方面我们解决各种问题。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-52925aa3f6f234d5a87104715b621432_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"720\" data-rawheight=\"353\" class=\"origin_image zh-lightbox-thumb\" width=\"720\" data-original=\"https://pic3.zhimg.com/v2-52925aa3f6f234d5a87104715b621432_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;720&#39; height=&#39;353&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"720\" data-rawheight=\"353\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"720\" data-original=\"https://pic3.zhimg.com/v2-52925aa3f6f234d5a87104715b621432_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-52925aa3f6f234d5a87104715b621432_b.jpg\"/></figure><p>通过上面的介绍，我们能够了解到神经网络之中损失函数和激活函数的作用，但实际上DNN可以使用的损失函数和激活函数有不少，这时我们应该如何去做选择呢？下面我们介绍一些DNN之中常见的损失函数和激活函数。</p><h2>2.交叉熵损失函数和Sigmoid激活函数</h2><p>在<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3MjA2NTQzMw%3D%3D%26mid%3D2247483911%26idx%3D1%26sn%3Dbcc0fe6a4a0c20a422f254b3264a5fb8%26chksm%3Dfcd7d1f1cba058e7bddefb3d47ba6f87879663c15a5cea5c6653138d0fb1aa9d3454e2ea605a%26scene%3D38%23wechat_redirect\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">深度神经网络之反向传播算法</a>之中，我们用的是均方差损失函数和Sigmoid激活函数，首先我们看看<b>均方差损失函数和Sigmoid激活函数</b>有什么问题。如下所示，是我们已经非常熟悉的Sigmoid激活函数表达式</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-d7581b3c23a221b105d868c1ef5346d5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1230\" data-rawheight=\"150\" class=\"origin_image zh-lightbox-thumb\" width=\"1230\" data-original=\"https://pic2.zhimg.com/v2-d7581b3c23a221b105d868c1ef5346d5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1230&#39; height=&#39;150&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1230\" data-rawheight=\"150\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1230\" data-original=\"https://pic2.zhimg.com/v2-d7581b3c23a221b105d868c1ef5346d5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-d7581b3c23a221b105d868c1ef5346d5_b.jpg\"/></figure><p>其中σ(z)的图像如下图所示，从图中可以看出，当z越来越大时，函数曲线也就变得越平缓，意味着此时导数σ′(z)也越小。同样，当z越来越小时，也会出现σ′(z)也越小。仅仅当z取值为0的附近时，导数σ′(z)取值较大。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-27940615247138c0f50336a0ca451ea4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"471\" data-rawheight=\"308\" class=\"origin_image zh-lightbox-thumb\" width=\"471\" data-original=\"https://pic1.zhimg.com/v2-27940615247138c0f50336a0ca451ea4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;471&#39; height=&#39;308&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"471\" data-rawheight=\"308\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"471\" data-original=\"https://pic1.zhimg.com/v2-27940615247138c0f50336a0ca451ea4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-27940615247138c0f50336a0ca451ea4_b.jpg\"/></figure><p>在<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3MjA2NTQzMw%3D%3D%26mid%3D2247483911%26idx%3D1%26sn%3Dbcc0fe6a4a0c20a422f254b3264a5fb8%26chksm%3Dfcd7d1f1cba058e7bddefb3d47ba6f87879663c15a5cea5c6653138d0fb1aa9d3454e2ea605a%26scene%3D38%23wechat_redirect\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">深度神经网络之反向传播算法</a>之中，我们了解到每次反向迭代递推时，都要乘以σ′(z)得到梯度变化值。而Sigmoid的曲线意味着在大多数时候，DNN的梯度变化值较小，则会导致W,b更新到极值的速度很慢。那么有什么办法可以改变这种情况呢？</p><p>常见的方法是选用交叉熵损失函数来代替均方差损失函数，首先来看看交叉熵损失函数的形式。其中 <b>∙</b> 为向量内积，我们在<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3MjA2NTQzMw%3D%3D%26mid%3D2247483814%26idx%3D1%26sn%3D16a56382d24e304a95ab2a2a028993c6%26chksm%3Dfcd7d250cba05b46e16e5db30a85965878d051a17517b90c27f3206d23c6f3784c4e363f06eb%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">机器学习之Logistic回归</a>中便用到类似的交叉熵损失函数形式。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-30554079359eb0811a44fe5e6fe33d92_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1240\" data-rawheight=\"122\" class=\"origin_image zh-lightbox-thumb\" width=\"1240\" data-original=\"https://pic3.zhimg.com/v2-30554079359eb0811a44fe5e6fe33d92_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1240&#39; height=&#39;122&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1240\" data-rawheight=\"122\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1240\" data-original=\"https://pic3.zhimg.com/v2-30554079359eb0811a44fe5e6fe33d92_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-30554079359eb0811a44fe5e6fe33d92_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-4c80aed29fc4a901d7ce212f7a949b84_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1266\" data-rawheight=\"526\" class=\"origin_image zh-lightbox-thumb\" width=\"1266\" data-original=\"https://pic1.zhimg.com/v2-4c80aed29fc4a901d7ce212f7a949b84_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1266&#39; height=&#39;526&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1266\" data-rawheight=\"526\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1266\" data-original=\"https://pic1.zhimg.com/v2-4c80aed29fc4a901d7ce212f7a949b84_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-4c80aed29fc4a901d7ce212f7a949b84_b.jpg\"/></figure><h2>3.对数似然损失函数和softmax激活函数</h2><p>前面我们假设模型的输出都是连续可导的值，但如果是分类问题，输出的是不同类别，那么怎么用DNN解决呢？比如我们有三个类别的分类问题，这样DNN输出层对应的便是三个神经元，每个神经元分别代表类别1、类别2、类别3，这样我们的期望输出应该是(1,0,0)、(0,1,0)、(0,0,1)，即样本真实类别对应的神经元输出应该无限接近或等于1。或者说，我们希望输出层的神经元对应的输出是若干个概率值，这若干个概率值即DNN模型对于输入值进行各类别的输出预测，同时这若干个概率之和为1。</p><p>很明显，现在普通DNN无法满足目前要求，我们需要作出相应改变，来让DNN分类模型输出层的输出值在0到1之间，同时所有输出值之和为1。为此，我们定义输出层第i个神经元的激活函数如下所示</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-a9a092d7537ce6291797810ca32a2152_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1288\" data-rawheight=\"304\" class=\"origin_image zh-lightbox-thumb\" width=\"1288\" data-original=\"https://pic3.zhimg.com/v2-a9a092d7537ce6291797810ca32a2152_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1288&#39; height=&#39;304&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1288\" data-rawheight=\"304\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1288\" data-original=\"https://pic3.zhimg.com/v2-a9a092d7537ce6291797810ca32a2152_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-a9a092d7537ce6291797810ca32a2152_b.jpg\"/></figure><p>下面我们通过例子来描述softmax激活函数在前向传播算法中的应用，假设输出层为三个神经元，未激活的输出为(3,1,-3)，求出各自的指数表达式为(20,2.7,0.05)，归一化后为22.75，求出三个类别的概率为(0.88,0.12,0)。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-b9660d6bdd085910b46eb689432f00d3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"600\" data-rawheight=\"336\" class=\"origin_image zh-lightbox-thumb\" width=\"600\" data-original=\"https://pic4.zhimg.com/v2-b9660d6bdd085910b46eb689432f00d3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;600&#39; height=&#39;336&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"600\" data-rawheight=\"336\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"600\" data-original=\"https://pic4.zhimg.com/v2-b9660d6bdd085910b46eb689432f00d3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-b9660d6bdd085910b46eb689432f00d3_b.jpg\"/></figure><p>对于用作分类的softmax激活函数，对应的损失函数一般都是用对数似然函数，函数表达式如下所示。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-b7ac7cf206f4c8d9088bf67ca21fe816_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1262\" data-rawheight=\"398\" class=\"origin_image zh-lightbox-thumb\" width=\"1262\" data-original=\"https://pic3.zhimg.com/v2-b7ac7cf206f4c8d9088bf67ca21fe816_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1262&#39; height=&#39;398&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1262\" data-rawheight=\"398\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1262\" data-original=\"https://pic3.zhimg.com/v2-b7ac7cf206f4c8d9088bf67ca21fe816_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-b7ac7cf206f4c8d9088bf67ca21fe816_b.jpg\"/></figure><p>可见损失函数只是和真实类别对应的输出有关，这样假设真实类别是第i类，则其他不属于第i类序号对应的神经元梯度导数为0。对于真实类别第i类，所对应的梯度计算为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-fabc1b786a70d444a61670d4c97bcfe2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1264\" data-rawheight=\"536\" class=\"origin_image zh-lightbox-thumb\" width=\"1264\" data-original=\"https://pic3.zhimg.com/v2-fabc1b786a70d444a61670d4c97bcfe2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1264&#39; height=&#39;536&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1264\" data-rawheight=\"536\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1264\" data-original=\"https://pic3.zhimg.com/v2-fabc1b786a70d444a61670d4c97bcfe2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-fabc1b786a70d444a61670d4c97bcfe2_b.jpg\"/></figure><p>可见，梯度计算相对较简单，也不会出现前面训练速度慢的问题。同样对于上面的例子，经过softmax函数激活后的概率输出为(0.88,0.12,0)，对第二类训练样本反向传播时，反向传播梯度的偏倚向量为(0.88,0.12-1,0)。</p><h2>4.DNN其他激活函数</h2><h2>4.1 ReLU激活函数</h2><p>ReLU(Rectified Linear Unit)表达式如下所示，也就是说，大于等于0则激活后不变，小于0则激活后为0。这个函数有什么意义呢？ReLU激活函数在梯度爆炸和梯度消失方面有重要应用。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-d4eba26b6a816e23edeb3897e5a9652c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1234\" data-rawheight=\"124\" class=\"origin_image zh-lightbox-thumb\" width=\"1234\" data-original=\"https://pic1.zhimg.com/v2-d4eba26b6a816e23edeb3897e5a9652c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1234&#39; height=&#39;124&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1234\" data-rawheight=\"124\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1234\" data-original=\"https://pic1.zhimg.com/v2-d4eba26b6a816e23edeb3897e5a9652c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-d4eba26b6a816e23edeb3897e5a9652c_b.jpg\"/></figure><p>那什么是梯度爆炸和梯度消失呢？可以简单理解为，反向传播算法过程中，由于我们使用的是矩阵求导的链式法则，会有一系列连乘运算。如果连乘的数字在每层都是大于1的，则梯度越往前乘越大，最后导致梯度爆炸。同理，如果连乘的数字在每层都是小于1的，则梯度越往前乘越小，最后导致梯度消失。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-bf245756311c3a0e645759f5c397feaf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1268\" data-rawheight=\"426\" class=\"origin_image zh-lightbox-thumb\" width=\"1268\" data-original=\"https://pic4.zhimg.com/v2-bf245756311c3a0e645759f5c397feaf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1268&#39; height=&#39;426&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1268\" data-rawheight=\"426\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1268\" data-original=\"https://pic4.zhimg.com/v2-bf245756311c3a0e645759f5c397feaf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-bf245756311c3a0e645759f5c397feaf_b.jpg\"/></figure><h2>4.2 Tanh激活函数</h2><p>Tanh激活函数是Sigmoid函数的变种，Tanh表达式如下所示。Tanh和Sigmoid函数的不同点是Tanh函数的输出值落在[-1,1]之间，因此Tanh输出可以进行标准化。同时Tanh自变量变化较大时，曲线变得平坦的幅度没有Sigmoid那么大，这样求梯度变化值有一些优势。当然，是使用Tanh函数还是使用Sigmoid函数需要根据具体问题而定。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-a2f80777d80ba722b3cdb9c2732c6071_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1242\" data-rawheight=\"244\" class=\"origin_image zh-lightbox-thumb\" width=\"1242\" data-original=\"https://pic2.zhimg.com/v2-a2f80777d80ba722b3cdb9c2732c6071_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1242&#39; height=&#39;244&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1242\" data-rawheight=\"244\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1242\" data-original=\"https://pic2.zhimg.com/v2-a2f80777d80ba722b3cdb9c2732c6071_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-a2f80777d80ba722b3cdb9c2732c6071_b.jpg\"/></figure><h2>4.3 Softplus激活函数</h2><p>Softplus激活函数是Sigmoid函数的原函数，表达式如下所示，Softplus函数和ReLU函数图像类似。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-98b8c6707c7952a8059944801a8fda06_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1222\" data-rawheight=\"124\" class=\"origin_image zh-lightbox-thumb\" width=\"1222\" data-original=\"https://pic3.zhimg.com/v2-98b8c6707c7952a8059944801a8fda06_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1222&#39; height=&#39;124&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1222\" data-rawheight=\"124\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1222\" data-original=\"https://pic3.zhimg.com/v2-98b8c6707c7952a8059944801a8fda06_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-98b8c6707c7952a8059944801a8fda06_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-4d9834b37a3705da3332ff6af6f6eaaf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"536\" data-rawheight=\"410\" class=\"origin_image zh-lightbox-thumb\" width=\"536\" data-original=\"https://pic4.zhimg.com/v2-4d9834b37a3705da3332ff6af6f6eaaf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;536&#39; height=&#39;410&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"536\" data-rawheight=\"410\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"536\" data-original=\"https://pic4.zhimg.com/v2-4d9834b37a3705da3332ff6af6f6eaaf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-4d9834b37a3705da3332ff6af6f6eaaf_b.jpg\"/></figure><h2>4.4 PReLU激活函数</h2><p>PReLU激活函数是ReLU的变种，特点是如果激活值小于0，激活值不是简单的变为0，而是逐渐的变化。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-b59b1ad8f400c5ba40c8792af23b1d2b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"605\" data-rawheight=\"381\" class=\"origin_image zh-lightbox-thumb\" width=\"605\" data-original=\"https://pic4.zhimg.com/v2-b59b1ad8f400c5ba40c8792af23b1d2b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;605&#39; height=&#39;381&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"605\" data-rawheight=\"381\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"605\" data-original=\"https://pic4.zhimg.com/v2-b59b1ad8f400c5ba40c8792af23b1d2b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-b59b1ad8f400c5ba40c8792af23b1d2b_b.jpg\"/></figure><p>和普通的机器学习算法一样，DNN也会遇到过拟合的问题，需要考虑泛化，下篇文章我们将进行讲解深度神经网络之中的正则化问题。</p><p>参考</p><blockquote><a href=\"https://www.zhihu.com/question/22334626/answer/21036590\" class=\"internal\">知乎-神经网络激励函数的作用是什么？有没有形象的解释?-颜沁睿</a><br/><a href=\"https://link.zhihu.com/?target=https%3A//www.cnblogs.com/pinard/p/6437495.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">刘建平Pinard-深度神经网络(DNN)损失函数和激活函数的选择(BP)</a></blockquote><h2>5.推广</h2><p>更多内容请关注公众号<b>谓之小一</b>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p><p><a href=\"https://link.zhihu.com/?target=http%3A//weixin.qq.com/r/Ty_4oDPE2W6mrXfD93pd\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">weixin.qq.com/r/Ty_4oDP</span><span class=\"invisible\">E2W6mrXfD93pd</span><span class=\"ellipsis\"></span></a> (二维码自动识别)</p><p></p>", 
            "topic": [
                {
                    "tag": "神经网络", 
                    "tagLink": "https://api.zhihu.com/topics/19607065"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": [
                {
                    "userName": "南华是条街", 
                    "userLink": "https://www.zhihu.com/people/52fe08f57e4b4709d3699f9d9fe7eef7", 
                    "content": "谢谢谢谢", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/38612478", 
            "userName": "谓之小一", 
            "userLink": "https://www.zhihu.com/people/761dda159c5cec5fd926f761ab5f4ccf", 
            "upvote": 0, 
            "title": "深度神经网络之反向传播算法", 
            "content": "<blockquote>由于较多公式，所以我将部分内容转为图片进行上传，请见谅。清晰版请访问<a href=\"https://link.zhihu.com/?target=https%3A//weizhixiaoyi.com\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">weizhixiaoyi.com</span><span class=\"invisible\"></span></a>查看，你也可以关注我公众号<b>谓之小一</b>，后台直接向我要pdf版本，如有相关问题可公众号后台询问，随时回答。</blockquote><h2>1.DNN反向传播算法简介</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-684d3eab41be9066a8b0b404e772f95f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1268\" data-rawheight=\"252\" class=\"origin_image zh-lightbox-thumb\" width=\"1268\" data-original=\"https://pic4.zhimg.com/v2-684d3eab41be9066a8b0b404e772f95f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1268&#39; height=&#39;252&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1268\" data-rawheight=\"252\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1268\" data-original=\"https://pic4.zhimg.com/v2-684d3eab41be9066a8b0b404e772f95f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-684d3eab41be9066a8b0b404e772f95f_b.jpg\"/></figure><p>现在对应到我们的DNN模型之中，即输入层有n_in个神经元，输出层有n_out个神经元，再加上一些含有若干个神经元的隐含层。此时我们需要找到所有隐含层和输出层所对应的线性系数矩阵W、偏倚向量b，希望通过DNN对所有的训练样本计算后，计算结果能够等于或很接近样本输出，当有新的测试样本数据时，能够有效预测样本输出。但怎样找到合适的线形系数矩阵W和偏倚变量b呢?</p><p>回顾我们前面学习的<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3MjA2NTQzMw%3D%3D%26mid%3D2247483814%26idx%3D1%26sn%3D16a56382d24e304a95ab2a2a028993c6%26chksm%3Dfcd7d250cba05b46e16e5db30a85965878d051a17517b90c27f3206d23c6f3784c4e363f06eb%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">机器学习之Logistic回归</a>、<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3MjA2NTQzMw%3D%3D%26mid%3D2247483818%26idx%3D1%26sn%3D50c634d8b00877134558125c4a718fd7%26chksm%3Dfcd7d25ccba05b4a62adfb2717650441f30d636056fcb37529ef34a51b94e453a534b7ca0a48%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">机器学习之SVM支持向量机</a>等机器学习算法，很容易联想到，我们可以用一个合适的损失函数来度量训练样本的输出损失。然后对损失函数优化，求损失函数最小化的极值，此时对应的线性系数矩阵W，偏倚变量b便是我们希望得到的结果。深度神经网络中，损失函数优化极值求解的过程，通常是利用梯度下降法迭代完成的。当然也可以利用其他的迭代方法，比如牛顿法或拟牛顿法。梯度下降算法以前在<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3MjA2NTQzMw%3D%3D%26mid%3D2247483804%26idx%3D1%26sn%3Dbfc7d6f51e4db6cf0028b33b60690ff6%26chksm%3Dfcd7d26acba05b7cdea7c083736b7348d1e3d3f3b5d21a34c12f35b4e57f1f7650499b73d911%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">机器学习之线形回归</a>中有过详细介绍，有兴趣可以回顾一下。</p><p>对DNN损失函数用梯度下降法进行迭代优化求极小值的过程，便是我们的<b>反向传播算法(Back Propagation,BP)</b>。</p><h2>2.DNN反向传播算法数学推导</h2><p>进行DNN反向传播算法之前，我们需要选择一个损失函数，来度量计算样本的输出和真实样本之间的损失。但训练时的计算样本输出怎么得到呢？</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-71448ec7e868b0f9bf199d58cb90ba3d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1268\" data-rawheight=\"236\" class=\"origin_image zh-lightbox-thumb\" width=\"1268\" data-original=\"https://pic2.zhimg.com/v2-71448ec7e868b0f9bf199d58cb90ba3d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1268&#39; height=&#39;236&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1268\" data-rawheight=\"236\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1268\" data-original=\"https://pic2.zhimg.com/v2-71448ec7e868b0f9bf199d58cb90ba3d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-71448ec7e868b0f9bf199d58cb90ba3d_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-9efb67d321149226fe2c5226c7fb0762_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1262\" data-rawheight=\"244\" class=\"origin_image zh-lightbox-thumb\" width=\"1262\" data-original=\"https://pic3.zhimg.com/v2-9efb67d321149226fe2c5226c7fb0762_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1262&#39; height=&#39;244&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1262\" data-rawheight=\"244\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1262\" data-original=\"https://pic3.zhimg.com/v2-9efb67d321149226fe2c5226c7fb0762_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-9efb67d321149226fe2c5226c7fb0762_b.jpg\"/></figure><p>通过损失函数，我们能够用梯度下降法来迭代求解每一层的W，b。首先计算的是输出层，其中输出层的W，b满足下式</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-de9f52c384c8ba68f509ac7755f036b0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1252\" data-rawheight=\"228\" class=\"origin_image zh-lightbox-thumb\" width=\"1252\" data-original=\"https://pic1.zhimg.com/v2-de9f52c384c8ba68f509ac7755f036b0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1252&#39; height=&#39;228&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1252\" data-rawheight=\"228\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1252\" data-original=\"https://pic1.zhimg.com/v2-de9f52c384c8ba68f509ac7755f036b0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-de9f52c384c8ba68f509ac7755f036b0_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-a8218edbb151b4f3504bd7b622913761_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1262\" data-rawheight=\"476\" class=\"origin_image zh-lightbox-thumb\" width=\"1262\" data-original=\"https://pic2.zhimg.com/v2-a8218edbb151b4f3504bd7b622913761_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1262&#39; height=&#39;476&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1262\" data-rawheight=\"476\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1262\" data-original=\"https://pic2.zhimg.com/v2-a8218edbb151b4f3504bd7b622913761_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-a8218edbb151b4f3504bd7b622913761_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-724d355c6f8398004fe92653cf717be4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1246\" data-rawheight=\"192\" class=\"origin_image zh-lightbox-thumb\" width=\"1246\" data-original=\"https://pic1.zhimg.com/v2-724d355c6f8398004fe92653cf717be4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1246&#39; height=&#39;192&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1246\" data-rawheight=\"192\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1246\" data-original=\"https://pic1.zhimg.com/v2-724d355c6f8398004fe92653cf717be4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-724d355c6f8398004fe92653cf717be4_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-097a2a4eb6ebd00e069a1fc77de0e649_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1258\" data-rawheight=\"262\" class=\"origin_image zh-lightbox-thumb\" width=\"1258\" data-original=\"https://pic2.zhimg.com/v2-097a2a4eb6ebd00e069a1fc77de0e649_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1258&#39; height=&#39;262&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1258\" data-rawheight=\"262\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1258\" data-original=\"https://pic2.zhimg.com/v2-097a2a4eb6ebd00e069a1fc77de0e649_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-097a2a4eb6ebd00e069a1fc77de0e649_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-a5ce9b408bbc264de37214bebf5e3f5e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1260\" data-rawheight=\"540\" class=\"origin_image zh-lightbox-thumb\" width=\"1260\" data-original=\"https://pic3.zhimg.com/v2-a5ce9b408bbc264de37214bebf5e3f5e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1260&#39; height=&#39;540&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1260\" data-rawheight=\"540\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1260\" data-original=\"https://pic3.zhimg.com/v2-a5ce9b408bbc264de37214bebf5e3f5e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-a5ce9b408bbc264de37214bebf5e3f5e_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3bc9d6314aefcaf0e814fd6a912c2cb5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1266\" data-rawheight=\"528\" class=\"origin_image zh-lightbox-thumb\" width=\"1266\" data-original=\"https://pic2.zhimg.com/v2-3bc9d6314aefcaf0e814fd6a912c2cb5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1266&#39; height=&#39;528&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1266\" data-rawheight=\"528\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1266\" data-original=\"https://pic2.zhimg.com/v2-3bc9d6314aefcaf0e814fd6a912c2cb5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-3bc9d6314aefcaf0e814fd6a912c2cb5_b.jpg\"/></figure><h2>3.DNN反向传播算法过程</h2><p>梯度下降算法有批量(Batch)，小批量(Mini-Batch)，随机三种方式，采用哪种方式取决于我们的问题而定。为简化描述，这里采用最基本的批量梯度下降法来描述反向传播算法。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-15604e1028d3a5f4f94dae4667e3458b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1256\" data-rawheight=\"932\" class=\"origin_image zh-lightbox-thumb\" width=\"1256\" data-original=\"https://pic4.zhimg.com/v2-15604e1028d3a5f4f94dae4667e3458b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1256&#39; height=&#39;932&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1256\" data-rawheight=\"932\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1256\" data-original=\"https://pic4.zhimg.com/v2-15604e1028d3a5f4f94dae4667e3458b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-15604e1028d3a5f4f94dae4667e3458b_b.jpg\"/></figure><p>通过深度神经网络之中的前向传播算法和反向传播算法的结合，我们能够利用DNN模型去解决各种分类或回归问题，但对于不同问题，效果如何呢？是否会过拟合呢？我们将在下次文章中详细介绍损失函数和激活函数的选择、正则化方面的知识点，来让深度神经网络能更精确的解决我们的问题。</p><p>参考</p><blockquote><a href=\"https://link.zhihu.com/?target=https%3A//www.cnblogs.com/pinard/p/6422831.html%23%21comments\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">刘建平Pinard_深度神经网络(DNN)反向传播算法(BP)</a></blockquote><h2>4.推广</h2><p>更多内容请关注公众号<b>谓之小一</b>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p><p><a href=\"https://link.zhihu.com/?target=http%3A//weixin.qq.com/r/Ty_4oDPE2W6mrXfD93pd\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">weixin.qq.com/r/Ty_4oDP</span><span class=\"invisible\">E2W6mrXfD93pd</span><span class=\"ellipsis\"></span></a> (二维码自动识别)</p><p></p>", 
            "topic": [
                {
                    "tag": "神经网络", 
                    "tagLink": "https://api.zhihu.com/topics/19607065"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/38566805", 
            "userName": "谓之小一", 
            "userLink": "https://www.zhihu.com/people/761dda159c5cec5fd926f761ab5f4ccf", 
            "upvote": 0, 
            "title": "深度神经网络之前向传播算法", 
            "content": "<blockquote>由于较多公式，所以我将部分内容转为图片进行上传，请见谅。清晰版请访问<a href=\"https://link.zhihu.com/?target=https%3A//weizhixiaoyi.com\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">weizhixiaoyi.com</span><span class=\"invisible\"></span></a>查看，你也可以关注我公众号<b>谓之小一</b>，后台直接向我要pdf版本，如有相关问题可公众号后台询问，随时回答。</blockquote><h2>1.深度神经网络简介</h2><p><b>深度神经网络(Deep Neural Networks,DNN)</b>从字面上理解，也就是深层次的神经网络，从网络结构上看来就是有多个隐含层的神经网络。深度神经网络不仅能够用于分类和回归，在降维、聚类、语音识别、图像识别方面也有许多应用。由于神经网络内容较多，将分多次写作，本次主要讲解深度神经网络中的前向传播算法，后续还有反向传播算法、损失函数和激活函数、正则化。</p><h2>2.从感知机到神经网络</h2><p>在<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3MjA2NTQzMw%3D%3D%26mid%3D2247483814%26idx%3D1%26sn%3D16a56382d24e304a95ab2a2a028993c6%26chksm%3Dfcd7d250cba05b46e16e5db30a85965878d051a17517b90c27f3206d23c6f3784c4e363f06eb%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">机器学习之Logistic回归</a>之中，我们利用过感知机的模型。如下图所示，也就是有若干个输入和一个输出的感知机模型。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-404c4617306a4b7bbfaced76008147fe_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"280\" data-rawheight=\"138\" class=\"content_image\" width=\"280\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;280&#39; height=&#39;138&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"280\" data-rawheight=\"138\" class=\"content_image lazy\" width=\"280\" data-actualsrc=\"https://pic3.zhimg.com/v2-404c4617306a4b7bbfaced76008147fe_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>感知机通过输入和输出学习得到一个线性模型，得到中间输出结果z。然后利用激活函数，从而得到我们希望的结果，例如1或-1。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-a5ab00650620b7760dfe042aa14e5467_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1618\" data-rawheight=\"334\" class=\"origin_image zh-lightbox-thumb\" width=\"1618\" data-original=\"https://pic4.zhimg.com/v2-a5ab00650620b7760dfe042aa14e5467_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1618&#39; height=&#39;334&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1618\" data-rawheight=\"334\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1618\" data-original=\"https://pic4.zhimg.com/v2-a5ab00650620b7760dfe042aa14e5467_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-a5ab00650620b7760dfe042aa14e5467_b.jpg\"/></figure><p>上述模型只能用于二元分类，且无法学习比较复杂的非线形模型。而神经网络则是在感知机的模型上做扩展，主要增加以下三点。</p><ul><li><b>增加隐含层：</b>如下图所示，隐含层可以有多层，增加模型的表达能力。当然隐含层增加，模型的复杂度也就会增加。</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-59b2b5f146d181f8fcf51b531b303b59_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"540\" data-rawheight=\"211\" class=\"origin_image zh-lightbox-thumb\" width=\"540\" data-original=\"https://pic2.zhimg.com/v2-59b2b5f146d181f8fcf51b531b303b59_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;540&#39; height=&#39;211&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"540\" data-rawheight=\"211\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"540\" data-original=\"https://pic2.zhimg.com/v2-59b2b5f146d181f8fcf51b531b303b59_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-59b2b5f146d181f8fcf51b531b303b59_b.jpg\"/></figure><ul><li><b>输出层的神经元可以有多个输出：</b>这样模型便能够灵活的应用于分类和回归，以及其他的机器学习领域，比如降维和聚类。如下图所示，输出层有4个神经元。</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-29ead50f84a4429f9116814abf4ac1e8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"333\" data-rawheight=\"274\" class=\"content_image\" width=\"333\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;333&#39; height=&#39;274&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"333\" data-rawheight=\"274\" class=\"content_image lazy\" width=\"333\" data-actualsrc=\"https://pic1.zhimg.com/v2-29ead50f84a4429f9116814abf4ac1e8_b.jpg\"/></figure><ul><li><b>扩展激活函数：</b>感知机的激活函数sign(z)处理能力有限，因此神经网络一般使用其他激活函数，比如我们在<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3MjA2NTQzMw%3D%3D%26mid%3D2247483814%26idx%3D1%26sn%3D16a56382d24e304a95ab2a2a028993c6%26chksm%3Dfcd7d250cba05b46e16e5db30a85965878d051a17517b90c27f3206d23c6f3784c4e363f06eb%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">逻辑回归</a>里面使用的Sigmoid函数。当然还有tanx,softmax,ReLU等激活函数，通过使用不同的激活函数，神经网络的表达能力也就不同，对于各种常用的激活函数，我们在后面会进行专门介绍。</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-ebeb05a2754a12250d4e2fc6bef86ff2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1594\" data-rawheight=\"194\" class=\"origin_image zh-lightbox-thumb\" width=\"1594\" data-original=\"https://pic3.zhimg.com/v2-ebeb05a2754a12250d4e2fc6bef86ff2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1594&#39; height=&#39;194&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1594\" data-rawheight=\"194\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1594\" data-original=\"https://pic3.zhimg.com/v2-ebeb05a2754a12250d4e2fc6bef86ff2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-ebeb05a2754a12250d4e2fc6bef86ff2_b.jpg\"/></figure><h2>3.DNN基本结构</h2><p>从DNN按照不同层的位置来划分，DNN内部的神经网络层可以分为三类，分别是输入层、隐含层、输出层。如下图所示，一般来说第一层是输入层，最后一层是输出层，而中间的层数都是隐含层。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-3e7e36c0d07afc704a829bf14731815b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"560\" data-rawheight=\"279\" class=\"origin_image zh-lightbox-thumb\" width=\"560\" data-original=\"https://pic4.zhimg.com/v2-3e7e36c0d07afc704a829bf14731815b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;560&#39; height=&#39;279&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"560\" data-rawheight=\"279\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"560\" data-original=\"https://pic4.zhimg.com/v2-3e7e36c0d07afc704a829bf14731815b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-3e7e36c0d07afc704a829bf14731815b_b.jpg\"/></figure><p>DNN的层与层之间是全连接的，也就是说，第i层的任意一个神经元一定与第i+1层的任意一个神经元相连。虽然DNN看起来复杂，但是从局部模型来说，还是和感知机相同，即线性关系z加上激活函数σ(z)。由于DNN层数较多，那么线性关系系数w和偏移量b也就很多。但具体的参数在DNN之中如何定义呢？</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-af5c8e9ed6977d4567df4ef0edec1ec6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"617\" data-rawheight=\"243\" class=\"origin_image zh-lightbox-thumb\" width=\"617\" data-original=\"https://pic3.zhimg.com/v2-af5c8e9ed6977d4567df4ef0edec1ec6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;617&#39; height=&#39;243&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"617\" data-rawheight=\"243\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"617\" data-original=\"https://pic3.zhimg.com/v2-af5c8e9ed6977d4567df4ef0edec1ec6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-af5c8e9ed6977d4567df4ef0edec1ec6_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-631ca0eb1d7385caf459d441ba452655_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1618\" data-rawheight=\"390\" class=\"origin_image zh-lightbox-thumb\" width=\"1618\" data-original=\"https://pic2.zhimg.com/v2-631ca0eb1d7385caf459d441ba452655_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1618&#39; height=&#39;390&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1618\" data-rawheight=\"390\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1618\" data-original=\"https://pic2.zhimg.com/v2-631ca0eb1d7385caf459d441ba452655_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-631ca0eb1d7385caf459d441ba452655_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ec4d53b7054ac2658186c20759eee4a0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"298\" data-rawheight=\"243\" class=\"content_image\" width=\"298\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;298&#39; height=&#39;243&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"298\" data-rawheight=\"243\" class=\"content_image lazy\" width=\"298\" data-actualsrc=\"https://pic1.zhimg.com/v2-ec4d53b7054ac2658186c20759eee4a0_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-0dd5fc4da5827558b5b71e9285d663d2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1596\" data-rawheight=\"118\" class=\"origin_image zh-lightbox-thumb\" width=\"1596\" data-original=\"https://pic3.zhimg.com/v2-0dd5fc4da5827558b5b71e9285d663d2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1596&#39; height=&#39;118&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1596\" data-rawheight=\"118\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1596\" data-original=\"https://pic3.zhimg.com/v2-0dd5fc4da5827558b5b71e9285d663d2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-0dd5fc4da5827558b5b71e9285d663d2_b.jpg\"/></figure><h2>4.DNN前向传播算法的数学原理</h2><p>我们已经了解DNN中线性关系系数w和偏倚量b的定义。现在假设选择的激活函数是σ(z)，隐含层和输出层的输出值为a。则对于下述的三层DNN，我们利用和感知机一样的思路，将上一层的输出当作下一层的输入，然后计算下一层的输出，重复下去，也就是DNN的前向传播算法。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-72c7151619ebdaecf8a6e0c0ad23ade9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1070\" data-rawheight=\"755\" class=\"origin_image zh-lightbox-thumb\" width=\"1070\" data-original=\"https://pic2.zhimg.com/v2-72c7151619ebdaecf8a6e0c0ad23ade9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1070&#39; height=&#39;755&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1070\" data-rawheight=\"755\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1070\" data-original=\"https://pic2.zhimg.com/v2-72c7151619ebdaecf8a6e0c0ad23ade9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-72c7151619ebdaecf8a6e0c0ad23ade9_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-824e4b4fe673d699efa159d8b3a56b54_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1604\" data-rawheight=\"580\" class=\"origin_image zh-lightbox-thumb\" width=\"1604\" data-original=\"https://pic1.zhimg.com/v2-824e4b4fe673d699efa159d8b3a56b54_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1604&#39; height=&#39;580&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1604\" data-rawheight=\"580\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1604\" data-original=\"https://pic1.zhimg.com/v2-824e4b4fe673d699efa159d8b3a56b54_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-824e4b4fe673d699efa159d8b3a56b54_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-94b058370a5b697f61e722d463e6f22e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1612\" data-rawheight=\"248\" class=\"origin_image zh-lightbox-thumb\" width=\"1612\" data-original=\"https://pic3.zhimg.com/v2-94b058370a5b697f61e722d463e6f22e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1612&#39; height=&#39;248&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1612\" data-rawheight=\"248\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1612\" data-original=\"https://pic3.zhimg.com/v2-94b058370a5b697f61e722d463e6f22e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-94b058370a5b697f61e722d463e6f22e_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-54c4ebc1febfead85736f74943beb3a2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1614\" data-rawheight=\"378\" class=\"origin_image zh-lightbox-thumb\" width=\"1614\" data-original=\"https://pic3.zhimg.com/v2-54c4ebc1febfead85736f74943beb3a2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1614&#39; height=&#39;378&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1614\" data-rawheight=\"378\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1614\" data-original=\"https://pic3.zhimg.com/v2-54c4ebc1febfead85736f74943beb3a2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-54c4ebc1febfead85736f74943beb3a2_b.jpg\"/></figure><h2>5.DNN前向传播算法</h2><p>DNN前向传播算法也就是利用若干个权重系数矩阵W，偏倚向量，输入值向量x进行一系列线形运算和激活运算。从输入层开始，一层层的向后进行运算，直到运算到输出层，得到输出结果为止。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-a92a46c0a11857fb2f2e10eb773b3ca3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1578\" data-rawheight=\"410\" class=\"origin_image zh-lightbox-thumb\" width=\"1578\" data-original=\"https://pic4.zhimg.com/v2-a92a46c0a11857fb2f2e10eb773b3ca3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1578&#39; height=&#39;410&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1578\" data-rawheight=\"410\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1578\" data-original=\"https://pic4.zhimg.com/v2-a92a46c0a11857fb2f2e10eb773b3ca3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-a92a46c0a11857fb2f2e10eb773b3ca3_b.jpg\"/></figure><p>单独看DNN前向传播算法，通过运算之后，得到的结果并没有什么意义，误差似乎特别大。而且怎么得到初始的矩阵W,偏倚向量b，最优的矩阵W,偏倚向量b呢？下篇文章将通过深度神经网络之反向传播算法来解决这些问题。</p><p>参考</p><blockquote><a href=\"https://link.zhihu.com/?target=http%3A//www.cnblogs.com/pinard/p/6418668.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">刘建平Pinard_深度神经网络（DNN）模型与前向传播算法</a></blockquote><h2>6.推广</h2><p>更多内容请关注公众号<b>谓之小一</b>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p><p><a href=\"https://link.zhihu.com/?target=http%3A//weixin.qq.com/r/Ty_4oDPE2W6mrXfD93pd\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">weixin.qq.com/r/Ty_4oDP</span><span class=\"invisible\">E2W6mrXfD93pd</span><span class=\"ellipsis\"></span></a> (二维码自动识别)</p><p></p><p></p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "神经网络", 
                    "tagLink": "https://api.zhihu.com/topics/19607065"
                }, 
                {
                    "tag": "算法", 
                    "tagLink": "https://api.zhihu.com/topics/19553510"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/38042361", 
            "userName": "谓之小一", 
            "userLink": "https://www.zhihu.com/people/761dda159c5cec5fd926f761ab5f4ccf", 
            "upvote": 1, 
            "title": "效率软件推荐（一）", 
            "content": "<p>工欲善其事，必先利其器，为此给大家推荐几款效率软件，帮助你高效学习和工作。</p><ul><li>滴答清单：解决拖延症&amp;&amp;健忘症，高效完成任务和规划时间。</li><li>印象笔记： 收集各类信息，捕捉灵感，做你的<b>第二大脑</b>。</li><li>夸克浏览器：极简风格，满足你对浏览器最本质的需求。</li></ul><h2>1.滴答清单</h2><p>身边的人多多少都有点拖延症(包括我)，不到DeadLine的时候，都不会想到去解决问题。为此还耽误了几次事情，但被老师或同学批评几次之后，痛定思痛，决定认真改变拖延症的习惯。奈何自制力又不是太强，没监督或者提醒的话，不久之后又恢复到以前的状态。某天闲逛知乎中，发现滴答清单这款软件，真心不错，用一段时间之后，我多年的老(tuo)寒(yan)腿(zheng)也给治愈啦。另外，多逛知乎也是有用处的，硬是逛出来一篇中文核心论文《面向知乎的个性化推荐模型研究》，在知网现在也能检索到啦，大雾。</p><p>解决拖延症，首要具备的便是任务提醒功能，滴答清单默认每天9点通知该天内所有事项。另外，如果为某件事情设置了具体时间，滴答将会在该时间点时进行提醒。对于未完成的任务，滴答清单图标右上角会时刻显示通知，督促自己完成。</p><p>针对需要完成的每项任务，可增加标签和项目分组。通过标签和分组的结合，能够将任务具体到生活中的各个方面。对于不同的任务，可设置高、中、低、无优先级，优先完成最重要的事情，即使在有限的时间内，也能高效的完成工作。比如<b>文章写作</b>这项任务，我定义为中午12点之前完成，标签为公众号和博客，分组到写作模块。当然想要对某项任务添加更具体的描述时，滴答清单支持照片、位置、录音、附件等信息。当完成某项任务，消除待办事项时，发出滴答的声音，满满的自豪感。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-5235bc3763e261548cab6111e610396c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"750\" data-rawheight=\"1334\" class=\"origin_image zh-lightbox-thumb\" width=\"750\" data-original=\"https://pic1.zhimg.com/v2-5235bc3763e261548cab6111e610396c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;750&#39; height=&#39;1334&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"750\" data-rawheight=\"1334\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"750\" data-original=\"https://pic1.zhimg.com/v2-5235bc3763e261548cab6111e610396c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-5235bc3763e261548cab6111e610396c_b.jpg\"/></figure><p>滴答清单支持在日历上添加任务，当接到某项任务时，随手记下来，当作备忘录，以防忘记。如需回顾过去某天的任务，可根据日期进行查看具体事项。另外滴答清单内置番茄计时，让自己更加专注工作。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-dea6e48eade0801fe7241d0d64364d27_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"750\" data-rawheight=\"1334\" class=\"origin_image zh-lightbox-thumb\" width=\"750\" data-original=\"https://pic4.zhimg.com/v2-dea6e48eade0801fe7241d0d64364d27_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;750&#39; height=&#39;1334&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"750\" data-rawheight=\"1334\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"750\" data-original=\"https://pic4.zhimg.com/v2-dea6e48eade0801fe7241d0d64364d27_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-dea6e48eade0801fe7241d0d64364d27_b.jpg\"/></figure><p>滴答清单风格偏于简约型，没有任何多余功能，看着就非常舒服。当然想要体验更多功能的话，可以升级到高级会员，但我也没有体验过，在此也就不再介绍，其实普通版足以满足个人需求。利用滴答清单设定任务提醒，能够有效督促自己，克服拖延症。对于生活中的日常事情，可随手添加到滴答之中，以防忘记。通过滴答清单，帮助你高效完成任务和规划时间，在滴答清单中记录和规划事情，用更少的时间达到目标。</p><h2>2.印象笔记</h2><p>学习和工作之中，必然包含着各种各样的信息，如果仅仅是通过大脑记忆的话，无法达到对信息的有效收集和整理。另外当需要寻找以往的知识或资料时，很难能够从大脑之中检索出来，当然学霸除外。因此，需要寻找<b>第二大脑</b>来帮助我们收集和整理信息，当需要利用以往知识点的时候，快速检索并加以利用。 好，主角也该上场啦，印象笔记。通过印象笔记在手机、平板、电脑上捕捉和共享灵感，跨平台的印象笔记帮助你永久保存所有信息，将想法转换为行动，做你的第二大脑。 先说一下我是怎么利用利用印象笔记的吧。印象笔记支持标签和笔记本两种分类模式，两种方式的结合能够覆盖到生活和学习之中的方方面面。信息对于我来说分为三个模式，分别为<b>信息输入</b>、<b>信息加工(学习)</b>、<b>信息输出</b>，所以我也就将笔记本分为三种模式，记为0、1、2。</p><p>信息输入模块包含两个笔记本，第一个笔记本为杂乱信息收集(0Inbox)，白天收集到的信息全部放在该笔记本之中，然后晚上将收集到的信息细分到信息加工模块。第二个笔记本为灵感收集(0InSpiration)，将突然想到的灵感记录到该笔记本之中，然后利用空闲时间进行扩充和整理。</p><p>信息加工(学习)模块用于记录生活、学习、工作之中的事情。生活(1HeartOfLife)、学习(1SelfStudy)、工作(1WorkStatus)笔记本组再分为相应笔记本，笔记本之中的每个笔记另外加上标签，当我们需要寻找过去的某条笔记或者知识点时，利用印象笔记的搜索功能能够秒搜索到所需要的信息。</p><p>信息输出模块是将以往所收集到的信息进行总结、深度加工整理到该笔记本之中，另外个人重要信息也会收集到之中。比如我在信息加工(学习)模块总结得到的经验和教训、所写的文章、项目规划等都会记录到信息输出模块。从杂乱的信息之中，经过上述三步便能进行结构化处理，方便自己学习，也方便以后利用。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-048fd44c550d3817a9e9cfcd0028aa67_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"394\" data-rawheight=\"1184\" class=\"content_image\" width=\"394\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;394&#39; height=&#39;1184&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"394\" data-rawheight=\"1184\" class=\"content_image lazy\" width=\"394\" data-actualsrc=\"https://pic4.zhimg.com/v2-048fd44c550d3817a9e9cfcd0028aa67_b.jpg\"/></figure><p>印象笔记的笔记本只支持二级分组，如果想要覆盖生活中所有知识的话，创建很多笔记本，难免会冗余。此时我们可以利用印象笔记的分类标签，标签支持无限嵌套。写笔记时对每个笔记添加不同的标签，这样就可以覆盖各个方面的知识点。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-97f19b41f3e5c1bf32dfa3871640c83a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1478\" data-rawheight=\"1418\" class=\"origin_image zh-lightbox-thumb\" width=\"1478\" data-original=\"https://pic3.zhimg.com/v2-97f19b41f3e5c1bf32dfa3871640c83a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1478&#39; height=&#39;1418&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1478\" data-rawheight=\"1418\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1478\" data-original=\"https://pic3.zhimg.com/v2-97f19b41f3e5c1bf32dfa3871640c83a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-97f19b41f3e5c1bf32dfa3871640c83a_b.jpg\"/></figure><p>另外印象笔记支持多种收集方式，比如在网页之中，如果看到有趣的信息，便可利用印象笔记剪辑功能，一步保存网页之中的信息。另外我个人也关注了很多公众号，对于有趣的信息，通常想要收藏到印象笔记之中。我们只需要预先关注<b>我的印象笔记</b>服务号，绑定个人信息之后，找到想要收藏的公众号文章，只需一键发送到公众号，便可收集到个人印象笔记之中。同时印象笔记支持添加语音，pdf文档等形式的信息。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-a879f9a0d0935aca23b51084c3b8c0a2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2560\" data-rawheight=\"1600\" class=\"origin_image zh-lightbox-thumb\" width=\"2560\" data-original=\"https://pic3.zhimg.com/v2-a879f9a0d0935aca23b51084c3b8c0a2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2560&#39; height=&#39;1600&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2560\" data-rawheight=\"1600\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2560\" data-original=\"https://pic3.zhimg.com/v2-a879f9a0d0935aca23b51084c3b8c0a2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-a879f9a0d0935aca23b51084c3b8c0a2_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-a98baad04a724218603ac6c28dd714e4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"750\" data-rawheight=\"1334\" class=\"origin_image zh-lightbox-thumb\" width=\"750\" data-original=\"https://pic1.zhimg.com/v2-a98baad04a724218603ac6c28dd714e4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;750&#39; height=&#39;1334&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"750\" data-rawheight=\"1334\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"750\" data-original=\"https://pic1.zhimg.com/v2-a98baad04a724218603ac6c28dd714e4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-a98baad04a724218603ac6c28dd714e4_b.jpg\"/></figure><p>印象笔记分为三种模式，分别为免费账户、标准账户、高级账户。免费账户功能较少，支持两台同步设备，每月上传流量为60M，支持搜索图片内的文字等功能。标准账户和高级账户功能差不多，但目前印象笔记高级账户在做活动，价格更便宜一些，年费价格大概是80左右。对于付费产品，我的看法是价格反映的是产品的品质，至于贵不贵，取决于它对人的价值。</p><p>印象笔记高级账户支持多平台登陆(无限制)、同步所有设备、每月10G上传流量、标注pdf功能、笔记演示、文档搜索等功能。说实话，印象笔记中的搜索功能非常强，图片和文档之中的信息能够轻松搜索出来。即使忘记笔记放在哪个笔记本分组里，也可以秒搜索到。最后还有个很惊艳的功能，文档扫描。大一时候记了很多高数笔记，但到大三之后，又不舍的扔，又没有空闲地方去放置。所以利用印象笔记文档扫描功能将所有的高数笔记扫描到印象笔记之中，扫描的内容非常清晰，而且还可以在扫描后的文档上进行编辑。笔记移动到印象笔记之后，想要找高数中知识点，只需搜索相应内容即可，基本上是秒找到高数内容。如果是纸质笔记本的话，还需要一页页去查询，很繁琐。利用印象笔记的话，节省了很大功夫，现在纸质笔记本也就可以直接扔啦。</p><p>利用印象笔记，记录生活、工作、学习中方方面面知识点，将所有信息保存到<b>第二大脑</b>之中，然后结构化管理，智能记录，有序生活。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-6e747e0a52572501d228653f52600141_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2560\" data-rawheight=\"1600\" class=\"origin_image zh-lightbox-thumb\" width=\"2560\" data-original=\"https://pic2.zhimg.com/v2-6e747e0a52572501d228653f52600141_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2560&#39; height=&#39;1600&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2560\" data-rawheight=\"1600\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2560\" data-original=\"https://pic2.zhimg.com/v2-6e747e0a52572501d228653f52600141_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-6e747e0a52572501d228653f52600141_b.jpg\"/></figure><h2>3.夸克浏览器</h2><p>电脑上浏览器我也就不推荐啦，多数用的都是Google Chrome浏览器，优点很多，大家慢慢探索就好。在这儿推荐下手机上的一个小众浏览器，<b>夸克</b>。</p><p>我也曾经用过iPhone版Chrome浏览器，但每次想要搜索东西的时候，打开软件的时间就需要5s，就算进入到搜索界面之后也会有一些杂乱信息，很容易让自己分心，影响工作效率。这里更别提手机版的百度首页了，信息杂乱程度惨不忍睹。而夸克浏览器就不一样了，打开软件时间基本上是在1s左右，进入软件之后，只留有一个搜索框，没必要做任何多余的事情，果然是够快、够简约。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-e2ef95abe2a5751a8289a97f2a24da3d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"750\" data-rawheight=\"1334\" class=\"origin_image zh-lightbox-thumb\" width=\"750\" data-original=\"https://pic2.zhimg.com/v2-e2ef95abe2a5751a8289a97f2a24da3d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;750&#39; height=&#39;1334&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"750\" data-rawheight=\"1334\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"750\" data-original=\"https://pic2.zhimg.com/v2-e2ef95abe2a5751a8289a97f2a24da3d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-e2ef95abe2a5751a8289a97f2a24da3d_b.jpg\"/></figure><p>简约并不意味着简单，夸克浏览器简约之中更是带着很多亮点功能。浏览器底部可以进入一些常用网站，比如少数派、爱范儿、简书等，在小程序出来之前，我经常用这种方式看资讯，使用体验相当不错。对于一些需要用到的网站，但是又不想去下载APP，这样倒是一个不错的解决方法。当然现在小程序出来之后，在小程序看资讯也是很好的方法。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-7056869c579c68fef57e4654d9a8b8b4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"750\" data-rawheight=\"1334\" class=\"origin_image zh-lightbox-thumb\" width=\"750\" data-original=\"https://pic1.zhimg.com/v2-7056869c579c68fef57e4654d9a8b8b4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;750&#39; height=&#39;1334&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"750\" data-rawheight=\"1334\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"750\" data-original=\"https://pic1.zhimg.com/v2-7056869c579c68fef57e4654d9a8b8b4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-7056869c579c68fef57e4654d9a8b8b4_b.jpg\"/></figure><p>总而言之、言而总之，夸克是一款以轻、快为核心，设计风格简约的浏览器，是一款专注用户浏览体验的信息获取工具。启动时无任何多余加载项，瞬间启动无需等待。利用极简思路解决信息冗余，满足用户对于浏览器最本质的需求。同时浏览器本身从底栏自动缩放、菜单分层设计、导航栏设置等方面，给用户带来沉浸式的浏览体验。</p><p>介于篇幅有限，这篇文章就介绍这么多，下篇文章将重点介绍YoMail、Kindle、Typora、Ulysse软件。</p><h2>4.推广</h2><p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p><p><a href=\"https://link.zhihu.com/?target=http%3A//weixin.qq.com/r/Ty_4oDPE2W6mrXfD93pd\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">weixin.qq.com/r/Ty_4oDP</span><span class=\"invisible\">E2W6mrXfD93pd</span><span class=\"ellipsis\"></span></a> (二维码自动识别)</p><p></p><p></p>", 
            "topic": [
                {
                    "tag": "滴答清单", 
                    "tagLink": "https://api.zhihu.com/topics/20046180"
                }, 
                {
                    "tag": "印象笔记", 
                    "tagLink": "https://api.zhihu.com/topics/19703613"
                }, 
                {
                    "tag": "效率", 
                    "tagLink": "https://api.zhihu.com/topics/19556677"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/37934992", 
            "userName": "谓之小一", 
            "userLink": "https://www.zhihu.com/people/761dda159c5cec5fd926f761ab5f4ccf", 
            "upvote": 2, 
            "title": "网店工商信息图片文字提取", 
            "content": "<p>首先非常抱歉，最近一段时间由于学校课程作业较多，外加个人较懒，所以一直没有更新文章，以后一定会勤奋点，多加更新。正如前几天在stormzhang<b>(张哥)</b>的公众号里看到的一样，写作其实并不难，每个人都可以做到，但是长期坚持写作就非常难，这也是写作者想要长期创作遇到的第一个大问题，所以贵在坚持。另外长期写作的第二大问题是什么呢？你猜猜看，看看我们认为的是不是一样。</p><p>最近主要是完成专业内的一些课程作业，比如Oracle数据库、JaveEE、搜索引擎等作业。国内大学总是会学很多课程，其实对多数学生来说，一些课程都不知道学着有什么意义。这点国外做的较是不错，在英国UWS当交换生的时候，可以选择自己喜欢的课程，这样也就有很大的兴趣去学习这些知识点。</p><p>在解决这些课程作业之中，有件事感觉可以和大家分享一下。我们都知道计算机行业技术更新非常快，然而JavaEE老师教的知识点还是10多年前的内容，每次课程结束之中还需完成一个实验。但就是这样一个简单实验，却需要我们学生花费2天或者3天时间去完成。花这么长时间，按理来说应该很难吧，恰恰相反，实验很简单，那为什么还要花这么长时间呢。其实多数时间都是用在各种环境配置、参数设置、寻找各种jar包中，实在不需要写多少代码。比如我需要调用某个jar包，版本太高不行，版本太低不行，来来回回换个好几个，遇到问题想去查一些博客，竟然都是10年前的资料。完成一次实验之后，至此JaveEE的实验我再也没有去做，每次要交的时候，都是借用同学的电脑给老师展示一下，然后拿个分数就走。不是说我懒，没有什么探索、钻研精神，全然是因为学习这种东西实在没有什么用处，还浪费很多时间，不如利用这些时间去完成一些自己比较感兴趣的事情。</p><p>另外需要声明一点的是，我的意思并不是旧的东西就没有用，而是强调在实用性和意义方面。比如数据结构、网络原理、操作系统，这样原理性的知识点，沉淀起来才是精华。但对于JavaEE这种实际开发技术来说，我认为过于陈旧的东西实在没有必要去学习。另外针对JaveEE开发这门课，任课老师为什么就不能更新一下知识点，来教一些更新的技术呢。既然如此，我的目标又不是追求多高多高的GPA，那么不如利用这些时间来解决一些自己比较感兴趣的问题，做一些有意义的事情较好。</p><p>专业课程作业之外，还有一个实训作业，也就是从<a href=\"https://link.zhihu.com/?target=http%3A//www.cnsoftbei.com/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">中软杯</a>12个题目之中选出来一个完成，然后进行答辩，由指导老师进行评分。这个我感觉还是比较有意思的，所以选了个网店工商信息图片文字提取的题目，然后花四天时间完成，下面主要和大家分享一下问题的解决思路。</p><h2>1.网店工商信息图片文字提取</h2><p>图片内容如下所示，但每张图片中信息出现的位置不尽相同，题目要求所写的程序能够完成如下几个功能点。</p><ul><li>程序能够识别不同格式的图片，并能够提取所要求的信息。</li><li>从图片之中提取企业注册号和企业名称信息，并保存到Excel表格之中。</li><li>程序能够自动读取企业工商信息图片所在的文件夹路径。</li><li>识别速度保持在60秒识别50张图片，识别正确率保证在95%以上。</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-7857e8234e40d56f67cb21740ffa5ccb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"430\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic4.zhimg.com/v2-7857e8234e40d56f67cb21740ffa5ccb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;430&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"430\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic4.zhimg.com/v2-7857e8234e40d56f67cb21740ffa5ccb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-7857e8234e40d56f67cb21740ffa5ccb_b.jpg\"/></figure><h2>2.Tess4j</h2><p>了解题目要求之后，我们便开始来解决问题。首先明确一点的是，肯定不能从头去写文字识别算法或者文字识别程序，OCR(Optical Character Recognition , 光学字符识别)发展这么多年来，开源的库肯定不少，只需找到适合中文识别的类库或者项目即可。</p><p>个人采用的是Tess4j开源库，其中Tess4j是由Tesseract扩展而来，Tesseract是HP实验室开发由Google维护的开源OCR引擎，Tess4j支持Tiff,jpeg,gif,png,pdf等多种格式识别。我们只需要在<a href=\"https://link.zhihu.com/?target=https%3A//sourceforge.net/projects/tess4j/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">sourceforge.net/project</span><span class=\"invisible\">s/tess4j/</span><span class=\"ellipsis\"></span></a>下载类库，然后编写下述代码便可实现文字识别，使用方法很简单。如果你要使用的话，请注意package,imageFile,instance的位置。</p><div class=\"highlight\"><pre><code class=\"language-java\"><span class=\"kn\">package</span> <span class=\"nn\">net.sourceforge.tess4j.example</span><span class=\"o\">;</span>\n\n<span class=\"kn\">import</span> <span class=\"nn\">java.io.File</span><span class=\"o\">;</span>\n<span class=\"kn\">import</span> <span class=\"nn\">net.sourceforge.tess4j.*</span><span class=\"o\">;</span>\n\n<span class=\"kd\">public</span> <span class=\"kd\">class</span> <span class=\"nc\">TesseractExample1</span> <span class=\"o\">{</span>\n    <span class=\"kd\">public</span> <span class=\"kd\">static</span> <span class=\"kt\">void</span> <span class=\"nf\">main</span><span class=\"o\">(</span><span class=\"n\">String</span><span class=\"o\">[]</span> <span class=\"n\">args</span><span class=\"o\">)</span> <span class=\"o\">{</span>        \n \n        <span class=\"n\">File</span> <span class=\"n\">imageFile</span> <span class=\"o\">=</span> <span class=\"k\">new</span> <span class=\"n\">File</span><span class=\"o\">(</span><span class=\"s\">&#34;/Users/zhenhai/Downloads/SoftwareCup/Tess4J/test/resources/tianmao/1.png&#34;</span><span class=\"o\">);</span>\n        <span class=\"n\">ITesseract</span> <span class=\"n\">instance</span> <span class=\"o\">=</span> <span class=\"k\">new</span> <span class=\"n\">Tesseract</span><span class=\"o\">();</span>  <span class=\"c1\">// JNA Interface Mapping\n</span><span class=\"c1\"></span>        <span class=\"c1\">// ITesseract instance = new Tesseract1(); // JNA Direct Mapping\n</span><span class=\"c1\"></span>        <span class=\"n\">instance</span><span class=\"o\">.</span><span class=\"na\">setDatapath</span><span class=\"o\">(</span><span class=\"s\">&#34;/Users/zhenhai/Downloads/SoftwareCup/Tess4J&#34;</span><span class=\"o\">);</span>\n\t\t<span class=\"n\">instance</span><span class=\"o\">.</span><span class=\"na\">setLanguage</span><span class=\"o\">(</span><span class=\"s\">&#34;chi_sim&#34;</span><span class=\"o\">);</span>\n\n        <span class=\"k\">try</span> <span class=\"o\">{</span>\n            <span class=\"n\">String</span> <span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">instance</span><span class=\"o\">.</span><span class=\"na\">doOCR</span><span class=\"o\">(</span><span class=\"n\">imageFile</span><span class=\"o\">);</span>\n            <span class=\"n\">System</span><span class=\"o\">.</span><span class=\"na\">out</span><span class=\"o\">.</span><span class=\"na\">println</span><span class=\"o\">(</span><span class=\"n\">result</span><span class=\"o\">);</span>\n        <span class=\"o\">}</span> <span class=\"k\">catch</span> <span class=\"o\">(</span><span class=\"n\">TesseractException</span> <span class=\"n\">e</span><span class=\"o\">)</span> <span class=\"o\">{</span>\n            <span class=\"n\">System</span><span class=\"o\">.</span><span class=\"na\">err</span><span class=\"o\">.</span><span class=\"na\">println</span><span class=\"o\">(</span><span class=\"n\">e</span><span class=\"o\">.</span><span class=\"na\">getMessage</span><span class=\"o\">());</span>\n        <span class=\"o\">}</span>\n    <span class=\"o\">}</span>\n<span class=\"o\">}</span></code></pre></div><h2>3.网店工商信息图片文字提取</h2><p>下载的tess4j项目自带英文字体库，而我们需要识别中文信息，所以需下载中文简体字体库。字体库下载完成之后，将题目提供给我们的图片进行识别，识别之后发现准确率很低，而且识别时间过长，所以需要对图片进行处理。</p><blockquote>企业注册号 : 913302055612570鄄7 ′<br/>企业名称: 宁麦皮中哲票广鲳I忏善 蓼鬓′墓示埔壹甬<br/>类 型 霉嫣膘占辆 虫资) 趴辕~蓼唧 `<br/>住惑7妻 踢「【庄北长兴路689弄22号11巾童A1壬蔚雀菅业^ 刁乏喔憩」壹雇<br/>法 人: 杨禾口荣<br/>成立时间:2010-08-26 甬 甬<br/>注册资本 : 1000万人民币元 / /<br/>营业I言【j目〖艮:2010-08洲:i墅o碾言壹 鹏 莹鬓、′墓示簪<br/>经莒范戛蓼反逼卫 目艮饰日勺扎匕发、 零售、 薯批愤嵩爵签稻昊信息的咨询 ; 服装i氦十犹撕{<br/>菖〈茵珥跨止\\ 懦牌苣理 广告服务、 企业苣癫颧琨蓼 扩〈喔圃蓼′<br/>登i 机关 : 浙江雀宁波市江北区工商『壬政苣王里局<br/>核准时间 : 2015-12-24 <b>日q </b>日辄</blockquote><h2>3.1去除水印</h2><p>首先能够看到，提供的图片带有<b>天猫营业执照信息公示专用</b>水印，所以我们需要进行去水印处理。花了很长时间在网上找去水印的开源代码，但多数都需要先提供水印模版，然后才能进行去水印处理。水印模版不是问题，我们直接截取水印图片即可，但重点是去水印处理之后，水印去除效果并不是很理想。观察一番之后，发现所有的图片水印都是同一个颜色，那么我们是不是可以把水印的rgb值改为和背景色相同，实验之后发现果然可以，由于代码比较简单，此处就不再贴出代码，可以自行尝试一下。然后重新对图片进行识别，发现准确率还是很低，那是什么原因呢？</p><blockquote>企业注册号 : 913302055612570鄄7 ′<br/>企业名称: 宁麦皮中哲票广鲳I忏善 蓼鬓′墓示埔壹甬<br/>类 型 霉裴章膘占辆 虫资) 趴辕~苜趴 `<br/>住惑)妻 踢「【庄北长兴路689弄22号11巾童A1壬蔚雀菅业^ 刁乏喔憩」壹雇<br/>法 人: 杨禾口荣<br/>成立时间:2010-08-26 甬 甬<br/>注册资本 : 1000万人民币元 / /<br/>营业I言【j目〖艮:2010-08洲:i墅o蔺言壹 鹏 莹鬓、′墓示簪<br/>经莒范戛蓼反逼卫 目艮饰日勺扎匕发、 零售、 薯批愤嵩爵垦稻昊信息的咨询 ; 眼装i氦十犹撕{<br/>菖〈茵珥跨止\\ 懦牌苣理 广告目艮务、 企业苣癫颧寰蓼 扩〈喔圃蓼′<br/>登i 机关 : 浙江雀宁波市江北区工商『壬政苣王里局<br/>核准时间 : 2015-12-24 <b>日q </b>日辄</blockquote><h2>3.2图片格式转换</h2><p>将图片放大之后，发现图片处于全黑的状态，完全看不到任何字。然后尝试将图片格式转换为其他格式，比如jpg，当然这里不是直接改后缀名，而是利用在线图片格式转换工具。当然你也可以转换成其他格式，看看效果如何，转换完成之后，再次进行图片文字识别，发现准确率有较大提升。</p><blockquote>企业注丹舟号 : 9133020……612…70177<br/>企业名称 : 宁波中哲慕尚电子商务有限公司<br/>类 型 : 有限责任公司〈法人独资)<br/>住 所 二 宁波市江才匕长兴路689弄22号11瞳A112室<br/>法定代表人: 杨禾口荣<br/>成立时间 : 2010-08-26<br/>注册资本 : 1000万人民币元<br/>营业期限 : 2010-08-26至2020-08-25<br/>经菖范围 : 服装、 箱包、 鞋帽眼饰的批发 零售、 网上批发` 零售及棺关信息的咨询 : 眼装i毓十<br/>、 企业品牌营王里、 广 告眼务、 企业盲理咨询。<br/>登记机关 : 浙江雀宁波市麦工才匕区工商肴壬政苣王里局<br/>核准时间 : 2015-12-24</blockquote><h2>3.3分区域识别</h2><p>图片识别准确率有一定程度提升之后，但是时间还是很高，大概15s左右，所以需要进一步优化。由于题目只需要我们识别企业注册号和企业名称，所以没有必要识别整张图片。但我们又不知道企业名称和企业注册号处于图片的什么位置，所以只能扫描着进行文字图片识别。我这里采用的是每次识别图片高度的18%，例第一次识别区域为0-18%，如果没有识别到我们所需的文字信息，下次识别图片15%-32%，这样就不会遇到文字刚好被识别区域切割的问题。</p><p>那这样识别会识别到很多重复区域，时间怎么会提升呢？其实不然，观察题目所给的50张图片，其中有46张图片的信息都是在头部，那么第一次扫描便能得到所需的信息，综合来看时间有很大程度提升。另外如果长时间未能识别到某张图片信息，那么则自动放弃识别。更改程序后重新识别图片，时间有很大程度提升，每张图片的识别速度在3s左右。</p><blockquote>企业注册号 : 913302055612570177<br/>企业名称 : 宁波中哲票尚电子商务有眼公司</blockquote><p>对于本张图片来说，企业注册号已经能够正确识别，但是企业名称还是有个别字错误，比如将<b>幕</b>识别成<b>票</b>，将<b>限</b>识别成<b>眼</b>，因此需要进一步优化。</p><h2>3.4 图片二值化</h2><p>为进一步提高准确率，我们将图片二值化，然后再对图片放大10倍，其实放大倍数越高，识别准确率也应该越高。这里为了在时间和准确度之间做个平衡，对图片只放大10倍。</p><div class=\"highlight\"><pre><code class=\"language-java\"><span class=\"n\">BufferedImage</span> <span class=\"n\">textImage</span> <span class=\"o\">=</span> <span class=\"n\">ImageHelper</span><span class=\"o\">.</span><span class=\"na\">convertImageToGrayscale</span><span class=\"o\">(</span><span class=\"n\">ImageHelper</span><span class=\"o\">.</span><span class=\"na\">getSubImage</span><span class=\"o\">(</span><span class=\"n\">image</span><span class=\"o\">,</span> <span class=\"n\">0</span><span class=\"o\">,</span> <span class=\"n\">startHeight</span><span class=\"o\">,</span> <span class=\"n\">resetWidth</span><span class=\"o\">,</span> <span class=\"n\">resetHeight</span><span class=\"o\">));</span>\n\n<span class=\"n\">textImage</span> <span class=\"o\">=</span> <span class=\"n\">ImageHelper</span><span class=\"o\">.</span><span class=\"na\">convertImageToBinary</span><span class=\"o\">(</span><span class=\"n\">textImage</span><span class=\"o\">);</span>\n\n<span class=\"n\">textImage</span> <span class=\"o\">=</span> <span class=\"n\">ImageHelper</span><span class=\"o\">.</span><span class=\"na\">getScaledInstance</span><span class=\"o\">(</span><span class=\"n\">textImage</span><span class=\"o\">,</span> <span class=\"n\">textImage</span><span class=\"o\">.</span><span class=\"na\">getWidth</span><span class=\"o\">()</span> <span class=\"o\">*</span> <span class=\"n\">10</span><span class=\"o\">,</span> <span class=\"n\">textImage</span><span class=\"o\">.</span><span class=\"na\">getHeight</span><span class=\"o\">()</span> <span class=\"o\">*</span> <span class=\"n\">10</span><span class=\"o\">);</span></code></pre></div><p>图片放大10倍之后，我们再次对图片进行识别，发现企业注册号和企业名称完全正确。</p><blockquote>企业注册号 : 913302055612570177<br/>企业名称 : 宁波中哲幕尚电子商务有限公司</blockquote><h2>3.5图片模糊寻找和结果导出</h2><p>图片模糊寻找的意思也就是，给出图片文件夹的大致路径，然后程序能够找到正确的图片路径，并能够正确进行文字识别。比如给定/Users/zhenhai/Downloads/SoftwareCup/Tess4J路径，程序能够找到/Users/zhenhai/Downloads/SoftwareCup/Tess4J/test/resources/tianmao1/1.jpg路径。然后将识别到的结果导出到Excel表格，问题也很简单，这里也就不给出相应代码。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-6e35022ae33c2ce32f0bab2d90d19170_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1578\" data-rawheight=\"800\" class=\"origin_image zh-lightbox-thumb\" width=\"1578\" data-original=\"https://pic1.zhimg.com/v2-6e35022ae33c2ce32f0bab2d90d19170_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1578&#39; height=&#39;800&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1578\" data-rawheight=\"800\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1578\" data-original=\"https://pic1.zhimg.com/v2-6e35022ae33c2ce32f0bab2d90d19170_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-6e35022ae33c2ce32f0bab2d90d19170_b.jpg\"/></figure><p>至此已经能够识别图片，而且准确率挺不错，准确率在95%左右，识别成功之后也能够正常导出至Excel表格。但时间依旧不理想，目前识别50张图片大概在2分钟30s左右。</p><h2>4.待优化</h2><p>图片文字识别方面只做了4天，之前也没有做过相关问题，所以还是有很大的优化空间。</p><ul><li>利用多线程，识别时间应该能够减少1分钟，达到1分钟30s识别50张(猜测)。</li><li>由于我们直接利用网上的字库，没有对字库做任何训练。比如可以将出现频率较高的词设置更高的优先级，这样不仅能够提高准确率，而且能够进一步降低时间，比如上述的<b>限</b>不会再识别成<b>眼</b>。</li><li>图片大小不一，可以将图片设置为平均宽度和高度，然后再进行分区域识别。而且每次识别时候不是识别企业注册号和企业名称的完整信息，而只是试探识别这几个字，如果识别成功之后，然后再扩大识别宽度，提取所需要的完整信息。</li></ul><h2>5.推广</h2><p>更多内容请关注公众号<b>谓之小一</b>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p><p><a href=\"https://link.zhihu.com/?target=http%3A//weixin.qq.com/r/Ty_4oDPE2W6mrXfD93pd\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">weixin.qq.com/r/Ty_4oDP</span><span class=\"invisible\">E2W6mrXfD93pd</span><span class=\"ellipsis\"></span></a> (二维码自动识别)</p><p></p><p></p>", 
            "topic": [
                {
                    "tag": "OCR（光学字符识别）", 
                    "tagLink": "https://api.zhihu.com/topics/19574441"
                }, 
                {
                    "tag": "文字识别（技术）", 
                    "tagLink": "https://api.zhihu.com/topics/19704439"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/37019560", 
            "userName": "谓之小一", 
            "userLink": "https://www.zhihu.com/people/761dda159c5cec5fd926f761ab5f4ccf", 
            "upvote": 6, 
            "title": "机器学习之Apriori算法", 
            "content": "<blockquote>由于文中含有公式，所以我将部分内容转为图片进行上传，请见谅。清晰版请访问<a href=\"https://link.zhihu.com/?target=https%3A//weizhixiaoyi.com\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">weizhixiaoyi.com</span><span class=\"invisible\"></span></a>查看，你也可以关注我公众号<b>谓之小一</b>，后台直接向我要pdf版本，如有相关问题可公众号后台询问，随时回答。</blockquote><h2>1.Apriori算法简介</h2><p><b>Apriori</b>算法是常用于挖掘出数据关联规则的算法，能够发现事物数据库中频繁出现的数据集，这些联系构成的规则可帮助用户找出某些行为特征，以便进行企业决策。例如，某食品商店希望发现顾客的购买行为，通过购物篮分析得到<b>大部分顾客会在一次购物中同时购买面包和牛奶</b>，那么该商店便可以通过降价促销面包的同时提高面包和牛奶的销量。了解Apriori算法推导之前，我们先介绍一些基本概念。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-b67bbe6c25d020c76fdb6fd99d719e7e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1606\" data-rawheight=\"1194\" class=\"origin_image zh-lightbox-thumb\" width=\"1606\" data-original=\"https://pic3.zhimg.com/v2-b67bbe6c25d020c76fdb6fd99d719e7e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1606&#39; height=&#39;1194&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1606\" data-rawheight=\"1194\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1606\" data-original=\"https://pic3.zhimg.com/v2-b67bbe6c25d020c76fdb6fd99d719e7e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-b67bbe6c25d020c76fdb6fd99d719e7e_b.jpg\"/></figure><p>关联规则的挖掘目标是<b>找出所有的频繁项集</b>和<b>根据频繁项集产生强关联规则</b>。对于Apriori算法来说，其目标是找出所有的频繁项集，因此对于数据集合中的频繁数据集，我们需要自定义评估标准来找出频繁项集，常用的评估标准就是用上述介绍的支持度。</p><h2>2.Apriori算法原理</h2><p>Apriori算法是经典生成关联规则的频繁项集挖掘算法，其目标是<b>找到最多的K项频繁集</b>。那么什么是最多的K项频繁集呢？例如当我们找到符合支持度的频繁集AB和ABE，我们会选择3项频繁集ABE。下面我们介绍Apriori算法选择频繁K项集过程。</p><p>Apriori算法采用迭代的方法，先搜索出候选1项集以及对应的支持度，剪枝去掉低于支持度的候选1项集，得到频繁1项集。然后对剩下的频繁1项集进行连接，得到候选2项集，筛选去掉低于支持度的候选2项集，得到频繁2项集。如此迭代下去，直到无法找到频繁k+1集为止，对应的频繁k项集的集合便是算法的输出结果。我们可以通过下面例子来看到具体迭代过程。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-0ed620ed7a425c8b72e2e60bf60b2be0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"754\" data-rawheight=\"516\" class=\"origin_image zh-lightbox-thumb\" width=\"754\" data-original=\"https://pic1.zhimg.com/v2-0ed620ed7a425c8b72e2e60bf60b2be0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;754&#39; height=&#39;516&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"754\" data-rawheight=\"516\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"754\" data-original=\"https://pic1.zhimg.com/v2-0ed620ed7a425c8b72e2e60bf60b2be0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-0ed620ed7a425c8b72e2e60bf60b2be0_b.jpg\"/></figure><p>数据集包含4条记录{&#39;134&#39;,&#39;235&#39;,&#39;1235&#39;,&#39;25&#39;}，我们利用Apriori算法来寻找频繁k项集，最小支持度设置为50%。首先生成候选1项集，共包含五个数据{&#39;1&#39;,&#39;2&#39;,&#39;3&#39;,&#39;4&#39;,&#39;5&#39;}，计算5个数据的支持度，然后对低于支持度的数据进行剪枝。其中数据{4}支持度为25%，低于最小支持度，进行剪枝处理，最终频繁1项集为{&#39;1&#39;,&#39;2&#39;,&#39;3&#39;,&#39;5&#39;}。根据频繁1项集连接得到候选2项集{&#39;12&#39;,&#39;13&#39;,&#39;15&#39;,&#39;23&#39;,&#39;25&#39;,&#39;35&#39;}，其中数据{&#39;12&#39;,&#39;15&#39;}低于最低支持度，进行剪枝处理，得到频繁2项集为{&#39;13&#39;,&#39;23&#39;,&#39;25&#39;,&#39;35&#39;}。如此迭代下去，最终能够得到频繁3项集{&#39;235&#39;}，由于数据无法再进行连接，算法至此结束。</p><h2>3.Apriori算法流程</h2><p>从Apriori算法原理中我们能够总结如下算法流程，其中输入数据为数据集合D和最小支持度α，输出数据为最大的频繁k项集。</p><ul><li>扫描数据集，得到所有出现过的数据，作为候选1项集。</li><li>挖掘频繁k项集。</li><ul><li>扫描计算候选k项集的支持度。</li><li>剪枝去掉候选k项集中支持度低于最小支持度α的数据集，得到频繁k项集。如果频繁k项集为空，则返回频繁k-1项集的集合作为算法结果，算法结束。如果得到的频繁k项集只有一项，则直接返回频繁k项集的集合作为算法结果，算法结束。</li><li>基于频繁k项集，连接生成候选k+1项集。</li></ul><li>利用步骤2，迭代得到k=k+1项集结果。</li></ul><h2>4.Apriori算法优缺点</h2><h2>4.1优点</h2><ul><li>适合稀疏数据集。</li><li>算法原理简单，易实现。</li><li>适合事务数据库的关联规则挖掘。</li></ul><h2>4.2缺点</h2><ul><li>可能产生庞大的候选集。</li><li>算法需多次遍历数据集，算法效率低，耗时。</li></ul><h2>5.推广</h2><p>更多内容请关注公众号<b>谓之小一</b>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p><p><a href=\"https://link.zhihu.com/?target=http%3A//weixin.qq.com/r/Ty_4oDPE2W6mrXfD93pd\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">weixin.qq.com/r/Ty_4oDP</span><span class=\"invisible\">E2W6mrXfD93pd</span><span class=\"ellipsis\"></span></a> (二维码自动识别)</p><p>参考</p><p><a href=\"https://link.zhihu.com/?target=http%3A//www.cnblogs.com/pinard/p/6293298.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">刘建平Pinard-Apriori算法原理总结</a></p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "算法", 
                    "tagLink": "https://api.zhihu.com/topics/19553510"
                }, 
                {
                    "tag": "Apriori", 
                    "tagLink": "https://api.zhihu.com/topics/20181764"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/36875421", 
            "userName": "谓之小一", 
            "userLink": "https://www.zhihu.com/people/761dda159c5cec5fd926f761ab5f4ccf", 
            "upvote": 3, 
            "title": "机器学习之朴素贝叶斯算法", 
            "content": "<blockquote>由于较多公式，所以我将部分内容转为图片进行上传，请见谅。清晰版请访问<a href=\"https://link.zhihu.com/?target=https%3A//weizhixiaoyi.com\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">weizhixiaoyi.com</span><span class=\"invisible\"></span></a>查看，你也可以关注我公众号<b>谓之小一</b>，后台直接向我要pdf版本，如有相关问题可公众号后台询问，随时回答。</blockquote><h2>1.朴素贝叶斯简介</h2><p><b>朴素贝叶斯(Naive Bayesian)</b>算法能够根据数据加先验概率来估计后验概率，在垃圾邮件分类、文本分类、信用等级评定等多分类问题中得到广泛应用。对于多数的分类算法，比如决策树、KNN等，他们都是判别方法，也就是直接学习出特征输出Y和特征X之间的关系。但朴素贝叶斯和多数分类算法都不同，朴素贝叶斯是生成算法，也就是先找出特征输出Y和特征X的联合分布P(X,Y)，然后用P(Y|X)=P(X,Y)/P(X)得出。</p><p>朴素贝叶斯算法的优点在于简单易懂、学习效率高，在某些领域的分类问题中能够与决策树相媲美。但朴素贝叶斯算法以自变量之间的独立性和连续变量的正态性假设为前提，会导致算法精度在一定程度上受到影响。</p><h2>2.朴素贝叶斯算法模型</h2><h2>2.1统计知识回顾</h2><p>深入算法原理之前，我们先来回顾下统计学的相关知识。</p><ul><li><b>条件概率公式</b></li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-c6b64cb65978067c3d24a92a3d7a5589_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1582\" data-rawheight=\"130\" class=\"origin_image zh-lightbox-thumb\" width=\"1582\" data-original=\"https://pic2.zhimg.com/v2-c6b64cb65978067c3d24a92a3d7a5589_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1582&#39; height=&#39;130&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1582\" data-rawheight=\"130\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1582\" data-original=\"https://pic2.zhimg.com/v2-c6b64cb65978067c3d24a92a3d7a5589_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-c6b64cb65978067c3d24a92a3d7a5589_b.jpg\"/></figure><ul><li><b>条件概率公式</b></li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-b1841834ef75f3ab6fbaa967e0d993d0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1550\" data-rawheight=\"288\" class=\"origin_image zh-lightbox-thumb\" width=\"1550\" data-original=\"https://pic1.zhimg.com/v2-b1841834ef75f3ab6fbaa967e0d993d0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1550&#39; height=&#39;288&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1550\" data-rawheight=\"288\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1550\" data-original=\"https://pic1.zhimg.com/v2-b1841834ef75f3ab6fbaa967e0d993d0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-b1841834ef75f3ab6fbaa967e0d993d0_b.jpg\"/></figure><ul><li><b>全概率公式</b></li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-7a4ec9c5b74ac827590af82c5ae6181c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1564\" data-rawheight=\"152\" class=\"origin_image zh-lightbox-thumb\" width=\"1564\" data-original=\"https://pic1.zhimg.com/v2-7a4ec9c5b74ac827590af82c5ae6181c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1564&#39; height=&#39;152&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1564\" data-rawheight=\"152\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1564\" data-original=\"https://pic1.zhimg.com/v2-7a4ec9c5b74ac827590af82c5ae6181c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-7a4ec9c5b74ac827590af82c5ae6181c_b.jpg\"/></figure><p>经过上面统计学知识，我们能够得出贝叶斯公式。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-2410266829f2eab19e58822fdfc1a32d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1580\" data-rawheight=\"150\" class=\"origin_image zh-lightbox-thumb\" width=\"1580\" data-original=\"https://pic2.zhimg.com/v2-2410266829f2eab19e58822fdfc1a32d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1580&#39; height=&#39;150&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1580\" data-rawheight=\"150\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1580\" data-original=\"https://pic2.zhimg.com/v2-2410266829f2eab19e58822fdfc1a32d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-2410266829f2eab19e58822fdfc1a32d_b.jpg\"/></figure><h2>2.2朴素贝叶斯模型</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-d8aaf0cbd34611ffdd0a91dd541224dd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1588\" data-rawheight=\"1308\" class=\"origin_image zh-lightbox-thumb\" width=\"1588\" data-original=\"https://pic2.zhimg.com/v2-d8aaf0cbd34611ffdd0a91dd541224dd_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1588&#39; height=&#39;1308&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1588\" data-rawheight=\"1308\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1588\" data-original=\"https://pic2.zhimg.com/v2-d8aaf0cbd34611ffdd0a91dd541224dd_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-d8aaf0cbd34611ffdd0a91dd541224dd_b.jpg\"/></figure><h2>2.3朴素贝叶斯推断</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3adc8a2fc432603af03fb862ec3e95c1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1588\" data-rawheight=\"816\" class=\"origin_image zh-lightbox-thumb\" width=\"1588\" data-original=\"https://pic2.zhimg.com/v2-3adc8a2fc432603af03fb862ec3e95c1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1588&#39; height=&#39;816&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1588\" data-rawheight=\"816\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1588\" data-original=\"https://pic2.zhimg.com/v2-3adc8a2fc432603af03fb862ec3e95c1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-3adc8a2fc432603af03fb862ec3e95c1_b.jpg\"/></figure><h2>2.4朴素贝叶斯参数估计</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-7616a7fda109e25d828a889b30bd1b40_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1608\" data-rawheight=\"1500\" class=\"origin_image zh-lightbox-thumb\" width=\"1608\" data-original=\"https://pic1.zhimg.com/v2-7616a7fda109e25d828a889b30bd1b40_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1608&#39; height=&#39;1500&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1608\" data-rawheight=\"1500\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1608\" data-original=\"https://pic1.zhimg.com/v2-7616a7fda109e25d828a889b30bd1b40_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-7616a7fda109e25d828a889b30bd1b40_b.jpg\"/></figure><h2>3.朴素贝叶斯算法流程</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-c6afb46b76f0837591c7643bccb56fd8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1608\" data-rawheight=\"1208\" class=\"origin_image zh-lightbox-thumb\" width=\"1608\" data-original=\"https://pic1.zhimg.com/v2-c6afb46b76f0837591c7643bccb56fd8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1608&#39; height=&#39;1208&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1608\" data-rawheight=\"1208\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1608\" data-original=\"https://pic1.zhimg.com/v2-c6afb46b76f0837591c7643bccb56fd8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-c6afb46b76f0837591c7643bccb56fd8_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-0f3dba9bdf8055b861ed516a16bd131a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1596\" data-rawheight=\"546\" class=\"origin_image zh-lightbox-thumb\" width=\"1596\" data-original=\"https://pic3.zhimg.com/v2-0f3dba9bdf8055b861ed516a16bd131a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1596&#39; height=&#39;546&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1596\" data-rawheight=\"546\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1596\" data-original=\"https://pic3.zhimg.com/v2-0f3dba9bdf8055b861ed516a16bd131a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-0f3dba9bdf8055b861ed516a16bd131a_b.jpg\"/></figure><p>从上面计算可以看出，朴素贝叶斯没有复杂的求导和矩阵运算，因此效率很高。但朴素贝叶斯假设数据特征之间相互独立，如果数据特征之间关联性比较强的话，我们尽量不要使用朴素贝叶斯算法，考虑其他分类方法比较好。</p><h2>4.Sklearn实现朴素贝叶斯</h2><p>利用sklearn自带的iris数据集进行训练，选取70%的数据当作训练集，30%的数据当作测试集。因iris数据集为连续值，所以采用GaussianNB模型，训练后模型得分为0.9333333333。更多关于sklearn.naive_bayes的使用技巧可以访问<a href=\"https://link.zhihu.com/?target=http%3A//scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html%23sklearn.naive_bayes.GaussianNB\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">官方教程</a>。</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"kn\">from</span> <span class=\"nn\">sklearn.naive_bayes</span> <span class=\"k\">import</span> <span class=\"n\">GaussianNB</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.datasets</span> <span class=\"k\">import</span> <span class=\"n\">load_iris</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.model_selection</span> <span class=\"k\">import</span> <span class=\"n\">train_test_split</span>\n\n<span class=\"c1\">#load data</span>\n<span class=\"n\">iris</span><span class=\"o\">=</span><span class=\"n\">load_iris</span><span class=\"p\">()</span>\n<span class=\"n\">X</span><span class=\"o\">=</span><span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">data</span>\n<span class=\"n\">y</span><span class=\"o\">=</span><span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">target</span>\n<span class=\"n\">X_train</span><span class=\"p\">,</span><span class=\"n\">X_test</span><span class=\"p\">,</span><span class=\"n\">y_train</span><span class=\"p\">,</span><span class=\"n\">y_test</span><span class=\"o\">=</span><span class=\"n\">train_test_split</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span><span class=\"n\">y</span><span class=\"p\">,</span><span class=\"n\">test_size</span><span class=\"o\">=</span><span class=\"mf\">0.3</span><span class=\"p\">,</span><span class=\"n\">random_state</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n\n<span class=\"n\">mnb</span><span class=\"o\">=</span><span class=\"n\">GaussianNB</span><span class=\"p\">()</span>\n<span class=\"n\">mnb</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">,</span><span class=\"n\">y_train</span><span class=\"p\">)</span>\n\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">mnb</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">X_test</span><span class=\"p\">))</span>\n<span class=\"c1\"># [0 1 1 0 2 2 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 2 0 2 1 0 0 1 2 1 2 1 2 2 0 1</span>\n<span class=\"c1\">#  0 1 2 2 0 1 2 1]</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">y_test</span><span class=\"p\">)</span>\n<span class=\"c1\"># [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2 1 2 1 2 2 0 1</span>\n<span class=\"c1\">#  0 1 2 2 0 2 2 1]</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">mnb</span><span class=\"o\">.</span><span class=\"n\">score</span><span class=\"p\">(</span><span class=\"n\">X_test</span><span class=\"p\">,</span><span class=\"n\">y_test</span><span class=\"p\">))</span>\n<span class=\"c1\"># 0.933333333333</span></code></pre></div><h2>5.朴素贝叶斯优缺点</h2><h2>5.1优点</h2><ul><li>具有稳定的分类效率。</li><li>对缺失数据不敏感，算法也比较简单。</li><li>对小规模数据表现良好，能处理多分类任务，适合增量式训练。尤其是数据量超出内存后，我们可以一批批的去增量训练。</li></ul><h2>5.2缺点</h2><ul><li>对输入数据的表达形式比较敏感，需针对不同类型数据采用不同模型。</li><li>由于我们是使用数据加先验概率预测后验概率，所以分类决策存在一定的错误率。</li><li>假设各特征之间相互独立，但实际生活中往往不成立，因此对特征个数比较多或特征之间相关性比较大的数据来说，分类效果可能不是太好。</li></ul><h2>6.推广</h2><p>更多内容请关注公众号<b>谓之小一</b>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p><p><a href=\"https://link.zhihu.com/?target=http%3A//weixin.qq.com/r/Ty_4oDPE2W6mrXfD93pd\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">weixin.qq.com/r/Ty_4oDP</span><span class=\"invisible\">E2W6mrXfD93pd</span><span class=\"ellipsis\"></span></a> (二维码自动识别)</p><p>参考</p><blockquote><a href=\"https://link.zhihu.com/?target=https%3A//www.cnblogs.com/pinard/p/6069267.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">朴素贝叶斯算法原理小结 - 刘建平Pinard - 博客园</a></blockquote><p></p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "算法", 
                    "tagLink": "https://api.zhihu.com/topics/19553510"
                }, 
                {
                    "tag": "贝叶斯理论", 
                    "tagLink": "https://api.zhihu.com/topics/19632222"
                }
            ], 
            "comments": [
                {
                    "userName": "10先生", 
                    "userLink": "https://www.zhihu.com/people/fa80327fff1328bb30d9713d7676f027", 
                    "content": "写的很棒", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "谓之小一", 
                            "userLink": "https://www.zhihu.com/people/761dda159c5cec5fd926f761ab5f4ccf", 
                            "content": "<p>谢谢😁</p>", 
                            "likes": 0, 
                            "replyToAuthor": "10先生"
                        }
                    ]
                }, 
                {
                    "userName": "leshi", 
                    "userLink": "https://www.zhihu.com/people/80b392a719c457f1c8dcdf550912fa9b", 
                    "content": "其实在参数估计那一段有些不明白，这三种情况都是独立出现的吗？", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/36802527", 
            "userName": "谓之小一", 
            "userLink": "https://www.zhihu.com/people/761dda159c5cec5fd926f761ab5f4ccf", 
            "upvote": 11, 
            "title": "机器学习之K近邻(KNN)算法", 
            "content": "<h2>1.KNN简介</h2><p><b>K近邻(K-Nearest Neighbors, KNN)</b>算法既可处理分类问题，也可处理回归问题，其中分类和回归的主要区别在于最后做预测时的决策方式不同。KNN做<b>分类</b>预测时一般采用多数表决法，即训练集里和预测样本特征最近的K个样本，预测结果为里面有最多类别数的类别。KNN做<b>回归</b>预测时一般采用平均法，预测结果为最近的K个样本数据的平均值。其中KNN分类方法的思想对回归方法同样适用，因此本文主要讲解KNN分类问题，下面我们通过一个简单例子来了解下KNN算法流程。</p><p>如下图所示，我们想要知道绿色点要被决定赋予哪个类，是红色三角形还是蓝色正方形？我们利用KNN思想，如果假设<b>K=3</b>，选取三个距离最近的类别点，由于红色三角形所占比例为2/3，因此绿色点被赋予红色三角形类别。如果假设<b>K=5</b>，由于蓝色正方形所占比例为3/5，因此绿色点被赋予蓝色正方形类别。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-40167051d05d8123986882ec5e0e0b82_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"342\" data-rawheight=\"316\" class=\"content_image\" width=\"342\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;342&#39; height=&#39;316&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"342\" data-rawheight=\"316\" class=\"content_image lazy\" width=\"342\" data-actualsrc=\"https://pic3.zhimg.com/v2-40167051d05d8123986882ec5e0e0b82_b.jpg\"/></figure><p>从上面实例，我们可以总结下KNN算法过程</p><ol><li>计算测试数据与各个训练数据之间的距离。</li><li>按照距离的递增关系进行排序，选取距离最小的K个点。</li><li>确定前K个点所在类别的出现频率，返回前K个点中出现频率最高的类别作为测试数据的预测分类。</li></ol><p>从KNN算法流程中，我们也能够看出KNN算法三个重要特征，即距离度量方式、K值的选取和分类决策规则。</p><ul><li><b>距离度量方式：</b>KNN算法常用欧式距离度量方式，当然我们也可以采用其他距离度量方式，比如曼哈顿距离，相应公式如下所示。</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-a7820ec1af55f362fd5f4cf6477b5850_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1584\" data-rawheight=\"326\" class=\"origin_image zh-lightbox-thumb\" width=\"1584\" data-original=\"https://pic1.zhimg.com/v2-a7820ec1af55f362fd5f4cf6477b5850_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1584&#39; height=&#39;326&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1584\" data-rawheight=\"326\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1584\" data-original=\"https://pic1.zhimg.com/v2-a7820ec1af55f362fd5f4cf6477b5850_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-a7820ec1af55f362fd5f4cf6477b5850_b.jpg\"/></figure><ul><li><b>K值的选取：</b>KNN算法决策结果很大程度上取决于K值的选择。选择较小的K值相当于用较小领域中的训练实例进行预测，训练误差会减小，但同时整体模型变得复杂，容易过拟合。选择较大的K值相当于用较大领域中训练实例进行预测，可以减小泛化误差，但同时整体模型变得简单，预测误差会增大。</li><li><b>分类决策规则：</b>KNN分类决策规则经常使用我们前面提到的多数表决法，在此不再赘述。</li></ul><p>KNN要选取前K个最近的距离点，因此我们就要计算预测点与所有点之间的距离。但如果样本点达到几十万，样本特征有上千，那么KNN暴力计算距离的话，时间成本将会很高。因此暴力计算只适合少量样本的简单模型，那么有没有什么方法适用于大样本数据，有效降低距离计算成本呢？那是当然的，我们下面主要介绍KD树和球树方法。</p><h2>2.KD树原理</h2><p>KD树算法没有一开始就尝试对测试样本进行分类，而是先对训练集建模，建立的模型就是KD树，建立好模型之后再对测试集做预测。KD树就是K个特征维度的树，注意KD树中K和KNN中的K意思不同。KD树中的K代表样本特征的维数，为了防止混淆，后面我们称KD树中特征维数为n。KD树可以有效减少最近邻搜索次数，主要分为建树、搜索最近邻、预测步骤，下面我们对KD树进行详细讲解。</p><h2>2.1KD树建立</h2><p>下述为KD树构建步骤，包括寻找划分特征、确定划分点、确定左子空间和右子空间、递归构建KD树。</p><ol><li><b>寻找划分特征：</b>KD树是从m个样本的n维特征中，分别计算n个特征取值的方差，用方差最大的第k维特征nk来作为根节点。</li><li><b>确定划分点：</b>选择特征nk的中位数nkv所对应的样本作为划分点。</li><li><b>确定左子空间和右子空间：</b>对于所有第k维特征取值小于nkv的样本划入左子树，所有第k维特征取值大于nkv的样本划入右子树。</li><li><b>递归构建KD树：</b>对于左子树和右子树，采用和上述同样的方法来找方差最大的特征生成新节点，递归的构建KD树。</li></ol><p>我们举例来说明KD树构建过程，假如有二维样本6个，分别为{(2,3)，(5,4)，(9,6)，(4,7)，(8,1)，(7,2)}，KD树过程构建过程如下</p><ol><li><b>寻找划分特征：</b>6个数据点在x,y维度上方差分别为6.97,5.37，x轴上方差更大，用第1维度特征建树。</li><li><b>确定划分点：</b>根据x维度上的值将数据排序，6个数据中x的中值为7，所以划分点数据为(7,2)，该节点的分割超平面便是x=7直线。</li><li><b>确定左子空间和右子空间：</b>分割超平面x=7将空间分为两部分。x&lt;=7的部分为左子空间，包含节点为{(2,3)，(5,4)，(4,7)}。x&gt;7的部分为右子空间，包含节点为{(9,6)，(8,1)}。</li><li><b>递归构建KD树：</b>用同样的方法划分左子树{(2,3)，(5,4)，(4,7)}和右子树{(9,6)，(8,1)}，最终得到KD树。</li></ol><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-542ea87cbebc16c286b27e76f519b4c6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"389\" data-rawheight=\"249\" class=\"content_image\" width=\"389\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;389&#39; height=&#39;249&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"389\" data-rawheight=\"249\" class=\"content_image lazy\" width=\"389\" data-actualsrc=\"https://pic3.zhimg.com/v2-542ea87cbebc16c286b27e76f519b4c6_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-f1615108ab9d697aaa0182255ae43e89_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"218\" data-rawheight=\"220\" class=\"content_image\" width=\"218\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;218&#39; height=&#39;220&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"218\" data-rawheight=\"220\" class=\"content_image lazy\" width=\"218\" data-actualsrc=\"https://pic2.zhimg.com/v2-f1615108ab9d697aaa0182255ae43e89_b.jpg\"/></figure><h2>2.2KD树搜索最近邻</h2><p>当我们生成KD树后，就可以预测测试样本集里面的样本目标点。</p><ol><li><b>二叉搜索：</b>对于目标点，通过二叉搜索，能够很快在KD树里面找到包含目标点的<b>叶子节点</b>。</li><li><b>回溯：</b>为找到最近邻，还需要进行<b>回溯</b>操作，算法沿搜索路径反向查找是否有距离查询点更近的数据点。以目标点为圆心，目标点到叶子节点的距离为半径，得到一个超球体，最邻近点一定在这个超球体内部。</li><li><b>更新最近邻：</b>返回叶子节点的父节点，检查另一叶子节点包含的超矩形体是否和超球体相交，如果相交就到这个子节点中寻找是否有更近的最近邻，有的话就更新最近邻。如果不相交就直接返回父节点的父节点，在另一子树继续搜索最近邻。当回溯到根节点时，算法结束，此时保存的最近邻节点就是最终的最近邻。</li></ol><p>为方便理解上述过程，我们利用<b>2.1建立的KD树</b>来寻找(2,4.5)的最近邻。</p><ol><li><b>二叉搜索：</b>首先从(7,2)节点查找到(5,4)节点。由于目标点y=4.5，同时分割超平面为y=4，因此进入右子空间(4,7)进行查找，形成搜索路径{(7,2)-&gt;(5,4)-&gt;(4,7)}。</li><li><b>回溯：</b>节点(4,7)与目标查找点距离为3.202，回溯到父节点(5,4)与目标查找点之间距离为3.041，所以(5,4)为查询点的最近邻。以目标点(2,4.5)为圆心，以3.041为半径作圆，最近邻一定在超球体内部。</li><li><b>更新最近邻：</b>该圆和y = 4超平面交割，所以需要进入(5,4)左子空间进行查找，将(2,3)节点加入搜索路径{(7,2)-&gt;(2,3)}。回溯至(2,3)叶子节点，(2,4.5)到(2,3)的距离比到(5,4)要近，所以最近邻点更新为(2,3)，最近距离更新为1.5。回溯至(7,2)，以(2,4.5)为圆心，1.5为半径作圆，发现并不和x = 7分割超平面交割，至此搜索路径回溯完成，完成更新最近邻操作，返回最近邻点(2,3)。</li></ol><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-57ca07ceeb9f50f9c919647905efecdc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"559\" data-rawheight=\"258\" class=\"origin_image zh-lightbox-thumb\" width=\"559\" data-original=\"https://pic1.zhimg.com/v2-57ca07ceeb9f50f9c919647905efecdc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;559&#39; height=&#39;258&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"559\" data-rawheight=\"258\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"559\" data-original=\"https://pic1.zhimg.com/v2-57ca07ceeb9f50f9c919647905efecdc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-57ca07ceeb9f50f9c919647905efecdc_b.jpg\"/></figure><h2>2.3KD树预测</h2><p>根据KD树搜索最近邻的方法，我们能够得到第一个最近邻数据点，然后把它置为已选。然后忽略置为已选的样本，重新选择最近邻，这样运行K次，就能得到K个最近邻。如果是KNN分类，根据多数表决法，预测结果为K个最近邻类别中有最多类别数的类别。如果是KNN回归，根据平均法，预测结果为K个最近邻样本输出的平均值。</p><h2>3.球树原理</h2><p>KD树算法能够提高KNN搜索效率，但在某些时候效率并不高，比如处理不均匀分布的数据集时。如下图所示，如果黑色的实例点离目标点(星点)再远一点，那么虚线会像红线那样扩大，导致与左上方矩形的右下角相交。既然相交那就要检查左上方矩形，而实际上最近的点离目标点(星点)很近，检查左上方矩形区域已是多余。因此KD树把二维平面划分成矩形会带来无效搜索的问题。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-80fb72bccd057c027cfdaccc38f9a552_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"276\" data-rawheight=\"253\" class=\"content_image\" width=\"276\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;276&#39; height=&#39;253&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"276\" data-rawheight=\"253\" class=\"content_image lazy\" width=\"276\" data-actualsrc=\"https://pic3.zhimg.com/v2-80fb72bccd057c027cfdaccc38f9a552_b.jpg\"/></figure><p>为优化超矩形体带来的搜索效率问题，我们在此介绍球树算法来进一步提高最近邻搜索效率。</p><h2>3.1球树建立</h2><p>球树的每个分割块都是超球体，而不像KD树中的超矩形体，这样在做最近邻搜索是可以避免无效搜索，下面我们介绍球树构建过程</p><ol><li><b>构建超球体：</b>超球体是可以包含所有样本的最小球体。</li><li><b>划分子超球体：</b>从超球体中选择一个离超球体中心最远的点，然后选择第二个点离第一个点最远，将球中所有的点分配到离这两个聚类中心最近的一个。然后计算每个聚类的中心，以及聚类能够包含它所有数据点所需的最小半径，这样我们便得到两个子超球体，和KD树中的左右子树对应。</li><li><b>递归：</b>对上述两个子超球体，递归执行步骤2，最终得到球树。</li></ol><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-df1d6ea0b739c05332709faa4580e361_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"444\" data-rawheight=\"286\" class=\"origin_image zh-lightbox-thumb\" width=\"444\" data-original=\"https://pic2.zhimg.com/v2-df1d6ea0b739c05332709faa4580e361_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;444&#39; height=&#39;286&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"444\" data-rawheight=\"286\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"444\" data-original=\"https://pic2.zhimg.com/v2-df1d6ea0b739c05332709faa4580e361_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-df1d6ea0b739c05332709faa4580e361_b.jpg\"/></figure><h2>3.2球树搜索最近邻</h2><p>KD树在搜索路径优化时使用的是两点之间的距离来判断，而球树使用的是两边之和大于第三边来判断。相对来说球树的判断更加复杂，但却避免一些无效的搜索，下述为球树搜索最近邻过程。</p><ol><li>自上而下贯穿整棵树找出包含目标点所在的叶子，并在这个球里找出与目标点最邻近的点，这将确定目标点距离最邻近点的上限值。</li><li>然后和KD树查找相同，检查兄弟结点，如果目标点到兄弟结点中心的距离超过兄弟结点的半径与当前的上限值之和，那么兄弟结点里不可能存在一个更近的点。否则进一步检查位于兄弟结点以下的子树。</li><li>检查完兄弟节点后，向父节点回溯，继续搜索最小邻近值。当回溯到根节点时，此时的最小邻近值就是最终的搜索结果。</li></ol><h2>3.3球树预测</h2><p>根据球树搜索最近邻的方法，我们能够得到第一个最近邻数据点，然后把它置为已选。然后忽略置为已选的样本，重新选择最近邻，这样运行K次，就能得到K个最近邻。如果是KNN分类，根据多数表决法，预测结果为K个最近邻类别中有最多类别数的类别。如果是KNN回归，根据平均法，预测结果为K个最近邻样本输出的平均值。</p><h2>4.KNN算法扩展</h2><p>有时我们会遇到样本中某个类别数的样本非常少，甚至少于我们实现定义的K，这将导致稀有样本在找K个最近邻的时候会把距离较远的无关样本考虑进来，进而导致预测不准确。为解决此类问题，我们先设定最近邻的一个最大距离，也就是说，我们在一定范围内搜索最近邻，这个距离称为限定半径。</p><h2>5.Sklearn实现KNN算法</h2><p>下述代码是利用iris数据进行分类，我们经常需要通过改变参数来让模型达到分类结果，具体参数设置可参考<a href=\"https://link.zhihu.com/?target=http%3A//scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">sklearn官方教程</a>。</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"kn\">from</span> <span class=\"nn\">sklearn.neighbors</span> <span class=\"k\">import</span> <span class=\"n\">KNeighborsClassifier</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.datasets</span> <span class=\"k\">import</span> <span class=\"n\">load_iris</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.model_selection</span> <span class=\"k\">import</span> <span class=\"n\">train_test_split</span>\n\n<span class=\"c1\">#load iris data</span>\n<span class=\"n\">iris</span><span class=\"o\">=</span><span class=\"n\">load_iris</span><span class=\"p\">()</span>\n<span class=\"n\">X</span><span class=\"o\">=</span><span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">data</span>\n<span class=\"n\">y</span><span class=\"o\">=</span><span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">target</span>\n<span class=\"n\">X_train</span><span class=\"p\">,</span><span class=\"n\">X_test</span><span class=\"p\">,</span><span class=\"n\">y_train</span><span class=\"p\">,</span><span class=\"n\">y_test</span><span class=\"o\">=</span><span class=\"n\">train_test_split</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span><span class=\"n\">y</span><span class=\"p\">,</span><span class=\"n\">test_size</span><span class=\"o\">=</span><span class=\"mf\">0.3</span><span class=\"p\">,</span><span class=\"n\">random_state</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n\n<span class=\"n\">knn</span><span class=\"o\">=</span><span class=\"n\">KNeighborsClassifier</span><span class=\"p\">(</span><span class=\"n\">algorithm</span><span class=\"o\">=</span><span class=\"s1\">&#39;kd_tree&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">knn</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">,</span><span class=\"n\">y_train</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">knn</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">X_test</span><span class=\"p\">))</span>\n<span class=\"c1\"># [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2 1 2 1 2 2 0 1</span>\n<span class=\"c1\">#  0 1 2 2 0 1 2 1]</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">y_test</span><span class=\"p\">)</span>\n<span class=\"c1\"># [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2 1 2 1 2 2 0 1</span>\n<span class=\"c1\">#  0 1 2 2 0 2 2 1]</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">knn</span><span class=\"o\">.</span><span class=\"n\">score</span><span class=\"p\">(</span><span class=\"n\">X_test</span><span class=\"p\">,</span><span class=\"n\">y_test</span><span class=\"p\">))</span>\n<span class=\"c1\"># 0.977777777778</span></code></pre></div><h2>6.KNN优缺点</h2><h2>6.1优点</h2><ul><li>即可处理分类也可处理回归问题。</li><li>对数据没有假设，准确度高，对异常点不敏感。</li><li>比较适合样本容量大的类域进行自动分类，对样本容量较小的类域容易产生误分。</li><li>主要靠周围有限的邻近样本进行分类或回归，比较适合类域交叉或重叠较多的待分样本集。</li></ul><h2>6.2缺点</h2><ul><li>计算量大，尤其是特征维数较多时候。</li><li>样本不平衡时，对稀有类别的预测准确率低。</li><li>KD树、球树之类的模型建立时需要大量的内存。</li><li>使用懒惰学习方法，基本上不学习，导致预测时速度较慢。</li></ul><h2>7.推广</h2><p>更多内容请关注公众号<b>谓之小一</b>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p><p><a href=\"https://link.zhihu.com/?target=http%3A//weixin.qq.com/r/Ty_4oDPE2W6mrXfD93pd\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">weixin.qq.com/r/Ty_4oDP</span><span class=\"invisible\">E2W6mrXfD93pd</span><span class=\"ellipsis\"></span></a> (二维码自动识别)</p><p>参考</p><blockquote><a href=\"https://link.zhihu.com/?target=http%3A//www.cnblogs.com/pinard/p/6061661.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">刘建平Pinard_K近邻法(KNN)原理小结</a><br/><a href=\"https://link.zhihu.com/?target=https%3A//www.cnblogs.com/ybjourney/p/4702562.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Yabea_K-近邻(KNN)算法</a></blockquote><p></p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "算法", 
                    "tagLink": "https://api.zhihu.com/topics/19553510"
                }, 
                {
                    "tag": "k近邻法", 
                    "tagLink": "https://api.zhihu.com/topics/20050158"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/36771062", 
            "userName": "谓之小一", 
            "userLink": "https://www.zhihu.com/people/761dda159c5cec5fd926f761ab5f4ccf", 
            "upvote": 10, 
            "title": "机器学习之K均值(K-Means)算法", 
            "content": "<blockquote>由于较多公式，所以我将部分内容转为图片进行上传，请见谅。清晰版请访问<a href=\"https://link.zhihu.com/?target=https%3A//weizhixiaoyi.com\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">weizhixiaoyi.com</span><span class=\"invisible\"></span></a>查看，你也可以关注我公众号<b>谓之小一</b>，后台直接向我要pdf版本，如有相关问题可公众号后台询问，随时回答。</blockquote><h2>1.K-Means简介</h2><p><b>K均值(K-Means)</b>算法是无监督的聚类方法，实现起来比较简单，聚类效果也比较好，因此应用很广泛。K-Means算法针对不同应用场景，有不同方面的改进。我们从最传统的K-Means算法讲起，然后在此基础上介绍初始化质心优化<b>K-Means++</b>算法，距离计算优化<b>Elkan K-Means</b>算法和大样本情况下<b>Mini Batch K-Means</b>算法。</p><p>K-Means算法的思想很简单，对于给定的样本集，按照样本之间的距离大小，将样本集划分为K个簇。让簇内的点尽可能紧密的连在一起，而让簇间的距离尽量的大，下面我们引入K-Means目标函数。</p><p>假设样本集输入变量为(x1,x2,x3,…,xm)，样本集划分为K个簇(C1,C2,C3,…,Ck)，则我们的目标是最小化平方误差E。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-f63d2db151a0bd6df3fb7ed1358a4124_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1592\" data-rawheight=\"154\" class=\"origin_image zh-lightbox-thumb\" width=\"1592\" data-original=\"https://pic1.zhimg.com/v2-f63d2db151a0bd6df3fb7ed1358a4124_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1592&#39; height=&#39;154&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1592\" data-rawheight=\"154\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1592\" data-original=\"https://pic1.zhimg.com/v2-f63d2db151a0bd6df3fb7ed1358a4124_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-f63d2db151a0bd6df3fb7ed1358a4124_b.jpg\"/></figure><p>其中μi是簇Ci的均值向量，也可称作质心，表达式为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-68dc97f9c55f63142ca1dd2727ecc0dd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1608\" data-rawheight=\"138\" class=\"origin_image zh-lightbox-thumb\" width=\"1608\" data-original=\"https://pic2.zhimg.com/v2-68dc97f9c55f63142ca1dd2727ecc0dd_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1608&#39; height=&#39;138&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1608\" data-rawheight=\"138\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1608\" data-original=\"https://pic2.zhimg.com/v2-68dc97f9c55f63142ca1dd2727ecc0dd_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-68dc97f9c55f63142ca1dd2727ecc0dd_b.jpg\"/></figure><p>如果直接求解上述最小值的话，那么为NP Hard问题，因此K-Means算法采用启发式的迭代方法。下面我们通过一个简单聚类来介绍K-Means算法迭代过程。</p><ul><li>如图(a)所示：表示初始化数据集。</li><li>如图(b)所示：假设K=2，随机选择两个点作为类别质心，分别为图中的红色质心和蓝色质心。</li><li>如图(c)所示：分别求样本点xi到这两个质心的距离，并标记每个样本点的类别为距离质心最近的类别。划分得到两个簇C1和C2，完成一次迭代。</li><li>如图(d)所示：对标记为红色的点和蓝色的点分别求新的质心。</li><li>如图(e)所示：重复图(c)(d)过程，标记每个样本点的类别为距离质心最近的类别，重新划分得到两个簇C1和C2。</li><li>如图(f)所示：直到质心不再改变后完成迭代，最终得到两个簇C1和C2。</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-d72b1ba5b1abebd970520642290e9451_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"575\" data-rawheight=\"391\" class=\"origin_image zh-lightbox-thumb\" width=\"575\" data-original=\"https://pic2.zhimg.com/v2-d72b1ba5b1abebd970520642290e9451_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;575&#39; height=&#39;391&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"575\" data-rawheight=\"391\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"575\" data-original=\"https://pic2.zhimg.com/v2-d72b1ba5b1abebd970520642290e9451_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-d72b1ba5b1abebd970520642290e9451_b.jpg\"/></figure><h2>2.K-Means算法流程</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-c69fe38a69331ea21e5e31be2234433f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1596\" data-rawheight=\"904\" class=\"origin_image zh-lightbox-thumb\" width=\"1596\" data-original=\"https://pic4.zhimg.com/v2-c69fe38a69331ea21e5e31be2234433f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1596&#39; height=&#39;904&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1596\" data-rawheight=\"904\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1596\" data-original=\"https://pic4.zhimg.com/v2-c69fe38a69331ea21e5e31be2234433f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-c69fe38a69331ea21e5e31be2234433f_b.jpg\"/></figure><p>对于K-Means算法，首先要注意K值的选择和K个初始化质心的选择。</p><ul><li><b>对于K值的选择:</b>我们可以通过对数据的先验经验选择合适的K值，如果没有先验条件的话，还可以通过交叉验证选择合适的K值。</li><li><b>对于K个初始化质心:</b>由于我们采用启发式迭代方法，K个初始化质心的位置选择对最后的聚类结果和运行时间都有较大的影响，最好选择的K个质心不要离得太近。</li></ul><h2>3.初始化优化K-Means++</h2><p>如果是完全随机的选择， 算法的收敛可能很慢。我们在此介绍K-Means++算法，针对随机初始化质心进行优化，具体算法流程如下所示。</p><ul><li>从输入的数据点集合中随机选择一个点作为第一个聚类中心μ1。</li><li>对于数据集中的每个点xi，计算与他最近的聚类中心距离。</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-899ac8008a44dc59deeba95a80d8ed1a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1608\" data-rawheight=\"144\" class=\"origin_image zh-lightbox-thumb\" width=\"1608\" data-original=\"https://pic3.zhimg.com/v2-899ac8008a44dc59deeba95a80d8ed1a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1608&#39; height=&#39;144&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1608\" data-rawheight=\"144\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1608\" data-original=\"https://pic3.zhimg.com/v2-899ac8008a44dc59deeba95a80d8ed1a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-899ac8008a44dc59deeba95a80d8ed1a_b.jpg\"/></figure><ul><li>选择一个数据点作为新的聚类中心，其中D(x)较大的点被选作新的聚类中心的概率较大。</li><li>重复上述两步，直到选择出K个聚类中心。然后利用这K个质心来作为初始化质心去运行传统K-Means算法。</li></ul><h2>4.距离计算优化Elkan K-Means算法</h2><p>传统K-Means算法中，我们每次迭代时都要计算所有样本点到所有质心之间的距离，那么有没有什么方法来减少计算次数呢? Elkan K-Means算法提出利用两边之和大于第三边、两边之差小于第三边的三角形特性来减少距离的计算。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-99747991d53d343fdfc3a2507894b043_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1594\" data-rawheight=\"256\" class=\"origin_image zh-lightbox-thumb\" width=\"1594\" data-original=\"https://pic4.zhimg.com/v2-99747991d53d343fdfc3a2507894b043_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1594&#39; height=&#39;256&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1594\" data-rawheight=\"256\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1594\" data-original=\"https://pic4.zhimg.com/v2-99747991d53d343fdfc3a2507894b043_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-99747991d53d343fdfc3a2507894b043_b.jpg\"/></figure><p>Elkan K-Means迭代速度比传统K-Means算法迭代速度有较大提高，但如果我们的样本特征是稀疏的，或者有缺失值的话，此种方法便不再使用。</p><h2>5.大样本优化Mini Batch K-Means算法</h2><p>传统的K-Means算法中需要计算所有样本点到所有质心的距离，计算复杂度较高。如果样本量非常大的情况下，比如数据量达到10万，特征在100以上，此时用传统K-Means算法非常耗时。故此针对大样本情况下采用Mini Batch K-Means算法。</p><p>Mini Batch K-Means采用无放回随机采样的方法从样本集中选取部分数据，然后用选取的数据进行传统的K-Means算法训练。然后进行迭代并更新质心，直到质心稳定或达到指定的迭代次数。</p><p>Mini Batch K-Means可以避免样本量太大带来的计算问题，算法收敛速度也能够加快，当然带来的代价就是我们的聚类精确度降低。为增加算法的准确性，我们可以多训练几次Mini Batch K-Means算法，用不同的随机采样集来得到聚类簇，选择其中最优的聚类簇。</p><h2>6.Sklearn实现K-Means算法</h2><p>我们经常需要通过改变参数来让模型达到聚类结果，具体参数设置可参考<a href=\"https://link.zhihu.com/?target=http%3A//scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">sklearn官方教程</a>。</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"kn\">from</span> <span class=\"nn\">sklearn.cluster</span> <span class=\"k\">import</span> <span class=\"n\">KMeans</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.datasets</span> <span class=\"k\">import</span> <span class=\"n\">load_iris</span>\n<span class=\"kn\">import</span> <span class=\"nn\">matplotlib.pyplot</span> <span class=\"k\">as</span> <span class=\"nn\">plt</span>\n\n<span class=\"c1\">#load iris</span>\n<span class=\"n\">iris</span><span class=\"o\">=</span><span class=\"n\">load_iris</span><span class=\"p\">()</span>\n<span class=\"n\">X</span><span class=\"o\">=</span><span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">[:,:</span><span class=\"mi\">2</span><span class=\"p\">]</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span>\n<span class=\"c1\">#150,2</span>\n\n<span class=\"c1\">#plot data</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">figure</span><span class=\"p\">()</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">scatter</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">[:,</span><span class=\"mi\">0</span><span class=\"p\">],</span><span class=\"n\">X</span><span class=\"p\">[:,</span><span class=\"mi\">1</span><span class=\"p\">],</span><span class=\"n\">c</span><span class=\"o\">=</span><span class=\"s1\">&#39;blue&#39;</span><span class=\"p\">,</span>\n            <span class=\"n\">marker</span><span class=\"o\">=</span><span class=\"s1\">&#39;o&#39;</span><span class=\"p\">,</span><span class=\"n\">label</span><span class=\"o\">=</span><span class=\"s1\">&#39;point&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">legend</span><span class=\"p\">(</span><span class=\"n\">loc</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span></code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-89eb7a23c85150e7db11f4f0ea17e551_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"912\" data-rawheight=\"674\" class=\"origin_image zh-lightbox-thumb\" width=\"912\" data-original=\"https://pic2.zhimg.com/v2-89eb7a23c85150e7db11f4f0ea17e551_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;912&#39; height=&#39;674&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"912\" data-rawheight=\"674\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"912\" data-original=\"https://pic2.zhimg.com/v2-89eb7a23c85150e7db11f4f0ea17e551_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-89eb7a23c85150e7db11f4f0ea17e551_b.jpg\"/></figure><div class=\"highlight\"><pre><code class=\"language-text\"># fit data\nkmeans=KMeans(n_clusters=3)\nkmeans.fit(X)\nlabel_pred=kmeans.labels_\n\n#plot answer\nplt.figure()\nx0 = X[label_pred == 0]\nx1 = X[label_pred == 1]\nx2 = X[label_pred == 2]\nplt.scatter(x0[:, 0], x0[:, 1], c = &#34;red&#34;,\n            marker=&#39;o&#39;, label=&#39;label0&#39;)\nplt.scatter(x1[:, 0], x1[:, 1], c = &#34;green&#34;,\n            marker=&#39;*&#39;, label=&#39;label1&#39;)\nplt.scatter(x2[:, 0], x2[:, 1], c = &#34;blue&#34;,\n            marker=&#39;+&#39;, label=&#39;label2&#39;)\nplt.legend(loc=2)\nplt.show()</code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-14778fef5ec25bd9a861455f7882c314_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"900\" data-rawheight=\"668\" class=\"origin_image zh-lightbox-thumb\" width=\"900\" data-original=\"https://pic1.zhimg.com/v2-14778fef5ec25bd9a861455f7882c314_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;900&#39; height=&#39;668&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"900\" data-rawheight=\"668\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"900\" data-original=\"https://pic1.zhimg.com/v2-14778fef5ec25bd9a861455f7882c314_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-14778fef5ec25bd9a861455f7882c314_b.jpg\"/></figure><h2>7.K-Means算法优缺点</h2><h2>7.1优点</h2><ul><li>聚类效果较优。</li><li>原理简单，实现容易，收敛速度快。</li><li>需要调整的参数较少，通常只需要调整簇数K。</li></ul><h2>7.2缺点</h2><ul><li>K值选取不好把握。</li><li>对噪音和异常点比较敏感。</li><li>采用迭代方法，得到的结果是局部最优。</li><li>如果各隐含类别的数据不平衡，则聚类效果不佳。</li></ul><h2>8.推广</h2><p>更多内容请关注公众号<b>谓之小一</b>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p><p><a href=\"https://link.zhihu.com/?target=http%3A//weixin.qq.com/r/Ty_4oDPE2W6mrXfD93pd\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">weixin.qq.com/r/Ty_4oDP</span><span class=\"invisible\">E2W6mrXfD93pd</span><span class=\"ellipsis\"></span></a> (二维码自动识别)</p><p>参考</p><ul><li><a href=\"https://link.zhihu.com/?target=http%3A//www.cnblogs.com/pinard/p/6164214.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">刘建平Pinard_K-Means聚类算法原理</a></li></ul>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "算法", 
                    "tagLink": "https://api.zhihu.com/topics/19553510"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/36695691", 
            "userName": "谓之小一", 
            "userLink": "https://www.zhihu.com/people/761dda159c5cec5fd926f761ab5f4ccf", 
            "upvote": 1, 
            "title": "机器学习之最大期望(EM)算法", 
            "content": "<blockquote>由于较多公式，所以我将部分内容转为图片进行上传，请见谅。清晰版请访问<a href=\"https://link.zhihu.com/?target=https%3A//weizhixiaoyi.com\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">weizhixiaoyi.com</span><span class=\"invisible\"></span></a>查看，你也可以关注我公众号<b>谓之小一</b>，后台直接向我要pdf版本，如有相关问题可公众号后台询问，随时回答。</blockquote><h2>1.EM算法简介</h2><p><b>最大期望(Expectation Maximum)算法</b>是一种迭代优化算法，其计算方法是每次迭代分为<b>期望(E)步</b>和<b>最大(M)步</b>。我们先看下最大期望算法能够解决什么样的问题。</p><p>假如班级里有50个男生和50个女生，且男生站左，女生站右。我们假定男生和女生的身高分布分别服从正态分布。这时我们用极大似然法，分别通过这50个男生和50个女生的样本来估计这两个正态分布的参数，便可知道男女身高分布的情况。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-2a31ef0cb2dab6edfd65070024a320b6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1588\" data-rawheight=\"116\" class=\"origin_image zh-lightbox-thumb\" width=\"1588\" data-original=\"https://pic3.zhimg.com/v2-2a31ef0cb2dab6edfd65070024a320b6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1588&#39; height=&#39;116&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1588\" data-rawheight=\"116\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1588\" data-original=\"https://pic3.zhimg.com/v2-2a31ef0cb2dab6edfd65070024a320b6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-2a31ef0cb2dab6edfd65070024a320b6_b.jpg\"/></figure><p>但我们面对下类问题如何解决呢？就是这50个男生和50个女生混在一起，我们拥有100个人的身高数据，但却不知道这100个同学中的每个人是男生还是女生。通常来说，我们只有知道了精确的男女身高的正态分布参数才能知道每位同学更有可能是男生还是女生，从另一方面来说我们只有知道每个人是男生还是女生才能尽可能准确的估计男女生各自身高的正态分布参数。但现在两者都不知道，如何去估计呢？ </p><p>EM算法表示我们可以用迭代的方法来解决问题。我们先假设男生身高和女生身高分布的参数(初始值)，然后根据这些参数去判断每个人是男生还是女生，之后根据标注后的样本反过来重新估计这些参数。多次重复上述过程，直到稳定。这样说还是有点抽象，我们先从抛硬币实例来讲解EM算法流程，然后再讲解具体数学推导和原理。</p><h2>2.EM算法实例</h2><p>假如现在我们有两枚硬币1和2，随机抛掷后面朝上概率分别为P1，P2。为了估计两硬币概率，我们做如下实验，每次取一枚硬币，连掷5次后得到如下结果。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-e1cd1e561b12f18e72eedc1fe2e88190_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1604\" data-rawheight=\"484\" class=\"origin_image zh-lightbox-thumb\" width=\"1604\" data-original=\"https://pic1.zhimg.com/v2-e1cd1e561b12f18e72eedc1fe2e88190_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1604&#39; height=&#39;484&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1604\" data-rawheight=\"484\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1604\" data-original=\"https://pic1.zhimg.com/v2-e1cd1e561b12f18e72eedc1fe2e88190_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-e1cd1e561b12f18e72eedc1fe2e88190_b.jpg\"/></figure><p>我们可以很方便的估计出硬币1概率P1=0.4，硬币2概率P2=0.5。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-1ffa22dc64e7b391432dd6f6ded6c0e3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1586\" data-rawheight=\"264\" class=\"origin_image zh-lightbox-thumb\" width=\"1586\" data-original=\"https://pic4.zhimg.com/v2-1ffa22dc64e7b391432dd6f6ded6c0e3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1586&#39; height=&#39;264&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1586\" data-rawheight=\"264\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1586\" data-original=\"https://pic4.zhimg.com/v2-1ffa22dc64e7b391432dd6f6ded6c0e3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-1ffa22dc64e7b391432dd6f6ded6c0e3_b.jpg\"/></figure><p>下面我们增加问题难度。如果并不知道每次投掷时所使用的硬币标记，那么如何估计P1和P2呢?</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-a3902a0799f68fa68ad97bd36947f2b1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1606\" data-rawheight=\"480\" class=\"origin_image zh-lightbox-thumb\" width=\"1606\" data-original=\"https://pic2.zhimg.com/v2-a3902a0799f68fa68ad97bd36947f2b1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1606&#39; height=&#39;480&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1606\" data-rawheight=\"480\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1606\" data-original=\"https://pic2.zhimg.com/v2-a3902a0799f68fa68ad97bd36947f2b1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-a3902a0799f68fa68ad97bd36947f2b1_b.jpg\"/></figure><p>此时我们加入<b>隐含变量z</b>，可以把它认为是一个5维的向量(z1,z2,z3,z4,z5)，代表每次投掷时所使用的硬币。比如z1就代表第一轮投掷时所使用的是硬币1还是2，我们必须先估计出z，然后才能进一步估计P1和P2。</p><p>我们先随机初始化一个P1和P2，用它来估计z，然后基于z按照最大似然概率方法去估计新的P1和P2。如果估计出的P1和P2与我们初始化的P1和P2一样，说明P1和P2很有可能就是真实的值。如果新估计出来的P1和P2和我们初始化时的值差别很大，那么我们继续用新的P1和P2迭代，直到收敛。</p><p>例如假设P1=0.2和P2=0.7，然后我们看看第一轮投掷的最可能是哪个硬币。如果是硬币1，得出3正2反的概率为0.2*0.2*0.2*0.8*0.8=0.00512，如果是硬币2，得出3正2反的概率为0.7*0.7*0.7*0.3*0.3=0.03087。然后依次求出其他4轮中的相应概率，接下来便可根据最大似然方法得到每轮中最有可能的硬币。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-01ce7212d6dedb71974619f0909dd47f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1604\" data-rawheight=\"478\" class=\"origin_image zh-lightbox-thumb\" width=\"1604\" data-original=\"https://pic4.zhimg.com/v2-01ce7212d6dedb71974619f0909dd47f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1604&#39; height=&#39;478&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1604\" data-rawheight=\"478\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1604\" data-original=\"https://pic4.zhimg.com/v2-01ce7212d6dedb71974619f0909dd47f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-01ce7212d6dedb71974619f0909dd47f_b.jpg\"/></figure><p>我们把上面的值作为z的估计值(2,1,1,2,1)，然后按照最大似然概率方法来估计新的P1和P2。得到</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-d0dda67e986bbb25e2ca731531b16e7c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1584\" data-rawheight=\"268\" class=\"origin_image zh-lightbox-thumb\" width=\"1584\" data-original=\"https://pic1.zhimg.com/v2-d0dda67e986bbb25e2ca731531b16e7c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1584&#39; height=&#39;268&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1584\" data-rawheight=\"268\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1584\" data-original=\"https://pic1.zhimg.com/v2-d0dda67e986bbb25e2ca731531b16e7c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-d0dda67e986bbb25e2ca731531b16e7c_b.jpg\"/></figure><p>P1和P2的最大似然估计是0.4和0.5，那么对比下我们初识化时的P1和P2。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-ca5b805b12c45b0638096760cdef03d1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1604\" data-rawheight=\"178\" class=\"origin_image zh-lightbox-thumb\" width=\"1604\" data-original=\"https://pic2.zhimg.com/v2-ca5b805b12c45b0638096760cdef03d1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1604&#39; height=&#39;178&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1604\" data-rawheight=\"178\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1604\" data-original=\"https://pic2.zhimg.com/v2-ca5b805b12c45b0638096760cdef03d1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-ca5b805b12c45b0638096760cdef03d1_b.jpg\"/></figure><p>可以预估，我们继续按照上面思路，用估计出的P1和P2再来估计z，再用z来估计新的P1和P2，反复迭代下去，可以最终得到P1=0.4，P2=0.5。然后无论怎样迭代，P1和P2的值都会保持0.4和0.5不变，于是我们就找到P1和P2的最大似然估计。</p><p>上面我们用最大似然方法估计出z值，然后再用z值按照最大似然概率方法估计新的P1和P2。也就是说，我们使用了最有可能的z值，而不是所有的z值。如果考虑所有可能的z值，对每一个z值都估计出新的P1和P2，将每一个z值概率大小作为权重，将所有新的P1和P2分别加权相加，这样估计出的P1和P2是否会更优呢?</p><p>但所有的z值共有2^5=32种，我们是否进行32次估计呢？当然不是，我们利用期望来简化运算。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-c67807167657b72428537dca76d3c28e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1600\" data-rawheight=\"478\" class=\"origin_image zh-lightbox-thumb\" width=\"1600\" data-original=\"https://pic3.zhimg.com/v2-c67807167657b72428537dca76d3c28e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1600&#39; height=&#39;478&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1600\" data-rawheight=\"478\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1600\" data-original=\"https://pic3.zhimg.com/v2-c67807167657b72428537dca76d3c28e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-c67807167657b72428537dca76d3c28e_b.jpg\"/></figure><p>利用上面表格，我们可以算出每轮投掷种使用硬币1或者使用硬币2的概率。比如第一轮使用硬币1的概率</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-03366118343132027cf38600f09f85c2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1590\" data-rawheight=\"144\" class=\"origin_image zh-lightbox-thumb\" width=\"1590\" data-original=\"https://pic3.zhimg.com/v2-03366118343132027cf38600f09f85c2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1590&#39; height=&#39;144&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1590\" data-rawheight=\"144\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1590\" data-original=\"https://pic3.zhimg.com/v2-03366118343132027cf38600f09f85c2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-03366118343132027cf38600f09f85c2_b.jpg\"/></figure><p>相应的算出其他4轮的概率。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-d1d0b3c15b9863626f2eb80d8dd206c0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1598\" data-rawheight=\"472\" class=\"origin_image zh-lightbox-thumb\" width=\"1598\" data-original=\"https://pic1.zhimg.com/v2-d1d0b3c15b9863626f2eb80d8dd206c0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1598&#39; height=&#39;472&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1598\" data-rawheight=\"472\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1598\" data-original=\"https://pic1.zhimg.com/v2-d1d0b3c15b9863626f2eb80d8dd206c0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-d1d0b3c15b9863626f2eb80d8dd206c0_b.jpg\"/></figure><p>上表中表示期望值，例如0.86表示此轮中使用硬币2的概率是0.86。前面方法我们按照最大似然概率直接将第一轮估计为硬币2，此时我们更加严谨，只说有0.14的概率是硬币1，有0.86的概率是硬币2。这样我们在估计P1或者P2时，就可以用上全部的数据，而不是部分的数据。此步我们实际上是估计出z的概率分布，称为<b>E步</b>。</p><p>按照期望最大似然概率的法则来估计出新的P1和P2。以P1估计为例，第一轮的3正2反相当于有0.14*3=0.42的概率为正，有0.14*2的概率为反。然后依次计算出其他四轮。那么我们可以得到P1概率，可以看到改变了z的估计方法后，新估计出的P1要更加接近0.4，原因是我们使用了所有抛掷的数据，而不是部分的数据。此步中我们根据E步中求出z的概率分布，依据最大似然概率法则去估计P1和P2，称为<b>M步</b>。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-094c067ddf01e3ff8db558ef7f9787f9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1598\" data-rawheight=\"552\" class=\"origin_image zh-lightbox-thumb\" width=\"1598\" data-original=\"https://pic2.zhimg.com/v2-094c067ddf01e3ff8db558ef7f9787f9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1598&#39; height=&#39;552&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1598\" data-rawheight=\"552\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1598\" data-original=\"https://pic2.zhimg.com/v2-094c067ddf01e3ff8db558ef7f9787f9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-094c067ddf01e3ff8db558ef7f9787f9_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-7ad0b43f932a33b15b1282d07f3603a8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1580\" data-rawheight=\"140\" class=\"origin_image zh-lightbox-thumb\" width=\"1580\" data-original=\"https://pic1.zhimg.com/v2-7ad0b43f932a33b15b1282d07f3603a8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1580&#39; height=&#39;140&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1580\" data-rawheight=\"140\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1580\" data-original=\"https://pic1.zhimg.com/v2-7ad0b43f932a33b15b1282d07f3603a8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-7ad0b43f932a33b15b1282d07f3603a8_b.jpg\"/></figure><p>上面我们是通过迭代来得到P1和P2。但是我们同时想要知道，新估计出的P1和P2一定会更接近真实的P1和P2吗，能够收敛吗？迭代一定会收敛到真实的P1和P2吗？下面我们将从数学推导方法详解EM算法。</p><h2>3.EM算法推导</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-c8952f0eab3b4d1dfe9d294371309367_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1602\" data-rawheight=\"250\" class=\"origin_image zh-lightbox-thumb\" width=\"1602\" data-original=\"https://pic4.zhimg.com/v2-c8952f0eab3b4d1dfe9d294371309367_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1602&#39; height=&#39;250&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1602\" data-rawheight=\"250\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1602\" data-original=\"https://pic4.zhimg.com/v2-c8952f0eab3b4d1dfe9d294371309367_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-c8952f0eab3b4d1dfe9d294371309367_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-9727af27b8f7ece8263aad76085af17f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1600\" data-rawheight=\"234\" class=\"origin_image zh-lightbox-thumb\" width=\"1600\" data-original=\"https://pic4.zhimg.com/v2-9727af27b8f7ece8263aad76085af17f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1600&#39; height=&#39;234&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1600\" data-rawheight=\"234\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1600\" data-original=\"https://pic4.zhimg.com/v2-9727af27b8f7ece8263aad76085af17f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-9727af27b8f7ece8263aad76085af17f_b.jpg\"/></figure><p>上面方程是无法直接求出θ的，因此需要一些特殊技巧，在此我们引入<b>Jensen不等式</b>。</p><blockquote>设f是定义域为实数的函数，如果对于所有的实数X，f(X)的二次导数大于等于0，那么f是凸函数。相反，f(X)的二次导数小于0，那么f是凹函数。<br/><b>Jensen不等式定义：</b>如果f是凸函数,X是随机变量，那么E[f(X)]≥f(E(X))。相反，如果f式凹函数，X是随机变量，那么E[f(X)]≤f(E[X])。当且仅当X是常量时，上式取等号，其中E[X]表示X的期望。</blockquote><p>我们再回到上述推导过程，得到如下方程式。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-5a266bf92625830eaa146eb535bdfd4c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1574\" data-rawheight=\"390\" class=\"origin_image zh-lightbox-thumb\" width=\"1574\" data-original=\"https://pic1.zhimg.com/v2-5a266bf92625830eaa146eb535bdfd4c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1574&#39; height=&#39;390&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1574\" data-rawheight=\"390\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1574\" data-original=\"https://pic1.zhimg.com/v2-5a266bf92625830eaa146eb535bdfd4c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-5a266bf92625830eaa146eb535bdfd4c_b.jpg\"/></figure><p>我们来解释下怎样得到的方程式(1)和方程式(2)，上面(1)式中引入一个未知的新的分布<i>Qi</i>，第二式用到Jensen不等式。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-dd2795e57683cacf130fcdb12b743a31_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1590\" data-rawheight=\"188\" class=\"origin_image zh-lightbox-thumb\" width=\"1590\" data-original=\"https://pic2.zhimg.com/v2-dd2795e57683cacf130fcdb12b743a31_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1590&#39; height=&#39;188&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1590\" data-rawheight=\"188\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1590\" data-original=\"https://pic2.zhimg.com/v2-dd2795e57683cacf130fcdb12b743a31_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-dd2795e57683cacf130fcdb12b743a31_b.jpg\"/></figure><p>如果要满足Jensen不等式的等号，那么需要满足X为常量，即为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-39cf6909d59f9ebc93e9d8e640d1f190_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1584\" data-rawheight=\"150\" class=\"origin_image zh-lightbox-thumb\" width=\"1584\" data-original=\"https://pic1.zhimg.com/v2-39cf6909d59f9ebc93e9d8e640d1f190_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1584&#39; height=&#39;150&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1584\" data-rawheight=\"150\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1584\" data-original=\"https://pic1.zhimg.com/v2-39cf6909d59f9ebc93e9d8e640d1f190_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-39cf6909d59f9ebc93e9d8e640d1f190_b.jpg\"/></figure><p>那么稍加改变能够得到</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-b3fded75302687012f7b17232908cc42_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1592\" data-rawheight=\"236\" class=\"origin_image zh-lightbox-thumb\" width=\"1592\" data-original=\"https://pic3.zhimg.com/v2-b3fded75302687012f7b17232908cc42_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1592&#39; height=&#39;236&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1592\" data-rawheight=\"236\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1592\" data-original=\"https://pic3.zhimg.com/v2-b3fded75302687012f7b17232908cc42_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-b3fded75302687012f7b17232908cc42_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-21de91b6c08dd088862631012d84b84f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1586\" data-rawheight=\"312\" class=\"origin_image zh-lightbox-thumb\" width=\"1586\" data-original=\"https://pic4.zhimg.com/v2-21de91b6c08dd088862631012d84b84f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1586&#39; height=&#39;312&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1586\" data-rawheight=\"312\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1586\" data-original=\"https://pic4.zhimg.com/v2-21de91b6c08dd088862631012d84b84f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-21de91b6c08dd088862631012d84b84f_b.jpg\"/></figure><p>因此得到下列方程，其中方程(3)利用到条件概率公式。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-f295cf3305748163d3f4b5c4c04ddf16_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1572\" data-rawheight=\"246\" class=\"origin_image zh-lightbox-thumb\" width=\"1572\" data-original=\"https://pic3.zhimg.com/v2-f295cf3305748163d3f4b5c4c04ddf16_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1572&#39; height=&#39;246&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1572\" data-rawheight=\"246\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1572\" data-original=\"https://pic3.zhimg.com/v2-f295cf3305748163d3f4b5c4c04ddf16_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-f295cf3305748163d3f4b5c4c04ddf16_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-abfc37bea9905dcd10f45e1dee1595a6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1590\" data-rawheight=\"250\" class=\"origin_image zh-lightbox-thumb\" width=\"1590\" data-original=\"https://pic3.zhimg.com/v2-abfc37bea9905dcd10f45e1dee1595a6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1590&#39; height=&#39;250&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1590\" data-rawheight=\"250\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1590\" data-original=\"https://pic3.zhimg.com/v2-abfc37bea9905dcd10f45e1dee1595a6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-abfc37bea9905dcd10f45e1dee1595a6_b.jpg\"/></figure><p>去掉上式中常数部分，则我们需要极大化的对数似然下界为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-51df6f8380bfd48d90d64cfd7aa1867d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1664\" data-rawheight=\"414\" class=\"origin_image zh-lightbox-thumb\" width=\"1664\" data-original=\"https://pic2.zhimg.com/v2-51df6f8380bfd48d90d64cfd7aa1867d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1664&#39; height=&#39;414&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1664\" data-rawheight=\"414\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1664\" data-original=\"https://pic2.zhimg.com/v2-51df6f8380bfd48d90d64cfd7aa1867d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-51df6f8380bfd48d90d64cfd7aa1867d_b.jpg\"/></figure><h2>4.EM算法流程</h2><p>现在我们总结下EM算法流程。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-6ab478be52bbf71fb9bbf9cc38c7661d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1614\" data-rawheight=\"970\" class=\"origin_image zh-lightbox-thumb\" width=\"1614\" data-original=\"https://pic2.zhimg.com/v2-6ab478be52bbf71fb9bbf9cc38c7661d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1614&#39; height=&#39;970&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1614\" data-rawheight=\"970\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1614\" data-original=\"https://pic2.zhimg.com/v2-6ab478be52bbf71fb9bbf9cc38c7661d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-6ab478be52bbf71fb9bbf9cc38c7661d_b.jpg\"/></figure><h2>5.EM算法的收敛性</h2><p>我们现在来解答下<b>2.EM算法实例</b>中问题，即EM算法能够保证收敛吗？如果EM算法收敛，那么能够保证收敛到全局最大值吗？</p><p>首先我们来看下第一个问题，EM算法能够保证收敛吗?要证明EM算法收敛，则我们需要证明对数似然函数的值在迭代的过程中一直增大。即</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-1d52b6d3c3a0dda863abf4177663b7c4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1582\" data-rawheight=\"156\" class=\"origin_image zh-lightbox-thumb\" width=\"1582\" data-original=\"https://pic1.zhimg.com/v2-1d52b6d3c3a0dda863abf4177663b7c4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1582&#39; height=&#39;156&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1582\" data-rawheight=\"156\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1582\" data-original=\"https://pic1.zhimg.com/v2-1d52b6d3c3a0dda863abf4177663b7c4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-1d52b6d3c3a0dda863abf4177663b7c4_b.jpg\"/></figure><p>由于</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-e8be33dd27d2f710b1d5f0d7b6db6e50_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1584\" data-rawheight=\"152\" class=\"origin_image zh-lightbox-thumb\" width=\"1584\" data-original=\"https://pic1.zhimg.com/v2-e8be33dd27d2f710b1d5f0d7b6db6e50_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1584&#39; height=&#39;152&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1584\" data-rawheight=\"152\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1584\" data-original=\"https://pic1.zhimg.com/v2-e8be33dd27d2f710b1d5f0d7b6db6e50_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-e8be33dd27d2f710b1d5f0d7b6db6e50_b.jpg\"/></figure><p>令</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-c8f999ba505ce31bca3a39386355fd94_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1582\" data-rawheight=\"146\" class=\"origin_image zh-lightbox-thumb\" width=\"1582\" data-original=\"https://pic1.zhimg.com/v2-c8f999ba505ce31bca3a39386355fd94_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1582&#39; height=&#39;146&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1582\" data-rawheight=\"146\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1582\" data-original=\"https://pic1.zhimg.com/v2-c8f999ba505ce31bca3a39386355fd94_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-c8f999ba505ce31bca3a39386355fd94_b.jpg\"/></figure><p>上两式相减得到</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-2af3825e6f51c488cfb2c32c6b70b86e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1502\" data-rawheight=\"148\" class=\"origin_image zh-lightbox-thumb\" width=\"1502\" data-original=\"https://pic3.zhimg.com/v2-2af3825e6f51c488cfb2c32c6b70b86e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1502&#39; height=&#39;148&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1502\" data-rawheight=\"148\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1502\" data-original=\"https://pic3.zhimg.com/v2-2af3825e6f51c488cfb2c32c6b70b86e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-2af3825e6f51c488cfb2c32c6b70b86e_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-1a340baa652efe9484b02e1c75e1ef28_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1496\" data-rawheight=\"192\" class=\"origin_image zh-lightbox-thumb\" width=\"1496\" data-original=\"https://pic1.zhimg.com/v2-1a340baa652efe9484b02e1c75e1ef28_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1496&#39; height=&#39;192&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1496\" data-rawheight=\"192\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1496\" data-original=\"https://pic1.zhimg.com/v2-1a340baa652efe9484b02e1c75e1ef28_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-1a340baa652efe9484b02e1c75e1ef28_b.jpg\"/></figure><p>要证明EM算法的收敛性，我们只要证明上式的右边是非负即可。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b4337395074eaf9e9f5b708a639b1f3d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1620\" data-rawheight=\"150\" class=\"origin_image zh-lightbox-thumb\" width=\"1620\" data-original=\"https://pic2.zhimg.com/v2-b4337395074eaf9e9f5b708a639b1f3d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1620&#39; height=&#39;150&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1620\" data-rawheight=\"150\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1620\" data-original=\"https://pic2.zhimg.com/v2-b4337395074eaf9e9f5b708a639b1f3d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-b4337395074eaf9e9f5b708a639b1f3d_b.jpg\"/></figure><p> 而对于第二部分，我们有</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-0bced9ab422cdc5527539f1c082c9197_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1548\" data-rawheight=\"386\" class=\"origin_image zh-lightbox-thumb\" width=\"1548\" data-original=\"https://pic4.zhimg.com/v2-0bced9ab422cdc5527539f1c082c9197_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1548&#39; height=&#39;386&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1548\" data-rawheight=\"386\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1548\" data-original=\"https://pic4.zhimg.com/v2-0bced9ab422cdc5527539f1c082c9197_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-0bced9ab422cdc5527539f1c082c9197_b.jpg\"/></figure><p>其中第(6)式用到了Jensen不等式，只不过和第<b>3.EM算法推导</b>中使用相反，第(7)式用到了概率分布累计为1的性质。至此我们得到</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-69061ddd1f2103f219b82b6e2da49676_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1564\" data-rawheight=\"154\" class=\"origin_image zh-lightbox-thumb\" width=\"1564\" data-original=\"https://pic3.zhimg.com/v2-69061ddd1f2103f219b82b6e2da49676_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1564&#39; height=&#39;154&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1564\" data-rawheight=\"154\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1564\" data-original=\"https://pic3.zhimg.com/v2-69061ddd1f2103f219b82b6e2da49676_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-69061ddd1f2103f219b82b6e2da49676_b.jpg\"/></figure><p>证明了EM算法的收敛性。</p><p>从上面的推导可以看出，EM算法可以保证收敛到一个稳定点，但是却不能保证收敛到全局的极大值点，因此它是局部最优的算法。当然，如果我们的优化目标L(θ,θj)是凸的，则EM算法可以保证收敛到全局最大值，这点和梯度下降法中迭代算法相同。</p><h2>6.Sklearn实现EM算法</h2><p>高斯混合模型(GMM)使用高斯分布作为参数模型，利用期望最大(EM)算法进行训练，有兴趣了解高斯混合模型的同学可以去<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3Fsrc%3D11%26timestamp%3D1525932817%26ver%3D867%26signature%3DU4UxviydVELD71Ju%2AbRWvh0ziFU57aNoPTZkVu5ShBEH7lxe1PLqxQnZ-xVSFgXdw5GcsWcYF5W1llR4dQ2yrsj0t2syeXgEggjm%2AbudZlpZdMQMLOcXB-FnvKKlkV2H%26new%3D1\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">这儿</a>。下列代码来自于<a href=\"https://link.zhihu.com/?target=http%3A//scikit-learn.org/stable/auto_examples/mixture/plot_gmm_covariances.html%23sphx-glr-auto-examples-mixture-plot-gmm-covariances-py\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Sklearn官网GMM模块</a>，利用高斯混合模型确定iris聚类。</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"kn\">import</span> <span class=\"nn\">matplotlib</span> <span class=\"k\">as</span> <span class=\"nn\">mpl</span>\n<span class=\"kn\">import</span> <span class=\"nn\">matplotlib.pyplot</span> <span class=\"k\">as</span> <span class=\"nn\">plt</span>\n\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn</span> <span class=\"k\">import</span> <span class=\"n\">datasets</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.mixture</span> <span class=\"k\">import</span> <span class=\"n\">GaussianMixture</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.model_selection</span> <span class=\"k\">import</span> <span class=\"n\">StratifiedKFold</span>\n\n<span class=\"n\">colors</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s1\">&#39;navy&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;turquoise&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;darkorange&#39;</span><span class=\"p\">]</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">make_ellipses</span><span class=\"p\">(</span><span class=\"n\">gmm</span><span class=\"p\">,</span> <span class=\"n\">ax</span><span class=\"p\">):</span>\n    <span class=\"k\">for</span> <span class=\"n\">n</span><span class=\"p\">,</span> <span class=\"n\">color</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">colors</span><span class=\"p\">):</span>\n        <span class=\"k\">if</span> <span class=\"n\">gmm</span><span class=\"o\">.</span><span class=\"n\">covariance_type</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;full&#39;</span><span class=\"p\">:</span>\n            <span class=\"n\">covariances</span> <span class=\"o\">=</span> <span class=\"n\">gmm</span><span class=\"o\">.</span><span class=\"n\">covariances_</span><span class=\"p\">[</span><span class=\"n\">n</span><span class=\"p\">][:</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"p\">:</span><span class=\"mi\">2</span><span class=\"p\">]</span>\n        <span class=\"k\">elif</span> <span class=\"n\">gmm</span><span class=\"o\">.</span><span class=\"n\">covariance_type</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;tied&#39;</span><span class=\"p\">:</span>\n            <span class=\"n\">covariances</span> <span class=\"o\">=</span> <span class=\"n\">gmm</span><span class=\"o\">.</span><span class=\"n\">covariances_</span><span class=\"p\">[:</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"p\">:</span><span class=\"mi\">2</span><span class=\"p\">]</span>\n        <span class=\"k\">elif</span> <span class=\"n\">gmm</span><span class=\"o\">.</span><span class=\"n\">covariance_type</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;diag&#39;</span><span class=\"p\">:</span>\n            <span class=\"n\">covariances</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">diag</span><span class=\"p\">(</span><span class=\"n\">gmm</span><span class=\"o\">.</span><span class=\"n\">covariances_</span><span class=\"p\">[</span><span class=\"n\">n</span><span class=\"p\">][:</span><span class=\"mi\">2</span><span class=\"p\">])</span>\n        <span class=\"k\">elif</span> <span class=\"n\">gmm</span><span class=\"o\">.</span><span class=\"n\">covariance_type</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;spherical&#39;</span><span class=\"p\">:</span>\n            <span class=\"n\">covariances</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">eye</span><span class=\"p\">(</span><span class=\"n\">gmm</span><span class=\"o\">.</span><span class=\"n\">means_</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">])</span> <span class=\"o\">*</span> <span class=\"n\">gmm</span><span class=\"o\">.</span><span class=\"n\">covariances_</span><span class=\"p\">[</span><span class=\"n\">n</span><span class=\"p\">]</span>\n        <span class=\"n\">v</span><span class=\"p\">,</span> <span class=\"n\">w</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linalg</span><span class=\"o\">.</span><span class=\"n\">eigh</span><span class=\"p\">(</span><span class=\"n\">covariances</span><span class=\"p\">)</span>\n        <span class=\"n\">u</span> <span class=\"o\">=</span> <span class=\"n\">w</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">/</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linalg</span><span class=\"o\">.</span><span class=\"n\">norm</span><span class=\"p\">(</span><span class=\"n\">w</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">])</span>\n        <span class=\"n\">angle</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">arctan2</span><span class=\"p\">(</span><span class=\"n\">u</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">u</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">])</span>\n        <span class=\"n\">angle</span> <span class=\"o\">=</span> <span class=\"mi\">180</span> <span class=\"o\">*</span> <span class=\"n\">angle</span> <span class=\"o\">/</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">pi</span>  <span class=\"c1\"># convert to degrees</span>\n        <span class=\"n\">v</span> <span class=\"o\">=</span> <span class=\"mf\">2.</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sqrt</span><span class=\"p\">(</span><span class=\"mf\">2.</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sqrt</span><span class=\"p\">(</span><span class=\"n\">v</span><span class=\"p\">)</span>\n        <span class=\"n\">ell</span> <span class=\"o\">=</span> <span class=\"n\">mpl</span><span class=\"o\">.</span><span class=\"n\">patches</span><span class=\"o\">.</span><span class=\"n\">Ellipse</span><span class=\"p\">(</span><span class=\"n\">gmm</span><span class=\"o\">.</span><span class=\"n\">means_</span><span class=\"p\">[</span><span class=\"n\">n</span><span class=\"p\">,</span> <span class=\"p\">:</span><span class=\"mi\">2</span><span class=\"p\">],</span> <span class=\"n\">v</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">v</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">],</span>\n                                  <span class=\"mi\">180</span> <span class=\"o\">+</span> <span class=\"n\">angle</span><span class=\"p\">,</span> <span class=\"n\">color</span><span class=\"o\">=</span><span class=\"n\">color</span><span class=\"p\">)</span>\n        <span class=\"n\">ell</span><span class=\"o\">.</span><span class=\"n\">set_clip_box</span><span class=\"p\">(</span><span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">bbox</span><span class=\"p\">)</span>\n        <span class=\"n\">ell</span><span class=\"o\">.</span><span class=\"n\">set_alpha</span><span class=\"p\">(</span><span class=\"mf\">0.5</span><span class=\"p\">)</span>\n        <span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">add_artist</span><span class=\"p\">(</span><span class=\"n\">ell</span><span class=\"p\">)</span>\n\n<span class=\"n\">iris</span> <span class=\"o\">=</span> <span class=\"n\">datasets</span><span class=\"o\">.</span><span class=\"n\">load_iris</span><span class=\"p\">()</span>\n\n<span class=\"c1\"># Break up the dataset into non-overlapping training (75%)</span>\n<span class=\"c1\"># and testing (25%) sets.</span>\n<span class=\"n\">skf</span> <span class=\"o\">=</span> <span class=\"n\">StratifiedKFold</span><span class=\"p\">(</span><span class=\"n\">n_splits</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">)</span>\n<span class=\"c1\"># Only take the first fold.</span>\n<span class=\"n\">train_index</span><span class=\"p\">,</span> <span class=\"n\">test_index</span> <span class=\"o\">=</span> <span class=\"nb\">next</span><span class=\"p\">(</span><span class=\"nb\">iter</span><span class=\"p\">(</span><span class=\"n\">skf</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">(</span><span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">,</span> <span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">target</span><span class=\"p\">)))</span>\n\n\n<span class=\"n\">X_train</span> <span class=\"o\">=</span> <span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">[</span><span class=\"n\">train_index</span><span class=\"p\">]</span>\n<span class=\"n\">y_train</span> <span class=\"o\">=</span> <span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">target</span><span class=\"p\">[</span><span class=\"n\">train_index</span><span class=\"p\">]</span>\n<span class=\"n\">X_test</span> <span class=\"o\">=</span> <span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">[</span><span class=\"n\">test_index</span><span class=\"p\">]</span>\n<span class=\"n\">y_test</span> <span class=\"o\">=</span> <span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">target</span><span class=\"p\">[</span><span class=\"n\">test_index</span><span class=\"p\">]</span>\n\n<span class=\"n\">n_classes</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">unique</span><span class=\"p\">(</span><span class=\"n\">y_train</span><span class=\"p\">))</span>\n\n<span class=\"c1\"># Try GMMs using different types of covariances.</span>\n<span class=\"n\">estimators</span> <span class=\"o\">=</span> <span class=\"nb\">dict</span><span class=\"p\">((</span><span class=\"n\">cov_type</span><span class=\"p\">,</span> <span class=\"n\">GaussianMixture</span><span class=\"p\">(</span><span class=\"n\">n_components</span><span class=\"o\">=</span><span class=\"n\">n_classes</span><span class=\"p\">,</span>\n                   <span class=\"n\">covariance_type</span><span class=\"o\">=</span><span class=\"n\">cov_type</span><span class=\"p\">,</span> <span class=\"n\">max_iter</span><span class=\"o\">=</span><span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"n\">random_state</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">))</span>\n                  <span class=\"k\">for</span> <span class=\"n\">cov_type</span> <span class=\"ow\">in</span> <span class=\"p\">[</span><span class=\"s1\">&#39;spherical&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;diag&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;tied&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;full&#39;</span><span class=\"p\">])</span>\n\n<span class=\"n\">n_estimators</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">estimators</span><span class=\"p\">)</span>\n\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">figure</span><span class=\"p\">(</span><span class=\"n\">figsize</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">3</span> <span class=\"o\">*</span> <span class=\"n\">n_estimators</span> <span class=\"o\">//</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">6</span><span class=\"p\">))</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplots_adjust</span><span class=\"p\">(</span><span class=\"n\">bottom</span><span class=\"o\">=.</span><span class=\"mi\">01</span><span class=\"p\">,</span> <span class=\"n\">top</span><span class=\"o\">=</span><span class=\"mf\">0.95</span><span class=\"p\">,</span> <span class=\"n\">hspace</span><span class=\"o\">=.</span><span class=\"mi\">15</span><span class=\"p\">,</span> <span class=\"n\">wspace</span><span class=\"o\">=.</span><span class=\"mi\">05</span><span class=\"p\">,</span>\n                    <span class=\"n\">left</span><span class=\"o\">=.</span><span class=\"mi\">01</span><span class=\"p\">,</span> <span class=\"n\">right</span><span class=\"o\">=.</span><span class=\"mi\">99</span><span class=\"p\">)</span>\n\n\n<span class=\"k\">for</span> <span class=\"n\">index</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">name</span><span class=\"p\">,</span> <span class=\"n\">estimator</span><span class=\"p\">)</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">estimators</span><span class=\"o\">.</span><span class=\"n\">items</span><span class=\"p\">()):</span>\n    <span class=\"c1\"># Since we have class labels for the training data, we can</span>\n    <span class=\"c1\"># initialize the GMM parameters in a supervised manner.</span>\n    <span class=\"n\">estimator</span><span class=\"o\">.</span><span class=\"n\">means_init</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"n\">X_train</span><span class=\"p\">[</span><span class=\"n\">y_train</span> <span class=\"o\">==</span> <span class=\"n\">i</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n                                    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">n_classes</span><span class=\"p\">)])</span>\n\n    <span class=\"c1\"># Train the other parameters using the EM algorithm.</span>\n    <span class=\"n\">estimator</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">)</span>\n\n    <span class=\"n\">h</span> <span class=\"o\">=</span> <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplot</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">n_estimators</span> <span class=\"o\">//</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">index</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n    <span class=\"n\">make_ellipses</span><span class=\"p\">(</span><span class=\"n\">estimator</span><span class=\"p\">,</span> <span class=\"n\">h</span><span class=\"p\">)</span>\n\n    <span class=\"k\">for</span> <span class=\"n\">n</span><span class=\"p\">,</span> <span class=\"n\">color</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">colors</span><span class=\"p\">):</span>\n        <span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">[</span><span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">target</span> <span class=\"o\">==</span> <span class=\"n\">n</span><span class=\"p\">]</span>\n        <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">scatter</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">[:,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">data</span><span class=\"p\">[:,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">s</span><span class=\"o\">=</span><span class=\"mf\">0.8</span><span class=\"p\">,</span> <span class=\"n\">color</span><span class=\"o\">=</span><span class=\"n\">color</span><span class=\"p\">,</span>\n                    <span class=\"n\">label</span><span class=\"o\">=</span><span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">target_names</span><span class=\"p\">[</span><span class=\"n\">n</span><span class=\"p\">])</span>\n    <span class=\"c1\"># Plot the test data with crosses</span>\n    <span class=\"k\">for</span> <span class=\"n\">n</span><span class=\"p\">,</span> <span class=\"n\">color</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">colors</span><span class=\"p\">):</span>\n        <span class=\"n\">data</span> <span class=\"o\">=</span> <span class=\"n\">X_test</span><span class=\"p\">[</span><span class=\"n\">y_test</span> <span class=\"o\">==</span> <span class=\"n\">n</span><span class=\"p\">]</span>\n        <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">scatter</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">[:,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">data</span><span class=\"p\">[:,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">marker</span><span class=\"o\">=</span><span class=\"s1\">&#39;x&#39;</span><span class=\"p\">,</span> <span class=\"n\">color</span><span class=\"o\">=</span><span class=\"n\">color</span><span class=\"p\">)</span>\n\n    <span class=\"n\">y_train_pred</span> <span class=\"o\">=</span> <span class=\"n\">estimator</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">)</span>\n    <span class=\"n\">train_accuracy</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">y_train_pred</span><span class=\"o\">.</span><span class=\"n\">ravel</span><span class=\"p\">()</span> <span class=\"o\">==</span> <span class=\"n\">y_train</span><span class=\"o\">.</span><span class=\"n\">ravel</span><span class=\"p\">())</span> <span class=\"o\">*</span> <span class=\"mi\">100</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">text</span><span class=\"p\">(</span><span class=\"mf\">0.05</span><span class=\"p\">,</span> <span class=\"mf\">0.9</span><span class=\"p\">,</span> <span class=\"s1\">&#39;Train accuracy: </span><span class=\"si\">%.1f</span><span class=\"s1\">&#39;</span> <span class=\"o\">%</span> <span class=\"n\">train_accuracy</span><span class=\"p\">,</span>\n             <span class=\"n\">transform</span><span class=\"o\">=</span><span class=\"n\">h</span><span class=\"o\">.</span><span class=\"n\">transAxes</span><span class=\"p\">)</span>\n\n    <span class=\"n\">y_test_pred</span> <span class=\"o\">=</span> <span class=\"n\">estimator</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">X_test</span><span class=\"p\">)</span>\n    <span class=\"n\">test_accuracy</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">y_test_pred</span><span class=\"o\">.</span><span class=\"n\">ravel</span><span class=\"p\">()</span> <span class=\"o\">==</span> <span class=\"n\">y_test</span><span class=\"o\">.</span><span class=\"n\">ravel</span><span class=\"p\">())</span> <span class=\"o\">*</span> <span class=\"mi\">100</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">text</span><span class=\"p\">(</span><span class=\"mf\">0.05</span><span class=\"p\">,</span> <span class=\"mf\">0.8</span><span class=\"p\">,</span> <span class=\"s1\">&#39;Test accuracy: </span><span class=\"si\">%.1f</span><span class=\"s1\">&#39;</span> <span class=\"o\">%</span> <span class=\"n\">test_accuracy</span><span class=\"p\">,</span>\n             <span class=\"n\">transform</span><span class=\"o\">=</span><span class=\"n\">h</span><span class=\"o\">.</span><span class=\"n\">transAxes</span><span class=\"p\">)</span>\n\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">xticks</span><span class=\"p\">(())</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">yticks</span><span class=\"p\">(())</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">title</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"p\">)</span>\n\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">legend</span><span class=\"p\">(</span><span class=\"n\">scatterpoints</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">loc</span><span class=\"o\">=</span><span class=\"s1\">&#39;lower right&#39;</span><span class=\"p\">,</span> <span class=\"n\">prop</span><span class=\"o\">=</span><span class=\"nb\">dict</span><span class=\"p\">(</span><span class=\"n\">size</span><span class=\"o\">=</span><span class=\"mi\">12</span><span class=\"p\">))</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span></code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-4d1c9ff9f8675b6a97393a8328866c9a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"930\" data-rawheight=\"926\" class=\"origin_image zh-lightbox-thumb\" width=\"930\" data-original=\"https://pic3.zhimg.com/v2-4d1c9ff9f8675b6a97393a8328866c9a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;930&#39; height=&#39;926&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"930\" data-rawheight=\"926\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"930\" data-original=\"https://pic3.zhimg.com/v2-4d1c9ff9f8675b6a97393a8328866c9a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-4d1c9ff9f8675b6a97393a8328866c9a_b.jpg\"/></figure><h2>7.EM算法优缺点</h2><h2>7.1优点</h2><ul><li>聚类。</li><li>算法计算结果稳定、准确。</li><li>EM算法自收敛，既不需要事先设定类别，也不需要数据间的两两比较合并等操作。</li></ul><h2>7.2缺点</h2><ul><li>对初始化数据敏感。</li><li>EM算法计算复杂，收敛较慢，不适于大规模数据集和高维数据。</li><li>当所要优化的函数不是凸函数时，EM算法容易给出局部最优解，而不是全局最优解。</li></ul><h2>8.推广</h2><p>更多内容请关注公众号<b>谓之小一</b>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p><p><a href=\"https://link.zhihu.com/?target=http%3A//weixin.qq.com/r/Ty_4oDPE2W6mrXfD93pd\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">weixin.qq.com/r/Ty_4oDP</span><span class=\"invisible\">E2W6mrXfD93pd</span><span class=\"ellipsis\"></span></a> (二维码自动识别)</p><p class=\"ztext-empty-paragraph\"><br/></p><p>参考</p><ul><li><a href=\"https://link.zhihu.com/?target=https%3A//www.jianshu.com/p/1121509ac1dc\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">milter_如何感性地理解EM算法</a></li><li><a href=\"https://link.zhihu.com/?target=https%3A//www.cnblogs.com/pinard/p/6912636.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">刘建平Pinard_EM算法原理总结</a></li><li><a href=\"https://link.zhihu.com/?target=http%3A//scikit-learn.org/stable/modules/mixture.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Gaussian mixture models</a></li></ul><p></p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "算法", 
                    "tagLink": "https://api.zhihu.com/topics/19553510"
                }, 
                {
                    "tag": "期望", 
                    "tagLink": "https://api.zhihu.com/topics/19632805"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/36521514", 
            "userName": "谓之小一", 
            "userLink": "https://www.zhihu.com/people/761dda159c5cec5fd926f761ab5f4ccf", 
            "upvote": 4, 
            "title": "机器学习之自适应增强(Adaboost)", 
            "content": "<blockquote>由于较多公式，所以我将部分内容转为图片进行上传，请见谅。清晰版请访问<a href=\"https://link.zhihu.com/?target=https%3A//weizhixiaoyi.com\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">weizhixiaoyi.com</span><span class=\"invisible\"></span></a>查看，你也可以关注我公众号<b>谓之小一</b>，后台直接向我要pdf版本，如有相关问题可公众号后台询问，随时回答。</blockquote><h2>1.Adaboost简介</h2><p><b>Adaptive boosting(自适应增强)</b>是一种迭代算法，其核心思想是针对同一个训练集训练不同的弱分类器，然后把这些弱分类器集合起来，构成一个强分类器，Adaboost可处理分类和回归问题。了解Adaboost算法之前，我们先学习下<b>Boost(增强)</b>和<b>Adaptive(自适应)</b>的概念。</p><h2>1.1集成学习之Boosting</h2><p>集成学习不是单独的机器学习方法，而是通过构建并结合多个机器学习器来完成任务，集成学习可以用于分类问题集成、回归问题集成、特征选取集成、异常点检测集成等方面。其思想是对于训练数据集，我们通过训练若干个个体学习器，通过一定的结合策略形成一个强学习器，以达到博采众长的目的。在<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3MjA2NTQzMw%3D%3D%26mid%3D2247483845%26idx%3D1%26sn%3D5484385408d694ba03a8bdc3a03c2263%26chksm%3Dfcd7d233cba05b25b0f65289f8416466df9124b019b60704b4e3875d9864ff90e3388c666f66%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">机器学习之随机森林</a>中我们已经用到集成学习中的bagging方法，此处我们详细介绍集成学习中的Boosting方法。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-1da21d57240c05a8d444dfafd9e7b91c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1412\" data-rawheight=\"662\" class=\"origin_image zh-lightbox-thumb\" width=\"1412\" data-original=\"https://pic1.zhimg.com/v2-1da21d57240c05a8d444dfafd9e7b91c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1412&#39; height=&#39;662&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1412\" data-rawheight=\"662\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1412\" data-original=\"https://pic1.zhimg.com/v2-1da21d57240c05a8d444dfafd9e7b91c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-1da21d57240c05a8d444dfafd9e7b91c_b.jpg\"/></figure><p>从上图可以看出，Boosting算法的工作机制是从训练集用初始权重训练出一个弱学习器1，根据弱学习器的学习误差率来更新训练样本的权重，使得之前弱学习器1中学习误差率高的训练样本点权重变高。然后这些误差率高的点在弱学习器2中得到更高的重视，利用调整权重后的训练集来训练弱学习器2。如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。</p><h2>1.2Adaptive自适应</h2><p><b>Adaptive(自适应)</b>体现在前一个基本分类器分错的样本会得到加强，加权后的全体样本再次被用来训练下一个基本分类器。同时在每一轮中加入一个新的弱分类器，直到得到某个预定的足够小的错误率或达到预定的最大迭代次数。</p><h2>1.3Adaboost流程</h2><p>结合<b>Adaptive(自适应)</b>和<b>Boost(增强)</b>概念，我们来具体介绍下<b>Adaboost</b>迭代算法流程。</p><ul><li>初始化训练数据的权值分布。如果有N个样本，则每一个训练样本最开始都被赋予相同的权值1/N。</li><li>训练弱分类器。训练过程中，如果某个样本点已经被准确的分类，那么在构造下一个训练集中，他的权值会被降低。相反，如果某个样本点没有被准确分类，那么它的权值就会得到提高。权值更新过的样本集会被用于训练下一个分类器，整个训练过程如此迭代的进行下去。</li><li>多个弱分类器组合成强分类器。各个弱分类器的训练过程结束后，增加分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用；而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。</li></ul><p>那么我们便要思考，<b>如何计算学习误差率e？</b>,<b>如何得到弱学习器权重系数α?</b> ,<b>如何更新样本权重D？</b>,<b>使用哪种结合策略？</b>我们将在Adaboost分类和回归算法中给出详细解答。</p><h2>2.Adaboost分类算法</h2><p>假设我们的训练样本为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-d994c21fbf45b20280cd18ce9c11c76d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1598\" data-rawheight=\"106\" class=\"origin_image zh-lightbox-thumb\" width=\"1598\" data-original=\"https://pic2.zhimg.com/v2-d994c21fbf45b20280cd18ce9c11c76d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1598&#39; height=&#39;106&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1598\" data-rawheight=\"106\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1598\" data-original=\"https://pic2.zhimg.com/v2-d994c21fbf45b20280cd18ce9c11c76d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-d994c21fbf45b20280cd18ce9c11c76d_b.jpg\"/></figure><p>训练集在第k个弱学习器的输出权重为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-54cfe505537e6ce099c3f5de0578a89a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1610\" data-rawheight=\"128\" class=\"origin_image zh-lightbox-thumb\" width=\"1610\" data-original=\"https://pic3.zhimg.com/v2-54cfe505537e6ce099c3f5de0578a89a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1610&#39; height=&#39;128&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1610\" data-rawheight=\"128\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1610\" data-original=\"https://pic3.zhimg.com/v2-54cfe505537e6ce099c3f5de0578a89a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-54cfe505537e6ce099c3f5de0578a89a_b.jpg\"/></figure><p>Adaboost分类算法中包含二元分类和多元分类问题，多元分类问题为二元分类的推广。为方便推导，此处我们假设是二元分类问题，输出类别为{-1,1}。那么第k个弱分类器Gk(x)在训练集上的<b>学习误差率</b>为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-8a50309d803edccc3e98c3c101ecbb35_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1586\" data-rawheight=\"150\" class=\"origin_image zh-lightbox-thumb\" width=\"1586\" data-original=\"https://pic2.zhimg.com/v2-8a50309d803edccc3e98c3c101ecbb35_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1586&#39; height=&#39;150&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1586\" data-rawheight=\"150\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1586\" data-original=\"https://pic2.zhimg.com/v2-8a50309d803edccc3e98c3c101ecbb35_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-8a50309d803edccc3e98c3c101ecbb35_b.jpg\"/></figure><p>对于二元分类问题，第k个<b>弱分类器Gk(x)的权重系数</b>为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-259cdf7752f6cdd2dd5a0d836006d601_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1594\" data-rawheight=\"126\" class=\"origin_image zh-lightbox-thumb\" width=\"1594\" data-original=\"https://pic2.zhimg.com/v2-259cdf7752f6cdd2dd5a0d836006d601_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1594&#39; height=&#39;126&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1594\" data-rawheight=\"126\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1594\" data-original=\"https://pic2.zhimg.com/v2-259cdf7752f6cdd2dd5a0d836006d601_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-259cdf7752f6cdd2dd5a0d836006d601_b.jpg\"/></figure><p>从上式可以看出，如果分类误差率ek越大，则对应的弱分类器权重系数αk越小。也就是说，误差率越大的弱分类器权重系数越小。</p><p>假设第k个弱分类器的样本集权重系数为D(k)=(wk1,wk2,…,wkm)，则<b>更新后的第k+1个弱分类器的样本集权重系数</b>如下所示，此处Zk是规范化因子。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-4c3e1714b70204ba3b778773c6357d87_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1610\" data-rawheight=\"282\" class=\"origin_image zh-lightbox-thumb\" width=\"1610\" data-original=\"https://pic4.zhimg.com/v2-4c3e1714b70204ba3b778773c6357d87_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1610&#39; height=&#39;282&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1610\" data-rawheight=\"282\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1610\" data-original=\"https://pic4.zhimg.com/v2-4c3e1714b70204ba3b778773c6357d87_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-4c3e1714b70204ba3b778773c6357d87_b.jpg\"/></figure><p>从wk+1,i计算公式可以看出，如果第i个样本分类错误，则yiGk(xi)&lt;0，导致样本的权重在第k+1个弱分类器中增大。如果分类正确，则权重在第k+1个弱分类器中减少。具体为什么选择上述<b>权重系数</b>和<b>样本权重更新</b>公式，我们在下面讲<b>Adaboost损失函数</b>时会详细介绍。</p><p>最后Adaboost分类问题采用<b>加权平均法结合策略</b>，最终的强分类器为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-1d3a64d2e6e83e807fdd201f3ed96e6f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1604\" data-rawheight=\"156\" class=\"origin_image zh-lightbox-thumb\" width=\"1604\" data-original=\"https://pic4.zhimg.com/v2-1d3a64d2e6e83e807fdd201f3ed96e6f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1604&#39; height=&#39;156&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1604\" data-rawheight=\"156\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1604\" data-original=\"https://pic4.zhimg.com/v2-1d3a64d2e6e83e807fdd201f3ed96e6f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-1d3a64d2e6e83e807fdd201f3ed96e6f_b.jpg\"/></figure><p>对于<b>Adaboost多元分类算法</b>，其实原理和二元分类类似，最主要区别在弱分类器的系数上。比如Adaboost SAMME算法，它的弱分类器的系数如下所示，其中R为类别数。从下式可以看出，如果是二元分类(R=2)，则下式和上述二元分类算法中的弱分类器的系数一致。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-e8146feaf11e60cde8f38310bba4b1e0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1594\" data-rawheight=\"126\" class=\"origin_image zh-lightbox-thumb\" width=\"1594\" data-original=\"https://pic1.zhimg.com/v2-e8146feaf11e60cde8f38310bba4b1e0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1594&#39; height=&#39;126&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1594\" data-rawheight=\"126\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1594\" data-original=\"https://pic1.zhimg.com/v2-e8146feaf11e60cde8f38310bba4b1e0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-e8146feaf11e60cde8f38310bba4b1e0_b.jpg\"/></figure><h2>3.Adaboost回归算法</h2><p>假设我们的训练样本为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-790d5b5d80423952bc6f7900dd084403_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1594\" data-rawheight=\"118\" class=\"origin_image zh-lightbox-thumb\" width=\"1594\" data-original=\"https://pic4.zhimg.com/v2-790d5b5d80423952bc6f7900dd084403_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1594&#39; height=&#39;118&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1594\" data-rawheight=\"118\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1594\" data-original=\"https://pic4.zhimg.com/v2-790d5b5d80423952bc6f7900dd084403_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-790d5b5d80423952bc6f7900dd084403_b.jpg\"/></figure><p>训练集在第k个弱学习器的输出权重为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-275f97a09533cec5b5adb953b081d4fc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1622\" data-rawheight=\"120\" class=\"origin_image zh-lightbox-thumb\" width=\"1622\" data-original=\"https://pic1.zhimg.com/v2-275f97a09533cec5b5adb953b081d4fc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1622&#39; height=&#39;120&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1622\" data-rawheight=\"120\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1622\" data-original=\"https://pic1.zhimg.com/v2-275f97a09533cec5b5adb953b081d4fc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-275f97a09533cec5b5adb953b081d4fc_b.jpg\"/></figure><p>我们先看回归问题中的误差率。对于第k个弱学习器，计算他在训练集上的最大误差</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3dbf2aa723b36b2ca039e3672ae23525_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1608\" data-rawheight=\"114\" class=\"origin_image zh-lightbox-thumb\" width=\"1608\" data-original=\"https://pic2.zhimg.com/v2-3dbf2aa723b36b2ca039e3672ae23525_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1608&#39; height=&#39;114&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1608\" data-rawheight=\"114\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1608\" data-original=\"https://pic2.zhimg.com/v2-3dbf2aa723b36b2ca039e3672ae23525_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-3dbf2aa723b36b2ca039e3672ae23525_b.jpg\"/></figure><p>然后计算每个样本的相对误差</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-779db06c975c9f279e1a508b0d9ac5e9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1504\" data-rawheight=\"128\" class=\"origin_image zh-lightbox-thumb\" width=\"1504\" data-original=\"https://pic2.zhimg.com/v2-779db06c975c9f279e1a508b0d9ac5e9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1504&#39; height=&#39;128&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1504\" data-rawheight=\"128\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1504\" data-original=\"https://pic2.zhimg.com/v2-779db06c975c9f279e1a508b0d9ac5e9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-779db06c975c9f279e1a508b0d9ac5e9_b.jpg\"/></figure><p>上述公式是损失为线性的情况，如果我们采用平方误差或指数误差，则相对误差如下所示。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-7a1bc9b213144c7cd3843cbac7ea3e01_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1620\" data-rawheight=\"290\" class=\"origin_image zh-lightbox-thumb\" width=\"1620\" data-original=\"https://pic2.zhimg.com/v2-7a1bc9b213144c7cd3843cbac7ea3e01_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1620&#39; height=&#39;290&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1620\" data-rawheight=\"290\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1620\" data-original=\"https://pic2.zhimg.com/v2-7a1bc9b213144c7cd3843cbac7ea3e01_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-7a1bc9b213144c7cd3843cbac7ea3e01_b.jpg\"/></figure><p>最终得到第k个弱学习器的<b>学习误差率</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-9fbf80fe90a55dbcfc22e3dfb8ed8ece_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1592\" data-rawheight=\"146\" class=\"origin_image zh-lightbox-thumb\" width=\"1592\" data-original=\"https://pic3.zhimg.com/v2-9fbf80fe90a55dbcfc22e3dfb8ed8ece_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1592&#39; height=&#39;146&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1592\" data-rawheight=\"146\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1592\" data-original=\"https://pic3.zhimg.com/v2-9fbf80fe90a55dbcfc22e3dfb8ed8ece_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-9fbf80fe90a55dbcfc22e3dfb8ed8ece_b.jpg\"/></figure><p>那么<b>弱学习器的权重系数</b>为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-bc65ed6c2a5a7a5a3872f8df9f564160_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1600\" data-rawheight=\"134\" class=\"origin_image zh-lightbox-thumb\" width=\"1600\" data-original=\"https://pic1.zhimg.com/v2-bc65ed6c2a5a7a5a3872f8df9f564160_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1600&#39; height=&#39;134&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1600\" data-rawheight=\"134\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1600\" data-original=\"https://pic1.zhimg.com/v2-bc65ed6c2a5a7a5a3872f8df9f564160_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-bc65ed6c2a5a7a5a3872f8df9f564160_b.jpg\"/></figure><p>然后<b>更新样本权重D</b>，第k+1个弱分类器的样本集权重系数如下所示，其中Zk为规范化因子。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-61860bc134623935258ce65bb1ce1cd2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1602\" data-rawheight=\"300\" class=\"origin_image zh-lightbox-thumb\" width=\"1602\" data-original=\"https://pic3.zhimg.com/v2-61860bc134623935258ce65bb1ce1cd2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1602&#39; height=&#39;300&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1602\" data-rawheight=\"300\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1602\" data-original=\"https://pic3.zhimg.com/v2-61860bc134623935258ce65bb1ce1cd2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-61860bc134623935258ce65bb1ce1cd2_b.jpg\"/></figure><p>最后Adaboost回归问题采用<b>加权平均法结合策略</b>，最终的强回归器为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-cfb195f6b6ea040939126e094cda504b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1610\" data-rawheight=\"158\" class=\"origin_image zh-lightbox-thumb\" width=\"1610\" data-original=\"https://pic4.zhimg.com/v2-cfb195f6b6ea040939126e094cda504b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1610&#39; height=&#39;158&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1610\" data-rawheight=\"158\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1610\" data-original=\"https://pic4.zhimg.com/v2-cfb195f6b6ea040939126e094cda504b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-cfb195f6b6ea040939126e094cda504b_b.jpg\"/></figure><h2>4.Adaboost损失函数</h2><p>上述我们介绍了Adaboost分类的弱学习器权重系数公式和样本权重更新公式，但并没具体解释公式来源，此处我们通过Adaboost损失函数来进行推导。Adaboost模型是<b>加法模型</b>，学习算法为<b>前向分布学习算法</b>，损失函数为指数函数。</p><ul><li><b>加法模型：</b>最终强分类器是若干个弱分类器加权平均得到。</li><li><b>前向分布算法：</b>算法是通过一轮轮弱学习器得到，利用前一轮弱学习器的结果来更新后一个弱学习器的训练权重。</li></ul><p>假设第k-1轮和第k轮强学习器为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-dd8ad1582f070ed906e73dd4df11ab3f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1626\" data-rawheight=\"300\" class=\"origin_image zh-lightbox-thumb\" width=\"1626\" data-original=\"https://pic4.zhimg.com/v2-dd8ad1582f070ed906e73dd4df11ab3f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1626&#39; height=&#39;300&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1626\" data-rawheight=\"300\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1626\" data-original=\"https://pic4.zhimg.com/v2-dd8ad1582f070ed906e73dd4df11ab3f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-dd8ad1582f070ed906e73dd4df11ab3f_b.jpg\"/></figure><p>因此我们可以得到</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-9e045bd9fc644d09a8bb8cd568243324_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1638\" data-rawheight=\"108\" class=\"origin_image zh-lightbox-thumb\" width=\"1638\" data-original=\"https://pic1.zhimg.com/v2-9e045bd9fc644d09a8bb8cd568243324_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1638&#39; height=&#39;108&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1638\" data-rawheight=\"108\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1638\" data-original=\"https://pic1.zhimg.com/v2-9e045bd9fc644d09a8bb8cd568243324_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-9e045bd9fc644d09a8bb8cd568243324_b.jpg\"/></figure><p>可见强学习器是通过前向分布算法一步步得到。Adaboost损失函数为指数函数，即定义损失函数为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-606e9c5d0783b53a89e34cb553362281_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1608\" data-rawheight=\"168\" class=\"origin_image zh-lightbox-thumb\" width=\"1608\" data-original=\"https://pic2.zhimg.com/v2-606e9c5d0783b53a89e34cb553362281_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1608&#39; height=&#39;168&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1608\" data-rawheight=\"168\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1608\" data-original=\"https://pic2.zhimg.com/v2-606e9c5d0783b53a89e34cb553362281_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-606e9c5d0783b53a89e34cb553362281_b.jpg\"/></figure><p>利用前向分布学习算法的关系可以得到损失函数为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-8b8bcf913d2a07e05745f799d4d75e7e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1596\" data-rawheight=\"160\" class=\"origin_image zh-lightbox-thumb\" width=\"1596\" data-original=\"https://pic3.zhimg.com/v2-8b8bcf913d2a07e05745f799d4d75e7e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1596&#39; height=&#39;160&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1596\" data-rawheight=\"160\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1596\" data-original=\"https://pic3.zhimg.com/v2-8b8bcf913d2a07e05745f799d4d75e7e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-8b8bcf913d2a07e05745f799d4d75e7e_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-62baef17cdb5a0ea442072bcb812e79a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1638\" data-rawheight=\"258\" class=\"origin_image zh-lightbox-thumb\" width=\"1638\" data-original=\"https://pic3.zhimg.com/v2-62baef17cdb5a0ea442072bcb812e79a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1638&#39; height=&#39;258&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1638\" data-rawheight=\"258\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1638\" data-original=\"https://pic3.zhimg.com/v2-62baef17cdb5a0ea442072bcb812e79a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-62baef17cdb5a0ea442072bcb812e79a_b.jpg\"/></figure><p>首先我们求Gk(x)可以得到</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-eeeea91c98ee8aa6a936df52b7b0b618_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1590\" data-rawheight=\"160\" class=\"origin_image zh-lightbox-thumb\" width=\"1590\" data-original=\"https://pic1.zhimg.com/v2-eeeea91c98ee8aa6a936df52b7b0b618_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1590&#39; height=&#39;160&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1590\" data-rawheight=\"160\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1590\" data-original=\"https://pic1.zhimg.com/v2-eeeea91c98ee8aa6a936df52b7b0b618_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-eeeea91c98ee8aa6a936df52b7b0b618_b.jpg\"/></figure><p>将Gk(x)代入损失函数，并对α进行求导，使其等于0，于是我们得到</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-62f93fc98d05d111c1cc89ecf735a1e4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1604\" data-rawheight=\"138\" class=\"origin_image zh-lightbox-thumb\" width=\"1604\" data-original=\"https://pic1.zhimg.com/v2-62f93fc98d05d111c1cc89ecf735a1e4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1604&#39; height=&#39;138&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1604\" data-rawheight=\"138\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1604\" data-original=\"https://pic1.zhimg.com/v2-62f93fc98d05d111c1cc89ecf735a1e4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-62f93fc98d05d111c1cc89ecf735a1e4_b.jpg\"/></figure><p>其中ek为我们前面介绍的<b>分类误差率</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-f583bca0f26ca0270a1f9db9b872ba85_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1588\" data-rawheight=\"178\" class=\"origin_image zh-lightbox-thumb\" width=\"1588\" data-original=\"https://pic2.zhimg.com/v2-f583bca0f26ca0270a1f9db9b872ba85_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1588&#39; height=&#39;178&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1588\" data-rawheight=\"178\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1588\" data-original=\"https://pic2.zhimg.com/v2-f583bca0f26ca0270a1f9db9b872ba85_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-f583bca0f26ca0270a1f9db9b872ba85_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-452d90009f9f110d76f36726e7be6aa0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1604\" data-rawheight=\"518\" class=\"origin_image zh-lightbox-thumb\" width=\"1604\" data-original=\"https://pic1.zhimg.com/v2-452d90009f9f110d76f36726e7be6aa0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1604&#39; height=&#39;518&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1604\" data-rawheight=\"518\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1604\" data-original=\"https://pic1.zhimg.com/v2-452d90009f9f110d76f36726e7be6aa0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-452d90009f9f110d76f36726e7be6aa0_b.jpg\"/></figure><h2>5.Adaboost算法正则化</h2><p>为了防止Adaboost过拟合，通常也会加入正则化项，这个正则化项通常称为步长(learning rate)。定义为ν,对于前面的弱学习器的迭代</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-eeeb97e308c37a13769b4989fa744326_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1598\" data-rawheight=\"130\" class=\"origin_image zh-lightbox-thumb\" width=\"1598\" data-original=\"https://pic3.zhimg.com/v2-eeeb97e308c37a13769b4989fa744326_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1598&#39; height=&#39;130&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1598\" data-rawheight=\"130\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1598\" data-original=\"https://pic3.zhimg.com/v2-eeeb97e308c37a13769b4989fa744326_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-eeeb97e308c37a13769b4989fa744326_b.jpg\"/></figure><p>如果我们加上正则化项，则有</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-62a4f0d5cf42c74719a470416fb6bb8c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1598\" data-rawheight=\"122\" class=\"origin_image zh-lightbox-thumb\" width=\"1598\" data-original=\"https://pic1.zhimg.com/v2-62a4f0d5cf42c74719a470416fb6bb8c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1598&#39; height=&#39;122&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1598\" data-rawheight=\"122\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1598\" data-original=\"https://pic1.zhimg.com/v2-62a4f0d5cf42c74719a470416fb6bb8c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-62a4f0d5cf42c74719a470416fb6bb8c_b.jpg\"/></figure><p>v的取值为(0,1]。对于同样的训练集，较小的ν意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。</p><h2>6.Sklearn实现Adaboost算法</h2><p>我们经常需要通过改变参数来让模型达到更好的分类或回归结果，具体参数设置可参考<a href=\"https://link.zhihu.com/?target=http%3A//scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">sklearn官方教程</a>。</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n<span class=\"kn\">import</span> <span class=\"nn\">matplotlib.pyplot</span> <span class=\"k\">as</span> <span class=\"nn\">plt</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.ensemble</span> <span class=\"k\">import</span> <span class=\"n\">AdaBoostClassifier</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.tree</span> <span class=\"k\">import</span> <span class=\"n\">DecisionTreeClassifier</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.datasets</span> <span class=\"k\">import</span> <span class=\"n\">make_gaussian_quantiles</span>\n\n<span class=\"c1\">#生成二维正态分布，生成的数据分为两类，500个样本，2个样本特征，协方差系数为2</span>\n<span class=\"n\">X1</span><span class=\"p\">,</span><span class=\"n\">y1</span><span class=\"o\">=</span><span class=\"n\">make_gaussian_quantiles</span><span class=\"p\">(</span><span class=\"n\">cov</span><span class=\"o\">=</span><span class=\"mf\">2.0</span><span class=\"p\">,</span><span class=\"n\">n_samples</span><span class=\"o\">=</span><span class=\"mi\">500</span><span class=\"p\">,</span>\n                              <span class=\"n\">n_features</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span><span class=\"n\">n_classes</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span>\n                              <span class=\"n\">random_state</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"c1\">#生成二维正态分布，生成的数据分为两类，500个样本，2个样本特征，协方差系数为1.5</span>\n<span class=\"n\">X2</span><span class=\"p\">,</span><span class=\"n\">y2</span><span class=\"o\">=</span><span class=\"n\">make_gaussian_quantiles</span><span class=\"p\">(</span><span class=\"n\">mean</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span><span class=\"mi\">3</span><span class=\"p\">),</span><span class=\"n\">cov</span><span class=\"o\">=</span><span class=\"mf\">1.5</span><span class=\"p\">,</span>\n                              <span class=\"n\">n_samples</span><span class=\"o\">=</span><span class=\"mi\">400</span><span class=\"p\">,</span><span class=\"n\">n_features</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span>\n                              <span class=\"n\">n_classes</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span><span class=\"n\">random_state</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"c1\">#将两组数据合为一组</span>\n<span class=\"n\">X</span><span class=\"o\">=</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">concatenate</span><span class=\"p\">((</span><span class=\"n\">X1</span><span class=\"p\">,</span><span class=\"n\">X2</span><span class=\"p\">))</span>\n<span class=\"n\">y</span><span class=\"o\">=</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">concatenate</span><span class=\"p\">((</span><span class=\"n\">y1</span><span class=\"p\">,</span><span class=\"o\">-</span><span class=\"n\">y2</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">))</span>\n\n<span class=\"c1\">#绘画生成的数据点</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">figure</span><span class=\"p\">()</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">scatter</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">[:,</span><span class=\"mi\">0</span><span class=\"p\">],</span><span class=\"n\">X</span><span class=\"p\">[:,</span><span class=\"mi\">1</span><span class=\"p\">],</span><span class=\"n\">marker</span><span class=\"o\">=</span><span class=\"s1\">&#39;o&#39;</span><span class=\"p\">,</span><span class=\"n\">c</span><span class=\"o\">=</span><span class=\"n\">y</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ca125586f76446482ba1e867bb01194c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"710\" class=\"origin_image zh-lightbox-thumb\" width=\"960\" data-original=\"https://pic1.zhimg.com/v2-ca125586f76446482ba1e867bb01194c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;960&#39; height=&#39;710&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"710\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"960\" data-original=\"https://pic1.zhimg.com/v2-ca125586f76446482ba1e867bb01194c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-ca125586f76446482ba1e867bb01194c_b.jpg\"/></figure><div class=\"highlight\"><pre><code class=\"language-text\"># 训练数据\nclf=AdaBoostClassifier(DecisionTreeClassifier(\n                       max_depth=2,min_samples_split=20,\n                       min_samples_leaf=5),algorithm=&#34;SAMME&#34;,\n                       n_estimators=200,learning_rate=0.8)\nclf.fit(X,y)\n\n#将训练结果绘画出来\nplt.figure()\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n                     np.arange(y_min, y_max, 0.02))\nZ = clf.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\ncs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\nplt.scatter(X[:, 0], X[:, 1], marker=&#39;o&#39;, c=y)\nplt.show()\n\n#训练模型的得分\nprint(clf.score(X,y))\n#0.913333333333</code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-cae0c4eb580bcc867f895478614ee141_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"940\" data-rawheight=\"700\" class=\"origin_image zh-lightbox-thumb\" width=\"940\" data-original=\"https://pic2.zhimg.com/v2-cae0c4eb580bcc867f895478614ee141_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;940&#39; height=&#39;700&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"940\" data-rawheight=\"700\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"940\" data-original=\"https://pic2.zhimg.com/v2-cae0c4eb580bcc867f895478614ee141_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-cae0c4eb580bcc867f895478614ee141_b.jpg\"/></figure><h2>7.Adaboost算法优缺点</h2><h2>7.1Adaboost优点</h2><ul><li>不容易发生过拟合。</li><li>Adaboost是一种有很高精度的分类器。</li><li>当使用简单分类器时，计算出的结果是可理解的。</li><li>可以使用各种方法构建子分类器，Adaboost算法提供的是框架。</li></ul><h2>7.2Adaboost缺点</h2><ul><li>训练时间过长。</li><li>执行效果依赖于弱分类器的选择。</li><li>对样本敏感，异常样本在迭代中可能会获得较高的权重，影响最终的强学习器的预测准确性。</li></ul><h2>8.推广</h2><p>更多内容请关注公众号<b>谓之小一</b>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p><p><a href=\"https://link.zhihu.com/?target=http%3A//weixin.qq.com/r/Ty_4oDPE2W6mrXfD93pd\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">weixin.qq.com/r/Ty_4oDP</span><span class=\"invisible\">E2W6mrXfD93pd</span><span class=\"ellipsis\"></span></a> (二维码自动识别)</p><p class=\"ztext-empty-paragraph\"><br/></p><p>文章参考</p><ul><li><a href=\"https://link.zhihu.com/?target=https%3A//www.cnblogs.com/pinard/p/6133937.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">集成学习之Adaboost算法原理小结</a></li></ul><p></p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "adaboost", 
                    "tagLink": "https://api.zhihu.com/topics/19719221"
                }, 
                {
                    "tag": "算法", 
                    "tagLink": "https://api.zhihu.com/topics/19553510"
                }
            ], 
            "comments": [
                {
                    "userName": "饭饭", 
                    "userLink": "https://www.zhihu.com/people/154117a036efdd19f7678a570833e015", 
                    "content": "<p>求问 N个弱分类器，可以是不同的模型吗 例如线性模型的和SVM ,DNN等组合 ； 另问  N个弱分类器的N是怎么确定的</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "谓之小一", 
                            "userLink": "https://www.zhihu.com/people/761dda159c5cec5fd926f761ab5f4ccf", 
                            "content": "<ol><li>N个弱分类器不可以是不同模型，因为第i+1个弱分类器是靠第i个弱分类器迭代得到，相同才能迭代。</li><li>N也就是我们弱学习器的迭代次数。如果N太小，容易欠拟合，N太大，又容易过拟合。一般默认是50，实际调参过程中，可以和learning_rate一起考虑。根据模型得分结果，N做相应改变。</li></ol>", 
                            "likes": 0, 
                            "replyToAuthor": "饭饭"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/36339161", 
            "userName": "谓之小一", 
            "userLink": "https://www.zhihu.com/people/761dda159c5cec5fd926f761ab5f4ccf", 
            "upvote": 13, 
            "title": "机器学习之梯度提升决策树(GBDT)", 
            "content": "<blockquote>由于较多公式，所以我将部分内容转为图片进行上传，请见谅。清晰版请访问<a href=\"https://link.zhihu.com/?target=https%3A//weizhixiaoyi.com\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">weizhixiaoyi.com</span><span class=\"invisible\"></span></a>查看，你也可以关注我公众号<b>谓之小一</b>，后台直接向我要pdf版本，如有相关问题可公众号后台询问，随时回答。</blockquote><h2>1.GBDT算法简介</h2><p><b>GBDT(Gradient Boosting Decision Tree)</b>是一种迭代的决策树算法，由多棵决策树组成，所有树的结论累加起来作为最终答案，我们根据其名字(<b>Gradient Boosting Decision Tree</b>)来展开推导过程。决策树(<b>Decision Tree</b>)我们已经不再陌生，在之前介绍到的<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3MjA2NTQzMw%3D%3D%26mid%3D2247483837%26idx%3D1%26sn%3Df73ca53c5d50f7cd090ba3bc0e17c56b%26chksm%3Dfcd7d24bcba05b5d157f93577bc7d41856dc4e19374c20f30167e9bfd9dc0956ed09d28431f3%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">机器学习之决策树(C4.5算法)</a>、<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3MjA2NTQzMw%3D%3D%26mid%3D2247483841%26idx%3D1%26sn%3Db67c59dc4284f0b363b2de881c5da729%26chksm%3Dfcd7d237cba05b21d80927e03e8b875e080cd0722496a5779eb3c8d285ea520902e813252ce0%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">机器学习之分类与回归树(CART)</a>、<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3MjA2NTQzMw%3D%3D%26mid%3D2247483845%26idx%3D1%26sn%3D5484385408d694ba03a8bdc3a03c2263%26chksm%3Dfcd7d233cba05b25b0f65289f8416466df9124b019b60704b4e3875d9864ff90e3388c666f66%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">机器学习之随机森林</a>中已经多次接触，在此不再赘述。但<b>Boosting</b>和<b>Gradient</b>方法是什么含义呢，又如何跟Decision Tree相结合?首先我们来了解集成学习中的Boosting概念。</p><h2>1.1集成学习之Boosting</h2><p>集成学习不是单独的机器学习方法，而是通过构建并结合多个机器学习器来完成任务，集成学习可以用于分类问题集成、回归问题集成、特征选取集成、异常点检测集成等方面。其思想是对于训练数据集，我们通过训练若干个个体学习器，通过一定的结合策略形成一个强学习器，以达到博采众长的目的。在<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3MjA2NTQzMw%3D%3D%26mid%3D2247483845%26idx%3D1%26sn%3D5484385408d694ba03a8bdc3a03c2263%26chksm%3Dfcd7d233cba05b25b0f65289f8416466df9124b019b60704b4e3875d9864ff90e3388c666f66%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">机器学习之随机森林</a>中我们已经用到集成学习中的bagging方法，此处我们详细介绍集成学习中的Boosting方法。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-1da21d57240c05a8d444dfafd9e7b91c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1412\" data-rawheight=\"662\" class=\"origin_image zh-lightbox-thumb\" width=\"1412\" data-original=\"https://pic1.zhimg.com/v2-1da21d57240c05a8d444dfafd9e7b91c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1412&#39; height=&#39;662&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1412\" data-rawheight=\"662\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1412\" data-original=\"https://pic1.zhimg.com/v2-1da21d57240c05a8d444dfafd9e7b91c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-1da21d57240c05a8d444dfafd9e7b91c_b.jpg\"/></figure><p>从上图可以看出，Boosting算法的工作机制是从训练集用初始权重训练出一个弱学习器1，根据弱学习器的学习误差率来更新训练样本的权重，使得之前弱学习器1中学习误差率高的训练样本点权重变高。然后这些误差率高的点在弱学习器2中得到更高的重视，利用调整权重后的训练集来训练弱学习器2。如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。了解Boosting方法后，我们便可将Boosting方法和Decision Tree相结合便可得到<b>Boosting Decision Tree</b>。</p><h2>1.2 Boosting Decision Tree</h2><p><b>提升树(Boosting Decision Tree)</b>由于输出样本是连续值，因此我们通过迭代多棵回归树来共同决策。在<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3MjA2NTQzMw%3D%3D%26mid%3D2247483841%26idx%3D1%26sn%3Db67c59dc4284f0b363b2de881c5da729%26chksm%3Dfcd7d237cba05b21d80927e03e8b875e080cd0722496a5779eb3c8d285ea520902e813252ce0%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">机器学习之分类与回归树(CART)</a>中我们已经详细推导分类树与回归树的构建过程，在此不再赘述。</p><p>我们利用平方误差来表示损失函数，其中每一棵回归树学习的是之前所有树的结论和残差，拟合得到一个当前的残差回归树。其中残差=真实值-预测值，提升树即是整个迭代过程生成的回归树的累加。</p><p>我们通过以下例子来详解算法过程，希望通过训练提升树来预测年龄。训练集是4个人，A、B、C、D年龄分别是14、16、24、26。样本中有购物金额、上网时长、经常到百度知道提问等特征。提升树的过程如下</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-616ff0c92bae55a4801bb43a0cbdb8eb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1071\" data-rawheight=\"722\" class=\"origin_image zh-lightbox-thumb\" width=\"1071\" data-original=\"https://pic4.zhimg.com/v2-616ff0c92bae55a4801bb43a0cbdb8eb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1071&#39; height=&#39;722&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1071\" data-rawheight=\"722\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1071\" data-original=\"https://pic4.zhimg.com/v2-616ff0c92bae55a4801bb43a0cbdb8eb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-616ff0c92bae55a4801bb43a0cbdb8eb_b.jpg\"/></figure><p>我们能够直观的看到，预测值等于所有树值的累加，如<b>A的预测值=树1左节点(15)+树2左节点(-1)=14</b>。因此给定当前决策树模型ft-1(x)，只需拟合决策树的残差，便可迭代得到提升树，算法过程如下</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-56e20c590e364e54ca836422419f60dd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1580\" data-rawheight=\"408\" class=\"origin_image zh-lightbox-thumb\" width=\"1580\" data-original=\"https://pic2.zhimg.com/v2-56e20c590e364e54ca836422419f60dd_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1580&#39; height=&#39;408&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1580\" data-rawheight=\"408\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1580\" data-original=\"https://pic2.zhimg.com/v2-56e20c590e364e54ca836422419f60dd_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-56e20c590e364e54ca836422419f60dd_b.jpg\"/></figure><p>我们介绍了<b>Boosting Decision Tree</b>的基本思路，但是没有解决损失函数拟合方法的问题。针对这个问题，Freidman提出用损失函数的负梯度来拟合本轮损失的近似值，进而拟合一个CART回归树。了解Boosting Decision Tree方法后，我们便可将Gradient与Boosting Decision Tree相结合得到<b>Gradient Boosting Decision Tree的负梯度拟合</b>。</p><h2>1.3GBDT负梯度拟合</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-aa1af5129569b799e46e89dcf88e01bb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1612\" data-rawheight=\"164\" class=\"origin_image zh-lightbox-thumb\" width=\"1612\" data-original=\"https://pic4.zhimg.com/v2-aa1af5129569b799e46e89dcf88e01bb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1612&#39; height=&#39;164&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1612\" data-rawheight=\"164\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1612\" data-original=\"https://pic4.zhimg.com/v2-aa1af5129569b799e46e89dcf88e01bb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-aa1af5129569b799e46e89dcf88e01bb_b.jpg\"/></figure><p>我们利用损失函数L的负梯度来拟合本轮损失函数的近似值，进而拟合一个CART回归树。其中第t轮的第i个样本的损失函数的负梯度表示为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-024228101f3ba2e1265be6242b58667d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1606\" data-rawheight=\"288\" class=\"origin_image zh-lightbox-thumb\" width=\"1606\" data-original=\"https://pic2.zhimg.com/v2-024228101f3ba2e1265be6242b58667d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1606&#39; height=&#39;288&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1606\" data-rawheight=\"288\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1606\" data-original=\"https://pic2.zhimg.com/v2-024228101f3ba2e1265be6242b58667d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-024228101f3ba2e1265be6242b58667d_b.jpg\"/></figure><p>针对每一个叶子节点中的样本，我们求出使损失函数最小，也就是拟合叶子节点最好的输出值ctj。其中决策树中叶节点值已经生成一遍，此步目的是稍加改变决策树中叶节点值，希望拟合的误差越来越小。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-e1abd710cd40147759335c4cfb883a59_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1572\" data-rawheight=\"142\" class=\"origin_image zh-lightbox-thumb\" width=\"1572\" data-original=\"https://pic2.zhimg.com/v2-e1abd710cd40147759335c4cfb883a59_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1572&#39; height=&#39;142&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1572\" data-rawheight=\"142\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1572\" data-original=\"https://pic2.zhimg.com/v2-e1abd710cd40147759335c4cfb883a59_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-e1abd710cd40147759335c4cfb883a59_b.jpg\"/></figure><p>这样我们便得到本轮的决策树拟合函数</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-9f320242951cee66e63ab201e27b830d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1598\" data-rawheight=\"144\" class=\"origin_image zh-lightbox-thumb\" width=\"1598\" data-original=\"https://pic2.zhimg.com/v2-9f320242951cee66e63ab201e27b830d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1598&#39; height=&#39;144&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1598\" data-rawheight=\"144\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1598\" data-original=\"https://pic2.zhimg.com/v2-9f320242951cee66e63ab201e27b830d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-9f320242951cee66e63ab201e27b830d_b.jpg\"/></figure><p>从而本轮最终得到的强学习器表达式如下</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-83cf10fab511d46b44739a7b91a78ab0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1596\" data-rawheight=\"150\" class=\"origin_image zh-lightbox-thumb\" width=\"1596\" data-original=\"https://pic1.zhimg.com/v2-83cf10fab511d46b44739a7b91a78ab0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1596&#39; height=&#39;150&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1596\" data-rawheight=\"150\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1596\" data-original=\"https://pic1.zhimg.com/v2-83cf10fab511d46b44739a7b91a78ab0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-83cf10fab511d46b44739a7b91a78ab0_b.jpg\"/></figure><p>通过损失函数的负梯度拟合，我们找到一种通用的拟合损失函数的方法，这样无论是分类问题还是回归问题，我们通过其损失函数的负梯度拟合，就可以用GBDT来解决我们的分类回归问题。</p><h2>2.GBDT回归算法</h2><p>通过上述GBDT负梯度拟合我们来总结下GBDT的回归算法，为什么没有加上分类算法是因为分类算法的输出是不连续的类别值，需要一些处理才能使用负梯度，我们将在下一节详细介绍GBDT分类算法。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-a421c4563b0d2c6157fab19e9adffeef_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1600\" data-rawheight=\"1516\" class=\"origin_image zh-lightbox-thumb\" width=\"1600\" data-original=\"https://pic4.zhimg.com/v2-a421c4563b0d2c6157fab19e9adffeef_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1600&#39; height=&#39;1516&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1600\" data-rawheight=\"1516\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1600\" data-original=\"https://pic4.zhimg.com/v2-a421c4563b0d2c6157fab19e9adffeef_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-a421c4563b0d2c6157fab19e9adffeef_b.jpg\"/></figure><h2>3.GBDT分类算法</h2><p>GBDT分类算法在思想上和回归算法没有区别，但是由于样本输出不是连续的值，而是离散的类别，导致我们无法直接从输出类别去拟合类别输出的误差。为解决此问题，我们尝试用类似于逻辑回归的对数似然损失函数的方法,也就是说我们用的是类别的预测概率值和真实概率值来拟合损失函数。对于对数似然损失函数，我们有二元分类和多元分类的区别。</p><h2>3.1二元GBDT分类算法</h2><p>对于二元GBDT，如果用类似于逻辑回归的对数似然损失函数，则损失函数表示为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-1babfef6bf4bfb4e220f98c9932e726c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1618\" data-rawheight=\"314\" class=\"origin_image zh-lightbox-thumb\" width=\"1618\" data-original=\"https://pic1.zhimg.com/v2-1babfef6bf4bfb4e220f98c9932e726c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1618&#39; height=&#39;314&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1618\" data-rawheight=\"314\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1618\" data-original=\"https://pic1.zhimg.com/v2-1babfef6bf4bfb4e220f98c9932e726c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-1babfef6bf4bfb4e220f98c9932e726c_b.jpg\"/></figure><p>对于生成的决策树，我们各个叶子节点的最佳残差拟合值为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-1568e95819d485efaafb7aa774d41bf7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1630\" data-rawheight=\"132\" class=\"origin_image zh-lightbox-thumb\" width=\"1630\" data-original=\"https://pic4.zhimg.com/v2-1568e95819d485efaafb7aa774d41bf7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1630&#39; height=&#39;132&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1630\" data-rawheight=\"132\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1630\" data-original=\"https://pic4.zhimg.com/v2-1568e95819d485efaafb7aa774d41bf7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-1568e95819d485efaafb7aa774d41bf7_b.jpg\"/></figure><p>由于上式比较难优化，我们一般使用近似值代替</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-af6ab5d15d6980173820ef2441e70bf4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1632\" data-rawheight=\"160\" class=\"origin_image zh-lightbox-thumb\" width=\"1632\" data-original=\"https://pic1.zhimg.com/v2-af6ab5d15d6980173820ef2441e70bf4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1632&#39; height=&#39;160&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1632\" data-rawheight=\"160\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1632\" data-original=\"https://pic1.zhimg.com/v2-af6ab5d15d6980173820ef2441e70bf4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-af6ab5d15d6980173820ef2441e70bf4_b.jpg\"/></figure><p>除了负梯度计算和叶子节点的最佳残差拟合的线性搜索外，二元GBDT分类和GBDT回归算法过程相同。</p><h2>3.2多元GBDT分类算法</h2><p>多元GBDT要比二元GBDT复杂一些，对应的是多元逻辑回归和二元逻辑回归的复杂度差别。假如类别数为K，则我们的对数似然函数为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-0e304012d3130a1546e0306e6b25c4ac_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1608\" data-rawheight=\"152\" class=\"origin_image zh-lightbox-thumb\" width=\"1608\" data-original=\"https://pic1.zhimg.com/v2-0e304012d3130a1546e0306e6b25c4ac_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1608&#39; height=&#39;152&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1608\" data-rawheight=\"152\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1608\" data-original=\"https://pic1.zhimg.com/v2-0e304012d3130a1546e0306e6b25c4ac_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-0e304012d3130a1546e0306e6b25c4ac_b.jpg\"/></figure><p>其中如果样本输出类别为k，则yk=1。第k类的概率pk(x)的表达式为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-3fb1b825d5b21ee02f35780829be162a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1618\" data-rawheight=\"150\" class=\"origin_image zh-lightbox-thumb\" width=\"1618\" data-original=\"https://pic3.zhimg.com/v2-3fb1b825d5b21ee02f35780829be162a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1618&#39; height=&#39;150&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1618\" data-rawheight=\"150\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1618\" data-original=\"https://pic3.zhimg.com/v2-3fb1b825d5b21ee02f35780829be162a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-3fb1b825d5b21ee02f35780829be162a_b.jpg\"/></figure><p>集合上两式，我们可以计算出第t轮的第i个样本对应类别l的负梯度误差为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-0034749477fb6f75924ae8f6991e397b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1584\" data-rawheight=\"154\" class=\"origin_image zh-lightbox-thumb\" width=\"1584\" data-original=\"https://pic4.zhimg.com/v2-0034749477fb6f75924ae8f6991e397b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1584&#39; height=&#39;154&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1584\" data-rawheight=\"154\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1584\" data-original=\"https://pic4.zhimg.com/v2-0034749477fb6f75924ae8f6991e397b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-0034749477fb6f75924ae8f6991e397b_b.jpg\"/></figure><p>其实这里的误差就是样本i对应类别l的真实概率和t-1轮预测概率的差值。对于生成的决策树，我们各个叶子节点的最佳残差拟合值为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-8b6e656546bf824e82c4c808014156fa_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1596\" data-rawheight=\"160\" class=\"origin_image zh-lightbox-thumb\" width=\"1596\" data-original=\"https://pic3.zhimg.com/v2-8b6e656546bf824e82c4c808014156fa_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1596&#39; height=&#39;160&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1596\" data-rawheight=\"160\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1596\" data-original=\"https://pic3.zhimg.com/v2-8b6e656546bf824e82c4c808014156fa_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-8b6e656546bf824e82c4c808014156fa_b.jpg\"/></figure><p>由于上式比较难优化，我们用近似值代替</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-afc68be53988578fda032374d8c3031b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1596\" data-rawheight=\"156\" class=\"origin_image zh-lightbox-thumb\" width=\"1596\" data-original=\"https://pic4.zhimg.com/v2-afc68be53988578fda032374d8c3031b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1596&#39; height=&#39;156&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1596\" data-rawheight=\"156\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1596\" data-original=\"https://pic4.zhimg.com/v2-afc68be53988578fda032374d8c3031b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-afc68be53988578fda032374d8c3031b_b.jpg\"/></figure><p>除了负梯度计算和叶子节点的最佳残差拟合的线性搜索，多元GBDT分类和二元GBDT分类以及GBDT回归算法过程相同。</p><h2>4.GBDT损失函数</h2><p>对于<b>回归算法</b>，常用损失函数有均方差、绝对损失、Huber损失和分位数损失。</p><ul><li>均方差损失。</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-f7a23fc97fab4788cc32a9236ac6ab20_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1604\" data-rawheight=\"162\" class=\"origin_image zh-lightbox-thumb\" width=\"1604\" data-original=\"https://pic1.zhimg.com/v2-f7a23fc97fab4788cc32a9236ac6ab20_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1604&#39; height=&#39;162&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1604\" data-rawheight=\"162\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1604\" data-original=\"https://pic1.zhimg.com/v2-f7a23fc97fab4788cc32a9236ac6ab20_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-f7a23fc97fab4788cc32a9236ac6ab20_b.jpg\"/></figure><ul><li>绝对损失和对应的负梯度误差。</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-e6569b667f350e860bc720814955b5a2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1582\" data-rawheight=\"230\" class=\"origin_image zh-lightbox-thumb\" width=\"1582\" data-original=\"https://pic3.zhimg.com/v2-e6569b667f350e860bc720814955b5a2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1582&#39; height=&#39;230&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1582\" data-rawheight=\"230\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1582\" data-original=\"https://pic3.zhimg.com/v2-e6569b667f350e860bc720814955b5a2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-e6569b667f350e860bc720814955b5a2_b.jpg\"/></figure><ul><li>Huber损失是均方差和绝对损失的折衷产物，对于远离中心的异常点，采用绝对损失，而中心点附近采用均方差。这个界限一般用分位数点来度量，损失函数和对应的负梯度误差如下</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-4e8ca47f800a72abe9322fa5827a557d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1600\" data-rawheight=\"320\" class=\"origin_image zh-lightbox-thumb\" width=\"1600\" data-original=\"https://pic2.zhimg.com/v2-4e8ca47f800a72abe9322fa5827a557d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1600&#39; height=&#39;320&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1600\" data-rawheight=\"320\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1600\" data-original=\"https://pic2.zhimg.com/v2-4e8ca47f800a72abe9322fa5827a557d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-4e8ca47f800a72abe9322fa5827a557d_b.jpg\"/></figure><ul><li>分位数损失和负梯度误差如下所示。其中其中theta为分位数，需要我们在回归前指定。</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ff318c9d9d6bdb40c4d978e9d512ef33_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1562\" data-rawheight=\"294\" class=\"origin_image zh-lightbox-thumb\" width=\"1562\" data-original=\"https://pic4.zhimg.com/v2-ff318c9d9d6bdb40c4d978e9d512ef33_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1562&#39; height=&#39;294&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1562\" data-rawheight=\"294\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1562\" data-original=\"https://pic4.zhimg.com/v2-ff318c9d9d6bdb40c4d978e9d512ef33_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-ff318c9d9d6bdb40c4d978e9d512ef33_b.jpg\"/></figure><p>对于Huber损失和分位数损失，主要用于健壮回归，也就是减少异常点对损失函数的影响。</p><p>对于分类算法，常用损失函数有指数损失函数和对数损失函数。</p><ul><li>对数损失函数，分为二元分类和多元分类两种，我们已在上述3.1和3.2节进行介绍。</li><li>指数损失函数</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-42ed48afe3b0a4f6e73bf706539ecd2c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1500\" data-rawheight=\"108\" class=\"origin_image zh-lightbox-thumb\" width=\"1500\" data-original=\"https://pic1.zhimg.com/v2-42ed48afe3b0a4f6e73bf706539ecd2c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1500&#39; height=&#39;108&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1500\" data-rawheight=\"108\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1500\" data-original=\"https://pic1.zhimg.com/v2-42ed48afe3b0a4f6e73bf706539ecd2c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-42ed48afe3b0a4f6e73bf706539ecd2c_b.jpg\"/></figure><h2>5.GBDT正则化</h2><p>针对GBDT正则化，我们通过子采样比例方法和定义步长v方法来防止过拟合。</p><ul><li><b>子采样比例:</b>通过不放回抽样的子采样比例（subsample），取值为(0,1]。如果取值为1，则全部样本都使用。如果取值小于1，利用部分样本去做GBDT的决策树拟合。选择小于1的比例可以减少方差，防止过拟合，但是会增加样本拟合的偏差。因此取值不能太低，推荐在[0.5, 0.8]之间。</li><li><b>定义步长v:</b>针对弱学习器的迭代，我们定义步长v，取值为(0,1]。对于同样的训练集学习效果，较小的v意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-955f842076ce802025d7731aa95ea0f1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1598\" data-rawheight=\"92\" class=\"origin_image zh-lightbox-thumb\" width=\"1598\" data-original=\"https://pic2.zhimg.com/v2-955f842076ce802025d7731aa95ea0f1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1598&#39; height=&#39;92&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1598\" data-rawheight=\"92\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1598\" data-original=\"https://pic2.zhimg.com/v2-955f842076ce802025d7731aa95ea0f1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-955f842076ce802025d7731aa95ea0f1_b.jpg\"/></figure><h2>6.Sklearn实现GBDT算法</h2><p>我们经常需要通过改变参数来让模型达到更好的分类或回归结果，具体参数设置可参考<a href=\"https://link.zhihu.com/?target=http%3A//scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">sklearn官方教程</a>。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">from</span> <span class=\"nn\">sklearn.ensemble</span> <span class=\"kn\">import</span> <span class=\"n\">GradientBoostingRegressor</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.datasets</span> <span class=\"kn\">import</span> <span class=\"n\">make_regression</span>\n\n<span class=\"n\">X</span><span class=\"p\">,</span><span class=\"n\">y</span><span class=\"o\">=</span><span class=\"n\">make_regression</span><span class=\"p\">(</span><span class=\"n\">n_samples</span><span class=\"o\">=</span><span class=\"mi\">1000</span><span class=\"p\">,</span><span class=\"n\">n_features</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span>\n                    <span class=\"n\">n_informative</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span><span class=\"n\">random_state</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">:</span><span class=\"mi\">10</span><span class=\"p\">],</span><span class=\"n\">y</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">:</span><span class=\"mi\">10</span><span class=\"p\">])</span>\n<span class=\"c1\">### X Number</span>\n<span class=\"c1\"># [[-0.34323505  0.73129362  0.07077408 -0.78422138]</span>\n<span class=\"c1\">#  [-0.02852887 -0.30937759 -0.32473027  0.2847906 ]</span>\n<span class=\"c1\">#  [ 2.00921856  0.42218461 -0.48981473 -0.85152258]</span>\n<span class=\"c1\">#  [ 0.15081821  0.54565732 -0.25547079 -0.35687153]</span>\n<span class=\"c1\">#  [-0.97240289  1.49613964  1.34622107 -1.49026539]</span>\n<span class=\"c1\">#  [ 1.00610171  1.42889242  0.02479266 -0.69043143]</span>\n<span class=\"c1\">#  [ 0.77083696  0.96234174  0.24316822  0.45730965]</span>\n<span class=\"c1\">#  [ 0.8717585  -0.6374392   0.37450029  0.74681383]</span>\n<span class=\"c1\">#  [ 0.69178453 -0.23550331  0.56438821  2.01124319]</span>\n<span class=\"c1\">#  [ 0.52904524  0.14844958  0.42262862  0.47689837]]</span>\n\n<span class=\"c1\">### Y Number</span>\n<span class=\"c1\"># [ -12.63291254    2.12821377  -34.59433043    6.2021494   -18.03000376</span>\n<span class=\"c1\">#    32.9524098    85.33550027   15.3410771   124.47105816   40.98334709]</span>\n\n\n<span class=\"n\">clf</span><span class=\"o\">=</span><span class=\"n\">GradientBoostingRegressor</span><span class=\"p\">(</span><span class=\"n\">n_estimators</span><span class=\"o\">=</span><span class=\"mi\">150</span><span class=\"p\">,</span><span class=\"n\">learning_rate</span><span class=\"o\">=</span><span class=\"mf\">0.6</span><span class=\"p\">,</span>\n                              <span class=\"n\">max_depth</span><span class=\"o\">=</span><span class=\"mi\">15</span><span class=\"p\">,</span><span class=\"n\">random_state</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"n\">loss</span><span class=\"o\">=</span><span class=\"s1\">&#39;ls&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">clf</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span><span class=\"n\">y</span><span class=\"p\">)</span>\n\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">clf</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">([[</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">]]))</span>\n<span class=\"c1\"># [ 25.62761791]</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">clf</span><span class=\"o\">.</span><span class=\"n\">score</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span><span class=\"n\">y</span><span class=\"p\">))</span>\n<span class=\"c1\"># 0.999999999987</span></code></pre></div><h2>7.GBDT优缺点</h2><h2>7.1优点</h2><ul><li>相对少的调参时间情况下可以得到较高的准确率。</li><li>可灵活处理各种类型数据，包括连续值和离散值，使用范围广。</li><li>可使用一些健壮的损失函数，对异常值的鲁棒性较强，比如Huber损失函数。</li></ul><h2>7.2缺点</h2><ul><li>弱学习器之间存在依赖关系，难以并行训练数据。</li></ul><h2>8.推广</h2><p>更多内容请关注公众号<b>谓之小一</b>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p><p><a href=\"https://link.zhihu.com/?target=http%3A//weixin.qq.com/r/Ty_4oDPE2W6mrXfD93pd\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">weixin.qq.com/r/Ty_4oDP</span><span class=\"invisible\">E2W6mrXfD93pd</span><span class=\"ellipsis\"></span></a> (二维码自动识别)</p><p class=\"ztext-empty-paragraph\"><br/></p><p>文章参考</p><ul><li><a href=\"https://link.zhihu.com/?target=https%3A//www.cnblogs.com/pinard/p/6140514.html%23%21comments\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">刘建平Pinard_梯度提升树(GBDT)原理小结</a></li><li><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/taoqick/article/details/72822727\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">taotick_GBDT梯度提升决策树</a></li></ul>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "gbdt", 
                    "tagLink": "https://api.zhihu.com/topics/20066371"
                }, 
                {
                    "tag": "boosting", 
                    "tagLink": "https://api.zhihu.com/topics/20049590"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/36283018", 
            "userName": "谓之小一", 
            "userLink": "https://www.zhihu.com/people/761dda159c5cec5fd926f761ab5f4ccf", 
            "upvote": 4, 
            "title": "机器学习之随机森林", 
            "content": "<blockquote>由于较多公式，所以我将部分内容转为图片进行上传，请见谅。清晰版请访问<a href=\"https://link.zhihu.com/?target=https%3A//weizhixiaoyi.com\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">weizhixiaoyi.com</span><span class=\"invisible\"></span></a>查看，你也可以关注我公众号<b>谓之小一</b>，后台直接向我要pdf版本，如有相关问题可公众号后台询问，随时回答。</blockquote><h2>1.随机森林简介</h2><p>随机森林(Random Forest)是一个非常灵活的机器学习方法，从市场营销到医疗保险有着众多的应用。例如用于市场营销对客户获取和存留建模或预测病人的疾病风险和易感性。随机森林能够用于<b>分类</b>和<b>回归</b>问题，可以处理大量特征，并能够帮助估计用于建模数据变量的重要性。我们先了解随机森林中<b>森林</b>和<b>随机</b>的概念。</p><h2>1.1集成学习</h2><p><b>集成学习</b>是将多个模型进行组合来解决单一的预测问题。其原理是生成多个分类器模型，各自独立的学习并做出预测，这些预测最后结合起来得到预测结果，因此和单独分类器相比结果会更好。</p><p>单个决策树在机器学习中比作普通学习，那么成百上千棵决策树便叫做集成学习，成百上千棵树也便组成了<b>森林</b>。</p><h2>1.2随机决策树</h2><p>我们知道随机森林是将其他的模型进行聚合， 但具体是哪种模型呢？从其名称也可以看出，随机森林聚合的是分类（或回归） 树。</p><p>那么我们如何生成成百上千棵决策树呢？如果选择样本集N中全部数据生成众多决策树，那么生成的决策树都相同，得到预测结果便没有实际意义。因此我们采用的方法是从样本集N中有放回的随机采样选出n个样本(n随机的概念。</p><h2>1.3随机森林算法</h2><p>由于这些树是随机生成的，大部分的树对解决分类或回归问题是没有意义的，那么生成上万的树有什么好处呢？</p><p>好处便是生成的决策树中有少数非常好的决策树。当你要做预测的时候，新的观察值随着决策树自上而下的预测并被赋予一个预测值或标签。一旦森林中的每棵树都有了预测值或标签，所有的预测结果将被归总到一起，所有树的投票做为最终的预测结果。简单来说，99.9%不相关的树做出的预测结果涵盖所有的情况，这些预测结果将会彼此抵消。少数优秀的树将会脱颖而出，从而得到一个好的预测结果。随机森林算法如下所示</p><ul><li>从样本集N中有放回随机采样选出n个样本。</li><li>从所有特征中随机选择k个特征，对选出的样本利用这些特征建立决策树(一般是CART方法)。</li><li>重复以上两步m次，生成m棵决策树，形成随机森林，其中生成的决策树不剪枝。</li><li>对于新数据，经过每棵决策树投票分类</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-d7f033b90dedd14d6511292254a5da27_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1278\" data-rawheight=\"956\" class=\"origin_image zh-lightbox-thumb\" width=\"1278\" data-original=\"https://pic4.zhimg.com/v2-d7f033b90dedd14d6511292254a5da27_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1278&#39; height=&#39;956&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1278\" data-rawheight=\"956\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1278\" data-original=\"https://pic4.zhimg.com/v2-d7f033b90dedd14d6511292254a5da27_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-d7f033b90dedd14d6511292254a5da27_b.jpg\"/></figure><h2>2.CART算法</h2><p>随机森林包含众多决策树，能够用于<b>分类</b>和<b>回归</b>问题。决策树算法一般包括ID3、C4.5、CART算法，这里我们给出CART(分类与回归树)算法的详细推导过程。</p><h2>2.1CART分类树算法详解</h2><p>CART分类树预测分类离散型数据，采用基尼指数选择最优特征，同时决定该特征的最优二值切分点。分类过程中，假设有K个类，样本点属于第k个类的概率为Pk，则概率分布的基尼指数定义为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-780d955260d9a2ba8508c1601588b88a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1560\" data-rawheight=\"174\" class=\"origin_image zh-lightbox-thumb\" width=\"1560\" data-original=\"https://pic3.zhimg.com/v2-780d955260d9a2ba8508c1601588b88a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1560&#39; height=&#39;174&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1560\" data-rawheight=\"174\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1560\" data-original=\"https://pic3.zhimg.com/v2-780d955260d9a2ba8508c1601588b88a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-780d955260d9a2ba8508c1601588b88a_b.jpg\"/></figure><p>根据基尼指数定义，可以得到样本集合D的基尼指数，其中Ck表示数据集D中属于第k类的样本子集。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-95300197189b4b1b65eb42d1a6bbd7fd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1586\" data-rawheight=\"160\" class=\"origin_image zh-lightbox-thumb\" width=\"1586\" data-original=\"https://pic2.zhimg.com/v2-95300197189b4b1b65eb42d1a6bbd7fd_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1586&#39; height=&#39;160&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1586\" data-rawheight=\"160\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1586\" data-original=\"https://pic2.zhimg.com/v2-95300197189b4b1b65eb42d1a6bbd7fd_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-95300197189b4b1b65eb42d1a6bbd7fd_b.jpg\"/></figure><p>如果数据集D根据特征A在某一取值a上进行分割，得到D1,D2两部分后，那么在特征A下集合D的基尼系数如下所示。其中基尼系数Gini(D)表示集合D的不确定性，基尼系数Gini(D,A)表示A=a分割后集合D的不确定性。基尼指数越大，样本集合的不确定性越大。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-a6f4d87b5eee51c147fb91e31e51d48e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1590\" data-rawheight=\"156\" class=\"origin_image zh-lightbox-thumb\" width=\"1590\" data-original=\"https://pic3.zhimg.com/v2-a6f4d87b5eee51c147fb91e31e51d48e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1590&#39; height=&#39;156&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1590\" data-rawheight=\"156\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1590\" data-original=\"https://pic3.zhimg.com/v2-a6f4d87b5eee51c147fb91e31e51d48e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-a6f4d87b5eee51c147fb91e31e51d48e_b.jpg\"/></figure><p>对于属性A，分别计算任意属性值将数据集划分为两部分之后的Gain_Gini，选取其中的最小值，作为属性A得到的最优二分方案。然后对于训练集S，计算所有属性的最优二分方案，选取其中的最小值，作为样本及S的最优二分方案。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-8ce2b82f547858f9e604151bbd90bd23_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1602\" data-rawheight=\"262\" class=\"origin_image zh-lightbox-thumb\" width=\"1602\" data-original=\"https://pic4.zhimg.com/v2-8ce2b82f547858f9e604151bbd90bd23_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1602&#39; height=&#39;262&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1602\" data-rawheight=\"262\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1602\" data-original=\"https://pic4.zhimg.com/v2-8ce2b82f547858f9e604151bbd90bd23_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-8ce2b82f547858f9e604151bbd90bd23_b.jpg\"/></figure><h2>2.2CART分类树实例详解</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-37fcb1d050923b846f344d83384c27bc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1246\" data-rawheight=\"1274\" class=\"origin_image zh-lightbox-thumb\" width=\"1246\" data-original=\"https://pic1.zhimg.com/v2-37fcb1d050923b846f344d83384c27bc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1246&#39; height=&#39;1274&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1246\" data-rawheight=\"1274\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1246\" data-original=\"https://pic1.zhimg.com/v2-37fcb1d050923b846f344d83384c27bc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-37fcb1d050923b846f344d83384c27bc_b.jpg\"/></figure><p>针对上述离散型数据，按照<b>体温为恒温和非恒温</b>进行划分。其中恒温时包括哺乳类5个、鸟类2个，非恒温时包括爬行类3个、鱼类3个、两栖类2个，如下所示我们计算D1,D2的基尼指数。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-f477552326b3f2f39404e3fb7de3787c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1584\" data-rawheight=\"280\" class=\"origin_image zh-lightbox-thumb\" width=\"1584\" data-original=\"https://pic1.zhimg.com/v2-f477552326b3f2f39404e3fb7de3787c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1584&#39; height=&#39;280&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1584\" data-rawheight=\"280\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1584\" data-original=\"https://pic1.zhimg.com/v2-f477552326b3f2f39404e3fb7de3787c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-f477552326b3f2f39404e3fb7de3787c_b.jpg\"/></figure><p>然后计算得到特征<b>体温</b>下数据集的Gini指数，最后我们选择Gain_Gini最小的特征和相应的划分。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-0d3d4c5093aa272f4072703afdee8b36_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1588\" data-rawheight=\"134\" class=\"origin_image zh-lightbox-thumb\" width=\"1588\" data-original=\"https://pic3.zhimg.com/v2-0d3d4c5093aa272f4072703afdee8b36_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1588&#39; height=&#39;134&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1588\" data-rawheight=\"134\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1588\" data-original=\"https://pic3.zhimg.com/v2-0d3d4c5093aa272f4072703afdee8b36_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-0d3d4c5093aa272f4072703afdee8b36_b.jpg\"/></figure><h2>2.3CART回归树算法详解</h2><p>CART回归树预测回归连续型数据，假设X与Y分别是输入和输出变量，并且Y是连续变量。在训练数据集所在的输入空间中，递归的将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-5283b27704fbd79178e968aa379f7fdc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1574\" data-rawheight=\"116\" class=\"origin_image zh-lightbox-thumb\" width=\"1574\" data-original=\"https://pic1.zhimg.com/v2-5283b27704fbd79178e968aa379f7fdc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1574&#39; height=&#39;116&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1574\" data-rawheight=\"116\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1574\" data-original=\"https://pic1.zhimg.com/v2-5283b27704fbd79178e968aa379f7fdc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-5283b27704fbd79178e968aa379f7fdc_b.jpg\"/></figure><p><b>选择最优切分变量j与切分点s</b>：遍历变量j，对规定的切分变量j扫描切分点s，选择使下式得到最小值时的(j,s)对。其中Rm是被划分的输入空间，cm是空间Rm对应的固定输出值。</p><figure data-size=\"small\"><noscript><img src=\"https://pic3.zhimg.com/v2-0f0e50eee43308c563b468c27ba39f82_b.jpg\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"944\" data-rawheight=\"158\" class=\"origin_image zh-lightbox-thumb\" width=\"944\" data-original=\"https://pic3.zhimg.com/v2-0f0e50eee43308c563b468c27ba39f82_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;944&#39; height=&#39;158&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"944\" data-rawheight=\"158\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"944\" data-original=\"https://pic3.zhimg.com/v2-0f0e50eee43308c563b468c27ba39f82_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-0f0e50eee43308c563b468c27ba39f82_b.jpg\"/></figure><p><b>用选定的(j,s)对，划分区域并决定相应的输出值</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-9907b77a00a79e03c51fbf28a696998a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1568\" data-rawheight=\"372\" class=\"origin_image zh-lightbox-thumb\" width=\"1568\" data-original=\"https://pic3.zhimg.com/v2-9907b77a00a79e03c51fbf28a696998a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1568&#39; height=&#39;372&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1568\" data-rawheight=\"372\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1568\" data-original=\"https://pic3.zhimg.com/v2-9907b77a00a79e03c51fbf28a696998a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-9907b77a00a79e03c51fbf28a696998a_b.jpg\"/></figure><p><b>继续对两个子区域调用上述步骤</b>，<b>将输入空间划分为M个区域R1,R2,…,Rm，生成决策树。</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-2246d6ccdbc8da8059fcc9fd8aadc9b8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1588\" data-rawheight=\"174\" class=\"origin_image zh-lightbox-thumb\" width=\"1588\" data-original=\"https://pic1.zhimg.com/v2-2246d6ccdbc8da8059fcc9fd8aadc9b8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1588&#39; height=&#39;174&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1588\" data-rawheight=\"174\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1588\" data-original=\"https://pic1.zhimg.com/v2-2246d6ccdbc8da8059fcc9fd8aadc9b8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-2246d6ccdbc8da8059fcc9fd8aadc9b8_b.jpg\"/></figure><p>当输入空间划分确定时，可以用<b>平方误差</b>来表示回归树对于训练数据的预测方法，用平方误差最小的准则求解每个单元上的最优输出值。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-851d114f504be680d16ce3b4f15d474f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1608\" data-rawheight=\"144\" class=\"origin_image zh-lightbox-thumb\" width=\"1608\" data-original=\"https://pic4.zhimg.com/v2-851d114f504be680d16ce3b4f15d474f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1608&#39; height=&#39;144&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1608\" data-rawheight=\"144\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1608\" data-original=\"https://pic4.zhimg.com/v2-851d114f504be680d16ce3b4f15d474f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-851d114f504be680d16ce3b4f15d474f_b.jpg\"/></figure><h2>2.4CART回归树实例详解</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-297ce1628209a6fba9e8ba7e04d6eb6b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1242\" data-rawheight=\"192\" class=\"origin_image zh-lightbox-thumb\" width=\"1242\" data-original=\"https://pic4.zhimg.com/v2-297ce1628209a6fba9e8ba7e04d6eb6b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1242&#39; height=&#39;192&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1242\" data-rawheight=\"192\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1242\" data-original=\"https://pic4.zhimg.com/v2-297ce1628209a6fba9e8ba7e04d6eb6b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-297ce1628209a6fba9e8ba7e04d6eb6b_b.jpg\"/></figure><p>考虑如上所示的连续性变量，根据给定的数据点，考虑<b>1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5</b>切分点。对各切分点依次求出<b>R1,R2,c1,c2及m(s)</b>，例如当切分点s=1.5时，得到R1={1},R2={2,3,4,5,6,7,8,9,10}，其中c1,c2,m(s)如下所示。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-306c3f89443131a6260f2a8be559c583_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1538\" data-rawheight=\"470\" class=\"origin_image zh-lightbox-thumb\" width=\"1538\" data-original=\"https://pic4.zhimg.com/v2-306c3f89443131a6260f2a8be559c583_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1538&#39; height=&#39;470&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1538\" data-rawheight=\"470\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1538\" data-original=\"https://pic4.zhimg.com/v2-306c3f89443131a6260f2a8be559c583_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-306c3f89443131a6260f2a8be559c583_b.jpg\"/></figure><p><b>依次改变(j,s)对，可以得到s及m(s)的计算结果</b>，如下表所示。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-957d3265bbede6933088b9ccb4c36c94_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1244\" data-rawheight=\"192\" class=\"origin_image zh-lightbox-thumb\" width=\"1244\" data-original=\"https://pic1.zhimg.com/v2-957d3265bbede6933088b9ccb4c36c94_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1244&#39; height=&#39;192&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1244\" data-rawheight=\"192\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1244\" data-original=\"https://pic1.zhimg.com/v2-957d3265bbede6933088b9ccb4c36c94_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-957d3265bbede6933088b9ccb4c36c94_b.jpg\"/></figure><p>当x=6.5时，此时R1={1,2,3,4,5,6},R2={7,8,9,10},c1=6.24,c2=8.9。<b>回归树T1(x)</b>为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ced4e485ca0247e001abee28a225e0c0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1628\" data-rawheight=\"258\" class=\"origin_image zh-lightbox-thumb\" width=\"1628\" data-original=\"https://pic1.zhimg.com/v2-ced4e485ca0247e001abee28a225e0c0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1628&#39; height=&#39;258&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1628\" data-rawheight=\"258\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1628\" data-original=\"https://pic1.zhimg.com/v2-ced4e485ca0247e001abee28a225e0c0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-ced4e485ca0247e001abee28a225e0c0_b.jpg\"/></figure><p><b>然后我们利用f1(x)拟合训练数据的残差</b>，如下表所示</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-e52211e6c4078a82a7862716e64fd5b1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1242\" data-rawheight=\"192\" class=\"origin_image zh-lightbox-thumb\" width=\"1242\" data-original=\"https://pic2.zhimg.com/v2-e52211e6c4078a82a7862716e64fd5b1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1242&#39; height=&#39;192&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1242\" data-rawheight=\"192\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1242\" data-original=\"https://pic2.zhimg.com/v2-e52211e6c4078a82a7862716e64fd5b1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-e52211e6c4078a82a7862716e64fd5b1_b.jpg\"/></figure><p><b>用f1(x)拟合训练数据得到平方误差</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-73d4172314554e26884e59f71e117ade_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1622\" data-rawheight=\"158\" class=\"origin_image zh-lightbox-thumb\" width=\"1622\" data-original=\"https://pic3.zhimg.com/v2-73d4172314554e26884e59f71e117ade_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1622&#39; height=&#39;158&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1622\" data-rawheight=\"158\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1622\" data-original=\"https://pic3.zhimg.com/v2-73d4172314554e26884e59f71e117ade_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-73d4172314554e26884e59f71e117ade_b.jpg\"/></figure><p>第二步求T2(x)与求T1(x)方法相同，只是拟合的数据是上表的残差。可以得到</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-d6dabacc2b5a64586eac9e52f0c2f50b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1576\" data-rawheight=\"328\" class=\"origin_image zh-lightbox-thumb\" width=\"1576\" data-original=\"https://pic4.zhimg.com/v2-d6dabacc2b5a64586eac9e52f0c2f50b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1576&#39; height=&#39;328&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1576\" data-rawheight=\"328\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1576\" data-original=\"https://pic4.zhimg.com/v2-d6dabacc2b5a64586eac9e52f0c2f50b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-d6dabacc2b5a64586eac9e52f0c2f50b_b.jpg\"/></figure><p>用f2(x)拟合训练数据的平方误差</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-1be746ed0d18e26d6c59dd18dea5541d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1584\" data-rawheight=\"164\" class=\"origin_image zh-lightbox-thumb\" width=\"1584\" data-original=\"https://pic2.zhimg.com/v2-1be746ed0d18e26d6c59dd18dea5541d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1584&#39; height=&#39;164&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1584\" data-rawheight=\"164\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1584\" data-original=\"https://pic2.zhimg.com/v2-1be746ed0d18e26d6c59dd18dea5541d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-1be746ed0d18e26d6c59dd18dea5541d_b.jpg\"/></figure><p>继续求得T3(x)、T4(x)、T5(x)、T6(x)，如下所示</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-fe3555e4c826a75a380517089bbd1a20_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1542\" data-rawheight=\"850\" class=\"origin_image zh-lightbox-thumb\" width=\"1542\" data-original=\"https://pic1.zhimg.com/v2-fe3555e4c826a75a380517089bbd1a20_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1542&#39; height=&#39;850&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1542\" data-rawheight=\"850\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1542\" data-original=\"https://pic1.zhimg.com/v2-fe3555e4c826a75a380517089bbd1a20_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-fe3555e4c826a75a380517089bbd1a20_b.jpg\"/></figure><p>用f6(x)拟合训练数据的平方损失误差如下所示，假设此时已经满足误差要求，那么f(x)=f6(x)便是所求的回归树。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-59b9da1849fc31a74ce7f7350ea9437a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1558\" data-rawheight=\"168\" class=\"origin_image zh-lightbox-thumb\" width=\"1558\" data-original=\"https://pic3.zhimg.com/v2-59b9da1849fc31a74ce7f7350ea9437a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1558&#39; height=&#39;168&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1558\" data-rawheight=\"168\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1558\" data-original=\"https://pic3.zhimg.com/v2-59b9da1849fc31a74ce7f7350ea9437a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-59b9da1849fc31a74ce7f7350ea9437a_b.jpg\"/></figure><h2>3.Sklearn实现随机森林</h2><p>我们经常需要通过改变参数来让模型达到更好的分类或回归结果，具体参数设置可参考<a href=\"https://link.zhihu.com/?target=http%3A//scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">sklearn官方教程</a>。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">from</span> <span class=\"nn\">sklearn.ensemble</span> <span class=\"kn\">import</span> <span class=\"n\">RandomForestClassifier</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.datasets</span> <span class=\"kn\">import</span> <span class=\"n\">make_classification</span>\n\n<span class=\"n\">X</span><span class=\"p\">,</span><span class=\"n\">y</span><span class=\"o\">=</span><span class=\"n\">make_classification</span><span class=\"p\">(</span><span class=\"n\">n_samples</span><span class=\"o\">=</span><span class=\"mi\">1000</span><span class=\"p\">,</span><span class=\"n\">n_features</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span>\n                        <span class=\"n\">n_informative</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span><span class=\"n\">n_redundant</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span>\n                        <span class=\"n\">random_state</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"n\">shuffle</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">[:</span><span class=\"mi\">10</span><span class=\"p\">],</span><span class=\"n\">y</span><span class=\"p\">[:</span><span class=\"mi\">10</span><span class=\"p\">])</span>\n<span class=\"c1\">#  X</span>\n<span class=\"c1\"># [[-1.66853167 -1.29901346  0.2746472  -0.60362044]</span>\n<span class=\"c1\">#  [-2.9728827  -1.08878294  0.70885958  0.42281857]</span>\n<span class=\"c1\">#  [-0.59614125 -1.37007001 -3.11685659  0.64445203]</span>\n<span class=\"c1\">#  [-1.06894674 -1.17505738 -1.91374267  0.66356158]</span>\n<span class=\"c1\">#  [-1.30526888 -0.96592566 -0.1540724   1.19361168]</span>\n<span class=\"c1\">#  [-2.18261832 -0.97011387 -0.09816121 -0.88661426]</span>\n<span class=\"c1\">#  [-1.24797892 -1.13094525 -0.14735366  1.05980629]</span>\n<span class=\"c1\">#  [-1.35308792 -1.06633681  0.02624662 -0.11433516]</span>\n<span class=\"c1\">#  [-1.13449871 -1.27403448  0.74355352  0.21035937]</span>\n<span class=\"c1\">#  [-0.38457445 -1.08840346 -0.00592741  1.36606007]]</span>\n\n<span class=\"c1\">#  y</span>\n<span class=\"c1\"># [0 0 0 0 0 0 0 0 0 0]</span>\n\n<span class=\"n\">clf</span><span class=\"o\">=</span><span class=\"n\">RandomForestClassifier</span><span class=\"p\">(</span><span class=\"n\">max_depth</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span><span class=\"n\">random_state</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n<span class=\"n\">clf</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span><span class=\"n\">y</span><span class=\"p\">)</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">clf</span><span class=\"o\">.</span><span class=\"n\">feature_importances_</span><span class=\"p\">)</span>\n<span class=\"c1\"># [ 0.17287856  0.80608704  0.01884792  0.00218648]</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">clf</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">([[</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">0</span><span class=\"p\">]]))</span>\n<span class=\"c1\"># [1]</span></code></pre></div><h2>4.随机森林优缺点</h2><h2>4.1优点</h2><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>决策树选择部分样本及部分特征，一定程度上避免过拟合。</li><li>决策树随机选择样本并随机选择特征，模型具有很好的抗噪能力，性能稳定。</li><li>能够处理高维度数据，并且不用做特征选择，能够展现出哪些变量比较重要。</li><li>对缺失值不敏感，如果有很大一部分的特征遗失，仍可以维持准确度。</li><li>训练时树与树之间是相互独立的，训练速度快，容易做成并行化方法。</li><li>随机森林有oob，不需要单独划分交叉验证集。</li></ul><h2>4.2缺点</h2><ul><li>可能有很多相似决策树，掩盖真实结果。</li><li>对小数据或低维数据可能不能产生很好分类。</li><li>产生众多决策树，算法较慢。</li></ul><h2>5.推广</h2><p>更多内容请关注公众号<b>谓之小一</b>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p><p><a href=\"https://link.zhihu.com/?target=http%3A//weixin.qq.com/r/Ty_4oDPE2W6mrXfD93pd\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">weixin.qq.com/r/Ty_4oDP</span><span class=\"invisible\">E2W6mrXfD93pd</span><span class=\"ellipsis\"></span></a> (二维码自动识别)</p><p></p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": [
                {
                    "userName": "chocolate", 
                    "userLink": "https://www.zhihu.com/people/d42da3a651f5cd298cd48c7324ba222e", 
                    "content": "<p>有点地方写错了m(s) 这个公式minc2 的话应该是yi - c2 而不是c1 </p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "谓之小一", 
                            "userLink": "https://www.zhihu.com/people/761dda159c5cec5fd926f761ab5f4ccf", 
                            "content": "<p>最近几天没看知乎，刚看到你的消息。确实写错了，已经改正，谢了。</p>", 
                            "likes": 0, 
                            "replyToAuthor": "chocolate"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/36108972", 
            "userName": "谓之小一", 
            "userLink": "https://www.zhihu.com/people/761dda159c5cec5fd926f761ab5f4ccf", 
            "upvote": 96, 
            "title": "机器学习之分类与回归树(CART)", 
            "content": "<blockquote>由于较多公式，所以我将部分内容转为图片进行上传，请见谅。清晰版请访问<a href=\"https://link.zhihu.com/?target=https%3A//weizhixiaoyi.com\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">weizhixiaoyi.com</span><span class=\"invisible\"></span></a>查看，你也可以关注我公众号<b>谓之小一</b>，后台直接向我要pdf版本，如有相关问题直接后台询问，随时回答。</blockquote><h2>1.分类与回归树简介</h2><p>分类与回归树的英文是<i>Classfication And Regression Tree</i>，缩写为CART。CART算法采用<b>二分递归分割的技术</b>将当前样本集分为两个子样本集，使得生成的每个非叶子节点都有两个分支。非叶子节点的特征取值为<b>True</b>和<b>False</b>，左分支取值为<b>True</b>，右分支取值为<b>False</b>，因此CART算法生成的决策树是结构简洁的二叉树。CART可以处理连续型变量和离散型变量，利用训练数据递归的划分特征空间进行建树，用验证数据进行剪枝。</p><ul><li>如果待预测分类是离散型数据，则CART生成分类决策树。</li><li>如果待预测分类是连续性数据，则CART生成回归决策树。</li></ul><h2>2.CART分类树</h2><h2>2.1算法详解</h2><p>CART分类树预测分类离散型数据，采用基尼指数选择最优特征，同时决定该特征的最优二值切分点。分类过程中，假设有K个类，样本点属于第k个类的概率为Pk，则概率分布的基尼指数定义为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-780d955260d9a2ba8508c1601588b88a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1560\" data-rawheight=\"174\" class=\"origin_image zh-lightbox-thumb\" width=\"1560\" data-original=\"https://pic3.zhimg.com/v2-780d955260d9a2ba8508c1601588b88a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1560&#39; height=&#39;174&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1560\" data-rawheight=\"174\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1560\" data-original=\"https://pic3.zhimg.com/v2-780d955260d9a2ba8508c1601588b88a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-780d955260d9a2ba8508c1601588b88a_b.jpg\"/></figure><p>根据基尼指数定义，可以得到样本集合D的基尼指数，其中Ck表示数据集D中属于第k类的样本子集。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-95300197189b4b1b65eb42d1a6bbd7fd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1586\" data-rawheight=\"160\" class=\"origin_image zh-lightbox-thumb\" width=\"1586\" data-original=\"https://pic2.zhimg.com/v2-95300197189b4b1b65eb42d1a6bbd7fd_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1586&#39; height=&#39;160&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1586\" data-rawheight=\"160\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1586\" data-original=\"https://pic2.zhimg.com/v2-95300197189b4b1b65eb42d1a6bbd7fd_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-95300197189b4b1b65eb42d1a6bbd7fd_b.jpg\"/></figure><p>如果数据集D根据特征A在某一取值a上进行分割，得到D1,D2两部分后，那么在特征A下集合D的基尼系数如下所示。其中基尼系数Gini(D)表示集合D的不确定性，基尼系数Gini(D,A)表示A=a分割后集合D的不确定性。基尼指数越大，样本集合的不确定性越大。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-a6f4d87b5eee51c147fb91e31e51d48e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1590\" data-rawheight=\"156\" class=\"origin_image zh-lightbox-thumb\" width=\"1590\" data-original=\"https://pic3.zhimg.com/v2-a6f4d87b5eee51c147fb91e31e51d48e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1590&#39; height=&#39;156&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1590\" data-rawheight=\"156\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1590\" data-original=\"https://pic3.zhimg.com/v2-a6f4d87b5eee51c147fb91e31e51d48e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-a6f4d87b5eee51c147fb91e31e51d48e_b.jpg\"/></figure><p>对于属性A，分别计算任意属性值将数据集划分为两部分之后的Gain_Gini，选取其中的最小值，作为属性A得到的最优二分方案。然后对于训练集S，计算所有属性的最优二分方案，选取其中的最小值，作为样本及S的最优二分方案。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-8ce2b82f547858f9e604151bbd90bd23_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1602\" data-rawheight=\"262\" class=\"origin_image zh-lightbox-thumb\" width=\"1602\" data-original=\"https://pic4.zhimg.com/v2-8ce2b82f547858f9e604151bbd90bd23_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1602&#39; height=&#39;262&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1602\" data-rawheight=\"262\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1602\" data-original=\"https://pic4.zhimg.com/v2-8ce2b82f547858f9e604151bbd90bd23_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-8ce2b82f547858f9e604151bbd90bd23_b.jpg\"/></figure><h2>2.1实例详解</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-37fcb1d050923b846f344d83384c27bc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1246\" data-rawheight=\"1274\" class=\"origin_image zh-lightbox-thumb\" width=\"1246\" data-original=\"https://pic1.zhimg.com/v2-37fcb1d050923b846f344d83384c27bc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1246&#39; height=&#39;1274&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1246\" data-rawheight=\"1274\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1246\" data-original=\"https://pic1.zhimg.com/v2-37fcb1d050923b846f344d83384c27bc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-37fcb1d050923b846f344d83384c27bc_b.jpg\"/></figure><p>针对上述离散型数据，按照<b>体温为恒温和非恒温</b>进行划分。其中恒温时包括哺乳类5个、鸟类2个，非恒温时包括爬行类3个、鱼类3个、两栖类2个，如下所示我们计算D1,D2的基尼指数。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-f477552326b3f2f39404e3fb7de3787c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1584\" data-rawheight=\"280\" class=\"origin_image zh-lightbox-thumb\" width=\"1584\" data-original=\"https://pic1.zhimg.com/v2-f477552326b3f2f39404e3fb7de3787c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1584&#39; height=&#39;280&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1584\" data-rawheight=\"280\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1584\" data-original=\"https://pic1.zhimg.com/v2-f477552326b3f2f39404e3fb7de3787c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-f477552326b3f2f39404e3fb7de3787c_b.jpg\"/></figure><p>然后计算得到特征<b>体温</b>下数据集的Gini指数，最后我们选择Gain_Gini最小的特征和相应的划分。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-0d3d4c5093aa272f4072703afdee8b36_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1588\" data-rawheight=\"134\" class=\"origin_image zh-lightbox-thumb\" width=\"1588\" data-original=\"https://pic3.zhimg.com/v2-0d3d4c5093aa272f4072703afdee8b36_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1588&#39; height=&#39;134&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1588\" data-rawheight=\"134\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1588\" data-original=\"https://pic3.zhimg.com/v2-0d3d4c5093aa272f4072703afdee8b36_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-0d3d4c5093aa272f4072703afdee8b36_b.jpg\"/></figure><h2>3.CART回归树</h2><h2>3.1算法详解</h2><p>CART回归树预测回归连续型数据，假设X与Y分别是输入和输出变量，并且Y是连续变量。在训练数据集所在的输入空间中，递归的将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-5283b27704fbd79178e968aa379f7fdc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1574\" data-rawheight=\"116\" class=\"origin_image zh-lightbox-thumb\" width=\"1574\" data-original=\"https://pic1.zhimg.com/v2-5283b27704fbd79178e968aa379f7fdc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1574&#39; height=&#39;116&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1574\" data-rawheight=\"116\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1574\" data-original=\"https://pic1.zhimg.com/v2-5283b27704fbd79178e968aa379f7fdc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-5283b27704fbd79178e968aa379f7fdc_b.jpg\"/></figure><p><b>选择最优切分变量j与切分点s</b>：遍历变量j，对规定的切分变量j扫描切分点s，选择使下式得到最小值时的(j,s)对。其中Rm是被划分的输入空间，cm是空间Rm对应的固定输出值。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-57a4d9349ca011a5dfb2b31a6f72e83b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1564\" data-rawheight=\"160\" class=\"origin_image zh-lightbox-thumb\" width=\"1564\" data-original=\"https://pic4.zhimg.com/v2-57a4d9349ca011a5dfb2b31a6f72e83b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1564&#39; height=&#39;160&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1564\" data-rawheight=\"160\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1564\" data-original=\"https://pic4.zhimg.com/v2-57a4d9349ca011a5dfb2b31a6f72e83b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-57a4d9349ca011a5dfb2b31a6f72e83b_b.jpg\"/></figure><p><b>用选定的(j,s)对，划分区域并决定相应的输出值</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-9907b77a00a79e03c51fbf28a696998a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1568\" data-rawheight=\"372\" class=\"origin_image zh-lightbox-thumb\" width=\"1568\" data-original=\"https://pic3.zhimg.com/v2-9907b77a00a79e03c51fbf28a696998a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1568&#39; height=&#39;372&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1568\" data-rawheight=\"372\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1568\" data-original=\"https://pic3.zhimg.com/v2-9907b77a00a79e03c51fbf28a696998a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-9907b77a00a79e03c51fbf28a696998a_b.jpg\"/></figure><p><b>继续对两个子区域调用上述步骤</b>，<b>将输入空间划分为M个区域R1,R2,…,Rm，生成决策树。</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-2246d6ccdbc8da8059fcc9fd8aadc9b8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1588\" data-rawheight=\"174\" class=\"origin_image zh-lightbox-thumb\" width=\"1588\" data-original=\"https://pic1.zhimg.com/v2-2246d6ccdbc8da8059fcc9fd8aadc9b8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1588&#39; height=&#39;174&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1588\" data-rawheight=\"174\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1588\" data-original=\"https://pic1.zhimg.com/v2-2246d6ccdbc8da8059fcc9fd8aadc9b8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-2246d6ccdbc8da8059fcc9fd8aadc9b8_b.jpg\"/></figure><p>当输入空间划分确定时，可以用<b>平方误差</b>来表示回归树对于训练数据的预测方法，用平方误差最小的准则求解每个单元上的最优输出值。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-851d114f504be680d16ce3b4f15d474f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1608\" data-rawheight=\"144\" class=\"origin_image zh-lightbox-thumb\" width=\"1608\" data-original=\"https://pic4.zhimg.com/v2-851d114f504be680d16ce3b4f15d474f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1608&#39; height=&#39;144&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1608\" data-rawheight=\"144\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1608\" data-original=\"https://pic4.zhimg.com/v2-851d114f504be680d16ce3b4f15d474f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-851d114f504be680d16ce3b4f15d474f_b.jpg\"/></figure><h2>3.2实例详解</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-297ce1628209a6fba9e8ba7e04d6eb6b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1242\" data-rawheight=\"192\" class=\"origin_image zh-lightbox-thumb\" width=\"1242\" data-original=\"https://pic4.zhimg.com/v2-297ce1628209a6fba9e8ba7e04d6eb6b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1242&#39; height=&#39;192&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1242\" data-rawheight=\"192\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1242\" data-original=\"https://pic4.zhimg.com/v2-297ce1628209a6fba9e8ba7e04d6eb6b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-297ce1628209a6fba9e8ba7e04d6eb6b_b.jpg\"/></figure><p>考虑如上所示的连续性变量，根据给定的数据点，考虑<b>1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5</b>切分点。对各切分点依次求出<b>R1,R2,c1,c2及m(s)</b>，例如当切分点s=1.5时，得到R1={1},R2={2,3,4,5,6,7,8,9,10}，其中c1,c2,m(s)如下所示。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-2d1776b7d35dbf6a4e5508e56fbb46c4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1612\" data-rawheight=\"484\" class=\"origin_image zh-lightbox-thumb\" width=\"1612\" data-original=\"https://pic1.zhimg.com/v2-2d1776b7d35dbf6a4e5508e56fbb46c4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1612&#39; height=&#39;484&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1612\" data-rawheight=\"484\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1612\" data-original=\"https://pic1.zhimg.com/v2-2d1776b7d35dbf6a4e5508e56fbb46c4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-2d1776b7d35dbf6a4e5508e56fbb46c4_b.jpg\"/></figure><p><b>依次改变(j,s)对，可以得到s及m(s)的计算结果</b>，如下表所示。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-1434199dedf4ec8aae25def3ef804222_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1314\" data-rawheight=\"254\" class=\"origin_image zh-lightbox-thumb\" width=\"1314\" data-original=\"https://pic3.zhimg.com/v2-1434199dedf4ec8aae25def3ef804222_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1314&#39; height=&#39;254&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1314\" data-rawheight=\"254\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1314\" data-original=\"https://pic3.zhimg.com/v2-1434199dedf4ec8aae25def3ef804222_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-1434199dedf4ec8aae25def3ef804222_b.jpg\"/></figure><p>当x=6.5时，此时R1={1,2,3,4,5,6},R2={7,8,9,10},c1=6.24,c2=8.9。<b>回归树T1(x)</b>为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ced4e485ca0247e001abee28a225e0c0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1628\" data-rawheight=\"258\" class=\"origin_image zh-lightbox-thumb\" width=\"1628\" data-original=\"https://pic1.zhimg.com/v2-ced4e485ca0247e001abee28a225e0c0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1628&#39; height=&#39;258&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1628\" data-rawheight=\"258\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1628\" data-original=\"https://pic1.zhimg.com/v2-ced4e485ca0247e001abee28a225e0c0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-ced4e485ca0247e001abee28a225e0c0_b.jpg\"/></figure><p><b>然后我们利用f1(x)拟合训练数据的残差</b>，如下表所示</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-e52211e6c4078a82a7862716e64fd5b1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1242\" data-rawheight=\"192\" class=\"origin_image zh-lightbox-thumb\" width=\"1242\" data-original=\"https://pic2.zhimg.com/v2-e52211e6c4078a82a7862716e64fd5b1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1242&#39; height=&#39;192&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1242\" data-rawheight=\"192\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1242\" data-original=\"https://pic2.zhimg.com/v2-e52211e6c4078a82a7862716e64fd5b1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-e52211e6c4078a82a7862716e64fd5b1_b.jpg\"/></figure><p><b>用f1(x)拟合训练数据得到平方误差</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-73d4172314554e26884e59f71e117ade_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1622\" data-rawheight=\"158\" class=\"origin_image zh-lightbox-thumb\" width=\"1622\" data-original=\"https://pic3.zhimg.com/v2-73d4172314554e26884e59f71e117ade_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1622&#39; height=&#39;158&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1622\" data-rawheight=\"158\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1622\" data-original=\"https://pic3.zhimg.com/v2-73d4172314554e26884e59f71e117ade_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-73d4172314554e26884e59f71e117ade_b.jpg\"/></figure><p>第二步求T2(x)与求T1(x)方法相同，只是拟合的数据是上表的残差。可以得到</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-d6dabacc2b5a64586eac9e52f0c2f50b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1576\" data-rawheight=\"328\" class=\"origin_image zh-lightbox-thumb\" width=\"1576\" data-original=\"https://pic4.zhimg.com/v2-d6dabacc2b5a64586eac9e52f0c2f50b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1576&#39; height=&#39;328&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1576\" data-rawheight=\"328\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1576\" data-original=\"https://pic4.zhimg.com/v2-d6dabacc2b5a64586eac9e52f0c2f50b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-d6dabacc2b5a64586eac9e52f0c2f50b_b.jpg\"/></figure><p>用f2(x)拟合训练数据的平方误差</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-1be746ed0d18e26d6c59dd18dea5541d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1584\" data-rawheight=\"164\" class=\"origin_image zh-lightbox-thumb\" width=\"1584\" data-original=\"https://pic2.zhimg.com/v2-1be746ed0d18e26d6c59dd18dea5541d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1584&#39; height=&#39;164&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1584\" data-rawheight=\"164\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1584\" data-original=\"https://pic2.zhimg.com/v2-1be746ed0d18e26d6c59dd18dea5541d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-1be746ed0d18e26d6c59dd18dea5541d_b.jpg\"/></figure><p>继续求得T3(x)、T4(x)、T5(x)、T6(x)，如下所示</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-fe3555e4c826a75a380517089bbd1a20_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1542\" data-rawheight=\"850\" class=\"origin_image zh-lightbox-thumb\" width=\"1542\" data-original=\"https://pic1.zhimg.com/v2-fe3555e4c826a75a380517089bbd1a20_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1542&#39; height=&#39;850&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1542\" data-rawheight=\"850\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1542\" data-original=\"https://pic1.zhimg.com/v2-fe3555e4c826a75a380517089bbd1a20_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-fe3555e4c826a75a380517089bbd1a20_b.jpg\"/></figure><p>用f6(x)拟合训练数据的平方损失误差如下所示，假设此时已经满足误差要求，那么f(x)=f6(x)便是所求的回归树。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-59b9da1849fc31a74ce7f7350ea9437a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1558\" data-rawheight=\"168\" class=\"origin_image zh-lightbox-thumb\" width=\"1558\" data-original=\"https://pic3.zhimg.com/v2-59b9da1849fc31a74ce7f7350ea9437a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1558&#39; height=&#39;168&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1558\" data-rawheight=\"168\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1558\" data-original=\"https://pic3.zhimg.com/v2-59b9da1849fc31a74ce7f7350ea9437a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-59b9da1849fc31a74ce7f7350ea9437a_b.jpg\"/></figure><h2>4.CART剪枝</h2><blockquote>此处我们介绍代价复杂度剪枝算法<br/></blockquote><p>我们将一颗充分生长的树称为<b>T0</b> ，希望减少树的大小来防止过拟化。但同时去掉一些节点后预测的误差可能会增大，那么如何达到这两个变量之间的平衡则是问题的关键。因此我们用一个变量α 来平衡，定义损失函数如下</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-4a54ca3e3f7e436f048913f2dd341beb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1588\" data-rawheight=\"122\" class=\"origin_image zh-lightbox-thumb\" width=\"1588\" data-original=\"https://pic4.zhimg.com/v2-4a54ca3e3f7e436f048913f2dd341beb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1588&#39; height=&#39;122&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1588\" data-rawheight=\"122\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1588\" data-original=\"https://pic4.zhimg.com/v2-4a54ca3e3f7e436f048913f2dd341beb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-4a54ca3e3f7e436f048913f2dd341beb_b.jpg\"/></figure><ul><li>T为任意子树，|T|为子树T的叶子节点个数。</li><li>α是参数，权衡拟合程度与树的复杂度。</li><li>C(T)为预测误差，可以是平方误差也可以是基尼指数，C(T)衡量训练数据的拟合程度。</li></ul><p><b>那么我们如何找到这个合适的α来使拟合程度与复杂度之间达到最好的平衡呢？</b>准确的方法就是将α从0取到正无穷，对于每一个固定的α，我们都可以找到使得Cα(T)最小的最优子树T(α)。</p><ul><li>当α很小的时候，T0 是这样的最优子树.</li><li>当α很大的时候，单独一个根节点就是最优子树。</li></ul><p>尽管α的取值无限多，但是T0的子树是有限个。Tn是最后剩下的根结点，子树生成是根据前一个子树Ti，剪掉某个内部节点后，生成Ti+1。然后对这样的子树序列分别用测试集进行<b>交叉验证</b>，找到最优的那个子树作为我们的决策树。子树序列如下</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-b9d9f35561e7fd6276ab3920af2b8f64_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1594\" data-rawheight=\"130\" class=\"origin_image zh-lightbox-thumb\" width=\"1594\" data-original=\"https://pic1.zhimg.com/v2-b9d9f35561e7fd6276ab3920af2b8f64_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1594&#39; height=&#39;130&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1594\" data-rawheight=\"130\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1594\" data-original=\"https://pic1.zhimg.com/v2-b9d9f35561e7fd6276ab3920af2b8f64_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-b9d9f35561e7fd6276ab3920af2b8f64_b.jpg\"/></figure><p><b>因此CART剪枝分为两部分，分别是生成子树序列和交叉验证，</b>在此不再详细介绍。</p><h2>5.Sklearn实现</h2><p>我们以sklearn中iris数据作为训练集，iris属性特征包括花萼长度、花萼宽度、花瓣长度、花瓣宽度，类别共三类，分别为Setosa、Versicolour、Virginca。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">from</span> <span class=\"nn\">sklearn.datasets</span> <span class=\"kn\">import</span> <span class=\"n\">load_iris</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn</span> <span class=\"kn\">import</span> <span class=\"n\">tree</span>\n\n<span class=\"c1\">#load data</span>\n<span class=\"n\">iris</span><span class=\"o\">=</span><span class=\"n\">load_iris</span><span class=\"p\">()</span>\n<span class=\"n\">X</span><span class=\"o\">=</span><span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">data</span>\n<span class=\"n\">y</span><span class=\"o\">=</span><span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">target</span>\n<span class=\"n\">clf</span><span class=\"o\">=</span><span class=\"n\">tree</span><span class=\"o\">.</span><span class=\"n\">DecisionTreeClassifier</span><span class=\"p\">()</span>\n<span class=\"n\">clf</span><span class=\"o\">=</span><span class=\"n\">clf</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span><span class=\"n\">y</span><span class=\"p\">)</span>\n\n<span class=\"c1\">#export the decision tree</span>\n<span class=\"kn\">import</span> <span class=\"nn\">graphviz</span>\n<span class=\"c1\">#export_graphviz support a variety of aesthetic options</span>\n<span class=\"n\">dot_data</span><span class=\"o\">=</span><span class=\"n\">tree</span><span class=\"o\">.</span><span class=\"n\">export_graphviz</span><span class=\"p\">(</span><span class=\"n\">clf</span><span class=\"p\">,</span><span class=\"n\">out_file</span><span class=\"o\">=</span><span class=\"bp\">None</span><span class=\"p\">,</span>\n                              <span class=\"n\">feature_names</span><span class=\"o\">=</span><span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">feature_names</span><span class=\"p\">,</span>\n                              <span class=\"n\">class_names</span><span class=\"o\">=</span><span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">target_names</span><span class=\"p\">,</span>\n                              <span class=\"n\">filled</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span><span class=\"n\">rounded</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span>\n                              <span class=\"n\">special_characters</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n\n<span class=\"n\">graph</span><span class=\"o\">=</span><span class=\"n\">graphviz</span><span class=\"o\">.</span><span class=\"n\">Source</span><span class=\"p\">(</span><span class=\"n\">dot_data</span><span class=\"p\">)</span>\n<span class=\"n\">graph</span><span class=\"o\">.</span><span class=\"n\">view</span><span class=\"p\">()</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-3b1135c4a0d3261fdc0bacf6477e100c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2034\" data-rawheight=\"1522\" class=\"origin_image zh-lightbox-thumb\" width=\"2034\" data-original=\"https://pic1.zhimg.com/v2-3b1135c4a0d3261fdc0bacf6477e100c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2034&#39; height=&#39;1522&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2034\" data-rawheight=\"1522\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2034\" data-original=\"https://pic1.zhimg.com/v2-3b1135c4a0d3261fdc0bacf6477e100c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-3b1135c4a0d3261fdc0bacf6477e100c_b.jpg\"/></figure><h2>6.推广</h2><p>更多内容请关注公众号<b>谓之小一</b>，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p><p><a href=\"https://link.zhihu.com/?target=http%3A//weixin.qq.com/r/Ty_4oDPE2W6mrXfD93pd\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">weixin.qq.com/r/Ty_4oDP</span><span class=\"invisible\">E2W6mrXfD93pd</span><span class=\"ellipsis\"></span></a> (二维码自动识别)</p><p></p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "决策树", 
                    "tagLink": "https://api.zhihu.com/topics/19569936"
                }
            ], 
            "comments": [
                {
                    "userName": "流氓兔", 
                    "userLink": "https://www.zhihu.com/people/9ccd0d65bb2a456c4d5a592d08678c3c", 
                    "content": "<p>公式有错基尼指数</p>", 
                    "likes": 3, 
                    "childComments": []
                }, 
                {
                    "userName": "石小秀", 
                    "userLink": "https://www.zhihu.com/people/cfa52ff251b26dd73f8c0c559adb0d71", 
                    "content": "<p>感觉写的是GBDT诶</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "与笨", 
                            "userLink": "https://www.zhihu.com/people/9797337a9f786635abc5a4200595ef74", 
                            "content": "<p>上文的分类回归树的回归方法感觉也和GBDT很像，这里用的是真实残差。但事实上感觉分类回归树不应该这样解释呀，在学习学些别的文章看看。</p>", 
                            "likes": 1, 
                            "replyToAuthor": "石小秀"
                        }, 
                        {
                            "userName": "AINIVERSHERRY", 
                            "userLink": "https://www.zhihu.com/people/17dce6569d210efe4155c46fe5ca1366", 
                            "content": "<p>例子讲的应该是GBDT...</p>", 
                            "likes": 0, 
                            "replyToAuthor": "石小秀"
                        }
                    ]
                }, 
                {
                    "userName": "啦啦啦啊啦啦", 
                    "userLink": "https://www.zhihu.com/people/95bb555646e253bc7fb35c43928e1195", 
                    "content": "<p>写的看不太懂........</p><p></p>", 
                    "likes": 1, 
                    "childComments": []
                }, 
                {
                    "userName": "zsldatadata", 
                    "userLink": "https://www.zhihu.com/people/bde50ea9528b6851f6420e79e17b9582", 
                    "content": "<p>不错，还是得有实例才知道具体怎么计算的。。</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "大萌王朝高肃卿", 
                    "userLink": "https://www.zhihu.com/people/99b4beba6d7dd3c6c0e1c288f4749be4", 
                    "content": "<p>CART回归树搞错了吧，你写的是提升树。不过写的挺好，我无意中多学一个知识点</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "Brokenwind", 
                    "userLink": "https://www.zhihu.com/people/55f47c80f874c4de0264eed130b91335", 
                    "content": "<p>写的挺不错的，就是后边回归树写成了提升树</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "震灵", 
                    "userLink": "https://www.zhihu.com/people/ff2f6b898c04b2187afe377e46c75f25", 
                    "content": "<p>计算实例的公式感觉有问题，M(s)就是一个简单的平方误差计算，却写的那么复杂。。。</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "唐先生", 
                    "userLink": "https://www.zhihu.com/people/92a4c4d1ef1dc511de7e40bb04a73722", 
                    "content": "<p>找了这么多网站，唯一一个给出提升树计算细节的</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "MrTuesday", 
                    "userLink": "https://www.zhihu.com/people/fe8772ab5f91d5658eb700bf9b9ebae0", 
                    "content": "<p><b>看了一半怀疑是不是在看GBDT 看了评论还真是 不过写的很清楚</b></p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "雨岸", 
                    "userLink": "https://www.zhihu.com/people/d6f1fe1e7de35a696ef5a8c2a021069d", 
                    "content": "<p>感觉cart计算一次总方差就行了，没必要对残差也分裂吧</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "刘洋涛", 
                    "userLink": "https://www.zhihu.com/people/31a500d1e0ec06b4264481cf02612059", 
                    "content": "<p>请问回归树内容中，Cm是空间Rm对应的固定输出值，这个Cm怎么计算的，取空间Rm中所有输出值的均值吗？</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "无双", 
                    "userLink": "https://www.zhihu.com/people/0b35b7bcc7d68537b81f666471de899f", 
                    "content": "<p>公式有误，望更新</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "粥粥洋", 
                    "userLink": "https://www.zhihu.com/people/6d76f553d40f06276bdd628a7819a4e8", 
                    "content": "<p>你好，三个min公式最后一个min是错的，请尽快更新</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "发烧的小龙虾", 
                    "userLink": "https://www.zhihu.com/people/94042f58808c23868153314e1f367408", 
                    "content": "这个貌似是提升树？", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "曲松", 
                    "userLink": "https://www.zhihu.com/people/5912815a279a6c2708123afc9cddd7d1", 
                    "content": "回归树没看懂 加了例子更看不懂了", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/35927191", 
            "userName": "谓之小一", 
            "userLink": "https://www.zhihu.com/people/761dda159c5cec5fd926f761ab5f4ccf", 
            "upvote": 9, 
            "title": "机器学习之决策树(C4.5算法)", 
            "content": "<blockquote>由于较多公式，所以我将部分内容转为图片进行上传，请见谅。清晰版请访问<a href=\"https://link.zhihu.com/?target=https%3A//weizhixiaoyi.com\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">weizhixiaoyi.com</span><span class=\"invisible\"></span></a>查看，你也可以关注我公众号‘谓之小一’，后台直接向我要pdf版本，如有相关问题直接后台询问，随时回答。</blockquote><h2>1.决策树简介</h2><p>我们已有如下所示数据集，特征属性包含天气、温度、湿度、风速，然后根据这些数据去分类或预测能否去打高尔夫球，针对此类问题你会怎么解决呢。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-f14f0bc5545a4c0f48f8f0d0fe7e3f76_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1610\" data-rawheight=\"1164\" class=\"origin_image zh-lightbox-thumb\" width=\"1610\" data-original=\"https://pic3.zhimg.com/v2-f14f0bc5545a4c0f48f8f0d0fe7e3f76_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1610&#39; height=&#39;1164&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1610\" data-rawheight=\"1164\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1610\" data-original=\"https://pic3.zhimg.com/v2-f14f0bc5545a4c0f48f8f0d0fe7e3f76_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-f14f0bc5545a4c0f48f8f0d0fe7e3f76_b.jpg\"/></figure><p>正当你苦思冥想之时，天空之中突然飘来一张决策图，发现这图好像一张倒着的树啊，于是你命名为决策树。你发现可直接根据决策树得到相应结果，高兴的像个300斤的孩子。但下次再面临这样的问题时，还能够那么好运嘛？于是你陷入苦苦思考之中，怎样才能得到分类决策树呢。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-1d09560b7878cf61b4a834ba8bbc2844_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"998\" data-rawheight=\"626\" class=\"origin_image zh-lightbox-thumb\" width=\"998\" data-original=\"https://pic1.zhimg.com/v2-1d09560b7878cf61b4a834ba8bbc2844_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;998&#39; height=&#39;626&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"998\" data-rawheight=\"626\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"998\" data-original=\"https://pic1.zhimg.com/v2-1d09560b7878cf61b4a834ba8bbc2844_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-1d09560b7878cf61b4a834ba8bbc2844_b.jpg\"/></figure><h2>2.C4.5算法</h2><p><b>上古之神赐予你智慧</b>：C4.5是一系列用在机器学习和数据挖掘中分类问题的算法，它的目标是监督学习。即给定一个数据集，其中的每一个元组都能用一组属性值描述，每一个元组属于一个互斥的类别中的某一类。C4.5的目标是通过学习，找到一个从属性值到类别的映射关系，并且这个映射能够用于对新的类别未知的实体进行分类。</p><p>C4.5是在ID3的基础上提出的。ID3算法用来构造决策树。决策树是一种类似流程图的树结构，其中每个内部节点（非树叶节点）表示在一个属性上的测试，每个分枝代表一个测试输出，而每个树叶节点存放一个类标号。一旦建立好决策树，对于一个未给定类标号的元组，跟踪一条有根节点到叶节点的路径，该叶节点就存放着该元组的预测。上述数据集有4个属性，属性集合A={天气、温度、湿度、风速}，类别集合D={进行、取消}。我们先做一些假设</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3fc1a2eba9f58cbad83ce626fc106021_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1612\" data-rawheight=\"352\" class=\"origin_image zh-lightbox-thumb\" width=\"1612\" data-original=\"https://pic2.zhimg.com/v2-3fc1a2eba9f58cbad83ce626fc106021_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1612&#39; height=&#39;352&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1612\" data-rawheight=\"352\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1612\" data-original=\"https://pic2.zhimg.com/v2-3fc1a2eba9f58cbad83ce626fc106021_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-3fc1a2eba9f58cbad83ce626fc106021_b.jpg\"/></figure><h2>2.1信息增益</h2><p>信息增益实际上是ID3算法中用来进行属性选择度量的，具有较高信息增益的属性来作为节点N的分裂属性。该属性使结果划分中的元组分类所需信息量最小。</p><p><b>计算类别信息熵</b>:类别信息熵表示的是所有样本中各种类别出现的不确定之和。根据熵的概念，熵越大，不确定性就越大，把事情理解清楚所需要的信息量就越多。对D中的元组分类所需的期望信息表达式如下，同时计算出上述数据的期望信息</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ab7572fca7aacbf005240f4d81e6fccf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1608\" data-rawheight=\"314\" class=\"origin_image zh-lightbox-thumb\" width=\"1608\" data-original=\"https://pic4.zhimg.com/v2-ab7572fca7aacbf005240f4d81e6fccf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1608&#39; height=&#39;314&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1608\" data-rawheight=\"314\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1608\" data-original=\"https://pic4.zhimg.com/v2-ab7572fca7aacbf005240f4d81e6fccf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-ab7572fca7aacbf005240f4d81e6fccf_b.jpg\"/></figure><p><b>计算每个属性的信息熵</b>:每个属性的信息熵相当于条件熵。表示的是在某种属性的条件下，各种类别出现的不确定之和。属性的信息熵越大，表示这个属性中拥有的样本类别越乱。现在假定按照属性集合C划分D中的元组，且属性Ci将D划分成v个不同的类。在该划分之后，为了得到准确的分类还需要下式进行度量。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-70140ad6174be3959368c34a1ddcfcf4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1614\" data-rawheight=\"408\" class=\"origin_image zh-lightbox-thumb\" width=\"1614\" data-original=\"https://pic1.zhimg.com/v2-70140ad6174be3959368c34a1ddcfcf4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1614&#39; height=&#39;408&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1614\" data-rawheight=\"408\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1614\" data-original=\"https://pic1.zhimg.com/v2-70140ad6174be3959368c34a1ddcfcf4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-70140ad6174be3959368c34a1ddcfcf4_b.jpg\"/></figure><p><b>计算信息增益</b>:信息增益=熵-条件熵，在这里表示为类别信息熵-属性信息熵。它表示的是信息不确定性减少的程度。如果一个属性的信息增益越大，就表示用这个属性进行样本划分可以更好的减少划分后样本的不确定性，选择该属性就可以更快更好的完成我们的分类目标。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-45883693b52744eef1f8752c3e139236_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1586\" data-rawheight=\"344\" class=\"origin_image zh-lightbox-thumb\" width=\"1586\" data-original=\"https://pic3.zhimg.com/v2-45883693b52744eef1f8752c3e139236_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1586&#39; height=&#39;344&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1586\" data-rawheight=\"344\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1586\" data-original=\"https://pic3.zhimg.com/v2-45883693b52744eef1f8752c3e139236_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-45883693b52744eef1f8752c3e139236_b.jpg\"/></figure><p>但是我们假设这种情况，每个属性中每个类别都只有一个样本，那这样属性信息熵就等于0，根据信息增益就无法选择出有效分类特征，所以C4.5算法选择使用信息增益率对ID3进行改进。</p><h2>2.2信息增益率</h2><p><b>计算属性分裂信息度量</b>:用分裂信息度量来考虑某种属性进行分裂时分支的数量信息和尺寸信息，我们把这些信息称为属性的内在信息。信息增益率用<b>信息增益 / 内在信息</b>表示，信息增益率会导致属性的重要性随着内在信息的增大而减小<b>（也就是说，如果这个属性本身不确定性就很大，那我就越不倾向于选取它）</b>，这样算是对单纯用信息增益有所补偿。信息增益率定义如下</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-56fa2fd6fb274a5af00d862832bfe5f9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1594\" data-rawheight=\"844\" class=\"origin_image zh-lightbox-thumb\" width=\"1594\" data-original=\"https://pic2.zhimg.com/v2-56fa2fd6fb274a5af00d862832bfe5f9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1594&#39; height=&#39;844&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1594\" data-rawheight=\"844\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1594\" data-original=\"https://pic2.zhimg.com/v2-56fa2fd6fb274a5af00d862832bfe5f9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-56fa2fd6fb274a5af00d862832bfe5f9_b.jpg\"/></figure><p>天气的信息增益率最高，选择天气为分裂属性。分裂之后，天气是“阴”的条件下无其他分裂点，所以把它定义为叶子节点，选择较乱的结点继续分裂。重复上述过程，直到算法完成，我们便可得到决策树。</p><h2>3.树剪枝</h2><p>决策树创建过程中，由于数据中的噪声和离群点，许多分支反应的是训练数据中的异常。剪枝方法是用来处理这种过分拟合的问题，通常剪枝方法都是使用统计度量，减去最不可靠的分支。减枝方法分为先减枝和后剪枝。</p><h2>3.1先剪枝</h2><p>先剪枝方法通过提前停止树的构造(比如决定在某个节点不再分裂或划分训练元组的自己)。但先剪枝有个视野效果缺点问题，也就是说在相同的标准下，也许当前扩展不能满足要求，但更进一步又能满足要求，这样会过早停止树的生长。先剪枝可通过以下方法</p><ul><li>当决策树达到一定的高度就停止决策树的生长。</li><li>到达节点的实例个数小于某个阈值的时候也可以停止树的生长，不足之处是不能处理那些数量比较小的特殊情况。</li><li>计算每次扩展对系统性能的增益，如果小于某个阈值就可以停止树的生长。</li></ul><h2>3.2后剪枝</h2><p>后剪枝是由完全生长的树剪去子树而形成，通过删除节点的分支并用树叶来替换它，树叶一般用子树中最频繁的类别来标记。C4.5采用悲观剪枝法，它使用训练集生成决策树，然后对生成的决策树进行剪枝，通过对比剪枝前后分类错误率来验证是否进行剪枝。</p><p>把一颗子树的分类(具有多个叶子结点)的分类用一个叶子节点替换的话，在训练集上的误判率肯定是上升的，但是在新数据上则不一定，于是我们需要把子树的误判计算加上一个经验性的惩罚因子。对于一颗叶子节点，它覆盖了N个样本，其中有E个错误，那么该叶子节点的错误率为(E+0.5)/N。这个0.5就是惩罚因子，那么一颗子树，他有L个叶子节点，那么该子树的误判率为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-e68c2b8a461db348a3fb47613b05ba71_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1606\" data-rawheight=\"150\" class=\"origin_image zh-lightbox-thumb\" width=\"1606\" data-original=\"https://pic2.zhimg.com/v2-e68c2b8a461db348a3fb47613b05ba71_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1606&#39; height=&#39;150&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1606\" data-rawheight=\"150\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1606\" data-original=\"https://pic2.zhimg.com/v2-e68c2b8a461db348a3fb47613b05ba71_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-e68c2b8a461db348a3fb47613b05ba71_b.jpg\"/></figure><p>这样的话我们可以看到一颗子树虽然有多个子节点，但由于加上惩罚因子，所以子树的误判率未必占到便宜。剪枝后内部节点变成叶子节点，其误判个数J也需要加上一个惩罚因子，变成J+0.5。那么子树是否可以被剪枝就取决于剪枝后的错误J+0.5的标准范围内。对于样本的误差率e，我们可以根据经验把它估计成各种各样的分布模型，比如二项式分布或正态分布。</p><p>假如决策树正确分类的样本值为1，错误分类的样本值为0，该树错误分类的概率(误判率)为e(<b>e为分布的固有属性，可以统计出来</b>)，那么树的误判次数就是伯努利分布，我们可以估计出概述的误差次数均值和标准值。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-b1b28f0b284d0be0e8ff786f82f68202_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1612\" data-rawheight=\"240\" class=\"origin_image zh-lightbox-thumb\" width=\"1612\" data-original=\"https://pic3.zhimg.com/v2-b1b28f0b284d0be0e8ff786f82f68202_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1612&#39; height=&#39;240&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1612\" data-rawheight=\"240\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1612\" data-original=\"https://pic3.zhimg.com/v2-b1b28f0b284d0be0e8ff786f82f68202_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-b1b28f0b284d0be0e8ff786f82f68202_b.jpg\"/></figure><p>把子树替换成叶子节点后，该叶子的误判次数也是伯努利分布，其概率误判率为(E+0.5)/N，因此叶子节点的误判次数均值为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-de2e6a3eb24efce1584da2ee5cb10f31_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1592\" data-rawheight=\"124\" class=\"origin_image zh-lightbox-thumb\" width=\"1592\" data-original=\"https://pic2.zhimg.com/v2-de2e6a3eb24efce1584da2ee5cb10f31_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1592&#39; height=&#39;124&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1592\" data-rawheight=\"124\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1592\" data-original=\"https://pic2.zhimg.com/v2-de2e6a3eb24efce1584da2ee5cb10f31_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-de2e6a3eb24efce1584da2ee5cb10f31_b.jpg\"/></figure><p>使用训练数据时，子树总是比替换为一个叶节点后产生的误差小，但是使用校正后有误差计算方法却并非如此，当子树的误判个数减去标准差后大于对应叶节点的误判个数，就决定剪枝</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-46e2087f183e67ae675fd438916602e0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1588\" data-rawheight=\"126\" class=\"origin_image zh-lightbox-thumb\" width=\"1588\" data-original=\"https://pic1.zhimg.com/v2-46e2087f183e67ae675fd438916602e0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1588&#39; height=&#39;126&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1588\" data-rawheight=\"126\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1588\" data-original=\"https://pic1.zhimg.com/v2-46e2087f183e67ae675fd438916602e0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-46e2087f183e67ae675fd438916602e0_b.jpg\"/></figure><p>上述条件就是剪枝的标准。当然并不一定非要减去标准差，可以给定任意的置信区间，我们设定一定的显著性因子，就可以估算出误判次数的上下界。</p><h2>4.Sklearn实现决策树</h2><p>我们以sklearn中iris数据作为训练集，iris属性特征包括花萼长度、花萼宽度、花瓣长度、花瓣宽度，类别共三类，分别为Setosa、Versicolour、Virginca。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">from</span> <span class=\"nn\">sklearn.datasets</span> <span class=\"kn\">import</span> <span class=\"n\">load_iris</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn</span> <span class=\"kn\">import</span> <span class=\"n\">tree</span>\n\n<span class=\"c1\">#引入数据</span>\n<span class=\"n\">iris</span><span class=\"o\">=</span><span class=\"n\">load_iris</span><span class=\"p\">()</span>\n<span class=\"n\">X</span><span class=\"o\">=</span><span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">data</span>\n<span class=\"n\">y</span><span class=\"o\">=</span><span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">target</span>\n\n<span class=\"c1\">#训练数据和模型,采用ID3或C4.5训练</span>\n<span class=\"n\">clf</span><span class=\"o\">=</span><span class=\"n\">tree</span><span class=\"o\">.</span><span class=\"n\">DecisionTreeClassifier</span><span class=\"p\">(</span><span class=\"n\">criterion</span><span class=\"o\">=</span><span class=\"s1\">&#39;entropy&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">clf</span><span class=\"o\">=</span><span class=\"n\">clf</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span><span class=\"n\">y</span><span class=\"p\">)</span>\n\n\n<span class=\"c1\">#引入graphviz模块用来导出图,结果图如下所示</span>\n<span class=\"kn\">import</span> <span class=\"nn\">graphviz</span>\n<span class=\"n\">dot_data</span><span class=\"o\">=</span><span class=\"n\">tree</span><span class=\"o\">.</span><span class=\"n\">export_graphviz</span><span class=\"p\">(</span><span class=\"n\">clf</span><span class=\"p\">,</span><span class=\"n\">out_file</span><span class=\"o\">=</span><span class=\"bp\">None</span><span class=\"p\">,</span>\n                              <span class=\"n\">feature_names</span><span class=\"o\">=</span><span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">feature_names</span><span class=\"p\">,</span>\n                              <span class=\"n\">class_names</span><span class=\"o\">=</span><span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">target_names</span><span class=\"p\">,</span>\n                              <span class=\"n\">filled</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span><span class=\"n\">rounded</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span>\n                              <span class=\"n\">special_characters</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n\n<span class=\"n\">graph</span><span class=\"o\">=</span><span class=\"n\">graphviz</span><span class=\"o\">.</span><span class=\"n\">Source</span><span class=\"p\">(</span><span class=\"n\">dot_data</span><span class=\"p\">)</span>\n<span class=\"n\">graph</span><span class=\"o\">.</span><span class=\"n\">view</span><span class=\"p\">()</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-afbf1db8ff0de33e4fc8bc9f384c481d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2030\" data-rawheight=\"1530\" class=\"origin_image zh-lightbox-thumb\" width=\"2030\" data-original=\"https://pic2.zhimg.com/v2-afbf1db8ff0de33e4fc8bc9f384c481d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2030&#39; height=&#39;1530&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2030\" data-rawheight=\"1530\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2030\" data-original=\"https://pic2.zhimg.com/v2-afbf1db8ff0de33e4fc8bc9f384c481d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-afbf1db8ff0de33e4fc8bc9f384c481d_b.jpg\"/></figure><h2>5.实际使用技巧</h2><ul><li>对于拥有大量特征的数据决策树会出现过拟合的现象。获得一个合适的样本比例和特征数量十分重要，因为在高维空间中只有少量的样本的树是十分容易过拟合的。</li><li>训练之前平衡数据集，以防止决策树偏向于主导类。可以通过从每个类中抽取相等数量的样本来进行类平衡，或者优选地通过将每个类的样本权重 (<code>sample_weight</code>) 的和归一化为相同的值。</li><li>考虑实现进行降维(PCA、ICA)，使决策树能够更好地找到具有分辨性的特征。</li><li>通过 <code>export</code> 功能可以可视化您的决策树。使用 <code>max_depth=3</code>作为初始树深度，让决策树知道如何适应您的数据，然后再增加树的深度。</li><li>填充树的样本数量会增加树的每个附加级别。使用 <code>max_depth</code> 来控制树的大小防止过拟合。</li><li>通过使用 <code>min_samples_split</code> 和 <code>min_samples_leaf</code> 来控制叶节点上的样本数量。当这个值很小时意味着生成的决策树将会过拟合，然而当这个值很大时将会不利于决策树的对样本的学习。<br/>6.推广</li></ul><p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p><p><a href=\"https://link.zhihu.com/?target=http%3A//weixin.qq.com/r/Ty_4oDPE2W6mrXfD93pd\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">weixin.qq.com/r/Ty_4oDP</span><span class=\"invisible\">E2W6mrXfD93pd</span><span class=\"ellipsis\"></span></a> (二维码自动识别)</p><p></p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "决策树", 
                    "tagLink": "https://api.zhihu.com/topics/19569936"
                }, 
                {
                    "tag": "sklearn", 
                    "tagLink": "https://api.zhihu.com/topics/20063470"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/35708083", 
            "userName": "谓之小一", 
            "userLink": "https://www.zhihu.com/people/761dda159c5cec5fd926f761ab5f4ccf", 
            "upvote": 53, 
            "title": "Python之Sklearn使用教程", 
            "content": "<h2>1.Sklearn简介</h2><p>Scikit-learn(sklearn)是机器学习中常用的第三方模块，对常用的机器学习方法进行了封装，包括回归(Regression)、降维(Dimensionality Reduction)、分类(Classfication)、聚类(Clustering)等方法。当我们面临机器学习问题时，便可根据下图来选择相应的方法。Sklearn具有以下特点：</p><ul><li>简单高效的数据挖掘和数据分析工具</li><li>让每个人能够在复杂环境中重复使用</li><li>建立NumPy、Scipy、MatPlotLib之上</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-11a0911e98b81e1b438699b766dd2f6e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1133\" data-rawheight=\"656\" class=\"origin_image zh-lightbox-thumb\" width=\"1133\" data-original=\"https://pic3.zhimg.com/v2-11a0911e98b81e1b438699b766dd2f6e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1133&#39; height=&#39;656&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1133\" data-rawheight=\"656\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1133\" data-original=\"https://pic3.zhimg.com/v2-11a0911e98b81e1b438699b766dd2f6e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-11a0911e98b81e1b438699b766dd2f6e_b.jpg\"/></figure><h2>2.Sklearn安装</h2><p>Sklearn安装要求<code>Python(&gt;=2.7 or &gt;=3.3)</code>、<code>NumPy (&gt;= 1.8.2)</code>、<code>SciPy (&gt;= 0.13.3)</code>。如果已经安装NumPy和SciPy，安装scikit-learn可以使用<code>pip install -U scikit-learn</code>。</p><h2>3.Sklearn通用学习模式</h2><p>Sklearn中包含众多机器学习方法，但各种学习方法大致相同，我们在这里介绍Sklearn通用学习模式。首先引入需要训练的数据，Sklearn自带部分数据集，也可以通过相应方法进行构造，<code>4.Sklearn datasets</code>中我们会介绍如何构造数据。然后选择相应机器学习方法进行训练，训练过程中可以通过一些技巧调整参数，使得学习准确率更高。模型训练完成之后便可预测新数据，然后我们还可以通过<code>MatPlotLib</code>等方法来直观的展示数据。另外还可以将我们已训练好的Model进行保存，方便移动到其他平台，不必重新训练。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">from</span> <span class=\"nn\">sklearn</span> <span class=\"kn\">import</span> <span class=\"n\">datasets</span><span class=\"c1\">#引入数据集,sklearn包含众多数据集</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.model_selection</span> <span class=\"kn\">import</span> <span class=\"n\">train_test_split</span><span class=\"c1\">#将数据分为测试集和训练集</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.neighbors</span> <span class=\"kn\">import</span> <span class=\"n\">KNeighborsClassifier</span><span class=\"c1\">#利用邻近点方式训练数据</span>\n\n<span class=\"c1\">###引入数据###</span>\n<span class=\"n\">iris</span><span class=\"o\">=</span><span class=\"n\">datasets</span><span class=\"o\">.</span><span class=\"n\">load_iris</span><span class=\"p\">()</span><span class=\"c1\">#引入iris鸢尾花数据,iris数据包含4个特征变量</span>\n<span class=\"n\">iris_X</span><span class=\"o\">=</span><span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"c1\">#特征变量</span>\n<span class=\"n\">iris_y</span><span class=\"o\">=</span><span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">target</span><span class=\"c1\">#目标值</span>\n<span class=\"n\">X_train</span><span class=\"p\">,</span><span class=\"n\">X_test</span><span class=\"p\">,</span><span class=\"n\">y_train</span><span class=\"p\">,</span><span class=\"n\">y_test</span><span class=\"o\">=</span><span class=\"n\">train_test_split</span><span class=\"p\">(</span><span class=\"n\">iris_X</span><span class=\"p\">,</span><span class=\"n\">iris_y</span><span class=\"p\">,</span><span class=\"n\">test_size</span><span class=\"o\">=</span><span class=\"mf\">0.3</span><span class=\"p\">)</span><span class=\"c1\">#利用train_test_split进行将训练集和测试集进行分开，test_size占30%</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">y_train</span><span class=\"p\">)</span><span class=\"c1\">#我们看到训练数据的特征值分为3类</span>\n<span class=\"s1\">&#39;&#39;&#39;\n</span><span class=\"s1\">[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n</span><span class=\"s1\"> 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n</span><span class=\"s1\"> 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n</span><span class=\"s1\"> 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n</span><span class=\"s1\"> 2 2]\n</span><span class=\"s1\"> &#39;&#39;&#39;</span>\n\n<span class=\"c1\">###训练数据###</span>\n<span class=\"n\">knn</span><span class=\"o\">=</span><span class=\"n\">KNeighborsClassifier</span><span class=\"p\">()</span><span class=\"c1\">#引入训练方法</span>\n<span class=\"n\">knn</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">,</span><span class=\"n\">y_train</span><span class=\"p\">)</span><span class=\"c1\">#进行填充测试数据进行训练</span>\n\n<span class=\"c1\">###预测数据###</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">knn</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">X_test</span><span class=\"p\">))</span><span class=\"c1\">#预测特征值</span>\n<span class=\"s1\">&#39;&#39;&#39;\n</span><span class=\"s1\">[1 1 1 0 2 2 1 1 1 0 0 0 2 2 0 1 2 2 0 1 0 0 0 0 0 0 2 1 0 0 0 1 0 2 0 2 0\n</span><span class=\"s1\"> 1 2 1 0 0 1 0 2]\n</span><span class=\"s1\">&#39;&#39;&#39;</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">y_test</span><span class=\"p\">)</span><span class=\"c1\">#真实特征值</span>\n<span class=\"s1\">&#39;&#39;&#39;\n</span><span class=\"s1\">[1 1 1 0 1 2 1 1 1 0 0 0 2 2 0 1 2 2 0 1 0 0 0 0 0 0 2 1 0 0 0 1 0 2 0 2 0\n</span><span class=\"s1\"> 1 2 1 0 0 1 0 2]\n</span><span class=\"s1\">&#39;&#39;&#39;</span></code></pre></div><h2>4.Sklearn datasets</h2><p>Sklearn提供一些标准数据，我们不必再从其他网站寻找数据进行训练。例如我们上面用来训练的<code>load_iris</code>数据，可以很方便的返回数据特征变量和目标值。除了引入数据之外，我们还可以通过<code>load_sample_images()</code>来引入图片。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-ba6b56e44216ab40a00b32a003c1df25_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1600\" data-rawheight=\"300\" class=\"origin_image zh-lightbox-thumb\" width=\"1600\" data-original=\"https://pic2.zhimg.com/v2-ba6b56e44216ab40a00b32a003c1df25_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1600&#39; height=&#39;300&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1600\" data-rawheight=\"300\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1600\" data-original=\"https://pic2.zhimg.com/v2-ba6b56e44216ab40a00b32a003c1df25_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-ba6b56e44216ab40a00b32a003c1df25_b.jpg\"/></figure><p>除了sklearn提供的一些数据之外，还可以自己来构造一些数据帮助我们学习。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">from</span> <span class=\"nn\">sklearn</span> <span class=\"kn\">import</span> <span class=\"n\">datasets</span><span class=\"c1\">#引入数据集</span>\n<span class=\"c1\">#构造的各种参数可以根据自己需要调整</span>\n<span class=\"n\">X</span><span class=\"p\">,</span><span class=\"n\">y</span><span class=\"o\">=</span><span class=\"n\">datasets</span><span class=\"o\">.</span><span class=\"n\">make_regression</span><span class=\"p\">(</span><span class=\"n\">n_samples</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">,</span><span class=\"n\">n_features</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"n\">n_targets</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"n\">noise</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n\n<span class=\"c1\">###绘制构造的数据###</span>\n<span class=\"kn\">import</span> <span class=\"nn\">matplotlib.pyplot</span> <span class=\"kn\">as</span> <span class=\"nn\">plt</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">figure</span><span class=\"p\">()</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">scatter</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span><span class=\"n\">y</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span></code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-68aa886d9739dc74bb7cf8226276cba8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"774\" data-rawheight=\"568\" class=\"origin_image zh-lightbox-thumb\" width=\"774\" data-original=\"https://pic1.zhimg.com/v2-68aa886d9739dc74bb7cf8226276cba8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;774&#39; height=&#39;568&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"774\" data-rawheight=\"568\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"774\" data-original=\"https://pic1.zhimg.com/v2-68aa886d9739dc74bb7cf8226276cba8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-68aa886d9739dc74bb7cf8226276cba8_b.jpg\"/></figure><h2>5.Sklearn Model的属性和功能</h2><p>数据训练完成之后得到模型，我们可以根据不同模型得到相应的属性和功能，并将其输出得到直观结果。假如通过线性回归训练之后得到线性函数<code>y=0.3x+1</code>，我们可通过<code>_coef</code>得到模型的系数为0.3，通过<code>_intercept</code>得到模型的截距为1。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">from</span> <span class=\"nn\">sklearn</span> <span class=\"kn\">import</span> <span class=\"n\">datasets</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.linear_model</span> <span class=\"kn\">import</span> <span class=\"n\">LinearRegression</span><span class=\"c1\">#引入线性回归模型</span>\n\n<span class=\"c1\">###引入数据###</span>\n<span class=\"n\">load_data</span><span class=\"o\">=</span><span class=\"n\">datasets</span><span class=\"o\">.</span><span class=\"n\">load_boston</span><span class=\"p\">()</span>\n<span class=\"n\">data_X</span><span class=\"o\">=</span><span class=\"n\">load_data</span><span class=\"o\">.</span><span class=\"n\">data</span>\n<span class=\"n\">data_y</span><span class=\"o\">=</span><span class=\"n\">load_data</span><span class=\"o\">.</span><span class=\"n\">target</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">data_X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span>\n<span class=\"c1\">#(506, 13)data_X共13个特征变量</span>\n\n<span class=\"c1\">###训练数据###</span>\n<span class=\"n\">model</span><span class=\"o\">=</span><span class=\"n\">LinearRegression</span><span class=\"p\">()</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">data_X</span><span class=\"p\">,</span><span class=\"n\">data_y</span><span class=\"p\">)</span>\n<span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">data_X</span><span class=\"p\">[:</span><span class=\"mi\">4</span><span class=\"p\">,:])</span><span class=\"c1\">#预测前4个数据</span>\n\n<span class=\"c1\">###属性和功能###</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">coef_</span><span class=\"p\">)</span>\n<span class=\"s1\">&#39;&#39;&#39;\n</span><span class=\"s1\">[ -1.07170557e-01   4.63952195e-02   2.08602395e-02   2.68856140e+00\n</span><span class=\"s1\">  -1.77957587e+01   3.80475246e+00   7.51061703e-04  -1.47575880e+00\n</span><span class=\"s1\">   3.05655038e-01  -1.23293463e-02  -9.53463555e-01   9.39251272e-03\n</span><span class=\"s1\">  -5.25466633e-01]\n</span><span class=\"s1\">&#39;&#39;&#39;</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">intercept_</span><span class=\"p\">)</span>\n<span class=\"c1\">#36.4911032804</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">get_params</span><span class=\"p\">())</span><span class=\"c1\">#得到模型的参数</span>\n<span class=\"c1\">#{&#39;copy_X&#39;: True, &#39;normalize&#39;: False, &#39;n_jobs&#39;: 1, &#39;fit_intercept&#39;: True}</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">score</span><span class=\"p\">(</span><span class=\"n\">data_X</span><span class=\"p\">,</span><span class=\"n\">data_y</span><span class=\"p\">))</span><span class=\"c1\">#对训练情况进行打分</span>\n<span class=\"c1\">#0.740607742865</span></code></pre></div><h2>6.Sklearn数据预处理</h2><p>数据集的标准化对于大部分机器学习算法来说都是一种常规要求，如果单个特征没有或多或少地接近于标准正态分布，那么它可能并不能在项目中表现出很好的性能。在实际情况中,我们经常忽略特征的分布形状，直接去均值来对某个特征进行中心化，再通过除以非常量特征(non-constant features)的标准差进行缩放。</p><p>例如, 许多学习算法中目标函数的基础都是假设所有的特征都是零均值并且具有同一阶数上的方差(比如径向基函数、支持向量机以及L1L2正则化项等)。如果某个特征的方差比其他特征大几个数量级，那么它就会在学习算法中占据主导位置，导致学习器并不能像我们说期望的那样，从其他特征中学习。例如我们可以通过Scale将数据缩放，达到标准化的目的。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">from</span> <span class=\"nn\">sklearn</span> <span class=\"kn\">import</span> <span class=\"n\">preprocessing</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"kn\">as</span> <span class=\"nn\">np</span>\n<span class=\"n\">a</span><span class=\"o\">=</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([[</span><span class=\"mi\">10</span><span class=\"p\">,</span><span class=\"mf\">2.7</span><span class=\"p\">,</span><span class=\"mf\">3.6</span><span class=\"p\">],</span>\n            <span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">100</span><span class=\"p\">,</span><span class=\"mi\">5</span><span class=\"p\">,</span><span class=\"o\">-</span><span class=\"mi\">2</span><span class=\"p\">],</span>\n            <span class=\"p\">[</span><span class=\"mi\">120</span><span class=\"p\">,</span><span class=\"mi\">20</span><span class=\"p\">,</span><span class=\"mi\">40</span><span class=\"p\">]],</span><span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">float64</span><span class=\"p\">)</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">)</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">preprocessing</span><span class=\"o\">.</span><span class=\"n\">scale</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">))</span><span class=\"c1\">#将值的相差度减小</span>\n<span class=\"s1\">&#39;&#39;&#39;\n</span><span class=\"s1\">[[  10.     2.7    3.6]\n</span><span class=\"s1\"> [-100.     5.    -2. ]\n</span><span class=\"s1\"> [ 120.    20.    40\n</span><span class=\"s1\">[[ 0.         -0.85170713 -0.55138018]\n</span><span class=\"s1\"> [-1.22474487 -0.55187146 -0.852133  ]\n</span><span class=\"s1\"> [ 1.22474487  1.40357859  1.40351318]]\n</span><span class=\"s1\">&#39;&#39;&#39;</span></code></pre></div><p>我们来看下预处理前和预处理预处理后的差别，预处理之前模型评分为<code>0.511111111111</code>，预处理后模型评分为<code>0.933333333333</code>，可以看到预处理对模型评分有很大程度的提升。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">from</span> <span class=\"nn\">sklearn.model_selection</span> <span class=\"kn\">import</span> <span class=\"n\">train_test_split</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.datasets.samples_generator</span> <span class=\"kn\">import</span> <span class=\"n\">make_classification</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.svm</span> <span class=\"kn\">import</span> <span class=\"n\">SVC</span>\n<span class=\"kn\">import</span> <span class=\"nn\">matplotlib.pyplot</span> <span class=\"kn\">as</span> <span class=\"nn\">plt</span>\n\n<span class=\"c1\">###生成的数据如下图所示###</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">figure</span>\n<span class=\"n\">X</span><span class=\"p\">,</span><span class=\"n\">y</span><span class=\"o\">=</span><span class=\"n\">make_classification</span><span class=\"p\">(</span><span class=\"n\">n_samples</span><span class=\"o\">=</span><span class=\"mi\">300</span><span class=\"p\">,</span><span class=\"n\">n_features</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span><span class=\"n\">n_redundant</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"n\">n_informative</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span>             <span class=\"n\">random_state</span><span class=\"o\">=</span><span class=\"mi\">22</span><span class=\"p\">,</span><span class=\"n\">n_clusters_per_class</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"n\">scale</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">scatter</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">[:,</span><span class=\"mi\">0</span><span class=\"p\">],</span><span class=\"n\">X</span><span class=\"p\">[:,</span><span class=\"mi\">1</span><span class=\"p\">],</span><span class=\"n\">c</span><span class=\"o\">=</span><span class=\"n\">y</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>\n\n<span class=\"c1\">###利用minmax方式对数据进行规范化###</span>\n<span class=\"n\">X</span><span class=\"o\">=</span><span class=\"n\">preprocessing</span><span class=\"o\">.</span><span class=\"n\">minmax_scale</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span><span class=\"c1\">#feature_range=(-1,1)可设置重置范围</span>\n<span class=\"n\">X_train</span><span class=\"p\">,</span><span class=\"n\">X_test</span><span class=\"p\">,</span><span class=\"n\">y_train</span><span class=\"p\">,</span><span class=\"n\">y_test</span><span class=\"o\">=</span><span class=\"n\">train_test_split</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span><span class=\"n\">y</span><span class=\"p\">,</span><span class=\"n\">test_size</span><span class=\"o\">=</span><span class=\"mf\">0.3</span><span class=\"p\">)</span>\n<span class=\"n\">clf</span><span class=\"o\">=</span><span class=\"n\">SVC</span><span class=\"p\">()</span>\n<span class=\"n\">clf</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">,</span><span class=\"n\">y_train</span><span class=\"p\">)</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">clf</span><span class=\"o\">.</span><span class=\"n\">score</span><span class=\"p\">(</span><span class=\"n\">X_test</span><span class=\"p\">,</span><span class=\"n\">y_test</span><span class=\"p\">))</span>\n<span class=\"c1\">#0.933333333333</span>\n<span class=\"c1\">#没有规范化之前我们的训练分数为0.511111111111,规范化后为0.933333333333,准确度有很大提升</span></code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-18e3a4ed11984cfabc45eed92f4b29a5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"728\" data-rawheight=\"544\" class=\"origin_image zh-lightbox-thumb\" width=\"728\" data-original=\"https://pic2.zhimg.com/v2-18e3a4ed11984cfabc45eed92f4b29a5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;728&#39; height=&#39;544&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"728\" data-rawheight=\"544\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"728\" data-original=\"https://pic2.zhimg.com/v2-18e3a4ed11984cfabc45eed92f4b29a5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-18e3a4ed11984cfabc45eed92f4b29a5_b.jpg\"/></figure><h2>7.交叉验证</h2><p>交叉验证的基本思想是将原始数据进行分组，一部分做为训练集来训练模型，另一部分做为测试集来评价模型。交叉验证用于评估模型的预测性能，尤其是训练好的模型在新数据上的表现，可以在一定程度上减小过拟合。还可以从有限的数据中获取尽可能多的有效信息。</p><p>机器学习任务中，拿到数据后，我们首先会将原始数据集分为三部分：<b>训练集、验证集和测试集</b>。 训练集用于训练模型，验证集用于模型的参数选择配置，测试集对于模型来说是未知数据，用于评估模型的泛化能力。不同的划分会得到不同的最终模型。</p><p>以前我们是直接将数据分割成70%的训练数据和测试数据，现在我们利用K折交叉验证分割数据，首先将数据分为5组，然后再从5组数据之中选择不同数据进行训练。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-a61706538a88d879749a05e83794d718_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"866\" data-rawheight=\"1212\" class=\"origin_image zh-lightbox-thumb\" width=\"866\" data-original=\"https://pic1.zhimg.com/v2-a61706538a88d879749a05e83794d718_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;866&#39; height=&#39;1212&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"866\" data-rawheight=\"1212\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"866\" data-original=\"https://pic1.zhimg.com/v2-a61706538a88d879749a05e83794d718_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-a61706538a88d879749a05e83794d718_b.jpg\"/></figure><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">from</span> <span class=\"nn\">sklearn.datasets</span> <span class=\"kn\">import</span> <span class=\"n\">load_iris</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.model_selection</span> <span class=\"kn\">import</span> <span class=\"n\">train_test_split</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.neighbors</span> <span class=\"kn\">import</span> <span class=\"n\">KNeighborsClassifier</span>\n\n<span class=\"c1\">###引入数据###</span>\n<span class=\"n\">iris</span><span class=\"o\">=</span><span class=\"n\">load_iris</span><span class=\"p\">()</span>\n<span class=\"n\">X</span><span class=\"o\">=</span><span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">data</span>\n<span class=\"n\">y</span><span class=\"o\">=</span><span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">target</span>\n\n<span class=\"c1\">###训练数据###</span>\n<span class=\"n\">X_train</span><span class=\"p\">,</span><span class=\"n\">X_test</span><span class=\"p\">,</span><span class=\"n\">y_train</span><span class=\"p\">,</span><span class=\"n\">y_test</span><span class=\"o\">=</span><span class=\"n\">train_test_split</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span><span class=\"n\">y</span><span class=\"p\">,</span><span class=\"n\">test_size</span><span class=\"o\">=</span><span class=\"mf\">0.3</span><span class=\"p\">)</span>\n<span class=\"c1\">#引入交叉验证,数据分为5组进行训练</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.model_selection</span> <span class=\"kn\">import</span> <span class=\"n\">cross_val_score</span>\n<span class=\"n\">knn</span><span class=\"o\">=</span><span class=\"n\">KNeighborsClassifier</span><span class=\"p\">(</span><span class=\"n\">n_neighbors</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">)</span><span class=\"c1\">#选择邻近的5个点</span>\n<span class=\"n\">scores</span><span class=\"o\">=</span><span class=\"n\">cross_val_score</span><span class=\"p\">(</span><span class=\"n\">knn</span><span class=\"p\">,</span><span class=\"n\">X</span><span class=\"p\">,</span><span class=\"n\">y</span><span class=\"p\">,</span><span class=\"n\">cv</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span><span class=\"n\">scoring</span><span class=\"o\">=</span><span class=\"s1\">&#39;accuracy&#39;</span><span class=\"p\">)</span><span class=\"c1\">#评分方式为accuracy</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">scores</span><span class=\"p\">)</span><span class=\"c1\">#每组的评分结果</span>\n<span class=\"c1\">#[ 0.96666667  1.          0.93333333  0.96666667  1.        ]5组数据</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">scores</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">())</span><span class=\"c1\">#平均评分结果</span>\n<span class=\"c1\">#0.973333333333</span></code></pre></div><p>那么是否<b>n_neighbor=5</b>便是最好呢，我们来调整参数来看模型最终训练分数。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">from</span> <span class=\"nn\">sklearn</span> <span class=\"kn\">import</span> <span class=\"n\">datasets</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.model_selection</span> <span class=\"kn\">import</span> <span class=\"n\">train_test_split</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.neighbors</span> <span class=\"kn\">import</span> <span class=\"n\">KNeighborsClassifier</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.model_selection</span> <span class=\"kn\">import</span> <span class=\"n\">cross_val_score</span><span class=\"c1\">#引入交叉验证</span>\n<span class=\"kn\">import</span>  <span class=\"nn\">matplotlib.pyplot</span> <span class=\"kn\">as</span> <span class=\"nn\">plt</span>\n<span class=\"c1\">###引入数据###</span>\n<span class=\"n\">iris</span><span class=\"o\">=</span><span class=\"n\">datasets</span><span class=\"o\">.</span><span class=\"n\">load_iris</span><span class=\"p\">()</span>\n<span class=\"n\">X</span><span class=\"o\">=</span><span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">data</span>\n<span class=\"n\">y</span><span class=\"o\">=</span><span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">target</span>\n<span class=\"c1\">###设置n_neighbors的值为1到30,通过绘图来看训练分数###</span>\n<span class=\"n\">k_range</span><span class=\"o\">=</span><span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">31</span><span class=\"p\">)</span>\n<span class=\"n\">k_score</span><span class=\"o\">=</span><span class=\"p\">[]</span>\n<span class=\"k\">for</span> <span class=\"n\">k</span> <span class=\"ow\">in</span> <span class=\"n\">k_range</span><span class=\"p\">:</span>\n    <span class=\"n\">knn</span><span class=\"o\">=</span><span class=\"n\">KNeighborsClassifier</span><span class=\"p\">(</span><span class=\"n\">n_neighbors</span><span class=\"o\">=</span><span class=\"n\">k</span><span class=\"p\">)</span>\n    <span class=\"n\">scores</span><span class=\"o\">=</span><span class=\"n\">cross_val_score</span><span class=\"p\">(</span><span class=\"n\">knn</span><span class=\"p\">,</span><span class=\"n\">X</span><span class=\"p\">,</span><span class=\"n\">y</span><span class=\"p\">,</span><span class=\"n\">cv</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">,</span><span class=\"n\">scoring</span><span class=\"o\">=</span><span class=\"s1\">&#39;accuracy&#39;</span><span class=\"p\">)</span><span class=\"c1\">#for classfication</span>\n    <span class=\"n\">k_score</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">loss</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">())</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">figure</span><span class=\"p\">()</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">k_range</span><span class=\"p\">,</span><span class=\"n\">k_score</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">xlabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;Value of k for KNN&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">ylabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;CrossValidation accuracy&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>\n<span class=\"c1\">#K过大会带来过拟合问题,我们可以选择12-18之间的值</span></code></pre></div><p>我们可以看到n_neighbor在12-18之间评分比较高，实际项目之中我们可以通过这种方式来选择不同参数。另外我们还可以选择<code>2-fold Cross Validation</code>,<code>Leave-One-Out Cross Validation</code>等方法来分割数据，比较不同方法和参数得到最优结果。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-2ad7f37f5fbf9567f00cd2d693ef36e7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"718\" data-rawheight=\"550\" class=\"origin_image zh-lightbox-thumb\" width=\"718\" data-original=\"https://pic4.zhimg.com/v2-2ad7f37f5fbf9567f00cd2d693ef36e7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;718&#39; height=&#39;550&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"718\" data-rawheight=\"550\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"718\" data-original=\"https://pic4.zhimg.com/v2-2ad7f37f5fbf9567f00cd2d693ef36e7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-2ad7f37f5fbf9567f00cd2d693ef36e7_b.jpg\"/></figure><p>我们将上述代码中的循环部分改变一下，评分函数改为<code>neg_mean_squared_error</code>，便得到对于不同参数时的损失函数。</p><div class=\"highlight\"><pre><code class=\"language-text\">for k in k_range:\n    knn=KNeighborsClassifier(n_neighbors=k)\n    loss=-cross_val_score(knn,X,y,cv=10,scoring=&#39;neg_mean_squared_error&#39;)# for regression\n    k_score.append(loss.mean())</code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-72f23d13ae6d127cd543c173fa5e556e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"754\" data-rawheight=\"560\" class=\"origin_image zh-lightbox-thumb\" width=\"754\" data-original=\"https://pic3.zhimg.com/v2-72f23d13ae6d127cd543c173fa5e556e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;754&#39; height=&#39;560&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"754\" data-rawheight=\"560\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"754\" data-original=\"https://pic3.zhimg.com/v2-72f23d13ae6d127cd543c173fa5e556e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-72f23d13ae6d127cd543c173fa5e556e_b.jpg\"/></figure><h2>8.过拟合问题</h2><p>什么是过拟合问题呢？例如下面这张图片，黑色线已经可以很好的分类出红色点和蓝色点，但是在机器学习过程中，模型过于纠结准确度，便形成了绿色线的结果。然后在预测测试数据集结果的过程中往往会浪费很多时间并且准确率不是太好。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-17db9ea9595c80988a3125d871e83eac_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1390\" data-rawheight=\"1150\" class=\"origin_image zh-lightbox-thumb\" width=\"1390\" data-original=\"https://pic1.zhimg.com/v2-17db9ea9595c80988a3125d871e83eac_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1390&#39; height=&#39;1150&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1390\" data-rawheight=\"1150\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1390\" data-original=\"https://pic1.zhimg.com/v2-17db9ea9595c80988a3125d871e83eac_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-17db9ea9595c80988a3125d871e83eac_b.jpg\"/></figure><p>我们先举例如何辨别<b>overfitting</b>问题。Sklearn.learning_curve中的learning curve可以很直观的看出Model学习的进度，对比发现有没有过拟合。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">from</span> <span class=\"nn\">sklearn.model_selection</span> <span class=\"kn\">import</span> <span class=\"n\">learning_curve</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.datasets</span> <span class=\"kn\">import</span> <span class=\"n\">load_digits</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.svm</span> <span class=\"kn\">import</span> <span class=\"n\">SVC</span>\n<span class=\"kn\">import</span> <span class=\"nn\">matplotlib.pyplot</span> <span class=\"kn\">as</span> <span class=\"nn\">plt</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"kn\">as</span> <span class=\"nn\">np</span>\n\n<span class=\"c1\">#引入数据</span>\n<span class=\"n\">digits</span><span class=\"o\">=</span><span class=\"n\">load_digits</span><span class=\"p\">()</span>\n<span class=\"n\">X</span><span class=\"o\">=</span><span class=\"n\">digits</span><span class=\"o\">.</span><span class=\"n\">data</span>\n<span class=\"n\">y</span><span class=\"o\">=</span><span class=\"n\">digits</span><span class=\"o\">.</span><span class=\"n\">target</span>\n\n<span class=\"c1\">#train_size表示记录学习过程中的某一步,比如在10%,25%...的过程中记录一下</span>\n<span class=\"n\">train_size</span><span class=\"p\">,</span><span class=\"n\">train_loss</span><span class=\"p\">,</span><span class=\"n\">test_loss</span><span class=\"o\">=</span><span class=\"n\">learning_curve</span><span class=\"p\">(</span>\n    <span class=\"n\">SVC</span><span class=\"p\">(</span><span class=\"n\">gamma</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">),</span><span class=\"n\">X</span><span class=\"p\">,</span><span class=\"n\">y</span><span class=\"p\">,</span><span class=\"n\">cv</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">,</span><span class=\"n\">scoring</span><span class=\"o\">=</span><span class=\"s1\">&#39;neg_mean_squared_error&#39;</span><span class=\"p\">,</span>\n    <span class=\"n\">train_sizes</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mf\">0.1</span><span class=\"p\">,</span><span class=\"mf\">0.25</span><span class=\"p\">,</span><span class=\"mf\">0.5</span><span class=\"p\">,</span><span class=\"mf\">0.75</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n<span class=\"p\">)</span>\n<span class=\"n\">train_loss_mean</span><span class=\"o\">=-</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">train_loss</span><span class=\"p\">,</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"n\">test_loss_mean</span><span class=\"o\">=-</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">test_loss</span><span class=\"p\">,</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">figure</span><span class=\"p\">()</span>\n<span class=\"c1\">#将每一步进行打印出来</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">train_size</span><span class=\"p\">,</span><span class=\"n\">train_loss_mean</span><span class=\"p\">,</span><span class=\"s1\">&#39;o-&#39;</span><span class=\"p\">,</span><span class=\"n\">color</span><span class=\"o\">=</span><span class=\"s1\">&#39;r&#39;</span><span class=\"p\">,</span><span class=\"n\">label</span><span class=\"o\">=</span><span class=\"s1\">&#39;Training&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">train_size</span><span class=\"p\">,</span><span class=\"n\">test_loss_mean</span><span class=\"p\">,</span><span class=\"s1\">&#39;o-&#39;</span><span class=\"p\">,</span><span class=\"n\">color</span><span class=\"o\">=</span><span class=\"s1\">&#39;g&#39;</span><span class=\"p\">,</span><span class=\"n\">label</span><span class=\"o\">=</span><span class=\"s1\">&#39;Cross-validation&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">legend</span><span class=\"p\">(</span><span class=\"s1\">&#39;best&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span></code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-c0444889ff6a6998d70e9beec0a53e45_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"664\" data-rawheight=\"516\" class=\"origin_image zh-lightbox-thumb\" width=\"664\" data-original=\"https://pic2.zhimg.com/v2-c0444889ff6a6998d70e9beec0a53e45_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;664&#39; height=&#39;516&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"664\" data-rawheight=\"516\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"664\" data-original=\"https://pic2.zhimg.com/v2-c0444889ff6a6998d70e9beec0a53e45_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-c0444889ff6a6998d70e9beec0a53e45_b.jpg\"/></figure><p>如果我们改变gamma的值，那么会改变相应的Loss函数。损失函数便在10左右停留，此时便能直观的看出过拟合。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-d3e4d0afbde0c3b9fb897e5128cbc02f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"590\" data-rawheight=\"446\" class=\"origin_image zh-lightbox-thumb\" width=\"590\" data-original=\"https://pic4.zhimg.com/v2-d3e4d0afbde0c3b9fb897e5128cbc02f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;590&#39; height=&#39;446&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"590\" data-rawheight=\"446\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"590\" data-original=\"https://pic4.zhimg.com/v2-d3e4d0afbde0c3b9fb897e5128cbc02f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-d3e4d0afbde0c3b9fb897e5128cbc02f_b.jpg\"/></figure><p>下面我们通过修改gamma参数来修正过拟合问题。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">from</span> <span class=\"nn\">sklearn.model_selection</span> <span class=\"kn\">import</span>  <span class=\"n\">validation_curve</span><span class=\"c1\">#将learning_curve改为validation_curve</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.datasets</span> <span class=\"kn\">import</span> <span class=\"n\">load_digits</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.svm</span> <span class=\"kn\">import</span> <span class=\"n\">SVC</span>\n<span class=\"kn\">import</span> <span class=\"nn\">matplotlib.pyplot</span> <span class=\"kn\">as</span> <span class=\"nn\">plt</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"kn\">as</span> <span class=\"nn\">np</span>\n<span class=\"c1\">#引入数据</span>\n<span class=\"n\">digits</span><span class=\"o\">=</span><span class=\"n\">load_digits</span><span class=\"p\">()</span>\n<span class=\"n\">X</span><span class=\"o\">=</span><span class=\"n\">digits</span><span class=\"o\">.</span><span class=\"n\">data</span>\n<span class=\"n\">y</span><span class=\"o\">=</span><span class=\"n\">digits</span><span class=\"o\">.</span><span class=\"n\">target</span>\n\n<span class=\"c1\">#改变param来观察Loss函数情况</span>\n<span class=\"n\">param_range</span><span class=\"o\">=</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">logspace</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">6</span><span class=\"p\">,</span><span class=\"o\">-</span><span class=\"mf\">2.3</span><span class=\"p\">,</span><span class=\"mi\">5</span><span class=\"p\">)</span>\n<span class=\"n\">train_loss</span><span class=\"p\">,</span><span class=\"n\">test_loss</span><span class=\"o\">=</span><span class=\"n\">validation_curve</span><span class=\"p\">(</span>\n    <span class=\"n\">SVC</span><span class=\"p\">(),</span><span class=\"n\">X</span><span class=\"p\">,</span><span class=\"n\">y</span><span class=\"p\">,</span><span class=\"n\">param_name</span><span class=\"o\">=</span><span class=\"s1\">&#39;gamma&#39;</span><span class=\"p\">,</span><span class=\"n\">param_range</span><span class=\"o\">=</span><span class=\"n\">param_range</span><span class=\"p\">,</span><span class=\"n\">cv</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">,</span>\n    <span class=\"n\">scoring</span><span class=\"o\">=</span><span class=\"s1\">&#39;neg_mean_squared_error&#39;</span>\n<span class=\"p\">)</span>\n<span class=\"n\">train_loss_mean</span><span class=\"o\">=-</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">train_loss</span><span class=\"p\">,</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"n\">test_loss_mean</span><span class=\"o\">=-</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">test_loss</span><span class=\"p\">,</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">figure</span><span class=\"p\">()</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">param_range</span><span class=\"p\">,</span><span class=\"n\">train_loss_mean</span><span class=\"p\">,</span><span class=\"s1\">&#39;o-&#39;</span><span class=\"p\">,</span><span class=\"n\">color</span><span class=\"o\">=</span><span class=\"s1\">&#39;r&#39;</span><span class=\"p\">,</span><span class=\"n\">label</span><span class=\"o\">=</span><span class=\"s1\">&#39;Training&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">param_range</span><span class=\"p\">,</span><span class=\"n\">test_loss_mean</span><span class=\"p\">,</span><span class=\"s1\">&#39;o-&#39;</span><span class=\"p\">,</span><span class=\"n\">color</span><span class=\"o\">=</span><span class=\"s1\">&#39;g&#39;</span><span class=\"p\">,</span><span class=\"n\">label</span><span class=\"o\">=</span><span class=\"s1\">&#39;Cross-validation&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">xlabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;gamma&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">ylabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;loss&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">legend</span><span class=\"p\">(</span><span class=\"n\">loc</span><span class=\"o\">=</span><span class=\"s1\">&#39;best&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span></code></pre></div><p>通过改变不同的gamma值我们可以看到Loss函数的变化情况。从图中可以看到，如果gamma的值大于0.001便会出现过拟合的问题，那么我们构建模型时gamma参数设置应该小于0.001。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-fc7900978d9f6e671c535586f4701230_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"708\" data-rawheight=\"550\" class=\"origin_image zh-lightbox-thumb\" width=\"708\" data-original=\"https://pic1.zhimg.com/v2-fc7900978d9f6e671c535586f4701230_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;708&#39; height=&#39;550&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"708\" data-rawheight=\"550\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"708\" data-original=\"https://pic1.zhimg.com/v2-fc7900978d9f6e671c535586f4701230_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-fc7900978d9f6e671c535586f4701230_b.jpg\"/></figure><h2>9.保存模型</h2><p>我们花费很长时间用来训练数据，调整参数，得到最优模型。但如果改变平台，我们还需要重新训练数据和修正参数来得到模型，将会非常的浪费时间。此时我们可以先将model保存起来，然后便可以很方便的将模型迁移。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">from</span> <span class=\"nn\">sklearn</span> <span class=\"kn\">import</span> <span class=\"n\">svm</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn</span> <span class=\"kn\">import</span> <span class=\"n\">datasets</span>\n\n<span class=\"c1\">#引入和训练数据</span>\n<span class=\"n\">iris</span><span class=\"o\">=</span><span class=\"n\">datasets</span><span class=\"o\">.</span><span class=\"n\">load_iris</span><span class=\"p\">()</span>\n<span class=\"n\">X</span><span class=\"p\">,</span><span class=\"n\">y</span><span class=\"o\">=</span><span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">,</span><span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">target</span>\n<span class=\"n\">clf</span><span class=\"o\">=</span><span class=\"n\">svm</span><span class=\"o\">.</span><span class=\"n\">SVC</span><span class=\"p\">()</span>\n<span class=\"n\">clf</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span><span class=\"n\">y</span><span class=\"p\">)</span>\n\n<span class=\"c1\">#引入sklearn中自带的保存模块</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.externals</span> <span class=\"kn\">import</span> <span class=\"n\">joblib</span>\n<span class=\"c1\">#保存model</span>\n<span class=\"n\">joblib</span><span class=\"o\">.</span><span class=\"n\">dump</span><span class=\"p\">(</span><span class=\"n\">clf</span><span class=\"p\">,</span><span class=\"s1\">&#39;sklearn_save/clf.pkl&#39;</span><span class=\"p\">)</span>\n\n<span class=\"c1\">#重新加载model，只有保存一次后才能加载model</span>\n<span class=\"n\">clf3</span><span class=\"o\">=</span><span class=\"n\">joblib</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"s1\">&#39;sklearn_save/clf.pkl&#39;</span><span class=\"p\">)</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">clf3</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">:</span><span class=\"mi\">1</span><span class=\"p\">]))</span>\n<span class=\"c1\">#存放model能够更快的获得以前的结果</span></code></pre></div><h2>10.推广</h2><p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p><p><a href=\"https://link.zhihu.com/?target=http%3A//weixin.qq.com/r/Ty_4oDPE2W6mrXfD93pd\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">weixin.qq.com/r/Ty_4oDP</span><span class=\"invisible\">E2W6mrXfD93pd</span><span class=\"ellipsis\"></span></a> (二维码自动识别)</p><p></p>", 
            "topic": [
                {
                    "tag": "Python 库", 
                    "tagLink": "https://api.zhihu.com/topics/19644560"
                }, 
                {
                    "tag": "sklearn", 
                    "tagLink": "https://api.zhihu.com/topics/20063470"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/35364122", 
            "userName": "谓之小一", 
            "userLink": "https://www.zhihu.com/people/761dda159c5cec5fd926f761ab5f4ccf", 
            "upvote": 0, 
            "title": "机器学习之SVM支持向量机（二）", 
            "content": "<blockquote>由于较多公式，所以我将部分内容转为图片进行上传，请见谅。清晰版请访问<a href=\"https://link.zhihu.com/?target=https%3A//weizhixiaoyi.com\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">weizhixiaoyi.com</span><span class=\"invisible\"></span></a>查看，你也可以关注我公众号‘谓之小一’，后台直接向我要pdf版本，如有相关问题直接后台询问，随时回答。</blockquote><h2>1.知识回顾</h2><p><a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3MjA2NTQzMw%3D%3D%26mid%3D2247483818%26idx%3D1%26sn%3D50c634d8b00877134558125c4a718fd7%26chksm%3Dfcd7d25ccba05b4a62adfb2717650441f30d636056fcb37529ef34a51b94e453a534b7ca0a48%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">机器学习之SVM支持向量机（一）</a>中我们介绍了<b>SVM损失函数</b>、<b>最大间隔分类</b>、<b>为什么SVM能形成最大间隔分类器</b>、<b>核函数</b>、<b>SVM中Gaussian Kernel的使用</b>知识点。上文我们从Logistic Regression损失函数中推出SVM损失函数，本篇文章我们将更加直观的分析得到SVM损失函数、如何求解SVM对偶问题、如何解决outliers点，并且最终利用sklearn实现SVM。</p><h2>2.函数间隔和几何间隔</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-9352261e3b45568ac78fe5bca27eaaf8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1604\" data-rawheight=\"172\" class=\"origin_image zh-lightbox-thumb\" width=\"1604\" data-original=\"https://pic1.zhimg.com/v2-9352261e3b45568ac78fe5bca27eaaf8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1604&#39; height=&#39;172&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1604\" data-rawheight=\"172\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1604\" data-original=\"https://pic1.zhimg.com/v2-9352261e3b45568ac78fe5bca27eaaf8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-9352261e3b45568ac78fe5bca27eaaf8_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ed280f333b51bb681be5fb90b598124f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"611\" data-rawheight=\"384\" class=\"origin_image zh-lightbox-thumb\" width=\"611\" data-original=\"https://pic4.zhimg.com/v2-ed280f333b51bb681be5fb90b598124f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;611&#39; height=&#39;384&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"611\" data-rawheight=\"384\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"611\" data-original=\"https://pic4.zhimg.com/v2-ed280f333b51bb681be5fb90b598124f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-ed280f333b51bb681be5fb90b598124f_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-c1ad66cc5e6e6e3315b6a5561462a8a7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1602\" data-rawheight=\"816\" class=\"origin_image zh-lightbox-thumb\" width=\"1602\" data-original=\"https://pic4.zhimg.com/v2-c1ad66cc5e6e6e3315b6a5561462a8a7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1602&#39; height=&#39;816&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1602\" data-rawheight=\"816\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1602\" data-original=\"https://pic4.zhimg.com/v2-c1ad66cc5e6e6e3315b6a5561462a8a7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-c1ad66cc5e6e6e3315b6a5561462a8a7_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-c15ea6b38e81d41d87f9d37ebd63f627_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"150\" data-rawheight=\"150\" class=\"content_image\" width=\"150\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;150&#39; height=&#39;150&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"150\" data-rawheight=\"150\" class=\"content_image lazy\" width=\"150\" data-actualsrc=\"https://pic4.zhimg.com/v2-c15ea6b38e81d41d87f9d37ebd63f627_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-98128bdc99d4d5bfdf4e9264ccf968b5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1612\" data-rawheight=\"1204\" class=\"origin_image zh-lightbox-thumb\" width=\"1612\" data-original=\"https://pic2.zhimg.com/v2-98128bdc99d4d5bfdf4e9264ccf968b5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1612&#39; height=&#39;1204&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1612\" data-rawheight=\"1204\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1612\" data-original=\"https://pic2.zhimg.com/v2-98128bdc99d4d5bfdf4e9264ccf968b5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-98128bdc99d4d5bfdf4e9264ccf968b5_b.jpg\"/></figure><h2>3.原始问题到对偶问题的求解</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-9b2fcb9245439ea0d5ba60ea598e7336_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1618\" data-rawheight=\"1272\" class=\"origin_image zh-lightbox-thumb\" width=\"1618\" data-original=\"https://pic3.zhimg.com/v2-9b2fcb9245439ea0d5ba60ea598e7336_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1618&#39; height=&#39;1272&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1618\" data-rawheight=\"1272\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1618\" data-original=\"https://pic3.zhimg.com/v2-9b2fcb9245439ea0d5ba60ea598e7336_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-9b2fcb9245439ea0d5ba60ea598e7336_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-55541566f6385f75343ad17732e18f8e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1600\" data-rawheight=\"1512\" class=\"origin_image zh-lightbox-thumb\" width=\"1600\" data-original=\"https://pic3.zhimg.com/v2-55541566f6385f75343ad17732e18f8e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1600&#39; height=&#39;1512&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1600\" data-rawheight=\"1512\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1600\" data-original=\"https://pic3.zhimg.com/v2-55541566f6385f75343ad17732e18f8e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-55541566f6385f75343ad17732e18f8e_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-b04821b5c64f69b8cffa05f70b8bc2ef_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1604\" data-rawheight=\"1230\" class=\"origin_image zh-lightbox-thumb\" width=\"1604\" data-original=\"https://pic4.zhimg.com/v2-b04821b5c64f69b8cffa05f70b8bc2ef_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1604&#39; height=&#39;1230&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1604\" data-rawheight=\"1230\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1604\" data-original=\"https://pic4.zhimg.com/v2-b04821b5c64f69b8cffa05f70b8bc2ef_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-b04821b5c64f69b8cffa05f70b8bc2ef_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-46a4d74b4681465ad57e868cdbb08024_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1516\" data-rawheight=\"962\" class=\"origin_image zh-lightbox-thumb\" width=\"1516\" data-original=\"https://pic1.zhimg.com/v2-46a4d74b4681465ad57e868cdbb08024_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1516&#39; height=&#39;962&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1516\" data-rawheight=\"962\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1516\" data-original=\"https://pic1.zhimg.com/v2-46a4d74b4681465ad57e868cdbb08024_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-46a4d74b4681465ad57e868cdbb08024_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-9fbac00e283cdc68d89cb4f1959c5244_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1584\" data-rawheight=\"862\" class=\"origin_image zh-lightbox-thumb\" width=\"1584\" data-original=\"https://pic1.zhimg.com/v2-9fbac00e283cdc68d89cb4f1959c5244_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1584&#39; height=&#39;862&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1584\" data-rawheight=\"862\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1584\" data-original=\"https://pic1.zhimg.com/v2-9fbac00e283cdc68d89cb4f1959c5244_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-9fbac00e283cdc68d89cb4f1959c5244_b.jpg\"/></figure><h2>4.松弛变量处理outliers方法</h2><p>实际项目中会有数据点含有噪音，即偏离正常位置很远的数据点，我们称之为outlier。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-1a51ab3968c6fd68b98a052bcd304b2d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"391\" data-rawheight=\"384\" class=\"content_image\" width=\"391\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;391&#39; height=&#39;384&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"391\" data-rawheight=\"384\" class=\"content_image lazy\" width=\"391\" data-actualsrc=\"https://pic2.zhimg.com/v2-1a51ab3968c6fd68b98a052bcd304b2d_b.jpg\"/></figure><p>为了处理这种情况，SVM允许在一定程度上偏离一下超平面。为此我们稍加改变以前的约束条件，即</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-01717522663d62aa63c87f6d0c6f886a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1620\" data-rawheight=\"1110\" class=\"origin_image zh-lightbox-thumb\" width=\"1620\" data-original=\"https://pic3.zhimg.com/v2-01717522663d62aa63c87f6d0c6f886a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1620&#39; height=&#39;1110&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1620\" data-rawheight=\"1110\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1620\" data-original=\"https://pic3.zhimg.com/v2-01717522663d62aa63c87f6d0c6f886a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-01717522663d62aa63c87f6d0c6f886a_b.jpg\"/></figure><p>分析方法和前面相同，此处不再赘述。结合机器学习之SVM支持向量机（一）中的描述我们便能更好的理解C的作用和为什么C通常设置的都较大。</p><h2>5.Sklearn实现SVM支持向量机</h2><p>我们常用到的核函数包括线性核、多项式核、高斯核、sigmoid核。在<b>机器学习之SVM支持向量机（一）</b>中我们已经利用高斯核详细介绍了核函数的意义，所以不再利用其他核函数举例，有兴趣的同学可以去（一）中看详细内容。此处我们给出线性核和多项式核函数的代码，并使用了少量数据绘制出图形。因SVM选取核函数会涉及到较多内容，介于篇幅有限，不再这篇文章中解释，后续会详细写篇<b>SVM核函数的应用</b>。</p><h2>5.1线性</h2><div class=\"highlight\"><pre><code class=\"language-text\">from sklearn import svm\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(0)\nx=np.r_[np.random.randn(20,2)-[2,2],np.random.randn(20,2)+[2,2]]#正态分布产生数字20行2列\ny=[0]*20+[1]*20#20个class0,20个class1\nclf=svm.SVC(kernel=&#39;linear&#39;)#使用线性核\nclf.fit(x,y)\nw=clf.coef_[0]#获取w\na=-w[0]/w[1]#斜率\n\n#画图\nxx=np.linspace(-5,5)\nyy=a*xx-(clf.intercept_[0])/w[1]\nb=clf.support_vectors_[0]\nyy_down=a*xx+(b[1]-a*b[0])\nb=clf.support_vectors_[-1]\nyy_up=a*xx+(b[1]-a*b[0])\nplt.figure(figsize=(8,4))\nplt.plot(xx,yy)\nplt.plot(xx,yy_down)\nplt.plot(xx,yy_up)\nplt.scatter(clf.support_vectors_[:,0],clf.support_vectors_[:,1],s=80)\nplt.scatter(x[:,0],x[:,1],c=y,cmap=plt.cm.Paired)\nplt.axis(&#39;tight&#39;)\nplt.show()</code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-fa3f7c5fc1366b8c1a5317db302be21d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"878\" data-rawheight=\"454\" class=\"origin_image zh-lightbox-thumb\" width=\"878\" data-original=\"https://pic2.zhimg.com/v2-fa3f7c5fc1366b8c1a5317db302be21d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;878&#39; height=&#39;454&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"878\" data-rawheight=\"454\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"878\" data-original=\"https://pic2.zhimg.com/v2-fa3f7c5fc1366b8c1a5317db302be21d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-fa3f7c5fc1366b8c1a5317db302be21d_b.jpg\"/></figure><h2>5.2非线性</h2><div class=\"highlight\"><pre><code class=\"language-text\">import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_moons\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import  Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import LinearSVC\nX, y = make_moons( n_samples=100, noise=0.15, random_state=42 )\n\ndef plot_dataset(X, y, axes):\n    plt.plot( X[:,0][y==0], X[:,1][y==0], &#34;bs&#34; )\n    plt.plot( X[:,0][y==1], X[:,1][y==1], &#34;g^&#34; )\n    plt.axis( axes )\n    plt.grid( True, which=&#34;both&#34; )\n    plt.xlabel(r&#34;$x_l$&#34;)\n    plt.ylabel(r&#34;$x_2$&#34;)\n\n# contour函数是画出轮廓，需要给出X和Y的网格，以及对应的Z，它会画出Z的边界（相当于边缘检测及可视化）\ndef plot_predict(clf, axes):\n    x0s = np.linspace(axes[0], axes[1], 100)\n    x1s = np.linspace(axes[2], axes[3], 100)\n    x0, x1 = np.meshgrid( x0s, x1s )\n    X = np.c_[x0.ravel(), x1.ravel()]\n    y_pred = clf.predict( X ).reshape( x0.shape )\n    y_decision = clf.decision_function( X ).reshape( x0.shape )\n    plt.contour( x0, x1, y_pred, cmap=plt.cm.winter, alpha=0.5 )\n    plt.contour( x0, x1, y_decision, cmap=plt.cm.winter, alpha=0.2 )\n\npolynomial_svm_clf = Pipeline([ (&#34;poly_featutres&#34;, PolynomialFeatures(degree=3)),\n                                (&#34;scaler&#34;, StandardScaler()),\n                                (&#34;svm_clf&#34;, LinearSVC(C=10, loss=&#34;hinge&#34;, random_state=42)  )\n                            ])#多项式核函数\npolynomial_svm_clf.fit( X, y )\nplot_dataset( X, y, [-1.5, 2.5, -1, 1.5] )\nplot_predict( polynomial_svm_clf, [-1.5, 2.5, -1, 1.5] )\nplt.show()</code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-d22a0888b69d142c75bfd81a96eb0133_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"922\" data-rawheight=\"706\" class=\"origin_image zh-lightbox-thumb\" width=\"922\" data-original=\"https://pic4.zhimg.com/v2-d22a0888b69d142c75bfd81a96eb0133_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;922&#39; height=&#39;706&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"922\" data-rawheight=\"706\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"922\" data-original=\"https://pic4.zhimg.com/v2-d22a0888b69d142c75bfd81a96eb0133_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-d22a0888b69d142c75bfd81a96eb0133_b.jpg\"/></figure><h2>6.推广</h2><p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p><p><a href=\"https://link.zhihu.com/?target=http%3A//weixin.qq.com/r/Ty_4oDPE2W6mrXfD93pd\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">weixin.qq.com/r/Ty_4oDP</span><span class=\"invisible\">E2W6mrXfD93pd</span><span class=\"ellipsis\"></span></a> (二维码自动识别)</p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "SVM", 
                    "tagLink": "https://api.zhihu.com/topics/19583524"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/35249324", 
            "userName": "谓之小一", 
            "userLink": "https://www.zhihu.com/people/761dda159c5cec5fd926f761ab5f4ccf", 
            "upvote": 1, 
            "title": "机器学习之SVM支持向量机（一）", 
            "content": "<p>我们思考这样一个问题，给两个标签，蓝色和红色点，数据有两个特征(x,y)。我们想要一个分类器，给定一对(x,y)，能找到很好的分类边界，判断是蓝色点还是红色点。对于下图的数据，我们如何解决呢。本文通过引入Support Vector Machine（SVM）算法来详解此类问题。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-7cbe27a12a9d2961f9208ac9145be7df_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"575\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic4.zhimg.com/v2-7cbe27a12a9d2961f9208ac9145be7df_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;640&#39; height=&#39;575&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"575\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https://pic4.zhimg.com/v2-7cbe27a12a9d2961f9208ac9145be7df_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-7cbe27a12a9d2961f9208ac9145be7df_b.jpg\"/></figure><h2>1.SVM损失函数</h2><p>针对前面介绍的机器学习之线性回归、机器学习之Logistic回归，我们已经了解Cost Function的概念，这里我们利用Logistic Regression的损失函数来引入SVM损失函数。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-97b41df70574ceae848e8f430814784a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1602\" data-rawheight=\"950\" class=\"origin_image zh-lightbox-thumb\" width=\"1602\" data-original=\"https://pic3.zhimg.com/v2-97b41df70574ceae848e8f430814784a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1602&#39; height=&#39;950&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1602\" data-rawheight=\"950\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1602\" data-original=\"https://pic3.zhimg.com/v2-97b41df70574ceae848e8f430814784a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-97b41df70574ceae848e8f430814784a_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-91486179456139b287513bee1af80478_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1318\" data-rawheight=\"510\" class=\"origin_image zh-lightbox-thumb\" width=\"1318\" data-original=\"https://pic1.zhimg.com/v2-91486179456139b287513bee1af80478_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1318&#39; height=&#39;510&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1318\" data-rawheight=\"510\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1318\" data-original=\"https://pic1.zhimg.com/v2-91486179456139b287513bee1af80478_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-91486179456139b287513bee1af80478_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-b037f935e7ee1e770251ed43e7765a26_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1634\" data-rawheight=\"818\" class=\"origin_image zh-lightbox-thumb\" width=\"1634\" data-original=\"https://pic3.zhimg.com/v2-b037f935e7ee1e770251ed43e7765a26_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1634&#39; height=&#39;818&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1634\" data-rawheight=\"818\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1634\" data-original=\"https://pic3.zhimg.com/v2-b037f935e7ee1e770251ed43e7765a26_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-b037f935e7ee1e770251ed43e7765a26_b.jpg\"/></figure><h2>2.最大间隔分类</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-7e244d0fbe4715825482eb8624d43965_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1612\" data-rawheight=\"376\" class=\"origin_image zh-lightbox-thumb\" width=\"1612\" data-original=\"https://pic2.zhimg.com/v2-7e244d0fbe4715825482eb8624d43965_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1612&#39; height=&#39;376&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1612\" data-rawheight=\"376\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1612\" data-original=\"https://pic2.zhimg.com/v2-7e244d0fbe4715825482eb8624d43965_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-7e244d0fbe4715825482eb8624d43965_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-4363813afd2c590f155aa5ec3b580ced_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1318\" data-rawheight=\"330\" class=\"origin_image zh-lightbox-thumb\" width=\"1318\" data-original=\"https://pic2.zhimg.com/v2-4363813afd2c590f155aa5ec3b580ced_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1318&#39; height=&#39;330&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1318\" data-rawheight=\"330\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1318\" data-original=\"https://pic2.zhimg.com/v2-4363813afd2c590f155aa5ec3b580ced_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-4363813afd2c590f155aa5ec3b580ced_b.jpg\"/></figure><p>我们设<b>C为非常大的值</b>，例如1000000。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-01db7fc14be26d2189a337300e173d08_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1588\" data-rawheight=\"514\" class=\"origin_image zh-lightbox-thumb\" width=\"1588\" data-original=\"https://pic1.zhimg.com/v2-01db7fc14be26d2189a337300e173d08_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1588&#39; height=&#39;514&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1588\" data-rawheight=\"514\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1588\" data-original=\"https://pic1.zhimg.com/v2-01db7fc14be26d2189a337300e173d08_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-01db7fc14be26d2189a337300e173d08_b.jpg\"/></figure><p>SVM是一个<b>最大间隔分类器</b>，如下图所示，我们可以把黑线、红线、蓝线中任意一条当作decision boundary，但重点是哪一条最好呢？我们将在<b>模块3</b>中详细介绍为什么SVM能形成最大间隔分类器和如何正确选择分类边界。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-5a5dfbece39ae5e589f01b14d1576a02_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"746\" data-rawheight=\"498\" class=\"origin_image zh-lightbox-thumb\" width=\"746\" data-original=\"https://pic3.zhimg.com/v2-5a5dfbece39ae5e589f01b14d1576a02_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;746&#39; height=&#39;498&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"746\" data-rawheight=\"498\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"746\" data-original=\"https://pic3.zhimg.com/v2-5a5dfbece39ae5e589f01b14d1576a02_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-5a5dfbece39ae5e589f01b14d1576a02_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>我们希望一条直线可以很好的分开正样本和负样本，但当有一个异常点时，我们需要很大范围的改变直线，当然这是不理智的。黑色线时C很大的情况，红色线时C不是非常大，C设置很大表示对分类错误的惩罚。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-8a22f6ebf2bb2b29fc3f91038fc70bbd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"972\" data-rawheight=\"456\" class=\"origin_image zh-lightbox-thumb\" width=\"972\" data-original=\"https://pic2.zhimg.com/v2-8a22f6ebf2bb2b29fc3f91038fc70bbd_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;972&#39; height=&#39;456&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"972\" data-rawheight=\"456\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"972\" data-original=\"https://pic2.zhimg.com/v2-8a22f6ebf2bb2b29fc3f91038fc70bbd_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-8a22f6ebf2bb2b29fc3f91038fc70bbd_b.jpg\"/></figure><h2>3.SVM最大间隔分类</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ef792df82d1a7fe3cd833f96e9663768_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1622\" data-rawheight=\"260\" class=\"origin_image zh-lightbox-thumb\" width=\"1622\" data-original=\"https://pic1.zhimg.com/v2-ef792df82d1a7fe3cd833f96e9663768_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1622&#39; height=&#39;260&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1622\" data-rawheight=\"260\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1622\" data-original=\"https://pic1.zhimg.com/v2-ef792df82d1a7fe3cd833f96e9663768_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-ef792df82d1a7fe3cd833f96e9663768_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-e754243188b2f287cf919671e4bd9ca8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1270\" data-rawheight=\"674\" class=\"origin_image zh-lightbox-thumb\" width=\"1270\" data-original=\"https://pic1.zhimg.com/v2-e754243188b2f287cf919671e4bd9ca8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1270&#39; height=&#39;674&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1270\" data-rawheight=\"674\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1270\" data-original=\"https://pic1.zhimg.com/v2-e754243188b2f287cf919671e4bd9ca8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-e754243188b2f287cf919671e4bd9ca8_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-7ed91a5107a6f648c6297a4b707d5588_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1552\" data-rawheight=\"1092\" class=\"origin_image zh-lightbox-thumb\" width=\"1552\" data-original=\"https://pic1.zhimg.com/v2-7ed91a5107a6f648c6297a4b707d5588_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1552&#39; height=&#39;1092&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1552\" data-rawheight=\"1092\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1552\" data-original=\"https://pic1.zhimg.com/v2-7ed91a5107a6f648c6297a4b707d5588_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-7ed91a5107a6f648c6297a4b707d5588_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-0abbf6b93975d0798b2d90971cc3ed10_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1186\" data-rawheight=\"664\" class=\"origin_image zh-lightbox-thumb\" width=\"1186\" data-original=\"https://pic1.zhimg.com/v2-0abbf6b93975d0798b2d90971cc3ed10_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1186&#39; height=&#39;664&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1186\" data-rawheight=\"664\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1186\" data-original=\"https://pic1.zhimg.com/v2-0abbf6b93975d0798b2d90971cc3ed10_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-0abbf6b93975d0798b2d90971cc3ed10_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-b98723dcbe7e2864f7043a4797e43f7a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1624\" data-rawheight=\"176\" class=\"origin_image zh-lightbox-thumb\" width=\"1624\" data-original=\"https://pic3.zhimg.com/v2-b98723dcbe7e2864f7043a4797e43f7a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1624&#39; height=&#39;176&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1624\" data-rawheight=\"176\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1624\" data-original=\"https://pic3.zhimg.com/v2-b98723dcbe7e2864f7043a4797e43f7a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-b98723dcbe7e2864f7043a4797e43f7a_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ea0ce8e653988ed52456b05957a5a373_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1160\" data-rawheight=\"394\" class=\"origin_image zh-lightbox-thumb\" width=\"1160\" data-original=\"https://pic4.zhimg.com/v2-ea0ce8e653988ed52456b05957a5a373_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1160&#39; height=&#39;394&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1160\" data-rawheight=\"394\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1160\" data-original=\"https://pic4.zhimg.com/v2-ea0ce8e653988ed52456b05957a5a373_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-ea0ce8e653988ed52456b05957a5a373_b.jpg\"/></figure><h2>4.核函数</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-24107725c6940105ee9eee91ef4a06f8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1622\" data-rawheight=\"382\" class=\"origin_image zh-lightbox-thumb\" width=\"1622\" data-original=\"https://pic1.zhimg.com/v2-24107725c6940105ee9eee91ef4a06f8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1622&#39; height=&#39;382&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1622\" data-rawheight=\"382\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1622\" data-original=\"https://pic1.zhimg.com/v2-24107725c6940105ee9eee91ef4a06f8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-24107725c6940105ee9eee91ef4a06f8_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-2407d06a483920aa4a5521887349c0cc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1182\" data-rawheight=\"650\" class=\"origin_image zh-lightbox-thumb\" width=\"1182\" data-original=\"https://pic1.zhimg.com/v2-2407d06a483920aa4a5521887349c0cc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1182&#39; height=&#39;650&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1182\" data-rawheight=\"650\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1182\" data-original=\"https://pic1.zhimg.com/v2-2407d06a483920aa4a5521887349c0cc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-2407d06a483920aa4a5521887349c0cc_b.jpg\"/></figure><p>那么除了将fn定义为x的幂次项组合，还有其他方法表示f吗？此处我们引入核函数，对于非线性拟合，我们通过输入原始向量与landmark点之间的相似度来计算核值f，我们称相似度函数为核函数，下述核函数为高斯核函数。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-4353d0df94cf24e77a145dd34434d51a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1162\" data-rawheight=\"660\" class=\"origin_image zh-lightbox-thumb\" width=\"1162\" data-original=\"https://pic3.zhimg.com/v2-4353d0df94cf24e77a145dd34434d51a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1162&#39; height=&#39;660&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1162\" data-rawheight=\"660\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1162\" data-original=\"https://pic3.zhimg.com/v2-4353d0df94cf24e77a145dd34434d51a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-4353d0df94cf24e77a145dd34434d51a_b.jpg\"/></figure><p>x和l越相似，f越接近于1。x和l相差越远，f越接近于0。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-d0d9d4ef5e27a571f8fe0d2c466a0979_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1178\" data-rawheight=\"576\" class=\"origin_image zh-lightbox-thumb\" width=\"1178\" data-original=\"https://pic2.zhimg.com/v2-d0d9d4ef5e27a571f8fe0d2c466a0979_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1178&#39; height=&#39;576&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1178\" data-rawheight=\"576\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1178\" data-original=\"https://pic2.zhimg.com/v2-d0d9d4ef5e27a571f8fe0d2c466a0979_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-d0d9d4ef5e27a571f8fe0d2c466a0979_b.jpg\"/></figure><p>下图中横坐标为x的两个维度值，高为f。制高点为x=l的情况，此时f=1。随着x与l的远离，f逐渐下降，趋近于0。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-fc60a65340ee277730b6f3225caef82c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1192\" data-rawheight=\"652\" class=\"origin_image zh-lightbox-thumb\" width=\"1192\" data-original=\"https://pic1.zhimg.com/v2-fc60a65340ee277730b6f3225caef82c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1192&#39; height=&#39;652&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1192\" data-rawheight=\"652\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1192\" data-original=\"https://pic1.zhimg.com/v2-fc60a65340ee277730b6f3225caef82c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-fc60a65340ee277730b6f3225caef82c_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-9084acf2c91a68be905ade6dc130105e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1612\" data-rawheight=\"470\" class=\"origin_image zh-lightbox-thumb\" width=\"1612\" data-original=\"https://pic3.zhimg.com/v2-9084acf2c91a68be905ade6dc130105e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1612&#39; height=&#39;470&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1612\" data-rawheight=\"470\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1612\" data-original=\"https://pic3.zhimg.com/v2-9084acf2c91a68be905ade6dc130105e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-9084acf2c91a68be905ade6dc130105e_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-647f50b7c40204073c963477d4baf8f6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1182\" data-rawheight=\"624\" class=\"origin_image zh-lightbox-thumb\" width=\"1182\" data-original=\"https://pic3.zhimg.com/v2-647f50b7c40204073c963477d4baf8f6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1182&#39; height=&#39;624&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1182\" data-rawheight=\"624\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1182\" data-original=\"https://pic3.zhimg.com/v2-647f50b7c40204073c963477d4baf8f6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-647f50b7c40204073c963477d4baf8f6_b.jpg\"/></figure><h2>5.SVM中Gaussian Kernel的使用</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b1499984ffbcc20ed76f96db8d37e335_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1510\" data-rawheight=\"286\" class=\"origin_image zh-lightbox-thumb\" width=\"1510\" data-original=\"https://pic2.zhimg.com/v2-b1499984ffbcc20ed76f96db8d37e335_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1510&#39; height=&#39;286&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1510\" data-rawheight=\"286\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1510\" data-original=\"https://pic2.zhimg.com/v2-b1499984ffbcc20ed76f96db8d37e335_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-b1499984ffbcc20ed76f96db8d37e335_b.jpg\"/></figure><p>我们选择m个训练数据，并取这m个训练数据为m个landmark点（不考虑正样本还是负样本）。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-43fac223c69406ea55d1ecb038114c84_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1102\" data-rawheight=\"656\" class=\"origin_image zh-lightbox-thumb\" width=\"1102\" data-original=\"https://pic1.zhimg.com/v2-43fac223c69406ea55d1ecb038114c84_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1102&#39; height=&#39;656&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1102\" data-rawheight=\"656\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1102\" data-original=\"https://pic1.zhimg.com/v2-43fac223c69406ea55d1ecb038114c84_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-43fac223c69406ea55d1ecb038114c84_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-f2f346f7e1b99797370290bd1898ea12_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1178\" data-rawheight=\"666\" class=\"origin_image zh-lightbox-thumb\" width=\"1178\" data-original=\"https://pic3.zhimg.com/v2-f2f346f7e1b99797370290bd1898ea12_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1178&#39; height=&#39;666&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1178\" data-rawheight=\"666\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1178\" data-original=\"https://pic3.zhimg.com/v2-f2f346f7e1b99797370290bd1898ea12_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-f2f346f7e1b99797370290bd1898ea12_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-aad5f4f65faffce9010d12b6f2625a20_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1634\" data-rawheight=\"304\" class=\"origin_image zh-lightbox-thumb\" width=\"1634\" data-original=\"https://pic1.zhimg.com/v2-aad5f4f65faffce9010d12b6f2625a20_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1634&#39; height=&#39;304&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1634\" data-rawheight=\"304\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1634\" data-original=\"https://pic1.zhimg.com/v2-aad5f4f65faffce9010d12b6f2625a20_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-aad5f4f65faffce9010d12b6f2625a20_b.jpg\"/></figure><p>如下图所示，这里与之前的损失函数区别在于用kernel f代替了x。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-1275e1d3d72495e08336cea5b2cbeb04_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1170\" data-rawheight=\"634\" class=\"origin_image zh-lightbox-thumb\" width=\"1170\" data-original=\"https://pic1.zhimg.com/v2-1275e1d3d72495e08336cea5b2cbeb04_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1170&#39; height=&#39;634&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1170\" data-rawheight=\"634\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1170\" data-original=\"https://pic1.zhimg.com/v2-1275e1d3d72495e08336cea5b2cbeb04_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-1275e1d3d72495e08336cea5b2cbeb04_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-4120dc10864efb69955f133696bdfc59_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1594\" data-rawheight=\"412\" class=\"origin_image zh-lightbox-thumb\" width=\"1594\" data-original=\"https://pic2.zhimg.com/v2-4120dc10864efb69955f133696bdfc59_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1594&#39; height=&#39;412&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1594\" data-rawheight=\"412\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1594\" data-original=\"https://pic2.zhimg.com/v2-4120dc10864efb69955f133696bdfc59_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-4120dc10864efb69955f133696bdfc59_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-5378f85a0cbe1d971ea1cbbef24b0315_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1114\" data-rawheight=\"462\" class=\"origin_image zh-lightbox-thumb\" width=\"1114\" data-original=\"https://pic2.zhimg.com/v2-5378f85a0cbe1d971ea1cbbef24b0315_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1114&#39; height=&#39;462&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1114\" data-rawheight=\"462\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1114\" data-original=\"https://pic2.zhimg.com/v2-5378f85a0cbe1d971ea1cbbef24b0315_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-5378f85a0cbe1d971ea1cbbef24b0315_b.jpg\"/></figure><p>通常我们会从一些常用的核函数中选择，根据问题数据的不同，选择不同的参数，实际上就是得到不同的核函数。经常用到的核函数包括线性核、多项式核、高斯核。</p><p>由于本篇幅文章过长，我们将在下篇文章内详细介绍SVM算法中对偶问题的求解、C为何设置非常大、几种不同的核函数、SVM应用。如你在文中发现错误，欢迎指出，我会尽快更正。</p><h2>5.推广</h2><p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p><p><a href=\"https://link.zhihu.com/?target=http%3A//weixin.qq.com/r/Ty_4oDPE2W6mrXfD93pd\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">weixin.qq.com/r/Ty_4oDP</span><span class=\"invisible\">E2W6mrXfD93pd</span><span class=\"ellipsis\"></span></a> (二维码自动识别)</p><p></p><p></p><p></p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "SVM", 
                    "tagLink": "https://api.zhihu.com/topics/19583524"
                }, 
                {
                    "tag": "kernel（核函数）", 
                    "tagLink": "https://api.zhihu.com/topics/19619891"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/35056484", 
            "userName": "谓之小一", 
            "userLink": "https://www.zhihu.com/people/761dda159c5cec5fd926f761ab5f4ccf", 
            "upvote": 0, 
            "title": "机器学习之Logistic回归", 
            "content": "<blockquote>由于较多公式，不想再在知乎重新编辑一次，所以就直接截图啦。清晰版访问<a href=\"https://link.zhihu.com/?target=https%3A//weizhixiaoyi.com\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">weizhixiaoyi.com</span><span class=\"invisible\"></span></a></blockquote><h2>1.Logistic回归简介</h2><p>线性回归能够找到一个假设函数来估计原函数，从而根据特征变量来得到假设值，但线性回归模型不能达到分类的效果。在线性回归的基础上，我们将假设值和概率结合得到分类器，达到分类的效果。虽然Logistic回归是回归模型，但在实际项目中我们经常用于分类问题。</p><h2>2.Sigmoid函数</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-8de606b7a01209ef9dad6241a8f88524_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1612\" data-rawheight=\"1358\" class=\"origin_image zh-lightbox-thumb\" width=\"1612\" data-original=\"https://pic1.zhimg.com/v2-8de606b7a01209ef9dad6241a8f88524_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1612&#39; height=&#39;1358&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1612\" data-rawheight=\"1358\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1612\" data-original=\"https://pic1.zhimg.com/v2-8de606b7a01209ef9dad6241a8f88524_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-8de606b7a01209ef9dad6241a8f88524_b.jpg\"/></figure><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\">#plot sigmoid function </span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"kn\">as</span> <span class=\"nn\">np</span>\n<span class=\"kn\">import</span> <span class=\"nn\">matplotlib.pyplot</span> <span class=\"kn\">as</span> <span class=\"nn\">plt</span>\n\n<span class=\"c1\">##sigmoid function</span>\n<span class=\"n\">x</span><span class=\"o\">=</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">5</span><span class=\"p\">,</span><span class=\"mi\">5</span><span class=\"p\">,</span><span class=\"mf\">0.1</span><span class=\"p\">)</span>\n<span class=\"n\">y</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"o\">/</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"o\">+</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"n\">x</span><span class=\"p\">))</span>\n\n<span class=\"c1\">#plot</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">figure</span><span class=\"p\">()</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span><span class=\"n\">y</span><span class=\"p\">,</span><span class=\"n\">color</span><span class=\"o\">=</span><span class=\"s1\">&#39;red&#39;</span><span class=\"p\">,</span><span class=\"n\">linewidth</span><span class=\"o\">=</span><span class=\"s1\">&#39;2&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"o\">=</span><span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">gca</span><span class=\"p\">()</span>\n<span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">spines</span><span class=\"p\">[</span><span class=\"s1\">&#39;right&#39;</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_color</span><span class=\"p\">(</span><span class=\"s1\">&#39;none&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">spines</span><span class=\"p\">[</span><span class=\"s1\">&#39;top&#39;</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_color</span><span class=\"p\">(</span><span class=\"s1\">&#39;none&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">xaxis</span><span class=\"o\">.</span><span class=\"n\">set_ticks_position</span><span class=\"p\">(</span><span class=\"s1\">&#39;bottom&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">spines</span><span class=\"p\">[</span><span class=\"s1\">&#39;bottom&#39;</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_position</span><span class=\"p\">((</span><span class=\"s1\">&#39;data&#39;</span><span class=\"p\">,</span><span class=\"mi\">0</span><span class=\"p\">))</span>\n<span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">yaxis</span><span class=\"o\">.</span><span class=\"n\">set_ticks_position</span><span class=\"p\">(</span><span class=\"s1\">&#39;left&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">spines</span><span class=\"p\">[</span><span class=\"s1\">&#39;left&#39;</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_position</span><span class=\"p\">((</span><span class=\"s1\">&#39;data&#39;</span><span class=\"p\">,</span><span class=\"mi\">0</span><span class=\"p\">))</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">xlabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;independent variable&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">ylabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;dependent variable&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span></code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-d895a55ee0181c398db422a68d4809d7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"690\" data-rawheight=\"570\" class=\"origin_image zh-lightbox-thumb\" width=\"690\" data-original=\"https://pic4.zhimg.com/v2-d895a55ee0181c398db422a68d4809d7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;690&#39; height=&#39;570&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"690\" data-rawheight=\"570\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"690\" data-original=\"https://pic4.zhimg.com/v2-d895a55ee0181c398db422a68d4809d7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-d895a55ee0181c398db422a68d4809d7_b.jpg\"/></figure><h2>3.Logistic回归推导</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-d901ec8a5bfb3598b55863f6936378c9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1628\" data-rawheight=\"1336\" class=\"origin_image zh-lightbox-thumb\" width=\"1628\" data-original=\"https://pic2.zhimg.com/v2-d901ec8a5bfb3598b55863f6936378c9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1628&#39; height=&#39;1336&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1628\" data-rawheight=\"1336\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1628\" data-original=\"https://pic2.zhimg.com/v2-d901ec8a5bfb3598b55863f6936378c9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-d901ec8a5bfb3598b55863f6936378c9_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-26c7fd4c969852d57e8f31b498376bda_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1634\" data-rawheight=\"1530\" class=\"origin_image zh-lightbox-thumb\" width=\"1634\" data-original=\"https://pic3.zhimg.com/v2-26c7fd4c969852d57e8f31b498376bda_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1634&#39; height=&#39;1530&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1634\" data-rawheight=\"1530\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1634\" data-original=\"https://pic3.zhimg.com/v2-26c7fd4c969852d57e8f31b498376bda_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-26c7fd4c969852d57e8f31b498376bda_b.jpg\"/></figure><h2>4.1梯度下降算法简述</h2><p>实际生活中我们有时也利用梯度下降算法，比如我们处在一座山的某处位置，但我们并不知道如何下山，于是决定走一步算一步，但每次都沿着最陡峭的地点下山，也就是沿着梯度的负方向前进。但有事也会遇见问题，不能每次都到达山脚，可能到达山峰的某个局部最低点。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-5a26c9a4ff24427870764542154e484b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1680\" data-rawheight=\"866\" class=\"origin_image zh-lightbox-thumb\" width=\"1680\" data-original=\"https://pic4.zhimg.com/v2-5a26c9a4ff24427870764542154e484b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1680&#39; height=&#39;866&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1680\" data-rawheight=\"866\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1680\" data-original=\"https://pic4.zhimg.com/v2-5a26c9a4ff24427870764542154e484b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-5a26c9a4ff24427870764542154e484b_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>从上面解释可以看出，梯度下降不一定能够找到全局最优解，有可能是局部最优解，但此种方法已能帮助我们求解Logistic回归问题。另外如果求解的函数是凸函数，梯度下降法得到得解一定是全局最优解。</p><h2>4.2 梯度下降算法相关概念</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b5a9765499f68f02581bfc6f742e1f65_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1616\" data-rawheight=\"666\" class=\"origin_image zh-lightbox-thumb\" width=\"1616\" data-original=\"https://pic2.zhimg.com/v2-b5a9765499f68f02581bfc6f742e1f65_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1616&#39; height=&#39;666&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1616\" data-rawheight=\"666\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1616\" data-original=\"https://pic2.zhimg.com/v2-b5a9765499f68f02581bfc6f742e1f65_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-b5a9765499f68f02581bfc6f742e1f65_b.jpg\"/></figure><h2>4.3梯度下降算法过程</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-cb5e28ec0ce7c1712e6f4037accb79e4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1616\" data-rawheight=\"1206\" class=\"origin_image zh-lightbox-thumb\" width=\"1616\" data-original=\"https://pic1.zhimg.com/v2-cb5e28ec0ce7c1712e6f4037accb79e4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1616&#39; height=&#39;1206&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1616\" data-rawheight=\"1206\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1616\" data-original=\"https://pic1.zhimg.com/v2-cb5e28ec0ce7c1712e6f4037accb79e4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-cb5e28ec0ce7c1712e6f4037accb79e4_b.jpg\"/></figure><h2>5.Logistic回归实现</h2><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"kn\">as</span> <span class=\"nn\">np</span>\n<span class=\"kn\">import</span> <span class=\"nn\">matplotlib.pyplot</span> <span class=\"kn\">as</span> <span class=\"nn\">plt</span>\n<span class=\"kn\">from</span> <span class=\"nn\">matplotlib.colors</span> <span class=\"kn\">import</span> <span class=\"n\">ListedColormap</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn</span> <span class=\"kn\">import</span> <span class=\"n\">datasets</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.model_selection</span> <span class=\"kn\">import</span> <span class=\"n\">train_test_split</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.linear_model</span> <span class=\"kn\">import</span> <span class=\"n\">LogisticRegression</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.preprocessing</span> <span class=\"kn\">import</span> <span class=\"n\">StandardScaler</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">plot_decision_regions</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">classifier</span><span class=\"p\">,</span> <span class=\"n\">test_idx</span><span class=\"o\">=</span><span class=\"bp\">None</span><span class=\"p\">,</span> <span class=\"n\">resolution</span><span class=\"o\">=</span><span class=\"mf\">0.02</span><span class=\"p\">):</span>\n    <span class=\"c1\"># setup marker generator and color map</span>\n    <span class=\"n\">markers</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"s1\">&#39;s&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;x&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;o&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;^&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;v&#39;</span><span class=\"p\">)</span>\n    <span class=\"n\">colors</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"s1\">&#39;red&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;blue&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;lightgreen&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;cyan&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;gray&#39;</span><span class=\"p\">)</span>\n    <span class=\"n\">cmap</span> <span class=\"o\">=</span> <span class=\"n\">ListedColormap</span><span class=\"p\">(</span><span class=\"n\">colors</span><span class=\"p\">[:</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">unique</span><span class=\"p\">(</span><span class=\"n\">y</span><span class=\"p\">))])</span>\n    <span class=\"c1\"># plot the decision surface</span>\n    <span class=\"n\">x1_min</span><span class=\"p\">,</span> <span class=\"n\">x1_max</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"p\">[:,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"nb\">min</span><span class=\"p\">()</span> <span class=\"o\">-</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">[:,</span> <span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"nb\">max</span><span class=\"p\">()</span> <span class=\"o\">+</span> <span class=\"mi\">1</span>\n    <span class=\"n\">x2_min</span><span class=\"p\">,</span> <span class=\"n\">x2_max</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"p\">[:,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"nb\">min</span><span class=\"p\">()</span> <span class=\"o\">-</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">[:,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"nb\">max</span><span class=\"p\">()</span> <span class=\"o\">+</span> <span class=\"mi\">1</span>\n    <span class=\"n\">xx1</span><span class=\"p\">,</span> <span class=\"n\">xx2</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">meshgrid</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"n\">x1_min</span><span class=\"p\">,</span> <span class=\"n\">x1_max</span><span class=\"p\">,</span> <span class=\"n\">resolution</span><span class=\"p\">),</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"n\">x2_min</span><span class=\"p\">,</span> <span class=\"n\">x2_max</span><span class=\"p\">,</span> <span class=\"n\">resolution</span><span class=\"p\">))</span>\n    <span class=\"n\">Z</span> <span class=\"o\">=</span> <span class=\"n\">classifier</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"n\">xx1</span><span class=\"o\">.</span><span class=\"n\">ravel</span><span class=\"p\">(),</span> <span class=\"n\">xx2</span><span class=\"o\">.</span><span class=\"n\">ravel</span><span class=\"p\">()])</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">)</span>\n    <span class=\"n\">Z</span> <span class=\"o\">=</span> <span class=\"n\">Z</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"n\">xx1</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">contourf</span><span class=\"p\">(</span><span class=\"n\">xx1</span><span class=\"p\">,</span> <span class=\"n\">xx2</span><span class=\"p\">,</span> <span class=\"n\">Z</span><span class=\"p\">,</span> <span class=\"n\">alpha</span><span class=\"o\">=</span><span class=\"mf\">0.4</span><span class=\"p\">,</span> <span class=\"n\">cmap</span><span class=\"o\">=</span><span class=\"n\">cmap</span><span class=\"p\">)</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">xlim</span><span class=\"p\">(</span><span class=\"n\">xx1</span><span class=\"o\">.</span><span class=\"nb\">min</span><span class=\"p\">(),</span> <span class=\"n\">xx1</span><span class=\"o\">.</span><span class=\"nb\">max</span><span class=\"p\">())</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">ylim</span><span class=\"p\">(</span><span class=\"n\">xx2</span><span class=\"o\">.</span><span class=\"nb\">min</span><span class=\"p\">(),</span> <span class=\"n\">xx2</span><span class=\"o\">.</span><span class=\"nb\">max</span><span class=\"p\">())</span>\n    <span class=\"c1\"># plot class samples</span>\n    <span class=\"k\">for</span> <span class=\"n\">idx</span><span class=\"p\">,</span> <span class=\"n\">cl</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">unique</span><span class=\"p\">(</span><span class=\"n\">y</span><span class=\"p\">)):</span>\n        <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">scatter</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"o\">=</span><span class=\"n\">X</span><span class=\"p\">[</span><span class=\"n\">y</span> <span class=\"o\">==</span> <span class=\"n\">cl</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">y</span><span class=\"o\">=</span><span class=\"n\">X</span><span class=\"p\">[</span><span class=\"n\">y</span> <span class=\"o\">==</span> <span class=\"n\">cl</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span><span class=\"n\">alpha</span><span class=\"o\">=</span><span class=\"mf\">0.8</span><span class=\"p\">,</span> <span class=\"n\">c</span><span class=\"o\">=</span><span class=\"n\">cmap</span><span class=\"p\">(</span><span class=\"n\">idx</span><span class=\"p\">),</span><span class=\"n\">marker</span><span class=\"o\">=</span><span class=\"n\">markers</span><span class=\"p\">[</span><span class=\"n\">idx</span><span class=\"p\">],</span> <span class=\"n\">label</span><span class=\"o\">=</span><span class=\"n\">cl</span><span class=\"p\">)</span>\n    <span class=\"c1\"># highlight test samples</span>\n    <span class=\"k\">if</span> <span class=\"n\">test_idx</span><span class=\"p\">:</span>\n        <span class=\"n\">X_test</span><span class=\"p\">,</span> <span class=\"n\">y_test</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"p\">[</span><span class=\"n\">test_idx</span><span class=\"p\">,</span> <span class=\"p\">:],</span> <span class=\"n\">y</span><span class=\"p\">[</span><span class=\"n\">test_idx</span><span class=\"p\">]</span>\n        <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">scatter</span><span class=\"p\">(</span><span class=\"n\">X_test</span><span class=\"p\">[:,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">X_test</span><span class=\"p\">[:,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">c</span><span class=\"o\">=</span><span class=\"s1\">&#39;blue&#39;</span><span class=\"p\">,</span> <span class=\"n\">alpha</span><span class=\"o\">=</span><span class=\"mf\">1.0</span><span class=\"p\">,</span> <span class=\"n\">linewidth</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">marker</span><span class=\"o\">=</span><span class=\"s1\">&#39;o&#39;</span><span class=\"p\">,</span> <span class=\"n\">s</span><span class=\"o\">=</span><span class=\"mi\">55</span><span class=\"p\">,</span> <span class=\"n\">label</span><span class=\"o\">=</span><span class=\"s1\">&#39;test set&#39;</span><span class=\"p\">)</span>\n\n<span class=\"n\">iris</span> <span class=\"o\">=</span> <span class=\"n\">datasets</span><span class=\"o\">.</span><span class=\"n\">load_iris</span><span class=\"p\">()</span>\n<span class=\"n\">X</span> <span class=\"o\">=</span> <span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">data</span><span class=\"p\">[:,</span> <span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">]]</span>\n<span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">iris</span><span class=\"o\">.</span><span class=\"n\">target</span>\n<span class=\"n\">X_train</span><span class=\"p\">,</span> <span class=\"n\">X_test</span><span class=\"p\">,</span> <span class=\"n\">y_train</span><span class=\"p\">,</span> <span class=\"n\">y_test</span> <span class=\"o\">=</span> <span class=\"n\">train_test_split</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">test_size</span><span class=\"o\">=</span><span class=\"mf\">0.3</span><span class=\"p\">,</span> <span class=\"n\">random_state</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n<span class=\"c1\">#为了追求机器学习的最佳性能，我们将特征缩放</span>\n<span class=\"n\">sc</span> <span class=\"o\">=</span> <span class=\"n\">StandardScaler</span><span class=\"p\">()</span>\n<span class=\"n\">sc</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">)</span><span class=\"c1\">#估算每个特征的平均值和标准差</span>\n<span class=\"n\">X_train_std</span><span class=\"o\">=</span><span class=\"n\">sc</span><span class=\"o\">.</span><span class=\"n\">transform</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">)</span><span class=\"c1\">#用同样的参数来标准化测试集，使得测试集和训练集之间有可比性</span>\n<span class=\"n\">X_test_std</span><span class=\"o\">=</span><span class=\"n\">sc</span><span class=\"o\">.</span><span class=\"n\">transform</span><span class=\"p\">(</span><span class=\"n\">X_test</span><span class=\"p\">)</span>\n<span class=\"n\">X_combined_std</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">vstack</span><span class=\"p\">((</span><span class=\"n\">X_train_std</span><span class=\"p\">,</span> <span class=\"n\">X_test_std</span><span class=\"p\">))</span>\n<span class=\"n\">y_combined</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">hstack</span><span class=\"p\">((</span><span class=\"n\">y_train</span><span class=\"p\">,</span> <span class=\"n\">y_test</span><span class=\"p\">))</span>\n\n<span class=\"c1\">#训练感知机模型</span>\n<span class=\"n\">lr</span> <span class=\"o\">=</span> <span class=\"n\">LogisticRegression</span><span class=\"p\">(</span><span class=\"n\">C</span><span class=\"o\">=</span><span class=\"mf\">1000.0</span><span class=\"p\">,</span><span class=\"n\">random_state</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span><span class=\"c1\">#迭代次数为1000次,random_state设置随机种子，每次迭代都有相同的训练集顺序</span>\n<span class=\"n\">lr</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">X_train_std</span><span class=\"p\">,</span> <span class=\"n\">y_train</span><span class=\"p\">)</span>\n<span class=\"n\">lr</span><span class=\"o\">.</span><span class=\"n\">predict_proba</span><span class=\"p\">(</span><span class=\"n\">X_test_std</span><span class=\"p\">)</span>\n\n<span class=\"c1\">#绘图</span>\n<span class=\"n\">plot_decision_regions</span><span class=\"p\">(</span><span class=\"n\">X_combined_std</span><span class=\"p\">,</span> <span class=\"n\">y_combined</span><span class=\"p\">,</span> <span class=\"n\">classifier</span><span class=\"o\">=</span><span class=\"n\">lr</span><span class=\"p\">,</span> <span class=\"n\">test_idx</span><span class=\"o\">=</span><span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">105</span><span class=\"p\">,</span><span class=\"mi\">150</span><span class=\"p\">))</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">xlabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;petal length [standardized]&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">ylabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;petal width [standardized]&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">legend</span><span class=\"p\">(</span><span class=\"n\">loc</span><span class=\"o\">=</span><span class=\"s1\">&#39;upper left&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span></code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-f87fdf89f7963637e8df9af5f3f31832_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"914\" data-rawheight=\"668\" class=\"origin_image zh-lightbox-thumb\" width=\"914\" data-original=\"https://pic3.zhimg.com/v2-f87fdf89f7963637e8df9af5f3f31832_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;914&#39; height=&#39;668&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"914\" data-rawheight=\"668\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"914\" data-original=\"https://pic3.zhimg.com/v2-f87fdf89f7963637e8df9af5f3f31832_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-f87fdf89f7963637e8df9af5f3f31832_b.jpg\"/></figure><h2>6.推广</h2><p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p><p><a href=\"https://link.zhihu.com/?target=http%3A//weixin.qq.com/r/Ty_4oDPE2W6mrXfD93pd\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">weixin.qq.com/r/Ty_4oDP</span><span class=\"invisible\">E2W6mrXfD93pd</span><span class=\"ellipsis\"></span></a> (二维码自动识别)</p><p>参考</p><p>[^1]: <a href=\"https://www.zhihu.com/people/maigo/activities\" class=\"internal\">知乎用户</a></p><p>[^2]: <a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/programmer_wei/article/details/52072939\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Logistic Regression（逻辑回归）原理及公式推导</a></p><p>[^3]: <a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/javaisnotgood/article/details/78873819\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">logstic回归损失函数及梯度下降公式推导 - CSDN博客</a></p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "Logistic回归", 
                    "tagLink": "https://api.zhihu.com/topics/19842407"
                }, 
                {
                    "tag": "梯度下降", 
                    "tagLink": "https://api.zhihu.com/topics/19650497"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/34970466", 
            "userName": "谓之小一", 
            "userLink": "https://www.zhihu.com/people/761dda159c5cec5fd926f761ab5f4ccf", 
            "upvote": 1, 
            "title": "机器学习之线性回归", 
            "content": "<h2>1.线性回归分析（ Linear Regression Analysis）</h2><p><b>线性回归分析（Regression Analysis）</b>：其数据集是给定一个函数和他的一些坐标点，然后通过回归分析的算法，来估计原函数的模型，求得最符合这些数据集的函数解析式。然后我们就可以用来预估未知数据，输入一个自变量便会根据这个模型解析式输出因变量，这些自变量就是特征向量，因变量即为标签，而且标签的值是建立在连续范围内的。<br/>通俗来讲就是我们在做数学题的时候，解未知数的方法。假如给定自变量和函数，通过函数处理自变量，然后获得函数的解。而回归分析便是相当于给定自变量和函数的解，然后去求函数。如下图所示，我们已经知道红色点坐标，然后回归得到直线，回归分析属于<b>监督学习</b>。<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-6d49864514ba5beca5f47fd2f3cb8e9f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"686\" data-rawheight=\"514\" class=\"origin_image zh-lightbox-thumb\" width=\"686\" data-original=\"https://pic4.zhimg.com/v2-6d49864514ba5beca5f47fd2f3cb8e9f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;686&#39; height=&#39;514&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"686\" data-rawheight=\"514\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"686\" data-original=\"https://pic4.zhimg.com/v2-6d49864514ba5beca5f47fd2f3cb8e9f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-6d49864514ba5beca5f47fd2f3cb8e9f_b.jpg\"/></figure><p>上图只是简单的一元线性分析，回归后我们可以得到如$f(x)=a*x+b$的函数表达式，但更多情况下我们是求解多元线性回归问题，那应该如何解决呢。</p><h2>2.模型表达</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-8156e380ba77f21614290fc4735d8f10_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1632\" data-rawheight=\"740\" class=\"origin_image zh-lightbox-thumb\" width=\"1632\" data-original=\"https://pic1.zhimg.com/v2-8156e380ba77f21614290fc4735d8f10_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1632&#39; height=&#39;740&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1632\" data-rawheight=\"740\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1632\" data-original=\"https://pic1.zhimg.com/v2-8156e380ba77f21614290fc4735d8f10_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-8156e380ba77f21614290fc4735d8f10_b.jpg\"/></figure><h2>3.梯度下降算法</h2><h2>3.1梯度下降算法简述</h2><p>实际生活中我们有时也利用梯度下降算法，比如我们处在一座山的某处位置，但我们并不知道如何下山，于是决定走一步算一步，但每次都沿着最陡峭的地点下山，也就是沿着梯度的负方向前进。但有事也会遇见问题，不能每次都能到达山脚，可能到达山峰的某个局部最低点。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-63b735bc008835e50e5bed5b1af5b3da_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1680\" data-rawheight=\"866\" class=\"origin_image zh-lightbox-thumb\" width=\"1680\" data-original=\"https://pic3.zhimg.com/v2-63b735bc008835e50e5bed5b1af5b3da_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1680&#39; height=&#39;866&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1680\" data-rawheight=\"866\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1680\" data-original=\"https://pic3.zhimg.com/v2-63b735bc008835e50e5bed5b1af5b3da_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-63b735bc008835e50e5bed5b1af5b3da_b.jpg\"/></figure><p>从上面解释可以看出，梯度下降不一定能够找到全局最优解，有可能是局部最优解，但此种方法已能帮助我们求解线性回归问题。另外如果求解的函数是凸函数，梯度下降法得到得解一定是全局最优解。</p><h2>3.2 梯度下降算法相关概念</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-575cdc6b7a5fe014e514fc785f514dd5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1596\" data-rawheight=\"508\" class=\"origin_image zh-lightbox-thumb\" width=\"1596\" data-original=\"https://pic2.zhimg.com/v2-575cdc6b7a5fe014e514fc785f514dd5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1596&#39; height=&#39;508&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1596\" data-rawheight=\"508\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1596\" data-original=\"https://pic2.zhimg.com/v2-575cdc6b7a5fe014e514fc785f514dd5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-575cdc6b7a5fe014e514fc785f514dd5_b.jpg\"/></figure><h2>3.3梯度下降算法过程</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-80265d7cea72d136af2c911131369750_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1660\" data-rawheight=\"942\" class=\"origin_image zh-lightbox-thumb\" width=\"1660\" data-original=\"https://pic1.zhimg.com/v2-80265d7cea72d136af2c911131369750_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1660&#39; height=&#39;942&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1660\" data-rawheight=\"942\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1660\" data-original=\"https://pic1.zhimg.com/v2-80265d7cea72d136af2c911131369750_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-80265d7cea72d136af2c911131369750_b.jpg\"/></figure><h2>4.线性回归算法实现</h2><p>为研究公司盈利提升幅度受电视、广播、报纸的投入的影响程度，利用多元线性回归来分析数据。其中数据下载地址为<a href=\"https://link.zhihu.com/?target=http%3A//www-bcf.usc.edu/~gareth/ISL/Advertising.csv\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">www-bcf.usc.edu/~gareth</span><span class=\"invisible\">/ISL/Advertising.csv</span><span class=\"ellipsis\"></span></a> ，为能够绘制出三维图片，此处只选择电视、广播的广告投入对公司盈利提升幅度的影响。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"kn\">as</span> <span class=\"nn\">np</span>\n<span class=\"kn\">import</span> <span class=\"nn\">pandas</span> <span class=\"kn\">as</span> <span class=\"nn\">pd</span>\n<span class=\"kn\">import</span> <span class=\"nn\">matplotlib.pyplot</span> <span class=\"kn\">as</span> <span class=\"nn\">plt</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn</span> <span class=\"kn\">import</span> <span class=\"n\">linear_model</span>\n<span class=\"kn\">from</span> <span class=\"nn\">mpl_toolkits.mplot3d</span> <span class=\"kn\">import</span> <span class=\"n\">axes3d</span>\n<span class=\"kn\">import</span> <span class=\"nn\">seaborn</span> <span class=\"kn\">as</span> <span class=\"nn\">sns</span>\n\n<span class=\"c1\">#read_csv</span>\n<span class=\"n\">readdata</span><span class=\"o\">=</span><span class=\"n\">pd</span><span class=\"o\">.</span><span class=\"n\">read_csv</span><span class=\"p\">(</span><span class=\"s1\">&#39;data/Advertising.csv&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">data</span><span class=\"o\">=</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">(</span><span class=\"n\">readdata</span><span class=\"o\">.</span><span class=\"n\">values</span><span class=\"p\">)</span>\n\n<span class=\"c1\">#训练数据</span>\n<span class=\"n\">X_train</span><span class=\"o\">=</span><span class=\"n\">data</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">:</span><span class=\"mi\">150</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">:</span><span class=\"mi\">3</span><span class=\"p\">]</span>\n<span class=\"n\">Y_train</span><span class=\"o\">=</span><span class=\"n\">data</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">:</span><span class=\"mi\">150</span><span class=\"p\">,</span><span class=\"mi\">3</span><span class=\"p\">]</span>\n\n<span class=\"c1\">#测试数据</span>\n<span class=\"n\">X_test</span><span class=\"o\">=</span><span class=\"n\">data</span><span class=\"p\">[</span><span class=\"mi\">150</span><span class=\"p\">:</span><span class=\"mi\">200</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">:</span><span class=\"mi\">3</span><span class=\"p\">]</span>\n<span class=\"n\">Y_test</span><span class=\"o\">=</span><span class=\"n\">data</span><span class=\"p\">[</span><span class=\"mi\">150</span><span class=\"p\">:</span><span class=\"mi\">200</span><span class=\"p\">,</span><span class=\"mi\">3</span><span class=\"p\">]</span>\n\n<span class=\"c1\">#回归分析</span>\n<span class=\"n\">regr</span> <span class=\"o\">=</span> <span class=\"n\">linear_model</span><span class=\"o\">.</span><span class=\"n\">LinearRegression</span><span class=\"p\">()</span>\n<span class=\"c1\">#进行training set和test set的fit，即是训练的过程</span>\n<span class=\"n\">regr</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">,</span> <span class=\"n\">Y_train</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># 打印出相关系数和截距等信息</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;Coefficients: </span><span class=\"se\">\\n</span><span class=\"s1\">&#39;</span><span class=\"p\">,</span> <span class=\"n\">regr</span><span class=\"o\">.</span><span class=\"n\">coef_</span><span class=\"p\">)</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;Intercept: &#39;</span><span class=\"p\">,</span> <span class=\"n\">regr</span><span class=\"o\">.</span><span class=\"n\">intercept_</span><span class=\"p\">)</span>\n<span class=\"c1\"># The mean square error</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s2\">&#34;Residual sum of squares: </span><span class=\"si\">%.2f</span><span class=\"s2\">&#34;</span>\n      <span class=\"o\">%</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">((</span><span class=\"n\">regr</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">X_test</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"n\">Y_test</span><span class=\"p\">)</span> <span class=\"o\">**</span> <span class=\"mi\">2</span><span class=\"p\">))</span>\n<span class=\"c1\"># Explained variance score: 1 is perfect prediction</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;Variance score: </span><span class=\"si\">%.2f</span><span class=\"s1\">&#39;</span> <span class=\"o\">%</span> <span class=\"n\">regr</span><span class=\"o\">.</span><span class=\"n\">score</span><span class=\"p\">(</span><span class=\"n\">X_test</span><span class=\"p\">,</span> <span class=\"n\">Y_test</span><span class=\"p\">))</span>\n\n<span class=\"c1\">#得出回归函数 并自定义数据</span>\n<span class=\"n\">X_line</span><span class=\"o\">=</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linspace</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">300</span><span class=\"p\">)</span>\n<span class=\"n\">Y_line</span><span class=\"o\">=</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linspace</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">50</span><span class=\"p\">)</span>\n<span class=\"n\">Z_line</span><span class=\"o\">=</span><span class=\"mf\">0.04699836</span><span class=\"o\">*</span><span class=\"n\">X_line</span><span class=\"o\">+</span><span class=\"mf\">0.17913965</span><span class=\"o\">*</span><span class=\"n\">Y_line</span><span class=\"o\">+</span><span class=\"mf\">3.00431061176</span>\n\n<span class=\"c1\">#画图</span>\n<span class=\"n\">fig</span><span class=\"o\">=</span><span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">figure</span><span class=\"p\">()</span>\n<span class=\"n\">ax</span> <span class=\"o\">=</span> <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplot</span><span class=\"p\">(</span><span class=\"mi\">111</span><span class=\"p\">,</span> <span class=\"n\">projection</span><span class=\"o\">=</span><span class=\"s1\">&#39;3d&#39;</span><span class=\"p\">)</span>  <span class=\"c1\"># 创建一个三维的绘图工程</span>\n<span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">scatter</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"p\">[:,</span><span class=\"mi\">1</span><span class=\"p\">],</span><span class=\"n\">data</span><span class=\"p\">[:,</span><span class=\"mi\">2</span><span class=\"p\">],</span><span class=\"n\">data</span><span class=\"p\">[:,</span><span class=\"mi\">3</span><span class=\"p\">],</span><span class=\"n\">c</span><span class=\"o\">=</span><span class=\"s1\">&#39;red&#39;</span><span class=\"p\">,)</span>  <span class=\"c1\"># 绘制数据点</span>\n<span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">X_line</span><span class=\"p\">,</span><span class=\"n\">Y_line</span><span class=\"p\">,</span><span class=\"n\">Z_line</span><span class=\"p\">,</span><span class=\"n\">c</span><span class=\"o\">=</span><span class=\"s1\">&#39;blue&#39;</span><span class=\"p\">)</span><span class=\"c1\">#绘制回归曲线</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-2c594fed1ea46ee2e4833830d9b0db42_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"556\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic3.zhimg.com/v2-2c594fed1ea46ee2e4833830d9b0db42_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;640&#39; height=&#39;556&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"556\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https://pic3.zhimg.com/v2-2c594fed1ea46ee2e4833830d9b0db42_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-2c594fed1ea46ee2e4833830d9b0db42_b.jpg\"/></figure><p>其中红色为数据点，蓝色线便为我们回归之后的曲线，这里我们是利用sklearn进行线性回归分析，后续会写出sklearn教程。如有错误之处还请指正，谢谢。</p><hr/><h2>5.推广</h2><p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。<br/></p><p><a href=\"https://link.zhihu.com/?target=http%3A//weixin.qq.com/r/Ty_4oDPE2W6mrXfD93pd\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">weixin.qq.com/r/Ty_4oDP</span><span class=\"invisible\">E2W6mrXfD93pd</span><span class=\"ellipsis\"></span></a> (二维码自动识别)</p><p></p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "线性规划", 
                    "tagLink": "https://api.zhihu.com/topics/19760799"
                }, 
                {
                    "tag": "多元线性回归", 
                    "tagLink": "https://api.zhihu.com/topics/19665399"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/34901641", 
            "userName": "谓之小一", 
            "userLink": "https://www.zhihu.com/people/761dda159c5cec5fd926f761ab5f4ccf", 
            "upvote": 3, 
            "title": "机器学习知识体系", 
            "content": "<h2> 1.什么是机器学习</h2><blockquote>机器学习（Machine Learning, ML）是一门多领域交叉学科，涉及概率论、统计学、逼近论、凸分析、算法复杂度理论等多门学科。专门研究计算机怎样模拟或实现人类的学习行为，以获取新的知识或技能，重新组织已有的知识结构使之不断改善自身的性能。<br/></blockquote><p>上述为<b>百度百科</b>定义，而在现实生活中，我们主要会碰到两类问题。一类是我们知道怎么去通过算法将输入转化为输出，通过学习此类模式得到相应输出结果。另一类是寻找不到此类模式，通过深度学习去做。</p><ul><li>给定一定的输入，通过施加一定条件或算法，得到最终的输出，类似于下图模式。</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b00a8d7dd8e4788745a8eb1971289e11_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"547\" data-rawheight=\"217\" class=\"origin_image zh-lightbox-thumb\" width=\"547\" data-original=\"https://pic2.zhimg.com/v2-b00a8d7dd8e4788745a8eb1971289e11_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;547&#39; height=&#39;217&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"547\" data-rawheight=\"217\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"547\" data-original=\"https://pic2.zhimg.com/v2-b00a8d7dd8e4788745a8eb1971289e11_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-b00a8d7dd8e4788745a8eb1971289e11_b.jpg\"/></figure><ul><li>以字符识别为例，输入的是手写数字图片，输出0-9字符串，我们并不知道怎么把输入转换成输出，因为手写体因人而异，随机性很大。换句话说就是我们缺的是知识如何映射，不过幸运的是我们有实例数据，而把这个知识通过机器学出来的过程叫做机器学习。</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-b43e64e9c0b2df63158cc74391fc8eee_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"509\" data-rawheight=\"298\" class=\"origin_image zh-lightbox-thumb\" width=\"509\" data-original=\"https://pic3.zhimg.com/v2-b43e64e9c0b2df63158cc74391fc8eee_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;509&#39; height=&#39;298&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"509\" data-rawheight=\"298\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"509\" data-original=\"https://pic3.zhimg.com/v2-b43e64e9c0b2df63158cc74391fc8eee_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-b43e64e9c0b2df63158cc74391fc8eee_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>2.机器学习体系概括</h2><p>机器学习包含多交叉学科，同时也在很多方面得到应用，如自然语言处理、图像处理、数据挖掘、推荐系统领域等。机器学习包含监督学习、无监督学习、半监督学习、强化学习、深度学习、迁移学习等，还有各种工具和框架的应用，因此Machine Learning的过程也是漫长而有趣的。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-43ec8f094fb862125246e15b4a4d269c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1011\" data-rawheight=\"1502\" class=\"origin_image zh-lightbox-thumb\" width=\"1011\" data-original=\"https://pic1.zhimg.com/v2-43ec8f094fb862125246e15b4a4d269c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1011&#39; height=&#39;1502&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1011\" data-rawheight=\"1502\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1011\" data-original=\"https://pic1.zhimg.com/v2-43ec8f094fb862125246e15b4a4d269c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-43ec8f094fb862125246e15b4a4d269c_b.jpg\"/></figure><p>下图为开发者平台CSDN上王小雷整理的机器学习算法汇总，其中包含很多机器学习算法，知识体系较为庞大。目前个人已掌握知识点主要在监督学习、无监督学习、集成学习算法、降维方面，所以先给大家介绍这几类机器学习算法，半监督学习、强化学习、深度学习和迁移学习个人会继续学习。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-fbaf7be6c987d121400fff729419f9ab_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1240\" data-rawheight=\"2219\" class=\"origin_image zh-lightbox-thumb\" width=\"1240\" data-original=\"https://pic4.zhimg.com/v2-fbaf7be6c987d121400fff729419f9ab_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1240&#39; height=&#39;2219&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1240\" data-rawheight=\"2219\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1240\" data-original=\"https://pic4.zhimg.com/v2-fbaf7be6c987d121400fff729419f9ab_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-fbaf7be6c987d121400fff729419f9ab_b.jpg\"/></figure><p>机器学习算法中常用到的便是<b>监督学习</b>和<b>无监督学习</b>，监督学习包含<b>回归</b>和<b>分类</b>两方面，无监督学习为<b>聚类</b>。</p><p><b>监督学习（Supervised Learning）</b></p><p>当你有一些问题和他们的答案时，你要做的有监督学习就是学习这些已经知道答案的问题，当你具备此类学习的经验时，便是学习的成果。然后当你接受到一个新的此类问题时，便可通过学习得到的经验，得出新问题的答案。当我们有一些样本数据集时，对于每个单一的数据根据他的特征向量我们要去判断他的标签，那么就是监督学习。监督学习分为<b>回归分析（Regression Analysis）</b>和<b>分类（Classification）</b>两类。</p><ul><li><b>回归分析（Regression Analysis）</b>：其数据集是给定一个函数和他的一些坐标点，然后通过回归分析的算法，来估计原函数的模型，求得最符合这些数据集的函数解析式。然后我们就可以用来预估未知数据，输入一个自变量便会根据这个模型解析式输出因变量，这些自变量就是特征向量，因变量即为标签，而且标签的值是建立在连续范围的。</li><li>分类（Classfication）：其数据集由特征变量和标签组成，当你学习这些数据之后，给你一个只知道特征向量不知道标签的数据，让你求他的标签是哪一个？分类和回归的主要区别就是输出结果是连续还是离散。</li></ul><p><b>无监督学习（Unsupervised Learning）</b></p><p>我们有一些问题，但是不知道答案，我们要做的无监督学习就是按照他们的性质把他们自动地分成很多组，每组的问题是具有类似性质的（比如数学问题会聚集在一组，英语问题聚集在一组……）。</p><p>所有的数据只有特征向量没有标签，但是可以发现这些数据呈现出聚群的结构，本质是相似的类型会聚集在一起。把这些没有标签的数据分成各个组合便是聚类。比如每天都会搜到大量新闻，然后把它们全部聚类，就会自动分成几十个不同的组（比如娱乐、科技、政治…），每个组内新闻都具有相似的内容结构。</p><h2>3.如何开始学习</h2><p>开始机器学习之前必须要有一定的数学知识，因为各算法之中涉及很多公式推导，用到的主要数学知识点为微积分、概率论、大学中高等数学知识点，忘记的同学可以在学习算法的过程中复习下。另外我们还需要掌握一门编程语言，这里推荐大家学习Python，为什么选择Python在这儿也就不讨论了，知乎平台上有很多介绍。</p><p>很好，我们掌握一门编程语言和数学知识之后便可开始Machine Learning，此过程中将使用相应Python标准库和第三方库，大家可以参考我以前写的文章，Python之NumPy使用教程、Python之Pandas使用教程、Python之MatPlotLib使用教程。中间过程中如涉及到其他Python库的使用，会及时写出相应教程。接下来一段时间将持续更新各种机器学习算法，包括线性回归、Logistic回归、支持向量机SVM、决策树、EM等算法。</p><hr/><h2>4.推广</h2><p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p><p><a href=\"https://link.zhihu.com/?target=http%3A//weixin.qq.com/r/Ty_4oDPE2W6mrXfD93pd\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">weixin.qq.com/r/Ty_4oDP</span><span class=\"invisible\">E2W6mrXfD93pd</span><span class=\"ellipsis\"></span></a> (二维码自动识别)</p><p></p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/34710330", 
            "userName": "谓之小一", 
            "userLink": "https://www.zhihu.com/people/761dda159c5cec5fd926f761ab5f4ccf", 
            "upvote": 0, 
            "title": "Markdown写作教程", 
            "content": "<p>文档发到知乎上格式有点变化，详细访问此<a href=\"https://link.zhihu.com/?target=https%3A//weizhixiaoyi.com/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">链接</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//weizhixiaoyi.com/2018/03/16/Mac%2BHexo%2BGitHub%25E5%258D%259A%25E5%25AE%25A2%25E6%2590%25AD%25E5%25BB%25BA%25E6%2595%2599%25E7%25A8%258B/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">博客搭建教程</a>写完之后，很多同学都比较感兴趣，那么在此也写下Markdown写作教程，更快捷、方便的写作博文。当然网上已经有太多人写Markdown教程，那么很多人难免问这个问题，为什么你还要花时间去写如此一篇教程呢？是因为哪怕是同样的内容，写出来之后便有我自己的风格，能让更多人浅显易懂的了解这些知识，同时也能增加我的个人理解，共同学习。</p><h2>1.为什么选择Markdown</h2><p>首先通过Github搭建博客，我们只能用Markdown文档，然后直接转换成Html网页展现出来。但除了这个原因之外，更吸引我的地方便是其简单、快捷、高效的写作方式，通过轻文本标记语言，利用简洁的语法进行排版，达到所见及所得的效果，书写的过程只考虑内容和文字本身，写作的过程便是享受。目前CSDN、简书、博客园、知乎等平台均支持Markdown写作。当然富文本写作方式也有其迷人之处，比如我用<b>印象笔记</b>至此已经写了200+的笔记，有时间给大家写个印象笔记教程，如何高效收集、管理生活中的知识点与信息流。</p><h2>2.标题</h2><p>标题通过<code>#</code>的个数进行区分，Markdown共支持6级标题。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-455d7989a8f92f2cedd4a9acd0245627_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1624\" data-rawheight=\"1242\" class=\"origin_image zh-lightbox-thumb\" width=\"1624\" data-original=\"https://pic4.zhimg.com/v2-455d7989a8f92f2cedd4a9acd0245627_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1624&#39; height=&#39;1242&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1624\" data-rawheight=\"1242\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1624\" data-original=\"https://pic4.zhimg.com/v2-455d7989a8f92f2cedd4a9acd0245627_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-455d7989a8f92f2cedd4a9acd0245627_b.jpg\"/></figure><h2>3.字体设置</h2><h2>3.1粗体</h2><p>文字前后加<code>**</code>来表示粗体。</p><div class=\"highlight\"><pre><code class=\"language-text\">**粗体**</code></pre></div><p><b>粗体</b></p><h2>3.2斜体</h2><p>文字前后加<code>*</code>来表示斜体。</p><div class=\"highlight\"><pre><code class=\"language-text\">*斜体*</code></pre></div><p><i>斜体</i></p><h2>3.3粗斜体</h2><p>文字前后加<code>***</code>来表示粗斜体。</p><div class=\"highlight\"><pre><code class=\"language-text\">***粗斜体***</code></pre></div><p><b><i>粗斜体</i></b></p><h2>3.4下划线</h2><p>文字前后加<code>&lt;u&gt;</code> <code>&lt;/u&gt;</code>来表示下划线。</p><div class=\"highlight\"><pre><code class=\"language-text\">&lt;u&gt;下滑线&lt;/u&gt;</code></pre></div><p><u>下划线</u></p><h2>3.5删除线</h2><p>文字前后加<code>~~</code>来表示删除线。</p><div class=\"highlight\"><pre><code class=\"language-text\">~~删除线~~</code></pre></div><p>~~删除线~~</p><h2>3.6标记</h2><p>文字前后加`来表示标记，该符号位于Esc键下面。</p><div class=\"highlight\"><pre><code class=\"language-text\">`标记`</code></pre></div><p><code>标记</code></p><h2>3.7Html标签</h2><div class=\"highlight\"><pre><code class=\"language-text\">&lt;font face=&#34;微软雅黑&#34; color=&#34;red&#34; size=&#34;6&#34;&gt;字体及字体颜色和大小&lt;/font&gt;</code></pre></div><p>字体及字体颜色和大小</p><h2>4.列表</h2><h2>4.1有序列表</h2><p>采用<code>1.</code> 后加空格形式表示有序列表。</p><div class=\"highlight\"><pre><code class=\"language-text\">1. 有序列表1\n2. 有序列表2\n3. 有序列表3</code></pre></div><ol><li>有序列表1</li><li>有序列表2</li><li>有序列表3</li></ol><h2>4.2无序列表</h2><p>采用<code>+</code> <code>-</code> <code>*</code> <code>=</code>符号表示无序列表，支持多级嵌套。</p><div class=\"highlight\"><pre><code class=\"language-text\">+ 有序列表1\n+ + 有序列表1.1\n+ + 有序列表1.2\n+ 有序列表2\n+ 有序列表3</code></pre></div><ul><li>无序列表1</li><ul><li>无序列表1.1</li><li>无序列表1.2</li></ul><li>无序列表2</li><li>无序列表3</li></ul><h2>4.3未完成列表</h2><p>采用<code>- []</code>表示未完成任务，各符号间均有空格。</p><div class=\"highlight\"><pre><code class=\"language-text\">- [ ] 未完成任务1\n- [ ] 未完成任务2\n- [ ] 未完成任务3</code></pre></div><ul><li>[ ] 未完成任务1</li><li>[ ] 未完成任务2</li><li>[ ] 未完成任务3</li></ul><h2>4.4已完成任务</h2><p>采用<code>- [x]</code>表示已完成任务，各符号间均有空格。同时可直接在未完成任务间<code>打勾</code>来转换成已完成任务。</p><div class=\"highlight\"><pre><code class=\"language-text\">- [x] 已完成任务1\n- [x] 已完成任务2\n- [x] 已完成任务3</code></pre></div><ul><li>[x] 已完成任务1</li><li>[x] 已完成任务2</li><li>[x] 已完成任务3</li></ul><h2>5.表格</h2><p>表格对齐方式</p><ul><li>居左：:----</li><li>居中：:----:或-----</li><li>居由：----:</li></ul><div class=\"highlight\"><pre><code class=\"language-text\">| 标题1           |      标题2      |           标题3 |\n| :-------------- | :-------------: | --------------: |\n| 居左测试文本1.1 | 居中测试文本2.1 | 居右测试文本3.1 |\n| 居左测试文本1.2 | 居中测试文本2.2 | 居右测试文本3.2 |</code></pre></div><p>| 标题1           |      标题2      |           标题3 |<br/>| :-------------- | :-------------: | --------------: |<br/>| 居左测试文本1.1 | 居中测试文本2.1 | 居右测试文本3.1 |<br/>| 居左测试文本1.2 | 居中测试文本2.2 | 居右测试文本3.2 |</p><h2>6.段落和换行</h2><h2>6.1首行缩进方式</h2><ul><li><code>&amp;emsp;</code>中文空格</li><li><code>&amp;ensp;</code>半中文空格</li><li><code>&amp;nbsp;</code>英文空格</li><li>输入法切换到全角双击空格</li></ul><h2>6.2换行</h2><ul><li>换行处连续打两个空格</li><li>换行处使用<code>&lt;br&gt;</code>进行换行</li></ul><h2>6.3空行</h2><ul><li>空行处连续打两个空格</li><li>换行处使用<code>&lt;br&gt;</code>进行空行</li></ul><h2>6.引用和代码块</h2><h2>6.1引用</h2><p>若在文章中需要引入一段话等，可以采用引用的方式呈现，支持多级引用。</p><div class=\"highlight\"><pre><code class=\"language-text\">&gt; 引用1\n&gt; &gt; 引用1.1\n&gt; &gt; 引用1.2\n&gt; 引用2</code></pre></div><blockquote>引用1<br/>引用1.1<br/>引用1.2<br/><br/>引用2</blockquote><h2>6.2代码块</h2><p>代码前后添加```表示代码块。</p><div class=\"highlight\"><pre><code class=\"language-text\">```Python\nprint(&#39;代码块&#39;)\n```\nprint（&#39;代码块&#39;）</code></pre></div><h2>7.链接</h2><h2>7.1图片链接</h2><p>采用<code>![]()</code>来表示图片链接。</p><div class=\"highlight\"><pre><code class=\"language-text\">![图片名称](链接地址)</code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-237fc3c3b95db80da164669f738c4b8e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1908\" data-rawheight=\"1066\" class=\"origin_image zh-lightbox-thumb\" width=\"1908\" data-original=\"https://pic3.zhimg.com/v2-237fc3c3b95db80da164669f738c4b8e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1908&#39; height=&#39;1066&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1908\" data-rawheight=\"1066\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1908\" data-original=\"https://pic3.zhimg.com/v2-237fc3c3b95db80da164669f738c4b8e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-237fc3c3b95db80da164669f738c4b8e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>7.2文字链接</h2><p>采用<code>[]()</code>表示文字链接。</p><div class=\"highlight\"><pre><code class=\"language-text\">[链接名称](链接地址)</code></pre></div><p><a href=\"https://zhuanlan.zhihu.com/p/34710330/weizhixiaoyi.com\" class=\"internal\">文字链接</a></p><h2>7.3参考链接</h2><p>采用<code>[ ]:</code>表示参考链接，注意符号后有空格。</p><div class=\"highlight\"><pre><code class=\"language-text\">[ ]: url title</code></pre></div><h2>8.分割线</h2><p>上下文无关时可使用分割符进行分开。</p><ul><li>连续多个<code>-</code>  (&gt;=3)</li><li>连续多个<code>*</code> （&gt;=3）</li><li>连续多个下划线<code>_</code> （&gt;=3）</li></ul><div class=\"highlight\"><pre><code class=\"language-text\">---分割线\n***分割线\n___分割线</code></pre></div><hr/><hr/><hr/><h2>9.脚注和注释</h2><h2>9.1脚注</h2><p>采用`[^]:表示脚注，注意空格。</p><div class=\"highlight\"><pre><code class=\"language-text\">[^]: 脚注</code></pre></div><h2>9.2注释</h2><p>采用<code>&lt;!----&gt;</code>表示注释.</p><div class=\"highlight\"><pre><code class=\"language-text\">&lt;!--注释--&gt;</code></pre></div><h2>11.转义</h2><p>Markdown通过反斜杠<code>\\</code>来插入在语法中有其他意义的符号，Markdown支持以下符号来进行转义。</p><div class=\"highlight\"><pre><code class=\"language-text\">\\\\反斜线\n\\`反引号\n\\*星号\n\\_下划线\n\\{}花括号\n\\[]方括号\n\\()括弧\n\\#井字号\n\\+加号\n\\-减号\n\\.英文句点\n\\!感叹号</code></pre></div><p>反斜线<br/>`反引号<br/>*星号<br/>_下划线<br/>{}花括号<br/>[]方括号<br/>()括弧<br/>#井字号<br/>+加号<br/>-减号<br/>.英文句点<br/>!感叹号</p><h2>12.目录</h2><p>采用<code>[TOC]</code>来生成文章目录。</p><div class=\"highlight\"><pre><code class=\"language-text\">[TOC]</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-768913474fa1ff6f10128ef989c67802_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1604\" data-rawheight=\"1478\" class=\"origin_image zh-lightbox-thumb\" width=\"1604\" data-original=\"https://pic3.zhimg.com/v2-768913474fa1ff6f10128ef989c67802_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1604&#39; height=&#39;1478&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1604\" data-rawheight=\"1478\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1604\" data-original=\"https://pic3.zhimg.com/v2-768913474fa1ff6f10128ef989c67802_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-768913474fa1ff6f10128ef989c67802_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><hr/><h2>13.推广</h2><p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p><p><a href=\"https://link.zhihu.com/?target=http%3A//weixin.qq.com/r/Ty_4oDPE2W6mrXfD93pd\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">weixin.qq.com/r/Ty_4oDP</span><span class=\"invisible\">E2W6mrXfD93pd</span><span class=\"ellipsis\"></span></a> (二维码自动识别)</p><p></p>", 
            "topic": [
                {
                    "tag": "Markdown语法", 
                    "tagLink": "https://api.zhihu.com/topics/20053651"
                }, 
                {
                    "tag": "博客", 
                    "tagLink": "https://api.zhihu.com/topics/19550419"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/34654952", 
            "userName": "谓之小一", 
            "userLink": "https://www.zhihu.com/people/761dda159c5cec5fd926f761ab5f4ccf", 
            "upvote": 33, 
            "title": "Mac+Hexo+GitHub搭建博客教程", 
            "content": "<p></p><h2>1.为什么写博客</h2><p>以前利用Jekyll+Github搭建过几次博客，但每次博客搭建完成后都没有继续坚持写博文，直到最近找实习才认识到技术博客的重要性。曾经学习的很多知识点都已经忘记啦，所以下定决心这次认真总结以前学习的知识点，认真写点技术文章。</p><h2>2.Mac+Hexo+GitHub博客</h2><p>现在GitHub博客主流的就是Jekyll和Hexo两种搭建方式，选择Jekyll还是Hexo就根据个人喜好啦，但个人更推荐使用Hexo。</p><ul><li>Jekyll没有本地服务器，无法实现本地文章预览，需要上传到WEB容器中才能预览功能，而Hexo可以通过简单的命令实现本地预览功能，并直接发布到WEB容器中实现同步。</li><li>Jekyll主题和Hexo主题对比而言，Hexo主题更加简洁美观(个人观点)。</li></ul><p>选择GitHub的原因不用多说，程序员的乐园，更是支持pages功能，虽然很多其他社区也支持，比如GitLab、coding、码云等，但GitHub更加活跃，自己的项目就是放在上面，所以更加方便。但GitHub有最大一点不好之处便是<i>百度爬虫无法爬取博客内容</i>，自己也找了好久解决方法，比如利用coding托管(免费版绑定域名有广告)、CDN加速(对于流量太小的网站没什么用)，所以暂时没什么太好的解决方法。</p><h2>3.博客本地环境搭建</h2><h2>3.1安装Node.js和Git</h2><p>Mac上安装可以选择图形化方式和终端安装，此处直接使用客户端方式安装。Node.js官网下载文件，根据提示安装即可，安装成功后在目录<i>/usr/local/bin</i>目录下。测试Node.js和npm，出现下述信息则安装成功。</p><div class=\"highlight\"><pre><code class=\"language-text\">node -v\nv8.10.0\nnpm -v\n5.6.0</code></pre></div><p>Git官网下载相应文件根据提示直接进行安装，检查git是否安装成功，直接查看git版本。</p><div class=\"highlight\"><pre><code class=\"language-text\">Git --version \ngit version 2.15.0</code></pre></div><h2>3.2安装Hexo</h2><p>Node.js和Git都安装成功后开始安装Hexo。安装时注意权限问题，加上sudo，其中-g表示全局安装。</p><div class=\"highlight\"><pre><code class=\"language-text\">sudo npm install -g hexo</code></pre></div><h2>3.3博客初始化</h2><p>创建存储博客的文件，比如命名为myblog，然后进入到myblog之中。</p><div class=\"highlight\"><pre><code class=\"language-text\">cd myblog</code></pre></div><p>执行下述命令初始化本地博客，下载一系列文件。</p><div class=\"highlight\"><pre><code class=\"language-text\">hexo init</code></pre></div><p>执行下述命令安装npm。</p><div class=\"highlight\"><pre><code class=\"language-text\">sudo npm install</code></pre></div><p>执行下述命令生成本地网页文件并开启服务器，然后通过http://localhost:4000查看本地博客。</p><div class=\"highlight\"><pre><code class=\"language-text\">hexo g\nhexo s</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-dcb8d3dcc5d2b9b39c1b8d1149ccfd6e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2556\" data-rawheight=\"1400\" class=\"origin_image zh-lightbox-thumb\" width=\"2556\" data-original=\"https://pic3.zhimg.com/v2-dcb8d3dcc5d2b9b39c1b8d1149ccfd6e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2556&#39; height=&#39;1400&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2556\" data-rawheight=\"1400\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2556\" data-original=\"https://pic3.zhimg.com/v2-dcb8d3dcc5d2b9b39c1b8d1149ccfd6e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-dcb8d3dcc5d2b9b39c1b8d1149ccfd6e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>4.本地博客关联GitHub</h2><h2>4.1本地博客代码上传GitHub</h2><p>注册并登陆GitHub账号后，新建仓库，名称必须为<code>user.github.io</code>，如<code>weizhixiaoyi.github.io</code>。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-6f84f67fac26c334bd5e4369c07ac72c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2048\" data-rawheight=\"1276\" class=\"origin_image zh-lightbox-thumb\" width=\"2048\" data-original=\"https://pic1.zhimg.com/v2-6f84f67fac26c334bd5e4369c07ac72c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2048&#39; height=&#39;1276&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2048\" data-rawheight=\"1276\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2048\" data-original=\"https://pic1.zhimg.com/v2-6f84f67fac26c334bd5e4369c07ac72c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-6f84f67fac26c334bd5e4369c07ac72c_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>终端cd到myblog文件夹下，打开_config.yml文件。或者用其他文本编辑器打开可以，推荐sublime。</p><div class=\"highlight\"><pre><code class=\"language-vim\"><span class=\"nx\">vim</span> <span class=\"nx\">_config</span>.<span class=\"nx\">yml</span></code></pre></div><p>打开后到文档最后部分，将deploy配置如下。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">deploy</span><span class=\"p\">:</span>\n  <span class=\"nb\">type</span><span class=\"p\">:</span> <span class=\"n\">git</span>\n  <span class=\"n\">repository</span><span class=\"p\">:</span> <span class=\"n\">https</span><span class=\"p\">:</span><span class=\"o\">//</span><span class=\"n\">github</span><span class=\"o\">.</span><span class=\"n\">com</span><span class=\"o\">/</span><span class=\"n\">weizhixiaoyi</span><span class=\"o\">/</span><span class=\"n\">weizhixiaoyi</span><span class=\"o\">.</span><span class=\"n\">github</span><span class=\"o\">.</span><span class=\"n\">io</span><span class=\"o\">.</span><span class=\"n\">git</span>\n  <span class=\"n\">branch</span><span class=\"p\">:</span> <span class=\"n\">master</span></code></pre></div><p>其中将repository中<code>weizhixiaoyi</code>改为自己的用户名，注意type、repository、branch后均有空格。通过如下命令在myblog下生成静态文件并上传到服务器。</p><div class=\"highlight\"><pre><code class=\"language-text\">hexo g\nhexo d</code></pre></div><p>若执行<code>hexo g</code>出错则执行<code>npm install hexo --save</code>，若执行<code>hexo d</code>出错则执行<code>npm install hexo-deployer-git --save</code>。错误修正后再次执行<code>hexo g</code>和<code>hexo d</code>上传到服务器。</p><p>若未关联GitHub，执行<code>hexo d</code>时会提示输入GitHub账号用户名和密码，即:</p><div class=\"highlight\"><pre><code class=\"language-text\">username for &#39;https://github.com&#39;:\npassword for &#39;https://github.com&#39;:</code></pre></div><p><code>hexo d</code>执行成功后便可通过<a href=\"https://link.zhihu.com/?target=https%3A//weizhixiaoyi.github.io\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">weizhixiaoyi.github.io</span><span class=\"invisible\"></span></a>访问博客，看到的内容和http://localhost:4000相同。</p><h2>4.2添加ssh keys到GitHub</h2><p>添加ssh key后不需要每次更新博客再输入用户名和密码。首先检查本地是否包含ssh keys。如果存在则直接将ssh key添加到GitHub之中，否则新生成ssh key。</p><p>执行下述命令生成新的ssh key，将<code>your_email@example.com</code>改成自己注册的GitHub邮箱地址。默认会在<code>~/.ssh/id_rsa.pub</code>中生成<code>id_rsa</code>和<code>id_rsa.pub</code>文件。</p><div class=\"highlight\"><pre><code class=\"language-text\">ssh-keygen -t rsa -C &#34;your_email@exampl&#34;</code></pre></div><p>Mac下利用<code>open ~/.ssh</code>打开文件夹，打开id_rsa.pub文件，里面的信息即为ssh key，将此信息复制到GitHub的Add ssh key<code>路径GitHub-&gt;Setting-&gt;SSH keys-&gt;add SSH key</code>中即可。Title里填写任意标题，将复制的内容粘贴到key中，点击Add key完成添加。</p><p>此时本地博客内容便已关联到GitHub之中，本地博客改变之后，通过<code>hexo g</code>和<code>hexo d</code>便可更新到GitHub之中，通过<a href=\"https://link.zhihu.com/?target=https%3A//weizhixiaoyi.github.io\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">weizhixiaoyi.github.io</span><span class=\"invisible\"></span></a>访问便可看到更新内容。</p><h2>5.更换Hexo主题</h2><p>可以选择Hexo主题官网页面搜索喜欢的theme，这里我选择hexo-theme-next当作自己主题，hex-theme-next主题是GitHub中hexo主题star最高的项目，非常推荐使用。</p><p>终端cd到myblog目录下执行如下所示命令。</p><div class=\"highlight\"><pre><code class=\"language-text\">git clone https://github.com/iissnan/hexo-theme-next themes/next</code></pre></div><p>将blog目录下_config.yml里的theme的名称<code>landscape</code>更改为<code>next</code>。</p><p>执行如下命令（每次部署文章的步骤）</p><div class=\"highlight\"><pre><code class=\"language-text\">hexo g  //生成缓存和静态文件\nhexo d  //重新部署到服务器</code></pre></div><p>当本地博客部署到服务器后，网页端无变化时可以采用下述命令。</p><div class=\"highlight\"><pre><code class=\"language-text\">hexo clean  //清楚缓存文件(db.json)和已生成的静态文件(public)</code></pre></div><h2>6.配置Hexo-theme-next主题</h2><p>Hexo-theme-next主题为精于心、简于形，简洁的界面中能够呈现丰富的内容，访问<a href=\"https://link.zhihu.com/?target=http%3A//theme-next.iissnan.com/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">next官网</a>查看配置内容。配置文件主要修改主题next文件夹中_config.yml文件，next有三种主题选择，分别为Muse、Mist、Pisces三种，个人选择的是Pisces主题。主题增加标签、分类、归档、喜欢（书籍和电影信息流）、文章阅读统计、访问人数统计、评论等功能，博客界面如下所示。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-8d6fc2fdae34493520e440be73781541_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2560\" data-rawheight=\"1406\" class=\"origin_image zh-lightbox-thumb\" width=\"2560\" data-original=\"https://pic2.zhimg.com/v2-8d6fc2fdae34493520e440be73781541_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2560&#39; height=&#39;1406&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2560\" data-rawheight=\"1406\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2560\" data-original=\"https://pic2.zhimg.com/v2-8d6fc2fdae34493520e440be73781541_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-8d6fc2fdae34493520e440be73781541_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-51f7cad3eda92fe5c59744a4232ed3a7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2556\" data-rawheight=\"1400\" class=\"origin_image zh-lightbox-thumb\" width=\"2556\" data-original=\"https://pic4.zhimg.com/v2-51f7cad3eda92fe5c59744a4232ed3a7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2556&#39; height=&#39;1400&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2556\" data-rawheight=\"1400\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2556\" data-original=\"https://pic4.zhimg.com/v2-51f7cad3eda92fe5c59744a4232ed3a7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-51f7cad3eda92fe5c59744a4232ed3a7_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-8b7d61a7f028042fb2273dc8b0140e23_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2558\" data-rawheight=\"1404\" class=\"origin_image zh-lightbox-thumb\" width=\"2558\" data-original=\"https://pic4.zhimg.com/v2-8b7d61a7f028042fb2273dc8b0140e23_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2558&#39; height=&#39;1404&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2558\" data-rawheight=\"1404\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2558\" data-original=\"https://pic4.zhimg.com/v2-8b7d61a7f028042fb2273dc8b0140e23_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-8b7d61a7f028042fb2273dc8b0140e23_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>6.1增加标签、分类、归档页</h2><p>首先取消next/config.yml文件中的<code>tags</code> <code>catagories</code> <code>archive</code>前面的<code>#</code>。</p><p>增加标签页，通过<code>hexo new page &#39;tags&#39;</code>增加新界面，在myblog/sources中发现多了tags文件夹，修改index.md中内容，将type更改为<code>tags</code>。利用<code>hexo g</code>和<code>hexo d</code>将界面重新上传到服务器便可看到新增加的标签页，分类和归档页同理。</p><h2>6.2增加‘喜欢’界面图片流</h2><p>‘喜欢’界面用于展现自己看过的书籍和电影，通过图片流的形式进行安装。</p><p>从GitHub上<a href=\"https://link.zhihu.com/?target=https%3A//github.com/weizhixiaoyi\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/weizhixiaoyi</span><span class=\"invisible\"></span></a> 中的themes/next/scripts下载image-stream.js，放到你的主题/scripts目录中。如果博客主题已经默认引入了jQuery，建议在配置中将image_stream.jquery设置为false。</p><div class=\"highlight\"><pre><code class=\"language-text\">image_stream:\n    jquery: false</code></pre></div><p>在Hexo博客的本地目录中创建一个favorite页面目录，同6.1步骤。并在Next主题中配置config.yml，配置如下所示，其中heart表示图标为心形。</p><div class=\"highlight\"><pre><code class=\"language-text\">menu:\n  home: / || home\n  about: /about/ || user\n  favorite: /favorite/ || heart\n  tags: /tags/ || tags\n  categories: /categories/ || th\n  archives: /archives/ || archive</code></pre></div><p>然后在source/favorite/index.md中使用插件自定义的两个模版来生成页面，index.md内容格式如下所示。</p><div class=\"highlight\"><pre><code class=\"language-text\">{% stream %}\n\n{% figure https://img3.doubanio.com/view/photo/raw/public/p2203001610.jpg\n[《万物理论》]（https://movie.douban.com/subject/24815950/）%}\n\n{% endstream %}</code></pre></div><h2>6.3文章阅读统计</h2><p>文章阅读统计采用LeanCloud，能够提供直观的文章被访问次数，方便作者了解文章写作的质量。Next主题支持leancloud统计，但需要提供app<i>id和app</i>key，因此我们需另外注册leancloud账号，注册过程在此便不再赘述。</p><p>注册成功之后进行创建新应用，设置相应用户名便创建成功。进入用户界面创建Class，在此需要注意的是Class名称必须为Counter，之后此表便是文章数量统计表。然后我们进入设置中的应用key模块便可获得app<i>id和app</i>key，进入next主题的config.yml中，找到leancloud位置复制即可，同时将enable设置为true。另外我们也可以在后台人为修改文章访问量，比如将Python之NumPy使用教程访问量增加。</p><div class=\"highlight\"><pre><code class=\"language-text\">leancloud_visitors:\n  enable: true \n  app_id: Sj2lCA09ErubMSsa2v9oFU9Y-gzGzoHsz #&lt;app_id&gt;\n  app_key: qJejurdHKM06N75OQedX4SDK #&lt;app_key&gt;</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-abce24ea9b5a2b1c232d14a1f04ac907_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2560\" data-rawheight=\"1404\" class=\"origin_image zh-lightbox-thumb\" width=\"2560\" data-original=\"https://pic4.zhimg.com/v2-abce24ea9b5a2b1c232d14a1f04ac907_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2560&#39; height=&#39;1404&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2560\" data-rawheight=\"1404\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2560\" data-original=\"https://pic4.zhimg.com/v2-abce24ea9b5a2b1c232d14a1f04ac907_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-abce24ea9b5a2b1c232d14a1f04ac907_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>6.4增加百度统计</h2><p>百度统计能够清晰看出网站访问数据。在百度官网注册账号后，添加绑定个人网站，在管理页面中找到<code>代码获取</code>。</p><div class=\"highlight\"><pre><code class=\"language-js\"><span class=\"o\">&lt;</span><span class=\"nx\">script</span><span class=\"o\">&gt;</span>\n<span class=\"kd\">var</span> <span class=\"nx\">_hmt</span> <span class=\"o\">=</span> <span class=\"nx\">_hmt</span> <span class=\"o\">||</span> <span class=\"p\">[];</span>\n<span class=\"p\">(</span><span class=\"kd\">function</span><span class=\"p\">()</span> <span class=\"p\">{</span>\n  <span class=\"kd\">var</span> <span class=\"nx\">hm</span> <span class=\"o\">=</span> <span class=\"nb\">document</span><span class=\"p\">.</span><span class=\"nx\">createElement</span><span class=\"p\">(</span><span class=\"s2\">&#34;script&#34;</span><span class=\"p\">);</span>\n  <span class=\"nx\">hm</span><span class=\"p\">.</span><span class=\"nx\">src</span> <span class=\"o\">=</span> <span class=\"s2\">&#34;https://hm.baidu.com/hm.js?b54e835b3551fd0696954b3aedf5d645&#34;</span><span class=\"p\">;</span>\n  <span class=\"kd\">var</span> <span class=\"nx\">s</span> <span class=\"o\">=</span> <span class=\"nb\">document</span><span class=\"p\">.</span><span class=\"nx\">getElementsByTagName</span><span class=\"p\">(</span><span class=\"s2\">&#34;script&#34;</span><span class=\"p\">)[</span><span class=\"mi\">0</span><span class=\"p\">];</span> \n  <span class=\"nx\">s</span><span class=\"p\">.</span><span class=\"nx\">parentNode</span><span class=\"p\">.</span><span class=\"nx\">insertBefore</span><span class=\"p\">(</span><span class=\"nx\">hm</span><span class=\"p\">,</span> <span class=\"nx\">s</span><span class=\"p\">);</span>\n<span class=\"p\">})();</span>\n<span class=\"o\">&lt;</span><span class=\"err\">/script&gt;</span>\n</code></pre></div><p>将代码中<code>b54e835b3551fd0696954b3aedf5d645</code>复制到next主题_config.yml的<code>baidu_analytics</code>中。接下来通过<code>代码安装检查</code>来检查代码是否安装成功，安装成功后便可查看网站详细统计信息。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-dfa358af549268166cc4eb50efd4ee61_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2558\" data-rawheight=\"1396\" class=\"origin_image zh-lightbox-thumb\" width=\"2558\" data-original=\"https://pic2.zhimg.com/v2-dfa358af549268166cc4eb50efd4ee61_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2558&#39; height=&#39;1396&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2558\" data-rawheight=\"1396\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2558\" data-original=\"https://pic2.zhimg.com/v2-dfa358af549268166cc4eb50efd4ee61_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-dfa358af549268166cc4eb50efd4ee61_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>6.4增加评论功能</h2><p>多说、网易云跟帖关闭，畅言需要备案，disqus被墙而且界面不是太美观。新出来的来必力倒是挺不错，支持QQ、微信、微博、百度、人人账号登陆，可以选择常用表情和gif动画，并支持自定义搜索表情。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-a26858def2ac7e041376a0ec89c11034_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1198\" data-rawheight=\"1076\" class=\"origin_image zh-lightbox-thumb\" width=\"1198\" data-original=\"https://pic1.zhimg.com/v2-a26858def2ac7e041376a0ec89c11034_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1198&#39; height=&#39;1076&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1198\" data-rawheight=\"1076\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1198\" data-original=\"https://pic1.zhimg.com/v2-a26858def2ac7e041376a0ec89c11034_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-a26858def2ac7e041376a0ec89c11034_b.jpg\"/></figure><p>进入来必力官网注册账号，填写网站域名，进入代码管理界面获得data-uid，复制到next主题<i>config.yml中的livere</i>uid处便可，重新提交网站便可看到评论专区。<code>编写文章时应在头部添加comments: true</code></p><h2>7.绑定个人域名</h2><p>现在使用的域名<code>weizhixiaoyi.github.io</code>是github提供的二级域名，也可绑定自己的个性域名<code>weizhixiaoyi.com</code>。域名是在阿里云购买，年费为55元，也可以在狗爹<code>https://sg.godaddy.com</code>购买，购买好域名之后便可以直接解析。</p><h2>7.1GitHub端</h2><p>在next主题中source文件夹中创建<code>CNAME</code>文件，没有后缀名，然后将个人域名<code>weizhixiaoyi.com</code>添加进<code>CNAME</code>文件即可，然后通过<code>hexo g</code> <code>hexo d</code>重新部署网站。</p><h2>7.2域名解析</h2><p>如果将域名指向另外一个域名，需要增加CNAME记录。登陆阿里云官网，进入控制台中域名设置，添加解析。</p><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>记录类型：CNAME</li><li>主机记录：@</li><li>解析线路：默认</li><li>记录值：<a href=\"https://link.zhihu.com/?target=http%3A//weizhixiaoyi.github.io\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">weizhixiaoyi.github.io</span><span class=\"invisible\"></span></a></li></ul><p>解析成功后，等待几分钟便可登陆<a href=\"https://link.zhihu.com/?target=http%3A//weizhixiaoyi.com\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">weizhixiaoyi.com</span><span class=\"invisible\"></span></a>查看网站内容。</p><h2>7.博客SEO优化</h2><p>SEO优化也就是搜索引擎优化，搜索引擎优化即为增加博客内容被搜索引擎爬取次数，以此增加博客的点击率和曝光度。如果想让自己博客更加容易被搜索到，便是让百度爬虫、谷歌爬虫主动去爬取自己博客内容，但由于Github博客屏蔽百度爬虫，所以只能将自己的博客收录到谷歌，当然这种方法适合于墙外用户。</p><h2>7.1确认收录情况</h2><p>在谷歌上搜索<code>site:weizhixiaoyi.com</code>，如果能搜索内容就已经被谷歌收录，否则就没有被谷歌收录。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-7b827b18aa3efb3b428853517ff0aec5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2496\" data-rawheight=\"1402\" class=\"origin_image zh-lightbox-thumb\" width=\"2496\" data-original=\"https://pic2.zhimg.com/v2-7b827b18aa3efb3b428853517ff0aec5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2496&#39; height=&#39;1402&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2496\" data-rawheight=\"1402\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2496\" data-original=\"https://pic2.zhimg.com/v2-7b827b18aa3efb3b428853517ff0aec5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-7b827b18aa3efb3b428853517ff0aec5_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>7.1网站身份验证</h2><p>验证网站的目的就是证明你是网站的所有者，这里使用站长平台功能进行验证，另外没有梯子的朋友可以通过<code>shadowsock</code>+<code>搬瓦工</code>自行搭建。</p><p>进入谷歌站长平台中的搜索引擎提交入口，添加域名，选择验证方式。个人选择的是在网页中添加标签，进入next主题文件夹，然后找到layout/<i>partials/，打开head.swig文件，在theme</i>google<i>site</i>verification处添加如下信息。</p><div class=\"highlight\"><pre><code class=\"language-text\">{% if theme.google_site_verification %}\n  &lt;meta name=&#34;google-site-verification&#34; content=&#34;E1Oy09IV-Rsypa8wpY-yrplcH8RMIHLCzj3m91nX1Eo&#34; /&gt;\n{% endif %}</code></pre></div><p>然后回到<code>myblog</code>文件夹下将<i>config.yml中google</i>site_vertification设置为<code>true</code>。当然你也可以选择其他验证方式，比如添加html文档。信息添加成功之后便可利用<code>hexo g</code>和<code>hexo d</code>更新博客内容，至此网站身份验证结束。</p><h2>7.2添加Sitemap</h2><p>sitemap站点地图是一种文件格式，可以通过该文件列出您网站上的链接，从而将您网站内容告知谷歌和其他搜索引擎。</p><p>首先安装针对谷歌的插件<code>npm install hexo-generator-sitemap --save</code>，然后进入<code>myblog</code>文件夹下将<code>sitemap</code>设置如下。</p><div class=\"highlight\"><pre><code class=\"language-text\"># sitemap\nsitemap:\n  path: sitemap.xml</code></pre></div><h2>7.3谷歌收录博客</h2><p>谷歌收录操作比较简单，就是向Google站长工具提交sitemap，成功登陆Google账号后，添加站点验证。站点验证通过后找到站点地图界面，然后进行添加站点地图地址就行啦。等待1天后通过<code>site:weizhixiaoyi.com</code>能够搜索到博客内容，便证明谷歌搜索引擎已收录网站内容。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-1650958ed6e88a7b875c4be258914e28_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2500\" data-rawheight=\"1370\" class=\"origin_image zh-lightbox-thumb\" width=\"2500\" data-original=\"https://pic1.zhimg.com/v2-1650958ed6e88a7b875c4be258914e28_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2500&#39; height=&#39;1370&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2500\" data-rawheight=\"1370\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2500\" data-original=\"https://pic1.zhimg.com/v2-1650958ed6e88a7b875c4be258914e28_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-1650958ed6e88a7b875c4be258914e28_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>另外也可通过bing站长管理工具进行收录网站内容，将网站内容呈现给更多需要帮助的人。针对百度爬虫不能爬取Github博客内容问题，我尝试过利用coding托管(免费版绑定域名有广告)、CDN加速(对于流量太小的网站没什么用)，但感觉效果都不是太好，所以问题亟待解决，等找到合适的解决办法之后再告知大家。</p><h2>8.ToDoList</h2><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>寻找更好的方法解决百度爬虫无法爬取博客内容的问题</li><li>博客增加转发功能</li></ul><hr/><h2>9.推广</h2><p>更多内容请关注公众号’谓之小一’，若有疑问可在公众号后台提问，随时回答，欢迎关注，内容转载请注明出处。</p><p><a href=\"https://link.zhihu.com/?target=http%3A//weixin.qq.com/r/Ty_4oDPE2W6mrXfD93pd\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">weixin.qq.com/r/Ty_4oDP</span><span class=\"invisible\">E2W6mrXfD93pd</span><span class=\"ellipsis\"></span></a> (二维码自动识别)</p><p></p>", 
            "topic": [
                {
                    "tag": "博客", 
                    "tagLink": "https://api.zhihu.com/topics/19550419"
                }, 
                {
                    "tag": "Hexo", 
                    "tagLink": "https://api.zhihu.com/topics/19851557"
                }, 
                {
                    "tag": "Github Pages", 
                    "tagLink": "https://api.zhihu.com/topics/19792731"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/34553389", 
            "userName": "谓之小一", 
            "userLink": "https://www.zhihu.com/people/761dda159c5cec5fd926f761ab5f4ccf", 
            "upvote": 4, 
            "title": "Python之MatPlotLib使用教程", 
            "content": "<h2>1.Matplotlib简介</h2><ol><li>Matplotlib是非常强大的python画图工具</li><li>Matplotlib可以画图线图、散点图、等高线图、条形图、柱形图、3D图形、图形动画等。 </li></ol><h2>2.Matplotlib安装</h2><div class=\"highlight\"><pre><code class=\"language-text\">pip3 install matplotlib#python3</code></pre></div><h2>3.Matplotlib引入</h2><div class=\"highlight\"><pre><code class=\"language-text\">import matplotlib.pyplot as plt#为方便简介为plt\nimport numpy as np#画图过程中会使用numpy\nimport pandas as pd#画图过程中会使用pandas</code></pre></div><h2>4.Matplotlib基本应用</h2><div class=\"highlight\"><pre><code class=\"language-text\">x=np.linspace(-1,1,50)#定义x数据范围\ny1=2*x+1#定义y数据范围\ny2=x**2\nplt.figure()#定义一个图像窗口\nplt.plot(x,y)#plot()画出曲线\nplt.show()#显示图像</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-d4811b3dacf4d66c0bf5e4522ef40aff_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"854\" data-rawheight=\"648\" class=\"origin_image zh-lightbox-thumb\" width=\"854\" data-original=\"https://pic4.zhimg.com/v2-d4811b3dacf4d66c0bf5e4522ef40aff_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;854&#39; height=&#39;648&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"854\" data-rawheight=\"648\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"854\" data-original=\"https://pic4.zhimg.com/v2-d4811b3dacf4d66c0bf5e4522ef40aff_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-d4811b3dacf4d66c0bf5e4522ef40aff_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>4.1figure图像</h2><p>matplotlib的figure为单独图像窗口，小窗口内还可以有更多的小图片。</p><div class=\"highlight\"><pre><code class=\"language-text\">x=np.linspace(-3,3,50)#50为生成的样本数\ny1=2*x+1\ny2=x**2\nplt.figure(num=1,figsize=(8,5))#定义编号为1 大小为(8,5)\nplt.plot(x,y1,color=&#39;red&#39;,linewidth=2,linestyle=&#39;--&#39;)#颜色为红色，线宽度为2，线风格为--\nplt.plot(x,y2)#进行画图\nplt.show()#显示图</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-b312c874134072b83e1145d17444617a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"748\" data-rawheight=\"474\" class=\"origin_image zh-lightbox-thumb\" width=\"748\" data-original=\"https://pic3.zhimg.com/v2-b312c874134072b83e1145d17444617a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;748&#39; height=&#39;474&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"748\" data-rawheight=\"474\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"748\" data-original=\"https://pic3.zhimg.com/v2-b312c874134072b83e1145d17444617a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-b312c874134072b83e1145d17444617a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>4.2设置坐标轴</h2><div class=\"highlight\"><pre><code class=\"language-text\">x=np.linspace(-3,3,50)\ny1=2*x+1\ny2=x**2\nplt.figure(num=2,figsize=(8,5))\nplt.plot(x,y1,color=&#39;red&#39;,linewidth=2,linestyle=&#39;-&#39;)\nplt.plot(x,y2)#进行画图\nplt.xlim(-1,2)\nplt.ylim(-2,3)\nplt.xlabel(&#34;I&#39;m x&#34;)\nplt.ylabel(&#34;I&#39;m y&#34;)\nplt.show()</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-29045ee17401b6e8f0819c58248c1ee7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"874\" data-rawheight=\"580\" class=\"origin_image zh-lightbox-thumb\" width=\"874\" data-original=\"https://pic4.zhimg.com/v2-29045ee17401b6e8f0819c58248c1ee7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;874&#39; height=&#39;580&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"874\" data-rawheight=\"580\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"874\" data-original=\"https://pic4.zhimg.com/v2-29045ee17401b6e8f0819c58248c1ee7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-29045ee17401b6e8f0819c58248c1ee7_b.jpg\"/></figure><p>自定义坐标轴</p><div class=\"highlight\"><pre><code class=\"language-text\">x=np.linspace(-3,3,50)\ny1=2*x+1\ny2=x**2\nplt.figure(num=2,figsize=(8,5))\nplt.plot(x,y1,color=&#39;red&#39;,linewidth=2,linestyle=&#39;-&#39;)\nplt.plot(x,y2)#进行画图\nplt.xlim(-1,2)\nplt.ylim(-2,3)\nplt.xlabel(&#34;I&#39;m x&#34;)\nplt.ylabel(&#34;I&#39;m y&#34;)\nnew_ticks=np.linspace(-1,2,5)#小标从-1到2分为5个单位\nprint(new_ticks)\n#[-1.   -0.25  0.5   1.25  2.  ]\nplt.xticks(new_ticks)#进行替换新下标\nplt.yticks([-2,-1,1,2,],\n           [r&#39;$really\\ bad$&#39;,&#39;$bad$&#39;,&#39;$well$&#39;,&#39;$really\\ well$&#39;])\nplt.show()</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-afb878b59b6bc10be358e3ea9f2fb5a3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"756\" data-rawheight=\"502\" class=\"origin_image zh-lightbox-thumb\" width=\"756\" data-original=\"https://pic4.zhimg.com/v2-afb878b59b6bc10be358e3ea9f2fb5a3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;756&#39; height=&#39;502&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"756\" data-rawheight=\"502\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"756\" data-original=\"https://pic4.zhimg.com/v2-afb878b59b6bc10be358e3ea9f2fb5a3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-afb878b59b6bc10be358e3ea9f2fb5a3_b.jpg\"/></figure><p>设置边框属性</p><div class=\"highlight\"><pre><code class=\"language-text\">x=np.linspace(-3,3,50)\ny1=2*x+1\ny2=x**2\nplt.figure(num=2,figsize=(8,5))\nplt.plot(x,y1,color=&#39;red&#39;,linewidth=2,linestyle=&#39;--&#39;)\nplt.plot(x,y2)#进行画图\nplt.xlim(-1,2)\nplt.ylim(-2,3)\nnew_ticks=np.linspace(-1,2,5)#小标从-1到2分为5个单位\nplt.xticks(new_ticks)#进行替换新下标\nplt.yticks([-2,-1,1,2,],\n           [r&#39;$really\\ bad$&#39;,&#39;$bad$&#39;,&#39;$well$&#39;,&#39;$really\\ well$&#39;])\nax=plt.gca()#gca=get current axis\nax.spines[&#39;right&#39;].set_color(&#39;none&#39;)#边框属性设置为none 不显示\nax.spines[&#39;top&#39;].set_color(&#39;none&#39;)\nplt.show()</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-5532543ff41a93b22d92cfc3ea64ac86_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"950\" data-rawheight=\"578\" class=\"origin_image zh-lightbox-thumb\" width=\"950\" data-original=\"https://pic3.zhimg.com/v2-5532543ff41a93b22d92cfc3ea64ac86_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;950&#39; height=&#39;578&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"950\" data-rawheight=\"578\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"950\" data-original=\"https://pic3.zhimg.com/v2-5532543ff41a93b22d92cfc3ea64ac86_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-5532543ff41a93b22d92cfc3ea64ac86_b.jpg\"/></figure><p>调整移动坐标轴</p><div class=\"highlight\"><pre><code class=\"language-text\">x=np.linspace(-3,3,50)\ny1=2*x+1\ny2=x**2\nplt.figure(num=2,figsize=(8,5))\nplt.plot(x,y1,color=&#39;red&#39;,linewidth=2,linestyle=&#39;--&#39;)\nplt.plot(x,y2)#进行画图\nplt.xlim(-1,2)\nplt.ylim(-2,3)\nnew_ticks=np.linspace(-1,2,5)#小标从-1到2分为5个单位\nplt.xticks(new_ticks)#进行替换新下标\nplt.yticks([-2,-1,1,2,],\n           [r&#39;$really\\ bad$&#39;,&#39;$bad$&#39;,&#39;$well$&#39;,&#39;$really\\ well$&#39;])\nax=plt.gca()#gca=get current axis\nax.spines[&#39;right&#39;].set_color(&#39;none&#39;)#边框属性设置为none 不显示\nax.spines[&#39;top&#39;].set_color(&#39;none&#39;)\nax.xaxis.set_ticks_position(&#39;bottom&#39;)#使用xaxis.set_ticks_position设置x坐标刻度数字或名称的位置 所有属性为top、bottom、both、default、none\nax.spines[&#39;bottom&#39;].set_position((&#39;data&#39;, 0))#使用.spines设置边框x轴；使用.set_position设置边框位置，y=0位置 位置所有属性有outward、axes、data\nax.yaxis.set_ticks_position(&#39;left&#39;)\nax.spines[&#39;left&#39;].set_position((&#39;data&#39;,0))#坐标中心点在(0,0)位置\nplt.show()</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-a820d92997a90c743b2bc870edb0ffb8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"892\" data-rawheight=\"562\" class=\"origin_image zh-lightbox-thumb\" width=\"892\" data-original=\"https://pic1.zhimg.com/v2-a820d92997a90c743b2bc870edb0ffb8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;892&#39; height=&#39;562&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"892\" data-rawheight=\"562\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"892\" data-original=\"https://pic1.zhimg.com/v2-a820d92997a90c743b2bc870edb0ffb8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-a820d92997a90c743b2bc870edb0ffb8_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>4.3添加图例</h2><p>matplotlib中legend图例帮助我们展示数据对应的图像名称。</p><div class=\"highlight\"><pre><code class=\"language-text\">x=np.linspace(-3,3,50)\ny1=2*x+1\ny2=x**2\nplt.figure(num=2,figsize=(8,5))\nplt.xlim(-1,2)\nplt.ylim(-2,3)\nnew_ticks=np.linspace(-1,2,5)#小标从-1到2分为5个单位\nplt.xticks(new_ticks)#进行替换新下标\nplt.yticks([-2,-1,1,2,],\n           [r&#39;$really\\ bad$&#39;,&#39;$bad$&#39;,&#39;$well$&#39;,&#39;$really\\ well$&#39;])\n\nl1,=plt.plot(x,y1,color=&#39;red&#39;,linewidth=2,linestyle=&#39;--&#39;,label=&#39;linear line&#39;)\nl2,=plt.plot(x,y2,label=&#39;square line&#39;)#进行画图\nplt.legend(loc=&#39;best&#39;)#显示在最好的位置\nplt.show()#显示图</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-2f6c58ff672acbf70432944628557dbf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"766\" data-rawheight=\"490\" class=\"origin_image zh-lightbox-thumb\" width=\"766\" data-original=\"https://pic4.zhimg.com/v2-2f6c58ff672acbf70432944628557dbf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;766&#39; height=&#39;490&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"766\" data-rawheight=\"490\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"766\" data-original=\"https://pic4.zhimg.com/v2-2f6c58ff672acbf70432944628557dbf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-2f6c58ff672acbf70432944628557dbf_b.jpg\"/></figure><p>调整位置和名称，单独修改label信息，我们可以在plt.legend输入更多参数</p><div class=\"highlight\"><pre><code class=\"language-text\">plt.legend(handles=[l1, l2], labels=[&#39;up&#39;, &#39;down&#39;],  loc=&#39;best&#39;)\n#loc有很多参数 其中best自分配最佳位置\n&#39;&#39;&#39;\n &#39;best&#39; : 0,          \n &#39;upper right&#39;  : 1,\n &#39;upper left&#39;   : 2,\n &#39;lower left&#39;   : 3,\n &#39;lower right&#39;  : 4,\n &#39;right&#39;        : 5,\n &#39;center left&#39;  : 6,\n &#39;center right&#39; : 7,\n &#39;lower center&#39; : 8,\n &#39;upper center&#39; : 9,\n &#39;center&#39;       : 10,\n &#39;&#39;&#39;</code></pre></div><h2>4.4标注</h2><div class=\"highlight\"><pre><code class=\"language-text\">x=np.linspace(-3,3,50)\ny = 2*x + 1\nplt.figure(num=1, figsize=(8, 5))\nplt.plot(x, y,)\n\n#移动坐标轴\nax = plt.gca()\nax.spines[&#39;right&#39;].set_color(&#39;none&#39;)\nax.spines[&#39;top&#39;].set_color(&#39;none&#39;)\nax.xaxis.set_ticks_position(&#39;bottom&#39;)\nax.spines[&#39;bottom&#39;].set_position((&#39;data&#39;, 0))\nax.yaxis.set_ticks_position(&#39;left&#39;)\nax.spines[&#39;left&#39;].set_position((&#39;data&#39;, 0))\n\n#标注信息\nx0=1\ny0=2*x0+1\nplt.scatter(x0,y0,s=50,color=&#39;b&#39;)\nplt.plot([x0,x0],[y0,0],&#39;k--&#39;,lw=2.5)#连接(x0,y0)(x0,0) k表示黑色 lw=2.5表示线粗细\n#xycoords=&#39;data&#39;是基于数据的值来选位置，xytext=(+30,-30)和textcoords=&#39;offset points&#39;对于标注位置描述和xy偏差值，arrowprops对图中箭头类型设置\nplt.annotate(r&#39;$2x0+1=%s$&#39; % y0, xy=(x0, y0), xycoords=&#39;data&#39;, xytext=(+30, -30),\n             textcoords=&#39;offset points&#39;, fontsize=16,\n             arrowprops=dict(arrowstyle=&#39;-&gt;&#39;, connectionstyle=&#34;arc3,rad=.2&#34;))\n#添加注视text（-3.7,3）表示选取text位置 空格需要用\\进行转译 fontdict设置文本字体             \nplt.text(-3.7, 3, r&#39;$This\\ is\\ the\\ some\\ text. \\mu\\ \\sigma_i\\ \\alpha_t$&#39;,\n         fontdict={&#39;size&#39;: 16, &#39;color&#39;: &#39;r&#39;})\nplt.show()</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-644976b68395020c1d57ea416feb1077_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"780\" data-rawheight=\"480\" class=\"origin_image zh-lightbox-thumb\" width=\"780\" data-original=\"https://pic4.zhimg.com/v2-644976b68395020c1d57ea416feb1077_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;780&#39; height=&#39;480&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"780\" data-rawheight=\"480\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"780\" data-original=\"https://pic4.zhimg.com/v2-644976b68395020c1d57ea416feb1077_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-644976b68395020c1d57ea416feb1077_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>4.5能见度调整</h2><div class=\"highlight\"><pre><code class=\"language-text\">x=np.linspace(-3, 3, 50)\ny=0.1*x\nplt.figure()\nplt.plot(x, y, linewidth=10, zorder=1)\nplt.ylim(-2, 2)\n\n#移动坐标轴\nax = plt.gca()\nax.spines[&#39;right&#39;].set_color(&#39;none&#39;)\nax.spines[&#39;top&#39;].set_color(&#39;none&#39;)\nax.spines[&#39;top&#39;].set_color(&#39;none&#39;)\nax.xaxis.set_ticks_position(&#39;bottom&#39;)\nax.spines[&#39;bottom&#39;].set_position((&#39;data&#39;, 0))\nax.yaxis.set_ticks_position(&#39;left&#39;)\nax.spines[&#39;left&#39;].set_position((&#39;data&#39;, 0))\n\n#label.set_fontsize(12)重新调整字体大小 bbox设置目的内容的透明度相关参数 facecolor调节box前景色 edgecolor设置边框 alpha设置透明度 zorder设置图层顺序\nfor label in ax.get_xticklabels() + ax.get_yticklabels():\n    label.set_fontsize(12)\n    label.set_bbox(dict(facecolor=&#39;red&#39;, edgecolor=&#39;None&#39;, alpha=0.7, zorder=2))\nplt.show()</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ab76e21df555e13c69bbbe16f7d99348_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"834\" data-rawheight=\"664\" class=\"origin_image zh-lightbox-thumb\" width=\"834\" data-original=\"https://pic1.zhimg.com/v2-ab76e21df555e13c69bbbe16f7d99348_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;834&#39; height=&#39;664&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"834\" data-rawheight=\"664\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"834\" data-original=\"https://pic1.zhimg.com/v2-ab76e21df555e13c69bbbe16f7d99348_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-ab76e21df555e13c69bbbe16f7d99348_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>5.画图种类</h2><h2>5.1Scatter散点图</h2><div class=\"highlight\"><pre><code class=\"language-text\">n=1024\nX=np.random.normal(0,1,n)#每一个点的X值\nY=np.random.normal(0,1,n)#每一个点的Y值\nT=np.arctan2(Y,X)#arctan2返回给定的X和Y值的反正切值\n#scatter画散点图 size=75 颜色为T 透明度为50% 利用xticks函数来隐藏x坐标轴\nplt.scatter(X,Y,s=75,c=T,alpha=0.5)\nplt.xlim(-1.5,1.5)\nplt.xticks(())#忽略xticks\nplt.ylim(-1.5,1.5)\nplt.yticks(())#忽略yticks\nplt.show()</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-4b3740dc344f79aa0325589ef265f3ca_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"812\" data-rawheight=\"624\" class=\"origin_image zh-lightbox-thumb\" width=\"812\" data-original=\"https://pic3.zhimg.com/v2-4b3740dc344f79aa0325589ef265f3ca_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;812&#39; height=&#39;624&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"812\" data-rawheight=\"624\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"812\" data-original=\"https://pic3.zhimg.com/v2-4b3740dc344f79aa0325589ef265f3ca_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-4b3740dc344f79aa0325589ef265f3ca_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>5.2条形图</h2><div class=\"highlight\"><pre><code class=\"language-text\">#基本图形\nn=12\nX=np.arange(n)\nY1=(1-X/float(n))*np.random.uniform(0.5,1,n)\nY2=(1-X/float(n))*np.random.uniform(0.5,1,n)\nplt.bar(X,+Y1,facecolor=&#39;#9999ff&#39;,edgecolor=&#39;white&#39;)\nplt.bar(X,-Y2,facecolor=&#39;#ff9999&#39;,edgecolor=&#39;white&#39;)\n\n#标记值\nfor x,y in zip(X,Y1):#zip表示可以传递两个值\n    plt.text(x+0.4,y+0.05,&#39;%.2f&#39;%y,ha=&#39;center&#39;,va=&#39;bottom&#39;)#ha表示横向对齐 bottom表示向下对齐\nfor x,y in zip(X,Y2):\n    plt.text(x+0.4,-y-0.05,&#39;%.2f&#39;%y,ha=&#39;center&#39;,va=&#39;top&#39;)\nplt.xlim(-0.5,n)\nplt.xticks(())#忽略xticks\nplt.ylim(-1.25,1.25)\nplt.yticks(())#忽略yticks\nplt.show()</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-53a4c4552696d86bdd998e0b870120f4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"802\" data-rawheight=\"618\" class=\"origin_image zh-lightbox-thumb\" width=\"802\" data-original=\"https://pic1.zhimg.com/v2-53a4c4552696d86bdd998e0b870120f4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;802&#39; height=&#39;618&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"802\" data-rawheight=\"618\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"802\" data-original=\"https://pic1.zhimg.com/v2-53a4c4552696d86bdd998e0b870120f4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-53a4c4552696d86bdd998e0b870120f4_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>5.3等高线图</h2><div class=\"highlight\"><pre><code class=\"language-text\">n=256\nx=np.linspace(-3,3,n)\ny=np.linspace(-3,3,n)\nX,Y=np.meshgrid(x,y)#meshgrid从坐标向量返回坐标矩阵\n#f函数用来计算高度值 利用contour函数把颜色加进去 位置参数依次为x,y,f(x,y)，透明度为0.75，并将f(x,y)的值对应到camp之中\ndef f(x,y):\n    return (1 - x / 2 + x ** 5 + y ** 3) * np.exp(-x ** 2 - y ** 2)\nplt.contourf(X,Y,f(X,Y),8,alpha=0.75,cmap=plt.cm.hot)#8表示等高线分成多少份 alpha表示透明度 cmap表示color map\n#使用plt.contour函数进行等高线绘制 参数依次为x,y,f(x,y)，颜色选择黑色，线条宽度为0.5\nC=plt.contour(X,Y,f(X,Y),8,colors=&#39;black&#39;,linewidth=0.5)\n#使用plt.clabel添加高度数值 inline控制是否将label画在线里面，字体大小为10\nplt.clabel(C,inline=True,fontsize=10)\nplt.xticks(())#隐藏坐标轴\nplt.yticks(())\nplt.show()</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-23a143084e7ec972c06bd08e9dff06f1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"800\" data-rawheight=\"608\" class=\"origin_image zh-lightbox-thumb\" width=\"800\" data-original=\"https://pic2.zhimg.com/v2-23a143084e7ec972c06bd08e9dff06f1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;800&#39; height=&#39;608&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"800\" data-rawheight=\"608\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"800\" data-original=\"https://pic2.zhimg.com/v2-23a143084e7ec972c06bd08e9dff06f1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-23a143084e7ec972c06bd08e9dff06f1_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>5.4Image图片</h2><p>利用matplotlib打印出图像</p><div class=\"highlight\"><pre><code class=\"language-text\">a = np.array([0.313660827978, 0.365348418405, 0.423733120134,\n              0.365348418405, 0.439599930621, 0.525083754405,\n              0.423733120134, 0.525083754405, 0.651536351379]).reshape(3,3)\n#origin=&#39;lower&#39;代表的就是选择的原点位置\nplt.imshow(a,interpolation=&#39;nearest&#39;,cmap=&#39;bone&#39;,origin=&#39;lower&#39;)#cmap为color map\nplt.colorbar(shrink=.92)#右边颜色说明 shrink参数是将图片长度变为原来的92%\nplt.xticks(())\nplt.yticks(())\nplt.show()</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-5de128e38d2d392a02b55650785193ae_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"756\" data-rawheight=\"642\" class=\"origin_image zh-lightbox-thumb\" width=\"756\" data-original=\"https://pic3.zhimg.com/v2-5de128e38d2d392a02b55650785193ae_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;756&#39; height=&#39;642&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"756\" data-rawheight=\"642\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"756\" data-original=\"https://pic3.zhimg.com/v2-5de128e38d2d392a02b55650785193ae_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-5de128e38d2d392a02b55650785193ae_b.jpg\"/></figure><p>出图方式 此处采用内插法中的nearest-neighbor<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-526450944d4b46113a3c00d28cc1913b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1888\" data-rawheight=\"986\" class=\"origin_image zh-lightbox-thumb\" width=\"1888\" data-original=\"https://pic4.zhimg.com/v2-526450944d4b46113a3c00d28cc1913b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1888&#39; height=&#39;986&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1888\" data-rawheight=\"986\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1888\" data-original=\"https://pic4.zhimg.com/v2-526450944d4b46113a3c00d28cc1913b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-526450944d4b46113a3c00d28cc1913b_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>5.53D图像</h2><div class=\"highlight\"><pre><code class=\"language-text\">import numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D#需另外导入模块Axes 3D\nfig=plt.figure()#定义图像窗口\nax=Axes3D(fig)#在窗口上添加3D坐标轴\n#将X和Y值编织成栅格\nX=np.arange(-4,4,0.25)\nY=np.arange(-4,4,0.25)\nX,Y=np.meshgrid(X,Y)\nR=np.sqrt(X**2+Y**2)\nZ=np.sin(R)#高度值\n#将colormap rainbow填充颜色，之后将三维图像投影到XY平面做等高线图，其中ratride和cstride表示row和column的宽度\nax.plot_surface(X,Y,Z,rstride=1,cstride=1,cmap=plt.get_cmap(&#39;rainbow&#39;))#rstride表示图像中分割线的跨图\n#添加XY平面等高线 投影到z平面\nax.contourf(X,Y,Z,zdir=&#39;z&#39;,offset=-2,cmap=plt.get_cmap(&#39;rainbow&#39;))#把图像进行投影的图形 offset表示比0坐标轴低两个位置\nax.set_zlim(-2,2)\nplt.show()</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-648bd607c8bc2862015d35ec2d9cbe4a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"796\" data-rawheight=\"604\" class=\"origin_image zh-lightbox-thumb\" width=\"796\" data-original=\"https://pic3.zhimg.com/v2-648bd607c8bc2862015d35ec2d9cbe4a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;796&#39; height=&#39;604&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"796\" data-rawheight=\"604\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"796\" data-original=\"https://pic3.zhimg.com/v2-648bd607c8bc2862015d35ec2d9cbe4a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-648bd607c8bc2862015d35ec2d9cbe4a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>6.多图合并显示</h2><h2>6.1Subplot多合一显示</h2><p>均匀图中图：MatPlotLib可以组合许多的小图在大图中显示，使用的方法叫做subplot。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">figure</span><span class=\"p\">()</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplot</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">)</span><span class=\"c1\">#表示整个图像分割成2行2列，当前位置为1</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">([</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">],[</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">])</span><span class=\"c1\">#横坐标变化为[0,1] 竖坐标变化为[0,2]</span>\n\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplot</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span><span class=\"mi\">3</span><span class=\"p\">,</span><span class=\"mi\">4</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">([</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">],[</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">2</span><span class=\"p\">])</span>\n\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplot</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span><span class=\"mi\">3</span><span class=\"p\">,</span><span class=\"mi\">5</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">([</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">],[</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">3</span><span class=\"p\">])</span>\n\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplot</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span><span class=\"mi\">3</span><span class=\"p\">,</span><span class=\"mi\">6</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">([</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">],[</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">4</span><span class=\"p\">])</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-7213451f734ce4a12094ddf20769c14f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"886\" data-rawheight=\"678\" class=\"origin_image zh-lightbox-thumb\" width=\"886\" data-original=\"https://pic4.zhimg.com/v2-7213451f734ce4a12094ddf20769c14f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;886&#39; height=&#39;678&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"886\" data-rawheight=\"678\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"886\" data-original=\"https://pic4.zhimg.com/v2-7213451f734ce4a12094ddf20769c14f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-7213451f734ce4a12094ddf20769c14f_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>不均匀图中图</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">figure</span><span class=\"p\">()</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplot</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">)</span><span class=\"c1\">#将整个窗口分割成2行1列，当前位置表示第一个图</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">([</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">],[</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">])</span><span class=\"c1\">#横坐标变化为[0,1],竖坐标变化为[0,1]</span>\n\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplot</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span><span class=\"mi\">3</span><span class=\"p\">,</span><span class=\"mi\">4</span><span class=\"p\">)</span><span class=\"c1\">#将整个窗口分割成2行3列，当前位置为4</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">([</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">],[</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">2</span><span class=\"p\">])</span>\n\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplot</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span><span class=\"mi\">3</span><span class=\"p\">,</span><span class=\"mi\">5</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">([</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">],[</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">3</span><span class=\"p\">])</span>\n\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplot</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span><span class=\"mi\">3</span><span class=\"p\">,</span><span class=\"mi\">6</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">([</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">],[</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">4</span><span class=\"p\">])</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b3ce716d0b0cda227b46424bef974019_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"880\" data-rawheight=\"698\" class=\"origin_image zh-lightbox-thumb\" width=\"880\" data-original=\"https://pic2.zhimg.com/v2-b3ce716d0b0cda227b46424bef974019_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;880&#39; height=&#39;698&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"880\" data-rawheight=\"698\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"880\" data-original=\"https://pic2.zhimg.com/v2-b3ce716d0b0cda227b46424bef974019_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-b3ce716d0b0cda227b46424bef974019_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>6.2SubPlot分格显示</h2><p>方法一</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">import</span> <span class=\"nn\">matplotlib.gridspec</span> <span class=\"kn\">as</span> <span class=\"nn\">gridspec</span><span class=\"c1\">#引入新模块</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">figure</span><span class=\"p\">()</span>\n<span class=\"s1\">&#39;&#39;&#39;\n</span><span class=\"s1\">使用plt.subplot2grid创建第一个小图，(3,3)表示将整个图像分割成3行3列，(0,0)表示从第0行0列开始作图，colspan=3表示列的跨度为3。colspan和rowspan缺省时默认跨度为1\n</span><span class=\"s1\">&#39;&#39;&#39;</span>\n<span class=\"n\">ax1</span> <span class=\"o\">=</span> <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplot2grid</span><span class=\"p\">((</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">colspan</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">)</span>  <span class=\"c1\"># stands for axes</span>\n<span class=\"n\">ax1</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">([</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">])</span>\n<span class=\"n\">ax1</span><span class=\"o\">.</span><span class=\"n\">set_title</span><span class=\"p\">(</span><span class=\"s1\">&#39;ax1_title&#39;</span><span class=\"p\">)</span><span class=\"c1\">#设置图的标题</span>\n\n<span class=\"c1\">#将图像分割成3行3列，从第1行0列开始做图，列的跨度为2</span>\n<span class=\"n\">ax2</span> <span class=\"o\">=</span> <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplot2grid</span><span class=\"p\">((</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"n\">colspan</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n\n<span class=\"c1\">#将图像分割成3行3列，从第1行2列开始做图，行的跨度为2</span>\n<span class=\"n\">ax3</span> <span class=\"o\">=</span> <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplot2grid</span><span class=\"p\">((</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">),</span> <span class=\"n\">rowspan</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n\n<span class=\"c1\">#将图像分割成3行3列，从第2行0列开始做图，行与列的跨度默认为1</span>\n<span class=\"n\">ax4</span> <span class=\"o\">=</span> <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplot2grid</span><span class=\"p\">((</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">))</span>\n<span class=\"n\">ax4</span><span class=\"o\">.</span><span class=\"n\">scatter</span><span class=\"p\">([</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">])</span>\n<span class=\"n\">ax4</span><span class=\"o\">.</span><span class=\"n\">set_xlabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;ax4_x&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">ax4</span><span class=\"o\">.</span><span class=\"n\">set_ylabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;ax4_y&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">ax5</span> <span class=\"o\">=</span> <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplot2grid</span><span class=\"p\">((</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">))</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-9bac0d84c42395b216f2b53e782feaae_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"946\" data-rawheight=\"688\" class=\"origin_image zh-lightbox-thumb\" width=\"946\" data-original=\"https://pic3.zhimg.com/v2-9bac0d84c42395b216f2b53e782feaae_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;946&#39; height=&#39;688&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"946\" data-rawheight=\"688\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"946\" data-original=\"https://pic3.zhimg.com/v2-9bac0d84c42395b216f2b53e782feaae_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-9bac0d84c42395b216f2b53e782feaae_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>方法二</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">figure</span><span class=\"p\">()</span>\n<span class=\"n\">gs</span> <span class=\"o\">=</span> <span class=\"n\">gridspec</span><span class=\"o\">.</span><span class=\"n\">GridSpec</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">)</span><span class=\"c1\">#将图像分割成3行3列</span>\n<span class=\"n\">ax6</span> <span class=\"o\">=</span> <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplot</span><span class=\"p\">(</span><span class=\"n\">gs</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"p\">:])</span><span class=\"c1\">#gs[0:1]表示图占第0行和所有列</span>\n<span class=\"n\">ax7</span> <span class=\"o\">=</span> <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplot</span><span class=\"p\">(</span><span class=\"n\">gs</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"p\">:</span><span class=\"mi\">2</span><span class=\"p\">])</span><span class=\"c1\">#gs[1,:2]表示图占第1行和第二列前的所有列</span>\n<span class=\"n\">ax8</span> <span class=\"o\">=</span> <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplot</span><span class=\"p\">(</span><span class=\"n\">gs</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">:,</span> <span class=\"mi\">2</span><span class=\"p\">])</span>\n<span class=\"n\">ax9</span> <span class=\"o\">=</span> <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplot</span><span class=\"p\">(</span><span class=\"n\">gs</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">])</span>\n<span class=\"n\">ax10</span> <span class=\"o\">=</span> <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplot</span><span class=\"p\">(</span><span class=\"n\">gs</span><span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mi\">2</span><span class=\"p\">])</span><span class=\"c1\">#gs[-1.-2]表示这个图占倒数第1行和倒数第2行</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-b5925d97f6469210a552dae67feb4938_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"916\" data-rawheight=\"654\" class=\"origin_image zh-lightbox-thumb\" width=\"916\" data-original=\"https://pic1.zhimg.com/v2-b5925d97f6469210a552dae67feb4938_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;916&#39; height=&#39;654&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"916\" data-rawheight=\"654\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"916\" data-original=\"https://pic1.zhimg.com/v2-b5925d97f6469210a552dae67feb4938_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-b5925d97f6469210a552dae67feb4938_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>方法三</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"s1\">&#39;&#39;&#39;\n</span><span class=\"s1\">建立一个2行2列的图像窗口，sharex=True表示共享x轴坐标，sharey=True表示共享y轴坐标，((ax11,ax12),(ax13,1x14))表示从到至右一次存放ax11,ax12,ax13,ax114\n</span><span class=\"s1\">&#39;&#39;&#39;</span>\n<span class=\"n\">f</span><span class=\"p\">,</span> <span class=\"p\">((</span><span class=\"n\">ax11</span><span class=\"p\">,</span> <span class=\"n\">ax12</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"n\">ax13</span><span class=\"p\">,</span> <span class=\"n\">ax14</span><span class=\"p\">))</span> <span class=\"o\">=</span> <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplots</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">sharex</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">,</span> <span class=\"n\">sharey</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n<span class=\"n\">ax11</span><span class=\"o\">.</span><span class=\"n\">scatter</span><span class=\"p\">([</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">2</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">2</span><span class=\"p\">])</span><span class=\"n\">ax11</span><span class=\"o\">.</span><span class=\"n\">scatter</span> <span class=\"err\">坐标范围</span><span class=\"n\">x为</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">2</span><span class=\"p\">]</span><span class=\"err\">，</span><span class=\"n\">y为</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">2</span><span class=\"p\">]</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">tight_layout</span><span class=\"p\">()</span><span class=\"c1\">#表示紧凑显示图像</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p> <p class=\"ztext-empty-paragraph\"><br/></p><h2>6.3图中图</h2><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">fig</span><span class=\"o\">=</span><span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">figure</span><span class=\"p\">()</span>\n<span class=\"c1\">#创建数据</span>\n<span class=\"n\">x</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">2</span><span class=\"p\">,</span><span class=\"mi\">3</span><span class=\"p\">,</span><span class=\"mi\">4</span><span class=\"p\">,</span><span class=\"mi\">5</span><span class=\"p\">,</span><span class=\"mi\">6</span><span class=\"p\">,</span><span class=\"mi\">7</span><span class=\"p\">]</span>\n<span class=\"n\">y</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">3</span><span class=\"p\">,</span><span class=\"mi\">4</span><span class=\"p\">,</span><span class=\"mi\">2</span><span class=\"p\">,</span><span class=\"mi\">5</span><span class=\"p\">,</span><span class=\"mi\">8</span><span class=\"p\">,</span><span class=\"mi\">6</span><span class=\"p\">]</span>\n\n<span class=\"c1\">#绘制大图：假设大图的大小为10，那么大图被包含在由(1,1)开始，宽8高8的坐标系之中。</span>\n<span class=\"n\">left</span><span class=\"p\">,</span> <span class=\"n\">bottom</span><span class=\"p\">,</span> <span class=\"n\">width</span><span class=\"p\">,</span> <span class=\"n\">height</span> <span class=\"o\">=</span> <span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"mf\">0.8</span><span class=\"p\">,</span> <span class=\"mf\">0.8</span>\n<span class=\"n\">ax1</span> <span class=\"o\">=</span> <span class=\"n\">fig</span><span class=\"o\">.</span><span class=\"n\">add_axes</span><span class=\"p\">([</span><span class=\"n\">left</span><span class=\"p\">,</span> <span class=\"n\">bottom</span><span class=\"p\">,</span> <span class=\"n\">width</span><span class=\"p\">,</span> <span class=\"n\">height</span><span class=\"p\">])</span>  <span class=\"c1\"># main axes</span>\n<span class=\"n\">ax1</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"s1\">&#39;r&#39;</span><span class=\"p\">)</span><span class=\"c1\">#绘制大图，颜色为red</span>\n<span class=\"n\">ax1</span><span class=\"o\">.</span><span class=\"n\">set_xlabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;x&#39;</span><span class=\"p\">)</span><span class=\"c1\">#横坐标名称为x</span>\n<span class=\"n\">ax1</span><span class=\"o\">.</span><span class=\"n\">set_ylabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;y&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">ax1</span><span class=\"o\">.</span><span class=\"n\">set_title</span><span class=\"p\">(</span><span class=\"s1\">&#39;title&#39;</span><span class=\"p\">)</span><span class=\"c1\">#图名称为title</span>\n\n<span class=\"c1\">#绘制小图，注意坐标系位置和大小的改变</span>\n<span class=\"n\">ax2</span> <span class=\"o\">=</span> <span class=\"n\">fig</span><span class=\"o\">.</span><span class=\"n\">add_axes</span><span class=\"p\">([</span><span class=\"mf\">0.2</span><span class=\"p\">,</span> <span class=\"mf\">0.6</span><span class=\"p\">,</span> <span class=\"mf\">0.25</span><span class=\"p\">,</span> <span class=\"mf\">0.25</span><span class=\"p\">])</span>\n<span class=\"n\">ax2</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"s1\">&#39;b&#39;</span><span class=\"p\">)</span><span class=\"c1\">#颜色为buue</span>\n<span class=\"n\">ax2</span><span class=\"o\">.</span><span class=\"n\">set_xlabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;x&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">ax2</span><span class=\"o\">.</span><span class=\"n\">set_ylabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;y&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">ax2</span><span class=\"o\">.</span><span class=\"n\">set_title</span><span class=\"p\">(</span><span class=\"s1\">&#39;title inside 1&#39;</span><span class=\"p\">)</span>\n\n<span class=\"c1\">#绘制第二个小兔</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">axes</span><span class=\"p\">([</span><span class=\"mf\">0.6</span><span class=\"p\">,</span> <span class=\"mf\">0.2</span><span class=\"p\">,</span> <span class=\"mf\">0.25</span><span class=\"p\">,</span> <span class=\"mf\">0.25</span><span class=\"p\">])</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">y</span><span class=\"p\">[::</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"s1\">&#39;g&#39;</span><span class=\"p\">)</span><span class=\"c1\">#将y进行逆序</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">xlabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;x&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">ylabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;y&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">title</span><span class=\"p\">(</span><span class=\"s1\">&#39;title inside 2&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-fe61b00f83fe1c8aab93e9bc63eb93c0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"918\" data-rawheight=\"712\" class=\"origin_image zh-lightbox-thumb\" width=\"918\" data-original=\"https://pic1.zhimg.com/v2-fe61b00f83fe1c8aab93e9bc63eb93c0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;918&#39; height=&#39;712&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"918\" data-rawheight=\"712\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"918\" data-original=\"https://pic1.zhimg.com/v2-fe61b00f83fe1c8aab93e9bc63eb93c0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-fe61b00f83fe1c8aab93e9bc63eb93c0_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>6.4次坐标轴</h2><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">x</span><span class=\"o\">=</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">10</span><span class=\"p\">,</span><span class=\"mf\">0.1</span><span class=\"p\">)</span>\n<span class=\"n\">y1</span><span class=\"o\">=</span><span class=\"mf\">0.5</span><span class=\"o\">*</span><span class=\"n\">x</span><span class=\"o\">**</span><span class=\"mi\">2</span>\n<span class=\"n\">y2</span><span class=\"o\">=-</span><span class=\"mi\">1</span><span class=\"o\">*</span><span class=\"n\">y1</span>\n<span class=\"n\">fig</span><span class=\"p\">,</span> <span class=\"n\">ax1</span> <span class=\"o\">=</span> <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplots</span><span class=\"p\">()</span>\n\n<span class=\"n\">ax2</span> <span class=\"o\">=</span> <span class=\"n\">ax1</span><span class=\"o\">.</span><span class=\"n\">twinx</span><span class=\"p\">()</span><span class=\"c1\">#镜像显示</span>\n<span class=\"n\">ax1</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y1</span><span class=\"p\">,</span> <span class=\"s1\">&#39;g-&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">ax2</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y2</span><span class=\"p\">,</span> <span class=\"s1\">&#39;b-&#39;</span><span class=\"p\">)</span>\n\n<span class=\"n\">ax1</span><span class=\"o\">.</span><span class=\"n\">set_xlabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;X data&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">ax1</span><span class=\"o\">.</span><span class=\"n\">set_ylabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;Y1 data&#39;</span><span class=\"p\">,</span> <span class=\"n\">color</span><span class=\"o\">=</span><span class=\"s1\">&#39;g&#39;</span><span class=\"p\">)</span><span class=\"c1\">#第一个y坐标轴</span>\n<span class=\"n\">ax2</span><span class=\"o\">.</span><span class=\"n\">set_ylabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;Y2 data&#39;</span><span class=\"p\">,</span> <span class=\"n\">color</span><span class=\"o\">=</span><span class=\"s1\">&#39;b&#39;</span><span class=\"p\">)</span><span class=\"c1\">#第二个y坐标轴</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-483b44c840d5ebc301bc5ffe8dccd1b1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"940\" data-rawheight=\"702\" class=\"origin_image zh-lightbox-thumb\" width=\"940\" data-original=\"https://pic2.zhimg.com/v2-483b44c840d5ebc301bc5ffe8dccd1b1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;940&#39; height=&#39;702&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"940\" data-rawheight=\"702\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"940\" data-original=\"https://pic2.zhimg.com/v2-483b44c840d5ebc301bc5ffe8dccd1b1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-483b44c840d5ebc301bc5ffe8dccd1b1_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>7.动画</h2><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">from</span> <span class=\"nn\">matplotlib</span> <span class=\"kn\">import</span> <span class=\"n\">animation</span><span class=\"c1\">#引入新模块</span>\n<span class=\"n\">fig</span><span class=\"p\">,</span><span class=\"n\">ax</span><span class=\"o\">=</span><span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplots</span><span class=\"p\">()</span>\n<span class=\"n\">x</span><span class=\"o\">=</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">2</span><span class=\"o\">*</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">pi</span><span class=\"p\">,</span><span class=\"mf\">0.01</span><span class=\"p\">)</span><span class=\"c1\">#数据为0~2PI范围内的正弦曲线</span>\n<span class=\"n\">line</span><span class=\"p\">,</span><span class=\"o\">=</span><span class=\"n\">ax</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sin</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">))</span><span class=\"c1\"># line表示列表</span>\n\n<span class=\"c1\">#构造自定义动画函数animate，用来更新每一帧上x和y坐标值，参数表示第i帧</span>\n<span class=\"k\">def</span> <span class=\"nf\">animate</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">):</span>\n    <span class=\"n\">line</span><span class=\"o\">.</span><span class=\"n\">set_ydata</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sin</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"o\">+</span><span class=\"n\">i</span><span class=\"o\">/</span><span class=\"mi\">100</span><span class=\"p\">))</span>\n    <span class=\"k\">return</span> <span class=\"n\">line</span><span class=\"p\">,</span>\n\n<span class=\"c1\">#构造开始帧函数init</span>\n<span class=\"k\">def</span> <span class=\"nf\">init</span><span class=\"p\">():</span>\n    <span class=\"n\">line</span><span class=\"o\">.</span><span class=\"n\">set_ydata</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sin</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">))</span>\n    <span class=\"k\">return</span> <span class=\"n\">line</span><span class=\"p\">,</span>\n\n<span class=\"c1\"># frame表示动画长度，一次循环所包含的帧数；interval表示更新频率 </span>\n<span class=\"c1\"># blit选择更新所有点，还是仅更新新变化产生的点。应该选True，但mac用户选择False。</span>\n<span class=\"n\">ani</span><span class=\"o\">=</span><span class=\"n\">animation</span><span class=\"o\">.</span><span class=\"n\">FuncAnimation</span><span class=\"p\">(</span><span class=\"n\">fig</span><span class=\"o\">=</span><span class=\"n\">fig</span><span class=\"p\">,</span><span class=\"n\">func</span><span class=\"o\">=</span><span class=\"n\">animate</span><span class=\"p\">,</span><span class=\"n\">frames</span><span class=\"o\">=</span><span class=\"mi\">200</span><span class=\"p\">,</span><span class=\"n\">init_func</span><span class=\"o\">=</span><span class=\"n\">init</span><span class=\"p\">,</span><span class=\"n\">interval</span><span class=\"o\">=</span><span class=\"mi\">20</span><span class=\"p\">,</span><span class=\"n\">blit</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-35d8b91153ed0f4775333a15c9e500db_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"900\" data-rawheight=\"688\" class=\"origin_image zh-lightbox-thumb\" width=\"900\" data-original=\"https://pic4.zhimg.com/v2-35d8b91153ed0f4775333a15c9e500db_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;900&#39; height=&#39;688&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"900\" data-rawheight=\"688\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"900\" data-original=\"https://pic4.zhimg.com/v2-35d8b91153ed0f4775333a15c9e500db_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-35d8b91153ed0f4775333a15c9e500db_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>MatPlotLib之中还有很多画图方法，由于篇幅有限不再赘述，更多内容参考MatPlotLib Tutorials。</b></p><hr/><p>更多内容请关注公众号&#39;谓之小一&#39;，若有疑问可在公众号后台提问，随时回答，内容转载请注明出处。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><a href=\"https://link.zhihu.com/?target=http%3A//weixin.qq.com/r/Ty_4oDPE2W6mrXfD93pd\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">weixin.qq.com/r/Ty_4oDP</span><span class=\"invisible\">E2W6mrXfD93pd</span><span class=\"ellipsis\"></span></a> (二维码自动识别)</p><p></p>", 
            "topic": [
                {
                    "tag": "Python 库", 
                    "tagLink": "https://api.zhihu.com/topics/19644560"
                }, 
                {
                    "tag": "Matplotlib", 
                    "tagLink": "https://api.zhihu.com/topics/19748855"
                }, 
                {
                    "tag": "数据挖掘", 
                    "tagLink": "https://api.zhihu.com/topics/19553534"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/34380194", 
            "userName": "谓之小一", 
            "userLink": "https://www.zhihu.com/people/761dda159c5cec5fd926f761ab5f4ccf", 
            "upvote": 1, 
            "title": "Python中NumPy使用技巧", 
            "content": "<h2>1.NumPy概述 </h2><p>NumPy(Numerical Python)是用Python进行科学计算的基础软件包。包含以下特点：<br/> 1. 强大的N维数组对象Array<br/> 2. 成熟的函数库<br/> 3. 用于集成C/C++和Fortran代码的工具<br/> 4. 实用的线性代数、傅立叶变换和随机生成函数</p><h2>2.NumPy安装</h2><div class=\"highlight\"><pre><code class=\"language-text\">pip install numpy或pip3 install numpy</code></pre></div><h2>3.NumPy引入</h2><div class=\"highlight\"><pre><code class=\"language-text\">import numpy as np#为了方便实用numpy 采用np简写</code></pre></div><h2>4.NumPy方法</h2><div class=\"highlight\"><pre><code class=\"language-text\">array=np.array([[1,2,3],[4,5,6]])#将列表转换为矩阵 并转换为int类型\nprint(array)\n&#39;&#39;&#39;\n[[1 2 3]\n [4 5 6]]\n &#39;&#39;&#39;</code></pre></div><h2>4.1NumPy属性</h2><div class=\"highlight\"><pre><code class=\"language-text\">print(&#39;array of dim:&#39;,array.ndim)#矩阵的维度\n#array of dim:2\nprint(&#39;array of shape&#39;,array.shape)#矩阵的行数和列数\n#array of shape:(2,3)\nprint(&#39;number of size:&#39;,array.size)#元素的个数\n#number of size:6</code></pre></div><h2>4.2NumPy创建Array</h2><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>array:创建数组</li><li>dtype:指定数据类型</li><li>zeros:创建数据全为0</li><li>ones:创建数据全为1</li><li>empty:创建数据接近0</li><li>arange:指定范围内创建数据</li><li>linspace创建线段</li></ul><p>创建数组</p><div class=\"highlight\"><pre><code class=\"language-text\">a=np.array([1,2,3])\nprint(a)\n#[1,2,3]</code></pre></div><p>指定数据dtype</p><div class=\"highlight\"><pre><code class=\"language-text\">a=np.array([1,2,3],dtype=np.int)#指定为int类型\nprint(a.dtype)\n#int 64\nb=np.array([1,2,3],dtype=np.float)#指定为float类型\nprint(b.dtype)\n#float 64</code></pre></div><p>创建特定数据</p><div class=\"highlight\"><pre><code class=\"language-text\">a=np.array([[1,2,3],[4,5,6]])#矩阵 2行3列\nprint(a)\n&#39;&#39;&#39;\n[[1 2 3]\n [4 5 6]]\n &#39;&#39;&#39;</code></pre></div><p>创建全0数组</p><div class=\"highlight\"><pre><code class=\"language-text\">a=np.zeros((2,3))#数据全0 2行3列\nprint(a)\n&#39;&#39;&#39;\n[[0 0 0]\n [0 0 0]]\n &#39;&#39;&#39;</code></pre></div><p>创建全1数组 指定特定类型dtype</p><div class=\"highlight\"><pre><code class=\"language-text\">a=np.zeros((2,3),dtype=np.int)#数据全1 2行3列 同时指定类型\nprint(a)\n&#39;&#39;&#39;\n[[1 1 1]\n [1 1 1]]\n &#39;&#39;&#39;</code></pre></div><p>创建全空数组 每个值接近0</p><div class=\"highlight\"><pre><code class=\"language-text\">a=np.empty(2,3)#数据全为empty 3行4列\nprint(a)\n&#39;&#39;&#39;\n[[  0.00000000e+000   0.00000000e+000   2.12704693e-314]\n [  2.12706024e-314   2.12706024e-314   2.12706024e-314]]\n &#39;&#39;&#39;</code></pre></div><p>用array创建连续数组</p><div class=\"highlight\"><pre><code class=\"language-text\">a=np.arange(1,10,2)#1到10的数据 2步长\nprint(a)\n#[1 3 5 7 9]</code></pre></div><p>用reshape改变数据形状</p><div class=\"highlight\"><pre><code class=\"language-text\">a=np.arange(6).reshape(2,3)\nprint(a)\n&#39;&#39;&#39;\n[[0 1 2]\n [3 4 5]]\n &#39;&#39;&#39;</code></pre></div><p>用linspace创建线段形数据</p><div class=\"highlight\"><pre><code class=\"language-text\">a=np.linspace(1,10,20)#开始端1 结束端5 分割成10个数据 生成线段\nprint(a)\n&#39;&#39;&#39;\n[ 1.          1.44444444  1.88888889  2.33333333  2.77777778  3.22222222\n  3.66666667  4.11111111  4.55555556  5.        ]\n  &#39;&#39;&#39;</code></pre></div><h2>4.3NumPy基础运算</h2><p>基础运算之加、减、三角函数等</p><div class=\"highlight\"><pre><code class=\"language-text\">a=np.array([10,20,30,40])\nb=np.arange(4) #array[0,1,2,3]\n\nc=a+b#加法运算\nprint(c)\n#[10,21,32,43]\n\nc=a-b#减法运算\nprint(c)\n#[10.19,28,37]\n\nc=10*np.sin(a)#三角函数运算\n#[-5.44021111,  9.12945251, -9.88031624,  7.4511316 ]\n\nprint(b&lt;3)#逻辑判断\n#[ True  True  True False]\n\nd=np.random.random((2,3))#随机生成2行3列的矩阵\nprint(d)\n&#39;&#39;&#39;\n[[ 0.21116981  0.0804489   0.51855475]\n [ 0.38359164  0.55852973  0.73218811]]\n&#39;&#39;&#39;\nprint(np.sum(d))#元素求和\n#2.48448292958\nprint(np.max(d))#元素求最大值\n#0.732188108709\nprint(np.min(d))#元素求最小值\n#0.0804488978886</code></pre></div><p>多维矩阵运算</p><div class=\"highlight\"><pre><code class=\"language-text\">a=np.array([[1,1],[0,1]])\nb=np.arange(4).reshape((2,2))\n\nc=np.dot(a,b)#或c=a.dot(b)矩阵运算\nprint(c)\n&#39;&#39;&#39;\n[[2 4]\n [2 3]]\n &#39;&#39;&#39;</code></pre></div><p>对行或列执行查找运算</p><div class=\"highlight\"><pre><code class=\"language-text\">a=np.array([[1,2],[3,4]])\nprint(a)\n&#39;&#39;&#39;\n[[1,2]\n [3,4]]\n &#39;&#39;&#39;\nprint(np.max(a,axis=0))#axis=0时是对列进行操作\n#[3,4]\nprint(np.min(a,axis=1))#axis=1是对行进行操作\n#[1,3]</code></pre></div><p>矩阵索引操作</p><div class=\"highlight\"><pre><code class=\"language-text\">A=np.arange(2,14).reshape(3,4)\nprint(A)\n&#39;&#39;&#39;\n[[2,3,4,5]\n [6,7,8,9]\n [10,11,12,13]]\n &#39;&#39;&#39;\nprint(np.argmax(A))#矩阵中最大元素的索引\n#11\nprint(np.argmin(A))#矩阵中最小元素的索引\n#0\nprint(np.mean(A))#或者np.average(A)求解矩阵均值\n#7.5\nprint(np.cumsum(A))#矩阵累加函数\n#[2 5 9 14 20 27 35 44 54 65 77 90]\nprint(np.diff(A))#矩阵累差函数\n&#39;&#39;&#39;\n[[1 1 1]\n [1 1 1]\n [1 1 1]]\n &#39;&#39;&#39;\nprint(np.nonzero(A))#将非0元素的行与列坐标分割开来\n#(array([0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2]), array([0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3]))</code></pre></div><p>矩阵排序、转置、替换操作</p><div class=\"highlight\"><pre><code class=\"language-text\">A=np.arange(14,2,-1).reshape((3,4))\nprint(A)\n&#39;&#39;&#39;\n[[14 13 12 11]\n [10  9  8  7]\n [ 6  5  4  3]]\n &#39;&#39;&#39;\nprint(np.sort(A))#排序\n&#39;&#39;&#39;\n[[11 12 13 14]\n [ 7  8  9 10]\n [ 3  4  5  6]]\n &#39;&#39;&#39;\n\nprint(np.transpose(A))\n&#39;&#39;&#39;\n[[14 10  6]\n [13  9  5]\n [12  8  4]\n [11  7  3]]\n &#39;&#39;&#39;\n\nprint(np.clip(A,5,9))#替换 判断当前矩阵元素是否比最小值小或比最大值大 若是则替换\n&#39;&#39;&#39;\n[[9 9 9 9]\n [9 9 8 7]\n [6 5 5 5]]\n &#39;&#39;&#39;</code></pre></div><h2>5.索引</h2><p>一维索引</p><div class=\"highlight\"><pre><code class=\"language-text\">A=np.arange(0,12)\nprint(A)\n#[ 0  1  2  3  4  5  6  7  8  9 10 11]\nprint(A[1])#一维索引\n#1\n\nA=np.arange(0,12).reshape((3,4))\nprint(A[0])\n#[0,1,2,3]</code></pre></div><p>二维索引</p><div class=\"highlight\"><pre><code class=\"language-text\">A=np.arange(0,12).reshape((3,4))\nprint(A)\n&#39;&#39;&#39;\n[[ 0  1  2  3]\n [ 4  5  6  7]\n [ 8  9 10 11]]\n &#39;&#39;&#39;\nprint(A[1][1])#或者A[1,1]\n#5\nprint(A[1,1:3])#切片处理\n#[5,6]\n\nfor row in A:\n    print(A)\n&#39;&#39;&#39;\n[0 1 2 3]\n[4 5 6 7]\n[ 8  9 10 11]\n &#39;&#39;&#39;\nfor col in A:\n    print(col)\n&#39;&#39;&#39;\n[0 4 8]\n[1 5 9]\n[ 2  6 10]\n[ 3  7 11]\n &#39;&#39;&#39;\n\nfor item in A.flat:\n    print(item)\n&#39;&#39;&#39;\n0\n1\n...\n10\n11\n&#39;&#39;&#39;</code></pre></div><h2>6.NumPy之Array合并</h2><div class=\"highlight\"><pre><code class=\"language-text\">A=np.array([1,1,1])\nB=np.array([2,2,2])\nprint(np.vstack((A,B)))#上下合并\n&#39;&#39;&#39;\n[[1 1 1]\n [2 2 2]]\n &#39;&#39;&#39;\nprint(np.hstack((A,B)))#左右合并\n#[1 1 1 2 2 2]</code></pre></div><p>增加维度</p><div class=\"highlight\"><pre><code class=\"language-text\">A=np.array([1,1,1])\nprint(A.shape)\n#(3,)\nprint(A[np.newaxis,:])\n#[[1 1 1]]\nprint(A[np.newaxis,:].shape)#newaxis增加维度\n#(1,3)\n\nprint(A[:,np.newaxis])\n&#39;&#39;&#39;\n[[1]\n [1]\n [1]]\n &#39;&#39;&#39;\nprint(A[:,np.newaxis].shape)\n#（3,1）</code></pre></div><p>多矩阵合并</p><div class=\"highlight\"><pre><code class=\"language-text\">A = np.array([1,1,1])[:,np.newaxis]\nB = np.array([2,2,2])[:,np.newaxis]\nprint(np.concatenate((A,B,B,A),axis=0))#0表示上下合并\n&#39;&#39;&#39;\n[[1]\n [1]\n [1]\n [2]\n [2]\n [2]\n [2]\n [2]\n [2]\n [1]\n [1]\n [1]]\n &#39;&#39;&#39;\nprint(np.concatenate((A,B,B,A),axis=1))#1表示左右合并\n&#39;&#39;&#39;\n[[1 2 2 1]\n [1 2 2 1]\n [1 2 2 1]]\n &#39;&#39;&#39;</code></pre></div><h2>7.NumPy分割</h2><div class=\"highlight\"><pre><code class=\"language-text\">A=np.arange(12).reshape((3,4))\nprint(A)\n&#39;&#39;&#39;\n[[ 0  1  2  3]\n [ 4  5  6  7]\n [ 8  9 10 11]]\n &#39;&#39;&#39;\nprint(np.split(A,3,axis=0))#横向分割成3部分 或者np.vsplit(A,3)\n#[array([[0, 1, 2, 3]]), array([[4, 5, 6, 7]]), array([[ 8,  9, 10, 11]])]\n\nprint(np.split(A,2,axis=1))#竖向分割成2部分 或者np.hsplit(A,2)\n&#39;&#39;&#39;\n[array([[0, 1],\n       [4, 5],\n       [8, 9]]), array([[ 2,  3],\n       [ 6,  7],\n       [10, 11]])]\n &#39;&#39;&#39;\n\nprint(np.array_split(A,3,axis=1))#不等量分割成3部分\n&#39;&#39;&#39;\n[array([[0, 1],\n       [4, 5],\n       [8, 9]]), array([[ 2],\n       [ 6],\n       [10]]), array([[ 3],\n       [ 7],\n       [11]])]\n&#39;&#39;&#39;</code></pre></div><h2>8.NumPy中copy和deep copy</h2><p>&#39;=&#39;赋值方式会带有关联性</p><div class=\"highlight\"><pre><code class=\"language-text\">a=np.arange(4)\nprint(a)\n#[1 2 3 4]\nb=a\nc=a\nd=b\nprint(b is a)\n#True\nprint(c is a)\n#True\nprint(d is a)\n#True\n\nb[0]=5#改变b的值，a,c,d同样会进行改变\nprint(a)\n#[5 2 3 4]</code></pre></div><p>&#39;copy()&#39;赋值方式没有关联性</p><div class=\"highlight\"><pre><code class=\"language-text\">a=np.arange(4)#deep copy\nprint(a)\n#[0 1 2 3]\nb=a.copy()\na[0]=5\nprint(b)#值并不发生改变\n#[0 1 2 3]</code></pre></div><hr/><p>更多内容请关注公众号&#39;谓之小一&#39;，若有疑问可在公众号后台提问，随时回答。</p><p><a href=\"https://link.zhihu.com/?target=http%3A//weixin.qq.com/r/Ty_4oDPE2W6mrXfD93pd\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">weixin.qq.com/r/Ty_4oDP</span><span class=\"invisible\">E2W6mrXfD93pd</span><span class=\"ellipsis\"></span></a> (二维码自动识别)</p><p></p>", 
            "topic": [
                {
                    "tag": "numpy", 
                    "tagLink": "https://api.zhihu.com/topics/19834165"
                }, 
                {
                    "tag": "Python 库", 
                    "tagLink": "https://api.zhihu.com/topics/19644560"
                }, 
                {
                    "tag": "数据挖掘", 
                    "tagLink": "https://api.zhihu.com/topics/19553534"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/34429851", 
            "userName": "谓之小一", 
            "userLink": "https://www.zhihu.com/people/761dda159c5cec5fd926f761ab5f4ccf", 
            "upvote": 2, 
            "title": "Python之Pandas使用教程", 
            "content": "<h2>1.Pandas概述</h2><ol><li>Pandas是Python的一个数据分析包，该工具为解决数据分析任务而创建。</li><li>Pandas纳入大量库和标准数据模型，提供高效的操作数据集所需的工具。</li><li>Pandas提供大量能使我们快速便捷地处理数据的函数和方法。</li><li>Pandas是字典形式，基于NumPy创建，让NumPy为中心的应用变得更加简单。<br/>2.Pandas安装</li></ol><div class=\"highlight\"><pre><code class=\"language-text\">pip3 install pandas</code></pre></div><h2>3.Pandas引入</h2><div class=\"highlight\"><pre><code class=\"language-text\">import pandas as pd#为了方便实用pandas 采用pd简写</code></pre></div><h2>4.Pandas数据结构</h2><h2>4.1Series</h2><div class=\"highlight\"><pre><code class=\"language-text\">import numpy as np\nimport pandas as pd\ns=pd.Series([1,2,3,np.nan,5,6])\nprint(s)#索引在左边 值在右边\n&#39;&#39;&#39;\n0    1.0\n1    2.0\n2    3.0\n3    NaN\n4    5.0\n5    6.0\ndtype: float64\n &#39;&#39;&#39;</code></pre></div><h2>4.2DataFrame</h2><p>DataFrame是表格型数据结构，包含一组有序的列，每列可以是不同的值类型。DataFrame有行索引和列索引，可以看成由Series组成的字典。</p><div class=\"highlight\"><pre><code class=\"language-text\">dates=pd.date_range(&#39;20180310&#39;,periods=6)\ndf = pd.DataFrame(np.random.randn(6,4), index=dates, columns=[&#39;A&#39;,&#39;B&#39;,&#39;C&#39;,&#39;D&#39;])#生成6行4列位置\nprint(df)#输出6行4列的表格\n&#39;&#39;&#39;\n                   A         B         C         D\n2018-03-10 -0.092889 -0.503172  0.692763 -1.261313\n2018-03-11 -0.895628 -2.300249 -1.098069  0.468986\n2018-03-12  0.084732 -1.275078  1.638007 -0.291145\n2018-03-13 -0.561528  0.431088  0.430414  1.065939\n2018-03-14  1.485434 -0.341404  0.267613 -1.493366\n2018-03-15 -1.671474  0.110933  1.688264 -0.910599\n  &#39;&#39;&#39;\nprint(df[&#39;B&#39;])\n&#39;&#39;&#39;\n2018-03-10   -0.927291\n2018-03-11   -0.406842\n2018-03-12   -0.088316\n2018-03-13   -1.631055\n2018-03-14   -0.929926\n2018-03-15   -0.010904\nFreq: D, Name: B, dtype: float64\n &#39;&#39;&#39;\n\n#创建特定数据的DataFrame\ndf_1=pd.DataFrame({&#39;A&#39; : 1.,\n                    &#39;B&#39; : pd.Timestamp(&#39;20180310&#39;),\n                    &#39;C&#39; : pd.Series(1,index=list(range(4)),dtype=&#39;float32&#39;),\n                    &#39;D&#39; : np.array([3] * 4,dtype=&#39;int32&#39;),\n                    &#39;E&#39; : pd.Categorical([&#34;test&#34;,&#34;train&#34;,&#34;test&#34;,&#34;train&#34;]),\n                    &#39;F&#39; : &#39;foo&#39;\n                    })\nprint(df_1)\n&#39;&#39;&#39;\n     A          B    C  D      E    F\n0  1.0 2018-03-10  1.0  3   test  foo\n1  1.0 2018-03-10  1.0  3  train  foo\n2  1.0 2018-03-10  1.0  3   test  foo\n3  1.0 2018-03-10  1.0  3  train  foo\n&#39;&#39;&#39;\nprint(df_1.dtypes)\n&#39;&#39;&#39;\nA           float64\nB    datetime64[ns]\nC           float32\nD             int32\nE          category\nF            object\ndtype: object\n&#39;&#39;&#39;\nprint(df_1.index)#行的序号\n#Int64Index([0, 1, 2, 3], dtype=&#39;int64&#39;)\nprint(df_1.columns)#列的序号名字\n#Index([&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;, &#39;E&#39;, &#39;F&#39;], dtype=&#39;object&#39;)\nprint(df_1.values)#把每个值进行打印出来\n&#39;&#39;&#39;\n[[1.0 Timestamp(&#39;2018-03-10 00:00:00&#39;) 1.0 3 &#39;test&#39; &#39;foo&#39;]\n [1.0 Timestamp(&#39;2018-03-10 00:00:00&#39;) 1.0 3 &#39;train&#39; &#39;foo&#39;]\n [1.0 Timestamp(&#39;2018-03-10 00:00:00&#39;) 1.0 3 &#39;test&#39; &#39;foo&#39;]\n [1.0 Timestamp(&#39;2018-03-10 00:00:00&#39;) 1.0 3 &#39;train&#39; &#39;foo&#39;]]\n &#39;&#39;&#39;\nprint(df_1.describe())#数字总结\n&#39;&#39;&#39;\n         A    C    D\ncount  4.0  4.0  4.0\nmean   1.0  1.0  3.0\nstd    0.0  0.0  0.0\nmin    1.0  1.0  3.0\n25%    1.0  1.0  3.0\n50%    1.0  1.0  3.0\n75%    1.0  1.0  3.0\nmax    1.0  1.0  3.0\n&#39;&#39;&#39;\nprint(df_1.T)#翻转数据\n&#39;&#39;&#39;\n                     0                    1                    2  \\\nA                    1                    1                    1   \nB  2018-03-10 00:00:00  2018-03-10 00:00:00  2018-03-10 00:00:00   \nC                    1                    1                    1   \nD                    3                    3                    3   \nE                 test                train                 test   \nF                  foo                  foo                  foo   \n\n                     3  \nA                    1  \nB  2018-03-10 00:00:00  \nC                    1  \nD                    3  \nE                train  \nF                  foo  \n&#39;&#39;&#39;\nprint(df_1.sort_index(axis=1, ascending=False))#axis等于1按列进行排序 如ABCDEFG 然后ascending倒叙进行显示\n&#39;&#39;&#39;\n     F      E  D    C          B    A\n0  foo   test  3  1.0 2018-03-10  1.0\n1  foo  train  3  1.0 2018-03-10  1.0\n2  foo   test  3  1.0 2018-03-10  1.0\n3  foo  train  3  1.0 2018-03-10  1.0\n&#39;&#39;&#39;\nprint(df_1.sort_values(by=&#39;E&#39;))#按值进行排序\n&#39;&#39;&#39;\n     A          B    C  D      E    F\n0  1.0 2018-03-10  1.0  3   test  foo\n2  1.0 2018-03-10  1.0  3   test  foo\n1  1.0 2018-03-10  1.0  3  train  foo\n3  1.0 2018-03-10  1.0  3  train  foo\n&#39;&#39;&#39;</code></pre></div><h2>5.Pandas选择数据</h2><div class=\"highlight\"><pre><code class=\"language-text\">dates=pd.date_range(&#39;20180310&#39;,periods=6)\ndf = pd.DataFrame(np.random.randn(6,4), index=dates, columns=[&#39;A&#39;,&#39;B&#39;,&#39;C&#39;,&#39;D&#39;])#生成6行4列位置\nprint(df)\n&#39;&#39;&#39;\n                   A         B         C         D\n2018-03-10 -0.520509 -0.136602 -0.516984  1.357505\n2018-03-11  0.332656 -0.094633  0.382384 -0.914339\n2018-03-12  0.499960  1.576897  2.128730  2.197465\n2018-03-13  0.540385  0.427337 -0.591381  0.126503\n2018-03-14  0.191962  1.237843  1.903370  2.155366\n2018-03-15 -0.188331 -0.578581 -0.845854 -0.056373\n &#39;&#39;&#39;\nprint(df[&#39;A&#39;])#或者df.A 选择某列\n&#39;&#39;&#39;\n2018-03-10   -0.520509\n2018-03-11    0.332656\n2018-03-12    0.499960\n2018-03-13    0.540385\n2018-03-14    0.191962\n2018-03-15   -0.188331\n&#39;&#39;&#39;</code></pre></div><p>切片选择</p><div class=\"highlight\"><pre><code class=\"language-text\">print(df[0:3], df[&#39;20180310&#39;:&#39;20180314&#39;])#两次进行选择 第一次切片选择 第二次按照筛选条件进行选择\n&#39;&#39;&#39;\n                   A         B         C         D\n2018-03-10 -0.520509 -0.136602 -0.516984  1.357505\n2018-03-11  0.332656 -0.094633  0.382384 -0.914339\n2018-03-12  0.499960  1.576897  2.128730  2.197465                    \n                  A         B         C         D\n2018-03-10 -0.520509 -0.136602 -0.516984  1.357505\n2018-03-11  0.332656 -0.094633  0.382384 -0.914339\n2018-03-12  0.499960  1.576897  2.128730  2.197465\n2018-03-13  0.540385  0.427337 -0.591381  0.126503\n2018-03-14  0.191962  1.237843  1.903370  2.155366\n &#39;&#39;&#39;</code></pre></div><p>根据标签loc-行标签进行选择数据</p><div class=\"highlight\"><pre><code class=\"language-text\">print(df.loc[&#39;20180312&#39;, [&#39;A&#39;,&#39;B&#39;]])#按照行标签进行选择 精确选择\n &#39;&#39;&#39;\nA    0.499960\nB    1.576897\nName: 2018-03-12 00:00:00, dtype: float64\n&#39;&#39;&#39;</code></pre></div><p>根据序列iloc-行号进行选择数据</p><div class=\"highlight\"><pre><code class=\"language-text\">print(df.iloc[3, 1])#输出第三行第一列的数据\n#0.427336827399\n\nprint(df.iloc[3:5,0:2])#进行切片选择\n &#39;&#39;&#39;\n                   A         B\n2018-03-13  0.540385  0.427337\n2018-03-14  0.191962  1.237843\n &#39;&#39;&#39;\n\nprint(df.iloc[[1,2,4],[0,2]])#进行不连续筛选\n&#39;&#39;&#39;\n                   A         C\n2018-03-11  0.332656  0.382384\n2018-03-12  0.499960  2.128730\n2018-03-14  0.191962  1.903370\n &#39;&#39;&#39;</code></pre></div><p>根据混合的两种ix</p><div class=\"highlight\"><pre><code class=\"language-text\">print(df.ix[:3, [&#39;A&#39;, &#39;C&#39;]])\n&#39;&#39;&#39;\n                   A         C\n2018-03-10 -0.919275 -1.356037\n2018-03-11  0.010171 -0.380010\n2018-03-12  0.285251 -1.174265\n &#39;&#39;&#39;</code></pre></div><p>根据判断筛选</p><div class=\"highlight\"><pre><code class=\"language-text\">print(df[df.A &gt; 0])#筛选出df.A大于0的元素 布尔条件筛选\n&#39;&#39;&#39;\n                   A         B         C         D\n2018-03-11  0.332656 -0.094633  0.382384 -0.914339\n2018-03-12  0.499960  1.576897  2.128730  2.197465\n2018-03-13  0.540385  0.427337 -0.591381  0.126503\n2018-03-14  0.191962  1.237843  1.903370  2.155366\n &#39;&#39;&#39;</code></pre></div><h2>6.Pandas设置数据</h2><p>根据loc和iloc设置</p><div class=\"highlight\"><pre><code class=\"language-text\">dates = pd.date_range(&#39;20180310&#39;, periods=6)\ndf = pd.DataFrame(np.arange(24).reshape((6,4)), index=dates, columns=[&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;])\nprint(df)\n&#39;&#39;&#39;\n             A   B     C   D\n2018-03-10   0   1     2   3\n2018-03-11   4   5     6   7\n2018-03-12   8   9  1111  11\n2018-03-13  12  13    14  15\n2018-03-14  16  17    18  19\n2018-03-15  20  21    22  23\n&#39;&#39;&#39;\n\ndf.iloc[2,2] = 999#单点设置\ndf.loc[&#39;2018-03-13&#39;, &#39;D&#39;] = 999\nprint(df)\n&#39;&#39;&#39;\n            A   B    C    D\n2018-03-10  0   1    2    3\n2018-03-11  0   5    6    7\n2018-03-12  0   9  999   11\n2018-03-13  0  13   14  999\n2018-03-14  0  17   18   19\n2018-03-15  0  21   22   23\n&#39;&#39;&#39;</code></pre></div><p>根据条件设置</p><div class=\"highlight\"><pre><code class=\"language-text\">df[df.A&gt;0]=999#将df.A大于0的值改变\nprint(df)\n&#39;&#39;&#39;\n              A   B    C    D\n2018-03-10    0   1    2    3\n2018-03-11  999   5    6    7\n2018-03-12  999   9  999   11\n2018-03-13  999  13   14  999\n2018-03-14  999  17   18   19\n2018-03-15  999  21   22   23\n &#39;&#39;&#39;</code></pre></div><p>根据行或列设置</p><div class=\"highlight\"><pre><code class=\"language-text\">df[&#39;F&#39;]=np.nan\nprint(df)\n&#39;&#39;&#39;\n              A   B    C   D\n2018-03-10    0   1    2 NaN\n2018-03-11  999   5    6 NaN\n2018-03-12  999   9  999 NaN\n2018-03-13  999  13   14 NaN\n2018-03-14  999  17   18 NaN\n2018-03-15  999  21   22 NaN\n &#39;&#39;&#39;</code></pre></div><p>添加数据</p><div class=\"highlight\"><pre><code class=\"language-text\">df[&#39;E&#39;]  = pd.Series([1,2,3,4,5,6], index=pd.date_range(&#39;20180313&#39;, periods=6))#增加一列\nprint(df)\n&#39;&#39;&#39;\n              A   B    C   D    E\n2018-03-10    0   1    2 NaN  NaN\n2018-03-11  999   5    6 NaN  NaN\n2018-03-12  999   9  999 NaN  NaN\n2018-03-13  999  13   14 NaN  1.0\n2018-03-14  999  17   18 NaN  2.0\n2018-03-15  999  21   22 NaN  3.0\n&#39;&#39;&#39;</code></pre></div><h2>7.Pandas处理丢失数据</h2><p>处理数据中NaN数据</p><div class=\"highlight\"><pre><code class=\"language-text\">dates = pd.date_range(&#39;20180310&#39;, periods=6)\ndf = pd.DataFrame(np.arange(24).reshape((6,4)), index=dates, columns=[&#39;A&#39;, &#39;B&#39;, &#39;C&#39;, &#39;D&#39;])\ndf.iloc[0,1]=np.nan\ndf.iloc[1,2]=np.nan\nprint(df)\n&#39;&#39;&#39;\n             A     B     C   D\n2018-03-10   0   NaN   2.0   3\n2018-03-11   4   5.0   NaN   7\n2018-03-12   8   9.0  10.0  11\n2018-03-13  12  13.0  14.0  15\n2018-03-14  16  17.0  18.0  19\n2018-03-15  20  21.0  22.0  23\n&#39;&#39;&#39;</code></pre></div><p>使用dropna（）函数去掉NaN的行或列</p><div class=\"highlight\"><pre><code class=\"language-text\">print(df.dropna(axis=0,how=&#39;any&#39;#))#0对行进行操作 1对列进行操作 any:只要存在NaN即可drop掉 all:必须全部是NaN才可drop\n&#39;&#39;&#39;\n             A     B     C   D\n2018-03-12   8   9.0  10.0  11\n2018-03-13  12  13.0  14.0  15\n2018-03-14  16  17.0  18.0  19\n2018-03-15  20  21.0  22.0  23\n &#39;&#39;&#39;</code></pre></div><p>使用fillna（）函数替换NaN值 </p><div class=\"highlight\"><pre><code class=\"language-text\">print(df.fillna(value=0))#将NaN值替换为0\n&#39;&#39;&#39;\n             A     B     C   D\n2018-03-10   0   0.0   2.0   3\n2018-03-11   4   5.0   0.0   7\n2018-03-12   8   9.0  10.0  11\n2018-03-13  12  13.0  14.0  15\n2018-03-14  16  17.0  18.0  19\n2018-03-15  20  21.0  22.0  23\n &#39;&#39;&#39;</code></pre></div><p>使用isnull()函数判断数据是否丢失</p><div class=\"highlight\"><pre><code class=\"language-text\">print(pd.isnull(df))#矩阵用布尔来进行表示 是nan为ture 不是nan为false\n&#39;&#39;&#39;\n                A      B      C      D\n2018-03-10  False   True  False  False\n2018-03-11  False  False   True  False\n2018-03-12  False  False  False  False\n2018-03-13  False  False  False  False\n2018-03-14  False  False  False  False\n2018-03-15  False  False  False  False\n &#39;&#39;&#39;\nprint(np.any(df.isnull()))#判断数据中是否会存在NaN值\n#True</code></pre></div><h2>8.Pandas导入导出</h2><p>pandas可以读取与存取像csv、excel、json、html、pickle等格式的资料，详细说明请看<a href=\"https://link.zhihu.com/?target=http%3A//pandas.pydata.org/pandas-docs/stable/io.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">官方资料</a></p><div class=\"highlight\"><pre><code class=\"language-text\">data=pd.read_csv(&#39;test1.csv&#39;)#读取csv文件\ndata.to_pickle(&#39;test2.pickle&#39;)#将资料存取成pickle文件 \n#其他文件导入导出方式相同</code></pre></div><h2>9.Pandas合并数据</h2><p>axis合并方向</p><div class=\"highlight\"><pre><code class=\"language-text\">df1 = pd.DataFrame(np.ones((3,4))*0, columns=[&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;])\ndf2 = pd.DataFrame(np.ones((3,4))*1, columns=[&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;])\ndf3 = pd.DataFrame(np.ones((3,4))*2, columns=[&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;])\nres = pd.concat([df1, df2, df3], axis=0, ignore_index=True)#0表示竖项合并 1表示横项合并 ingnore_index重置序列index index变为0 1 2 3 4 5 6 7 8\nprint(res)\n&#39;&#39;&#39;\n     a    b    c    d\n0  0.0  0.0  0.0  0.0\n1  0.0  0.0  0.0  0.0\n2  0.0  0.0  0.0  0.0\n3  1.0  1.0  1.0  1.0\n4  1.0  1.0  1.0  1.0\n5  1.0  1.0  1.0  1.0\n6  2.0  2.0  2.0  2.0\n7  2.0  2.0  2.0  2.0\n8  2.0  2.0  2.0  2.0\n &#39;&#39;&#39;</code></pre></div><p>join合并方式</p><div class=\"highlight\"><pre><code class=\"language-text\">df1 = pd.DataFrame(np.ones((3,4))*0, columns=[&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;], index=[1,2,3])\ndf2 = pd.DataFrame(np.ones((3,4))*1, columns=[&#39;b&#39;,&#39;c&#39;,&#39;d&#39;, &#39;e&#39;], index=[2,3,4])\nprint(df1)\n&#39;&#39;&#39;\n     a    b    c    d\n1  0.0  0.0  0.0  0.0\n2  0.0  0.0  0.0  0.0\n3  0.0  0.0  0.0  0.0\n &#39;&#39;&#39;\nprint(df2)\n&#39;&#39;&#39;\n     b    c    d    e\n2  1.0  1.0  1.0  1.0\n3  1.0  1.0  1.0  1.0\n4  1.0  1.0  1.0  1.0\n &#39;&#39;&#39;\nres=pd.concat([df1,df2],axis=1,join=&#39;outer&#39;)#行往外进行合并\nprint(res)\n&#39;&#39;&#39;\n     a    b    c    d    b    c    d    e\n1  0.0  0.0  0.0  0.0  NaN  NaN  NaN  NaN\n2  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0\n3  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0\n4  NaN  NaN  NaN  NaN  1.0  1.0  1.0  1.0\n &#39;&#39;&#39;\n\nres=pd.concat([df1,df2],axis=1,join=&#39;outer&#39;)#行相同的进行合并\nprint(res)\n&#39;&#39;&#39;\n     a    b    c    d    b    c    d    e\n2  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0\n3  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0\n&#39;&#39;&#39;\n\nres=pd.concat([df1,df2],axis=1,join_axes=[df1.index])#以df1的序列进行合并 df2中没有的序列NaN值填充\nprint(res)\n&#39;&#39;&#39;\n     a    b    c    d    b    c    d    e\n1  0.0  0.0  0.0  0.0  NaN  NaN  NaN  NaN\n2  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0\n3  0.0  0.0  0.0  0.0  1.0  1.0  1.0  1.0\n&#39;&#39;&#39;</code></pre></div><p>append添加数据</p><div class=\"highlight\"><pre><code class=\"language-text\">df1 = pd.DataFrame(np.ones((3,4))*0, columns=[&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;])\ndf2 = pd.DataFrame(np.ones((3,4))*1, columns=[&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;])\ndf3 = pd.DataFrame(np.ones((3,4))*1, columns=[&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;])\ns1 = pd.Series([1,2,3,4], index=[&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;])\n\nres=df1.append(df2,ignore_index=True)#将df2合并到df1的下面 并重置index\nprint(res)\n&#39;&#39;&#39;\n     a    b    c    d\n0  0.0  0.0  0.0  0.0\n1  0.0  0.0  0.0  0.0\n2  0.0  0.0  0.0  0.0\n3  1.0  1.0  1.0  1.0\n4  1.0  1.0  1.0  1.0\n5  1.0  1.0  1.0  1.0\n&#39;&#39;&#39;\n\nres=df1.append(s1,ignore_index=True)#将s1合并到df1下面 并重置index\nprint(res)\n&#39;&#39;&#39;\n     a    b    c    d\n0  0.0  0.0  0.0  0.0\n1  0.0  0.0  0.0  0.0\n2  0.0  0.0  0.0  0.0\n3  1.0  2.0  3.0  4.0\n&#39;&#39;&#39;</code></pre></div><h2>10.Pandas合并merge</h2><p>依据一组key合并</p><div class=\"highlight\"><pre><code class=\"language-text\">left = pd.DataFrame({&#39;key&#39;: [&#39;K0&#39;, &#39;K1&#39;, &#39;K2&#39;, &#39;K3&#39;],\n                     &#39;A&#39;: [&#39;A0&#39;, &#39;A1&#39;, &#39;A2&#39;, &#39;A3&#39;],\n                     &#39;B&#39;: [&#39;B0&#39;, &#39;B1&#39;, &#39;B2&#39;, &#39;B3&#39;]})\nprint(left)\n&#39;&#39;&#39;\n    A   B key\n0  A0  B0  K0\n1  A1  B1  K1\n2  A2  B2  K2\n3  A3  B3  K3\n&#39;&#39;&#39;\nright = pd.DataFrame({&#39;key&#39;: [&#39;K0&#39;, &#39;K1&#39;, &#39;K2&#39;, &#39;K3&#39;],\n                      &#39;C&#39;: [&#39;C0&#39;, &#39;C1&#39;, &#39;C2&#39;,  &#39;C3&#39;],\n                      &#39;D&#39;: [&#39;D0&#39;, &#39;D1&#39;, &#39;D2&#39;, &#39;D3&#39;]})\nprint(right)\n&#39;&#39;&#39;\n    C   D key\n0  C0  D0  K0\n1  C1  D1  K1\n2  C2  D2  K2\n3  C3  D3  K3\n&#39;&#39;&#39;\nres=pd.merge(left,right,on=&#39;key&#39;)\nprint(res)\n&#39;&#39;&#39;\n    A   B key   C   D\n0  A0  B0  K0  C0  D0\n1  A1  B1  K1  C1  D1\n2  A2  B2  K2  C2  D2\n3  A3  B3  K3  C3  D3\n&#39;&#39;&#39;</code></pre></div><p>依据两组key合并</p><div class=\"highlight\"><pre><code class=\"language-text\">left = pd.DataFrame({&#39;key1&#39;: [&#39;K0&#39;, &#39;K0&#39;, &#39;K1&#39;, &#39;K2&#39;],\n                             &#39;key2&#39;: [&#39;K0&#39;, &#39;K1&#39;, &#39;K0&#39;, &#39;K1&#39;],\n                             &#39;A&#39;: [&#39;A0&#39;, &#39;A1&#39;, &#39;A2&#39;, &#39;A3&#39;],\n                             &#39;B&#39;: [&#39;B0&#39;, &#39;B1&#39;, &#39;B2&#39;, &#39;B3&#39;]})\nprint(left)\n&#39;&#39;&#39;\n    A   B key1 key2\n0  A0  B0   K0   K0\n1  A1  B1   K0   K1\n2  A2  B2   K1   K0\n3  A3  B3   K2   K1\n &#39;&#39;&#39;\nright = pd.DataFrame({&#39;key1&#39;: [&#39;K0&#39;, &#39;K1&#39;, &#39;K1&#39;, &#39;K2&#39;],\n                              &#39;key2&#39;: [&#39;K0&#39;, &#39;K0&#39;, &#39;K0&#39;, &#39;K0&#39;],\n                              &#39;C&#39;: [&#39;C0&#39;, &#39;C1&#39;, &#39;C2&#39;, &#39;C3&#39;],\n                              &#39;D&#39;: [&#39;D0&#39;, &#39;D1&#39;, &#39;D2&#39;, &#39;D3&#39;]})\nprint(right)\n&#39;&#39;&#39;\n    C   D key1 key2\n0  C0  D0   K0   K0\n1  C1  D1   K1   K0\n2  C2  D2   K1   K0\n3  C3  D3   K2   K0\n &#39;&#39;&#39;\n\nres=pd.merge(left,right,on=[&#39;key1&#39;,&#39;key2&#39;],how=&#39;inner&#39;)#内联合并\nprint(res)\n&#39;&#39;&#39;\n    A   B key1 key2   C   D\n0  A0  B0   K0   K0  C0  D0\n1  A2  B2   K1   K0  C1  D1\n2  A2  B2   K1   K0  C2  D2\n&#39;&#39;&#39;\n\nres=pd.merge(left,right,on=[&#39;key1&#39;,&#39;key2&#39;],how=&#39;outer&#39;)#外联合并\nprint(res)\n&#39;&#39;&#39;\n     A    B key1 key2    C    D\n0   A0   B0   K0   K0   C0   D0\n1   A1   B1   K0   K1  NaN  NaN\n2   A2   B2   K1   K0   C1   D1\n3   A2   B2   K1   K0   C2   D2\n4   A3   B3   K2   K1  NaN  NaN\n5  NaN  NaN   K2   K0   C3   D3\n&#39;&#39;&#39;\n\nres=pd.merge(left,right,on=[&#39;key1&#39;,&#39;key2&#39;],how=&#39;left&#39;)#左联合并\n&#39;&#39;&#39;\n    A   B key1 key2    C    D\n0  A0  B0   K0   K0   C0   D0\n1  A1  B1   K0   K1  NaN  NaN\n2  A2  B2   K1   K0   C1   D1\n3  A2  B2   K1   K0   C2   D2\n4  A3  B3   K2   K1  NaN  NaN\n&#39;&#39;&#39;\n\nres=pd.merge(left,right,on=[&#39;key1&#39;,&#39;key2&#39;],how=&#39;right&#39;)#右联合并\nprint(res)\n&#39;&#39;&#39;\n     A    B key1 key2   C   D\n0   A0   B0   K0   K0  C0  D0\n1   A2   B2   K1   K0  C1  D1\n2   A2   B2   K1   K0  C2  D2\n3  NaN  NaN   K2   K0  C3  D3\n&#39;&#39;&#39;</code></pre></div><p>Indicator合并</p><div class=\"highlight\"><pre><code class=\"language-text\">df1 = pd.DataFrame({&#39;col1&#39;:[0,1], &#39;col_left&#39;:[&#39;a&#39;,&#39;b&#39;]})\nprint(df1)\n&#39;&#39;&#39;\n   col1 col_left\n0     0        a\n1     1        b\n &#39;&#39;&#39;\ndf2 = pd.DataFrame({&#39;col1&#39;:[1,2,2],&#39;col_right&#39;:[2,2,2]})\nprint(df2)\n&#39;&#39;&#39;\n   col1  col_right\n0     1          2\n1     2          2\n2     2          2\n &#39;&#39;&#39;\n\nres=pd.merge(df1,df2,on=&#39;col1&#39;,how=&#39;outer&#39;,indicator=True)#依据col1进行合并 并启用indicator=True输出每项合并方式\nprint(res)\n&#39;&#39;&#39;\n   col1 col_left  col_right      _merge\n0     0        a        NaN   left_only\n1     1        b        2.0        both\n2     2      NaN        2.0  right_only\n3     2      NaN        2.0  right_only\n&#39;&#39;&#39;\n\nres = pd.merge(df1, df2, on=&#39;col1&#39;, how=&#39;outer&#39;, indicator=&#39;indicator_column&#39;)#自定义indicator column名称\nprint(res)\n&#39;&#39;&#39;\n   col1 col_left  col_right indicator_column\n0     0        a        NaN        left_only\n1     1        b        2.0             both\n2     2      NaN        2.0       right_only\n3     2      NaN        2.0       right_only\n&#39;&#39;&#39;</code></pre></div><p>依据index合并</p><div class=\"highlight\"><pre><code class=\"language-text\">left = pd.DataFrame({&#39;A&#39;: [&#39;A0&#39;, &#39;A1&#39;, &#39;A2&#39;],\n                                  &#39;B&#39;: [&#39;B0&#39;, &#39;B1&#39;, &#39;B2&#39;]},\n                                  index=[&#39;K0&#39;, &#39;K1&#39;, &#39;K2&#39;])\nprint(left)\n&#39;&#39;&#39;\n     A   B\nK0  A0  B0\nK1  A1  B1\nK2  A2  B2\n &#39;&#39;&#39;\nright = pd.DataFrame({&#39;C&#39;: [&#39;C0&#39;, &#39;C2&#39;, &#39;C3&#39;],\n                                     &#39;D&#39;: [&#39;D0&#39;, &#39;D2&#39;, &#39;D3&#39;]},\n                                      index=[&#39;K0&#39;, &#39;K2&#39;, &#39;K3&#39;])\nprint(right)\n&#39;&#39;&#39;\n     C   D\nK0  C0  D0\nK2  C2  D2\nK3  C3  D3\n&#39;&#39;&#39;\n\nres=pd.merge(left,right,left_index=True,right_index=True,how=&#39;outer&#39;)#根据index索引进行合并 并选择外联合并\nprint(res)\n&#39;&#39;&#39;\n      A    B    C    D\nK0   A0   B0   C0   D0\nK1   A1   B1  NaN  NaN\nK2   A2   B2   C2   D2\nK3  NaN  NaN   C3   D3\n&#39;&#39;&#39;\n\nres=pd.merge(left,right,left_index=True,right_index=True,how=&#39;inner&#39;)\nprint(res)\n&#39;&#39;&#39;\n     A   B   C   D\nK0  A0  B0  C0  D0\nK2  A2  B2  C2  D2\n&#39;&#39;&#39;</code></pre></div><hr/><p>更多内容请关注公众号&#39;谓之小一&#39;，若有疑问可在公众号后台提问，随时回答，内容转载请注明出处。「谓之小一」希望提供给读者别处看不到的内容，关于互联网、机器学习、数据挖掘、编程、书籍、生活...<br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p><a href=\"https://link.zhihu.com/?target=http%3A//weixin.qq.com/r/Ty_4oDPE2W6mrXfD93pd\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">weixin.qq.com/r/Ty_4oDP</span><span class=\"invisible\">E2W6mrXfD93pd</span><span class=\"ellipsis\"></span></a> (二维码自动识别)</p><p></p><p></p>", 
            "topic": [
                {
                    "tag": "Python 库", 
                    "tagLink": "https://api.zhihu.com/topics/19644560"
                }, 
                {
                    "tag": "数据挖掘", 
                    "tagLink": "https://api.zhihu.com/topics/19553534"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/34429973", 
            "userName": "谓之小一", 
            "userLink": "https://www.zhihu.com/people/761dda159c5cec5fd926f761ab5f4ccf", 
            "upvote": 0, 
            "title": "面向知乎的个性化推荐模型研究论文", 
            "content": "<h2>面向知乎的个性化推荐模型研究 </h2><p>《<a href=\"https://link.zhihu.com/?target=https%3A//github.com/XiaoYiii/Paper/blob/master/Paper/%25E9%259D%25A2%25E5%2590%2591%25E7%259F%25A5%25E4%25B9%258E%25E7%259A%2584%25E4%25B8%25AA%25E6%2580%25A7%25E5%258C%2596%25E6%258E%25A8%25E8%258D%2590%25E6%25A8%25A1%25E5%259E%258B%25E7%25A0%2594%25E7%25A9%25B6.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">面向知乎的个性化推荐模型研究</a>》论文是大二暑假完成的，已投到《计算机应用与软件》中文核心期刊。论文主要对知乎提出一种基于混合算法的个性化推荐模型。论文基于用户模型、问题模型、推荐模型构建推荐系统，提出Person Rank、Problem Rank，并结合其他算法来优化推荐结果，利用数据训练推荐模型，最终得到面向知乎的个性化推荐模型。<br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-2474f12f805cc0c2ff53b1de2b3b84ef_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2480\" data-rawheight=\"3509\" class=\"origin_image zh-lightbox-thumb\" width=\"2480\" data-original=\"https://pic4.zhimg.com/v2-2474f12f805cc0c2ff53b1de2b3b84ef_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2480&#39; height=&#39;3509&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2480\" data-rawheight=\"3509\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2480\" data-original=\"https://pic4.zhimg.com/v2-2474f12f805cc0c2ff53b1de2b3b84ef_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-2474f12f805cc0c2ff53b1de2b3b84ef_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-6be51402dee29603318fa666bed8bcf6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2480\" data-rawheight=\"3509\" class=\"origin_image zh-lightbox-thumb\" width=\"2480\" data-original=\"https://pic3.zhimg.com/v2-6be51402dee29603318fa666bed8bcf6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2480&#39; height=&#39;3509&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2480\" data-rawheight=\"3509\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2480\" data-original=\"https://pic3.zhimg.com/v2-6be51402dee29603318fa666bed8bcf6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-6be51402dee29603318fa666bed8bcf6_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-a30bfbab9948aec7600f7c53254c41cd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2480\" data-rawheight=\"3509\" class=\"origin_image zh-lightbox-thumb\" width=\"2480\" data-original=\"https://pic2.zhimg.com/v2-a30bfbab9948aec7600f7c53254c41cd_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2480&#39; height=&#39;3509&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2480\" data-rawheight=\"3509\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2480\" data-original=\"https://pic2.zhimg.com/v2-a30bfbab9948aec7600f7c53254c41cd_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-a30bfbab9948aec7600f7c53254c41cd_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-6a99b8c4d230653b38d5bd2ee2065b3d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2480\" data-rawheight=\"3509\" class=\"origin_image zh-lightbox-thumb\" width=\"2480\" data-original=\"https://pic2.zhimg.com/v2-6a99b8c4d230653b38d5bd2ee2065b3d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2480&#39; height=&#39;3509&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2480\" data-rawheight=\"3509\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2480\" data-original=\"https://pic2.zhimg.com/v2-6a99b8c4d230653b38d5bd2ee2065b3d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-6a99b8c4d230653b38d5bd2ee2065b3d_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-92ab4308198a809ff5f859bce7ff8d22_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2480\" data-rawheight=\"3509\" class=\"origin_image zh-lightbox-thumb\" width=\"2480\" data-original=\"https://pic3.zhimg.com/v2-92ab4308198a809ff5f859bce7ff8d22_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2480&#39; height=&#39;3509&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2480\" data-rawheight=\"3509\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2480\" data-original=\"https://pic3.zhimg.com/v2-92ab4308198a809ff5f859bce7ff8d22_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-92ab4308198a809ff5f859bce7ff8d22_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p>更多内容请关注公众号&#39;谓之小一&#39;，若有疑问可在公众号后台提问，随时回答，内容转载请注明出处。「谓之小一」希望提供给读者别处看不到的内容，关于互联网、机器学习、数据挖掘、编程、书籍、生活...<br/></p><p><a href=\"https://link.zhihu.com/?target=http%3A//weixin.qq.com/r/Ty_4oDPE2W6mrXfD93pd\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">weixin.qq.com/r/Ty_4oDP</span><span class=\"invisible\">E2W6mrXfD93pd</span><span class=\"ellipsis\"></span></a> (二维码自动识别)</p><p></p><p></p>", 
            "topic": [
                {
                    "tag": "个性化推荐", 
                    "tagLink": "https://api.zhihu.com/topics/19569242"
                }, 
                {
                    "tag": "数据挖掘", 
                    "tagLink": "https://api.zhihu.com/topics/19553534"
                }
            ], 
            "comments": []
        }
    ], 
    "url": "https://zhuanlan.zhihu.com/weizhixiaoyi"
}
