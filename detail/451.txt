{
    "title": "有三AI学院-深度学习框架", 
    "description": "", 
    "followers": [
        "https://www.zhihu.com/people/dreamer-1-81", 
        "https://www.zhihu.com/people/liu-si-zhe-40", 
        "https://www.zhihu.com/people/wang-a-fei-37", 
        "https://www.zhihu.com/people/cheng-shuai-25-51", 
        "https://www.zhihu.com/people/xqf-virtual", 
        "https://www.zhihu.com/people/wei-meng-41-71", 
        "https://www.zhihu.com/people/lu-xi-an-92-26", 
        "https://www.zhihu.com/people/zhangshulin-48", 
        "https://www.zhihu.com/people/zhihunb", 
        "https://www.zhihu.com/people/xiao-ma-jia-55-39", 
        "https://www.zhihu.com/people/xing-meng-qing-bei", 
        "https://www.zhihu.com/people/qing-feng-6-69", 
        "https://www.zhihu.com/people/chen-mei-ling-96", 
        "https://www.zhihu.com/people/liu-kai-86-37", 
        "https://www.zhihu.com/people/darrenzhang-44", 
        "https://www.zhihu.com/people/wuchannn", 
        "https://www.zhihu.com/people/xie-lu-60-97", 
        "https://www.zhihu.com/people/18910265573", 
        "https://www.zhihu.com/people/WayneZeng", 
        "https://www.zhihu.com/people/shi-dian-7-68", 
        "https://www.zhihu.com/people/xiaomizhou94", 
        "https://www.zhihu.com/people/chai-bin-38", 
        "https://www.zhihu.com/people/ran-da-di-de-zhong-shi-xin-tu", 
        "https://www.zhihu.com/people/tianyige", 
        "https://www.zhihu.com/people/wuzhong-42", 
        "https://www.zhihu.com/people/yi-nian-shi-er-yue", 
        "https://www.zhihu.com/people/tang-xing-wang-67", 
        "https://www.zhihu.com/people/dai-wen-65", 
        "https://www.zhihu.com/people/1994m8u2l372012", 
        "https://www.zhihu.com/people/ye-shen-xing-63", 
        "https://www.zhihu.com/people/chang-feng-16-71", 
        "https://www.zhihu.com/people/zhoupan-93", 
        "https://www.zhihu.com/people/ni-qi-ge", 
        "https://www.zhihu.com/people/alahlll", 
        "https://www.zhihu.com/people/daiyizheng123", 
        "https://www.zhihu.com/people/diao-jin-hui-19", 
        "https://www.zhihu.com/people/nan-yu-28", 
        "https://www.zhihu.com/people/kylegao-41", 
        "https://www.zhihu.com/people/yu-ren-4-38", 
        "https://www.zhihu.com/people/aegency", 
        "https://www.zhihu.com/people/li-sheng-guo-36", 
        "https://www.zhihu.com/people/taoreal", 
        "https://www.zhihu.com/people/fei-cun-37", 
        "https://www.zhihu.com/people/zhang-ming-ming-86-69", 
        "https://www.zhihu.com/people/leng-shu-ling", 
        "https://www.zhihu.com/people/li-ming-rui-66", 
        "https://www.zhihu.com/people/jiong-xian-sen-4", 
        "https://www.zhihu.com/people/yaoqingyuan", 
        "https://www.zhihu.com/people/nan-gong-han-90", 
        "https://www.zhihu.com/people/ding-ding-33-32-3", 
        "https://www.zhihu.com/people/linuxcpp", 
        "https://www.zhihu.com/people/vincent-90-24", 
        "https://www.zhihu.com/people/hu-jue-yue-48", 
        "https://www.zhihu.com/people/nan-gua-dou-zi", 
        "https://www.zhihu.com/people/tony-parker-41", 
        "https://www.zhihu.com/people/lucky-45-9-77", 
        "https://www.zhihu.com/people/the-sky-69-37", 
        "https://www.zhihu.com/people/holygao", 
        "https://www.zhihu.com/people/chen-da-xin-36", 
        "https://www.zhihu.com/people/tian-xing-36", 
        "https://www.zhihu.com/people/brillgold", 
        "https://www.zhihu.com/people/dragons-imagine", 
        "https://www.zhihu.com/people/xu-x-83", 
        "https://www.zhihu.com/people/xue-gan-82", 
        "https://www.zhihu.com/people/stone-mr-89", 
        "https://www.zhihu.com/people/leo-lee-58-57", 
        "https://www.zhihu.com/people/ye-he-hua-god", 
        "https://www.zhihu.com/people/xiao-guan-32-24", 
        "https://www.zhihu.com/people/zhang-mian-66", 
        "https://www.zhihu.com/people/li-sheng-38-23", 
        "https://www.zhihu.com/people/fenggege", 
        "https://www.zhihu.com/people/ying-ying-ying-vue", 
        "https://www.zhihu.com/people/xiao-er-lai-ge-id", 
        "https://www.zhihu.com/people/ji-kang-39-28", 
        "https://www.zhihu.com/people/xiao-xuan-99-3", 
        "https://www.zhihu.com/people/zhi-zhe-zai-cao-mang", 
        "https://www.zhihu.com/people/yildhd-wang", 
        "https://www.zhihu.com/people/tu-ran-shuai-liao-51", 
        "https://www.zhihu.com/people/wei-hua-51", 
        "https://www.zhihu.com/people/liu-xiao-yong-80-42", 
        "https://www.zhihu.com/people/xu-tian-jiao-67", 
        "https://www.zhihu.com/people/locker87", 
        "https://www.zhihu.com/people/zha-xiao-xin-3", 
        "https://www.zhihu.com/people/li-ren-29", 
        "https://www.zhihu.com/people/94na-mo-wei-xiao", 
        "https://www.zhihu.com/people/jia-wu-33", 
        "https://www.zhihu.com/people/thorne-7", 
        "https://www.zhihu.com/people/xiao-qi-14-78-49", 
        "https://www.zhihu.com/people/deng-wei-88-40", 
        "https://www.zhihu.com/people/ha-ha-8-37", 
        "https://www.zhihu.com/people/kim-74-51", 
        "https://www.zhihu.com/people/chong-chong-35-35", 
        "https://www.zhihu.com/people/wang-bao-yu-87", 
        "https://www.zhihu.com/people/remember-72-82", 
        "https://www.zhihu.com/people/jay-Happy", 
        "https://www.zhihu.com/people/she-liang", 
        "https://www.zhihu.com/people/sun-pan-90-59", 
        "https://www.zhihu.com/people/mf-wl", 
        "https://www.zhihu.com/people/yu-wen-shu-xue-18", 
        "https://www.zhihu.com/people/wang-jing-bo-27-88", 
        "https://www.zhihu.com/people/fairytale-56", 
        "https://www.zhihu.com/people/mengtan", 
        "https://www.zhihu.com/people/dong-fang-dan-82", 
        "https://www.zhihu.com/people/xiao-hai-99-78", 
        "https://www.zhihu.com/people/gushan", 
        "https://www.zhihu.com/people/li-xiang-73-34", 
        "https://www.zhihu.com/people/TAT_hanxiao", 
        "https://www.zhihu.com/people/quxiaofeng", 
        "https://www.zhihu.com/people/zhao-hu-41-13", 
        "https://www.zhihu.com/people/roger-gou", 
        "https://www.zhihu.com/people/wang-fen-fen-16", 
        "https://www.zhihu.com/people/long-gang-62-42", 
        "https://www.zhihu.com/people/chen-fu-duo-98", 
        "https://www.zhihu.com/people/wu-zhi-zhe-2-62", 
        "https://www.zhihu.com/people/zhouyuangan", 
        "https://www.zhihu.com/people/xiaojidan", 
        "https://www.zhihu.com/people/wang-peng-cheng-39-36", 
        "https://www.zhihu.com/people/gao-shu-60", 
        "https://www.zhihu.com/people/ling-hua-bao-22", 
        "https://www.zhihu.com/people/soga-zh", 
        "https://www.zhihu.com/people/peng-pai-zhong", 
        "https://www.zhihu.com/people/wang-peng-qiang-66", 
        "https://www.zhihu.com/people/sybil12"
    ], 
    "article": [
        {
            "url": "https://zhuanlan.zhihu.com/p/61095501", 
            "userName": "言有三-龙鹏", 
            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
            "upvote": 57, 
            "title": "【完结】12大深度学习开源框架(caffe,tf,pytorch,mxnet等)快速入门项目", 
            "content": "<p></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-96ad0bac56cd5d57a4f0149051a9e0e7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"608\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-96ad0bac56cd5d57a4f0149051a9e0e7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;608&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"608\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-96ad0bac56cd5d57a4f0149051a9e0e7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-96ad0bac56cd5d57a4f0149051a9e0e7_b.jpg\"/></figure><p><b>这是一篇总结文，给大家来捋清楚12大深度学习开源框架的快速入门，这是有三AI的GitHub项目，欢迎大家star/fork。</b></p><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/longpeng2008/yousan.ai\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/longpeng2008</span><span class=\"invisible\">/yousan.ai</span><span class=\"ellipsis\"></span></a></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-40b97e0ba201bd603545cdd139b34890_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"630\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-40b97e0ba201bd603545cdd139b34890_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;630&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"630\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-40b97e0ba201bd603545cdd139b34890_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-40b97e0ba201bd603545cdd139b34890_b.jpg\"/></figure><h2><b>1  概述</b></h2><h3>1.1 开源框架总览</h3><p>现如今开源生态非常完善，深度学习相关的开源框架众多，光是为人熟知的就有caffe，tensorflow，pytorch/caffe2，keras，mxnet，paddldpaddle，theano，cntk，deeplearning4j，matconvnet等。</p><p>如何选择最适合你的开源框架是一个问题。有三AI在前段时间里，给大家整理了<b>12个深度学习开源框架快速入门的教程和代码</b>，供初学者进行挑选，一个合格的深度学习算法工程师怎么着得熟悉其中的3个以上吧。</p><p>下面是各大开源框架的一个总览。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-d2af0dc0108888b3eea1140d20656883_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"608\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-d2af0dc0108888b3eea1140d20656883_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;608&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"608\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-d2af0dc0108888b3eea1140d20656883_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-d2af0dc0108888b3eea1140d20656883_b.jpg\"/></figure><p>在这里我们还有一些框架没有放上来，是因为它们已经升级为大家更喜欢或者使用起来更加简单的版本，比如从torch-&gt;pytorch，从theano到lasagne。另外这些框架都支持CUDA，因此编程语言这里也没有写上cuda。</p><p>在选择开源框架时，要考虑很多原因，比如开源生态的完善性，比如自己项目的需求，比如自己熟悉的语言。当然，现在已经有很多开源框架之间进行互转的开源工具如MMDNN等，也降低了大家迁移框架的学习成本。</p><p>除此之外还有tiny-dnn，ConvNetJS，MarVin，Neon等等小众，以及CoreML等移动端框架，就不再一一介绍。</p><p>总的来说对于选择什么样的框架，有三可以给出一些建议。</p><p><b>(1) 不管怎么说，tensorflow/pytorch你都必须会，这是目前开发者最喜欢，开源项目最丰富的两个框架。</b></p><p><b>(2) 如果你要进行移动端算法的开发，那么Caffe是不能不会的。</b></p><p><b>(3) 如果你非常熟悉Matlab，matconvnet你不应该错过。</b></p><p><b>(4) 如果你追求高效轻量，那么darknet和mxnet你不能不熟悉。</b></p><p><b>(5) 如果你很懒，想写最少的代码完成任务，那么用keras吧。</b></p><p><b>(6) 如果你是java程序员，那么掌握deeplearning4j没错的。</b></p><p><b>其他的框架，也自有它的特点，大家可以自己多去用用。</b></p><h3><b>1.2 如何学习开源框架</b></h3><p>要掌握好一个开源框架，通常需要做到以下几点：</p><p>(1) 熟练掌握不同任务数据的准备和使用。</p><p>(2) 熟练掌握模型的定义。</p><p>(3) 熟练掌握训练过程和结果的可视化。</p><p>(4) 熟练掌握训练方法和测试方法。</p><p>一个框架，官方都会开放有若干的案例，最常见的案例就是以<b>MNISI数据接口+预训练模型</b>的形式，供大家快速获得结果，但是这明显还不够，学习不应该停留在跑通官方的demo上，而是要解决实际的问题。</p><p>我们要学会从<b>自定义数据读取接口，自定义网络的搭建，模型的训练，模型的可视化，模型的测试与部署等全方位</b>进行掌握。</p><p>因此，我们开设了一个《2小时快速入门开源框架系列》，以一个<b>图像分类任务为基准</b>，带领大家一步一步入门，后续会增加分割，检测等任务。</p><p>这是一个二分类任务，给大家准备了<b>500张微笑表情</b>的图片、<b>500张无表情</b>的图片，放置在git工程的data目录下，图片预览如下，已经全部缩放到60*60的大小：</p><p>这是无表情的图片：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-c3fd061c8619d7a1f4287ac49d5d1a45_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"561\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-c3fd061c8619d7a1f4287ac49d5d1a45_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;561&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"561\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-c3fd061c8619d7a1f4287ac49d5d1a45_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-c3fd061c8619d7a1f4287ac49d5d1a45_b.jpg\"/></figure><p>这是微笑表情的图片。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-0ea360b00f4c838e49115b92845aec3b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"565\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-0ea360b00f4c838e49115b92845aec3b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;565&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"565\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-0ea360b00f4c838e49115b92845aec3b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-0ea360b00f4c838e49115b92845aec3b_b.jpg\"/></figure><p>因此，我们的目标就是利用这500张图片完成好这个图像分类任务。</p><p>在下面的所有框架的学习过程中，我们都要完成下面这个流程，只有这样，才能叫做真正的完成了一个训练任务。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-7fbe42b356a10e73678f74c843c6d9ad_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"137\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-7fbe42b356a10e73678f74c843c6d9ad_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;137&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"137\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-7fbe42b356a10e73678f74c843c6d9ad_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-7fbe42b356a10e73678f74c843c6d9ad_b.jpg\"/></figure><p>另外，所有的框架都使用同样的一个模型，这是一个3层卷积+2层全连接的网络，由卷积+BN层+激活层组成，有的使用带步长的卷积，有的使用池化，差别不大。</p><p>输入图像，48*48*3的RGB彩色图。</p><p>第一层卷积，通道数12，卷积核3*3。</p><p>第二层卷积，通道数24，卷积核3*3。</p><p>第三层卷积，通道数48，卷积核3*3。</p><p>第一层全连接，通道数128。</p><p>第二层全连接，通道数2，即类别数。</p><p>网络结构如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-2360e3d6c31d76ef98e992db4da9c9a7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"678\" data-rawheight=\"1312\" class=\"origin_image zh-lightbox-thumb\" width=\"678\" data-original=\"https://pic4.zhimg.com/v2-2360e3d6c31d76ef98e992db4da9c9a7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;678&#39; height=&#39;1312&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"678\" data-rawheight=\"1312\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"678\" data-original=\"https://pic4.zhimg.com/v2-2360e3d6c31d76ef98e992db4da9c9a7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-2360e3d6c31d76ef98e992db4da9c9a7_b.jpg\"/></figure><p>这是最简单的一种网络结构，优化的时候根据不同的框架，采用了略有不同的方案。因为此处的目标不是为了比较各个框架的性能，所以没有刻意保持完全一致。</p><h2><b>2  开源框架</b></h2><p>下面我们开始对各个框架进行简述。</p><p>2.1 Caffe</p><p>github地址：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/BVLC/caffe\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/BVLC/caffe</span><span class=\"invisible\"></span></a>。</p><p>(1) 概述：</p><p>Caffe是伯克利的贾扬清主导开发，以C++/CUDA代码为主，最早的深度学习框架之一，比TensorFlow、Mxnet、Pytorch等都更早，需要进行编译安装。支持命令行、Python和Matlab接口，单机多卡、多机多卡等都可以很方便的使用。目前master分支已经停止更新，intel分支等还在维护，caffe框架已经非常稳定。</p><p>(2)caffe的使用通常是下面的流程：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-13e4dfcd4c9394748f36dcef8faf335e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"178\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-13e4dfcd4c9394748f36dcef8faf335e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;178&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"178\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-13e4dfcd4c9394748f36dcef8faf335e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-13e4dfcd4c9394748f36dcef8faf335e_b.jpg\"/></figure><p>以上的流程相互之间是解耦合的，所以caffe的使用非常优雅简单。</p><p>(3) caffe有很明显的优点和缺点。</p><p>优点：</p><ul><li>以C++/CUDA/python代码为主，速度快，性能高。</li><li>工厂设计模式，代码结构清晰，可读性和拓展性强。</li><li>支持命令行、Python和Matlab接口，使用方便。</li><li>CPU和GPU之间切换方便，多GPU训练方便。</li><li>工具丰富，社区活跃。</li></ul><p>缺点：</p><ul><li>源代码修改门槛较高，需要实现前向反向传播，以及CUDA代码。</li><li>不支持自动求导。</li><li>不支持模型级并行，只支持数据级并行</li><li>不适合于非图像任务。</li></ul><p><b>鉴于caffe的学习有一定门槛，我给新手们提供一个自己录制的视频。</b></p><a href=\"https://link.zhihu.com/?target=https%3A//study.163.com/course/courseMain.htm%3Fshare%3D2%26shareId%3D400000000640089%26courseId%3D1006238015%26_trace_c_p_k2_%3De3f63523394c47388798148b5aff24e7\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">有三说深度学习 - 网易云课堂</a><p><b>其他框架后续也会录制，完整的系列视频在网易云上，见《有三说深度学习》。</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-226589d2d7968b533eddcf0cb405c38e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"480\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic3.zhimg.com/v2-226589d2d7968b533eddcf0cb405c38e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;640&#39; height=&#39;480&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"480\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https://pic3.zhimg.com/v2-226589d2d7968b533eddcf0cb405c38e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-226589d2d7968b533eddcf0cb405c38e_b.jpg\"/></figure><p>同时可以看下面的快速入门文档，以及阅读相关的源代码。</p><a href=\"https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029846%26idx%3D1%26sn%3D0c343cfd0ede5c8ae1405bd6348aefad%26chksm%3D871342abb064cbbd7fe31fb3c55f23875f27e48fb8354e9855823b1701f1227c71b4eb00de50%26scene%3D21%23wechat_redirect\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic4.zhimg.com/v2-98953b3f2fbb01d771291f3117ae0e5f_180x120.jpg\" data-image-width=\"732\" data-image-height=\"462\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【caffe速成】caffe图像分类从模型自定义到测试</a><p><b>2.2 Tensorflow</b></p><p>github地址：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/tensorflow/tensorflow\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/tensorflow/t</span><span class=\"invisible\">ensorflow</span><span class=\"ellipsis\"></span></a>。</p><p>(1) 概述</p><p>TensorFlow是Google brain推出的开源机器学习库，可用作各类深度学习相关的任务。</p><p>TensorFlow = Tensor + Flow，Tensor就是张量，代表N维数组，这与Caffe中的blob是类似的；Flow即流，代表基于数据流图的计算。</p><p>(2) 特点</p><p>TensorFlow最大的特点是计算图，即先定义好图，然后进行运算，所以所有的TensorFlow代码，都包含两部分：</p><ul><li>创建计算图，表示计算的数据流。它做了什么呢？实际上就是定义好了一些操作，你可以将它看做是Caffe中的prototxt的定义过程。</li><li>运行会话，执行图中的运算，可以看作是Caffe中的训练过程。只是TensorFlow的会话比Caffe灵活很多，由于是Python 接口，取中间结果分析，Debug等方便很多。</li></ul><p><b>目前tensorflow已经更新到2.0，由于精力原因，笔者的代码仍然以1.x版本为例。</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-42e8503364c67c6ecad6a0e09975c42c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"469\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-42e8503364c67c6ecad6a0e09975c42c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;469&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"469\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-42e8503364c67c6ecad6a0e09975c42c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-42e8503364c67c6ecad6a0e09975c42c_b.jpg\"/></figure><a href=\"https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029846%26idx%3D2%26sn%3D7c2582243bcd8f8b491e8e466a21978f%26chksm%3D871342abb064cbbd0cba24b408ceda2b64a7c8b6baa07f9f8f56cd4d1233caa0b80fe357753e%26scene%3D21%23wechat_redirect\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic4.zhimg.com/v2-98953b3f2fbb01d771291f3117ae0e5f_180x120.jpg\" data-image-width=\"732\" data-image-height=\"462\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【tensorflow速成】Tensorflow图像分类从模型自定义到测试</a><h3><b>2.3 Pytorch</b></h3><p>github地址：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/pytorch/pytorch\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/pytorch/pyto</span><span class=\"invisible\">rch</span><span class=\"ellipsis\"></span></a>。</p><p>(1) 概述：一句话总结Pytorch = Python + Torch。</p><p>Torch是纽约大学的一个机器学习开源框架，几年前在学术界非常流行，包括Lecun等大佬都在使用。但是由于使用的是一种绝大部分人绝对没有听过的Lua语言，导致很多人都被吓退。后来随着Python的生态越来越完善，Facebook人工智能研究院推出了Pytorch并开源。Pytorch不是简单的封装Torch 并提供Python接口，而是对Tensor以上的所有代码进行了重构，同TensorFlow一样，增加了自动求导。</p><p>后来Caffe2全部并入Pytorch，如今已经成为了非常流行的框架。很多最新的研究如风格化、GAN等大多数采用Pytorch源码。</p><p>(2) 特点</p><ul><li>动态图计算。TensorFlow从静态图发展到了动态图机制Eager Execution，pytorch则一开始就是动态图机制。动态图机制的好处就是随时随地修改，随处debug，没有类似编译的过程。</li><li>简单。相比TensorFlow1.0中Tensor、Variable、Session等概念充斥，数据读取接口频繁更新，tf.nn、tf.layers、tf.contrib各自重复，Pytorch则是从Tensor到Variable再到nn.Module，最新的Pytorch已经将Tensor和Variable合并，这分别就是从数据张量到网络的抽象层次的递进。有人调侃TensorFlow的设计是“make it complicated”，那么 Pytorch的设计就是“keep it simple”。</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-7c6bda0744df0ec2c47f846b5ca97ceb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"443\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-7c6bda0744df0ec2c47f846b5ca97ceb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;443&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"443\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-7c6bda0744df0ec2c47f846b5ca97ceb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-7c6bda0744df0ec2c47f846b5ca97ceb_b.jpg\"/></figure><a href=\"https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029881%26idx%3D1%26sn%3D3c869fcee3b48d3582952ab9a0683ea6%26chksm%3D87134284b064cb924c5e7231b3f2c36ba27e3a689b067f569f2e086f62b18413bcebc5987a07%26scene%3D21%23wechat_redirect\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic4.zhimg.com/v2-98953b3f2fbb01d771291f3117ae0e5f_180x120.jpg\" data-image-width=\"732\" data-image-height=\"462\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【pytorch速成】Pytorch图像分类从模型自定义到测试</a><h3><b>2.4 Mxnet</b></h3><p>github地址：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/apache/incubator-mxnet\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/apache/incub</span><span class=\"invisible\">ator-mxnet</span><span class=\"ellipsis\"></span></a>。</p><p>(1) 概述</p><p>Mxnet是由李沐等人领导开发的非常灵活，扩展性很强的框架，被Amazon定为官方框架。</p><p>(2) 特点</p><p>Mxnet同时拥有命令式编程和符号式编程的特点。在命令式编程上MXNet提供张量运算，进行模型的迭代训练和更新中的控制逻辑；在声明式编程中MXNet支持符号表达式，用来描述神经网络，并利用系统提供的自动求导来训练模型。Mxnet性能非常高，推荐资源不够的同学使用。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-70bd0e737cb2e65c0cb85e0f081e6226_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"480\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic3.zhimg.com/v2-70bd0e737cb2e65c0cb85e0f081e6226_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;640&#39; height=&#39;480&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"480\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https://pic3.zhimg.com/v2-70bd0e737cb2e65c0cb85e0f081e6226_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-70bd0e737cb2e65c0cb85e0f081e6226_b.jpg\"/></figure><a href=\"https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029904%26idx%3D1%26sn%3D0bdc6947f5ac68e7f68426b9d076b4ab%26chksm%3D8713436db064ca7b3b2a2a1d6a8d24c15069f72655e2c39e2498051fa0e56bbcf6fe9c332d5b%26scene%3D21%23wechat_redirect\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic4.zhimg.com/v2-98953b3f2fbb01d771291f3117ae0e5f_180x120.jpg\" data-image-width=\"732\" data-image-height=\"462\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【mxnet速成】mxnet图像分类从模型自定义到测试</a><h3><b>2.5 Keras</b></h3><p>github网址：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/keras-team/keras\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/keras-team/k</span><span class=\"invisible\">eras</span><span class=\"ellipsis\"></span></a>。</p><p>(1) 概述</p><p>Keras是一个对小白用户非常友好而简单的深度学习框架，严格来说并不是一个开源框架，而是一个高度模块化的神经网络库。</p><p>Keras在高层可以调用TensorFlow，CNTK，Theano，还有更多的库也在被陆续支持中。 Keras的特点是能够快速实现模型的搭建，是高效地进行科学研究的关键。</p><p>(2) 特点</p><ul><li>高度模块化，搭建网络非常简洁。</li><li>API很简单，具有统一的风格。</li><li>容易扩展，只需使用python添加新类和函数。</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ee0c0ec9c1675fc8dda4b55773da5d54_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"760\" data-rawheight=\"568\" class=\"origin_image zh-lightbox-thumb\" width=\"760\" data-original=\"https://pic1.zhimg.com/v2-ee0c0ec9c1675fc8dda4b55773da5d54_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;760&#39; height=&#39;568&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"760\" data-rawheight=\"568\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"760\" data-original=\"https://pic1.zhimg.com/v2-ee0c0ec9c1675fc8dda4b55773da5d54_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-ee0c0ec9c1675fc8dda4b55773da5d54_b.jpg\"/></figure><a href=\"https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029896%26idx%3D1%26sn%3Df3f7b9cf69c514f45d1d14205f879270%26chksm%3D87134375b064ca6323354c40f3e55b02ff0d1d24f3dacfc980190d51f20ec9ec16e60c1a4741%26scene%3D21%23wechat_redirect\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-2d9ceed78978badf52f685b50ced44c6_ipico.jpg\" data-image-width=\"358\" data-image-height=\"358\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【Keras速成】Keras图像分类从模型自定义到测试</a><h3><b>2.6 Paddlepaddle</b></h3><p>github网址：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/PaddlePaddle/Paddle\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/PaddlePaddle</span><span class=\"invisible\">/Paddle</span><span class=\"ellipsis\"></span></a>。</p><p>(1) 概述</p><p>正所谓Google有Tensorflow，Facebook有Pytorch，Amazon有Mxnet，作为国内机器学习的先驱，百度也有PaddlePaddle，其中Paddle即Parallel Distributed Deep Learning(并行分布式深度学习)。</p><p>(2) 特点</p><p>paddlepaddle的性能也很不错，整体使用起来与tensorflow非常类似，拥有中文帮助文档，在百度内部也被用于推荐等任务。另外，配套了一个可视化框架visualdl，与tensorboard也有异曲同工之妙。国产框架不多，大家多支持啊！</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-099cbd801e1122ffdea2ebc2a54772ea_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"774\" data-rawheight=\"219\" class=\"origin_image zh-lightbox-thumb\" width=\"774\" data-original=\"https://pic3.zhimg.com/v2-099cbd801e1122ffdea2ebc2a54772ea_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;774&#39; height=&#39;219&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"774\" data-rawheight=\"219\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"774\" data-original=\"https://pic3.zhimg.com/v2-099cbd801e1122ffdea2ebc2a54772ea_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-099cbd801e1122ffdea2ebc2a54772ea_b.jpg\"/></figure><a href=\"https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029887%26idx%3D1%26sn%3D645b97809c24922352a0b39f19c9ef0c%26chksm%3D87134282b064cb9441af68124d205d9c7dedcaeb09788f4d586b949584e556eddd3a72217f69%26scene%3D21%23wechat_redirect\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic4.zhimg.com/v2-98953b3f2fbb01d771291f3117ae0e5f_180x120.jpg\" data-image-width=\"732\" data-image-height=\"462\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【paddlepaddle速成】paddlepaddle图像分类从模型自定义到测试</a><h3><b>2.7 CNTK</b></h3><p>github地址：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Microsoft/CNTK\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/Microsoft/CN</span><span class=\"invisible\">TK</span><span class=\"ellipsis\"></span></a>。</p><p>(1) 概述</p><p>CNTK是微软开源的深度学习工具包，它通过有向图将神经网络描述为一系列计算步骤。在有向图中，叶节点表示输入值或网络参数，而其他节点表示其输入上的矩阵运算。 </p><p>CNTK允许用户非常轻松地实现和组合流行的模型，包括前馈DNN，卷积网络（CNN）和循环网络（RNN / LSTM）。与目前大部分框架一样，实现了自动求导，利用随机梯度下降方法进行优化。</p><p>(2)特点</p><ul><li>CNTK性能较高，按照其官方的说法，比其他的开源框架性能都更高。</li><li>适合做语音，CNTK本就是微软语音团队开源的，自然是更合适做语音任务，使用RNN等模型，以及在时空尺度分别进行卷积非常容易。</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-7770a68cfc508c9eac23f4c2557de3ff_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"258\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic4.zhimg.com/v2-7770a68cfc508c9eac23f4c2557de3ff_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;640&#39; height=&#39;258&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"258\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https://pic4.zhimg.com/v2-7770a68cfc508c9eac23f4c2557de3ff_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-7770a68cfc508c9eac23f4c2557de3ff_b.jpg\"/></figure><a href=\"https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649030976%26idx%3D1%26sn%3D0befc170a93d365b780c5f05b2f510a4%26chksm%3D8712bf3db065362b0aeaae82bdbac467be5697b34cac2797e2ee15cdcb97474c08640482bf07%26scene%3D21%23wechat_redirect\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic4.zhimg.com/v2-98953b3f2fbb01d771291f3117ae0e5f_180x120.jpg\" data-image-width=\"732\" data-image-height=\"462\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【cntk速成】cntk图像分类从模型自定义到测试</a><h3>2.8 Matconvnet</h3><p>github地址：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/vlfeat/matconvnet\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/vlfeat/matco</span><span class=\"invisible\">nvnet</span><span class=\"ellipsis\"></span></a>。</p><p>(1) 概述</p><p>不同于各类深度学习框架广泛使用的语言Python，MatConvnet是用matlab作为接口语言的开源深度学习库，底层语言是cuda。</p><p>(2) 特点</p><p>因为是在matlab下面，所以debug的过程非常的方便，而且本身就有很多的研究者一直都使用matlab语言，所以其实该语言的群体非常大。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-c1a886039b02c13cd069b28e312e738a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"328\" data-rawheight=\"450\" class=\"content_image\" width=\"328\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;328&#39; height=&#39;450&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"328\" data-rawheight=\"450\" class=\"content_image lazy\" width=\"328\" data-actualsrc=\"https://pic3.zhimg.com/v2-c1a886039b02c13cd069b28e312e738a_b.jpg\"/></figure><a href=\"https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649032109%26idx%3D2%26sn%3Da6ff48ec0ae5d8e7a494df7e564d9ac9%26chksm%3D8712bbd0b06532c61d98c786ba1773c29c3dcf1291f9c3cd052c5643e24699eef28481e94b8e%26scene%3D21%23wechat_redirect\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic4.zhimg.com/v2-98953b3f2fbb01d771291f3117ae0e5f_180x120.jpg\" data-image-width=\"732\" data-image-height=\"462\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【MatConvnet速成】MatConvnet图像分类从模型自定义到测试</a><h3>2.9 Deeplearning4j</h3><p>github地址：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/deeplearning4j/deeplearning4j\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/deeplearning</span><span class=\"invisible\">4j/deeplearning4j</span><span class=\"ellipsis\"></span></a>。</p><p>(1) 概述</p><p>不同于深度学习广泛应用的语言Python，DL4J是为java和jvm编写的开源深度学习库，支持各种深度学习模型。</p><p>(2)特点</p><p>DL4J最重要的特点是支持分布式，可以在Spark和Hadoop上运行，支持分布式CPU和GPU运行。DL4J是为商业环境，而非研究所设计的，因此更加贴近某些生产环境。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-5d9a142e187344667d9914441f9f4007_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"572\" data-rawheight=\"434\" class=\"origin_image zh-lightbox-thumb\" width=\"572\" data-original=\"https://pic4.zhimg.com/v2-5d9a142e187344667d9914441f9f4007_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;572&#39; height=&#39;434&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"572\" data-rawheight=\"434\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"572\" data-original=\"https://pic4.zhimg.com/v2-5d9a142e187344667d9914441f9f4007_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-5d9a142e187344667d9914441f9f4007_b.jpg\"/></figure><a href=\"https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649032012%26idx%3D1%26sn%3Df74c7084621f367adb2518ebec61ca42%26chksm%3D8712bb31b06532270b9ca9550ab48ff78adaad7d38c73645cfb8888218b8e177bebd5d401aa9%26scene%3D21%23wechat_redirect\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-2d9ceed78978badf52f685b50ced44c6_ipico.jpg\" data-image-width=\"358\" data-image-height=\"358\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【DL4J速成】Deeplearning4j图像分类从模型自定义到测试</a><p><br/>2.10 Chainer</p><p>github地址：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/chainer/chainer\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/chainer/chai</span><span class=\"invisible\">ner</span><span class=\"ellipsis\"></span></a>。</p><p>(1) 概述</p><p>chainer也是一个基于python的深度学习框架，能够轻松直观地编写复杂的神经网络架构，在日本企业中应用广泛。</p><p>(2) 特点</p><p>chainer采用“Define-by-Run”方案，即通过实际的前向计算动态定义网络。更确切地说，chainer存储计算历史而不是编程逻辑，pytorch的动态图机制思想主要就来源于chainer。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-3e61edea3bb5d9236550a32e0c272933_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"798\" data-rawheight=\"436\" class=\"origin_image zh-lightbox-thumb\" width=\"798\" data-original=\"https://pic4.zhimg.com/v2-3e61edea3bb5d9236550a32e0c272933_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;798&#39; height=&#39;436&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"798\" data-rawheight=\"436\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"798\" data-original=\"https://pic4.zhimg.com/v2-3e61edea3bb5d9236550a32e0c272933_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-3e61edea3bb5d9236550a32e0c272933_b.jpg\"/></figure><a href=\"https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649031911%26idx%3D2%26sn%3Da95856836c0d8832b9e5fe49704c6313%26chksm%3D8712ba9ab065338c28d86ff10bff58bcda404bfc0efc68f0a9c0ba20a2126d917cc701c6b4ae%26scene%3D21%23wechat_redirect\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-2d9ceed78978badf52f685b50ced44c6_ipico.jpg\" data-image-width=\"358\" data-image-height=\"358\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【chainer速成】chainer图像分类从模型自定义到测试</a><h3>2.11 Lasagne/Theano</h3><p>github地址：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Lasagne/Lasagne\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/Lasagne/Lasa</span><span class=\"invisible\">gne</span><span class=\"ellipsis\"></span></a>。</p><p>(1)概述</p><p>Lasagen其实就是封装了theano，后者是一个很老牌的框架，在2008年的时候就由Yoshua Bengio领导的蒙特利尔LISA组开源了。</p><p>(2)特点</p><p>theano的使用成本高，需要从底层开始写代码构建模型，Lasagen对其进行了封装，使得theano使用起来更简单。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-8ac8045b94ddba11faba6169dcc6832c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"480\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic1.zhimg.com/v2-8ac8045b94ddba11faba6169dcc6832c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;640&#39; height=&#39;480&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"480\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https://pic1.zhimg.com/v2-8ac8045b94ddba11faba6169dcc6832c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-8ac8045b94ddba11faba6169dcc6832c_b.jpg\"/></figure><a href=\"https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649032128%26idx%3D2%26sn%3Df889e2255c0ec4960f76ad0383363849%26chksm%3D8712bbbdb06532ab136c0a485bebc5cf181f24a34bec7bab3f97f66be627e09d939b997dae90%26scene%3D21%23wechat_redirect\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic4.zhimg.com/v2-98953b3f2fbb01d771291f3117ae0e5f_180x120.jpg\" data-image-width=\"732\" data-image-height=\"462\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【Lasagne速成】Lasagne/Theano图像分类从模型自定义到测试</a><h3><b>2.12 Darknet</b></h3><p>github地址：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/pjreddie/darknet\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/pjreddie/dar</span><span class=\"invisible\">knet</span><span class=\"ellipsis\"></span></a>。</p><p>(1) 概述</p><p>Darknet本身是Joseph Redmon为了Yolo系列开发的框架。</p><p>Joseph Redmon提出了Yolo v1，Yolo v2，Yolo v3。</p><p>(2) 特点</p><p>Darknet几乎没有依赖库，是从C和CUDA开始撰写的深度学习开源框架，支持CPU和GPU。Darknet跟caffe颇有几分相似之处，却更加轻量级，非常值得学习使用。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-26b2cba98299ea6f8bf3845c7ea51ff9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"480\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic2.zhimg.com/v2-26b2cba98299ea6f8bf3845c7ea51ff9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;640&#39; height=&#39;480&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"480\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https://pic2.zhimg.com/v2-26b2cba98299ea6f8bf3845c7ea51ff9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-26b2cba98299ea6f8bf3845c7ea51ff9_b.jpg\"/></figure><a href=\"https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649032137%26idx%3D2%26sn%3Da9770ead4a9d03f0a4e75be86657453b%26chksm%3D8712bbb4b06532a27f0f02ac30ce4a18eeac1f72ac54b3c5c1b6ec13fee29f03bfc0dcaa1efd%26scene%3D21%23wechat_redirect\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic4.zhimg.com/v2-98953b3f2fbb01d771291f3117ae0e5f_180x120.jpg\" data-image-width=\"732\" data-image-height=\"462\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【darknet速成】Darknet图像分类从模型自定义到测试</a><p> 1、今天开源的这一套代码还只包含图像分类任务，后续我们会增加其他计算机视觉任务，欢迎小伙伴们前来参与，需要力量！</p><p>2、开源框架众多，使用过程中必会出现N多问题，如果你想要更多的交流，就来有三AI知识星球吧，来日方长。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-91c77f5ae147c9a4d4f66a71d121ff56_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"690\" data-rawheight=\"374\" class=\"origin_image zh-lightbox-thumb\" width=\"690\" data-original=\"https://pic3.zhimg.com/v2-91c77f5ae147c9a4d4f66a71d121ff56_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;690&#39; height=&#39;374&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"690\" data-rawheight=\"374\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"690\" data-original=\"https://pic3.zhimg.com/v2-91c77f5ae147c9a4d4f66a71d121ff56_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-91c77f5ae147c9a4d4f66a71d121ff56_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "Caffe（深度学习框架）", 
                    "tagLink": "https://api.zhihu.com/topics/20019488"
                }, 
                {
                    "tag": "TensorFlow", 
                    "tagLink": "https://api.zhihu.com/topics/20032249"
                }, 
                {
                    "tag": "PyTorch", 
                    "tagLink": "https://api.zhihu.com/topics/20075993"
                }
            ], 
            "comments": [
                {
                    "userName": "触摸到彩虹", 
                    "userLink": "https://www.zhihu.com/people/8c4b0cce2fd5349597a13dfc567d3f12", 
                    "content": "写的很好！！！", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/60547570", 
            "userName": "言有三-龙鹏", 
            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
            "upvote": 3, 
            "title": "【Lasagne速成】Lasagne/Theano图像分类从模型自定义到测试", 
            "content": "<p>欢迎来到专栏<b>《2小时玩转开源框架系列》，这是我们第11篇，前面已经说过了caffe，tensorflow，pytorch，mxnet，keras，paddlepaddle，cntk，chainer，deeplearning4j，matconvnet。</b></p><p>今天说Lasagne，本文所用到的数据，代码请参考我们官方git </p><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/longpeng2008/LongPeng_ML_Course\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/longpeng2008</span><span class=\"invisible\">/LongPeng_ML_Course</span><span class=\"ellipsis\"></span></a></p><p>                                                                                                                           作者&amp;编辑 | 言有三</p><h2><br/><b>1 Lasagne是什么</b></h2><p>说了这么久的开源框架，我们好像一直忘了一个很老牌的框架，就是theano对不对，在2008年的时候，这个框架就由Yoshua Bengio领导的蒙特利尔LISA组开源了。</p><p>一直没说theano是因为它的使用成本真的有点高，需要从底层开始写代码构建模型，不过今天说的这个是封装了theano的高层框架，即Lasagen，它使得theano使用起来更简单。</p><div class=\"highlight\"><pre><code class=\"language-text\">官网地址：http://lasagne.readthedocs.io/en/latest/index.html\nGitHub： https://github.com/Lasagne/Lasagne</code></pre></div><h2><b>2 Lasagne训练准备</b></h2><p><b>2.1 Lasagne安装</b></p><p>Lasagne安装很简单，只需要在终端输入下面命令即可安装：</p><div class=\"highlight\"><pre><code class=\"language-text\">pip install Lasagne</code></pre></div><p><b>2.2 数据读取</b></p><p>由于没有特别好的接口，因此我们自己定义一个类就行了，实现从数据集中读取，以及产生list，格式就是每一个类存在一个单独的文件夹下，主体代码如下。</p><div class=\"highlight\"><pre><code class=\"language-text\">class Dataset:\n    def __init__(self, rootpath, imgwidth, imgheight, trainratio=0.9):\n        self.rootpath = rootpath\n        list_dirs = os.walk(self.rootpath)\n        count = 0\n        numofclasses = 0\n        self.subdirs = []\n        ##遍历文件夹\n        for root, dirs, files in list_dirs:\n            for d in dirs:\n                self.subdirs.append(os.path.join(root,d))\n        label = 0\n        self.imagedatas = []\n        self.labeldatas = []\n        for subdir in self.subdirs:\n             images = glob.iglob(os.path.join(subdir,&#39;*.jpg&#39;))\n             for image in images:  \n                  imagedata = cv2.imread(image,1)\n                  imagedata = cv2.resize(imagedata,(imgwidth,imgheight)) \n                  imagedata = imagedata.astype(np.float) / 255.0\n                  imagedata = imagedata - [0.5,0.5,0.5]\n                  imagedata = imagedata.transpose((2,0,1))\n\n        self.imagedatas.append(imagedata)\n        self.labeldatas.append(label)\n        label = label + 1\n        self.imagedatas = np.array(self.imagedatas).astype(np.float32)\n        self.labeldatas = np.array(self.labeldatas).astype(np.int32)\n        indices = np.arange(len(self.imagedatas))\n\n        np.random.shuffle(indices)\n        splitindex = int(trainratio*self.imagedatas.shape[0])\n \n        self.imagetraindatas = self.imagedatas[0:splitindex].copy()\n        self.labeltraindatas = self.labeldatas[0:splitindex].copy()\n        self.imagevaldatas = self.imagedatas[splitindex:].copy()    \n        self.labelvaldatas = self.labeldatas[splitindex:].copy()    \n\n     ##定义数据迭代接口\n    def iterate_minibatches(self, inputs, targets, batchsize, shuffle=False):\n        assert len(inputs) == len(targets)\n        if shuffle:\n            indices = np.arange(len(inputs))\n            print &#34;indices type=&#34;,type(indices)\n            np.random.shuffle(indices)\n\n        for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n            if shuffle:\n                excerpt = indices[start_idx:start_idx + batchsize]\n            else:\n                excerpt = slice(start_idx, start_idx + batchsize)\n            yield inputs[excerpt], targets[excerpt]</code></pre></div><p>以上就实现了将一个数据集下的不同子文件夹的图片随机分成了训练集和测试集，并提供了统一的接口。当然这里只做了最简单的数据预处理而没有做数据增强，这就留待读者自己去完成了。</p><p><b>2.3 网络定义</b></p><p>基本上和所有python库的方法是一样的，调用接口就行。</p><div class=\"highlight\"><pre><code class=\"language-text\">def simpleconv3(input_var=None):\n    network = lasagne.layers.InputLayer(shape=(None, 3, 48, 48),\n                                        input_var=input_var)\n\n    network = lasagne.layers.Conv2DLayer(\n            network, num_filters=12, filter_size=(3, 3),\n            nonlinearity=lasagne.nonlinearities.rectify,\n            W=lasagne.init.GlorotUniform())\n    network = batch_norm(network)\n    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n\n    network = lasagne.layers.Conv2DLayer(\n            network, num_filters=24, filter_size=(3, 3),\n            nonlinearity=lasagne.nonlinearities.rectify,\n            W=lasagne.init.GlorotUniform())\n    network = batch_norm(network)\n    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n    network = lasagne.layers.Conv2DLayer(\n            network, num_filters=48, filter_size=(3, 3),\n            nonlinearity=lasagne.nonlinearities.rectify,\n            W=lasagne.init.GlorotUniform())\n    network = batch_norm(network)\n    network = lasagne.layers.MaxPool2DLayer(network, pool_size=(2, 2))\n\n    network = lasagne.layers.DenseLayer(\n            lasagne.layers.dropout(network, p=.5),\n            num_units=128,\n            nonlinearity=lasagne.nonlinearities.rectify)\n\n    network = lasagne.layers.DenseLayer(\n            lasagne.layers.dropout(network, p=.5),\n            num_units=2,\n            nonlinearity=lasagne.nonlinearities.softmax)\n\n    return network</code></pre></div><p>以上定义的就是一个3层卷积2层全连接的网络，使用lasagne.layers接口。</p><h2><b>3 模型训练</b></h2><p>1、首先通过Theano里的tensor对输入和输出进行定义</p><div class=\"highlight\"><pre><code class=\"language-text\">input_var = T.tensor4(&#39;inputs&#39;) \ntarget_var = T.ivector(&#39;targets&#39;)</code></pre></div><p>inputs是一个四维的张量，targets是一个ivector变量。</p><p>2、调用lasagne.objectives里的损失函数接口：</p><div class=\"highlight\"><pre><code class=\"language-text\">network = simpleconv3(input_var)\nprediction = lasagne.layers.get_output(network)\nloss =lasagne.objectives.categorical_crossentropy(prediction, target_var) \nloss = loss.mean()</code></pre></div><p>network即网络模型，prediction表示它的输出，损失函数categorical_crossentropy就是交叉熵了。</p><p>验证集和测试集上的定义与此类似，只需要更改deterministic为deterministic=True，这样会屏蔽掉所有的dropout层，如下：</p><div class=\"highlight\"><pre><code class=\"language-text\">test_prediction = lasagne.layers.get_output(network, deterministic=True)\ntest_loss = lasagne.objectives.categorical_crossentropy(test_prediction, target_var)\ntest_loss = test_loss.mean()</code></pre></div><p>接下来就是训练方法，使用nesterov_momentum法：</p><div class=\"highlight\"><pre><code class=\"language-text\">params = lasagne.layers.get_all_params(network, trainable=True) \nupdates = lasagne.updates.nesterov_momentum( loss, params, learning_rate=0.01, momentum=0.9\n)</code></pre></div><p>最后定义训练函数：</p><div class=\"highlight\"><pre><code class=\"language-text\">train_fn = theano.function([input_var, target_var], loss, updates=updates)</code></pre></div><p>接收两个输入input_var, target_var，利用updates表达式更新参数。如果是用于验证和测试，就不需要进行网络参数的更新，而且可以增加精度等变量，这时这样定义：</p><div class=\"highlight\"><pre><code class=\"language-text\">test_acc = T.mean(T.eq(T.argmax(test_prediction, axis=1), target_var), dtype=theano.config.floatX)\nval_fn = theano.function([input_var, target_var], [test_loss, test_acc])</code></pre></div><p>最后，每一个epoch取得数据训练进行训练：</p><div class=\"highlight\"><pre><code class=\"language-text\">train_data =  mydataset.iterate_minibatches(mydataset.imagetraindatas,mydataset.labeltraindatas,16,True)\ntrain_batches = 0\n   for input_batch, target_batch in train_data:\n       train_loss += train_fn(input_batch, target_batch)\n       prediction = lasagne.layers.get_output(network)\n       train_batches += 1\n   print(&#34;Epoch %d: Train Loss %g&#34; % (epoch + 1, train_loss / train_batches))</code></pre></div><p>结果如下，老样子，测试集合精度90%，模型过拟合。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-8c8fa0701b5d34cc06a31b0eedb355ec_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"480\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic1.zhimg.com/v2-8c8fa0701b5d34cc06a31b0eedb355ec_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;640&#39; height=&#39;480&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"480\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https://pic1.zhimg.com/v2-8c8fa0701b5d34cc06a31b0eedb355ec_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-8c8fa0701b5d34cc06a31b0eedb355ec_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-22baf17e3fe2770d88a2e84790d8387f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"480\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic4.zhimg.com/v2-22baf17e3fe2770d88a2e84790d8387f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;640&#39; height=&#39;480&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"480\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https://pic4.zhimg.com/v2-22baf17e3fe2770d88a2e84790d8387f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-22baf17e3fe2770d88a2e84790d8387f_b.jpg\"/></figure><p>以上就是Lasagne从数据准备，模型定义到输出结果的整个流程，想要体验可以去参考git代码。</p><p>Lasagne/Theano给我最大的感觉就是慢，比至今用过的每一个框架都要慢，不过了解一下并没有坏处，毕竟Theano曾经辉煌。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-60feffab1842911ce09ae93eeaae724f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"3999\" data-rawheight=\"2250\" class=\"origin_image zh-lightbox-thumb\" width=\"3999\" data-original=\"https://pic4.zhimg.com/v2-60feffab1842911ce09ae93eeaae724f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;3999&#39; height=&#39;2250&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"3999\" data-rawheight=\"2250\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"3999\" data-original=\"https://pic4.zhimg.com/v2-60feffab1842911ce09ae93eeaae724f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-60feffab1842911ce09ae93eeaae724f_b.jpg\"/></figure><blockquote>本系列完整文章：</blockquote><p>第一篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029846%26idx%3D1%26sn%3D0c343cfd0ede5c8ae1405bd6348aefad%26chksm%3D871342abb064cbbd7fe31fb3c55f23875f27e48fb8354e9855823b1701f1227c71b4eb00de50%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【caffe速成】caffe图像分类从模型自定义到测试</a></p><p>第二篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029846%26idx%3D2%26sn%3D7c2582243bcd8f8b491e8e466a21978f%26chksm%3D871342abb064cbbd0cba24b408ceda2b64a7c8b6baa07f9f8f56cd4d1233caa0b80fe357753e%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【tensorflow速成】Tensorflow图像分类从模型自定义到测试</a></p><p>第三篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029881%26idx%3D1%26sn%3D3c869fcee3b48d3582952ab9a0683ea6%26chksm%3D87134284b064cb924c5e7231b3f2c36ba27e3a689b067f569f2e086f62b18413bcebc5987a07%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【pytorch速成】Pytorch图像分类从模型自定义到测试</a></p><p>第四篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029887%26idx%3D1%26sn%3D645b97809c24922352a0b39f19c9ef0c%26chksm%3D87134282b064cb9441af68124d205d9c7dedcaeb09788f4d586b949584e556eddd3a72217f69%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【paddlepaddle速成】paddlepaddle图像分类从模型自定义到测试</a></p><p>第五篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029896%26idx%3D1%26sn%3Df3f7b9cf69c514f45d1d14205f879270%26chksm%3D87134375b064ca6323354c40f3e55b02ff0d1d24f3dacfc980190d51f20ec9ec16e60c1a4741%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【Keras速成】Keras图像分类从模型自定义到测试</a></p><p>第六篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029904%26idx%3D1%26sn%3D0bdc6947f5ac68e7f68426b9d076b4ab%26chksm%3D8713436db064ca7b3b2a2a1d6a8d24c15069f72655e2c39e2498051fa0e56bbcf6fe9c332d5b%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【mxnet速成】mxnet图像分类从模型自定义到测试</a></p><p>第七篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649030976%26idx%3D1%26sn%3D0befc170a93d365b780c5f05b2f510a4%26chksm%3D8712bf3db065362b0aeaae82bdbac467be5697b34cac2797e2ee15cdcb97474c08640482bf07%26token%3D1695328272%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【cntk速成】cntk图像分类从模型自定义到测试</a></p><p>第八篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26tempkey%3DMTAwMF93Q3VDbklva2NBZDJXV1dnNWloZjJ2YWtZWmJ2aTRRaGVDMVNfbERSN2xVYXdXd3B4eWczWWdMbnQzRjZrRVpmSGRYMndrLTQ0VnhPcURTNGhsS3dYWGd3MTk0UGtmZkltU0U2U01OUmRfQzcySDVObDlxM3R2U3Q0SlNQWGRMc1NZeWtWTlVRN1NpRlJaWXRYcGdydUNOTUR6clUzVkNleVlKUE5nfn4%253D%26chksm%3D0712ba9b3065338de13ea030951ba2c25c0363d958c905e12c937c2acffa991313abb4d9f751%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【chainer速成】chainer图像分类从模型自定义到测试</a></p><p>第九篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649032012%26idx%3D1%26sn%3Df74c7084621f367adb2518ebec61ca42%26chksm%3D8712bb31b06532270b9ca9550ab48ff78adaad7d38c73645cfb8888218b8e177bebd5d401aa9%26token%3D1258619827%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【DL4J速成】Deeplearning4j图像分类从模型自定义到测试</a></p><p>第十篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649032109%26idx%3D2%26sn%3Da6ff48ec0ae5d8e7a494df7e564d9ac9%26chksm%3D8712bbd0b06532c61d98c786ba1773c29c3dcf1291f9c3cd052c5643e24699eef28481e94b8e%26token%3D1258619827%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【MatConvnet速成】MatConvnet图像分类从模型自定义到测试</a></p><p>第十一篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649032128%26idx%3D2%26sn%3Df889e2255c0ec4960f76ad0383363849%26chksm%3D8712bbbdb06532ab136c0a485bebc5cf181f24a34bec7bab3f97f66be627e09d939b997dae90%26token%3D598159941%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【Lasagne速成】Lasagne/Theano图像分类从模型自定义到测试</a></p><p>第十二篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649032137%26idx%3D2%26sn%3Da9770ead4a9d03f0a4e75be86657453b%26chksm%3D8712bbb4b06532a27f0f02ac30ce4a18eeac1f72ac54b3c5c1b6ec13fee29f03bfc0dcaa1efd%26token%3D1446627305%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【darknet速成】Darknet图像分类从模型自定义到测试</a></p><p></p>", 
            "topic": [
                {
                    "tag": "开源项目", 
                    "tagLink": "https://api.zhihu.com/topics/19565961"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "卷积神经网络（CNN）", 
                    "tagLink": "https://api.zhihu.com/topics/20043586"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/60607178", 
            "userName": "言有三-龙鹏", 
            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
            "upvote": 15, 
            "title": "【darknet速成】Darknet图像分类从模型自定义到测试", 
            "content": "<p>欢迎来到专栏<b>《2小时玩转开源框架系列》，这是我们第12篇文章，前面已经说过了caffe，tensorflow，pytorch，mxnet，keras，paddlepaddle，cntk，chainer，deeplearning4j，matconvnet，lasagne。</b></p><p>今天说darknet，也是最后一个框架了，本文所用到的数据，代码请参考我们官方git </p><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/longpeng2008/LongPeng_ML_Course\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/longpeng2008</span><span class=\"invisible\">/LongPeng_ML_Course</span><span class=\"ellipsis\"></span></a></p><p>                                                                                                                           作者&amp;编辑 | 言有三</p><h2><br/><b>1 Darknet是什么</b></h2><p>首先不得不夸奖一下Darknet的主页风格不错。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-2ee74131bef866faf4e9fd4f783dfed5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"582\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-2ee74131bef866faf4e9fd4f783dfed5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;582&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"582\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-2ee74131bef866faf4e9fd4f783dfed5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-2ee74131bef866faf4e9fd4f783dfed5_b.jpg\"/></figure><div class=\"highlight\"><pre><code class=\"language-text\">官网地址：https://pjreddie.com/darknet/\nGitHub： https://github.com/pjreddie/darknet</code></pre></div><p>Darknet本身是Joseph Redmon为了Yolo系列开发的框架。</p><p>Joseph Redmon，一个从look once，到look Better, Faster, Stronger，到An Incremental Improvement，也就是从Yolo v1，干到Yolo v2，Yolo v3的男人，头像很应景。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-cc82ab8fc0ce0541b64e3e333018990b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"400\" data-rawheight=\"400\" class=\"content_image\" width=\"400\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;400&#39; height=&#39;400&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"400\" data-rawheight=\"400\" class=\"content_image lazy\" width=\"400\" data-actualsrc=\"https://pic4.zhimg.com/v2-cc82ab8fc0ce0541b64e3e333018990b_b.jpg\"/></figure><p>Darknet几乎没有依赖库，是从C和CUDA开始撰写的深度学习开源框架，支持CPU和GPU。</p><p>咱们的第一个开源框架说的是Caffe，现在这最后一个Darknet跟caffe倒是颇有几分相似之处，只是更加轻量级。</p><h2><b>2 Darknet结构解读</b></h2><p>首先我们看下Darknet的代码结构如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-2a9ee498c318bb7695f46e17ee67715f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"251\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-2a9ee498c318bb7695f46e17ee67715f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;251&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"251\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-2a9ee498c318bb7695f46e17ee67715f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-2a9ee498c318bb7695f46e17ee67715f_b.jpg\"/></figure><p>cfg，data，examples，include，python，src，scripts几个子目录。</p><p><b>2.1 data目录</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-9f677057c90e34b7ade4406ef7339bf1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"694\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-9f677057c90e34b7ade4406ef7339bf1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;694&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"694\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-9f677057c90e34b7ade4406ef7339bf1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-9f677057c90e34b7ade4406ef7339bf1_b.jpg\"/></figure><p>以上就是data目录的内容，包含了各种各样的文件。图片就是测试文件了，不必说。我们首先看看imagenet.labels.list和imagenet.shortnames.list里面是什么。</p><p>imagenet.labels.list是：</p><div class=\"highlight\"><pre><code class=\"language-text\">n02120505\nn02104365\nn02086079\nn02101556\n·············</code></pre></div><p>看得出来就是imagenet的类别代号，与之对应的imagenet.shortnames.list里是：</p><div class=\"highlight\"><pre><code class=\"language-text\">kit fox    \nEnglish setter    \nSiberian husky    \nAustralian terrier     \n ·············</code></pre></div><p>可知这两个文件配套存储了imagenet1000的类别信息。</p><p>接着看9k.labels，9names，9k.trees，里面存储的就是Yolo9000论文中对应的9418个类别了。coco.names，openimages.names，voc.names都类似。</p><p><b>2.2 cfg目录</b></p><p>cfg，下面包含两类文件，一个是.data，一个是.cfg文件。我们打开imagenet1k.data文件看下，可知它配置的就是训练数据集的信息：</p><div class=\"highlight\"><pre><code class=\"language-text\">classes=1000 ##分类类别数\ntrain  = /data/imagenet/imagenet1k.train.list ##训练文件\nvalid  = /data/imagenet/imagenet1k.valid.list ##测试文件\nbackup = /home/pjreddie/backup/ ##训练结果保存文件夹\nlabels = data/imagenet.labels.list #标签\nnames  = data/imagenet.shortnames.list \ntop=5</code></pre></div><p>另一类就是.cfg文件，我们打开cifar.cfg文件查看。</p><div class=\"highlight\"><pre><code class=\"language-text\">##---------1 优化参数配置---------##\n[net]\nbatch=128 \nsubdivisions=1\nheight=28\nwidth=28\nchannels=3\nmax_crop=32\nmin_crop=32\n\n##数据增强参数\nhue=.1\nsaturation=.75\nexposure=.75\n\n##学习率策略\nlearning_rate=0.4\npolicy=poly\npower=4\n\nmax_batches = 5000 ##迭代次数\nmomentum=0.9 ##动量项\ndecay=0.0005 ##正则项\n\n##---------2 网络参数配置---------##\n[convolutional]\nbatch_normalize=1  ##是否使用batch_normalization\nfilters=128\nsize=3\nstride=1\npad=1\nactivation=leaky ##激活函数\n\n[convolutional]\nbatch_normalize=1\nfilters=128\nsize=3\nstride=1\npad=1\nactivation=leaky\n\n[convolutional]\nbatch_normalize=1\nfilters=128\nsize=3\nstride=1\npad=1\nactivation=leaky\n\n[maxpool]\nsize=2\nstride=2\n\n[dropout]\nprobability=.5\n\n[convolutional]\nbatch_normalize=1\nfilters=256\nsize=3\nstride=1\npad=1\nactivation=leaky\n\n[convolutional]\nbatch_normalize=1\nfilters=256\nsize=3\nstride=1\npad=1\nactivation=leaky\n\n[convolutional]\nbatch_normalize=1\nfilters=256\nsize=3\nstride=1\npad=1\nactivation=leaky\n\n[maxpool]\nsize=2\nstride=2\n\n[dropout]\nprobability=.5\n\n[convolutional]\nbatch_normalize=1\nfilters=512\nsize=3\nstride=1\npad=1\nactivation=leaky\n\n[convolutional]\nbatch_normalize=1\nfilters=512\nsize=3\nstride=1\npad=1\nactivation=leaky\n\n[convolutional]\nbatch_normalize=1\nfilters=512\nsize=3\nstride=1\npad=1\nactivation=leaky\n\n[dropout]\nprobability=.5\n\n[convolutional]\nfilters=10\nsize=1\nstride=1\npad=1\nactivation=leaky\n\n[avgpool]\n\n[softmax]\ngroups=1\n</code></pre></div><p>包含两部分，第一部分就是优化参数的定义，类似于caffe的solver.prototxt文件。第二部分就是网络定义，类似于caffe的train.prototxt文件，不同的是网络层用[]来声明，batch normalization以及激活函数等配置进了[convolutional]里面。</p><p>最后的avgpool不需要配置池化半径，softmax不需要配置输入输出，在最后设置group参数。</p><p>你可能好奇，那残差网络怎么弄呢？</p><div class=\"highlight\"><pre><code class=\"language-text\">[shortcut]    \nactivation=leaky    \nfrom=-3    </code></pre></div><p>如上，通过一个from=-3参数来进行配置，就是往后退3个block的意思了。</p><p><b>2.3 python目录</b></p><p>下面只有两个文件，即darknet.py和proverbot.py。前者就是python调用yolo模型的案例，后者没什么用。</p><p><b>2.4 include，src，examples目录</b></p><p>include和src就是具体的函数实现了，卷积等各类操作都在这里。examples就是高层任务的定义，包括classifier，detector，代码的解读就超过本文的内容了，以后详解。</p><h2><b>3 数据准备和模型定义</b></h2><p><b>3.1 数据准备</b></p><p>前面已经把该介绍的都介绍了，下面就开始准备数据进行训练。跟caffe一样，数据准备的流程非常简单。</p><p>首先，在data目录下建立我们自己的任务，按照如下目录，把文件准备好</p><div class=\"highlight\"><pre><code class=\"language-text\">├── genedata.sh\n├── labels.txt\n├── test\n├── test.list\n├── train\n└── train.list</code></pre></div><p>使用如下命令生成文件</p><div class=\"highlight\"><pre><code class=\"language-text\">find `pwd`/train -name \\*.jpg &gt; train.list\nfind `pwd`/test -name \\*.jpg &gt; test.list</code></pre></div><p>其中每一行都存储一个文件，而标签是通过后缀获得的。</p><div class=\"highlight\"><pre><code class=\"language-text\">/Users/longpeng/Desktop/darknet/data/mouth/train/60_smile.jpg\n/Users/longpeng/Desktop/darknet/data/mouth/train/201_smile.jpg\n/Users/longpeng/Desktop/darknet/data/mouth/train/35_neutral.jpg\n/Users/longpeng/Desktop/darknet/data/mouth/train/492_smile.jpg</code></pre></div><p>标签的内容存在labels.txt里面，如下</p><div class=\"highlight\"><pre><code class=\"language-text\">neutral\nsmile</code></pre></div><p><b>3.2 配置训练文件路径和网络</b></p><p>去cfg目录下建立文件mouth.data和mouth.cfg，mouth.data内容如下：</p><div class=\"highlight\"><pre><code class=\"language-text\">classes=2\ntrain  = data/mouth/train.list\nvalid  = data/mouth/test.list\nlabels = data/mouth/labels.txt\nbackup = mouth/\ntop=5</code></pre></div><p>mouth.cfg内容如下：</p><div class=\"highlight\"><pre><code class=\"language-text\">[net]\nbatch=16\nsubdivisions=1\nheight=48\nwidth=48\nchannels=3\nmax_crop=48\nmin_crop=48\n\nhue=.1\nsaturation=.75\nexposure=.75\n\nlearning_rate=0.01\npolicy=poly\npower=4\nmax_batches = 5000\nmomentum=0.9\ndecay=0.0005\n\n[convolutional]\nbatch_normalize=1\nfilters=12\nsize=3\nstride=1\npad=1\nactivation=leaky\n\n[maxpool]\nsize=2\nstride=2\n\n[convolutional]\nbatch_normalize=1\nfilters=24\nsize=1\nstride=1\npad=1\nactivation=leaky\n\n[maxpool]\nsize=2\nstride=2\n\n[convolutional]\nbatch_normalize=1\nfilters=48\nsize=3\nstride=1\npad=1\nactivation=leaky\n\n[maxpool]\nsize=2\nstride=2\n\n[connected]\noutput=128\nactivation=relu\n\n[connected]\noutput=2\nactivation=linear\n\n[softmax]</code></pre></div><p>在这里我们用上了一点数据增强操作，大家在后面会看到它的威力。</p><h2><b>4 模型训练</b></h2><p>使用如下命令进行训练：</p><div class=\"highlight\"><pre><code class=\"language-text\">./darknet classifier train cfg/mouth.data cfg/mouth.cfg</code></pre></div><p>训练结果如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-a37e8ac54adac8a19f0b4fafa9058f58_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"331\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-a37e8ac54adac8a19f0b4fafa9058f58_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;331&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"331\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-a37e8ac54adac8a19f0b4fafa9058f58_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-a37e8ac54adac8a19f0b4fafa9058f58_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-477d353e76a2af416704f8e9e12b43d3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"181\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-477d353e76a2af416704f8e9e12b43d3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;181&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"181\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-477d353e76a2af416704f8e9e12b43d3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-477d353e76a2af416704f8e9e12b43d3_b.jpg\"/></figure><p>上面每一行展示的分别是：batch数目，epoch数目，损失，平均损失，学习率，时间，见过的样本数目。</p><p>将最后的结果提取出来进行显示，损失变化如下，可知收敛非常完美。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-26b2cba98299ea6f8bf3845c7ea51ff9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"480\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic2.zhimg.com/v2-26b2cba98299ea6f8bf3845c7ea51ff9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;640&#39; height=&#39;480&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"480\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https://pic2.zhimg.com/v2-26b2cba98299ea6f8bf3845c7ea51ff9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-26b2cba98299ea6f8bf3845c7ea51ff9_b.jpg\"/></figure><p>训练完之后使用如下脚本进行测试。</p><div class=\"highlight\"><pre><code class=\"language-text\">./darknet classifier valid cfg/mouth.data cfg/mouth.cfg mouth/mouth_50.weights</code></pre></div><p>一个样本的结果如下：</p><div class=\"highlight\"><pre><code class=\"language-text\">darknet/data/mouth/test/27_smile.jpg, 1, 0.006881, 0.993119,\n99: top 1: 0.960000, top 5: 1.000000</code></pre></div><p>依次表示样本darknet/data/mouth/test/27_smile.jpg，被分为类别1，分类为0和1的概率是0.006881, 0.993119，该样本是第99个测试样本，此时top1和top5的平均准确率分为是0.96和1。</p><p>到这里，我们只用了不到500个样本，就完成了一个精度不错的分类器的训练，如此轻量级的darknet，我决定粉了。</p><h2><b>总结</b></h2><p>本文讲解了如何使用darknet深度学习框架完成一个分类任务，框架固然小众，但是速度真快，而且非常轻便，推荐每一个玩深度学习，尤其是计算机视觉的朋友都用起来。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-60feffab1842911ce09ae93eeaae724f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"3999\" data-rawheight=\"2250\" class=\"origin_image zh-lightbox-thumb\" width=\"3999\" data-original=\"https://pic4.zhimg.com/v2-60feffab1842911ce09ae93eeaae724f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;3999&#39; height=&#39;2250&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"3999\" data-rawheight=\"2250\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"3999\" data-original=\"https://pic4.zhimg.com/v2-60feffab1842911ce09ae93eeaae724f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-60feffab1842911ce09ae93eeaae724f_b.jpg\"/></figure><blockquote>开源框架速成系列：</blockquote><p>第一篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029846%26idx%3D1%26sn%3D0c343cfd0ede5c8ae1405bd6348aefad%26chksm%3D871342abb064cbbd7fe31fb3c55f23875f27e48fb8354e9855823b1701f1227c71b4eb00de50%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【caffe速成】caffe图像分类从模型自定义到测试</a></p><p>第二篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029846%26idx%3D2%26sn%3D7c2582243bcd8f8b491e8e466a21978f%26chksm%3D871342abb064cbbd0cba24b408ceda2b64a7c8b6baa07f9f8f56cd4d1233caa0b80fe357753e%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【tensorflow速成】Tensorflow图像分类从模型自定义到测试</a></p><p>第三篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029881%26idx%3D1%26sn%3D3c869fcee3b48d3582952ab9a0683ea6%26chksm%3D87134284b064cb924c5e7231b3f2c36ba27e3a689b067f569f2e086f62b18413bcebc5987a07%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【pytorch速成】Pytorch图像分类从模型自定义到测试</a></p><p>第四篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029887%26idx%3D1%26sn%3D645b97809c24922352a0b39f19c9ef0c%26chksm%3D87134282b064cb9441af68124d205d9c7dedcaeb09788f4d586b949584e556eddd3a72217f69%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【paddlepaddle速成】paddlepaddle图像分类从模型自定义到测试</a></p><p>第五篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029896%26idx%3D1%26sn%3Df3f7b9cf69c514f45d1d14205f879270%26chksm%3D87134375b064ca6323354c40f3e55b02ff0d1d24f3dacfc980190d51f20ec9ec16e60c1a4741%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【Keras速成】Keras图像分类从模型自定义到测试</a></p><p>第六篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029904%26idx%3D1%26sn%3D0bdc6947f5ac68e7f68426b9d076b4ab%26chksm%3D8713436db064ca7b3b2a2a1d6a8d24c15069f72655e2c39e2498051fa0e56bbcf6fe9c332d5b%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【mxnet速成】mxnet图像分类从模型自定义到测试</a></p><p>第七篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649030976%26idx%3D1%26sn%3D0befc170a93d365b780c5f05b2f510a4%26chksm%3D8712bf3db065362b0aeaae82bdbac467be5697b34cac2797e2ee15cdcb97474c08640482bf07%26token%3D1695328272%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【cntk速成】cntk图像分类从模型自定义到测试</a></p><p>第八篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26tempkey%3DMTAwMF93Q3VDbklva2NBZDJXV1dnNWloZjJ2YWtZWmJ2aTRRaGVDMVNfbERSN2xVYXdXd3B4eWczWWdMbnQzRjZrRVpmSGRYMndrLTQ0VnhPcURTNGhsS3dYWGd3MTk0UGtmZkltU0U2U01OUmRfQzcySDVObDlxM3R2U3Q0SlNQWGRMc1NZeWtWTlVRN1NpRlJaWXRYcGdydUNOTUR6clUzVkNleVlKUE5nfn4%253D%26chksm%3D0712ba9b3065338de13ea030951ba2c25c0363d958c905e12c937c2acffa991313abb4d9f751%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【chainer速成】chainer图像分类从模型自定义到测试</a></p><p>第九篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649032012%26idx%3D1%26sn%3Df74c7084621f367adb2518ebec61ca42%26chksm%3D8712bb31b06532270b9ca9550ab48ff78adaad7d38c73645cfb8888218b8e177bebd5d401aa9%26token%3D1258619827%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【DL4J速成】Deeplearning4j图像分类从模型自定义到测试</a></p><p>第十篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649032109%26idx%3D2%26sn%3Da6ff48ec0ae5d8e7a494df7e564d9ac9%26chksm%3D8712bbd0b06532c61d98c786ba1773c29c3dcf1291f9c3cd052c5643e24699eef28481e94b8e%26token%3D1258619827%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【MatConvnet速成】MatConvnet图像分类从模型自定义到测试</a> </p><p>第十一篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649032128%26idx%3D2%26sn%3Df889e2255c0ec4960f76ad0383363849%26chksm%3D8712bbbdb06532ab136c0a485bebc5cf181f24a34bec7bab3f97f66be627e09d939b997dae90%26token%3D598159941%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【Lasagne速成】Lasagne/Theano图像分类从模型自定义到测试</a> </p><p>第十二篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649032137%26idx%3D2%26sn%3Da9770ead4a9d03f0a4e75be86657453b%26chksm%3D8712bbb4b06532a27f0f02ac30ce4a18eeac1f72ac54b3c5c1b6ec13fee29f03bfc0dcaa1efd%26token%3D1446627305%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【darknet速成】Darknet图像分类从模型自定义到测试</a></p><p></p><p></p>", 
            "topic": [
                {
                    "tag": "开源项目", 
                    "tagLink": "https://api.zhihu.com/topics/19565961"
                }, 
                {
                    "tag": "卷积神经网络（CNN）", 
                    "tagLink": "https://api.zhihu.com/topics/20043586"
                }, 
                {
                    "tag": "开源软件", 
                    "tagLink": "https://api.zhihu.com/topics/19552811"
                }
            ], 
            "comments": [
                {
                    "userName": "飞飞", 
                    "userLink": "https://www.zhihu.com/people/dfee2064cf7036cf27c493de8ad2adb1", 
                    "content": "<p>配置文件里面写top5或者top2有没有什么区别呢？</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "言有三-龙鹏", 
                            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
                            "content": "自然是有区别的", 
                            "likes": 0, 
                            "replyToAuthor": "飞飞"
                        }, 
                        {
                            "userName": "飞飞", 
                            "userLink": "https://www.zhihu.com/people/dfee2064cf7036cf27c493de8ad2adb1", 
                            "content": "我试了一下就是出来5个置信度，不知道还有没有其它区别", 
                            "likes": 0, 
                            "replyToAuthor": "言有三-龙鹏"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/60477288", 
            "userName": "言有三-龙鹏", 
            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
            "upvote": 8, 
            "title": "【MatConvnet速成】MatConvnet图像分类从模型自定义到测试", 
            "content": "<p>欢迎来到专栏<b>《2小时玩转开源框架系列》，这是我们第10篇，前面已经说过了caffe，tensorflow，pytorch，mxnet，keras，paddlepaddle，cntk，chainer，deeplearning4j。</b></p><p>今天说MatConvnet，本文所用到的数据，代码请参考我们官方git</p><div class=\"highlight\"><pre><code class=\"language-text\">https://github.com/longpeng2008/LongPeng_ML_Course</code></pre></div><p>                                                                                                                         作者&amp;编辑 | 言有三</p><h2><br/><b>1 MatConvnet是什么</b></h2><p>不同于各类深度学习框架广泛使用的语言Python，MatConvnet是用matlab作为接口语言的开源深度学习库，底层语言是cuda。 </p><div class=\"highlight\"><pre><code class=\"language-text\">官网地址为：http://www.vlfeat.org/matconvnet/\ngithub地址为：https://github.com/vlfeat/matconvnet</code></pre></div><p>因为是在matlab下面，所以debug的过程非常的方便，而且本身就有很多的研究者一直都使用matlab语言，所以<b>其实该语言的群体非常大</b>。<br/></p><p>在用python之前，我也是用matlab的，那个经典的deep-learning-toolbox的代码其实也非常值得研读，说起来，matlab还是非常做图像处理的。</p><h2><b>2 MatConvnet训练准备</b></h2><p><b>2.1 安装</b></p><p>以linux系统为例，首先要安装好matlab，这个大家自己搞定吧。然后，在matlab环境下进行安装，几行代码就可以。</p><div class=\"highlight\"><pre><code class=\"language-text\">mex -setup ##设置好编译器\nuntar(&#39;http://www.vlfeat.org/matconvnet/download/matconvnet-1.0-beta25.tar.gz&#39;) ; \ncd matconvnet-1.0-beta25\nrun matlab/vl_compilenn ;</code></pre></div><p>没有报错的话就完成了，完成后为了确保没有问题，先用官方的例子确认一下。</p><div class=\"highlight\"><pre><code class=\"language-text\">%下载预训练模型\nurlwrite(...  &#39;http://www.vlfeat.org/matconvnet/models/imagenet-vgg-f.mat&#39;, ...  &#39;imagenet-vgg-f.mat&#39;) \n%设置环境\nrun matlab/vl_setupnn \n%载入模型\nnet = load(&#39;imagenet-vgg-f.mat&#39;) ;\nnet = vl_simplenn_tidy(net)\n%读取图像并预处理\nim = imread(&#39;peppers.png&#39;) ;\nim_ = single(im) ; % note: 255 range\nim_ = imresize(im_, net.meta.normalization.imageSize(1:2)) ;\nim_ = im_ - net.meta.normalization.averageImage ;\n%得到结果\nres = vl_simplenn(net, im_) ;\nresult.scores = squeeze(gather(res(end).x)) ;\n[bestScore, best] = max(scores) ;\nfigure(1) ; clf ; imagesc(im) ;\ntitle(sprintf(&#39;%s (%d), score %.3f&#39;,...\n  net.meta.classes.description{best}, best, bestScore)) ;\n</code></pre></div><p>成功的话结果如下：<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3a5eb0d7a1940c31de208fca95446299_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"560\" data-rawheight=\"420\" class=\"origin_image zh-lightbox-thumb\" width=\"560\" data-original=\"https://pic2.zhimg.com/v2-3a5eb0d7a1940c31de208fca95446299_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;560&#39; height=&#39;420&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"560\" data-rawheight=\"420\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"560\" data-original=\"https://pic2.zhimg.com/v2-3a5eb0d7a1940c31de208fca95446299_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-3a5eb0d7a1940c31de208fca95446299_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>更复杂的还可以用DagNN wrapper的API，不过这不是本文的主要目标，因此不再讲述。</p><p>当然，我们是要用GPU的，所以还要完成GPU编译，按照这里来：</p><div class=\"highlight\"><pre><code class=\"language-text\">http://www.vlfeat.org/matconvnet/install/</code></pre></div><p>因为版本不一定完全匹配，所以用nvcc编译。</p><div class=\"highlight\"><pre><code class=\"language-text\">vl_compilenn(&#39;enableGpu&#39;, true, &#39;cudaRoot&#39;, &#39;/usr/local/cuda&#39;,&#39;cudaMethod&#39;, &#39;nvcc&#39;)</code></pre></div><p><b>2.2 数据准备</b></p><p>前面讲了官方的例子，接下来就是要用我们自己的例子了，第一步还是老规矩，准备数据，完整的代码如下。</p><div class=\"highlight\"><pre><code class=\"language-text\">function imdb = mydataset(datadir)\ninputSize =[48,48,1];\nsubdir=dir(datadir);\nimdb.images.data=[];\nimdb.images.labels=[];\nimdb.images.set = [] ;\nimdb.meta.sets = {&#39;train&#39;, &#39;val&#39;, &#39;test&#39;} ;\nimage_counter=0;\ntrainratio=0.8;\nsubdir\nfor i=3:length(subdir)\n        imgfiles=dir(fullfile(datadir,subdir(i).name));\n        imgpercategory_count=length(imgfiles)-2;\n        disp([i-2 imgpercategory_count]);\n        image_counter=image_counter+imgpercategory_count;\n        for j=3:length(imgfiles)\n            img=imread(fullfile(datadir,subdir(i).name,imgfiles(j).name));\n            img=imresize(img, inputSize(1:2));\n            img=single(img);\n%             [~,~,d]=size(img);\n%             if d==3\n%                 img=rgb2gray(img);\n%                 continue;\n%             end\n            imdb.images.data(:,:,:,end+1)=single(img);\n            imdb.images.labels(end+1)= i-2;\n            if j-2&lt;imgpercategory_count*trainratio\n                imdb.images.set(end+1)=1;\n            else\n                imdb.images.set(end+1)=3;\n            end\n        end\nend\ndataMean=mean(imdb.images.data,4);\nimdb.images.data = single(bsxfun(@minus,imdb.images.data, dataMean)) ;\nimdb.images.data_mean = dataMean;\nend</code></pre></div><p>如果使用过Matlab的同学，应该一下就看懂了，实际上就是3个步骤：</p><p>1）使用fullfile函数遍历图像。</p><p>2）预处理，包括缩放，类型转换等。</p><p>3）生成IMDB格式数据集。</p><p><b>2.3 网络定义</b></p><p>还是跟以前一样，定义一个3层的卷积神经网络，非常简单，不做过多注释了噢。</p><div class=\"highlight\"><pre><code class=\"language-text\">function net =simpleconv3()\nrng(&#39;default&#39;);\nrng(0) ; \n\nf=1/100 ;\nusebatchNormalization = true ;\n\nnet.layers = {};\nnet.layers{end+1} = struct(&#39;type&#39;, &#39;conv&#39;, ...\n                          &#39;weights&#39;, {{f*randn(3,3,3,12, &#39;single&#39;), zeros(1, 12, &#39;single&#39;)}}, ...\n                          &#39;stride&#39;, 1, ...\n                          &#39;pad&#39;, 1) ;\nnet.layers{end+1} = struct(&#39;type&#39;, &#39;pool&#39;, ...\n                          &#39;method&#39;, &#39;max&#39;, ...\n                          &#39;pool&#39;, [2 2], ...\n                          &#39;stride&#39;, 2, ...\n                          &#39;pad&#39;, 0) ;\nnet.layers{end+1} = struct(&#39;type&#39;, &#39;relu&#39;) ;\n\nnet.layers{end+1} = struct(&#39;type&#39;, &#39;conv&#39;, ...\n                          &#39;weights&#39;, {{f*randn(3,3,12,24, &#39;single&#39;),zeros(1,24,&#39;single&#39;)}}, ...\n                          &#39;stride&#39;, 1, ...\n                          &#39;pad&#39;, 1) ;\nnet.layers{end+1} = struct(&#39;type&#39;, &#39;pool&#39;, ...\n                          &#39;method&#39;, &#39;max&#39;, ...\n                          &#39;pool&#39;, [2 2], ...\n                          &#39;stride&#39;, 2, ...\n                          &#39;pad&#39;, 0) ;\nnet.layers{end+1} = struct(&#39;type&#39;, &#39;relu&#39;) ;\n\nnet.layers{end+1} = struct(&#39;type&#39;, &#39;conv&#39;, ...\n                          &#39;weights&#39;, {{f*randn(3,3,24,48, &#39;single&#39;),zeros(1,48,&#39;single&#39;)}}, ...\n                          &#39;stride&#39;, 1, ...\n                          &#39;pad&#39;, 1) ;\nnet.layers{end+1} = struct(&#39;type&#39;, &#39;pool&#39;, ...\n                          &#39;method&#39;, &#39;max&#39;, ...\n                          &#39;pool&#39;, [2 2], ...\n                          &#39;stride&#39;, 2, ...\n                          &#39;pad&#39;, 0) ;\nnet.layers{end+1} = struct(&#39;type&#39;, &#39;relu&#39;) ;\n\nnet.layers{end+1} = struct(&#39;type&#39;, &#39;conv&#39;, ...\n  &#39;weights&#39;, {{f*randn(6,6,48,2, &#39;single&#39;),zeros(1,2,&#39;single&#39;)}}, ...\n  &#39;stride&#39;, 1, ...\n  &#39;pad&#39;, 0) ;\n\n\nnet.layers{end+1} = struct(&#39;type&#39;, &#39;softmaxloss&#39;) ;\n\nnet = insertBnorm(net, 1) ;\nnet = insertBnorm(net, 5) ;\nnet = insertBnorm(net, 9) ;\n\n% Meta parameters\nnet.meta.inputSize = [48 48 3] ;\nnet.meta.trainOpts.learningRate = logspace(-2, -5, 100);\nnet.meta.trainOpts.numEpochs = 50 ;\nnet.meta.trainOpts.batchSize = 16 ;\n\n% Fill in defaul values\nnet = vl_simplenn_tidy(net) ;\n\nend\n\n% --------------------------------------------------------------------\n</code></pre></div><h2><b>3 模型训练</b></h2><p>完整代码如下。</p><div class=\"highlight\"><pre><code class=\"language-text\">function [net, info] = trainconv3()\nglobal datadir;\nrun matlab/vl_setupnn ; %初始化\n\ndatadir=&#39;/home/longpeng/project/LongPeng_ML_Course/projects/classification/matconvnet/conv3/mouth&#39;;\nopts.expDir = fullfile(&#39;/home/longpeng/project/LongPeng_ML_Course/projects/classification/matconvnet/conv3/&#39;,&#39;imdb&#39;) ;\nopts.imdbPath = fullfile(opts.expDir, &#39;imdb.mat&#39;);\n\nif exist(opts.imdbPath,&#39;file&#39;)\n   imdb=load(opts.imdbPath);\nelse\n   imdb=mydataset(datadir);\n   mkdir(opts.expDir) ;\n   save(opts.imdbPath, &#39;-struct&#39;, &#39;imdb&#39;) ;\nend\n\nnet=simpleconv3();\nnet.meta.normalization.averageImage =imdb.images.data_mean ;\nopts.train.gpus=1;\n\n[net, info] = cnn_train(net, imdb, getBatch(opts), ...\n &#39;expDir&#39;, opts.expDir, ...\n net.meta.trainOpts, ...\n opts.train, ...\n &#39;val&#39;, find(imdb.images.set == 3)) ;\n\nfunction fn = getBatch(opts)\n% --------------------------------------------------------------------\n   fn = @(x,y) getSimpleNNBatch(x,y) ;\nend\n\nfunction [images, labels]  = getSimpleNNBatch(imdb, batch)\n   images = imdb.images.data(:,:,:,batch) ;\n   labels = imdb.images.labels(1,batch) ;\n   if opts.train.gpus &gt; 0\n       images = gpuArray(images) ;\n   end\nend\nend\n</code></pre></div><p>总共就这么几个步骤：</p><p>1）初始化环境，run matlab/vl_setupnn 。</p><p>2）定义网络：net=simpleconv3()。</p><p>3）调用训练接口：[net, info] = cnn_train(net, imdb, getBatch(opts)。</p><h2><b>4 可视化</b></h2><p>工具已经给封装好了可视化，直接运行代码就会跳出来，可以看出收敛正常，略有过拟合，展示的分别是softmax损失和错误率。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-5d847a881f0d5a22d38729a79504935f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"323\" data-rawheight=\"450\" class=\"content_image\" width=\"323\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;323&#39; height=&#39;450&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"323\" data-rawheight=\"450\" class=\"content_image lazy\" width=\"323\" data-actualsrc=\"https://pic4.zhimg.com/v2-5d847a881f0d5a22d38729a79504935f_b.jpg\"/></figure><h2><b>5 测试</b></h2><div class=\"highlight\"><pre><code class=\"language-text\">%netpath=[opts.expDir &#39;/net-epoch-50.mat&#39;];    \nnetpath=&#39;/home/longpeng/project/LongPeng_ML_Course/projects/classification/matconvnet/conv3/imdb/net-epoch-50.mat&#39;;    \nclass=1;index=1;    \ndatadir=&#39;/home/longpeng/project/LongPeng_ML_Course/projects/classification/matconvnet/conv3/mouth&#39;;    \nsubdir=dir(datadir);    \nimgfiles=dir(fullfile(datadir,subdir(class+2).name));    \nimg=imread(fullfile(datadir,subdir(class+2).name,imgfiles(index+2).name));    \nimshow(img);    \nnet=load(netpath);    \nnet=net.net;    \nim_=single(img);    \nim_=imresize(im_,net.meta.inputSize(1:2));    \nim_=im_ - net.meta.normalization.averageImage;    \nopts.batchNormalization = false ;    \nnet.layers{end}.type = &#39;softmax&#39;;    \nres=vl_simplenn(net,im_);    \nscores=squeeze(gather(res(end).x));    \n[bestScore,best]=max(scores);    \nstr=[subdir(best+2).name &#39;:&#39; num2str(bestScore)];    \ntitle(str);    \ndisp(str);    </code></pre></div><p>从上面可以看出，就是载入模型，完成正确的预处理，然后进行分类。</p><p>一个样本的结果如下，0:0.99968，表示分类为类别0的概率是0.99968，可知结果正确，0代表的类别就是中性表情。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-a12bd9dbb308c83e5f764785bb810d4b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"83\" data-rawheight=\"98\" class=\"content_image\" width=\"83\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;83&#39; height=&#39;98&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"83\" data-rawheight=\"98\" class=\"content_image lazy\" width=\"83\" data-actualsrc=\"https://pic4.zhimg.com/v2-a12bd9dbb308c83e5f764785bb810d4b_b.jpg\"/></figure><h2><b>总结</b></h2><p>有很多的优秀代码仍然使用matconvnet，而且它的社区所包含的预训练模型也非常多，非常适合训练过程中进行调试，建议大家有Matlab环境和多余精力的可以学习一下，学习成本很低，技多不压身嘛。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-60feffab1842911ce09ae93eeaae724f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"3999\" data-rawheight=\"2250\" class=\"origin_image zh-lightbox-thumb\" width=\"3999\" data-original=\"https://pic4.zhimg.com/v2-60feffab1842911ce09ae93eeaae724f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;3999&#39; height=&#39;2250&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"3999\" data-rawheight=\"2250\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"3999\" data-original=\"https://pic4.zhimg.com/v2-60feffab1842911ce09ae93eeaae724f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-60feffab1842911ce09ae93eeaae724f_b.jpg\"/></figure><blockquote>本系列完整文章：</blockquote><p>第一篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029846%26idx%3D1%26sn%3D0c343cfd0ede5c8ae1405bd6348aefad%26chksm%3D871342abb064cbbd7fe31fb3c55f23875f27e48fb8354e9855823b1701f1227c71b4eb00de50%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【caffe速成】caffe图像分类从模型自定义到测试</a></p><p>第二篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029846%26idx%3D2%26sn%3D7c2582243bcd8f8b491e8e466a21978f%26chksm%3D871342abb064cbbd0cba24b408ceda2b64a7c8b6baa07f9f8f56cd4d1233caa0b80fe357753e%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【tensorflow速成】Tensorflow图像分类从模型自定义到测试</a></p><p>第三篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029881%26idx%3D1%26sn%3D3c869fcee3b48d3582952ab9a0683ea6%26chksm%3D87134284b064cb924c5e7231b3f2c36ba27e3a689b067f569f2e086f62b18413bcebc5987a07%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【pytorch速成】Pytorch图像分类从模型自定义到测试</a></p><p>第四篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029887%26idx%3D1%26sn%3D645b97809c24922352a0b39f19c9ef0c%26chksm%3D87134282b064cb9441af68124d205d9c7dedcaeb09788f4d586b949584e556eddd3a72217f69%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【paddlepaddle速成】paddlepaddle图像分类从模型自定义到测试</a></p><p>第五篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029896%26idx%3D1%26sn%3Df3f7b9cf69c514f45d1d14205f879270%26chksm%3D87134375b064ca6323354c40f3e55b02ff0d1d24f3dacfc980190d51f20ec9ec16e60c1a4741%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【Keras速成】Keras图像分类从模型自定义到测试</a></p><p>第六篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029904%26idx%3D1%26sn%3D0bdc6947f5ac68e7f68426b9d076b4ab%26chksm%3D8713436db064ca7b3b2a2a1d6a8d24c15069f72655e2c39e2498051fa0e56bbcf6fe9c332d5b%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【mxnet速成】mxnet图像分类从模型自定义到测试</a></p><p>第七篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649030976%26idx%3D1%26sn%3D0befc170a93d365b780c5f05b2f510a4%26chksm%3D8712bf3db065362b0aeaae82bdbac467be5697b34cac2797e2ee15cdcb97474c08640482bf07%26token%3D1695328272%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【cntk速成】cntk图像分类从模型自定义到测试</a></p><p>第八篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26tempkey%3DMTAwMF93Q3VDbklva2NBZDJXV1dnNWloZjJ2YWtZWmJ2aTRRaGVDMVNfbERSN2xVYXdXd3B4eWczWWdMbnQzRjZrRVpmSGRYMndrLTQ0VnhPcURTNGhsS3dYWGd3MTk0UGtmZkltU0U2U01OUmRfQzcySDVObDlxM3R2U3Q0SlNQWGRMc1NZeWtWTlVRN1NpRlJaWXRYcGdydUNOTUR6clUzVkNleVlKUE5nfn4%253D%26chksm%3D0712ba9b3065338de13ea030951ba2c25c0363d958c905e12c937c2acffa991313abb4d9f751%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【chainer速成】chainer图像分类从模型自定义到测试</a></p><p>第九篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649032012%26idx%3D1%26sn%3Df74c7084621f367adb2518ebec61ca42%26chksm%3D8712bb31b06532270b9ca9550ab48ff78adaad7d38c73645cfb8888218b8e177bebd5d401aa9%26token%3D1258619827%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【DL4J速成】Deeplearning4j图像分类从模型自定义到测试</a></p><p>第十篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649032109%26idx%3D2%26sn%3Da6ff48ec0ae5d8e7a494df7e564d9ac9%26chksm%3D8712bbd0b06532c61d98c786ba1773c29c3dcf1291f9c3cd052c5643e24699eef28481e94b8e%26token%3D1258619827%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【MatConvnet速成】MatConvnet图像分类从模型自定义到测试</a></p><p>第十一篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649032128%26idx%3D2%26sn%3Df889e2255c0ec4960f76ad0383363849%26chksm%3D8712bbbdb06532ab136c0a485bebc5cf181f24a34bec7bab3f97f66be627e09d939b997dae90%26token%3D598159941%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【Lasagne速成】Lasagne/Theano图像分类从模型自定义到测试</a></p><p>第十二篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649032137%26idx%3D2%26sn%3Da9770ead4a9d03f0a4e75be86657453b%26chksm%3D8712bbb4b06532a27f0f02ac30ce4a18eeac1f72ac54b3c5c1b6ec13fee29f03bfc0dcaa1efd%26token%3D1446627305%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【darknet速成】Darknet图像分类从模型自定义到测试</a></p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "开源项目", 
                    "tagLink": "https://api.zhihu.com/topics/19565961"
                }, 
                {
                    "tag": "MATLAB", 
                    "tagLink": "https://api.zhihu.com/topics/19559252"
                }
            ], 
            "comments": [
                {
                    "userName": "秋狂言", 
                    "userLink": "https://www.zhihu.com/people/f6a60af687127150329504c561236305", 
                    "content": "<p>不知Matconvnet是否还在更新…</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "言有三-龙鹏", 
                            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
                            "content": "<p>应该不维护了</p>", 
                            "likes": 0, 
                            "replyToAuthor": "秋狂言"
                        }
                    ]
                }, 
                {
                    "userName": "知乎用户", 
                    "userLink": "https://www.zhihu.com/people/0", 
                    "content": "<p>貌似已经停止维护了</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "言有三-龙鹏", 
                            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
                            "content": "<p>好像是的</p>", 
                            "likes": 0, 
                            "replyToAuthor": "知乎用户"
                        }
                    ]
                }, 
                {
                    "userName": "aluluba", 
                    "userLink": "https://www.zhihu.com/people/fcc60fc97aa16698204157212403d18e", 
                    "content": "不知道你有没有感觉Matconvnet的速度惊人的快，我最近在测试一个超分辨网络，同样的模型，用Matconvnet跑比用keras+tensorflow或者caffe+matlab快5倍到10倍，我感觉挺神奇[思考]", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "言有三-龙鹏", 
                            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
                            "content": "哈哈，这个我倒是没有注意", 
                            "likes": 0, 
                            "replyToAuthor": "aluluba"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/60119869", 
            "userName": "言有三-龙鹏", 
            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
            "upvote": 8, 
            "title": "【DL4J速成】Deeplearning4j图像分类从模型自定义到测试", 
            "content": "<p>欢迎来到专栏<b>《2小时玩转开源框架系列》，这是我们第九篇，前面已经说过了caffe，tensorflow，pytorch，mxnet，keras，paddlepaddle，cntk，chainer。</b></p><p>今天说Deeplearning4j(DL4J)，本文所用到的数据，代码请参考我们官方git</p><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/longpeng2008/LongPeng_ML_Course\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/longpeng2008</span><span class=\"invisible\">/LongPeng_ML_Course</span><span class=\"ellipsis\"></span></a></p><p>                                                                                                                作者&amp;编辑 | 胡郡郡 言有三<br/></p><h2><b>1 Deeplearning4j(DL4J)是什么</b></h2><p>不同于深度学习广泛应用的语言Python，DL4J是为<b>java和jvm</b>编写的开源深度学习库，支持各种深度学习模型。</p><p>DL4J最重要的特点是支持分布式，可以在Spark和Hadoop上运行，支持分布式CPU和GPU运行。DL4J是为商业环境，而非研究所设计的，因此更加贴近某些生产环境。</p><h2><b>2 DL4J训练准备</b></h2><p><b>2.1 DL4J安装</b></p><p>系统要求：</p><ul><li>Java：开发者版7或更新版本（仅支持64位版本）</li><li>Apache Maven：Maven是针对Java的项目管理工具，兼容IntelliJ等IDE，可以让我们轻松安装DL4J项目库</li><li>IntelliJ IDEA (建议)或 Eclipse</li><li>Git</li></ul><p>官方提供了很多DL4J的示例。可以通过以下命令下载安装：</p><div class=\"highlight\"><pre><code class=\"language-text\">$ git clone https://github.com/deeplearning4j/dl4j-examples.git\n$ cd dl4j-examples/\n$ mvn clean install</code></pre></div><p>mvn clean install 目的是为了安装所依赖的相关包。</p><p>然后将下载的dl4j-examples导入到IntelliJ IDEA中，点击自己想要试的例子进行运行。</p><p><b>2.2 数据准备</b></p><p>DL4J有自己的特殊的数据结构DataVec，所有的输入数据在进入神经网络之前要先经过向量化。向量化后的结果就是<b>一个行数不限的单列矩阵</b>。</p><p>熟悉Hadoop/MapReduce的朋友肯定知道它的输入用InputFormat来确定具体的InputSplit和RecordReader。DataVec也有自己FileSplit和RecordReader，并且对于不同的数据类型（文本、CSV、音频、图像、视频等），有不同的RecordReader，下面是一个图像的例子。</p><div class=\"highlight\"><pre><code class=\"language-text\">int height = 48;  // 输入图像高度\nint width = 48;   // 输入图像宽度\nint channels = 3; // 输入图像通道数\nint outputNum = 2; // 2分类\nint batchSize = 64;\nint nEpochs = 100;  \nint seed = 1234;\nRandom randNumGen = new Random(seed);\n\n// 训练数据的向量化\nFile trainData = new File(inputDataDir + &#34;/train&#34;);\nFileSplit trainSplit = new FileSplit(trainData, NativeImageLoader.ALLOWED_FORMATS, randNumGen);\nParentPathLabelGenerator labelMaker = new ParentPathLabelGenerator(); // parent path as the image label\nImageRecordReader trainRR = new ImageRecordReader(height, width, channels, labelMaker);\ntrainRR.initialize(trainSplit);\nDataSetIterator trainIter = new RecordReaderDataSetIterator(trainRR, batchSize, 1, outputNum);\n\n// 将像素从0-255缩放到0-1 (用min-max的方式进行缩放)\nDataNormalization scaler = new ImagePreProcessingScaler(0, 1);\nscaler.fit(trainIter);\ntrainIter.setPreProcessor(scaler);\n\n// 测试数据的向量化\nFile testData = new File(inputDataDir + &#34;/test&#34;);\nFileSplit testSplit = new FileSplit(testData, NativeImageLoader.ALLOWED_FORMATS, randNumGen);\nImageRecordReader testRR = new ImageRecordReader(height, width, channels, labelMaker);\ntestRR.initialize(testSplit);\nDataSetIterator testIter = new RecordReaderDataSetIterator(testRR, batchSize, 1, outputNum);\ntestIter.setPreProcessor(scaler); // same normalization for better results</code></pre></div><p>数据准备的过程分成以下几个步骤：</p><p>1）通过FileSplit处理输入文件，FileSplit决定了文件的分布式的分发和处理。</p><p>2）ParentPathLabelGenerator通过父目录来直接生成标签，这个生成标签的接口非常方便，比如说如果是二分类，我们先将两个父目录设定为0和1，然后再分别在里面放置对应的图像就行。</p><p>3）通过ImageRecordReader读入输入图像。RecordReader是DataVec中的一个类，ImageRecordReader是RecordReader中的一个子类，这样就可以将输入图像转成向量化的带有索引的数据。</p><p>4）生成DataSetIterator，实现了对输入数据集的迭代。</p><p><b>2.3 网络定义</b></p><p>在Deeplearning4j中，添加一个层的方式是通过NeuralNetConfiguration.Builder()调用layer，指定其在所有层中的输入及输出节点数nIn和nOut，激活方式activation，层的类型如ConvolutionLayer等。</p><div class=\"highlight\"><pre><code class=\"language-text\">// 设置网络层及超参数\nMultiLayerConfiguration conf = new NeuralNetConfiguration.Builder()\n   .seed(seed)\n   .l2(0.0005)\n   .updater(new Adam(0.0001))\n   .weightInit(WeightInit.XAVIER)\n   .list()\n   .layer(0, new ConvolutionLayer.Builder(3, 3)\n       .nIn(channels)\n       .stride(2, 2)\n       .nOut(12)\n       .activation(Activation.RELU)\n       .weightInit(WeightInit.XAVIER)\n       .build())\n   .layer(1, new BatchNormalization.Builder()\n       .nIn(12)\n       .nOut(12)\n       .build())\n   .layer(2, new ConvolutionLayer.Builder(3, 3)\n       .nIn(12)\n       .stride(2, 2)\n       .nOut(24)\n       .activation(Activation.RELU)\n       .weightInit(WeightInit.XAVIER)\n       .build())\n   .layer(3, new BatchNormalization.Builder()\n       .nIn(24)\n       .nOut(24)\n       .build())\n   .layer(4, new ConvolutionLayer.Builder(3, 3)\n       .nIn(24)\n       .stride(2, 2)\n       .nOut(48)\n       .activation(Activation.RELU)\n       .weightInit(WeightInit.XAVIER)\n       .build())\n   .layer(5, new BatchNormalization.Builder()\n       .nIn(48)\n       .nOut(48)\n       .build())\n   .layer(6, new DenseLayer.Builder().activation(Activation.RELU)\n       .nOut(128).build())\n   .layer(7, new OutputLayer.Builder(LossFunctions.LossFunction.NEGATIVELOGLIKELIHOOD)\n       .nOut(outputNum)\n       .activation(Activation.SOFTMAX)\n       .build())\n   .setInputType(InputType.convolutionalFlat(48, 48, 3)) // InputType.convolutional for normal image\n   .backprop(true).pretrain(false).build();</code></pre></div><p>这里的网络结构和之前的caffe、tensorflow、pytorch等框架采用的网络结构是一样的，都是一个3层的神经网络。</p><h2><b>3 模型训练</b></h2><p>数据准备好了，网络也建好了，接下来就可以训练了。</p><div class=\"highlight\"><pre><code class=\"language-text\">// 新建一个多层网络模型\nMultiLayerNetwork net = new MultiLayerNetwork(conf);\nnet.init();\n// 训练的过程中同时进行评估\nfor (int i = 0; i &lt; nEpochs; i++) {\n   net.fit(trainIter);\n   log.info(&#34;Completed epoch &#34; + i);\n   Evaluation trainEval = net.evaluate(trainIter);\n   Evaluation eval = net.evaluate(testIter);\n   log.info(&#34;train: &#34; + trainEval.precision());\n   log.info(&#34;val: &#34; + eval.precision());\n   trainIter.reset();\n   testIter.reset();\n}\n//保存模型\n\nModelSerializer.writeModel(net, new File(modelDir + &#34;/mouth-model.zip&#34;), true);\n</code></pre></div><p>训练的过程非常简单直观，直接通过<b>net.fit()</b>加载trainIter就可以，其中trainIter在数据准备中已经定义好了。</p><p>通过<b>net.evaluate(trainIter)和net.evaluate(testIter)</b>的方式来评估训练和测试的表现，这里我们将每个epoch的准确率打印出来。</p><h2><b>4 可视化</b></h2><p>DL4J提供的用户界面可以在浏览器中看到实时的训练过程。</p><p>第一步：</p><p>将用户界面依赖项添加到pom文件中：</p><div class=\"highlight\"><pre><code class=\"language-text\">&lt;dependency&gt;\n       &lt;groupId&gt;org.deeplearning4j&lt;/groupId&gt;\n       &lt;artifactId&gt;deeplearning4j-ui_2.10&lt;/artifactId&gt;\n       &lt;version&gt;${dl4j.version}&lt;/version&gt;\n&lt;/dependency&gt;</code></pre></div><p>第二步：</p><p>在项目中启动用户界面</p><div class=\"highlight\"><pre><code class=\"language-text\">//初始化用户界面后端,获取一个UI实例\nUIServer uiServer = UIServer.getInstance();\n//设置网络信息（随时间变化的梯度、分值等）的存储位置。这里将其存储于内存。\nStatsStorage statsStorage = new InMemoryStatsStorage(); \n//将StatsStorage实例连接至用户界面，让StatsStorage的内容能够被可视化\nuiServer.attach(statsStorage);\n//添加StatsListener来在网络定型时收集这些信息\nnet.setListeners(new StatsListener(statsStorage));</code></pre></div><p>首先我们初始化一个用户界面后端，设置网络信息的存储位置。</p><p>这里将其存储于内存，也可以放入文件中，通过new FileStatsStorage(File)的方式实现。</p><p>再将StatsStorage实例连接至用户界面，让StatsStorage的内容能够被可视化。</p><p>最后添加StatsListener监听，在网络定型时收集这些信息。</p><p>默认的浏览器地址是：http://localhost:9000/train/overview</p><p>下面可视化一下损失函数值随迭代次数的变化曲线</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-cfb740bde2c36b03a929fa18553e8556_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"339\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-cfb740bde2c36b03a929fa18553e8556_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;339&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"339\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-cfb740bde2c36b03a929fa18553e8556_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-cfb740bde2c36b03a929fa18553e8556_b.jpg\"/></figure><p>模型页面中可以直观感受我们建立的模型</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-771e00695b81b89d22f47846ddde9ca4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"1357\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-771e00695b81b89d22f47846ddde9ca4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;1357&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"1357\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-771e00695b81b89d22f47846ddde9ca4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-771e00695b81b89d22f47846ddde9ca4_b.jpg\"/></figure><p>看一下最后的训练集和测试集的准确率</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-99452a099dfd55cb7c8287f1b097c1a9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"572\" data-rawheight=\"434\" class=\"origin_image zh-lightbox-thumb\" width=\"572\" data-original=\"https://pic2.zhimg.com/v2-99452a099dfd55cb7c8287f1b097c1a9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;572&#39; height=&#39;434&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"572\" data-rawheight=\"434\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"572\" data-original=\"https://pic2.zhimg.com/v2-99452a099dfd55cb7c8287f1b097c1a9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-99452a099dfd55cb7c8287f1b097c1a9_b.jpg\"/></figure><p>有一些过拟合，主要原因还是数据太少。</p><p>以上就是我们用自己的数据在DL4J框架上实践的内容，完整代码可以参考官方git。 </p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-60feffab1842911ce09ae93eeaae724f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"3999\" data-rawheight=\"2250\" class=\"origin_image zh-lightbox-thumb\" width=\"3999\" data-original=\"https://pic4.zhimg.com/v2-60feffab1842911ce09ae93eeaae724f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;3999&#39; height=&#39;2250&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"3999\" data-rawheight=\"2250\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"3999\" data-original=\"https://pic4.zhimg.com/v2-60feffab1842911ce09ae93eeaae724f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-60feffab1842911ce09ae93eeaae724f_b.jpg\"/></figure><blockquote>本系列完整文章：</blockquote><p>第一篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029846%26idx%3D1%26sn%3D0c343cfd0ede5c8ae1405bd6348aefad%26chksm%3D871342abb064cbbd7fe31fb3c55f23875f27e48fb8354e9855823b1701f1227c71b4eb00de50%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【caffe速成】caffe图像分类从模型自定义到测试</a></p><p>第二篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029846%26idx%3D2%26sn%3D7c2582243bcd8f8b491e8e466a21978f%26chksm%3D871342abb064cbbd0cba24b408ceda2b64a7c8b6baa07f9f8f56cd4d1233caa0b80fe357753e%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【tensorflow速成】Tensorflow图像分类从模型自定义到测试</a></p><p>第三篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029881%26idx%3D1%26sn%3D3c869fcee3b48d3582952ab9a0683ea6%26chksm%3D87134284b064cb924c5e7231b3f2c36ba27e3a689b067f569f2e086f62b18413bcebc5987a07%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【pytorch速成】Pytorch图像分类从模型自定义到测试</a></p><p>第四篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029887%26idx%3D1%26sn%3D645b97809c24922352a0b39f19c9ef0c%26chksm%3D87134282b064cb9441af68124d205d9c7dedcaeb09788f4d586b949584e556eddd3a72217f69%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【paddlepaddle速成】paddlepaddle图像分类从模型自定义到测试</a></p><p>第五篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029896%26idx%3D1%26sn%3Df3f7b9cf69c514f45d1d14205f879270%26chksm%3D87134375b064ca6323354c40f3e55b02ff0d1d24f3dacfc980190d51f20ec9ec16e60c1a4741%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【Keras速成】Keras图像分类从模型自定义到测试</a></p><p>第六篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029904%26idx%3D1%26sn%3D0bdc6947f5ac68e7f68426b9d076b4ab%26chksm%3D8713436db064ca7b3b2a2a1d6a8d24c15069f72655e2c39e2498051fa0e56bbcf6fe9c332d5b%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【mxnet速成】mxnet图像分类从模型自定义到测试</a></p><p>第七篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649030976%26idx%3D1%26sn%3D0befc170a93d365b780c5f05b2f510a4%26chksm%3D8712bf3db065362b0aeaae82bdbac467be5697b34cac2797e2ee15cdcb97474c08640482bf07%26token%3D1695328272%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【cntk速成】cntk图像分类从模型自定义到测试</a></p><p>第八篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26tempkey%3DMTAwMF93Q3VDbklva2NBZDJXV1dnNWloZjJ2YWtZWmJ2aTRRaGVDMVNfbERSN2xVYXdXd3B4eWczWWdMbnQzRjZrRVpmSGRYMndrLTQ0VnhPcURTNGhsS3dYWGd3MTk0UGtmZkltU0U2U01OUmRfQzcySDVObDlxM3R2U3Q0SlNQWGRMc1NZeWtWTlVRN1NpRlJaWXRYcGdydUNOTUR6clUzVkNleVlKUE5nfn4%253D%26chksm%3D0712ba9b3065338de13ea030951ba2c25c0363d958c905e12c937c2acffa991313abb4d9f751%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【chainer速成】chainer图像分类从模型自定义到测试</a></p><p>第九篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649032012%26idx%3D1%26sn%3Df74c7084621f367adb2518ebec61ca42%26chksm%3D8712bb31b06532270b9ca9550ab48ff78adaad7d38c73645cfb8888218b8e177bebd5d401aa9%26token%3D1258619827%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【DL4J速成】Deeplearning4j图像分类从模型自定义到测试</a></p><p>第十篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649032109%26idx%3D2%26sn%3Da6ff48ec0ae5d8e7a494df7e564d9ac9%26chksm%3D8712bbd0b06532c61d98c786ba1773c29c3dcf1291f9c3cd052c5643e24699eef28481e94b8e%26token%3D1258619827%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【MatConvnet速成】MatConvnet图像分类从模型自定义到测试</a></p><p>第十一篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649032128%26idx%3D2%26sn%3Df889e2255c0ec4960f76ad0383363849%26chksm%3D8712bbbdb06532ab136c0a485bebc5cf181f24a34bec7bab3f97f66be627e09d939b997dae90%26token%3D598159941%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【Lasagne速成】Lasagne/Theano图像分类从模型自定义到测试</a></p><p>第十二篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649032137%26idx%3D2%26sn%3Da9770ead4a9d03f0a4e75be86657453b%26chksm%3D8712bbb4b06532a27f0f02ac30ce4a18eeac1f72ac54b3c5c1b6ec13fee29f03bfc0dcaa1efd%26token%3D1446627305%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【darknet速成】Darknet图像分类从模型自定义到测试</a></p><p></p>", 
            "topic": [
                {
                    "tag": "开源项目", 
                    "tagLink": "https://api.zhihu.com/topics/19565961"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "Java", 
                    "tagLink": "https://api.zhihu.com/topics/19561132"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/59538584", 
            "userName": "言有三-龙鹏", 
            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
            "upvote": 6, 
            "title": "【chainer速成】chainer图像分类从模型自定义到测试", 
            "content": "<p>欢迎来到专栏<b>《2小时玩转开源框架系列》，这是我们第八篇，前面已经说过了caffe，tensorflow，pytorch，mxnet，keras，paddlepaddle，cntk。</b></p><p>今天说chainer，本文所用到的数据，代码请参考我们官方git：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/longpeng2008/LongPeng_ML_Course\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/longpeng2008</span><span class=\"invisible\">/LongPeng_ML_Course</span><span class=\"ellipsis\"></span></a></p><p>                                                                                                                            作者&amp;编辑 | 汤兴旺<br/></p><h2><b>1 chainer是什么</b></h2><p>chainer是一个基于python的深度学习框架，能够轻松直观地编写复杂的神经网络架构。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-d9e59094b915e361ad1954a5d662373b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"806\" data-rawheight=\"235\" class=\"origin_image zh-lightbox-thumb\" width=\"806\" data-original=\"https://pic4.zhimg.com/v2-d9e59094b915e361ad1954a5d662373b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;806&#39; height=&#39;235&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"806\" data-rawheight=\"235\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"806\" data-original=\"https://pic4.zhimg.com/v2-d9e59094b915e361ad1954a5d662373b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-d9e59094b915e361ad1954a5d662373b_b.jpg\"/></figure><p>当前大多数深度学习框架都基于<b>“Define-and-Run”</b>方案。也就是说，首先定义网络，然后用户定期向其提供小批量的训练数据。由于网络静态定义的，因此所有的逻辑必须作为数据嵌入到网络架构中。</p><p>相反，chainer采用<b>“Define-by-Run”</b>方案，即通过实际的前向计算动态定义网络。更确切地说，chainer存储计算历史而不是编程逻辑。这样，Chainer不需要将条件和循环引入网络定义 。chainer的核心理念就是<b>Define-by-Run。</b></p><h2><b>2 chainer训练准备</b></h2><p><b>2.1 chainer安装</b></p><p>chainer安装很简单，只需要在终端输入下面命令即可安装：</p><div class=\"highlight\"><pre><code class=\"language-text\">pip install chainer</code></pre></div><p><b>2.2 数据读取</b></p><p>在chainer中读取数据是非常简单的。数据读取部分的代码如下：</p><div class=\"highlight\"><pre><code class=\"language-text\">import numpy as np\nimport os\nfrom PIL import Image\nimport glob\nfrom chainer.datasets import tuple_dataset\nclass Dataset():\n    def __init__(self, path, width=60, height=60):\n        channels = 3\n        path = glob.glob(&#39;./mouth/*&#39;)\n        pathsAndLabels = []\n        index = 0\n        for p in path:\n            print(p + &#34;,&#34; + str(index))\n            pathsAndLabels.append(np.asarray([p, index]))\n            index = index + 1\n        allData = []\n        for pathAndLabel in pathsAndLabels:\n            path = pathAndLabel[0]\n            label = pathAndLabel[1]\n            imagelist = glob.glob(path + &#34;/*&#34;)\n            for imgName in imagelist:\n                allData.append([imgName, label])\n        allData = np.random.permutation(allData)\n        imageData = []\n        labelData = []</code></pre></div><p>下面解释下在chainer中读取数据的一些特色，完整代码请移步github。</p><p>在chainer中我们通过chainer.datasets模块来获取数据集，其最基本的数据集就是一个数组，平时最常见的NumPy和CuPy数组都可以直接用作数据集。在本实例中我们采用的是元组数据集即TupleDataset()来获取数据。</p><p><b>2.3 网络定义</b></p><p>它的网络定义和pytorch基本上是相似的，如下：</p><div class=\"highlight\"><pre><code class=\"language-text\">class MyModel(Chain):\n    def __init__(self):\n        super(MyModel, self).__init__()\n        with self.init_scope():\n            self.conv1 = L.Convolution2D(\n                   in_channels=3, out_channels=12, ksize=3, stride=2)\n            self.bn1 = L.BatchNormalization(12)\n            self.conv2 = L.Convolution2D(\n                   in_channels=12, out_channels=24, ksize=3, stride=2)\n            self.bn2 = L.BatchNormalization(24)\n            self.conv3 = L.Convolution2D(\n                   in_channels=24, out_channels=48, ksize=3, stride=2)\n            self.bn3 = L.BatchNormalization(48)\n            self.fc1 = L.Linear(None, 1200)\n            self.fc2 = L.Linear(1200, 128)\n            self.fc3 = L.Linear(128, 2)\n    def __call__(self,x):\n        return self.forward(x)\n    def forward(self, x):\n        h1 = F.relu(self.conv1(x))\n        h2 = F.relu(self.conv2(h1))\n        h3 = F.relu(self.conv3(h2))\n        h4 = F.relu(self.fc1(h3))\n        h5 = F.relu(self.fc2(h4))\n        x = self.fc3(h5)\n        return (x)</code></pre></div><p>上面的例子和之前说过的caffe、tensorflow、pytorch等框架采用的网络结构是一样。这里不在赘述，我具体说下这个框架的特色。</p><p>(1) MyModel(Chain)</p><p>Chain在chainer中是一个定义模型的类，我们把模型MyModel定义为Chain的子类，即继承Chain这个类，这和Pytorch中的nn.module类似。以后我们在模型定义时都可以通过Chain来构建具有潜在深层功能和链接层次的模型。</p><p>(2) Link和Function</p><p>在Chainer中，神经网络的每一层都可以认为是由两种广泛类型的函数之一组成即Link和Function。</p><p>其中Function是一个没有可学习参数的函数，而LInk是包括参数的，我们也能把Link理解成一个赋予其参数的Function。</p><p>在我们使用它之前，我们首先需要导入相应的模块，如下：</p><div class=\"highlight\"><pre><code class=\"language-text\">​import chainer.links as L\nimport chainer.functions as F</code></pre></div><p>另外在平时使用时我们喜欢用L替代Link，用F代替Function。如L.Convolution2D和F.relu</p><p>(3) __call__</p><p>对于__call__它的作用就是使我们的chain像一个函数一样容易被调用。</p><p><b>3 模型训练</b></p><p>数据加载和网络定义好后，我们就可以进行模型训练了，话不多说，我们直接上代码。</p><div class=\"highlight\"><pre><code class=\"language-text\">model = L.Classifier(MyModel())\nif os.path.isfile(&#39;./dataset.pickle&#39;):\n    print(&#34;dataset.pickle is exist. loading...&#34;)\n    with open(&#39;./dataset.pickle&#39;, mode=&#39;rb&#39;) as f:\n        train, test = pickle.load(f)\n        print(&#34;Loaded&#34;)\n    else:\n        datasets = dataset.Dataset(&#34;mouth&#34;)\n        train, test = datasets.get_dataset()\n        with open(&#39;./dataset.pickle&#39;, mode=&#39;wb&#39;) as f:\n            pickle.dump((train, test), f)\n            print(&#34;saving train and test...&#34;)\n    optimizer = optimizers.MomentumSGD(lr=0.001, momentum=0.5)\n    optimizer.setup(model)\n    train_iter = iterators.SerialIterator(train, 64)\n    test_iter = iterators.SerialIterator(test, 64, repeat=False, shuffle=True)\n    updater = training.StandardUpdater(train_iter, optimizer, device=-1)\n    trainer = training.Trainer(updater, (800, &#39;epoch&#39;),        out=&#39;{}_model_result&#39;.format(MyModel.__class__.__name__))</code></pre></div><p>在chainer中，模型训练可以分为如下6个步骤，个人认为这6个步骤是非常好理解的。</p><p><b>Step-01-Dataset</b></p><p>第一步当然就是加载我们的数据集了，我们通常都是通过下面方法加载数据集：</p><p>train, test = datasets.get_dataset()</p><p><b>Step-02-Iterator</b></p><p>chainer提供了一些Iterator，通常我们采用下面的方法来从数据集中获取小批量的数据进行迭代。</p><p>train_iter = iterators.SerialIterator(train, batchsize)<br/>test_iter = iterators.SerialIterator(test, batchsize, repeat=False, shuffle=True)</p><p><b>Step-03-Model</b></p><p>在chainer中chainer.links.Classifier是一个简单的分类器模型，尽管它里面有许多参数如predictor、lossfun和accfun，但我们只需赋予其一个参数那就是predictor，即你定义过的模型。</p><p>model = L.Classifier(MyModel())</p><p><b>Step-04-Optimizer</b></p><p>模型弄好后，接下来当然是优化了，在chainer.optimizers中有许多我们常见的优化器，部分优化器如下：</p><div class=\"highlight\"><pre><code class=\"language-text\">1、chainer.optimizers.AdaDelta\n2、chainer.optimizers.AdaGrad\n3、chainer.optimizers.AdaDelta  \n4、chainer.optimizers.AdaGrad    \n5、chainer.optimizers.Adam  \n6、chainer.optimizers.CorrectedMomentumSGD   .    \n7、chainer.optimizers.MomentumSGD   \n8、chainer.optimizers.NesterovAG  \n9、chainer.optimizers.RMSprop   \n10、chainer.optimizers.RMSpropGraves   \n...</code></pre></div><p><b>Step-05-Updater</b></p><p>当我们想要训练神经网络时，我们必须运行多次更新参数，这在chainer中就是Updater所做的工作，在本例我们使用的是 training.StandardUpdater。</p><p><b>Step-06-Trainer</b></p><p>上面的工作做完之后我们需要做的就是训练了。在chainer中，训练模型采用的是 training.Trainer()。</p><h2><b>4 可视化</b></h2><div class=\"highlight\"><pre><code class=\"language-text\">trainer.extend(extensions.dump_graph(&#34;main/loss&#34;))\ntrainer.extend(extensions.Evaluator(test_iter, model, device=-1))\ntrainer.extend(extensions.LogReport())\ntrainer.extend(extensions.PrintReport( [&#39;epoch&#39;, &#39;main/loss&#39;, &#39;validation/main/loss&#39;, &#39;main/accuracy&#39;, &#39;validation/main/accuracy&#39;]))\ntrainer.extend(extensions.PlotReport([&#39;main/loss&#39;, &#39;validation/main/loss&#39;], x_key=&#39;epoch&#39;, file_name=&#39;loss.png&#39;))\ntrainer.extend(extensions.PlotReport([&#39;main/accuracy&#39;, &#39;validation/main/accuracy&#39;], x_key=&#39;epoch&#39;, file_name=&#39;accuracy.png&#39;))\ntrainer.extend(extensions.ProgressBar())</code></pre></div><p>在chainer中可视化是非常方便的，我们常通过trainer.extend()来实现我们的可视化，其有下面几种可视化的方式。</p><div class=\"highlight\"><pre><code class=\"language-text\">1、chainer.training.extensions.PrintReport    \n2、chainer.training.extensions.ProgressBar   \n3、chainer.training.extensions.LogReport    \n4、chainer.training.extensions.PlotReport   \n5、chainer.training.extensions.VariableStatisticsPlot  \n6、chainer.training.extensions.dump_graph    </code></pre></div><p>以上就是利用chain来做一个图像分类任务的一个小例子。完整代码可以看配套的git项目，我们看看训练中的记录，如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-3a80e97ec1f497b0843840f04e8f1a0b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"798\" data-rawheight=\"436\" class=\"origin_image zh-lightbox-thumb\" width=\"798\" data-original=\"https://pic4.zhimg.com/v2-3a80e97ec1f497b0843840f04e8f1a0b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;798&#39; height=&#39;436&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"798\" data-rawheight=\"436\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"798\" data-original=\"https://pic4.zhimg.com/v2-3a80e97ec1f497b0843840f04e8f1a0b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-3a80e97ec1f497b0843840f04e8f1a0b_b.jpg\"/></figure><p><b>总结</b> ：</p><p>本文讲解了如何使用chainer深度学习框架完成一个分类任务，尽管这个框架用的人不多，但这个框架使用起来还是比较方便的，您在用吗？如果您在用，可以联系我们一起交流下！</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-60feffab1842911ce09ae93eeaae724f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"3999\" data-rawheight=\"2250\" class=\"origin_image zh-lightbox-thumb\" width=\"3999\" data-original=\"https://pic4.zhimg.com/v2-60feffab1842911ce09ae93eeaae724f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;3999&#39; height=&#39;2250&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"3999\" data-rawheight=\"2250\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"3999\" data-original=\"https://pic4.zhimg.com/v2-60feffab1842911ce09ae93eeaae724f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-60feffab1842911ce09ae93eeaae724f_b.jpg\"/></figure><blockquote>本系列完整文章：</blockquote><p>第一篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029846%26idx%3D1%26sn%3D0c343cfd0ede5c8ae1405bd6348aefad%26chksm%3D871342abb064cbbd7fe31fb3c55f23875f27e48fb8354e9855823b1701f1227c71b4eb00de50%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【caffe速成】caffe图像分类从模型自定义到测试</a></p><p>第二篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029846%26idx%3D2%26sn%3D7c2582243bcd8f8b491e8e466a21978f%26chksm%3D871342abb064cbbd0cba24b408ceda2b64a7c8b6baa07f9f8f56cd4d1233caa0b80fe357753e%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【tensorflow速成】Tensorflow图像分类从模型自定义到测试</a></p><p>第三篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029881%26idx%3D1%26sn%3D3c869fcee3b48d3582952ab9a0683ea6%26chksm%3D87134284b064cb924c5e7231b3f2c36ba27e3a689b067f569f2e086f62b18413bcebc5987a07%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【pytorch速成】Pytorch图像分类从模型自定义到测试</a></p><p>第四篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029887%26idx%3D1%26sn%3D645b97809c24922352a0b39f19c9ef0c%26chksm%3D87134282b064cb9441af68124d205d9c7dedcaeb09788f4d586b949584e556eddd3a72217f69%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【paddlepaddle速成】paddlepaddle图像分类从模型自定义到测试</a></p><p>第五篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029896%26idx%3D1%26sn%3Df3f7b9cf69c514f45d1d14205f879270%26chksm%3D87134375b064ca6323354c40f3e55b02ff0d1d24f3dacfc980190d51f20ec9ec16e60c1a4741%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【Keras速成】Keras图像分类从模型自定义到测试</a></p><p>第六篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029904%26idx%3D1%26sn%3D0bdc6947f5ac68e7f68426b9d076b4ab%26chksm%3D8713436db064ca7b3b2a2a1d6a8d24c15069f72655e2c39e2498051fa0e56bbcf6fe9c332d5b%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【mxnet速成】mxnet图像分类从模型自定义到测试</a></p><p>第七篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649030976%26idx%3D1%26sn%3D0befc170a93d365b780c5f05b2f510a4%26chksm%3D8712bf3db065362b0aeaae82bdbac467be5697b34cac2797e2ee15cdcb97474c08640482bf07%26token%3D1695328272%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【cntk速成】cntk图像分类从模型自定义到测试</a></p><p>第八篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26tempkey%3DMTAwMF93Q3VDbklva2NBZDJXV1dnNWloZjJ2YWtZWmJ2aTRRaGVDMVNfbERSN2xVYXdXd3B4eWczWWdMbnQzRjZrRVpmSGRYMndrLTQ0VnhPcURTNGhsS3dYWGd3MTk0UGtmZkltU0U2U01OUmRfQzcySDVObDlxM3R2U3Q0SlNQWGRMc1NZeWtWTlVRN1NpRlJaWXRYcGdydUNOTUR6clUzVkNleVlKUE5nfn4%253D%26chksm%3D0712ba9b3065338de13ea030951ba2c25c0363d958c905e12c937c2acffa991313abb4d9f751%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【chainer速成】chainer图像分类从模型自定义到测试</a></p><p>第九篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649032012%26idx%3D1%26sn%3Df74c7084621f367adb2518ebec61ca42%26chksm%3D8712bb31b06532270b9ca9550ab48ff78adaad7d38c73645cfb8888218b8e177bebd5d401aa9%26token%3D1258619827%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【DL4J速成】Deeplearning4j图像分类从模型自定义到测试</a></p><p>第十篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649032109%26idx%3D2%26sn%3Da6ff48ec0ae5d8e7a494df7e564d9ac9%26chksm%3D8712bbd0b06532c61d98c786ba1773c29c3dcf1291f9c3cd052c5643e24699eef28481e94b8e%26token%3D1258619827%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【MatConvnet速成】MatConvnet图像分类从模型自定义到测试</a></p><p>第十一篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649032128%26idx%3D2%26sn%3Df889e2255c0ec4960f76ad0383363849%26chksm%3D8712bbbdb06532ab136c0a485bebc5cf181f24a34bec7bab3f97f66be627e09d939b997dae90%26token%3D598159941%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【Lasagne速成】Lasagne/Theano图像分类从模型自定义到测试</a></p><p>第十二篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649032137%26idx%3D2%26sn%3Da9770ead4a9d03f0a4e75be86657453b%26chksm%3D8712bbb4b06532a27f0f02ac30ce4a18eeac1f72ac54b3c5c1b6ec13fee29f03bfc0dcaa1efd%26token%3D1446627305%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【darknet速成】Darknet图像分类从模型自定义到测试</a></p><p></p>", 
            "topic": [
                {
                    "tag": "开源软件", 
                    "tagLink": "https://api.zhihu.com/topics/19552811"
                }, 
                {
                    "tag": "Chainer", 
                    "tagLink": "https://api.zhihu.com/topics/20073057"
                }, 
                {
                    "tag": "TensorFlow：实战Google深度学习框架（书籍）", 
                    "tagLink": "https://api.zhihu.com/topics/20135062"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/54899446", 
            "userName": "言有三-龙鹏", 
            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
            "upvote": 2, 
            "title": "【cntk速成】cntk图像分类从模型自定义到测试", 
            "content": "<p> 欢迎来到专栏<b>《2小时玩转开源框架系列》，这是我们第七篇，前面已经说过了caffe，tensorflow，pytorch，mxnet，keras，paddlepaddle</b>。</p><p>今天说cntk，本文所用到的数据，代码请参考我们官方git：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/longpeng2008/LongPeng_ML_Course\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/longpeng2008</span><span class=\"invisible\">/LongPeng_ML_Course</span><span class=\"ellipsis\"></span></a></p><h2><b>01 CNTK是什么</b></h2><p>地址：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Microsoft/CNTK\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/Microsoft/CN</span><span class=\"invisible\">TK</span><span class=\"ellipsis\"></span></a></p><p>CNTK是微软开源的深度学习工具包，它通过有向图将神经网络描述为一系列计算步骤。在有向图中，叶节点表示输入值或网络参数，而其他节点表示其输入上的矩阵运算。 </p><p>CNTK允许用户非常轻松地实现和组合流行的模型，包括前馈DNN，卷积网络（CNN）和循环网络（RNN / LSTM）。与目前大部分框架一样，实现了自动求导，利用随机梯度下降方法进行优化。</p><p>cntk有什么特点呢？</p><p><b>1.1 性能较高</b></p><p>按照其官方的说法，比其他的开源框架性能都更高。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-b9ace0a942ac63cbdf6a656cc9a0e608_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"626\" data-rawheight=\"352\" class=\"origin_image zh-lightbox-thumb\" width=\"626\" data-original=\"https://pic1.zhimg.com/v2-b9ace0a942ac63cbdf6a656cc9a0e608_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;626&#39; height=&#39;352&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"626\" data-rawheight=\"352\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"626\" data-original=\"https://pic1.zhimg.com/v2-b9ace0a942ac63cbdf6a656cc9a0e608_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-b9ace0a942ac63cbdf6a656cc9a0e608_b.jpg\"/></figure><p>笔者在实际进行实验的时候，确实也发现它的训练比较快。</p><p><b>1.2 适合做语音</b></p><p>CNTK本就是微软语音团队开源的，自然是更合适做语音任务，<b>使用RNN等模型，以及在时空尺度分别进行卷积非常容易</b>。</p><p>当然，现在的背靠python的这些框架已经大同小异，未来实现大一统并非不可能。</p><h2><b>02 CNTK模型训练</b></h2><p>pip安装一条命令即可，可以选择安装cpu或者gpu版本。</p><p>pip install cntk/cntk-gpu。</p><p>接下来就是数据的准备，模型的定义，结果的保存与分析。</p><p>在此之前，我们先看官方的分类案例，直观感受一下，代码比较长。</p><div class=\"highlight\"><pre><code class=\"language-text\">from __future__ import print_function\nimport numpy as np\nimport cntk as C\nfrom cntk.learners import sgd\nfrom cntk.logging import ProgressPrinter\nfrom cntk.layers import Dense, Sequential\ndef generate_random_data(sample_size, feature_dim, num_classes):\n    # Create synthetic data using NumPy.\n    Y = np.random.randint(size=(sample_size, 1), low=0, high=num_classes)\n\n    # Make sure that the data is separable\n    X = (np.random.randn(sample_size, feature_dim) + 3) * (Y + 1)\n    X = X.astype(np.float32)\n    # converting class 0 into the vector &#34;1 0 0&#34;,\n    # class 1 into vector &#34;0 1 0&#34;, ...\n    class_ind = [Y == class_number for class_number in range(num_classes)]\n    Y = np.asarray(np.hstack(class_ind), dtype=np.float32)\n    return X, Ydef ffnet():\n   inputs = 2\n   outputs = 2\n   layers = 2\n   hidden_dimension = 50\n\n   # input variables denoting the features and label data\n   features = C.input_variable((inputs), np.float32)\n   label = C.input_variable((outputs), np.float32)\n\n   # Instantiate the feedforward classification model\n   my_model = Sequential ([\n                   Dense(hidden_dimension, activation=C.sigmoid),\n                   Dense(outputs)])\n   z = my_model(features)\n\n   ce = C.cross_entropy_with_softmax(z, label)\n   pe = C.classification_error(z, label)\n\n   # Instantiate the trainer object to drive the model training\n   lr_per_minibatch = C.learning_parameter_schedule(0.125)\n   progress_printer = ProgressPrinter(0)\n   trainer = C.Trainer(z, (ce, pe), [sgd(z.parameters, lr=lr_per_minibatch)], [progress_printer])\n\n   # Get minibatches of training data and perform model training\n   minibatch_size = 25\n   num_minibatches_to_train = 1024\n\n   aggregate_loss = 0.0\n   for i in range(num_minibatches_to_train):\n       train_features, labels = generate_random_data(minibatch_size, inputs, outputs)\n       # Specify the mapping of input variables in the model to actual minibatch data to be trained with\n       trainer.train_minibatch({features : train_features, label : labels})\n       sample_count = trainer.previous_minibatch_sample_count\n       aggregate_loss += trainer.previous_minibatch_loss_average * sample_count\n\n   last_avg_error = aggregate_loss / trainer.total_number_of_samples_seen\n\n   test_features, test_labels = generate_random_data(minibatch_size, inputs, outputs)\n   avg_error = trainer.test_minibatch({features : test_features, label : test_labels})\n   print(&#39; error rate on an unseen minibatch: {}&#39;.format(avg_error))\n   return last_avg_error, avg_errornp.random.seed(98052)ffnet()</code></pre></div><p>上面就是一个两层的全连接神经网络，使用input_variable封装数据，使用Sequential定义模型，使用train_minibatch({features : train_features, label : labels})来feed数据，与tf，pytorch等框架都是一样的，的确是没有什么好说的。</p><p><b>2.1 数据读取</b></p><p>这里需要用到接口，io.ImageDeserializer与C.io.StreamDefs，C.io.StreamDef。</p><p>它可以直接输入如下格式的txt文件用于图像分类问题。</p><div class=\"highlight\"><pre><code class=\"language-text\">../../../../datas/mouth/1/182smile.jpg1    \n../../../../datas/mouth/1/435smile.jpg1    \n../../../../datas/mouth/0/40neutral.jpg0    \n../../../../datas/mouth/1/206smile.jpg1    </code></pre></div><p>注意上面采用的分隔符是&#39;\\t&#39;，这一点与MXNet相同，与caffe不同，完整的解析代码如下：</p><div class=\"highlight\"><pre><code class=\"language-text\">C.io.MinibatchSource(C.io.ImageDeserializer(map_file, C.io.StreamDefs(    \nfeatures = C.io.StreamDef(field=&#39;image&#39;, transforms=transforms),    \nlabels   = C.io.StreamDef(field=&#39;label&#39;, shape=num_classes)    \n)))    </code></pre></div><p>在对图像数据进行封装的时候，添加了transform，所以可以在这里进行数据预处理操作。</p><p>常用的裁剪与缩放如下：</p><div class=\"highlight\"><pre><code class=\"language-text\">transform.crop(crop_type=&#39;randomside&#39;, side_ratio=0.8)\ntransform.scale(width=image_width, height=image_height, channels=num_channels, interpolations=&#39;linear&#39;)</code></pre></div><p>C.io.MinibatchSource的返回就是数据指针，可以直接用于训练。</p><p><b>2.2 网络定义</b></p><p>与tensorflow和pytorch颇为相似，如下</p><div class=\"highlight\"><pre><code class=\"language-text\">def simpleconv3(input, out_dims):\n   with C.layers.default_options(init=C.glorot_uniform(), activation=C.relu):\n       net = C.layers.Convolution((3,3), 12, pad=True)(input)\n       net = C.layers.MaxPooling((3,3), strides=(2,2))(net)\n\n       net = C.layers.Convolution((3,3), 24, pad=True)(net)\n       net = C.layers.MaxPooling((3,3), strides=(2,2))(net)\n\n       net = C.layers.Convolution((3,3), 48, pad=True)(net)\n       net = C.layers.MaxPooling((3,3), strides=(2,2))(net)\n\n       net = C.layers.Dense(128)(net)\n       net = C.layers.Dense(out_dims, activation=None)(net)\n\n   return net</code></pre></div><p><b>2.3 损失函数与分类错误率指标定义</b></p><p>如下，model_func就是上面的net，input_var_norm和label_var分别就是数据和标签。</p><div class=\"highlight\"><pre><code class=\"language-text\">z = model_func(input_var_norm, out_dims=2)    \nce = C.cross_entropy_with_softmax(z, label_var)    \npe = C.classification_error(z, label_var)   </code></pre></div><p><b>2.4 训练参数</b></p><p>就是学习率，优化方法，epoch等配置。</p><div class=\"highlight\"><pre><code class=\"language-text\">epoch_size     = 900    \nminibatch_size = 64      \nlr_per_minibatch       = C.learning_rate_schedule([0.01]*100 + [0.003]*100 + [0.001],    \nC.UnitType.minibatch, epoch_size)    \nm = C.momentum_schedule(0.9)    \nl2_reg_weight          = 0.001    \nlearner = C.momentum_sgd(z.parameters,    \nlr = lr_per_minibatch,    \nmomentum = m,    \nl2_regularization_weight=l2_reg_weight)    \nprogress_printer = C.logging.ProgressPrinter(tag=&#39;Training&#39;, num_epochs=max_epochs)    \ntrainer = C.Trainer(z, (ce, pe), [learner], [progress_printer])    </code></pre></div><p>注意学习率的配置比较灵活，通过learning_rate_schedule接口，上面的C.learning_rate_schedule([0.01]*100 + [0.003]*100 + [0.001]意思是，在0～100 epoch，使用0.01的学习率，100～100+100 epoch，使用0.003学习率，此后使用0.001学习率。</p><p><b>2.5 训练与保存</b></p><p>使用数据指针的next_minibatch获取训练数据，trainer的train_minibatch进行训练，可以看出cntk非常强调minibatch的概念，实际上学习率和优化方法都可以针对单个样本进行设置。</p><div class=\"highlight\"><pre><code class=\"language-text\">for epoch in range(max_epochs):    \n   sample_count = 0      \n   while sample_count &lt; epoch_size:    \n      data = reader_train.next_minibatch(min(minibatch_size, epoch_size - sample_count), input_map=input_map)    \n      trainer.train_minibatch(data)    </code></pre></div><p>模型的保存就一行代码：</p><div class=\"highlight\"><pre><code class=\"language-text\">z.save(&#34;simpleconv3.dnn&#34;)</code></pre></div><p><b>2.6 可视化</b></p><p>需要可视化的内容不多，就是loss曲线和精度曲线，所以可以直接自己添加代码，用上面的模型训练最后的loss如下，更好参数可自己调。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-fce7b54e417d162040aad06adbcd584d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"287\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic2.zhimg.com/v2-fce7b54e417d162040aad06adbcd584d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;640&#39; height=&#39;287&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"287\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https://pic2.zhimg.com/v2-fce7b54e417d162040aad06adbcd584d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-fce7b54e417d162040aad06adbcd584d_b.jpg\"/></figure><h2><b>03 CNTK模型测试</b></h2><p>测试就是载入模型，做好与训练时同样的预处理操作然后forward就行了。</p><div class=\"highlight\"><pre><code class=\"language-text\">import ***\nmodel_file = sys.argv[1]\nimage_list = sys.argv[2]\nmodel = C.load_model(model_file)\n\ncount = 0\nacc = 0\nimagepaths = open(image_list,&#39;r&#39;).readlines()\nfor imagepath in imagepaths:\n   imagepath,label = imagepath.strip().split(&#39;\\t&#39;)\n   im = Image.open(imagepath)\n   print imagepath\n   print &#34;im size&#34;,im.size\n   image_data = np.array(im,dtype=np.float32)\n   image_data = cv2.resize(image_data,(image_width,image_height))\n   image_data = np.ascontiguousarray(np.transpose(image_data, (2, 0, 1)))\n   output = model.eval({model.arguments[0]:[image_data]})[0]\n   print output\n   print label,np.argmax(np.squeeze(output))\n   if str(label) == str(np.argmax(np.squeeze(output))):\n       acc = acc + 1\n   count = count + 1\nprint &#34;acc=&#34;,float(acc) / float(count)</code></pre></div><p>最终模型训练集准确率91%，测试集准确率88%，大家可以自己去做更多调试。</p><h2><b>总结</b></h2><p>相比于tensorflow，pytorch，cntk固然是比较小众，但也不失为一个优秀的平台，尤其是对于语音任务，感兴趣大家可以自行体验。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-60feffab1842911ce09ae93eeaae724f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"3999\" data-rawheight=\"2250\" class=\"origin_image zh-lightbox-thumb\" width=\"3999\" data-original=\"https://pic4.zhimg.com/v2-60feffab1842911ce09ae93eeaae724f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;3999&#39; height=&#39;2250&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"3999\" data-rawheight=\"2250\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"3999\" data-original=\"https://pic4.zhimg.com/v2-60feffab1842911ce09ae93eeaae724f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-60feffab1842911ce09ae93eeaae724f_b.jpg\"/></figure><blockquote>本系列完整文章：</blockquote><p>第一篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029846%26idx%3D1%26sn%3D0c343cfd0ede5c8ae1405bd6348aefad%26chksm%3D871342abb064cbbd7fe31fb3c55f23875f27e48fb8354e9855823b1701f1227c71b4eb00de50%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【caffe速成】caffe图像分类从模型自定义到测试</a></p><p>第二篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029846%26idx%3D2%26sn%3D7c2582243bcd8f8b491e8e466a21978f%26chksm%3D871342abb064cbbd0cba24b408ceda2b64a7c8b6baa07f9f8f56cd4d1233caa0b80fe357753e%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【tensorflow速成】Tensorflow图像分类从模型自定义到测试</a></p><p>第三篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029881%26idx%3D1%26sn%3D3c869fcee3b48d3582952ab9a0683ea6%26chksm%3D87134284b064cb924c5e7231b3f2c36ba27e3a689b067f569f2e086f62b18413bcebc5987a07%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【pytorch速成】Pytorch图像分类从模型自定义到测试</a></p><p>第四篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029887%26idx%3D1%26sn%3D645b97809c24922352a0b39f19c9ef0c%26chksm%3D87134282b064cb9441af68124d205d9c7dedcaeb09788f4d586b949584e556eddd3a72217f69%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【paddlepaddle速成】paddlepaddle图像分类从模型自定义到测试</a></p><p>第五篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029896%26idx%3D1%26sn%3Df3f7b9cf69c514f45d1d14205f879270%26chksm%3D87134375b064ca6323354c40f3e55b02ff0d1d24f3dacfc980190d51f20ec9ec16e60c1a4741%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【Keras速成】Keras图像分类从模型自定义到测试</a></p><p>第六篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029904%26idx%3D1%26sn%3D0bdc6947f5ac68e7f68426b9d076b4ab%26chksm%3D8713436db064ca7b3b2a2a1d6a8d24c15069f72655e2c39e2498051fa0e56bbcf6fe9c332d5b%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【mxnet速成】mxnet图像分类从模型自定义到测试</a></p><p>第七篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649030976%26idx%3D1%26sn%3D0befc170a93d365b780c5f05b2f510a4%26chksm%3D8712bf3db065362b0aeaae82bdbac467be5697b34cac2797e2ee15cdcb97474c08640482bf07%26token%3D1695328272%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【cntk速成】cntk图像分类从模型自定义到测试</a></p><p>第八篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26tempkey%3DMTAwMF93Q3VDbklva2NBZDJXV1dnNWloZjJ2YWtZWmJ2aTRRaGVDMVNfbERSN2xVYXdXd3B4eWczWWdMbnQzRjZrRVpmSGRYMndrLTQ0VnhPcURTNGhsS3dYWGd3MTk0UGtmZkltU0U2U01OUmRfQzcySDVObDlxM3R2U3Q0SlNQWGRMc1NZeWtWTlVRN1NpRlJaWXRYcGdydUNOTUR6clUzVkNleVlKUE5nfn4%253D%26chksm%3D0712ba9b3065338de13ea030951ba2c25c0363d958c905e12c937c2acffa991313abb4d9f751%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【chainer速成】chainer图像分类从模型自定义到测试</a></p><p>第九篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649032012%26idx%3D1%26sn%3Df74c7084621f367adb2518ebec61ca42%26chksm%3D8712bb31b06532270b9ca9550ab48ff78adaad7d38c73645cfb8888218b8e177bebd5d401aa9%26token%3D1258619827%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【DL4J速成】Deeplearning4j图像分类从模型自定义到测试</a></p><p>第十篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649032109%26idx%3D2%26sn%3Da6ff48ec0ae5d8e7a494df7e564d9ac9%26chksm%3D8712bbd0b06532c61d98c786ba1773c29c3dcf1291f9c3cd052c5643e24699eef28481e94b8e%26token%3D1258619827%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【MatConvnet速成】MatConvnet图像分类从模型自定义到测试</a></p><p>第十一篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649032128%26idx%3D2%26sn%3Df889e2255c0ec4960f76ad0383363849%26chksm%3D8712bbbdb06532ab136c0a485bebc5cf181f24a34bec7bab3f97f66be627e09d939b997dae90%26token%3D598159941%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【Lasagne速成】Lasagne/Theano图像分类从模型自定义到测试</a></p><p>第十二篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649032137%26idx%3D2%26sn%3Da9770ead4a9d03f0a4e75be86657453b%26chksm%3D8712bbb4b06532a27f0f02ac30ce4a18eeac1f72ac54b3c5c1b6ec13fee29f03bfc0dcaa1efd%26token%3D1446627305%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【darknet速成】Darknet图像分类从模型自定义到测试</a></p>", 
            "topic": [
                {
                    "tag": "CNTK", 
                    "tagLink": "https://api.zhihu.com/topics/20046732"
                }, 
                {
                    "tag": "开源项目", 
                    "tagLink": "https://api.zhihu.com/topics/19565961"
                }, 
                {
                    "tag": "微软亚洲研究院", 
                    "tagLink": "https://api.zhihu.com/topics/19612923"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/51012194", 
            "userName": "言有三-龙鹏", 
            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
            "upvote": 173, 
            "title": "【技术综述】万字长文详解Faster RCNN源代码（一）", 
            "content": "<p>首发于《有三AI》</p><a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/Bnibfng4Sv6qbMk5BEFwnQ\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic2.zhimg.com/v2-0bbf50e70d464dde8021140a6051652d_180x120.jpg\" data-image-width=\"1200\" data-image-height=\"900\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【技术综述】万字长文详解Faster RCNN源代码</a><p class=\"ztext-empty-paragraph\"><br/></p><p><i>Faster R-CNN将分成四部分介绍。总共有Faster R-CNN概述，py-faster-rcnn框架解读，网络分析，和训练与测试四部分内容。第一篇将介绍Faster R-CNN概述。下一篇可以详见【技术综述】万字长文详解Faster RCNN源代码（二）。</i></p><p class=\"ztext-empty-paragraph\"><br/></p><blockquote><b>1. Faster R-CNN概述</b></blockquote><div class=\"highlight\"><pre><code class=\"language-text\">1.1 基础</code></pre></div><p>目标检测任务关注的是图片中特定目标物体的位置。一个检测任务包含两个子任务，<b>其一是输出这一目标的类别信息，属于分类任务。其二是输出目标的具体位置信息，属于定位任务。</b></p><p>分类的结果是一个类别标签，对于单分类任务而言，它就是一个数，对于多分类任务，就是一个向量。定位任务的输出是一个位置，用矩形框表示，包含矩形框左上角或中间位置的x，y坐标和矩形框的宽度高度。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-7d4bc81991e646767ffb23915177ee92_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"850\" data-rawheight=\"455\" class=\"origin_image zh-lightbox-thumb\" width=\"850\" data-original=\"https://pic3.zhimg.com/v2-7d4bc81991e646767ffb23915177ee92_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;850&#39; height=&#39;455&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"850\" data-rawheight=\"455\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"850\" data-original=\"https://pic3.zhimg.com/v2-7d4bc81991e646767ffb23915177ee92_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-7d4bc81991e646767ffb23915177ee92_b.jpg\"/></figure><p>目标检测在生活中具有非常广泛的应用，它也经过了非常长的发展阶段。与计算机视觉领域里大部分的算法一样，也经历了<b>从传统的人工设计特征和浅层分类器的思路，到大数据时代使用深度神经网络进行特征学习</b>的思路这一过程。</p><p>相信大家已经看过很多目标检测的原理综述性文章，如果没有，就参考<b>本公众号今天发的另一篇文章《一文道尽R-CNN系列目标检测》。</b></p><p>本文包括两个部分：</p><ol><li>对经典算法Faster R-CNN的源代码进行详细的说明，选用的代码为caffe版本， 链接为：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/rbgirshick/py-faster-rcnn\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/rbgirshick/p</span><span class=\"invisible\">y-faster-rcnn</span><span class=\"ellipsis\"></span></a>。</li><li>基于该框架完成一个简单的实践。在正式解读代码之前，要先说清楚两个重要概念，rpn与roi pooling。</li></ol><div class=\"highlight\"><pre><code class=\"language-text\">1.2 roi pooing</code></pre></div><p>通常我们训练一次取多张图，也就是一个batch，一个batch中图大小一致，这是为了<b>从源头上把控，从而获得固定维度的特征</b>。所以在早期进行目标检测，会将候选的区域进行<b>裁剪或缩放到统一尺度，如下图红色框。</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-09671993d185a15edc6e681118a97d65_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"864\" data-rawheight=\"418\" class=\"origin_image zh-lightbox-thumb\" width=\"864\" data-original=\"https://pic2.zhimg.com/v2-09671993d185a15edc6e681118a97d65_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;864&#39; height=&#39;418&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"864\" data-rawheight=\"418\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"864\" data-original=\"https://pic2.zhimg.com/v2-09671993d185a15edc6e681118a97d65_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-09671993d185a15edc6e681118a97d65_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-34bfe648a6c434e6a2fb99b62975eec3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"864\" data-rawheight=\"291\" class=\"origin_image zh-lightbox-thumb\" width=\"864\" data-original=\"https://pic4.zhimg.com/v2-34bfe648a6c434e6a2fb99b62975eec3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;864&#39; height=&#39;291&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"864\" data-rawheight=\"291\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"864\" data-original=\"https://pic4.zhimg.com/v2-34bfe648a6c434e6a2fb99b62975eec3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-34bfe648a6c434e6a2fb99b62975eec3_b.jpg\"/></figure><p>副作用很明显，一次只选到了目标一部分，或者把目标变形了。</p><p><b>sppnet不从源头而是在最后一环的特征上做处理</b>，将任意尺度(大于4*4)大小的特征，进行3种pooling，串接起来得到固定的21维，从而避免了固定尺寸输入的约束，如下。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-28d288af8d257ddc8db8cf0d7549bc6a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1678\" data-rawheight=\"1020\" class=\"origin_image zh-lightbox-thumb\" width=\"1678\" data-original=\"https://pic3.zhimg.com/v2-28d288af8d257ddc8db8cf0d7549bc6a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1678&#39; height=&#39;1020&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1678\" data-rawheight=\"1020\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1678\" data-original=\"https://pic3.zhimg.com/v2-28d288af8d257ddc8db8cf0d7549bc6a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-28d288af8d257ddc8db8cf0d7549bc6a_b.jpg\"/></figure><p>当然你用的时候，不必要局限于4*4，2*2，1*1。</p><p>以前为了满足全连接层固定大小输入，需要在输入图进行缩放，然后提取特征。现在既然已经可以在特征空间进行缩放了，<b>那么就不必要求输入一样大了</b>。</p><p>原来的那些<b>只是因为在原始空间中有了轻微的位置或者尺寸改变就要重新提取特征的大量重复操作</b>，就不需要了，因为任意的原始图像中的输入是可以映射到特征图中的，卷积只会改变空间分辨率，不改变比例和位置。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-401523ddaecddbf0463ee51b0bb3d337_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"590\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-401523ddaecddbf0463ee51b0bb3d337_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;590&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"590\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-401523ddaecddbf0463ee51b0bb3d337_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-401523ddaecddbf0463ee51b0bb3d337_b.jpg\"/></figure><p>映射很简单，等比例缩放即可，实现时考虑好padding，stride等操作。</p><p>那这跟roi pooling有什么关系呢？</p><p>ROI pooling是一个简化的spp池化，不需要这么复杂，<b>直接一次分块pooling</b>就行了，<b>在经过了从原始空间到特征空间的映射之后，设定好一个pooled_w，一个pooled_h</b>，就是将W*H的输入pooling为pooled_w*pooled_h的特征图，然后放入全连接层。 它完成的下面的输入输出的变换。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-7e0c4ebb77621a6e99f45b67856ef31f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"108\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-7e0c4ebb77621a6e99f45b67856ef31f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;108&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"108\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-7e0c4ebb77621a6e99f45b67856ef31f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-7e0c4ebb77621a6e99f45b67856ef31f_b.jpg\"/></figure><div class=\"highlight\"><pre><code class=\"language-text\">1.3 RPN</code></pre></div><p>region proposal network，可知道这就是一个网络，而且是一个小小的网络。它<b>解决了roi pooling的输入问题</b>，就是如何得到一系列的proposal，也就是原图中的候选框。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-a37091c12b41d1f9da7ac64ed69dff4d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"1051\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-a37091c12b41d1f9da7ac64ed69dff4d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;1051&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"1051\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-a37091c12b41d1f9da7ac64ed69dff4d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-a37091c12b41d1f9da7ac64ed69dff4d_b.jpg\"/></figure><p>图比较难画就借用原文了，如上，而rpn网络的功能示意图如下。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-39c6bfdfb24d97c479dfabbf902ec878_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"966\" data-rawheight=\"968\" class=\"origin_image zh-lightbox-thumb\" width=\"966\" data-original=\"https://pic1.zhimg.com/v2-39c6bfdfb24d97c479dfabbf902ec878_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;966&#39; height=&#39;968&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"966\" data-rawheight=\"968\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"966\" data-original=\"https://pic1.zhimg.com/v2-39c6bfdfb24d97c479dfabbf902ec878_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-39c6bfdfb24d97c479dfabbf902ec878_b.jpg\"/></figure><p>最开始当我们想要去提取一些区域来做分类和回归时，那就穷举嘛，搞不同大小，不同比例的框，在图像中从左到右，从上到下滑动，如上图，一个就选了9种。这样计算量很大，所以selective search利用语义信息改进，避免了滑动，但还是需要在原图中操作，因为特征提取不能共享。</p><p>现在不是有了sppnet和roi pooling的框架，<b>把上面的这个在原图中进行穷举滑动的操作，换到了比原图小很多的特征空间（比如224*224 --&gt; 13*13），还是搞滑动窗口，就得到了rpn，如下图。</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-2370c497db0f997d6b8d5ff1e11787f6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"859\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-2370c497db0f997d6b8d5ff1e11787f6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;859&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"859\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-2370c497db0f997d6b8d5ff1e11787f6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-2370c497db0f997d6b8d5ff1e11787f6_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-34ac6b7a0c39d3cd3f8ecce099300771_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"84\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-34ac6b7a0c39d3cd3f8ecce099300771_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;84&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"84\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-34ac6b7a0c39d3cd3f8ecce099300771_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-34ac6b7a0c39d3cd3f8ecce099300771_b.jpg\"/></figure><p>rpn实现了了上面的输入输出。不同与简单滑窗的是，网络会对这个过程进行学习，得到更有效的框。</p><p>剩下的就是一些普通cnn的知识，不用多说，有了上面的这些基础后，我们开始解读代码。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>更多详细内容关注微信公众号：有三AI</p><p class=\"ztext-empty-paragraph\"><br/></p><a href=\"https://zhuanlan.zhihu.com/p/51013203\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic2.zhimg.com/v2-4517631ac09052e699a5aef3d288d9a5_180x120.jpg\" data-image-width=\"2000\" data-image-height=\"1023\" class=\"internal\">龙鹏：【技术综述】万字长文详解Faster RCNN源代码（二）</a><a href=\"https://zhuanlan.zhihu.com/p/51014564\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic2.zhimg.com/v2-4517631ac09052e699a5aef3d288d9a5_180x120.jpg\" data-image-width=\"2000\" data-image-height=\"1023\" class=\"internal\">龙鹏：【技术综述】万字长文详解Faster RCNN源代码（三）</a><a href=\"https://zhuanlan.zhihu.com/p/51014458\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic2.zhimg.com/v2-4517631ac09052e699a5aef3d288d9a5_180x120.jpg\" data-image-width=\"2000\" data-image-height=\"1023\" class=\"internal\">龙鹏：【技术综述】万字长文详解Faster RCNN源代码（四）</a><p></p>", 
            "topic": [
                {
                    "tag": "卷积神经网络（CNN）", 
                    "tagLink": "https://api.zhihu.com/topics/20043586"
                }, 
                {
                    "tag": "目标检测", 
                    "tagLink": "https://api.zhihu.com/topics/19596960"
                }, 
                {
                    "tag": "Caffe（深度学习框架）", 
                    "tagLink": "https://api.zhihu.com/topics/20019488"
                }
            ], 
            "comments": [
                {
                    "userName": "小杰", 
                    "userLink": "https://www.zhihu.com/people/78860e32c4d0242fbda567fa2de1cff3", 
                    "content": "该评论已删除", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "言有三-龙鹏", 
                            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
                            "content": "<p>看完了还不会运行吗😄，不会我们下次写个手把手教你一步步运行</p>", 
                            "likes": 1, 
                            "replyToAuthor": "小杰"
                        }
                    ]
                }, 
                {
                    "userName": "知乎用户", 
                    "userLink": "https://www.zhihu.com/people/0", 
                    "content": "<p>RPN，将了个大概，没有具体的细说</p>", 
                    "likes": 2, 
                    "childComments": [
                        {
                            "userName": "言有三-龙鹏", 
                            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
                            "content": "对的", 
                            "likes": 0, 
                            "replyToAuthor": "知乎用户"
                        }
                    ]
                }, 
                {
                    "userName": "叶不知", 
                    "userLink": "https://www.zhihu.com/people/487592dfc7915f766ada77f248672867", 
                    "content": "<p>这叫详解源码？</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "稳稳", 
                    "userLink": "https://www.zhihu.com/people/ac8d1e18a7eda5fb8219d6558869dce8", 
                    "content": "<p>我想问一下,经过1*1卷积层降维之后,输出的预测概率和预测回归参数是对20000anchor的预测吗还是.并且怎么将anchortargetcreator选出来的256个anchor所对应起来求loss呢</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "略略略", 
                    "userLink": "https://www.zhihu.com/people/e9087975c0b2243f3c9b55e794143f7d", 
                    "content": "<p>没感觉很详细，还不如直接去看论文</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "静水流深", 
                    "userLink": "https://www.zhihu.com/people/e70f1e17623180012977724c2d615157", 
                    "content": "<p>很详细</p><p></p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "言有三-龙鹏", 
                            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
                            "content": "🤝🤝🤝谢谢", 
                            "likes": 0, 
                            "replyToAuthor": "静水流深"
                        }
                    ]
                }, 
                {
                    "userName": "佛小九", 
                    "userLink": "https://www.zhihu.com/people/0bf75b1ddee536ce6ceccb8332addd0e", 
                    "content": "真的很好", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "言有三-龙鹏", 
                            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
                            "content": "谢谢🙏", 
                            "likes": 0, 
                            "replyToAuthor": "佛小九"
                        }
                    ]
                }, 
                {
                    "userName": "Mengcius", 
                    "userLink": "https://www.zhihu.com/people/a68987c65f70664cc653c58cc99315b9", 
                    "content": "请问RPN中的3*3卷积起什么作用？一个滑动窗口是对应9个anchor吗？", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "言有三-龙鹏", 
                            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
                            "content": "三种形状三种尺度就是9个，卷积都是用于提取信息", 
                            "likes": 0, 
                            "replyToAuthor": "Mengcius"
                        }, 
                        {
                            "userName": "Mengcius", 
                            "userLink": "https://www.zhihu.com/people/a68987c65f70664cc653c58cc99315b9", 
                            "content": "那每个anchor是怎么对应到RPN中间那个256维的向量的呢？一个anchor对应一个向量吗？还是9个anchor对应一个？[好奇]", 
                            "likes": 0, 
                            "replyToAuthor": "言有三-龙鹏"
                        }
                    ]
                }, 
                {
                    "userName": "轩辕十四", 
                    "userLink": "https://www.zhihu.com/people/b366ca09e7a2b3f188475f5336e9f3c2", 
                    "content": "<p>标题党，这也叫详解，哈哈哈</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/51013203", 
            "userName": "言有三-龙鹏", 
            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
            "upvote": 28, 
            "title": "【技术综述】万字长文详解Faster RCNN源代码（二）", 
            "content": "<p>首发于《有三AI》</p><a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/Bnibfng4Sv6qbMk5BEFwnQ\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic2.zhimg.com/v2-0bbf50e70d464dde8021140a6051652d_180x120.jpg\" data-image-width=\"1200\" data-image-height=\"900\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【技术综述】万字长文详解Faster RCNN源代码</a><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-a6ac134cfaa85c8d21960296fe0a13e3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1402\" data-rawheight=\"416\" class=\"origin_image zh-lightbox-thumb\" width=\"1402\" data-original=\"https://pic4.zhimg.com/v2-a6ac134cfaa85c8d21960296fe0a13e3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1402&#39; height=&#39;416&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1402\" data-rawheight=\"416\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1402\" data-original=\"https://pic4.zhimg.com/v2-a6ac134cfaa85c8d21960296fe0a13e3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-a6ac134cfaa85c8d21960296fe0a13e3_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><i>Faster R-CNN将分成四部分介绍。总共有Faster R-CNN概述，py-faster-rcnn框架解读，网络分析，和训练与测试四部分内容。第二篇将对py-faster-rcnn框架进行解读。由于此部分内容较多，包括了caffe-fast-rcnn，data，experiments，Models目录，Lib目录，和tools 目录这六部分，所以分成两篇介绍。下一篇可以详见【技术综述】万字长文详解Faster RCNN源代码（三）。</i></p><p class=\"ztext-empty-paragraph\"><br/></p><blockquote><b>2. py-faster-rcnn框架解读</b></blockquote><p>Faster R-CNN源代码的熟悉几乎是所有从事目标检测的人员必须迈过的坎，由于目录结构比较复杂而且广泛为人所用，涉及的东西非常多，所以我们使用的时候维持该目录结构不变，下面首先对它的目录结构进行完整的分析。</p><p>目录下包括caffe-fast-rcnn，data，experiments，lib，models，tools几大模块。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-d384a0c6a341dc2727b48b7cbb7c6e84_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"613\" data-rawheight=\"229\" class=\"origin_image zh-lightbox-thumb\" width=\"613\" data-original=\"https://pic1.zhimg.com/v2-d384a0c6a341dc2727b48b7cbb7c6e84_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;613&#39; height=&#39;229&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"613\" data-rawheight=\"229\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"613\" data-original=\"https://pic1.zhimg.com/v2-d384a0c6a341dc2727b48b7cbb7c6e84_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-d384a0c6a341dc2727b48b7cbb7c6e84_b.jpg\"/></figure><div class=\"highlight\"><pre><code class=\"language-text\">2.1 caffe-fast-rcnn</code></pre></div><p>这是rcnn系列框架的caffe，因为目标检测中使用到了很多官方caffe中不包括的网络层，所以必须进行定制。这里需要注意的是caffe是以子模块的方式被包含在其中，所以使用git clone命令下载代码将得到空文件夹，必须要加上递归参数-recursive，具体做法如下：</p><blockquote>git clone --recursive <a href=\"https://link.zhihu.com/?target=https%3A//github.com/rbgirshick/py-faster-rcnn.git\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/rbgirshick/p</span><span class=\"invisible\">y-faster-rcnn.git</span><span class=\"ellipsis\"></span></a></blockquote><div class=\"highlight\"><pre><code class=\"language-text\">2.2 data</code></pre></div><p>该文件夹下包含两个子文件夹，一个是scripts，一个是demo。其中demo就是用于存放测试的图像，script存储着若干脚本，它用于获取一些预训练的模型。</p><p>比如运行fetch_imagenet_models.sh脚本，会在当前的文件夹下建立imagenet_models目录，并下载VGG_CNN_M_1024.v2.caffemodel VGG16.v2.caffemodel ZF.v2.caffemodel模型，这些是在ImageNet上预先训练好的模型，将用于初始化我们的检测模型的训练。</p><p>另外还在该目录下建立数据集的软链接。通常情况下，对于一些通用的数据集，我们会将它们放在公用的目录而不是某一个项目下，所以这里通常需要建立通用数据集的软链接，比如PASCAL VOC的目录。</p><blockquote>ln -s /your/path/to/VOC2012/VOCdevkit VOCdevkit2012</blockquote><div class=\"highlight\"><pre><code class=\"language-text\">2.3 experiments</code></pre></div><p>下面分为3个目录，分别是log，cfgs，scripts。cfgs存放的就是配置文件，比如faster_rcnn_end2end的配置如下:</p><blockquote>EXP_DIR: faster_rcnn_end2end<br/>TRAIN:<br/>  HAS_RPN: True<br/>  IMS_PER_BATCH: 1<br/>  BBOX_NORMALIZE_TARGETS_PRECOMPUTED: True<br/>  RPN_POSITIVE_OVERLAP: 0.7<br/>  RPN_BATCHSIZE: 256<br/>  PROPOSAL_METHOD: gt<br/>  BG_THRESH_LO: 0.0<br/>TEST:<br/>  HAS_RPN: True</blockquote><p>其中比较重要的包括HAS_RPN，RPN_BATCHSIZE等。Log目录就存放log日志。scripts存放的就是bash训练脚本，可以用end2end或者alt_opt两种方式训练。</p><p>每一个训练脚本都包含两个步骤，训练和测试。</p><blockquote>time ./tools/train_net.py --gpu ${GPU_ID} \\<br/>  --solver models/${PT_DIR}/${NET}/faster_rcnn_end2end/solver.prototxt \\<br/>  --weights data/imagenet_models/${NET}.v2.caffemodel \\<br/>  --imdb ${TRAIN_IMDB} \\<br/>  --iters ${ITERS} \\<br/>  --cfg experiments/cfgs/faster_rcnn_end2end.yml \\<br/>  ${EXTRA_ARGS}</blockquote><p>如上就是训练部分代码，对于默认的任务，我们不需要修改这里的代码，<b>但是如果我们不想用预训练，或者相关的配置发生了变化，比如yml格式的配置文件，预训练模型的前缀格式，需要配置自定义的数据集</b>等等，则需要修改此处代码。</p><div class=\"highlight\"><pre><code class=\"language-text\">2.4 Models目录</code></pre></div><p>该目录下包含两个文件夹，coco和pascal voc，可知是两个通用数据集。在各个数据集的子目录下存储了一系列模型结构的配置，如models/pascal/VGG_CNN_M_1024/目录，存储的就是用于训练coco数据集的VGG模型。</p><p>在该目录下，有fast_rcnn，faster_rcnn_alt_opt，faster_rcnn_end_to_end三套模型结构，各自有所不同。</p><p>fast_rcnn即fast_rcnn方法，它下面只包含了train.prototxt，test.prototxt，solver.prototxt三个文件，它对rcnn的改进主要在于重用了卷积特征，没有region proposal框架。</p><p>faster_rcnn_alt_opt，faster_rcnn_end_to_end都是faster rcnn框架，包括了region proposal模块。在faster_rcnn_alt_opt目录下，包含了4个训练文件和对应的solver文件，为：</p><blockquote>stage1_fast_rcnn_solver30k40k.pt，stage1_fast_rcnn_train.pt，stage1_rpn_solver60k80k.pt，stage1_rpn_train.pt，stage2_fast_rcnn_solver30k40k.pt，stage2_fast_rcnn_train.pt，<br/>stage2_rpn_solver60k80k.pt，stage2_rpn_train.pt。</blockquote><p>其中stage1过程是分别采用了ImageNet分类任务上训练好的模型进行region proposal的学习和faster rcnn检测的学习，stage2则是在stage1已经训练好的模型的基础上进行进一步的学习。</p><p>faster_rcnn_end_to_end就是端到端的训练方法，使用起来更加简单，所以我们这一小节会使用faster_rcnn_end_to_end方法，目录下只包括了train.prototxt，test.prototxt，solver.prototxt。</p><p>当要在我们自己的数据集上完成检测任务的时候，就可以建立与pascal_voc和coco平级的目录。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>更多详细内容关注微信公众号：有三AI</p><a href=\"https://zhuanlan.zhihu.com/p/51012194\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic2.zhimg.com/v2-4517631ac09052e699a5aef3d288d9a5_180x120.jpg\" data-image-width=\"2000\" data-image-height=\"1023\" class=\"internal\">龙鹏：【技术综述】万字长文详解Faster RCNN源代码（一）</a><a href=\"https://zhuanlan.zhihu.com/p/51014564\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic2.zhimg.com/v2-4517631ac09052e699a5aef3d288d9a5_180x120.jpg\" data-image-width=\"2000\" data-image-height=\"1023\" class=\"internal\">龙鹏：【技术综述】万字长文详解Faster RCNN源代码（三）</a><a href=\"https://zhuanlan.zhihu.com/p/51014458\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic2.zhimg.com/v2-4517631ac09052e699a5aef3d288d9a5_180x120.jpg\" data-image-width=\"2000\" data-image-height=\"1023\" class=\"internal\">龙鹏：【技术综述】万字长文详解Faster RCNN源代码（四）</a><p></p>", 
            "topic": [
                {
                    "tag": "目标检测", 
                    "tagLink": "https://api.zhihu.com/topics/19596960"
                }, 
                {
                    "tag": "计算机视觉", 
                    "tagLink": "https://api.zhihu.com/topics/19590195"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "卷积神经网络（CNN）", 
                    "tagLink": "https://api.zhihu.com/topics/20043586"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/51014564", 
            "userName": "言有三-龙鹏", 
            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
            "upvote": 64, 
            "title": "【技术综述】万字长文详解Faster RCNN源代码（三）", 
            "content": "<p>首发于《有三AI》</p><a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/Bnibfng4Sv6qbMk5BEFwnQ\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic2.zhimg.com/v2-0bbf50e70d464dde8021140a6051652d_180x120.jpg\" data-image-width=\"1200\" data-image-height=\"900\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【技术综述】万字长文详解Faster RCNN源代码</a><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-a6ac134cfaa85c8d21960296fe0a13e3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1402\" data-rawheight=\"416\" class=\"origin_image zh-lightbox-thumb\" width=\"1402\" data-original=\"https://pic4.zhimg.com/v2-a6ac134cfaa85c8d21960296fe0a13e3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1402&#39; height=&#39;416&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1402\" data-rawheight=\"416\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1402\" data-original=\"https://pic4.zhimg.com/v2-a6ac134cfaa85c8d21960296fe0a13e3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-a6ac134cfaa85c8d21960296fe0a13e3_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><i>Faster R-CNN将分成四部分介绍。总共有Faster R-CNN概述，py-faster-rcnn框架解读，网络分析，和训练与测试四部分内容。第三篇将续写上一篇继续对py-faster-rcnn框架进行解读。下一篇可以详见【技术综述】万字长文详解Faster RCNN源代码（四）。</i></p><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-text\">2.5 Lib目录</code></pre></div><p>lib目录下包含了非常多的子目录，包括datasets，fast_rcnn，nms，pycocotools，roi_data_layer，rpn，transform，utils，这是faster rcnn框架中很多方法的实现目录，下面对其进行详细解读。</p><p>(1) utils目录</p><p>这是最基础的一个目录，主要就是blob.py和bbox.pyx。blob.py用于将图像进行预处理，包括减去均值，缩放等操作，然后封装到caffe的blob中。</p><blockquote>for i in xrange(num_images):<br/>        im = ims[i]<br/>        blob[i, 0:im.shape[0], 0:im.shape[1], :] = im<br/>    channel_swap = (0, 3, 1, 2)<br/>    blob = blob.transpose(channel_swap)</blockquote><p>封装的核心代码如上，首先按照图像的存储格式(H,W,C)进行赋值，然后调整通道和高，宽的顺序，这在我们使用训练好的模型进行预测的时候是必要的操作。而且，有的时候采用RGB格式进行训练，有的使用采用BGR格式进行训练，也需要做对应的调整。</p><p>bbox.pyx用于计算两个box集合的overlaps，即重叠度。一个输入是(N,4)形状的真值boxes，一个输入是(K,4)形状的查询boxes，输出为(N,K)形状，即逐个box相互匹配的结果。</p><p>(2) datasets目录</p><p>datasets目录下有目录VOCdevkit-matlab-wrapper，tools，以及脚本coco.py，pascal_voc.py，voc_eval.py，factory.py，ds_util.py，imdb.py。</p><p>我们按照调用关系来看，首先是ds_util.py，它包含了一些最基础的函数，比如unique_boxes函数，可以通过不同尺度因子的缩放，从一系列的框中获取不重复框的数组指标，用于过滤重复框。</p><p>具体实现采用了hash的方法，代码如下：</p><blockquote>v = np.array([1, 1e3, 1e6, 1e9])<br/>hashes = np.round(boxes * scale).dot(v).astype(<a href=\"https://link.zhihu.com/?target=http%3A//np.int\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">np.int</span><span class=\"invisible\"></span></a>)</blockquote><p>box的4个坐标，与上面的v进行外积后转换为一个数，再进行判断。</p><p>xywh_to_xyxy，xyxy_to_xywh函数分别是框的两种表达形式的相互转换，前者采用一个顶点坐标和长宽表示矩形，后者采用两个顶点坐标表示矩形，各有用途。</p><p>validata_boxes函数用于去除无效框，即超出边界或者左右，上下，不满足几何关系的点，比如右边顶点的x坐标小于左边顶点。filer_small_boxes用于去掉过小的检测框。</p><p>接下来看imdb.py，这是数据集类imdb的定义脚本，非常重要。从它的初始化函数_init_可以看出，类成员变量包括数据集的名字self._name，检测的类别名字self._classes与具体的数量self._num_classes，候选区域的选取方法self._obj_proposer，roi数据集self._roidb与它的指针self._roidb_handler，候选框提取默认采用了selective_search方法。</p><p>roidb是它最重要的数据结构，它是一个数组。数组中的每一个元素其实就是一张图的属性，以字典的形式存储它的若干属性，共4个key，为boxes，gt_overlaps，gt_classes，flipped。</p><p>候选框boxes就是一个图像中的若干的box，每一个box是一个4维的向量，包含左上角和右下角的坐标。类别信息gt_classes，就是对应boxes中各个box的类别信息。真值gt_overlaps，它的维度大小为boxes的个数乘以类别的数量，可知存储的就是输入box和真实标注之间的重叠度。另外如果设置了变量flipped，还可以存储该图像的翻转版本，这就是一个镜像操作，是最常用的数据增强操作。</p><p>roidb的生成调用了create_roidb_from_box_list函数，它将输入的box_list添加到roidb中。如果没有gt_roidb的输入，那么就是下面的逻辑，可见就是将boxes存入数据库中，并初始化gt_overlaps，gt_classes等变量。</p><blockquote>boxes = box_list[i]<br/>num_boxes = boxes.shape[0]<br/>overlaps = np.zeros((num_boxes, self.num_classes), dtype=np.float32)<br/>overlaps = scipy.sparse.csr_matrix(overlaps)<br/>roidb.append({<br/>                &#39;boxes&#39; : boxes,<br/>                &#39;gt_classes&#39; : np.zeros((num_boxes,), dtype=np.int32),<br/>                &#39;gt_overlaps&#39; : overlaps,<br/>                &#39;flipped&#39; : False,<br/>                &#39;seg_areas&#39; : np.zeros((num_boxes,), dtype=np.float32),<br/>            })</blockquote><p>如果输入gt_roidb非空，则需要将输入的box与其进行比对计算得到gt_overlaps，代码如下：</p><blockquote>if gt_roidb is not None and gt_roidb[i][&#39;boxes&#39;].size &gt; 0:<br/>gt_boxes = gt_roidb[i][&#39;boxes&#39;]<br/>    gt_classes = gt_roidb[i][&#39;gt_classes&#39;]<br/>    gt_overlaps = bbox_overlaps(boxes.astype(np.float),gt_boxes.astype(np.float))<br/>    argmaxes = gt_overlaps.argmax(axis=1)<br/>    maxes = gt_overlaps.max(axis=1)<br/>I = np.where(maxes &gt; 0)[0]<br/>overlaps[I, gt_classes[argmaxes[I]]] = maxes[I]</blockquote><p>将输入的boxes与数据库中boxes进行比对，调用了bbox_overlaps函数。比对完之后结果存入 overlaps。</p><p>bbox_overlaps的结果overlaps是一个二维矩阵，第一维大小等于输入boxes中的框的数量，第二维就是类别数目，所存储的每一个值就是与真实标注进行最佳匹配的结果，即重叠度。但是最后存储的时候调用了overlaps = scipy.sparse.csr_matrix(overlaps)进行稀疏压缩，因为其中大部分的值其实是空的，一张图包含的类别数目有限。</p><p>还有一个变量gt_classes，在从该函数创建的时候并未赋值，即等于0，因为这个函数是用于将从rpn框架中返回的框添加到数据库中，并非是真实的标注。当gt_classes非零，说明是真实的标注，这样的数据集就是train或者val数据集，它们在一开始就被创建，反之则是test数据集。gt_classes非零的样本和为零的样本在数据集中是连续存储的。</p><p>该脚本中另一个重要的函数就是evaluate_recall，这就是用于计算average iou的函数。它的输入包括candidate_boxes，即候选框。假如没有输入，则评估时取该roidb中的非真值box。threholds，即IoU阈值，如果没有输入则默认从0.5到0.95，按照0.05的步长迭代。area，用于评估的面积大小阈值，默认覆盖0到1e10的尺度，尺度是指框的面积。还有一个limit，用于限制评估的框的数量。</p><p>返回平均召回率average recall，每一个IoU重合度阈值下的召回向量，设定的IoU阈值向量，以及所有的真值标签。</p><p>当进行评估的时候，首先要按照上面设计的面积大小阈值，得到有效的index。</p><blockquote>max_gt_overlaps = self.roidb[i][&#39;gt_overlaps&#39;].toarray().max(axis=1)<br/>gt_inds = np.where((self.roidb[i][&#39;gt_classes&#39;] &gt; 0) &amp;<br/>      (max_gt_overlaps == 1))[0]   首先获得需要评估的index<br/>gt_boxes = self.roidb[i][&#39;boxes&#39;][gt_inds, :]  得到对应的boxes<br/>gt_areas = self.roidb[i][&#39;seg_areas&#39;][gt_inds]<br/>valid_gt_inds = np.where((gt_areas &gt;= area_range[0]) &amp;<br/>       (gt_areas &lt;= area_range[1]))[0]  得到符合面积约束的index<br/>gt_boxes = gt_boxes[valid_gt_inds, :]<br/>num_pos += len(valid_gt_inds)  记录符合条件的框的个数</blockquote><p>计算重叠度的过程是对每一个真值box进行遍历，寻找到与其重叠度最大的候选框，得到各个真值box的被重叠度。挑选其中被重叠度最高的真值box，然后找到对应的与其重叠度最高的box，得到了一组匹配和相应的重叠度。标记这两个box，后续的迭代不再使用，然后循环计算，直到所有的真值框被遍历完毕。</p><p>pascal_voc.py和coco.py就是利用上面的几个脚本来创建对应这两个数据集的格式，用于后续对模型的测试，下面就是pascal voc的数据库的创建过程。</p><blockquote>def _load_pascal_annotation(self, index):<br/>        &#34;&#34;&#34;<br/>        Load image and bounding boxes info from XML file in the PASCAL VOC<br/>        format.<br/>        &#34;&#34;&#34;<br/>        filename = os.path.join(self._data_path, &#39;Annotations&#39;, index + &#39;.xml&#39;)<br/>        tree = ET.parse(filename)<br/>        objs = tree.findall(&#39;object&#39;)<br/>        if not self.config[&#39;use_diff&#39;]:<br/>            non_diff_objs = [<br/>                obj for obj in objs if int(obj.find(&#39;difficult&#39;).text) == 0]<br/>            objs = non_diff_objs<br/>        num_objs = len(objs)<br/>        boxes = np.zeros((num_objs, 4), dtype=np.uint16)<br/>        gt_classes = np.zeros((num_objs), dtype=np.int32)<br/>        overlaps = np.zeros((num_objs, self.num_classes), dtype=np.float32)<br/> <br/># &#34;Seg&#34; area for pascal is just the box area<br/>        seg_areas = np.zeros((num_objs), dtype=np.float32)<br/> <br/>        # Load object bounding boxes into a data frame.<br/>        for ix, obj in enumerate(objs):<br/>            bbox = obj.find(&#39;bndbox&#39;)<br/>            # Make pixel indexes 0-based<br/>            x1 = float(bbox.find(&#39;xmin&#39;).text) - 1<br/>            y1 = float(bbox.find(&#39;ymin&#39;).text) - 1<br/>            x2 = float(bbox.find(&#39;xmax&#39;).text) - 1<br/>            y2 = float(bbox.find(&#39;ymax&#39;).text) - 1<br/>            cls = self._class_to_ind[obj.find(&#39;name&#39;).text.lower().strip()]<br/>            boxes[ix, :] = [x1, y1, x2, y2]<br/>            gt_classes[ix] = cls<br/>            overlaps[ix, cls] = 1.0<br/>            seg_areas[ix] = (x2 - x1 + 1) * (y2 - y1 + 1)<br/> <br/>        overlaps = scipy.sparse.csr_matrix(overlaps)<br/> <br/>        return {&#39;boxes&#39; : boxes,<br/>                &#39;gt_classes&#39;: gt_classes,<br/>                &#39;gt_overlaps&#39; : overlaps,<br/>                &#39;flipped&#39; : False,<br/>                &#39;seg_areas&#39; : seg_areas}</blockquote><p>从上面脚本可知，输入就是xml格式的标注文件，通过obj变量获得x1，y1，x2，y2，即标注信息，以及cls类别信息，并标注overlaps等于1。另外seg_areas实际上就是标注框的面积。</p><p>(3) nms目录</p><p>该目录下主要是cpu和gpu版本的非极大值抑制计算方法，非极大抑制算法在目标检测中应用相当广泛，其主要目的是消除多余的框，找到最佳的物体检测位置。</p><p>实现的核心思想是首先将各个框的置信度进行排序，然后选择其中置信度最高的框A，将其作为标准，同时设置一个阈值。然后开始遍历其他框，当其他框B与A的重合程度超过阈值就将B舍弃掉，然后在剩余的框中选择置信度最大的框，重复上述操作。</p><p>我们以py_cpu_nms.py为例，并添加了注释。</p><blockquote>import numpy as np<br/>def py_cpu_nms(dets, thresh):<br/>    &#34;&#34;&#34;Pure Python NMS baseline.&#34;&#34;&#34;<br/>    x1 = dets[:, 0]<br/>    y1 = dets[:, 1]<br/>    x2 = dets[:, 2]<br/>    y2 = dets[:, 3]<br/>    scores = dets[:, 4]  <br/>    areas = (x2 - x1 + 1) * (y2 - y1 + 1)  #此处用于计算每一个框的面积<br/>    order = scores.argsort()[::-1]    #按照分数大小对其进行从高到低排序<br/> <br/>    keep = []<br/>    while order.size &gt; 0:<br/>        i = order[0]     #取分数最高的那个框<br/>        keep.append(i)  #保留这个框<br/>        xx1 = np.maximum(x1[i], x1[order[1:]])<br/>        yy1 = np.maximum(y1[i], y1[order[1:]])<br/>        xx2 = np.minimum(x2[i], x2[order[1:]])<br/>        yy2 = np.minimum(y2[i], y2[order[1:]])        #计算当前分数最大矩形框与其他矩形框的相交后的坐标<br/> <br/>        w = np.maximum(0.0, xx2 - xx1 + 1)<br/>        h = np.maximum(0.0, yy2 - yy1 + 1)     <br/>        inter = w * h      #计算相交框的面积<br/>        ovr = inter / (areas[i] + areas[order[1:]] - inter)  #计算IOU：重叠面积/(面积1+面积2-重叠面积)<br/> <br/>        inds = np.where(ovr &lt;= thresh)[0]   #取出IOU小于阈值的框<br/>        order = order[inds + 1]     #更新排序序列<br/> <br/>    return keep</blockquote><p>(4) roi data layer目录</p><p>该目录下有3个脚本，layer.py，minibatch.py，roidb.py。</p><p>layer.py包含了caffe的RoIDataLayer网络层的实现。通常来说一个caffe网络层的实现，需要包括setup，forward，backward等函数的实现，对于数据层还需实现shuffle，批量获取数据等函数。</p><p>Roidatalayer是一个数据层，也是训练时的输入层，其中最重要的函数是setup函数，用于设置各类输出数据的尺度信息。</p><p>根据是否有RPN模块，这两种情况下的配置是不一样的，我们直接看caffe的网络配置就能明白，比较fast rcnn和faster rcnn。</p><p>首先是fast rcnn：</p><blockquote>name: &#34;VGG_CNN_M_1024&#34;<br/>layer {<br/>  name: &#39;data&#39;<br/>  type: &#39;Python&#39;<br/>  top: &#39;data&#39;<br/>  top: &#39;rois&#39;<br/>  top: &#39;labels&#39;<br/>  top: &#39;bbox_targets&#39;<br/>  top: &#39;bbox_inside_weights&#39;<br/>  top: &#39;bbox_outside_weights&#39;<br/>  python_param {<br/>    module: &#39;roi_data_layer.layer&#39;<br/>    layer: &#39;RoIDataLayer&#39;<br/>    param_str: &#34;&#39;num_classes&#39;: 21&#34;<br/>  }<br/>}</blockquote><p>可以看到，它的top输出为rois，labels，bbox_targets, bbox_inside_weights, bbox_outside_weights总共5个属性。</p><p>rois是selective search方法提取出的候选区域，尺度为(1,5)，按照(index,x1,y1,x2,y2)的格式来存储。labels和bbox_targets是区域的分类和回归标签，bbox_inside_weights是正样本回归loss的权重，默认为1，负样本为0，表明在回归任务中，只采用正样本进行计算。bbox_outside_weights用于平衡正负样本的权重，它们将在计算SmoothL1Loss的时候被使用，各自的计算方法如下：</p><blockquote>bbox_inside_weights[labels == 1, :] = np.array(cfg.TRAIN.RPN_BBOX_INSIDE_WEIGHTS)<br/>if cfg.TRAIN.RPN_POSITIVE_WEIGHT &lt; 0:<br/>    # uniform weighting of examples (given non-uniform sampling)<br/>    num_examples = np.sum(labels &gt;= 0)<br/>    positive_weights = np.ones((1, 4)) * 1.0 / num_examples<br/>    negative_weights = np.ones((1, 4)) * 1.0 / num_examples<br/>else:<br/>    assert ((cfg.TRAIN.RPN_POSITIVE_WEIGHT &gt; 0) &amp;<br/>            (cfg.TRAIN.RPN_POSITIVE_WEIGHT &lt; 1))<br/>    positive_weights = (cfg.TRAIN.RPN_POSITIVE_WEIGHT /<br/>                        np.sum(labels == 1))<br/>    negative_weights = ((1.0 - cfg.TRAIN.RPN_POSITIVE_WEIGHT) /<br/>                        np.sum(labels == 0))<br/>bbox_outside_weights[labels == 1, :] = positive_weights<br/>bbox_outside_weights[labels == 0, :] = negative_weights</blockquote><p>然后是faster rcnn：</p><blockquote>name: &#34;VGG_CNN_M_1024&#34;<br/>layer {<br/>  name: &#39;input-data&#39;<br/>  type: &#39;Python&#39;<br/>  top: &#39;data&#39;<br/>  top: &#39;im_info&#39;<br/>  top: &#39;gt_boxes&#39;<br/>  python_param {<br/>    module: &#39;roi_data_layer.layer&#39;<br/>    layer: &#39;RoIDataLayer&#39;<br/>    param_str: &#34;&#39;num_classes&#39;: 21&#34;<br/>  }<br/>}</blockquote><p>可以看到，它的top输出是im_info，gt_boxes，两者的尺度分别为(1,3)和(1,4)。而上面的rois，labels，bbox_targets, bbox_inside_weights, bbox_outside_weights全部通过rpn框架来生成，rpn框架我们下面讲。</p><p>roi_data中需要批量获取数据，实现就在minibatch.py中了，它实现一次从roidb中获取多个样本的操作，主要函数是get_minibatch，根据是否使用rpn来进行操作。</p><p>如果使用rpn，则只需要输出gt_boxes和im_info，直接从roidb数据库中获取即可。如果不使用rpn，则需要自己来生成前景和背景的rois训练图片，调用了两个函数_sample_rois，_project_im_rois。</p><p>_sample_rois函数生成前景和背景，接口如下：</p><blockquote>_sample_rois(roidb, fg_rois_per_image, rois_per_image, num_classes)</blockquote><p>通过rois_per_image指定需要生成的训练样本的数量， fg_rois_per_image指定前景正样本的数量。一个前景正样本就是满足与真值box中的最大重叠度大于一定阈值cfg.TRAIN.FG_THRESH的样本，一个背景就是与真值box中的最大重叠度大于一定阈值cfg.TRAIN.FG_THRELO，小于一定阈值cfg.TRAIN.BG_THRESH_SH的样本，选择样本的方法当然就是从符合条件的样本中随机选择，如果满足条件的样本不够，那就按照最低值来选择。</p><p>_project_im_rois就是一个缩放，因为训练的时候使用了不同的尺度。</p><p>(5) rpn目录</p><p>该目录就是region proposal模块，包含有generate_anchors.py，proposal_layer.py，anchor_target_layer.py，proposal_target_layer.py，generate.py脚本。</p><p>rpn有几个任务需要完成，产生一些anchors，完成anchor到图像空间的映射，得到训练样本。</p><p>generate_anchors脚本就是用于产生anchors，它使用16*16的参考窗口，产生3个比例(1:1,1:2,2:1)，三个缩放尺度(0.5, 1, 2)的anchors，共9个。在原论文中对应到原始图像空间，3个尺度是(128, 256与512)，代码如下：</p><blockquote>def generate_anchors(base_size=16, ratios=[0.5, 1, 2],<br/>                     scales=2**np.arange(3, 6)):<br/>    base_anchor = np.array([1, 1, base_size, base_size]) - 1<br/>    ratio_anchors = _ratio_enum(base_anchor, ratios)<br/>    anchors = np.vstack([_scale_enum(ratio_anchors[i, :], scales)<br/>                         for i in xrange(ratio_anchors.shape[0])])<br/>    return anchors</blockquote><p>anchor_target_layer.py就是实现了AnchorTargetLayer，它与generate_anchors配合使用，共同产生anchors的样本rpn，用于rpn的分类和回归任务，anchor_target_layer层的caffe配置如下。</p><blockquote>layer {<br/>  name: &#39;rpn-data&#39;<br/>  type: &#39;Python&#39;<br/>  bottom: &#39;rpn_cls_score&#39;<br/>  bottom: &#39;gt_boxes&#39;<br/>  bottom: &#39;im_info&#39;<br/>  bottom: &#39;data&#39;<br/>  top: &#39;rpn_labels&#39;<br/>  top: &#39;rpn_bbox_targets&#39;<br/>  top: &#39;rpn_bbox_inside_weights&#39;<br/>  top: &#39;rpn_bbox_outside_weights&#39;<br/>  python_param {<br/>    module: &#39;rpn.anchor_target_layer&#39;<br/>    layer: &#39;AnchorTargetLayer&#39;<br/>    param_str: &#34;&#39;feat_stride&#39;: 16&#34;<br/>  }<br/>}</blockquote><p>可知，anchor_target_layer的输入是gt_boxes，im_info，rpn_cls_score，data，输出就是rpn_labels，rpn_bbox_targets, rpn_bbox_inside_weights, rpn_bbox_outside_weights。rpn_cls_score是rpn网络的第一个卷积的分类分支的输出，rpn网络正是通过rpn-data的指导学习到了如何提取proposals。</p><p>假如输入rpn网络的为256*13*13的特征，那么一个rpn的输出通常是一个18*13*13的分类特征图和一个36*13*13的回归特征图，它们都是通过1*1的卷积生成。13*13就是特征图的大小，它并不会改变。一个18*1*1就对应每一个位置的9个anchor的分类信息，这里的分类不管具体的类别，只分前景与背景，anchor显示里面有物体存在时可对其进行回归。一个36*1*1就对应每一个位置的9个anchor的回归信息，这是一个相对值。后面要做的，就是利用这些anchors，生成propasals了。</p><p>proposal_layer脚本，定义了ProposalLayer的类，就是从rpn的输出开始，得到最终的proposals，输入有三个，网络配置如下。</p><blockquote>layer {<br/>  name: &#39;proposal&#39;<br/>  type: &#39;Python&#39;<br/>  bottom: &#39;rpn_cls_prob_reshape&#39;<br/>  bottom: &#39;rpn_bbox_pred&#39;<br/>  bottom: &#39;im_info&#39;<br/>  top: &#39;rpn_rois&#39;<br/>#  top: &#39;rpn_scores&#39;<br/>  python_param {<br/>    module: &#39;rpn.proposal_layer&#39;<br/>    layer: &#39;ProposalLayer&#39;<br/>    param_str: &#34;&#39;feat_stride&#39;: 16&#34;<br/>  }<br/>}</blockquote><p>可以看到，输入了rpn_bbox_pred，rpn_cls_pro_shape以及im_info，输出rpn_rois，也就是object proposals，由bbox_transform_inv函数完成坐标变换，接口如下：</p><p>proposals = bbox_transform_inv(anchors, bbox_deltas)</p><p>这里的bbox_deltas就是上面的rpn_bbox_pred，它就是预测的anchor的偏移量，它的尺寸大小是(1, 4 * A, H, W)，其中H，W就是特征图的大小，而A就是基础anchors的个数，就是上面的9。Anchors的大小则是(K * A, 4)，其中K是偏移位置的种类，偏移位置就是将anchors在特征图上进行滑动的偏移量，可知包含了x和y两个方向，产生的方法如下：</p><blockquote>shift_x = np.arange(0, width) * self._feat_stride<br/>shift_y = np.arange(0, height) * self._feat_stride<br/>shift_x, shift_y = np.meshgrid(shift_x, shift_y)<br/>shifts = np.vstack((shift_x.ravel(), shift_y.ravel(),<br/>         shift_x.ravel(), shift_y.ravel())).transpose()</blockquote><p>得到了初始的proposals之后，再经过裁剪，过滤，排序，非极大值抑制后就可以用了。</p><p>proposal_target_layer，就是从上面选择出的object proposals采样得到训练样本，流程与上面roi_data_layer层中没有rpn模块时产生训练样本类似，因此不再赘述。</p><blockquote>labels, rois, bbox_targets, bbox_inside_weights = _sample_rois(<br/>            all_rois, gt_boxes, fg_rois_per_image,<br/>            rois_per_image, self._num_classes)</blockquote><p>最后是generate脚本，就是高层的调用脚本，即使用RPN方法从imdb或者图像中产生proposals。</p><p>(6) fast_rcnn目录</p><p>该目录有bbox_transform.py，config.py，nms.wrapper.py，test.py，train.py几个脚本。</p><p>config.py是一个配置参数的脚本，它配置了非常多的默认变量，非常重要。如果想要修改，也不应该在该脚本中修改，而是到先前提到的experements目录下进行配置。</p><p>配置包含两部分，一个是训练部分的配置，一个是测试部分的配置。训练部分的配置如下，我们添加注释。</p><blockquote># Training options<br/>__C.TRAIN.SCALES = (600,) #训练尺度，可以配置为一个数组<br/>__C.TRAIN.MAX_SIZE = 1000 #缩放后图像最长边的上限<br/>__C.TRAIN.IMS_PER_BATCH = 2 #每一个batch使用的图像数量<br/>__C.TRAIN.BATCH_SIZE = 128 #每一个batch中使用的ROIs的数量<br/>__C.TRAIN.FG_FRACTION = 0.25 #每一个batch中前景的比例<br/>__C.TRAIN.FG_THRESH = 0.5 #ROI前景阈值<br/>__C.TRAIN.BG_THRESH_HI = 0.5 #ROI背景高阈值<br/>__C.TRAIN.BG_THRESH_LO = 0.1 #ROI背景低阈值<br/>__C.TRAIN.USE_FLIPPED = True #训练时是否进行水平翻转<br/>__C.TRAIN.BBOX_REG = True #是否训练回归<br/>__C.TRAIN.BBOX_THRESH = 0.5 #用于训练回归的roi与真值box的重叠阈值<br/>__C.TRAIN.SNAPSHOT_ITERS = 10000 #snapshot间隔<br/>__C.TRAIN.SNAPSHOT_INFIX = &#39;&#39; #snapshot前缀<br/>__C.TRAIN.BBOX_NORMALIZE_TARGETS = True #bbox归一化方法，去均值和方差<br/>__C.TRAIN.BBOX_NORMALIZE_MEANS = (0.0, 0.0, 0.0, 0.0)<br/>__C.TRAIN.BBOX_NORMALIZE_STDS = (0.1, 0.1, 0.2, 0.2)<br/>__C.TRAIN.BBOX_INSIDE_WEIGHTS = (1.0, 1.0, 1.0, 1.0) #rpn 前景box权重<br/>__C.TRAIN.PROPOSAL_METHOD = &#39;selective_search&#39; #默认proposal方法<br/>__C.TRAIN.ASPECT_GROUPING = True #一个batch中选择尺度相似的样本<br/>__C.TRAIN.HAS_RPN = False #是否使用RPN<br/>__C.TRAIN.RPN_POSITIVE_OVERLAP = 0.7 #正样本IoU阈值<br/>__C.TRAIN.RPN_NEGATIVE_OVERLAP = 0.3 #负样本IoU阈值<br/>__C.TRAIN.RPN_CLOBBER_POSITIVES = False<br/>__C.TRAIN.RPN_FG_FRACTION = 0.5 #前景样本的比例<br/>__C.TRAIN.RPN_BATCHSIZE = 256 #RPN样本数量<br/>__C.TRAIN.RPN_NMS_THRESH = 0.7 #NMS阈值<br/>__C.TRAIN.RPN_PRE_NMS_TOP_N = 12000 #使用NMS前，要保留的top scores的box数量<br/>__C.TRAIN.RPN_POST_NMS_TOP_N = 2000 #使用NMS后，要保留的top scores的box数量<br/>__C.TRAIN.RPN_MIN_SIZE = 16 #原始图像空间中的proposal最小尺寸阈值</blockquote><p>测试时相关配置类似，此处不再一一解释。</p><p>bbox_transform.py中的bbox_transform函数计算的是两个N * 4的矩阵之间的相关回归矩阵，两个输入矩阵一个是anchors，一个是gt boxes，本质上是在求解每一个anchor相对于它的对应gt box的（dx, dy, dw, dh）的四个回归值，返回结果的shape为[N, 4]，使用了log指数变换。</p><p>bbox_transform.py中的bbox_transform_inv函数用于将rpn网络产生的deltas进行变换处理，求出变换后的对应到原始图像空间的boxes，它输入boxes和deltas，boxes表示原始anchors，即未经任何处理仅仅是经过平移之后产生的anchors，shape为[N, 4]，N表示anchors的数目。deltas就是RPN网络产生的数据，即网络&#39;rpn_bbox_pred&#39;层的输出，shape为[N, (1 + classes) * 4]，classes表示类别数目，1 表示背景，N表示anchors的数目，核心代码如下。</p><blockquote>widths = boxes[:, 2] - boxes[:, 0] + 1.0<br/>heights = boxes[:, 3] - boxes[:, 1] + 1.0<br/>ctr_x = boxes[:, 0] + 0.5 * widths<br/>ctr_y = boxes[:, 1] + 0.5 * heights<br/>dx = deltas[:, 0::4]<br/>dy = deltas[:, 1::4]<br/>dw = deltas[:, 2::4]<br/>dh = deltas[:, 3::4]<br/>pred_ctr_x = dx * widths[:, np.newaxis] + ctr_x[:, np.newaxis]<br/>pred_ctr_y = dy * heights[:, np.newaxis] + ctr_y[:, np.newaxis]<br/>pred_w = np.exp(dw) * widths[:, np.newaxis]<br/>pred_h = np.exp(dh) * heights[:, np.newaxis]<br/>pred_boxes = np.zeros(deltas.shape, dtype=deltas.dtype)<br/># x1<br/>pred_boxes[:, 0::4] = pred_ctr_x - 0.5 * pred_w<br/># y1<br/>pred_boxes[:, 1::4] = pred_ctr_y - 0.5 * pred_h<br/># x2<br/>pred_boxes[:, 2::4] = pred_ctr_x + 0.5 * pred_w<br/># y2<br/>pred_boxes[:, 3::4] = pred_ctr_y + 0.5 * pred_h</blockquote><p>可以看出，它与bbox_transform是配合使用的，bbox_transform使用了对数变换将anchor存储下来，而bbox_transform_inv则将其恢复到图像空间。</p><p>网络的回归坐标预测的是一个经过平移和尺度缩放的因子，如果采用原始的图像坐标，则可能覆盖从0～1000这样几个数量级差距的数值，很难优化。</p><p>train.py和test.py分别是训练主脚本和测试主脚本。在训练主脚本中，定义了一个类solverWrapper，包含训练的函数和存储模型结果的函数。</p><p>test.py脚本中最重要的函数是im_detect，它的输入是caffe的模型指针，输入BGR顺序的彩色图像，以及可选的R*4的候选框，这适用于使用selective search提取候选框的方法，拥有rpn框架的faster rcnn则不需要。</p><p>返回包括两个值，一个是scores，一个是boxes。scores就是各个候选框中各个类别的概率，boxes就是各个候选中的目标的回归坐标。</p><p>im_detect方法首先调用_get_blobs函数，它输入im和boxes。在_get_blobs函数中首先调用_get_image_blob获得不同尺度的测试输入，测试尺度在cfg.TEST.SCALES中进行配置。</p><p>假如没有rpn网络，则boxes非空，这时候需要配置的输入为blobs[&#39;rois&#39;]。调用_get_rois_blob函数，它会调用_project_im_rois得到不同尺度的输入RoI。</p><p>假如有rpn网络，则需要配置blobs[&#39;im_info&#39;]，它会用于辅助RPN框架从特征空间到原始图像空间的映射。</p><p>Forward部分代码如下：</p><blockquote>forward_kwargs = {&#39;data&#39;: blobs[&#39;data&#39;].astype(np.float32, copy=False)}<br/>if cfg.TEST.HAS_RPN:<br/>   forward_kwargs[&#39;im_info&#39;] = blobs[&#39;im_info&#39;].astype(np.float32, copy=False)<br/>else:<br/>   forward_kwargs[&#39;rois&#39;] = blobs[&#39;rois&#39;].astype(np.float32, copy=False)<br/>blobs_out = net.forward(**forward_kwargs)</blockquote><p>前向传播的结果在blobs_out中，分类器如果使用SVM，则分类结果为scores = net.blobs[&#39;cls_score&#39;].data，如果使用cnn softmax，则分类结果为scores = blobs_out[&#39;cls_prob&#39;]。</p><p>如果有边界回归网络，获取回归结果的代码如下：</p><blockquote>box_deltas = blobs_out[&#39;bbox_pred&#39;]<br/>pred_boxes = bbox_transform_inv(boxes, box_deltas)<br/>pred_boxes = clip_boxes(pred_boxes, im.shape)</blockquote><p>可知原始的回归结果是一个偏移量，它需要通过bbox_transform_inv反投影到图像空间。test.py脚本中还包含函数apply_nms，用于对网络输出的结果进行非极大值抑制。</p><div class=\"highlight\"><pre><code class=\"language-text\">2.6 tools 目录</code></pre></div><p>该目录包含的就是最高层的可执行脚本，包括_init_paths.py，compress_net.py，demo.py，eval_recall.py，reval.py，rpn_genetate.py这几个脚本。</p><p>_init_paths.py，用于初始化若干路径，包括caffe的路径以及lib的路径，一般大型的工程用这样的一个文件剥离出路径是很好的选择。</p><p>compress_net.py，这是用于压缩参数的脚本，使用了SVD矩阵分解的方法来对模型进行压缩，这通常对于全连接层是非常有效的，因为对于一些经典的网络如AlexNet，VGGNet等，全连接层占据了网络的绝大部分参数。脚本中给出的例子对VGGNet的fc6层和fc7层进行了压缩，读者可以使用这个脚本去对更多的带全连接层的网络进行压缩尝试。</p><p>demo.py，这是一个demo演示脚本，调用了fast_rcnn中的test脚本中的检测函数，使用了工程自带的一些图像以及预先提取好的proposal，配置好模型之后就可以进行演示。如果要测试自己的模型和数据，也可以非常方便进行修改。</p><p>eval_recall.py，这是用于在测试数据集上对所训练的模型进行评估的脚本，默认使用的数据集是voc_2007_test，它会统计在不同阈值下的检测框召回率。</p><p>reval.py：对已经检测好的结果进行评估。</p><p>rpn_genetate.py，这个脚本调用了rpn中的genetate函数，产生一个测试数据集的proposal并将其存储到pkl文件。</p><p>test_net.py，测试训练好的fast rcnn网络的脚本，调用了fast rcnn的test函数。</p><p>train_faster_rcnn_alt_opt.py，这是faster rcnn文章中的使用交替的训练方法来训练faster rcnn网络的具体实现，它包括4个阶段，分别是：</p><p>RPN第1阶段，使用在imagenet分类任务上进行训练的模型来初始化参数，生成proposals。</p><p>fast rcnn第1阶段，使用在imagenet分类任务上进行训练的模型来初始化参数，使用刚刚生成的proposal进行fast rcnn的训练。</p><p>RPN第2阶段，使用fast rcnn训练好的参数进行初始化，并生成proposal。</p><p>fast rcnn 第2阶段，使用RPN第2阶段中的模型进行参数初始化。</p><p>train_net.py，训练脚本。</p><p>train_svms.py，R-CNN网络的SVM训练脚本，可以不关注。</p><p>在熟悉了框架后，就可以使用我们的数据进行训练了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>更多详细内容关注微信公众号：有三AI</p><a href=\"https://zhuanlan.zhihu.com/p/51012194\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic2.zhimg.com/v2-4517631ac09052e699a5aef3d288d9a5_180x120.jpg\" data-image-width=\"2000\" data-image-height=\"1023\" class=\"internal\">龙鹏：【技术综述】万字长文详解Faster RCNN源代码（一）</a><a href=\"https://zhuanlan.zhihu.com/p/51013203\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic2.zhimg.com/v2-4517631ac09052e699a5aef3d288d9a5_180x120.jpg\" data-image-width=\"2000\" data-image-height=\"1023\" class=\"internal\">龙鹏：【技术综述】万字长文详解Faster RCNN源代码（二）</a><a href=\"https://zhuanlan.zhihu.com/p/51014458\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic2.zhimg.com/v2-4517631ac09052e699a5aef3d288d9a5_180x120.jpg\" data-image-width=\"2000\" data-image-height=\"1023\" class=\"internal\">龙鹏：【技术综述】万字长文详解Faster RCNN源代码（四）</a><p></p>", 
            "topic": [
                {
                    "tag": "神经网络", 
                    "tagLink": "https://api.zhihu.com/topics/19607065"
                }, 
                {
                    "tag": "卷积神经网络（CNN）", 
                    "tagLink": "https://api.zhihu.com/topics/20043586"
                }, 
                {
                    "tag": "目标检测", 
                    "tagLink": "https://api.zhihu.com/topics/19596960"
                }
            ], 
            "comments": [
                {
                    "userName": "素谷黎维", 
                    "userLink": "https://www.zhihu.com/people/72b94e0f790b1036bdbfefc7da39dd69", 
                    "content": "先支持一下", 
                    "likes": 1, 
                    "childComments": [
                        {
                            "userName": "言有三-龙鹏", 
                            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
                            "content": "谢谢", 
                            "likes": 0, 
                            "replyToAuthor": "素谷黎维"
                        }
                    ]
                }, 
                {
                    "userName": "知乎用户", 
                    "userLink": "https://www.zhihu.com/people/0", 
                    "content": "gt overlaps不是重叠度，而是各类别的分数", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "oozz", 
                    "userLink": "https://www.zhihu.com/people/0e01e7a2fb3d0a8536468bfe874620d0", 
                    "content": "看到过对fasterRcnn写得最全的。", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "言有三-龙鹏", 
                            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
                            "content": "谢谢🙏", 
                            "likes": 0, 
                            "replyToAuthor": "oozz"
                        }
                    ]
                }, 
                {
                    "userName": "Jimmy", 
                    "userLink": "https://www.zhihu.com/people/a63904b4bc05437ac7c86cce09202542", 
                    "content": "我现在有个手势数据集 ASL，我想通过 faster rcnn把图片中的手扣出来。这就是我对这个网络的要求。然后把扣出来的手势图再拿去别的 VGG网络去分类识别。但是有的人说 faster rcnn需要的数据集是已经把手框出来的数据集，也就是说提前标记出来的。但是我的数据集是没有任何标记的。这样的话。我这个扣出来的是不是就做不成了。", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "言有三-龙鹏", 
                            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
                            "content": "你要先标注，才能做检测", 
                            "likes": 0, 
                            "replyToAuthor": "Jimmy"
                        }
                    ]
                }, 
                {
                    "userName": "kinredon", 
                    "userLink": "https://www.zhihu.com/people/29ce601445f0e55c81cd399007cee025", 
                    "content": "<p>\"而且，有的时候采用RGB格式进行训练，有的使用采用BGR格式进行训练，也需要做对应的调整。\" 请问什么时候采用RGB，什么时候采用BGR呢？我看有些实现使用BGR进行训练，这样的目的是什么？</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "言有三-龙鹏", 
                            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
                            "content": "没什么，就是个人习惯噢", 
                            "likes": 0, 
                            "replyToAuthor": "kinredon"
                        }, 
                        {
                            "userName": "kinredon", 
                            "userLink": "https://www.zhihu.com/people/29ce601445f0e55c81cd399007cee025", 
                            "content": "谢谢回复。请问现在在imagenet上pretrained model一般使用的是BGR还是RGB的通道顺序？pytorch model zone里面用的是哪个顺序呢", 
                            "likes": 0, 
                            "replyToAuthor": "kinredon"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/51014458", 
            "userName": "言有三-龙鹏", 
            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
            "upvote": 13, 
            "title": "【技术综述】万字长文详解Faster RCNN源代码（四）", 
            "content": "<p>首发于《有三AI》</p><a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/Bnibfng4Sv6qbMk5BEFwnQ\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic2.zhimg.com/v2-0bbf50e70d464dde8021140a6051652d_180x120.jpg\" data-image-width=\"1200\" data-image-height=\"900\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【技术综述】万字长文详解Faster RCNN源代码</a><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-a6ac134cfaa85c8d21960296fe0a13e3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1402\" data-rawheight=\"416\" class=\"origin_image zh-lightbox-thumb\" width=\"1402\" data-original=\"https://pic4.zhimg.com/v2-a6ac134cfaa85c8d21960296fe0a13e3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1402&#39; height=&#39;416&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1402\" data-rawheight=\"416\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1402\" data-original=\"https://pic4.zhimg.com/v2-a6ac134cfaa85c8d21960296fe0a13e3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-a6ac134cfaa85c8d21960296fe0a13e3_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><i>Faster R-CNN将分成四部分介绍。总共有Faster R-CNN概述，py-faster-rcnn框架解读，网络分析，和训练与测试四部分内容。第四篇将介绍网络分析，和训练与测试。</i></p><p class=\"ztext-empty-paragraph\"><br/></p><blockquote><b>3. 网络分析</b></blockquote><p>下面我们开始一个任务，就来个猫脸检测吧，使用VGG CNN 1024网络，看一下网络结构图，然后我们按模块解析一下。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-244d11c8f696f4a6aa4cbae3f0b7f662_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"2105\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-244d11c8f696f4a6aa4cbae3f0b7f662_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;2105&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"2105\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-244d11c8f696f4a6aa4cbae3f0b7f662_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-244d11c8f696f4a6aa4cbae3f0b7f662_b.jpg\"/></figure><div class=\"highlight\"><pre><code class=\"language-text\">3.1 input</code></pre></div><blockquote>layer {<br/>  name: &#39;input-data&#39;<br/>  type: &#39;Python&#39;<br/>  top: &#39;data&#39;<br/>  top: &#39;im_info&#39;<br/>  top: &#39;gt_boxes&#39;<br/>  python_param {<br/>    module: &#39;roi_data_layer.layer&#39;<br/>    layer: &#39;RoIDataLayer&#39;<br/>    param_str: &#34;&#39;num_classes&#39;: 2&#34;<br/>  }<br/>}</blockquote><p>这里要改的，就是num_classes，因为我们只有一个猫脸，前景类别数目等于1。</p><div class=\"highlight\"><pre><code class=\"language-text\">3.2 rpn</code></pre></div><blockquote>layer {<br/>  name: &#34;rpn_conv/3x3&#34;<br/>  type: &#34;Convolution&#34;<br/>  bottom: &#34;conv5&#34;<br/>  top: &#34;rpn/output&#34;<br/>  param { lr_mult: 1.0 }<br/>  param { lr_mult: 2.0 }<br/>  convolution_param {<br/>    num_output: 256<br/>    kernel_size: 3 pad: 1 stride: 1<br/>    weight_filler { type: &#34;gaussian&#34; std: 0.01 }<br/>    bias_filler { type: &#34;constant&#34; value: 0 }<br/>  }<br/>}<br/>layer {<br/>  name: &#34;rpn_relu/3x3&#34;<br/>  type: &#34;ReLU&#34;<br/>  bottom: &#34;rpn/output&#34;<br/>  top: &#34;rpn/output&#34;<br/>}<br/>layer {<br/>  name: &#34;rpn_cls_score&#34;<br/>  type: &#34;Convolution&#34;<br/>  bottom: &#34;rpn/output&#34;<br/>  top: &#34;rpn_cls_score&#34;<br/>  param { lr_mult: 1.0 }<br/>  param { lr_mult: 2.0 }<br/>  convolution_param {<br/>    num_output: 18   # 2(bg/fg) * 9(anchors)<br/>    kernel_size: 1 pad: 0 stride: 1<br/>    weight_filler { type: &#34;gaussian&#34; std: 0.01 }<br/>    bias_filler { type: &#34;constant&#34; value: 0 }<br/>  }<br/>}<br/> <br/>layer {<br/>  name: &#34;rpn_bbox_pred&#34;<br/>  type: &#34;Convolution&#34;<br/>  bottom: &#34;rpn/output&#34;<br/>  top: &#34;rpn_bbox_pred&#34;<br/>  param { lr_mult: 1.0 }<br/>  param { lr_mult: 2.0 }<br/>  convolution_param {<br/>    num_output: 36   # 4 * 9(anchors)<br/>    kernel_size: 1 pad: 0 stride: 1<br/>    weight_filler { type: &#34;gaussian&#34; std: 0.01 }<br/>    bias_filler { type: &#34;constant&#34; value: 0 }<br/>  }<br/>}</blockquote><p>具体的网络拓扑结构图如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-1f8c4055d2ab8706cac9a4acb9bcf62b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"899\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-1f8c4055d2ab8706cac9a4acb9bcf62b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;899&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"899\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-1f8c4055d2ab8706cac9a4acb9bcf62b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-1f8c4055d2ab8706cac9a4acb9bcf62b_b.jpg\"/></figure><p>从上图可以看出，rpn网络的输入来自于conv5卷积层的输出，后面接了rpn_conv/3x3层，输出通道数为256，stride=1。</p><p>rpn_conv/3x3层产生了两个分支，一个是rpn_cls_score，一个是rpn_bbox_pred，分别是分类和回归框的特征。</p><p>rpn_cls_score输出为18个通道，这是9个anchors的前背景概率，它一边和gt_boxes，im_info，data一起作为AnchorTargetLayer层的输入，产生分类的真值rpn_labels，回归的真值rpn_bbox_targets。另一边则经过rpn_cls_score_reshape进行reshape，然后与rpn_labels一起产生分类损失。</p><p>rpn_bbox_ppred输出为36个通道，就是9个anchors的回归预测结果，它与rpn_bbox_targets比较产生回归损失。rpn_cls_score_reshape重新reshape后得到rpn_cls_prob，rpn_cls_prob_shape，它与rpn_bbox_pred以及输入，共同得到了region prososal，就是候选的检测框。</p><p>在ProposalLayer层中配置了一个重要参数，就是feat_stride，这是前面的卷积层的feat_stride大小。ProposalLayer层完成的功能就是根据RPN的输出结果，提取出所需的目标框，而目标框是在原始的图像空间，所以这里需要预先计算出feat_stride的大小。ProposalLayer层的输出与data层一起获得最终的proposal roi，这将作为roi pooling层的输入。</p><div class=\"highlight\"><pre><code class=\"language-text\">3.3 roi pooing</code></pre></div><p>前面得到了proposal roi之后，就可以进行roi pooling层，配置如下：</p><blockquote>layer {<br/>  name: &#34;roi_pool5&#34;<br/>  type: &#34;ROIPooling&#34;<br/>  bottom: &#34;conv5&#34;<br/>  bottom: &#34;rois&#34;<br/>  top: &#34;pool5&#34;<br/>  roi_pooling_param {<br/>    pooled_w: 6<br/>    pooled_h: 6<br/>    spatial_scale: 0.0625 # 1/16<br/>  }<br/>}</blockquote><p>可以看到，它配置了几个参数，最终spatial_scale对应的就是前面的feat_stride，等于1/16，用于从图像空间的roi到特征空间roi的投影。</p><p>而pooled_w，pooled_h则是最终要pooling的特征图的大小，这里配置为6*6，从13*13的输入下采样而来。</p><p class=\"ztext-empty-paragraph\"><br/></p><blockquote><b>4. 训练与测试</b></blockquote><p>写到这里我们就简略一些。要做的就是三步，为了简单，保持使用pascal接口，步骤如下。</p><p>(1)准备voc格式的数据，可以找开源数据集或者使用labelme等工具集标注，然后配置好路径。替换掉pacvoc的ImageSets/Main目录下面的文件list，以及JPEGS和Annotations目录下的文件。</p><p>(2)然后到lib\\datasets\\pascal_voc.py中更改self._classes中的类别，由于我们这里是二分类的检测，所以将多余的类别删除，只保留背景，添加face类别。</p><p>(3)使用experements/tools下面的脚本训练吧。</p><p><b>遇到了坑，就直接跳和爬吧！</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-fe1dd5068736611de31311f74ec4a8bb_b.gif\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"300\" data-rawheight=\"300\" data-thumbnail=\"https://pic4.zhimg.com/v2-fe1dd5068736611de31311f74ec4a8bb_b.jpg\" class=\"content_image\" width=\"300\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;300&#39; height=&#39;300&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"300\" data-rawheight=\"300\" data-thumbnail=\"https://pic4.zhimg.com/v2-fe1dd5068736611de31311f74ec4a8bb_b.jpg\" class=\"content_image lazy\" width=\"300\" data-actualsrc=\"https://pic4.zhimg.com/v2-fe1dd5068736611de31311f74ec4a8bb_b.gif\"/></figure><p>感受一下大小脸，大姿态，遮挡，误检，漏检。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-a5303b77d3a325815b031beabc41bece_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"204\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-a5303b77d3a325815b031beabc41bece_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;204&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"204\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-a5303b77d3a325815b031beabc41bece_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-a5303b77d3a325815b031beabc41bece_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-35938d929a0a4cacfee728a600623ce8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"204\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-35938d929a0a4cacfee728a600623ce8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;204&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"204\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-35938d929a0a4cacfee728a600623ce8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-35938d929a0a4cacfee728a600623ce8_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b950900547a047ce8517a3da8d3160f5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"204\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-b950900547a047ce8517a3da8d3160f5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;204&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"204\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-b950900547a047ce8517a3da8d3160f5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-b950900547a047ce8517a3da8d3160f5_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-f2f587e3305ca5f12c683cbfb327bbe1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"192\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-f2f587e3305ca5f12c683cbfb327bbe1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;192&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"192\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-f2f587e3305ca5f12c683cbfb327bbe1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-f2f587e3305ca5f12c683cbfb327bbe1_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b19d2b83a7a406353366fa9d6f38e3a5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"136\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-b19d2b83a7a406353366fa9d6f38e3a5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;136&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"136\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-b19d2b83a7a406353366fa9d6f38e3a5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-b19d2b83a7a406353366fa9d6f38e3a5_b.jpg\"/></figure><p>路漫漫其修远兮.......</p><p class=\"ztext-empty-paragraph\"><br/></p><p>更多详细内容关注微信公众号：有三AI</p><a href=\"https://zhuanlan.zhihu.com/p/51012194\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic2.zhimg.com/v2-4517631ac09052e699a5aef3d288d9a5_180x120.jpg\" data-image-width=\"2000\" data-image-height=\"1023\" class=\"internal\">龙鹏：【技术综述】万字长文详解Faster RCNN源代码（一）</a><a href=\"https://zhuanlan.zhihu.com/p/51013203\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic2.zhimg.com/v2-4517631ac09052e699a5aef3d288d9a5_180x120.jpg\" data-image-width=\"2000\" data-image-height=\"1023\" class=\"internal\">龙鹏：【技术综述】万字长文详解Faster RCNN源代码（二）</a><a href=\"https://zhuanlan.zhihu.com/p/51014564\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic2.zhimg.com/v2-4517631ac09052e699a5aef3d288d9a5_180x120.jpg\" data-image-width=\"2000\" data-image-height=\"1023\" class=\"internal\">龙鹏：【技术综述】万字长文详解Faster RCNN源代码（三）</a><p></p>", 
            "topic": [
                {
                    "tag": "神经网络", 
                    "tagLink": "https://api.zhihu.com/topics/19607065"
                }, 
                {
                    "tag": "卷积神经网络（CNN）", 
                    "tagLink": "https://api.zhihu.com/topics/20043586"
                }, 
                {
                    "tag": "目标检测", 
                    "tagLink": "https://api.zhihu.com/topics/19596960"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50068781", 
            "userName": "言有三-龙鹏", 
            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
            "upvote": 9, 
            "title": "【github干货】主流深度学习开源框架从入门到熟练", 
            "content": "<p></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-6ecd13326c44fc56cbb67508b2593e00_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2476\" data-rawheight=\"680\" class=\"origin_image zh-lightbox-thumb\" width=\"2476\" data-original=\"https://pic1.zhimg.com/v2-6ecd13326c44fc56cbb67508b2593e00_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2476&#39; height=&#39;680&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2476\" data-rawheight=\"680\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2476\" data-original=\"https://pic1.zhimg.com/v2-6ecd13326c44fc56cbb67508b2593e00_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-6ecd13326c44fc56cbb67508b2593e00_b.jpg\"/></figure><p>今天送上有三AI学院第一个github项目</p><blockquote><b>01 项目背景</b></blockquote><p>目前深度学习框架呈百家争鸣之态势，光是为人熟知的就有caffe，tensorflow，pytorch/caffe2，keras，mxnet，paddldpaddle，theano，cntk，tiny-dnn，deeplearning4j，matconvnet等，一个合格的深度学习算法工程师怎么着都得熟悉<b>其中的3个以上</b>吧。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-e88d91042a3909e6e56df96b734e5b51_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"484\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-e88d91042a3909e6e56df96b734e5b51_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;484&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"484\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-e88d91042a3909e6e56df96b734e5b51_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-e88d91042a3909e6e56df96b734e5b51_b.jpg\"/></figure><p>而且，学习不应该停留在官方的demo上，而是要学会<b>从自定义数据的读取，自定义网络的搭建，模型的训练，模型的可视化，模型的测试与部署等全方位进行掌握</b>。</p><p>因此，我们开设了这个github项目，以图像分类任务为基准，带领大家一步一步入门，后续会增加分割，检测等任务。</p><p>当前的图像分类任务为二分类的表情分类任务，为微笑与非微笑两个类别，案例样本如下。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-7c89cd1d9c095dee75f5286a99bfce03_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"561\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-7c89cd1d9c095dee75f5286a99bfce03_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;561&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"561\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-7c89cd1d9c095dee75f5286a99bfce03_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-7c89cd1d9c095dee75f5286a99bfce03_b.jpg\"/></figure><p>无表情</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-7c8f9b9b46fdbc9eefa8b5181323f0c3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"565\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-7c8f9b9b46fdbc9eefa8b5181323f0c3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;565&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"565\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-7c8f9b9b46fdbc9eefa8b5181323f0c3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-7c8f9b9b46fdbc9eefa8b5181323f0c3_b.jpg\"/></figure><p>微笑</p><blockquote><b>02 框架简介</b></blockquote><p>总的来说，深度学习系统通常有两种编程方式，一种是<b>声明式编程(declarative programming)</b>，用户只需要声明要做什么，而具体执行则由系统完成。以Caffe，TensorFlow的计算图为代表。优点是由于在真正开始计算的时候已经拿到了整个计算图，所以可以做一系列优化来提升性能。实现辅助函数也容易，例如对任何计算图都提供forward和backward函数，另外也方便对计算图进行可视化，将图保存到硬盘和从硬盘读取。缺点是debug很麻烦，监视一个复杂的计算图中的某个节点的中间结果并不简单，逻辑控制也不方便。</p><p>一种是<b>命令式编程(imperative programming)</b>，以numpy，torch/pytorch为代表，每个语句按照原来的意思顺序执行。它 的特点是语义上容易理解，灵活，可以精确控制行为。通常可以无缝地和主语言交互，方便地利用主语言的各类算法，工具包，debug和性能调试器，但是实现统一的辅助函数和提供整体优化都很困难。</p><p><b>综上，各有优劣，自由选择，这里不是为了给大家详细介绍框架，而是快速扫盲，更多请阅读对应文章和官方文档，相关代码进行学习。</b></p><div class=\"highlight\"><pre><code class=\"language-text\">2.1 caffe</code></pre></div><p>Caffe是伯克利的贾扬清主导开发，以C++/CUDA 代码为主，最早的深度学习框架之一，比 TensorFlow、Mxnet、Pytorch 等都更早，需要进行编译安装。</p><p>支持命令行、Python和Matlab接口，单机多卡、多机多卡等都可以很方便的使用，快速入门见下文。</p><a href=\"https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029846%26idx%3D1%26sn%3D0c343cfd0ede5c8ae1405bd6348aefad%26chksm%3D871342abb064cbbd7fe31fb3c55f23875f27e48fb8354e9855823b1701f1227c71b4eb00de50%26scene%3D21%23wechat_redirect\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【caffe速成】caffe图像分类从模型自定义到测试</a><div class=\"highlight\"><pre><code class=\"language-text\">2.2 tensorflow</code></pre></div><p>TensorFlow是Google brain推出的开源机器学习库，与Caffe一样，主要用作深度学习相关的任务。与Caffe相比TensorFlow的安装简单很多。</p><p>TensorFlow = Tensor + Flow，Tensor 就是张量，代表N维数组，这与Caffe中的blob是类似的；Flow即流，代表基于数据流图的计算。</p><p>TensorFlow最大的特点是计算图，即先定义好图，然后进行运算，所以所有的TensorFlow代码，都包含两部分：</p><p>(1) 创建计算图，表示计算的数据流。它做了什么呢？实际上就是定义好了一些操作，你可以将它看做是Caffe中的prototxt 的定义过程。</p><p>(2)运行会话，执行图中的运算，可以看作是Caffe中的训练过程。只是TensorFlow的会话比Caffe灵活很多，由于是Python 接口，取中间结果分析，Debug等方便很多，快速入门见下文。</p><a href=\"https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029846%26idx%3D2%26sn%3D7c2582243bcd8f8b491e8e466a21978f%26chksm%3D871342abb064cbbd0cba24b408ceda2b64a7c8b6baa07f9f8f56cd4d1233caa0b80fe357753e%26scene%3D21%23wechat_redirect\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【tensorflow速成】Tensorflow图像分类从模型自定义到测试</a><div class=\"highlight\"><pre><code class=\"language-text\">2.3 mxnet</code></pre></div><p>mxnet是amazon的官方框架，它尝试将上面说的两种模式无缝的结合起来。在命令式编程上MXNet提供张量运算，进行模型的迭代训练和更新中的控制逻辑；在声明式编程中MXNet支持符号表达式，用来描述神经网络，并利用系统提供的自动求导来训练模型，快速入门见下文。</p><a href=\"https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029904%26idx%3D1%26sn%3D0bdc6947f5ac68e7f68426b9d076b4ab%26chksm%3D8713436db064ca7b3b2a2a1d6a8d24c15069f72655e2c39e2498051fa0e56bbcf6fe9c332d5b%26scene%3D21%23wechat_redirect\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【mxnet速成】mxnet图像分类从模型自定义到测试</a><div class=\"highlight\"><pre><code class=\"language-text\">2.4 pytorch </code></pre></div><p>一句话总结Pytorch = Python + Torch。</p><p>Torch是纽约大学的一个机器学习开源框架，几年前在学术界非常流行，包括Lecun等大佬都在使用。但是由于使用的是一种绝大部分人绝对没有听过的Lua语言，导致很多人都被吓退。后来随着Python的生态越来越完善，Facebook人工智能研究院推出了Pytorch并开源。Pytorch不是简单的封装Torch 并提供Python 接口，而是对Tensor以上的所有代码进行了重构，同TensorFlow一样，增加了自动求导。</p><p>后来Caffe2全部并入Pytorch，如今已经成为了非常流行的框架。很多最新的研究如风格化、GAN等大多数采用Pytorch源码，快速入门见下文。</p><a href=\"https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029881%26idx%3D1%26sn%3D3c869fcee3b48d3582952ab9a0683ea6%26chksm%3D87134284b064cb924c5e7231b3f2c36ba27e3a689b067f569f2e086f62b18413bcebc5987a07%26scene%3D21%23wechat_redirect\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【pytorch速成】Pytorch图像分类从模型自定义到测试</a><div class=\"highlight\"><pre><code class=\"language-text\">2.5 paddlepaddle </code></pre></div><p>正所谓google有tensorflow，facebook有pytorch，amazon有mxnet，作为国内机器学习的先驱，百度也有PaddlePaddle，其中Paddle即Parallel Distributed Deep Learning(并行分布式深度学习)，整体使用起来与tensorflow非常类似，可参考下文。</p><a href=\"https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029887%26idx%3D1%26sn%3D645b97809c24922352a0b39f19c9ef0c%26chksm%3D87134282b064cb9441af68124d205d9c7dedcaeb09788f4d586b949584e556eddd3a72217f69%26scene%3D21%23wechat_redirect\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【paddlepaddle速成】paddlepaddle图像分类从模型自定义到测试</a><div class=\"highlight\"><pre><code class=\"language-text\">2.6 keras</code></pre></div><p>Keras是一个非常流行、简单的深度学习框架，它的设计参考了torch，用Python语言编写，是一个高度模块化的神经网络库，能够在TensorFlow，CNTK或Theano之上运行。 Keras的特点是能够快速实现模型的搭建，是高效地进行科学研究的关键，可参考下文。</p><a href=\"https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029896%26idx%3D1%26sn%3Df3f7b9cf69c514f45d1d14205f879270%26chksm%3D87134375b064ca6323354c40f3e55b02ff0d1d24f3dacfc980190d51f20ec9ec16e60c1a4741%26scene%3D21%23wechat_redirect\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-2d9ceed78978badf52f685b50ced44c6_ipico.jpg\" data-image-width=\"358\" data-image-height=\"358\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【Keras速成】Keras图像分类从模型自定义到测试</a><div class=\"highlight\"><pre><code class=\"language-text\">2.7 其他</code></pre></div><p>除了以上最常用的框架，还有theano，cntk，tiny-dnn，deeplearning 4j，matconvnet等，我们后续会补充教程，增强github案例，欢迎大家关注，下面是github项目链接，欢迎来做贡献。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-c6998a26c413f0b2e4229ef628bbda20_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"921\" data-rawheight=\"921\" class=\"origin_image zh-lightbox-thumb\" width=\"921\" data-original=\"https://pic1.zhimg.com/v2-c6998a26c413f0b2e4229ef628bbda20_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;921&#39; height=&#39;921&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"921\" data-rawheight=\"921\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"921\" data-original=\"https://pic1.zhimg.com/v2-c6998a26c413f0b2e4229ef628bbda20_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-c6998a26c413f0b2e4229ef628bbda20_b.jpg\"/></figure><p>以及与该项目配套的教程，<b>前十名</b>转发截图到后台，可找我领取免费码。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-06c32f742ec1076efadc6ecffcfa55ec_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"720\" data-rawheight=\"1280\" class=\"origin_image zh-lightbox-thumb\" width=\"720\" data-original=\"https://pic1.zhimg.com/v2-06c32f742ec1076efadc6ecffcfa55ec_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;720&#39; height=&#39;1280&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"720\" data-rawheight=\"1280\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"720\" data-original=\"https://pic1.zhimg.com/v2-06c32f742ec1076efadc6ecffcfa55ec_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-06c32f742ec1076efadc6ecffcfa55ec_b.jpg\"/></figure><p></p><p></p>", 
            "topic": [
                {
                    "tag": "Caffe（深度学习框架）", 
                    "tagLink": "https://api.zhihu.com/topics/20019488"
                }, 
                {
                    "tag": "Torch (深度学习框架)", 
                    "tagLink": "https://api.zhihu.com/topics/20047018"
                }, 
                {
                    "tag": "开源项目", 
                    "tagLink": "https://api.zhihu.com/topics/19565961"
                }
            ], 
            "comments": [
                {
                    "userName": "知乎用户", 
                    "userLink": "https://www.zhihu.com/people/0", 
                    "content": "<p>Kears 现在由蛮主流了</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "言有三-龙鹏", 
                            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
                            "content": "很适合入门", 
                            "likes": 0, 
                            "replyToAuthor": "知乎用户"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/45554447", 
            "userName": "言有三-龙鹏", 
            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
            "upvote": 5, 
            "title": "【mxnet速成】mxnet图像分类从模型自定义到测试", 
            "content": "<p>这一次我们讲讲mxnet，相关的代码、数据都在我们 Git 上，希望大家 Follow 一下这个 Git 项目，后面会持续更新不同框架下的任务。</p><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/longpeng2008/LongPeng_ML_Course\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/longpeng2008</span><span class=\"invisible\">/LongPeng_ML_Course</span><span class=\"ellipsis\"></span></a> </p><h2>01 <b>mxnet是什么</b></h2><p>mxnet是amazon的官方框架，下面参考mxnet的官方简介</p><div class=\"highlight\"><pre><code class=\"language-text\">https://mxnet-bing.readthedocs.io/en/latest/zh/overview.html</code></pre></div><p>深度学习系统通常有两种编程方式，一种是<b>声明式编程(declarative programming)</b>，用户只需要声明要做什么，而具体执行则由系统完成。以Caffe，TensorFlow的计算图为代表。优点是由于在真正开始计算的时候已经拿到了整个计算图，所以可以做一系列优化来提升性能。实现辅助函数也容易，例如对任何计算图都提供forward和backward函数，另外也方便对计算图进行可视化，将图保存到硬盘和从硬盘读取。缺点是debug很麻烦，监视一个复杂的计算图中的某个节点的中间结果并不简单，逻辑控制也不方便。</p><p>一种是<b>命令式编程(imperative programming)</b>，以numpy，torch/pytorch为代表，每个语句按照原来的意思顺序执行。它 的特点是语义上容易理解，灵活，可以精确控制行为。通常可以无缝地和主语言交互，方便地利用主语言的各类算法，工具包，debug和性能调试器，但是实现统一的辅助函数和提供整体优化都很困难。</p><p>MXNet尝试将两种模式无缝的结合起来。在命令式编程上MXNet提供张量运算，进行模型的迭代训练和更新中的控制逻辑；在声明式编程中MXNet支持符号表达式，用来描述神经网络，并利用系统提供的自动求导来训练模型。</p><p>随着pytorch的崛起，mxnet已经掉出前三梯队，但不影响喜欢它的人使用。相比于重量级的tensorflow，mxnet非常轻量，占用内存少，分布式训练方便，常被用于比赛刷榜（见笔者以前用来刷榜的文）。</p><p><a href=\"https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649028744%26idx%3D1%26sn%3D49425665ad69c96f33a60e6f0fe50cde%26chksm%3D871346f5b064cfe393445c6fc8ffa0b159df03eecf007dfa4ba4866dac0fa644ef92cf2bf80a%26scene%3D21%23wechat_redirect\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">如何步入深度学习刷榜第一重境界</a><br/></p><h2>02 <b>mxnet安装配置</b></h2><p>喜欢自定义安装和精确控制版本的朋友，可以自行编译，喜欢偷懒的pip安装即可，方便快捷。</p><div class=\"highlight\"><pre><code class=\"language-text\">sudo pip install mxnet</code></pre></div><p>不过如果你要多机多卡使用，还是源码编译安装吧。</p><div class=\"highlight\"><pre><code class=\"language-text\">https://github.com/apache/incubator-mxnet</code></pre></div><h2>03 <b>mxnet自定义数据</b></h2><p>下面就开始我们的任务，跟以往项目一样，从自定义数据和自定义网络开始。</p><p>mxnet分类任务要求的输入分类文件的格式与caffe不一样，为下面的格式，其中分别是序号，标签，路径</p><div class=\"highlight\"><pre><code class=\"language-text\">01../../../../../datas/mouth/1/182smile.jpg\n11../../../../../datas/mouth/1/435smile.jpg</code></pre></div><p>数据的载入需要用到接口DataBatch和DataIter</p><div class=\"highlight\"><pre><code class=\"language-text\">https://mxnet.incubator.apache.org/api/python/io/io.html</code></pre></div><p>首先我们定义一下相关的参数配置，主要用于载入训练/测试数据集路径data-train，data-val，rgb均值rgb-mean，类别数目num-classes与训练样本集大小num-examples</p><div class=\"highlight\"><pre><code class=\"language-text\">def add_data_args(parser):\n    data = parser.add_argument_group(&#39;Data&#39;, &#39;the input images&#39;)\n    data.add_argument(&#39;--data-train&#39;, type=str, help=&#39;the training data&#39;)\n    data.add_argument(&#39;--data-val&#39;, type=str, help=&#39;the validation data&#39;)\n    data.add_argument(&#39;--rgb-mean&#39;, type=str, default=&#39;123.68,116.779,103.939&#39;,help=&#39;a tuple of size 3 for the mean rgb&#39;)\n    data.add_argument(&#39;--pad-size&#39;, type=int, default=0,\nhelp=&#39;padding the input image&#39;)\n    data.add_argument(&#39;--image-shape&#39;, type=str,\nhelp=&#39;the image shape feed into the network, e.g. (3,224,224)&#39;)\n    data.add_argument(&#39;--num-classes&#39;, type=int,help=&#39;the number of classes&#39;)\n    data.add_argument(&#39;--num-examples&#39;, type=int, help=&#39;the number of training examples&#39;)\n    data.add_argument(&#39;--data-nthreads&#39;, type=int, default=4,help=&#39;number of threads for data decoding&#39;)\n    data.add_argument(&#39;--benchmark&#39;, type=int, default=0,\nhelp=&#39;if 1, then feed the network with synthetic data&#39;)\n    data.add_argument(&#39;--dtype&#39;, type=str, default=&#39;float32&#39;,help=&#39;data type: float32 or float16&#39;)\n    return data</code></pre></div><p>然后，使用mx.img.ImageIter来载入图像数据</p><div class=\"highlight\"><pre><code class=\"language-text\"> train = mx.img.ImageIter(\n        label_width         = 1,\n        path_root    = &#39;data/&#39;, \n        path_imglist         = args.data_train,\n        data_shape          = (3, N_pix, N_pix),\n        batch_size          = args.batch_size,\n        rand_crop           = True,\n        rand_resize         = True,\n        rand_mirror         = True,\n        shuffle             = True,\n        brightness          = 0.4,\n        contrast            = 0.4,\n        saturation          = 0.4,\n        pca_noise           = 0.1,\n        num_parts           = nworker,\n        part_index          = rank)</code></pre></div><p>注意到上面配置了rand_crop，rand_resize，rand_mirror，shuffle，brightness，contrast，saturation，pca_noise等选项，这些就是常见的数据增强操作了，如果不懂，可以去看看我们以前的文章</p><p><a href=\"https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029048%26idx%3D1%26sn%3Dec708683cb6a3c2ed048a945a7150b79%26chksm%3D871347c5b064ced3fd3d57c5c79df0087890efb10898076efb14e9ece8ddf38906dbaf33af2c%26scene%3D21%23wechat_redirect\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">[综述类] 一文道尽深度学习中的数据增强方法（上）</a></p><p><a href=\"https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029110%26idx%3D1%26sn%3D4debbbe890b48ab739fec5967868746b%26chksm%3D8713478bb064ce9da68dd57b419ddebd22884c05747abb9286c1e5bc6563702f2a1fd4bcac64%26scene%3D21%23wechat_redirect\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【技术综述】深度学习中的数据增强（下）</a></p><p><a href=\"https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029159%26idx%3D1%26sn%3D0d9d4bf2f504f34c5f2a444e2ca3d6d1%26chksm%3D8713445ab064cd4c02d19bd4bd206609de36cea301277be0c239c4f94c2355ee5ec9afb871be%26scene%3D21%23wechat_redirect\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【开源框架】一文道尽主流开源框架中的数据增强</a><br/></p><p>mxnet的数据增强接口使用非常的方便，定义如下</p><div class=\"highlight\"><pre><code class=\"language-text\">def add_data_aug_args(parser):\n    aug = parser.add_argument_group(\n        &#39;Image augmentations&#39;, &#39;implemented in src/io/image_aug_default.cc&#39;)\n    aug.add_argument(&#39;--random-crop&#39;, type=int, default=1,help=&#39;if or not randomly crop the image&#39;)\n    aug.add_argument(&#39;--random-mirror&#39;, type=int, default=1,help=&#39;if or not randomly flip horizontally&#39;)\n    aug.add_argument(&#39;--max-random-h&#39;, type=int, default=0,help=&#39;max change of hue, whose range is [0, 180]&#39;)\n    aug.add_argument(&#39;--max-random-s&#39;, type=int, default=0,help=&#39;max change of saturation, whose range is [0, 255]&#39;)\n    aug.add_argument(&#39;--max-random-l&#39;, type=int, default=0,help=&#39;max change of intensity, whose range is [0, 255]&#39;)\n    aug.add_argument(&#39;--max-random-aspect-ratio&#39;, type=float, default=0,help=&#39;max change of aspect ratio, whose range is [0, 1]&#39;)\n    aug.add_argument(&#39;--max-random-rotate-angle&#39;, type=int, default=0,help=&#39;max angle to rotate, whose range is [0, 360]&#39;)\n    aug.add_argument(&#39;--max-random-shear-ratio&#39;, type=float, default=0,help=&#39;max ratio to shear, whose range is [0, 1]&#39;)\n    aug.add_argument(&#39;--max-random-scale&#39;, type=float, default=1,help=&#39;max ratio to scale&#39;)\n    aug.add_argument(&#39;--min-random-scale&#39;, type=float, default=1,help=&#39;min ratio to scale, should &gt;= img_size/input_shape. otherwise use --pad-size&#39;)\n    return aug</code></pre></div><p>可以看到level &gt;= 1，就可以使用随机裁剪，镜像操作，level &gt;= 2，就可以使用对比度变换操作，level &gt;= 3，就可以使用旋转，缩放等操作。</p><div class=\"highlight\"><pre><code class=\"language-text\">def set_data_aug_level(aug, level):\n    if level &gt;= 1:\n        aug.set_defaults(random_crop=1, random_mirror=1)\n    if level &gt;= 2:\n        aug.set_defaults(max_random_h=36, max_random_s=50, max_random_l=50)\n    if level &gt;= 3:\n        aug.set_defaults(max_random_rotate_angle=10, max_random_shear_ratio=0.1, max_random_aspect_ratio=0.25)</code></pre></div><h2>04 <b>mxnet网络搭建</b></h2><p>同样是三层卷积，两层全连接的网络，话不多说，直接上代码，使用到的api是mxnet.symbol</p><div class=\"highlight\"><pre><code class=\"language-text\">import mxnet as mx\ndef get_symbol(num_classes, **kwargs):\n   if &#39;use_global_stats&#39; not in kwargs:\n       use_global_stats = False\n   else:\n       use_global_stats = kwargs[&#39;use_global_stats&#39;]\n   data = mx.symbol.Variable(name=&#39;data&#39;)\n   conv1 = mx.symbol.Convolution(name=&#39;conv1&#39;, data=data , num_filter=12, kernel=(3,3), stride=(2,2), no_bias=True)\n   conv1_bn = mx.symbol.BatchNorm(name=&#39;conv1_bn&#39;, data=conv1 , use_global_stats=use_global_stats, fix_gamma=False, eps=0.000100)\n   conv1_scale = conv1_bn\n   relu1 = mx.symbol.Activation(name=&#39;relu1&#39;, data=conv1_scale , act_type=&#39;relu&#39;)\n   conv2 = mx.symbol.Convolution(name=&#39;conv2&#39;, data=relu1 , num_filter=24, kernel=(3,3), stride=(2,2), no_bias=True)\n   conv2_bn = mx.symbol.BatchNorm(name=&#39;conv2_bn&#39;, data=conv2 , use_global_stats=use_global_stats, fix_gamma=False, eps=0.000100)\n   conv2_scale = conv2_bn\n   relu2 = mx.symbol.Activation(name=&#39;relu2&#39;, data=conv2_scale , act_type=&#39;relu&#39;)\n   conv3 = mx.symbol.Convolution(name=&#39;conv3&#39;, data=relu2 , num_filter=48, kernel=(3,3), stride=(2,2), no_bias=True)\n   conv3_bn = mx.symbol.BatchNorm(name=&#39;conv3_bn&#39;, data=conv3 , use_global_stats=use_global_stats, fix_gamma=False, eps=0.000100)\n   conv3_scale = conv3_bn\n   relu3 = mx.symbol.Activation(name=&#39;relu3&#39;, data=conv3_scale , act_type=&#39;relu&#39;)\n   pool = mx.symbol.Pooling(name=&#39;pool&#39;, data=relu3 , pooling_convention=&#39;full&#39;, global_pool=True, kernel=(1,1), pool_type=&#39;avg&#39;)\n   fc = mx.symbol.Convolution(name=&#39;fc&#39;, data=pool , num_filter=num_classes, pad=(0, 0), kernel=(1,1), stride=(1,1), no_bias=False)\n   flatten = mx.symbol.Flatten(data=fc, name=&#39;flatten&#39;)\n   softmax = mx.symbol.SoftmaxOutput(data=flatten, name=&#39;softmax&#39;)\n   return softmax\nif __name__ == &#34;__main__&#34;:\n   net = get_symbol(2)  ##二分类任务\n   net.save(&#39;simpleconv3-symbol.json&#39;)</code></pre></div><p>最后我们可以将其存到json文件里，net.save(&#39;simpleconv3-symbol.json&#39;)，下面是conv1的部分，详细大家可以至git查看</p><div class=\"highlight\"><pre><code class=\"language-text\">    {\n      &#34;op&#34;: &#34;Convolution&#34;, \n      &#34;name&#34;: &#34;conv1&#34;, \n      &#34;attr&#34;: {\n        &#34;kernel&#34;: &#34;(3, 3)&#34;, \n        &#34;no_bias&#34;: &#34;True&#34;, \n        &#34;num_filter&#34;: &#34;12&#34;, \n        &#34;stride&#34;: &#34;(2, 2)&#34;\n      }, \n      &#34;inputs&#34;: [[0, 0, 0], [1, 0, 0]]\n    },</code></pre></div><h2>05 <b>模型训练、测试</b></h2><p><b>5.1 模型训练</b></p><p>准备工作都做好了，训练代码非常简洁，下面就是全部的代码</p><div class=\"highlight\"><pre><code class=\"language-text\">import os\nimport argparse\nimport logging\nlogging.basicConfig(level=logging.DEBUG)\nfrom common import find_mxnet\nfrom common import data, fit\nimport mxnet as mx\nimport os, urllib\nif __name__ == &#34;__main__&#34;:\n   parser = argparse.ArgumentParser(description=&#34;simple conv3 net&#34;,\n                                    formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n   train = fit.add_fit_args(parser)\n   data.add_data_args(parser)\n   aug = data.add_data_aug_args(parser)\n   data.set_data_aug_level(parser, 1)\n   parser.set_defaults(image_shape=&#39;3,48,48&#39;, num_epochs=200,\n                       lr=.001, wd=0)\n   args = parser.parse_args()\n   # define simpleconv3\n   net = mx.sym.load(&#39;models/simple-conv3-symbol.json&#39;)\n   print &#34;net&#34;,net\n   # train\n   fit.fit(args        = args,\n           network     = net,\n           data_loader = data.get_rec_iter)</code></pre></div><p>其中调用了fit接口定义优化目标和策略，我们只分析其中的核心代码，首先是模型创建</p><div class=\"highlight\"><pre><code class=\"language-text\">   model = mx.mod.Module(\n       context       = devs,\n       symbol        = network\n   )</code></pre></div><p>然后是optimizer配置，默认使用adam</p><div class=\"highlight\"><pre><code class=\"language-text\">   optimizer_params = {\n           &#39;learning_rate&#39;: lr,\n           &#39;wd&#39; : args.wd\n   }\n</code></pre></div><p>初始化</p><div class=\"highlight\"><pre><code class=\"language-text\">   initializer = mx.init.Xavier(rnd_type=&#39;gaussian&#39;, factor_type=&#34;in&#34;, magnitude=2.34)</code></pre></div><p>最后是完整的接口</p><div class=\"highlight\"><pre><code class=\"language-text\">   model.fit(train,\n       begin_epoch        = args.load_epoch if args.load_epoch else 0,\n       num_epoch          = args.num_epochs,\n       eval_data          = val,\n       eval_metric        = eval_metrics,\n       kvstore            = kv,\n       optimizer          = args.optimizer,\n       optimizer_params   = optimizer_params,\n       initializer        = initializer,\n       arg_params         = arg_params,\n       aux_params         = aux_params,\n       batch_end_callback = batch_end_callbacks,\n       epoch_end_callback = checkpoint,\n       allow_missing      = True,\n       monitor            = monitor)</code></pre></div><p>然后开始愉快的训练吧</p><div class=\"highlight\"><pre><code class=\"language-text\">python train.py --gpus 0 \\\n   --data-train data/train.txt \\\n   --model-prefix &#39;models/simple-conv3&#39; \\\n   --batch-size 80 --num-classes 2 --num-examples 900 2&gt;&amp;1 | tee log.txt</code></pre></div><p>训练模型会存为simple-conv3-epoch.params的格式。</p><p><b>5.2 训练过程可视化</b></p><p>由于前面我们的tensorflow，pytorch，keras都使用了tensorborad进行可视化，mxnet也可以借助tensorboard进行可视化，只需要再设计一些mxnet接口即可。具体方法不再赘述，参考<a href=\"https://link.zhihu.com/?target=https%3A//github.com/awslabs/mxboard\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/awslabs/mxbo</span><span class=\"invisible\">ard</span><span class=\"ellipsis\"></span></a></p><p>网络结构的可视化则可用mx.viz.plot_network(sym).view()。</p><p><b>5.3 模型测试</b></p><p>使用mx.model.load_checkpoint载入预训练的模型，如下</p><div class=\"highlight\"><pre><code class=\"language-text\">epoch = int(sys.argv[1]) #check point step\ngpu_id = int(sys.argv[2]) #GPU ID for infer\nprefix = sys.argv[3]\nctx = mx.gpu(gpu_id)\nsym, arg_params, aux_params = mx.model.load_checkpoint(prefix, epoch)\narg_params, aux_params = ch_dev(arg_params, aux_params, ctx)</code></pre></div><p>然后使用bind接口进行forward，具体操作如下</p><div class=\"highlight\"><pre><code class=\"language-text\">sym  = mx.symbol.SoftmaxOutput(data = sym, name = &#39;softmax&#39;)       \n\nimg_full_name = os.path.join(imgdir,imgname)\nimg = cv2.cvtColor(cv2.imread(img_full_name), cv2.COLOR_BGR2RGB)\nimg = np.float32(img)\nrows, cols = img.shape[:2]\nresize_width = 48\nresize_height = 48\nimg = cv2.resize(img, (resize_width, resize_height), interpolation=cv2.INTER_CUBIC)\nh, w, _ = img.shape\n\nimg_crop = img[0:h,0:w] ##此处使用整图\nimg_crop = np.swapaxes(img_crop, 0, 2)\nimg_crop = np.swapaxes(img_crop, 1, 2)  # mxnet的训练是rgb的顺序输入，所以需要调整为r,g,b训练\nimg_crop = img_crop[np.newaxis, :]\n\narg_params[&#34;data&#34;] = mx.nd.array(img_crop, ctx)\narg_params[&#34;softmax_label&#34;] = mx.nd.empty((1,), ctx)\nexe = sym.bind(ctx, arg_params ,args_grad=None, grad_req=&#34;null&#34;, aux_states=aux_params)\nexe.forward(is_train=False)\nprobs = exe.outputs[0].asnumpy()</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-60feffab1842911ce09ae93eeaae724f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"3999\" data-rawheight=\"2250\" class=\"origin_image zh-lightbox-thumb\" width=\"3999\" data-original=\"https://pic4.zhimg.com/v2-60feffab1842911ce09ae93eeaae724f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;3999&#39; height=&#39;2250&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"3999\" data-rawheight=\"2250\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"3999\" data-original=\"https://pic4.zhimg.com/v2-60feffab1842911ce09ae93eeaae724f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-60feffab1842911ce09ae93eeaae724f_b.jpg\"/></figure><blockquote>本系列完整文章：</blockquote><p>第一篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029846%26idx%3D1%26sn%3D0c343cfd0ede5c8ae1405bd6348aefad%26chksm%3D871342abb064cbbd7fe31fb3c55f23875f27e48fb8354e9855823b1701f1227c71b4eb00de50%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【caffe速成】caffe图像分类从模型自定义到测试</a></p><p>第二篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029846%26idx%3D2%26sn%3D7c2582243bcd8f8b491e8e466a21978f%26chksm%3D871342abb064cbbd0cba24b408ceda2b64a7c8b6baa07f9f8f56cd4d1233caa0b80fe357753e%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【tensorflow速成】Tensorflow图像分类从模型自定义到测试</a></p><p>第三篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029881%26idx%3D1%26sn%3D3c869fcee3b48d3582952ab9a0683ea6%26chksm%3D87134284b064cb924c5e7231b3f2c36ba27e3a689b067f569f2e086f62b18413bcebc5987a07%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【pytorch速成】Pytorch图像分类从模型自定义到测试</a></p><p>第四篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029887%26idx%3D1%26sn%3D645b97809c24922352a0b39f19c9ef0c%26chksm%3D87134282b064cb9441af68124d205d9c7dedcaeb09788f4d586b949584e556eddd3a72217f69%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【paddlepaddle速成】paddlepaddle图像分类从模型自定义到测试</a></p><p>第五篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029896%26idx%3D1%26sn%3Df3f7b9cf69c514f45d1d14205f879270%26chksm%3D87134375b064ca6323354c40f3e55b02ff0d1d24f3dacfc980190d51f20ec9ec16e60c1a4741%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【Keras速成】Keras图像分类从模型自定义到测试</a></p><p>第六篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029904%26idx%3D1%26sn%3D0bdc6947f5ac68e7f68426b9d076b4ab%26chksm%3D8713436db064ca7b3b2a2a1d6a8d24c15069f72655e2c39e2498051fa0e56bbcf6fe9c332d5b%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【mxnet速成】mxnet图像分类从模型自定义到测试</a></p><p>第七篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649030976%26idx%3D1%26sn%3D0befc170a93d365b780c5f05b2f510a4%26chksm%3D8712bf3db065362b0aeaae82bdbac467be5697b34cac2797e2ee15cdcb97474c08640482bf07%26token%3D1695328272%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【cntk速成】cntk图像分类从模型自定义到测试</a></p><p>第八篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26tempkey%3DMTAwMF93Q3VDbklva2NBZDJXV1dnNWloZjJ2YWtZWmJ2aTRRaGVDMVNfbERSN2xVYXdXd3B4eWczWWdMbnQzRjZrRVpmSGRYMndrLTQ0VnhPcURTNGhsS3dYWGd3MTk0UGtmZkltU0U2U01OUmRfQzcySDVObDlxM3R2U3Q0SlNQWGRMc1NZeWtWTlVRN1NpRlJaWXRYcGdydUNOTUR6clUzVkNleVlKUE5nfn4%253D%26chksm%3D0712ba9b3065338de13ea030951ba2c25c0363d958c905e12c937c2acffa991313abb4d9f751%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【chainer速成】chainer图像分类从模型自定义到测试</a></p><p>第九篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649032012%26idx%3D1%26sn%3Df74c7084621f367adb2518ebec61ca42%26chksm%3D8712bb31b06532270b9ca9550ab48ff78adaad7d38c73645cfb8888218b8e177bebd5d401aa9%26token%3D1258619827%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【DL4J速成】Deeplearning4j图像分类从模型自定义到测试</a></p><p>第十篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649032109%26idx%3D2%26sn%3Da6ff48ec0ae5d8e7a494df7e564d9ac9%26chksm%3D8712bbd0b06532c61d98c786ba1773c29c3dcf1291f9c3cd052c5643e24699eef28481e94b8e%26token%3D1258619827%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【MatConvnet速成】MatConvnet图像分类从模型自定义到测试</a></p><p>第十一篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649032128%26idx%3D2%26sn%3Df889e2255c0ec4960f76ad0383363849%26chksm%3D8712bbbdb06532ab136c0a485bebc5cf181f24a34bec7bab3f97f66be627e09d939b997dae90%26token%3D598159941%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【Lasagne速成】Lasagne/Theano图像分类从模型自定义到测试</a></p><p>第十二篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649032137%26idx%3D2%26sn%3Da9770ead4a9d03f0a4e75be86657453b%26chksm%3D8712bbb4b06532a27f0f02ac30ce4a18eeac1f72ac54b3c5c1b6ec13fee29f03bfc0dcaa1efd%26token%3D1446627305%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【darknet速成】Darknet图像分类从模型自定义到测试</a></p>", 
            "topic": [
                {
                    "tag": "MXNet", 
                    "tagLink": "https://api.zhihu.com/topics/20044209"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "开源项目", 
                    "tagLink": "https://api.zhihu.com/topics/19565961"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/45377175", 
            "userName": "言有三-龙鹏", 
            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
            "upvote": 15, 
            "title": "【Keras速成】Keras图像分类从模型自定义到测试", 
            "content": "<p>首这一次我们讲讲keras这个简单、流行的深度学习框架，一个图像分类任务从训练到测试出结果的全流程。</p><p>相关的代码、数据都在我们 Git 上，希望大家 Follow 一下这个 Git 项目，后面会持续更新不同框架下的任务。</p><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/longpeng2008/LongPeng_ML_Course\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/longpeng2008</span><span class=\"invisible\">/LongPeng_ML_Course</span><span class=\"ellipsis\"></span></a> </p><h2>01 keras是什么</h2><p>Keras是一个非常流行、简单的深度学习框架，它的设计参考了torch，用Python语言编写，是一个高度模块化的神经网络库，支持GPU和CPU。能够在TensorFlow，CNTK或Theano之上运行。 Keras的特点是能够快速实现模型的搭建， 简单方便地让你实现从想法到实验验证的转化，这都是高效地进行科学研究的关键。</p><h2>02 Keras 安装配置</h2><p>Keras的安装非常简单，但是需要先安装一个后端框架作为支撑，TensorFlow， CNTK，Theano都可以，但是官网上强烈建议使用TensorFlow作为Keras的后端进行使用。本例以TensorFlow 1.4.0 版本作为Keras的后端进行测试。</p><div class=\"highlight\"><pre><code class=\"language-text\">sudo pip install tensorflow==1.4.0\nsudo pip install keras==2.1.4</code></pre></div><p>通过上面两条命令就可以完成TensorFlow和Keras的安装，此处需要注意的一点是Keras的版本和TensorFlow的版本要对应，否则会出现意外的错误。具体版本对应关系可在网上进行查询。</p><h2>03 Keras 自定义数据</h2><p><b>3.1 MNIST实例</b></p><p>MNIST手写字符分类被认为是深度学习框架里的“Hello Word！”，下面简单介绍一下MNIST数据集案例的测试。Keras的官方github的example目录下提供了几个MNIST案例的代码，下载mnist_mlp.py，mnist_cnn.py文件，本地运行即可，其他文件读者也可以自行测试。</p><p><b>3.2  数据定义</b></p><p>前面我们介绍了MNIST数据集实例，很多读者在学习深度学习框架的时候都卡在了这一步，运行完MNIST实例之后无从下手，很大原因可能是因为不知道怎么处理自己的数据集，这一节我们通过一个简单的图像二分类案例，介绍如何实现一个自定义的数据集。</p><p>数据处理有几种方式，一种是像MNIST、CIFAR数据集，这些数据集的特点是已经为用户打包封装好了数据。用户只要load_data即可实现数据导入。其实就是事先把数据进行解析，然后保存到.pkl 或者.h5等文件中，然后在训练模型的时候直接导入，输入到网络中；另一种是直接从本地读取文件，解析成网络需要的格式，输入网络进行训练。但是实际情况是，为了某一个项目我们不可能总是找到相应的打包好的数据集供使用，这时候自己建立一个dataset就十分重要。</p><p>Keras提供了一个图像数据的数据增强文件，调用这个文件我们可以实现网络数据加载的功能。</p><p>此处采用keras的processing模块里的ImageDataGenerator类定义一个图像分类任务的dataset生成器：</p><div class=\"highlight\"><pre><code class=\"language-text\">train_data_dir = &#39;../../../../datas/head/train/&#39;\nvalidation_data_dir = &#39;../../../../datas/head/val&#39;\n# augmentation configuration we will use for training\ntrain_datagen = ImageDataGenerator(\n        rescale=1. / 255,\n        shear_range=0.2,\n        zoom_range=0.2,\n        horizontal_flip=True)\n# augmentation configuration use for testing only rescaling\nval_datagen = ImageDataGenerator(rescale=1. / 255)\ntrain_generator = train_datagen.flow_from_directory(\n        train_data_dir,\n        target_size=(48, 48),\n        batch_size=16)\nval_generator = val_datagen.flow_from_directory(\n        validation_data_dir,\n        target_size=(48, 48),\n        batch_size=16)</code></pre></div><p>下面简单地介绍一下上面的代码，完整代码请移步Git工程。</p><p>Keras的processing模块中提供了一个能够实时进行数据增强的图像生成类ImagGenerator，该类下面有一个函数flow_from_directory，顾名思义该函数就是从文件夹中获取图像数据。关于ImageGenerator更多的使用可以参考官方源码。数据集结构组织如下：</p><div class=\"highlight\"><pre><code class=\"language-text\">datas/train/left/*.jpg\ndatas/train/right/*.jpg\ndatas/val/left/*.jpg\ndatas/val/right/*.jpg</code></pre></div><p>此处还需要注意的一点是，我们现在进行的是简单的图像分类任务训练，假如要完成语义分割，目标检测等任务，则需要自定义一个类（继承ImageDataGenerator），具体实现可以查询相关代码进行参考。</p><h2>04 Keras 网络搭建</h2><p>Keras网络模型搭建有两种形式，Sequential 顺序模型和使用函数式API的 Model 类模型。本教程的例子采用一个简单的三层卷积，以及两层全连接和一个分类层组成的网络模型。由于函数式API更灵活方便，因此下面采用函数式方法搭建模型，模型定义如下：</p><p><b>4.1  函数式API</b></p><div class=\"highlight\"><pre><code class=\"language-text\">def simpleconv3(input_shape=(48, 48, 3), classes=2):\n    img_input = Input(shape=input_shape)\n    bn_axis = 3\n    x = Conv2D(12, (3, 3), strides=(2, 2), padding=&#39;same&#39;, name=&#39;conv1&#39;)(img_input)\n    x = BatchNormalization(axis=bn_axis, name=&#39;bn_conv1&#39;)(x)\n    x = Activation(&#39;relu&#39;)(x)\n    x = Conv2D(24, (3, 3), strides=(2, 2), padding=&#39;same&#39;, name=&#39;conv2&#39;)(x)\n    x = BatchNormalization(axis=bn_axis, name=&#39;bn_conv2&#39;)(x)\n    x = Activation(&#39;relu&#39;)(x)\n    x = Conv2D(48, (3, 3), strides=(2, 2), padding=&#39;same&#39;, name=&#39;conv3&#39;)(x)\n    x = BatchNormalization(axis=bn_axis, name=&#39;bn_conv3&#39;)(x)\n    x = Activation(&#39;relu&#39;)(x)\n    x = Flatten()(x)\n    x = Dense(1200, activation=&#39;relu&#39;)(x)\n    x = Dense(128, activation=&#39;relu&#39;)(x)\n    x = Dense(classes, activation=&#39;softmax&#39;)(x)\n    model = Model(img_input, x)\n    return model</code></pre></div><p>x = Conv2D(12, (3, 3), strides=(2, 2), padding=&#39;same&#39;, name=&#39;conv1&#39;)(img_input)</p><p>即输出是12通道，卷积核大小3*3，步长为2，padding=&#39;same&#39;表示边缘补零</p><p>x = BatchNormalization(axis=bn_axis, name=&#39;bn_conv1&#39;)(x)</p><p>axis表示需要归一化的坐标轴，bn_axis=3，由于采用TensorFlow作为后端，因此这句代码表示在通道数坐标轴进行归一化。</p><p>x = Flatten()(x) 表示将卷积特征图进行拉伸，以便和全连接层Dense()进行连接。</p><p>x = Dense(1200, activation=&#39;relu&#39;)(x)</p><p>Dense()实现全连接层的功能，1200是输出维度，‘relu&#39;表示激活函数，使用其他函数可以自行修改。</p><p>最后一层采用‘softmax’激活函数实现分类功能。</p><p>最终返回Model，包含网络的输入和输出。</p><p><b>4.2 模型编译</b></p><p>网络搭建完成，在网络训练前需要进行编译，包括学习方法、损失函数、评估标准等，这些参数分别可以从optimizer、loss、metric模块中导入。具体代码如下：</p><div class=\"highlight\"><pre><code class=\"language-text\">from keras.optimizers import SGD\nfrom keras.losses import binary_crossentropy\nfrom keras.metrics import binary_accuracy\nfrom keras.callbacks import TensorBoard\ntensorboard = TensorBoard(log_dir=(&#39;./logs&#39;))\ncallbacks = []\ncallbacks.append(tensorboard)\nloss = binary_crossentropy\nmetrics = [binary_accuracy]\noptimizer = SGD(lr=0.001, decay=1e-6, momentum=0.9)</code></pre></div><p>其中callbacks模块包含了TensorBoard， ModelCheckpoint，LearningRateScheduler等功能，分别可以用来可视化模型，设置模型检查点，以及设置学习率策略。</p><h2>05 模型训练、测试</h2><p><b>5.1 模型训练</b></p><p>Keras模型训练过程非常简单，只需一行代码，设置几个参数即可，具体代码如下：</p><div class=\"highlight\"><pre><code class=\"language-text\">history = model.fit_generator(\n        train_generator,\n        steps_per_epoch=num_train_samples // batch_size,\n        epochs=epochs,\n        callbacks=callbacks,\n        validation_data=val_generator,\n        validation_steps=num_val_samples // batch_size)</code></pre></div><p>首先指定数据生成器，train_generator, 前面介绍过；steps_per_epoch是每次epoch循环的次数，通过训练样本数除以batch_size得到；epochs是整个数据集重复多少次训练。</p><p>Keras是高度封装的，在模型训练过程中，看不到网络的预测结果和网络的反向传播过程，只需定义好损失函数，事实上，网络定义中的模型输出会包含网络的输入和输出。</p><p><b>5.2 训练过程可视化</b></p><p>keras可以采用tensorboard实现训练过程的可视化。执行完下面的命令就可以在浏览器访问<a href=\"https://link.zhihu.com/?target=http%3A//127.0.0.1%3A6006\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">127.0.0.1:6006</span><span class=\"invisible\"></span></a>查看效果。</p><p>tensorboard --logdir 日志文件路径（默认路径=‘./logs’’）</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-515b1a682f1ec248e7139af85b665173_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"685\" data-rawheight=\"240\" class=\"origin_image zh-lightbox-thumb\" width=\"685\" data-original=\"https://pic4.zhimg.com/v2-515b1a682f1ec248e7139af85b665173_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;685&#39; height=&#39;240&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"685\" data-rawheight=\"240\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"685\" data-original=\"https://pic4.zhimg.com/v2-515b1a682f1ec248e7139af85b665173_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-515b1a682f1ec248e7139af85b665173_b.jpg\"/></figure><p>上面是分别是训练和测试过程的loss和accuracy。</p><p><b>5.3 模型测试</b></p><div class=\"highlight\"><pre><code class=\"language-text\">model = simpleconv3()\nmodel.load_weights(model_path, by_name=True)\nimage_path = &#39;../../../../datas/head/train/0/1left.jpg&#39;\nimg = Image.open(image_path)\nimg = img_to_array(img)\nimg = cv2.resize(img, image_size)\nimg = np.expand_dims(img, axis=0)\nimg = preprocess_input(img)\nresult = model.predict(img, batch_size=1)\nprint(result)</code></pre></div><p>以上代码简单介绍一下：模型测试流程非常清晰，首先加载模型，加载参数&gt;&gt;将数据输入网络&gt;&gt;模型预测。</p><h2>06 模型保存和导入</h2><div class=\"highlight\"><pre><code class=\"language-text\">model = train_model(model, loss, metrics,  optimizer, num_epochs)\nos.mkdir(&#39;models&#39;)\nmodel.save_weights(&#39;models/model.h5&#39;)</code></pre></div><p>模型训练完成后，仅需用model.save_weights(&#39;models/model.h5&#39;)一句代码就可以完成模型的保存。同样，模型的导入采用model.load_weights(model_path, by_name=True)，需要注意的是要设置by_name=True，这样就能保证和模型名称一样的参数都能加载到模型，当然模型定义要和参数是匹配的，假如要进行fine-tune我们只需保证需要重新训练或者新加的网络层的名称和预加载模型参数名称不一样就可以。</p><h2>07 总结</h2><p>以上内容涵盖了采用keras进行分类任务的全部流程，从数据导入、模型搭建、模型训练、测试，模型保存和导入几个方面分别进行了介绍。当然这只是一些基本的应用，还有一些高级、个性化功能需要我们进一步学习，有机会，下一次介绍一下自定义网络层、设置check_point、特征可视化等特性。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-60feffab1842911ce09ae93eeaae724f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"3999\" data-rawheight=\"2250\" class=\"origin_image zh-lightbox-thumb\" width=\"3999\" data-original=\"https://pic4.zhimg.com/v2-60feffab1842911ce09ae93eeaae724f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;3999&#39; height=&#39;2250&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"3999\" data-rawheight=\"2250\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"3999\" data-original=\"https://pic4.zhimg.com/v2-60feffab1842911ce09ae93eeaae724f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-60feffab1842911ce09ae93eeaae724f_b.jpg\"/></figure><blockquote>本系列完整文章：</blockquote><p>第一篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029846%26idx%3D1%26sn%3D0c343cfd0ede5c8ae1405bd6348aefad%26chksm%3D871342abb064cbbd7fe31fb3c55f23875f27e48fb8354e9855823b1701f1227c71b4eb00de50%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【caffe速成】caffe图像分类从模型自定义到测试</a></p><p>第二篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029846%26idx%3D2%26sn%3D7c2582243bcd8f8b491e8e466a21978f%26chksm%3D871342abb064cbbd0cba24b408ceda2b64a7c8b6baa07f9f8f56cd4d1233caa0b80fe357753e%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【tensorflow速成】Tensorflow图像分类从模型自定义到测试</a></p><p>第三篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029881%26idx%3D1%26sn%3D3c869fcee3b48d3582952ab9a0683ea6%26chksm%3D87134284b064cb924c5e7231b3f2c36ba27e3a689b067f569f2e086f62b18413bcebc5987a07%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【pytorch速成】Pytorch图像分类从模型自定义到测试</a></p><p>第四篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029887%26idx%3D1%26sn%3D645b97809c24922352a0b39f19c9ef0c%26chksm%3D87134282b064cb9441af68124d205d9c7dedcaeb09788f4d586b949584e556eddd3a72217f69%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【paddlepaddle速成】paddlepaddle图像分类从模型自定义到测试</a></p><p>第五篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029896%26idx%3D1%26sn%3Df3f7b9cf69c514f45d1d14205f879270%26chksm%3D87134375b064ca6323354c40f3e55b02ff0d1d24f3dacfc980190d51f20ec9ec16e60c1a4741%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【Keras速成】Keras图像分类从模型自定义到测试</a></p><p>第六篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029904%26idx%3D1%26sn%3D0bdc6947f5ac68e7f68426b9d076b4ab%26chksm%3D8713436db064ca7b3b2a2a1d6a8d24c15069f72655e2c39e2498051fa0e56bbcf6fe9c332d5b%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【mxnet速成】mxnet图像分类从模型自定义到测试</a></p><p>第七篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649030976%26idx%3D1%26sn%3D0befc170a93d365b780c5f05b2f510a4%26chksm%3D8712bf3db065362b0aeaae82bdbac467be5697b34cac2797e2ee15cdcb97474c08640482bf07%26token%3D1695328272%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【cntk速成】cntk图像分类从模型自定义到测试</a></p><p>第八篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26tempkey%3DMTAwMF93Q3VDbklva2NBZDJXV1dnNWloZjJ2YWtZWmJ2aTRRaGVDMVNfbERSN2xVYXdXd3B4eWczWWdMbnQzRjZrRVpmSGRYMndrLTQ0VnhPcURTNGhsS3dYWGd3MTk0UGtmZkltU0U2U01OUmRfQzcySDVObDlxM3R2U3Q0SlNQWGRMc1NZeWtWTlVRN1NpRlJaWXRYcGdydUNOTUR6clUzVkNleVlKUE5nfn4%253D%26chksm%3D0712ba9b3065338de13ea030951ba2c25c0363d958c905e12c937c2acffa991313abb4d9f751%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【chainer速成】chainer图像分类从模型自定义到测试</a></p><p>第九篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649032012%26idx%3D1%26sn%3Df74c7084621f367adb2518ebec61ca42%26chksm%3D8712bb31b06532270b9ca9550ab48ff78adaad7d38c73645cfb8888218b8e177bebd5d401aa9%26token%3D1258619827%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【DL4J速成】Deeplearning4j图像分类从模型自定义到测试</a></p><p>第十篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649032109%26idx%3D2%26sn%3Da6ff48ec0ae5d8e7a494df7e564d9ac9%26chksm%3D8712bbd0b06532c61d98c786ba1773c29c3dcf1291f9c3cd052c5643e24699eef28481e94b8e%26token%3D1258619827%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【MatConvnet速成】MatConvnet图像分类从模型自定义到测试</a></p><p>第十一篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649032128%26idx%3D2%26sn%3Df889e2255c0ec4960f76ad0383363849%26chksm%3D8712bbbdb06532ab136c0a485bebc5cf181f24a34bec7bab3f97f66be627e09d939b997dae90%26token%3D598159941%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【Lasagne速成】Lasagne/Theano图像分类从模型自定义到测试</a></p><p>第十二篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649032137%26idx%3D2%26sn%3Da9770ead4a9d03f0a4e75be86657453b%26chksm%3D8712bbb4b06532a27f0f02ac30ce4a18eeac1f72ac54b3c5c1b6ec13fee29f03bfc0dcaa1efd%26token%3D1446627305%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【darknet速成】Darknet图像分类从模型自定义到测试</a></p>", 
            "topic": [
                {
                    "tag": "Keras", 
                    "tagLink": "https://api.zhihu.com/topics/20052139"
                }, 
                {
                    "tag": "TensorFlow", 
                    "tagLink": "https://api.zhihu.com/topics/20032249"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/45370103", 
            "userName": "言有三-龙鹏", 
            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
            "upvote": 3, 
            "title": "【开源框架】paddlepaddle图像分类从模型自定义到测试", 
            "content": "<p>这一次我们讲讲paddlepadle这个百度开源的机器学习框架，一个图像分类任务从训练到测试出结果的全流程。</p><p>将涉及到paddlepaddle和visualdl，git如下：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/PaddlePaddle\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/PaddlePaddle</span><span class=\"invisible\"></span></a></p><p>相关的代码、数据都在我们 Git 上，希望大家 Follow 一下这个 Git 项目，后面会持续更新不同框架下的任务。</p><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/longpeng2008/LongPeng_ML_Course\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">longpeng2008/LongPeng_ML_Course</a></p><h2><b>01 paddlepaddle是什么</b></h2><p>正所谓google有tensorflow，facebook有pytorch，amazon有mxnet，作为国内机器学习的先驱，百度也有PaddlePaddle，其中Paddle即Parallel Distributed Deep Learning(并行分布式深度学习)，整体使用起来与tensorflow非常类似。</p><div class=\"highlight\"><pre><code class=\"language-text\">sudo pip install paddlepaddle</code></pre></div><p>安装就是一条命令，话不多说上代码。</p><h2>02 paddlepaddle训练</h2><p>训练包括三部分，数据的定义，网络的定义，以及可视化和模型的存储。</p><p><b>2.1 数据定义</b></p><p>定义一个图像分类任务的dataset如下：</p><div class=\"highlight\"><pre><code class=\"language-text\">from multiprocessing import cpu_count\nimport paddle.v2 as paddle\nclass Dataset:\n    def __init__(self,cropsize,resizesize):\n        self.cropsize = cropsize\n        self.resizesize = resizesize\n    def train_mapper(self,sample):\n        img, label = sample\n        img = paddle.image.load_image(img)\n        img = paddle.image.simple_transform(img, self.resizesize, self.cropsize, True)\n        #print &#34;train_mapper&#34;,img.shape,label\n        return img.flatten().astype(&#39;float32&#39;), label\n    def test_mapper(self,sample):\n        img, label = sample\n        img = paddle.image.load_image(img)\n        img = paddle.image.simple_transform(img, self.resizesize, self.cropsize, False)\n        #print &#34;test_mapper&#34;,img.shape,label\n        return img.flatten().astype(&#39;float32&#39;), label\n    def train_reader(self,train_list, buffered_size=1024):\n        def reader():\n            with open(train_list, &#39;r&#39;) as f:\n                lines = [line.strip() for line in f.readlines()]\n                print &#34;len of train dataset=&#34;,len(lines)\n                for line in lines:\n                    img_path, lab = line.strip().split(&#39; &#39;)\n                    yield img_path, int(lab)\n        return paddle.reader.xmap_readers(self.train_mapper, reader,\n                                          cpu_count(), buffered_size)\n    def test_reader(self,test_list, buffered_size=1024):\n        def reader():\n            with open(test_list, &#39;r&#39;) as f:\n                lines = [line.strip() for line in f.readlines()]\n                print &#34;len of val dataset=&#34;,len(lines)\n                for line in lines:\n                    img_path, lab = line.strip().split(&#39; &#39;)\n                    yield img_path, int(lab)\n        return paddle.reader.xmap_readers(self.test_mapper, reader,\n                                          cpu_count(), buffered_size)</code></pre></div><p>从上面代码可以看出:</p><p>(1) 使用了paddle.image.load_image进行图片的读取，<br/>paddle.image.simple_transform进行了简单的图像变换，这里只有图像crop操作，更多的使用可以参考API。</p><p>(2)  使用了paddle.reader.xmap_readers进行数据的映射。</p><p><b>2.2 网络定义</b></p><div class=\"highlight\"><pre><code class=\"language-text\"># coding=utf-8\nimport paddle.fluid as fluid\ndef simplenet(input):\n   # 定义卷积块\n   conv1 = fluid.layers.conv2d(input=input, num_filters=12,stride=2,padding=1,filter_size=3,act=&#34;relu&#34;)\n   bn1 = fluid.layers.batch_norm(input=conv1)\n   conv2 = fluid.layers.conv2d(input=bn1, num_filters=12,stride=2,padding=1,filter_size=3,act=&#34;relu&#34;)\n   bn2 = fluid.layers.batch_norm(input=conv2)\n   conv3 = fluid.layers.conv2d(input=bn2, num_filters=12,stride=2,padding=1,filter_size=3,act=&#34;relu&#34;)\n   bn3 = fluid.layers.batch_norm(input=conv3)\n   fc1 = fluid.layers.fc(input=bn3, size=128, act=None)\n   return fc1,conv1</code></pre></div><p>与之前的caffe，pytorch，tensorflow框架一样，定义了一个3层卷积与2层全连接的网络。为了能够更好的进行可视化，我们使用了PaddlePaddle Fluid，Fluid的设计也是用来让用户像Pytorch和Tensorflow Eager Execution一样可以执行动态计算而不需要创建图。</p><p><b>2.3 可视化</b></p><p>paddlepaddle有与之配套使用的可视化框架，即visualdl。</p><p>visualdl是百度数据可视化实验室发布的深度学习可视化平台，它的定位与tensorboard很像，可视化内容包含了向量，参数直方图分布，模型结构，图像等功能，以后我们会详细给大家讲述，这次直接在代码中展示如何使用。</p><p>安装使用pip install --upgrade visualdl，使用下面的命令可以查看官方例子：</p><div class=\"highlight\"><pre><code class=\"language-text\">vdl_create_scratch_log\nvisualDL --logdir ./scratch_log --port 8080\nhttp://127.0.0.1:8080</code></pre></div><p>下面是loss和直方图的查看</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-be87cd7926e91c997ca6d8ff6ad4b91f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"499\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-be87cd7926e91c997ca6d8ff6ad4b91f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;499&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"499\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-be87cd7926e91c997ca6d8ff6ad4b91f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-be87cd7926e91c997ca6d8ff6ad4b91f_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-9944251e67453040596b9aedc9cfb897_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"800\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb\" width=\"800\" data-original=\"https://pic4.zhimg.com/v2-9944251e67453040596b9aedc9cfb897_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;800&#39; height=&#39;600&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"800\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"800\" data-original=\"https://pic4.zhimg.com/v2-9944251e67453040596b9aedc9cfb897_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-9944251e67453040596b9aedc9cfb897_b.jpg\"/></figure><p>在咱们项目中，具体使用方法如下</p><div class=\"highlight\"><pre><code class=\"language-text\"># 首先定义相关变量\n# 创建VisualDL，并指定log存储路径\nlogdir = &#34;./logs&#34;\nlogwriter = LogWriter(logdir, sync_cycle=10)\n# 创建loss的趋势图\nwith logwriter.mode(&#34;train&#34;) as writer:\n   loss_scalar = writer.scalar(&#34;loss&#34;)\n# 创建acc的趋势图\nwith logwriter.mode(&#34;train&#34;) as writer:\n   acc_scalar = writer.scalar(&#34;acc&#34;)\n# 定义输出频率\nnum_samples = 4\n# 创建卷积层和输出图像的图形化展示\nwith logwriter.mode(&#34;train&#34;) as writer:\n   conv_image = writer.image(&#34;conv_image&#34;, num_samples, 1)\n   input_image = writer.image(&#34;input_image&#34;, num_samples, 1)\n# 创建可视化的训练模型结构\nwith logwriter.mode(&#34;train&#34;) as writer:\n   param1_histgram = writer.histogram(&#34;param1&#34;, 100)</code></pre></div><p>然后在训练过程中进行记录，这是完整的训练代码，红色部分就是记录结果。</p><div class=\"highlight\"><pre><code class=\"language-text\"># coding=utf-8\nimport numpy as np\nimport os\nimport paddle.fluid as fluid\nimport paddle.fluid.framework as framework\nimport paddle.v2 as paddle\nfrom paddle.fluid.initializer import NormalInitializer\nfrom paddle.fluid.param_attr import ParamAttr\nfrom visualdl import LogWriter\nfrom dataset import Dataset\nfrom net_fluid import simplenet\n\n# 创建VisualDL，并指定当前该项目的VisualDL的路径\nlogdir = &#34;./logs&#34;\nlogwriter = LogWriter(logdir, sync_cycle=10)\n\n# 创建loss的趋势图\nwith logwriter.mode(&#34;train&#34;) as writer:\n   loss_scalar = writer.scalar(&#34;loss&#34;)\n\n# 创建acc的趋势图\nwith logwriter.mode(&#34;train&#34;) as writer:\n   acc_scalar = writer.scalar(&#34;acc&#34;)\n\n# 定义输出频率\nnum_samples = 4\n# 创建卷积层和输出图像的图形化展示\nwith logwriter.mode(&#34;train&#34;) as writer:\n   conv_image = writer.image(&#34;conv_image&#34;, num_samples, 1)\n   input_image = writer.image(&#34;input_image&#34;, num_samples, 1)\n\n# 创建可视化的训练模型结构\nwith logwriter.mode(&#34;train&#34;) as writer:\n   param1_histgram = writer.histogram(&#34;param1&#34;, 100)\n\ndef train(use_cuda, learning_rate, num_passes, BATCH_SIZE=128):\n   class_dim = 2\n   image_shape = [3, 48, 48]\n   image = fluid.layers.data(name=&#39;image&#39;, shape=image_shape, dtype=&#39;float32&#39;)\n   label = fluid.layers.data(name=&#39;label&#39;, shape=[1], dtype=&#39;int64&#39;)\n\n   net, conv1 = simplenet(image)\n   # 获取全连接输出\n   predict = fluid.layers.fc(\n       input=net,\n       size=class_dim,\n       act=&#39;softmax&#39;,\n       param_attr=ParamAttr(name=&#34;param1&#34;, initializer=NormalInitializer()))\n\n   # 获取损失\n   cost = fluid.layers.cross_entropy(input=predict, label=label)\n   avg_cost = fluid.layers.mean(x=cost)\n\n   # 计算batch，从而来求平均的准确率\n   batch_size = fluid.layers.create_tensor(dtype=&#39;int64&#39;)\n   print &#34;batchsize=&#34;,batch_size\n   batch_acc = fluid.layers.accuracy(input=predict, label=label, total=batch_size)\n\n   # 定义优化方法\n   optimizer = fluid.optimizer.Momentum(\n       learning_rate=learning_rate,\n       momentum=0.9,\n       regularization=fluid.regularizer.L2Decay(5 * 1e-5))\n\n   opts = optimizer.minimize(avg_cost)\n\n   # 是否使用GPU\n   place = fluid.CUDAPlace(0) if use_cuda else fluid.CPUPlace()\n   # 创建调试器\n   exe = fluid.Executor(place)\n   # 初始化调试器\n   exe.run(fluid.default_startup_program())\n   # 保存结果\n   model_save_dir = &#34;./models&#34;\n\n   # 获取训练数据\n   resizesize = 60\n   cropsize = 48\n   mydata = Dataset(cropsize=cropsize,resizesize=resizesize)\n   mydatareader = mydata.train_reader(train_list=&#39;./all_shuffle_train.txt&#39;)\n   train_reader = paddle.batch(reader=paddle.reader.shuffle(reader=mydatareader,buf_size=50000),batch_size=128)\n \n   # 指定数据和label的对应关系\n   feeder = fluid.DataFeeder(place=place, feed_list=[image, label])\n\n   step = 0\n   sample_num = 0\n   start_up_program = framework.default_startup_program()\n   param1_var = start_up_program.global_block().var(&#34;param1&#34;)\n\n   accuracy = fluid.average.WeightedAverage()\n   # 开始训练，使用循环的方式来指定训多少个Pass\n   for pass_id in range(num_passes):\n       # 从训练数据中按照一个个batch来读取数据\n       accuracy.reset()\n       for batch_id, data in enumerate(train_reader()):\n           loss, conv1_out, param1, acc, weight = exe.run(fluid.default_main_program(),\n                                                          feed=feeder.feed(data),\n                                                          fetch_list=[avg_cost, conv1, param1_var, batch_acc,batch_size])\n           accuracy.add(value=acc, weight=weight)\n           pass_acc = accuracy.eval()\n\n           # 重新启动图形化展示组件\n           if sample_num == 0:\n               input_image.start_sampling()\n               conv_image.start_sampling()\n           # 获取taken\n           idx1 = input_image.is_sample_taken()\n           idx2 = conv_image.is_sample_taken()\n           # 保证它们的taken是一样的\n           assert idx1 == idx2\n           idx = idx1\n           if idx != -1:\n               # 加载输入图像的数据数据\n               image_data = data[0][0]\n               input_image_data = np.transpose(\n                   image_data.reshape(image_shape), axes=[1, 2, 0])\n               input_image.set_sample(idx, input_image_data.shape,\n                                      input_image_data.flatten())\n               # 加载卷积数据\n               conv_image_data = conv1_out[0][0]\n               conv_image.set_sample(idx, conv_image_data.shape,\n                                     conv_image_data.flatten())\n               # 完成输出一次\n               sample_num += 1\n               if sample_num % num_samples == 0:\n                   input_image.finish_sampling()\n                   conv_image.finish_sampling()\n                   sample_num = 0\n\n           # 加载趋势图的数据\n           loss_scalar.add_record(step, loss)\n           acc_scalar.add_record(step, acc)\n           # 添加模型结构数据\n           param1_histgram.add_record(step, param1.flatten())\n \n           # 输出训练日志\n           print(&#34;loss:&#34; + str(loss) + &#34; acc:&#34; + str(acc) + &#34; pass_acc:&#34; + str(pass_acc))\n           step += 1\n           model_path = os.path.join(model_save_dir,str(pass_id))\n           if not os.path.exists(model_save_dir):\n               os.mkdir(model_save_dir)\n           fluid.io.save_inference_model(model_path,[&#39;image&#39;],[predict],exe)\n\n\nif __name__ == &#39;__main__&#39;:\n   # 开始训练\n   train(use_cuda=False, learning_rate=0.005, num_passes=300)</code></pre></div><p><b>2.4 训练结果</b></p><p>看看acc和loss的曲线，可见已经收敛</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ac03e9b162e1ac2275c730d7f372e098_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"661\" data-rawheight=\"295\" class=\"origin_image zh-lightbox-thumb\" width=\"661\" data-original=\"https://pic1.zhimg.com/v2-ac03e9b162e1ac2275c730d7f372e098_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;661&#39; height=&#39;295&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"661\" data-rawheight=\"295\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"661\" data-original=\"https://pic1.zhimg.com/v2-ac03e9b162e1ac2275c730d7f372e098_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-ac03e9b162e1ac2275c730d7f372e098_b.jpg\"/></figure><h2>03 paddlepaddle测试</h2><div class=\"highlight\"><pre><code class=\"language-text\">训练的时候使用了fluid，测试的时候也需要定义调试器，加载训练好的模型，完整的代码如下\n# encoding:utf-8\nimport sys\nimport numpy as np\nimport paddle.v2 as paddle\nfrom PIL import Image\nimport os\nimport cv2\n# coding=utf-8\nimport numpy as np\nimport paddle.fluid as fluid\nimport paddle.fluid.framework as framework\nimport paddle.v2 as paddle\nfrom paddle.fluid.initializer import NormalInitializer\nfrom paddle.fluid.param_attr import ParamAttr\nfrom visualdl import LogWriter\nfrom net_fluid import simplenet\n\nif __name__ == &#34;__main__&#34;:\n   # 开始预测\n   type_size = 2\n   testsize = 48\n\n   imagedir = sys.argv[1]\n   images = os.listdir(imagedir)\n \n   # 定义调试器\n   save_dirname = &#34;./models/299&#34;\n   exe = fluid.Executor(fluid.CPUPlace())\n   inference_scope = fluid.core.Scope()\n   with fluid.scope_guard(inference_scope):\n   # 加载模型\n [inference_program,feed_target_names,fetch_targets] = fluid.io.load_inference_model(save_dirname,exe)\n\n       predicts = np.zeros((type_size,1))\n       for image in images:\n           imagepath = os.path.join(imagedir,image)\n           img = paddle.image.load_image(imagepath)\n           img = paddle.image.simple_transform(img,testsize,testsize,False)\n           img = img[np.newaxis,:]\n\n           #print img.shape\n\n           results = np.argsort(-exe.run(inference_program,feed={feed_target_names[0]:img},\n                   fetch_list=fetch_targets)[0])\n           label = results[0][0]\n           predicts[label] += 1\n \n   print predicts</code></pre></div><p>由于所有框架的测试流程都差不多，所以就不对每一部分进行解释了，大家可以自行去看代码。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-60feffab1842911ce09ae93eeaae724f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"3999\" data-rawheight=\"2250\" class=\"origin_image zh-lightbox-thumb\" width=\"3999\" data-original=\"https://pic4.zhimg.com/v2-60feffab1842911ce09ae93eeaae724f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;3999&#39; height=&#39;2250&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"3999\" data-rawheight=\"2250\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"3999\" data-original=\"https://pic4.zhimg.com/v2-60feffab1842911ce09ae93eeaae724f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-60feffab1842911ce09ae93eeaae724f_b.jpg\"/></figure><blockquote>本系列完整文章：</blockquote><p>第一篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029846%26idx%3D1%26sn%3D0c343cfd0ede5c8ae1405bd6348aefad%26chksm%3D871342abb064cbbd7fe31fb3c55f23875f27e48fb8354e9855823b1701f1227c71b4eb00de50%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【caffe速成】caffe图像分类从模型自定义到测试</a></p><p>第二篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029846%26idx%3D2%26sn%3D7c2582243bcd8f8b491e8e466a21978f%26chksm%3D871342abb064cbbd0cba24b408ceda2b64a7c8b6baa07f9f8f56cd4d1233caa0b80fe357753e%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【tensorflow速成】Tensorflow图像分类从模型自定义到测试</a></p><p>第三篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029881%26idx%3D1%26sn%3D3c869fcee3b48d3582952ab9a0683ea6%26chksm%3D87134284b064cb924c5e7231b3f2c36ba27e3a689b067f569f2e086f62b18413bcebc5987a07%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【pytorch速成】Pytorch图像分类从模型自定义到测试</a></p><p>第四篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029887%26idx%3D1%26sn%3D645b97809c24922352a0b39f19c9ef0c%26chksm%3D87134282b064cb9441af68124d205d9c7dedcaeb09788f4d586b949584e556eddd3a72217f69%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【paddlepaddle速成】paddlepaddle图像分类从模型自定义到测试</a></p><p>第五篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029896%26idx%3D1%26sn%3Df3f7b9cf69c514f45d1d14205f879270%26chksm%3D87134375b064ca6323354c40f3e55b02ff0d1d24f3dacfc980190d51f20ec9ec16e60c1a4741%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【Keras速成】Keras图像分类从模型自定义到测试</a></p><p>第六篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029904%26idx%3D1%26sn%3D0bdc6947f5ac68e7f68426b9d076b4ab%26chksm%3D8713436db064ca7b3b2a2a1d6a8d24c15069f72655e2c39e2498051fa0e56bbcf6fe9c332d5b%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【mxnet速成】mxnet图像分类从模型自定义到测试</a></p><p>第七篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649030976%26idx%3D1%26sn%3D0befc170a93d365b780c5f05b2f510a4%26chksm%3D8712bf3db065362b0aeaae82bdbac467be5697b34cac2797e2ee15cdcb97474c08640482bf07%26token%3D1695328272%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【cntk速成】cntk图像分类从模型自定义到测试</a></p><p>第八篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26tempkey%3DMTAwMF93Q3VDbklva2NBZDJXV1dnNWloZjJ2YWtZWmJ2aTRRaGVDMVNfbERSN2xVYXdXd3B4eWczWWdMbnQzRjZrRVpmSGRYMndrLTQ0VnhPcURTNGhsS3dYWGd3MTk0UGtmZkltU0U2U01OUmRfQzcySDVObDlxM3R2U3Q0SlNQWGRMc1NZeWtWTlVRN1NpRlJaWXRYcGdydUNOTUR6clUzVkNleVlKUE5nfn4%253D%26chksm%3D0712ba9b3065338de13ea030951ba2c25c0363d958c905e12c937c2acffa991313abb4d9f751%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【chainer速成】chainer图像分类从模型自定义到测试</a></p><p>第九篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649032012%26idx%3D1%26sn%3Df74c7084621f367adb2518ebec61ca42%26chksm%3D8712bb31b06532270b9ca9550ab48ff78adaad7d38c73645cfb8888218b8e177bebd5d401aa9%26token%3D1258619827%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【DL4J速成】Deeplearning4j图像分类从模型自定义到测试</a></p><p>第十篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649032109%26idx%3D2%26sn%3Da6ff48ec0ae5d8e7a494df7e564d9ac9%26chksm%3D8712bbd0b06532c61d98c786ba1773c29c3dcf1291f9c3cd052c5643e24699eef28481e94b8e%26token%3D1258619827%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【MatConvnet速成】MatConvnet图像分类从模型自定义到测试</a></p><p>第十一篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649032128%26idx%3D2%26sn%3Df889e2255c0ec4960f76ad0383363849%26chksm%3D8712bbbdb06532ab136c0a485bebc5cf181f24a34bec7bab3f97f66be627e09d939b997dae90%26token%3D598159941%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【Lasagne速成】Lasagne/Theano图像分类从模型自定义到测试</a></p><p>第十二篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649032137%26idx%3D2%26sn%3Da9770ead4a9d03f0a4e75be86657453b%26chksm%3D8712bbb4b06532a27f0f02ac30ce4a18eeac1f72ac54b3c5c1b6ec13fee29f03bfc0dcaa1efd%26token%3D1446627305%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【darknet速成】Darknet图像分类从模型自定义到测试</a></p>", 
            "topic": [
                {
                    "tag": "计算机视觉", 
                    "tagLink": "https://api.zhihu.com/topics/19590195"
                }, 
                {
                    "tag": "开源", 
                    "tagLink": "https://api.zhihu.com/topics/19562746"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/43330185", 
            "userName": "言有三-龙鹏", 
            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
            "upvote": 11, 
            "title": "【学习必备】Awesomes的github项目大全！", 
            "content": "<p>作者 | 言有三</p><p>编辑 | 言有三</p><p>随着开源已经成为技术人员的习惯和基本素质，小白们也能够利用好这些资源快速入手体验项目，那么，深度学习方向有哪些优质的资源呢！</p><blockquote>01 awesome-python</blockquote><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-f802748dbaf15a342c26feba90440d91_b.jpg\" data-rawwidth=\"1080\" data-rawheight=\"53\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-f802748dbaf15a342c26feba90440d91_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;53&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1080\" data-rawheight=\"53\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-f802748dbaf15a342c26feba90440d91_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-f802748dbaf15a342c26feba90440d91_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-bb30f3c71bded3fca0a3389f75f64858_b.jpg\" data-rawwidth=\"1080\" data-rawheight=\"414\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-bb30f3c71bded3fca0a3389f75f64858_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;414&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1080\" data-rawheight=\"414\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-bb30f3c71bded3fca0a3389f75f64858_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-bb30f3c71bded3fca0a3389f75f64858_b.jpg\"/></figure><a data-draft-node=\"block\" data-draft-type=\"link-card\" href=\"https://link.zhihu.com/?target=https%3A//github.com/vinta/awesome-python\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/vinta/awesom</span><span class=\"invisible\">e-python</span><span class=\"ellipsis\"></span></a><p>这些都是精选的使用Python框架的开源库，5万多颗星，我们看看deep learning这个菜单下的资源就能窥见一斑。</p><blockquote>02 awesome-deep-learning</blockquote><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-4a9642680656701e28f537343a182ecf_b.jpg\" data-rawwidth=\"1080\" data-rawheight=\"59\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-4a9642680656701e28f537343a182ecf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;59&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1080\" data-rawheight=\"59\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-4a9642680656701e28f537343a182ecf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-4a9642680656701e28f537343a182ecf_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ac54ca79c68cc907cdb949e402fcf573_b.jpg\" data-rawwidth=\"1080\" data-rawheight=\"710\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-ac54ca79c68cc907cdb949e402fcf573_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;710&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1080\" data-rawheight=\"710\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-ac54ca79c68cc907cdb949e402fcf573_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-ac54ca79c68cc907cdb949e402fcf573_b.jpg\"/></figure><a data-draft-node=\"block\" data-draft-type=\"link-card\" href=\"https://link.zhihu.com/?target=https%3A//github.com/ChristosChristofidis/awesome-deep-learning\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/ChristosChri</span><span class=\"invisible\">stofidis/awesome-deep-learning</span><span class=\"ellipsis\"></span></a><p>与深度学习有关的优秀教程，项目，框架，社区，这还需要多说什么吗？快一万颗星了，入手吧。</p><blockquote>03 awesome-courses</blockquote><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-76e077af2e2efcd91638c6db3479576a_b.jpg\" data-rawwidth=\"1080\" data-rawheight=\"741\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-76e077af2e2efcd91638c6db3479576a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;741&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1080\" data-rawheight=\"741\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-76e077af2e2efcd91638c6db3479576a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-76e077af2e2efcd91638c6db3479576a_b.jpg\"/></figure><a data-draft-node=\"block\" data-draft-type=\"link-card\" href=\"https://link.zhihu.com/?target=https%3A//github.com/prakhar1989/awesome-courses\" data-image=\"https://pic2.zhimg.com/v2-586d5d47aa10da93b38b36db9781b871_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">prakhar1989/awesome-courses</a><p>与计算机科学有关的精品课程大全集，难道你要错过？</p><blockquote>04 awesome-machine-learning</blockquote><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ab20f67893b364b97793ae7b8dcbf9c4_b.jpg\" data-rawwidth=\"1080\" data-rawheight=\"58\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-ab20f67893b364b97793ae7b8dcbf9c4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;58&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1080\" data-rawheight=\"58\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-ab20f67893b364b97793ae7b8dcbf9c4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-ab20f67893b364b97793ae7b8dcbf9c4_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-9c888434cc7789e339f801afc4409772_b.jpg\" data-rawwidth=\"1080\" data-rawheight=\"569\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-9c888434cc7789e339f801afc4409772_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;569&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1080\" data-rawheight=\"569\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-9c888434cc7789e339f801afc4409772_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-9c888434cc7789e339f801afc4409772_b.jpg\"/></figure><a data-draft-node=\"block\" data-draft-type=\"link-card\" href=\"https://link.zhihu.com/?target=https%3A//github.com/josephmisiti/awesome-machine-learning\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/josephmisiti</span><span class=\"invisible\">/awesome-machine-learning</span><span class=\"ellipsis\"></span></a><p>，与2有点类似，不过更加专注，都是与机器学习有关的开源框架，值得关注！</p><blockquote>05 awesome-datascience</blockquote><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-5d1dee775aed84d49f0d8fb6565d016c_b.jpg\" data-rawwidth=\"1080\" data-rawheight=\"60\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-5d1dee775aed84d49f0d8fb6565d016c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;60&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1080\" data-rawheight=\"60\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-5d1dee775aed84d49f0d8fb6565d016c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-5d1dee775aed84d49f0d8fb6565d016c_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-665ad7eac8497ebd6ff27606a8515727_b.jpg\" data-rawwidth=\"1080\" data-rawheight=\"817\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-665ad7eac8497ebd6ff27606a8515727_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;817&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1080\" data-rawheight=\"817\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-665ad7eac8497ebd6ff27606a8515727_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-665ad7eac8497ebd6ff27606a8515727_b.jpg\"/></figure><a data-draft-node=\"block\" data-draft-type=\"link-card\" href=\"https://link.zhihu.com/?target=https%3A//github.com/bulutyazilim/awesome-datascience\" data-image=\"https://pic3.zhimg.com/v2-375d6ce4bce9a2a42762fccda8e16bf2_ipico.jpg\" data-image-width=\"240\" data-image-height=\"240\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">bulutyazilim/awesome-datascience</a><p>要想成为数据科学家的手中必备，从数据集到博客到大公司的资源，就服它。</p><blockquote>06 awesome-deep-learning-papers</blockquote><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ada350b87b9cd4fe73fea436a3a7397b_b.jpg\" data-rawwidth=\"1080\" data-rawheight=\"52\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-ada350b87b9cd4fe73fea436a3a7397b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;52&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1080\" data-rawheight=\"52\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-ada350b87b9cd4fe73fea436a3a7397b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-ada350b87b9cd4fe73fea436a3a7397b_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-45b227d3fa5477f73ea2c0ceb658ff79_b.jpg\" data-rawwidth=\"1080\" data-rawheight=\"495\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-45b227d3fa5477f73ea2c0ceb658ff79_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;495&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1080\" data-rawheight=\"495\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-45b227d3fa5477f73ea2c0ceb658ff79_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-45b227d3fa5477f73ea2c0ceb658ff79_b.jpg\"/></figure><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/terryum/awesome-deep-learning-papers\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/terryum/awes</span><span class=\"invisible\">ome-deep-learning-papers</span><span class=\"ellipsis\"></span></a>，这个相必大家不陌生了，整理深度学习有关的优秀papers，从出来那一天就让人兴奋。</p><blockquote>07 awesome-computer-vision</blockquote><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-991089dec91570bc08f1fc11a8fba343_b.jpg\" data-rawwidth=\"1080\" data-rawheight=\"485\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-991089dec91570bc08f1fc11a8fba343_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;485&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1080\" data-rawheight=\"485\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-991089dec91570bc08f1fc11a8fba343_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-991089dec91570bc08f1fc11a8fba343_b.jpg\"/></figure><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/jbhuang0604/awesome-computer-vision\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/jbhuang0604/</span><span class=\"invisible\">awesome-computer-vision</span><span class=\"ellipsis\"></span></a></p><p>与计算机视觉有关的东西，真的是什么都有！另外再附上与NLP有关的项目，<a href=\"https://link.zhihu.com/?target=https%3A//github.com/keon/awesome-nlp\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/keon/awesome</span><span class=\"invisible\">-nlp</span><span class=\"ellipsis\"></span></a>，以及另一个类似的项目，<a href=\"https://link.zhihu.com/?target=https%3A//github.com/kjw0612/awesome-deep-vision\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/kjw0612/awes</span><span class=\"invisible\">ome-deep-vision</span><span class=\"ellipsis\"></span></a></p><blockquote>08 awesome-ios-android</blockquote><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-8be283f4b36fc7ee7dfd34c5241a6c2a_b.jpg\" data-rawwidth=\"1080\" data-rawheight=\"558\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-8be283f4b36fc7ee7dfd34c5241a6c2a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;558&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1080\" data-rawheight=\"558\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-8be283f4b36fc7ee7dfd34c5241a6c2a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-8be283f4b36fc7ee7dfd34c5241a6c2a_b.jpg\"/></figure><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/vsouza/awesome-ios\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/vsouza/aweso</span><span class=\"invisible\">me-ios</span><span class=\"ellipsis\"></span></a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/snowdream/awesome-android\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/snowdream/aw</span><span class=\"invisible\">esome-android</span><span class=\"ellipsis\"></span></a></p><p>and so son，ios与android开发必备，你懂的</p><blockquote>09 各个计算机视觉方向资源</blockquote><p>现在来看看那些与各个方向有关的资源！</p><p class=\"ztext-empty-paragraph\"><br/></p><p>想搞gan吗？？</p><a data-draft-node=\"block\" data-draft-type=\"link-card\" href=\"https://link.zhihu.com/?target=https%3A//github.com/nightrome/really-awesome-gan\" data-image=\"https://pic3.zhimg.com/v2-f9f47f34e7f452c23a8b2b0227ea42f2_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">nightrome/really-awesome-gan</a><a data-draft-node=\"block\" data-draft-type=\"link-card\" href=\"https://link.zhihu.com/?target=https%3A//github.com/hindupuravinash/the-gan-zoo\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/hindupuravin</span><span class=\"invisible\">ash/the-gan-zoo</span><span class=\"ellipsis\"></span></a><p class=\"ztext-empty-paragraph\"><br/></p><p>想搞人脸吗？</p><a data-draft-node=\"block\" data-draft-type=\"link-card\" href=\"https://link.zhihu.com/?target=https%3A//github.com/polarisZhao/awesome-face\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/polarisZhao/</span><span class=\"invisible\">awesome-face</span><span class=\"ellipsis\"></span></a><p class=\"ztext-empty-paragraph\"><br/></p><p>想做语义分割吗？</p><a data-draft-node=\"block\" data-draft-type=\"link-card\" href=\"https://link.zhihu.com/?target=https%3A//github.com/mrgloom/awesome-semantic-segmentation\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/mrgloom/awes</span><span class=\"invisible\">ome-semantic-segmentation</span><span class=\"ellipsis\"></span></a><p class=\"ztext-empty-paragraph\"><br/></p><p>想搞跟踪吗？</p><a data-draft-node=\"block\" data-draft-type=\"link-card\" href=\"https://link.zhihu.com/?target=https%3A//github.com/foolwood/benchmark_results\" data-image=\"https://pic1.zhimg.com/v2-025e21c6abadd24c05fb806368d7b668_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">foolwood/benchmark_results</a><p class=\"ztext-empty-paragraph\"><br/></p><p>想搞三维重建吗？</p><a data-draft-node=\"block\" data-draft-type=\"link-card\" href=\"https://link.zhihu.com/?target=https%3A//github.com/openMVG/awesome_3DReconstruction_list\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/openMVG/awes</span><span class=\"invisible\">ome_3DReconstruction_list</span><span class=\"ellipsis\"></span></a><p class=\"ztext-empty-paragraph\"><br/></p><p>想搞风格化？</p><a data-draft-node=\"block\" data-draft-type=\"link-card\" href=\"https://link.zhihu.com/?target=https%3A//github.com/Yanyousan/Neural-Style-Transfer-Papers\" data-image=\"https://pic4.zhimg.com/v2-889093f50d8163592ae87f14ffa5f123_ipico.jpg\" data-image-width=\"420\" data-image-height=\"420\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Yanyousan/Neural-Style-Transfer-Papers</a><p class=\"ztext-empty-paragraph\"><br/></p><p>想搞目标检测？</p><a data-draft-node=\"block\" data-draft-type=\"link-card\" href=\"https://link.zhihu.com/?target=https%3A//github.com/amusi/awesome-object-detection\" data-image=\"https://pic4.zhimg.com/v2-20f5b045f67b44583dded742da0ebd1b_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">amusi/awesome-object-detection</a><p class=\"ztext-empty-paragraph\"><br/></p><p>还是？？</p><blockquote>10 sindresorhus/awesome</blockquote><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-be6dfa8f9786fa43a2e12a464d939a30_b.jpg\" data-rawwidth=\"1080\" data-rawheight=\"66\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-be6dfa8f9786fa43a2e12a464d939a30_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;66&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1080\" data-rawheight=\"66\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-be6dfa8f9786fa43a2e12a464d939a30_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-be6dfa8f9786fa43a2e12a464d939a30_b.jpg\"/></figure><p>最后这个是真绝了，送上一个awesome大礼包，这位仁兄真的是有心了！十万颗星星快到手了！</p><a data-draft-node=\"block\" data-draft-type=\"link-card\" href=\"https://link.zhihu.com/?target=https%3A//github.com/sindresorhus/awesome%23databases\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/sindresorhus</span><span class=\"invisible\">/awesome#databases</span><span class=\"ellipsis\"></span></a><p><b>程序员做事，真是滴水不漏！</b></p><p><b>老铁们，看着还缺点什么，去补一刀吧！</b></p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>配合我们上期文章阅读效果更佳噢。</b></p><a data-draft-node=\"block\" data-draft-type=\"link-card\" href=\"https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029676%26idx%3D1%26sn%3D4cfda77b8d656380099dab0af77b2a5a%26chksm%3D87134251b064cb47b1ebf4be564158c2e72f8c3dd9d2bc0ace198ca8a32ee72b3edf5006e3ad%26scene%3D21%23wechat_redirect\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【求职必备】学深度学习，这些公众号你有关注吗？</a><p><b>为初学者准备的快速入门计算机视觉公开课《AI 图像识别项目从入门到上线》上线了，将讲述从零基础到完成一个实际的项目到微信小程序上线的整个流程，欢迎交流捧场。</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-987ed000905ce02dca2a36700f2a2013_b.jpg\" data-rawwidth=\"900\" data-rawheight=\"3861\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"900\" data-original=\"https://pic4.zhimg.com/v2-987ed000905ce02dca2a36700f2a2013_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;900&#39; height=&#39;3861&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"900\" data-rawheight=\"3861\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"900\" data-original=\"https://pic4.zhimg.com/v2-987ed000905ce02dca2a36700f2a2013_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-987ed000905ce02dca2a36700f2a2013_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "GitHub", 
                    "tagLink": "https://api.zhihu.com/topics/19566035"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "求职", 
                    "tagLink": "https://api.zhihu.com/topics/19551771"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/34455109", 
            "userName": "言有三-龙鹏", 
            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
            "upvote": 35, 
            "title": "【技术综述】如何Finetune一个小网络到移动端(时空性能分析篇)", 
            "content": "<h2><b>0 引言</b></h2><p>现在很多的图像算法都是离线计算的，而学术界刷榜单那些模型，什么vgg16，resnet152是不能直接拿来用的，所以，对于一个深度学习算法工程师来说，如果在这些模型的基础上，设计出一个又小又快的满足业务需求的模型，是必备技能，今天就来简单讨论一下这个问题。</p><p>首先，祭出一个baseline，来自Google的mobilenet，算是学术界祭出的真正有意义的移动端模型。</p><p>当然，这里我们要稍微修改一下，毕竟原始的mobilenet是分类模型过于简单无法展开更多，我们以更加复杂通用的一个任务开始，分割，同时修改一下初始输入尺度，毕竟224这个尺度在移动端不一定被采用，我们以更小的一个尺度开始，以MacBookPro为计算平台。</p><p>在原有mobilenet的基础上添加反卷积，输入网络尺度160*160，网络结构参考mobilenet，只是在最后加上反卷积如下</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-66cd74ab71aa0f1c02956570dadb3173_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"304\" data-rawheight=\"936\" class=\"content_image\" width=\"304\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;304&#39; height=&#39;936&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"304\" data-rawheight=\"936\" class=\"content_image lazy\" width=\"304\" data-actualsrc=\"https://pic4.zhimg.com/v2-66cd74ab71aa0f1c02956570dadb3173_b.jpg\"/></figure><p>如果谁有可以可视化caffe网络结构图并保存成高清图片的方法，请告诉我一下，netscope不能保存图，graphviz的图又效果很差，所以这里没有放完整结构图。</p><p>不过，大家可以去参考mobilenet，然后我们在mac上跑一遍，看看时间代价如下：</p><p>其中黄色高亮是统计的每一个module的时间和。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-be3d5894459071fe9c4473ab84734bca_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"718\" data-rawheight=\"2908\" class=\"origin_image zh-lightbox-thumb\" width=\"718\" data-original=\"https://pic3.zhimg.com/v2-be3d5894459071fe9c4473ab84734bca_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;718&#39; height=&#39;2908&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"718\" data-rawheight=\"2908\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"718\" data-original=\"https://pic3.zhimg.com/v2-be3d5894459071fe9c4473ab84734bca_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-be3d5894459071fe9c4473ab84734bca_b.jpg\"/></figure><p>准备工作完毕，接下来开始干活。</p><h2><b>1 分析网络的性能瓶颈</b></h2><blockquote>1.1 运行时间和计算代价分析</blockquote><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-5482c3b56cd3b0d8a927f03119520e56_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"460\" data-rawheight=\"719\" class=\"origin_image zh-lightbox-thumb\" width=\"460\" data-original=\"https://pic3.zhimg.com/v2-5482c3b56cd3b0d8a927f03119520e56_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;460&#39; height=&#39;719&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"460\" data-rawheight=\"719\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"460\" data-original=\"https://pic3.zhimg.com/v2-5482c3b56cd3b0d8a927f03119520e56_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-5482c3b56cd3b0d8a927f03119520e56_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-23185e46236e57cc50a72dfe2ddf5113_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"535\" data-rawheight=\"716\" class=\"origin_image zh-lightbox-thumb\" width=\"535\" data-original=\"https://pic4.zhimg.com/v2-23185e46236e57cc50a72dfe2ddf5113_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;535&#39; height=&#39;716&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"535\" data-rawheight=\"716\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"535\" data-original=\"https://pic4.zhimg.com/v2-23185e46236e57cc50a72dfe2ddf5113_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-23185e46236e57cc50a72dfe2ddf5113_b.jpg\"/></figure><p>上面两图分别是网络的计算时间和计算量，从上面我们总结几条规律：</p><p>(1) 耗时前5，conv2_1_sep，conv6_sep，conv3_1_sep，conv3_1_dw，conv2_1_dw。</p><p>我们看看为什么，</p><p>conv2_1_dw计算量，32*80*80*3*3*1=1843200</p><p>conv2_1_sep计算量，32*80*80*1*1*64=13107200</p><p>conv3_1_dw计算量，128*40*40*3*3*1=1843200</p><p>conv3_1_sep计算量，128*40*40*1*1*128=26214400</p><p>conv6_sep计算量，1024*5*5*1*1*1024=26214400</p><p>上面可以看出，计算量最大的是conv6_sep，conv2_1_sep，理论上conv2_1_dw计算量与conv2_1_sep不在一个量级，但是实际上相当，这是库实现的问题。</p><p>(2) 从conv5_1到conv5_5，由于尺度不发生变化，通道数不发生变化，所以耗时都是接近的，且dw模块/sep模块耗时比例约为1:3。</p><p>前者计算量：512*10*10*3*3</p><p>后者计算量：512*10*10*1*1*512</p><p>这一段网络结构是利用网络深度增加了非线性，所以对于复杂程度不同的问题，我们可以缩减这一段的深度。</p><blockquote>1.2 网络参数量分析</blockquote><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-ddb1e554d35964cdedf608ac5638b26e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"504\" data-rawheight=\"683\" class=\"origin_image zh-lightbox-thumb\" width=\"504\" data-original=\"https://pic3.zhimg.com/v2-ddb1e554d35964cdedf608ac5638b26e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;504&#39; height=&#39;683&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"504\" data-rawheight=\"683\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"504\" data-original=\"https://pic3.zhimg.com/v2-ddb1e554d35964cdedf608ac5638b26e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-ddb1e554d35964cdedf608ac5638b26e_b.jpg\"/></figure><p>从上面我们可以看出，参数量集中在conv6_sep，conv5_6_sep，conv5_1~5_5，所以要压缩模型，应该从这里地方入手。</p><p>当我们想设计更小的mobilenet网络时，有3招是基本的，一定要用。</p><p>(1) 降低输入分辨率，根据实际问题来设定。</p><p>(2) 调整网络宽度，也就是channel数量。</p><p>(3) 调整网络深度，比如从conv4_2到conv5_6这一段，都可以先去试一试。</p><h2><b>2 开始调整网络</b></h2><p>在做这件事之前，我们先看看经典网络结构的一些东西，更具体可以参考之前的文章。</p><a href=\"https://zhuanlan.zhihu.com/p/25797790\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic2.zhimg.com/v2-250016bc1e7e898c11192cb24786f719_180x120.jpg\" data-image-width=\"5473\" data-image-height=\"3868\" class=\"internal\">龙鹏：【技术综述】为了压榨CNN模型，这几年大家都干了什么</a><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-d21494f9b590caff586f6567620d98f9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1729\" data-rawheight=\"376\" class=\"origin_image zh-lightbox-thumb\" width=\"1729\" data-original=\"https://pic2.zhimg.com/v2-d21494f9b590caff586f6567620d98f9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1729&#39; height=&#39;376&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1729\" data-rawheight=\"376\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1729\" data-original=\"https://pic2.zhimg.com/v2-d21494f9b590caff586f6567620d98f9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-d21494f9b590caff586f6567620d98f9_b.jpg\"/></figure><p>从上面的表看，主流网络第一个卷积，kernel=3，stride=2，featuremap=64，mobilenet系列已经降到了32。</p><p>第1层是提取边缘等信息的，当然是featuremap数量越大越好，但是其实边缘检测方向是有限的，很多信息是冗余的， 由于mobilenet优异的性能，事实证明，最底层的卷积featuremap channel=32已经够用。</p><p>实际的任务中，大家可以看conv1占据的时间来调整，不过大部分情况下只需要选择好输入尺度大小做训练，然后套用上面的参数即可，毕竟这一层占据的时间和参数，都不算多，32已经足够好足够优异，不太需要去调整的。</p><p>自从任意的卷积可以采用3*3替代且计算量更小后，网络结构中现在只剩下3*3和1*1的卷积，其他的尺寸可以先不考虑。</p><p>采用80*80输入，砍掉conv5_6和conv6，得到的模型各层花费时间如下</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-e468e7f3159c88653adcf7a521af5557_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"648\" data-rawheight=\"2908\" class=\"origin_image zh-lightbox-thumb\" width=\"648\" data-original=\"https://pic4.zhimg.com/v2-e468e7f3159c88653adcf7a521af5557_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;648&#39; height=&#39;2908&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"648\" data-rawheight=\"2908\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"648\" data-original=\"https://pic4.zhimg.com/v2-e468e7f3159c88653adcf7a521af5557_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-e468e7f3159c88653adcf7a521af5557_b.jpg\"/></figure><p>总共274ms，我们称这个模型为<b>mobilenet_v0</b>。</p><blockquote><b>2.1 如何决定输入尺度</b></blockquote><p>输入尺度绝对是任务驱动的，不同的任务需要不同的输入尺度，分割比分类需要尺度一般更大，检测又比分割所需要的尺度更大，在这里，我们限定一个比较简单的分割任务，然后将输入尺度定为80*80，就将该任务称为A吧。</p><blockquote><b>2.2 如何调整网络宽度与深度</b></blockquote><p>通道数决定网络的宽度，对时间和网络大小的贡献是一个乘因子，这是优化模型首先要做的，下面开始做。</p><div class=\"highlight\"><pre><code class=\"language-text\">2.2.1 反卷积</code></pre></div><p>看上面的模型我们可以看出，反卷积所占用时间远远大于前面提取特征的卷积，这是因为我们没有去优化过这个参数。那么，到底选择多少才合适呢？</p><p>在这里经验就比较有用了。卷积提取特征的过程，是featuremap尺度变小，channel变大，反卷积正好相反，featuremap不断变大，通道数不断变小。这里有4次放大2倍的卷积，考虑到每次缩放一倍，所以第一次的channel数量不能小于2^4=16，一不做二不休，我们干脆就干为16。</p><p>我们称这个模型为mobilenet_v1</p><p>我们看下时间对比</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-b98dc35e80cc49cd2d96b8df0d2bef00_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1729\" data-rawheight=\"273\" class=\"origin_image zh-lightbox-thumb\" width=\"1729\" data-original=\"https://pic1.zhimg.com/v2-b98dc35e80cc49cd2d96b8df0d2bef00_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1729&#39; height=&#39;273&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1729\" data-rawheight=\"273\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1729\" data-original=\"https://pic1.zhimg.com/v2-b98dc35e80cc49cd2d96b8df0d2bef00_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-b98dc35e80cc49cd2d96b8df0d2bef00_b.jpg\"/></figure><p>再看下性能对比。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-7bbdbf9bf0991bd0b4fd1b7ca967f490_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1617\" data-rawheight=\"204\" class=\"origin_image zh-lightbox-thumb\" width=\"1617\" data-original=\"https://pic1.zhimg.com/v2-7bbdbf9bf0991bd0b4fd1b7ca967f490_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1617&#39; height=&#39;204&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1617\" data-rawheight=\"204\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1617\" data-original=\"https://pic1.zhimg.com/v2-7bbdbf9bf0991bd0b4fd1b7ca967f490_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-7bbdbf9bf0991bd0b4fd1b7ca967f490_b.jpg\"/></figure><p>这样，一举将模型压缩5倍，时间压缩5倍，而且现在反卷积的时间代价几乎已经可以忽略。</p><div class=\"highlight\"><pre><code class=\"language-text\">2.2.2 粗暴地减少网络宽度</code></pre></div><p>接下来我们再返回第1部分，conv5_1到conv5_5的计算量和时间代价都是不小的，且这一部分featuremap大小不再发生变化。这意味着什么？这意味着这一部分，纯粹是为了增加网络的非线性性。</p><p>下面我们直接将conv5_1到conv5_5的featuremap从512全部干到256，称其为mobilenet2.1.1，再看精度和时间代价。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-0982789dc5be9462540d469aeac8c773_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1617\" data-rawheight=\"204\" class=\"origin_image zh-lightbox-thumb\" width=\"1617\" data-original=\"https://pic4.zhimg.com/v2-0982789dc5be9462540d469aeac8c773_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1617&#39; height=&#39;204&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1617\" data-rawheight=\"204\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1617\" data-original=\"https://pic4.zhimg.com/v2-0982789dc5be9462540d469aeac8c773_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-0982789dc5be9462540d469aeac8c773_b.jpg\"/></figure><p>时间代价和网络大小又有了明显下降，不过精度也有下降。</p><div class=\"highlight\"><pre><code class=\"language-text\">2.2.3 粗暴地减少网络深度</code></pre></div><p>网络层数决定网络的深度，在一定的范围内，深度越深，网络的性能就越优异。但是从第一张图我们可看出来了，网络越深，featureamap越小，channel数越多，这个时候的计算量也是不小的。</p><p>所以，针对特定的任务去优化模型的时候，我们有必要去优化网络的深度，当然是在满足精度的前提下，越小越好。</p><p>我们从一个比较好的起点开始，从mobilenet_v1开始吧，直接砍掉conv5_5这个block，将其称为mobilenet_v2.1.2。</p><p>下面来看看比较。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-c53166d440f1babb38cf876f47faab31_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1617\" data-rawheight=\"204\" class=\"origin_image zh-lightbox-thumb\" width=\"1617\" data-original=\"https://pic2.zhimg.com/v2-c53166d440f1babb38cf876f47faab31_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1617&#39; height=&#39;204&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1617\" data-rawheight=\"204\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1617\" data-original=\"https://pic2.zhimg.com/v2-c53166d440f1babb38cf876f47faab31_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-c53166d440f1babb38cf876f47faab31_b.jpg\"/></figure><p>从结果来看，精度下降尚且不算很明显，不过时间的优化很有限，模型大小压缩也有限。</p><p>下面在集中看一下同时粗暴地减少网络深度和宽度的结果，称其为mobilenet_v2.1.3</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-c967f91c0928276abf5617aee860ecee_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1617\" data-rawheight=\"204\" class=\"origin_image zh-lightbox-thumb\" width=\"1617\" data-original=\"https://pic3.zhimg.com/v2-c967f91c0928276abf5617aee860ecee_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1617&#39; height=&#39;204&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1617\" data-rawheight=\"204\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1617\" data-original=\"https://pic3.zhimg.com/v2-c967f91c0928276abf5617aee860ecee_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-c967f91c0928276abf5617aee860ecee_b.jpg\"/></figure><p>以损失将近1%的代价，将模型压缩到2.7m，40ms以内，这样的结果，得看实际应用能不能满足要求了。</p><p>总之，粗暴地直接减小深度和宽度，都会造成性能的下降。</p><div class=\"highlight\"><pre><code class=\"language-text\">2.2.4 怎么弥补通道的损失</code></pre></div><p>从上面我们可以看出，减少深度和宽度，虽然减小了模型，但是都带来了精度的损失，很多时候这种精度损失导致模型无法上线。所以，我们需要一些其他方法来解决这个问题。</p><div class=\"highlight\"><pre><code class=\"language-text\">2.2.4.1 crelu通道补偿</code></pre></div><p>从上面可以看出，网络宽度对结果的影响非常严重，如果我们可以想办法维持原来的网络宽度，且不显著增加计算量，那就完美了。正好有这样的方法，来源于这篇文章《Understanding and Improving Convolutional Neural Networks via Concatenated Rectified Linear Units》，它指出网络的参数有互补的现象，如果将减半后的通道补上它的反，会基本上相当于原有的模型，虽然原文针对的是网络浅层有这样的现象，不过深层我们不妨一试，将其用于参数量和计算代价都比较大的conv5_1到conv5_4，我们直接从mobilenet_v2.1.3开始，增加conv5_1到conv5_4的网络宽度，称之为mobilenet_v2.1.4。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-08fc41322d5747c5d602921f94ef1277_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1617\" data-rawheight=\"204\" class=\"origin_image zh-lightbox-thumb\" width=\"1617\" data-original=\"https://pic4.zhimg.com/v2-08fc41322d5747c5d602921f94ef1277_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1617&#39; height=&#39;204&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1617\" data-rawheight=\"204\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1617\" data-original=\"https://pic4.zhimg.com/v2-08fc41322d5747c5d602921f94ef1277_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-08fc41322d5747c5d602921f94ef1277_b.jpg\"/></figure><div class=\"highlight\"><pre><code class=\"language-text\">2.2.4.2 skip connect，融合不同层的信息</code></pre></div><p>这是说的不能再多，用的不能再多了的技术。从FCN开始，为了恢复分割细节，从底层添加branch到高层几乎就是必用的技巧了，它不一定能在精度指标上有多少提升，但是对于分割的细节一般是正向的。</p><p>我们直接从mobilenet_v2.1.3开始，添加3个尺度的skip connection。由于底层的channel数量较大，deconv后的channel数量较小，因此我们添加1*1卷积改变通道，剩下来就有了两种方案，1，concat。2，eltwise。</p><p>针对这两种方案，我们分别进行试验。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-c75474d954402352ed4dc0cb6a0ff5e7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1617\" data-rawheight=\"271\" class=\"origin_image zh-lightbox-thumb\" width=\"1617\" data-original=\"https://pic4.zhimg.com/v2-c75474d954402352ed4dc0cb6a0ff5e7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1617&#39; height=&#39;271&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1617\" data-rawheight=\"271\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1617\" data-original=\"https://pic4.zhimg.com/v2-c75474d954402352ed4dc0cb6a0ff5e7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-c75474d954402352ed4dc0cb6a0ff5e7_b.jpg\"/></figure><p>从上表可以看出，两个方案都不错，时间代价和模型大小增加都很小，而精度提升较大。</p><p>现在反过头回去看刚开始的模型v0，在精确度没有下降的情况下，我们已经把速度优化了5倍以上，模型大小压缩到原来的1/10，已经满足一个通用的线上模型了。</p><p class=\"ztext-empty-paragraph\"><br/></p><blockquote>当然，我们不可能道尽所有的技术，而接着上面的思路，也还有很多可以做的事情，本篇的重点，是让大家<b>学会分析性能网络的性能瓶颈，从而针对性的去优化网络</b>。更多类似技巧和实验，作为技术人员，自己尝试去吧。</blockquote><p class=\"ztext-empty-paragraph\"><br/></p><p><b>更多请移步</b></p><blockquote>1，我的gitchat达人课</blockquote><a href=\"https://link.zhihu.com/?target=http%3A//gitbook.cn/gitchat/column/5a6011fbbd5ff2623773394c\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic4.zhimg.com/v2-778539fd0d72d5e46ebabd371db4df83_180x120.jpg\" data-image-width=\"1920\" data-image-height=\"1080\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">龙鹏的达人课</a><blockquote>2，AI技术公众号,《与有三学AI》</blockquote><a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649028681%26idx%3D1%26sn%3D44147bf8c9006c5d72dd51ed2f2871ba%26chksm%3D87134634b064cf22501feda3afff66beec38a09ec0ff91ac58c70a4f8c00da83b8ecfbf50175%23rd\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-2d9ceed78978badf52f685b50ced44c6_ipico.jpg\" data-image-width=\"358\" data-image-height=\"358\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">一文说说这十多年来计算机玩摄影的历史</a><blockquote>3，以及摄影号，《有三工作室》</blockquote><a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3MzczNzY0Mw%3D%3D%26mid%3D2648543699%26idx%3D1%26sn%3Da17c65aac9a2d07af0af141b4c0048bb%26chksm%3D87234517b054cc01458e30468906cc59010a1fd0a74db8b3a935ef0d83e7d17b163f274a23c2%26scene%3D21%23wechat_redirect\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-8a86f4fc840333cf9e0dea9544dc9dde_ipico.jpg\" data-image-width=\"640\" data-image-height=\"640\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【摄影大咖2】论自拍，我只服这位悬崖上的自拍狂</a><p></p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "AI技术", 
                    "tagLink": "https://api.zhihu.com/topics/20106982"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": [
                {
                    "userName": "kaka366", 
                    "userLink": "https://www.zhihu.com/people/d6fb841aea355166ad9038ae8be941a9", 
                    "content": "用caffe？", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "言有三-龙鹏", 
                            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
                            "content": "嗯，用caffe", 
                            "likes": 0, 
                            "replyToAuthor": "kaka366"
                        }
                    ]
                }, 
                {
                    "userName": "Big Fish", 
                    "userLink": "https://www.zhihu.com/people/d934ea45b23f9992a71acbeb17777902", 
                    "content": "实用！", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "言有三-龙鹏", 
                            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
                            "content": "不灌水不写虚头巴脑的东西😁", 
                            "likes": 0, 
                            "replyToAuthor": "Big Fish"
                        }
                    ]
                }, 
                {
                    "userName": "不懂就要问", 
                    "userLink": "https://www.zhihu.com/people/8e6a22c52c8d36531a760bb728e1f95b", 
                    "content": "<p>很厉害 有github链接吗 </p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "言有三-龙鹏", 
                            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
                            "content": "最近比较忙还没放，五一之前放上去", 
                            "likes": 0, 
                            "replyToAuthor": "不懂就要问"
                        }
                    ]
                }, 
                {
                    "userName": "花好月圆", 
                    "userLink": "https://www.zhihu.com/people/be89ae99b2185a8de57d2e10fd560e26", 
                    "content": "来个github链接吧，多谢！", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "言有三-龙鹏", 
                            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
                            "content": "五一前一定放上去", 
                            "likes": 0, 
                            "replyToAuthor": "花好月圆"
                        }
                    ]
                }, 
                {
                    "userName": "荷叶卷", 
                    "userLink": "https://www.zhihu.com/people/f2b01896a4e6d17e775a7b28be65e215", 
                    "content": "<p>期待 github 链接嘻嘻嘻</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "言有三-龙鹏", 
                            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
                            "content": "会随新书以及gitchat课程一起上线", 
                            "likes": 0, 
                            "replyToAuthor": "荷叶卷"
                        }, 
                        {
                            "userName": "雨人", 
                            "userLink": "https://www.zhihu.com/people/ceac4d25e23c74565fadc3d1a57c6d19", 
                            "content": "<p>新书好像没有第八章的代码？</p>", 
                            "likes": 0, 
                            "replyToAuthor": "言有三-龙鹏"
                        }
                    ]
                }, 
                {
                    "userName": "小灰灰超", 
                    "userLink": "https://www.zhihu.com/people/56d8f862c20782123fff71af1758a0b5", 
                    "content": "想问一下 那个时间开销是用什么工具统计的", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "言有三-龙鹏", 
                            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
                            "content": "mac上自己写代码", 
                            "likes": 0, 
                            "replyToAuthor": "小灰灰超"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/39714186", 
            "userName": "言有三-龙鹏", 
            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
            "upvote": 7, 
            "title": "【开源框架】Google百度到微博，优酷腾讯到抖音，这些爬虫", 
            "content": "<p>文章首发于微信公众号 《与有三学AI》</p><a data-draft-node=\"block\" data-draft-type=\"link-card\" href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029331%26idx%3D1%26sn%3D8b2f18a7f5ccc04d073bb32ce1765648%26chksm%3D871344aeb064cdb8e604931892d4193703aaaaacdd44c697277c9bb377c02da302bcf3b2c02c%23rd\" data-image=\"https://pic3.zhimg.com/v2-2d9ceed78978badf52f685b50ced44c6_ipico.jpg\" data-image-width=\"358\" data-image-height=\"358\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【开源框架】从Google百度到微博，优酷腾讯到抖音，这些爬虫你用过了吗？</a><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-aa824d74439f3ea97f20d909f927018c_b.jpg\" data-rawwidth=\"750\" data-rawheight=\"328\" data-caption=\"\" data-size=\"normal\" class=\"origin_image zh-lightbox-thumb\" width=\"750\" data-original=\"https://pic1.zhimg.com/v2-aa824d74439f3ea97f20d909f927018c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;750&#39; height=&#39;328&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"750\" data-rawheight=\"328\" data-caption=\"\" data-size=\"normal\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"750\" data-original=\"https://pic1.zhimg.com/v2-aa824d74439f3ea97f20d909f927018c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-aa824d74439f3ea97f20d909f927018c_b.jpg\"/></figure><p>今天给大家带来一篇关于数据爬虫使用的文章，该篇文章介绍的爬虫可以爬取Google、Bing、百度三大搜索引擎以及微博中的图片，也可爬取优酷、腾讯、抖音等各大视频网站的视频。</p><blockquote>01概述</blockquote><p>该篇文章主要是搜集了爬图片爬视频的git项目，并给出详细的使用过程，省去了很多找爬虫工具的时间。这些工具亲测暂时有效，早用早好，免得失效。后面我们的git也会更新一些爬虫，欢迎大家关注。</p><blockquote>02图片爬虫</blockquote><p><b>2.1</b> <b>三大搜索引擎</b> </p><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/sczhengyabin/Image-Downloader\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/sczhengyabin</span><span class=\"invisible\">/Image-Downloader</span><span class=\"ellipsis\"></span></a><br/><br/>可以按要求爬取百度、Bing、Google上的图片，并且提供了GUI方便操作，使用方法如下：</p><p>（1）根据该项目的readme配置适合自己电脑的环境就可以使用。使用方法是：python image_downloader_gui.py，会出现界面如下所示（该界面已在爬取状态）：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-b3e1e865f7132d5106b9b854d8074f48_b.jpg\" data-rawwidth=\"532\" data-rawheight=\"487\" data-caption=\"\" data-size=\"normal\" class=\"origin_image zh-lightbox-thumb\" width=\"532\" data-original=\"https://pic1.zhimg.com/v2-b3e1e865f7132d5106b9b854d8074f48_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;532&#39; height=&#39;487&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"532\" data-rawheight=\"487\" data-caption=\"\" data-size=\"normal\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"532\" data-original=\"https://pic1.zhimg.com/v2-b3e1e865f7132d5106b9b854d8074f48_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-b3e1e865f7132d5106b9b854d8074f48_b.jpg\"/></figure><p>（2）如上图所示，keywords中可以输入自己要爬的关键字，多关键字用逗号隔开，这种方式输入英文关键字和中文关键字对于Windows和Linux是没有区别的，都可以按正常流程爬取。还有一种方式输入关键字是Load File，这种方式是把关键字写到一个txt文件，关键字之间也需用逗号隔开，用load的方式输入，这时候要注意对于txt文件中有中文关键字时，Windows系统爬取会因为编码的原因导致报错，从而爬取失败。【注】：一般建议在关键字不超过100个时，直接把txt中关键字复制粘贴到keywords，以这种方式爬取，避免不必要的error（适用于Windows系统）<br/>（3）下图爬取的关键字是cat，rose，爬取的引擎是Google，爬取的图片将在Output所示的路径下，可根据Max number/keywords设置想要爬去的图片数量，最大值可设为2000</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-707329383ff102bc4652c84b924afadf_b.jpg\" data-rawwidth=\"526\" data-rawheight=\"484\" data-caption=\"\" data-size=\"normal\" class=\"origin_image zh-lightbox-thumb\" width=\"526\" data-original=\"https://pic4.zhimg.com/v2-707329383ff102bc4652c84b924afadf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;526&#39; height=&#39;484&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"526\" data-rawheight=\"484\" data-caption=\"\" data-size=\"normal\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"526\" data-original=\"https://pic4.zhimg.com/v2-707329383ff102bc4652c84b924afadf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-707329383ff102bc4652c84b924afadf_b.jpg\"/></figure><p>（4）cat，rose的爬取结果如下图，会生成以关键字为名的文件夹，爬取的图片在相应关键字的文件夹下</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-cc83bcda7dd088277fba2afbe321781b_b.jpg\" data-rawwidth=\"403\" data-rawheight=\"152\" data-caption=\"\" data-size=\"normal\" class=\"content_image\" width=\"403\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;403&#39; height=&#39;152&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"403\" data-rawheight=\"152\" data-caption=\"\" data-size=\"normal\" class=\"content_image lazy\" width=\"403\" data-actualsrc=\"https://pic4.zhimg.com/v2-cc83bcda7dd088277fba2afbe321781b_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-db20704f0acbb623dea168b620ce34ea_b.jpg\" data-rawwidth=\"476\" data-rawheight=\"210\" data-caption=\"\" data-size=\"normal\" class=\"origin_image zh-lightbox-thumb\" width=\"476\" data-original=\"https://pic3.zhimg.com/v2-db20704f0acbb623dea168b620ce34ea_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;476&#39; height=&#39;210&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"476\" data-rawheight=\"210\" data-caption=\"\" data-size=\"normal\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"476\" data-original=\"https://pic3.zhimg.com/v2-db20704f0acbb623dea168b620ce34ea_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-db20704f0acbb623dea168b620ce34ea_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-1845ab8195c715b2bc4f74a4c4903a52_b.jpg\" data-rawwidth=\"469\" data-rawheight=\"237\" data-caption=\"\" data-size=\"normal\" class=\"origin_image zh-lightbox-thumb\" width=\"469\" data-original=\"https://pic3.zhimg.com/v2-1845ab8195c715b2bc4f74a4c4903a52_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;469&#39; height=&#39;237&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"469\" data-rawheight=\"237\" data-caption=\"\" data-size=\"normal\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"469\" data-original=\"https://pic3.zhimg.com/v2-1845ab8195c715b2bc4f74a4c4903a52_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-1845ab8195c715b2bc4f74a4c4903a52_b.jpg\"/></figure><p>(5) 可以以同样的方式在Bing和百度上爬取图片，比如还是爬取cat，rose。如果不改变保存路径（Output），爬取得到的图片会和在Google上爬取的图片在同一个文件夹下，下图是在Google和Bing上爬取的图片。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-4c5bdfcce466de3b3b8f1c94bdf2bb2a_b.jpg\" data-rawwidth=\"452\" data-rawheight=\"271\" data-caption=\"\" data-size=\"normal\" class=\"origin_image zh-lightbox-thumb\" width=\"452\" data-original=\"https://pic3.zhimg.com/v2-4c5bdfcce466de3b3b8f1c94bdf2bb2a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;452&#39; height=&#39;271&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"452\" data-rawheight=\"271\" data-caption=\"\" data-size=\"normal\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"452\" data-original=\"https://pic3.zhimg.com/v2-4c5bdfcce466de3b3b8f1c94bdf2bb2a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-4c5bdfcce466de3b3b8f1c94bdf2bb2a_b.jpg\"/></figure><p>这个爬虫足够满足小型项目第一批数据集的积累，还有就是如果在一个引擎上重复爬取同一个关键字，并不会出现命名冲突，因为会被覆盖。</p><p><b>2.2</b> <b>微博</b> </p><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/yAnXImIN/weiboPicDownloader.git\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/yAnXImIN/wei</span><span class=\"invisible\">boPicDownloader.git</span><span class=\"ellipsis\"></span></a>，可以爬取微博某用户的图片，使用方法如下：<br/>（1）该爬虫是由java写的，具体使用见下图</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-2f7b2623f51cb2893c897201953362a0_b.jpg\" data-rawwidth=\"538\" data-rawheight=\"263\" data-caption=\"\" data-size=\"normal\" class=\"origin_image zh-lightbox-thumb\" width=\"538\" data-original=\"https://pic1.zhimg.com/v2-2f7b2623f51cb2893c897201953362a0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;538&#39; height=&#39;263&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"538\" data-rawheight=\"263\" data-caption=\"\" data-size=\"normal\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"538\" data-original=\"https://pic1.zhimg.com/v2-2f7b2623f51cb2893c897201953362a0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-2f7b2623f51cb2893c897201953362a0_b.jpg\"/></figure><p>（2）经过对我自己微博的爬取，发现对于已设为私密的图片是不能爬取的，公开的图片爬取是比较完整的，结果如下图</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-d0467aa805739ec0539918bdeee300dd_b.jpg\" data-rawwidth=\"322\" data-rawheight=\"165\" data-caption=\"\" data-size=\"normal\" class=\"content_image\" width=\"322\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;322&#39; height=&#39;165&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"322\" data-rawheight=\"165\" data-caption=\"\" data-size=\"normal\" class=\"content_image lazy\" width=\"322\" data-actualsrc=\"https://pic2.zhimg.com/v2-d0467aa805739ec0539918bdeee300dd_b.jpg\"/></figure><p>以下两个爬虫也是与图片视频相关的，我附上git地址，有需要的可以了解一下。</p><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/iawia002/Lulu.githttps%3A//github.com/EvilCult/Video-Downloader.git\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/iawia002/Lul</span><span class=\"invisible\">u.githttps://github.com/EvilCult/Video-Downloader.git</span><span class=\"ellipsis\"></span></a></p><blockquote>03视频爬虫</blockquote><p><b>3.1</b> <b>annie</b> </p><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/iawia002/annie.git\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/iawia002/ann</span><span class=\"invisible\">ie.git</span><span class=\"ellipsis\"></span></a><br/>Annie是一款以go语言编码的视频下载工具，使用便捷并支持抖音、腾讯视频等多个网站视频和图像的下载，其支持站点如下图所示：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-5fe4a47455000211e333a90087f441f5_b.jpg\" data-rawwidth=\"640\" data-rawheight=\"528\" data-caption=\"\" data-size=\"normal\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic2.zhimg.com/v2-5fe4a47455000211e333a90087f441f5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;640&#39; height=&#39;528&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"640\" data-rawheight=\"528\" data-caption=\"\" data-size=\"normal\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https://pic2.zhimg.com/v2-5fe4a47455000211e333a90087f441f5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-5fe4a47455000211e333a90087f441f5_b.jpg\"/></figure><p>（1）根据readme中的安装说明安装好ffmpeg和annie，其中ffmpeg是用于融合分段下载的视频。</p><p>（2）图片下载：在终端运行以下命令，下载时会显示图片的信息和下载进度。图片下载不拘于之前展示的网站，但要提供图片详细的地址，精确到后缀名。<br/>        $ annie http://…  (图片网址) </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-d1125687a44d3bcedffda5586a6644e8_b.jpg\" data-rawwidth=\"640\" data-rawheight=\"114\" data-caption=\"\" data-size=\"normal\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic1.zhimg.com/v2-d1125687a44d3bcedffda5586a6644e8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;640&#39; height=&#39;114&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"640\" data-rawheight=\"114\" data-caption=\"\" data-size=\"normal\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https://pic1.zhimg.com/v2-d1125687a44d3bcedffda5586a6644e8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-d1125687a44d3bcedffda5586a6644e8_b.jpg\"/></figure><p>（3）视频下载：在终端运行以下命令，下载时同样会显示视频的信息和下载进度。对于存在多种清晰度的视频，可通过可选参数进行选择下载，默认下载当前网址的高清度视频，因此可先查看当前视频的信息，在通过可选参数进行有选择的下载。<br/>        $ annie ［可选参数］http://…  (视频网址) </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-89e02845a92635ae865cd0c1a5f41464_b.jpg\" data-rawwidth=\"640\" data-rawheight=\"92\" data-caption=\"\" data-size=\"normal\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic1.zhimg.com/v2-89e02845a92635ae865cd0c1a5f41464_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;640&#39; height=&#39;92&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"640\" data-rawheight=\"92\" data-caption=\"\" data-size=\"normal\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https://pic1.zhimg.com/v2-89e02845a92635ae865cd0c1a5f41464_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-89e02845a92635ae865cd0c1a5f41464_b.jpg\"/></figure><p>[可选参数]主要有以下几个，可进行组合搭配使用：<br/>-i  仅展示信息，不进行下载<br/>-p 下载当前网址的整个播放列表<br/>-f 读取视频信息中的地址（用于控制下载视频精度）</p><p>视频下载会下载到当前目录下，且仅支持开源视频的下载，对于部分vip视频或付费视频，annie会被禁止访问其下载地址（所以即使有了annie，也不要想着我以后可以不花钱看电影了）。最后展示一下视频下载结果：<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-46808e09f15414fe797da2361a96c636_b.jpg\" data-rawwidth=\"640\" data-rawheight=\"382\" data-caption=\"\" data-size=\"normal\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic3.zhimg.com/v2-46808e09f15414fe797da2361a96c636_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;640&#39; height=&#39;382&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"640\" data-rawheight=\"382\" data-caption=\"\" data-size=\"normal\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https://pic3.zhimg.com/v2-46808e09f15414fe797da2361a96c636_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-46808e09f15414fe797da2361a96c636_b.jpg\"/></figure><p><b>3.2</b> <b>抖音爬虫</b> </p><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/loadchange/amemv-crawler.git\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/loadchange/a</span><span class=\"invisible\">memv-crawler.git</span><span class=\"ellipsis\"></span></a><br/><br/>amemv-crawler是一个Python脚本，可以下载指定抖音用户的全部视频(含收藏)，也可以下载指定主题(挑战)或音乐下的全部视频，运行环境为python3。使用方法如下：</p><p>（1）将项目下载到本地，目录如下图所示，修改其中的amemv-video-ripper.py，找到第131行的内置函数generateSignature，将所有调用该函数的语句注释后就可以使用。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-7df909ae8d96f1c731d9c1ab37a42376_b.jpg\" data-rawwidth=\"640\" data-rawheight=\"227\" data-caption=\"\" data-size=\"normal\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic3.zhimg.com/v2-7df909ae8d96f1c731d9c1ab37a42376_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;640&#39; height=&#39;227&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"640\" data-rawheight=\"227\" data-caption=\"\" data-size=\"normal\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https://pic3.zhimg.com/v2-7df909ae8d96f1c731d9c1ab37a42376_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-7df909ae8d96f1c731d9c1ab37a42376_b.jpg\"/></figure><p>（2）视频下载：在当前目录下执行命令。要同时下载多个抖音号的视频时，将网址以逗号隔开，download文件夹中会自动建立以抖音号为名的文件夹存储相应的抖音视频。<br/>         $ python amemv-video-ripper.py https://…</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-61477768f6d42fafdccf52d4d18de922_b.jpg\" data-rawwidth=\"640\" data-rawheight=\"244\" data-caption=\"\" data-size=\"normal\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic3.zhimg.com/v2-61477768f6d42fafdccf52d4d18de922_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;640&#39; height=&#39;244&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"640\" data-rawheight=\"244\" data-caption=\"\" data-size=\"normal\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https://pic3.zhimg.com/v2-61477768f6d42fafdccf52d4d18de922_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-61477768f6d42fafdccf52d4d18de922_b.jpg\"/></figure><p>更多，欢迎到知乎专栏去投稿与交流，配套资料将放出在github，可扫描二维码进入。</p><p><a href=\"https://zhuanlan.zhihu.com/c_151876233\" class=\"internal\"><span class=\"invisible\">https://</span><span class=\"visible\">zhuanlan.zhihu.com/c_15</span><span class=\"invisible\">1876233</span><span class=\"ellipsis\"></span></a> (二维码自动识别)</p><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/longpeng2008/LongPeng_ML_Course\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/longpeng2008</span><span class=\"invisible\">/LongPeng_ML_Course</span><span class=\"ellipsis\"></span></a> (二维码自动识别)</p><p><b>打一个小广告，我的计算机视觉公开课《AI</b> <b>图像识别项目从入门到上线》上线了，将讲述从零基础到完成一个实际的项目到微信小程序上线的整个流程，欢迎交流捧场。</b><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-9a6efe0a858eb67f83d514b716657617_b.jpg\" data-rawwidth=\"640\" data-rawheight=\"2746\" data-caption=\"\" data-size=\"normal\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic4.zhimg.com/v2-9a6efe0a858eb67f83d514b716657617_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;640&#39; height=&#39;2746&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"640\" data-rawheight=\"2746\" data-caption=\"\" data-size=\"normal\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https://pic4.zhimg.com/v2-9a6efe0a858eb67f83d514b716657617_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-9a6efe0a858eb67f83d514b716657617_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-397c17737b0a2a9a809fcd34420c3076_b.jpg\" data-rawwidth=\"268\" data-rawheight=\"325\" data-caption=\"\" data-size=\"normal\" class=\"content_image\" width=\"268\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;268&#39; height=&#39;325&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"268\" data-rawheight=\"325\" data-caption=\"\" data-size=\"normal\" class=\"content_image lazy\" width=\"268\" data-actualsrc=\"https://pic3.zhimg.com/v2-397c17737b0a2a9a809fcd34420c3076_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-caf2bc6ada0920ad1ce07c326564dad9_b.jpg\" data-rawwidth=\"640\" data-rawheight=\"10\" data-caption=\"\" data-size=\"normal\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic2.zhimg.com/v2-caf2bc6ada0920ad1ce07c326564dad9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;640&#39; height=&#39;10&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"640\" data-rawheight=\"10\" data-caption=\"\" data-size=\"normal\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https://pic2.zhimg.com/v2-caf2bc6ada0920ad1ce07c326564dad9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-caf2bc6ada0920ad1ce07c326564dad9_b.jpg\"/></figure><p>如果想加入我们，后台留言吧</p><p><b>更多请移步</b></p><blockquote>1，我的gitchat达人课</blockquote><a data-draft-node=\"block\" data-draft-type=\"link-card\" href=\"https://link.zhihu.com/?target=http%3A//gitbook.cn/gitchat/column/5a6011fbbd5ff2623773394c\" data-image=\"https://pic4.zhimg.com/v2-778539fd0d72d5e46ebabd371db4df83_180x120.jpg\" data-image-width=\"1920\" data-image-height=\"1080\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">龙鹏的达人课</a><blockquote>2，AI技术公众号,《与有三学AI》</blockquote><a data-draft-node=\"block\" data-draft-type=\"link-card\" href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649028681%26idx%3D1%26sn%3D44147bf8c9006c5d72dd51ed2f2871ba%26chksm%3D87134634b064cf22501feda3afff66beec38a09ec0ff91ac58c70a4f8c00da83b8ecfbf50175%23rd\" data-image=\"https://pic3.zhimg.com/v2-2d9ceed78978badf52f685b50ced44c6_ipico.jpg\" data-image-width=\"358\" data-image-height=\"358\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">一文说说这十多年来计算机玩摄影的历史</a><blockquote>3，以及摄影号，《有三工作室》</blockquote><a data-draft-node=\"block\" data-draft-type=\"link-card\" href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3MzczNzY0Mw%3D%3D%26mid%3D2648543699%26idx%3D1%26sn%3Da17c65aac9a2d07af0af141b4c0048bb%26chksm%3D87234517b054cc01458e30468906cc59010a1fd0a74db8b3a935ef0d83e7d17b163f274a23c2%26scene%3D21%23wechat_redirect\" data-image=\"https://pic3.zhimg.com/v2-8a86f4fc840333cf9e0dea9544dc9dde_ipico.jpg\" data-image-width=\"640\" data-image-height=\"640\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【摄影大咖2】论自拍，我只服这位悬崖上的自拍狂</a><p></p>", 
            "topic": [], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/39456589", 
            "userName": "言有三-龙鹏", 
            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
            "upvote": 7, 
            "title": "【开源框架】Tensorflow图像分类从模型自定义到测试", 
            "content": "<p>上一篇介绍了 Caffe ，这篇将介绍 TensorFlow 相关的内容。</p><h2>1 什么是 TensorFlow</h2><p>TensorFlow 是 Google brain 推出的开源机器学习库，与 Caffe 一样，主要用作深度学习相关的任务。</p><p>与 Caffe 相比 TensorFlow 的安装简单很多，一条 pip 命令就可以解决，新手也不会误入各种坑。</p><p>TensorFlow = Tensor + Flow</p><p>Tensor 就是张量，代表 N 维数组，与 Caffe 中的 blob 是类似的；Flow 即流，代表基于数据流图的计算。神经网络的运算过程，就是数据从一层流动到下一层，在 Caffe 的每一个中间 layer 参数中，都有 bottom 和 top，这就是一个分析和处理的过程。TensorFlow 更直接强调了这个过程。</p><p>TensorFlow 最大的特点是计算图，即先定义好图，然后进行运算，所以所有的 TensorFlow 代码，都包含两部分：</p><p>（1）创建计算图，表示计算的数据流。它做了什么呢？实际上就是定义好了一些操作，你可以将它看做是 Caffe 中的 prototxt 的定义过程。</p><p>（2）运行会话，执行图中的运算，可以看作是 Caffe 中的训练过程。只是 TensorFlow 的会话比 Caffe 灵活很多，由于是 Python 接口，取中间结果分析，Debug 等方便很多。</p><h2>2 TensorFlow 训练</h2><p>咱们这是实战课，没有这么多时间去把所有事情细节都说清楚，而是抓住主要脉络。有了 TensorFlow 这个工具后，我们接下来的任务就是开始训练模型。训练模型，包括数据准备、模型定义、结果保存与分析。</p><p><b>2.1 数据准备</b></p><p>上一节我们说过 Caffe 中的数据准备，只需要准备一个 list 文件，其中每一行存储 image、labelid 就可以了，那是 Caffe 默认的分类网络的 imagedata 层的输入格式。如果想定义自己的输入格式，可以去新建自定义的 Data Layer，而 Caffe 官方的 data layer 和 imagedata layer 都非常稳定，几乎没有变过，这是我更欣赏 Caffe 的一个原因。因为输入数据，简单即可。相比之下，TensorFlow 中的数据输入接口就要复杂很多，更新也非常快，我知乎有一篇文章，说过从《从 Caffe 到 TensorFlow 1，IO 操作》，有兴趣的读者可以了解一下。</p><p>这里我们不再说 TensorFlow 中有多少种数据 IO 方法，先确定好我们的数据格式，那就是跟 Caffe 一样，准备好一个 list，它的格式一样是 image、labelid，然后再看如何将数据读入 TensorFlow 进行训练。</p><p>我们定义一个类，叫 imagedata，模仿 Caffe 中的使用方式。代码如下，源代码可移步 Git 公众号：</p><div class=\"highlight\"><pre><code class=\"language-text\">import tensorflow as tf\n    from tensorflow.contrib.data import Dataset\n    from tensorflow.python.framework import dtypes\n    from tensorflow.python.framework.ops import convert_to_tensor\n    import numpy as np\n    class ImageData:        def read_txt_file(self):\n            self.img_paths = []\n            self.labels = []\n            for line in open(self.txt_file, &#39;r&#39;):\n                items = line.split(&#39; &#39;)\n                self.img_paths.append(items[0])\n                self.labels.append(int(items[1]))\n        def __init__(self, txt_file, batch_size, num_classes,\n                     image_size,buffer_scale=100):\n            self.image_size = image_size\n            self.batch_size = batch_size\n            self.txt_file = txt_file ##txt list file,stored as: imagename id            self.num_classes = num_classes\n            buffer_size = batch_size * buffer_scale\n\n        # 读取图片        self.read_txt_file()\n        self.dataset_size = len(self.labels) \n        print &#34;num of train datas=&#34;,self.dataset_size\n        # 转换成Tensor        self.img_paths = convert_to_tensor(self.img_paths, dtype=dtypes.string)\n        self.labels = convert_to_tensor(self.labels, dtype=dtypes.int32)\n\n        # 创建数据集        data = Dataset.from_tensor_slices((self.img_paths, self.labels))\n        print &#34;data type=&#34;,type(data)\n        data = data.map(self.parse_function)\n        data = data.repeat(1000)\n        data = data.shuffle(buffer_size=buffer_size)\n\n        # 设置self data Batch        self.data = data.batch(batch_size)\n        print &#34;self.data type=&#34;,type(self.data)\n\n        def augment_dataset(self,image,size):\n            distorted_image = tf.image.random_brightness(image,\n                                               max_delta=63)\n            distorted_image = tf.image.random_contrast(distorted_image,\n                                             lower=0.2, upper=1.8)\n            # Subtract off the mean and divide by the variance of the pixels.            float_image = tf.image.per_image_standardization(distorted_image)\n            return float_image\n\n        def parse_function(self, filename, label):\n            label_ = tf.one_hot(label, self.num_classes)\n            img = tf.read_file(filename)\n            img = tf.image.decode_jpeg(img, channels=3)\n            img = tf.image.convert_image_dtype(img, dtype = tf.float32)\n            img = tf.random_crop(img,[self.image_size[0],self.image_size[1],3])\n            img = tf.image.random_flip_left_right(img)\n            img = self.augment_dataset(img,self.image_size)\n            return img, label_</code></pre></div><p>下面来分析上面的代码，类是 ImageData，它包含几个函数，<code>__init__</code>构造函数，<code>read_txt_file</code>数据读取函数，<code>parse_function</code>数据预处理函数，<code>augment_dataset</code>数据增强函数。</p><p>我们直接看构造函数吧，分为几个步骤：</p><p>（1）读取变量，文本 list 文件<code>txt_file</code>，批处理大小<code>batch_size</code>，类别数<code>num_classes</code>，要处理成的图片大小<code>image_size</code>，一个内存变量<code>buffer_scale=100</code>。</p><p>（2）在获取完这些值之后，就到了<code>read_txt_file</code>函数。代码很简单，就是利用<code>self.img_paths</code>和 <code>self.labels</code>存储输入 txt 中的文件列表和对应的 label，这一点和 Caffe 很像了。</p><p>（3）然后，就是分别将<code>img_paths</code>和 <code>labels</code> 转换为 Tensor，函数是<code>convert_to_tensor</code>，这是 Tensor 内部的数据结构。</p><p>（4）创建 dataset，<code>Dataset.from_tensor_slices</code>，这一步，是为了将 img 和 label 合并到一个数据格式，此后我们将利用它的接口，来循环读取数据做训练。当然，创建好 dataset 之后，我们需要给它赋值才能真正的有数据。<code>data.map</code> 就是数据的预处理，包括读取图片、转换格式、随机旋转等操作，可以在这里做。</p><p>data = data.repeat(1000) 是将数据复制 1000 份，这可以满足我们训练 1000 个 epochs。<code>data = data.shuffle(buffer_size=buffer_size)</code>就是数据 shuffle 了，<code>buffer_size</code>就是在做 shuffle 操作时的控制变量，内存越大，就可以用越大的值。</p><p>（5）给 selft.data 赋值，我们每次训练的时候，是取一个 batchsize 的数据，所以 self.data = data.batch(batch_size)，就是从上面创建的 dataset 中，一次取一个 batch 的数据。</p><p>到此，数据接口就定义完毕了，接下来在训练代码中看如何使用迭代器进行数据读取就可以了。</p><p>关于更多 TensorFlow 的数据读取方法，请移步知乎专栏和公众号。</p><p><b>2.2 模型定义</b></p><p>创建数据接口后，我们开始定义一个网络。</p><div class=\"highlight\"><pre><code class=\"language-text\">def simpleconv3net(x):        x_shape = tf.shape(x)\n        with tf.variable_scope(&#34;conv3_net&#34;):\n            conv1 = tf.layers.conv2d(x, name=&#34;conv1&#34;, filters=12,kernel_size=[3,3], strides=(2,2), activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer(),bias_initializer=tf.contrib.layers.xavier_initializer())\n            bn1 = tf.layers.batch_normalization(conv1, training=True, name=&#39;bn1&#39;)\n            conv2 = tf.layers.conv2d(bn1, name=&#34;conv2&#34;, filters=24,kernel_size=[3,3], strides=(2,2), activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer(),bias_initializer=tf.contrib.layers.xavier_initializer())\n            bn2 = tf.layers.batch_normalization(conv2, training=True, name=&#39;bn2&#39;)\n            conv3 = tf.layers.conv2d(bn2, name=&#34;conv3&#34;, filters=48,kernel_size=[3,3], strides=(2,2), activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer(),bias_initializer=tf.contrib.layers.xavier_initializer())\n            bn3 = tf.layers.batch_normalization(conv3, training=True, name=&#39;bn3&#39;)\n            conv3_flat = tf.reshape(bn3, [-1, 5 * 5 * 48])\n            dense = tf.layers.dense(inputs=conv3_flat, units=128, activation=tf.nn.relu,name=&#34;dense&#34;,kernel_initializer=tf.contrib.layers.xavier_initializer())\n            logits= tf.layers.dense(inputs=dense, units=2, activation=tf.nn.relu,name=&#34;logits&#34;,kernel_initializer=tf.contrib.layers.xavier_initializer())\n\n            if debug:\n                print &#34;x size=&#34;,x.shape\n                print &#34;relu_conv1 size=&#34;,conv1.shape\n                print &#34;relu_conv2 size=&#34;,conv2.shape\n                print &#34;relu_conv3 size=&#34;,conv3.shape\n                print &#34;dense size=&#34;,dense.shape\n                print &#34;logits size=&#34;,logits.shape\n\n        return logits</code></pre></div><p>上面就是我们定义的网络，是一个简单的3层卷积。在 tf.layers 下，有各种网络层，这里就用到了 <code>tf.layers.conv2d</code>，<code>tf.layers.batch_normalization</code>和 <code>tf.layers.dense</code>，分别是卷积层，BN 层和全连接层。我们以一个卷积层为例：</p><div class=\"highlight\"><pre><code class=\"language-text\">conv1 = tf.layers.conv2d(x, name=&#34;conv1&#34;, filters=12,kernel_size=[3,3], strides=(2,2), activation=tf.nn.relu,kernel_initializer=tf.contrib.layers.xavier_initializer(),bias_initializer=tf.contrib.layers.xavier_initializer())</code></pre></div><p>x 即输入，name 是网络名字，filters 是卷积核数量，<code>kernel_size</code>即卷积核大小，<code>strides</code> 是卷积 <code>stride，activation</code> 即激活函数，<code>kernel_initializer</code>和<code>bias_initializer</code>分别是初始化方法。可见已经将激活函数整合进了卷积层，更全面的参数，请自查 API。</p><p>其实网络的定义，还有其他接口，tf.nn、tf.layers、tf.contrib，各自重复，在我看来有些混乱。</p><p>这里之所以用 tf.layers，就是因为参数丰富，适合从头训练一个模型。</p><p><b>2.3 模型训练</b></p><p>老规矩，我们直接上代码，其实很简单。</p><div class=\"highlight\"><pre><code class=\"language-text\">////-------1 定义一些全局变量-------////\n\n    from dataset import *\n    from net import simpleconv3net\n    import sys\n    import cv2\n    txtfile = sys.argv[1]\n    batch_size = 16    num_classes = 2    image_size = (48,48)\n    learning_rate = 0.01    debug=False\n////-------2 载入网络结构，定义损失函数，创建计算图-------////\n\n    dataset = ImageData(txtfile,batch_size,num_classes,image_size)\n    iterator = dataset.data.make_one_shot_iterator()\n    dataset_size = dataset.dataset_size\n    batch_images,batch_labels = iterator.get_next()\n    Ylogits = simpleconv3net(batch_images)\n\n    print &#34;Ylogits size=&#34;,Ylogits.shape\n\n    Y = tf.nn.softmax(Ylogits)\n    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits, labels=batch_labels)\n    cross_entropy = tf.reduce_mean(cross_entropy)\n    correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(batch_labels, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n    with tf.control_dependencies(update_ops):\n        train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)\n\n    saver = tf.train.Saver()\n    in_steps = 100\n    checkpoint_dir = &#39;checkpoints/&#39;\n    if not os.path.exists(checkpoint_dir):\n        os.mkdir(checkpoint_dir)\n    log_dir = &#39;logs/&#39;\n    if not os.path.exists(log_dir):\n        os.mkdir(log_dir)\n    summary = tf.summary.FileWriter(logdir=log_dir)\n    loss_summary = tf.summary.scalar(&#34;loss&#34;, cross_entropy)\n    acc_summary = tf.summary.scalar(&#34;acc&#34;, accuracy)\n    image_summary = tf.summary.image(&#34;image&#34;, batch_images)\n////-------3 执行会话，保存相关变量，还可以添加一些debug函数来查看中间结果-------////\n\n    with tf.Session() as sess:  \n        init = tf.global_variables_initializer()\n        sess.run(init)  \n        steps = 10000  \n        for i in range(steps): \n            _,cross_entropy_,accuracy_,batch_images_,batch_labels_,loss_summary_,acc_summary_,image_summary_ = sess.run([train_step,cross_entropy,accuracy,batch_images,batch_labels,loss_summary,acc_summary,image_summary])\n            if i % in_steps == 0 :\n                print i,&#34;iterations,loss=&#34;,cross_entropy_,&#34;acc=&#34;,accuracy_\n                saver.save(sess, checkpoint_dir + &#39;model.ckpt&#39;, global_step=i)    \n                summary.add_summary(loss_summary_, i)\n                summary.add_summary(acc_summary_, i)\n                summary.add_summary(image_summary_, i)\n                #print &#34;predict=&#34;,Ylogits,&#34; labels=&#34;,batch_labels\n\n                if debug:\n                    imagedebug = batch_images_[0].copy()\n                    imagedebug = np.squeeze(imagedebug)\n                    print imagedebug,imagedebug.shape\n                    print np.max(imagedebug)\n                    imagelabel = batch_labels_[0].copy()\n                    print np.squeeze(imagelabel)\n\n                    imagedebug = cv2.cvtColor((imagedebug*255).astype(np.uint8),cv2.COLOR_RGB2BGR)\n                    cv2.namedWindow(&#34;debug image&#34;,0)\n                    cv2.imshow(&#34;debug image&#34;,imagedebug)\n                    k = cv2.waitKey(0)\n                    if k == ord(&#39;q&#39;):\n                        break\n</code></pre></div><p><b>2.4 可视化</b></p><p>TensorFlow 很方便的一点，就是 Tensorboard 可视化。Tensorboard 的具体原理就不细说了，很简单，就是三步。</p><p>第一步，创建日志目录。</p><div class=\"highlight\"><pre><code class=\"language-text\">log_dir = &#39;logs/&#39;    if not os.path.exists(log_dir):\n        os.mkdir(log_dir)</code></pre></div><p>第二步，创建 summary 操作并分配标签，如我们要记录 loss、acc 和迭代中的图片，则创建了下面的变量：</p><div class=\"highlight\"><pre><code class=\"language-text\">loss_summary = tf.summary.scalar(&#34;loss&#34;, cross_entropy)acc_summary = tf.summary.scalar(&#34;acc&#34;, accuracy)image_summary = tf.summary.image(&#34;image&#34;, batch_images)</code></pre></div><p>第三步，session 中记录结果，如下面代码：</p><div class=\"highlight\"><pre><code class=\"language-text\">_,cross_entropy_,accuracy_,batch_images_,batch_labels_,loss_summary_,acc_summary_,image_summary_ = sess.run([train_step,cross_entropy,accuracy,batch_images,batch_labels,loss_summary,acc_summary,image_summary])</code></pre></div><p>查看训练过程和最终结果时使用：</p><div class=\"highlight\"><pre><code class=\"language-text\">tensorboard --logdir=logs</code></pre></div><p>Loss 和 acc 的曲线图如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3ae884d7da00df97b6772f5855c40bbd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1072\" data-rawheight=\"603\" class=\"origin_image zh-lightbox-thumb\" width=\"1072\" data-original=\"https://pic2.zhimg.com/v2-3ae884d7da00df97b6772f5855c40bbd_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1072&#39; height=&#39;603&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1072\" data-rawheight=\"603\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1072\" data-original=\"https://pic2.zhimg.com/v2-3ae884d7da00df97b6772f5855c40bbd_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-3ae884d7da00df97b6772f5855c40bbd_b.jpg\"/></figure><h2>3 TensorFlow 测试</h2><p>上面已经训练好了模型，我们接下来的目标，就是要用它来做 inference 了。同样给出代码。</p><div class=\"highlight\"><pre><code class=\"language-text\">import tensorflow as tf\nfrom net import simpleconv3net\nimport sys\nimport numpy as np\nimport cv2\nimport os\n\ntestsize = 48\nx = tf.placeholder(tf.float32, [1,testsize,testsize,3])\ny = simpleconv3net(x)\ny = tf.nn.softmax(y)\n\nlines = open(sys.argv[2]).readlines()\ncount = 0\nacc = 0\nposacc = 0\nnegacc = 0\nposcount = 0\nnegcount = 0\n\nwith tf.Session() as sess:  \n    init = tf.global_variables_initializer()\n    sess.run(init)  \n    saver = tf.train.Saver()\n    saver.restore(sess,sys.argv[1])\n    \n    #test one by one, you can change it into batch inputs\n    for line in lines:\n        imagename,label = line.strip().split(&#39; &#39;)\n        img = tf.read_file(imagename)\n        img = tf.image.decode_jpeg(img,channels = 3)\n        img = tf.image.convert_image_dtype(img,dtype = tf.float32)\n        img = tf.image.resize_images(img,(testsize,testsize),method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n        img = tf.image.per_image_standardization(img)\n\n        imgnumpy = img.eval()\n        imgs = np.zeros([1,testsize,testsize,3],dtype=np.float32)\n        imgs[0:1,] = imgnumpy\n\n        result = sess.run(y, feed_dict={x:imgs})\n        result = np.squeeze(result)\n        if result[0] &gt; result[1]:\n            predict = 0\n        else:\n            predict = 1\n\n        count = count + 1\n        if str(predict) == &#39;0&#39;:\n            negcount = negcount + 1\n            if str(label) == str(predict):\n                negacc = negacc + 1\n                acc = acc + 1\n        else:\n            poscount = poscount + 1\n            if str(label) == str(predict):\n                posacc = posacc + 1\n                acc = acc + 1\n    \n        print result\nprint &#34;acc = &#34;,float(acc) / float(count)\nprint &#34;poscount=&#34;,poscount\nprint &#34;posacc = &#34;,float(posacc) / float(poscount)\nprint &#34;negcount=&#34;,negcount\nprint &#34;negacc = &#34;,float(negacc) / float(negcount)</code></pre></div><p>从上面的代码可知，与 Train 时同样，需要定义模型，这个跟 Caffe 在测试时使用的 Deploy 是一样的。</p><p>然后，用 restore 函数从 saver 中载入参数，读取图像并准备好网络的格式，sess.run 就可以得到最终的结果了。</p><p><b>总结：</b>本篇内容讲解了一个最简单的分类例子，相比大部分已封装好的 mnist 或 cifar 为例的代码来说更实用。我们自己准备了数据集，自己设计了网络并进行了结果可视化，学习了如何使用已经训练好的模型做预测。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-60feffab1842911ce09ae93eeaae724f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"3999\" data-rawheight=\"2250\" class=\"origin_image zh-lightbox-thumb\" width=\"3999\" data-original=\"https://pic4.zhimg.com/v2-60feffab1842911ce09ae93eeaae724f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;3999&#39; height=&#39;2250&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"3999\" data-rawheight=\"2250\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"3999\" data-original=\"https://pic4.zhimg.com/v2-60feffab1842911ce09ae93eeaae724f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-60feffab1842911ce09ae93eeaae724f_b.jpg\"/></figure><blockquote>本系列完整文章：</blockquote><p>第一篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029846%26idx%3D1%26sn%3D0c343cfd0ede5c8ae1405bd6348aefad%26chksm%3D871342abb064cbbd7fe31fb3c55f23875f27e48fb8354e9855823b1701f1227c71b4eb00de50%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【caffe速成】caffe图像分类从模型自定义到测试</a></p><p>第二篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029846%26idx%3D2%26sn%3D7c2582243bcd8f8b491e8e466a21978f%26chksm%3D871342abb064cbbd0cba24b408ceda2b64a7c8b6baa07f9f8f56cd4d1233caa0b80fe357753e%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【tensorflow速成】Tensorflow图像分类从模型自定义到测试</a></p><p>第三篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029881%26idx%3D1%26sn%3D3c869fcee3b48d3582952ab9a0683ea6%26chksm%3D87134284b064cb924c5e7231b3f2c36ba27e3a689b067f569f2e086f62b18413bcebc5987a07%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【pytorch速成】Pytorch图像分类从模型自定义到测试</a></p><p>第四篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029887%26idx%3D1%26sn%3D645b97809c24922352a0b39f19c9ef0c%26chksm%3D87134282b064cb9441af68124d205d9c7dedcaeb09788f4d586b949584e556eddd3a72217f69%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【paddlepaddle速成】paddlepaddle图像分类从模型自定义到测试</a></p><p>第五篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029896%26idx%3D1%26sn%3Df3f7b9cf69c514f45d1d14205f879270%26chksm%3D87134375b064ca6323354c40f3e55b02ff0d1d24f3dacfc980190d51f20ec9ec16e60c1a4741%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【Keras速成】Keras图像分类从模型自定义到测试</a></p><p>第六篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029904%26idx%3D1%26sn%3D0bdc6947f5ac68e7f68426b9d076b4ab%26chksm%3D8713436db064ca7b3b2a2a1d6a8d24c15069f72655e2c39e2498051fa0e56bbcf6fe9c332d5b%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【mxnet速成】mxnet图像分类从模型自定义到测试</a></p><p>第七篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649030976%26idx%3D1%26sn%3D0befc170a93d365b780c5f05b2f510a4%26chksm%3D8712bf3db065362b0aeaae82bdbac467be5697b34cac2797e2ee15cdcb97474c08640482bf07%26token%3D1695328272%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【cntk速成】cntk图像分类从模型自定义到测试</a></p><p>第八篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26tempkey%3DMTAwMF93Q3VDbklva2NBZDJXV1dnNWloZjJ2YWtZWmJ2aTRRaGVDMVNfbERSN2xVYXdXd3B4eWczWWdMbnQzRjZrRVpmSGRYMndrLTQ0VnhPcURTNGhsS3dYWGd3MTk0UGtmZkltU0U2U01OUmRfQzcySDVObDlxM3R2U3Q0SlNQWGRMc1NZeWtWTlVRN1NpRlJaWXRYcGdydUNOTUR6clUzVkNleVlKUE5nfn4%253D%26chksm%3D0712ba9b3065338de13ea030951ba2c25c0363d958c905e12c937c2acffa991313abb4d9f751%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【chainer速成】chainer图像分类从模型自定义到测试</a></p><p>第九篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649032012%26idx%3D1%26sn%3Df74c7084621f367adb2518ebec61ca42%26chksm%3D8712bb31b06532270b9ca9550ab48ff78adaad7d38c73645cfb8888218b8e177bebd5d401aa9%26token%3D1258619827%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【DL4J速成】Deeplearning4j图像分类从模型自定义到测试</a></p><p>第十篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649032109%26idx%3D2%26sn%3Da6ff48ec0ae5d8e7a494df7e564d9ac9%26chksm%3D8712bbd0b06532c61d98c786ba1773c29c3dcf1291f9c3cd052c5643e24699eef28481e94b8e%26token%3D1258619827%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【MatConvnet速成】MatConvnet图像分类从模型自定义到测试</a></p><p>第十一篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649032128%26idx%3D2%26sn%3Df889e2255c0ec4960f76ad0383363849%26chksm%3D8712bbbdb06532ab136c0a485bebc5cf181f24a34bec7bab3f97f66be627e09d939b997dae90%26token%3D598159941%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【Lasagne速成】Lasagne/Theano图像分类从模型自定义到测试</a></p><p>第十二篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649032137%26idx%3D2%26sn%3Da9770ead4a9d03f0a4e75be86657453b%26chksm%3D8712bbb4b06532a27f0f02ac30ce4a18eeac1f72ac54b3c5c1b6ec13fee29f03bfc0dcaa1efd%26token%3D1446627305%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【darknet速成】Darknet图像分类从模型自定义到测试</a></p>", 
            "topic": [
                {
                    "tag": "科技", 
                    "tagLink": "https://api.zhihu.com/topics/19556664"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "TensorFlow", 
                    "tagLink": "https://api.zhihu.com/topics/20032249"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/39455845", 
            "userName": "言有三-龙鹏", 
            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
            "upvote": 6, 
            "title": "【开源框架】caffe图像分类从模型自定义到测试", 
            "content": "<p>这一次我们讲讲 Caffe 这个主流的开源框架，从训练到测试出结果的全流程。</p><p>到此，我必须假设大家已经有了深度学习的基础知识并了解卷积网络的工作原理。</p><p>相关的代码、数据都在我们 Git 上，希望大家 Follow 一下这个 Git 项目，后面会持续更新不同框架下的任务。</p><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/longpeng2008/LongPeng_ML_Course\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">longpeng2008/LongPeng_ML_Course</a> </p><p>这一篇我们说一个分类任务，给大家准备了 500 张微笑的图片、500 张非微笑的图片，放置在 data 目录下，图片预览如下，已经缩放到 60*60 的大小：</p><p>这是非微笑的图片：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-ce76ef3f6054b6a69f54c5ba1d5bcfba_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1164\" data-rawheight=\"605\" class=\"origin_image zh-lightbox-thumb\" width=\"1164\" data-original=\"https://pic3.zhimg.com/v2-ce76ef3f6054b6a69f54c5ba1d5bcfba_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1164&#39; height=&#39;605&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1164\" data-rawheight=\"605\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1164\" data-original=\"https://pic3.zhimg.com/v2-ce76ef3f6054b6a69f54c5ba1d5bcfba_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-ce76ef3f6054b6a69f54c5ba1d5bcfba_b.jpg\"/></figure><p>这是微笑的图片：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-7235774a303401fbbbf7e9a7f209e9e3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1167\" data-rawheight=\"610\" class=\"origin_image zh-lightbox-thumb\" width=\"1167\" data-original=\"https://pic4.zhimg.com/v2-7235774a303401fbbbf7e9a7f209e9e3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1167&#39; height=&#39;610&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1167\" data-rawheight=\"610\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1167\" data-original=\"https://pic4.zhimg.com/v2-7235774a303401fbbbf7e9a7f209e9e3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-7235774a303401fbbbf7e9a7f209e9e3_b.jpg\"/></figure><h2>1 Caffe 是什么</h2><p>Caffe 是以 C++/CUDA 代码为主，最早的深度学习框架之一，比 TensorFlow、Mxnet、Pytorch 等都更早，支持命令行、Python 和 Matlab 接口，单机多卡、多机多卡等都可以很方便的使用，CPU 和 GPU 之间无缝切换。</p><p>对于入门级别的任务，如图像分类，Caffe 上手的成本最低，几乎不需要写一行代码就可以开始训练，所以我推荐 Caffe 作为入门学习的框架。</p><p>Caffe 相对于 TensorFlow 等使用 pip 一键安装的方式来说，编译安装稍微麻烦一些，但其实依旧很简单，我们以 Ubuntu 16.04 为例子，官网的安装脚本足够用了，它有一些依赖库。</p><div class=\"highlight\"><pre><code class=\"language-text\">sudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libhdf5-serial-dev protobuf-compiler\nsudo apt-get install --no-install-recommends libboost-all-devsudo apt-get install libatlas-base-dev\nsudo apt-get install libgflags-dev libgoogle-glog-dev liblmdb-dev</code></pre></div><p>装完之后，到 <a href=\"https://link.zhihu.com/?target=https%3A//github.com/BVLC/caffe\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Git 上 clone 代码</a>，修改 Makefile.config 就可以进行编译安装，如果其中有任何问题，多 Google，还有什么问题，在读者圈留言吧。当然，对于有 GPU 的读者，还需要安装 cuda 以及 Nvidia 驱动。</p><h2>2 Caffe 训练</h2><p>Caffe 完成一个训练，必要准备以下资料：一个是 train.prototxt 作为网络配置文件，另一个是 solver.prototxt 作为优化参数配置文件，再一个是训练文件 list。</p><p>另外，在大多数情况下，需要一个预训练模型作为权重的初始化。</p><p>（1）准备网络配置文件</p><p>我们准备了一个 3*3 的卷积神经网络，它的 caffe.prototxt 文件是这样的：</p><div class=\"highlight\"><pre><code class=\"language-text\">name: &#34;mouth&#34;layer {\n  name: &#34;data&#34;  type: &#34;ImageData&#34;  top: &#34;data&#34;  top: &#34;clc-label&#34;  image_data_param {\n    source: &#34;all_shuffle_train.txt&#34;    batch_size: 96    shuffle: true\n  }\n  transform_param {\n    mean_value: 104.008    mean_value: 116.669    mean_value: 122.675    crop_size: 48    mirror: true\n  }\n  include: { phase: TRAIN}\n}\nlayer {\n  name: &#34;data&#34;  type: &#34;ImageData&#34;  top: &#34;data&#34;  top: &#34;clc-label&#34;  image_data_param {\n    source: &#34;all_shuffle_val.txt&#34;    batch_size: 30    shuffle: false\n  }\n  transform_param {\n    mean_value: 104.008    mean_value: 116.669    mean_value: 122.675    crop_size: 48    mirror: false\n  }\n  include: { phase: TEST}\n}\n\nlayer {\n  name: &#34;conv1&#34;  type: &#34;Convolution&#34;  bottom: &#34;data&#34;  top: &#34;conv1&#34;  param {\n    lr_mult: 1    decay_mult: 1  }\n  param {\n    lr_mult: 2    decay_mult: 0  }\n  convolution_param {\n    num_output: 12    pad: 1    kernel_size: 3    stride: 2    weight_filler {\n      type: &#34;xavier&#34;      std: 0.01    }\n    bias_filler {\n      type: &#34;constant&#34;      value: 0.2    }\n  }\n}layer {\n  name: &#34;relu1&#34;  type: &#34;ReLU&#34;  bottom: &#34;conv1&#34;  top: &#34;conv1&#34;}layer {\n  name: &#34;conv2&#34;  type: &#34;Convolution&#34;  bottom: &#34;conv1&#34;  top: &#34;conv2&#34;  param {\n    lr_mult: 1    decay_mult: 1  }\n  param {\n    lr_mult: 2    decay_mult: 0  }\n  convolution_param {\n    num_output: 20    kernel_size: 3    stride: 2    pad: 1    weight_filler {\n      type: &#34;xavier&#34;      std: 0.1    }\n    bias_filler {\n      type: &#34;constant&#34;      value: 0.2    }\n  }\n}layer {\n  name: &#34;relu2&#34;  type: &#34;ReLU&#34;  bottom: &#34;conv2&#34;  top: &#34;conv2&#34;}layer {\n  name: &#34;conv3&#34;  type: &#34;Convolution&#34;  bottom: &#34;conv2&#34;  top: &#34;conv3&#34;  param {\n    lr_mult: 1    decay_mult: 1  }\n  param {\n    lr_mult: 2    decay_mult: 0  }\n  convolution_param {\n    num_output: 40    kernel_size: 3    stride: 2    pad: 1    weight_filler {\n      type: &#34;xavier&#34;      std: 0.1    }\n    bias_filler {\n      type: &#34;constant&#34;      value: 0.2    }\n  }\n}layer {\n  name: &#34;relu3&#34;  type: &#34;ReLU&#34;  bottom: &#34;conv3&#34;  top: &#34;conv3&#34;}\nlayer {\n  name: &#34;ip1-mouth&#34;  type: &#34;InnerProduct&#34;  bottom: &#34;conv3&#34;  top: &#34;pool-mouth&#34;  param {\n    lr_mult: 1    decay_mult: 1  }\n  param {\n    lr_mult: 2    decay_mult: 0  }\n  inner_product_param {\n    num_output: 128    weight_filler {\n      type: &#34;xavier&#34;    }\n    bias_filler {\n      type: &#34;constant&#34;      value: 0    }\n  }\n}\n\nlayer {\n    bottom: &#34;pool-mouth&#34;    top: &#34;fc-mouth&#34;    name: &#34;fc-mouth&#34;    type: &#34;InnerProduct&#34;    param {\n        lr_mult: 1        decay_mult: 1    }\n    param {\n        lr_mult: 2        decay_mult: 1    }\n    inner_product_param {\n        num_output: 2        weight_filler {\n            type: &#34;xavier&#34;        }\n        bias_filler {\n            type: &#34;constant&#34;            value: 0        }\n    }\n}\nlayer {\n    bottom: &#34;fc-mouth&#34;    bottom: &#34;clc-label&#34;    name: &#34;loss&#34;    type: &#34;SoftmaxWithLoss&#34;    top: &#34;loss&#34;}\nlayer {\n    bottom: &#34;fc-mouth&#34;    bottom: &#34;clc-label&#34;    top: &#34;acc&#34;    name: &#34;acc&#34;    type: &#34;Accuracy&#34;    include {\n        phase: TRAIN\n    }\n    include {\n        phase: TEST\n    }\n}</code></pre></div><p>可以看出，Caffe 的这个网络配置文件，每一个卷积层，都是以 layer{} 的形式定义，layer 的bottom、top 就是它的输入输出，type 就是它的类型，有的是数据层、有的是卷积层、有的是 loss 层。</p><p>我们采用 netscope 来可视化一下这个模型，<a href=\"https://link.zhihu.com/?target=http%3A//ethereon.github.io/netscope/%23/editor\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">网址详见这里</a>。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-e2313c38d48f1182882b1e6a87b384a6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"424\" data-rawheight=\"710\" class=\"origin_image zh-lightbox-thumb\" width=\"424\" data-original=\"https://pic3.zhimg.com/v2-e2313c38d48f1182882b1e6a87b384a6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;424&#39; height=&#39;710&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"424\" data-rawheight=\"710\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"424\" data-original=\"https://pic3.zhimg.com/v2-e2313c38d48f1182882b1e6a87b384a6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-e2313c38d48f1182882b1e6a87b384a6_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>从上面看很直观的看到，网络的输入层是 data 层，后面接了3个卷积层，其中每一个卷积层都后接了一个 relu 层，最后 ip1-mouth、fc-mouth 是全连接层。Loss 和 acc 分别是计算 loss 和 acc 的层。</p><p>各层的配置有一些参数，比如 conv1 有卷积核的学习率、卷积核的大小、输出通道数、初始化方法等，这些可以后续详细了解。</p><p>（2）准备训练 list</p><p>我们看上面的 data layer，可以看到 <code>image_data_param</code> 里面，有</p><div class=\"highlight\"><pre><code class=\"language-text\">source: &#34;all_shuffle_train.txt&#34;</code></pre></div><p>它是什么呢，就是输入用于训练的 list，它的内容是这样的：</p><div class=\"highlight\"><pre><code class=\"language-text\">../../../../datas/mouth/1/182smile.jpg 1../../../../datas/mouth/1/435smile.jpg 1../../../../datas/mouth/0/40neutral.jpg 0../../../../datas/mouth/1/206smile.jpg 1../../../../datas/mouth/0/458neutral.jpg 0../../../../datas/mouth/0/158neutral.jpg 0../../../../datas/mouth/1/322smile.jpg 1../../../../datas/mouth/1/83smile.jpg 1../../../../datas/mouth/0/403neutral.jpg 0../../../../datas/mouth/1/425smile.jpg 1../../../../datas/mouth/1/180smile.jpg 1../../../../datas/mouth/1/233smile.jpg 1../../../../datas/mouth/1/213smile.jpg 1../../../../datas/mouth/1/144smile.jpg 1../../../../datas/mouth/0/327neutral.jpg 0</code></pre></div><p>格式就是，图片的名字 + 空格 + label，这就是 Caffe 用于图片分类，默认的输入格式。</p><p>（3）准备优化配置文件：</p><div class=\"highlight\"><pre><code class=\"language-text\">net: &#34;./train.prototxt&#34;test_iter: 100test_interval: 10base_lr: 0.00001momentum: 0.9type: &#34;Adam&#34;lr_policy: &#34;fixed&#34;display: 100max_iter: 10000snapshot: 2000snapshot_prefix: &#34;./snaps/conv3_finetune&#34;solver_mode: GPU</code></pre></div><p>介绍一下上面的参数。</p><p>net 是网络的配置路径：</p><ul><li><code>Test_interval</code>是指训练迭代多少次之后，进行一次测试。<code>test_iter</code>是测试多少个batch，如果它等于 1，就说明只取一个 batchsize 的数据来做测试，如果 batchsize 太小，那么对于分类任务来说统计出来的指标也不可信，所以最好一次测试，用到所有测试数据。因为，常令<code>test_iter*test_batchsize=</code>测试集合的大小。</li><li><code>base_lr</code>、<code>momentum</code>、<code>type</code>、<code>lr_policy</code>是和学习率有关的参数，<code>base_lr</code>和<code>lr_policy</code>决定了学习率大小如何变化。type 是优化的方法，以后再谈。<code>max_iter</code>是最大的迭代次数，snapshot 是每迭代多少次之后存储迭代结果，<code>snapshot_prefix</code>为存储结果的目录，caffe 存储的模型后缀是 .caffemodel。<code>solver_mode</code>可以指定用 GPU 或者 CPU 进行训练。</li></ul><p>（4）训练与结果可视化</p><p>我们利用 C++ 的接口进行训练，命令如下：</p><div class=\"highlight\"><pre><code class=\"language-text\">SOLVER=./solver.prototxt\nWEIGHTS=./init.caffemodel\n../../../../libs/Caffe_Long/build/tools/caffe train -solver $SOLVER -weights $WEIGHTS -gpu 0 2&gt;&amp;1 | tee log.txt</code></pre></div><p>其中，caffe train 就是指定训练。</p><p>我们可以利用脚本可视化一下训练结果，具体参考git项目：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-e4a81903206bdc4347d03c9abee8c684_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"607\" data-rawheight=\"473\" class=\"origin_image zh-lightbox-thumb\" width=\"607\" data-original=\"https://pic1.zhimg.com/v2-e4a81903206bdc4347d03c9abee8c684_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;607&#39; height=&#39;473&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"607\" data-rawheight=\"473\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"607\" data-original=\"https://pic1.zhimg.com/v2-e4a81903206bdc4347d03c9abee8c684_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-e4a81903206bdc4347d03c9abee8c684_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-b60895f914d9868e4551d214dd249626_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"591\" data-rawheight=\"461\" class=\"origin_image zh-lightbox-thumb\" width=\"591\" data-original=\"https://pic3.zhimg.com/v2-b60895f914d9868e4551d214dd249626_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;591&#39; height=&#39;461&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"591\" data-rawheight=\"461\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"591\" data-original=\"https://pic3.zhimg.com/v2-b60895f914d9868e4551d214dd249626_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-b60895f914d9868e4551d214dd249626_b.jpg\"/></figure><h2>3 Caffe 测试</h2><p>上面我们得到了训练结果，下面开始采用自己的图片进行测试。</p><ul><li>train.prototxt 与 test.prototxt 的区别</li></ul><p>训练时的网络配置与测试时的网络配置是不同的，测试没有 acc 层，也没有 loss 层，取输出的 softmax 就是分类的结果。同时，输入层的格式也有出入，不需要再输入 label，也不需要指定图片 list，但是要指定输入尺度，我们看一下 test.prototxt 和可视化结果。</p><div class=\"highlight\"><pre><code class=\"language-text\">name: &#34;mouth&#34;layer {\n  name: &#34;data&#34;  type: &#34;Input&#34;  top: &#34;data&#34;  input_param { shape: { dim: 1 dim: 3 dim: 48 dim: 48 } }\n}\nlayer {\n  name: &#34;conv1&#34;  type: &#34;Convolution&#34;  bottom: &#34;data&#34;  top: &#34;conv1&#34;  param {\n    lr_mult: 1    decay_mult: 1  }\n  param {\n    lr_mult: 2    decay_mult: 0  }\n  convolution_param {\n    num_output: 12    pad: 1    kernel_size: 3    stride: 2    weight_filler {\n      type: &#34;xavier&#34;      std: 0.01    }\n    bias_filler {\n      type: &#34;constant&#34;      value: 0.2    }\n  }\n}layer {\n  name: &#34;relu1&#34;  type: &#34;ReLU&#34;  bottom: &#34;conv1&#34;  top: &#34;conv1&#34;}layer {\n  name: &#34;conv2&#34;  type: &#34;Convolution&#34;  bottom: &#34;conv1&#34;  top: &#34;conv2&#34;  param {\n    lr_mult: 1    decay_mult: 1  }\n  param {\n    lr_mult: 2    decay_mult: 0  }\n  convolution_param {\n    num_output: 20    kernel_size: 3    stride: 2    pad: 1    weight_filler {\n      type: &#34;xavier&#34;      std: 0.1    }\n    bias_filler {\n      type: &#34;constant&#34;      value: 0.2    }\n  }\n}layer {\n  name: &#34;relu2&#34;  type: &#34;ReLU&#34;  bottom: &#34;conv2&#34;  top: &#34;conv2&#34;}layer {\n  name: &#34;conv3&#34;  type: &#34;Convolution&#34;  bottom: &#34;conv2&#34;  top: &#34;conv3&#34;  param {\n    lr_mult: 1    decay_mult: 1  }\n  param {\n    lr_mult: 2    decay_mult: 0  }\n  convolution_param {\n    num_output: 40    kernel_size: 3    stride: 2    pad: 1    weight_filler {\n      type: &#34;xavier&#34;      std: 0.1    }\n    bias_filler {\n      type: &#34;constant&#34;      value: 0.2    }\n  }\n}layer {\n  name: &#34;relu3&#34;  type: &#34;ReLU&#34;  bottom: &#34;conv3&#34;  top: &#34;conv3&#34;}\nlayer {\n  name: &#34;ip1-mouth&#34;  type: &#34;InnerProduct&#34;  bottom: &#34;conv3&#34;  top: &#34;pool-mouth&#34;  param {\n    lr_mult: 1    decay_mult: 1  }\n  param {\n    lr_mult: 2    decay_mult: 0  }\n  inner_product_param {\n    num_output: 128    weight_filler {\n      type: &#34;xavier&#34;    }\n    bias_filler {\n      type: &#34;constant&#34;      value: 0    }\n  }\n}\nlayer {\n    bottom: &#34;pool-mouth&#34;    top: &#34;fc-mouth&#34;    name: &#34;fc-mouth&#34;    type: &#34;InnerProduct&#34;    param {\n        lr_mult: 1        decay_mult: 1    }\n    param {\n        lr_mult: 2        decay_mult: 1    }\n    inner_product_param {\n        num_output: 2        weight_filler {\n            type: &#34;xavier&#34;        }\n        bias_filler {\n            type: &#34;constant&#34;            value: 0        }\n    }\n}layer {\n    bottom: &#34;fc-mouth&#34;    name: &#34;loss&#34;    type: &#34;Softmax&#34;    top: &#34;prob&#34;}</code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-7273741cf28dcb09b59b8669f44f36f9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"422\" data-rawheight=\"650\" class=\"origin_image zh-lightbox-thumb\" width=\"422\" data-original=\"https://pic2.zhimg.com/v2-7273741cf28dcb09b59b8669f44f36f9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;422&#39; height=&#39;650&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"422\" data-rawheight=\"650\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"422\" data-original=\"https://pic2.zhimg.com/v2-7273741cf28dcb09b59b8669f44f36f9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-7273741cf28dcb09b59b8669f44f36f9_b.jpg\"/></figure><ul><li>使用 Python 进行测试</li></ul><p>由于 Python 目前广泛使用，下面使用 Python 来进行测试，它要做的就是导入模型、导入图片、输出结果。</p><p>下面是所有的代码，我们详细解释下：</p><div class=\"highlight\"><pre><code class=\"language-text\">---代码段1，这一段，我导入一些基本库，同时导入caffe的路径---#_*_ coding:utf8import sys\nsys.path.insert(0, &#39;../../../../../libs/Caffe_Long/python/&#39;)import caffeimport os,shutilimport numpy as npfrom PIL import Image as PILImagefrom PIL import ImageMathimport matplotlib.pyplot as pltimport timeimport cv2\n\n---代码段2，这一段，我们添加一个参数解释器，方便参数管理---\ndebug=Trueimport argparsedef parse_args():   parser = argparse.ArgumentParser(description=&#39;test resnet model for portrait segmentation&#39;)\n   parser.add_argument(&#39;--model&#39;, dest=&#39;model_proto&#39;, help=&#39;the model&#39;, default=&#39;test.prototxt&#39;, type=str)\n   parser.add_argument(&#39;--weights&#39;, dest=&#39;model_weight&#39;, help=&#39;the weights&#39;, default=&#39;./test.caffemodel&#39;, type=str)\n   parser.add_argument(&#39;--testsize&#39;, dest=&#39;testsize&#39;, help=&#39;inference size&#39;, default=60,type=int)\n   parser.add_argument(&#39;--src&#39;, dest=&#39;img_folder&#39;, help=&#39;the src image folder&#39;, type=str, default=&#39;./&#39;)\n   parser.add_argument(&#39;--gt&#39;, dest=&#39;gt&#39;, help=&#39;the gt&#39;, type=int, default=0)\n   args = parser.parse_args()\n   return args\ndef start_test(model_proto,model_weight,img_folder,testsize):---代码段3，这一段，我们就完成了网络的初始化---\n   caffe.set_device(0)\n   #caffe.set_mode_cpu()   net = caffe.Net(model_proto, model_weight, caffe.TEST)\n   imgs = os.listdir(img_folder)\n\n   pos = 0   neg = 0\n   for imgname in imgs:\n---代码段4，这一段，是读取图片并进行预处理，还记得我们之前的训练，是采用 BGR 的输入格式，减去了图像均值吧，同时，输入网络的图像，也需要 resize 到相应尺度。预处理是通过 caffe 的类，transformer 来完成，set_mean 完成均值，set_transpose 完成维度的替换，因为 caffe blob 的格式是 batch、channel、height、width，而 numpy 图像的维度是 height、width、channel 的顺序---\n\n      imgtype = imgname.split(&#39;.&#39;)[-1]\n      imgid = imgname.split(&#39;.&#39;)[0]\n      if imgtype != &#39;png&#39; and imgtype != &#39;jpg&#39; and imgtype != &#39;JPG&#39; and imgtype != &#39;jpeg&#39; and imgtype != &#39;tif&#39; and imgtype != &#39;bmp&#39;:\n          print imgtype,&#34;error&#34;          continue      imgpath = os.path.join(img_folder,imgname)\n\n      img = cv2.imread(imgpath)\n      if img is None:\n          print &#34;---------img is empty---------&#34;,imgpath\n          continue\n      img = cv2.resize(img,(testsize,testsize))\n\n      transformer = caffe.io.Transformer({&#39;data&#39;: net.blobs[&#39;data&#39;].data.shape})\n      transformer.set_mean(&#39;data&#39;, np.array([104.008,116.669,122.675]))\n      transformer.set_transpose(&#39;data&#39;, (2,0,1))\n\n---代码段5，这一段，就得到了输出结果了，并做一些可视化显示---\n      out = net.forward_all(data=np.asarray([transformer.preprocess(&#39;data&#39;, img)]))\n\n      result = out[&#39;prob&#39;][0]\n      print &#34;---------result prob---------&#34;,result,&#34;-------result size--------&#34;,result.shape\n      probneutral = result[0]\n      print &#34;prob neutral&#34;,probneutral \n\n      probsmile = result[1]\n      print &#34;prob smile&#34;,probsmile\n      problabel = -1      probstr = &#39;none&#39;      if probneutral &gt; probsmile:\n          probstr = &#34;neutral:&#34;+str(probneutral)\n          pos = pos + 1      else:\n          probstr = &#34;smile:&#34;+str(probsmile)\n          neg = neg + 1\n      if debug:\n         showimg = cv2.resize(img,(256,256))\n         cv2.putText(showimg,probstr,(30,50),cv2.FONT_HERSHEY_SIMPLEX,1,(0,0,255),1)\n         cv2.imshow(&#34;test&#34;,showimg)\n         k = cv2.waitKey(0)\n         if k == ord(&#39;q&#39;):\n             break\n   print &#34;pos=&#34;,pos \n   print &#34;neg=&#34;,neg \nif __name__ == &#39;__main__&#39;:\n    args = parse_args()\n    start_test(args.model_proto,args.model_weight,args.img_folder,args.testsize)</code></pre></div><ul><li>Caffe 代码阅读</li></ul><p>经过前面的介绍，我们已经学会了 Caffe 的基本使用，但是我们不能停留于此。Caffe 是一个非常优秀的开源框架，有必要去细读它的源代码。</p><p>至于怎么读 Caffe 的代码，建议阅读我写的Caffe 代码阅读系列内容，以及我的 AI 公众号；或者可参考其他资料。</p><p><b>总结</b>：虽然现在很多人没有从 Caffe 开始学，但是希望提升自己 C++ 水平和更深刻理解深度学习中的一些源码的，建议从 Caffe 开始学起。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-60feffab1842911ce09ae93eeaae724f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"3999\" data-rawheight=\"2250\" class=\"origin_image zh-lightbox-thumb\" width=\"3999\" data-original=\"https://pic4.zhimg.com/v2-60feffab1842911ce09ae93eeaae724f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;3999&#39; height=&#39;2250&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"3999\" data-rawheight=\"2250\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"3999\" data-original=\"https://pic4.zhimg.com/v2-60feffab1842911ce09ae93eeaae724f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-60feffab1842911ce09ae93eeaae724f_b.jpg\"/></figure><blockquote>本系列完整文章：</blockquote><p>第一篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029846%26idx%3D1%26sn%3D0c343cfd0ede5c8ae1405bd6348aefad%26chksm%3D871342abb064cbbd7fe31fb3c55f23875f27e48fb8354e9855823b1701f1227c71b4eb00de50%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【caffe速成】caffe图像分类从模型自定义到测试</a></p><p>第二篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029846%26idx%3D2%26sn%3D7c2582243bcd8f8b491e8e466a21978f%26chksm%3D871342abb064cbbd0cba24b408ceda2b64a7c8b6baa07f9f8f56cd4d1233caa0b80fe357753e%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【tensorflow速成】Tensorflow图像分类从模型自定义到测试</a></p><p>第三篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029881%26idx%3D1%26sn%3D3c869fcee3b48d3582952ab9a0683ea6%26chksm%3D87134284b064cb924c5e7231b3f2c36ba27e3a689b067f569f2e086f62b18413bcebc5987a07%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【pytorch速成】Pytorch图像分类从模型自定义到测试</a></p><p>第四篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029887%26idx%3D1%26sn%3D645b97809c24922352a0b39f19c9ef0c%26chksm%3D87134282b064cb9441af68124d205d9c7dedcaeb09788f4d586b949584e556eddd3a72217f69%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【paddlepaddle速成】paddlepaddle图像分类从模型自定义到测试</a></p><p>第五篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029896%26idx%3D1%26sn%3Df3f7b9cf69c514f45d1d14205f879270%26chksm%3D87134375b064ca6323354c40f3e55b02ff0d1d24f3dacfc980190d51f20ec9ec16e60c1a4741%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【Keras速成】Keras图像分类从模型自定义到测试</a></p><p>第六篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029904%26idx%3D1%26sn%3D0bdc6947f5ac68e7f68426b9d076b4ab%26chksm%3D8713436db064ca7b3b2a2a1d6a8d24c15069f72655e2c39e2498051fa0e56bbcf6fe9c332d5b%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【mxnet速成】mxnet图像分类从模型自定义到测试</a></p><p>第七篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649030976%26idx%3D1%26sn%3D0befc170a93d365b780c5f05b2f510a4%26chksm%3D8712bf3db065362b0aeaae82bdbac467be5697b34cac2797e2ee15cdcb97474c08640482bf07%26token%3D1695328272%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【cntk速成】cntk图像分类从模型自定义到测试</a></p><p>第八篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26tempkey%3DMTAwMF93Q3VDbklva2NBZDJXV1dnNWloZjJ2YWtZWmJ2aTRRaGVDMVNfbERSN2xVYXdXd3B4eWczWWdMbnQzRjZrRVpmSGRYMndrLTQ0VnhPcURTNGhsS3dYWGd3MTk0UGtmZkltU0U2U01OUmRfQzcySDVObDlxM3R2U3Q0SlNQWGRMc1NZeWtWTlVRN1NpRlJaWXRYcGdydUNOTUR6clUzVkNleVlKUE5nfn4%253D%26chksm%3D0712ba9b3065338de13ea030951ba2c25c0363d958c905e12c937c2acffa991313abb4d9f751%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【chainer速成】chainer图像分类从模型自定义到测试</a></p><p>第九篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649032012%26idx%3D1%26sn%3Df74c7084621f367adb2518ebec61ca42%26chksm%3D8712bb31b06532270b9ca9550ab48ff78adaad7d38c73645cfb8888218b8e177bebd5d401aa9%26token%3D1258619827%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【DL4J速成】Deeplearning4j图像分类从模型自定义到测试</a></p><p>第十篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649032109%26idx%3D2%26sn%3Da6ff48ec0ae5d8e7a494df7e564d9ac9%26chksm%3D8712bbd0b06532c61d98c786ba1773c29c3dcf1291f9c3cd052c5643e24699eef28481e94b8e%26token%3D1258619827%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【MatConvnet速成】MatConvnet图像分类从模型自定义到测试</a></p><p>第十一篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649032128%26idx%3D2%26sn%3Df889e2255c0ec4960f76ad0383363849%26chksm%3D8712bbbdb06532ab136c0a485bebc5cf181f24a34bec7bab3f97f66be627e09d939b997dae90%26token%3D598159941%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【Lasagne速成】Lasagne/Theano图像分类从模型自定义到测试</a></p><p>第十二篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649032137%26idx%3D2%26sn%3Da9770ead4a9d03f0a4e75be86657453b%26chksm%3D8712bbb4b06532a27f0f02ac30ce4a18eeac1f72ac54b3c5c1b6ec13fee29f03bfc0dcaa1efd%26token%3D1446627305%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【darknet速成】Darknet图像分类从模型自定义到测试</a></p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "开源", 
                    "tagLink": "https://api.zhihu.com/topics/19562746"
                }, 
                {
                    "tag": "Caffe（深度学习框架）", 
                    "tagLink": "https://api.zhihu.com/topics/20019488"
                }
            ], 
            "comments": [
                {
                    "userName": "小熊先生的猫", 
                    "userLink": "https://www.zhihu.com/people/e9d4ddb81885a82af6945931b7e545e9", 
                    "content": "<p>您好，很感谢您的分享，我有一个疑问，prototxt有 lr_mult和没有的区别在哪？因为我看一个vgg16,places365的vgg16就有，而faster rcnn的vgg16没有</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "言有三-龙鹏", 
                            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
                            "content": "这是一个控制学习率乘因子的变量，默认等于1", 
                            "likes": 0, 
                            "replyToAuthor": "小熊先生的猫"
                        }, 
                        {
                            "userName": "小熊先生的猫", 
                            "userLink": "https://www.zhihu.com/people/e9d4ddb81885a82af6945931b7e545e9", 
                            "content": "<p>谢谢您的解答。</p><p>那么第一个name: \"VGG-Places365\"<br>input: \"data\"<br>input_dim: 10<br>input_dim: 3<br>input_dim: 224<br>input_dim: 224<br>layer {<br>  name: \"conv1_1\"<br>  type: \"Convolution\"<br>  bottom: \"data\"<br>  top: \"conv1_1\"<br>  param {<br>    lr_mult: 1.0<br>    decay_mult: 1.0<br>  }<br>  param {<br>    lr_mult: 2.0<br>    decay_mult: 0.0<br>  }<br>  convolution_param {<br>    num_output: 64<br>    pad: 1<br>    kernel_size: 3<br>    weight_filler {<br>      type: \"gaussian\"<br>      std: 0.01<br>    }<br>    bias_filler {<br>      type: \"constant\"<br>      value: 0.0<br>    }<br>  }<br>}</p><p>第二个name: \"VGG_ILSVRC_16_layers\"<br>input: \"data\"<br>input_dim: 10<br>input_dim: 3<br>input_dim: 224<br>input_dim: 224<br>layer {<br>  name: \"conv1_1\"<br>  type: \"Convolution\"<br>  bottom: \"data\"<br>  top: \"conv1_1a\"<br>  convolution_param {<br>    num_output: 64<br>    pad: 1<br>    kernel_size: 3<br>  }<br>}</p><p>是否有本质区别呢？个人理解为初始化方式不一样。因此是否可以认为是不同版本的vgg16？万分感谢</p>", 
                            "likes": 0, 
                            "replyToAuthor": "言有三-龙鹏"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/39455807", 
            "userName": "言有三-龙鹏", 
            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
            "upvote": 15, 
            "title": "【开源框架】Pytorch图像分类从模型自定义到测试", 
            "content": "<p>前面已跟大家介绍了 Caffe 和 TensorFlow，现在说说 Pytorch，集齐三大主流框架以后方便召唤模型。</p><h2>1 什么是 Pytorch</h2><p>一句话总结 Pytorch = Python + Torch。</p><p>Torch 是纽约大学的一个机器学习开源框架，几年前在学术界非常流行，包括 Lecun 等大佬都在使用。但是由于使用的是一种绝大部分人绝对没有听过的 Lua 语言，导致很多人都被吓退。后来随着 Python 的生态越来越完善，Facebook 人工智能研究院推出了 Pytorch 并开源。Pytorch 不是简单的封装 Torch 并提供 Python 接口，而是对 Tensor 以上的所有代码进行了重构，同 TensorFlow 一样，增加了自动求导。</p><p>后来 Caffe2 全部并入 Pytorch，如今已经成为了非常流行的框架。很多最新的研究如风格化、GAN 等大多数采用 Pytorch 源码，这也是我们必须要讲解它的原因。</p><p><b>Pytorch 有什么特点呢</b></p><p>（1）动态图计算。前面说过，TensorFlow 是采用静态图的，先定义好图，然后在 session 中运算。图一旦定义好后，是不能随意修改的。当然了，现在 TensorFlow 也引入了动态图机制 Eager Execution，只是不如 Pytorch 直观。那动态图有什么好处呢？TensorFlow 要查看变量结果，必须在 sess 中，sess 的角色是 C 语言的执行，而之前的图定义是编译。而 Pytorch 就好像是脚本语言，随时随地修改，随处 debug，没有一个类似编译的过程，这比 TensorFlow 要灵活很多。</p><p>（2）简单。TensorFlow 的学习成本真的不低，对于新手来说，Tensor、Variable、Session 等概念充斥，数据读取接口频繁更新，tf.nn、tf.layers、tf.contrib 各自重复，Pytorch 则是从 Tensor 到 Variable 再到 nn.Module，分别就是从数据张量到网络的抽象层次的递进。有人调侃 TensorFlow 的设计是“make it complicated”，那么 Pytorch 的设计就是“keep it simple”。</p><p><b>Pytorch 重要概念</b></p><p>（1）Tensor</p><p>这几大框架都有基本的数据结构，Caffe 是 blob，TensorFlow 和 Pytorch 都是 Tensor，都是高维数组。Pytorch 中的 Tensor 使用与 Numpy 的数组非常相似，两者可以互转且共享内存。</p><p>（2）Variable</p><p>Variable 封装 Tensor，然后提供反向传播，自动计算梯度。Variable 实际上包含3个属性，data，即 Tensor 内容；Grad，是与 data 对应的梯度；grad_fn，计算反向传播的函数。当然了，就在不到一个月前Pytorch 0.4.0已经合并了Variable和Tensor。</p><p>（3）Nn.module</p><p>抽象好的网络数据结构，可以表示为网络的一层，也可以表示为一个网络结构。在实际使用过程中，经常会定义自己的网络，并继承 nn.Module。具体的使用，我们看下面的网络定义吧。</p><h2>2 Pytorch 训练</h2><p>安装咱们就不说了，接下来的任务就是开始训练模型。训练模型包括数据准备、模型定义、结果保存与分析。</p><p><b>2.1 数据准备与读取</b></p><p>前面已经介绍了 Caffe 和 TensorFlow 的数据读取，两者的输入都是图片 list，但是读取操作过程差异非常大，Pytorch 与这两个又有很大的差异。这一次，直接利用文件夹作为输入，这是 Pytorch 更加方便的做法。</p><p>数据读取的完整代码如下：</p><div class=\"highlight\"><pre><code class=\"language-text\">    data_dir = &#39;../../../../datas/head/&#39;    \n    data_transforms = {\n        &#39;train&#39;: transforms.Compose([\n            transforms.RandomSizedCrop(48),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5])\n        ]),\n        &#39;val&#39;: transforms.Compose([\n            transforms.Scale(64),\n            transforms.CenterCrop(48),\n            transforms.ToTensor(),\n            transforms.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5])\n        ]),\n    }\n    image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n                                              data_transforms[x]) for x in [&#39;train&#39;, &#39;val&#39;]}\n    dataloders = {x: torch.utils.data.DataLoader(image_datasets[x],\n                                                 batch_size=16,\n                                                 shuffle=True,\n                                                 num_workers=4) for x in [&#39;train&#39;, &#39;val&#39;]}</code></pre></div><p>下面一个一个解释，完整代码请移步 Git 工程。</p><p>（1）datasets.ImageFolder</p><p>Pytorch 的 torchvision 模块中提供了一个 dataset 包，它包含了一些基本的数据集如 mnist、coco、imagenet 和一个通用的数据加载器 ImageFolder，可以制定train和val目录，而其中的每一个目录则分为不同类别的文件夹。</p><p>具体的请到 Git 工程中查看。</p><div class=\"highlight\"><pre><code class=\"language-text\">├── train\n│   ├── 0\n│   ├── 1\n└── val\n    ├── 0\n    ├── 1</code></pre></div><p>imagefolder 有3个成员变量。</p><ul><li><code>self.classes</code>：用一个 list 保存类名，就是文件夹的名字。</li><li><code>self.class_to_idx</code>：类名对应的索引，可以理解为 0、1、2、3 等。</li><li><code>self.imgs</code>：保存（imgpath，class），是图片和类别的数组。</li></ul><p>不同文件夹下的图，会被当作不同的类，天生就用于图像分类任务。</p><p>（2）Transforms</p><p>这一点跟 Caffe 非常类似，就是定义了一系列数据集的预处理和增强操作。到此，数据接口就定义完毕了，接下来在训练代码中看如何使用迭代器进行数据读取就可以了，包括 scale、减均值等。</p><p>（3）torch.utils.data.DataLoader</p><p>这就是创建了一个 batch，生成真正网络的输入。关于更多 Pytorch 的数据读取方法，请移步知乎专栏和公众号，链接在前面的课程中有给出。</p><p><b>2.2 模型定义</b></p><p>如下：</p><div class=\"highlight\"><pre><code class=\"language-text\">    import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    import numpy as np\n    class simpleconv3(nn.Module):`\n    def __init__(self):\n        super(simpleconv3,self).__init__()\n        self.conv1 = nn.Conv2d(3, 12, 3, 2)\n        self.bn1 = nn.BatchNorm2d(12)\n        self.conv2 = nn.Conv2d(12, 24, 3, 2)\n        self.bn2 = nn.BatchNorm2d(24)\n        self.conv3 = nn.Conv2d(24, 48, 3, 2)\n        self.bn3 = nn.BatchNorm2d(48)\n        self.fc1 = nn.Linear(48 * 5 * 5 , 1200)\n        self.fc2 = nn.Linear(1200 , 128)\n        self.fc3 = nn.Linear(128 , 2)\n    def forward(self , x):\n        x = F.relu(self.bn1(self.conv1(x)))\n        #print &#34;bn1 shape&#34;,x.shape        \n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.relu(self.bn3(self.conv3(x)))\n        x = x.view(-1 , 48 * 5 * 5) \n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x</code></pre></div><p>这三节课的任务，都是采用一个简单的 3 层卷积 + 2 层全连接层的网络结构。根据上面的网络结构的定义，需要做以下事情。</p><p>（1）simpleconv3(nn.Module)</p><p>继承 nn.Module，前面已经说过，Pytorch 的网络层是包含在 nn.Module 里，所以所有的网络定义，都需要继承该网络层。</p><p>并实现 super 方法，如下：</p><div class=\"highlight\"><pre><code class=\"language-text\">super(simpleconv3,self).__init__()</code></pre></div><p>这个，就当作一个标准，执行就可以了。</p><p>（2）网络结构的定义</p><p>都在 nn 包里，举例说明：</p><div class=\"highlight\"><pre><code class=\"language-text\">torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)</code></pre></div><p>完整的接口如上，定义的第一个卷积层如下：</p><div class=\"highlight\"><pre><code class=\"language-text\">nn.Conv2d(3, 12, 3, 2)</code></pre></div><p>即输入通道为3，输出通道为12，卷积核大小为3，stride=2，其他的层就不一一介绍了，大家可以自己去看 nn 的 API。</p><p>（3）forward</p><p>backward 方法不需要自己实现，但是 forward 函数是必须要自己实现的，从上面可以看出，forward 函数也是非常简单，串接各个网络层就可以了。</p><p>对比 Caffe 和 TensorFlow 可以看出，Pytorch 的网络定义更加简单，初始化方法都没有显示出现，因为 Pytorch 已经提供了默认初始化。</p><p>如果我们想实现自己的初始化，可以这么做：</p><div class=\"highlight\"><pre><code class=\"language-text\">init.xavier_uniform(self.conv1.weight)init.constant(self.conv1.bias, 0.1)</code></pre></div><p>它会对 conv1 的权重和偏置进行初始化。如果要对所有 conv 层使用 xavier 初始化呢？可以定义一个函数：</p><div class=\"highlight\"><pre><code class=\"language-text\">def weights_init(m):    \n    if isinstance(m, nn.Conv2d):\n        xavier(m.weight.data)\n        xavier(m.bias.data)  \n    net = Net()   \n    net.apply(weights_init)</code></pre></div><h2>3 模型训练</h2><p>网络定义和数据加载都定义好之后，就可以进行训练了，老规矩先上代码：</p><div class=\"highlight\"><pre><code class=\"language-text\">    def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n        for epoch in range(num_epochs):\n            print(&#39;Epoch {}/{}&#39;.format(epoch, num_epochs - 1))\n            for phase in [&#39;train&#39;, &#39;val&#39;]:\n                if phase == &#39;train&#39;:\n                    scheduler.step()\n                    model.train(True)  \n                else:\n                    model.train(False)  \n                    running_loss = 0.0                running_corrects = 0.0\n                for data in dataloders[phase]:\n                    inputs, labels = data\n                    if use_gpu:\n                        inputs = Variable(inputs.cuda())\n                        labels = Variable(labels.cuda())\n                    else:\n                        inputs, labels = Variable(inputs), Variable(labels)\n\n                    optimizer.zero_grad()\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs.data, 1)\n                    loss = criterion(outputs, labels)\n                    if phase == &#39;train&#39;:\n                        loss.backward()\n                        optimizer.step()\n\n                    running_loss += loss.data.item()\n                    running_corrects += torch.sum(preds == labels).item()\n\n                epoch_loss = running_loss / dataset_sizes[phase]\n                epoch_acc = running_corrects / dataset_sizes[phase]\n\n                if phase == &#39;train&#39;:\n                    writer.add_scalar(&#39;data/trainloss&#39;, epoch_loss, epoch)\n                    writer.add_scalar(&#39;data/trainacc&#39;, epoch_acc, epoch)\n                else:\n                    writer.add_scalar(&#39;data/valloss&#39;, epoch_loss, epoch)\n                    writer.add_scalar(&#39;data/valacc&#39;, epoch_acc, epoch)\n\n                print(&#39;{} Loss: {:.4f} Acc: {:.4f}&#39;.format(\n                phase, epoch_loss, epoch_acc))\n        writer.export_scalars_to_json(&#34;./all_scalars.json&#34;)\n        writer.close()\n        return model\n</code></pre></div><p>分析一下上面的代码，外层循环是 epoches，然后利用 for data in dataloders[phase] 循环取一个 epoch 的数据，并塞入 variable，送入 model。需要注意的是，每一次 forward 要将梯度清零，即 <code>optimizer.zero_grad()</code>，因为梯度会记录前一次的状态，然后计算 loss，反向传播。</p><div class=\"highlight\"><pre><code class=\"language-text\">loss.backward()\noptimizer.step()</code></pre></div><p>下面可以分别得到预测结果和 loss，每一次 epoch 完成计算。</p><div class=\"highlight\"><pre><code class=\"language-text\">epoch_loss = running_loss / dataset_sizes[phase]\nepoch_acc = running_corrects / dataset_sizes[phase]\n      \n_, preds = torch.max(outputs.data, 1)\nloss = criterion(outputs, labels)</code></pre></div><h2><b>4 可视化</b></h2><p>可视化是非常重要的，鉴于 TensorFlow 的可视化非常方便，我们选择了一个开源工具包，tensorboardx，安装方法为 pip install tensorboardx，使用非常简单。</p><p>第一步，引入包定义创建：</p><div class=\"highlight\"><pre><code class=\"language-text\">from tensorboardX import SummaryWriter\nwriter = SummaryWriter()</code></pre></div><p>第二步，记录变量，如 train 阶段的 loss，<code>writer.add_scalar(&#39;data/trainloss&#39;, epoch_loss, epoch)</code>。</p><p>按照以上操作就完成了，完整代码可以看上面的 Git 项目，我们看看训练中的记录。Loss 和 acc 的曲线图如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-4fe0222b447401110dc9b67b1a095edf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"812\" data-rawheight=\"287\" class=\"origin_image zh-lightbox-thumb\" width=\"812\" data-original=\"https://pic4.zhimg.com/v2-4fe0222b447401110dc9b67b1a095edf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;812&#39; height=&#39;287&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"812\" data-rawheight=\"287\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"812\" data-original=\"https://pic4.zhimg.com/v2-4fe0222b447401110dc9b67b1a095edf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-4fe0222b447401110dc9b67b1a095edf_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>网络的收敛没有 Caffe 和 TensorFlow 好，大家可以自己去调试调试参数了，随便折腾吧。</p><h2>5 Pytorch 测试</h2><p>上面已经训练好了模型，接下来的目标就是要用它来做 inference 了，同样，给出代码。</p><div class=\"highlight\"><pre><code class=\"language-text\">    import torch\n    import torch.nn as nn\n    import torch.optim as optim\n    from torch.optim import lr_scheduler\n    from torch.autograd import Variable\n    import torchvision\n    from torchvision import datasets, models, transforms\n    import time\n    import os\n    from PIL import Image\n    import sys\n    import torch.nn.functional as F\n\n    from net import simpleconv3\n    data_transforms =  transforms.Compose([\n                transforms.Resize(48),\n                transforms.ToTensor(),\n                transforms.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5])])\n\n    net = simpleconv3()\n    modelpath = sys.argv[1]\n    net.load_state_dict(torch.load(modelpath,map_location=lambda storage,loc: storage))\n\n    imagepath = sys.argv[2]\n    image = Image.open(imagepath)\n    imgblob = data_transforms(image).unsqueeze(0)\n    imgblob = Variable(imgblob)\n\n    torch.no_grad()\n\n    predict = F.softmax(net(imgblob))\n    print(predict)</code></pre></div><p>从上面的代码可知，做了几件事：</p><ul><li>定义网络并使用 <code>torch.load</code> 和<code>load_state_dict</code>载入模型。</li><li>用 PIL 的 Image 包读取图片，这里没有用 OpenCV，因为 Pytorch 默认的图片读取工具就是 PIL 的 Image，它会将图片按照 RGB 的格式，归一化到 0～1 之间。读取图片之后，必须转化为 Variable 变量。</li><li>evaluation 的时候，必须设置<code>torch.no_grad()</code>，然后就可以调用 softmax 函数得到结果了。</li></ul><p>总结：本节讲了如何用 Pytorch 完成一个分类任务，并学习了可视化以及使用训练好的模型做测试。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-60feffab1842911ce09ae93eeaae724f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"3999\" data-rawheight=\"2250\" class=\"origin_image zh-lightbox-thumb\" width=\"3999\" data-original=\"https://pic4.zhimg.com/v2-60feffab1842911ce09ae93eeaae724f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;3999&#39; height=&#39;2250&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"3999\" data-rawheight=\"2250\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"3999\" data-original=\"https://pic4.zhimg.com/v2-60feffab1842911ce09ae93eeaae724f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-60feffab1842911ce09ae93eeaae724f_b.jpg\"/></figure><blockquote>本系列完整文章：</blockquote><p>第一篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029846%26idx%3D1%26sn%3D0c343cfd0ede5c8ae1405bd6348aefad%26chksm%3D871342abb064cbbd7fe31fb3c55f23875f27e48fb8354e9855823b1701f1227c71b4eb00de50%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【caffe速成】caffe图像分类从模型自定义到测试</a></p><p>第二篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029846%26idx%3D2%26sn%3D7c2582243bcd8f8b491e8e466a21978f%26chksm%3D871342abb064cbbd0cba24b408ceda2b64a7c8b6baa07f9f8f56cd4d1233caa0b80fe357753e%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【tensorflow速成】Tensorflow图像分类从模型自定义到测试</a></p><p>第三篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029881%26idx%3D1%26sn%3D3c869fcee3b48d3582952ab9a0683ea6%26chksm%3D87134284b064cb924c5e7231b3f2c36ba27e3a689b067f569f2e086f62b18413bcebc5987a07%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【pytorch速成】Pytorch图像分类从模型自定义到测试</a></p><p>第四篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029887%26idx%3D1%26sn%3D645b97809c24922352a0b39f19c9ef0c%26chksm%3D87134282b064cb9441af68124d205d9c7dedcaeb09788f4d586b949584e556eddd3a72217f69%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【paddlepaddle速成】paddlepaddle图像分类从模型自定义到测试</a></p><p>第五篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029896%26idx%3D1%26sn%3Df3f7b9cf69c514f45d1d14205f879270%26chksm%3D87134375b064ca6323354c40f3e55b02ff0d1d24f3dacfc980190d51f20ec9ec16e60c1a4741%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【Keras速成】Keras图像分类从模型自定义到测试</a></p><p>第六篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029904%26idx%3D1%26sn%3D0bdc6947f5ac68e7f68426b9d076b4ab%26chksm%3D8713436db064ca7b3b2a2a1d6a8d24c15069f72655e2c39e2498051fa0e56bbcf6fe9c332d5b%26token%3D1879088111%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【mxnet速成】mxnet图像分类从模型自定义到测试</a></p><p>第七篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649030976%26idx%3D1%26sn%3D0befc170a93d365b780c5f05b2f510a4%26chksm%3D8712bf3db065362b0aeaae82bdbac467be5697b34cac2797e2ee15cdcb97474c08640482bf07%26token%3D1695328272%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【cntk速成】cntk图像分类从模型自定义到测试</a></p><p>第八篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26tempkey%3DMTAwMF93Q3VDbklva2NBZDJXV1dnNWloZjJ2YWtZWmJ2aTRRaGVDMVNfbERSN2xVYXdXd3B4eWczWWdMbnQzRjZrRVpmSGRYMndrLTQ0VnhPcURTNGhsS3dYWGd3MTk0UGtmZkltU0U2U01OUmRfQzcySDVObDlxM3R2U3Q0SlNQWGRMc1NZeWtWTlVRN1NpRlJaWXRYcGdydUNOTUR6clUzVkNleVlKUE5nfn4%253D%26chksm%3D0712ba9b3065338de13ea030951ba2c25c0363d958c905e12c937c2acffa991313abb4d9f751%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【chainer速成】chainer图像分类从模型自定义到测试</a></p><p>第九篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649032012%26idx%3D1%26sn%3Df74c7084621f367adb2518ebec61ca42%26chksm%3D8712bb31b06532270b9ca9550ab48ff78adaad7d38c73645cfb8888218b8e177bebd5d401aa9%26token%3D1258619827%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【DL4J速成】Deeplearning4j图像分类从模型自定义到测试</a></p><p>第十篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649032109%26idx%3D2%26sn%3Da6ff48ec0ae5d8e7a494df7e564d9ac9%26chksm%3D8712bbd0b06532c61d98c786ba1773c29c3dcf1291f9c3cd052c5643e24699eef28481e94b8e%26token%3D1258619827%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【MatConvnet速成】MatConvnet图像分类从模型自定义到测试</a></p><p>第十一篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649032128%26idx%3D2%26sn%3Df889e2255c0ec4960f76ad0383363849%26chksm%3D8712bbbdb06532ab136c0a485bebc5cf181f24a34bec7bab3f97f66be627e09d939b997dae90%26token%3D598159941%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【Lasagne速成】Lasagne/Theano图像分类从模型自定义到测试</a></p><p>第十二篇：<a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649032137%26idx%3D2%26sn%3Da9770ead4a9d03f0a4e75be86657453b%26chksm%3D8712bbb4b06532a27f0f02ac30ce4a18eeac1f72ac54b3c5c1b6ec13fee29f03bfc0dcaa1efd%26token%3D1446627305%26lang%3Dzh_CN%23rd\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【darknet速成】Darknet图像分类从模型自定义到测试</a></p><p></p>", 
            "topic": [
                {
                    "tag": "PyTorch", 
                    "tagLink": "https://api.zhihu.com/topics/20075993"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": [
                {
                    "userName": "小小将", 
                    "userLink": "https://www.zhihu.com/people/fb31fbdb268e466a0928518a201696a9", 
                    "content": "新版本不是已经没有Variable的概念了？", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "言有三-龙鹏", 
                            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
                            "content": "对，已经合并了", 
                            "likes": 0, 
                            "replyToAuthor": "小小将"
                        }, 
                        {
                            "userName": "言有三-龙鹏", 
                            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
                            "content": "文中提了", 
                            "likes": 0, 
                            "replyToAuthor": "小小将"
                        }
                    ]
                }, 
                {
                    "userName": "墨白", 
                    "userLink": "https://www.zhihu.com/people/d053c578b4f9d81ade764100c9c2a2da", 
                    "content": "<p>请问用自己训练好的模型来做新的图片分类，是测试那一步吗？</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "言有三-龙鹏", 
                            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
                            "content": "是的", 
                            "likes": 0, 
                            "replyToAuthor": "墨白"
                        }
                    ]
                }, 
                {
                    "userName": "墨白", 
                    "userLink": "https://www.zhihu.com/people/d053c578b4f9d81ade764100c9c2a2da", 
                    "content": "<p>我还想问一下，from net import simpleconv3，这个net是您先放在目录下面吗？应该先要加载模型才出来？</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "言有三-龙鹏", 
                            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
                            "content": "就在目录下面的", 
                            "likes": 0, 
                            "replyToAuthor": "墨白"
                        }
                    ]
                }, 
                {
                    "userName": "Flander丶", 
                    "userLink": "https://www.zhihu.com/people/84edc655cd47432a3739733b3f77f7d8", 
                    "content": "<p>请问github地址是什么？</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "言有三-龙鹏", 
                            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
                            "content": "<a href=\"http://link.zhihu.com/?target=https%3A//github.com/longpeng2008/LongPeng_ML_Course\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/longpeng2008</span><span class=\"invisible\">/LongPeng_ML_Course</span><span class=\"ellipsis\"></span></a>", 
                            "likes": 0, 
                            "replyToAuthor": "Flander丶"
                        }
                    ]
                }, 
                {
                    "userName": "王二的石锅拌饭", 
                    "userLink": "https://www.zhihu.com/people/247726b78109b760cc18c1b04bcc4b11", 
                    "content": "你好，我在使用代码中的tensorboardX的时候在往runs文件写训练日志时，经常是训练到某个epoch后出现，出现文件已存在，无法写入，这是怎么回事?", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "王二的石锅拌饭", 
                    "userLink": "https://www.zhihu.com/people/247726b78109b760cc18c1b04bcc4b11", 
                    "content": "你好，我在使用代码中的tensorboardX的时候在往runs文件写训练日志时，经常是训练到某个epoch后出现，出现文件已存在，无法写入，这是怎么回事?", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "言有三-龙鹏", 
                            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
                            "content": "这个我倒是没有遇到过呢，不太清楚", 
                            "likes": 0, 
                            "replyToAuthor": "王二的石锅拌饭"
                        }
                    ]
                }, 
                {
                    "userName": "Jemi", 
                    "userLink": "https://www.zhihu.com/people/2337cff91ebb3b98a0b8446e6b9988cd", 
                    "content": "您好，测试数据集放在哪里呢？都没看到测试集的存放位置", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "言有三-龙鹏", 
                            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
                            "content": "在git中的pytorch的目录下", 
                            "likes": 0, 
                            "replyToAuthor": "Jemi"
                        }, 
                        {
                            "userName": "言有三-龙鹏", 
                            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
                            "content": "<p>最早期的数据集被换掉了，现在文章中重新说明了一下。</p>", 
                            "likes": 0, 
                            "replyToAuthor": "Jemi"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/38755174", 
            "userName": "言有三-龙鹏", 
            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
            "upvote": 5, 
            "title": "【开源框架】一文道尽主流开源框架中的数据增强", 
            "content": "<p>​文章首发于微信公众号 《与有三学AI》</p><a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649029159%26idx%3D1%26sn%3D0d9d4bf2f504f34c5f2a444e2ca3d6d1%26chksm%3D8713445ab064cd4c02d19bd4bd206609de36cea301277be0c239c4f94c2355ee5ec9afb871be%23rd\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-2d9ceed78978badf52f685b50ced44c6_ipico.jpg\" data-image-width=\"358\" data-image-height=\"358\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【开源框架】一文道尽主流开源框架中的数据增强</a><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-dfad6482bb0c2f3208b2bbab3929939b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"751\" data-rawheight=\"213\" class=\"origin_image zh-lightbox-thumb\" width=\"751\" data-original=\"https://pic4.zhimg.com/v2-dfad6482bb0c2f3208b2bbab3929939b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;751&#39; height=&#39;213&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"751\" data-rawheight=\"213\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"751\" data-original=\"https://pic4.zhimg.com/v2-dfad6482bb0c2f3208b2bbab3929939b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-dfad6482bb0c2f3208b2bbab3929939b_b.jpg\"/></figure><p><br/>大家都知道有效的数据对于深度学习的重要性，然而有时能够获取的数据确实有限，为了让模型更加鲁棒，我们可以添加数据或者对已有数据做数据增强。下面我具体阐述四个深度学习框架，包括Caffe，Tensorflow，Pytorch，Mxnet。</p><p class=\"ztext-empty-paragraph\"><br/></p><blockquote>01 Caffe</blockquote><p>Caffe实现数据增强，需要用到三个文件。<br/>1.第一个文件为网络配置文件prototxt，见下图。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-1940eb9f2eae968549d63c4d0612bf46_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"339\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic3.zhimg.com/v2-1940eb9f2eae968549d63c4d0612bf46_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;640&#39; height=&#39;339&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"339\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https://pic3.zhimg.com/v2-1940eb9f2eae968549d63c4d0612bf46_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-1940eb9f2eae968549d63c4d0612bf46_b.jpg\"/></figure><p>2.第二个文件为数据输入层cpp<br/>即image_data_layer.cpp，所在位置为下图。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-cd87108345255a4804b72dfe7ad6ea50_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"400\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic1.zhimg.com/v2-cd87108345255a4804b72dfe7ad6ea50_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;640&#39; height=&#39;400&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"400\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https://pic1.zhimg.com/v2-cd87108345255a4804b72dfe7ad6ea50_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-cd87108345255a4804b72dfe7ad6ea50_b.jpg\"/></figure><p>3. 第3个为caffe的proto配置文件<br/>image_data_layer.cpp中的变量在其中进行定义，proto所在位置为下图。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-5b446f586d00d542e771e8c6158eab4f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"438\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic4.zhimg.com/v2-5b446f586d00d542e771e8c6158eab4f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;640&#39; height=&#39;438&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"438\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https://pic4.zhimg.com/v2-5b446f586d00d542e771e8c6158eab4f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-5b446f586d00d542e771e8c6158eab4f_b.jpg\"/></figure><p>下面我们具体分析。<br/>上面的网络结构中的数据输入层，类型是ImageData，所以数据源是图片。以图中image_data_param对象为例，image_data_param中配置了训练图片的路径source，batchsize，shuffle等，而这些参数的定义，是采用Google的protobuf协议，在caffe.proto中定义的。所以让我们来看proto文件。</p><p>下图是image_data_param在proto文件中的对应定义。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ed0df74f7d1544e5ed65af9c0adad6f0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"394\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic1.zhimg.com/v2-ed0df74f7d1544e5ed65af9c0adad6f0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;640&#39; height=&#39;394&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"394\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https://pic1.zhimg.com/v2-ed0df74f7d1544e5ed65af9c0adad6f0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-ed0df74f7d1544e5ed65af9c0adad6f0_b.jpg\"/></figure><p>从图中我们可以看到prototxt中的source，batch_size，shuffle三个变量。这三个变量在Caffe的运行过程中，只需要载入一次，它用于初始化网络，比如网络输入数据，batchsize，是否打乱文件list等操作，下面颜色标注即对它们的使用。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-12b1bd3e621fbecd9d0abe9137abb684_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"723\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic1.zhimg.com/v2-12b1bd3e621fbecd9d0abe9137abb684_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;640&#39; height=&#39;723&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"723\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https://pic1.zhimg.com/v2-12b1bd3e621fbecd9d0abe9137abb684_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-12b1bd3e621fbecd9d0abe9137abb684_b.jpg\"/></figure><p>image_data_layer.cpp是数据读取的cpp，其中有两个函数，一个set_up函数，用于网络数据输入层初始化，还有一个load_batch函数用于批量载入数据。load_batch函数中调用了data_transformer中的一个函数Transform, 所以我们应该再去看data_transformer.cpp。Transform函数它的输入是cv图片和Caffe的blob指针。在这个函数中我们就可以使用各种数据增强函数了。举一个图片转换的例子。以下图片是我在data_transformer.cpp文件中定义的一个旋转函数。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-04b4ab75dfa213ff3300f00982742a1a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"123\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic3.zhimg.com/v2-04b4ab75dfa213ff3300f00982742a1a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;640&#39; height=&#39;123&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"123\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https://pic3.zhimg.com/v2-04b4ab75dfa213ff3300f00982742a1a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-04b4ab75dfa213ff3300f00982742a1a_b.jpg\"/></figure><p>之后在Transform函数中选择是否调用这个函数。大家可以个性化定义函数，用来做数据增强。自定义了函数之后，修改了caffe源码，记得重新编译Caffe。之后在prototxt中声明调用就好，如下图。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-0e1bbef9113b86a0738892100d3ce499_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"458\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic2.zhimg.com/v2-0e1bbef9113b86a0738892100d3ce499_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;640&#39; height=&#39;458&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"458\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https://pic2.zhimg.com/v2-0e1bbef9113b86a0738892100d3ce499_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-0e1bbef9113b86a0738892100d3ce499_b.jpg\"/></figure><p>以上就是在caffe中做数据增强的具体流程。主要是要在caffe.proto中定义操作，在data_transformer.cpp应用操作。</p><p class=\"ztext-empty-paragraph\"><br/></p><blockquote>02 Tensorflow</blockquote><p>因为tensorflow官方已经封装好了常用的操作，所以直接调用API即可。相关操作在Images模块中，为了方便我们直接看下截图。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-657f286b47dec45658d62262601646d2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"539\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic3.zhimg.com/v2-657f286b47dec45658d62262601646d2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;640&#39; height=&#39;539&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"539\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https://pic3.zhimg.com/v2-657f286b47dec45658d62262601646d2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-657f286b47dec45658d62262601646d2_b.jpg\"/></figure><p>官方的API中数据增强操作非常全面, 而Caffe官方没有提供，需要自己定义。不禁感叹caffe确实有逼格。我截图的第一个版块是对图片的编码解码，下面分别是对图片按类别整理的操作，大家要想了解清楚，还是自己去研读。</p><p class=\"ztext-empty-paragraph\"><br/></p><blockquote>03 Pytorch</blockquote><p>下面看下Pytorch的数据增强使用，依旧是贴心的API接口。Pytorch的数据转换主要是在PIL.Image上，下面来看官方文档，如下图所示。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-c94c8fa5e87aa604c0de6b9572051891_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"466\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic2.zhimg.com/v2-c94c8fa5e87aa604c0de6b9572051891_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;640&#39; height=&#39;466&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"466\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https://pic2.zhimg.com/v2-c94c8fa5e87aa604c0de6b9572051891_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-c94c8fa5e87aa604c0de6b9572051891_b.jpg\"/></figure><p>和Tensorflow一样通过模块调用相应操作即可，这些真没啥好说的。</p><p class=\"ztext-empty-paragraph\"><br/></p><blockquote>04 Mxnet</blockquote><p>Mxnet小巧且功能强大。与Caffe相比，Caffe训练过程中会保存每一层的参数，而Mxnet只保留当前正在前向或者反向传播的参数。Mxnet更适合分布式训练，一般比赛刷榜用的多。具体操作大家去看Image API模块，下图是一个截图。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-26c1576de0bc960bf1586db3fbc8ddae_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"463\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic3.zhimg.com/v2-26c1576de0bc960bf1586db3fbc8ddae_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;640&#39; height=&#39;463&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"463\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https://pic3.zhimg.com/v2-26c1576de0bc960bf1586db3fbc8ddae_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-26c1576de0bc960bf1586db3fbc8ddae_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><blockquote>05 总结</blockquote><p>1. 以上方法所做的皆为有监督的数据增强方法，生成的图片，是在已有的图片上直接做几何变换或像素变换，即训练的时候在线使用。当然了，这些数据增强操作毕竟有限，且不一定对所有任务适合。Google最新的研究AutoAugment，是无监督的方法，它能够对不同的任务自动学习不同的操作，是未来重要的研究方向。另外，无监督的数据增强方法还有如GAN，可以做一些数据生成的离线数据增强，也是比较有意思的研究方向。</p><p>2. Caffe使用更多的数据增强操作必须修改源码，而其它三个框架则是通过高层API直接使用。为了增进理解，我们应该多去研究caffe源码。<br/><br/>更多的caffe源码阅读以及数据增强方法的研究，可以参考我们公众号的相关专题和知乎文章，欢迎阅读，下面是相关链接。</p><p><a href=\"https://zhuanlan.zhihu.com/c_151876233\" class=\"internal\"><span class=\"invisible\">https://</span><span class=\"visible\">zhuanlan.zhihu.com/c_15</span><span class=\"invisible\">1876233</span><span class=\"ellipsis\"></span></a> (二维码自动识别)</p><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/longpeng2008/LongPeng_ML_Course\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/longpeng2008</span><span class=\"invisible\">/LongPeng_ML_Course</span><span class=\"ellipsis\"></span></a> (二维码自动识别)</p><p><br/><b>打一个小广告，本公众号的计算机视觉公开课《AI</b> <b>图像识别项目从入门到上线》上线了，将讲述从零基础到完成一个实际的项目到微信小程序上线的整个流程，欢迎交流捧场。</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-9a6efe0a858eb67f83d514b716657617_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"2746\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic4.zhimg.com/v2-9a6efe0a858eb67f83d514b716657617_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;640&#39; height=&#39;2746&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"2746\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https://pic4.zhimg.com/v2-9a6efe0a858eb67f83d514b716657617_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-9a6efe0a858eb67f83d514b716657617_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-397c17737b0a2a9a809fcd34420c3076_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"268\" data-rawheight=\"325\" class=\"content_image\" width=\"268\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;268&#39; height=&#39;325&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"268\" data-rawheight=\"325\" class=\"content_image lazy\" width=\"268\" data-actualsrc=\"https://pic3.zhimg.com/v2-397c17737b0a2a9a809fcd34420c3076_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-caf2bc6ada0920ad1ce07c326564dad9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"10\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic2.zhimg.com/v2-caf2bc6ada0920ad1ce07c326564dad9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;640&#39; height=&#39;10&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"10\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https://pic2.zhimg.com/v2-caf2bc6ada0920ad1ce07c326564dad9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-caf2bc6ada0920ad1ce07c326564dad9_b.jpg\"/></figure><p>如果想加入我们，后台留言吧</p><p><b>更多请移步</b></p><blockquote>1，我的gitchat达人课</blockquote><a href=\"https://link.zhihu.com/?target=http%3A//gitbook.cn/gitchat/column/5a6011fbbd5ff2623773394c\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic4.zhimg.com/v2-778539fd0d72d5e46ebabd371db4df83_180x120.jpg\" data-image-width=\"1920\" data-image-height=\"1080\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">龙鹏的达人课</a><blockquote>2，AI技术公众号,《与有三学AI》</blockquote><a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649028681%26idx%3D1%26sn%3D44147bf8c9006c5d72dd51ed2f2871ba%26chksm%3D87134634b064cf22501feda3afff66beec38a09ec0ff91ac58c70a4f8c00da83b8ecfbf50175%23rd\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-2d9ceed78978badf52f685b50ced44c6_ipico.jpg\" data-image-width=\"358\" data-image-height=\"358\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">一文说说这十多年来计算机玩摄影的历史</a><blockquote>3，以及摄影号，《有三工作室》</blockquote><a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3MzczNzY0Mw%3D%3D%26mid%3D2648543699%26idx%3D1%26sn%3Da17c65aac9a2d07af0af141b4c0048bb%26chksm%3D87234517b054cc01458e30468906cc59010a1fd0a74db8b3a935ef0d83e7d17b163f274a23c2%26scene%3D21%23wechat_redirect\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-8a86f4fc840333cf9e0dea9544dc9dde_ipico.jpg\" data-image-width=\"640\" data-image-height=\"640\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【摄影大咖2】论自拍，我只服这位悬崖上的自拍狂</a><p></p>", 
            "topic": [
                {
                    "tag": "Hadoop", 
                    "tagLink": "https://api.zhihu.com/topics/19563390"
                }, 
                {
                    "tag": "Java", 
                    "tagLink": "https://api.zhihu.com/topics/19561132"
                }, 
                {
                    "tag": "TensorFlow", 
                    "tagLink": "https://api.zhihu.com/topics/20032249"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/33659313", 
            "userName": "言有三-龙鹏", 
            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
            "upvote": 8, 
            "title": "【从caffe到Tensorflow 1】io 操作", 
            "content": "<p>本文首发于微信公众号《与有三学AI》</p><a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649028828%26idx%3D1%26sn%3Dfc27959f2a810c8562d109cce754e985%26chksm%3D871346a1b064cfb73b0186306f3d8a366c97dd1e37ebeacb80b44a325282cc0264318febd4ac%23rd\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-25ebd31d3f8c97e0429d01a94d0d1e2c_ipico.jpg\" data-image-width=\"225\" data-image-height=\"225\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【从caffe到Tensorflow 1】io 操作</a><p>最近项目要频繁用到tensorflow，所以不得不认真研究下tensorflow而不是跟之前一样遇到了就搞一下了。</p><p>首先我觉得所有这些框架里面caffe是最清晰的，所以就算是学习tensorflow，我也会以caffe的思路去学习，这就是这个系列的用意。</p><p>今天是第1篇，咱们说io操作，也就是文件读取，载入内存。</p><blockquote><b>Caffe的io操作</b></blockquote><p>caffe的io，是通过在prototxt中定义数据输入，默认支持data，imagedata，hdf5data，window data等类型。Data layer，输入是LMDB数据格式，image data 支持的是image list的数据格式。</p><p>对于LMDB来说，我们在caffe layer中配置准备好的二进制数据即可。</p><p>对于image data，我们准备一个data list，官方的image data是一个分类任务的list，格式为每行image，label，当然随着任务的不同我们可以自定义。比如分割任务image，mask。检测任务，image num of object, object rect1,object rect2等。</p><p>典型的格式是这样：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-4b563b95b19ff9db1014868466000de8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"814\" data-rawheight=\"712\" class=\"origin_image zh-lightbox-thumb\" width=\"814\" data-original=\"https://pic1.zhimg.com/v2-4b563b95b19ff9db1014868466000de8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;814&#39; height=&#39;712&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"814\" data-rawheight=\"712\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"814\" data-original=\"https://pic1.zhimg.com/v2-4b563b95b19ff9db1014868466000de8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-4b563b95b19ff9db1014868466000de8_b.jpg\"/></figure><p>具体的载入，就是在相关层的DataLayerSetUp函数中设置好输入大小，load_batch函数中，读取原始数据，再利用data_transform塞入内存。</p><p>当然caffe也可以自定义python层使用，不过我还是更习惯c++，何况这里比较的也是官方自带的layer。</p><p>\t从上面我们可以看出，caffe的io都是从文件中载入，只是文件的组织方式不同。</p><p>Tensorflow的io输入则要复杂，全面很多，我们参考tensorflow1.5的API。</p><a href=\"https://link.zhihu.com/?target=https%3A//www.tensorflow.org/api_docs/python/tf/data\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">tensorflow.org/api_docs</span><span class=\"invisible\">/python/tf/data</span><span class=\"ellipsis\"></span></a><blockquote><b>Tensorflow的io操作</b></blockquote><p>Tensorflow不止是读取文件这一种方法，它可以包含以下几种方式。</p><ul><li>预加载数据： 在TensorFlow图中定义常量或变量来保存所有数据(仅适用于数据量比较小的情况)</li></ul><div class=\"highlight\"><pre><code class=\"language-text\">import tensorflow as tf\n# 设计Graph\nx1 = tf.constant([2, 3, 4])\nx2 = tf.constant([4, 0, 1])\ny = tf.add(x1, x2)\nwith tf.Session() as sess:\n   print sess.run(y)</code></pre></div><p>如上，x1，x2都是预加载好的数据。在设计Graph的时候，x1和x2就已经被定义成了两个有值的列表，在计算y的时候直接取x1和x2的值。这种方法的问题是将数据直接内嵌到Graph中，再把Graph传入Session中运行。当数据量比较大时，Graph的传输会遇到效率问题。</p><ul><li>Feeding 它定义变量的时候用占位符替代数据，待运行的时候填充数据。</li></ul><div class=\"highlight\"><pre><code class=\"language-text\">import tensorflow as tf\nx1 = tf.placeholder(tf.int16)\nx2 = tf.placeholder(tf.int16)\ny = tf.add(x1, x2)\n# 用Python产生数据\nli1 = [2, 3, 4]\nli2 = [4, 0, 1]\n# 打开一个session --&gt; 喂数据 --&gt; 计算y\nwith tf.Session() as sess:\n   print sess.run(y, feed_dict={x1: li1, x2: li2})</code></pre></div><p>定义的时候，x1, x2只是占位符所以没有具体的值，运行的时候使用sess.run()中的feed_dict参数，将Python产生的数据喂给后端，并计算y。</p><ul><li>Reading From File</li></ul><p>前两种方法很方便，但是遇到大型数据的时候就会很吃力，即使是Feeding，中间环节的增加也是不小的开销，比如数据类型转换等等。而且，面对复杂类型的数据，也是处理不过来的。因此与caffe一样，tensorflow也是支持从文件中读取数据。 </p><p>下面举一个利用队列读取硬盘中的数据到内存的例子：假如需要读取的数据存在一个list中。这篇博客举了一个很好的例子；</p><a href=\"https://link.zhihu.com/?target=http%3A//honggang.io/2016/08/19/tensorflow-data-reading/\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">honggang.io/2016/08/19/</span><span class=\"invisible\">tensorflow-data-reading/</span><span class=\"ellipsis\"></span></a><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-4b4d155361e2123359df9ce58f692a7f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1991\" data-rawheight=\"495\" class=\"origin_image zh-lightbox-thumb\" width=\"1991\" data-original=\"https://pic4.zhimg.com/v2-4b4d155361e2123359df9ce58f692a7f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1991&#39; height=&#39;495&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1991\" data-rawheight=\"495\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1991\" data-original=\"https://pic4.zhimg.com/v2-4b4d155361e2123359df9ce58f692a7f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-4b4d155361e2123359df9ce58f692a7f_b.jpg\"/></figure><p>在上图中，首先由一个单线程把文件名堆入队列，两个Reader同时从队列中取文件名并读取数据，Decoder将读出的数据解码后堆入样本队列。</p><p>利用了string_input_producer + tf.TextLineReader() + train.start_queue_runners来读取数据，string_input_producer的定义在</p><a href=\"https://link.zhihu.com/?target=https%3A//www.github.com/tensorflow/tensorflow/blob/r1.5/tensorflow/python/training/input.py\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">tensorflow/python/training/input.py</a><div class=\"highlight\"><pre><code class=\"language-text\">string_input_producer(\n    string_tensor,\n    num_epochs=None,\n    shuffle=True,\n    seed=None,\n    capacity=32,\n    shared_name=None,\n    name=None,\n    cancel_op=None\n)</code></pre></div><p>从上面可见，可以指定num_epochs，是否shuffle等，这就是一个最简单的从文件中读取的例子了。</p><p>假设有文件A.csv如下：</p><div class=\"highlight\"><pre><code class=\"language-text\">Alpha1,A1\nAlpha2,A2\nAlpha3,A3</code></pre></div><p>单个reader读取单个数据脚本如下；</p><div class=\"highlight\"><pre><code class=\"language-text\">import tensorflow as tf\nfilenames = [&#39;A.csv&#39;] 必须要以数组的形式\nfilename_queue = tf.train.string_input_producer(filenames, shuffle=False)\nreader = tf.TextLineReader()# 定义Reader\nkey, value = reader.read(filename_queue)\n# 定义Decoder\nexample, label = tf.decode_csv(value, record_defaults=[[&#39;null&#39;], [&#39;null&#39;]])\n# 运行Graph\nwith tf.Session() as sess:\n   coord = tf.train.Coordinator()  #创建一个协调器，管理线程\n   threads = tf.train.start_queue_runners(coord=coord)  #启动QueueRunner, 此时文件名队列已经进队。\n   for i in range(10):\n       print example.eval()   #取样本的时候，一个Reader先从文件名队列中取出文件名，读出数据，Decoder解析后进入样本队列。\n   coord.request_stop()\n   coord.join(threads)</code></pre></div><p>讲了上面的基础例子之后，我们开始看更复杂的例子。</p><p>上面的例子包含两类，一种是从placeholder读内存中的数据，一种是使用queue读硬盘中的数据，而1.3以后的Dataset API同时支持从内存和硬盘的读取。 </p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>它们支持多种类型的输入，分别是FixedLengthRecordDataset, TextLineDataset, TFRecordDataset类型的。</b></p><p class=\"ztext-empty-paragraph\"><br/></p><p>TextLineDataset：这个函数的输入是一个文件的列表，输出是一个dataset。dataset中的每一个元素就对应了文件中的一行。可以使用这个函数来读入CSV文件，跟上面例子类似。</p><p>TFRecordDataset：这个函数是用来读TFRecord文件的，dataset中的每一个元素就是一个TFExample，这是很常用的。</p><p>FixedLengthRecordDataset：这个函数的输入是一个文件的列表和一个record_bytes，之后dataset的每一个元素就是文件中固定字节数record_bytes的内容。通常用来读取以二进制形式保存的文件，如CIFAR10数据集就是这种形式。。</p><p>迭代器：提供了一种一次获取一个数据集元素的方法。</p><p>所有定义都在<a href=\"https://link.zhihu.com/?target=https%3A//github.com/tensorflow/tensorflow/tree/r1.5/tensorflow\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">tensorflow</a>/<a href=\"https://link.zhihu.com/?target=https%3A//github.com/tensorflow/tensorflow/tree/r1.5/tensorflow/python\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">python</a>/<a href=\"https://link.zhihu.com/?target=https%3A//github.com/tensorflow/tensorflow/tree/r1.5/tensorflow/python/data\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">data</a>/<a href=\"https://link.zhihu.com/?target=https%3A//github.com/tensorflow/tensorflow/tree/r1.5/tensorflow/python/data/ops\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">ops</a>/readers.py<b>中。</b></p><p>参考文章</p><a href=\"https://zhuanlan.zhihu.com/p/30751039\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic4.zhimg.com/v2-74505d6493b8913cc70014005389dcf3_180x120.jpg\" data-image-width=\"688\" data-image-height=\"387\" class=\"internal\">何之源：TensorFlow全新的数据读取方式：Dataset API入门教程</a><b><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-30da651739862d432e35975acb8ae15d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1472\" data-rawheight=\"446\" class=\"origin_image zh-lightbox-thumb\" width=\"1472\" data-original=\"https://pic2.zhimg.com/v2-30da651739862d432e35975acb8ae15d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1472&#39; height=&#39;446&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1472\" data-rawheight=\"446\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1472\" data-original=\"https://pic2.zhimg.com/v2-30da651739862d432e35975acb8ae15d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-30da651739862d432e35975acb8ae15d_b.jpg\"/></figure></b><p>我们先理解一下dataset是什么？</p><p>Dataset可以看作是相同类型“元素”的有序列表，而单个“元素”可以是向量，也可以是字符串、图片，甚至是tuple或者dict。</p><p>先以最简单的，Dataset的每一个元素是一个数字为例：</p><div class=\"highlight\"><pre><code class=\"language-text\">import tensorflow as tf\nimport numpy as np\ndataset = tf.data.Dataset.from_tensor_slices(np.array([1.0, 2.0, 3.0, 4.0, 5.0]))</code></pre></div><p>这样，我们就创建了一个dataset，这个dataset中含有5个元素，分别是1.0, 2.0, 3.0, 4.0, 5.0。</p><p>如何将这个dataset中的元素取出呢？方法是从Dataset中示例化一个Iterator，然后对Iterator进行迭代。</p><div class=\"highlight\"><pre><code class=\"language-text\">iterator = dataset.make_one_shot_iterator()\none_element = iterator.get_next()\nwith tf.Session() as sess:\nfor i in range(5):\n       print(sess.run(one_element))</code></pre></div><p>对应的输出结果应该就是从1.0到5.0。语句iterator = dataset.make_one_shot_iterator()从dataset中实例化了一个Iterator，这个Iterator是一个“one shot iterator”，即只能从头到尾读取一次。one_element = iterator.get_next()表示从iterator里取出一个元素，调用sess.run(one_element)后，才能真正地取出一个值。</p><p>如果一个dataset中元素被读取完了，再尝试sess.run(one_element)的话，就会抛出tf.errors.OutOfRangeError异常，这个行为与使用队列方式读取数据的行为是一致的。在实际程序中，可以在外界捕捉这个异常以判断数据是否读取完，请参考下面的代码：</p><div class=\"highlight\"><pre><code class=\"language-text\">dataset = tf.data.Dataset.from_tensor_slices(np.array([1.0, 2.0, 3.0, 4.0, 5.0]))\niterator = dataset.make_one_shot_iterator()\none_element = iterator.get_next()\nwith tf.Session() as sess:\ntry:\nwhile True:\n           print(sess.run(one_element))\nexcept tf.errors.OutOfRangeError:\n       print(&#34;end!&#34;)</code></pre></div><p>dataset还可以有一些基本的数据变换操作，即transform操作，常见的有map，batch，shuffle，repeat</p><p>把数据+1dataset <b>=</b> dataset<b>.</b>map(lambda x: x <b>+</b> 1)</p><p>组合成batch，dataset <b>=</b> dataset<b>.</b>batch(32)</p><p>进行shuffle，dataset <b>=</b> dataset<b>.</b>shuffle(buffer_size<b>=</b>10000)</p><p>repeat 组成多个epoch，dataset <b>=</b> dataset<b>.</b>repeat(5)</p><blockquote><b>来一个实例 \t</b></blockquote><p>理解了dataset之后，我们再看如何从文件中读取数据。由于tfrecord是非常常用的格式，下面我们就以这个为例。 </p><p>假如我们有两个文件夹，一个是整理好的固定大小的图片，一个是对应label图片，这是一个分割任务,下面我们开始做。</p><ul><li>处理成tfrecord格式</li></ul><p>首先，我们要把数据处理成tfrecord格式。</p><p>我们先定义一下存储格式：</p><p>直接贴完整代码了</p><div class=\"highlight\"><pre><code class=\"language-text\">import tensorflow as tf\nimport os\nimport sys\n\ndef _bytes_feature(value):\n   return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\ndef _convert_to_example(image_buffer, mask_buffer, filename, mask_filename):\n   example = tf.train.Example(features=tf.train.Features(feature={\n       &#39;image&#39;: _bytes_feature(image_buffer),\n       &#39;mask&#39;: _bytes_feature(mask_buffer),\n       &#34;filename&#34;: _bytes_feature(bytes(filename.encode(&#34;UTF-8&#34;))),\n       &#34;mask_filename&#34;: _bytes_feature(bytes(mask_filename.encode(&#34;UTF-8&#34;)))\n       #&#34;filename&#34;: _bytes_feature(bytes(filename, encoding=&#34;UTF-8&#34;)),\n       #&#34;mask_filename&#34;: _bytes_feature(bytes(mask_filename, encoding=&#34;UTF-8&#34;))\n   }))\n   return example\n\nfiles = os.listdir(sys.argv[1])\nmask_dir = sys.argv[2]\nwriter = tf.python_io.TFRecordWriter(sys.argv[3])\nfor file in files:\n  filename = file\n  mask_filename = os.path.join(mask_dir,filename.split(&#39;.&#39;)[0] + &#34;.png&#34;)\n  filename = os.path.join(sys.argv[1],filename)\n  try:\n     image_buffer = tf.gfile.FastGFile(filename, &#39;rb&#39;).read()\n     mask_buffer = tf.gfile.FastGFile(mask_filename, &#39;rb&#39;).read()\n\n     print &#34;filename=&#34;,filename\n     example = _convert_to_example(image_buffer, mask_buffer, filename, mask_filename)\n     writer.write(example.SerializeToString())\n\n  except StopIteration as e:\n     print &#34;error&#34;</code></pre></div><p>_convert_to_example这个函数，就是定义存储的格式；tf.gfile.FastGFile就是读取图片原始文件格式且不编解码，writer = tf.python_io.TFRecordWriter(sys.argv[3])是定义writer,写起来其实挺简单。</p><p>tf.train.Example是一个protocol buffer，定义在</p><a href=\"https://link.zhihu.com/?target=https%3A//www.github.com/tensorflow/tensorflow/blob/r1.5/tensorflow/core/example/example.proto\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic4.zhimg.com/v2-a8be12cfa91ed9443fcb4be496e51f2b_ipico.jpg\" data-image-width=\"322\" data-image-height=\"322\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">tensorflow/tensorflow</a><p>将数据填入到Example后就可以序列化为一个字符串。一个Example中包含Features，Features里包含Feature，每一个feature其实就是一个字典，如上面的一个字典包含4个字段。</p><ul><li>读取tf.records</li></ul><p>读取数据就可以使用tf.TFRecordReader的tf.parse_single_example解析器。它将Example protocol buffer解析为张量。</p><p>简单的利用队列读取，可以采用下面的方法</p><div class=\"highlight\"><pre><code class=\"language-text\">filename_queue = tf.train.string_input_producer([filename])\nreader = tf.TFRecordReader()\n   _, serialized_example = reader.read(filename_queue)   #返回文件名和文件\n   features = tf.parse_single_example(serialized_example,\n       features={&#39;label&#39;: tf.FixedLenFeature([], tf.int64),\n                 ‘img_raw&#39; : tf.FixedLenFeature([], tf.string),})\nimg = tf.decode_raw(features[&#39;image&#39;], tf.uint8)\nlabel = tf.decode_raw(features[&#39;mask&#39;], tf.uint8)</code></pre></div><p>不过，我们这里利用新的API的dataset来读取，更加高效。直接贴上代码如下：</p><p>上面定义过_convert_to_example，我们这里先定义一个读取格式。</p><div class=\"highlight\"><pre><code class=\"language-text\">def _extract_features(example):\n   features = {\n       &#34;image&#34;: tf.FixedLenFeature((), tf.string),\n       &#34;mask&#34;: tf.FixedLenFeature((), tf.string)\n}\n获取一个example\nparsed_example = tf.parse_single_example(example, features)\n得到原始图并转换格式，set_shape是必须的，因为没有存储尺寸信息。\n   images = tf.cast(tf.image.decode_jpeg(parsed_example[&#34;image&#34;]), dtype=tf.float32)\n   images.set_shape([224, 224, 3])\n   masks = tf.cast(tf.image.decode_jpeg(parsed_example[&#34;mask&#34;]), dtype=tf.float32) / 255.\n   masks.set_shape([224, 224, 1])\n   return images, masks</code></pre></div><p>下面这个函数就是create迭代器了，在这里我们使用最简单的iterator，one-shot iterator来迭代，当然它只支持在一个dataset上迭代一次，不需要显式初始化。这里不需要怀疑epoch的问题，因为dataset.repeat(num_epoch)就会设置epoch数目，所以虽然只在dataset上迭代一次，但是已经遍历过数据epoch次。</p><div class=\"highlight\"><pre><code class=\"language-text\">def create_one_shot_iterator(filenames, batch_size, num_epoch):\n   dataset = tf.data.TFRecordDataset(filenames)\n   dataset = dataset.map(_extract_features)\n   dataset = dataset.shuffle(buffer_size=batch_size)\n   dataset = dataset.batch(batch_size)\n   dataset = dataset.repeat(num_epoch)\nreturn dataset.make_one_shot_iterator()\n用的时候，就是\ntrain_iterator = create_one_shot_iterator(train_files, train_batch_size, num_epoch=num_epochs)\nnext_images, next_masks = train_iterator.get_next()\n当然读取出来之后可以做一些数据增强的操作。</code></pre></div><p>就这样完毕！</p><blockquote>1，我的gitchat达人课</blockquote><a href=\"https://link.zhihu.com/?target=http%3A//gitbook.cn/gitchat/column/5a6011fbbd5ff2623773394c\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic4.zhimg.com/v2-778539fd0d72d5e46ebabd371db4df83_180x120.jpg\" data-image-width=\"1920\" data-image-height=\"1080\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">龙鹏的达人课</a><blockquote>2，AI技术公众号,《与有三学AI》</blockquote><a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649028813%26idx%3D1%26sn%3Dad34ce9ed0286d9ea13854b063a1b630%26chksm%3D871346b0b064cfa64d2703c1a667fa2356a666682b01dd0841bad2ad99fcd889b60be3c64c60%23rd\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-25ebd31d3f8c97e0429d01a94d0d1e2c_ipico.jpg\" data-image-width=\"225\" data-image-height=\"225\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">[caffe解读] caffe从数学公式到代码实现5-caffe中的卷积</a><blockquote>3，以及摄影号，《有三工作室》</blockquote><a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3MzczNzY0Mw%3D%3D%26mid%3D2648543628%26idx%3D1%26sn%3D5573c69f7b54acd6e2bfcc123d7a9988%26chksm%3D87234548b054cc5e1aed7177a537fef841768acff589a93d8234fab318bbe682a9da40192134%23rd\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-8a86f4fc840333cf9e0dea9544dc9dde_ipico.jpg\" data-image-width=\"640\" data-image-height=\"640\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">冯小刚说，“他懂我”</a><p></p>", 
            "topic": [
                {
                    "tag": "TensorFlow", 
                    "tagLink": "https://api.zhihu.com/topics/20032249"
                }, 
                {
                    "tag": "Caffe（深度学习框架）", 
                    "tagLink": "https://api.zhihu.com/topics/20019488"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": [
                {
                    "userName": "知乎用户", 
                    "userLink": "https://www.zhihu.com/people/0", 
                    "content": "<p>请教下Tensorflow如何读取caffe生成的LMDB文件。 需求出发点是imagenet 训练集源文件约140G，生成的LMDB约187G。硬盘占用太多，不想再重新生成TFrecord。 ps. 文件都是使用NFS挂载到训练机器上，所以尽量使用数据库文件而非jpg文件增加速度。</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/33644573", 
            "userName": "言有三-龙鹏", 
            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
            "upvote": 10, 
            "title": "【caffe教程5】caffe中的卷积", 
            "content": "<p>本文首发于微信公众号《与有三学AI》</p><a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649028813%26idx%3D1%26sn%3Dad34ce9ed0286d9ea13854b063a1b630%26chksm%3D871346b0b064cfa64d2703c1a667fa2356a666682b01dd0841bad2ad99fcd889b60be3c64c60%23rd\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-25ebd31d3f8c97e0429d01a94d0d1e2c_ipico.jpg\" data-image-width=\"225\" data-image-height=\"225\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">[caffe解读] caffe从数学公式到代码实现5-caffe中的卷积</a><p>今天要讲的就是跟卷积相关的一些layer了</p><p>im2col_layer.cpp</p><p>base_conv_layer.cpp</p><p>conv_layer.cpp</p><p>deconv_layer.cpp</p><p>inner_product_layer.cpp</p><p class=\"ztext-empty-paragraph\"><br/></p><blockquote><b>1 im2col_layer.cpp</b></blockquote><p>这是caffe里面的重要操作，caffe为什么这么耗显存，跟这个有很大关系。im2col的目的，就是把要滑动卷积的图像，先一次性存起来，然后再进行矩阵乘操作。简单来说，它的输入是一个C*H*W的blob，经过im2col操作会变成K&#39; x (H x W) 的矩阵，其中K&#39; =C*kernel_r*kernel_r，kernel_r就是卷积核的大小，这里只看正方形的卷积核。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>如果不用这样的操作，贾扬清有一个吐槽，对于输入大小为W*H，维度为D的blob，卷积核为M*K*K，那么如果利用for循环，会是这样的一个操作，6层for循环，计算效率是极其低下的。</p><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-text\">for w in 1..W\n for h in 1..H\n   for x in 1..K\n     for y in 1..K\n       for m in 1..M\n         for d in 1..D\n           output(w, h, m) += input(w+x, h+y, d) * filter(m, x, y, d)\n         end\n       end\n     end\n   end\n end\nend</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>具体im2col是什么原理呢？先贴出<b><a href=\"https://www.zhihu.com/people/jiayangqing%22%20%5Ct%20%22_blank\" class=\"internal\">贾扬清</a></b>的回答。</p><p><a href=\"https://www.zhihu.com/question/28385679\" class=\"internal\"><span class=\"invisible\">https://www.</span><span class=\"visible\">zhihu.com/question/2838</span><span class=\"invisible\">5679</span><span class=\"ellipsis\"></span></a></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-f43008c4fabac221db206a41d7500a53_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"720\" data-rawheight=\"540\" class=\"origin_image zh-lightbox-thumb\" width=\"720\" data-original=\"https://pic4.zhimg.com/v2-f43008c4fabac221db206a41d7500a53_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;720&#39; height=&#39;540&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"720\" data-rawheight=\"540\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"720\" data-original=\"https://pic4.zhimg.com/v2-f43008c4fabac221db206a41d7500a53_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-f43008c4fabac221db206a41d7500a53_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>上面说了，要把C*H*W的blob，变成K&#39; x (H x W)或者 (H x W) xK&#39; 的矩阵,把filters也复制成一个大矩阵，这样两者直接相乘就得到结果，下面看一个简单小例子。</p><p>借用网友一张图，虽然和caffe细节上不同，但是还是有助于理解。<a href=\"https://link.zhihu.com/?target=http%3A//blog.csdn.net/mrhiuser/article/details/52672824\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">blog.csdn.net/mrhiuser/</span><span class=\"invisible\">article/details/52672824</span><span class=\"ellipsis\"></span></a></p><p>4*4的原始数据，进行stride=1的3*3操作，其中im2col的操作就是：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-0b3ac9e7ba3e6c75572d76766ff95b80_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1168\" data-rawheight=\"1050\" class=\"origin_image zh-lightbox-thumb\" width=\"1168\" data-original=\"https://pic1.zhimg.com/v2-0b3ac9e7ba3e6c75572d76766ff95b80_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1168&#39; height=&#39;1050&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1168\" data-rawheight=\"1050\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1168\" data-original=\"https://pic1.zhimg.com/v2-0b3ac9e7ba3e6c75572d76766ff95b80_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-0b3ac9e7ba3e6c75572d76766ff95b80_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>也就是说4*4的矩阵，经过了im2col后，变成了9*4的矩阵，卷积核可以做同样扩展，卷积操作就变成了两个矩阵相乘。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>下面看im2col的代码；</p><div class=\"highlight\"><pre><code class=\"language-text\">template &lt;typename Dtype&gt;\nvoid im2col_cpu(const Dtype* data_im, const int channels,\n   const int height, const int width, const int kernel_h, const int kernel_w,\n   const int pad_h, const int pad_w,\n   const int stride_h, const int stride_w,\n   const int dilation_h, const int dilation_w,\nDtype* data_col) {\n  //输入为data_im，kernel_h，kernel_w以及各类卷积参数，输出就是data_col。\n  //out_put_h，out_put_w，是输出的图像尺寸。\n const int output_h = (height + 2 * pad_h -\n   (dilation_h * (kernel_h - 1) + 1)) / stride_h + 1;\n const int output_w = (width + 2 * pad_w -\n   (dilation_w * (kernel_w - 1) + 1)) / stride_w + 1;\n const int channel_size = height * width;\n //外层channel循环不管\n for (int channel = channels; channel--; data_im += channel_size) {\n    //这是一个关于kernel_row和kernel_col的2层循环 \n   for (int kernel_row = 0; kernel_row &lt; kernel_h; kernel_row++) {\nfor (int kernel_col = 0; kernel_col &lt; kernel_w; kernel_col++) {\n       int input_row = -pad_h + kernel_row * dilation_h;\n        //这是一个关于output_h和output_w的循环，这实际上就是上图例子中每一行的数据 \n  for (int output_rows = output_h; output_rows; output_rows--) {\n          //边界条件属特殊情况，可以细下推敲\n         if (!is_a_ge_zero_and_a_lt_b(input_row, height)) {\n for (int output_cols = output_w; output_cols; output_cols--) {\n             *(data_col++) = 0;\n           }\n         } else {\n           int input_col = -pad_w + kernel_col * dilation_w;\n           for (int output_col = output_w; output_col; output_col--) {\nif (is_a_ge_zero_and_a_lt_b(input_col, width)) {\n//这就是核心的赋值语句，按照循环的顺序，我们可以知道是按照输出output_col*output_h的尺寸，一截一截地串接成了一个col。\n               *(data_col++) = data_im[input_row * width + input_col];\n             } else {\n               *(data_col++) = 0;\n             }\n             input_col += stride_w;\n           }\n         }\n         input_row += stride_h;\n       }\n     }\n   }\n }\n}</code></pre></div><p>相关注释已经放在了上面，col2im的操作非常类似，可以自行看源码，这一段要自己写出来怕是需要调试一些时间。 </p><p class=\"ztext-empty-paragraph\"><br/></p><p>有了上面的核心代码后，Forward只需要调用im2col，输入为bottom_data，输出为top_data，Backward只需要调用col2im，输入为top_diff，输出为bottom_diff即可，代码就不贴出了。</p><p class=\"ztext-empty-paragraph\"><br/></p><blockquote><b>2 conv_layer.cpp，base_conv_layer.cpp</b></blockquote><p>数学定义不用说，我们直接看代码，这次要两个一起看。由于conv_layer.cpp依赖于base_conv_layer.cpp，我们先来看看base_conv_layer.hpp中包含了什么东西，非常多。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>base_conv_layer.hpp变量：</p><div class=\"highlight\"><pre><code class=\"language-text\">/// @brief The spatial dimensions of a filter kernel.\nBlob&lt;int&gt; kernel_shape_;\n /// @brief The spatial dimensions of the stride.\n Blob&lt;int&gt; stride_;\n /// @brief The spatial dimensions of the padding.\n Blob&lt;int&gt; pad_;\n /// @brief The spatial dimensions of the dilation.\n Blob&lt;int&gt; dilation_;\n /// @brief The spatial dimensions of the convolution input.\n Blob&lt;int&gt; conv_input_shape_;\n /// @brief The spatial dimensions of the col_buffer.\n vector&lt;int&gt; col_buffer_shape_;\n /// @brief The spatial dimensions of the output.\nvector&lt;int&gt; output_shape_;\n const vector&lt;int&gt;* bottom_shape_;\n\n int num_spatial_axes_;\n int bottom_dim_;\n int top_dim_;\n\n int channel_axis_;\n int num_;\n int channels_;\n int group_;\n int out_spatial_dim_;\n int weight_offset_;\n int num_output_;\n bool bias_term_;\n bool is_1x1_;\n bool force_nd_im2col_;\n\n int num_kernels_im2col_;\n int num_kernels_col2im_;\n int conv_out_channels_;\n int conv_in_channels_;\n int conv_out_spatial_dim_;\n int kernel_dim_;\n int col_offset_;\n int output_offset_;\n\n Blob&lt;Dtype&gt; col_buffer_;\n Blob&lt;Dtype&gt; bias_multiplier_;</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>非常之多，因为卷积发展到现在，已经有很多的参数需要控制。无法一一解释了，stride_，pad_，dilation是和卷积步长有关参数，kernel_shape_是卷积核大小，conv_input_shape_是输入大小，output_shape是输出大小，其他都是以后遇到了再说，现在我们先绕过。更具体的解答，有一篇博客可以参考<a href=\"https://link.zhihu.com/?target=http%3A//blog.csdn.net/lanxuecc/article/details/53188738\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">blog.csdn.net/lanxuecc/</span><span class=\"invisible\">article/details/53188738</span><span class=\"ellipsis\"></span></a></p><p class=\"ztext-empty-paragraph\"><br/></p><p>下面直接看conv_layer.cpp</p><p>既然是卷积，输出的大小就取决于很多参数，所以先要计算输出的大小。</p><div class=\"highlight\"><pre><code class=\"language-text\">void ConvolutionLayer&lt;Dtype&gt;::compute_output_shape() {\n const int* kernel_shape_data = this-&gt;kernel_shape_.cpu_data();\n const int* stride_data = this-&gt;stride_.cpu_data();\n const int* pad_data = this-&gt;pad_.cpu_data();\n const int* dilation_data = this-&gt;dilation_.cpu_data();\n this-&gt;output_shape_.clear();\n for (int i = 0; i &lt; this-&gt;num_spatial_axes_; ++i) {\n   // i + 1 to skip channel axis\n   const int input_dim = this-&gt;input_shape(i + 1);\n   const int kernel_extent = dilation_data[i] * (kernel_shape_data[i] - 1) + 1;\n   const int output_dim = (input_dim + 2 * pad_data[i] - kernel_extent)\n       / stride_data[i] + 1;\n   this-&gt;output_shape_.push_back(output_dim);\n }\n}</code></pre></div><p>然后，在forward函数中，</p><div class=\"highlight\"><pre><code class=\"language-text\">template &lt;typename Dtype&gt;\nvoid ConvolutionLayer&lt;Dtype&gt;::Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {\n const Dtype* weight = this-&gt;blobs_[0]-&gt;cpu_data();\n for (int i = 0; i &lt; bottom.size(); ++i) {\n   const Dtype* bottom_data = bottom[i]-&gt;cpu_data();\n   Dtype* top_data = top[i]-&gt;mutable_cpu_data();\n   for (int n = 0; n &lt; this-&gt;num_; ++n) {\n     this-&gt;forward_cpu_gemm(bottom_data + n * this-&gt;bottom_dim_, weight,\n         top_data + n * this-&gt;top_dim_);\n     if (this-&gt;bias_term_) {\n       const Dtype* bias = this-&gt;blobs_[1]-&gt;cpu_data();\n       this-&gt;forward_cpu_bias(top_data + n * this-&gt;top_dim_, bias);\n     }\n   }\n }\n}</code></pre></div><p>我们知道卷积层的输入，是一个blob，输出是一个blob，从上面代码知道卷积核的权重存在了this-&gt;blobs_[0]-&gt;cpu_data()中, this-&gt;blobs_[1]-&gt;cpu_data()则是bias，当然不一定有值。外层循环大小为bottom.size()，可见其实可以有多个输入。</p><p>看看里面最核心的函数，this-&gt;forward_cpu_gemm。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>而这个函数是在这里被调用的；输入input，输出col_buff，关于这个函数的解析，<a href=\"https://link.zhihu.com/?target=https%3A//tangxman.github.io/2015/12/07/caffe-conv/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">tangxman.github.io/2015</span><span class=\"invisible\">/12/07/caffe-conv/</span><span class=\"ellipsis\"></span></a>解释地挺详细，我大概总结一下。</p><p>首先，按照调用顺序，对于3*3等正常的卷积，forward_cpu_gemm会调用conv_im2col_cpu函数（在base_conv_layer.hpp中），它的作用看名字就知道，将图像先转换为一个大矩阵，将卷积核也按列复制成大矩阵；</p><p class=\"ztext-empty-paragraph\"><br/></p><p>然后利用caffe_cpu_gemm计算矩阵相乘得到卷积后的结果。</p><div class=\"highlight\"><pre><code class=\"language-text\">template &lt;typename Dtype&gt;\nvoid BaseConvolutionLayer&lt;Dtype&gt;::forward_cpu_gemm(const Dtype* input,\n   const Dtype* weights, Dtype* output, bool skip_im2col) {\n const Dtype* col_buff = input;\n if (!is_1x1_) {\n   if (!skip_im2col) {\n      // 如果没有1x1卷积，也没有skip_im2col    \n      // 则使用conv_im2col_cpu对使用卷积核滑动过程中的每一个kernel大小的图像块    \n      // 变成一个列向量，形成一个height=kernel_dim_的    \n      // width = 卷积后图像heght*卷积后图像width   \n     conv_im2col_cpu(input, col_buffer_.mutable_cpu_data());\n   }\n   col_buff = col_buffer_.cpu_data();\n }\n // 使用caffe的cpu_gemm来进行计算  \n // 假设输入是20个feature map，输出是10个feature map，group_=2\n // 那么他就会把这个训练网络分解成两个10-&gt;5的网络，由于两个网络结构是\n // 一模一样的，那么就可以利用多个GPU完成训练加快训练速度\n for (int g = 0; g &lt; group_; ++g) {\n   caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans, CblasNoTrans, conv_out_channels_ /\n       group_, conv_out_spatial_dim_, kernel_dim_,\n       (Dtype)1., weights + weight_offset_ * g, col_buff + col_offset_ * g,\n       (Dtype)0., output + output_offset_ * g);\n   //weights &lt;--- blobs_[0]-&gt;cpu_data()。类比全连接层，\n   //weights为权重，col_buff相当与数据，矩阵相乘weights×col_buff. \n   //其中，weights的维度为(conv_out_channels_ /group_) x kernel_dim_，\n   //col_buff的维度为kernel_dim_ x conv_out_spatial_dim_， \n   //output的维度为(conv_out_channels_ /group_) x conv_out_spatial_dim_.\n }\n}</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>反向传播：</p><div class=\"highlight\"><pre><code class=\"language-text\">template &lt;typename Dtype&gt;\nvoid ConvolutionLayer&lt;Dtype&gt;::Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,\n     const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) {\n const Dtype* weight = this-&gt;blobs_[0]-&gt;cpu_data();\n Dtype* weight_diff = this-&gt;blobs_[0]-&gt;mutable_cpu_diff();\n for (int i = 0; i &lt; top.size(); ++i) {\n   const Dtype* top_diff = top[i]-&gt;cpu_diff();\n   const Dtype* bottom_data = bottom[i]-&gt;cpu_data();\n   Dtype* bottom_diff = bottom[i]-&gt;mutable_cpu_diff();\n   // Bias gradient, if necessary.\n   if (this-&gt;bias_term_ &amp;&amp; this-&gt;param_propagate_down_[1]) {\n     Dtype* bias_diff = this-&gt;blobs_[1]-&gt;mutable_cpu_diff();\n     for (int n = 0; n &lt; this-&gt;num_; ++n) {\n       this-&gt;backward_cpu_bias(bias_diff, top_diff + n * this-&gt;top_dim_);\n     }\n   }\n   if (this-&gt;param_propagate_down_[0] || propagate_down[i]) {\n     for (int n = 0; n &lt; this-&gt;num_; ++n) {\n       // gradient w.r.t. weight. Note that we will accumulate diffs.\n       if (this-&gt;param_propagate_down_[0]) {\n         this-&gt;weight_cpu_gemm(bottom_data + n * this-&gt;bottom_dim_,\n             top_diff + n * this-&gt;top_dim_, weight_diff);\n       }\n       // gradient w.r.t. bottom data, if necessary.\n       if (propagate_down[i]) {\n         this-&gt;backward_cpu_gemm(top_diff + n * this-&gt;top_dim_, weight,\n             bottom_diff + n * this-&gt;bottom_dim_);\n       }\n     }\n   }\n }</code></pre></div><p>略去bias，从上面源码可以看出，有this-&gt;weight_cpu_gemm和this-&gt;backward_cpu_gemm两项。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>this-&gt;backward_cpu_gemm是计算bottom_data的反向传播的，也就是feature map的反向传播。</b></p><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-text\">template &lt;typename Dtype&gt;\nvoid BaseConvolutionLayer&lt;Dtype&gt;::backward_cpu_gemm(const Dtype* output,\n   const Dtype* weights, Dtype* input) {\n Dtype* col_buff = col_buffer_.mutable_cpu_data();\n if (is_1x1_) {\n   col_buff = input;\n }\n for (int g = 0; g &lt; group_; ++g) {\n   caffe_cpu_gemm&lt;Dtype&gt;(CblasTrans, CblasNoTrans, kernel_dim_ / group_,\n       conv_out_spatial_dim_, conv_out_channels_ / group_,\n       (Dtype)1., weights + weight_offset_ * g, output + output_offset_ * g,\n       (Dtype)0., col_buff + col_offset_ * g);\n }\n if (!is_1x1_) {\n   conv_col2im_cpu(col_buff, input);</code></pre></div><p>}</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>weight_cpu_gemm是计算权重的反向传播的；</b></p><div class=\"highlight\"><pre><code class=\"language-text\">template &lt;typename Dtype&gt;\nvoid BaseConvolutionLayer&lt;Dtype&gt;::weight_cpu_gemm(const Dtype* input,\n   const Dtype* output, Dtype* weights) {\n const Dtype* col_buff = input;\n if (!is_1x1_) {\n   conv_im2col_cpu(input, col_buffer_.mutable_cpu_data());\n   col_buff = col_buffer_.cpu_data();\n }\n for (int g = 0; g &lt; group_; ++g) {\n   caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans, CblasTrans, conv_out_channels_ / group_,\n       kernel_dim_, conv_out_spatial_dim_,\n       (Dtype)1., output + output_offset_ * g, col_buff + col_offset_ * g,\n       (Dtype)1., weights + weight_offset_ * g);\n }\n}</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>其中诸多细节，看不懂就再去看源码，一次看不懂就看多次。</p><blockquote><b>3 deconv_layer.cpp</b></blockquote><p><b><a href=\"https://www.zhihu.com/question/63890195\" class=\"internal\"><span class=\"invisible\">https://www.</span><span class=\"visible\">zhihu.com/question/6389</span><span class=\"invisible\">0195</span><span class=\"ellipsis\"></span></a></b></p><p><b><a href=\"https://link.zhihu.com/?target=https%3A//buptldy.github.io/2016/10/29/2016-10-29-deconv/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">buptldy.github.io/2016/</span><span class=\"invisible\">10/29/2016-10-29-deconv/</span><span class=\"ellipsis\"></span></a></b></p><p class=\"ztext-empty-paragraph\"><br/></p><b><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-7c40d1c26524e3a7cb5c70bf244e438b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"244\" data-rawheight=\"259\" class=\"content_image\" width=\"244\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;244&#39; height=&#39;259&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"244\" data-rawheight=\"259\" class=\"content_image lazy\" width=\"244\" data-actualsrc=\"https://pic4.zhimg.com/v2-7c40d1c26524e3a7cb5c70bf244e438b_b.jpg\"/></figure></b><p class=\"ztext-empty-paragraph\"><br/></p><p>卷积，就是将下图转换为上图，一个输出像素，和9个输入像素有关。反卷积则反之，计算反卷积的时候，就是把上图输入的像素乘以卷积核，然后放在下图对应的输出各个位置，移动输入像素，最后把所有相同位置的输出相加。</p><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-text\">template &lt;typename Dtype&gt;\nvoid DeconvolutionLayer&lt;Dtype&gt;::Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,\n     const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {\n const Dtype* weight = this-&gt;blobs_[0]-&gt;cpu_data();\n for (int i = 0; i &lt; bottom.size(); ++i) {\n   const Dtype* bottom_data = bottom[i]-&gt;cpu_data();\n   Dtype* top_data = top[i]-&gt;mutable_cpu_data();\n   for (int n = 0; n &lt; this-&gt;num_; ++n) {\n     this-&gt;backward_cpu_gemm(bottom_data + n * this-&gt;bottom_dim_, weight,\n         top_data + n * this-&gt;top_dim_);\n     if (this-&gt;bias_term_) {\n       const Dtype* bias = this-&gt;blobs_[1]-&gt;cpu_data();\n       this-&gt;forward_cpu_bias(top_data + n * this-&gt;top_dim_, bias);\n     }\n   }\n }\n}</code></pre></div><p>forward直接调用了backward_cpu_gemm函数，反向的时候就直接调用forward函数，这里肯定是需要反复去理解的，一次不懂就多次。</p><div class=\"highlight\"><pre><code class=\"language-text\">template &lt;typename Dtype&gt;\nvoid DeconvolutionLayer&lt;Dtype&gt;::Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) {\n const Dtype* weight = this-&gt;blobs_[0]-&gt;cpu_data();\n Dtype* weight_diff = this-&gt;blobs_[0]-&gt;mutable_cpu_diff();\n for (int i = 0; i &lt; top.size(); ++i) {\n   const Dtype* top_diff = top[i]-&gt;cpu_diff();\n   const Dtype* bottom_data = bottom[i]-&gt;cpu_data();\n   Dtype* bottom_diff = bottom[i]-&gt;mutable_cpu_diff();\n   // Bias gradient, if necessary.\n   if (this-&gt;bias_term_ &amp;&amp; this-&gt;param_propagate_down_[1]) {\n     Dtype* bias_diff = this-&gt;blobs_[1]-&gt;mutable_cpu_diff();\n     for (int n = 0; n &lt; this-&gt;num_; ++n) {\n       this-&gt;backward_cpu_bias(bias_diff, top_diff + n * this-&gt;top_dim_);\n     }\n   }\n   if (this-&gt;param_propagate_down_[0] || propagate_down[i]) {\n     for (int n = 0; n &lt; this-&gt;num_; ++n) {\n       // Gradient w.r.t. weight. Note that we will accumulate diffs.\n       if (this-&gt;param_propagate_down_[0]) {\n         this-&gt;weight_cpu_gemm(top_diff + n * this-&gt;top_dim_,\n             bottom_data + n * this-&gt;bottom_dim_, weight_diff);\n       }\n       // Gradient w.r.t. bottom data, if necessary, reusing the column buffer\n       // we might have just computed above.\n       if (propagate_down[i]) {\n         this-&gt;forward_cpu_gemm(top_diff + n * this-&gt;top_dim_, weight,\n             bottom_diff + n * this-&gt;bottom_dim_,\n             this-&gt;param_propagate_down_[0]);\n       }\n     }\n   }</code></pre></div><blockquote><b>4 inner_product_layerfilter.hpp</b></blockquote><p>既然卷积层已经读过了，现在该读一读全连接层了。</p><p>全连接层和卷积层的区别是什么？就是没有局部连接，每一个输出都跟所有输入有关，如果输入feature map是H*W，那么去卷积它的核也是这么大，得到的输出是一个1*1的值。</p><p>它在setup函数里面要做一些事情，其中最重要的就是设定weights的尺寸，下面就是关键代码。num_output是一个输出标量数，比如imagenet1000类，最终输出一个1000维的向量。</p><p>K是一个样本的大小，当axis=1，实际上就是把每一个输入样本压缩成一个数，C*H*W经过全连接变成1个数。</p><div class=\"highlight\"><pre><code class=\"language-text\">const int num_output = this-&gt;layer_param_.inner_product_param().num_output();\nK_ = bottom[0]-&gt;count(axis);\n   // Initialize the weights\n   vector&lt;int&gt; weight_shape(2);\n   if (transpose_) {\n     weight_shape[0] = K_;\n     weight_shape[1] = N_;\n   } else {\n     weight_shape[0] = N_;\n     weight_shape[1] = K_;\n   }</code></pre></div><p>所以，weight的大小就是N*K_。</p><p>有了这个之后，forward就跟conv_layer是一样的了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>好了，这一节虽然没有复杂的公式，但是很多东西够大家喝一壶了，得仔细推敲才能好好理解的。caffe_cpu_gemm是整节计算的核心，感兴趣的去看吧！</p><p><b>更多请移步</b></p><blockquote>1，我的gitchat达人课</blockquote><a href=\"https://link.zhihu.com/?target=http%3A//gitbook.cn/gitchat/column/5a6011fbbd5ff2623773394c\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic4.zhimg.com/v2-778539fd0d72d5e46ebabd371db4df83_180x120.jpg\" data-image-width=\"1920\" data-image-height=\"1080\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">龙鹏的达人课</a><blockquote>2，AI技术公众号,《与有三学AI》</blockquote><a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649028681%26idx%3D1%26sn%3D44147bf8c9006c5d72dd51ed2f2871ba%26chksm%3D87134634b064cf22501feda3afff66beec38a09ec0ff91ac58c70a4f8c00da83b8ecfbf50175%23rd\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-2d9ceed78978badf52f685b50ced44c6_ipico.jpg\" data-image-width=\"358\" data-image-height=\"358\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">一文说说这十多年来计算机玩摄影的历史</a><blockquote>3，以及摄影号，《有三工作室》</blockquote><a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3MzczNzY0Mw%3D%3D%26mid%3D2648543628%26idx%3D1%26sn%3D5573c69f7b54acd6e2bfcc123d7a9988%26chksm%3D87234548b054cc5e1aed7177a537fef841768acff589a93d8234fab318bbe682a9da40192134%23rd\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-8a86f4fc840333cf9e0dea9544dc9dde_ipico.jpg\" data-image-width=\"640\" data-image-height=\"640\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">冯小刚说，“他懂我”</a><p></p>", 
            "topic": [
                {
                    "tag": "Caffe（深度学习框架）", 
                    "tagLink": "https://api.zhihu.com/topics/20019488"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "开源项目", 
                    "tagLink": "https://api.zhihu.com/topics/19565961"
                }
            ], 
            "comments": [
                {
                    "userName": "王引", 
                    "userLink": "https://www.zhihu.com/people/19fef76648aff443b4f3e8771012aec4", 
                    "content": "<p>void im2col_cpu,请问下这里的边界条件怎么理解呢</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "言有三-龙鹏", 
                            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
                            "content": "具体是指什么？", 
                            "likes": 0, 
                            "replyToAuthor": "王引"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/33458145", 
            "userName": "言有三-龙鹏", 
            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
            "upvote": 5, 
            "title": "【caffe教程4】caffe中的7大loss", 
            "content": "<p>本文首发于微信公众号《与有三学AI》</p><a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649028807%26idx%3D1%26sn%3Da56b840c8ac417388a513ebe1d84d4e3%26chksm%3D871346bab064cfacfb5d4ba30451b9f30da4e353009fcb024688647fcc4eb3d3b404c1cd4ef0%23rd\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-25ebd31d3f8c97e0429d01a94d0d1e2c_ipico.jpg\" data-image-width=\"225\" data-image-height=\"225\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">[caffe解读] caffe从数学公式到代码实现4-认识caffe自带的7大loss</a><blockquote>本节说caffe中常见loss的推导，具体包含下面的cpp。</blockquote><div class=\"highlight\"><pre><code class=\"language-text\">multinomial_logistic_loss_layer.cpp\nsoftmax_loss_layer.cpp\neuclidean_loss_layer.cpp\nsigmoid_cross_entropy_loss_layer.cpp\ncontrastive_loss_layer.cpp\nhinge_loss_layer.cpp\ninfogain_loss_layer.cpp\n</code></pre></div><blockquote><b>1 multinomial_logistic_loss_layer.cpp</b></blockquote><p><b>数学定义</b></p><p>x是输入，y是label，l是loss</p><p><img src=\"https://www.zhihu.com/equation?tex=l%28x%2Cy%29%3D-%E2%88%91_i%E2%88%91_%7Bk%3D0%7D%5ECy_k+log%28x_i%29\" alt=\"l(x,y)=-∑_i∑_{k=0}^Cy_k log(x_i)\" eeimg=\"1\"/> </p><p>上述式只有当样本i属于第k类时，<img src=\"https://www.zhihu.com/equation?tex=y_k\" alt=\"y_k\" eeimg=\"1\"/> =1，其他情况 <img src=\"https://www.zhihu.com/equation?tex=y_k\" alt=\"y_k\" eeimg=\"1\"/>=0，我们计不为0的 <img src=\"https://www.zhihu.com/equation?tex=y_k\" alt=\"y_k\" eeimg=\"1\"/> =y。 </p><p><img src=\"https://www.zhihu.com/equation?tex=%E2%88%82l%28x%2Cy%29%2F%E2%88%82x_i%3D-1%2Fx_i\" alt=\"∂l(x,y)/∂x_i=-1/x_i\" eeimg=\"1\"/> </p><p><b>forward &amp; backward</b></p><p>forward对所有 <img src=\"https://www.zhihu.com/equation?tex=x_i\" alt=\"x_i\" eeimg=\"1\"/> 求和，注意，此处没有对图像维度的像素进行归一化，只对batch size维度进行归一化，num就是batch size，这与下面的softmaxlosslayer是不一样的。</p><div class=\"highlight\"><pre><code class=\"language-text\">void MultinomialLogisticLossLayer&lt;Dtype&gt;::Forward_cpu(\nconst vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {\nconst Dtype* bottom_data = bottom[0]-&gt;cpu_data();\nconst Dtype* bottom_label = bottom[1]-&gt;cpu_data();\nint num = bottom[0]-&gt;num();\nint dim = bottom[0]-&gt;count() / bottom[0]-&gt;num();\nDtype loss = 0;\nfor (int i = 0; i &lt; num; ++i) {\nint label = static_cast&lt;int&gt;(bottom_label[i]);\nDtype prob = std::max( bottom_data[i * dim + label], Dtype(kLOG_THRESHOLD));\nloss -= log(prob);\n}\ntop[0]-&gt;mutable_cpu_data()[0] = loss / num;\n}</code></pre></div><p>backward可以自己去看，很简单就不说了</p><blockquote><b>2 softmax_layer.cpp</b></blockquote><p><b>数学定义</b></p><p>softmax是我们最熟悉的了，分类任务中使用它，分割任务中依然使用它。Softmax loss实际上是由softmax和cross-entropy loss组合而成，两者放一起数值计算更加稳定。</p><p>令z是softmax_with_loss层的输入，f(z)是softmax的输出，则</p><p><img src=\"https://www.zhihu.com/equation?tex=f%28z_k+%29%3De%5E%7B%28z_k+%29%7D%2F%28%E2%88%91_je%5E%7Bz_j+%7D+%29\" alt=\"f(z_k )=e^{(z_k )}/(∑_je^{z_j } )\" eeimg=\"1\"/> </p><p>单个像素i的softmax loss等于cross-entropy error如下</p><p><img src=\"https://www.zhihu.com/equation?tex=l%28y%2Cz%29%3D-%E2%88%91_%7B%28k%3D0%7D%5ECy_c+log%28f%28z_c%29%29\" alt=\"l(y,z)=-∑_{(k=0}^Cy_c log(f(z_c))\" eeimg=\"1\"/> </p><p>展开上式： </p><p><img src=\"https://www.zhihu.com/equation?tex=l%28y%2Cz%29%3Dlog%E2%88%91_je%5E%7B%28z_j+%29%7D+-loge%5E%7B%28z_y+%29%7D\" alt=\"l(y,z)=log∑_je^{(z_j )} -loge^{(z_y )}\" eeimg=\"1\"/> </p><p>在网络中，z是即bottom blob，l(y,z)是top blob,反向传播时，就是要根据top blob diff得到bottom blob diff，所以要得到 <img src=\"https://www.zhihu.com/equation?tex=%E2%88%82l%28y%2Cz%29%2F%E2%88%82z\" alt=\"∂l(y,z)/∂z\" eeimg=\"1\"/> </p><p>下面求loss对z的第k个节点的梯度</p><p><img src=\"https://www.zhihu.com/equation?tex=%E2%88%82l%28y%2Cz%29%2F%E2%88%82z%3Df%28z_y+%29-1%2C+when+y%3Dk%3B+f%28z_k+%29+else\" alt=\"∂l(y,z)/∂z=f(z_y )-1, when y=k; f(z_k ) else\" eeimg=\"1\"/> </p><p>可见，传给groundtruth label节点和非groundtruth label是的梯度是不一样的。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>forward就不看了，看看backward吧。</b></p><div class=\"highlight\"><pre><code class=\"language-text\">Dtype* bottom_diff = bottom[0]-&gt;mutable_cpu_diff();\nconst Dtype* prob_data = prob_.cpu_data();\ncaffe_copy(prob_.count(), prob_data, bottom_diff);\nconst Dtype* label = bottom[1]-&gt;cpu_data();\nint dim = prob_.count() / outer_num_;\nint count = 0;\nfor (int i = 0; i &lt; outer_num_; ++i) {\nfor (int j = 0; j &lt; inner_num_; ++j) {\n const int label_value = static_cast&lt;int&gt;(label[i * inner_num_ +\nj]);\nif (has_ignore_label_ &amp;&amp; label_value == ignore_label_) {\nfor (int c = 0; c &lt; bottom[0]-&gt;shape(softmax_axis_); ++c) {\nbottom_diff[i * dim + c * inner_num_ + j] = 0;\n}\n} else {\nbottom_diff[i * dim + label_value * inner_num_ + j] -= 1;\n++count;\n}\n}\n}</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><b>Test_softmax_with_loss_layer.cpp</b></p><p>作为loss层，很有必要测试一下，测试也分两块，forward和backward。</p><p>Forward测试是这样的，定义了个bottom blob data和bottom blob label，给data塞入高斯分布数据，给label塞入0～4。</p><div class=\"highlight\"><pre><code class=\"language-text\">blob_bottom_data_(new\nBlob&lt;Dtype&gt;(10, 5, 2, 3)),\nblob_bottom_label_(new\nBlob&lt;Dtype&gt;(10, 1, 2, 3)),</code></pre></div><p>然后分别ingore其中的一个label做5次，最后比较，代码如下。</p><div class=\"highlight\"><pre><code class=\"language-text\">Dtype accum_loss = 0;\nfor (int label = 0; label &lt; 5; ++label) {\nlayer_param.mutable_loss_param()-&gt;set_ignore_label(label);\nlayer.reset(new SoftmaxWithLossLayer&lt;Dtype&gt;(layer_param));\nlayer-&gt;SetUp(this-&gt;blob_bottom_vec_,\nthis-&gt;blob_top_vec_);\nlayer-&gt;Forward(this-&gt;blob_bottom_vec_, this-&gt;blob_top_vec_);\n accum_loss += this-&gt;blob_top_loss_-&gt;cpu_data()[0];\n}\n// Check that each label was included all but\nonce.\nEXPECT_NEAR(4 * full_loss, accum_loss, 1e-4);</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>至于backwards，直接套用checker.CheckGradientExhaustive就行，它自己会利用数值微分的方法和你写的backwards来比较精度。</p><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-text\">TYPED_TEST(SoftmaxWithLossLayerTest,\nTestGradientIgnoreLabel) {\ntypedef typename TypeParam::Dtype Dtype;\nLayerParameter layer_param;\n//\nlabels are in {0, ..., 4}, so we&#39;ll ignore about a fifth of them\nlayer_param.mutable_loss_param()-&gt;set_ignore_label(0);\nSoftmaxWithLossLayer&lt;Dtype&gt; layer(layer_param);\nGradientChecker&lt;Dtype&gt; checker(1e-2, 1e-2, 1701);\nchecker.CheckGradientExhaustive(&amp;layer, this-&gt;blob_bottom_vec_,\nthis-&gt;blob_top_vec_, 0);\n}</code></pre></div><blockquote><b>3 eulidean_loss_layer.cpp</b></blockquote><p>euclidean loss就是定位检测任务中常用的loss。</p><p><b>数学定义：</b></p><p><img src=\"https://www.zhihu.com/equation?tex=l%3D%E2%88%91_%7Bi%3D1%7D%5EN%7Cx_i%5E1-x_i%5E2+%7C%5E2%2F%282N%29\" alt=\"l=∑_{i=1}^N|x_i^1-x_i^2 |^2/(2N)\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%E2%88%82y%2F%E2%88%82x_i%5E1+%3D%28x_i%5E1-x_i%5E2%29%2FN\" alt=\"∂y/∂x_i^1 =(x_i^1-x_i^2)/N\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%E2%88%82y%2F%E2%88%82x_i%5E2+%3D-%28x_i%5E1-x_i%5E2%29%2FN\" alt=\"∂y/∂x_i^2 =-(x_i^1-x_i^2)/N\" eeimg=\"1\"/> </p><p><b>forward &amp; backward</b></p><div class=\"highlight\"><pre><code class=\"language-text\">void EuclideanLossLayer&lt;Dtype&gt;::Backward_cpu(const\nvector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,\nconst vector&lt;bool&gt;&amp; propagate_down, const\nvector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) {\nfor (int i = 0; i &lt; 2; ++i) {\nif (propagate_down[i]) {\nconst Dtype sign = (i == 0) ? 1 : -1;\nconst Dtype alpha = sign * top[0]-&gt;cpu_diff()[0] / bottom[i]-&gt;num();\ncaffe_cpu_axpby( bottom[i]-&gt;count(),  // count\nalpha, // alpha\ndiff_.cpu_data(), // a\nDtype(0), // beta\nbottom[i]-&gt;mutable_cpu_diff()); // b\n}\n}\n}</code></pre></div><p>testcpp就不说了。</p><p class=\"ztext-empty-paragraph\"><br/></p><blockquote><b>4 sigmoid_cross_entropy_loss_layer.cpp</b></blockquote><p>与softmax loss的应用场景不同，这个loss不是用来分类的，而是用于预测概率，所以在loss中，没有类别的累加项。</p><p>令第i个节点输入 <img src=\"https://www.zhihu.com/equation?tex=x_i\" alt=\"x_i\" eeimg=\"1\"/> ，输出总loss为l，label为 <img src=\"https://www.zhihu.com/equation?tex=y_i\" alt=\"y_i\" eeimg=\"1\"/> ，则loss定义如下</p><p><img src=\"https://www.zhihu.com/equation?tex=l%3D%28x%2Cy%29%3D-%E2%88%91_iy_i+log%28p_i%29%29%2B%281-y_i%29log%E2%81%A1%281-p_i%29\" alt=\"l=(x,y)=-∑_iy_i log(p_i))+(1-y_i)log⁡(1-p_i)\" eeimg=\"1\"/> </p><p>其中</p><p><img src=\"https://www.zhihu.com/equation?tex=p_i%3D1%2F%281%2Bexp%E2%81%A1%28-x_i%29%29\" alt=\"p_i=1/(1+exp⁡(-x_i))\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=p_i\" alt=\"p_i\" eeimg=\"1\"/> ， <img src=\"https://www.zhihu.com/equation?tex=y_i\" alt=\"y_i\" eeimg=\"1\"/> 都是处于0～1之间的值。</p><p>上式子有个等价转换，这是为了更好的理解caffe 中forward的计算，公司太多我就直接借用了，如果遇到了原作者请通知我添加转载申明。</p><p><a href=\"https://link.zhihu.com/?target=http%3A//blog.csdn.net/u012235274/article/details/51361290\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">blog.csdn.net/u01223527</span><span class=\"invisible\">4/article/details/51361290</span><span class=\"ellipsis\"></span></a></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-70203ed967a41892e424dfea23ffd2c6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1229\" data-rawheight=\"1325\" class=\"origin_image zh-lightbox-thumb\" width=\"1229\" data-original=\"https://pic3.zhimg.com/v2-70203ed967a41892e424dfea23ffd2c6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1229&#39; height=&#39;1325&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1229\" data-rawheight=\"1325\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1229\" data-original=\"https://pic3.zhimg.com/v2-70203ed967a41892e424dfea23ffd2c6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-70203ed967a41892e424dfea23ffd2c6_b.jpg\"/></figure><p>反向求导公式如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-b82b6446683207fa225fadbfe5d064eb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1206\" data-rawheight=\"148\" class=\"origin_image zh-lightbox-thumb\" width=\"1206\" data-original=\"https://pic4.zhimg.com/v2-b82b6446683207fa225fadbfe5d064eb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1206&#39; height=&#39;148&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1206\" data-rawheight=\"148\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1206\" data-original=\"https://pic4.zhimg.com/v2-b82b6446683207fa225fadbfe5d064eb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-b82b6446683207fa225fadbfe5d064eb_b.jpg\"/></figure><p>在经过上面的转换后，避开了数值计算不稳定的情况，caffe中的源码是对整个的bottom[0]-&gt;count进行计算累加的，没有区分label项。</p><div class=\"highlight\"><pre><code class=\"language-text\">for (int i = 0; i &lt; bottom[0]-&gt;count(); ++i) {\nconst int target_value = static_cast&lt;int&gt;(target[i]);\nif (has_ignore_label_ &amp;&amp; target_value == ignore_label_) {\ncontinue;\n}\nloss -= input_data[i] * (target[i] - (input_data[i] &gt;= 0)) - log(1 + exp(input_data[i] - 2 * input_data[i] * (input_data[i] &gt;= 0)));\n++valid_count;\n}</code></pre></div><p>反向传播很简单就不看了</p><p class=\"ztext-empty-paragraph\"><br/></p><blockquote><b>5 contrastive_loss_layer.cpp</b></blockquote><p>有一类网络叫siamese network，它的输入是成对的。比如输入两张大小相同的图，网络输出计算其是否匹配。所采用的损失函数就是contrastive loss</p><p><b>数学定义：</b></p><p><img src=\"https://www.zhihu.com/equation?tex=l%3D%E2%88%91_n%28yd%5E2%2B%281-y%29max%E2%81%A1%28margin-d%2C0%29%5E2+%29\" alt=\"l=∑_n(yd^2+(1-y)max⁡(margin-d,0)^2 )\" eeimg=\"1\"/> </p><p>d就是欧氏距离，y是标签，如果两个样本匹配，则为1，否则为0. 当y=1，loss就是欧氏距离，说明匹配的样本距离越大，loss越大。当y=0，是，就是与阈值margin的欧式距离，说明不匹配的样本，欧氏距离应该越大越好，超过阈值最好，loss就等于0.</p><p class=\"ztext-empty-paragraph\"><br/></p><p>反向传播其实就是分y=1和y=0两种情况下的euclidean loss的反向传导，由于与euclidean<br/>loss非常相似，不再赘述。</p><p class=\"ztext-empty-paragraph\"><br/></p><blockquote><b>6 hinge_loss_layer.cpp</b></blockquote><p>这是一个多分类的loss, 也是SVM的目标函数，没有学习参数的全连接层InnerProductLayer+HingeLossLayer就等价于SVM。</p><p><b>数学定义</b></p><p><img src=\"https://www.zhihu.com/equation?tex=E%3D%E2%88%91_%7Bn%3D1%7D%5EN%E2%88%91_k%5EK%7Bmax%E2%81%A1%280%2C1-%CE%B4_%7Bln%3Dk%7Dt_%7Bnk%7D%7D%29%5Ep+%2FN\" alt=\"E=∑_{n=1}^N∑_k^K{max⁡(0,1-δ_{ln=k}t_{nk}})^p /N\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%CE%B4%7Bcondition%7D%3D%7B1%2Cwhen+condition+is+true%2C-1%2Cwhen+condition+is+false%7D\" alt=\"δ{condition}={1,when condition is true,-1,when condition is false}\" eeimg=\"1\"/> </p><p>这个层的输入bottom[0]是一个N*C*H*W的blob，其中取值任意值，label就是N*1*1*1，其中存储的就是整型的label{0,1,2,…,k}。 <img src=\"https://www.zhihu.com/equation?tex=t_%7Bnk%7D\" alt=\"t_{nk}\" eeimg=\"1\"/> 相当于SVM中的 <img src=\"https://www.zhihu.com/equation?tex=X%5ETW\" alt=\"X^TW\" eeimg=\"1\"/> 。</p><p>参考博客<a href=\"https://link.zhihu.com/?target=http%3A//blog.leanote.com/post/braveapple/Hinge-Loss-%25E7%259A%2584%25E7%2590%2586%25E8%25A7%25A3\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">blog.leanote.com/post/b</span><span class=\"invisible\">raveapple/Hinge-Loss-%E7%9A%84%E7%90%86%E8%A7%A3</span><span class=\"ellipsis\"></span></a></p><p class=\"ztext-empty-paragraph\"><br/></p><p>假如预测类别数是K个，正确的label是M，预测值为 <img src=\"https://www.zhihu.com/equation?tex=t_%7Bnk%7D\" alt=\"t_{nk}\" eeimg=\"1\"/> ，即是第n个样本对第k类的预测值，那么当k=M时。</p><p><img src=\"https://www.zhihu.com/equation?tex=max%E2%81%A1%280%2C1-%CE%B4_%7Bln%3Dk%7Dt_%7Bnk%7D%29%3Dmax%280%2C1-t_%7Bnk%7D%29%5Ep\" alt=\"max⁡(0,1-δ_{ln=k}t_{nk})=max(0,1-t_{nk})^p\" eeimg=\"1\"/> </p><p>当k!=M时，</p><p><img src=\"https://www.zhihu.com/equation?tex=max%E2%81%A1%280%2C1-%CE%B4_%7Bln%3Dk%7Dt_%7Bnk%7D%29%3Dmax%280%2C1%2Bt_%7Bnk%7D%29%5Ep\" alt=\"max⁡(0,1-δ_{ln=k}t_{nk})=max(0,1+t_{nk})^p\" eeimg=\"1\"/> </p><p>其中p=1，p=2分别对应L1范数和L2范数，以L1为例</p><p><img src=\"https://www.zhihu.com/equation?tex=loss%3D%3D+%E2%88%91_%7B%28k%E2%89%A0K%29%7D%5EKmax%E2%81%A1%280%2C1%2Btk%29%2Bmax%E2%81%A1%280%2C1-tk%29\" alt=\"loss== ∑_{(k≠K)}^Kmax⁡(0,1+tk)+max⁡(0,1-tk)\" eeimg=\"1\"/> </p><p><b>Forward</b></p><div class=\"highlight\"><pre><code class=\"language-text\">caffe_copy(count, bottom_data, bottom_diff);\nfor (int i = 0; i &lt; num; ++i) { bottom_diff[i * dim + static_cast&lt;int&gt;(label[i])] *= -1;\n}\nfor (int i = 0; i &lt; num; ++i) {\nfor (int j = 0; j &lt; dim; ++j) {\nbottom_diff[i * dim + j] = std::max( Dtype(0), 1 + bottom_diff[i * dim + j]);\n}\n}\nDtype* loss = top[0]-&gt;mutable_cpu_data();\nswitch (this-&gt;layer_param_.hinge_loss_param().norm()) {\ncase HingeLossParameter_Norm_L1:\nloss[0] = caffe_cpu_asum(count, bottom_diff) / num; break;\ncase HingeLossParameter_Norm_L2:\nloss[0] = caffe_cpu_dot(count, bottom_diff, bottom_diff) / num; break;\ndefault: LOG(FATAL) &lt;&lt; &#34;Unknown Norm&#34;;\n}</code></pre></div><p>只需要根据上面的转化后的式子，在正确label处乘以-1，然后累加即可。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>再看反向梯度求导：</p><p>当进行一次forward之后，</p><p>bottom_diff=[max(0,1+t0),max(0,1+t1),...,max(0,1−tk),...,max(0,1−tK-1)]</p><p>我们现在要求梯度，期望是1+tk&gt;0时为1，1-tk&gt;0时为-1，其他情况为0，</p><p>当任意一项1+tk或者1-tk&lt;0时，会有梯度=0。实际上就是求上面向量各自元素的符号，当1+tk&gt;0，sign(1+tk)，只是max(0,1−tk)这个应该反过来，当1-tk&gt;0时，sign(tk-1)=-1。</p><p>代码如下：</p><div class=\"highlight\"><pre><code class=\"language-text\">Dtype* bottom_diff = bottom[0]-&gt;mutable_cpu_diff();\nconst Dtype* label = bottom[1]-&gt;cpu_data();\nint num = bottom[0]-&gt;num();\nint count = bottom[0]-&gt;count();\nint dim = count / num;\nfor (int i = 0; i &lt; num; ++i) {\nbottom_diff[i * dim + static_cast&lt;int&gt;(label[i])] *= -1;\n}\nconst Dtype loss_weight = top[0]-&gt;cpu_diff()[0];\nswitch (this-&gt;layer_param_.hinge_loss_param().norm()) {\ncase HingeLossParameter_Norm_L1:\ncaffe_cpu_sign(count, bottom_diff, bottom_diff);\ncaffe_scal(count, loss_weight / num, bottom_diff); \nbreak;\ncase HingeLossParameter_Norm_L2:\ncaffe_scal(count, loss_weight * 2 / num, bottom_diff);\nbreak;\ndefault: LOG(FATAL) &lt;&lt; &#34;Unknown Norm&#34;;\n}</code></pre></div><p>}</p><blockquote><b>7 infogain_loss_layer.cpp</b></blockquote><p>数学定义：</p><p>输入bottom_data是N*C*H*W维向量，bottom_label是N*1*1*1维向量，存储的就是类别数。它还有个可选的bottom[2]，是一个infogain matrix矩阵,它的维度等于num_of_label * num_of_label。每个通道c预测的是第c类的概率，取值0～1，所有c个通道的概率相加=1。是不是像soft Max？</p><p>实际上它内部就定义了shared_ptr&lt;Layer&lt;Dtype&gt; &gt; softmax_layer_，用于映射输入。</p><p>Loss定义如下：</p><p><img src=\"https://www.zhihu.com/equation?tex=l%3D-%E2%88%91_%7Bn%3D1%7D%5ENH_%7Bln%7Dlog%E2%81%A1%28p_n+%29%2FN\" alt=\"l=-∑_{n=1}^NH_{ln}log⁡(p_n )/N\" eeimg=\"1\"/> = <img src=\"https://www.zhihu.com/equation?tex=E%3D-%E2%88%91_%7Bn%3D1%7D%5EN%E2%88%91_%7Bk%3D1%7D%5EKH_%7Bln%2Ck%7Dlog%E2%81%A1%28p_%7Bnk%7D+%29%2FN\" alt=\"E=-∑_{n=1}^N∑_{k=1}^KH_{ln,k}log⁡(p_{nk} )/N\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%E2%88%82l%2F%E2%88%82z_n%3D%E2%88%91_%7Bk%3D1%7D%5EKH_%7Bln%2Ck%7D+%E2%88%82+log%E2%81%A1%28%28p_%7Bnk%7D%29%2F%E2%88%82z_n%29%2FN\" alt=\"∂l/∂z_n=∑_{k=1}^KH_{ln,k} ∂ log⁡((p_{nk})/∂z_n)/N\" eeimg=\"1\"/> </p><p>其中 <img src=\"https://www.zhihu.com/equation?tex=p_%7Bnk%7D+%3De%5E%7Bz_k+%7D%2F%28%E2%88%91_je%5E%7Bz_j+%7D+%29\" alt=\"p_{nk} =e^{z_k }/(∑_je^{z_j } )\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=H_%7Bln%7D\" alt=\"H_{ln}\" eeimg=\"1\"/> 代表H的第ln行，K是所有类别数，如果H是一个单位矩阵，那么只有对角线有值，回到文章开头，这就是multinomial_logistic_loss_layer。当H是一个普通矩阵时，当groundtruth label为k， <img src=\"https://www.zhihu.com/equation?tex=H_%7Bln%2Ck%27%7D%EF%BC%8Ck%27%5Cne%7Bk%7D\" alt=\"H_{ln,k&#39;}，k&#39;\\ne{k}\" eeimg=\"1\"/> 时，值也可以非零。这样各个类别之间就不存在竞争关系了，后来的mask-rcnn中实际上loss也就是去除了这重竞争关系。</p><p><b>Forward：</b></p><div class=\"highlight\"><pre><code class=\"language-text\">void InfogainLossLayer&lt;Dtype&gt;::Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,\nconst vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {\nsoftmax_layer_-&gt;Forward(softmax_bottom_vec_, softmax_top_vec_);\nconst Dtype* prob_data = prob_.cpu_data();\nconst Dtype* bottom_label = bottom[1]-&gt;cpu_data();\nconst Dtype* infogain_mat = NULL;\nif (bottom.size() &lt; 3) { infogain_mat = infogain_.cpu_data();\n} else { infogain_mat = bottom[2]-&gt;cpu_data();}\nint count = 0;\nDtype loss = 0;\nfor (int i = 0; i &lt; outer_num_; ++i) {\nfor (int j = 0; j &lt; inner_num_; j++) {\nconst int label_value = static_cast&lt;int&gt;(bottom_label[i * inner_num_ + j]);\nif (has_ignore_label_ &amp;&amp; label_value == ignore_label_) {continue;}\nDCHECK_GE(label_value, 0);\nDCHECK_LT(label_value, num_labels_);\nfor (int l = 0; l &lt; num_labels_; l++)\n{  loss -= infogain_mat[label_value * num_labels_ + l] * log(std::max( prob_data[i * inner_num_*num_labels_ + l * inner_num_ + j],\nDtype(kLOG_THRESHOLD)));}\n++count; } }\ntop[0]-&gt;mutable_cpu_data()[0] = loss / get_normalizer(normalization_, count);\nif (top.size() == 2) { top[1]-&gt;ShareData(prob_); }}</code></pre></div><p>上面与multinomial_logistic_loss_layer的区别就在于每一项乘了infogain_mat[label_value *<br/>num_labels_ + l]。</p><p><b>Backward:</b></p><div class=\"highlight\"><pre><code class=\"language-text\">for (int l = 0; l &lt; num_labels_; ++l) {\n    bottom_diff[i * dim + l * inner_num_ + j] = prob_data[i*dim + l*inner_num_ + j]*sum_rows_H[label_value] - infogain_mat[label_value * num_labels_ + l];\n   }</code></pre></div><p>这个，看了4篇了大家不妨自己推一推？不行咱再一起来</p><p><b>更多请移步</b></p><blockquote>1，我的gitchat达人课</blockquote><a href=\"https://link.zhihu.com/?target=http%3A//gitbook.cn/gitchat/column/5a6011fbbd5ff2623773394c\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic4.zhimg.com/v2-778539fd0d72d5e46ebabd371db4df83_180x120.jpg\" data-image-width=\"1920\" data-image-height=\"1080\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">龙鹏的达人课</a><blockquote>2，AI技术公众号,《与有三学AI》</blockquote><a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649028681%26idx%3D1%26sn%3D44147bf8c9006c5d72dd51ed2f2871ba%26chksm%3D87134634b064cf22501feda3afff66beec38a09ec0ff91ac58c70a4f8c00da83b8ecfbf50175%23rd\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-2d9ceed78978badf52f685b50ced44c6_ipico.jpg\" data-image-width=\"358\" data-image-height=\"358\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">一文说说这十多年来计算机玩摄影的历史</a><blockquote>3，以及摄影号，《有三工作室》</blockquote><a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3MzczNzY0Mw%3D%3D%26mid%3D2648543628%26idx%3D1%26sn%3D5573c69f7b54acd6e2bfcc123d7a9988%26chksm%3D87234548b054cc5e1aed7177a537fef841768acff589a93d8234fab318bbe682a9da40192134%23rd\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-8a86f4fc840333cf9e0dea9544dc9dde_ipico.jpg\" data-image-width=\"640\" data-image-height=\"640\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">冯小刚说，“他懂我”</a><p></p><p></p><p></p><p></p><p></p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "Caffe（深度学习框架）", 
                    "tagLink": "https://api.zhihu.com/topics/20019488"
                }, 
                {
                    "tag": "C++", 
                    "tagLink": "https://api.zhihu.com/topics/19584970"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/33379727", 
            "userName": "言有三-龙鹏", 
            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
            "upvote": 2, 
            "title": "【caffe教程3】caffe中的数据shape相关类", 
            "content": "<p>本文首发于微信公众号《与有三学AI》</p><a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649028805%26idx%3D1%26sn%3D859d6dfb04761f82c957e408e2b5fab7%26chksm%3D871346b8b064cfae55bfd1b3bd659da309b038a06b63b7b371c63a9fb28f7ad5887fa707f3e7%23rd\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-25ebd31d3f8c97e0429d01a94d0d1e2c_ipico.jpg\" data-image-width=\"225\" data-image-height=\"225\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">[caffe解读] caffe从数学公式到代码实现3-shape相关类</a><p>接着上一篇说，本篇开始读layers下面的一些与blob shape有关的layer，比如flatten_layer.cpp等，具体包括的在下面；</p><div class=\"highlight\"><pre><code class=\"language-text\">flatten_layer.cpp\nslice_layer.cpp\nsplit_layer.cpp\ntile_layer.cpp\nconcat_layer.cpp\nreduction_layer.cpp\neltwise_layer.cpp\ncrop_layer.cpp\npooling_layer.cpp\nscale_layer.cpp</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>conv与deconv虽然也与shape有关，但是由于比较复杂，我们以后专门留一篇来说。下面这些层，<b>如果你没有仔细读过源码，那么建议你来读一读，因为有很多并没有想象中那么简单</b>。</p><blockquote><b>1 flatten_layer.cpp</b></blockquote><p>Flatten layer的作用是把一个维度为n * c * h * w的输入转化为一个维度为 n* (c*h*w)的向量输出，虽然在我们看来不一样，但是在blob看来，输入和输出的数据存储是没有差异的，只是记录的shape信息不同。所以forward和backward只是数据拷贝</p><div class=\"highlight\"><pre><code class=\"language-text\">template &lt;typename Dtype&gt;\nvoid FlattenLayer&lt;Dtype&gt;::Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,\n const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) { top[0]-&gt;ShareData(*bottom[0]);\n}\n\ntemplate &lt;typename Dtype&gt;\nvoid FlattenLayer&lt;Dtype&gt;::Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,\nconst vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) {\nbottom[0]-&gt;ShareDiff(*top[0]);\n}</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><blockquote><b>2 slice_layer.cpp</b></blockquote><p>Slice layer 的作用是将bottom按照需要分解成多个tops，它的定义如下：</p><div class=\"highlight\"><pre><code class=\"language-text\">message SliceParameter {\n// By default, SliceLayer\nconcatenates blobs along the &#34;channels&#34; axis (1).\noptional int32 axis = 3 [default = 1];\nrepeated uint32 slice_point = 2;\n// DEPRECATED: alias for &#34;axis&#34; -- does not support negative indexing.\noptional uint32 slice_dim = 1 [default = 1];\n}</code></pre></div><p>默认axis是1，也就是blob的第1个维度，即channel通道，这也是我经常使用的，一般用于有多种label时分离label。</p><p>前向反向时小心计算好offset就行，有兴趣可以去看。</p><p class=\"ztext-empty-paragraph\"><br/></p><blockquote><b>3 split_layer.cpp</b></blockquote><p>它的作用是将输入复制多份。</p><p>Forward: 在前向的时候，top[i]=bottom[0]，直接赋值。</p><div class=\"highlight\"><pre><code class=\"language-text\">template &lt;typename Dtype&gt;\nvoid SplitLayer&lt;Dtype&gt;::Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,\nconst vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {\nfor (int i = 0; i &lt; top.size(); ++i) {\n   top[i]-&gt;ShareData(*bottom[0]);\n}\n}</code></pre></div><p>Backward: 在反向的时候，需要将所有top的diff叠加起来。</p><div class=\"highlight\"><pre><code class=\"language-text\">template &lt;typename Dtype&gt;\nvoid SplitLayer&lt;Dtype&gt;::Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,\nconst vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) {\nif (!propagate_down[0]) { return; }\nif (top.size() == 1) { caffe_copy(count_, top[0]-&gt;cpu_diff(), bottom[0]-&gt;mutable_cpu_diff());\nreturn;\n}\ncaffe_add(count_, top[0]-&gt;cpu_diff(), top[1]-&gt;cpu_diff(),\nbottom[0]-&gt;mutable_cpu_diff());\n// Add remaining top blob diffs.\nfor (int i = 2; i &lt; top.size(); ++i) {\nconst Dtype* top_diff = top[i]-&gt;cpu_diff();\nDtype* bottom_diff = bottom[0]-&gt;mutable_cpu_diff();\ncaffe_axpy(count_, Dtype(1.), top_diff, bottom_diff);\n}\n}</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><blockquote><b>4 tile_layer.cpp</b></blockquote><p><b>数学定义:</b></p><p>将数据按照某个维度扩大n倍，看下面forward源码，将bottom_data的前inner_dim_个数据复制了tiles份，反向时将对应diff累加回去即可。</p><div class=\"highlight\"><pre><code class=\"language-text\">Forward，backward与split layer很像。\nvoid TileLayer&lt;Dtype&gt;::Forward_cpu(\n   const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {\n const Dtype* bottom_data = bottom[0]-&gt;cpu_data();\n Dtype* top_data = top[0]-&gt;mutable_cpu_data();\n for (int i = 0; i &lt; outer_dim_; ++i) {\n   for (int t = 0; t &lt; tiles_; ++t) {\n     caffe_copy(inner_dim_, bottom_data, top_data);\n     top_data += inner_dim_;\n   }\n   bottom_data += inner_dim_;\n }\n}</code></pre></div><blockquote><b>5 </b>c<b>oncat_layer.cpp</b></blockquote><p>与slice_layer是反向操作，将多个bottom blob合并成一个top_data，forward，backward计算好index就行。</p><p class=\"ztext-empty-paragraph\"><br/></p><blockquote><b>6 reduction_layer.cpp</b></blockquote><p>顾名思义，这是一个降维的层。</p><p><b>数学定义:</b></p><div class=\"highlight\"><pre><code class=\"language-text\">message ReductionParameter {\nenum ReductionOp {\nSUM = 1;\nASUM = 2;\nSUMSQ = 3;\nMEAN = 4;\n}\noptional ReductionOp operation = 1 [default = SUM]; // reduction operation The first axis to reduce to a scalar -- may be negative to index from the end (e.g., -1 for the last axis). (Currently, only reduction along ALL &#34;tail&#34; axes is supported;\nreduction of axis M through N, where N &lt; num_axes - 1, is unsupported.)\nSuppose we have an n-axis bottom Blob with shape:\n(d0, d1, d2, ..., d(m-1), dm, d(m+1), ..., d(n-1)).\nIf axis == m, the output Blob will have shape  (d0, d1, d2, ..., d(m-1)),\nand the ReductionOp operation is performed (d0 * d1 * d2 * ... * d(m-1))\ntimes, each including (dm * d(m+1) * ... * d(n-1)) individual data.\nIf axis == 0 (the default), the output Blob always has the empty shape\n(count 1), performing reduction across the entire input often useful for creating new loss functions.\noptional int32 axis = 2 [default = 0];\noptional float coeff = 3 [default = 1.0]; // coefficient for output\n}</code></pre></div><p>从上面可以看出，reduct有4类操作，sum，mean，asum，sumsq，分别是求和，求绝对值和，求平方和与平均。它会从axis这个维度开始去降维，比如当axis=0，就是从0开始将所有blob降维，最终会得到一个标量数，常用于loss。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在reshape函数中可以看到，</p><div class=\"highlight\"><pre><code class=\"language-text\">axis_ = bottom[0]-&gt;CanonicalAxisIndex(this-&gt;layer_param_.reduction_param().axis());\nvector&lt;int&gt; top_shape(bottom[0]-&gt;shape().begin(), bottom[0]-&gt;shape().begin() + axis_);\ntop[0]-&gt;Reshape(top_shape);\nnum_ = bottom[0]-&gt;count(0, axis_);\ndim_ = bottom[0]-&gt;count(axis_);\nCHECK_EQ(num_, top[0]-&gt;count());</code></pre></div><p>通过reduction_param().axis())设置维度之后，top[0]的元素数目就是num_ =<br/>bottom[0]-&gt;count(0, axis_);我们假设输入blob是10*3*224*224，如果axis=0，那么top[0]=10*1*1*1；如果axis=1，那么top[0]=10*3*1*1，以此类推。</p><p>Forward和Backward对应这4个操作去看代码即可，只要知道反向的时候，top的每一个元素的梯度会反传给bottom的多个元素。</p><p class=\"ztext-empty-paragraph\"><br/></p><blockquote><b>7 eltwise_layer.cpp</b></blockquote><p>eltwise是一个有多个bottom输入，一个top输出的layer，对逐个的元素进行操作，所bottom[i]和top[j]的大小都是相等的。Eltwise参数有相乘PROB，相加SUM，求MAX。对于SUM操作，该层定义了 coeff 参数，用于调整权重。 对于PROB操作，设定了stable_prod_grad #[default = true ] 来选择是否渐进较慢的梯度计算方法，forward过程不需要说太多，而对于backward，有必要说一下。下面举prob操作的例子；</p><p><img src=\"https://www.zhihu.com/equation?tex=f%28x%29%3Dx_1+x_2+%E2%80%A6x_n\" alt=\"f(x)=x_1 x_2 …x_n\" eeimg=\"1\"/> </p><p>则 <img src=\"https://www.zhihu.com/equation?tex=%E2%88%82f%28x%29%2Fx_i%3Dx_1+x_2+%E2%80%A6x_%7Bi-1%7D+x_%7Bi%2B1%7D...x_n\" alt=\"∂f(x)/x_i=x_1 x_2 …x_{i-1} x_{i+1}...x_n\" eeimg=\"1\"/> </p><p>我们看相应函数，这只是内循环，实际上还有外循环。</p><div class=\"highlight\"><pre><code class=\"language-text\">case EltwiseParameter_EltwiseOp_PROD:\nif (stable_prod_grad_) { bool initialized = false;\nfor (int j = 0; j &lt; bottom.size(); ++j) {\nif (i == j) { continue; }\nif (!initialized) { \ncaffe_copy(count, bottom[j]-&gt;cpu_data(), bottom_diff);\ninitialized = true;\n} else {\ncaffe_mul(count, bottom[j]-&gt;cpu_data(), bottom_diff,\nbottom_diff);\n}\n}\n} else {\ncaffe_div(count, top_data, bottom_data, bottom_diff);\n}\ncaffe_mul(count, bottom_diff, top_diff, bottom_diff);</code></pre></div><p>当stable_prod_grad = false时，直接对应了上面的式 top_data/bottom_data*bottom_diff，但是如果stable_prod_grad = true，差异在哪呢？反正我是没看出啥区别，只是为true时没有利用已经计算号的结果，计算更慢了。</p><blockquote><b>8 crop_layer.cpp</b></blockquote><p>crop layer改变blob的第2，3个维度，而不是改变前两个维度，也没有复杂的数学操作，所以只需要记录下offset即可，感兴趣还是去看源码。</p><blockquote><b>9 pooling_layer.cpp</b></blockquote><p>pooling layer想必大家都很熟悉了，caffe官方的有MAX，MEAN两种，还保留了一种random的没有实现。 <b>Max和Mean的区别会在什么地方呢？主要就是max会存在一个mask，因为它要记录对top有贡献的那个元素，在梯度反传的时候，也只会反传到1个元素，而mean则会反传到r*r个元素，r就是滤波的半径</b>。</p><p>其他的倒是没有需要特别注意的地方，主要就是bottom到top到index到计算，细节处小心即可。</p><blockquote><b>10 bnll_layer.cpp</b></blockquote><p><b>数学定义:</b></p><p><img src=\"https://www.zhihu.com/equation?tex=f%28x%29%3D%7B%28x%2Blog%E2%81%A1%281%2Bexp%E2%81%A1%28-x%29%29+when+x%3E0%3B+log%E2%81%A1%281%2Bexp%E2%81%A1%28x%29+%29+when+x%3C0%29%7D\" alt=\"f(x)={(x+log⁡(1+exp⁡(-x)) when x&gt;0; log⁡(1+exp⁡(x) ) when x&lt;0)}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%E2%88%82f%28x%29%3Dexp%E2%81%A1%28x%29%2F%281%2Bexp%E2%81%A1%28x%29%29\" alt=\"∂f(x)=exp⁡(x)/(1+exp⁡(x))\" eeimg=\"1\"/> </p><p>就这么多。</p><blockquote><b>11 scale_layer.cpp</b></blockquote><p>scale这个layer绝对比你想象中复杂多。我们通常以为是这样就完了</p><p><img src=\"https://www.zhihu.com/equation?tex=f%28x%29%3D%E2%81%A1ax\" alt=\"f(x)=⁡ax\" eeimg=\"1\"/> </p><p>其中a是一个标量，x是一个矢量，在caffe中就是blob，但是实际上a也可以是blob，它可以有如下尺寸，见scale参数的定义：</p><div class=\"highlight\"><pre><code class=\"language-text\">message ScaleParameter {\n// The first axis of bottom[0] (the first input Blob) along which to apply, bottom[1] (the second input Blob). May be negative to index from the end (e.g., -1 for the last\naxis).\n// For example, if bottom[0] is 4D with shape 100x3x40x60, the output top[0] will have the same shape, and bottom[1] may have any of the following shapes (for the given value of axis): \n(axis == 0 == -4) 100; 100x3; 100x3x40; 100x3x40x60\n(axis == 1 == -3) 3; 3x40; 3x40x60\n(axis == 2 == -2) 40; 40x60\n(axis == 3 == -1) 60\n// Furthermore, bottom[1] may have the empty shape (regardless of the value of &#34;axis&#34;) a scalar multiplier.\noptional int32 axis = 1 [default = 1];\n// (num_axes is ignored unless just one bottom is given and the scale is a learned parameter of the layer. Otherwise, num_axes is determined by the number of axes by the second bottom.)\n// The number of axes of the input (bottom[0]) covered by the scale\n// parameter, or -1 to cover all axes of bottom[0] starting from `axis`.\n// Set num_axes := 0, to\nmultiply with a zero-axis Blob: a scalar.\noptional int32 num_axes = 2 [default = 1];\n// (filler is ignored unless just one bottom is given and the scale is\n// a learned parameter of the layer.)\n// The initialization for the learned scale parameter.\n// Default is the unit (1) initialization, resulting in the ScaleLayer\n// initially performing the identity operation.\noptional FillerParameter filler = 3;\n\n// Whether to also learn a bias (equivalent to a ScaleLayer+BiasLayer, but\n// may be more efficient). Initialized with bias_filler (defaults to 0).\noptional bool bias_term = 4 [default = false];\noptional FillerParameter bias_filler = 5;\n}</code></pre></div><p>从上面我们可以知道这些信息；</p><p>（1） scale_layer是输入输出可以都是1个，但是，输入可以是两个，也就是bottom[1]是scale，当没有bottom[1]时，就是通过一个标量参数来实现scale。</p><p>（2） scale可以有多种尺寸。从1维到4维。</p><p>上面举了例子，当输入x是100x3x40x60，scale blob可以是100; 100x3;<br/>100x3x40; 100x3x40x60这几种尺寸，所以在forward，backward的时候，需要对上尺寸。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>这一节看起来比较乱，就当读书笔记吧，只是有很多细节，真的需要自己去抠才知道坑在哪。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>更多请移步</b></p><blockquote>1，我的gitchat达人课</blockquote><a href=\"https://link.zhihu.com/?target=http%3A//gitbook.cn/gitchat/column/5a6011fbbd5ff2623773394c\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic4.zhimg.com/v2-778539fd0d72d5e46ebabd371db4df83_180x120.jpg\" data-image-width=\"1920\" data-image-height=\"1080\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">龙鹏的达人课</a><blockquote>2，AI技术公众号,《与有三学AI》</blockquote><a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649028807%26idx%3D1%26sn%3Da56b840c8ac417388a513ebe1d84d4e3%26chksm%3D871346bab064cfacfb5d4ba30451b9f30da4e353009fcb024688647fcc4eb3d3b404c1cd4ef0%23rd\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-25ebd31d3f8c97e0429d01a94d0d1e2c_ipico.jpg\" data-image-width=\"225\" data-image-height=\"225\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">[caffe解读] caffe从数学公式到代码实现4-认识caffe自带的7大loss</a><blockquote>3，以及摄影号，《有三工作室》</blockquote><a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3MzczNzY0Mw%3D%3D%26mid%3D2648543628%26idx%3D1%26sn%3D5573c69f7b54acd6e2bfcc123d7a9988%26chksm%3D87234548b054cc5e1aed7177a537fef841768acff589a93d8234fab318bbe682a9da40192134%23rd\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-8a86f4fc840333cf9e0dea9544dc9dde_ipico.jpg\" data-image-width=\"640\" data-image-height=\"640\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">冯小刚说，“他懂我”</a><p></p><p></p><p></p>", 
            "topic": [
                {
                    "tag": "Caffe（深度学习框架）", 
                    "tagLink": "https://api.zhihu.com/topics/20019488"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "代码", 
                    "tagLink": "https://api.zhihu.com/topics/19559575"
                }
            ], 
            "comments": [
                {
                    "userName": "孙攀", 
                    "userLink": "https://www.zhihu.com/people/d711cdb2cab264f4b37d65f0cbc8222d", 
                    "content": "<p><b>reduction_layer的 </b>top[0]的元素数目就是num_ =</p><p>bottom[0]-&gt;count(0, axis_);我们假设输入blob是10*3*224*224，如果axis=0，那么top[0]=10*1*1*1；如果axis=1，那么top[0]=10*3*1*1，以此类推。</p><p><br></p><p>应该是top[0]=10和top[0]=10*3吧，源代码里</p><p>vector&lt;int&gt; top_shape(bottom[0]-&gt;shape().begin(),</p><p>                        bottom[0]-&gt;shape().begin() + axis_);</p><p>  top[0]-&gt;Reshape(top_shape);</p><p><br></p><p>top[0]的shape就是axis维度前的shape呀</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/33332648", 
            "userName": "言有三-龙鹏", 
            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
            "upvote": 3, 
            "title": "【caffe教程2】caffe中的基础函数", 
            "content": "<p>本文首发于微信公众号《与有三学AI》</p><a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649028781%26idx%3D1%26sn%3D4b8dc2c6098fdd143a3e0c10d1b7e1eb%26chksm%3D871346d0b064cfc6fa80afc8b63d9c9eec446dc38bbca15fee977b01edf3d03b00627e69a0f0%23rd\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-2d9ceed78978badf52f685b50ced44c6_ipico.jpg\" data-image-width=\"358\" data-image-height=\"358\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">[caffe解读] caffe从数学公式到代码实现2-基础函数类</a><p>接着上一篇，本篇就开始读layers下面的cpp，先看一下layers下面都有哪些cpp。</p><div class=\"highlight\"><pre><code class=\"language-text\">absval_layer.cpp\naccuracy_layer.cpp\nargmax_layer.cpp\nbase_conv_layer.cpp\nbase_data_layer.cpp\nbatch_norm_layer.cpp\nbatch_reindex_layer.cpp\nbias_layer.cpp\nbnll_layer.cpp\nconcat_layer.cpp\ncontrastive_loss_layer.cpp\nconv_layer.cpp\ncrop_layer.cpp\ncudnn_conv_layer.cpp\ncudnn_lcn_layer.cpp\ncudnn_lrn_layer.cpp\ncudnn_pooling_layer.cpp\ncudnn_relu_layer.cpp\ncudnn_sigmoid_layer.cpp\ncudnn_softmax_layer.cpp\ncudnn_tanh_layer.cpp\ndata_layer.cpp\ndeconv_layer.cpp\ndropout_layer.cpp\ndummy_data_layer.cpp\neltwise_layer.cpp\nelu_layer.cpp\nembed_layer.cpp\neuclidean_loss_layer.cpp\nexp_layer.cpp\nfilter_layer.cpp\nflatten_layer.cpp\nhdf5_data_layer.cpp\nhdf5_output_layer.cpp\nhinge_loss_layer.cpp\nim2col_layer.cpp\nimage_data_layer.cpp\ninfogain_loss_layer.cpp\ninner_product_layer.cpp\ninput_layer.cpp\nlog_layer.cpp\nloss_layer.cpp\nlrn_layer.cpp\nlstm_layer.cpp\nlstm_unit_layer.cpp\nmemory_data_layer.cpp\nmultinomial_logistic_loss_layer.cpp\nmvn_layer.cpp\nneuron_layer.cpp\nparameter_layer.cpp\npooling_layer.cpp\npower_layer.cpp\nprelu_layer.cpp\nrecurrent_layer.cpp\nreduction_layer.cpp\nrelu_layer.cpp\nreshape_layer.cpp\nrnn_layer.cpp\nscale_layer.cpp\nsigmoid_cross_entropy_loss_layer.cpp\nsigmoid_layer.cpp\nsilence_layer.cpp\nslice_layer.cpp\nsoftmax_layer.cpp\nsoftmax_loss_layer.cpp\nsplit_layer.cpp\nspp_layer.cpp\ntanh_layer.cpp\nthreshold_layer.cpp\ntile_layer.cpp\nwindow_data_layer.cpp</code></pre></div><p>其中，下吗这些layer是不需要反向传播的，大部分都是io类，我们就不讲了，自己去看。</p><div class=\"highlight\"><pre><code class=\"language-text\">threshold_layer.cpp\naccuracy_layer.cpp\nargmax_layer.cpp\ndata_layer.cpp\nimage_data_layer.cpp\ninput_layer.cpp\nwindow_data_layer.cpp\nparameter_layer.cpp\nmemory_data_layer.cpp\ndummy_data_layer.cpp\nhdf5_data_layer.cpp\nhdf5_output_layer.cpp\nneuron_layer.cpp\nsilence_layer.cpp\nreshape_layer.cpp\nrnn_layer.cpp\nbase_data_layer.cpp</code></pre></div><p>剩下的就是要讲的，我们先从官方的开始看，后面再看自己写的以及一些开源的。这些layers大概有这么几大类，基础数学函数类，blob shape操作类，loss类。</p><p>本节先看一些基础函数类的layer，都只有一个输入，一个输出。注意其中有一些是容许inplace 的layer，有一些是不容许的。所谓inplace，输入输出共用一块内存，在layer的传播过程中，直接覆盖，省内存。Caffe在开源框架中，是比较占内存的了。</p><blockquote><b>1 absval_layer.cpp</b></blockquote><p><b>数学定义:</b></p><p><img src=\"https://www.zhihu.com/equation?tex=f%28x%29%3D%7Cx%7C\" alt=\"f(x)=|x|\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%E2%88%82f%28x%29%29%2F%E2%88%82x%3D%7B1%2C+x%3E0+%3B+-1%2Cx%3C0%7D\" alt=\"∂f(x))/∂x={1, x&gt;0 ; -1,x&lt;0}\" eeimg=\"1\"/> </p><p>由于这是第一个例子，我们说的详细些；</p><p><b>Forward:</b></p><div class=\"highlight\"><pre><code class=\"language-text\">template &lt;typename Dtype&gt;\nvoid AbsValLayer&lt;Dtype&gt;::Forward_cpu(\nconst vector&lt;Blob&lt;Dtype&gt;*&gt;&amp;\nbottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {\nconst int count = top[0]-&gt;count();\nDtype* top_data = top[0]-&gt;mutable_cpu_data();\ncaffe_abs(count, bottom[0]-&gt;cpu_data(),\ntop_data);\n}</code></pre></div><p>其中，count是blob的size，等于N*C*H*W，bottom[0]是输入x，top[0]是输出f(x)，利用mutable_cpu_data来写入，cpu_data来读取。</p><p><b>Backward:</b></p><div class=\"highlight\"><pre><code class=\"language-text\">template &lt;typename Dtype&gt;\nvoid AbsValLayer&lt;Dtype&gt;::Backward_cpu(const\nvector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,\nconst vector&lt;bool&gt;&amp;\npropagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) {\nconst int count = top[0]-&gt;count();\nconst Dtype* top_diff = top[0]-&gt;cpu_diff();\nif (propagate_down[0]) {\nconst Dtype* bottom_data = bottom[0]-&gt;cpu_data();\n  Dtype* bottom_diff = bottom[0]-&gt;mutable_cpu_diff();\n  caffe_cpu_sign(count, bottom_data, bottom_diff);\n  caffe_mul(count, bottom_diff, top_diff,\nbottom_diff);\n}\n}</code></pre></div><p>根据梯度下降法和链式法则，</p><p>一次标准的梯度更新过程如下，wt+1=wt+Δwt，对于sgd算法，其中 wt=−η⋅gt ，w为参数，t为时序，Δ为更新量，η为学习率，g为梯度</p><p>其中梯度g就是</p><p><img src=\"https://www.zhihu.com/equation?tex=%E2%88%87x%3D%E2%88%82f%28x%29%2F%E2%88%82x+%E2%88%87f%28x%29\" alt=\"∇x=∂f(x)/∂x ∇f(x)\" eeimg=\"1\"/> </p><p>在backward中，我们只需要计算出即可，至于上面的符号，学习率等在其他地方处理，其实就是</p><p><img src=\"https://www.zhihu.com/equation?tex=%E2%88%87bottom%3D%E2%88%82top%28bottom%29%2F%E2%88%82bottom+%E2%88%87top%28bottom%29\" alt=\"∇bottom=∂top(bottom)/∂bottom ∇top(bottom)\" eeimg=\"1\"/> </p><p>其中其实top_diff，就是对应：</p><p>const Dtype* top_diff = top[0]-&gt;cpu_diff();</p><p>在这里我们知道了，cpu_data就是bottom的data，而cpu_diff就是它存储的梯度，有疑问可以返回上一篇。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>propagate_down是一个参数，用于控制是否容许梯度下传的，</p><p>caffe_cpu_sign(count, bottom_data, bottom_diff);实际上就是计算了梯度，</p><p>再利用caffe_mul(count, bottom_diff, top_diff, bottom_diff);</p><p>就得到了<img src=\"https://www.zhihu.com/equation?tex=%E2%88%87x\" alt=\"∇x\" eeimg=\"1\"/> </p><p>没有对应的test文件，就不解析了。</p><blockquote><b>2 exp_layer.cpp</b></blockquote><p>看下caffe 关于其参数的定义：</p><p><b>数学定义:</b></p><div class=\"highlight\"><pre><code class=\"language-text\">// Message that stores parameters used by ExpLayer\nmessage ExpParameter {\n// ExpLayer computes\noutputs y = base ^ (shift + scale * x), for base &gt; 0.\n// Or if base is set to\nthe default (-1), base is set to e,\n// so y = exp(shift + scale * x).\noptional float base = 1\n[default = -1.0];\noptional float scale = 2\n[default = 1.0];\noptional float shift = 3\n[default = 0.0];</code></pre></div><p>从下面的setuplayer中可以看出，如果base不是-1，则必须是大于0的数，也就是-2，-3等是不支持的。</p><div class=\"highlight\"><pre><code class=\"language-text\">const Dtype base = this-&gt;layer_param_.exp_param().base();\nif (base != Dtype(-1)) {\n  CHECK_GT(base, 0) &lt;&lt; &#34;base must be strictly positive.&#34;;\n}</code></pre></div><p><img src=\"https://www.zhihu.com/equation?tex=f%28x%29%3Dbase%5E%7B%CE%B1x%2B%CE%B2%7D\" alt=\"f(x)=base^{αx+β}\" eeimg=\"1\"/> </p><p>当base=-1，也就是默认时f(x)=e^{αx+β}，就是我们熟悉的指数函数了</p><p>还记得指数函数求导吧；</p><p><img src=\"https://www.zhihu.com/equation?tex=%E2%88%82f%28x%29%3Dln%E2%81%A1%28a%29a%5E%7Bx%7D\" alt=\"∂f(x)=ln⁡(a)a^{x}\" eeimg=\"1\"/> </p><blockquote><b>3 log_layer.cpp</b></blockquote><p><b>数学定义:</b></p><p><img src=\"https://www.zhihu.com/equation?tex=f%28x%29%3Dlog_%7Bbase%7D%28%CE%B1x%2B%5Cbeta%29\" alt=\"f(x)=log_{base}(αx+\\beta)\" eeimg=\"1\"/> </p><p>同样base=-1是默认值，否则必须大于0 </p><p><img src=\"https://www.zhihu.com/equation?tex=%E2%88%82f%28x%29%3Dlog%28base%29%2F%28%CE%B1x%2B%CE%B2%29\" alt=\"∂f(x)=log(base)/(αx+β)\" eeimg=\"1\"/> </p><blockquote>4 power_layer.cpp</blockquote><p><b>数学定义:</b></p><p><img src=\"https://www.zhihu.com/equation?tex=f%28x%29%3D%28%CE%B1x%2B%CE%B2%29%5Ep\" alt=\"f(x)=(αx+β)^p\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%E2%88%82f%28x%29%3D%CE%B1p%28%CE%B1x%2B%CE%B2%29%5E%7Bp-1%7D\" alt=\"∂f(x)=αp(αx+β)^{p-1}\" eeimg=\"1\"/> </p><p>梯度也是很简单的，不过为了提高计算效率，caffe尽可能的复用了中间结果，尤其是在反向传播的时候，分两种case，完整的计算大家还是去看代码，这里粘代码太难受了。</p><blockquote>5 tanh_layer.cpp</blockquote><p><b>数学定义:</b></p><p><img src=\"https://www.zhihu.com/equation?tex=f%28x%29%3Dtanh%28x%29%3D%28e%5Ex-e%5E%7B-x%7D%29%2F%28e%5Ex%2Be%5E%7B-x%7D+%29\" alt=\"f(x)=tanh(x)=(e^x-e^{-x})/(e^x+e^{-x} )\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%E2%88%82f%28x%29%3D1-tanh%28x%29%2Atanh%E2%81%A1%28x%29\" alt=\"∂f(x)=1-tanh(x)*tanh⁡(x)\" eeimg=\"1\"/> </p><p><b>这一次咱们遇到有test layer，仔细说说。</b></p><p>在caffe/test目录下test_tanh_layer.cpp</p><p>所谓测试，就是要验证网络的正向和反向。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>这个文件是这样测试的：</p><p>先定义了个tanh_naïve函数，然后利用GaussianFille初始化一个bottom，将其通过forward函数，把出来的结果和tanh_naïve的结果进行比对，完整代码如下，感受一下：</p><div class=\"highlight\"><pre><code class=\"language-text\">void TestForward(Dtype filler_std) {\n  FillerParameter filler_param;\n  filler_param.set_std(filler_std);\n  GaussianFiller&lt;Dtype&gt; filler(filler_param);\n  filler.Fill(this-&gt;blob_bottom_);\n\n  LayerParameter layer_param;\n  TanHLayer&lt;Dtype&gt; layer(layer_param);\n  layer.SetUp(this-&gt;blob_bottom_vec_,\nthis-&gt;blob_top_vec_);\n  layer.Forward(this-&gt;blob_bottom_vec_,\nthis-&gt;blob_top_vec_);\n// Now, check values\nconst Dtype* bottom_data = this-&gt;blob_bottom_-&gt;cpu_data();\nconst Dtype* top_data = this-&gt;blob_top_-&gt;cpu_data();\nconst Dtype min_precision = 1e-5;\nfor (int i = 0; i &lt; this-&gt;blob_bottom_-&gt;count();\n++i) {\n    Dtype expected_value =\ntanh_naive(bottom_data[i]);\n    Dtype precision = std::max(\n      Dtype(std::abs(expected_value * Dtype(1e-4))),\nmin_precision);\n    EXPECT_NEAR(expected_value, top_data[i],\nprecision);\n  }\n}</code></pre></div><p>EXPECT_NEAR函数就会检查梯度是否正确，如果过不了，就得回去看forward函数是否有错了。</p><p>反向验证：</p><div class=\"highlight\"><pre><code class=\"language-text\">void TestBackward(Dtype filler_std) {\n  FillerParameter filler_param;\n  filler_param.set_std(filler_std);\n  GaussianFiller&lt;Dtype&gt; filler(filler_param);\n  filler.Fill(this-&gt;blob_bottom_);\n\n  LayerParameter layer_param;\n  TanHLayer&lt;Dtype&gt; layer(layer_param);\n  GradientChecker&lt;Dtype&gt; checker(1e-2, 1e-2, 1701);\n  checker.CheckGradientEltwise(&amp;layer, this-&gt;blob_bottom_vec_,\nthis-&gt;blob_top_vec_);\n}</code></pre></div><p>其中GradientChecker(const Dtype stepsize, const Dtype threshold,</p><p>const unsigned int seed = 1701, const Dtype kink = 0.,const Dtype kink_range = -1)</p><p>可以设置stepwise和误差阈值，CheckGradientEltwise是逐个像素检查。</p><blockquote><b>6 sigmoid_layer.cpp</b></blockquote><p><b>数学定义:</b></p><p><img src=\"https://www.zhihu.com/equation?tex=f%28x%29%3Dsigmoid%28x%29%3D1%2F%281%2Be%5E%7B-x%7D+%29\" alt=\"f(x)=sigmoid(x)=1/(1+e^{-x} )\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%E2%88%82f%28x%29%3D%5B1-sigmoid%28x%29%5D%2Asigmoid%28x%29\" alt=\"∂f(x)=[1-sigmoid(x)]*sigmoid(x)\" eeimg=\"1\"/> </p><blockquote><b>7 relu_layer.cpp</b></blockquote><p><b>数学定义:</b></p><p><img src=\"https://www.zhihu.com/equation?tex=f%28x%29%3D%28x%2C+when+x%3E0%3B+ax+%2C+when+x%3C0%29\" alt=\"f(x)=(x, when x&gt;0; ax , when x&lt;0)\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%E2%88%82f%28x%29%3D%281%2C+when+x%3E0%3B+a%2C+when+x%3C0%29\" alt=\"∂f(x)=(1, when x&gt;0; a, when x&lt;0)\" eeimg=\"1\"/> </p><p>其中negative_slope a默认=0，退化为f(x)=max(x,0)</p><p>上面的relu其实包含了我们常说的relu和ReLU和LeakyReLU</p><blockquote>8 prelu_layer.cpp</blockquote><p>与LeakyReLU不同的是，负号部分的参数a是可学习的并不固定。所以，在反向传播时，该参数需要求导，默认a=0.25。</p><p><b>数学定义</b></p><p><img src=\"https://www.zhihu.com/equation?tex=f%28x%29%3D%28x%2C+when+x%3E0%3B+ax+%2C+when+x%3C0%29\" alt=\"f(x)=(x, when x&gt;0; ax , when x&lt;0)\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%E2%88%82f%28x%29%3D%281%2C+when+x%3E0%3B+a%2Cwhen+x%3C0%29\" alt=\"∂f(x)=(1, when x&gt;0; a,when x&lt;0)\" eeimg=\"1\"/> </p><p>此处的a是个变量。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>首先，对x也就是bottom的求导</p><p><img src=\"https://www.zhihu.com/equation?tex=%E2%88%87x%3D%E2%88%87y%5B%28x%3E0%29%2Ba%28x%3C0%29%5D\" alt=\"∇x=∇y[(x&gt;0)+a(x&lt;0)]\" eeimg=\"1\"/> </p><p>代码如下</p><div class=\"highlight\"><pre><code class=\"language-text\">if (propagate_down[0]) {\nDtype* bottom_diff = bottom[0]-&gt;mutable_cpu_diff();\nfor (int i = 0; i &lt; count; ++i) {\nint c = (i / dim) % channels / div_factor;\nbottom_diff[i] = top_diff[i] * ((bottom_data[i] &gt; 0)\n+ slope_data[c] * (bottom_data[i] &lt;= 0));\n}\n}</code></pre></div><p>而scale参数的求导，则会稍微复杂些，如下</p><div class=\"highlight\"><pre><code class=\"language-text\">if (this-&gt;param_propagate_down_[0]) {\n  Dtype* slope_diff = this-&gt;blobs_[0]-&gt;mutable_cpu_diff();\nfor (int i = 0; i &lt; count; ++i) {\nint c = (i / dim) % channels / div_factor;\n    slope_diff[c] += top_diff[i] * bottom_data[i]\n* (bottom_data[i] &lt;= 0);\n  }\n}</code></pre></div><p>因为对于blob中第i个数据， 当i不等于k时，yi 与xk是没有关系的，但是a却与blob中的所有数据有关系。</p><p>我们重新表述一下</p><p><img src=\"https://www.zhihu.com/equation?tex=%E2%88%82f%28a%29%3D%5Csum_%7Bi%7D%5E%7B%7D%7Bx_i%7D%28x_i%3C0%29+\" alt=\"∂f(a)=\\sum_{i}^{}{x_i}(x_i&lt;0) \" eeimg=\"1\"/> </p><p><b>9 elu_layer.cpp</b></p><p><b>数学定义:</b></p><p><img src=\"https://www.zhihu.com/equation?tex=f%28x%2Ca%29%3D%7B%28x%2C+when+x%3E0%3B+a%28exp%28x%29-1%29+%2C+when+x%3C0%29%7D\" alt=\"f(x,a)={(x, when x&gt;0; a(exp(x)-1) , when x&lt;0)}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%E2%88%82f%28x%2Ca%29%3D%7B%281+%2Cwhen+x%3E0%3B+f%28x%2Ca%29%2Ba%2C+when+x%3C0%29%7D\" alt=\"∂f(x,a)={(1 ,when x&gt;0; f(x,a)+a, when x&lt;0)}\" eeimg=\"1\"/> </p><p>1<b>0 bnll_layer.cpp</b></p><p>数学定义:</p><p><img src=\"https://www.zhihu.com/equation?tex=f%28x%29+%3D+%7B%28x%2Blog%E2%81%A1%281%2Bexp%E2%81%A1%28-x%29%29+when+x%3E0%3B+log%E2%81%A1%281%2Bexp%E2%81%A1%28x%29+%29+when+x%3C0%7D\" alt=\"f(x) = {(x+log⁡(1+exp⁡(-x)) when x&gt;0; log⁡(1+exp⁡(x) ) when x&lt;0}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%E2%88%82f%28x%29%3Dexp%E2%81%A1%28x%29%2F%281%2Bexp%E2%81%A1%28x%29%29\" alt=\"∂f(x)=exp⁡(x)/(1+exp⁡(x))\" eeimg=\"1\"/> </p><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-text\">知乎的公式编辑实在不行，严重打击了我写以后的文章的积极性</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><b>更多请移步</b></p><blockquote>1，我的gitchat达人课</blockquote><a href=\"https://link.zhihu.com/?target=http%3A//gitbook.cn/gitchat/column/5a6011fbbd5ff2623773394c\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic4.zhimg.com/v2-778539fd0d72d5e46ebabd371db4df83_180x120.jpg\" data-image-width=\"1920\" data-image-height=\"1080\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">龙鹏的达人课</a><blockquote>2，AI技术公众号,《与有三学AI》</blockquote><a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649028805%26idx%3D1%26sn%3D859d6dfb04761f82c957e408e2b5fab7%26chksm%3D871346b8b064cfae55bfd1b3bd659da309b038a06b63b7b371c63a9fb28f7ad5887fa707f3e7%23rd\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-25ebd31d3f8c97e0429d01a94d0d1e2c_ipico.jpg\" data-image-width=\"225\" data-image-height=\"225\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">[caffe解读] caffe从数学公式到代码实现3-shape相关类</a><blockquote>3，以及摄影号，《有三工作室》</blockquote><a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3MzczNzY0Mw%3D%3D%26mid%3D2648543628%26idx%3D1%26sn%3D5573c69f7b54acd6e2bfcc123d7a9988%26chksm%3D87234548b054cc5e1aed7177a537fef841768acff589a93d8234fab318bbe682a9da40192134%23rd\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-8a86f4fc840333cf9e0dea9544dc9dde_ipico.jpg\" data-image-width=\"640\" data-image-height=\"640\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">冯小刚说，“他懂我”</a><p></p><p></p><p></p>", 
            "topic": [
                {
                    "tag": "Caffe（深度学习框架）", 
                    "tagLink": "https://api.zhihu.com/topics/20019488"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "代码", 
                    "tagLink": "https://api.zhihu.com/topics/19559575"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/33314378", 
            "userName": "言有三-龙鹏", 
            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
            "upvote": 9, 
            "title": "【caffe教程1】 caffe代码导论", 
            "content": "<p>本文首发于微信公众号《与有三学AI》</p><a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649028759%26idx%3D1%26sn%3D7f7260f530cabb950a6191b0002dfbed%26chksm%3D871346eab064cffc4be460b750ad57270d603a8874c3bed63d9bde463d115cb811297167767d%23rd\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-25ebd31d3f8c97e0429d01a94d0d1e2c_ipico.jpg\" data-image-width=\"225\" data-image-height=\"225\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">[caffe解读] caffe从数学公式到代码实现1-导论</a><blockquote><i>我这个系列caffe代码解读跟大部分人的思路不一样，一般读caffe代码思路是按照caffe的层级结构来，blob到layer到net各自分层来读，但我想提供一个另外的思路，从数学公式到代码实现。</i></blockquote><p class=\"ztext-empty-paragraph\"><br/></p><p>从每一个文件背后具体的数学含义来读，这对于我们非数学系或者数学基础不是很好的工程人员来说，是比较适合的。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>那么，我会采取什么样的形式呢，就是，layer definition，caffe layer，caffe test layer的格式，举个例子来说，比如softmax，那我就打算从softmax的数学定义，caffe softmax层的实现，caffe softmax test layer的实现。一定要加上test layer，因为当我们自己实现某些类时，往往需要梯度反向求导，这时候最好自己写test来验证自己的代码是否正确。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>好了，下面就开始吧。当然，现在这是第一篇，所以我们还是不可避免先打下基础，先要阅读下面的内容，对caffe的代码有基本的了解。这是include/caffe下面的代码list。</p><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-text\">blob.hpp\ncaffe.hpp\ncommon.hpp\ndata_transformer.hpp\nfiller.hpp\ninternal_thread.hpp\nlayer.hpp\nlayer_factory.hpp\nnet.hpp\nparallel.hpp\nsgd_solvers.hpp\nsolver.hpp\nsolver_factory.hpp\nsyncedmem.hpp</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>一个一个来。</p><blockquote><b>1 blob.hpp&amp;cpp</b></blockquote><p>blob是caffe中的基础数据单元，一个blob是一个四维张量，（N，C，H，W），N是batch size大小，C是channel，H，W分别是图像宽高，由于caffe擅长于做图像，所以这个定义天然适合图像。故一个256*256的rgb图像，blob size是（1,3,256,256）。</p><p>blob.hpp，需要注意的就是下面的变量和函数</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-e42a5ae6e567ff3df88366b805a52dc3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1038\" data-rawheight=\"360\" class=\"origin_image zh-lightbox-thumb\" width=\"1038\" data-original=\"https://pic4.zhimg.com/v2-e42a5ae6e567ff3df88366b805a52dc3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1038&#39; height=&#39;360&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1038\" data-rawheight=\"360\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1038\" data-original=\"https://pic4.zhimg.com/v2-e42a5ae6e567ff3df88366b805a52dc3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-e42a5ae6e567ff3df88366b805a52dc3_b.jpg\"/></figure><p>其中data_存储数据，diff_存储梯度，shape_分别是blob_的尺度，count_是所有数据数目，即N*C*H*W。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>以后要访问这些数据，就会用到下面的函数，其中cpu_data是只读，mutable_cpt_data是可写，gpu类似。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-93dedb578a8ecab8effe22423ba70a6b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"870\" data-rawheight=\"580\" class=\"origin_image zh-lightbox-thumb\" width=\"870\" data-original=\"https://pic4.zhimg.com/v2-93dedb578a8ecab8effe22423ba70a6b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;870&#39; height=&#39;580&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"870\" data-rawheight=\"580\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"870\" data-original=\"https://pic4.zhimg.com/v2-93dedb578a8ecab8effe22423ba70a6b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-93dedb578a8ecab8effe22423ba70a6b_b.jpg\"/></figure><p>上面还有一个疑问，那就是初次见到SyncedMemory类会不知道它是做什么的，它主要负责在GPU或者CPU上分配内存以及保持数据的同步作用。</p><p>可参考下面资料。</p><p><a href=\"https://link.zhihu.com/?target=http%3A//blog.csdn.net/xizero00/article/details/51001206\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">blog.csdn.net/xizero00/</span><span class=\"invisible\">article/details/51001206</span><span class=\"ellipsis\"></span></a></p><p><a href=\"https://link.zhihu.com/?target=http%3A//www.cnblogs.com/korbin/p/5606770.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">cnblogs.com/korbin/p/56</span><span class=\"invisible\">06770.html</span><span class=\"ellipsis\"></span></a></p><p>由于展开是另一个篇幅，因此我们不过多停留在此，知道blob是通过这样的方式存取即可。</p><p class=\"ztext-empty-paragraph\"><br/></p><blockquote><b>2 </b>caffe.hpp，common.hpp，internal_thread.hpp, parallel.hpp, syncedmem.hpp，solver_factory.hpp，layer_factory.hpp，sgd_solvers.hpp</blockquote><p>把这几个放这里，是因为其中一些是gpu编程和内存等较为底层的编程的，看起来比较费劲，我们一般的应用其实也不需要对此有太深了解，大家会用即可。另外还有sovler这个类大家仔细读读即可。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>caffe.hpp包含其他基础hpp。</p><p>internal_thread.hpp,与线程有关的变量函数。</p><p>parallel.hpp,与并行有关的变量函数。</p><p>syncedmem.hpp，内存分配和Caffe的底层数据的切换</p><p>solver_factory.hpp，layer_factory.hpp，顾名思义，分别是caffe solver的工厂类模板定义和普通layer的模板定义。</p><p>举例拿sovler来多说几句，solver_factory.hpp，其中solver指的是优化方法，由于caffe优化采用的就是梯度下降的方法，包括SGD，NesterovSolver，RMSPropSolver，AdamSolver等通通都定义在sgd_solvers.hpp中。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>工厂设计模型，简单了解如下</p><p><a href=\"https://link.zhihu.com/?target=http%3A//developer.51cto.com/art/201107/277728.htm\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">developer.51cto.com/art</span><span class=\"invisible\">/201107/277728.htm</span><span class=\"ellipsis\"></span></a></p><p><a href=\"https://link.zhihu.com/?target=http%3A//alanse7en.github.io/caffedai-ma-jie-xi-4/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">alanse7en.github.io/caf</span><span class=\"invisible\">fedai-ma-jie-xi-4/</span><span class=\"ellipsis\"></span></a></p><p class=\"ztext-empty-paragraph\"><br/></p><p>深入了解需要自己去看，从代码的角度来看就是解决重复造轮子的问题，减少重复代码，在caffe的面试中经常会问到噢。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>看下它的代码，重要变量两个</p><p>typedef Solver&lt;Dtype&gt;* (*Creator)(const<br/>SolverParameter&amp;);</p><p>typedef std::map&lt;string, Creator&gt;<br/>CreatorRegistry;</p><p>重要函数两个个，</p><div class=\"highlight\"><pre><code class=\"language-text\">static CreatorRegistry&amp; Registry() { static CreatorRegistry*\ng_registry_ = new CreatorRegistry();\n   return *g_registry_;\n}\nstatic Solver&lt;Dtype&gt;* CreateSolver(const\nSolverParameter&amp; param) {\n   const string&amp; type = param.type();\n   CreatorRegistry&amp; registry = Registry();\n   CHECK_EQ(registry.count(type), 1) &lt;&lt; &#34;Unknown solver type: &#34; &lt;&lt; type\n       &lt;&lt; &#34; (known types: &#34; &lt;&lt; SolverTypeListString() &lt;&lt; &#34;)&#34;;\n   return registry[type](param);\n }</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>其中需要注意的是，SolverParameter是一个配置参数不说，CreatorRegistry就是我们以后自定义层需要知道的，需要知道registry是一个map，存储的就是字符串以及对应的以函数指针形式存储的Creator类型的函数，而注册都会在cpp中进行，以后详解。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>common.hpp，是一些与io有关的函数与变量，cpu与gpu模式设定变量Brew<br/>mode_;函数set_mode，setDevice，以及与随机数有关的函数变量shared_ptr&lt;RNG&gt;<br/>random_generator_;</p><p class=\"ztext-empty-paragraph\"><br/></p><blockquote><b>3 datatransform.hpp</b></blockquote><p>这是很重要的一个文件，当我们自定义数据层的时候会用到，它的作用就是从磁盘中读取数据塞进caffe定义的变量内存中。从它的头文件就可以看出，它依赖于blob,common,以及caffe.pb.h</p><div class=\"highlight\"><pre><code class=\"language-text\">#include &#34;caffe/blob.hpp&#34;\n#include &#34;caffe/common.hpp&#34;\n#include &#34;caffe/proto/caffe.pb.h&#34;</code></pre></div><p>caffe.pb.h中就包含了序列化的变量。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>datatransform.hpp中的变量如下：</p><div class=\"highlight\"><pre><code class=\"language-text\">shared_ptr&lt;Caffe::RNG&gt; rng_;\nPhase phase_;\nBlob&lt;Dtype&gt; data_mean_;\nvector&lt;Dtype&gt; mean_values_;</code></pre></div><p>可见存储了常见的mean_value。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>datatransform.hpp中的的核心是重载的transform函数，它可以按照不同的输入来载入数据，我们平常在caffe内部做的随机crop，flip等等操作都在这里完成，具体大家可以去研究源码，静下心看非常简单。</p><div class=\"highlight\"><pre><code class=\"language-text\">void Transform(const vector&lt;Datum&gt; &amp; datum_vector,Blob&lt;Dtype&gt;* transformed_blob);\nvoid Transform(const vector&lt;cv::Mat&gt; &amp; mat_vector, Blob&lt;Dtype&gt;*\ntransformed_blob);\nvoid Transform(const cv::Mat&amp; cv_img, Blob&lt;Dtype&gt;* transformed_blob);</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><blockquote><b>4 filler.hpp</b></blockquote><p>它没有对应的cpp，所有实现都在hpp中，因为很简单，它就是对权重初始化的，其中包含，constantfiller，Gaussianfiller，XavierFiller，MSRAFiller等等，相信大家都比较熟了。</p><p class=\"ztext-empty-paragraph\"><br/></p><blockquote><b>5 solver.hpp</b></blockquote><p>这就是caffe 迭代求解优化的函数定义，其中重要变量loss就在这里，这就是训练caffe时显示出的loss的来源</p><div class=\"highlight\"><pre><code class=\"language-text\">vector&lt;Dtype&gt; losses_;\n Dtype smoothed_loss_;\n SolverParameter param_;</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>迭代优化的函数，</p><div class=\"highlight\"><pre><code class=\"language-text\">virtual void Solve(const char* resume_file = NULL);\ninline void Solve(const string resume_file) { Solve(resume_file.c_str()); }\nvoid Step(int iters);</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><blockquote><b>6 layer.hpp</b></blockquote><p>这就是一个层的定义了，想必大家很有兴趣，那具体都有什么呢？</p><p>我们首先看变量，</p><div class=\"highlight\"><pre><code class=\"language-text\">LayerParameter layer_param_;\n vector&lt;Dtype&gt; loss_;</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>然后看重要函数</p><p>LayerSetUp，用于layer初始化，一般是定义一些shape，初始化一些变量。</p><div class=\"highlight\"><pre><code class=\"language-text\">virtual void LayerSetUp(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {}</code></pre></div><p>Forward，Backward的cpu和gpu版本，除了数据层外始终成对存在的前向和反向函数，forward是基于bottom计算top，backward则是基于top计算bottom，很好理解。</p><div class=\"highlight\"><pre><code class=\"language-text\">virtual void Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) = 0;\nvirtual void Forward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) {\n   Fackward_cpu(bottom,top);\n }\nvirtual void Backward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top,\nconst vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) = 0;\nvirtual void Backward_gpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top, const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom) {\n   Backward_cpu(top, propagate_down, bottom);\n }</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><blockquote><b>7 net.hpp</b></blockquote><p>这是最大的一个hpp了，也是最高层的，就是整个网络的定义。</p><p>看看重要变量：</p><div class=\"highlight\"><pre><code class=\"language-text\">vector&lt;shared_ptr&lt;Layer&lt;Dtype&gt; &gt; &gt; layers_;\n vector&lt;string&gt; layer_names_;</code></pre></div><p>layers_就是所有层，layer_names_;存储了名字，以后在我们inference的时候会需要经常用到。</p><div class=\"highlight\"><pre><code class=\"language-text\">vector&lt;float&gt; params_lr_;\n vector&lt;bool&gt; has_params_lr_;</code></pre></div><p>上面是每一层学习率的参数，在我们想要固定某些层不让其学习，或者调整不同层的学习率时，会非常重要。其实还有很多重要变量如， </p><div class=\"highlight\"><pre><code class=\"language-text\">vector&lt;Dtype&gt; blob_loss_weights_;\nPhase phase_;</code></pre></div><p>都是经常接触的，不一一描述了大家自己看代码。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>下面是一个重载的重要函数，</p><div class=\"highlight\"><pre><code class=\"language-text\">void CopyTrainedLayersFrom(const\nNetParameter&amp; param);\nvoid CopyTrainedLayersFrom(const string\ntrained_filename);\nvoid CopyTrainedLayersFromBinaryProto(const string\ntrained_filename);\nvoid CopyTrainedLayersFromHDF5(const string\ntrained_filename);</code></pre></div><p>它是重要的初始化网络的方法，可以实现不同形式输入的初始化，在inference时会经常使用的。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>好了，基础就这么多，并没有非常细致的讲述而只是对重要内容进行介绍，在开始下面的文章之前，一定要熟读上面的这些hpp和对应cpp文件，对它有什么是什么，熟练于胸。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>更多请移步</b></p><blockquote>1，我的gitchat达人课</blockquote><a href=\"https://link.zhihu.com/?target=http%3A//gitbook.cn/gitchat/column/5a6011fbbd5ff2623773394c\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic4.zhimg.com/v2-778539fd0d72d5e46ebabd371db4df83_180x120.jpg\" data-image-width=\"1920\" data-image-height=\"1080\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">龙鹏的达人课</a><blockquote>2，AI技术公众号,《与有三学AI》</blockquote><a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649028781%26idx%3D1%26sn%3D4b8dc2c6098fdd143a3e0c10d1b7e1eb%26chksm%3D871346d0b064cfc6fa80afc8b63d9c9eec446dc38bbca15fee977b01edf3d03b00627e69a0f0%23rd\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-2d9ceed78978badf52f685b50ced44c6_ipico.jpg\" data-image-width=\"358\" data-image-height=\"358\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">[caffe解读] caffe从数学公式到代码实现2-基础函数类</a><blockquote>3，以及摄影号，《有三工作室》</blockquote><a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3MzczNzY0Mw%3D%3D%26mid%3D2648543628%26idx%3D1%26sn%3D5573c69f7b54acd6e2bfcc123d7a9988%26chksm%3D87234548b054cc5e1aed7177a537fef841768acff589a93d8234fab318bbe682a9da40192134%23rd\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-8a86f4fc840333cf9e0dea9544dc9dde_ipico.jpg\" data-image-width=\"640\" data-image-height=\"640\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">冯小刚说，“他懂我”</a><p></p><p></p>", 
            "topic": [
                {
                    "tag": "Caffe（深度学习框架）", 
                    "tagLink": "https://api.zhihu.com/topics/20019488"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "开源项目", 
                    "tagLink": "https://api.zhihu.com/topics/19565961"
                }
            ], 
            "comments": [
                {
                    "userName": "爱猫的鱼", 
                    "userLink": "https://www.zhihu.com/people/3b7c7dc9ea7713aff217d5dd0b4e5a53", 
                    "content": "<p>很好</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/33076280", 
            "userName": "言有三-龙鹏", 
            "userLink": "https://www.zhihu.com/people/0c847e12ed6e97608c7377bcef7b837d", 
            "upvote": 12, 
            "title": "【技术综述】如何步入深度学习刷榜第一重境界", 
            "content": "<p>本文首发于微信公众号《与有三学AI》</p><a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649028744%26idx%3D1%26sn%3D49425665ad69c96f33a60e6f0fe50cde%26chksm%3D871346f5b064cfe393445c6fc8ffa0b159df03eecf007dfa4ba4866dac0fa644ef92cf2bf80a%23rd\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-25ebd31d3f8c97e0429d01a94d0d1e2c_ipico.jpg\" data-image-width=\"225\" data-image-height=\"225\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">如何步入深度学习刷榜第一重境界</a><p>实际上笔者也没多少刷榜经验，毕竟不擅长之前老大也没有任务指派，今年10月份得闲了个把月，没那么多事所以也就参加了一个场景分类的比赛，链接如下，</p><p><a href=\"https://link.zhihu.com/?target=https%3A//challenger.ai/competition/scene/leaderboard/test_a\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">全球AI挑战赛场景分类</a>，</p><p>刷了一个月之后最好成绩也就杀进前15然后就接着干项目去了。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-f954aca506a416aaae3993640397f52b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"698\" data-rawheight=\"609\" class=\"origin_image zh-lightbox-thumb\" width=\"698\" data-original=\"https://pic4.zhimg.com/v2-f954aca506a416aaae3993640397f52b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;698&#39; height=&#39;609&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"698\" data-rawheight=\"609\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"698\" data-original=\"https://pic4.zhimg.com/v2-f954aca506a416aaae3993640397f52b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-f954aca506a416aaae3993640397f52b_b.jpg\"/></figure><p>与第一名差一个点，7000张，80类，基本上每一类差1张图。到比赛结束的时候排在第20名左右，与第一名还是差一个点。</p><p>说出去好像是有点不太好意思，但是作为第一次刷比赛，一个月也不能白费，毕竟绩效打在那里。现在的比赛听说还有专业刷榜团队的，也是666。</p><p>下面也简单分享一下。</p><div class=\"highlight\"><pre><code class=\"language-text\">0 刷的是什么比赛？</code></pre></div><p>场景分类，80类日常生活中比较多的场景，这个在以后的社交应用中还是有需求的，相关最大的比赛是place365，有兴趣可以去看。眼下这个，是创新工场，今日头条，搜狗等一起搞的比赛，train数据集就不大，只有50000+，测试数据集7000+。</p><p>下面举10类吧</p><p>0/航站楼：airport_terminal</p><p>1/停机坪：landing_field</p><p>2/机舱：airplane_cabin</p><p>3/游乐场：amusement_park</p><p>4/冰场：skating_rink</p><p>5/舞台：arena/performance</p><p>6/艺术室：art_room</p><p>7/流水线：assembly_line</p><p>8/棒球场：baseball_field</p><p>9/橄榄球场：football_field</p><p>10/足球场：soccer_field</p><div class=\"highlight\"><pre><code class=\"language-text\">1 为什么叫第一境界？</code></pre></div><p>我觉得怎么着刷榜这事也得有个三个境界，像笔者这样，<b>一个人拿现有的模型，4块K40，兼职刷上一个月，最后提交也只融合了两个模型的</b>，怎么看都是处于刚入门的第一境界，大部分人其实也就是这个境界。</p><p>而到了第二三境界，至少得有个集群，得有一群人来尝试各种方案，而顶尖的团队对网络结构肯定是需要调优设计的，历年夺冠的那些网络alexnet，googlenet，resnet，senet无一例外。</p><p>不过设计强大的网络结构从时间代价，计算资源代价和算法能力都有比较高的要求，大部分人可能就是从<b>数据层面</b>做文章了，比如清洗数据，数据增强，搞搞不均衡样本等。</p><div class=\"highlight\"><pre><code class=\"language-text\">2 怎么一步刷到比较优的单模型？</code></pre></div><p>这是最关键的第一步。</p><p>有几点一定是要形成共识的。</p><p>(1) 由于我是只有4个卡，用caffe或者tensorflow都是不可能的，我用了mxnet，并且在训练的过程中都放开了所有参数，实际上也做过固定某些参数的实验，但是效果并不好。224的尺度，放开全部训练的话，4块卡resnet152 batchsize可以到96。在实验的过程中，batchsize越大，指标就越高，几个网络都能观测到相关结论。</p><p>(2) 由于训练数据少，使用当前数据从头训练大模型不太现实，所以，先找到相关数据集比赛finetune过的大网络，resnet系列找了一个resnet152，dpn系列找了一个dpn92，各自先训练。</p><p>(3) <b>从尽量大的模型开始</b>，机器啃得动的就行，毕竟这个任务里面有很多类还是很难的，小网络搞不定，resnet系至少得50层以上。</p><p>在刷这个比赛的时候，从imagenet mxnet<br/>model的模型fine-tune过来，链接在下面。</p><p><a href=\"https://link.zhihu.com/?target=http%3A//data.mxnet.io/models/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">data.mxnet.io/models/</span><span class=\"invisible\"></span></a></p><p>实验了18，50，152层的网络，使用本比赛50000+的数据进行简单的参数调优，</p><p>解释一下，lr是学习率，Lr=0.01(10)代表在10个epochs后下降一个数量级，从0.01到0.001，实际上在10个epoch以后都收敛了，所以后面没有做更多step lr的比较，大家感兴趣可以去尝试。w是weight decay，m是momentum，bs是batch size，单个k40 gpu。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-0e61d91710797949d8d57b7226fc4abd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1286\" data-rawheight=\"304\" class=\"origin_image zh-lightbox-thumb\" width=\"1286\" data-original=\"https://pic2.zhimg.com/v2-0e61d91710797949d8d57b7226fc4abd_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1286&#39; height=&#39;304&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1286\" data-rawheight=\"304\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1286\" data-original=\"https://pic2.zhimg.com/v2-0e61d91710797949d8d57b7226fc4abd_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-0e61d91710797949d8d57b7226fc4abd_b.jpg\"/></figure><p>从上面可以看出，从18层到152层精度毫无疑问是提升的。虽然参数没有调到各自最优，但基本能反应问题。尤其注意的是res18我加了weight decay来增加模型复杂度，不然没有上90%的可能。从resent152到resent200指标就没什么提升了，而且res200远远没有res152参数好调。</p><p>单模型单个crop 94%的精度已经差不多了，</p><p>(4) 理论上随着训练尺度增加，在一定范围内性能也会增加，但是训练尺度的增加会导致能使用的batchsize减小，所以笔者最后统一采用224这个尺度。听说有人用到了700以上的尺度，只能说，真土豪也。</p><p>(5) 单个模型，多个crop会对结果有所提升，有的团队用到了上百个crop，笔者最后用了10个crop，没有去尝试更多，毕竟测试也是很花时间，这点资源一个人搞不过来。</p><p>有了以上的共识后，那就开始干起来，过段时间我会重新整理把项目git传上去，前段时间服务器意外格式化丢了全部训练文件，一时还没有恢复。如果对此感兴趣，请持续关注。</p><p>总结：单模型，以resent152为例。</p><p>训练尺度224*224，数据增强采用了水平flip和随机crop，random<br/>resize参数照搬googlenet那套，放开所有参数，使用resnet152-place365，即在place365数据集上进行训练过的模型，然后使用当前的训练数据集进行finetune，validation数据集进行测试。</p><p>数据增强参数偷懒截个图，实际上这些mxnet全部都已经集成好了，直接设置开关即可。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-4d9f2149c9dd9a4e1aa85323d5f09a27_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"656\" data-rawheight=\"467\" class=\"origin_image zh-lightbox-thumb\" width=\"656\" data-original=\"https://pic4.zhimg.com/v2-4d9f2149c9dd9a4e1aa85323d5f09a27_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;656&#39; height=&#39;467&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"656\" data-rawheight=\"467\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"656\" data-original=\"https://pic4.zhimg.com/v2-4d9f2149c9dd9a4e1aa85323d5f09a27_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-4d9f2149c9dd9a4e1aa85323d5f09a27_b.jpg\"/></figure><p>训练参数，lr=0.01，分别在10,20,40个epoch时下调学习率，最后采用10个crop，分别为四个角，中心以及水平翻转。</p><p>在试用了多个batchsize之后，最大的能用的batchsize取得最优，resnet152单个模型能到97%。</p><p>在测试的时候有trick，采纳dpn的思想，使用较小的尺度训练，使用较大的尺度测试，最终在略大于224的尺度上，有微小的提升，对于刷榜来说还有很重要的，毕竟0.5%可以干掉10个人。</p><div class=\"highlight\"><pre><code class=\"language-text\">3 怎么做模型融合？</code></pre></div><p>不同网络架构，但能力相当的模型进行融合，结果会稳定提升。</p><p>笔者单模型10个crop，resnet152得到0.971，dpn92得到0.965，两者融合后即到0.978.</p><p>要想得到最优，需采用不同的epoch进行融合，这个需要花时间去测试；所以就会出现两个单模型最优，融合之后缺不是最优的情况。</p><p>这个时候，需要把各自错误样本拿出来分析，我当时没有太多时间和耐心去尝试各种方案。</p><p>这就是提交比赛的最后结果，两个现有的模型在224尺度用4张卡训练，融合之后，在比赛结束前的一个月，能排在15名左右，比赛结束后我回去一看，test_a也在20名以内，test_b也差不多，由于test_b比较难，所有参赛队伍的成绩都下降了4个点左右。</p><div class=\"highlight\"><pre><code class=\"language-text\">4 哪些trick比较关键</code></pre></div><p>虽然提交的结果非常简单，笔者还是实验过很多参数的，稍微有些经验拿出来分享下，有些参数是不能乱调的，有些则不需要调。</p><p>(1) finetune很关键</p><p>从相关大数据集上训练好的模型开始finetune，基本上可以肯定会比从不相关大数据集上训练的模型，或者从头开始训练更好，这个大家应该是通识了。</p><p>(2) 学习率lr和batchsize</p><p>学习率和batch size是成对的参数，batch size增大N倍，相当于将梯度的方差减少N倍，也就是梯度方差更小了，更加准确，更加容易跳出局部最优，带来的后果就是收敛更慢，这时候为了提高训练速度，可以将lr增加sqrt(N)倍。</p><p><b><i>学习率是最关键的参数了</i></b>，没得说，只能自己从大到小开始尝试。</p><p>笔者列举一个例子：dpn92, lr_step_epochs=&#39;10,20,30,40&#39;,w=0,m=0,bs=64,</p><p>lr取0.001，0.005，0.01，0.01，分别看train和val的acc。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-090f58a0052eaec88faf034e01a1bef0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1310\" data-rawheight=\"266\" class=\"origin_image zh-lightbox-thumb\" width=\"1310\" data-original=\"https://pic1.zhimg.com/v2-090f58a0052eaec88faf034e01a1bef0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1310&#39; height=&#39;266&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1310\" data-rawheight=\"266\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1310\" data-original=\"https://pic1.zhimg.com/v2-090f58a0052eaec88faf034e01a1bef0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-090f58a0052eaec88faf034e01a1bef0_b.jpg\"/></figure><p>从小到大，从欠拟合到过拟合，很明显。</p><p><b><i>batch size相对来说没有lr那么敏感，但是对结果也是至关重要的。</i></b></p><p>下面是resnet152的batchsize的实验，mul_val是多个crop</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-39d787a133bab67e4b7f1ab52c039752_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"438\" data-rawheight=\"278\" class=\"origin_image zh-lightbox-thumb\" width=\"438\" data-original=\"https://pic3.zhimg.com/v2-39d787a133bab67e4b7f1ab52c039752_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;438&#39; height=&#39;278&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"438\" data-rawheight=\"278\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"438\" data-original=\"https://pic3.zhimg.com/v2-39d787a133bab67e4b7f1ab52c039752_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-39d787a133bab67e4b7f1ab52c039752_b.jpg\"/></figure><p>下面是dpn92的batchsize的实验，mul_val是多个crop</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-30dc831b2b74db87855bcf171f2deffc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"530\" data-rawheight=\"288\" class=\"origin_image zh-lightbox-thumb\" width=\"530\" data-original=\"https://pic1.zhimg.com/v2-30dc831b2b74db87855bcf171f2deffc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;530&#39; height=&#39;288&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"530\" data-rawheight=\"288\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"530\" data-original=\"https://pic1.zhimg.com/v2-30dc831b2b74db87855bcf171f2deffc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-30dc831b2b74db87855bcf171f2deffc_b.jpg\"/></figure><p>看的出来，指标有所上升。</p><p>当然了，还是那句话，单个最优的模型融合起来并不能保证结果最优。</p><p>(3) weight decay和momentum</p><p>这两个参数，对于小模型的训练是比较关键的，不过越大越不敏感。</p><p>下面是res18的训练结果，从结果看来差异是很大的。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-6d72fd07f11e8e63d14bc5acd7046bb1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1070\" data-rawheight=\"126\" class=\"origin_image zh-lightbox-thumb\" width=\"1070\" data-original=\"https://pic2.zhimg.com/v2-6d72fd07f11e8e63d14bc5acd7046bb1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1070&#39; height=&#39;126&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1070\" data-rawheight=\"126\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1070\" data-original=\"https://pic2.zhimg.com/v2-6d72fd07f11e8e63d14bc5acd7046bb1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-6d72fd07f11e8e63d14bc5acd7046bb1_b.jpg\"/></figure><p>Res50，差距就不明显了。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-cc30632e5a9b878468122fb0b68e9d85_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1042\" data-rawheight=\"136\" class=\"origin_image zh-lightbox-thumb\" width=\"1042\" data-original=\"https://pic2.zhimg.com/v2-cc30632e5a9b878468122fb0b68e9d85_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1042&#39; height=&#39;136&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1042\" data-rawheight=\"136\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1042\" data-original=\"https://pic2.zhimg.com/v2-cc30632e5a9b878468122fb0b68e9d85_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-cc30632e5a9b878468122fb0b68e9d85_b.jpg\"/></figure><p>Senet50也是。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-6694c48bcd222424d4d09eb18203a034_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1042\" data-rawheight=\"130\" class=\"origin_image zh-lightbox-thumb\" width=\"1042\" data-original=\"https://pic1.zhimg.com/v2-6694c48bcd222424d4d09eb18203a034_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1042&#39; height=&#39;130&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1042\" data-rawheight=\"130\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1042\" data-original=\"https://pic1.zhimg.com/v2-6694c48bcd222424d4d09eb18203a034_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-6694c48bcd222424d4d09eb18203a034_b.jpg\"/></figure><p>我的结论是这个参数可以去调一调，不过对于大模型可能不是很必要，我都用的是0 </p><p>(4) 测试网络</p><p>下面是单个crop和10个crop的比较</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-191e77feee4c0b501724b0c442c40a30_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1296\" data-rawheight=\"378\" class=\"origin_image zh-lightbox-thumb\" width=\"1296\" data-original=\"https://pic1.zhimg.com/v2-191e77feee4c0b501724b0c442c40a30_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1296&#39; height=&#39;378&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1296\" data-rawheight=\"378\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1296\" data-original=\"https://pic1.zhimg.com/v2-191e77feee4c0b501724b0c442c40a30_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-191e77feee4c0b501724b0c442c40a30_b.jpg\"/></figure><p>很明显，不管是什么网络，多个crop会有很明显的提升，上面稳定提升2%以上，更多的crop笔者没尝试，因为实在是太慢了。</p><p>另一方面，借鉴dpn的思想，用小尺度训练，大尺度测试可能也有微小的点提升。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-6cac0a25d686e1b96680403c6145040e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1160\" data-rawheight=\"308\" class=\"origin_image zh-lightbox-thumb\" width=\"1160\" data-original=\"https://pic3.zhimg.com/v2-6cac0a25d686e1b96680403c6145040e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1160&#39; height=&#39;308&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1160\" data-rawheight=\"308\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1160\" data-original=\"https://pic3.zhimg.com/v2-6cac0a25d686e1b96680403c6145040e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-6cac0a25d686e1b96680403c6145040e_b.jpg\"/></figure><p>(5) 数据增强</p><p>本任务中复杂的数据增强没怎么用，使用的是mxnet level=1的数据增强，估计是因为模型已经在大数据库上训练过，au=1就是基本的crop，flip，random<br/>resize，au=2会做图像旋转，au=3会再加上颜色扰动，实际的项目中我们还是会做一点的。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-467b7bd30a43916056710d055e9c9e19_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1298\" data-rawheight=\"224\" class=\"origin_image zh-lightbox-thumb\" width=\"1298\" data-original=\"https://pic2.zhimg.com/v2-467b7bd30a43916056710d055e9c9e19_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1298&#39; height=&#39;224&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1298\" data-rawheight=\"224\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1298\" data-original=\"https://pic2.zhimg.com/v2-467b7bd30a43916056710d055e9c9e19_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-467b7bd30a43916056710d055e9c9e19_b.jpg\"/></figure><p>就这么多，不知道你对是否有用，下回搞点机器搞点时间去刷个大榜试试。</p><p><b>更多请移步</b></p><blockquote>1，我的gitchat达人课</blockquote><a href=\"https://link.zhihu.com/?target=http%3A//gitbook.cn/gitchat/column/5a6011fbbd5ff2623773394c\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic4.zhimg.com/v2-778539fd0d72d5e46ebabd371db4df83_180x120.jpg\" data-image-width=\"1920\" data-image-height=\"1080\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">龙鹏的达人课</a><blockquote>2，AI技术公众号,《与有三学AI》</blockquote><a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649028759%26idx%3D1%26sn%3D7f7260f530cabb950a6191b0002dfbed%26chksm%3D871346eab064cffc4be460b750ad57270d603a8874c3bed63d9bde463d115cb811297167767d%23rd\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-25ebd31d3f8c97e0429d01a94d0d1e2c_ipico.jpg\" data-image-width=\"225\" data-image-height=\"225\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">[caffe解读] caffe从数学公式到代码实现1-导论</a><blockquote>3，以及摄影号，《有三工作室》</blockquote><a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3MzczNzY0Mw%3D%3D%26mid%3D2648543628%26idx%3D1%26sn%3D5573c69f7b54acd6e2bfcc123d7a9988%26chksm%3D87234548b054cc5e1aed7177a537fef841768acff589a93d8234fab318bbe682a9da40192134%23rd\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-8a86f4fc840333cf9e0dea9544dc9dde_ipico.jpg\" data-image-width=\"640\" data-image-height=\"640\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">冯小刚说，“他懂我”</a><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-946679151c94cb87962c5a382218d3c4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"1080\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-946679151c94cb87962c5a382218d3c4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;1080&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"1080\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-946679151c94cb87962c5a382218d3c4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-946679151c94cb87962c5a382218d3c4_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "计算机视觉", 
                    "tagLink": "https://api.zhihu.com/topics/19590195"
                }
            ], 
            "comments": []
        }
    ], 
    "url": "https://zhuanlan.zhihu.com/c_1005857083643764736"
}
