{
    "title": "羽落的编程日记", 
    "description": "Python为主的编程心得、文章、入门教程。\n未来会有java了，嗯（笑", 
    "followers": [
        "https://www.zhihu.com/people/zhang-fang-kai-80", 
        "https://www.zhihu.com/people/zhao-yu-73-75", 
        "https://www.zhihu.com/people/a-hui-79-15", 
        "https://www.zhihu.com/people/LIAOYUANGANG", 
        "https://www.zhihu.com/people/wj2014-59", 
        "https://www.zhihu.com/people/yuan-wei-62-15-36", 
        "https://www.zhihu.com/people/ceo404", 
        "https://www.zhihu.com/people/bu-yu-53", 
        "https://www.zhihu.com/people/player-qian", 
        "https://www.zhihu.com/people/hu-yan-yu-52", 
        "https://www.zhihu.com/people/chen-lin-38-42-11", 
        "https://www.zhihu.com/people/cuda", 
        "https://www.zhihu.com/people/xiao-jing-bo-53", 
        "https://www.zhihu.com/people/lesmillsxx", 
        "https://www.zhihu.com/people/ban-shi-yang-ge", 
        "https://www.zhihu.com/people/ai-lao-lao-de-xiao-gong-ji", 
        "https://www.zhihu.com/people/ZERO_0", 
        "https://www.zhihu.com/people/he-yan-liang", 
        "https://www.zhihu.com/people/bintie", 
        "https://www.zhihu.com/people/ai-rui-ke-xian-sheng-15", 
        "https://www.zhihu.com/people/pei-gen-91", 
        "https://www.zhihu.com/people/dongdong5820", 
        "https://www.zhihu.com/people/dichengsiyu", 
        "https://www.zhihu.com/people/chen-yan-su-89", 
        "https://www.zhihu.com/people/la-la-la-8-58-55", 
        "https://www.zhihu.com/people/overad", 
        "https://www.zhihu.com/people/tang-wei-tuo-90", 
        "https://www.zhihu.com/people/wang-liao-52", 
        "https://www.zhihu.com/people/hu-cheng-52-24", 
        "https://www.zhihu.com/people/feng-zhong-qi-shi-58", 
        "https://www.zhihu.com/people/ma-lin-lin", 
        "https://www.zhihu.com/people/zhui-feng-de-sao-nian-007", 
        "https://www.zhihu.com/people/zhi-hu-29-20", 
        "https://www.zhihu.com/people/he-he-91-41-72", 
        "https://www.zhihu.com/people/mu-tou-13-30", 
        "https://www.zhihu.com/people/qiu-qiu-10-63-51", 
        "https://www.zhihu.com/people/alahlll", 
        "https://www.zhihu.com/people/dan-chao-fan-99-9", 
        "https://www.zhihu.com/people/leng-z-70", 
        "https://www.zhihu.com/people/jiu-bu-gao-su-ni-42", 
        "https://www.zhihu.com/people/feng-xue-han-6", 
        "https://www.zhihu.com/people/lao-zi-deng-chang-75", 
        "https://www.zhihu.com/people/ren-min-14-52", 
        "https://www.zhihu.com/people/ke-ge-bu-ting", 
        "https://www.zhihu.com/people/hu-yan-yuan-2", 
        "https://www.zhihu.com/people/qidianban1930", 
        "https://www.zhihu.com/people/huhenghh4"
    ], 
    "article": [
        {
            "url": "https://zhuanlan.zhihu.com/p/84327339", 
            "userName": "羽落", 
            "userLink": "https://www.zhihu.com/people/b95f10995f987d8e01ace57dc0fb160d", 
            "upvote": 12, 
            "title": "一篇文章教你读懂哈希表-HashMap", 
            "content": "<blockquote>题图Pid=68670770</blockquote><p>在最近的学习过程中，发现身边很多朋友对哈希表的原理和应用场景不甚了解，处于会用但不知道什么时候该用的状态，所以我找出了刚学习Java时写的HashMap实现，并以此为基础拓展关于哈希表的实现原理。</p><h2>什么是哈希表？</h2><blockquote><a href=\"https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%25E6%2595%25A3%25E5%2588%2597%25E8%25A1%25A8/10027933\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">散列表</a>（Hash table，也叫哈希表），是根据关键码值(Key value)而直接进行访问的<a href=\"https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%25E6%2595%25B0%25E6%258D%25AE%25E7%25BB%2593%25E6%259E%2584/1450\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">数据结构</a>。也就是说，它通过把关键码值映射到表中一个位置来访问记录，以加快查找的速度。这个映射函数叫做<a href=\"https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%25E6%2595%25A3%25E5%2588%2597%25E5%2587%25BD%25E6%2595%25B0/2366288\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">散列函数</a>，存放记录的<a href=\"https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%25E6%2595%25B0%25E7%25BB%2584/3794097\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">数组</a>叫做<a href=\"https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%25E6%2595%25A3%25E5%2588%2597%25E8%25A1%25A8/10027933\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">散列表</a>。<br/>给定表M，存在函数f(key)，对任意给定的关键字值key，代入函数后若能得到包含该关键字的记录在表中的地址，则称表M为哈希(Hash）表，函数f(key)为哈希(Hash) 函数。</blockquote><p>以上正式的解释摘自百度百科<a href=\"https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%25E5%2593%2588%25E5%25B8%258C%25E8%25A1%25A8/5981869\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">哈希表</a>页面。</p><p>从这段解释中，我们理应知道的：</p><ul><li>哈希表是一种数据结构</li><li>哈希表表示了关键码值和记录的映射关系</li><li>哈希表可以加快查找速度</li><li>任意哈希表，都满足有哈希函数f(key)，代入任意key值都可以获取包含该key值的记录在表中的地址</li></ul><hr/><p>官方解释听过了，那么如何用大白话来解释呢？</p><p>简单的来说，哈希表是一种表结构，我们可以直接根据给定的key值计算出目标位置。在工程中这一表结构实现通常采用数组。</p><p>与普通的列表不同的地方在于，普通列表仅能通过下标来获取目标位置的值，而哈希表可以根据给定的key计算得到目标位置的值。</p><p>在列表查找中，使用最广泛的二分查找算法，复杂度为O(log2n)，但其始终只能用于有序列表。普通无序列表只能采用遍历查找，复杂度为O(n)。</p><p>而拥有较为理想的哈希函数实现的哈希表，对其任意元素的查找速度始终为常数级，即O(1)。</p><hr/><p>图解：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-08c23e9e5a602d3bb414d0005e5a53ec_b.jpg\" data-rawwidth=\"886\" data-rawheight=\"494\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"886\" data-original=\"https://pic1.zhimg.com/v2-08c23e9e5a602d3bb414d0005e5a53ec_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;886&#39; height=&#39;494&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"886\" data-rawheight=\"494\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"886\" data-original=\"https://pic1.zhimg.com/v2-08c23e9e5a602d3bb414d0005e5a53ec_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-08c23e9e5a602d3bb414d0005e5a53ec_b.jpg\"/></figure><p>在一个典型的哈希表实现中，我们将数组总长度设为模数，将key值直接对其取模，所得的值为数组下标。</p><p>如图所示的三组数据，分别被映射到下标为0和7的位置中，显而易见的，第1组数据和第3组数据发生了哈希碰撞。</p><hr/><h2>如何解决哈希碰撞？</h2><p>常用的解决方案有散列法和拉链法。散列法又分为开放寻址法和再散列法等，此处不做展开。java中使用的实现为拉链法，即：在每个冲突处构建链表，将所有冲突值链入链表，如同拉链一般一个元素扣一个元素，故名拉链法。</p><p>需要注意的是，如果遭到恶意哈希碰撞攻击，拉链法会导致哈希表退化为链表，即所有元素都被存储在同一个节点的链表中，此时哈希表的查找速度=链表遍历查找速度=O(n)。</p><h2>哈希表有什么优势？</h2><p>通过前面的概念了解，哈希表的优点呼之欲出：通过关键值计算直接获取目标位置，对于海量数据中的精确查找有非常惊人的速度提升，理论上即使有无限的数据量，一个实现良好的哈希表依旧可以保持O(1)的查找速度，而O(n)的普通列表此时已经无法正常执行查找操作（实际上不可能，受到JVM可用内存限制，机器内存限制等）。</p><h2>哈希表的主要应用场景</h2><p>在工程上，经常用于通过名称指定配置信息、通过关键字传递参数、建立对象与对象的映射关系等。目前最流行的NoSql数据库之一Redis，整体的使用了哈希表思想。</p><p>一言以蔽之，所有使用了键值对的地方，都运用到了哈希表思想。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>Java中的哈希表实现-HashMap</h2><p>在正式开始对HashMap的介绍和实现之前，你应当知道以下这些知识：</p><p><b>任意数对2的N次方取模时，等同于其和2的N次方-1作位于运算。</b></p><p>公式表述为：</p><div class=\"highlight\"><pre><code class=\"language-text\">k % 2^n = k &amp; (2^n - 1)</code></pre></div><p>而位于运算相比于取模运算速度大幅度提升（按照Bruce Eckel给出的数据，大约可以提升5～8倍）。</p><p><b>负载因子</b></p><p><b>负载因子</b>是哈希表的重要参数，其定义为：哈希表中已存有的元素与哈希表长度的比值。</p><p>它是一个浮点数，表示哈希表目前的装满程度。由于表长是定值，而表中元素的个数越大，表中空余位置就会更少，发生碰撞的可能性也会进一步增大。</p><p>哈希表的扩容策略依赖于负载因子阈值。基于性能与空间的选择，JDK标准库将HashMap的负载因子阈值定为<b>0.75</b>。</p><hr/><p><b>HashMap继承体系</b></p><p>首先来看HashMap的继承体系：</p><div class=\"highlight\"><pre><code class=\"language-java\"><span class=\"kd\">public</span> <span class=\"kd\">class</span> <span class=\"nc\">HashMap</span><span class=\"o\">&lt;</span><span class=\"n\">K</span><span class=\"o\">,</span><span class=\"n\">V</span><span class=\"o\">&gt;</span> <span class=\"kd\">extends</span> <span class=\"n\">AbstractMap</span><span class=\"o\">&lt;</span><span class=\"n\">K</span><span class=\"o\">,</span><span class=\"n\">V</span><span class=\"o\">&gt;</span> <span class=\"kd\">implements</span> <span class=\"n\">Map</span><span class=\"o\">&lt;</span><span class=\"n\">K</span><span class=\"o\">,</span><span class=\"n\">V</span><span class=\"o\">&gt;</span>\n\n<span class=\"kd\">public</span> <span class=\"kd\">abstract</span> <span class=\"kd\">class</span> <span class=\"nc\">AbstractMap</span><span class=\"o\">&lt;</span><span class=\"n\">K</span><span class=\"o\">,</span><span class=\"n\">V</span><span class=\"o\">&gt;</span> <span class=\"kd\">implements</span> <span class=\"n\">Map</span><span class=\"o\">&lt;</span><span class=\"n\">K</span><span class=\"o\">,</span><span class=\"n\">V</span><span class=\"o\">&gt;</span>\n\n<span class=\"kd\">public</span> <span class=\"kd\">interface</span> <span class=\"nc\">Map</span><span class=\"o\">&lt;</span><span class=\"n\">K</span><span class=\"o\">,</span> <span class=\"n\">V</span><span class=\"o\">&gt;</span></code></pre></div><p>可以看到，抽象类AbstractMap就是对Map接口的抽象实现，HashMap通过继承AbstractMap间接实现了Map接口，同时自身直接声明了对Map接口的实现，即HashMap就是Map接口的直接实现。</p><p>Map接口中定义了一个Map实现类必须要实现的方法。所有Map实现类都应当实现这些方法。</p><h2>Map接口定义的需要实现的方法：</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-536c38b71f2f214542295d3ae1a3a9ee_b.jpg\" data-rawwidth=\"441\" data-rawheight=\"350\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"441\" data-original=\"https://pic3.zhimg.com/v2-536c38b71f2f214542295d3ae1a3a9ee_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;441&#39; height=&#39;350&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"441\" data-rawheight=\"350\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"441\" data-original=\"https://pic3.zhimg.com/v2-536c38b71f2f214542295d3ae1a3a9ee_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-536c38b71f2f214542295d3ae1a3a9ee_b.jpg\"/></figure><p>在本篇文章剩余的篇幅中，将会基于Map接口实现一个我们自己的HashMap。</p><h2>MyHashMap实现：</h2><p>在动手之前，先分析清楚Map接口提供的方法，实现了哪些功能。其中关键的方法提取出来，结果为：</p><div class=\"highlight\"><pre><code class=\"language-text\">//实现查找功能。\n//containsKey基于此方法实现。\nV get(Object key);\n//实现新增功能。\n//由于哈希表同key覆盖特性，此方法同时实现了更新操作。\nV put(K key, V value);\n//实现删除功能。\nV remove(Object key);\n//实现对Map的遍历功能。\nSet&lt;Map.Entry&lt;K, V&gt;&gt; entrySet();\nCollection&lt;V&gt; values();\nSet&lt;K&gt; keySet();</code></pre></div><p>我们的HashMap采用泛型数组作为存储数据的结构。此时应用到两个类Node和Entry。Node类用作拉链法链表节点，其中每个Node存储了一个Entry类，Entry中包含了Key和Value，是真正存储数据的类型。</p><p>前文所述的与模运算等价的位与运算，当且仅当模数为2的N次幂时才会生效。所以我们的HashMap初始的数组长度将会定为16，扩容策略为每次扩容为上一次长度的2倍，负载因子0.75（这也是JDK标准库所采用的配置）。</p><div class=\"highlight\"><pre><code class=\"language-text\">public class MyHashMap&lt;K, V&gt; implements Map&lt;K, V&gt; {\n    private class Node {\n        private MyEntry&lt;K, V&gt; entry = null;\n        public Node next = null;\n    }\n\n    class MyEntry&lt;K, V&gt; implements Entry&lt;K, V&gt; {\n        private int hash;\n        private K key;\n        private V value;\n    }\n\n    //常量区\n    private static final double LOAD_FACTOR = 0.75;       //负载因子阈值\n    private static final int INITIAL_SIZE = 16;           //数组初始大小\n\n    //成员变量区\n    private int element_count = 0;        //当前元素计数\n    private Node[] node_list = (Node[]) Array.newInstance(Node.class, INITIAL_SIZE);       //存储数组。\n\n    //略去Map列表的实现方法\n}</code></pre></div><p>值得注意的是</p><div class=\"highlight\"><pre><code class=\"language-text\">private Node[] node_list = (Node[]) Array.newInstance(Node.class, INITIAL_SIZE)</code></pre></div><p>Java中并不支持直接申请泛型类的数组。只能通过Array.newInstance静态方法构造数组并强制转换为泛型类的数组。</p><p>resize操作时同样需要用到此方法。</p><p>Hash表的核心操作就是通过对key值的计算直接查找目标元素下标，因此我们首先参考标准库编写(fuzhi)出getIndex方法：</p><div class=\"highlight\"><pre><code class=\"language-text\">private int getIndex( int hash, int mod ){\n    return (hash &amp; 0x7fffffff) &amp; (mod - 1);\n}</code></pre></div><p>(hash &amp; 0x7fffffff)是为了确保结果为正数。</p><p>为什么要对0x7fffffff做位于操作？</p><p>0x7fffffff是int可以表达的最大正整数，除了首位为0其他31位都为1。正数&amp; 0x7fffffff结果为其本身，负数&amp; 0x7fffffff结果为正数。</p><p>为什么不用Math.abs？</p><p>前面说过，位运算很快。而且由于Math.abs只是简单的return -a，因此Math.abs(<i>Integer</i>.<b><i>MIN_VALUE</i></b>)时结果仍然为负数，如下图所示：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b91bf8a53b5d92ca3e3e90966e87ed1d_b.jpg\" data-rawwidth=\"751\" data-rawheight=\"373\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"751\" data-original=\"https://pic2.zhimg.com/v2-b91bf8a53b5d92ca3e3e90966e87ed1d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;751&#39; height=&#39;373&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"751\" data-rawheight=\"373\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"751\" data-original=\"https://pic2.zhimg.com/v2-b91bf8a53b5d92ca3e3e90966e87ed1d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-b91bf8a53b5d92ca3e3e90966e87ed1d_b.jpg\"/></figure><p>hash &amp; 0x7fffffff保证结果为正数。</p><p>（结果是不是负数的绝对值不重要，只要参数同样时每次计算都可以得出同样的结果，就可以作为哈希函数）</p><p>基于getIndex方法，我们可以写出put和remove方法。</p><div class=\"highlight\"><pre><code class=\"language-text\">@Override\npublic V put( K key, V value ){\n    put(new MyEntry&lt;&gt;(key, value), node_list, true);\n    return value;\n}\n\nprivate void put( MyEntry&lt;K, V&gt; entry, Node[] target, boolean check ){\n    put(new Node(entry), target, check);\n}\n\n/**\n * 如果目标位置为空，则创建节点并保存目标位置\n * 否则在列表中查找并替换重复项。\n * 如果没有重复项，则插入链表尾部。\n *\n * @param node   : 被加入数组的节点。\n * @param target : 目标数组。\n * @param check : 指示方法是否检查数组的当前元素数量。\n */\nprivate void put( Node node, Node[] target, boolean check ){\n    int index = getIndex(node.getEntry().getHash(), node_list.length);\n    if (target[index] == null) {\n        target[index] = new Node(null);\n    }\n    if (target[index].next == null) {\n        target[index].next = node;\n        if (check) {\n            //检查哈希表大小\n            ++element_count;\n            checkLoadFactor();\n        }\n        return;\n    }\n\n    Node temp = target[index].next;\n    while (temp != null) {\n        if (temp.getEntry().getHash() == node.getEntry().getHash()) {\n            temp.setEntry(node.getEntry());\n            return;\n        }\n        if (temp.next == null) {\n            temp.next = node;\n            temp.next.next = null;        //截断节点，防止出现循环引用\n            if (check) {\n                //检查哈希表大小\n                ++element_count;\n                checkLoadFactor();\n            }\n        }\n        temp = temp.next;\n    }\n}</code></pre></div><p>其中几个值得注意的点：</p><p><i>check参数：</i>指示方法是否检查数组的当前元素数量。由于扩容时同样会使用这个方法作数组元素的迁移行为，一个检查的开关是必须的，否则会出现死循环。</p><p>temp.next.next = null ：同样，在数据迁移操作时，如果未截断链表的每个节点，会导致新老数组中对应列表发生串联，最终产生死循环。</p><p>最终MyHashMap中将集成经典的链表操作。</p><p>接着实现remove方法：</p><div class=\"highlight\"><pre><code class=\"language-text\">@Override\npublic V remove( Object key ){\n    if (key == null) {\n        return null;\n    }\n\n    int index = getIndex(key.hashCode(), node_list.length);\n    if (node_list[index] == null || node_list[index].next == null) {\n        return null;\n    }\n\n    //在目标位置的链表中查找目标键值。\n    Node last = node_list[index];\n    Node current = node_list[index].next;\n    while (current != null) {\n        if (current.getEntry().getHash() == key.hashCode()) {\n            last.next = current.next;\n            --element_count;                            //减少数组元素计数\n            return current.getEntry().getValue();\n        }\n        last = last.next;\n        current = current.next;\n    }\n\n    return null;\n}</code></pre></div><p>在remove方法中，将会计算得到目标节点下标，遍历目标链表节点，当查找到目标元素时，断开并重连链表将目标元素从链表中移除。</p><p>非常典型的链表操作。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>接下来实现最重要的get操作。然而在HashMap的CRUD三个操作中，get操作最为简单，因为其不需要移动链表节点或改变链表结构，仅需要遍历链表即可。</p><div class=\"highlight\"><pre><code class=\"language-text\">/**\n * 从Map中查找目标Key。\n * @param key\n * @return\n */\n@Override\npublic V get( Object key ){\n    int index = getIndex(key.hashCode(), node_list.length);\n    //目标位置为空则直接返回null\n    if (node_list[index] == null || node_list[index].next == null) {\n        return null;\n    }\n\n    //目标位置不为空则遍历链表，查找相同的key\n    Node temp = node_list[index].next;\n    while (temp != null) {\n        if (temp.getEntry().getHash() == key.hashCode()) {\n            return temp.getEntry().getValue();\n        }\n        temp = temp.next;\n    }\n    return null;\n}</code></pre></div><p>接下来是resize方法，它实现了数组元素的迁移操作。</p><p>但在resize方法之前，我们先来看一个有趣的方法，也是我的实现中不同于JDK标准库的方法，它提供了对元素数组的遍历操作，采用双指针法实现。它接受一个Consumer接口作为参数，它会对当前数组中的所有Node调用Consumer.accept方法。</p><p>values方法，containsValue方法，keySet方法，entrySet方法都基于它来实现：</p><div class=\"highlight\"><pre><code class=\"language-text\">//遍历list，并对其中的每一个元素执行指定的操作\nprivate void traversing( Node[] nl, Consumer&lt;Node&gt; con ){\n    int head = 0, foot = nl.length - 1;\n    Node node;\n    while (head &lt;= foot) {\n        if (nl[head] != null &amp;&amp; nl[head].next != null) {\n            node = nl[head];\n            while ((node = node.next) != null) {\n                con.accept(node);\n            }\n        }\n        if (nl[foot] != null &amp;&amp; nl[foot].next != null) {\n            node = nl[foot];\n            while ((node = node.next) != null) {\n                con.accept(node);\n            }\n        }\n        ++head;\n        --foot;\n    }\n}</code></pre></div><p>有了traversing方法，可以用轻松（甚至是偷懒）的方式写出values，keySet，entrySet，containsValue：</p><div class=\"highlight\"><pre><code class=\"language-text\">@Override\npublic Collection&lt;V&gt; values(){\n    Collection&lt;V&gt; collection = new ArrayList&lt;&gt;();\n    traversing(node_list, (node -&gt; {\n        collection.add(node.getEntry().getValue());\n    }));\n    return collection;\n}\n\n@Override\npublic Set&lt;K&gt; keySet(){\n    Set&lt;K&gt; set = new HashSet&lt;&gt;();\n    traversing(node_list, (node -&gt; {\n        set.add(node.entry.getKey());\n    }));\n    return set;\n}\n\n@Override\npublic Set&lt;Entry&lt;K, V&gt;&gt; entrySet(){\n    Set&lt;Entry&lt;K, V&gt;&gt; set = new HashSet&lt;&gt;();\n    traversing(node_list, ( node ) -&gt; {\n        set.add(node.getEntry());\n    });\n\n    return set;\n}\n\n//在最坏情况下，这种实现会将HashMap遍历两次。\n//这样写仅仅是为了偷懒。\n//如果你要写一个用于生产环境的containsValue，不要这样做。\n@Override\npublic boolean containsValue( Object value ){\n    //遍历哈希表查找值\n    for (Entry&lt;K, V&gt; entry : entrySet()) {\n        V temp_value = entry.getValue();\n        if (temp_value != null &amp;&amp; temp_value.equals(value)) {\n            return true;\n        }\n    }\n    return false;\n}</code></pre></div><p>用于对HashMap进行扩容的resize方法如下，它的实现原理非常简单易懂：创建一个新数组，随后调用traversing和本类的put方法将原始数组中的所有元素插入到新数组中，最终使用新数组替换原始数组。</p><p>随便一提，(hash &amp; 0x7fffffff) &amp; (mod - 1)可以保证将每个链表中的元素平均的放入新数组中的两个对应位置。</p><div class=\"highlight\"><pre><code class=\"language-text\">/**\n * 列表扩容。\n */\nprivate void resize(){\n    //创建新列表\n    Node[] new_list = (Node[]) Array.newInstance(Node.class, node_list.length &lt;&lt; 1);\n    traversing(node_list, (node -&gt; {\n        put(node, new_list, false);\n    }));\n    //移动完成后替换当前列表。\n    node_list = new_list;\n}</code></pre></div><p><b>大功告成！Map接口中的所有核心方法都被实现了。</b></p><hr/><p>在<a href=\"https://link.zhihu.com/?target=https%3A//github.com/OrsPced/MyJavaStandardLibrary\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">OrsPced的Github</a>可以找到本文中的完整实现。</p><p>如果有更好的想法，评论或建议，欢迎在评论区提出。</p><h2><b>对阅读至此的您表示诚挚的感谢。</b></h2>", 
            "topic": [
                {
                    "tag": "哈希函数", 
                    "tagLink": "https://api.zhihu.com/topics/19631819"
                }, 
                {
                    "tag": "Java", 
                    "tagLink": "https://api.zhihu.com/topics/19561132"
                }, 
                {
                    "tag": "Java 编程", 
                    "tagLink": "https://api.zhihu.com/topics/19582744"
                }
            ], 
            "comments": [
                {
                    "userName": "今晚月色真美", 
                    "userLink": "https://www.zhihu.com/people/71235e5f89340cb32fe62106593653e7", 
                    "content": "再来一篇redis源码分析就更加好了<a href=\"https://pic4.zhimg.com/v2-db92f653a2ec17ea3ff309d6d56e8507.gif\" class=\"comment_sticker\" data-width=\"0\" data-height=\"0\" data-sticker-id=\"980770591112015872\">[吃瓜]</a>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "yami我tm舔爆", 
                    "userLink": "https://www.zhihu.com/people/e495332df1f523b13363fe276b62c962", 
                    "content": "给yami点赞，yamiprpr", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "Yang", 
                    "userLink": "https://www.zhihu.com/people/55cb54f7cf6a88bbc1342f37c8c2ac3a", 
                    "content": "可是这和我是个冷酷无情的药师有什么关系呢", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/71137275", 
            "userName": "羽落", 
            "userLink": "https://www.zhihu.com/people/b95f10995f987d8e01ace57dc0fb160d", 
            "upvote": 53, 
            "title": "使用免费代理池+Python+Requests抓取B站用户信息并入库（附源码+导出脚本+CSV数据）", 
            "content": "<blockquote>题图Pid=37176247</blockquote><p>实际上这是一篇拖了很久才写出来的文章，算是对自己前段时间对于爬虫学习的总结吧。</p><blockquote>速度：                ☆☆☆                 #速度较快。免费代理池在深夜最高速度可达8000 items / min，正常速度约为800~1200 items/min左右，且没有做代理复用操作。在一定程度上缓存并复用代理将会在一定程度上提升性能。<br/>数据完整性：      ☆☆                   #完整性较差。无法保证数据完整写入。在扫尾工作时随机丢失大量数据，试了各种方法后均无法解决。<br/>本机性能开销：   ☆☆☆☆☆          #开销极大。运行时使用了10P*16T运行爬虫，可完全跑满7700HQ@3.4gHz（大量操作用于和Redis操作，等待Redis反馈等。）<br/>最终数据量：约2,000,000 / 24h</blockquote><p>原先使用的方案是Docker+Splash解析b站用户信息页，但Splash速度较慢，性能消耗较高，实际运行速度只有约150items / min（对比直接访问api：800~1200items / min），遂采用直接访问ajax的方式获取数据。数据展示：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-6312041603558a25f30f697de0713bf8_b.png\" data-rawwidth=\"1211\" data-rawheight=\"63\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"1211\" data-original=\"https://pic1.zhimg.com/v2-6312041603558a25f30f697de0713bf8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1211&#39; height=&#39;63&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1211\" data-rawheight=\"63\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1211\" data-original=\"https://pic1.zhimg.com/v2-6312041603558a25f30f697de0713bf8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-6312041603558a25f30f697de0713bf8_b.png\"/></figure><p>行数1382231，截止uid同样为1382231，但数据并不完整，丢失数据集中在结尾处，我通过设置起止UID反复运行爬虫来补全数据。</p><h2>前期准备：</h2><p>分析网页个人信息首页可以得到B站关于用户信息的API（截止2019.4可用）：</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"n\">生日</span><span class=\"err\">，</span><span class=\"n\">ID</span><span class=\"err\">，</span><span class=\"n\">签名</span><span class=\"err\">，</span><span class=\"n\">等级</span><span class=\"err\">，</span><span class=\"n\">大会员状态</span> <span class=\"o\">=</span> <span class=\"s2\">&#34;https://api.bilibili.com/x/space/acc/info?mid=</span><span class=\"si\">{uid}</span><span class=\"s2\">&#34;</span>\n<span class=\"n\">收藏夹状态</span> <span class=\"o\">=</span> <span class=\"s2\">&#34;https://api.bilibili.com/medialist/gateway/base/created?pn=1&amp;ps=10&amp;up_mid=</span><span class=\"si\">{uid}</span><span class=\"s2\">&#34;</span>\n<span class=\"n\">关注数</span><span class=\"err\">，</span><span class=\"n\">粉丝数</span> <span class=\"o\">=</span> <span class=\"s2\">&#34;https://api.bilibili.com/x/relation/stat?vmid=</span><span class=\"si\">{uid}</span><span class=\"s2\">&#34;</span>\n<span class=\"n\">直播间信息</span> <span class=\"o\">=</span> <span class=\"s2\">&#34;https://api.live.bilibili.com/room/v1/Room/getRoomInfoOld?mid=</span><span class=\"si\">{uid}</span><span class=\"s2\">&#34;</span>\n<span class=\"n\">投稿信息</span> <span class=\"o\">=</span>  <span class=\"s2\">&#34;https://api.bilibili.com/x/space/navnum?mid=</span><span class=\"si\">{uid}</span><span class=\"s2\">&#34;</span>\n<span class=\"n\">总计播放量</span> <span class=\"o\">=</span> <span class=\"s2\">&#34;https://api.bilibili.com/x/space/upstat?mid=</span><span class=\"si\">{uid}</span><span class=\"s2\">&#34;</span>\n\n<span class=\"n\">视频信息</span> <span class=\"o\">=</span> <span class=\"s2\">&#34;https://api.bilibili.com/x/web-interface/archive/stat?aid=</span><span class=\"si\">{aid}</span><span class=\"s2\">&#34;</span>\n<span class=\"n\">通过av号获取作者</span> <span class=\"o\">=</span> <span class=\"s2\">&#34;https://api.bilibili.com/x/tag/archive/tags?aid=</span><span class=\"si\">{aid}</span><span class=\"s2\">&#34;</span></code></pre></div><p>接着，创建格式如下的数据库，并设置uid字段为唯一主键：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-d85f7b0b58b7a4de08b77ba5c41ff273_b.jpg\" data-rawwidth=\"930\" data-rawheight=\"748\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"930\" data-original=\"https://pic4.zhimg.com/v2-d85f7b0b58b7a4de08b77ba5c41ff273_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;930&#39; height=&#39;748&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"930\" data-rawheight=\"748\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"930\" data-original=\"https://pic4.zhimg.com/v2-d85f7b0b58b7a4de08b77ba5c41ff273_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-d85f7b0b58b7a4de08b77ba5c41ff273_b.jpg\"/></figure><p>在DataBaseSettings.ini中以json格式写入数据库配置：</p><div class=\"highlight\"><pre><code class=\"language-text\">{\n\t&#34;default&#34;: {\n\t\t&#34;HOST&#34;: &#34;***.***.***.***&#34;,\n\t\t&#34;PORT&#34;: 10060,\n\t\t&#34;DB&#34;: &#34;bilibili_user_info&#34;,\n\t\t&#34;USER&#34;: &#34;bili_user_Robot&#34;,\n\t\t&#34;PASSWORD&#34;: &#34;PASSWORD&#34;,\n\t\t&#34;CHARSET&#34;: &#34;utf8&#34;\n\t}\n}</code></pre></div><p>并在settings.py中写入读取语句：</p><div class=\"highlight\"><pre><code class=\"language-text\">import json\nwith open(&#34;DataBaseSettings.ini&#34;,&#34;r&#34;) as fp:\n    sqlsetting = json.loads(fp.read())[&#34;default&#34;]</code></pre></div><p>由于使用了Redis来进行代理暂存和请求分发，所以还需要写入Redis地址：</p><div class=\"highlight\"><pre><code class=\"language-text\">REDIS_HOST = &#34;192.168.229.128&#34;\nREDIS_PORT = 6379</code></pre></div><h2>模块划分：</h2><ul><li>MainProcess.py：主模块。负责启动并维护所有子进程，负责请求生成和请求速度控制、请求计数、维护数据完整性、控制所有进程存活状态检测和退出等。</li><li><a href=\"https://link.zhihu.com/?target=http%3A//crawlprocess.py/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">CrawlProcess.py</a>：主要的工作进程。负责启动并维护所有子线程，监控子线程工作状态，子线程执行数据爬取，数据格式化，并将格式化完成的数据提交给数据存储进程，由数据存储进程统一插入数据库。</li><li>requests_api.py：包装requests.get，以简单的方式通过代理访问网站。</li><li>mysql_api.py ：以子进程方式启动。负责接收数据并缓存，缓存满或接收EXIT时存储至MySQL。</li><li>settings.py：保存项目的配置信息，例如进程数/线程数、MySQL配置、Redis配置、代理池配置等。</li></ul><h2>问题分析：</h2><p>在多进程/多线程、高负载的情况下，如何保证数据的完整性？</p><ul><li>开启进程管道，多个进程统一将爬取到的数据通过管道传递给数据库进程，由数据存储进程负责执行数据插入。（但事实证明并不需要这么做。单机MySQL可以轻松应对每秒130+的数据插入。证明对SQL的认识存在不足。</li></ul><p>搭建免费代理池，如何保证代理可用性？</p><ul><li>很遗憾，免费代理的可用性是无法保证的。即便是Github上大量代理池实现采用的打分制，也依旧无法代理的质量。我采用的方式是开启大量线程（大量&gt;128）对库中存在的线程进行验证，当不可用代理达到一定比例时立刻开始下一轮验证，否则等待一次循环时间。</li></ul><p>在有大量子进程，如何保证所有子进程正常退出？</p><ul><li>我采用的方式是通过管道发布EXIT信号，子进程在接收到EXIT信号后，将等待所有子线程执行完当前正在处理的请求，将所有请求提交至管道后退出自身，主进程等待数据库插入完毕，存储Redis中剩余的请求队列，展示统计信息，最后退出进程。</li></ul><p>如何对请求计数？</p><ul><li>我采用的方式是每个进程每秒向主进程汇报一次工作状态，由主进程统一收集计数并打印统计信息。</li></ul><p>如何处理不存在的用户？</p><ul><li>为了方便统计，在遇到不存在的用户时，直接生成一个所有数据项为-404、生日为1970-01-01的错误数据项插入数据库。</li></ul><p>退出时如何保证数据插入完毕？</p><ul><li>我采用的方式非常粗暴：</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-3a6175c7e7f55e569064dda92c4cf707_b.jpg\" data-rawwidth=\"845\" data-rawheight=\"420\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"845\" data-original=\"https://pic4.zhimg.com/v2-3a6175c7e7f55e569064dda92c4cf707_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;845&#39; height=&#39;420&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"845\" data-rawheight=\"420\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"845\" data-original=\"https://pic4.zhimg.com/v2-3a6175c7e7f55e569064dda92c4cf707_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-3a6175c7e7f55e569064dda92c4cf707_b.jpg\"/></figure><p>对数据存储进程发布exit信息，然后等待10秒。</p><p>实际上有更优雅的实现方式，例如在插入完成后通过回调对 指定端口发送消息。</p><h2>重难点：</h2><ul><li>多进程/多线程、高负载情况下的数据同步问题：实际上到最后我也没能完全解决这个问题，在程序结束时依旧会出现大量数据丢失的情况。</li><li>对于代理池的维护问题：由于代理池没有使用统一的api，而是直接从Redis中获取，因此无法解决多个进程使用同一个代理地址导致代理快速被ban、多个进程使用同一个失效代理导致浪费大量时间等问题。这个问题可以通过搭建一个统一、成熟的API来解决。例如常见的通过轻量级http服务器提供对代理池的访问。</li><li>空数据、无效数据处理：前文提到，直接生成404数据项插入数据库。</li><li>脏数据：由于要访问5个不同的api才能拼接成一条数据，所以如果代理中途失效，就会出现脏数据。我的解决方案是每次都重新获取一个代理，直到访问成功为止（更好的办法是，存储使用的代理，直到代理失效时才重新获取）。</li></ul><h2>性能瓶颈：</h2><p>最大的性能瓶颈就是对Redis的反复访问。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-c69c19f47dd1857c6ceae347ce7141fa_b.png\" data-rawwidth=\"854\" data-rawheight=\"142\" data-size=\"normal\" class=\"origin_image zh-lightbox-thumb\" width=\"854\" data-original=\"https://pic3.zhimg.com/v2-c69c19f47dd1857c6ceae347ce7141fa_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;854&#39; height=&#39;142&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"854\" data-rawheight=\"142\" data-size=\"normal\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"854\" data-original=\"https://pic3.zhimg.com/v2-c69c19f47dd1857c6ceae347ce7141fa_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-c69c19f47dd1857c6ceae347ce7141fa_b.png\"/><figcaption>简单粗暴没有任何优化的访问方式</figcaption></figure><p>可以看到在while True这里，仅仅只是粗暴的get_proxy后立即访问，捕获访问异常并重试，直到访问成功为止，没有做代理保存复用等操作，性能开销极大。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>代理质量也是原因之一。</p><p>由于免费代理的质量非常低下，访问成功率可能不足20%。在10c*16t下正常速度只能达到1k/min左右的访问速度（每条数据由5个url组成，实际上应该*5），相当于每个线程均摊下来访问速度只有可怜的10s/条，这样导致的不断获取代理并重试是消耗计算性能的罪魁祸首。</p><p>当然，作为一个学习性质的练手项目（划重点：一分钱没花），没有adsl拨号主机/高质量代理池，这样的速度已经基本达到了我的预期（原计划5kw数据）。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>额外的：</h2><p>由于腾讯云没有提供大数据库的导出/备份下载功能，我编写了数据库导出为csv的脚本，源码同样附在下方。</p><hr/><p>在<a href=\"https://link.zhihu.com/?target=https%3A//github.com/OrsPced/BilibiliUserInfo\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">OrsPced的Github</a>，你能阅读到项目的源码。</p><p>在<a href=\"https://link.zhihu.com/?target=https%3A//github.com/OrsPced/BilibiliUserInfo/tree/master/export\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">export</a>文件夹下，你能看到数据库导出为csv的脚本。</p><p>在<a href=\"https://link.zhihu.com/?target=https%3A//github.com/OrsPced/BilibiliUserInfo/tree/master/export/Result\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Result</a>文件夹下，你能看到最终生成的csv文件。每50w行分页，最终有三个csv文件，每个的大小约为45m。</p><hr/><h2>END.</h2>", 
            "topic": [
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }, 
                {
                    "tag": "爬虫 (计算机网络)", 
                    "tagLink": "https://api.zhihu.com/topics/19577498"
                }, 
                {
                    "tag": "python爬虫", 
                    "tagLink": "https://api.zhihu.com/topics/20086364"
                }
            ], 
            "comments": [
                {
                    "userName": "船长", 
                    "userLink": "https://www.zhihu.com/people/1a8e19b3ced052ad0ddf4beb544c0df6", 
                    "content": "别的不说，用户信息是很敏感的东西，很容易吃官司的", 
                    "likes": 2, 
                    "childComments": [
                        {
                            "userName": "羽落", 
                            "userLink": "https://www.zhihu.com/people/b95f10995f987d8e01ace57dc0fb160d", 
                            "content": "没有盈利，数据量不大（相对于b站4亿多的注册用户），没有手机号 身份证号等敏感信息，问题不大吧……应该……", 
                            "likes": 1, 
                            "replyToAuthor": "船长"
                        }, 
                        {
                            "userName": "船长", 
                            "userLink": "https://www.zhihu.com/people/1a8e19b3ced052ad0ddf4beb544c0df6", 
                            "content": "不是按百分比算的，500条就算大量了，我也是做爬虫的，个人建议，不管是自己爬着玩还是工作需求，用户信息这块不要碰", 
                            "likes": 1, 
                            "replyToAuthor": "羽落"
                        }
                    ]
                }, 
                {
                    "userName": "redwolf33790", 
                    "userLink": "https://www.zhihu.com/people/31cc18ea3d0a1a6d37dec2b94a95262c", 
                    "content": "我是小白，最近在学SCRAPY，请问一下高手为什么不用SCRAPY呢？SCRAPY有什么缺点？", 
                    "likes": 1, 
                    "childComments": [
                        {
                            "userName": "羽落", 
                            "userLink": "https://www.zhihu.com/people/b95f10995f987d8e01ace57dc0fb160d", 
                            "content": "我不是高手，我写这个纯粹就是为了玩儿……scrapy作为一个框架，对于练手性质的项目而言过于庞大（除非你就是要练scrapy），而且手动控制爬虫流程，有助于更加深刻的理解爬虫项目中“抓取数据”之外的部分，而这部分很多时候恰恰是最关键的", 
                            "likes": 2, 
                            "replyToAuthor": "redwolf33790"
                        }, 
                        {
                            "userName": "redwolf33790", 
                            "userLink": "https://www.zhihu.com/people/31cc18ea3d0a1a6d37dec2b94a95262c", 
                            "content": "感谢回复，你有什么办法解决<a href=\"http://link.zhihu.com/?target=http%3A//1688.com\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">1688.com</span><span class=\"invisible\"></span></a>的反爬吗？这个网站 点得稍微快些都会触发滑动条", 
                            "likes": 1, 
                            "replyToAuthor": "羽落"
                        }
                    ]
                }, 
                {
                    "userName": "一帆风顺xzf", 
                    "userLink": "https://www.zhihu.com/people/d7f34be4c18c53ca7dcb22847fac063c", 
                    "content": "请问一下免费代理是用的哪个网站的？", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "追魂索魄", 
                    "userLink": "https://www.zhihu.com/people/23ad69eeb1d4fb91f640bf3ea2058358", 
                    "content": "个人信息“敏感”不“敏感”并不是由你定义的。即使是抓取到的数据最好也不要乱分享。", 
                    "likes": 1, 
                    "childComments": [
                        {
                            "userName": "羽落", 
                            "userLink": "https://www.zhihu.com/people/b95f10995f987d8e01ace57dc0fb160d", 
                            "content": "已知悉，感谢您的提醒", 
                            "likes": 0, 
                            "replyToAuthor": "追魂索魄"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/64475480", 
            "userName": "羽落", 
            "userLink": "https://www.zhihu.com/people/b95f10995f987d8e01ace57dc0fb160d", 
            "upvote": 4, 
            "title": "自己动手，用Python一键下载P站每日排行榜", 
            "content": "<p>题图Pid=27899671</p><p>前传：<a href=\"https://zhuanlan.zhihu.com/p/64462335\" class=\"internal\">自己动手，用Python实现Pixiv动图下载器（附模拟登录流程）</a></p><p>在上一篇文章中，我们用requests实现了模拟登录和GIF下载，这篇文章将把重点放在静态图片的爬取和排行榜数据的获取上。</p><h2><b>静态图片下载：</b></h2><p>首先，P站的每一张静态图片，都有square（正矩形，于推荐栏中显示）、small（本体，用于作品页面展示）、original（原图，用于点击图片放大后显示）三个版本，我们要抓取的显然是original，也就是原图版本：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-fba6a89050c9296aade25146f49cc6c3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1611\" data-rawheight=\"484\" class=\"origin_image zh-lightbox-thumb\" width=\"1611\" data-original=\"https://pic4.zhimg.com/v2-fba6a89050c9296aade25146f49cc6c3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1611&#39; height=&#39;484&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1611\" data-rawheight=\"484\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1611\" data-original=\"https://pic4.zhimg.com/v2-fba6a89050c9296aade25146f49cc6c3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-fba6a89050c9296aade25146f49cc6c3_b.jpg\"/></figure><p>可以看到Referer值。把url单独复制到新窗口中打开会得到403（服务器充分地理解了您的请求，并且不想理您），但直接在控制台中双击请求可以正常访问，所以模拟请求时要携带Referer头。</p><p>我们应该如何获得原图URL？</p><p>原图链接的结构很容易分析出来：<a href=\"https://link.zhihu.com/?target=https%3A//i.pximg.net/img-original/img/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">i.pximg.net/img-origina</span><span class=\"invisible\">l/img/</span><span class=\"ellipsis\"></span></a>{发布时间：年/月/日/时/分/秒}/{pid}_p{页数}.{格式}，但分析源码后发现，发布时间并没有以数字格式放在源代码中，而是字符串形式的GMT+0。如果要通过这种方法实现爬取，那么就必须在复杂的字符串操作下修改为日本时区GMT+9，显然是一种不优雅的方案。</p><p>再次分析请求，这次我在<a href=\"https://link.zhihu.com/?target=https%3A//www.pixiv.net/ajax/illust/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">pixiv.net/ajax/illust/</span><span class=\"invisible\"></span></a>请求下发现了urls字段，其中存储了图片各个版本对应的url：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-44c43450be0c21bd209d98888cf6eb03_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1645\" data-rawheight=\"542\" class=\"origin_image zh-lightbox-thumb\" width=\"1645\" data-original=\"https://pic4.zhimg.com/v2-44c43450be0c21bd209d98888cf6eb03_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1645&#39; height=&#39;542&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1645\" data-rawheight=\"542\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1645\" data-original=\"https://pic4.zhimg.com/v2-44c43450be0c21bd209d98888cf6eb03_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-44c43450be0c21bd209d98888cf6eb03_b.jpg\"/></figure><p>接下来的事情就变得十分方便：</p><p>① 构造请求头，访问<a href=\"https://link.zhihu.com/?target=https%3A//www.pixiv.net/ajax/illust/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">pixiv.net/ajax/illust/</span><span class=\"invisible\"></span></a>{pid}，解析出original url</p><p>② 下载图片</p><p>③ 判断下载是否成功，如果成功则使用replace生成下一P的url并回到②</p><div class=\"highlight\"><pre><code class=\"language-text\">cookie = &#34;testcookie&#34;\nurl = &#34;https://www.pixiv.net/member_illust.php?mode=medium&amp;illust_id={pid}&#34;\nimg_info_url = &#34;https://www.pixiv.net/ajax/illust/{pid}&#34;\ndef dowload(pid) :\n    headers = {&#34;user-agent&#34;: UserAgent().random,\n               &#34;cookie&#34;: cookie,\n               &#34;referer&#34;: url.format(pid=pid)}\n    info_url = img_info_url.format(pid=pid)\n    res = requests.get(info_url,headers=headers)\n    js = json.loads(res.text)\n    img_url = js[&#34;body&#34;][&#34;urls&#34;][&#34;original&#34;]\n    replace_template = &#34;_p{page}&#34;\n\n    count = -1\n    while True :\n        img_url = img_url.replace(replace_template.format(page=count),replace_template.format(page=count+1))\n        res = requests.get(img_url,headers=headers)\n        count+=1\n        if res.status_code != 200 :\n            break\n        with open(str(count)+&#34;.jpg&#34;,&#34;wb+&#34;) as fp :\n            fp.write(res.content)</code></pre></div><p>运行此代码并传入pid，就可以下载任意静态作品中的所有图片了。</p><hr/><h2>抓取排行榜数据：</h2><p>P站的排行榜页面主体第一页的加载方式接近静态加载（截止本文撰写日期2019.5.13），只要拉到页面底部触发动态加载，就会看到一个通过ranking.php的请求：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-4a2607c95c18310e4160998f77cd11cf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1205\" data-rawheight=\"434\" class=\"origin_image zh-lightbox-thumb\" width=\"1205\" data-original=\"https://pic4.zhimg.com/v2-4a2607c95c18310e4160998f77cd11cf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1205&#39; height=&#39;434&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1205\" data-rawheight=\"434\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1205\" data-original=\"https://pic4.zhimg.com/v2-4a2607c95c18310e4160998f77cd11cf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-4a2607c95c18310e4160998f77cd11cf_b.jpg\"/></figure><p>其中的数据格式展开后为：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-434062b979f1ef078da894f959b2db61_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"993\" data-rawheight=\"491\" class=\"origin_image zh-lightbox-thumb\" width=\"993\" data-original=\"https://pic2.zhimg.com/v2-434062b979f1ef078da894f959b2db61_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;993&#39; height=&#39;491&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"993\" data-rawheight=\"491\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"993\" data-original=\"https://pic2.zhimg.com/v2-434062b979f1ef078da894f959b2db61_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-434062b979f1ef078da894f959b2db61_b.jpg\"/></figure><p>数据信息非常详尽，包括：当前排名，作品pid，作品tag，作品预览图url等信息，用作排行榜下载已是绰绰有余。</p><hr/><p>整理思路，排行榜下载器的功能划分约为：<br/>① 拉取排行榜数据。拉取数据后将其以PID形式传递给下载器，由下载器负责下载。</p><p>② GIF下载器。虽然将GIF模块单独摘出来会导致模拟登录被执行两次，但好处是GIFDownload可以作为一个单独的项目发布出去（因为懒）。</p><p>③ 静态图片下载。由于GIF与jpg/png采取完全不一致的显示策略，所以这两个模块的拆分是必须的。</p><p>具体实现这里写不下就不写了（笑）</p><p>详细代码已经上传到了github：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/OrsPced/PixivRankDownload\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">羽落的Github</a></p><p>依旧提供exe版的下载链接（虽然每天下载排行榜上的图片是一个很奇怪的需求吧？）：</p><p><a href=\"https://link.zhihu.com/?target=https%3A//pan.baidu.com/s/1TRRxQd9F714eYD7gG2pBiQ\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">PixivRankDownload</a> 提取码：ng9e</p><hr/><p>如果你觉得我写的烂呢你知道该怎么做，如果你觉得我写的好那么可以点赞支持作者继续创作，当然如果你觉得我写的有用，那么收藏一下也是极好的哟~！</p><hr/><p>文末广告时间：寻求广东/湖北地区的爬虫实习/全职岗位，19年应届毕业生，软件技术专业，熟悉Requests、Scrapy、Beautiful Soup等第三方库，熟练使用MySQL，Ridis。</p><p>可知乎私信或者邮件至stardust.whc@gmail.com</p>", 
            "topic": [
                {
                    "tag": "python爬虫", 
                    "tagLink": "https://api.zhihu.com/topics/20086364"
                }, 
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }, 
                {
                    "tag": "爬虫 (计算机网络)", 
                    "tagLink": "https://api.zhihu.com/topics/19577498"
                }
            ], 
            "comments": [
                {
                    "userName": "一夫日虫虫", 
                    "userLink": "https://www.zhihu.com/people/d538f823cc7758bcaf70f265db9835d1", 
                    "content": "[耶]好好努力", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "羽落", 
                            "userLink": "https://www.zhihu.com/people/b95f10995f987d8e01ace57dc0fb160d", 
                            "content": "多谢支持！", 
                            "likes": 0, 
                            "replyToAuthor": "一夫日虫虫"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/62621734", 
            "userName": "羽落", 
            "userLink": "https://www.zhihu.com/people/b95f10995f987d8e01ace57dc0fb160d", 
            "upvote": 57, 
            "title": "使用Python破解代理网站反爬策略，获取大量免费代理", 
            "content": "<blockquote>题图Pid=54895318</blockquote><p>最近在做关于某视频网站爬虫时发现了一个可以通过API大量提取免费代理的网站，但美中不足的是该网站的反爬措施相当严格（？），便着手破解。</p><p>直接使用requests.get访问，返回的结果为混淆后的JS代码，查看状态码为521：</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"kn\">import</span> <span class=\"nn\">requests</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"n\">requests</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s2\">&#34;http://www.66ip.cn/mo.php?tqsl=1024&#34;</span><span class=\"p\">,</span> <span class=\"n\">headers</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s2\">&#34;User-Agent&#34;</span><span class=\"p\">:</span> <span class=\"s2\">&#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36 Edge/18.17763&#34;</span><span class=\"p\">,})</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">response</span><span class=\"o\">.</span><span class=\"n\">text</span><span class=\"p\">)</span>\n<span class=\"o\">&lt;</span><span class=\"n\">script</span><span class=\"o\">&gt;</span><span class=\"n\">var</span> <span class=\"n\">x</span><span class=\"o\">=</span><span class=\"s2\">&#34;@@@17@chars@d@substr@return@while@fromCharCode@eval@join@firstChild@Tue@RegExp@onreadystatechange@for@2@5@if@@Array@toString@parseInt@as@1555403838@hantom@@catch@09@innerHTML@__p@addEventListener@attachEvent@@Apr@2Bn@gCZ@div@window@@@5L@new@@@@GMT@reverse@@@@@rOm9XFMtA3QKV7nYsPGT4lifyWwkq5vcjH2IdxUoCbhERLaz81DNB6@function@DOMContentLoaded@location@match@0xFF@replace@@challenge@String@@href@SGZ@@16@false@@cookie@Path@@@e@0xEDB88320@charAt@8@g@@@f@@Expires@length@https@@search@37@4@0@@36@@1@@a@QM@try@@@split@FWC@1500@D@setTimeout@@toLowerCase@JgSe0upZ@@captcha@19@@@18@@@@else@var@@B6hQ@charCodeAt@pathname@document@@createElement@__jsl_clearance&#34;</span><span class=\"o\">.</span><span class=\"n\">replace</span><span class=\"p\">(</span><span class=\"o\">/@*</span><span class=\"err\">$</span><span class=\"o\">/</span><span class=\"p\">,</span><span class=\"s2\">&#34;&#34;</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">(</span><span class=\"s2\">&#34;@&#34;</span><span class=\"p\">),</span><span class=\"n\">y</span><span class=\"o\">=</span><span class=\"s2\">&#34;3r 42=1o(){3d(&#39;1q.23=1q.40+1q.2q.1t(/[</span><span class=\"se\">\\\\</span><span class=\"s2\">?|&amp;]3i-20/,</span><span class=\"se\">\\\\</span><span class=\"s2\">&#39;</span><span class=\"se\">\\\\</span><span class=\"s2\">&#39;)&#39;,3b);41.29=&#39;44=q.4|2t|&#39;+(1o(){3r 42=m(+[[-~[]]+[j]]),2=[&#39;%&#39;,[-~[-~</span><span class=\"si\">{}</span><span class=\"s2\">-~</span><span class=\"si\">{}</span><span class=\"s2\">]],&#39;3a%16&#39;,[!/!/+[]][2t].2f(-~[-~</span><span class=\"si\">{}</span><span class=\"s2\">-~</span><span class=\"si\">{}</span><span class=\"s2\">]),&#39;17&#39;,[</span><span class=\"si\">{}</span><span class=\"s2\">+[]][2t].2f(i-~[]-~</span><span class=\"si\">{}</span><span class=\"s2\">-~</span><span class=\"si\">{}</span><span class=\"s2\">),&#39;1c&#39;,[-~((-~</span><span class=\"si\">{}</span><span class=\"s2\">&lt;&lt;((+!!/!/)|-~(+!!/!/))))]+(19[&#39;11&#39;+&#39;r&#39;+&#39;p&#39;]+[]+[[]][2t]).2f((+!</span><span class=\"si\">{}</span><span class=\"s2\">)),&#39;24&#39;,[-~(+[])-~[]+2s],&#39;35&#39;,[-~(+[])-~[]+2s]+(-~[(-~</span><span class=\"si\">{}</span><span class=\"s2\">+[-~</span><span class=\"si\">{}</span><span class=\"s2\">-~</span><span class=\"si\">{}</span><span class=\"s2\">]&gt;&gt;-~</span><span class=\"si\">{}</span><span class=\"s2\">-~</span><span class=\"si\">{}</span><span class=\"s2\">)+(-~</span><span class=\"si\">{}</span><span class=\"s2\">+[-~</span><span class=\"si\">{}</span><span class=\"s2\">-~</span><span class=\"si\">{}</span><span class=\"s2\">]&gt;&gt;-~</span><span class=\"si\">{}</span><span class=\"s2\">-~</span><span class=\"si\">{}</span><span class=\"s2\">)]+[]+[[]][2t])+(-~[(-~</span><span class=\"si\">{}</span><span class=\"s2\">+[-~</span><span class=\"si\">{}</span><span class=\"s2\">-~</span><span class=\"si\">{}</span><span class=\"s2\">]&gt;&gt;-~</span><span class=\"si\">{}</span><span class=\"s2\">-~</span><span class=\"si\">{}</span><span class=\"s2\">)+(-~</span><span class=\"si\">{}</span><span class=\"s2\">+[-~</span><span class=\"si\">{}</span><span class=\"s2\">-~</span><span class=\"si\">{}</span><span class=\"s2\">]&gt;&gt;-~</span><span class=\"si\">{}</span><span class=\"s2\">-~</span><span class=\"si\">{}</span><span class=\"s2\">)]+[]+[[]][2t]),&#39;3t%&#39;,(-~(+!!/!/)+[]+[[]][2t]),&#39;3c&#39;];h(3r 38=2t;38&lt;2.2n;38++){42.1i()[38]=2[38]};8 42.c(&#39;&#39;)})()+&#39;;2m=e, 26-15-3j u:2r:3m 1h;2a=/;&#39;};k((1o(){36{8 !!19.12;}t(2d){8 27;}})()){41.12(&#39;1p&#39;,42,27)}3q{41.13(&#39;g&#39;,42)}&#34;</span><span class=\"p\">,</span><span class=\"n\">f</span><span class=\"o\">=</span><span class=\"n\">function</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span><span class=\"n\">y</span><span class=\"p\">){</span><span class=\"n\">var</span> <span class=\"n\">a</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"n\">b</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"n\">c</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">;</span><span class=\"n\">x</span><span class=\"o\">=</span><span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">(</span><span class=\"s2\">&#34;&#34;</span><span class=\"p\">);</span><span class=\"n\">y</span><span class=\"o\">=</span><span class=\"n\">y</span><span class=\"o\">||</span><span class=\"mi\">99</span><span class=\"p\">;</span><span class=\"k\">while</span><span class=\"p\">((</span><span class=\"n\">a</span><span class=\"o\">=</span><span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">shift</span><span class=\"p\">())</span><span class=\"o\">&amp;&amp;</span><span class=\"p\">(</span><span class=\"n\">b</span><span class=\"o\">=</span><span class=\"n\">a</span><span class=\"o\">.</span><span class=\"n\">charCodeAt</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span><span class=\"o\">-</span><span class=\"mf\">77.5</span><span class=\"p\">))</span><span class=\"n\">c</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"n\">Math</span><span class=\"o\">.</span><span class=\"nb\">abs</span><span class=\"p\">(</span><span class=\"n\">b</span><span class=\"p\">)</span><span class=\"o\">&lt;</span><span class=\"mi\">13</span><span class=\"err\">?</span><span class=\"p\">(</span><span class=\"n\">b</span><span class=\"o\">+</span><span class=\"mf\">48.5</span><span class=\"p\">):</span><span class=\"n\">parseInt</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span><span class=\"mi\">36</span><span class=\"p\">))</span><span class=\"o\">+</span><span class=\"n\">y</span><span class=\"o\">*</span><span class=\"n\">c</span><span class=\"p\">;</span><span class=\"k\">return</span> <span class=\"n\">c</span><span class=\"p\">},</span><span class=\"n\">z</span><span class=\"o\">=</span><span class=\"n\">f</span><span class=\"p\">(</span><span class=\"n\">y</span><span class=\"o\">.</span><span class=\"n\">match</span><span class=\"p\">(</span><span class=\"o\">/</span>\\<span class=\"n\">w</span><span class=\"o\">/</span><span class=\"n\">g</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">sort</span><span class=\"p\">(</span><span class=\"n\">function</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span><span class=\"n\">y</span><span class=\"p\">){</span><span class=\"k\">return</span> <span class=\"n\">f</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span><span class=\"o\">-</span><span class=\"n\">f</span><span class=\"p\">(</span><span class=\"n\">y</span><span class=\"p\">)})</span><span class=\"o\">.</span><span class=\"n\">pop</span><span class=\"p\">());</span><span class=\"k\">while</span><span class=\"p\">(</span><span class=\"n\">z</span><span class=\"o\">++</span><span class=\"p\">)</span><span class=\"k\">try</span><span class=\"p\">{</span><span class=\"nb\">eval</span><span class=\"p\">(</span><span class=\"n\">y</span><span class=\"o\">.</span><span class=\"n\">replace</span><span class=\"p\">(</span><span class=\"o\">/</span>\\<span class=\"n\">b</span>\\<span class=\"n\">w</span><span class=\"o\">+</span>\\<span class=\"n\">b</span><span class=\"o\">/</span><span class=\"n\">g</span><span class=\"p\">,</span> <span class=\"n\">function</span><span class=\"p\">(</span><span class=\"n\">y</span><span class=\"p\">){</span><span class=\"k\">return</span> <span class=\"n\">x</span><span class=\"p\">[</span><span class=\"n\">f</span><span class=\"p\">(</span><span class=\"n\">y</span><span class=\"p\">,</span><span class=\"n\">z</span><span class=\"p\">)</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">||</span><span class=\"p\">(</span><span class=\"s2\">&#34;_&#34;</span><span class=\"o\">+</span><span class=\"n\">y</span><span class=\"p\">)}));</span><span class=\"k\">break</span><span class=\"p\">}</span><span class=\"n\">catch</span><span class=\"p\">(</span><span class=\"n\">_</span><span class=\"p\">){}</span><span class=\"o\">&lt;/</span><span class=\"n\">script</span><span class=\"o\">&gt;</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">response</span><span class=\"o\">.</span><span class=\"n\">status_code</span><span class=\"p\">)</span>\n<span class=\"mi\">521</span></code></pre></div><p>查看请求头：</p><div class=\"highlight\"><pre><code class=\"language-text\">&gt;&gt;&gt; print(response.headers)\n{&#39;Server&#39;: &#39;nginx&#39;, &#39;Date&#39;: &#39;Tue, 16 Apr 2019 08:33:41 GMT&#39;, &#39;Transfer-Encoding&#39;: &#39;chunked&#39;, &#39;Connection&#39;: &#39;close, close&#39;, &#39;X-Via-JSL&#39;: &#39;b3ca7e7,-&#39;, &#39;Set-Cookie&#39;: &#39;__jsluid=800e5382bd0c39f56b244d87cf2615a3; max-age=31536000; path=/; HttpOnly&#39;}</code></pre></div><p>搜索并整理资料后得到的答案是：这段JS代码会在混淆后的字符串中生成JS代码字符串，再将其eval执行真正的逻辑代码生成cookie，和headers中的Set-Cookie项合并，最后刷新网页用真正的cookie访问服务器得到数据。</p><p>于是第一反应是执行js，但该段js用js2py/execjs执行均会报错（涉及到一个暗坑，见文末彩蛋），遂采用selenium+ChromeDriver的方式取得请求头。由于cookie通常拥有一定的有效期，为了降低调用浏览器的频率，我们获取到cookie后将其保存，下次检测到cookie失效时再调用，代码如下：</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"kn\">from</span> <span class=\"nn\">selenium</span> <span class=\"k\">import</span> <span class=\"n\">webdriver</span>\n<span class=\"n\">driver</span> <span class=\"o\">=</span> <span class=\"n\">webdriver</span><span class=\"o\">.</span><span class=\"n\">Chrome</span><span class=\"p\">()</span>\n<span class=\"n\">driver</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s2\">&#34;http://www.66ip.cn/mo.php?tqsl=1024&#34;</span><span class=\"p\">)</span>\n<span class=\"n\">cookie</span> <span class=\"o\">=</span> <span class=\"n\">driver</span><span class=\"o\">.</span><span class=\"n\">get_cookies</span><span class=\"p\">()</span>\n<span class=\"n\">driver</span><span class=\"o\">.</span><span class=\"n\">close</span><span class=\"p\">()</span></code></pre></div><p>检查获取到的cookie：</p><div class=\"highlight\"><pre><code class=\"language-text\">&gt;&gt;&gt; print(cookie)\n[{&#39;domain&#39;: &#39;www.66ip.cn&#39;, &#39;expiry&#39;: 1586943449.796784, &#39;httpOnly&#39;: True, &#39;name&#39;: &#39;__jsluid&#39;, &#39;path&#39;: &#39;/&#39;, &#39;secure&#39;: False, &#39;value&#39;: &#39;73da79ccc591971704ffebff501eb26e&#39;}, {&#39;domain&#39;: &#39;www.66ip.cn&#39;, &#39;expiry&#39;: 1555411050, &#39;httpOnly&#39;: False, &#39;name&#39;: &#39;__jsl_clearance&#39;, &#39;path&#39;: &#39;/&#39;, &#39;secure&#39;: False, &#39;value&#39;: &#39;1555407450.549|0|Ad6%2B78qFTS188pb2kOoKzQtjo2Y%3D&#39;}]</code></pre></div><p>检查浏览器中的cookie：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-c6667a9ac85af2bf166cfb9d48e9506e_b.jpg\" data-rawwidth=\"630\" data-rawheight=\"389\" data-size=\"normal\" class=\"origin_image zh-lightbox-thumb\" width=\"630\" data-original=\"https://pic3.zhimg.com/v2-c6667a9ac85af2bf166cfb9d48e9506e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;630&#39; height=&#39;389&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"630\" data-rawheight=\"389\" data-size=\"normal\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"630\" data-original=\"https://pic3.zhimg.com/v2-c6667a9ac85af2bf166cfb9d48e9506e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-c6667a9ac85af2bf166cfb9d48e9506e_b.jpg\"/><figcaption>使用浏览器打开网页时生成的cookie</figcaption></figure><p>不难发现最后使用的cookie就是driver.get_cookies()的name与value生成的键值对。OK，那么现在生成cookie并测试：</p><div class=\"highlight\"><pre><code class=\"language-text\">cookie = driver.get_cookies()\nstr_cookie = &#34;&#34;\nfor data in cookie:\n    str_cookie += data[&#34;name&#34;] + &#34;=&#34; + data[&#34;value&#34;] + &#34;; &#34;\nstr_cookie = str_cookie[:-2]\nresponse = requests.get(&#34;http://www.66ip.cn/mo.php?tqsl=1024&#34;,headers={\n            &#34;User-Agent&#34;: &#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36&#34;,\n            &#34;cookie&#34; : str_cookie})\nprint(response)\n&gt;&gt;&gt; &lt;Response [200]&gt;</code></pre></div><p>（在重新整理、测试这一段代码时发现了一个新的细节：JS生成的代码与User-Agent相关，换句话说并不能通过fake_useragent的random功能用随机字符串去访问。）</p><p>OK！似乎一切都完成了，我们添加无头模式参数再次测试：</p><div class=\"highlight\"><pre><code class=\"language-text\">from selenium import webdriver\n#添加无头参数\nfrom selenium.webdriver.chrome.options import Options\nchrome_options = Options()\nchrome_options.add_argument(&#39;--headless&#39;)\n\ndriver = webdriver.Chrome(options=chrome_options)\ndriver.get(&#34;http://www.66ip.cn/mo.php?tqsl=1024&#34;)\ncookie = driver.get_cookies()\nstr_cookie = &#34;&#34;\nfor data in cookie:\n    str_cookie += data[&#34;name&#34;] + &#34;=&#34; + data[&#34;value&#34;] + &#34;; &#34;\nstr_cookie = str_cookie[:-2]\nresponse = requests.get(&#34;http://www.66ip.cn/mo.php?tqsl=1024&#34;,headers={\n            &#34;User-Agent&#34;: &#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36&#34;,\n            &#34;cookie&#34; : str_cookie})\nprint(response)\n&gt;&gt;&gt; &lt;Response [521]&gt;</code></pre></div><p>奇怪的是，在无头模式下获取的Cookie并不能用来作为requests.get()的参数，以此作为参数时依然会被反爬措施拦截。推测是因为无头模式启动时会设置window[&#39;__phantomas&#39;]对象的值，使其不为&#34;undefined&#34;，从而破坏最终输出，使校验失败。</p><p>虽然在GUI模式下启动Chrome也可以凑合用，但如果就这样甘于GUI模式，每次运行时一个chrome一闪而过，简直将Python的优雅破坏殆尽（其实是不想在挂着爬虫玩游戏时被打扰）（笑）</p><p>从JS本身入手，使用JS在线格式化工具查看格式化后的JS代码：</p><div class=\"highlight\"><pre><code class=\"language-js\"><span class=\"kd\">var</span> <span class=\"nx\">x</span> <span class=\"o\">=</span> <span class=\"s2\">&#34;@@@17@chars@d@substr@return@while@fromCharCode@eval@join@firstChild@Tue@RegExp@onreadystatechange@for@2@5@if@@Array@toString@parseInt@as@1555403838@hantom@@catch@09@innerHTML@__p@addEventListener@attachEvent@@Apr@2Bn@gCZ@div@window@@@5L@new@@@@GMT@reverse@@@@@rOm9XFMtA3QKV7nYsPGT4lifyWwkq5vcjH2IdxUoCbhERLaz81DNB6@function@DOMContentLoaded@location@match@0xFF@replace@@challenge@String@@href@SGZ@@16@false@@cookie@Path@@@e@0xEDB88320@charAt@8@g@@@f@@Expires@length@https@@search@37@4@0@@36@@1@@a@QM@try@@@split@FWC@1500@D@setTimeout@@toLowerCase@JgSe0upZ@@captcha@19@@@18@@@@else@var@@B6hQ@charCodeAt@pathname@document@@createElement@__jsl_clearance&#34;</span><span class=\"p\">.</span><span class=\"nx\">replace</span><span class=\"p\">(</span><span class=\"sr\">/@*$/</span><span class=\"p\">,</span> <span class=\"s2\">&#34;&#34;</span><span class=\"p\">).</span><span class=\"nx\">split</span><span class=\"p\">(</span><span class=\"s2\">&#34;@&#34;</span><span class=\"p\">),</span>\n<span class=\"nx\">y</span> <span class=\"o\">=</span> <span class=\"s2\">&#34;3r 42=1o(){3d(&#39;1q.23=1q.40+1q.2q.1t(/[\\\\?|&amp;]3i-20/,\\\\&#39;\\\\&#39;)&#39;,3b);41.29=&#39;44=q.4|2t|&#39;+(1o(){3r 42=m(+[[-~[]]+[j]]),2=[&#39;%&#39;,[-~[-~{}-~{}]],&#39;3a%16&#39;,[!/!/+[]][2t].2f(-~[-~{}-~{}]),&#39;17&#39;,[{}+[]][2t].2f(i-~[]-~{}-~{}),&#39;1c&#39;,[-~((-~{}&lt;&lt;((+!!/!/)|-~(+!!/!/))))]+(19[&#39;11&#39;+&#39;r&#39;+&#39;p&#39;]+[]+[[]][2t]).2f((+!{})),&#39;24&#39;,[-~(+[])-~[]+2s],&#39;35&#39;,[-~(+[])-~[]+2s]+(-~[(-~{}+[-~{}-~{}]&gt;&gt;-~{}-~{})+(-~{}+[-~{}-~{}]&gt;&gt;-~{}-~{})]+[]+[[]][2t])+(-~[(-~{}+[-~{}-~{}]&gt;&gt;-~{}-~{})+(-~{}+[-~{}-~{}]&gt;&gt;-~{}-~{})]+[]+[[]][2t]),&#39;3t%&#39;,(-~(+!!/!/)+[]+[[]][2t]),&#39;3c&#39;];h(3r 38=2t;38&lt;2.2n;38++){42.1i()[38]=2[38]};8 42.c(&#39;&#39;)})()+&#39;;2m=e, 26-15-3j u:2r:3m 1h;2a=/;&#39;};k((1o(){36{8 !!19.12;}t(2d){8 27;}})()){41.12(&#39;1p&#39;,42,27)}3q{41.13(&#39;g&#39;,42)}&#34;</span><span class=\"p\">,</span>\n<span class=\"nx\">f</span> <span class=\"o\">=</span> <span class=\"kd\">function</span><span class=\"p\">(</span><span class=\"nx\">x</span><span class=\"p\">,</span> <span class=\"nx\">y</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"kd\">var</span> <span class=\"nx\">a</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">,</span>\n    <span class=\"nx\">b</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">,</span>\n    <span class=\"nx\">c</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">;</span>\n    <span class=\"nx\">x</span> <span class=\"o\">=</span> <span class=\"nx\">x</span><span class=\"p\">.</span><span class=\"nx\">split</span><span class=\"p\">(</span><span class=\"s2\">&#34;&#34;</span><span class=\"p\">);</span>\n    <span class=\"nx\">y</span> <span class=\"o\">=</span> <span class=\"nx\">y</span> <span class=\"o\">||</span> <span class=\"mi\">99</span><span class=\"p\">;</span>\n    <span class=\"k\">while</span> <span class=\"p\">((</span><span class=\"nx\">a</span> <span class=\"o\">=</span> <span class=\"nx\">x</span><span class=\"p\">.</span><span class=\"nx\">shift</span><span class=\"p\">())</span> <span class=\"o\">&amp;&amp;</span> <span class=\"p\">(</span><span class=\"nx\">b</span> <span class=\"o\">=</span> <span class=\"nx\">a</span><span class=\"p\">.</span><span class=\"nx\">charCodeAt</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"mf\">77.5</span><span class=\"p\">))</span> <span class=\"nx\">c</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"nb\">Math</span><span class=\"p\">.</span><span class=\"nx\">abs</span><span class=\"p\">(</span><span class=\"nx\">b</span><span class=\"p\">)</span> <span class=\"o\">&lt;</span> <span class=\"mi\">13</span> <span class=\"o\">?</span> <span class=\"p\">(</span><span class=\"nx\">b</span> <span class=\"o\">+</span> <span class=\"mf\">48.5</span><span class=\"p\">)</span> <span class=\"o\">:</span> <span class=\"nb\">parseInt</span><span class=\"p\">(</span><span class=\"nx\">a</span><span class=\"p\">,</span> <span class=\"mi\">36</span><span class=\"p\">))</span> <span class=\"o\">+</span> <span class=\"nx\">y</span> <span class=\"o\">*</span> <span class=\"nx\">c</span><span class=\"p\">;</span>\n    <span class=\"k\">return</span> <span class=\"nx\">c</span>\n<span class=\"p\">},</span>\n<span class=\"nx\">z</span> <span class=\"o\">=</span> <span class=\"nx\">f</span><span class=\"p\">(</span><span class=\"nx\">y</span><span class=\"p\">.</span><span class=\"nx\">match</span><span class=\"p\">(</span><span class=\"sr\">/\\w/g</span><span class=\"p\">).</span><span class=\"nx\">sort</span><span class=\"p\">(</span><span class=\"kd\">function</span><span class=\"p\">(</span><span class=\"nx\">x</span><span class=\"p\">,</span> <span class=\"nx\">y</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"k\">return</span> <span class=\"nx\">f</span><span class=\"p\">(</span><span class=\"nx\">x</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"nx\">f</span><span class=\"p\">(</span><span class=\"nx\">y</span><span class=\"p\">)</span>\n<span class=\"p\">}).</span><span class=\"nx\">pop</span><span class=\"p\">());</span>\n<span class=\"k\">while</span> <span class=\"p\">(</span><span class=\"nx\">z</span><span class=\"o\">++</span><span class=\"p\">)</span> <span class=\"k\">try</span> <span class=\"p\">{</span>\n    <span class=\"c1\">//重点\n</span><span class=\"c1\"></span>    <span class=\"nb\">eval</span><span class=\"p\">(</span><span class=\"nx\">y</span><span class=\"p\">.</span><span class=\"nx\">replace</span><span class=\"p\">(</span><span class=\"sr\">/\\b\\w+\\b/g</span><span class=\"p\">,</span>\n    <span class=\"kd\">function</span><span class=\"p\">(</span><span class=\"nx\">y</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n        <span class=\"k\">return</span> <span class=\"nx\">x</span><span class=\"p\">[</span><span class=\"nx\">f</span><span class=\"p\">(</span><span class=\"nx\">y</span><span class=\"p\">,</span> <span class=\"nx\">z</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"o\">||</span> <span class=\"p\">(</span><span class=\"s2\">&#34;_&#34;</span> <span class=\"o\">+</span> <span class=\"nx\">y</span><span class=\"p\">)</span>\n    <span class=\"p\">}));</span>\n    <span class=\"k\">break</span>\n<span class=\"p\">}</span> <span class=\"k\">catch</span><span class=\"p\">(</span><span class=\"nx\">_</span><span class=\"p\">)</span> <span class=\"p\">{}</span>\n</code></pre></div><p>（吐槽一句知乎编辑Python为什么没有彩色提示？）</p><p>可以看到最终使用eval执行了生成的字符串，我们用console.log替换eval，复制到浏览器中执行查看输出：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-20bbe44f611ddb2e74877aeebcbac1e5_b.jpg\" data-rawwidth=\"1920\" data-rawheight=\"651\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"1920\" data-original=\"https://pic2.zhimg.com/v2-20bbe44f611ddb2e74877aeebcbac1e5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1920&#39; height=&#39;651&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1920\" data-rawheight=\"651\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1920\" data-original=\"https://pic2.zhimg.com/v2-20bbe44f611ddb2e74877aeebcbac1e5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-20bbe44f611ddb2e74877aeebcbac1e5_b.jpg\"/></figure><p>获取到了新代码，格式化后分析：</p><div class=\"highlight\"><pre><code class=\"language-text\">var _42 = function() {\n    setTimeout(&#39;location.href=location.pathname+location.search.replace(/[\\?|&amp;]captcha-challenge/,\\&#39;\\&#39;)&#39;, 1500);\n    //生成cookie\n    document.cookie = &#39;__jsl_clearance=1555403838.17|0|&#39; + (function() {\n        var _42 = Array( + [[ - ~ []] + [5]]),\n        _2 = [&#39;%&#39;, [ - ~ [ - ~ {} - ~ {}]], &#39;FWC%2Bn&#39;, [!/!/ + []][0].charAt( - ~ [ - ~ {} - ~ {}]), &#39;gCZ&#39;, [{} + []][0].charAt(2 - ~ [] - ~ {} - ~ {}), &#39;5L&#39;, \n        [ - ~ (( - ~ {} &lt;&lt; (( + !!/!/) | -~ ( + !!/!/))))] + (window[&#39;__p&#39; + &#39;hantom&#39; + &#39;as&#39;] + [] + [[]][0]).charAt(( + !{})), &#39;SGZ&#39;, [ - ~ ( + []) - ~ [] + 4], &#39;QM&#39;, \n        [ - ~ ( + []) - ~ [] + 4] + ( - ~ [( - ~ {} + [ - ~ {} - ~ {}] &gt;&gt; -~ {} - ~ {}) + ( - ~ {} + [ - ~ {} - ~ {}] &gt;&gt; -~ {} - ~ {})] + [] + [[]][0]) + \n        ( - ~ [( - ~ {} + [ - ~ {} - ~ {}] &gt;&gt; -~ {} - ~ {}) + ( - ~ {} + [ - ~ {} - ~ {}] &gt;&gt; -~ {} - ~ {})] + [] + [[]][0]), \n        &#39;B6hQ%&#39;, ( - ~ ( + !!/!/) + [] + [[]][0]), &#39;D&#39;];\n        for (var _38 = 0; _38 &lt; _2.length; _38++) {\n            _42.reverse()[_38] = _2[_38]\n        };\n        return _42.join(&#39;&#39;)\n    })() + &#39;;Expires=Tue, 16-Apr-19 09:37:18 GMT;Path=/;&#39;\n};\nif ((function() {\n    try {\n        return !! window.addEventListener;\n    } catch(e) {\n        return false;\n    }\n})()) {\n    document.addEventListener(&#39;DOMContentLoaded&#39;, _42, false)\n} else {\n    document.attachEvent(&#39;onreadystatechange&#39;, _42)\n}</code></pre></div><p>可以看到最终生成的cookie被赋值给了document.cookie。我们将生成语句复制出来运行，查看结果：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-33e187b342344b668fc148aea27783f8_b.jpg\" data-rawwidth=\"1570\" data-rawheight=\"310\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"1570\" data-original=\"https://pic1.zhimg.com/v2-33e187b342344b668fc148aea27783f8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1570&#39; height=&#39;310&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1570\" data-rawheight=\"310\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1570\" data-original=\"https://pic1.zhimg.com/v2-33e187b342344b668fc148aea27783f8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-33e187b342344b668fc148aea27783f8_b.jpg\"/></figure><p>NICE！似乎已经万事大吉了。</p><p>整理思路：访问API，得到&lt;Response [521]&gt;，保存headers中的cookie，运行js代码，将js生成的cookie与headers中的cookie合并后再次请求API得到数据。</p><p>根据思路写出代码（js运行库选择js2py）：</p><div class=\"highlight\"><pre><code class=\"language-text\">def main():\n    response = requests.get(&#34;http://www.66ip.cn/mo.php?tqsl=1024&#34;,\n                    headers={&#34;User-Agent&#34;: &#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36 Edge/18.17763&#34;,})\n    #保存第一段cookie\n    cookie=response.headers[&#34;Set-Cookie&#34;]\n    js = response.text.encode(&#34;utf8&#34;).decode(&#34;utf8&#34;)\n    #删除script标签并替换eval。\n    js = js.replace(&#34;&lt;script&gt;&#34;,&#34;&#34;).replace(&#34;&lt;/script&gt;&#34;,&#34;&#34;).replace(&#34;{eval(&#34;,&#34;{var data1 = (&#34;).replace(chr(0),chr(32))\n    #使用js2py的js交互功能获得刚才赋值的data1对象\n    context = js2py.EvalJs()\n    context.execute(js)\n    js_temp = context.data1\n    \n    #找到cookie生成语句的起始位置\n    index1 = js_temp.find(&#34;document.&#34;)\n    index2 = js_temp.find(&#34;};if((&#34;)\n    #故技重施，替换代码中的对象以获得数据\n    js_temp = js_temp[index1:index2].replace(&#34;document.cookie&#34;,&#34;data2&#34;)\n    context.execute(js_temp)\n    data = context.data2\n    \n    #合并cookie，重新请求网站。\n    cookie += &#34;;&#34;+data\n    response = requests.get(&#34;http://www.66ip.cn/mo.php?tqsl=1024&#34;, headers={\n        &#34;User-Agent&#34;: &#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36 Edge/18.17763&#34;,\n        &#34;cookie&#34; : cookie,\n    })\n    return response\nif __name__ == &#34;__main__&#34; :\n    main()</code></pre></div><p>检查返回值：</p><div class=\"highlight\"><pre><code class=\"language-text\">&lt;Response [200]&gt;\n#正文省略</code></pre></div><p>大功告成！</p><p>（实际上这个网站有多个不同的反爬BACKUP，这只是其中出现频率最高、也是破解难度最低的一种，但只要不频繁抓取，并不会触发BACKUP）（碎碎念）</p><p>正文完，以下是彩蛋时间。</p><hr/><p>记性好的同学可能还记得前文所说的“但该段js用js2py/execjs执行均会报错”，眼尖的同学也会看到最终代码中的诡异的replace(chr(0),chr(32))。那么这一段是什么意思呢？</p><p>实际上这里从response.text直接获取到的代码，空格全部是ascii下编码为0（即C语言中的字符串结束符&#39;\\0&#39;）的空字符，而jy2py/execjs/node.js等实现下均将其视为非法字符——换句话说，这段js直接输出为.js文件，用各种方法都是无法运行的。</p><p>而windows的复制，则会自动将这些NUL全部替换为ASCII的空格符，由此导致一个近乎灵异的状况——with open write后的文件无论被任何方式运行都会报错，而复制文件内容到另一个文件保存后却可以运行，可以说是个不友好的“彩蛋”吧。</p><p>毕竟为这个奇怪的实现花掉了一个小时。（笑）（也可能是这个实现在某些地方有着别的用途？）</p><hr/><p>文末卫星时间：近期将会发布一个b站用户信息爬虫以及爬取的数据，自行实现免费的ProxyPool，爬虫框架只使用了Requests。实测在代理池稳定后，抓取速度约为800 items / min。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-956b0805fa784659c3e402eadd159f6e_b.jpg\" data-rawwidth=\"715\" data-rawheight=\"408\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"715\" data-original=\"https://pic3.zhimg.com/v2-956b0805fa784659c3e402eadd159f6e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;715&#39; height=&#39;408&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"715\" data-rawheight=\"408\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"715\" data-original=\"https://pic3.zhimg.com/v2-956b0805fa784659c3e402eadd159f6e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-956b0805fa784659c3e402eadd159f6e_b.jpg\"/></figure><p>文末广告时间：寻求广州/武汉地区的爬虫实习/全职岗位，19年应届毕业生，软件技术专业，熟悉爬虫常用的Requests、Scrapy、Beautiful Soup等第三方库，熟练使用MySQL，Ridis。</p><p>可知乎私信或者邮件至stardust.whc@gmail.com详谈。</p><p>——</p><p>搜索了一下，知乎上还没有人发过HTTP 521的解决方案，所以我这篇算是知乎首发吧？（笑）</p><p>END.</p>", 
            "topic": [
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }, 
                {
                    "tag": "python爬虫", 
                    "tagLink": "https://api.zhihu.com/topics/20086364"
                }, 
                {
                    "tag": "爬虫 (计算机网络)", 
                    "tagLink": "https://api.zhihu.com/topics/19577498"
                }
            ], 
            "comments": [
                {
                    "userName": "道信", 
                    "userLink": "https://www.zhihu.com/people/26588556232688e22aecf1bb829344f7", 
                    "content": "沙发", 
                    "likes": 1, 
                    "childComments": []
                }, 
                {
                    "userName": "凉皮", 
                    "userLink": "https://www.zhihu.com/people/95d3ecfaf2c04de2e4ad762d4a40fec0", 
                    "content": "最近刚开始自学python，之前没有任何编程经验，一个人学习真没意思呀[捂脸]", 
                    "likes": 2, 
                    "childComments": [
                        {
                            "userName": "羽落", 
                            "userLink": "https://www.zhihu.com/people/b95f10995f987d8e01ace57dc0fb160d", 
                            "content": "我从C -&gt; C++ -&gt; Python都是一路自学过来的，最近已经在找工作了。别灰心，每个人都是一步一步走过来的", 
                            "likes": 6, 
                            "replyToAuthor": "凉皮"
                        }, 
                        {
                            "userName": "Alpha Li", 
                            "userLink": "https://www.zhihu.com/people/88b2b461fe85b3010564653b3887c7df", 
                            "content": "我也在学，数据分析方向的", 
                            "likes": 0, 
                            "replyToAuthor": "凉皮"
                        }
                    ]
                }, 
                {
                    "userName": "yami我tm舔爆", 
                    "userLink": "https://www.zhihu.com/people/e495332df1f523b13363fe276b62c962", 
                    "content": "yami加油(ง •̀_•́)ง", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "emmm", 
                    "userLink": "https://www.zhihu.com/people/8ec1b95c4610cb536d9c7e2c17a71872", 
                    "content": "<p><a href=\"http://link.zhihu.com/?target=http%3A//www.66ip.cn/nmtq.php%3Fgetnum%3D10\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">66ip.cn/nmtq.php?</span><span class=\"invisible\">getnum=10</span><span class=\"ellipsis\"></span></a> 我一直都在这个网站搞代理,不过不需要cookies吧?我一直没用</p>", 
                    "likes": 1, 
                    "childComments": []
                }, 
                {
                    "userName": "emmm", 
                    "userLink": "https://www.zhihu.com/people/8ec1b95c4610cb536d9c7e2c17a71872", 
                    "content": "<p>我是直接访问,不过有时候会返回拒接连接.... 不知道是不是网络问题,要请求好几次,特别是scrapy框架吐血</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "emmm", 
                    "userLink": "https://www.zhihu.com/people/8ec1b95c4610cb536d9c7e2c17a71872", 
                    "content": "<p>今天试了下..... 返回521了..</p><a class=\"comment_sticker\" href=\"https://pic2.zhimg.com/v2-e213ddb29e5a2adb54d6343d5dea27d1.gif\" data-width=\"\" data-height=\"\">[摊手]</a>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "羽落", 
                            "userLink": "https://www.zhihu.com/people/b95f10995f987d8e01ace57dc0fb160d", 
                            "content": "不瞒你说，我自己用的代理池已经改回selenium了。这篇文章的目的还是学习为主", 
                            "likes": 0, 
                            "replyToAuthor": "emmm"
                        }, 
                        {
                            "userName": "emmm", 
                            "userLink": "https://www.zhihu.com/people/8ec1b95c4610cb536d9c7e2c17a71872", 
                            "content": "<p>你还是用这个网站抓的ip吗?我试了用selenium抓,但是网页加载好慢,有什么办法能确定页面出现ip直接抓取,不需要等待页面加载完成嘛?</p>", 
                            "likes": 0, 
                            "replyToAuthor": "羽落"
                        }
                    ]
                }, 
                {
                    "userName": "观棋不宇", 
                    "userLink": "https://www.zhihu.com/people/44fa28b1ed774d3186b9c09048792955", 
                    "content": "这确实是一种最典型的cookie加密，楼主加油＾０＾~", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "狄克推多", 
                    "userLink": "https://www.zhihu.com/people/bdf004e1e036e9de7dd865abc3dada60", 
                    "content": "这是加速乐，之前也破解过", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "redwolf33790", 
                    "userLink": "https://www.zhihu.com/people/31cc18ea3d0a1a6d37dec2b94a95262c", 
                    "content": "我想问一下，网站 把反爬搞得那么复杂，浏览器是怎么区分 这些网站 的规则 的？", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "羽落", 
                            "userLink": "https://www.zhihu.com/people/b95f10995f987d8e01ace57dc0fb160d", 
                            "content": "并不复杂，对于浏览器而言就是执行了一段js代码，和你打开百度时加载的js代码没有本质上的区别。除非你想在没有浏览器环境的情况下获取js的运行结果，情况才会变得复杂起来。", 
                            "likes": 0, 
                            "replyToAuthor": "redwolf33790"
                        }, 
                        {
                            "userName": "redwolf33790", 
                            "userLink": "https://www.zhihu.com/people/31cc18ea3d0a1a6d37dec2b94a95262c", 
                            "content": "能加一下您 的微信，向您请教一下爬虫的问题吗？", 
                            "likes": 0, 
                            "replyToAuthor": "羽落"
                        }
                    ]
                }, 
                {
                    "userName": "三六十八", 
                    "userLink": "https://www.zhihu.com/people/c39ca3725781ea03267493c8e226e55a", 
                    "content": "写到ridis的时候觉得大神已经开始飘了，哈哈哈，厉害，膝盖给你了", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/64462335", 
            "userName": "羽落", 
            "userLink": "https://www.zhihu.com/people/b95f10995f987d8e01ace57dc0fb160d", 
            "upvote": 3, 
            "title": "自己动手，用Python实现Pixiv动图下载器（附模拟登录流程）", 
            "content": "<blockquote>题图Pid=74478717</blockquote><p>在P站上看到好看的GIF却苦于不能直接下载的时候，身为程序猿应该怎么办？</p><p>当然是重复造轮子（划掉）</p><p>当然是自己写一个动图下载器了！</p><hr/><p>老规矩，首先分析GIF请求信息。开启Chrome-F12-Network页面后刷新GIF页面，检查所有请求，会看到一个名为ugoira_meta的数据包，查看详细信息：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-dc4c3058b028f147e693b1f42a638cb5_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1643\" data-rawheight=\"231\" class=\"origin_image zh-lightbox-thumb\" width=\"1643\" data-original=\"https://pic2.zhimg.com/v2-dc4c3058b028f147e693b1f42a638cb5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1643&#39; height=&#39;231&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1643\" data-rawheight=\"231\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1643\" data-original=\"https://pic2.zhimg.com/v2-dc4c3058b028f147e693b1f42a638cb5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-dc4c3058b028f147e693b1f42a638cb5_b.png\"/></figure><p>能够看到一个名为originalSrc，值为一个zip文件url的属性。</p><p>展开frames字段：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-821faf344335b9775954d4d9bb08bd37_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"498\" data-rawheight=\"322\" class=\"origin_image zh-lightbox-thumb\" width=\"498\" data-original=\"https://pic4.zhimg.com/v2-821faf344335b9775954d4d9bb08bd37_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;498&#39; height=&#39;322&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"498\" data-rawheight=\"322\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"498\" data-original=\"https://pic4.zhimg.com/v2-821faf344335b9775954d4d9bb08bd37_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-821faf344335b9775954d4d9bb08bd37_b.jpg\"/></figure><p>可以看到这就是动图的帧信息了。</p><p>我们使用代码将originalSrc字段显示的zip文件下载下来：</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"n\">ua</span> <span class=\"o\">=</span> <span class=\"s2\">&#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36&#34;</span>\n<span class=\"n\">cookie</span> <span class=\"o\">=</span> <span class=\"s2\">&#34;testcookie&#34;</span>\n<span class=\"n\">referer</span> <span class=\"o\">=</span> <span class=\"s2\">&#34;https://www.pixiv.net/member_illust.php?mode=medium&amp;illust_id=74512123&#34;</span>\n\n<span class=\"n\">zip_url</span> <span class=\"o\">=</span> <span class=\"s2\">&#34;https://i.pximg.net/img-zip-ugoira/img/2019/05/02/14/18/59/74512123_ugoira1920x1080.zip&#34;</span>\n<span class=\"n\">headers</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"s2\">&#34;user-agent&#34;</span> <span class=\"p\">:</span> <span class=\"n\">ua</span><span class=\"p\">,</span>\n    <span class=\"s2\">&#34;cookie&#34;</span> <span class=\"p\">:</span> <span class=\"n\">cookie</span><span class=\"p\">,</span>\n    <span class=\"s2\">&#34;referer&#34;</span> <span class=\"p\">:</span> <span class=\"n\">referer</span><span class=\"p\">,</span>\n<span class=\"p\">}</span>\n<span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"n\">requests</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"n\">url</span><span class=\"p\">,</span><span class=\"n\">headers</span><span class=\"o\">=</span><span class=\"n\">headers</span><span class=\"p\">)</span>\n<span class=\"k\">with</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"s2\">&#34;test.zip&#34;</span><span class=\"p\">,</span><span class=\"s2\">&#34;wb+&#34;</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">fp</span> <span class=\"p\">:</span>\n    <span class=\"n\">fp</span><span class=\"o\">.</span><span class=\"n\">write</span><span class=\"p\">(</span><span class=\"n\">response</span><span class=\"o\">.</span><span class=\"n\">content</span><span class=\"p\">)</span></code></pre></div><p>运行后，脚本目录下会生成名为test.zip的压缩文件，解压后可以看到以6位数字顺序命名的jpg文件（和frames字段一一对应）。</p><p>在网页上任何时刻右键另存为都只能得到一个独立的jpg文件，显然网页并没有帮我们把静态帧合并为GIF，仅仅只是循环展示静态帧，那么合并这一步就要我们自己实现了。</p><p>至此得到下载GIF的过程：</p><p>① 构造请求头，请求ugoira_meta接口得到originalSrc url</p><p>② 再次构造请求头下载originalSrc ZIP文件</p><p>③ 在本地解压ZIP得到静态帧，利用frames字段提供的信息合并出GIF</p><p>④ 清除静态帧文件。</p><p>代码如下（GIF生成使用了imageio）：</p><div class=\"highlight\"><pre><code class=\"language-text\">from fake_useragent import UserAgent\nimport json,requests,zipfile,os\nimport imageio\n\ndef download(pid) :\n    file_path = str(pid)\n    headers = {&#34;user-agent&#34;: UserAgent().random,&#34;cookie&#34; : cookie}\n    # 获取gif信息，提取zip url\n    gif_info = json.loads(requests.get(self.gif_info_url.format(pid=pid), headers=headers).text)\n    delay = [item[&#34;delay&#34;] for item in gif_info[&#34;body&#34;][&#34;frames&#34;]]\n    delay = sum(delay) / len(delay)\n    zip_url = gif_info[&#34;body&#34;][&#34;originalSrc&#34;]\n\n    # 下载压缩包\n    gif_data = requests.get(zip_url, headers=headers)\n    gif_data = gif_data.content\n    try:\n        os.mkdir(file_path)\n    except Exception:\n        pass\n    zip_path = os.path.join(file_path, &#34;temp.zip&#34;)\n    with open(zip_path, &#34;wb+&#34;) as fp:\n        fp.write(gif_data)\n    # 生成文件\n    temp_file_list = []\n    zipo = zipfile.ZipFile(zip_path, &#34;r&#34;)\n    for file in zipo.namelist():\n        temp_file_list.append(os.path.join(file_path, file))\n        zipo.extract(file, file_path)\n    zipo.close()\n    # 读取所有静态图片，合成gif\n    image_data = []\n    for file in temp_file_list:\n        image_data.append(imageio.imread(file))\n    imageio.mimsave(os.path.join(file_path, str(pid) + &#34;.gif&#34;), image_data, &#34;GIF&#34;, duration=delay / 1000)\n    # 清除所有中间文件。\n    for file in temp_file_list:\n        os.remove(file)\n    os.remove(zip_path)</code></pre></div><hr/><h2>关于cookie：</h2><p>实际上P站的反爬机制非常简单，只有两点：cookie和referer。</p><p>referer的意思是，Pixiv会检查页面内每一个请求的referer地址，页面内请求必须是由对应的页面发出的（比如上文中下载的zip文件），所以我们需要在请求中附带referer信息。</p><p>cookie无需解释，只需要进行模拟登录并保存cookie即可</p><p>分析P站登录请求：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-0ea212b7248b4730e7fdd3bc1de70e3d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1116\" data-rawheight=\"459\" class=\"origin_image zh-lightbox-thumb\" width=\"1116\" data-original=\"https://pic2.zhimg.com/v2-0ea212b7248b4730e7fdd3bc1de70e3d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1116&#39; height=&#39;459&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1116\" data-rawheight=\"459\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1116\" data-original=\"https://pic2.zhimg.com/v2-0ea212b7248b4730e7fdd3bc1de70e3d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-0ea212b7248b4730e7fdd3bc1de70e3d_b.jpg\"/></figure><p>pixiv_id&amp;password：账号密码</p><p>source：登录平台标识</p><p>ref：反爬标识</p><p>return_to：登录后跳转的网页。</p><p>而这个不知所以的post_key......</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-459d7df66f12ef5d1cee3e5e85fe92ea_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"850\" data-rawheight=\"244\" class=\"origin_image zh-lightbox-thumb\" width=\"850\" data-original=\"https://pic3.zhimg.com/v2-459d7df66f12ef5d1cee3e5e85fe92ea_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;850&#39; height=&#39;244&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"850\" data-rawheight=\"244\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"850\" data-original=\"https://pic3.zhimg.com/v2-459d7df66f12ef5d1cee3e5e85fe92ea_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-459d7df66f12ef5d1cee3e5e85fe92ea_b.jpg\"/></figure><p>在源码中直接ctrl+f就可以看到，它是直接写在源码里的，甚至没有加密，但每次访问时都会随机生成。</p><p>（可以说是非常支持第三方小工具的发展了）</p><p>至此，得出模拟登录流程：</p><p>① 访问登录页面，提取post_key</p><p>② 通过post请求提交账号密码</p><p>③ 保存cookie</p><p>下面是代码：</p><div class=\"highlight\"><pre><code class=\"language-text\">login_post_url = &#34;https://accounts.pixiv.net/api/login?lang=zh&#34;\nlogin_data_url = &#34;https://accounts.pixiv.net/login?lang=zh&amp;source=pc&amp;view_type=page&amp;ref=wwwtop_accounts_index&#34;\n\nsession = requests.session()\ndata = session.get(url=login_data_url, headers=self.headers).content.decode(&#34;utf8&#34;)\npost_key = bs4.BeautifulSoup(data, &#34;lxml&#34;).find(attrs={&#34;name&#34;: &#34;post_key&#34;})[&#34;value&#34;]\nlogin_data = {\n    &#34;pixiv_id&#34;: PIXIV_ID,\n    &#34;password&#34;: PASSWORD,\n    &#34;post_key&#34;: post_key,\n    &#34;source&#34;: &#34;pc&#34;,\n    &#34;ref&#34;: &#34;wwwtop_accounts_index&#34;,\n    &#34;return_to&#34;: &#34;https://www.pixiv.net/&#34;,\n}\nsession.post(url=login_post_url, data=login_data)\nsession.close()\n\ncookey = requests.utils.dict_from_cookiejar(session.cookies)\ncookie = &#34;&#34;\nfor k, v in cookey.items():\n    cookie += k + &#34;=&#34; + v + &#34;; &#34;</code></pre></div><p>写一个类，每次启动时自动登录并获取最新的cookie，就可以愉快的下载GIF啦！</p><p>刚开始写文章，欢迎各位 dalao 批评指正</p><p>详细代码见<a href=\"https://link.zhihu.com/?target=https%3A//github.com/OrsPced/PixivRankDownload/blob/master/GIFDownload.py\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">我的github</a></p><p>exe版（点击即用）：<a href=\"https://link.zhihu.com/?target=https%3A//pan.baidu.com/s/1eOyFg2huGEaredZ4TGvaWQ\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">百度云链接</a> 提取码：y2wv</p><p>END. </p><hr/><p>文末广告时间：寻求广州/武汉地区的爬虫实习/全职岗位，19年应届毕业生，软件技术专业，熟悉Requests、Scrapy、Beautiful Soup等第三方库，熟练使用MySQL，Ridis。</p><p>可知乎私信或者邮件至stardust.whc@gmail.com</p>", 
            "topic": [
                {
                    "tag": "Python 入门", 
                    "tagLink": "https://api.zhihu.com/topics/19661050"
                }, 
                {
                    "tag": "python爬虫", 
                    "tagLink": "https://api.zhihu.com/topics/20086364"
                }, 
                {
                    "tag": "爬虫 (计算机网络)", 
                    "tagLink": "https://api.zhihu.com/topics/19577498"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/64595889", 
            "userName": "羽落", 
            "userLink": "https://www.zhihu.com/people/b95f10995f987d8e01ace57dc0fb160d", 
            "upvote": 1, 
            "title": "要租房又不想自己找怎么办？用Scrapy爬取租房信息", 
            "content": "<div class=\"highlight\"><pre><code class=\"language-text\">题图Pid=75051139</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>对于临近毕业的大学生（指自己）而言，怎样寻找便宜又实惠的房源无疑是人人都在关心的问题，今天就来看看怎样用爬虫技术快速抓取房源信息。</p><h2><b>运行环境：</b></h2><p>Python 3.6.3</p><p>Scrapy  : 1.5.1</p><p>Twisted   : 18.9.0</p><p>BeautifulSoup ：4.6.3</p><p>fake_useragent  ：0.1.11</p><p>js2py ：0.60</p><p>PyMySQL ：0.9.3</p><h2>难度：初级</h2><p>网站总体难度较低，但联系方式的获取需要运用正则表达式/js引擎提取参数，携带参数和cookie后访问接口，因此对于爬虫初学者而言具有一定难度。</p><h2>爬取到的数据：</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-073f0871582cc625be4af001c13b205a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1619\" data-rawheight=\"643\" class=\"origin_image zh-lightbox-thumb\" width=\"1619\" data-original=\"https://pic3.zhimg.com/v2-073f0871582cc625be4af001c13b205a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1619&#39; height=&#39;643&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1619\" data-rawheight=\"643\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1619\" data-original=\"https://pic3.zhimg.com/v2-073f0871582cc625be4af001c13b205a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-073f0871582cc625be4af001c13b205a_b.jpg\"/></figure><p>（由于这个网站并不是把所有数据按顺序展示，而是每一页近乎随机的展示60条房源信息，且并不保证每一页数据不重复，因此一次运行无法爬取所有数据。本次运行选取的地区为广州，爬取的数据为2279条，时间为2019-05-01 23:53:13 ~ 2019-05-02 02:56:01。）</p><h2>网站分析：</h2><p>本次爬取的网站反爬手段相当单一，几乎所有数据都是静态加载，因此采取最初级的获取列表-获取单页-提取数据-翻页的流程即可。</p><p>在几个月前爬取这个网站时，房源联系人的联系方式都直接写在页面源码里，但几个月后再次爬取，发现联系方式变为了ajax动态加载，且ajax接口参数依赖于页面源码，导致速度被大大拖慢，目前没有找到可行的解决方法，希望有思路的朋友指点一二。</p><p>首先，创建scrapy项目：</p><div class=\"highlight\"><pre><code class=\"language-text\">scrapy startproject zufang</code></pre></div><p>在zufang/settings.py里，填写如下内容：</p><div class=\"highlight\"><pre><code class=\"language-text\">HTTPERROR_ALLOWED_CODES = [404,400,502,503]\nDOWNLOAD_TIMEOUT = 5\nDOWNLOAD_DELAY = 1.5</code></pre></div><p>● HTTPERROR_ALLOWED_CODES ：允许被中间件处理的Response。状态码未在HTTPERROR_ALLOWED_CODES中声明的Response将被Scrapy引擎直接抛弃。</p><p>● DOWNLOAD_TIMEOUT ：最大超时时间。超过DOWNLOAD_TIMEOUT值的请求将会触发twisted.internet.TimeoutError异常（可被Middleware的process_exception方法捕获）。</p><p>● DOWNLOAD_DELAY ：下载延迟。Scrapy保证请求间隔不小于DOWNLOAD_DELAY。</p><p>在item.py中写入如下内容：</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"kn\">import</span> <span class=\"nn\">scrapy</span>\n\n<span class=\"k\">class</span> <span class=\"nc\">ZufangItem</span><span class=\"p\">(</span><span class=\"n\">scrapy</span><span class=\"o\">.</span><span class=\"n\">Item</span><span class=\"p\">):</span>\n    <span class=\"n\">house_url</span> <span class=\"o\">=</span> <span class=\"n\">scrapy</span><span class=\"o\">.</span><span class=\"n\">Field</span><span class=\"p\">()</span>      <span class=\"c1\">#房屋链接</span>\n    <span class=\"n\">house_name</span> <span class=\"o\">=</span> <span class=\"n\">scrapy</span><span class=\"o\">.</span><span class=\"n\">Field</span><span class=\"p\">()</span>     <span class=\"c1\">#房源名字</span>\n    <span class=\"n\">price</span> <span class=\"o\">=</span> <span class=\"n\">scrapy</span><span class=\"o\">.</span><span class=\"n\">Field</span><span class=\"p\">()</span>          <span class=\"c1\">#房源租金</span>\n    <span class=\"n\">house_type</span> <span class=\"o\">=</span> <span class=\"n\">scrapy</span><span class=\"o\">.</span><span class=\"n\">Field</span><span class=\"p\">()</span>     <span class=\"c1\">#房屋类型</span>\n    <span class=\"n\">house_area</span> <span class=\"o\">=</span> <span class=\"n\">scrapy</span><span class=\"o\">.</span><span class=\"n\">Field</span><span class=\"p\">()</span>     <span class=\"c1\">#房屋面积</span>\n    <span class=\"n\">rental_method</span> <span class=\"o\">=</span> <span class=\"n\">scrapy</span><span class=\"o\">.</span><span class=\"n\">Field</span><span class=\"p\">()</span>  <span class=\"c1\">#出租方式</span>\n    <span class=\"n\">community</span> <span class=\"o\">=</span> <span class=\"n\">scrapy</span><span class=\"o\">.</span><span class=\"n\">Field</span><span class=\"p\">()</span>      <span class=\"c1\">#所在小区</span>\n    <span class=\"n\">gender</span> <span class=\"o\">=</span> <span class=\"n\">scrapy</span><span class=\"o\">.</span><span class=\"n\">Field</span><span class=\"p\">()</span>         <span class=\"c1\">#性别要求</span>\n    <span class=\"n\">deposit</span> <span class=\"o\">=</span> <span class=\"n\">scrapy</span><span class=\"o\">.</span><span class=\"n\">Field</span><span class=\"p\">()</span>        <span class=\"c1\">#押金方式</span>\n    <span class=\"n\">contact</span> <span class=\"o\">=</span> <span class=\"n\">scrapy</span><span class=\"o\">.</span><span class=\"n\">Field</span><span class=\"p\">()</span>        <span class=\"c1\">#联系人</span>\n    <span class=\"n\">phone</span> <span class=\"o\">=</span> <span class=\"n\">scrapy</span><span class=\"o\">.</span><span class=\"n\">Field</span><span class=\"p\">()</span>          <span class=\"c1\">#联系手机</span>\n    <span class=\"n\">time</span> <span class=\"o\">=</span> <span class=\"n\">scrapy</span><span class=\"o\">.</span><span class=\"n\">Field</span><span class=\"p\">()</span>           <span class=\"c1\">#数据获取时间</span></code></pre></div><p>如果想要了解一个租房信息，那么价格、面积、地址等信息显然是我们关注的部分，保存房屋链接可以让我们看中某个房源时方便的找到房源页面而不需要重新搜索，数据获取时间可以一定程度上体现数据的可用性——毕竟好的房源不会在网站上挂太久（笑）</p><p>在settings/spiders中创建文件houseSpider.py，并写入如下内容：</p><div class=\"highlight\"><pre><code class=\"language-text\">import json,time\nfrom fake_useragent import UserAgent\nimport bs4\nimport zufang.items\nimport scrapy\nimport js2py\n\nclass houseSpider(scrapy.Spider):\n    name = &#34;mainSpider&#34;\n    ua = UserAgent()\n    urls = [&#34;https://gz.zu.anjuke.com/fangyuan/p1/&#34;,] #起始url地址</code></pre></div><p>这里要注意的是，文件名houseSpider.py和类名houseSpider都不影响爬虫调用，scrapy引擎只会根据类的name属性来查找、调用爬虫。</p><p>静态页面的分析为避免拖延篇幅就不做展开，直接贴代码：</p><div class=\"highlight\"><pre><code class=\"language-text\">    def start_requests(self):\n        for url in self.urls :\n            yield scrapy.Request(url = url,\n                                 headers={&#34;user-agent&#34;:self.ua.random},\n                                 callback=self.parse_house_list,\n                                dont_filter=True)\n\n    def parse_house_list(self, response):\n        #解析网页。提取并请求其中所有的房源信息页。\n        soup = bs4.BeautifulSoup(response.body.decode(&#34;utf8&#34;), &#34;lxml&#34;)\n        info_list = soup.find_all(class_=&#34;zu-itemmod&#34;)\n        url_list = [url.a[&#34;href&#34;] for url in info_list]\n        for url in url_list:\n            yield scrapy.Request(url = url,\n                                 headers={&#34;user-agent&#34;:self.ua.random},\n                                 callback=self.parse_house,\n                                dont_filter=True)\n\n        #请求下一页。\n        next_url = soup.find(class_=&#34;aNxt&#34;)\n        if next_url != None :\n            yield scrapy.Request(url=next_url[&#34;href&#34;],\n                                headers={&#34;user-agent&#34;: self.ua.random},\n                                callback=self.parse_house_list,\n                                dont_filter=True)\n\n    def parse_house(self,response):\n        cookie = response.headers[&#34;Set-Cookie&#34;].decode(&#34;utf8&#34;)\n        item = zufang.items.ZufangItem()\n        soup = bs4.BeautifulSoup(response.body.decode(&#34;utf8&#34;),&#34;lxml&#34;)\n        try :\n            #房屋链接\n            item[&#34;house_url&#34;] = response.url\n            #房源名字\n            item[&#34;house_name&#34;] = soup.find(class_=&#34;house-title&#34;).get_text()\n            #租金\n            item[&#34;price&#34;] = soup.find(class_=&#34;price&#34;).em.get_text()\n            #房屋类型\n            item[&#34;house_type&#34;] = soup.find_all(class_=&#34;house-info-item l-width&#34;)[0].find_all(name=&#34;span&#34;)[-1].get_text()\n            #房屋面积\n            item[&#34;house_area&#34;] = soup.find(class_=&#34;info-tag no-line&#34;).em.get_text()\n            #出租方式\n            item[&#34;rental_method&#34;] = soup.find(class_=&#34;full-line cf&#34;).find_all(name=&#34;span&#34;)[1].get_text()\n            #所在小区\n            item[&#34;community&#34;] = soup.find_all(class_=&#34;house-info-item l-width&#34;)[2].a.get_text()\n            #性别要求\n            gender = soup.find_all(class_=&#34;house-info-item&#34;)[-1].find_all(&#34;span&#34;)[-1].get_text()\n            if &#34;小区&#34; in gender :\n                gender = &#34;暂无&#34;\n            item[&#34;gender&#34;] = gender\n            #押金方式\n            item[&#34;deposit&#34;] = soup.find(class_=&#34;full-line cf&#34;).find_all(&#34;span&#34;)[1].get_text()\n            #联系人\n            item[&#34;contact&#34;] = soup.find(class_=&#34;broker-name&#34;).get_text()</code></pre></div><p>详细讲一点：联系方式。</p><p>在房源页面点击查看电话后，在Network页面搜索对应的号码，能看到一个专门用于获取电话的请求：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-0fd22f94c4a9c658382495dcb6adea54_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1774\" data-rawheight=\"411\" class=\"origin_image zh-lightbox-thumb\" width=\"1774\" data-original=\"https://pic1.zhimg.com/v2-0fd22f94c4a9c658382495dcb6adea54_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1774&#39; height=&#39;411&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1774\" data-rawheight=\"411\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1774\" data-original=\"https://pic1.zhimg.com/v2-0fd22f94c4a9c658382495dcb6adea54_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-0fd22f94c4a9c658382495dcb6adea54_b.jpg\"/></figure><p>查看请求链接：</p><div class=\"highlight\"><pre><code class=\"language-text\">https://gz.zu.anjuke.com/v3/ajax/getBrokerPhone/?broker_id=5688071&amp;token=0e03375fa6f3c05dd35e34aeb2b52590&amp;prop_id=1320361235&amp;prop_city_id=12&amp;house_type=1&amp;captcha=</code></pre></div><p>忽略为空的captcha，共有五个参数，分别是broker_id，token，prop_id，prop_city_id，house_type。</p><p>初学者看到这么多参数可能就晕了，但其实只要在源码里ctrl+f就可以看到：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-9dd6413570af62ae9ff29593b8e2d160_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"524\" data-rawheight=\"529\" class=\"origin_image zh-lightbox-thumb\" width=\"524\" data-original=\"https://pic1.zhimg.com/v2-9dd6413570af62ae9ff29593b8e2d160_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;524&#39; height=&#39;529&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"524\" data-rawheight=\"529\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"524\" data-original=\"https://pic1.zhimg.com/v2-9dd6413570af62ae9ff29593b8e2d160_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-9dd6413570af62ae9ff29593b8e2d160_b.jpg\"/></figure><p>五个参数都在源码里写着，我们需要做得事情只是把它提取出来而已。</p><p>用正则提取显然会比较麻烦，这时候我们可以用到js2py了。使用方式非常简单，获取脚本文本，建立js2py.EvalJs对象，执行js文本后获取返回值，然后用模板替换出url，直接请求即可。</p><div class=\"highlight\"><pre><code class=\"language-text\">phone_template = &#34;https://gz.zu.anjuke.com/v3/ajax/getBrokerPhone/?broker_id={broker_id}&amp;token={token}&amp;prop_id={prop_id}&amp;prop_city_id={prop_city_id}&amp;house_type={house_type}&#34;\n\njs = soup.find_all(name=&#34;script&#34;)\ncontext = js2py.EvalJs()\nfor i in js:\n    if &#34;brokerPhone&#34; in i.get_text():\n        context.execute(i.get_text())\n        data_dict = getattr(context, &#34;__Json4fe&#34;)\n\n        broker_id = data_dict[&#34;getPhoneParam&#34;][&#34;broker_id&#34;]\n        token = data_dict[&#34;token&#34;]\n        prop_id = data_dict[&#34;prop_id&#34;]\n        prop_city_id = data_dict[&#34;prop_city_id&#34;]\n        house_type = data_dict[&#34;house_type&#34;]\n        \n        yield scrapy.Request(url = self.phone_template.format(\n                        broker_id=broker_id,\n                        token=token,\n                        prop_id = prop_id,\n                        prop_city_id = prop_city_id,\n                        house_type = house_type,\n                    ),headers={&#34;user-agent&#34;:self.ua.random,&#34;cookie&#34; : cookie},\n                    meta = {&#34;item&#34;:item,&#34;url&#34; :response.url,},\n                    callback=self.parse_phone,dont_filter=True)\n                    break</code></pre></div><p>眼尖的读者应该看到了，headers里还额外提交了cookie参数，这个cookie是哪来的呢？</p><p>重新打开主页面的请求信息，可以看到Response Headers里set-cookie项：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-691121fd822c94fd7f53c387ed79e903_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1015\" data-rawheight=\"313\" class=\"origin_image zh-lightbox-thumb\" width=\"1015\" data-original=\"https://pic4.zhimg.com/v2-691121fd822c94fd7f53c387ed79e903_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1015&#39; height=&#39;313&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1015\" data-rawheight=\"313\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1015\" data-original=\"https://pic4.zhimg.com/v2-691121fd822c94fd7f53c387ed79e903_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-691121fd822c94fd7f53c387ed79e903_b.jpg\"/></figure><p>这里设置的cookie也是获取联系号码的必备参数，实际效果类似于<a href=\"https://zhuanlan.zhihu.com/p/64462335\" class=\"internal\">自己动手，用Python实现Pixiv动图下载器（附模拟登录流程）</a>中Pixiv使用的Referer头。</p><p>在发送请求前保存cookie即可：</p><div class=\"highlight\"><pre><code class=\"language-text\">cookie = response.headers[&#34;Set-Cookie&#34;].decode(&#34;utf8&#34;)</code></pre></div><p>获取到联系号码后，将其保存，并同时保存当前时间，最后返回处理完成的item：</p><div class=\"highlight\"><pre><code class=\"language-text\">    def parse_phone(self,response):\n        item = response.meta[&#34;item&#34;]\n        js = json.loads(response.text)\n        item[&#34;phone&#34;] = &#34;&#34;.join(js[&#34;val&#34;].split())\n        item[&#34;time&#34;] = time.strftime(&#34;%Y-%m-%d %H:%M:%S&#34;, time.localtime())\n        yield item</code></pre></div><p>在项目目录下新建run.py文件，并输入以下内容：</p><div class=\"highlight\"><pre><code class=\"language-text\">from scrapy import cmdline\n\nif __name__ == &#34;__main__&#34; :\n    cmdline.execute(&#34;scrapy crawl mainSpider&#34;.split())</code></pre></div><p>运行run.py即可直接启动Scrapy项目，而不用每次都切换到命令行界面了。</p><p>运行run.py，就可以在控制台中看到提取完成的数据了：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-5cd84bc880e7008c8a6571d446ef9de3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"740\" data-rawheight=\"333\" class=\"origin_image zh-lightbox-thumb\" width=\"740\" data-original=\"https://pic4.zhimg.com/v2-5cd84bc880e7008c8a6571d446ef9de3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;740&#39; height=&#39;333&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"740\" data-rawheight=\"333\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"740\" data-original=\"https://pic4.zhimg.com/v2-5cd84bc880e7008c8a6571d446ef9de3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-5cd84bc880e7008c8a6571d446ef9de3_b.jpg\"/></figure><h2>数据入库：</h2><p>对于这种小规模的爬虫，数据入库部分非常简单，无需在意性能开销、网络传输开销等，直接在Pipelines中编写SaveDataPipeline即可。利用twisted.enterprise.adbapi实现异步插入：</p><div class=\"highlight\"><pre><code class=\"language-text\">from twisted.enterprise import adbapi\nimport zufang.settings as settings\n\nclass SaveDataPipeline(object):\n    #SQL命令模板\n    insert_template = &#34;&#34;&#34;\n        INSERT INTO house_table VALUES(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s);\n    &#34;&#34;&#34;\n    def __init__(self):\n        self.dbpool = adbapi.ConnectionPool(&#34;pymysql&#34;,\n                                      host=settings.sqlsetting[&#34;HOST&#34;],\n                                      port=settings.sqlsetting[&#34;PORT&#34;],\n                                      db=settings.sqlsetting[&#34;DB&#34;],\n                                      user=settings.sqlsetting[&#34;USER&#34;],\n                                      password=settings.sqlsetting[&#34;PASSWORD&#34;],\n                                      charset=settings.sqlsetting[&#34;CHARSET&#34;],\n                                      cp_reconnect=True)                        #自动检测失效连接并重连。\n\n    def process_item(self, item, spider):\n        query = self.dbpool.runInteraction(self.insert_data,item)\n        query.addErrback(self.error_hander,item)\n        return item\n\n    def insert_data(self,cursor,item):\n        house_id = item[&#34;house_url&#34;].split(&#34;/&#34;)[-1].split(&#34;?&#34;)[0]\n        cursor.execute(self.insert_template,[house_id,item[&#34;house_url&#34;],item[&#34;house_name&#34;],\n                                            item[&#34;price&#34;],item[&#34;house_type&#34;],item[&#34;house_area&#34;],\n                                             item[&#34;rental_method&#34;],item[&#34;community&#34;],item[&#34;gender&#34;],\n                                             item[&#34;deposit&#34;],item[&#34;contact&#34;],item[&#34;phone&#34;],item[&#34;time&#34;]])\n\n    def error_hander(self,failure,item):\n        #由于网站数据的展示并非有序且唯一，所以主键重复是可能的。捕获后抛出即可。\n        if &#34;for key &#39;PRIMARY&#39;&#34; in str(failure) :\n            print(&#34;主键重复：&#34;,item[&#34;house_url&#34;].split(&#34;/&#34;)[-1])</code></pre></div><p>其中数据库配置在settings中读取：</p><div class=\"highlight\"><pre><code class=\"language-text\">with open(&#34;DataBaseSettings.ini&#34;,&#34;r&#34;) as fp:\n    sqlsetting = json.loads(fp.read())[&#34;default&#34;]</code></pre></div><p>DataBaseSettings.ini使用json格式存储配置，格式如下：</p><div class=\"highlight\"><pre><code class=\"language-text\">{\n    &#34;default&#34;:{\n        &#34;HOST&#34;:&#34;your DB host&#34;,\n        &#34;PORT&#34;:port,\n        &#34;DB&#34;:&#34;house_data&#34;,\n        &#34;USER&#34;:&#34;your DB username&#34;,\n        &#34;PASSWORD&#34;:&#34;your DB password&#34;,\n        &#34;CHARSET&#34;:&#34;utf8&#34;\n    }\n}</code></pre></div><p>（个人认为将敏感配置单独编写是非常好的习惯，可以有效避免诸如“某网站管理员将数据库密码明文上传至GITHUB”一类的惨案发生。）</p><hr/><p>你可以在我的github中下载到本篇文章中的源码：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/OrsPced/CrawlRentalHouse\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">OrsPced</a></p><hr/><h2><b>警告</b>：</h2><p>本篇文章原定发表日期为5月2日，因延迟发表，作者对文章中爬虫规则的有效性不做保证（但你可以留言让我改）</p><hr/><h2><b>END</b></h2>", 
            "topic": [
                {
                    "tag": "python爬虫", 
                    "tagLink": "https://api.zhihu.com/topics/20086364"
                }, 
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }, 
                {
                    "tag": "爬虫 (计算机网络)", 
                    "tagLink": "https://api.zhihu.com/topics/19577498"
                }
            ], 
            "comments": []
        }
    ], 
    "url": "https://zhuanlan.zhihu.com/OrsPced"
}
