{
    "title": "SongBird", 
    "description": "", 
    "followers": [
        "https://www.zhihu.com/people/mu-jin-li-63", 
        "https://www.zhihu.com/people/thinker-mike", 
        "https://www.zhihu.com/people/jian-zhi-66-56", 
        "https://www.zhihu.com/people/liu-kai-peng-24-53", 
        "https://www.zhihu.com/people/wei-lai-72-27", 
        "https://www.zhihu.com/people/higgrn", 
        "https://www.zhihu.com/people/fei-shou-shou-45", 
        "https://www.zhihu.com/people/mu-sen-sen-77-4", 
        "https://www.zhihu.com/people/cnworld", 
        "https://www.zhihu.com/people/tms-17-67", 
        "https://www.zhihu.com/people/ni-shuo-29-27", 
        "https://www.zhihu.com/people/zhou-xin-yu-73-35", 
        "https://www.zhihu.com/people/yu-sui-wu-tong", 
        "https://www.zhihu.com/people/youngzhi", 
        "https://www.zhihu.com/people/bai-bai-bai-23-58", 
        "https://www.zhihu.com/people/zhang-jie-luo", 
        "https://www.zhihu.com/people/jiadingujs", 
        "https://www.zhihu.com/people/zhi-yao-xin-gou-jue-lyh", 
        "https://www.zhihu.com/people/yu-tong-lan", 
        "https://www.zhihu.com/people/cao-ding-19", 
        "https://www.zhihu.com/people/wang-xiao-nao-40-93", 
        "https://www.zhihu.com/people/sdust", 
        "https://www.zhihu.com/people/lpz-71", 
        "https://www.zhihu.com/people/123098-21", 
        "https://www.zhihu.com/people/bang-di-70", 
        "https://www.zhihu.com/people/yi-ji-zhi-chang-71", 
        "https://www.zhihu.com/people/xu-yin-da-58", 
        "https://www.zhihu.com/people/cao-xiu-mian", 
        "https://www.zhihu.com/people/li-wei-long-78-50", 
        "https://www.zhihu.com/people/blueman", 
        "https://www.zhihu.com/people/chen-chao-90-23", 
        "https://www.zhihu.com/people/qia-ta-79", 
        "https://www.zhihu.com/people/jasonzhm-29", 
        "https://www.zhihu.com/people/arpriest", 
        "https://www.zhihu.com/people/wan-li-chao-46", 
        "https://www.zhihu.com/people/icaros", 
        "https://www.zhihu.com/people/mrazz", 
        "https://www.zhihu.com/people/tian-chang-di-jiu-63-41", 
        "https://www.zhihu.com/people/fen-hai-kuang-qu-13", 
        "https://www.zhihu.com/people/AI_CONTROL", 
        "https://www.zhihu.com/people/reseted1532435625700", 
        "https://www.zhihu.com/people/wang-wen-jie-72", 
        "https://www.zhihu.com/people/wang-jie-44-97", 
        "https://www.zhihu.com/people/dong-feng-66-72", 
        "https://www.zhihu.com/people/D.B.JIAN", 
        "https://www.zhihu.com/people/yangsiji9", 
        "https://www.zhihu.com/people/cutxru", 
        "https://www.zhihu.com/people/ge-bo-wen-89", 
        "https://www.zhihu.com/people/wang-shan-71-12", 
        "https://www.zhihu.com/people/li-tian-yi-25-80", 
        "https://www.zhihu.com/people/gu-yu-71-65", 
        "https://www.zhihu.com/people/chen-shi-heng-41", 
        "https://www.zhihu.com/people/zeng-shi-ping", 
        "https://www.zhihu.com/people/asdasd1dsadsa", 
        "https://www.zhihu.com/people/liu-kuan-43-75", 
        "https://www.zhihu.com/people/jsmith-85", 
        "https://www.zhihu.com/people/zhuo-ji-23-72", 
        "https://www.zhihu.com/people/ti-richardo", 
        "https://www.zhihu.com/people/tang-long-30-1", 
        "https://www.zhihu.com/people/newbin-15", 
        "https://www.zhihu.com/people/zero-35-40", 
        "https://www.zhihu.com/people/Poc2ZrBvZ86Hh8Pi9BXN", 
        "https://www.zhihu.com/people/bruce-wayne-60", 
        "https://www.zhihu.com/people/yang-xiao-meng-51-16", 
        "https://www.zhihu.com/people/aakk_2017", 
        "https://www.zhihu.com/people/ty-hao", 
        "https://www.zhihu.com/people/z55250825"
    ], 
    "article": [
        {
            "url": "https://zhuanlan.zhihu.com/p/56159173", 
            "userName": "周清逸", 
            "userLink": "https://www.zhihu.com/people/23dd9239a500559d8850eac45ed2038f", 
            "upvote": 16, 
            "title": "Adjoint Method", 
            "content": "<p>很多时候，我们会希望对一个系统的参数进行优化，此时系统所遵循的物理规律（一般用PDE形式表示）我们是知道的。这类问题称为PDE-constrained optimization，有许多应用场景。解决这类问题的一类方法称为adjoint method，这篇文章会简单介绍这种方法的基本思想。</p><p>首先明确问题：系统的所有参数记为 <img src=\"https://www.zhihu.com/equation?tex=p\" alt=\"p\" eeimg=\"1\"/> ，场变量记为 <img src=\"https://www.zhihu.com/equation?tex=x\" alt=\"x\" eeimg=\"1\"/> （都是向量），并且我们知道它们满足关系 <img src=\"https://www.zhihu.com/equation?tex=g%28x%2C+p%29+%3D+0\" alt=\"g(x, p) = 0\" eeimg=\"1\"/> 。在此基础上，希望优化 <img src=\"https://www.zhihu.com/equation?tex=p\" alt=\"p\" eeimg=\"1\"/> 使loss函数 <img src=\"https://www.zhihu.com/equation?tex=f%28x%2C+p%29\" alt=\"f(x, p)\" eeimg=\"1\"/> 最小化：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cmin_%7Bp%7D+f%28x%2C+p%29+%5Cmathrm%7B%2C~~~subject~to~~%7D+g%28x%2C+p%29+%3D+0.\" alt=\"\\min_{p} f(x, p) \\mathrm{,~~~subject~to~~} g(x, p) = 0.\" eeimg=\"1\"/> </p><p>如果函数 <img src=\"https://www.zhihu.com/equation?tex=f%28%5Ccdot%2C+%5Ccdot%29\" alt=\"f(\\cdot, \\cdot)\" eeimg=\"1\"/> 比较复杂，那么一个很自然的想法是基于梯度进行优化，也就是找出 <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7Bdf%7D%7Bdp%7D\" alt=\"\\frac{df}{dp}\" eeimg=\"1\"/> ，然后对参数 <img src=\"https://www.zhihu.com/equation?tex=p\" alt=\"p\" eeimg=\"1\"/> 进行梯度下降。但是这个导数怎么求呢？</p><p>一个非常暴力的做法是，每次对 <img src=\"https://www.zhihu.com/equation?tex=p\" alt=\"p\" eeimg=\"1\"/> 的一维 <img src=\"https://www.zhihu.com/equation?tex=p_%7Bi%7D\" alt=\"p_{i}\" eeimg=\"1\"/> 加一个小扰动，近似地求出 <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7Bdf%7D%7Bdp_%7Bi%7D%7D\" alt=\"\\frac{df}{dp_{i}}\" eeimg=\"1\"/> ，重复多次。思路很直接，可惜很多时候函数 <img src=\"https://www.zhihu.com/equation?tex=g%28%5Ccdot%2C+%5Ccdot%29\" alt=\"g(\\cdot, \\cdot)\" eeimg=\"1\"/> 的形式可能比较复杂，甚至规律本身是用PDE来表示的，那么对于给定的参数 <img src=\"https://www.zhihu.com/equation?tex=p\" alt=\"p\" eeimg=\"1\"/> 我们解 <img src=\"https://www.zhihu.com/equation?tex=x\" alt=\"x\" eeimg=\"1\"/> 的过程（也就是所谓的simulation）就会比较耗时。在这种情况下，为了一步梯度下降而进行多次simulation，是不现实的——算不动。</p><p>所谓adjoint method，就是针对这个问题而诞生的。让我们先看一个比较特殊的情形：</p><p>（1） <img src=\"https://www.zhihu.com/equation?tex=f%28x%2C+p%29%3Df%28x%29\" alt=\"f(x, p)=f(x)\" eeimg=\"1\"/> ，也就是说我们不对参数进行额外的惩罚。我们仍然要使用拉格朗日乘子法：定义 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D+%3D+f%28x%29+%2B+%5Clambda%5E%7B%5Ctop%7Dg%28x%2C+p%29%2C\" alt=\"\\mathcal{L} = f(x) + \\lambda^{\\top}g(x, p),\" eeimg=\"1\"/> 又由于函数 <img src=\"https://www.zhihu.com/equation?tex=g%28x%2C+p%29%3D0\" alt=\"g(x, p)=0\" eeimg=\"1\"/> ，因此 <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7Bdg%7D%7Bdp%7D+%3D+0.\" alt=\"\\frac{dg}{dp} = 0.\" eeimg=\"1\"/> 那么我们就得到（下角标是求偏导的意思）：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7Bdf%7D%7Bdp%7D+%3D+%5Cfrac%7Bd%5Cmathcal%7BL%7D%7D%7Bdp%7D+%3D+%28f_%7Bx%7D+%2B+%5Clambda%5E%7B%5Ctop%7D+g_%7Bx%7D%29x_%7Bp%7D+%2B+%5Clambda%5E%7B%5Ctop%7D+g_%7Bp%7D.\" alt=\"\\frac{df}{dp} = \\frac{d\\mathcal{L}}{dp} = (f_{x} + \\lambda^{\\top} g_{x})x_{p} + \\lambda^{\\top} g_{p}.\" eeimg=\"1\"/> </p><p>然后我们不是很喜欢这个 <img src=\"https://www.zhihu.com/equation?tex=x_%7Bp%7D\" alt=\"x_{p}\" eeimg=\"1\"/> ，所以我们将乘子取为满足：</p><p><img src=\"https://www.zhihu.com/equation?tex=f_%7Bx%7D+%2B+%5Clambda%5E%7B%5Ctop%7D+g_%7Bx%7D+%3D+0.\" alt=\"f_{x} + \\lambda^{\\top} g_{x} = 0.\" eeimg=\"1\"/> </p><p>那么 <img src=\"https://www.zhihu.com/equation?tex=d_%7Bp%7Df+%3D+%5Clambda%5E%7B%5Ctop%7D+g_%7Bp%7D%2C\" alt=\"d_{p}f = \\lambda^{\\top} g_{p},\" eeimg=\"1\"/> 梯度求完了。下面看稍普遍一点的情形。</p><p>（2） <img src=\"https://www.zhihu.com/equation?tex=f%28x%2C+p%29\" alt=\"f(x, p)\" eeimg=\"1\"/> 与 <img src=\"https://www.zhihu.com/equation?tex=p+\" alt=\"p \" eeimg=\"1\"/> 有显式的关系，我们发现最终的结果其实也很简单：</p><p><img src=\"https://www.zhihu.com/equation?tex=d_%7Bp%7Df+%3D+%5Clambda%5E%7B%5Ctop%7D+g_%7Bp%7D+%2B+f_%7Bp%7D%2C\" alt=\"d_{p}f = \\lambda^{\\top} g_{p} + f_{p},\" eeimg=\"1\"/> 相当于考虑对参数的惩罚之后，其实在梯度的表达式中体现为直接加了一项。</p><p>下面考虑一类更加复杂但更加有用的优化问题：</p><p>记时间为 <img src=\"https://www.zhihu.com/equation?tex=t\" alt=\"t\" eeimg=\"1\"/> ，我们考虑在时间段 <img src=\"https://www.zhihu.com/equation?tex=0+%5Cleqslant+t+%5Cleqslant+T\" alt=\"0 \\leqslant t \\leqslant T\" eeimg=\"1\"/> 内的优化。</p><p>系统： <img src=\"https://www.zhihu.com/equation?tex=g%28x%280%29%2C+p%29+%3D+0%2C+~~h%28x%2C+%5Cdot%7Bx%7D%2C+p%2C+t%29%3D0.\" alt=\"g(x(0), p) = 0, ~~h(x, \\dot{x}, p, t)=0.\" eeimg=\"1\"/> 也就是说确定初始状态 <img src=\"https://www.zhihu.com/equation?tex=x%280%29\" alt=\"x(0)\" eeimg=\"1\"/> ，后续 <img src=\"https://www.zhihu.com/equation?tex=x+\" alt=\"x \" eeimg=\"1\"/> 的演化遵循一个ODE。</p><p>Loss function： <img src=\"https://www.zhihu.com/equation?tex=F%28x%2C+p%29+%3D+%5Cmin_%7Bp%7D+%5Cint_%7B0%7D%5E%7BT%7Df%28x%2C+p%2C+t%29+dt.\" alt=\"F(x, p) = \\min_{p} \\int_{0}^{T}f(x, p, t) dt.\" eeimg=\"1\"/> 这是一个对时间的积分。</p><p>类似地，我们定义拉格朗日函数：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D+%3D+%5Cint_%7B0%7D%5E%7BT%7D+%5B+f%28x%2C+p%2C+t%29+%2B+%5Clambda%5E%7BT%7Dh%28x%2C+%5Cdot%7Bx%7D%2C+p%2C+t%29%5D++%2B+%5Cmu%5E%7B%5Ctop%7Dg%28x%280%29%2C+p%29%2C+\" alt=\"\\mathcal{L} = \\int_{0}^{T} [ f(x, p, t) + \\lambda^{T}h(x, \\dot{x}, p, t)]  + \\mu^{\\top}g(x(0), p), \" eeimg=\"1\"/> </p><p>求导得到：<img src=\"https://www.zhihu.com/equation?tex=d_%7Bp%7D%5Cmathcal%7BL%7D+%3D+%5Cint_%7B0%7D%5E%7BT%7D%5B+%5Cpartial_%7Bx%7Df~d_%7Bp%7Dx+%2B+%5Cpartial_%7Bp%7Df+%2B+%5Clambda%5E%7B%5Ctop%7D%28%5Cpartial_%7Bx%7Dh~d_%7Bp%7Dx+%2B+%5Cpartial_%7B%5Cdot%7Bx%7D%7Dh~d_%7Bp%7D%5Cdot%7Bx%7D+%2B+%5Cpartial_%7Bp%7Dh%29++%5Ddt+%2B+%5Cmu%5E%7B%5Ctop%7D%28+%5Cpartial_%7Bx%280%29%7Dg~d_%7Bp%7Dx%280%29%2B+%5Cpartial_%7Bp%7Dg%29\" alt=\"d_{p}\\mathcal{L} = \\int_{0}^{T}[ \\partial_{x}f~d_{p}x + \\partial_{p}f + \\lambda^{\\top}(\\partial_{x}h~d_{p}x + \\partial_{\\dot{x}}h~d_{p}\\dot{x} + \\partial_{p}h)  ]dt + \\mu^{\\top}( \\partial_{x(0)}g~d_{p}x(0)+ \\partial_{p}g)\" eeimg=\"1\"/> </p><p>但是我们不想看到 <img src=\"https://www.zhihu.com/equation?tex=d_%7Bp%7Dx\" alt=\"d_{p}x\" eeimg=\"1\"/> 或者 <img src=\"https://www.zhihu.com/equation?tex=d_%7Bp%7D%5Cdot%7Bx%7D\" alt=\"d_{p}\\dot{x}\" eeimg=\"1\"/> ，所以要化简一下。</p><p>考虑到：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Clambda%5E%7B%5Ctop%7D+%5Cpartial_%7B%5Cdot%7Bx%7D%7Dh~d_%7Bp%7D%5Cdot%7Bx%7D+%3D+%5Cfrac%7Bd%7D%7Bdt%7D%28%5Clambda%5E%7B%5Ctop%7D%5Cpartial_%7B%5Cdot%7Bx%7D%7Dh~d_%7Bp%7Dx%29+-+%5Cdot%7B%5Clambda%7D%5E%7B%5Ctop%7D%5Cpartial_%7B%5Cdot%7Bx%7D%7Dh~d_%7Bp%7Dx+-+%5Clambda%5E%7B%5Ctop%7D+d_%7Bt%7D%28%5Cpartial_%7B%5Cdot%7Bx%7D%7Dh%29d_%7Bp%7Dx%2C\" alt=\"\\lambda^{\\top} \\partial_{\\dot{x}}h~d_{p}\\dot{x} = \\frac{d}{dt}(\\lambda^{\\top}\\partial_{\\dot{x}}h~d_{p}x) - \\dot{\\lambda}^{\\top}\\partial_{\\dot{x}}h~d_{p}x - \\lambda^{\\top} d_{t}(\\partial_{\\dot{x}}h)d_{p}x,\" eeimg=\"1\"/> </p><p>我们有：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cint_%7B0%7D%5E%7BT%7D%5Clambda%5E%7B%5Ctop%7D%5Cpartial_%7B%5Cdot%7Bx%7D%7Dh~d_%7Bp%7D%5Cdot%7Bx%7Ddt+%3D+%5Clambda%5E%7B%5Ctop%7D%5Cpartial_%7B%5Cdot%7Bx%7D%7Dh~d_%7Bp%7Dx+%7C_%7B0%7D%5E%7BT%7D+-+%5Cint_%7B0%7D%5E%7BT%7D+%5B++%5Cdot%7B%5Clambda%7D%5E%7B%5Ctop%7D%5Cpartial_%7B%5Cdot%7Bx%7D%7Dh~d_%7Bp%7Dx+%2B+%5Clambda%5E%7B%5Ctop%7D+d_%7Bt%7D%28%5Cpartial_%7B%5Cdot%7Bx%7D%7Dh%29d_%7Bp%7Dx++%5D+dt%2C\" alt=\"\\int_{0}^{T}\\lambda^{\\top}\\partial_{\\dot{x}}h~d_{p}\\dot{x}dt = \\lambda^{\\top}\\partial_{\\dot{x}}h~d_{p}x |_{0}^{T} - \\int_{0}^{T} [  \\dot{\\lambda}^{\\top}\\partial_{\\dot{x}}h~d_{p}x + \\lambda^{\\top} d_{t}(\\partial_{\\dot{x}}h)d_{p}x  ] dt,\" eeimg=\"1\"/> </p><p>所以继续化简 <img src=\"https://www.zhihu.com/equation?tex=d_%7Bp%7D%5Cmathcal%7BL%7D\" alt=\"d_{p}\\mathcal{L}\" eeimg=\"1\"/> ： <img src=\"https://www.zhihu.com/equation?tex=d_%7Bp%7D%5Cmathcal%7BL%7D+%3D+%5Cint_%7B0%7D%5E%7BT%7D%5B+%5Cpartial_%7Bx%7Df+%2B+%5Clambda%5E%7B%5Ctop%7D%5Cpartial_%7Bx%7Dh+-+%5Cdot%7B%5Clambda%7D%5E%7B%5Ctop%7D%5Cpartial_%7B%5Cdot%7Bx%7D%7Dh+-+%5Clambda%5E%7B%5Ctop%7Dd_%7Bt%7D%28%5Cpartial_%7B%5Cdot%7Bx%7D%7Dh%29+%5Dd_%7Bp%7Dx~dt+%2B+%5Cint_%7B0%7D%5E%7BT%7D+%28%5Cpartial_%7Bp%7Df+%2B+%5Clambda%5E%7B%5Ctop%7D%5Cpartial_%7Bp%7Dh%29+dt+%2B+%5Clambda%5E%7B%5Ctop%7D%5Cpartial_%7B%5Cdot%7Bx%7D%7Dh~d_%7Bp%7Dx+%7C_%7B0%7D%5E%7BT%7D+%2B+%5Cmu%5E%7B%5Ctop%7D%28%5Cpartial_%7Bx%280%29%7Dg~d_%7Bp%7Dx%280%29%2B+%5Cpartial_%7Bp%7Dg%29%2C\" alt=\"d_{p}\\mathcal{L} = \\int_{0}^{T}[ \\partial_{x}f + \\lambda^{\\top}\\partial_{x}h - \\dot{\\lambda}^{\\top}\\partial_{\\dot{x}}h - \\lambda^{\\top}d_{t}(\\partial_{\\dot{x}}h) ]d_{p}x~dt + \\int_{0}^{T} (\\partial_{p}f + \\lambda^{\\top}\\partial_{p}h) dt + \\lambda^{\\top}\\partial_{\\dot{x}}h~d_{p}x |_{0}^{T} + \\mu^{\\top}(\\partial_{x(0)}g~d_{p}x(0)+ \\partial_{p}g),\" eeimg=\"1\"/> </p><p>最终得到：</p><p><img src=\"https://www.zhihu.com/equation?tex=d_%7Bp%7D%5Cmathcal%7BL%7D+%3D+%5Cint_%7B0%7D%5E%7BT%7D+%5B%5Cpartial_%7Bx%7Df+%2B+%5Clambda%5E%7B%5Ctop%7D%5Cpartial_%7Bx%7Dh+-+%5Cdot%7B%5Clambda%7D%5E%7B%5Ctop%7D%5Cpartial_%7B%5Cdot%7Bx%7D%7Dh+-+%5Clambda%5E%7B%5Ctop%7Dd_%7Bt%7D%28%5Cpartial_%7B%5Cdot%7Bx%7D%7Dh%29%5D+d_%7Bp%7Dx~dt+%2B+%5Cint_%7B0%7D%5E%7BT%7D+%28%5Cpartial_%7Bp%7Df+%2B+%5Clambda%5E%7B%5Ctop%7D%5Cpartial_%7Bp%7Dh%29+dt+%2B+%5Clambda%5E%7B%5Ctop%7D%5Cpartial_%7B%5Cdot%7Bx%7D%7Dh~d_%7Bp%7Dx+%7C_%7BT%7D+%2B+%5B%5Cmu%5E%7B%5Ctop%7D+%5Cpartial_%7Bx%280%29%7Dg+-+%5Clambda%5E%7B%5Ctop%7D%5Cpartial_%7B%5Cdot%7Bx%7D%7Dh~d_%7Bp%7Dx++%5D%7C_%7B0%7Dd_%7Bp%7Dx%280%29+%2B+%5Cmu%5E%7B%5Ctop%7D+%5Cpartial_%7Bp%7Dg.\" alt=\"d_{p}\\mathcal{L} = \\int_{0}^{T} [\\partial_{x}f + \\lambda^{\\top}\\partial_{x}h - \\dot{\\lambda}^{\\top}\\partial_{\\dot{x}}h - \\lambda^{\\top}d_{t}(\\partial_{\\dot{x}}h)] d_{p}x~dt + \\int_{0}^{T} (\\partial_{p}f + \\lambda^{\\top}\\partial_{p}h) dt + \\lambda^{\\top}\\partial_{\\dot{x}}h~d_{p}x |_{T} + [\\mu^{\\top} \\partial_{x(0)}g - \\lambda^{\\top}\\partial_{\\dot{x}}h~d_{p}x  ]|_{0}d_{p}x(0) + \\mu^{\\top} \\partial_{p}g.\" eeimg=\"1\"/> </p><p>我们可以取乘子 <img src=\"https://www.zhihu.com/equation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5Cmu+\" alt=\"\\mu \" eeimg=\"1\"/> 使得两个中括号中的项都为零。</p><p>所以完整的优化过程中，单次梯度下降只需要三个步骤：</p><p>（1）对于当前的 <img src=\"https://www.zhihu.com/equation?tex=p\" alt=\"p\" eeimg=\"1\"/> ，根据 <img src=\"https://www.zhihu.com/equation?tex=g%28x%280%29%2C+p%29%3D0\" alt=\"g(x(0), p)=0\" eeimg=\"1\"/> 计算出 <img src=\"https://www.zhihu.com/equation?tex=x%280%29\" alt=\"x(0)\" eeimg=\"1\"/> ，再通过 <img src=\"https://www.zhihu.com/equation?tex=h%28x%2C+%5Cdot%7Bx%7D%2C+p%2C+t%29+%3D+0\" alt=\"h(x, \\dot{x}, p, t) = 0\" eeimg=\"1\"/> 解所有 <img src=\"https://www.zhihu.com/equation?tex=x%28t%29\" alt=\"x(t)\" eeimg=\"1\"/> 。</p><p>（2）根据两个中括号为零的条件，写出乘子所满足的ODE，解之得到 <img src=\"https://www.zhihu.com/equation?tex=%5Clambda%28t%29\" alt=\"\\lambda(t)\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5Cmu%28t%29\" alt=\"\\mu(t)\" eeimg=\"1\"/> 。注意这里解ODE在时间上是反向求解。</p><p>（3）计算梯度 <img src=\"https://www.zhihu.com/equation?tex=d_%7Bp%7DF+%3D+%5Cint_%7B0%7D%5E%7BT%7D%28%5Cpartial_%7Bp%7Df+%2B+%5Clambda%5E%7B%5Ctop%7D%5Cpartial_%7Bp%7Dh%29dt+%2B+%5Cmu%5E%7B%5Ctop%7D+%5Cpartial_%7Bp%7Dg%2C\" alt=\"d_{p}F = \\int_{0}^{T}(\\partial_{p}f + \\lambda^{\\top}\\partial_{p}h)dt + \\mu^{\\top} \\partial_{p}g,\" eeimg=\"1\"/> 其中乘子已经在第二步中算出来了，然后就可以梯度下降了。</p><p>这样，对于每一步梯度下降，我们只需要做少数几次simulation，外加解几个ODE，就足够了。虽说计算量还是挺大，但已经在可接受的范围内了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>adjoint method解带约束的优化问题，其应用主要有两方面：</p><p>（1）我们拿到一个参数未知的系统，可以通过收集到的输入输出数据，对系统的参数进行估计。loss体现的是系统输出与测量到的实际输出的差异。</p><p>（2）我们希望设计一个系统，达到某种特定的功能，这个时候与目标功能是否契合就体现在loss里，然后我们优化系统的参数，完成inverse design。以我粗浅的了解，造桥、造飞机、造光学器件等领域都有在用这种方法。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>后记：之所以想起来写adjoint method是因为，从暑研的时候他们设计器件就一直用这种方式，但当时数学细节没有时间抠得很仔细。最近看了相关方向的文章，顺手整理一下，内容基本上是照着<a href=\"https://link.zhihu.com/?target=https%3A//cs.stanford.edu/~ambrad/adjoint_tutorial.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">cs.stanford.edu/~ambrad</span><span class=\"invisible\">/adjoint_tutorial.pdf</span><span class=\"ellipsis\"></span></a>抄的，其实大家可以直接看那个tutorial。另一方面，等我有时间了可能会把跟光学neural network相关的几个工作整理在专栏里（目前了解的包括MIT Marin组、Stanford Shanhui组、Wisconsin Yu组）。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>其实time-dependent的adjoint method也可以用来解最优控制问题（下一篇文章应该会谈一谈控制）。如果控制问题本身的状态转移比较复杂，无法写出闭式解（一个很经典的能写出解的例子是，转移是线性的，loss是quadratic的），那么在考虑约束的前提下进行梯度下降应当是一个不错的选择。另一方面，最优控制中我们可以考虑系统转移时有随机性，那么adjoint method显然也可以把类似的随机性考虑在内，不知道有什么实际应用没。</p>", 
            "topic": [
                {
                    "tag": "拓扑优化（Topology Optimization）", 
                    "tagLink": "https://api.zhihu.com/topics/20069613"
                }, 
                {
                    "tag": "光学", 
                    "tagLink": "https://api.zhihu.com/topics/19561671"
                }
            ], 
            "comments": [
                {
                    "userName": "周清逸", 
                    "userLink": "https://www.zhihu.com/people/23dd9239a500559d8850eac45ed2038f", 
                    "content": "最近在玩脑叶公司，图片是兔子队长Myo，我觉得她非常可爱。", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "mmmmayi", 
                    "userLink": "https://www.zhihu.com/people/0f45ec62344f461b4e94205606e23ec2", 
                    "content": "<p>你好，我想问一下公式中参数上一点是什么意思？</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "周清逸", 
                            "userLink": "https://www.zhihu.com/people/23dd9239a500559d8850eac45ed2038f", 
                            "content": "啊就是对时间的导数…如果我写的不太清晰，你可以参考底下那个链接，它讲的比较好(｡･ω･｡)", 
                            "likes": 0, 
                            "replyToAuthor": "mmmmayi"
                        }, 
                        {
                            "userName": "mmmmayi", 
                            "userLink": "https://www.zhihu.com/people/0f45ec62344f461b4e94205606e23ec2", 
                            "content": "<p>好的，谢谢了</p>", 
                            "likes": 0, 
                            "replyToAuthor": "周清逸"
                        }
                    ]
                }, 
                {
                    "userName": "知乎用户", 
                    "userLink": "https://www.zhihu.com/people/0", 
                    "content": "<p>想问一下大神，拉格朗日乘子是时间的函数，这个是在ＰDE-constrained optimization常见的思路吗？该如何理解呢？</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "nanopho", 
                    "userLink": "https://www.zhihu.com/people/54a4ac9634ba4df4c51dd2f95cd53c95", 
                    "content": "期待光学神经网络中的相关内容解读", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/48239486", 
            "userName": "周清逸", 
            "userLink": "https://www.zhihu.com/people/23dd9239a500559d8850eac45ed2038f", 
            "upvote": 10, 
            "title": "W距离", 
            "content": "<p>前两周李老师上课讲了讲WGAN和W距离，用了两种证明方法，后一种比较优美；这周又捋了一遍，逻辑更清晰。这里做一下笔记，数学上很不严谨，感受一下就好。</p><p>所谓Generative Adversarial Network (GAN)，就是我们要找一个分布 <img src=\"https://www.zhihu.com/equation?tex=y%5Csim+P_%7Bg%7D%28y%3B%5Ctheta%29\" alt=\"y\\sim P_{g}(y;\\theta)\" eeimg=\"1\"/> ，去逼近真实分布 <img src=\"https://www.zhihu.com/equation?tex=x%5Csim+P_%7Br%7D%28x%29\" alt=\"x\\sim P_{r}(x)\" eeimg=\"1\"/> 。那么我们就需要一个函数来衡量两个概率分布之间的距离。一个简单的选择是K-L散度（或者变一下就是J-S散度），但是如果我的两个分布重叠的部分比较少（训练开始的时候一般是这样的），或者用数学上的语言就是支集，计算K-L散度会发散，这是我们不希望看到的。</p><p>所谓WGAN就是说，我们可以用另外的距离衡量方式来定义loss，也就是所谓的“推土机距离”（W距离）。Earth-Mover距离的定义为：</p><p><img src=\"https://www.zhihu.com/equation?tex=W+%3D+%5Cmin_%7B%5Cgamma%7D+%5Cmathbb%7BE%7D_%7B%28x%2Cy%29%5Csim+%5Cgamma%7D+%5C%7C+x-y+%5C%7C.\" alt=\"W = \\min_{\\gamma} \\mathbb{E}_{(x,y)\\sim \\gamma} \\| x-y \\|.\" eeimg=\"1\"/> </p><p>其中 <img src=\"https://www.zhihu.com/equation?tex=%5Cgamma\" alt=\"\\gamma\" eeimg=\"1\"/> 是二元的概率分布，满足约束条件（边缘概率）</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cint+%5Cgamma%28x%2Cy%29dx+%3D+P_%7Bg%7D%28y%3B%5Ctheta%29%2C\" alt=\"\\int \\gamma(x,y)dx = P_{g}(y;\\theta),\" eeimg=\"1\"/> <img src=\"https://www.zhihu.com/equation?tex=%5Cint+%5Cgamma%28x%2Cy%29dy+%3D+P_%7Br%7D%28x%29.\" alt=\"\\int \\gamma(x,y)dy = P_{r}(x).\" eeimg=\"1\"/> </p><p>这个推土机距离可以理解为把几堆土填到几个坑里，是很重要的概念，有广泛应用。</p><p>下面首先证明一个定理：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cmin_%7B%5Cgamma%7D+%5Cmathbb%7BE%7D_%7B%28x%2Cy%29%5Csim+%5Cgamma%7D+%5C%7C+x-y+%5C%7C+%3D+%5Cmax_%7B%5C%7C+f%5C%7C%5Cleqslant+1%7D+%5B%5Cint+P_%7Br%7D%28x%29f%28x%29dx-+%5Cint+P_%7Bg%7D%28y%3B%5Ctheta%29f%28y%29dy%5D.\" alt=\"\\min_{\\gamma} \\mathbb{E}_{(x,y)\\sim \\gamma} \\| x-y \\| = \\max_{\\| f\\|\\leqslant 1} [\\int P_{r}(x)f(x)dx- \\int P_{g}(y;\\theta)f(y)dy].\" eeimg=\"1\"/> </p><p>其实就是把W距离换成一个更简单的形式。针对约束条件，我们可以引入两个Lagrange乘子： <img src=\"https://www.zhihu.com/equation?tex=%5Cphi%28x%29\" alt=\"\\phi(x)\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5Cpsi%28y%29\" alt=\"\\psi(y)\" eeimg=\"1\"/> ，那么Lagrange函数写作：</p><p><img src=\"https://www.zhihu.com/equation?tex=L+%3D+%5Cint%5Cint+%5C%7C+x-y%5C%7C+%5Cgamma%28x%2Cy%29dxdy+-+%5Cint%5B%5Cint+%5Cgamma%28x%2Cy%29dy+-+P_%7Br%7D%28x%29%5D%5Cphi%28x%29dx+-+%5Cint%5B%5Cint+%5Cgamma%28x%2Cy%29dx+-+P_%7Bg%7D%28y%3B%5Ctheta%29%5D%5Cpsi%28y%29dy.\" alt=\"L = \\int\\int \\| x-y\\| \\gamma(x,y)dxdy - \\int[\\int \\gamma(x,y)dy - P_{r}(x)]\\phi(x)dx - \\int[\\int \\gamma(x,y)dx - P_{g}(y;\\theta)]\\psi(y)dy.\" eeimg=\"1\"/> </p><p>注：如果有童鞋不太理解函数乘子，可以先考虑离散、有限维的情形，也就是把 <img src=\"https://www.zhihu.com/equation?tex=%5Cgamma%28x%2Cy%29\" alt=\"\\gamma(x,y)\" eeimg=\"1\"/> 看成一个矩阵，那么边缘概率（一维向量）的约束条件就变成线性方程组，在这种情况下每一个等式要引入一个Lagrange乘子，对应一个“乘子向量”，这样想就可以拓展到无穷维了（这么说当然是不严谨的...只是辅助思考而已）。</p><p>对于 <img src=\"https://www.zhihu.com/equation?tex=%5Cgamma\" alt=\"\\gamma\" eeimg=\"1\"/> 做变分，然后把 <img src=\"https://www.zhihu.com/equation?tex=%5Cphi%28x%29\" alt=\"\\phi(x)\" eeimg=\"1\"/> 换成 <img src=\"https://www.zhihu.com/equation?tex=f%28x%29\" alt=\"f(x)\" eeimg=\"1\"/> 、 <img src=\"https://www.zhihu.com/equation?tex=%5Cpsi%28x%29\" alt=\"\\psi(x)\" eeimg=\"1\"/> 换成 <img src=\"https://www.zhihu.com/equation?tex=-f%28x%29\" alt=\"-f(x)\" eeimg=\"1\"/> ，就可以看到Lipschitz-1条件的雏形了。具体来说，我们的问题化为：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cmax_%7B%5Cphi%2C%5Cpsi%7D+%5Cmin_%7B%5Cgamma%7D+L%28%5Cgamma%2C+%5Cphi%2C+%5Cpsi%29.\" alt=\"\\max_{\\phi,\\psi} \\min_{\\gamma} L(\\gamma, \\phi, \\psi).\" eeimg=\"1\"/> </p><p>如果 <img src=\"https://www.zhihu.com/equation?tex=%5C%7C+x-y%5C%7C+-+%5Cphi%28x%29+-+%5Cpsi%28y%29+%3C+0\" alt=\"\\| x-y\\| - \\phi(x) - \\psi(y) &lt; 0\" eeimg=\"1\"/> ，那么对 <img src=\"https://www.zhihu.com/equation?tex=%5Cgamma\" alt=\"\\gamma\" eeimg=\"1\"/> 取最小应该是负无穷，不好，所以要求 <img src=\"https://www.zhihu.com/equation?tex=%5C%7C+x-y%5C%7C+%5Cgeqslant+%5Cphi%28x%29+%2B+%5Cpsi%28y%29\" alt=\"\\| x-y\\| \\geqslant \\phi(x) + \\psi(y)\" eeimg=\"1\"/> 成立。</p><p>那么为什么要把两个函数统一换成 <img src=\"https://www.zhihu.com/equation?tex=f%28x%29\" alt=\"f(x)\" eeimg=\"1\"/> 呢？这里其实可以“马后炮”地从GAN的角度去考虑。我们可以认为 <img src=\"https://www.zhihu.com/equation?tex=P_%7Br%7D%28x%29%5Cphi%28x%29dx\" alt=\"P_{r}(x)\\phi(x)dx\" eeimg=\"1\"/> 衡量“真的样本有多真”，而 <img src=\"https://www.zhihu.com/equation?tex=P_%7Bg%7D%28y%3B%5Ctheta%29%5Cpsi%28y%29dy\" alt=\"P_{g}(y;\\theta)\\psi(y)dy\" eeimg=\"1\"/> 衡量假的样本有多假，然而如果我只判断样本的真假，这实际上是个二分类问题，用一个分类器 <img src=\"https://www.zhihu.com/equation?tex=f\" alt=\"f\" eeimg=\"1\"/> 就可以完成了。</p><p>这样我们就可以看到 <img src=\"https://www.zhihu.com/equation?tex=f%28x%29-f%28y%29%5Cleqslant+%5C%7C+x-y%5C%7C\" alt=\"f(x)-f(y)\\leqslant \\| x-y\\|\" eeimg=\"1\"/> 这个条件的出处了。</p><p>在文献&#34;Improved Training of Wasserstein GANs&#34;中，证明了一定有一个取等号的解（真Lipschitz-1）。这样我们可以把跟 <img src=\"https://www.zhihu.com/equation?tex=%5Cgamma\" alt=\"\\gamma\" eeimg=\"1\"/> 有关的部分都拿掉，只剩下：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cmax_%7Bf%28x%29-f%28y%29%3D%5C%7C+x-y+%5C%7C%7D+%5B%5Cint+P_%7Br%7D%28x%29f%28x%29dx+-+%5Cint+P_%7Bg%7D%28y%3B%5Ctheta%29f%28y%29dy%5D.\" alt=\"\\max_{f(x)-f(y)=\\| x-y \\|} [\\int P_{r}(x)f(x)dx - \\int P_{g}(y;\\theta)f(y)dy].\" eeimg=\"1\"/> <br/> 这就是我们想要的形式了！<br/> </p><p>接下来是训练过程。为了方便，我们把 <img src=\"https://www.zhihu.com/equation?tex=f\" alt=\"f\" eeimg=\"1\"/> 记为 <img src=\"https://www.zhihu.com/equation?tex=f_%7Bw%7D\" alt=\"f_{w}\" eeimg=\"1\"/> ，其中的 <img src=\"https://www.zhihu.com/equation?tex=w\" alt=\"w\" eeimg=\"1\"/> 代表参数。为了使得生成的概率分布尽可能贴近真实分布，我们还需要优化 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> ，问题转化为</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cmin_%7B%5Ctheta%7D+%5Cmax_%7B%5C%7C+f_%7Bw%7D%5C%7C%5Cleqslant+1%7D+%5B%5Cmathbb%7BE%7D_%7BP_%7Br%7D%7D+f_%7Bw%7D%28x%29+-+%5Cmathbb%7BE%7D_%7BP_%7Bg%7D%7D+f_%7Bw%7D%28x%29%5D.\" alt=\"\\min_{\\theta} \\max_{\\| f_{w}\\|\\leqslant 1} [\\mathbb{E}_{P_{r}} f_{w}(x) - \\mathbb{E}_{P_{g}} f_{w}(x)].\" eeimg=\"1\"/> <br/> 具体方法是交替迭代：</p><p>（1）固定 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> ，更新 <img src=\"https://www.zhihu.com/equation?tex=f_%7Bw%7D\" alt=\"f_{w}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cmax_%7B%5C%7C+f_%7Bw%7D%5C%7C%5Cleqslant+1%7D+%5B%5Cmathbb%7BE%7D_%7BP_%7Br%7D%7D+f_%7Bw%7D%28x%29+-+%5Cmathbb%7BE%7D_%7BP_%7Bg%7D%7D+f_%7Bw%7D%28x%29%5D.\" alt=\"\\max_{\\| f_{w}\\|\\leqslant 1} [\\mathbb{E}_{P_{r}} f_{w}(x) - \\mathbb{E}_{P_{g}} f_{w}(x)].\" eeimg=\"1\"/> 第一项要大（真实样本打分高），第二项要小（虚假样本打分低）。很明显 <img src=\"https://www.zhihu.com/equation?tex=f_%7Bw%7D\" alt=\"f_{w}\" eeimg=\"1\"/> 可以看作一个discriminator。</p><p>（2）固定 <img src=\"https://www.zhihu.com/equation?tex=f_%7Bw%7D\" alt=\"f_{w}\" eeimg=\"1\"/> ，更新 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cmax_%7B%5Ctheta%7D+%5Cmathbb%7BE%7D_%7BP_%7Bg%7D%28x%3B%5Ctheta%29%7D+f_%7Bw%7D%28x%29.\" alt=\"\\max_{\\theta} \\mathbb{E}_{P_{g}(x;\\theta)} f_{w}(x).\" eeimg=\"1\"/> 希望最大化虚假样本的打分，也就是说generator的工作是要“欺骗”discriminator。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>实际优化的时候，Lipschitz-1条件是如何体现的呢？很简单，在loss里直接加一项：<br/> <img src=\"https://www.zhihu.com/equation?tex=%5Clambda+%28%5C%7C+%5Cnabla_%7B%5Chat%7Bx%7D%7D+f_%7Bw%7D%28%5Chat%7Bx%7D%29+%5C%7C_%7B2%7D-1%29%5E%7B2%7D.\" alt=\"\\lambda (\\| \\nabla_{\\hat{x}} f_{w}(\\hat{x}) \\|_{2}-1)^{2}.\" eeimg=\"1\"/> </p><p>其中 <img src=\"https://www.zhihu.com/equation?tex=%5Chat%7Bx%7D\" alt=\"\\hat{x}\" eeimg=\"1\"/> 是某个真实样本与某个生成样本作简单线性混合得到的。这个地方其实感觉有一点任意性。</p><p>这样训练出来比最开始强行控制weights的方法效果要好很多。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>与普通GAN的关系：</p><p>普通GAN在更新discriminator的时候是</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cmax_%7Bf_%7Bw%7D%7D+%5B%5Cmathbb%7BE%7D_%7BP_%7Br%7D%7D+%5Clog+f_%7Bw%7D%28x%29+%2B+%5Cmathbb%7BE%7D_%7BP_%7Bg%7D%7D+%5Clog+%281-f_%7Bw%7D%28x%29%29%5D.\" alt=\"\\max_{f_{w}} [\\mathbb{E}_{P_{r}} \\log f_{w}(x) + \\mathbb{E}_{P_{g}} \\log (1-f_{w}(x))].\" eeimg=\"1\"/> </p><p>相比之下WGAN是</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cmax_%7Bf_%7Bw%7D%7D+%5B%5Cmathbb%7BE%7D_%7BP_%7Br%7D%7D+f_%7Bw%7D%28x%29+%2B+%5Cmathbb%7BE%7D_%7BP_%7Bg%7D%7D+%281-f_%7Bw%7D%28x%29%29%5D.\" alt=\"\\max_{f_{w}} [\\mathbb{E}_{P_{r}} f_{w}(x) + \\mathbb{E}_{P_{g}} (1-f_{w}(x))].\" eeimg=\"1\"/> </p><p class=\"ztext-empty-paragraph\"><br/></p><p>思考：为什么不用这种形式的距离</p><p><img src=\"https://www.zhihu.com/equation?tex=W%3D%5Cmathbb%7BE%7D_%7Bx%5Csim+P_%7Br%7D%2Cy%5Csim+P_%7Bg%7D%7D+%5C%7C+x-y+%5C%7C.\" alt=\"W=\\mathbb{E}_{x\\sim P_{r},y\\sim P_{g}} \\| x-y \\|.\" eeimg=\"1\"/> </p><p>原因可能是多方面的：这个距离的定义好么（举反例）？训练的复杂性呢？</p><p class=\"ztext-empty-paragraph\"><br/></p><p>参考文献：</p><p>[1] Wasserstein GAN: <a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1701.07875\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">[1701.07875] Wasserstein GAN</a></p><p>[2] Improved Training of Wasserstein GANs: <a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1704.00028.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/pdf/1704.0002</span><span class=\"invisible\">8.pdf</span><span class=\"ellipsis\"></span></a></p><p>[3] <a href=\"https://link.zhihu.com/?target=https%3A//www.math.ucdavis.edu/~qlxia/Research/monge.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">math.ucdavis.edu/~qlxia</span><span class=\"invisible\">/Research/monge.pdf</span><span class=\"ellipsis\"></span></a></p><p>[4] <a href=\"https://link.zhihu.com/?target=https%3A//vincentherrmann.github.io/blog/wasserstein/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Wasserstein GAN and the Kantorovich-Rubinstein Duality</a> 这个博客写的很好，从有限维度切入的</p><p>感想：</p><p>李老师很厉害，至少他讲的这些东西他自己都很清楚细节。他上课的时候让同学上黑板去推导，“推不出来的地方就说明你还没有理解”...我觉得现在（至少我们系）大家对数学的重视程度确实不太够，很多重要的内容都没有学过（也没有老师开课来讲，就很虚弱）。如果要做工程科学的research，还得严格要求自己，再努力学习、多多加强才可以。</p>", 
            "topic": [
                {
                    "tag": "生成对抗网络（GAN）", 
                    "tagLink": "https://api.zhihu.com/topics/20070859"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": [
                {
                    "userName": "周清逸", 
                    "userLink": "https://www.zhihu.com/people/23dd9239a500559d8850eac45ed2038f", 
                    "content": "待会加一点乘子的内容，再加一个参考文献", 
                    "likes": 1, 
                    "childComments": []
                }, 
                {
                    "userName": "周清逸", 
                    "userLink": "https://www.zhihu.com/people/23dd9239a500559d8850eac45ed2038f", 
                    "content": "<p>加好了</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "知乎用户", 
                    "userLink": "https://www.zhihu.com/people/0", 
                    "content": "真的很优美！", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "Willl", 
                    "userLink": "https://www.zhihu.com/people/f2923546b22d16d9446280debe906c14", 
                    "content": "<p>这是什么课啊0 0</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "周清逸", 
                            "userLink": "https://www.zhihu.com/people/23dd9239a500559d8850eac45ed2038f", 
                            "content": "李老师开的电磁大数据-理论与算法…但我觉得改名叫“神经网络讨论班”比较贴切:D", 
                            "likes": 0, 
                            "replyToAuthor": "Willl"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/44955064", 
            "userName": "周清逸", 
            "userLink": "https://www.zhihu.com/people/23dd9239a500559d8850eac45ed2038f", 
            "upvote": 6, 
            "title": "暑期交流", 
            "content": "<p>我于2017年12月份报名参加了Stanford大学主持的UGVR（undergraduate visiting research）项目，通过初选之后，二月份进行了面试。报名的时候，填写的导师是Jonathan Fan教授，方向是电磁器件的优化（比较偏向光学方向），然后面试的时候就是Jon亲自来进行面试。当时我还很惊奇，因为并不是每个教授都会亲自面试候选人，特别是很多老教授会让研究生来做这些事情。后来到组里看了情况之后，我才理解Jon重视UGVR项目的原因，当然这是后话。</p><p>面试时候的情形我记得很清楚，北京时间夜里两点，Stanford那边就是上午十一点。家里网络不好，因此还特意跑到学校。我之前有仔细读过Fanlab有关meta-surface的几篇论文，然后Jon好像对于我懂一点点mechine learning比较感兴趣（他问我为什么要用RNN来做信号处理…），我就跟Jon讨论了一些可能性，包括在level-set设计方法中引入neural net…也就面试了15分钟左右，然后过了几星期就收到邮件，幸运地获得了交流机会。</p><p>然后就是做一些准备，包括跟同一个项目的小伙伴们见了面，还跟嘉麒师兄取得了联系，对于“究竟要做什么”有了一个简单的了解。面签的时候，我们四个小伙伴被集体check...后来其他同学的签证陆续都issue了，只有我一直很绝望地等到了8月份...在签证check期间我也没有闲着，一直在通过各种手段跟大使馆联系，然后回复也很简单：“我们就是要check你，理由不告诉你，等着吧。”按照使馆网站的说法，签证被check在60天以内都是正常的，所以61天的时候我就打电话，理直气壮地说明了情况，最后我在电话里跟使馆工作人员确认情况时，对面突然不说话了...反正就是挺无语的，有时候甚至觉得要是痛快一点给个拒签也就不用天天提心吊胆了...</p><p>8月10号签证issue了，15号拿到，然后就跟教授和罗老师联系。Jon非常善良，跟我说“如果我方便就去一趟”，然后罗老师也非常迅速地帮忙办好了手续。最终我乘坐8月22号的飞机顺利抵达Stanford，开始了不到三周的暑研生活。到达那天下午，去跟教授和组里同学见了一面，简单打了下招呼（当天Jon过生日，但是我没有把礼物带在身上…）。</p><p>Stanford校园里餐厅还挺多的，但是有一些假期里不开，并且食堂非常难吃…所以我住在Jenkins house的时候，常去吃的就是treehouse和panda express。校园里有很多stop牌子，汽车在人行道前面必须要停一下。我感觉自己一直不太习惯车让人，然后过马路的时候总是比较怂（包括后来有了自行车也是这样）。</p><p>工作方面，Jon这边一个很主要的工作内容是用topology optimization来对meta-surface的单元进行设计。理由很简单，如果给定一个meta-surface的构成，那么由入射光场可以计算出出射光场，但是在设计的时候问题没有这么简单，我们希望找到具有一定响应特性的pattern，实质上是一个复杂度很高的组合优化问题。对于这个问题，topology optimization方法的处理思路就是，在一开始的时候先不限定器件的折射率（归一化）必须是binary pattern，而是在[0,1]范围内，然后用梯度下降的方式，去优化这个器件，使得它的响应特性接近我所需要的响应，最后再把这个优化出来的器件强行推到0/1。用这种方法做出来的器件，可以具有很高的大角度偏光效率，相比传统方法具有明显的优势。</p><p>我的任务也很简单，首先是尝试machine learning里面常用的几个优化器，包括Momentum、AdaGrad、RMSprop和Adam，然后发现在这个问题里RMSprop的效果是最好的，稳定好于原始的梯度下降，用比较少的迭代数就可以达到更高的偏光效率；其次，尝试了一下在优化的过程中把器件的meshgrid逐渐增大，这样一开始的分辨率很低，计算起来就省时间，事实证明这个做法也是work的，能省一半时间，效率基本没有区别；最后又针对二维的meta-surface做了一些尝试，主要是在优化的目标函数里面加入regularization，希望在优化过程中能够更有效地避免小的feature的出现（因为这些小feature在加工的时候是做不出来的），但是这些尝试并不能work的很好。</p><p>总体而言，主要工作集中在optimization算法的改进上，跟machine learning没什么关系...嘉麒之前做了一个工作是用GAN来生成pattern，我觉得很有意思。后续可能要建立一个相关的数据集，方便社区的人来从这个比较新颖的角度看待meta-surface。对物理工作者来说，可能不可避免地要转变思维，去把data driven的研究方式跟传统的理论建模-实验验证结合起来，相信会有很多重要的结果涌现。</p><p>走之前跟Jon扯了扯申请的事情，然后他直接就问我是不是要推荐信...要到推荐信之后去旧金山随便转了转。除此之外，三周基本上一直在努力工作，没怎么出去转，因此也没有过多的“生活见闻”。我觉得此行主要的收获还是对于这个领域有了一些基本的了解，对于光器件小型化的未来也有一些想法（虽然跟Jon扯这些的时候，他一直非常冷漠，仿佛我们两个不在一个波段），也算是正经地接触到了applied physics的科研，感觉这段交流经历会对申请有比较大的影响。</p><p>我非常感谢罗老师，她在整个过程中一直在帮助大家解决生活上的大小问题；还要感谢实验室的同学们，特别是嘉麒师兄全程指导我…</p><p>总的来说这次交流活动令我受益良多。但是也看到了很多问题，比如明显感觉到北京大学对于出国交流的支持还是不太够（一万的奖学金都不太愿意利利索索地给，好像还要面试），跟清华差太远。UGVR项目仍然是一个好项目，但貌似北大同学都很少报名参加。其实如果感觉跟项目里的某位教授研究方向比较match，还是可以试着申请一下，毕竟官方项目还是有诸多方便之处的。</p>", 
            "topic": [
                {
                    "tag": "游学", 
                    "tagLink": "https://api.zhihu.com/topics/19651051"
                }, 
                {
                    "tag": "暑期", 
                    "tagLink": "https://api.zhihu.com/topics/19603728"
                }, 
                {
                    "tag": "暑期项目", 
                    "tagLink": "https://api.zhihu.com/topics/19769810"
                }
            ], 
            "comments": [
                {
                    "userName": "周清逸", 
                    "userLink": "https://www.zhihu.com/people/23dd9239a500559d8850eac45ed2038f", 
                    "content": "专栏已经鸽了快一年了，发个文章吧…", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/31274357", 
            "userName": "周清逸", 
            "userLink": "https://www.zhihu.com/people/23dd9239a500559d8850eac45ed2038f", 
            "upvote": 5, 
            "title": "论文笔记：Label-Free Supervision of Neural Networks with Physics and Domain Knowledge", 
            "content": "<p>论文地址：<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1609.05566\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">[1609.05566] Label-Free Supervision of Neural Networks with Physics and Domain Knowledge</a></p><p>这篇论文貌似在AAAI 2017上获了奖...大意就是说，训练神经网络可以不需要带label的训练数据，而是完全借助先验知识。把主要内容稍微梳理了一下...我的LaTex用的很烂...大家不要吐槽...</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-1a9668b34d4b89f5400a691fbbd2f0cb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"978\" data-rawheight=\"1396\" class=\"origin_image zh-lightbox-thumb\" width=\"978\" data-original=\"https://pic4.zhimg.com/v2-1a9668b34d4b89f5400a691fbbd2f0cb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;978&#39; height=&#39;1396&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"978\" data-rawheight=\"1396\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"978\" data-original=\"https://pic4.zhimg.com/v2-1a9668b34d4b89f5400a691fbbd2f0cb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-1a9668b34d4b89f5400a691fbbd2f0cb_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-079a67be36b65946ae06013a8e42400e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"938\" data-rawheight=\"1466\" class=\"origin_image zh-lightbox-thumb\" width=\"938\" data-original=\"https://pic3.zhimg.com/v2-079a67be36b65946ae06013a8e42400e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;938&#39; height=&#39;1466&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"938\" data-rawheight=\"1466\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"938\" data-original=\"https://pic3.zhimg.com/v2-079a67be36b65946ae06013a8e42400e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-079a67be36b65946ae06013a8e42400e_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-81fbda7b58aa40cf20778ef139dda88e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"968\" data-rawheight=\"1496\" class=\"origin_image zh-lightbox-thumb\" width=\"968\" data-original=\"https://pic3.zhimg.com/v2-81fbda7b58aa40cf20778ef139dda88e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;968&#39; height=&#39;1496&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"968\" data-rawheight=\"1496\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"968\" data-original=\"https://pic3.zhimg.com/v2-81fbda7b58aa40cf20778ef139dda88e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-81fbda7b58aa40cf20778ef139dda88e_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-2dd5365519c56fc96b356169f8a36998_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"956\" data-rawheight=\"1464\" class=\"origin_image zh-lightbox-thumb\" width=\"956\" data-original=\"https://pic1.zhimg.com/v2-2dd5365519c56fc96b356169f8a36998_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;956&#39; height=&#39;1464&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"956\" data-rawheight=\"1464\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"956\" data-original=\"https://pic1.zhimg.com/v2-2dd5365519c56fc96b356169f8a36998_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-2dd5365519c56fc96b356169f8a36998_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-c3bf5f229f50a0ebd1b89ec0a37482bc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"1474\" class=\"origin_image zh-lightbox-thumb\" width=\"960\" data-original=\"https://pic1.zhimg.com/v2-c3bf5f229f50a0ebd1b89ec0a37482bc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;960&#39; height=&#39;1474&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"960\" data-rawheight=\"1474\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"960\" data-original=\"https://pic1.zhimg.com/v2-c3bf5f229f50a0ebd1b89ec0a37482bc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-c3bf5f229f50a0ebd1b89ec0a37482bc_b.jpg\"/></figure><p>总结：</p><p>不需要加标签的训练数据，可以极大减少工作量。有很多相关工作（参考Related Works），其中“提高复杂度”、“大方差”之类的思想似乎是作者从其他人那里学来的。</p><p>对于其他问题，“找约束”的过程需要简化（标准化）。</p><p>总而言之，这篇论文的新颖性还是很不错的。造loss的过程有点玄学的味道...不知道能不能很好地应用在更复杂一些的情形中（灵感，灵感）。</p><p></p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": [
                {
                    "userName": "李源", 
                    "userLink": "https://www.zhihu.com/people/52be58d57ab14c9b3eca614cae47892d", 
                    "content": "很新颖啊", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/29922689", 
            "userName": "周清逸", 
            "userLink": "https://www.zhihu.com/people/23dd9239a500559d8850eac45ed2038f", 
            "upvote": 44, 
            "title": "ANN解ODE的尝试...", 
            "content": "<p>好像好久没更新了...</p><p>今天上午刚刚完成了Andrew Ng在coursera上的机器学习课程。课程难度不大，编程使用matlab，适合非计算机专业的同学入门使用。</p><p>下午用matlab写了一个小程序，尝试使用人工神经网络解简单的常微分方程。</p><p>众所周知，人工神经网络是一种强大的机器学习算法。它不仅可以认小猫小狗、识别手写数字，还可以用来解微分方程！那么具体是怎么实现的呢？</p><p>我们考虑形式如下的一个方程：</p><p><img src=\"https://www.zhihu.com/equation?tex=G%28%5Cvec%7Bx%7D%2C%5CPsi%28%5Cvec%7Bx%7D%29%2C%5Cbigtriangledown%5CPsi%28%5Cvec%7Bx%7D%29%2C%5Cbigtriangledown%5E%7B2%7D%5CPsi%28%5Cvec%7Bx%7D%29%29%3D0.\" alt=\"G(\\vec{x},\\Psi(\\vec{x}),\\bigtriangledown\\Psi(\\vec{x}),\\bigtriangledown^{2}\\Psi(\\vec{x}))=0.\" eeimg=\"1\"/> </p><p>其中 <img src=\"https://www.zhihu.com/equation?tex=%5Cvec%7Bx%7D%5Cin+D\" alt=\"\\vec{x}\\in D\" eeimg=\"1\"/> 。 <img src=\"https://www.zhihu.com/equation?tex=%5Cvec%7Bx%7D%3D%28x_%7B1%7D%2C...x_%7Bn%7D%29\" alt=\"\\vec{x}=(x_{1},...x_{n})\" eeimg=\"1\"/> 是一个n维向量。在给定边界条件的情况下，我们希望求出 <img src=\"https://www.zhihu.com/equation?tex=%5CPsi%28%5Cvec%7Bx%7D%29\" alt=\"\\Psi(\\vec{x})\" eeimg=\"1\"/> 的数值解。</p><p>为了便于使用机器学习算法来处理，我们可以将求数值解的问题转化为一个优化问题。取试探解的形式为 <img src=\"https://www.zhihu.com/equation?tex=%5CPsi_%7Bt%7D%28%5Cvec%7Bx%7D%2C%5Cvec%7Bp%7D%29\" alt=\"\\Psi_{t}(\\vec{x},\\vec{p})\" eeimg=\"1\"/> ，其中 <img src=\"https://www.zhihu.com/equation?tex=%5Cvec%7Bp%7D\" alt=\"\\vec{p}\" eeimg=\"1\"/> 是待定的参数，我们要优化如下函数：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cunderset%7B%5Cvec%7Bp%7D%7D%7Bmin%7D%5Csum_%7B%5Cvec%7Bx_%7Bi%7D%7D%7D+%28+G%28%5Cvec%7Bx_%7Bi%7D%7D%2C%5CPsi_%7Bt%7D%28%5Cvec%7Bx_%7Bi%7D%7D%2C%5Cvec%7Bp%7D%29%2C%5Cbigtriangledown%5CPsi_%7Bt%7D%28%5Cvec%7Bx_%7Bi%7D%7D%2C%5Cvec%7Bp%7D%29%2C%5Cbigtriangledown%5E%7B2%7D%5CPsi_%7Bt%7D%28%5Cvec%7Bx_%7Bi%7D%7D%2C%5Cvec%7Bp%7D%29%29+%29%5E%7B2%7D.\" alt=\"\\underset{\\vec{p}}{min}\\sum_{\\vec{x_{i}}} ( G(\\vec{x_{i}},\\Psi_{t}(\\vec{x_{i}},\\vec{p}),\\bigtriangledown\\Psi_{t}(\\vec{x_{i}},\\vec{p}),\\bigtriangledown^{2}\\Psi_{t}(\\vec{x_{i}},\\vec{p})) )^{2}.\" eeimg=\"1\"/> </p><p>回想起我们的解必须要满足边界条件。我们可以把待求函数分解为如下两部分之和（边界条件齐次化）：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5CPsi_%7Bt%7D%28%5Cvec%7Bx%7D%29%3DA%28%5Cvec%7Bx%7D%29%2BF%28%5Cvec%7Bx%7D%2CN%28%5Cvec%7Bx%7D%2C%5Cvec%7Bp%7D%29%29.\" alt=\"\\Psi_{t}(\\vec{x})=A(\\vec{x})+F(\\vec{x},N(\\vec{x},\\vec{p})).\" eeimg=\"1\"/> </p><p>其中，第一部分只要求满足边界条件；第二部分要求满足齐次边界条件，而 <img src=\"https://www.zhihu.com/equation?tex=N%28%5Cvec%7Bx%7D%2C%5Cvec%7Bp%7D%29\" alt=\"N(\\vec{x},\\vec{p})\" eeimg=\"1\"/> 就是神经网络的输出。</p><p>在这里引入神经网络，数学上是有依据的——仅有一个隐藏层的神经网络，便可以任意逼近欧氏空间的紧子集上的任意光滑函数（结论可能不确切，我暂时也懒得去证...）。那么我们的问题就变成，对于区域内的每一个点 <img src=\"https://www.zhihu.com/equation?tex=%5Cvec%7Bx%7D+\" alt=\"\\vec{x} \" eeimg=\"1\"/> ，将其输入神经网络，经过计算得到对应的 <img src=\"https://www.zhihu.com/equation?tex=%5CPsi_%7Bt%7D%28%5Cvec%7Bx%7D%29\" alt=\"\\Psi_{t}(\\vec{x})\" eeimg=\"1\"/> 及其一阶导（梯度）、...，进而算出当前网络对应的cost function（也就是：<img src=\"https://www.zhihu.com/equation?tex=%5Csum_%7B%5Cvec%7Bx_%7Bi%7D%7D%7D+%28+G%28%5Cvec%7Bx_%7Bi%7D%7D%2C%5CPsi_%7Bt%7D%28%5Cvec%7Bx_%7Bi%7D%7D%2C%5Cvec%7Bp%7D%29%2C%5Cbigtriangledown%5CPsi_%7Bt%7D%28%5Cvec%7Bx_%7Bi%7D%7D%2C%5Cvec%7Bp%7D%29%2C%5Cbigtriangledown%5E%7B2%7D%5CPsi_%7Bt%7D%28%5Cvec%7Bx_%7Bi%7D%7D%2C%5Cvec%7Bp%7D%29%29+%29%5E%7B2%7D.\" alt=\"\\sum_{\\vec{x_{i}}} ( G(\\vec{x_{i}},\\Psi_{t}(\\vec{x_{i}},\\vec{p}),\\bigtriangledown\\Psi_{t}(\\vec{x_{i}},\\vec{p}),\\bigtriangledown^{2}\\Psi_{t}(\\vec{x_{i}},\\vec{p})) )^{2}.\" eeimg=\"1\"/> ）的数值。</p><p>为了使用软件进行优化，我们最好要提供cost function对网络中各个权值的导数（方便使用梯度下降法快速求取极小值）。当然这一般是一个比较繁琐的过程..........对于特定的方程，一旦给出了cost function的具体形式，以及其对参数向量 <img src=\"https://www.zhihu.com/equation?tex=%5Cvec%7Bp%7D\" alt=\"\\vec{p}\" eeimg=\"1\"/> 的导数的具体形式，剩下的工作就可以完全由matlab完成。</p><p>如果你觉得上面这段话说的比较模糊，可以参考这篇论文：Artificial neural networks for solving ordinary and partial differential equations （网址：<a href=\"https://link.zhihu.com/?target=http%3A//ieeexplore.ieee.org/abstract/document/712178/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">ieeexplore.ieee.org/abs</span><span class=\"invisible\">tract/document/712178/</span><span class=\"ellipsis\"></span></a>）。</p><p>接下来是具体的实现。我选取的方程形式十分简单（论文中的Problem 2）：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7Bd%5CPsi%7D%7Bdx%7D%2B%5Cfrac%7B1%7D%7B5%7D%5CPsi%3De%5E%7B-x%2F5%7Dcos%28x%29.\" alt=\"\\frac{d\\Psi}{dx}+\\frac{1}{5}\\Psi=e^{-x/5}cos(x).\" eeimg=\"1\"/> 边界条件为： <img src=\"https://www.zhihu.com/equation?tex=%5CPsi%280%29%3D0.\" alt=\"\\Psi(0)=0.\" eeimg=\"1\"/> 对应试探解的形式为： <img src=\"https://www.zhihu.com/equation?tex=%5CPsi_%7Bt%7D%28x%29%3DxN%28x%2C%5Cvec%7Bp%7D%29.\" alt=\"\\Psi_{t}(x)=xN(x,\\vec{p}).\" eeimg=\"1\"/> </p><p>为此，我在matlab中简单构建了一个三层的神经网络：第一层是输入，仅包含一个元；第二层是隐藏层，包含三个神经元；第三层是输出，注意我们现在不是处理分类问题（不限制输出范围为[0,1]），所以输出结果是隐藏层输出的线性组合（不加sigmoid函数）。</p><p>求导大概算了一张纸...向量化还不太熟练...估计代码里也有一点冗余...最终的优化使用了matlab的fminunc函数，迭代次数100次（对于0&lt;x&lt;10，100次迭代已经能够得到不错的结果）。大家可以看下图感受一下，蓝色圈代表严格解，红色叉是训练出来的数值解。</p><figure><noscript><img src=\"https://pic2.zhimg.com/v2-97f6e770fdb965bcdbfe8697f32d211d_b.jpg\" data-caption=\"\" data-rawwidth=\"1580\" data-rawheight=\"1108\" class=\"origin_image zh-lightbox-thumb\" width=\"1580\" data-original=\"https://pic2.zhimg.com/v2-97f6e770fdb965bcdbfe8697f32d211d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1580&#39; height=&#39;1108&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-rawwidth=\"1580\" data-rawheight=\"1108\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1580\" data-original=\"https://pic2.zhimg.com/v2-97f6e770fdb965bcdbfe8697f32d211d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-97f6e770fdb965bcdbfe8697f32d211d_b.jpg\"/></figure><p>离0越远的地方偏离越明显（可以预料）。cost function取值在10^-4量级，还算比较理想。如果希望得到更好的结果，最简单的方法就是增加隐藏层中神经元的个数。</p><p>其实神经网络也可以用来解稍微复杂一些的ODE以及PDE。这个方法比较麻烦的一点在于，对于一个新的方程，必须手动给出解的形式（依据边界条件），如果希望训练速度较快，最好手动给出cost function对参数的导数（非常麻烦...估计复杂一点的PDE会算到怀疑人生）。</p><p>我对微分方程数值解这个领域其实没有什么了解，因此不评价用ANN解方程有没有明显优劣（估计会比传统算法慢）。按照论文的说法，用ANN解方程有如下几个优点：（1）尽管训练样本必须是离散的，ANN给出的近似解是连续的（光滑的）；（2）所需模型参数少，空间开销小；（3）普适性强——可以扩展到解不规则边界的PDE...方便硬件实现/并行计算（@高能效的小伙伴们^_^）。</p><p>idea来自于某位老师...虽然我很好奇但谷歌出来的相关内容不是很多...知乎上相关问题也不多（有个问题底下只有二平大神一个回答）。估计没人做的原因还是速度慢？</p><p>接下来打算考虑一下解非线性方程...嗯~如果有时间还得学学python但估计回学校就被作业淹没了QAQ</p><p>附：代码</p><p>main.m：</p><div class=\"highlight\"><pre><code class=\"language-matlab\"><span class=\"n\">x</span><span class=\"p\">=</span><span class=\"mi\">0</span><span class=\"p\">:</span><span class=\"mf\">0.10</span><span class=\"p\">:</span><span class=\"mi\">10</span><span class=\"p\">;</span><span class=\"c\">%41</span>\n<span class=\"n\">m</span><span class=\"p\">=</span><span class=\"nb\">size</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">);</span>\n<span class=\"n\">n</span><span class=\"p\">=</span><span class=\"mi\">3</span><span class=\"p\">;</span>\n<span class=\"n\">A</span><span class=\"p\">=</span><span class=\"mi\">0</span><span class=\"p\">;</span>\n\n<span class=\"n\">w</span><span class=\"p\">=</span><span class=\"n\">rand_Init_Weights</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">);</span>\n<span class=\"n\">b</span><span class=\"p\">=</span><span class=\"n\">rand_Init_Weights</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">);</span>\n<span class=\"n\">v</span><span class=\"p\">=</span><span class=\"n\">rand_Init_Weights</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">);</span>\n\n<span class=\"n\">init_param</span><span class=\"p\">=[</span><span class=\"n\">w</span><span class=\"p\">;</span><span class=\"n\">b</span><span class=\"p\">;</span><span class=\"n\">v</span><span class=\"p\">];</span>\n\n<span class=\"n\">options</span><span class=\"p\">=</span><span class=\"n\">optimset</span><span class=\"p\">(</span><span class=\"s\">&#39;Display&#39;</span><span class=\"p\">,</span><span class=\"s\">&#39;off&#39;</span><span class=\"p\">,</span><span class=\"s\">&#39;GradObj&#39;</span><span class=\"p\">,</span> <span class=\"s\">&#39;on&#39;</span><span class=\"p\">,</span><span class=\"s\">&#39;MaxIter&#39;</span><span class=\"p\">,</span> <span class=\"mi\">100</span><span class=\"p\">);</span>\n<span class=\"p\">[</span><span class=\"n\">param</span><span class=\"p\">,</span><span class=\"n\">cost</span><span class=\"p\">,</span><span class=\"n\">exit_flag</span><span class=\"p\">]=</span><span class=\"c\">...</span>\n    <span class=\"n\">fminunc</span><span class=\"p\">(@(</span><span class=\"n\">p</span><span class=\"p\">)(</span><span class=\"n\">nnCostFunction</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"p\">,</span><span class=\"n\">x</span><span class=\"p\">,</span><span class=\"n\">A</span><span class=\"p\">,</span><span class=\"n\">n</span><span class=\"p\">))</span> <span class=\"p\">,</span> <span class=\"n\">init_param</span><span class=\"p\">,</span> <span class=\"n\">options</span><span class=\"p\">);</span>\n\n<span class=\"nb\">disp</span><span class=\"p\">(</span><span class=\"n\">cost</span><span class=\"p\">);</span>\n\n<span class=\"n\">w</span><span class=\"p\">=</span><span class=\"n\">param</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">:</span><span class=\"n\">n</span><span class=\"p\">,:);</span>\n<span class=\"n\">b</span><span class=\"p\">=</span><span class=\"n\">param</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">:</span><span class=\"mi\">2</span><span class=\"o\">*</span><span class=\"n\">n</span><span class=\"p\">,:);</span>\n<span class=\"n\">v</span><span class=\"p\">=</span><span class=\"n\">param</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"o\">*</span><span class=\"n\">n</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">:</span><span class=\"mi\">3</span><span class=\"o\">*</span><span class=\"n\">n</span><span class=\"p\">,:);</span>\n\n<span class=\"n\">y</span><span class=\"p\">=</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">w</span><span class=\"p\">,</span><span class=\"n\">b</span><span class=\"p\">,</span><span class=\"n\">v</span><span class=\"p\">,</span><span class=\"n\">x</span><span class=\"p\">,</span><span class=\"n\">A</span><span class=\"p\">);</span>\n<span class=\"n\">y_r</span><span class=\"p\">=</span><span class=\"nb\">exp</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mf\">0.2</span><span class=\"o\">*</span><span class=\"n\">x</span><span class=\"p\">)</span><span class=\"o\">.*</span><span class=\"nb\">sin</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">);</span>\n<span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span><span class=\"n\">y</span><span class=\"p\">,</span><span class=\"s\">&#39;r-x&#39;</span><span class=\"p\">);</span>\n<span class=\"n\">hold</span> <span class=\"n\">on</span><span class=\"p\">;</span>\n<span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span><span class=\"n\">y_r</span><span class=\"p\">,</span><span class=\"s\">&#39;b-o&#39;</span><span class=\"p\">);</span></code></pre></div><p>nnCostFunction.m：</p><div class=\"highlight\"><pre><code class=\"language-matlab\"><span class=\"k\">function</span><span class=\"w\"> </span>[J,grad] <span class=\"p\">=</span><span class=\"w\"> </span><span class=\"nf\">nnCostFunction</span><span class=\"p\">(</span>param,x,A,n<span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"w\">\n</span><span class=\"w\"></span><span class=\"n\">w</span><span class=\"p\">=</span><span class=\"n\">param</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">:</span><span class=\"n\">n</span><span class=\"p\">,:);</span>\n<span class=\"n\">b</span><span class=\"p\">=</span><span class=\"n\">param</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">:</span><span class=\"mi\">2</span><span class=\"o\">*</span><span class=\"n\">n</span><span class=\"p\">,:);</span>\n<span class=\"n\">v</span><span class=\"p\">=</span><span class=\"n\">param</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"o\">*</span><span class=\"n\">n</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">:</span><span class=\"mi\">3</span><span class=\"o\">*</span><span class=\"n\">n</span><span class=\"p\">,:);</span>\n\n<span class=\"n\">m</span><span class=\"p\">=</span><span class=\"nb\">size</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span><span class=\"mi\">2</span><span class=\"p\">);</span>\n<span class=\"n\">n</span><span class=\"p\">=</span><span class=\"nb\">size</span><span class=\"p\">(</span><span class=\"n\">w</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">);</span>\n<span class=\"n\">J</span><span class=\"p\">=</span><span class=\"mi\">0</span><span class=\"p\">;</span>\n<span class=\"n\">grad_w</span><span class=\"p\">=</span><span class=\"nb\">zeros</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">);</span>\n<span class=\"n\">grad_b</span><span class=\"p\">=</span><span class=\"nb\">zeros</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">);</span>\n<span class=\"n\">grad_v</span><span class=\"p\">=</span><span class=\"nb\">zeros</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">);</span>\n\n<span class=\"k\">for</span> <span class=\"nb\">i</span><span class=\"p\">=</span><span class=\"mi\">1</span><span class=\"p\">:</span><span class=\"n\">m</span>\n    <span class=\"n\">sum</span><span class=\"p\">=</span><span class=\"mi\">0</span><span class=\"p\">;</span>\n    <span class=\"k\">for</span> <span class=\"nb\">j</span><span class=\"p\">=</span><span class=\"mi\">1</span><span class=\"p\">:</span><span class=\"n\">n</span>\n        <span class=\"n\">sigma</span><span class=\"p\">=</span><span class=\"n\">sigmoid</span><span class=\"p\">(</span><span class=\"n\">w</span><span class=\"p\">(</span><span class=\"nb\">j</span><span class=\"p\">)</span><span class=\"o\">*</span><span class=\"n\">x</span><span class=\"p\">(</span><span class=\"nb\">i</span><span class=\"p\">)</span><span class=\"o\">+</span><span class=\"n\">b</span><span class=\"p\">(</span><span class=\"nb\">j</span><span class=\"p\">));</span>\n        <span class=\"n\">sum</span><span class=\"p\">=</span><span class=\"n\">sum</span><span class=\"o\">+</span><span class=\"n\">v</span><span class=\"p\">(</span><span class=\"nb\">j</span><span class=\"p\">)</span><span class=\"o\">*</span><span class=\"n\">sigma</span><span class=\"o\">*</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"o\">+</span><span class=\"mf\">0.2</span><span class=\"o\">*</span><span class=\"n\">x</span><span class=\"p\">(</span><span class=\"nb\">i</span><span class=\"p\">)</span><span class=\"o\">+</span><span class=\"n\">w</span><span class=\"p\">(</span><span class=\"nb\">j</span><span class=\"p\">)</span><span class=\"o\">*</span><span class=\"n\">x</span><span class=\"p\">(</span><span class=\"nb\">i</span><span class=\"p\">)</span><span class=\"o\">*</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"o\">-</span><span class=\"n\">sigma</span><span class=\"p\">));</span>\n    <span class=\"k\">end</span><span class=\"p\">;</span>\n    <span class=\"n\">tmp</span><span class=\"p\">=(</span><span class=\"mf\">0.2</span><span class=\"o\">*</span><span class=\"n\">A</span><span class=\"o\">-</span> <span class=\"p\">(</span><span class=\"nb\">exp</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mf\">0.2</span><span class=\"o\">*</span><span class=\"n\">x</span><span class=\"p\">(</span><span class=\"nb\">i</span><span class=\"p\">))</span><span class=\"o\">*</span><span class=\"nb\">cos</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">(</span><span class=\"nb\">i</span><span class=\"p\">)))</span> <span class=\"o\">+</span><span class=\"n\">sum</span><span class=\"p\">);</span>\n    <span class=\"n\">J</span><span class=\"p\">=</span><span class=\"n\">J</span><span class=\"o\">+</span><span class=\"p\">(</span><span class=\"mf\">0.5</span><span class=\"o\">/</span><span class=\"n\">m</span><span class=\"p\">)</span><span class=\"o\">*</span><span class=\"n\">tmp</span>^<span class=\"mi\">2</span><span class=\"p\">;</span>\n    <span class=\"n\">tmp_sigma</span><span class=\"p\">=</span><span class=\"n\">sigmoid</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">(</span><span class=\"nb\">i</span><span class=\"p\">)</span><span class=\"o\">*</span><span class=\"n\">w</span><span class=\"o\">+</span><span class=\"n\">b</span><span class=\"p\">);</span>\n    \n    <span class=\"n\">grad_w</span><span class=\"p\">=</span><span class=\"n\">grad_w</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"o\">/</span><span class=\"n\">m</span><span class=\"p\">)</span><span class=\"o\">*</span><span class=\"n\">tmp</span><span class=\"o\">*</span><span class=\"p\">(</span><span class=\"c\">...</span>\n        <span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"o\">*</span><span class=\"n\">x</span><span class=\"p\">(</span><span class=\"nb\">i</span><span class=\"p\">)</span><span class=\"o\">+</span><span class=\"mf\">0.2</span><span class=\"o\">*</span><span class=\"n\">x</span><span class=\"p\">(</span><span class=\"nb\">i</span><span class=\"p\">)</span><span class=\"o\">*</span><span class=\"n\">x</span><span class=\"p\">(</span><span class=\"nb\">i</span><span class=\"p\">))</span><span class=\"o\">*</span><span class=\"p\">(</span><span class=\"n\">v</span><span class=\"o\">.*</span><span class=\"n\">tmp_sigma</span><span class=\"o\">.*</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"o\">-</span><span class=\"n\">tmp_sigma</span><span class=\"p\">))</span> <span class=\"o\">+</span><span class=\"c\">...</span>\n        <span class=\"n\">x</span><span class=\"p\">(</span><span class=\"nb\">i</span><span class=\"p\">)</span><span class=\"o\">*</span><span class=\"n\">x</span><span class=\"p\">(</span><span class=\"nb\">i</span><span class=\"p\">)</span><span class=\"o\">*</span><span class=\"p\">(</span><span class=\"n\">v</span><span class=\"o\">.*</span><span class=\"n\">w</span><span class=\"o\">.*</span><span class=\"n\">tmp_sigma</span><span class=\"o\">.*</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"o\">-</span><span class=\"n\">tmp_sigma</span><span class=\"p\">)</span><span class=\"o\">.*</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"o\">-</span><span class=\"mi\">2</span><span class=\"o\">*</span><span class=\"n\">tmp_sigma</span><span class=\"p\">)));</span>\n    \n    <span class=\"n\">grad_b</span><span class=\"p\">=</span><span class=\"n\">grad_b</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"o\">/</span><span class=\"n\">m</span><span class=\"p\">)</span><span class=\"o\">*</span><span class=\"n\">tmp</span><span class=\"o\">*</span><span class=\"p\">(</span><span class=\"c\">...</span>\n        <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"o\">+</span><span class=\"mf\">0.2</span><span class=\"o\">*</span><span class=\"n\">x</span><span class=\"p\">(</span><span class=\"nb\">i</span><span class=\"p\">))</span><span class=\"o\">*</span><span class=\"n\">tmp_sigma</span><span class=\"o\">.*</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"o\">-</span><span class=\"n\">tmp_sigma</span><span class=\"p\">)</span><span class=\"o\">.*</span><span class=\"n\">v</span><span class=\"o\">+</span><span class=\"c\">...</span>\n        <span class=\"n\">x</span><span class=\"p\">(</span><span class=\"nb\">i</span><span class=\"p\">)</span><span class=\"o\">*</span><span class=\"n\">tmp_sigma</span><span class=\"o\">.*</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"o\">-</span><span class=\"n\">tmp_sigma</span><span class=\"p\">)</span><span class=\"o\">.*</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"o\">-</span><span class=\"mi\">2</span><span class=\"o\">*</span><span class=\"n\">tmp_sigma</span><span class=\"p\">)</span><span class=\"o\">.*</span><span class=\"n\">v</span><span class=\"o\">.*</span><span class=\"n\">w</span><span class=\"p\">);</span>\n    \n    <span class=\"n\">grad_v</span><span class=\"p\">=</span><span class=\"n\">grad_v</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"o\">/</span><span class=\"n\">m</span><span class=\"p\">)</span><span class=\"o\">*</span><span class=\"n\">tmp</span><span class=\"o\">*</span><span class=\"p\">(</span><span class=\"c\">...</span>\n        <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"o\">+</span><span class=\"mf\">0.2</span><span class=\"o\">*</span><span class=\"n\">x</span><span class=\"p\">(</span><span class=\"nb\">i</span><span class=\"p\">))</span><span class=\"o\">*</span><span class=\"n\">tmp_sigma</span> <span class=\"o\">+</span> <span class=\"c\">...</span>\n        <span class=\"n\">x</span><span class=\"p\">(</span><span class=\"nb\">i</span><span class=\"p\">)</span><span class=\"o\">*</span><span class=\"n\">w</span><span class=\"o\">.*</span><span class=\"n\">tmp_sigma</span><span class=\"o\">.*</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"o\">-</span><span class=\"n\">tmp_sigma</span><span class=\"p\">));</span>\n<span class=\"k\">end</span><span class=\"p\">;</span>\n\n<span class=\"n\">grad</span><span class=\"p\">=[</span><span class=\"n\">grad_w</span><span class=\"p\">;</span><span class=\"n\">grad_b</span><span class=\"p\">;</span><span class=\"n\">grad_v</span><span class=\"p\">];</span>\n<span class=\"k\">end</span></code></pre></div><p>rand_Init_Weights.m：</p><div class=\"highlight\"><pre><code class=\"language-matlab\"><span class=\"k\">function</span><span class=\"w\"> </span>W <span class=\"p\">=</span><span class=\"w\"> </span><span class=\"nf\">rand_Init_Weights</span><span class=\"p\">(</span>r, c<span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"w\"></span><span class=\"c\">%RANDINITIALIZEWEIGHTS Randomly initialize the weights of a layer</span>\n<span class=\"n\">W</span> <span class=\"p\">=</span> <span class=\"nb\">zeros</span><span class=\"p\">(</span><span class=\"n\">r</span><span class=\"p\">,</span> <span class=\"n\">c</span><span class=\"p\">);</span>\n\n<span class=\"n\">epsilon_init</span> <span class=\"p\">=</span> <span class=\"mf\">0.10</span><span class=\"p\">;</span>\n<span class=\"n\">W</span><span class=\"p\">=</span><span class=\"nb\">rand</span><span class=\"p\">(</span><span class=\"n\">r</span><span class=\"p\">,</span> <span class=\"n\">c</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"mi\">2</span> <span class=\"o\">*</span> <span class=\"n\">epsilon_init</span> <span class=\"o\">-</span> <span class=\"n\">epsilon_init</span><span class=\"p\">;</span>\n<span class=\"k\">end</span></code></pre></div><p>sigmoid.m：</p><div class=\"highlight\"><pre><code class=\"language-matlab\"><span class=\"k\">function</span><span class=\"w\"> </span>g <span class=\"p\">=</span><span class=\"w\"> </span><span class=\"nf\">sigmoid</span><span class=\"p\">(</span>z<span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"w\"></span><span class=\"c\">%SIGMOID Compute sigmoid functoon</span>\n<span class=\"c\">%   J = SIGMOID(z) computes the sigmoid of z.</span>\n<span class=\"n\">g</span> <span class=\"p\">=</span> <span class=\"mf\">1.0</span> <span class=\"o\">./</span> <span class=\"p\">(</span><span class=\"mf\">1.0</span> <span class=\"o\">+</span> <span class=\"nb\">exp</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"n\">z</span><span class=\"p\">));</span>\n<span class=\"k\">end</span></code></pre></div><p>predict.m：</p><div class=\"highlight\"><pre><code class=\"language-matlab\"><span class=\"k\">function</span><span class=\"w\"> </span>[y] <span class=\"p\">=</span><span class=\"w\"> </span><span class=\"nf\">predict</span><span class=\"p\">(</span>w,b,v,x,A<span class=\"p\">)</span><span class=\"w\">\n</span><span class=\"w\"></span><span class=\"c\">%predict</span>\n    <span class=\"n\">m</span><span class=\"p\">=</span><span class=\"nb\">size</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span><span class=\"mi\">2</span><span class=\"p\">);</span>\n    <span class=\"n\">y</span><span class=\"p\">=</span><span class=\"nb\">zeros</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"n\">m</span><span class=\"p\">);</span>\n    <span class=\"k\">for</span> <span class=\"nb\">i</span><span class=\"p\">=</span><span class=\"mi\">1</span><span class=\"p\">:</span><span class=\"n\">m</span>\n        <span class=\"n\">y</span><span class=\"p\">(</span><span class=\"nb\">i</span><span class=\"p\">)=</span><span class=\"n\">A</span><span class=\"o\">+</span><span class=\"n\">x</span><span class=\"p\">(</span><span class=\"nb\">i</span><span class=\"p\">)</span><span class=\"o\">*</span><span class=\"n\">sum</span><span class=\"p\">(</span><span class=\"n\">v</span><span class=\"o\">.*</span><span class=\"n\">sigmoid</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">(</span><span class=\"nb\">i</span><span class=\"p\">)</span><span class=\"o\">*</span><span class=\"n\">w</span><span class=\"o\">+</span><span class=\"n\">b</span><span class=\"p\">));</span>\n    <span class=\"k\">end</span><span class=\"p\">;</span>\n<span class=\"k\">end</span></code></pre></div><p></p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "神经网络", 
                    "tagLink": "https://api.zhihu.com/topics/19607065"
                }, 
                {
                    "tag": "计算物理学", 
                    "tagLink": "https://api.zhihu.com/topics/19682213"
                }
            ], 
            "comments": [
                {
                    "userName": "周清逸", 
                    "userLink": "https://www.zhihu.com/people/23dd9239a500559d8850eac45ed2038f", 
                    "content": "<p>平时代码写的比较少...风格什么的求大家轻喷QAQ</p>", 
                    "likes": 1, 
                    "childComments": []
                }, 
                {
                    "userName": "东林钟声", 
                    "userLink": "https://www.zhihu.com/people/32c9821f66cc21611b55e64c4f3af363", 
                    "content": "挺好的，我之前是用cnn拟合pid控制器水了篇文章，我之前认识伯克利一个博士用nn解hjb，没太看懂😂不过思路跟你的是差不多的", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "2prime", 
                            "userLink": "https://www.zhihu.com/people/910050206c6c9b1d57ec9bab0f4c9bda", 
                            "content": "求问解hjb的文献", 
                            "likes": 0, 
                            "replyToAuthor": "东林钟声"
                        }, 
                        {
                            "userName": "东林钟声", 
                            "userLink": "https://www.zhihu.com/people/32c9821f66cc21611b55e64c4f3af363", 
                            "content": "<p>我找找，之前看最优控制的时候看了些。</p>", 
                            "likes": 1, 
                            "replyToAuthor": "2prime"
                        }
                    ]
                }, 
                {
                    "userName": "AVIDA", 
                    "userLink": "https://www.zhihu.com/people/d6c33a202bd48d16c9087409a7d8ab8b", 
                    "content": "所以这个图是随意配的吗w", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "周清逸", 
                            "userLink": "https://www.zhihu.com/people/23dd9239a500559d8850eac45ed2038f", 
                            "content": "嗯～看心情^_^", 
                            "likes": 0, 
                            "replyToAuthor": "AVIDA"
                        }
                    ]
                }, 
                {
                    "userName": "「已注销」", 
                    "userLink": "https://www.zhihu.com/people/626afcb8e7c52a53e65ddc8da4e470c9", 
                    "content": "斯郭伊内(=^･ｪ･^=)", 
                    "likes": 1, 
                    "childComments": []
                }, 
                {
                    "userName": "梁昊", 
                    "userLink": "https://www.zhihu.com/people/69398e1ad4dc9428f32cc65b82e73859", 
                    "content": "<p>在求解微分方程和泛函极值问题之间相互转换的技术叫变分法，不论是用来推公式还是直接数值应用都是很广泛的，不过好像少有你那么简单粗暴的...</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "周清逸", 
                            "userLink": "https://www.zhihu.com/people/23dd9239a500559d8850eac45ed2038f", 
                            "content": "变分法不是很懂I _ I 不过感觉影响偏差的主要还是神经元数目？", 
                            "likes": 0, 
                            "replyToAuthor": "梁昊"
                        }, 
                        {
                            "userName": "梁昊", 
                            "userLink": "https://www.zhihu.com/people/69398e1ad4dc9428f32cc65b82e73859", 
                            "content": "<p>啊你有物理背景应该很熟悉变分法这一套才对。神经元数目对应到微分方程数值解法上可能能对应截断精度的阶数，但我这里试的也只是一个二阶方法。一直以来都觉得智能计算只适合那些“难”的问题，因为人们对那些问题没有成熟的理解，所以类似于神网这种瞎蒙的才会表现得较好。</p>", 
                            "likes": 0, 
                            "replyToAuthor": "周清逸"
                        }
                    ]
                }, 
                {
                    "userName": "梁昊", 
                    "userLink": "https://www.zhihu.com/people/69398e1ad4dc9428f32cc65b82e73859", 
                    "content": "<p>我数值测试了一下，用CN方法解这个方程，步长起码得取到0.5以上才会出现你这种肉眼可见的偏差...何况你这里似乎是每个迭代步都需要求解一个非线性极值问题？</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "知乎用户", 
                    "userLink": "https://www.zhihu.com/people/0", 
                    "content": "但这样参数的数目是不是会很多，和直接得到一个数值解需要的存储量（前者存参数），后者存一系列函数值）相当？", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "周清逸", 
                            "userLink": "https://www.zhihu.com/people/23dd9239a500559d8850eac45ed2038f", 
                            "content": "这个网络比较小所以参数其实挺少的…不过我猜大家比较关心的还是算法速度～", 
                            "likes": 0, 
                            "replyToAuthor": "知乎用户"
                        }
                    ]
                }, 
                {
                    "userName": "Yupeng", 
                    "userLink": "https://www.zhihu.com/people/9a8e80543e042053e2dd8efa43596818", 
                    "content": "这个方法有人做。而且我也在做。ode其实都很好说，但问题到了diffusion的pde后会收敛速度很慢，而且准确度较低。。。现在也在想解决办法。", 
                    "likes": 1, 
                    "childComments": []
                }, 
                {
                    "userName": "严妆", 
                    "userLink": "https://www.zhihu.com/people/7a0e562d489c9e2bae033373fda16514", 
                    "content": "用pytorch在写，借楼问问，pytorch自动求导x.grad为什么返回nonetype? 不能直接带到式子里面", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/28348392", 
            "userName": "周清逸", 
            "userLink": "https://www.zhihu.com/people/23dd9239a500559d8850eac45ed2038f", 
            "upvote": 4, 
            "title": "信息与香农...", 
            "content": "<p>当我们谈“信息量”，我们在说什么？</p><p>直观上，信息量与概率有一定的联系。扔一个骰子，如果我告诉你“点数大于3”，你知道点数可能是4、5、6（概率是二分之一）；如果我告诉你点数是6，那么你知道点数是6（概率是六分之一）。显然知道点数是6这种情况，你获得的信息要多一些。基于上述考虑，Hartley提出用消息出现概率的对数测度作为信息的度量单位：<img src=\"https://www.zhihu.com/equation?tex=I%28x_%7Bi%7D%29%3Dlog%5Cfrac%7B1%7D%7BP%28x_%7Bi%7D%29%7D%3D-logP%28x_%7Bi%7D%29.\" alt=\"I(x_{i})=log\\frac{1}{P(x_{i})}=-logP(x_{i}).\" eeimg=\"1\"/>（为什么要用对数？考虑一下A发生同时B发生的情形就可以理解了——概率是相乘，信息量则是求和，取对数是很自然的选择）。</p><p>对于一个离散的信源，它发出了由很多符号组成的消息。我们想计算每个符号所含信息量的统计平均值。假设符号数目N（比如我的消息是由26个英文字母组成，那么N就是26），离散信源的平均信息量为：<img src=\"https://www.zhihu.com/equation?tex=H%28X%29%3D-%5Csum_%7Bi%3D1%7D%5E%7BN%7D+P%28x_%7Bi%7D%29logP%28x_%7Bi%7D%29.\" alt=\"H(X)=-\\sum_{i=1}^{N} P(x_{i})logP(x_{i}).\" eeimg=\"1\"/>类似的公式大家在热力学与统计物理里是见过的，可以考虑一下，有什么联系与区别呢（这问题可以单独写一篇文章233）？</p><p>类似地（需要一点计算，思路就是把连续的分布离散化再取极限，然后扔掉奇怪的项），定义连续信息源的平均信息量： <img src=\"https://www.zhihu.com/equation?tex=H%28X%29%3D-%5Cint_%7B-%5Cinfty%7D%5E%7B%5Cinfty%7Dp%28x%29logp%28x%29dx.\" alt=\"H(X)=-\\int_{-\\infty}^{\\infty}p(x)logp(x)dx.\" eeimg=\"1\"/></p><p>我们不禁要问：在怎样的概率分布下，这个信息量取最大值？我们有一个显然的约束条件：概率密度函数的归一化 <img src=\"https://www.zhihu.com/equation?tex=%5Cint_%7B-%5Cinfty%7D%5E%7B%5Cinfty%7D+p%28x%29+dx%3D1.\" alt=\"\\int_{-\\infty}^{\\infty} p(x) dx=1.\" eeimg=\"1\"/> 然而我们还必须对消息源的输出作出进一步限制。考虑实际情况，一种常见的限制是“均方值受限”（实际上就是信号的功率受到限制），可以记为 <img src=\"https://www.zhihu.com/equation?tex=%5Cint_%7B-%5Cinfty%7D%5E%7B%5Cinfty%7Dx%5E%7B2%7D+p%28x%29+dx%3D%5Csigma+%5E%7B2%7D.\" alt=\"\\int_{-\\infty}^{\\infty}x^{2} p(x) dx=\\sigma ^{2}.\" eeimg=\"1\"/> 在这两个约束条件之下，求出使 <img src=\"https://www.zhihu.com/equation?tex=H%28X%29\" alt=\"H(X)\" eeimg=\"1\"/> 取最大值的概率密度函数。可以使用Lagrange乘子法来求解。引入乘子 <img src=\"https://www.zhihu.com/equation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5Cmu\" alt=\"\\mu\" eeimg=\"1\"/> ：</p><p><img src=\"https://www.zhihu.com/equation?tex=F%28X%29%3D%5Cint%5B-p%28x%29logp%28x%29%2B%5Clambda+p%28x%29+x%5E%7B2%7D+%2B%5Cmu+p%28x%29%5Ddx.\" alt=\"F(X)=\\int[-p(x)logp(x)+\\lambda p(x) x^{2} +\\mu p(x)]dx.\" eeimg=\"1\"/> 并由 <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+F%7D%7B%5Cpartial+p%7D%3D0\" alt=\"\\frac{\\partial F}{\\partial p}=0\" eeimg=\"1\"/> 得到： <img src=\"https://www.zhihu.com/equation?tex=-1-log+p%28x%29+%2B%5Clambda+x%5E%7B2%7D+%2B+%5Cmu+%3D0.\" alt=\"-1-log p(x) +\\lambda x^{2} + \\mu =0.\" eeimg=\"1\"/></p><p>再借助约束条件，解得：</p><p><img src=\"https://www.zhihu.com/equation?tex=p%28x%29%3D%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D+%5Csigma%7De%5E%7B-%5Cfrac%7Bx%5E2%7D%7B2%5Csigma+%5E2%7D%7D\" alt=\"p(x)=\\frac{1}{\\sqrt{2\\pi} \\sigma}e^{-\\frac{x^2}{2\\sigma ^2}}\" eeimg=\"1\"/> 为最佳概率密度函数。代入 <img src=\"https://www.zhihu.com/equation?tex=H%28X%29\" alt=\"H(X)\" eeimg=\"1\"/> 不难求得最佳分布对应的信息量： <img src=\"https://www.zhihu.com/equation?tex=H%28X%29%3Dln+%5Csigma+%5Csqrt%7B2%5Cpi%7D+%2B+%5Cfrac%7B1%7D%7B2%7D%3Dln+%5Csigma+%5Csqrt%7B2%5Cpi+e%7D+%3Dlog_%7B2%7D+%5Csigma+%5Csqrt%7B2%5Cpi+e%7D%28bit%29.\" alt=\"H(X)=ln \\sigma \\sqrt{2\\pi} + \\frac{1}{2}=ln \\sigma \\sqrt{2\\pi e} =log_{2} \\sigma \\sqrt{2\\pi e}(bit).\" eeimg=\"1\"/> 注意我们把单位换成了比特。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>香农公式针对的是有扰信道——接收到的信号是发送信号和信道噪声的线性叠加： <img src=\"https://www.zhihu.com/equation?tex=y%3Dx%2Bn.\" alt=\"y=x+n.\" eeimg=\"1\"/> 所谓通信，简单来说，就是信源发了一串消息 <img src=\"https://www.zhihu.com/equation?tex=x\" alt=\"x\" eeimg=\"1\"/> ，但是我们不能完全消除噪声的影响，所以总是可能出现错误（误码率不可能为0），那么接收者的任务，就是根据自己收到的一串消息 <img src=\"https://www.zhihu.com/equation?tex=y\" alt=\"y\" eeimg=\"1\"/> ，来想方设法“猜出”信源发出的消息 <img src=\"https://www.zhihu.com/equation?tex=x\" alt=\"x\" eeimg=\"1\"/> 可能是怎样的。</p><p>了解了这一点，我们来引入“互信息量”的概念。对于两个离散信源X、Y，X发送符号的概率场一般是已知的，<img src=\"https://www.zhihu.com/equation?tex=P%28x_%7Bi%7D%29\" alt=\"P(x_{i})\" eeimg=\"1\"/>即为先验概率。接收端每次收到一个<img src=\"https://www.zhihu.com/equation?tex=y_%7Bj%7D\" alt=\"y_{j}\" eeimg=\"1\"/>，都要重新估计发送端各符号<img src=\"https://www.zhihu.com/equation?tex=x_%7Bi%7D\" alt=\"x_{i}\" eeimg=\"1\"/>的概率分布，这一条件概率<img src=\"https://www.zhihu.com/equation?tex=P%28x_%7Bi%7D%2Fy_%7Bj%7D%29\" alt=\"P(x_{i}/y_{j})\" eeimg=\"1\"/>称为后验概率。定义互信息量为：</p><p><img src=\"https://www.zhihu.com/equation?tex=I%28x_%7Bi%7D%2Cy_%7Bj%7D%29%3Dlog%5Cfrac%7BP%28x_%7Bi%7D%2Fy_%7Bj%7D%29%7D%7BP%28x_%7Bi%7D%29%7D.\" alt=\"I(x_{i},y_{j})=log\\frac{P(x_{i}/y_{j})}{P(x_{i})}.\" eeimg=\"1\"/><br/></p><p>它的物理意义是接收端所能获取的关于信源X的信息量（在实际通信系统中，真正传输的信息量就是这个互信息量。）。而互信息量的统计平均值可以衡量通信系统每个符号传输信息的效率（平均互信息量）：</p><br/><p><img src=\"https://www.zhihu.com/equation?tex=I%28X%2CY%29%3D%5Csum_%7Bi%7D%5Csum_%7Bj%7D+P%28x_%7Bi%7Dy_%7Bj%7D%29+I%28x_%7Bi%7D%2Cy_%7Bj%7D%29.\" alt=\"I(X,Y)=\\sum_{i}\\sum_{j} P(x_{i}y_{j}) I(x_{i},y_{j}).\" eeimg=\"1\"/>对于连续情形，相应的定义是<img src=\"https://www.zhihu.com/equation?tex=I%28X%2CY%29%3D%5Cint_%7B-%5Cinfty%7D+%5E+%7B%5Cinfty%7D+%5Cint_%7B-%5Cinfty%7D+%5E+%7B%5Cinfty%7D+p%28xy%29+log%5Cfrac%7Bp%28xy%29%7D%7Bp%28x%29p%28y%29%7D+dxdy.\" alt=\"I(X,Y)=\\int_{-\\infty} ^ {\\infty} \\int_{-\\infty} ^ {\\infty} p(xy) log\\frac{p(xy)}{p(x)p(y)} dxdy.\" eeimg=\"1\"/></p><p>为了方便，再引入“条件熵”：若已知X中出现<img src=\"https://www.zhihu.com/equation?tex=x_%7Bi%7D\" alt=\"x_{i}\" eeimg=\"1\"/>，此时Y中出现<img src=\"https://www.zhihu.com/equation?tex=y_%7Bj%7D\" alt=\"y_{j}\" eeimg=\"1\"/>的条件平均信息量（条件熵）为：</p><p><img src=\"https://www.zhihu.com/equation?tex=H%28Y%2FX%29%3D%5Csum_%7Bi%7DP%28x_%7Bi%7D%29+%5Csum_%7Bj%7D%5B-P%28y_%7Bj%7D%2Fx_%7Bi%7D%29logP%28y_%7Bj%7D%2Fx_%7Bi%7D%29%5D%3D-%5Csum_%7Bi%7D%5Csum_%7Bj%7D+P%28x_%7Bi%7Dy_%7Bj%7D%29+logP%28y_%7Bj%7D%2Fx_%7Bi%7D%29.\" alt=\"H(Y/X)=\\sum_{i}P(x_{i}) \\sum_{j}[-P(y_{j}/x_{i})logP(y_{j}/x_{i})]=-\\sum_{i}\\sum_{j} P(x_{i}y_{j}) logP(y_{j}/x_{i}).\" eeimg=\"1\"/><br/></p><p>对于连续情形，相应的定义是：<img src=\"https://www.zhihu.com/equation?tex=H%28Y%2FX%29%3D%5Cint_%7B-%5Cinfty%7D%5E%7B%5Cinfty%7Dp%28x%29dx+%5Cint_%7B-%5Cinfty%7D%5E%7B%5Cinfty%7D+p%28y%2Fx%29+logp%28y%2Fx%29dy.\" alt=\"H(Y/X)=\\int_{-\\infty}^{\\infty}p(x)dx \\int_{-\\infty}^{\\infty} p(y/x) logp(y/x)dy.\" eeimg=\"1\"/></p><p>可以证明<img src=\"https://www.zhihu.com/equation?tex=I%28X%2CY%29%3DH%28Y%29-H%28Y%2FX%29\" alt=\"I(X,Y)=H(Y)-H(Y/X)\" eeimg=\"1\"/>成立，这个公式在之后的计算中会用到。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>对于一个实际的通信系统，我们设信道噪声的概率密度函数为 <img src=\"https://www.zhihu.com/equation?tex=f%28n%29\" alt=\"f(n)\" eeimg=\"1\"/> （认为噪声与信号统计独立），那么条件概率密度函数 <img src=\"https://www.zhihu.com/equation?tex=p%28y%2Fx%29\" alt=\"p(y/x)\" eeimg=\"1\"/> 就等于 <img src=\"https://www.zhihu.com/equation?tex=f%28n%29\" alt=\"f(n)\" eeimg=\"1\"/> ，即： <img src=\"https://www.zhihu.com/equation?tex=p%28y%2Fx%29%3Df%28y-x%29%3Df%28n%29.\" alt=\"p(y/x)=f(y-x)=f(n).\" eeimg=\"1\"/> 再根据条件熵定义，得：<img src=\"https://www.zhihu.com/equation?tex=H%28Y%2FX%29%3D%5Cint_%7B-%5Cinfty%7D%5E%7B%5Cinfty%7Dp%28x%29dx+%5Cint_%7B-%5Cinfty%7D%5E%7B%5Cinfty%7D+p%28y%2Fx%29+logp%28y%2Fx%29dy+%3D-%5Cint_%7B-%5Cinfty%7D%5E%7B%5Cinfty%7D+f%28n%29+logf%28n%29+dn+%3DH%28N%29.\" alt=\"H(Y/X)=\\int_{-\\infty}^{\\infty}p(x)dx \\int_{-\\infty}^{\\infty} p(y/x) logp(y/x)dy =-\\int_{-\\infty}^{\\infty} f(n) logf(n) dn =H(N).\" eeimg=\"1\"/>（第二个等号用到了噪声与信号统计独立的条件）。</p><p>对于有扰离散信道，定义最高信息传输速率为信道容量： <img src=\"https://www.zhihu.com/equation?tex=C%3Dmax%5BH%28X%29-H%28X%2FY%29%5D+r\" alt=\"C=max[H(X)-H(X/Y)] r\" eeimg=\"1\"/> （其中 <img src=\"https://www.zhihu.com/equation?tex=r\" alt=\"r\" eeimg=\"1\"/> 是发送端单位时间内发出符号的个数，这里就用到了互信息量与条件熵的关系）。</p><p>根据Nyquist采样定理，如果我们发出的连续信号，频带限于 <img src=\"https://www.zhihu.com/equation?tex=W\" alt=\"W\" eeimg=\"1\"/> ，那么它可以用频率 <img src=\"https://www.zhihu.com/equation?tex=2W\" alt=\"2W\" eeimg=\"1\"/> 的离散信号来无失真地表示（信息量是相等的）。基于此，有扰连续信道的信道容量可以表示为：</p><p><img src=\"https://www.zhihu.com/equation?tex=C%3Dmax%5BH%28Y%29-H%28Y%2FX%29%5D+%5Ccdot+2W.\" alt=\"C=max[H(Y)-H(Y/X)] \\cdot 2W.\" eeimg=\"1\"/></p><p>为了求得信道容量的理论上限，我们需要借助之前的结论。假设噪声是加性高斯白噪声（AWGN），信号功率记为 <img src=\"https://www.zhihu.com/equation?tex=S\" alt=\"S\" eeimg=\"1\"/> ，噪声功率记为 <img src=\"https://www.zhihu.com/equation?tex=N\" alt=\"N\" eeimg=\"1\"/> ，在平均功率受限的条件下我们知道：</p><p><img src=\"https://www.zhihu.com/equation?tex=H%28Y%29%3Dlog_%7B2%7D+%5Csqrt%7B2%5Cpi+e%28S%2BN%29%7D%3B+H%28Y%2FX%29%3Dlog_%7B2%7D%5Csqrt%7B2%5Cpi+eN%7D.\" alt=\"H(Y)=log_{2} \\sqrt{2\\pi e(S+N)}; H(Y/X)=log_{2}\\sqrt{2\\pi eN}.\" eeimg=\"1\"/></p><p>代入可以计算出有扰连续信道的信道容量：</p><p><img src=\"https://www.zhihu.com/equation?tex=C%3DWlog_%7B2%7D+%281%2B%5Cfrac%7BS%7D%7BN%7D%29.\" alt=\"C=Wlog_{2} (1+\\frac{S}{N}).\" eeimg=\"1\"/> 单位为 <img src=\"https://www.zhihu.com/equation?tex=bit%2Fs\" alt=\"bit/s\" eeimg=\"1\"/> .</p><p>这就是著名的香农公式，它告诉我们提升信噪比可以增大信道容量；另一方面，如果增大 <img src=\"https://www.zhihu.com/equation?tex=W\" alt=\"W\" eeimg=\"1\"/> ，并不能使 <img src=\"https://www.zhihu.com/equation?tex=C\" alt=\"C\" eeimg=\"1\"/> 无限制地增大——因为频带宽度会影响到噪声的功率。对于高斯白噪声，我们记它的单边功率谱密度为 <img src=\"https://www.zhihu.com/equation?tex=n_0\" alt=\"n_0\" eeimg=\"1\"/> ，那么将 <img src=\"https://www.zhihu.com/equation?tex=N%3Dn_%7B0%7DW\" alt=\"N=n_{0}W\" eeimg=\"1\"/> 代入：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Clim_%7BW+%5Crightarrow+%5Cinfty%7DC%3D%5Clim_%7BW%5Crightarrow+%5Cinfty%7D+Wlog_%7B2%7D+%281%2B%5Cfrac%7BS%7D%7Bn_%7B0%7D+W%7D%29+%5Capprox+1.44%5Cfrac%7BS%7D%7Bn_%7B0%7D%7D.\" alt=\"\\lim_{W \\rightarrow \\infty}C=\\lim_{W\\rightarrow \\infty} Wlog_{2} (1+\\frac{S}{n_{0} W}) \\approx 1.44\\frac{S}{n_{0}}.\" eeimg=\"1\"/>即便带宽很大，信道容量仍然是有限的。</p><p>香农公式虽然给出了信道容量的理论极限，但并没有说明如何接近这一极限。事实上，大部分通信工程师每天的工作，就是研究如何使实际的通信系统尽可能接近这个极限。</p><p>吐槽：虽然我们嘴上说“通信都是数学”，但更准确的说法应该是“计算”而不是“数学”。从这个角度讲，通信其实跟物理有点类似，可能还要偏工程一点。香农的工作是一切通信理论的基础，可以算是数学...但是后来工程师们所作的事情，则是在追求一些可以应用的结果。</p>", 
            "topic": [
                {
                    "tag": "通信", 
                    "tagLink": "https://api.zhihu.com/topics/19560870"
                }, 
                {
                    "tag": "信息论", 
                    "tagLink": "https://api.zhihu.com/topics/19612134"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/27908867", 
            "userName": "周清逸", 
            "userLink": "https://www.zhihu.com/people/23dd9239a500559d8850eac45ed2038f", 
            "upvote": 8, 
            "title": "电磁场中粒子的运动方程", 
            "content": "<p>这是一个略显宏大的话题...实在懒得打太多字所以让我们默认一些基本事实：</p><p>（1）相对论力学中，自由粒子的作用量是 <img src=\"https://www.zhihu.com/equation?tex=S%3D-mc%5Cint_%7B1%7D%5E%7B2%7Dds\" alt=\"S=-mc\\int_{1}^{2}ds\" eeimg=\"1\"/> ，其中积分沿着粒子在两个特定事件间的世界线进行， <img src=\"https://www.zhihu.com/equation?tex=ds\" alt=\"ds\" eeimg=\"1\"/> 代表元间隔。</p><p>（2）带电粒子与电磁场有相互作用，这一相互作用对应的作用量是 <img src=\"https://www.zhihu.com/equation?tex=-%5Cfrac%7Be%7D%7Bc%7D%5Cint_%7B1%7D%5E%7B2%7DA_%7Bi%7Ddx%5E%7Bi%7D\" alt=\"-\\frac{e}{c}\\int_{1}^{2}A_{i}dx^{i}\" eeimg=\"1\"/> ，这里的 <img src=\"https://www.zhihu.com/equation?tex=A_%7Bi%7D\" alt=\"A_{i}\" eeimg=\"1\"/> 是称为“四维势”的四维矢量， <img src=\"https://www.zhihu.com/equation?tex=e\" alt=\"e\" eeimg=\"1\"/> 代表粒子所带电荷量，它是一个标量。</p><p>以上两条的得到，首先是由于我们希望作用量是一个标量（从而改变坐标系的时候，物体的运动规律不发生改变，符合相对性原理）；其次是基于实验事实（比如说，在低速情形下，我们将Lagrange量对 <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7Bv%7D%7Bc%7D\" alt=\"\\frac{v}{c}\" eeimg=\"1\"/> 展开并忽略高阶项，希望得到的结果能够回归经典的分析力学）。</p><p>由此，我们知道，电磁场中电荷的作用量将会是如下形式：</p><p><img src=\"https://www.zhihu.com/equation?tex=S%3D%5Cint_%7B1%7D%5E%7B2%7D%28-mcds-%5Cfrac%7Be%7D%7Bc%7DA_%7Bi%7Ddx%5E%7Bi%7D%29.\" alt=\"S=\\int_{1}^{2}(-mcds-\\frac{e}{c}A_{i}dx^{i}).\" eeimg=\"1\"/></p><p>四维矢量 <img src=\"https://www.zhihu.com/equation?tex=A_%7Bi%7D\" alt=\"A_{i}\" eeimg=\"1\"/> 的三个空间分量组成一个三维矢量 <img src=\"https://www.zhihu.com/equation?tex=%5Ctextbf%7BA%7D\" alt=\"\\textbf{A}\" eeimg=\"1\"/> ，称为“矢势”；时间分量 <img src=\"https://www.zhihu.com/equation?tex=%5Cphi\" alt=\"\\phi\" eeimg=\"1\"/> 称为“标势”。我们希望提取出Lagrange量的具体形式，因此将作用量改写为：</p><p><img src=\"https://www.zhihu.com/equation?tex=S%3D%5Cint_%7B1%7D%5E%7B2%7D%28-mc%5E%7B2%7D%5Csqrt%7B1-%5Cfrac%7Bv%5E2%7D%7Bc%5E2%7D%7D+%2B+%5Cfrac%7Be%7D%7Bc%7D%5Ctextbf%7BA%7D%5Ccdot+%5Ctextbf%7Bv%7D-e%5Cphi+%29dt.\" alt=\"S=\\int_{1}^{2}(-mc^{2}\\sqrt{1-\\frac{v^2}{c^2}} + \\frac{e}{c}\\textbf{A}\\cdot \\textbf{v}-e\\phi )dt.\" eeimg=\"1\"/></p><p>从而Lagrange量为：</p><p><img src=\"https://www.zhihu.com/equation?tex=L%3D-mc%5E%7B2%7D%5Csqrt%7B1-%5Cfrac%7Bv%5E2%7D%7Bc%5E2%7D%7D+%2B+%5Cfrac%7Be%7D%7Bc%7D%5Ctextbf%7BA%7D%5Ccdot+%5Ctextbf%7Bv%7D-e%5Cphi.\" alt=\"L=-mc^{2}\\sqrt{1-\\frac{v^2}{c^2}} + \\frac{e}{c}\\textbf{A}\\cdot \\textbf{v}-e\\phi.\" eeimg=\"1\"/></p><p>下面我们要求出带电粒子在电磁场中的运动方程。借助力学原理，运动方程就是： <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7Bd%7D%7Bdt%7D%5Cfrac%7B%5Cpartial+L%7D%7B%5Cpartial+%5Ctextbf%7Bv%7D%7D%3D%5Cfrac%7B%5Cpartial+L%7D%7B%5Cpartial+%5Ctextbf%7Br%7D%7D\" alt=\"\\frac{d}{dt}\\frac{\\partial L}{\\partial \\textbf{v}}=\\frac{\\partial L}{\\partial \\textbf{r}}\" eeimg=\"1\"/> 。分别求导：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+L%7D%7B%5Cpartial+%5Ctextbf%7Bv%7D%7D%3D%5Cfrac%7Bm%5Ctextbf%7Bv%7D%7D%7B%5Csqrt%7B1-%5Cfrac%7Bv%5E2%7D%7Bc%5E2%7D%7D%7D+%2B+%5Cfrac%7Be%7D%7Bc%7D%5Ctextbf%7BA%7D%3D%5Ctextbf%7Bp%7D%2B%5Cfrac%7Be%7D%7Bc%7D%5Ctextbf%7BA%7D.\" alt=\"\\frac{\\partial L}{\\partial \\textbf{v}}=\\frac{m\\textbf{v}}{\\sqrt{1-\\frac{v^2}{c^2}}} + \\frac{e}{c}\\textbf{A}=\\textbf{p}+\\frac{e}{c}\\textbf{A}.\" eeimg=\"1\"/></p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+L%7D%7B%5Cpartial+%5Ctextbf%7Br%7D%7D%3D%5Cfrac%7Be%7D%7Bc%7D%5Cbigtriangledown%28%5Ctextbf%7BA%7D%5Ccdot+%5Ctextbf%7Bv%7D%29-e%5Cbigtriangledown+%5Cphi%3D%5Cfrac%7Be%7D%7Bc%7D%28%5Ctextbf%7Bv%7D%5Ccdot+%5Cbigtriangledown%29%5Ctextbf%7BA%7D%2B%5Cfrac%7Be%7D%7Bc%7D%5Ctextbf%7Bv%7D%5Ctimes+rot%5Ctextbf%7BA%7D-e%5Cbigtriangledown+%5Cphi.\" alt=\"\\frac{\\partial L}{\\partial \\textbf{r}}=\\frac{e}{c}\\bigtriangledown(\\textbf{A}\\cdot \\textbf{v})-e\\bigtriangledown \\phi=\\frac{e}{c}(\\textbf{v}\\cdot \\bigtriangledown)\\textbf{A}+\\frac{e}{c}\\textbf{v}\\times rot\\textbf{A}-e\\bigtriangledown \\phi.\" eeimg=\"1\"/></p><p>因此Lagrange方程具有如下形式：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7Bd%7D%7Bdt%7D%28%5Ctextbf%7Bp%7D%2B%5Cfrac%7Be%7D%7Bc%7D%5Ctextbf%7BA%7D%29%3D%5Cfrac%7Be%7D%7Bc%7D%28%5Ctextbf%7Bv%7D%5Ccdot+%5Cbigtriangledown%29%5Ctextbf%7BA%7D%2B%5Cfrac%7Be%7D%7Bc%7D%5Ctextbf%7Bv%7D%5Ctimes+rot%5Ctextbf%7BA%7D-e%5Cbigtriangledown+%5Cphi.\" alt=\"\\frac{d}{dt}(\\textbf{p}+\\frac{e}{c}\\textbf{A})=\\frac{e}{c}(\\textbf{v}\\cdot \\bigtriangledown)\\textbf{A}+\\frac{e}{c}\\textbf{v}\\times rot\\textbf{A}-e\\bigtriangledown \\phi.\" eeimg=\"1\"/></p><p>矢势对进行微分时，需要注意其微分包含两部分——一部分对应场本身随时间变化（偏导数 <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+%5Ctextbf%7BA%7D%7D%7B%5Cpartial+t%7D\" alt=\"\\frac{\\partial \\textbf{A}}{\\partial t}\" eeimg=\"1\"/> ），另一部分（ <img src=\"https://www.zhihu.com/equation?tex=%5Ctextbf%7Bv%7D%5Ccdot+%5Cbigtriangledown+%5Ctextbf%7BA%7D\" alt=\"\\textbf{v}\\cdot \\bigtriangledown \\textbf{A}\" eeimg=\"1\"/> ）对应由于粒子位置改变而导致粒子所在处 <img src=\"https://www.zhihu.com/equation?tex=%5Ctextbf%7BA%7D\" alt=\"\\textbf{A}\" eeimg=\"1\"/> 取值的变化。代入方程，稍加化简不难得到：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7Bd%5Ctextbf%7Bp%7D%7D%7Bdt%7D%3D-%5Cfrac%7Be%7D%7Bc%7D%5Cfrac%7B%5Cpartial+%5Ctextbf%7BA%7D%7D%7B%5Cpartial+t%7D-e%5Cbigtriangledown%5Cphi+%2B+%5Cfrac%7Be%7D%7Bc%7D%5Ctextbf%7Bv%7D%5Ctimes+rot%5Ctextbf%7BA%7D.\" alt=\"\\frac{d\\textbf{p}}{dt}=-\\frac{e}{c}\\frac{\\partial \\textbf{A}}{\\partial t}-e\\bigtriangledown\\phi + \\frac{e}{c}\\textbf{v}\\times rot\\textbf{A}.\" eeimg=\"1\"/></p><p>左边是粒子动量的变化率，右边（从经典力学的角度来看）就是粒子所受的力。我们把“力”中与粒子速度无关的部分定义为“电场力”，将与粒子速度有关的部分定义为“磁场力”。这就是场论中电场强度和磁场强度的由来。</p><p>电场强度： <img src=\"https://www.zhihu.com/equation?tex=%5Ctextbf%7BE%7D%3D-%5Cfrac%7B1%7D%7Bc%7D%5Cfrac%7B%5Cpartial+%5Ctextbf%7BA%7D%7D%7B%5Cpartial+t%7D-%5Cbigtriangledown+%5Cphi.\" alt=\"\\textbf{E}=-\\frac{1}{c}\\frac{\\partial \\textbf{A}}{\\partial t}-\\bigtriangledown \\phi.\" eeimg=\"1\"/></p><p>磁场强度：  <img src=\"https://www.zhihu.com/equation?tex=+%5Ctextbf%7BH%7D%3Drot%5Ctextbf%7BA%7D.\" alt=\" \\textbf{H}=rot\\textbf{A}.\" eeimg=\"1\"/></p><p>以上借助最小作用量原理，变分得到了带电粒子在电磁场中的运动方程。未来我们将会不断看到，分析力学中的原理并不仅仅在纯力学领域有用（预习场论有感...扯远了...）。</p><p>有了粒子在电磁场中运动的方程，我们就可以针对许多特殊的情形来求解这个方程。</p><p>需要注意的是有的书可能记号或单位制与这里用的不一样...这种方法得到的电场强度和磁场强度量纲相同，不同于经典电磁学中的结果，不过单位制的选取其实并不是我们关注的重点。</p><p>（以上内容主要参考朗道《场论》...只是因为上学期理论力学里恰好学过这部分内容所以才写一下...）</p>", 
            "topic": [
                {
                    "tag": "电磁学", 
                    "tagLink": "https://api.zhihu.com/topics/19589402"
                }, 
                {
                    "tag": "电动力学", 
                    "tagLink": "https://api.zhihu.com/topics/19667158"
                }, 
                {
                    "tag": "理论力学", 
                    "tagLink": "https://api.zhihu.com/topics/19731957"
                }
            ], 
            "comments": [
                {
                    "userName": "言志青", 
                    "userLink": "https://www.zhihu.com/people/62db6c8184daa0b7196972baa8faf604", 
                    "content": "周神太强了", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "周清逸", 
                            "userLink": "https://www.zhihu.com/people/23dd9239a500559d8850eac45ed2038f", 
                            "content": "高超！", 
                            "likes": 0, 
                            "replyToAuthor": "言志青"
                        }, 
                        {
                            "userName": "言志青", 
                            "userLink": "https://www.zhihu.com/people/62db6c8184daa0b7196972baa8faf604", 
                            "content": "早看到这篇文章电动就不会跪了。。。", 
                            "likes": 0, 
                            "replyToAuthor": "周清逸"
                        }
                    ]
                }, 
                {
                    "userName": "知乎用户", 
                    "userLink": "https://www.zhihu.com/people/0", 
                    "content": "<p>周神太强了</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "周清逸", 
                            "userLink": "https://www.zhihu.com/people/23dd9239a500559d8850eac45ed2038f", 
                            "content": "韩神orz", 
                            "likes": 0, 
                            "replyToAuthor": "知乎用户"
                        }
                    ]
                }, 
                {
                    "userName": "唐子骞", 
                    "userLink": "https://www.zhihu.com/people/6862a5ebef6b345a2db0c01845956ed7", 
                    "content": "周神太强了", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "周清逸", 
                            "userLink": "https://www.zhihu.com/people/23dd9239a500559d8850eac45ed2038f", 
                            "content": "瑟瑟发抖………QAQ", 
                            "likes": 0, 
                            "replyToAuthor": "唐子骞"
                        }
                    ]
                }, 
                {
                    "userName": "张文韬", 
                    "userLink": "https://www.zhihu.com/people/580c8db6e88bd6dc922400359f96de42", 
                    "content": "周神太强了", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/27603079", 
            "userName": "周清逸", 
            "userLink": "https://www.zhihu.com/people/23dd9239a500559d8850eac45ed2038f", 
            "upvote": 6, 
            "title": "Hamilton-Jacobi方程", 
            "content": "上一篇文章中，我们介绍了正则变换。正则变换的一个重要应用是导出所谓的Hamilton-Jacobi方程。<p>首先我们知道，一个力学体系的作用量可以如下表示：<img src=\"https://www.zhihu.com/equation?tex=S%3D%5Cint+_%7B1%7D%5E2+Ldt%3D%5Cint+_%7B1%7D%5E2+%28%5Cdot%7Bq_%7B%5Calpha%7D%7Dp_%7B%5Calpha%7D-H%29dt.\" alt=\"S=\\int _{1}^2 Ldt=\\int _{1}^2 (\\dot{q_{\\alpha}}p_{\\alpha}-H)dt.\" eeimg=\"1\"/>从而我们可以得到：<img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+S%7D%7B%5Cpartial+t%7D%2BH%28q%2Cp%2Ct%29%3D0.\" alt=\"\\frac{\\partial S}{\\partial t}+H(q,p,t)=0.\" eeimg=\"1\"/> 以及 <img src=\"https://www.zhihu.com/equation?tex=p_%7B%5Calpha%7D%3D%5Cfrac%7B%5Cpartial+S%7D%7B%5Cpartial+q_%7B%5Calpha%7D%7D.\" alt=\"p_{\\alpha}=\\frac{\\partial S}{\\partial q_{\\alpha}}.\" eeimg=\"1\"/> 将所有广义动量消去，可以写出方程的最原始形式：<img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+S%7D%7B%5Cpartial+t%7D%2BH%28q_%7B1%7D%2C...q_%7Bs%7D%2C%5Cfrac%7B%5Cpartial+S%7D%7B%5Cpartial+q_%7B1%7D%7D%2C...%2C%5Cfrac%7B%5Cpartial+S%7D%7B%5Cpartial+q_%7Bs%7D%7D%2Ct%29%3D0.\" alt=\"\\frac{\\partial S}{\\partial t}+H(q_{1},...q_{s},\\frac{\\partial S}{\\partial q_{1}},...,\\frac{\\partial S}{\\partial q_{s}},t)=0.\" eeimg=\"1\"/>并且希望能够从这个方程出发，解出S的形式（原则上是可以解出来的，只不过求解过程一般会比较复杂）：<img src=\"https://www.zhihu.com/equation?tex=S%3Df%28t%2Cq_%7B1%7D%2C...%2Cq_%7Bs%7D%3B%5Calpha_%7B1%7D%2C...%2C%5Calpha_%7Bs%7D%29%2BA.%0A\" alt=\"S=f(t,q_{1},...,q_{s};\\alpha_{1},...,\\alpha_{s})+A.\n\" eeimg=\"1\"/> 其中<img src=\"https://www.zhihu.com/equation?tex=%5Calpha_%7B1%7D%2C...%2C%5Calpha_%7Bs%7D%2CA\" alt=\"\\alpha_{1},...,\\alpha_{s},A\" eeimg=\"1\"/>是常数。之所以写成这样的形式，是由于原方程中独立变量的数目有s+1个。</p><p>这时候，我们突发奇想：可以把<img src=\"https://www.zhihu.com/equation?tex=%5Calpha_%7B1%7D%2C...%2C%5Calpha_%7Bs%7D\" alt=\"\\alpha_{1},...,\\alpha_{s}\" eeimg=\"1\"/>看作变量（或者更进一步，看作某种形式的广义动量），将<img src=\"https://www.zhihu.com/equation?tex=f%28t%2Cq%2C%5Calpha%29\" alt=\"f(t,q,\\alpha)\" eeimg=\"1\"/>作为生成函数，进行正则变换。我们来看一下会得到怎样的结果。</p><p><img src=\"https://www.zhihu.com/equation?tex=df%28t%2Cq%2C%5Calpha%29%3Dp_%7Bi%7Ddq_%7Bi%7D+%2B+%5Cbeta_%7Bi%7D+d%5Calpha_%7Bi%7D+%2B%28H%27-H%29dt.\" alt=\"df(t,q,\\alpha)=p_{i}dq_{i} + \\beta_{i} d\\alpha_{i} +(H&#39;-H)dt.\" eeimg=\"1\"/>妙处已经很明显了，我们还是把偏导按部就班地写出来：<img src=\"https://www.zhihu.com/equation?tex=p_%7Bi%7D%3D%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+q_%7Bi%7D%7D%2C%5Cbeta+_%7Bi%7D%3D%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+%5Calpha_%7Bi%7D%7D%2C+H%27%3DH%2B%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+t%7D.\" alt=\"p_{i}=\\frac{\\partial f}{\\partial q_{i}},\\beta _{i}=\\frac{\\partial f}{\\partial \\alpha_{i}}, H&#39;=H+\\frac{\\partial f}{\\partial t}.\" eeimg=\"1\"/><br/></p><p>从第三个式子和<img src=\"https://www.zhihu.com/equation?tex=S%3Df%28t%2Cq%2C%5Calpha%29\" alt=\"S=f(t,q,\\alpha)\" eeimg=\"1\"/>的关系可以看出<img src=\"https://www.zhihu.com/equation?tex=H%27%3D0%5CRightarrow+%5Cdot%7B%5Calpha_%7Bi%7D%7D%3D-%5Cfrac%7B%5Cpartial+H%27%7D%7B%5Cpartial+%5Cbeta_%7Bi%7D%7D%3D0%2C+%5Cdot%7B%5Cbeta_%7Bi%7D%7D%3D%5Cfrac%7B%5Cpartial+H%27%7D%7B%5Cpartial+%5Calpha_%7Bi%7D%7D%3D0.\" alt=\"H&#39;=0\\Rightarrow \\dot{\\alpha_{i}}=-\\frac{\\partial H&#39;}{\\partial \\beta_{i}}=0, \\dot{\\beta_{i}}=\\frac{\\partial H&#39;}{\\partial \\alpha_{i}}=0.\" eeimg=\"1\"/> 显然，经过正则变换之后，新的广义坐标、广义动量都是不随时间变化的常量！</p><p>鉴于<img src=\"https://www.zhihu.com/equation?tex=%5Cbeta+_%7Bi%7D%3D%5Cfrac%7B%5Cpartial+f%7D%7B%5Cpartial+%5Calpha_%7Bi%7D%7D\" alt=\"\\beta _{i}=\\frac{\\partial f}{\\partial \\alpha_{i}}\" eeimg=\"1\"/>，并且<img src=\"https://www.zhihu.com/equation?tex=f%28t%2Cq%2C%5Calpha%29\" alt=\"f(t,q,\\alpha)\" eeimg=\"1\"/>的形式我们已经解出来了，考虑到新的广义坐标、广义动量不随时间变化，可以把旧的广义坐标反解出来，用时间和2s个常数表示。</p><p>至此，利用Hamilton-Jacobi方程解决问题的基本步骤就完成了。这一过程当中，比较困难的一步显然是求S（只要求得了作用量的函数形式，后面的步骤都顺理成章）。为了简化方程<img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+S%7D%7B%5Cpartial+t%7D%2BH%28q_%7B1%7D%2C...q_%7Bs%7D%2C%5Cfrac%7B%5Cpartial+S%7D%7B%5Cpartial+q_%7B1%7D%7D%2C...%2C%5Cfrac%7B%5Cpartial+S%7D%7B%5Cpartial+q_%7Bs%7D%7D%2Ct%29%3D0\" alt=\"\\frac{\\partial S}{\\partial t}+H(q_{1},...q_{s},\\frac{\\partial S}{\\partial q_{1}},...,\\frac{\\partial S}{\\partial q_{s}},t)=0\" eeimg=\"1\"/> 的求解过程，我们可以采用“分离变量法”，具体过程如下：</p><p>如果某一个广义坐标（记作<img src=\"https://www.zhihu.com/equation?tex=q_%7B1%7D\" alt=\"q_{1}\" eeimg=\"1\"/>）和它的导数在方程中总是“结对出现“，也就是说，总是以函数<img src=\"https://www.zhihu.com/equation?tex=%5Cphi%28q_%7B1%7D%2C%5Cfrac%7B%5Cpartial+S%7D%7B%5Cpartial+q_%7B1%7D%7D%29\" alt=\"\\phi(q_{1},\\frac{\\partial S}{\\partial q_{1}})\" eeimg=\"1\"/>的形式出现：<img src=\"https://www.zhihu.com/equation?tex=%5CPhi%0A%5C%7B+%0Aq_%7Bi%7D%2C+t%2C+%5Cfrac%7B%5Cpartial+S%7D%7B%5Cpartial+q_%7Bi%7D%7D%2C+%5Cfrac%7B%5Cpartial+S%7D%7B%5Cpartial+t%7D%2C%0A%5Cphi%28q_%7B1%7D%2C%5Cfrac%7B%5Cpartial+S%7D%7B%5Cpartial+q_%7B1%7D%7D%29%0A+%5C%7D%3D0.\" alt=\"\\Phi\n\\{ \nq_{i}, t, \\frac{\\partial S}{\\partial q_{i}}, \\frac{\\partial S}{\\partial t},\n\\phi(q_{1},\\frac{\\partial S}{\\partial q_{1}})\n \\}=0.\" eeimg=\"1\"/> 这时我们可以直接分离变量：设解<img src=\"https://www.zhihu.com/equation?tex=S%3DS%27%28q_%7Bi%7D%2Ct%29%2BS_%7B1%7D%28q_%7B1%7D%29.\" alt=\"S=S&#39;(q_{i},t)+S_{1}(q_{1}).\" eeimg=\"1\"/> 方程变为：<img src=\"https://www.zhihu.com/equation?tex=%5CPhi%0A%5C%7B+%0Aq_%7Bi%7D%2C+t%2C+%5Cfrac%7B%5Cpartial+S%27%7D%7B%5Cpartial+q_%7Bi%7D%7D%2C+%5Cfrac%7B%5Cpartial+S%27%7D%7B%5Cpartial+t%7D%2C%0A%5Cphi%28q_%7B1%7D%2C%5Cfrac%7B%5Cpartial+S_%7B1%7D%7D%7B%5Cpartial+q_%7B1%7D%7D%29%0A+%5C%7D%3D0.\" alt=\"\\Phi\n\\{ \nq_{i}, t, \\frac{\\partial S&#39;}{\\partial q_{i}}, \\frac{\\partial S&#39;}{\\partial t},\n\\phi(q_{1},\\frac{\\partial S_{1}}{\\partial q_{1}})\n \\}=0.\" eeimg=\"1\"/> 进而可以实现变量的分离：<img src=\"https://www.zhihu.com/equation?tex=%5CPhi%0A%5C%7B+%0Aq_%7Bi%7D%2C+t%2C+%5Cfrac%7B%5Cpartial+S%27%7D%7B%5Cpartial+q_%7Bi%7D%7D%2C+%5Cfrac%7B%5Cpartial+S%27%7D%7B%5Cpartial+t%7D%2C%0A%5Calpha_%7B1%7D%0A%5C%7D%3D0.\" alt=\"\\Phi\n\\{ \nq_{i}, t, \\frac{\\partial S&#39;}{\\partial q_{i}}, \\frac{\\partial S&#39;}{\\partial t},\n\\alpha_{1}\n\\}=0.\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5Cphi%28q_%7B1%7D%2C%5Cfrac%7B%5Cpartial+S_%7B1%7D%7D%7B%5Cpartial+q_%7B1%7D%7D%29%3D%5Calpha_%7B1%7D.%0A+\" alt=\"\\phi(q_{1},\\frac{\\partial S_{1}}{\\partial q_{1}})=\\alpha_{1}.\n \" eeimg=\"1\"/> 简化了方程！</p><p>下面我们看一个具体的栗子。</p><p>在重力场中抛出质点，广义坐标取为x,y（g沿-y方向），用Hamilton-Jacobi方程解质点的运动。<br/></p><p>首先写出Hamilton量：<img src=\"https://www.zhihu.com/equation?tex=H%3Dmgy%2B%5Cfrac%7B1%7D%7B2m%7D%5B%28%5Cfrac%7B%5Cpartial+S%7D%7B%5Cpartial+x%7D%29%5E2+%2B+%28%5Cfrac%7B%5Cpartial+S%7D%7B%5Cpartial+y%7D%29%5E2%5D.\" alt=\"H=mgy+\\frac{1}{2m}[(\\frac{\\partial S}{\\partial x})^2 + (\\frac{\\partial S}{\\partial y})^2].\" eeimg=\"1\"/></p><p>对应的方程是：<img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+S%7D%7B%5Cpartial+t%7D%2B%5Bmgy%2B%5Cfrac%7B1%7D%7B2m%7D%28%5Cfrac%7B%5Cpartial+S%7D%7B%5Cpartial+y%7D%29%5E2%5D+%2B%5Cfrac%7B1%7D%7B2m%7D+%28%5Cfrac%7B%5Cpartial+S%7D%7B%5Cpartial+x%7D%29%5E2%3D0.\" alt=\"\\frac{\\partial S}{\\partial t}+[mgy+\\frac{1}{2m}(\\frac{\\partial S}{\\partial y})^2] +\\frac{1}{2m} (\\frac{\\partial S}{\\partial x})^2=0.\" eeimg=\"1\"/> 第一次分离：<img src=\"https://www.zhihu.com/equation?tex=S%3DS%27%2BS_%7B1%7D%28y%29%5CRightarrow+mgy%2B%5Cfrac%7B1%7D%7B2m%7D%28%5Cfrac%7B%5Cpartial+S_%7B1%7D%7D%7B%5Cpartial+y%7D%29%5E2%3D%5Calpha_%7B1%7D%2C+%5Cfrac%7B%5Cpartial+S%27%7D%7B%5Cpartial+t%7D+%2B%5Calpha_%7B1%7D+%2B+%5Cfrac%7B1%7D%7B2m%7D%28%5Cfrac%7B%5Cpartial+S%27%7D%7B%5Cpartial+x%7D%29%5E2%3D0.\" alt=\"S=S&#39;+S_{1}(y)\\Rightarrow mgy+\\frac{1}{2m}(\\frac{\\partial S_{1}}{\\partial y})^2=\\alpha_{1}, \\frac{\\partial S&#39;}{\\partial t} +\\alpha_{1} + \\frac{1}{2m}(\\frac{\\partial S&#39;}{\\partial x})^2=0.\" eeimg=\"1\"/></p><p>第二次分离：<img src=\"https://www.zhihu.com/equation?tex=S%27%3DS%27%27%2BS_%7B2%7D%28x%29%5CRightarrow+%5Cfrac%7B1%7D%7B2m%7D%28%5Cfrac%7B%5Cpartial+S_%7B2%7D%7D%7B%5Cpartial+x%7D%29%5E2%3D%5Calpha_%7B2%7D%2C+%5Cfrac%7B%5Cpartial+S%27%27%7D%7B%5Cpartial+t%7D+%2B%5Calpha_%7B1%7D+%2B+%5Calpha_%7B2%7D%3D0.\" alt=\"S&#39;=S&#39;&#39;+S_{2}(x)\\Rightarrow \\frac{1}{2m}(\\frac{\\partial S_{2}}{\\partial x})^2=\\alpha_{2}, \\frac{\\partial S&#39;&#39;}{\\partial t} +\\alpha_{1} + \\alpha_{2}=0.\" eeimg=\"1\"/></p><p>考虑到<img src=\"https://www.zhihu.com/equation?tex=%5Calpha_%7B1%7D%2C+%5Calpha_%7B2%7D\" alt=\"\\alpha_{1}, \\alpha_{2}\" eeimg=\"1\"/>都是常量，可以通过简单的积分解出：<img src=\"https://www.zhihu.com/equation?tex=S%28x%2Cy%2Ct%29%3D-%5Cfrac%7B2%7D%7B3g%7D%5Csqrt%7B%5Cfrac%7B2%7D%7Bm%7D%7D%28%5Calpha_%7B1%7D-mgy%29%5E%5Cfrac%7B3%7D%7B2%7D+%2B+%5Csqrt%7B2m%5Calpha_%7B2%7D%7Dx-%28%5Calpha_%7B1%7D%2B%5Calpha_%7B2%7D%29t%2BA.\" alt=\"S(x,y,t)=-\\frac{2}{3g}\\sqrt{\\frac{2}{m}}(\\alpha_{1}-mgy)^\\frac{3}{2} + \\sqrt{2m\\alpha_{2}}x-(\\alpha_{1}+\\alpha_{2})t+A.\" eeimg=\"1\"/> 再求导计算<img src=\"https://www.zhihu.com/equation?tex=%5Cbeta\" alt=\"\\beta\" eeimg=\"1\"/>就可以得到：<img src=\"https://www.zhihu.com/equation?tex=%5Cbeta_%7B1%7D%3D%5Cfrac%7B%5Cpartial+S%7D%7B%5Cpartial+%5Calpha_%7B1%7D%7D%3D-%5Cfrac%7B1%7D%7Bg%7D%5Csqrt%7B%5Cfrac%7B2%7D%7Bm%7D%7D%28+%5Calpha_%7B1%7D-mgy+%29%5E%5Cfrac%7B1%7D%7B2%7D-t.\" alt=\"\\beta_{1}=\\frac{\\partial S}{\\partial \\alpha_{1}}=-\\frac{1}{g}\\sqrt{\\frac{2}{m}}( \\alpha_{1}-mgy )^\\frac{1}{2}-t.\" eeimg=\"1\"/> 以及 <img src=\"https://www.zhihu.com/equation?tex=%5Cbeta_%7B2%7D%3D%5Cfrac%7B%5Cpartial+S%7D%7B%5Cpartial+%5Calpha_%7B2%7D%7D%3D%5Cfrac%7Bmx%7D%7B%5Csqrt%7B2m%5Calpha_%7B2%7D%7D%7D-t.\" alt=\"\\beta_{2}=\\frac{\\partial S}{\\partial \\alpha_{2}}=\\frac{mx}{\\sqrt{2m\\alpha_{2}}}-t.\" eeimg=\"1\"/></p><p>求解运动，希望知道广义坐标随时间的变化关系。从<img src=\"https://www.zhihu.com/equation?tex=%5Cbeta_%7B1%7D%2C+%5Cbeta_%7B2%7D\" alt=\"\\beta_{1}, \\beta_{2}\" eeimg=\"1\"/>的表达式反解x和y得到：</p><p><img src=\"https://www.zhihu.com/equation?tex=x%3D%28%5Cbeta_%7B2%7D%2Bt%29%5Csqrt%7B%5Cfrac%7B2%5Calpha_%7B2%7D%7D%7Bm%7D%7D%2C+y%3D%5Cfrac%7B%5Calpha_%7B1%7D%7D%7Bmg%7D-%5Cfrac%7Bg%7D%7B2%7D%28%5Cbeta_%7B1%7D%2Bt%29%5E2.\" alt=\"x=(\\beta_{2}+t)\\sqrt{\\frac{2\\alpha_{2}}{m}}, y=\\frac{\\alpha_{1}}{mg}-\\frac{g}{2}(\\beta_{1}+t)^2.\" eeimg=\"1\"/> 至此问题还没有结束——要求得完整的解，还需要将参数<img src=\"https://www.zhihu.com/equation?tex=%5Calpha%2C%5Cbeta\" alt=\"\\alpha,\\beta\" eeimg=\"1\"/>用初始条件<img src=\"https://www.zhihu.com/equation?tex=x_%7B0%7D%2Cy_%7B0%7D%2Cv_%7Bx0%7D%2Cv_%7By0%7D\" alt=\"x_{0},y_{0},v_{x0},v_{y0}\" eeimg=\"1\"/>表示。最终得到的结果应当与直接用牛二定律计算出的结果完全相同（这部分计算很简单就不写了...）。<br/></p><p>值得说明的是，求解力学问题时，Hamilton-Jacobi方程与基于Hamilton量的Hamilton正则方程、基于Lagrange量的Lagrange方程是完全等价的。正如大家所见，解Hamilton-Jacobi方程的过程往往显得比较繁杂。为了简化方程，往往需要分离变量，从而在描述力学问题时，根据问题本身的特性选取合适的坐标是十分关键的。</p>", 
            "topic": [
                {
                    "tag": "物理学", 
                    "tagLink": "https://api.zhihu.com/topics/19556950"
                }, 
                {
                    "tag": "力学", 
                    "tagLink": "https://api.zhihu.com/topics/19615099"
                }, 
                {
                    "tag": "分析力学", 
                    "tagLink": "https://api.zhihu.com/topics/20001695"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/27529375", 
            "userName": "周清逸", 
            "userLink": "https://www.zhihu.com/people/23dd9239a500559d8850eac45ed2038f", 
            "upvote": 19, 
            "title": "正则变换", 
            "content": "我们知道，对于一个力学体系，如果可以写出体系的哈密顿函数：<img src=\"https://www.zhihu.com/equation?tex=H%28q%2Cp%2Ct%29%3D%5CSigma+p_%7Bi%7D%5Cdot%7Bq_%7Bi%7D%7D+-L\" alt=\"H(q,p,t)=\\Sigma p_{i}\\dot{q_{i}} -L\" eeimg=\"1\"/>，那么就可以通过解正则方程<img src=\"https://www.zhihu.com/equation?tex=%5Cdot%7Bq_%7Bi%7D%7D%3D%5Cfrac%7B%5Cpartial+H%7D%7B%5Cpartial+p_%7Bi%7D%7D%2C+%5Cdot%7Bp_%7Bi%7D%7D%3D-%5Cfrac%7B%5Cpartial+H%7D%7B%5Cpartial+q_%7Bi%7D%7D\" alt=\"\\dot{q_{i}}=\\frac{\\partial H}{\\partial p_{i}}, \\dot{p_{i}}=-\\frac{\\partial H}{\\partial q_{i}}\" eeimg=\"1\"/>，来求得系统的演化。<p>有时，我们会希望把独立变量p、q变换为另一组变量P、Q：<img src=\"https://www.zhihu.com/equation?tex=Q_%7Bi%7D%3DQ_%7Bi%7D%28q%2Cp%2Ct%29%2C+P_%7Bi%7D%3DP_%7Bi%7D%28q%2Cp%2Ct%29.\" alt=\"Q_{i}=Q_{i}(q,p,t), P_{i}=P_{i}(q,p,t).\" eeimg=\"1\"/>然而，并不是说我们随手写一个变换关系，运动方程（用新的变量表示）都能保持正则方程的形式（也就是说，我们希望能找到一个函数K，使新的运动方程能够写成：<img src=\"https://www.zhihu.com/equation?tex=%5Cdot%7BQ_%7Bi%7D%7D%3D%5Cfrac%7B%5Cpartial+K%7D%7B%5Cpartial+P_%7Bi%7D%7D%2C%5Cdot%7BP_%7Bi%7D%7D%3D-%5Cfrac%7B%5Cpartial+K%7D%7B%5Cpartial+Q_%7Bi%7D%7D.\" alt=\"\\dot{Q_{i}}=\\frac{\\partial K}{\\partial P_{i}},\\dot{P_{i}}=-\\frac{\\partial K}{\\partial Q_{i}}.\" eeimg=\"1\"/>的形式）。</p><p>那么我们就来探究什么样的变换能够保持运动方程的形式~这样的变换，称为“正则变换”。</p><p>根据最小作用量原理（又称作哈密顿原理），已知<img src=\"https://www.zhihu.com/equation?tex=%5Cdelta+%5Cint_%7B%7D%5E%7B%7D+%28%5Csum_%7Bi%7Dp_%7Bi%7Ddq_%7Bi%7D-Hdt%29%3D0.\" alt=\"\\delta \\int_{}^{} (\\sum_{i}p_{i}dq_{i}-Hdt)=0.\" eeimg=\"1\"/>成立，并且希望下式也成立：<img src=\"https://www.zhihu.com/equation?tex=%5Cdelta+%5Cint_%7B%7D%5E%7B%7D+%28%5Csum_%7Bi%7DP_%7Bi%7DdQ_%7Bi%7D+-Kdt%29%3D0.\" alt=\"\\delta \\int_{}^{} (\\sum_{i}P_{i}dQ_{i} -Kdt)=0.\" eeimg=\"1\"/></p><p>根据变分原理，如果两个式子中，被积分的部分仅仅相差一个函数F的全微分（F是广义坐标、广义动量和时间的函数），那么这一项对应的积分值是常数，变分自然为0，从而运动方程的美丽形式得到了保留！</p><p>所以，我们取：<img src=\"https://www.zhihu.com/equation?tex=%5Csum_%7Bi%7D+p_%7Bi%7Ddq_%7Bi%7D-Hdt%3D%5Csum_%7Bi%7DP_%7Bi%7DdQ_%7Bi%7D-Kdt%2BdF.\" alt=\"\\sum_{i} p_{i}dq_{i}-Hdt=\\sum_{i}P_{i}dQ_{i}-Kdt+dF.\" eeimg=\"1\"/></p><p>满足这个关系的变换就是正则变换。</p><p>上面的关系可以改造为：<img src=\"https://www.zhihu.com/equation?tex=dF%3D%5Csum_%7Bi%7D+p_%7Bi%7Ddq_%7Bi%7D-%5Csum_%7Bi%7DP_%7Bi%7DdQ_%7Bi%7D%2B%28K-H%29dt.\" alt=\"dF=\\sum_{i} p_{i}dq_{i}-\\sum_{i}P_{i}dQ_{i}+(K-H)dt.\" eeimg=\"1\"/></p><p>进而能够看出关系：<img src=\"https://www.zhihu.com/equation?tex=p_%7Bi%7D%3D%5Cfrac%7B%5Cpartial+F%7D%7B%5Cpartial+q_%7Bi%7D%7D%2CP_%7Bi%7D%3D-%5Cfrac%7B%5Cpartial+F%7D%7B%5Cpartial+Q_%7Bi%7D%7D%2CK%3DH%2B%5Cfrac%7B%5Cpartial+F%7D%7B%5Cpartial+t%7D.\" alt=\"p_{i}=\\frac{\\partial F}{\\partial q_{i}},P_{i}=-\\frac{\\partial F}{\\partial Q_{i}},K=H+\\frac{\\partial F}{\\partial t}.\" eeimg=\"1\"/></p><p>经过仔细的考虑，不难意识到，函数F（称之为“生成函数”）与正则变换 是对应的。如果给定一个生成函数F，就能求得正则变换的形式。具体来说，首先借助方程<img src=\"https://www.zhihu.com/equation?tex=p_%7Bi%7D%3D%5Cfrac%7B%5Cpartial+F%7D%7B%5Cpartial+q_%7Bi%7D%7D\" alt=\"p_{i}=\\frac{\\partial F}{\\partial q_{i}}\" eeimg=\"1\"/>反解出Q(q,p,t)，得出新的广义坐标与老坐标、老动量的关系；再将这个关系代入<img src=\"https://www.zhihu.com/equation?tex=P_%7Bi%7D%3D-%5Cfrac%7B%5Cpartial+F%7D%7B%5Cpartial+Q_%7Bi%7D%7D\" alt=\"P_{i}=-\\frac{\\partial F}{\\partial Q_{i}}\" eeimg=\"1\"/>，轻松得到新动量与老坐标、老动量的关系。</p><p>反过来说，根据给定的正则变换关系式，也可以推出生成函数F的具体表达式。借助变换关系，将p、P（作为q、Q、t的函数）写出，然后根据偏微分关系：<img src=\"https://www.zhihu.com/equation?tex=p_%7Bi%7D%3D%5Cfrac%7B%5Cpartial+F%7D%7B%5Cpartial+q_%7Bi%7D%7D%2CP_%7Bi%7D%3D-%5Cfrac%7B%5Cpartial+F%7D%7B%5Cpartial+Q_%7Bi%7D%7D%2CK%3DH%2B%5Cfrac%7B%5Cpartial+F%7D%7B%5Cpartial+t%7D.\" alt=\"p_{i}=\\frac{\\partial F}{\\partial q_{i}},P_{i}=-\\frac{\\partial F}{\\partial Q_{i}},K=H+\\frac{\\partial F}{\\partial t}.\" eeimg=\"1\"/>原则上就可以积分得到F。当然咯，这个积分的过程可能会不太好搞...如果退一步，只需要验证某一个变换是不是正则变换，那只需要根据变换方程写出P、p的具体形式（q、Q、t的函数），将P对q求导，将p对Q求导，比较导数是否相等，就可以了（F的二阶导可交换顺序）。</p><p>有的时候，像上面那样采用q、Q表示生成函数可能会显得不方便。怎么办呢？我们可以直接用Legendre变换，把独立变量“改”一下。举个栗子，如果我们想用q和P表示生成函数，只需要将之前的公式<img src=\"https://www.zhihu.com/equation?tex=dF%3D%5Csum_%7Bi%7D+p_%7Bi%7Ddq_%7Bi%7D-%5Csum_%7Bi%7DP_%7Bi%7DdQ_%7Bi%7D%2B%28K-H%29dt.\" alt=\"dF=\\sum_{i} p_{i}dq_{i}-\\sum_{i}P_{i}dQ_{i}+(K-H)dt.\" eeimg=\"1\"/>改写为：</p><p><img src=\"https://www.zhihu.com/equation?tex=d%28F%2B%5Csum_%7Bi%7DP_%7Bi%7DQ_%7Bi%7D%29%3D%5Csum_%7Bi%7D+p_%7Bi%7Ddq_%7Bi%7D%2B%5Csum_%7Bi%7DQ_%7Bi%7DdP_%7Bi%7D%2B%28K-H%29dt.\" alt=\"d(F+\\sum_{i}P_{i}Q_{i})=\\sum_{i} p_{i}dq_{i}+\\sum_{i}Q_{i}dP_{i}+(K-H)dt.\" eeimg=\"1\"/>左边的部分就是新的生成函数的微分，我们把这个函数记为<img src=\"https://www.zhihu.com/equation?tex=%5CPhi+\" alt=\"\\Phi \" eeimg=\"1\"/>，就有关系：<br/></p><p><img src=\"https://www.zhihu.com/equation?tex=p_%7Bi%7D%3D%5Cfrac%7B%5Cpartial+%5CPhi%7D%7B%5Cpartial+q_%7Bi%7D%7D%2CQ_%7Bi%7D%3D%5Cfrac%7B%5Cpartial+%5CPhi%7D%7B%5Cpartial+P_%7Bi%7D%7D%2CK%3DH%2B%5Cfrac%7B%5Cpartial+%5CPhi%7D%7B%5Cpartial+t%7D.\" alt=\"p_{i}=\\frac{\\partial \\Phi}{\\partial q_{i}},Q_{i}=\\frac{\\partial \\Phi}{\\partial P_{i}},K=H+\\frac{\\partial \\Phi}{\\partial t}.\" eeimg=\"1\"/>（同样道理，我们可以在独立变量取作p、Q或者p、P的情况下，计算出类似的变换形式）。</p><p>正则变换有很多用处，其中之一就是用于导出另一种运动方程——Hamilton-Jacobi方程。</p>", 
            "topic": [
                {
                    "tag": "物理学", 
                    "tagLink": "https://api.zhihu.com/topics/19556950"
                }, 
                {
                    "tag": "力学", 
                    "tagLink": "https://api.zhihu.com/topics/19615099"
                }
            ], 
            "comments": [
                {
                    "userName": "Lagrange", 
                    "userLink": "https://www.zhihu.com/people/b077e179902a86034b15c7f2e59bf944", 
                    "content": "<p>请问作者为什么在建立哈密顿雅可比方程，导出S的意义的时候，一开始都强调了第二类生成函数？我自己试了试，而且仔细看了书上的推导步骤，感觉S是第一类生成函数也可以啊？求教~</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "鱼苔半削", 
                    "userLink": "https://www.zhihu.com/people/ea30f188cb3c2891c834dc29b91b7d92", 
                    "content": "“根据变分原理，如果两个式子中，被积分的部分仅仅相差一个函数F的全微分（F是广义坐标、广义动量和时间的函数）”<br>F不是q和Q,t的函数吗？", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }
    ], 
    "url": "https://zhuanlan.zhihu.com/c_103370134"
}
