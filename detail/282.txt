{
    "title": "深度学习杂谈", 
    "description": "", 
    "followers": [
        "https://www.zhihu.com/people/keloli", 
        "https://www.zhihu.com/people/xiao-tan-23", 
        "https://www.zhihu.com/people/xu-yin-da-58", 
        "https://www.zhihu.com/people/shawnyuen", 
        "https://www.zhihu.com/people/jack-tang-40-49", 
        "https://www.zhihu.com/people/bai-si-bu-de-qi-jie-64-70", 
        "https://www.zhihu.com/people/fu-fu-123-38", 
        "https://www.zhihu.com/people/ke-ai-de-cheng-xu-yuan-26", 
        "https://www.zhihu.com/people/qing-gan-gan-qing-14", 
        "https://www.zhihu.com/people/kong-xue-45-77", 
        "https://www.zhihu.com/people/yang-ru-chu", 
        "https://www.zhihu.com/people/teng-xun-shou-ji-guan-jia-fen-si-tuan", 
        "https://www.zhihu.com/people/ji-ji-xiang-sang-38", 
        "https://www.zhihu.com/people/yi-ling-lao-zu-wei-wu-xian-1-9", 
        "https://www.zhihu.com/people/lang-ge-li-ge-lang-86-17", 
        "https://www.zhihu.com/people/su-mu-zhe-1-15", 
        "https://www.zhihu.com/people/chong-shang-yun-xiao-91", 
        "https://www.zhihu.com/people/athena-74-94-34", 
        "https://www.zhihu.com/people/henryyu-23", 
        "https://www.zhihu.com/people/hei-ye-de-ying-huo-chong-62", 
        "https://www.zhihu.com/people/bei-ji-xing-de-yan-lei-68-73", 
        "https://www.zhihu.com/people/wei-xing-gang-99", 
        "https://www.zhihu.com/people/xie-chao-76-76", 
        "https://www.zhihu.com/people/lei-yuan-31-61", 
        "https://www.zhihu.com/people/shuai-qi-de-steve-32", 
        "https://www.zhihu.com/people/ddyxx", 
        "https://www.zhihu.com/people/lele-66-58-76", 
        "https://www.zhihu.com/people/forloverj", 
        "https://www.zhihu.com/people/liu-meng-yin-68", 
        "https://www.zhihu.com/people/wu-li-chun-ren", 
        "https://www.zhihu.com/people/ai-xue-xi-de-mao-83-33", 
        "https://www.zhihu.com/people/xun-zhi-28-12", 
        "https://www.zhihu.com/people/sha-tan-28-69", 
        "https://www.zhihu.com/people/zha-bi-xiao-xin-a-67", 
        "https://www.zhihu.com/people/liu-hong-26-39", 
        "https://www.zhihu.com/people/la-la-la-67-23-56", 
        "https://www.zhihu.com/people/xiao-guang-ge-99-88", 
        "https://www.zhihu.com/people/xiao-hong-mao-1-8", 
        "https://www.zhihu.com/people/chen-zhi-qiang-47-80", 
        "https://www.zhihu.com/people/mo-yan-8-3-17", 
        "https://www.zhihu.com/people/bamf-60", 
        "https://www.zhihu.com/people/hong-hu-63-46", 
        "https://www.zhihu.com/people/san-ke-ya-1", 
        "https://www.zhihu.com/people/guan-su-wei-7", 
        "https://www.zhihu.com/people/song-yu-ting-28-98", 
        "https://www.zhihu.com/people/HaoxuanLi", 
        "https://www.zhihu.com/people/qing-yin-10-96", 
        "https://www.zhihu.com/people/liu-ming-8-69-45", 
        "https://www.zhihu.com/people/royal-30-75", 
        "https://www.zhihu.com/people/la-la-la-8-58-55", 
        "https://www.zhihu.com/people/da-bo-wen-11-27", 
        "https://www.zhihu.com/people/zjzjxxww", 
        "https://www.zhihu.com/people/wo-ai-tu-dou-si-6", 
        "https://www.zhihu.com/people/fh-li-54", 
        "https://www.zhihu.com/people/larry-11-52", 
        "https://www.zhihu.com/people/jie-shang", 
        "https://www.zhihu.com/people/123456ddd-42", 
        "https://www.zhihu.com/people/li-lin-can-67", 
        "https://www.zhihu.com/people/meng-luo-fan-chen-90", 
        "https://www.zhihu.com/people/ran-ran-ran-28-24", 
        "https://www.zhihu.com/people/zzzhao-zhu-lin", 
        "https://www.zhihu.com/people/ni-chao-23", 
        "https://www.zhihu.com/people/chaoli-10", 
        "https://www.zhihu.com/people/cong-cong-91-87", 
        "https://www.zhihu.com/people/13013015325", 
        "https://www.zhihu.com/people/mi-fei-si-te-42", 
        "https://www.zhihu.com/people/lan-xing-31-18", 
        "https://www.zhihu.com/people/xue-luo-xin-ya", 
        "https://www.zhihu.com/people/thikingfly", 
        "https://www.zhihu.com/people/wu-xue-jie-30", 
        "https://www.zhihu.com/people/wang-ran-79-95", 
        "https://www.zhihu.com/people/liu-wei-feng-35", 
        "https://www.zhihu.com/people/li-xia-26-80", 
        "https://www.zhihu.com/people/san-bai-87-64", 
        "https://www.zhihu.com/people/yi-meng-qian-nian-25", 
        "https://www.zhihu.com/people/yang-yin-tao-19", 
        "https://www.zhihu.com/people/h-h-39-60", 
        "https://www.zhihu.com/people/wu-shi-93-35", 
        "https://www.zhihu.com/people/hu-yi-liang", 
        "https://www.zhihu.com/people/Steven_Jokes", 
        "https://www.zhihu.com/people/cao-li-wei-30", 
        "https://www.zhihu.com/people/bljh", 
        "https://www.zhihu.com/people/shao-lin-xiao-zi-24", 
        "https://www.zhihu.com/people/yu-bao-tu-78", 
        "https://www.zhihu.com/people/an-gui-ding-fan-gen-tou", 
        "https://www.zhihu.com/people/yi-yuan-48-45", 
        "https://www.zhihu.com/people/ghul-24", 
        "https://www.zhihu.com/people/tianyige", 
        "https://www.zhihu.com/people/xi-an-25-74", 
        "https://www.zhihu.com/people/yu-mo-ye-81", 
        "https://www.zhihu.com/people/ali-61-67-7", 
        "https://www.zhihu.com/people/13167742589", 
        "https://www.zhihu.com/people/jack-61-23", 
        "https://www.zhihu.com/people/wei-wen-2-93-35", 
        "https://www.zhihu.com/people/bao-bei-50-52-83", 
        "https://www.zhihu.com/people/sa-gen-de-huo-long", 
        "https://www.zhihu.com/people/zhang-yan-yun-36", 
        "https://www.zhihu.com/people/jackstark-87", 
        "https://www.zhihu.com/people/morny_dew", 
        "https://www.zhihu.com/people/zhu-zhu-hui-pa-shu-250", 
        "https://www.zhihu.com/people/jeff-soong", 
        "https://www.zhihu.com/people/jokielueng", 
        "https://www.zhihu.com/people/zero123-88", 
        "https://www.zhihu.com/people/mlxZone", 
        "https://www.zhihu.com/people/zhi2008", 
        "https://www.zhihu.com/people/hong-alex", 
        "https://www.zhihu.com/people/yi-wan-tang-kai-shui-yi", 
        "https://www.zhihu.com/people/a-ying-28-97", 
        "https://www.zhihu.com/people/spongebob-53-24", 
        "https://www.zhihu.com/people/Radon86", 
        "https://www.zhihu.com/people/lxmgoddess", 
        "https://www.zhihu.com/people/yawu-99", 
        "https://www.zhihu.com/people/resnet1906", 
        "https://www.zhihu.com/people/que-96-32", 
        "https://www.zhihu.com/people/xiaojidan", 
        "https://www.zhihu.com/people/jian-yue-39-18", 
        "https://www.zhihu.com/people/zhen-bu-sheng-jiu-li", 
        "https://www.zhihu.com/people/jac123", 
        "https://www.zhihu.com/people/duan-xing-99", 
        "https://www.zhihu.com/people/lonlon-ago", 
        "https://www.zhihu.com/people/xyc-84-81", 
        "https://www.zhihu.com/people/simon_xumeng", 
        "https://www.zhihu.com/people/sptspt", 
        "https://www.zhihu.com/people/eeawi", 
        "https://www.zhihu.com/people/ocarina2015", 
        "https://www.zhihu.com/people/wang-xiang-48-59", 
        "https://www.zhihu.com/people/hai-feng-65-67", 
        "https://www.zhihu.com/people/fxd-88", 
        "https://www.zhihu.com/people/invisible-16", 
        "https://www.zhihu.com/people/ITgeek", 
        "https://www.zhihu.com/people/lon31_1", 
        "https://www.zhihu.com/people/lu-yao-zhi-xiao-song", 
        "https://www.zhihu.com/people/dong-feng-66-72", 
        "https://www.zhihu.com/people/wangong", 
        "https://www.zhihu.com/people/miao-miao-miao-40-51-52", 
        "https://www.zhihu.com/people/Sqlver", 
        "https://www.zhihu.com/people/roger-gou", 
        "https://www.zhihu.com/people/li-tian-ba-22-89", 
        "https://www.zhihu.com/people/sheng-bi-de-wang-zi-42", 
        "https://www.zhihu.com/people/ti-yan-zhe-97", 
        "https://www.zhihu.com/people/zack-lu", 
        "https://www.zhihu.com/people/cheng-liu-xiang-28", 
        "https://www.zhihu.com/people/ma-liang-83-3", 
        "https://www.zhihu.com/people/terry-xiao-37", 
        "https://www.zhihu.com/people/yang-troy-89", 
        "https://www.zhihu.com/people/zhi-chen-hu-78", 
        "https://www.zhihu.com/people/skylakex64", 
        "https://www.zhihu.com/people/bei-zi-ya-ya", 
        "https://www.zhihu.com/people/dyixy", 
        "https://www.zhihu.com/people/wzz-74-1", 
        "https://www.zhihu.com/people/dai-wei-66-30", 
        "https://www.zhihu.com/people/chen-zhao-wei-16-2", 
        "https://www.zhihu.com/people/liu-xin-5-44-87", 
        "https://www.zhihu.com/people/wang-hg", 
        "https://www.zhihu.com/people/wu-wei-57-75", 
        "https://www.zhihu.com/people/liu-xiao-yong-80-42", 
        "https://www.zhihu.com/people/shi-qing-chun-18", 
        "https://www.zhihu.com/people/wu-long-69", 
        "https://www.zhihu.com/people/xiao-bai-71-23-76", 
        "https://www.zhihu.com/people/li-qian-yong-60", 
        "https://www.zhihu.com/people/fen-dou-qing-chun-1-26", 
        "https://www.zhihu.com/people/hkustqq", 
        "https://www.zhihu.com/people/thomas-24-14", 
        "https://www.zhihu.com/people/jing-lin-89", 
        "https://www.zhihu.com/people/zhao-wei-11594", 
        "https://www.zhihu.com/people/wei-sheng-19-77", 
        "https://www.zhihu.com/people/elasine", 
        "https://www.zhihu.com/people/zhang-zhang-93-49-58", 
        "https://www.zhihu.com/people/lei-tao-6", 
        "https://www.zhihu.com/people/wu-jun-2-10", 
        "https://www.zhihu.com/people/shui-liu-yun", 
        "https://www.zhihu.com/people/guai-guai-23-85", 
        "https://www.zhihu.com/people/astronstar", 
        "https://www.zhihu.com/people/misaka-76-72", 
        "https://www.zhihu.com/people/ren-jun-99-11", 
        "https://www.zhihu.com/people/vampireghost12", 
        "https://www.zhihu.com/people/pken", 
        "https://www.zhihu.com/people/zheng-jian-yang-56", 
        "https://www.zhihu.com/people/wang-peng-cheng-39-36", 
        "https://www.zhihu.com/people/sun-tian-yue", 
        "https://www.zhihu.com/people/gary-58-74", 
        "https://www.zhihu.com/people/libin-sui", 
        "https://www.zhihu.com/people/lan-lan-lan-lan-96-82", 
        "https://www.zhihu.com/people/wizardforcel", 
        "https://www.zhihu.com/people/li-gei-ta", 
        "https://www.zhihu.com/people/zhao-zhao-69-41", 
        "https://www.zhihu.com/people/timz917", 
        "https://www.zhihu.com/people/chen-yu-27-40-42", 
        "https://www.zhihu.com/people/15994998139", 
        "https://www.zhihu.com/people/fzalin", 
        "https://www.zhihu.com/people/mao-bo-he-39-79", 
        "https://www.zhihu.com/people/shi-kong-chuan-suo-64", 
        "https://www.zhihu.com/people/yu-shaw-70", 
        "https://www.zhihu.com/people/ren-gan-16", 
        "https://www.zhihu.com/people/applover-5", 
        "https://www.zhihu.com/people/kao-yu-38", 
        "https://www.zhihu.com/people/qu-qi-50-39", 
        "https://www.zhihu.com/people/rui-zong-yun", 
        "https://www.zhihu.com/people/meng-321", 
        "https://www.zhihu.com/people/luo-gao-hui-9", 
        "https://www.zhihu.com/people/leigifted", 
        "https://www.zhihu.com/people/luo-mu-qing-han-65", 
        "https://www.zhihu.com/people/zhong-er-cheng-xu-yuan", 
        "https://www.zhihu.com/people/youknow-62", 
        "https://www.zhihu.com/people/jiang-xiao-yu-4-85", 
        "https://www.zhihu.com/people/hui-bu-qu-liao-95-96", 
        "https://www.zhihu.com/people/bai-jun-50-17", 
        "https://www.zhihu.com/people/walker-star", 
        "https://www.zhihu.com/people/da-qiang-12-71", 
        "https://www.zhihu.com/people/da-kai-8-50", 
        "https://www.zhihu.com/people/wang-yuan-35-44-45", 
        "https://www.zhihu.com/people/wo-shi-xiao-ming-45", 
        "https://www.zhihu.com/people/ilvylb", 
        "https://www.zhihu.com/people/moni-gg", 
        "https://www.zhihu.com/people/zhu-forrest", 
        "https://www.zhihu.com/people/hong-shao-rou-58-92", 
        "https://www.zhihu.com/people/jluautolab", 
        "https://www.zhihu.com/people/na-lan-56-89", 
        "https://www.zhihu.com/people/shi-jia-mo-ni-da-fa-shi", 
        "https://www.zhihu.com/people/jeremy-luo-87", 
        "https://www.zhihu.com/people/aibaozi-31", 
        "https://www.zhihu.com/people/vivi-11-5-99", 
        "https://www.zhihu.com/people/jay-Happy", 
        "https://www.zhihu.com/people/shan-jian-de-feng-1992", 
        "https://www.zhihu.com/people/balldog", 
        "https://www.zhihu.com/people/wu-zhan-68", 
        "https://www.zhihu.com/people/zhi-bai-41-24", 
        "https://www.zhihu.com/people/kfliubo", 
        "https://www.zhihu.com/people/li-xu-96-81", 
        "https://www.zhihu.com/people/xinikaola", 
        "https://www.zhihu.com/people/helloworld-12-26", 
        "https://www.zhihu.com/people/bo-ya-jue-xian-zsw", 
        "https://www.zhihu.com/people/chen-zhi-ying-59-11", 
        "https://www.zhihu.com/people/xia-zheng-79", 
        "https://www.zhihu.com/people/wu-tingxi", 
        "https://www.zhihu.com/people/lundin", 
        "https://www.zhihu.com/people/feng-xing-long-5", 
        "https://www.zhihu.com/people/yun-zhen-6-79", 
        "https://www.zhihu.com/people/wen-hong-chen", 
        "https://www.zhihu.com/people/wangxin-29-49", 
        "https://www.zhihu.com/people/a3146654", 
        "https://www.zhihu.com/people/chen-xiao-feng-14-77", 
        "https://www.zhihu.com/people/fenggege", 
        "https://www.zhihu.com/people/BingtaoGao", 
        "https://www.zhihu.com/people/young-mao-72", 
        "https://www.zhihu.com/people/guo-chao-41-97", 
        "https://www.zhihu.com/people/xiaofengzi0224", 
        "https://www.zhihu.com/people/chun-he-jing-ming-39", 
        "https://www.zhihu.com/people/long-ri-yao", 
        "https://www.zhihu.com/people/liu-bo-wen-82-29", 
        "https://www.zhihu.com/people/xiang-qi-lin-33", 
        "https://www.zhihu.com/people/yildhd-wang", 
        "https://www.zhihu.com/people/li-wen-qi-14-93", 
        "https://www.zhihu.com/people/an2642221", 
        "https://www.zhihu.com/people/rand-liu-55", 
        "https://www.zhihu.com/people/li-dong-40-53", 
        "https://www.zhihu.com/people/cocola100", 
        "https://www.zhihu.com/people/qiangli-88", 
        "https://www.zhihu.com/people/wj2014-59", 
        "https://www.zhihu.com/people/ma-pei-67", 
        "https://www.zhihu.com/people/dongli.um", 
        "https://www.zhihu.com/people/wan-yong-qing-80", 
        "https://www.zhihu.com/people/wan-dao-ming-74", 
        "https://www.zhihu.com/people/bang-bang-ni-33", 
        "https://www.zhihu.com/people/juan-jie-96-47", 
        "https://www.zhihu.com/people/amds-75", 
        "https://www.zhihu.com/people/chu-yun-fei-42-59", 
        "https://www.zhihu.com/people/liu-bo-te-81", 
        "https://www.zhihu.com/people/you-suo-bu-wei-30-76", 
        "https://www.zhihu.com/people/di-wang-gan-xuan-bu-cheng-wei-xing-qiu", 
        "https://www.zhihu.com/people/tui-xiao-fei-39", 
        "https://www.zhihu.com/people/ru-yi-50-98", 
        "https://www.zhihu.com/people/jie-ke-1-26", 
        "https://www.zhihu.com/people/orackxudan", 
        "https://www.zhihu.com/people/nan-de-able", 
        "https://www.zhihu.com/people/jian-po-qin-xin-38", 
        "https://www.zhihu.com/people/xiao-zhong-gong-zuo-shi", 
        "https://www.zhihu.com/people/an-ze-feng-93"
    ], 
    "article": [
        {
            "url": "https://zhuanlan.zhihu.com/p/72859378", 
            "userName": "嘿芝麻", 
            "userLink": "https://www.zhihu.com/people/bf4c9d3f23ce4db98f7a40d12fd0851b", 
            "upvote": 47, 
            "title": "Missing Labels in Object Detection(CVPR2019)解读", 
            "content": "<p></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-66a7acdd2b7eb4d94393dc23e62711ac_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1471\" data-rawheight=\"306\" class=\"origin_image zh-lightbox-thumb\" width=\"1471\" data-original=\"https://pic1.zhimg.com/v2-66a7acdd2b7eb4d94393dc23e62711ac_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1471&#39; height=&#39;306&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1471\" data-rawheight=\"306\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1471\" data-original=\"https://pic1.zhimg.com/v2-66a7acdd2b7eb4d94393dc23e62711ac_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-66a7acdd2b7eb4d94393dc23e62711ac_b.jpg\"/></figure><p> 论文链接：<a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_CVPRW_2019/papers/Weakly%2520Supervised%2520Learning%2520for%2520Real-World%2520Computer%2520Vision%2520Applications/Xu_Missing_Labels_in_Object_Detection_CVPRW_2019_paper.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Missing Labels in Object Detection</a></p><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p> 本文主要贡献：</p><p class=\"ztext-empty-paragraph\"><br/></p><ol><li>在不同丢失程度的instance-level 标签下全监督目标检测（FSOD）的影响。</li><li>首次提出在全监督和弱监督中间（即：部分监督信息）的训练方式和训练网络。</li></ol><h2>1 在不同丢失程度的instance-level 标签下全监督目标检测（FSOD）的影响</h2><p>1.1 在不同instance-level missing label 比例下，WSOD和FSOD两类方法的mAP比较 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-6eeaa46afa66efa03e4335a681bc3b58_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"661\" data-rawheight=\"579\" class=\"origin_image zh-lightbox-thumb\" width=\"661\" data-original=\"https://pic1.zhimg.com/v2-6eeaa46afa66efa03e4335a681bc3b58_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;661&#39; height=&#39;579&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"661\" data-rawheight=\"579\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"661\" data-original=\"https://pic1.zhimg.com/v2-6eeaa46afa66efa03e4335a681bc3b58_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-6eeaa46afa66efa03e4335a681bc3b58_b.jpg\"/></figure><p> 对于全监督的网络作者采用了RCNN Faster-RCNN YOLO和SSD四种网络，弱监督网络采用的是经典的WSDDN网络。显而易见，示例级别的标签丢失越多，mAP越低。</p><blockquote><b>博主有话说：</b> 本文在Introdction就放一个这个实验图，虽然很好说明FSOD方法随着label的减少，mAP指标急剧下降。但是在这个部分讲的实验设置不全，本人读到这里的时候，对FSOD这四种网络的训练方式有很多疑问，但在3.1得到解答（后面再吐槽吧）。所以本人觉得在前期如果不能大篇幅说明，则要有一句详细实验情况见后文之类的说明，这样对读者会比较友好。<br/> </blockquote><h2>2 首次提出在全监督和弱监督中间（即：部分监督信息）的训练方式和训练网络</h2><p>2.1 丢失Label的数据集 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-135a84f67887a317dde92490e0a74eb6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"750\" data-rawheight=\"738\" class=\"origin_image zh-lightbox-thumb\" width=\"750\" data-original=\"https://pic3.zhimg.com/v2-135a84f67887a317dde92490e0a74eb6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;750&#39; height=&#39;738&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"750\" data-rawheight=\"738\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"750\" data-original=\"https://pic3.zhimg.com/v2-135a84f67887a317dde92490e0a74eb6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-135a84f67887a317dde92490e0a74eb6_b.jpg\"/></figure><p>  在本文的主要方法部分，对之前本人提到的疑问进行了进行了实验数据集的说明。具体的算法如上图，简单的说：对于每一个类别，随机丢失Mr的比例的label，并把这些存到一个数组，另外剩下的也存到一个数组。</p><blockquote><b>博主有话说：</b> 通过对这一部分的阅读，尤其是这一句：“we also record images without any instance-level labels after dropping, which will not be sampled when training the FSOD models.”，本人理解是针对FSOD的网络，在训练过程中只对有label的进行训练，随机除去label的图片就不进行采样了；在测试过程中全部图片一起来。如果是这样子的话，本人觉得这有违背本文前面提到的弱监督训练定义，弱监督即只有image-level的信息输入网络训练，但是就这个实验而言，不训练只有image-level的图片，算是弱监督吗？难道这个就是Hybid Supervise Object Detection ？<br/> </blockquote><hr/><p><b>博主题外话</b> ：顺便提一下，作者在3.2小节的第一段倒数第三行，单词写错了。不是off-the-shell，应该是off-the-shelf，中文意思是 现成的。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-58b15d72f402758ccf8786bc969dca59_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"763\" data-rawheight=\"260\" class=\"origin_image zh-lightbox-thumb\" width=\"763\" data-original=\"https://pic2.zhimg.com/v2-58b15d72f402758ccf8786bc969dca59_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;763&#39; height=&#39;260&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"763\" data-rawheight=\"260\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"763\" data-original=\"https://pic2.zhimg.com/v2-58b15d72f402758ccf8786bc969dca59_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-58b15d72f402758ccf8786bc969dca59_b.jpg\"/></figure><hr/><p> 2.2 teacher-student Learning </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-01073236376dec87eabaed7efce6f214_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1042\" data-rawheight=\"675\" class=\"origin_image zh-lightbox-thumb\" width=\"1042\" data-original=\"https://pic1.zhimg.com/v2-01073236376dec87eabaed7efce6f214_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1042&#39; height=&#39;675&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1042\" data-rawheight=\"675\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1042\" data-original=\"https://pic1.zhimg.com/v2-01073236376dec87eabaed7efce6f214_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-01073236376dec87eabaed7efce6f214_b.jpg\"/></figure><p> 本文提出了一种“Teacher-Student”模式的训练网络，本文采用<a href=\"https://link.zhihu.com/?target=https%3A//ivul.kaust.edu.sa/Documents/Publications/2018/W2F%2520A%2520Weakly-Supervised%2520to%2520Fully-Supervised%2520Framework.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">W2F</a>网络进行产生伪标签信息，以弥补那些没有标签（之前随机删除了）的图片信息。Student分支可以使用现成的Faster-RCNN等网络，Teacher分支结构如上图：主要由RPN、MID、ICRs三个模块组成。PRN就是Faster-RCNN这种two-stage的方法用的一样，用来产生proposals。MID用来预测区域结果，是一个loss（loss见下图）。ICRs这个模块是借用了2017年CVPR文章中的结构<a href=\"https://link.zhihu.com/?target=http%3A//openaccess.thecvf.com/content_cvpr_2017/papers/Tang_Multiple_Instance_Detection_CVPR_2017_paper.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Multiple Instance Detection Network with Online Instance Classifier Refinement</a> 本文的K设置为3。 MID的loss如右图：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-bae592af759458e534f612add45626a9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"363\" data-rawheight=\"95\" class=\"content_image\" width=\"363\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;363&#39; height=&#39;95&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"363\" data-rawheight=\"95\" class=\"content_image lazy\" width=\"363\" data-actualsrc=\"https://pic2.zhimg.com/v2-bae592af759458e534f612add45626a9_b.jpg\"/></figure><p> 其中，Llab和Lcls都是普通的交叉熵损失，Llab就是利用了<a href=\"https://link.zhihu.com/?target=https%3A//www.robots.ox.ac.uk/~vgg/publications/2016/Bilen16/bilen16.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">WSDDN</a>这篇论文中的loss。最后一个Lreg回归损失使用的是L1损失。</p><p class=\"ztext-empty-paragraph\"><br/></p><blockquote><b>博主有话说：</b> 第一，本人感觉“Teacher-Student”模式和级连网络相似，都是一个分支训练好后的输出作为下一个网络的输入进行训练，这里要单独训练Teacher这个分支。通过Teacher分支来把弱监督的信息转换成强监督的信息用来训练Student分支。 第二，根据论文对Teacher分支的介绍和上图结构，输入的训练图像的真实标签（GT）只有一只羊，而训练的Teacher模型预测的伪标签（PGT）全部三只羊都框住了，本人觉得不可思议，如此的话可以直接只用Teacher模型做预测，实验部分也没有对此实验。另外，这一部分让本人产生另一种功能假想：上图网络输入的图片，真实标签其实有三只羊全部框住的，在上面提到的随机删除标签中，把两只羊的label删除了。一张图的真实的标签不全，本人觉得好像也可以一定程度上理解为弱监督。 第三，ICRs模块这个结构就是一个级连的分支，和本文提出的“Teacher-Student”网络模式有异曲同工之妙。<br/> </blockquote><p>最后，本文loss由三部分组成： </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-7dfe130d613339778b6e8a0bf4418f6b_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"438\" data-rawheight=\"39\" class=\"origin_image zh-lightbox-thumb\" width=\"438\" data-original=\"https://pic4.zhimg.com/v2-7dfe130d613339778b6e8a0bf4418f6b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;438&#39; height=&#39;39&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"438\" data-rawheight=\"39\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"438\" data-original=\"https://pic4.zhimg.com/v2-7dfe130d613339778b6e8a0bf4418f6b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-7dfe130d613339778b6e8a0bf4418f6b_b.png\"/></figure><p> 第一个RPN loss和Faster-RCNN一样，第二个MID loss上面提过，第三个ICR loss 和Multiple Instance Detection Network with Online Instance Classifier Refinement论文的loss是一样的，也是和WSDDNloss有类似之处。</p><h2>3 实验部分</h2><p>讲两个本人觉得比较有意思的实验 3.1  </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-7c99494a69c35fdc76e1a0f306ef2cdd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"500\" data-rawheight=\"415\" class=\"origin_image zh-lightbox-thumb\" width=\"500\" data-original=\"https://pic2.zhimg.com/v2-7c99494a69c35fdc76e1a0f306ef2cdd_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;500&#39; height=&#39;415&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"500\" data-rawheight=\"415\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"500\" data-original=\"https://pic2.zhimg.com/v2-7c99494a69c35fdc76e1a0f306ef2cdd_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-7c99494a69c35fdc76e1a0f306ef2cdd_b.jpg\"/></figure><p> 在这个实验中，两个发现：1）在coco数据集少量标签的缺失不会影响准确率。2）牛和桌子的mAP相比，随着missing Label rate 的上升，牛的mAP下降的更剧烈。</p><blockquote><b>博主有话说：</b> 本人觉得实验有意思的地方在 探究了一张图片目标个数的问题。在弱监督目标检测的领域，只有image-level的标签，检测全是一个比较难的问题。在这个实验中，说明了一张图片中，由于牛的数量会比桌子的数量多，所以影响mAP，上面的右边图也可以看出，在含有牛的图片中，一般有多只牛，而桌子的图片中一般只包含一张桌子。本人觉得，从这个角度探究提升弱监督mAP是一个挺好的点子💡<br/> </blockquote><p>3.2 </p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-caae34cb09b2e9b15e23cbbba724784e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"502\" data-rawheight=\"197\" class=\"origin_image zh-lightbox-thumb\" width=\"502\" data-original=\"https://pic3.zhimg.com/v2-caae34cb09b2e9b15e23cbbba724784e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;502&#39; height=&#39;197&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"502\" data-rawheight=\"197\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"502\" data-original=\"https://pic3.zhimg.com/v2-caae34cb09b2e9b15e23cbbba724784e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-caae34cb09b2e9b15e23cbbba724784e_b.jpg\"/></figure><p> 这里作者诠释了小目标，提出的在一个数据集中去除小目标的方法可以参考～ 1）计算所有instance的平均面积：u 2）保留所有面积大于u的instance。 保留下来的都是normal object了。</p><hr/><p>其他实验就比较普通了，一大波截图来了： </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-5350d8147276d963e828517715f2c9db_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"500\" data-rawheight=\"467\" class=\"origin_image zh-lightbox-thumb\" width=\"500\" data-original=\"https://pic4.zhimg.com/v2-5350d8147276d963e828517715f2c9db_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;500&#39; height=&#39;467&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"500\" data-rawheight=\"467\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"500\" data-original=\"https://pic4.zhimg.com/v2-5350d8147276d963e828517715f2c9db_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-5350d8147276d963e828517715f2c9db_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-cd4dd581eea2efaeb923fa37c578e08a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"513\" data-rawheight=\"551\" class=\"origin_image zh-lightbox-thumb\" width=\"513\" data-original=\"https://pic3.zhimg.com/v2-cd4dd581eea2efaeb923fa37c578e08a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;513&#39; height=&#39;551&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"513\" data-rawheight=\"551\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"513\" data-original=\"https://pic3.zhimg.com/v2-cd4dd581eea2efaeb923fa37c578e08a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-cd4dd581eea2efaeb923fa37c578e08a_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-6577d41e0268312e91da53c982fd6ae2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1053\" data-rawheight=\"337\" class=\"origin_image zh-lightbox-thumb\" width=\"1053\" data-original=\"https://pic3.zhimg.com/v2-6577d41e0268312e91da53c982fd6ae2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1053&#39; height=&#39;337&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1053\" data-rawheight=\"337\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1053\" data-original=\"https://pic3.zhimg.com/v2-6577d41e0268312e91da53c982fd6ae2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-6577d41e0268312e91da53c982fd6ae2_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-21e0f4d81fc0dbcefb2e492cda0edea3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1041\" data-rawheight=\"868\" class=\"origin_image zh-lightbox-thumb\" width=\"1041\" data-original=\"https://pic4.zhimg.com/v2-21e0f4d81fc0dbcefb2e492cda0edea3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1041&#39; height=&#39;868&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1041\" data-rawheight=\"868\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1041\" data-original=\"https://pic4.zhimg.com/v2-21e0f4d81fc0dbcefb2e492cda0edea3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-21e0f4d81fc0dbcefb2e492cda0edea3_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p> 本人拙见，请各位读者指教！</p><p></p>", 
            "topic": [
                {
                    "tag": "目标检测", 
                    "tagLink": "https://api.zhihu.com/topics/19596960"
                }
            ], 
            "comments": [
                {
                    "userName": "日记本", 
                    "userLink": "https://www.zhihu.com/people/7aa569d338cedb00447a77d06280c01d", 
                    "content": "是什么给了我自信点进来看的[语塞]", 
                    "likes": 7, 
                    "childComments": [
                        {
                            "userName": "嘿芝麻", 
                            "userLink": "https://www.zhihu.com/people/bf4c9d3f23ce4db98f7a40d12fd0851b", 
                            "content": "哪里有问题还请大佬指教๛ก(ｰ̀ωｰ́ก) ", 
                            "likes": 1, 
                            "replyToAuthor": "日记本"
                        }
                    ]
                }, 
                {
                    "userName": "zhuxqstyle", 
                    "userLink": "https://www.zhihu.com/people/5b8a53d7ab6ad7ec5138d8bf26b9e7cd", 
                    "content": "为什么给我推送这个?", 
                    "likes": 1, 
                    "childComments": [
                        {
                            "userName": "嘿芝麻", 
                            "userLink": "https://www.zhihu.com/people/bf4c9d3f23ce4db98f7a40d12fd0851b", 
                            "content": "hh you need it🌝", 
                            "likes": 0, 
                            "replyToAuthor": "zhuxqstyle"
                        }
                    ]
                }, 
                {
                    "userName": "刘三炮", 
                    "userLink": "https://www.zhihu.com/people/4a0e1347e654a629474969ab88d82c0d", 
                    "content": "我大概是膨胀了，来看这个", 
                    "likes": 1, 
                    "childComments": []
                }, 
                {
                    "userName": "emmm祖国万岁", 
                    "userLink": "https://www.zhihu.com/people/5ea533fda307de8ef18d4233cebf9851", 
                    "content": "那个能给我讲讲键盘的使用方法吗[发呆]（俺这刚通电[飙泪笑][飙泪笑]）", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "嘿芝麻", 
                            "userLink": "https://www.zhihu.com/people/bf4c9d3f23ce4db98f7a40d12fd0851b", 
                            "content": "啥😂", 
                            "likes": 0, 
                            "replyToAuthor": "emmm祖国万岁"
                        }
                    ]
                }, 
                {
                    "userName": "摘走野菊", 
                    "userLink": "https://www.zhihu.com/people/bc89c53a7afb6b5072adbdc1e4c9a366", 
                    "content": "谁敢给我的勇气让我点进来看", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "我当时就火了", 
                    "userLink": "https://www.zhihu.com/people/cb7b75b503fa4e04c2f7839c0f4a8b46", 
                    "content": "？？？？？叔叔我小学刚毕业 这对于我来说有点困难", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "龍鳯茶行18083336197", 
                    "userLink": "https://www.zhihu.com/people/eca9c9fab6485bc568c5baa0ce9490df", 
                    "content": "Smile and let everyone know that today you're a lot stronger than you were yesterday.", 
                    "likes": 1, 
                    "childComments": []
                }, 
                {
                    "userName": "ZHAOSHUMIN", 
                    "userLink": "https://www.zhihu.com/people/ee24af0a4e37f9975f7e667ae5a980e7", 
                    "content": "我一个在读高中生  为什么被推送了这个", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "百撕不得骑姐", 
                    "userLink": "https://www.zhihu.com/people/8668ebd8e286b947398222fbf066c3d8", 
                    "content": "为什么我会有勇气点这个[捂脸]", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/66351550", 
            "userName": "小白", 
            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
            "upvote": 1, 
            "title": "PyQT5使用", 
            "content": "<p>1，想使用可视化的编程需要重新按照qt5</p><div class=\"highlight\"><pre><code class=\"language-text\">pip install PyQt5-tools -i http://pypi.douban.com/simple --trusted-host=pypi.douban.com</code></pre></div><p>2，pycharm配置界面工具</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-c8163dc7f3d8afdbf66b79f2c1b1f810_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"523\" data-rawheight=\"309\" class=\"origin_image zh-lightbox-thumb\" width=\"523\" data-original=\"https://pic1.zhimg.com/v2-c8163dc7f3d8afdbf66b79f2c1b1f810_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;523&#39; height=&#39;309&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"523\" data-rawheight=\"309\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"523\" data-original=\"https://pic1.zhimg.com/v2-c8163dc7f3d8afdbf66b79f2c1b1f810_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-c8163dc7f3d8afdbf66b79f2c1b1f810_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-40a6a8a844efef0b74c67c53fd161fdb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"331\" data-rawheight=\"347\" class=\"content_image\" width=\"331\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;331&#39; height=&#39;347&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"331\" data-rawheight=\"347\" class=\"content_image lazy\" width=\"331\" data-actualsrc=\"https://pic4.zhimg.com/v2-40a6a8a844efef0b74c67c53fd161fdb_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ebc341cf83c15e936cf4dcba610f6b57_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"932\" data-rawheight=\"437\" class=\"origin_image zh-lightbox-thumb\" width=\"932\" data-original=\"https://pic4.zhimg.com/v2-ebc341cf83c15e936cf4dcba610f6b57_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;932&#39; height=&#39;437&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"932\" data-rawheight=\"437\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"932\" data-original=\"https://pic4.zhimg.com/v2-ebc341cf83c15e936cf4dcba610f6b57_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-ebc341cf83c15e936cf4dcba610f6b57_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-c56625f69994aa589e3f2b7577e52ceb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"492\" data-rawheight=\"271\" class=\"origin_image zh-lightbox-thumb\" width=\"492\" data-original=\"https://pic4.zhimg.com/v2-c56625f69994aa589e3f2b7577e52ceb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;492&#39; height=&#39;271&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"492\" data-rawheight=\"271\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"492\" data-original=\"https://pic4.zhimg.com/v2-c56625f69994aa589e3f2b7577e52ceb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-c56625f69994aa589e3f2b7577e52ceb_b.jpg\"/></figure><p>3，pycharm配置将界面转成py代码</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-ec1a1e0d2cfc97b450263ca370cdcb3a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"908\" data-rawheight=\"471\" class=\"origin_image zh-lightbox-thumb\" width=\"908\" data-original=\"https://pic3.zhimg.com/v2-ec1a1e0d2cfc97b450263ca370cdcb3a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;908&#39; height=&#39;471&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"908\" data-rawheight=\"471\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"908\" data-original=\"https://pic3.zhimg.com/v2-ec1a1e0d2cfc97b450263ca370cdcb3a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-ec1a1e0d2cfc97b450263ca370cdcb3a_b.jpg\"/></figure><div class=\"highlight\"><pre><code class=\"language-text\">-m PyQt5.uic.pyuic $FileName$ -o $FileNameWithoutExtension$.py\n$FileDir$</code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-6ba81b46ee64ea272f4ba62c1a61e42e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"635\" data-rawheight=\"512\" class=\"origin_image zh-lightbox-thumb\" width=\"635\" data-original=\"https://pic3.zhimg.com/v2-6ba81b46ee64ea272f4ba62c1a61e42e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;635&#39; height=&#39;512&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"635\" data-rawheight=\"512\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"635\" data-original=\"https://pic3.zhimg.com/v2-6ba81b46ee64ea272f4ba62c1a61e42e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-6ba81b46ee64ea272f4ba62c1a61e42e_b.jpg\"/></figure><p>生成Py代码</p>", 
            "topic": [
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/59994806", 
            "userName": "小白", 
            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
            "upvote": 1, 
            "title": "mAP代码解析", 
            "content": "<p></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-4a055e4923a4302092bc22171fbeeb11_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"402\" data-rawheight=\"125\" class=\"content_image\" width=\"402\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;402&#39; height=&#39;125&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"402\" data-rawheight=\"125\" class=\"content_image lazy\" width=\"402\" data-actualsrc=\"https://pic2.zhimg.com/v2-4a055e4923a4302092bc22171fbeeb11_b.jpg\"/></figure><p>insert在list内插入一个数字</p><p>rec.insert(0, 0.0)在0位置插入0</p><p>rec是召回率</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-695367c36ff302d99bac04aa465b618d_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"502\" data-rawheight=\"38\" class=\"origin_image zh-lightbox-thumb\" width=\"502\" data-original=\"https://pic2.zhimg.com/v2-695367c36ff302d99bac04aa465b618d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;502&#39; height=&#39;38&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"502\" data-rawheight=\"38\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"502\" data-original=\"https://pic2.zhimg.com/v2-695367c36ff302d99bac04aa465b618d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-695367c36ff302d99bac04aa465b618d_b.png\"/></figure><p>prec是准确率</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-c94d9390bfe6331d83a8427478451709_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"531\" data-rawheight=\"31\" class=\"origin_image zh-lightbox-thumb\" width=\"531\" data-original=\"https://pic2.zhimg.com/v2-c94d9390bfe6331d83a8427478451709_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;531&#39; height=&#39;31&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"531\" data-rawheight=\"31\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"531\" data-original=\"https://pic2.zhimg.com/v2-c94d9390bfe6331d83a8427478451709_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-c94d9390bfe6331d83a8427478451709_b.png\"/></figure><p>所以，召回率为1/n时，准确率可以为1（只要检出的一个文件就是正确的文件）</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-b194a5615581deca63122111b73c9d88_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"247\" data-rawheight=\"42\" class=\"content_image\" width=\"247\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;247&#39; height=&#39;42&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"247\" data-rawheight=\"42\" class=\"content_image lazy\" width=\"247\" data-actualsrc=\"https://pic1.zhimg.com/v2-b194a5615581deca63122111b73c9d88_b.jpg\"/></figure><p>range:第一个参数是开始数字（包含），第二参数是结束数字（不包含），第三个是步长</p><p>从最后一个点开始，准确率都是小于它的召回率的最大值</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-009c489d8147f0024945ded8110695e7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"363\" data-rawheight=\"78\" class=\"content_image\" width=\"363\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;363&#39; height=&#39;78&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"363\" data-rawheight=\"78\" class=\"content_image lazy\" width=\"363\" data-actualsrc=\"https://pic4.zhimg.com/v2-009c489d8147f0024945ded8110695e7_b.jpg\"/></figure><p>记录召回率变化的位置</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-523b2b30a4fd67c344eccde6116ba35c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"289\" data-rawheight=\"59\" class=\"content_image\" width=\"289\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;289&#39; height=&#39;59&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"289\" data-rawheight=\"59\" class=\"content_image lazy\" width=\"289\" data-actualsrc=\"https://pic1.zhimg.com/v2-523b2b30a4fd67c344eccde6116ba35c_b.jpg\"/></figure><p>在召回率改变的地方乘准确率，得到AP</p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/53877207", 
            "userName": "陀飞轮", 
            "userLink": "https://www.zhihu.com/people/9414183bd7a1ed7354cbbdbaa41a870c", 
            "upvote": 36, 
            "title": "IoU-Net", 
            "content": "<p><b>Cascade-RCNN</b></p><p>在讲IoU-Net之前，先了解一下IoU-Net之前的工作，其中cascade-rcnn对于iou这个变量进行了非常有价值的研究。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-7e0448b4d5092b72ddf11ce8edc165e7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"591\" data-rawheight=\"230\" class=\"origin_image zh-lightbox-thumb\" width=\"591\" data-original=\"https://pic4.zhimg.com/v2-7e0448b4d5092b72ddf11ce8edc165e7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;591&#39; height=&#39;230&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"591\" data-rawheight=\"230\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"591\" data-original=\"https://pic4.zhimg.com/v2-7e0448b4d5092b72ddf11ce8edc165e7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-7e0448b4d5092b72ddf11ce8edc165e7_b.jpg\"/></figure><p>这张图来自于cascade-rcnn，改论文的作者发现在classification中，指定不同的IoU划分正负样本，会导致bbox reg的行为完全不一样。如上左图所示，横轴是输入的proposal的IoU，纵轴是经过bbox reg之后的bbox和ground-truth的IoU。可以看到，低IoU threshold训练对于低IoU的样本有更好的改善，但是对于高IoU的样本就不如高threshold的有用。原因在于不同threshold下样本的分布会不一致，也就导致同一个threshold很难对所有样本都有效。</p><p>一个直接的想法是，为什么不可以直接使用高IoU呢？这就会导致第二个问题，也就是proposal的质量过差，导致高IoU的正样本数太少。如果强行这样训练的话，这就会导致严重的过拟合问题。如上右图所示，threshold从0.5到0.6，最终的detection AP有些许上升，但是进一步提升到0.7，AP会急剧下降。（搬运自<a href=\"https://zhuanlan.zhihu.com/p/35882192\" class=\"internal\">Naiyan Wang：CVPR18 Detection文章选介（上）</a>）</p><p>简而言之，上左图表示的是，划分正负样本的指定IoU对于IoU一致的proposal输入越有帮助 ，上右图表示的是，划分正负样本的指定IoU对于bbox reg之后IoU一致的AP越有帮助。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-0cf1c435347925172866e3cadb7b4823_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"453\" data-rawheight=\"265\" class=\"origin_image zh-lightbox-thumb\" width=\"453\" data-original=\"https://pic4.zhimg.com/v2-0cf1c435347925172866e3cadb7b4823_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;453&#39; height=&#39;265&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"453\" data-rawheight=\"265\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"453\" data-original=\"https://pic4.zhimg.com/v2-0cf1c435347925172866e3cadb7b4823_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-0cf1c435347925172866e3cadb7b4823_b.jpg\"/></figure><p>所以为了解决以上两个问题，作者提出了一个Cascade RCNN head的办法（如上图所示）。即有多个IoU threshold递增的header，每一级使用上一级refine过后的bbox作为输入。这样可以保证每一级的header都可以得到足够多的正样本，且正样本的质量可以逐级提升。</p><p>可以看到，划分正负样本的指定IoU、proposal IoU还有bbox reg后的IoU之间相互牵扯，相互影响，cascade-rcnn的方法直接且暴力，通过多阶段精炼proposal的方法来改善回归框的质量。然而这种方法并不是长久之计，首先多阶段必定会影响速度，并且占memory，其次每个阶段的IoU需要自己  设置      ，不够general。</p><p>不同于之前的工作，像Iterative bbox refinement，每次refine使用的是同样threshold训练出来的header，这会导致IoU到一定值之后很难更进一步改善；像Integral loss，只有一个bbox reg，但是有针对不同threshold的classifier，这本质上也并不能改善最终定位的精度。</p><h2><b>motivation</b></h2><p><b>1.bad money drives out good</b></p><p>大多数的现代二阶段框架目标检测器中，分类和定位用不同的方式处理，具体的，给定一个提议，每个类的概率自然地做为分类分数，边界框回归模块寻找proposal最适应ground-truth的最优转换。但是，在这个过程中，定位分数是缺失的。</p><p>这会导致2个问题：</p><p>1.定位准确的检测框因为分类分数低经过NMS而被剔除掉。</p><p>2.定位分数的缺失，导致广泛采用的边界框回归过程难以解释。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-a991c98396fac263c1bcaffeee5dfa45_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"864\" data-rawheight=\"197\" class=\"origin_image zh-lightbox-thumb\" width=\"864\" data-original=\"https://pic2.zhimg.com/v2-a991c98396fac263c1bcaffeee5dfa45_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;864&#39; height=&#39;197&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"864\" data-rawheight=\"197\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"864\" data-original=\"https://pic2.zhimg.com/v2-a991c98396fac263c1bcaffeee5dfa45_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-a991c98396fac263c1bcaffeee5dfa45_b.jpg\"/></figure><p>该图反应的是第一个问题，黄色框是gt，红色和绿色框式检测框，显然上面3个图例中，绿框比红框定位精准，但是，由于红框分类分数高于绿框，导致NMS过程中，反而保留了定位较差的绿框。</p><p><b>2.Non-monotonicity</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-1de9cb45ac90d1be70c7f05bae2bf15e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"887\" data-rawheight=\"354\" class=\"origin_image zh-lightbox-thumb\" width=\"887\" data-original=\"https://pic3.zhimg.com/v2-1de9cb45ac90d1be70c7f05bae2bf15e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;887&#39; height=&#39;354&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"887\" data-rawheight=\"354\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"887\" data-original=\"https://pic3.zhimg.com/v2-1de9cb45ac90d1be70c7f05bae2bf15e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-1de9cb45ac90d1be70c7f05bae2bf15e_b.jpg\"/></figure><p>该图反应的是第二个问题，由于边界框回归难以解释，迭代回归反而使得检测框定位精度变差。</p><p><b>IoU-Net3个贡献：</b></p><p>1.提出了IoU-Net来学会预测检测框和gt之间的IoU。</p><p>2.提出IoU引导的NMS。</p><p>3.提出一种基于优化的边界框微调方法。</p><h2><b>Delving into object localization</b></h2><p><b>1.Misalignment</b></p><p>之前针对nms过程的工作主要有parameter-free（NMS、soft-NMS）和parameter-based两类。然而parameter-based的方法需要更多的资源，难以在真实场景应用。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-d23874ed7da885dc1a5bd4cf7f524d5f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"794\" data-rawheight=\"320\" class=\"origin_image zh-lightbox-thumb\" width=\"794\" data-original=\"https://pic4.zhimg.com/v2-d23874ed7da885dc1a5bd4cf7f524d5f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;794&#39; height=&#39;320&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"794\" data-rawheight=\"320\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"794\" data-original=\"https://pic4.zhimg.com/v2-d23874ed7da885dc1a5bd4cf7f524d5f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-d23874ed7da885dc1a5bd4cf7f524d5f_b.jpg\"/></figure><p>在NMS过程中，检测框的质量和分类分数不成正比。如果一个目标有多个检测框，在NMS的过程中，只保留分数最高的边界框，然而由于不对齐，定位更好的边界框被剔除掉了，导致目标定位较差。</p><p>IoU-guided NMS,检测框的质量和定位分数近似成正比。这将导致选取定位分数更高的检测框时，目标定位大概率也比较好。上图(b)中，左上角有一些偏离红线的点，影响了精度，作者论文中提到说，可以通过增加样本一定程度上缓解这个问题。当边界框与ground-truth有较低的IoU时，特征上有较大的差异，使得IoU的估计不准确，降低了refinement和suppression的性能。经验上，训练时可以通过采样更多的低IoU的边界框部分的解决这个问题。</p><p>我们将这个现象归因为大多数基于CNN的目标检测器只是区分前景和背景。如上图(a)所示，将IoU大于0.5的proposal视为正样本，将IoU小于0.5的propsal视为负样本，本质上还是将proposal做分类，整个过程中并没有体现出正样本的质量的不同，正样本的地位都是相同的，并没有学习到proposal包含对应目标的知识量。假如说网络能够学习到IoU这个变量，不就侧面说明了网络学习到了，proposal所包含目标的质量如何。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-6865e462b4fd367e64f6b4b5d72746d4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"437\" data-rawheight=\"315\" class=\"origin_image zh-lightbox-thumb\" width=\"437\" data-original=\"https://pic1.zhimg.com/v2-6865e462b4fd367e64f6b4b5d72746d4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;437&#39; height=&#39;315&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"437\" data-rawheight=\"315\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"437\" data-original=\"https://pic1.zhimg.com/v2-6865e462b4fd367e64f6b4b5d72746d4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-6865e462b4fd367e64f6b4b5d72746d4_b.jpg\"/></figure><p>上图中可以看到，No-NMS可以认为是正样本的数量的上界。IoU-Guided NMS比NMS保留了更加多的精准定位的边界框，NMS在IoU为0.9的时候，边界框的数量减少了一半多。</p><p><b>2.Non-monotonic</b></p><p>通常，单个目标的定位可以分为基于边界框和基于分割的方法。基于分割的方法目标为对每一个实例产生一个像素级的分割，但是不可避免的需要额外的分割标注。该论文基于边界框的方法。</p><p>对于许多应用场景来说，改善定位精度是至关重要的。之前的许多工作，都是通过迭代边界框回归来改善定位精度的。然而，cascade-rcnn中指出，使用边界框回归超过2次，就不能进一步改善定位精度。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-7466509e87456dd5a2c64b59d9ee286c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"784\" data-rawheight=\"304\" class=\"origin_image zh-lightbox-thumb\" width=\"784\" data-original=\"https://pic1.zhimg.com/v2-7466509e87456dd5a2c64b59d9ee286c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;784&#39; height=&#39;304&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"784\" data-rawheight=\"304\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"784\" data-original=\"https://pic1.zhimg.com/v2-7466509e87456dd5a2c64b59d9ee286c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-7466509e87456dd5a2c64b59d9ee286c_b.jpg\"/></figure><p>如上图中的蓝线所示，基于迭代边界框回归的情况下，随着迭代次数的增加，定位准确率是非单调的。非单调性和不可解释性带来了应用上的困难。此外，检测框没有定位精度，我们不能细粒度的对检测框精炼进行控制，比如对不同的边界框使用自适应的迭代数量。</p><p>而使用该论文提出的基于优化的边界框回归方法，，随着迭代次数的增加，定位准确率是单调的。</p><h2><b>IoU-Net</b></h2><p><b>1.Learning to predict IoU</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-5efced8171c38bee7fe8f505bade3b90_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"859\" data-rawheight=\"336\" class=\"origin_image zh-lightbox-thumb\" width=\"859\" data-original=\"https://pic1.zhimg.com/v2-5efced8171c38bee7fe8f505bade3b90_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;859&#39; height=&#39;336&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"859\" data-rawheight=\"336\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"859\" data-original=\"https://pic1.zhimg.com/v2-5efced8171c38bee7fe8f505bade3b90_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-5efced8171c38bee7fe8f505bade3b90_b.jpg\"/></figure><p>该论文提出的IoU-Net，利用FPN生成的特征对每一个边界框的定位准确度进行评估。用于训练IoU-Net分支的边界框和标签通过ground-truth增强生成得到（不是RPN生成的proposal）。具体来说，就是对所有训练集中的ground-truth边界框，用一系列随即参数转化这些边界框，产生一个候选边界框集合（Here, jittering is performed by adding noise to the box coordinates）。移除IoU小于0.5的边界框。经验上，这样做能够带来更好的结果和鲁棒性。</p><p>对于每一个边界框，特征从FPN和PrRoIPooling中提取出来，然后进入2个用于生成IoU预测的fc层。为了更好的结果，使用类别感知的IoU预测器。</p><p>该IoU预测器可以和大多数存在的基于RoI的检测器兼容。对于特定的检测器来说，因为IoU-Net的训练过程是独立的，所以对于输入分布的改变具有鲁棒性。并且为了能够判断出边界框的质量，IoU-Net分支调整了FPN中的特征抽取。</p><p><b>2.IoU-guided NMS</b></p><p>为了解决分类分数和定位准确度的不对齐，提出了IoU-guided NMS过程，该过程中，分类分数和定位准确度是分开的。简单来说，使用IoU来做为边界框排序的关键字，而不是分类分数。保留IoU最大的，剔除超过阈值的边界框并且更新IoU最大的分类分数。可以认为是一个分数聚类的过程：对于和同一个ground-truth匹配的一组边界框，我们选取最置信的类标签预测。如下图伪代码所示。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b3642b41dd301bf3c7973a93573d76b1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"869\" data-rawheight=\"549\" class=\"origin_image zh-lightbox-thumb\" width=\"869\" data-original=\"https://pic2.zhimg.com/v2-b3642b41dd301bf3c7973a93573d76b1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;869&#39; height=&#39;549&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"869\" data-rawheight=\"549\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"869\" data-original=\"https://pic2.zhimg.com/v2-b3642b41dd301bf3c7973a93573d76b1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-b3642b41dd301bf3c7973a93573d76b1_b.jpg\"/></figure><p><b>3.Bounding box refinement</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-fcd74e430ecb227cbe0595c35c17f85d_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"550\" data-rawheight=\"44\" class=\"origin_image zh-lightbox-thumb\" width=\"550\" data-original=\"https://pic2.zhimg.com/v2-fcd74e430ecb227cbe0595c35c17f85d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;550&#39; height=&#39;44&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"550\" data-rawheight=\"44\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"550\" data-original=\"https://pic2.zhimg.com/v2-fcd74e430ecb227cbe0595c35c17f85d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-fcd74e430ecb227cbe0595c35c17f85d_b.png\"/></figure><p>迭代边界框回归方法容易受到输入分布改变的影响，并且可能导致非单调的定位改善。为了处理这个问题，该论文提出了一种基于优化的边界框微调方法，利用IoU-Net作为一个鲁棒的定位精度评估器。另外，IoU评估器能作为一种自适应迭代微调阶段提前停止的条件。</p><p>IoU-Net直接评估IoU(boxdet, boxgt)。由于提出的PrRoIPooling层能够计算边界框坐标的梯度，所以我们能直接使用梯度下降的方法得到上式的最优解。将IoU的估计作为优化目标，我们用计算得到的梯度迭代微调边界框坐标，来最大化检测框和匹配的ground-truth之间的IoU。另外，预测得到的IoU是每个边界框定位置信度的可解释指标，并有助于解释已执行的转化过程。</p><p>在下图伪代码中的第6行，使用该轴上的边界框的大小来调整与坐标相关的缩放。(e.g.▽x1/width(bj)).利用一次边界框回归对坐标进行初始化。</p><p>该论文提出的PrRoIPooling避免了任何的坐标量化同时在边界框坐标上有一个连续的梯度。坐标落在合适的位置使得IoU评估值最大。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-f86e893a49e44ec314ea4041e1ea5b94_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"584\" data-rawheight=\"32\" class=\"origin_image zh-lightbox-thumb\" width=\"584\" data-original=\"https://pic1.zhimg.com/v2-f86e893a49e44ec314ea4041e1ea5b94_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;584&#39; height=&#39;32&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"584\" data-rawheight=\"32\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"584\" data-original=\"https://pic1.zhimg.com/v2-f86e893a49e44ec314ea4041e1ea5b94_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-f86e893a49e44ec314ea4041e1ea5b94_b.png\"/></figure><p>IC是插值系数。</p><p>对于给定的bin和feature map进行二阶积分：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-3f2f0d61a3ddd39b03c17eeef44c33e0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"427\" data-rawheight=\"102\" class=\"origin_image zh-lightbox-thumb\" width=\"427\" data-original=\"https://pic1.zhimg.com/v2-3f2f0d61a3ddd39b03c17eeef44c33e0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;427&#39; height=&#39;102&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"427\" data-rawheight=\"102\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"427\" data-original=\"https://pic1.zhimg.com/v2-3f2f0d61a3ddd39b03c17eeef44c33e0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-3f2f0d61a3ddd39b03c17eeef44c33e0_b.jpg\"/></figure><p>PrRoIPooling是可微分的。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-3f2f0d61a3ddd39b03c17eeef44c33e0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"427\" data-rawheight=\"102\" class=\"origin_image zh-lightbox-thumb\" width=\"427\" data-original=\"https://pic1.zhimg.com/v2-3f2f0d61a3ddd39b03c17eeef44c33e0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;427&#39; height=&#39;102&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"427\" data-rawheight=\"102\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"427\" data-original=\"https://pic1.zhimg.com/v2-3f2f0d61a3ddd39b03c17eeef44c33e0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-3f2f0d61a3ddd39b03c17eeef44c33e0_b.jpg\"/></figure><p>IoU-Net分支在训练的时候的target应该是提取出来的RoIs和gt之间的IoU,这样，该分支通过学习，最后实现了这样一个功能：我把RoI的Pooling后的特征输入进去，就得到了这个RoI的IoU。既有一个：box的位置--Pooling得到的特征--预测出该位置的IoU，这样一个逻辑关系。</p><p>在测试时，得到一个detected box，通过IoU-Net计算发现IoU不是1，于是把1当做目标对detected box的位置求导，按照上一段的逻辑关系，是可以实现的。作者也给出了求导的链式法则。得到新的detected box后，由于位置改变，pooling的特征改变，IoU也会更加精准，然后不断优化，直到满足次数或者结果没有提升。</p><p><b>4.Joint training</b></p><p>train：</p><p>用PrRoIPooling layer替代RoIPooling layer</p><p>IoU预测器和R-CNN分支同时训练</p><p>使用Smooth-L1 loss 训练IoU预测器</p><p>IoU预测器的训练数据通过随即参数产生</p><p>IoU标签归一化到[-1,1]之间</p><p>inference：</p><p>首先边界框回归产生初始坐标，然后对所有检测框进行IoU-guided NMS，选取出前100最高分类置信度的边界框进行基于优化的边界框微调。</p><h2><b>Experiments</b></h2><p><b>1.IoU-guided NMS</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-5b7d9f3990a35a9509c8aa1b52d148f6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"867\" data-rawheight=\"259\" class=\"origin_image zh-lightbox-thumb\" width=\"867\" data-original=\"https://pic3.zhimg.com/v2-5b7d9f3990a35a9509c8aa1b52d148f6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;867&#39; height=&#39;259&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"867\" data-rawheight=\"259\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"867\" data-original=\"https://pic3.zhimg.com/v2-5b7d9f3990a35a9509c8aa1b52d148f6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-5b7d9f3990a35a9509c8aa1b52d148f6_b.jpg\"/></figure><p>在高IoU 度量下，IoU-guided NMS显著的高于baseline。</p><p>当定位精度要求更高时，IoU-guided NMS和其他方法的差距会更大。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-45f8a11d0951b06f9ebad2e84e2b8c18_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"417\" data-rawheight=\"327\" class=\"content_image\" width=\"417\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;417&#39; height=&#39;327&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"417\" data-rawheight=\"327\" class=\"content_image lazy\" width=\"417\" data-actualsrc=\"https://pic1.zhimg.com/v2-45f8a11d0951b06f9ebad2e84e2b8c18_b.jpg\"/></figure><p>该图是IoU和召回率的关系，No-NMS是召回率的上界，可以看到IoU-NMS的召回率更高，且在更高的IoU下，IoU-NMS与上界的差距越小。</p><p><b>2.Refinement</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-f4340261afecf638e8c887a9d685e4d8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"868\" data-rawheight=\"208\" class=\"origin_image zh-lightbox-thumb\" width=\"868\" data-original=\"https://pic1.zhimg.com/v2-f4340261afecf638e8c887a9d685e4d8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;868&#39; height=&#39;208&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"868\" data-rawheight=\"208\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"868\" data-original=\"https://pic1.zhimg.com/v2-f4340261afecf638e8c887a9d685e4d8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-f4340261afecf638e8c887a9d685e4d8_b.jpg\"/></figure><p>应用边界框微调，使得定位目标更加准确，可以看到在cascade-rcnn这种3阶段的检测器上也有大幅度的提升，尤其是高IoU阈值的情况下。</p><p>3.<b>Joint training</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-5b7d9f3990a35a9509c8aa1b52d148f6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"867\" data-rawheight=\"259\" class=\"origin_image zh-lightbox-thumb\" width=\"867\" data-original=\"https://pic3.zhimg.com/v2-5b7d9f3990a35a9509c8aa1b52d148f6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;867&#39; height=&#39;259&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"867\" data-rawheight=\"259\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"867\" data-original=\"https://pic3.zhimg.com/v2-5b7d9f3990a35a9509c8aa1b52d148f6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-5b7d9f3990a35a9509c8aa1b52d148f6_b.jpg\"/></figure><p>由上表可以看出IoU预测器可以使网络学到更具判别力的特征，IoU-NMS和Refine都能进一步的提升精度。另外，由上表可以看出，在低定位要求时，会得到较差的结果，这是由于IoU的估计错误。当边界框与ground-truth有较低的IoU时，特征上有较大的差异，IoU的评估不准确，使得refinement和suppression的过程会使精度降低。经验上，训练时可以通过采样更多的低IoU的边界框部分的解决这个问题。例如，预测IoU为0.55，然而实际的IoU为0.3。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-5262ea0fab36ab4e2250f4739a68c448_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"862\" data-rawheight=\"67\" class=\"origin_image zh-lightbox-thumb\" width=\"862\" data-original=\"https://pic1.zhimg.com/v2-5262ea0fab36ab4e2250f4739a68c448_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;862&#39; height=&#39;67&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"862\" data-rawheight=\"67\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"862\" data-original=\"https://pic1.zhimg.com/v2-5262ea0fab36ab4e2250f4739a68c448_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-5262ea0fab36ab4e2250f4739a68c448_b.png\"/></figure><p>由上表可知，在计算开销可以容忍的情况下，IoU-Net提升了精度。</p><p><b>Reference:</b></p><p><a href=\"https://zhuanlan.zhihu.com/p/35882192\" class=\"internal\">Naiyan Wang：CVPR18 Detection文章选介（上）</a></p><p><a href=\"https://zhuanlan.zhihu.com/p/45316739\" class=\"internal\">扬之水：目标检测论文阅读：IoU-Net</a></p>", 
            "topic": [
                {
                    "tag": "目标检测", 
                    "tagLink": "https://api.zhihu.com/topics/19596960"
                }, 
                {
                    "tag": "计算机视觉", 
                    "tagLink": "https://api.zhihu.com/topics/19590195"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": [
                {
                    "userName": "不造取什么名字", 
                    "userLink": "https://www.zhihu.com/people/eecb28beccd28dc38d29a3ff2c7d4018", 
                    "content": "<p>请问iou label归一化到[-1,1]之间是什么意思。。。预测值也归一到[-1,1]之间么？ 这样test的时候预测值不是也会在【-1，1】之间么 ，真实的iou 不是应该在[0,1]么，是test的时候再把预测值归一到[0,1]？</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/58202079", 
            "userName": "小白", 
            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
            "upvote": 0, 
            "title": "faster rcnn训练前准备-生成plk训练数据", 
            "content": "<div class=\"highlight\"><pre><code class=\"language-text\">from __future__ import division\nimport sys\nfrom optparse import OptionParser\nimport pickle\nfrom keras import backend as K\nfrom keras_frcnn import config, data_generators\n\nsys.setrecursionlimit(40000)\n\nparser = OptionParser()\n\nparser.add_option(&#34;-p&#34;, &#34;--path&#34;, dest=&#34;train_path&#34;, help=&#34;Path to training data.&#34;, default=&#34;D:/CAR2019_MAKE/train/data&#34;)\nparser.add_option(&#34;-o&#34;, &#34;--parser&#34;, dest=&#34;parser&#34;, help=&#34;Parser to use. One of simple or pascal_voc&#34;,\n                  default=&#34;pascal_voc&#34;)\nparser.add_option(&#34;-n&#34;, &#34;--num_rois&#34;, type=&#34;int&#34;, dest=&#34;num_rois&#34;, help=&#34;Number of RoIs to process at once.&#34;,\n                  default=32)\nparser.add_option(&#34;--network&#34;, dest=&#34;network&#34;, help=&#34;Base network to use. Supports vgg or resnet50.&#34;, default=&#39;vgg&#39;)\nparser.add_option(&#34;--hf&#34;, dest=&#34;horizontal_flips&#34;, help=&#34;Augment with horizontal flips in training. (Default=false).&#34;,\n                  action=&#34;store_true&#34;, default=False)\nparser.add_option(&#34;--vf&#34;, dest=&#34;vertical_flips&#34;, help=&#34;Augment with vertical flips in training. (Default=false).&#34;,\n                  action=&#34;store_true&#34;, default=False)\nparser.add_option(&#34;--rot&#34;, &#34;--rot_90&#34;, dest=&#34;rot_90&#34;,\n                  help=&#34;Augment with 90 degree rotations in training. (Default=false).&#34;,\n                  action=&#34;store_true&#34;, default=False)\nparser.add_option(&#34;--num_epochs&#34;, type=&#34;int&#34;, dest=&#34;num_epochs&#34;, help=&#34;Number of epochs.&#34;, default=2000)\nparser.add_option(&#34;--config_filename&#34;, dest=&#34;config_filename&#34;, help=\n&#34;Location to store all the metadata related to the training (to be used when testing).&#34;,\n                  default=&#34;config.pickle&#34;)\nparser.add_option(&#34;--output_weight_path&#34;, dest=&#34;output_weight_path&#34;, help=&#34;Output path for weights.&#34;,\n                  default=&#39;./model_frcnn.hdf5&#39;)\nparser.add_option(&#34;--input_weight_path&#34;, dest=&#34;input_weight_path&#34;,\n                  help=&#34;Input path for weights. If not specified, will try to load default weights provided by keras.&#34;)\n\n(options, args) = parser.parse_args()\n\nif not options.train_path:  # if filename is not given\n    parser.error(&#39;Error: path to training data must be specified. Pass --path to command line&#39;)\n\nif options.parser == &#39;pascal_voc&#39;:\n    from keras_frcnn.pascal_voc_parser import get_data\nelif options.parser == &#39;simple&#39;:\n    from keras_frcnn.simple_parser import get_data\nelse:\n    raise ValueError(&#34;Command line option parser must be one of &#39;pascal_voc&#39; or &#39;simple&#39;&#34;)\n\n# pass the settings from the command line, and persist them in the config object\nC = config.Config()\n\nC.use_horizontal_flips = bool(options.horizontal_flips)\nC.use_vertical_flips = bool(options.vertical_flips)\nC.rot_90 = bool(options.rot_90)\n\nC.model_path = options.output_weight_path\nC.num_rois = int(options.num_rois)\nfrom keras_frcnn import vgg as nn\n\n\nall_imgs, classes_count, class_mapping = get_data(options.train_path)\ntrain_imgs = [s for s in all_imgs if s[&#39;imageset&#39;] == &#39;trainval&#39;]\ndata_gen_train = data_generators.get_anchor_gt(train_imgs, classes_count, C, nn.get_img_output_length,\n                                               K.image_dim_ordering(), mode=&#39;train&#39;)\n\ni = 0\nwhile True:\n    print(&#39;-----------------------&#39;)\n    X, Y, img_data = next(data_gen_train)\n    temp_dict = {&#39;X&#39;:X,&#39;Y&#39;:Y,&#39;img_data&#39;:img_data}\n    with open(&#39;D:/CAR2019_MAKE/train/plks/&#39;+str(i)+&#39;.plk&#39;, &#39;wb&#39;) as config_f:\n        pickle.dump(temp_dict, config_f)\n    i += 1\n    print(i)</code></pre></div><p>不需要每训练一张图片在计算其信息，每次的计算的信息都是一样的，所以先将这些信息计算好在存放到plk中，后续训练直接调用。</p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/58201592", 
            "userName": "小白", 
            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
            "upvote": 0, 
            "title": "其他-2.读取整个数据集的信息", 
            "content": "<div class=\"highlight\"><pre><code class=\"language-text\">import os\nimport cv2\nimport xml.etree.ElementTree as ET\n\ndef get_data(data_paths,visualise = False):\n\tall_imgs = []\n\n\tclasses_count = {}\n\n\tclass_mapping = {}\n\n\tdata_path = data_paths\n\n\tannot_path = os.path.join(data_path, &#39;xmls&#39;)\n\timgs_path = os.path.join(data_path, &#39;imgs&#39;)\n\timgsets_path_trainval = os.path.join(data_path, &#39;txts&#39;,&#39;train.txt&#39;)\n\n\ttrainval_files = []\n\ttest_files = []\n\ttry:\n\t\twith open(imgsets_path_trainval) as f:\n\t\t\tfor line in f:\n\t\t\t\ttrainval_files.append(line.strip() + &#39;.jpg&#39;)\n\texcept Exception as e:\n\t\tprint(e)\n\n\tannots = [os.path.join(annot_path, s) for s in os.listdir(annot_path)]\n\tprint(annots)\n\tidx = 0\n\tfor annot in annots:\n\t\ttry:\n\t\t\tidx += 1\n\t\t\tet = ET.parse(annot)\n\t\t\telement = et.getroot()\n\n\t\t\telement_objs = element.findall(&#39;object&#39;)\n\t\t\telement_filename = element.find(&#39;filename&#39;).text\n\t\t\telement_width = int(element.find(&#39;size&#39;).find(&#39;width&#39;).text)\n\t\t\telement_height = int(element.find(&#39;size&#39;).find(&#39;height&#39;).text)\n\n\t\t\tif len(element_objs) &gt; 0:\n\t\t\t\tannotation_data = {&#39;filepath&#39;: os.path.join(imgs_path, element_filename), &#39;width&#39;: element_width,\n\t\t\t\t\t\t\t\t   &#39;height&#39;: element_height, &#39;bboxes&#39;: []}\n\n\t\t\t\tif element_filename in trainval_files:\n\t\t\t\t\tannotation_data[&#39;imageset&#39;] = &#39;trainval&#39;\n\t\t\t\telif element_filename in test_files:\n\t\t\t\t\tannotation_data[&#39;imageset&#39;] = &#39;test&#39;\n\t\t\t\telse:\n\t\t\t\t\tannotation_data[&#39;imageset&#39;] = &#39;trainval&#39;\n\n\t\t\tfor element_obj in element_objs:\n\t\t\t\tclass_name = element_obj.find(&#39;name&#39;).text\n\t\t\t\tif class_name not in classes_count:\n\t\t\t\t\tclasses_count[class_name] = 1\n\t\t\t\telse:\n\t\t\t\t\tclasses_count[class_name] += 1\n\n\t\t\t\tif class_name not in class_mapping:\n\t\t\t\t\tclass_mapping[class_name] = len(class_mapping)\n\n\t\t\t\tobj_bbox = element_obj.find(&#39;bndbox&#39;)\n\t\t\t\tx1 = int(round(float(obj_bbox.find(&#39;xmin&#39;).text)))\n\t\t\t\ty1 = int(round(float(obj_bbox.find(&#39;ymin&#39;).text)))\n\t\t\t\tx2 = int(round(float(obj_bbox.find(&#39;xmax&#39;).text)))\n\t\t\t\ty2 = int(round(float(obj_bbox.find(&#39;ymax&#39;).text)))\n\t\t\t\tdifficulty = int(element_obj.find(&#39;difficult&#39;).text) == 1\n\t\t\t\tannotation_data[&#39;bboxes&#39;].append(\n\t\t\t\t\t{&#39;class&#39;: class_name, &#39;x1&#39;: x1, &#39;x2&#39;: x2, &#39;y1&#39;: y1, &#39;y2&#39;: y2, &#39;difficult&#39;: difficulty})\n\t\t\tall_imgs.append(annotation_data)\n\n\t\t\tif visualise:\n\t\t\t\timg = cv2.imread(annotation_data[&#39;filepath&#39;])\n\t\t\t\tfor bbox in annotation_data[&#39;bboxes&#39;]:\n\t\t\t\t\tcv2.rectangle(img, (bbox[&#39;x1&#39;], bbox[&#39;y1&#39;]), (bbox[&#39;x2&#39;], bbox[&#39;y2&#39;]), (0, 0, 255))\n\t\t\t\tcv2.imshow(&#39;img&#39;, img)\n\t\t\t\tcv2.waitKey(0)\n\t\texcept Exception as e:\n\t\t\tprint(e)\n\t\t\tcontinue\n\treturn all_imgs, classes_count, class_mapping\n</code></pre></div><a href=\"https://zhuanlan.zhihu.com/p/31468683\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-6b1b5e6540c0c74b94641d9d57a2c824_ipico.jpg\" data-image-width=\"490\" data-image-height=\"612\" class=\"internal\">小白：XML文件读取与VOC数据集使用（pascal_voc_parser.py）</a><p>这个有些地方要稍微改一下，其他差不多。</p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/58179246", 
            "userName": "小白", 
            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
            "upvote": 1, 
            "title": "其他-1.显示xml文件中标定框", 
            "content": "<div class=\"highlight\"><pre><code class=\"language-text\">#显示xml是否准确\nimport xml.etree.ElementTree as ET\nimport cv2\nimport os\n\nxml_path = &#39;D:/CAR2019_MAKE/train/data/xmls&#39;\nimg_path = &#39;D:/CAR2019_MAKE/train/data/imgs&#39;\n\nfor name in os.listdir(xml_path):\n    print(&#39;name&#39;, name)\n    #打开xml文档\n    tree = ET.parse(os.path.join(xml_path,name))\n\n    img = cv2.imread(os.path.join(img_path,name.split(&#39;.&#39;)[0]+&#39;.jpg&#39;))\n\n    #得到文档元素对象\n    root = tree.getroot()\n    allObjects = root.findall(&#39;object&#39;)\n    for i in range(len(allObjects)):\n        object = allObjects[i]\n        objectName = object.find(&#39;name&#39;).text\n        if objectName == &#39;car&#39; :\n            xmin = int(object.find(&#39;bndbox&#39;).find(&#39;xmin&#39;).text)\n            ymin = int(object.find(&#39;bndbox&#39;).find(&#39;ymin&#39;).text)\n            xmax = int(object.find(&#39;bndbox&#39;).find(&#39;xmax&#39;).text)\n            ymax = int(object.find(&#39;bndbox&#39;).find(&#39;ymax&#39;).text)\n            cv2.rectangle(img,(xmin, ymin),(xmax, ymax),[255,255,255])\n\n        if objectName == &#39;bigcar&#39;:\n            xmin = int(object.find(&#39;bndbox&#39;).find(&#39;xmin&#39;).text)\n            ymin = int(object.find(&#39;bndbox&#39;).find(&#39;ymin&#39;).text)\n            xmax = int(object.find(&#39;bndbox&#39;).find(&#39;xmax&#39;).text)\n            ymax = int(object.find(&#39;bndbox&#39;).find(&#39;ymax&#39;).text)\n            cv2.rectangle(img,(xmin, ymin),(xmax, ymax),[255,255,0])\n\n        if objectName == &#39;other&#39;:\n            xmin = int(object.find(&#39;bndbox&#39;).find(&#39;xmin&#39;).text)\n            ymin = int(object.find(&#39;bndbox&#39;).find(&#39;ymin&#39;).text)\n            xmax = int(object.find(&#39;bndbox&#39;).find(&#39;xmax&#39;).text)\n            ymax = int(object.find(&#39;bndbox&#39;).find(&#39;ymax&#39;).text)\n            cv2.rectangle(img,(xmin, ymin),(xmax, ymax),[0,0,0])\n\n    cv2.imshow(&#39;img&#39;,img)\n    cv2.waitKey()</code></pre></div><p></p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/58171633", 
            "userName": "小白", 
            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
            "upvote": 0, 
            "title": "图像预处理-2生成检测数据", 
            "content": "<div class=\"highlight\"><pre><code class=\"language-text\">#日期：2019-03-03\n#作者：huang guo jie\n#功能：生成训练faster rcnn要用的数据\n\nimport xml.dom.minidom\nimport cv2\nimport numpy as np\nfrom PIL import Image\nimport os\n\ndef create_xml(filename,img_width,img_height,boxes,save_path):\n    &#39;&#39;&#39;\n    Record the information of a picture and the box information it contains in the xml file\n    :param filename:input image name\n    :param img_width: input image width\n    :param img_height: input image height\n    :param boxes:Boxes is a list, a record is a dict containing xmin, ymin, xmax, ymax four keywords\n    (xmin, ymin: coordinates of the upper-left point, xmax, ymax: coordinates of the lower-right point)\n    :param save_path: Should include the file name\n    :return:None\n    &#39;&#39;&#39;\n    doc = xml.dom.minidom.Document()\n    root = doc.createElement(&#39;annotation&#39;)\n    doc.appendChild(root)\n    nodeSize = doc.createElement(&#39;size&#39;)\n    nodeWidth = doc.createElement(&#39;width&#39;)\n    nodeWidth.appendChild(doc.createTextNode(str(img_width)))\n    nodeHeight = doc.createElement(&#39;height&#39;)\n    nodeHeight.appendChild(doc.createTextNode(str(img_height)))\n    nodeSize.appendChild(nodeWidth)\n    nodeSize.appendChild(nodeHeight)\n    nodeFilename = doc.createElement(&#39;filename&#39;)\n    nodeFilename.appendChild(doc.createTextNode(filename))\n    root.appendChild(nodeFilename)\n    root.appendChild(nodeSize)\n    for box in boxes:\n        nodeObject = doc.createElement(&#39;object&#39;)\n        nodeName = doc.createElement(&#39;name&#39;)\n        nodeName.appendChild(doc.createTextNode(&#39;car&#39;))\n        nodeDifficult = doc.createElement(&#39;difficult&#39;)\n        nodeDifficult.appendChild(doc.createTextNode(&#39;0&#39;))\n        nodeBndbox = doc.createElement(&#39;bndbox&#39;)\n        nodeXmin = doc.createElement(&#39;xmin&#39;)\n        nodeXmin.appendChild(doc.createTextNode(str(box[&#39;xmin&#39;])))\n        nodeYmin = doc.createElement(&#39;ymin&#39;)\n        nodeYmin.appendChild(doc.createTextNode(str(box[&#39;ymin&#39;])))\n        nodeXmax = doc.createElement(&#39;xmax&#39;)\n        nodeXmax.appendChild(doc.createTextNode(str(box[&#39;xmax&#39;])))\n        nodeYmax = doc.createElement(&#39;ymax&#39;)\n        nodeYmax.appendChild(doc.createTextNode(str(box[&#39;ymax&#39;])))\n        nodeBndbox.appendChild(nodeXmin)\n        nodeBndbox.appendChild(nodeYmin)\n        nodeBndbox.appendChild(nodeXmax)\n        nodeBndbox.appendChild(nodeYmax)\n        nodeObject.appendChild(nodeDifficult)\n        nodeObject.appendChild(nodeName)\n        nodeObject.appendChild(nodeBndbox)\n        root.appendChild(nodeObject)\n    fp = open(save_path, &#39;w&#39;)\n    doc.writexml(fp, indent=&#39;\\t&#39;, addindent=&#39;\\t&#39;, newl=&#39;\\n&#39;, encoding=&#34;utf-8&#34;)\n    \ndef write_txt(write_path,write_content):\n    f = open(write_path,&#39;a&#39;)\n    f.write(write_content+&#39;\\n&#39;)\n    f.close()\n    \ndef intersection(ai, bi):\n    x = max(ai[0], bi[0])\n    y = max(ai[1], bi[1])\n    w = min(ai[2], bi[2]) - x\n    h = min(ai[3], bi[3]) - y\n    if w &lt; 0 or h &lt; 0:\n        return 0\n    return w * h\n\ndef random_choice_samples(mylist,num):\n    choice_nums = np.random.choice(len(mylist),num)\n    temp = []\n    for i in choice_nums:\n        temp.append(mylist[i])\n    return temp\n\ndef create_img(car_f,background_path,train_data_path,name):\n    c_width = 512\n    car_paths = []\n    background = cv2.imread(background_path)\n    background_height,background_width = background.shape[0],background.shape[1]\n    if background_height&gt;c_width:\n        background = background[:c_width,:,:]\n    if background_width&gt;1024:\n        background = background[:,:1024,:]\n    background_height, background_width = background.shape[0], background.shape[1]\n\n    for f in os.listdir(car_f):\n        car_f_colors = [os.path.join(car_f,f)]\n        car_path_temp = []\n        for car_f_color in car_f_colors:\n            for car_name in os.listdir(car_f_color):\n                car_path_temp.append(os.path.join(car_f_color,car_name))\n            car_paths.append(car_path_temp)\n    n = np.random.choice([4,8,16,32,64],1)[0]\n    choice_samples = random_choice_samples(car_paths[0],int(n/4))+random_choice_samples(car_paths[1],int(n/4))+random_choice_samples(car_paths[2],int(n/4))+random_choice_samples(car_paths[3],1)\n\n    boxes = []\n    box_dict = {}\n    for sample in choice_samples:\n        car = cv2.imread(sample)\n        car = Image.fromarray(car)\n        degree = np.random.choice(360,1)\n        car = car.rotate(degree[0],expand=1)\n        car = np.array(car)\n        car_height,car_width = car.shape[0],car.shape[1]\n\n        flag = False\n        while flag==False:\n            xmin = np.random.choice(background_width-car_width)\n            ymin = np.random.choice(background_height-car_height)\n            box_dict[&#39;xmin&#39;] = xmin\n            box_dict[&#39;ymin&#39;] = ymin\n            box_dict[&#39;xmax&#39;] =  xmin + car_width\n            box_dict[&#39;ymax&#39;] =  ymin + car_height\n            flag = True\n\n\n            for box in boxes:\n                area = intersection([box_dict[&#39;xmin&#39;], box_dict[&#39;ymin&#39;], box_dict[&#39;xmax&#39;], box_dict[&#39;ymax&#39;]],\n                             [box[&#39;xmin&#39;], box[&#39;ymin&#39;], box[&#39;xmax&#39;], box[&#39;ymax&#39;]])\n                if area&gt;0:\n                    flag = False\n                    continue\n\n        boxes.append({&#39;xmin&#39;:xmin,&#39;ymin&#39;:ymin,&#39;xmax&#39;:xmin + car_width,&#39;ymax&#39;:ymin + car_height})\n\n        for x in range(car_width):\n            for y in range(car_height):\n                if sum(car[y,x,:])!=0:\n                    background[ymin+y,xmin+x,:] = car[y,x,:]\n\n    background_height, background_width = background.shape[0], background.shape[1]\n    if background_height&lt;c_width:\n        background_zeros = np.zeros(shape=(c_width,background_width,3))\n        background_zeros[:background_height,:background_width,:] = background\n        background = background_zeros\n    background_height, background_width = background.shape[0], background.shape[1]\n    if background_width&lt;1024:\n        background_zeros = np.zeros(shape=(c_width,1024,3))\n        background_zeros[:background_height, :background_width, :] = background\n        background = background_zeros\n\n\n    create_xml(filename=name+&#39;.jpg&#39;,img_width=background_width,img_height=background_height,boxes=boxes,save_path=train_data_path+&#39;/xmls/&#39;+name+&#39;.xml&#39;)\n    cv2.imwrite(train_data_path+&#39;/imgs/&#39;+name+&#39;.jpg&#39;,background)\n    write_txt(train_data_path+&#39;/txts/train.txt&#39;,name)\n    \nbackground_path = &#39;C:/Users/huang/Desktop/CAR2019_MAKE/train/img_rw_background&#39;\ncars_path = &#39;C:/Users/huang/Desktop/CAR2019_MAKE/train/cars&#39;\nsave_path = &#39;C:/Users/huang/Desktop/CAR2019_MAKE/train/data&#39;\nflag = 0\nfor background_name in os.listdir(background_path):\n    background_path_one = os.path.join(background_path,background_name)\n    for _ in range(15):\n        flag += 1\n        create_img(car_f=cars_path, background_path=background_path_one,train_data_path=save_path, name=str(flag))\n        print(flag)</code></pre></div><hr/><p><b>background_path</b></p><p>即为没有车辆仅存在背景的图片路径</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-f3be0e755ac5adff9f003bf6fb7fff2c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"812\" data-rawheight=\"328\" class=\"origin_image zh-lightbox-thumb\" width=\"812\" data-original=\"https://pic1.zhimg.com/v2-f3be0e755ac5adff9f003bf6fb7fff2c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;812&#39; height=&#39;328&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"812\" data-rawheight=\"328\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"812\" data-original=\"https://pic1.zhimg.com/v2-f3be0e755ac5adff9f003bf6fb7fff2c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-f3be0e755ac5adff9f003bf6fb7fff2c_b.jpg\"/></figure><p><b>cars_path</b></p><p>各种汽车图片在的路径</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-dc41f785ee2f68967619f8aeb2a02b3c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"708\" data-rawheight=\"230\" class=\"origin_image zh-lightbox-thumb\" width=\"708\" data-original=\"https://pic1.zhimg.com/v2-dc41f785ee2f68967619f8aeb2a02b3c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;708&#39; height=&#39;230&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"708\" data-rawheight=\"230\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"708\" data-original=\"https://pic1.zhimg.com/v2-dc41f785ee2f68967619f8aeb2a02b3c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-dc41f785ee2f68967619f8aeb2a02b3c_b.jpg\"/></figure><p><b>save_path</b></p><p>保存生成的图片和各种信息路径</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-bacd4d21adc82d59f35f3e4f47855033_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"494\" data-rawheight=\"217\" class=\"origin_image zh-lightbox-thumb\" width=\"494\" data-original=\"https://pic4.zhimg.com/v2-bacd4d21adc82d59f35f3e4f47855033_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;494&#39; height=&#39;217&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"494\" data-rawheight=\"217\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"494\" data-original=\"https://pic4.zhimg.com/v2-bacd4d21adc82d59f35f3e4f47855033_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-bacd4d21adc82d59f35f3e4f47855033_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/58122798", 
            "userName": "小白", 
            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
            "upvote": 0, 
            "title": "图片预处理-1.将车辆剪裁下来", 
            "content": "<div class=\"highlight\"><pre><code class=\"language-text\">#日期：2019-03-02\n#作者：huang guo jie\n#功能：将car,bigcar,other从图片中剪裁下来，并将原图中的非car对象替换为黑色\n\nimport xml.etree.ElementTree as ET\nimport cv2\nimport os\n\ntrain_data_xml_path = &#39;C:/Users/huang/Desktop/CAR2019/train/xml&#39;\npositive_path = &#39;C:/Users/huang/Desktop/CAR2019_MAKE/train/single_cell/positive&#39;\nnegative_path = &#39;C:/Users/huang/Desktop/CAR2019_MAKE/train/single_cell/negative&#39;\nimg_rw_background_save_path = &#39;C:/Users/huang/Desktop/CAR2019_MAKE/train/img_rw_background&#39;\nimg_rw_path = &#39;C:/Users/huang/Desktop/CAR2019_MAKE/train/img_rw&#39;\ncar_index = 0\nindex = 0\n\nfor name in os.listdir(train_data_xml_path):\n    print(&#39;name&#39;, name)\n    img = cv2.imread(os.path.join(img_rw_path, name.split(&#39;.&#39;)[0] + &#39;.jpg&#39;))\n    #打开xml文档\n    tree = ET.parse(os.path.join(train_data_xml_path,name))\n\n    #得到文档元素对象\n    root = tree.getroot()\n    allObjects = root.findall(&#39;object&#39;)\n\n    for i in range(len(allObjects)):\n        object = allObjects[i]\n        objectName = object.find(&#39;name&#39;).text\n\n        if objectName == &#39;car&#39; or objectName == &#39;bigcar&#39; or objectName == &#39;other&#39;:\n            xmin = int(object.find(&#39;bndbox&#39;).find(&#39;xmin&#39;).text)\n            ymin = int(object.find(&#39;bndbox&#39;).find(&#39;ymin&#39;).text)\n            xmax = int(object.find(&#39;bndbox&#39;).find(&#39;xmax&#39;).text)\n            ymax = int(object.find(&#39;bndbox&#39;).find(&#39;ymax&#39;).text)\n            img[ymin:ymax, xmin:xmax, :] = 0\n\n            if objectName == &#39;car&#39;:\n                car_index += 1\n                car = img[ymin:ymax,xmin:xmax,:]\n                cv2.imwrite(os.path.join(positive_path, str(car_index) +&#39;.jpg&#39;),car)\n            else:\n                index +=  1\n                other = img[ymin:ymax,xmin:xmax,:]\n                cv2.imwrite(os.path.join(negative_path, str(index) + &#39;.jpg&#39;), other)\n                \n    cv2.imwrite(os.path.join(img_rw_background_save_path, name.split(&#39;.&#39;)[0] + &#39;.jpg&#39;), img)</code></pre></div><p></p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/57294296", 
            "userName": "小白", 
            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
            "upvote": 0, 
            "title": "阿里云使用（5）", 
            "content": "<p>1，源码目录结构</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-e78e8565e826da5374d6801922c763f7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"414\" data-rawheight=\"210\" class=\"content_image\" width=\"414\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;414&#39; height=&#39;210&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"414\" data-rawheight=\"210\" class=\"content_image lazy\" width=\"414\" data-actualsrc=\"https://pic4.zhimg.com/v2-e78e8565e826da5374d6801922c763f7_b.jpg\"/></figure><p>2，index.html文件</p><div class=\"highlight\"><pre><code class=\"language-text\">&lt;div class=&#34;file-box&#34;&gt;\n    &lt;img id=&#34;preview&#34; /&gt;\n    &lt;input type=&#34;text&#34; id=&#34;imgfield&#34; class=&#34;txt&#34; placeholder=&#34;预览&#34;&gt;\n    &lt;input type=&#34;file&#34; name=&#34;file&#34; id = &#34;input_file&#34; accept=&#34;image/gif,image/jpeg,image/jpg,image/png,image/svg&#34; onchange=&#34;imgPreview(this)&#34; &gt;  \n&lt;/div&gt;\n\n&lt;table id=&#34;table-result&#34; width=&#34;80%&#34; align=&#34;center&#34; border=&#34;1&#34;&gt;\n    &lt;tr&gt;\n        &lt;th&gt;标签&lt;/th&gt;\n        &lt;th&gt;概率&lt;/th&gt;\n    &lt;/tr&gt;\n    &lt;tbody id=&#34;tbody-result&#34;&gt;\n    &lt;/tbody&gt;\n&lt;/table&gt;\n\n\n&lt;script src=&#34;https://cdn.staticfile.org/jquery/1.10.2/jquery.min.js&#34;&gt;\n&lt;/script&gt;\n&lt;script type=&#34;text/javascript&#34;&gt;\n\tfunction imgPreview(fileDom) {\n\t\t//判断是否支持FileReader\n\t\tif(window.FileReader) {\n\t\t\tvar reader = new FileReader();\n\t\t} else {\n\t\t\talert(&#34;您的设备不支持图片预览功能，如需该功能请升级您的设备！&#34;);\n\t\t}\n\t\t//获取文件\n\t\tvar file = fileDom.files[0];\n\t\tvar imageType = /^image\\//;\n\t\t//是否是图片\n\t\tif(!imageType.test(file.type)) {\n\t\t\talert(&#34;请选择图片！&#34;);\n\t\t\treturn;\n\t\t}\n\t\t//读取完成\n\t\treader.onload = function(e) {\n\t\t\t//获取图片dom\n\t\t\tvar img = document.getElementById(&#34;preview&#34;);\n\t\t\t//图片路径设置为读取的图片\n\t\t\timg.src = e.target.result;\n\t\t};\n\t\treader.readAsDataURL(file);\n\t\t\n\t\tvar tbody = window.document.getElementById(&#34;tbody-result&#34;);\n\t\tvar formData = new FormData(); \n\t\tformData.append(&#39;image&#39;, $(&#39;#input_file&#39;)[0].files[0]);  //添加图片信息的参数\n\t\tformData.append(&#39;sizeid&#39;,123);  //添加其他参数\n\t\t$.ajax({\n\t\t\turl: &#39;/predict&#39;,\n\t\t\ttype: &#39;POST&#39;,\n\t\t\tcache: false, //上传文件不需要缓存\n\t\t\tdata: formData,\n\t\t\tprocessData: false, // 告诉jQuery不要去处理发送的数据\n\t\t\tcontentType: false, // 告诉jQuery不要去设置Content-Type请求头\n\t\t\tsuccess: function (obj) {\n\t\t\tvar description = &#34;&#34;;\n\t\t\tvar str = &#34;&#34;;\n\t\t\tfor(var i in obj[&#34;predictions&#34;]){ \n\t\t\t\n\t\t\t str += &#34;&lt;tr&gt;&#34; +\n                            &#34;&lt;td align=&#39;center&#39;&gt;&#34; + obj[&#34;predictions&#34;][i][&#34;label&#34;] + &#34;&lt;/td&gt;&#34; +\n                            &#34;&lt;td align=&#39;center&#39;&gt;&#34; + obj[&#34;predictions&#34;][i][&#34;probability&#34;] + &#34;&lt;/td&gt;&#34; +\n                            &#34;&lt;/tr&gt;&#34;;\n            }\n            tbody.innerHTML = str;\n\n\t\t},\n\t\terror: function () {\n\t\t\talert(&#34;查询失败&#34;)\n\t\t}\n\t\t})  \n\n\t}\n&lt;/script&gt;\n\n</code></pre></div><p>3，run_model_server.py做的修改</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-c1fae8797fcdae0a7f94484ba652d804_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"441\" data-rawheight=\"93\" class=\"origin_image zh-lightbox-thumb\" width=\"441\" data-original=\"https://pic1.zhimg.com/v2-c1fae8797fcdae0a7f94484ba652d804_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;441&#39; height=&#39;93&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"441\" data-rawheight=\"93\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"441\" data-original=\"https://pic1.zhimg.com/v2-c1fae8797fcdae0a7f94484ba652d804_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-c1fae8797fcdae0a7f94484ba652d804_b.jpg\"/></figure><p>4，效果</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-fa101e193b5d47971f5d974d5458b4e2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1214\" data-rawheight=\"724\" class=\"origin_image zh-lightbox-thumb\" width=\"1214\" data-original=\"https://pic3.zhimg.com/v2-fa101e193b5d47971f5d974d5458b4e2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1214&#39; height=&#39;724&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1214\" data-rawheight=\"724\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1214\" data-original=\"https://pic3.zhimg.com/v2-fa101e193b5d47971f5d974d5458b4e2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-fa101e193b5d47971f5d974d5458b4e2_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "阿里云", 
                    "tagLink": "https://api.zhihu.com/topics/19560108"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/57097618", 
            "userName": "小白", 
            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
            "upvote": 0, 
            "title": "阿里云使用（4）", 
            "content": "<p><b>step1:</b></p><p>文件settings.py</p><div class=\"highlight\"><pre><code class=\"language-text\"># initialize Redis connection settings\nREDIS_HOST = &#34;localhost&#34;\nREDIS_PORT = 6379\nREDIS_DB = 0\n\n# initialize constants used to control image spatial dimensions and\n# data type\nIMAGE_WIDTH = 224\nIMAGE_HEIGHT = 224\nIMAGE_CHANS = 3\nIMAGE_DTYPE = &#34;float32&#34;\n\n# initialize constants used for server queuing\nIMAGE_QUEUE = &#34;image_queue&#34;\nBATCH_SIZE = 32\nSERVER_SLEEP = 0.25\nCLIENT_SLEEP = 0.25</code></pre></div><p>文件helpers.py</p><div class=\"highlight\"><pre><code class=\"language-text\"># import the necessary packages\nimport numpy as np\nimport base64\nimport sys\n\ndef base64_encode_image(a):\n\t# base64 encode the input NumPy array\n\treturn base64.b64encode(a).decode(&#34;utf-8&#34;)\n\ndef base64_decode_image(a, dtype, shape):\n\t# if this is Python 3, we need the extra step of encoding the\n\t# serialized NumPy string as a byte object\n\tif sys.version_info.major == 3:\n\t\ta = bytes(a, encoding=&#34;utf-8&#34;)\n\n\t# convert the string to a NumPy array using the supplied data\n\t# type and target shape\n\ta = np.frombuffer(base64.decodestring(a), dtype=dtype)\n\ta = a.reshape(shape)\n\n\t# return the decoded image\n\treturn a</code></pre></div><p>文件run_web_server.py</p><div class=\"highlight\"><pre><code class=\"language-text\"># import the necessary packages\nfrom keras.preprocessing.image import img_to_array\nfrom keras.applications import imagenet_utils\nfrom PIL import Image\nimport numpy as np\nimport settings\nimport helpers\nimport flask\nimport redis\nimport uuid\nimport time\nimport json\nimport io\n\n# initialize our Flask application and Redis server\napp = flask.Flask(__name__)\ndb = redis.StrictRedis(host=settings.REDIS_HOST,\n\tport=settings.REDIS_PORT, db=settings.REDIS_DB)\n\ndef prepare_image(image, target):\n\t# if the image mode is not RGB, convert it\n\tif image.mode != &#34;RGB&#34;:\n\t\timage = image.convert(&#34;RGB&#34;)\n\n\t# resize the input image and preprocess it\n\timage = image.resize(target)\n\timage = img_to_array(image)\n\timage = np.expand_dims(image, axis=0)\n\timage = imagenet_utils.preprocess_input(image)\n\n\t# return the processed image\n\treturn image\n\n@app.route(&#34;/&#34;)\ndef homepage():\n\treturn &#34;Welcome to the PyImageSearch Keras REST API!&#34;\n\n@app.route(&#34;/predict&#34;, methods=[&#34;POST&#34;])\ndef predict():\n\t# initialize the data dictionary that will be returned from the\n\t# view\n\tdata = {&#34;success&#34;: False}\n\n\t# ensure an image was properly uploaded to our endpoint\n\tif flask.request.method == &#34;POST&#34;:\n\t\tif flask.request.files.get(&#34;image&#34;):\n\t\t\t# read the image in PIL format and prepare it for\n\t\t\t# classification\n\t\t\timage = flask.request.files[&#34;image&#34;].read()\n\t\t\timage = Image.open(io.BytesIO(image))\n\t\t\timage = prepare_image(image,\n\t\t\t\t(settings.IMAGE_WIDTH, settings.IMAGE_HEIGHT))\n\n\t\t\t# ensure our NumPy array is C-contiguous as well,\n\t\t\t# otherwise we won&#39;t be able to serialize it\n\t\t\timage = image.copy(order=&#34;C&#34;)\n\n\t\t\t# generate an ID for the classification then add the\n\t\t\t# classification ID + image to the queue\n\t\t\tk = str(uuid.uuid4())\n\t\t\timage = helpers.base64_encode_image(image)\n\t\t\td = {&#34;id&#34;: k, &#34;image&#34;: image}\n\t\t\tdb.rpush(settings.IMAGE_QUEUE, json.dumps(d))\n\n\t\t\t# keep looping until our model server returns the output\n\t\t\t# predictions\n\t\t\twhile True:\n\t\t\t\t# attempt to grab the output predictions\n\t\t\t\toutput = db.get(k)\n\n\t\t\t\t# check to see if our model has classified the input\n\t\t\t\t# image\n\t\t\t\tif output is not None:\n\t\t\t\t\t# add the output predictions to our data\n\t\t\t\t\t# dictionary so we can return it to the client\n\t\t\t\t\toutput = output.decode(&#34;utf-8&#34;)\n\t\t\t\t\tdata[&#34;predictions&#34;] = json.loads(output)\n\n\t\t\t\t\t# delete the result from the database and break\n\t\t\t\t\t# from the polling loop\n\t\t\t\t\tdb.delete(k)\n\t\t\t\t\tbreak\n\n\t\t\t\t# sleep for a small amount to give the model a chance\n\t\t\t\t# to classify the input image\n\t\t\t\ttime.sleep(settings.CLIENT_SLEEP)\n\n\t\t\t# indicate that the request was a success\n\t\t\tdata[&#34;success&#34;] = True\n\n\t# return the data dictionary as a JSON response\n\treturn flask.jsonify(data)\n\n# for debugging purposes, it&#39;s helpful to start the Flask testing\n# server (don&#39;t use this for production\nif __name__ == &#34;__main__&#34;:\n\tprint(&#34;* Starting web service...&#34;)\n\tapp.run()</code></pre></div><p>文件run_model_server.py</p><div class=\"highlight\"><pre><code class=\"language-text\"># import the necessary packages\nfrom keras.applications import ResNet50\nfrom keras.applications import imagenet_utils\nimport numpy as np\nimport settings\nimport helpers\nimport redis\nimport time\nimport json\n\n# connect to Redis server\ndb = redis.StrictRedis(host=settings.REDIS_HOST,\n\tport=settings.REDIS_PORT, db=settings.REDIS_DB)\n\ndef classify_process():\n\t# load the pre-trained Keras model (here we are using a model\n\t# pre-trained on ImageNet and provided by Keras, but you can\n\t# substitute in your own networks just as easily)\n\tprint(&#34;* Loading model...&#34;)\n\tmodel = ResNet50(weights=&#34;imagenet&#34;)\n\tprint(&#34;* Model loaded&#34;)\n\n\t# continually pool for new images to classify\n\twhile True:\n\t\t# attempt to grab a batch of images from the database, then\n\t\t# initialize the image IDs and batch of images themselves\n\t\tqueue = db.lrange(settings.IMAGE_QUEUE, 0,\n\t\t\tsettings.BATCH_SIZE - 1)\n\t\timageIDs = []\n\t\tbatch = None\n\n\t\t# loop over the queue\n\t\tfor q in queue:\n\t\t\t# deserialize the object and obtain the input image\n\t\t\tq = json.loads(q.decode(&#34;utf-8&#34;))\n\t\t\timage = helpers.base64_decode_image(q[&#34;image&#34;],\n\t\t\t\tsettings.IMAGE_DTYPE,\n\t\t\t\t(1, settings.IMAGE_HEIGHT, settings.IMAGE_WIDTH,\n\t\t\t\t\tsettings.IMAGE_CHANS))\n\n\t\t\t# check to see if the batch list is None\n\t\t\tif batch is None:\n\t\t\t\tbatch = image\n\n\t\t\t# otherwise, stack the data\n\t\t\telse:\n\t\t\t\tbatch = np.vstack([batch, image])\n\n\t\t\t# update the list of image IDs\n\t\t\timageIDs.append(q[&#34;id&#34;])\n\n\t\t# check to see if we need to process the batch\n\t\tif len(imageIDs) &gt; 0:\n\t\t\t# classify the batch\n\t\t\tprint(&#34;* Batch size: {}&#34;.format(batch.shape))\n\t\t\tpreds = model.predict(batch)\n\t\t\tresults = imagenet_utils.decode_predictions(preds)\n\n\t\t\t# loop over the image IDs and their corresponding set of\n\t\t\t# results from our model\n\t\t\tfor (imageID, resultSet) in zip(imageIDs, results):\n\t\t\t\t# initialize the list of output predictions\n\t\t\t\toutput = []\n\n\t\t\t\t# loop over the results and add them to the list of\n\t\t\t\t# output predictions\n\t\t\t\tfor (imagenetID, label, prob) in resultSet:\n\t\t\t\t\tr = {&#34;label&#34;: label, &#34;probability&#34;: float(prob)}\n\t\t\t\t\toutput.append(r)\n\n\t\t\t\t# store the output predictions in the database, using\n\t\t\t\t# the image ID as the key so we can fetch the results\n\t\t\t\tdb.set(imageID, json.dumps(output))\n\n\t\t\t# remove the set of images from our queue\n\t\t\tdb.ltrim(settings.IMAGE_QUEUE, len(imageIDs), -1)\n\n\t\t# sleep for a small amount\n\t\ttime.sleep(settings.SERVER_SLEEP)\n\n# if this is the main thread of execution start the model server\n# process\nif __name__ == &#34;__main__&#34;:\n\tclassify_process()</code></pre></div><p>文件keras_rest_api_app.wsgi</p><div class=\"highlight\"><pre><code class=\"language-text\"># add our app to the system path\nimport sys\nsys.path.insert(0, &#34;/var/www/html/keras-complete-rest-api&#34;)\n\n# import the application and away we go...\nfrom run_web_server import app as application</code></pre></div><p>文件stress_test.py</p><div class=\"highlight\"><pre><code class=\"language-text\"># USAGE\n# python stress_test.py\n\n# import the necessary packages\nfrom threading import Thread\nimport requests\nimport time\n\n# initialize the Keras REST API endpoint URL along with the input\n# image path\nKERAS_REST_API_URL = &#34;http://localhost/predict&#34;\nIMAGE_PATH = &#34;jemma.png&#34;\n\n# initialize the number of requests for the stress test along with\n# the sleep amount between requests\nNUM_REQUESTS = 500\nSLEEP_COUNT = 0.05\n\ndef call_predict_endpoint(n):\n\t# load the input image and construct the payload for the request\n\timage = open(IMAGE_PATH, &#34;rb&#34;).read()\n\tpayload = {&#34;image&#34;: image}\n\n\t# submit the request\n\tr = requests.post(KERAS_REST_API_URL, files=payload).json()\n\n\t# ensure the request was sucessful\n\tif r[&#34;success&#34;]:\n\t\tprint(&#34;[INFO] thread {} OK&#34;.format(n))\n\n\t# otherwise, the request failed\n\telse:\n\t\tprint(&#34;[INFO] thread {} FAILED&#34;.format(n))\n\n# loop over the number of threads\nfor i in range(0, NUM_REQUESTS):\n\t# start a new thread to call the API\n\tt = Thread(target=call_predict_endpoint, args=(i,))\n\tt.daemon = True\n\tt.start()\n\ttime.sleep(SLEEP_COUNT)\n\n# insert a long sleep so we can wait until the server is finished\n# processing the images\ntime.sleep(300)</code></pre></div><hr/><p><b>step2:</b></p><p>安装redis</p><a href=\"https://zhuanlan.zhihu.com/p/57083152\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\"internal\">小白：阿里云使用(3)</a><p>安装pip3</p><div class=\"highlight\"><pre><code class=\"language-text\">sudo apt-get install python3-pip</code></pre></div><p>pip是默认安装的软件在python2下的，pip3默认安装在python3下的</p><p>安装如下软件</p><div class=\"highlight\"><pre><code class=\"language-text\">pip3 install numpy\npip3 install scipy h5py\npip3 install tensorflow \npip3 install keras\npip3 install flask gevent\npip3 install imutils requests\npip3 install redis\npip3 install Pillow</code></pre></div><p>修改默认python为python3</p><div class=\"highlight\"><pre><code class=\"language-text\">cd /usr/bin\nsudo rm -rf python\nsudo ln -s /usr/bin/python3  /usr/bin/python</code></pre></div><hr/><p><b>step3:</b></p><p>按装apache和wsgi（命令分开执行）</p><div class=\"highlight\"><pre><code class=\"language-text\">sudo apt-get install apache2\nsudo apt-get install libapache2-mod-wsgi-py3\nsudo a2enmod wsgi</code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-43f8e0f172f9eedcd6f6f4874b39ec6f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"673\" data-rawheight=\"554\" class=\"origin_image zh-lightbox-thumb\" width=\"673\" data-original=\"https://pic4.zhimg.com/v2-43f8e0f172f9eedcd6f6f4874b39ec6f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;673&#39; height=&#39;554&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"673\" data-rawheight=\"554\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"673\" data-original=\"https://pic4.zhimg.com/v2-43f8e0f172f9eedcd6f6f4874b39ec6f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-43f8e0f172f9eedcd6f6f4874b39ec6f_b.jpg\"/></figure><hr/><p>step4:</p><p>var在该目录下，而不在root目录下</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3e186d7182e6055061c30de51d22c59d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"496\" data-rawheight=\"221\" class=\"origin_image zh-lightbox-thumb\" width=\"496\" data-original=\"https://pic2.zhimg.com/v2-3e186d7182e6055061c30de51d22c59d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;496&#39; height=&#39;221&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"496\" data-rawheight=\"221\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"496\" data-original=\"https://pic2.zhimg.com/v2-3e186d7182e6055061c30de51d22c59d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-3e186d7182e6055061c30de51d22c59d_b.jpg\"/></figure><p>创建keras-complete-rest-api目录并上传文件</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-a4e9c8a56609ace0e0374e0921f2916f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"429\" data-rawheight=\"269\" class=\"origin_image zh-lightbox-thumb\" width=\"429\" data-original=\"https://pic4.zhimg.com/v2-a4e9c8a56609ace0e0374e0921f2916f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;429&#39; height=&#39;269&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"429\" data-rawheight=\"269\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"429\" data-original=\"https://pic4.zhimg.com/v2-a4e9c8a56609ace0e0374e0921f2916f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-a4e9c8a56609ace0e0374e0921f2916f_b.jpg\"/></figure><p>通过以下方式将它 sym-link 到 /var/www/html：</p><div class=\"highlight\"><pre><code class=\"language-text\">cd /var/www/html/\nsudo ln -s /home/keras-complete-rest-api keras-complete-rest-api</code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-35bc0c3c6775edc63611d1f784c1d086_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"420\" data-rawheight=\"242\" class=\"content_image\" width=\"420\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;420&#39; height=&#39;242&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"420\" data-rawheight=\"242\" class=\"content_image lazy\" width=\"420\" data-actualsrc=\"https://pic3.zhimg.com/v2-35bc0c3c6775edc63611d1f784c1d086_b.jpg\"/></figure><p>创建了该文件夹的快捷方式</p><hr/><p><b>step5:</b></p><p>更新你的 Apache 配置以指向 Flask 应用程序。为了配置 Apache 以便指向我们的 Flask 应用程序，我们需要编辑 /etc/apache2/sites-available/000-default.conf。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-220b7e34b460098fb536ef1ce995a97b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"832\" data-rawheight=\"455\" class=\"origin_image zh-lightbox-thumb\" width=\"832\" data-original=\"https://pic4.zhimg.com/v2-220b7e34b460098fb536ef1ce995a97b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;832&#39; height=&#39;455&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"832\" data-rawheight=\"455\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"832\" data-original=\"https://pic4.zhimg.com/v2-220b7e34b460098fb536ef1ce995a97b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-220b7e34b460098fb536ef1ce995a97b_b.jpg\"/></figure><p>我没有使用虚拟环境，所以只需要配置这么一点</p><p>重新启动 Apache Web 服务器，使配置生效</p><div class=\"highlight\"><pre><code class=\"language-text\">sudo service apache2 restart</code></pre></div><p>将会出现</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-b8b3d039b806af6792a7762375e6f140_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"547\" data-rawheight=\"200\" class=\"origin_image zh-lightbox-thumb\" width=\"547\" data-original=\"https://pic1.zhimg.com/v2-b8b3d039b806af6792a7762375e6f140_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;547&#39; height=&#39;200&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"547\" data-rawheight=\"200\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"547\" data-original=\"https://pic1.zhimg.com/v2-b8b3d039b806af6792a7762375e6f140_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-b8b3d039b806af6792a7762375e6f140_b.jpg\"/></figure><div class=\"highlight\"><pre><code class=\"language-text\">sudo service apache2 start\nredis-server</code></pre></div><hr/><p><b>step6:</b></p><p>到你存放文件的目录，启动服务</p><div class=\"highlight\"><pre><code class=\"language-text\">python run_model_server.py</code></pre></div><p>测试服务</p><div class=\"highlight\"><pre><code class=\"language-text\">python stress_test.py </code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-b1d4a686082017dc25f116242bf19312_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"439\" data-rawheight=\"253\" class=\"origin_image zh-lightbox-thumb\" width=\"439\" data-original=\"https://pic3.zhimg.com/v2-b1d4a686082017dc25f116242bf19312_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;439&#39; height=&#39;253&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"439\" data-rawheight=\"253\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"439\" data-original=\"https://pic3.zhimg.com/v2-b1d4a686082017dc25f116242bf19312_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-b1d4a686082017dc25f116242bf19312_b.jpg\"/></figure><p>附：</p><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/u012968002/article/details/80393806\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">深度学习部署--搭建后台服务器 - JIN JI 2013.12.24 - CSDN博客</a><p></p>", 
            "topic": [
                {
                    "tag": "阿里云", 
                    "tagLink": "https://api.zhihu.com/topics/19560108"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/57083152", 
            "userName": "小白", 
            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
            "upvote": 0, 
            "title": "阿里云使用(3)", 
            "content": "<p>step1:</p><p>安装Redis，redis是一个key-value存储系统。</p><div class=\"highlight\"><pre><code class=\"language-text\">wget http://download.redis.io/redis-stable.tar.gz\ntar xvzf redis-stable.tar.gz\ncd redis-stable\nmake\nsudo make install</code></pre></div><p>要启动Redis服务器</p><div class=\"highlight\"><pre><code class=\"language-text\">redis-server</code></pre></div><p>在另一个终端中，您可以验证Redis是否已启动并正在运行：</p><div class=\"highlight\"><pre><code class=\"language-text\">redis-cli ping</code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-880bef9701f87627e005be11b3e94734_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"946\" data-rawheight=\"162\" class=\"origin_image zh-lightbox-thumb\" width=\"946\" data-original=\"https://pic1.zhimg.com/v2-880bef9701f87627e005be11b3e94734_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;946&#39; height=&#39;162&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"946\" data-rawheight=\"162\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"946\" data-original=\"https://pic1.zhimg.com/v2-880bef9701f87627e005be11b3e94734_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-880bef9701f87627e005be11b3e94734_b.jpg\"/></figure><p>step2</p><p>安装anconda</p><a href=\"https://zhuanlan.zhihu.com/p/57035300\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\"internal\"><span class=\"invisible\">https://</span><span class=\"visible\">zhuanlan.zhihu.com/p/57</span><span class=\"invisible\">035300</span><span class=\"ellipsis\"></span></a><a href=\"https://zhuanlan.zhihu.com/p/57035300\" class=\"internal\">小白：如何使用阿里云（2）</a><a href=\"https://zhuanlan.zhihu.com/p/57035300\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\"internal\">小白：如何使用阿里云（2）</a><p class=\"ztext-empty-paragraph\"><br/></p><p>step3</p><p>创建服务器脚本run_keras_server.py</p><div class=\"highlight\"><pre><code class=\"language-text\"># import the necessary packages\nfrom keras.applications import ResNet50\nfrom keras.preprocessing.image import img_to_array\nfrom keras.applications import imagenet_utils\nfrom threading import Thread\nfrom PIL import Image\nimport numpy as np\nimport base64\nimport flask\nimport redis\nimport uuid\nimport time\nimport json\nimport sys\nimport io\n\n# initialize constants used to control image spatial dimensions and\n# data type\nIMAGE_WIDTH = 224\nIMAGE_HEIGHT = 224\nIMAGE_CHANS = 3\nIMAGE_DTYPE = &#34;float32&#34;\n\n# initialize constants used for server queuing\nIMAGE_QUEUE = &#34;image_queue&#34;\nBATCH_SIZE = 32\nSERVER_SLEEP = 0.25\nCLIENT_SLEEP = 0.25\n\n# initialize our Flask application, Redis server, and Keras model\napp = flask.Flask(__name__)\ndb = redis.StrictRedis(host=&#34;localhost&#34;, port=6379, db=0)\nmodel = None\n\ndef base64_encode_image(a):\n\t# base64 encode the input NumPy array\n\treturn base64.b64encode(a).decode(&#34;utf-8&#34;)\n\ndef base64_decode_image(a, dtype, shape):\n\t# if this is Python 3, we need the extra step of encoding the\n\t# serialized NumPy string as a byte object\n\tif sys.version_info.major == 3:\n\t\ta = bytes(a, encoding=&#34;utf-8&#34;)\n\n\t# convert the string to a NumPy array using the supplied data\n\t# type and target shape\n\ta = np.frombuffer(base64.decodestring(a), dtype=dtype)\n\ta = a.reshape(shape)\n\n\t# return the decoded image\n\treturn a\n\ndef prepare_image(image, target):\n\t# if the image mode is not RGB, convert it\n\tif image.mode != &#34;RGB&#34;:\n\t\timage = image.convert(&#34;RGB&#34;)\n\n\t# resize the input image and preprocess it\n\timage = image.resize(target)\n\timage = img_to_array(image)\n\timage = np.expand_dims(image, axis=0)\n\timage = imagenet_utils.preprocess_input(image)\n\n\t# return the processed image\n\treturn image\n\ndef classify_process():\n\t# load the pre-trained Keras model (here we are using a model\n\t# pre-trained on ImageNet and provided by Keras, but you can\n\t# substitute in your own networks just as easily)\n\tprint(&#34;* Loading model...&#34;)\n\tmodel = ResNet50(weights=&#34;imagenet&#34;)\n\tprint(&#34;* Model loaded&#34;)\n\n\t# continually poll for new images to classify\n\twhile True:\n\t\t# attempt to grab a batch of images from the database, then\n\t\t# initialize the image IDs and batch of images themselves\n\t\tqueue = db.lrange(IMAGE_QUEUE, 0, BATCH_SIZE - 1)\n\t\timageIDs = []\n\t\tbatch = None\n\n\t\t# loop over the queue\n\t\tfor q in queue:\n\t\t\t# deserialize the object and obtain the input image\n\t\t\tq = json.loads(q.decode(&#34;utf-8&#34;))\n\t\t\timage = base64_decode_image(q[&#34;image&#34;], IMAGE_DTYPE,\n\t\t\t\t(1, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_CHANS))\n\n\t\t\t# check to see if the batch list is None\n\t\t\tif batch is None:\n\t\t\t\tbatch = image\n\n\t\t\t# otherwise, stack the data\n\t\t\telse:\n\t\t\t\tbatch = np.vstack([batch, image])\n\n\t\t\t# update the list of image IDs\n\t\t\timageIDs.append(q[&#34;id&#34;])\n\t\t# check to see if we need to process the batch\n\t\tif len(imageIDs) &gt; 0:\n\t\t\t# classify the batch\n\t\t\tprint(&#34;* Batch size: {}&#34;.format(batch.shape))\n\t\t\tpreds = model.predict(batch)\n\t\t\tresults = imagenet_utils.decode_predictions(preds)\n\n\t\t\t# loop over the image IDs and their corresponding set of\n\t\t\t# results from our model\n\t\t\tfor (imageID, resultSet) in zip(imageIDs, results):\n\t\t\t\t# initialize the list of output predictions\n\t\t\t\toutput = []\n\n\t\t\t\t# loop over the results and add them to the list of\n\t\t\t\t# output predictions\n\t\t\t\tfor (imagenetID, label, prob) in resultSet:\n\t\t\t\t\tr = {&#34;label&#34;: label, &#34;probability&#34;: float(prob)}\n\t\t\t\t\toutput.append(r)\n\n\t\t\t\t# store the output predictions in the database, using\n\t\t\t\t# the image ID as the key so we can fetch the results\n\t\t\t\tdb.set(imageID, json.dumps(output))\n\n\t\t\t# remove the set of images from our queue\n\t\t\tdb.ltrim(IMAGE_QUEUE, len(imageIDs), -1)\n\n\t\t# sleep for a small amount\n\t\ttime.sleep(SERVER_SLEEP)\n\n@app.route(&#34;/predict&#34;, methods=[&#34;POST&#34;])\ndef predict():\n\t# initialize the data dictionary that will be returned from the\n\t# view\n\tdata = {&#34;success&#34;: False}\n\n\t# ensure an image was properly uploaded to our endpoint\n\tif flask.request.method == &#34;POST&#34;:\n\t\tif flask.request.files.get(&#34;image&#34;):\n\t\t\t# read the image in PIL format and prepare it for\n\t\t\t# classification\n\t\t\timage = flask.request.files[&#34;image&#34;].read()\n\t\t\timage = Image.open(io.BytesIO(image))\n\t\t\timage = prepare_image(image, (IMAGE_WIDTH, IMAGE_HEIGHT))\n\n\t\t\t# ensure our NumPy array is C-contiguous as well,\n\t\t\t# otherwise we won&#39;t be able to serialize it\n\t\t\timage = image.copy(order=&#34;C&#34;)\n\n\t\t\t# generate an ID for the classification then add the\n\t\t\t# classification ID + image to the queue\n\t\t\tk = str(uuid.uuid4())\n\t\t\td = {&#34;id&#34;: k, &#34;image&#34;: base64_encode_image(image)}\n\t\t\tdb.rpush(IMAGE_QUEUE, json.dumps(d))\n\t\t\t# keep looping until our model server returns the output\n\t\t\t# predictions\n\t\t\twhile True:\n\t\t\t\t# attempt to grab the output predictions\n\t\t\t\toutput = db.get(k)\n\n\t\t\t\t# check to see if our model has classified the input\n\t\t\t\t# image\n\t\t\t\tif output is not None:\n \t\t\t\t\t# add the output predictions to our data\n \t\t\t\t\t# dictionary so we can return it to the client\n\t\t\t\t\toutput = output.decode(&#34;utf-8&#34;)\n\t\t\t\t\tdata[&#34;predictions&#34;] = json.loads(output)\n\n\t\t\t\t\t# delete the result from the database and break\n\t\t\t\t\t# from the polling loop\n\t\t\t\t\tdb.delete(k)\n\t\t\t\t\tbreak\n\n\t\t\t\t# sleep for a small amount to give the model a chance\n\t\t\t\t# to classify the input image\n\t\t\t\ttime.sleep(CLIENT_SLEEP)\n\n\t\t\t# indicate that the request was a success\n\t\t\tdata[&#34;success&#34;] = True\n\n\t# return the data dictionary as a JSON response\n\treturn flask.jsonify(data)\n\n# if this is the main thread of execution first load the model and\n# then start the server\nif __name__ == &#34;__main__&#34;:\n\t# load the function used to classify input images in a *separate*\n\t# thread than the one used for main classification\n\tprint(&#34;* Starting model service...&#34;)\n\tt = Thread(target=classify_process, args=())\n\tt.daemon = True\n\tt.start()\n\n\t# start the web server\n\tprint(&#34;* Starting web service...&#34;)\n\tapp.run()</code></pre></div><p>上传脚本，启动服务</p><div class=\"highlight\"><pre><code class=\"language-text\">python run_keras_server.py </code></pre></div><p>step4:</p><p>测试服务，simple_request.py</p><div class=\"highlight\"><pre><code class=\"language-text\"># import the necessary packages\nimport requests\n \n# initialize the Keras REST API endpoint URL along with the input\n# image path\nKERAS_REST_API_URL = &#34;http://localhost:5000/predict&#34;\nIMAGE_PATH = &#34;jemma.png&#34;\n\n# load the input image and construct the payload for the request\nimage = open(IMAGE_PATH, &#34;rb&#34;).read()\npayload = {&#34;image&#34;: image}\n \n# submit the request\nr = requests.post(KERAS_REST_API_URL, files=payload).json()\n \n# ensure the request was sucessful\nif r[&#34;success&#34;]:\n\t# loop over the predictions and display them\n\tfor (i, result) in enumerate(r[&#34;predictions&#34;]):\n\t\tprint(&#34;{}. {}: {:.4f}&#34;.format(i + 1, result[&#34;label&#34;],\n\t\t\tresult[&#34;probability&#34;]))\n \n# otherwise, the request failed\nelse:\n\tprint(&#34;Request failed&#34;)</code></pre></div><a href=\"https://link.zhihu.com/?target=https%3A//www.pyimagesearch.com/2018/01/29/scalable-keras-deep-learning-rest-api/\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic2.zhimg.com/v2-7dec7246430d4b13fd3d0037ee60b4b1_ipico.jpg\" data-image-width=\"600\" data-image-height=\"658\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">A scalable Keras + deep learning REST API - PyImageSearch</a><p>附：</p><p>配置虚拟环境</p><p>创建虚拟的python环境，你需要安装pip，virtualenv和virtualenvwrapper：</p><div class=\"highlight\"><pre><code class=\"language-text\">cd ~\nwget https://bootstrap.pypa.io/get-pip.py\nsudo python get-pip.py\nsudo pip install virtualenv virtualenvwrapper</code></pre></div><p>您还需要编辑〜/.bashrc</p><div class=\"highlight\"><pre><code class=\"language-text\"># virtualenv and virtualenvwrapper\nexport WORKON_HOME=$HOME/.virtualenvs\nsource /usr/local/bin/virtualenvwrapper.sh</code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-11179ee36e5aad1baf4ed8c85e7feb08_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"639\" data-rawheight=\"174\" class=\"origin_image zh-lightbox-thumb\" width=\"639\" data-original=\"https://pic1.zhimg.com/v2-11179ee36e5aad1baf4ed8c85e7feb08_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;639&#39; height=&#39;174&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"639\" data-rawheight=\"174\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"639\" data-original=\"https://pic1.zhimg.com/v2-11179ee36e5aad1baf4ed8c85e7feb08_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-11179ee36e5aad1baf4ed8c85e7feb08_b.jpg\"/></figure><p>激活</p><div class=\"highlight\"><pre><code class=\"language-text\">source ~/.bashrc</code></pre></div><p> 创建一个Python虚拟环境</p><div class=\"highlight\"><pre><code class=\"language-text\">mkvirtualenv keras_flask -p python3</code></pre></div><p>出现错误：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-f5ec6f1a71815b358975418152bb0019_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"629\" data-rawheight=\"146\" class=\"origin_image zh-lightbox-thumb\" width=\"629\" data-original=\"https://pic2.zhimg.com/v2-f5ec6f1a71815b358975418152bb0019_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;629&#39; height=&#39;146&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"629\" data-rawheight=\"146\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"629\" data-original=\"https://pic2.zhimg.com/v2-f5ec6f1a71815b358975418152bb0019_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-f5ec6f1a71815b358975418152bb0019_b.jpg\"/></figure><div class=\"highlight\"><pre><code class=\"language-text\">sudo apt-get install python3-distutils</code></pre></div><p>如果你需要提供网页服务并不需要修改redis的host和port，其相当于服务器来讲就是本地服务。只需要修改：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-41e86c344cc53cf19c129a51d14565a0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"394\" data-rawheight=\"79\" class=\"content_image\" width=\"394\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;394&#39; height=&#39;79&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"394\" data-rawheight=\"79\" class=\"content_image lazy\" width=\"394\" data-actualsrc=\"https://pic1.zhimg.com/v2-41e86c344cc53cf19c129a51d14565a0_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "阿里云", 
                    "tagLink": "https://api.zhihu.com/topics/19560108"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/57035300", 
            "userName": "小白", 
            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
            "upvote": 0, 
            "title": "如何使用阿里云（2）", 
            "content": "<p>step1:</p><p>安装keras,可以参考</p><a href=\"https://link.zhihu.com/?target=https%3A//keras-cn.readthedocs.io/en/latest/for_beginners/keras_linux/\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic2.zhimg.com/v2-af38dd5a6e8cc825d7e285e9ca852db9_180x120.jpg\" data-image-width=\"1235\" data-image-height=\"773\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Keras linux - Keras中文文档</a><p>安装开发包打开<code>终端</code>输入：</p><div class=\"highlight\"><pre><code class=\"language-text\"># 系统升级\nsudo apt update\nsudo apt upgrade\n# 安装python基础开发包\nsudo apt install -y python-dev python-pip python-nose gcc g++ git gfortran vim</code></pre></div><p>上传安装文件</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-997b0b17d5147c867e81b0ea361fe34b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"551\" data-rawheight=\"298\" class=\"origin_image zh-lightbox-thumb\" width=\"551\" data-original=\"https://pic4.zhimg.com/v2-997b0b17d5147c867e81b0ea361fe34b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;551&#39; height=&#39;298&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"551\" data-rawheight=\"298\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"551\" data-original=\"https://pic4.zhimg.com/v2-997b0b17d5147c867e81b0ea361fe34b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-997b0b17d5147c867e81b0ea361fe34b_b.jpg\"/></figure><p>运行命令</p><div class=\"highlight\"><pre><code class=\"language-text\">bash Anaconda3-4.2.0-Linux-x86_64.sh</code></pre></div><p>将Anconda设置为默认环境</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-56126998b5152933bbb06edb06b7a155_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"416\" data-rawheight=\"340\" class=\"content_image\" width=\"416\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;416&#39; height=&#39;340&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"416\" data-rawheight=\"340\" class=\"content_image lazy\" width=\"416\" data-actualsrc=\"https://pic2.zhimg.com/v2-56126998b5152933bbb06edb06b7a155_b.jpg\"/></figure><p>在文本最后添加命令：export PATH=~/anaconda3/bin:$PATH</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-4fdd67d2d10bb5b1b951da4916dbb2e4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"492\" data-rawheight=\"269\" class=\"origin_image zh-lightbox-thumb\" width=\"492\" data-original=\"https://pic1.zhimg.com/v2-4fdd67d2d10bb5b1b951da4916dbb2e4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;492&#39; height=&#39;269&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"492\" data-rawheight=\"269\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"492\" data-original=\"https://pic1.zhimg.com/v2-4fdd67d2d10bb5b1b951da4916dbb2e4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-4fdd67d2d10bb5b1b951da4916dbb2e4_b.jpg\"/></figure><p>重启环境变量：source ~/.bashrc</p><p>输入：anaconda -V</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-c158fda7eaf1afc23fcd1e9d72d8308f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"581\" data-rawheight=\"110\" class=\"origin_image zh-lightbox-thumb\" width=\"581\" data-original=\"https://pic4.zhimg.com/v2-c158fda7eaf1afc23fcd1e9d72d8308f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;581&#39; height=&#39;110&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"581\" data-rawheight=\"110\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"581\" data-original=\"https://pic4.zhimg.com/v2-c158fda7eaf1afc23fcd1e9d72d8308f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-c158fda7eaf1afc23fcd1e9d72d8308f_b.jpg\"/></figure><p>step2:</p><p>Putty是免费开源的linux连接工具，其可以向远程linux发送命令</p><p>filezilla是免费开源的文件传输工具</p><a href=\"https://link.zhihu.com/?target=https%3A//filezilla-project.org/download.php%3Ftype%3Dclient\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic4.zhimg.com/v2-01e8a38e051fc3a45abe69846e24a31b_180x120.jpg\" data-image-width=\"300\" data-image-height=\"239\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Download FileZilla Client for Windows (64bit)</a><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-c36e455b238c898449fce1066e0398b0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1116\" data-rawheight=\"625\" class=\"origin_image zh-lightbox-thumb\" width=\"1116\" data-original=\"https://pic1.zhimg.com/v2-c36e455b238c898449fce1066e0398b0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1116&#39; height=&#39;625&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1116\" data-rawheight=\"625\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1116\" data-original=\"https://pic1.zhimg.com/v2-c36e455b238c898449fce1066e0398b0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-c36e455b238c898449fce1066e0398b0_b.jpg\"/></figure><p>step3:</p><p>开放80端口使外网可以访问</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-c95289ecbddfda30626591b84f46063b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"820\" data-rawheight=\"380\" class=\"origin_image zh-lightbox-thumb\" width=\"820\" data-original=\"https://pic4.zhimg.com/v2-c95289ecbddfda30626591b84f46063b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;820&#39; height=&#39;380&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"820\" data-rawheight=\"380\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"820\" data-original=\"https://pic4.zhimg.com/v2-c95289ecbddfda30626591b84f46063b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-c95289ecbddfda30626591b84f46063b_b.jpg\"/></figure><p>现在你申请的都是VPC，所以看到网卡类型只有内网的时候不要慌，无需配置。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-4aafef7d730ab45cc567f2157585375a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1303\" data-rawheight=\"364\" class=\"origin_image zh-lightbox-thumb\" width=\"1303\" data-original=\"https://pic3.zhimg.com/v2-4aafef7d730ab45cc567f2157585375a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1303&#39; height=&#39;364&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1303\" data-rawheight=\"364\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1303\" data-original=\"https://pic3.zhimg.com/v2-4aafef7d730ab45cc567f2157585375a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-4aafef7d730ab45cc567f2157585375a_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-20592943b8c0fde04a3543dc0c987089_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1167\" data-rawheight=\"451\" class=\"origin_image zh-lightbox-thumb\" width=\"1167\" data-original=\"https://pic2.zhimg.com/v2-20592943b8c0fde04a3543dc0c987089_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1167&#39; height=&#39;451&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1167\" data-rawheight=\"451\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1167\" data-original=\"https://pic2.zhimg.com/v2-20592943b8c0fde04a3543dc0c987089_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-20592943b8c0fde04a3543dc0c987089_b.jpg\"/></figure><a href=\"https://link.zhihu.com/?target=https%3A//blog.keras.io/building-a-simple-keras-deep-learning-rest-api.html\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-3ac6d59dae30ac406fdafb37345f7956_180x120.jpg\" data-image-width=\"800\" data-image-height=\"339\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Building a simple Keras + deep learning REST API</a><p></p>", 
            "topic": [
                {
                    "tag": "阿里云", 
                    "tagLink": "https://api.zhihu.com/topics/19560108"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/57027294", 
            "userName": "小白", 
            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
            "upvote": 1, 
            "title": "如何使用阿里云（1）", 
            "content": "<p>step1:</p><p>购买ESC</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-a08ab3850b2326a3e4e85fd6c17f0072_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1249\" data-rawheight=\"633\" class=\"origin_image zh-lightbox-thumb\" width=\"1249\" data-original=\"https://pic3.zhimg.com/v2-a08ab3850b2326a3e4e85fd6c17f0072_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1249&#39; height=&#39;633&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1249\" data-rawheight=\"633\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1249\" data-original=\"https://pic3.zhimg.com/v2-a08ab3850b2326a3e4e85fd6c17f0072_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-a08ab3850b2326a3e4e85fd6c17f0072_b.jpg\"/></figure><p>step2:</p><p>修改密码，这是登陆时用的密码</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-090694f73c626fb4b600e268e33a9763_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"654\" data-rawheight=\"295\" class=\"origin_image zh-lightbox-thumb\" width=\"654\" data-original=\"https://pic4.zhimg.com/v2-090694f73c626fb4b600e268e33a9763_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;654&#39; height=&#39;295&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"654\" data-rawheight=\"295\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"654\" data-original=\"https://pic4.zhimg.com/v2-090694f73c626fb4b600e268e33a9763_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-090694f73c626fb4b600e268e33a9763_b.jpg\"/></figure><p>linux用户名是:root，密码是刚才设置的</p><p>windows用户名是：administrator</p><p>step3:</p><p>下载putty，这个是远程连接linux用的<a href=\"https://link.zhihu.com/?target=https%3A//www.chiark.greenend.org.uk/~sgtatham/putty/latest.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Download PuTTY: latest release (0.70)</a></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-33dea3436f3558a62caefc5e13dd6070_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"731\" data-rawheight=\"407\" class=\"origin_image zh-lightbox-thumb\" width=\"731\" data-original=\"https://pic1.zhimg.com/v2-33dea3436f3558a62caefc5e13dd6070_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;731&#39; height=&#39;407&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"731\" data-rawheight=\"407\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"731\" data-original=\"https://pic1.zhimg.com/v2-33dea3436f3558a62caefc5e13dd6070_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-33dea3436f3558a62caefc5e13dd6070_b.jpg\"/></figure><p>step4:</p><p>用putty远程登陆linux</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-76332c914b41aa45a50abb087cc1e325_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"865\" data-rawheight=\"840\" class=\"origin_image zh-lightbox-thumb\" width=\"865\" data-original=\"https://pic2.zhimg.com/v2-76332c914b41aa45a50abb087cc1e325_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;865&#39; height=&#39;840&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"865\" data-rawheight=\"840\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"865\" data-original=\"https://pic2.zhimg.com/v2-76332c914b41aa45a50abb087cc1e325_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-76332c914b41aa45a50abb087cc1e325_b.jpg\"/></figure><p>IP地址就是填这个</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-37690c3fd6994a50778b53b20f19173b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"669\" data-rawheight=\"422\" class=\"origin_image zh-lightbox-thumb\" width=\"669\" data-original=\"https://pic4.zhimg.com/v2-37690c3fd6994a50778b53b20f19173b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;669&#39; height=&#39;422&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"669\" data-rawheight=\"422\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"669\" data-original=\"https://pic4.zhimg.com/v2-37690c3fd6994a50778b53b20f19173b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-37690c3fd6994a50778b53b20f19173b_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "阿里云", 
                    "tagLink": "https://api.zhihu.com/topics/19560108"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/56711282", 
            "userName": "小白", 
            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
            "upvote": 1, 
            "title": "重装NVIDIA驱动遇到的一个问题（已解决）", 
            "content": "<p>今天开机时发现Ubuntu图形界面循环登录了，本打算按之前配置caffe时的装NVIDIA驱动的方法重装一下驱动（参照<a href=\"https://zhuanlan.zhihu.com/p/33484702\" class=\"internal\">小白：手把手教你在Ubuntu 16.04下配置GPU版Caffe（1）</a>），结果又遇到在字符界面无法登录的意外。情况是这样的：在进入控制台输完用户名后还未输密码，就一直提示Login incorrect...</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-e1f3c60dac868f2dc613537c02bd21d6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"349\" data-rawheight=\"209\" class=\"content_image\" width=\"349\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;349&#39; height=&#39;209&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"349\" data-rawheight=\"209\" class=\"content_image lazy\" width=\"349\" data-actualsrc=\"https://pic3.zhimg.com/v2-e1f3c60dac868f2dc613537c02bd21d6_b.jpg\"/></figure><p>然后又回到开始，让你输用户名。。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-740d5e5fc54902d0ba0a47e624deeddd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"302\" data-rawheight=\"118\" class=\"content_image\" width=\"302\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;302&#39; height=&#39;118&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"302\" data-rawheight=\"118\" class=\"content_image lazy\" width=\"302\" data-actualsrc=\"https://pic2.zhimg.com/v2-740d5e5fc54902d0ba0a47e624deeddd_b.jpg\"/></figure><p>我的第一反应是键盘坏了，结果换了键盘还是这样。无法进入控制台就无法重装驱动，难道只能重装系统了吗？于是我又不得不寻找另一种进入字符界面重装驱动的办法，终于找到了解决方案。先记录下来，以备不时之需。</p><hr/><p>解决方案如下：</p><p>1. 重启，在GRUB菜单选择Ubuntu高级选项（Advanced options for Ubuntu），如下图所示。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-4f2637dfd8f92b5be74a33a8ccf2d677_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"4032\" data-rawheight=\"3024\" class=\"origin_image zh-lightbox-thumb\" width=\"4032\" data-original=\"https://pic4.zhimg.com/v2-4f2637dfd8f92b5be74a33a8ccf2d677_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;4032&#39; height=&#39;3024&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"4032\" data-rawheight=\"3024\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"4032\" data-original=\"https://pic4.zhimg.com/v2-4f2637dfd8f92b5be74a33a8ccf2d677_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-4f2637dfd8f92b5be74a33a8ccf2d677_b.jpg\"/></figure><p>2. 进入recovery mode</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-4f2637dfd8f92b5be74a33a8ccf2d677_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"4032\" data-rawheight=\"3024\" class=\"origin_image zh-lightbox-thumb\" width=\"4032\" data-original=\"https://pic4.zhimg.com/v2-4f2637dfd8f92b5be74a33a8ccf2d677_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;4032&#39; height=&#39;3024&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"4032\" data-rawheight=\"3024\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"4032\" data-original=\"https://pic4.zhimg.com/v2-4f2637dfd8f92b5be74a33a8ccf2d677_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-4f2637dfd8f92b5be74a33a8ccf2d677_b.jpg\"/></figure><p>3. 在recovery mode下选择root</p><p>4. 然后在出现的命令行里输入：</p><div class=\"highlight\"><pre><code class=\"language-text\">mount -o remount，rw /\nreboot  #重启</code></pre></div><p>5. 重启后再次进入recovery mode模式 ，然后选择grub项，然后运行返回后可以发现首行变为“Recovery Menu （filesystem state： read-write）”（在该状态下就可以进入root项的命令行里操作了）。</p><p>6. 再选择root项，登录root帐号，在命令行里切换到驱动安装文件NVIDIA-Linux-x86_64-3xx.xx.run所在目录，并输入：</p><div class=\"highlight\"><pre><code class=\"language-text\">service lightdm stop  #关闭图形界面（无需sudo，因为此时已经是root，下同）\nsh NVIDIA-Linux-x86_64-3xx.xx.run --uninstall  #卸载之前崩坏的驱动\nsh NVIDIA-Linux-x86_64-3xx.xx.run  #重新安装驱动\nnvidia-smi  #查看驱动是否安装成功\nreboot  #重启</code></pre></div><p>注意：这里不能直接启动图形界面（service lightdm start），否则会报错，重启即可解决问题。</p><p>当然这个办法不限于重装驱动的情况，控制台进不去的问题均可尝试此方法。</p><p>欢迎关注公众号：huangxiaobai880</p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/54562341", 
            "userName": "小白", 
            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
            "upvote": 1, 
            "title": "参考白", 
            "content": "<p> AnilK.Jain等为解决图像中色彩偏差的问题,把图像中像素的亮度按照由高到低的顺序来排列，提取排列前5%的像素，若这些像素的数目足够多(例如：大于100)，就将它们的亮度作为/参考白0(Reference white),将其色彩的R、G、B分量值都调整为最大的255，整幅图像其他像素的色彩分量也按照这个尺度变化。使非参考白的部分像素的RGB值也有相应提高，从而保证光照能够尽可能小的影响图像。根据上述原理，具体实现方法如下：</p><p>(1)  统计每个灰度值的像素数，通过循环得到排列前5%的像素灰度值作为参考白，则参考白像素的亮度平均值aveGray为：</p><p>                aveGray = Grayref/GrayrefNum</p><p>                式中Grayref为参考白灰度值；GrayrefNum用为参考白像素数；</p><p>(2)  计算光照补偿系数coe：</p><p>                coe=255/aveGray</p><p>(3)  原像素值分别乘以光照补偿系数coe，得到光照补偿后的像素值。</p><div class=\"highlight\"><pre><code class=\"language-text\">import cv2\nimport numpy as np\nimport os\n\n\ndef reference_white(img):\n\n    img_b = np.sort(img[:, :, 0].reshape( -1))\n    img_g = np.sort(img[:, :, 1].reshape( -1))\n    img_r = np.sort(img[:, :, 2].reshape( -1))\n    num = int(img_b.shape[0] * 0.05)\n    img_b_mean = np.mean(img_b[-num:])\n    img_g_mean = np.mean(img_g[-num:])\n    img_r_mean = np.mean(img_r[-num:])\n    img[:, :, 0] = np.uint8(np.minimum(img[:, :, 0] * (255 / img_b_mean), 255))\n    img[:, :, 1] = np.uint8(np.minimum(img[:, :, 1] * (255 / img_g_mean), 255))\n    img[:, :, 2] = np.uint8(np.minimum(img[:, :, 2] * (255 / img_r_mean), 255))\n    return img\n\ntrain_data_path = &#39;C:/Users/huang/Desktop/CAR2019/test/img&#39;\nfor name in os.listdir(train_data_path):\n    img = cv2.imread(os.path.join(train_data_path, name))\n    m = reference_white(img)\n    cv2.imshow(&#39;img&#39;,m)\n    cv2.waitKey()</code></pre></div><hr/><p>上面是我在网上看的一个博客所讲的参考白，下面是另一篇论文里面的参考白</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-3da09882d7e4603b21cb68b0f0e6a8c8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"763\" data-rawheight=\"268\" class=\"origin_image zh-lightbox-thumb\" width=\"763\" data-original=\"https://pic1.zhimg.com/v2-3da09882d7e4603b21cb68b0f0e6a8c8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;763&#39; height=&#39;268&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"763\" data-rawheight=\"268\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"763\" data-original=\"https://pic1.zhimg.com/v2-3da09882d7e4603b21cb68b0f0e6a8c8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-3da09882d7e4603b21cb68b0f0e6a8c8_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-28065711821913babac22fdda02df808_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"742\" data-rawheight=\"348\" class=\"origin_image zh-lightbox-thumb\" width=\"742\" data-original=\"https://pic1.zhimg.com/v2-28065711821913babac22fdda02df808_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;742&#39; height=&#39;348&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"742\" data-rawheight=\"348\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"742\" data-original=\"https://pic1.zhimg.com/v2-28065711821913babac22fdda02df808_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-28065711821913babac22fdda02df808_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-e6f5ea379f7844db2a43b9bfa25724bb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"736\" data-rawheight=\"307\" class=\"origin_image zh-lightbox-thumb\" width=\"736\" data-original=\"https://pic4.zhimg.com/v2-e6f5ea379f7844db2a43b9bfa25724bb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;736&#39; height=&#39;307&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"736\" data-rawheight=\"307\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"736\" data-original=\"https://pic4.zhimg.com/v2-e6f5ea379f7844db2a43b9bfa25724bb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-e6f5ea379f7844db2a43b9bfa25724bb_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-3b9847dd836ed34ebe054f1aab84d5ae_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"743\" data-rawheight=\"334\" class=\"origin_image zh-lightbox-thumb\" width=\"743\" data-original=\"https://pic3.zhimg.com/v2-3b9847dd836ed34ebe054f1aab84d5ae_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;743&#39; height=&#39;334&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"743\" data-rawheight=\"334\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"743\" data-original=\"https://pic3.zhimg.com/v2-3b9847dd836ed34ebe054f1aab84d5ae_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-3b9847dd836ed34ebe054f1aab84d5ae_b.jpg\"/></figure><div class=\"highlight\"><pre><code class=\"language-text\">#日期：2019-03-02\n#作者：huang guo jie\n#功能：参考白\nimport cv2\nimport numpy as np\nimport os\n\ndef reference_white(img):\n    img_shape = img.shape\n    img_b = img[:,:,0].astype(&#39;float&#39;)\n    img_g = img[:,:,1].astype(&#39;float&#39;)\n    img_r = img[:,:,2].astype(&#39;float&#39;)\n    \n    img_ycbcr = cv2.cvtColor(img, cv2.COLOR_BGR2YCR_CB)\n    img_y = img_ycbcr[:,:,0]\n    img_y_sort = np.sort(img_y.reshape( -1))\n    num = int(img_y_sort.shape[0] * 0.03)\n    T1 = img_y_sort[-num]\n    T2 = img_y_sort[num]\n    \n    n1 = 0\n    n2 = 0\n    img_r_big_sum = 0\n    img_g_big_sum = 0\n    img_b_big_sum = 0\n    img_r_small_sum = 0\n    img_g_small_sum = 0\n    img_b_small_sum = 0\n    \n    for i in range(img_shape[0]):\n        for j in range(img_shape[1]):\n            if img_y[i, j] &gt;= T1:\n                img_r_big_sum =+ img_r[i,j]\n                img_g_big_sum =+ img_g[i,j]\n                img_b_big_sum =+ img_b[i,j]\n                n1 =+ 1\n\n            if img_y[i, j] &lt;= T2:\n                img_r_small_sum = + img_r[i, j]\n                img_g_small_sum = + img_g[i, j]\n                img_b_small_sum = + img_b[i, j]\n                n2 =+ 1\n    \n    r_mean1 = img_r_big_sum/n1\n    g_mean1 = img_g_big_sum/n1\n    b_mean1 = img_b_big_sum/n1\n    \n    r_mean2 = img_r_small_sum/n2\n    g_mean2 = img_g_small_sum/n2\n    b_mean2 = img_b_small_sum/n2\n\n    gama_r = -1\n    gama_g = -1\n    gama_b = -1\n    \n    print(r_mean2, r_mean1)\n    for i in range(img_shape[0]):\n        for j in range(img_shape[1]):\n            img_r[i,j] = (img_r[i,j] - r_mean2)/(r_mean2 - r_mean1)*gama_r\n            img_g[i,j] = (img_g[i,j] - g_mean2)/(g_mean2 - g_mean1)*gama_g\n            img_b[i,j] = (img_b[i,j] - b_mean2)/(b_mean2 - b_mean1)*gama_b\n            \n    img_new = np.zeros(shape=[img_shape[0], img_shape[1], 3])\n    img_new[:,:,0] = img_b\n    img_new[:,:,1] = img_g\n    img_new[:,:,2] = img_r\n\n    img_new = np.minimum(img_new, 1)\n    img_new = np.maximum(img_new,0)\n    img_new = np.uint8(img_new*255)\n\n    return img_new\n\ntrain_data_path = &#39;C:/Users/huang/Desktop/CAR2019/train/img&#39;\ntrain_data_save_path = &#39;C:/Users/huang/Desktop/CAR2019_MAKE/train/img_rw&#39;\nfor name in os.listdir(train_data_path):\n    print(name)\n    img = cv2.imread(os.path.join(train_data_path, name))\n    img_rw = reference_white(img)\n    cv2.imwrite(os.path.join(train_data_save_path, name), img_rw)</code></pre></div><p></p>", 
            "topic": [
                {
                    "tag": "图像处理", 
                    "tagLink": "https://api.zhihu.com/topics/19556376"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/52317757", 
            "userName": "小白", 
            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
            "upvote": 1, 
            "title": "源码分析：遥感图像数据集-DOTA（DOTA.py）", 
            "content": "<p></p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/CAPTAIN-WHU/DOTA_devkit\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-e0b960b7d0de1997a88f76300946a1f2_ipico.jpg\" data-image-width=\"420\" data-image-height=\"420\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">CAPTAIN-WHU/DOTA_devkit</a><p>该文件的作用：读取和显示框</p><hr/><p>本人是在windows运行该代码的，而源码是在linux下允许的，我在源码的最后加了一句：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-436a76365cacc8a0835c89f78536b1b4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"464\" data-rawheight=\"171\" class=\"origin_image zh-lightbox-thumb\" width=\"464\" data-original=\"https://pic1.zhimg.com/v2-436a76365cacc8a0835c89f78536b1b4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;464&#39; height=&#39;171&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"464\" data-rawheight=\"171\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"464\" data-original=\"https://pic1.zhimg.com/v2-436a76365cacc8a0835c89f78536b1b4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-436a76365cacc8a0835c89f78536b1b4_b.jpg\"/></figure><p>才会显示图片和框。你还需要将1处改成，你的example所在的文件夹。</p><p>此外，你需要安装shapely</p><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/u010205128/article/details/81003845\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-652b2f31322d6087066110a2f307b89c_180x120.jpg\" data-image-width=\"797\" data-image-height=\"279\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">windows 安装python包 shapely出错 Command &#34;python setup.py egg_info&#34; failed with error code 1 in</a><p>可以参考这个，试了是有用的。我还执行了命令：</p><div class=\"highlight\"><pre><code class=\"language-text\">pip install geos</code></pre></div><p>运行结果：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-0582fd29983a9a12c3465a8de8167546_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1025\" data-rawheight=\"599\" class=\"origin_image zh-lightbox-thumb\" width=\"1025\" data-original=\"https://pic3.zhimg.com/v2-0582fd29983a9a12c3465a8de8167546_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1025&#39; height=&#39;599&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1025\" data-rawheight=\"599\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1025\" data-original=\"https://pic3.zhimg.com/v2-0582fd29983a9a12c3465a8de8167546_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-0582fd29983a9a12c3465a8de8167546_b.jpg\"/></figure><hr/><div class=\"highlight\"><pre><code class=\"language-text\">examplesplit = DOTA(&#39;./example&#39;)</code></pre></div><p>创建一个对象，你需要传入数据所在的文件夹，以对对象做初始化。文件类包含两个文件，一个是图片，另一个是标签。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-5432d75a0e292767870cf38906b4646b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"283\" data-rawheight=\"83\" class=\"content_image\" width=\"283\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;283&#39; height=&#39;83&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"283\" data-rawheight=\"83\" class=\"content_image lazy\" width=\"283\" data-actualsrc=\"https://pic4.zhimg.com/v2-5432d75a0e292767870cf38906b4646b_b.jpg\"/></figure><p>初始化对象：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-aac0f46b575597e3bcf93f6880447082_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"524\" data-rawheight=\"193\" class=\"origin_image zh-lightbox-thumb\" width=\"524\" data-original=\"https://pic3.zhimg.com/v2-aac0f46b575597e3bcf93f6880447082_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;524&#39; height=&#39;193&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"524\" data-rawheight=\"193\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"524\" data-original=\"https://pic3.zhimg.com/v2-aac0f46b575597e3bcf93f6880447082_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-aac0f46b575597e3bcf93f6880447082_b.jpg\"/></figure><p>21-22行：根据传入的路径得到图片和标签所在的文件夹路径。</p><p>25-26行：创建一个空字典，具体用法参考：<a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/dpengwang/article/details/79308064\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">python中defaultdict的用法详解</a></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-ea462b415f52c514f99b967037017d6d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"449\" data-rawheight=\"277\" class=\"origin_image zh-lightbox-thumb\" width=\"449\" data-original=\"https://pic2.zhimg.com/v2-ea462b415f52c514f99b967037017d6d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;449&#39; height=&#39;277&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"449\" data-rawheight=\"277\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"449\" data-original=\"https://pic2.zhimg.com/v2-ea462b415f52c514f99b967037017d6d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-ea462b415f52c514f99b967037017d6d_b.jpg\"/></figure><p>函数作用：返回文件下指定后缀文件的路径</p><p>dir：文件夹路径；ext：后缀（得到指定后缀文件的路径）</p><p>22行：os.walk得到的是，root，该root下的文件夹名称，该root下的文件名称</p><p>23行：遍历文件夹下的所有文件</p><p>24行：得到文件的路径</p><p>25行：得到后缀</p><p>26-29行：是否得到指定后缀的文件</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-bf86f4ad885f0c132be5159b0e06e496_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"507\" data-rawheight=\"87\" class=\"origin_image zh-lightbox-thumb\" width=\"507\" data-original=\"https://pic3.zhimg.com/v2-bf86f4ad885f0c132be5159b0e06e496_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;507&#39; height=&#39;87&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"507\" data-rawheight=\"87\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"507\" data-original=\"https://pic3.zhimg.com/v2-bf86f4ad885f0c132be5159b0e06e496_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-bf86f4ad885f0c132be5159b0e06e496_b.jpg\"/></figure><p>fullname：文件的全路径</p><p>返回的是：文件名</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-3da6759c4a6fd039c02201e5e8c183fc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"178\" data-rawheight=\"22\" class=\"content_image\" width=\"178\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;178&#39; height=&#39;22&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"178\" data-rawheight=\"22\" class=\"content_image lazy\" width=\"178\" data-actualsrc=\"https://pic1.zhimg.com/v2-3da6759c4a6fd039c02201e5e8c183fc_b.jpg\"/></figure><p>去掉后缀</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-0be5e95ca7d3ae783b9b692082ed499a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"104\" data-rawheight=\"24\" class=\"content_image\" width=\"104\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;104&#39; height=&#39;24&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"104\" data-rawheight=\"24\" class=\"content_image lazy\" width=\"104\" data-actualsrc=\"https://pic3.zhimg.com/v2-0be5e95ca7d3ae783b9b692082ed499a_b.jpg\"/></figure><p>从路径中得到文件名</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-aeae315f07610ecb598b0bf179e01baf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"224\" class=\"origin_image zh-lightbox-thumb\" width=\"450\" data-original=\"https://pic4.zhimg.com/v2-aeae315f07610ecb598b0bf179e01baf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;450&#39; height=&#39;224&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"224\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"450\" data-original=\"https://pic4.zhimg.com/v2-aeae315f07610ecb598b0bf179e01baf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-aeae315f07610ecb598b0bf179e01baf_b.jpg\"/></figure><p>函数作用：得到每一张图片的信息和每一个类的信息</p><p>33行：每一张图片对应一个图片中各个对象的信息类</p><p>36行：每一个对象存在于那些图片中（一幅图片可以出现多次）</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-e7520fb5682910393c8906be3f4fa50b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"527\" data-rawheight=\"638\" class=\"origin_image zh-lightbox-thumb\" width=\"527\" data-original=\"https://pic4.zhimg.com/v2-e7520fb5682910393c8906be3f4fa50b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;527&#39; height=&#39;638&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"527\" data-rawheight=\"638\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"527\" data-original=\"https://pic4.zhimg.com/v2-e7520fb5682910393c8906be3f4fa50b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-e7520fb5682910393c8906be3f4fa50b_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-3673773dd26e725440f9c861f9b6d9c4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"692\" data-rawheight=\"478\" class=\"origin_image zh-lightbox-thumb\" width=\"692\" data-original=\"https://pic1.zhimg.com/v2-3673773dd26e725440f9c861f9b6d9c4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;692&#39; height=&#39;478&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"692\" data-rawheight=\"478\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"692\" data-original=\"https://pic1.zhimg.com/v2-3673773dd26e725440f9c861f9b6d9c4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-3673773dd26e725440f9c861f9b6d9c4_b.jpg\"/></figure><p>48-53行：根据不同的python版本打开文件</p><p>55行：开始读取文件里的每一行</p><p>61行：line.strip()移除末尾空格换行符等<a href=\"https://link.zhihu.com/?target=http%3A//www.runoob.com/python/att-string-strip.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Python strip()方法</a>；split空格分开行</p><p>65-76行：得到图片名称和难度</p><p>79行：得到4个点的坐标</p><p>84行后：得到坐标围城的面积</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-ccb4668f9be433b115b58a9dd782e972_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"512\" data-rawheight=\"336\" class=\"origin_image zh-lightbox-thumb\" width=\"512\" data-original=\"https://pic3.zhimg.com/v2-ccb4668f9be433b115b58a9dd782e972_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;512&#39; height=&#39;336&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"512\" data-rawheight=\"336\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"512\" data-original=\"https://pic3.zhimg.com/v2-ccb4668f9be433b115b58a9dd782e972_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-ccb4668f9be433b115b58a9dd782e972_b.jpg\"/></figure><p>44行：将str变成list</p><p>45-46行：如果不指定类则返回所有图片名称</p><p>47行后：得到指定类所在的图片名称</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-5c7453f76322e4886865f33215ff76b4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"512\" data-rawheight=\"116\" class=\"origin_image zh-lightbox-thumb\" width=\"512\" data-original=\"https://pic1.zhimg.com/v2-5c7453f76322e4886865f33215ff76b4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;512&#39; height=&#39;116&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"512\" data-rawheight=\"116\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"512\" data-original=\"https://pic1.zhimg.com/v2-5c7453f76322e4886865f33215ff76b4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-5c7453f76322e4886865f33215ff76b4_b.jpg\"/></figure><p>判断obj的属性，hasattr()函数用于判断对象是否包含对应的属性。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-197b0673b7b5d326c4543dbdf2780d3e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"538\" data-rawheight=\"328\" class=\"origin_image zh-lightbox-thumb\" width=\"538\" data-original=\"https://pic3.zhimg.com/v2-197b0673b7b5d326c4543dbdf2780d3e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;538&#39; height=&#39;328&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"538\" data-rawheight=\"328\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"538\" data-original=\"https://pic3.zhimg.com/v2-197b0673b7b5d326c4543dbdf2780d3e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-197b0673b7b5d326c4543dbdf2780d3e_b.jpg\"/></figure><p>读取所有imgids图片并存在imgs内</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-aa49ffd7840cb73981bf1445abdb1165_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"569\" data-rawheight=\"295\" class=\"origin_image zh-lightbox-thumb\" width=\"569\" data-original=\"https://pic2.zhimg.com/v2-aa49ffd7840cb73981bf1445abdb1165_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;569&#39; height=&#39;295&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"569\" data-rawheight=\"295\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"569\" data-original=\"https://pic2.zhimg.com/v2-aa49ffd7840cb73981bf1445abdb1165_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-aa49ffd7840cb73981bf1445abdb1165_b.jpg\"/></figure><p>返回所有需要的对象，如果只有imgId则返回该图片中的对象，如果catNms不为空则值返回图片中的该类</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-344075fdbe6112c22b673cc8c70acc93_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"465\" data-rawheight=\"339\" class=\"origin_image zh-lightbox-thumb\" width=\"465\" data-original=\"https://pic4.zhimg.com/v2-344075fdbe6112c22b673cc8c70acc93_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;465&#39; height=&#39;339&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"465\" data-rawheight=\"339\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"465\" data-original=\"https://pic4.zhimg.com/v2-344075fdbe6112c22b673cc8c70acc93_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-344075fdbe6112c22b673cc8c70acc93_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-f21fc7d5b6b642450f6ed69695f2e643_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"649\" data-rawheight=\"268\" class=\"origin_image zh-lightbox-thumb\" width=\"649\" data-original=\"https://pic4.zhimg.com/v2-f21fc7d5b6b642450f6ed69695f2e643_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;649&#39; height=&#39;268&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"649\" data-rawheight=\"268\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"649\" data-original=\"https://pic4.zhimg.com/v2-f21fc7d5b6b642450f6ed69695f2e643_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-f21fc7d5b6b642450f6ed69695f2e643_b.jpg\"/></figure><p>77行：显示图片</p><p>78行：关闭坐标轴</p><p>80行：Get Current Axes得到当前坐标轴</p><p>81行：设置属性</p><p>87行：产生随机颜色，每个对象有不同颜色</p><p>94-95行：绘制面积</p><p>96-97行：绘制边界</p><p>98-99行：绘制原点</p>", 
            "topic": [
                {
                    "tag": "数据", 
                    "tagLink": "https://api.zhihu.com/topics/19554449"
                }, 
                {
                    "tag": "遥感", 
                    "tagLink": "https://api.zhihu.com/topics/19948721"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/52297407", 
            "userName": "小白", 
            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
            "upvote": 5, 
            "title": "遥感图像数据集-DOTA", 
            "content": "<p></p><a href=\"https://link.zhihu.com/?target=http%3A//captain.whu.edu.cn/DOTAweb/\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic4.zhimg.com/v2-c665d503135bcdbcbece16253bc86037_180x120.jpg\" data-image-width=\"1892\" data-image-height=\"1499\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">DOTA</a><a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1711.10398.pdf\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">论文链接：DOTA: A Large-scale Dataset for Object Detection in Aerial Images</a><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-a2160e0d3afe2410667d9b31fd26fe8c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"676\" data-rawheight=\"59\" class=\"origin_image zh-lightbox-thumb\" width=\"676\" data-original=\"https://pic1.zhimg.com/v2-a2160e0d3afe2410667d9b31fd26fe8c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;676&#39; height=&#39;59&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"676\" data-rawheight=\"59\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"676\" data-original=\"https://pic1.zhimg.com/v2-a2160e0d3afe2410667d9b31fd26fe8c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-a2160e0d3afe2410667d9b31fd26fe8c_b.jpg\"/></figure><p>关于航空影像中物体检测的大型数据集：DOTA</p><p><b>图片张数</b>：2806</p><p><b>图片大小</b>：4000*4000</p><p><b>分类数</b>：15</p><p><b>包含物体数</b>：188, 282</p><p><b>实例标记方式</b>：任意4边形；顶点按顺时针顺序排列；第一个点（x1，y1），通常表示对象的“头部”；没有视觉线索来决定第一点，所以我们通常会选择左上角为起点</p><p><b>数据集划分</b>：1/2的原始图像作为训练集，1/6作为验证集，1/3作为测试集</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-8d93c883e451222a547da6d046dfc0af_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"805\" data-rawheight=\"631\" class=\"origin_image zh-lightbox-thumb\" width=\"805\" data-original=\"https://pic4.zhimg.com/v2-8d93c883e451222a547da6d046dfc0af_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;805&#39; height=&#39;631&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"805\" data-rawheight=\"631\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"805\" data-original=\"https://pic4.zhimg.com/v2-8d93c883e451222a547da6d046dfc0af_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-8d93c883e451222a547da6d046dfc0af_b.jpg\"/></figure><p><b>评估</b></p><p>从原始图像裁剪一系列1024×1024个补丁，步幅设置为512。</p><p>采用与PASCAL VOC相同的mAP计算。</p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": [
                {
                    "userName": "月光神偷", 
                    "userLink": "https://www.zhihu.com/people/ac7a3705f84d5688779da6dfcc8a535d", 
                    "content": "<p>作者你好，请问测试时，裁剪之后将结果进行合并有代码可以参考吗？</p>", 
                    "likes": 1, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/51708428", 
            "userName": "小白", 
            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
            "upvote": 24, 
            "title": "深度学习难分样本挖掘（Hard Mining）", 
            "content": "<p>《深度学习样本训练策略》</p><p>最近看了几篇文章关于难分样本的挖掘，如何将难分样本抽取出来，通过训练，使得正负样本数量均衡。一般用来减少实验结果的假阳性问题。</p><p>论文：Training Region-based Object Detectors with Online Hard Example Mining</p><p><a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1604.03540.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/pdf/1604.0354</span><span class=\"invisible\">0.pdf</span><span class=\"ellipsis\"></span></a></p><p>代码：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/abhi2610/ohem\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">abhi2610/ohem</a></p><p><b>概念：</b>对于分类来说：</p><ul><li>正样本：我们想要正确分类出的类别所对应的样本，例如，我们需要对一张图片分类，确定是否属于猫，那么在训练的时候，猫的图片就是正样本。</li><li>负样本：根据上面的例子，不是猫的其他所有的图片都是负样本</li><li>难分正样本(hard positives)：错分成负样本的正样本，也可以是训练过程中损失最高的正样本</li><li>难分负样本(hard negatives)：错分成正样本的负样本，也可以是训练过程中损失最高的负样本</li><li>易分正样本(easy positive)：容易正确分类的正样本，该类的概率最高。也可以是训练过程中损失最低的正样本</li><li>易分负样本(easy negatives)：容易正确分类的负样本，该类的概率最高。也可以是训练过程中损失最低的负样本。</li></ul><p><b>核心思想：</b></p><p>用分类器对样本进行分类，把其中错误分类的样本(hard negative)放入负样本集合再继续训练分类器。</p><p>关键是找出影响网络性能的一些训练样本，针对性的进行处理。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-368ee3efb3a5f06d5c1517cef2f1d2dd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"491\" data-rawheight=\"230\" class=\"origin_image zh-lightbox-thumb\" width=\"491\" data-original=\"https://pic2.zhimg.com/v2-368ee3efb3a5f06d5c1517cef2f1d2dd_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;491&#39; height=&#39;230&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"491\" data-rawheight=\"230\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"491\" data-original=\"https://pic2.zhimg.com/v2-368ee3efb3a5f06d5c1517cef2f1d2dd_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-368ee3efb3a5f06d5c1517cef2f1d2dd_b.jpg\"/></figure><p>简单来说就是把难分的样本，剔除出来，放到另外一个地方里面。最后将难分样本，给负样本，加强训练分类器。但是，这样又会造成数据不平衡的问题，下面会讲到。</p><p><b>方法：离线和在线</b></p><ul><li><b>离线：</b></li></ul><p>在样本训练过程中，会将训练结果与GroundTruth计算IOU。通常会设定一个阈值（0.5），结果超过阈值认为是正样本，低于一定阈值的则认为是负样本，然后扔进网络中训练。</p><p>但是，随着训练的进行，这样的策略也许会出现一个问题，那就是正样本的数量会远远小于负样本，这样会导致数据的分布不平衡，使得网络的训练结果不是很好。</p><p>当然有些论文作者针对这种导致不平衡的数据，提出了一种对称的模型。就是类似上图，将Hard Posiotive也重新赋给正样本。</p><ul><li><b>在线：</b></li></ul><p>CVPR2016的<b>Training Region-based Object Detectors with Online Hard Example Mining</b>(oral)将难分样本挖掘(hard example mining)机制嵌入到SGD算法中，使得Fast R-CNN在训练的过程中根据region proposal的损失自动选取合适的Region Proposal作为正负例训练。</p><p>上面的论文就是讲的在线的方法：<b>Online Hard Example Mining</b>，简称<b>OHEM</b></p><p>实验结果表明使用OHEM（Online Hard Example Mining）机制可以使得Fast R-CNN算法在VOC2007和VOC2012上mAP提高 4%左右。</p><p>即：训练的时候选择hard negative来进行迭代,从而提高训练的效果。</p><p>简单来说就是从ROI中选择hard，而不是简单的采样。 </p><p>Forward: 全部的ROI通过网络，根据loss排序; </p><p>Backward：根据排序，选择B/N个loss值最大的（worst）样本来后向传播更新model的weights. </p><p>这里会有一个问题，即位置相近的ROI在map中可能对应的是同一个位置，loss值是相近的，所以针对这个问题，提出的解决方法是：对hard做nms，然后再选择B/N个ROI反向传播，这里nms选择的IoU=0.7。 </p><p>在后向传播时，直觉想到的方法就是将那些未被选中的ROI的loss直接设置为0即可，但这实际上还是将所有的ROI进行反向传播，时间和空间消耗都很大，所以作者在这里提出了本文的网络框架，用两隔网络，一个只用来前向传播，另一个则根据选择的ROIs进行后向传播，的确增加了空间消耗（1G），但是有效的减少了时间消耗，实际的实验结果也是可以接受的。 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-797f0f419105e50134d8bfe455476e67_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1198\" data-rawheight=\"650\" class=\"origin_image zh-lightbox-thumb\" width=\"1198\" data-original=\"https://pic4.zhimg.com/v2-797f0f419105e50134d8bfe455476e67_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1198&#39; height=&#39;650&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1198\" data-rawheight=\"650\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1198\" data-original=\"https://pic4.zhimg.com/v2-797f0f419105e50134d8bfe455476e67_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-797f0f419105e50134d8bfe455476e67_b.jpg\"/></figure><p>给定图像和选择性搜索RoI，卷积网络计算转换特征映射。 在（a）中，只读RoI网络在特征映射和所有RoI上运行正向传递（以绿色箭头显示）。 然后Hard RoI模块使用这些RoI损失来选择B个样本。 在（b）中，RoI网络使用这些硬性示例来计算前向和后向通道（以红色箭头示出）。</p><p>想法很类似于新扩展一个空间，错放错误样本，然后单独训练这些样本，更新权重。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>扩展idea</b></p><p>难分样本挖掘的思想同样可以利用到图像的语义分割上。</p><p>可以对难以分割的样本，或者无法分割的样本，单独建立字典或者模型来训练，更新网络权重。</p><p>用于不平衡数据的扩增也是一个不错的选择。</p><p>文中图片中的思想可以借鉴哦。读者可以自定义一个自己的Hard ROI模块哟~</p><p>【参考】</p><p><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/u014381600/article/details/79161261\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">hard negative mining分析得最好的理解</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/qq_29981283/article/details/83350062\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">hard example mining(困难样本挖掘)</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/u013608402/article/details/51275486\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">论文笔记--FaceNet &amp; Online Hard Example Mining</a> </p><p></p><p></p><p></p><p></p><p></p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "样本均衡", 
                    "tagLink": "https://api.zhihu.com/topics/20139188"
                }, 
                {
                    "tag": "新想法", 
                    "tagLink": "https://api.zhihu.com/topics/19775093"
                }
            ], 
            "comments": [
                {
                    "userName": "大螃蟹", 
                    "userLink": "https://www.zhihu.com/people/a835a9ab0b099c1c529de8ffd150d386", 
                    "content": "<p>棒</p><p></p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "TonyLee", 
                    "userLink": "https://www.zhihu.com/people/7271966ba265f1cf845984e193f306e6", 
                    "content": "<p>感觉难负样本是loss小的，后面困难样本是loss大的，前后要训练的东西有点矛盾</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "zhouyuangan", 
                    "userLink": "https://www.zhihu.com/people/82938bcb340c064d418114c58dfa19d1", 
                    "content": "<p>博主的文章，思路写的不错，很清晰明了哦</p><a class=\"comment_sticker\" href=\"https://pic2.zhimg.com/v2-90359a720808ff45062287127cfa1039.gif\" data-width=\"\" data-height=\"\">[爱心]</a>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/51611463", 
            "userName": "小白", 
            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
            "upvote": 0, 
            "title": "变量和基本数据类型", 
            "content": "<p><a href=\"https://link.zhihu.com/?target=https%3A//study.163.com/course/courseLearn.htm%3FcourseId%3D1005360017%23/learn/video%3FlessonId%3D1052554016%26courseId%3D1005360017\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">C语言零基础入门【基础教程】 - 章节1课时3</a></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-edeffbf1a24d3823c68ba0b164c15377_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"783\" data-rawheight=\"153\" class=\"origin_image zh-lightbox-thumb\" width=\"783\" data-original=\"https://pic4.zhimg.com/v2-edeffbf1a24d3823c68ba0b164c15377_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;783&#39; height=&#39;153&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"783\" data-rawheight=\"153\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"783\" data-original=\"https://pic4.zhimg.com/v2-edeffbf1a24d3823c68ba0b164c15377_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-edeffbf1a24d3823c68ba0b164c15377_b.jpg\"/></figure><h2>不要超过255个字符</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-b521e603eac548cd0d821076cf3312bb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"761\" data-rawheight=\"255\" class=\"origin_image zh-lightbox-thumb\" width=\"761\" data-original=\"https://pic4.zhimg.com/v2-b521e603eac548cd0d821076cf3312bb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;761&#39; height=&#39;255&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"761\" data-rawheight=\"255\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"761\" data-original=\"https://pic4.zhimg.com/v2-b521e603eac548cd0d821076cf3312bb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-b521e603eac548cd0d821076cf3312bb_b.jpg\"/></figure><hr/><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-e3492fd6040f91a9911be6754c4ffa5a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"895\" data-rawheight=\"665\" class=\"origin_image zh-lightbox-thumb\" width=\"895\" data-original=\"https://pic3.zhimg.com/v2-e3492fd6040f91a9911be6754c4ffa5a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;895&#39; height=&#39;665&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"895\" data-rawheight=\"665\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"895\" data-original=\"https://pic3.zhimg.com/v2-e3492fd6040f91a9911be6754c4ffa5a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-e3492fd6040f91a9911be6754c4ffa5a_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-b52ca158936eb5d5b12e387af28485a7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"809\" data-rawheight=\"570\" class=\"origin_image zh-lightbox-thumb\" width=\"809\" data-original=\"https://pic4.zhimg.com/v2-b52ca158936eb5d5b12e387af28485a7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;809&#39; height=&#39;570&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"809\" data-rawheight=\"570\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"809\" data-original=\"https://pic4.zhimg.com/v2-b52ca158936eb5d5b12e387af28485a7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-b52ca158936eb5d5b12e387af28485a7_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-025a3c3bcfdc6897ce64ea0ec997ca5b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1015\" data-rawheight=\"625\" class=\"origin_image zh-lightbox-thumb\" width=\"1015\" data-original=\"https://pic4.zhimg.com/v2-025a3c3bcfdc6897ce64ea0ec997ca5b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1015&#39; height=&#39;625&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1015\" data-rawheight=\"625\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1015\" data-original=\"https://pic4.zhimg.com/v2-025a3c3bcfdc6897ce64ea0ec997ca5b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-025a3c3bcfdc6897ce64ea0ec997ca5b_b.jpg\"/></figure><ul><li>后面加一个f是单精度类型，默认是双精度类型</li><li>sizeof是这个变量占用多少字节数</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-a7fab38d230c175d35dfe05022fa8583_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1167\" data-rawheight=\"447\" class=\"origin_image zh-lightbox-thumb\" width=\"1167\" data-original=\"https://pic4.zhimg.com/v2-a7fab38d230c175d35dfe05022fa8583_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1167&#39; height=&#39;447&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1167\" data-rawheight=\"447\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1167\" data-original=\"https://pic4.zhimg.com/v2-a7fab38d230c175d35dfe05022fa8583_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-a7fab38d230c175d35dfe05022fa8583_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "数据", 
                    "tagLink": "https://api.zhihu.com/topics/19554449"
                }, 
                {
                    "tag": "编程语言", 
                    "tagLink": "https://api.zhihu.com/topics/19552826"
                }, 
                {
                    "tag": "数据类型", 
                    "tagLink": "https://api.zhihu.com/topics/19640364"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/51603024", 
            "userName": "小白", 
            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
            "upvote": 0, 
            "title": "程序重要构成——算法初涉", 
            "content": "<p>视频：<a href=\"https://link.zhihu.com/?target=https%3A//study.163.com/course/courseLearn.htm%3FcourseId%3D1005360017%23/learn/video%3FlessonId%3D1052557010%26courseId%3D1005360017\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">C语言零基础入门【基础教程】章节1课时2</a></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-a27d756de8be7ce5bbbd55eb36a67f66_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"638\" data-rawheight=\"440\" class=\"origin_image zh-lightbox-thumb\" width=\"638\" data-original=\"https://pic3.zhimg.com/v2-a27d756de8be7ce5bbbd55eb36a67f66_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;638&#39; height=&#39;440&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"638\" data-rawheight=\"440\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"638\" data-original=\"https://pic3.zhimg.com/v2-a27d756de8be7ce5bbbd55eb36a67f66_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-a27d756de8be7ce5bbbd55eb36a67f66_b.jpg\"/></figure><hr/><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-241e75d357a169136c0f17c60200db79_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1196\" data-rawheight=\"694\" class=\"origin_image zh-lightbox-thumb\" width=\"1196\" data-original=\"https://pic2.zhimg.com/v2-241e75d357a169136c0f17c60200db79_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1196&#39; height=&#39;694&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1196\" data-rawheight=\"694\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1196\" data-original=\"https://pic2.zhimg.com/v2-241e75d357a169136c0f17c60200db79_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-241e75d357a169136c0f17c60200db79_b.jpg\"/></figure><hr/><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-30dd1ea1ffe4758568d8451fffb4c076_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1303\" data-rawheight=\"711\" class=\"origin_image zh-lightbox-thumb\" width=\"1303\" data-original=\"https://pic3.zhimg.com/v2-30dd1ea1ffe4758568d8451fffb4c076_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1303&#39; height=&#39;711&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1303\" data-rawheight=\"711\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1303\" data-original=\"https://pic3.zhimg.com/v2-30dd1ea1ffe4758568d8451fffb4c076_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-30dd1ea1ffe4758568d8451fffb4c076_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "C（编程语言）", 
                    "tagLink": "https://api.zhihu.com/topics/19561633"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/51563737", 
            "userName": "小白", 
            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
            "upvote": 8, 
            "title": "vs2017安装以编写c语言", 
            "content": "<p><b>安装这两个插件就行了</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-96f32c559e70c4a1af3601dcabdab3ea_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"841\" data-rawheight=\"230\" class=\"origin_image zh-lightbox-thumb\" width=\"841\" data-original=\"https://pic3.zhimg.com/v2-96f32c559e70c4a1af3601dcabdab3ea_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;841&#39; height=&#39;230&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"841\" data-rawheight=\"230\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"841\" data-original=\"https://pic3.zhimg.com/v2-96f32c559e70c4a1af3601dcabdab3ea_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-96f32c559e70c4a1af3601dcabdab3ea_b.jpg\"/></figure><hr/><p><b>创建第一个项目</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-2ffac017d76c26718c5a076d64612d2f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"631\" data-rawheight=\"484\" class=\"origin_image zh-lightbox-thumb\" width=\"631\" data-original=\"https://pic4.zhimg.com/v2-2ffac017d76c26718c5a076d64612d2f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;631&#39; height=&#39;484&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"631\" data-rawheight=\"484\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"631\" data-original=\"https://pic4.zhimg.com/v2-2ffac017d76c26718c5a076d64612d2f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-2ffac017d76c26718c5a076d64612d2f_b.jpg\"/></figure><hr/><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-a570ff8654b13330a12c1390f160efe8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"955\" data-rawheight=\"582\" class=\"origin_image zh-lightbox-thumb\" width=\"955\" data-original=\"https://pic1.zhimg.com/v2-a570ff8654b13330a12c1390f160efe8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;955&#39; height=&#39;582&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"955\" data-rawheight=\"582\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"955\" data-original=\"https://pic1.zhimg.com/v2-a570ff8654b13330a12c1390f160efe8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-a570ff8654b13330a12c1390f160efe8_b.jpg\"/></figure><hr/><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-7899407c317f28cf32d5c3abbdf942e3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"660\" data-rawheight=\"330\" class=\"origin_image zh-lightbox-thumb\" width=\"660\" data-original=\"https://pic4.zhimg.com/v2-7899407c317f28cf32d5c3abbdf942e3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;660&#39; height=&#39;330&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"660\" data-rawheight=\"330\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"660\" data-original=\"https://pic4.zhimg.com/v2-7899407c317f28cf32d5c3abbdf942e3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-7899407c317f28cf32d5c3abbdf942e3_b.jpg\"/></figure><hr/><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-e6b4efa71f5a117ecbe4df4d9efcc4d6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"955\" data-rawheight=\"582\" class=\"origin_image zh-lightbox-thumb\" width=\"955\" data-original=\"https://pic3.zhimg.com/v2-e6b4efa71f5a117ecbe4df4d9efcc4d6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;955&#39; height=&#39;582&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"955\" data-rawheight=\"582\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"955\" data-original=\"https://pic3.zhimg.com/v2-e6b4efa71f5a117ecbe4df4d9efcc4d6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-e6b4efa71f5a117ecbe4df4d9efcc4d6_b.jpg\"/></figure><blockquote>注意：C++ 是在 C 语言的基础上进行的扩展，所有在本质上，C++ 已经包含了 C 语言的所有内容，所以大部分 IDE 会默认创建后缀名为 <code>.cpp</code> 的C++ 源文件。为了大家养成良好的规范，写 C 语言代码，就创建后缀名为 <code>.c</code> 的源文件。</blockquote><hr/><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-e91a99771e7ee50e520359ad8526b2ae_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1230\" data-rawheight=\"276\" class=\"origin_image zh-lightbox-thumb\" width=\"1230\" data-original=\"https://pic3.zhimg.com/v2-e91a99771e7ee50e520359ad8526b2ae_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1230&#39; height=&#39;276&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1230\" data-rawheight=\"276\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1230\" data-original=\"https://pic3.zhimg.com/v2-e91a99771e7ee50e520359ad8526b2ae_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-e91a99771e7ee50e520359ad8526b2ae_b.jpg\"/></figure><p>一键运行程序集成了编译，链接，显示这三个步骤</p><hr/><p><b>VS2017新增安装插件</b></p><p>按照上面的方法安装的VS2017是无法做MFC开发的，这是因为在安装的时候少选的MFC。这时我们要新加MFC，但又不想把VS2017删了在装。可以按一下步骤操作：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-5e0930770d845ff6c993830678e496e2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"697\" data-rawheight=\"300\" class=\"origin_image zh-lightbox-thumb\" width=\"697\" data-original=\"https://pic3.zhimg.com/v2-5e0930770d845ff6c993830678e496e2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;697&#39; height=&#39;300&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"697\" data-rawheight=\"300\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"697\" data-original=\"https://pic3.zhimg.com/v2-5e0930770d845ff6c993830678e496e2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-5e0930770d845ff6c993830678e496e2_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-e3422ec83a2b05b51b1b3c91f6b26b53_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"691\" data-rawheight=\"292\" class=\"origin_image zh-lightbox-thumb\" width=\"691\" data-original=\"https://pic4.zhimg.com/v2-e3422ec83a2b05b51b1b3c91f6b26b53_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;691&#39; height=&#39;292&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"691\" data-rawheight=\"292\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"691\" data-original=\"https://pic4.zhimg.com/v2-e3422ec83a2b05b51b1b3c91f6b26b53_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-e3422ec83a2b05b51b1b3c91f6b26b53_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-84e9609b3227938b8475ea5915bb2a51_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1026\" data-rawheight=\"457\" class=\"origin_image zh-lightbox-thumb\" width=\"1026\" data-original=\"https://pic2.zhimg.com/v2-84e9609b3227938b8475ea5915bb2a51_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1026&#39; height=&#39;457&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1026\" data-rawheight=\"457\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1026\" data-original=\"https://pic2.zhimg.com/v2-84e9609b3227938b8475ea5915bb2a51_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-84e9609b3227938b8475ea5915bb2a51_b.jpg\"/></figure><p>把这个勾上，默认是不勾选的，底下默认设置就可以了。</p>", 
            "topic": [
                {
                    "tag": "C（编程语言）", 
                    "tagLink": "https://api.zhihu.com/topics/19561633"
                }
            ], 
            "comments": [
                {
                    "userName": "知乎用户", 
                    "userLink": "https://www.zhihu.com/people/0", 
                    "content": "奈何我的窗口运行就直接关闭了", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "小白", 
                            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
                            "content": "<p>调试那儿选开始执行（不调试）或者ctrl+f5</p>", 
                            "likes": 0, 
                            "replyToAuthor": "知乎用户"
                        }, 
                        {
                            "userName": "梓泽语宇", 
                            "userLink": "https://www.zhihu.com/people/f7e10383f21cc71d323276ef1acf6550", 
                            "content": "或者在最后加上 一行readkeys", 
                            "likes": 0, 
                            "replyToAuthor": "知乎用户"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50289995", 
            "userName": "小白", 
            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
            "upvote": 1, 
            "title": "pyinstaller踩坑", 
            "content": "<p>一，</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-d398b5bc1896b07ddeb37e9d8ffc7f78_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1958\" data-rawheight=\"1050\" class=\"origin_image zh-lightbox-thumb\" width=\"1958\" data-original=\"https://pic1.zhimg.com/v2-d398b5bc1896b07ddeb37e9d8ffc7f78_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1958&#39; height=&#39;1050&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1958\" data-rawheight=\"1050\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1958\" data-original=\"https://pic1.zhimg.com/v2-d398b5bc1896b07ddeb37e9d8ffc7f78_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-d398b5bc1896b07ddeb37e9d8ffc7f78_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-232e570fda88d1143cba2b40ef65eed5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1958\" data-rawheight=\"1050\" class=\"origin_image zh-lightbox-thumb\" width=\"1958\" data-original=\"https://pic2.zhimg.com/v2-232e570fda88d1143cba2b40ef65eed5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1958&#39; height=&#39;1050&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1958\" data-rawheight=\"1050\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1958\" data-original=\"https://pic2.zhimg.com/v2-232e570fda88d1143cba2b40ef65eed5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-232e570fda88d1143cba2b40ef65eed5_b.jpg\"/></figure><p>使用pip install --upgrade setuptools 更新到最新版本后问题解决</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-ccd26431a3bf7ffb0c9ddc918ca8b351_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"954\" data-rawheight=\"153\" class=\"origin_image zh-lightbox-thumb\" width=\"954\" data-original=\"https://pic2.zhimg.com/v2-ccd26431a3bf7ffb0c9ddc918ca8b351_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;954&#39; height=&#39;153&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"954\" data-rawheight=\"153\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"954\" data-original=\"https://pic2.zhimg.com/v2-ccd26431a3bf7ffb0c9ddc918ca8b351_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-ccd26431a3bf7ffb0c9ddc918ca8b351_b.png\"/></figure><p>在相应路径下创建空文件</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-8dccd3a2a4d303380f9fad608b16cbe6_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"961\" data-rawheight=\"158\" class=\"origin_image zh-lightbox-thumb\" width=\"961\" data-original=\"https://pic3.zhimg.com/v2-8dccd3a2a4d303380f9fad608b16cbe6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;961&#39; height=&#39;158&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"961\" data-rawheight=\"158\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"961\" data-original=\"https://pic3.zhimg.com/v2-8dccd3a2a4d303380f9fad608b16cbe6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-8dccd3a2a4d303380f9fad608b16cbe6_b.png\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-435df468aa2e8ac6b57ddf9ecd3e93f8_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"974\" data-rawheight=\"121\" class=\"origin_image zh-lightbox-thumb\" width=\"974\" data-original=\"https://pic1.zhimg.com/v2-435df468aa2e8ac6b57ddf9ecd3e93f8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;974&#39; height=&#39;121&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"974\" data-rawheight=\"121\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"974\" data-original=\"https://pic1.zhimg.com/v2-435df468aa2e8ac6b57ddf9ecd3e93f8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-435df468aa2e8ac6b57ddf9ecd3e93f8_b.png\"/></figure><p>二：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-8fd7fc568b2a6d09e290ad4127945cd8_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"926\" data-rawheight=\"132\" class=\"origin_image zh-lightbox-thumb\" width=\"926\" data-original=\"https://pic1.zhimg.com/v2-8fd7fc568b2a6d09e290ad4127945cd8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;926&#39; height=&#39;132&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"926\" data-rawheight=\"132\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"926\" data-original=\"https://pic1.zhimg.com/v2-8fd7fc568b2a6d09e290ad4127945cd8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-8fd7fc568b2a6d09e290ad4127945cd8_b.png\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-9ec06acff2bb9f672394208e9b20d5bf_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"735\" data-rawheight=\"67\" class=\"origin_image zh-lightbox-thumb\" width=\"735\" data-original=\"https://pic4.zhimg.com/v2-9ec06acff2bb9f672394208e9b20d5bf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;735&#39; height=&#39;67&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"735\" data-rawheight=\"67\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"735\" data-original=\"https://pic4.zhimg.com/v2-9ec06acff2bb9f672394208e9b20d5bf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-9ec06acff2bb9f672394208e9b20d5bf_b.png\"/></figure><p>三，</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-2494f57893d89b94455fe9a4dff3a025_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"524\" data-rawheight=\"94\" class=\"origin_image zh-lightbox-thumb\" width=\"524\" data-original=\"https://pic2.zhimg.com/v2-2494f57893d89b94455fe9a4dff3a025_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;524&#39; height=&#39;94&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"524\" data-rawheight=\"94\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"524\" data-original=\"https://pic2.zhimg.com/v2-2494f57893d89b94455fe9a4dff3a025_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-2494f57893d89b94455fe9a4dff3a025_b.jpg\"/></figure><p>ico是一种图像格式，改后缀名是没有用的，需要找网站生成。</p>", 
            "topic": [
                {
                    "tag": "exe", 
                    "tagLink": "https://api.zhihu.com/topics/19944251"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50194372", 
            "userName": "小白", 
            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
            "upvote": 1, 
            "title": "PYQT5应用", 
            "content": "<div class=\"highlight\"><pre><code class=\"language-text\"># -*- coding: utf-8 -*-\n\n# Form implementation generated from reading ui file &#39;hgj.ui&#39;\n#\n# Created by: PyQt5 UI code generator 5.11.3\n#\n# WARNING! All changes made in this file will be lost!\n\nfrom PyQt5 import QtCore, QtGui, QtWidgets\n\nclass Ui_Form(object):\n    def setupUi(self, Form):\n        Form.setObjectName(&#34;Form&#34;)\n        Form.resize(1082, 585)\n        self.textEdit = QtWidgets.QTextEdit(Form)\n        self.textEdit.setGeometry(QtCore.QRect(930, 300, 131, 261))\n        self.textEdit.setObjectName(&#34;textEdit&#34;)\n        self.pushButton = QtWidgets.QPushButton(Form)\n        self.pushButton.setGeometry(QtCore.QRect(950, 100, 75, 23))\n        self.pushButton.setObjectName(&#34;pushButton&#34;)\n        self.pushButton_2 = QtWidgets.QPushButton(Form)\n        self.pushButton_2.setGeometry(QtCore.QRect(950, 170, 75, 23))\n        self.pushButton_2.setObjectName(&#34;pushButton_2&#34;)\n        self.graphicsView = QtWidgets.QGraphicsView(Form)\n        self.graphicsView.setGeometry(QtCore.QRect(10, 10, 901, 561))\n        self.graphicsView.setObjectName(&#34;graphicsView&#34;)\n\n        self.retranslateUi(Form)\n        QtCore.QMetaObject.connectSlotsByName(Form)\n\n    def retranslateUi(self, Form):\n        _translate = QtCore.QCoreApplication.translate\n        Form.setWindowTitle(_translate(&#34;Form&#34;, &#34;大河科技&#34;))\n        self.pushButton.setText(_translate(&#34;Form&#34;, &#34;打开&#34;))\n        self.pushButton_2.setText(_translate(&#34;Form&#34;, &#34;保存&#34;))\n\n</code></pre></div><hr/><div class=\"highlight\"><pre><code class=\"language-text\">from hgj import Ui_Form\nfrom PyQt5 import QtWidgets\nimport cv2\nimport numpy as np\nfrom PyQt5.QtGui import *\nfrom PyQt5.QtWidgets import *\n\ndef cv_read(file_path):\n    cv_img = cv2.imdecode(np.fromfile(file_path, dtype=np.uint8), -1)\n    return cv_img\n\n\ndef CV2QImage(cv_image):\n    width = cv_image.shape[1]  # 获取图片宽度\n    height = cv_image.shape[0]  # 获取图片高度\n    \n    pixmap = QPixmap(width, height)  # 根据已知的高度和宽度新建一个空的QPixmap,\n    qimg = pixmap.toImage()  # 将pximap转换为QImage类型的qimg\n\n    # 循环读取cv_image的每个像素的r,g,b值，构成qRgb对象，再设置为qimg内指定位置的像素\n    for row in range(0, height):\n        for col in range(0, width):\n            r = cv_image[row, col, 0]\n            g = cv_image[row, col, 1]\n            b = cv_image[row, col, 2]\n            \n            pix = qRgb(r, g, b)\n            qimg.setPixel(col, row, pix)\n    \n    return qimg  # 转换完成，返回\n\n\nclass MainWindow(QMainWindow, Ui_Form):\n    def __init__(self, parent = None):\n        super(MainWindow,self).__init__(parent)\n        self.setupUi(self)\n        self.a = 0\n        self.pushButton.clicked.connect(self.open_file)\n        self.pushButton_2.clicked.connect(self.save_file)\n        \n    def showText(self):\n        self.a += 1\n        self.textEdit.setText(str(self.a))\n        print(self.a)\n        \n    def open_file(self):\n        fileName1, filetype = QFileDialog.getOpenFileName()  # 设置文件扩展名过滤,注意用双分号间隔\n        img = cv_read(fileName1)  # 读取图像\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # 转换图像通道\n        frame = CV2QImage(img)\n        pix = QPixmap.fromImage(frame)\n        self.item = QGraphicsPixmapItem(pix)  # 创建像素图元\n        self.scene = QGraphicsScene()  # 创建场景\n        self.scene.addItem(self.item)\n        self.graphicsView.setScene(self.scene)\n\n        \n    def save_file(self):\n        fileName2, ok2 = QFileDialog.getSaveFileName()\n        print(fileName2)\n\n\n        \nif __name__ == &#34;__main__&#34;:\n    import sys\n    app = QtWidgets.QApplication(sys.argv)\n\n    ui = MainWindow()\n    ui.setFixedSize(ui.width(),ui.height())\n\n    ui.show()\n    sys.exit(app.exec_())</code></pre></div><p></p>", 
            "topic": [
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/40778602", 
            "userName": "小白", 
            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
            "upvote": 5, 
            "title": "windows下tensorflow生成C++包（1）", 
            "content": "<p></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-c30d0fadbd649d6b2b668babb3876f2d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb\" width=\"450\" data-original=\"https://pic2.zhimg.com/v2-c30d0fadbd649d6b2b668babb3876f2d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;450&#39; height=&#39;299&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"450\" data-original=\"https://pic2.zhimg.com/v2-c30d0fadbd649d6b2b668babb3876f2d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-c30d0fadbd649d6b2b668babb3876f2d_b.jpg\"/></figure><p>怎么把我们训练好的tensorflow模型被C++调用呢？当然是做一个tensorflow的C++库了。windows下需要用CMake编译，本文就是记录这个过程的。</p><p><b>首先，不要试图将tensorflow官网上的CC文件直接给C++调用</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-6bfd96ba54c1d87e8ea68ed1961be19c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"465\" data-rawheight=\"217\" class=\"origin_image zh-lightbox-thumb\" width=\"465\" data-original=\"https://pic1.zhimg.com/v2-6bfd96ba54c1d87e8ea68ed1961be19c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;465&#39; height=&#39;217&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"465\" data-rawheight=\"217\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"465\" data-original=\"https://pic1.zhimg.com/v2-6bfd96ba54c1d87e8ea68ed1961be19c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-6bfd96ba54c1d87e8ea68ed1961be19c_b.jpg\"/></figure><p>这是不行的，比如:</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-83fb727b7eccd98e5f19c2979fe5858d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"538\" data-rawheight=\"487\" class=\"origin_image zh-lightbox-thumb\" width=\"538\" data-original=\"https://pic2.zhimg.com/v2-83fb727b7eccd98e5f19c2979fe5858d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;538&#39; height=&#39;487&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"538\" data-rawheight=\"487\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"538\" data-original=\"https://pic2.zhimg.com/v2-83fb727b7eccd98e5f19c2979fe5858d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-83fb727b7eccd98e5f19c2979fe5858d_b.jpg\"/></figure><p>那么就开始吧！</p><hr/><p><b>step1:安装<a href=\"https://link.zhihu.com/?target=https%3A//visualstudio.microsoft.com/zh-hans/%3Frr%3Dhttps%253A%252F%252Fwww.baidu.com%252Flink%253Furl%253DxTCzD9J8vFTjJvyupEbRo9mfPPDPW8xUTupgqs7iQcB37sK7H6AiCZDwxWiPXgxR%2526wd%253D%2526eqid%253D80309a250008bb11000000035b5c820e\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">vs</a></b></p><p>因为CMake在工作时会<b>自动</b>调用它</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-a98bd81167b02387d8c46301597b1b5f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"714\" data-rawheight=\"436\" class=\"origin_image zh-lightbox-thumb\" width=\"714\" data-original=\"https://pic4.zhimg.com/v2-a98bd81167b02387d8c46301597b1b5f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;714&#39; height=&#39;436&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"714\" data-rawheight=\"436\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"714\" data-original=\"https://pic4.zhimg.com/v2-a98bd81167b02387d8c46301597b1b5f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-a98bd81167b02387d8c46301597b1b5f_b.jpg\"/></figure><hr/><p><b>step2:安装<u><a href=\"https://link.zhihu.com/?target=http%3A//swig.org/download.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Swigwin-3.0.12</a></u></b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-950ce8f132b9b5ab289e203428cbf59d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"682\" data-rawheight=\"313\" class=\"origin_image zh-lightbox-thumb\" width=\"682\" data-original=\"https://pic2.zhimg.com/v2-950ce8f132b9b5ab289e203428cbf59d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;682&#39; height=&#39;313&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"682\" data-rawheight=\"313\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"682\" data-original=\"https://pic2.zhimg.com/v2-950ce8f132b9b5ab289e203428cbf59d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-950ce8f132b9b5ab289e203428cbf59d_b.jpg\"/></figure><p>下载解压以后即可使用，无需安装。在使用CMake时，要配置调用它。</p><hr/><p><b>step3:安装<u><a href=\"https://link.zhihu.com/?target=https%3A//git-scm.com/downloads\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Git</a></u></b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b4e46ac829ea729891e71c8e856a81dd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"437\" data-rawheight=\"158\" class=\"origin_image zh-lightbox-thumb\" width=\"437\" data-original=\"https://pic2.zhimg.com/v2-b4e46ac829ea729891e71c8e856a81dd_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;437&#39; height=&#39;158&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"437\" data-rawheight=\"158\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"437\" data-original=\"https://pic2.zhimg.com/v2-b4e46ac829ea729891e71c8e856a81dd_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-b4e46ac829ea729891e71c8e856a81dd_b.jpg\"/></figure><p>安装它的原因是：CMake在编译的时候要<b>自动</b>调用它。虽然，你不需要使用它。</p><hr/><p><b>step4:安装<u><a href=\"https://link.zhihu.com/?target=https%3A//cmake.org/download/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">CMake-3.8.0</a></u></b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-11382bd52e33628e4382ee5bea48c6ce_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1079\" data-rawheight=\"570\" class=\"origin_image zh-lightbox-thumb\" width=\"1079\" data-original=\"https://pic3.zhimg.com/v2-11382bd52e33628e4382ee5bea48c6ce_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1079&#39; height=&#39;570&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1079\" data-rawheight=\"570\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1079\" data-original=\"https://pic3.zhimg.com/v2-11382bd52e33628e4382ee5bea48c6ce_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-11382bd52e33628e4382ee5bea48c6ce_b.jpg\"/></figure><p>默认设置就行</p><p><b>step5:安装</b><a href=\"https://link.zhihu.com/?target=https%3A//mirrors.tuna.tsinghua.edu.cn/anaconda/archive/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">anconda</a></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-ed1300b43438c5cc01c7cd4f3ac17196_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"592\" data-rawheight=\"198\" class=\"origin_image zh-lightbox-thumb\" width=\"592\" data-original=\"https://pic3.zhimg.com/v2-ed1300b43438c5cc01c7cd4f3ac17196_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;592&#39; height=&#39;198&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"592\" data-rawheight=\"198\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"592\" data-original=\"https://pic3.zhimg.com/v2-ed1300b43438c5cc01c7cd4f3ac17196_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-ed1300b43438c5cc01c7cd4f3ac17196_b.jpg\"/></figure><p>CMake在工作时，会自动调用本机的python。因为anconda集成了许多包，tensorflow在编译时是需要一些依赖包的。</p><hr/><p><b>step6:下载<a href=\"https://link.zhihu.com/?target=https%3A//github.com/tensorflow/tensorflow\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">tensorflow源码</a></b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-fcfb743a8ef6ffc6a73ebca8dcd1a95f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"824\" data-rawheight=\"427\" class=\"origin_image zh-lightbox-thumb\" width=\"824\" data-original=\"https://pic4.zhimg.com/v2-fcfb743a8ef6ffc6a73ebca8dcd1a95f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;824&#39; height=&#39;427&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"824\" data-rawheight=\"427\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"824\" data-original=\"https://pic4.zhimg.com/v2-fcfb743a8ef6ffc6a73ebca8dcd1a95f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-fcfb743a8ef6ffc6a73ebca8dcd1a95f_b.jpg\"/></figure><p>解压就行</p><hr/><p><b>step7:使用CMake</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-6a3ea0297cbccbbbaeee598f20323bae_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"939\" data-rawheight=\"660\" class=\"origin_image zh-lightbox-thumb\" width=\"939\" data-original=\"https://pic3.zhimg.com/v2-6a3ea0297cbccbbbaeee598f20323bae_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;939&#39; height=&#39;660&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"939\" data-rawheight=\"660\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"939\" data-original=\"https://pic3.zhimg.com/v2-6a3ea0297cbccbbbaeee598f20323bae_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-6a3ea0297cbccbbbaeee598f20323bae_b.jpg\"/></figure><p>出现这两个你就算成功了</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-0a7e4b56d045e31f152bb0ea56b2affc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"215\" data-rawheight=\"57\" class=\"content_image\" width=\"215\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;215&#39; height=&#39;57&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"215\" data-rawheight=\"57\" class=\"content_image lazy\" width=\"215\" data-actualsrc=\"https://pic1.zhimg.com/v2-0a7e4b56d045e31f152bb0ea56b2affc_b.jpg\"/></figure><p>参考博客：</p><p><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/jacke121/article/details/80473648\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">VS2015编译TensorFlow C++程序完全攻略</a></p>", 
            "topic": [
                {
                    "tag": "TensorFlow", 
                    "tagLink": "https://api.zhihu.com/topics/20032249"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "卷积神经网络（CNN）", 
                    "tagLink": "https://api.zhihu.com/topics/20043586"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/39973479", 
            "userName": "小白", 
            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
            "upvote": 28, 
            "title": "OpenAI之Glow,基于流的可逆生成模型", 
            "content": "<p>看到最近比较火的一篇图像生成的文章，很有意思．效果也非常的好．值得深入学习一番，欢迎大家指正，一起学习．</p><p>论文链接：<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1807.03039\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">https://www.paperweekly.site/papers/2101</a></p><p>代码链接：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/openai/glow\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/openai/glow</span><span class=\"invisible\"></span></a></p><p>先放上几张Demo.</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-16b7a3e15a4eddbca2dca92bf485ac78_b.jpg\" data-size=\"normal\" data-rawwidth=\"430\" data-rawheight=\"466\" class=\"origin_image zh-lightbox-thumb\" width=\"430\" data-original=\"https://pic1.zhimg.com/v2-16b7a3e15a4eddbca2dca92bf485ac78_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;430&#39; height=&#39;466&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"430\" data-rawheight=\"466\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"430\" data-original=\"https://pic1.zhimg.com/v2-16b7a3e15a4eddbca2dca92bf485ac78_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-16b7a3e15a4eddbca2dca92bf485ac78_b.jpg\"/><figcaption>微笑刘亦菲：将笑容的选项调至最大，尽情的微笑吧</figcaption></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-7396a0c87253f9c18e7735de1f19b758_b.jpg\" data-size=\"normal\" data-rawwidth=\"446\" data-rawheight=\"455\" class=\"origin_image zh-lightbox-thumb\" width=\"446\" data-original=\"https://pic1.zhimg.com/v2-7396a0c87253f9c18e7735de1f19b758_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;446&#39; height=&#39;455&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"446\" data-rawheight=\"455\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"446\" data-original=\"https://pic1.zhimg.com/v2-7396a0c87253f9c18e7735de1f19b758_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-7396a0c87253f9c18e7735de1f19b758_b.jpg\"/><figcaption>小眼睛微笑版刘亦菲</figcaption></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-fccc1b9920873b9cb1b28fbe3b00e3e6_b.jpg\" data-size=\"normal\" data-rawwidth=\"445\" data-rawheight=\"448\" class=\"origin_image zh-lightbox-thumb\" width=\"445\" data-original=\"https://pic3.zhimg.com/v2-fccc1b9920873b9cb1b28fbe3b00e3e6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;445&#39; height=&#39;448&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"445\" data-rawheight=\"448\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"445\" data-original=\"https://pic3.zhimg.com/v2-fccc1b9920873b9cb1b28fbe3b00e3e6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-fccc1b9920873b9cb1b28fbe3b00e3e6_b.jpg\"/><figcaption>老年白发版微笑刘亦菲</figcaption></figure><p>下面的是作者在博客里给出的demo，可以看出图像生成过度非常的自然．</p><a class=\"video-box\" href=\"https://link.zhihu.com/?target=https%3A//www.zhihu.com/video/1002681358216462336\" target=\"_blank\" data-video-id=\"\" data-video-playable=\"true\" data-name=\"\" data-poster=\"https://pic1.zhimg.com/80/v2-14100f37a6f2673652f62c556b9d56c8_b.jpg\" data-lens-id=\"1002681358216462336\"><img class=\"thumbnail\" src=\"https://pic1.zhimg.com/80/v2-14100f37a6f2673652f62c556b9d56c8_b.jpg\"/><span class=\"content\"><span class=\"title\"><span class=\"z-ico-extern-gray\"></span><span class=\"z-ico-extern-blue\"></span></span><span class=\"url\"><span class=\"z-ico-video\"></span>https://www.zhihu.com/video/1002681358216462336</span></span></a><p>直观的了解大佬的工作之后，想要理解，还是要读论文．</p><h2>Glow: Generative Flow with Invertible 1x1 Convolutions</h2><p>基于流的可逆生成模型的优缺点就不再详述了．（网上很多）</p><p>１．背景</p><p>　　文章的工作其实是对输入的复杂的高维数据进行一个非线性变换，同时这个变换是可逆的．通过这个变换将输入的高维数据映射到潜在空间，产生独立的潜在变量（反之亦然）．</p><p>　　a.假设定义x是一个高维的随机向量，并且x的真实分布p(x)未知</p><p>　　b.模型： <img src=\"https://www.zhihu.com/equation?tex=p_%7B%5Ctheta%7D\" alt=\"p_{\\theta}\" eeimg=\"1\"/>(x)， <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 参数</p><p>　　那么，需要最小化的对数似然函数为：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5E%7BN%7D%7B-logp_%7B%5Ctheta%7D%28x%5E%7Bi%7D%29%7D\" alt=\"\\frac{1}{N}\\sum_{i=1}^{N}{-logp_{\\theta}(x^{i})}\" eeimg=\"1\"/> 　</p><p>　　c.潜在空间变量z ,先验概率分布为 <img src=\"https://www.zhihu.com/equation?tex=p_%7B%5Ctheta%7D\" alt=\"p_{\\theta}\" eeimg=\"1\"/>(z)=N(z;0,I)，文章中满足高斯正态分布．</p><p>　　为了实现空间之间的映射，可以找一个可逆的映射函数，比如f，使得<img src=\"https://www.zhihu.com/equation?tex=z%3Df_%7B%5Ctheta%7D%28x%29\" alt=\"z=f_{\\theta}(x)\" eeimg=\"1\"/> ,同时 <img src=\"https://www.zhihu.com/equation?tex=x%3Dg_%7B%5Ctheta%7D%28z%29\" alt=\"x=g_{\\theta}(z)\" eeimg=\"1\"/> , <img src=\"https://www.zhihu.com/equation?tex=g_%7B%5Ctheta%7D%5E%7B-1%7D%3Df_%7B%5Ctheta%7D\" alt=\"g_{\\theta}^{-1}=f_{\\theta}\" eeimg=\"1\"/> </p><p>　　假设函数f由一系列的变换函数组合而成 <img src=\"https://www.zhihu.com/equation?tex=f%3Df_%7B1%7D%5Ccirc\" alt=\"f=f_{1}\\circ\" eeimg=\"1\"/> <img src=\"https://www.zhihu.com/equation?tex=f_%7B2%7D%5Ccirc\" alt=\"f_{2}\\circ\" eeimg=\"1\"/> <img src=\"https://www.zhihu.com/equation?tex=%5Ccdot%5Ccdot%5Ccdot\" alt=\"\\cdot\\cdot\\cdot\" eeimg=\"1\"/> <img src=\"https://www.zhihu.com/equation?tex=%5Ccirc\" alt=\"\\circ\" eeimg=\"1\"/> <img src=\"https://www.zhihu.com/equation?tex=f_%7BK%7D\" alt=\"f_{K}\" eeimg=\"1\"/> ,可以想象出这是一个flow，它的构成可以写成 <img src=\"https://www.zhihu.com/equation?tex=x%5Cleftrightarrow\" alt=\"x\\leftrightarrow\" eeimg=\"1\"/> <img src=\"https://www.zhihu.com/equation?tex=h_%7B1%7D\" alt=\"h_{1}\" eeimg=\"1\"/> <img src=\"https://www.zhihu.com/equation?tex=%5Cleftrightarrow\" alt=\"\\leftrightarrow\" eeimg=\"1\"/> <img src=\"https://www.zhihu.com/equation?tex=h_%7B2%7D\" alt=\"h_{2}\" eeimg=\"1\"/> <img src=\"https://www.zhihu.com/equation?tex=%5Ccdot%5Ccdot%5Ccdot\" alt=\"\\cdot\\cdot\\cdot\" eeimg=\"1\"/> <img src=\"https://www.zhihu.com/equation?tex=z\" alt=\"z\" eeimg=\"1\"/> :　 <img src=\"https://www.zhihu.com/equation?tex=x\" alt=\"x\" eeimg=\"1\"/> 经过 <img src=\"https://www.zhihu.com/equation?tex=f_%7B1%7D\" alt=\"f_{1}\" eeimg=\"1\"/> 变换成 <img src=\"https://www.zhihu.com/equation?tex=h_%7B1%7D\" alt=\"h_{1}\" eeimg=\"1\"/> ,等等等一直到最后．当然不要忘记这是个可逆的过程．　</p><p>　　但是寻找这样一个可逆的映射难度比较大，文章通过引入一个雅克比矩阵来辅助实现映射．由雅克比行列式的一个重要性质：一个可逆函数的雅克比矩阵的逆矩阵即为该函数反函数的雅克比矩阵．基于雅克比矩阵的知识的基础上，就可以实现映射：</p><p><img src=\"https://www.zhihu.com/equation?tex=p_%7B%5Ctheta%7D%28x%29%3Dp_%7B%5Ctheta%7D%28z%29%5Cleft%7C%5Cdet%5Cfrac%7B%5Cpartial%28z%29%7D%7B%5Cpartial%28x%29%7D+%5Cright%7C\" alt=\"p_{\\theta}(x)=p_{\\theta}(z)\\left|\\det\\frac{\\partial(z)}{\\partial(x)} \\right|\" eeimg=\"1\"/> </p><p>左右两端同时求对数</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-9b8c2d34f06664b720002b565b812229_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"545\" data-rawheight=\"141\" class=\"origin_image zh-lightbox-thumb\" width=\"545\" data-original=\"https://pic2.zhimg.com/v2-9b8c2d34f06664b720002b565b812229_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;545&#39; height=&#39;141&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"545\" data-rawheight=\"141\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"545\" data-original=\"https://pic2.zhimg.com/v2-9b8c2d34f06664b720002b565b812229_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-9b8c2d34f06664b720002b565b812229_b.jpg\"/></figure><p>即：三角矩阵行列式的值等于对角线值的相乘，展开为log对数的时候就是sum.</p><p>2．网络框架</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-7a281e7cd07724dc4d1d2511d674f6ab_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"475\" data-rawheight=\"273\" class=\"origin_image zh-lightbox-thumb\" width=\"475\" data-original=\"https://pic4.zhimg.com/v2-7a281e7cd07724dc4d1d2511d674f6ab_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;475&#39; height=&#39;273&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"475\" data-rawheight=\"273\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"475\" data-original=\"https://pic4.zhimg.com/v2-7a281e7cd07724dc4d1d2511d674f6ab_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-7a281e7cd07724dc4d1d2511d674f6ab_b.jpg\"/></figure><p>　　可以看到整个模型右边，整个流需要分为K次单步流来完成．右边的框架中绿色的模块都跟左边的整个模块一样．下面介绍左边这个绿色模块．</p><p>　　简单来说就是三个部分．第一个部分<b>actnorm</b>就是把激活神经元归一化(想象成数据与处理)；第二个部分<b>可逆1x1卷积</b>；第三个部分<b>Affine Transformation</b>仿射变换．</p><p><b>Invertible 1x1 Convolution</b></p><p>可逆 1x1 卷积是在 NICE 和 RealNVP 上的改进，NICE 和 RealNVP 是反转通道的排序，而可逆 1×1 卷积替换该固定置换，其中权重矩阵被初始化为随机旋转矩阵。具有相等数量的输入和输出通道的 1×1 卷积是置换操作的概括。通过矩阵的简化计算，可以简化整体的计算量。</p><p><b>Affine Transformation</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-3df7b2a2ad65dc906b66bb660e357d03_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"673\" data-rawheight=\"112\" class=\"origin_image zh-lightbox-thumb\" width=\"673\" data-original=\"https://pic4.zhimg.com/v2-3df7b2a2ad65dc906b66bb660e357d03_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;673&#39; height=&#39;112&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"673\" data-rawheight=\"112\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"673\" data-original=\"https://pic4.zhimg.com/v2-3df7b2a2ad65dc906b66bb660e357d03_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-3df7b2a2ad65dc906b66bb660e357d03_b.jpg\"/></figure><p>　　仿射变换层可以借助于文中给的表格来理解．正如文章中所介绍的分为三个部分<b>Zero initialization，Split and concatenation，Permutation</b></p><p>本文Glow与之前的RealNVP不同的地方时添加可逆 1x1 卷积，以及删除其他组件，从而整体简化架构。</p><p>　　RealNVP的基本步骤是：</p><p>　　 (1)通过在通道的排列改变为（固定）shuffling 排列。</p><p>　　 (2) 从特征维的中间向下将输入分成两部分：A 和 B。</p><p>　　 (3) 将 A 输入浅层卷积神经网络。根据神经网络的输出对 B 进行线性变换。</p><p>　　 (4) 连接 A 和 B。</p><p>　　本文用卷积神经网络替换之前的置换操作．从表格可见，</p><p>　　Glow的基本步骤是：</p><p>　　(1)可逆 1x1 卷积</p><p>　　(2)从输入tensor的通道维度上将其分成两半 <img src=\"https://www.zhihu.com/equation?tex=x_%7Ba%7D%2Cx_%7Bb%7D\" alt=\"x_{a},x_{b}\" eeimg=\"1\"/> </p><p>　　(3)对 <img src=\"https://www.zhihu.com/equation?tex=x_%7Bb%7D\" alt=\"x_{b}\" eeimg=\"1\"/> 做非线性变换（输入卷积神经网络 <img src=\"https://www.zhihu.com/equation?tex=NN%28%29\" alt=\"NN()\" eeimg=\"1\"/> ），根据其输出对 <img src=\"https://www.zhihu.com/equation?tex=x_%7Ba%7D\" alt=\"x_{a}\" eeimg=\"1\"/> 做仿射耦合变换， <img src=\"https://www.zhihu.com/equation?tex=%5Codot\" alt=\"\\odot\" eeimg=\"1\"/> 为为哈达马积，也就是矩阵的乘法表示</p><p>　　(4)将仿射耦合变换得到的tensor,与 <img src=\"https://www.zhihu.com/equation?tex=x_%7Bb%7D\" alt=\"x_{b}\" eeimg=\"1\"/> 在通道上concat在一起</p><p>　　对于整个网络而言，同时还加入了多尺度的信息，意思就是每一次做流生成潜在变量时，由于要将两个潜在变量拼接后输出，为了保证多尺度就每次保留一个潜在变量，将另一个潜在变量返回到输入再次进行流操作， 经过 L−1 次流操作后将不再返回到输入而是直接输出和之前的 L−1 个潜在变量拼接形成最后的潜在变量的输出。</p><p>3．<b>Simple python implementation of the invertible 1X1 convolution</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-9464d200947acced668cb885199b65cb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"673\" data-rawheight=\"71\" class=\"origin_image zh-lightbox-thumb\" width=\"673\" data-original=\"https://pic4.zhimg.com/v2-9464d200947acced668cb885199b65cb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;673&#39; height=&#39;71&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"673\" data-rawheight=\"71\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"673\" data-original=\"https://pic4.zhimg.com/v2-9464d200947acced668cb885199b65cb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-9464d200947acced668cb885199b65cb_b.jpg\"/></figure><p>结合图表里边的公式理解如下可逆 1x1 卷积代码</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># Invertible 1x1 conv</span>\n<span class=\"k\">def</span> <span class=\"nf\">invertible_1x1_conv</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"p\">,</span> <span class=\"n\">z</span><span class=\"p\">,</span> <span class=\"n\">logdet</span><span class=\"p\">,</span> <span class=\"n\">reverse</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">):</span>\n    <span class=\"k\">if</span> <span class=\"bp\">True</span><span class=\"p\">:</span>  <span class=\"c1\"># Set to &#34;False&#34; to use the LU-decomposed version</span>\n        <span class=\"k\">with</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">variable_scope</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"p\">):</span>\n            <span class=\"n\">shape</span> <span class=\"o\">=</span> <span class=\"n\">Z</span><span class=\"o\">.</span><span class=\"n\">int_shape</span><span class=\"p\">(</span><span class=\"n\">z</span><span class=\"p\">)</span>\n            <span class=\"n\">w_shape</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">3</span><span class=\"p\">],</span> <span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">3</span><span class=\"p\">]]</span>\n            <span class=\"c1\"># Sample a random orthogonal matrix:</span>\n            <span class=\"n\">w_init</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linalg</span><span class=\"o\">.</span><span class=\"n\">qr</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span>\n                <span class=\"o\">*</span><span class=\"n\">w_shape</span><span class=\"p\">))[</span><span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">astype</span><span class=\"p\">(</span><span class=\"s1\">&#39;float32&#39;</span><span class=\"p\">)</span>\n            <span class=\"n\">w</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">get_variable</span><span class=\"p\">(</span><span class=\"s2\">&#34;W&#34;</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">,</span> <span class=\"n\">initializer</span><span class=\"o\">=</span><span class=\"n\">w_init</span><span class=\"p\">)</span>\n            <span class=\"c1\">#dlogdet = tf.linalg.LinearOperator(w).log_abs_determinant() * shape[1]*shape[2]</span>\n            <span class=\"n\">dlogdet</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">cast</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">log</span><span class=\"p\">(</span><span class=\"nb\">abs</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">matrix_determinant</span><span class=\"p\">(</span>\n                <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">cast</span><span class=\"p\">(</span><span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"s1\">&#39;float64&#39;</span><span class=\"p\">)))),</span> <span class=\"s1\">&#39;float32&#39;</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">*</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">]</span>\n            <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">reverse</span><span class=\"p\">:</span>\n                <span class=\"n\">_w</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"n\">w_shape</span><span class=\"p\">)</span>\n                <span class=\"n\">z</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">conv2d</span><span class=\"p\">(</span><span class=\"n\">z</span><span class=\"p\">,</span> <span class=\"n\">_w</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span>\n                                 <span class=\"s1\">&#39;SAME&#39;</span><span class=\"p\">,</span> <span class=\"n\">data_format</span><span class=\"o\">=</span><span class=\"s1\">&#39;NHWC&#39;</span><span class=\"p\">)</span>\n                <span class=\"n\">logdet</span> <span class=\"o\">+=</span> <span class=\"n\">dlogdet</span>\n                <span class=\"k\">return</span> <span class=\"n\">z</span><span class=\"p\">,</span> <span class=\"n\">logdet</span>\n            <span class=\"k\">else</span><span class=\"p\">:</span>\n                <span class=\"n\">_w</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">matrix_inverse</span><span class=\"p\">(</span><span class=\"n\">w</span><span class=\"p\">)</span>\n                <span class=\"n\">_w</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"n\">_w</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">+</span><span class=\"n\">w_shape</span><span class=\"p\">)</span>\n                <span class=\"n\">z</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">conv2d</span><span class=\"p\">(</span><span class=\"n\">z</span><span class=\"p\">,</span> <span class=\"n\">_w</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span>\n                                 <span class=\"s1\">&#39;SAME&#39;</span><span class=\"p\">,</span> <span class=\"n\">data_format</span><span class=\"o\">=</span><span class=\"s1\">&#39;NHWC&#39;</span><span class=\"p\">)</span>\n                <span class=\"n\">logdet</span> <span class=\"o\">-=</span> <span class=\"n\">dlogdet</span>\n                <span class=\"k\">return</span> <span class=\"n\">z</span><span class=\"p\">,</span> <span class=\"n\">logdet</span></code></pre></div><p>代码通过布尔量reverse控制卷积的前向或者反向运算．</p><p>首先，从上一层输出get到各种shape．然后初始化权重矩阵(random orthogonal matrix),由表中公式可知，接下来计算的是权重W的对数行列式．</p><p>如果是前向，那么先reshape权重矩阵，然后做 1x1 卷积，再计算对数行列式．</p><p>如果是反向，那么先对权重矩阵求逆，reshape之后1x1 卷积，再计算对数行列式．</p><p>４．总结</p><p>　　文章提出了一种新型流Glow，并在标准图像建模基准上的对数似然性方面展示了更先进的定量性能。此外，文章证明了在高分辨率人脸训练时，作者的模型能够合成逼真的图像。</p><p>＊＊＊＊＊＊＊＊＊＊＊＊＊＊＊＊＊＊　完　＊＊＊＊＊＊＊＊＊＊＊＊＊＊＊＊＊＊＊＊</p><hr/><p><b>参考</b>：</p><ol><li>Glow: Generative Flow with Invertible 1x1 Convolutions；Diederik P. Kingma*, Prafulla　Dhariwal　OpenAI, San Francisco</li><li>CSDN博客：<a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/qq_36356761/article/details/80986710\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">blog.csdn.net/qq_363567</span><span class=\"invisible\">61/article/details/80986710</span><span class=\"ellipsis\"></span></a></li><li>paperweekly：<a href=\"https://link.zhihu.com/?target=https%3A//www.paperweekly.site/papers/2101\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">paperweekly.site/papers</span><span class=\"invisible\">/2101</span><span class=\"ellipsis\"></span></a></li><li>机器之心：OpenAI提出可逆生成模型Glow</li><li>量子位：OpenAI开源最新生成模型，分分钟合成超逼真人像</li></ol>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "人工智能", 
                    "tagLink": "https://api.zhihu.com/topics/19551275"
                }
            ], 
            "comments": [
                {
                    "userName": "David 9", 
                    "userLink": "https://www.zhihu.com/people/b1b4702f5c5e535176afef5292b42750", 
                    "content": "<p>这篇文章写的也很清晰易懂，推荐：</p><p><a href=\"http://link.zhihu.com/?target=http%3A//nooverfit.com/wp/gan%25E5%2592%258Cvae%25E9%2583%25BDout%25E4%25BA%2586%25EF%25BC%259F%25E7%2590%2586%25E8%25A7%25A3%25E5%259F%25BA%25E4%25BA%258E%25E6%25B5%2581%25E7%259A%2584%25E7%2594%259F%25E6%2588%2590%25E6%25A8%25A1%25E5%259E%258B%25EF%25BC%2588flow-based%25EF%25BC%2589-glow%25EF%25BC%258Crealnvp%25E5%2592%258Cnice/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">GAN和VAE都out了？理解基于流的生成模型（flow-based）: Glow，RealNVP和NICE，David 9的挖坑贴</a></p>", 
                    "likes": 1, 
                    "childComments": []
                }, 
                {
                    "userName": "仟原", 
                    "userLink": "https://www.zhihu.com/people/bbc19f6d60cb1c7e6904647a7d6e2293", 
                    "content": "请问一下这个在线的demo传送门在哪，在官网blog找了很久都找不着呢", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "小白", 
                            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
                            "content": "<p>就是在blog主页上</p>", 
                            "likes": 0, 
                            "replyToAuthor": "仟原"
                        }, 
                        {
                            "userName": "仟原", 
                            "userLink": "https://www.zhihu.com/people/bbc19f6d60cb1c7e6904647a7d6e2293", 
                            "content": "那个在线上传图片的tool是下架了吗，glow的页面上没有诶", 
                            "likes": 0, 
                            "replyToAuthor": "小白"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/38430800", 
            "userName": "小白", 
            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
            "upvote": 2, 
            "title": "学术会议-交钱", 
            "content": "<p></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-15437ec5965620bf3c2a896cef4be468_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb\" width=\"450\" data-original=\"https://pic1.zhimg.com/v2-15437ec5965620bf3c2a896cef4be468_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;450&#39; height=&#39;299&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"450\" data-original=\"https://pic1.zhimg.com/v2-15437ec5965620bf3c2a896cef4be468_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-15437ec5965620bf3c2a896cef4be468_b.jpg\"/></figure><p><b>step1:在会议网站上填好交钱的相关信息</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-f65af97368fc967e852e8801b2020ba7_b.jpg\" data-size=\"normal\" data-rawwidth=\"293\" data-rawheight=\"295\" class=\"content_image\" width=\"293\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;293&#39; height=&#39;295&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"293\" data-rawheight=\"295\" class=\"content_image lazy\" width=\"293\" data-actualsrc=\"https://pic4.zhimg.com/v2-f65af97368fc967e852e8801b2020ba7_b.jpg\"/><figcaption>登陆</figcaption></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-52ddabb909f4565566481b49bca33b73_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"649\" data-rawheight=\"197\" class=\"origin_image zh-lightbox-thumb\" width=\"649\" data-original=\"https://pic4.zhimg.com/v2-52ddabb909f4565566481b49bca33b73_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;649&#39; height=&#39;197&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"649\" data-rawheight=\"197\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"649\" data-original=\"https://pic4.zhimg.com/v2-52ddabb909f4565566481b49bca33b73_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-52ddabb909f4565566481b49bca33b73_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-6480da653228ac4a883bdca15e2e14f6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"655\" data-rawheight=\"339\" class=\"origin_image zh-lightbox-thumb\" width=\"655\" data-original=\"https://pic3.zhimg.com/v2-6480da653228ac4a883bdca15e2e14f6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;655&#39; height=&#39;339&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"655\" data-rawheight=\"339\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"655\" data-original=\"https://pic3.zhimg.com/v2-6480da653228ac4a883bdca15e2e14f6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-6480da653228ac4a883bdca15e2e14f6_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ef77079e445120f91b92a4cec6172eb4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"647\" data-rawheight=\"487\" class=\"origin_image zh-lightbox-thumb\" width=\"647\" data-original=\"https://pic1.zhimg.com/v2-ef77079e445120f91b92a4cec6172eb4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;647&#39; height=&#39;487&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"647\" data-rawheight=\"487\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"647\" data-original=\"https://pic1.zhimg.com/v2-ef77079e445120f91b92a4cec6172eb4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-ef77079e445120f91b92a4cec6172eb4_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-6c57b359773c34c368715e669e382d98_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"632\" data-rawheight=\"541\" class=\"origin_image zh-lightbox-thumb\" width=\"632\" data-original=\"https://pic1.zhimg.com/v2-6c57b359773c34c368715e669e382d98_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;632&#39; height=&#39;541&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"632\" data-rawheight=\"541\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"632\" data-original=\"https://pic1.zhimg.com/v2-6c57b359773c34c368715e669e382d98_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-6c57b359773c34c368715e669e382d98_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-f00a3c3a19292f10573ef5307fe4a130_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"720\" data-rawheight=\"389\" class=\"origin_image zh-lightbox-thumb\" width=\"720\" data-original=\"https://pic1.zhimg.com/v2-f00a3c3a19292f10573ef5307fe4a130_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;720&#39; height=&#39;389&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"720\" data-rawheight=\"389\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"720\" data-original=\"https://pic1.zhimg.com/v2-f00a3c3a19292f10573ef5307fe4a130_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-f00a3c3a19292f10573ef5307fe4a130_b.jpg\"/></figure><p>提交后你就会收到邮件：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-621d4b3c74307f28478f77792f647147_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"822\" data-rawheight=\"338\" class=\"origin_image zh-lightbox-thumb\" width=\"822\" data-original=\"https://pic4.zhimg.com/v2-621d4b3c74307f28478f77792f647147_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;822&#39; height=&#39;338&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"822\" data-rawheight=\"338\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"822\" data-original=\"https://pic4.zhimg.com/v2-621d4b3c74307f28478f77792f647147_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-621d4b3c74307f28478f77792f647147_b.jpg\"/></figure><p>之后，你要把请将汇款凭条放在回执表中照相后<b>转成PDF</b>上传到网站上,就是这：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-cd9052eac577d16fd60c4b49818243ac_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"827\" data-rawheight=\"479\" class=\"origin_image zh-lightbox-thumb\" width=\"827\" data-original=\"https://pic1.zhimg.com/v2-cd9052eac577d16fd60c4b49818243ac_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;827&#39; height=&#39;479&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"827\" data-rawheight=\"479\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"827\" data-original=\"https://pic1.zhimg.com/v2-cd9052eac577d16fd60c4b49818243ac_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-cd9052eac577d16fd60c4b49818243ac_b.jpg\"/></figure><p>ps:在汇款上写上你的论文编号，在把它放在word中转成pdf文件上传。就张这个样：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-6204909315d8a181cf5f8fc0abd948eb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"770\" data-rawheight=\"711\" class=\"origin_image zh-lightbox-thumb\" width=\"770\" data-original=\"https://pic4.zhimg.com/v2-6204909315d8a181cf5f8fc0abd948eb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;770&#39; height=&#39;711&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"770\" data-rawheight=\"711\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"770\" data-original=\"https://pic4.zhimg.com/v2-6204909315d8a181cf5f8fc0abd948eb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-6204909315d8a181cf5f8fc0abd948eb_b.jpg\"/></figure><p>（我们实验有3篇论文投的这个会议一起汇过去的，所以有3个文章编号）</p><p>附：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-e4b57a6dccf1bff6d543863832de5f47_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"293\" data-rawheight=\"304\" class=\"content_image\" width=\"293\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;293&#39; height=&#39;304&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"293\" data-rawheight=\"304\" class=\"content_image lazy\" width=\"293\" data-actualsrc=\"https://pic4.zhimg.com/v2-e4b57a6dccf1bff6d543863832de5f47_b.jpg\"/></figure><p>总结下来，苏州大学的抬头写：苏州大学</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-5be4283549b8ac6ab02fc4513d309d43_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"248\" data-rawheight=\"395\" class=\"content_image\" width=\"248\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;248&#39; height=&#39;395&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"248\" data-rawheight=\"395\" class=\"content_image lazy\" width=\"248\" data-actualsrc=\"https://pic4.zhimg.com/v2-5be4283549b8ac6ab02fc4513d309d43_b.jpg\"/></figure><p>苏州大学的纳税人识别号是：123200004660073435</p><p><b>step2：向其指定的账户你交钱了</b></p><p>在发给你录用邮件中有这个邮件</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ee3e5ed2bf8b18eb6de8be75206d652b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"275\" data-rawheight=\"73\" class=\"content_image\" width=\"275\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;275&#39; height=&#39;73&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"275\" data-rawheight=\"73\" class=\"content_image lazy\" width=\"275\" data-actualsrc=\"https://pic4.zhimg.com/v2-ee3e5ed2bf8b18eb6de8be75206d652b_b.jpg\"/></figure><p>打开看一下，相关会议信息都在里面</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ba9622667a646b068523a78e852347b4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"811\" data-rawheight=\"262\" class=\"origin_image zh-lightbox-thumb\" width=\"811\" data-original=\"https://pic1.zhimg.com/v2-ba9622667a646b068523a78e852347b4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;811&#39; height=&#39;262&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"811\" data-rawheight=\"262\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"811\" data-original=\"https://pic1.zhimg.com/v2-ba9622667a646b068523a78e852347b4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-ba9622667a646b068523a78e852347b4_b.jpg\"/></figure><h2><b>！！！重要的事情打三个感叹号，这只是一个示列。不是每一个会议都交一样的钱，也不是同一个转账账户</b></h2><p><b>Tip1:你要找到交多少钱</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-8bae55cc263f42d47b879719a3431674_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"706\" data-rawheight=\"42\" class=\"origin_image zh-lightbox-thumb\" width=\"706\" data-original=\"https://pic1.zhimg.com/v2-8bae55cc263f42d47b879719a3431674_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;706&#39; height=&#39;42&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"706\" data-rawheight=\"42\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"706\" data-original=\"https://pic1.zhimg.com/v2-8bae55cc263f42d47b879719a3431674_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-8bae55cc263f42d47b879719a3431674_b.jpg\"/></figure><p>一般，我们是学生，非IEEE会员，发一篇论文。所以，我给2号留灯，我交4000块。</p><p><b>Tip2:交钱的截止日期</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-11867ebb5b98b668ff8882a274299049_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"345\" data-rawheight=\"25\" class=\"content_image\" width=\"345\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;345&#39; height=&#39;25&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"345\" data-rawheight=\"25\" class=\"content_image lazy\" width=\"345\" data-actualsrc=\"https://pic2.zhimg.com/v2-11867ebb5b98b668ff8882a274299049_b.jpg\"/></figure><p>在它的截止日期钱把钱打过去，不要自找麻烦！</p><p><b>Tip3:我要向哪个账户打钱</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-948757de2472f592dcc60e4329d3df3f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"356\" data-rawheight=\"73\" class=\"content_image\" width=\"356\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;356&#39; height=&#39;73&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"356\" data-rawheight=\"73\" class=\"content_image lazy\" width=\"356\" data-actualsrc=\"https://pic4.zhimg.com/v2-948757de2472f592dcc60e4329d3df3f_b.jpg\"/></figure><p>保险起见，我还是把账户信息打码吧。万一，你投其它会议也向这个账户打钱了。</p><p><b>Tip4:打钱的一些备注信息</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-5629b52ddaf143e1318901839d1c719f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"425\" data-rawheight=\"21\" class=\"origin_image zh-lightbox-thumb\" width=\"425\" data-original=\"https://pic4.zhimg.com/v2-5629b52ddaf143e1318901839d1c719f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;425&#39; height=&#39;21&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"425\" data-rawheight=\"21\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"425\" data-original=\"https://pic4.zhimg.com/v2-5629b52ddaf143e1318901839d1c719f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-5629b52ddaf143e1318901839d1c719f_b.jpg\"/></figure><p><b>Tip5:怎么拿发票</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ee52b74164a5523c96074b0bf3f7a57f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"161\" data-rawheight=\"31\" class=\"content_image\" width=\"161\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;161&#39; height=&#39;31&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"161\" data-rawheight=\"31\" class=\"content_image lazy\" width=\"161\" data-actualsrc=\"https://pic4.zhimg.com/v2-ee52b74164a5523c96074b0bf3f7a57f_b.jpg\"/></figure><p>一般都是开会时去拿，这个一定不能忘了。要不然开会的钱你就要自己掏了，4000大洋了！</p><p><b>Tip6:超页</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-4f57c43bdf93ca16a4d5743ea24bd6e2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"691\" data-rawheight=\"37\" class=\"origin_image zh-lightbox-thumb\" width=\"691\" data-original=\"https://pic3.zhimg.com/v2-4f57c43bdf93ca16a4d5743ea24bd6e2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;691&#39; height=&#39;37&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"691\" data-rawheight=\"37\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"691\" data-original=\"https://pic3.zhimg.com/v2-4f57c43bdf93ca16a4d5743ea24bd6e2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-4f57c43bdf93ca16a4d5743ea24bd6e2_b.jpg\"/></figure><p>一般会议是最多6页，在多每一页就要收你300大洋喽！</p><p><b>Tip6:看下面的图</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-567a01805bd6f2f93f51d0cb22242d68_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"310\" data-rawheight=\"24\" class=\"content_image\" width=\"310\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;310&#39; height=&#39;24&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"310\" data-rawheight=\"24\" class=\"content_image lazy\" width=\"310\" data-actualsrc=\"https://pic1.zhimg.com/v2-567a01805bd6f2f93f51d0cb22242d68_b.jpg\"/></figure><p>因为，我还没把钱交过去，后面补怎么弄。</p>", 
            "topic": [
                {
                    "tag": "学术", 
                    "tagLink": "https://api.zhihu.com/topics/19558360"
                }, 
                {
                    "tag": "科研", 
                    "tagLink": "https://api.zhihu.com/topics/19556895"
                }
            ], 
            "comments": [
                {
                    "userName": "GJM", 
                    "userLink": "https://www.zhihu.com/people/3e9e68a69e9a3ab482f2164c9df19f5c", 
                    "content": "感谢楼主分享，太需要这个了", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "附庸风雅", 
                    "userLink": "https://www.zhihu.com/people/20f65793581568ebcef7713519614133", 
                    "content": "这个会议好像是我们学校信息所开的😂", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/38404490", 
            "userName": "小白", 
            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
            "upvote": 3, 
            "title": "学术会议最终论文上传与电子版权转让", 
            "content": "<p></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ac4ba1444fedf9bff5c24a3890abd07f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb\" width=\"450\" data-original=\"https://pic4.zhimg.com/v2-ac4ba1444fedf9bff5c24a3890abd07f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;450&#39; height=&#39;299&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"450\" data-original=\"https://pic4.zhimg.com/v2-ac4ba1444fedf9bff5c24a3890abd07f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-ac4ba1444fedf9bff5c24a3890abd07f_b.jpg\"/></figure><p><b>step1:论文通过IEEE PDF eXpress检测</b></p><a href=\"https://link.zhihu.com/?target=https%3A//www.pdf-express.org/\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">pdf-express.org/</span><span class=\"invisible\"></span></a><p>会议ID在发你的录用邮件中有就是这个：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-332c7da8b774550b668e75ce869c5d9e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"301\" data-rawheight=\"107\" class=\"content_image\" width=\"301\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;301&#39; height=&#39;107&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"301\" data-rawheight=\"107\" class=\"content_image lazy\" width=\"301\" data-actualsrc=\"https://pic3.zhimg.com/v2-332c7da8b774550b668e75ce869c5d9e_b.jpg\"/></figure><p>打开文件看见：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-0e393e8f9ac51cabc8011688e6739e7d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"406\" data-rawheight=\"165\" class=\"content_image\" width=\"406\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;406&#39; height=&#39;165&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"406\" data-rawheight=\"165\" class=\"content_image lazy\" width=\"406\" data-actualsrc=\"https://pic2.zhimg.com/v2-0e393e8f9ac51cabc8011688e6739e7d_b.jpg\"/></figure><p>账号和密码和你注册的会议一致（我想你投的会议和IEEE联系好，IEEE就会有你注册该会议的账号和密码信息）</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-7d057eff15413e98634fe9d46700b7ed_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"550\" data-rawheight=\"221\" class=\"origin_image zh-lightbox-thumb\" width=\"550\" data-original=\"https://pic2.zhimg.com/v2-7d057eff15413e98634fe9d46700b7ed_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;550&#39; height=&#39;221&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"550\" data-rawheight=\"221\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"550\" data-original=\"https://pic2.zhimg.com/v2-7d057eff15413e98634fe9d46700b7ed_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-7d057eff15413e98634fe9d46700b7ed_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-aa96caed7ddacbfb7f685f5fc2cf29f6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"509\" data-rawheight=\"92\" class=\"origin_image zh-lightbox-thumb\" width=\"509\" data-original=\"https://pic3.zhimg.com/v2-aa96caed7ddacbfb7f685f5fc2cf29f6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;509&#39; height=&#39;92&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"509\" data-rawheight=\"92\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"509\" data-original=\"https://pic3.zhimg.com/v2-aa96caed7ddacbfb7f685f5fc2cf29f6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-aa96caed7ddacbfb7f685f5fc2cf29f6_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-3555353a32659e77bb2d368f9166cf00_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"553\" data-rawheight=\"265\" class=\"origin_image zh-lightbox-thumb\" width=\"553\" data-original=\"https://pic1.zhimg.com/v2-3555353a32659e77bb2d368f9166cf00_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;553&#39; height=&#39;265&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"553\" data-rawheight=\"265\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"553\" data-original=\"https://pic1.zhimg.com/v2-3555353a32659e77bb2d368f9166cf00_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-3555353a32659e77bb2d368f9166cf00_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-be18ba59bdf6139555b0c65dcc40da2a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"607\" data-rawheight=\"286\" class=\"origin_image zh-lightbox-thumb\" width=\"607\" data-original=\"https://pic3.zhimg.com/v2-be18ba59bdf6139555b0c65dcc40da2a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;607&#39; height=&#39;286&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"607\" data-rawheight=\"286\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"607\" data-original=\"https://pic3.zhimg.com/v2-be18ba59bdf6139555b0c65dcc40da2a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-be18ba59bdf6139555b0c65dcc40da2a_b.jpg\"/></figure><p>之后，你的注册邮箱将会收到两份邮件。一封是接受邮件，一封是结果邮件。收到这个就代表你的PDF文档符合要求</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-d6a6e86b9fee52ad6793d1fe43b8bd93_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1221\" data-rawheight=\"55\" class=\"origin_image zh-lightbox-thumb\" width=\"1221\" data-original=\"https://pic4.zhimg.com/v2-d6a6e86b9fee52ad6793d1fe43b8bd93_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1221&#39; height=&#39;55&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1221\" data-rawheight=\"55\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1221\" data-original=\"https://pic4.zhimg.com/v2-d6a6e86b9fee52ad6793d1fe43b8bd93_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-d6a6e86b9fee52ad6793d1fe43b8bd93_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-c4aca40dd93fae695f037261eea2d03a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"556\" data-rawheight=\"291\" class=\"origin_image zh-lightbox-thumb\" width=\"556\" data-original=\"https://pic3.zhimg.com/v2-c4aca40dd93fae695f037261eea2d03a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;556&#39; height=&#39;291&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"556\" data-rawheight=\"291\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"556\" data-original=\"https://pic3.zhimg.com/v2-c4aca40dd93fae695f037261eea2d03a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-c4aca40dd93fae695f037261eea2d03a_b.jpg\"/></figure><p>附：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-39929b80f4ad30888b9a3efc173196bd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"748\" data-rawheight=\"332\" class=\"origin_image zh-lightbox-thumb\" width=\"748\" data-original=\"https://pic2.zhimg.com/v2-39929b80f4ad30888b9a3efc173196bd_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;748&#39; height=&#39;332&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"748\" data-rawheight=\"332\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"748\" data-original=\"https://pic2.zhimg.com/v2-39929b80f4ad30888b9a3efc173196bd_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-39929b80f4ad30888b9a3efc173196bd_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-6be5c555a7bb57ef620ec3ec8724dadd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"713\" data-rawheight=\"246\" class=\"origin_image zh-lightbox-thumb\" width=\"713\" data-original=\"https://pic2.zhimg.com/v2-6be5c555a7bb57ef620ec3ec8724dadd_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;713&#39; height=&#39;246&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"713\" data-rawheight=\"246\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"713\" data-original=\"https://pic2.zhimg.com/v2-6be5c555a7bb57ef620ec3ec8724dadd_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-6be5c555a7bb57ef620ec3ec8724dadd_b.jpg\"/></figure><p>收到这个是代表不符合，是windows自带的word转pdf的问题。这个只是字没有完全嵌入。</p><p>找一个mac电脑将word转换成pdf</p><p><b>step2：上传最终文稿</b></p><p>依次选择</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b59c94d62d10a8b4cf447f7d8e8c1569_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"369\" data-rawheight=\"179\" class=\"content_image\" width=\"369\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;369&#39; height=&#39;179&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"369\" data-rawheight=\"179\" class=\"content_image lazy\" width=\"369\" data-actualsrc=\"https://pic2.zhimg.com/v2-b59c94d62d10a8b4cf447f7d8e8c1569_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-28a785d648f3d608b98bd4bc39b49da5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"352\" data-rawheight=\"144\" class=\"content_image\" width=\"352\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;352&#39; height=&#39;144&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"352\" data-rawheight=\"144\" class=\"content_image lazy\" width=\"352\" data-actualsrc=\"https://pic2.zhimg.com/v2-28a785d648f3d608b98bd4bc39b49da5_b.jpg\"/></figure><p>（建议论文题目和作者就不要改了，要不然超麻烦）</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-1bc495eb932a69eeae04427a5433c016_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"599\" data-rawheight=\"229\" class=\"origin_image zh-lightbox-thumb\" width=\"599\" data-original=\"https://pic3.zhimg.com/v2-1bc495eb932a69eeae04427a5433c016_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;599&#39; height=&#39;229&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"599\" data-rawheight=\"229\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"599\" data-original=\"https://pic3.zhimg.com/v2-1bc495eb932a69eeae04427a5433c016_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-1bc495eb932a69eeae04427a5433c016_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-23b98a42ead8cf3a43fc1117305e70b9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"570\" data-rawheight=\"226\" class=\"origin_image zh-lightbox-thumb\" width=\"570\" data-original=\"https://pic2.zhimg.com/v2-23b98a42ead8cf3a43fc1117305e70b9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;570&#39; height=&#39;226&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"570\" data-rawheight=\"226\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"570\" data-original=\"https://pic2.zhimg.com/v2-23b98a42ead8cf3a43fc1117305e70b9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-23b98a42ead8cf3a43fc1117305e70b9_b.jpg\"/></figure><p>把通过PDF的格式检查的论文上传</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-369c48fdb2bb8dc48585b71f2615436a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"589\" data-rawheight=\"205\" class=\"origin_image zh-lightbox-thumb\" width=\"589\" data-original=\"https://pic3.zhimg.com/v2-369c48fdb2bb8dc48585b71f2615436a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;589&#39; height=&#39;205&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"589\" data-rawheight=\"205\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"589\" data-original=\"https://pic3.zhimg.com/v2-369c48fdb2bb8dc48585b71f2615436a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-369c48fdb2bb8dc48585b71f2615436a_b.jpg\"/></figure><p>你的注册邮箱将收到</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-f7e8995a38dddb412f048f9311737b11_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1124\" data-rawheight=\"450\" class=\"origin_image zh-lightbox-thumb\" width=\"1124\" data-original=\"https://pic2.zhimg.com/v2-f7e8995a38dddb412f048f9311737b11_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1124&#39; height=&#39;450&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1124\" data-rawheight=\"450\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1124\" data-original=\"https://pic2.zhimg.com/v2-f7e8995a38dddb412f048f9311737b11_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-f7e8995a38dddb412f048f9311737b11_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-810b9741f06ae51dea4b1a198c43d424_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"639\" data-rawheight=\"353\" class=\"origin_image zh-lightbox-thumb\" width=\"639\" data-original=\"https://pic1.zhimg.com/v2-810b9741f06ae51dea4b1a198c43d424_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;639&#39; height=&#39;353&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"639\" data-rawheight=\"353\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"639\" data-original=\"https://pic1.zhimg.com/v2-810b9741f06ae51dea4b1a198c43d424_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-810b9741f06ae51dea4b1a198c43d424_b.jpg\"/></figure><p><b>step3：签署电子版权</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-7fab28e8228d3e9c75bca26d70b3066d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"877\" data-rawheight=\"150\" class=\"origin_image zh-lightbox-thumb\" width=\"877\" data-original=\"https://pic2.zhimg.com/v2-7fab28e8228d3e9c75bca26d70b3066d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;877&#39; height=&#39;150&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"877\" data-rawheight=\"150\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"877\" data-original=\"https://pic2.zhimg.com/v2-7fab28e8228d3e9c75bca26d70b3066d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-7fab28e8228d3e9c75bca26d70b3066d_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-efb29ef720b0885a9756f19f641fce63_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1156\" data-rawheight=\"446\" class=\"origin_image zh-lightbox-thumb\" width=\"1156\" data-original=\"https://pic4.zhimg.com/v2-efb29ef720b0885a9756f19f641fce63_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1156&#39; height=&#39;446&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1156\" data-rawheight=\"446\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1156\" data-original=\"https://pic4.zhimg.com/v2-efb29ef720b0885a9756f19f641fce63_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-efb29ef720b0885a9756f19f641fce63_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-4c1295a3d25bd2a76630107369da3a03_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"961\" data-rawheight=\"534\" class=\"origin_image zh-lightbox-thumb\" width=\"961\" data-original=\"https://pic4.zhimg.com/v2-4c1295a3d25bd2a76630107369da3a03_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;961&#39; height=&#39;534&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"961\" data-rawheight=\"534\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"961\" data-original=\"https://pic4.zhimg.com/v2-4c1295a3d25bd2a76630107369da3a03_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-4c1295a3d25bd2a76630107369da3a03_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-c0ee78d3f37c21ea820cc8bc8da06e5c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"880\" data-rawheight=\"511\" class=\"origin_image zh-lightbox-thumb\" width=\"880\" data-original=\"https://pic1.zhimg.com/v2-c0ee78d3f37c21ea820cc8bc8da06e5c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;880&#39; height=&#39;511&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"880\" data-rawheight=\"511\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"880\" data-original=\"https://pic1.zhimg.com/v2-c0ee78d3f37c21ea820cc8bc8da06e5c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-c0ee78d3f37c21ea820cc8bc8da06e5c_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-37fb0c9226b31273d9f8f341a1968ab2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"647\" data-rawheight=\"361\" class=\"origin_image zh-lightbox-thumb\" width=\"647\" data-original=\"https://pic3.zhimg.com/v2-37fb0c9226b31273d9f8f341a1968ab2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;647&#39; height=&#39;361&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"647\" data-rawheight=\"361\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"647\" data-original=\"https://pic3.zhimg.com/v2-37fb0c9226b31273d9f8f341a1968ab2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-37fb0c9226b31273d9f8f341a1968ab2_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-d07e75103a66056d85d42bf03be082ef_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"967\" data-rawheight=\"450\" class=\"origin_image zh-lightbox-thumb\" width=\"967\" data-original=\"https://pic4.zhimg.com/v2-d07e75103a66056d85d42bf03be082ef_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;967&#39; height=&#39;450&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"967\" data-rawheight=\"450\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"967\" data-original=\"https://pic4.zhimg.com/v2-d07e75103a66056d85d42bf03be082ef_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-d07e75103a66056d85d42bf03be082ef_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-938b5f067a2bcd64911bca3ec8dbf9c5_b.jpg\" data-size=\"normal\" data-rawwidth=\"878\" data-rawheight=\"388\" class=\"origin_image zh-lightbox-thumb\" width=\"878\" data-original=\"https://pic2.zhimg.com/v2-938b5f067a2bcd64911bca3ec8dbf9c5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;878&#39; height=&#39;388&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"878\" data-rawheight=\"388\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"878\" data-original=\"https://pic2.zhimg.com/v2-938b5f067a2bcd64911bca3ec8dbf9c5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-938b5f067a2bcd64911bca3ec8dbf9c5_b.jpg\"/><figcaption>这个问你是不是政府雇员什么的，一般学生勾最后一个就行了</figcaption></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-1b7087b79ddd835f1c19259357a1a283_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"642\" data-rawheight=\"422\" class=\"origin_image zh-lightbox-thumb\" width=\"642\" data-original=\"https://pic4.zhimg.com/v2-1b7087b79ddd835f1c19259357a1a283_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;642&#39; height=&#39;422&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"642\" data-rawheight=\"422\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"642\" data-original=\"https://pic4.zhimg.com/v2-1b7087b79ddd835f1c19259357a1a283_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-1b7087b79ddd835f1c19259357a1a283_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-b31b3b230f7372fac86238ee759cf00a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"887\" data-rawheight=\"339\" class=\"origin_image zh-lightbox-thumb\" width=\"887\" data-original=\"https://pic3.zhimg.com/v2-b31b3b230f7372fac86238ee759cf00a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;887&#39; height=&#39;339&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"887\" data-rawheight=\"339\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"887\" data-original=\"https://pic3.zhimg.com/v2-b31b3b230f7372fac86238ee759cf00a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-b31b3b230f7372fac86238ee759cf00a_b.jpg\"/></figure><p>签完之后，你将会收到这个</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-a57f130570e5295c1a592d1514d2d61a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"641\" data-rawheight=\"216\" class=\"origin_image zh-lightbox-thumb\" width=\"641\" data-original=\"https://pic3.zhimg.com/v2-a57f130570e5295c1a592d1514d2d61a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;641&#39; height=&#39;216&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"641\" data-rawheight=\"216\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"641\" data-original=\"https://pic3.zhimg.com/v2-a57f130570e5295c1a592d1514d2d61a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-a57f130570e5295c1a592d1514d2d61a_b.jpg\"/></figure><p>自此，最终论文提交和电子版权转让就结束了，接下来就是交钱了。</p><p><b>ps:最近运气有点好啊，买了三场球都中了</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-f021b8e09e9cfe658ebceed0b13535a9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"2160\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-f021b8e09e9cfe658ebceed0b13535a9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;2160&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"2160\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-f021b8e09e9cfe658ebceed0b13535a9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-f021b8e09e9cfe658ebceed0b13535a9_b.jpg\"/></figure><p>希望下四场也一样</p>", 
            "topic": [
                {
                    "tag": "论文", 
                    "tagLink": "https://api.zhihu.com/topics/19572442"
                }, 
                {
                    "tag": "教育", 
                    "tagLink": "https://api.zhihu.com/topics/19553176"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/38181216", 
            "userName": "小白", 
            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
            "upvote": 35, 
            "title": "图像超分辨率综述（深度学习方向）", 
            "content": "<p></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-c30d0fadbd649d6b2b668babb3876f2d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb\" width=\"450\" data-original=\"https://pic2.zhimg.com/v2-c30d0fadbd649d6b2b668babb3876f2d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;450&#39; height=&#39;299&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"450\" data-original=\"https://pic2.zhimg.com/v2-c30d0fadbd649d6b2b668babb3876f2d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-c30d0fadbd649d6b2b668babb3876f2d_b.jpg\"/></figure><p><b>目标：</b></p><p>单图像超分辨率技术涉及到增加小图像的大小，同时尽可能地防止其质量下降。</p><p><b>方法：</b></p><p><b>SRCNN相关的介绍都在这里：</b></p><a href=\"https://zhuanlan.zhihu.com/p/38166906\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-7ddb099e4c6fa7d6603cf774219f55b8_120x160.jpg\" data-image-width=\"364\" data-image-height=\"526\" class=\"internal\">小白：图像超分辨-SRCNN（目录）</a><hr/><p><b>SRCNN的缺点：</b></p><p>通过最小化 MSE 获取的图像过于平滑，无法捕捉模型输出和真值图像之间的感知区别。</p><p><b>感知损失（Perceptual loss）：</b></p><ul><li>从预训练卷积神经网络中提取的图像高级特征表示。图像分类网络把物体细节的信息存储在特征图中，这就为提升后的图像尽可能地逼真提供可能。</li><li>这篇论文同样使用了更加复杂的网络</li></ul><a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1603.08155.pdf\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Perceptual Losses for Real-Time Style Transfer and Super-Resolution</a><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-923c1efce691c2f017b84fb2a2da4a8f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"480\" data-rawheight=\"338\" class=\"origin_image zh-lightbox-thumb\" width=\"480\" data-original=\"https://pic4.zhimg.com/v2-923c1efce691c2f017b84fb2a2da4a8f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;480&#39; height=&#39;338&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"480\" data-rawheight=\"338\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"480\" data-original=\"https://pic4.zhimg.com/v2-923c1efce691c2f017b84fb2a2da4a8f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-923c1efce691c2f017b84fb2a2da4a8f_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-a8ce6a3b6164ddace83a36df4f68bf1f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"480\" data-rawheight=\"340\" class=\"origin_image zh-lightbox-thumb\" width=\"480\" data-original=\"https://pic4.zhimg.com/v2-a8ce6a3b6164ddace83a36df4f68bf1f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;480&#39; height=&#39;340&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"480\" data-rawheight=\"340\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"480\" data-original=\"https://pic4.zhimg.com/v2-a8ce6a3b6164ddace83a36df4f68bf1f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-a8ce6a3b6164ddace83a36df4f68bf1f_b.jpg\"/></figure><p>(感知损失用于图像风格转换)</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-3e5a178ab06e997e1914eec37474a6fc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"673\" data-rawheight=\"374\" class=\"origin_image zh-lightbox-thumb\" width=\"673\" data-original=\"https://pic1.zhimg.com/v2-3e5a178ab06e997e1914eec37474a6fc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;673&#39; height=&#39;374&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"673\" data-rawheight=\"374\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"673\" data-original=\"https://pic1.zhimg.com/v2-3e5a178ab06e997e1914eec37474a6fc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-3e5a178ab06e997e1914eec37474a6fc_b.jpg\"/></figure><p>(图像超分辨率)</p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/huangguojie880/fast-neural-style-keras\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic4.zhimg.com/v2-a3703e2b11e6d5a2257366a5e392811f_ipico.jpg\" data-image-width=\"420\" data-image-height=\"420\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">huangguojie880/fast-neural-style-keras</a><hr/><p><b>更加复杂的网络结构</b></p><a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1609.04802.pdf\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network</a><p>它是使用的ResNet,其特点：</p><ol><li>使用了反卷积</li><li>使用了PRelu</li><li>还用了GAN结构</li></ol><a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1707.02921.pdf\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Enhanced Deep Residual Networks for Single Image Super-Resolution</a><p>使用更加复杂的ResNet结构</p><ol><li>移除残差网络中的批归一化</li><li>把残差层的数量从 16 增加到 32</li></ol><p>另外几篇推荐论文：</p><a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1612.07919.pdf\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">EnhanceNet: Single Image Super-Resolution Through Automated Texture Synthesis</a><p>另一个深度学习超分辨率论文</p><a href=\"https://link.zhihu.com/?target=http%3A//yima.csl.illinois.edu/psfile/ECE598-08/CVPR2008-SR.pdf\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Image Super-Resolution as Sparse Representation of Raw Image Patches</a><p>这篇是一个传统方法，用的是稀疏表示这应该是非深度学习方法的图像超分辨率的最好了，发表在CVPR2008，SRCNN就有借鉴它。</p><p><b>字典：</b></p><p>无论有多少句子需要被书写，对于一个句子来说它最本质的特征是什么呢？毫无疑问，是一个个构成这个句子的单词（对英语来说）或字（对汉语来说）。所以我们可以很傲娇的这样认为，无论人类的知识有多么浩繁，也无论人类的科技有多么发达，一本长不过20厘米，宽不过15厘米，厚不过4厘米的新华字典或牛津字典足以表达人类从古至今乃至未来的所有知识，那些知识只不过是字典中字的排列组合罢了！直到这里，我相信相当一部分读者或许在心中已经明白了字典学习的第一个好处——它实质上是对于庞大数据集的一种降维表示。第二，正如同字是句子最质朴的特征一样，字典学习总是尝试学习蕴藏在样本背后最质朴的特征（假如样本最质朴的特征就是样本最好的特征）</p><p><b>稀疏：</b></p><p>用尽可能少的资源表示尽可能多的知识，这种表示还能带来一个附加的好处，即计算速度快。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-6d4781bc81678c3d44e31041ac7d4cdc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"978\" data-rawheight=\"980\" class=\"origin_image zh-lightbox-thumb\" width=\"978\" data-original=\"https://pic1.zhimg.com/v2-6d4781bc81678c3d44e31041ac7d4cdc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;978&#39; height=&#39;980&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"978\" data-rawheight=\"980\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"978\" data-original=\"https://pic1.zhimg.com/v2-6d4781bc81678c3d44e31041ac7d4cdc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-6d4781bc81678c3d44e31041ac7d4cdc_b.jpg\"/></figure><a href=\"https://link.zhihu.com/?target=https%3A//www.cnblogs.com/hdu-zsk/p/5954658.html\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic2.zhimg.com/v2-1f742c3dc636e1df883c9e68dc291489_120x160.jpg\" data-image-width=\"214\" data-image-height=\"380\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Dictionary Learning(字典学习、稀疏表示以及其他)</a><a href=\"https://link.zhihu.com/?target=https%3A//github.com/luckyjie/digital-image-processing/tree/master/ScSR\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-43bbc866cbe2696edd4aa248a0bbc52c_ipico.jpg\" data-image-width=\"420\" data-image-height=\"420\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">上面论文的源码</a><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/jiangjieqazwsx/article/details/50414259\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">上面论文的博客解析</a><a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1706.09077.pdf\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Super-Resolution via Deep Learning</a><p>这个好像在github上有许多高赞实现</p><a href=\"https://link.zhihu.com/?target=https%3A//www.researchgate.net/publication/272031125_Super-resolution_A_comprehensive_survey\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Super-resolution: A comprehensive survey</a><p>这个是一个关于图像超分辨率的调查报告</p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "神经网络", 
                    "tagLink": "https://api.zhihu.com/topics/19607065"
                }
            ], 
            "comments": [
                {
                    "userName": "知乎用户", 
                    "userLink": "https://www.zhihu.com/people/0", 
                    "content": "<p>写得不错!谢谢!</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/37593115", 
            "userName": "小白", 
            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
            "upvote": 0, 
            "title": "python对字典类型按值进行排序", 
            "content": "<p></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-1afa112d20750cfb521b35ad1baa4e4f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb\" width=\"450\" data-original=\"https://pic4.zhimg.com/v2-1afa112d20750cfb521b35ad1baa4e4f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;450&#39; height=&#39;299&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"450\" data-original=\"https://pic4.zhimg.com/v2-1afa112d20750cfb521b35ad1baa4e4f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-1afa112d20750cfb521b35ad1baa4e4f_b.jpg\"/></figure><div class=\"highlight\"><pre><code class=\"language-text\">import operator\ngt_counter_per_class = {&#39;tap&#39;: 18, &#39;countertop&#39;: 21, &#39;sink&#39;: 14, &#39;bowl&#39;: 15, &#39;bottle&#39;: 11, &#39;chair&#39;: 106}\nprint(gt_counter_per_class)\nprint(gt_counter_per_class.items())\nprint(sorted(gt_counter_per_class.items(), key=operator.itemgetter(1)))\nprint(sorted(gt_counter_per_class, key=operator.itemgetter(1)))</code></pre></div><p>输出：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-94d7d4825b6d5e1c98b2662648c74174_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"675\" data-rawheight=\"103\" class=\"origin_image zh-lightbox-thumb\" width=\"675\" data-original=\"https://pic1.zhimg.com/v2-94d7d4825b6d5e1c98b2662648c74174_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;675&#39; height=&#39;103&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"675\" data-rawheight=\"103\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"675\" data-original=\"https://pic1.zhimg.com/v2-94d7d4825b6d5e1c98b2662648c74174_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-94d7d4825b6d5e1c98b2662648c74174_b.jpg\"/></figure><a href=\"https://link.zhihu.com/?target=http%3A//www.runoob.com/python/python-func-sorted.html\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Python sorted() 函数</a><a href=\"https://link.zhihu.com/?target=https%3A//www.cnblogs.com/zhoufankui/p/6274172.html\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">python中的operator.itemgetter函数 - 蜗牛到牛 - 博客园</a><a href=\"https://link.zhihu.com/?target=https%3A//www.cnblogs.com/sysu-blackbear/p/3283993.html\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">[转].Python中sorted函数的用法 - 中大黑熊 - 博客园</a><p></p>", 
            "topic": [
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }, 
                {
                    "tag": "编程语言", 
                    "tagLink": "https://api.zhihu.com/topics/19552826"
                }, 
                {
                    "tag": "编程", 
                    "tagLink": "https://api.zhihu.com/topics/19554298"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/37473684", 
            "userName": "小白", 
            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
            "upvote": 3, 
            "title": "mAP解析", 
            "content": "<p></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-1afa112d20750cfb521b35ad1baa4e4f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb\" width=\"450\" data-original=\"https://pic4.zhimg.com/v2-1afa112d20750cfb521b35ad1baa4e4f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;450&#39; height=&#39;299&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"450\" data-original=\"https://pic4.zhimg.com/v2-1afa112d20750cfb521b35ad1baa4e4f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-1afa112d20750cfb521b35ad1baa4e4f_b.jpg\"/></figure><p><b>mAP与AP的区别：</b></p><p>AP：average precision，每一类别P值的平均值</p><p>mAP：mean average precision，对所有类别的AP取均值</p><hr/><p><b>准确率(Precision)、 召回率(Recall):</b></p><p>             召回率(Recall)      =  系统检索到的相关文件 / 系统所有相关的文件总数<br/>             准确率(Precision) =  系统检索到的相关文件 / 系统所有检索到的文件总数</p><p>图示表示如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-d4cd98cf6b983e1fec0c4d967b5a5634_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"440\" data-rawheight=\"276\" class=\"origin_image zh-lightbox-thumb\" width=\"440\" data-original=\"https://pic1.zhimg.com/v2-d4cd98cf6b983e1fec0c4d967b5a5634_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;440&#39; height=&#39;276&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"440\" data-rawheight=\"276\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"440\" data-original=\"https://pic1.zhimg.com/v2-d4cd98cf6b983e1fec0c4d967b5a5634_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-d4cd98cf6b983e1fec0c4d967b5a5634_b.jpg\"/></figure><hr/><p>Recall = {0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1}时：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3a959e9631b2434026880f5ed6f93f2d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"547\" data-rawheight=\"411\" class=\"origin_image zh-lightbox-thumb\" width=\"547\" data-original=\"https://pic2.zhimg.com/v2-3a959e9631b2434026880f5ed6f93f2d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;547&#39; height=&#39;411&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"547\" data-rawheight=\"411\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"547\" data-original=\"https://pic2.zhimg.com/v2-3a959e9631b2434026880f5ed6f93f2d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-3a959e9631b2434026880f5ed6f93f2d_b.jpg\"/></figure><p>这样得到的AP曲线是间断的</p><hr/><p>假设这N个样本中有M个正例，那么我们会得到M个recall值（1/M, 2/M, ..., M/M）,对于每个recall值r，我们可以计算出对应的最大precision，然后对这M个precision值取平均即得到最后的AP值，这样得到的是连续的。</p><p><img src=\"https://www.zhihu.com/equation?tex=AP+%3D%5Cfrac%7B1%7D%7B11%7D+%5Csum_%7Brecall%7D%5E%7B%7D%7BPrecision%28Recall%29%7D+\" alt=\"AP =\\frac{1}{11} \\sum_{recall}^{}{Precision(Recall)} \" eeimg=\"1\"/> </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-a4b5883747d2f275cdadbef7b7c501dc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"690\" data-rawheight=\"331\" class=\"origin_image zh-lightbox-thumb\" width=\"690\" data-original=\"https://pic1.zhimg.com/v2-a4b5883747d2f275cdadbef7b7c501dc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;690&#39; height=&#39;331&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"690\" data-rawheight=\"331\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"690\" data-original=\"https://pic1.zhimg.com/v2-a4b5883747d2f275cdadbef7b7c501dc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-a4b5883747d2f275cdadbef7b7c501dc_b.jpg\"/></figure><p><b>？Recall为3/6时，max precision对应的不应该是3/6吗</b></p><p><b>不是，还是4/7。</b>max precision取值范围在 <img src=\"https://www.zhihu.com/equation?tex=r%5E%7B%27%7D\" alt=\"r^{&#39;}\" eeimg=\"1\"/> 大于等于 <img src=\"https://www.zhihu.com/equation?tex=r\" alt=\"r\" eeimg=\"1\"/> 里面找，召回率大于3/6的序号是6到20，明显4/7最大。</p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/35695143", 
            "userName": "小白", 
            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
            "upvote": 1, 
            "title": "tensorflow趣味玩法二：微信监管训练", 
            "content": "<p>我们在使用tensorflow训练比较复杂的深度学习模型的时候，尤其在数据量很大的情况下，需要花费大量的时间才能得到比较理想的模型。然而我们却不太可能一直在机器前守着，监管训练的整个过程。在这里，我们希望实现的是在手机上借助微信来远程实时监管机器上的训练任务。下面介绍的是通过微信监管tensorflow框架下mnist训练的一个有趣的小例子。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-0d0239b8bb5c05bdf280b232ac1c825e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb\" width=\"450\" data-original=\"https://pic3.zhimg.com/v2-0d0239b8bb5c05bdf280b232ac1c825e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;450&#39; height=&#39;299&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"450\" data-original=\"https://pic3.zhimg.com/v2-0d0239b8bb5c05bdf280b232ac1c825e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-0d0239b8bb5c05bdf280b232ac1c825e_b.jpg\"/></figure><p>首先我们需要安装python的itchat包，一个开源的微信个人号接口（python2下亲测可用）：</p><div class=\"highlight\"><pre><code class=\"language-text\">pip install itchat</code></pre></div><p>在导入一些必要的库后，我们首先创建一个线程锁（[注：<a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/zhou8201/article/details/72758953\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">线程的锁函数：threading.Lock() 返回锁对象 Lock</a>]）和初始化一个训练状态标志：</p><div class=\"highlight\"><pre><code class=\"language-text\">lock = threading.Lock()\nrunning = False</code></pre></div><p>以及设置训练网络的超参数（包括学习率、最大迭代次数、一个batch的样本数和显示训练信息的频率）：</p><div class=\"highlight\"><pre><code class=\"language-text\">learning_rate = 0.001\ntraining_iters = 200000\nbatch_size = 128\ndisplay_step = 10</code></pre></div><p>接下来定义一个训练网络的函数（其中wechat_name为要监管训练的微信用户名，param为设置的超参数）：</p><div class=\"highlight\"><pre><code class=\"language-text\">def nn_train(wechat_name, param):</code></pre></div><p>由于该函数比较简单，只是增加了训练状态监测和用itchat.send模块来实现训练信息发送的部分，在这里就不多讲了。该网络由2个卷积层、2个池化层、1个全连接层和1个分类层构成，具体实现可参考：</p><p><a href=\"https://link.zhihu.com/?target=http%3A//www.cnblogs.com/denny402/p/5853538.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">tensorflow学习笔记五：mnist实例--卷积神经网络(CNN） - denny402 - 博客园</a></p><p>接下来，我们编写itchat的处理函数：</p><div class=\"highlight\"><pre><code class=\"language-text\">@itchat.msg_register([itchat.content.TEXT])\ndef chat_trigger(msg):</code></pre></div><p>添加开始功能（运行代码登录网页版微信后，发送“开始”后，即可开始训练网络）：</p><div class=\"highlight\"><pre><code class=\"language-text\">if msg[&#39;Text&#39;] == u&#39;开始&#39;:\n    print(&#39;Starting&#39;)\n    with lock:\n        run_state = running\n    if not run_state:\n        try:\n            threading.Thread(target=nn_train, args=(&#39;filehelper&#39;, (learning_rate, training_iters, batch_size, display_step))).start()\n        except:\n            msg.reply(&#39;Running&#39;)</code></pre></div><p>添加停止功能（运行代码登录网页版微信后，发送“停止”后，即可终止网络训练）：</p><div class=\"highlight\"><pre><code class=\"language-text\">elif msg[&#39;Text&#39;] == u&#39;停止&#39;:\n    print(&#39;Stopping&#39;)\n    with lock:\n        running = False</code></pre></div><p>添加超参数查看功能（运行代码登录网页版微信后，发送“参数”后，即可显示当前超参数设置）：</p><div class=\"highlight\"><pre><code class=\"language-text\">elif msg[&#39;Text&#39;] == u&#39;参数&#39;:\n    itchat.send(&#39;lr=%f, ti=%d, bs=%d, ds=%d&#39;%(learning_rate, training_iters, batch_size, display_step), &#39;filehelper&#39;)</code></pre></div><p>添加退出登录功能（运行代码登录网页版微信后，发送“退出”后，即可退出登录）：</p><div class=\"highlight\"><pre><code class=\"language-text\">elif msg[&#39;Text&#39;] == u&#39;退出&#39;:\n    itchat.logout()</code></pre></div><p>添加超参数设置功能（运行代码登录网页版微信后，按“超参数名[注：lr、ti、bs、ds可选] 超参数值”的格式发送信息后，即可设置对应的超参数）：</p><div class=\"highlight\"><pre><code class=\"language-text\">else:\n    try:\n        param = msg[&#39;Text&#39;].split()\n        key, value = param\n        print(key, value)\n        if key == &#39;lr&#39;:\n            learning_rate = float(value)\n        elif key == &#39;ti&#39;:\n            training_iters = int(value)\n        elif key == &#39;bs&#39;:\n            batch_size = int(value)\n        elif key == &#39;ds&#39;:\n            display_step = int(value)\n    except:\n        pass</code></pre></div><p>最后，主函数写上通过扫描终端显示的二维码自动登录网页版微信的功能以及运行程序（hotReload=True表示在一定时间内即使退出了python程序再打开就不需要再扫码了；enableCmdQR=2表示在终端显示二维码，实验是在Ubuntu16.04下进行的，终端背景色为暗色，块字符的宽度为一个字符[正常应为两字符]，故赋值为2，若终端背景色为浅色，可赋值为-2）：</p><div class=\"highlight\"><pre><code class=\"language-text\">if __name__ == &#39;__main__&#39;:\n    itchat.auto_login(hotReload=True, enableCmdQR=2)\n    itchat.run()</code></pre></div><p>以下是完整代码tf_wechat.py（执行python tf_wechat.py，会在终端显示二维码，扫码成功登录网页版微信后，可实现itchat的处理函数中所描述的功能，运行结束后当前目录下的data文件夹中保存了下载下来的mnist数据集压缩包）：</p><div class=\"highlight\"><pre><code class=\"language-text\"># -*- coding: utf-8 -*-\n\n&#39;&#39;&#39;\nA Convolutional Network implementation example using TensorFlow library.\nThis example is using the MNIST database of handwritten digits\n(http://yann.lecun.com/exdb/mnist/)\nAuthor: Aymeric Damien\nProject: https://github.com/aymericdamien/TensorFlow-Examples/\n\n\nAdd a itchat controller with multi thread\n&#39;&#39;&#39;\n\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\n# Import MNIST data\nfrom tensorflow.examples.tutorials.mnist import input_data\n\n# Import itchat &amp; threading\nimport itchat\nimport threading\n\n# Create a running status flag\nlock = threading.Lock()\nrunning = False\n\n# Parameters\nlearning_rate = 0.001\ntraining_iters = 200000\nbatch_size = 128\ndisplay_step = 10\n\ndef nn_train(wechat_name, param):\n    global lock, running\n    # Lock\n    with lock:\n        running = True\n\n    # mnist data reading\n    mnist = input_data.read_data_sets(&#34;data/&#34;, one_hot=True)\n\n    # Parameters\n    # learning_rate = 0.001\n    # training_iters = 200000\n    # batch_size = 128\n    # display_step = 10\n    learning_rate, training_iters, batch_size, display_step = param\n\n    # Network Parameters\n    n_input = 784 # MNIST data input (img shape: 28*28)\n    n_classes = 10 # MNIST total classes (0-9 digits)\n    dropout = 0.75 # Dropout, probability to keep units\n\n    # tf Graph input\n    x = tf.placeholder(tf.float32, [None, n_input])\n    y = tf.placeholder(tf.float32, [None, n_classes])\n    keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n\n\n    # Create some wrappers for simplicity\n    def conv2d(x, W, b, strides=1):\n        # Conv2D wrapper, with bias and relu activation\n        x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding=&#39;SAME&#39;)\n        x = tf.nn.bias_add(x, b)\n        return tf.nn.relu(x)\n\n\n    def maxpool2d(x, k=2):\n        # MaxPool2D wrapper\n        return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n                            padding=&#39;SAME&#39;)\n\n\n    # Create model\n    def conv_net(x, weights, biases, dropout):\n        # Reshape input picture\n        x = tf.reshape(x, shape=[-1, 28, 28, 1])\n\n        # Convolution Layer\n        conv1 = conv2d(x, weights[&#39;wc1&#39;], biases[&#39;bc1&#39;])\n        # Max Pooling (down-sampling)\n        conv1 = maxpool2d(conv1, k=2)\n\n        # Convolution Layer\n        conv2 = conv2d(conv1, weights[&#39;wc2&#39;], biases[&#39;bc2&#39;])\n        # Max Pooling (down-sampling)\n        conv2 = maxpool2d(conv2, k=2)\n\n        # Fully connected layer\n        # Reshape conv2 output to fit fully connected layer input\n        fc1 = tf.reshape(conv2, [-1, weights[&#39;wd1&#39;].get_shape().as_list()[0]])\n        fc1 = tf.add(tf.matmul(fc1, weights[&#39;wd1&#39;]), biases[&#39;bd1&#39;])\n        fc1 = tf.nn.relu(fc1)\n        # Apply Dropout\n        fc1 = tf.nn.dropout(fc1, dropout)\n\n        # Output, class prediction\n        out = tf.add(tf.matmul(fc1, weights[&#39;out&#39;]), biases[&#39;out&#39;])\n        return out\n\n    # Store layers weight &amp; bias\n    weights = {\n        # 5x5 conv, 1 input, 32 outputs\n        &#39;wc1&#39;: tf.Variable(tf.random_normal([5, 5, 1, 32])),\n        # 5x5 conv, 32 inputs, 64 outputs\n        &#39;wc2&#39;: tf.Variable(tf.random_normal([5, 5, 32, 64])),\n        # fully connected, 7*7*64 inputs, 1024 outputs\n        &#39;wd1&#39;: tf.Variable(tf.random_normal([7*7*64, 1024])),\n        # 1024 inputs, 10 outputs (class prediction)\n        &#39;out&#39;: tf.Variable(tf.random_normal([1024, n_classes]))\n    }\n\n    biases = {\n        &#39;bc1&#39;: tf.Variable(tf.random_normal([32])),\n        &#39;bc2&#39;: tf.Variable(tf.random_normal([64])),\n        &#39;bd1&#39;: tf.Variable(tf.random_normal([1024])),\n        &#39;out&#39;: tf.Variable(tf.random_normal([n_classes]))\n    }\n\n    # Construct model\n    pred = conv_net(x, weights, biases, keep_prob)\n\n    # Define loss and optimizer\n    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n\n    # Evaluate model\n    correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\n\n    # Initializing the variables\n    init = tf.global_variables_initializer()\n\n    # Launch the graph\n    with tf.Session() as sess:\n        sess.run(init)\n        step = 1\n        # Keep training until reach max iterations\n        print(&#39;Wait for lock&#39;)\n        with lock:\n            run_state = running\n        print(&#39;Start&#39;)\n        while step * batch_size &lt; training_iters and run_state:\n            batch_x, batch_y = mnist.train.next_batch(batch_size)\n            # Run optimization op (backprop)\n            sess.run(optimizer, feed_dict={x: batch_x, y: batch_y,\n                                        keep_prob: dropout})\n            if step % display_step == 0:\n                # Calculate batch loss and accuracy\n                loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,\n                                                                y: batch_y,\n                                                                keep_prob: 1.})\n                print(&#34;Iter &#34; + str(step*batch_size) + &#34;, Minibatch Loss= &#34; + \\\n                    &#34;{:.6f}&#34;.format(loss) + &#34;, Training Accuracy= &#34; + \\\n                    &#34;{:.5f}&#34;.format(acc))\n                itchat.send(&#34;Iter &#34; + str(step*batch_size) + &#34;, Minibatch Loss= &#34; + \\\n                    &#34;{:.6f}&#34;.format(loss) + &#34;, Training Accuracy= &#34; + \\\n                            &#34;{:.5f}&#34;.format(acc), wechat_name)\n            step += 1\n            with lock:\n                run_state = running\n        print(&#34;Optimization Finished!&#34;)\n        itchat.send(&#34;Optimization Finished!&#34;, wechat_name)\n\n        # Calculate accuracy for 256 mnist test images\n        print(&#34;Testing Accuracy:&#34;, \\\n            sess.run(accuracy, feed_dict={x: mnist.test.images[:256],\n                                        y: mnist.test.labels[:256],\n                                        keep_prob: 1.}))\n        itchat.send(&#34;Testing Accuracy: %s&#34; %\n            sess.run(accuracy, feed_dict={x: mnist.test.images[:256],\n                                        y: mnist.test.labels[:256],\n                                          keep_prob: 1.}), wechat_name)\n\n    with lock:\n        running = False\n\n@itchat.msg_register([itchat.content.TEXT])\ndef chat_trigger(msg):\n    global lock, running, learning_rate, training_iters, batch_size, display_step\n    if msg[&#39;Text&#39;] == u&#39;开始&#39;:\n        print(&#39;Starting&#39;)\n        with lock:\n            run_state = running\n        if not run_state:\n            try:\n                threading.Thread(target=nn_train, args=(&#39;filehelper&#39;, (learning_rate, training_iters, batch_size, display_step))).start()\n            except:\n                msg.reply(&#39;Running&#39;)\n    elif msg[&#39;Text&#39;] == u&#39;停止&#39;:\n        print(&#39;Stopping&#39;)\n        with lock:\n            running = False\n    elif msg[&#39;Text&#39;] == u&#39;参数&#39;:\n        itchat.send(&#39;lr=%f, ti=%d, bs=%d, ds=%d&#39;%(learning_rate, training_iters, batch_size, display_step), &#39;filehelper&#39;)\n    elif msg[&#39;Text&#39;] == u&#39;退出&#39;:\n        itchat.logout()\n    else:\n        try:\n            param = msg[&#39;Text&#39;].split()\n            key, value = param\n            print(key, value)\n            if key == &#39;lr&#39;:\n                learning_rate = float(value)\n            elif key == &#39;ti&#39;:\n                training_iters = int(value)\n            elif key == &#39;bs&#39;:\n                batch_size = int(value)\n            elif key == &#39;ds&#39;:\n                display_step = int(value)\n        except:\n            pass\n\n\nif __name__ == &#39;__main__&#39;:\n    itchat.auto_login(hotReload=True, enableCmdQR=2)\n    itchat.run()</code></pre></div><p>最后当然是附上最终效果图啦，酷！</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-b8d41e78624cd263f4002dc4db62e2c3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"1920\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-b8d41e78624cd263f4002dc4db62e2c3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;1920&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"1920\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-b8d41e78624cd263f4002dc4db62e2c3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-b8d41e78624cd263f4002dc4db62e2c3_b.jpg\"/></figure><p>更多关于itchat的使用可参考：</p><p><a href=\"https://link.zhihu.com/?target=http%3A//itchat.readthedocs.io/zh/latest/tutorial/tutorial0/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">基础新手入门 - itchat</a></p><p>欢迎关注公众号：huangxiaobai880</p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/35681051", 
            "userName": "小白", 
            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
            "upvote": 1, 
            "title": "tensorflow趣味玩法一：池塘涟漪模拟", 
            "content": "<p>除了实现机器学习算法求解模型来解决实际工程中的问题，tensorflow还有一些趣味玩法，比如在模拟仿真动态过程方面也丝毫不逊色。在这里，我们将使用tensorflow框架，建立偏微分方程模型来模拟仿真雨点落入一块方形水池后引起涟漪的动态过程（[注：关于偏微分方程的相关知识，可参考<a href=\"https://link.zhihu.com/?target=http%3A//open.163.com/special/opencourse/multivariable.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">麻省理工学院公开课：多变量微积分_全35集_网易公开课</a>]）。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-0d0239b8bb5c05bdf280b232ac1c825e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb\" width=\"450\" data-original=\"https://pic3.zhimg.com/v2-0d0239b8bb5c05bdf280b232ac1c825e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;450&#39; height=&#39;299&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"450\" data-original=\"https://pic3.zhimg.com/v2-0d0239b8bb5c05bdf280b232ac1c825e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-0d0239b8bb5c05bdf280b232ac1c825e_b.jpg\"/></figure><p>在导入一些必要的库后，我们首先定义一个显示当前池塘状态对应图像的函数：</p><div class=\"highlight\"><pre><code class=\"language-text\">def DisplayArray(a, fmt=&#39;jpeg&#39;, rng=[0,1]):\n  &#34;&#34;&#34;Display an array as a picture.&#34;&#34;&#34;\n  a = (a - rng[0])/float(rng[1] - rng[0])*255\n  a = np.uint8(np.clip(a, 0, 255))\n  im = PIL.Image.fromarray(a)\n  im.show()</code></pre></div><p>该函数将矩阵a中所有元素值拉伸至0-255整型范围内，并将其作为图片显示出来（[注：<a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/qq1483661204/article/details/78150203\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Numpy 中clip函数的使用</a>]）。</p><p>接着分别定义一个构造卷积核和一个实现卷积操作的函数，最终实现图像的拉普拉斯变换（拉普拉斯滤波核和图像的卷积）：</p><div class=\"highlight\"><pre><code class=\"language-text\">def make_kernel(a):\n  &#34;&#34;&#34;Transform a 2D array into a convolution kernel&#34;&#34;&#34;\n  a = np.asarray(a)\n  a = a.reshape(list(a.shape) + [1,1])\n  return tf.constant(a, dtype=1)\n\ndef simple_conv(x, k):\n  &#34;&#34;&#34;A simplified 2D convolution operation&#34;&#34;&#34;\n  x = tf.expand_dims(tf.expand_dims(x, 0), -1)\n  y = tf.nn.depthwise_conv2d(x, k, [1, 1, 1, 1], padding=&#39;SAME&#39;)\n  return y[0, :, :, 0]\n\ndef laplace(x):\n  &#34;&#34;&#34;Compute the 2D laplacian of an array&#34;&#34;&#34;\n  laplace_k = make_kernel([[0.5, 1.0, 0.5],\n                           [1.0, -6., 1.0],\n                           [0.5, 1.0, 0.5]])\n  return simple_conv(x, laplace_k)</code></pre></div><p>定义正方形池塘的边长：</p><div class=\"highlight\"><pre><code class=\"language-text\">N = 500</code></pre></div><p>初始化正方形池塘前后状态对应的图像：</p><div class=\"highlight\"><pre><code class=\"language-text\">u_init = np.zeros([N, N], dtype=&#34;float32&#34;)\nut_init = np.zeros([N, N], dtype=&#34;float32&#34;)</code></pre></div><p>随机产生40个雨点坐标和强弱值并加入到后一个状态的图像中显示：</p><div class=\"highlight\"><pre><code class=\"language-text\">for n in range(40):\n  a,b = np.random.randint(0, N, 2)\n  u_init[a,b] = np.random.uniform()\n\nDisplayArray(u_init, rng=[-0.1, 0.1])</code></pre></div><p>仿真参数设置（时间分辨率和水波阻尼系数）：</p><div class=\"highlight\"><pre><code class=\"language-text\">eps = tf.placeholder(tf.float32, shape=())\ndamping = tf.placeholder(tf.float32, shape=())</code></pre></div><p>正方形池塘前后状态变量设置：</p><div class=\"highlight\"><pre><code class=\"language-text\">U  = tf.Variable(u_init)\nUt = tf.Variable(ut_init)</code></pre></div><p>建立离散偏微分方程：</p><div class=\"highlight\"><pre><code class=\"language-text\">U_ = U + eps * Ut\nUt_ = Ut + eps * (laplace(U) - damping * Ut)</code></pre></div><p>正方形池塘状态变量更新规则：</p><div class=\"highlight\"><pre><code class=\"language-text\">step = tf.group(\n  U.assign(U_),\n  Ut.assign(Ut_))</code></pre></div><p>使用GPU并创建交互式会话：</p><div class=\"highlight\"><pre><code class=\"language-text\">config = tf.ConfigProto(device_count = {&#39;GPU&#39;: 1})\nsess = tf.InteractiveSession(config=config)</code></pre></div><p>初始化所有变量：</p><div class=\"highlight\"><pre><code class=\"language-text\">init = tf.global_variables_initializer()\nsess.run(init)</code></pre></div><p>开始仿真（迭代1000次，每50次保存一次状态图像，共保存了20张图）：</p><div class=\"highlight\"><pre><code class=\"language-text\">j = 97 # The name of saved pictures started with &#34;rain_a(0).jpeg&#34;\n# Run 1000 steps of PDE\nfor i in range(1000):\n  # Step simulation\n  step.run({eps: 0.03, damping: 0.04})\n  # Visualize every 50 steps\n  if i % 50 == 0:\n    DisplayArray(U.eval(), rng=[-0.1, 0.1])\n    a = (U.eval() + 0.1)/0.2*255\n    a = np.uint8(np.clip(a, 0, 255))\n    imsave(&#39;rain_&#39; + chr(j) + &#39;(%d).jpeg&#39; %i, a)\n    j = j + 1</code></pre></div><p>执行shell命令创建results文件夹，将所有保存的图片移动到该文件夹下，并在results文件夹下制作gif动画（[注：<a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/robertsong2004/article/details/50664748\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">[小技巧] Linux 下转换图片到 gif</a>]）：</p><div class=\"highlight\"><pre><code class=\"language-text\">os.system(&#39;mkdir results; mv *.jpeg results; cd results; convert -delay 20 -loop 0 *.jpeg rain.gif&#39;)</code></pre></div><p>以下是完整代码rain.py（执行python rain.py，可在当前目录下得到results文件夹，里面保存了rain.gif和rain_a(0).jpeg~rain_t(950).jpeg）：</p><div class=\"highlight\"><pre><code class=\"language-text\"># -*- coding: utf-8 -*-\n\nimport os\nimport numpy as np\nfrom scipy.misc import imsave\nimport PIL.Image\nimport tensorflow as tf\n\ndef DisplayArray(a, fmt=&#39;jpeg&#39;, rng=[0,1]):\n  &#34;&#34;&#34;Display an array as a picture.&#34;&#34;&#34;\n  a = (a - rng[0])/float(rng[1] - rng[0])*255\n  a = np.uint8(np.clip(a, 0, 255))\n  im = PIL.Image.fromarray(a)\n  im.show()\n\ndef make_kernel(a):\n  &#34;&#34;&#34;Transform a 2D array into a convolution kernel&#34;&#34;&#34;\n  a = np.asarray(a)\n  a = a.reshape(list(a.shape) + [1,1])\n  return tf.constant(a, dtype=1)\n\ndef simple_conv(x, k):\n  &#34;&#34;&#34;A simplified 2D convolution operation&#34;&#34;&#34;\n  x = tf.expand_dims(tf.expand_dims(x, 0), -1)\n  y = tf.nn.depthwise_conv2d(x, k, [1, 1, 1, 1], padding=&#39;SAME&#39;)\n  return y[0, :, :, 0]\n\ndef laplace(x):\n  &#34;&#34;&#34;Compute the 2D laplacian of an array&#34;&#34;&#34;\n  laplace_k = make_kernel([[0.5, 1.0, 0.5],\n                           [1.0, -6., 1.0],\n                           [0.5, 1.0, 0.5]])\n  return simple_conv(x, laplace_k)\n\nN = 500\n\n# Initial Conditions -- some rain drops hit a pond\n\n# Set everything to zero\nu_init = np.zeros([N, N], dtype=&#34;float32&#34;)\nut_init = np.zeros([N, N], dtype=&#34;float32&#34;)\n\n# Some rain drops hit a pond at random points\nfor n in range(40):\n  a,b = np.random.randint(0, N, 2)\n  u_init[a,b] = np.random.uniform()\n\nDisplayArray(u_init, rng=[-0.1, 0.1])\n\n# Parameters:\n# eps -- time resolution\n# damping -- wave damping\neps = tf.placeholder(tf.float32, shape=())\ndamping = tf.placeholder(tf.float32, shape=())\n\n# Create variables for simulation state\nU  = tf.Variable(u_init)\nUt = tf.Variable(ut_init)\n\n# Discretized PDE update rules\nU_ = U + eps * Ut\nUt_ = Ut + eps * (laplace(U) - damping * Ut)\n\n# Operation to update the state\nstep = tf.group(\n  U.assign(U_),\n  Ut.assign(Ut_))\n\nconfig = tf.ConfigProto(device_count = {&#39;GPU&#39;: 1})\nsess = tf.InteractiveSession(config=config)\n\n# Initialize state to initial conditions\ninit = tf.global_variables_initializer()\nsess.run(init)\nj = 97 # The name of saved pictures started with &#34;rain_a(0).jpeg&#34;\n# Run 1000 steps of PDE\nfor i in range(1000):\n  # Step simulation\n  step.run({eps: 0.03, damping: 0.04})\n  # Visualize every 50 steps\n  if i % 50 == 0:\n    DisplayArray(U.eval(), rng=[-0.1, 0.1])\n    a = (U.eval() + 0.1)/0.2*255\n    a = np.uint8(np.clip(a, 0, 255))\n    imsave(&#39;rain_&#39; + chr(j) + &#39;(%d).jpeg&#39; %i, a)\n    j = j + 1\n\n# Create the directory named &#34;results&#34; and move all JPEG files to the directory\n# Make the GIF file and save it in the directory\nos.system(&#39;mkdir results; mv *.jpeg results; cd results; convert -delay 20 -loop 0 *.jpeg rain.gif&#39;)</code></pre></div><p>最后当然是附上最终效果图啦，酷！</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-045e6d132605cdd173da3105d654cb80_b.gif\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"500\" data-rawheight=\"500\" data-thumbnail=\"https://pic1.zhimg.com/v2-045e6d132605cdd173da3105d654cb80_b.jpg\" class=\"origin_image zh-lightbox-thumb\" width=\"500\" data-original=\"https://pic1.zhimg.com/v2-045e6d132605cdd173da3105d654cb80_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;500&#39; height=&#39;500&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"500\" data-rawheight=\"500\" data-thumbnail=\"https://pic1.zhimg.com/v2-045e6d132605cdd173da3105d654cb80_b.jpg\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"500\" data-original=\"https://pic1.zhimg.com/v2-045e6d132605cdd173da3105d654cb80_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-045e6d132605cdd173da3105d654cb80_b.gif\"/></figure><p>欢迎关注公众号：huangxiaobai880</p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/34454094", 
            "userName": "小白", 
            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
            "upvote": 0, 
            "title": "python基础-数据类型", 
            "content": "<p></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-3d900de7a83ac1bfa2dd32d4ce51b3b7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb\" width=\"450\" data-original=\"https://pic4.zhimg.com/v2-3d900de7a83ac1bfa2dd32d4ce51b3b7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;450&#39; height=&#39;299&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"450\" data-original=\"https://pic4.zhimg.com/v2-3d900de7a83ac1bfa2dd32d4ce51b3b7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-3d900de7a83ac1bfa2dd32d4ce51b3b7_b.jpg\"/></figure><blockquote>Python 中的变量不需要声明。每个变量在使用前都必须赋值，变量赋值以后该变量才会被创建。<br/>在 Python 中，变量就是变量，它没有类型，我们所说的&#34;类型&#34;是变量所指的内存中对象的类型。</blockquote><hr/><p><b>1，python内支持的数据类型：</b></p><ul><li>Number（数字）</li><li>String（字符串）</li><li>List（列表）</li><li>Tuple（元组）</li><li>Sets（集合）</li><li>Dictionary（字典）</li></ul><blockquote><b>注：int、float、bool、complex（复数）是Number（数字）里的一个小类，它与基础的数据类型不是并列的</b></blockquote><hr/><p><b>2，数据类型的转换</b></p><p>你只需要将数据类型作为函数名即可</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-2e610aee83ce597d292cb59247cc8c92_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"298\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic3.zhimg.com/v2-2e610aee83ce597d292cb59247cc8c92_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;640&#39; height=&#39;298&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"298\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https://pic3.zhimg.com/v2-2e610aee83ce597d292cb59247cc8c92_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-2e610aee83ce597d292cb59247cc8c92_b.jpg\"/></figure><hr/><p><b>3，Number（数字）</b></p><p>内置的 type() 函数可以用来查询变量所指的对象类型。</p><p>此外还可以用 isinstance 来判断。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-503d698df1a038e1b33c0a30edbedc69_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"555\" data-rawheight=\"127\" class=\"origin_image zh-lightbox-thumb\" width=\"555\" data-original=\"https://pic2.zhimg.com/v2-503d698df1a038e1b33c0a30edbedc69_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;555&#39; height=&#39;127&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"555\" data-rawheight=\"127\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"555\" data-original=\"https://pic2.zhimg.com/v2-503d698df1a038e1b33c0a30edbedc69_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-503d698df1a038e1b33c0a30edbedc69_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-d292e6652951aefae308ffb8d35386b4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"305\" data-rawheight=\"166\" class=\"content_image\" width=\"305\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;305&#39; height=&#39;166&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"305\" data-rawheight=\"166\" class=\"content_image lazy\" width=\"305\" data-actualsrc=\"https://pic1.zhimg.com/v2-d292e6652951aefae308ffb8d35386b4_b.jpg\"/></figure><p>在混合计算时，Python会把整型转换成为浮点数。</p><hr/><p><b>4，String（字符串）</b></p><p>Python中的字符串用单引号(&#39;)或双引号(&#34;)括起来，同时使用反斜杠(\\)转义特殊字符。</p><p>字符串的截取的语法格式如下：</p><div class=\"highlight\"><pre><code class=\"language-text\">变量[头下标:尾下标]</code></pre></div><p>索引值以 0 为开始值，-1 为从末尾的开始位置。</p><p>加号 (+) 是字符串的连接符， 星号 (*) 表示复制当前字符串，紧跟的数字为复制的次数。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ee7e8cbb24650bce8eff7bdc5d0ac99f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"457\" data-rawheight=\"506\" class=\"origin_image zh-lightbox-thumb\" width=\"457\" data-original=\"https://pic4.zhimg.com/v2-ee7e8cbb24650bce8eff7bdc5d0ac99f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;457&#39; height=&#39;506&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"457\" data-rawheight=\"506\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"457\" data-original=\"https://pic4.zhimg.com/v2-ee7e8cbb24650bce8eff7bdc5d0ac99f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-ee7e8cbb24650bce8eff7bdc5d0ac99f_b.jpg\"/></figure><p>Python 使用反斜杠(\\)转义特殊字符<b>，如果你不想让反斜杠发生转义，可以在字符串前面添加一个 r</b>，表示原始字符串：</p><div class=\"highlight\"><pre><code class=\"language-text\">print(&#39;Ru\\noob&#39;)\nRu\noob\nprint(r&#39;Ru\\noob&#39;)\nRu\\noob</code></pre></div><p><b>Python 字符串不能被改变。向一个索引位置赋值，比如word[0] = &#39;m&#39;会导致错误。</b></p><div class=\"highlight\"><pre><code class=\"language-text\">a = &#39;aaaaa&#39;\\\n    &#39;cccccc&#39;\nprint(a)\n输出：\naaaaacccccc</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p><b>5，列表</b></p><p>List（列表） 是 Python 中使用最频繁的数据类型。</p><p><b>列表是写在方括号[]之间、用逗号分隔开的元素列表。</b></p><p>和字符串一样，列表同样可以被索引和截取，列表被截取后返回一个包含所需元素的新列表。</p><p>列表截取的语法格式如下：</p><div class=\"highlight\"><pre><code class=\"language-text\">变量[头下标:尾下标]</code></pre></div><p><b>索引值以 0 为开始值，-1 为从末尾的开始位置。</b></p><p><b>加号（+）是列表连接运算符，星号（*）是重复操作。</b></p><p>List内置了有很多方法，例如append()、pop()等等</p><hr/><p><b>6，元组</b></p><p>元组（tuple）与列表类似，不同之处在于元组的元素不能修改。元组写在小括号(<b>()</b>)里，元素之间用逗号隔开。</p><hr/><p><b>7，set</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-6e3ca0f0755b5b263f14806b2d260b71_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"791\" data-rawheight=\"112\" class=\"origin_image zh-lightbox-thumb\" width=\"791\" data-original=\"https://pic2.zhimg.com/v2-6e3ca0f0755b5b263f14806b2d260b71_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;791&#39; height=&#39;112&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"791\" data-rawheight=\"112\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"791\" data-original=\"https://pic2.zhimg.com/v2-6e3ca0f0755b5b263f14806b2d260b71_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-6e3ca0f0755b5b263f14806b2d260b71_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-536efc91c0f1e3a9a577d9db70533312_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"460\" data-rawheight=\"66\" class=\"origin_image zh-lightbox-thumb\" width=\"460\" data-original=\"https://pic3.zhimg.com/v2-536efc91c0f1e3a9a577d9db70533312_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;460&#39; height=&#39;66&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"460\" data-rawheight=\"66\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"460\" data-original=\"https://pic3.zhimg.com/v2-536efc91c0f1e3a9a577d9db70533312_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-536efc91c0f1e3a9a577d9db70533312_b.jpg\"/></figure><hr/><p><b>8,Dictionary（字典）</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-e72b557370881f1ec8b02606f17f5598_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"813\" data-rawheight=\"169\" class=\"origin_image zh-lightbox-thumb\" width=\"813\" data-original=\"https://pic1.zhimg.com/v2-e72b557370881f1ec8b02606f17f5598_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;813&#39; height=&#39;169&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"813\" data-rawheight=\"169\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"813\" data-original=\"https://pic1.zhimg.com/v2-e72b557370881f1ec8b02606f17f5598_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-e72b557370881f1ec8b02606f17f5598_b.jpg\"/></figure><p>参考博客：</p><p><a href=\"https://link.zhihu.com/?target=http%3A//www.runoob.com/python3/python3-data-type.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Python3 基本数据类型 | 菜鸟教程</a></p><p>欢迎关注公众号：huangxiaobai880</p><a class=\"video-box\" href=\"https://link.zhihu.com/?target=https%3A//www.zhihu.com/video/956500905420414976\" target=\"_blank\" data-video-id=\"\" data-video-playable=\"true\" data-name=\"\" data-poster=\"https://pic1.zhimg.com/80/v2-f29e629b0a4b7b05d4e2b4182d62c2fc_b.jpg\" data-lens-id=\"956500905420414976\"><img class=\"thumbnail\" src=\"https://pic1.zhimg.com/80/v2-f29e629b0a4b7b05d4e2b4182d62c2fc_b.jpg\"/><span class=\"content\"><span class=\"title\"><span class=\"z-ico-extern-gray\"></span><span class=\"z-ico-extern-blue\"></span></span><span class=\"url\"><span class=\"z-ico-video\"></span>https://www.zhihu.com/video/956500905420414976</span></span></a><p></p>", 
            "topic": [
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/34666052", 
            "userName": "小白", 
            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
            "upvote": 2, 
            "title": "python实战-人脸检测", 
            "content": "<p></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-0d0239b8bb5c05bdf280b232ac1c825e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb\" width=\"450\" data-original=\"https://pic3.zhimg.com/v2-0d0239b8bb5c05bdf280b232ac1c825e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;450&#39; height=&#39;299&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"450\" data-original=\"https://pic3.zhimg.com/v2-0d0239b8bb5c05bdf280b232ac1c825e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-0d0239b8bb5c05bdf280b232ac1c825e_b.jpg\"/></figure><p><b>项目说明</b></p><p class=\"ztext-empty-paragraph\"><br/></p><p>该项目的任务是利用python的cv2库，实现在一张图片里检测出所有的人脸。工程文件可以去我网盘下载：</p><p>链接：<a href=\"https://link.zhihu.com/?target=https%3A//pan.baidu.com/s/1hZHxpKMjroY3M_Ddv_9u6g\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">pan.baidu.com/s/1hZHxpK</span><span class=\"invisible\">MjroY3M_Ddv_9u6g</span><span class=\"ellipsis\"></span></a> 密码：24gh</p><hr/><p><b>开发环境准备</b></p><p class=\"ztext-empty-paragraph\"><br/></p><p>1. Anaconda2的安装（请参考<a href=\"https://zhuanlan.zhihu.com/p/33541302\" class=\"internal\">手把手教你在Ubuntu 16.04下配置GPU版Caffe（2）</a>中的python环境配置部分）；</p><p>2.python的cv2库安装：</p><div class=\"highlight\"><pre><code class=\"language-text\">pip install opencv-python</code></pre></div><hr/><p><b>源代码讲解</b></p><p class=\"ztext-empty-paragraph\"><br/></p><p>1.faceDetector.py文件解析</p><div class=\"highlight\"><pre><code class=\"language-text\">import cv2\n\nclass FaceDetector:\n  def __init__(self, faceCascadePath):\n    self.faceCascade = cv2.CascadeClassifier(faceCascadePath)\n  def detect(self, image, scaleFactor = 1.1, minNeighbors = 5, minSize = (30, 30)):\n    rects = self.faceCascade.detectMultiScale(image, scaleFactor = scaleFactor, minNeighbors = minNeighbors, minSize = minSize, flags = cv2.CASCADE_SCALE_IMAGE)\n    return rects</code></pre></div><p>该文件定义了一个人脸检测的类FaceDetector，其初始化函数调用了已有的Haar级联分类器（xml文件，已经预先训练过，可以识别人脸）。该类中还定义了一个成员函数detect，调用了分类器对象的成员函数detectMultiScale，其输入参数的含义如下：</p><p>1.image：需要检测人脸的图像</p><p>2.scaleFactor：该参数表示在每个图像扫描尺度上，图像的大小减少了多少。该值用于创建规模金字塔，以便在图像的多个尺度上检测人脸(一些面孔可能更靠前，因此更大。其他的脸在背景中可能显得更小，因此使用不同的尺度扫描）。</p><p>3.minNeighbors：该参数表示每个窗口应该有多少个相邻部分一起来认定是否是一张脸。级联分类器将检测到一张脸周围的多个窗口。这个参数控制需要检测多少个矩形，以便将窗口扫描部分标记为一张脸。</p><p>4. minSize：该参数表示一组宽度和高度(以像素为单位)，表示窗口的最小大小，小于这个大小的边框被忽略。</p><p>5.flags：该参数在新版的cv2中是不需要的，暂时不管了，去掉同样可以，具体可参考：<a href=\"https://link.zhihu.com/?target=https%3A//docs.opencv.org/2.4/modules/objdetect/doc/cascade_classification.html%3Fhighlight%3Ddetectmultiscale%23cascadeclassifier-detectmultiscale\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Cascade Classification</a></p><p>函数输出：</p><p>返回一组包含图像中人脸边框的元组列表。这些列表中的元组仅仅是边框左上角位置的坐标(x, y)，以及边框的宽度w和高度h。</p><p>通过该文件定义的人脸检测类FaceDetector，我们就可以完成检测人脸的任务。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>2.faceVisual.py文件解析</p><p>首先看命令行传递参数设置部分：</p><div class=\"highlight\"><pre><code class=\"language-text\">ap = argparse.ArgumentParser()\nap.add_argument(&#34;-f&#34;, &#34;--face&#34;, required = True, help = &#34;Path to where the face cascade resides&#34;)\nap.add_argument(&#34;-i&#34;, &#34;--image&#34;, required = True, help = &#34;Path to where the image file resides&#34;)\nap.add_argument(&#34;-r&#34;, &#34;--result&#34;, default = &#34;result.jpg&#34;, help = &#34;Path to where the result resides&#34;)\nargs = vars(ap.parse_args())</code></pre></div><p>命令行传递的参数如下：</p><p>1.-f：Haar级联分类器的路径</p><p>2.-i：需要检测人脸的图片路径</p><p>3.-r：保存检测结果的图片路径，默认为当前目录下的result.jpg</p><p>vars函数的用法可参考：<a href=\"https://link.zhihu.com/?target=https%3A//www.cnblogs.com/sesshoumaru/p/6130031.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Python内置函数(66)--vars - 十月狐狸 - 博客园</a></p><div class=\"highlight\"><pre><code class=\"language-text\">image = cv2.imread(args[&#34;image&#34;])</code></pre></div><p>读入需要检测人脸的图片。</p><div class=\"highlight\"><pre><code class=\"language-text\">gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)</code></pre></div><p>将读入的图片转为灰度图。</p><div class=\"highlight\"><pre><code class=\"language-text\">fd = FaceDetector(args[&#34;face&#34;])</code></pre></div><p>加载Haar级联分类器。</p><div class=\"highlight\"><pre><code class=\"language-text\">faceRects = fd.detect(gray, scaleFactor = 1.1, minNeighbors = 8, minSize = (30, 30))</code></pre></div><p>设置相应的输入参数，得到图中检测到所有人脸的边框。</p><div class=\"highlight\"><pre><code class=\"language-text\">print &#34;I found {} face(s)&#34;.format(len(faceRects))</code></pre></div><p>打印图中检测到的人脸数量。</p><div class=\"highlight\"><pre><code class=\"language-text\">for x, y, w, h in faceRects:\n  cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)</code></pre></div><p>在图中绘制出所有的边框，设置边框颜色为绿色，线宽为2。</p><div class=\"highlight\"><pre><code class=\"language-text\">cv2.imshow(&#34;Faces&#34;, image)\ncv2.imwrite(args[&#34;result&#34;], image)\ncv2.waitKey(0)</code></pre></div><p>显示并保存检测结果图，键盘按任意键退出显示。</p><hr/><p><b>Demo</b></p><p class=\"ztext-empty-paragraph\"><br/></p><p>在工程根目录下执行：</p><div class=\"highlight\"><pre><code class=\"language-text\">python faceVisual.py -f data/haarcascades_cuda/haarcascade_frontalface_default.xml -i family_portrait_photograph.jpg -r family_portrait_photograph_result.jpg\npython faceVisual.py -f data/haarcascades_cuda/haarcascade_frontalface_default.xml -i wulinwaizhuan.jpg -r wulinwaizhuan_result.jpg</code></pre></div><p>分别显示I found 10 face(s)和I found 6 face(s)及其对应的检测结果图并保存。</p><p>最后当然是附上人脸检测的最终效果图啦！</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-ae8c653577062fecc706063ddc3d6a1e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1197\" data-rawheight=\"776\" class=\"origin_image zh-lightbox-thumb\" width=\"1197\" data-original=\"https://pic3.zhimg.com/v2-ae8c653577062fecc706063ddc3d6a1e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1197&#39; height=&#39;776&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1197\" data-rawheight=\"776\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1197\" data-original=\"https://pic3.zhimg.com/v2-ae8c653577062fecc706063ddc3d6a1e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-ae8c653577062fecc706063ddc3d6a1e_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-8b7b2a6d68994f86a148f85798d22c7b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb\" width=\"450\" data-original=\"https://pic4.zhimg.com/v2-8b7b2a6d68994f86a148f85798d22c7b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;450&#39; height=&#39;299&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"450\" data-original=\"https://pic4.zhimg.com/v2-8b7b2a6d68994f86a148f85798d22c7b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-8b7b2a6d68994f86a148f85798d22c7b_b.jpg\"/></figure><p>欢迎关注公众号：huangxiaobai880</p><a class=\"video-box\" href=\"https://link.zhihu.com/?target=https%3A//www.zhihu.com/video/958646318126686208\" target=\"_blank\" data-video-id=\"\" data-video-playable=\"true\" data-name=\"\" data-poster=\"https://pic1.zhimg.com/80/v2-f29e629b0a4b7b05d4e2b4182d62c2fc_b.jpg\" data-lens-id=\"958646318126686208\"><img class=\"thumbnail\" src=\"https://pic1.zhimg.com/80/v2-f29e629b0a4b7b05d4e2b4182d62c2fc_b.jpg\"/><span class=\"content\"><span class=\"title\"><span class=\"z-ico-extern-gray\"></span><span class=\"z-ico-extern-blue\"></span></span><span class=\"url\"><span class=\"z-ico-video\"></span>https://www.zhihu.com/video/958646318126686208</span></span></a><p></p>", 
            "topic": [
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }
            ], 
            "comments": [
                {
                    "userName": "EzioAuditore", 
                    "userLink": "https://www.zhihu.com/people/29dc24326a8715f14aad2dfe34ab314c", 
                    "content": "<p>学长，想读论文在哪里找？我只知道论文的名字和作者</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/34255971", 
            "userName": "小白", 
            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
            "upvote": 6, 
            "title": "手把手教你在Ubuntu 16.04下配置GPU版Caffe2", 
            "content": "<p>相比Caffe，Caffe2的安装配置就要容易多了，不过这里还是记录一下，顺便提一下Detectron的安装以及Mask R-CNN的demo。与机器、GPU相关的环境配置同之前一样，在此不再赘述。python环境为Ubuntu16.04自带的python2.7，具体安装配置步骤可参考官网：<a href=\"https://link.zhihu.com/?target=https%3A//caffe2.ai/docs/getting-started.html%3Fplatform%3Dubuntu%26configuration%3Dcompile\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Install</a></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-4869c96d46e4cc955a90a1f2c4069995_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb\" width=\"450\" data-original=\"https://pic2.zhimg.com/v2-4869c96d46e4cc955a90a1f2c4069995_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;450&#39; height=&#39;299&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"450\" data-original=\"https://pic2.zhimg.com/v2-4869c96d46e4cc955a90a1f2c4069995_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-4869c96d46e4cc955a90a1f2c4069995_b.jpg\"/></figure><p><b><i>step1</i></b></p><p>step1:安装依赖项</p><div class=\"highlight\"><pre><code class=\"language-text\">sudo apt-get update\nsudo apt-get install libgtk2.0-dev libpng-dev libpng16-dev python-pip\nsudo apt-get install libopencv-objdetect-dev libopencv-highgui-dev libopencv-legacy-dev libopencv-contrib-dev libopencv-videostab-dev libopencv-superres-dev libopencv-ocl-dev libcv-dev libhighgui-dev libcvaux-dev\nsudo apt-get install -y --no-install-recommends build-essential cmake git libgoogle-glog-dev libgtest-dev libiomp-dev libleveldb-dev liblmdb-dev libopencv-dev libopenmpi-dev libsnappy-dev libprotobuf-dev openmpi-bin openmpi-doc protobuf-compiler protobuf-c-compiler libgflags-dev\nsudo pip install future numpy protobuf flask graphviz jupyter matplotlib pydot python-nvd3 pyyaml requests scikit-image scipy setuptools tornado</code></pre></div><p><b><i>step2</i></b></p><p>step2:Caffe2源码包下载编译安装</p><div class=\"highlight\"><pre><code class=\"language-text\">cd ~\ngit clone --recursive https://github.com/caffe2/caffe2.git\ncd caffe2\nmkdir build\ncd build\ncmake ..\nsudo make install -j8</code></pre></div><p><b><i>step3</i></b></p><p>step3:配置环境变量</p><p>打开~/.bashrc文件：</p><div class=\"highlight\"><pre><code class=\"language-text\">sudo gedit ~/.bashrc</code></pre></div><p>将以下内容写入到~/.bashrc尾部：</p><div class=\"highlight\"><pre><code class=\"language-text\">export PYTHONPATH=$PYTHONPATH:/home/username/caffe2/build</code></pre></div><p>注：username为用户名。</p><p>保存后执行：</p><div class=\"highlight\"><pre><code class=\"language-text\">source ~/.bashrc</code></pre></div><p>使配置生效。</p><p>注：为避免影响caffe的python接口正常使用，可以考虑不配置环境变量，在使用caffe2前先执行：</p><div class=\"highlight\"><pre><code class=\"language-text\">export PYTHONPATH=$PYTHONPATH:/home/username/caffe2/build</code></pre></div><p>亦可。</p><p><b><i>step4</i></b></p><p>step4:安装完成测试</p><p>终端执行：</p><div class=\"highlight\"><pre><code class=\"language-text\">cd ~ &amp;&amp; python -c &#39;from caffe2.python import core&#39; 2&gt;/dev/null &amp;&amp; echo &#34;Success&#34; || echo &#34;Failure&#34;</code></pre></div><p>若显示Success，则安装成功。</p><p>继续执行以检测GPU是否安装成功：</p><div class=\"highlight\"><pre><code class=\"language-text\">python -m caffe2/python/operator_test/relu_op_test</code></pre></div><p>若出现错误提示：</p><div class=\"highlight\"><pre><code class=\"language-text\">ImportError: No module named hypothesis</code></pre></div><p>则安装一下hypothesis模块即可：</p><div class=\"highlight\"><pre><code class=\"language-text\">sudo pip install hypothesis</code></pre></div><p>最终显示OK，说明GPU安装成功。</p><p><b><i>step5</i></b></p><p>step5:其他一些库的安装</p><p>（1）安装python依赖项</p><div class=\"highlight\"><pre><code class=\"language-text\">sudo pip install opencv-python&gt;=3.0\nsudo pip install Cython mock</code></pre></div><p>（2）安装COCO API</p><div class=\"highlight\"><pre><code class=\"language-text\">git clone https://github.com/cocodataset/cocoapi.git\ncd cocoapi/PythonAPI\nsudo make install -j8</code></pre></div><p>若不报错，则安装成功。</p><p>（3）安装目标检测库Detectron</p><div class=\"highlight\"><pre><code class=\"language-text\">git clone https://github.com/facebookresearch/detectron\ncd detectron/lib\nmake -j8\ncd ../tests\npython -m test_spatial_narrow_as_op</code></pre></div><p>最终显示OK，说明Detectron安装成功，即可调用提供的模型进行推理测试。</p><p><b><i>step6</i></b></p><p>step6:MS COCO 2014数据集的下载及其链接</p><p>（1）安装aria2</p><div class=\"highlight\"><pre><code class=\"language-text\">sudo apt-get install aria2</code></pre></div><p>（2）下载数据集</p><div class=\"highlight\"><pre><code class=\"language-text\">aria2c -c http://msvocds.blob.core.windows.net/annotations-1-0-3/instances_train-val2014.zip \naria2c -c http://msvocds.blob.core.windows.net/coco2014/train2014.zip \naria2c -c http://msvocds.blob.core.windows.net/coco2014/val2014.zip</code></pre></div><p>（3）链接数据集</p><div class=\"highlight\"><pre><code class=\"language-text\">ln -s /path/to/coco $DETECTRON/lib/datasets/data/coco</code></pre></div><p>注：/path/to/coco为MS COCO 2014数据集的下载路径，$DETECTRON为Detectron的根目录。</p><p><b><i>step7</i></b></p><p>step7:Mask R-CNN demo</p><p>如果要在本地图像上运行，只需要使用infer_simple.py工具，在Detectron的根目录下执行：</p><div class=\"highlight\"><pre><code class=\"language-text\">python tools/infer_simple.py --cfg configs/12_2017_baselines/e2e_mask_rcnn_R-101-FPN_2x.yaml --output-dir /tmp/detectron-visualizations --image-ext jpg --wts https://s3-us-west-2.amazonaws.com/detectron/35861858/12_2017_baselines/e2e_mask_rcnn_R-101-FPN_2x.yaml.02_32_51.SgT4y1cO/output/train/coco_2014_train:coco_2014_valminusminival/generalized_rcnn/model_final.pkl demo</code></pre></div><p>就能得到如下图所示的输出，其结果保存在/tmp/detectron-visualizations目录下。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-ba9d630a12f9c42da7efd086fa7a9ec9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1280\" data-rawheight=\"688\" class=\"origin_image zh-lightbox-thumb\" width=\"1280\" data-original=\"https://pic2.zhimg.com/v2-ba9d630a12f9c42da7efd086fa7a9ec9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1280&#39; height=&#39;688&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1280\" data-rawheight=\"688\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1280\" data-original=\"https://pic2.zhimg.com/v2-ba9d630a12f9c42da7efd086fa7a9ec9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-ba9d630a12f9c42da7efd086fa7a9ec9_b.jpg\"/></figure><p>欢迎关注公众号：huangxiaobai880</p><a class=\"video-box\" href=\"https://link.zhihu.com/?target=https%3A//www.zhihu.com/video/954451299345940480\" target=\"_blank\" data-video-id=\"\" data-video-playable=\"true\" data-name=\"\" data-poster=\"https://pic1.zhimg.com/80/v2-f29e629b0a4b7b05d4e2b4182d62c2fc_b.jpg\" data-lens-id=\"954451299345940480\"><img class=\"thumbnail\" src=\"https://pic1.zhimg.com/80/v2-f29e629b0a4b7b05d4e2b4182d62c2fc_b.jpg\"/><span class=\"content\"><span class=\"title\"><span class=\"z-ico-extern-gray\"></span><span class=\"z-ico-extern-blue\"></span></span><span class=\"url\"><span class=\"z-ico-video\"></span>https://www.zhihu.com/video/954451299345940480</span></span></a><p></p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/34156101", 
            "userName": "小白", 
            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
            "upvote": 0, 
            "title": "windows安装pycocotools", 
            "content": "<div class=\"highlight\"><pre><code class=\"language-text\">pip install git+https://github.com/philferriere/cocoapi.git#egg=pycocotools^&amp;subdirectory=PythonAPI</code></pre></div><p>在必应国际版中搜的答案，还是可以很好地代替百度的。</p><p>在anconda运行这个命令就可以了，亲测有用。</p><p><b>有大神可以解释一下这个命令是干嘛用的吗？</b></p><p>欢迎关注公众号：huangxiaobai880</p><a class=\"video-box\" href=\"https://link.zhihu.com/?target=https%3A//www.zhihu.com/video/952838832803082240\" target=\"_blank\" data-video-id=\"\" data-video-playable=\"true\" data-name=\"\" data-poster=\"https://pic1.zhimg.com/80/v2-f29e629b0a4b7b05d4e2b4182d62c2fc_b.jpg\" data-lens-id=\"952838832803082240\"><img class=\"thumbnail\" src=\"https://pic1.zhimg.com/80/v2-f29e629b0a4b7b05d4e2b4182d62c2fc_b.jpg\"/><span class=\"content\"><span class=\"title\"><span class=\"z-ico-extern-gray\"></span><span class=\"z-ico-extern-blue\"></span></span><span class=\"url\"><span class=\"z-ico-video\"></span>https://www.zhihu.com/video/952838832803082240</span></span></a><p></p>", 
            "topic": [
                {
                    "tag": "安装", 
                    "tagLink": "https://api.zhihu.com/topics/19608575"
                }
            ], 
            "comments": [
                {
                    "userName": "新手村民工", 
                    "userLink": "https://www.zhihu.com/people/f2c649baf6b9c176f7c7adda60b5ef2d", 
                    "content": "<p>我用了这个办法，但是不行，让我输入username，我输了但是又让我输入password，但是password输入不进去，不知道怎么回事儿</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/33541302", 
            "userName": "小白", 
            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
            "upvote": 7, 
            "title": "手把手教你在Ubuntu 16.04下配置GPU版Caffe（2）", 
            "content": "<p>之前的博客中比较详尽地讲解了和GPU相关的一些环境的配置，下面讲解caffe的python环境及其依赖项的安装。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-0d0239b8bb5c05bdf280b232ac1c825e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb\" width=\"450\" data-original=\"https://pic3.zhimg.com/v2-0d0239b8bb5c05bdf280b232ac1c825e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;450&#39; height=&#39;299&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"450\" data-original=\"https://pic3.zhimg.com/v2-0d0239b8bb5c05bdf280b232ac1c825e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-0d0239b8bb5c05bdf280b232ac1c825e_b.jpg\"/></figure><p><b><i>step3</i></b></p><p>step3:安装Caffe的python环境</p><p>（1）Caffe的官方网站推荐使用Anaconda，在Anaconda的官方网站（<a href=\"https://link.zhihu.com/?target=https%3A//www.anaconda.com/download/%23linux\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Downloads</a>）下载安装文件（我下的是Anaconda2-4.1.1-Linux-x86_64.sh，其中4.1.1是Anaconda版本号），切换到该安装文件所在的目录，执行：</p><div class=\"highlight\"><pre><code class=\"language-text\">bash Anaconda2-4.1.1-Linux-x86_64.sh</code></pre></div><p>整个安装过程选择默认即可。</p><p>（2）添加Anaconda Library Path</p><p>打开~/.bashrc文件：</p><div class=\"highlight\"><pre><code class=\"language-text\">sudo gedit ~/.bashrc </code></pre></div><p>将以下内容写入到~/.bashrc尾部：</p><div class=\"highlight\"><pre><code class=\"language-text\">export LD_LIBRARY_PATH=&#34;/home/username/anaconda2/lib:$LD_LIBRARY_PATH&#34;</code></pre></div><p>注：username为用户名。</p><p>保存后执行：</p><div class=\"highlight\"><pre><code class=\"language-text\">source ~/.bashrc</code></pre></div><p>使配置生效。</p><p>（3）查看Anaconda是否安装完毕</p><p>终端输入：</p><div class=\"highlight\"><pre><code class=\"language-text\">python --version</code></pre></div><p>若显示：</p><div class=\"highlight\"><pre><code class=\"language-text\">Python 2.7.12 :: Anaconda custom (64-bit)</code></pre></div><p>则安装完毕。</p><p>附：Anaconda 的 gcc 版本问题</p><p>Anaconda的python对应的gcc版本是gcc4.4.7，ubuntu自带的Python对应的gcc版本是gcc5或gcc6（通常和系统当前的gcc版本一致）。那么，需改变anaconda的Python对应的gcc版本，使其和系统当前的gcc版本一致。命令如下：</p><div class=\"highlight\"><pre><code class=\"language-text\">conda install libgcc</code></pre></div><p><b><i>step4</i></b></p><p>step4:安装Caffe的依赖项</p><div class=\"highlight\"><pre><code class=\"language-text\">sudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libhdf5-serial-dev protobuf-compiler protobuf-c-compiler\nsudo apt-get install libboost-all-dev build-essential libatlas-base-dev\nsudo apt-get install libgflags-dev libgoogle-glog-dev liblmdb-dev\nsudo apt-get install python-numpy python-scipy python-matplotlib python-sklearn python-skimage python-h5py python-protobuf python-leveldb python-networkx python-nose python-pandas python-gflags cython ipython </code></pre></div><p><b><i>step5</i></b></p><p>step5:Caffe编译及其python接口配置</p><p>（1）安装Caffe的python依赖库</p><p>访问并打开Caffe的github网站：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/BVLC/caffe\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">BVLC/caffe</a>，下载并解压Caffe源码包到Home目录下，进入caffe-master下的python目录，执行：</p><div class=\"highlight\"><pre><code class=\"language-text\">pip install -r requirements.txt</code></pre></div><p>这样做的目的是安装requirements.txt中所罗列出的python包，也可以执行以下命令来完成安装：</p><div class=\"highlight\"><pre><code class=\"language-text\">for req in $(cat requirements.txt); do pip install $req; done</code></pre></div><p>（2）编译Caffe及其python接口</p><p>进入caffe-master目录，复制一份Makefile.config.examples并重命名为Makefile.config，命令如下：</p><div class=\"highlight\"><pre><code class=\"language-text\">cp Makefile.config.examples Makefile.config</code></pre></div><p>打开Makefile.config，修改其中的一些配置并保存，修改后的文件如下：</p><div class=\"highlight\"><pre><code class=\"language-text\">## Refer to Caffe | Installation\n# Contributions simplifying and improving our build system are welcome!\n\n# cuDNN acceleration switch (uncomment to build with cuDNN).\nUSE_CUDNN := 1\n\n# CPU-only switch (uncomment to build without GPU support).\n# CPU_ONLY := 1\n\n# uncomment to disable IO dependencies and corresponding data layers\n# USE_OPENCV := 0\n# USE_LEVELDB := 0\n# USE_LMDB := 0\n\n# uncomment to allow MDB_NOLOCK when reading LMDB files (only if necessary)\n#\tYou should not set this flag if you will be reading LMDBs with any\n#\tpossibility of simultaneous read and write\n# ALLOW_LMDB_NOLOCK := 1\n\n# Uncomment if you&#39;re using OpenCV 3\n# OPENCV_VERSION := 3\n\n# To customize your choice of compiler, uncomment and set the following.\n# N.B. the default for Linux is g++ and the default for OSX is clang++\n# CUSTOM_CXX := g++\n\n# CUDA directory contains bin/ and lib/ directories that we need.\nCUDA_DIR := /usr/local/cuda\n# On Ubuntu 14.04, if cuda tools are installed via\n# &#34;sudo apt-get install nvidia-cuda-toolkit&#34; then use this instead:\n# CUDA_DIR := /usr\n\n# CUDA architecture setting: going with all of them.\n# For CUDA &lt; 6.0, comment the *_50 through *_61 lines for compatibility.\n# For CUDA &lt; 8.0, comment the *_60 and *_61 lines for compatibility.\nCUDA_ARCH := -gencode arch=compute_20,code=sm_20 \\\n\t\t-gencode arch=compute_20,code=sm_21 \\\n\t\t-gencode arch=compute_30,code=sm_30 \\\n\t\t-gencode arch=compute_35,code=sm_35 \\\n\t\t-gencode arch=compute_50,code=sm_50 \\\n\t\t-gencode arch=compute_52,code=sm_52 \\\n\t\t-gencode arch=compute_60,code=sm_60 \\\n\t\t-gencode arch=compute_61,code=sm_61 \\\n\t\t-gencode arch=compute_61,code=compute_61\n\n# BLAS choice:\n# atlas for ATLAS (default)\n# mkl for MKL\n# open for OpenBlas\nBLAS := atlas\n# Custom (MKL/ATLAS/OpenBLAS) include and lib directories.\n# Leave commented to accept the defaults for your choice of BLAS\n# (which should work)!\n# BLAS_INCLUDE := /path/to/your/blas\n# BLAS_LIB := /path/to/your/blas\n\n# Homebrew puts openblas in a directory that is not on the standard search path\n# BLAS_INCLUDE := $(shell brew --prefix openblas)/include\n# BLAS_LIB := $(shell brew --prefix openblas)/lib\n\n# This is required only if you will compile the matlab interface.\n# MATLAB directory should contain the mex binary in /bin.\n# MATLAB_DIR := /usr/local\n# MATLAB_DIR := /Applications/MATLAB_R2012b.app\n\n# NOTE: this is required only if you will compile the python interface.\n# We need to be able to find Python.h and numpy/arrayobject.h.\n# PYTHON_INCLUDE := /usr/include/python2.7 \\\n\t\t# /usr/lib/python2.7/dist-packages/numpy/core/include\n# Anaconda Python distribution is quite popular. Include path:\n# Verify anaconda location, sometimes it&#39;s in root.\nANACONDA_HOME := $(HOME)/anaconda2\nPYTHON_INCLUDE := $(ANACONDA_HOME)/include \\\n\t\t$(ANACONDA_HOME)/include/python2.7 \\\n\t\t$(ANACONDA_HOME)/lib/python2.7/site-packages/numpy/core/include\n\n# Uncomment to use Python 3 (default is Python 2)\n# PYTHON_LIBRARIES := boost_python3 python3.5m\n# PYTHON_INCLUDE := /usr/include/python3.5m \\\n#                 /usr/lib/python3.5/dist-packages/numpy/core/include\n\n# We need to be able to find libpythonX.X.so or .dylib.\n# PYTHON_LIB := /usr/lib\nPYTHON_LIB := $(ANACONDA_HOME)/lib\n\n# Homebrew installs numpy in a non standard path (keg only)\n# PYTHON_INCLUDE += $(dir $(shell python -c &#39;import numpy.core; print(numpy.core.__file__)&#39;))/include\n# PYTHON_LIB += $(shell brew --prefix numpy)/lib\n\n# Uncomment to support layers written in Python (will link against Python libs)\nWITH_PYTHON_LAYER := 1\n\n# Whatever else you find you need goes here.\nINCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include /usr/include/hdf5/serial\nLIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib /usr/lib/x86_64-linux-gnu /usr/lib/x86_64-linux-gnu/hdf5/serial\n\n# If Homebrew is installed at a non standard location (for example your home directory) and you use it for general dependencies\n# INCLUDE_DIRS += $(shell brew --prefix)/include\n# LIBRARY_DIRS += $(shell brew --prefix)/lib\n\n# NCCL acceleration switch (uncomment to build with NCCL)\n# NVIDIA/nccl (last tested version: v1.2.3-1+cuda8.0)\n# USE_NCCL := 1\n\n# Uncomment to use `pkg-config` to specify OpenCV library paths.\n# (Usually not necessary -- OpenCV libraries are normally installed in one of the above $LIBRARY_DIRS.)\n# USE_PKG_CONFIG := 1\n\n# N.B. both build and distribute dirs are cleared on `make clean`\nBUILD_DIR := build\nDISTRIBUTE_DIR := distribute\n\n# Uncomment for debugging. Does not work on OSX due to Removing -DNDEBUG from COMMON_FLAGS in Makefile breaks OS  X build · Issue #171 · BVLC/caffe\n# DEBUG := 1\n\n# The ID of the GPU that &#39;make runtest&#39; will use to run unit tests.\nTEST_GPUID := 0\n\n# enable pretty build (comment to see full commands)\nQ ?= @</code></pre></div><p>现对修改的地方做一下讲解：</p><p>1.若要使用cuDNN加速，则USE_CUDNN := 1这一行取消注释；</p><p>2.若要使用Anaconda的python环境，则与python相关的一些路径需要换成Anaconda的，对应的配置如下：</p><div class=\"highlight\"><pre><code class=\"language-text\"># NOTE: this is required only if you will compile the python interface.\n# We need to be able to find Python.h and numpy/arrayobject.h.\n# PYTHON_INCLUDE := /usr/include/python2.7 \\\n\t\t# /usr/lib/python2.7/dist-packages/numpy/core/include\n# Anaconda Python distribution is quite popular. Include path:\n# Verify anaconda location, sometimes it&#39;s in root.\nANACONDA_HOME := $(HOME)/anaconda2\nPYTHON_INCLUDE := $(ANACONDA_HOME)/include \\\n\t\t$(ANACONDA_HOME)/include/python2.7 \\\n\t\t$(ANACONDA_HOME)/lib/python2.7/site-packages/numpy/core/include\n\n# Uncomment to use Python 3 (default is Python 2)\n# PYTHON_LIBRARIES := boost_python3 python3.5m\n# PYTHON_INCLUDE := /usr/include/python3.5m \\\n#                 /usr/lib/python3.5/dist-packages/numpy/core/include\n\n# We need to be able to find libpythonX.X.so or .dylib.\n# PYTHON_LIB := /usr/lib\nPYTHON_LIB := $(ANACONDA_HOME)/lib</code></pre></div><p>3.若后续使用Caffe时需要用到python类型的层，则WITH_PYTHON_LAYER := 1这一行取消注释；</p><p>4.由于Ubuntu16.04的文件包含位置发生了变化，尤其是需要用到的hdf5的位置，因此需要在Makefile.config中添加这一路径，否则在编译Caffe的时候会提醒找不到hdf5.h或者hdf5_hl.h，错误提示为：</p><div class=\"highlight\"><pre><code class=\"language-text\">./include/caffe/util/hdf5.hpp:6:18:fatal error:hdf5.h:No such file or directory</code></pre></div><p>对应的配置如下：</p><div class=\"highlight\"><pre><code class=\"language-text\"># Whatever else you find you need goes here.\nINCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/include /usr/include/hdf5/serial\nLIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib /usr/lib/x86_64-linux-gnu /usr/lib/x86_64-linux-gnu/hdf5/serial</code></pre></div><p>修改完成后，在caffe-master目录下执行编译命令：</p><div class=\"highlight\"><pre><code class=\"language-text\">make all -j8</code></pre></div><p>若出现错误提示：</p><div class=\"highlight\"><pre><code class=\"language-text\">fatal error:caffe/proto/caffe.pb.h:No such file or directory</code></pre></div><p>则有可能是编译太快出现的错误，执行：</p><div class=\"highlight\"><pre><code class=\"language-text\">make clean</code></pre></div><p>后再进行编译。</p><p>编译成功后，继续执行：</p><div class=\"highlight\"><pre><code class=\"language-text\">make test -j8\nmake runtest -j8\nmake pycaffe   # 编译Caffe的python接口\nmake distribute</code></pre></div><p>此时，Caffe及其python接口配置完成。</p><p>（3）Caffe的python接口测试</p><p>执行python进入python环境，输入以下命令：</p><div class=\"highlight\"><pre><code class=\"language-text\">import sys\nsys.path.append(&#39;/home/username/caffe-master/python&#39;)\nimport caffe</code></pre></div><p>注：username为用户名。</p><p>若不报错，则Caffe的python接口配置完成。</p><p>欢迎关注公众号：huangxiaobai880</p><a class=\"video-box\" href=\"https://link.zhihu.com/?target=https%3A//www.zhihu.com/video/942871411233132544\" target=\"_blank\" data-video-id=\"\" data-video-playable=\"true\" data-name=\"\" data-poster=\"https://pic1.zhimg.com/80/v2-f29e629b0a4b7b05d4e2b4182d62c2fc_b.jpg\" data-lens-id=\"942871411233132544\"><img class=\"thumbnail\" src=\"https://pic1.zhimg.com/80/v2-f29e629b0a4b7b05d4e2b4182d62c2fc_b.jpg\"/><span class=\"content\"><span class=\"title\"><span class=\"z-ico-extern-gray\"></span><span class=\"z-ico-extern-blue\"></span></span><span class=\"url\"><span class=\"z-ico-video\"></span>https://www.zhihu.com/video/942871411233132544</span></span></a><p></p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/33484702", 
            "userName": "小白", 
            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
            "upvote": 5, 
            "title": "手把手教你在Ubuntu 16.04下配置GPU版Caffe（1）", 
            "content": "<p>搞了半个月终于弄完了，反复配置了好几次，主要是遇到报错时忘了解决办法，不得不花费好长时间。可以说，caffe是目前我遇到过的最难配置的东西了。所以，这次干脆将其记录下来。这里由于过程较为复杂，就不再录制视频了，各位看官若存在疑问可留言。接下来默认已安装了Ubuntu 16.04 64bit操作系统，并拥有具备CuDNN能力的NVIDIA GTX显卡。参考机器配置如下：</p><p>Ubuntu 16.04 64bit</p><p>Intel i7 CPU</p><p>8G内存</p><p>GTX Titan X显卡（12G显存）</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-786071f367197166c0dfe633bb7a27f2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb\" width=\"450\" data-original=\"https://pic3.zhimg.com/v2-786071f367197166c0dfe633bb7a27f2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;450&#39; height=&#39;299&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"450\" data-original=\"https://pic3.zhimg.com/v2-786071f367197166c0dfe633bb7a27f2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-786071f367197166c0dfe633bb7a27f2_b.jpg\"/></figure><p><b><i>  首先，配置前建议先熟悉一下Linux命令，否则后续配置过程中因不熟悉Linux命令而胡乱搞一通，很容易泪崩。</i></b></p><p><b><i>step1</i></b></p><p>step1:安装英伟达显卡驱动</p><p>（1）先在官网上下载对应的驱动程序（下载地址：<a href=\"https://link.zhihu.com/?target=http%3A//www.nvidia.cn/Download/index.aspx%3Flang%3Dcn\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">NVIDIA 驱动程序下载</a>）</p><p>  注：有时候会只显示一个网页文档，那就右击把它下载下来，下载后的文件名形式为：NVIDIA-Linux-x86_64-3xx.xx.run，例如我下载的是NVIDIA-Linux-x86_64-375.26.run，375.26为英伟达显卡驱动版本号。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-115ae8b2c087d1edb188ccc54598f419_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1158\" data-rawheight=\"812\" class=\"origin_image zh-lightbox-thumb\" width=\"1158\" data-original=\"https://pic2.zhimg.com/v2-115ae8b2c087d1edb188ccc54598f419_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1158&#39; height=&#39;812&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1158\" data-rawheight=\"812\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1158\" data-original=\"https://pic2.zhimg.com/v2-115ae8b2c087d1edb188ccc54598f419_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-115ae8b2c087d1edb188ccc54598f419_b.jpg\"/></figure><p>（2）禁用开源nouveau驱动</p><p>  a.先把vim编辑器装上：终端输入：</p><div class=\"highlight\"><pre><code class=\"language-text\">sudo apt-get install vim</code></pre></div><p>若装不上，输入：</p><div class=\"highlight\"><pre><code class=\"language-text\">sudo apt-get update</code></pre></div><p>更新下apt源，然后终端输入：</p><div class=\"highlight\"><pre><code class=\"language-text\">sudo vim /etc/modprobe.d/disable-nouveau.conf</code></pre></div><p>编辑文件/etc/modprobe.d/disable-nouveau.conf，按i进入编辑模式，输入以下两行：</p><div class=\"highlight\"><pre><code class=\"language-text\">blacklist nouveau\noptions nouveau modeset = 0</code></pre></div><p>按esc键退出编辑，输入:wq保存退出，然后输入：</p><div class=\"highlight\"><pre><code class=\"language-text\">sudo ldconfig</code></pre></div><p>再执行：</p><div class=\"highlight\"><pre><code class=\"language-text\">sudo update-initramfs -u</code></pre></div><p>使配置生效。</p><p>  注：关于vim编辑器的使用可参考这篇博客：<a href=\"https://link.zhihu.com/?target=https%3A//www.cnblogs.com/lijia0511/p/5644566.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">vim简单使用教程 - 走在大牛的路上 - 博客园</a><br/>        b.配置kernel以text模式启动</p><p>  终端执行：</p><div class=\"highlight\"><pre><code class=\"language-text\">sudo vim /etc/default/grub</code></pre></div><p>编辑文件/etc/default/grub，修改</p><div class=\"highlight\"><pre><code class=\"language-text\">GRUB_CMDLINE_LINUX_DEFAULT=&#34;quiet splash&#34;\nGRUB_CMDLINE_LINUX=&#34;&#34;</code></pre></div><p>这两行为：</p><div class=\"highlight\"><pre><code class=\"language-text\">GRUB_CMDLINE_LINUX_DEFAULT=&#34;quietsplash text&#34;\nGRUB_CMDLINE_LINUX=&#34;rdblacklist=nouveaunouveau.modeset=0&#34;</code></pre></div><p>保存退出后，执行：</p><div class=\"highlight\"><pre><code class=\"language-text\">sudo update-grub</code></pre></div><p>以更新grub。</p><p>  c.重启电脑，在text模式下用命令行安装驱动</p><p>  在进入用户登陆界面后，按Ctrl+Alt+F1进入控制台，输入用户名密码登陆后，输入：</p><div class=\"highlight\"><pre><code class=\"language-text\">sudo service lightdm stop</code></pre></div><p>关闭图形界面（切记，否则英伟达显卡驱动会安装失败），然后cd命令切换到英伟达显卡驱动安装文件NVIDIA-Linux-x86_64-3xx.xx.run所在的目录，执行：</p><div class=\"highlight\"><pre><code class=\"language-text\">sudo sh NVIDIA-Linux-x86_64-3xx.xx.run</code></pre></div><p>选Accept，根据提示一路Yes和OK到底完成安装（安装成功检查：输入：</p><div class=\"highlight\"><pre><code class=\"language-text\">nvidia-smi</code></pre></div><p>命令查看英伟达显卡驱动信息），再输入：</p><div class=\"highlight\"><pre><code class=\"language-text\">sudo service lightdm start</code></pre></div><p>回到登陆界面登陆。</p><p>  d.结束后再次修改grub文件</p><p>  终端执行：</p><div class=\"highlight\"><pre><code class=\"language-text\">sudo vim /etc/default/grub</code></pre></div><p>编辑文件/etc/default/grub，修改</p><div class=\"highlight\"><pre><code class=\"language-text\">GRUB_CMDLINE_LINUX_DEFAULT=&#34;quietsplash text&#34;\nGRUB_CMDLINE_LINUX=&#34;rdblacklist=nouveaunouveau.modeset=0&#34;</code></pre></div><p>这两行为：</p><div class=\"highlight\"><pre><code class=\"language-text\">GRUB_CMDLINE_LINUX_DEFAULT=&#34;quiet splash&#34;\nGRUB_CMDLINE_LINUX=&#34;rdblacklist=nouveaunouveau.modeset=0&#34;</code></pre></div><p>保存退出后，执行：</p><div class=\"highlight\"><pre><code class=\"language-text\">sudo update-grub</code></pre></div><p>以更新grub，重启完成。</p><p>附：解决ubuntu无限重复登陆</p><p>我觉得是三种原因：</p><p>（1）先前安装的英伟达显卡驱动崩溃，解决方法是进入控制台关闭图形界面后，输入：</p><div class=\"highlight\"><pre><code class=\"language-text\">sudo sh NVIDIA-Linux-x86_64-3xx.xx.run --uninstall</code></pre></div><p>将崩溃的英伟达显卡驱动卸载干净，再重新安装；</p><p>（2）nouveau的黑名单不见了，解决方法如安装，重新写blacklist，重新刷grub；</p><p>（3）.Xauthority文件中文件夹权限冲突，解决方法是进入命令行模式输入：</p><div class=\"highlight\"><pre><code class=\"language-text\">sudo rm ~/.Xauthority</code></pre></div><p>来删除该文件。</p><p><b><i>step2</i></b></p><p>step2:安装CUDA8.0以及cuDNN</p><p>（1）先在官网上下载对应的CUDA安装文件，这里选择run安装（下载地址：<a href=\"https://link.zhihu.com/?target=https%3A//developer.nvidia.com/cuda-80-ga2-download-archive\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">CUDA Toolkit 8.0 - Feb 2017</a>）</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-a55f267ae235017fb2f4e30b88c046ab_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"655\" data-rawheight=\"884\" class=\"origin_image zh-lightbox-thumb\" width=\"655\" data-original=\"https://pic4.zhimg.com/v2-a55f267ae235017fb2f4e30b88c046ab_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;655&#39; height=&#39;884&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"655\" data-rawheight=\"884\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"655\" data-original=\"https://pic4.zhimg.com/v2-a55f267ae235017fb2f4e30b88c046ab_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-a55f267ae235017fb2f4e30b88c046ab_b.jpg\"/></figure><p>  下载完成后，打开终端切换到该安装文件所在目录，执行：</p><div class=\"highlight\"><pre><code class=\"language-text\">sudo sh cuda_8.0.61_375.26_linux.run</code></pre></div><p>按空格跳过前面的协议，按提示输accept，提示安装图形驱动时输no（英伟达显卡驱动之前已安装），后面按提示输yes和默认路径安装即可。</p><p>（2）环境变量配置</p><p>打开~/.bashrc文件： </p><div class=\"highlight\"><pre><code class=\"language-text\">sudo gedit ~/.bashrc </code></pre></div><p>将以下内容写入到~/.bashrc尾部：</p><div class=\"highlight\"><pre><code class=\"language-text\">export PATH=/usr/local/cuda-8.0/bin${PATH:+:${PATH}}\nexport LD_LIBRARY_PATH=/usr/local/cuda-8.0/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}</code></pre></div><p>保存后执行：</p><div class=\"highlight\"><pre><code class=\"language-text\">source ~/.bashrc</code></pre></div><p>使配置生效。</p><p>（3）配置cuDNN</p><p>cuDNN是GPU加速计算深层神经网络的库。</p><p>首先去官网 <a href=\"https://link.zhihu.com/?target=https%3A//developer.nvidia.com/rdp/cudnn-download\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">developer.nvidia.com/rd</span><span class=\"invisible\">p/cudnn-download</span><span class=\"ellipsis\"></span></a> 下载cuDNN，需要注册一个账号才能下载。下载版本号如下图：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-003aa01e6d639a62536043e4e23e4a47_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"922\" data-rawheight=\"454\" class=\"origin_image zh-lightbox-thumb\" width=\"922\" data-original=\"https://pic4.zhimg.com/v2-003aa01e6d639a62536043e4e23e4a47_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;922&#39; height=&#39;454&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"922\" data-rawheight=\"454\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"922\" data-original=\"https://pic4.zhimg.com/v2-003aa01e6d639a62536043e4e23e4a47_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-003aa01e6d639a62536043e4e23e4a47_b.jpg\"/></figure><p>下载cuDNN5.1之后切换到下载目录进行解压：</p><div class=\"highlight\"><pre><code class=\"language-text\">sudo tar -zxvf ./cudnn-8.0-linux-x64-v5.1.tgz </code></pre></div><p>进入cuDNN5.1解压之后的include目录，在命令行进行如下操作：</p><div class=\"highlight\"><pre><code class=\"language-text\">cd cuda/include\nsudo cp cudnn.h /usr/local/cuda/include  #复制头文件</code></pre></div><p>再将进入lib64目录下的动态文件进行复制和链接：</p><div class=\"highlight\"><pre><code class=\"language-text\">cd ..\ncd lib64\nsudo cp lib* /usr/local/cuda/lib64/    #复制动态链接库\ncd /usr/local/cuda/lib64/\nsudo rm -rf libcudnn.so libcudnn.so.5    #删除原有动态文件\nsudo ln -s libcudnn.so.5.1.10 libcudnn.so.5  #生成软衔接\nsudo ln -s libcudnn.so.5 libcudnn.so      #生成软链接\nsudo ldconfig      #使配置生效</code></pre></div><p>（4）测试CUDA的samples</p><div class=\"highlight\"><pre><code class=\"language-text\">cd /usr/local/cuda-8.0/samples/1_Utilities/deviceQuery\nsudo make \nsudo ./deviceQuery</code></pre></div><p>如果显示一些关于GPU的信息，则说明安装成功。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-5bb679f8b9982f179ca9df9244ff0eaf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"887\" data-rawheight=\"562\" class=\"origin_image zh-lightbox-thumb\" width=\"887\" data-original=\"https://pic4.zhimg.com/v2-5bb679f8b9982f179ca9df9244ff0eaf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;887&#39; height=&#39;562&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"887\" data-rawheight=\"562\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"887\" data-original=\"https://pic4.zhimg.com/v2-5bb679f8b9982f179ca9df9244ff0eaf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-5bb679f8b9982f179ca9df9244ff0eaf_b.jpg\"/></figure><p>注：CUDA版本可通过命令：</p><div class=\"highlight\"><pre><code class=\"language-text\">nvcc --version</code></pre></div><p>查看。</p><p>欢迎关注公众号：huangxiaobai880</p><a class=\"video-box\" href=\"https://link.zhihu.com/?target=https%3A//www.zhihu.com/video/942705913874870272\" target=\"_blank\" data-video-id=\"\" data-video-playable=\"true\" data-name=\"\" data-poster=\"https://pic1.zhimg.com/80/v2-f29e629b0a4b7b05d4e2b4182d62c2fc_b.jpg\" data-lens-id=\"942705913874870272\"><img class=\"thumbnail\" src=\"https://pic1.zhimg.com/80/v2-f29e629b0a4b7b05d4e2b4182d62c2fc_b.jpg\"/><span class=\"content\"><span class=\"title\"><span class=\"z-ico-extern-gray\"></span><span class=\"z-ico-extern-blue\"></span></span><span class=\"url\"><span class=\"z-ico-video\"></span>https://www.zhihu.com/video/942705913874870272</span></span></a><p></p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": [
                {
                    "userName": "HAPPYGOING", 
                    "userLink": "https://www.zhihu.com/people/4eca55597f11bd87f7f48026a006562f", 
                    "content": "<p>在 2 （b）步骤如果遇到dns-clean.service的报错；可以执行如下操作：</p><ol><li> Ctrl+alt + F1 进入命令行登录</li><li>执行 sudo vim /lib/systemd/system/dns-clean.service，输入密码</li><li>按 i 编辑</li><li>将ExecStartPre=/bin/mkdir /var/run/pppconfig 改为<br>ExecStartPre=/bin/mkdir -p /var/run/pppconfig(添加 -p 参数)</li><li> esc退出编辑， ：wq保存</li><li>reboot</li></ol>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "猪猪会爬树250", 
                    "userLink": "https://www.zhihu.com/people/83e9292231447ee9f81b71f5cf404a71", 
                    "content": "<p>您好，我遇到了ERROR:Unable to load the 'nvidia-drm' kernel module.请问您遇到过吗？好多次都是这样了。。。</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "小白", 
                            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
                            "content": "<p><a href=\"http://link.zhihu.com/?target=https%3A//blog.csdn.net/ksws0292756/article/details/79160742\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">blog.csdn.net/ksws02927</span><span class=\"invisible\">56/article/details/79160742</span><span class=\"ellipsis\"></span></a></p><p>看下这个</p>", 
                            "likes": 0, 
                            "replyToAuthor": "猪猪会爬树250"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/33110614", 
            "userName": "小白", 
            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
            "upvote": 1, 
            "title": "keras与tensorflow一起用", 
            "content": "<p></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-f478e77d344dd6994cfa84a122902290_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb\" width=\"450\" data-original=\"https://pic1.zhimg.com/v2-f478e77d344dd6994cfa84a122902290_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;450&#39; height=&#39;299&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"450\" data-original=\"https://pic1.zhimg.com/v2-f478e77d344dd6994cfa84a122902290_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-f478e77d344dd6994cfa84a122902290_b.jpg\"/></figure><p>写在前面：keras中是用model来管理数据流的，也是根据model来更新权值的，就是目标数据来更新网络，在对一些不是特别复杂的操作时是够用了。但是，在面对复杂的操作时，keras就有点麻烦了。一般论文都是根据loss来更新权值，有时还用到多个loss安不同的权值来更新网络，这时按loss来更新权值就显的很方便了，tensorflow就是这种方式。所以，可不可以将这两种方式结合在一起呢？</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-b184cf81b038cbf2a30123bbf93fbc1e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"637\" data-rawheight=\"183\" class=\"origin_image zh-lightbox-thumb\" width=\"637\" data-original=\"https://pic3.zhimg.com/v2-b184cf81b038cbf2a30123bbf93fbc1e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;637&#39; height=&#39;183&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"637\" data-rawheight=\"183\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"637\" data-original=\"https://pic3.zhimg.com/v2-b184cf81b038cbf2a30123bbf93fbc1e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-b184cf81b038cbf2a30123bbf93fbc1e_b.jpg\"/></figure><ol><li>一个网络一个单独的文件</li><li>不指定Input，在创建model时指定</li><li>with tf.variable_scope(name):一定要有，给在这个网络下的所有变量附上名字</li><li>返回的是一个model</li><li>keras来管理model会很方便，<b>它支持复用</b></li><li><b>还有就要夸一下谷歌了，大场就是不一样，竟然完美支持keras。附上一个谷歌的不要翻墙就可以等的tensorflow网址</b><a href=\"https://link.zhihu.com/?target=https%3A//tensorflow.google.cn/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">TensorFlow</a></li></ol><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-e26b3b0ba745570e58aef90c626217ae_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"508\" data-rawheight=\"164\" class=\"origin_image zh-lightbox-thumb\" width=\"508\" data-original=\"https://pic3.zhimg.com/v2-e26b3b0ba745570e58aef90c626217ae_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;508&#39; height=&#39;164&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"508\" data-rawheight=\"164\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"508\" data-original=\"https://pic3.zhimg.com/v2-e26b3b0ba745570e58aef90c626217ae_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-e26b3b0ba745570e58aef90c626217ae_b.jpg\"/></figure><p>创建模型，并顺手编译一下</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-8dfd4bbd1c3e94fc4fd352631e4200fe_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"817\" data-rawheight=\"216\" class=\"origin_image zh-lightbox-thumb\" width=\"817\" data-original=\"https://pic3.zhimg.com/v2-8dfd4bbd1c3e94fc4fd352631e4200fe_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;817&#39; height=&#39;216&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"817\" data-rawheight=\"216\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"817\" data-original=\"https://pic3.zhimg.com/v2-8dfd4bbd1c3e94fc4fd352631e4200fe_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-8dfd4bbd1c3e94fc4fd352631e4200fe_b.jpg\"/></figure><p>用传统的keras方法训练一下</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-8553cd8889f3175e77996c0f7a396561_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"873\" data-rawheight=\"518\" class=\"origin_image zh-lightbox-thumb\" width=\"873\" data-original=\"https://pic2.zhimg.com/v2-8553cd8889f3175e77996c0f7a396561_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;873&#39; height=&#39;518&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"873\" data-rawheight=\"518\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"873\" data-original=\"https://pic2.zhimg.com/v2-8553cd8889f3175e77996c0f7a396561_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-8553cd8889f3175e77996c0f7a396561_b.jpg\"/></figure><p>想用tf来训练，这就尴尬了！我用tf训练的是model的权值啊，怎么model的预测值毛都没变！！！而我的os是有效的啊！！！</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-c92e34f8c477c2ce383b297e1d615f4c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"900\" data-rawheight=\"527\" class=\"origin_image zh-lightbox-thumb\" width=\"900\" data-original=\"https://pic1.zhimg.com/v2-c92e34f8c477c2ce383b297e1d615f4c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;900&#39; height=&#39;527&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"900\" data-rawheight=\"527\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"900\" data-original=\"https://pic1.zhimg.com/v2-c92e34f8c477c2ce383b297e1d615f4c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-c92e34f8c477c2ce383b297e1d615f4c_b.jpg\"/></figure><p>这样就有效果了！！！</p><a class=\"video-box\" href=\"https://link.zhihu.com/?target=https%3A//www.zhihu.com/video/937791702736519168\" target=\"_blank\" data-video-id=\"\" data-video-playable=\"true\" data-name=\"\" data-poster=\"https://pic1.zhimg.com/80/v2-f29e629b0a4b7b05d4e2b4182d62c2fc_b.jpg\" data-lens-id=\"937791702736519168\"><img class=\"thumbnail\" src=\"https://pic1.zhimg.com/80/v2-f29e629b0a4b7b05d4e2b4182d62c2fc_b.jpg\"/><span class=\"content\"><span class=\"title\"><span class=\"z-ico-extern-gray\"></span><span class=\"z-ico-extern-blue\"></span></span><span class=\"url\"><span class=\"z-ico-video\"></span>https://www.zhihu.com/video/937791702736519168</span></span></a><p></p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": [
                {
                    "userName": "ZL LI", 
                    "userLink": "https://www.zhihu.com/people/66ee49125ef13cfc327305a6e8cef6d1", 
                    "content": "<p>代码看不清啊</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "小白", 
                            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
                            "content": "<p>哪里看不清</p>", 
                            "likes": 0, 
                            "replyToAuthor": "ZL LI"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/32882985", 
            "userName": "小白", 
            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
            "upvote": 2, 
            "title": "python二进制文件的读取", 
            "content": "<p></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-f557290bae0f940dda29fbaff3bab965_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb\" width=\"450\" data-original=\"https://pic2.zhimg.com/v2-f557290bae0f940dda29fbaff3bab965_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;450&#39; height=&#39;299&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"450\" data-original=\"https://pic2.zhimg.com/v2-f557290bae0f940dda29fbaff3bab965_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-f557290bae0f940dda29fbaff3bab965_b.jpg\"/></figure><p>为什么要讨论二进制文件的读取，因为我们处理的数据不一定就是语音和图像。有时我们需要将普通数据保存读取。这是推荐一个非常好的库：<i><b>pickle，它会直接把对象原封不动的保存（它会记录数组的相关信息），在原封不动的读取。</b></i></p><div class=\"highlight\"><pre><code class=\"language-text\">import pickle\na = np.array([[1,2,3],[4,5,6],[7,8,9],[11,12,13]]).astype(&#39;float32&#39;)\nwith open(&#39;a.bin&#39;, &#39;wb&#39;) as config_f:\n    pickle.dump(a, config_f)</code></pre></div><p>这个就是保存一个数组【<b><i>pickle.dump和pickle.dumps大家一定一定要注意</i></b>】</p><div class=\"highlight\"><pre><code class=\"language-text\">import pickle\nwith open(&#39;a.bin&#39;, &#39;rb&#39;) as f_in:\n    C = pickle.load(f_in)</code></pre></div><p>这是读取一个数组，<b><i>惊不惊喜!意不意外！太方便了！</i></b></p><p><b>这是一个很重要的分割线，下面是用tf进行的二进制文件的操作，有点麻烦。老实说，我特别不喜欢tf的二进制读取，特别麻烦。</b></p><p>-----------------------------------------------------------------------------------------------</p><hr/><p>写一个二进文件</p><div class=\"highlight\"><pre><code class=\"language-text\">import numpy as np\na = np.array([1,2]).astype(np.float32)\nwith open(&#39;a.bin&#39;, &#39;wb&#39;) as fp:\n    fp.write(a.tostring())</code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-7aa9693ea85230acf2d44b3da8871981_b.jpg\" data-size=\"normal\" data-rawwidth=\"228\" data-rawheight=\"117\" class=\"content_image\" width=\"228\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;228&#39; height=&#39;117&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"228\" data-rawheight=\"117\" class=\"content_image lazy\" width=\"228\" data-actualsrc=\"https://pic2.zhimg.com/v2-7aa9693ea85230acf2d44b3da8871981_b.jpg\"/><figcaption>得到二进制打开是这样的</figcaption></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-8f306a14471f1ab58ddc1b24dd3eb046_b.jpg\" data-size=\"normal\" data-rawwidth=\"281\" data-rawheight=\"112\" class=\"content_image\" width=\"281\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;281&#39; height=&#39;112&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"281\" data-rawheight=\"112\" class=\"content_image lazy\" width=\"281\" data-actualsrc=\"https://pic3.zhimg.com/v2-8f306a14471f1ab58ddc1b24dd3eb046_b.jpg\"/><figcaption>把a改为[1,2,3]</figcaption></figure><p>这证明我们成功的将一个二进制文件写进去了</p><ol><li>a要是一个numpy数组</li><li>open当文件不存的时候会自动创建</li><li>tostring()：存入的二进制一般是字符串类型</li><li>其会一行的一行的依次保存数据</li></ol><div class=\"highlight\"><pre><code class=\"language-text\">import tensorflow as tf\n\nfiles = [&#39;a.bin&#39;]\n\nfilename_queue = tf.train.string_input_producer(files, num_epochs=1)\nreader = tf.WholeFileReader()\n_, value = reader.read(filename_queue)\nvalue = tf.decode_raw(value, tf.float32)\n\nsv = tf.train.Supervisor()\nwith sv.managed_session() as sess:\n    while True:\n        try:\n            features = sess.run(value)\n            print(features)\n        finally:\n            pass</code></pre></div><p>二进制文件读取</p><ol><li>tf.train.Supervisor()：必须要用这个创建sess。用tf.Session()要启动队列什么的，比较麻烦</li><li>tf.decode_raw(value, tf.float32)：要与你存储时候的是一致的</li><li>读取的时候，它不会保留你的形状，只会依次读取。比如形状是（3,2）存储，读出来是（6，）</li><li>tf.WholeFileReader()：是直接将你给定的文件全部读进来</li><li>tf还有一种读法：tf.FixedLengthRecordReader(record_bytes)</li></ol><blockquote>①当读的文件的字节长度小于给定的长度时，直接跳过该文件<br/>②当文件的长度大于给定的字节时，它只会从开始读读到指定的长度，其余的它是不读的。<b>总得来说，它并没有我们想的那么智能</b></blockquote><div class=\"highlight\"><pre><code class=\"language-text\">import numpy as np\nimport librosa\n\na = np.array([[1,2,3],[4,5,6],[7,8,9],[11,12,13]]).astype(&#39;float32&#39;)\nc= np.array([[70,80,90]]).astype(&#39;float32&#39;)\n\nwith open(&#39;./etc_t/SF1/a.bin&#39;, &#39;wb&#39;) as fp:\n    fp.write(a.tostring())\n\nwith open(&#39;./etc_t/SF1/c.bin&#39;, &#39;wb&#39;) as fp:\n    fp.write(c.tostring())\n\nb = np.array([[10,20,30],[40,50,60]]).astype(&#39;float32&#39;)\nwith open(&#39;./etc_t/TM3/b.bin&#39;, &#39;wb&#39;) as fp:\n    fp.write(b.tostring())</code></pre></div><p>我创建了a,b,c三个文件。我读的固定长度是：2 * 3 * 4（4是一个字节的长度）</p><div class=\"highlight\"><pre><code class=\"language-text\">import tensorflow as tf\nbatch_size = 2\nrecord_bytes = batch_size * 3 * 4\nfiles_SF1 = tf.gfile.Glob(&#39;./etc_t/SF1/*.bin&#39;)\nfiles_TM3 = tf.gfile.Glob(&#39;./etc_t/TM3/*.bin&#39;)\n\nfilename_queue = tf.train.string_input_producer(files_SF1)\nreader = tf.FixedLengthRecordReader(record_bytes)\n_, value = reader.read(filename_queue)\nvalue = tf.decode_raw(value, tf.float32)\n\nfilename_queue2 = tf.train.string_input_producer(files_TM3)\nreader = tf.FixedLengthRecordReader(record_bytes)\n_, value2 = reader.read(filename_queue2)\nvalue2 = tf.decode_raw(value2, tf.float32)\n\nsv = tf.train.Supervisor()\n\nwhile True:\n    with sv.managed_session() as sess:\n        v1R,v2R = sess.run([value,value2])\n        v1R = v1R.reshape(-1,3,1)\n        v2R = v2R.reshape(-1,3,1)\n        print(v1R)\n        print(v2R)\n输出：\n[[[ 1.]\n  [ 2.]\n  [ 3.]]\n\n [[ 4.]\n  [ 5.]\n  [ 6.]]]\n[[[ 10.]\n  [ 20.]\n  [ 30.]]\n\n [[ 40.]\n  [ 50.]\n  [ 60.]]]\n会发现，c文件直接是被忽略的，而a文件只会读取前6个数，即使它有12个数</code></pre></div><p>看来看去还是tf读取二进制文件最方便</p><p>欢迎关注公众号：huangxiaobai880</p><a class=\"video-box\" href=\"https://link.zhihu.com/?target=https%3A//www.zhihu.com/video/935479478189232128\" target=\"_blank\" data-video-id=\"\" data-video-playable=\"true\" data-name=\"\" data-poster=\"https://pic1.zhimg.com/80/v2-f29e629b0a4b7b05d4e2b4182d62c2fc_b.jpg\" data-lens-id=\"935479478189232128\"><img class=\"thumbnail\" src=\"https://pic1.zhimg.com/80/v2-f29e629b0a4b7b05d4e2b4182d62c2fc_b.jpg\"/><span class=\"content\"><span class=\"title\"><span class=\"z-ico-extern-gray\"></span><span class=\"z-ico-extern-blue\"></span></span><span class=\"url\"><span class=\"z-ico-video\"></span>https://www.zhihu.com/video/935479478189232128</span></span></a><p></p>", 
            "topic": [
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }, 
                {
                    "tag": "numpy", 
                    "tagLink": "https://api.zhihu.com/topics/19834165"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/32732057", 
            "userName": "小白", 
            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
            "upvote": 1, 
            "title": "TIMIT数据库使用", 
            "content": "<p>首先是介绍数据库结构的：</p><a href=\"https://link.zhihu.com/?target=http%3A//blog.csdn.net/jie8895010/article/details/52421901%3FlocationNum%3D4\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic2.zhimg.com/v2-7f0c746aaa368a21042d07f4c4c808e5_180x120.jpg\" data-image-width=\"632\" data-image-height=\"161\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">TIMIT数据库（一）：介绍 - CSDN博客</a><a href=\"https://link.zhihu.com/?target=http%3A//blog.csdn.net/jie8895010/article/details/52425784\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">TIMIT数据库（二）：文件目录及结构 - CSDN博客</a><p>其次是其wav文件是不可以直接读取的：</p><a href=\"https://link.zhihu.com/?target=http%3A//blog.csdn.net/wuxianfeng1987/article/details/75271252\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Timit SPHERE格式转换</a><p></p>", 
            "topic": [
                {
                    "tag": "数据", 
                    "tagLink": "https://api.zhihu.com/topics/19554449"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/32463512", 
            "userName": "小白", 
            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
            "upvote": 0, 
            "title": "python与numpy使用的一些小tips(目录)", 
            "content": "<p>一，图像处理</p><a href=\"https://zhuanlan.zhihu.com/p/32464309\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-ab2553b87657e132e4062a236d8bda9e_120x160.jpg\" data-image-width=\"3120\" data-image-height=\"4160\" class=\"internal\">将图片转化为灰度图片</a><a href=\"https://zhuanlan.zhihu.com/p/32144501\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-f27b23aa520771e454c5e979f762e1a6_180x120.jpg\" data-image-width=\"540\" data-image-height=\"260\" class=\"internal\">得到图片的边缘</a><a href=\"https://zhuanlan.zhihu.com/p/32144501\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-f27b23aa520771e454c5e979f762e1a6_180x120.jpg\" data-image-width=\"540\" data-image-height=\"260\" class=\"internal\">图像旋转</a><a href=\"https://zhuanlan.zhihu.com/p/32209457\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-86a1b253b10498b7f1dd76e180c98c44_120x160.jpg\" data-image-width=\"3120\" data-image-height=\"4160\" class=\"internal\">pyplot的imshow和cv2.imread的显示与读图的格式</a><p>二，声音处理</p><a href=\"https://zhuanlan.zhihu.com/p/31913377\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic2.zhimg.com/v2-7e65c9d0d3b01fcc23a886dc55c19cf1_180x120.jpg\" data-image-width=\"600\" data-image-height=\"384\" class=\"internal\">保存语音</a><a href=\"https://zhuanlan.zhihu.com/p/31913377\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic2.zhimg.com/v2-7e65c9d0d3b01fcc23a886dc55c19cf1_180x120.jpg\" data-image-width=\"600\" data-image-height=\"384\" class=\"internal\">对语音进行滤波</a><a href=\"https://zhuanlan.zhihu.com/p/32292150\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-aa9a332b5e45c0428e4da19b04ce8994_120x160.jpg\" data-image-width=\"3120\" data-image-height=\"4160\" class=\"internal\">推荐一个很不错的语音库librosa</a><a href=\"https://zhuanlan.zhihu.com/p/32292150\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-aa9a332b5e45c0428e4da19b04ce8994_120x160.jpg\" data-image-width=\"3120\" data-image-height=\"4160\" class=\"internal\">加载语音</a><a href=\"https://zhuanlan.zhihu.com/p/32292150\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-aa9a332b5e45c0428e4da19b04ce8994_120x160.jpg\" data-image-width=\"3120\" data-image-height=\"4160\" class=\"internal\">短时傅里叶变换</a><a href=\"https://zhuanlan.zhihu.com/p/32292150\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-aa9a332b5e45c0428e4da19b04ce8994_120x160.jpg\" data-image-width=\"3120\" data-image-height=\"4160\" class=\"internal\">短时傅里叶反变换</a><p>三，keras</p><a href=\"https://zhuanlan.zhihu.com/p/32209457\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-86a1b253b10498b7f1dd76e180c98c44_120x160.jpg\" data-image-width=\"3120\" data-image-height=\"4160\" class=\"internal\">keras返回loss形式的讨论</a><p>四，其它</p><a href=\"https://zhuanlan.zhihu.com/p/31907231\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-ef677efb016dab7ad82f7710209a7586_180x120.jpg\" data-image-width=\"600\" data-image-height=\"397\" class=\"internal\">matplotlib的plot使用</a><a href=\"https://zhuanlan.zhihu.com/p/31907231\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-ef677efb016dab7ad82f7710209a7586_180x120.jpg\" data-image-width=\"600\" data-image-height=\"397\" class=\"internal\">python的len()函数</a><a href=\"https://zhuanlan.zhihu.com/p/31907231\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-ef677efb016dab7ad82f7710209a7586_180x120.jpg\" data-image-width=\"600\" data-image-height=\"397\" class=\"internal\">numpy取整函数</a><a href=\"https://zhuanlan.zhihu.com/p/31913377\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic2.zhimg.com/v2-7e65c9d0d3b01fcc23a886dc55c19cf1_180x120.jpg\" data-image-width=\"600\" data-image-height=\"384\" class=\"internal\">相对路径</a><a href=\"https://zhuanlan.zhihu.com/p/32141245\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-ee0ba175473f535736e15298a765d5be_180x120.jpg\" data-image-width=\"900\" data-image-height=\"596\" class=\"internal\">python的数据类型与运算</a><a href=\"https://zhuanlan.zhihu.com/p/32141245\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-ee0ba175473f535736e15298a765d5be_180x120.jpg\" data-image-width=\"900\" data-image-height=\"596\" class=\"internal\">numpy的数据类型与运算</a><a href=\"https://zhuanlan.zhihu.com/p/32144501\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-f27b23aa520771e454c5e979f762e1a6_180x120.jpg\" data-image-width=\"540\" data-image-height=\"260\" class=\"internal\">numpy中数据类型的存储格式与维度调换</a><a href=\"https://zhuanlan.zhihu.com/p/32209457\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-86a1b253b10498b7f1dd76e180c98c44_120x160.jpg\" data-image-width=\"3120\" data-image-height=\"4160\" class=\"internal\">plt的一些参数设置的讨论</a><a href=\"https://zhuanlan.zhihu.com/p/32292150\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-aa9a332b5e45c0428e4da19b04ce8994_120x160.jpg\" data-image-width=\"3120\" data-image-height=\"4160\" class=\"internal\">如何取数据的实部与虚部</a><a href=\"https://zhuanlan.zhihu.com/p/32292150\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-aa9a332b5e45c0428e4da19b04ce8994_120x160.jpg\" data-image-width=\"3120\" data-image-height=\"4160\" class=\"internal\">如何把实部与虚部再次拼接在一起</a><p>欢迎关注公众号：huangxiaobai880</p><a class=\"video-box\" href=\"https://link.zhihu.com/?target=https%3A//www.zhihu.com/video/930095134524084224\" target=\"_blank\" data-video-id=\"\" data-video-playable=\"true\" data-name=\"\" data-poster=\"https://pic1.zhimg.com/80/v2-f29e629b0a4b7b05d4e2b4182d62c2fc_b.jpg\" data-lens-id=\"930095134524084224\"><img class=\"thumbnail\" src=\"https://pic1.zhimg.com/80/v2-f29e629b0a4b7b05d4e2b4182d62c2fc_b.jpg\"/><span class=\"content\"><span class=\"title\"><span class=\"z-ico-extern-gray\"></span><span class=\"z-ico-extern-blue\"></span></span><span class=\"url\"><span class=\"z-ico-video\"></span>https://www.zhihu.com/video/930095134524084224</span></span></a><p></p>", 
            "topic": [
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }, 
                {
                    "tag": "numpy", 
                    "tagLink": "https://api.zhihu.com/topics/19834165"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/32292150", 
            "userName": "小白", 
            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
            "upvote": 1, 
            "title": "python与numpy使用的一些小tips(6)", 
            "content": "<p></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-fe83386c18b9e1408cdc6fc2d7ea194d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb\" width=\"450\" data-original=\"https://pic2.zhimg.com/v2-fe83386c18b9e1408cdc6fc2d7ea194d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;450&#39; height=&#39;299&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"450\" data-original=\"https://pic2.zhimg.com/v2-fe83386c18b9e1408cdc6fc2d7ea194d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-fe83386c18b9e1408cdc6fc2d7ea194d_b.jpg\"/></figure><p><b><i>1，推荐一个很不错的语音库librosa</i></b></p><p><a href=\"https://link.zhihu.com/?target=http%3A//librosa.github.io/librosa/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">librosa官网</a>（虽然登不上，但翻墙可以）</p><p><b><i>2，加载语音</i></b></p><p><a href=\"https://link.zhihu.com/?target=http%3A//librosa.github.io/librosa/generated/librosa.core.load.html%23librosa.core.load\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">librosa.core.load - librosa 0.5.1 documentation</a></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-b6676cddd448fdc85aac56b889b15d8a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"754\" data-rawheight=\"126\" class=\"origin_image zh-lightbox-thumb\" width=\"754\" data-original=\"https://pic3.zhimg.com/v2-b6676cddd448fdc85aac56b889b15d8a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;754&#39; height=&#39;126&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"754\" data-rawheight=\"126\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"754\" data-original=\"https://pic3.zhimg.com/v2-b6676cddd448fdc85aac56b889b15d8a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-b6676cddd448fdc85aac56b889b15d8a_b.jpg\"/></figure><p>需要注意的是，它是会改变声音的采样频率的</p><ol><li>sr:设置采样pl</li><li>mono:是否转化为单声道</li></ol><div class=\"highlight\"><pre><code class=\"language-text\">y, sr = librosa.load(&#39;./arctic_a0003.wav&#39;,sr = 16000)\nprint(sr)\nprint(len(y))\ny, sr = librosa.load(&#39;./arctic_a0003.wav&#39;)\nprint(sr)\nprint(len(y))\n输出：\n16000\n53841\n22050\n74200</code></pre></div><p><b><i>3，短时傅里叶变换</i></b></p><p><b><i>【</i></b><a href=\"https://link.zhihu.com/?target=http%3A//librosa.github.io/librosa/generated/librosa.core.stft.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">librosa.core.stft - librosa 0.5.1 documentation</a><b><i>】</i></b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-1e40c9ba059273507dcb1964da711dfc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"745\" data-rawheight=\"166\" class=\"origin_image zh-lightbox-thumb\" width=\"745\" data-original=\"https://pic1.zhimg.com/v2-1e40c9ba059273507dcb1964da711dfc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;745&#39; height=&#39;166&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"745\" data-rawheight=\"166\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"745\" data-original=\"https://pic1.zhimg.com/v2-1e40c9ba059273507dcb1964da711dfc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-1e40c9ba059273507dcb1964da711dfc_b.jpg\"/></figure><ol><li>n_fft:一次变换的点数（如果比窗口小会用0补齐）</li><li>hop_length:每一次移动多长，默认是窗口的1/4</li><li>win_len:窗口长度</li><li>center:当n_fft需要补齐的时候补齐的方式</li></ol><p><b><i>4，短时傅里叶反变换</i></b></p><p>【<a href=\"https://link.zhihu.com/?target=http%3A//librosa.github.io/librosa/generated/librosa.core.istft.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">librosa.core.istft - librosa 0.5.1 documentation</a>】</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-3067d924ae30dd4d3f0b990c6da14ce6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"729\" data-rawheight=\"136\" class=\"origin_image zh-lightbox-thumb\" width=\"729\" data-original=\"https://pic3.zhimg.com/v2-3067d924ae30dd4d3f0b990c6da14ce6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;729&#39; height=&#39;136&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"729\" data-rawheight=\"136\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"729\" data-original=\"https://pic3.zhimg.com/v2-3067d924ae30dd4d3f0b990c6da14ce6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-3067d924ae30dd4d3f0b990c6da14ce6_b.jpg\"/></figure><p>只需要将stft的那个矩阵原封不动的传入进去就可以恢复信号</p><p>需要说明的是：重建信号的质量和n_fft的长度无关【因为我试过不同长度的n_fft,听上去没任何差别】</p><p><b><i>5，如何取数据的实部与虚部</i></b></p><p><b><i>【</i></b><a href=\"https://link.zhihu.com/?target=https%3A//jingyan.baidu.com/article/0320e2c128d52f1b87507b20.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">采用Python语言如何建立复数并访问虚部实部和模</a><b><i>】</i></b></p><p><b><i>6，如何把实部与虚部再次拼接在一起</i></b></p><div class=\"highlight\"><pre><code class=\"language-text\">y, sr = librosa.load(&#39;./arctic_a0003.wav&#39;,sr = 16000)\nD = librosa.stft(y,n_fft=2048)\nE = D.real + (D.imag)*1j\ny_hat = librosa.istft(E)\nlibrosa.output.write_wav(&#39;3.wav&#39;,y_hat,16000)</code></pre></div><p>这样是可以无失真的复原信号的</p><p>欢迎关注公众号：huangxiaobai880</p><a class=\"video-box\" href=\"https://link.zhihu.com/?target=https%3A//www.zhihu.com/video/928210458257805312\" target=\"_blank\" data-video-id=\"\" data-video-playable=\"true\" data-name=\"\" data-poster=\"https://pic1.zhimg.com/80/v2-f29e629b0a4b7b05d4e2b4182d62c2fc_b.jpg\" data-lens-id=\"928210458257805312\"><img class=\"thumbnail\" src=\"https://pic1.zhimg.com/80/v2-f29e629b0a4b7b05d4e2b4182d62c2fc_b.jpg\"/><span class=\"content\"><span class=\"title\"><span class=\"z-ico-extern-gray\"></span><span class=\"z-ico-extern-blue\"></span></span><span class=\"url\"><span class=\"z-ico-video\"></span>https://www.zhihu.com/video/928210458257805312</span></span></a><p></p>", 
            "topic": [
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }, 
                {
                    "tag": "numpy", 
                    "tagLink": "https://api.zhihu.com/topics/19834165"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/32209457", 
            "userName": "小白", 
            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
            "upvote": 1, 
            "title": "python与numpy使用的一些小tips(5)", 
            "content": "<p></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-094ff54a1e27799b7b4024bac57a9786_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb\" width=\"450\" data-original=\"https://pic3.zhimg.com/v2-094ff54a1e27799b7b4024bac57a9786_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;450&#39; height=&#39;299&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"450\" data-original=\"https://pic3.zhimg.com/v2-094ff54a1e27799b7b4024bac57a9786_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-094ff54a1e27799b7b4024bac57a9786_b.jpg\"/></figure><p><b><i>1，keras返回loss形式的讨论</i></b></p><p>情况1：同一个网络对同时对两个输入产生两个输出</p><p>返回形式是【a,b,c】(其中a=b+c)</p><p>情况2：同一个网络单输入单输出</p><p>返回形式是【a】</p><p>情况3：两个网络每个网络有两个输出</p><p>返回形式是【a,b,c,d,e】(其中a=b+c+d+e)</p><p>总结起来：几个输出，几个loss+1.第一个是和</p><p><b><i>2, pyplot的imshow和cv2.imread的显示与读图的格式</i></b></p><p><i>pyplot官网截图【</i><a href=\"https://link.zhihu.com/?target=https%3A//matplotlib.org/api/_as_gen/matplotlib.pyplot.imshow.html%3Fhighlight%3Dimshow%23matplotlib.pyplot.imshow\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">matplotlib.pyplot.imshow - Matplotlib 2.1.1 documentation</a><i>】</i></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-b21d1b4e3c9d8af617bda73bfd53b86f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"955\" data-rawheight=\"302\" class=\"origin_image zh-lightbox-thumb\" width=\"955\" data-original=\"https://pic4.zhimg.com/v2-b21d1b4e3c9d8af617bda73bfd53b86f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;955&#39; height=&#39;302&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"955\" data-rawheight=\"302\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"955\" data-original=\"https://pic4.zhimg.com/v2-b21d1b4e3c9d8af617bda73bfd53b86f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-b21d1b4e3c9d8af617bda73bfd53b86f_b.jpg\"/></figure><p>它是RGB显示的</p><p>cv2.imread官网截图【<a href=\"https://link.zhihu.com/?target=https%3A//docs.opencv.org/3.3.1/d4/da8/group__imgcodecs.html%23ga288b8b3da0892bd651fce07b3bbd3a56\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Image file reading and writing</a>】</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-9328b705e607bda1628e42ca848c96c6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"917\" data-rawheight=\"609\" class=\"origin_image zh-lightbox-thumb\" width=\"917\" data-original=\"https://pic3.zhimg.com/v2-9328b705e607bda1628e42ca848c96c6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;917&#39; height=&#39;609&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"917\" data-rawheight=\"609\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"917\" data-original=\"https://pic3.zhimg.com/v2-9328b705e607bda1628e42ca848c96c6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-9328b705e607bda1628e42ca848c96c6_b.jpg\"/></figure><p>cv2.imread的读图是BGR</p><div class=\"highlight\"><pre><code class=\"language-text\">import matplotlib.pylab as plt\nimg = cv2.imread(&#39;1.png&#39;)\nplt.imshow(img[:,:,::-1])\nplt.show()</code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-4d84ef78d13b1e2a3a1091f7fbb9d34a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"645\" data-rawheight=\"567\" class=\"origin_image zh-lightbox-thumb\" width=\"645\" data-original=\"https://pic3.zhimg.com/v2-4d84ef78d13b1e2a3a1091f7fbb9d34a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;645&#39; height=&#39;567&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"645\" data-rawheight=\"567\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"645\" data-original=\"https://pic3.zhimg.com/v2-4d84ef78d13b1e2a3a1091f7fbb9d34a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-4d84ef78d13b1e2a3a1091f7fbb9d34a_b.jpg\"/></figure><div class=\"highlight\"><pre><code class=\"language-text\">import matplotlib.pylab as plt\n\nimg = cv2.imread(&#39;1.png&#39;)\nplt.imshow(img)\nplt.show()</code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-76f1e1bad968a676f2b4d889b048227c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"635\" data-rawheight=\"558\" class=\"origin_image zh-lightbox-thumb\" width=\"635\" data-original=\"https://pic1.zhimg.com/v2-76f1e1bad968a676f2b4d889b048227c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;635&#39; height=&#39;558&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"635\" data-rawheight=\"558\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"635\" data-original=\"https://pic1.zhimg.com/v2-76f1e1bad968a676f2b4d889b048227c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-76f1e1bad968a676f2b4d889b048227c_b.jpg\"/></figure><p><b><i>3, plt的一些参数设置的讨论</i></b></p><p>①plt.axis(&#34;off&#34;)：关闭坐标轴</p><div class=\"highlight\"><pre><code class=\"language-text\">import matplotlib.pylab as plt\n\nimg = cv2.imread(&#39;1.png&#39;)\nplt.imshow(img[:,:,::-1])\nplt.axis(&#34;off&#34;)\nplt.show()\n</code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-1423afe81ed7a70d9fb8198e14f9bc63_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"637\" data-rawheight=\"560\" class=\"origin_image zh-lightbox-thumb\" width=\"637\" data-original=\"https://pic4.zhimg.com/v2-1423afe81ed7a70d9fb8198e14f9bc63_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;637&#39; height=&#39;560&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"637\" data-rawheight=\"560\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"637\" data-original=\"https://pic4.zhimg.com/v2-1423afe81ed7a70d9fb8198e14f9bc63_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-1423afe81ed7a70d9fb8198e14f9bc63_b.jpg\"/></figure><p>①plt.clf()：清空坐标轴里面的数据</p><div class=\"highlight\"><pre><code class=\"language-text\">import matplotlib.pylab as plt\nimg = cv2.imread(&#39;1.png&#39;)\nplt.imshow(img[:,:,::-1])\nplt.axis(&#34;off&#34;)\nplt.clf()\nplt.show()</code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-5e3df13201956aa3c82fbf54471c0dc3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"637\" data-rawheight=\"560\" class=\"origin_image zh-lightbox-thumb\" width=\"637\" data-original=\"https://pic4.zhimg.com/v2-5e3df13201956aa3c82fbf54471c0dc3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;637&#39; height=&#39;560&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"637\" data-rawheight=\"560\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"637\" data-original=\"https://pic4.zhimg.com/v2-5e3df13201956aa3c82fbf54471c0dc3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-5e3df13201956aa3c82fbf54471c0dc3_b.jpg\"/></figure><p>③plt.close()：关闭坐标轴</p><div class=\"highlight\"><pre><code class=\"language-text\">import matplotlib.pylab as plt\n\nimg = cv2.imread(&#39;1.png&#39;)\nplt.imshow(img[:,:,::-1])\nplt.axis(&#34;off&#34;)\nplt.clf()\nplt.close()\nplt.show()\n输出：\n没有显示</code></pre></div><p>④plt.savefig():保存坐标，即使是空也会保存一个空图片。它会保存这个坐标轴上画的所有东西</p><div class=\"highlight\"><pre><code class=\"language-text\">import matplotlib.pylab as plt\nplt.savefig(&#34;figures/current_batch_%s.png&#34; % &#39;hhhh&#39;)</code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-cfad7c776b7657faf952f5dcf384f102_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"710\" data-rawheight=\"477\" class=\"origin_image zh-lightbox-thumb\" width=\"710\" data-original=\"https://pic3.zhimg.com/v2-cfad7c776b7657faf952f5dcf384f102_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;710&#39; height=&#39;477&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"710\" data-rawheight=\"477\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"710\" data-original=\"https://pic3.zhimg.com/v2-cfad7c776b7657faf952f5dcf384f102_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-cfad7c776b7657faf952f5dcf384f102_b.jpg\"/></figure><p>欢迎关注公众号：huangxiaobai880</p><a class=\"video-box\" href=\"https://link.zhihu.com/?target=https%3A//www.zhihu.com/video/927204188407091200\" target=\"_blank\" data-video-id=\"\" data-video-playable=\"true\" data-name=\"\" data-poster=\"https://pic1.zhimg.com/80/v2-f29e629b0a4b7b05d4e2b4182d62c2fc_b.jpg\" data-lens-id=\"927204188407091200\"><img class=\"thumbnail\" src=\"https://pic1.zhimg.com/80/v2-f29e629b0a4b7b05d4e2b4182d62c2fc_b.jpg\"/><span class=\"content\"><span class=\"title\"><span class=\"z-ico-extern-gray\"></span><span class=\"z-ico-extern-blue\"></span></span><span class=\"url\"><span class=\"z-ico-video\"></span>https://www.zhihu.com/video/927204188407091200</span></span></a><p></p>", 
            "topic": [
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }, 
                {
                    "tag": "numpy", 
                    "tagLink": "https://api.zhihu.com/topics/19834165"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/32144501", 
            "userName": "小白", 
            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
            "upvote": 0, 
            "title": "python与numpy使用的一些小tips(4)", 
            "content": "<p></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-f478e77d344dd6994cfa84a122902290_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb\" width=\"450\" data-original=\"https://pic1.zhimg.com/v2-f478e77d344dd6994cfa84a122902290_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;450&#39; height=&#39;299&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"450\" data-original=\"https://pic1.zhimg.com/v2-f478e77d344dd6994cfa84a122902290_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-f478e77d344dd6994cfa84a122902290_b.jpg\"/></figure><p><b><i>1，numpy中数据类型的存储格式与维度调换</i></b></p><div class=\"highlight\"><pre><code class=\"language-text\">a = np.random.randint(0,255,(3,1,2))\nprint(a)\nprint(a.shape)\nprint(&#39;***********************************&#39;)\nprint(np.expand_dims(a, 3))\nprint(np.expand_dims(a, 3).shape)\nprint(&#39;***********************************&#39;)\nprint(np.transpose(a, [1, 2, 0]))\nprint(np.transpose(a, [1, 2, 0]).shape)</code></pre></div><p>输出：</p><div class=\"highlight\"><pre><code class=\"language-text\">[[[242 179]]\n\n [[ 29 144]]\n\n [[ 67  42]]]\n(3, 1, 2)\n当shape有三个维度时，其分别表示：batch_size,行，列\n***********************************\n[[[[242]\n   [179]]]\n\n\n [[[ 29]\n   [144]]]\n\n\n [[[ 67]\n   [ 42]]]]\n(3, 1, 2, 1)\n当shape有四个维度时，其分别表示：batch_size,深度，行，列\n***********************************\n[[[242  29  67]\n  [179 144  42]]]\n(1, 2, 3)\n其表示：batch_size为1,2行，3列。它会把原来batch上的数据变成深度</code></pre></div><p><b><i>2，得到图片的边缘</i></b></p><div class=\"highlight\"><pre><code class=\"language-text\">import cv2\nimg = cv2.imread(&#34;1.png&#34;, 0)\nimg = cv2.GaussianBlur(img, (3, 3), 0)\ncanny = cv2.Canny(img, 50, 150)\ncanny = 255 - canny\ncv2.imshow(&#39;Canny&#39;, canny)\ncv2.waitKey(0)\ncv2.destroyAllWindows()</code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-45d0bed811f35f53d8dc4ad2083de636_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1221\" data-rawheight=\"614\" class=\"origin_image zh-lightbox-thumb\" width=\"1221\" data-original=\"https://pic3.zhimg.com/v2-45d0bed811f35f53d8dc4ad2083de636_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1221&#39; height=&#39;614&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1221\" data-rawheight=\"614\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1221\" data-original=\"https://pic3.zhimg.com/v2-45d0bed811f35f53d8dc4ad2083de636_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-45d0bed811f35f53d8dc4ad2083de636_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-a4f22c113adbe795732e31abac7217eb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1064\" data-rawheight=\"576\" class=\"origin_image zh-lightbox-thumb\" width=\"1064\" data-original=\"https://pic4.zhimg.com/v2-a4f22c113adbe795732e31abac7217eb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1064&#39; height=&#39;576&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1064\" data-rawheight=\"576\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1064\" data-original=\"https://pic4.zhimg.com/v2-a4f22c113adbe795732e31abac7217eb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-a4f22c113adbe795732e31abac7217eb_b.jpg\"/></figure><hr/><p><b><i>3，图像旋转</i></b></p><div class=\"highlight\"><pre><code class=\"language-text\">import cv2\nimport numpy as np\n\nimg = cv2.imread(&#39;1.png&#39;, 1)\nrows, cols, channel = img.shape\n\nM = cv2.getRotationMatrix2D((cols / 2, rows / 3), 90, 0.4)\ndst = cv2.warpAffine(img, M, (cols, rows))\n\ncv2.imshow(&#39;img&#39;, dst)\ncv2.waitKey(0)\ncv2.destroyAllWindows()</code></pre></div><ol><li>cv2.getRotationMatrix2D：获得变换矩阵。第一参数指定旋转圆点；第二个参数指定旋转角度；第二个参数指定缩放比例</li><li>cv2.warpAffine：将仿射变换应用于图像</li></ol><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-b24592c964fe88c7d31e15871c1d5652_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1222\" data-rawheight=\"619\" class=\"origin_image zh-lightbox-thumb\" width=\"1222\" data-original=\"https://pic3.zhimg.com/v2-b24592c964fe88c7d31e15871c1d5652_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1222&#39; height=&#39;619&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1222\" data-rawheight=\"619\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1222\" data-original=\"https://pic3.zhimg.com/v2-b24592c964fe88c7d31e15871c1d5652_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-b24592c964fe88c7d31e15871c1d5652_b.jpg\"/></figure><p>另一种方法：</p><div class=\"highlight\"><pre><code class=\"language-text\">img = Image.fromarray(img)\nimg = img.convert(&#39;RGBA&#39;)\nimg = img.rotate(40,expand=1)\np = Image.new(&#39;RGBA&#39;, img.size,(255,)*4)\nout = Image.composite(img, p, img)\nimg = np.array(out)\ncv2.imshow(&#39;img&#39;,img)\ncv2.waitKey()</code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-dc73f2f3e8cac9b33b7e7df218e043b6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"156\" data-rawheight=\"156\" class=\"content_image\" width=\"156\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;156&#39; height=&#39;156&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"156\" data-rawheight=\"156\" class=\"content_image lazy\" width=\"156\" data-actualsrc=\"https://pic3.zhimg.com/v2-dc73f2f3e8cac9b33b7e7df218e043b6_b.jpg\"/></figure><p>欢迎关注公众号：huangxiaobai880</p><a class=\"video-box\" href=\"https://link.zhihu.com/?target=https%3A//www.zhihu.com/video/926478859589779456\" target=\"_blank\" data-video-id=\"\" data-video-playable=\"true\" data-name=\"\" data-poster=\"https://pic1.zhimg.com/80/v2-f29e629b0a4b7b05d4e2b4182d62c2fc_b.jpg\" data-lens-id=\"926478859589779456\"><img class=\"thumbnail\" src=\"https://pic1.zhimg.com/80/v2-f29e629b0a4b7b05d4e2b4182d62c2fc_b.jpg\"/><span class=\"content\"><span class=\"title\"><span class=\"z-ico-extern-gray\"></span><span class=\"z-ico-extern-blue\"></span></span><span class=\"url\"><span class=\"z-ico-video\"></span>https://www.zhihu.com/video/926478859589779456</span></span></a><p></p>", 
            "topic": [
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }, 
                {
                    "tag": "numpy", 
                    "tagLink": "https://api.zhihu.com/topics/19834165"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/32141245", 
            "userName": "小白", 
            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
            "upvote": 0, 
            "title": "python与numpy使用的一些小tips(3)", 
            "content": "<p></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-0f68acca9c1a531b74bc048ddf22866e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb\" width=\"450\" data-original=\"https://pic3.zhimg.com/v2-0f68acca9c1a531b74bc048ddf22866e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;450&#39; height=&#39;299&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"450\" data-original=\"https://pic3.zhimg.com/v2-0f68acca9c1a531b74bc048ddf22866e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-0f68acca9c1a531b74bc048ddf22866e_b.jpg\"/></figure><p><b><i>1，python的数据类型与运算</i></b></p><p>我们主要讨论的是python与numpy的<b><i>[]</i></b>的不同</p><p><b><i>python中[]表示的是列表，而不是矩阵！！！非常重要！！！</i></b>其+号用于组合列表，*号用于重复列表。它是没有-和/操作的</p><p>除</p><div class=\"highlight\"><pre><code class=\"language-text\">t = [2]\na = [1]\nprint(t/a)\n输出：\nTraceback (most recent call last):\n  File &#34;D:/huang/pix2pix_keras/t.py&#34;, line 14, in &lt;module&gt;\n    print(t/a)\nTypeError: unsupported operand type(s) for /: &#39;list&#39; and &#39;list&#39;</code></pre></div><p>减法</p><div class=\"highlight\"><pre><code class=\"language-text\">t = [2]\na = [1]\nprint(t-a)\n输出：\nTraceback (most recent call last):\n  File &#34;D:/huang/pix2pix_keras/t.py&#34;, line 14, in &lt;module&gt;\n    print(t-a)\nTypeError: unsupported operand type(s) for -: &#39;list&#39; and &#39;list&#39;</code></pre></div><p>加法</p><div class=\"highlight\"><pre><code class=\"language-text\">t = [2]\na = [1]\nprint(t+a)\n输出：\n[2, 1]</code></pre></div><p>乘法</p><div class=\"highlight\"><pre><code class=\"language-text\">t = [2]\nprint(t*8)\n输出：\n[2, 2, 2, 2, 2, 2, 2, 2]</code></pre></div><p><b><i>那么把列表强制转换为int型运算可以吗？答案是同样不可以！！！原因是根本不可以转换</i></b></p><div class=\"highlight\"><pre><code class=\"language-text\">t = int([2])\na = int([1])\nprint(t*a)\n输出：\nTraceback (most recent call last):\n  File &#34;D:/huang/pix2pix_keras/t.py&#34;, line 12, in &lt;module&gt;\n    t = int([2])\nTypeError: int() argument must be a string, a bytes-like object or a number, not &#39;list&#39;</code></pre></div><p><b><i>那我们就是想让列表的元素进行加减乘除呢？答案是先将其转换到numpy数据类型在进行相应的运算！</i></b></p><div class=\"highlight\"><pre><code class=\"language-text\">t = np.array([2])\na = np.array([1])\nprint(t*a)\n输出：\n[2]</code></pre></div><p>python其它数据类型出门左拐【<a href=\"https://link.zhihu.com/?target=https%3A//www.cnblogs.com/linjiqin/p/3608541.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">python数据类型详解 - Ruthless - 博客园</a>】</p><p><i><b>2，numpy的数据类型与运算</b></i></p><p>我们主要讨论的是numpy的减法，<b><i>numpy竟然支持不同shape的数据减法！！！【？但是在它的官网上找了半天，也没有找到它减法的官方说明】它是用广播机制进行不同shape的减法的。两个还不错的参考【</i></b><a href=\"https://link.zhihu.com/?target=http%3A//blog.csdn.net/tender_night/article/details/52576424\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">numpy中的广播（broadcast） - CSDN博客</a><b><i>】【</i></b><a href=\"https://link.zhihu.com/?target=http%3A//blog.csdn.net/qq_18433441/article/details/56834207\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">operands could not be broadcast together with shapes</a><b><i>】总结下来是这样的：</i></b></p><ol><li>让所有输入数组都向其中shape最长的数组看齐，shape中不足的部分都通过在<b><i>前面</i></b>加1补齐</li><li>Numpy从最后开始往前逐个比较它们的维度（dimensions）大小。比较过程中，如果<b><i>两者的对应维度相同，或者其中之一（或者全是）等于1</i></b>，比较继续进行直到最前面的维度。否则，你将看到ValueError错误出现（如，&#34;operands could not be broadcast together with shapes ...&#34;）。</li><li>当输入数组的某个轴的长度为1时，将此轴复制到与另一个array相同</li></ol><div class=\"highlight\"><pre><code class=\"language-text\">t = np.array([2,1,3])\na = np.array([1])\nprint(t-a)\n输出：\n[1 0 2]</code></pre></div><ol><li>t,a：shape长度相同不需要补1</li><li>a的维度是1，符合2</li><li>复制a与t相同a变为[1,1,1]</li></ol><div class=\"highlight\"><pre><code class=\"language-text\">t = np.array([2,1,3])\na = np.array([1,2])\nprint(t-a)\n输出：\nTraceback (most recent call last):\n  File &#34;D:/huang/pix2pix_keras/t.py&#34;, line 14, in &lt;module&gt;\n    print(t-a)\nValueError: operands could not be broadcast together with shapes (3,) (2,) </code></pre></div><ol><li>t,a：shape长度相同不需要补1</li><li>a的shape[0]是2，而t的shape[0]是3。不符合2报错</li></ol><p>欢迎关注公众号：huangxiaobai880</p><a class=\"video-box\" href=\"https://link.zhihu.com/?target=https%3A//www.zhihu.com/video/926392235866619904\" target=\"_blank\" data-video-id=\"\" data-video-playable=\"true\" data-name=\"\" data-poster=\"https://pic1.zhimg.com/80/v2-3dcae07b5cf612f52f0a42eebf245e60_b.jpg\" data-lens-id=\"926392235866619904\"><img class=\"thumbnail\" src=\"https://pic1.zhimg.com/80/v2-3dcae07b5cf612f52f0a42eebf245e60_b.jpg\"/><span class=\"content\"><span class=\"title\"><span class=\"z-ico-extern-gray\"></span><span class=\"z-ico-extern-blue\"></span></span><span class=\"url\"><span class=\"z-ico-video\"></span>https://www.zhihu.com/video/926392235866619904</span></span></a><p></p>", 
            "topic": [
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }, 
                {
                    "tag": "numpy", 
                    "tagLink": "https://api.zhihu.com/topics/19834165"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/32120049", 
            "userName": "小白", 
            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
            "upvote": 7, 
            "title": "手把手教你解决win10开机长时间黑屏", 
            "content": "<p></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-e4646f4a55aced0c3eb49719eca7fcac_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb\" width=\"450\" data-original=\"https://pic1.zhimg.com/v2-e4646f4a55aced0c3eb49719eca7fcac_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;450&#39; height=&#39;299&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"450\" data-original=\"https://pic1.zhimg.com/v2-e4646f4a55aced0c3eb49719eca7fcac_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-e4646f4a55aced0c3eb49719eca7fcac_b.jpg\"/></figure><p>操作演示：</p><p>step1:点击属性</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-4e0e7dd22f91ed99390efeb28ffbd080_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"737\" data-rawheight=\"461\" class=\"origin_image zh-lightbox-thumb\" width=\"737\" data-original=\"https://pic1.zhimg.com/v2-4e0e7dd22f91ed99390efeb28ffbd080_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;737&#39; height=&#39;461&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"737\" data-rawheight=\"461\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"737\" data-original=\"https://pic1.zhimg.com/v2-4e0e7dd22f91ed99390efeb28ffbd080_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-4e0e7dd22f91ed99390efeb28ffbd080_b.jpg\"/></figure><p>step2:点击设备管理</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-c09725d1d175e4dc458d69986917fae9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"547\" data-rawheight=\"253\" class=\"origin_image zh-lightbox-thumb\" width=\"547\" data-original=\"https://pic2.zhimg.com/v2-c09725d1d175e4dc458d69986917fae9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;547&#39; height=&#39;253&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"547\" data-rawheight=\"253\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"547\" data-original=\"https://pic2.zhimg.com/v2-c09725d1d175e4dc458d69986917fae9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-c09725d1d175e4dc458d69986917fae9_b.jpg\"/></figure><p>step3:就是这个AMD显卡和intel的不兼容导致的</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-06f97844bbdca9a7a706cedf09091ff7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"412\" data-rawheight=\"253\" class=\"content_image\" width=\"412\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;412&#39; height=&#39;253&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"412\" data-rawheight=\"253\" class=\"content_image lazy\" width=\"412\" data-actualsrc=\"https://pic4.zhimg.com/v2-06f97844bbdca9a7a706cedf09091ff7_b.jpg\"/></figure><p>step4:右键AMD选项，选择更新驱动程序</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-550af28a2d6401785388b5919e143f38_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"327\" data-rawheight=\"79\" class=\"content_image\" width=\"327\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;327&#39; height=&#39;79&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"327\" data-rawheight=\"79\" class=\"content_image lazy\" width=\"327\" data-actualsrc=\"https://pic1.zhimg.com/v2-550af28a2d6401785388b5919e143f38_b.jpg\"/></figure><p>step5:选择第二个浏览我的选项</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-744b3ca5752fbbe6a51cb131f239c366_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"565\" data-rawheight=\"338\" class=\"origin_image zh-lightbox-thumb\" width=\"565\" data-original=\"https://pic3.zhimg.com/v2-744b3ca5752fbbe6a51cb131f239c366_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;565&#39; height=&#39;338&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"565\" data-rawheight=\"338\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"565\" data-original=\"https://pic3.zhimg.com/v2-744b3ca5752fbbe6a51cb131f239c366_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-744b3ca5752fbbe6a51cb131f239c366_b.jpg\"/></figure><p>step6:选择让我从计算机上选项</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-dff2b97486fc63ca57a0bfb87fd52bc6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"590\" data-rawheight=\"386\" class=\"origin_image zh-lightbox-thumb\" width=\"590\" data-original=\"https://pic3.zhimg.com/v2-dff2b97486fc63ca57a0bfb87fd52bc6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;590&#39; height=&#39;386&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"590\" data-rawheight=\"386\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"590\" data-original=\"https://pic3.zhimg.com/v2-dff2b97486fc63ca57a0bfb87fd52bc6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-dff2b97486fc63ca57a0bfb87fd52bc6_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-e6c98668a89d66dfd71fe7b21283b600_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"603\" data-rawheight=\"466\" class=\"origin_image zh-lightbox-thumb\" width=\"603\" data-original=\"https://pic1.zhimg.com/v2-e6c98668a89d66dfd71fe7b21283b600_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;603&#39; height=&#39;466&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"603\" data-rawheight=\"466\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"603\" data-original=\"https://pic1.zhimg.com/v2-e6c98668a89d66dfd71fe7b21283b600_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-e6c98668a89d66dfd71fe7b21283b600_b.jpg\"/></figure><p>step7:选择Microsoft选项</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-6f35698df440c2fd5045ad37dd1d50ca_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"608\" data-rawheight=\"468\" class=\"origin_image zh-lightbox-thumb\" width=\"608\" data-original=\"https://pic3.zhimg.com/v2-6f35698df440c2fd5045ad37dd1d50ca_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;608&#39; height=&#39;468&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"608\" data-rawheight=\"468\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"608\" data-original=\"https://pic3.zhimg.com/v2-6f35698df440c2fd5045ad37dd1d50ca_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-6f35698df440c2fd5045ad37dd1d50ca_b.jpg\"/></figure><p>点击下一步，搞定。</p><p>欢迎关注公众号：huangxiaobai880</p><a class=\"video-box\" href=\"https://link.zhihu.com/?target=https%3A//www.zhihu.com/video/926133920477618176\" target=\"_blank\" data-video-id=\"\" data-video-playable=\"true\" data-name=\"\" data-poster=\"https://pic1.zhimg.com/80/v2-09d744dc9d12502fe651ebdc02eecfa0_b.jpg\" data-lens-id=\"926133920477618176\"><img class=\"thumbnail\" src=\"https://pic1.zhimg.com/80/v2-09d744dc9d12502fe651ebdc02eecfa0_b.jpg\"/><span class=\"content\"><span class=\"title\"><span class=\"z-ico-extern-gray\"></span><span class=\"z-ico-extern-blue\"></span></span><span class=\"url\"><span class=\"z-ico-video\"></span>https://www.zhihu.com/video/926133920477618176</span></span></a><p></p>", 
            "topic": [
                {
                    "tag": "安装", 
                    "tagLink": "https://api.zhihu.com/topics/19608575"
                }
            ], 
            "comments": [
                {
                    "userName": "Kikujiro", 
                    "userLink": "https://www.zhihu.com/people/e46a67688e5a797a5b098813930e1356", 
                    "content": "题主我这黑屏  开都开不下来   安全模式也进不了    怎么进行你的一", 
                    "likes": 1, 
                    "childComments": []
                }, 
                {
                    "userName": "知乎用户", 
                    "userLink": "https://www.zhihu.com/people/0", 
                    "content": "<p>开机黑屏终于解决了</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "小白", 
                            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
                            "content": "我也是摸索了好长时间", 
                            "likes": 0, 
                            "replyToAuthor": "知乎用户"
                        }
                    ]
                }, 
                {
                    "userName": "嗯浪荡不羁", 
                    "userLink": "https://www.zhihu.com/people/a1fb2607a42c7a33ea30433cabf6c242", 
                    "content": "楼主是开机短暂黑屏的解决方案吗", 
                    "likes": 1, 
                    "childComments": []
                }, 
                {
                    "userName": "jiongjiong", 
                    "userLink": "https://www.zhihu.com/people/4cf5acfd5542b199cf866bb98434c38f", 
                    "content": "黑屏一会然后亮屏一秒又黑屏一会怎么办？", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "相臀师", 
                    "userLink": "https://www.zhihu.com/people/26dfdbf2f03b2216217fd561a3b14b21", 
                    "content": "<p>请问显卡是N卡也可以用类似的方法吗，每次开机都四分多钟，欢迎界面转圈转个三分五十几秒。</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "Shaw", 
                    "userLink": "https://www.zhihu.com/people/3d770a379d9a74b3ba70318b7b08ef85", 
                    "content": "确实解决了黑屏问题..可是这样就用不了独显只能用集显了..游戏都玩不了了..只能这样了吗...？有没有办法能用独显又不黑屏的.._(:_」∠)_", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/32034357", 
            "userName": "小白", 
            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
            "upvote": 8, 
            "title": "Python的h5文件的创建与读取", 
            "content": "<p>源码地址：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/tdeboissiere/DeepLearningImplementations/tree/master/pix2pix/src\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">pix2pix源码地址</a></p><p>论文地址：<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1611.07004\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Image-to-Image Translation with Conditional Adversarial Network</a></p><p>文章目录：<a href=\"https://zhuanlan.zhihu.com/p/32186417\" class=\"internal\">深度学习一行一行敲pix2pix网络-keras版(目录)</a></p><p>视频目录：<a href=\"https://zhuanlan.zhihu.com/p/32186753\" class=\"internal\">深度学习一行一行敲pix2pix网络-keras版(视频目录)</a></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-dce09c2e55f8be7bf2cca9e78e1b32f3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb\" width=\"450\" data-original=\"https://pic4.zhimg.com/v2-dce09c2e55f8be7bf2cca9e78e1b32f3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;450&#39; height=&#39;299&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"450\" data-original=\"https://pic4.zhimg.com/v2-dce09c2e55f8be7bf2cca9e78e1b32f3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-dce09c2e55f8be7bf2cca9e78e1b32f3_b.jpg\"/></figure><p>代码流程图：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-ec470d5eebc987d283e32f7e3e8f2942_b.jpg\" data-size=\"normal\" data-rawwidth=\"311\" data-rawheight=\"192\" class=\"content_image\" width=\"311\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;311&#39; height=&#39;192&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"311\" data-rawheight=\"192\" class=\"content_image lazy\" width=\"311\" data-actualsrc=\"https://pic3.zhimg.com/v2-ec470d5eebc987d283e32f7e3e8f2942_b.jpg\"/><figcaption>本章代码流程</figcaption></figure><p>这章是关于--make_dataset.py的</p><hr/><p>-------------------------------①------------------------------</p><p><b><i>step1:得到处理后的文件路径与名称</i></b></p><div class=\"highlight\"><pre><code class=\"language-text\">file_name = os.path.basename(jpeg_dir.rstrip(&#34;/&#34;))\nhdf5_file = os.path.join(data_dir, &#34;%s_data.h5&#34; % file_name)</code></pre></div><ol><li><b><i>rstrip() </i></b>：删除 string 字符串末尾的指定字符（默认为空格）【<a href=\"https://link.zhihu.com/?target=http%3A//www.runoob.com/python/att-string-rstrip.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Python rstrip()方法</a>】</li><li>os.path.basename：返回path最后的文件名【<a href=\"https://link.zhihu.com/?target=https%3A//www.cnblogs.com/wuxie1989/p/5623435.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">python os.path模块常用方法详解</a>】附录有该函数的解析</li><li>&#34;%s_data.h5&#34; % file_name:其实还可以用format:&#34;{}_data.h5&#34; .format(file_name)</li></ol><div class=\"highlight\"><pre><code class=\"language-text\">with h5py.File(hdf5_file, &#34;w&#34;) as hfw:</code></pre></div><p>创建一个h5文件，后面的参数说明了是只读还是写模式【<a href=\"https://link.zhihu.com/?target=http%3A//blog.csdn.net/yudf2010/article/details/50353292\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">python开源库--h5py快速指南 - CSDN博客</a>】</p><div class=\"highlight\"><pre><code class=\"language-text\">for dset_type in [&#34;train&#34;, &#34;test&#34;, &#34;val&#34;]:</code></pre></div><p>遍历data文件夹下的三个子文件</p><div class=\"highlight\"><pre><code class=\"language-text\">list_img = [img for img in Path(jpeg_dir).glob(&#39;%s/*.jpg&#39; % dset_type)]\nlist_img = [str(img) for img in list_img]\nlist_img.extend(list(Path(jpeg_dir).glob(&#39;%s/*.png&#39; % dset_type)))\nlist_img = list(map(str, list_img))\nlist_img = np.array(list_img)</code></pre></div><p>好吧，我不想分析这一段感觉作者写麻烦了。我自己写了一段，同样可以实现以上的功能。<b><i>这段就是得到每一个子文件夹内文件的所有路径。</i></b></p><div class=\"highlight\"><pre><code class=\"language-text\">list_img = [os.path.join(jpeg_dir,dset_type,name) for name in os.listdir(os.path.join(jpeg_dir,dset_type))]\nlist_img = np.array(list_img)</code></pre></div><p>np.array：将python的list数据类型转换成np.array类型。</p><hr/><p>-------------------------------②------------------------------</p><p><b><i>step2:将图片处理成h5格式</i></b></p><div class=\"highlight\"><pre><code class=\"language-text\">data_full = hfw.create_dataset(&#34;%s_data_full&#34; % dset_type,\n                               (0, nb_channels, size, size),\n                               maxshape=(None, 3, size, size),\n                               dtype=np.uint8)\n\ndata_sketch = hfw.create_dataset(&#34;%s_data_sketch&#34; % dset_type,\n                                 (0, nb_channels, size, size),\n                                 maxshape=(None, 3, size, size),\n                                 dtype=np.uint8)</code></pre></div><p>【<a href=\"https://link.zhihu.com/?target=http%3A//docs.h5py.org/en/latest/high/dataset.html%23creating-datasets\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">create_dataset的官网说明</a>】<b><i>由于这也是我第一次接触这个，以下是我个人的一些见解。</i></b></p><p>create_dataset函数是在一个h5文件中创建一个dataset，需要说明的是一个h5文件可以创建多个dataset以存储不同功能的数据。</p><ol><li>第一个参数是dataset的名字</li><li>dataset里面一个数据的具体大小</li><li>dataset里面最多可以存储多少数据,大多数第一个参数是None说明可以存无限个</li></ol><div class=\"highlight\"><pre><code class=\"language-text\">num_files = len(list_img)\nchunk_size = 100\nnum_chunks = num_files / chunk_size\narr_chunks = np.array_split(np.arange(num_files), num_chunks)</code></pre></div><ol><li><b><i>np.array_split(np.arange(num_files), num_chunks)：这个函数特别特别的好，因为它可以将一组输平均分，而且不足的单成一组</i></b></li><li>因为图片太多了，我们需要分组将其存储到h5文件中</li></ol><div class=\"highlight\"><pre><code class=\"language-text\">for chunk_idx in tqdm(arr_chunks):\n    list_img_path = list_img[chunk_idx].tolist()\n    output = parmap.map(format_image, list_img_path, size, nb_channels, pm_parallel=False)</code></pre></div><ol><li>tolist():将array变成numpy的list，这相当于一个迭代器</li><li>parmap.map：其是将list_img_path里面的量一个一个的送入到format_image函数中【<a href=\"https://link.zhihu.com/?target=http%3A//parmap.readthedocs.io/en/stable/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Welcome to parmap’s documentation!</a>】附录有解析</li></ol><div class=\"highlight\"><pre><code class=\"language-text\">arr_img_full = np.concatenate([o[0] for o in output], axis=0)\narr_img_sketch = np.concatenate([o[1] for o in output], axis=0)</code></pre></div><ol><li>将output的输出分开，且按照第一个维度连接在一起。</li><li>format_image返回的有两个元素list</li></ol><div class=\"highlight\"><pre><code class=\"language-text\">data_full.resize(data_full.shape[0] + arr_img_full.shape[0], axis=0)\ndata_sketch.resize(data_sketch.shape[0] + arr_img_sketch.shape[0], axis=0)</code></pre></div><ol><li>调整dataset的大小，<b><i>我们调整的只是个数</i></b>。</li><li>注意是先把dataset变大在向里面加数据。</li></ol><div class=\"highlight\"><pre><code class=\"language-text\">data_full[-arr_img_full.shape[0]:] = arr_img_full.astype(np.uint8)\ndata_sketch[-arr_img_sketch.shape[0]:] = arr_img_sketch.astype(np.uint8)</code></pre></div><p>向dataset的相应位置存储图片</p><hr/><p>测试：</p><p><b><i>tip:想从后面数几个数：-arr_img_full.shape[0]:</i></b></p><div class=\"highlight\"><pre><code class=\"language-text\">#顾名思义这个函数就是检查，你转换的是否成功的\ndef check_HDF5(jpeg_dir, nb_channels):\n    &#34;&#34;&#34;\n    Plot images with landmarks to check the processing\n    &#34;&#34;&#34;\n\n    # Get hdf5 file\n    file_name = os.path.basename(jpeg_dir.rstrip(&#34;/&#34;))\n    hdf5_file = os.path.join(data_dir, &#34;%s_data.h5&#34; % file_name)\n\n    with h5py.File(hdf5_file, &#34;r&#34;) as hf:#注意这里是&#34;r&#34;只读模式\n        data_full = hf[&#34;train_data_full&#34;]\n        data_sketch = hf[&#34;train_data_sketch&#34;]#得到相应的dataset\n        for i in range(data_full.shape[0]):\n            plt.figure()\n            img = data_full[i, :, :, :].transpose(1,2,0)\n            img2 = data_sketch[i, :, :, :].transpose(1,2,0)#得到图片，将深度在调到最后\n            img = np.concatenate((img, img2), axis=1)#将图片在次拼在一起\n            if nb_channels == 1:\n                plt.imshow(img[:, :, 0], cmap=&#34;gray&#34;)\n            else:\n                plt.imshow(img)\n            plt.show()\n            plt.clf()\n            plt.close()\n使用h5文件的思路是：打开h5文件，选取相应的dataset（这时就相当于一个batch了，有四个维度）。很方便</code></pre></div><p>附：</p><hr/><p><b><i>1， os.path.basename</i></b></p><div class=\"highlight\"><pre><code class=\"language-text\">&gt;&gt;&gt; os.path.basename(&#39;c:\\\\test.csv&#39;) \n&#39;test.csv&#39; \n&gt;&gt;&gt; os.path.basename(&#39;c:\\\\csv&#39;) \n&#39;csv&#39; （这里csv被当作文件名处理了） \n&gt;&gt;&gt; os.path.basename(&#39;c:\\\\csv\\\\&#39;) \n&#39;&#39; </code></pre></div><hr/><p><b><i>2， np.array_split函数</i></b></p><div class=\"highlight\"><pre><code class=\"language-text\">num_files = 5\nprint(num_files)\nchunk_size = 2\nnum_chunks = num_files / chunk_size\nprint(num_chunks)\narr_chunks = np.array_split(np.arange(num_files), num_chunks)\nprint(arr_chunks)\n输出：\n5\n2.5\n[array([0, 1, 2]), array([3, 4])]</code></pre></div><hr/><p><b><i>3，map函数</i></b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-9594fb737d138fc3b86137d92e7ede03_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"600\" data-rawheight=\"227\" class=\"origin_image zh-lightbox-thumb\" width=\"600\" data-original=\"https://pic4.zhimg.com/v2-9594fb737d138fc3b86137d92e7ede03_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;600&#39; height=&#39;227&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"600\" data-rawheight=\"227\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"600\" data-original=\"https://pic4.zhimg.com/v2-9594fb737d138fc3b86137d92e7ede03_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-9594fb737d138fc3b86137d92e7ede03_b.jpg\"/></figure><ol><li>format_image：传入一个功能函数</li><li>list_img_path：一个list相当于一个迭代器</li><li>size, nb_channels:后面两个参数是传给第一个函数format_image的</li><li>pm_parallel：设置map，<b><i>但不清楚是干嘛的</i></b></li><li>功能是循环读出list_img_path的数据到format_image函数中，结果放在一个list内返回</li></ol><p>注：【<a href=\"https://link.zhihu.com/?target=http%3A//blog.csdn.net/chenjinyu_tang/article/details/8136841\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Python中*args 和**kwargs的用法</a>】当函数的参数不确定时，可以使用*args 和**kwargs，*args 没有key值，**kwargs有key值。这就很好的区别了那些参数是传给function的，哪些是去设置map函数的</p><hr/><p><b><i>4，format_image函数</i></b></p><div class=\"highlight\"><pre><code class=\"language-text\">def format_image(img_path, size, nb_channels):\n    &#34;&#34;&#34;\n    Load img with opencv and reshape\n    &#34;&#34;&#34;\n\n    if nb_channels == 1:\n        img = cv2.imread(img_path, 0)\n        img = np.expand_dims(img, axis=-1)\n    else:\n        img = cv2.imread(img_path)\n        img = img[:, :, ::-1]  # GBR to RGB\n\n    w = img.shape[1]\n#读取图片\n\n    # Slice image in 2 to get both parts\n    img_full = img[:, :w // 2, :]\n    img_sketch = img[:, w // 2:, :]\n#将图片切成两部分\n\n    img_full = cv2.resize(img_full, (size, size), interpolation=cv2.INTER_AREA)\n    img_sketch = cv2.resize(img_sketch, (size, size), interpolation=cv2.INTER_AREA)\n#将图片规整到相应大小\n    if nb_channels == 1:\n        img_full = np.expand_dims(img_full, -1)\n        img_sketch = np.expand_dims(img_sketch, -1)\n\n    img_full = np.expand_dims(img_full, 0).transpose(0, 3, 1, 2)\n    img_sketch = np.expand_dims(img_sketch, 0).transpose(0, 3, 1, 2)\n#将图片增加一个第一维度，即batch。然后在将深度维度放到1维度\n\n    return img_full, img_sketch</code></pre></div><p><b><i>？cv2.imread读的是GBR</i></b></p><p><b><i>?python读图片的函数，那些是RGB哪些是</i></b></p><p><i><b>?img[:, :, ::-1]是怎么实现调换通道的：</b> ::-1就是调换顺序</i></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-053c8e8bd37f547f2191ce5ce233d1d2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"512\" data-rawheight=\"256\" class=\"origin_image zh-lightbox-thumb\" width=\"512\" data-original=\"https://pic3.zhimg.com/v2-053c8e8bd37f547f2191ce5ce233d1d2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;512&#39; height=&#39;256&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"512\" data-rawheight=\"256\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"512\" data-original=\"https://pic3.zhimg.com/v2-053c8e8bd37f547f2191ce5ce233d1d2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-053c8e8bd37f547f2191ce5ce233d1d2_b.jpg\"/></figure><p>最后附上我的结果：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-8f9ca6430e1c8000187b0c3f9e2aea6b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"800\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb\" width=\"800\" data-original=\"https://pic4.zhimg.com/v2-8f9ca6430e1c8000187b0c3f9e2aea6b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;800&#39; height=&#39;600&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"800\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"800\" data-original=\"https://pic4.zhimg.com/v2-8f9ca6430e1c8000187b0c3f9e2aea6b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-8f9ca6430e1c8000187b0c3f9e2aea6b_b.jpg\"/></figure><p>欢迎关注公众号：huangxiaobai880</p><a class=\"video-box\" href=\"https://link.zhihu.com/?target=https%3A//www.zhihu.com/video/924964085190230016\" target=\"_blank\" data-video-id=\"\" data-video-playable=\"true\" data-name=\"\" data-poster=\"https://pic4.zhimg.com/80/v2-cda01aabf3a7ee3ecfb021cffadd137b_b.jpg\" data-lens-id=\"924964085190230016\"><img class=\"thumbnail\" src=\"https://pic4.zhimg.com/80/v2-cda01aabf3a7ee3ecfb021cffadd137b_b.jpg\"/><span class=\"content\"><span class=\"title\"><span class=\"z-ico-extern-gray\"></span><span class=\"z-ico-extern-blue\"></span></span><span class=\"url\"><span class=\"z-ico-video\"></span>https://www.zhihu.com/video/924964085190230016</span></span></a><p></p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "源码阅读", 
                    "tagLink": "https://api.zhihu.com/topics/19593602"
                }
            ], 
            "comments": [
                {
                    "userName": "胡苇", 
                    "userLink": "https://www.zhihu.com/people/ed9403c2bdd9db4ae437ad8194240fed", 
                    "content": "<p>cv2读取的通道顺序似乎应该是BGR呀？这样的话倒序排列才是RGB，文中是笔误了？</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "小白", 
                            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
                            "content": "<p>嗯，是笔误，cv2读出来的是BGR</p>", 
                            "likes": 0, 
                            "replyToAuthor": "胡苇"
                        }
                    ]
                }, 
                {
                    "userName": "嵩山旺仔", 
                    "userLink": "https://www.zhihu.com/people/1775e515b158e32d12de9e94d655e7b7", 
                    "content": "<p>您好，您知道下载的数据集是.h5格式的如何转为原图吗？</p><p></p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/32103148", 
            "userName": "小白", 
            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
            "upvote": 2, 
            "title": "Editplus与fastston的一些小tips", 
            "content": "<p></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-accefe2c692145a99af0306c48f890f7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb\" width=\"450\" data-original=\"https://pic4.zhimg.com/v2-accefe2c692145a99af0306c48f890f7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;450&#39; height=&#39;299&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"450\" data-original=\"https://pic4.zhimg.com/v2-accefe2c692145a99af0306c48f890f7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-accefe2c692145a99af0306c48f890f7_b.jpg\"/></figure><p><b><i>1,Editplus保存后，不产生bak文件</i></b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-1ab6b7bd2515a901153546bab76f59a2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"272\" data-rawheight=\"282\" class=\"content_image\" width=\"272\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;272&#39; height=&#39;282&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"272\" data-rawheight=\"282\" class=\"content_image lazy\" width=\"272\" data-actualsrc=\"https://pic3.zhimg.com/v2-1ab6b7bd2515a901153546bab76f59a2_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-cce60dc0c867c65efbcb39b6055f43b8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"561\" data-rawheight=\"498\" class=\"origin_image zh-lightbox-thumb\" width=\"561\" data-original=\"https://pic1.zhimg.com/v2-cce60dc0c867c65efbcb39b6055f43b8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;561&#39; height=&#39;498&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"561\" data-rawheight=\"498\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"561\" data-original=\"https://pic1.zhimg.com/v2-cce60dc0c867c65efbcb39b6055f43b8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-cce60dc0c867c65efbcb39b6055f43b8_b.jpg\"/></figure><p><b><i>2，Editplus注册码</i></b></p><p>用户名：freeuser </p><p>序列号：F15AD-12490-DAZF5-E4W30-E7T80 </p><p><b><i>3，faststone注册码</i></b></p><p>name：bluman</p><p>serial/序列号/注册码</p><p class=\"ztext-empty-paragraph\"><br/></p><p>VPISCJULXUFGDDXYAUYF</p><p>欢迎关注公众号：huangxiaobai880</p><a class=\"video-box\" href=\"https://link.zhihu.com/?target=https%3A//www.zhihu.com/video/926134091210964992\" target=\"_blank\" data-video-id=\"\" data-video-playable=\"true\" data-name=\"\" data-poster=\"https://pic1.zhimg.com/80/v2-09d744dc9d12502fe651ebdc02eecfa0_b.jpg\" data-lens-id=\"926134091210964992\"><img class=\"thumbnail\" src=\"https://pic1.zhimg.com/80/v2-09d744dc9d12502fe651ebdc02eecfa0_b.jpg\"/><span class=\"content\"><span class=\"title\"><span class=\"z-ico-extern-gray\"></span><span class=\"z-ico-extern-blue\"></span></span><span class=\"url\"><span class=\"z-ico-video\"></span>https://www.zhihu.com/video/926134091210964992</span></span></a><p></p>", 
            "topic": [
                {
                    "tag": "安装", 
                    "tagLink": "https://api.zhihu.com/topics/19608575"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/32007274", 
            "userName": "小白", 
            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
            "upvote": 8, 
            "title": "手把手教你在win10下安装keras的plot_model环境", 
            "content": "<p><a href=\"https://link.zhihu.com/?target=http%3A//v.youku.com/v_show/id_XMzI0MjQ3OTQ3Ng%3D%3D.html%3Fspm%3Da2h0k.8191407.0.0%26from%3Ds1.8-1-1.2\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">手把手教你在win10下安装keras的plot_model环境</a>（视频）</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-7a555fb7f0006289e64d29e1194d4067_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb\" width=\"450\" data-original=\"https://pic4.zhimg.com/v2-7a555fb7f0006289e64d29e1194d4067_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;450&#39; height=&#39;299&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"450\" data-original=\"https://pic4.zhimg.com/v2-7a555fb7f0006289e64d29e1194d4067_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-7a555fb7f0006289e64d29e1194d4067_b.jpg\"/></figure><p>首先运行keras.utils的plot_model功能，需要安装官网上说的两个依赖包【<a href=\"https://link.zhihu.com/?target=http%3A//keras-cn.readthedocs.io/en/latest/other/visualization/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">可视化visualization - Keras中文文档</a>】：</p><ul><li>pip install  pydot-ng</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-e6db94ded3f3b1d89e90fb6226f4d329_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"536\" data-rawheight=\"93\" class=\"origin_image zh-lightbox-thumb\" width=\"536\" data-original=\"https://pic2.zhimg.com/v2-e6db94ded3f3b1d89e90fb6226f4d329_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;536&#39; height=&#39;93&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"536\" data-rawheight=\"93\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"536\" data-original=\"https://pic2.zhimg.com/v2-e6db94ded3f3b1d89e90fb6226f4d329_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-e6db94ded3f3b1d89e90fb6226f4d329_b.jpg\"/></figure><ul><li>pip install graphviz</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-969301644959a894b1e97e63a51e912d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"516\" data-rawheight=\"85\" class=\"origin_image zh-lightbox-thumb\" width=\"516\" data-original=\"https://pic2.zhimg.com/v2-969301644959a894b1e97e63a51e912d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;516&#39; height=&#39;85&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"516\" data-rawheight=\"85\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"516\" data-original=\"https://pic2.zhimg.com/v2-969301644959a894b1e97e63a51e912d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-969301644959a894b1e97e63a51e912d_b.jpg\"/></figure><ul><li>pip install  pydot（最好也安装一下这个）</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-254fa0b5793664f46c1a7d6e93c6864a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"528\" data-rawheight=\"90\" class=\"origin_image zh-lightbox-thumb\" width=\"528\" data-original=\"https://pic3.zhimg.com/v2-254fa0b5793664f46c1a7d6e93c6864a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;528&#39; height=&#39;90&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"528\" data-rawheight=\"90\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"528\" data-original=\"https://pic3.zhimg.com/v2-254fa0b5793664f46c1a7d6e93c6864a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-254fa0b5793664f46c1a7d6e93c6864a_b.jpg\"/></figure><p>但即使安装完了，还是会报下面这个错位。</p><p><b>raise ImportError ：Failed to import pydot. You must install pydot  and graphviz for `pydotprint` to work.</b></p><p><b>因为graphviz还需要安装其他的一个东西，graphviz的客户端</b></p><ul><li>下载客户端【<a href=\"https://link.zhihu.com/?target=https%3A//graphviz.gitlab.io/_pages/Download/Download_windows.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">graphviz的客户端地址</a>】</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-7c4af11c312866a3dfedfec01c334465_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"733\" data-rawheight=\"390\" class=\"origin_image zh-lightbox-thumb\" width=\"733\" data-original=\"https://pic2.zhimg.com/v2-7c4af11c312866a3dfedfec01c334465_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;733&#39; height=&#39;390&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"733\" data-rawheight=\"390\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"733\" data-original=\"https://pic2.zhimg.com/v2-7c4af11c312866a3dfedfec01c334465_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-7c4af11c312866a3dfedfec01c334465_b.jpg\"/></figure><ul><li>傻瓜式的点默认设置就可以了，但一定要记一下你的安装地址后面要用</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-9804fc96949b407563b7fa7d0da48c57_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"591\" data-rawheight=\"489\" class=\"origin_image zh-lightbox-thumb\" width=\"591\" data-original=\"https://pic4.zhimg.com/v2-9804fc96949b407563b7fa7d0da48c57_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;591&#39; height=&#39;489&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"591\" data-rawheight=\"489\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"591\" data-original=\"https://pic4.zhimg.com/v2-9804fc96949b407563b7fa7d0da48c57_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-9804fc96949b407563b7fa7d0da48c57_b.jpg\"/></figure><ul><li>将安装的客户端配置一下环境变量</li></ul><ol><li>在win10搜索框中搜环境</li></ol><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-01757f413b37269eb8a6e858f46424cf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"491\" data-rawheight=\"689\" class=\"origin_image zh-lightbox-thumb\" width=\"491\" data-original=\"https://pic4.zhimg.com/v2-01757f413b37269eb8a6e858f46424cf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;491&#39; height=&#39;689&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"491\" data-rawheight=\"689\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"491\" data-original=\"https://pic4.zhimg.com/v2-01757f413b37269eb8a6e858f46424cf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-01757f413b37269eb8a6e858f46424cf_b.jpg\"/></figure><p>2. 点击编辑系统环境变量</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-de44bc862c4a3ca017afb367f82834a9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"494\" data-rawheight=\"615\" class=\"origin_image zh-lightbox-thumb\" width=\"494\" data-original=\"https://pic2.zhimg.com/v2-de44bc862c4a3ca017afb367f82834a9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;494&#39; height=&#39;615&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"494\" data-rawheight=\"615\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"494\" data-original=\"https://pic2.zhimg.com/v2-de44bc862c4a3ca017afb367f82834a9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-de44bc862c4a3ca017afb367f82834a9_b.jpg\"/></figure><p>3. 找到系统变量里的path</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-361f7150d15065782d1b6c1d13c070c9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"604\" data-rawheight=\"646\" class=\"origin_image zh-lightbox-thumb\" width=\"604\" data-original=\"https://pic2.zhimg.com/v2-361f7150d15065782d1b6c1d13c070c9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;604&#39; height=&#39;646&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"604\" data-rawheight=\"646\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"604\" data-original=\"https://pic2.zhimg.com/v2-361f7150d15065782d1b6c1d13c070c9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-361f7150d15065782d1b6c1d13c070c9_b.jpg\"/></figure><p>4. 在点击编辑</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-aa547b7fc76337ad9b09996227560f39_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"607\" data-rawheight=\"655\" class=\"origin_image zh-lightbox-thumb\" width=\"607\" data-original=\"https://pic2.zhimg.com/v2-aa547b7fc76337ad9b09996227560f39_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;607&#39; height=&#39;655&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"607\" data-rawheight=\"655\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"607\" data-original=\"https://pic2.zhimg.com/v2-aa547b7fc76337ad9b09996227560f39_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-aa547b7fc76337ad9b09996227560f39_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-e51e8628c20ea6db88b1ebe2c3223ef0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"510\" data-rawheight=\"541\" class=\"origin_image zh-lightbox-thumb\" width=\"510\" data-original=\"https://pic1.zhimg.com/v2-e51e8628c20ea6db88b1ebe2c3223ef0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;510&#39; height=&#39;541&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"510\" data-rawheight=\"541\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"510\" data-original=\"https://pic1.zhimg.com/v2-e51e8628c20ea6db88b1ebe2c3223ef0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-e51e8628c20ea6db88b1ebe2c3223ef0_b.jpg\"/></figure><p>5.点击新建，然后将安装的graphviz的bin路径添加进去</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-4a16c2907ef55cb3f1eceaf89800131b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"731\" data-rawheight=\"325\" class=\"origin_image zh-lightbox-thumb\" width=\"731\" data-original=\"https://pic4.zhimg.com/v2-4a16c2907ef55cb3f1eceaf89800131b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;731&#39; height=&#39;325&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"731\" data-rawheight=\"325\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"731\" data-original=\"https://pic4.zhimg.com/v2-4a16c2907ef55cb3f1eceaf89800131b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-4a16c2907ef55cb3f1eceaf89800131b_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-5bee9a88503e769e2660fcf1fac2cf13_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"513\" data-rawheight=\"544\" class=\"origin_image zh-lightbox-thumb\" width=\"513\" data-original=\"https://pic4.zhimg.com/v2-5bee9a88503e769e2660fcf1fac2cf13_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;513&#39; height=&#39;544&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"513\" data-rawheight=\"544\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"513\" data-original=\"https://pic4.zhimg.com/v2-5bee9a88503e769e2660fcf1fac2cf13_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-5bee9a88503e769e2660fcf1fac2cf13_b.jpg\"/></figure><p>最后是一路点确定就可以了，还有一个非常重要：<b><i>重启才会生效！重启才会生效！重启才会生效！</i></b></p><p>最后给出一个生成示列（网络很大，截取其中的一部分）</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-28c4a6dc8aa7c18877b5b98074a0ddb8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"793\" data-rawheight=\"563\" class=\"origin_image zh-lightbox-thumb\" width=\"793\" data-original=\"https://pic1.zhimg.com/v2-28c4a6dc8aa7c18877b5b98074a0ddb8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;793&#39; height=&#39;563&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"793\" data-rawheight=\"563\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"793\" data-original=\"https://pic1.zhimg.com/v2-28c4a6dc8aa7c18877b5b98074a0ddb8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-28c4a6dc8aa7c18877b5b98074a0ddb8_b.jpg\"/></figure><p>欢迎关注公众号：huangxiaobai880</p><a class=\"video-box\" href=\"https://link.zhihu.com/?target=https%3A//www.zhihu.com/video/924614186774978560\" target=\"_blank\" data-video-id=\"\" data-video-playable=\"true\" data-name=\"\" data-poster=\"https://pic4.zhimg.com/80/v2-cda01aabf3a7ee3ecfb021cffadd137b_b.jpg\" data-lens-id=\"924614186774978560\"><img class=\"thumbnail\" src=\"https://pic4.zhimg.com/80/v2-cda01aabf3a7ee3ecfb021cffadd137b_b.jpg\"/><span class=\"content\"><span class=\"title\"><span class=\"z-ico-extern-gray\"></span><span class=\"z-ico-extern-blue\"></span></span><span class=\"url\"><span class=\"z-ico-video\"></span>https://www.zhihu.com/video/924614186774978560</span></span></a><p></p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "安装", 
                    "tagLink": "https://api.zhihu.com/topics/19608575"
                }
            ], 
            "comments": [
                {
                    "userName": "李二奎", 
                    "userLink": "https://www.zhihu.com/people/680114c7bb3eba18a572178f9ac132f8", 
                    "content": "<p>恩公，您辛苦了。。</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "龙哥", 
                    "userLink": "https://www.zhihu.com/people/1e6044bc7e5ed078caac9123c05a99c4", 
                    "content": "<p>跟你照着做了，但是还是出现了</p><p><br></p><p><b>OSError</b>: `pydot` failed to call GraphViz.Please install GraphViz (<a href=\"http://link.zhihu.com/?target=https%3A//www.graphviz.org/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">https://www.http://graphviz.org/</a>) and ensure that its executables are in the $PATH.</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "魏之远", 
                            "userLink": "https://www.zhihu.com/people/fdd8765abb0b59cfcac3f8eaaad86e7e", 
                            "content": "<p>我也是 不知道哪里出问题了，import  graphviz还是不报错的</p>", 
                            "likes": 0, 
                            "replyToAuthor": "龙哥"
                        }
                    ]
                }, 
                {
                    "userName": "阿煦", 
                    "userLink": "https://www.zhihu.com/people/ab50eeb23a771ecf756842729b39c292", 
                    "content": "<p>按照你写的还是报错了。。。。</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "小白", 
                            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
                            "content": "<p>具体报什么错？</p>", 
                            "likes": 0, 
                            "replyToAuthor": "阿煦"
                        }, 
                        {
                            "userName": "黑猫", 
                            "userLink": "https://www.zhihu.com/people/40351b7f2cb7cd72f1fee4b1123587f7", 
                            "content": "<p>还是OSError: `pydot` failed to call GraphViz.Please install GraphViz (<a href=\"http://link.zhihu.com/?target=https%3A//www.graphviz.org/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Graphviz - Graph Visualization Software</a>) and ensure that its executables are in the $PATH.</p>", 
                            "likes": 0, 
                            "replyToAuthor": "小白"
                        }
                    ]
                }, 
                {
                    "userName": "黑猫", 
                    "userLink": "https://www.zhihu.com/people/40351b7f2cb7cd72f1fee4b1123587f7", 
                    "content": "<p>pydot已经停止开发了，python3.5以上已经用不起来了。对策是：</p><p>pip uninstall pydot</p><p>pip install pydotplus</p><p>然后找到keras里面的utils\\vis_utils.py，把里面的pydot的都替换成pydotplus。</p>", 
                    "likes": 2, 
                    "childComments": [
                        {
                            "userName": "张晶", 
                            "userLink": "https://www.zhihu.com/people/62f725828479606c06eb0a0717b1f0da", 
                            "content": "<p>感谢分享，按照您的建议，可以正确生成模型图了</p>", 
                            "likes": 0, 
                            "replyToAuthor": "黑猫"
                        }, 
                        {
                            "userName": "张晶", 
                            "userLink": "https://www.zhihu.com/people/62f725828479606c06eb0a0717b1f0da", 
                            "content": "<p>感谢分享，按照您的建议，可以正确生成模型图了</p>", 
                            "likes": 0, 
                            "replyToAuthor": "黑猫"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/31860060", 
            "userName": "小白", 
            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
            "upvote": 14, 
            "title": "手把手教你在win10下安装tensorflow与keras", 
            "content": "<p><a href=\"https://link.zhihu.com/?target=http%3A//v.youku.com/v_show/id_XMzI0MjQ3OTU1Ng%3D%3D.html%3Fspm%3Da2h0k.8191407.0.0%26from%3Ds1.8-1-1.2\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">手把手教你在win10下安装tensorflow与keras-CPU版本</a>【视频】</p><p><a href=\"https://link.zhihu.com/?target=http%3A//v.youku.com/v_show/id_XMzI0MjYzNzc2MA%3D%3D.html%3Fspm%3Da2h0k.8191407.0.0%26from%3Ds1.8-1-1.2\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">手把手教你在win10下安装tensorflow与keras-GPU版本</a>【视频】</p><p>搞了半天终于弄完了，安装了好几次，然后就忘了在要安装的时候又要花费好长时间。所以，这次干脆将其记录下来。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-e88eb1a256ee7e843cc529e08bb95f00_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb\" width=\"450\" data-original=\"https://pic1.zhimg.com/v2-e88eb1a256ee7e843cc529e08bb95f00_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;450&#39; height=&#39;299&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"450\" data-original=\"https://pic1.zhimg.com/v2-e88eb1a256ee7e843cc529e08bb95f00_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-e88eb1a256ee7e843cc529e08bb95f00_b.jpg\"/></figure><p><b><i>首先，安装一定要看官网，一定！一定！如果安装CPU版本的tensorflow直接跳过step1</i></b></p><p>step1:安装英伟达相应的程序</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-056321618b756a6b15426ee1d936a615_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"881\" data-rawheight=\"426\" class=\"origin_image zh-lightbox-thumb\" width=\"881\" data-original=\"https://pic2.zhimg.com/v2-056321618b756a6b15426ee1d936a615_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;881&#39; height=&#39;426&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"881\" data-rawheight=\"426\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"881\" data-original=\"https://pic2.zhimg.com/v2-056321618b756a6b15426ee1d936a615_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-056321618b756a6b15426ee1d936a615_b.jpg\"/></figure><ol><li>要安装CUDA8.0<b>（官网让你安装8.0，你就安装8.0.千万别小聪明的去安装更高的版本比如9.0.这是有教训的，我本来先安装的9.0版本的，没用！还要一个一个地把9.0版本的组件删掉，在安装8.0/(ㄒoㄒ)/~~）【</b><a href=\"https://link.zhihu.com/?target=https%3A//developer.nvidia.com/cuda-80-ga2-download-archive\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">CUDA Toolkit 8.0网址</a><b>】</b></li></ol><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-11ba92ebe952a268615c01feb6d7faba_b.jpg\" data-size=\"normal\" data-rawwidth=\"649\" data-rawheight=\"419\" class=\"origin_image zh-lightbox-thumb\" width=\"649\" data-original=\"https://pic3.zhimg.com/v2-11ba92ebe952a268615c01feb6d7faba_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;649&#39; height=&#39;419&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"649\" data-rawheight=\"419\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"649\" data-original=\"https://pic3.zhimg.com/v2-11ba92ebe952a268615c01feb6d7faba_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-11ba92ebe952a268615c01feb6d7faba_b.jpg\"/><figcaption>就安照这个下</figcaption></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-7a1671dafd4fac7a92c879c8142caf8e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"625\" data-rawheight=\"604\" class=\"origin_image zh-lightbox-thumb\" width=\"625\" data-original=\"https://pic3.zhimg.com/v2-7a1671dafd4fac7a92c879c8142caf8e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;625&#39; height=&#39;604&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"625\" data-rawheight=\"604\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"625\" data-original=\"https://pic3.zhimg.com/v2-7a1671dafd4fac7a92c879c8142caf8e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-7a1671dafd4fac7a92c879c8142caf8e_b.jpg\"/></figure><p>两个都下下来，先安装第一个，完成后在安第二个【ps:第二个是一个补丁】。都是默认设置，傻瓜式的安装。建议不要修改，相应的配置。</p><p>2. 下载与CUDA8.0对应的cuDNN v6.1.。<b><i>划重点：要与CUDA8.0对应的，其他的没用。这里我下的是CUDA8.0对应的cuDNN v6.0.也没什么问题【</i></b><a href=\"https://link.zhihu.com/?target=https%3A//developer.nvidia.com/rdp/form/cudnn-download-survey\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">CUDNN下载地址</a><b><i>】</i></b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-46e231a363e9317167a06da887fce2be_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"743\" data-rawheight=\"596\" class=\"origin_image zh-lightbox-thumb\" width=\"743\" data-original=\"https://pic3.zhimg.com/v2-46e231a363e9317167a06da887fce2be_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;743&#39; height=&#39;596&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"743\" data-rawheight=\"596\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"743\" data-original=\"https://pic3.zhimg.com/v2-46e231a363e9317167a06da887fce2be_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-46e231a363e9317167a06da887fce2be_b.jpg\"/></figure><p>之后将其解压：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-0aa6e986ab0a80ab80ee99d42f24a28e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"611\" data-rawheight=\"229\" class=\"origin_image zh-lightbox-thumb\" width=\"611\" data-original=\"https://pic3.zhimg.com/v2-0aa6e986ab0a80ab80ee99d42f24a28e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;611&#39; height=&#39;229&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"611\" data-rawheight=\"229\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"611\" data-original=\"https://pic3.zhimg.com/v2-0aa6e986ab0a80ab80ee99d42f24a28e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-0aa6e986ab0a80ab80ee99d42f24a28e_b.jpg\"/></figure><p>得到：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-d5947d9adc9e9a41e06ff4af4687d453_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"762\" data-rawheight=\"220\" class=\"origin_image zh-lightbox-thumb\" width=\"762\" data-original=\"https://pic4.zhimg.com/v2-d5947d9adc9e9a41e06ff4af4687d453_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;762&#39; height=&#39;220&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"762\" data-rawheight=\"220\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"762\" data-original=\"https://pic4.zhimg.com/v2-d5947d9adc9e9a41e06ff4af4687d453_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-d5947d9adc9e9a41e06ff4af4687d453_b.jpg\"/></figure><p>在将其复制到CUDA8相应的文件夹</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-a738438a9320baf4f99f2108268723dc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"874\" data-rawheight=\"423\" class=\"origin_image zh-lightbox-thumb\" width=\"874\" data-original=\"https://pic1.zhimg.com/v2-a738438a9320baf4f99f2108268723dc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;874&#39; height=&#39;423&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"874\" data-rawheight=\"423\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"874\" data-original=\"https://pic1.zhimg.com/v2-a738438a9320baf4f99f2108268723dc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-a738438a9320baf4f99f2108268723dc_b.jpg\"/></figure><p>这样，这一步算是完成了</p><p>step2:安装anconda</p><p>我安装的是Anaconda3-4.2.0-Windows-x86_64，大家可以到清华镜像去下载。由于墙的原因直接到外网下回很慢【<a href=\"https://link.zhihu.com/?target=https%3A//mirrors.tuna.tsinghua.edu.cn/anaconda/archive/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">清华镜像</a>】【你一定要下tensorflow支持的python版本，不是越高越好】之后就是傻瓜式的安装了，还是建议默认设置</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-bddd36ba25cf8edb801edbd5d4a22800_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"380\" data-rawheight=\"217\" class=\"content_image\" width=\"380\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;380&#39; height=&#39;217&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"380\" data-rawheight=\"217\" class=\"content_image lazy\" width=\"380\" data-actualsrc=\"https://pic1.zhimg.com/v2-bddd36ba25cf8edb801edbd5d4a22800_b.jpg\"/></figure><p>打开这个，输这个(CPU版本：pip install tensorflow[不需要后面的-gpu后缀])：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-7a9809996754f72b55c08c0f3e006274_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"616\" data-rawheight=\"152\" class=\"origin_image zh-lightbox-thumb\" width=\"616\" data-original=\"https://pic1.zhimg.com/v2-7a9809996754f72b55c08c0f3e006274_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;616&#39; height=&#39;152&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"616\" data-rawheight=\"152\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"616\" data-original=\"https://pic1.zhimg.com/v2-7a9809996754f72b55c08c0f3e006274_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-7a9809996754f72b55c08c0f3e006274_b.jpg\"/></figure><p>之后就是慢慢等就可以了,完成后在输入下面的[pip install keras]</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-d37458fdb055369982ec88e57752b922_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"568\" data-rawheight=\"91\" class=\"origin_image zh-lightbox-thumb\" width=\"568\" data-original=\"https://pic3.zhimg.com/v2-d37458fdb055369982ec88e57752b922_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;568&#39; height=&#39;91&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"568\" data-rawheight=\"91\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"568\" data-original=\"https://pic3.zhimg.com/v2-d37458fdb055369982ec88e57752b922_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-d37458fdb055369982ec88e57752b922_b.jpg\"/></figure><p>这样tensorflow和keras就算是完成了</p><p>【python -m pip install --upgrade pip】</p><p>step3:</p><p>安装pycharm，这是一款开发工具，建议下载2017.2.4的版本，更高的版本我式了有一点问题【<a href=\"https://link.zhihu.com/?target=https%3A//download.jetbrains.com/python/pycharm-community-2017.2.4.exe\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">2017.2.4 for Windows (exe)的pycharm</a>】之后就是傻瓜式的安装了，还是建议默认设置</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-c94bb7235977a6152b12e8b10cad839d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1026\" data-rawheight=\"563\" class=\"origin_image zh-lightbox-thumb\" width=\"1026\" data-original=\"https://pic2.zhimg.com/v2-c94bb7235977a6152b12e8b10cad839d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1026&#39; height=&#39;563&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1026\" data-rawheight=\"563\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1026\" data-original=\"https://pic2.zhimg.com/v2-c94bb7235977a6152b12e8b10cad839d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-c94bb7235977a6152b12e8b10cad839d_b.jpg\"/></figure><p>这样就大功告成啦！</p><p>附：需要把anconda环境引入到pycharm中</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-e7a47359c58622d8b2740472f168dd8b_b.jpg\" data-size=\"normal\" data-rawwidth=\"322\" data-rawheight=\"302\" class=\"content_image\" width=\"322\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;322&#39; height=&#39;302&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"322\" data-rawheight=\"302\" class=\"content_image lazy\" width=\"322\" data-actualsrc=\"https://pic4.zhimg.com/v2-e7a47359c58622d8b2740472f168dd8b_b.jpg\"/><figcaption>第一步</figcaption></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-d2055d049236e644a8c92c916a373508_b.jpg\" data-size=\"normal\" data-rawwidth=\"1024\" data-rawheight=\"682\" class=\"origin_image zh-lightbox-thumb\" width=\"1024\" data-original=\"https://pic1.zhimg.com/v2-d2055d049236e644a8c92c916a373508_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1024&#39; height=&#39;682&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1024\" data-rawheight=\"682\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1024\" data-original=\"https://pic1.zhimg.com/v2-d2055d049236e644a8c92c916a373508_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-d2055d049236e644a8c92c916a373508_b.jpg\"/><figcaption>第二步</figcaption></figure><p>将interpreter中的环境改成anconda环境【要先apply在点ok】</p><p>最后是测试：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-2970fe0221d49235a684139eb83f7928_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"631\" data-rawheight=\"189\" class=\"origin_image zh-lightbox-thumb\" width=\"631\" data-original=\"https://pic1.zhimg.com/v2-2970fe0221d49235a684139eb83f7928_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;631&#39; height=&#39;189&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"631\" data-rawheight=\"189\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"631\" data-original=\"https://pic1.zhimg.com/v2-2970fe0221d49235a684139eb83f7928_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-2970fe0221d49235a684139eb83f7928_b.jpg\"/></figure><p>建一个py文件，引入tensorflow和keras不报错。【cv2只是一个示列】</p><p>ps安装cv2的代码： </p><p><code>conda install --channel https://conda.anaconda.org/menpo opencv3</code> </p><p>欢迎关注公众号：huangxiaobai880</p><a class=\"video-box\" href=\"https://link.zhihu.com/?target=https%3A//www.zhihu.com/video/924614833696034816\" target=\"_blank\" data-video-id=\"\" data-video-playable=\"true\" data-name=\"\" data-poster=\"https://pic4.zhimg.com/80/v2-cda01aabf3a7ee3ecfb021cffadd137b_b.jpg\" data-lens-id=\"924614833696034816\"><img class=\"thumbnail\" src=\"https://pic4.zhimg.com/80/v2-cda01aabf3a7ee3ecfb021cffadd137b_b.jpg\"/><span class=\"content\"><span class=\"title\"><span class=\"z-ico-extern-gray\"></span><span class=\"z-ico-extern-blue\"></span></span><span class=\"url\"><span class=\"z-ico-video\"></span>https://www.zhihu.com/video/924614833696034816</span></span></a><p></p><p></p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": [
                {
                    "userName": "李德杰", 
                    "userLink": "https://www.zhihu.com/people/a96a94e876b874866b2abd7da8c772cf", 
                    "content": "<p>测试程序会有报错</p><p>Could not load 'nvcuda.dll'. The GPU version of TensorFlow requires that this DLL be installed in a directory that is named in your %PATH% environment variable. Typically it is installed in 'C:\\Windows\\System32'. If it is not present, ensure that you have a CUDA-capable GPU with the correct driver installed.</p><p>是不是因为显卡是AMD Radeon HD 8570M的原因啊？</p><p></p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "小白", 
                            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
                            "content": "是的", 
                            "likes": 0, 
                            "replyToAuthor": "李德杰"
                        }, 
                        {
                            "userName": "李德杰", 
                            "userLink": "https://www.zhihu.com/people/a96a94e876b874866b2abd7da8c772cf", 
                            "content": "<p>谢谢了</p>", 
                            "likes": 0, 
                            "replyToAuthor": "小白"
                        }
                    ]
                }, 
                {
                    "userName": "可上九天揽月", 
                    "userLink": "https://www.zhihu.com/people/48a3f4f6b022e133e029adc37e7549e3", 
                    "content": "<p>大佬好 我根据您的教程安装keras 在安装CUDA8.0时，下载Base Installer时，下完后 就成了</p><figure data-size=\"normal\"><img src=\"https://pic3.zhimg.com/50/v2-74d1ed2154eac74534cab5d92295508a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"515\" data-rawheight=\"224\" class=\"origin_image zh-lightbox-thumb\" width=\"515\" data-original=\"https://pic3.zhimg.com/50/v2-74d1ed2154eac74534cab5d92295508a_r.jpg\"></figure><p>请教下大佬 有什么建议</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "小白", 
                            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
                            "content": "<p>重新下载试试。</p>", 
                            "likes": 0, 
                            "replyToAuthor": "可上九天揽月"
                        }
                    ]
                }, 
                {
                    "userName": "可上九天揽月", 
                    "userLink": "https://www.zhihu.com/people/48a3f4f6b022e133e029adc37e7549e3", 
                    "content": "<p>感谢大佬回复， 下载两次了，都是</p><figure data-size=\"normal\"><img src=\"https://pic4.zhimg.com/50/v2-ee3d970a27b9bd84fca404f8c89266e3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"636\" data-rawheight=\"292\" class=\"origin_image zh-lightbox-thumb\" width=\"636\" data-original=\"https://pic4.zhimg.com/50/v2-ee3d970a27b9bd84fca404f8c89266e3_r.jpg\"></figure><p>在Temp下找不到cuda</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "小白", 
                            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
                            "content": "不太懂", 
                            "likes": 0, 
                            "replyToAuthor": "可上九天揽月"
                        }
                    ]
                }, 
                {
                    "userName": "可上九天揽月", 
                    "userLink": "https://www.zhihu.com/people/48a3f4f6b022e133e029adc37e7549e3", 
                    "content": "<p>大佬 我网上搜了下感觉我的问题有点不一样，我在描述下   我一开始在桌面上下载的，一共两个，其中一个打不开，另一个安装后，在响应的Temp文件下，找不到XUDA，有一推压缩包，</p><figure data-size=\"normal\"><img src=\"https://pic4.zhimg.com/50/v2-01a1c9e1aed2b5f6846ffdd53e860cba_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"545\" data-rawheight=\"617\" class=\"origin_image zh-lightbox-thumb\" width=\"545\" data-original=\"https://pic4.zhimg.com/50/v2-01a1c9e1aed2b5f6846ffdd53e860cba_r.jpg\"></figure><p>您觉的啥原因</p><p></p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "可上九天揽月", 
                    "userLink": "https://www.zhihu.com/people/48a3f4f6b022e133e029adc37e7549e3", 
                    "content": "<p>发出去后发现打错了不字，非常抱歉</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "绵羊jwy", 
                    "userLink": "https://www.zhihu.com/people/c4253cfd540e070688e7d87de6088840", 
                    "content": "您好，cpu版本和gpu版本有啥区别？", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "小白", 
                            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
                            "content": "<p>前者是在你的CPU上执行计算的，后者是在你的GPU上执行计算的。</p>", 
                            "likes": 0, 
                            "replyToAuthor": "绵羊jwy"
                        }
                    ]
                }, 
                {
                    "userName": "昵称写了还能改吗", 
                    "userLink": "https://www.zhihu.com/people/4b94d2914b42e6bc4bbbf7e55d3df62c", 
                    "content": "难道不用创建tensorflow环境吗？我看的很多教程倒要先创建环境再激活环境，然后必须要在环境下安装，而且python的版本还要和tensorflow的版本对应，这样下载的是对应的版本吗？？？", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "小白", 
                            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
                            "content": "<p>不是必要步骤，这样也行</p>", 
                            "likes": 0, 
                            "replyToAuthor": "昵称写了还能改吗"
                        }, 
                        {
                            "userName": "昵称写了还能改吗", 
                            "userLink": "https://www.zhihu.com/people/4b94d2914b42e6bc4bbbf7e55d3df62c", 
                            "content": "是吗，我按照网上方法已经失败好几次了，那我试试你这个方法吧", 
                            "likes": 0, 
                            "replyToAuthor": "小白"
                        }
                    ]
                }, 
                {
                    "userName": "绵羊jwy", 
                    "userLink": "https://www.zhihu.com/people/c4253cfd540e070688e7d87de6088840", 
                    "content": "pip装tebsorflow 装80%报错是什么情况？(一片红)", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "小白", 
                            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
                            "content": "<p>贴图看下</p>", 
                            "likes": 0, 
                            "replyToAuthor": "绵羊jwy"
                        }, 
                        {
                            "userName": "小白", 
                            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
                            "content": "<p>很有可能下载的时候连接中断了，可以更换pip源试试</p>", 
                            "likes": 0, 
                            "replyToAuthor": "绵羊jwy"
                        }
                    ]
                }, 
                {
                    "userName": "奇點PETER", 
                    "userLink": "https://www.zhihu.com/people/a1f5e248dbf29cec920ce29f624a256a", 
                    "content": "<p>之前自己下载的conda怎么都不好用。按照这个教程上面的下载安装好多问题都解决了，估计是版本太高的问题。果断收藏这篇教程。</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "肖阳", 
                    "userLink": "https://www.zhihu.com/people/f914b88aa8f441ced470ceaf6a42bde6", 
                    "content": "请问一下cuda里本来就有那三个文件夹，是替换掉吗？", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/31913377", 
            "userName": "小白", 
            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
            "upvote": 0, 
            "title": "python与numpy使用的一些小tips(2)", 
            "content": "<p></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-0f68acca9c1a531b74bc048ddf22866e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb\" width=\"450\" data-original=\"https://pic3.zhimg.com/v2-0f68acca9c1a531b74bc048ddf22866e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;450&#39; height=&#39;299&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"450\" data-original=\"https://pic3.zhimg.com/v2-0f68acca9c1a531b74bc048ddf22866e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-0f68acca9c1a531b74bc048ddf22866e_b.jpg\"/></figure><p><b><i>1，保存语音</i></b></p><div class=\"highlight\"><pre><code class=\"language-text\">from scipy.io import wavfile\nwavfile.write(path, rate, speech)</code></pre></div><ol><li>path:包含路径和要存储的名称</li><li>rate:存储的频率</li><li>speech:要存储的语音矩阵</li></ol><p>附：os.path.join(testspeechdir, &#39;AG_{0:03d}.wav&#39;.format(epoch))：在testspeechdir路径下存储名称是epoch</p><p><b><i>需要注意的是：</i></b></p><ol><li><b><i>speech的形状可以是一位的如shape为(32768，)或者二维的如(32768，1)。但不可以是三维的，因为深度学习的第一维是batch，要非常的注意。</i></b></li><li><b><i>如果保存的语音值不是在0到1之间，就要将其数据类型转化为int。在0到1时float类型是可以的</i></b></li></ol><p><b><i>讲两段语音放在一起保存</i></b></p><div class=\"highlight\"><pre><code class=\"language-text\">testA = SpeechGenerator(testimgdirA,speechlen)\ntestB = SpeechGenerator(testimgdirB,speechlen)\nA_speech_batch = next(testA)\nB_speech_batch = next(testB)\nC_speech_batch = np.concatenate([A_speech_batch,B_speech_batch],axis=1)\nC = ulaw2lin(C_speech_batch)\nprint(C_speech_batch.shape)\nprint(A_speech_batch)\nprint(B_speech_batch)\nwavfile.write(os.path.join(testspeechdir, &#39;AG_{0:03d}.wav&#39;.format(7)), 16000, C[0])\n输出：\n(1, 65536, 1)\n(1, 32768, 1)\n(1, 32768, 1)</code></pre></div><p><b><i>2，对语音进行滤波</i></b></p><div class=\"highlight\"><pre><code class=\"language-text\">from scipy import signal\nB_speech_batch = next(testB)\nb,a = signal.butter(3,0.05,&#39;low&#39;)\nC_speech_batch = signal.filtfilt(b,a,B_speech_batch[0,:,0])</code></pre></div><ol><li>signal.butter对于第二参数是0到1之间（当其他参数是默认值时）</li><li><b><i>对信号的要求是一维的【</i></b><a href=\"https://link.zhihu.com/?target=https%3A//docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.signal.butter.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">scipy.signal.butter</a><b><i>】</i></b></li></ol><p><b><i>3，相对路径</i></b></p><p>以下为建立路径所使用的几个特殊符号，及其所代表的意义。</p><ol><li><b><i>&#34;./&#34;：代表目前所在的目录</i></b></li><li><b><i>&#34;../&#34;：代表上一层目录</i></b></li></ol><p>欢迎关注公众号：huangxiaobai880</p><a class=\"video-box\" href=\"https://link.zhihu.com/?target=https%3A//www.zhihu.com/video/924617160326799360\" target=\"_blank\" data-video-id=\"\" data-video-playable=\"true\" data-name=\"\" data-poster=\"https://pic4.zhimg.com/80/v2-cda01aabf3a7ee3ecfb021cffadd137b_b.jpg\" data-lens-id=\"924617160326799360\"><img class=\"thumbnail\" src=\"https://pic4.zhimg.com/80/v2-cda01aabf3a7ee3ecfb021cffadd137b_b.jpg\"/><span class=\"content\"><span class=\"title\"><span class=\"z-ico-extern-gray\"></span><span class=\"z-ico-extern-blue\"></span></span><span class=\"url\"><span class=\"z-ico-video\"></span>https://www.zhihu.com/video/924617160326799360</span></span></a><p></p>", 
            "topic": [
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }, 
                {
                    "tag": "numpy", 
                    "tagLink": "https://api.zhihu.com/topics/19834165"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/31907231", 
            "userName": "小白", 
            "userLink": "https://www.zhihu.com/people/cc2e2e485f447f622011a614a93311b2", 
            "upvote": 0, 
            "title": "python与numpy使用的一些小tips(1)", 
            "content": "<p></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b1a595a5e5f6dd833da8972e7a6f8229_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb\" width=\"450\" data-original=\"https://pic2.zhimg.com/v2-b1a595a5e5f6dd833da8972e7a6f8229_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;450&#39; height=&#39;299&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"450\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"450\" data-original=\"https://pic2.zhimg.com/v2-b1a595a5e5f6dd833da8972e7a6f8229_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-b1a595a5e5f6dd833da8972e7a6f8229_b.jpg\"/></figure><hr/><p><b><i>1，python的len()函数</i></b></p><p>返回列表元素个数【<a href=\"https://link.zhihu.com/?target=http%3A//www.runoob.com/python/att-list-len.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Python List len()方法</a>】</p><p>需要说明的是：当一个如shape=(1,2,3)的numpy数组，它会放回1。因为python认为：这个数组有一个元素，而每一个元素的形状是(2,3).总的来说，len()函数返回的是其第一个维度的个数。底下是列子：</p><div class=\"highlight\"><pre><code class=\"language-text\">A = np.ones((1,2,3))\nprint(A.shape)\nprint(len(A))\nB = np.ones((6,2,3))\nprint(B.shape)\nprint(len(B))\n输出：\n(1, 2, 3)\n1\n(6, 2, 3)\n6</code></pre></div><hr/><p><b><i>2，matplotlib的plot使用</i></b></p><p>第一个需要注意的是plot(x,y)</p><ol><li>x和y必须有相同的第一个维度</li><li>x和y的维度必须不超过2</li></ol><div class=\"highlight\"><pre><code class=\"language-text\">画一个一维的，一条线：\nA = np.ones(1000)\nplt.plot(range(len(A)),A)\nplt.show()</code></pre></div><p>注：A = np.ones(<b><i>(1000,1)</i></b>)画出来的是一样的，最后要show一下才会出现图表</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-5482ede3328ed5143c22b94ca5909f62_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"578\" data-rawheight=\"433\" class=\"origin_image zh-lightbox-thumb\" width=\"578\" data-original=\"https://pic3.zhimg.com/v2-5482ede3328ed5143c22b94ca5909f62_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;578&#39; height=&#39;433&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"578\" data-rawheight=\"433\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"578\" data-original=\"https://pic3.zhimg.com/v2-5482ede3328ed5143c22b94ca5909f62_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-5482ede3328ed5143c22b94ca5909f62_b.jpg\"/></figure><div class=\"highlight\"><pre><code class=\"language-text\">画一个一维的，一条线：\nA = np.random.uniform(0,1,(1000,1))\nB = np.random.uniform(0,2,(1000,1))\nC = np.concatenate([A,B],axis=1)\nplt.plot(range(len(C)),C)\nplt.show()\nprint(C.shape)\n输出：\n(1000, 2)</code></pre></div><p>注：</p><ol><li>画N条线y的第二个为N个维度。</li><li>numpy.random.uniform(<i>low=0.0</i>, <i>high=1.0</i>, <i>size=None</i>)【<a href=\"https://link.zhihu.com/?target=https%3A//docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.random.uniform.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">numpy.random.uniform - NumPy v1.10 Manual</a>】产生一个[low,high)的均匀分布的随机数，形状为size</li></ol><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-1cec6b9947db16a3dece12a7bf665ba2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"588\" data-rawheight=\"445\" class=\"origin_image zh-lightbox-thumb\" width=\"588\" data-original=\"https://pic3.zhimg.com/v2-1cec6b9947db16a3dece12a7bf665ba2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;588&#39; height=&#39;445&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"588\" data-rawheight=\"445\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"588\" data-original=\"https://pic3.zhimg.com/v2-1cec6b9947db16a3dece12a7bf665ba2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-1cec6b9947db16a3dece12a7bf665ba2_b.jpg\"/></figure><p>对range讨论：</p><div class=\"highlight\"><pre><code class=\"language-text\">print(range(8))\nprint(range(8)[0])\nprint(range(8)[1])\nprint(type(range(8)))\nprint(len(range(8)))\n输出：\nrange(0, 8)\n0\n1\n&lt;class &#39;range&#39;&gt;\n8</code></pre></div><p>range输出的就是一个range类，其余的读者自行体会</p><p>保存图表</p><div class=\"highlight\"><pre><code class=\"language-text\">plt.savefig(os.path.join(testspeechdir, &#39;BF_{}_{}.png&#39;.format(epoch,index)))\nplt.close()</code></pre></div><ol><li>plt.savefig：图表路径包含名称</li><li><b><i>plt.close()：是将图表关闭，要不然它是接着上一次的画【</i></b><a href=\"https://link.zhihu.com/?target=http%3A//python.jobbole.com/85106/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">matplotlib 绘图可视化知识点整理 - Python - 伯乐在线</a><b><i>】</i></b></li><li>plt.axis([xmin, xmax, ymin, ymax])上面例子里的axis()命令给定了坐标范围。</li></ol><hr/><p><b><i>3，numpy取整函数</i></b></p><p>一，round四舍五入</p><div class=\"highlight\"><pre><code class=\"language-text\">a = np.float(3.3)\nprint(np.round(a))\nb = np.float(3.6)\nprint(np.round(b))\n输出：\n3\n4</code></pre></div><p>二，floor向下取整</p><div class=\"highlight\"><pre><code class=\"language-text\">a = np.float(3.3)\nprint(np.floor(a))\nb = np.float(3.6)\nprint(np.floor(b))\n输出：\n3.0\n3.0</code></pre></div><p>三，ceil向上取整</p><div class=\"highlight\"><pre><code class=\"language-text\">a = np.float(3.3)\nprint(np.ceil(a))\nb = np.float(3.6)\nprint(np.ceil(b))\n输出：\n4.0\n4.0</code></pre></div><hr/><p><b><i>4，一个小任务，把范围从0到255每一个值都可以取到的一组数，规整到0到255但只可以去其中的10个数</i></b></p><div class=\"highlight\"><pre><code class=\"language-text\">testA = SpeechGenerator(testimgdirA,speechlen)\nA_speech_batch = next(testA)\nAO_speech_batch = np.round(A_speech_batch/25.)*25\nC_speech_batch = np.concatenate([AO_speech_batch,A_speech_batch],axis=-1)\nplt.plot(range(len(C_speech_batch[0])),C_speech_batch[0])\nplt.show()</code></pre></div><p><b><i>注：A_speech_batch/25.因为A_speech_batch为float数据类型，它只可以和同类型的数相除。所以25后面要加一个小点</i></b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-5d32ab4aa2b0120940b3018b2a655e2c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"581\" data-rawheight=\"440\" class=\"origin_image zh-lightbox-thumb\" width=\"581\" data-original=\"https://pic1.zhimg.com/v2-5d32ab4aa2b0120940b3018b2a655e2c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;581&#39; height=&#39;440&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"581\" data-rawheight=\"440\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"581\" data-original=\"https://pic1.zhimg.com/v2-5d32ab4aa2b0120940b3018b2a655e2c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-5d32ab4aa2b0120940b3018b2a655e2c_b.jpg\"/></figure><p>欢迎关注公众号：huangxiaobai880</p><a class=\"video-box\" href=\"https://link.zhihu.com/?target=https%3A//www.zhihu.com/video/924616677998620672\" target=\"_blank\" data-video-id=\"\" data-video-playable=\"true\" data-name=\"\" data-poster=\"https://pic4.zhimg.com/80/v2-cda01aabf3a7ee3ecfb021cffadd137b_b.jpg\" data-lens-id=\"924616677998620672\"><img class=\"thumbnail\" src=\"https://pic4.zhimg.com/80/v2-cda01aabf3a7ee3ecfb021cffadd137b_b.jpg\"/><span class=\"content\"><span class=\"title\"><span class=\"z-ico-extern-gray\"></span><span class=\"z-ico-extern-blue\"></span></span><span class=\"url\"><span class=\"z-ico-video\"></span>https://www.zhihu.com/video/924616677998620672</span></span></a><p></p>", 
            "topic": [
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }, 
                {
                    "tag": "numpy", 
                    "tagLink": "https://api.zhihu.com/topics/19834165"
                }
            ], 
            "comments": []
        }
    ], 
    "url": "https://zhuanlan.zhihu.com/c_148591389"
}
