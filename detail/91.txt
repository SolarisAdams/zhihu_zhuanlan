{
    "title": "小强学AI", 
    "description": "AI是一个非常广泛的领域，我们从这里开始一步一步的进行AI的探索之路，让AI成为我们生活的一部分，让我们的生活更开心、更快乐", 
    "followers": [
        "https://www.zhihu.com/people/mo-mo-20-67-24", 
        "https://www.zhihu.com/people/qiaoliangxiang", 
        "https://www.zhihu.com/people/heyang-36", 
        "https://www.zhihu.com/people/zhang-you-mu-2", 
        "https://www.zhihu.com/people/rui-32-59-9", 
        "https://www.zhihu.com/people/wang-cong-51-28-15", 
        "https://www.zhihu.com/people/louis-38-16-73", 
        "https://www.zhihu.com/people/neko-28-33", 
        "https://www.zhihu.com/people/oscar-li-50", 
        "https://www.zhihu.com/people/xiao-ai-78-63", 
        "https://www.zhihu.com/people/tai-yang-cao-92", 
        "https://www.zhihu.com/people/lou-zhu-bie-bian-gu-shi-zhuang-bi-liao", 
        "https://www.zhihu.com/people/Steven_Jokes", 
        "https://www.zhihu.com/people/Leezhi403", 
        "https://www.zhihu.com/people/kong-chui-shun", 
        "https://www.zhihu.com/people/qiao-xu-26", 
        "https://www.zhihu.com/people/cloverying-85", 
        "https://www.zhihu.com/people/liang-51-3", 
        "https://www.zhihu.com/people/dafeng0321", 
        "https://www.zhihu.com/people/qianbuduo", 
        "https://www.zhihu.com/people/thisisbill", 
        "https://www.zhihu.com/people/jasonbourne110", 
        "https://www.zhihu.com/people/nasa-7-18", 
        "https://www.zhihu.com/people/gogo-74-91", 
        "https://www.zhihu.com/people/cui-ji-35", 
        "https://www.zhihu.com/people/lu-guo-de-xxx", 
        "https://www.zhihu.com/people/wang-tian-yuan-77", 
        "https://www.zhihu.com/people/owen-wang-24-1", 
        "https://www.zhihu.com/people/zhong-er-cheng-xu-yuan", 
        "https://www.zhihu.com/people/yongle-li-86", 
        "https://www.zhihu.com/people/xiaoxuanzaizai", 
        "https://www.zhihu.com/people/xia-72-78", 
        "https://www.zhihu.com/people/zhang-dong-67", 
        "https://www.zhihu.com/people/lutingming", 
        "https://www.zhihu.com/people/xiao-liu-mang-72-62", 
        "https://www.zhihu.com/people/larryang", 
        "https://www.zhihu.com/people/chen-chong-zheng-35", 
        "https://www.zhihu.com/people/shui-jia-xiao-huo-zi", 
        "https://www.zhihu.com/people/bin-li-44", 
        "https://www.zhihu.com/people/bu-xiang-chang-da-de-min-xiao-min", 
        "https://www.zhihu.com/people/liu-zhi-hu-25-51", 
        "https://www.zhihu.com/people/zhang-wen-tao-56-51", 
        "https://www.zhihu.com/people/geekchu", 
        "https://www.zhihu.com/people/he-zhi-qiang-52-74", 
        "https://www.zhihu.com/people/xxba-42", 
        "https://www.zhihu.com/people/xia-mi-41-21", 
        "https://www.zhihu.com/people/hai-shi-shen-lou-86", 
        "https://www.zhihu.com/people/liu-guo-qing-78-65", 
        "https://www.zhihu.com/people/mzf-93", 
        "https://www.zhihu.com/people/wei-yu-cong-61-9", 
        "https://www.zhihu.com/people/wpp-53-72", 
        "https://www.zhihu.com/people/luo-dan-de-qing-ren-77", 
        "https://www.zhihu.com/people/fu-max", 
        "https://www.zhihu.com/people/skyexin-70", 
        "https://www.zhihu.com/people/y-l-8-56", 
        "https://www.zhihu.com/people/wang-jing-69-10", 
        "https://www.zhihu.com/people/yang-lue-78", 
        "https://www.zhihu.com/people/huang-xing-10", 
        "https://www.zhihu.com/people/rumor-lee", 
        "https://www.zhihu.com/people/jeff-soong", 
        "https://www.zhihu.com/people/yang-troy-89", 
        "https://www.zhihu.com/people/willwinworld", 
        "https://www.zhihu.com/people/jian-jian-86-61", 
        "https://www.zhihu.com/people/sgs1991", 
        "https://www.zhihu.com/people/xu-jia-xuan-57", 
        "https://www.zhihu.com/people/chen-cai-43-25", 
        "https://www.zhihu.com/people/han-ma-94-6", 
        "https://www.zhihu.com/people/sfsf-88", 
        "https://www.zhihu.com/people/wu-xiao-18-45", 
        "https://www.zhihu.com/people/xu-kai-71-15", 
        "https://www.zhihu.com/people/cai-jun-39-90", 
        "https://www.zhihu.com/people/srlst", 
        "https://www.zhihu.com/people/mysh", 
        "https://www.zhihu.com/people/z-1222", 
        "https://www.zhihu.com/people/alwaysnow", 
        "https://www.zhihu.com/people/hallen-94", 
        "https://www.zhihu.com/people/mathshenli", 
        "https://www.zhihu.com/people/sun-zi-long-6", 
        "https://www.zhihu.com/people/zhi-bai-41-24", 
        "https://www.zhihu.com/people/weikang-ning", 
        "https://www.zhihu.com/people/xie-zhi-peng-2", 
        "https://www.zhihu.com/people/air-69-45", 
        "https://www.zhihu.com/people/xing-xing-85-75", 
        "https://www.zhihu.com/people/qing-qing-5-14", 
        "https://www.zhihu.com/people/chen-xi-63-6", 
        "https://www.zhihu.com/people/homer-wong-33", 
        "https://www.zhihu.com/people/shi-huai-2-67", 
        "https://www.zhihu.com/people/anny-14-93", 
        "https://www.zhihu.com/people/wang-shou-yi-60", 
        "https://www.zhihu.com/people/zdmbody", 
        "https://www.zhihu.com/people/xiao-qiang-78-77-59", 
        "https://www.zhihu.com/people/yang-fan-92-39", 
        "https://www.zhihu.com/people/hahaha-25-91", 
        "https://www.zhihu.com/people/jiu-ye-20-63", 
        "https://www.zhihu.com/people/tian-kong-88-19", 
        "https://www.zhihu.com/people/ji-le-chen-66", 
        "https://www.zhihu.com/people/g-sh-20", 
        "https://www.zhihu.com/people/lin-man-xiang-93", 
        "https://www.zhihu.com/people/wei-zhang-78-2", 
        "https://www.zhihu.com/people/hao-xue-6-2", 
        "https://www.zhihu.com/people/syoalg", 
        "https://www.zhihu.com/people/wai-xing-yi-ke", 
        "https://www.zhihu.com/people/wangtw", 
        "https://www.zhihu.com/people/clair.com", 
        "https://www.zhihu.com/people/shen-zi-jian-mo-ru-yun-piao-bo-88", 
        "https://www.zhihu.com/people/wei-rang-jian", 
        "https://www.zhihu.com/people/ai-rui-ke-17-84", 
        "https://www.zhihu.com/people/ricahrd_cai", 
        "https://www.zhihu.com/people/ququququqw", 
        "https://www.zhihu.com/people/li-shi-lin-93-31", 
        "https://www.zhihu.com/people/cui-yu-95-33", 
        "https://www.zhihu.com/people/ge-qi-xin-96", 
        "https://www.zhihu.com/people/wang-peng-cheng-39-36", 
        "https://www.zhihu.com/people/Ja1r0", 
        "https://www.zhihu.com/people/wzdnzd", 
        "https://www.zhihu.com/people/zhao-dou-dou-14-68", 
        "https://www.zhihu.com/people/shao-tian-zun", 
        "https://www.zhihu.com/people/angyneo", 
        "https://www.zhihu.com/people/zi-xuan-54-49", 
        "https://www.zhihu.com/people/xiao-qi-e-77-51", 
        "https://www.zhihu.com/people/zhe-ming-62", 
        "https://www.zhihu.com/people/xuefeng.zeng", 
        "https://www.zhihu.com/people/ding-yu-jie-58", 
        "https://www.zhihu.com/people/lixxx333", 
        "https://www.zhihu.com/people/xian-meng-64", 
        "https://www.zhihu.com/people/zhen-yuan-zi-58", 
        "https://www.zhihu.com/people/xuan-feng-liu-huo", 
        "https://www.zhihu.com/people/qu-guang", 
        "https://www.zhihu.com/people/zzzzzsad", 
        "https://www.zhihu.com/people/james-liu-62-62", 
        "https://www.zhihu.com/people/meng-tian-long-98", 
        "https://www.zhihu.com/people/hu-hu-hu-27-83", 
        "https://www.zhihu.com/people/xiao-hong-hua-71", 
        "https://www.zhihu.com/people/tu-miao-14", 
        "https://www.zhihu.com/people/chi-hong-liang", 
        "https://www.zhihu.com/people/qi-lan-91-31", 
        "https://www.zhihu.com/people/qin-ai-de-da-bao", 
        "https://www.zhihu.com/people/wu-yy-23", 
        "https://www.zhihu.com/people/wang-er-jie", 
        "https://www.zhihu.com/people/li-mo-8-74-27", 
        "https://www.zhihu.com/people/nei-xin-you-ge-xiao-nu-sheng", 
        "https://www.zhihu.com/people/David.Sunny", 
        "https://www.zhihu.com/people/qiu-zhi-58-84", 
        "https://www.zhihu.com/people/tian-cun-chun-shu", 
        "https://www.zhihu.com/people/shao-zi-55-28", 
        "https://www.zhihu.com/people/zhang-xiao-yue-35", 
        "https://www.zhihu.com/people/fang-sheng-feng", 
        "https://www.zhihu.com/people/Realbeige", 
        "https://www.zhihu.com/people/la-geek", 
        "https://www.zhihu.com/people/xiao-du-zhi-jin", 
        "https://www.zhihu.com/people/wan-er-23-10", 
        "https://www.zhihu.com/people/kankan", 
        "https://www.zhihu.com/people/xing-sheng-yu-yan-16", 
        "https://www.zhihu.com/people/wang-wei-14-18-90", 
        "https://www.zhihu.com/people/xiao-jian-38-77", 
        "https://www.zhihu.com/people/liu-yinhex", 
        "https://www.zhihu.com/people/cseeea", 
        "https://www.zhihu.com/people/wei-xin-81-7", 
        "https://www.zhihu.com/people/gu-yu-71-65", 
        "https://www.zhihu.com/people/Gtesla-10-49-76", 
        "https://www.zhihu.com/people/ma-zhen-70-43", 
        "https://www.zhihu.com/people/wang-kuan-22-74", 
        "https://www.zhihu.com/people/zone-26-42", 
        "https://www.zhihu.com/people/netlab7", 
        "https://www.zhihu.com/people/yong-yuan-qian-xing-92", 
        "https://www.zhihu.com/people/zzyx-zhihu", 
        "https://www.zhihu.com/people/yang-long-83-54", 
        "https://www.zhihu.com/people/huanyan3761", 
        "https://www.zhihu.com/people/qichao-tang", 
        "https://www.zhihu.com/people/wu-hua-27-44", 
        "https://www.zhihu.com/people/Mercedesbenz", 
        "https://www.zhihu.com/people/ru-yi-2-95", 
        "https://www.zhihu.com/people/maocaowu1268"
    ], 
    "article": [
        {
            "url": "https://zhuanlan.zhihu.com/p/26345801", 
            "userName": "李锋", 
            "userLink": "https://www.zhihu.com/people/4c1cb1778179af104f87eb56111ff960", 
            "upvote": 1, 
            "title": "深层学习入门的误区", 
            "content": "<p>来源于网络<figure><noscript><img src=\"https://pic2.zhimg.com/v2-02019868f594d170a70374be2dc23af1_b.png\" data-rawwidth=\"1548\" data-rawheight=\"752\" class=\"origin_image zh-lightbox-thumb\" width=\"1548\" data-original=\"https://pic2.zhimg.com/v2-02019868f594d170a70374be2dc23af1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1548&#39; height=&#39;752&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1548\" data-rawheight=\"752\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1548\" data-original=\"https://pic2.zhimg.com/v2-02019868f594d170a70374be2dc23af1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-02019868f594d170a70374be2dc23af1_b.png\"/></figure>---------------------------------------------------------------------------------------<figure><noscript><img src=\"https://pic4.zhimg.com/v2-e1b39dd64a27a1b7281707ab8b2cce7f_b.png\" data-rawwidth=\"1542\" data-rawheight=\"788\" class=\"origin_image zh-lightbox-thumb\" width=\"1542\" data-original=\"https://pic4.zhimg.com/v2-e1b39dd64a27a1b7281707ab8b2cce7f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1542&#39; height=&#39;788&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1542\" data-rawheight=\"788\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1542\" data-original=\"https://pic4.zhimg.com/v2-e1b39dd64a27a1b7281707ab8b2cce7f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-e1b39dd64a27a1b7281707ab8b2cce7f_b.png\"/></figure>---------------------------------------------------------------------------------------<figure><noscript><img src=\"https://pic1.zhimg.com/v2-1910a01d58a7e74be38ec302cd946f00_b.png\" data-rawwidth=\"1552\" data-rawheight=\"802\" class=\"origin_image zh-lightbox-thumb\" width=\"1552\" data-original=\"https://pic1.zhimg.com/v2-1910a01d58a7e74be38ec302cd946f00_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1552&#39; height=&#39;802&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1552\" data-rawheight=\"802\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1552\" data-original=\"https://pic1.zhimg.com/v2-1910a01d58a7e74be38ec302cd946f00_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-1910a01d58a7e74be38ec302cd946f00_b.png\"/></figure>---------------------------------------------------------------------------------------<figure><noscript><img src=\"https://pic4.zhimg.com/v2-22cbcbf11a3a8a1b2ecb5ab6aa97cef7_b.png\" data-rawwidth=\"1546\" data-rawheight=\"784\" class=\"origin_image zh-lightbox-thumb\" width=\"1546\" data-original=\"https://pic4.zhimg.com/v2-22cbcbf11a3a8a1b2ecb5ab6aa97cef7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1546&#39; height=&#39;784&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1546\" data-rawheight=\"784\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1546\" data-original=\"https://pic4.zhimg.com/v2-22cbcbf11a3a8a1b2ecb5ab6aa97cef7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-22cbcbf11a3a8a1b2ecb5ab6aa97cef7_b.png\"/></figure>---------------------------------------------------------------------------------------<figure><noscript><img src=\"https://pic2.zhimg.com/v2-3027708dc0c0664900ddc6e5063f66a9_b.png\" data-rawwidth=\"1550\" data-rawheight=\"786\" class=\"origin_image zh-lightbox-thumb\" width=\"1550\" data-original=\"https://pic2.zhimg.com/v2-3027708dc0c0664900ddc6e5063f66a9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1550&#39; height=&#39;786&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1550\" data-rawheight=\"786\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1550\" data-original=\"https://pic2.zhimg.com/v2-3027708dc0c0664900ddc6e5063f66a9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-3027708dc0c0664900ddc6e5063f66a9_b.png\"/></figure>---------------------------------------------------------------------------------------<figure><noscript><img src=\"https://pic1.zhimg.com/v2-191c018b1cbc788d066daa6aacab6c20_b.png\" data-rawwidth=\"1546\" data-rawheight=\"788\" class=\"origin_image zh-lightbox-thumb\" width=\"1546\" data-original=\"https://pic1.zhimg.com/v2-191c018b1cbc788d066daa6aacab6c20_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1546&#39; height=&#39;788&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1546\" data-rawheight=\"788\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1546\" data-original=\"https://pic1.zhimg.com/v2-191c018b1cbc788d066daa6aacab6c20_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-191c018b1cbc788d066daa6aacab6c20_b.png\"/></figure>---------------------------------------------------------------------------------------<figure><noscript><img src=\"https://pic3.zhimg.com/v2-746d6fa51ac9861b924ae83e0ca86b7e_b.png\" data-rawwidth=\"1546\" data-rawheight=\"812\" class=\"origin_image zh-lightbox-thumb\" width=\"1546\" data-original=\"https://pic3.zhimg.com/v2-746d6fa51ac9861b924ae83e0ca86b7e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1546&#39; height=&#39;812&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1546\" data-rawheight=\"812\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1546\" data-original=\"https://pic3.zhimg.com/v2-746d6fa51ac9861b924ae83e0ca86b7e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-746d6fa51ac9861b924ae83e0ca86b7e_b.png\"/></figure>---------------------------------------------------------------------------------------<figure><noscript><img src=\"https://pic3.zhimg.com/v2-6fe02c0250f051a14c50a097ff74d982_b.png\" data-rawwidth=\"1542\" data-rawheight=\"782\" class=\"origin_image zh-lightbox-thumb\" width=\"1542\" data-original=\"https://pic3.zhimg.com/v2-6fe02c0250f051a14c50a097ff74d982_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1542&#39; height=&#39;782&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1542\" data-rawheight=\"782\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1542\" data-original=\"https://pic3.zhimg.com/v2-6fe02c0250f051a14c50a097ff74d982_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-6fe02c0250f051a14c50a097ff74d982_b.png\"/></figure>---------------------------------------------------------------------------------------<figure><noscript><img src=\"https://pic1.zhimg.com/v2-14b89199419730cc1fbd0768ad5dad70_b.png\" data-rawwidth=\"1530\" data-rawheight=\"766\" class=\"origin_image zh-lightbox-thumb\" width=\"1530\" data-original=\"https://pic1.zhimg.com/v2-14b89199419730cc1fbd0768ad5dad70_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1530&#39; height=&#39;766&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1530\" data-rawheight=\"766\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1530\" data-original=\"https://pic1.zhimg.com/v2-14b89199419730cc1fbd0768ad5dad70_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-14b89199419730cc1fbd0768ad5dad70_b.png\"/></figure>---------------------------------------------------------------------------------------<figure><noscript><img src=\"https://pic1.zhimg.com/v2-86d864cd0a78e9da98e3bd5ebedf780c_b.png\" data-rawwidth=\"1498\" data-rawheight=\"822\" class=\"origin_image zh-lightbox-thumb\" width=\"1498\" data-original=\"https://pic1.zhimg.com/v2-86d864cd0a78e9da98e3bd5ebedf780c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1498&#39; height=&#39;822&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1498\" data-rawheight=\"822\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1498\" data-original=\"https://pic1.zhimg.com/v2-86d864cd0a78e9da98e3bd5ebedf780c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-86d864cd0a78e9da98e3bd5ebedf780c_b.png\"/></figure>---------------------------------------------------------------------------------------<figure><noscript><img src=\"https://pic2.zhimg.com/v2-7dc6c952802152340789a921a907d8dd_b.png\" data-rawwidth=\"1542\" data-rawheight=\"792\" class=\"origin_image zh-lightbox-thumb\" width=\"1542\" data-original=\"https://pic2.zhimg.com/v2-7dc6c952802152340789a921a907d8dd_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1542&#39; height=&#39;792&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1542\" data-rawheight=\"792\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1542\" data-original=\"https://pic2.zhimg.com/v2-7dc6c952802152340789a921a907d8dd_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-7dc6c952802152340789a921a907d8dd_b.png\"/></figure>---------------------------------------------------------------------------------------<figure><noscript><img src=\"https://pic4.zhimg.com/v2-5bedb05df19c7490a0070dd9eea17b8f_b.png\" data-rawwidth=\"1538\" data-rawheight=\"802\" class=\"origin_image zh-lightbox-thumb\" width=\"1538\" data-original=\"https://pic4.zhimg.com/v2-5bedb05df19c7490a0070dd9eea17b8f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1538&#39; height=&#39;802&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1538\" data-rawheight=\"802\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1538\" data-original=\"https://pic4.zhimg.com/v2-5bedb05df19c7490a0070dd9eea17b8f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-5bedb05df19c7490a0070dd9eea17b8f_b.png\"/></figure>---------------------------------------------------------------------------------------<figure><noscript><img src=\"https://pic3.zhimg.com/v2-569c09ea52ec1f5d41e3ee86739aeb5e_b.png\" data-rawwidth=\"1542\" data-rawheight=\"810\" class=\"origin_image zh-lightbox-thumb\" width=\"1542\" data-original=\"https://pic3.zhimg.com/v2-569c09ea52ec1f5d41e3ee86739aeb5e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1542&#39; height=&#39;810&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1542\" data-rawheight=\"810\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1542\" data-original=\"https://pic3.zhimg.com/v2-569c09ea52ec1f5d41e3ee86739aeb5e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-569c09ea52ec1f5d41e3ee86739aeb5e_b.png\"/></figure>---------------------------------------------------------------------------------------<figure><noscript><img src=\"https://pic1.zhimg.com/v2-126787821997556eae4ba57ffcaf7b7c_b.png\" data-rawwidth=\"1504\" data-rawheight=\"812\" class=\"origin_image zh-lightbox-thumb\" width=\"1504\" data-original=\"https://pic1.zhimg.com/v2-126787821997556eae4ba57ffcaf7b7c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1504&#39; height=&#39;812&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1504\" data-rawheight=\"812\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1504\" data-original=\"https://pic1.zhimg.com/v2-126787821997556eae4ba57ffcaf7b7c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-126787821997556eae4ba57ffcaf7b7c_b.png\"/></figure>---------------------------------------------------------------------------------------<figure><noscript><img src=\"https://pic2.zhimg.com/v2-b9976f97a320e0a735f7eab423d8b4a1_b.png\" data-rawwidth=\"1542\" data-rawheight=\"782\" class=\"origin_image zh-lightbox-thumb\" width=\"1542\" data-original=\"https://pic2.zhimg.com/v2-b9976f97a320e0a735f7eab423d8b4a1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1542&#39; height=&#39;782&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1542\" data-rawheight=\"782\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1542\" data-original=\"https://pic2.zhimg.com/v2-b9976f97a320e0a735f7eab423d8b4a1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-b9976f97a320e0a735f7eab423d8b4a1_b.png\"/></figure>---------------------------------------------------------------------------------------<figure><noscript><img src=\"https://pic2.zhimg.com/v2-b897097a396f78550df5f6ae19f0578d_b.png\" data-rawwidth=\"1542\" data-rawheight=\"822\" class=\"origin_image zh-lightbox-thumb\" width=\"1542\" data-original=\"https://pic2.zhimg.com/v2-b897097a396f78550df5f6ae19f0578d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1542&#39; height=&#39;822&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1542\" data-rawheight=\"822\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1542\" data-original=\"https://pic2.zhimg.com/v2-b897097a396f78550df5f6ae19f0578d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-b897097a396f78550df5f6ae19f0578d_b.png\"/></figure>---------------------------------------------------------------------------------------<figure><noscript><img src=\"https://pic2.zhimg.com/v2-78b9471924f66cd9ce6cc52f29286fc1_b.png\" data-rawwidth=\"1546\" data-rawheight=\"824\" class=\"origin_image zh-lightbox-thumb\" width=\"1546\" data-original=\"https://pic2.zhimg.com/v2-78b9471924f66cd9ce6cc52f29286fc1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1546&#39; height=&#39;824&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1546\" data-rawheight=\"824\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1546\" data-original=\"https://pic2.zhimg.com/v2-78b9471924f66cd9ce6cc52f29286fc1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-78b9471924f66cd9ce6cc52f29286fc1_b.png\"/></figure>---------------------------------------------------------------------------------------<figure><noscript><img src=\"https://pic1.zhimg.com/v2-3e8acd9d2160420b2000ab21c659b378_b.png\" data-rawwidth=\"1538\" data-rawheight=\"812\" class=\"origin_image zh-lightbox-thumb\" width=\"1538\" data-original=\"https://pic1.zhimg.com/v2-3e8acd9d2160420b2000ab21c659b378_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1538&#39; height=&#39;812&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1538\" data-rawheight=\"812\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1538\" data-original=\"https://pic1.zhimg.com/v2-3e8acd9d2160420b2000ab21c659b378_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-3e8acd9d2160420b2000ab21c659b378_b.png\"/></figure>---------------------------------------------------------------------------------------<figure><noscript><img src=\"https://pic2.zhimg.com/v2-b7cfe5c02c805c4c2298595074d4e391_b.png\" data-rawwidth=\"1516\" data-rawheight=\"764\" class=\"origin_image zh-lightbox-thumb\" width=\"1516\" data-original=\"https://pic2.zhimg.com/v2-b7cfe5c02c805c4c2298595074d4e391_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1516&#39; height=&#39;764&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1516\" data-rawheight=\"764\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1516\" data-original=\"https://pic2.zhimg.com/v2-b7cfe5c02c805c4c2298595074d4e391_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-b7cfe5c02c805c4c2298595074d4e391_b.png\"/></figure>---------------------------------------------------------------------------------------<figure><noscript><img src=\"https://pic1.zhimg.com/v2-141ecc08ed22d74a5ed244e334ea5ba8_b.png\" data-rawwidth=\"1514\" data-rawheight=\"826\" class=\"origin_image zh-lightbox-thumb\" width=\"1514\" data-original=\"https://pic1.zhimg.com/v2-141ecc08ed22d74a5ed244e334ea5ba8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1514&#39; height=&#39;826&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1514\" data-rawheight=\"826\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1514\" data-original=\"https://pic1.zhimg.com/v2-141ecc08ed22d74a5ed244e334ea5ba8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-141ecc08ed22d74a5ed244e334ea5ba8_b.png\"/></figure>---------------------------------------------------------------------------------------<figure><noscript><img src=\"https://pic4.zhimg.com/v2-5c3eb4516946c8dac8a42a38ea5cc173_b.png\" data-rawwidth=\"1542\" data-rawheight=\"810\" class=\"origin_image zh-lightbox-thumb\" width=\"1542\" data-original=\"https://pic4.zhimg.com/v2-5c3eb4516946c8dac8a42a38ea5cc173_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1542&#39; height=&#39;810&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1542\" data-rawheight=\"810\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1542\" data-original=\"https://pic4.zhimg.com/v2-5c3eb4516946c8dac8a42a38ea5cc173_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-5c3eb4516946c8dac8a42a38ea5cc173_b.png\"/></figure>---------------------------------------------------------------------------------------<figure><noscript><img src=\"https://pic1.zhimg.com/v2-b06740bb8720b556aca819f938fd7828_b.png\" data-rawwidth=\"1530\" data-rawheight=\"816\" class=\"origin_image zh-lightbox-thumb\" width=\"1530\" data-original=\"https://pic1.zhimg.com/v2-b06740bb8720b556aca819f938fd7828_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1530&#39; height=&#39;816&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1530\" data-rawheight=\"816\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1530\" data-original=\"https://pic1.zhimg.com/v2-b06740bb8720b556aca819f938fd7828_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-b06740bb8720b556aca819f938fd7828_b.png\"/></figure>---------------------------------------------------------------------------------------<figure><noscript><img src=\"https://pic1.zhimg.com/v2-9c3f9a3c32d14012957a1c868fe45f54_b.png\" data-rawwidth=\"1540\" data-rawheight=\"816\" class=\"origin_image zh-lightbox-thumb\" width=\"1540\" data-original=\"https://pic1.zhimg.com/v2-9c3f9a3c32d14012957a1c868fe45f54_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1540&#39; height=&#39;816&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1540\" data-rawheight=\"816\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1540\" data-original=\"https://pic1.zhimg.com/v2-9c3f9a3c32d14012957a1c868fe45f54_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-9c3f9a3c32d14012957a1c868fe45f54_b.png\"/></figure>---------------------------------------------------------------------------------------<figure><noscript><img src=\"https://pic2.zhimg.com/v2-e0c21d5e0b4c0bf4143ab8cbeae378c9_b.png\" data-rawwidth=\"1562\" data-rawheight=\"822\" class=\"origin_image zh-lightbox-thumb\" width=\"1562\" data-original=\"https://pic2.zhimg.com/v2-e0c21d5e0b4c0bf4143ab8cbeae378c9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1562&#39; height=&#39;822&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1562\" data-rawheight=\"822\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1562\" data-original=\"https://pic2.zhimg.com/v2-e0c21d5e0b4c0bf4143ab8cbeae378c9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-e0c21d5e0b4c0bf4143ab8cbeae378c9_b.png\"/></figure>---------------------------------------------------------------------------------------<figure><noscript><img src=\"https://pic2.zhimg.com/v2-5f95a2cd4dfbe24d231a252ad8bade81_b.png\" data-rawwidth=\"1550\" data-rawheight=\"816\" class=\"origin_image zh-lightbox-thumb\" width=\"1550\" data-original=\"https://pic2.zhimg.com/v2-5f95a2cd4dfbe24d231a252ad8bade81_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1550&#39; height=&#39;816&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1550\" data-rawheight=\"816\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1550\" data-original=\"https://pic2.zhimg.com/v2-5f95a2cd4dfbe24d231a252ad8bade81_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-5f95a2cd4dfbe24d231a252ad8bade81_b.png\"/></figure>---------------------------------------------------------------------------------------<figure><noscript><img src=\"https://pic1.zhimg.com/v2-6230d795bac86e411bafddbe37c48564_b.png\" data-rawwidth=\"1534\" data-rawheight=\"816\" class=\"origin_image zh-lightbox-thumb\" width=\"1534\" data-original=\"https://pic1.zhimg.com/v2-6230d795bac86e411bafddbe37c48564_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1534&#39; height=&#39;816&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1534\" data-rawheight=\"816\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1534\" data-original=\"https://pic1.zhimg.com/v2-6230d795bac86e411bafddbe37c48564_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-6230d795bac86e411bafddbe37c48564_b.png\"/></figure>---------------------------------------------------------------------------------------<figure><noscript><img src=\"https://pic1.zhimg.com/v2-13224e8b631d593bfb7c4ffe2025b378_b.png\" data-rawwidth=\"1562\" data-rawheight=\"762\" class=\"origin_image zh-lightbox-thumb\" width=\"1562\" data-original=\"https://pic1.zhimg.com/v2-13224e8b631d593bfb7c4ffe2025b378_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1562&#39; height=&#39;762&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1562\" data-rawheight=\"762\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1562\" data-original=\"https://pic1.zhimg.com/v2-13224e8b631d593bfb7c4ffe2025b378_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-13224e8b631d593bfb7c4ffe2025b378_b.png\"/></figure>---------------------------------------------------------------------------------------<figure><noscript><img src=\"https://pic1.zhimg.com/v2-535be44e6ddda0e4b6ea5719d8b33038_b.png\" data-rawwidth=\"1500\" data-rawheight=\"806\" class=\"origin_image zh-lightbox-thumb\" width=\"1500\" data-original=\"https://pic1.zhimg.com/v2-535be44e6ddda0e4b6ea5719d8b33038_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1500&#39; height=&#39;806&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1500\" data-rawheight=\"806\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1500\" data-original=\"https://pic1.zhimg.com/v2-535be44e6ddda0e4b6ea5719d8b33038_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-535be44e6ddda0e4b6ea5719d8b33038_b.png\"/></figure>---------------------------------------------------------------------------------------</p>", 
            "topic": [
                {
                    "tag": "人工智能", 
                    "tagLink": "https://api.zhihu.com/topics/19551275"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/26344224", 
            "userName": "李锋", 
            "userLink": "https://www.zhihu.com/people/4c1cb1778179af104f87eb56111ff960", 
            "upvote": 2, 
            "title": "Yoshua Bengio大神深度学习实战方法论解读 — 模型评估， 超参数调优，网格搜索，调试策略", 
            "content": "<p>来源：<a href=\"https://link.zhihu.com/?target=http%3A//nooverfit.com/wp/yoshua-bengio%25e5%25a4%25a7%25e7%25a5%259e%25e6%25b7%25b1%25e5%25ba%25a6%25e5%25ad%25a6%25e4%25b9%25a0-%25e5%25ae%259e%25e6%2588%2598%25e6%2596%25b9%25e6%25b3%2595%25e8%25ae%25ba%25e8%25a7%25a3%25e8%25af%25bb-%25e6%25a8%25a1%25e5%259e%258b%25e8%25af%2584%25e4%25bc%25b0%25ef%25bc%258c-%25e8%25b6%2585%25e5%258f%2582/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Yoshua Bengio大神深度学习实战方法论解读 --- 模型评估， 超参数调优，网格搜索，调试策略</a><br/></p><p>现代深度学习或机器学习，很大程度上是把所有问题转化为同一个“<b>模型训练”问题</b>。如何解决这个<b>模型训练的问题</b>成为了<a href=\"https://link.zhihu.com/?target=http%3A//baike.baidu.com/item/%25E6%2595%25B0%25E6%258D%25AE%25E7%25A7%2591%25E5%25AD%25A6%25E5%25AE%25B6\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">数据科学家</a>们的主攻问题。</p><p>鲜为人知的是，设计<b>机器学习模型、训练算法和目标函数</b>仅仅是工作的一部分。还有很重要的一部分是：数据科学家们要对数据和问题有更深层次的理解，对于<b>模型评估</b>， <b>超参数调优</b>，<b>网格搜索</b>，<b>调试策略</b>都有相当的实践经验。</p><p>正如<a href=\"https://link.zhihu.com/?target=https%3A//github.com/HFTrader/DeepLearningBook\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Deep Learning（Ian Goodfellow Yoshua Bengio</a>）一书中所说：</p><blockquote><p>Correct application of an algorithm depends on mastering some fairly simple methodology</p></blockquote><p>掌握一些简单的实战方法论，是不可或缺的增益。现在我们就把书中常用实战技巧总结给大家，相信有启示意义。</p><h2>模型评估</h2>\n来自：<a href=\"https://link.zhihu.com/?target=https%3A//www.slideshare.net/databricks/practical-machine-learning-pipelines-with-mllib\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">slideshare.net/databric</span><span class=\"invisible\">ks/practical-machine-learning-pipelines-with-mllib</span><span class=\"ellipsis\"></span></a> 的ML训练例子<p>按照<a href=\"https://link.zhihu.com/?target=https%3A//github.com/HFTrader/DeepLearningBook\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Deep Learning</a>一书中的说法是“性能度量”，但我更倾向于翻译为“模型评估”。“模型评估”看似是ML训练工作流的最后一步，但是这其实是要在拿到数据集时就应该认真考虑的。</p><p>所谓的测试集“<b>预测准确率</b>”或“<b>错误率</b>”在很多实际应用中是不够的。我们在一切训练工作前，就要确定一个实际有意义的目标。</p><p>比如，垃圾邮件检测系统会有两种错误：将正常邮件错误地归为垃圾邮件，将垃圾邮件错误地归为正常邮件。 阻止正常消息比允许可疑消息通过糟糕得多。</p><p>又比如，对于一种罕见疾病设计医疗测试。 假设每一百万人中只有一人患病。 我们只需要让分类器一直报告没有患者，就能轻易地在检测任务上实现99.9999%的正确率。 显然，正确率很难描述这种系统的性能。这就是常见的“<b>非平衡集</b>”预测。我们要关注的是那个极少量的标签。</p><p>对于非平衡集预测性能评估，我们有一个直观好用的工具：<a href=\"https://link.zhihu.com/?target=http%3A//baike.baidu.com/item/%25E6%25B7%25B7%25E6%25B7%2586%25E7%259F%25A9%25E9%2598%25B5\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">混淆矩阵</a></p>\n来自：<a href=\"https://link.zhihu.com/?target=http%3A//stats.stackexchange.com/questions/179835/how-to-build-a-confusion-matrix-for-a-multiclass-classifier\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">How to build a confusion matrix for a multiclass classifier?</a><p><b>横向的是模型对类别的选择，纵向是真实的类别标签</b>。所以落在对接线上的值是模型精确地分对类别的数量。对于特定应用，关注对角线是不够的，我们有时需要关注每个类各自的错误率（上图最右侧列）；有时，我们需要关注当模型区分某个类时，它的正确率（观察上图每一列数值）。</p><p>当然，还有许多其他的性能度量。 例如，我们可以度量点击率、收集用户满意度调查等等。 许多专业的应用领域也有特定的标准。</p><h2>是否收集更多数据？</h2><p>首先，确定训练集上的性能是否可接受。 如果模型在训练集上的性能就很差，学习算法都不能在训练集上学习出良好的模型，那么就没必要收集更多的数据。</p><p>如果训练集上的性能是可接受的，那么我们开始度量测试集上的性能。 如果测试集上的性能也是可以接受的，那么就顺利完成了。 <b>如果测试集上的性能比训练集的要差得多</b>，那么收集更多的数据是最有效的解决方案之一。</p><h2>超参数调优</h2><h4>手动调整超参数</h4><p>学习率可能是最重要的超参数。一张经典的图令人记忆深刻：</p>\n来自：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/HFTrader/DeepLearningBook\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">HFTrader/DeepLearningBook</a><blockquote><p>注意当<b>学习率</b>大于最优值时误差会有显著的提升。此图针对固定的训练时间，越小的学习率有时候可以以一个正比于学习率减小量的因素来减慢训练过程。泛化误差也会得到类似的曲线，由于正则项作用在学习率过大或过小处比较复杂。由于一个糟糕的优化从某种程度上说可以避免过拟合，即使是训练误差相同的点也会拥有完全不同的泛化误差。</p></blockquote><p>调整学习率外的其他参数时，需要同时监测训练误差和测试误差，以判断模型是否过拟合或欠拟合，然后适当调整其容量。必须知道调整后对模型的影响，其中很重要的一个影响是<b>模型容量（模型复杂度）</b>：</p>\n来自：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/HFTrader/DeepLearningBook\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">HFTrader/DeepLearningBook</a><h4>自动超参数优化算法</h4>网格搜索<p>如果有三个或更少的超参数时，常见的超参数搜索方法是网格搜索。 对于每个超参数，使用者选择一个较小的有限值集去探索。 然后，这些超参数笛卡尔乘积得到一组组超参数，网格搜索使用每组超参数训练模型。 挑选验证集误差最小的超参数作为最好的超参数。</p>随机搜索<p>如果超参数较多， 首先，我们为每个超参数定义一个边缘分布，在这些边缘分布上进行搜索。</p><p>所以总结<b>网格搜索</b>和<b>随机搜索</b>，<b>网格搜索是通过排列组合调整超参数，随机搜索是通过边缘分布调整超参数：</b></p><h2>调试策略</h2><p>当一个机器学习系统效果不好时，通常很难判断效果不好的原因是算法本身，还是算法代码编写错误。但是有一些小技巧可以记住：</p><ol><li><b>观察模型行为，找到异常的错误行为</b>。这个策略似乎非常简单，但是很多人都会忽视在训练完成之后，看看模型的<b>预测行为</b>，或者<b>生成模型的生成行为</b>有什么异常的地方。</li><li><b>观察训练误差和测试误差。</b>如果训练误差较低，但是测试误差较高，那么很有可能训练过程是在正常运行，但模型由于算法原因过拟合了。\n 如果测试误差没有被正确地度量，可能是由于训练后保存模型再重载去度量测试集时出现问题，或者是因为测试数据和训练数据预处理的方式不同。 \n如果训练和测试误差都很高，那么很难确定是软件错误，还是由于算法原因模型欠拟合。</li><li><b>拟合较小数据集。</b>现在一个小一点的数据集上跑训练，会有宏观的调试感觉。</li><li><b>打印有价值的log。</b>在训练的每一轮，打印有价值的log，并且可视化。</li><li>许多深度学习的每一步迭代到下一次迭代都会有一系列影响和特征，如，目标函数值变小，梯度变小，这些影响和特征也是重要线索。</li></ol><p>参考文献：</p><ol><li><a href=\"https://link.zhihu.com/?target=https%3A//github.com/HFTrader/DeepLearningBook\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">HFTrader/DeepLearningBook</a></li><li><a href=\"https://link.zhihu.com/?target=http%3A//machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">How to Grid Search Hyperparameters for Deep Learning Models in Python With Keras - Machine Learning Mastery</a></li><li><a href=\"https://link.zhihu.com/?target=https%3A//deeplearning4j.org/questions.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Questions to Ask When Applying Deep Learning</a></li><li><a href=\"https://link.zhihu.com/?target=https%3A//github.com/exacity/deeplearningbook-chinese\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">exacity/deeplearningbook-chinese</a></li><li><a href=\"https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/VC_dimension\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">VC dimension - Wikipedia</a></li></ol>", 
            "topic": [
                {
                    "tag": "人工智能", 
                    "tagLink": "https://api.zhihu.com/topics/19551275"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/26337563", 
            "userName": "李锋", 
            "userLink": "https://www.zhihu.com/people/4c1cb1778179af104f87eb56111ff960", 
            "upvote": 19, 
            "title": "常见的几种最优化方法", 
            "content": "<p>来源：<a href=\"https://link.zhihu.com/?target=http%3A//nooverfit.com\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">nooverfit.com</span><span class=\"invisible\"></span></a><br/></p><p>我们每个人都会在我们的生活或者工作中遇到各种各样的最优化问题，比如每个企业和个人都要考虑的一个问题“在一定成本下，如何使利润最大化”等。最优化方法是一种数学方法，它是研究在给定约束之下如何寻求某些因素(的量)，以使某一(或某些)指标达到最优的一些学科的总称。随着学习的深入，博主越来越发现最优化方法的重要性，学习和工作中遇到的大多问题都可以建模成一种最优化模型进行求解，比如我们现在学习的机器学习算法，大部分的机器学习算法的本质都是建立优化模型，通过最优化方法对目标函数（或损失函数）进行优化，从而训练出最好的模型。常见的最优化方法有梯度下降法、牛顿法和拟牛顿法、共轭梯度法等等。</p><h3>1. 梯度下降法（Gradient Descent）</h3><p>梯度下降法是最早最简单，也是最为常用的最优化方法。梯度下降法实现简单，当目标函数是凸函数时，梯度下降法的解是全局解。一般情况下，其解不保证是全局最优解，梯度下降法的速度也未必是最快的。<b>梯度下降法的优化思想是用当前位置负梯度方向作为搜索方向，因为该方向为当前位置的最快下降方向，所以也被称为是”最速下降法“。最速下降法越接近目标值，步长越小，前进越慢。</b>梯度下降法的搜索迭代示意图如下图所示：</p><p><b>梯度下降法的缺点：</b></p><p><b>　　（1）靠近极小值时收敛速度减慢，如下图所示；</b></p><p><b>　　（2）直线搜索时可能会产生一些问题；</b></p><p><b>　　（3）可能会“之字形”地下降。</b></p><p>从上图可以看出，梯度下降法在接近最优解的区域收敛速度明显变慢，利用梯度下降法求解需要很多次的迭代。</p><p>在机器学习中，基于基本的梯度下降法发展了两种梯度下降方法，分别为随机梯度下降法和批量梯度下降法。</p><p>比如对一个线性回归（Linear\n \nLogistics）模型，假设下面的h(x)是要拟合的函数，J(theta)为损失函数，theta是参数，要迭代求解的值，theta求解出来了那最终要拟合的函数h(theta)就出来了。其中m是训练集的样本个数，n是特征的个数。</p><p><b>1）批量梯度下降法（Batch Gradient Descent，BGD）</b></p><p>（1）将J(theta)对theta求偏导，得到每个theta对应的的梯度：</p><p>（2）由于是要最小化风险函数，所以按每个参数theta的梯度负方向，来更新每个theta：</p><p>（3）从上面公式可以注意到，它得到的是一个全局最优解，但是每迭代一步，都要用到训练集所有的数据，如果m很大，那么可想而知这种方法的迭代速度会相当的慢。所以，这就引入了另外一种方法——随机梯度下降。</p><p>对于批量梯度下降法，样本个数m，x为n维向量，一次迭代需要把m个样本全部带入计算，迭代一次计算量为m*n</p><blockquote>2</blockquote>。<p><b>　　2）随机梯度下降（Random Gradient Descent，RGD）</b></p><p>（1）上面的风险函数可以写成如下这种形式，损失函数对应的是训练集中每个样本的粒度，而上面批量梯度下降对应的是所有的训练样本：</p><p>（2）每个样本的损失函数，对theta求偏导得到对应梯度，来更新theta：</p><p>（3）随机梯度下降是通过每个样本来迭代更新一次，如果样本量很大的情况（例如几十万），那么可能只用其中几万条或者几千条的样本，就已经将theta迭代到最优解了，对比上面的批量梯度下降，迭代一次需要用到十几万训练样本，一次迭代不可能最优，如果迭代10次的话就需要遍历训练样本10次。但是，SGD伴随的一个问题是噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。</p><p>随机梯度下降每次迭代只使用一个样本，迭代一次计算量为n</p><blockquote>2</blockquote>，当样本个数m很大的时候，随机梯度下降迭代一次的速度要远高于批量梯度下降方法。<b>两者的关系可以这样理解：随机梯度下降方法以损失很小的一部分精确度和增加一定数量的迭代次数为代价，换取了总体的优化效率的提升。增加的迭代次数远远小于样本的数量。</b><p><b>对批量梯度下降法和随机梯度下降法的总结：</b></p><p><b>批量梯度下降—最小化所有训练样本的损失函数，使得最终求解的是全局的最优解，即求解的参数是使得风险函数最小，但是对于大规模样本问题效率低下。</b></p><p><b>　　随机梯度下降—最小化每条样本的损失函数，虽然不是每次迭代得到的损失函数都向着全局最优方向， 但是大的整体的方向是向全局最优解的，最终的结果往往是在全局最优解附近，适用于大规模训练样本情况。</b></p><h3>2. 牛顿法和拟牛顿法（Newton’s method &amp; Quasi-Newton Methods）</h3><p><b>1）牛顿法（Newton’s method）</b></p><p>牛顿法是一种在实数域和复数域上近似求解方程的方法。方法使用函数<i>f </i>(<i>x</i>)的泰勒级数的前面几项来寻找方程<i>f </i>(<i>x</i>) = 0的根。牛顿法最大的特点就在于它的收敛速度很快。</p><h2>　　具体步骤：</h2><p>首先，选择一个接近函数 <i>f </i>(<i>x</i>)零点的 <i>x</i>0，计算相应的 <i>f </i>(<i>x</i>0) 和切线斜率<i>f  ‘ </i>(<i>x</i>0)（这里<i>f ‘ </i>表示函数 <i>f  </i>的导数）。然后我们计算穿过点(<i>x</i>0,  <i>f  </i>(<i>x</i>0)) 并且斜率为<i>f </i>‘(<i>x</i>0)的直线和 <i>x </i>轴的交点的<i>x</i>坐标，也就是求如下方程的解：</p><p>我们将新求得的点的 <i>x </i>坐标命名为<i>x</i>1，通常<i>x</i>1会比<i>x</i>0更接近方程<i>f  </i>(<i>x</i>) = 0的解。因此我们现在可以利用<i>x</i>1开始下一轮迭代。迭代公式可化简为如下所示：</p><p>已经证明，如果<i>f  </i>‘ 是连续的，并且待求的零点<i>x</i>是孤立的，那么在零点<i>x</i>周围存在一个区域，只要初始值<i>x</i>0位于这个邻近区域内，那么牛顿法必定收敛。 并且，如果<i>f  </i>‘ (<i>x</i>)不为0, 那么牛顿法将具有平方收敛的性能. 粗略的说，这意味着每迭代一次，牛顿法结果的有效数字将增加一倍。</p><p>由于牛顿法是基于当前位置的切线来确定下一次的位置，所以牛顿法又被很形象地称为是”切线法”。牛顿法的搜索路径（二维情况）如下图所示：</p><p>牛顿法搜索动态示例图：</p><p><b>关于牛顿法和梯度下降法的效率对比：</b></p><p><b>　　从本质上去看，牛顿法是二阶收敛，梯度下降是一阶收敛，所以牛顿法就更快。如果更通俗地说的话，比如你想找一条最短的路径走到一个盆地的最底部，梯度下降法每次只从你当前所处位置选一个坡度最大的方向走一步，牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会变得更大。所以，可以说牛顿法比梯度下降法看得更远一点，能更快地走到最底部。（牛顿法目光更加长远，所以少走弯路；相对而言，梯度下降法只考虑了局部的最优，没有全局思想。）</b></p><p><b>　　根据wiki上的解释，从几何上说，牛顿法就是用一个二次曲面去拟合你当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径。</b></p><p>注：红色的牛顿法的迭代路径，绿色的是梯度下降法的迭代路径。</p><p><b>牛顿法的优缺点总结：</b></p><p><b>优点：二阶收敛，收敛速度快；</b></p><p><b>　　缺点：牛顿法是一种迭代算法，每一步都需要求解目标函数的Hessian矩阵的逆矩阵，计算比较复杂。</b></p><p><b>2）拟牛顿法（Quasi-Newton Methods）</b></p><p>拟牛顿法是求解非线性优化问题最有效的方法之一，于20世纪50年代由美国Argonne国家实验室的物理学家W.C.Davidon所提出来。Davidon设计的这种算法在当时看来是非线性优化领域最具创造性的发明之一。不久R.\n Fletcher和M. J. D. Powell证实了这种新的算法远比其他方法快速和可靠，使得非线性优化这门学科在一夜之间突飞猛进。</p><p><b>拟牛顿法的本质思想是改善牛顿法每次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它使用正定矩阵来近似Hessian矩阵的逆，从而简化了运算的复杂度。</b>拟牛顿法和最速下降法一样只要求每一步迭代时知道目标函数的梯度。通过测量梯度的变化，构造一个目标函数的模型使之足以产生超线性收敛性。这类方法大大优于最速下降法，尤其对于困难的问题。另外，因为拟牛顿法不需要二阶导数的信息，所以有时比牛顿法更为有效。如今，优化软件中包含了大量的拟牛顿算法用来解决无约束，约束，和大规模的优化问题。</p><p><b>具体步骤：</b></p><p>拟牛顿法的基本思想如下。首先构造目标函数在当前迭代xk的二次模型：</p><p>这里Bk是一个对称正定矩阵，于是我们取这个二次模型的最优解作为搜索方向，并且得到新的迭代点：</p><p>其中我们要求步长ak </p><p>满足Wolfe条件。这样的迭代与牛顿法类似，区别就在于用近似的Hesse矩阵Bk </p><p>代替真实的Hesse矩阵。所以拟牛顿法最关键的地方就是每一步迭代中矩阵Bk</p><p>的更新。现在假设得到一个新的迭代xk+1，并得到一个新的二次模型：</p><p>我们尽可能地利用上一步的信息来选取Bk。具体地，我们要求</p><p>从而得到</p><p>这个公式被称为割线方程。常用的拟牛顿法有DFP算法和BFGS算法。</p><h3>3. 共轭梯度法（Conjugate Gradient）</h3><p><b>共轭梯度法是介于最速下降法与牛顿法之间的一个方法，它仅需利用一阶导数信息，但克服了最速下降法收敛慢的缺点，又避免了牛顿法需要存储和计算Hesse矩阵并求逆的缺点，共轭梯度法不仅是解决大型线性方程组最有用的方法之一，也是解大型非线性最优化最有效的算法之一。</b> 在各种优化算法中，共轭梯度法是非常重要的一种。其优点是所需存储量小，具有步收敛性，稳定性高，而且不需要任何外来参数。</p><p>下图为共轭梯度法和梯度下降法搜索最优解的路径对比示意图：</p><p>注：绿色为梯度下降法，红色代表共轭梯度法</p><h3>4. 启发式优化方法</h3><p>启发式方法指人在解决问题时所采取的一种根据经验规则进行发现的方法。其特点是在解决问题时,利用过去的经验,选择已经行之有效的方法，而不是系统地、以确定的步骤去寻求答案。启发式优化方法种类繁多，包括经典的模拟退火方法、遗传算法、蚁群算法以及粒子群算法等等。</p><p>还有一种特殊的优化算法被称之多目标优化算法，它主要针对同时优化多个目标（两个及两个以上）的优化问题，这方面比较经典的算法有NSGAII算法、MOEA/D算法以及人工免疫算法等。</p>", 
            "topic": [
                {
                    "tag": "人工智能算法", 
                    "tagLink": "https://api.zhihu.com/topics/19691108"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/26335999", 
            "userName": "李锋", 
            "userLink": "https://www.zhihu.com/people/4c1cb1778179af104f87eb56111ff960", 
            "upvote": 25, 
            "title": "蒙特卡洛树搜索 MCTS 入门", 
            "content": "<p> 来源：<a href=\"https://link.zhihu.com/?target=http%3A//nooverfit.com\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">nooverfit.com</span><span class=\"invisible\"></span></a></p><h2>什么是 MCTS？</h2><p>全称 Monte Carlo Tree Search，是一种人工智能问题中做出最优决策的方法，一般是在组合博弈中的行动（move）规划形式。它结合了随机模拟的一般性和树搜索的准确性。</p><p>MCTS 受到快速关注主要是由计算机围棋程序的成功以及其潜在的在众多难题上的应用所致。超越博弈游戏本身，MCTS 理论上可以被用在以 {状态 state，行动 action} 对定义和用模拟进行预测输出结果的任何领域。</p><h2>基本算法</h2><p>基本的 MCTS 算法非常简单：根据模拟的输出结果，按照节点构造搜索树。其过程可以分为下面的若干步：<figure><noscript><img src=\"https://pic1.zhimg.com/v2-95b4323461b853e02c43b512a3432c64_b.png\" data-rawwidth=\"571\" data-rawheight=\"233\" class=\"origin_image zh-lightbox-thumb\" width=\"571\" data-original=\"https://pic1.zhimg.com/v2-95b4323461b853e02c43b512a3432c64_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;571&#39; height=&#39;233&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"571\" data-rawheight=\"233\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"571\" data-original=\"https://pic1.zhimg.com/v2-95b4323461b853e02c43b512a3432c64_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-95b4323461b853e02c43b512a3432c64_b.png\"/></figure></p><br/><p>搜索树的构建过程</p><ol><li>选择 Selection：从根节点 R 开始，递归选择最优的子节点（后面会解释）直到达到叶子节点 L。</li><li>扩展 Expansion：如果 L 不是一个终止节点（也就是，不会导致博弈游戏终止）那么就创建一个或者更多的字子节点，选择其中一个 C。</li><li>模拟 Simulation：从 C 开始运行一个模拟的输出，直到博弈游戏结束。</li><li>反向传播 Backpropagation：用模拟的结果输出更新当前行动序列。</li></ol><p>参看<a href=\"https://link.zhihu.com/?target=http%3A//mcts.ai/tutorial/index.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Tutorial</a>了解关于这个过程更多的信息。</p><p>每个节点并需包含两个重要的信息：一个是根据模拟结果估计的值和该节点已经被访问的次数。</p><p>按照最为简单和最节约内存的实现，MCTS 将在每个迭代过程中增加一个子节点。不过，要注意其实根据不同的应用这里也可以在每个迭代过程中增加超过一个子节点。</p><h2>节点选择</h2><h3>Bandits 和 UCB</h3><p>在树向下遍历时的节点选择通过选择最大化某个量来实现，这其实类似于\n Multiarmed bandit problem，其中的参与者必须选择一个 slot \nmachine（bandit）来最大化每一轮的估计的收益。我们可以使用 Upper Confidence \nBounds（UCB）公式常常被用来计算这个：</p><p>其中 v_i 是节点估计的值，n_i 是节点被访问的次数，而 N 则是其父节点已经被访问的总次数。C 是可调整参数。</p><h3>Exploitation 和 Exploration</h3><p>UCB\n 公式对已知收益的 exploitation 和鼓励接触那些相对未曾访问的节点的 exploration \n进行平衡。收益估计基于随机模拟，所以节点必须被访问若干次来缺包估计变得更加可信；MCTS \n估计会在搜索的开始不大可靠，而最终会在给定充分的时间后收敛到更加可靠的估计上，在无限时间下能够达到最优估计。</p><h3>MCTS 和 UCT</h3><p>Kocsis\n 和 Szepervari 在 2006 年首先构建了一个完备的 MCTS 算法，通过扩展 UCB 到 minimax 树搜索，并将其命名为 \nUpper Confidence Bounds for Trees（UCT）方法。这其实是用在当前众多 MCTS 实现中的算法版本。</p><p>UCT 可以被描述为 MCTS 的一个特例：UCT = MCTS + UCB。</p><h2>优点</h2><p>MCTS 提供了比传统树搜索更好的方法。</p><h3>Aheuristic</h3><p>MCTS\n \n不要求任何关于给定的领域策略或者具体实践知识来做出合理的决策。这个算法可以在没有任何关于博弈游戏除基本规则外的知识的情况下进行有效工作；这意味着一个简单的\n MCTS 实现可以重用在很多的博弈游戏中，只需要进行微小的调整，所以这也使得 MCTS 是对于一般的博弈游戏的很好的方法。</p><h3>Asymmetric</h3><p>MCTS 执行一种非对称的树的适应搜索空间拓扑结构的增长。这个算法会更频繁地访问更加有趣的节点，并聚焦其搜索时间在更加相关的树的部分。</p><p>非对称的增长</p><p>这使得 MCTS 更加适合那些有着更大的分支因子的博弈游戏，比如说 19X19 的围棋。这么大的组合空间会给标准的基于深度或者宽度的搜索方法带来问题，所以 MCTS 的适应性说明它（最终）可以找到那些更加优化的行动，并将搜索的工作聚焦在这些部分。</p><h3>任何时间</h3><p>算法可以在任何时间终止，并返回当前最有的估计。当前构造出来的搜索树可以被丢弃或者供后续重用。</p><h3>简洁</h3><p>算法实现非常方便（参见<a href=\"https://link.zhihu.com/?target=http%3A//www.kuqin.com/book/326085.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Code</a>）</p><h2>缺点</h2><p>MCTS 有很少的缺点，不过这些缺点也可能是非常关键的影响因素。</p><h3>行为能力</h3><p>MCTS 算法，根据其基本形式，在某些甚至不是很大的博弈游戏中在可承受的时间内也不能够找到最好的行动方式。这基本上是由于组合步的空间的全部大小所致，关键节点并不能够访问足够多的次数来给出合理的估计。</p><h3>速度</h3><p>MCTS\n \n搜索可能需要足够多的迭代才能收敛到一个很好的解上，这也是更加一般的难以优化的应用上的问题。例如，最佳的围棋程序可能需要百万次的交战和领域最佳和强化才能得到专家级的行动方案，而最有的\n GGP 实现对更加复杂的博弈游戏可能也就只要每秒钟数十次（领域无关的）交战。对可承受的行动时间，这样的 GGP \n可能很少有时间访问到每个合理的行动，所以这样的情形也不大可能出现表现非常好的搜索。</p><p>幸运的是，算法的性能可以通过一些技术显著提升。</p><h2>提升</h2><p>很多种 MCTS 强化的技术已经出现了。这些基本上可以归纳为领域知识或者领域独立两大类。</p><h3>领域知识</h3><p>特定博弈游戏的领域知识可以用在树上来过滤掉不合理的行动或者在模拟过程中产生重要的对局（更接近人类对手的表现）。这意味着交战结果将会更加的现实而不是随机的模拟，所以节点只需要少量的迭代就能给出一个现实的收益值。</p><p>领域知识可以产生巨大的性能提升，但在速度和一般性上也会有一定的损失。</p><h3>领域独立</h3><p>领域独立强化能够应用到所有的问题领域中。这些一般用在树种（如 AMAF），还有一些用在模拟（如 在交战时倾向于胜利的行动）。领域独立强化并不和特定的领域绑定，具有一般性，这也是当前研究的重心所在。</p><h2>背景和历史</h2><p>1928：John von Neumann 的 minimax 定理给出了关于对手树搜索的方法，这形成了计算机科学和人工智能的从诞生至今的决策制定基础。<br/>\n1940s：Monte Carlo 方法形成，作为一种通过随机采样解决不太适合树搜索解决的弱良定义问题的方法。<br/>\n2006：Rémi Coulomb 和其他研究者组合了上面两种想法给出了一个新的围棋程序中行动规划的观点——MCTS。Kocsis 和 Szepesvári 将此观点形式化进 UCT 算法。</p><h2>研究兴趣</h2><p>从 MCTS 诞生后几年内，就有超过 150 篇与 MCTS 相关的研究论文发布，平均下来是每两周一篇新的文章。这些文章中包含了大概 50 个推荐的变体、强化和优化，这和传统树搜索自其 1928 年诞生开始的加强的数量也差不太多。</p><p>这个新的研究领域当前是 AI 中非常热的研究话题，有很多的开放的研究问题有待发掘和解决。</p><h2>MCTS: 最新成果</h2><p>Imperial College London held the first international MCTS workshop in August 2010 on the theme of<i>MCTS: State of the Art</i>. Speakers included:<br/>\nO. Teytaud, “State of the Art: What is MCTS, where is it now, and where is it going?” 2010 [Online]. Available:<a href=\"https://link.zhihu.com/?target=http%3A//www.aigamesnetwork.org/_media/main%3Aevents%3Alondon2010.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">aigamesnetwork.org/_med</span><span class=\"invisible\">ia/main:events:london2010.pdf</span><span class=\"ellipsis\"></span></a><br/>\nM. Müller, “Challenges in Monte Carlo Tree Search,” 2010 [Online]. Available:<a href=\"https://link.zhihu.com/?target=http%3A//www.aigamesnetwork.org/_media/main%3Aevents%3Alondon2010-mcts-challenges.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">aigamesnetwork.org/_med</span><span class=\"invisible\">ia/main:events:london2010-mcts-challenges.pdf</span><span class=\"ellipsis\"></span></a><br/>\nR. Hayward, “MoHex: Computer Hex world champion,” 2010 [Online]. Available:<a href=\"https://link.zhihu.com/?target=http%3A//www.aigamesnetwork.org/_media/main%3Aevents%3Amohextalk.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">aigamesnetwork.org/_med</span><span class=\"invisible\">ia/main:events:mohextalk.pdf</span><span class=\"ellipsis\"></span></a><br/>\nH. Finnsson and Y. Björnsson, “CadiaPlayer: MCTS in General Game Playing,” 2010 [Online]. Available:<a href=\"https://link.zhihu.com/?target=http%3A//www.aigamesnetwork.org/_media/main%3Aevents%3Acadiaplayer_lic_slides_print.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">aigamesnetwork.org/_med</span><span class=\"invisible\">ia/main:events:cadiaplayer_lic_slides_print.pdf</span><span class=\"ellipsis\"></span></a><br/>\nA. Rimmel, “Havannah, Monte Carlo Enhancements and Linear Transforms,” 2010 [Online]. Available:<a href=\"https://link.zhihu.com/?target=http%3A//www.aigamesnetwork.org/_media/main%3Aevents%3Apresmctsworkshop_rimmel.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">aigamesnetwork.org/_med</span><span class=\"invisible\">ia/main:events:presmctsworkshop_rimmel.pdf</span><span class=\"ellipsis\"></span></a></p><p>参考文献：</p><ol><li><a href=\"https://link.zhihu.com/?target=http%3A//www.kuqin.com/shuoit/20160219/350769.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">蒙特卡洛树搜索 MCTS_算法艺术_酷勤网</a></li><li><a href=\"https://link.zhihu.com/?target=https%3A//www.caktusgroup.com/blog/2015/09/24/introduction-monte-carlo-tree-search-1/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Introduction to Monte Carlo Tree Search | Caktus Group</a></li></ol>", 
            "topic": [
                {
                    "tag": "人工智能", 
                    "tagLink": "https://api.zhihu.com/topics/19551275"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "蒙特卡洛方法", 
                    "tagLink": "https://api.zhihu.com/topics/19800937"
                }
            ], 
            "comments": [
                {
                    "userName": "逗几", 
                    "userLink": "https://www.zhihu.com/people/e9045542b5a82bf24269b82ac53c274c", 
                    "content": "请问这个「叶子节点 L」到底是哪个节点？根节点下面一层还是说可以更深？", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/25906433", 
            "userName": "李锋", 
            "userLink": "https://www.zhihu.com/people/4c1cb1778179af104f87eb56111ff960", 
            "upvote": 2, 
            "title": "机器学习：一步一步学习强化学习 例子1", 
            "content": "<p>本教程通过一个简单但全面的数值示例来介绍强化学习。 该示例描述了使用一个由无监督学习训练的代理来了解未知环境。 </p><p>假设我们在一个由门连接的建筑中有5个房间，如下图所示。 我们将为每个房间0到4编号。建筑物的外部可以被认为是一个大房间（5）。 注意，门1和4通过门通向房间5（外部）。 </p><p><figure><noscript><img src=\"https://pic1.zhimg.com/v2-d7f366bd54c6bf0be104a1d6f7730f6c_b.jpg\" data-rawwidth=\"1142\" data-rawheight=\"658\" class=\"origin_image zh-lightbox-thumb\" width=\"1142\" data-original=\"https://pic1.zhimg.com/v2-d7f366bd54c6bf0be104a1d6f7730f6c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1142&#39; height=&#39;658&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1142\" data-rawheight=\"658\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1142\" data-original=\"https://pic1.zhimg.com/v2-d7f366bd54c6bf0be104a1d6f7730f6c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-d7f366bd54c6bf0be104a1d6f7730f6c_b.jpg\"/></figure>我们可以通过图这样一个数据结构来表示各个房间节点之间的关系。</p><p><figure><noscript><img src=\"https://pic3.zhimg.com/v2-a81decb90af0198da872f0efaef9f746_b.jpg\" data-rawwidth=\"1036\" data-rawheight=\"618\" class=\"origin_image zh-lightbox-thumb\" width=\"1036\" data-original=\"https://pic3.zhimg.com/v2-a81decb90af0198da872f0efaef9f746_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1036&#39; height=&#39;618&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1036\" data-rawheight=\"618\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1036\" data-original=\"https://pic3.zhimg.com/v2-a81decb90af0198da872f0efaef9f746_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-a81decb90af0198da872f0efaef9f746_b.jpg\"/></figure>对于这个例子，我们把agent放在任何房间，从那个房间，去建筑物外（这将是我们的目标房间5）。 换句话说，目标房间是5号。要将这个房间设置为目标，我们将把奖励值关联到每个门（即节点之间的链接）。 直接通往目标门5有即刻奖励100.其他没有直接连接到目标房间的门没有奖励。 由于门是双向的（0到4，4到0），每个房间都分配了两个箭头。 同时每个箭头包含即时奖励值，如下所示： <br/></p><figure><noscript><img src=\"https://pic1.zhimg.com/v2-51ccbca9ea104ec829a088d77875d3fc_b.jpg\" data-rawwidth=\"1030\" data-rawheight=\"638\" class=\"origin_image zh-lightbox-thumb\" width=\"1030\" data-original=\"https://pic1.zhimg.com/v2-51ccbca9ea104ec829a088d77875d3fc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1030&#39; height=&#39;638&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1030\" data-rawheight=\"638\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1030\" data-original=\"https://pic1.zhimg.com/v2-51ccbca9ea104ec829a088d77875d3fc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-51ccbca9ea104ec829a088d77875d3fc_b.jpg\"/></figure><p>当然，房间5和自己相连，拥有100的奖励值，其它与5直连的房间都有100的奖励值。在Q学习中，目标是以最高的奖励值到达目标状态，因此，如果agent到达了目标，他将永远停留，这种类型的目标称为&#34;absorbing\n goal&#34;</p><p>假设我们的agent是一个能够通过实验学习的“哑巴虚拟机器人”。Agent能够从一个房间到达另一个房间，但是对环境一无所知，而且也不知道哪些门通向外面。</p><p>假设我们要为可能处于任意房间的agent建立模型。现在我们假设agent在房间2，我们希望agent能够通过学习达到5。<figure><noscript><img src=\"https://pic2.zhimg.com/v2-30da1936956a8683991969382a66ea81_b.jpg\" data-rawwidth=\"1132\" data-rawheight=\"650\" class=\"origin_image zh-lightbox-thumb\" width=\"1132\" data-original=\"https://pic2.zhimg.com/v2-30da1936956a8683991969382a66ea81_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1132&#39; height=&#39;650&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1132\" data-rawheight=\"650\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1132\" data-original=\"https://pic2.zhimg.com/v2-30da1936956a8683991969382a66ea81_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-30da1936956a8683991969382a66ea81_b.jpg\"/></figure></p><p>Q-learning中术语包括状态和行为。</p><p>我们把0-5每个房间称为状态，把agent从一个房间移动到另一房间称为行为（action），在图中，状态表示一个节点，行为表示一条有向弧。</p><figure><noscript><img src=\"https://pic3.zhimg.com/v2-131d8b4c4e99fea482dacba74076103e_b.jpg\" data-rawwidth=\"1032\" data-rawheight=\"640\" class=\"origin_image zh-lightbox-thumb\" width=\"1032\" data-original=\"https://pic3.zhimg.com/v2-131d8b4c4e99fea482dacba74076103e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1032&#39; height=&#39;640&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1032\" data-rawheight=\"640\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1032\" data-original=\"https://pic3.zhimg.com/v2-131d8b4c4e99fea482dacba74076103e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-131d8b4c4e99fea482dacba74076103e_b.jpg\"/></figure><br/><p>假设agent在状态2，因为2与3直连，因此可以从状态2可以移动到状态3。从状态2不能直接到状态1因为两者不直连。同理，从状态3可以到状态1或4或者回到2。如果agent在状态4，那么就有3个可能的行为，到0、5或者3。如果agent在状态1，那么可以到5或3，从0只能到达4。</p><p>我们可以根据状态间的关系和瞬时奖励值创建矩阵R。<figure><noscript><img src=\"https://pic3.zhimg.com/v2-03985330f40eab1b33ab55170a6603a2_b.jpg\" data-rawwidth=\"678\" data-rawheight=\"422\" class=\"origin_image zh-lightbox-thumb\" width=\"678\" data-original=\"https://pic3.zhimg.com/v2-03985330f40eab1b33ab55170a6603a2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;678&#39; height=&#39;422&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"678\" data-rawheight=\"422\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"678\" data-original=\"https://pic3.zhimg.com/v2-03985330f40eab1b33ab55170a6603a2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-03985330f40eab1b33ab55170a6603a2_b.jpg\"/></figure>                                      -1表示状态间不直连。例如状态0不能直接到达状态1</p><p>现在可以增加一个相似的矩阵Q，它可以看作是agent的“大脑”，表示了agent通过学习环境留下的“记忆”，矩阵Q的行表示agent当前的状态，列表示到达下一状态可能的行为。</p><p> 在开始时agent对环境一无所知，因此矩阵Q初始化为0。在本例中，为了简单起见，我们假设状态的数量已知，如果状态数量未知，矩阵Q初始化时只有一个元素，如果发现新状态后再添加行和列。</p><p>Q学习的转移规则是一个很简单的方程：</p><p><b>Q(state， action) = R(state， action) + Gamma * Max[Q(next state， all actions)]</b></p><p> 根据这个方程，一个值被赋予给矩阵Q中一个特定值，它等于R矩阵中对应值的和，和学习参数gamma，乘以下一状态中所有行为中Q值最高的值。</p><p>虚拟的agent通过经验进行学习，没有老师（称为无监督学习）。Agent从一个状态转移到另一状态不断的探索直到到达目标。我们把每一次探索称为一次迭代（episode）。每一个episode包含agent从初始状态移动到目标状态。每当agent到达目标状态，程序继续下一episode。</p><p>Q学习算法步骤如下：</p><ol><li>设置gamma参数以及矩阵R并填写对应的反馈值；</li><li> 初始化矩阵Q；每个值设置为0</li><li>对于每次episode：</li></ol><ul><ul><ul><li>随机选择一个初始状态；</li><li>进行循环，如果没有达到最终的目标 </li><ul><li>从当前状态的所有行为中选择一个</li><li>选择一个可能的action，然后考虑即将进行的下一个状态</li><li>从下一步状态可能的所有action中得到一个最大的Q值。</li><li>计算: Q(state, action) = R(state, action) + Gamma * Max[Q(next state, all actions)] </li><li>移动到下一个状态  </li></ul></ul></ul></ul><p>             结束算法。</p><p>Agent通过以上算法进行学习，每次episode相当于一次训练。在每一次训练中，agent探索环境(矩阵R表示)，接受奖励直到到达目标状态。训练的目的是增强agent的大脑，即矩阵Q。训练越多，Q结果越好。通过这种方式，如果Q被加强了，而不是反复探索，不断回到同一房间，就能快速找到目标状态。</p><p>参数gamma的取值范围是0-1，如果gamma趋近于0，则agent趋向于考虑瞬时奖励，如果接近1，则趋向于未来的奖励，延迟奖励。</p><p>为了使用矩阵Q，agent仅仅跟踪状态，从初始状态到目标状态。算法能够根据矩阵Q中记录的奖励值为当前状态找到具有最大奖励值的行为。</p><h2> 一步一步通过例子学习Q-Learning </h2><p>为了更好理解Q学习算法的工作原理，我们一步一步的进行演示，剩下的步骤可以参考源代码。</p><p>首先设置 Gamma = 0.8，初始化的状态为房间1。</p><p>利用0初始化Q矩阵 （agent大脑）<figure><noscript><img src=\"https://pic3.zhimg.com/v2-cfbacec6815af4e3c3b683c186cccce6_b.jpg\" data-rawwidth=\"456\" data-rawheight=\"368\" class=\"origin_image zh-lightbox-thumb\" width=\"456\" data-original=\"https://pic3.zhimg.com/v2-cfbacec6815af4e3c3b683c186cccce6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;456&#39; height=&#39;368&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"456\" data-rawheight=\"368\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"456\" data-original=\"https://pic3.zhimg.com/v2-cfbacec6815af4e3c3b683c186cccce6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-cfbacec6815af4e3c3b683c186cccce6_b.jpg\"/></figure>观察矩阵R的第二行，当前状态有两个行为，到达状态3或者状态5。通过随机的选择，我们选择到达状态5。<figure><noscript><img src=\"https://pic4.zhimg.com/v2-59f218c6fc6f37d5a63fa7665c0cd1d7_b.jpg\" data-rawwidth=\"618\" data-rawheight=\"400\" class=\"origin_image zh-lightbox-thumb\" width=\"618\" data-original=\"https://pic4.zhimg.com/v2-59f218c6fc6f37d5a63fa7665c0cd1d7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;618&#39; height=&#39;400&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"618\" data-rawheight=\"400\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"618\" data-original=\"https://pic4.zhimg.com/v2-59f218c6fc6f37d5a63fa7665c0cd1d7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-59f218c6fc6f37d5a63fa7665c0cd1d7_b.jpg\"/></figure>现在我们想像下如果agent到达状态5后会发生什么?观察矩阵R的第6行，它有三个行为，到达状态1、4，、5。</p><p><b>Q(state, action) = R(state, action) + Gamma * Max[Q(next state, all actions)]</b></p><p>Q(1, 5) = R(1, 5) + 0.8 * Max[Q(5, 1), Q(5, 4), Q(5, 5)] = 100 + 0.8 * 0 = 100</p><br/><p>因为矩阵Q初始化为0，Q(5，1)， Q(5，4)Q(5，5)的值都为0。Q(1，5)的计算结果为100因为矩阵R中R(5，1)的瞬时奖励为100。</p><p>下一状态5，成为当前状态。因为5是目标状态，我们结束一次episode。更新后的矩阵Q为：<figure><noscript><img src=\"https://pic1.zhimg.com/v2-68f29f8d7df8088f821a0148ae77e210_b.jpg\" data-rawwidth=\"594\" data-rawheight=\"360\" class=\"origin_image zh-lightbox-thumb\" width=\"594\" data-original=\"https://pic1.zhimg.com/v2-68f29f8d7df8088f821a0148ae77e210_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;594&#39; height=&#39;360&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"594\" data-rawheight=\"360\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"594\" data-original=\"https://pic1.zhimg.com/v2-68f29f8d7df8088f821a0148ae77e210_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-68f29f8d7df8088f821a0148ae77e210_b.jpg\"/></figure></p><p>下一个episode，我们随机选择一个初始状态。这一次我们选择状态3</p><p>观察矩阵R的第4行，它有三个行为，到达状态1，2或4。通过随机选择，我们到达状态1</p><p>Q(state, action) = R(state, action) + Gamma * Max[Q(next state, all actions)]</p><p>Q(3, 1) = R(3, 1) + 0.8 * Max[Q(1, 3), Q(1, 5)] = 0 + 0.8 * Max(0, 100) = 80 </p><figure><noscript><img src=\"https://pic3.zhimg.com/v2-ba8e6155f1e2db511803368361917b22_b.jpg\" data-rawwidth=\"696\" data-rawheight=\"366\" class=\"origin_image zh-lightbox-thumb\" width=\"696\" data-original=\"https://pic3.zhimg.com/v2-ba8e6155f1e2db511803368361917b22_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;696&#39; height=&#39;366&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"696\" data-rawheight=\"366\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"696\" data-original=\"https://pic3.zhimg.com/v2-ba8e6155f1e2db511803368361917b22_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-ba8e6155f1e2db511803368361917b22_b.jpg\"/></figure><p> 如果agent不断地学习，矩阵Q最终会收敛，如下图所示。</p><p><figure><noscript><img src=\"https://pic3.zhimg.com/v2-7650d88af2233cad31c7c4dafd05af76_b.jpg\" data-rawwidth=\"738\" data-rawheight=\"370\" class=\"origin_image zh-lightbox-thumb\" width=\"738\" data-original=\"https://pic3.zhimg.com/v2-7650d88af2233cad31c7c4dafd05af76_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;738&#39; height=&#39;370&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"738\" data-rawheight=\"370\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"738\" data-original=\"https://pic3.zhimg.com/v2-7650d88af2233cad31c7c4dafd05af76_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-7650d88af2233cad31c7c4dafd05af76_b.jpg\"/></figure>接下来可以对矩阵Q进行标准化</p><p><figure><noscript><img src=\"https://pic1.zhimg.com/v2-4a150d04376597cb8b9f77c366665408_b.jpg\" data-rawwidth=\"702\" data-rawheight=\"356\" class=\"origin_image zh-lightbox-thumb\" width=\"702\" data-original=\"https://pic1.zhimg.com/v2-4a150d04376597cb8b9f77c366665408_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;702&#39; height=&#39;356&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"702\" data-rawheight=\"356\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"702\" data-original=\"https://pic1.zhimg.com/v2-4a150d04376597cb8b9f77c366665408_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-4a150d04376597cb8b9f77c366665408_b.jpg\"/></figure>一旦矩阵Q趋于收敛，我们知道agent学到了到达目标状态的最优路径。跟踪最优状态的序列和弧就可以得到最高的奖励值。</p><figure><noscript><img src=\"https://pic3.zhimg.com/v2-af017662f97d9a2c5446724f620238c6_b.jpg\" data-rawwidth=\"1010\" data-rawheight=\"632\" class=\"origin_image zh-lightbox-thumb\" width=\"1010\" data-original=\"https://pic3.zhimg.com/v2-af017662f97d9a2c5446724f620238c6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1010&#39; height=&#39;632&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1010\" data-rawheight=\"632\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1010\" data-original=\"https://pic3.zhimg.com/v2-af017662f97d9a2c5446724f620238c6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-af017662f97d9a2c5446724f620238c6_b.jpg\"/></figure><p>例如，</p><blockquote><p>从初始状态2，agent以矩阵Q为指导；</p><p>从状态2，最大Q值指向状态3；</p><p>状态3，最大Q指指向1或4，假设随机选择1。</p><p>从状态1最大Q值指向状态5；</p></blockquote><p>因此序列为2-3-1-5。</p><p> ===============================================================</p><p>Java Code：</p><div class=\"highlight\"><pre><code class=\"language-text\">import java.util.Random;\n\npublic class QLearning1\n{\n    private static final int Q_SIZE = 6;\n    private static final double GAMMA = 0.8;\n    private static final int ITERATIONS = 10;\n    private static final int INITIAL_STATES[] = new int[] {1, 3, 5, 2, 4, 0};\n\n    private static final int R[][] = new int[][] {{-1, -1, -1, -1, 0, -1}, \n                                                  {-1, -1, -1, 0, -1, 100}, \n                                                  {-1, -1, -1, 0, -1, -1}, \n                                                  {-1, 0, 0, -1, 0, -1}, \n                                                  {0, -1, -1, 0, -1, 100}, \n                                                  {-1, 0, -1, -1, 0, 100}};\n\n    private static int q[][] = new int[Q_SIZE][Q_SIZE];\n    private static int currentState = 0;\n    \n    private static void train()\n    {\n        initialize();\n\n        // Perform training, starting at all initial states.\n        for(int j = 0; j &lt; ITERATIONS; j++)\n        {\n            for(int i = 0; i &lt; Q_SIZE; i++)\n            {\n                episode(INITIAL_STATES[i]);\n            } // i\n        } // j\n\n        System.out.println(&#34;Q Matrix values:&#34;);\n        for(int i = 0; i &lt; Q_SIZE; i++)\n        {\n            for(int j = 0; j &lt; Q_SIZE; j++)\n            {\n                System.out.print(q[i][j] + &#34;,\\t&#34;);\n            } // j\n            System.out.print(&#34;\\n&#34;);\n        } // i\n        System.out.print(&#34;\\n&#34;);\n\n        return;\n    }\n    \n    private static void test()\n    {\n        // Perform tests, starting at all initial states.\n        System.out.println(&#34;Shortest routes from initial states:&#34;);\n        for(int i = 0; i &lt; Q_SIZE; i++)\n        {\n            currentState = INITIAL_STATES[i];\n            int newState = 0;\n            do\n            {\n                newState = maximum(currentState, true);\n                System.out.print(currentState + &#34;, &#34;);\n                currentState = newState;\n            }while(currentState &lt; 5);\n            System.out.print(&#34;5\\n&#34;);\n        }\n\n        return;\n    }\n    \n    private static void episode(final int initialState)\n    {\n        currentState = initialState;\n\n        // Travel from state to state until goal state is reached.\n        do\n        {\n            chooseAnAction();\n        }while(currentState == 5);\n\n        // When currentState = 5, Run through the set once more for convergence.\n        for(int i = 0; i &lt; Q_SIZE; i++)\n        {\n            chooseAnAction();\n        }\n        return;\n    }\n    \n    private static void chooseAnAction()\n    {\n        int possibleAction = 0;\n\n        // Randomly choose a possible action connected to the current state.\n        possibleAction = getRandomAction(Q_SIZE);\n\n        if(R[currentState][possibleAction] &gt;= 0){\n            q[currentState][possibleAction] = reward(possibleAction);\n            currentState = possibleAction;\n        }\n        return;\n    }\n    \n    private static int getRandomAction(final int upperBound)\n    {\n        int action = 0;\n        boolean choiceIsValid = false;\n\n        // Randomly choose a possible action connected to the current state.\n        while(choiceIsValid == false)\n        {\n            // Get a random value between 0(inclusive) and 6(exclusive).\n            action = new Random().nextInt(upperBound);\n            if(R[currentState][action] &gt; -1){\n                choiceIsValid = true;\n            }\n        }\n\n        return action;\n    }\n    \n    private static void initialize()\n    {\n        for(int i = 0; i &lt; Q_SIZE; i++)\n        {\n            for(int j = 0; j &lt; Q_SIZE; j++)\n            {\n                q[i][j] = 0;\n            } // j\n        } // i\n        return;\n    }\n    \n    private static int maximum(final int State, final boolean ReturnIndexOnly)\n    {\n        // If ReturnIndexOnly = True, the Q matrix index is returned.\n        // If ReturnIndexOnly = False, the Q matrix value is returned.\n        int winner = 0;\n        boolean foundNewWinner = false;\n        boolean done = false;\n\n        while(!done)\n        {\n            foundNewWinner = false;\n            for(int i = 0; i &lt; Q_SIZE; i++)\n            {\n                if(i != winner){             // Avoid self-comparison.\n                    if(q[State][i] &gt; q[State][winner]){\n                        winner = i;\n                        foundNewWinner = true;\n                    }\n                }\n            }\n\n            if(foundNewWinner == false){\n                done = true;\n            }\n        }\n\n        if(ReturnIndexOnly == true){\n            return winner;\n        }else{\n            return q[State][winner];\n        }\n    }\n    \n    private static int reward(final int Action)\n    {\n        return (int)(R[currentState][Action] + (GAMMA * maximum(Action, false)));\n    }\n    \n    public static void main(String[] args)\n    {\n        train();\n        test();\n        return;\n    }\n\n}\n\n</code></pre></div><br/><p>===============================================================</p><p>参考资料：</p><p>原文链接：<a href=\"https://link.zhihu.com/?target=http%3A//mnemstudio.org/path-finding-q-learning-tutorial.htm\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">A Painless Q-Learning Tutorial</a></p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "强化学习 (Reinforcement Learning)", 
                    "tagLink": "https://api.zhihu.com/topics/20039099"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/25763342", 
            "userName": "李锋", 
            "userLink": "https://www.zhihu.com/people/4c1cb1778179af104f87eb56111ff960", 
            "upvote": 0, 
            "title": "深度学习第一部分：概述(2)", 
            "content": "<p>深度学习模型：</p><p>下面插图是一个典型的深度学习模型 <br/></p><figure><noscript><img src=\"https://pic3.zhimg.com/v2-abbe9e531f643cd061e4648c613be0ea_b.png\" data-rawwidth=\"1352\" data-rawheight=\"994\" class=\"origin_image zh-lightbox-thumb\" width=\"1352\" data-original=\"https://pic3.zhimg.com/v2-abbe9e531f643cd061e4648c613be0ea_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1352&#39; height=&#39;994&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1352\" data-rawheight=\"994\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1352\" data-original=\"https://pic3.zhimg.com/v2-abbe9e531f643cd061e4648c613be0ea_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-abbe9e531f643cd061e4648c613be0ea_b.png\"/></figure><p>让计算机识别一个由一组像素组成的图片里面的物体是比较困难的。深度学习是通过一系列处理简单的mapping迭代后，通过不同层的模型实现对物体的辨认的。</p><p>在这个插图里面，我们可以看</p><ul><li>第一层可视化层，顾名思义这里面的数据是通过图片直接输入过来的像素，是可以看到真实的图像的。</li><li>第二层是边界检测的隐藏层（隐藏层的第一层），主要是这里面的神经元对物体的边界具有积极的响应。这一层是隐藏层，原因是这里面生成的数据并不是有input数据直接进来的。</li><li>第三层是对轮廓和转角提供检测的隐藏层（隐藏层的第二层），主要是这里面的神经元对物体轮廓和角具有积极的响应。</li><li>第四层是对完整的对象进行检测的隐藏层（隐藏层的第三层），主要是这里面的神经元对完整的对象具有积极的响应。插图里面有轮胎的对象、人头等，通过这些对象的权重实现最终对目标物体的辨别与分类。</li></ul><p><b>每个节点的神经元的计算模型</b><br/></p><figure><noscript><img src=\"https://pic2.zhimg.com/v2-d997e21ed03f09d86ef718016fe94859_b.png\" data-rawwidth=\"1178\" data-rawheight=\"614\" class=\"origin_image zh-lightbox-thumb\" width=\"1178\" data-original=\"https://pic2.zhimg.com/v2-d997e21ed03f09d86ef718016fe94859_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1178&#39; height=&#39;614&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1178\" data-rawheight=\"614\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1178\" data-original=\"https://pic2.zhimg.com/v2-d997e21ed03f09d86ef718016fe94859_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-d997e21ed03f09d86ef718016fe94859_b.png\"/></figure><blockquote>左面图σ(wT x)是一个三层深度的运算模型，其中σ 是一个sigmoid函数。<br/><br/>右面图是以一个逻辑回归为基础的计算模型，深度只有一层。</blockquote><a href=\"https://link.zhihu.com/?target=http%3A//open.163.com/movie/2012/2/I/D/M8FH262HJ_M8FU27PID.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">关于神经元结构可以参看网易公开课</a><br/><p><b>与其他机器学习相比，深度学习的特点</b></p><p><figure><noscript><img src=\"https://pic2.zhimg.com/v2-1fef9729baab0cafd848aa1ed02ed771_b.png\" data-rawwidth=\"812\" data-rawheight=\"1070\" class=\"origin_image zh-lightbox-thumb\" width=\"812\" data-original=\"https://pic2.zhimg.com/v2-1fef9729baab0cafd848aa1ed02ed771_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;812&#39; height=&#39;1070&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"812\" data-rawheight=\"1070\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"812\" data-original=\"https://pic2.zhimg.com/v2-1fef9729baab0cafd848aa1ed02ed771_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-1fef9729baab0cafd848aa1ed02ed771_b.png\"/></figure>图中显示的是 基于规则的系统，传统的机器学习和表征学习。其中灰色的图片是机器中数据中学到的知识。</p><p>总而言之，深度学习是机器学习的一种方法，它深刻的学习人类大脑的工作模式。 近年来，它的流行性和实用性有了巨大的增长，在很大程度上是由于利用了更强大的计算机，更大的数据集，以培养更深层的网络。 未来的岁月充满了挑战和机遇，深化学习将会带入新的领域取得更大的进步。</p><p><figure><noscript><img src=\"https://pic1.zhimg.com/v2-091e0cb46a98f3079034bb1f074dbe28_b.png\" data-rawwidth=\"1224\" data-rawheight=\"578\" class=\"origin_image zh-lightbox-thumb\" width=\"1224\" data-original=\"https://pic1.zhimg.com/v2-091e0cb46a98f3079034bb1f074dbe28_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1224&#39; height=&#39;578&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1224\" data-rawheight=\"578\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1224\" data-original=\"https://pic1.zhimg.com/v2-091e0cb46a98f3079034bb1f074dbe28_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-091e0cb46a98f3079034bb1f074dbe28_b.png\"/></figure>自从隐藏层发明以来，我们发现差不多每2.4年，人工网络就会翻倍。</p><br/><blockquote><ol><li><p>Perceptron (Rosenblatt, 1958, 1962)\n</p></li><li><p>Adaptive linear element (Widrow and Hoff, 1960)\n</p></li><li><p>Neocognitron (Fukushima, 1980)\n</p></li><li><p>Early back-propagation network (Rumelhart et al., 1986b)\n</p></li><li><p>Recurrent neural network for speech recognition (Robinson and Fallside, 1991)\n</p></li><li><p>Multilayer perceptron for speech recognition (Bengio et al., 1991)\n</p></li><li><p>Mean field sigmoid belief network (Saul et al., 1996)\n</p></li><li><p>LeNet-5 (LeCun et al., 1998b)\n</p></li><li><p>Echo state network (Jaeger and Haas, 2004)\n</p></li><li><p>Deep belief network (Hinton et al., 2006)\n</p></li><li><p>GPU-accelerated convolutional network (Chellapilla et al., 2006)\n</p></li><li><p>Deep Boltzmann machine (Salakhutdinov and Hinton, 2009a)\n</p></li><li><p>GPU-accelerated deep belief network (Raina et al., 2009)\n</p></li><li><p>Unsupervised convolutional network (Jarrett et al., 2009)\n</p></li><li><p>GPU-accelerated multilayer perceptron (Ciresan et al., 2010)\n</p></li><li><p>OMP-1 network (Coates and Ng, 2011)\n</p></li><li><p>Distributed autoencoder (Le et al., 2012)\n</p></li><li><p>Multi-GPU convolutional network (Krizhevsky et al., 2012)\n</p></li><li><p>COTS HPC unsupervised convolutional network (Coates et al., 2013)\n</p></li><li><p>GoogLeNet (Szegedy et al., 2014a) </p></li></ol></blockquote><p><b>Imagenet的错误率</b><br/></p><p><figure><noscript><img src=\"https://pic2.zhimg.com/v2-96b651f3f6ec786e629d3e2eff3237d5_b.png\" data-rawwidth=\"1116\" data-rawheight=\"594\" class=\"origin_image zh-lightbox-thumb\" width=\"1116\" data-original=\"https://pic2.zhimg.com/v2-96b651f3f6ec786e629d3e2eff3237d5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1116&#39; height=&#39;594&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1116\" data-rawheight=\"594\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1116\" data-original=\"https://pic2.zhimg.com/v2-96b651f3f6ec786e629d3e2eff3237d5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-96b651f3f6ec786e629d3e2eff3237d5_b.png\"/></figure>深度神经网络的出现，是的Imagenet的每年Top1的分类的错误率一直在下降。</p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "人工智能", 
                    "tagLink": "https://api.zhihu.com/topics/19551275"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/25734786", 
            "userName": "李锋", 
            "userLink": "https://www.zhihu.com/people/4c1cb1778179af104f87eb56111ff960", 
            "upvote": 4, 
            "title": "深度学习第一部分：概述（1）", 
            "content": "<p>深度学习与机器学习、人工智能的关系</p><figure><noscript><img src=\"https://pic2.zhimg.com/v2-cc3e5a57f3d5e5733b2b8dc81e2d3051_b.png\" data-rawwidth=\"1158\" data-rawheight=\"1184\" class=\"origin_image zh-lightbox-thumb\" width=\"1158\" data-original=\"https://pic2.zhimg.com/v2-cc3e5a57f3d5e5733b2b8dc81e2d3051_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1158&#39; height=&#39;1184&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1158\" data-rawheight=\"1184\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1158\" data-original=\"https://pic2.zhimg.com/v2-cc3e5a57f3d5e5733b2b8dc81e2d3051_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-cc3e5a57f3d5e5733b2b8dc81e2d3051_b.png\"/></figure><br/><p>深度学习的应用场景</p><blockquote><ol><li>搜索引擎</li><li>反垃圾邮件</li><li>语音识别</li><li>图像识别</li><li>产品推荐</li></ol></blockquote><p>以图像识别为例介绍：</p><p><figure><noscript><img src=\"https://pic4.zhimg.com/v2-0d15071a1768c1b998ad6747f74ae74b_b.png\" data-rawwidth=\"1242\" data-rawheight=\"592\" class=\"origin_image zh-lightbox-thumb\" width=\"1242\" data-original=\"https://pic4.zhimg.com/v2-0d15071a1768c1b998ad6747f74ae74b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1242&#39; height=&#39;592&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1242\" data-rawheight=\"592\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1242\" data-original=\"https://pic4.zhimg.com/v2-0d15071a1768c1b998ad6747f74ae74b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-0d15071a1768c1b998ad6747f74ae74b_b.png\"/></figure>传统的算法是很难对猫的图像进行建模的。如何来做呢？</p><blockquote><ol><li>我们需要收集大量的猫的图片，这里面强调数量需要大，一般来说，我们建议每一个分类给神经网络的训练器做1万个图像，当然不是绝对的。</li><li>我们需要收集不同种类的猫的图片，这里面强调的是图片的种类要多，这里面的猫可能处于不同的位置，可能是不同的颜色，等等。</li><li>尽可能多的迭代训练，获得更好的训练模型</li><li>试一试大的神经网络，利用更多的层数，更多的神经元，更多的参数。</li><li>试一试小的神经网络。</li><li>避免过拟合，利用正则化方法，例如： L2 regularization 权重衰减（人们普遍认为：更小的权值w，从某种意义上说，表示网络的复杂度更低，对数据的拟合刚刚好【这个法则也叫做奥卡姆剃刀】。而在实际应用中，也验证了这一点，L2正则化的效果往往好于未经正则化的效果。），或者增加dropout神经网络层。 </li><li>修改神经网络的架构，例如：修改激活函数，调整网络层数等</li></ol></blockquote><p>影响深度学习的主要因素：</p><blockquote><ol><li> 能够使用的数据量，随着移动设备的逐步增多，以及物联网设备，传感器的增加海量的数据正在增加，对于海量的数据为深度学习提供了更多的训练素材。</li><li>能够使用的运算力，随着现在各种服务器云化的提供，我们可以更廉价的获取更多的计算能力。</li></ol></blockquote><p>从下面的例子中，我们可以看到数据量与准确性的关系：</p><ol><li>传统的算法与数据量的关系  <br/></li><li><figure><noscript><img src=\"https://pic3.zhimg.com/v2-d19356f51390eb1415098cf98f97b996_b.png\" data-rawwidth=\"1286\" data-rawheight=\"574\" class=\"origin_image zh-lightbox-thumb\" width=\"1286\" data-original=\"https://pic3.zhimg.com/v2-d19356f51390eb1415098cf98f97b996_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1286&#39; height=&#39;574&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1286\" data-rawheight=\"574\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1286\" data-original=\"https://pic3.zhimg.com/v2-d19356f51390eb1415098cf98f97b996_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-d19356f51390eb1415098cf98f97b996_b.png\"/></figure> 小规模神经网络 <figure><noscript><img src=\"https://pic4.zhimg.com/v2-cb0f2951bab7e89529b145fa2ce0bf0b_b.png\" data-rawwidth=\"904\" data-rawheight=\"542\" class=\"origin_image zh-lightbox-thumb\" width=\"904\" data-original=\"https://pic4.zhimg.com/v2-cb0f2951bab7e89529b145fa2ce0bf0b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;904&#39; height=&#39;542&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"904\" data-rawheight=\"542\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"904\" data-original=\"https://pic4.zhimg.com/v2-cb0f2951bab7e89529b145fa2ce0bf0b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-cb0f2951bab7e89529b145fa2ce0bf0b_b.png\"/></figure></li><li> 中等、大型神经网络</li></ol><figure><noscript><img src=\"https://pic1.zhimg.com/v2-b00221547384896f832d6d6aefe4f8c0_b.png\" data-rawwidth=\"1024\" data-rawheight=\"566\" class=\"origin_image zh-lightbox-thumb\" width=\"1024\" data-original=\"https://pic1.zhimg.com/v2-b00221547384896f832d6d6aefe4f8c0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1024&#39; height=&#39;566&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1024\" data-rawheight=\"566\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1024\" data-original=\"https://pic1.zhimg.com/v2-b00221547384896f832d6d6aefe4f8c0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-b00221547384896f832d6d6aefe4f8c0_b.png\"/></figure><blockquote><p>结论：</p><p>尽可能大的网络，尽可能多的数据</p></blockquote><p>训练开发的过程</p><p>还是前面的识别猫的例子，如果我们想在移动app端能够识别手机拍摄的照片是不是一只猫，我们需要训练一个能够识别猫的模型。</p><p>一般而言我们会将数据样本利用70%做训练与验证，30%做测试。测试的样本不会参与任何的模型训练过程。</p><p>在这个过程中我们会有这样的数据集的定义：</p><blockquote><ol><li>训练样本 (train set)：用来给算法做学习的数据集。</li><li>验证样本 (val set)：用来对算法准确性进行评估的样本。</li><li>测试样本 (test set)：不参与算法建模过程的样本数据集，用来做真实性的准确率的验证的数据集。</li></ol></blockquote><p>训练开发的过程</p><p>还是前面的识别猫的例子，如果我们想在移动app端能够识别手机拍摄的照片是不是一只猫，我们需要训练一个能够识别猫的模型。</p><p>一般而言我们会将数据样本利用70%做训练与验证，30%做测试。测试的样本不会参与任何的模型训练过程。</p><p>在这个过程中我们会有这样的数据集的定义：</p><blockquote><p>1.    \n训练样本\n(train set)：用来给算法做学习的数据集</p><p>2.    \n验证样本 (val\nset)：用来调整算法的参数，调整特征值，用来更好的获得预测效果的数据集。</p><p>3.    \n测试样本 (test\nset)：用来评判算法模型的数据集。</p></blockquote><p><b>注意：不同的数据集需要满足相同的数据分布，不同特征，不同样本的数据需要在不同的数据集中的分布应该是类似的。</b></p><p>如果我们发现模型预测的不够准确，可能主要是下面2个方面的原因导致</p><blockquote><ul><li>训练样本的数据不足以反映实际的真实数据情况。例如：我们训练的猫的数据样本，可能主要是由很多成年的猫组成，但是，如果实际的样本中如果存在一些幼崽的小猫的话，那么，预测的结果可能就不会理想。在这种情况下，我们就需要完善我们的数据集，包含更多情况的猫的训练数据。</li><li>过拟合的问题。 所谓的过拟合主要是指对于训练集有比较好的准确率，但是对于测试的准确率却比较低。在这种情况下，可以更新一下训练数据集，增加更多的训练数据。</li></ul></blockquote><p>经验总结：</p><blockquote><ol><li>尽可能从相同分布的数据集中提取训练和测试样本。</li><li>可以现在小批量的数据集上面做验证，这样可以减少迭代的时间。 </li><li>训练样本与测试样本的比例在70%与30%，在大量数据集的情况下，是没有问题的。如果数据集比较少的话，可以适当缩小测试样本的大小。</li><li> 如果出现过拟合，可以增加更多的数据集。</li></ol></blockquote>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "人工智能", 
                    "tagLink": "https://api.zhihu.com/topics/19551275"
                }
            ], 
            "comments": [
                {
                    "userName": "秋育天山", 
                    "userLink": "https://www.zhihu.com/people/c70c8ab74b43dbbe96485c843580aff3", 
                    "content": "<p>抢个前排沙发~</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "皮攀", 
                    "userLink": "https://www.zhihu.com/people/cb7e5f38d0efc3d5f1d5afb76df9b9a1", 
                    "content": "<p>对深度学习的整体框架和要点讲得很清楚，期待锋哥后续的文章。</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "空白少侠", 
                    "userLink": "https://www.zhihu.com/people/6bdf6de9c367ca094f394a978bc3b5c2", 
                    "content": "支持", 
                    "likes": 1, 
                    "childComments": [
                        {
                            "userName": "李锋", 
                            "userLink": "https://www.zhihu.com/people/4c1cb1778179af104f87eb56111ff960", 
                            "content": "<p>谢谢支持</p>", 
                            "likes": 0, 
                            "replyToAuthor": "空白少侠"
                        }
                    ]
                }
            ]
        }
    ], 
    "url": "https://zhuanlan.zhihu.com/ai-life"
}
