{
    "title": "休语的TensorFlow实验室", 
    "description": "", 
    "followers": [
        "https://www.zhihu.com/people/mao-mi-83-28", 
        "https://www.zhihu.com/people/zhao-ya-xing-93-53", 
        "https://www.zhihu.com/people/lin-xin-tan", 
        "https://www.zhihu.com/people/luweikxy", 
        "https://www.zhihu.com/people/alastair-owen", 
        "https://www.zhihu.com/people/liu-liu-41-44-78", 
        "https://www.zhihu.com/people/zhuang-ren-zhong", 
        "https://www.zhihu.com/people/guo-wei-47", 
        "https://www.zhihu.com/people/wang-yifan-96", 
        "https://www.zhihu.com/people/cindymarrie", 
        "https://www.zhihu.com/people/li-rui-hong-33", 
        "https://www.zhihu.com/people/dai-yuan-64-16", 
        "https://www.zhihu.com/people/drz-6", 
        "https://www.zhihu.com/people/henrickchen", 
        "https://www.zhihu.com/people/chen_qun", 
        "https://www.zhihu.com/people/thikingfly", 
        "https://www.zhihu.com/people/da-jia-kuai-le-89", 
        "https://www.zhihu.com/people/yang-troy-89", 
        "https://www.zhihu.com/people/mjfly-17", 
        "https://www.zhihu.com/people/piginzoo", 
        "https://www.zhihu.com/people/xie-shang-dong-48", 
        "https://www.zhihu.com/people/666233-95-78", 
        "https://www.zhihu.com/people/hzl68", 
        "https://www.zhihu.com/people/wang-xiao-dui-30-78", 
        "https://www.zhihu.com/people/xu-dong-bo", 
        "https://www.zhihu.com/people/zhong-xian-you-51", 
        "https://www.zhihu.com/people/wo-fei-wo-shi", 
        "https://www.zhihu.com/people/zhang-yin-10", 
        "https://www.zhihu.com/people/jjdbear", 
        "https://www.zhihu.com/people/will_max", 
        "https://www.zhihu.com/people/carter-86", 
        "https://www.zhihu.com/people/luo-yu-20-44", 
        "https://www.zhihu.com/people/shu_hui", 
        "https://www.zhihu.com/people/zhui-shao-nian", 
        "https://www.zhihu.com/people/hua-teng-lao-fu", 
        "https://www.zhihu.com/people/an-yu-78-48", 
        "https://www.zhihu.com/people/95079507", 
        "https://www.zhihu.com/people/hao-xin-long-61", 
        "https://www.zhihu.com/people/sun-tian-yue", 
        "https://www.zhihu.com/people/libin-sui", 
        "https://www.zhihu.com/people/zhu-forrest", 
        "https://www.zhihu.com/people/ke-wu-88", 
        "https://www.zhihu.com/people/lan-lan-lan-lan-96-82", 
        "https://www.zhihu.com/people/wang-peng-cheng-39-36", 
        "https://www.zhihu.com/people/pi-te-ou-de-man", 
        "https://www.zhihu.com/people/mao-qi-lin-54", 
        "https://www.zhihu.com/people/wizardforcel", 
        "https://www.zhihu.com/people/abelard", 
        "https://www.zhihu.com/people/jokielueng", 
        "https://www.zhihu.com/people/xie-zhao-peng", 
        "https://www.zhihu.com/people/yong-yuan-de-pg", 
        "https://www.zhihu.com/people/joker-20-55-46", 
        "https://www.zhihu.com/people/cheyang-58", 
        "https://www.zhihu.com/people/an-jing-69-75-87", 
        "https://www.zhihu.com/people/chu-qing-zhao", 
        "https://www.zhihu.com/people/hiwjcn", 
        "https://www.zhihu.com/people/zhang-hua-1-8", 
        "https://www.zhihu.com/people/chen-bo-86-42", 
        "https://www.zhihu.com/people/anjun", 
        "https://www.zhihu.com/people/lin-yi-quan", 
        "https://www.zhihu.com/people/laoyoutiao_trap", 
        "https://www.zhihu.com/people/ison-laihus", 
        "https://www.zhihu.com/people/hei-zi-89", 
        "https://www.zhihu.com/people/bai-ma-fei-ma-88-80-96", 
        "https://www.zhihu.com/people/PJCK", 
        "https://www.zhihu.com/people/phree-liu", 
        "https://www.zhihu.com/people/bingchan", 
        "https://www.zhihu.com/people/max-62-6", 
        "https://www.zhihu.com/people/luxuryzc", 
        "https://www.zhihu.com/people/yi-wei-3-38", 
        "https://www.zhihu.com/people/li-wen-kai-8", 
        "https://www.zhihu.com/people/li-rui-45-18", 
        "https://www.zhihu.com/people/yang-wei-da-56", 
        "https://www.zhihu.com/people/wyin-85", 
        "https://www.zhihu.com/people/wo-men-du-yiyang-39-74", 
        "https://www.zhihu.com/people/xuewei-84", 
        "https://www.zhihu.com/people/zhang-guan-hong-63", 
        "https://www.zhihu.com/people/zhao-lei-57-97", 
        "https://www.zhihu.com/people/dragonslayer-97", 
        "https://www.zhihu.com/people/li-hong-kai-66", 
        "https://www.zhihu.com/people/bbtsww1103", 
        "https://www.zhihu.com/people/April_Li", 
        "https://www.zhihu.com/people/wayne-su-95", 
        "https://www.zhihu.com/people/wang-jun-ting-40-46", 
        "https://www.zhihu.com/people/AI_Technology", 
        "https://www.zhihu.com/people/iequinox", 
        "https://www.zhihu.com/people/zhukovasky", 
        "https://www.zhihu.com/people/alice-alloc", 
        "https://www.zhihu.com/people/lu-shuo-77-92", 
        "https://www.zhihu.com/people/que-96-32", 
        "https://www.zhihu.com/people/wang-zhen-yu-64", 
        "https://www.zhihu.com/people/wang-ying-37", 
        "https://www.zhihu.com/people/fpga-97", 
        "https://www.zhihu.com/people/salafeng11", 
        "https://www.zhihu.com/people/panzhengyuxhu", 
        "https://www.zhihu.com/people/lan-bao-shi-er-zuan", 
        "https://www.zhihu.com/people/homer-wong-33", 
        "https://www.zhihu.com/people/pu-yu-18-29", 
        "https://www.zhihu.com/people/wuyuzaizai"
    ], 
    "article": [
        {
            "url": "https://zhuanlan.zhihu.com/p/33408359", 
            "userName": "休语", 
            "userLink": "https://www.zhihu.com/people/9fa98b02c1f908f202862a38bd15084f", 
            "upvote": 3, 
            "title": "【Python基础】静态方法和类方法", 
            "content": "<h2>【1】为什么要学习Python的基础知识</h2><p>稍微多看一点TensorFlow项目，就会觉得Python的基础知识实在是很重要。一方面，大牛们写的代码总会用到一些稍微深入一些的知识，比如装饰器、静态方法、类属性等等；另一方面，如果自己的代码想写的稍微好一点，就不能一个文件到底，或者一个函数十七八个参数。所以，最近决定学习一些Python的基础知识，也是遇到什么学什么，同时做个笔记。</p><h2>【2】静态方法和类方法初步</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-8a6695902de68f83914df08b07bdd934_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"491\" data-rawheight=\"788\" class=\"origin_image zh-lightbox-thumb\" width=\"491\" data-original=\"https://pic1.zhimg.com/v2-8a6695902de68f83914df08b07bdd934_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;491&#39; height=&#39;788&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"491\" data-rawheight=\"788\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"491\" data-original=\"https://pic1.zhimg.com/v2-8a6695902de68f83914df08b07bdd934_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-8a6695902de68f83914df08b07bdd934_b.jpg\"/></figure><h2>【3】静态方法和类方法的使用</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ade960bdf3dffdc6b10c720f893afa97_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"573\" data-rawheight=\"2019\" class=\"origin_image zh-lightbox-thumb\" width=\"573\" data-original=\"https://pic4.zhimg.com/v2-ade960bdf3dffdc6b10c720f893afa97_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;573&#39; height=&#39;2019&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"573\" data-rawheight=\"2019\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"573\" data-original=\"https://pic4.zhimg.com/v2-ade960bdf3dffdc6b10c720f893afa97_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-ade960bdf3dffdc6b10c720f893afa97_b.jpg\"/></figure><h2>【4】静态方法的一个例子</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-dbb4e21f26f6007d56f32e05282ff4aa_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"545\" data-rawheight=\"689\" class=\"origin_image zh-lightbox-thumb\" width=\"545\" data-original=\"https://pic3.zhimg.com/v2-dbb4e21f26f6007d56f32e05282ff4aa_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;545&#39; height=&#39;689&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"545\" data-rawheight=\"689\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"545\" data-original=\"https://pic3.zhimg.com/v2-dbb4e21f26f6007d56f32e05282ff4aa_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-dbb4e21f26f6007d56f32e05282ff4aa_b.jpg\"/></figure><h2>【5】类方法的一个例子</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-825b901105c76f2d34ea704101ce6d89_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"582\" data-rawheight=\"1012\" class=\"origin_image zh-lightbox-thumb\" width=\"582\" data-original=\"https://pic2.zhimg.com/v2-825b901105c76f2d34ea704101ce6d89_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;582&#39; height=&#39;1012&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"582\" data-rawheight=\"1012\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"582\" data-original=\"https://pic2.zhimg.com/v2-825b901105c76f2d34ea704101ce6d89_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-825b901105c76f2d34ea704101ce6d89_b.jpg\"/></figure><h2>【6】什么时候用静态方法，什么时候用类方法</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-529fdcfb5ed3ece1310ec783bfbedcd2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"835\" data-rawheight=\"687\" class=\"origin_image zh-lightbox-thumb\" width=\"835\" data-original=\"https://pic3.zhimg.com/v2-529fdcfb5ed3ece1310ec783bfbedcd2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;835&#39; height=&#39;687&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"835\" data-rawheight=\"687\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"835\" data-original=\"https://pic3.zhimg.com/v2-529fdcfb5ed3ece1310ec783bfbedcd2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-529fdcfb5ed3ece1310ec783bfbedcd2_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "TensorFlow", 
                    "tagLink": "https://api.zhihu.com/topics/20032249"
                }, 
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }, 
                {
                    "tag": "Python 入门", 
                    "tagLink": "https://api.zhihu.com/topics/19661050"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/33290557", 
            "userName": "休语", 
            "userLink": "https://www.zhihu.com/people/9fa98b02c1f908f202862a38bd15084f", 
            "upvote": 1, 
            "title": "【Python基础】Python的“模块”“包”&TensorFlow项目中的文件和文件夹", 
            "content": "<p>大多数的TensorFlow项目都长下面这个样子：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-8d0aa67c3d5fb1ca28c7442d456491c5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"422\" data-rawheight=\"775\" class=\"origin_image zh-lightbox-thumb\" width=\"422\" data-original=\"https://pic2.zhimg.com/v2-8d0aa67c3d5fb1ca28c7442d456491c5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;422&#39; height=&#39;775&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"422\" data-rawheight=\"775\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"422\" data-original=\"https://pic2.zhimg.com/v2-8d0aa67c3d5fb1ca28c7442d456491c5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-8d0aa67c3d5fb1ca28c7442d456491c5_b.jpg\"/></figure><p>为了理解TensorFlow项目中文件和文件夹是如何组织的，学习了《Python学习手册》第23章“模块包”，做个笔记。</p><h2>【1】文件、文件夹与Python“模块”和“包”概念的对应</h2><p>项目中的“文件”就是Python的“模块”；项目中的“目录（路径）”就是Python的“包”。</p><h2>【2】包导入要点</h2><p>【2-1】目录与包的关系</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-08c0704a57b3434109b0f8ca8be8a436_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"864\" data-rawheight=\"174\" class=\"origin_image zh-lightbox-thumb\" width=\"864\" data-original=\"https://pic3.zhimg.com/v2-08c0704a57b3434109b0f8ca8be8a436_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;864&#39; height=&#39;174&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"864\" data-rawheight=\"174\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"864\" data-original=\"https://pic3.zhimg.com/v2-08c0704a57b3434109b0f8ca8be8a436_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-08c0704a57b3434109b0f8ca8be8a436_b.jpg\"/></figure><p>【2-2】模块搜索路径</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-c447c7392ae381958db68bdc08035e24_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"600\" data-rawheight=\"399\" class=\"origin_image zh-lightbox-thumb\" width=\"600\" data-original=\"https://pic1.zhimg.com/v2-c447c7392ae381958db68bdc08035e24_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;600&#39; height=&#39;399&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"600\" data-rawheight=\"399\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"600\" data-original=\"https://pic1.zhimg.com/v2-c447c7392ae381958db68bdc08035e24_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-c447c7392ae381958db68bdc08035e24_b.jpg\"/></figure><h2>【3】__init__.py文件</h2><p>【3-1】什么地方要写__init__.py，什么地方不写</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-94d59097d22779424d9a292fead25be0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"484\" data-rawheight=\"276\" class=\"origin_image zh-lightbox-thumb\" width=\"484\" data-original=\"https://pic1.zhimg.com/v2-94d59097d22779424d9a292fead25be0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;484&#39; height=&#39;276&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"484\" data-rawheight=\"276\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"484\" data-original=\"https://pic1.zhimg.com/v2-94d59097d22779424d9a292fead25be0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-94d59097d22779424d9a292fead25be0_b.jpg\"/></figure><p>【3-2】__init__.py的作用</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-581810abb287518ec5450bb98f8393a5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"559\" data-rawheight=\"417\" class=\"origin_image zh-lightbox-thumb\" width=\"559\" data-original=\"https://pic2.zhimg.com/v2-581810abb287518ec5450bb98f8393a5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;559&#39; height=&#39;417&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"559\" data-rawheight=\"417\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"559\" data-original=\"https://pic2.zhimg.com/v2-581810abb287518ec5450bb98f8393a5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-581810abb287518ec5450bb98f8393a5_b.jpg\"/></figure><h2>【4】相对导入——Python 2 与Python 3的异同 &amp; 点号“.”的使用</h2><p>“相对导入”在Python2和Python3中有比较大的差别，可能带来一些麻烦，在这里仔细记录一下。</p><p>【4-1】相对导入是包自身的内部导入</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-599a6ab27ea02bc96e4cc54e4fd927b2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"561\" data-rawheight=\"179\" class=\"origin_image zh-lightbox-thumb\" width=\"561\" data-original=\"https://pic3.zhimg.com/v2-599a6ab27ea02bc96e4cc54e4fd927b2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;561&#39; height=&#39;179&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"561\" data-rawheight=\"179\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"561\" data-original=\"https://pic3.zhimg.com/v2-599a6ab27ea02bc96e4cc54e4fd927b2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-599a6ab27ea02bc96e4cc54e4fd927b2_b.jpg\"/></figure><p>【4-2】Python 2与Python 3的异同</p><p>Python 2与Python 3的相同点是：用点号“.”的from语句都是<b>只能（relative-only）</b>导入包目录下的文件。</p><p>Python 2和Python 3的一个重大区别：Python 2的import语句是<b>先</b>搜索包目录<b>再</b>搜索系统路径， Python 3的import语句是<b>只</b>搜索系统路径<b>不</b>搜索包路径。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-201666874c6fd15d936f018e1f94f64f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"750\" data-rawheight=\"844\" class=\"origin_image zh-lightbox-thumb\" width=\"750\" data-original=\"https://pic4.zhimg.com/v2-201666874c6fd15d936f018e1f94f64f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;750&#39; height=&#39;844&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"750\" data-rawheight=\"844\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"750\" data-original=\"https://pic4.zhimg.com/v2-201666874c6fd15d936f018e1f94f64f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-201666874c6fd15d936f018e1f94f64f_b.jpg\"/></figure><p>【4-3】点号“.”的使用</p><p>（1） from . import mod &amp; from .mod import spam</p><p>这里的点“.”可以大致看成linux里面的点“.”——当前路径。更精确的定义如下：</p><p>import a module named spam <b>located in the same package directory as the file in which this statement appears</b>.</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-55a5e8ff85fb7293fd90206e45564c5a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2028\" data-rawheight=\"266\" class=\"origin_image zh-lightbox-thumb\" width=\"2028\" data-original=\"https://pic3.zhimg.com/v2-55a5e8ff85fb7293fd90206e45564c5a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2028&#39; height=&#39;266&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2028\" data-rawheight=\"266\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2028\" data-original=\"https://pic3.zhimg.com/v2-55a5e8ff85fb7293fd90206e45564c5a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-55a5e8ff85fb7293fd90206e45564c5a_b.jpg\"/></figure><p>（2）如果不是from语句，是import语句呢？</p><p>如上文所述，Python 2的import语句是<b>先</b>搜索包目录<b>再</b>搜索系统路径， Python 3的import语句是<b>只</b>搜索系统路径<b>不</b>搜索包路径。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-561241c9084c2c133544eccd7c69340e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"491\" data-rawheight=\"132\" class=\"origin_image zh-lightbox-thumb\" width=\"491\" data-original=\"https://pic3.zhimg.com/v2-561241c9084c2c133544eccd7c69340e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;491&#39; height=&#39;132&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"491\" data-rawheight=\"132\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"491\" data-original=\"https://pic3.zhimg.com/v2-561241c9084c2c133544eccd7c69340e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-561241c9084c2c133544eccd7c69340e_b.jpg\"/></figure><p>（3）与linux一样，from语句中的两个点“..”表示上一层路径。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-85049e9ef38efbd1af49f18caa64c057_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"652\" data-rawheight=\"176\" class=\"origin_image zh-lightbox-thumb\" width=\"652\" data-original=\"https://pic4.zhimg.com/v2-85049e9ef38efbd1af49f18caa64c057_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;652&#39; height=&#39;176&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"652\" data-rawheight=\"176\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"652\" data-original=\"https://pic4.zhimg.com/v2-85049e9ef38efbd1af49f18caa64c057_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-85049e9ef38efbd1af49f18caa64c057_b.jpg\"/></figure><h2>【5】其他杂七杂八的知识</h2><p>【5-1】导入包内文件的方法并不唯一，除了相对导入，还可以绝对导入，即，从sys.path里包含的路径写到包内文件的完整路径。</p><p>【5-2】相对导入只能用于包内的导入</p><p>【5-3】相对导入只能用于from语句</p><p>【5-4】包是一个带有__init__.py的文件夹</p><p>【5-5】目录变对象：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-fbc965f0c65a5991db2ef4aefe69ee3c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"646\" data-rawheight=\"300\" class=\"origin_image zh-lightbox-thumb\" width=\"646\" data-original=\"https://pic1.zhimg.com/v2-fbc965f0c65a5991db2ef4aefe69ee3c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;646&#39; height=&#39;300&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"646\" data-rawheight=\"300\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"646\" data-original=\"https://pic1.zhimg.com/v2-fbc965f0c65a5991db2ef4aefe69ee3c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-fbc965f0c65a5991db2ef4aefe69ee3c_b.jpg\"/></figure><p>【5-6】路径太长的解决方法</p><p>(1) from dir1.dir2 import mod</p><p>(2) import dir1.dir2 as dir</p><p>import dir.mod</p><p>【5-7】关于Python的搜索路径</p><p>（1）Python总是先搜索项目的主目录</p><p>（2）Python的搜索路径本质上是线性的，搜索总是从左至右扫描，路径的最左侧就是最先列出。</p>", 
            "topic": [
                {
                    "tag": "TensorFlow", 
                    "tagLink": "https://api.zhihu.com/topics/20032249"
                }, 
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }, 
                {
                    "tag": "Python 入门", 
                    "tagLink": "https://api.zhihu.com/topics/19661050"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/33153473", 
            "userName": "休语", 
            "userLink": "https://www.zhihu.com/people/9fa98b02c1f908f202862a38bd15084f", 
            "upvote": 4, 
            "title": "TensorFlow的checkpoint中变量的重命名", 
            "content": "<p>TensorFlow里面，一般用tf.variable_scope()来规定变量的名字。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-3019d8630883537f19d348e8bab72d0f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"538\" data-rawheight=\"262\" class=\"origin_image zh-lightbox-thumb\" width=\"538\" data-original=\"https://pic4.zhimg.com/v2-3019d8630883537f19d348e8bab72d0f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;538&#39; height=&#39;262&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"538\" data-rawheight=\"262\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"538\" data-original=\"https://pic4.zhimg.com/v2-3019d8630883537f19d348e8bab72d0f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-3019d8630883537f19d348e8bab72d0f_b.jpg\"/></figure><p>现在，想把一个训练好的checkpoint里面的variable_scope替换掉，而且不想通过建立Graph来实现。</p><p>有大神写代码如下：</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"kn\">import</span> <span class=\"nn\">sys</span><span class=\"o\">,</span> <span class=\"nn\">getopt</span>\n\n<span class=\"kn\">import</span> <span class=\"nn\">tensorflow</span> <span class=\"k\">as</span> <span class=\"nn\">tf</span>\n\n<span class=\"n\">usage_str</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;python tensorflow_rename_variables.py --checkpoint_dir=path/to/dir/ &#39;</span> \\\n            <span class=\"s1\">&#39;--replace_from=substr --replace_to=substr --add_prefix=abc --dry_run&#39;</span>\n\n\n<span class=\"k\">def</span> <span class=\"nf\">rename</span><span class=\"p\">(</span><span class=\"n\">checkpoint_dir</span><span class=\"p\">,</span> <span class=\"n\">replace_from</span><span class=\"p\">,</span> <span class=\"n\">replace_to</span><span class=\"p\">,</span> <span class=\"n\">add_prefix</span><span class=\"p\">,</span> <span class=\"n\">dry_run</span><span class=\"p\">):</span>\n    <span class=\"n\">checkpoint</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"o\">.</span><span class=\"n\">get_checkpoint_state</span><span class=\"p\">(</span><span class=\"n\">checkpoint_dir</span><span class=\"p\">)</span>\n    <span class=\"k\">with</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">Session</span><span class=\"p\">()</span> <span class=\"k\">as</span> <span class=\"n\">sess</span><span class=\"p\">:</span>\n        <span class=\"k\">for</span> <span class=\"n\">var_name</span><span class=\"p\">,</span> <span class=\"n\">_</span> <span class=\"ow\">in</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">contrib</span><span class=\"o\">.</span><span class=\"n\">framework</span><span class=\"o\">.</span><span class=\"n\">list_variables</span><span class=\"p\">(</span><span class=\"n\">checkpoint_dir</span><span class=\"p\">):</span>\n            <span class=\"c1\"># Load the variable</span>\n            <span class=\"n\">var</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">contrib</span><span class=\"o\">.</span><span class=\"n\">framework</span><span class=\"o\">.</span><span class=\"n\">load_variable</span><span class=\"p\">(</span><span class=\"n\">checkpoint_dir</span><span class=\"p\">,</span> <span class=\"n\">var_name</span><span class=\"p\">)</span>\n\n            <span class=\"c1\"># Set the new name</span>\n            <span class=\"n\">new_name</span> <span class=\"o\">=</span> <span class=\"n\">var_name</span>\n            <span class=\"k\">if</span> <span class=\"kc\">None</span> <span class=\"ow\">not</span> <span class=\"ow\">in</span> <span class=\"p\">[</span><span class=\"n\">replace_from</span><span class=\"p\">,</span> <span class=\"n\">replace_to</span><span class=\"p\">]:</span>\n                <span class=\"n\">new_name</span> <span class=\"o\">=</span> <span class=\"n\">new_name</span><span class=\"o\">.</span><span class=\"n\">replace</span><span class=\"p\">(</span><span class=\"n\">replace_from</span><span class=\"p\">,</span> <span class=\"n\">replace_to</span><span class=\"p\">)</span>\n            <span class=\"k\">if</span> <span class=\"n\">add_prefix</span><span class=\"p\">:</span>\n                <span class=\"n\">new_name</span> <span class=\"o\">=</span> <span class=\"n\">add_prefix</span> <span class=\"o\">+</span> <span class=\"n\">new_name</span>\n\n            <span class=\"k\">if</span> <span class=\"n\">dry_run</span><span class=\"p\">:</span>\n                <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;</span><span class=\"si\">%s</span><span class=\"s1\"> would be renamed to </span><span class=\"si\">%s</span><span class=\"s1\">.&#39;</span> <span class=\"o\">%</span> <span class=\"p\">(</span><span class=\"n\">var_name</span><span class=\"p\">,</span> <span class=\"n\">new_name</span><span class=\"p\">))</span>\n            <span class=\"k\">else</span><span class=\"p\">:</span>\n                <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;Renaming </span><span class=\"si\">%s</span><span class=\"s1\"> to </span><span class=\"si\">%s</span><span class=\"s1\">.&#39;</span> <span class=\"o\">%</span> <span class=\"p\">(</span><span class=\"n\">var_name</span><span class=\"p\">,</span> <span class=\"n\">new_name</span><span class=\"p\">))</span>\n                <span class=\"c1\"># Rename the variable</span>\n                <span class=\"n\">var</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">Variable</span><span class=\"p\">(</span><span class=\"n\">var</span><span class=\"p\">,</span> <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"n\">new_name</span><span class=\"p\">)</span>\n\n        <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">dry_run</span><span class=\"p\">:</span>\n            <span class=\"c1\"># Save the variables</span>\n            <span class=\"n\">saver</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"o\">.</span><span class=\"n\">Saver</span><span class=\"p\">()</span>\n            <span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">global_variables_initializer</span><span class=\"p\">())</span>\n            <span class=\"n\">saver</span><span class=\"o\">.</span><span class=\"n\">save</span><span class=\"p\">(</span><span class=\"n\">sess</span><span class=\"p\">,</span> <span class=\"n\">checkpoint</span><span class=\"o\">.</span><span class=\"n\">model_checkpoint_path</span><span class=\"p\">)</span>\n\n\n<span class=\"k\">def</span> <span class=\"nf\">main</span><span class=\"p\">(</span><span class=\"n\">argv</span><span class=\"p\">):</span>\n    <span class=\"n\">checkpoint_dir</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>\n    <span class=\"n\">replace_from</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>\n    <span class=\"n\">replace_to</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>\n    <span class=\"n\">add_prefix</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>\n    <span class=\"n\">dry_run</span> <span class=\"o\">=</span> <span class=\"kc\">False</span>\n\n    <span class=\"k\">try</span><span class=\"p\">:</span>\n        <span class=\"n\">opts</span><span class=\"p\">,</span> <span class=\"n\">args</span> <span class=\"o\">=</span> <span class=\"n\">getopt</span><span class=\"o\">.</span><span class=\"n\">getopt</span><span class=\"p\">(</span><span class=\"n\">argv</span><span class=\"p\">,</span> <span class=\"s1\">&#39;h&#39;</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"s1\">&#39;help=&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;checkpoint_dir=&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;replace_from=&#39;</span><span class=\"p\">,</span>\n                                               <span class=\"s1\">&#39;replace_to=&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;add_prefix=&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;dry_run&#39;</span><span class=\"p\">])</span>\n    <span class=\"k\">except</span> <span class=\"n\">getopt</span><span class=\"o\">.</span><span class=\"n\">GetoptError</span><span class=\"p\">:</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">usage_str</span><span class=\"p\">)</span>\n        <span class=\"n\">sys</span><span class=\"o\">.</span><span class=\"n\">exit</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n    <span class=\"k\">for</span> <span class=\"n\">opt</span><span class=\"p\">,</span> <span class=\"n\">arg</span> <span class=\"ow\">in</span> <span class=\"n\">opts</span><span class=\"p\">:</span>\n        <span class=\"k\">if</span> <span class=\"n\">opt</span> <span class=\"ow\">in</span> <span class=\"p\">(</span><span class=\"s1\">&#39;-h&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;--help&#39;</span><span class=\"p\">):</span>\n            <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">usage_str</span><span class=\"p\">)</span>\n            <span class=\"n\">sys</span><span class=\"o\">.</span><span class=\"n\">exit</span><span class=\"p\">()</span>\n        <span class=\"k\">elif</span> <span class=\"n\">opt</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;--checkpoint_dir&#39;</span><span class=\"p\">:</span>\n            <span class=\"n\">checkpoint_dir</span> <span class=\"o\">=</span> <span class=\"n\">arg</span>\n        <span class=\"k\">elif</span> <span class=\"n\">opt</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;--replace_from&#39;</span><span class=\"p\">:</span>\n            <span class=\"n\">replace_from</span> <span class=\"o\">=</span> <span class=\"n\">arg</span>\n        <span class=\"k\">elif</span> <span class=\"n\">opt</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;--replace_to&#39;</span><span class=\"p\">:</span>\n            <span class=\"n\">replace_to</span> <span class=\"o\">=</span> <span class=\"n\">arg</span>\n        <span class=\"k\">elif</span> <span class=\"n\">opt</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;--add_prefix&#39;</span><span class=\"p\">:</span>\n            <span class=\"n\">add_prefix</span> <span class=\"o\">=</span> <span class=\"n\">arg</span>\n        <span class=\"k\">elif</span> <span class=\"n\">opt</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;--dry_run&#39;</span><span class=\"p\">:</span>\n            <span class=\"n\">dry_run</span> <span class=\"o\">=</span> <span class=\"kc\">True</span>\n\n    <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">checkpoint_dir</span><span class=\"p\">:</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;Please specify a checkpoint_dir. Usage:&#39;</span><span class=\"p\">)</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">usage_str</span><span class=\"p\">)</span>\n        <span class=\"n\">sys</span><span class=\"o\">.</span><span class=\"n\">exit</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n\n    <span class=\"n\">rename</span><span class=\"p\">(</span><span class=\"n\">checkpoint_dir</span><span class=\"p\">,</span> <span class=\"n\">replace_from</span><span class=\"p\">,</span> <span class=\"n\">replace_to</span><span class=\"p\">,</span> <span class=\"n\">add_prefix</span><span class=\"p\">,</span> <span class=\"n\">dry_run</span><span class=\"p\">)</span>\n\n\n<span class=\"k\">if</span> <span class=\"vm\">__name__</span> <span class=\"o\">==</span> <span class=\"s1\">&#39;__main__&#39;</span><span class=\"p\">:</span>\n    <span class=\"n\">main</span><span class=\"p\">(</span><span class=\"n\">sys</span><span class=\"o\">.</span><span class=\"n\">argv</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">:])</span></code></pre></div><p>具体用法：</p><div class=\"highlight\"><pre><code class=\"language-text\">python tensorflow_rename_variables.py --checkpoint_dir=path/to/dir --replace_from=scope1 --replace_to=scope1/model --add_prefix=abc/</code></pre></div><p>代码来源：</p><a href=\"https://link.zhihu.com/?target=https%3A//gist.github.com/batzner/7c24802dd9c5e15870b4b56e22135c96\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">gist.github.com/batzner</span><span class=\"invisible\">/7c24802dd9c5e15870b4b56e22135c96</span><span class=\"ellipsis\"></span></a><p>stackoverflow上的问题：</p><a href=\"https://link.zhihu.com/?target=https%3A//stackoverflow.com/questions/37086268/rename-variable-scope-of-saved-model-in-tensorflow\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-2d47e939feed796bcf7483d306661c88_ipico.jpg\" data-image-width=\"316\" data-image-height=\"316\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Rename variable scope of saved model in TensorFlow</a><p></p>", 
            "topic": [
                {
                    "tag": "TensorFlow", 
                    "tagLink": "https://api.zhihu.com/topics/20032249"
                }
            ], 
            "comments": [
                {
                    "userName": "划海听愿", 
                    "userLink": "https://www.zhihu.com/people/e564f64057f2547b47588ee8a7dc2b6c", 
                    "content": "<p>模型7G, 出如下错误: Cannot serialize protocol buffer of type tensorflow.GraphDef as the serialized size: 2G 该如何解决？</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/31666137", 
            "userName": "休语", 
            "userLink": "https://www.zhihu.com/people/9fa98b02c1f908f202862a38bd15084f", 
            "upvote": 1, 
            "title": "调试TensorFlow的过程中用到的一些PyCharm技巧——将2空格缩进改成4空格缩进，", 
            "content": "<p>我一直在用PyCharm调试TensorFlow的代码，准备记录一下学到的技巧。</p><p><b>【1】将2空格缩进改成4空格缩进</b></p><p>今天调试一个代码，原始的代码用的是2个空格的缩进，我想把缩进改成4个空格。查了半天，最后发现还挺简单的：在Settings-&gt;Code Style-&gt;Python里先将indent设置为4。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-d63a482a8d9b329dfe47cf73866b303b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1557\" data-rawheight=\"1019\" class=\"origin_image zh-lightbox-thumb\" width=\"1557\" data-original=\"https://pic4.zhimg.com/v2-d63a482a8d9b329dfe47cf73866b303b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1557&#39; height=&#39;1019&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1557\" data-rawheight=\"1019\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1557\" data-original=\"https://pic4.zhimg.com/v2-d63a482a8d9b329dfe47cf73866b303b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-d63a482a8d9b329dfe47cf73866b303b_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>然后，在工程区选中要修改的文件，点Code-&gt;Reformat Code，就可以了。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-e15f8892493922b44872d21331429dfe_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"553\" data-rawheight=\"991\" class=\"origin_image zh-lightbox-thumb\" width=\"553\" data-original=\"https://pic3.zhimg.com/v2-e15f8892493922b44872d21331429dfe_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;553&#39; height=&#39;991&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"553\" data-rawheight=\"991\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"553\" data-original=\"https://pic3.zhimg.com/v2-e15f8892493922b44872d21331429dfe_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-e15f8892493922b44872d21331429dfe_b.jpg\"/></figure><p></p><p></p>", 
            "topic": [
                {
                    "tag": "TensorFlow", 
                    "tagLink": "https://api.zhihu.com/topics/20032249"
                }, 
                {
                    "tag": "PyCharm", 
                    "tagLink": "https://api.zhihu.com/topics/19602434"
                }, 
                {
                    "tag": "PyCharm使用技巧", 
                    "tagLink": "https://api.zhihu.com/topics/20139535"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/31865749", 
            "userName": "休语", 
            "userLink": "https://www.zhihu.com/people/9fa98b02c1f908f202862a38bd15084f", 
            "upvote": 3, 
            "title": "TensorFlow保留中间层的Feature Map", 
            "content": "<p>【1】在inference里面（每个卷积层后面），写：</p><div class=\"highlight\"><pre><code class=\"language-text\">#卷积层\nconv1 = conv_layer_with_bn(norm1, [7, 7, images.get_shape().as_list()[3], 64], phase_train, name=&#34;conv1&#34;)\n#保存特征图\nshow_feature_map(layer = conv1, layer_name=&#34;conv1&#34;, num_or_size_splits=64, axis=3, max_outputs=3)</code></pre></div><p>其中，show_feature_map()的定义如下：</p><div class=\"highlight\"><pre><code class=\"language-text\">def show_feature_map(layer, layer_name, num_or_size_splits, axis, max_outputs):\n    split = tf.split(layer, num_or_size_splits=num_or_size_splits, axis=axis)\n    for i in range(num_or_size_splits):\n        tf.summary.image(layer_name + &#34;-&#34; + str(i), split[i], max_outputs)</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>【2】在“with tf.Graph().as_default():”的最后，写：</p><div class=\"highlight\"><pre><code class=\"language-text\">summary_op = tf.summary.merge_all()</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>【3】在“with tf.Session() as sess:”里面，读取完数据之后，写：</p><div class=\"highlight\"><pre><code class=\"language-text\">summary_writer = tf.summary.FileWriter(train_dir, tf.get_default_graph())</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>【4】在要保存summary的时候，写：</p><div class=\"highlight\"><pre><code class=\"language-text\">summary_str = sess.run(summary_op, feed_dict=feed_dict)\nsummary_writer.add_summary(summary_str, step)</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>【5】用TensorBoard查看，如下图所示：</p><figure data-size=\"small\"><noscript><img src=\"https://pic2.zhimg.com/v2-3802a34cd8c66de3a2d73fdfd6dcc509_b.jpg\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"757\" data-rawheight=\"570\" class=\"origin_image zh-lightbox-thumb\" width=\"757\" data-original=\"https://pic2.zhimg.com/v2-3802a34cd8c66de3a2d73fdfd6dcc509_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;757&#39; height=&#39;570&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"757\" data-rawheight=\"570\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"757\" data-original=\"https://pic2.zhimg.com/v2-3802a34cd8c66de3a2d73fdfd6dcc509_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-3802a34cd8c66de3a2d73fdfd6dcc509_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>【6】怎样将这些图片批量下载到本地？我写了一个python脚本：</p><div class=\"highlight\"><pre><code class=\"language-text\">from urllib import request\n\n\ndef get_feature_maps(layer_num, feature_map_num, layer_name=&#34;conv&#34;, save_dir=&#34;G:\\\\Deep_Learning_Experiments\\\\SegNet-TensorFlow-Serials\\\\Tensorflow-Feature-Map-SegNet-deconv-TSD-Lane-Train-200000-20171207\\\\Feature-Map&#34;):\n    url = &#39;http://127.0.0.1:6006/data/plugin/images/individualImage?ts=1512829497.9180086&amp;sample=0&amp;index=0&amp;tag=&#39; + str(layer_name) + str(\n        layer_num) + &#39;-&#39; + str(feature_map_num) + &#39;%2Fimage%2F0&amp;run=.&#39;\n    save_path = save_dir + &#34;\\\\&#34; + layer_name + str(layer_name) + str(layer_num) + &#39;-&#39; + str(feature_map_num) + &#34;.png&#34;\n    with request.urlopen(url) as web:\n        with open(save_path, &#39;wb&#39;) as outfile:\n            outfile.write(web.read())\n\ndef main():\n    for i in range(1,5):\n        for j in range(64):\n            get_feature_maps(i, j)\n\nif __name__ == &#34;__main__&#34;:\n    main()</code></pre></div><p></p><p></p><p></p><p></p>", 
            "topic": [
                {
                    "tag": "TensorFlow", 
                    "tagLink": "https://api.zhihu.com/topics/20032249"
                }
            ], 
            "comments": [
                {
                    "userName": "知乎用户", 
                    "userLink": "https://www.zhihu.com/people/0", 
                    "content": "<p>请问这里max_ouputs=3怎么理解？</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/31793836", 
            "userName": "休语", 
            "userLink": "https://www.zhihu.com/people/9fa98b02c1f908f202862a38bd15084f", 
            "upvote": 0, 
            "title": "Windows下TensorBoard的使用", 
            "content": "<p>网络终于搭好了，训练也收敛了，是时候来分析一波训练结果啦！我是在Ubuntu下进行的训练，把结果拷贝到了Windows下面，用TensorBoard看一看训练的过程和结果。</p><h2><b>【1】Windows下启动TensorBoard时，logdir的路径</b></h2><div class=\"highlight\"><pre><code class=\"language-text\">tensorboard --logdir=G://TENSORBOARD_LOG_DATA_BACKUP</code></pre></div><p>注意一定要用双斜杠：“<b>//</b>”</p><h2><b>【2】把结果导出到CSV文件</b></h2><p>点击Scalar：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-6aae06f5e2289746d06368d022734ae1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1466\" data-rawheight=\"184\" class=\"origin_image zh-lightbox-thumb\" width=\"1466\" data-original=\"https://pic2.zhimg.com/v2-6aae06f5e2289746d06368d022734ae1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1466&#39; height=&#39;184&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1466\" data-rawheight=\"184\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1466\" data-original=\"https://pic2.zhimg.com/v2-6aae06f5e2289746d06368d022734ae1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-6aae06f5e2289746d06368d022734ae1_b.jpg\"/></figure><p>选中左上角的“Show data download links”</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-86bc811dfda5aeadb5abcf5bf694e8ac_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"291\" data-rawheight=\"143\" class=\"content_image\" width=\"291\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;291&#39; height=&#39;143&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"291\" data-rawheight=\"143\" class=\"content_image lazy\" width=\"291\" data-actualsrc=\"https://pic1.zhimg.com/v2-86bc811dfda5aeadb5abcf5bf694e8ac_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>可以看到右边的那些图像旁边出现了这样的按钮：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-cf2c1545b2c5c4580a54fd74232f2244_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"344\" data-rawheight=\"273\" class=\"content_image\" width=\"344\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;344&#39; height=&#39;273&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"344\" data-rawheight=\"273\" class=\"content_image lazy\" width=\"344\" data-actualsrc=\"https://pic1.zhimg.com/v2-cf2c1545b2c5c4580a54fd74232f2244_b.jpg\"/></figure><p>点击进行下载，就可以得到CSV文件：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ab59a72b940fb2cd51163e1fac32b3d4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"241\" data-rawheight=\"426\" class=\"content_image\" width=\"241\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;241&#39; height=&#39;426&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"241\" data-rawheight=\"426\" class=\"content_image lazy\" width=\"241\" data-actualsrc=\"https://pic1.zhimg.com/v2-ab59a72b940fb2cd51163e1fac32b3d4_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "TensorFlow", 
                    "tagLink": "https://api.zhihu.com/topics/20032249"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/31739752", 
            "userName": "休语", 
            "userLink": "https://www.zhihu.com/people/9fa98b02c1f908f202862a38bd15084f", 
            "upvote": 0, 
            "title": "TensorFlow拆分Tensor：tf.dynamic_partition()和tf.split()", 
            "content": "<p>最近在写一个代码，需要把一幅图像RGB三个通道里面的RG加起来，B丢掉。</p><p>这里有一个“暗坑”：cv2的存储格式是BGR，而skimage的存储格式是RGB。</p><p>我现在用的是skimage，也就是说要前两个通道相加，丢弃第三个通道。</p><p>reduce_sum貌似不能对各个维度加权，用笨办法：将Tensor按通道拆分，再把前两个拼接起来，再用reduce_sum。</p><h2><b>tf.dynamic_partition()</b></h2><p>可以用tf.dynamic_partition，以下是官方文档的笔记：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ff41088991136a26d25ac46c31f16c10_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"874\" data-rawheight=\"2320\" class=\"origin_image zh-lightbox-thumb\" width=\"874\" data-original=\"https://pic1.zhimg.com/v2-ff41088991136a26d25ac46c31f16c10_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;874&#39; height=&#39;2320&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"874\" data-rawheight=\"2320\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"874\" data-original=\"https://pic1.zhimg.com/v2-ff41088991136a26d25ac46c31f16c10_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-ff41088991136a26d25ac46c31f16c10_b.jpg\"/></figure><h2><b>tf.split()</b></h2><p>也可以用tf.split(value, num_or_size_splits, axis=0, num=None, name=&#39;split&#39;)，更加简便:</p><p>基本的使用方法是：</p><p>tf.split</p><p>        (</p><p>           value=被split的Tensor, </p><p>           num_or_size_splits=[第一份Tensor在被split的维度上分了多少, 第二份Tensor在被                  split的维度上分了多少,……], </p><p>           axis=在哪个维度上被split</p><p>         )</p>", 
            "topic": [
                {
                    "tag": "TensorFlow", 
                    "tagLink": "https://api.zhihu.com/topics/20032249"
                }
            ], 
            "comments": [
                {
                    "userName": "gzchenAI", 
                    "userLink": "https://www.zhihu.com/people/473d3ae1615c8fcb4529eeeb8c53687c", 
                    "content": "<p>针对split的用法是否可以出一下实例看看呢</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/31668756", 
            "userName": "休语", 
            "userLink": "https://www.zhihu.com/people/9fa98b02c1f908f202862a38bd15084f", 
            "upvote": 1, 
            "title": "Python中测试程序运行速度", 
            "content": "<p>调试TensorFlow代码，免不了测试某一块程序的运行时间。在Python里，一般用time.time()和time.clock()测试时间。关于这两个函数哪个更准确，查到了如下资料：</p><p><a href=\"https://link.zhihu.com/?target=https%3A//www.cnblogs.com/youxin/p/3157099.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">python计算程序运行时间 - youxin - 博客园</a></p><blockquote>究竟是使用 time.clock() 精度高，还是使用 time.time() 精度更高，要视乎所在的平台来决定。总概来讲，在 Unix 系统中，建议使用 time.time()，在 Windows 系统中，建议使用 time.clock()。<br/><br/>这个结论也可以在 Python 的 timtit 模块中（用于简单测量程序代码执行时间的内建模块）得到论证：<br/><br/>if sys.platform == &#34;win32&#34;:<br/># On Windows, the best timer is time.clock()<br/>default_timer = time.clock<br/>else:<br/># On most other platforms the best timer is time.time()<br/>default_timer = time.time<br/>使用 timeit 代替 time，这样就可以实现跨平台的精度性</blockquote><p>这么说来，当然更好的实现是用Python 的 timtit 模块：</p><div class=\"highlight\"><pre><code class=\"language-text\">start = timeit.default_timer()\n... do something\nelapsed = (timeit.default_timer() - start)</code></pre></div><p>最后一个问题就是，这样计算得到的时间单位是多少？</p><p><a href=\"https://link.zhihu.com/?target=http%3A//blog.csdn.net/lanchunhui/article/details/50214533\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Python基础--timeit模块 - CSDN博客</a></p><p>答案是：“秒”</p><p></p>", 
            "topic": [
                {
                    "tag": "TensorFlow", 
                    "tagLink": "https://api.zhihu.com/topics/20032249"
                }, 
                {
                    "tag": "Python", 
                    "tagLink": "https://api.zhihu.com/topics/19552832"
                }, 
                {
                    "tag": "Python 入门", 
                    "tagLink": "https://api.zhihu.com/topics/19661050"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/31459527", 
            "userName": "休语", 
            "userLink": "https://www.zhihu.com/people/9fa98b02c1f908f202862a38bd15084f", 
            "upvote": 15, 
            "title": "TensorFlow的Summary", 
            "content": "<p>首先来看关于Summary的介绍（From <a href=\"https://link.zhihu.com/?target=http%3A//blog.csdn.net/shenxiaolu1984/article/details/52815641\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【TensorFlow动手玩】常用集合: Variable, Summary, 自定义</a>）：</p><h2><a href=\"https://link.zhihu.com/?target=https%3A//www.tensorflow.org/versions/r0.11/api_docs/python/train.html%23summary-operations\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Summary</a></h2><p><code>Summary</code>被收集在名为<code>tf.GraphKeys.SUMMARIES</code>的<code>colletion</code>中</p><h2>定义</h2><p><code>Summary</code>是对网络中<code>Tensor</code>取值进行监测的一种<code>Operation</code>。这些操作在图中是“外围”操作，不影响数据流本身。<b>注意：Summary本身也是一个op</b></p><h2>1.1 Tensorboard的数据形式</h2><p>Tensorboard可以记录与展示以下数据形式： <br/>（1）标量Scalars <br/>（2）图片Images <br/>（3）音频Audio <br/>（4）计算图Graph <br/>（5）数据分布Distribution <br/>（6）直方图Histograms <br/>（7）嵌入向量Embeddings</p><h2>1.2 Tensorboard的可视化过程</h2><p>（1）首先肯定是先建立一个graph,你想从这个graph中获取某些数据的信息</p><p>（2）确定要在graph中的哪些节点放置summary operations以记录信息 <br/>使用tf.summary.scalar记录标量 <br/>使用tf.summary.histogram记录数据的直方图 <br/>使用tf.summary.distribution记录数据的分布图 <br/>使用tf.summary.image记录图像数据 <br/>….</p><p>（3）<b>operations并不会去真的执行计算，除非你告诉他们需要去run,或者它被其他的需要run的operation所依赖</b>。sess.run(op) 或者sess.run(op-&gt;依赖之)</p><p>而我们<b>上一步创建的这些summary operations其实并不被其他节点依赖</b>，因此，我们<b>需要特地去运行所有的summary节点</b>。但是呢，一份程序下来可能有超多这样的summary 节点，要手动一个一个去启动自然是及其繁琐的，因此我们可以使用<b>tf.summary.merge_all</b>去将所有summary节点合并成一个节点，只要运行这个节点，就能<b>产生</b>所有我们之前设置的<b>summary data</b>。</p><p>（4）使用<b>tf.summary.FileWriter</b>将运行后输出的数据都保存到本地磁盘中</p><p>（5）运行整个程序，并在命令行输入运行tensorboard的指令，之后打开web端可查看可视化的结果</p><h2>用例</h2><p>我们模仿常见的训练过程，创建一个最简单的用例。</p><div class=\"highlight\"><pre><code class=\"language-text\"># 迭代的计数器\nglobal_step = tf.Variable(0, trainable=False)\n# 迭代的+1操作\nincrement_op = tf.assign_add(global_step, tf.constant(1))\n# 实例应用中，+1操作往往在`tf.train.Optimizer.apply_gradients`内部完成。\n\n# 创建一个根据计数器衰减的Tensor\nlr = tf.train.exponential_decay(0.1, global_step, decay_steps=1, decay_rate=0.9, staircase=False)\n\n# 把Tensor添加到观测中\ntf.scalar_summary(&#39;learning_rate&#39;, lr)\n\n# 并获取所有监测的操作`sum_opts`\nsum_ops = tf.merge_all_summaries()\n\n# 初始化sess\nsess = tf.Session()\ninit = tf.initialize_all_variables()\nsess.run(init)  # 在这里global_step被赋初值\n\n# 指定监测结果输出目录\nsummary_writer = tf.train.SummaryWriter(&#39;/tmp/log/&#39;, sess.graph)\n\n# 启动迭代\nfor step in range(0, 10):\n    s_val = sess.run(sum_ops)    # 获取serialized监测结果：bytes类型的字符串\n    summary_writer.add_summary(s_val, global_step=step)   # 写入文件\n    sess.run(increment_op)     # 计数器+1</code></pre></div><p>调用tf.scalar_summary系列函数时，就会向默认的<code>collection</code>中添加一个<code>Operation</code>。</p><p>再次回顾“<b>零存整取</b>”原则：创建网络的各个层次都可以添加监测；在添加完所有监测，初始化sess之前，统一用tf.merge_all_summaries获取。</p><h2>查看</h2><p>SummaryWriter文件中存储的是序列化的结果，需要借助<a href=\"https://link.zhihu.com/?target=https%3A//github.com/tensorflow/tensorflow/blob/r0.11/tensorflow/tensorboard/README.md\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">TensorBoard</a>才能查看。</p><p>在命令行中运行tensorboard，传入存储SummaryWriter文件的目录：</p><div class=\"highlight\"><pre><code class=\"language-text\">tensorboard --logdir /tmp/log </code></pre></div><p>完成后会提示：</p><div class=\"highlight\"><pre><code class=\"language-text\">You can navigate to http://127.0.1.1:6006</code></pre></div><p>可以直接使用服务器本地浏览器访问这个地址（本机6006端口），或者使用远程浏览器访问服务器ip地址的6006端口。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>参考资料：</p><p><a href=\"https://link.zhihu.com/?target=http%3A//blog.csdn.net/sinat_33761963/article/details/62433234\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">06：Tensorflow的可视化工具Tensorboard的初步使用 - CSDN博客</a></p><p><a href=\"https://link.zhihu.com/?target=http%3A//blog.csdn.net/shenxiaolu1984/article/details/52815641\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">【TensorFlow动手玩】常用集合: Variable, Summary, 自定义</a></p><p></p><p></p>", 
            "topic": [
                {
                    "tag": "TensorFlow", 
                    "tagLink": "https://api.zhihu.com/topics/20032249"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/31541154", 
            "userName": "休语", 
            "userLink": "https://www.zhihu.com/people/9fa98b02c1f908f202862a38bd15084f", 
            "upvote": 3, 
            "title": "Geoffrey Hinton的CapsNet（《Dynamic Routing Between Capsules》）中的tf.stop_gradient()", 
            "content": "<p><b>tf.stop_gradient()</b>:</p><p>stop_gradient(<br/>    input,<br/>    name=None ) </p><p>在<a href=\"https://link.zhihu.com/?target=https%3A//github.com/naturomics/CapsNet-Tensorflow\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">naturomics/CapsNet-Tensorflow</a> 实现的Capsule里面：</p><div class=\"highlight\"><pre><code class=\"language-text\">  # In forward, u_hat_stopped = u_hat; in backward, no gradient passed back from u_hat_stopped to u_hat\n    u_hat_stopped = tf.stop_gradient(u_hat, name=&#39;stop_gradient&#39;)</code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-15e24ecdb56302dc9c17eba6ac7e8131_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"360\" data-rawheight=\"270\" class=\"content_image\" width=\"360\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;360&#39; height=&#39;270&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"360\" data-rawheight=\"270\" class=\"content_image lazy\" width=\"360\" data-actualsrc=\"https://pic2.zhimg.com/v2-15e24ecdb56302dc9c17eba6ac7e8131_b.jpg\"/></figure><p>这里的u_hat_stopped是u_hat的拷贝，但是梯度不会从u_hat_stopped传到u_hat。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>下面是tensorflow的文档：</p><p>See the guide: <a href=\"https://link.zhihu.com/?target=https%3A//tensorflow.google.cn/versions/master/api_guides/python/train%23Gradient_Computation\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Training &gt; Gradient Computation</a></p><p>Stops gradient computation.</p><p>When executed in a graph, <b>this op outputs its input tensor as-is</b>.<b>直接拷贝输出</b></p><p>When building ops to compute gradients, this op prevents the contribution of its inputs to be taken into account. </p><p>在计算梯度的时候，输入（例如u_hat，被认为是不存在的）</p><p>Normally, the gradient generator adds ops to a graph to compute the derivatives（导数） of a specified &#39;loss&#39; by recursively finding out inputs that contributed to its computation. If you insert this op in the graph <b>it inputs are masked from the gradient generator</b>. They are not taken into account for computing gradients.</p><p>This is useful any time you want to compute a value with TensorFlow but need to pretend that the value was a constant. Some examples include:</p><ul><li>The <i>EM</i> algorithm where the <i>M-step</i> should not involve backpropagation through the output of the <i>E-step</i>.</li><li>Contrastive divergence training of Boltzmann machines where, when differentiating the energy function, the training must not backpropagate through the graph that generated the samples from the model.</li><li>Adversarial training, where no backprop should happen through the adversarial example generation process.</li></ul><p>Args:</p><ul><li><b>input</b>: A Tensor.</li><li><b>name</b>: A name for the operation (optional).</li></ul><p>Returns:</p><h2>A Tensor. Has the same type as input.</h2><p>代码和具体的解释如下：</p><div class=\"highlight\"><pre><code class=\"language-text\">   u_hat = tf.matmul(W, input, transpose_a=True)\n    assert u_hat.get_shape() == [cfg.batch_size, 1152, 10, 16, 1]\n\n    # In forward, u_hat_stopped = u_hat; in backward, no gradient passed back from u_hat_stopped to u_hat\n    u_hat_stopped = tf.stop_gradient(u_hat, name=&#39;stop_gradient&#39;)\n\n    # line 3,for r iterations do\n    for r_iter in range(cfg.iter_routing):\n        with tf.variable_scope(&#39;iter_&#39; + str(r_iter)):\n            # line 4:\n            # =&gt; [1, 1152, 10, 1, 1]\n            c_IJ = tf.nn.softmax(b_IJ, dim=2)\n\n            # At last iteration, use `u_hat` in order to receive gradients from the following graph\n            if r_iter == cfg.iter_routing - 1:\n                # line 5:\n                # weighting u_hat with c_IJ, element-wise in the last two dims\n                # =&gt; [batch_size, 1152, 10, 16, 1]\n                s_J = tf.multiply(c_IJ, u_hat)\n                # then sum in the second dim, resulting in [batch_size, 1, 10, 16, 1]\n                s_J = tf.reduce_sum(s_J, axis=1, keep_dims=True)\n                assert s_J.get_shape() == [cfg.batch_size, 1, 10, 16, 1]\n\n                # line 6:\n                # squash using Eq.1,\n                v_J = squash(s_J)\n                assert v_J.get_shape() == [cfg.batch_size, 1, 10, 16, 1]\n            elif r_iter &lt; cfg.iter_routing - 1:  # Inner iterations, do not apply backpropagation\n                s_J = tf.multiply(c_IJ, u_hat_stopped)\n                s_J = tf.reduce_sum(s_J, axis=1, keep_dims=True)\n                v_J = squash(s_J)\n\n                # line 7:\n                # reshape &amp; tile v_j from [batch_size ,1, 10, 16, 1] to [batch_size, 1152, 10, 16, 1]\n                # then matmul in the last tow dim: [16, 1].T x [16, 1] =&gt; [1, 1], reduce mean in the\n                # batch_size dim, resulting in [1, 1152, 10, 1, 1]\n                v_J_tiled = tf.tile(v_J, [1, 1152, 1, 1, 1])\n                u_produce_v = tf.matmul(u_hat_stopped, v_J_tiled, transpose_a=True)\n                assert u_produce_v.get_shape() == [cfg.batch_size, 1152, 10, 1, 1]\n\n                # b_IJ += tf.reduce_sum(u_produce_v, axis=0, keep_dims=True)\n                b_IJ += u_produce_v</code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-597286756cea4d2ce497ae6b9d115ff7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1505\" data-rawheight=\"430\" class=\"origin_image zh-lightbox-thumb\" width=\"1505\" data-original=\"https://pic4.zhimg.com/v2-597286756cea4d2ce497ae6b9d115ff7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1505&#39; height=&#39;430&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1505\" data-rawheight=\"430\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1505\" data-original=\"https://pic4.zhimg.com/v2-597286756cea4d2ce497ae6b9d115ff7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-597286756cea4d2ce497ae6b9d115ff7_b.jpg\"/></figure><p>在这个循环里，循环中间的u_hat实际上在是在一个batch里面的，不应该反向传播。但是，tensorflow对这个for循环的执行，实际上是展成一个长链（用tf.variable_scope(&#39;iter_&#39; + str(r_iter))充当namespace，区分不同的变量）【自己的理解，不确定对不对】，所以要保证中间循环的op节点不会产生梯度，对反向传播的梯度造成影响。所以这里采用了tf.stop_gradient()构造了一个类似“中间变量”的不传梯度的u_hat.</p><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/naturomics/CapsNet-Tensorflow\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">naturomics/CapsNet-Tensorflow</a>实现的第一个版本没有采用这个机制，报道迭代次数为1的时候效果最好；第二个版本使用了tf.stop_gradient()，貌似是迭代次数为3的时候效果最好。结合论文的意思，看来使用tf.stop_gradient()是正确的。</p><p></p>", 
            "topic": [
                {
                    "tag": "TensorFlow", 
                    "tagLink": "https://api.zhihu.com/topics/20032249"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/31308247", 
            "userName": "休语", 
            "userLink": "https://www.zhihu.com/people/9fa98b02c1f908f202862a38bd15084f", 
            "upvote": 7, 
            "title": "执行tf.convert_to_tensor()时，究竟发生了什么？", 
            "content": "<p></p><p></p><p></p><p>最近在看别人TensorFlow的代码，总想弄明白“这句命令什么时候执行，执行之后发生了什么”，特别是读取数据的时候。今天弄明白了tf.convert_to_tensor()执行的时候发生了什么，在这里做一下笔记。</p><p>先看代码：</p><div class=\"highlight\"><pre><code class=\"language-python\">        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">image_list</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">label_list</span> <span class=\"o\">=</span> <span class=\"n\">read_labeled_image_list</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">data_dir</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">data_list</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">images</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">convert_to_tensor</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">image_list</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">string</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">labels</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">convert_to_tensor</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">label_list</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">string</span><span class=\"p\">)</span></code></pre></div><p>这里的read_labeled_image_list是用来读取图像和标签的路径的，返回两个list。</p><p>打上断点，看这两句话执行之后的效果。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-fe3f11660ee3ce0274efdd7252c73861_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"837\" data-rawheight=\"175\" class=\"origin_image zh-lightbox-thumb\" width=\"837\" data-original=\"https://pic2.zhimg.com/v2-fe3f11660ee3ce0274efdd7252c73861_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;837&#39; height=&#39;175&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"837\" data-rawheight=\"175\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"837\" data-original=\"https://pic2.zhimg.com/v2-fe3f11660ee3ce0274efdd7252c73861_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-fe3f11660ee3ce0274efdd7252c73861_b.jpg\"/></figure><p>tf.convert_to_tensor()执行后返回一个Tensor，问题是，这个Tensor是什么样子的？</p><p>在TF的Graph中，Tensor是边，Op是点，TensorFlow将Tensor与对应的Op的关系描述为“The Operation that produces this tensor as an output.”</p><p>我们可以看到这个Tensor的Op是谁：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-e2cda8f62c39391676492fd594aa928c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1544\" data-rawheight=\"565\" class=\"origin_image zh-lightbox-thumb\" width=\"1544\" data-original=\"https://pic1.zhimg.com/v2-e2cda8f62c39391676492fd594aa928c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1544&#39; height=&#39;565&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1544\" data-rawheight=\"565\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1544\" data-original=\"https://pic1.zhimg.com/v2-e2cda8f62c39391676492fd594aa928c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-e2cda8f62c39391676492fd594aa928c_b.jpg\"/></figure><div class=\"highlight\"><pre><code class=\"language-protobuf\"><span class=\"n\">name</span><span class=\"o\">:</span> <span class=\"s\">&#34;create_inputs/Const&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"n\">op</span><span class=\"o\">:</span> <span class=\"s\">&#34;Const&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;dtype&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">type</span><span class=\"o\">:</span> <span class=\"n\">DT_STRING</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;value&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">tensor</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">dtype</span><span class=\"o\">:</span> <span class=\"n\">DT_STRING</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">tensor_shape</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"n\">dim</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>          <span class=\"n\">size</span><span class=\"o\">:</span> <span class=\"mi\">5598</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">string_val</span><span class=\"o\">:</span> <span class=\"s\">&#34;00000.png&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">string_val</span><span class=\"o\">:</span> <span class=\"s\">&#34;00001.png&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">string_val</span><span class=\"o\">:</span> <span class=\"s\">&#34;00002.png&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>                <span class=\"err\">……\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"p\">}</span></code></pre></div><p>可以看到执行完这两条语句之后，当前的Graph上多出来两个Op：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-51b44d3bfd7c61b777e257458bbf920a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1473\" data-rawheight=\"145\" class=\"origin_image zh-lightbox-thumb\" width=\"1473\" data-original=\"https://pic3.zhimg.com/v2-51b44d3bfd7c61b777e257458bbf920a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1473&#39; height=&#39;145&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1473\" data-rawheight=\"145\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1473\" data-original=\"https://pic3.zhimg.com/v2-51b44d3bfd7c61b777e257458bbf920a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-51b44d3bfd7c61b777e257458bbf920a_b.jpg\"/></figure><p>综上所述，执行tf.convert_to_tensor()的时候，在图上生成了一个Op，Op中保存了传入参数的数据。</p><p></p>", 
            "topic": [
                {
                    "tag": "TensorFlow", 
                    "tagLink": "https://api.zhihu.com/topics/20032249"
                }
            ], 
            "comments": [
                {
                    "userName": "Blue蟹将军", 
                    "userLink": "https://www.zhihu.com/people/6476fe0f19732fec6a8b20d05ddee615", 
                    "content": "<p>学习了！btw，请问您用的能显示变量的IDE是什么？Spyder只能显示内置变量，tensorflow的变量不能显示。</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "休语", 
                            "userLink": "https://www.zhihu.com/people/9fa98b02c1f908f202862a38bd15084f", 
                            "content": "编译器是pycharm~<br>如果觉得有帮助的话，就点个赞呗?谢谢啦~", 
                            "likes": 1, 
                            "replyToAuthor": "Blue蟹将军"
                        }, 
                        {
                            "userName": "Blue蟹将军", 
                            "userLink": "https://www.zhihu.com/people/6476fe0f19732fec6a8b20d05ddee615", 
                            "content": "<p>非常有帮助，再次感谢！</p>", 
                            "likes": 0, 
                            "replyToAuthor": "休语"
                        }
                    ]
                }, 
                {
                    "userName": "知乎用户", 
                    "userLink": "https://www.zhihu.com/people/0", 
                    "content": "<p>read_labeled_image_list 这个在哪有？</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/31402245", 
            "userName": "休语", 
            "userLink": "https://www.zhihu.com/people/9fa98b02c1f908f202862a38bd15084f", 
            "upvote": 0, 
            "title": "TensorFlow调试常用命令", 
            "content": "<p>总结一下调试TensorFlow代码的时候经常用到的命令：</p><p>用tensorboard查看当前Graph：</p><div class=\"highlight\"><pre><code class=\"language-text\">summary_write = tf.summary.FileWriter(&#34;G:\\\\TENSORBOARD_LOG_DATA&#34; , tf.get_default_graph())</code></pre></div><p>然后用tensorboard查看生成的Graph：</p><div class=\"highlight\"><pre><code class=\"language-ps1con\">tensorboard --logdir=G://TENSORBOARD_LOG_DATA</code></pre></div><p>查看当前Graph的Protobuf（Protobuf比较长的话，放在txt里面看着更方便）：</p><div class=\"highlight\"><pre><code class=\"language-text\">file = open(&#34;1.txt&#34;,&#34;w&#34;)\nfile.write(str(tf.get_default_graph().as_graph_def()))\nfile.close()</code></pre></div><p>查看Tensor每个维度：</p><div class=\"highlight\"><pre><code class=\"language-text\">images.get_shape().as_list()[3]</code></pre></div><p></p><p></p>", 
            "topic": [
                {
                    "tag": "TensorFlow", 
                    "tagLink": "https://api.zhihu.com/topics/20032249"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/31361295", 
            "userName": "休语", 
            "userLink": "https://www.zhihu.com/people/9fa98b02c1f908f202862a38bd15084f", 
            "upvote": 8, 
            "title": "TensorFlow中的Queue和QueueRunner", 
            "content": "<p>一句话概括就是：Queue-&gt;（构建图阶段）创建队列；QueueRunner-&gt;（构建图阶段）创建线程进行入队操作；tf.train.start_queue_runners()-&gt;（执行图阶段）填充队列；tf.train.Coordinator() 在线程出错时关闭之。 </p><p>【1】从流水线（pipeline）说起</p><p>最早用到的TensorFlow读数据的方法就是把数据集中所有的数据都读到内存中。但是有些数据集很大，没法一次读入内存。我们可以按batch读数据，一次读入一个batch。如果是纯串行的操作，即“读数据-&gt;计算-&gt;读数据”，这样的计算效率就相当低。</p><p>一个可行的改进就是“流水线”（pipeline）。流水线的一个简明的解释如下：（来自<a href=\"https://www.zhihu.com/people/young-cc-75\" class=\"internal\">young cc</a>的答案<a href=\"https://www.zhihu.com/question/35024996/answer/62322067\" class=\"internal\"><span class=\"invisible\">https://www.</span><span class=\"visible\">zhihu.com/question/3502</span><span class=\"invisible\">4996/answer/62322067</span><span class=\"ellipsis\"></span></a>：）</p><p>流水线(pipeline)是将组合逻辑进行分割，能让任务以类似并行方式处理，<b>提高系统频率，提高吞吐量(throughput),使各模块利用率达到最高</b>。<br/>举个例子，假设洗衣分为四个步骤，分别在四个设备上进行，其中漂洗机器，烘干机器，在衣架上折叠衣服，把衣服放进柜子里各用30分钟，全过程需两小时。现有四个人去洗衣服，若sequential处理，一个人完成全步骤别人才开始，每人两小时，四个人共用八小时。如下图：<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/390bc39fba2beed3db663a19673af545_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"884\" data-rawheight=\"655\" class=\"origin_image zh-lightbox-thumb\" width=\"884\" data-original=\"https://pic2.zhimg.com/390bc39fba2beed3db663a19673af545_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;884&#39; height=&#39;655&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"884\" data-rawheight=\"655\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"884\" data-original=\"https://pic2.zhimg.com/390bc39fba2beed3db663a19673af545_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/390bc39fba2beed3db663a19673af545_b.jpg\"/></figure><p>但如果利用pipeline式的流水处理，当某人完成某步骤，其所用的设备就空闲了，后面的人就开始使用，四个人洗衣服只用3.5小时就能完成。如下图：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/55192854ed0b8c60f021eea8daa7a26b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"899\" data-rawheight=\"658\" class=\"origin_image zh-lightbox-thumb\" width=\"899\" data-original=\"https://pic4.zhimg.com/55192854ed0b8c60f021eea8daa7a26b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;899&#39; height=&#39;658&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"899\" data-rawheight=\"658\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"899\" data-original=\"https://pic4.zhimg.com/55192854ed0b8c60f021eea8daa7a26b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/55192854ed0b8c60f021eea8daa7a26b_b.jpg\"/></figure><p>注意，pipeline<b>只是提高系统的吞吐量，不能改善单个任务的latency</b>。在实际电路中是在组合逻辑中插入register，分割组合逻辑实现pipeline，而register读写也需要时间，所以<b>单个任务的执行时间反而会增长</b>。另外在分割组合逻辑时，<b>使分割后的每段处理时间尽量相同</b>，因为系统时钟是由最慢的那段决定的。如下图：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/d3ff470b0dc54dc88406edc5b6fb5dbf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"929\" data-rawheight=\"628\" class=\"origin_image zh-lightbox-thumb\" width=\"929\" data-original=\"https://pic4.zhimg.com/d3ff470b0dc54dc88406edc5b6fb5dbf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;929&#39; height=&#39;628&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"929\" data-rawheight=\"628\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"929\" data-original=\"https://pic4.zhimg.com/d3ff470b0dc54dc88406edc5b6fb5dbf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/d3ff470b0dc54dc88406edc5b6fb5dbf_b.jpg\"/></figure><p>TensorFlow中，读取数据的线程负责把数据从硬盘读到内存中的队列里面，计算的线程从内存中的队列得到数据进行计算。也就是说，读数据的线程只管把数据读到内存中，计算的线程只管从内存中取数据。两者都不会空闲下来等对方。</p><p>【2】Queue：</p><p>Queue，队列，本身也是图中的一个节点。入队和出队的操作（enqueue, dequeue）也是图中的节点，可以修改Queue节点中的内容。类似Variable，用来存放数据。</p><p>如果<code>Queue</code>中的数据满了,那么<code>enqueue</code>操作将会阻塞,如果<code>Queue</code>是空的,那么<code>dequeue</code>操作就会阻塞。如果操作不当，可能会出现程序卡住的问题，我就遇到过这个情况。</p><p>在常用环境中,一般是有多个<code>enqueue</code>线程同时像<code>Queue</code>中放数据,有一个<code>dequeue</code>操作从<code>Queue</code>中取数据。一般来说<code>enqueue</code>线程就是准备数据的线程,<code>dequeue</code>线程就是训练数据的线程. </p><p>运行了以下代码（From <a href=\"https://link.zhihu.com/?target=http%3A//blog.csdn.net/lujiandong1/article/details/53369961\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">tensorflow中关于队列使用的实验 - CSDN博客</a>），以理解Queue究竟发生了什么</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"c1\">#-*- coding:utf-8 -*-  </span>\n<span class=\"kn\">import</span> <span class=\"nn\">tensorflow</span> <span class=\"k\">as</span> <span class=\"nn\">tf</span>  \n  \n<span class=\"c1\">#创建的图:一个先入先出队列,以及初始化,出队,+1,入队操作  </span>\n<span class=\"n\">q</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">FIFOQueue</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"s2\">&#34;float&#34;</span><span class=\"p\">)</span>  \n<span class=\"n\">init</span> <span class=\"o\">=</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">enqueue_many</span><span class=\"p\">(([</span><span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"mf\">0.2</span><span class=\"p\">,</span> <span class=\"mf\">0.3</span><span class=\"p\">],))</span>  \n<span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">dequeue</span><span class=\"p\">()</span>  \n<span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">x</span> <span class=\"o\">+</span> <span class=\"mi\">1</span>  \n<span class=\"n\">q_inc</span> <span class=\"o\">=</span> <span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">enqueue</span><span class=\"p\">([</span><span class=\"n\">y</span><span class=\"p\">])</span>  \n  \n<span class=\"c1\">#开启一个session,session是会话,会话的潜在含义是状态保持,各种tensor的状态保持  </span>\n<span class=\"k\">with</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">Session</span><span class=\"p\">()</span> <span class=\"k\">as</span> <span class=\"n\">sess</span><span class=\"p\">:</span>  \n        <span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">init</span><span class=\"p\">)</span>  \n  \n        <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">):</span>  \n                <span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">q_inc</span><span class=\"p\">)</span>  \n  \n        <span class=\"n\">quelen</span> <span class=\"o\">=</span>  <span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">size</span><span class=\"p\">())</span>  \n        <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">quelen</span><span class=\"p\">):</span>  \n                <span class=\"nb\">print</span> <span class=\"p\">(</span><span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">q</span><span class=\"o\">.</span><span class=\"n\">dequeue</span><span class=\"p\">()))</span> </code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"c1\"># -*- coding:utf-8 -*-</span>\n<span class=\"kn\">import</span> <span class=\"nn\">tensorflow</span> <span class=\"k\">as</span> <span class=\"nn\">tf</span>\n\n<span class=\"c1\"># 创建的图:一个先入先出队列,以及初始化,出队,+1,入队操作</span>\n<span class=\"n\">q</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">FIFOQueue</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"s2\">&#34;float&#34;</span><span class=\"p\">)</span></code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-63b8404c79a3de1e4121faa0e2a6977d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"184\" data-rawheight=\"116\" class=\"content_image\" width=\"184\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;184&#39; height=&#39;116&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"184\" data-rawheight=\"116\" class=\"content_image lazy\" width=\"184\" data-actualsrc=\"https://pic2.zhimg.com/v2-63b8404c79a3de1e4121faa0e2a6977d_b.jpg\"/></figure><div class=\"highlight\"><pre><code class=\"language-text\">init = q.enqueue_many(([0.1, 0.2, 0.3],))</code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-22456b26f1bc10a605fc42b56de05858_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"160\" data-rawheight=\"178\" class=\"content_image\" width=\"160\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;160&#39; height=&#39;178&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"160\" data-rawheight=\"178\" class=\"content_image lazy\" width=\"160\" data-actualsrc=\"https://pic1.zhimg.com/v2-22456b26f1bc10a605fc42b56de05858_b.jpg\"/></figure><div class=\"highlight\"><pre><code class=\"language-protobuf\"><span class=\"n\">node</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">name</span><span class=\"o\">:</span> <span class=\"s\">&#34;fifo_queue&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">op</span><span class=\"o\">:</span> <span class=\"s\">&#34;FIFOQueueV2&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;capacity&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">i</span><span class=\"o\">:</span> <span class=\"mi\">3</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;component_types&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">list</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"n\">type</span><span class=\"o\">:</span> <span class=\"n\">DT_FLOAT</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;container&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">s</span><span class=\"o\">:</span> <span class=\"s\">&#34;&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;shapes&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">list</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;shared_name&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">s</span><span class=\"o\">:</span> <span class=\"s\">&#34;&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"n\">node</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">name</span><span class=\"o\">:</span> <span class=\"s\">&#34;fifo_queue_EnqueueMany/component_0&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">op</span><span class=\"o\">:</span> <span class=\"s\">&#34;Const&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;dtype&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">type</span><span class=\"o\">:</span> <span class=\"n\">DT_FLOAT</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;value&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">tensor</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"n\">dtype</span><span class=\"o\">:</span> <span class=\"n\">DT_FLOAT</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"n\">tensor_shape</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>          <span class=\"n\">dim</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>            <span class=\"n\">size</span><span class=\"o\">:</span> <span class=\"mi\">3</span><span class=\"err\">\n</span><span class=\"err\"></span>          <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"n\">tensor_content</span><span class=\"o\">:</span> <span class=\"s\">&#34;\\315\\314\\314=\\315\\314L&gt;\\232\\231\\231&gt;&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"n\">node</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">name</span><span class=\"o\">:</span> <span class=\"s\">&#34;fifo_queue_EnqueueMany&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">op</span><span class=\"o\">:</span> <span class=\"s\">&#34;QueueEnqueueManyV2&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">input</span><span class=\"o\">:</span> <span class=\"s\">&#34;fifo_queue&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">input</span><span class=\"o\">:</span> <span class=\"s\">&#34;fifo_queue_EnqueueMany/component_0&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;Tcomponents&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">list</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"n\">type</span><span class=\"o\">:</span> <span class=\"n\">DT_FLOAT</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;timeout_ms&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">i</span><span class=\"o\">:</span> <span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"n\">versions</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">producer</span><span class=\"o\">:</span> <span class=\"mi\">24</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"p\">}</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-text\">x = q.dequeue()</code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-31ac0befc5f984f3fbf97b4146277f48_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"812\" data-rawheight=\"274\" class=\"origin_image zh-lightbox-thumb\" width=\"812\" data-original=\"https://pic1.zhimg.com/v2-31ac0befc5f984f3fbf97b4146277f48_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;812&#39; height=&#39;274&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"812\" data-rawheight=\"274\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"812\" data-original=\"https://pic1.zhimg.com/v2-31ac0befc5f984f3fbf97b4146277f48_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-31ac0befc5f984f3fbf97b4146277f48_b.jpg\"/></figure><div class=\"highlight\"><pre><code class=\"language-text\">y = x + 1</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-15eff08e19599688466af2543578e896_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"987\" data-rawheight=\"384\" class=\"origin_image zh-lightbox-thumb\" width=\"987\" data-original=\"https://pic3.zhimg.com/v2-15eff08e19599688466af2543578e896_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;987&#39; height=&#39;384&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"987\" data-rawheight=\"384\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"987\" data-original=\"https://pic3.zhimg.com/v2-15eff08e19599688466af2543578e896_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-15eff08e19599688466af2543578e896_b.jpg\"/></figure><div class=\"highlight\"><pre><code class=\"language-text\">q_inc = q.enqueue([y])</code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-7470db0aa263d0da136d7608de80e1f7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"956\" data-rawheight=\"473\" class=\"origin_image zh-lightbox-thumb\" width=\"956\" data-original=\"https://pic4.zhimg.com/v2-7470db0aa263d0da136d7608de80e1f7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;956&#39; height=&#39;473&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"956\" data-rawheight=\"473\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"956\" data-original=\"https://pic4.zhimg.com/v2-7470db0aa263d0da136d7608de80e1f7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-7470db0aa263d0da136d7608de80e1f7_b.jpg\"/></figure><div class=\"highlight\"><pre><code class=\"language-text\"># 开启一个session,session是会话,会话的潜在含义是状态保持,各种tensor的状态保持\nwith tf.Session() as sess:\n    sess.run(init)\n    for i in range(2):\n        sess.run(q_inc)\n    quelen = sess.run(q.size())\n    for i in range(quelen):\n        print(sess.run(q.dequeue()))</code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-1be3cf7ea8eac4357737cdf2b534dba5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"871\" data-rawheight=\"457\" class=\"origin_image zh-lightbox-thumb\" width=\"871\" data-original=\"https://pic2.zhimg.com/v2-1be3cf7ea8eac4357737cdf2b534dba5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;871&#39; height=&#39;457&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"871\" data-rawheight=\"457\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"871\" data-original=\"https://pic2.zhimg.com/v2-1be3cf7ea8eac4357737cdf2b534dba5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-1be3cf7ea8eac4357737cdf2b534dba5_b.jpg\"/></figure><p>输出结果为：</p><div class=\"highlight\"><pre><code class=\"language-text\">0.3\n1.1\n1.2</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"small\"><noscript><img src=\"https://pic3.zhimg.com/v2-4ece3e1f44e64ee3f63a9e93788b5222_b.jpg\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"960\" data-rawheight=\"591\" class=\"origin_image zh-lightbox-thumb\" width=\"960\" data-original=\"https://pic3.zhimg.com/v2-4ece3e1f44e64ee3f63a9e93788b5222_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;960&#39; height=&#39;591&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"960\" data-rawheight=\"591\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"960\" data-original=\"https://pic3.zhimg.com/v2-4ece3e1f44e64ee3f63a9e93788b5222_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-4ece3e1f44e64ee3f63a9e93788b5222_b.jpg\"/></figure><figure data-size=\"small\"><noscript><img src=\"https://pic1.zhimg.com/v2-01cc0723aec616cb14b6c7387b346c9c_b.jpg\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"960\" data-rawheight=\"923\" class=\"origin_image zh-lightbox-thumb\" width=\"960\" data-original=\"https://pic1.zhimg.com/v2-01cc0723aec616cb14b6c7387b346c9c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;960&#39; height=&#39;923&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"960\" data-rawheight=\"923\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"960\" data-original=\"https://pic1.zhimg.com/v2-01cc0723aec616cb14b6c7387b346c9c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-01cc0723aec616cb14b6c7387b346c9c_b.jpg\"/></figure><p>【3】QueueRunner</p><p>关于QueueRunner, <a href=\"https://link.zhihu.com/?target=http%3A//blog.csdn.net/lujiandong1/article/details/53369961\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">tensorflow中关于队列使用的实验 - CSDN博客</a> 讲得特别好：</p><blockquote>入队操作是从硬盘中读取输入，放到内存当中，速度较慢。 <br/><b>使用QueueRunner可以  <i>创建一系列新的线程  </i>进行入队操作，让主线程继续使用数据。</b><br/>如果在训练神经网络的场景中，就是训练网络和读取数据是异步的，主线程在训练网络，另一个线程在将数据从硬盘读入内存。</blockquote><p>官方文档中关于queue的介绍和对应的代码：</p><p>A typical queue-based pipeline for reading records from files has the following <i>stages</i>:</p><p>（1）The <b>list</b> of filenames</p><div class=\"highlight\"><pre><code class=\"language-text\">self.image_list, self.label_list = read_labeled_image_list(self.data_dir, self.data_list)\nself.images = tf.convert_to_tensor(self.image_list, dtype=tf.string)\nself.labels = tf.convert_to_tensor(self.label_list, dtype=tf.string)</code></pre></div><p>（2）Optional filename shuffling</p><p>Optional epoch limit</p><p>Filename queue</p><div class=\"highlight\"><pre><code class=\"language-text\">self.queue = tf.train.slice_input_producer([self.images, self.labels],\n                                                   shuffle=input_size is not None)</code></pre></div><p>（3）A Reader for the file format</p><div class=\"highlight\"><pre><code class=\"language-text\">img_contents = tf.read_file(input_queue[0])\nlabel_contents = tf.read_file(input_queue[1])</code></pre></div><p>（4）A decoder for a record read by the reader</p><div class=\"highlight\"><pre><code class=\"language-text\">img = tf.image.decode_png(img_contents, channels=3)\nlabel = tf.image.decode_png(label_contents, channels=1)</code></pre></div><p>（5）Optional preprocessing</p><div class=\"highlight\"><pre><code class=\"language-text\">   img_r, img_g, img_b = tf.split(axis=2, num_or_size_splits=3, value=img)\n    img = tf.cast(tf.concat(axis=2, values=[img_b, img_g, img_r]), dtype=tf.float32)\n    # Extract mean.\n    img -= img_mean\n    if input_size is not None:\n        h, w = input_size\n\n        if random_scale:\n            img, label = image_scaling(img, label)\n\n        if random_mirror:\n            img, label = image_mirroring(img, label)\n            \n        img, label = random_crop_and_pad_image_and_labels(img, label, h, w, ignore_label)</code></pre></div><h2>Filenames, shuffling, and epoch limits</h2><p>For the list of filenames, use either a constant string Tensor (like <code>[&#34;file0&#34;, &#34;file1&#34;]</code> or <code>[(&#34;file%d&#34; % i) for i in range(2)]</code>) or the <code><a href=\"https://link.zhihu.com/?target=https%3A//www.tensorflow.org/versions/master/api_docs/python/tf/train/match_filenames_once\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">tf.train.match_filenames_once</a></code> function.</p><p>Pass the list of filenames to the <code><a href=\"https://link.zhihu.com/?target=https%3A//www.tensorflow.org/versions/master/api_docs/python/tf/train/string_input_producer\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">tf.train.string_input_producer</a></code> function. <code>string_input_producer</code><b> creates a FIFO queue</b> for holding the filenames until the reader needs them. 【tf.train.slice_input_producer()应该也是“creates a FIFO queue”的。】</p><div class=\"highlight\"><pre><code class=\"language-text\">self.queue = tf.train.slice_input_producer([self.images, self.labels],\n                                                   shuffle=input_size is not None)</code></pre></div><p><code>string_input_producer</code> has options for shuffling and setting a maximum number of epochs. 【tf.train.slice_input_producer()应该也是】</p><p class=\"ztext-empty-paragraph\"><br/></p><p>A queue runner adds the whole list of filenames to the queue once for each epoch,【类似于enqueue_many()，每个epoch就enqueue_many()全部的list】 shuffling the filenames within an epoch if <code>shuffle=True</code>. 【shuffle】This procedure provides a uniform sampling of files, so that examples are not under- or over- sampled relative to each other.【统一的采样】</p><p>The queue runner works in <b>a thread separate from</b> <i>the reader that pulls filenames from the queue</i>【应该就是负责计算的线程】, so the shuffling and enqueuing process does not block the reader.</p><h2>File formats</h2><p>Select the <b>reader</b> that matches your input file format and pass the filename queue to the reader&#39;s read method. The read method outputs a key identifying the file and record (useful for debugging if you have some weird records), and a scalar string value. Use one (or more) of the decoder and conversion ops to decode this string into the tensors that make up an example.【讲的是文件的reader】</p><h2>CSV files</h2><p>To read text files in <a href=\"https://link.zhihu.com/?target=https%3A//tools.ietf.org/html/rfc4180\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">comma-separated value (CSV) format</a>, use a <code><a href=\"https://link.zhihu.com/?target=https%3A//www.tensorflow.org/versions/master/api_docs/python/tf/TextLineReader\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">tf.TextLineReader</a></code> with the <code><a href=\"https://link.zhihu.com/?target=https%3A//www.tensorflow.org/versions/master/api_docs/python/tf/decode_csv\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">tf.decode_csv</a></code>operation. For example:</p><p class=\"ztext-empty-paragraph\"><br/></p><p><code>filename_queue = tf.train.string_input_producer([&#34;file0.csv&#34;, &#34;file1.csv&#34;]) <b>#在图上添加一个Queue</b><br/><br/>reader = tf.TextLineReader()<br/>key, value = reader.read(filename_queue) <b>#Reader</b><br/><br/># Default values, in case of empty columns. Also specifies the type of the<br/># decoded result.<br/>record_defaults = [[1], [1], [1], [1], [1]]<br/>col1, col2, col3, col4, col5 = tf.decode_csv(<br/>    value, record_defaults=record_defaults)<br/>features = tf.stack([col1, col2, col3, col4])<br/><br/>with tf.Session() as sess:<br/>  # Start populating the filename queue.<br/>  coord = tf.train.Coordinator()<br/>  threads = tf.train.start_queue_runners(coord=coord)<br/><br/>  for i in range(1200):<br/>    # Retrieve a single instance:<br/>    example, label = sess.run([features, col5])<br/><br/>  coord.request_stop()<br/>  coord.join(threads)<br/></code></p><p>Each execution of <code>read</code> reads a single line from the file. The <code><b>decode_csv</b></code> op then <b>parses the result into a list of tensors</b>. The <code><b>record_defaults</b></code> argument <b>determines the type of the resulting tensors</b> and <b>sets the default value</b> to use if a value is missing in the input string.</p><p>You must call <code><b>tf.train.start_queue_runners</b></code> to <i>populate the queue</i> 【填充队列】 before you <b>call <code>run</code> or <code>eval</code> to execute the <code>read</code></b>【这里的read是图上的一个op】. Otherwise <code>read</code> <b>will block</b> 【遇到的“卡住”问题有可能是这个原因引起的】 while it waits for filenames from the queue.</p><h2>Fixed length records</h2><p>To read binary files in which each record is a fixed number of bytes, use <code><a href=\"https://link.zhihu.com/?target=https%3A//www.tensorflow.org/versions/master/api_docs/python/tf/FixedLengthRecordReader\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">tf.FixedLengthRecordReader</a></code> with the <code><a href=\"https://link.zhihu.com/?target=https%3A//www.tensorflow.org/versions/master/api_docs/python/tf/decode_raw\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">tf.decode_raw</a></code> operation. The <code>decode_raw</code> op converts from a string to a uint8 tensor.</p><p>For example, <a href=\"https://link.zhihu.com/?target=http%3A//www.cs.toronto.edu/~kriz/cifar.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">the CIFAR-10 dataset</a> uses a file format where each record is represented using a fixed number of bytes: 1 byte for the label followed by 3072 bytes of image data. Once you have a uint8 tensor, standard operations can slice out each piece and reformat as needed. For CIFAR-10, you can see how to do the reading and decoding in<code><a href=\"https://link.zhihu.com/?target=https%3A//github.com/tensorflow/models/tree/master/tutorials/image/cifar10/cifar10_input.py\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">tensorflow_models/tutorials/image/cifar10/cifar10_input.py</a></code> and described in <a href=\"https://link.zhihu.com/?target=https%3A//www.tensorflow.org/versions/master/tutorials/deep_cnn%23prepare-the-data\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">this tutorial</a>.</p><p>【这个不用着急，一般数据集都会提供，或者能找得到。】</p><h2>Standard TensorFlow format</h2><p>Another approach is to convert whatever data you have into a supported format. This approach makes it easier to mix and match data sets and network architectures. The recommended format for TensorFlow is a <a href=\"https://link.zhihu.com/?target=https%3A//www.tensorflow.org/versions/master/api_guides/python/python_io%23tfrecords_format_details\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">TFRecords file</a> containing<code><a href=\"https://zhuanlan.zhihu.com/%3C/code%3E/www.github.com/tensorflow/tensorflow/blob/master/tensorflow/core/example/example.proto\" class=\"internal\">tf.train.Example protocol buffers</a> (which contain <a href=\"https://link.zhihu.com/?target=https%3A//www.github.com/tensorflow/tensorflow/blob/master/tensorflow/core/example/feature.proto\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Features</a> as a field). You write a little program that gets your data, stuffs it in an Example protocol buffer, serializes the protocol buffer to a string, and then writes the string to a TFRecords file using the <a href=\"https://link.zhihu.com/?target=https%3A//www.tensorflow.org/versions/master/api_docs/python/tf/python_io/TFRecordWriter\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">tf.python_io.TFRecordWriter</a>. For example,<a href=\"https://link.zhihu.com/?target=https%3A//www.github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/how_tos/reading_data/convert_to_records.py\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">tensorflow/examples/how_tos/reading_data/convert_to_records.py</a> converts MNIST data to this format.</code></p><p>To read a file of TFRecords, use <code><a href=\"https://link.zhihu.com/?target=https%3A//www.tensorflow.org/versions/master/api_docs/python/tf/TFRecordReader\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">tf.TFRecordReader</a></code> with the <code><a href=\"https://link.zhihu.com/?target=https%3A//www.tensorflow.org/versions/master/api_docs/python/tf/parse_single_example\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">tf.parse_single_example</a></code> decoder. The <code>parse_single_example</code> op decodes the example protocol buffers into tensors. An MNIST example using the data produced by <code>convert_to_records</code> can be found in<code><a href=\"https://link.zhihu.com/?target=https%3A//www.github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/how_tos/reading_data/fully_connected_reader.py\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">tensorflow/examples/how_tos/reading_data/fully_connected_reader.py</a></code>, which you can compare with the <code>fully_connected_feed</code> version.</p><p>【TFRecord，一种TensorFlow的标准文件结构】</p><h2>Preprocessing</h2><p>You can then do any preprocessing of these examples you want. This would be any processing that doesn&#39;t depend on trainable parameters. Examples include normalization of your data, picking a random slice, adding noise or distortions, etc. See <code><a href=\"https://link.zhihu.com/?target=https%3A//github.com/tensorflow/models/tree/master/tutorials/image/cifar10/cifar10_input.py\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">tensorflow_models/tutorials/image/cifar10/cifar10_input.py</a></code> for an example.</p><p>【（1）可以看看example；（2）以下代码就是preprocessing】</p><div class=\"highlight\"><pre><code class=\"language-text\">    img_r, img_g, img_b = tf.split(axis=2, num_or_size_splits=3, value=img)\n    img = tf.cast(tf.concat(axis=2, values=[img_b, img_g, img_r]), dtype=tf.float32)\n    # Extract mean.\n    img -= img_mean\n    # img = tf.\n\n    label = tf.image.decode_png(label_contents, channels=1)\n\n    if input_size is not None:\n        h, w = input_size\n\n        if random_scale:\n            img, label = image_scaling(img, label)\n\n        if random_mirror:\n            img, label = image_mirroring(img, label)\n            \n        img, label = random_crop_and_pad_image_and_labels(img, label, h, w, ignore_label)</code></pre></div><h2>Batching</h2><p>At the end of the pipeline we use <b>another queue</b> <i>to batch</i> together examples for training, evaluation, or inference. For this we use a queue that randomizes the order of examples, using the <code><a href=\"https://link.zhihu.com/?target=https%3A//www.tensorflow.org/versions/master/api_docs/python/tf/train/shuffle_batch\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">tf.train.shuffle_batch</a></code>.</p><p>Example:</p><p class=\"ztext-empty-paragraph\"><br/></p><p><code>def read_my_file_format(filename_queue):<br/>  reader = tf.SomeReader()<br/>  key, record_string = reader.read(filename_queue)<br/>  example, label = tf.some_decoder(record_string)<br/>  processed_example = some_processing(example)<br/>  return processed_example, label<br/><br/>def input_pipeline(filenames, batch_size, num_epochs=None):<br/>  filename_queue = tf.train.string_input_producer(<br/>      filenames, num_epochs=num_epochs, shuffle=True)<b>#第一个Queue</b><br/>  example, label = read_my_file_format(filename_queue)<br/>  # min_after_dequeue defines how big a buffer we will randomly sample<br/>  #   from -- bigger means better shuffling but slower start up and more<br/>  #   memory used.<br/>  # capacity must be larger than min_after_dequeue and the amount larger<br/>  #   determines the maximum we will prefetch.  Recommendation:<br/>  #   min_after_dequeue + (num_threads + a small safety margin) * batch_size<br/>  min_after_dequeue = 10000<br/>  capacity = min_after_dequeue + 3 * batch_size<br/>  example_batch, label_batch = tf.train.shuffle_batch(<br/>      [example, label], batch_size=batch_size, capacity=capacity,<br/>      min_after_dequeue=min_after_dequeue) <b>#第二个Queue</b><br/>  return example_batch, label_batch<br/></code></p><p>If you need more parallelism or shuffling of examples between files, use multiple reader instances using the<code><a href=\"https://link.zhihu.com/?target=https%3A//www.tensorflow.org/versions/master/api_docs/python/tf/train/shuffle_batch_join\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">tf.train.shuffle_batch_join</a></code>. For example:</p><p class=\"ztext-empty-paragraph\"><br/></p><p><code>def read_my_file_format(filename_queue):<br/>  # Same as above<br/><br/>def input_pipeline(filenames, batch_size, read_threads, num_epochs=None):<br/>  filename_queue = tf.train.string_input_producer(<br/>      filenames, num_epochs=num_epochs, shuffle=True)<br/>  example_list = [read_my_file_format(filename_queue)<br/>                  for _ in range(read_threads)]<br/>  min_after_dequeue = 10000<br/>  capacity = min_after_dequeue + 3 * batch_size<br/>  example_batch, label_batch = tf.train.shuffle_batch_join(<br/>      example_list, batch_size=batch_size, capacity=capacity,<br/>      min_after_dequeue=min_after_dequeue)<br/>  return example_batch, label_batch<br/></code></p><p>You <b>still only use a single filename queue</b> 【就是第一个Queue，即“filename_queue = tf.train.string_input_producer()”】 that is <i>shared</i> by all the readers. That way <b>we ensure</b> 【来自TensorFlow官方的保证...】that the different readers use different files from the same epoch until all the files from the epoch have been started. (It is also usually sufficient to have a single thread filling the filename queue.)</p><p>An alternative is to use a single reader via the <code><a href=\"https://link.zhihu.com/?target=https%3A//www.tensorflow.org/versions/master/api_docs/python/tf/train/shuffle_batch\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">tf.train.shuffle_batch</a></code> with <code>num_threads</code> bigger than 1. This will make it read from a single file at the same time (but faster than with 1 thread), instead of N files at once. This can be important:</p><ul><li>If you have more reading threads than input files, to avoid the risk that you will have two threads reading the same example from the same file near each other.</li><li>Or if reading N files in parallel causes too many disk seeks.</li></ul><p><b>How many threads</b> do you need? the <code>tf.train.shuffle_batch*</code> functions <b>add a summary to the graph</b> that indicates how full the example queue is. <b>If you have enough reading threads, that summary will stay above zero. </b>You can <a href=\"https://link.zhihu.com/?target=https%3A//www.tensorflow.org/versions/master/get_started/summaries_and_tensorboard\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">view your summaries as training progresses using TensorBoard</a>.【有一个summary可以显示threads设置的是否适当。】</p><h2>Creating threads to prefetch using <code>QueueRunner</code> objects</h2><p>The short version: many of the <code>tf.train</code> functions listed above <b>add <code><a href=\"https://link.zhihu.com/?target=https%3A//www.tensorflow.org/versions/master/api_docs/python/tf/train/QueueRunner\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">tf.train.QueueRunner</a></code> objects to your graph</b>. <b>These require that you call <code><a href=\"https://link.zhihu.com/?target=https%3A//www.tensorflow.org/versions/master/api_docs/python/tf/train/start_queue_runners\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">tf.train.start_queue_runners</a></code> before running any training or inference steps, or it will hang forever.</b> This will <b>start threads that run the input pipeline, filling the example queue so that the dequeue to get the examples will succeed</b>. This is best combined with a <b><code><a href=\"https://link.zhihu.com/?target=https%3A//www.tensorflow.org/versions/master/api_docs/python/tf/train/Coordinator\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">tf.train.Coordinator</a></code> to cleanly shut down these threads when there are errors</b>. If you set a limit on the number of epochs, that will use an epoch counter that will need to be initialized. The recommended code pattern combining these is:</p><p class=\"ztext-empty-paragraph\"><br/></p><p><code># Create the graph, etc.<br/>init_op = tf.global_variables_initializer()<br/><br/># Create a session for running operations in the Graph.<br/>sess = tf.Session()<br/><br/># Initialize the variables (like the epoch counter).<br/>sess.run(init_op)<br/><br/># <b>Start input enqueue threads.</b><br/>coord = tf.train.Coordinator()<br/>threads = tf.train.start_queue_runners(sess=sess, coord=coord)<br/><br/>try:<br/>    while not coord.should_stop():<br/>        # Run training steps or whatever<br/>        sess.run(train_op)<br/><br/>except tf.errors.OutOfRangeError:<br/>    print(&#39;Done training -- epoch limit reached&#39;)<br/>finally:<br/>    # When done, ask the threads to stop.<br/>    coord.request_stop()<br/><br/># Wait for threads to finish.<br/>coord.join(threads)<br/>sess.close()<br/></code></p><h2>Aside: What is happening here?</h2><p>First we create the graph. It will have a few pipeline stages that <b>are connected by queues</b>. The first stage will generate filenames to read and <b>enqueue them in the filename queue</b>. The second stage consumes filenames (using a <code>Reader</code>), produces examples, and <b>enqueues them in an example queue</b>. Depending on how you have set things up, you may actually have <i>a few independent copies of the second stage</i>, so that you can <i>read from multiple files in parallel</i>. 【创建两个Queue，一个是filename_queue，一个是example_queue】</p><p>At the end of these stages is an enqueue operation, which enqueues into a queue that the next stage dequeues from. We want to start threads running these enqueuing operations, so that our training loop can dequeue examples from the example queue.【创建完两个Queue，添加一个enqueue的op，用类似tf.train.start_queue_runners()开一个线程，进行enqueue。】</p><p><b>The helpers in <code>tf.train</code></b> that create these queues and enqueuing operations <b>add a <code><a href=\"https://link.zhihu.com/?target=https%3A//www.tensorflow.org/versions/master/api_docs/python/tf/train/QueueRunner\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">tf.train.QueueRunner</a></code> to the graph</b> using the <code><a href=\"https://link.zhihu.com/?target=https%3A//www.tensorflow.org/versions/master/api_docs/python/tf/train/add_queue_runner\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">tf.train.add_queue_runner</a></code> function. Each <code>QueueRunner</code> is responsible for one stage【或者说，一个QueueRunner负责一个Queue】, and holds the list of enqueue operations that need to be run in threads【保存了enqueue operations的列表】. Once the graph is constructed, the<b><code><a href=\"https://link.zhihu.com/?target=https%3A//www.tensorflow.org/versions/master/api_docs/python/tf/train/start_queue_runners\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">tf.train.start_queue_runners</a></code> function asks each QueueRunner in the graph to start its threads running the enqueuing operations.</b></p><p>If all goes well, you can now run your training steps and<i> the queues will be filled by the background threads</i>. If you have set an epoch limit, at some point an attempt to dequeue examples will get an <code><a href=\"https://link.zhihu.com/?target=https%3A//www.tensorflow.org/versions/master/api_docs/python/tf/errors/OutOfRangeError\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">tf.errors.OutOfRangeError</a></code>. This is the TensorFlow equivalent of &#34;end of file&#34; (EOF) -- this means the epoch limit has been reached and no more examples are available.</p><p>The last ingredient is the <code><b><a href=\"https://link.zhihu.com/?target=https%3A//www.tensorflow.org/versions/master/api_docs/python/tf/train/Coordinator\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">tf.train.Coordinator</a></b></code>. This is responsible for <b>letting all the threads know if anything has signaled a shut down</b>. 【管理线程的关闭】Most commonly this would be because an exception was raised, for example one of the threads got an error when running some operation (or an ordinary Python exception).</p><h2>Aside: How clean shut-down when limiting epochs works</h2><p>Imagine you have a model that has set a limit on the number of epochs to train on. <i>That means that</i> <b>the thread generating filenames</b> will <i>only run that many times</i> before generating an <code>OutOfRange</code> error. The QueueRunner will catch that error, <b>close the filename queue, and exit the thread</b>. Closing the queue does two things:</p><ul><li>Any future <b>attempt</b> to <b>enqueue in the filename queue</b> will <b>generate an error</b>. 【不能enqueue filename_queue】At this point there shouldn&#39;t be any threads trying to do that, but this is helpful when queues are closed due to other errors.</li><li>Any current or future <b>dequeue</b> will either <b>succeed</b> (if there are enough elements left) or fail (with an <code>OutOfRange</code><b>error</b>) immediately.【succeed or error】 They won&#39;t block waiting for more elements to be enqueued,【因为不会再有enqueue了】 since by the previous point that can&#39;t happen.</li></ul><p>The point is that when the filename queue is closed, there will likely still be many filenames in that queue,【filename_queue剩余的element】 so the next stage of the pipeline (with the reader and other preprocessing) may continue running for some time.【接着运行一段时间】 Once the filename queue is exhausted, though, the next attempt to dequeue a filename (e.g. from a reader that has finished with the file it was working on) will trigger an <code>OutOfRange</code> error. In this case, though, you might have multiple threads associated with a single QueueRunner. If this isn&#39;t <b>the last thread in the QueueRunner</b>, 【QueueRunner里面有一堆thread】 the <code>OutOfRange</code> error just causes the one thread to exit. 【只让一个thread结束】This allows the other threads, which are still finishing up their last file, to proceed until they finish as well.【其他的thread可以接着完成工作】 (Assuming you are using a <code><a href=\"https://link.zhihu.com/?target=https%3A//www.tensorflow.org/versions/master/api_docs/python/tf/train/Coordinator\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">tf.train.Coordinator</a></code>, <b>other types of errors will cause all the threads to stop</b>.) Once all the reader threads hit the <code>OutOfRange</code> error, only then does <b>the next queue, the example queue</b>, gets <b>closed</b>.</p><p>Again, the example queue will have some elements queued, so training will continue until those are exhausted. If the example queue is a <code><a href=\"https://link.zhihu.com/?target=https%3A//www.tensorflow.org/versions/master/api_docs/python/tf/RandomShuffleQueue\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">tf.RandomShuffleQueue</a></code>, say because you are using <code>shuffle_batch</code> or <code>shuffle_batch_join</code>, it normally will <b>avoid</b> ever having <b>fewer than</b> its <code><b>min_after_dequeue</b></code> attr elements buffered. However, once the queue is closed that restriction will be lifted and the queue will eventually empty. At that point the actual training threads, when they try and dequeue from example queue, will start getting <code>OutOfRange</code> errors and exiting. Once all the training threads are done, <code><a href=\"https://link.zhihu.com/?target=https%3A//www.tensorflow.org/versions/master/api_docs/python/tf/train/Coordinator%23join\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">tf.train.Coordinator.join</a></code> will return and you can exit cleanly.</p><h2>Filtering records or producing multiple examples per record</h2><p>Instead of examples with shapes <code>[x, y, z]</code>, you will produce a batch of examples with shape <code>[batch, x, y, z]</code>. <i>The batch size can be 0</i> if you want to <i>filter this record out</i> (maybe it is in a hold-out set?), or bigger than 1 if you are <i>producing multiple examples per record</i>. 【batch SGD】Then simply set <code>enqueue_many=True</code> when calling one of the <b>batching functions</b> (such as <code>shuffle_batch</code> or <code>shuffle_batch_join</code>).</p><h2>Sparse input data</h2><p><b>SparseTensors don&#39;t play well with queues.</b> If you use SparseTensors you have to decode the string records using<code><a href=\"https://link.zhihu.com/?target=https%3A//www.tensorflow.org/versions/master/api_docs/python/tf/parse_example\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">tf.parse_example</a></code> <b>after</b> batching (instead of using <code>tf.parse_single_example</code> before batching). <b>SparseTensors在batching之后decode.</b></p><p class=\"ztext-empty-paragraph\"><br/></p><p>参考资料：</p><p><a href=\"https://link.zhihu.com/?target=http%3A//blog.csdn.net/u012436149/article/details/72353313\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">tensorflow学习笔记（四十二）：输入流水线 - CSDN博客</a></p><p><a href=\"https://link.zhihu.com/?target=http%3A//blog.csdn.net/lujiandong1/article/details/53369961\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">tensorflow中关于队列使用的实验 - CSDN博客</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//www.tensorflow.org/api_docs/python/tf/train/QueueRunner\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">tensorflow.org/api_docs</span><span class=\"invisible\">/python/tf/train/QueueRunner</span><span class=\"ellipsis\"></span></a></p><p></p><p></p><p></p>", 
            "topic": [
                {
                    "tag": "TensorFlow", 
                    "tagLink": "https://api.zhihu.com/topics/20032249"
                }
            ], 
            "comments": [
                {
                    "userName": "蛋疼揉揉继续疼", 
                    "userLink": "https://www.zhihu.com/people/51e16b28c6cc586bff9596498c1813ff", 
                    "content": "<p>感想分享</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/31310164", 
            "userName": "休语", 
            "userLink": "https://www.zhihu.com/people/9fa98b02c1f908f202862a38bd15084f", 
            "upvote": 7, 
            "title": "理解tf.train.slice_input_producer()和tf.train.batch()", 
            "content": "<p>TensorFlow确实不那么好上手，感觉应该一边学一边写点什么。这次还是研究TensorFlow读取数据的问题，主要是tf.train.slice_input_producer()和tf.train.batch()两个函数。</p><p>【1】tf.train.slice_input_producer()</p><div class=\"highlight\"><pre><code class=\"language-python\">     <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">queue</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"o\">.</span><span class=\"n\">slice_input_producer</span><span class=\"p\">([</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">images</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">labels</span><span class=\"p\">],</span>\n                                                   <span class=\"n\">shuffle</span><span class=\"o\">=</span><span class=\"n\">input_size</span> <span class=\"ow\">is</span> <span class=\"ow\">not</span> <span class=\"bp\">None</span><span class=\"p\">)</span></code></pre></div><p>先读文档：</p><p>输入：<b><code>tensor_list</code></b>: A list of <code>Tensor</code> objects. Every <code>Tensor</code> in <code>tensor_list</code> must have the same size in the first dimension.</p><p>这是因为第一个维度是样本数/标签数：</p><figure data-size=\"small\"><noscript><img src=\"https://pic1.zhimg.com/v2-004f27b9d2206cd8d5a8e4898abcc770_b.jpg\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"847\" data-rawheight=\"960\" class=\"origin_image zh-lightbox-thumb\" width=\"847\" data-original=\"https://pic1.zhimg.com/v2-004f27b9d2206cd8d5a8e4898abcc770_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;847&#39; height=&#39;960&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"847\" data-rawheight=\"960\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"847\" data-original=\"https://pic1.zhimg.com/v2-004f27b9d2206cd8d5a8e4898abcc770_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-004f27b9d2206cd8d5a8e4898abcc770_b.jpg\"/></figure><p>返回值：A list of tensors, one for each element of <code>tensor_list</code>. If the tensor in <code>tensor_list</code> has shape <code>[N, a, b, .., z]</code>, then the corresponding output tensor will have shape <code>[a, b, ..., z]</code>.</p><p>返回的结果如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-732f17dfef566a1847436d4badb50e14_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1534\" data-rawheight=\"142\" class=\"origin_image zh-lightbox-thumb\" width=\"1534\" data-original=\"https://pic1.zhimg.com/v2-732f17dfef566a1847436d4badb50e14_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1534&#39; height=&#39;142&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1534\" data-rawheight=\"142\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1534\" data-original=\"https://pic1.zhimg.com/v2-732f17dfef566a1847436d4badb50e14_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-732f17dfef566a1847436d4badb50e14_b.jpg\"/></figure><p>是一个list。</p><p>先看一下运行tf.train.slice_input_producer()之前图的状态，：</p><div class=\"highlight\"><pre><code class=\"language-protobuf\"><span class=\"n\">node</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">name</span><span class=\"o\">:</span> <span class=\"s\">&#34;create_inputs/Const&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">op</span><span class=\"o\">:</span> <span class=\"s\">&#34;Const&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;dtype&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">type</span><span class=\"o\">:</span> <span class=\"n\">DT_STRING</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;value&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">tensor</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"n\">dtype</span><span class=\"o\">:</span> <span class=\"n\">DT_STRING</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"n\">tensor_shape</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>          <span class=\"n\">dim</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>            <span class=\"n\">size</span><span class=\"o\">:</span> <span class=\"mi\">5598</span><span class=\"err\">\n</span><span class=\"err\"></span>          <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"n\">string_val</span><span class=\"o\">:</span> <span class=\"s\">&#34;00000-00000.png&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"n\">string_val</span><span class=\"o\">:</span> <span class=\"s\">&#34;00000-00001.png&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"n\">string_val</span><span class=\"o\">:</span> <span class=\"s\">&#34;00049-00089.jpg&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"n\">node</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">name</span><span class=\"o\">:</span> <span class=\"s\">&#34;create_inputs/Const_1&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">op</span><span class=\"o\">:</span> <span class=\"s\">&#34;Const&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;dtype&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">type</span><span class=\"o\">:</span> <span class=\"n\">DT_STRING</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;value&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">tensor</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"n\">dtype</span><span class=\"o\">:</span> <span class=\"n\">DT_STRING</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"n\">tensor_shape</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>          <span class=\"n\">dim</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>            <span class=\"n\">size</span><span class=\"o\">:</span> <span class=\"mi\">5598</span><span class=\"err\">\n</span><span class=\"err\"></span>          <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"n\">string_val</span><span class=\"o\">:</span> <span class=\"s\">&#34;00000-00000.png&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"n\">string_val</span><span class=\"o\">:</span> <span class=\"s\">&#34;00000-00001.png&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"n\">string_val</span><span class=\"o\">:</span> <span class=\"s\">&#34;00049-00089.jpg&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"n\">versions</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">producer</span><span class=\"o\">:</span> <span class=\"mi\">24</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"p\">}</span><span class=\"err\">\n</span></code></pre></div><p>看一下运行tf.train.slice_input_producer()之后图的状态：</p><div class=\"highlight\"><pre><code class=\"language-protobuf\"><span class=\"n\">node</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">name</span><span class=\"o\">:</span> <span class=\"s\">&#34;create_inputs/Const&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">op</span><span class=\"o\">:</span> <span class=\"s\">&#34;Const&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;dtype&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">type</span><span class=\"o\">:</span> <span class=\"n\">DT_STRING</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;value&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">tensor</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"n\">dtype</span><span class=\"o\">:</span> <span class=\"n\">DT_STRING</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"n\">tensor_shape</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>          <span class=\"n\">dim</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>            <span class=\"n\">size</span><span class=\"o\">:</span> <span class=\"mi\">5598</span><span class=\"err\">\n</span><span class=\"err\"></span>          <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"n\">string_val</span><span class=\"o\">:</span> <span class=\"s\">&#34;00000-00000.png&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"n\">string_val</span><span class=\"o\">:</span> <span class=\"s\">&#34;00000-00001.png&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"n\">string_val</span><span class=\"o\">:</span> <span class=\"s\">&#34;00000-00002.png&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"n\">node</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">name</span><span class=\"o\">:</span> <span class=\"s\">&#34;create_inputs/Const_1&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">op</span><span class=\"o\">:</span> <span class=\"s\">&#34;Const&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;dtype&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">type</span><span class=\"o\">:</span> <span class=\"n\">DT_STRING</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;value&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">tensor</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"n\">dtype</span><span class=\"o\">:</span> <span class=\"n\">DT_STRING</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"n\">tensor_shape</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>          <span class=\"n\">dim</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>            <span class=\"n\">size</span><span class=\"o\">:</span> <span class=\"mi\">5598</span><span class=\"err\">\n</span><span class=\"err\"></span>          <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"n\">string_val</span><span class=\"o\">:</span> <span class=\"s\">&#34;00000-00000.png&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"n\">string_val</span><span class=\"o\">:</span> <span class=\"s\">&#34;00000-00001.png&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"n\">string_val</span><span class=\"o\">:</span> <span class=\"s\">&#34;00000-00002.jpg&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"n\">node</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">name</span><span class=\"o\">:</span> <span class=\"s\">&#34;create_inputs/input_producer/Shape&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">op</span><span class=\"o\">:</span> <span class=\"s\">&#34;Const&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;dtype&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">type</span><span class=\"o\">:</span> <span class=\"n\">DT_INT32</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;value&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">tensor</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"n\">dtype</span><span class=\"o\">:</span> <span class=\"n\">DT_INT32</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"n\">tensor_shape</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>          <span class=\"n\">dim</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>            <span class=\"n\">size</span><span class=\"o\">:</span> <span class=\"mi\">1</span><span class=\"err\">\n</span><span class=\"err\"></span>          <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"n\">int_val</span><span class=\"o\">:</span> <span class=\"mi\">5598</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"n\">node</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">name</span><span class=\"o\">:</span> <span class=\"s\">&#34;create_inputs/input_producer/strided_slice/stack&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">op</span><span class=\"o\">:</span> <span class=\"s\">&#34;Const&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;dtype&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">type</span><span class=\"o\">:</span> <span class=\"n\">DT_INT32</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;value&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">tensor</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"n\">dtype</span><span class=\"o\">:</span> <span class=\"n\">DT_INT32</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"n\">tensor_shape</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>          <span class=\"n\">dim</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>            <span class=\"n\">size</span><span class=\"o\">:</span> <span class=\"mi\">1</span><span class=\"err\">\n</span><span class=\"err\"></span>          <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"n\">int_val</span><span class=\"o\">:</span> <span class=\"mi\">0</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"n\">node</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">name</span><span class=\"o\">:</span> <span class=\"s\">&#34;create_inputs/input_producer/strided_slice/stack_1&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">op</span><span class=\"o\">:</span> <span class=\"s\">&#34;Const&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;dtype&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">type</span><span class=\"o\">:</span> <span class=\"n\">DT_INT32</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;value&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">tensor</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"n\">dtype</span><span class=\"o\">:</span> <span class=\"n\">DT_INT32</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"n\">tensor_shape</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>          <span class=\"n\">dim</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>            <span class=\"n\">size</span><span class=\"o\">:</span> <span class=\"mi\">1</span><span class=\"err\">\n</span><span class=\"err\"></span>          <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"n\">int_val</span><span class=\"o\">:</span> <span class=\"mi\">1</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"n\">node</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">name</span><span class=\"o\">:</span> <span class=\"s\">&#34;create_inputs/input_producer/strided_slice/stack_2&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">op</span><span class=\"o\">:</span> <span class=\"s\">&#34;Const&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;dtype&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">type</span><span class=\"o\">:</span> <span class=\"n\">DT_INT32</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;value&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">tensor</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"n\">dtype</span><span class=\"o\">:</span> <span class=\"n\">DT_INT32</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"n\">tensor_shape</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>          <span class=\"n\">dim</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>            <span class=\"n\">size</span><span class=\"o\">:</span> <span class=\"mi\">1</span><span class=\"err\">\n</span><span class=\"err\"></span>          <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"n\">int_val</span><span class=\"o\">:</span> <span class=\"mi\">1</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"n\">node</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">name</span><span class=\"o\">:</span> <span class=\"s\">&#34;create_inputs/input_producer/strided_slice&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">op</span><span class=\"o\">:</span> <span class=\"s\">&#34;StridedSlice&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">input</span><span class=\"o\">:</span> <span class=\"s\">&#34;create_inputs/input_producer/Shape&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">input</span><span class=\"o\">:</span> <span class=\"s\">&#34;create_inputs/input_producer/strided_slice/stack&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">input</span><span class=\"o\">:</span> <span class=\"s\">&#34;create_inputs/input_producer/strided_slice/stack_1&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">input</span><span class=\"o\">:</span> <span class=\"s\">&#34;create_inputs/input_producer/strided_slice/stack_2&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;Index&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">type</span><span class=\"o\">:</span> <span class=\"n\">DT_INT32</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;T&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">type</span><span class=\"o\">:</span> <span class=\"n\">DT_INT32</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;begin_mask&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">i</span><span class=\"o\">:</span> <span class=\"mi\">0</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;ellipsis_mask&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">i</span><span class=\"o\">:</span> <span class=\"mi\">0</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;end_mask&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">i</span><span class=\"o\">:</span> <span class=\"mi\">0</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;new_axis_mask&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">i</span><span class=\"o\">:</span> <span class=\"mi\">0</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;shrink_axis_mask&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">i</span><span class=\"o\">:</span> <span class=\"mi\">1</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"n\">node</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">name</span><span class=\"o\">:</span> <span class=\"s\">&#34;create_inputs/input_producer/input_producer/range/start&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">op</span><span class=\"o\">:</span> <span class=\"s\">&#34;Const&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;dtype&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">type</span><span class=\"o\">:</span> <span class=\"n\">DT_INT32</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;value&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">tensor</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"n\">dtype</span><span class=\"o\">:</span> <span class=\"n\">DT_INT32</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"n\">tensor_shape</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"n\">int_val</span><span class=\"o\">:</span> <span class=\"mi\">0</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"n\">node</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">name</span><span class=\"o\">:</span> <span class=\"s\">&#34;create_inputs/input_producer/input_producer/range/delta&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">op</span><span class=\"o\">:</span> <span class=\"s\">&#34;Const&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;dtype&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">type</span><span class=\"o\">:</span> <span class=\"n\">DT_INT32</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;value&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">tensor</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"n\">dtype</span><span class=\"o\">:</span> <span class=\"n\">DT_INT32</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"n\">tensor_shape</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"n\">int_val</span><span class=\"o\">:</span> <span class=\"mi\">1</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"n\">node</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">name</span><span class=\"o\">:</span> <span class=\"s\">&#34;create_inputs/input_producer/input_producer/range&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">op</span><span class=\"o\">:</span> <span class=\"s\">&#34;Range&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">input</span><span class=\"o\">:</span> <span class=\"s\">&#34;create_inputs/input_producer/input_producer/range/start&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">input</span><span class=\"o\">:</span> <span class=\"s\">&#34;create_inputs/input_producer/strided_slice&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">input</span><span class=\"o\">:</span> <span class=\"s\">&#34;create_inputs/input_producer/input_producer/range/delta&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;Tidx&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">type</span><span class=\"o\">:</span> <span class=\"n\">DT_INT32</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"n\">node</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">name</span><span class=\"o\">:</span> <span class=\"s\">&#34;create_inputs/input_producer/input_producer/RandomShuffle&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">op</span><span class=\"o\">:</span> <span class=\"s\">&#34;RandomShuffle&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">input</span><span class=\"o\">:</span> <span class=\"s\">&#34;create_inputs/input_producer/input_producer/range&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;T&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">type</span><span class=\"o\">:</span> <span class=\"n\">DT_INT32</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;seed&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">i</span><span class=\"o\">:</span> <span class=\"mi\">1234</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;seed2&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">i</span><span class=\"o\">:</span> <span class=\"mi\">10</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"n\">node</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">name</span><span class=\"o\">:</span> <span class=\"s\">&#34;create_inputs/input_producer/input_producer&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">op</span><span class=\"o\">:</span> <span class=\"s\">&#34;FIFOQueueV2&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;capacity&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">i</span><span class=\"o\">:</span> <span class=\"mi\">32</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;component_types&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">list</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"n\">type</span><span class=\"o\">:</span> <span class=\"n\">DT_INT32</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;container&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">s</span><span class=\"o\">:</span> <span class=\"s\">&#34;&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;shapes&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">list</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"n\">shape</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;shared_name&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">s</span><span class=\"o\">:</span> <span class=\"s\">&#34;&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"n\">node</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">name</span><span class=\"o\">:</span> <span class=\"s\">&#34;create_inputs/input_producer/input_producer/input_producer_EnqueueMany&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">op</span><span class=\"o\">:</span> <span class=\"s\">&#34;QueueEnqueueManyV2&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">input</span><span class=\"o\">:</span> <span class=\"s\">&#34;create_inputs/input_producer/input_producer&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">input</span><span class=\"o\">:</span> <span class=\"s\">&#34;create_inputs/input_producer/input_producer/RandomShuffle&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;Tcomponents&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">list</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"n\">type</span><span class=\"o\">:</span> <span class=\"n\">DT_INT32</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;timeout_ms&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">i</span><span class=\"o\">:</span> <span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"n\">node</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">name</span><span class=\"o\">:</span> <span class=\"s\">&#34;create_inputs/input_producer/input_producer/input_producer_Close&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">op</span><span class=\"o\">:</span> <span class=\"s\">&#34;QueueCloseV2&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">input</span><span class=\"o\">:</span> <span class=\"s\">&#34;create_inputs/input_producer/input_producer&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;cancel_pending_enqueues&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">b</span><span class=\"o\">:</span> <span class=\"kc\">false</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"n\">node</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">name</span><span class=\"o\">:</span> <span class=\"s\">&#34;create_inputs/input_producer/input_producer/input_producer_Close_1&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">op</span><span class=\"o\">:</span> <span class=\"s\">&#34;QueueCloseV2&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">input</span><span class=\"o\">:</span> <span class=\"s\">&#34;create_inputs/input_producer/input_producer&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;cancel_pending_enqueues&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">b</span><span class=\"o\">:</span> <span class=\"kc\">true</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"n\">node</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">name</span><span class=\"o\">:</span> <span class=\"s\">&#34;create_inputs/input_producer/input_producer/input_producer_Size&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">op</span><span class=\"o\">:</span> <span class=\"s\">&#34;QueueSizeV2&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">input</span><span class=\"o\">:</span> <span class=\"s\">&#34;create_inputs/input_producer/input_producer&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"n\">node</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">name</span><span class=\"o\">:</span> <span class=\"s\">&#34;create_inputs/input_producer/input_producer/ToFloat&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">op</span><span class=\"o\">:</span> <span class=\"s\">&#34;Cast&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">input</span><span class=\"o\">:</span> <span class=\"s\">&#34;create_inputs/input_producer/input_producer/input_producer_Size&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;DstT&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">type</span><span class=\"o\">:</span> <span class=\"n\">DT_FLOAT</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;SrcT&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">type</span><span class=\"o\">:</span> <span class=\"n\">DT_INT32</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"n\">node</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">name</span><span class=\"o\">:</span> <span class=\"s\">&#34;create_inputs/input_producer/input_producer/mul/y&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">op</span><span class=\"o\">:</span> <span class=\"s\">&#34;Const&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;dtype&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">type</span><span class=\"o\">:</span> <span class=\"n\">DT_FLOAT</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;value&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">tensor</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"n\">dtype</span><span class=\"o\">:</span> <span class=\"n\">DT_FLOAT</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"n\">tensor_shape</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"n\">float_val</span><span class=\"o\">:</span> <span class=\"mf\">0.03125</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"n\">node</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">name</span><span class=\"o\">:</span> <span class=\"s\">&#34;create_inputs/input_producer/input_producer/mul&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">op</span><span class=\"o\">:</span> <span class=\"s\">&#34;Mul&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">input</span><span class=\"o\">:</span> <span class=\"s\">&#34;create_inputs/input_producer/input_producer/ToFloat&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">input</span><span class=\"o\">:</span> <span class=\"s\">&#34;create_inputs/input_producer/input_producer/mul/y&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;T&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">type</span><span class=\"o\">:</span> <span class=\"n\">DT_FLOAT</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"n\">node</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">name</span><span class=\"o\">:</span> <span class=\"s\">&#34;create_inputs/input_producer/input_producer/fraction_of_32_full/tags&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">op</span><span class=\"o\">:</span> <span class=\"s\">&#34;Const&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;dtype&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">type</span><span class=\"o\">:</span> <span class=\"n\">DT_STRING</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;value&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">tensor</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"n\">dtype</span><span class=\"o\">:</span> <span class=\"n\">DT_STRING</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"n\">tensor_shape</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"n\">string_val</span><span class=\"o\">:</span> <span class=\"s\">&#34;create_inputs/input_producer/input_producer/fraction_of_32_full&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"n\">node</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">name</span><span class=\"o\">:</span> <span class=\"s\">&#34;create_inputs/input_producer/input_producer/fraction_of_32_full&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">op</span><span class=\"o\">:</span> <span class=\"s\">&#34;ScalarSummary&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">input</span><span class=\"o\">:</span> <span class=\"s\">&#34;create_inputs/input_producer/input_producer/fraction_of_32_full/tags&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">input</span><span class=\"o\">:</span> <span class=\"s\">&#34;create_inputs/input_producer/input_producer/mul&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;T&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">type</span><span class=\"o\">:</span> <span class=\"n\">DT_FLOAT</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"n\">node</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">name</span><span class=\"o\">:</span> <span class=\"s\">&#34;create_inputs/input_producer/input_producer_Dequeue&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">op</span><span class=\"o\">:</span> <span class=\"s\">&#34;QueueDequeueV2&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">input</span><span class=\"o\">:</span> <span class=\"s\">&#34;create_inputs/input_producer/input_producer&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;component_types&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">list</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>        <span class=\"n\">type</span><span class=\"o\">:</span> <span class=\"n\">DT_INT32</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;timeout_ms&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">i</span><span class=\"o\">:</span> <span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"n\">node</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">name</span><span class=\"o\">:</span> <span class=\"s\">&#34;create_inputs/input_producer/Gather&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">op</span><span class=\"o\">:</span> <span class=\"s\">&#34;Gather&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">input</span><span class=\"o\">:</span> <span class=\"s\">&#34;create_inputs/Const&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">input</span><span class=\"o\">:</span> <span class=\"s\">&#34;create_inputs/input_producer/input_producer_Dequeue&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;Tindices&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">type</span><span class=\"o\">:</span> <span class=\"n\">DT_INT32</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;Tparams&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">type</span><span class=\"o\">:</span> <span class=\"n\">DT_STRING</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;validate_indices&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">b</span><span class=\"o\">:</span> <span class=\"kc\">true</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"n\">node</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">name</span><span class=\"o\">:</span> <span class=\"s\">&#34;create_inputs/input_producer/Gather_1&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">op</span><span class=\"o\">:</span> <span class=\"s\">&#34;Gather&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">input</span><span class=\"o\">:</span> <span class=\"s\">&#34;create_inputs/Const_1&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">input</span><span class=\"o\">:</span> <span class=\"s\">&#34;create_inputs/input_producer/input_producer_Dequeue&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;Tindices&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">type</span><span class=\"o\">:</span> <span class=\"n\">DT_INT32</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;Tparams&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">type</span><span class=\"o\">:</span> <span class=\"n\">DT_STRING</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">attr</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">key</span><span class=\"o\">:</span> <span class=\"s\">&#34;validate_indices&#34;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"n\">value</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>      <span class=\"n\">b</span><span class=\"o\">:</span> <span class=\"kc\">true</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"p\">}</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"n\">versions</span> <span class=\"p\">{</span><span class=\"err\">\n</span><span class=\"err\"></span>  <span class=\"n\">producer</span><span class=\"o\">:</span> <span class=\"mi\">24</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"p\">}</span><span class=\"err\">\n</span></code></pre></div><p>用tensorboard查看一下现在Graph的状态：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-fca2189b8462c47bf9b2634c1c45b599_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1263\" data-rawheight=\"588\" class=\"origin_image zh-lightbox-thumb\" width=\"1263\" data-original=\"https://pic2.zhimg.com/v2-fca2189b8462c47bf9b2634c1c45b599_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1263&#39; height=&#39;588&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1263\" data-rawheight=\"588\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1263\" data-original=\"https://pic2.zhimg.com/v2-fca2189b8462c47bf9b2634c1c45b599_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-fca2189b8462c47bf9b2634c1c45b599_b.jpg\"/></figure><p>其中input_producer内部：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ba1d8baeb2b36b784abeaf1bf89d6047_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1004\" data-rawheight=\"821\" class=\"origin_image zh-lightbox-thumb\" width=\"1004\" data-original=\"https://pic4.zhimg.com/v2-ba1d8baeb2b36b784abeaf1bf89d6047_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1004&#39; height=&#39;821&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1004\" data-rawheight=\"821\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1004\" data-original=\"https://pic4.zhimg.com/v2-ba1d8baeb2b36b784abeaf1bf89d6047_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-ba1d8baeb2b36b784abeaf1bf89d6047_b.jpg\"/></figure><p>当然我们最关心的是两个输入的Const是怎么与tf.train.slice_input_producer()生成的一堆op连接的：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-1d2cf86ffadd08d351e5b5bed2369c1d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1038\" data-rawheight=\"737\" class=\"origin_image zh-lightbox-thumb\" width=\"1038\" data-original=\"https://pic2.zhimg.com/v2-1d2cf86ffadd08d351e5b5bed2369c1d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1038&#39; height=&#39;737&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1038\" data-rawheight=\"737\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1038\" data-original=\"https://pic2.zhimg.com/v2-1d2cf86ffadd08d351e5b5bed2369c1d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-1d2cf86ffadd08d351e5b5bed2369c1d_b.jpg\"/></figure><p>这个Gather和Gather_1应该就是tf.train.slice_input_producer()返回的[Tensor,Tensor]:</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-9a40c4422ac54fd6ae1e27ef5ca5729c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"327\" data-rawheight=\"397\" class=\"content_image\" width=\"327\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;327&#39; height=&#39;397&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"327\" data-rawheight=\"397\" class=\"content_image lazy\" width=\"327\" data-actualsrc=\"https://pic1.zhimg.com/v2-9a40c4422ac54fd6ae1e27ef5ca5729c_b.jpg\"/></figure><p>接下来看，用tf.train.slice_input_producer()画了两个Gather的Op，生成[Tensor,Tensor]之后，是怎么处理的：</p><div class=\"highlight\"><pre><code class=\"language-python3\">      <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">image_list</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">label_list</span> <span class=\"o\">=</span> <span class=\"n\">read_labeled_image_list</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">data_dir</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">data_list</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">images</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">convert_to_tensor</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">image_list</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">string</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">labels</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">convert_to_tensor</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">label_list</span><span class=\"p\">,</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">string</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">queue</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"o\">.</span><span class=\"n\">slice_input_producer</span><span class=\"p\">([</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">images</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">labels</span><span class=\"p\">],</span>\n                                                   <span class=\"n\">shuffle</span><span class=\"o\">=</span><span class=\"n\">input_size</span> <span class=\"ow\">is</span> <span class=\"ow\">not</span> <span class=\"kc\">None</span><span class=\"p\">)</span> <span class=\"c1\"># not shuffling if it is val</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">image</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"n\">read_images_from_disk</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">queue</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">input_size</span><span class=\"p\">,</span> <span class=\"n\">random_scale</span><span class=\"p\">,</span> <span class=\"n\">random_mirror</span><span class=\"p\">,</span> <span class=\"n\">ignore_label</span><span class=\"p\">,</span> <span class=\"n\">img_mean</span><span class=\"p\">)</span></code></pre></div><p>我学习的这个代码自己实现了一个read_images_from_disk()函数：</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"k\">def</span> <span class=\"nf\">read_images_from_disk</span><span class=\"p\">(</span><span class=\"n\">input_queue</span><span class=\"p\">,</span> <span class=\"n\">input_size</span><span class=\"p\">,</span> <span class=\"n\">random_scale</span><span class=\"p\">,</span> <span class=\"n\">random_mirror</span><span class=\"p\">,</span> <span class=\"n\">ignore_label</span><span class=\"p\">,</span> <span class=\"n\">img_mean</span><span class=\"p\">):</span> <span class=\"c1\"># optional pre-processing arguments</span>\n    <span class=\"n\">img_contents</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">read_file</span><span class=\"p\">(</span><span class=\"n\">input_queue</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">])</span>\n    <span class=\"n\">label_contents</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">read_file</span><span class=\"p\">(</span><span class=\"n\">input_queue</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">])</span>\n    <span class=\"n\">img</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">image</span><span class=\"o\">.</span><span class=\"n\">decode_png</span><span class=\"p\">(</span><span class=\"n\">img_contents</span><span class=\"p\">,</span> <span class=\"n\">channels</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">)</span>\n    <span class=\"n\">img_r</span><span class=\"p\">,</span> <span class=\"n\">img_g</span><span class=\"p\">,</span> <span class=\"n\">img_b</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">(</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">num_or_size_splits</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"n\">value</span><span class=\"o\">=</span><span class=\"n\">img</span><span class=\"p\">)</span>\n    <span class=\"n\">img</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">cast</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">concat</span><span class=\"p\">(</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">values</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">img_b</span><span class=\"p\">,</span> <span class=\"n\">img_g</span><span class=\"p\">,</span> <span class=\"n\">img_r</span><span class=\"p\">]),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n    <span class=\"c1\"># Extract mean.</span>\n    <span class=\"n\">img</span> <span class=\"o\">-=</span> <span class=\"n\">img_mean</span>\n    <span class=\"c1\"># img = tf.</span>\n\n    <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">image</span><span class=\"o\">.</span><span class=\"n\">decode_png</span><span class=\"p\">(</span><span class=\"n\">label_contents</span><span class=\"p\">,</span> <span class=\"n\">channels</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n\n    <span class=\"k\">if</span> <span class=\"n\">input_size</span> <span class=\"ow\">is</span> <span class=\"ow\">not</span> <span class=\"kc\">None</span><span class=\"p\">:</span>\n        <span class=\"n\">h</span><span class=\"p\">,</span> <span class=\"n\">w</span> <span class=\"o\">=</span> <span class=\"n\">input_size</span>\n\n        <span class=\"k\">if</span> <span class=\"n\">random_scale</span><span class=\"p\">:</span>\n            <span class=\"n\">img</span><span class=\"p\">,</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"n\">image_scaling</span><span class=\"p\">(</span><span class=\"n\">img</span><span class=\"p\">,</span> <span class=\"n\">label</span><span class=\"p\">)</span>\n\n        <span class=\"k\">if</span> <span class=\"n\">random_mirror</span><span class=\"p\">:</span>\n            <span class=\"n\">img</span><span class=\"p\">,</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"n\">image_mirroring</span><span class=\"p\">(</span><span class=\"n\">img</span><span class=\"p\">,</span> <span class=\"n\">label</span><span class=\"p\">)</span>\n            \n        <span class=\"n\">img</span><span class=\"p\">,</span> <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"n\">random_crop_and_pad_image_and_labels</span><span class=\"p\">(</span><span class=\"n\">img</span><span class=\"p\">,</span> <span class=\"n\">label</span><span class=\"p\">,</span> <span class=\"n\">h</span><span class=\"p\">,</span> <span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">ignore_label</span><span class=\"p\">)</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">img</span><span class=\"p\">,</span> <span class=\"n\">label</span></code></pre></div><p>注意到tf.read_file()的输入是一个类型为string的<b>Tensor</b>。input_queue[0]，input_queue[1]都是包含一个string的Tensor。</p><p><code>read_file(<br/>    filename,<br/>    name=None</code> <code>)</code> </p><p>Args:</p><ul><li><b><code>filename</code></b>: A <b><code>Tensor</code></b> of type <code>string</code>.</li><li><b><code>name</code></b>: A name for the operation (optional).</li></ul><p>Returns:</p><p>A Tensor of type string.</p><p>看看执行完两句tf.read_file()，Graph发生了什么变化：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-6b383b79b3f33b8c4fbf2913a720319b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"836\" data-rawheight=\"733\" class=\"origin_image zh-lightbox-thumb\" width=\"836\" data-original=\"https://pic4.zhimg.com/v2-6b383b79b3f33b8c4fbf2913a720319b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;836&#39; height=&#39;733&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"836\" data-rawheight=\"733\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"836\" data-original=\"https://pic4.zhimg.com/v2-6b383b79b3f33b8c4fbf2913a720319b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-6b383b79b3f33b8c4fbf2913a720319b_b.jpg\"/></figure><p>由此可见，TensorFlow读取数据的时候，其实并没有真正读取数据（所谓“读取”者，即非“读取”，名为“读取”。。。），而是通过图像名的list生成Constant的Tensor，然后通过tf.train.slice_input_producer()每次取一对【图像-标签】对，交给ReadFile这个Op。</p><p>接下来的代码对数据进行了处理，注意这里重载了“-”运算符。</p><div class=\"highlight\"><pre><code class=\"language-python3\">  <span class=\"n\">img_contents</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">read_file</span><span class=\"p\">(</span><span class=\"n\">input_queue</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">])</span>\n    <span class=\"n\">label_contents</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">read_file</span><span class=\"p\">(</span><span class=\"n\">input_queue</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">])</span>\n    <span class=\"c1\"># Yuxuan: Change to png</span>\n    <span class=\"n\">img</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">image</span><span class=\"o\">.</span><span class=\"n\">decode_png</span><span class=\"p\">(</span><span class=\"n\">img_contents</span><span class=\"p\">,</span> <span class=\"n\">channels</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">)</span>\n    <span class=\"n\">img_r</span><span class=\"p\">,</span> <span class=\"n\">img_g</span><span class=\"p\">,</span> <span class=\"n\">img_b</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">(</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">num_or_size_splits</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"n\">value</span><span class=\"o\">=</span><span class=\"n\">img</span><span class=\"p\">)</span>\n    <span class=\"n\">img</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">cast</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">concat</span><span class=\"p\">(</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">values</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">img_b</span><span class=\"p\">,</span> <span class=\"n\">img_g</span><span class=\"p\">,</span> <span class=\"n\">img_r</span><span class=\"p\">]),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>\n    <span class=\"c1\"># Extract mean.</span>\n    <span class=\"n\">img</span> <span class=\"o\">-=</span> <span class=\"n\">img_mean</span>\n    <span class=\"c1\"># img = tf.</span>\n\n    <span class=\"n\">label</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">image</span><span class=\"o\">.</span><span class=\"n\">decode_png</span><span class=\"p\">(</span><span class=\"n\">label_contents</span><span class=\"p\">,</span> <span class=\"n\">channels</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span></code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-06442322b9ac107d67161dc8ed20d838_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"481\" data-rawheight=\"834\" class=\"origin_image zh-lightbox-thumb\" width=\"481\" data-original=\"https://pic1.zhimg.com/v2-06442322b9ac107d67161dc8ed20d838_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;481&#39; height=&#39;834&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"481\" data-rawheight=\"834\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"481\" data-original=\"https://pic1.zhimg.com/v2-06442322b9ac107d67161dc8ed20d838_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-06442322b9ac107d67161dc8ed20d838_b.jpg\"/></figure><p>弄明白了“每执行一条tf的语句，就等于在Graph上添加一个op”，下面的if也很好理解了，这里是说如果设置了input_size，random_scale和random_mirror，就在Graph上添加一系列预处理的op，反之，则不添加op。</p><p>【2】tf.train.batch()</p><p>先看看在什么地方调用的tf.train.batch()</p><div class=\"highlight\"><pre><code class=\"language-text\">  coord = tf.train.Coordinator()\n    \n    with tf.name_scope(&#34;create_inputs&#34;):\n        reader = ImageReader(\n            args.data_dir,\n            args.data_list,\n            input_size,\n            args.random_scale,\n            args.random_mirror,\n            args.ignore_label,\n            IMG_MEAN,\n            coord)\n        image_batch, label_batch = reader.dequeue(args.batch_size)</code></pre></div><p>tf.train.batch()在ImageReader类的dequeue()函数中使用</p><div class=\"highlight\"><pre><code class=\"language-text\">  def dequeue(self, num_elements):\n        image_batch, label_batch = tf.train.batch([self.image, self.label],\n                                                  num_elements)\n        return image_batch, label_batch</code></pre></div><p>看一下文档：</p><h2>tf.train.batch</h2><p class=\"ztext-empty-paragraph\"><br/></p><p><code>batch(<br/>    tensors,<br/>    batch_size,<br/>    num_threads=1,<br/>    capacity=32,<br/>    enqueue_many=False,<br/>    shapes=None,<br/>    dynamic_pad=False,<br/>    allow_smaller_final_batch=False,<br/>    shared_name=None,<br/>    name=None</code> <code>)</code> </p><p>Creates batches of tensors in <code>tensors</code>.</p><p>The argument <code>tensors</code> can be a list or a dictionary of tensors. </p><p>在这里就是[self.image, self.label]</p><p>The value returned by the function will be of the same type as <code>tensors</code>.</p><p>image_batch, label_batch和self.image, self.label的类型一样，但是shape并不一样：</p><p>注意self.image是(713,713,3),image_batch是(1,713,713,3)，其中1是batch size.</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-75ae1f18a9a4b8c4f16c91a006db7182_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1534\" data-rawheight=\"285\" class=\"origin_image zh-lightbox-thumb\" width=\"1534\" data-original=\"https://pic3.zhimg.com/v2-75ae1f18a9a4b8c4f16c91a006db7182_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1534&#39; height=&#39;285&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1534\" data-rawheight=\"285\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1534\" data-original=\"https://pic3.zhimg.com/v2-75ae1f18a9a4b8c4f16c91a006db7182_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-75ae1f18a9a4b8c4f16c91a006db7182_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-da063e5b815b5ee4a67d1266a8372250_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1549\" data-rawheight=\"346\" class=\"origin_image zh-lightbox-thumb\" width=\"1549\" data-original=\"https://pic1.zhimg.com/v2-da063e5b815b5ee4a67d1266a8372250_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1549&#39; height=&#39;346&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1549\" data-rawheight=\"346\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1549\" data-original=\"https://pic1.zhimg.com/v2-da063e5b815b5ee4a67d1266a8372250_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-da063e5b815b5ee4a67d1266a8372250_b.jpg\"/></figure><p>This function is implemented using a queue. <b>A <code>QueueRunner</code> for the queue is added to the current <code>Graph</code>&#39;s <code>QUEUE_RUNNER</code></b> collection.</p><p>关于Queue和QueueRunner：</p><p><a href=\"https://zhuanlan.zhihu.com/p/31361295\" class=\"internal\">TensorFlow中的Queue和QueueRunner</a></p><p>If <code>enqueue_many</code> is <code>False</code>, <code><b>tensors</b></code> is assumed to represent <b>a single example【注意控制的是输入的tensors】</b>. An input tensor with shape <code>[x, y, z]</code> will be output as a tensor with shape <code>[batch_size, x, y, z]</code>.</p><p>If <code>enqueue_many</code> is <code>True</code>, <code><b>tensors</b></code> is assumed to represent <b>a batch of examples</b>, where <b>the first dimension is indexed by example</b>, and all members of <code>tensors</code> should have the same size in the first dimension. <b>【输入的就是一个batch】</b> If an input tensor has shape <code>[*, x, y, z]</code>, the output will have shape <code>[batch_size, x, y, z]</code>. <b>【调整batch_size的大小】</b>The <code>capacity</code> argument controls the how long the prefetching is allowed to grow the queues.</p><p>The <b>returned operation</b> is a <b>dequeue</b> operation and will throw <code>tf.errors.OutOfRangeError</code> if the input queue is exhausted. If this operation is <b>feeding another input queue</b>, its <b>queue runner</b> will catch this exception,【<b>这种情况由queue runner负责管理】</b> however, if this operation is used in your <b>main thread</b> you are responsible for catching this yourself.【<b>main thread-&gt;自行管理</b>】</p><p><b><i>N.B.:</i> If <code>dynamic_pad</code> is <code>False</code>, you must ensure that either (i) the <code>shapes</code> argument is passed, or (ii) all of the tensors in <code>tensors</code> must have fully-defined shapes. <code>ValueError</code> will be raised if neither of these conditions holds.</b></p><p>If <code>dynamic_pad</code> is <code>True</code>, it is sufficient that the <i>rank</i> of the tensors is known【张量的“阶”即张量的维数】, but individual dimensions may have shape <code>None</code>. In this case, for each enqueue the dimensions with value <code><b>None</b></code> may have <b>a variable length</b>; upon dequeue, the output tensors will be padded on the right to the maximum shape of the tensors in the current minibatch. For numbers, this padding takes value 0. For strings, this padding is the empty string. See <code>PaddingFIFOQueue</code> for more info.</p><p>If <code>allow_smaller_final_batch</code> is <code>True</code>, a smaller batch value than <code>batch_size</code> is returned when the queue is closed and there are not enough elements to fill the batch, otherwise the pending elements are discarded.【Queue最后一个batch怎么处理：丢弃还是保留】 In addition, all output tensors&#39; static shapes, as accessed via the <code>shape</code> property will have <b>a first <code>Dimension</code></b> value of <code><b>None</b></code>, and <b>operations that depend on fixed batch_size would fail</b>.【看这个情况，还是不要用了。。。】</p><p>Args:</p><ul><li><b><code>tensors</code></b>: The list or dictionary of tensors to enqueue. <b>入队的tensors</b></li><li><b><code>batch_size</code></b>: The new batch size pulled from the queue. </li><li><b><code>num_threads</code></b>: The number of threads enqueuing <code>tensors</code>. The batching will be nondeterministic if <code>num_threads &gt; 1</code>.</li><li><b><code>capacity</code></b>: An integer. The maximum number of elements in the queue.</li><li><b><code>enqueue_many</code></b>: Whether each tensor in <code>tensors</code> is a single example.</li><li><b><code>shapes</code></b>: (Optional) The shapes for each example. Defaults to the inferred shapes for <code>tensors</code>.</li><li><b><code>dynamic_pad</code></b>: Boolean. Allow variable dimensions in input shapes. The given dimensions are padded upon dequeue so that tensors within a batch have the same shapes.</li><li><b><code>allow_smaller_final_batch</code></b>: (Optional) Boolean. If <code>True</code>, allow the final batch to be smaller if there are insufficient items left in the queue.</li><li><b><code>shared_name</code></b>: (Optional). If set, this queue will be shared under the given name across multiple sessions.</li><li><b><code>name</code></b>: (Optional) A name for the operations.</li></ul><p>Returns:</p><h2>A list or dictionary of tensors with the same types as </h2><h2><code>tensors</code> (except if the input is a list of one element, </h2><h2>then it returns a tensor, not a list).</h2><p>Raises:</p><ul><li><b><code>ValueError</code></b>: If the <code>shapes</code> are not specified, and cannot be inferred from the elements of <code>tensors</code>.</li></ul><p>这是执行tf.train.batch()之前的Graph：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-178efeaea829afee0be3afa0a93dadeb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"973\" data-rawheight=\"745\" class=\"origin_image zh-lightbox-thumb\" width=\"973\" data-original=\"https://pic4.zhimg.com/v2-178efeaea829afee0be3afa0a93dadeb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;973&#39; height=&#39;745&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"973\" data-rawheight=\"745\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"973\" data-original=\"https://pic4.zhimg.com/v2-178efeaea829afee0be3afa0a93dadeb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-178efeaea829afee0be3afa0a93dadeb_b.jpg\"/></figure><p>这是执行tf.train.batch()之后的Graph:</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-4301eb4f4326370109a5ee6ab1fb468f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"962\" data-rawheight=\"796\" class=\"origin_image zh-lightbox-thumb\" width=\"962\" data-original=\"https://pic4.zhimg.com/v2-4301eb4f4326370109a5ee6ab1fb468f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;962&#39; height=&#39;796&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"962\" data-rawheight=\"796\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"962\" data-original=\"https://pic4.zhimg.com/v2-4301eb4f4326370109a5ee6ab1fb468f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-4301eb4f4326370109a5ee6ab1fb468f_b.jpg\"/></figure><p>查看一下多出来的batch：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-54888d5c43b89577cd6f97cee0fe4c53_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1404\" data-rawheight=\"560\" class=\"origin_image zh-lightbox-thumb\" width=\"1404\" data-original=\"https://pic4.zhimg.com/v2-54888d5c43b89577cd6f97cee0fe4c53_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1404&#39; height=&#39;560&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1404\" data-rawheight=\"560\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1404\" data-original=\"https://pic4.zhimg.com/v2-54888d5c43b89577cd6f97cee0fe4c53_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-54888d5c43b89577cd6f97cee0fe4c53_b.jpg\"/></figure><p>可以看到内部的fifo_queue的各种操作。</p><p></p><p></p>", 
            "topic": [
                {
                    "tag": "TensorFlow", 
                    "tagLink": "https://api.zhihu.com/topics/20032249"
                }
            ], 
            "comments": [
                {
                    "userName": "judy", 
                    "userLink": "https://www.zhihu.com/people/33880806148398db37cf25349c918ba9", 
                    "content": "<p>prefetch是不是enqueue啊？</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "Kyle Huang", 
                    "userLink": "https://www.zhihu.com/people/d8e8b12400d887406a9e08d5d5372a24", 
                    "content": "<p>非常详细的讲解。我在使用中发现一个问题</p><p>比如我的data是72xWxH, slice_input_producer(epoch=3), tf.train.batch(batch_size=9).  按照我的理解，8个batch(9xW*H)就能遍历全部的数据，3个epoch应该只需要24个batch. 但程序实际循环了2500次才跳出self.coord.should_stop()，差不多是100倍。为什么？ </p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/31456792", 
            "userName": "休语", 
            "userLink": "https://www.zhihu.com/people/9fa98b02c1f908f202862a38bd15084f", 
            "upvote": 0, 
            "title": "TensorFlow的全局变量“global variables”", 
            "content": "<p>TensorFlow有一个colletion机制，<a href=\"https://link.zhihu.com/?target=http%3A//blog.csdn.net/shenxiaolu1984/article/details/52815641\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">blog.csdn.net/shenxiaol</span><span class=\"invisible\">u1984/article/details/52815641</span><span class=\"ellipsis\"></span></a> 这里描述得非常好：“<b>零存整取</b>”</p><blockquote>tensorflow用集合<code>colletion</code>组织不同类别的对象。<code>tf.GraphKeys</code>中包含了所有默认集合的名称。<br/><code>collection</code>提供了一种“<b>零存整取</b>”的思路：在任意位置，任意层次都可以创造对象，存入相应<code>collection</code>中；创造完成后，统一从一个<code>collection</code>中取出一类变量，施加相应操作。<br/>例如，<code>tf.Optimizer</code>只优化<code>tf.GraphKeys.TRAINABLE_VARIABLES</code>中的变量。</blockquote><p>global_variables也是一个collection。</p><p>TensorFlow维护的几个collection：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-05fe8783e2b1f3e9fa26ed2d649e46c0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1084\" data-rawheight=\"314\" class=\"origin_image zh-lightbox-thumb\" width=\"1084\" data-original=\"https://pic1.zhimg.com/v2-05fe8783e2b1f3e9fa26ed2d649e46c0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1084&#39; height=&#39;314&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1084\" data-rawheight=\"314\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1084\" data-original=\"https://pic1.zhimg.com/v2-05fe8783e2b1f3e9fa26ed2d649e46c0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-05fe8783e2b1f3e9fa26ed2d649e46c0_b.jpg\"/></figure><p>【1】先看文档</p><h2>tf.global_variables</h2><p><code>global_variables(scope=None)</code> </p><div class=\"highlight\"><pre><code class=\"language-text\">Defined in tensorflow/python/ops/variables.py.</code></pre></div><p>See the guide: <a href=\"https://link.zhihu.com/?target=https%3A//www.tensorflow.org/versions/master/api_guides/python/state_ops%23Variable_helper_functions\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Variables &gt; Variable helper functions</a></p><p>Returns <b>global variables</b>.</p><p>Global variables are variables that <b>are shared across machines</b> in a distributed environment. </p><p>这句话似乎并没有什么帮助。。。</p><p>The <code>Variable()</code>constructor or <code>get_variable()</code> automatically <b>adds new variables</b> to the graph <b>collection<code>GraphKeys.GLOBAL_VARIABLES</code></b>. 【<b>Variable()和get_variable()会生成新的global variable</b>】</p><p>This convenience function returns the contents of that collection.</p><p>An alternative to global variables are local variables. See <code><a href=\"https://link.zhihu.com/?target=https%3A//www.tensorflow.org/versions/master/api_docs/python/tf/local_variables\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">tf.local_variables</a></code></p><p>Args:</p><ul><li><code><b>scope</b></code>: (Optional.) A string. If supplied, the resulting list <b>is filtered</b> to include <b>only items</b> whose <code>name</code> attribute <b>matches <code>scope</code> using <code>re.match</code></b>. 【name用正则匹配name】Items <b>without a <code>name</code> attribute</b> are <b>never returned</b> if a scope is supplied.【没有name的不会被匹配】 The choice of <code>re.match</code> means that a <code>scope</code> without special tokens filters by prefix.</li></ul><p>Returns:</p><h2>A list of <code>Variable</code> objects.</h2><p>【2】通过tf.global_variables()查看“全局变量”</p><div class=\"highlight\"><pre><code class=\"language-text\">restore_var = [v for v in tf.global_variables()]</code></pre></div><p>以下是PSPNet的全部“global variables”，可以看到基本上是各层的权重和各种参数：</p><div class=\"highlight\"><pre><code class=\"language-text\">&lt;tf.Variable &#39;conv1_1_3x3_s2/weights:0&#39; shape=(3, 3, 3, 64) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv1_1_3x3_s2_bn/conv1_1_3x3_s2_bn/gamma:0&#39; shape=(64,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv1_1_3x3_s2_bn/conv1_1_3x3_s2_bn/beta:0&#39; shape=(64,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv1_1_3x3_s2_bn/conv1_1_3x3_s2_bn/moving_mean:0&#39; shape=(64,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv1_1_3x3_s2_bn/conv1_1_3x3_s2_bn/moving_variance:0&#39; shape=(64,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv1_2_3x3/weights:0&#39; shape=(3, 3, 64, 64) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv1_2_3x3_bn/conv1_2_3x3_bn/gamma:0&#39; shape=(64,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv1_2_3x3_bn/conv1_2_3x3_bn/beta:0&#39; shape=(64,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv1_2_3x3_bn/conv1_2_3x3_bn/moving_mean:0&#39; shape=(64,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv1_2_3x3_bn/conv1_2_3x3_bn/moving_variance:0&#39; shape=(64,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv1_3_3x3/weights:0&#39; shape=(3, 3, 64, 128) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv1_3_3x3_bn/conv1_3_3x3_bn/gamma:0&#39; shape=(128,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv1_3_3x3_bn/conv1_3_3x3_bn/beta:0&#39; shape=(128,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv1_3_3x3_bn/conv1_3_3x3_bn/moving_mean:0&#39; shape=(128,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv1_3_3x3_bn/conv1_3_3x3_bn/moving_variance:0&#39; shape=(128,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv2_1_1x1_proj/weights:0&#39; shape=(1, 1, 128, 256) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv2_1_1x1_proj_bn/conv2_1_1x1_proj_bn/gamma:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv2_1_1x1_proj_bn/conv2_1_1x1_proj_bn/beta:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv2_1_1x1_proj_bn/conv2_1_1x1_proj_bn/moving_mean:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv2_1_1x1_proj_bn/conv2_1_1x1_proj_bn/moving_variance:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv2_1_1x1_reduce/weights:0&#39; shape=(1, 1, 128, 64) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv2_1_1x1_reduce_bn/conv2_1_1x1_reduce_bn/gamma:0&#39; shape=(64,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv2_1_1x1_reduce_bn/conv2_1_1x1_reduce_bn/beta:0&#39; shape=(64,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv2_1_1x1_reduce_bn/conv2_1_1x1_reduce_bn/moving_mean:0&#39; shape=(64,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv2_1_1x1_reduce_bn/conv2_1_1x1_reduce_bn/moving_variance:0&#39; shape=(64,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv2_1_3x3/weights:0&#39; shape=(3, 3, 64, 64) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv2_1_3x3_bn/conv2_1_3x3_bn/gamma:0&#39; shape=(64,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv2_1_3x3_bn/conv2_1_3x3_bn/beta:0&#39; shape=(64,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv2_1_3x3_bn/conv2_1_3x3_bn/moving_mean:0&#39; shape=(64,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv2_1_3x3_bn/conv2_1_3x3_bn/moving_variance:0&#39; shape=(64,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv2_1_1x1_increase/weights:0&#39; shape=(1, 1, 64, 256) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv2_1_1x1_increase_bn/conv2_1_1x1_increase_bn/gamma:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv2_1_1x1_increase_bn/conv2_1_1x1_increase_bn/beta:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv2_1_1x1_increase_bn/conv2_1_1x1_increase_bn/moving_mean:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv2_1_1x1_increase_bn/conv2_1_1x1_increase_bn/moving_variance:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv2_2_1x1_reduce/weights:0&#39; shape=(1, 1, 256, 64) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv2_2_1x1_reduce_bn/conv2_2_1x1_reduce_bn/gamma:0&#39; shape=(64,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv2_2_1x1_reduce_bn/conv2_2_1x1_reduce_bn/beta:0&#39; shape=(64,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv2_2_1x1_reduce_bn/conv2_2_1x1_reduce_bn/moving_mean:0&#39; shape=(64,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv2_2_1x1_reduce_bn/conv2_2_1x1_reduce_bn/moving_variance:0&#39; shape=(64,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv2_2_3x3/weights:0&#39; shape=(3, 3, 64, 64) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv2_2_3x3_bn/conv2_2_3x3_bn/gamma:0&#39; shape=(64,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv2_2_3x3_bn/conv2_2_3x3_bn/beta:0&#39; shape=(64,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv2_2_3x3_bn/conv2_2_3x3_bn/moving_mean:0&#39; shape=(64,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv2_2_3x3_bn/conv2_2_3x3_bn/moving_variance:0&#39; shape=(64,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv2_2_1x1_increase/weights:0&#39; shape=(1, 1, 64, 256) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv2_2_1x1_increase_bn/conv2_2_1x1_increase_bn/gamma:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv2_2_1x1_increase_bn/conv2_2_1x1_increase_bn/beta:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv2_2_1x1_increase_bn/conv2_2_1x1_increase_bn/moving_mean:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv2_2_1x1_increase_bn/conv2_2_1x1_increase_bn/moving_variance:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv2_3_1x1_reduce/weights:0&#39; shape=(1, 1, 256, 64) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv2_3_1x1_reduce_bn/conv2_3_1x1_reduce_bn/gamma:0&#39; shape=(64,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv2_3_1x1_reduce_bn/conv2_3_1x1_reduce_bn/beta:0&#39; shape=(64,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv2_3_1x1_reduce_bn/conv2_3_1x1_reduce_bn/moving_mean:0&#39; shape=(64,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv2_3_1x1_reduce_bn/conv2_3_1x1_reduce_bn/moving_variance:0&#39; shape=(64,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv2_3_3x3/weights:0&#39; shape=(3, 3, 64, 64) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv2_3_3x3_bn/conv2_3_3x3_bn/gamma:0&#39; shape=(64,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv2_3_3x3_bn/conv2_3_3x3_bn/beta:0&#39; shape=(64,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv2_3_3x3_bn/conv2_3_3x3_bn/moving_mean:0&#39; shape=(64,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv2_3_3x3_bn/conv2_3_3x3_bn/moving_variance:0&#39; shape=(64,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv2_3_1x1_increase/weights:0&#39; shape=(1, 1, 64, 256) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv2_3_1x1_increase_bn/conv2_3_1x1_increase_bn/gamma:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv2_3_1x1_increase_bn/conv2_3_1x1_increase_bn/beta:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv2_3_1x1_increase_bn/conv2_3_1x1_increase_bn/moving_mean:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv2_3_1x1_increase_bn/conv2_3_1x1_increase_bn/moving_variance:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_1_1x1_proj/weights:0&#39; shape=(1, 1, 256, 512) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_1_1x1_proj_bn/conv3_1_1x1_proj_bn/gamma:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_1_1x1_proj_bn/conv3_1_1x1_proj_bn/beta:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_1_1x1_proj_bn/conv3_1_1x1_proj_bn/moving_mean:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_1_1x1_proj_bn/conv3_1_1x1_proj_bn/moving_variance:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_1_1x1_reduce/weights:0&#39; shape=(1, 1, 256, 128) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_1_1x1_reduce_bn/conv3_1_1x1_reduce_bn/gamma:0&#39; shape=(128,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_1_1x1_reduce_bn/conv3_1_1x1_reduce_bn/beta:0&#39; shape=(128,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_1_1x1_reduce_bn/conv3_1_1x1_reduce_bn/moving_mean:0&#39; shape=(128,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_1_1x1_reduce_bn/conv3_1_1x1_reduce_bn/moving_variance:0&#39; shape=(128,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_1_3x3/weights:0&#39; shape=(3, 3, 128, 128) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_1_3x3_bn/conv3_1_3x3_bn/gamma:0&#39; shape=(128,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_1_3x3_bn/conv3_1_3x3_bn/beta:0&#39; shape=(128,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_1_3x3_bn/conv3_1_3x3_bn/moving_mean:0&#39; shape=(128,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_1_3x3_bn/conv3_1_3x3_bn/moving_variance:0&#39; shape=(128,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_1_1x1_increase/weights:0&#39; shape=(1, 1, 128, 512) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_1_1x1_increase_bn/conv3_1_1x1_increase_bn/gamma:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_1_1x1_increase_bn/conv3_1_1x1_increase_bn/beta:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_1_1x1_increase_bn/conv3_1_1x1_increase_bn/moving_mean:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_1_1x1_increase_bn/conv3_1_1x1_increase_bn/moving_variance:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_2_1x1_reduce/weights:0&#39; shape=(1, 1, 512, 128) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_2_1x1_reduce_bn/conv3_2_1x1_reduce_bn/gamma:0&#39; shape=(128,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_2_1x1_reduce_bn/conv3_2_1x1_reduce_bn/beta:0&#39; shape=(128,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_2_1x1_reduce_bn/conv3_2_1x1_reduce_bn/moving_mean:0&#39; shape=(128,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_2_1x1_reduce_bn/conv3_2_1x1_reduce_bn/moving_variance:0&#39; shape=(128,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_2_3x3/weights:0&#39; shape=(3, 3, 128, 128) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_2_3x3_bn/conv3_2_3x3_bn/gamma:0&#39; shape=(128,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_2_3x3_bn/conv3_2_3x3_bn/beta:0&#39; shape=(128,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_2_3x3_bn/conv3_2_3x3_bn/moving_mean:0&#39; shape=(128,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_2_3x3_bn/conv3_2_3x3_bn/moving_variance:0&#39; shape=(128,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_2_1x1_increase/weights:0&#39; shape=(1, 1, 128, 512) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_2_1x1_increase_bn/conv3_2_1x1_increase_bn/gamma:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_2_1x1_increase_bn/conv3_2_1x1_increase_bn/beta:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_2_1x1_increase_bn/conv3_2_1x1_increase_bn/moving_mean:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_2_1x1_increase_bn/conv3_2_1x1_increase_bn/moving_variance:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_3_1x1_reduce/weights:0&#39; shape=(1, 1, 512, 128) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_3_1x1_reduce_bn/conv3_3_1x1_reduce_bn/gamma:0&#39; shape=(128,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_3_1x1_reduce_bn/conv3_3_1x1_reduce_bn/beta:0&#39; shape=(128,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_3_1x1_reduce_bn/conv3_3_1x1_reduce_bn/moving_mean:0&#39; shape=(128,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_3_1x1_reduce_bn/conv3_3_1x1_reduce_bn/moving_variance:0&#39; shape=(128,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_3_3x3/weights:0&#39; shape=(3, 3, 128, 128) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_3_3x3_bn/conv3_3_3x3_bn/gamma:0&#39; shape=(128,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_3_3x3_bn/conv3_3_3x3_bn/beta:0&#39; shape=(128,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_3_3x3_bn/conv3_3_3x3_bn/moving_mean:0&#39; shape=(128,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_3_3x3_bn/conv3_3_3x3_bn/moving_variance:0&#39; shape=(128,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_3_1x1_increase/weights:0&#39; shape=(1, 1, 128, 512) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_3_1x1_increase_bn/conv3_3_1x1_increase_bn/gamma:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_3_1x1_increase_bn/conv3_3_1x1_increase_bn/beta:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_3_1x1_increase_bn/conv3_3_1x1_increase_bn/moving_mean:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_3_1x1_increase_bn/conv3_3_1x1_increase_bn/moving_variance:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_4_1x1_reduce/weights:0&#39; shape=(1, 1, 512, 128) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_4_1x1_reduce_bn/conv3_4_1x1_reduce_bn/gamma:0&#39; shape=(128,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_4_1x1_reduce_bn/conv3_4_1x1_reduce_bn/beta:0&#39; shape=(128,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_4_1x1_reduce_bn/conv3_4_1x1_reduce_bn/moving_mean:0&#39; shape=(128,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_4_1x1_reduce_bn/conv3_4_1x1_reduce_bn/moving_variance:0&#39; shape=(128,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_4_3x3/weights:0&#39; shape=(3, 3, 128, 128) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_4_3x3_bn/conv3_4_3x3_bn/gamma:0&#39; shape=(128,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_4_3x3_bn/conv3_4_3x3_bn/beta:0&#39; shape=(128,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_4_3x3_bn/conv3_4_3x3_bn/moving_mean:0&#39; shape=(128,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_4_3x3_bn/conv3_4_3x3_bn/moving_variance:0&#39; shape=(128,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_4_1x1_increase/weights:0&#39; shape=(1, 1, 128, 512) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_4_1x1_increase_bn/conv3_4_1x1_increase_bn/gamma:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_4_1x1_increase_bn/conv3_4_1x1_increase_bn/beta:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_4_1x1_increase_bn/conv3_4_1x1_increase_bn/moving_mean:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv3_4_1x1_increase_bn/conv3_4_1x1_increase_bn/moving_variance:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_1_1x1_proj/weights:0&#39; shape=(1, 1, 512, 1024) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_1_1x1_proj_bn/conv4_1_1x1_proj_bn/gamma:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_1_1x1_proj_bn/conv4_1_1x1_proj_bn/beta:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_1_1x1_proj_bn/conv4_1_1x1_proj_bn/moving_mean:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_1_1x1_proj_bn/conv4_1_1x1_proj_bn/moving_variance:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_1_1x1_reduce/weights:0&#39; shape=(1, 1, 512, 256) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_1_1x1_reduce_bn/conv4_1_1x1_reduce_bn/gamma:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_1_1x1_reduce_bn/conv4_1_1x1_reduce_bn/beta:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_1_1x1_reduce_bn/conv4_1_1x1_reduce_bn/moving_mean:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_1_1x1_reduce_bn/conv4_1_1x1_reduce_bn/moving_variance:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_1_3x3/weights:0&#39; shape=(3, 3, 256, 256) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_1_3x3_bn/conv4_1_3x3_bn/gamma:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_1_3x3_bn/conv4_1_3x3_bn/beta:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_1_3x3_bn/conv4_1_3x3_bn/moving_mean:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_1_3x3_bn/conv4_1_3x3_bn/moving_variance:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_1_1x1_increase/weights:0&#39; shape=(1, 1, 256, 1024) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_1_1x1_increase_bn/conv4_1_1x1_increase_bn/gamma:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_1_1x1_increase_bn/conv4_1_1x1_increase_bn/beta:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_1_1x1_increase_bn/conv4_1_1x1_increase_bn/moving_mean:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_1_1x1_increase_bn/conv4_1_1x1_increase_bn/moving_variance:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_2_1x1_reduce/weights:0&#39; shape=(1, 1, 1024, 256) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_2_1x1_reduce_bn/conv4_2_1x1_reduce_bn/gamma:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_2_1x1_reduce_bn/conv4_2_1x1_reduce_bn/beta:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_2_1x1_reduce_bn/conv4_2_1x1_reduce_bn/moving_mean:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_2_1x1_reduce_bn/conv4_2_1x1_reduce_bn/moving_variance:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_2_3x3/weights:0&#39; shape=(3, 3, 256, 256) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_2_3x3_bn/conv4_2_3x3_bn/gamma:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_2_3x3_bn/conv4_2_3x3_bn/beta:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_2_3x3_bn/conv4_2_3x3_bn/moving_mean:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_2_3x3_bn/conv4_2_3x3_bn/moving_variance:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_2_1x1_increase/weights:0&#39; shape=(1, 1, 256, 1024) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_2_1x1_increase_bn/conv4_2_1x1_increase_bn/gamma:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_2_1x1_increase_bn/conv4_2_1x1_increase_bn/beta:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_2_1x1_increase_bn/conv4_2_1x1_increase_bn/moving_mean:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_2_1x1_increase_bn/conv4_2_1x1_increase_bn/moving_variance:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_3_1x1_reduce/weights:0&#39; shape=(1, 1, 1024, 256) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_3_1x1_reduce_bn/conv4_3_1x1_reduce_bn/gamma:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_3_1x1_reduce_bn/conv4_3_1x1_reduce_bn/beta:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_3_1x1_reduce_bn/conv4_3_1x1_reduce_bn/moving_mean:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_3_1x1_reduce_bn/conv4_3_1x1_reduce_bn/moving_variance:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_3_3x3/weights:0&#39; shape=(3, 3, 256, 256) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_3_3x3_bn/conv4_3_3x3_bn/gamma:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_3_3x3_bn/conv4_3_3x3_bn/beta:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_3_3x3_bn/conv4_3_3x3_bn/moving_mean:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_3_3x3_bn/conv4_3_3x3_bn/moving_variance:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_3_1x1_increase/weights:0&#39; shape=(1, 1, 256, 1024) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_3_1x1_increase_bn/conv4_3_1x1_increase_bn/gamma:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_3_1x1_increase_bn/conv4_3_1x1_increase_bn/beta:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_3_1x1_increase_bn/conv4_3_1x1_increase_bn/moving_mean:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_3_1x1_increase_bn/conv4_3_1x1_increase_bn/moving_variance:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_4_1x1_reduce/weights:0&#39; shape=(1, 1, 1024, 256) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_4_1x1_reduce_bn/conv4_4_1x1_reduce_bn/gamma:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_4_1x1_reduce_bn/conv4_4_1x1_reduce_bn/beta:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_4_1x1_reduce_bn/conv4_4_1x1_reduce_bn/moving_mean:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_4_1x1_reduce_bn/conv4_4_1x1_reduce_bn/moving_variance:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_4_3x3/weights:0&#39; shape=(3, 3, 256, 256) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_4_3x3_bn/conv4_4_3x3_bn/gamma:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_4_3x3_bn/conv4_4_3x3_bn/beta:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_4_3x3_bn/conv4_4_3x3_bn/moving_mean:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_4_3x3_bn/conv4_4_3x3_bn/moving_variance:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_4_1x1_increase/weights:0&#39; shape=(1, 1, 256, 1024) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_4_1x1_increase_bn/conv4_4_1x1_increase_bn/gamma:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_4_1x1_increase_bn/conv4_4_1x1_increase_bn/beta:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_4_1x1_increase_bn/conv4_4_1x1_increase_bn/moving_mean:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_4_1x1_increase_bn/conv4_4_1x1_increase_bn/moving_variance:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_5_1x1_reduce/weights:0&#39; shape=(1, 1, 1024, 256) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_5_1x1_reduce_bn/conv4_5_1x1_reduce_bn/gamma:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_5_1x1_reduce_bn/conv4_5_1x1_reduce_bn/beta:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_5_1x1_reduce_bn/conv4_5_1x1_reduce_bn/moving_mean:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_5_1x1_reduce_bn/conv4_5_1x1_reduce_bn/moving_variance:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_5_3x3/weights:0&#39; shape=(3, 3, 256, 256) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_5_3x3_bn/conv4_5_3x3_bn/gamma:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_5_3x3_bn/conv4_5_3x3_bn/beta:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_5_3x3_bn/conv4_5_3x3_bn/moving_mean:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_5_3x3_bn/conv4_5_3x3_bn/moving_variance:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_5_1x1_increase/weights:0&#39; shape=(1, 1, 256, 1024) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_5_1x1_increase_bn/conv4_5_1x1_increase_bn/gamma:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_5_1x1_increase_bn/conv4_5_1x1_increase_bn/beta:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_5_1x1_increase_bn/conv4_5_1x1_increase_bn/moving_mean:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_5_1x1_increase_bn/conv4_5_1x1_increase_bn/moving_variance:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_6_1x1_reduce/weights:0&#39; shape=(1, 1, 1024, 256) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_6_1x1_reduce_bn/conv4_6_1x1_reduce_bn/gamma:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_6_1x1_reduce_bn/conv4_6_1x1_reduce_bn/beta:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_6_1x1_reduce_bn/conv4_6_1x1_reduce_bn/moving_mean:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_6_1x1_reduce_bn/conv4_6_1x1_reduce_bn/moving_variance:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_6_3x3/weights:0&#39; shape=(3, 3, 256, 256) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_6_3x3_bn/conv4_6_3x3_bn/gamma:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_6_3x3_bn/conv4_6_3x3_bn/beta:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_6_3x3_bn/conv4_6_3x3_bn/moving_mean:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_6_3x3_bn/conv4_6_3x3_bn/moving_variance:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_6_1x1_increase/weights:0&#39; shape=(1, 1, 256, 1024) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_6_1x1_increase_bn/conv4_6_1x1_increase_bn/gamma:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_6_1x1_increase_bn/conv4_6_1x1_increase_bn/beta:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_6_1x1_increase_bn/conv4_6_1x1_increase_bn/moving_mean:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_6_1x1_increase_bn/conv4_6_1x1_increase_bn/moving_variance:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_7_1x1_reduce/weights:0&#39; shape=(1, 1, 1024, 256) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_7_1x1_reduce_bn/conv4_7_1x1_reduce_bn/gamma:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_7_1x1_reduce_bn/conv4_7_1x1_reduce_bn/beta:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_7_1x1_reduce_bn/conv4_7_1x1_reduce_bn/moving_mean:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_7_1x1_reduce_bn/conv4_7_1x1_reduce_bn/moving_variance:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_7_3x3/weights:0&#39; shape=(3, 3, 256, 256) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_7_3x3_bn/conv4_7_3x3_bn/gamma:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_7_3x3_bn/conv4_7_3x3_bn/beta:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_7_3x3_bn/conv4_7_3x3_bn/moving_mean:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_7_3x3_bn/conv4_7_3x3_bn/moving_variance:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_7_1x1_increase/weights:0&#39; shape=(1, 1, 256, 1024) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_7_1x1_increase_bn/conv4_7_1x1_increase_bn/gamma:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_7_1x1_increase_bn/conv4_7_1x1_increase_bn/beta:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_7_1x1_increase_bn/conv4_7_1x1_increase_bn/moving_mean:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_7_1x1_increase_bn/conv4_7_1x1_increase_bn/moving_variance:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_8_1x1_reduce/weights:0&#39; shape=(1, 1, 1024, 256) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_8_1x1_reduce_bn/conv4_8_1x1_reduce_bn/gamma:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_8_1x1_reduce_bn/conv4_8_1x1_reduce_bn/beta:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_8_1x1_reduce_bn/conv4_8_1x1_reduce_bn/moving_mean:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_8_1x1_reduce_bn/conv4_8_1x1_reduce_bn/moving_variance:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_8_3x3/weights:0&#39; shape=(3, 3, 256, 256) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_8_3x3_bn/conv4_8_3x3_bn/gamma:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_8_3x3_bn/conv4_8_3x3_bn/beta:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_8_3x3_bn/conv4_8_3x3_bn/moving_mean:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_8_3x3_bn/conv4_8_3x3_bn/moving_variance:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_8_1x1_increase/weights:0&#39; shape=(1, 1, 256, 1024) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_8_1x1_increase_bn/conv4_8_1x1_increase_bn/gamma:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_8_1x1_increase_bn/conv4_8_1x1_increase_bn/beta:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_8_1x1_increase_bn/conv4_8_1x1_increase_bn/moving_mean:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_8_1x1_increase_bn/conv4_8_1x1_increase_bn/moving_variance:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_9_1x1_reduce/weights:0&#39; shape=(1, 1, 1024, 256) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_9_1x1_reduce_bn/conv4_9_1x1_reduce_bn/gamma:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_9_1x1_reduce_bn/conv4_9_1x1_reduce_bn/beta:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_9_1x1_reduce_bn/conv4_9_1x1_reduce_bn/moving_mean:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_9_1x1_reduce_bn/conv4_9_1x1_reduce_bn/moving_variance:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_9_3x3/weights:0&#39; shape=(3, 3, 256, 256) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_9_3x3_bn/conv4_9_3x3_bn/gamma:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_9_3x3_bn/conv4_9_3x3_bn/beta:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_9_3x3_bn/conv4_9_3x3_bn/moving_mean:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_9_3x3_bn/conv4_9_3x3_bn/moving_variance:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_9_1x1_increase/weights:0&#39; shape=(1, 1, 256, 1024) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_9_1x1_increase_bn/conv4_9_1x1_increase_bn/gamma:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_9_1x1_increase_bn/conv4_9_1x1_increase_bn/beta:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_9_1x1_increase_bn/conv4_9_1x1_increase_bn/moving_mean:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_9_1x1_increase_bn/conv4_9_1x1_increase_bn/moving_variance:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_10_1x1_reduce/weights:0&#39; shape=(1, 1, 1024, 256) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_10_1x1_reduce_bn/conv4_10_1x1_reduce_bn/gamma:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_10_1x1_reduce_bn/conv4_10_1x1_reduce_bn/beta:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_10_1x1_reduce_bn/conv4_10_1x1_reduce_bn/moving_mean:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_10_1x1_reduce_bn/conv4_10_1x1_reduce_bn/moving_variance:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_10_3x3/weights:0&#39; shape=(3, 3, 256, 256) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_10_3x3_bn/conv4_10_3x3_bn/gamma:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_10_3x3_bn/conv4_10_3x3_bn/beta:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_10_3x3_bn/conv4_10_3x3_bn/moving_mean:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_10_3x3_bn/conv4_10_3x3_bn/moving_variance:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_10_1x1_increase/weights:0&#39; shape=(1, 1, 256, 1024) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_10_1x1_increase_bn/conv4_10_1x1_increase_bn/gamma:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_10_1x1_increase_bn/conv4_10_1x1_increase_bn/beta:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_10_1x1_increase_bn/conv4_10_1x1_increase_bn/moving_mean:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_10_1x1_increase_bn/conv4_10_1x1_increase_bn/moving_variance:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_11_1x1_reduce/weights:0&#39; shape=(1, 1, 1024, 256) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_11_1x1_reduce_bn/conv4_11_1x1_reduce_bn/gamma:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_11_1x1_reduce_bn/conv4_11_1x1_reduce_bn/beta:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_11_1x1_reduce_bn/conv4_11_1x1_reduce_bn/moving_mean:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_11_1x1_reduce_bn/conv4_11_1x1_reduce_bn/moving_variance:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_11_3x3/weights:0&#39; shape=(3, 3, 256, 256) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_11_3x3_bn/conv4_11_3x3_bn/gamma:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_11_3x3_bn/conv4_11_3x3_bn/beta:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_11_3x3_bn/conv4_11_3x3_bn/moving_mean:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_11_3x3_bn/conv4_11_3x3_bn/moving_variance:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_11_1x1_increase/weights:0&#39; shape=(1, 1, 256, 1024) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_11_1x1_increase_bn/conv4_11_1x1_increase_bn/gamma:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_11_1x1_increase_bn/conv4_11_1x1_increase_bn/beta:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_11_1x1_increase_bn/conv4_11_1x1_increase_bn/moving_mean:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_11_1x1_increase_bn/conv4_11_1x1_increase_bn/moving_variance:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_12_1x1_reduce/weights:0&#39; shape=(1, 1, 1024, 256) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_12_1x1_reduce_bn/conv4_12_1x1_reduce_bn/gamma:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_12_1x1_reduce_bn/conv4_12_1x1_reduce_bn/beta:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_12_1x1_reduce_bn/conv4_12_1x1_reduce_bn/moving_mean:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_12_1x1_reduce_bn/conv4_12_1x1_reduce_bn/moving_variance:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_12_3x3/weights:0&#39; shape=(3, 3, 256, 256) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_12_3x3_bn/conv4_12_3x3_bn/gamma:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_12_3x3_bn/conv4_12_3x3_bn/beta:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_12_3x3_bn/conv4_12_3x3_bn/moving_mean:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_12_3x3_bn/conv4_12_3x3_bn/moving_variance:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_12_1x1_increase/weights:0&#39; shape=(1, 1, 256, 1024) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_12_1x1_increase_bn/conv4_12_1x1_increase_bn/gamma:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_12_1x1_increase_bn/conv4_12_1x1_increase_bn/beta:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_12_1x1_increase_bn/conv4_12_1x1_increase_bn/moving_mean:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_12_1x1_increase_bn/conv4_12_1x1_increase_bn/moving_variance:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_13_1x1_reduce/weights:0&#39; shape=(1, 1, 1024, 256) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_13_1x1_reduce_bn/conv4_13_1x1_reduce_bn/gamma:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_13_1x1_reduce_bn/conv4_13_1x1_reduce_bn/beta:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_13_1x1_reduce_bn/conv4_13_1x1_reduce_bn/moving_mean:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_13_1x1_reduce_bn/conv4_13_1x1_reduce_bn/moving_variance:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_13_3x3/weights:0&#39; shape=(3, 3, 256, 256) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_13_3x3_bn/conv4_13_3x3_bn/gamma:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_13_3x3_bn/conv4_13_3x3_bn/beta:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_13_3x3_bn/conv4_13_3x3_bn/moving_mean:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_13_3x3_bn/conv4_13_3x3_bn/moving_variance:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_13_1x1_increase/weights:0&#39; shape=(1, 1, 256, 1024) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_13_1x1_increase_bn/conv4_13_1x1_increase_bn/gamma:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_13_1x1_increase_bn/conv4_13_1x1_increase_bn/beta:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_13_1x1_increase_bn/conv4_13_1x1_increase_bn/moving_mean:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_13_1x1_increase_bn/conv4_13_1x1_increase_bn/moving_variance:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_14_1x1_reduce/weights:0&#39; shape=(1, 1, 1024, 256) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_14_1x1_reduce_bn/conv4_14_1x1_reduce_bn/gamma:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_14_1x1_reduce_bn/conv4_14_1x1_reduce_bn/beta:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_14_1x1_reduce_bn/conv4_14_1x1_reduce_bn/moving_mean:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_14_1x1_reduce_bn/conv4_14_1x1_reduce_bn/moving_variance:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_14_3x3/weights:0&#39; shape=(3, 3, 256, 256) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_14_3x3_bn/conv4_14_3x3_bn/gamma:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_14_3x3_bn/conv4_14_3x3_bn/beta:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_14_3x3_bn/conv4_14_3x3_bn/moving_mean:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_14_3x3_bn/conv4_14_3x3_bn/moving_variance:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_14_1x1_increase/weights:0&#39; shape=(1, 1, 256, 1024) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_14_1x1_increase_bn/conv4_14_1x1_increase_bn/gamma:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_14_1x1_increase_bn/conv4_14_1x1_increase_bn/beta:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_14_1x1_increase_bn/conv4_14_1x1_increase_bn/moving_mean:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_14_1x1_increase_bn/conv4_14_1x1_increase_bn/moving_variance:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_15_1x1_reduce/weights:0&#39; shape=(1, 1, 1024, 256) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_15_1x1_reduce_bn/conv4_15_1x1_reduce_bn/gamma:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_15_1x1_reduce_bn/conv4_15_1x1_reduce_bn/beta:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_15_1x1_reduce_bn/conv4_15_1x1_reduce_bn/moving_mean:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_15_1x1_reduce_bn/conv4_15_1x1_reduce_bn/moving_variance:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_15_3x3/weights:0&#39; shape=(3, 3, 256, 256) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_15_3x3_bn/conv4_15_3x3_bn/gamma:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_15_3x3_bn/conv4_15_3x3_bn/beta:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_15_3x3_bn/conv4_15_3x3_bn/moving_mean:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_15_3x3_bn/conv4_15_3x3_bn/moving_variance:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_15_1x1_increase/weights:0&#39; shape=(1, 1, 256, 1024) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_15_1x1_increase_bn/conv4_15_1x1_increase_bn/gamma:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_15_1x1_increase_bn/conv4_15_1x1_increase_bn/beta:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_15_1x1_increase_bn/conv4_15_1x1_increase_bn/moving_mean:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_15_1x1_increase_bn/conv4_15_1x1_increase_bn/moving_variance:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_16_1x1_reduce/weights:0&#39; shape=(1, 1, 1024, 256) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_16_1x1_reduce_bn/conv4_16_1x1_reduce_bn/gamma:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_16_1x1_reduce_bn/conv4_16_1x1_reduce_bn/beta:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_16_1x1_reduce_bn/conv4_16_1x1_reduce_bn/moving_mean:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_16_1x1_reduce_bn/conv4_16_1x1_reduce_bn/moving_variance:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_16_3x3/weights:0&#39; shape=(3, 3, 256, 256) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_16_3x3_bn/conv4_16_3x3_bn/gamma:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_16_3x3_bn/conv4_16_3x3_bn/beta:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_16_3x3_bn/conv4_16_3x3_bn/moving_mean:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_16_3x3_bn/conv4_16_3x3_bn/moving_variance:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_16_1x1_increase/weights:0&#39; shape=(1, 1, 256, 1024) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_16_1x1_increase_bn/conv4_16_1x1_increase_bn/gamma:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_16_1x1_increase_bn/conv4_16_1x1_increase_bn/beta:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_16_1x1_increase_bn/conv4_16_1x1_increase_bn/moving_mean:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_16_1x1_increase_bn/conv4_16_1x1_increase_bn/moving_variance:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_17_1x1_reduce/weights:0&#39; shape=(1, 1, 1024, 256) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_17_1x1_reduce_bn/conv4_17_1x1_reduce_bn/gamma:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_17_1x1_reduce_bn/conv4_17_1x1_reduce_bn/beta:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_17_1x1_reduce_bn/conv4_17_1x1_reduce_bn/moving_mean:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_17_1x1_reduce_bn/conv4_17_1x1_reduce_bn/moving_variance:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_17_3x3/weights:0&#39; shape=(3, 3, 256, 256) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_17_3x3_bn/conv4_17_3x3_bn/gamma:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_17_3x3_bn/conv4_17_3x3_bn/beta:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_17_3x3_bn/conv4_17_3x3_bn/moving_mean:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_17_3x3_bn/conv4_17_3x3_bn/moving_variance:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_17_1x1_increase/weights:0&#39; shape=(1, 1, 256, 1024) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_17_1x1_increase_bn/conv4_17_1x1_increase_bn/gamma:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_17_1x1_increase_bn/conv4_17_1x1_increase_bn/beta:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_17_1x1_increase_bn/conv4_17_1x1_increase_bn/moving_mean:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_17_1x1_increase_bn/conv4_17_1x1_increase_bn/moving_variance:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_18_1x1_reduce/weights:0&#39; shape=(1, 1, 1024, 256) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_18_1x1_reduce_bn/conv4_18_1x1_reduce_bn/gamma:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_18_1x1_reduce_bn/conv4_18_1x1_reduce_bn/beta:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_18_1x1_reduce_bn/conv4_18_1x1_reduce_bn/moving_mean:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_18_1x1_reduce_bn/conv4_18_1x1_reduce_bn/moving_variance:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_18_3x3/weights:0&#39; shape=(3, 3, 256, 256) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_18_3x3_bn/conv4_18_3x3_bn/gamma:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_18_3x3_bn/conv4_18_3x3_bn/beta:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_18_3x3_bn/conv4_18_3x3_bn/moving_mean:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_18_3x3_bn/conv4_18_3x3_bn/moving_variance:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_18_1x1_increase/weights:0&#39; shape=(1, 1, 256, 1024) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_18_1x1_increase_bn/conv4_18_1x1_increase_bn/gamma:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_18_1x1_increase_bn/conv4_18_1x1_increase_bn/beta:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_18_1x1_increase_bn/conv4_18_1x1_increase_bn/moving_mean:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_18_1x1_increase_bn/conv4_18_1x1_increase_bn/moving_variance:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_19_1x1_reduce/weights:0&#39; shape=(1, 1, 1024, 256) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_19_1x1_reduce_bn/conv4_19_1x1_reduce_bn/gamma:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_19_1x1_reduce_bn/conv4_19_1x1_reduce_bn/beta:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_19_1x1_reduce_bn/conv4_19_1x1_reduce_bn/moving_mean:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_19_1x1_reduce_bn/conv4_19_1x1_reduce_bn/moving_variance:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_19_3x3/weights:0&#39; shape=(3, 3, 256, 256) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_19_3x3_bn/conv4_19_3x3_bn/gamma:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_19_3x3_bn/conv4_19_3x3_bn/beta:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_19_3x3_bn/conv4_19_3x3_bn/moving_mean:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_19_3x3_bn/conv4_19_3x3_bn/moving_variance:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_19_1x1_increase/weights:0&#39; shape=(1, 1, 256, 1024) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_19_1x1_increase_bn/conv4_19_1x1_increase_bn/gamma:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_19_1x1_increase_bn/conv4_19_1x1_increase_bn/beta:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_19_1x1_increase_bn/conv4_19_1x1_increase_bn/moving_mean:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_19_1x1_increase_bn/conv4_19_1x1_increase_bn/moving_variance:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_20_1x1_reduce/weights:0&#39; shape=(1, 1, 1024, 256) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_20_1x1_reduce_bn/conv4_20_1x1_reduce_bn/gamma:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_20_1x1_reduce_bn/conv4_20_1x1_reduce_bn/beta:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_20_1x1_reduce_bn/conv4_20_1x1_reduce_bn/moving_mean:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_20_1x1_reduce_bn/conv4_20_1x1_reduce_bn/moving_variance:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_20_3x3/weights:0&#39; shape=(3, 3, 256, 256) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_20_3x3_bn/conv4_20_3x3_bn/gamma:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_20_3x3_bn/conv4_20_3x3_bn/beta:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_20_3x3_bn/conv4_20_3x3_bn/moving_mean:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_20_3x3_bn/conv4_20_3x3_bn/moving_variance:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_20_1x1_increase/weights:0&#39; shape=(1, 1, 256, 1024) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_20_1x1_increase_bn/conv4_20_1x1_increase_bn/gamma:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_20_1x1_increase_bn/conv4_20_1x1_increase_bn/beta:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_20_1x1_increase_bn/conv4_20_1x1_increase_bn/moving_mean:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_20_1x1_increase_bn/conv4_20_1x1_increase_bn/moving_variance:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_21_1x1_reduce/weights:0&#39; shape=(1, 1, 1024, 256) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_21_1x1_reduce_bn/conv4_21_1x1_reduce_bn/gamma:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_21_1x1_reduce_bn/conv4_21_1x1_reduce_bn/beta:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_21_1x1_reduce_bn/conv4_21_1x1_reduce_bn/moving_mean:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_21_1x1_reduce_bn/conv4_21_1x1_reduce_bn/moving_variance:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_21_3x3/weights:0&#39; shape=(3, 3, 256, 256) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_21_3x3_bn/conv4_21_3x3_bn/gamma:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_21_3x3_bn/conv4_21_3x3_bn/beta:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_21_3x3_bn/conv4_21_3x3_bn/moving_mean:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_21_3x3_bn/conv4_21_3x3_bn/moving_variance:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_21_1x1_increase/weights:0&#39; shape=(1, 1, 256, 1024) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_21_1x1_increase_bn/conv4_21_1x1_increase_bn/gamma:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_21_1x1_increase_bn/conv4_21_1x1_increase_bn/beta:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_21_1x1_increase_bn/conv4_21_1x1_increase_bn/moving_mean:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_21_1x1_increase_bn/conv4_21_1x1_increase_bn/moving_variance:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_22_1x1_reduce/weights:0&#39; shape=(1, 1, 1024, 256) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_22_1x1_reduce_bn/conv4_22_1x1_reduce_bn/gamma:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_22_1x1_reduce_bn/conv4_22_1x1_reduce_bn/beta:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_22_1x1_reduce_bn/conv4_22_1x1_reduce_bn/moving_mean:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_22_1x1_reduce_bn/conv4_22_1x1_reduce_bn/moving_variance:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_22_3x3/weights:0&#39; shape=(3, 3, 256, 256) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_22_3x3_bn/conv4_22_3x3_bn/gamma:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_22_3x3_bn/conv4_22_3x3_bn/beta:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_22_3x3_bn/conv4_22_3x3_bn/moving_mean:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_22_3x3_bn/conv4_22_3x3_bn/moving_variance:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_22_1x1_increase/weights:0&#39; shape=(1, 1, 256, 1024) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_22_1x1_increase_bn/conv4_22_1x1_increase_bn/gamma:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_22_1x1_increase_bn/conv4_22_1x1_increase_bn/beta:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_22_1x1_increase_bn/conv4_22_1x1_increase_bn/moving_mean:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_22_1x1_increase_bn/conv4_22_1x1_increase_bn/moving_variance:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_23_1x1_reduce/weights:0&#39; shape=(1, 1, 1024, 256) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_23_1x1_reduce_bn/conv4_23_1x1_reduce_bn/gamma:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_23_1x1_reduce_bn/conv4_23_1x1_reduce_bn/beta:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_23_1x1_reduce_bn/conv4_23_1x1_reduce_bn/moving_mean:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_23_1x1_reduce_bn/conv4_23_1x1_reduce_bn/moving_variance:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_23_3x3/weights:0&#39; shape=(3, 3, 256, 256) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_23_3x3_bn/conv4_23_3x3_bn/gamma:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_23_3x3_bn/conv4_23_3x3_bn/beta:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_23_3x3_bn/conv4_23_3x3_bn/moving_mean:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_23_3x3_bn/conv4_23_3x3_bn/moving_variance:0&#39; shape=(256,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_23_1x1_increase/weights:0&#39; shape=(1, 1, 256, 1024) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_23_1x1_increase_bn/conv4_23_1x1_increase_bn/gamma:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_23_1x1_increase_bn/conv4_23_1x1_increase_bn/beta:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_23_1x1_increase_bn/conv4_23_1x1_increase_bn/moving_mean:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv4_23_1x1_increase_bn/conv4_23_1x1_increase_bn/moving_variance:0&#39; shape=(1024,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_1_1x1_proj/weights:0&#39; shape=(1, 1, 1024, 2048) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_1_1x1_proj_bn/conv5_1_1x1_proj_bn/gamma:0&#39; shape=(2048,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_1_1x1_proj_bn/conv5_1_1x1_proj_bn/beta:0&#39; shape=(2048,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_1_1x1_proj_bn/conv5_1_1x1_proj_bn/moving_mean:0&#39; shape=(2048,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_1_1x1_proj_bn/conv5_1_1x1_proj_bn/moving_variance:0&#39; shape=(2048,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_1_1x1_reduce/weights:0&#39; shape=(1, 1, 1024, 512) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_1_1x1_reduce_bn/conv5_1_1x1_reduce_bn/gamma:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_1_1x1_reduce_bn/conv5_1_1x1_reduce_bn/beta:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_1_1x1_reduce_bn/conv5_1_1x1_reduce_bn/moving_mean:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_1_1x1_reduce_bn/conv5_1_1x1_reduce_bn/moving_variance:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_1_3x3/weights:0&#39; shape=(3, 3, 512, 512) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_1_3x3_bn/conv5_1_3x3_bn/gamma:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_1_3x3_bn/conv5_1_3x3_bn/beta:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_1_3x3_bn/conv5_1_3x3_bn/moving_mean:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_1_3x3_bn/conv5_1_3x3_bn/moving_variance:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_1_1x1_increase/weights:0&#39; shape=(1, 1, 512, 2048) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_1_1x1_increase_bn/conv5_1_1x1_increase_bn/gamma:0&#39; shape=(2048,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_1_1x1_increase_bn/conv5_1_1x1_increase_bn/beta:0&#39; shape=(2048,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_1_1x1_increase_bn/conv5_1_1x1_increase_bn/moving_mean:0&#39; shape=(2048,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_1_1x1_increase_bn/conv5_1_1x1_increase_bn/moving_variance:0&#39; shape=(2048,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_2_1x1_reduce/weights:0&#39; shape=(1, 1, 2048, 512) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_2_1x1_reduce_bn/conv5_2_1x1_reduce_bn/gamma:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_2_1x1_reduce_bn/conv5_2_1x1_reduce_bn/beta:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_2_1x1_reduce_bn/conv5_2_1x1_reduce_bn/moving_mean:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_2_1x1_reduce_bn/conv5_2_1x1_reduce_bn/moving_variance:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_2_3x3/weights:0&#39; shape=(3, 3, 512, 512) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_2_3x3_bn/conv5_2_3x3_bn/gamma:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_2_3x3_bn/conv5_2_3x3_bn/beta:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_2_3x3_bn/conv5_2_3x3_bn/moving_mean:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_2_3x3_bn/conv5_2_3x3_bn/moving_variance:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_2_1x1_increase/weights:0&#39; shape=(1, 1, 512, 2048) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_2_1x1_increase_bn/conv5_2_1x1_increase_bn/gamma:0&#39; shape=(2048,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_2_1x1_increase_bn/conv5_2_1x1_increase_bn/beta:0&#39; shape=(2048,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_2_1x1_increase_bn/conv5_2_1x1_increase_bn/moving_mean:0&#39; shape=(2048,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_2_1x1_increase_bn/conv5_2_1x1_increase_bn/moving_variance:0&#39; shape=(2048,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_3_1x1_reduce/weights:0&#39; shape=(1, 1, 2048, 512) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_3_1x1_reduce_bn/conv5_3_1x1_reduce_bn/gamma:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_3_1x1_reduce_bn/conv5_3_1x1_reduce_bn/beta:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_3_1x1_reduce_bn/conv5_3_1x1_reduce_bn/moving_mean:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_3_1x1_reduce_bn/conv5_3_1x1_reduce_bn/moving_variance:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_3_3x3/weights:0&#39; shape=(3, 3, 512, 512) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_3_3x3_bn/conv5_3_3x3_bn/gamma:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_3_3x3_bn/conv5_3_3x3_bn/beta:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_3_3x3_bn/conv5_3_3x3_bn/moving_mean:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_3_3x3_bn/conv5_3_3x3_bn/moving_variance:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_3_1x1_increase/weights:0&#39; shape=(1, 1, 512, 2048) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_3_1x1_increase_bn/conv5_3_1x1_increase_bn/gamma:0&#39; shape=(2048,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_3_1x1_increase_bn/conv5_3_1x1_increase_bn/beta:0&#39; shape=(2048,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_3_1x1_increase_bn/conv5_3_1x1_increase_bn/moving_mean:0&#39; shape=(2048,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_3_1x1_increase_bn/conv5_3_1x1_increase_bn/moving_variance:0&#39; shape=(2048,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_3_pool1_conv/weights:0&#39; shape=(1, 1, 2048, 512) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_3_pool1_conv_bn/conv5_3_pool1_conv_bn/gamma:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_3_pool1_conv_bn/conv5_3_pool1_conv_bn/beta:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_3_pool1_conv_bn/conv5_3_pool1_conv_bn/moving_mean:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_3_pool1_conv_bn/conv5_3_pool1_conv_bn/moving_variance:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_3_pool2_conv/weights:0&#39; shape=(1, 1, 2048, 512) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_3_pool2_conv_bn/conv5_3_pool2_conv_bn/gamma:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_3_pool2_conv_bn/conv5_3_pool2_conv_bn/beta:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_3_pool2_conv_bn/conv5_3_pool2_conv_bn/moving_mean:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_3_pool2_conv_bn/conv5_3_pool2_conv_bn/moving_variance:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_3_pool3_conv/weights:0&#39; shape=(1, 1, 2048, 512) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_3_pool3_conv_bn/conv5_3_pool3_conv_bn/gamma:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_3_pool3_conv_bn/conv5_3_pool3_conv_bn/beta:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_3_pool3_conv_bn/conv5_3_pool3_conv_bn/moving_mean:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_3_pool3_conv_bn/conv5_3_pool3_conv_bn/moving_variance:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_3_pool6_conv/weights:0&#39; shape=(1, 1, 2048, 512) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_3_pool6_conv_bn/conv5_3_pool6_conv_bn/gamma:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_3_pool6_conv_bn/conv5_3_pool6_conv_bn/beta:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_3_pool6_conv_bn/conv5_3_pool6_conv_bn/moving_mean:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_3_pool6_conv_bn/conv5_3_pool6_conv_bn/moving_variance:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_4/weights:0&#39; shape=(3, 3, 4096, 512) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_4_bn/conv5_4_bn/gamma:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_4_bn/conv5_4_bn/beta:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_4_bn/conv5_4_bn/moving_mean:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv5_4_bn/conv5_4_bn/moving_variance:0&#39; shape=(512,) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv6/weights:0&#39; shape=(1, 1, 512, 5) dtype=float32_ref&gt;\n&lt;tf.Variable &#39;conv6/biases:0&#39; shape=(5,) dtype=float32_ref&gt;</code></pre></div><p></p><p></p><p></p>", 
            "topic": [
                {
                    "tag": "TensorFlow", 
                    "tagLink": "https://api.zhihu.com/topics/20032249"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/31501009", 
            "userName": "休语", 
            "userLink": "https://www.zhihu.com/people/9fa98b02c1f908f202862a38bd15084f", 
            "upvote": 1, 
            "title": "遇到的一些TensorFlow函数速记：tf.squeeze()，tf.one_hot()，tf.less_equal()，tf.where()", 
            "content": "<p><b>tf.squeeze()</b>：把是1的维度压缩掉，如果不指定axis，压缩掉所有是1的维度；如果指定axis，压缩掉指定的axis。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>tf.one_hot()</b>：</p><p><code>one_hot(<br/>    indices,<br/>    depth,<br/>    on_value=None,<br/>    off_value=None,<br/>    axis=None,<br/>    dtype=None,<br/>    name=None</code> <code>)</code> </p><div class=\"highlight\"><pre><code class=\"language-text\">Defined in tensorflow/python/ops/array_ops.py.</code></pre></div><p>See the guide: <a href=\"https://link.zhihu.com/?target=https%3A//tensorflow.google.cn/versions/master/api_guides/python/array_ops%23Slicing_and_Joining\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Tensor Transformations &gt; Slicing and Joining</a></p><p>Returns a one-hot tensor. <b>返回一个one-hot tensor</b></p><p>The locations represented by indices in <code>indices</code> take value <code>on_value</code>, while all other locations take value <code>off_value</code>.</p><p><b>把输入的Tensor当做下标。输入的indices有多少个，就是有多少个样本（或者说有多少行）；depth相当于类别数，也就是每行有多少列。indices的数值就是在每一行的各个列的下标（所以可以有负数）。</b></p><p><b>以下的示例代码来自</b><a href=\"https://link.zhihu.com/?target=http%3A//blog.csdn.net/wenqiwenqi123/article/details/78055740\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">tensorflow--tf.one_hot()函数示例 - wenqiwenqi123的博客 - CSDN博客</a>：</p><p>具体用法以及作用见以下代码：</p><ol><li><b>import</b> numpy as np  </li><li><b>import</b> tensorflow as tf  </li><li>SIZE=6 </li><li>CLASS=8 </li><li>label1=tf.constant([0,1,2,3,4,5,6,7])  </li><li>sess1=tf.Session()  </li><li><b>print</b>(&#39;label1:&#39;,sess1.run(label1))  </li><li>b = tf.one_hot(label1,CLASS,1,0)  </li><li>with tf.Session() as sess:  </li><li>  sess.run(tf.global_variables_initializer())  </li><li>  sess.run(b)  </li><li><b>print</b>(&#39;after one_hot&#39;,sess.run(b))  </li></ol><p class=\"ztext-empty-paragraph\"><br/></p><p>最后的输出为：</p><p class=\"ztext-empty-paragraph\"><br/></p><p>label1: [0 1 2 3 4 5 6 7]<br/>after one_hot:</p><p>[[1 0 0 0 0 0 0 0]<br/> [0 1 0 0 0 0 0 0]<br/> [0 0 1 0 0 0 0 0]<br/> [0 0 0 1 0 0 0 0]<br/> [0 0 0 0 1 0 0 0]<br/> [0 0 0 0 0 1 0 0]<br/> [0 0 0 0 0 0 1 0]<br/> [0 0 0 0 0 0 0 1]]</p><p><code>on_value</code> and <code>off_value</code> must have matching data types. If <code>dtype</code> is also provided, they must be the same data type as specified by <code>dtype</code>.</p><p>on_value和off_value是下标标出的位置的值和下表没标出的位置的值。</p><p>（参考：<a href=\"https://link.zhihu.com/?target=http%3A//blog.csdn.net/wenqiwenqi123/article/details/78055740\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">tensorflow--tf.one_hot()函数示例 - wenqiwenqi123的博客 - CSDN博客</a>）</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>tf.less_equal()</b>:</p><p><code>less_equal(<br/>    x,<br/>    y,<br/>    name=None<br/>)</code></p><p>点对点比较x和y， x &lt;= y返回1，x &gt; y返回0，生成一个和x，y一样大的Tensor</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>tf.where()</b>:</p><p><code>where(<br/>    condition,<br/>    x=None,<br/>    y=None,<br/>    name=None</code> <code>)</code> </p><div class=\"highlight\"><pre><code class=\"language-text\">Defined in tensorflow/python/ops/array_ops.py.</code></pre></div><p>See the guides: <a href=\"https://link.zhihu.com/?target=https%3A//tensorflow.google.cn/api_guides/python/control_flow_ops%23Comparison_Operators\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Control Flow &gt; Comparison Operators</a>, <a href=\"https://link.zhihu.com/?target=https%3A//tensorflow.google.cn/api_guides/python/math_ops%23Sequence_Comparison_and_Indexing\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Math &gt; Sequence Comparison and Indexing</a></p><p>Return the elements, either from <code>x</code> or <code>y</code>, depending on the <code>condition</code>.</p><p>If both <code>x</code> and <code>y</code> are None, then this operation returns the coordinates of true elements of <code>condition</code>. The coordinates are returned in a 2-D tensor where the first dimension (rows) represents the number of true elements, and the second dimension (columns) represents the coordinates of the true elements. Keep in mind, <b>the shape of the output tensor can vary depending on how many true values there are in input</b>. Indices are output in row-major order.</p><p><b>这种情况：</b></p><p><b>返回bool型tensor中为True的位置</b></p><p><b># ‘input’ tensor is </b></p><p><b>#[[True, False]</b></p><p><b>#[True, False]]</b></p><p><b># ‘input’ 有两个’True’,那么输出两个坐标值.</b></p><p><b># ‘input’的rank为2, 所以每个坐标为具有两个维度.</b></p><p><b>where(input) ==&gt;</b></p><p><b>[[0, 0],</b></p><p><b>[1, 0]]</b></p><p><b>2-D tensor每行是一个True的坐标，行数就是True的数量。</b></p><p>If both non-None, <code>x</code> and <code>y</code> must have the same shape. The <code>condition</code> tensor must be a scalar if <code>x</code> and <code>y</code> are scalar. If <code>x</code> and <code>y</code> are vectors of higher rank, then <code>condition</code> must be either a vector with size matching the first dimension of <code>x</code>, or must have the same shape as <code>x</code>.</p><p><b>这种情况下有点像C++里的条件表达式：    (C) ? A: B;     (C)满足时候，执行A,否则执行B。</b></p><p>The <b><code>condition</code> tensor acts as a mask that chooses</b>, based on the value at each element, whether the corresponding element / row in the output should be taken from <code>x</code> (if true) or <code>y</code> (if false).</p><p>If <code>condition</code> is a vector and <code>x</code> and <code>y</code> are higher rank matrices, then it chooses which row (outer dimension) to copy from <code>x</code> and <code>y</code>. If <code>condition</code> has the same shape as <code>x</code> and <code>y</code>, then it chooses which element to copy from <code>x</code> and <code>y</code>.</p><p>Args:</p><ul><li><b><code>condition</code></b>: A <code>Tensor</code> of type <code>bool</code> </li><li><b><code>x</code></b>: A Tensor which may have the same shape as <code>condition</code>. If <code>condition</code> is rank 1, <code>x</code> may have higher rank, but its first dimension must match the size of <code>condition</code>.</li><li><b><code>y</code></b>: A <code>tensor</code> with the same shape and type as <code>x</code>.</li><li><b><code>name</code></b>: A name of the operation (optional)</li></ul><p>Returns:</p><h2>A <code>Tensor</code> with the same type and shape as <code>x</code>, <code>y</code> if they are non-None. A <code>Tensor</code> with shape <code>(num_true, dim_size(condition))</code>.</h2><p>Raises:</p><ul><li><b><code>ValueError</code></b>: When exactly one of <code>x</code> or <code>y</code> is non-None.</li></ul><p><b>tf.squeeze()，tf.less_equal()，tf.where()的联合使用：</b></p><p><b>indices = tf.squeeze(tf.where(tf.less_equal(raw_gt, args.num_classes - 1)), 1)</b></p><p><b>tf.less_equal()生成一个Tensor，把raw_gt里面小于args.num_classes - 1的标记出来（标记为True）</b></p><p><b>tf.where()提出来为True的这个Tensor的下标</b></p><p><b>因为tf.where()得到的结果（强制性地）有两个维度，第二个维度一定是1（因为只有1列，输入的是一个90×90展成的8100的行向量，所以下标只有一个维度），用tf.squeeze()把这个维度压缩掉。</b></p><p>参考资料：</p><p><a href=\"https://link.zhihu.com/?target=http%3A//blog.csdn.net/lenbow/article/details/52152766\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Tensorflow一些常用基本概念与函数（1） - CSDN博客</a></p><p></p>", 
            "topic": [
                {
                    "tag": "TensorFlow", 
                    "tagLink": "https://api.zhihu.com/topics/20032249"
                }
            ], 
            "comments": []
        }
    ], 
    "url": "https://zhuanlan.zhihu.com/c_144895076"
}
