{
    "title": "机器学习", 
    "description": "", 
    "followers": [
        "https://www.zhihu.com/people/real-david", 
        "https://www.zhihu.com/people/spring-wang-33", 
        "https://www.zhihu.com/people/liang-de-peng", 
        "https://www.zhihu.com/people/cang-hai-yue-ming-34-7", 
        "https://www.zhihu.com/people/chen-zhao-wei-16-2", 
        "https://www.zhihu.com/people/chen-bo-86-42", 
        "https://www.zhihu.com/people/edwin-ed", 
        "https://www.zhihu.com/people/song-wen-30", 
        "https://www.zhihu.com/people/yuan-sheng-58", 
        "https://www.zhihu.com/people/wu-qi-mo-50", 
        "https://www.zhihu.com/people/ping-an-yiyu", 
        "https://www.zhihu.com/people/moni-gg", 
        "https://www.zhihu.com/people/you-ling-yang-guang", 
        "https://www.zhihu.com/people/ohno_han", 
        "https://www.zhihu.com/people/tian-yao-54-24", 
        "https://www.zhihu.com/people/buzhidao-37-14", 
        "https://www.zhihu.com/people/yun-he-73-56", 
        "https://www.zhihu.com/people/lan-lan-lan-lan-96-82"
    ], 
    "article": [
        {
            "url": "https://zhuanlan.zhihu.com/p/49742502", 
            "userName": "徽州风韵", 
            "userLink": "https://www.zhihu.com/people/53bd6f8b93873b03c5edb714aea2a456", 
            "upvote": 2, 
            "title": "聚类算法-层次聚类", 
            "content": "<p>层次聚类试图在不同层次对数据集进行划分，从而形成树形的聚类结构，有“自底向上”的聚合策略和“自顶向下”的分拆策略，下面介绍一种聚合策略的聚类算法-AGNES。它先将数据集中的每个样本看作一个初始聚类簇，然后找出距离最近的两个聚类簇进行合并，不断地重复，直至达到预设的聚类簇个数。给定聚类簇 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BC_i%7D%5C%5D\" alt=\"\\[{C_i}\\]\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BC_j%7D%5C%5D\" alt=\"\\[{C_j}\\]\" eeimg=\"1\"/> ，有以下几种计算距离的方法：</p><ul><li>最小距离： <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bd_%7B%5Cmin+%7D%7D%5Cleft%28+%7B%7BC_i%7D%2C%7BC_j%7D%7D+%5Cright%29+%3D+%5Cmathop+%7B%5Cmin+%7D%5Climits_%7Bx+%5Cin+%7BC_i%7D%2Cz+%5Cin+%7BC_j%7D%7D+%7B%5Crm%7Bdist%7D%7D%5Cleft%28+%7Bx%2Cz%7D+%5Cright%29%5C%5D\" alt=\"\\[{d_{\\min }}\\left( {{C_i},{C_j}} \\right) = \\mathop {\\min }\\limits_{x \\in {C_i},z \\in {C_j}} {\\rm{dist}}\\left( {x,z} \\right)\\]\" eeimg=\"1\"/> ；</li><li>最大距离： <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bd_%7B%5Cmax+%7D%7D%5Cleft%28+%7B%7BC_i%7D%2C%7BC_j%7D%7D+%5Cright%29+%3D+%5Cmathop+%7B%5Cmax+%7D%5Climits_%7Bx+%5Cin+%7BC_i%7D%2Cz+%5Cin+%7BC_j%7D%7D+%7B%5Crm%7Bdist%7D%7D%5Cleft%28+%7Bx%2Cz%7D+%5Cright%29%5C%5D\" alt=\"\\[{d_{\\max }}\\left( {{C_i},{C_j}} \\right) = \\mathop {\\max }\\limits_{x \\in {C_i},z \\in {C_j}} {\\rm{dist}}\\left( {x,z} \\right)\\]\" eeimg=\"1\"/> ；</li><li>平均距离： <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bd_%7B%7B%5Crm%7Bavg%7D%7D%7D%7D%5Cleft%28+%7B%7BC_i%7D%2C%7BC_j%7D%7D+%5Cright%29+%3D+%5Cfrac%7B1%7D%7B%7B%5Cleft%7C+%7B%7BC_i%7D%7D+%5Cright%7C%5Cleft%7C+%7B%7BC_j%7D%7D+%5Cright%7C%7D%7D%5Csum%5Climits_%7Bx+%5Cin+%7BC_i%7D%7D+%7B%5Csum%5Climits_%7Bz+%5Cin+%7BC_j%7D%7D+%7B%7B%5Crm%7Bdist%7D%7D%5Cleft%28+%7Bx%2Cz%7D+%5Cright%29%7D+%7D+%5C%5D\" alt=\"\\[{d_{{\\rm{avg}}}}\\left( {{C_i},{C_j}} \\right) = \\frac{1}{{\\left| {{C_i}} \\right|\\left| {{C_j}} \\right|}}\\sum\\limits_{x \\in {C_i}} {\\sum\\limits_{z \\in {C_j}} {{\\rm{dist}}\\left( {x,z} \\right)} } \\]\" eeimg=\"1\"/> </li></ul><p>具体算法如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-88218a3a409f5fb265abf8e0581812ec_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"442\" data-rawheight=\"638\" class=\"origin_image zh-lightbox-thumb\" width=\"442\" data-original=\"https://pic1.zhimg.com/v2-88218a3a409f5fb265abf8e0581812ec_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;442&#39; height=&#39;638&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"442\" data-rawheight=\"638\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"442\" data-original=\"https://pic1.zhimg.com/v2-88218a3a409f5fb265abf8e0581812ec_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-88218a3a409f5fb265abf8e0581812ec_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "聚类算法", 
                    "tagLink": "https://api.zhihu.com/topics/19627785"
                }, 
                {
                    "tag": "聚类", 
                    "tagLink": "https://api.zhihu.com/topics/19590190"
                }, 
                {
                    "tag": "无监督学习", 
                    "tagLink": "https://api.zhihu.com/topics/19590194"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/49565880", 
            "userName": "徽州风韵", 
            "userLink": "https://www.zhihu.com/people/53bd6f8b93873b03c5edb714aea2a456", 
            "upvote": 1, 
            "title": "聚类算法-密度聚类", 
            "content": "<p>“基于密度的聚类”（density-based clustering）从样本密度的角度来考察样本之间的可连续性，并基于可连接样本不断扩展聚类簇以获得最终的聚类结果。下面主要介绍一种经典的密度聚类算法-DBSCAN。它既可以适用于凸样本集，也可以适用于非凸样本集。给定数据集 <img src=\"https://www.zhihu.com/equation?tex=%5C%5BD+%3D+%5Cleft%5C%7B+%7B%7Bx_1%7D%2C%7Bx_2%7D%2C+%5Ccdot++%5Ccdot++%5Ccdot+%2C%7Bx_m%7D%7D+%5Cright%5C%7D%5C%5D\" alt=\"\\[D = \\left\\{ {{x_1},{x_2}, \\cdot  \\cdot  \\cdot ,{x_m}} \\right\\}\\]\" eeimg=\"1\"/> ，先介绍几个概念：</p><ul><li><b><img src=\"https://www.zhihu.com/equation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"/>-领域：</b>对某样本 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bx_j%7D+%5Cin+D%5C%5D\" alt=\"\\[{x_j} \\in D\\]\" eeimg=\"1\"/> ，其 <img src=\"https://www.zhihu.com/equation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"/>-领域包含样本集 <img src=\"https://www.zhihu.com/equation?tex=%5C%5BD%5C%5D\" alt=\"\\[D\\]\" eeimg=\"1\"/> 中与 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bx_j%7D%5C%5D\" alt=\"\\[{x_j}\\]\" eeimg=\"1\"/> 的距离不大于 <img src=\"https://www.zhihu.com/equation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"/> 的样本，即 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BN_%5Cepsilon+%7D%5Cleft%28+%7B%7Bx_j%7D%7D+%5Cright%29+%3D+%5Cleft%5C%7B+%7B%7Bx_i%7D+%5Cin+D%7C%7B%5Crm%7Bdist%7D%7D%5Cleft%28+%7B%7Bx_i%7D%2C%7Bx_j%7D%7D+%5Cright%29++%5Cle+%5Cepsilon+%7D+%5Cright%5C%7D%5C%5D\" alt=\"\\[{N_\\epsilon }\\left( {{x_j}} \\right) = \\left\\{ {{x_i} \\in D|{\\rm{dist}}\\left( {{x_i},{x_j}} \\right)  \\le \\epsilon } \\right\\}\\]\" eeimg=\"1\"/> ；</li><li><b>核心对象：</b>若 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bx_j%7D%5C%5D\" alt=\"\\[{x_j}\\]\" eeimg=\"1\"/> 的 <img src=\"https://www.zhihu.com/equation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"/>-领域至少包含 <img src=\"https://www.zhihu.com/equation?tex=MinPts\" alt=\"MinPts\" eeimg=\"1\"/> 个样本，即 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cleft%7C+%7B%7BN_%5Cvarepsilon+%7D%5Cleft%28+%7B%7Bx_j%7D%7D+%5Cright%29%7D+%5Cright%7C+%5Cge+MinPts%5C%5D\" alt=\"\\[\\left| {{N_\\varepsilon }\\left( {{x_j}} \\right)} \\right| \\ge MinPts\\]\" eeimg=\"1\"/> ，则 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7Bx_j%7D%7D%5C%5D\" alt=\"\\[{{x_j}}\\]\" eeimg=\"1\"/> 是一个核心对象；</li><li><b>密度直达：</b>若 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7Bx_j%7D%7D%5C%5D\" alt=\"\\[{{x_j}}\\]\" eeimg=\"1\"/> 位于 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7Bx_i%7D%7D%5C%5D\" alt=\"\\[{{x_i}}\\]\" eeimg=\"1\"/> 的 <img src=\"https://www.zhihu.com/equation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"/>-领域中，且 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7Bx_i%7D%7D%5C%5D\" alt=\"\\[{{x_i}}\\]\" eeimg=\"1\"/> 是核心对象，则称 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7Bx_j%7D%7D%5C%5D\" alt=\"\\[{{x_j}}\\]\" eeimg=\"1\"/> 由 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7Bx_i%7D%7D%5C%5D\" alt=\"\\[{{x_i}}\\]\" eeimg=\"1\"/> 密度直达；</li><li><b>密度可达：</b>对于 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7Bx_i%7D%7D%5C%5D\" alt=\"\\[{{x_i}}\\]\" eeimg=\"1\"/> 与 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7Bx_j%7D%7D%5C%5D\" alt=\"\\[{{x_j}}\\]\" eeimg=\"1\"/> ，若存在样本序列 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bp_1%7D%2C%7Bp_2%7D%2C+%5Ccdot++%5Ccdot++%5Ccdot+%2C%7Bp_n%7D%5C%5D\" alt=\"\\[{p_1},{p_2}, \\cdot  \\cdot  \\cdot ,{p_n}\\]\" eeimg=\"1\"/> ，其中 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bp_1%7D+%3D+%7Bx_i%7D%2C%7Bp_n%7D+%3D+%7Bx_j%7D%5C%5D\" alt=\"\\[{p_1} = {x_i},{p_n} = {x_j}\\]\" eeimg=\"1\"/> 且 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bp_%7Bi+%2B+1%7D%7D%5C%5D\" alt=\"\\[{p_{i + 1}}\\]\" eeimg=\"1\"/> 由 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bp_%7Bi+%7D%7D%5C%5D\" alt=\"\\[{p_{i }}\\]\" eeimg=\"1\"/> 密度直达，则称 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bx_j%7D%5C%5D\" alt=\"\\[{x_j}\\]\" eeimg=\"1\"/> 由 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bx_i%7D%5C%5D\" alt=\"\\[{x_i}\\]\" eeimg=\"1\"/> 密度可达；</li><li><b>密度相连：</b>对于 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7Bx_i%7D%7D%5C%5D\" alt=\"\\[{{x_i}}\\]\" eeimg=\"1\"/> 与 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7Bx_j%7D%7D%5C%5D\" alt=\"\\[{{x_j}}\\]\" eeimg=\"1\"/>，若存在 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bx_k%7D%5C%5D\" alt=\"\\[{x_k}\\]\" eeimg=\"1\"/> 使得 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7Bx_i%7D%7D%5C%5D\" alt=\"\\[{{x_i}}\\]\" eeimg=\"1\"/> 与 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7Bx_j%7D%7D%5C%5D\" alt=\"\\[{{x_j}}\\]\" eeimg=\"1\"/> 均由 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bx_k%7D%5C%5D\" alt=\"\\[{x_k}\\]\" eeimg=\"1\"/> 密度可达，则称 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7Bx_i%7D%7D%5C%5D\" alt=\"\\[{{x_i}}\\]\" eeimg=\"1\"/> 与 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7Bx_j%7D%7D%5C%5D\" alt=\"\\[{{x_j}}\\]\" eeimg=\"1\"/> 密度相连。</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-97932d09dae312d45cb2759d3919e7d8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"539\" data-rawheight=\"246\" class=\"origin_image zh-lightbox-thumb\" width=\"539\" data-original=\"https://pic1.zhimg.com/v2-97932d09dae312d45cb2759d3919e7d8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;539&#39; height=&#39;246&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"539\" data-rawheight=\"246\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"539\" data-original=\"https://pic1.zhimg.com/v2-97932d09dae312d45cb2759d3919e7d8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-97932d09dae312d45cb2759d3919e7d8_b.jpg\"/></figure><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5BMinPts+%3D+3%5C%5D\" alt=\"\\[MinPts = 3\\]\" eeimg=\"1\"/> ，圆形虚线是 <img src=\"https://www.zhihu.com/equation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"/>-领域， <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bx_1%7D%5C%5D\" alt=\"\\[{x_1}\\]\" eeimg=\"1\"/> 是核心对象， <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bx_2%7D%5C%5D\" alt=\"\\[{x_2}\\]\" eeimg=\"1\"/> 由 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bx_1%7D%5C%5D\" alt=\"\\[{x_1}\\]\" eeimg=\"1\"/> 密度可直达，<img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bx_3%7D%5C%5D\" alt=\"\\[{x_3}\\]\" eeimg=\"1\"/> 由 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bx_1%7D%5C%5D\" alt=\"\\[{x_1}\\]\" eeimg=\"1\"/> 密度可达，<img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bx_3%7D%5C%5D\" alt=\"\\[{x_3}\\]\" eeimg=\"1\"/> 与 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bx_4%7D%5C%5D\" alt=\"\\[{x_4}\\]\" eeimg=\"1\"/> 密度相连。</p><p>DBSCAN 的具体算法如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-e0b1c8ae4c302426bd3a7e4cdd8faebe_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"628\" data-rawheight=\"696\" class=\"origin_image zh-lightbox-thumb\" width=\"628\" data-original=\"https://pic3.zhimg.com/v2-e0b1c8ae4c302426bd3a7e4cdd8faebe_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;628&#39; height=&#39;696&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"628\" data-rawheight=\"696\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"628\" data-original=\"https://pic3.zhimg.com/v2-e0b1c8ae4c302426bd3a7e4cdd8faebe_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-e0b1c8ae4c302426bd3a7e4cdd8faebe_b.jpg\"/></figure><ul><li><b>1~7 行：</b>根据领域参数 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cleft%28+%7B%5Cepsilon+%2CMinPts%7D+%5Cright%29%5C%5D\" alt=\"\\[\\left( {\\epsilon ,MinPts} \\right)\\]\" eeimg=\"1\"/> 找出所有核心对象；</li><li><b>10~24 行：</b>从任一核心对象为出发点，找出由其密度可达的样本，生成聚类簇，直到所有核心对象均被访问过为止；</li><li>一些异常样本点或者说少量游离于簇外的样本点，这些点不在任何一个核心对象在周围，在DBSCAN 中，我们一般将这些样本点标记为噪音点；</li></ul><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"kn\">from</span> <span class=\"nn\">sklearn.cluster</span> <span class=\"k\">import</span> <span class=\"n\">DBSCAN</span>\n<span class=\"n\">y_pred</span> <span class=\"o\">=</span> <span class=\"n\">DBSCAN</span><span class=\"p\">(</span><span class=\"n\">eps</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">fit_predict</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">scatter</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">[:,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">X</span><span class=\"p\">[:,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">c</span><span class=\"o\">=</span><span class=\"n\">y_pred</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span></code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-8d5db3d32360ccf6aaf802510a5a2275_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"375\" data-rawheight=\"239\" class=\"content_image\" width=\"375\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;375&#39; height=&#39;239&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"375\" data-rawheight=\"239\" class=\"content_image lazy\" width=\"375\" data-actualsrc=\"https://pic2.zhimg.com/v2-8d5db3d32360ccf6aaf802510a5a2275_b.jpg\"/></figure><p>图中紫色的点属于噪音点。</p>", 
            "topic": [
                {
                    "tag": "聚类算法", 
                    "tagLink": "https://api.zhihu.com/topics/19627785"
                }, 
                {
                    "tag": "聚类", 
                    "tagLink": "https://api.zhihu.com/topics/19590190"
                }, 
                {
                    "tag": "无监督学习", 
                    "tagLink": "https://api.zhihu.com/topics/19590194"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/49035254", 
            "userName": "徽州风韵", 
            "userLink": "https://www.zhihu.com/people/53bd6f8b93873b03c5edb714aea2a456", 
            "upvote": 0, 
            "title": "聚类算法-原型聚类", 
            "content": "<h2><b>一、KMeans</b></h2><p>样本集合 <img src=\"https://www.zhihu.com/equation?tex=%5C%5BD+%3D+%5Cleft%5C%7B+%7B%7Bx_1%7D%2C%7Bx_2%7D%2C+%5Ccdot++%5Ccdot++%5Ccdot+%2C%7Bx_m%7D%7D+%5Cright%5C%7D%5C%5D\" alt=\"\\[D = \\left\\{ {{x_1},{x_2}, \\cdot  \\cdot  \\cdot ,{x_m}} \\right\\}\\]\" eeimg=\"1\"/> ，将该集合划分为 <img src=\"https://www.zhihu.com/equation?tex=k\" alt=\"k\" eeimg=\"1\"/> 个簇 <img src=\"https://www.zhihu.com/equation?tex=%5C%5BC+%3D+%5Cleft%5C%7B+%7B%7BC_1%7D%2C%7BC_2%7D%2C+%5Ccdot++%5Ccdot++%5Ccdot+%2C%7BC_k%7D%7D+%5Cright%5C%7D%5C%5D\" alt=\"\\[C = \\left\\{ {{C_1},{C_2}, \\cdot  \\cdot  \\cdot ,{C_k}} \\right\\}\\]\" eeimg=\"1\"/> ，希望簇内的点足够的近，我们的目标是最小化平方误差：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5BE+%3D+%5Csum%5Climits_%7Bi+%3D+1%7D%5Ek+%7B%5Csum%5Climits_%7Bx+%5Cin+%7BC_i%7D%7D+%7B%5Cleft%5C%7C+%7Bx+-+%7B%5Cmu+_i%7D%7D+%5Cright%5C%7C_2%5E2%7D+%7D+%5Ctag%7B1%7D%5C%5D\" alt=\"\\[E = \\sum\\limits_{i = 1}^k {\\sum\\limits_{x \\in {C_i}} {\\left\\| {x - {\\mu _i}} \\right\\|_2^2} } \\tag{1}\\]\" eeimg=\"1\"/></p><p>其中 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Cmu+_i%7D+%3D+%5Cfrac%7B1%7D%7B%7B%5Cleft%7C+%7B%7BC_i%7D%7D+%5Cright%7C%7D%7D%5Csum%5Climits_%7Bx+%5Cin+%7BC_i%7D%7D+x+%5C%5D\" alt=\"\\[{\\mu _i} = \\frac{1}{{\\left| {{C_i}} \\right|}}\\sum\\limits_{x \\in {C_i}} x \\]\" eeimg=\"1\"/> 是簇 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7BC_i%7D%7D%5C%5D\" alt=\"\\[{{C_i}}\\]\" eeimg=\"1\"/> 的均值向量， <img src=\"https://www.zhihu.com/equation?tex=E\" alt=\"E\" eeimg=\"1\"/> 越小则簇内的样本越相似。直接最小化公式 (1) 很难，是一个NP问题。但是我们可以采用贪心算法，通过迭代近似地求解公式 (1) 。具体算法如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-722ec08da17c4f65ac95a7330b46c2a3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"645\" data-rawheight=\"497\" class=\"origin_image zh-lightbox-thumb\" width=\"645\" data-original=\"https://pic4.zhimg.com/v2-722ec08da17c4f65ac95a7330b46c2a3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;645&#39; height=&#39;497&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"645\" data-rawheight=\"497\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"645\" data-original=\"https://pic4.zhimg.com/v2-722ec08da17c4f65ac95a7330b46c2a3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-722ec08da17c4f65ac95a7330b46c2a3_b.jpg\"/></figure><p>下面通过两个例子直观地感受下 K-Means 算法。</p><p>1）产生 1000 个二维样本集合</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n<span class=\"kn\">import</span> <span class=\"nn\">matplotlib.pyplot</span> <span class=\"k\">as</span> <span class=\"nn\">plt</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.datasets.samples_generator</span> <span class=\"k\">import</span> <span class=\"n\">make_blobs</span>\n<span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">make_blobs</span><span class=\"p\">(</span><span class=\"n\">n_samples</span><span class=\"o\">=</span><span class=\"mi\">1000</span><span class=\"p\">,</span> <span class=\"n\">n_features</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">centers</span><span class=\"o\">=</span><span class=\"p\">[[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">,</span><span class=\"mi\">2</span><span class=\"p\">]],</span> <span class=\"n\">cluster_std</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mf\">0.4</span><span class=\"p\">,</span> <span class=\"mf\">0.2</span><span class=\"p\">,</span> <span class=\"mf\">0.2</span><span class=\"p\">,</span> <span class=\"mf\">0.2</span><span class=\"p\">],</span> \n                  <span class=\"n\">random_state</span> <span class=\"o\">=</span><span class=\"mi\">9</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">scatter</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">[:,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">X</span><span class=\"p\">[:,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">marker</span><span class=\"o\">=</span><span class=\"s1\">&#39;o&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span></code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-745f17ad9f47d6dcd5334c6621d8eba9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"366\" data-rawheight=\"245\" class=\"content_image\" width=\"366\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;366&#39; height=&#39;245&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"366\" data-rawheight=\"245\" class=\"content_image lazy\" width=\"366\" data-actualsrc=\"https://pic2.zhimg.com/v2-745f17ad9f47d6dcd5334c6621d8eba9_b.jpg\"/></figure><p>采用 sklearn 中的 K-Means 算法进行聚类：</p><div class=\"highlight\"><pre><code class=\"language-text\">from sklearn.cluster import KMeans\ny_pred = KMeans(n_clusters=4, random_state=9).fit_predict(X)\nplt.scatter(X[:, 0], X[:, 1], c=y_pred)\nplt.show()</code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-0f4a4e9297a74c09d25fc662dd7f357d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"367\" data-rawheight=\"246\" class=\"content_image\" width=\"367\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;367&#39; height=&#39;246&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"367\" data-rawheight=\"246\" class=\"content_image lazy\" width=\"367\" data-actualsrc=\"https://pic2.zhimg.com/v2-0f4a4e9297a74c09d25fc662dd7f357d_b.jpg\"/></figure><p>可以很好地将数据分为 4 个类</p><h2><b>二、KernelKMeans</b></h2><p>生成 1000 个二维非凸数据集。</p><div class=\"highlight\"><pre><code class=\"language-text\">from sklearn.datasets import make_circles\nX,y=make_circles(n_samples=1000,factor=0.3,noise=0.1)\nplt.scatter(X[:, 0], X[:, 1], c=y)\nplt.show()</code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-8fbb11af150ecb257f9fd69bf907556f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"380\" data-rawheight=\"247\" class=\"content_image\" width=\"380\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;380&#39; height=&#39;247&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"380\" data-rawheight=\"247\" class=\"content_image lazy\" width=\"380\" data-actualsrc=\"https://pic4.zhimg.com/v2-8fbb11af150ecb257f9fd69bf907556f_b.jpg\"/></figure><p>同样采用 sklearn 中的 K-Means 算法进行聚类：</p><div class=\"highlight\"><pre><code class=\"language-text\">from sklearn.cluster import KMeans\ny_pred = KMeans(n_clusters=2, random_state=9).fit_predict(X)\nplt.scatter(X[:, 0], X[:, 1], c=y_pred)\nplt.show()</code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-d1eace8d185e4436048e8851fa6b63ae_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"378\" data-rawheight=\"249\" class=\"content_image\" width=\"378\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;378&#39; height=&#39;249&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"378\" data-rawheight=\"249\" class=\"content_image lazy\" width=\"378\" data-actualsrc=\"https://pic3.zhimg.com/v2-d1eace8d185e4436048e8851fa6b63ae_b.jpg\"/></figure><p>可以直观地看出 K-Means 不能很好地解决线性不可分数据的聚类问题。对于这类问题，我们可以采用核技术将数据映射到另外一个线性可分空间。</p><p>非线性核函数 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cphi+%5C%5D\" alt=\"\\[\\phi \\]\" eeimg=\"1\"/> ， <img src=\"https://www.zhihu.com/equation?tex=%5C%5BK%5Cleft%28+%7Bx%2Cy%7D+%5Cright%29+%3D+%5Cphi+%7B%5Cleft%28+x+%5Cright%29%5ET%7D%5Cphi+%5Cleft%28+y+%5Cright%29%5C%5D\" alt=\"\\[K\\left( {x,y} \\right) = \\phi {\\left( x \\right)^T}\\phi \\left( y \\right)\\]\" eeimg=\"1\"/> 公式 (1) 变为：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5BE+%3D+%5Csum%5Climits_%7Bi+%3D+1%7D%5Ek+%7B%5Csum%5Climits_%7Bx+%5Cin+%7BC_i%7D%7D+%7Bw%5Cleft%28+x+%5Cright%29%7B%7B%5Cleft%5C%7C+%7B%5Cphi+%5Cleft%28+x+%5Cright%29+-+%7Bm_i%7D%7D+%5Cright%5C%7C%7D%5E2%7D%7D+%7D+%5Ctag%7B2%7D%5C%5D\" alt=\"\\[E = \\sum\\limits_{i = 1}^k {\\sum\\limits_{x \\in {C_i}} {w\\left( x \\right){{\\left\\| {\\phi \\left( x \\right) - {m_i}} \\right\\|}^2}} } \\tag{2}\\]\" eeimg=\"1\"/> </p><p>其中 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bm_i%7D+%3D+%5Cfrac%7B%7B%5Csum%5Cnolimits_%7Bx+%5Cin+%7BC_i%7D%7D+%7Bw%5Cleft%28+x+%5Cright%29%5Cphi+%5Cleft%28+x+%5Cright%29%7D+%7D%7D%7B%7B%5Csum%5Cnolimits_%7Bx+%5Cin+%7BC_i%7D%7D+%7Bw%5Cleft%28+x+%5Cright%29%7D+%7D%7D%5C%5D\" alt=\"\\[{m_i} = \\frac{{\\sum\\nolimits_{x \\in {C_i}} {w\\left( x \\right)\\phi \\left( x \\right)} }}{{\\sum\\nolimits_{x \\in {C_i}} {w\\left( x \\right)} }}\\]\" eeimg=\"1\"/> ， <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bw%5Cleft%28+x+%5Cright%29%7D%5C%5D\" alt=\"\\[{w\\left( x \\right)}\\]\" eeimg=\"1\"/> 为每个样本的权重。下面我们计算样本到第 <img src=\"https://www.zhihu.com/equation?tex=i\" alt=\"i\" eeimg=\"1\"/> 个中心点的欧式距离：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D++%26%7B%5Cleft%5C%7C+%7B%5Cphi+%5Cleft%28+x+%5Cright%29+-+%7Bm_i%7D%7D+%5Cright%5C%7C%5E2%7D%5C%5C++%26%3D+%7B%5Cleft%5C%7C+%7B%5Cphi+%5Cleft%28+x+%5Cright%29+-+%5Cfrac%7B%7B%5Csum%5Cnolimits_%7By+%5Cin+%7BC_i%7D%7D+%7Bw%5Cleft%28+y+%5Cright%29%5Cphi+%5Cleft%28+y+%5Cright%29%7D+%7D%7D%7B%7B%5Csum%5Cnolimits_%7By+%5Cin+%7BC_i%7D%7D+%7Bw%5Cleft%28+y+%5Cright%29%7D+%7D%7D%7D+%5Cright%5C%7C%5E2%7D%5C%5C++%26%3D+%5Cphi+%7B%5Cleft%28+x+%5Cright%29%5ET%7D%5Cphi+%5Cleft%28+x+%5Cright%29+-+2%5Cfrac%7B%7B%5Csum%5Cnolimits_%7By+%5Cin+%7BC_i%7D%7D+%7Bw%5Cleft%28+y+%5Cright%29%5Cphi+%7B%7B%5Cleft%28+x+%5Cright%29%7D%5ET%7D%5Cphi+%5Cleft%28+y+%5Cright%29%7D+%7D%7D%7B%7B%5Csum%5Cnolimits_%7By+%5Cin+%7BC_i%7D%7D+%7Bw%5Cleft%28+y+%5Cright%29%7D+%7D%7D+%2B+%5Cfrac%7B%7B%5Csum%5Cnolimits_%7Bz%2Cy+%5Cin+%7BC_i%7D%7D+%7Bw%5Cleft%28+y+%5Cright%29w%5Cleft%28+z+%5Cright%29%5Cphi+%7B%7B%5Cleft%28+z+%5Cright%29%7D%5ET%7D%5Cphi+%5Cleft%28+y+%5Cright%29%7D+%7D%7D%7B%7B%7B%7B%5Cleft%28+%7B%5Csum%5Cnolimits_%7By+%5Cin+%7BC_i%7D%7D+%7Bw%5Cleft%28+y+%5Cright%29%7D+%7D+%5Cright%29%7D%5E2%7D%7D%7D%5C%5C++%26%3D+K%5Cleft%28+%7Bx%2Cx%7D+%5Cright%29+-+2%5Cfrac%7B%7B%5Csum%5Cnolimits_%7By+%5Cin+%7BC_i%7D%7D+%7Bw%5Cleft%28+y+%5Cright%29K%5Cleft%28+%7Bx%2Cy%7D+%5Cright%29%7D+%7D%7D%7B%7B%5Csum%5Cnolimits_%7By+%5Cin+%7BC_i%7D%7D+%7Bw%5Cleft%28+y+%5Cright%29%7D+%7D%7D+%2B+%5Cfrac%7B%7B%5Csum%5Cnolimits_%7Bz%2Cy+%5Cin+%7BC_i%7D%7D+%7Bw%5Cleft%28+y+%5Cright%29w%5Cleft%28+z+%5Cright%29K%5Cleft%28+%7Bz%2Cy%7D+%5Cright%29%7D+%7D%7D%7B%7B%7B%7B%5Cleft%28+%7B%5Csum%5Cnolimits_%7By+%5Cin+%7BC_i%7D%7D+%7Bw%5Cleft%28+y+%5Cright%29%7D+%7D+%5Cright%29%7D%5E2%7D%7D%7D++%5Ctag%7B3%7D+%5Cend%7Balign%7D\" alt=\"\\begin{align}  &amp;{\\left\\| {\\phi \\left( x \\right) - {m_i}} \\right\\|^2}\\\\  &amp;= {\\left\\| {\\phi \\left( x \\right) - \\frac{{\\sum\\nolimits_{y \\in {C_i}} {w\\left( y \\right)\\phi \\left( y \\right)} }}{{\\sum\\nolimits_{y \\in {C_i}} {w\\left( y \\right)} }}} \\right\\|^2}\\\\  &amp;= \\phi {\\left( x \\right)^T}\\phi \\left( x \\right) - 2\\frac{{\\sum\\nolimits_{y \\in {C_i}} {w\\left( y \\right)\\phi {{\\left( x \\right)}^T}\\phi \\left( y \\right)} }}{{\\sum\\nolimits_{y \\in {C_i}} {w\\left( y \\right)} }} + \\frac{{\\sum\\nolimits_{z,y \\in {C_i}} {w\\left( y \\right)w\\left( z \\right)\\phi {{\\left( z \\right)}^T}\\phi \\left( y \\right)} }}{{{{\\left( {\\sum\\nolimits_{y \\in {C_i}} {w\\left( y \\right)} } \\right)}^2}}}\\\\  &amp;= K\\left( {x,x} \\right) - 2\\frac{{\\sum\\nolimits_{y \\in {C_i}} {w\\left( y \\right)K\\left( {x,y} \\right)} }}{{\\sum\\nolimits_{y \\in {C_i}} {w\\left( y \\right)} }} + \\frac{{\\sum\\nolimits_{z,y \\in {C_i}} {w\\left( y \\right)w\\left( z \\right)K\\left( {z,y} \\right)} }}{{{{\\left( {\\sum\\nolimits_{y \\in {C_i}} {w\\left( y \\right)} } \\right)}^2}}}  \\tag{3} \\end{align}\" eeimg=\"1\"/> </p><p>具体算法如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-5b70879219d1b033eb5eac3ac517b3b2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"623\" data-rawheight=\"406\" class=\"origin_image zh-lightbox-thumb\" width=\"623\" data-original=\"https://pic3.zhimg.com/v2-5b70879219d1b033eb5eac3ac517b3b2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;623&#39; height=&#39;406&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"623\" data-rawheight=\"406\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"623\" data-original=\"https://pic3.zhimg.com/v2-5b70879219d1b033eb5eac3ac517b3b2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-5b70879219d1b033eb5eac3ac517b3b2_b.jpg\"/></figure><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"kn\">from</span> <span class=\"nn\">sklearn.base</span> <span class=\"k\">import</span> <span class=\"n\">BaseEstimator</span><span class=\"p\">,</span> <span class=\"n\">ClusterMixin</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.metrics.pairwise</span> <span class=\"k\">import</span> <span class=\"n\">pairwise_kernels</span>\n<span class=\"kn\">from</span> <span class=\"nn\">sklearn.utils</span> <span class=\"k\">import</span> <span class=\"n\">check_random_state</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">compute_dist</span><span class=\"p\">(</span><span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">dist</span><span class=\"p\">,</span> <span class=\"n\">labels</span><span class=\"p\">,</span> <span class=\"n\">n_clusters</span><span class=\"p\">,</span> <span class=\"n\">sw</span><span class=\"p\">):</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">n_clusters</span><span class=\"p\">):</span>\n        <span class=\"n\">mask</span> <span class=\"o\">=</span> <span class=\"n\">labels</span> <span class=\"o\">==</span> <span class=\"n\">i</span>\n        <span class=\"k\">if</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">mask</span><span class=\"p\">)</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n            <span class=\"k\">raise</span> <span class=\"ne\">ValueError</span><span class=\"p\">(</span><span class=\"s2\">&#34;Empty cluster found, try smaller n_cluster.&#34;</span><span class=\"p\">)</span>\n        <span class=\"n\">denom</span> <span class=\"o\">=</span> <span class=\"n\">sw</span><span class=\"p\">[</span><span class=\"n\">mask</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">()</span>\n        <span class=\"n\">denomsq</span> <span class=\"o\">=</span> <span class=\"n\">denom</span> <span class=\"o\">*</span> <span class=\"n\">denom</span>\n        <span class=\"n\">KK</span> <span class=\"o\">=</span> <span class=\"n\">K</span><span class=\"p\">[</span><span class=\"n\">mask</span><span class=\"p\">][:,</span> <span class=\"n\">mask</span><span class=\"p\">]</span>\n        <span class=\"n\">dist</span><span class=\"p\">[:,</span> <span class=\"n\">i</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">outer</span><span class=\"p\">(</span><span class=\"n\">sw</span><span class=\"p\">[</span><span class=\"n\">mask</span><span class=\"p\">],</span> <span class=\"n\">sw</span><span class=\"p\">[</span><span class=\"n\">mask</span><span class=\"p\">])</span> <span class=\"o\">*</span> <span class=\"n\">KK</span> <span class=\"o\">/</span> <span class=\"n\">denomsq</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"mi\">2</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">sw</span><span class=\"p\">[</span><span class=\"n\">mask</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">K</span><span class=\"p\">[:,</span> <span class=\"n\">mask</span><span class=\"p\">],</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">denom</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">KernelKMeans</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">n_clusters</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">max_iter</span><span class=\"o\">=</span><span class=\"mi\">50</span><span class=\"p\">,</span> <span class=\"n\">gamma</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">degree</span><span class=\"o\">=</span><span class=\"mi\">3</span><span class=\"p\">,</span><span class=\"n\">coef0</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">):</span>\n    <span class=\"n\">K</span> <span class=\"o\">=</span> <span class=\"n\">pairwise_kernels</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">metric</span><span class=\"o\">=</span><span class=\"s2\">&#34;rbf&#34;</span><span class=\"p\">,</span><span class=\"n\">gamma</span><span class=\"o\">=</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n    <span class=\"n\">n_samples</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n    \n    <span class=\"n\">sw</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">ones</span><span class=\"p\">(</span><span class=\"n\">n_samples</span><span class=\"p\">)</span>\n    <span class=\"n\">rs</span> <span class=\"o\">=</span> <span class=\"n\">check_random_state</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n    <span class=\"n\">labels</span> <span class=\"o\">=</span> <span class=\"n\">rs</span><span class=\"o\">.</span><span class=\"n\">randint</span><span class=\"p\">(</span><span class=\"n\">n_clusters</span><span class=\"p\">,</span> <span class=\"n\">size</span><span class=\"o\">=</span><span class=\"n\">n_samples</span><span class=\"p\">)</span>\n    <span class=\"n\">dist</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">n_samples</span><span class=\"p\">,</span> <span class=\"n\">n_clusters</span><span class=\"p\">))</span>\n    \n    <span class=\"k\">for</span> <span class=\"n\">it</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">max_iter</span><span class=\"p\">):</span>\n        <span class=\"n\">dist</span><span class=\"o\">.</span><span class=\"n\">fill</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n        <span class=\"n\">compute_dist</span><span class=\"p\">(</span><span class=\"n\">K</span><span class=\"p\">,</span> <span class=\"n\">dist</span><span class=\"p\">,</span> <span class=\"n\">labels</span><span class=\"p\">,</span> <span class=\"n\">n_clusters</span><span class=\"p\">,</span> <span class=\"n\">sw</span><span class=\"p\">)</span>\n        <span class=\"n\">labels</span> <span class=\"o\">=</span> <span class=\"n\">dist</span><span class=\"o\">.</span><span class=\"n\">argmin</span><span class=\"p\">(</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">labels</span>\n\n<span class=\"n\">y_pred</span> <span class=\"o\">=</span> <span class=\"n\">KernelKMeans</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">scatter</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">[:,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">X</span><span class=\"p\">[:,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">c</span><span class=\"o\">=</span><span class=\"n\">y_pred</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span> </code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-e4678803e93e89979f17d468aeab7c2c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"379\" data-rawheight=\"247\" class=\"content_image\" width=\"379\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;379&#39; height=&#39;247&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"379\" data-rawheight=\"247\" class=\"content_image lazy\" width=\"379\" data-actualsrc=\"https://pic1.zhimg.com/v2-e4678803e93e89979f17d468aeab7c2c_b.jpg\"/></figure><h2><b>三、学习向量量化(LVQ)</b></h2><p>与前面介绍的 KMeans 算法不同的是：LVQ 样本是带有类别标记的，用来辅助聚类。给定样本集 <img src=\"https://www.zhihu.com/equation?tex=%5C%5BD+%3D+%5Cleft%5C%7B+%7B%5Cleft%28+%7B%7Bx_1%7D%2C%7By_1%7D%7D+%5Cright%29%2C%5Cleft%28+%7B%7Bx_2%7D%2C%7By_2%7D%7D+%5Cright%29%2C+%5Ccdot++%5Ccdot++%5Ccdot+%2C%5Cleft%28+%7B%7Bx_m%7D%2C%7By_m%7D%7D+%5Cright%29%7D+%5Cright%5C%7D%5C%5D\" alt=\"\\[D = \\left\\{ {\\left( {{x_1},{y_1}} \\right),\\left( {{x_2},{y_2}} \\right), \\cdot  \\cdot  \\cdot ,\\left( {{x_m},{y_m}} \\right)} \\right\\}\\]\" eeimg=\"1\"/> ，其中 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bx_i%7D+%5Cin+%7BR%5En%7D%5C%5D\" alt=\"\\[{x_i} \\in {R^n}\\]\" eeimg=\"1\"/> 表示样本 <img src=\"https://www.zhihu.com/equation?tex=i\" alt=\"i\" eeimg=\"1\"/> 的 <img src=\"https://www.zhihu.com/equation?tex=n\" alt=\"n\" eeimg=\"1\"/> 维特征向量， <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7By_i%7D+%5Cin+Y%5C%5D\" alt=\"\\[{y_i} \\in Y\\]\" eeimg=\"1\"/> 表示样本 <img src=\"https://www.zhihu.com/equation?tex=i\" alt=\"i\" eeimg=\"1\"/> 的标记，LVQ的目标是学习一组原型向量 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cleft%5C%7B+%7B%7Bp_1%7D%2C%7Bp_2%7D%2C+%5Ccdot++%5Ccdot++%5Ccdot+%2C%7Bp_q%7D%7D+%5Cright%5C%7D%5C%5D\" alt=\"\\[\\left\\{ {{p_1},{p_2}, \\cdot  \\cdot  \\cdot ,{p_q}} \\right\\}\\]\" eeimg=\"1\"/> ， <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bp_j%7D%5C%5D\" alt=\"\\[{p_j}\\]\" eeimg=\"1\"/> 表示第 <img src=\"https://www.zhihu.com/equation?tex=j\" alt=\"j\" eeimg=\"1\"/> 个聚类簇，簇标记 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bt_j%7D+%5Cin+Y%5C%5D\" alt=\"\\[{t_j} \\in Y\\]\" eeimg=\"1\"/> 。具体算法如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-93fa6b8ade194b1b8b1ee7b528c8bd2e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"516\" data-rawheight=\"369\" class=\"origin_image zh-lightbox-thumb\" width=\"516\" data-original=\"https://pic3.zhimg.com/v2-93fa6b8ade194b1b8b1ee7b528c8bd2e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;516&#39; height=&#39;369&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"516\" data-rawheight=\"369\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"516\" data-original=\"https://pic3.zhimg.com/v2-93fa6b8ade194b1b8b1ee7b528c8bd2e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-93fa6b8ade194b1b8b1ee7b528c8bd2e_b.jpg\"/></figure><ul><li>初始化：对于第 <img src=\"https://www.zhihu.com/equation?tex=j\" alt=\"j\" eeimg=\"1\"/> 个簇，从类别标记为 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bt_j%7D%5C%5D\" alt=\"\\[{t_j}\\]\" eeimg=\"1\"/> 的样本中随机选取一个样本作为 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bp_j%7D%5C%5D\" alt=\"\\[{p_j}\\]\" eeimg=\"1\"/> 初始值；</li><li>第 6~10 行：若 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bx_j%7D%5C%5D\" alt=\"\\[{x_j}\\]\" eeimg=\"1\"/> 与最近的原型向量 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bp_%7B%7Bi%5E+%2A+%7D%7D%7D%5C%5D\" alt=\"\\[{p_{{i^ * }}}\\]\" eeimg=\"1\"/> 的标记相同，则令 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bp_%7B%7Bi%5E+%2A+%7D%7D%7D%5C%5D\" alt=\"\\[{p_{{i^ * }}}\\]\" eeimg=\"1\"/> 向 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bx_j%7D%5C%5D\" alt=\"\\[{x_j}\\]\" eeimg=\"1\"/> 靠拢，否则远离 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bx_j%7D%5C%5D\" alt=\"\\[{x_j}\\]\" eeimg=\"1\"/> </li></ul><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cleft%5C%7B+%7B%5Cbegin%7Barray%7D%7B%2A%7B20%7D%7Bc%7D%7D+%7B%7B%7B%5Cleft%5C%7C+%7Bp%27+-+%7Bx_j%7D%7D+%5Cright%5C%7C%7D_2%7D+%3D+%7B%7B%5Cleft%5C%7C+%7B%7Bp_%7B%7Bi%5E+%2A+%7D%7D%7D+%2B+%5Ceta+%5Cleft%28+%7B%7Bx_j%7D+-+%7Bp_%7B%7Bi%5E+%2A+%7D%7D%7D%7D+%5Cright%29+-+%7Bx_j%7D%7D+%5Cright%5C%7C%7D_2%7D+%3D+%5Cleft%28+%7B1+-+%5Ceta+%7D+%5Cright%29%7B%7B%5Cleft%5C%7C+%7B%7Bp_%7B%7Bi%5E+%2A+%7D%7D%7D+-+%7Bx_j%7D%7D+%5Cright%5C%7C%7D_2%7D%5C+%5C+if%5C+%5C+%7By_j%7D+%3D+%7Bp_%7B%7Bi%5E+%2A+%7D%7D%7D%7D%5C%5C+%7B%7B%7B%5Cleft%5C%7C+%7Bp%27+-+%7Bx_j%7D%7D+%5Cright%5C%7C%7D_2%7D+%3D+%7B%7B%5Cleft%5C%7C+%7B%7Bp_%7B%7Bi%5E+%2A+%7D%7D%7D+-+%5Ceta+%5Cleft%28+%7B%7Bx_j%7D+-+%7Bp_%7B%7Bi%5E+%2A+%7D%7D%7D%7D+%5Cright%29+-+%7Bx_j%7D%7D+%5Cright%5C%7C%7D_2%7D+%3D+%5Cleft%28+%7B1+%2B+%5Ceta+%7D+%5Cright%29%7B%7B%5Cleft%5C%7C+%7B%7Bp_%7B%7Bi%5E+%2A+%7D%7D%7D+-+%7Bx_j%7D%7D+%5Cright%5C%7C%7D_2%7D%5C+%5C+if%5C+%5C+%7By_j%7D+%5Cne+%7Bp_%7B%7Bi%5E+%2A+%7D%7D%7D%7D+%5Cend%7Barray%7D%7D+%5Cright.%5C%5D\" alt=\"\\[\\left\\{ {\\begin{array}{*{20}{c}} {{{\\left\\| {p&#39; - {x_j}} \\right\\|}_2} = {{\\left\\| {{p_{{i^ * }}} + \\eta \\left( {{x_j} - {p_{{i^ * }}}} \\right) - {x_j}} \\right\\|}_2} = \\left( {1 - \\eta } \\right){{\\left\\| {{p_{{i^ * }}} - {x_j}} \\right\\|}_2}\\ \\ if\\ \\ {y_j} = {p_{{i^ * }}}}\\\\ {{{\\left\\| {p&#39; - {x_j}} \\right\\|}_2} = {{\\left\\| {{p_{{i^ * }}} - \\eta \\left( {{x_j} - {p_{{i^ * }}}} \\right) - {x_j}} \\right\\|}_2} = \\left( {1 + \\eta } \\right){{\\left\\| {{p_{{i^ * }}} - {x_j}} \\right\\|}_2}\\ \\ if\\ \\ {y_j} \\ne {p_{{i^ * }}}} \\end{array}} \\right.\\]\" eeimg=\"1\"/> </p><p>当得到一组原型向量 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cleft%5C%7B+%7B%7Bp_1%7D%2C%7Bp_2%7D%2C+%5Ccdot++%5Ccdot++%5Ccdot+%2C%7Bp_q%7D%7D+%5Cright%5C%7D%5C%5D\" alt=\"\\[\\left\\{ {{p_1},{p_2}, \\cdot  \\cdot  \\cdot ,{p_q}} \\right\\}\\]\" eeimg=\"1\"/> 后，就可以实现对样本集合进行划分。对于任意样本<img src=\"https://www.zhihu.com/equation?tex=%5C%5Bx%5C%5D\" alt=\"\\[x\\]\" eeimg=\"1\"/> ，将它划分到与其距离最近的原型向量所代表的簇中。每个原型向量 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bp_i%7D%5C%5D\" alt=\"\\[{p_i}\\]\" eeimg=\"1\"/> 定义了与之相关的一个区域 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BR_i%7D%5C%5D\" alt=\"\\[{R_i}\\]\" eeimg=\"1\"/> ，该区域中每个样本与 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bp_i%7D%5C%5D\" alt=\"\\[{p_i}\\]\" eeimg=\"1\"/> 的距离不大于它与其他原型向量 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bp_%7Bi%27%7D%7D%5C%5D\" alt=\"\\[{p_{i&#39;}}\\]\" eeimg=\"1\"/> <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cleft%28+%7Bi%27+%5Cne+i%7D+%5Cright%29%5C%5D\" alt=\"\\[\\left( {i&#39; \\ne i} \\right)\\]\" eeimg=\"1\"/> 的距离。由此形成了对样本空间的簇划分 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cleft%5C%7B+%7B%7BR_1%7D%2C%7BR_2%7D%2C+%5Ccdot++%5Ccdot++%5Ccdot+%2C%7BR_q%7D%7D+%5Cright%5C%7D%5C%5D\" alt=\"\\[\\left\\{ {{R_1},{R_2}, \\cdot  \\cdot  \\cdot ,{R_q}} \\right\\}\\]\" eeimg=\"1\"/> ，该划分通常称为“Voronoi剖分”。</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"k\">def</span> <span class=\"nf\">compute_dist</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">p</span><span class=\"p\">):</span>\n    <span class=\"n\">XX</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"o\">**</span><span class=\"mi\">2</span><span class=\"p\">,</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n    <span class=\"n\">PP</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"o\">**</span><span class=\"mi\">2</span><span class=\"p\">,</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n    <span class=\"n\">XP</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">)</span>\n    <span class=\"n\">dist</span> <span class=\"o\">=</span> <span class=\"n\">XX</span> <span class=\"o\">-</span> <span class=\"mi\">2</span><span class=\"o\">*</span><span class=\"n\">XP</span>\n    <span class=\"n\">dist</span> <span class=\"o\">=</span> <span class=\"n\">dist</span><span class=\"o\">.</span><span class=\"n\">T</span> <span class=\"o\">+</span> <span class=\"n\">PP</span>\n    <span class=\"k\">return</span> <span class=\"n\">dist</span><span class=\"o\">.</span><span class=\"n\">T</span>\n<span class=\"k\">def</span> <span class=\"nf\">lvq</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">t</span><span class=\"p\">,</span> <span class=\"n\">eta</span><span class=\"o\">=</span><span class=\"mf\">0.24</span><span class=\"p\">,</span> <span class=\"n\">max_iter</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">):</span>\n    <span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">t</span><span class=\"p\">),</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]))</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"n\">t</span><span class=\"p\">:</span>\n        <span class=\"n\">mask</span> <span class=\"o\">=</span> <span class=\"n\">i</span><span class=\"o\">==</span><span class=\"n\">y</span>\n        <span class=\"n\">tmp</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"p\">[</span><span class=\"n\">mask</span><span class=\"p\">]</span>\n        <span class=\"n\">index</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randint</span><span class=\"p\">(</span><span class=\"n\">tmp</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">])</span>\n        <span class=\"n\">p</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">,:]</span> <span class=\"o\">=</span> <span class=\"n\">tmp</span><span class=\"p\">[</span><span class=\"n\">index</span><span class=\"p\">]</span>\n    <span class=\"k\">for</span> <span class=\"n\">it</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">max_iter</span><span class=\"p\">):</span>\n        <span class=\"n\">dist</span> <span class=\"o\">=</span> <span class=\"n\">compute_dist</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span><span class=\"n\">p</span><span class=\"p\">)</span>\n        <span class=\"n\">labels</span> <span class=\"o\">=</span> <span class=\"n\">dist</span><span class=\"o\">.</span><span class=\"n\">argmin</span><span class=\"p\">(</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n        <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]):</span>\n            <span class=\"n\">i_</span> <span class=\"o\">=</span> <span class=\"n\">labels</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span>\n            <span class=\"k\">if</span> <span class=\"n\">y</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span> <span class=\"o\">==</span> <span class=\"n\">t</span><span class=\"p\">[</span><span class=\"n\">i_</span><span class=\"p\">]:</span>\n                <span class=\"n\">tmp_eta</span> <span class=\"o\">=</span> <span class=\"n\">eta</span>\n            <span class=\"k\">else</span><span class=\"p\">:</span>\n                <span class=\"n\">tmp_eta</span> <span class=\"o\">=</span> <span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"o\">*</span><span class=\"n\">eta</span>\n            <span class=\"n\">p</span><span class=\"p\">[</span><span class=\"n\">i_</span><span class=\"p\">,:]</span> <span class=\"o\">=</span> <span class=\"n\">p</span><span class=\"p\">[</span><span class=\"n\">i_</span><span class=\"p\">,:]</span> <span class=\"o\">+</span> <span class=\"n\">tmp_eta</span><span class=\"o\">*</span><span class=\"p\">(</span> <span class=\"n\">X</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">,:]</span> <span class=\"o\">-</span> <span class=\"n\">p</span><span class=\"p\">[</span><span class=\"n\">i_</span><span class=\"p\">,:])</span>\n    <span class=\"n\">dist</span> <span class=\"o\">=</span> <span class=\"n\">compute_dist</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span><span class=\"n\">p</span><span class=\"p\">)</span>\n    <span class=\"n\">labels</span> <span class=\"o\">=</span> <span class=\"n\">dist</span><span class=\"o\">.</span><span class=\"n\">argmin</span><span class=\"p\">(</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">labels</span><span class=\"p\">,</span><span class=\"n\">p</span>\n<span class=\"n\">y_pred</span><span class=\"p\">,</span><span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"n\">lvq</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span><span class=\"n\">y</span><span class=\"p\">,[</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">2</span><span class=\"p\">,</span><span class=\"mi\">3</span><span class=\"p\">])</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">scatter</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">[:,</span> <span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">X</span><span class=\"p\">[:,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">c</span><span class=\"o\">=</span><span class=\"n\">y_pred</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">scatter</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"p\">[:,</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">p</span><span class=\"p\">[:,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">marker</span><span class=\"o\">=</span><span class=\"s1\">&#39;^&#39;</span><span class=\"p\">,</span> <span class=\"n\">c</span><span class=\"o\">=</span><span class=\"s1\">&#39;r&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span></code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-15a1a9ce3b8469d362b4b53be2a1c730_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"367\" data-rawheight=\"246\" class=\"content_image\" width=\"367\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;367&#39; height=&#39;246&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"367\" data-rawheight=\"246\" class=\"content_image lazy\" width=\"367\" data-actualsrc=\"https://pic1.zhimg.com/v2-15a1a9ce3b8469d362b4b53be2a1c730_b.jpg\"/></figure><h2><b>四、高斯混合聚类</b></h2><p>高斯混合聚类(GMM)采用<b>概率模型</b>来表达聚类原型，同一“类”的数据属于同一种概率分布。多元高斯分布，其概率密度函数为：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5Bp%5Cleft%28+%7Bx%7C%5Cmu+%2C%5CSigma+%7D+%5Cright%29+%3D+%5Cfrac%7B1%7D%7B%7B%7B%7B%5Cleft%28+%7B2%5Cpi+%7D+%5Cright%29%7D%5E%7B%5Cfrac%7Bn%7D%7B2%7D%7D%7D%7B%7B%5Cleft%7C+%5CSigma++%5Cright%7C%7D%5E%7B%5Cfrac%7B1%7D%7B2%7D%7D%7D%7D%7D%5Cexp+%5Cleft%28+%7B+-+%5Cfrac%7B1%7D%7B2%7D%7B%7B%5Cleft%28+%7Bx+-+%5Cmu+%7D+%5Cright%29%7D%5ET%7D%7B%5CSigma+%5E%7B+-+1%7D%7D%5Cleft%28+%7Bx+-+%5Cmu+%7D+%5Cright%29%7D+%5Cright%29+%5Ctag%7B4%7D%5C%5D\" alt=\"\\[p\\left( {x|\\mu ,\\Sigma } \\right) = \\frac{1}{{{{\\left( {2\\pi } \\right)}^{\\frac{n}{2}}}{{\\left| \\Sigma  \\right|}^{\\frac{1}{2}}}}}\\exp \\left( { - \\frac{1}{2}{{\\left( {x - \\mu } \\right)}^T}{\\Sigma ^{ - 1}}\\left( {x - \\mu } \\right)} \\right) \\tag{4}\\]\" eeimg=\"1\"/> </p><p>其中 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cmu+%5C%5D\" alt=\"\\[\\mu \\]\" eeimg=\"1\"/> 是 <img src=\"https://www.zhihu.com/equation?tex=n\" alt=\"n\" eeimg=\"1\"/> 维均值向量， <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5CSigma+%5C%5D\" alt=\"\\[\\Sigma \\]\" eeimg=\"1\"/> 是 <img src=\"https://www.zhihu.com/equation?tex=%5C%5Bn+%5Ctimes+n%5C%5D\" alt=\"\\[n \\times n\\]\" eeimg=\"1\"/> 的协方差矩阵。高斯混合分布的概率密度函数为：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bp_M%7D%5Cleft%28+x+%5Cright%29+%3D+%5Csum%5Climits_%7Bi+%3D+1%7D%5Ek+%7B%7B%5Calpha+_i%7D+%5Ccdot+p%5Cleft%28+%7Bx%7C%7B%5Cmu+_i%7D%2C%7B%5CSigma+_i%7D%7D+%5Cright%29%7D+%5Ctag%7B5%7D+%5C%5D\" alt=\"\\[{p_M}\\left( x \\right) = \\sum\\limits_{i = 1}^k {{\\alpha _i} \\cdot p\\left( {x|{\\mu _i},{\\Sigma _i}} \\right)} \\tag{5} \\]\" eeimg=\"1\"/> </p><p>该分布由 <img src=\"https://www.zhihu.com/equation?tex=k\" alt=\"k\" eeimg=\"1\"/> 个多元高斯分布混合组成，其中 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5Cmu+_i%7D%7D%5C%5D\" alt=\"\\[{{\\mu _i}}\\]\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5CSigma+_i%7D%7D%5C%5D\" alt=\"\\[{{\\Sigma _i}}\\]\" eeimg=\"1\"/> 是第 <img src=\"https://www.zhihu.com/equation?tex=i\" alt=\"i\" eeimg=\"1\"/> 个高斯分布的参数， <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Calpha+_i%7D+%3E+0%5C%5D\" alt=\"\\[{\\alpha _i} &gt; 0\\]\" eeimg=\"1\"/> 为对应的混合系数， <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Csum%5Cnolimits_%7Bi+%3D+1%7D%5Ek+%7B%7B%5Calpha+_i%7D%7D++%3D+1%5C%5D\" alt=\"\\[\\sum\\nolimits_{i = 1}^k {{\\alpha _i}}  = 1\\]\" eeimg=\"1\"/> 。</p><p>假设样本的生成过程由高斯混合分布给出：1）根据 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Calpha+_1%7D%2C%7B%5Calpha+_2%7D%2C+%5Ccdot++%5Ccdot++%5Ccdot+%2C%7B%5Calpha+_k%7D%5C%5D\" alt=\"\\[{\\alpha _1},{\\alpha _2}, \\cdot  \\cdot  \\cdot ,{\\alpha _k}\\]\" eeimg=\"1\"/> 定义的先验分布选择高斯混合成分，其中 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Calpha+_i%7D%5C%5D\" alt=\"\\[{\\alpha _i}\\]\" eeimg=\"1\"/> 为选择第 <img src=\"https://www.zhihu.com/equation?tex=i\" alt=\"i\" eeimg=\"1\"/> 个混合成分的概率；2）根据被选择的混合成分的概率密度函数进行采样，从而生成相应的样本。</p><p>样本集 <img src=\"https://www.zhihu.com/equation?tex=%5C%5BD+%3D+%5Cleft%5C%7B+%7B%7Bx_1%7D%2C%7Bx_2%7D%2C+%5Ccdot++%5Ccdot++%5Ccdot+%2C%7Bx_m%7D%7D+%5Cright%5C%7D%5C%5D\" alt=\"\\[D = \\left\\{ {{x_1},{x_2}, \\cdot  \\cdot  \\cdot ,{x_m}} \\right\\}\\]\" eeimg=\"1\"/> ， <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bz_j%7D+%5Cin+%5Cleft%5C%7B+%7B1%2C2%2C+%5Ccdot++%5Ccdot++%5Ccdot+%2Ck%7D+%5Cright%5C%7D%5C%5D\" alt=\"\\[{z_j} \\in \\left\\{ {1,2, \\cdot  \\cdot  \\cdot ,k} \\right\\}\\]\" eeimg=\"1\"/> 表示生成样本 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bx_j%7D%5C%5D\" alt=\"\\[{x_j}\\]\" eeimg=\"1\"/> 的高斯混合成分，其取值未知，其中 <img src=\"https://www.zhihu.com/equation?tex=%5C%5Bp%5Cleft%28+%7B%7Bz_j%7D+%3D+i%7D+%5Cright%29+%3D+%7B%5Calpha+_i%7D%5C%5D\" alt=\"\\[p\\left( {{z_j} = i} \\right) = {\\alpha _i}\\]\" eeimg=\"1\"/> 。 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bz_j%7D%5C%5D\" alt=\"\\[{z_j}\\]\" eeimg=\"1\"/> 的后验分布为：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D++%7Bp_M%7D%5Cleft%28+%7B%7Bz_j%7D+%3D+i%7C%7Bx_j%7D%7D+%5Cright%29+%26%3D+%5Cfrac%7B%7Bp%5Cleft%28+%7B%7Bz_j%7D+%3D+i%7D+%5Cright%29+%5Ccdot+%7Bp_M%7D%5Cleft%28+%7B%7Bx_j%7D%7C%7Bz_j%7D+%3D+i%7D+%5Cright%29%7D%7D%7B%7B%7Bp_M%7D%5Cleft%28+%7B%7Bx_j%7D%7D+%5Cright%29%7D%7D%5C%5C++%26%3D+%5Cfrac%7B%7B%7B%5Calpha+_i%7D+%5Ccdot+p%5Cleft%28+%7B%7Bx_j%7D%7C%7B%5Cmu+_i%7D%2C%7B%5CSigma+_i%7D%7D+%5Cright%29%7D%7D%7B%7B%5Csum%5Climits_%7Bl+%3D+1%7D%5Ek+%7B%7B%5Calpha+_l%7D+%5Ccdot+p%5Cleft%28+%7B%7Bx_j%7D%7C%7B%5Cmu+_l%7D%2C%7B%5CSigma+_l%7D%7D+%5Cright%29%7D+%7D%7D+%5Ctag%7B6%7D+%5Cend%7Balign%7D\" alt=\"\\begin{align}  {p_M}\\left( {{z_j} = i|{x_j}} \\right) &amp;= \\frac{{p\\left( {{z_j} = i} \\right) \\cdot {p_M}\\left( {{x_j}|{z_j} = i} \\right)}}{{{p_M}\\left( {{x_j}} \\right)}}\\\\  &amp;= \\frac{{{\\alpha _i} \\cdot p\\left( {{x_j}|{\\mu _i},{\\Sigma _i}} \\right)}}{{\\sum\\limits_{l = 1}^k {{\\alpha _l} \\cdot p\\left( {{x_j}|{\\mu _l},{\\Sigma _l}} \\right)} }} \\tag{6} \\end{align}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bp_M%7D%5Cleft%28+%7B%7Bz_j%7D+%3D+i%7C%7Bx_j%7D%7D+%5Cright%29%5C%5D\" alt=\"\\[{p_M}\\left( {{z_j} = i|{x_j}} \\right)\\]\" eeimg=\"1\"/> 给出了样本 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7Bx_j%7D%7D%5C%5D\" alt=\"\\[{{x_j}}\\]\" eeimg=\"1\"/> 由第 <img src=\"https://www.zhihu.com/equation?tex=i\" alt=\"i\" eeimg=\"1\"/> 个高斯混合成分生成的后验概率，简记为 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Cgamma+_%7Bji%7D%7D%5C%5D\" alt=\"\\[{\\gamma _{ji}}\\]\" eeimg=\"1\"/> 。</p><p>如果公式 (5) 中的 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cleft%5C%7B+%7B%5Cleft%28+%7B%7B%5Calpha+_i%7D%2C%7B%5Cmu+_i%7D%2C%7B%5CSigma+_i%7D%7D+%5Cright%29%7C1+%5Cle+i+%5Cle+k%7D+%5Cright%5C%7D%5C%5D\" alt=\"\\[\\left\\{ {\\left( {{\\alpha _i},{\\mu _i},{\\Sigma _i}} \\right)|1 \\le i \\le k} \\right\\}\\]\" eeimg=\"1\"/> 已知，高斯混合聚类将把样本集 <img src=\"https://www.zhihu.com/equation?tex=D\" alt=\"D\" eeimg=\"1\"/> 划分为 <img src=\"https://www.zhihu.com/equation?tex=k\" alt=\"k\" eeimg=\"1\"/> 个簇类，每个样本 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7Bx_j%7D%7D%5C%5D\" alt=\"\\[{{x_j}}\\]\" eeimg=\"1\"/> 的簇标记 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Clambda+_j%7D%5C%5D\" alt=\"\\[{\\lambda _j}\\]\" eeimg=\"1\"/> 为：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Clambda+_j%7D+%3D+%5Cmathop+%7B%5Carg+%5Cmax+%7D%5Climits_%7Bi+%5Cin+%5Cleft%5C%7B+%7B1%2C2%2C+%5Ccdot++%5Ccdot++%5Ccdot+%2Ck%7D+%5Cright%5C%7D%7D+%7B%5Cgamma+_%7Bji%7D%7D+%5Ctag%7B7%7D%5C%5D\" alt=\"\\[{\\lambda _j} = \\mathop {\\arg \\max }\\limits_{i \\in \\left\\{ {1,2, \\cdot  \\cdot  \\cdot ,k} \\right\\}} {\\gamma _{ji}} \\tag{7}\\]\" eeimg=\"1\"/> </p><p>接下来如果求解参数 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cleft%5C%7B+%7B%5Cleft%28+%7B%7B%5Calpha+_i%7D%2C%7B%5Cmu+_i%7D%2C%7B%5CSigma+_i%7D%7D+%5Cright%29%7C1+%5Cle+i+%5Cle+k%7D+%5Cright%5C%7D%5C%5D\" alt=\"\\[\\left\\{ {\\left( {{\\alpha _i},{\\mu _i},{\\Sigma _i}} \\right)|1 \\le i \\le k} \\right\\}\\]\" eeimg=\"1\"/> ？给定样本集 <img src=\"https://www.zhihu.com/equation?tex=D\" alt=\"D\" eeimg=\"1\"/> ，采用极大似然估计法，即最大化：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5BLL%5Cleft%28+D+%5Cright%29+%3D+%5Csum%5Climits_%7Bj+%3D+1%7D%5Em+%7B%5Cln+%5Cleft%28+%7B%5Csum%5Climits_%7Bi+%3D+1%7D%5Ek+%7B%7B%5Calpha+_i%7D+%5Ccdot+p%5Cleft%28+%7B%7Bx_j%7D%7C%7B%5Cmu+_i%7D%2C%7B%5CSigma+_i%7D%7D+%5Cright%29%7D+%7D+%5Cright%29%7D++%2B+%5Clambda+%5Cleft%28+%7B%5Csum%5Climits_%7Bi+%3D+1%7D%5Ek+%7B%7B%5Calpha+_i%7D%7D++-+1%7D+%5Cright%29+%5Ctag%7B8%7D%5C%5D\" alt=\"\\[LL\\left( D \\right) = \\sum\\limits_{j = 1}^m {\\ln \\left( {\\sum\\limits_{i = 1}^k {{\\alpha _i} \\cdot p\\left( {{x_j}|{\\mu _i},{\\Sigma _i}} \\right)} } \\right)}  + \\lambda \\left( {\\sum\\limits_{i = 1}^k {{\\alpha _i}}  - 1} \\right) \\tag{8}\\]\" eeimg=\"1\"/> </p><ul><li>求解 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5Cmu+_i%7D%7D%5C%5D\" alt=\"\\[{{\\mu _i}}\\]\" eeimg=\"1\"/> </li></ul><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+%5Cfrac%7B%7B%5Cpartial+LL%5Cleft%28+D+%5Cright%29%7D%7D%7B%7B%5Cpartial+%7B%5Cmu+_i%7D%7D%7D+%3D+%5Csum%5Climits_%7Bj+%3D+1%7D%5Em+%7B%5Cleft%28+%7B%5Cfrac%7B%7B%7B%5Calpha+_i%7D%7D%7D%7B%7B%5Csum%5Climits_%7Bi+%3D+1%7D%5Ek+%7B%7B%5Calpha+_i%7D+%5Ccdot+p%5Cleft%28+%7B%7Bx_j%7D%7C%7B%5Cmu+_i%7D%2C%7B%5CSigma+_i%7D%7D+%5Cright%29%7D+%7D%7D%5Cfrac%7B%7B%5Cpartial+p%5Cleft%28+%7B%7Bx_j%7D%7C%7B%5Cmu+_i%7D%2C%7B%5CSigma+_i%7D%7D+%5Cright%29%7D%7D%7B%7B%5Cpartial+%7B%5Cmu+_i%7D%7D%7D%7D+%5Cright%29%7D++%5Ctag%7B9%7D%5Cend%7Balign%7D\" alt=\"\\begin{align} \\frac{{\\partial LL\\left( D \\right)}}{{\\partial {\\mu _i}}} = \\sum\\limits_{j = 1}^m {\\left( {\\frac{{{\\alpha _i}}}{{\\sum\\limits_{i = 1}^k {{\\alpha _i} \\cdot p\\left( {{x_j}|{\\mu _i},{\\Sigma _i}} \\right)} }}\\frac{{\\partial p\\left( {{x_j}|{\\mu _i},{\\Sigma _i}} \\right)}}{{\\partial {\\mu _i}}}} \\right)}  \\tag{9}\\end{align}\" eeimg=\"1\"/></p><p class=\"ztext-empty-paragraph\"><br/></p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+%26dp%5Cleft%28+%7Bx%7C%5Cmu+%2C%5CSigma+%7D+%5Cright%29+%3D+tr%5Cleft%28+%7Bdp%5Cleft%28+%7Bx%7C%5Cmu+%2C%5CSigma+%7D+%5Cright%29%7D+%5Cright%29%5C%5C++%26%3D++-+%5Cfrac%7B1%7D%7B2%7Dp%5Cleft%28+%7Bx%7C%5Cmu+%2C%5CSigma+%7D+%5Cright%29tr%5Cleft%28+%7Bd%7B%7B%5Cleft%28+%7Bx+-+%5Cmu+%7D+%5Cright%29%7D%5ET%7D+%5Ccdot+%7B%5CSigma+%5E%7B+-+1%7D%7D%5Cleft%28+%7Bx+-+%5Cmu+%7D+%5Cright%29+%2B+%7B%7B%5Cleft%28+%7Bx+-+%5Cmu+%7D+%5Cright%29%7D%5ET%7D%7B%5CSigma+%5E%7B+-+1%7D%7Dd%5Cleft%28+%7Bx+-+%5Cmu+%7D+%5Cright%29%7D+%5Cright%29%5C%5C++%26%3D+p%5Cleft%28+%7Bx%7C%5Cmu+%2C%5CSigma+%7D+%5Cright%29tr%5Cleft%28+%7B%7B%7B%5Cleft%28+%7Bx+-+%5Cmu+%7D+%5Cright%29%7D%5ET%7D%7B%5CSigma+%5E%7B+-+1%7D%7Dd%5Cmu+%7D+%5Cright%29%5C%5C++%26%5CRightarrow+%5Cfrac%7B%7B%5Cpartial+p%5Cleft%28+%7Bx%7C%5Cmu+%2C%5CSigma+%7D+%5Cright%29%7D%7D%7B%7B%5Cpartial+%5Cmu+%7D%7D+%3D+p%5Cleft%28+%7Bx%7C%5Cmu+%2C%5CSigma+%7D+%5Cright%29%7B%5CSigma+%5E%7B+-+1%7D%7D%5Cleft%28+%7Bx+-+%5Cmu+%7D+%5Cright%29+%5Ctag%7B10%7D+%5Cend%7Balign%7D\" alt=\"\\begin{align} &amp;dp\\left( {x|\\mu ,\\Sigma } \\right) = tr\\left( {dp\\left( {x|\\mu ,\\Sigma } \\right)} \\right)\\\\  &amp;=  - \\frac{1}{2}p\\left( {x|\\mu ,\\Sigma } \\right)tr\\left( {d{{\\left( {x - \\mu } \\right)}^T} \\cdot {\\Sigma ^{ - 1}}\\left( {x - \\mu } \\right) + {{\\left( {x - \\mu } \\right)}^T}{\\Sigma ^{ - 1}}d\\left( {x - \\mu } \\right)} \\right)\\\\  &amp;= p\\left( {x|\\mu ,\\Sigma } \\right)tr\\left( {{{\\left( {x - \\mu } \\right)}^T}{\\Sigma ^{ - 1}}d\\mu } \\right)\\\\  &amp;\\Rightarrow \\frac{{\\partial p\\left( {x|\\mu ,\\Sigma } \\right)}}{{\\partial \\mu }} = p\\left( {x|\\mu ,\\Sigma } \\right){\\Sigma ^{ - 1}}\\left( {x - \\mu } \\right) \\tag{10} \\end{align}\" eeimg=\"1\"/> </p><p>结合公式 (9) 和公式 (10)，并令 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cfrac%7B%7B%5Cpartial+LL%5Cleft%28+D+%5Cright%29%7D%7D%7B%7B%5Cpartial+%7B%5Cmu+_i%7D%7D%7D+%3D+0%5C%5D\" alt=\"\\[\\frac{{\\partial LL\\left( D \\right)}}{{\\partial {\\mu _i}}} = 0\\]\" eeimg=\"1\"/> ，可得：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+%5Cfrac%7B%7B%5Cpartial+LL%5Cleft%28+D+%5Cright%29%7D%7D%7B%7B%5Cpartial+%7B%5Cmu+_i%7D%7D%7D+%26%3D+%5Csum%5Climits_%7Bj+%3D+1%7D%5Em+%7B%5Cleft%28+%7B%5Cfrac%7B%7B%7B%5Calpha+_i%7Dp%5Cleft%28+%7B%7Bx_j%7D%7C%7B%5Cmu+_i%7D%2C%7B%5CSigma+_i%7D%7D+%5Cright%29%7D%7D%7B%7B%5Csum%5Climits_%7Bi+%3D+1%7D%5Ek+%7B%7B%5Calpha+_i%7D+%5Ccdot+p%5Cleft%28+%7B%7Bx_j%7D%7C%7B%5Cmu+_i%7D%2C%7B%5CSigma+_i%7D%7D+%5Cright%29%7D+%7D%7D%5CSigma+_i%5E%7B+-+1%7D%5Cleft%28+%7B%7Bx_j%7D+-+%7B%5Cmu+_i%7D%7D+%5Cright%29%7D+%5Cright%29%7D+%5C%5C++%26%3D+%5Csum%5Climits_%7Bj+%3D+1%7D%5Em+%7B%5Cleft%28+%7B%7B%5Cgamma+_%7Bji%7D%7D%5CSigma+_i%5E%7B+-+1%7D%5Cleft%28+%7B%7Bx_j%7D+-+%7B%5Cmu+_i%7D%7D+%5Cright%29%7D+%5Cright%29%7D+%5C%5C++%5CRightarrow+%7B%5Cmu+_i%7D+%26%3D+%5Cfrac%7B%7B%5Csum%5Climits_%7Bj+%3D+1%7D%5Em+%7B%7B%5Cgamma+_%7Bji%7D%7D%7Bx_j%7D%7D+%7D%7D%7B%7B%5Csum%5Climits_%7Bj+%3D+1%7D%5Em+%7B%7B%5Cgamma+_%7Bji%7D%7D%7D+%7D%7D+%5Ctag%7B11%7D+%5Cend%7Balign%7D\" alt=\"\\begin{align} \\frac{{\\partial LL\\left( D \\right)}}{{\\partial {\\mu _i}}} &amp;= \\sum\\limits_{j = 1}^m {\\left( {\\frac{{{\\alpha _i}p\\left( {{x_j}|{\\mu _i},{\\Sigma _i}} \\right)}}{{\\sum\\limits_{i = 1}^k {{\\alpha _i} \\cdot p\\left( {{x_j}|{\\mu _i},{\\Sigma _i}} \\right)} }}\\Sigma _i^{ - 1}\\left( {{x_j} - {\\mu _i}} \\right)} \\right)} \\\\  &amp;= \\sum\\limits_{j = 1}^m {\\left( {{\\gamma _{ji}}\\Sigma _i^{ - 1}\\left( {{x_j} - {\\mu _i}} \\right)} \\right)} \\\\  \\Rightarrow {\\mu _i} &amp;= \\frac{{\\sum\\limits_{j = 1}^m {{\\gamma _{ji}}{x_j}} }}{{\\sum\\limits_{j = 1}^m {{\\gamma _{ji}}} }} \\tag{11} \\end{align}\" eeimg=\"1\"/> </p><ul><li>求解 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5CSigma+_i%7D%7D%5C%5D\" alt=\"\\[{{\\Sigma _i}}\\]\" eeimg=\"1\"/></li></ul><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cfrac%7B%7B%5Cpartial+LL%5Cleft%28+D+%5Cright%29%7D%7D%7B%7B%5Cpartial+%7B%5CSigma+_i%7D%7D%7D+%3D+%5Csum%5Climits_%7Bj+%3D+1%7D%5Em+%7B%5Cleft%28+%7B%5Cfrac%7B%7B%7B%5Calpha+_i%7D%7D%7D%7B%7B%5Csum%5Climits_%7Bi+%3D+1%7D%5Ek+%7B%7B%5Calpha+_i%7D+%5Ccdot+p%5Cleft%28+%7Bx%7C%7B%5Cmu+_i%7D%2C%7B%5CSigma+_i%7D%7D+%5Cright%29%7D+%7D%7D%5Cfrac%7B%7B%5Cpartial+p%5Cleft%28+%7B%7Bx_j%7D%7C%7B%5Cmu+_i%7D%2C%7B%5CSigma+_i%7D%7D+%5Cright%29%7D%7D%7B%7B%5Cpartial+%7B%5CSigma+_i%7D%7D%7D%7D+%5Cright%29%7D+%5Ctag%7B12%7D%5C%5D\" alt=\"\\[\\frac{{\\partial LL\\left( D \\right)}}{{\\partial {\\Sigma _i}}} = \\sum\\limits_{j = 1}^m {\\left( {\\frac{{{\\alpha _i}}}{{\\sum\\limits_{i = 1}^k {{\\alpha _i} \\cdot p\\left( {x|{\\mu _i},{\\Sigma _i}} \\right)} }}\\frac{{\\partial p\\left( {{x_j}|{\\mu _i},{\\Sigma _i}} \\right)}}{{\\partial {\\Sigma _i}}}} \\right)} \\tag{12}\\]\" eeimg=\"1\"/></p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+%26dp%5Cleft%28+%7Bx%7C%5Cmu+%2C%5CSigma+%7D+%5Cright%29+%3D+tr%5Cleft%28+%7Bdp%5Cleft%28+%7Bx%7C%5Cmu+%2C%5CSigma+%7D+%5Cright%29%7D+%5Cright%29%5C%5C++%26%3D++-+%5Cfrac%7B1%7D%7B2%7Dp%5Cleft%28+%7Bx%7C%5Cmu+%2C%5CSigma+%7D+%5Cright%29tr%5Cleft%28+%7B%7B%7B%5Cleft%7C+%5CSigma++%5Cright%7C%7D%5E%7B+-+1%7D%7Dd%5Cleft%7C+%5CSigma++%5Cright%7C+%2B+%7B%7B%5Cleft%28+%7Bx+-+%5Cmu+%7D+%5Cright%29%7D%5ET%7Dd%5Cleft%28+%7B%7B%5CSigma+%5E%7B+-+1%7D%7D%7D+%5Cright%29+%5Ccdot+%5Cleft%28+%7Bx+-+%5Cmu+%7D+%5Cright%29%7D+%5Cright%29%5C%5C++%26%3D++-+%5Cfrac%7B1%7D%7B2%7Dp%5Cleft%28+%7Bx%7C%5Cmu+%2C%5CSigma+%7D+%5Cright%29tr%5Cleft%28+%7B%7B%7B%5Cleft%7C+%5CSigma++%5Cright%7C%7D%5E%7B+-+1%7D%7D%5Cleft%7C+%5CSigma++%5Cright%7Ctr%5Cleft%28+%7B%7B%5CSigma+%5E%7B+-+1%7D%7Dd%5CSigma+%7D+%5Cright%29+%2B+%7B%7B%5Cleft%28+%7Bx+-+%5Cmu+%7D+%5Cright%29%7D%5ET%7D%5Cleft%28+%7B+-+%7B%5CSigma+%5E%7B+-+1%7D%7Dd%5CSigma++%5Ccdot+%7B%5CSigma+%5E%7B+-+1%7D%7D%7D+%5Cright%29+%5Ccdot+%5Cleft%28+%7Bx+-+%5Cmu+%7D+%5Cright%29%7D+%5Cright%29%5C%5C++%26%3D++-+%5Cfrac%7B1%7D%7B2%7Dp%5Cleft%28+%7Bx%7C%5Cmu+%2C%5CSigma+%7D+%5Cright%29tr%5Cleft%28+%7B%7B%5CSigma+%5E%7B+-+1%7D%7Dd%5CSigma++-+%7B%5CSigma+%5E%7B+-+1%7D%7D%5Cleft%28+%7Bx+-+%5Cmu+%7D+%5Cright%29%7B%7B%5Cleft%28+%7Bx+-+%5Cmu+%7D+%5Cright%29%7D%5ET%7D%7B%5CSigma+%5E%7B+-+1%7D%7Dd%5CSigma+%7D+%5Cright%29%5C%5C++%26%5CRightarrow+%5Cfrac%7B%7B%5Cpartial+p%5Cleft%28+%7Bx%7C%5Cmu+%2C%5CSigma+%7D+%5Cright%29%7D%7D%7B%7B%5Cpartial+%5CSigma+%7D%7D+%3D++-+%5Cfrac%7B1%7D%7B2%7Dp%5Cleft%28+%7Bx%7C%5Cmu+%2C%5CSigma+%7D+%5Cright%29%5Cleft%28+%7B%7B%5CSigma+%5E%7B+-+1%7D%7D+-+%7B%5CSigma+%5E%7B+-+1%7D%7D%5Cleft%28+%7Bx+-+%5Cmu+%7D+%5Cright%29%7B%7B%5Cleft%28+%7Bx+-+%5Cmu+%7D+%5Cright%29%7D%5ET%7D%7B%5CSigma+%5E%7B+-+1%7D%7D%7D+%5Cright%29+%5Ctag%7B13%7D+%5Cend%7Balign%7D\" alt=\"\\begin{align} &amp;dp\\left( {x|\\mu ,\\Sigma } \\right) = tr\\left( {dp\\left( {x|\\mu ,\\Sigma } \\right)} \\right)\\\\  &amp;=  - \\frac{1}{2}p\\left( {x|\\mu ,\\Sigma } \\right)tr\\left( {{{\\left| \\Sigma  \\right|}^{ - 1}}d\\left| \\Sigma  \\right| + {{\\left( {x - \\mu } \\right)}^T}d\\left( {{\\Sigma ^{ - 1}}} \\right) \\cdot \\left( {x - \\mu } \\right)} \\right)\\\\  &amp;=  - \\frac{1}{2}p\\left( {x|\\mu ,\\Sigma } \\right)tr\\left( {{{\\left| \\Sigma  \\right|}^{ - 1}}\\left| \\Sigma  \\right|tr\\left( {{\\Sigma ^{ - 1}}d\\Sigma } \\right) + {{\\left( {x - \\mu } \\right)}^T}\\left( { - {\\Sigma ^{ - 1}}d\\Sigma  \\cdot {\\Sigma ^{ - 1}}} \\right) \\cdot \\left( {x - \\mu } \\right)} \\right)\\\\  &amp;=  - \\frac{1}{2}p\\left( {x|\\mu ,\\Sigma } \\right)tr\\left( {{\\Sigma ^{ - 1}}d\\Sigma  - {\\Sigma ^{ - 1}}\\left( {x - \\mu } \\right){{\\left( {x - \\mu } \\right)}^T}{\\Sigma ^{ - 1}}d\\Sigma } \\right)\\\\  &amp;\\Rightarrow \\frac{{\\partial p\\left( {x|\\mu ,\\Sigma } \\right)}}{{\\partial \\Sigma }} =  - \\frac{1}{2}p\\left( {x|\\mu ,\\Sigma } \\right)\\left( {{\\Sigma ^{ - 1}} - {\\Sigma ^{ - 1}}\\left( {x - \\mu } \\right){{\\left( {x - \\mu } \\right)}^T}{\\Sigma ^{ - 1}}} \\right) \\tag{13} \\end{align}\" eeimg=\"1\"/> </p><p> 结合公式 (12) 和公式 (13)，并令 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cfrac%7B%7B%5Cpartial+LL%5Cleft%28+D+%5Cright%29%7D%7D%7B%7B%5Cpartial+%7B%5CSigma+_i%7D%7D%7D+%3D+0%5C%5D\" alt=\"\\[\\frac{{\\partial LL\\left( D \\right)}}{{\\partial {\\Sigma _i}}} = 0\\]\" eeimg=\"1\"/> ，可得：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+%5Cfrac%7B%7B%5Cpartial+LL%5Cleft%28+D+%5Cright%29%7D%7D%7B%7B%5Cpartial+%7B%5CSigma+_i%7D%7D%7D+%26%3D+%5Csum%5Climits_%7Bj+%3D+1%7D%5Em+%7B%5Cleft%28+%7B+-+%5Cfrac%7B1%7D%7B2%7D%7B%5Cgamma+_%7Bji%7D%7D%5Cleft%28+%7B%5CSigma+_i%5E%7B+-+1%7D+-+%5CSigma+_i%5E%7B+-+1%7D%5Cleft%28+%7B%7Bx_j%7D+-+%7B%5Cmu+_i%7D%7D+%5Cright%29%7B%7B%5Cleft%28+%7B%7Bx_j%7D+-+%7B%5Cmu+_i%7D%7D+%5Cright%29%7D%5ET%7D%5CSigma+_i%5E%7B+-+1%7D%7D+%5Cright%29%7D+%5Cright%29%7D+%5C%5C++%5CRightarrow+%7B%5CSigma+_i%7D+%26%3D+%5Cfrac%7B%7B%5Csum%5Climits_%7Bj+%3D+1%7D%5Em+%7B%7B%5Cgamma+_%7Bji%7D%7D%5Cleft%28+%7B%7Bx_j%7D+-+%7B%5Cmu+_i%7D%7D+%5Cright%29%7B%7B%5Cleft%28+%7B%7Bx_j%7D+-+%7B%5Cmu+_i%7D%7D+%5Cright%29%7D%5ET%7D%7D+%7D%7D%7B%7B%5Csum%5Climits_%7Bj+%3D+1%7D%5Em+%7B%7B%5Cgamma+_%7Bji%7D%7D%7D+%7D%7D+%5Ctag%7B14%7D+%5Cend%7Balign%7D\" alt=\"\\begin{align} \\frac{{\\partial LL\\left( D \\right)}}{{\\partial {\\Sigma _i}}} &amp;= \\sum\\limits_{j = 1}^m {\\left( { - \\frac{1}{2}{\\gamma _{ji}}\\left( {\\Sigma _i^{ - 1} - \\Sigma _i^{ - 1}\\left( {{x_j} - {\\mu _i}} \\right){{\\left( {{x_j} - {\\mu _i}} \\right)}^T}\\Sigma _i^{ - 1}} \\right)} \\right)} \\\\  \\Rightarrow {\\Sigma _i} &amp;= \\frac{{\\sum\\limits_{j = 1}^m {{\\gamma _{ji}}\\left( {{x_j} - {\\mu _i}} \\right){{\\left( {{x_j} - {\\mu _i}} \\right)}^T}} }}{{\\sum\\limits_{j = 1}^m {{\\gamma _{ji}}} }} \\tag{14} \\end{align}\" eeimg=\"1\"/> </p><ul><li>求解 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5Calpha+_i%7D%7D%5C%5D\" alt=\"\\[{{\\alpha _i}}\\]\" eeimg=\"1\"/> </li></ul><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cfrac%7B%7B%5Cpartial+LL%5Cleft%28+D+%5Cright%29%7D%7D%7B%7B%5Cpartial+%7B%5Calpha+_i%7D%7D%7D+%3D+%5Csum%5Climits_%7Bj+%3D+1%7D%5Em+%7B%5Cleft%28+%7B%5Cfrac%7B%7Bp%5Cleft%28+%7B%7Bx_j%7D%7C%7B%5Cmu+_i%7D%2C%7B%5CSigma+_i%7D%7D+%5Cright%29%7D%7D%7B%7B%5Csum%5Climits_%7Bi+%3D+1%7D%5Ek+%7B%7B%5Calpha+_i%7D+%5Ccdot+p%5Cleft%28+%7Bx%7C%7B%5Cmu+_i%7D%2C%7B%5CSigma+_i%7D%7D+%5Cright%29%7D+%7D%7D%7D+%5Cright%29%7D++%2B+%5Clambda+%5Ctag%7B15%7D%5C%5D\" alt=\"\\[\\frac{{\\partial LL\\left( D \\right)}}{{\\partial {\\alpha _i}}} = \\sum\\limits_{j = 1}^m {\\left( {\\frac{{p\\left( {{x_j}|{\\mu _i},{\\Sigma _i}} \\right)}}{{\\sum\\limits_{i = 1}^k {{\\alpha _i} \\cdot p\\left( {x|{\\mu _i},{\\Sigma _i}} \\right)} }}} \\right)}  + \\lambda \\tag{15}\\]\" eeimg=\"1\"/></p><p>根据公式 (15) 令 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cfrac%7B%7B%5Cpartial+LL%5Cleft%28+D+%5Cright%29%7D%7D%7B%7B%5Cpartial+%7B%5Calpha+_i%7D%7D%7D+%3D+0%5C%5D\" alt=\"\\[\\frac{{\\partial LL\\left( D \\right)}}{{\\partial {\\alpha _i}}} = 0\\]\" eeimg=\"1\"/> ，可得：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+%5Csum%5Climits_%7Bj+%3D+1%7D%5Em+%7B%7B%5Cgamma+_%7Bji%7D%7D%7D++%2B+%5Clambda+%7B%5Calpha+_i%7D+%3D+0%5C%5C++%5CRightarrow+%7B%5Calpha+_i%7D+%3D+%5Cfrac%7B1%7D%7Bm%7D%5Csum%5Climits_%7Bj+%3D+1%7D%5Em+%7B%7B%5Cgamma+_%7Bji%7D%7D%7D++%5Ctag%7B16%7D+%5Cend%7Balign%7D\" alt=\"\\begin{align} \\sum\\limits_{j = 1}^m {{\\gamma _{ji}}}  + \\lambda {\\alpha _i} = 0\\\\  \\Rightarrow {\\alpha _i} = \\frac{1}{m}\\sum\\limits_{j = 1}^m {{\\gamma _{ji}}}  \\tag{16} \\end{align}\" eeimg=\"1\"/></p><p>可以采用 EM 算法迭代求解：1） 固定参数 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cleft%5C%7B+%7B%5Cleft%28+%7B%7B%5Calpha+_i%7D%2C%7B%5Cmu+_i%7D%2C%7B%5CSigma+_i%7D%7D+%5Cright%29%7C1+%5Cle+i+%5Cle+k%7D+%5Cright%5C%7D%5C%5D\" alt=\"\\[\\left\\{ {\\left( {{\\alpha _i},{\\mu _i},{\\Sigma _i}} \\right)|1 \\le i \\le k} \\right\\}\\]\" eeimg=\"1\"/> ，根据公式 (6) 计算每个样本属于每个高斯成分的后验概率 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5Cgamma+_%7Bji%7D%7D%7D%5C%5D\" alt=\"\\[{{\\gamma _{ji}}}\\]\" eeimg=\"1\"/> ；2）再根据公式 (11)、(14)、(16) 更新模型参数 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cleft%5C%7B+%7B%5Cleft%28+%7B%7B%5Calpha+_i%7D%2C%7B%5Cmu+_i%7D%2C%7B%5CSigma+_i%7D%7D+%5Cright%29%7C1+%5Cle+i+%5Cle+k%7D+%5Cright%5C%7D%5C%5D\" alt=\"\\[\\left\\{ {\\left( {{\\alpha _i},{\\mu _i},{\\Sigma _i}} \\right)|1 \\le i \\le k} \\right\\}\\]\" eeimg=\"1\"/> 。具体算法如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-46cf44a2a7fc3cac31f9e181b4481777_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"537\" data-rawheight=\"472\" class=\"origin_image zh-lightbox-thumb\" width=\"537\" data-original=\"https://pic4.zhimg.com/v2-46cf44a2a7fc3cac31f9e181b4481777_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;537&#39; height=&#39;472&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"537\" data-rawheight=\"472\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"537\" data-original=\"https://pic4.zhimg.com/v2-46cf44a2a7fc3cac31f9e181b4481777_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-46cf44a2a7fc3cac31f9e181b4481777_b.jpg\"/></figure><div class=\"highlight\"><pre><code class=\"language-text\">from sklearn.mixture import GaussianMixture\ngmm = GaussianMixture(n_components=4)\ngmm.fit(X)\ny_pred = gmm.predict(X)\nplt.scatter(X[:, 0], X[:, 1], c=y_pred)\nplt.show()</code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-1ddac070848e4fc1715c3fe448929f11_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"362\" data-rawheight=\"237\" class=\"content_image\" width=\"362\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;362&#39; height=&#39;237&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"362\" data-rawheight=\"237\" class=\"content_image lazy\" width=\"362\" data-actualsrc=\"https://pic2.zhimg.com/v2-1ddac070848e4fc1715c3fe448929f11_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "人工智能算法", 
                    "tagLink": "https://api.zhihu.com/topics/19691108"
                }, 
                {
                    "tag": "聚类算法", 
                    "tagLink": "https://api.zhihu.com/topics/19627785"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/45497946", 
            "userName": "徽州风韵", 
            "userLink": "https://www.zhihu.com/people/53bd6f8b93873b03c5edb714aea2a456", 
            "upvote": 0, 
            "title": "文章内容重复检测算法", 
            "content": "<p>对于一些比较热门的博客，每天有大量的文章提交，不可避免地要解决文章重复的问题。要求能做到实时检测，同时存储空间开销尽量小。</p><h2><b>一、Jaccard相似</b></h2><p> 集合 <img src=\"https://www.zhihu.com/equation?tex=S\" alt=\"S\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=T\" alt=\"T\" eeimg=\"1\"/> 的 Jaccard 相似定义为：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7BSIM%7D%7D%5Cleft%28+%7BS%2CT%7D+%5Cright%29+%3D+%5Cfrac%7B%7B%5Cleft%7C+%7BS+%5Ccap+T%7D+%5Cright%7C%7D%7D%7B%7B%5Cleft%7C+%7BS+%5Ccup+T%7D+%5Cright%7C%7D%7D+%5Ctag%7B1%7D+%5C%5D\" alt=\"\\[{\\rm{SIM}}\\left( {S,T} \\right) = \\frac{{\\left| {S \\cap T} \\right|}}{{\\left| {S \\cup T} \\right|}} \\tag{1} \\]\" eeimg=\"1\"/> </p><p>例如图 1 中 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7BSIM%7D%7D%5Cleft%28+%7BS%2CT%7D+%5Cright%29+%3D+%5Cfrac%7B3%7D%7B8%7D%5C%5D\" alt=\"\\[{\\rm{SIM}}\\left( {S,T} \\right) = \\frac{3}{8}\\]\" eeimg=\"1\"/> ，Jaccard 的值越大，两个集合重合度越大。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-57a4fac663be823b519e5cd8ed5ac412_b.jpg\" data-size=\"normal\" data-rawwidth=\"254\" data-rawheight=\"195\" class=\"content_image\" width=\"254\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;254&#39; height=&#39;195&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"254\" data-rawheight=\"195\" class=\"content_image lazy\" width=\"254\" data-actualsrc=\"https://pic3.zhimg.com/v2-57a4fac663be823b519e5cd8ed5ac412_b.jpg\"/><figcaption>图 1</figcaption></figure><p>所以我们可以采用 Jaccard 衡量两篇文章是否重复，这里从文章字符集层面比较两篇文章的重复度。</p><h2><b>二、Minhash</b></h2><h2><b>2. 1 k-shingles</b></h2><p>我们可以将文章的内容看成字符串，定义 k-shingles 为该文章中所有长度为 k 的子字符串，所以任意一篇文章都可以用  k-shingles 集合表示。文章 <img src=\"https://www.zhihu.com/equation?tex=%5Crm%7BD%3D%E2%80%9Cabcdabd%E2%80%9D%7D\" alt=\"\\rm{D=“abcdabd”}\" eeimg=\"1\"/> ，则2-shingles 集合为 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cleft%5C%7B+%7B%7B%5Crm%7Bab%2Cbc%2Ccd%2Cda%2Cbd%7D%7D%7D+%5Cright%5C%7D%5C%5D\" alt=\"\\[\\left\\{ {{\\rm{ab,bc,cd,da,bd}}} \\right\\}\\]\" eeimg=\"1\"/> ，我们注意到在文档 <img src=\"https://www.zhihu.com/equation?tex=D\" alt=\"D\" eeimg=\"1\"/> 中 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5Crm%7Bab%7D%7D%7D%5C%5D\" alt=\"\\[{{\\rm{ab}}}\\]\" eeimg=\"1\"/> 出现两次，但是在 2-shingles 集合中仅仅出现一次，所以这里的 k-Shingles 是一个 set 而不是 bag。</p><p>我们如何确定 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7Bk%7D%7D%5C%5D\" alt=\"\\[{\\rm{k}}\\]\" eeimg=\"1\"/> 值的大小呢？如果 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7Bk%7D%7D%5C%5D\" alt=\"\\[{\\rm{k}}\\]\" eeimg=\"1\"/> 取值非常小，那么长度为 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7Bk%7D%7D%5C%5D\" alt=\"\\[{\\rm{k}}\\]\" eeimg=\"1\"/> 的子字符串 shingle 会出现在大多数文章中，例如 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7Bk+%3D+1%7D%7D%5C%5D\" alt=\"\\[{\\rm{k = 1}}\\]\" eeimg=\"1\"/> ，大部分文章有相同的字符，所有文章之间的相似性都很高；所以 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7Bk%7D%7D%5C%5D\" alt=\"\\[{\\rm{k}}\\]\" eeimg=\"1\"/> 的取值应该足够的大，保证每个 shingle 出现在所有文章中的概率比较低。</p><h2><b>2.2 Hashing Shingles</b></h2><p>不直接将子字符串作为 shingle，我们采用 hash 函数将长度为 k 的子字符串映射到 hash 桶中，将 hash 桶的编号作为 shingle，那么文档就可以由 hash 值的集合表示。例如我们可以将   4-shingles 映射到 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cleft%5B+%7B0%2C%7B2%5E%7B32%7D%7D+-+1%7D+%5Cright%5D%5C%5D\" alt=\"\\[\\left[ {0,{2^{32}} - 1} \\right]\\]\" eeimg=\"1\"/> （假设文档有 <img src=\"https://www.zhihu.com/equation?tex=20\" alt=\"20\" eeimg=\"1\"/> 字符， <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B20%5E4%7D+%5Cll+%7B2%5E%7B32%7D%7D+-+1%5C%5D\" alt=\"\\[{20^4} \\ll {2^{32}} - 1\\]\" eeimg=\"1\"/> 不会出现太多的冲突）。</p><h2><b>2.3 Minhashing and Jaccard Similarity</b></h2><p>假设有四篇文章分别为 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BS_1%7D+%3D+%5Cleft%5C%7B+%7Ba%2Cd%7D+%5Cright%5C%7D%2C%7BS_2%7D+%3D+%5Cleft%5C%7B+c+%5Cright%5C%7D%2C%7BS_3%7D+%3D+%5Cleft%5C%7B+%7Bb%2Cd%2Ce%7D+%5Cright%5C%7D%2C%7BS_4%7D+%3D+%5Cleft%5C%7B+%7Ba%2Cc%2Cd%7D+%5Cright%5C%7D%5C%5D\" alt=\"\\[{S_1} = \\left\\{ {a,d} \\right\\},{S_2} = \\left\\{ c \\right\\},{S_3} = \\left\\{ {b,d,e} \\right\\},{S_4} = \\left\\{ {a,c,d} \\right\\}\\]\" eeimg=\"1\"/> ，可以用下面的矩阵表示，矩阵的元素只有 <img src=\"https://www.zhihu.com/equation?tex=0%2C1\" alt=\"0,1\" eeimg=\"1\"/> 两种取值， <img src=\"https://www.zhihu.com/equation?tex=1\" alt=\"1\" eeimg=\"1\"/> 表示文档含有对应的字符， <img src=\"https://www.zhihu.com/equation?tex=0\" alt=\"0\" eeimg=\"1\"/> 表示文档不含有对应的字符。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-839b93ea7011f37bfef0b802aa696769_b.jpg\" data-size=\"normal\" data-rawwidth=\"356\" data-rawheight=\"205\" class=\"content_image\" width=\"356\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;356&#39; height=&#39;205&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"356\" data-rawheight=\"205\" class=\"content_image lazy\" width=\"356\" data-actualsrc=\"https://pic2.zhimg.com/v2-839b93ea7011f37bfef0b802aa696769_b.jpg\"/><figcaption>图 2</figcaption></figure><p>随机打乱字符集的顺序，例如：原来的顺序为 <img src=\"https://www.zhihu.com/equation?tex=%5C%5Babcde%5C%5D\" alt=\"\\[abcde\\]\" eeimg=\"1\"/> ，打乱后的顺序为 <img src=\"https://www.zhihu.com/equation?tex=%5C%5Bbeadc%5C%5D\" alt=\"\\[beadc\\]\" eeimg=\"1\"/> ，那么矩阵的元素也会相应地发生变化，我们把打乱后对应的矩阵的每一列第一个为1的元素对应的字符作为此次对应文档的 minhash 值， <img src=\"https://www.zhihu.com/equation?tex=%5C%5Bh%5Cleft%28+%7B%7BS_1%7D%7D+%5Cright%29+%3D+a%2Ch%5Cleft%28+%7B%7BS_2%7D%7D+%5Cright%29+%3D+c%2Ch%5Cleft%28+%7B%7BS_3%7D%7D+%5Cright%29+%3D+b%2Ch%5Cleft%28+%7B%7BS_4%7D%7D+%5Cright%29+%3D+a%5C%5D\" alt=\"\\[h\\left( {{S_1}} \\right) = a,h\\left( {{S_2}} \\right) = c,h\\left( {{S_3}} \\right) = b,h\\left( {{S_4}} \\right) = a\\]\" eeimg=\"1\"/> 如下所示：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-4a3efe45dffb1d257bd533b88ac1ee04_b.jpg\" data-size=\"normal\" data-rawwidth=\"368\" data-rawheight=\"201\" class=\"content_image\" width=\"368\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;368&#39; height=&#39;201&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"368\" data-rawheight=\"201\" class=\"content_image lazy\" width=\"368\" data-actualsrc=\"https://pic1.zhimg.com/v2-4a3efe45dffb1d257bd533b88ac1ee04_b.jpg\"/><figcaption>图 3</figcaption></figure><p>minhash 和 Jaccard 相似之间有很重要的联系，我们以 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BS_1%7D%5C%5D\" alt=\"\\[{S_1}\\]\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BS_2%7D%5C%5D\" alt=\"\\[{S_2}\\]\" eeimg=\"1\"/> 说明，对于某一行 ，<img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BS_1%7D%5C%5D\" alt=\"\\[{S_1}\\]\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BS_2%7D%5C%5D\" alt=\"\\[{S_2}\\]\" eeimg=\"1\"/> 的取值分三种情况：</p><ol><li>type <img src=\"https://www.zhihu.com/equation?tex=%5Crm%7BX%7D\" alt=\"\\rm{X}\" eeimg=\"1\"/> ：<img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BS_1%7D%5C%5D\" alt=\"\\[{S_1}\\]\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BS_2%7D%5C%5D\" alt=\"\\[{S_2}\\]\" eeimg=\"1\"/> 取值都为 <img src=\"https://www.zhihu.com/equation?tex=1\" alt=\"1\" eeimg=\"1\"/> ；</li><li>type <img src=\"https://www.zhihu.com/equation?tex=%5Crm%7BY%7D\" alt=\"\\rm{Y}\" eeimg=\"1\"/> ：<img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BS_1%7D%5C%5D\" alt=\"\\[{S_1}\\]\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BS_2%7D%5C%5D\" alt=\"\\[{S_2}\\]\" eeimg=\"1\"/>一个取值为 <img src=\"https://www.zhihu.com/equation?tex=1\" alt=\"1\" eeimg=\"1\"/> ，另外一个取值为 <img src=\"https://www.zhihu.com/equation?tex=0\" alt=\"0\" eeimg=\"1\"/> ；</li><li>type <img src=\"https://www.zhihu.com/equation?tex=%5Crm%7BZ%7D\" alt=\"\\rm{Z}\" eeimg=\"1\"/> ：<img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BS_1%7D%5C%5D\" alt=\"\\[{S_1}\\]\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BS_2%7D%5C%5D\" alt=\"\\[{S_2}\\]\" eeimg=\"1\"/> 取值都为 <img src=\"https://www.zhihu.com/equation?tex=0\" alt=\"0\" eeimg=\"1\"/> 。</li></ol><p>假设矩阵中有 <img src=\"https://www.zhihu.com/equation?tex=x\" alt=\"x\" eeimg=\"1\"/> 行 type <img src=\"https://www.zhihu.com/equation?tex=%5Crm%7BX%7D\" alt=\"\\rm{X}\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=y\" alt=\"y\" eeimg=\"1\"/> 行 type <img src=\"https://www.zhihu.com/equation?tex=%5Crm%7BY%7D\" alt=\"\\rm{Y}\" eeimg=\"1\"/>， <img src=\"https://www.zhihu.com/equation?tex=x\" alt=\"x\" eeimg=\"1\"/> 为 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BS_1%7D+%5Ccap+%7BS_2%7D%5C%5D\" alt=\"\\[{S_1} \\cap {S_2}\\]\" eeimg=\"1\"/> 的大小， <img src=\"https://www.zhihu.com/equation?tex=y\" alt=\"y\" eeimg=\"1\"/> 为 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BS_1%7D+%5Ccup+%7BS_2%7D%5C%5D\" alt=\"\\[{S_1} \\cup {S_2}\\]\" eeimg=\"1\"/> 的大小。所以根据 Jaccard 的定义可得：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7BSIM%7D%7D%5Cleft%28+%7B%7BS_1%7D%2C%7BS_2%7D%7D+%5Cright%29+%3D+%5Cfrac%7Bx%7D%7B%7Bx+%2B+y%7D%7D+%5Ctag%7B2%7D+%5C%5D\" alt=\"\\[{\\rm{SIM}}\\left( {{S_1},{S_2}} \\right) = \\frac{x}{{x + y}} \\tag{2} \\]\" eeimg=\"1\"/> </p><p>接下来我们看看 <img src=\"https://www.zhihu.com/equation?tex=%5C%5Bh%5Cleft%28+%7B%7BS_1%7D%7D+%5Cright%29+%3D+h%5Cleft%28+%7B%7BS_2%7D%7D+%5Cright%29%5C%5D\" alt=\"\\[h\\left( {{S_1}} \\right) = h\\left( {{S_2}} \\right)\\]\" eeimg=\"1\"/> 的概率是多少？先随机打乱 rows 的顺序，那么矩阵的值也发生相应的变化，然后从上往下遍历 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BS_1%7D%5C%5D\" alt=\"\\[{S_1}\\]\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BS_2%7D%5C%5D\" alt=\"\\[{S_2}\\]\" eeimg=\"1\"/> 的元素，如果先遇到 type <img src=\"https://www.zhihu.com/equation?tex=%5Crm%7BX%7D\" alt=\"\\rm{X}\" eeimg=\"1\"/> 类型的行，那么 <img src=\"https://www.zhihu.com/equation?tex=%5C%5Bh%5Cleft%28+%7B%7BS_1%7D%7D+%5Cright%29+%3D+h%5Cleft%28+%7B%7BS_2%7D%7D+%5Cright%29%5C%5D\" alt=\"\\[h\\left( {{S_1}} \\right) = h\\left( {{S_2}} \\right)\\]\" eeimg=\"1\"/> ，如果先遇到 type <img src=\"https://www.zhihu.com/equation?tex=%5Crm%7BY%7D\" alt=\"\\rm{Y}\" eeimg=\"1\"/> 类型的行，那么 <img src=\"https://www.zhihu.com/equation?tex=%5C%5Bh%5Cleft%28+%7B%7BS_1%7D%7D+%5Cright%29+%5Cne+h%5Cleft%28+%7B%7BS_2%7D%7D+%5Cright%29%5C%5D\" alt=\"\\[h\\left( {{S_1}} \\right) \\ne h\\left( {{S_2}} \\right)\\]\" eeimg=\"1\"/> ，如果先遇到 type <img src=\"https://www.zhihu.com/equation?tex=%5Crm%7BZ%7D\" alt=\"\\rm{Z}\" eeimg=\"1\"/> 类型的行，那么跳过该行，继续往下遍历。所以 <img src=\"https://www.zhihu.com/equation?tex=%5C%5Bh%5Cleft%28+%7B%7BS_1%7D%7D+%5Cright%29+%3D+h%5Cleft%28+%7B%7BS_2%7D%7D+%5Cright%29%5C%5D\" alt=\"\\[h\\left( {{S_1}} \\right) = h\\left( {{S_2}} \\right)\\]\" eeimg=\"1\"/> 的概率为：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5BP%5Cleft%28+%7Bh%5Cleft%28+%7B%7BS_1%7D%7D+%5Cright%29+%3D+h%5Cleft%28+%7B%7BS_2%7D%7D+%5Cright%29%7D+%5Cright%29+%3D+%5Cfrac%7Bx%7D%7B%7Bx+%2B+y%7D%7D+%5Ctag%7B3%7D+%5C%5D\" alt=\"\\[P\\left( {h\\left( {{S_1}} \\right) = h\\left( {{S_2}} \\right)} \\right) = \\frac{x}{{x + y}} \\tag{3} \\]\" eeimg=\"1\"/> </p><p>得到一个重要的结论：<b>对于 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BS_1%7D%5C%5D\" alt=\"\\[{S_1}\\]\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BS_2%7D%5C%5D\" alt=\"\\[{S_2}\\]\" eeimg=\"1\"/>，它们的 minhash 值相等的概率等于它们的 Jaccard相似度。</b></p><h2><b>2.4 Minhash Signatures</b></h2><p>随机打乱 <img src=\"https://www.zhihu.com/equation?tex=n\" alt=\"n\" eeimg=\"1\"/> 次矩阵的行（ <img src=\"https://www.zhihu.com/equation?tex=n\" alt=\"n\" eeimg=\"1\"/> 的值远小于矩阵的行数 <img src=\"https://www.zhihu.com/equation?tex=M\" alt=\"M\" eeimg=\"1\"/> ），对于文档 <img src=\"https://www.zhihu.com/equation?tex=S\" alt=\"S\" eeimg=\"1\"/> ，可以得到 <img src=\"https://www.zhihu.com/equation?tex=n\" alt=\"n\" eeimg=\"1\"/> 个 minhash 的值，组成的集合 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cleft%5B+%7B%7Bh_1%7D%5Cleft%28+S+%5Cright%29%2C%7Bh_1%7D%5Cleft%28+S+%5Cright%29%2C+%5Ccdot++%5Ccdot++%5Ccdot+%2C%7Bh_n%7D%5Cleft%28+S+%5Cright%29%7D+%5Cright%5D%5C%5D\" alt=\"\\[\\left[ {{h_1}\\left( S \\right),{h_1}\\left( S \\right), \\cdot  \\cdot  \\cdot ,{h_n}\\left( S \\right)} \\right]\\]\" eeimg=\"1\"/> 称为 <b>Minhash Signatures。</b>如果矩阵很大的话，每次通过打乱矩阵的行得到 minhash 的值，这种方法是不可行的，我们可以通过 hash 函数模拟随机打乱的效果，选取 <img src=\"https://www.zhihu.com/equation?tex=n\" alt=\"n\" eeimg=\"1\"/> 个 hash 函数 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bh_1%7D%2C%7Bh_1%7D%2C+%5Ccdot++%5Ccdot++%5Ccdot+%2C%7Bh_n%7D%5C%5D\" alt=\"\\[{h_1},{h_1}, \\cdot  \\cdot  \\cdot ,{h_n}\\]\" eeimg=\"1\"/> ，计算签名矩阵 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7BSIG%7D%7D%5C%5D\" alt=\"\\[{\\rm{SIG}}\\]\" eeimg=\"1\"/> ， <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7BSIG%7D%7D%5Cleft%28+%7Bi%2Cc%7D+%5Cright%29%5C%5D\" alt=\"\\[{\\rm{SIG}}\\left( {i,c} \\right)\\]\" eeimg=\"1\"/> 表示第 <img src=\"https://www.zhihu.com/equation?tex=c\" alt=\"c\" eeimg=\"1\"/> 列对应第 <img src=\"https://www.zhihu.com/equation?tex=i\" alt=\"i\" eeimg=\"1\"/> 个hash函数的 minhash 值。</p><blockquote>1. Compute <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bh_1%7D%5Cleft%28+r+%5Cright%29%2C%7Bh_1%7D%5Cleft%28+r+%5Cright%29%2C+%5Ccdot++%5Ccdot++%5Ccdot+%2C%7Bh_n%7D%5Cleft%28+r+%5Cright%29%5C%5D\" alt=\"\\[{h_1}\\left( r \\right),{h_1}\\left( r \\right), \\cdot  \\cdot  \\cdot ,{h_n}\\left( r \\right)\\]\" eeimg=\"1\"/> ，where <img src=\"https://www.zhihu.com/equation?tex=%5C%5Br+%3D+%5Cleft%5B+%7B0%2C1%2C+%5Ccdot++%5Ccdot++%5Ccdot+%2CM%7D+%5Cright%5D%5C%5D\" alt=\"\\[r = \\left[ {0,1, \\cdot  \\cdot  \\cdot ,M} \\right]\\]\" eeimg=\"1\"/><br/>2. Initialization <img src=\"https://www.zhihu.com/equation?tex=%7B%5Crm%7BSIG%7D%7D%5Cleft%28+%7Bi%2Cc%7D+%5Cright%29+%3D+%5Cinfty+%5C+for%5C+all%5C+i+%5C+and%5C+c\" alt=\"{\\rm{SIG}}\\left( {i,c} \\right) = \\infty \\ for\\ all\\ i \\ and\\ c\" eeimg=\"1\"/><br/>3. For each column <img src=\"https://www.zhihu.com/equation?tex=c\" alt=\"c\" eeimg=\"1\"/> do the following:<br/>    (a) If <img src=\"https://www.zhihu.com/equation?tex=c\" alt=\"c\" eeimg=\"1\"/> has <img src=\"https://www.zhihu.com/equation?tex=0\" alt=\"0\" eeimg=\"1\"/> in row <img src=\"https://www.zhihu.com/equation?tex=r\" alt=\"r\" eeimg=\"1\"/> , do nothing<br/>    (b) If <img src=\"https://www.zhihu.com/equation?tex=c\" alt=\"c\" eeimg=\"1\"/> has <img src=\"https://www.zhihu.com/equation?tex=1\" alt=\"1\" eeimg=\"1\"/> in row <img src=\"https://www.zhihu.com/equation?tex=r\" alt=\"r\" eeimg=\"1\"/> ,then for each <img src=\"https://www.zhihu.com/equation?tex=%5C%5Bi+%3D+1%2C2%2C+%5Ccdot++%5Ccdot++%5Ccdot+%2Cn%5C%5D\" alt=\"\\[i = 1,2, \\cdot  \\cdot  \\cdot ,n\\]\" eeimg=\"1\"/> set <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7BSIG%7D%7D%5Cleft%28+%7Bi%2Cc%7D+%5Cright%29%5C%5D\" alt=\"\\[{\\rm{SIG}}\\left( {i,c} \\right)\\]\" eeimg=\"1\"/> to the<br/>        smaller of the current value of  <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7BSIG%7D%7D%5Cleft%28+%7Bi%2Cc%7D+%5Cright%29%5C%5D\" alt=\"\\[{\\rm{SIG}}\\left( {i,c} \\right)\\]\" eeimg=\"1\"/> and <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bh_i%7D%5Cleft%28+r+%5Cright%29%5C%5D\" alt=\"\\[{h_i}\\left( r \\right)\\]\" eeimg=\"1\"/> </blockquote><p>1. 取两个hash函数分别为 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bh_1%7D%5Cleft%28+x+%5Cright%29+%3D+%5Cleft%28+%7Bx+%2B+1%7D+%5Cright%29%5C%25+5%5C%5D\" alt=\"\\[{h_1}\\left( x \\right) = \\left( {x + 1} \\right)\\% 5\\]\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bh_2%7D%5Cleft%28+x+%5Cright%29+%3D+%5Cleft%28+%7B3x+%2B+1%7D+%5Cright%29%5C%25+5%5C%5D\" alt=\"\\[{h_2}\\left( x \\right) = \\left( {3x + 1} \\right)\\% 5\\]\" eeimg=\"1\"/> </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-7193deb0713789fa9a79d5bab401ff6b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"705\" data-rawheight=\"195\" class=\"origin_image zh-lightbox-thumb\" width=\"705\" data-original=\"https://pic4.zhimg.com/v2-7193deb0713789fa9a79d5bab401ff6b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;705&#39; height=&#39;195&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"705\" data-rawheight=\"195\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"705\" data-original=\"https://pic4.zhimg.com/v2-7193deb0713789fa9a79d5bab401ff6b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-7193deb0713789fa9a79d5bab401ff6b_b.jpg\"/></figure><p>2. 初始化 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7BSIG%7D%7D%5C%5D\" alt=\"\\[{\\rm{SIG}}\\]\" eeimg=\"1\"/> 矩阵</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-3ab3aa7ab374f318b21f59af1e2b90cf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"298\" data-rawheight=\"109\" class=\"content_image\" width=\"298\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;298&#39; height=&#39;109&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"298\" data-rawheight=\"109\" class=\"content_image lazy\" width=\"298\" data-actualsrc=\"https://pic4.zhimg.com/v2-3ab3aa7ab374f318b21f59af1e2b90cf_b.jpg\"/></figure><p><b>3.1. 第 0 行：</b><img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7BS_1%7D%7D%5C%5D\" alt=\"\\[{{S_1}}\\]\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BS_4%7D%5C%5D\" alt=\"\\[{S_4}\\]\" eeimg=\"1\"/> 的元素为 <img src=\"https://www.zhihu.com/equation?tex=1\" alt=\"1\" eeimg=\"1\"/>，比较 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bh_1%7D%5Cleft%28+0+%5Cright%29+%3D+1%2C%7Bh_2%7D%5Cleft%28+0+%5Cright%29+%3D+1%5C%5D\" alt=\"\\[{h_1}\\left( 0 \\right) = 1,{h_2}\\left( 0 \\right) = 1\\]\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7BSIG%7D%7D%5C%5D\" alt=\"\\[{\\rm{SIG}}\\]\" eeimg=\"1\"/> 中 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BS_1%7D%2C%7BS_4%7D%5C%5D\" alt=\"\\[{S_1},{S_4}\\]\" eeimg=\"1\"/> 对应的值。</p><p>    (a_1) <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bh_1%7D%5Cleft%28+0+%5Cright%29+%3D+1+%3C+%7B%5Crm%7BSIG%7D%7D%5Cleft%28+%7B%7Bh_1%7D%2C%7BS_1%7D%7D+%5Cright%29+%3D+%5Cinfty++%5CRightarrow+%7B%5Crm%7BSIG%7D%7D%5Cleft%28+%7B%7Bh_1%7D%2C%7BS_1%7D%7D+%5Cright%29+%3D+1%5C%5D\" alt=\"\\[{h_1}\\left( 0 \\right) = 1 &lt; {\\rm{SIG}}\\left( {{h_1},{S_1}} \\right) = \\infty  \\Rightarrow {\\rm{SIG}}\\left( {{h_1},{S_1}} \\right) = 1\\]\" eeimg=\"1\"/> </p><p>    (a_2) <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bh_2%7D%5Cleft%28+0+%5Cright%29+%3D+1+%3C+%7B%5Crm%7BSIG%7D%7D%5Cleft%28+%7B%7Bh_2%7D%2C%7BS_1%7D%7D+%5Cright%29+%3D+%5Cinfty++%5CRightarrow+%7B%5Crm%7BSIG%7D%7D%5Cleft%28+%7B%7Bh_2%7D%2C%7BS_1%7D%7D+%5Cright%29+%3D+1%5C%5D\" alt=\"\\[{h_2}\\left( 0 \\right) = 1 &lt; {\\rm{SIG}}\\left( {{h_2},{S_1}} \\right) = \\infty  \\Rightarrow {\\rm{SIG}}\\left( {{h_2},{S_1}} \\right) = 1\\]\" eeimg=\"1\"/> </p><p>    (b_1) <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bh_1%7D%5Cleft%28+0+%5Cright%29+%3D+1+%3C+%7B%5Crm%7BSIG%7D%7D%5Cleft%28+%7B%7Bh_1%7D%2C%7BS_4%7D%7D+%5Cright%29+%3D+%5Cinfty++%5CRightarrow+%7B%5Crm%7BSIG%7D%7D%5Cleft%28+%7B%7Bh_1%7D%2C%7BS_4%7D%7D+%5Cright%29+%3D+1%5C%5D\" alt=\"\\[{h_1}\\left( 0 \\right) = 1 &lt; {\\rm{SIG}}\\left( {{h_1},{S_4}} \\right) = \\infty  \\Rightarrow {\\rm{SIG}}\\left( {{h_1},{S_4}} \\right) = 1\\]\" eeimg=\"1\"/> </p><p>    (b_2) <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bh_2%7D%5Cleft%28+0+%5Cright%29+%3D+1+%3C+%7B%5Crm%7BSIG%7D%7D%5Cleft%28+%7B%7Bh_2%7D%2C%7BS_4%7D%7D+%5Cright%29+%3D+%5Cinfty++%5CRightarrow+%7B%5Crm%7BSIG%7D%7D%5Cleft%28+%7B%7Bh_2%7D%2C%7BS_4%7D%7D+%5Cright%29+%3D+1%5C%5D\" alt=\"\\[{h_2}\\left( 0 \\right) = 1 &lt; {\\rm{SIG}}\\left( {{h_2},{S_4}} \\right) = \\infty  \\Rightarrow {\\rm{SIG}}\\left( {{h_2},{S_4}} \\right) = 1\\]\" eeimg=\"1\"/> </p><p>所以 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7BSIG%7D%7D%5C%5D\" alt=\"\\[{\\rm{SIG}}\\]\" eeimg=\"1\"/> 矩阵变为：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b183e6d2b5aa1bbe0afffdf632389911_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"315\" data-rawheight=\"107\" class=\"content_image\" width=\"315\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;315&#39; height=&#39;107&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"315\" data-rawheight=\"107\" class=\"content_image lazy\" width=\"315\" data-actualsrc=\"https://pic2.zhimg.com/v2-b183e6d2b5aa1bbe0afffdf632389911_b.jpg\"/></figure><p><b>3.2. 第 1 行：</b> <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BS_3%7D%5C%5D\" alt=\"\\[{S_3}\\]\" eeimg=\"1\"/> 的元素为 <img src=\"https://www.zhihu.com/equation?tex=1\" alt=\"1\" eeimg=\"1\"/>，比较 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bh_1%7D%5Cleft%28+1+%5Cright%29+%3D+2%2C%7Bh_2%7D%5Cleft%28+1+%5Cright%29+%3D+4%5C%5D\" alt=\"\\[{h_1}\\left( 1 \\right) = 2,{h_2}\\left( 1 \\right) = 4\\]\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7BSIG%7D%7D%5C%5D\" alt=\"\\[{\\rm{SIG}}\\]\" eeimg=\"1\"/> 中 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BS_3%7D%5C%5D\" alt=\"\\[{S_3}\\]\" eeimg=\"1\"/> 对应的值。</p><p>    (a_1) <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bh_1%7D%5Cleft%28+1+%5Cright%29+%3D+2+%3C+%7B%5Crm%7BSIG%7D%7D%5Cleft%28+%7B%7Bh_1%7D%2C%7BS_3%7D%7D+%5Cright%29+%3D+%5Cinfty++%5CRightarrow+%7B%5Crm%7BSIG%7D%7D%5Cleft%28+%7B%7Bh_1%7D%2C%7BS_3%7D%7D+%5Cright%29+%3D+2%5C%5D\" alt=\"\\[{h_1}\\left( 1 \\right) = 2 &lt; {\\rm{SIG}}\\left( {{h_1},{S_3}} \\right) = \\infty  \\Rightarrow {\\rm{SIG}}\\left( {{h_1},{S_3}} \\right) = 2\\]\" eeimg=\"1\"/> </p><p>    (a_2) <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bh_2%7D%5Cleft%28+1+%5Cright%29+%3D+4+%3C+%7B%5Crm%7BSIG%7D%7D%5Cleft%28+%7B%7Bh_2%7D%2C%7BS_3%7D%7D+%5Cright%29+%3D+%5Cinfty++%5CRightarrow+%7B%5Crm%7BSIG%7D%7D%5Cleft%28+%7B%7Bh_2%7D%2C%7BS_3%7D%7D+%5Cright%29+%3D+4%5C%5D\" alt=\"\\[{h_2}\\left( 1 \\right) = 4 &lt; {\\rm{SIG}}\\left( {{h_2},{S_3}} \\right) = \\infty  \\Rightarrow {\\rm{SIG}}\\left( {{h_2},{S_3}} \\right) = 4\\]\" eeimg=\"1\"/> </p><p>所以 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7BSIG%7D%7D%5C%5D\" alt=\"\\[{\\rm{SIG}}\\]\" eeimg=\"1\"/> 矩阵变为：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-9fef9daadb44a8fce366325ede025df7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"315\" data-rawheight=\"107\" class=\"content_image\" width=\"315\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;315&#39; height=&#39;107&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"315\" data-rawheight=\"107\" class=\"content_image lazy\" width=\"315\" data-actualsrc=\"https://pic4.zhimg.com/v2-9fef9daadb44a8fce366325ede025df7_b.jpg\"/></figure><p><b>3.3. 第 2 行：</b><img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7BS_2%7D%7D%5C%5D\" alt=\"\\[{{S_2}}\\]\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BS_4%7D%5C%5D\" alt=\"\\[{S_4}\\]\" eeimg=\"1\"/> 的元素为 <img src=\"https://www.zhihu.com/equation?tex=1\" alt=\"1\" eeimg=\"1\"/>，比较 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bh_1%7D%5Cleft%28+2+%5Cright%29+%3D+3%2C%7Bh_2%7D%5Cleft%28+2+%5Cright%29+%3D+2%5C%5D\" alt=\"\\[{h_1}\\left( 2 \\right) = 3,{h_2}\\left( 2 \\right) = 2\\]\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7BSIG%7D%7D%5C%5D\" alt=\"\\[{\\rm{SIG}}\\]\" eeimg=\"1\"/> 中 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BS_2%7D%2C%7BS_4%7D%5C%5D\" alt=\"\\[{S_2},{S_4}\\]\" eeimg=\"1\"/> 对应的值。</p><p>    (a_1) <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bh_1%7D%5Cleft%28+2+%5Cright%29+%3D+3+%3C+%7B%5Crm%7BSIG%7D%7D%5Cleft%28+%7B%7Bh_1%7D%2C%7BS_2%7D%7D+%5Cright%29+%3D+%5Cinfty++%5CRightarrow+%7B%5Crm%7BSIG%7D%7D%5Cleft%28+%7B%7Bh_1%7D%2C%7BS_2%7D%7D+%5Cright%29+%3D+3%5C%5D\" alt=\"\\[{h_1}\\left( 2 \\right) = 3 &lt; {\\rm{SIG}}\\left( {{h_1},{S_2}} \\right) = \\infty  \\Rightarrow {\\rm{SIG}}\\left( {{h_1},{S_2}} \\right) = 3\\]\" eeimg=\"1\"/> </p><p>    (a_2) <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bh_2%7D%5Cleft%28+2+%5Cright%29+%3D+2+%3C+%7B%5Crm%7BSIG%7D%7D%5Cleft%28+%7B%7Bh_2%7D%2C%7BS_2%7D%7D+%5Cright%29+%3D+%5Cinfty++%5CRightarrow+%7B%5Crm%7BSIG%7D%7D%5Cleft%28+%7B%7Bh_2%7D%2C%7BS_2%7D%7D+%5Cright%29+%3D+2%5C%5D\" alt=\"\\[{h_2}\\left( 2 \\right) = 2 &lt; {\\rm{SIG}}\\left( {{h_2},{S_2}} \\right) = \\infty  \\Rightarrow {\\rm{SIG}}\\left( {{h_2},{S_2}} \\right) = 2\\]\" eeimg=\"1\"/> </p><p>    (b_1) <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bh_1%7D%5Cleft%28+2+%5Cright%29+%3D+3+%3E+%7B%5Crm%7BSIG%7D%7D%5Cleft%28+%7B%7Bh_1%7D%2C%7BS_4%7D%7D+%5Cright%29+%3D+1%5C%5D\" alt=\"\\[{h_1}\\left( 2 \\right) = 3 &gt; {\\rm{SIG}}\\left( {{h_1},{S_4}} \\right) = 1\\]\" eeimg=\"1\"/> 不改变 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7BSIG%7D%7D%5Cleft%28+%7B%7Bh_1%7D%2C%7BS_4%7D%7D+%5Cright%29%5C%5D\" alt=\"\\[{\\rm{SIG}}\\left( {{h_1},{S_4}} \\right)\\]\" eeimg=\"1\"/> 的值</p><p>    (b_2) <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bh_2%7D%5Cleft%28+2+%5Cright%29+%3D+2+%3E+%7B%5Crm%7BSIG%7D%7D%5Cleft%28+%7B%7Bh_2%7D%2C%7BS_4%7D%7D+%5Cright%29+%3D+1%5C%5D\" alt=\"\\[{h_2}\\left( 2 \\right) = 2 &gt; {\\rm{SIG}}\\left( {{h_2},{S_4}} \\right) = 1\\]\" eeimg=\"1\"/> 不改变 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7BSIG%7D%7D%5Cleft%28+%7B%7Bh_2%7D%2C%7BS_4%7D%7D+%5Cright%29%5C%5D\" alt=\"\\[{\\rm{SIG}}\\left( {{h_2},{S_4}} \\right)\\]\" eeimg=\"1\"/> 的值</p><p>所以 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7BSIG%7D%7D%5C%5D\" alt=\"\\[{\\rm{SIG}}\\]\" eeimg=\"1\"/> 矩阵变为：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-a04b9707264cbb5bf4f8478c741daf5d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"305\" data-rawheight=\"107\" class=\"content_image\" width=\"305\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;305&#39; height=&#39;107&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"305\" data-rawheight=\"107\" class=\"content_image lazy\" width=\"305\" data-actualsrc=\"https://pic2.zhimg.com/v2-a04b9707264cbb5bf4f8478c741daf5d_b.jpg\"/></figure><p><b>3.4. 第 3 行：</b> <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BS_1%7D%2C%7BS_3%7D%2C%7BS_4%7D%5C%5D\" alt=\"\\[{S_1},{S_3},{S_4}\\]\" eeimg=\"1\"/> 的元素为1，比较 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bh_1%7D%5Cleft%28+3+%5Cright%29+%3D+4%2C%7Bh_2%7D%5Cleft%28+3+%5Cright%29+%3D+0%5C%5D\" alt=\"\\[{h_1}\\left( 3 \\right) = 4,{h_2}\\left( 3 \\right) = 0\\]\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7BSIG%7D%7D%5C%5D\" alt=\"\\[{\\rm{SIG}}\\]\" eeimg=\"1\"/> 中 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BS_1%7D%2C%7BS_3%7D%5C%5D\" alt=\"\\[{S_1},{S_3}\\]\" eeimg=\"1\"/> <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BS_4%7D%5C%5D\" alt=\"\\[{S_4}\\]\" eeimg=\"1\"/> 对应的值。</p><p>    (a_1) <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bh_1%7D%5Cleft%28+3+%5Cright%29+%3D+4+%3E+%7B%5Crm%7BSIG%7D%7D%5Cleft%28+%7B%7Bh_1%7D%2C%7BS_1%7D%7D+%5Cright%29+%3D+1%5C%5D\" alt=\"\\[{h_1}\\left( 3 \\right) = 4 &gt; {\\rm{SIG}}\\left( {{h_1},{S_1}} \\right) = 1\\]\" eeimg=\"1\"/> 不改变 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7BSIG%7D%7D%5Cleft%28+%7B%7Bh_1%7D%2C%7BS_1%7D%7D+%5Cright%29%5C%5D\" alt=\"\\[{\\rm{SIG}}\\left( {{h_1},{S_1}} \\right)\\]\" eeimg=\"1\"/> 的值</p><p>    (a_2) <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bh_2%7D%5Cleft%28+3+%5Cright%29+%3D+0+%3C+%7B%5Crm%7BSIG%7D%7D%5Cleft%28+%7B%7Bh_2%7D%2C%7BS_1%7D%7D+%5Cright%29+%3D+1+%5CRightarrow+%7B%5Crm%7BSIG%7D%7D%5Cleft%28+%7B%7Bh_2%7D%2C%7BS_1%7D%7D+%5Cright%29+%3D+0%5C%5D\" alt=\"\\[{h_2}\\left( 3 \\right) = 0 &lt; {\\rm{SIG}}\\left( {{h_2},{S_1}} \\right) = 1 \\Rightarrow {\\rm{SIG}}\\left( {{h_2},{S_1}} \\right) = 0\\]\" eeimg=\"1\"/> </p><p>    (b_1) <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bh_1%7D%5Cleft%28+3+%5Cright%29+%3D+4+%3E+%7B%5Crm%7BSIG%7D%7D%5Cleft%28+%7B%7Bh_1%7D%2C%7BS_3%7D%7D+%5Cright%29+%3D+2%5C%5D\" alt=\"\\[{h_1}\\left( 3 \\right) = 4 &gt; {\\rm{SIG}}\\left( {{h_1},{S_3}} \\right) = 2\\]\" eeimg=\"1\"/> 不改变 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7BSIG%7D%7D%5Cleft%28+%7B%7Bh_1%7D%2C%7BS_3%7D%7D+%5Cright%29%5C%5D\" alt=\"\\[{\\rm{SIG}}\\left( {{h_1},{S_3}} \\right)\\]\" eeimg=\"1\"/> 的值</p><p>    (b_2) <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bh_2%7D%5Cleft%28+3+%5Cright%29+%3D+0+%3C+%7B%5Crm%7BSIG%7D%7D%5Cleft%28+%7B%7Bh_2%7D%2C%7BS_3%7D%7D+%5Cright%29+%3D+4+%5CRightarrow+%7B%5Crm%7BSIG%7D%7D%5Cleft%28+%7B%7Bh_2%7D%2C%7BS_3%7D%7D+%5Cright%29+%3D+0%5C%5D\" alt=\"\\[{h_2}\\left( 3 \\right) = 0 &lt; {\\rm{SIG}}\\left( {{h_2},{S_3}} \\right) = 4 \\Rightarrow {\\rm{SIG}}\\left( {{h_2},{S_3}} \\right) = 0\\]\" eeimg=\"1\"/> </p><p>    (c_1) <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bh_1%7D%5Cleft%28+3+%5Cright%29+%3D+4+%3E+%7B%5Crm%7BSIG%7D%7D%5Cleft%28+%7B%7Bh_1%7D%2C%7BS_4%7D%7D+%5Cright%29+%3D+1%5C%5D\" alt=\"\\[{h_1}\\left( 3 \\right) = 4 &gt; {\\rm{SIG}}\\left( {{h_1},{S_4}} \\right) = 1\\]\" eeimg=\"1\"/> 不改变 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7BSIG%7D%7D%5Cleft%28+%7B%7Bh_1%7D%2C%7BS_4%7D%7D+%5Cright%29%5C%5D\" alt=\"\\[{\\rm{SIG}}\\left( {{h_1},{S_4}} \\right)\\]\" eeimg=\"1\"/> 的值</p><p>    (c_2) <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bh_2%7D%5Cleft%28+3+%5Cright%29+%3D+0+%3C+%7B%5Crm%7BSIG%7D%7D%5Cleft%28+%7B%7Bh_2%7D%2C%7BS_4%7D%7D+%5Cright%29+%3D+1+%5CRightarrow+%7B%5Crm%7BSIG%7D%7D%5Cleft%28+%7B%7Bh_2%7D%2C%7BS_4%7D%7D+%5Cright%29+%3D+0%5C%5D\" alt=\"\\[{h_2}\\left( 3 \\right) = 0 &lt; {\\rm{SIG}}\\left( {{h_2},{S_4}} \\right) = 1 \\Rightarrow {\\rm{SIG}}\\left( {{h_2},{S_4}} \\right) = 0\\]\" eeimg=\"1\"/> </p><p>所以 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7BSIG%7D%7D%5C%5D\" alt=\"\\[{\\rm{SIG}}\\]\" eeimg=\"1\"/> 矩阵变为：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-b195e4de6e0963f98be9681493a38963_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"305\" data-rawheight=\"101\" class=\"content_image\" width=\"305\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;305&#39; height=&#39;101&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"305\" data-rawheight=\"101\" class=\"content_image lazy\" width=\"305\" data-actualsrc=\"https://pic4.zhimg.com/v2-b195e4de6e0963f98be9681493a38963_b.jpg\"/></figure><p><b>3.5. 第 4 行：</b> <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BS_3%7D%5C%5D\" alt=\"\\[{S_3}\\]\" eeimg=\"1\"/> 的元素为 <img src=\"https://www.zhihu.com/equation?tex=1\" alt=\"1\" eeimg=\"1\"/>，比较 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bh_1%7D%5Cleft%28+4+%5Cright%29+%3D+0%2C%7Bh_2%7D%5Cleft%28+4+%5Cright%29+%3D+3%5C%5D\" alt=\"\\[{h_1}\\left( 4 \\right) = 0,{h_2}\\left( 4 \\right) = 3\\]\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7BSIG%7D%7D%5C%5D\" alt=\"\\[{\\rm{SIG}}\\]\" eeimg=\"1\"/> 中 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BS_3%7D%5C%5D\" alt=\"\\[{S_3}\\]\" eeimg=\"1\"/> 对应的值。</p><p>    (a_1) <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bh_1%7D%5Cleft%28+4+%5Cright%29+%3D+0+%3C+%7B%5Crm%7BSIG%7D%7D%5Cleft%28+%7B%7Bh_1%7D%2C%7BS_3%7D%7D+%5Cright%29+%3D+2+%5CRightarrow+%7B%5Crm%7BSIG%7D%7D%5Cleft%28+%7B%7Bh_1%7D%2C%7BS_3%7D%7D+%5Cright%29+%3D+0%5C%5D\" alt=\"\\[{h_1}\\left( 4 \\right) = 0 &lt; {\\rm{SIG}}\\left( {{h_1},{S_3}} \\right) = 2 \\Rightarrow {\\rm{SIG}}\\left( {{h_1},{S_3}} \\right) = 0\\]\" eeimg=\"1\"/> </p><p>    (a_2) <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bh_2%7D%5Cleft%28+4+%5Cright%29+%3D+3+%3E+%7B%5Crm%7BSIG%7D%7D%5Cleft%28+%7B%7Bh_2%7D%2C%7BS_3%7D%7D+%5Cright%29+%3D+0%5C%5D\" alt=\"\\[{h_2}\\left( 4 \\right) = 3 &gt; {\\rm{SIG}}\\left( {{h_2},{S_3}} \\right) = 0\\]\" eeimg=\"1\"/> 不改变 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7BSIG%7D%7D%5Cleft%28+%7B%7Bh_2%7D%2C%7BS_3%7D%7D+%5Cright%29%5C%5D\" alt=\"\\[{\\rm{SIG}}\\left( {{h_2},{S_3}} \\right)\\]\" eeimg=\"1\"/> 的值</p><p>所以 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7BSIG%7D%7D%5C%5D\" alt=\"\\[{\\rm{SIG}}\\]\" eeimg=\"1\"/> 矩阵变为：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-430ee00b61818377f5f7ac5d2b0391bd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"305\" data-rawheight=\"104\" class=\"content_image\" width=\"305\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;305&#39; height=&#39;104&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"305\" data-rawheight=\"104\" class=\"content_image lazy\" width=\"305\" data-actualsrc=\"https://pic2.zhimg.com/v2-430ee00b61818377f5f7ac5d2b0391bd_b.jpg\"/></figure><p>从另一个角度求解<img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7BSIG%7D%7D%5C%5D\" alt=\"\\[{\\rm{SIG}}\\]\" eeimg=\"1\"/> 矩阵</p><p>1. 对于 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7BS_1%7D%7D%5C%5D\" alt=\"\\[{{S_1}}\\]\" eeimg=\"1\"/> ，只有 <img src=\"https://www.zhihu.com/equation?tex=0%2C3\" alt=\"0,3\" eeimg=\"1\"/> 两行元素为 <img src=\"https://www.zhihu.com/equation?tex=1\" alt=\"1\" eeimg=\"1\"/> 。</p><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cbegin%7Barray%7D%7Bl%7D+%7B%5Crm%7BSIG%7D%7D%5Cleft%28+%7B%7Bh_1%7D%2C%7BS_1%7D%7D+%5Cright%29+%3D+%5Cmin+%5Cleft%28+%7B%7Bh_1%7D%5Cleft%28+0+%5Cright%29%2C%7Bh_1%7D%5Cleft%28+3+%5Cright%29%7D+%5Cright%29+%3D+%5Cmin+%5Cleft%28+%7B1%2C4%7D+%5Cright%29+%3D+1%5C%5C+%7B%5Crm%7BSIG%7D%7D%5Cleft%28+%7B%7Bh_2%7D%2C%7BS_1%7D%7D+%5Cright%29+%3D+%5Cmin+%5Cleft%28+%7B%7Bh_2%7D%5Cleft%28+0+%5Cright%29%2C%7Bh_2%7D%5Cleft%28+3+%5Cright%29%7D+%5Cright%29+%3D+%5Cmin+%5Cleft%28+%7B1%2C0%7D+%5Cright%29+%3D+0+%5Cend%7Barray%7D%5C%5D\" alt=\"\\[\\begin{array}{l} {\\rm{SIG}}\\left( {{h_1},{S_1}} \\right) = \\min \\left( {{h_1}\\left( 0 \\right),{h_1}\\left( 3 \\right)} \\right) = \\min \\left( {1,4} \\right) = 1\\\\ {\\rm{SIG}}\\left( {{h_2},{S_1}} \\right) = \\min \\left( {{h_2}\\left( 0 \\right),{h_2}\\left( 3 \\right)} \\right) = \\min \\left( {1,0} \\right) = 0 \\end{array}\\]\" eeimg=\"1\"/> </p><p>2. 对于 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7BS_2%7D%7D%5C%5D\" alt=\"\\[{{S_2}}\\]\" eeimg=\"1\"/> ，只有第 <img src=\"https://www.zhihu.com/equation?tex=2\" alt=\"2\" eeimg=\"1\"/> 行元素为1.</p><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cbegin%7Barray%7D%7Bl%7D+%7B%5Crm%7BSIG%7D%7D%5Cleft%28+%7B%7Bh_1%7D%2C%7BS_2%7D%7D+%5Cright%29+%3D+%7Bh_1%7D%5Cleft%28+2+%5Cright%29+%3D+3%5C%5C+%7B%5Crm%7BSIG%7D%7D%5Cleft%28+%7B%7Bh_2%7D%2C%7BS_2%7D%7D+%5Cright%29+%3D+%7Bh_2%7D%5Cleft%28+2+%5Cright%29+%3D+2+%5Cend%7Barray%7D%5C%5D\" alt=\"\\[\\begin{array}{l} {\\rm{SIG}}\\left( {{h_1},{S_2}} \\right) = {h_1}\\left( 2 \\right) = 3\\\\ {\\rm{SIG}}\\left( {{h_2},{S_2}} \\right) = {h_2}\\left( 2 \\right) = 2 \\end{array}\\]\" eeimg=\"1\"/> </p><p>3. 对于 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7BS_3%7D%7D%5C%5D\" alt=\"\\[{{S_3}}\\]\" eeimg=\"1\"/> ，只有 <img src=\"https://www.zhihu.com/equation?tex=1%2C3%2C4\" alt=\"1,3,4\" eeimg=\"1\"/> 两行元素为 <img src=\"https://www.zhihu.com/equation?tex=1\" alt=\"1\" eeimg=\"1\"/> 。</p><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cbegin%7Barray%7D%7Bl%7D+%7B%5Crm%7BSIG%7D%7D%5Cleft%28+%7B%7Bh_1%7D%2C%7BS_3%7D%7D+%5Cright%29+%3D+%5Cmin+%5Cleft%28+%7B%7Bh_1%7D%5Cleft%28+1+%5Cright%29%2C%7Bh_1%7D%5Cleft%28+3+%5Cright%29%2C%7Bh_1%7D%5Cleft%28+4+%5Cright%29%7D+%5Cright%29+%3D+%5Cmin+%5Cleft%28+%7B1%2C4%2C0%7D+%5Cright%29+%3D+0%5C%5C+%7B%5Crm%7BSIG%7D%7D%5Cleft%28+%7B%7Bh_2%7D%2C%7BS_3%7D%7D+%5Cright%29+%3D+%5Cmin+%5Cleft%28+%7B%7Bh_2%7D%5Cleft%28+1+%5Cright%29%2C%7Bh_2%7D%5Cleft%28+3+%5Cright%29%2C%7Bh_2%7D%5Cleft%28+4+%5Cright%29%7D+%5Cright%29+%3D+%5Cmin+%5Cleft%28+%7B1%2C0%2C3%7D+%5Cright%29+%3D+0+%5Cend%7Barray%7D%5C%5D\" alt=\"\\[\\begin{array}{l} {\\rm{SIG}}\\left( {{h_1},{S_3}} \\right) = \\min \\left( {{h_1}\\left( 1 \\right),{h_1}\\left( 3 \\right),{h_1}\\left( 4 \\right)} \\right) = \\min \\left( {1,4,0} \\right) = 0\\\\ {\\rm{SIG}}\\left( {{h_2},{S_3}} \\right) = \\min \\left( {{h_2}\\left( 1 \\right),{h_2}\\left( 3 \\right),{h_2}\\left( 4 \\right)} \\right) = \\min \\left( {1,0,3} \\right) = 0 \\end{array}\\]\" eeimg=\"1\"/> </p><p>4. 对于 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7BS_4%7D%7D%5C%5D\" alt=\"\\[{{S_4}}\\]\" eeimg=\"1\"/> ，只有 <img src=\"https://www.zhihu.com/equation?tex=0%2C2%2C3\" alt=\"0,2,3\" eeimg=\"1\"/> 两行元素为 <img src=\"https://www.zhihu.com/equation?tex=1\" alt=\"1\" eeimg=\"1\"/> 。</p><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cbegin%7Barray%7D%7Bl%7D+%7B%5Crm%7BSIG%7D%7D%5Cleft%28+%7B%7Bh_1%7D%2C%7BS_4%7D%7D+%5Cright%29+%3D+%5Cmin+%5Cleft%28+%7B%7Bh_1%7D%5Cleft%28+0+%5Cright%29%2C%7Bh_1%7D%5Cleft%28+2+%5Cright%29%2C%7Bh_1%7D%5Cleft%28+3+%5Cright%29%7D+%5Cright%29+%3D+%5Cmin+%5Cleft%28+%7B1%2C3%2C4%7D+%5Cright%29+%3D+1%5C%5C+%7B%5Crm%7BSIG%7D%7D%5Cleft%28+%7B%7Bh_2%7D%2C%7BS_4%7D%7D+%5Cright%29+%3D+%5Cmin+%5Cleft%28+%7B%7Bh_2%7D%5Cleft%28+0+%5Cright%29%2C%7Bh_2%7D%5Cleft%28+2+%5Cright%29%2C%7Bh_2%7D%5Cleft%28+3+%5Cright%29%7D+%5Cright%29+%3D+%5Cmin+%5Cleft%28+%7B1%2C2%2C0%7D+%5Cright%29+%3D+0+%5Cend%7Barray%7D%5C%5D\" alt=\"\\[\\begin{array}{l} {\\rm{SIG}}\\left( {{h_1},{S_4}} \\right) = \\min \\left( {{h_1}\\left( 0 \\right),{h_1}\\left( 2 \\right),{h_1}\\left( 3 \\right)} \\right) = \\min \\left( {1,3,4} \\right) = 1\\\\ {\\rm{SIG}}\\left( {{h_2},{S_4}} \\right) = \\min \\left( {{h_2}\\left( 0 \\right),{h_2}\\left( 2 \\right),{h_2}\\left( 3 \\right)} \\right) = \\min \\left( {1,2,0} \\right) = 0 \\end{array}\\]\" eeimg=\"1\"/> </p><p>通过 Signatures 矩阵估计的 Jaccard 相似度为：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7BSIM%7D%7D%5Cleft%28+%7B%7BS_1%7D%2C%7BS_2%7D%7D+%5Cright%29+%3D+0%5C%5C+%7B%5Crm%7BSIM%7D%7D%5Cleft%28+%7B%7BS_1%7D%2C%7BS_3%7D%7D+%5Cright%29+%3D+%5Cfrac%7B1%7D%7B2%7D%5C%5C+%7B%5Crm%7BSIM%7D%7D%5Cleft%28+%7B%7BS_1%7D%2C%7BS_4%7D%7D+%5Cright%29+%3D+1%5C%5D\" alt=\"\\[{\\rm{SIM}}\\left( {{S_1},{S_2}} \\right) = 0\\\\ {\\rm{SIM}}\\left( {{S_1},{S_3}} \\right) = \\frac{1}{2}\\\\ {\\rm{SIM}}\\left( {{S_1},{S_4}} \\right) = 1\\]\" eeimg=\"1\"/> </p><p>根据图 2 可以计算真实的 Jaccard 相似度为：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7BSIM%7D%7D%5Cleft%28+%7B%7BS_1%7D%2C%7BS_2%7D%7D+%5Cright%29+%3D+0%5C%5C+%7B%5Crm%7BSIM%7D%7D%5Cleft%28+%7B%7BS_1%7D%2C%7BS_3%7D%7D+%5Cright%29+%3D+%5Cfrac%7B1%7D%7B4%7D%5C%5C+%7B%5Crm%7BSIM%7D%7D%5Cleft%28+%7B%7BS_1%7D%2C%7BS_4%7D%7D+%5Cright%29+%3D+%5Cfrac%7B2%7D%7B3%7D%5C%5D\" alt=\"\\[{\\rm{SIM}}\\left( {{S_1},{S_2}} \\right) = 0\\\\ {\\rm{SIM}}\\left( {{S_1},{S_3}} \\right) = \\frac{1}{4}\\\\ {\\rm{SIM}}\\left( {{S_1},{S_4}} \\right) = \\frac{2}{3}\\]\" eeimg=\"1\"/> </p><p>所以可以看出两者成正比关系，如果选取越多的 hash 函数，估计的 Jaccard 相似度越接近真实的 Jaccard 相似度。</p><h2><b>三、LSH（locality-sensitive hashing）</b></h2><p>我们可以采用前面介绍的 Minhash 近似地计算两篇文档的 Jaccard 相似度，从而可以判断两篇文档的相似度，但是还有一个问题，假设我们有 <img src=\"https://www.zhihu.com/equation?tex=1\" alt=\"1\" eeimg=\"1\"/> 百万篇文档，此时新增一篇文档，需要判断是否和库里的 <img src=\"https://www.zhihu.com/equation?tex=1\" alt=\"1\" eeimg=\"1\"/> 百万篇文档重复，常规做法需要和库里的 <img src=\"https://www.zhihu.com/equation?tex=1\" alt=\"1\" eeimg=\"1\"/> 百万篇文档进行一一比较，这样计算量很大，无法做到线上实时检测。其实我们只需要和最相似的文档比较，没有必要和全部的文档进行比较，为此引入 LSH 技术。</p><h2><b>3.1 LSH for Minhash Signatures</b></h2><p>LSH 的思想是：采用 hash 技术将原始的数据点映射到桶中，那么相似的数据点将以很大的概率hash到同一个桶中，相同桶中的数据点可以作为计算相似度的候选数据。不相似的数据点被hash到同一个桶中称为假正例（FP），相似的数据点被hash到不同桶中称为假负例（FN）。</p><h2><b>3.2 Banding技术</b></h2><p>将 Minhash Signatures 矩阵分成 <img src=\"https://www.zhihu.com/equation?tex=b\" alt=\"b\" eeimg=\"1\"/> 个 band，每个 band 有 <img src=\"https://www.zhihu.com/equation?tex=r\" alt=\"r\" eeimg=\"1\"/> 行组成，然后采用 hash 技术将每个 band 哈希到一个很大的哈希表中，只要哈希表中的桶足够的多，冲突的概率就很低（不同的 band 可以采用相同的 hash 函数，但是每个 band 有自己的哈希表）。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-ae4e44e091751e22912f4ba04c0ac4c6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"406\" data-rawheight=\"209\" class=\"content_image\" width=\"406\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;406&#39; height=&#39;209&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"406\" data-rawheight=\"209\" class=\"content_image lazy\" width=\"406\" data-actualsrc=\"https://pic3.zhimg.com/v2-ae4e44e091751e22912f4ba04c0ac4c6_b.jpg\"/></figure><p>假设有两篇文档，它们的 Jaccard 相似度为 <img src=\"https://www.zhihu.com/equation?tex=s\" alt=\"s\" eeimg=\"1\"/> ，由公式 (2) 和公式 (3) 可知，Minhash Signatures 矩阵中两篇文档某一行的 Minhash 值相等的概率为 <img src=\"https://www.zhihu.com/equation?tex=s\" alt=\"s\" eeimg=\"1\"/> 。</p><ul><li>对于任意一个 band，两篇文档在该 band 中的 Minhash 值都相同的概率为 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bs%5Er%7D%5C%5D\" alt=\"\\[{s^r}\\]\" eeimg=\"1\"/> ；</li><li>该 band 中至少有一个 Minhash 值不相同的概率为 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B1+-+%7Bs%5Er%7D%5C%5D\" alt=\"\\[1 - {s^r}\\]\" eeimg=\"1\"/> ；</li><li>对于 <img src=\"https://www.zhihu.com/equation?tex=b\" alt=\"b\" eeimg=\"1\"/> 个 band，每个 band 中至少有一个 Minhash 值不相同的概率 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Cleft%28+%7B1+-+%7Bs%5Er%7D%7D+%5Cright%29%5Eb%7D%5C%5D\" alt=\"\\[{\\left( {1 - {s^r}} \\right)^b}\\]\" eeimg=\"1\"/> ；</li><li>对于 <img src=\"https://www.zhihu.com/equation?tex=b\" alt=\"b\" eeimg=\"1\"/> 个 band，至少有一个 band 中 Minhash 相同的概率为 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B1+-+%7B%5Cleft%28+%7B1+-+%7Bs%5Er%7D%7D+%5Cright%29%5Eb%7D%5C%5D\" alt=\"\\[1 - {\\left( {1 - {s^r}} \\right)^b}\\]\" eeimg=\"1\"/> 。只要有一个 band 中的 Minhash 相同，作为计算相似度的候选文档。</li></ul><p>假设 <img src=\"https://www.zhihu.com/equation?tex=%5C%5Bb+%3D+20%2Cr+%3D+5%2Cs+%3D+0.8%5C%5D\" alt=\"\\[b = 20,r = 5,s = 0.8\\]\" eeimg=\"1\"/> ，可以得 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B1+-+%7B%5Cleft%28+%7B1+-+%7Bs%5Er%7D%7D+%5Cright%29%5Eb%7D+%3D+%7B%5Crm%7B0%7D%7D%7B%5Crm%7B.99965%7D%7D%5C%5D\" alt=\"\\[1 - {\\left( {1 - {s^r}} \\right)^b} = {\\rm{0}}{\\rm{.99965}}\\]\" eeimg=\"1\"/> ，也就是说， <img src=\"https://www.zhihu.com/equation?tex=20\" alt=\"20\" eeimg=\"1\"/> 个band 中至少有一个 band 相同的概率为 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7B0%7D%7D%7B%5Crm%7B.99965%7D%7D%5C%5D\" alt=\"\\[{\\rm{0}}{\\rm{.99965}}\\]\" eeimg=\"1\"/> ， <img src=\"https://www.zhihu.com/equation?tex=%5C%5B1+-+%7B%5Cleft%28+%7B1+-+%7Bs%5Er%7D%7D+%5Cright%29%5Eb%7D%5C%5D\" alt=\"\\[1 - {\\left( {1 - {s^r}} \\right)^b}\\]\" eeimg=\"1\"/> 称为 S-curve。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-ccb4563360401591e787ed37ee2a7066_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1069\" data-rawheight=\"513\" class=\"origin_image zh-lightbox-thumb\" width=\"1069\" data-original=\"https://pic3.zhimg.com/v2-ccb4563360401591e787ed37ee2a7066_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1069&#39; height=&#39;513&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1069\" data-rawheight=\"513\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1069\" data-original=\"https://pic3.zhimg.com/v2-ccb4563360401591e787ed37ee2a7066_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-ccb4563360401591e787ed37ee2a7066_b.jpg\"/></figure><p>总结下如果用 LSH+k-shingles+Minhash 实现实时去重检索。</p><ul><li><b>k-shingles</b>：选取适当的 <img src=\"https://www.zhihu.com/equation?tex=k\" alt=\"k\" eeimg=\"1\"/> 值，获取每篇文档的 k-shingles 集合，可以将每篇文档中的 shingle 字符串 hash 到一个整数，用来代替字符串，后续 Minhash 就是基于这个整数，例如采用 sha1 函数将每个字符 hash 到整数；                                                                                          </li></ul><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"kn\">import</span> <span class=\"nn\">struct</span>\n<span class=\"kn\">from</span> <span class=\"nn\">hashlib</span> <span class=\"k\">import</span> <span class=\"n\">sha1</span>\n<span class=\"k\">def</span> <span class=\"nf\">hash_fun</span><span class=\"p\">(</span><span class=\"n\">b</span><span class=\"p\">):</span>\n    <span class=\"k\">return</span> <span class=\"n\">struct</span><span class=\"o\">.</span><span class=\"n\">unpack</span><span class=\"p\">(</span><span class=\"s1\">&#39;&lt;I&#39;</span><span class=\"p\">,</span> <span class=\"n\">sha1</span><span class=\"p\">(</span><span class=\"n\">b</span><span class=\"o\">.</span><span class=\"n\">encode</span><span class=\"p\">(</span><span class=\"s1\">&#39;utf8&#39;</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">digest</span><span class=\"p\">()[:</span><span class=\"mi\">4</span><span class=\"p\">])[</span><span class=\"mi\">0</span><span class=\"p\">]</span></code></pre></div><ul><li><b>Minhash Signatures</b>：选取 <img src=\"https://www.zhihu.com/equation?tex=n\" alt=\"n\" eeimg=\"1\"/> 个 hash 函数，计算 Minhash Signatures 矩阵；</li><li>选取适当的 <img src=\"https://www.zhihu.com/equation?tex=b%2Cr\" alt=\"b,r\" eeimg=\"1\"/> 值，将 Minhash Signatures 矩阵划分为 <img src=\"https://www.zhihu.com/equation?tex=b\" alt=\"b\" eeimg=\"1\"/> 个 band，每个 band 有 <img src=\"https://www.zhihu.com/equation?tex=r\" alt=\"r\" eeimg=\"1\"/> 行；</li><li>选取 hash 函数，将每个 band 映射到对应的哈希表中，只要有一个 band 相同，可以选出作为计算相似度的候选样本。</li></ul><p>可以结合<a href=\"https://link.zhihu.com/?target=https%3A//github.com/ekzhu/datasketch\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">代码</a>理解原理。</p>", 
            "topic": [
                {
                    "tag": "算法", 
                    "tagLink": "https://api.zhihu.com/topics/19553510"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "去重", 
                    "tagLink": "https://api.zhihu.com/topics/19649943"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/45164073", 
            "userName": "徽州风韵", 
            "userLink": "https://www.zhihu.com/people/53bd6f8b93873b03c5edb714aea2a456", 
            "upvote": 2, 
            "title": "二维高斯分布的Gibbs Sampling", 
            "content": "<p>在<a href=\"https://zhuanlan.zhihu.com/p/42714332\" class=\"internal\">Latent Dirichlet Allocation-上篇</a>中介绍了 Gibbs Sampling 的原理，我们给出一个简单的应用，利用 Gibbs Sampling 对二维高斯分布进行采样，多维高斯分布概率密度为：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5BN%5Cleft%28+%7B%7B%5Crm%7Bx%7C%7D%7D%5Cmu+%2C%5CSigma+%7D+%5Cright%29%7B%5Crm%7B+%3D+%7D%7D%5Cfrac%7B1%7D%7B%7B%7B%7B%5Cleft%28+%7B2%5Cpi+%7D+%5Cright%29%7D%5E%7BD%2F2%7D%7D%7D%7D%5Cfrac%7B1%7D%7B%7B%7B%7B%5Cleft%7C+%5CSigma++%5Cright%7C%7D%5E%7B1%2F2%7D%7D%7D%7D%5Cexp+%5Cleft%5C%7B+%7B+-+%5Cfrac%7B1%7D%7B2%7D%7B%7B%5Cleft%28+%7B%7B%5Crm%7Bx%7D%7D+-+%5Cmu+%7D+%5Cright%29%7D%5E%7B%5Crm%7BT%7D%7D%7D%7B%5CSigma+%5E%7B+-+1%7D%7D%5Cleft%28+%7B%7B%5Crm%7Bx%7D%7D+-+%5Cmu+%7D+%5Cright%29%7D+%5Cright%5C%7D+%5Ctag%7B1%7D+%5C%5D\" alt=\"\\[N\\left( {{\\rm{x|}}\\mu ,\\Sigma } \\right){\\rm{ = }}\\frac{1}{{{{\\left( {2\\pi } \\right)}^{D/2}}}}\\frac{1}{{{{\\left| \\Sigma  \\right|}^{1/2}}}}\\exp \\left\\{ { - \\frac{1}{2}{{\\left( {{\\rm{x}} - \\mu } \\right)}^{\\rm{T}}}{\\Sigma ^{ - 1}}\\left( {{\\rm{x}} - \\mu } \\right)} \\right\\} \\tag{1} \\]\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cmu+%5C%5D\" alt=\"\\[\\mu \\]\" eeimg=\"1\"/> 是均值向量， <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5CSigma+%5C%5D\" alt=\"\\[\\Sigma \\]\" eeimg=\"1\"/> 是方差矩阵， <img src=\"https://www.zhihu.com/equation?tex=D\" alt=\"D\" eeimg=\"1\"/> 是 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7Bx%7D%7D%5C%5D\" alt=\"\\[{\\rm{x}}\\]\" eeimg=\"1\"/> 的维度。高斯分布概率密度的指数项是 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7Bx%7D%7D%5C%5D\" alt=\"\\[{\\rm{x}}\\]\" eeimg=\"1\"/> 的二次函数，可以进一步展开为 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B+-+%5Cfrac%7B1%7D%7B2%7D%5Cleft%28+%7B%7B%7B%5Crm%7Bx%7D%7D%5E%7B%5Crm%7BT%7D%7D%7D%7B%5CSigma+%5E%7B+-+1%7D%7D%7B%5Crm%7Bx%7D%7D+-+%7B%5Crm%7B2%7D%7D%7B%7B%5Crm%7Bx%7D%7D%5E%7B%5Crm%7BT%7D%7D%7D%7B%5CSigma+%5E%7B+-+1%7D%7D%5Cmu++%2B+%7B%5Crm%7Bconst%7D%7D%7D+%5Cright%29%5C%5D\" alt=\"\\[ - \\frac{1}{2}\\left( {{{\\rm{x}}^{\\rm{T}}}{\\Sigma ^{ - 1}}{\\rm{x}} - {\\rm{2}}{{\\rm{x}}^{\\rm{T}}}{\\Sigma ^{ - 1}}\\mu  + {\\rm{const}}} \\right)\\]\" eeimg=\"1\"/> 。从这里可以得到一个很好的结论：<b>如果已知一个随机变量服从高斯分布，那么只需要找到指数项的二次部分和一次部分，就可以得到这个分布的均值和方差。</b></p><p>Gibbs Sampling 需要求出条件高斯分布，将 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7Bx%7D%7D%5C%5D\" alt=\"\\[{\\rm{x}}\\]\" eeimg=\"1\"/> 分布两个子集 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5Crm%7Bx%7D%7D_a%7D+%5Cin+%7BR%5EM%7D%5C%5D\" alt=\"\\[{{\\rm{x}}_a} \\in {R^M}\\]\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5Crm%7Bx%7D%7D_b%7D+%5Cin+%7BR%5E%7BD+-+M%7D%7D%5C%5D\" alt=\"\\[{{\\rm{x}}_b} \\in {R^{D - M}}\\]\" eeimg=\"1\"/> ，同时也将对应的均值向量 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cmu+%5C%5D\" alt=\"\\[\\mu \\]\" eeimg=\"1\"/> 和方差矩阵 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5CSigma+%5C%5D\" alt=\"\\[\\Sigma \\]\" eeimg=\"1\"/> 进行划分，可得：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cbegin%7Barray%7D%7Bl%7D+%7B%5Crm%7Bx%7D%7D+%3D+%5Cleft%28+%7B%5Cbegin%7Barray%7D%7B%2A%7B20%7D%7Bc%7D%7D+%7B%7B%7B%5Crm%7Bx%7D%7D_a%7D%7D%5C%5C+%7B%7B%7B%5Crm%7Bx%7D%7D_b%7D%7D+%5Cend%7Barray%7D%7D+%5Cright%29%5C%5C+%5Cmu++%3D+%5Cleft%28+%7B%5Cbegin%7Barray%7D%7B%2A%7B20%7D%7Bc%7D%7D+%7B%7B%5Cmu+_a%7D%7D%5C%5C+%7B%7B%5Cmu+_b%7D%7D+%5Cend%7Barray%7D%7D+%5Cright%29%5C%5C+%5CSigma++%3D+%5Cleft%28+%7B%5Cbegin%7Barray%7D%7B%2A%7B20%7D%7Bc%7D%7D+%7B%7B%5CSigma+_%7Baa%7D%7D%7D%26%7B%7B%5CSigma+_%7Bab%7D%7D%7D%5C%5C+%7B%7B%5CSigma+_%7Bba%7D%7D%7D%26%7B%7B%5CSigma+_%7Bbb%7D%7D%7D+%5Cend%7Barray%7D%7D+%5Cright%29+%5Cend%7Barray%7D+%5Ctag%7B2%7D+%5C%5D\" alt=\"\\[\\begin{array}{l} {\\rm{x}} = \\left( {\\begin{array}{*{20}{c}} {{{\\rm{x}}_a}}\\\\ {{{\\rm{x}}_b}} \\end{array}} \\right)\\\\ \\mu  = \\left( {\\begin{array}{*{20}{c}} {{\\mu _a}}\\\\ {{\\mu _b}} \\end{array}} \\right)\\\\ \\Sigma  = \\left( {\\begin{array}{*{20}{c}} {{\\Sigma _{aa}}}&amp;{{\\Sigma _{ab}}}\\\\ {{\\Sigma _{ba}}}&amp;{{\\Sigma _{bb}}} \\end{array}} \\right) \\end{array} \\tag{2} \\]\" eeimg=\"1\"/> </p><p>因为 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5CSigma+%5E%7B%5Crm%7BT%7D%7D%7D+%3D+%5CSigma+%5C%5D\" alt=\"\\[{\\Sigma ^{\\rm{T}}} = \\Sigma \\]\" eeimg=\"1\"/> ，所以 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5CSigma+_%7Baa%7D%7D%7D%5C%5D\" alt=\"\\[{{\\Sigma _{aa}}}\\]\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5CSigma+_%7Bbb%7D%7D%7D%5C%5D\" alt=\"\\[{{\\Sigma _{bb}}}\\]\" eeimg=\"1\"/> 是对称矩阵， <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5CSigma+_%7Bba%7D%7D+%3D+%5CSigma+_%7Bab%7D%5E%7B%5Crm%7BT%7D%7D%5C%5D\" alt=\"\\[{\\Sigma _{ba}} = \\Sigma _{ab}^{\\rm{T}}\\]\" eeimg=\"1\"/> 。假设 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5CSigma+%5C%5D\" alt=\"\\[\\Sigma \\]\" eeimg=\"1\"/> 的逆矩阵为 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5CLambda+%5C%5D\" alt=\"\\[\\Lambda \\]\" eeimg=\"1\"/> 。</p><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5CLambda++%3D+%5Cleft%28+%7B%5Cbegin%7Barray%7D%7B%2A%7B20%7D%7Bc%7D%7D+%7B%7B%5CLambda+_%7Baa%7D%7D%7D%26%7B%7B%5CLambda+_%7Bab%7D%7D%7D%5C%5C+%7B%7B%5CLambda+_%7Bba%7D%7D%7D%26%7B%7B%5CLambda+_%7Bbb%7D%7D%7D+%5Cend%7Barray%7D%7D+%5Cright%29+%5Ctag%7B3%7D+%5C%5D\" alt=\"\\[\\Lambda  = \\left( {\\begin{array}{*{20}{c}} {{\\Lambda _{aa}}}&amp;{{\\Lambda _{ab}}}\\\\ {{\\Lambda _{ba}}}&amp;{{\\Lambda _{bb}}} \\end{array}} \\right) \\tag{3} \\]\" eeimg=\"1\"/> </p><p>将 (2) 和 (3) 带入公式 (1) 的指数项可得：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cbegin%7Barray%7D%7Bl%7D++-+%5Cfrac%7B1%7D%7B2%7D%7B%5Cleft%28+%7B%7B%5Crm%7Bx%7D%7D+-+%5Cmu+%7D+%5Cright%29%5E%7B%5Crm%7BT%7D%7D%7D%7B%5CSigma+%5E%7B+-+1%7D%7D%5Cleft%28+%7B%7B%5Crm%7Bx%7D%7D+-+%5Cmu+%7D+%5Cright%29%5C%5C+%7B%5Crm%7B+%3D+%7D%7D+-+%5Cfrac%7B1%7D%7B2%7D%7B%5Cleft%28+%7B%5Cbegin%7Barray%7D%7B%2A%7B20%7D%7Bc%7D%7D+%7B%7B%7B%5Crm%7Bx%7D%7D_a%7D+-+%7B%5Cmu+_a%7D%7D%5C%5C+%7B%7B%7B%5Crm%7Bx%7D%7D_b%7D+-+%7B%5Cmu+_b%7D%7D+%5Cend%7Barray%7D%7D+%5Cright%29%5E%7B%5Crm%7BT%7D%7D%7D%5Cleft%28+%7B%5Cbegin%7Barray%7D%7B%2A%7B20%7D%7Bc%7D%7D+%7B%7B%5CLambda+_%7Baa%7D%7D%7D%26%7B%7B%5CLambda+_%7Bab%7D%7D%7D%5C%5C+%7B%7B%5CLambda+_%7Bba%7D%7D%7D%26%7B%7B%5CLambda+_%7Bbb%7D%7D%7D+%5Cend%7Barray%7D%7D+%5Cright%29%5Cleft%28+%7B%5Cbegin%7Barray%7D%7B%2A%7B20%7D%7Bc%7D%7D+%7B%7B%7B%5Crm%7Bx%7D%7D_a%7D+-+%7B%5Cmu+_a%7D%7D%5C%5C+%7B%7B%7B%5Crm%7Bx%7D%7D_b%7D+-+%7B%5Cmu+_b%7D%7D+%5Cend%7Barray%7D%7D+%5Cright%29%5C%5C++%3D++-+%5Cfrac%7B1%7D%7B2%7D%7B%5Cleft%28+%7B%7B%7B%5Crm%7Bx%7D%7D_a%7D+-+%7B%5Cmu+_a%7D%7D+%5Cright%29%5E%7B%5Crm%7BT%7D%7D%7D%7B%5CLambda+_%7Baa%7D%7D%5Cleft%28+%7B%7B%7B%5Crm%7Bx%7D%7D_a%7D+-+%7B%5Cmu+_a%7D%7D+%5Cright%29+-+%5Cfrac%7B1%7D%7B2%7D%7B%5Cleft%28+%7B%7B%7B%5Crm%7Bx%7D%7D_a%7D+-+%7B%5Cmu+_a%7D%7D+%5Cright%29%5E%7B%5Crm%7BT%7D%7D%7D%7B%5CLambda+_%7Bab%7D%7D%5Cleft%28+%7B%7B%7B%5Crm%7Bx%7D%7D_b%7D+-+%7B%5Cmu+_b%7D%7D+%5Cright%29%5C%5C++%5C+%5C+%5C+-+%5Cfrac%7B1%7D%7B2%7D%7B%5Cleft%28+%7B%7B%7B%5Crm%7Bx%7D%7D_b%7D+-+%7B%5Cmu+_b%7D%7D+%5Cright%29%5E%7B%5Crm%7BT%7D%7D%7D%7B%5CLambda+_%7Bba%7D%7D%5Cleft%28+%7B%7B%7B%5Crm%7Bx%7D%7D_a%7D+-+%7B%5Cmu+_a%7D%7D+%5Cright%29+-+%5Cfrac%7B1%7D%7B2%7D%7B%5Cleft%28+%7B%7B%7B%5Crm%7Bx%7D%7D_b%7D+-+%7B%5Cmu+_b%7D%7D+%5Cright%29%5E%7B%5Crm%7BT%7D%7D%7D%7B%5CLambda+_%7Bbb%7D%7D%5Cleft%28+%7B%7B%7B%5Crm%7Bx%7D%7D_b%7D+-+%7B%5Cmu+_b%7D%7D+%5Cright%29+%5Cend%7Barray%7D+%5Ctag%7B4%7D+%5C%5D\" alt=\"\\[\\begin{array}{l}  - \\frac{1}{2}{\\left( {{\\rm{x}} - \\mu } \\right)^{\\rm{T}}}{\\Sigma ^{ - 1}}\\left( {{\\rm{x}} - \\mu } \\right)\\\\ {\\rm{ = }} - \\frac{1}{2}{\\left( {\\begin{array}{*{20}{c}} {{{\\rm{x}}_a} - {\\mu _a}}\\\\ {{{\\rm{x}}_b} - {\\mu _b}} \\end{array}} \\right)^{\\rm{T}}}\\left( {\\begin{array}{*{20}{c}} {{\\Lambda _{aa}}}&amp;{{\\Lambda _{ab}}}\\\\ {{\\Lambda _{ba}}}&amp;{{\\Lambda _{bb}}} \\end{array}} \\right)\\left( {\\begin{array}{*{20}{c}} {{{\\rm{x}}_a} - {\\mu _a}}\\\\ {{{\\rm{x}}_b} - {\\mu _b}} \\end{array}} \\right)\\\\  =  - \\frac{1}{2}{\\left( {{{\\rm{x}}_a} - {\\mu _a}} \\right)^{\\rm{T}}}{\\Lambda _{aa}}\\left( {{{\\rm{x}}_a} - {\\mu _a}} \\right) - \\frac{1}{2}{\\left( {{{\\rm{x}}_a} - {\\mu _a}} \\right)^{\\rm{T}}}{\\Lambda _{ab}}\\left( {{{\\rm{x}}_b} - {\\mu _b}} \\right)\\\\  \\ \\ \\ - \\frac{1}{2}{\\left( {{{\\rm{x}}_b} - {\\mu _b}} \\right)^{\\rm{T}}}{\\Lambda _{ba}}\\left( {{{\\rm{x}}_a} - {\\mu _a}} \\right) - \\frac{1}{2}{\\left( {{{\\rm{x}}_b} - {\\mu _b}} \\right)^{\\rm{T}}}{\\Lambda _{bb}}\\left( {{{\\rm{x}}_b} - {\\mu _b}} \\right) \\end{array} \\tag{4} \\]\" eeimg=\"1\"/> </p><p>如果 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%7B%5Crm%7Bx%7D%7D_b%7D%7D%5C%5D\" alt=\"\\[{{{\\rm{x}}_b}}\\]\" eeimg=\"1\"/> 已知， <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%7B%5Crm%7Bx%7D%7D_a%7D%7D%5C%5D\" alt=\"\\[{{{\\rm{x}}_a}}\\]\" eeimg=\"1\"/> 未知，公式 (4) 可以看做关于变量 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%7B%5Crm%7Bx%7D%7D_a%7D%7D%5C%5D\" alt=\"\\[{{{\\rm{x}}_a}}\\]\" eeimg=\"1\"/> 的二次式函数，对应的条件分布 <img src=\"https://www.zhihu.com/equation?tex=%5C%5Bp%5Cleft%28+%7B%7B%7B%5Crm%7Bx%7D%7D_a%7D%7C%7B%7B%5Crm%7Bx%7D%7D_b%7D%7D+%5Cright%29%5C%5D\" alt=\"\\[p\\left( {{{\\rm{x}}_a}|{{\\rm{x}}_b}} \\right)\\]\" eeimg=\"1\"/> 是高斯分布，所以可以通过二次部分和一次部分得到条件分布的均值 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Cmu+_%7Ba%7Cb%7D%7D%5C%5D\" alt=\"\\[{\\mu _{a|b}}\\]\" eeimg=\"1\"/> 和方差矩阵 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5CSigma+_%7Ba%7Cb%7D%7D%5C%5D\" alt=\"\\[{\\Sigma _{a|b}}\\]\" eeimg=\"1\"/> 。由公式 (4) 可知，关于 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%7B%5Crm%7Bx%7D%7D_a%7D%7D%5C%5D\" alt=\"\\[{{{\\rm{x}}_a}}\\]\" eeimg=\"1\"/> 二次和一次部分分别为：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cbegin%7Barray%7D%7Bl%7D++-+%5Cfrac%7B1%7D%7B2%7D%7B%5Crm%7Bx%7D%7D_a%5E%7B%5Crm%7BT%7D%7D%7B%5CLambda+_%7Baa%7D%7D%7B%7B%5Crm%7Bx%7D%7D_a%7D%5C%5C+%7B%5Crm%7Bx%7D%7D_a%5E%7B%5Crm%7BT%7D%7D%5Cleft%5C%7B+%7B%7B%5CLambda+_%7Baa%7D%7D%7B%7B%5Crm%7Bx%7D%7D_a%7D+-+%7B%5CLambda+_%7Bab%7D%7D%5Cleft%28+%7B%7B%7B%5Crm%7Bx%7D%7D_b%7D+-+%7B%5Cmu+_b%7D%7D+%5Cright%29%7D+%5Cright%5C%7D+%5Cend%7Barray%7D+%5Ctag%7B5%7D+%5C%5D\" alt=\"\\[\\begin{array}{l}  - \\frac{1}{2}{\\rm{x}}_a^{\\rm{T}}{\\Lambda _{aa}}{{\\rm{x}}_a}\\\\ {\\rm{x}}_a^{\\rm{T}}\\left\\{ {{\\Lambda _{aa}}{{\\rm{x}}_a} - {\\Lambda _{ab}}\\left( {{{\\rm{x}}_b} - {\\mu _b}} \\right)} \\right\\} \\end{array} \\tag{5} \\]\" eeimg=\"1\"/> </p><p>所以 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Cmu+_%7Ba%7Cb%7D%7D%5C%5D\" alt=\"\\[{\\mu _{a|b}}\\]\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5CSigma+_%7Ba%7Cb%7D%7D%5C%5D\" alt=\"\\[{\\Sigma _{a|b}}\\]\" eeimg=\"1\"/> 分别为：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cbegin%7Barray%7D%7Bl%7D+%7B%5CSigma+_%7Ba%7Cb%7D%7D+%3D+%5CLambda+_%7Baa%7D%5E%7B+-+1%7D%5C%5C+%7B%5Cmu+_%7Ba%7Cb%7D%7D+%3D+%7B%5Cmu+_a%7D+-+%5CLambda+_%7Baa%7D%5E%7B+-+1%7D%7B%5CLambda+_%7Bab%7D%7D%5Cleft%28+%7B%7B%7B%5Crm%7Bx%7D%7D_b%7D+-+%7B%5Cmu+_b%7D%7D+%5Cright%29+%5Cend%7Barray%7D+%5Ctag%7B6%7D+%5C%5D\" alt=\"\\[\\begin{array}{l} {\\Sigma _{a|b}} = \\Lambda _{aa}^{ - 1}\\\\ {\\mu _{a|b}} = {\\mu _a} - \\Lambda _{aa}^{ - 1}{\\Lambda _{ab}}\\left( {{{\\rm{x}}_b} - {\\mu _b}} \\right) \\end{array} \\tag{6} \\]\" eeimg=\"1\"/> </p><p>接先来求解 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5CLambda+_%7Baa%7D%7D%7D%5C%5D\" alt=\"\\[{{\\Lambda _{aa}}}\\]\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5CLambda+_%7Bab%7D%7D%7D%5C%5D\" alt=\"\\[{{\\Lambda _{ab}}}\\]\" eeimg=\"1\"/> ，因为 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5CLambda++%3D+%7B%5CSigma+%5E%7B+-+1%7D%7D%5C%5D\" alt=\"\\[\\Lambda  = {\\Sigma ^{ - 1}}\\]\" eeimg=\"1\"/> ，所以：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cbegin%7Barray%7D%7Bl%7D+%5Cleft%28+%7B%5Cbegin%7Barray%7D%7B%2A%7B20%7D%7Bc%7D%7D+%7B%7B%5CSigma+_%7Baa%7D%7D%7D%26%7B%7B%5CSigma+_%7Bab%7D%7D%7D%5C%5C+%7B%7B%5CSigma+_%7Bba%7D%7D%7D%26%7B%7B%5CSigma+_%7Bbb%7D%7D%7D+%5Cend%7Barray%7D%7D+%5Cright%29%5Cleft%28+%7B%5Cbegin%7Barray%7D%7B%2A%7B20%7D%7Bc%7D%7D+%7B%7B%5CLambda+_%7Baa%7D%7D%7D%26%7B%7B%5CLambda+_%7Bab%7D%7D%7D%5C%5C+%7B%7B%5CLambda+_%7Bba%7D%7D%7D%26%7B%7B%5CLambda+_%7Bbb%7D%7D%7D+%5Cend%7Barray%7D%7D+%5Cright%29+%3D+%5Cleft%28+%7B%5Cbegin%7Barray%7D%7B%2A%7B20%7D%7Bc%7D%7D+%7B%7BE_%7Baa%7D%7D%7D%26%7B%7B0_%7Bab%7D%7D%7D%5C%5C+%7B%7B0_%7Bba%7D%7D%7D%26%7B%7BE_%7Bbb%7D%7D%7D+%5Cend%7Barray%7D%7D+%5Cright%29%5C%5C++%5CRightarrow+%5Cleft%28+%7B%5Cbegin%7Barray%7D%7B%2A%7B20%7D%7Bc%7D%7D+%7B%7B%5CSigma+_%7Baa%7D%7D%7B%5CLambda+_%7Baa%7D%7D+%2B+%7B%5CSigma+_%7Bab%7D%7D%7B%5CLambda+_%7Bba%7D%7D%7D%26%7B%7B%5CSigma+_%7Baa%7D%7D%7B%5CLambda+_%7Bab%7D%7D+%2B+%7B%5CSigma+_%7Bab%7D%7D%7B%5CLambda+_%7Bbb%7D%7D%7D%5C%5C+%7B%7B%5CSigma+_%7Bba%7D%7D%7B%5CLambda+_%7Baa%7D%7D+%2B+%7B%5CSigma+_%7Bbb%7D%7D%7B%5CLambda+_%7Bba%7D%7D%7D%26%7B%7B%5CSigma+_%7Bba%7D%7D%7B%5CLambda+_%7Bab%7D%7D+%2B+%7B%5CSigma+_%7Bbb%7D%7D%7B%5CLambda+_%7Bbb%7D%7D%7D+%5Cend%7Barray%7D%7D+%5Cright%29+%3D+%5Cleft%28+%7B%5Cbegin%7Barray%7D%7B%2A%7B20%7D%7Bc%7D%7D+%7B%7BE_%7Baa%7D%7D%7D%26%7B%7B0_%7Bab%7D%7D%7D%5C%5C+%7B%7B0_%7Bba%7D%7D%7D%26%7B%7BE_%7Bbb%7D%7D%7D+%5Cend%7Barray%7D%7D+%5Cright%29%5C%5C++%5CRightarrow+%5Cleft%5C%7B+%5Cbegin%7Barray%7D%7Bl%7D+%7B%5CSigma+_%7Baa%7D%7D%7B%5CLambda+_%7Baa%7D%7D+%2B+%7B%5CSigma+_%7Bab%7D%7D%7B%5CLambda+_%7Bba%7D%7D+%3D+%7BE_%7Baa%7D%7D%5C%5C+%7B%5CSigma+_%7Bba%7D%7D%7B%5CLambda+_%7Baa%7D%7D+%2B+%7B%5CSigma+_%7Bbb%7D%7D%7B%5CLambda+_%7Bba%7D%7D+%3D+0%5C%5C+%7B%5CLambda+_%7Bab%7D%7D+%3D+%5CLambda+_%7Bba%7D%5E%7B%5Crm%7BT%7D%7D+%5Cend%7Barray%7D+%5Cright.%5C%5C++%5CRightarrow+%5Cleft%5C%7B+%5Cbegin%7Barray%7D%7Bl%7D+%7B%5CLambda+_%7Baa%7D%7D+%3D+%7B%5Cleft%28+%7B%7B%5CSigma+_%7Baa%7D%7D+-+%7B%5CSigma+_%7Bab%7D%7D%5CSigma+_%7Bbb%7D%5E%7B+-+1%7D%7B%5CSigma+_%7Bba%7D%7D%7D+%5Cright%29%5E%7B+-+1%7D%7D%5C%5C+%7B%5CLambda+_%7Bab%7D%7D+%3D++-+%7B%5Cleft%28+%7B%7B%5CSigma+_%7Baa%7D%7D+-+%7B%5CSigma+_%7Bab%7D%7D%5CSigma+_%7Bbb%7D%5E%7B+-+1%7D%7B%5CSigma+_%7Bba%7D%7D%7D+%5Cright%29%5E%7B+-+1%7D%7D%7B%5CSigma+_%7Bab%7D%7D%5CSigma+_%7Bbb%7D%5E%7B+-+1%7D+%5Cend%7Barray%7D+%5Cright.+%5Cend%7Barray%7D+%5Ctag%7B7%7D+%5C%5D\" alt=\"\\[\\begin{array}{l} \\left( {\\begin{array}{*{20}{c}} {{\\Sigma _{aa}}}&amp;{{\\Sigma _{ab}}}\\\\ {{\\Sigma _{ba}}}&amp;{{\\Sigma _{bb}}} \\end{array}} \\right)\\left( {\\begin{array}{*{20}{c}} {{\\Lambda _{aa}}}&amp;{{\\Lambda _{ab}}}\\\\ {{\\Lambda _{ba}}}&amp;{{\\Lambda _{bb}}} \\end{array}} \\right) = \\left( {\\begin{array}{*{20}{c}} {{E_{aa}}}&amp;{{0_{ab}}}\\\\ {{0_{ba}}}&amp;{{E_{bb}}} \\end{array}} \\right)\\\\  \\Rightarrow \\left( {\\begin{array}{*{20}{c}} {{\\Sigma _{aa}}{\\Lambda _{aa}} + {\\Sigma _{ab}}{\\Lambda _{ba}}}&amp;{{\\Sigma _{aa}}{\\Lambda _{ab}} + {\\Sigma _{ab}}{\\Lambda _{bb}}}\\\\ {{\\Sigma _{ba}}{\\Lambda _{aa}} + {\\Sigma _{bb}}{\\Lambda _{ba}}}&amp;{{\\Sigma _{ba}}{\\Lambda _{ab}} + {\\Sigma _{bb}}{\\Lambda _{bb}}} \\end{array}} \\right) = \\left( {\\begin{array}{*{20}{c}} {{E_{aa}}}&amp;{{0_{ab}}}\\\\ {{0_{ba}}}&amp;{{E_{bb}}} \\end{array}} \\right)\\\\  \\Rightarrow \\left\\{ \\begin{array}{l} {\\Sigma _{aa}}{\\Lambda _{aa}} + {\\Sigma _{ab}}{\\Lambda _{ba}} = {E_{aa}}\\\\ {\\Sigma _{ba}}{\\Lambda _{aa}} + {\\Sigma _{bb}}{\\Lambda _{ba}} = 0\\\\ {\\Lambda _{ab}} = \\Lambda _{ba}^{\\rm{T}} \\end{array} \\right.\\\\  \\Rightarrow \\left\\{ \\begin{array}{l} {\\Lambda _{aa}} = {\\left( {{\\Sigma _{aa}} - {\\Sigma _{ab}}\\Sigma _{bb}^{ - 1}{\\Sigma _{ba}}} \\right)^{ - 1}}\\\\ {\\Lambda _{ab}} =  - {\\left( {{\\Sigma _{aa}} - {\\Sigma _{ab}}\\Sigma _{bb}^{ - 1}{\\Sigma _{ba}}} \\right)^{ - 1}}{\\Sigma _{ab}}\\Sigma _{bb}^{ - 1} \\end{array} \\right. \\end{array} \\tag{7} \\]\" eeimg=\"1\"/> </p><p>所以可以得：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cbegin%7Barray%7D%7Bl%7D+%7B%5CSigma+_%7Ba%7Cb%7D%7D+%3D+%7B%5CSigma+_%7Baa%7D%7D+-+%7B%5CSigma+_%7Bab%7D%7D%5CSigma+_%7Bbb%7D%5E%7B+-+1%7D%7B%5CSigma+_%7Bba%7D%7D%5C%5C+%7B%5Cmu+_%7Ba%7Cb%7D%7D+%3D+%7B%5Cmu+_a%7D%7B%5Crm%7B+%2B+%7D%7D%7B%5CSigma+_%7Bab%7D%7D%5CSigma+_%7Bbb%7D%5E%7B+-+1%7D%5Cleft%28+%7B%7B%7B%5Crm%7Bx%7D%7D_b%7D+-+%7B%5Cmu+_b%7D%7D+%5Cright%29+%5Cend%7Barray%7D+%5Ctag%7B8%7D+%5C%5D\" alt=\"\\[\\begin{array}{l} {\\Sigma _{a|b}} = {\\Sigma _{aa}} - {\\Sigma _{ab}}\\Sigma _{bb}^{ - 1}{\\Sigma _{ba}}\\\\ {\\mu _{a|b}} = {\\mu _a}{\\rm{ + }}{\\Sigma _{ab}}\\Sigma _{bb}^{ - 1}\\left( {{{\\rm{x}}_b} - {\\mu _b}} \\right) \\end{array} \\tag{8} \\]\" eeimg=\"1\"/> </p><p>得到条件概率，我们就可以用 Gibbs Sampling，下面对比了从二维高斯分布中直接得到的样本和通过Gibbs Sampling采样得到的样本。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-ed7a55bb2f31f408ecfb7ff7c0d41602_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"704\" data-rawheight=\"258\" class=\"origin_image zh-lightbox-thumb\" width=\"704\" data-original=\"https://pic3.zhimg.com/v2-ed7a55bb2f31f408ecfb7ff7c0d41602_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;704&#39; height=&#39;258&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"704\" data-rawheight=\"258\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"704\" data-original=\"https://pic3.zhimg.com/v2-ed7a55bb2f31f408ecfb7ff7c0d41602_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-ed7a55bb2f31f408ecfb7ff7c0d41602_b.jpg\"/></figure><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n<span class=\"kn\">import</span> <span class=\"nn\">matplotlib.pyplot</span> <span class=\"k\">as</span> <span class=\"nn\">plt</span>\n\n<span class=\"n\">D</span> <span class=\"o\">=</span> <span class=\"mi\">2</span>\n<span class=\"n\">N</span> <span class=\"o\">=</span> <span class=\"mi\">10000</span>\n<span class=\"n\">Sigma</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([[</span><span class=\"mf\">1.0</span><span class=\"p\">,</span><span class=\"mf\">0.5</span><span class=\"p\">],[</span><span class=\"mf\">0.5</span><span class=\"p\">,</span><span class=\"mf\">1.0</span><span class=\"p\">]])</span>\n<span class=\"n\">mu</span> <span class=\"o\">=</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([[</span><span class=\"mi\">0</span><span class=\"p\">],[</span><span class=\"mi\">0</span><span class=\"p\">]])</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">generate_gaussians</span><span class=\"p\">():</span>\n    <span class=\"n\">R</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linalg</span><span class=\"o\">.</span><span class=\"n\">cholesky</span><span class=\"p\">(</span><span class=\"n\">Sigma</span><span class=\"p\">)</span>\n    <span class=\"n\">s</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">R</span><span class=\"p\">,</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">D</span><span class=\"p\">,</span> <span class=\"n\">N</span><span class=\"p\">))</span> <span class=\"o\">+</span> <span class=\"n\">mu</span>\n    <span class=\"k\">return</span> <span class=\"n\">s</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">gibbs_sample</span><span class=\"p\">(</span><span class=\"n\">sample_count</span><span class=\"p\">):</span>\n    <span class=\"c1\"># conditional gaussians</span>\n    <span class=\"k\">def</span> <span class=\"nf\">conditional_gaussians</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">,</span><span class=\"n\">g</span><span class=\"p\">):</span>\n        <span class=\"n\">mu_i</span> <span class=\"o\">=</span> <span class=\"n\">mu</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"n\">Sigma</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">,</span><span class=\"o\">~</span><span class=\"n\">i</span><span class=\"p\">]</span><span class=\"o\">*</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"o\">/</span><span class=\"n\">Sigma</span><span class=\"p\">[</span><span class=\"o\">~</span><span class=\"n\">i</span><span class=\"p\">,</span><span class=\"o\">~</span><span class=\"n\">i</span><span class=\"p\">])</span><span class=\"o\">*</span><span class=\"p\">(</span><span class=\"n\">g</span> <span class=\"o\">-</span> <span class=\"n\">mu</span><span class=\"p\">[</span><span class=\"o\">~</span><span class=\"n\">i</span><span class=\"p\">])</span>\n        <span class=\"n\">Sigma_i</span> <span class=\"o\">=</span> <span class=\"n\">Sigma</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">,</span><span class=\"n\">i</span><span class=\"p\">]</span> <span class=\"o\">-</span> <span class=\"n\">Sigma</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">,</span><span class=\"o\">~</span><span class=\"n\">i</span><span class=\"p\">]</span><span class=\"o\">*</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"o\">/</span><span class=\"n\">Sigma</span><span class=\"p\">[</span><span class=\"o\">~</span><span class=\"n\">i</span><span class=\"p\">,</span><span class=\"o\">~</span><span class=\"n\">i</span><span class=\"p\">])</span><span class=\"o\">*</span><span class=\"n\">Sigma</span><span class=\"p\">[</span><span class=\"o\">~</span><span class=\"n\">i</span><span class=\"p\">,</span><span class=\"n\">i</span><span class=\"p\">]</span>\n        <span class=\"k\">return</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sqrt</span><span class=\"p\">(</span><span class=\"n\">Sigma_i</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">mu_i</span>\n    <span class=\"n\">samples</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">D</span><span class=\"p\">,</span> <span class=\"n\">sample_count</span><span class=\"p\">))</span>\n    <span class=\"n\">samples</span><span class=\"p\">[:,</span> <span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mi\">4</span><span class=\"p\">,</span><span class=\"mi\">4</span><span class=\"p\">]</span><span class=\"c1\">#np.random.randn(1, 2) # initialize the first sample to some arbitrary value</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">sample_count</span><span class=\"p\">):</span>\n        <span class=\"n\">samples</span><span class=\"p\">[:,</span> <span class=\"n\">i</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">samples</span><span class=\"p\">[:,</span> <span class=\"n\">i</span> <span class=\"o\">-</span> <span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"c1\"># first set this sample equal to the previous sample</span>\n        <span class=\"n\">d</span> <span class=\"o\">=</span> <span class=\"n\">i</span> <span class=\"o\">%</span> <span class=\"n\">D</span>\n        <span class=\"n\">samples</span><span class=\"p\">[</span><span class=\"n\">d</span><span class=\"p\">,</span> <span class=\"n\">i</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">conditional_gaussians</span><span class=\"p\">(</span><span class=\"n\">d</span><span class=\"p\">,</span><span class=\"n\">samples</span><span class=\"p\">[</span><span class=\"o\">~</span><span class=\"n\">d</span><span class=\"p\">,</span> <span class=\"n\">i</span> <span class=\"o\">-</span> <span class=\"mi\">1</span><span class=\"p\">])</span> <span class=\"c1\"># 轮换采样</span>\n    <span class=\"k\">return</span> <span class=\"n\">samples</span>\n\n<span class=\"n\">samples_from_true_distribution</span> <span class=\"o\">=</span> <span class=\"n\">generate_gaussians</span><span class=\"p\">()</span>\n<span class=\"n\">samples_from_gibbs_sample</span> <span class=\"o\">=</span> <span class=\"n\">gibbs_sample</span><span class=\"p\">(</span><span class=\"n\">N</span><span class=\"p\">)</span>\n\n<span class=\"n\">fig</span><span class=\"p\">,</span> <span class=\"n\">axs</span> <span class=\"o\">=</span> <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplots</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"n\">figsize</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">12</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">),</span> <span class=\"n\">sharex</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span> <span class=\"n\">sharey</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n\n<span class=\"n\">axs</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">axis</span><span class=\"p\">([</span><span class=\"o\">-</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">])</span>\n<span class=\"n\">axs</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">samples_from_true_distribution</span><span class=\"p\">,</span> <span class=\"s1\">&#39;.&#39;</span><span class=\"p\">,</span> <span class=\"n\">alpha</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">)</span>\n<span class=\"n\">axs</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_title</span><span class=\"p\">(</span><span class=\"s1\">&#39;original p(a, b)&#39;</span><span class=\"p\">)</span>\n\n<span class=\"n\">axs</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">samples_from_gibbs_sample</span><span class=\"p\">,</span> <span class=\"s1\">&#39;k&#39;</span><span class=\"p\">,</span> <span class=\"n\">alpha</span><span class=\"o\">=</span><span class=\"mf\">0.8</span><span class=\"p\">)</span>\n<span class=\"n\">axs</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_title</span><span class=\"p\">(</span><span class=\"s1\">&#39;gibbs sampling path&#39;</span><span class=\"p\">)</span>\n\n<span class=\"n\">axs</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">samples_from_gibbs_sample</span><span class=\"p\">,</span> <span class=\"s1\">&#39;.g&#39;</span><span class=\"p\">,</span> <span class=\"n\">alpha</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">)</span>\n<span class=\"n\">axs</span><span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">set_title</span><span class=\"p\">(</span><span class=\"s1\">&#39;gibbs samples&#39;</span><span class=\"p\">)</span>\n\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span></code></pre></div><p></p>", 
            "topic": [
                {
                    "tag": "正态分布", 
                    "tagLink": "https://api.zhihu.com/topics/19638965"
                }, 
                {
                    "tag": "采样", 
                    "tagLink": "https://api.zhihu.com/topics/19691257"
                }, 
                {
                    "tag": "MCMC采样", 
                    "tagLink": "https://api.zhihu.com/topics/20080781"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/43823482", 
            "userName": "徽州风韵", 
            "userLink": "https://www.zhihu.com/people/53bd6f8b93873b03c5edb714aea2a456", 
            "upvote": 0, 
            "title": "Gradient Boosted Decision Tree", 
            "content": "<h2><b>一、Adaptive Boosted Decision Tree</b></h2><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-43bd29e94c83d9d5d213fe8ad01bb6e2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"646\" data-rawheight=\"291\" class=\"origin_image zh-lightbox-thumb\" width=\"646\" data-original=\"https://pic3.zhimg.com/v2-43bd29e94c83d9d5d213fe8ad01bb6e2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;646&#39; height=&#39;291&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"646\" data-rawheight=\"291\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"646\" data-original=\"https://pic3.zhimg.com/v2-43bd29e94c83d9d5d213fe8ad01bb6e2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-43bd29e94c83d9d5d213fe8ad01bb6e2_b.jpg\"/></figure><p>在 Random Forest 中，通过 bootstrapping 得到训练数据 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5Ctilde+D%7D_t%7D%5C%5D\" alt=\"\\[{{\\tilde D}_t}\\]\" eeimg=\"1\"/> ，Decision Tree 作为 base algorithm，每个 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5Ctilde+D%7D_t%7D%5C%5D\" alt=\"\\[{{\\tilde D}_t}\\]\" eeimg=\"1\"/> 作为训练数据可得到不同的 Decision Tree <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bg_t%7D%5C%5D\" alt=\"\\[{g_t}\\]\" eeimg=\"1\"/> ，然后通过 uniform 的方式将 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bg_t%7D%5C%5D\" alt=\"\\[{g_t}\\]\" eeimg=\"1\"/> 组合起来得到 <img src=\"https://www.zhihu.com/equation?tex=G\" alt=\"G\" eeimg=\"1\"/> 。类似前面介绍的 <b>linear</b> 和 <b>AdaBoost，</b>我们也希望不同的 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bg_t%7D%5C%5D\" alt=\"\\[{g_t}\\]\" eeimg=\"1\"/> 有不同的权重 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Calpha+_t%7D%5C%5D\" alt=\"\\[{\\alpha _t}\\]\" eeimg=\"1\"/> ，同时 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bg_t%7D%5C%5D\" alt=\"\\[{g_t}\\]\" eeimg=\"1\"/> 之间存在差异性。回顾下 AdaBoost 的思想：每一轮的时候，给训练数据新的权重 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5Crm%7Bu%7D%7D%5E%7B%5Cleft%28+t+%5Cright%29%7D%7D%5C%5D\" alt=\"\\[{{\\rm{u}}^{\\left( t \\right)}}\\]\" eeimg=\"1\"/> ，根据 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5Crm%7Bu%7D%7D%5E%7B%5Cleft%28+t+%5Cright%29%7D%7D%5C%5D\" alt=\"\\[{{\\rm{u}}^{\\left( t \\right)}}\\]\" eeimg=\"1\"/> 训练新的 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bg_t%7D%5C%5D\" alt=\"\\[{g_t}\\]\" eeimg=\"1\"/> ，然后可以得到 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bg_t%7D%5C%5D\" alt=\"\\[{g_t}\\]\" eeimg=\"1\"/> 对应的权重 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Calpha+_t%7D%5C%5D\" alt=\"\\[{\\alpha _t}\\]\" eeimg=\"1\"/> ，最后用 linear 的方式组合得到 <img src=\"https://www.zhihu.com/equation?tex=G\" alt=\"G\" eeimg=\"1\"/> ，如果 AdaBoost 中的 base algorithm 是 Decision Tree，这种模型称为 AdaBoost-D Tree。我们需要能够接收 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5Crm%7Bu%7D%7D%5E%7B%5Cleft%28+t+%5Cright%29%7D%7D%5C%5D\" alt=\"\\[{{\\rm{u}}^{\\left( t \\right)}}\\]\" eeimg=\"1\"/> 的 Decision Tree。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-4cdb704859c283cbf14ba4ce5eba0dda_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"541\" data-rawheight=\"236\" class=\"origin_image zh-lightbox-thumb\" width=\"541\" data-original=\"https://pic3.zhimg.com/v2-4cdb704859c283cbf14ba4ce5eba0dda_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;541&#39; height=&#39;236&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"541\" data-rawheight=\"236\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"541\" data-original=\"https://pic3.zhimg.com/v2-4cdb704859c283cbf14ba4ce5eba0dda_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-4cdb704859c283cbf14ba4ce5eba0dda_b.jpg\"/></figure><p>一般让 base algorithm 能够接收 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5Crm%7Bu%7D%7D%5E%7B%5Cleft%28+t+%5Cright%29%7D%7D%5C%5D\" alt=\"\\[{{\\rm{u}}^{\\left( t \\right)}}\\]\" eeimg=\"1\"/> 有以下两种方法：</p><p>1）从 base algorithm 出发，找出是哪里处理 <img src=\"https://www.zhihu.com/equation?tex=%5C%5BE_%7B%7B%5Ctext%7Bin%7D%7D%7D%5Eu%5C%5D\" alt=\"\\[E_{{\\text{in}}}^u\\]\" eeimg=\"1\"/> 的，把对应的权重加进去，例如<a href=\"https://zhuanlan.zhihu.com/p/43379045\" class=\"internal\">Aggregation Models-learning</a>中举的SVM例子。</p><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5BE_%7B%7B%5Crm%7Bin%7D%7D%7D%5E%7B%5Crm%7Bu%7D%7D%5Cleft%28+h+%5Cright%29+%3D+%5Cfrac%7B1%7D%7BN%7D%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%7B%7B%5Crm%7Bu%7D%7D_n%7D%7B%5Crm%7Berr%7D%7D%5Cleft%28+%7B%7By_n%7D%2Ch%5Cleft%28+%7B%7B%7B%5Crm%7Bx%7D%7D_n%7D%7D+%5Cright%29%7D+%5Cright%29%7D+%5Ctag%7B1%7D%5C%5D\" alt=\"\\[E_{{\\rm{in}}}^{\\rm{u}}\\left( h \\right) = \\frac{1}{N}\\sum\\limits_{n = 1}^N {{{\\rm{u}}_n}{\\rm{err}}\\left( {{y_n},h\\left( {{{\\rm{x}}_n}} \\right)} \\right)} \\tag{1}\\]\" eeimg=\"1\"/> </p><p>2）从训练数据出发，把 base algorithm 看作黑盒。回顾下前面，我们是从 bootstrap 引入权重 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Ctext%7Bu%7D%7D%5C%5D\" alt=\"\\[{\\text{u}}\\]\" eeimg=\"1\"/> 的概念， <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Ctext%7Bu%7D%7D%5C%5D\" alt=\"\\[{\\text{u}}\\]\" eeimg=\"1\"/> 的最初含义是 bootstrap 时候样本重复取到的次数。反过来，如果先知道 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Ctext%7Bu%7D%7D%5C%5D\" alt=\"\\[{\\text{u}}\\]\" eeimg=\"1\"/> ，可以以正比于 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Ctext%7Bu%7D%7D%5C%5D\" alt=\"\\[{\\text{u}}\\]\" eeimg=\"1\"/> 的概率进行 sample 得到训练数据 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5Ctilde+D%7D_t%7D%5C%5D\" alt=\"\\[{{\\tilde D}_t}\\]\" eeimg=\"1\"/> 。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-8a1f570426b38771f2ec78f9dc9a448f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"556\" data-rawheight=\"226\" class=\"origin_image zh-lightbox-thumb\" width=\"556\" data-original=\"https://pic4.zhimg.com/v2-8a1f570426b38771f2ec78f9dc9a448f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;556&#39; height=&#39;226&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"556\" data-rawheight=\"226\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"556\" data-original=\"https://pic4.zhimg.com/v2-8a1f570426b38771f2ec78f9dc9a448f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-8a1f570426b38771f2ec78f9dc9a448f_b.jpg\"/></figure><h2><b>二、Optimization View of AdaBoost</b></h2><p>前面我们知道 AdaBoost 中样本权重的迭代计算如下所示：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-d418e89040367eca5a1682ec4119d44b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"539\" data-rawheight=\"389\" class=\"origin_image zh-lightbox-thumb\" width=\"539\" data-original=\"https://pic4.zhimg.com/v2-d418e89040367eca5a1682ec4119d44b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;539&#39; height=&#39;389&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"539\" data-rawheight=\"389\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"539\" data-original=\"https://pic4.zhimg.com/v2-d418e89040367eca5a1682ec4119d44b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-d418e89040367eca5a1682ec4119d44b_b.jpg\"/></figure><p>上面推导过程中对于正确分类的样本 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7By_n%7D%7Bg_t%7D%5Cleft%28+%7B%7B%7B%5Ctext%7Bx%7D%7D_n%7D%7D+%5Cright%29+%3D+1%5C%5D\" alt=\"\\[{y_n}{g_t}\\left( {{{\\text{x}}_n}} \\right) = 1\\]\" eeimg=\"1\"/> ，错误分类的样本 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7By_n%7D%7Bg_t%7D%5Cleft%28+%7B%7B%7B%5Ctext%7Bx%7D%7D_n%7D%7D+%5Cright%29+%3D++-+1%5C%5D\" alt=\"\\[{y_n}{g_t}\\left( {{{\\text{x}}_n}} \\right) =  - 1\\]\" eeimg=\"1\"/> 。如果我们把 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7Bg_t%7D%5Cleft%28+%7B%7B%7B%5Ctext%7Bx%7D%7D_n%7D%7D+%5Cright%29%7D%5C%5D\" alt=\"\\[{{g_t}\\left( {{{\\text{x}}_n}} \\right)}\\]\" eeimg=\"1\"/> 看作是对 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%7B%5Ctext%7Bx%7D%7D_n%7D%7D%5C%5D\" alt=\"\\[{{{\\text{x}}_n}}\\]\" eeimg=\"1\"/> 的特征转换 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Cphi+_i%7D%5Cleft%28+%7B%7B%7B%5Ctext%7Bx%7D%7D_n%7D%7D+%5Cright%29%5C%5D\" alt=\"\\[{\\phi _i}\\left( {{{\\text{x}}_n}} \\right)\\]\" eeimg=\"1\"/> ， <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5Calpha+_t%7D%7D%5C%5D\" alt=\"\\[{{\\alpha _t}}\\]\" eeimg=\"1\"/> 就是线性模型中的权重 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5Ctext%7Bw%7D%7D_i%7D%5C%5D\" alt=\"\\[{{\\text{w}}_i}\\]\" eeimg=\"1\"/> 。回顾下前面介绍的 <a href=\"https://zhuanlan.zhihu.com/p/43043846\" class=\"internal\">SVM-Hard Margin</a>，不仅希望能够正确地分类，还希望所有样本点尽可能的远离分类超平面，对比 SVM 我们可以把 voting score 可以看作未正规化的距离（即没有除以 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Ctext%7Bw%7D%7D%5C%5D\" alt=\"\\[{\\text{w}}\\]\" eeimg=\"1\"/> 的长度），也是某种 margin，从效果上看，希望 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7By_n%7D%5Csum%5Cnolimits_%7Bt+%3D+1%7D%5ET+%7B%7B%5Calpha+_t%7D%7Bg_t%7D%5Cleft%28+%7B%7B%7B%5Ctext%7Bx%7D%7D_n%7D%7D+%5Cright%29%7D+%5C%5D\" alt=\"\\[{y_n}\\sum\\nolimits_{t = 1}^T {{\\alpha _t}{g_t}\\left( {{{\\text{x}}_n}} \\right)} \\]\" eeimg=\"1\"/> 越大越好，也就是 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Ctext%7Bu%7D%7D_n%5E%7B%5Cleft%28+%7BT+%2B+1%7D+%5Cright%29%7D%5C%5D\" alt=\"\\[{\\text{u}}_n^{\\left( {T + 1} \\right)}\\]\" eeimg=\"1\"/> 越小越好。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ee2e896fe09d0846e0d3e5d19a95dfd7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"507\" data-rawheight=\"342\" class=\"origin_image zh-lightbox-thumb\" width=\"507\" data-original=\"https://pic4.zhimg.com/v2-ee2e896fe09d0846e0d3e5d19a95dfd7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;507&#39; height=&#39;342&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"507\" data-rawheight=\"342\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"507\" data-original=\"https://pic4.zhimg.com/v2-ee2e896fe09d0846e0d3e5d19a95dfd7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-ee2e896fe09d0846e0d3e5d19a95dfd7_b.jpg\"/></figure><p>所以我们需要找到一系列的 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7Bg_t%7D%7D%5C%5D\" alt=\"\\[{{g_t}}\\]\" eeimg=\"1\"/> 使得公式 (2) 最小。</p><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%7B%5Ctext%7Bu%7D%7D_n%5E%7B%5Cleft%28+%7BT+%2B+1%7D+%5Cright%29%7D%7D++%3D+%5Cfrac%7B1%7D%7BN%7D%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%5Cexp+%5Cleft%28+%7B+-+%7By_n%7D%5Csum%5Climits_%7Bt+%3D+1%7D%5ET+%7B%7B%5Calpha+_t%7D%7Bg_t%7D%5Cleft%28+%7B%7B%7B%5Ctext%7Bx%7D%7D_n%7D%7D+%5Cright%29%7D+%7D+%5Cright%29%7D++%5Ctag%7B2%7D+%5C%5D\" alt=\"\\[\\sum\\limits_{n = 1}^N {{\\text{u}}_n^{\\left( {T + 1} \\right)}}  = \\frac{1}{N}\\sum\\limits_{n = 1}^N {\\exp \\left( { - {y_n}\\sum\\limits_{t = 1}^T {{\\alpha _t}{g_t}\\left( {{{\\text{x}}_n}} \\right)} } \\right)}  \\tag{2} \\]\" eeimg=\"1\"/> </p><p>回顾一下梯度下降法，如果想要最小化某个损失函数 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BE_%7Bin%7D%7D%5C%5D\" alt=\"\\[{E_{in}}\\]\" eeimg=\"1\"/> ，可以从当前点 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5Ctext%7Bw%7D%7D_t%7D%5C%5D\" alt=\"\\[{{\\text{w}}_t}\\]\" eeimg=\"1\"/> 出发，看看附近往哪个方向 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Ctext%7Bv%7D%7D%5C%5D\" alt=\"\\[{\\text{v}}\\]\" eeimg=\"1\"/> 走下降最快，可以对 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BE_%7Bin%7D%7D%5C%5D\" alt=\"\\[{E_{in}}\\]\" eeimg=\"1\"/> 进行一阶泰勒公式展开。</p><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BE_%7Bin%7D%7D%5Cleft%28+%7B%7B%7B%5Ctext%7Bw%7D%7D_t%7D+%2B+%5Ceta+%7B%5Ctext%7Bv%7D%7D%7D+%5Cright%29+%5Capprox+%7BE_%7Bin%7D%7D%5Cleft%28+%7B%7B%7B%5Ctext%7Bw%7D%7D_t%7D%7D+%5Cright%29+%2B+%5Ceta+%7B%7B%5Ctext%7Bv%7D%7D%5ET%7D%5Cnabla+%7BE_%7Bin%7D%7D%5Cleft%28+%7B%7B%7B%5Ctext%7Bw%7D%7D_t%7D%7D+%5Cright%29+%5Ctag%7B3%7D+%5C%5D\" alt=\"\\[{E_{in}}\\left( {{{\\text{w}}_t} + \\eta {\\text{v}}} \\right) \\approx {E_{in}}\\left( {{{\\text{w}}_t}} \\right) + \\eta {{\\text{v}}^T}\\nabla {E_{in}}\\left( {{{\\text{w}}_t}} \\right) \\tag{3} \\]\" eeimg=\"1\"/></p><p>其中步长 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Ceta++%3E+0%5C%5D\" alt=\"\\[\\eta  &gt; 0\\]\" eeimg=\"1\"/> ，所以想要 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BE_%7Bin%7D%7D%5C%5D\" alt=\"\\[{E_{in}}\\]\" eeimg=\"1\"/> 下降最快， 那么要求 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Ctext%7Bv%7D%7D%5C%5D\" alt=\"\\[{\\text{v}}\\]\" eeimg=\"1\"/> 是梯度 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cnabla+%7BE_%7Bin%7D%7D%5Cleft%28+%7B%7B%7B%5Ctext%7Bw%7D%7D_t%7D%7D+%5Cright%29%5C%5D\" alt=\"\\[\\nabla {E_{in}}\\left( {{{\\text{w}}_t}} \\right)\\]\" eeimg=\"1\"/> 的反方向。</p><p>我们能否用梯度下降法的思想求 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7Bg_t%7D%7D%5C%5D\" alt=\"\\[{{g_t}}\\]\" eeimg=\"1\"/> 呢？区别是公式 (3) 中的 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%7B%5Ctext%7Bw%7D%7D_t%7D%7D%5C%5D\" alt=\"\\[{{{\\text{w}}_t}}\\]\" eeimg=\"1\"/> 是一个向量，而我们现在要处理的是函数 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7Bg_t%7D%7D%5C%5D\" alt=\"\\[{{g_t}}\\]\" eeimg=\"1\"/> ，把 <img src=\"https://www.zhihu.com/equation?tex=h\" alt=\"h\" eeimg=\"1\"/> 当做一个方向，每次迭代找出最优的 <img src=\"https://www.zhihu.com/equation?tex=%5C%5Bh%5C%5D\" alt=\"\\[h\\]\" eeimg=\"1\"/> 使得 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5Chat+E%7D_%7B%7B%5Ctext%7BADA%7D%7D%7D%7D%5C%5D\" alt=\"\\[{{\\hat E}_{{\\text{ADA}}}}\\]\" eeimg=\"1\"/> 最小。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-59f28d296922b45c53a16614b6168389_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"575\" data-rawheight=\"212\" class=\"origin_image zh-lightbox-thumb\" width=\"575\" data-original=\"https://pic2.zhimg.com/v2-59f28d296922b45c53a16614b6168389_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;575&#39; height=&#39;212&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"575\" data-rawheight=\"212\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"575\" data-original=\"https://pic2.zhimg.com/v2-59f28d296922b45c53a16614b6168389_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-59f28d296922b45c53a16614b6168389_b.jpg\"/></figure><p>因为 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Ceta++%3E+0%5C%5D\" alt=\"\\[\\eta  &gt; 0\\]\" eeimg=\"1\"/> ，所以暂时固定 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Ceta+%5C%5D\" alt=\"\\[\\eta \\]\" eeimg=\"1\"/> ，寻找 <img src=\"https://www.zhihu.com/equation?tex=h\" alt=\"h\" eeimg=\"1\"/> 使得 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Csum%5Cnolimits_%7Bn+%3D+1%7D%5EN+%7B%7B%5Ctext%7Bu%7D%7D_n%5E%7B%5Cleft%28+t+%5Cright%29%7D%5Cleft%28+%7B+-+%7By_n%7Dh%5Cleft%28+%7B%7B%7B%5Ctext%7Bx%7D%7D_n%7D%7D+%5Cright%29%7D+%5Cright%29%7D+%5C%5D\" alt=\"\\[\\sum\\nolimits_{n = 1}^N {{\\text{u}}_n^{\\left( t \\right)}\\left( { - {y_n}h\\left( {{{\\text{x}}_n}} \\right)} \\right)} \\]\" eeimg=\"1\"/> 最小。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-d162a3ceb50e3fe8626d489ae9f17426_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"533\" data-rawheight=\"230\" class=\"origin_image zh-lightbox-thumb\" width=\"533\" data-original=\"https://pic3.zhimg.com/v2-d162a3ceb50e3fe8626d489ae9f17426_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;533&#39; height=&#39;230&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"533\" data-rawheight=\"230\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"533\" data-original=\"https://pic3.zhimg.com/v2-d162a3ceb50e3fe8626d489ae9f17426_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-d162a3ceb50e3fe8626d489ae9f17426_b.jpg\"/></figure><p> 第一项 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B+-+%5Csum%5Cnolimits_%7Bn+%3D+1%7D%5EN+%7B%7B%5Ctext%7Bu%7D%7D_n%5E%7B%5Cleft%28+t+%5Cright%29%7D%7D+%5C%5D\" alt=\"\\[ - \\sum\\nolimits_{n = 1}^N {{\\text{u}}_n^{\\left( t \\right)}} \\]\" eeimg=\"1\"/> 是已知的，所以需要最小化 <img src=\"https://www.zhihu.com/equation?tex=%5C%5BE_%7B%7B%5Ctext%7Bin%7D%7D%7D%5E%7B%7B%7B%5Ctext%7Bu%7D%7D%5E%7B%5Cleft%28+t+%5Cright%29%7D%7D%7D%5Cleft%28+h+%5Cright%29%5C%5D\" alt=\"\\[E_{{\\text{in}}}^{{{\\text{u}}^{\\left( t \\right)}}}\\left( h \\right)\\]\" eeimg=\"1\"/> ，这正是 AdaBoost 中 base algorithm 所做的事，根据 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%7B%5Ctext%7Bu%7D%7D%5E%7B%5Cleft%28+t+%5Cright%29%7D%7D%7D%5C%5D\" alt=\"\\[{{{\\text{u}}^{\\left( t \\right)}}}\\]\" eeimg=\"1\"/> 找出最佳的 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7Bg_t%7D%7D%5C%5D\" alt=\"\\[{{g_t}}\\]\" eeimg=\"1\"/> 。所以说 AdaBoost 中的  base algorithm 正好帮我们找到了梯度下降中下一步最好的函数方向。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-34276c3899caab721847437c254af0ea_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"569\" data-rawheight=\"90\" class=\"origin_image zh-lightbox-thumb\" width=\"569\" data-original=\"https://pic3.zhimg.com/v2-34276c3899caab721847437c254af0ea_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;569&#39; height=&#39;90&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"569\" data-rawheight=\"90\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"569\" data-original=\"https://pic3.zhimg.com/v2-34276c3899caab721847437c254af0ea_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-34276c3899caab721847437c254af0ea_b.jpg\"/></figure><p>对 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cexp+%5Cleft%28+%7B+-+%7By_n%7D%5Ceta+%7Bg_t%7D%5Cleft%28+%7B%7B%7B%5Ctext%7Bx%7D%7D_n%7D%7D+%5Cright%29%7D+%5Cright%29%5C%5D\" alt=\"\\[\\exp \\left( { - {y_n}\\eta {g_t}\\left( {{{\\text{x}}_n}} \\right)} \\right)\\]\" eeimg=\"1\"/> 进一步细分化简：</p><ul><li>分类正确：<img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7By_n%7D+%3D+%7Bg_t%7D%5Cleft%28+%7B%7B%7B%5Ctext%7Bx%7D%7D_n%7D%7D+%5Cright%29%EF%BC%9A%7B%5Ctext%7Bu%7D%7D_n%5E%7B%5Cleft%28+t+%5Cright%29%7D%5Cexp+%5Cleft%28+-%5Ceta++%5Cright%29%5C%5D\" alt=\"\\[{y_n} = {g_t}\\left( {{{\\text{x}}_n}} \\right)：{\\text{u}}_n^{\\left( t \\right)}\\exp \\left( -\\eta  \\right)\\]\" eeimg=\"1\"/> </li><li>分类错误： <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7By_n%7D+%5Cne+%7Bg_t%7D%5Cleft%28+%7B%7B%7B%5Ctext%7Bx%7D%7D_n%7D%7D+%5Cright%29%EF%BC%9A%7B%5Ctext%7Bu%7D%7D_n%5E%7B%5Cleft%28+t+%5Cright%29%7D%5Cexp+%5Cleft%28+%2B%5Ceta++%5Cright%29%5C%5D\" alt=\"\\[{y_n} \\ne {g_t}\\left( {{{\\text{x}}_n}} \\right)：{\\text{u}}_n^{\\left( t \\right)}\\exp \\left( +\\eta  \\right)\\]\" eeimg=\"1\"/></li></ul><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+++%7B%7B%5Chat+E%7D_%7B%7B%5Ctext%7BADA%7D%7D%7D%7D+%26%3D+%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%7B%5Ctext%7Bu%7D%7D_n%5E%7B%5Cleft%28+t+%5Cright%29%7D%5Cexp+%5Cleft%28+%7B+-+%7By_n%7D%5Ceta+%7Bg_t%7D%5Cleft%28+%7B%7B%7B%5Ctext%7Bx%7D%7D_n%7D%7D+%5Cright%29%7D+%5Cright%29%7D++%5Chfill+%5C%5C++++%26%3D+%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%5Cleft%5B%5Ckern-0.15em%5Cleft%5B+%7B%7By_n%7D+%3D+%7Bg_t%7D%5Cleft%28+%7B%7B%7B%5Ctext%7Bx%7D%7D_n%7D%7D+%5Cright%29%7D+++%5Cright%5D%5Ckern-0.15em%5Cright%5D%7B%5Ctext%7Bu%7D%7D_n%5E%7B%5Cleft%28+t+%5Cright%29%7D%5Cexp+%5Cleft%28+%7B+-+%5Ceta+%7D+%5Cright%29%7D++%2B+%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%5Cleft%5B%5Ckern-0.15em%5Cleft%5B+%7B%7By_n%7D+%5Cne+%7Bg_t%7D%5Cleft%28+%7B%7B%7B%5Ctext%7Bx%7D%7D_n%7D%7D+%5Cright%29%7D+++%5Cright%5D%5Ckern-0.15em%5Cright%5D%7B%5Ctext%7Bu%7D%7D_n%5E%7B%5Cleft%28+t+%5Cright%29%7D%5Cexp+%5Cleft%28+%5Ceta++%5Cright%29%7D++%5Chfill+%5C%5C+++%26+%3D+%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%7B%5Ctext%7Bu%7D%7D_n%5E%7B%5Cleft%28+t+%5Cright%29%7D%7D++%5Ccdot+%5Cleft%28+%7B%5Cfrac%7B%7B%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%5Cleft%5B%5Ckern-0.15em%5Cleft%5B+%7B%7By_n%7D+%3D+%7Bg_t%7D%5Cleft%28+%7B%7B%7B%5Ctext%7Bx%7D%7D_n%7D%7D+%5Cright%29%7D+++%5Cright%5D%5Ckern-0.15em%5Cright%5D%7B%5Ctext%7Bu%7D%7D_n%5E%7B%5Cleft%28+t+%5Cright%29%7D%7D+%7D%7D%7B%7B%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%7B%5Ctext%7Bu%7D%7D_n%5E%7B%5Cleft%28+t+%5Cright%29%7D%7D+%7D%7D%5Cexp+%5Cleft%28+%7B+-+%5Ceta+%7D+%5Cright%29+%2B+%5Cfrac%7B%7B%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%5Cleft%5B%5Ckern-0.15em%5Cleft%5B+%7B%7By_n%7D+%5Cne+%7Bg_t%7D%5Cleft%28+%7B%7B%7B%5Ctext%7Bx%7D%7D_n%7D%7D+%5Cright%29%7D+++%5Cright%5D%5Ckern-0.15em%5Cright%5D%7B%5Ctext%7Bu%7D%7D_n%5E%7B%5Cleft%28+t+%5Cright%29%7D%7D+%7D%7D%7B%7B%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%7B%5Ctext%7Bu%7D%7D_n%5E%7B%5Cleft%28+t+%5Cright%29%7D%7D+%7D%7D%5Cexp+%5Cleft%28+%5Ceta++%5Cright%29%7D+%5Cright%29+%5Chfill+%5C%5C++++%26%3D+%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%7B%5Ctext%7Bu%7D%7D_n%5E%7B%5Cleft%28+t+%5Cright%29%7D%7D++%5Ccdot+%5Cleft%28+%7B%5Cleft%28+%7B1+-+%7B%5Cvarepsilon+_t%7D%7D+%5Cright%29%5Cexp+%5Cleft%28+%7B+-+%5Ceta+%7D+%5Cright%29+%2B+%7B%5Cvarepsilon+_t%7D%5Cexp+%5Cleft%28+%5Ceta++%5Cright%29%7D+%5Cright%29+%5Chfill+%5C%5C++%5Ctag%7B4%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*}   {{\\hat E}_{{\\text{ADA}}}} &amp;= \\sum\\limits_{n = 1}^N {{\\text{u}}_n^{\\left( t \\right)}\\exp \\left( { - {y_n}\\eta {g_t}\\left( {{{\\text{x}}_n}} \\right)} \\right)}  \\hfill \\\\    &amp;= \\sum\\limits_{n = 1}^N {\\left[\\kern-0.15em\\left[ {{y_n} = {g_t}\\left( {{{\\text{x}}_n}} \\right)}   \\right]\\kern-0.15em\\right]{\\text{u}}_n^{\\left( t \\right)}\\exp \\left( { - \\eta } \\right)}  + \\sum\\limits_{n = 1}^N {\\left[\\kern-0.15em\\left[ {{y_n} \\ne {g_t}\\left( {{{\\text{x}}_n}} \\right)}   \\right]\\kern-0.15em\\right]{\\text{u}}_n^{\\left( t \\right)}\\exp \\left( \\eta  \\right)}  \\hfill \\\\   &amp; = \\sum\\limits_{n = 1}^N {{\\text{u}}_n^{\\left( t \\right)}}  \\cdot \\left( {\\frac{{\\sum\\limits_{n = 1}^N {\\left[\\kern-0.15em\\left[ {{y_n} = {g_t}\\left( {{{\\text{x}}_n}} \\right)}   \\right]\\kern-0.15em\\right]{\\text{u}}_n^{\\left( t \\right)}} }}{{\\sum\\limits_{n = 1}^N {{\\text{u}}_n^{\\left( t \\right)}} }}\\exp \\left( { - \\eta } \\right) + \\frac{{\\sum\\limits_{n = 1}^N {\\left[\\kern-0.15em\\left[ {{y_n} \\ne {g_t}\\left( {{{\\text{x}}_n}} \\right)}   \\right]\\kern-0.15em\\right]{\\text{u}}_n^{\\left( t \\right)}} }}{{\\sum\\limits_{n = 1}^N {{\\text{u}}_n^{\\left( t \\right)}} }}\\exp \\left( \\eta  \\right)} \\right) \\hfill \\\\    &amp;= \\sum\\limits_{n = 1}^N {{\\text{u}}_n^{\\left( t \\right)}}  \\cdot \\left( {\\left( {1 - {\\varepsilon _t}} \\right)\\exp \\left( { - \\eta } \\right) + {\\varepsilon _t}\\exp \\left( \\eta  \\right)} \\right) \\hfill \\\\  \\tag{4} \\end{align*}\" eeimg=\"1\"/> </p><p>求 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cfrac%7B%7B%5Cpartial+%7B%7B%5Chat+E%7D_%7B%7B%5Ctext%7BADA%7D%7D%7D%7D%7D%7D%7B%7B%5Cpartial+%5Ceta+%7D%7D%5C%5D\" alt=\"\\[\\frac{{\\partial {{\\hat E}_{{\\text{ADA}}}}}}{{\\partial \\eta }}\\]\" eeimg=\"1\"/> 并令导数为 <img src=\"https://www.zhihu.com/equation?tex=0\" alt=\"0\" eeimg=\"1\"/> ，可以得到 ：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Ceta++%3D+%5Cln+%5Cleft%28+%7B%5Csqrt+%7B%5Cfrac%7B%7B1+-+%7B%5Cvarepsilon+_t%7D%7D%7D%7B%7B%7B%5Cvarepsilon+_t%7D%7D%7D%7D+%7D+%5Cright%29+%5Ctag%7B5%7D+%5C%5D\" alt=\"\\[\\eta  = \\ln \\left( {\\sqrt {\\frac{{1 - {\\varepsilon _t}}}{{{\\varepsilon _t}}}} } \\right) \\tag{5} \\]\" eeimg=\"1\"/></p><p> 看着是否似曾相识，这个就是 AdaBoost 中的 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Calpha+_t%7D%5C%5D\" alt=\"\\[{\\alpha _t}\\]\" eeimg=\"1\"/> ，所以 AdaBoost 算法其实是在 gradient descent 上寻找下降最快的函数方向 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7Bg_t%7D%7D%5C%5D\" alt=\"\\[{{g_t}}\\]\" eeimg=\"1\"/> 和最佳的步长 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Calpha+_t%7D%5C%5D\" alt=\"\\[{\\alpha _t}\\]\" eeimg=\"1\"/> 。</p><h2><b>三、Gradient Boosting</b></h2><p><b>1. Gradient Boosting for Arbitrary Error Function</b></p><p>在 AdaBoost 中如果将 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cexp+%5C%5D\" alt=\"\\[\\exp \\]\" eeimg=\"1\"/> 看作一个 error 函数，AdaBoost 的每一轮就是在这个 error 上一步一步的做最佳化，每一步的时候找出一个 <img src=\"https://www.zhihu.com/equation?tex=h\" alt=\"h\" eeimg=\"1\"/> 和步长 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Ceta+%5C%5D\" alt=\"\\[\\eta \\]\" eeimg=\"1\"/> ，所以有两个部分，一个部分是对 <img src=\"https://www.zhihu.com/equation?tex=h\" alt=\"h\" eeimg=\"1\"/> 做最佳化，一部分是对 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Ceta+%5C%5D\" alt=\"\\[\\eta \\]\" eeimg=\"1\"/> 做最佳化。根据这个概念，我们可以进一步的延伸到不同的 error 函数，称为 GradientBoost。因为用的 Gradient，一般 error 函数需要可导。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-e205b1e222c007472d14bb57558a42e3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"525\" data-rawheight=\"389\" class=\"origin_image zh-lightbox-thumb\" width=\"525\" data-original=\"https://pic4.zhimg.com/v2-e205b1e222c007472d14bb57558a42e3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;525&#39; height=&#39;389&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"525\" data-rawheight=\"389\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"525\" data-original=\"https://pic4.zhimg.com/v2-e205b1e222c007472d14bb57558a42e3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-e205b1e222c007472d14bb57558a42e3_b.jpg\"/></figure><p><b>2. GradientBoost for Regression</b></p><p>我们利用 GradientBoost 思想求解 regression。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-e69334cdad1676b8ab1c94e166890216_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"570\" data-rawheight=\"258\" class=\"origin_image zh-lightbox-thumb\" width=\"570\" data-original=\"https://pic3.zhimg.com/v2-e69334cdad1676b8ab1c94e166890216_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;570&#39; height=&#39;258&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"570\" data-rawheight=\"258\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"570\" data-original=\"https://pic3.zhimg.com/v2-e69334cdad1676b8ab1c94e166890216_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-e69334cdad1676b8ab1c94e166890216_b.jpg\"/></figure><p>直观地看，如果 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cleft%28+%7B%7Bs_n%7D+-+%7By_n%7D%7D+%5Cright%29+%3E+0%5C%5D\" alt=\"\\[\\left( {{s_n} - {y_n}} \\right) &gt; 0\\]\" eeimg=\"1\"/> ，那么 <img src=\"https://www.zhihu.com/equation?tex=%5C%5Bh%5Cleft%28+%7B%7B%7B%5Ctext%7Bx%7D%7D_n%7D%7D+%5Cright%29+%3D++-+%5Cinfty+%5C%5D\" alt=\"\\[h\\left( {{{\\text{x}}_n}} \\right) =  - \\infty \\]\" eeimg=\"1\"/> ，如果 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cleft%28+%7B%7Bs_n%7D+-+%7By_n%7D%7D+%5Cright%29+%3C+0%5C%5D\" alt=\"\\[\\left( {{s_n} - {y_n}} \\right) &lt; 0\\]\" eeimg=\"1\"/> ，那么 <img src=\"https://www.zhihu.com/equation?tex=%5C%5Bh%5Cleft%28+%7B%7B%7B%5Ctext%7Bx%7D%7D_n%7D%7D+%5Cright%29+%3D++%2B+%5Cinfty+%5C%5D\" alt=\"\\[h\\left( {{{\\text{x}}_n}} \\right) =  + \\infty \\]\" eeimg=\"1\"/> ，好像 <img src=\"https://www.zhihu.com/equation?tex=E\" alt=\"E\" eeimg=\"1\"/> 就可以变得无限的小，得到 <img src=\"https://www.zhihu.com/equation?tex=%5C%5Bh%5Cleft%28+%7B%7B%7B%5Ctext%7Bx%7D%7D_n%7D%7D+%5Cright%29+%3D++-+%5Cinfty++%5Ccdot+%5Cleft%28+%7B%7Bs_n%7D+-+%7By_n%7D%7D+%5Cright%29%5C%5D\" alt=\"\\[h\\left( {{{\\text{x}}_n}} \\right) =  - \\infty  \\cdot \\left( {{s_n} - {y_n}} \\right)\\]\" eeimg=\"1\"/> 。这样得到的 <img src=\"https://www.zhihu.com/equation?tex=%5C%5Bh%5C%5D\" alt=\"\\[h\\]\" eeimg=\"1\"/> 不太对，其实我们需要找的是一个方向函数 <img src=\"https://www.zhihu.com/equation?tex=%5C%5Bh%5C%5D\" alt=\"\\[h\\]\" eeimg=\"1\"/> ，不太关系它的大小，因为在 <img src=\"https://www.zhihu.com/equation?tex=%5C%5Bh%5C%5D\" alt=\"\\[h\\]\" eeimg=\"1\"/> 的方向上走多大的步长可以由 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Ceta+%5C%5D\" alt=\"\\[\\eta \\]\" eeimg=\"1\"/> 决定，所以我们需要对 <img src=\"https://www.zhihu.com/equation?tex=%5C%5Bh%5C%5D\" alt=\"\\[h\\]\" eeimg=\"1\"/> 的大小做限制，比如令 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cleft%5C%7C+h+%5Cright%5C%7C+%3D+1%5C%5D\" alt=\"\\[\\left\\| h \\right\\| = 1\\]\" eeimg=\"1\"/> ，如果加这种限制，就变成解一个有约束项的最小化问题，增加了求解复杂度。类似正则化中我们可以加 <img src=\"https://www.zhihu.com/equation?tex=L2\" alt=\"L2\" eeimg=\"1\"/> 范数限制权重 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Ctext%7Bw%7D%7D%5C%5D\" alt=\"\\[{\\text{w}}\\]\" eeimg=\"1\"/> 的大小，我们可以用同样的方法约束 <img src=\"https://www.zhihu.com/equation?tex=h\" alt=\"h\" eeimg=\"1\"/> 的大小，在上面最佳化问题中加 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Cleft%28+%7Bh%5Cleft%28+%7B%7B%7B%5Ctext%7Bx%7D%7D_n%7D%7D+%5Cright%29%7D+%5Cright%29%5E2%7D%5C%5D\" alt=\"\\[{\\left( {h\\left( {{{\\text{x}}_n}} \\right)} \\right)^2}\\]\" eeimg=\"1\"/> 。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-55e69c956a49fa0ed15552674702b62f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"551\" data-rawheight=\"305\" class=\"origin_image zh-lightbox-thumb\" width=\"551\" data-original=\"https://pic4.zhimg.com/v2-55e69c956a49fa0ed15552674702b62f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;551&#39; height=&#39;305&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"551\" data-rawheight=\"305\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"551\" data-original=\"https://pic4.zhimg.com/v2-55e69c956a49fa0ed15552674702b62f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-55e69c956a49fa0ed15552674702b62f_b.jpg\"/></figure><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cmin+%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%7B%7B%5Cleft%28+%7Bh%5Cleft%28+%7B%7B%7B%5Ctext%7Bx%7D%7D_n%7D%7D+%5Cright%29+-+%5Cleft%28+%7B%7By_n%7D+-+%7Bs_n%7D%7D+%5Cright%29%7D+%5Cright%29%7D%5E2%7D%7D++%5Ctag%7B6%7D+%5C%5D\" alt=\"\\[\\min \\sum\\limits_{n = 1}^N {{{\\left( {h\\left( {{{\\text{x}}_n}} \\right) - \\left( {{y_n} - {s_n}} \\right)} \\right)}^2}}  \\tag{6} \\]\" eeimg=\"1\"/></p><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7By_n%7D+-+%7Bs_n%7D%7D%5C%5D\" alt=\"\\[{{y_n} - {s_n}}\\]\" eeimg=\"1\"/> 表示真实值和前面 <img src=\"https://www.zhihu.com/equation?tex=t-1\" alt=\"t-1\" eeimg=\"1\"/> 个 <img src=\"https://www.zhihu.com/equation?tex=g\" alt=\"g\" eeimg=\"1\"/> 组成的 <img src=\"https://www.zhihu.com/equation?tex=G\" alt=\"G\" eeimg=\"1\"/> 预测值之间的差，称为余数。如果想要预测值更好的接近真实值，我们希望第 <img src=\"https://www.zhihu.com/equation?tex=t\" alt=\"t\" eeimg=\"1\"/> 个 <img src=\"https://www.zhihu.com/equation?tex=%5C%5Bh%5Cleft%28+%7B%7B%7B%5Ctext%7Bx%7D%7D_n%7D%7D+%5Cright%29%5C%5D\" alt=\"\\[h\\left( {{{\\text{x}}_n}} \\right)\\]\" eeimg=\"1\"/> 的值和余数相等。所以我们可以在 <img src=\"https://www.zhihu.com/equation?tex=N\" alt=\"N\" eeimg=\"1\"/> 个样本 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cleft%28+%7B%7B%7B%5Ctext%7Bx%7D%7D_n%7D%2C%7By_n%7D+-+%7Bs_n%7D%7D+%5Cright%29%5C%5D\" alt=\"\\[\\left( {{{\\text{x}}_n},{y_n} - {s_n}} \\right)\\]\" eeimg=\"1\"/> 上做 squared-error 的 regression，得到最优的 <img src=\"https://www.zhihu.com/equation?tex=%5C%5Bh%5C%5D\" alt=\"\\[h\\]\" eeimg=\"1\"/> 就是 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7Bg_t%7D%7D%5C%5D\" alt=\"\\[{{g_t}}\\]\" eeimg=\"1\"/> 。</p><p>我们已经可以找到最佳的 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7Bg_t%7D%7D%5C%5D\" alt=\"\\[{{g_t}}\\]\" eeimg=\"1\"/> ，那么还需要找到最佳的步长 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Ceta+%5C%5D\" alt=\"\\[\\eta \\]\" eeimg=\"1\"/> 。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-7ecb5ed960f1a0600789a616ecf58048_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"562\" data-rawheight=\"327\" class=\"origin_image zh-lightbox-thumb\" width=\"562\" data-original=\"https://pic1.zhimg.com/v2-7ecb5ed960f1a0600789a616ecf58048_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;562&#39; height=&#39;327&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"562\" data-rawheight=\"327\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"562\" data-original=\"https://pic1.zhimg.com/v2-7ecb5ed960f1a0600789a616ecf58048_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-7ecb5ed960f1a0600789a616ecf58048_b.jpg\"/></figure><p>这是一个单变量最优解问题，可以直接求解析解，当然我们也可以看做是一个 regression 问题， <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7Bg_t%7D%5Cleft%28+%7B%7B%7B%5Ctext%7Bx%7D%7D_n%7D%7D+%5Cright%29%7D%5C%5D\" alt=\"\\[{{g_t}\\left( {{{\\text{x}}_n}} \\right)}\\]\" eeimg=\"1\"/> 可以看成对 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%7B%5Ctext%7Bx%7D%7D_n%7D%7D%5C%5D\" alt=\"\\[{{{\\text{x}}_n}}\\]\" eeimg=\"1\"/> 的特征转换， <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7By_n%7D+-+%7Bs_n%7D%7D%5C%5D\" alt=\"\\[{{y_n} - {s_n}}\\]\" eeimg=\"1\"/> 看做输出值， <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Ceta+%5C%5D\" alt=\"\\[\\eta \\]\" eeimg=\"1\"/> 可以看做权重。</p><p><b>3. Gradient Boosted Decision Tree</b></p><p>我们利用 Decision Tree 求解在 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cleft%28+%7B%7B%7B%5Ctext%7Bx%7D%7D_n%7D%2C%7By_n%7D+-+%7Bs_n%7D%7D+%5Cright%29%5C%5D\" alt=\"\\[\\left( {{{\\text{x}}_n},{y_n} - {s_n}} \\right)\\]\" eeimg=\"1\"/> 样本集上的回归问题，称为GBDT。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-f9efa2c4b8ac1085761fc8a05edfd7b8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"521\" data-rawheight=\"232\" class=\"origin_image zh-lightbox-thumb\" width=\"521\" data-original=\"https://pic1.zhimg.com/v2-f9efa2c4b8ac1085761fc8a05edfd7b8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;521&#39; height=&#39;232&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"521\" data-rawheight=\"232\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"521\" data-original=\"https://pic1.zhimg.com/v2-f9efa2c4b8ac1085761fc8a05edfd7b8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-f9efa2c4b8ac1085761fc8a05edfd7b8_b.jpg\"/></figure><p>前面介绍的 AdaBoost-DTree 是解决 binary classification 问题，而此处介绍的 GBDT 是解决 regression 问题。二者具有一定的相似性，可以说 GBDT 就是 AdaBoost-DTree 的regression版本</p>", 
            "topic": [
                {
                    "tag": "算法", 
                    "tagLink": "https://api.zhihu.com/topics/19553510"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "科技", 
                    "tagLink": "https://api.zhihu.com/topics/19556664"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/43753699", 
            "userName": "徽州风韵", 
            "userLink": "https://www.zhihu.com/people/53bd6f8b93873b03c5edb714aea2a456", 
            "upvote": 0, 
            "title": "Random Forest", 
            "content": "<h2><b>一、Random Forest</b></h2><p>前面介绍的 Bagging 采用 uniform 方式将所有的 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bg_t%7D%5C%5D\" alt=\"\\[{g_t}\\]\" eeimg=\"1\"/> 组合起来，在 <a href=\"https://zhuanlan.zhihu.com/p/43281027\" class=\"internal\">Aggregation Models-blending</a> 中证明 uniform 可以有效地降低 variance；而 Decision Tree 增大 variance，因为不同的样本，每次切割的方式不同，得到不同的树，对不同的样本比较敏感些。</p><p>把 Bagging 和 Decision Tree 结合起来，能发挥各自的优势，起到互补作用。通过 Bagging  方式把不同的决策树组合起来，可以有效地避免单个决策树造成的过拟合问题。这种算法就称为随机森林。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-e477c37a96ac7bdc37bd3380d2884fed_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"553\" data-rawheight=\"405\" class=\"origin_image zh-lightbox-thumb\" width=\"553\" data-original=\"https://pic2.zhimg.com/v2-e477c37a96ac7bdc37bd3380d2884fed_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;553&#39; height=&#39;405&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"553\" data-rawheight=\"405\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"553\" data-original=\"https://pic2.zhimg.com/v2-e477c37a96ac7bdc37bd3380d2884fed_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-e477c37a96ac7bdc37bd3380d2884fed_b.jpg\"/></figure><p>通过 bootstrap 得到不同的训练数据 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BD_t%7D%5C%5D\" alt=\"\\[{D_t}\\]\" eeimg=\"1\"/> ，从而得到不同的决策树。我们可以进一步地增加多样性，每次除了抽取不同样本外，我们再对特征进行随机抽取。例如：原来有 <img src=\"https://www.zhihu.com/equation?tex=d\" alt=\"d\" eeimg=\"1\"/> 个特征，现在随机选择 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bd%27%7D%5C%5D\" alt=\"\\[{d&#39;}\\]\" eeimg=\"1\"/> 个特征来建立决策树，那么每一轮得到的决策树都由不同的 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bd%27%7D%5C%5D\" alt=\"\\[{d&#39;}\\]\" eeimg=\"1\"/> 个特征构成，这类似是一种从 <img src=\"https://www.zhihu.com/equation?tex=d\" alt=\"d\" eeimg=\"1\"/> 维到 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bd%27%7D%5C%5D\" alt=\"\\[{d&#39;}\\]\" eeimg=\"1\"/> 维的特征转换，相当于是从高维到低维的投影，也就是说 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bd%27%7D%5C%5D\" alt=\"\\[{d&#39;}\\]\" eeimg=\"1\"/> 维 <img src=\"https://www.zhihu.com/equation?tex=z\" alt=\"z\" eeimg=\"1\"/> 空间其实就是 <img src=\"https://www.zhihu.com/equation?tex=d\" alt=\"d\" eeimg=\"1\"/> 维 <img src=\"https://www.zhihu.com/equation?tex=x\" alt=\"x\" eeimg=\"1\"/> 空间的一个随机子空间（subspace）。通常情况下，<img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bd%27%7D%5C%5D\" alt=\"\\[{d&#39;}\\]\" eeimg=\"1\"/> 远小于 <img src=\"https://www.zhihu.com/equation?tex=d\" alt=\"d\" eeimg=\"1\"/> ，从而保证算法更有效率。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-028e520f94a91c28a3f0a9643995105d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"540\" data-rawheight=\"262\" class=\"origin_image zh-lightbox-thumb\" width=\"540\" data-original=\"https://pic2.zhimg.com/v2-028e520f94a91c28a3f0a9643995105d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;540&#39; height=&#39;262&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"540\" data-rawheight=\"262\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"540\" data-original=\"https://pic2.zhimg.com/v2-028e520f94a91c28a3f0a9643995105d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-028e520f94a91c28a3f0a9643995105d_b.jpg\"/></figure><p>上面的方法是特征抽取，即对于某个特征 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5Crm%7Bx%7D%7D_%7B%7Bi_k%7D%7D%7D%5C%5D\" alt=\"\\[{{\\rm{x}}_{{i_k}}}\\]\" eeimg=\"1\"/> ，非 <img src=\"https://www.zhihu.com/equation?tex=0\" alt=\"0\" eeimg=\"1\"/> 即 <img src=\"https://www.zhihu.com/equation?tex=1\" alt=\"1\" eeimg=\"1\"/> 。除此之外，我们还可以对特征进行线性组合，达到多样性。</p><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5CPhi+%5Cleft%28+%7B%5Crm%7Bx%7D%7D+%5Cright%29+%3D+%7B%5Crm%7Bp%7D%7D+%5Ccdot+%7B%5Crm%7Bx%7D%7D+%5Ctag%7B1%7D+%5C%5D\" alt=\"\\[\\Phi \\left( {\\rm{x}} \\right) = {\\rm{p}} \\cdot {\\rm{x}} \\tag{1} \\]\" eeimg=\"1\"/> </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-456b8953dbc230e4c09939e9ef8a65ce_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"569\" data-rawheight=\"389\" class=\"origin_image zh-lightbox-thumb\" width=\"569\" data-original=\"https://pic3.zhimg.com/v2-456b8953dbc230e4c09939e9ef8a65ce_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;569&#39; height=&#39;389&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"569\" data-rawheight=\"389\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"569\" data-original=\"https://pic3.zhimg.com/v2-456b8953dbc230e4c09939e9ef8a65ce_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-456b8953dbc230e4c09939e9ef8a65ce_b.jpg\"/></figure><h2><b>二、Out-Of-Bag Estimate</b></h2><p>每次 bootstrap 得到不同的 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5Ctilde+D%7D_t%7D%5C%5D\" alt=\"\\[{{\\tilde D}_t}\\]\" eeimg=\"1\"/> ， <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5Ctilde+D%7D_t%7D%5C%5D\" alt=\"\\[{{\\tilde D}_t}\\]\" eeimg=\"1\"/> 是 <img src=\"https://www.zhihu.com/equation?tex=%5C%5BD%5C%5D\" alt=\"\\[D\\]\" eeimg=\"1\"/> 的一部分，有些样本不在 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5Ctilde+D%7D_t%7D%5C%5D\" alt=\"\\[{{\\tilde D}_t}\\]\" eeimg=\"1\"/> 中。例如图1中，红色 * 表示不在 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5Ctilde+D%7D_t%7D%5C%5D\" alt=\"\\[{{\\tilde D}_t}\\]\" eeimg=\"1\"/> 中的样本，对于 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bg_1%7D%5C%5D\" alt=\"\\[{g_1}\\]\" eeimg=\"1\"/> 的训练数据 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5Ctilde+D%7D_1%7D%5C%5D\" alt=\"\\[{{\\tilde D}_1}\\]\" eeimg=\"1\"/> 来说，样本 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cleft%28+%7B%7B%7B%5Crm%7Bx%7D%7D_2%7D%2C%7By_2%7D%7D+%5Cright%29%2C%5Cleft%28+%7B%7B%7B%5Crm%7Bx%7D%7D_3%7D%2C%7By_3%7D%7D+%5Cright%29%5C%5D\" alt=\"\\[\\left( {{{\\rm{x}}_2},{y_2}} \\right),\\left( {{{\\rm{x}}_3},{y_3}} \\right)\\]\" eeimg=\"1\"/> 不在其中，对于 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bg_2%7D%5C%5D\" alt=\"\\[{g_2}\\]\" eeimg=\"1\"/> 的训练数据 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5Ctilde+D%7D_2%7D%5C%5D\" alt=\"\\[{{\\tilde D}_2}\\]\" eeimg=\"1\"/> 来说， <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cleft%28+%7B%7B%7B%5Crm%7Bx%7D%7D_1%7D%2C%7By_1%7D%7D+%5Cright%29%2C%5Cleft%28+%7B%7B%7B%5Crm%7Bx%7D%7D_2%7D%2C%7By_2%7D%7D+%5Cright%29%5C%5D\" alt=\"\\[\\left( {{{\\rm{x}}_1},{y_1}} \\right),\\left( {{{\\rm{x}}_2},{y_2}} \\right)\\]\" eeimg=\"1\"/>不在其中。对于每个 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bg_t%7D%5C%5D\" alt=\"\\[{g_t}\\]\" eeimg=\"1\"/> ，没有包含进去的样本称为 out-of-bag(OOB) example。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-013387aa8a8072f2151d1e674e57fcb7_b.jpg\" data-size=\"normal\" data-rawwidth=\"296\" data-rawheight=\"142\" class=\"content_image\" width=\"296\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;296&#39; height=&#39;142&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"296\" data-rawheight=\"142\" class=\"content_image lazy\" width=\"296\" data-actualsrc=\"https://pic4.zhimg.com/v2-013387aa8a8072f2151d1e674e57fcb7_b.jpg\"/><figcaption>图 1</figcaption></figure><p> 那么对于 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bg_t%7D%5C%5D\" alt=\"\\[{g_t}\\]\" eeimg=\"1\"/> 有多少 OOB 样本呢？假设 <img src=\"https://www.zhihu.com/equation?tex=D\" alt=\"D\" eeimg=\"1\"/> 的数量为 <img src=\"https://www.zhihu.com/equation?tex=N\" alt=\"N\" eeimg=\"1\"/>，bootstrap 的样本数量也为 <img src=\"https://www.zhihu.com/equation?tex=%5C%5BN%5C%5D\" alt=\"\\[N\\]\" eeimg=\"1\"/> ，某个样本 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cleft%28+%7B%7B%7B%5Crm%7Bx%7D%7D_n%7D%2C%7By_n%7D%7D+%5Cright%29%5C%5D\" alt=\"\\[\\left( {{{\\rm{x}}_n},{y_n}} \\right)\\]\" eeimg=\"1\"/> 属于 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bg_t%7D%5C%5D\" alt=\"\\[{g_t}\\]\" eeimg=\"1\"/> 的概率为 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Cleft%28+%7B1+-+%5Cfrac%7B1%7D%7BN%7D%7D+%5Cright%29%5EN%7D%5C%5D\" alt=\"\\[{\\left( {1 - \\frac{1}{N}} \\right)^N}\\]\" eeimg=\"1\"/> ，如果当 <img src=\"https://www.zhihu.com/equation?tex=%5C%5BN%5C%5D\" alt=\"\\[N\\]\" eeimg=\"1\"/> 非常大。</p><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Cleft%28+%7B1+-+%5Cfrac%7B1%7D%7BN%7D%7D+%5Cright%29%5EN%7D+%3D+%5Cfrac%7B1%7D%7B%7B%7B%7B%5Cleft%28+%7B%5Cfrac%7BN%7D%7B%7BN+-+1%7D%7D%7D+%5Cright%29%7D%5EN%7D%7D%7D+%3D+%5Cfrac%7B1%7D%7B%7B%7B%7B%5Cleft%28+%7B1+%2B+%5Cfrac%7B1%7D%7B%7BN+-+1%7D%7D%7D+%5Cright%29%7D%5EN%7D%7D%7D+%3D+%5Cfrac%7B1%7D%7Be%7D+%5Ctag%7B2%7D+%5C%5D\" alt=\"\\[{\\left( {1 - \\frac{1}{N}} \\right)^N} = \\frac{1}{{{{\\left( {\\frac{N}{{N - 1}}} \\right)}^N}}} = \\frac{1}{{{{\\left( {1 + \\frac{1}{{N - 1}}} \\right)}^N}}} = \\frac{1}{e} \\tag{2} \\]\" eeimg=\"1\"/> </p><p>所以对于 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bg_t%7D%5C%5D\" alt=\"\\[{g_t}\\]\" eeimg=\"1\"/> 的 OOB 数量大概为 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cfrac%7B1%7D%7Be%7DN%5C%5D\" alt=\"\\[\\frac{1}{e}N\\]\" eeimg=\"1\"/> ，也就是大概三分之一的样本在 bootstrap 中没有被抽中。</p><p>OOB 样本可以作为 Val 数据集用来验证模型。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-061b8f4e4c04e99f0720bcc85d3245ef_b.jpg\" data-size=\"normal\" data-rawwidth=\"563\" data-rawheight=\"315\" class=\"origin_image zh-lightbox-thumb\" width=\"563\" data-original=\"https://pic4.zhimg.com/v2-061b8f4e4c04e99f0720bcc85d3245ef_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;563&#39; height=&#39;315&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"563\" data-rawheight=\"315\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"563\" data-original=\"https://pic4.zhimg.com/v2-061b8f4e4c04e99f0720bcc85d3245ef_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-061b8f4e4c04e99f0720bcc85d3245ef_b.jpg\"/><figcaption>图 2</figcaption></figure><p>在 Validation 表中， <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BD_%7B%7B%5Crm%7Btrain%7D%7D%7D%7D%5C%5D\" alt=\"\\[{D_{{\\rm{train}}}}\\]\" eeimg=\"1\"/> 用来训练 <img src=\"https://www.zhihu.com/equation?tex=%5C%5Bg_m%5E+-+%5C%5D\" alt=\"\\[g_m^ - \\]\" eeimg=\"1\"/> ， <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BD_%7B%7B%5Crm%7Bval%7D%7D%7D%7D%5C%5D\" alt=\"\\[{D_{{\\rm{val}}}}\\]\" eeimg=\"1\"/> 用来计算各个 <img src=\"https://www.zhihu.com/equation?tex=%5C%5Bg_m%5E+-+%5C%5D\" alt=\"\\[g_m^ - \\]\" eeimg=\"1\"/> 的 performance。其中 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BD_%7B%7B%5Crm%7Btrain%7D%7D%7D%7D%5C%5D\" alt=\"\\[{D_{{\\rm{train}}}}\\]\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BD_%7B%7B%5Crm%7Bval%7D%7D%7D%7D%5C%5D\" alt=\"\\[{D_{{\\rm{val}}}}\\]\" eeimg=\"1\"/> 没有交集，并且 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BD_%7B%7B%5Crm%7Btrain%7D%7D%7D%7D%5C%5D\" alt=\"\\[{D_{{\\rm{train}}}}\\]\" eeimg=\"1\"/> 数量是 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BD_%7B%7B%5Crm%7Bval%7D%7D%7D%7D%5C%5D\" alt=\"\\[{D_{{\\rm{val}}}}\\]\" eeimg=\"1\"/> 数量的几倍。再看看 OOB 表中， <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5Ctilde+D%7D_t%7D%5C%5D\" alt=\"\\[{{\\tilde D}_t}\\]\" eeimg=\"1\"/> 是用来训练 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bg_t%7D%5C%5D\" alt=\"\\[{g_t}\\]\" eeimg=\"1\"/> ，对应的红色 * 是 OOB 样本，大约有 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cfrac%7B1%7D%7Be%7DN%5C%5D\" alt=\"\\[\\frac{1}{e}N\\]\" eeimg=\"1\"/> ，这样样本没有参与训练 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bg_t%7D%5C%5D\" alt=\"\\[{g_t}\\]\" eeimg=\"1\"/> ，类似于 Validation 表中的 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BD_%7B%7B%5Crm%7Bval%7D%7D%7D%7D%5C%5D\" alt=\"\\[{D_{{\\rm{val}}}}\\]\" eeimg=\"1\"/> ，所以 OOB 样本可以用来计算对应 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bg_t%7D%5C%5D\" alt=\"\\[{g_t}\\]\" eeimg=\"1\"/> 的performance。但是我们不关心单个 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bg_t%7D%5C%5D\" alt=\"\\[{g_t}\\]\" eeimg=\"1\"/> 的 performance ，而更关心 <img src=\"https://www.zhihu.com/equation?tex=G\" alt=\"G\" eeimg=\"1\"/> 的 performance。分为三步：</p><ul><li>对每个样本 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cleft%28+%7B%7B%7B%5Crm%7Bx%7D%7D_n%7D%2C%7By_n%7D%7D+%5Cright%29%5C%5D\" alt=\"\\[\\left( {{{\\rm{x}}_n},{y_n}} \\right)\\]\" eeimg=\"1\"/> ，找出它是哪些 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cleft%5C%7B+%7B%7Bg_t%7D%7D+%5Cright%5C%7D%5C%5D\" alt=\"\\[\\left\\{ {{g_t}} \\right\\}\\]\" eeimg=\"1\"/> 的 OOB 样本，计算它在这些 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cleft%5C%7B+%7B%7Bg_t%7D%7D+%5Cright%5C%7D%5C%5D\" alt=\"\\[\\left\\{ {{g_t}} \\right\\}\\]\" eeimg=\"1\"/> 上的值 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cleft%5C%7B+%7B%7Bg_t%7D%5Cleft%28+%7B%7B%7B%5Crm%7Bx%7D%7D_n%7D%7D+%5Cright%29%7D+%5Cright%5C%7D%5C%5D\" alt=\"\\[\\left\\{ {{g_t}\\left( {{{\\rm{x}}_n}} \\right)} \\right\\}\\]\" eeimg=\"1\"/>；例如样本 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cleft%28+%7B%7B%7B%5Crm%7Bx%7D%7D_N%7D%2C%7By_N%7D%7D+%5Cright%29%5C%5D\" alt=\"\\[\\left( {{{\\rm{x}}_N},{y_N}} \\right)\\]\" eeimg=\"1\"/> 是 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bg_2%7D%2C%7Bg_3%7D%2C%7Bg_T%7D%5C%5D\" alt=\"\\[{g_2},{g_3},{g_T}\\]\" eeimg=\"1\"/> 的 OOB 样本， 对应的输出值分别为：<img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bg_2%7D%5Cleft%28+%7B%7B%7B%5Crm%7Bx%7D%7D_N%7D%7D+%5Cright%29%2C%7Bg_3%7D%5Cleft%28+%7B%7B%7B%5Crm%7Bx%7D%7D_N%7D%7D+%5Cright%29%2C%7Bg_T%7D%5Cleft%28+%7B%7B%7B%5Crm%7Bx%7D%7D_N%7D%7D+%5Cright%29%5C%5D\" alt=\"\\[{g_2}\\left( {{{\\rm{x}}_N}} \\right),{g_3}\\left( {{{\\rm{x}}_N}} \\right),{g_T}\\left( {{{\\rm{x}}_N}} \\right)\\]\" eeimg=\"1\"/> 。</li><li>求 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cleft%5C%7B+%7B%7Bg_t%7D%5Cleft%28+%7B%7B%7B%5Crm%7Bx%7D%7D_n%7D%7D+%5Cright%29%7D+%5Cright%5C%7D%5C%5D\" alt=\"\\[\\left\\{ {{g_t}\\left( {{{\\rm{x}}_n}} \\right)} \\right\\}\\]\" eeimg=\"1\"/> 的平均值作为 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%7B%5Crm%7Bx%7D%7D_n%7D%7D%5C%5D\" alt=\"\\[{{{\\rm{x}}_n}}\\]\" eeimg=\"1\"/> 的输出 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BG_n%5E+-+%5Cleft%28+%7B%7B%7B%5Crm%7Bx%7D%7D_n%7D%7D+%5Cright%29%7D%5C%5D\" alt=\"\\[{G_n^ - \\left( {{{\\rm{x}}_n}} \\right)}\\]\" eeimg=\"1\"/> ； <img src=\"https://www.zhihu.com/equation?tex=%5C%5BG_N%5E+-+%5Cleft%28+%7B%7B%7B%5Crm%7Bx%7D%7D_N%7D%7D+%5Cright%29+%3D+%7B%5Crm%7Baverage%7D%7D%5Cleft%28+%7B%7Bg_2%7D%2C%7Bg_3%7D%2C%7Bg_T%7D%7D+%5Cright%29%5C%5D\" alt=\"\\[G_N^ - \\left( {{{\\rm{x}}_N}} \\right) = {\\rm{average}}\\left( {{g_2},{g_3},{g_T}} \\right)\\]\" eeimg=\"1\"/> </li><li>计算 <img src=\"https://www.zhihu.com/equation?tex=G\" alt=\"G\" eeimg=\"1\"/> 的 performance <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BE_%7Boob%7D%7D%5Cleft%28+G+%5Cright%29+%3D+%5Cfrac%7B1%7D%7BN%7D%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%7B%5Crm%7Berr%7D%7D%5Cleft%28+%7B%7By_n%7D%2CG_n%5E+-+%5Cleft%28+%7B%7B%7B%5Crm%7Bx%7D%7D_n%7D%7D+%5Cright%29%7D+%5Cright%29%7D+%5Ctag%7B3%7D+%5C%5D\" alt=\"\\[{E_{oob}}\\left( G \\right) = \\frac{1}{N}\\sum\\limits_{n = 1}^N {{\\rm{err}}\\left( {{y_n},G_n^ - \\left( {{{\\rm{x}}_n}} \\right)} \\right)} \\tag{3} \\]\" eeimg=\"1\"/></li></ul><p>称为 bagging 或者 Random Forest 的 self-validation。</p><h2><b>三、Feature Selection</b></h2><p>如果样本特征过多，我们希望舍弃一些特征，通常希望舍弃的特征分为两类：一、冗余，即重复或者非常相似的特征，例如“年龄”和“生日”；二、不相关，即特征与预测的结果没有关系，例如“疾病预测”时候引入“保险类型”的特征。如何用随机森林进行特征选择？</p><p><b>1. permutation test</b></p><p>采用 random test 思想：如果某个特征很重要，把该特征用随机的值替换掉，那么模型的表现一定会变差。有两种方法生成随机值：一、根据 uniform 或 gaussian 分布生成随机值，但是有个缺点，改变了该特征的整体分布 <img src=\"https://www.zhihu.com/equation?tex=%5C%5BP%5Cleft%28+%7B%7B%7B%5Crm%7Bx%7D%7D_i%7D%7D+%5Cright%29%5C%5D\" alt=\"\\[P\\left( {{{\\rm{x}}_i}} \\right)\\]\" eeimg=\"1\"/> ；二、permutation方法，将原来的 <img src=\"https://www.zhihu.com/equation?tex=N\" alt=\"N\" eeimg=\"1\"/> 个样本的第 <img src=\"https://www.zhihu.com/equation?tex=i\" alt=\"i\" eeimg=\"1\"/> 个特征值打乱（对某个特征重新洗牌），这样的好处是不改变第 <img src=\"https://www.zhihu.com/equation?tex=i\" alt=\"i\" eeimg=\"1\"/> 个特征的整体分布 <img src=\"https://www.zhihu.com/equation?tex=%5C%5BP%5Cleft%28+%7B%7B%7B%5Crm%7Bx%7D%7D_i%7D%7D+%5Cright%29%5C%5D\" alt=\"\\[P\\left( {{{\\rm{x}}_i}} \\right)\\]\" eeimg=\"1\"/> ，这种做法称为 permutation test，即在计算第 <img src=\"https://www.zhihu.com/equation?tex=i\" alt=\"i\" eeimg=\"1\"/> 个特征的重要性时，将 <img src=\"https://www.zhihu.com/equation?tex=N\" alt=\"N\" eeimg=\"1\"/> 个样本的第 <img src=\"https://www.zhihu.com/equation?tex=i\" alt=\"i\" eeimg=\"1\"/> 个特征重新洗牌，洗牌前样本集为 <img src=\"https://www.zhihu.com/equation?tex=%5C%5BD%5C%5D\" alt=\"\\[D\\]\" eeimg=\"1\"/> ，洗牌后样本集为 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BD%5E%7B%5Cleft%28+p+%5Cright%29%7D%7D%5C%5D\" alt=\"\\[{D^{\\left( p \\right)}}\\]\" eeimg=\"1\"/> ，然后比较 <img src=\"https://www.zhihu.com/equation?tex=%5C%5BD%5C%5D\" alt=\"\\[D\\]\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BD%5E%7B%5Cleft%28+p+%5Cright%29%7D%7D%5C%5D\" alt=\"\\[{D^{\\left( p \\right)}}\\]\" eeimg=\"1\"/> 训练出来的模型的表现，如果差异性比较大，那么第 <img src=\"https://www.zhihu.com/equation?tex=i\" alt=\"i\" eeimg=\"1\"/> 个特征就比较重要，第 <img src=\"https://www.zhihu.com/equation?tex=i\" alt=\"i\" eeimg=\"1\"/> 个特征的重要性为： <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7Bimportance%7D%7D%28i%29+%3D+%7B%5Crm%7Bperformance%7D%7D%28D%29+-+%7B%5Crm%7Bperformance%7D%7D%28%7BD%5E%7B%5Cleft%28+p+%5Cright%29%7D%7D%29+%5Ctag%7B4%7D+%5C%5D\" alt=\"\\[{\\rm{importance}}(i) = {\\rm{performance}}(D) - {\\rm{performance}}({D^{\\left( p \\right)}}) \\tag{4} \\]\" eeimg=\"1\"/> </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-560aac5545c7ad556b91bb6b382e0cb9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"650\" data-rawheight=\"384\" class=\"origin_image zh-lightbox-thumb\" width=\"650\" data-original=\"https://pic2.zhimg.com/v2-560aac5545c7ad556b91bb6b382e0cb9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;650&#39; height=&#39;384&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"650\" data-rawheight=\"384\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"650\" data-original=\"https://pic2.zhimg.com/v2-560aac5545c7ad556b91bb6b382e0cb9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-560aac5545c7ad556b91bb6b382e0cb9_b.jpg\"/></figure><p><b>2. Feature Importance in Original Random Forest</b></p><p>前面介绍随机森林的 performance 可以用 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BE_%7Boob%7D%7D%5Cleft%28+G+%5Cright%29%5C%5D\" alt=\"\\[{E_{oob}}\\left( G \\right)\\]\" eeimg=\"1\"/> 来衡量，计算第 <img src=\"https://www.zhihu.com/equation?tex=i\" alt=\"i\" eeimg=\"1\"/> 个特征的重要性时，对 <img src=\"https://www.zhihu.com/equation?tex=N\" alt=\"N\" eeimg=\"1\"/> 个样本的第<img src=\"https://www.zhihu.com/equation?tex=i\" alt=\"i\" eeimg=\"1\"/>个特征重新洗牌得到样本集 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BD%5E%7B%5Cleft%28+p+%5Cright%29%7D%7D%5C%5D\" alt=\"\\[{D^{\\left( p \\right)}}\\]\" eeimg=\"1\"/>，需要重新训练模型得到 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BE_%7Boob%7D%7D%5Cleft%28+%7B%7BG%5E%7B%5Cleft%28+p+%5Cright%29%7D%7D%7D+%5Cright%29%5C%5D\" alt=\"\\[{E_{oob}}\\left( {{G^{\\left( p \\right)}}} \\right)\\]\" eeimg=\"1\"/>，计算每个特征的重要性时，都需要重新训练一次模型，这个过程非常繁琐耗时。为了简化计算复杂度，我们把 permutation 的操作从 train 阶段移动 OOB validation上。在 train 阶段采用 <img src=\"https://www.zhihu.com/equation?tex=%5C%5BD%5C%5D\" alt=\"\\[D\\]\" eeimg=\"1\"/> 作为训练数据（仅需要 train 一次）；在 OOB validation 阶段，上面介绍计算 OOB validation 分为三个步，在第一步中采用 permutation 替换 OOB 样本的第 <img src=\"https://www.zhihu.com/equation?tex=i\" alt=\"i\" eeimg=\"1\"/> 个特征值。例如： <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cleft%28+%7B%7B%7B%5Crm%7Bx%7D%7D_n%7D%2C%7By_n%7D%7D+%5Cright%29%5C%5D\" alt=\"\\[\\left( {{{\\rm{x}}_n},{y_n}} \\right)\\]\" eeimg=\"1\"/> 是 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7Bg_t%7D%7D%5C%5D\" alt=\"\\[{{g_t}}\\]\" eeimg=\"1\"/> 的 OOB 样本，计算 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7Bg_t%7D%5Cleft%28+%7B%7B%7B%5Crm%7Bx%7D%7D_n%7D%7D+%5Cright%29%7D%5C%5D\" alt=\"\\[{{g_t}\\left( {{{\\rm{x}}_n}} \\right)}\\]\" eeimg=\"1\"/> 时，先对 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7Bg_t%7D%7D%5C%5D\" alt=\"\\[{{g_t}}\\]\" eeimg=\"1\"/> 的 OOB 所有样本的第 <img src=\"https://www.zhihu.com/equation?tex=i\" alt=\"i\" eeimg=\"1\"/> 个特征进行重新洗牌。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-301d2d26f0f1d49f6c8475adc651dc9a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"645\" data-rawheight=\"357\" class=\"origin_image zh-lightbox-thumb\" width=\"645\" data-original=\"https://pic3.zhimg.com/v2-301d2d26f0f1d49f6c8475adc651dc9a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;645&#39; height=&#39;357&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"645\" data-rawheight=\"357\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"645\" data-original=\"https://pic3.zhimg.com/v2-301d2d26f0f1d49f6c8475adc651dc9a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-301d2d26f0f1d49f6c8475adc651dc9a_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "算法", 
                    "tagLink": "https://api.zhihu.com/topics/19553510"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/43379045", 
            "userName": "徽州风韵", 
            "userLink": "https://www.zhihu.com/people/53bd6f8b93873b03c5edb714aea2a456", 
            "upvote": 1, 
            "title": "Aggregation Models-learning", 
            "content": "<h2><b>一、Bagging（learning+uniform）</b></h2><p>前面介绍的 blending 中 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bg_t%7D%5C%5D\" alt=\"\\[{g_t}\\]\" eeimg=\"1\"/> 都是已知的，接下来 learning 中 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bg_t%7D%5C%5D\" alt=\"\\[{g_t}\\]\" eeimg=\"1\"/> 都是未知的，需要一边学习一边把它们组合起来形成 <img src=\"https://www.zhihu.com/equation?tex=G\" alt=\"G\" eeimg=\"1\"/> 。我们知道如果 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bg_t%7D%5C%5D\" alt=\"\\[{g_t}\\]\" eeimg=\"1\"/> 之间的差异性比较大，那么往往组合起来的模型效果相对比较好些。有以下几种方法可以得到多样性的 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bg_t%7D%5C%5D\" alt=\"\\[{g_t}\\]\" eeimg=\"1\"/> ：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-db87daecf8a3f01a50c8661fd29031ad_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"675\" data-rawheight=\"215\" class=\"origin_image zh-lightbox-thumb\" width=\"675\" data-original=\"https://pic2.zhimg.com/v2-db87daecf8a3f01a50c8661fd29031ad_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;675&#39; height=&#39;215&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"675\" data-rawheight=\"215\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"675\" data-original=\"https://pic2.zhimg.com/v2-db87daecf8a3f01a50c8661fd29031ad_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-db87daecf8a3f01a50c8661fd29031ad_b.jpg\"/></figure><p>下面我们介绍数据的随机性，先介绍 bootstrapping 的概念，是一种从给定样本集 <img src=\"https://www.zhihu.com/equation?tex=D\" alt=\"D\" eeimg=\"1\"/> 中有放回的均匀抽样，也就是说，每当选中一个样本，它等可能地被再次选中并被再次添加到训练集 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5Ctilde+D%7D_t%7D%5C%5D\" alt=\"\\[{{\\tilde D}_t}\\]\" eeimg=\"1\"/> 中。Bagging 就是利用 bootstrapping 方法每次产生不同的训练数据 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5Ctilde+D%7D_t%7D%5C%5D\" alt=\"\\[{{\\tilde D}_t}\\]\" eeimg=\"1\"/> ，然后再结合一些 base algorithm 得到 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bg_t%7D%5C%5D\" alt=\"\\[{g_t}\\]\" eeimg=\"1\"/>，那么 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bg_t%7D%5C%5D\" alt=\"\\[{g_t}\\]\" eeimg=\"1\"/> 存在一定的差异性，最后将所有的 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bg_t%7D%5C%5D\" alt=\"\\[{g_t}\\]\" eeimg=\"1\"/> 按照 uniform 的方式结合起来，Bagging 也称为 bootstrap aggregation。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-47a16b2ecdbc40d26d6093300f213494_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"295\" data-rawheight=\"181\" class=\"content_image\" width=\"295\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;295&#39; height=&#39;181&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"295\" data-rawheight=\"181\" class=\"content_image lazy\" width=\"295\" data-actualsrc=\"https://pic1.zhimg.com/v2-47a16b2ecdbc40d26d6093300f213494_b.jpg\"/></figure><h2><b>二、Adaptive Boosting（learning+non-uniform）</b></h2><p><b>1. Motivation </b></p><p>Bagging 中每个 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bg_t%7D%5C%5D\" alt=\"\\[{g_t}\\]\" eeimg=\"1\"/> 的权重大小相同，而我们希望不同的 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bg_t%7D%5C%5D\" alt=\"\\[{g_t}\\]\" eeimg=\"1\"/> 有不同的权重 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Calpha+_t%7D%5C%5D\" alt=\"\\[{\\alpha _t}\\]\" eeimg=\"1\"/> （类似前面介绍的股票预测，希望给擅长的人更多的票数），同时在训练 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bg_t%7D%5C%5D\" alt=\"\\[{g_t}\\]\" eeimg=\"1\"/> 的时候又希望 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bg_t%7D%5C%5D\" alt=\"\\[{g_t}\\]\" eeimg=\"1\"/> 之间存在差异性。我们可以通过调整每个样本的权重得到不同的 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bg_t%7D%5C%5D\" alt=\"\\[{g_t}\\]\" eeimg=\"1\"/> ，不同的 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bg_t%7D%5C%5D\" alt=\"\\[{g_t}\\]\" eeimg=\"1\"/> 乘以不同的系数 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Calpha+_t%7D%5C%5D\" alt=\"\\[{\\alpha _t}\\]\" eeimg=\"1\"/> 进行组合。我们先看一个例子，假设老师教四位小朋友在众多水果中认出苹果。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-2092c19b5d1b9767f16d185b833d23f7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"887\" data-rawheight=\"554\" class=\"origin_image zh-lightbox-thumb\" width=\"887\" data-original=\"https://pic4.zhimg.com/v2-2092c19b5d1b9767f16d185b833d23f7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;887&#39; height=&#39;554&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"887\" data-rawheight=\"554\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"887\" data-original=\"https://pic4.zhimg.com/v2-2092c19b5d1b9767f16d185b833d23f7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-2092c19b5d1b9767f16d185b833d23f7_b.jpg\"/></figure><ul><li>A小朋友说苹果是圆的，这时候老师把分类错误的水果用蓝色标出来，并进行放大；</li><li>B小朋友更关注A小朋友分类错误的水果，B小朋友说苹果是红色的，这时候老师再把分类错误的水果用蓝色标出来；</li><li>C小朋友更关注B小朋友分类错误的水果，C小朋友说苹果是绿色的，这时候老师再把分类错误的水果用蓝色标出来；</li><li>D小朋友更关注C小朋友分类错误的水果，D小朋友说苹果上面有梗。</li></ul><p>经过每位小朋友一步一步不断地推理，最终可以得到相对准确的苹果的定义，如果我们把小朋友对苹果的认识融合起来，即苹果是圆的，红色的，也可能是绿色的，上面有梗。那么很容易把苹果从众多水果中分辨出来。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-f14d6b7d2226f556c9174671da3eb8d4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"648\" data-rawheight=\"353\" class=\"origin_image zh-lightbox-thumb\" width=\"648\" data-original=\"https://pic1.zhimg.com/v2-f14d6b7d2226f556c9174671da3eb8d4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;648&#39; height=&#39;353&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"648\" data-rawheight=\"353\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"648\" data-original=\"https://pic1.zhimg.com/v2-f14d6b7d2226f556c9174671da3eb8d4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-f14d6b7d2226f556c9174671da3eb8d4_b.jpg\"/></figure><p><b>2. Diversity by Re-weighting</b></p><p>在 Bagging 中，我们采用 bootstrap 生成训练数据，假设样本集 <img src=\"https://www.zhihu.com/equation?tex=D\" alt=\"D\" eeimg=\"1\"/> 中有4个样本，在训练 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bg_t%7D%5C%5D\" alt=\"\\[{g_t}\\]\" eeimg=\"1\"/> 时，产生的训练数据为 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5Ctilde+D%7D_t%7D%5C%5D\" alt=\"\\[{{\\tilde D}_t}\\]\" eeimg=\"1\"/> ，如下所示：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-425af44b7d9d3420c452459d9d4e236d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"593\" data-rawheight=\"274\" class=\"origin_image zh-lightbox-thumb\" width=\"593\" data-original=\"https://pic2.zhimg.com/v2-425af44b7d9d3420c452459d9d4e236d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;593&#39; height=&#39;274&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"593\" data-rawheight=\"274\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"593\" data-original=\"https://pic2.zhimg.com/v2-425af44b7d9d3420c452459d9d4e236d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-425af44b7d9d3420c452459d9d4e236d_b.jpg\"/></figure><p>从另一个角度看 Bagging， 其实就是通过 bootstrap 产生权重 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5Crm%7Bu%7D%7D%5E%7B%5Cleft%28+t+%5Cright%29%7D%7D%5C%5D\" alt=\"\\[{{\\rm{u}}^{\\left( t \\right)}}\\]\" eeimg=\"1\"/> 作为样本 error 的权重，然后通过 Base Algorithm 算法使得 <img src=\"https://www.zhihu.com/equation?tex=%5C%5BE_%7B%7B%5Crm%7Bin%7D%7D%7D%5E%7B%5Crm%7Bu%7D%7D%5Cleft%28+h+%5Cright%29%5C%5D\" alt=\"\\[E_{{\\rm{in}}}^{\\rm{u}}\\left( h \\right)\\]\" eeimg=\"1\"/> 最小，得到不同的 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bg_t%7D%5C%5D\" alt=\"\\[{g_t}\\]\" eeimg=\"1\"/> ，这种算法叫做 Weighted Base Algorithm，其目标是最小化 bootstrap-weighted error，如公式 (1)。</p><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5BE_%7B%7B%5Crm%7Bin%7D%7D%7D%5E%7B%5Crm%7Bu%7D%7D%5Cleft%28+h+%5Cright%29+%3D+%5Cfrac%7B1%7D%7BN%7D%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%7B%7B%5Crm%7Bu%7D%7D_n%7D%7B%5Crm%7Berr%7D%7D%5Cleft%28+%7B%7By_n%7D%2Ch%5Cleft%28+%7B%7B%7B%5Crm%7Bx%7D%7D_n%7D%7D+%5Cright%29%7D+%5Cright%29%7D+%5Ctag%7B1%7D++%5C%5D\" alt=\"\\[E_{{\\rm{in}}}^{\\rm{u}}\\left( h \\right) = \\frac{1}{N}\\sum\\limits_{n = 1}^N {{{\\rm{u}}_n}{\\rm{err}}\\left( {{y_n},h\\left( {{{\\rm{x}}_n}} \\right)} \\right)} \\tag{1}  \\]\" eeimg=\"1\"/> </p><p>我们如何将 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5Crm%7Bu%7D%7D%5E%7B%5Cleft%28+t+%5Cright%29%7D%7D%5C%5D\" alt=\"\\[{{\\rm{u}}^{\\left( t \\right)}}\\]\" eeimg=\"1\"/> 融入到 Base Algorithm 中？我们举两个例子：</p><p>1）<a href=\"https://zhuanlan.zhihu.com/p/43169033\" class=\"internal\">SVM-Soft Margin</a></p><p>在 Soft-Margin SVM 中引入犯错惩罚项 <img src=\"https://www.zhihu.com/equation?tex=%5C%5BC+%5Ccdot+%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%7B%5Cxi+_n%7D%7D+%5C%5D\" alt=\"\\[C \\cdot \\sum\\limits_{n = 1}^N {{\\xi _n}} \\]\" eeimg=\"1\"/> ，如果每个样本的 error 有相应的权重 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5Crm%7Bu%7D%7D%5E%7B%5Cleft%28+t+%5Cright%29%7D%7D%5C%5D\" alt=\"\\[{{\\rm{u}}^{\\left( t \\right)}}\\]\" eeimg=\"1\"/> ，那么犯错惩罚项变为 <img src=\"https://www.zhihu.com/equation?tex=%5C%5BC+%5Ccdot+%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7Bu_n%5E%7B%5Cleft%28+t+%5Cright%29%7D%7B%5Cxi+_n%7D%7D+%5C%5D\" alt=\"\\[C \\cdot \\sum\\limits_{n = 1}^N {u_n^{\\left( t \\right)}{\\xi _n}} \\]\" eeimg=\"1\"/> ，通过对偶问题的转化， <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bu_n%5E%7B%5Cleft%28+t+%5Cright%29%7D%7D%5C%5D\" alt=\"\\[{u_n^{\\left( t \\right)}}\\]\" eeimg=\"1\"/> 可变为 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Ba_n%7D%5C%5D\" alt=\"\\[{a_n}\\]\" eeimg=\"1\"/> 的上界约束项 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B0+%5Cle+%7Ba_n%7D+%5Cle+Cu_n%5E%7B%5Cleft%28+t+%5Cright%29%7D%5C%5D\" alt=\"\\[0 \\le {a_n} \\le Cu_n^{\\left( t \\right)}\\]\" eeimg=\"1\"/> ，大家可以参考 Soft-Margin SVM 对偶问题进行推导。</p><p>2）logistic regression</p><p>正比于 <img src=\"https://www.zhihu.com/equation?tex=%5C%5Bu_n%5E%7B%5Cleft%28+t+%5Cright%29%7D%5C%5D\" alt=\"\\[u_n^{\\left( t \\right)}\\]\" eeimg=\"1\"/> 的概率对样本进行采样。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-696f966c5604664304a87eb0a6aa10e7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"554\" data-rawheight=\"97\" class=\"origin_image zh-lightbox-thumb\" width=\"554\" data-original=\"https://pic4.zhimg.com/v2-696f966c5604664304a87eb0a6aa10e7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;554&#39; height=&#39;97&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"554\" data-rawheight=\"97\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"554\" data-original=\"https://pic4.zhimg.com/v2-696f966c5604664304a87eb0a6aa10e7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-696f966c5604664304a87eb0a6aa10e7_b.jpg\"/></figure><p>假设我们现在可以把 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5Crm%7Bu%7D%7D%5E%7B%5Cleft%28+t+%5Cright%29%7D%7D%5C%5D\" alt=\"\\[{{\\rm{u}}^{\\left( t \\right)}}\\]\" eeimg=\"1\"/> 融入到 Base Algorithm 中，不同的 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5Crm%7Bu%7D%7D%5E%7B%5Cleft%28+t+%5Cright%29%7D%7D%5C%5D\" alt=\"\\[{{\\rm{u}}^{\\left( t \\right)}}\\]\" eeimg=\"1\"/> 得到不同的 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bg_t%7D%5C%5D\" alt=\"\\[{g_t}\\]\" eeimg=\"1\"/> ，那么我们怎样确定 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5Crm%7Bu%7D%7D%5E%7B%5Cleft%28+t+%5Cright%29%7D%7D%5C%5D\" alt=\"\\[{{\\rm{u}}^{\\left( t \\right)}}\\]\" eeimg=\"1\"/> 使得 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bg_t%7D%5C%5D\" alt=\"\\[{g_t}\\]\" eeimg=\"1\"/> 间存在差异性呢？</p><p>先看下 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bg_t%7D%5C%5D\" alt=\"\\[{g_t}\\]\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bg_%7Bt+%2B+1%7D%7D%5C%5D\" alt=\"\\[{g_{t + 1}}\\]\" eeimg=\"1\"/> 是如何得到的？</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-d11da9e1bc07b98e01c1ce8c736f83a4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"552\" data-rawheight=\"190\" class=\"origin_image zh-lightbox-thumb\" width=\"552\" data-original=\"https://pic1.zhimg.com/v2-d11da9e1bc07b98e01c1ce8c736f83a4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;552&#39; height=&#39;190&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"552\" data-rawheight=\"190\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"552\" data-original=\"https://pic1.zhimg.com/v2-d11da9e1bc07b98e01c1ce8c736f83a4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-d11da9e1bc07b98e01c1ce8c736f83a4_b.jpg\"/></figure><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bg_t%7D%5C%5D\" alt=\"\\[{g_t}\\]\" eeimg=\"1\"/> 是由 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5Crm%7Bu%7D%7D%5E%7B%5Cleft%28+t+%5Cright%29%7D%7D%5C%5D\" alt=\"\\[{{\\rm{u}}^{\\left( t \\right)}}\\]\" eeimg=\"1\"/> 得到， <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bg_%7Bt+%2B+1%7D%7D%5C%5D\" alt=\"\\[{g_{t + 1}}\\]\" eeimg=\"1\"/> 是由 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5Crm%7Bu%7D%7D%5E%7B%5Cleft%28+%7Bt+%2B+1%7D+%5Cright%29%7D%7D%5C%5D\" alt=\"\\[{{\\rm{u}}^{\\left( {t + 1} \\right)}}\\]\" eeimg=\"1\"/> 得到。如果用 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bg_t%7D%5C%5D\" alt=\"\\[{g_t}\\]\" eeimg=\"1\"/> 预测 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5Crm%7Bu%7D%7D%5E%7B%5Cleft%28+%7Bt+%2B+1%7D+%5Cright%29%7D%7D%5C%5D\" alt=\"\\[{{\\rm{u}}^{\\left( {t + 1} \\right)}}\\]\" eeimg=\"1\"/> 的样本得到的 error 很大，那么由 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5Crm%7Bu%7D%7D%5E%7B%5Cleft%28+%7Bt+%2B+1%7D+%5Cright%29%7D%7D%5C%5D\" alt=\"\\[{{\\rm{u}}^{\\left( {t + 1} \\right)}}\\]\" eeimg=\"1\"/> 得到的 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bg_%7Bt+%2B+1%7D%7D%5C%5D\" alt=\"\\[{g_{t + 1}}\\]\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bg_t%7D%5C%5D\" alt=\"\\[{g_t}\\]\" eeimg=\"1\"/> 会有很大的差别。我们希望 error 等于 <img src=\"https://www.zhihu.com/equation?tex=0.5\" alt=\"0.5\" eeimg=\"1\"/> （随机猜测，不是 error 越大越好吗，为什么不是等于 <img src=\"https://www.zhihu.com/equation?tex=1\" alt=\"1\" eeimg=\"1\"/> ？对于分类问题，error 等于 <img src=\"https://www.zhihu.com/equation?tex=1\" alt=\"1\" eeimg=\"1\"/> 表示全部分类错误，我们只要对模型的结果取反，即：如果模型预测是 <img src=\"https://www.zhihu.com/equation?tex=-1\" alt=\"-1\" eeimg=\"1\"/> ，我们将结果变为 <img src=\"https://www.zhihu.com/equation?tex=1\" alt=\"1\" eeimg=\"1\"/> ，模型预测是 <img src=\"https://www.zhihu.com/equation?tex=1\" alt=\"1\" eeimg=\"1\"/> ，将结果变为 <img src=\"https://www.zhihu.com/equation?tex=-1\" alt=\"-1\" eeimg=\"1\"/> ，这是一个类似准确度为 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B100%5C%25+%5C%5D\" alt=\"\\[100\\% \\]\" eeimg=\"1\"/> 的分类器），最大限度保证 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bg_%7Bt+%2B+1%7D%7D%5C%5D\" alt=\"\\[{g_{t + 1}}\\]\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bg_t%7D%5C%5D\" alt=\"\\[{g_t}\\]\" eeimg=\"1\"/> 有较大的差异性。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-e116f01ffcb2bfe27c753ea31809d4e7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"570\" data-rawheight=\"365\" class=\"origin_image zh-lightbox-thumb\" width=\"570\" data-original=\"https://pic4.zhimg.com/v2-e116f01ffcb2bfe27c753ea31809d4e7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;570&#39; height=&#39;365&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"570\" data-rawheight=\"365\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"570\" data-original=\"https://pic4.zhimg.com/v2-e116f01ffcb2bfe27c753ea31809d4e7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-e116f01ffcb2bfe27c753ea31809d4e7_b.jpg\"/></figure><p>要让犯错的 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Csquare+_%7Bt+%2B+1%7D%7D%5C%5D\" alt=\"\\[{\\square _{t + 1}}\\]\" eeimg=\"1\"/> 和没有犯错的 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Cbigcirc+_%7Bt+%2B+1%7D%7D%5C%5D\" alt=\"\\[{\\bigcirc _{t + 1}}\\]\" eeimg=\"1\"/> 相等，一种简单的做法是，分别对犯错的 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bu_n%5E%7B%5Cleft%28+t+%5Cright%29%7D%7D%5C%5D\" alt=\"\\[{u_n^{\\left( t \\right)}}\\]\" eeimg=\"1\"/> 和未犯错的 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bu_n%5E%7B%5Cleft%28+t+%5Cright%29%7D%7D%5C%5D\" alt=\"\\[{u_n^{\\left( t \\right)}}\\]\" eeimg=\"1\"/> 进行缩放。错误率：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Cvarepsilon+_t%7D+%3D+%5Cfrac%7B%7B%5Csum%5Cnolimits_%7Bn+%3D+1%7D%5EN+%7Bu_n%5E%7B%5Cleft%28+t+%5Cright%29%7D%5Cleft%5B%5Ckern-0.15em%5Cleft%5B+%7B%7By_n%7D+%5Cne+%7Bg_t%7D%5Cleft%28+%7B%7B%7B%5Ctext%7Bx%7D%7D_n%7D%7D+%5Cright%29%7D+++%5Cright%5D%5Ckern-0.15em%5Cright%5D%7D+%7D%7D%7B%7B%5Csum%5Cnolimits_%7Bn+%3D+1%7D%5EN+%7Bu_n%5E%7B%5Cleft%28+t+%5Cright%29%7D%7D+%7D%7D+%5Ctag%7B2%7D+%5C%5D\" alt=\"\\[{\\varepsilon _t} = \\frac{{\\sum\\nolimits_{n = 1}^N {u_n^{\\left( t \\right)}\\left[\\kern-0.15em\\left[ {{y_n} \\ne {g_t}\\left( {{{\\text{x}}_n}} \\right)}   \\right]\\kern-0.15em\\right]} }}{{\\sum\\nolimits_{n = 1}^N {u_n^{\\left( t \\right)}} }} \\tag{2} \\]\" eeimg=\"1\"/> </p><p><b>3.  Adaptive Boosting Algorithm</b></p><p>定义尺度因子  </p><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cbegin%7Bgathered%7D+++%7B%5Cdiamondsuit+_t%7D+%3D+%5Csqrt+%7B%5Cfrac%7B%7B1+-+%7B%5Cvarepsilon+_t%7D%7D%7D%7B%7B%7B%5Cvarepsilon+_t%7D%7D%7D%7D++%5Chfill+%5C%5C+++%7B%5Ctext%7Bincorrect%7D%7D+%5Cleftarrow+%7B%5Ctext%7Bincorrect%7D%7D+%5Ccdot+%7B%5Cdiamondsuit+_t%7D+%5Chfill+%5C%5C+++%7B%5Ctext%7Bcorrect%7D%7D+%5Cleftarrow+%7B%5Ctext%7Bcorrect%7D%7D%2F%7B%5Cdiamondsuit+_t%7D+%5Chfill+%5C%5C++%5Cend%7Bgathered%7D++%5Ctag%7B3%7D+%5C%5D\" alt=\"\\[\\begin{gathered}   {\\diamondsuit _t} = \\sqrt {\\frac{{1 - {\\varepsilon _t}}}{{{\\varepsilon _t}}}}  \\hfill \\\\   {\\text{incorrect}} \\leftarrow {\\text{incorrect}} \\cdot {\\diamondsuit _t} \\hfill \\\\   {\\text{correct}} \\leftarrow {\\text{correct}}/{\\diamondsuit _t} \\hfill \\\\  \\end{gathered}  \\tag{3} \\]\" eeimg=\"1\"/> </p><p>如果 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Cvarepsilon+_t%7D+%5Cleqslant+%5Cfrac%7B1%7D%7B2%7D+%5CRightarrow+%7B%5Cdiamondsuit+_t%7D+%5Cgeqslant+1%5C%5D\" alt=\"\\[{\\varepsilon _t} \\leqslant \\frac{1}{2} \\Rightarrow {\\diamondsuit _t} \\geqslant 1\\]\" eeimg=\"1\"/> ，增大分类错误样本的权重，降低分类正确样本的权重，类似前面老师学生的例子中，老师每次都会重点突出上一个学生分类错误的水果，让下一个学生更加专注分类错误的水果。得到 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7Bg_t%7D%7D%5C%5D\" alt=\"\\[{{g_t}}\\]\" eeimg=\"1\"/> 后，怎么确定每个 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7Bg_t%7D%7D%5C%5D\" alt=\"\\[{{g_t}}\\]\" eeimg=\"1\"/> 的权重呢？我们希望错误率低的 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7Bg_t%7D%7D%5C%5D\" alt=\"\\[{{g_t}}\\]\" eeimg=\"1\"/> 有相对较大的权重。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-24e889aa830b37f7aba8d51430169a41_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"518\" data-rawheight=\"178\" class=\"origin_image zh-lightbox-thumb\" width=\"518\" data-original=\"https://pic2.zhimg.com/v2-24e889aa830b37f7aba8d51430169a41_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;518&#39; height=&#39;178&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"518\" data-rawheight=\"178\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"518\" data-original=\"https://pic2.zhimg.com/v2-24e889aa830b37f7aba8d51430169a41_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-24e889aa830b37f7aba8d51430169a41_b.jpg\"/></figure><p>我们总结下 Adaptive Boosting 算法， <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Cvarepsilon+_t%7D+%5Cleqslant+%5Cfrac%7B1%7D%7B2%7D%5C%5D\" alt=\"\\[{\\varepsilon _t} \\leqslant \\frac{1}{2}\\]\" eeimg=\"1\"/> 表明只要选取的 Base Algorithm 比乱猜的好一点，经过 Adaptive Boosting 算法多次迭代后，就可以得到相对比较好的模型 <img src=\"https://www.zhihu.com/equation?tex=G\" alt=\"G\" eeimg=\"1\"/> ，起到了 Boosting 提升的效果。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-80fa5e2f8983392853d75aebaf621edf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"554\" data-rawheight=\"291\" class=\"origin_image zh-lightbox-thumb\" width=\"554\" data-original=\"https://pic4.zhimg.com/v2-80fa5e2f8983392853d75aebaf621edf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;554&#39; height=&#39;291&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"554\" data-rawheight=\"291\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"554\" data-original=\"https://pic4.zhimg.com/v2-80fa5e2f8983392853d75aebaf621edf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-80fa5e2f8983392853d75aebaf621edf_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/43281027", 
            "userName": "徽州风韵", 
            "userLink": "https://www.zhihu.com/people/53bd6f8b93873b03c5edb714aea2a456", 
            "upvote": 0, 
            "title": "Aggregation Models-blending", 
            "content": "<h2><b>一、引言</b></h2><p>先通过例子说明为什么需要 Aggregation？假设你有 <img src=\"https://www.zhihu.com/equation?tex=T\" alt=\"T\" eeimg=\"1\"/> 个好朋友，每个人都向你推荐明天股票的走势，对应的建议分别是 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bg_1%7D%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2C%7Bg_T%7D%5C%5D\" alt=\"\\[{g_1}, \\cdot \\cdot \\cdot ,{g_T}\\]\" eeimg=\"1\"/> ，那么你该怎样进行选择呢？可能有以下几种方式：</p><ul><li>选择预测股票走势最准的那个朋友，直接听他的建议就好了。—<b>validation</b></li></ul><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+G%5Cleft%28+%7B%5Crm%7Bx%7D%7D+%5Cright%29+%3D+%7Bg_%7B%7Bt_%2A%7D%7D%7D%5C+%5Cleft%28+%7B%5Crm%7Bx%7D%7D+%5Cright%29%7B%5Crm%7Bwith%7D%7D+%5C+%7Bt_%2A%7D+%3D+%5Cmathop+%7B%5Crm%7Bargmin%7D+%7D%5Climits_%7Bt+%5Cin+%5Cleft%5C%7B+%7B1%2C2%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2CT%7D+%5Cright%5C%7D%7D+%7BE_%7B%7B%5Crm%7Bval%7D%7D%7D%7D%5Cleft%28+%7Bg_t%5E+-+%7D+%5Cright%29+%5Ctag%7B1%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} G\\left( {\\rm{x}} \\right) = {g_{{t_*}}}\\ \\left( {\\rm{x}} \\right){\\rm{with}} \\ {t_*} = \\mathop {\\rm{argmin} }\\limits_{t \\in \\left\\{ {1,2, \\cdot \\cdot \\cdot ,T} \\right\\}} {E_{{\\rm{val}}}}\\left( {g_t^ - } \\right) \\tag{1} \\end{align*}\" eeimg=\"1\"/> </p><ul><li>对所有朋友的建议进行唱票，一人一票，选择票数多的那个结果。—<b>uniformly</b></li></ul><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+G%5Cleft%28+%7B%5Crm%7Bx%7D%7D+%5Cright%29+%3D+%7B%5Crm%7Bsign%7D%7D%5Cleft%28+%7B%5Csum%5Climits_%7Bt+%3D+1%7D%5ET+%7B1+%5Ccdot+%7Bg_t%7D%5Cleft%28+%7B%5Crm%7Bx%7D%7D+%5Cright%29%7D+%7D+%5Cright%29+%5Ctag%7B2%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} G\\left( {\\rm{x}} \\right) = {\\rm{sign}}\\left( {\\sum\\limits_{t = 1}^T {1 \\cdot {g_t}\\left( {\\rm{x}} \\right)} } \\right) \\tag{2} \\end{align*}\" eeimg=\"1\"/> </p><ul><li>可能你的朋友预测股票的水平高低不一，希望给水平高的朋友多一点投票权重，水平低的朋友少点投票权重—<b>non-uniformly</b></li></ul><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+G%5Cleft%28+%7B%5Crm%7Bx%7D%7D+%5Cright%29+%3D+%7B%5Crm%7Bsign%7D%7D%5Cleft%28+%7B%5Csum%5Climits_%7Bt+%3D+1%7D%5ET+%7B%7B%5Calpha+_t%7D+%5Ccdot+%7Bg_t%7D%5Cleft%28+%7B%5Crm%7Bx%7D%7D+%5Cright%29%7D+%7D+%5Cright%29%5C+%7B%5Crm%7Bwith%7D%7D+%5C+%7B%5Calpha+_t%7D+%5Cge+0+%5Ctag%7B3%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} G\\left( {\\rm{x}} \\right) = {\\rm{sign}}\\left( {\\sum\\limits_{t = 1}^T {{\\alpha _t} \\cdot {g_t}\\left( {\\rm{x}} \\right)} } \\right)\\ {\\rm{with}} \\ {\\alpha _t} \\ge 0 \\tag{3} \\end{align*}\" eeimg=\"1\"/> </p><p>     1） <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Calpha+_t%7D+%3D+%5Cleft%5B%5Ckern-0.15em%5Cleft%5B+%7B%7BE_%7B%7B%5Crm%7Bval%7D%7D%7D%7D%5Cleft%28+%7Bg_t%5E+-+%7D+%5Cright%29%7B%5Crm%7Bsmallest%7D%7D%7D+%5Cright%5D%5Ckern-0.15em%5Cright%5D%5C%5D\" alt=\"\\[{\\alpha _t} = \\left[\\kern-0.15em\\left[ {{E_{{\\rm{val}}}}\\left( {g_t^ - } \\right){\\rm{smallest}}} \\right]\\kern-0.15em\\right]\\]\" eeimg=\"1\"/> ：退缩为 <b>validation</b>；</p><p>     2） <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Calpha+_t%7D+%3D+1%5C%5D\" alt=\"\\[{\\alpha _t} = 1\\]\" eeimg=\"1\"/> ：退缩为 <b>uniformly vote</b></p><ul><li>可能你的朋友有的擅长预测科技股，不擅长预测传统股票，而有的朋友擅长预测传统股票，不擅长预测科技股，所以这种情况下，针对不同类型的股票，需要给每个人不同的权重。—<b>conditionally</b></li></ul><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+G%5Cleft%28+%7B%5Crm%7Bx%7D%7D+%5Cright%29+%3D+%7B%5Crm%7Bsign%7D%7D%5Cleft%28+%7B%5Csum%5Climits_%7Bt+%3D+1%7D%5ET+%7B%7Bq_t%7D%5Cleft%28+%7B%5Crm%7Bx%7D%7D+%5Cright%29+%5Ccdot+%7Bg_t%7D%5Cleft%28+%7B%5Crm%7Bx%7D%7D+%5Cright%29%7D+%7D+%5Cright%29%5C+%7B%5Crm%7Bwith%7D%7D%5C+%7Bq_t%7D%5Cleft%28+%7B%5Crm%7Bx%7D%7D+%5Cright%29+%5Cge+0+%5Ctag%7B4%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} G\\left( {\\rm{x}} \\right) = {\\rm{sign}}\\left( {\\sum\\limits_{t = 1}^T {{q_t}\\left( {\\rm{x}} \\right) \\cdot {g_t}\\left( {\\rm{x}} \\right)} } \\right)\\ {\\rm{with}}\\ {q_t}\\left( {\\rm{x}} \\right) \\ge 0 \\tag{4} \\end{align*}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bq_t%7D%5Cleft%28+%7B%5Crm%7Bx%7D%7D+%5Cright%29%7B%5Crm%7B+%3D+%7D%7D%7B%5Calpha+_t%7D%5C%5D\" alt=\"\\[{q_t}\\left( {\\rm{x}} \\right){\\rm{ = }}{\\alpha _t}\\]\" eeimg=\"1\"/> ：退缩为<b>non-uniformly</b></p><p>所以 Aggregation 的思想是：把多个 hypotheses 组合起来，得到更好的预测结果。</p><p>为什么 Aggregation 能够有很好的效果？我们通过两个简单的例子说明下</p><p><b>1. feature transform</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-dba5473f462450fc1307cf07594641a7_b.jpg\" data-size=\"normal\" data-rawwidth=\"523\" data-rawheight=\"198\" class=\"origin_image zh-lightbox-thumb\" width=\"523\" data-original=\"https://pic4.zhimg.com/v2-dba5473f462450fc1307cf07594641a7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;523&#39; height=&#39;198&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"523\" data-rawheight=\"198\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"523\" data-original=\"https://pic4.zhimg.com/v2-dba5473f462450fc1307cf07594641a7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-dba5473f462450fc1307cf07594641a7_b.jpg\"/><figcaption>图 1</figcaption></figure><p>假设有两类 hypotheses 分别是垂直分类面和水平分类面，那么无论哪一类分类面都无法达到最佳的分类效果。如果采用 Aggregation 将两类分类面组合起来，得到的模型比任何一类分类面效果都要好。如图 1 所示，两个垂直分类面和一个水平分类面通过 uniformly 的方式组合起来。</p><p><b>2. regularization</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-d5cfe68f81d653ae89ad47e3efd52f5a_b.jpg\" data-size=\"normal\" data-rawwidth=\"520\" data-rawheight=\"206\" class=\"origin_image zh-lightbox-thumb\" width=\"520\" data-original=\"https://pic3.zhimg.com/v2-d5cfe68f81d653ae89ad47e3efd52f5a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;520&#39; height=&#39;206&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"520\" data-rawheight=\"206\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"520\" data-original=\"https://pic3.zhimg.com/v2-d5cfe68f81d653ae89ad47e3efd52f5a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-d5cfe68f81d653ae89ad47e3efd52f5a_b.jpg\"/><figcaption>图 2</figcaption></figure><p>图 2 中可以得到很多个分类面（灰色线表示），都可以将样本点正确地分开。我们希望得到中间黑色的分类面，有更强的泛化能力，类似前面介绍的 SVM 。我们可以采用 Aggregation 将灰色线表示的分类面组合起来，得到黑色分类面。（在平均化过程，可以把某个过拟合的分类面进行弱化，从而起到 regularization 效果）</p><p>Aggregation 可以分为以下几类</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-4c0dbe69b03552924e0f083403f885b4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"968\" data-rawheight=\"212\" class=\"origin_image zh-lightbox-thumb\" width=\"968\" data-original=\"https://pic1.zhimg.com/v2-4c0dbe69b03552924e0f083403f885b4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;968&#39; height=&#39;212&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"968\" data-rawheight=\"212\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"968\" data-original=\"https://pic1.zhimg.com/v2-4c0dbe69b03552924e0f083403f885b4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-4c0dbe69b03552924e0f083403f885b4_b.jpg\"/></figure><ul><li><b>blending</b>：提前已知 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7Bg_t%7D%7D%5C%5D\" alt=\"\\[{{g_t}}\\]\" eeimg=\"1\"/> ，然后做 Aggregation；</li><li><b>learning</b>：提前不知道 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7Bg_t%7D%7D%5C%5D\" alt=\"\\[{{g_t}}\\]\" eeimg=\"1\"/> ，在 Aggregation 中学习 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7Bg_t%7D%7D%5C%5D\" alt=\"\\[{{g_t}}\\]\" eeimg=\"1\"/> 。</li></ul><p>下面分别介绍这六种方式的 Aggregation 。</p><h2><b>二、voting/averaging（blending+uniform ）</b></h2><p><b>1. Classification</b></p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+G%5Cleft%28+%7B%5Crm%7Bx%7D%7D+%5Cright%29+%3D+%7B%5Crm%7Bsign%7D%7D%5Cleft%28+%7B%5Csum%5Climits_%7Bt+%3D+1%7D%5ET+%7B1+%5Ccdot+%7Bg_t%7D%5Cleft%28+%7B%5Crm%7Bx%7D%7D+%5Cright%29%7D+%7D+%5Cright%29+%5Ctag%7B5%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} G\\left( {\\rm{x}} \\right) = {\\rm{sign}}\\left( {\\sum\\limits_{t = 1}^T {1 \\cdot {g_t}\\left( {\\rm{x}} \\right)} } \\right) \\tag{5} \\end{align*}\" eeimg=\"1\"/> </p><p>三种情况：</p><ul><li>每个 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7Bg_t%7D%7D%5C%5D\" alt=\"\\[{{g_t}}\\]\" eeimg=\"1\"/> 完全相同，那么得到的 <img src=\"https://www.zhihu.com/equation?tex=%5C%5BG%5Cleft%28+%7B%5Crm%7Bx%7D%7D+%5Cright%29%5C%5D\" alt=\"\\[G\\left( {\\rm{x}} \\right)\\]\" eeimg=\"1\"/> 和单个 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7Bg_t%7D%5Cleft%28+%7B%5Crm%7Bx%7D%7D+%5Cright%29%7D%5C%5D\" alt=\"\\[{{g_t}\\left( {\\rm{x}} \\right)}\\]\" eeimg=\"1\"/> 也完全相同；</li><li>每个 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7Bg_t%7D%7D%5C%5D\" alt=\"\\[{{g_t}}\\]\" eeimg=\"1\"/> 有差别，通过一人一票投票方式，可以得到更好的模型，如图 1；</li><li>多分类问题，选择票数最多的那一类。</li></ul><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5BG%5Cleft%28+%7B%5Crm%7Bx%7D%7D+%5Cright%29+%3D+%5Cmathop+%7B%5Carg+%5Cmax+%7D%5Climits_%7B1+%5Cle+k+%5Cle+K%7D+%5Csum%5Climits_%7Bt+%3D+1%7D%5ET+%7B%5Cleft%5B%5Ckern-0.15em%5Cleft%5B+%7B%7Bg_t%7D%5Cleft%28+%7B%5Crm%7Bx%7D%7D+%5Cright%29+%3D+k%7D+%5Cright%5D%5Ckern-0.15em%5Cright%5D%7D+%5C%5D\" alt=\"\\[G\\left( {\\rm{x}} \\right) = \\mathop {\\arg \\max }\\limits_{1 \\le k \\le K} \\sum\\limits_{t = 1}^T {\\left[\\kern-0.15em\\left[ {{g_t}\\left( {\\rm{x}} \\right) = k} \\right]\\kern-0.15em\\right]} \\]\" eeimg=\"1\"/> </p><p><b>2. Regression</b></p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+G%5Cleft%28+%7B%5Crm%7Bx%7D%7D+%5Cright%29+%3D+%5Cfrac%7B1%7D%7BT%7D%7B%5Csum%5Climits_%7Bt+%3D+1%7D%5ET+%7B%7Bg_t%7D%5Cleft%28+%7B%5Crm%7Bx%7D%7D+%5Cright%29%7D+%7D+%5Ctag%7B6%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} G\\left( {\\rm{x}} \\right) = \\frac{1}{T}{\\sum\\limits_{t = 1}^T {{g_t}\\left( {\\rm{x}} \\right)} } \\tag{6} \\end{align*}\" eeimg=\"1\"/> </p><p>两种情况：</p><ul><li>每个 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7Bg_t%7D%7D%5C%5D\" alt=\"\\[{{g_t}}\\]\" eeimg=\"1\"/> 完全相同，那么得到的 <img src=\"https://www.zhihu.com/equation?tex=%5C%5BG%5Cleft%28+%7B%5Crm%7Bx%7D%7D+%5Cright%29%5C%5D\" alt=\"\\[G\\left( {\\rm{x}} \\right)\\]\" eeimg=\"1\"/> 和单个 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7Bg_t%7D%5Cleft%28+%7B%5Crm%7Bx%7D%7D+%5Cright%29%7D%5C%5D\" alt=\"\\[{{g_t}\\left( {\\rm{x}} \\right)}\\]\" eeimg=\"1\"/> 也完全相同；</li><li>每个 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7Bg_t%7D%7D%5C%5D\" alt=\"\\[{{g_t}}\\]\" eeimg=\"1\"/> 有差别，有的 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bg_t%7D%5Cleft%28+%7B%5Crm%7Bx%7D%7D+%5Cright%29+%3E+f%5Cleft%28+%7B%5Crm%7Bx%7D%7D+%5Cright%29%5C%5D\" alt=\"\\[{g_t}\\left( {\\rm{x}} \\right) &gt; f\\left( {\\rm{x}} \\right)\\]\" eeimg=\"1\"/> ，有的 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bg_t%7D%5Cleft%28+%7B%5Crm%7Bx%7D%7D+%5Cright%29+%3C+f%5Cleft%28+%7B%5Crm%7Bx%7D%7D+%5Cright%29%5C%5D\" alt=\"\\[{g_t}\\left( {\\rm{x}} \\right) &lt; f\\left( {\\rm{x}} \\right)\\]\" eeimg=\"1\"/> （这里 <img src=\"https://www.zhihu.com/equation?tex=%5C%5Bf%5Cleft%28+%7B%5Crm%7Bx%7D%7D+%5Cright%29%5C%5D\" alt=\"\\[f\\left( {\\rm{x}} \\right)\\]\" eeimg=\"1\"/> 是样本 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7Bx%7D%7D%5C%5D\" alt=\"\\[{\\rm{x}}\\]\" eeimg=\"1\"/> 对应的真实值），求平均过程，会消除大于和小于的误差，从而得到更好的模型。</li></ul><p>我们从理论上分析下 <img src=\"https://www.zhihu.com/equation?tex=%5C%5BG%5Cleft%28+%7B%5Crm%7Bx%7D%7D+%5Cright%29%5C%5D\" alt=\"\\[G\\left( {\\rm{x}} \\right)\\]\" eeimg=\"1\"/> 比单个 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7Bg_t%7D%5Cleft%28+%7B%5Crm%7Bx%7D%7D+%5Cright%29%7D%5C%5D\" alt=\"\\[{{g_t}\\left( {\\rm{x}} \\right)}\\]\" eeimg=\"1\"/> 效果要好，对于给定的样本点 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7Bx%7D%7D%5C%5D\" alt=\"\\[{\\rm{x}}\\]\" eeimg=\"1\"/> ，我们有：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%7B%5Crm%7Bavg%7D%7D%5Cleft%28+%7B%5Cleft%28+%7B%7Bg_t%7D+-+f%7D+%5Cright%29%5E%7B2%7D%7D+%5Cright%29+%26%3D+%7B%5Crm%7Bavg%7D%7D%5Cleft%28+%7B%7Bg_t%7D%5E2+-+2%7Bg_t%7Df+%2B+%7Bf%5E2%7D%7D+%5Cright%29%5C%5C+%26%3D+%7B%5Crm%7Bavg%7D%7D%5Cleft%28+%7B%7Bg_t%7D%5E2%7D+%5Cright%29+-+2Gf+%2B+%7Bf%5E2%7D%5C%5C+%26%3D+%7B%5Crm%7Bavg%7D%7D%5Cleft%28+%7B%7Bg_t%7D%5E2%7D+%5Cright%29+-+%7BG%5E2%7D+%2B+%7B%5Cleft%28+%7BG+-+f%7D+%5Cright%29%5E2%7D%5C%5C+%26%3D+%7B%5Crm%7Bavg%7D%7D%5Cleft%28+%7B%7Bg_t%7D%5E2+-+2%7Bg_t%7DG+%2B+%7BG%5E2%7D%7D+%5Cright%29+%2B+%7B%5Cleft%28+%7BG+-+f%7D+%5Cright%29%5E2%7D%5C%5C+%26%3D+%7B%5Crm%7Bavg%7D%7D%5Cleft%28+%7B%7B%7B%5Cleft%28+%7B%7Bg_t%7D+-+G%7D+%5Cright%29%7D%5E2%7D%7D+%5Cright%29+%2B+%7B%5Cleft%28+%7BG+-+f%7D+%5Cright%29%5E2%7D%5C%5C+%5Ctag%7B7%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} {\\rm{avg}}\\left( {\\left( {{g_t} - f} \\right)^{2}} \\right) &amp;= {\\rm{avg}}\\left( {{g_t}^2 - 2{g_t}f + {f^2}} \\right)\\\\ &amp;= {\\rm{avg}}\\left( {{g_t}^2} \\right) - 2Gf + {f^2}\\\\ &amp;= {\\rm{avg}}\\left( {{g_t}^2} \\right) - {G^2} + {\\left( {G - f} \\right)^2}\\\\ &amp;= {\\rm{avg}}\\left( {{g_t}^2 - 2{g_t}G + {G^2}} \\right) + {\\left( {G - f} \\right)^2}\\\\ &amp;= {\\rm{avg}}\\left( {{{\\left( {{g_t} - G} \\right)}^2}} \\right) + {\\left( {G - f} \\right)^2}\\\\ \\tag{7} \\end{align*}\" eeimg=\"1\"/> </p><p>推导过程中应用了 <img src=\"https://www.zhihu.com/equation?tex=G+%3D+%5Cfrac%7B1%7D%7BT%7D%7B%5Csum%5Climits_%7Bt+%3D+1%7D%5ET+%7B%7Bg_t%7D%7D+%7D%3D%7B%5Crm%7Bavg%7D%7D%5Cleft%28+%7B%7Bg_t%7D%7D+%5Cright%29\" alt=\"G = \\frac{1}{T}{\\sum\\limits_{t = 1}^T {{g_t}} }={\\rm{avg}}\\left( {{g_t}} \\right)\" eeimg=\"1\"/> ，把公式 (7) 扩展到全部样本上，可以得到</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%26%7B%5Crm%7Bavg%7D%7D%5Cleft%28+%7B%7BE_%7Bout%7D%7D%5Cleft%28+%7B%7Bg_t%7D%7D+%5Cright%29%7D+%5Cright%29%7B%5Crm%7B+%3D+avg%7D%7D%5Cleft%28+%7B%5Cvarepsilon+%7B%7B%5Cleft%28+%7B%7Bg_t%7D+-+G%7D+%5Cright%29%7D%5E2%7D%7D+%5Cright%29%7B%5Crm%7B+%2B+%7D%7D%7BE_%7Bout%7D%7D%5Cleft%28+G+%5Cright%29%5C%5C+%26%5CRightarrow+%7B%5Crm%7Bavg%7D%7D%5Cleft%28+%7B%7BE_%7Bout%7D%7D%5Cleft%28+%7B%7Bg_t%7D%7D+%5Cright%29%7D+%5Cright%29+%5Cge+%7BE_%7Bout%7D%7D%5Cleft%28+G+%5Cright%29+%5Ctag%7B8%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} &amp;{\\rm{avg}}\\left( {{E_{out}}\\left( {{g_t}} \\right)} \\right){\\rm{ = avg}}\\left( {\\varepsilon {{\\left( {{g_t} - G} \\right)}^2}} \\right){\\rm{ + }}{E_{out}}\\left( G \\right)\\\\ &amp;\\Rightarrow {\\rm{avg}}\\left( {{E_{out}}\\left( {{g_t}} \\right)} \\right) \\ge {E_{out}}\\left( G \\right) \\tag{8} \\end{align*}\" eeimg=\"1\"/> </p><p>所以从平均的角度看，最后的平均误差 <img src=\"https://www.zhihu.com/equation?tex=%5C%5BG%5C%5D\" alt=\"\\[G\\]\" eeimg=\"1\"/> 要比 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7Bg_t%7D%7D%5C%5D\" alt=\"\\[{{g_t}}\\]\" eeimg=\"1\"/> 小。我们进一步考虑一种获得 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7Bg_t%7D%7D%5C%5D\" alt=\"\\[{{g_t}}\\]\" eeimg=\"1\"/> 的方法：</p><blockquote>for t=1,2,...,T:<br/>  1) request size-N data <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BD_t%7D%5C%5D\" alt=\"\\[{D_t}\\]\" eeimg=\"1\"/> from <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BP%5EN%7D%5C%5D\" alt=\"\\[{P^N}\\]\" eeimg=\"1\"/> <br/>  2) obtain <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7Bg_t%7D%7D%5C%5D\" alt=\"\\[{{g_t}}\\]\" eeimg=\"1\"/> by <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm+A%7D%5Cleft%28+%7B%7BD_t%7D%7D+%5Cright%29%5C%5D\" alt=\"\\[{\\rm A}\\left( {{D_t}} \\right)\\]\" eeimg=\"1\"/> <br/> <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cbar+g+%3D+%5Cmathop+%7B%5Clim+%7D%5Climits_%7BT+%5Cto+%5Cinfty+%7D+G+%3D+%5Cmathop+%7B%5Clim+%7D%5Climits_%7BT+%5Cto+%5Cinfty+%7D+%5Cfrac%7B1%7D%7BT%7D%5Csum%5Climits_%7Bt+%3D+1%7D%5ET+%7B%7Bg_t%7D%7D+%3D+%5Cvarepsilon+%5Cleft%28+%7B%7B%5Crm+A%7D%5Cleft%28+D+%5Cright%29%7D+%5Cright%29%5C%5D\" alt=\"\\[\\bar g = \\mathop {\\lim }\\limits_{T \\to \\infty } G = \\mathop {\\lim }\\limits_{T \\to \\infty } \\frac{1}{T}\\sum\\limits_{t = 1}^T {{g_t}} = \\varepsilon \\left( {{\\rm A}\\left( D \\right)} \\right)\\]\" eeimg=\"1\"/> </blockquote><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cbegin%7Barray%7D%7Bl%7D+%7B%5Crm%7Bavg%7D%7D%5Cleft%28+%7B%7BE_%7Bout%7D%7D%5Cleft%28+%7B%7Bg_t%7D%7D+%5Cright%29%7D+%5Cright%29%7B%5Crm%7B+%3D+avg%7D%7D%5Cleft%28+%7B%5Cvarepsilon+%7B%7B%5Cleft%28+%7B%7Bg_t%7D+-+%5Cbar+g%7D+%5Cright%29%7D%5E2%7D%7D+%5Cright%29%7B%5Crm%7B+%2B+%7D%7D%7BE_%7Bout%7D%7D%5Cleft%28+%7B%5Cbar+g%7D+%5Cright%29+%5Cend%7Barray%7D+%5Ctag%7B9%7D+%5C%5D\" alt=\"\\[\\begin{array}{l} {\\rm{avg}}\\left( {{E_{out}}\\left( {{g_t}} \\right)} \\right){\\rm{ = avg}}\\left( {\\varepsilon {{\\left( {{g_t} - \\bar g} \\right)}^2}} \\right){\\rm{ + }}{E_{out}}\\left( {\\bar g} \\right) \\end{array} \\tag{9} \\]\" eeimg=\"1\"/> </p><blockquote>expected performance of A = expected deviation to consensus<br/>                                                +performance of consensus<br/>1) expected deviation to consensus: called variance<br/>2) performance of consensus: called bias</blockquote><p>Uniform Blending 弱化公式 (9) 中第一项 variance 的值，从而演算法的表现就更好了，能得到更加稳定的表现。</p><p>对于 uniform blending，一般要求 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bg_t%7D%5C%5D\" alt=\"\\[{g_t}\\]\" eeimg=\"1\"/> 之间要有差别，这样通过不同 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bg_t%7D%5C%5D\" alt=\"\\[{g_t}\\]\" eeimg=\"1\"/> 的组合能够得到更好的模型。</p><h2><b>二、linear （blending+non-uniform）</b></h2><p>上面介绍的 Uniform Blending 中每个 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bg_t%7D%5C%5D\" alt=\"\\[{g_t}\\]\" eeimg=\"1\"/> 的权重都是 1，即求平均的思想。下面介绍 linear Blending 中每个 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bg_t%7D%5C%5D\" alt=\"\\[{g_t}\\]\" eeimg=\"1\"/> 有不同的权重 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Calpha+_t%7D%5C%5D\" alt=\"\\[{\\alpha _t}\\]\" eeimg=\"1\"/> ，如公式 (10) 所示。</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+G%5Cleft%28+%7B%5Crm%7Bx%7D%7D+%5Cright%29+%3D+%7B%5Crm%7Bsign%7D%7D%5Cleft%28+%7B%5Csum%5Climits_%7Bt+%3D+1%7D%5ET+%7B%7B%5Calpha+_t%7D%7Bg_t%7D%5Cleft%28+%7B%5Crm%7Bx%7D%7D+%5Cright%29%7D+%7D+%5Cright%29%5C+%7B%5Crm%7Bwith%7D%7D%5C+%7B%5Calpha+_t%7D+%5Cge+0+%5Ctag%7B10%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} G\\left( {\\rm{x}} \\right) = {\\rm{sign}}\\left( {\\sum\\limits_{t = 1}^T {{\\alpha _t}{g_t}\\left( {\\rm{x}} \\right)} } \\right)\\ {\\rm{with}}\\ {\\alpha _t} \\ge 0 \\tag{10} \\end{align*}\" eeimg=\"1\"/> </p><p>如何求解 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Calpha+_t%7D%5C%5D\" alt=\"\\[{\\alpha _t}\\]\" eeimg=\"1\"/> ，例如 linear blending for regression，也就是找到一组 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Calpha+%5C%5D\" alt=\"\\[\\alpha \\]\" eeimg=\"1\"/> 使 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BE_%7B%7B%5Crm%7Bin%7D%7D%7D%7D%5Cleft%28+%5Calpha+%5Cright%29%5C%5D\" alt=\"\\[{E_{{\\rm{in}}}}\\left( \\alpha \\right)\\]\" eeimg=\"1\"/> 最小，如公式 (11)所示。</p><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cmathop+%7B%5Cmin+%7D%5Climits_%7B%7B%5Calpha+_t%7D+%5Cge+0%7D+%5Cfrac%7B1%7D%7BN%7D%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%7B%7B%5Cleft%28+%7B%7By_n%7D+-+%5Csum%5Climits_%7Bt+%3D+1%7D%5ET+%7B%7B%5Calpha+_t%7D%7Bg_t%7D%5Cleft%28+%7B%7B%7B%5Crm%7Bx%7D%7D_n%7D%7D+%5Cright%29%7D+%7D+%5Cright%29%7D%5E2%7D%7D+%5Ctag%7B11%7D+%5C%5D\" alt=\"\\[\\mathop {\\min }\\limits_{{\\alpha _t} \\ge 0} \\frac{1}{N}\\sum\\limits_{n = 1}^N {{{\\left( {{y_n} - \\sum\\limits_{t = 1}^T {{\\alpha _t}{g_t}\\left( {{{\\rm{x}}_n}} \\right)} } \\right)}^2}} \\tag{11} \\]\" eeimg=\"1\"/> </p><p>公式 (11) 看起来似从相识，与加了特征化的 linear regression  相比，多了约束条件 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Calpha+_t%7D+%5Cge+0%5C%5D\" alt=\"\\[{\\alpha _t} \\ge 0\\]\" eeimg=\"1\"/> </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-47608e63d77f1ab01586056e0abe18c8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"915\" data-rawheight=\"229\" class=\"origin_image zh-lightbox-thumb\" width=\"915\" data-original=\"https://pic1.zhimg.com/v2-47608e63d77f1ab01586056e0abe18c8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;915&#39; height=&#39;229&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"915\" data-rawheight=\"229\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"915\" data-original=\"https://pic1.zhimg.com/v2-47608e63d77f1ab01586056e0abe18c8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-47608e63d77f1ab01586056e0abe18c8_b.jpg\"/></figure><p>如果能把约束条件去掉就更好了，直接可以用 linear regression 求解 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Calpha+_t%7D%5C%5D\" alt=\"\\[{\\alpha _t}\\]\" eeimg=\"1\"/> 。我们来分析下， <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Calpha+_t%7D+%3C+0%5C%5D\" alt=\"\\[{\\alpha _t} &lt; 0\\]\" eeimg=\"1\"/> ，对于分类问题：也就是将正类看成负类，负类当成正类即可；对于回归问题：类似将 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7Bg_t%7D%7D%5C%5D\" alt=\"\\[{{g_t}}\\]\" eeimg=\"1\"/> 变号，不影响最后结果。所以可以把约束条件去掉。</p><h2><b>三、stacking（blending+conditional）</b></h2><p>在上节的介绍的 linear Blending 中， <img src=\"https://www.zhihu.com/equation?tex=G\" alt=\"G\" eeimg=\"1\"/> 是 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7Bg_t%7D%7D%5C%5D\" alt=\"\\[{{g_t}}\\]\" eeimg=\"1\"/> 的线性组合。当然<img src=\"https://www.zhihu.com/equation?tex=G\" alt=\"G\" eeimg=\"1\"/> 也可以是 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7Bg_t%7D%7D%5C%5D\" alt=\"\\[{{g_t}}\\]\" eeimg=\"1\"/> 的非线性组合，这种形式称为 stacking。优点是模型更复杂，能力更强大，但是容易过拟合。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-dcba52b805278bbcf7e2197d75e7221a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"906\" data-rawheight=\"393\" class=\"origin_image zh-lightbox-thumb\" width=\"906\" data-original=\"https://pic3.zhimg.com/v2-dcba52b805278bbcf7e2197d75e7221a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;906&#39; height=&#39;393&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"906\" data-rawheight=\"393\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"906\" data-original=\"https://pic3.zhimg.com/v2-dcba52b805278bbcf7e2197d75e7221a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-dcba52b805278bbcf7e2197d75e7221a_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/43191666", 
            "userName": "徽州风韵", 
            "userLink": "https://www.zhihu.com/people/53bd6f8b93873b03c5edb714aea2a456", 
            "upvote": 2, 
            "title": "Support Vector Regression", 
            "content": "<h2><b>一、Kernel Ridge Regression</b></h2><p>在 Rdige 回归中使用核技巧，如公式 (1)所示</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Cmathop+%7B%5Cmin+%7D%5Climits_%7B%5Crm%7Bw%7D%7D+%5Cfrac%7B%5Clambda+%7D%7BN%7D%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%5Crm%7Bw%7D%7D+%2B+%5Cfrac%7B1%7D%7BN%7D%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%5Cleft%28+%7B%7By_n%7D+-+%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%7B%5Crm%7Bz%7D%7D_n%7D%7D+%5Cright%29%5E%7B2%7D%7D+%5Ctag%7B1%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} \\mathop {\\min }\\limits_{\\rm{w}} \\frac{\\lambda }{N}{{\\rm{w}}^T}{\\rm{w}} + \\frac{1}{N}\\sum\\limits_{n = 1}^N {\\left( {{y_n} - {{\\rm{w}}^T}{{\\rm{z}}_n}} \\right)^{2}} \\tag{1} \\end{align*}\" eeimg=\"1\"/></p><p>根据<a href=\"https://zhuanlan.zhihu.com/p/43130548\" class=\"internal\">SVM-Kernel</a>中 <b>Representer Theorem</b> 可得公式 (1) 最优解 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5Crm%7Bw%7D%7D%5E%2A%7D+%3D+%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%7B%5Cbeta+_n%7D%7B%7B%5Crm%7Bz%7D%7D_n%7D%7D+%5C%5D\" alt=\"\\[{{\\rm{w}}^*} = \\sum\\limits_{n = 1}^N {{\\beta _n}{{\\rm{z}}_n}} \\]\" eeimg=\"1\"/> 。将 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5Crm%7Bw%7D%7D%5E%2A%7D%5C%5D\" alt=\"\\[{{\\rm{w}}^*}\\]\" eeimg=\"1\"/> 带入公式 (1) 得：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cbegin%7Barray%7D%7Bl%7D+E+%3D+%5Cfrac%7B%5Clambda+%7D%7BN%7D%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%5Csum%5Climits_%7Bm+%3D+1%7D%5EN+%7B%7B%5Cbeta+_n%7D%7B%5Cbeta+_m%7DK%5Cleft%28+%7B%7B%7B%5Crm%7Bx%7D%7D_n%7D%2C%7B%7B%5Crm%7Bx%7D%7D_m%7D%7D+%5Cright%29%7D+%7D+%2B+%5Cfrac%7B1%7D%7BN%7D%7B%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%5Cleft%28+%7B%7By_n%7D+-+%5Csum%5Climits_%7Bm+%3D+1%7D%5EN+%7B%7B%5Cbeta+_m%7DK%5Cleft%28+%7B%7B%7B%5Crm%7Bx%7D%7D_n%7D%2C%7B%7B%5Crm%7Bx%7D%7D_m%7D%7D+%5Cright%29%7D+%7D+%5Cright%29%7D+%5E2%7D%5C%5C+%3D+%5Cfrac%7B%5Clambda+%7D%7BN%7D%7B%5Cbeta+%5ET%7DK%5Cbeta+%2B+%5Cfrac%7B1%7D%7BN%7D%7B%5Cleft%28+%7By+-+K%5Cbeta+%7D+%5Cright%29%5ET%7D%5Cleft%28+%7By+-+K%5Cbeta+%7D+%5Cright%29+%5Ctag%7B2%7D+%5Cend%7Barray%7D%5C%5D\" alt=\"\\[\\begin{array}{l} E = \\frac{\\lambda }{N}\\sum\\limits_{n = 1}^N {\\sum\\limits_{m = 1}^N {{\\beta _n}{\\beta _m}K\\left( {{{\\rm{x}}_n},{{\\rm{x}}_m}} \\right)} } + \\frac{1}{N}{\\sum\\limits_{n = 1}^N {\\left( {{y_n} - \\sum\\limits_{m = 1}^N {{\\beta _m}K\\left( {{{\\rm{x}}_n},{{\\rm{x}}_m}} \\right)} } \\right)} ^2}\\\\ = \\frac{\\lambda }{N}{\\beta ^T}K\\beta + \\frac{1}{N}{\\left( {y - K\\beta } \\right)^T}\\left( {y - K\\beta } \\right) \\tag{2} \\end{array}\\]\" eeimg=\"1\"/></p><p>对 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cbeta+%5C%5D\" alt=\"\\[\\beta \\]\" eeimg=\"1\"/> 求导得：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Cfrac%7B%7B%5Cpartial+E%7D%7D%7B%7B%5Cpartial+%5Cbeta+%7D%7D+%3D+%5Cfrac%7B2%7D%7BN%7D%5Cleft%28+%7B%5Clambda+%7BK%5ET%7D%5Cbeta+%2B+%7BK%5ET%7DK%5Cbeta+-+%7BK%5ET%7Dy%7D+%5Cright%29%5C%5C+%3D+%5Cfrac%7B2%7D%7BN%7D%7BK%5ET%7D%5Cleft%28+%7B%5Cleft%28+%7B%5Clambda+%7BI_%7BN+%5Ctimes+N%7D%7D+%2B+K%7D+%5Cright%29%5Cbeta+-+y%7D+%5Cright%29+%5Ctag%7B3%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} \\frac{{\\partial E}}{{\\partial \\beta }} = \\frac{2}{N}\\left( {\\lambda {K^T}\\beta + {K^T}K\\beta - {K^T}y} \\right)\\\\ = \\frac{2}{N}{K^T}\\left( {\\left( {\\lambda {I_{N \\times N}} + K} \\right)\\beta - y} \\right) \\tag{3} \\end{align*}\" eeimg=\"1\"/> </p><p>令导数为0，可以得到解析解：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Cbeta+%3D+%7B%5Cleft%28+%7B%5Clambda+%7BI_%7BN+%5Ctimes+N%7D%7D+%2B+K%7D+%5Cright%29%5E+-+%7Dy+%5Ctag%7B4%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} \\beta = {\\left( {\\lambda {I_{N \\times N}} + K} \\right)^ - }y \\tag{4} \\end{align*}\" eeimg=\"1\"/></p><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Clambda+%7BI_%7BN+%5Ctimes+N%7D%7D+%2B+K%7D%5C%5D\" alt=\"\\[{\\lambda {I_{N \\times N}} + K}\\]\" eeimg=\"1\"/> 的逆一定存在，因为 <img src=\"https://www.zhihu.com/equation?tex=%5C%5BK%5C%5D\" alt=\"\\[K\\]\" eeimg=\"1\"/> 是半正定矩阵且 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Clambda+%3E+0%5C%5D\" alt=\"\\[\\lambda &gt; 0\\]\" eeimg=\"1\"/> 。和 Linear Ridge Regression 进行对比：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-26d80790eb7bcf0462d1d089fc2fe44c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"896\" data-rawheight=\"526\" class=\"origin_image zh-lightbox-thumb\" width=\"896\" data-original=\"https://pic1.zhimg.com/v2-26d80790eb7bcf0462d1d089fc2fe44c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;896&#39; height=&#39;526&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"896\" data-rawheight=\"526\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"896\" data-original=\"https://pic1.zhimg.com/v2-26d80790eb7bcf0462d1d089fc2fe44c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-26d80790eb7bcf0462d1d089fc2fe44c_b.jpg\"/></figure><h2><b>二、SVR原问题</b></h2><p>类似 soft margin SVM 我们希望训练出来的模型有更强的泛化能力，能够容忍一定的噪声干扰。也就是在训练时候，模型的输出值和真实值相差在一定的范围内，我们就认为没有犯错，如图 1 所示，具体数学表达见公式 (5)。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-ca29f6269b5c621e60f685e7b4c4bbce_b.jpg\" data-size=\"normal\" data-rawwidth=\"207\" data-rawheight=\"205\" class=\"content_image\" width=\"207\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;207&#39; height=&#39;205&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"207\" data-rawheight=\"205\" class=\"content_image lazy\" width=\"207\" data-actualsrc=\"https://pic3.zhimg.com/v2-ca29f6269b5c621e60f685e7b4c4bbce_b.jpg\"/><figcaption>图 1</figcaption></figure><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%7B%5Crm%7Berr%7D%7D%5Cleft%28+%7By%2Cs%7D+%5Cright%29+%3D+%5Cmax+%5Cleft%28+%7B0%2C%5Cleft%7C+%7Bs+-+y%7D+%5Cright%7C+-+%5Cvarepsilon+%7D+%5Cright%29+%5Ctag%7B5%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} {\\rm{err}}\\left( {y,s} \\right) = \\max \\left( {0,\\left| {s - y} \\right| - \\varepsilon } \\right) \\tag{5} \\end{align*}\" eeimg=\"1\"/></p><p> 带有 <img src=\"https://www.zhihu.com/equation?tex=L2\" alt=\"L2\" eeimg=\"1\"/> 正则化的 SVR 损失函数为：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Cmathop+%7B%5Cmin+%7D%5Climits_%7B%7B%5Crm%7Bb%2Cw%7D%7D%7D+%5Cfrac%7B1%7D%7B2%7D%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%5Crm%7Bw%7D%7D+%2B+C%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%5Cmax+%5Cleft%28+%7B0%2C%5Cleft%7C+%7B%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%7B%5Crm%7Bz%7D%7D_n%7D+%2B+b+-+%7By_n%7D%7D+%5Cright%7C+-+%5Cvarepsilon+%7D+%5Cright%29%7D+%5Ctag%7B6%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} \\mathop {\\min }\\limits_{{\\rm{b,w}}} \\frac{1}{2}{{\\rm{w}}^T}{\\rm{w}} + C\\sum\\limits_{n = 1}^N {\\max \\left( {0,\\left| {{{\\rm{w}}^T}{{\\rm{z}}_n} + b - {y_n}} \\right| - \\varepsilon } \\right)} \\tag{6} \\end{align*}\" eeimg=\"1\"/></p><p> 公式 (6) 可进一步的变为公式 (7)：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Cmathop+%7B%5Cmin+%7D%5Climits_%7B%7B%5Crm%7Bb%2Cw%2C%7D%7D%5Cxi+%7D+%5C+%26%5Cfrac%7B1%7D%7B2%7D%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%5Crm%7Bw%7D%7D+%2B+C%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%7B%5Cxi+_n%7D%7D+%5C%5C+s.t.%5C+%26%5Cleft%7C+%7B%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%7B%5Crm%7Bz%7D%7D_n%7D+%2B+b+-+%7By_n%7D%7D+%5Cright%7C+%5Cle+%7B%5Cxi+_n%7D+%2B+%5Cvarepsilon+%5C%5C+%26%7B%5Cxi+_n%7D+%5Cge+0+%5Ctag%7B7%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} \\mathop {\\min }\\limits_{{\\rm{b,w,}}\\xi } \\ &amp;\\frac{1}{2}{{\\rm{w}}^T}{\\rm{w}} + C\\sum\\limits_{n = 1}^N {{\\xi _n}} \\\\ s.t.\\ &amp;\\left| {{{\\rm{w}}^T}{{\\rm{z}}_n} + b - {y_n}} \\right| \\le {\\xi _n} + \\varepsilon \\\\ &amp;{\\xi _n} \\ge 0 \\tag{7} \\end{align*}\" eeimg=\"1\"/></p><p>公式 (6) 和公式(7)是等价的，我们简单说明下。</p><ul><li><img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cleft%7C+%7B%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%7B%5Crm%7Bz%7D%7D_n%7D+%2B+b+-+%7By_n%7D%7D+%5Cright%7C+-+%5Cvarepsilon+%5Cle+0%5C%5D\" alt=\"\\[\\left| {{{\\rm{w}}^T}{{\\rm{z}}_n} + b - {y_n}} \\right| - \\varepsilon \\le 0\\]\" eeimg=\"1\"/> ：在误差允许范围内，所以对应的惩罚应该为 <img src=\"https://www.zhihu.com/equation?tex=0\" alt=\"0\" eeimg=\"1\"/> 。在公式 (6)中 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cmax+%5Cleft%28+%7B0%2C%5Cleft%7C+%7B%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%7B%5Crm%7Bz%7D%7D_n%7D+%2B+b+-+%7By_n%7D%7D+%5Cright%7C+-+%5Cvarepsilon+%7D+%5Cright%29+%3D+0%5C%5D\" alt=\"\\[\\max \\left( {0,\\left| {{{\\rm{w}}^T}{{\\rm{z}}_n} + b - {y_n}} \\right| - \\varepsilon } \\right) = 0\\]\" eeimg=\"1\"/> ，公式 (7) 中 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Cxi+_n%7D+%5Cge+0%5C%5D\" alt=\"\\[{\\xi _n} \\ge 0\\]\" eeimg=\"1\"/> ，如果最小化损失函数，那么 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Cxi+_n%7D%5C%5D\" alt=\"\\[{\\xi _n}\\]\" eeimg=\"1\"/> 需取到最小值，所以<img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Cxi+_n%7D+%3D+0%5C%5D\" alt=\"\\[{\\xi _n} = 0\\]\" eeimg=\"1\"/> 。</li><li><img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cleft%7C+%7B%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%7B%5Crm%7Bz%7D%7D_n%7D+%2B+b+-+%7By_n%7D%7D+%5Cright%7C+-+%5Cvarepsilon+%3E+0%5C%5D\" alt=\"\\[\\left| {{{\\rm{w}}^T}{{\\rm{z}}_n} + b - {y_n}} \\right| - \\varepsilon &gt; 0\\]\" eeimg=\"1\"/> ：超出误差允许范围，需要进行一定的惩罚，惩罚大小为 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cleft%7C+%7B%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%7B%5Crm%7Bz%7D%7D_n%7D+%2B+b+-+%7By_n%7D%7D+%5Cright%7C+-+%5Cvarepsilon+%5C%5D\" alt=\"\\[\\left| {{{\\rm{w}}^T}{{\\rm{z}}_n} + b - {y_n}} \\right| - \\varepsilon \\]\" eeimg=\"1\"/> 。 在公式 (6) 中损失函数惩罚 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cleft%7C+%7B%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%7B%5Crm%7Bz%7D%7D_n%7D+%2B+b+-+%7By_n%7D%7D+%5Cright%7C+-+%5Cvarepsilon+%5C%5D\" alt=\"\\[\\left| {{{\\rm{w}}^T}{{\\rm{z}}_n} + b - {y_n}} \\right| - \\varepsilon \\]\" eeimg=\"1\"/>，在公式(7)中 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cleft%7C+%7B%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%7B%5Crm%7Bz%7D%7D_n%7D+%2B+b+-+%7By_n%7D%7D+%5Cright%7C+-+%5Cvarepsilon+%5Cle+%7B%5Cxi+_n%7D%5C%5D\" alt=\"\\[\\left| {{{\\rm{w}}^T}{{\\rm{z}}_n} + b - {y_n}} \\right| - \\varepsilon \\le {\\xi _n}\\]\" eeimg=\"1\"/> ，如果最小化损失函数，那么 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Cxi+_n%7D%5C%5D\" alt=\"\\[{\\xi _n}\\]\" eeimg=\"1\"/> 需取到最小值，所以 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Cxi+_n%7D+%3D+%5Cleft%7C+%7B%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%7B%5Crm%7Bz%7D%7D_n%7D+%2B+b+-+%7By_n%7D%7D+%5Cright%7C+-+%5Cvarepsilon+%5C%5D\" alt=\"\\[{\\xi _n} = \\left| {{{\\rm{w}}^T}{{\\rm{z}}_n} + b - {y_n}} \\right| - \\varepsilon \\]\" eeimg=\"1\"/> 。</li></ul><p>所以可以看出公式 (6) 和公式 (7) 是等价的。进一步把公式 (7) 中的非线性约束变为线性约束。</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Cmathop+%7B%5Cmin+%7D%5Climits_%7B%7B%5Crm%7Bb%2Cw%2C%7D%7D%7B%5Cxi+%5E+%5Cvee+%7D%2C%7B%5Cxi+%5E+%5Cwedge+%7D%7D+%5C+%26%5Cfrac%7B1%7D%7B2%7D%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%5Crm%7Bw%7D%7D+%2B+C%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%5Cleft%28+%7B%5Cxi+_n%5E+%5Cvee+%2B+%5Cxi+_n%5E+%5Cwedge+%7D+%5Cright%29%7D+%5C%5C+s.t.+%5C+%26-+%5Cvarepsilon+-+%5Cxi+_n%5E+%5Cvee+%5Cle+%7By_n%7D+-+%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%7B%5Crm%7Bz%7D%7D_n%7D+-+b+%5Cle+%5Cvarepsilon+%2B+%5Cxi+_n%5E+%5Cwedge+%5C%5C+%26%5Cxi+_n%5E+%5Cvee+%5Cge+0%2C%5Cxi+_n%5E+%5Cwedge+%5Cge+0+%5Ctag%7B8%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} \\mathop {\\min }\\limits_{{\\rm{b,w,}}{\\xi ^ \\vee },{\\xi ^ \\wedge }} \\ &amp;\\frac{1}{2}{{\\rm{w}}^T}{\\rm{w}} + C\\sum\\limits_{n = 1}^N {\\left( {\\xi _n^ \\vee + \\xi _n^ \\wedge } \\right)} \\\\ s.t. \\ &amp;- \\varepsilon - \\xi _n^ \\vee \\le {y_n} - {{\\rm{w}}^T}{{\\rm{z}}_n} - b \\le \\varepsilon + \\xi _n^ \\wedge \\\\ &amp;\\xi _n^ \\vee \\ge 0,\\xi _n^ \\wedge \\ge 0 \\tag{8} \\end{align*}\" eeimg=\"1\"/> </p><p>同样可以采用 QP 求解，有 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Ctilde+d+%2B+1+%2B+2N%5C%5D\" alt=\"\\[\\tilde d + 1 + 2N\\]\" eeimg=\"1\"/> 个参数和 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B2N+%2B+2N%5C%5D\" alt=\"\\[2N + 2N\\]\" eeimg=\"1\"/> 个约束项。我们需要解决对 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Ctilde+d%7D%5C%5D\" alt=\"\\[{\\tilde d}\\]\" eeimg=\"1\"/> 的依赖问题。</p><h2><b>三、SVR对偶问题</b></h2><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+L%5Cleft%28+%7B%7Bb%2C%5Crm%7Bw%2C%7D%7D%7B%5Cxi+%5E+%5Cvee+%7D%2C%7B%5Cxi+%5E+%5Cwedge+%7D%7D+%5Cright%29+%3D+%5Cfrac%7B1%7D%7B2%7D%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%5Crm%7Bw%7D%7D+%2B+C%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%5Cleft%28+%7B%5Cxi+_n%5E+%5Cvee+%2B+%5Cxi+_n%5E+%5Cwedge+%7D+%5Cright%29%7D+%2B+%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%5Calpha+_n%5E+%5Cvee+%5Cleft%28+%7B+-+%5Cvarepsilon+-+%5Cxi+_n%5E+%5Cvee+-+%7By_n%7D%7B%5Crm%7B+%2B+%7D%7D%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%7B%5Crm%7Bz%7D%7D_n%7D+%2B+b%7D+%5Cright%29%7D+%5C%5C+%2B+%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%5Calpha+_n%5E+%5Cwedge+%5Cleft%28+%7B+-+%5Cvarepsilon+-+%5Cxi+_n%5E+%5Cwedge+%2B+%7By_n%7D+-+%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%7B%5Crm%7Bz%7D%7D_n%7D+-+b%7D+%5Cright%29%7D+%7B%5Crm%7B+%2B+%7D%7D%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B+-+%5Cbeta+_n%5E+%5Cwedge+%5Cxi+_n%5E+%5Cwedge+%7D+%2B+%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B+-+%5Cbeta+_n%5E+%5Cvee+%5Cxi+_n%5E+%5Cvee+%7D+%5Ctag%7B8%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} L\\left( {{b,\\rm{w,}}{\\xi ^ \\vee },{\\xi ^ \\wedge }} \\right) = \\frac{1}{2}{{\\rm{w}}^T}{\\rm{w}} + C\\sum\\limits_{n = 1}^N {\\left( {\\xi _n^ \\vee + \\xi _n^ \\wedge } \\right)} + \\sum\\limits_{n = 1}^N {\\alpha _n^ \\vee \\left( { - \\varepsilon - \\xi _n^ \\vee - {y_n}{\\rm{ + }}{{\\rm{w}}^T}{{\\rm{z}}_n} + b} \\right)} \\\\ + \\sum\\limits_{n = 1}^N {\\alpha _n^ \\wedge \\left( { - \\varepsilon - \\xi _n^ \\wedge + {y_n} - {{\\rm{w}}^T}{{\\rm{z}}_n} - b} \\right)} {\\rm{ + }}\\sum\\limits_{n = 1}^N { - \\beta _n^ \\wedge \\xi _n^ \\wedge } + \\sum\\limits_{n = 1}^N { - \\beta _n^ \\vee \\xi _n^ \\vee } \\tag{8} \\end{align*}\" eeimg=\"1\"/></p><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cbegin%7Barray%7D%7Bl%7D+%5Cfrac%7B%7B%5Cpartial+L%7D%7D%7B%7B%5Cpartial+b%7D%7D+%3D+%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%5Cleft%28+%7B%5Calpha+_n%5E+%5Cvee+-+%5Calpha+_n%5E+%5Cwedge+%7D+%5Cright%29%7D+%3D+0%5C%5C+%5Cfrac%7B%7B%5Cpartial+L%7D%7D%7B%7B%5Cpartial+%7B%5Crm%7Bw%7D%7D%7D%7D+%3D+%7B%5Crm%7Bw+%2B+%7D%7D%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%5Cleft%28+%7B%5Calpha+_n%5E+%5Cvee+-+%5Calpha+_n%5E+%5Cwedge+%7D+%5Cright%29%7B%7B%5Crm%7Bz%7D%7D_n%7D%7D+%3D+0+%5CRightarrow+%7B%5Crm%7Bw%7D%7D+%3D+%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%5Cleft%28+%7B%5Calpha+_n%5E+%5Cwedge+-+%5Calpha+_n%5E+%5Cvee+%7D+%5Cright%29%7B%7B%5Crm%7Bz%7D%7D_n%7D%7D+%5C%5C+%5Cfrac%7B%7B%5Cpartial+L%7D%7D%7B%7B%5Cpartial+%5Cxi+_n%5E+%5Cvee+%7D%7D+%3D+C+-+%5Calpha+_n%5E+%5Cvee+-+%5Cbeta+_n%5E+%5Cvee+%3D+0+%5CRightarrow+%5Cbeta+_n%5E+%5Cvee+%3D+C+-+%5Calpha+_n%5E+%5Cvee+%5C%5C+%5Cfrac%7B%7B%5Cpartial+L%7D%7D%7B%7B%5Cpartial+%5Cxi+_n%5E+%5Cwedge+%7D%7D+%3D+C+-+%5Calpha+_n%5E+%5Cwedge+-+%5Cbeta+_n%5E+%5Cwedge+%3D+0+%5CRightarrow+%5Cbeta+_n%5E+%5Cwedge+%3D+C+-+%5Calpha+_n%5E+%5Cwedge+%5Ctag%7B9%7D+%5Cend%7Barray%7D%5C%5D\" alt=\"\\[\\begin{array}{l} \\frac{{\\partial L}}{{\\partial b}} = \\sum\\limits_{n = 1}^N {\\left( {\\alpha _n^ \\vee - \\alpha _n^ \\wedge } \\right)} = 0\\\\ \\frac{{\\partial L}}{{\\partial {\\rm{w}}}} = {\\rm{w + }}\\sum\\limits_{n = 1}^N {\\left( {\\alpha _n^ \\vee - \\alpha _n^ \\wedge } \\right){{\\rm{z}}_n}} = 0 \\Rightarrow {\\rm{w}} = \\sum\\limits_{n = 1}^N {\\left( {\\alpha _n^ \\wedge - \\alpha _n^ \\vee } \\right){{\\rm{z}}_n}} \\\\ \\frac{{\\partial L}}{{\\partial \\xi _n^ \\vee }} = C - \\alpha _n^ \\vee - \\beta _n^ \\vee = 0 \\Rightarrow \\beta _n^ \\vee = C - \\alpha _n^ \\vee \\\\ \\frac{{\\partial L}}{{\\partial \\xi _n^ \\wedge }} = C - \\alpha _n^ \\wedge - \\beta _n^ \\wedge = 0 \\Rightarrow \\beta _n^ \\wedge = C - \\alpha _n^ \\wedge \\tag{9} \\end{array}\\]\" eeimg=\"1\"/> </p><p>将公式 (9) 得到的结果代入公式 (8) 可得对偶问题：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Cmathop+%7B%5Cmin+%7D%5Climits_%7B%7B%5Calpha+%5E+%5Cwedge+%7D%2C%7B%5Calpha+%5E+%5Cvee+%7D%7D%5C+%26+%5Cfrac%7B1%7D%7B2%7D%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%5Csum%5Climits_%7Bm+%3D+1%7D%5EN+%7B%5Cleft%28+%7B%5Calpha+_n%5E+%5Cwedge+-+%5Calpha+_n%5E+%5Cvee+%7D+%5Cright%29%5Cleft%28+%7B%5Calpha+_m%5E+%5Cwedge+-+%5Calpha+_m%5E+%5Cvee+%7D+%5Cright%29%7BK_%7Bn%2Cm%7D%7D%7D+%2B+%7D+%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%5Cleft%28+%7B%5Cleft%28+%7B%5Cvarepsilon+-+%7By_n%7D%7D+%5Cright%29%5Calpha+_n%5E+%5Cwedge+%2B+%5Cleft%28+%7B%5Cvarepsilon+%2B+%7By_n%7D%7D+%5Cright%29%5Calpha+_n%5E+%5Cvee+%7D+%5Cright%29%7D+%5C%5C+s.t.%5C+%26%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%5Cleft%28+%7B%5Calpha+_n%5E+%5Cvee+-+%5Calpha+_n%5E+%5Cwedge+%7D+%5Cright%29%7D+%3D+0%5C%5C+%260+%5Cle+%5Calpha+_n%5E+%5Cwedge+%5Cle+C%2C0+%5Cle+%5Calpha+_n%5E+%5Cvee+%5Cle+C+%5Ctag%7B10%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} \\mathop {\\min }\\limits_{{\\alpha ^ \\wedge },{\\alpha ^ \\vee }}\\ &amp; \\frac{1}{2}\\sum\\limits_{n = 1}^N {\\sum\\limits_{m = 1}^N {\\left( {\\alpha _n^ \\wedge - \\alpha _n^ \\vee } \\right)\\left( {\\alpha _m^ \\wedge - \\alpha _m^ \\vee } \\right){K_{n,m}}} + } \\sum\\limits_{n = 1}^N {\\left( {\\left( {\\varepsilon - {y_n}} \\right)\\alpha _n^ \\wedge + \\left( {\\varepsilon + {y_n}} \\right)\\alpha _n^ \\vee } \\right)} \\\\ s.t.\\ &amp;\\sum\\limits_{n = 1}^N {\\left( {\\alpha _n^ \\vee - \\alpha _n^ \\wedge } \\right)} = 0\\\\ &amp;0 \\le \\alpha _n^ \\wedge \\le C,0 \\le \\alpha _n^ \\vee \\le C \\tag{10} \\end{align*}\" eeimg=\"1\"/> </p><p>根据互补松弛性质</p><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cbegin%7Barray%7D%7Bl%7D+%5Calpha+_n%5E+%5Cvee+%5Cleft%28+%7B+-+%5Cvarepsilon+-+%5Cxi+_n%5E+%5Cvee+-+%7By_n%7D%7B%5Crm%7B+%2B+%7D%7D%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%7B%5Crm%7Bz%7D%7D_n%7D+%2B+b%7D+%5Cright%29+%3D+0%5C%5C+%5Calpha+_n%5E+%5Cwedge+%5Cleft%28+%7B+-+%5Cvarepsilon+-+%5Cxi+_n%5E+%5Cwedge+%2B+%7By_n%7D+-+%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%7B%5Crm%7Bz%7D%7D_n%7D+-+b%7D+%5Cright%29+%3D+0+%5Ctag%7B11%7D+%5Cend%7Barray%7D%5C%5D\" alt=\"\\[\\begin{array}{l} \\alpha _n^ \\vee \\left( { - \\varepsilon - \\xi _n^ \\vee - {y_n}{\\rm{ + }}{{\\rm{w}}^T}{{\\rm{z}}_n} + b} \\right) = 0\\\\ \\alpha _n^ \\wedge \\left( { - \\varepsilon - \\xi _n^ \\wedge + {y_n} - {{\\rm{w}}^T}{{\\rm{z}}_n} - b} \\right) = 0 \\tag{11} \\end{array}\\]\" eeimg=\"1\"/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-1a3229a0f58eded0511f0caf73d7f90b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"689\" data-rawheight=\"480\" class=\"origin_image zh-lightbox-thumb\" width=\"689\" data-original=\"https://pic4.zhimg.com/v2-1a3229a0f58eded0511f0caf73d7f90b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;689&#39; height=&#39;480&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"689\" data-rawheight=\"480\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"689\" data-original=\"https://pic4.zhimg.com/v2-1a3229a0f58eded0511f0caf73d7f90b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-1a3229a0f58eded0511f0caf73d7f90b_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "回归", 
                    "tagLink": "https://api.zhihu.com/topics/19755107"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/43169033", 
            "userName": "徽州风韵", 
            "userLink": "https://www.zhihu.com/people/53bd6f8b93873b03c5edb714aea2a456", 
            "upvote": 1, 
            "title": "SVM-Soft Margin", 
            "content": "<h2><b>一、引言</b></h2><p>前面介绍的 Hard Margin SVM 容易过拟合，主要原因：一、由于我们的SVM模型（即kernel）过于复杂，转换的维度太多，过于 powerful 了；二、由于我们坚持要将所有的样本都分类正确，即不允许错误存在，造成模型过于复杂。如图 4.1 右侧所示。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-6d8dceb6c51cdeccc591648036591eb0_b.jpg\" data-size=\"normal\" data-rawwidth=\"693\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb\" width=\"693\" data-original=\"https://pic1.zhimg.com/v2-6d8dceb6c51cdeccc591648036591eb0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;693&#39; height=&#39;299&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"693\" data-rawheight=\"299\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"693\" data-original=\"https://pic1.zhimg.com/v2-6d8dceb6c51cdeccc591648036591eb0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-6d8dceb6c51cdeccc591648036591eb0_b.jpg\"/><figcaption>图 4.1</figcaption></figure><p>如何避免过拟合？具体方法是允许有分类错误的点，即把某些点当作是 noise，放弃这些noise点，但是尽量让这些 noise 个数越少越好。我们可以修改 <a href=\"https://zhuanlan.zhihu.com/p/43043846\" class=\"internal\">Hard Margin</a> 中公式 (9) 达到目的。</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Cmathop+%7B%5Cmin+%7D%5Climits_%7Bb%2C%7B%5Crm%7Bw%7D%7D%7D+%5C+%26%5Cfrac%7B1%7D%7B2%7D%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%5Crm%7Bw%7D%7D+%2B+C+%5Ccdot+%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%5Cleft%5B%5Ckern-0.15em%5Cleft%5B+%7B%7By_n%7D+%5Cne+%7B%5Crm%7Bsign%7D%7D%5Cleft%28+%7B%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%7B%5Crm%7Bz%7D%7D_n%7D+%2B+b%7D+%5Cright%29%7D+%5Cright%5D%5Ckern-0.15em%5Cright%5D%7D+%5C%5C+s.t.%5C+%5C+%26%7By_n%7D%5Cleft%28+%7B%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%7B%5Crm%7Bz%7D%7D_n%7D+%2B+b%7D+%5Cright%29+%5Cge+1%5C+%7B%5Crm%7Bfor+%5C+correct%7D%7D%5C+n%5C%5C+%26%7By_n%7D%5Cleft%28+%7B%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%7B%5Crm%7Bz%7D%7D_n%7D+%2B+b%7D+%5Cright%29+%5Cge+-+%5Cinfty%5C+%7B%5Crm%7Bfor%5C+incorrect%7D%7D%5C+n+%5Ctag%7B1%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} \\mathop {\\min }\\limits_{b,{\\rm{w}}} \\ &amp;\\frac{1}{2}{{\\rm{w}}^T}{\\rm{w}} + C \\cdot \\sum\\limits_{n = 1}^N {\\left[\\kern-0.15em\\left[ {{y_n} \\ne {\\rm{sign}}\\left( {{{\\rm{w}}^T}{{\\rm{z}}_n} + b} \\right)} \\right]\\kern-0.15em\\right]} \\\\ s.t.\\ \\ &amp;{y_n}\\left( {{{\\rm{w}}^T}{{\\rm{z}}_n} + b} \\right) \\ge 1\\ {\\rm{for \\ correct}}\\ n\\\\ &amp;{y_n}\\left( {{{\\rm{w}}^T}{{\\rm{z}}_n} + b} \\right) \\ge - \\infty\\ {\\rm{for\\ incorrect}}\\ n \\tag{1} \\end{align*}\" eeimg=\"1\"/> </p><p>公式 (1) 中，正确分类的点需满足 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7By_n%7D%5Cleft%28+%7B%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%7B%5Crm%7Bz%7D%7D_n%7D+%2B+b%7D+%5Cright%29+%5Cge+1%5C%5D\" alt=\"\\[{y_n}\\left( {{{\\rm{w}}^T}{{\\rm{z}}_n} + b} \\right) \\ge 1\\]\" eeimg=\"1\"/> 约束条件；部分不正确分类的点不加任何约束条件 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7By_n%7D%5Cleft%28+%7B%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%7B%5Crm%7Bz%7D%7D_n%7D+%2B+b%7D+%5Cright%29+%5Cge+-+%5Cinfty+%5C%5D\" alt=\"\\[{y_n}\\left( {{{\\rm{w}}^T}{{\\rm{z}}_n} + b} \\right) \\ge - \\infty \\]\" eeimg=\"1\"/> ；需要限制不正确分类点的数量 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%5Cleft%5B%5Ckern-0.15em%5Cleft%5B+%7B%7By_n%7D+%5Cne+%7B%5Crm%7Bsign%7D%7D%5Cleft%28+%7B%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%7B%5Crm%7Bz%7D%7D_n%7D+%2B+b%7D+%5Cright%29%7D+%5Cright%5D%5Ckern-0.15em%5Cright%5D%7D+%5C%5D\" alt=\"\\[\\sum\\limits_{n = 1}^N {\\left[\\kern-0.15em\\left[ {{y_n} \\ne {\\rm{sign}}\\left( {{{\\rm{w}}^T}{{\\rm{z}}_n} + b} \\right)} \\right]\\kern-0.15em\\right]} \\]\" eeimg=\"1\"/> ；参数 <img src=\"https://www.zhihu.com/equation?tex=C\" alt=\"C\" eeimg=\"1\"/> 用于权衡 large margin 和 noise tolerance。</p><p>公式 (1) 可以进一步地调整为：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Cmathop+%7B%5Cmin+%7D%5Climits_%7Bb%2C%7B%5Crm%7Bw%7D%7D%7D+%5C+%26%5Cfrac%7B1%7D%7B2%7D%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%5Crm%7Bw%7D%7D+%2B+C+%5Ccdot+%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%5Cleft%5B%5Ckern-0.15em%5Cleft%5B+%7B%7By_n%7D+%5Cne+%7B%5Crm%7Bsign%7D%7D%5Cleft%28+%7B%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%7B%5Crm%7Bz%7D%7D_n%7D+%2B+b%7D+%5Cright%29%7D+%5Cright%5D%5Ckern-0.15em%5Cright%5D%7D+%5C%5C+s.t.%5C+%5C+%26%7By_n%7D%5Cleft%28+%7B%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%7B%5Crm%7Bz%7D%7D_n%7D+%2B+b%7D+%5Cright%29+%5Cge+1+-+%5Cinfty+%5Cleft%5B%5Ckern-0.15em%5Cleft%5B+%7B%7By_n%7D+%5Cne+%7B%5Crm%7Bsign%7D%7D%5Cleft%28+%7B%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%7B%5Crm%7Bz%7D%7D_n%7D+%2B+b%7D+%5Cright%29%7D+%5Cright%5D%5Ckern-0.15em%5Cright%5D%7B%5Crm%7Bfor%5C+all%7D%7D%5C+n+%5Ctag%7B2%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} \\mathop {\\min }\\limits_{b,{\\rm{w}}} \\ &amp;\\frac{1}{2}{{\\rm{w}}^T}{\\rm{w}} + C \\cdot \\sum\\limits_{n = 1}^N {\\left[\\kern-0.15em\\left[ {{y_n} \\ne {\\rm{sign}}\\left( {{{\\rm{w}}^T}{{\\rm{z}}_n} + b} \\right)} \\right]\\kern-0.15em\\right]} \\\\ s.t.\\ \\ &amp;{y_n}\\left( {{{\\rm{w}}^T}{{\\rm{z}}_n} + b} \\right) \\ge 1 - \\infty \\left[\\kern-0.15em\\left[ {{y_n} \\ne {\\rm{sign}}\\left( {{{\\rm{w}}^T}{{\\rm{z}}_n} + b} \\right)} \\right]\\kern-0.15em\\right]{\\rm{for\\ all}}\\ n \\tag{2} \\end{align*}\" eeimg=\"1\"/> </p><p>公式 (2) 仍然存在两个问题。一、含有非线性项，无法采用 QP 求解；二、对于犯错误的点，有的离边界很近，即 error 小，而有的离边界很远，error 很大，上式的条件和目标没有区分small error 和 large error。我们引入 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Cxi+_n%7D%5C%5D\" alt=\"\\[{\\xi _n}\\]\" eeimg=\"1\"/> 作为惩罚项，表示每个点犯错程度。 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Cxi+_n%7D+%3D+0%5C%5D\" alt=\"\\[{\\xi _n} = 0\\]\" eeimg=\"1\"/> 表示正确分类； <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Cxi+_n%7D%5C%5D\" alt=\"\\[{\\xi _n}\\]\" eeimg=\"1\"/> 越大表示犯错越严重，即离边界越大；</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-2ee02c7432f2d0b72b7f38640e70264d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"915\" data-rawheight=\"280\" class=\"origin_image zh-lightbox-thumb\" width=\"915\" data-original=\"https://pic2.zhimg.com/v2-2ee02c7432f2d0b72b7f38640e70264d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;915&#39; height=&#39;280&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"915\" data-rawheight=\"280\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"915\" data-original=\"https://pic2.zhimg.com/v2-2ee02c7432f2d0b72b7f38640e70264d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-2ee02c7432f2d0b72b7f38640e70264d_b.jpg\"/></figure><p>可以采用 QP 求解 Soft-Margin SVM， <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Ctilde+d+%2B+N+%2B+1%5C%5D\" alt=\"\\[\\tilde d + N + 1\\]\" eeimg=\"1\"/> 个参数， <img src=\"https://www.zhihu.com/equation?tex=2N\" alt=\"2N\" eeimg=\"1\"/> 个约束项。</p><h2><b>二、对偶问题</b></h2><p>类似Hard Margin，原问题依赖 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Ctilde+d%7D%5C%5D\" alt=\"\\[{\\tilde d}\\]\" eeimg=\"1\"/> ，同样把原问题转化为对偶问题求解。</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%26%5Cmathop+%7B%5Cmax+%7D%5Climits_%7B%5Calpha+%2C%5Cbeta+%7D+%5Cmathop+%7B%5Cmin+%7D%5Climits_%7Bb%2C%7B%5Crm%7Bw%7D%7D%2C%5Cxi+%7D+%5C+%5Cleft%28+%7B%5Cfrac%7B1%7D%7B2%7D%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%5Crm%7Bw%7D%7D+%2B+C+%5Ccdot+%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%7B%5Cxi+_n%7D%7D+%2B+%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%7B%5Calpha+_n%7D%5Cleft%28+%7B1+-+%7B%5Cxi+_n%7D+-+%7By_n%7D%5Cleft%28+%7B%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%7B%5Crm%7Bz%7D%7D_n%7D+%2B+b%7D+%5Cright%29%7D+%5Cright%29%7D+%2B+%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B+-+%7B%5Cbeta+_n%7D%7B%5Cxi+_n%7D%7D+%7D+%5Cright%29%5C%5C+%26s.t.%5C+%7B%5Calpha+_n%7D+%5Cge+0%2C%5C+%7B%5Cbeta+_n%7D+%5Cge+0%2C%7B%5Crm%7Bfor%5C+all%7D%7D%5C+n+%5Ctag%7B3%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} &amp;\\mathop {\\max }\\limits_{\\alpha ,\\beta } \\mathop {\\min }\\limits_{b,{\\rm{w}},\\xi } \\ \\left( {\\frac{1}{2}{{\\rm{w}}^T}{\\rm{w}} + C \\cdot \\sum\\limits_{n = 1}^N {{\\xi _n}} + \\sum\\limits_{n = 1}^N {{\\alpha _n}\\left( {1 - {\\xi _n} - {y_n}\\left( {{{\\rm{w}}^T}{{\\rm{z}}_n} + b} \\right)} \\right)} + \\sum\\limits_{n = 1}^N { - {\\beta _n}{\\xi _n}} } \\right)\\\\ &amp;s.t.\\ {\\alpha _n} \\ge 0,\\ {\\beta _n} \\ge 0,{\\rm{for\\ all}}\\ n \\tag{3} \\end{align*}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cbegin%7Barray%7D%7Bl%7D+L%5Cleft%28+%7Bb%2C%7B%5Crm%7Bw%7D%7D%2C%5Cxi+%7D+%5Cright%29%5Cmathop+%7B+%3D+%5Cmax+%7D%5Climits_%7B%5Calpha+%2C%5Cbeta+%7D+%5Cmathop+%7B%5Cmin+%7D%5Climits_%7Bb%2C%7B%5Crm%7Bw%7D%7D%2C%5Cxi+%7D+%5Cleft%28+%7B%5Cfrac%7B1%7D%7B2%7D%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%5Crm%7Bw%7D%7D+%2B+C+%5Ccdot+%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%7B%5Cxi+_n%7D%7D+%2B+%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%7B%5Calpha+_n%7D%5Cleft%28+%7B1+-+%7B%5Cxi+_n%7D+-+%7By_n%7D%5Cleft%28+%7B%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%7B%5Crm%7Bz%7D%7D_n%7D+%2B+b%7D+%5Cright%29%7D+%5Cright%29%7D+%2B+%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B+-+%7B%5Cbeta+_n%7D%7B%5Cxi+_n%7D%7D+%7D+%5Cright%29%5C%5C+%5Cfrac%7B%7B%5Cpartial+L%7D%7D%7B%7B%5Cpartial+b%7D%7D+%3D+-+%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%7B%5Calpha+_n%7D%7By_n%7D%7D+%3D+0+%5CRightarrow+%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%7B%5Calpha+_n%7D%7By_n%7D%7D+%3D+0%5C%5C+%5Cfrac%7B%7B%5Cpartial+L%7D%7D%7B%7B%5Cpartial+%7B%5Crm%7Bw%7D%7D%7D%7D+%3D+%7B%5Crm%7Bw%7D%7D+-+%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%7B%5Calpha+_n%7D%7By_n%7D%7B%7B%5Crm%7Bz%7D%7D_n%7D%7D+%3D+0+%5CRightarrow+%7B%5Crm%7Bw%7D%7D+%3D+%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%7B%5Calpha+_n%7D%7By_n%7D%7B%7B%5Crm%7Bz%7D%7D_n%7D%7D+%5C%5C+%5Cfrac%7B%7B%5Cpartial+L%7D%7D%7B%7B%5Cpartial+%7B%5Cxi+_n%7D%7D%7D+%3D+C+-+%7B%5Calpha+_n%7D+-+%7B%5Cbeta+_n%7D+%3D+0+%5CRightarrow+%7B%5Cbeta+_n%7D+%3D+C+-+%7B%5Calpha+_n%7D+%5Ctag%7B4%7D+%5Cend%7Barray%7D%5C%5D\" alt=\"\\[\\begin{array}{l} L\\left( {b,{\\rm{w}},\\xi } \\right)\\mathop { = \\max }\\limits_{\\alpha ,\\beta } \\mathop {\\min }\\limits_{b,{\\rm{w}},\\xi } \\left( {\\frac{1}{2}{{\\rm{w}}^T}{\\rm{w}} + C \\cdot \\sum\\limits_{n = 1}^N {{\\xi _n}} + \\sum\\limits_{n = 1}^N {{\\alpha _n}\\left( {1 - {\\xi _n} - {y_n}\\left( {{{\\rm{w}}^T}{{\\rm{z}}_n} + b} \\right)} \\right)} + \\sum\\limits_{n = 1}^N { - {\\beta _n}{\\xi _n}} } \\right)\\\\ \\frac{{\\partial L}}{{\\partial b}} = - \\sum\\limits_{n = 1}^N {{\\alpha _n}{y_n}} = 0 \\Rightarrow \\sum\\limits_{n = 1}^N {{\\alpha _n}{y_n}} = 0\\\\ \\frac{{\\partial L}}{{\\partial {\\rm{w}}}} = {\\rm{w}} - \\sum\\limits_{n = 1}^N {{\\alpha _n}{y_n}{{\\rm{z}}_n}} = 0 \\Rightarrow {\\rm{w}} = \\sum\\limits_{n = 1}^N {{\\alpha _n}{y_n}{{\\rm{z}}_n}} \\\\ \\frac{{\\partial L}}{{\\partial {\\xi _n}}} = C - {\\alpha _n} - {\\beta _n} = 0 \\Rightarrow {\\beta _n} = C - {\\alpha _n} \\tag{4} \\end{array}\\]\" eeimg=\"1\"/></p><p>将公式 (4) 得到的结果带入公式 (3)，可得对偶问题：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cbegin%7Barray%7D%7Bl%7D+%5Cmathop+%7B%5Cmin+%7D%5Climits_%5Calpha+%5Cfrac%7B1%7D%7B2%7D%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%5Csum%5Climits_%7Bm+%3D+1%7D%5EN+%7B%7B%5Calpha+_n%7D%7B%5Calpha+_m%7D%7By_n%7D%7By_m%7D%7B%5Crm%7Bz%7D%7D_n%5ET%7B%7B%5Crm%7Bz%7D%7D_m%7D%7D+%7D+-+%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%7B%5Calpha+_n%7D%7D+%5C%5C+s.t.%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%7B%5Calpha+_n%7D%7By_n%7D%7D+%3D+0%5C%5C+0+%5Cle+%7B%5Calpha+_n%7D+%5Cle+C%2Cn+%3D+1%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2CN+%5Ctag%7B5%7D+%5Cend%7Barray%7D%5C%5D\" alt=\"\\[\\begin{array}{l} \\mathop {\\min }\\limits_\\alpha \\frac{1}{2}\\sum\\limits_{n = 1}^N {\\sum\\limits_{m = 1}^N {{\\alpha _n}{\\alpha _m}{y_n}{y_m}{\\rm{z}}_n^T{{\\rm{z}}_m}} } - \\sum\\limits_{n = 1}^N {{\\alpha _n}} \\\\ s.t.\\sum\\limits_{n = 1}^N {{\\alpha _n}{y_n}} = 0\\\\ 0 \\le {\\alpha _n} \\le C,n = 1, \\cdot \\cdot \\cdot ,N \\tag{5} \\end{array}\\]\" eeimg=\"1\"/></p><p>和 <a href=\"https://zhuanlan.zhihu.com/p/43043846\" class=\"internal\">Hard Margin</a>中对偶问题比较，仅仅约束条件中 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Calpha+_n%7D%5C%5D\" alt=\"\\[{\\alpha _n}\\]\" eeimg=\"1\"/> 多了上限 <img src=\"https://www.zhihu.com/equation?tex=C\" alt=\"C\" eeimg=\"1\"/> 。Soft Margin 对偶问题有 <img src=\"https://www.zhihu.com/equation?tex=N\" alt=\"N\" eeimg=\"1\"/> 个参数和 <img src=\"https://www.zhihu.com/equation?tex=2N%2B1\" alt=\"2N+1\" eeimg=\"1\"/> 个约束项，同样<a href=\"https://zhuanlan.zhihu.com/p/43130548\" class=\"internal\">Kernel</a>也适用Soft Margin。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-7e877b0a02eaa3bc6f0ac468cdd7c939_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"591\" data-rawheight=\"242\" class=\"origin_image zh-lightbox-thumb\" width=\"591\" data-original=\"https://pic2.zhimg.com/v2-7e877b0a02eaa3bc6f0ac468cdd7c939_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;591&#39; height=&#39;242&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"591\" data-rawheight=\"242\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"591\" data-original=\"https://pic2.zhimg.com/v2-7e877b0a02eaa3bc6f0ac468cdd7c939_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-7e877b0a02eaa3bc6f0ac468cdd7c939_b.jpg\"/></figure><p>当通过QP求出 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Calpha+%5C%5D\" alt=\"\\[\\alpha \\]\" eeimg=\"1\"/> ，我们可以求出 <img src=\"https://www.zhihu.com/equation?tex=b\" alt=\"b\" eeimg=\"1\"/> 。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-dec16a2ab77d850d5281c017431f7b6b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"665\" data-rawheight=\"380\" class=\"origin_image zh-lightbox-thumb\" width=\"665\" data-original=\"https://pic4.zhimg.com/v2-dec16a2ab77d850d5281c017431f7b6b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;665&#39; height=&#39;380&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"665\" data-rawheight=\"380\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"665\" data-original=\"https://pic4.zhimg.com/v2-dec16a2ab77d850d5281c017431f7b6b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-dec16a2ab77d850d5281c017431f7b6b_b.jpg\"/></figure><h2><b>三、总结</b></h2><p>样本点可以分为三类，如图 4.2 所示。</p><ul><li><img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Calpha+_n%7D+%3D+0%5C%5D\" alt=\"\\[{\\alpha _n} = 0\\]\" eeimg=\"1\"/> ：可得 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Cxi+_n%7D+%3D+0%5C%5D\" alt=\"\\[{\\xi _n} = 0\\]\" eeimg=\"1\"/> ，表明该类样本点没有犯错，在margin之外或者margin边界上。</li><li><img src=\"https://www.zhihu.com/equation?tex=%5C%5B0+%3C+%7B%5Calpha+_n%7D+%3C+C%5C%5D\" alt=\"\\[0 &lt; {\\alpha _n} &lt; C\\]\" eeimg=\"1\"/> ：可得 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Cxi+_n%7D+%3D+0%2C%7By_n%7D%5Cleft%28+%7B%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%7B%5Crm%7Bz%7D%7D_n%7D+%2B+b%7D+%5Cright%29%7B%5Crm%7B+%3D+%7D%7D1%5C%5D\" alt=\"\\[{\\xi _n} = 0,{y_n}\\left( {{{\\rm{w}}^T}{{\\rm{z}}_n} + b} \\right){\\rm{ = }}1\\]\" eeimg=\"1\"/> ，该类点没有犯错，在margin边界上，可以用来确定 <img src=\"https://www.zhihu.com/equation?tex=b\" alt=\"b\" eeimg=\"1\"/> 的值。</li><li><img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Calpha+_n%7D+%3D+C%5C%5D\" alt=\"\\[{\\alpha _n} = C\\]\" eeimg=\"1\"/> ： <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Cxi+_n%7D%7B%5Crm%7B+%3D+%7D%7D1+-+%7By_n%7D%5Cleft%28+%7B%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%7B%5Crm%7Bz%7D%7D_n%7D+%2B+b%7D+%5Cright%29%5C%5D\" alt=\"\\[{\\xi _n}{\\rm{ = }}1 - {y_n}\\left( {{{\\rm{w}}^T}{{\\rm{z}}_n} + b} \\right)\\]\" eeimg=\"1\"/> ，不确定 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Cxi+_n%7D%5C%5D\" alt=\"\\[{\\xi _n}\\]\" eeimg=\"1\"/> 是否为0。当 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Cxi+_n%7D%7B%5Crm%7B+%3D+0%7D%7D%5C%5D\" alt=\"\\[{\\xi _n}{\\rm{ = 0}}\\]\" eeimg=\"1\"/> ，在margin边界上； <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Cxi+_n%7D%5C%5D\" alt=\"\\[{\\xi _n}\\]\" eeimg=\"1\"/> 越大，犯错越大。</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-620eb2884f29a11d90181c6a7be3bd51_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"703\" data-rawheight=\"228\" class=\"origin_image zh-lightbox-thumb\" width=\"703\" data-original=\"https://pic2.zhimg.com/v2-620eb2884f29a11d90181c6a7be3bd51_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;703&#39; height=&#39;228&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"703\" data-rawheight=\"228\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"703\" data-original=\"https://pic2.zhimg.com/v2-620eb2884f29a11d90181c6a7be3bd51_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-620eb2884f29a11d90181c6a7be3bd51_b.jpg\"/></figure><p><a href=\"https://zhuanlan.zhihu.com/p/43130548\" class=\"internal\">上一篇</a></p>", 
            "topic": [
                {
                    "tag": "SVM", 
                    "tagLink": "https://api.zhihu.com/topics/19583524"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/43130548", 
            "userName": "徽州风韵", 
            "userLink": "https://www.zhihu.com/people/53bd6f8b93873b03c5edb714aea2a456", 
            "upvote": 1, 
            "title": "SVM-Kernel", 
            "content": "<h2><b>一、引言</b></h2><p>回归上一节内容，找出依赖 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Ctilde+d%7D%5C%5D\" alt=\"\\[{\\tilde d}\\]\" eeimg=\"1\"/>的计算，如公式 (1) 所示。</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%7B%5Crm%7Bw%7D%7D+%26%3D+%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%7B%5Calpha+_n%7D%7By_n%7D%7B%7B%5Crm%7Bz%7D%7D_n%7D%7D+%5C%5C+%7Bq_%7Bn%2Cm%7D%7D+%26%3D+%7By_n%7D%7By_m%7D%7B%5Crm%7Bz%7D%7D_n%5ET%7B%7B%5Crm%7Bz%7D%7D_m%7D+%5C%5C+%26%3D+%7By_n%7D%7By_m%7DK%5Cleft%28+%7B%7B%7B%5Crm%7Bx%7D%7D_n%7D%2C%7B%7B%5Crm%7Bx%7D%7D_m%7D%7D+%5Cright%29%5C%5C+b+%26%3D+%7By_s%7D+-+%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%7B%5Crm%7Bz%7D%7D_s%7D+%5C%5C+%26%3D+%7By_s%7D+-+%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%7B%5Calpha+_n%7D%7By_n%7D%7B%5Crm%7Bz%7D%7D_n%5ET%7B%7B%5Crm%7Bz%7D%7D_s%7D%7D+%5C%5C+%26%3D+%7By_s%7D+-+%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%7B%5Calpha+_n%7D%7By_n%7DK%5Cleft%28+%7B%7B%7B%5Crm%7Bx%7D%7D_n%7D%2C%7B%7B%5Crm%7Bx%7D%7D_s%7D%7D+%5Cright%29%7D+%5C%5C+%7Bg_%7B%7B%5Crm%7BSVM%7D%7D%7D%7D%5Cleft%28+%7B%5Crm%7Bx%7D%7D+%5Cright%29+%26%3D+%7B%5Crm%7Bsign%7D%7D%5Cleft%28+%7B%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%5Crm%7Bz%7D%7D+%2B+b%7D+%5Cright%29+%5C%5C+%26%3D+%7B%5Crm%7Bsign%7D%7D%5Cleft%28+%7B%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%7B%5Calpha+_n%7D%7By_n%7D%7B%5Crm%7Bz%7D%7D_n%5ET%7B%5Crm%7Bz%7D%7D%7D+%2B+b%7D+%5Cright%29+%5C%5C+%26%3D+%7B%5Crm%7Bsign%7D%7D%5Cleft%28+%7B%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%7B%5Calpha+_n%7D%7By_n%7DK%5Cleft%28+%7B%7B%7B%5Crm%7Bx%7D%7D_n%7D%2C%7B%5Crm%7Bx%7D%7D%7D+%5Cright%29%7D+%2B+b%7D+%5Cright%29+%5Ctag%7B1%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} {\\rm{w}} &amp;= \\sum\\limits_{n = 1}^N {{\\alpha _n}{y_n}{{\\rm{z}}_n}} \\\\ {q_{n,m}} &amp;= {y_n}{y_m}{\\rm{z}}_n^T{{\\rm{z}}_m} \\\\ &amp;= {y_n}{y_m}K\\left( {{{\\rm{x}}_n},{{\\rm{x}}_m}} \\right)\\\\ b &amp;= {y_s} - {{\\rm{w}}^T}{{\\rm{z}}_s} \\\\ &amp;= {y_s} - \\sum\\limits_{n = 1}^N {{\\alpha _n}{y_n}{\\rm{z}}_n^T{{\\rm{z}}_s}} \\\\ &amp;= {y_s} - \\sum\\limits_{n = 1}^N {{\\alpha _n}{y_n}K\\left( {{{\\rm{x}}_n},{{\\rm{x}}_s}} \\right)} \\\\ {g_{{\\rm{SVM}}}}\\left( {\\rm{x}} \\right) &amp;= {\\rm{sign}}\\left( {{{\\rm{w}}^T}{\\rm{z}} + b} \\right) \\\\ &amp;= {\\rm{sign}}\\left( {\\sum\\limits_{n = 1}^N {{\\alpha _n}{y_n}{\\rm{z}}_n^T{\\rm{z}}} + b} \\right) \\\\ &amp;= {\\rm{sign}}\\left( {\\sum\\limits_{n = 1}^N {{\\alpha _n}{y_n}K\\left( {{{\\rm{x}}_n},{\\rm{x}}} \\right)} + b} \\right) \\tag{1} \\end{align*}\" eeimg=\"1\"/> </p><p>从公式 (1) 中可以看出，如果我们能找到核函数 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BK_%5CPhi+%7D%5Cleft%28+%7B%7B%5Crm%7Bx%7D%7D%2C%7B%5Crm%7Bx%27%7D%7D%7D+%5Cright%29+%3D+%5CPhi+%7B%5Cleft%28+%7B%5Crm%7Bx%7D%7D+%5Cright%29%5ET%7D%5CPhi+%5Cleft%28+%7B%7B%5Crm%7Bx%27%7D%7D%7D+%5Cright%29%5C%5D\" alt=\"\\[{K_\\Phi }\\left( {{\\rm{x}},{\\rm{x&#39;}}} \\right) = \\Phi {\\left( {\\rm{x}} \\right)^T}\\Phi \\left( {{\\rm{x&#39;}}} \\right)\\]\" eeimg=\"1\"/> ，就可以避免将样本 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7Bx%7D%7D+%5Cin+%7BR%5Ed%7D%5C%5D\" alt=\"\\[{\\rm{x}} \\in {R^d}\\]\" eeimg=\"1\"/> 映射到高纬空间 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7Bz%7D%7D+%5Cin+%7BR%5E%7B%5Ctilde+d%7D%7D%5C%5D\" alt=\"\\[{\\rm{z}} \\in {R^{\\tilde d}}\\]\" eeimg=\"1\"/> 中做内积运算，也就是解决对 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Ctilde+d%7D%5C%5D\" alt=\"\\[{\\tilde d}\\]\" eeimg=\"1\"/> 的依赖问题。另外从公式 (1) 中还可以看出，我们无需直接求出 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7Bw%7D%7D%5C%5D\" alt=\"\\[{\\rm{w}}\\]\" eeimg=\"1\"/> ， <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bg_%7B%7B%5Crm%7BSVM%7D%7D%7D%7D%5C%5D\" alt=\"\\[{g_{{\\rm{SVM}}}}\\]\" eeimg=\"1\"/> 可以用对偶中求出来的 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Calpha+%5C%5D\" alt=\"\\[\\alpha \\]\" eeimg=\"1\"/> 和核函数表示。最终的优化问题同样可以用二次规划求解。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-793a6602493fcac7b342e350421cae89_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"910\" data-rawheight=\"611\" class=\"origin_image zh-lightbox-thumb\" width=\"910\" data-original=\"https://pic2.zhimg.com/v2-793a6602493fcac7b342e350421cae89_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;910&#39; height=&#39;611&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"910\" data-rawheight=\"611\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"910\" data-original=\"https://pic2.zhimg.com/v2-793a6602493fcac7b342e350421cae89_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-793a6602493fcac7b342e350421cae89_b.jpg\"/></figure><h2><b>二、多项式核函数</b></h2><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5CPhi+_2%7D%5Cleft%28+%7B%5Crm%7Bx%7D%7D+%5Cright%29+%3D+%5Cleft%28+%7B1%2C%5Csqrt+%7B2%5Cgamma+%7D+%7Bx_1%7D%2C%5Csqrt+%7B2%5Cgamma+%7D+%7Bx_2%7D%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2C%5Csqrt+%7B2%5Cgamma+%7D+%7Bx_d%7D%2C%5Cgamma+%7Bx_1%7D%5E2%2C%5Cgamma+%7Bx_1%7D%7Bx_2%7D%2C+%5Ccdot+%5Ccdot+%5Ccdot+%5Cgamma+%7Bx_1%7D%7Bx_d%7D%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2C%5Cgamma+%7Bx_d%7D%5E2%7D+%5Cright%29%5C%5D\" alt=\"\\[{\\Phi _2}\\left( {\\rm{x}} \\right) = \\left( {1,\\sqrt {2\\gamma } {x_1},\\sqrt {2\\gamma } {x_2}, \\cdot \\cdot \\cdot ,\\sqrt {2\\gamma } {x_d},\\gamma {x_1}^2,\\gamma {x_1}{x_2}, \\cdot \\cdot \\cdot \\gamma {x_1}{x_d}, \\cdot \\cdot \\cdot ,\\gamma {x_d}^2} \\right)\\]\" eeimg=\"1\"/> ，其中 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cgamma+%3E+0%5C%5D\" alt=\"\\[\\gamma &gt; 0\\]\" eeimg=\"1\"/> 。将样本从 <img src=\"https://www.zhihu.com/equation?tex=%5C%5Bd%5C%5D\" alt=\"\\[d\\]\" eeimg=\"1\"/> 维的空间映射到 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bd%5E2%7D+%2B+1%5C%5D\" alt=\"\\[{d^2} + 1\\]\" eeimg=\"1\"/> 维空间。假设原空间是 <img src=\"https://www.zhihu.com/equation?tex=1000\" alt=\"1000\" eeimg=\"1\"/> 维，投影后的空间高达 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B10%5E6%7D%5C%5D\" alt=\"\\[{10^6}\\]\" eeimg=\"1\"/> 维，如果是 <img src=\"https://www.zhihu.com/equation?tex=10\" alt=\"10\" eeimg=\"1\"/> 次甚至更高次多项式，可想而知映射后的维度高的惊人，出现<b>维度灾难</b>，在映射后的空间无论是计算还是数据储存都是很大的问题。</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+K%5Cleft%28+%7B%7B%5Crm%7Bx%7D%7D%2C%7B%5Crm%7Bx%27%7D%7D%7D+%5Cright%29+%26%3D+%7B%5CPhi+_2%7D%7B%5Cleft%28+%7B%5Crm%7Bx%7D%7D+%5Cright%29%5ET%7D%7B%5CPhi+_2%7D%5Cleft%28+%7B%7B%5Crm%7Bx%27%7D%7D%7D+%5Cright%29%5C%5C+%26%3D+1+%2B+%7B2%5Cgamma+%7D%5Csum%5Climits_%7Bi+%3D+1%7D%5Ed+%7B%7Bx_i%7D%7B%7Bx%27%7D_i%7D%7D+%2B+%7B%7B%5Cgamma+%5E2%7D%7D%5Csum%5Climits_%7Bi+%3D+1%7D%5Ed+%7B%5Csum%5Climits_%7Bj+%3D+1%7D%5Ed+%7B%7Bx_i%7D%7Bx_j%7D%7B%7Bx%27%7D_i%7D%7B%7Bx%27%7D_j%7D%7D+%7D+%5C%5C+%26%3D+1+%2B+%7B2%5Cgamma+%7D%5Csum%5Climits_%7Bi+%3D+1%7D%5Ed+%7B%7Bx_i%7D%7B%7Bx%27%7D_i%7D%7D+%2B+%7B%7B%5Cgamma+%5E2%7D%7D%5Csum%5Climits_%7Bi+%3D+1%7D%5Ed+%7B%7Bx_i%7D%7B%7Bx%27%7D_i%7D%7D+%5Csum%5Climits_%7Bj+%3D+1%7D%5Ed+%7B%7Bx_j%7D%7B%7Bx%27%7D_j%7D%7D+%5C%5C+%26%3D+1+%2B+%7B2%5Cgamma+%7D%7B%7B%5Crm%7Bx%7D%7D%5ET%7D%7B%5Crm%7Bx%27+%2B+%7D%7D%7B%7B%7B%5Cgamma+%5E2%7D%7D%5Cleft%28+%7B%7B%7B%5Crm%7Bx%7D%7D%5ET%7D%7B%5Crm%7Bx%27%7D%7D%7D+%5Cright%29%5E2%7D%5C%5C+%26%3D%7B%5Cleft%28+%7B1+%2B+%5Cgamma+%7B%7B%5Crm%7Bx%7D%7D%5ET%7D%7B%5Crm%7Bx%27%7D%7D%7D+%5Cright%29%5E2%7D+%5Ctag%7B2%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} K\\left( {{\\rm{x}},{\\rm{x&#39;}}} \\right) &amp;= {\\Phi _2}{\\left( {\\rm{x}} \\right)^T}{\\Phi _2}\\left( {{\\rm{x&#39;}}} \\right)\\\\ &amp;= 1 + {2\\gamma }\\sum\\limits_{i = 1}^d {{x_i}{{x&#39;}_i}} + {{\\gamma ^2}}\\sum\\limits_{i = 1}^d {\\sum\\limits_{j = 1}^d {{x_i}{x_j}{{x&#39;}_i}{{x&#39;}_j}} } \\\\ &amp;= 1 + {2\\gamma }\\sum\\limits_{i = 1}^d {{x_i}{{x&#39;}_i}} + {{\\gamma ^2}}\\sum\\limits_{i = 1}^d {{x_i}{{x&#39;}_i}} \\sum\\limits_{j = 1}^d {{x_j}{{x&#39;}_j}} \\\\ &amp;= 1 + {2\\gamma }{{\\rm{x}}^T}{\\rm{x&#39; + }}{{{\\gamma ^2}}\\left( {{{\\rm{x}}^T}{\\rm{x&#39;}}} \\right)^2}\\\\ &amp;={\\left( {1 + \\gamma {{\\rm{x}}^T}{\\rm{x&#39;}}} \\right)^2} \\tag{2} \\end{align*}\" eeimg=\"1\"/> </p><p>结合公式 (1) 和 公式(2) ，我们发现无需把 <img src=\"https://www.zhihu.com/equation?tex=%5Crm%7Bx%7D\" alt=\"\\rm{x}\" eeimg=\"1\"/> 映射到高维空间做内积运算，可以直接在原空间做内积运算。复杂度由原来的 <img src=\"https://www.zhihu.com/equation?tex=O%28d%5E%7B2%7D%29\" alt=\"O(d^{2})\" eeimg=\"1\"/> 变为 <img src=\"https://www.zhihu.com/equation?tex=O%28d%29\" alt=\"O(d)\" eeimg=\"1\"/> 。我们可以得到更一般的多项式核函数：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%7BK_Q%7D%5Cleft%28+%7B%7B%5Crm%7Bx%7D%7D%2C%7B%5Crm%7Bx%27%7D%7D%7D+%5Cright%29+%3D+%7B%5Cleft%28+%7B%5Czeta+%2B+%5Cgamma+%7B%7B%5Crm%7Bx%7D%7D%5ET%7D%7B%5Crm%7Bx%27%7D%7D%7D+%5Cright%29%5EQ%7D%5C+%5C+%7B%5Crm%7Bwith%7D%7D%5C+%5Cgamma+%3E+0%2C%5Czeta+%5Cge+0+%5Ctag%7B3%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} {K_Q}\\left( {{\\rm{x}},{\\rm{x&#39;}}} \\right) = {\\left( {\\zeta + \\gamma {{\\rm{x}}^T}{\\rm{x&#39;}}} \\right)^Q}\\ \\ {\\rm{with}}\\ \\gamma &gt; 0,\\zeta \\ge 0 \\tag{3} \\end{align*}\" eeimg=\"1\"/> </p><h2><b>三、高斯核函数</b></h2><p>前面介绍的多项式核函数映射后的空间是有限维的，是否存在核函数，使得投影后的空间是无限维的。假设 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7Bx%7D%7D%5C%5D\" alt=\"\\[{\\rm{x}}\\]\" eeimg=\"1\"/> 退化为一维<img src=\"https://www.zhihu.com/equation?tex=x\" alt=\"x\" eeimg=\"1\"/> ，由公式 (4)</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+K%5Cleft%28+%7Bx%2Cx%27%7D+%5Cright%29+%26%3D+%5Cexp+%5Cleft%28+%7B+-+%7B%7B%5Cleft%28+%7Bx+-+x%27%7D+%5Cright%29%7D%5E2%7D%7D+%5Cright%29%5C%5C+%26%3D+%5Cexp+%5Cleft%28+%7B+-+%7Bx%5E2%7D%7D+%5Cright%29%5Cexp+%5Cleft%28+%7B+-+%7B%7B%5Cleft%28+%7Bx%27%7D+%5Cright%29%7D%5E2%7D%7D+%5Cright%29%5Cexp+%5Cleft%28+%7B2xx%27%7D+%5Cright%29%5C%5C+%26%5Cmathop+%3D+%5Climits%5E%7B%7B%5Crm%7BTaylor%7D%7D%7D+%5Cexp+%5Cleft%28+%7B+-+%7Bx%5E2%7D%7D+%5Cright%29%5Cexp+%5Cleft%28+%7B+-+%7B%7B%5Cleft%28+%7Bx%27%7D+%5Cright%29%7D%5E2%7D%7D+%5Cright%29%5Csum%5Climits_%7Bi+%3D+0%7D%5E%5Cinfty+%7B%5Cfrac%7B%7B%7B%7B%5Cleft%28+%7B2xx%27%7D+%5Cright%29%7D%5Ei%7D%7D%7D%7B%7Bi%21%7D%7D%7D+%5C%5C+%26%3D+%5Csum%5Climits_%7Bi+%3D+0%7D%5E%5Cinfty+%7B%5Cexp+%5Cleft%28+%7B+-+%7Bx%5E2%7D%7D+%5Cright%29%5Cexp+%5Cleft%28+%7B+-+%7B%7B%5Cleft%28+%7Bx%27%7D+%5Cright%29%7D%5E2%7D%7D+%5Cright%29%5Csqrt+%7B%5Cfrac%7B%7B%7B2%5Ei%7D%7D%7D%7B%7Bi%21%7D%7D%7D+%5Csqrt+%7B%5Cfrac%7B%7B%7B2%5Ei%7D%7D%7D%7B%7Bi%21%7D%7D%7D+%7Bx%5Ei%7D%7B%7B%5Cleft%28+%7Bx%27%7D+%5Cright%29%7D%5Ei%7D%7D+%5C%5C+%26%3D+%5CPhi+%7B%5Cleft%28+x+%5Cright%29%5ET%7D%5CPhi+%5Cleft%28+%7Bx%27%7D+%5Cright%29+%5Ctag%7B4%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} K\\left( {x,x&#39;} \\right) &amp;= \\exp \\left( { - {{\\left( {x - x&#39;} \\right)}^2}} \\right)\\\\ &amp;= \\exp \\left( { - {x^2}} \\right)\\exp \\left( { - {{\\left( {x&#39;} \\right)}^2}} \\right)\\exp \\left( {2xx&#39;} \\right)\\\\ &amp;\\mathop = \\limits^{{\\rm{Taylor}}} \\exp \\left( { - {x^2}} \\right)\\exp \\left( { - {{\\left( {x&#39;} \\right)}^2}} \\right)\\sum\\limits_{i = 0}^\\infty {\\frac{{{{\\left( {2xx&#39;} \\right)}^i}}}{{i!}}} \\\\ &amp;= \\sum\\limits_{i = 0}^\\infty {\\exp \\left( { - {x^2}} \\right)\\exp \\left( { - {{\\left( {x&#39;} \\right)}^2}} \\right)\\sqrt {\\frac{{{2^i}}}{{i!}}} \\sqrt {\\frac{{{2^i}}}{{i!}}} {x^i}{{\\left( {x&#39;} \\right)}^i}} \\\\ &amp;= \\Phi {\\left( x \\right)^T}\\Phi \\left( {x&#39;} \\right) \\tag{4} \\end{align*}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5CPhi+%5Cleft%28+x+%5Cright%29+%3D+%5Cexp+%5Cleft%28+%7B+-+%7Bx%5E2%7D%7D+%5Cright%29%5Cleft%28+%7B1%2C%5Csqrt+%7B%5Cfrac%7B%7B%7B2%5E1%7D%7D%7D%7B%7B1%21%7D%7D%7D+x%2C%5Csqrt+%7B%5Cfrac%7B%7B%7B2%5E2%7D%7D%7D%7B%7B2%21%7D%7D%7D+%7Bx%5E2%7D%2C+%5Ccdot+%5Ccdot+%5Ccdot+%7D+%5Cright%29%5C%5D\" alt=\"\\[\\Phi \\left( x \\right) = \\exp \\left( { - {x^2}} \\right)\\left( {1,\\sqrt {\\frac{{{2^1}}}{{1!}}} x,\\sqrt {\\frac{{{2^2}}}{{2!}}} {x^2}, \\cdot \\cdot \\cdot } \\right)\\]\" eeimg=\"1\"/> ，样本 <img src=\"https://www.zhihu.com/equation?tex=x\" alt=\"x\" eeimg=\"1\"/> 经过 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5CPhi+%5C%5D\" alt=\"\\[\\Phi \\]\" eeimg=\"1\"/> 映射到无限维的空间中，更一般的高斯核函数。</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+K%5Cleft%28+%7B%7B%5Crm%7Bx%7D%7D%2C%7B%5Crm%7Bx%27%7D%7D%7D+%5Cright%29+%3D+%5Cexp+%5Cleft%28+%7B+-+%5Cgamma+%7B%7B%5Cleft%5C%7C+%7B%7B%5Crm%7Bx%7D%7D+-+%7B%5Crm%7Bx%27%7D%7D%7D+%5Cright%5C%7C%7D%5E2%7D%7D+%5Cright%29+%5Ctag%7B5%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} K\\left( {{\\rm{x}},{\\rm{x&#39;}}} \\right) = \\exp \\left( { - \\gamma {{\\left\\| {{\\rm{x}} - {\\rm{x&#39;}}} \\right\\|}^2}} \\right) \\tag{5} \\end{align*}\" eeimg=\"1\"/></p><h2><b>四、总结</b></h2><p>有效核函数可以用 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5CPhi+%7B%5Cleft%28+%7B%5Crm%7Bx%7D%7D+%5Cright%29%5ET%7D%5CPhi+%5Cleft%28+%7B%7B%5Crm%7Bx%27%7D%7D%7D+%5Cright%29%5C%5D\" alt=\"\\[\\Phi {\\left( {\\rm{x}} \\right)^T}\\Phi \\left( {{\\rm{x&#39;}}} \\right)\\]\" eeimg=\"1\"/> 形式表示，矩阵 <img src=\"https://www.zhihu.com/equation?tex=K\" alt=\"K\" eeimg=\"1\"/>（其中<img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7BK_%7Bij%7D%7D+%3D+K%5Cleft%28+%7B%7B%7B%5Crm%7Bx%7D%7D_i%7D%2C%7B%7B%5Crm%7Bx%7D%7D_j%7D%7D+%5Cright%29%5C%5D\" alt=\"\\[{K_{ij}} = K\\left( {{{\\rm{x}}_i},{{\\rm{x}}_j}} \\right)\\]\" eeimg=\"1\"/> ）是半正定矩阵。下面对前面介绍的核函数进行比较。</p><p><b>1. 多项式核函数</b></p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%7BK_Q%7D%5Cleft%28+%7B%7B%5Crm%7Bx%7D%7D%2C%7B%5Crm%7Bx%27%7D%7D%7D+%5Cright%29+%3D+%7B%5Cleft%28+%7B%5Czeta+%2B+%5Cgamma+%7B%7B%5Crm%7Bx%7D%7D%5ET%7D%7B%5Crm%7Bx%27%7D%7D%7D+%5Cright%29%5EQ%7D%5C+%5C+%7B%5Crm%7Bwith%7D%7D%5C+%5Cgamma+%3E+0%2C%5Czeta+%5Cge+0+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} {K_Q}\\left( {{\\rm{x}},{\\rm{x&#39;}}} \\right) = {\\left( {\\zeta + \\gamma {{\\rm{x}}^T}{\\rm{x&#39;}}} \\right)^Q}\\ \\ {\\rm{with}}\\ \\gamma &gt; 0,\\zeta \\ge 0 \\end{align*}\" eeimg=\"1\"/> </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-d2013a8d5ef22728d545bb2037732822_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"649\" data-rawheight=\"266\" class=\"origin_image zh-lightbox-thumb\" width=\"649\" data-original=\"https://pic3.zhimg.com/v2-d2013a8d5ef22728d545bb2037732822_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;649&#39; height=&#39;266&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"649\" data-rawheight=\"266\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"649\" data-original=\"https://pic3.zhimg.com/v2-d2013a8d5ef22728d545bb2037732822_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-d2013a8d5ef22728d545bb2037732822_b.jpg\"/></figure><p><b>2. 高斯核函数</b></p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+K%5Cleft%28+%7B%7B%5Crm%7Bx%7D%7D%2C%7B%5Crm%7Bx%27%7D%7D%7D+%5Cright%29+%3D+%5Cexp+%5Cleft%28+%7B+-+%5Cgamma+%7B%7B%5Cleft%5C%7C+%7B%7B%5Crm%7Bx%7D%7D+-+%7B%5Crm%7Bx%27%7D%7D%7D+%5Cright%5C%7C%7D%5E2%7D%7D+%5Cright%29+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} K\\left( {{\\rm{x}},{\\rm{x&#39;}}} \\right) = \\exp \\left( { - \\gamma {{\\left\\| {{\\rm{x}} - {\\rm{x&#39;}}} \\right\\|}^2}} \\right) \\end{align*}\" eeimg=\"1\"/> </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-5039a3657e961e970f9e0c58508fe0a2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"665\" data-rawheight=\"293\" class=\"origin_image zh-lightbox-thumb\" width=\"665\" data-original=\"https://pic3.zhimg.com/v2-5039a3657e961e970f9e0c58508fe0a2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;665&#39; height=&#39;293&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"665\" data-rawheight=\"293\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"665\" data-original=\"https://pic3.zhimg.com/v2-5039a3657e961e970f9e0c58508fe0a2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-5039a3657e961e970f9e0c58508fe0a2_b.jpg\"/></figure><p>注释：</p><p>Cons：</p><ul><li>因为映射后的空间是无限维的，所以 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7Bw%7D%7D%5C%5D\" alt=\"\\[{\\rm{w}}\\]\" eeimg=\"1\"/> 也是无限维的，无法直接求解；</li><li>指数运算相对耗时；</li><li>模型过于强大，容易overfit。</li></ul><p><b>3. Representer Theorem</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-6f5d6911619d5c3e9efcba7e37f6495a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"919\" data-rawheight=\"632\" class=\"origin_image zh-lightbox-thumb\" width=\"919\" data-original=\"https://pic3.zhimg.com/v2-6f5d6911619d5c3e9efcba7e37f6495a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;919&#39; height=&#39;632&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"919\" data-rawheight=\"632\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"919\" data-original=\"https://pic3.zhimg.com/v2-6f5d6911619d5c3e9efcba7e37f6495a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-6f5d6911619d5c3e9efcba7e37f6495a_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-c9dd82d5a78e0bcffa67d312361a7170_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"922\" data-rawheight=\"662\" class=\"origin_image zh-lightbox-thumb\" width=\"922\" data-original=\"https://pic1.zhimg.com/v2-c9dd82d5a78e0bcffa67d312361a7170_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;922&#39; height=&#39;662&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"922\" data-rawheight=\"662\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"922\" data-original=\"https://pic1.zhimg.com/v2-c9dd82d5a78e0bcffa67d312361a7170_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-c9dd82d5a78e0bcffa67d312361a7170_b.jpg\"/></figure><p><a href=\"https://zhuanlan.zhihu.com/p/43043846\" class=\"internal\">上一篇</a> </p><p><a href=\"https://zhuanlan.zhihu.com/p/43169033\" class=\"internal\">下一篇</a></p>", 
            "topic": [
                {
                    "tag": "SVM", 
                    "tagLink": "https://api.zhihu.com/topics/19583524"
                }, 
                {
                    "tag": "kernel（核函数）", 
                    "tagLink": "https://api.zhihu.com/topics/19619891"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/43043846", 
            "userName": "徽州风韵", 
            "userLink": "https://www.zhihu.com/people/53bd6f8b93873b03c5edb714aea2a456", 
            "upvote": 0, 
            "title": "SVM-Hard Margin", 
            "content": "<h2><b>一、引言</b></h2><p><b>1. 问题</b></p><p>SVM是一种二分类模型，不仅将样本正确地分开，而且样本被尽可能宽的明显的间隔分开，如图 2.1 所示，图中三个分类器都可将样本正确地分开，第三个分类器中离超平面最近的样本的距离比前两个大，具有更强的鲁棒性。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-19f7687e82154ca16b0310ffea5c6afc_b.jpg\" data-size=\"normal\" data-rawwidth=\"636\" data-rawheight=\"192\" class=\"origin_image zh-lightbox-thumb\" width=\"636\" data-original=\"https://pic1.zhimg.com/v2-19f7687e82154ca16b0310ffea5c6afc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;636&#39; height=&#39;192&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"636\" data-rawheight=\"192\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"636\" data-original=\"https://pic1.zhimg.com/v2-19f7687e82154ca16b0310ffea5c6afc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-19f7687e82154ca16b0310ffea5c6afc_b.jpg\"/><figcaption>图 2.1</figcaption></figure><p>上面的问题可以用数学方式表示为：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Cmathop+%7B%5Cmax+%7D%5Climits_%5Crm%7Bw%7D+%5C+%5C+%5C+%26%7B%5Crm%7Bmargin%7D%7D%5Cleft%28+%5Crm%7Bw%7D+%5Cright%29%5C%5C+s.t.%5C+%5C+%5C+%26%5Crm%7Bw+%5C+classifiles+%5C+every%7D+%5C+%5Cleft%28+%7B%7B%7B%5Crm%7Bx%7D%7D_n%7D%2C%7By_n%7D%7D+%5Cright%29+%5C+correctly%5C%5C+%26%7B%5Crm%7Bmargin%7D%7D%5Cleft%28+%5Crm%7Bw%7D+%5Cright%29%7B%5Crm%7B+%3D+%7D%7D%5Cmathop+%7B%5Cmin+%7D%5Climits_%7Bn+%3D+1%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2CN%7D+%5Crm%7Bdistance%7D%5Cleft%28+%7B%7B%7B%5Crm%7Bx%7D%7D_n%7D%2Cw%7D+%5Cright%29+%5Ctag%7B1%7D+%5Cend%7Balign%2A%7D+\" alt=\"\\begin{align*} \\mathop {\\max }\\limits_\\rm{w} \\ \\ \\ &amp;{\\rm{margin}}\\left( \\rm{w} \\right)\\\\ s.t.\\ \\ \\ &amp;\\rm{w \\ classifiles \\ every} \\ \\left( {{{\\rm{x}}_n},{y_n}} \\right) \\ correctly\\\\ &amp;{\\rm{margin}}\\left( \\rm{w} \\right){\\rm{ = }}\\mathop {\\min }\\limits_{n = 1, \\cdot \\cdot \\cdot ,N} \\rm{distance}\\left( {{{\\rm{x}}_n},w} \\right) \\tag{1} \\end{align*} \" eeimg=\"1\"/> </p><p>所以我们的目标是寻找这个超平面 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%5Crm%7Bx%7D%7D+%2B+b%5C%5D\" alt=\"\\[{{\\rm{w}}^T}{\\rm{x}} + b\\]\" eeimg=\"1\"/> 。</p><p><b>2. 点到平面距离</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-8659c7ab93e428469a072eed6412fcd7_b.jpg\" data-size=\"normal\" data-rawwidth=\"306\" data-rawheight=\"288\" class=\"content_image\" width=\"306\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;306&#39; height=&#39;288&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"306\" data-rawheight=\"288\" class=\"content_image lazy\" width=\"306\" data-actualsrc=\"https://pic4.zhimg.com/v2-8659c7ab93e428469a072eed6412fcd7_b.jpg\"/><figcaption>图 2.2</figcaption></figure><p>求点 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7Bx%7D%7D%5C%5D\" alt=\"\\[{\\rm{x}}\\]\" eeimg=\"1\"/> 到超平面 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%5Crm%7Bx%7D%7D+%2B+b%5C%5D\" alt=\"\\[{{\\rm{w}}^T}{\\rm{x}} + b\\]\" eeimg=\"1\"/> 的距离，平面上任意两个点 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5Crm%7Bx%27%7D%7D%7D%5C%5D\" alt=\"\\[{{\\rm{x&#39;}}}\\]\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5Crm%7Bx%27%27%7D%7D%7D%5C%5D\" alt=\"\\[{{\\rm{x&#39;&#39;}}}\\]\" eeimg=\"1\"/> ，即 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%5Crm%7Bx%27+%2B+b+%3D+0%7D%7D%5C%5D\" alt=\"\\[{{\\rm{w}}^T}{\\rm{x&#39; + b = 0}}\\]\" eeimg=\"1\"/> ， <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%5Crm%7Bx%27%27+%2B+b+%3D+0%7D%7D%5C%5D\" alt=\"\\[{{\\rm{w}}^T}{\\rm{x&#39;&#39; + b = 0}}\\]\" eeimg=\"1\"/> 。我们可以得 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%5Crm%7Bx%27+%3D+%7D%7D+-+%7B%5Crm%7Bb%2C%7D%7D%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%5Crm%7Bx%27%27+%3D+%7D%7D+-+%7B%5Crm%7Bb%2C%7D%7D%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%5Cleft%28+%7B%7B%5Crm%7Bx%27%27%7D%7D+-+%7B%5Crm%7Bx%27%7D%7D%7D+%5Cright%29%7B%5Crm%7B+%3D+0%7D%7D%5C%5D\" alt=\"\\[{{\\rm{w}}^T}{\\rm{x&#39; = }} - {\\rm{b,}}{{\\rm{w}}^T}{\\rm{x&#39;&#39; = }} - {\\rm{b,}}{{\\rm{w}}^T}\\left( {{\\rm{x&#39;&#39;}} - {\\rm{x&#39;}}} \\right){\\rm{ = 0}}\\]\" eeimg=\"1\"/> 。</p><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7Bx%7D%7D%5C%5D\" alt=\"\\[{\\rm{x}}\\]\" eeimg=\"1\"/> 到超平面的距离为 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7Bx%7D%7D%5C%5D\" alt=\"\\[{\\rm{x}}\\]\" eeimg=\"1\"/> 到平面上任意点的距离在法向量 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7Bw%7D%7D%5C%5D\" alt=\"\\[{\\rm{w}}\\]\" eeimg=\"1\"/> 上的投影。所以可得点到平面距离为：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%7B%5Crm%7Bdistance%7D%7D%5Cleft%28+%7B%7B%5Crm%7Bx%7D%7D%2C%7B%5Crm%7Bw%7D%7D%2C%7B%5Crm%7Bb%7D%7D%7D+%5Cright%29+%3D+%5Cleft%7C+%7B%5Cfrac%7B%7B%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7D%7D%7B%7B%5Cleft%5C%7C+%7B%5Crm%7Bw%7D%7D+%5Cright%5C%7C%7D%7D%5Cleft%28+%7B%7B%5Crm%7Bx%7D%7D+-+%7B%5Crm%7Bx%27%7D%7D%7D+%5Cright%29%7D+%5Cright%7C+%3D+%5Cfrac%7B1%7D%7B%7B%5Cleft%5C%7C+%7B%5Crm%7Bw%7D%7D+%5Cright%5C%7C%7D%7D%5Cleft%7C+%7B%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%5Crm%7Bx%7D%7D+%2B+b%7D+%5Cright%7C+%5Ctag%7B2%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} {\\rm{distance}}\\left( {{\\rm{x}},{\\rm{w}},{\\rm{b}}} \\right) = \\left| {\\frac{{{{\\rm{w}}^T}}}{{\\left\\| {\\rm{w}} \\right\\|}}\\left( {{\\rm{x}} - {\\rm{x&#39;}}} \\right)} \\right| = \\frac{1}{{\\left\\| {\\rm{w}} \\right\\|}}\\left| {{{\\rm{w}}^T}{\\rm{x}} + b} \\right| \\tag{2} \\end{align*}\" eeimg=\"1\"/> </p><p>对于线性分类器</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+y+%3D+%5Crm%7Bsign%7D%5Cleft%28+%7B%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%5Crm%7Bx%7D%7D+%2B+b%7D+%5Cright%29+%3D+%5Cleft%5C%7B+%7B%5Cbegin%7Barray%7D%7B%2A%7B20%7D%7Bc%7D%7D+%7B+%2B+1%7D%26%7B%7B%5Crm%7Bcorrectly%7D%7D%7D%5C%5C+%7B+-+1%7D%26%7B%7B%5Crm%7Bincorrectly%7D%7D%7D+%5Cend%7Barray%7D%7D+%5Cright.+%5Ctag%7B3%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} y = \\rm{sign}\\left( {{{\\rm{w}}^T}{\\rm{x}} + b} \\right) = \\left\\{ {\\begin{array}{*{20}{c}} { + 1}&amp;{{\\rm{correctly}}}\\\\ { - 1}&amp;{{\\rm{incorrectly}}} \\end{array}} \\right. \\tag{3} \\end{align*}\" eeimg=\"1\"/> </p><p>在 SVM Hard Margin中，要求每个样本都被正确地分类， <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7By_n%7D%5Cleft%28+%7B%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%7B%5Crm%7Bx%7D%7D_n%7D+%2B+b%7D+%5Cright%29+%3E+0%5C%5D\" alt=\"\\[{y_n}\\left( {{{\\rm{w}}^T}{{\\rm{x}}_n} + b} \\right) &gt; 0\\]\" eeimg=\"1\"/> 。结合公式 (2) 可得：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%7B%5Crm%7Bdistance%7D%7D%5Cleft%28+%7B%7B%7B%5Crm%7Bx%7D%7D_n%7D%2C%7B%5Crm%7Bw%7D%7D%2C%7B%5Crm%7Bb%7D%7D%7D+%5Cright%29+%3D+%5Cfrac%7B1%7D%7B%7B%5Cleft%5C%7C+%7B%5Crm%7Bw%7D%7D+%5Cright%5C%7C%7D%7D%7By_n%7D%5Cleft%28+%7B%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%7B%5Crm%7Bx%7D%7D_n%7D+%2B+b%7D+%5Cright%29+%5Ctag%7B4%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} {\\rm{distance}}\\left( {{{\\rm{x}}_n},{\\rm{w}},{\\rm{b}}} \\right) = \\frac{1}{{\\left\\| {\\rm{w}} \\right\\|}}{y_n}\\left( {{{\\rm{w}}^T}{{\\rm{x}}_n} + b} \\right) \\tag{4} \\end{align*}\" eeimg=\"1\"/> </p><h2><b>二、Hard Margin原问题</b></h2><p>根据公式 (1) 和 (2) ，我们可将Hard Margin问题表述为：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Cmathop+%7B%5Cmax+%7D%5Climits_%7Bb%2C%5Crm%7Bw%7D%7D+%5C+%5C+%5C+%26%7B%5Crm%7Bmargin%7D%7D%5Cleft%28+b%2C%5Crm%7Bw%7D+%5Cright%29%5C%5C+s.t.%5C+%5C+%5C+%26+%5Crm%7Bevery%7D+%5C+%5C+%7By_n%7D%5Cleft%28+%7B%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%7B%5Crm%7Bx%7D%7D_n%7D+%2B+b%7D+%5Cright%29+%3E+0+%5C%5C+%26%7B%5Crm%7Bmargin%7D%7D%5Cleft%28+b%2C%5Crm%7Bw%7D+%5Cright%29%3D+%5Cmathop+%7B%5Cmin+%7D%5Climits_%7Bn+%3D+1%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2CN%7D+%5Cfrac%7B1%7D%7B%7B%5Cleft%5C%7C+%7B%5Crm%7Bw%7D%7D+%5Cright%5C%7C%7D%7D%7By_n%7D%5Cleft%28+%7B%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%7B%5Crm%7Bx%7D%7D_n%7D+%2B+b%7D+%5Cright%29+%5Ctag%7B5%7D+%5Cend%7Balign%2A%7D+\" alt=\"\\begin{align*} \\mathop {\\max }\\limits_{b,\\rm{w}} \\ \\ \\ &amp;{\\rm{margin}}\\left( b,\\rm{w} \\right)\\\\ s.t.\\ \\ \\ &amp; \\rm{every} \\ \\ {y_n}\\left( {{{\\rm{w}}^T}{{\\rm{x}}_n} + b} \\right) &gt; 0 \\\\ &amp;{\\rm{margin}}\\left( b,\\rm{w} \\right)= \\mathop {\\min }\\limits_{n = 1, \\cdot \\cdot \\cdot ,N} \\frac{1}{{\\left\\| {\\rm{w}} \\right\\|}}{y_n}\\left( {{{\\rm{w}}^T}{{\\rm{x}}_n} + b} \\right) \\tag{5} \\end{align*} \" eeimg=\"1\"/> </p><p>我们对 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7Bw%7D%7D%5C%5D\" alt=\"\\[{\\rm{w}}\\]\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5C%5Bb%5C%5D\" alt=\"\\[b\\]\" eeimg=\"1\"/> 进行相同的放缩，得到超平面<img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Ceta+%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%5Crm%7Bx%7D%7D+%2B+%5Ceta+b+%3D+0%5C%5D\" alt=\"\\[\\eta {{\\rm{w}}^T}{\\rm{x}} + \\eta b = 0\\]\" eeimg=\"1\"/> 其中 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Ceta+%5Cne+0%5C%5D\" alt=\"\\[\\eta \\ne 0\\]\" eeimg=\"1\"/>。得到的超平面和原来的超平面<img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%5Crm%7Bx%7D%7D+%2B+b+%3D+0%5C%5D\" alt=\"\\[{{\\rm{w}}^T}{\\rm{x}} + b = 0\\]\" eeimg=\"1\"/> 是同一个平面。那么我们就可以调整 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7Bw%7D%7D%5C%5D\" alt=\"\\[{\\rm{w}}\\]\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5C%5Bb%5C%5D\" alt=\"\\[b\\]\" eeimg=\"1\"/>使得公式 (6) 成立：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Cmathop+%7B%5Cmin+%7D%5Climits_%7Bn+%3D+1%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2CN%7D+%7By_n%7D%5Cleft%28+%7B%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%7B%5Crm%7Bx%7D%7D_n%7D+%2B+b%7D+%5Cright%29+%3D+1+%5Ctag%7B6%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} \\mathop {\\min }\\limits_{n = 1, \\cdot \\cdot \\cdot ,N} {y_n}\\left( {{{\\rm{w}}^T}{{\\rm{x}}_n} + b} \\right) = 1 \\tag{6} \\end{align*}\" eeimg=\"1\"/> </p><p>所以公式 (5) 可以进一步化简为：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Cmathop+%7B%5Cmax+%7D%5Climits_%7Bb%2C%5Crm%7Bw%7D%7D+%5C+%5C+%5C+%26%5Cfrac%7B1%7D%7B%7B%5Cleft%5C%7C+%7B%5Crm%7Bw%7D%7D+%5Cright%5C%7C%7D%7D+%5C%5C+s.t.%5C+%5C+%5C+%26+%5Cmathop+%7B%5Cmin+%7D%5Climits_%7Bn+%3D+1%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2CN%7D+%7By_n%7D%5Cleft%28+%7B%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%7B%5Crm%7Bx%7D%7D_n%7D+%2B+b%7D+%5Cright%29+%3D+1+%5C%5C+%5Ctag%7B7%7D+%5Cend%7Balign%2A%7D+\" alt=\"\\begin{align*} \\mathop {\\max }\\limits_{b,\\rm{w}} \\ \\ \\ &amp;\\frac{1}{{\\left\\| {\\rm{w}} \\right\\|}} \\\\ s.t.\\ \\ \\ &amp; \\mathop {\\min }\\limits_{n = 1, \\cdot \\cdot \\cdot ,N} {y_n}\\left( {{{\\rm{w}}^T}{{\\rm{x}}_n} + b} \\right) = 1 \\\\ \\tag{7} \\end{align*} \" eeimg=\"1\"/> </p><p>对于约束条件不好处理，我们试着扩大约束条件即<img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7By_n%7D%5Cleft%28+%7B%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%7B%5Crm%7Bx%7D%7D_n%7D+%2B+b%7D+%5Cright%29+%5Cge+1%5C%5D\" alt=\"\\[{y_n}\\left( {{{\\rm{w}}^T}{{\\rm{x}}_n} + b} \\right) \\ge 1\\]\" eeimg=\"1\"/> ，新的约束条件包含了原约束条件，我们发现即使扩大约束条件，最优解还是在原约束条件中取得。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-f3c1444b41660e82301e5dd4490bf817_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"937\" data-rawheight=\"270\" class=\"origin_image zh-lightbox-thumb\" width=\"937\" data-original=\"https://pic4.zhimg.com/v2-f3c1444b41660e82301e5dd4490bf817_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;937&#39; height=&#39;270&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"937\" data-rawheight=\"270\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"937\" data-original=\"https://pic4.zhimg.com/v2-f3c1444b41660e82301e5dd4490bf817_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-f3c1444b41660e82301e5dd4490bf817_b.jpg\"/></figure><p>结合公式 (7) 可得：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Cmathop+%7B%5Cmin+%7D%5Climits_%7Bb%2C%5Crm%7Bw%7D%7D+%5C+%5C+%5C+%26+%5Cfrac%7B1%7D%7B2%7D%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%5Crm%7Bw%7D%7D%5C%5C+s.t.%5C+%5C+%5C+%26+%7By_n%7D%5Cleft%28+%7B%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%7B%5Crm%7Bx%7D%7D_n%7D+%2B+b%7D+%5Cright%29+%5Cge+1+%5C+%7B%5Crm%7Bfor%5C+all%7D%7D+%5C+n%5C%5C+%5Ctag%7B8%7D+%5Cend%7Balign%2A%7D+\" alt=\"\\begin{align*} \\mathop {\\min }\\limits_{b,\\rm{w}} \\ \\ \\ &amp; \\frac{1}{2}{{\\rm{w}}^T}{\\rm{w}}\\\\ s.t.\\ \\ \\ &amp; {y_n}\\left( {{{\\rm{w}}^T}{{\\rm{x}}_n} + b} \\right) \\ge 1 \\ {\\rm{for\\ all}} \\ n\\\\ \\tag{8} \\end{align*} \" eeimg=\"1\"/> </p><p>这是一个二次规划问题，当然在编程时候，可以采用已有的二次规划库函数来求解。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-8a893d778e19c76e9300a9a83c3c6387_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"663\" data-rawheight=\"311\" class=\"origin_image zh-lightbox-thumb\" width=\"663\" data-original=\"https://pic4.zhimg.com/v2-8a893d778e19c76e9300a9a83c3c6387_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;663&#39; height=&#39;311&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"663\" data-rawheight=\"311\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"663\" data-original=\"https://pic4.zhimg.com/v2-8a893d778e19c76e9300a9a83c3c6387_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-8a893d778e19c76e9300a9a83c3c6387_b.jpg\"/></figure><p>得到 <img src=\"https://www.zhihu.com/equation?tex=%5C%5Bb%2C%7B%5Crm%7Bw%7D%7D%5C%5D\" alt=\"\\[b,{\\rm{w}}\\]\" eeimg=\"1\"/> 后，对于给定的 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7Bx%7D%7D%5C%5D\" alt=\"\\[{\\rm{x}}\\]\" eeimg=\"1\"/> 我们可以根据公式 (9) 做出预测：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%7B%7B%5Crm%7Bg%7D%7D_%7B%7B%5Crm%7BSVM%7D%7D%7D%7D%5Cleft%28+%7B%5Crm%7Bx%7D%7D+%5Cright%29%7B%5Crm%7B+%3D+sign%7D%7D%5Cleft%28+%7B%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%5Crm%7Bx%7D%7D+%2B+b%7D+%5Cright%29+%5Ctag%7B9%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} {{\\rm{g}}_{{\\rm{SVM}}}}\\left( {\\rm{x}} \\right){\\rm{ = sign}}\\left( {{{\\rm{w}}^T}{\\rm{x}} + b} \\right) \\tag{9} \\end{align*}\" eeimg=\"1\"/> </p><h2><b>三、Hard Margin对偶问题</b></h2><p><b>1. 非线性</b></p><p>如果在原空间样本不是线性可分的，我们可以把样本映射到更高纬的空间，使其更容易分开，如图 2.3 所示。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-94bce4bcb317c3f5f03b5fac002c127e_b.jpg\" data-size=\"normal\" data-rawwidth=\"600\" data-rawheight=\"292\" class=\"origin_image zh-lightbox-thumb\" width=\"600\" data-original=\"https://pic3.zhimg.com/v2-94bce4bcb317c3f5f03b5fac002c127e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;600&#39; height=&#39;292&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"600\" data-rawheight=\"292\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"600\" data-original=\"https://pic3.zhimg.com/v2-94bce4bcb317c3f5f03b5fac002c127e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-94bce4bcb317c3f5f03b5fac002c127e_b.jpg\"/><figcaption>图 2.3</figcaption></figure><p>数学上可以表示为：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Cmathop+%7B%5Cmin+%7D%5Climits_%7Bb%2C%5Crm%7Bw%7D%7D+%5C+%5C+%5C+%26+%5Cfrac%7B1%7D%7B2%7D%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%5Crm%7Bw%7D%7D%5C%5C+s.t.%5C+%5C+%5C+%26+%7By_n%7D%5Cleft%28+%7B%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%7B%5Crm%7Bz%7D%7D_n%7D+%2B+b%7D+%5Cright%29+%5Cge+1+%5C+%7B%5Crm%7Bfor%5C+all%7D%7D+%5C+n%5C%5C+%5Ctag%7B10%7D+%5Cend%7Balign%2A%7D+\" alt=\"\\begin{align*} \\mathop {\\min }\\limits_{b,\\rm{w}} \\ \\ \\ &amp; \\frac{1}{2}{{\\rm{w}}^T}{\\rm{w}}\\\\ s.t.\\ \\ \\ &amp; {y_n}\\left( {{{\\rm{w}}^T}{{\\rm{z}}_n} + b} \\right) \\ge 1 \\ {\\rm{for\\ all}} \\ n\\\\ \\tag{10} \\end{align*} \" eeimg=\"1\"/> </p><p>其中 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5Crm%7Bz%7D%7D_n%7D+%3D+%5CPhi+%5Cleft%28+%7B%7B%7B%5Crm%7Bx%7D%7D_n%7D%7D+%5Cright%29%2C%7B%7B%5Crm%7Bz%7D%7D_n%7D+%5Cin+%7BR%5E%7B%5Ctilde+d%7D%7D%5C%5D\" alt=\"\\[{{\\rm{z}}_n} = \\Phi \\left( {{{\\rm{x}}_n}} \\right),{{\\rm{z}}_n} \\in {R^{\\tilde d}}\\]\" eeimg=\"1\"/> ，对于公式 (10) 可以先求出 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5Crm%7Bz%7D%7D_n%7D%5C%5D\" alt=\"\\[{{\\rm{z}}_n}\\]\" eeimg=\"1\"/> ，然后采用 <img src=\"https://www.zhihu.com/equation?tex=+QP+\" alt=\" QP \" eeimg=\"1\"/> 求解，有 <img src=\"https://www.zhihu.com/equation?tex=N\" alt=\"N\" eeimg=\"1\"/> 个约束项， <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Ctilde+d+%2B+1%5C%5D\" alt=\"\\[\\tilde d + 1\\]\" eeimg=\"1\"/> 个变量，如果 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Ctilde+d%7D%5C%5D\" alt=\"\\[{\\tilde d}\\]\" eeimg=\"1\"/> 非常大，那么采用 <img src=\"https://www.zhihu.com/equation?tex=QP\" alt=\"QP\" eeimg=\"1\"/> 就变得比较困难。我们是否可以对公式 (9) 做进一步的变化，使 <b>SVM 优化问题和 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Ctilde+d%7D%5C%5D\" alt=\"\\[{\\tilde d}\\]\" eeimg=\"1\"/> 无关</b>？</p><p><b>2. 对偶问题</b></p><p>根据<a href=\"https://zhuanlan.zhihu.com/p/42883486\" class=\"internal\">Lagrangian对偶</a>可知，公式 (10) 优化问题满足 Slater 条件，所以强对偶性成立，原问题可以转化为求解对偶问题。</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%26%5Cmathop+%7B%5Cmax+%7D%5Climits_%5Calpha+%5Cmathop+%7B%5Cmin+%7D%5Climits_%7Bb%2C%7B%5Crm%7Bw%7D%7D%7D+%5Cleft%28+%7B%5Cfrac%7B1%7D%7B2%7D%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%5Crm%7Bw%7D%7D+%2B+%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%7B%5Calpha+_n%7D%5Cleft%28+%7B1+-+%7By_n%7D%5Cleft%28+%7B%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%7B%5Crm%7Bx%7D%7D_n%7D+%2B+b%7D+%5Cright%29%7D+%5Cright%29%7D+%7D+%5Cright%29%5C%5C+%26s.t.+%5C+%5C+%7B%5Calpha+_n%7D+%5Cge+0%2C+%5C+n+%3D+1%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2CN+%5Ctag%7B11%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} &amp;\\mathop {\\max }\\limits_\\alpha \\mathop {\\min }\\limits_{b,{\\rm{w}}} \\left( {\\frac{1}{2}{{\\rm{w}}^T}{\\rm{w}} + \\sum\\limits_{n = 1}^N {{\\alpha _n}\\left( {1 - {y_n}\\left( {{{\\rm{w}}^T}{{\\rm{x}}_n} + b} \\right)} \\right)} } \\right)\\\\ &amp;s.t. \\ \\ {\\alpha _n} \\ge 0, \\ n = 1, \\cdot \\cdot \\cdot ,N \\tag{11} \\end{align*}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+L%5Cleft%28+%7Bb%2C%7B%5Crm%7Bw%7D%7D%2C%5Calpha+%7D+%5Cright%29+%26%3D+%5Cfrac%7B1%7D%7B2%7D%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%5Crm%7Bw%7D%7D+%2B+%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%7B%5Calpha+_n%7D%5Cleft%28+%7B1+-+%7By_n%7D%5Cleft%28+%7B%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%7B%5Crm%7Bx%7D%7D_n%7D+%2B+b%7D+%5Cright%29%7D+%5Cright%29%7D+%5C%5C+%5Cfrac%7B%7B%5Cpartial+L%7D%7D%7B%7B%5Cpartial+b%7D%7D+%26%3D+-+%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%7B%5Calpha+_n%7D%7By_n%7D%7D+%3D+0+%5CRightarrow+%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%7B%5Calpha+_n%7D%7By_n%7D%7D+%3D+0%5C%5C+%5Cfrac%7B%7B%5Cpartial+L%7D%7D%7B%7B%5Cpartial+%7B%5Crm%7Bw%7D%7D%7D%7D+%26%3D+%7B%5Crm%7Bw%7D%7D+-+%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%7B%5Calpha+_n%7D%7By_n%7D%7B%7B%5Crm%7Bz%7D%7D_n%7D%7D+%5CRightarrow+%7B%5Crm%7Bw%7D%7D+%3D+%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%7B%5Calpha+_n%7D%7By_n%7D%7B%7B%5Crm%7Bz%7D%7D_n%7D%7D+%5Ctag%7B12%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} L\\left( {b,{\\rm{w}},\\alpha } \\right) &amp;= \\frac{1}{2}{{\\rm{w}}^T}{\\rm{w}} + \\sum\\limits_{n = 1}^N {{\\alpha _n}\\left( {1 - {y_n}\\left( {{{\\rm{w}}^T}{{\\rm{x}}_n} + b} \\right)} \\right)} \\\\ \\frac{{\\partial L}}{{\\partial b}} &amp;= - \\sum\\limits_{n = 1}^N {{\\alpha _n}{y_n}} = 0 \\Rightarrow \\sum\\limits_{n = 1}^N {{\\alpha _n}{y_n}} = 0\\\\ \\frac{{\\partial L}}{{\\partial {\\rm{w}}}} &amp;= {\\rm{w}} - \\sum\\limits_{n = 1}^N {{\\alpha _n}{y_n}{{\\rm{z}}_n}} \\Rightarrow {\\rm{w}} = \\sum\\limits_{n = 1}^N {{\\alpha _n}{y_n}{{\\rm{z}}_n}} \\tag{12} \\end{align*}\" eeimg=\"1\"/> </p><p>将公式 (12) 得到的结果带入公式 (11)，可得到对偶问题：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Cmathop+%7B%5Cmin+%7D%5Climits_%5Calpha+%26%5Cfrac%7B1%7D%7B2%7D%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%5Csum%5Climits_%7Bm+%3D+1%7D%5EN+%7B%7B%5Calpha+_n%7D%7B%5Calpha+_m%7D%7By_n%7D%7By_m%7D%7B%5Crm%7Bz%7D%7D_n%5ET%7B%7B%5Crm%7Bz%7D%7D_m%7D%7D+%7D+-+%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%7B%5Calpha+_n%7D%7D+%5C%5C+s.t.+%5C+%5C+%26%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%7B%5Calpha+_n%7D%7By_n%7D%7D+%3D+0%5C%5C+%26%7B%5Calpha+_n%7D+%5Cge+0%2C%5C+n+%3D+1%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2CN+%5Ctag%7B13%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} \\mathop {\\min }\\limits_\\alpha &amp;\\frac{1}{2}\\sum\\limits_{n = 1}^N {\\sum\\limits_{m = 1}^N {{\\alpha _n}{\\alpha _m}{y_n}{y_m}{\\rm{z}}_n^T{{\\rm{z}}_m}} } - \\sum\\limits_{n = 1}^N {{\\alpha _n}} \\\\ s.t. \\ \\ &amp;\\sum\\limits_{n = 1}^N {{\\alpha _n}{y_n}} = 0\\\\ &amp;{\\alpha _n} \\ge 0,\\ n = 1, \\cdot \\cdot \\cdot ,N \\tag{13} \\end{align*}\" eeimg=\"1\"/> </p><p>公式 (13) 也是一个二次规划问题，有 <img src=\"https://www.zhihu.com/equation?tex=N%2B1\" alt=\"N+1\" eeimg=\"1\"/> 个约束项， <img src=\"https://www.zhihu.com/equation?tex=N\" alt=\"N\" eeimg=\"1\"/> 个变量，解决了公式 (10) 中的优化问题受限于 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Ctilde+d%7D%5C%5D\" alt=\"\\[{\\tilde d}\\]\" eeimg=\"1\"/> 。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-f17e9875083ce1b9f83e12cb543fa43d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"861\" data-rawheight=\"443\" class=\"origin_image zh-lightbox-thumb\" width=\"861\" data-original=\"https://pic2.zhimg.com/v2-f17e9875083ce1b9f83e12cb543fa43d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;861&#39; height=&#39;443&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"861\" data-rawheight=\"443\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"861\" data-original=\"https://pic2.zhimg.com/v2-f17e9875083ce1b9f83e12cb543fa43d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-f17e9875083ce1b9f83e12cb543fa43d_b.jpg\"/></figure><p>当求解出 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Calpha+%5C%5D\" alt=\"\\[\\alpha \\]\" eeimg=\"1\"/> 后，根据互补松弛性质可得：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%26%7B%5Calpha+_n%7D%5Cleft%28+%7B1+-+%7By_n%7D%5Cleft%28+%7B%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%7B%5Crm%7Bx%7D%7D_n%7D+%2B+b%7D+%5Cright%29%7D+%5Cright%29+%3D+0%5C%5C+%26+%7B%5Crm%7Bwhen%7D%7D+%5C+%7B%5Calpha+_s%7D+%3E+0%5CRightarrow+b+%3D+%7By_s%7D+-+%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%7B%5Crm%7Bx%7D%7D_s%7D+%5Ctag%7B13%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} &amp;{\\alpha _n}\\left( {1 - {y_n}\\left( {{{\\rm{w}}^T}{{\\rm{x}}_n} + b} \\right)} \\right) = 0\\\\ &amp; {\\rm{when}} \\ {\\alpha _s} &gt; 0\\Rightarrow b = {y_s} - {{\\rm{w}}^T}{{\\rm{x}}_s} \\tag{13} \\end{align*}\" eeimg=\"1\"/> </p><p>得到 <img src=\"https://www.zhihu.com/equation?tex=%5C%5Bb%2C%7B%5Crm%7Bw%7D%7D%5C%5D\" alt=\"\\[b,{\\rm{w}}\\]\" eeimg=\"1\"/> 后，对于给定的 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7Bx%7D%7D%5C%5D\" alt=\"\\[{\\rm{x}}\\]\" eeimg=\"1\"/> 我们可以根据公式 (14) 做出预测：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%7B%7B%5Crm%7Bg%7D%7D_%7B%7B%5Crm%7BSVM%7D%7D%7D%7D%5Cleft%28+%7B%5Crm%7Bx%7D%7D+%5Cright%29%7B%5Crm%7B+%3D+sign%7D%7D%5Cleft%28+%7B%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%5CPhi+%5Cleft%28+%7B%5Crm%7Bx%7D%7D+%5Cright%29+%2B+b%7D+%5Cright%29+%5Ctag%7B14%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} {{\\rm{g}}_{{\\rm{SVM}}}}\\left( {\\rm{x}} \\right){\\rm{ = sign}}\\left( {{{\\rm{w}}^T}\\Phi \\left( {\\rm{x}} \\right) + b} \\right) \\tag{14} \\end{align*}\" eeimg=\"1\"/> </p><p><b>3. 总结</b></p><p>我们知道当 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Calpha+_n%7D+%3E+0%5C%5D\" alt=\"\\[{\\alpha _n} &gt; 0\\]\" eeimg=\"1\"/> ，则 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7By_n%7D%5Cleft%28+%7B%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%7B%5Crm%7Bx%7D%7D_n%7D+%2B+b%7D+%5Cright%29+%3D+1%5C%5D\" alt=\"\\[{y_n}\\left( {{{\\rm{w}}^T}{{\\rm{x}}_n} + b} \\right) = 1\\]\" eeimg=\"1\"/> ，这些样本点是位于“边界”处，如图 2.4 所示。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-a74024b6e3b461b5859d656353e599fe_b.jpg\" data-size=\"normal\" data-rawwidth=\"230\" data-rawheight=\"231\" class=\"content_image\" width=\"230\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;230&#39; height=&#39;231&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"230\" data-rawheight=\"231\" class=\"content_image lazy\" width=\"230\" data-actualsrc=\"https://pic3.zhimg.com/v2-a74024b6e3b461b5859d656353e599fe_b.jpg\"/><figcaption>图 2.4</figcaption></figure><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7Bw%7D%7D%5C%5D\" alt=\"\\[{\\rm{w}}\\]\" eeimg=\"1\"/> 的值仅由部分位于“边界”的样本点决定的。，这些样本点称为<b>support vectors</b>。</p><p>1）only SV needed to compute <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7Bw%3Aw%7D%7D+%3D+%5Csum%5Climits_%7Bn+%3D+1%7D%5EN+%7B%7B%5Calpha+_n%7D%7By_n%7D%7B%7B%5Crm%7Bz%7D%7D_n%7D%7D+%3D+%5Csum%5Climits_%7B%7B%5Crm%7BSV%7D%7D%7D+%7B%7B%5Calpha+_n%7D%7By_n%7D%7B%7B%5Crm%7Bz%7D%7D_n%7D%7D+%5C%5D\" alt=\"\\[{\\rm{w:w}} = \\sum\\limits_{n = 1}^N {{\\alpha _n}{y_n}{{\\rm{z}}_n}} = \\sum\\limits_{{\\rm{SV}}} {{\\alpha _n}{y_n}{{\\rm{z}}_n}} \\]\" eeimg=\"1\"/> </p><p>2）only SV needed to compute <img src=\"https://www.zhihu.com/equation?tex=%5C%5Bb%3Ab+%3D+%7By_n%7D+-+%7B%7B%5Crm%7Bw%7D%7D%5ET%7D%7B%7B%5Crm%7Bx%7D%7D_n%7D%5C%5D\" alt=\"\\[b:b = {y_n} - {{\\rm{w}}^T}{{\\rm{x}}_n}\\]\" eeimg=\"1\"/> with any <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7BSV%7D%7D%5Cleft%28+%7B%7B%7B%5Crm%7Bz%7D%7D_n%7D%2C%7By_n%7D%7D+%5Cright%29%5C%5D\" alt=\"\\[{\\rm{SV}}\\left( {{{\\rm{z}}_n},{y_n}} \\right)\\]\" eeimg=\"1\"/> </p><p>其中 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7BSV%7D%7D%5Cleft%28+%7B%7B%5Crm%7Bpositive%7D%7D%5C+%7B%5Calpha+_n%7D%7D+%5Cright%29+%5Csubseteq+%7B%5Crm%7BSV+%5C+candidates%5Cleft%28on+%5C+boundary+%5Cright%29%7D%7D%5C%5D\" alt=\"\\[{\\rm{SV}}\\left( {{\\rm{positive}}\\ {\\alpha _n}} \\right) \\subseteq {\\rm{SV \\ candidates\\left(on \\ boundary \\right)}}\\]\" eeimg=\"1\"/> </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-389a932ca2291476c88fb67129ef3417_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"898\" data-rawheight=\"456\" class=\"origin_image zh-lightbox-thumb\" width=\"898\" data-original=\"https://pic4.zhimg.com/v2-389a932ca2291476c88fb67129ef3417_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;898&#39; height=&#39;456&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"898\" data-rawheight=\"456\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"898\" data-original=\"https://pic4.zhimg.com/v2-389a932ca2291476c88fb67129ef3417_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-389a932ca2291476c88fb67129ef3417_b.jpg\"/></figure><p>公式 (13) 彻底解决对 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Ctilde+d%7D%5C%5D\" alt=\"\\[{\\tilde d}\\]\" eeimg=\"1\"/> 的依赖问题吗？我们会发现 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7Bq_%7Bn%2Cm%7D%7D+%3D+%7By_n%7D%7By_m%7D%7B%5Crm%7Bz%7D%7D_n%5ET%7B%7B%5Crm%7Bz%7D%7D_m%7D%5C%5D\" alt=\"\\[{q_{n,m}} = {y_n}{y_m}{\\rm{z}}_n^T{{\\rm{z}}_m}\\]\" eeimg=\"1\"/> ，其中 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7B%5Crm%7Bz%7D%7D_n%7D+%5Cin+%7BR%5E%7B%5Ctilde+d%7D%7D%2C%7B%7B%5Crm%7Bz%7D%7D_m%7D+%5Cin+%7BR%5E%7B%5Ctilde+d%7D%7D%5C%5D\" alt=\"\\[{{\\rm{z}}_n} \\in {R^{\\tilde d}},{{\\rm{z}}_m} \\in {R^{\\tilde d}}\\]\" eeimg=\"1\"/> ，仍然受限于 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%5Ctilde+d%7D%5C%5D\" alt=\"\\[{\\tilde d}\\]\" eeimg=\"1\"/> ，下一节将介绍核函数彻底地解决这个问题。</p><p><a href=\"https://zhuanlan.zhihu.com/p/42883486\" class=\"internal\">上一篇</a> </p><p><a href=\"https://zhuanlan.zhihu.com/p/43130548\" class=\"internal\">下一篇</a></p>", 
            "topic": [
                {
                    "tag": "SVM", 
                    "tagLink": "https://api.zhihu.com/topics/19583524"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/42883486", 
            "userName": "徽州风韵", 
            "userLink": "https://www.zhihu.com/people/53bd6f8b93873b03c5edb714aea2a456", 
            "upvote": 0, 
            "title": "SVM-Lagrangian对偶", 
            "content": "<h2><b>一、Lagrangian对偶函数</b></h2><p><b>1. Lagrangian函数</b></p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Cbegin%7Barray%7D%7Bl%7D+%5Cbegin%7Barray%7D%7B%2A%7B20%7D%7Bc%7D%7D+%7B%5Cmin%7D%26%7B%7Bf_0%7D%5Cleft%28+x+%5Cright%29%7D+%5Cend%7Barray%7D%5C%5C+%5Cbegin%7Barray%7D%7B%2A%7B20%7D%7Bc%7D%7D+%7Bs.t.%7D%26%7B%7Bf_i%7D%5Cleft%28+x+%5Cright%29+%5Cle+0%2C%7D%26%7Bi+%3D+1%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2Cm%7D%5C%5C+%7B%7D%26%7B%7Bh_i%7D%5Cleft%28+x+%5Cright%29+%3D+0%7D%26%7Bi+%3D+1%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2Cp%7D+%5Cend%7Barray%7D+%5Cend%7Barray%7D+%5Ctag%7B1%7D%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} \\begin{array}{l} \\begin{array}{*{20}{c}} {\\min}&amp;{{f_0}\\left( x \\right)} \\end{array}\\\\ \\begin{array}{*{20}{c}} {s.t.}&amp;{{f_i}\\left( x \\right) \\le 0,}&amp;{i = 1, \\cdot \\cdot \\cdot ,m}\\\\ {}&amp;{{h_i}\\left( x \\right) = 0}&amp;{i = 1, \\cdot \\cdot \\cdot ,p} \\end{array} \\end{array} \\tag{1}\\end{align*}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7Bf_i%7D%5Cleft%28+x+%5Cright%29+%5Cle+0%7D%5C%5D\" alt=\"\\[{{f_i}\\left( x \\right) \\le 0}\\]\" eeimg=\"1\"/> 称为不等式约束， <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%7B%7Bh_i%7D%5Cleft%28+x+%5Cright%29+%3D+0%7D%5C%5D\" alt=\"\\[{{h_i}\\left( x \\right) = 0}\\]\" eeimg=\"1\"/> 称为等式约束，定义域 <img src=\"https://www.zhihu.com/equation?tex=D+%3D+%5Cbigcap%5Climits_%7Bi+%3D+0%7D%5Em+%7B%7B%5Crm%7Bdom%7D%7D%7Bf_i%7D%7D+%5Ccap+%5Cbigcap%5Climits_%7Bi+%3D+0%7D%5Ep+%7B%7B%5Crm%7Bdom%7D%7D%7Bh_i%7D%7D+\" alt=\"D = \\bigcap\\limits_{i = 0}^m {{\\rm{dom}}{f_i}} \\cap \\bigcap\\limits_{i = 0}^p {{\\rm{dom}}{h_i}} \" eeimg=\"1\"/> ，优化问题最优值为 <img src=\"https://www.zhihu.com/equation?tex=%7Bp%5E%2A%7D\" alt=\"{p^*}\" eeimg=\"1\"/> 。Lagrangian函数 <img src=\"https://www.zhihu.com/equation?tex=%5C%5BL%3A%7BR%5En%7D+%5Ctimes+%7BR%5Em%7D+%5Ctimes+%7BR%5Ep%7D+%5Cto+R%5C%5D\" alt=\"\\[L:{R^n} \\times {R^m} \\times {R^p} \\to R\\]\" eeimg=\"1\"/> 为：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+L%5Cleft%28+%7Bx%2C%5Clambda+%2C%5Cnu+%7D+%5Cright%29+%3D+%7Bf_0%7D%5Cleft%28+x+%5Cright%29+%2B+%5Csum%5Climits_%7Bi+%3D+1%7D%5Em+%7B%7B%5Clambda+_i%7D%7Bf_i%7D%5Cleft%28+x+%5Cright%29%7D+%2B+%5Csum%5Climits_%7Bi+%3D+1%7D%5Ep+%7B%7B%5Cnu+_i%7D%7Bh_i%7D%5Cleft%28+x+%5Cright%29%7D+%5Ctag%7B2%7D%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} L\\left( {x,\\lambda ,\\nu } \\right) = {f_0}\\left( x \\right) + \\sum\\limits_{i = 1}^m {{\\lambda _i}{f_i}\\left( x \\right)} + \\sum\\limits_{i = 1}^p {{\\nu _i}{h_i}\\left( x \\right)} \\tag{2}\\end{align*}\" eeimg=\"1\"/> <b>2. Lagrangian对偶函数</b></p><p>Lagrangian对偶函数 <img src=\"https://www.zhihu.com/equation?tex=%5C%5Bg%3A%7BR%5Em%7D+%5Ctimes+%7BR%5Ep%7D+%5Cto+R%5C%5D\" alt=\"\\[g:{R^m} \\times {R^p} \\to R\\]\" eeimg=\"1\"/> 为Lagrangian函数关于 <img src=\"https://www.zhihu.com/equation?tex=x\" alt=\"x\" eeimg=\"1\"/> 取最小值。</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+g%5Cleft%28+%7B%5Clambda+%2C%5Cnu+%7D+%5Cright%29+%3D+%5Cmathop+%7B%5Cinf+%7D%5Climits_%7Bx+%5Cin+D%7D+L%5Cleft%28+%7Bx%2C%5Clambda+%2C%5Cnu+%7D+%5Cright%29+%3D+%5Cmathop+%7B%5Cinf+%7D%5Climits_%7Bx+%5Cin+D%7D+%5Cleft%28+%7B%7Bf_0%7D%5Cleft%28+x+%5Cright%29+%2B+%5Csum%5Climits_%7Bi+%3D+1%7D%5Em+%7B%7B%5Clambda+_i%7D%7Bf_i%7D%5Cleft%28+x+%5Cright%29%7D+%2B+%5Csum%5Climits_%7Bi+%3D+1%7D%5Ep+%7B%7B%5Cnu+_i%7D%7Bh_i%7D%5Cleft%28+x+%5Cright%29%7D+%7D+%5Cright%29+%5Ctag%7B3%7D%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} g\\left( {\\lambda ,\\nu } \\right) = \\mathop {\\inf }\\limits_{x \\in D} L\\left( {x,\\lambda ,\\nu } \\right) = \\mathop {\\inf }\\limits_{x \\in D} \\left( {{f_0}\\left( x \\right) + \\sum\\limits_{i = 1}^m {{\\lambda _i}{f_i}\\left( x \\right)} + \\sum\\limits_{i = 1}^p {{\\nu _i}{h_i}\\left( x \\right)} } \\right) \\tag{3}\\end{align*}\" eeimg=\"1\"/> <b>3. 最优值的下界</b></p><p>对偶函数给出了原问题 (1) 最优值 <img src=\"https://www.zhihu.com/equation?tex=%7Bp%5E%2A%7D\" alt=\"{p^*}\" eeimg=\"1\"/> 的下界，即对于任意 <img src=\"https://www.zhihu.com/equation?tex=%5Clambda+%5Csucceq+0\" alt=\"\\lambda \\succeq 0\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5Cnu+\" alt=\"\\nu \" eeimg=\"1\"/> 公式 (4) 成立。</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+g%5Cleft%28+%7B%5Clambda+%2C%5Cnu+%7D+%5Cright%29+%5Cle+%7Bp%5E%2A%7D+%5Ctag%7B4%7D%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} g\\left( {\\lambda ,\\nu } \\right) \\le {p^*} \\tag{4}\\end{align*}\" eeimg=\"1\"/> </p><p>证明：对任意一个可行点 <img src=\"https://www.zhihu.com/equation?tex=x\" alt=\"x\" eeimg=\"1\"/> ，则满足 <img src=\"https://www.zhihu.com/equation?tex=%7Bf_i%7D%5Cleft%28+x+%5Cright%29+%5Cle+0%2C%7Bh_i%7D%5Cleft%28+x+%5Cright%29+%3D+0\" alt=\"{f_i}\\left( x \\right) \\le 0,{h_i}\\left( x \\right) = 0\" eeimg=\"1\"/> ，因为 <img src=\"https://www.zhihu.com/equation?tex=%5Clambda+%5Csucceq+0\" alt=\"\\lambda \\succeq 0\" eeimg=\"1\"/> 所以有</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Cbegin%7Barray%7D%7Bl%7D+%5Csum%5Climits_%7Bi+%3D+1%7D%5Em+%7B%7B%5Clambda+_i%7D%7Bf_i%7D%5Cleft%28+x+%5Cright%29%7D+%2B+%5Csum%5Climits_%7Bi+%3D+1%7D%5Ep+%7B%7B%5Cnu+_i%7D%7Bh_i%7D%5Cleft%28+x+%5Cright%29%7D+%5Cle+0%5C%5C+%5CRightarrow+%7Bf_0%7D%5Cleft%28+x+%5Cright%29+%2B+%5Csum%5Climits_%7Bi+%3D+1%7D%5Em+%7B%7B%5Clambda+_i%7D%7Bf_i%7D%5Cleft%28+x+%5Cright%29%7D+%2B+%5Csum%5Climits_%7Bi+%3D+1%7D%5Ep+%7B%7B%5Cnu+_i%7D%7Bh_i%7D%5Cleft%28+x+%5Cright%29%7D+%5Cle+%7Bf_0%7D%5Cleft%28+x+%5Cright%29%5C%5C+g%5Cleft%28+%7B%5Clambda+%2C%5Cnu+%7D+%5Cright%29+%3D+%5Cmathop+%7B%5Cinf+%7D%5Climits_%7Bx+%5Cin+D%7D+L%5Cleft%28+%7Bx%2C%5Clambda+%2C%5Cnu+%7D+%5Cright%29+%5Cle+%7Bf_0%7D%5Cleft%28+x+%5Cright%29+%2B+%5Csum%5Climits_%7Bi+%3D+1%7D%5Em+%7B%7B%5Clambda+_i%7D%7Bf_i%7D%5Cleft%28+x+%5Cright%29%7D+%2B+%5Csum%5Climits_%7Bi+%3D+1%7D%5Ep+%7B%7B%5Cnu+_i%7D%7Bh_i%7D%5Cleft%28+x+%5Cright%29%7D+%5Cle+%7Bf_0%7D%5Cleft%28+x+%5Cright%29+%5Cend%7Barray%7D+%5Ctag%7B5%7D%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} \\begin{array}{l} \\sum\\limits_{i = 1}^m {{\\lambda _i}{f_i}\\left( x \\right)} + \\sum\\limits_{i = 1}^p {{\\nu _i}{h_i}\\left( x \\right)} \\le 0\\\\ \\Rightarrow {f_0}\\left( x \\right) + \\sum\\limits_{i = 1}^m {{\\lambda _i}{f_i}\\left( x \\right)} + \\sum\\limits_{i = 1}^p {{\\nu _i}{h_i}\\left( x \\right)} \\le {f_0}\\left( x \\right)\\\\ g\\left( {\\lambda ,\\nu } \\right) = \\mathop {\\inf }\\limits_{x \\in D} L\\left( {x,\\lambda ,\\nu } \\right) \\le {f_0}\\left( x \\right) + \\sum\\limits_{i = 1}^m {{\\lambda _i}{f_i}\\left( x \\right)} + \\sum\\limits_{i = 1}^p {{\\nu _i}{h_i}\\left( x \\right)} \\le {f_0}\\left( x \\right) \\end{array} \\tag{5}\\end{align*}\" eeimg=\"1\"/> </p><p>对任意的 <img src=\"https://www.zhihu.com/equation?tex=x\" alt=\"x\" eeimg=\"1\"/> 都满足 <img src=\"https://www.zhihu.com/equation?tex=%5C%5Bg%5Cleft%28+%7B%5Clambda+%2C%5Cnu+%7D+%5Cright%29+%5Cle+%7Bf_0%7D%5Cleft%28+x+%5Cright%29%5C%5D\" alt=\"\\[g\\left( {\\lambda ,\\nu } \\right) \\le {f_0}\\left( x \\right)\\]\" eeimg=\"1\"/> ，所以 <img src=\"https://www.zhihu.com/equation?tex=%5C%5Bg%5Cleft%28+%7B%5Clambda+%2C%5Cnu+%7D+%5Cright%29+%5Cle+%7Bp%5E%2A%7D%5C%5D\" alt=\"\\[g\\left( {\\lambda ,\\nu } \\right) \\le {p^*}\\]\" eeimg=\"1\"/> 。</p><h2><b>二、Lagrangian对偶问题</b></h2><p>Lagrangian对偶函数能给出最好的下界是什么，可将这个问题表述为优化问题：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Cbegin%7Barray%7D%7B%2A%7B20%7D%7Bc%7D%7D+%7B%5Cmax+%7D%26%7Bg%5Cleft%28+%7B%5Clambda+%2C%5Cnu+%7D+%5Cright%29%7D%5C%5C+%7Bs.t.%7D%26%7B%5Clambda+%5Csucceq+0%7D+%5Cend%7Barray%7D+%5Ctag%7B6%7D%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} \\begin{array}{*{20}{c}} {\\max }&amp;{g\\left( {\\lambda ,\\nu } \\right)}\\\\ {s.t.}&amp;{\\lambda \\succeq 0} \\end{array} \\tag{6}\\end{align*}\" eeimg=\"1\"/> <b>1. 弱对偶性</b></p><p><img src=\"https://www.zhihu.com/equation?tex=%7Bd%5E%2A%7D\" alt=\"{d^*}\" eeimg=\"1\"/> 表示 Lagrangian 对偶问题的最优值，即原问题最优值 <img src=\"https://www.zhihu.com/equation?tex=%7Bp%5E%2A%7D\" alt=\"{p^*}\" eeimg=\"1\"/> 的最好下界，根据公式 (4)，有</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%7Bd%5E%2A%7D+%5Cle+%7Bp%5E%2A%7D+%5Ctag%7B7%7D%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} {d^*} \\le {p^*} \\tag{7}\\end{align*}\" eeimg=\"1\"/> </p><p>这个性质称为<b>弱对偶性，</b> <img src=\"https://www.zhihu.com/equation?tex=%7Bp%5E%2A%7D+-+%7Bd%5E%2A%7D\" alt=\"{p^*} - {d^*}\" eeimg=\"1\"/> 是原问题的最优对偶间隙。</p><p><b>2. 强对偶性</b></p><p>如果等式</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%7Bd%5E%2A%7D%3D+%7Bp%5E%2A%7D+%5Ctag%7B8%7D%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} {d^*}= {p^*} \\tag{8}\\end{align*}\" eeimg=\"1\"/> </p><p>成立，那么<b>强对偶性</b>成立。</p><p>一般情况强对偶性不成立，但是如果原问题 (1) 是凸问题，即可以表述为如下形式：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Cbegin%7Barray%7D%7Bl%7D+%5Cbegin%7Barray%7D%7B%2A%7B20%7D%7Bc%7D%7D+%7B%5Cmin%7D%26%7B%7Bf_0%7D%5Cleft%28+x+%5Cright%29%7D+%5Cend%7Barray%7D%5C%5C+%5Cbegin%7Barray%7D%7B%2A%7B20%7D%7Bc%7D%7D+%7Bs.t.%7D%26%7B%7Bf_i%7D%5Cleft%28+x+%5Cright%29+%5Cle+0%2C%7D%26%7Bi+%3D+1%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2Cm%7D%5C%5C+%7B%7D%26%7BAx+%3D+b%7D+%5Cend%7Barray%7D+%5Cend%7Barray%7D+%5Ctag%7B9%7D%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} \\begin{array}{l} \\begin{array}{*{20}{c}} {\\min}&amp;{{f_0}\\left( x \\right)} \\end{array}\\\\ \\begin{array}{*{20}{c}} {s.t.}&amp;{{f_i}\\left( x \\right) \\le 0,}&amp;{i = 1, \\cdot \\cdot \\cdot ,m}\\\\ {}&amp;{Ax = b} \\end{array} \\end{array} \\tag{9}\\end{align*}\" eeimg=\"1\"/> </p><p>函数 <img src=\"https://www.zhihu.com/equation?tex=%7Bf_0%7D%5Cleft%28+x+%5Cright%29%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2C%7Bf_m%7D%5Cleft%28+x+%5Cright%29\" alt=\"{f_0}\\left( x \\right), \\cdot \\cdot \\cdot ,{f_m}\\left( x \\right)\" eeimg=\"1\"/> 是凸函数并且满足 <b>Slater 约束准则</b>，则强对偶性成立。</p><p><b>3. Slater 约束准则</b></p><p>存在一点 <img src=\"https://www.zhihu.com/equation?tex=x+%5Cin+%5Crm%7Brelint%7D%5C+D\" alt=\"x \\in \\rm{relint}\\ D\" eeimg=\"1\"/> （ <img src=\"https://www.zhihu.com/equation?tex=x\" alt=\"x\" eeimg=\"1\"/> 在定义域内，不在边界上）满足：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Cbegin%7Barray%7D%7Bl%7D+%7Bf_i%7D%5Cleft%28+x+%5Cright%29+%3C+0%2Ci+%3D+1%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2Cm%5C%5C+Ax+%3D+b+%5Cend%7Barray%7D+%5Ctag%7B10%7D%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} \\begin{array}{l} {f_i}\\left( x \\right) &lt; 0,i = 1, \\cdot \\cdot \\cdot ,m\\\\ Ax = b \\end{array} \\tag{10}\\end{align*}\" eeimg=\"1\"/> </p><p>当 Slater 条件成立且原问题是凸问题时，强对偶性成立。</p><p>证明：定义 <img src=\"https://www.zhihu.com/equation?tex=%7B%5Crm+A%7D+%5Cin+%7BR%5Em%7D+%5Ctimes+%7BR%5Ep%7D+%5Ctimes+R\" alt=\"{\\rm A} \\in {R^m} \\times {R^p} \\times R\" eeimg=\"1\"/> 为：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Cbegin%7Barray%7D%7Bl%7D+%7B%5Crm+A%7D+%3D+%5Cleft%5C%7B+%7B%5Cleft%28+%7Bu%2Cv%2Ct%7D+%5Cright%29%7C%5Cexists+x+%5Cin+D%2C%7Bf_i%7D%5Cleft%28+x+%5Cright%29+%5Cle+%7Bu_i%7D%2Ci+%3D+1%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2Cm%2C%7D+%5Cright.%5C%5C+%5Cqquad+%5Cleft.+%7B%7Bh_i%7D%5Cleft%28+x+%5Cright%29+%3D+%7Bv_i%7D%2Ci+%3D+1%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2Cp%2C%7Bf_0%7D%5Cleft%28+x+%5Cright%29+%5Cle+t%7D+%5Cright%5C%7D+%5Cend%7Barray%7D+%5Ctag%7B11%7D%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} \\begin{array}{l} {\\rm A} = \\left\\{ {\\left( {u,v,t} \\right)|\\exists x \\in D,{f_i}\\left( x \\right) \\le {u_i},i = 1, \\cdot \\cdot \\cdot ,m,} \\right.\\\\ \\qquad \\left. {{h_i}\\left( x \\right) = {v_i},i = 1, \\cdot \\cdot \\cdot ,p,{f_0}\\left( x \\right) \\le t} \\right\\} \\end{array} \\tag{11}\\end{align*}\" eeimg=\"1\"/> </p><p>如果原问题是凸问题，则 (11) 中定义的集合也是凸集。定义另外一个凸集 <img src=\"https://www.zhihu.com/equation?tex=%7B%5Crm+B%7D\" alt=\"{\\rm B}\" eeimg=\"1\"/> 为：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%7B%5Crm+B%7D+%3D+%5Cleft%5C%7B+%7B%5Cleft%28+%7B0%2C0%2Cs%7D+%5Cright%29+%5Cin+%7BR%5Em%7D+%5Ctimes+%7BR%5Ep%7D+%5Ctimes+R%7Cs+%3C+%7Bp%5E%2A%7D%7D+%5Cright%5C%7D+%5Ctag%7B12%7D%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} {\\rm B} = \\left\\{ {\\left( {0,0,s} \\right) \\in {R^m} \\times {R^p} \\times R|s &lt; {p^*}} \\right\\} \\tag{12}\\end{align*}\" eeimg=\"1\"/> </p><p>则集合 <img src=\"https://www.zhihu.com/equation?tex=%7B%5Crm+A%7D\" alt=\"{\\rm A}\" eeimg=\"1\"/> 和集合 <img src=\"https://www.zhihu.com/equation?tex=%7B%5Crm+B%7D\" alt=\"{\\rm B}\" eeimg=\"1\"/> 不相交。简单证明下，假设存在 <img src=\"https://www.zhihu.com/equation?tex=%5Cleft%28+%7Bu%2Cv%2Ct%7D+%5Cright%29+%5Cin+%7B%5Crm+A%7D+%5Ccap+%7B%5Crm+B%7D\" alt=\"\\left( {u,v,t} \\right) \\in {\\rm A} \\cap {\\rm B}\" eeimg=\"1\"/> ，因为 <img src=\"https://www.zhihu.com/equation?tex=%5Cleft%28+%7Bu%2Cv%2Ct%7D+%5Cright%29+%5Cin+%7B%5Crm+B%7D\" alt=\"\\left( {u,v,t} \\right) \\in {\\rm B}\" eeimg=\"1\"/> ，所以有 <img src=\"https://www.zhihu.com/equation?tex=t+%3C+%7Bp%5E%2A%7D\" alt=\"t &lt; {p^*}\" eeimg=\"1\"/> ，又因为 <img src=\"https://www.zhihu.com/equation?tex=%5Cleft%28+%7Bu%2Cv%2Ct%7D+%5Cright%29+%5Cin+%7B%5Crm+A%7D\" alt=\"\\left( {u,v,t} \\right) \\in {\\rm A}\" eeimg=\"1\"/> 所以有 <img src=\"https://www.zhihu.com/equation?tex=%7Bf_0%7D%5Cleft%28+x+%5Cright%29+%5Cle+t+%3C+%7Bp%5E%2A%7D\" alt=\"{f_0}\\left( x \\right) \\le t &lt; {p^*}\" eeimg=\"1\"/> ，与 <img src=\"https://www.zhihu.com/equation?tex=%7Bp%5E%2A%7D\" alt=\"{p^*}\" eeimg=\"1\"/> 是原问题最优解相矛盾，所以集合 <img src=\"https://www.zhihu.com/equation?tex=%7B%5Crm+A%7D\" alt=\"{\\rm A}\" eeimg=\"1\"/> 和集合 <img src=\"https://www.zhihu.com/equation?tex=%7B%5Crm+B%7D\" alt=\"{\\rm B}\" eeimg=\"1\"/> 不相交。</p><p>根据分离超平面定理，存在 <img src=\"https://www.zhihu.com/equation?tex=%5Cleft%28+%7B%5Ctilde+%5Clambda+%2C%5Ctilde+%5Cnu+%2C%5Cmu+%7D+%5Cright%29+%5Cne+0\" alt=\"\\left( {\\tilde \\lambda ,\\tilde \\nu ,\\mu } \\right) \\ne 0\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5Calpha+\" alt=\"\\alpha \" eeimg=\"1\"/> 使得：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Cleft%28+%7Bu%2Cv%2Ct%7D+%5Cright%29+%5Cin+%7B%5Crm+A%7D+%5CRightarrow+%7B%7B%5Ctilde+%5Clambda+%7D%5ET%7Du+%2B+%7B%7B%5Ctilde+%5Cnu+%7D%5ET%7Dv+%2B+%5Cmu+t+%5Cge+%5Calpha+%5Ctag%7B13%7D%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} \\left( {u,v,t} \\right) \\in {\\rm A} \\Rightarrow {{\\tilde \\lambda }^T}u + {{\\tilde \\nu }^T}v + \\mu t \\ge \\alpha \\tag{13}\\end{align*}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Cleft%28+%7Bu%2Cv%2Ct%7D+%5Cright%29+%5Cin+%7B%5Crm+B%7D+%5CRightarrow+%7B%7B%5Ctilde+%5Clambda+%7D%5ET%7Du+%2B+%7B%7B%5Ctilde+%5Cnu+%7D%5ET%7Dv+%2B+%5Cmu+t+%5Cle+%5Calpha+%5Ctag%7B14%7D%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} \\left( {u,v,t} \\right) \\in {\\rm B} \\Rightarrow {{\\tilde \\lambda }^T}u + {{\\tilde \\nu }^T}v + \\mu t \\le \\alpha \\tag{14}\\end{align*}\" eeimg=\"1\"/> </p><p>根据 (13) 有 <img src=\"https://www.zhihu.com/equation?tex=%5Ctilde+%5Clambda+%5Csucceq+0%2C%5Cmu+%5Cge+0\" alt=\"\\tilde \\lambda \\succeq 0,\\mu \\ge 0\" eeimg=\"1\"/> （因为 <img src=\"https://www.zhihu.com/equation?tex=u%2Ct\" alt=\"u,t\" eeimg=\"1\"/> 是无下界的，如果 <img src=\"https://www.zhihu.com/equation?tex=%5Ctilde+%5Clambda+%2C%5Cmu+\" alt=\"\\tilde \\lambda ,\\mu \" eeimg=\"1\"/> 任意一个是负数，则 <img src=\"https://www.zhihu.com/equation?tex=%7B%7B%5Ctilde+%5Clambda+%7D%5ET%7Du+%2B+%7B%7B%5Ctilde+%5Cnu+%7D%5ET%7Dv+%2B+%5Cmu+t\" alt=\"{{\\tilde \\lambda }^T}u + {{\\tilde \\nu }^T}v + \\mu t\" eeimg=\"1\"/> 在 <img src=\"https://www.zhihu.com/equation?tex=%7B%5Crm+A%7D\" alt=\"{\\rm A}\" eeimg=\"1\"/> 上无下界），根据公式 (12) 和公式 (14) 有对于任意 <img src=\"https://www.zhihu.com/equation?tex=t+%3C+%7Bp%5E%2A%7D\" alt=\"t &lt; {p^*}\" eeimg=\"1\"/> ，满足 <img src=\"https://www.zhihu.com/equation?tex=%5Cmu+t+%5Cle+%5Calpha\" alt=\"\\mu t \\le \\alpha\" eeimg=\"1\"/> ，则 <img src=\"https://www.zhihu.com/equation?tex=%5Cmu+%7Bp%5E%2A%7D+%5Cle+%5Calpha+\" alt=\"\\mu {p^*} \\le \\alpha \" eeimg=\"1\"/> （假设 <img src=\"https://www.zhihu.com/equation?tex=%5Cmu+%7Bp%5E%2A%7D+%3E+%5Calpha+\" alt=\"\\mu {p^*} &gt; \\alpha \" eeimg=\"1\"/> ，则肯定存在无限小的 <img src=\"https://www.zhihu.com/equation?tex=%5Cvarepsilon+%3E+0\" alt=\"\\varepsilon &gt; 0\" eeimg=\"1\"/> 满足 <img src=\"https://www.zhihu.com/equation?tex=%5Cmu+%5Cleft%28+%7B%7Bp%5E%2A%7D+-+%5Cvarepsilon+%7D+%5Cright%29+%3E+%5Calpha+\" alt=\"\\mu \\left( {{p^*} - \\varepsilon } \\right) &gt; \\alpha \" eeimg=\"1\"/> ，与对于任意 <img src=\"https://www.zhihu.com/equation?tex=t+%3C+%7Bp%5E%2A%7D\" alt=\"t &lt; {p^*}\" eeimg=\"1\"/> ，满足 <img src=\"https://www.zhihu.com/equation?tex=%5Cmu+t+%5Cle+%5Calpha\" alt=\"\\mu t \\le \\alpha\" eeimg=\"1\"/> 矛盾）。再结合公式 (13)得：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Cbegin%7Barray%7D%7Bl%7D+%7B%7B%5Ctilde+%5Clambda+%7D%5ET%7Du+%2B+%7B%7B%5Ctilde+%5Cnu+%7D%5ET%7Dv+%2B+%5Cmu+t+%5Cge+%5Calpha+%5Cge+%5Cmu+%7Bp%5E%2A%7D%5C%5C+%5Csum%5Climits_%7Bi+%3D+1%7D%5Em+%7B%7B%7B%5Ctilde+%5Clambda+%7D_i%7D%7Bf_i%7D%5Cleft%28+x+%5Cright%29%7D+%2B+%7B%7B%5Ctilde+%5Cnu+%7D%5ET%7D%5Cleft%28+%7BAx+-+b%7D+%5Cright%29+%2B+%5Cmu+%7Bf_0%7D%5Cleft%28+x+%5Cright%29+%5Cge+%5Calpha+%5Cge+%5Cmu+%7Bp%5E%2A%7D+%5Cend%7Barray%7D+%5Ctag%7B15%7D%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} \\begin{array}{l} {{\\tilde \\lambda }^T}u + {{\\tilde \\nu }^T}v + \\mu t \\ge \\alpha \\ge \\mu {p^*}\\\\ \\sum\\limits_{i = 1}^m {{{\\tilde \\lambda }_i}{f_i}\\left( x \\right)} + {{\\tilde \\nu }^T}\\left( {Ax - b} \\right) + \\mu {f_0}\\left( x \\right) \\ge \\alpha \\ge \\mu {p^*} \\end{array} \\tag{15}\\end{align*}\" eeimg=\"1\"/> </p><ul><li>当 <img src=\"https://www.zhihu.com/equation?tex=%5Cmu+%3E+0\" alt=\"\\mu &gt; 0\" eeimg=\"1\"/> ，公式 (15) 两边除以 <img src=\"https://www.zhihu.com/equation?tex=%5Cmu+\" alt=\"\\mu \" eeimg=\"1\"/> 得：</li></ul><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+L%5Cleft%28+%7Bx%2C%5Ctilde+%5Clambda+%2F%5Cmu+%2C%5Ctilde+%5Cnu+%2F%5Cmu+%7D+%5Cright%29+%5Cge+%7Bp%5E%2A%7D+%5Ctag%7B16%7D%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} L\\left( {x,\\tilde \\lambda /\\mu ,\\tilde \\nu /\\mu } \\right) \\ge {p^*} \\tag{16}\\end{align*}\" eeimg=\"1\"/> </p><p>令 <img src=\"https://www.zhihu.com/equation?tex=%5Clambda+%3D+%5Ctilde+%5Clambda+%2F%5Cmu+%2C%5Cnu+%3D+%5Ctilde+%5Cnu+%2F%5Cmu\" alt=\"\\lambda = \\tilde \\lambda /\\mu ,\\nu = \\tilde \\nu /\\mu\" eeimg=\"1\"/> ，则 <img src=\"https://www.zhihu.com/equation?tex=L%5Cleft%28+%7Bx%2C%5Clambda+%2C%5Cnu+%7D+%5Cright%29+%5Cge+%7Bp%5E%2A%7D\" alt=\"L\\left( {x,\\lambda ,\\nu } \\right) \\ge {p^*}\" eeimg=\"1\"/> ，所以</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+g%5Cleft%28+%7B%5Clambda+%2C%5Cnu+%7D+%5Cright%29+%3D+%5Cmathop+%7B%5Cinf+%7D%5Climits_%7Bx+%5Cin+D%7D+L%5Cleft%28+%7Bx%2C%5Clambda+%2C%5Cnu+%7D+%5Cright%29+%5Cge+%7Bp%5E%2A%7D+%5Ctag%7B17%7D%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} g\\left( {\\lambda ,\\nu } \\right) = \\mathop {\\inf }\\limits_{x \\in D} L\\left( {x,\\lambda ,\\nu } \\right) \\ge {p^*} \\tag{17}\\end{align*}\" eeimg=\"1\"/> </p><p>结合公式 (4) 可以得到 <img src=\"https://www.zhihu.com/equation?tex=g%5Cleft%28+%7B%5Clambda+%2C%5Cnu+%7D+%5Cright%29+%3D+%7Bp%5E%2A%7D\" alt=\"g\\left( {\\lambda ,\\nu } \\right) = {p^*}\" eeimg=\"1\"/> ，即强对偶性成立。</p><ul><li>当 <img src=\"https://www.zhihu.com/equation?tex=%5Cmu+%7B%5Crm%7B+%3D+%7D%7D0\" alt=\"\\mu {\\rm{ = }}0\" eeimg=\"1\"/> ，由公式 (14) 得 <img src=\"https://www.zhihu.com/equation?tex=%5Cmu+t+%5Cle+%5Calpha+%5CRightarrow+%5Calpha+%5Cge+0\" alt=\"\\mu t \\le \\alpha \\Rightarrow \\alpha \\ge 0\" eeimg=\"1\"/> ，再有公式 (13) 得：</li></ul><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Csum%5Climits_%7Bi+%3D+1%7D%5Em+%7B%7B%7B%5Ctilde+%5Clambda+%7D_i%7D%7Bf_i%7D%5Cleft%28+x+%5Cright%29%7D+%2B+%7B%7B%5Ctilde+%5Cnu+%7D%5ET%7D%5Cleft%28+%7BAx+-+b%7D+%5Cright%29+%5Cge+0+%5Ctag%7B18%7D%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} \\sum\\limits_{i = 1}^m {{{\\tilde \\lambda }_i}{f_i}\\left( x \\right)} + {{\\tilde \\nu }^T}\\left( {Ax - b} \\right) \\ge 0 \\tag{18}\\end{align*}\" eeimg=\"1\"/> </p><p>存在一个点 <img src=\"https://www.zhihu.com/equation?tex=%7B%5Ctilde+x%7D\" alt=\"{\\tilde x}\" eeimg=\"1\"/> 满足Slater 条件， <img src=\"https://www.zhihu.com/equation?tex=A%5Ctilde+x+%3D+b%2C%7Bf_i%7D%5Cleft%28+%7B%5Ctilde+x%7D+%5Cright%29+%3C+0\" alt=\"A\\tilde x = b,{f_i}\\left( {\\tilde x} \\right) &lt; 0\" eeimg=\"1\"/> ，所以得：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Csum%5Climits_%7Bi+%3D+1%7D%5Em+%7B%7B%7B%5Ctilde+%5Clambda+%7D_i%7D%7Bf_i%7D%5Cleft%28%7B%5Ctilde+x%7D+%5Cright%29%7D+%5Cge+0+%5Ctag%7B19%7D%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} \\sum\\limits_{i = 1}^m {{{\\tilde \\lambda }_i}{f_i}\\left({\\tilde x} \\right)} \\ge 0 \\tag{19}\\end{align*}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%7Bf_i%7D%5Cleft%28+%7B%5Ctilde+x%7D+%5Cright%29+%3C+0%2C%7B%7B%5Ctilde+%5Clambda+%7D_i%7D+%5Cge+0\" alt=\"{f_i}\\left( {\\tilde x} \\right) &lt; 0,{{\\tilde \\lambda }_i} \\ge 0\" eeimg=\"1\"/> ，所以 <img src=\"https://www.zhihu.com/equation?tex=%5Ctilde+%5Clambda+%3D+0\" alt=\"\\tilde \\lambda = 0\" eeimg=\"1\"/> 。再结合 <img src=\"https://www.zhihu.com/equation?tex=%5Cleft%28+%7B%5Ctilde+%5Clambda+%2C%5Ctilde+%5Cnu+%2C%5Cmu+%7D+%5Cright%29+%5Cne+0\" alt=\"\\left( {\\tilde \\lambda ,\\tilde \\nu ,\\mu } \\right) \\ne 0\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5Cmu+%3D+0\" alt=\"\\mu = 0\" eeimg=\"1\"/> ，则 <img src=\"https://www.zhihu.com/equation?tex=%5Ctilde+%5Cnu+%5Cne+0\" alt=\"\\tilde \\nu \\ne 0\" eeimg=\"1\"/> ，再结合公式 (18)，对于任意 <img src=\"https://www.zhihu.com/equation?tex=x+%5Cin+D\" alt=\"x \\in D\" eeimg=\"1\"/> 则满足：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%7B%7B%5Ctilde+%5Cnu+%7D%5ET%7D%5Cleft%28+%7BAx+-+b%7D+%5Cright%29+%5Cge+0+%5Ctag%7B20%7D%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} {{\\tilde \\nu }^T}\\left( {Ax - b} \\right) \\ge 0 \\tag{20}\\end{align*}\" eeimg=\"1\"/> </p><p>又因为 <img src=\"https://www.zhihu.com/equation?tex=%7B%5Ctilde+x%7D+%5Cin+%5Crm%7Brelint%7D%5C+D\" alt=\"{\\tilde x} \\in \\rm{relint}\\ D\" eeimg=\"1\"/> 满足 <img src=\"https://www.zhihu.com/equation?tex=%7B%7B%5Ctilde+%5Cnu+%7D%5ET%7D%5Cleft%28+%7BA%5Ctilde+x+-+b%7D+%5Cright%29+%3D+0\" alt=\"{{\\tilde \\nu }^T}\\left( {A\\tilde x - b} \\right) = 0\" eeimg=\"1\"/> ，肯定存在一个点 <img src=\"https://www.zhihu.com/equation?tex=x+%5Cin+D\" alt=\"x \\in D\" eeimg=\"1\"/>满足 <img src=\"https://www.zhihu.com/equation?tex=%7B%7B%5Ctilde+%5Cnu+%7D%5ET%7D%5Cleft%28+%7BAx+-+b%7D+%5Cright%29+%3C+0\" alt=\"{{\\tilde \\nu }^T}\\left( {Ax - b} \\right) &lt; 0\" eeimg=\"1\"/> （类似线性函数在定义域内部而不是边界有零点，肯定存在大于 <img src=\"https://www.zhihu.com/equation?tex=0\" alt=\"0\" eeimg=\"1\"/> 和小于 <img src=\"https://www.zhihu.com/equation?tex=0\" alt=\"0\" eeimg=\"1\"/> 的点），与公式 (20) 矛盾，所以 <img src=\"https://www.zhihu.com/equation?tex=%5Cmu+\" alt=\"\\mu \" eeimg=\"1\"/> 不为 <img src=\"https://www.zhihu.com/equation?tex=0\" alt=\"0\" eeimg=\"1\"/> 。</p><p>综上证明得满足Slater 条件，则强对偶性成立。</p><h2><b>三、KKT条件</b></h2><p><b>1. 互补松弛性</b></p><p>假设强对偶性成立， <img src=\"https://www.zhihu.com/equation?tex=%7Bx%5E%2A%7D\" alt=\"{x^*}\" eeimg=\"1\"/> 是原问题的最优解， <img src=\"https://www.zhihu.com/equation?tex=%5Cleft%28+%7B%7B%5Clambda+%5E%2A%7D%2C%7B%5Cnu+%5E%2A%7D%7D+%5Cright%29\" alt=\"\\left( {{\\lambda ^*},{\\nu ^*}} \\right)\" eeimg=\"1\"/> 是对偶问题的最优解。</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%7Bf_0%7D%5Cleft%28+%7B%7Bx%5E%2A%7D%7D+%5Cright%29+%26%3D+g%5Cleft%28+%7B%7B%5Clambda+%5E%2A%7D%2C%7B%5Cnu+%5E%2A%7D%7D+%5Cright%29%5C%5C+%26%3D+%5Cmathop+%7B%5Cinf+%7D%5Climits_%7Bx+%5Cin+D%7D+%5Cleft%28+%7B%7Bf_0%7D%5Cleft%28+x+%5Cright%29+%2B+%5Csum%5Climits_%7Bi+%3D+1%7D%5Em+%7B%5Clambda+_i%5E%2A%7Bf_i%7D%5Cleft%28+x+%5Cright%29%7D+%2B+%5Csum%5Climits_%7Bi+%3D+1%7D%5Ep+%7B%5Cnu+_i%5E%2A%7Bh_i%7D%5Cleft%28+x+%5Cright%29%7D+%7D+%5Cright%29%5C%5C+%26+%5Cle+%7Bf_0%7D%5Cleft%28+%7B%7Bx%5E%2A%7D%7D+%5Cright%29+%2B+%5Csum%5Climits_%7Bi+%3D+1%7D%5Em+%7B%5Clambda+_i%5E%2A%7Bf_i%7D%5Cleft%28+%7B%7Bx%5E%2A%7D%7D+%5Cright%29%7D+%2B+%5Csum%5Climits_%7Bi+%3D+1%7D%5Ep+%7B%5Cnu+_i%5E%2A%7Bh_i%7D%5Cleft%28+%7B%7Bx%5E%2A%7D%7D+%5Cright%29%7D+%5C%5C+%26+%5Cle+%7Bf_0%7D%5Cleft%28+%7B%7Bx%5E%2A%7D%7D+%5Cright%29+%5Ctag%7B21%7D%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} {f_0}\\left( {{x^*}} \\right) &amp;= g\\left( {{\\lambda ^*},{\\nu ^*}} \\right)\\\\ &amp;= \\mathop {\\inf }\\limits_{x \\in D} \\left( {{f_0}\\left( x \\right) + \\sum\\limits_{i = 1}^m {\\lambda _i^*{f_i}\\left( x \\right)} + \\sum\\limits_{i = 1}^p {\\nu _i^*{h_i}\\left( x \\right)} } \\right)\\\\ &amp; \\le {f_0}\\left( {{x^*}} \\right) + \\sum\\limits_{i = 1}^m {\\lambda _i^*{f_i}\\left( {{x^*}} \\right)} + \\sum\\limits_{i = 1}^p {\\nu _i^*{h_i}\\left( {{x^*}} \\right)} \\\\ &amp; \\le {f_0}\\left( {{x^*}} \\right) \\tag{21}\\end{align*}\" eeimg=\"1\"/> </p><p>所以可以得到 <img src=\"https://www.zhihu.com/equation?tex=%5Csum%5Climits_%7Bi+%3D+1%7D%5Em+%7B%5Clambda+_i%5E%2A%7Bf_i%7D%5Cleft%28+%7B%7Bx%5E%2A%7D%7D+%5Cright%29%7D+%2B+%5Csum%5Climits_%7Bi+%3D+1%7D%5Ep+%7B%5Cnu+_i%5E%2A%7Bh_i%7D%5Cleft%28+%7B%7Bx%5E%2A%7D%7D+%5Cright%29%7D+%3D+0+%5CRightarrow+%5Csum%5Climits_%7Bi+%3D+1%7D%5Em+%7B%5Clambda+_i%5E%2A%7Bf_i%7D%5Cleft%28+%7B%7Bx%5E%2A%7D%7D+%5Cright%29%7D+%3D+0\" alt=\"\\sum\\limits_{i = 1}^m {\\lambda _i^*{f_i}\\left( {{x^*}} \\right)} + \\sum\\limits_{i = 1}^p {\\nu _i^*{h_i}\\left( {{x^*}} \\right)} = 0 \\Rightarrow \\sum\\limits_{i = 1}^m {\\lambda _i^*{f_i}\\left( {{x^*}} \\right)} = 0\" eeimg=\"1\"/> ，又因为 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Clambda+_i%5E%2A+%5Cge+0%2C%7Bf_i%7D%5Cleft%28+%7B%7Bx%5E%2A%7D%7D+%5Cright%29+%5Cle+0%5C%5D\" alt=\"\\[\\lambda _i^* \\ge 0,{f_i}\\left( {{x^*}} \\right) \\le 0\\]\" eeimg=\"1\"/> ，所以可以得到：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Clambda+_i%5E%2A%7Bf_i%7D%5Cleft%28+%7B%7Bx%5E%2A%7D%7D+%5Cright%29+%3D+0%2C%5C+%5C+%5C+i+%3D+1%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2Cm+%5Ctag%7B22%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} \\lambda _i^*{f_i}\\left( {{x^*}} \\right) = 0,\\ \\ \\ i = 1, \\cdot \\cdot \\cdot ,m \\tag{22} \\end{align*}\" eeimg=\"1\"/> </p><p><b>2. KKT 最优性条件</b></p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Cnabla+%7Bf_0%7D%5Cleft%28+%7B%7Bx%5E%2A%7D%7D+%5Cright%29+%2B+%5Csum%5Climits_%7Bi+%3D+1%7D%5Em+%7B%5Clambda+_i%5E%2A%5Cnabla+%7Bf_i%7D%5Cleft%28+%7B%7Bx%5E%2A%7D%7D+%5Cright%29%7D+%2B+%5Csum%5Climits_%7Bi+%3D+1%7D%5Ep+%7B%5Cnu+_i%5E%2A%5Cnabla+%7Bh_i%7D%5Cleft%28+%7B%7Bx%5E%2A%7D%7D+%5Cright%29%7D+%3D+0%5C%5C+%7Bf_i%7D%5Cleft%28+%7B%7Bx%5E%2A%7D%7D+%5Cright%29+%5Cle+0%2C%5C+%5C+%5C+i+%3D+1%2C+%5Ccdot+%5Ccdot+%5Ccdot+m%5C%5C+%7Bh_i%7D%5Cleft%28+%7B%7Bx%5E%2A%7D%7D+%5Cright%29+%3D+0%2C%5C+%5C+%5C+i+%3D+1%2C+%5Ccdot+%5Ccdot+%5Ccdot+p%5C%5C+%5Clambda+_i%5E%2A+%5Cge+0%2C%5C+%5C+%5C+i+%3D+1%2C+%5Ccdot+%5Ccdot+%5Ccdot+m%5C%5C+%5Clambda+_i%5E%2A%7Bf_i%7D%5Cleft%28+%7B%7Bx%5E%2A%7D%7D+%5Cright%29+%3D+0%2C%5C+%5C+%5C+i+%3D+1%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2Cm+%5Ctag%7B23%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} \\nabla {f_0}\\left( {{x^*}} \\right) + \\sum\\limits_{i = 1}^m {\\lambda _i^*\\nabla {f_i}\\left( {{x^*}} \\right)} + \\sum\\limits_{i = 1}^p {\\nu _i^*\\nabla {h_i}\\left( {{x^*}} \\right)} = 0\\\\ {f_i}\\left( {{x^*}} \\right) \\le 0,\\ \\ \\ i = 1, \\cdot \\cdot \\cdot m\\\\ {h_i}\\left( {{x^*}} \\right) = 0,\\ \\ \\ i = 1, \\cdot \\cdot \\cdot p\\\\ \\lambda _i^* \\ge 0,\\ \\ \\ i = 1, \\cdot \\cdot \\cdot m\\\\ \\lambda _i^*{f_i}\\left( {{x^*}} \\right) = 0,\\ \\ \\ i = 1, \\cdot \\cdot \\cdot ,m \\tag{23} \\end{align*}\" eeimg=\"1\"/> </p><p>当原问题是凸问题时，满足KKT条件的点也是原问题和对偶问题的最优解。</p><p><a href=\"https://zhuanlan.zhihu.com/p/43043846\" class=\"internal\">下一篇</a></p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "SVM", 
                    "tagLink": "https://api.zhihu.com/topics/19583524"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/42876464", 
            "userName": "徽州风韵", 
            "userLink": "https://www.zhihu.com/people/53bd6f8b93873b03c5edb714aea2a456", 
            "upvote": 0, 
            "title": "矩阵求导", 
            "content": "<p>回顾下高数里介绍的全微分公式：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+df+%3D+%5Csum%5Climits_%7Bi%7D+%7B%5Cfrac%7B%7B%5Cpartial+f%7D%7D%7B%7B%5Cpartial+%7Bx_i%7D%7D%7Dd%7Bx_i%7D%7D+%3D+%7B%5Cfrac%7B%7B%5Cpartial+f%7D%7D%7B%7B%5Cpartial+%7B%5Crm%7Bx%7D%7D%7D%7D%5ET%7Dd%7B%5Crm%7Bx%7D%7D+%5Ctag%7B1%7D%5Cend%7Balign%2A%7D+\" alt=\"\\begin{align*} df = \\sum\\limits_{i} {\\frac{{\\partial f}}{{\\partial {x_i}}}d{x_i}} = {\\frac{{\\partial f}}{{\\partial {\\rm{x}}}}^T}d{\\rm{x}} \\tag{1}\\end{align*} \" eeimg=\"1\"/> </p><p>标量对矩阵的求导同样也可以用类似全微分公式表示：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+df+%3D+%5Csum%5Climits_%7Bi%2Cj%7D+%7B%5Cfrac%7B%7B%5Cpartial+f%7D%7D%7B%7B%5Cpartial+%7Bx_%7Bi%2Cj%7D%7D%7D%7Dd%7Bx_%7Bi%2Cj%7D%7D%7D+%3D+tr%5Cleft%28+%7B%7B%7B%5Cfrac%7B%7B%5Cpartial+f%7D%7D%7B%7B%5Cpartial+%7B%5Crm%7BX%7D%7D%7D%7D%7D%5ET%7Dd%7B%5Crm%7BX%7D%7D%7D+%5Cright%29+%5Ctag%7B2%7D%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} df = \\sum\\limits_{i,j} {\\frac{{\\partial f}}{{\\partial {x_{i,j}}}}d{x_{i,j}}} = tr\\left( {{{\\frac{{\\partial f}}{{\\partial {\\rm{X}}}}}^T}d{\\rm{X}}} \\right) \\tag{2}\\end{align*}\" eeimg=\"1\"/> </p><p>所以我们可以得到标量关于矩阵的导数，其中 <img src=\"https://www.zhihu.com/equation?tex=tr\" alt=\"tr\" eeimg=\"1\"/> 表示迹是方阵对角线元素之和， <img src=\"https://www.zhihu.com/equation?tex=%5C%5Btr%5Cleft%28+%7B%7BA%5ET%7DB%7D+%5Cright%29+%3D+%5Csum%5Climits_%7Bi%2Cj%7D+%7B%7BA_%7Bi%2Cj%7D%7D%7BB_%7Bij%7D%7D%7D+%5C%5D\" alt=\"\\[tr\\left( {{A^T}B} \\right) = \\sum\\limits_{i,j} {{A_{i,j}}{B_{ij}}} \\]\" eeimg=\"1\"/> ，下面给出迹的一些性质：</p><ul><li>标量的迹等于标量： <img src=\"https://www.zhihu.com/equation?tex=%5C%5Ba+%3D+tr%5Cleft%28+a+%5Cright%29%5C%5D\" alt=\"\\[a = tr\\left( a \\right)\\]\" eeimg=\"1\"/> ；</li><li>转置： <img src=\"https://www.zhihu.com/equation?tex=%5C%5Btr%5Cleft%28+%7B%7BA%5ET%7D%7D+%5Cright%29+%3D+tr%5Cleft%28+A+%5Cright%29%5C%5D\" alt=\"\\[tr\\left( {{A^T}} \\right) = tr\\left( A \\right)\\]\" eeimg=\"1\"/> ；</li><li>线性： <img src=\"https://www.zhihu.com/equation?tex=%5C%5Btr%5Cleft%28+%7Ba+%5Ccdot+A+%5Cpm+b+%5Ccdot+B%7D+%5Cright%29+%3D+a+%5Ccdot+tr%5Cleft%28+A+%5Cright%29+%5Cpm+b+%5Ccdot+tr%5Cleft%28+B+%5Cright%29%5C%5D\" alt=\"\\[tr\\left( {a \\cdot A \\pm b \\cdot B} \\right) = a \\cdot tr\\left( A \\right) \\pm b \\cdot tr\\left( B \\right)\\]\" eeimg=\"1\"/> ；</li><li>循环性质： <img src=\"https://www.zhihu.com/equation?tex=%5C%5Btr%5Cleft%28+%7BABC%7D+%5Cright%29+%3D+tr%5Cleft%28+%7BCAB%7D+%5Cright%29+%3D+tr%5Cleft%28+%7BBCA%7D+%5Cright%29%5C%5D\" alt=\"\\[tr\\left( {ABC} \\right) = tr\\left( {CAB} \\right) = tr\\left( {BCA} \\right)\\]\" eeimg=\"1\"/> ；</li><li>逐元素乘法交换： <img src=\"https://www.zhihu.com/equation?tex=%5C%5Btr%5Cleft%28+%7B%7BA%5ET%7D%5Cleft%28+%7BB+%5Codot+C%7D+%5Cright%29%7D+%5Cright%29+%3D+tr%5Cleft%28+%7B%7B%7B%5Cleft%28+%7BA+%5Codot+B%7D+%5Cright%29%7D%5ET%7DC%7D+%5Cright%29%5C%5D\" alt=\"\\[tr\\left( {{A^T}\\left( {B \\odot C} \\right)} \\right) = tr\\left( {{{\\left( {A \\odot B} \\right)}^T}C} \\right)\\]\" eeimg=\"1\"/> </li></ul><p>矩阵微分的性质：</p><ul><li>加减法： <img src=\"https://www.zhihu.com/equation?tex=%5C%5Bd%5Cleft%28+%7BX+%5Cpm+Y%7D+%5Cright%29+%3D+dX+%5Cpm+dY%5C%5D\" alt=\"\\[d\\left( {X \\pm Y} \\right) = dX \\pm dY\\]\" eeimg=\"1\"/> ；</li><li>矩阵乘法： <img src=\"https://www.zhihu.com/equation?tex=%5C%5Bd%5Cleft%28+%7BXY%7D+%5Cright%29+%3D+%5Cleft%28+%7BdX%7D+%5Cright%29Y+%2B+X%5Cleft%28+%7BdY%7D+%5Cright%29%5C%5D\" alt=\"\\[d\\left( {XY} \\right) = \\left( {dX} \\right)Y + X\\left( {dY} \\right)\\]\" eeimg=\"1\"/> ；</li><li>逐元素乘法： <img src=\"https://www.zhihu.com/equation?tex=%5C%5Bd%5Cleft%28+%7BX+%5Codot+Y%7D+%5Cright%29+%3D+%5Cleft%28+%7BdX%7D+%5Cright%29+%5Codot+Y+%2B+X+%5Codot+%5Cleft%28+%7BdY%7D+%5Cright%29%5C%5D\" alt=\"\\[d\\left( {X \\odot Y} \\right) = \\left( {dX} \\right) \\odot Y + X \\odot \\left( {dY} \\right)\\]\" eeimg=\"1\"/> ；</li><li>逐元素函数： <img src=\"https://www.zhihu.com/equation?tex=%5C%5Bd%5Csigma+%5Cleft%28+X+%5Cright%29+%3D+%5Csigma+%27%5Cleft%28+X+%5Cright%29+%5Codot+dX%5C%5D\" alt=\"\\[d\\sigma \\left( X \\right) = \\sigma &#39;\\left( X \\right) \\odot dX\\]\" eeimg=\"1\"/> </li></ul><p><a href=\"https://zhuanlan.zhihu.com/p/24709748\" class=\"internal\">矩阵求导术</a></p>", 
            "topic": [
                {
                    "tag": "矩阵", 
                    "tagLink": "https://api.zhihu.com/topics/19650614"
                }, 
                {
                    "tag": "矩阵运算", 
                    "tagLink": "https://api.zhihu.com/topics/19598145"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/42860420", 
            "userName": "徽州风韵", 
            "userLink": "https://www.zhihu.com/people/53bd6f8b93873b03c5edb714aea2a456", 
            "upvote": 1, 
            "title": "Latent Dirichlet Allocation-下篇", 
            "content": "<h2><b>二、主题模型</b></h2><h2><b>2.1 Unigram Mode</b></h2><p>假设我们的词典中一共有 <img src=\"https://www.zhihu.com/equation?tex=V\" alt=\"V\" eeimg=\"1\"/> 个词，Unigram Model就是认为上帝按照下面游戏规则产生文本的。</p><p><b>Game 2.1 Unigram Model</b></p><blockquote> 1. 上帝只有一个骰子，这个骰子有 <img src=\"https://www.zhihu.com/equation?tex=+V+\" alt=\" V \" eeimg=\"1\"/> 个面，每个面对应一个词，各个面概率不同；<br/> 2. 每抛一次骰子，抛出的面就对应的产生一个词，如果一篇文档中有 <img src=\"https://www.zhihu.com/equation?tex=+n+\" alt=\" n \" eeimg=\"1\"/> 个词，那么就独立地抛 <img src=\"https://www.zhihu.com/equation?tex=+n+\" alt=\" n \" eeimg=\"1\"/> 次骰子产生这 <img src=\"https://www.zhihu.com/equation?tex=+n+\" alt=\" n \" eeimg=\"1\"/> 个词</blockquote><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-2d7146b8b99b4292745cb5a979acebe7_b.jpg\" data-size=\"normal\" data-rawwidth=\"403\" data-rawheight=\"193\" class=\"content_image\" width=\"403\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;403&#39; height=&#39;193&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"403\" data-rawheight=\"193\" class=\"content_image lazy\" width=\"403\" data-actualsrc=\"https://pic4.zhimg.com/v2-2d7146b8b99b4292745cb5a979acebe7_b.jpg\"/><figcaption>图 2.1</figcaption></figure><p>骰子各个面的概率记为 <img src=\"https://www.zhihu.com/equation?tex=+%5Cvec+p+%3D+%5Cleft%28+%7B%7Bp_1%7D%2C%7Bp_2%7D%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2C%7Bp_V%7D%7D+%5Cright%29\" alt=\" \\vec p = \\left( {{p_1},{p_2}, \\cdot \\cdot \\cdot ,{p_V}} \\right)\" eeimg=\"1\"/> ，对于一篇文档 <img src=\"https://www.zhihu.com/equation?tex=+%5Cvec+w+%3D+%5Cleft%28+%7B%7Bw_1%7D%2C%7Bw_2%7D%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2C%7Bw_n%7D%7D+%5Cright%29\" alt=\" \\vec w = \\left( {{w_1},{w_2}, \\cdot \\cdot \\cdot ,{w_n}} \\right)\" eeimg=\"1\"/> ，生成该文档的概率为：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+p%5Cleft%28+%7B%7Bw_1%7D%2C%7Bw_2%7D%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2C%7Bw_n%7D%7D+%5Cright%29+%3D+%5Cprod%5Climits_%7Bi+%3D+1%7D%5En+%7Bp%5Cleft%28+%7B%7Bw_i%7D%7D+%5Cright%29%7D+%5Ctag%7B59%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} p\\left( {{w_1},{w_2}, \\cdot \\cdot \\cdot ,{w_n}} \\right) = \\prod\\limits_{i = 1}^n {p\\left( {{w_i}} \\right)} \\tag{59} \\end{align*}\" eeimg=\"1\"/> </p><p>假设我们预料是由 <img src=\"https://www.zhihu.com/equation?tex=+m+\" alt=\" m \" eeimg=\"1\"/> 篇文档组成即 <img src=\"https://www.zhihu.com/equation?tex=+W+%3D+%5Cleft%28+%7B%7B%7B%5Cvec+w%7D_1%7D%2C%7B%7B%5Cvec+w%7D_2%7D%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2C%7B%7B%5Cvec+w%7D_m%7D%7D+%5Cright%29\" alt=\" W = \\left( {{{\\vec w}_1},{{\\vec w}_2}, \\cdot \\cdot \\cdot ,{{\\vec w}_m}} \\right)\" eeimg=\"1\"/> ，每篇文档是相互独立的，则该预料的概率为：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+p%5Cleft%28+%7BW%7C%5Cvec+p%7D+%5Cright%29+%3D+%5Cprod%5Climits_%7Bi+%3D+1%7D%5En+%7Bp%5Cleft%28+%7B%7B%7B%5Cvec+w%7D_i%7D%7C%5Cvec+p%7D+%5Cright%29%7D+%5Ctag%7B60%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} p\\left( {W|\\vec p} \\right) = \\prod\\limits_{i = 1}^n {p\\left( {{{\\vec w}_i}|\\vec p} \\right)} \\tag{60} \\end{align*}\" eeimg=\"1\"/> </p><p>假设预料中总共有 <img src=\"https://www.zhihu.com/equation?tex=+N+\" alt=\" N \" eeimg=\"1\"/> 个词，每个词 <img src=\"https://www.zhihu.com/equation?tex=+%7Bw_i%7D+\" alt=\" {w_i} \" eeimg=\"1\"/> 的词频为 <img src=\"https://www.zhihu.com/equation?tex=+%7Bn_i%7D+\" alt=\" {n_i} \" eeimg=\"1\"/> ，那么 <img src=\"https://www.zhihu.com/equation?tex=%5Cvec+n+%3D+%5Cleft%28+%7B%7Bn_1%7D%2C%7Bn_2%7D%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2C%7Bn_V%7D%7D+%5Cright%29+\" alt=\"\\vec n = \\left( {{n_1},{n_2}, \\cdot \\cdot \\cdot ,{n_V}} \\right) \" eeimg=\"1\"/> 服从多项式分布，可参考1.5节的多项式分布概念。</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+p%5Cleft%28+%7B%5Cvec+n%7C%5Cvec+p%7D+%5Cright%29+%3D+Mult%5Cleft%28+%7B%5Cvec+n%7C%5Cvec+p%2CN%7D+%5Cright%29+%3D+%5Cleft%28+%7B%5Cbegin%7Barray%7D%7B%2A%7B20%7D%7Bc%7D%7D+N%5C%5C+%7B%5Cvec+n%7D+%5Cend%7Barray%7D%7D+%5Cright%29%5Cprod%5Climits_%7Bk+%3D+1%7D%5EV+%7Bp_k%5E%7B%7Bn_k%7D%7D%7D+%5Ctag%7B61%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} p\\left( {\\vec n|\\vec p} \\right) = Mult\\left( {\\vec n|\\vec p,N} \\right) = \\left( {\\begin{array}{*{20}{c}} N\\\\ {\\vec n} \\end{array}} \\right)\\prod\\limits_{k = 1}^V {p_k^{{n_k}}} \\tag{61} \\end{align*}\" eeimg=\"1\"/> </p><p>此时公式（60）为：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+p%5Cleft%28+%7BW%7C%5Cvec+p%7D+%5Cright%29+%3D+%5Cprod%5Climits_%7Bi+%3D+1%7D%5En+%7Bp%5Cleft%28+%7B%7B%7B%5Cvec+w%7D_i%7D%7C%5Cvec+p%7D+%5Cright%29%7D+%3D+%5Cprod%5Climits_%7Bk+%3D+1%7D%5EV+%7Bp_k%5E%7B%7Bn_k%7D%7D%7D+%5Ctag%7B62%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} p\\left( {W|\\vec p} \\right) = \\prod\\limits_{i = 1}^n {p\\left( {{{\\vec w}_i}|\\vec p} \\right)} = \\prod\\limits_{k = 1}^V {p_k^{{n_k}}} \\tag{62} \\end{align*}\" eeimg=\"1\"/> </p><p>我们需要估计模型中的参数 <img src=\"https://www.zhihu.com/equation?tex=+%5Cvec+p\" alt=\" \\vec p\" eeimg=\"1\"/> ，可以用最大似然估计：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%7B%7B%5Chat+p%7D_%7BML%7D%7D+%3D+%5Cmathop+%7B%5Carg+%5Cmax+%7D%5Climits_%7B%5Cvec+p%7D+%5Csum%5Climits_%7Bk+%3D+1%7D%5EV+%7B%7Bn_k%7D%5Cln+%5Cleft%28+%7B%7Bp_k%7D%7D+%5Cright%29%7D+%5C%5C+s.t.%7B%5Crm%7B+%7D%7D%5Csum%5Climits_%7Bk+%3D+1%7D%5EV+%7B%7Bp_k%7D%7D+%3D+1%2C%5Csum%5Climits_%7Bk+%3D+1%7D%5EV+%7B%7Bn_k%7D%7D+%3D+N+%5Ctag%7B63%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} {{\\hat p}_{ML}} = \\mathop {\\arg \\max }\\limits_{\\vec p} \\sum\\limits_{k = 1}^V {{n_k}\\ln \\left( {{p_k}} \\right)} \\\\ s.t.{\\rm{ }}\\sum\\limits_{k = 1}^V {{p_k}} = 1,\\sum\\limits_{k = 1}^V {{n_k}} = N \\tag{63} \\end{align*}\" eeimg=\"1\"/> </p><p>于是参数 <img src=\"https://www.zhihu.com/equation?tex=+%7Bp_k%7D+\" alt=\" {p_k} \" eeimg=\"1\"/> 的估计值就是：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+L%5Cleft%28+%7B%5Cvec+p%7D+%5Cright%29+%26%3D+%5Csum%5Climits_%7Bk+%3D+1%7D%5EV+%7B%7Bn_k%7D%5Cln+%5Cleft%28+%7B%7Bp_k%7D%7D+%5Cright%29%7D+%2B+%5Clambda+%5Cleft%28+%7B%5Csum%5Climits_%7Bk+%3D+1%7D%5EV+%7B%7Bp_k%7D%7D+-+1%7D+%5Cright%29%5C%5C+%5Cfrac%7B%7B%5Cpartial+L%5Cleft%28+%7B%5Cvec+p%7D+%5Cright%29%7D%7D%7B%7B%5Cpartial+%7Bp_k%7D%7D%7D+%26%3D+%5Cfrac%7B%7B%7Bn_k%7D%7D%7D%7B%7B%7Bp_k%7D%7D%7D+%2B+%5Clambda+%3D+0+%5CRightarrow+%7Bp_k%7D+%3D+-+%5Cfrac%7B%7B%7Bn_k%7D%7D%7D%7B%5Clambda+%7D%5C%5C+%5Csum%5Climits_%7Bk+%3D+1%7D%5EV+%7B%7Bn_k%7D%7D+%26%3D+N+%5CRightarrow+%5Clambda+%3D+-+N%5C%5C+%7Bp_k%7D+%26%3D+%5Cfrac%7B%7B%7Bn_k%7D%7D%7D%7BN%7D+%5Ctag%7B64%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} L\\left( {\\vec p} \\right) &amp;= \\sum\\limits_{k = 1}^V {{n_k}\\ln \\left( {{p_k}} \\right)} + \\lambda \\left( {\\sum\\limits_{k = 1}^V {{p_k}} - 1} \\right)\\\\ \\frac{{\\partial L\\left( {\\vec p} \\right)}}{{\\partial {p_k}}} &amp;= \\frac{{{n_k}}}{{{p_k}}} + \\lambda = 0 \\Rightarrow {p_k} = - \\frac{{{n_k}}}{\\lambda }\\\\ \\sum\\limits_{k = 1}^V {{n_k}} &amp;= N \\Rightarrow \\lambda = - N\\\\ {p_k} &amp;= \\frac{{{n_k}}}{N} \\tag{64} \\end{align*}\" eeimg=\"1\"/> </p><h2><b>2.2 贝叶斯Unigram Model</b></h2><p>对于以上模型，统计学家中贝叶斯学派就不同意了，为什么上帝只有一个固定的筛子呢，在贝叶斯学派看来，一切参数都是随机变量，模型中 <img src=\"https://www.zhihu.com/equation?tex=+%5Cvec+p+\" alt=\" \\vec p \" eeimg=\"1\"/> 不是唯一固定的，而是服从一个分布，所以贝叶斯 Unigram Model 游戏规则变为：</p><p><b>Game 2.2 贝叶斯Unigram Model</b></p><blockquote>1. 上帝有一个装有无穷多个骰子的坛子，每个骰子有 <img src=\"https://www.zhihu.com/equation?tex=+V+\" alt=\" V \" eeimg=\"1\"/> 个面，每个面对应一个词，各个面概率不同；<br/>2. 上帝从坛子中抽一个骰子出来，然后用这个骰子不断地抛，产生预料中的所有词。</blockquote><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-3a7431c8bb75b92a4947e6c0472cd4be_b.jpg\" data-size=\"normal\" data-rawwidth=\"451\" data-rawheight=\"215\" class=\"origin_image zh-lightbox-thumb\" width=\"451\" data-original=\"https://pic3.zhimg.com/v2-3a7431c8bb75b92a4947e6c0472cd4be_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;451&#39; height=&#39;215&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"451\" data-rawheight=\"215\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"451\" data-original=\"https://pic3.zhimg.com/v2-3a7431c8bb75b92a4947e6c0472cd4be_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-3a7431c8bb75b92a4947e6c0472cd4be_b.jpg\"/><figcaption>图 2.2</figcaption></figure><p>上帝这个坛子里面有些骰子数量多，有些骰子数量少，所以从概率分布的角度看，坛子里面的骰子 <img src=\"https://www.zhihu.com/equation?tex=+%5Cvec+p+\" alt=\" \\vec p \" eeimg=\"1\"/> 服从一个概率分布 <img src=\"https://www.zhihu.com/equation?tex=+p%5Cleft%28+%7B%5Cvec+p%7D+%5Cright%29\" alt=\" p\\left( {\\vec p} \\right)\" eeimg=\"1\"/> ，这个分布称为参数 <img src=\"https://www.zhihu.com/equation?tex=+%5Cvec+p\" alt=\" \\vec p\" eeimg=\"1\"/> 的先验分布。先验分布 <img src=\"https://www.zhihu.com/equation?tex=+p%5Cleft%28+%7B%5Cvec+p%7D+%5Cright%29+\" alt=\" p\\left( {\\vec p} \\right) \" eeimg=\"1\"/> 可以有多种选择，注意到 <img src=\"https://www.zhihu.com/equation?tex=+%5Cvec+n+\" alt=\" \\vec n \" eeimg=\"1\"/> 是服从多项式分布的， <img src=\"https://www.zhihu.com/equation?tex=p%5Cleft%28+%7B%5Cvec+n%7C%5Cvec+p%7D+%5Cright%29+%3D+Mult%5Cleft%28+%7B%5Cvec+n%7C%5Cvec+p%2CN%7D+%5Cright%29\" alt=\"p\\left( {\\vec n|\\vec p} \\right) = Mult\\left( {\\vec n|\\vec p,N} \\right)\" eeimg=\"1\"/> ，回顾1.7节可知， <img src=\"https://www.zhihu.com/equation?tex=p%5Cleft%28+%7B%5Cvec+p%7D+%5Cright%29+\" alt=\"p\\left( {\\vec p} \\right) \" eeimg=\"1\"/> 最好的选择是 Dirichlet 分布：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+p%5Cleft%28+%7B%5Cvec+p%7C%5Cvec+%5Calpha+%7D+%5Cright%29+%3D+Dir%5Cleft%28+%7B%5Cvec+p%7C%5Cvec+%5Calpha+%7D+%5Cright%29+%3D+%5Cfrac%7B1%7D%7B%7B%5CDelta+%5Cleft%28+%7B%5Cvec+%5Calpha+%7D+%5Cright%29%7D%7D%5Cprod%5Climits_%7Bk+%3D+1%7D%5EV+%7Bp_k%5E%7B%7B%5Calpha+_k%7D+-+1%7D%7D+%2C%5Cvec+%5Calpha+%3D+%5Cleft%28+%7B%7B%5Calpha+_1%7D%2C%7B%5Calpha+_2%7D%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2C%7B%5Calpha+_V%7D%7D+%5Cright%29+%5Ctag%7B65%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} p\\left( {\\vec p|\\vec \\alpha } \\right) = Dir\\left( {\\vec p|\\vec \\alpha } \\right) = \\frac{1}{{\\Delta \\left( {\\vec \\alpha } \\right)}}\\prod\\limits_{k = 1}^V {p_k^{{\\alpha _k} - 1}} ,\\vec \\alpha = \\left( {{\\alpha _1},{\\alpha _2}, \\cdot \\cdot \\cdot ,{\\alpha _V}} \\right) \\tag{65} \\end{align*}\" eeimg=\"1\"/> </p><p>于是，在给定了参数 <img src=\"https://www.zhihu.com/equation?tex=+%5Cvec+p+\" alt=\" \\vec p \" eeimg=\"1\"/> 的先验分布 <img src=\"https://www.zhihu.com/equation?tex=+Dir%5Cleft%28+%7B%5Cvec+p%7C%5Cvec+%5Calpha+%7D+%5Cright%29+\" alt=\" Dir\\left( {\\vec p|\\vec \\alpha } \\right) \" eeimg=\"1\"/> 时候，语料中各个词出现的次数服从多项式分布 <img src=\"https://www.zhihu.com/equation?tex=+%5Cvec+n+%5Csim+Mult%5Cleft%28+%7B%5Cvec+n%7C%5Cvec+p%2CN%7D+%5Cright%29\" alt=\" \\vec n \\sim Mult\\left( {\\vec n|\\vec p,N} \\right)\" eeimg=\"1\"/> ，所以后验分布为：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+p%5Cleft%28+%7B%5Cvec+p%7C%5Cvec+n%2C%5Cvec+%5Calpha+%7D+%5Cright%29+%3D+%5Cfrac%7B%7Bp%5Cleft%28+%7B%5Cvec+n%7C%5Cvec+p%7D+%5Cright%29p%5Cleft%28+%7B%5Cvec+p%7C%5Cvec+%5Calpha+%7D+%5Cright%29%7D%7D%7B%7B%5Cint+%7Bp%5Cleft%28+%7B%5Cvec+n%7C%5Cvec+p%7D+%5Cright%29p%5Cleft%28+%7B%5Cvec+p%7C%5Cvec+%5Calpha+%7D+%5Cright%29d%5Cvec+p%7D+%7D%7D+%3D+%5Cfrac%7B1%7D%7B%7B%5CDelta+%5Cleft%28+%7B%5Cvec+%5Calpha+%2B+%5Cvec+n%7D+%5Cright%29%7D%7D%5Cprod%5Climits_%7Bk+%3D+1%7D%5EV+%7Bp_k%5E%7B%7B%5Calpha+_k%7D+%2B+%7Bn_k%7D+-+1%7D%7D+%5Ctag%7B66%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} p\\left( {\\vec p|\\vec n,\\vec \\alpha } \\right) = \\frac{{p\\left( {\\vec n|\\vec p} \\right)p\\left( {\\vec p|\\vec \\alpha } \\right)}}{{\\int {p\\left( {\\vec n|\\vec p} \\right)p\\left( {\\vec p|\\vec \\alpha } \\right)d\\vec p} }} = \\frac{1}{{\\Delta \\left( {\\vec \\alpha + \\vec n} \\right)}}\\prod\\limits_{k = 1}^V {p_k^{{\\alpha _k} + {n_k} - 1}} \\tag{66} \\end{align*}\" eeimg=\"1\"/> </p><p>对参数 <img src=\"https://www.zhihu.com/equation?tex=+%5Cvec+p+\" alt=\" \\vec p \" eeimg=\"1\"/> 采用贝叶斯估计，假设参数 <img src=\"https://www.zhihu.com/equation?tex=+%5Cvec+p+\" alt=\" \\vec p \" eeimg=\"1\"/> 服从 <img src=\"https://www.zhihu.com/equation?tex=+Dir%5Cleft%28+%7B%5Cvec+p%7C%5Cvec+%5Calpha+%7D+%5Cright%29+\" alt=\" Dir\\left( {\\vec p|\\vec \\alpha } \\right) \" eeimg=\"1\"/> 分布，我们利用样本信息对 <img src=\"https://www.zhihu.com/equation?tex=+%5Cvec+p\" alt=\" \\vec p\" eeimg=\"1\"/> 的先验分布进行修正，得到 <img src=\"https://www.zhihu.com/equation?tex=+%5Cvec+p+\" alt=\" \\vec p \" eeimg=\"1\"/> 的后验分布也是服从 <img src=\"https://www.zhihu.com/equation?tex=+Dir%5Cleft%28+%7B%5Cvec+p%7C%5Cvec+%5Calpha+%2B+%5Cvec+n%7D+%5Cright%29+\" alt=\" Dir\\left( {\\vec p|\\vec \\alpha + \\vec n} \\right) \" eeimg=\"1\"/> 分布。可以用 <img src=\"https://www.zhihu.com/equation?tex=+%5Cvec+p+\" alt=\" \\vec p \" eeimg=\"1\"/> 的期望值作为参数 <img src=\"https://www.zhihu.com/equation?tex=+%5Cvec+p+\" alt=\" \\vec p \" eeimg=\"1\"/> 的估计值。由1.6节可知， <img src=\"https://www.zhihu.com/equation?tex=%5Cvec+p+\" alt=\"\\vec p \" eeimg=\"1\"/> 的期望值为：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+E%5Cleft%28+%7B%5Cvec+p%7D+%5Cright%29+%3D+%5Cleft%28+%7B%5Cfrac%7B%7B%7Bn_1%7D+%2B+%7B%5Calpha+_1%7D%7D%7D%7B%7B%5Csum%5Cnolimits_%7Bk+%3D+1%7D%5EV+%7B%5Cleft%28+%7B%7Bn_k%7D+%2B+%7B%5Calpha+_k%7D%7D+%5Cright%29%7D+%7D%7D%2C%5Cfrac%7B%7B%7Bn_2%7D+%2B+%7B%5Calpha+_2%7D%7D%7D%7B%7B%5Csum%5Cnolimits_%7Bk+%3D+1%7D%5EV+%7B%5Cleft%28+%7B%7Bn_k%7D+%2B+%7B%5Calpha+_k%7D%7D+%5Cright%29%7D+%7D%7D%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2C%5Cfrac%7B%7B%7Bn_V%7D+%2B+%7B%5Calpha+_V%7D%7D%7D%7B%7B%5Csum%5Cnolimits_%7Bk+%3D+1%7D%5EV+%7B%5Cleft%28+%7B%7Bn_k%7D+%2B+%7B%5Calpha+_k%7D%7D+%5Cright%29%7D+%7D%7D%7D+%5Cright%29+%5Ctag%7B67%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} E\\left( {\\vec p} \\right) = \\left( {\\frac{{{n_1} + {\\alpha _1}}}{{\\sum\\nolimits_{k = 1}^V {\\left( {{n_k} + {\\alpha _k}} \\right)} }},\\frac{{{n_2} + {\\alpha _2}}}{{\\sum\\nolimits_{k = 1}^V {\\left( {{n_k} + {\\alpha _k}} \\right)} }}, \\cdot \\cdot \\cdot ,\\frac{{{n_V} + {\\alpha _V}}}{{\\sum\\nolimits_{k = 1}^V {\\left( {{n_k} + {\\alpha _k}} \\right)} }}} \\right) \\tag{67} \\end{align*}\" eeimg=\"1\"/> </p><p>接下来我们计算语料产生的概率，开始并不知道上帝到底用哪个骰子，所以每个骰子都有可能被使用，使用的概率由 <img src=\"https://www.zhihu.com/equation?tex=+p%5Cleft%28+%7B%5Cvec+p%7C%5Cvec+%5Calpha+%7D+%5Cright%29+\" alt=\" p\\left( {\\vec p|\\vec \\alpha } \\right) \" eeimg=\"1\"/> 决定的，对于每个具体的骰子，由该骰子产生预料的概率为 <img src=\"https://www.zhihu.com/equation?tex=p%5Cleft%28+%7BW%7C%5Cvec+p%7D+%5Cright%29\" alt=\"p\\left( {W|\\vec p} \\right)\" eeimg=\"1\"/> ，所以语料产生的概率为：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+p%5Cleft%28+%7BW%7C%5Cvec+%5Calpha+%7D+%5Cright%29+%26%3D+%5Cint+%7Bp%5Cleft%28+%7BW%7C%5Cvec+p%7D+%5Cright%29p%5Cleft%28+%7B%5Cvec+p%7C%5Cvec+%5Calpha+%7D+%5Cright%29d%5Cvec+p%7D+%5C%5C+%26%3D+%5Cint+%7B%5Cprod%5Climits_%7Bk+%3D+1%7D%5EV+%7Bp_k%5E%7B%7Bn_k%7D%7DDir%5Cleft%28+%7B%5Cvec+p%7C%5Cvec+%5Calpha+%7D+%5Cright%29%7D+d%5Cvec+p%7D+%5C%5C+%26%3D+%5Cint+%7B%5Cprod%5Climits_%7Bk+%3D+1%7D%5EV+%7Bp_k%5E%7B%7Bn_k%7D%7D%5Cfrac%7B1%7D%7B%7B%5CDelta+%5Cleft%28+%7B%5Cvec+%5Calpha+%7D+%5Cright%29%7D%7D%5Cprod%5Climits_%7Bk+%3D+1%7D%5EV+%7Bp_k%5E%7B%7B%5Calpha+_k%7D+-+1%7D%7D+%7D+d%5Cvec+p%7D+%5C%5C+%26%3D+%5Cfrac%7B1%7D%7B%7B%5CDelta+%5Cleft%28+%7B%5Cvec+%5Calpha+%7D+%5Cright%29%7D%7D%5Cint+%7B%5Cprod%5Climits_%7Bk+%3D+1%7D%5EV+%7Bp_k%5E%7B%7Bn_k%7D+%2B+%7B%5Calpha+_k%7D+-+1%7D%7D+d%5Cvec+p%7D+%5C%5C+%26%3D+%5Cfrac%7B%7B%5CDelta+%5Cleft%28+%7B%5Cvec+%5Calpha+%2B+%5Cvec+n%7D+%5Cright%29%7D%7D%7B%7B%5CDelta+%5Cleft%28+%7B%5Cvec+%5Calpha+%7D+%5Cright%29%7D%7D+%5Ctag%7B68%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} p\\left( {W|\\vec \\alpha } \\right) &amp;= \\int {p\\left( {W|\\vec p} \\right)p\\left( {\\vec p|\\vec \\alpha } \\right)d\\vec p} \\\\ &amp;= \\int {\\prod\\limits_{k = 1}^V {p_k^{{n_k}}Dir\\left( {\\vec p|\\vec \\alpha } \\right)} d\\vec p} \\\\ &amp;= \\int {\\prod\\limits_{k = 1}^V {p_k^{{n_k}}\\frac{1}{{\\Delta \\left( {\\vec \\alpha } \\right)}}\\prod\\limits_{k = 1}^V {p_k^{{\\alpha _k} - 1}} } d\\vec p} \\\\ &amp;= \\frac{1}{{\\Delta \\left( {\\vec \\alpha } \\right)}}\\int {\\prod\\limits_{k = 1}^V {p_k^{{n_k} + {\\alpha _k} - 1}} d\\vec p} \\\\ &amp;= \\frac{{\\Delta \\left( {\\vec \\alpha + \\vec n} \\right)}}{{\\Delta \\left( {\\vec \\alpha } \\right)}} \\tag{68} \\end{align*}\" eeimg=\"1\"/> </p><h2><b>2.3 PLSA</b></h2><p><b>1. PLSA Model</b></p><p>概率隐语义分析，是主题模型的一种。上面介绍的 Unigram Model 相对简单，没有考虑文档有多个主题的情况，一般一篇文档可以由多个主题（Topic）组成，文档中的每个词都是由一个固定的 Topic 生成的，所以 PLSA 的游戏规则为：</p><p><b>Game 2.3 PLSA Topic Model</b></p><blockquote>1. 上帝有两种类型的骰子，一类是doc-topic，每个骰子有 <img src=\"https://www.zhihu.com/equation?tex=+K+\" alt=\" K \" eeimg=\"1\"/> 个面，每个面是一个 topic 编号；一类是 topic-word，每个骰子有 <img src=\"https://www.zhihu.com/equation?tex=+V+\" alt=\" V \" eeimg=\"1\"/> 个面，每个面对应一个词。<br/> 2. 上帝一共有 <img src=\"https://www.zhihu.com/equation?tex=+K+\" alt=\" K \" eeimg=\"1\"/> 个topic-word骰子，每个骰子有一个编号，从 <img src=\"https://www.zhihu.com/equation?tex=1\" alt=\"1\" eeimg=\"1\"/> 到 <img src=\"https://www.zhihu.com/equation?tex=+K\" alt=\" K\" eeimg=\"1\"/> 。<br/> 3. 生成每篇文档之前，上帝都先为这篇文档制造一个特定的 doc-topic 骰子，然后重复下面过程生成文档的词：<br/>  1） 投掷这个 doc-topic 骰子，得到一个编号为 <img src=\"https://www.zhihu.com/equation?tex=+z+\" alt=\" z \" eeimg=\"1\"/> 的topic<br/>  2） 选择 <img src=\"https://www.zhihu.com/equation?tex=+K+\" alt=\" K \" eeimg=\"1\"/> 个topic-word骰子中编号为 <img src=\"https://www.zhihu.com/equation?tex=+z+\" alt=\" z \" eeimg=\"1\"/> 的骰子，投掷这个骰子，得到一个词。</blockquote><p><b>2. EM算法推导PLSA</b></p><p>PLSA 模型中 doc-topic 和 topic-word 的每个面的概率值是固定的，所以属于点估计，但是PLSA模型既含有观测变量 <img src=\"https://www.zhihu.com/equation?tex=%7Bd_i%7D%2C%7Bw_j%7D\" alt=\"{d_i},{w_j}\" eeimg=\"1\"/> ，又含有隐变量 <img src=\"https://www.zhihu.com/equation?tex=+%7Bz_k%7D\" alt=\" {z_k}\" eeimg=\"1\"/> ，就不能简单地直接使用极大似然估计法估计模型参数，我们可以采用 EM 算法估计参数。我们先介绍推导过程用到的符号含义：</p><ul><li><img src=\"https://www.zhihu.com/equation?tex=D+%3D+%5Cleft%5C%7B+%7B%7Bd_1%7D%2C%7Bd_2%7D%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2C%7Bd_N%7D%7D+%5Cright%5C%7D\" alt=\"D = \\left\\{ {{d_1},{d_2}, \\cdot \\cdot \\cdot ,{d_N}} \\right\\}\" eeimg=\"1\"/> ：表示语料中 <img src=\"https://www.zhihu.com/equation?tex=+N+\" alt=\" N \" eeimg=\"1\"/> 篇文档；</li><li><img src=\"https://www.zhihu.com/equation?tex=W+%3D+%5Cleft%5C%7B+%7B%7Bw_1%7D%2C%7Bw_2%7D%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2C%7Bw_M%7D%7D+%5Cright%5C%7D\" alt=\"W = \\left\\{ {{w_1},{w_2}, \\cdot \\cdot \\cdot ,{w_M}} \\right\\}\" eeimg=\"1\"/> ：表示语料中 <img src=\"https://www.zhihu.com/equation?tex=+M+\" alt=\" M \" eeimg=\"1\"/> 个词组；</li><li><img src=\"https://www.zhihu.com/equation?tex=n%5Cleft%28+%7B%7Bd_i%7D%2C%7Bw_j%7D%7D+%5Cright%29\" alt=\"n\\left( {{d_i},{w_j}} \\right)\" eeimg=\"1\"/> ：表示词 <img src=\"https://www.zhihu.com/equation?tex=+%7Bw_j%7D+\" alt=\" {w_j} \" eeimg=\"1\"/> 在文档 <img src=\"https://www.zhihu.com/equation?tex=+%7Bd_i%7D+\" alt=\" {d_i} \" eeimg=\"1\"/> 中出现的频次， <img src=\"https://www.zhihu.com/equation?tex=%7B%5Crm%7BN%7D%7D+%3D+%7B%5Cleft%28+%7Bn%5Cleft%28+%7B%7Bd_i%7D%2C%7Bw_j%7D%7D+%5Cright%29%7D+%5Cright%29_%7Bij%7D%7D+%5Cin+%7BR%5E%7BN+%5Ctimes+M%7D%7D\" alt=\"{\\rm{N}} = {\\left( {n\\left( {{d_i},{w_j}} \\right)} \\right)_{ij}} \\in {R^{N \\times M}}\" eeimg=\"1\"/> ；</li><li><img src=\"https://www.zhihu.com/equation?tex=Z+%3D+%5Cleft%5C%7B+%7B%7Bz_1%7D%2C%7Bz_2%7D%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2C%7Bz_K%7D%7D+%5Cright%5C%7D\" alt=\"Z = \\left\\{ {{z_1},{z_2}, \\cdot \\cdot \\cdot ,{z_K}} \\right\\}\" eeimg=\"1\"/> ：表示 <img src=\"https://www.zhihu.com/equation?tex=+K+\" alt=\" K \" eeimg=\"1\"/> 个主题，每篇文档可以有多个主题；</li><li><img src=\"https://www.zhihu.com/equation?tex=p%5Cleft%28+%7B%7Bw_j%7D%7Cd%7B%7D_i%7D+%5Cright%29\" alt=\"p\\left( {{w_j}|d{}_i} \\right)\" eeimg=\"1\"/> ：表示词 <img src=\"https://www.zhihu.com/equation?tex=+%7Bw_j%7D+\" alt=\" {w_j} \" eeimg=\"1\"/> 在给定文档 <img src=\"https://www.zhihu.com/equation?tex=+%7Bd_i%7D+\" alt=\" {d_i} \" eeimg=\"1\"/> 中出现的概率；</li><li><img src=\"https://www.zhihu.com/equation?tex=p%5Cleft%28+%7B%7Bz_k%7D%7Cd%7B%7D_i%7D+%5Cright%29\" alt=\"p\\left( {{z_k}|d{}_i} \\right)\" eeimg=\"1\"/> ：表示主题 <img src=\"https://www.zhihu.com/equation?tex=+%7Bz_k%7D+\" alt=\" {z_k} \" eeimg=\"1\"/> 在给定文档 <img src=\"https://www.zhihu.com/equation?tex=+%7Bd_i%7D+\" alt=\" {d_i} \" eeimg=\"1\"/> 下出现的概率；</li><li><img src=\"https://www.zhihu.com/equation?tex=p%5Cleft%28+%7B%7Bw_j%7D%7C%7Bz_k%7D%7D+%5Cright%29\" alt=\"p\\left( {{w_j}|{z_k}} \\right)\" eeimg=\"1\"/> ：表示词 <img src=\"https://www.zhihu.com/equation?tex=+%7Bw_j%7D+\" alt=\" {w_j} \" eeimg=\"1\"/> 在给定主题 <img src=\"https://www.zhihu.com/equation?tex=+%7Bz_k%7D+\" alt=\" {z_k} \" eeimg=\"1\"/> 下出现的概率。</li></ul><p>一般给定语料， <img src=\"https://www.zhihu.com/equation?tex=%7Bd_i%7D%2C%7Bw_j%7D+\" alt=\"{d_i},{w_j} \" eeimg=\"1\"/> 是可以观测的， <img src=\"https://www.zhihu.com/equation?tex=%7Bz_k%7D+\" alt=\"{z_k} \" eeimg=\"1\"/> 是隐变量，不可以直观地观测到。我们定义“doc-word”的生成模型，如图 2.3 所示。</p><blockquote>1）select a document <img src=\"https://www.zhihu.com/equation?tex=+%7Bd_i%7D+\" alt=\" {d_i} \" eeimg=\"1\"/> with probability <img src=\"https://www.zhihu.com/equation?tex=+p%5Cleft%28+%7B%7Bd_i%7D%7D+%5Cright%29\" alt=\" p\\left( {{d_i}} \\right)\" eeimg=\"1\"/> ；<br/>2）pick a latent class <img src=\"https://www.zhihu.com/equation?tex=+%7Bz_k%7D+\" alt=\" {z_k} \" eeimg=\"1\"/> with probability <img src=\"https://www.zhihu.com/equation?tex=p%5Cleft%28+%7B%7Bz_k%7D%7C%7Bd_i%7D%7D+%5Cright%29\" alt=\"p\\left( {{z_k}|{d_i}} \\right)\" eeimg=\"1\"/>； <br/>3）generate a word <img src=\"https://www.zhihu.com/equation?tex=+%7Bw_j%7D+\" alt=\" {w_j} \" eeimg=\"1\"/> with probability <img src=\"https://www.zhihu.com/equation?tex=+p%5Cleft%28+%7B%7Bw_j%7D%7C%7Bz_k%7D%7D+%5Cright%29\" alt=\" p\\left( {{w_j}|{z_k}} \\right)\" eeimg=\"1\"/> ；</blockquote><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-46a54404d7792241f744ea3c9f027258_b.jpg\" data-size=\"normal\" data-rawwidth=\"999\" data-rawheight=\"292\" class=\"origin_image zh-lightbox-thumb\" width=\"999\" data-original=\"https://pic1.zhimg.com/v2-46a54404d7792241f744ea3c9f027258_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;999&#39; height=&#39;292&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"999\" data-rawheight=\"292\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"999\" data-original=\"https://pic1.zhimg.com/v2-46a54404d7792241f744ea3c9f027258_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-46a54404d7792241f744ea3c9f027258_b.jpg\"/><figcaption>图 2.3</figcaption></figure><p>下面进入正题，用 EM 算法进行模型参数估计，似然函数为：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+L+%26%3D+%5Cprod%5Climits_%7Bi+%3D+1%7D%5EN+%7B%5Cprod%5Climits_%7Bj+%3D+1%7D%5EM+%7Bp%7B%7B%5Cleft%28+%7B%7Bd_i%7D%2C%7Bw_j%7D%7D+%5Cright%29%7D%5E%7Bn%5Cleft%28+%7B%7Bd_i%7D%2C%7Bw_j%7D%7D+%5Cright%29%7D%7D%7D+%7D+%5C%5C+%5Clog+L+%26%3D+%5Csum%5Climits_%7Bi+%3D+1%7D%5EN+%7B%5Csum%5Climits_%7Bj+%3D+1%7D%5EM+%7Bn%5Cleft%28+%7B%7Bd_i%7D%2C%7Bw_j%7D%7D+%5Cright%29%5Clog+p%5Cleft%28+%7B%7Bd_i%7D%2C%7Bw_j%7D%7D+%5Cright%29%7D+%7D+%5C%5C+%26%3D+%5Csum%5Climits_%7Bi+%3D+1%7D%5EN+%7B%5Csum%5Climits_%7Bj+%3D+1%7D%5EM+%7Bn%5Cleft%28+%7B%7Bd_i%7D%2C%7Bw_j%7D%7D+%5Cright%29%5Clog+%5Cleft%5B+%7B%5Csum%5Climits_%7Bk+%3D+1%7D%5EK+%7Bp%5Cleft%28+%7B%7Bd_i%7D%7D+%5Cright%29p%5Cleft%28+%7B%7Bz_k%7D%7C%7Bd_i%7D%7D+%5Cright%29p%5Cleft%28+%7B%7Bw_j%7D%7C%7Bz_k%7D%7D+%5Cright%29%7D+%7D+%5Cright%5D%7D+%7D+%5C%5C+%26%3D+%5Csum%5Climits_%7Bi+%3D+1%7D%5EN+%7B%5Csum%5Climits_%7Bj+%3D+1%7D%5EM+%7Bn%5Cleft%28+%7B%7Bd_i%7D%2C%7Bw_j%7D%7D+%5Cright%29%5Cleft%5B+%7B%5Clog+p%5Cleft%28+%7B%7Bd_i%7D%7D+%5Cright%29+%2B+%5Clog+%5Csum%5Climits_%7Bk+%3D+1%7D%5EK+%7Bp%5Cleft%28+%7B%7Bz_k%7D%7C%7Bd_i%7D%7D+%5Cright%29p%5Cleft%28+%7B%7Bw_j%7D%7C%7Bz_k%7D%7D+%5Cright%29%7D+%7D+%5Cright%5D%7D+%7D+%5C%5C+%26%3D+%5Csum%5Climits_%7Bi+%3D+1%7D%5EN+%7Bn%5Cleft%28+%7B%7Bd_i%7D%7D+%5Cright%29%5Clog+p%5Cleft%28+%7B%7Bd_i%7D%7D+%5Cright%29%7D+%2B+%5Csum%5Climits_%7Bi+%3D+1%7D%5EN+%7B%5Csum%5Climits_%7Bj+%3D+1%7D%5EM+%7Bn%5Cleft%28+%7B%7Bd_i%7D%2C%7Bw_j%7D%7D+%5Cright%29%5Clog+%5Csum%5Climits_%7Bk+%3D+1%7D%5EK+%7Bp%5Cleft%28+%7B%7Bz_k%7D%7C%7Bd_i%7D%7D+%5Cright%29p%5Cleft%28+%7B%7Bw_j%7D%7C%7Bz_k%7D%7D+%5Cright%29%7D+%7D+%7D+%5Ctag%7B69%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} L &amp;= \\prod\\limits_{i = 1}^N {\\prod\\limits_{j = 1}^M {p{{\\left( {{d_i},{w_j}} \\right)}^{n\\left( {{d_i},{w_j}} \\right)}}} } \\\\ \\log L &amp;= \\sum\\limits_{i = 1}^N {\\sum\\limits_{j = 1}^M {n\\left( {{d_i},{w_j}} \\right)\\log p\\left( {{d_i},{w_j}} \\right)} } \\\\ &amp;= \\sum\\limits_{i = 1}^N {\\sum\\limits_{j = 1}^M {n\\left( {{d_i},{w_j}} \\right)\\log \\left[ {\\sum\\limits_{k = 1}^K {p\\left( {{d_i}} \\right)p\\left( {{z_k}|{d_i}} \\right)p\\left( {{w_j}|{z_k}} \\right)} } \\right]} } \\\\ &amp;= \\sum\\limits_{i = 1}^N {\\sum\\limits_{j = 1}^M {n\\left( {{d_i},{w_j}} \\right)\\left[ {\\log p\\left( {{d_i}} \\right) + \\log \\sum\\limits_{k = 1}^K {p\\left( {{z_k}|{d_i}} \\right)p\\left( {{w_j}|{z_k}} \\right)} } \\right]} } \\\\ &amp;= \\sum\\limits_{i = 1}^N {n\\left( {{d_i}} \\right)\\log p\\left( {{d_i}} \\right)} + \\sum\\limits_{i = 1}^N {\\sum\\limits_{j = 1}^M {n\\left( {{d_i},{w_j}} \\right)\\log \\sum\\limits_{k = 1}^K {p\\left( {{z_k}|{d_i}} \\right)p\\left( {{w_j}|{z_k}} \\right)} } } \\tag{69} \\end{align*}\" eeimg=\"1\"/> </p><p>对于给定训练预料，希望公式 (69) 最大化。 <img src=\"https://www.zhihu.com/equation?tex=p%5Cleft%28+%7B%7Bz_k%7D%7C%7Bd_i%7D%7D+%5Cright%29+%E5%92%8C+p%5Cleft%28+%7B%7Bw_j%7D%7C%7Bz_k%7D%7D+%5Cright%29+\" alt=\"p\\left( {{z_k}|{d_i}} \\right) 和 p\\left( {{w_j}|{z_k}} \\right) \" eeimg=\"1\"/> 是 PLSA 模型需要求解的参数，按照通常的做法是令偏导数 为 <img src=\"https://www.zhihu.com/equation?tex=0\" alt=\"0\" eeimg=\"1\"/> ，但是参数是以求和的形式出现在对数函数里面，求导后会变得很复杂。 <img src=\"https://www.zhihu.com/equation?tex=n%5Cleft%28+%7B%7Bd_i%7D%7D+%5Cright%29+\" alt=\"n\\left( {{d_i}} \\right) \" eeimg=\"1\"/> 表示第 <img src=\"https://www.zhihu.com/equation?tex=+i+\" alt=\" i \" eeimg=\"1\"/> 篇文档的词数，所以当预料固定，公式 (69) 中第一项可以看作常量，所以只要最大化 (69) 中的第二项即可，如公式(70)所示。</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Csum%5Climits_%7Bi+%3D+1%7D%5EN+%7B%5Csum%5Climits_%7Bj+%3D+1%7D%5EM+%7Bn%5Cleft%28+%7B%7Bd_i%7D%2C%7Bw_j%7D%7D+%5Cright%29%5Clog+%5Csum%5Climits_%7Bk+%3D+1%7D%5EK+%7Bp%5Cleft%28+%7B%7Bz_k%7D%7C%7Bd_i%7D%7D+%5Cright%29p%5Cleft%28+%7B%7Bw_j%7D%7C%7Bz_k%7D%7D+%5Cright%29%7D+%7D+%7D+%5Ctag%7B70%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} \\sum\\limits_{i = 1}^N {\\sum\\limits_{j = 1}^M {n\\left( {{d_i},{w_j}} \\right)\\log \\sum\\limits_{k = 1}^K {p\\left( {{z_k}|{d_i}} \\right)p\\left( {{w_j}|{z_k}} \\right)} } } \\tag{70} \\end{align*}\" eeimg=\"1\"/> </p><p>引入 <img src=\"https://www.zhihu.com/equation?tex=+%7BQ_%7Bi%2Cj%7D%7D%5Cleft%28+%7B%7Bz_k%7D%7D+%5Cright%29+\" alt=\" {Q_{i,j}}\\left( {{z_k}} \\right) \" eeimg=\"1\"/> 表示 <img src=\"https://www.zhihu.com/equation?tex=+%7Bz_k%7D+\" alt=\" {z_k} \" eeimg=\"1\"/> 的概率分布 <img src=\"https://www.zhihu.com/equation?tex=%EF%BC%88%5Csum%5Climits_%7Bk+%3D+1%7D%5EK+%7B%7BQ_%7Bi%2Cj%7D%7D%5Cleft%28+%7B%7Bz_k%7D%7D+%5Cright%29%7D+%3D+1%2C%7BQ_%7Bi%2Cj%7D%7D%5Cleft%28+%7B%7Bz_k%7D%7D+%5Cright%29+%5Cge+0%EF%BC%89\" alt=\"（\\sum\\limits_{k = 1}^K {{Q_{i,j}}\\left( {{z_k}} \\right)} = 1,{Q_{i,j}}\\left( {{z_k}} \\right) \\ge 0）\" eeimg=\"1\"/> ，根据 Jensen不等式得：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Csum%5Climits_%7Bi+%3D+1%7D%5EN+%7B%5Csum%5Climits_%7Bj+%3D+1%7D%5EM+%7Bn%5Cleft%28+%7B%7Bd_i%7D%2C%7Bw_j%7D%7D+%5Cright%29%5Clog+%5Csum%5Climits_%7Bk+%3D+1%7D%5EK+%7B%7BQ_%7Bi%2Cj%7D%7D%5Cleft%28+%7B%7Bz_k%7D%7D+%5Cright%29%5Cfrac%7B%7Bp%5Cleft%28+%7B%7Bz_k%7D%7C%7Bd_i%7D%7D+%5Cright%29p%5Cleft%28+%7B%7Bw_j%7D%7C%7Bz_k%7D%7D+%5Cright%29%7D%7D%7B%7B%7BQ_%7Bi%2Cj%7D%7D%5Cleft%28+%7B%7Bz_k%7D%7D+%5Cright%29%7D%7D%7D+%7D+%7D+%5C%5C+%7B%5Crm%7B+%7D%7D+%5Cge+%5Csum%5Climits_%7Bi+%3D+1%7D%5EN+%7B%5Csum%5Climits_%7Bj+%3D+1%7D%5EM+%7Bn%5Cleft%28+%7B%7Bd_i%7D%2C%7Bw_j%7D%7D+%5Cright%29%5Csum%5Climits_%7Bk+%3D+1%7D%5EK+%7B%7BQ_%7Bi%2Cj%7D%7D%5Cleft%28+%7B%7Bz_k%7D%7D+%5Cright%29%5Clog+%5Cfrac%7B%7Bp%5Cleft%28+%7B%7Bz_k%7D%7C%7Bd_i%7D%7D+%5Cright%29p%5Cleft%28+%7B%7Bw_j%7D%7C%7Bz_k%7D%7D+%5Cright%29%7D%7D%7B%7B%7BQ_%7Bi%2Cj%7D%7D%5Cleft%28+%7B%7Bz_k%7D%7D+%5Cright%29%7D%7D%7D+%7D+%7D+%5Ctag%7B71%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} \\sum\\limits_{i = 1}^N {\\sum\\limits_{j = 1}^M {n\\left( {{d_i},{w_j}} \\right)\\log \\sum\\limits_{k = 1}^K {{Q_{i,j}}\\left( {{z_k}} \\right)\\frac{{p\\left( {{z_k}|{d_i}} \\right)p\\left( {{w_j}|{z_k}} \\right)}}{{{Q_{i,j}}\\left( {{z_k}} \\right)}}} } } \\\\ {\\rm{ }} \\ge \\sum\\limits_{i = 1}^N {\\sum\\limits_{j = 1}^M {n\\left( {{d_i},{w_j}} \\right)\\sum\\limits_{k = 1}^K {{Q_{i,j}}\\left( {{z_k}} \\right)\\log \\frac{{p\\left( {{z_k}|{d_i}} \\right)p\\left( {{w_j}|{z_k}} \\right)}}{{{Q_{i,j}}\\left( {{z_k}} \\right)}}} } } \\tag{71} \\end{align*}\" eeimg=\"1\"/> </p><p>当 <img src=\"https://www.zhihu.com/equation?tex=%7BQ_%7Bi%2Cj%7D%7D%5Cleft%28+%7B%7Bz_k%7D%7D+%5Cright%29+%3D+%5Cfrac%7B%7Bp%5Cleft%28+%7B%7Bz_k%7D%7C%7Bd_i%7D%7D+%5Cright%29p%5Cleft%28+%7B%7Bw_j%7D%7C%7Bz_k%7D%7D+%5Cright%29%7D%7D%7B%7B%5Csum%5Climits_%7Bl+%3D+1%7D%5EK+%7Bp%5Cleft%28+%7B%7Bz_l%7D%7C%7Bd_i%7D%7D+%5Cright%29p%5Cleft%28+%7B%7Bw_j%7D%7C%7Bz_l%7D%7D+%5Cright%29%7D+%7D%7D+%3D+%5Cfrac%7B%7Bp%5Cleft%28+%7B%7Bw_j%7D%2C%7Bz_k%7D%7C%7Bd_i%7D%7D+%5Cright%29%7D%7D%7B%7B%5Csum%5Climits_%7Bl+%3D+1%7D%5EK+%7Bp%5Cleft%28+%7B%7Bw_j%7D%2C%7Bz_l%7D%7C%7Bd_i%7D%7D+%5Cright%29%7D+%7D%7D+%3D+p%5Cleft%28+%7B%7Bz_k%7D%7C%7Bw_j%7D%2C%7Bd_i%7D%7D+%5Cright%29\" alt=\"{Q_{i,j}}\\left( {{z_k}} \\right) = \\frac{{p\\left( {{z_k}|{d_i}} \\right)p\\left( {{w_j}|{z_k}} \\right)}}{{\\sum\\limits_{l = 1}^K {p\\left( {{z_l}|{d_i}} \\right)p\\left( {{w_j}|{z_l}} \\right)} }} = \\frac{{p\\left( {{w_j},{z_k}|{d_i}} \\right)}}{{\\sum\\limits_{l = 1}^K {p\\left( {{w_j},{z_l}|{d_i}} \\right)} }} = p\\left( {{z_k}|{w_j},{d_i}} \\right)\" eeimg=\"1\"/> 时，公式 (71) 不等式中等号成立，所以只需要最大化：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%26+%5Cmathop+%7B%5Cmax+%7D%5Climits_%7Bp%5Cleft%28+%7B%7Bz_k%7D%7C%7Bd_i%7D%7D+%5Cright%29%2Cp%5Cleft%28+%7B%7Bw_j%7D%7C%7Bz_k%7D%7D+%5Cright%29%7D+%7B%5Crm%7B+%7D%7D%5Csum%5Climits_%7Bi+%3D+1%7D%5EN+%7B%5Csum%5Climits_%7Bj+%3D+1%7D%5EM+%7Bn%5Cleft%28+%7B%7Bd_i%7D%2C%7Bw_j%7D%7D+%5Cright%29%5Csum%5Climits_%7Bk+%3D+1%7D%5EK+%7B%7BQ_%7Bi%2Cj%7D%7D%5Cleft%28+%7B%7Bz_k%7D%7D+%5Cright%29%5Clog+%5Cfrac%7B%7Bp%5Cleft%28+%7B%7Bz_k%7D%7C%7Bd_i%7D%7D+%5Cright%29p%5Cleft%28+%7B%7Bw_j%7D%7C%7Bz_k%7D%7D+%5Cright%29%7D%7D%7B%7B%7BQ_%7Bi%2Cj%7D%7D%5Cleft%28+%7B%7Bz_k%7D%7D+%5Cright%29%7D%7D%7D+%7D+%7D+%5C%5C+%26+s.t.%7B%5Crm%7B+%7D%7D%5Csum%5Climits_%7Bj+%3D+1%7D%5EM+%7Bp%5Cleft%28+%7B%7Bw_j%7D%7C%7Bz_k%7D%7D+%5Cright%29%7D+%3D+1%2C%7B%5Crm%7B+%7D%7D%5Csum%5Climits_%7Bk+%3D+1%7D%5EK+%7Bp%5Cleft%28+%7B%7Bz_k%7D%7C%7Bd_i%7D%7D+%5Cright%29%7D+%3D+1+%5Ctag%7B72%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} &amp; \\mathop {\\max }\\limits_{p\\left( {{z_k}|{d_i}} \\right),p\\left( {{w_j}|{z_k}} \\right)} {\\rm{ }}\\sum\\limits_{i = 1}^N {\\sum\\limits_{j = 1}^M {n\\left( {{d_i},{w_j}} \\right)\\sum\\limits_{k = 1}^K {{Q_{i,j}}\\left( {{z_k}} \\right)\\log \\frac{{p\\left( {{z_k}|{d_i}} \\right)p\\left( {{w_j}|{z_k}} \\right)}}{{{Q_{i,j}}\\left( {{z_k}} \\right)}}} } } \\\\ &amp; s.t.{\\rm{ }}\\sum\\limits_{j = 1}^M {p\\left( {{w_j}|{z_k}} \\right)} = 1,{\\rm{ }}\\sum\\limits_{k = 1}^K {p\\left( {{z_k}|{d_i}} \\right)} = 1 \\tag{72} \\end{align*}\" eeimg=\"1\"/> </p><p>根据拉格朗日乘子法</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%26l+%3D+%5Csum%5Climits_%7Bi+%3D+1%7D%5EN+%7B%5Csum%5Climits_%7Bj+%3D+1%7D%5EM+%7Bn%5Cleft%28+%7B%7Bd_i%7D%2C%7Bw_j%7D%7D+%5Cright%29%5Csum%5Climits_%7Bk+%3D+1%7D%5EK+%7B%7BQ_%7Bi%2Cj%7D%7D%5Cleft%28+%7B%7Bz_k%7D%7D+%5Cright%29%5Clog+%5Cfrac%7B%7Bp%5Cleft%28+%7B%7Bz_k%7D%7C%7Bd_i%7D%7D+%5Cright%29p%5Cleft%28+%7B%7Bw_j%7D%7C%7Bz_k%7D%7D+%5Cright%29%7D%7D%7B%7B%7BQ_%7Bi%2Cj%7D%7D%5Cleft%28+%7B%7Bz_k%7D%7D+%5Cright%29%7D%7D%7D+%7D+%7D+%5C%5C%26%2B+%7B%5Clambda+_1%7D%5Cleft%28+%7B%5Csum%5Climits_%7Bj+%3D+1%7D%5EM+%7Bp%5Cleft%28+%7B%7Bw_j%7D%7C%7Bz_k%7D%7D+%5Cright%29%7D+-+1%7D+%5Cright%29+%2B+%7B%5Clambda+_2%7D%5Cleft%28+%7B%5Csum%5Climits_%7Bk+%3D+1%7D%5EK+%7Bp%5Cleft%28+%7B%7Bz_j%7D%7C%7Bd_i%7D%7D+%5Cright%29%7D+-+1%7D+%5Cright%29%5C%5C+%26%5Cfrac%7B%7B%5Cpartial+l%7D%7D%7B%7B%5Cpartial+p%5Cleft%28+%7B%7Bw_j%7D%7C%7Bz_k%7D%7D+%5Cright%29%7D%7D+%3D+%5Csum%5Climits_%7Bi+%3D+1%7D%5EN+%7B%5Cfrac%7B%7Bn%5Cleft%28+%7B%7Bd_i%7D%2C%7Bw_j%7D%7D+%5Cright%29%7BQ_%7Bi%2Cj%7D%7D%5Cleft%28+%7B%7Bz_k%7D%7D+%5Cright%29%7D%7D%7B%7Bp%5Cleft%28+%7B%7Bw_j%7D%7C%7Bz_k%7D%7D+%5Cright%29%7D%7D%7D+%2B+%7B%5Clambda+_1%7D%2C%5C%5C%26%7B%5Crm%7B+%7D%7D%5Cfrac%7B%7B%5Cpartial+l%7D%7D%7B%7B%5Cpartial+p%5Cleft%28+%7B%7Bz_k%7D%7C%7Bd_i%7D%7D+%5Cright%29%7D%7D+%3D+%5Csum%5Climits_%7Bj+%3D+1%7D%5EM+%7B%5Cfrac%7B%7Bn%5Cleft%28+%7B%7Bd_i%7D%2C%7Bw_j%7D%7D+%5Cright%29%7BQ_%7Bi%2Cj%7D%7D%5Cleft%28+%7B%7Bz_k%7D%7D+%5Cright%29%7D%7D%7B%7Bp%5Cleft%28+%7B%7Bz_k%7D%7C%7Bd_i%7D%7D+%5Cright%29%7D%7D%7D+%2B+%7B%5Clambda+_2%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} &amp;l = \\sum\\limits_{i = 1}^N {\\sum\\limits_{j = 1}^M {n\\left( {{d_i},{w_j}} \\right)\\sum\\limits_{k = 1}^K {{Q_{i,j}}\\left( {{z_k}} \\right)\\log \\frac{{p\\left( {{z_k}|{d_i}} \\right)p\\left( {{w_j}|{z_k}} \\right)}}{{{Q_{i,j}}\\left( {{z_k}} \\right)}}} } } \\\\&amp;+ {\\lambda _1}\\left( {\\sum\\limits_{j = 1}^M {p\\left( {{w_j}|{z_k}} \\right)} - 1} \\right) + {\\lambda _2}\\left( {\\sum\\limits_{k = 1}^K {p\\left( {{z_j}|{d_i}} \\right)} - 1} \\right)\\\\ &amp;\\frac{{\\partial l}}{{\\partial p\\left( {{w_j}|{z_k}} \\right)}} = \\sum\\limits_{i = 1}^N {\\frac{{n\\left( {{d_i},{w_j}} \\right){Q_{i,j}}\\left( {{z_k}} \\right)}}{{p\\left( {{w_j}|{z_k}} \\right)}}} + {\\lambda _1},\\\\&amp;{\\rm{ }}\\frac{{\\partial l}}{{\\partial p\\left( {{z_k}|{d_i}} \\right)}} = \\sum\\limits_{j = 1}^M {\\frac{{n\\left( {{d_i},{w_j}} \\right){Q_{i,j}}\\left( {{z_k}} \\right)}}{{p\\left( {{z_k}|{d_i}} \\right)}}} + {\\lambda _2} \\end{align*}\" eeimg=\"1\"/> </p><p>所以可得：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+p%5Cleft%28+%7B%7Bw_j%7D%7C%7Bz_k%7D%7D+%5Cright%29+%26%3D+-+%5Cfrac%7B%7B%5Csum%5Climits_%7Bi+%3D+1%7D%5EN+%7Bn%5Cleft%28+%7B%7Bd_i%7D%2C%7Bw_j%7D%7D+%5Cright%29%7BQ_%7Bi%2Cj%7D%7D%5Cleft%28+%7B%7Bz_k%7D%7D+%5Cright%29%7D+%7D%7D%7B%7B%7B%5Clambda+_1%7D%7D%7D%2C%7B%5Crm%7B+%7D%7D%5Csum%5Climits_%7Bj+%3D+1%7D%5EM+%7Bp%5Cleft%28+%7B%7Bw_j%7D%7C%7Bz_k%7D%7D+%5Cright%29%7D+%3D+1%5C%5C+%26+%5CRightarrow+%7B%5Clambda+_1%7D+%3D+-+%5Csum%5Climits_%7Bj+%3D+1%7D%5EM+%7B%5Csum%5Climits_%7Bi+%3D+1%7D%5EN+%7Bn%5Cleft%28+%7B%7Bd_i%7D%2C%7Bw_j%7D%7D+%5Cright%29%7BQ_%7Bi%2Cj%7D%7D%5Cleft%28+%7B%7Bz_k%7D%7D+%5Cright%29%7D+%7D+%5C%5C+%26+%5CRightarrow+p%5Cleft%28+%7B%7Bw_j%7D%7C%7Bz_k%7D%7D+%5Cright%29+%3D+%5Cfrac%7B%7B%5Csum%5Climits_%7Bi+%3D+1%7D%5EN+%7Bn%5Cleft%28+%7B%7Bd_i%7D%2C%7Bw_j%7D%7D+%5Cright%29%7BQ_%7Bi%2Cj%7D%7D%5Cleft%28+%7B%7Bz_k%7D%7D+%5Cright%29%7D+%7D%7D%7B%7B%5Csum%5Climits_%7Bj+%3D+1%7D%5EM+%7B%5Csum%5Climits_%7Bi+%3D+1%7D%5EN+%7Bn%5Cleft%28+%7B%7Bd_i%7D%2C%7Bw_j%7D%7D+%5Cright%29%7BQ_%7Bi%2Cj%7D%7D%5Cleft%28+%7B%7Bz_k%7D%7D+%5Cright%29%7D+%7D+%7D%7D%5C%5C+p%5Cleft%28+%7B%7Bz_k%7D%7C%7Bd_i%7D%7D+%5Cright%29+%26%3D+-+%5Cfrac%7B%7B%5Csum%5Climits_%7Bj+%3D+1%7D%5EM+%7Bn%5Cleft%28+%7B%7Bd_i%7D%2C%7Bw_j%7D%7D+%5Cright%29%7BQ_%7Bi%2Cj%7D%7D%5Cleft%28+%7B%7Bz_k%7D%7D+%5Cright%29%7D+%7D%7D%7B%7B%7B%5Clambda+_2%7D%7D%7D%2C%7B%5Crm%7B+%7D%7D%5Csum%5Climits_%7Bk+%3D+1%7D%5EK+%7Bp%5Cleft%28+%7B%7Bz_k%7D%7C%7Bd_i%7D%7D+%5Cright%29%7D+%3D+1%5C%5C+%26+%5CRightarrow+%7B%5Clambda+_2%7D+%3D+-+%5Csum%5Climits_%7Bk+%3D+1%7D%5EK+%7B%5Csum%5Climits_%7Bj+%3D+1%7D%5EM+%7Bn%5Cleft%28+%7B%7Bd_i%7D%2C%7Bw_j%7D%7D+%5Cright%29%7BQ_%7Bi%2Cj%7D%7D%5Cleft%28+%7B%7Bz_k%7D%7D+%5Cright%29%7D+%3D+%7D+n%5Cleft%28+%7B%7Bd_i%7D%7D+%5Cright%29%5C%5C+%26+%5CRightarrow+p%5Cleft%28+%7B%7Bz_k%7D%7C%7Bd_i%7D%7D+%5Cright%29+%3D+%5Cfrac%7B%7B%5Csum%5Climits_%7Bj+%3D+1%7D%5EM+%7Bn%5Cleft%28+%7B%7Bd_i%7D%2C%7Bw_j%7D%7D+%5Cright%29%7BQ_%7Bi%2Cj%7D%7D%5Cleft%28+%7B%7Bz_k%7D%7D+%5Cright%29%7D+%7D%7D%7B%7Bn%5Cleft%28+%7B%7Bd_i%7D%7D+%5Cright%29%7D%7D+%5Ctag%7B73%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} p\\left( {{w_j}|{z_k}} \\right) &amp;= - \\frac{{\\sum\\limits_{i = 1}^N {n\\left( {{d_i},{w_j}} \\right){Q_{i,j}}\\left( {{z_k}} \\right)} }}{{{\\lambda _1}}},{\\rm{ }}\\sum\\limits_{j = 1}^M {p\\left( {{w_j}|{z_k}} \\right)} = 1\\\\ &amp; \\Rightarrow {\\lambda _1} = - \\sum\\limits_{j = 1}^M {\\sum\\limits_{i = 1}^N {n\\left( {{d_i},{w_j}} \\right){Q_{i,j}}\\left( {{z_k}} \\right)} } \\\\ &amp; \\Rightarrow p\\left( {{w_j}|{z_k}} \\right) = \\frac{{\\sum\\limits_{i = 1}^N {n\\left( {{d_i},{w_j}} \\right){Q_{i,j}}\\left( {{z_k}} \\right)} }}{{\\sum\\limits_{j = 1}^M {\\sum\\limits_{i = 1}^N {n\\left( {{d_i},{w_j}} \\right){Q_{i,j}}\\left( {{z_k}} \\right)} } }}\\\\ p\\left( {{z_k}|{d_i}} \\right) &amp;= - \\frac{{\\sum\\limits_{j = 1}^M {n\\left( {{d_i},{w_j}} \\right){Q_{i,j}}\\left( {{z_k}} \\right)} }}{{{\\lambda _2}}},{\\rm{ }}\\sum\\limits_{k = 1}^K {p\\left( {{z_k}|{d_i}} \\right)} = 1\\\\ &amp; \\Rightarrow {\\lambda _2} = - \\sum\\limits_{k = 1}^K {\\sum\\limits_{j = 1}^M {n\\left( {{d_i},{w_j}} \\right){Q_{i,j}}\\left( {{z_k}} \\right)} = } n\\left( {{d_i}} \\right)\\\\ &amp; \\Rightarrow p\\left( {{z_k}|{d_i}} \\right) = \\frac{{\\sum\\limits_{j = 1}^M {n\\left( {{d_i},{w_j}} \\right){Q_{i,j}}\\left( {{z_k}} \\right)} }}{{n\\left( {{d_i}} \\right)}} \\tag{73} \\end{align*}\" eeimg=\"1\"/> </p><p>总结 EM 算法为：</p><ul><li><b>E-step</b> 随机初始化变量 <img src=\"https://www.zhihu.com/equation?tex=+p%5Cleft%28+%7B%7Bz_k%7D%7C%7Bd_i%7D%7D+%5Cright%29%EF%BC%8Cp%5Cleft%28+%7B%7Bw_j%7D%7C%7Bz_k%7D%7D+%5Cright%29\" alt=\" p\\left( {{z_k}|{d_i}} \\right)，p\\left( {{w_j}|{z_k}} \\right)\" eeimg=\"1\"/> ，计算隐变量后验概率。</li></ul><p><img src=\"https://www.zhihu.com/equation?tex=%7BQ_%7Bi%2Cj%7D%7D%5Cleft%28+%7B%7Bz_k%7D%7D+%5Cright%29+%3D+%5Cfrac%7B%7Bp%5Cleft%28+%7B%7Bz_k%7D%7C%7Bd_i%7D%7D+%5Cright%29p%5Cleft%28+%7B%7Bw_j%7D%7C%7Bz_k%7D%7D+%5Cright%29%7D%7D%7B%7B%5Csum%5Climits_%7Bl+%3D+1%7D%5EK+%7Bp%5Cleft%28+%7B%7Bz_l%7D%7C%7Bd_i%7D%7D+%5Cright%29p%5Cleft%28+%7B%7Bw_j%7D%7C%7Bz_l%7D%7D+%5Cright%29%7D+%7D%7D+%3D+%5Cfrac%7B%7Bp%5Cleft%28+%7B%7Bw_j%7D%2C%7Bz_k%7D%7C%7Bd_i%7D%7D+%5Cright%29%7D%7D%7B%7B%5Csum%5Climits_%7Bl+%3D+1%7D%5EK+%7Bp%5Cleft%28+%7B%7Bw_j%7D%2C%7Bz_l%7D%7C%7Bd_i%7D%7D+%5Cright%29%7D+%7D%7D+%3D+p%5Cleft%28+%7B%7Bz_k%7D%7C%7Bw_j%7D%2C%7Bd_i%7D%7D+%5Cright%29\" alt=\"{Q_{i,j}}\\left( {{z_k}} \\right) = \\frac{{p\\left( {{z_k}|{d_i}} \\right)p\\left( {{w_j}|{z_k}} \\right)}}{{\\sum\\limits_{l = 1}^K {p\\left( {{z_l}|{d_i}} \\right)p\\left( {{w_j}|{z_l}} \\right)} }} = \\frac{{p\\left( {{w_j},{z_k}|{d_i}} \\right)}}{{\\sum\\limits_{l = 1}^K {p\\left( {{w_j},{z_l}|{d_i}} \\right)} }} = p\\left( {{z_k}|{w_j},{d_i}} \\right)\" eeimg=\"1\"/> </p><ul><li><b>M-step</b> 最大化似然函数，更新变量 <img src=\"https://www.zhihu.com/equation?tex=p%5Cleft%28+%7B%7Bz_k%7D%7C%7Bd_i%7D%7D+%5Cright%29%EF%BC%8Cp%5Cleft%28+%7B%7Bw_j%7D%7C%7Bz_k%7D%7D+%5Cright%29\" alt=\"p\\left( {{z_k}|{d_i}} \\right)，p\\left( {{w_j}|{z_k}} \\right)\" eeimg=\"1\"/> </li></ul><p><img src=\"https://www.zhihu.com/equation?tex=p%5Cleft%28+%7B%7Bw_j%7D%7C%7Bz_k%7D%7D+%5Cright%29+%3D+%5Cfrac%7B%7B%5Csum%5Climits_%7Bi+%3D+1%7D%5EN+%7Bn%5Cleft%28+%7B%7Bd_i%7D%2C%7Bw_j%7D%7D+%5Cright%29%7BQ_%7Bi%2Cj%7D%7D%5Cleft%28+%7B%7Bz_k%7D%7D+%5Cright%29%7D+%7D%7D%7B%7B%5Csum%5Climits_%7Bj+%3D+1%7D%5EM+%7B%5Csum%5Climits_%7Bi+%3D+1%7D%5EN+%7Bn%5Cleft%28+%7B%7Bd_i%7D%2C%7Bw_j%7D%7D+%5Cright%29%7BQ_%7Bi%2Cj%7D%7D%5Cleft%28+%7B%7Bz_k%7D%7D+%5Cright%29%7D+%7D+%7D%7D%2C%7B%5Crm%7B+%7D%7Dp%5Cleft%28+%7B%7Bz_k%7D%7C%7Bd_i%7D%7D+%5Cright%29+%3D+%5Cfrac%7B%7B%5Csum%5Climits_%7Bj+%3D+1%7D%5EM+%7Bn%5Cleft%28+%7B%7Bd_i%7D%2C%7Bw_j%7D%7D+%5Cright%29%7BQ_%7Bi%2Cj%7D%7D%5Cleft%28+%7B%7Bz_k%7D%7D+%5Cright%29%7D+%7D%7D%7B%7Bn%5Cleft%28+%7B%7Bd_i%7D%7D+%5Cright%29%7D%7D\" alt=\"p\\left( {{w_j}|{z_k}} \\right) = \\frac{{\\sum\\limits_{i = 1}^N {n\\left( {{d_i},{w_j}} \\right){Q_{i,j}}\\left( {{z_k}} \\right)} }}{{\\sum\\limits_{j = 1}^M {\\sum\\limits_{i = 1}^N {n\\left( {{d_i},{w_j}} \\right){Q_{i,j}}\\left( {{z_k}} \\right)} } }},{\\rm{ }}p\\left( {{z_k}|{d_i}} \\right) = \\frac{{\\sum\\limits_{j = 1}^M {n\\left( {{d_i},{w_j}} \\right){Q_{i,j}}\\left( {{z_k}} \\right)} }}{{n\\left( {{d_i}} \\right)}}\" eeimg=\"1\"/> </p><ul><li>  重复1、2两步，直到收敛。</li></ul><h2><b>2.4 LDA</b></h2><p>对于 PLSA 模型，贝叶斯学派表示不同意，为什么上帝只有一个 doc-topic 骰子，为什么上帝只有固定 <img src=\"https://www.zhihu.com/equation?tex=+K+\" alt=\" K \" eeimg=\"1\"/> 个 topic-word骰子？ <img src=\"https://www.zhihu.com/equation?tex=p%5Cleft%28+%7B%7Bz_k%7D%7C%7Bd_i%7D%7D+%5Cright%29+\" alt=\"p\\left( {{z_k}|{d_i}} \\right) \" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=+p%5Cleft%28+%7B%7Bw_j%7D%7C%7Bz_k%7D%7D+%5Cright%29+\" alt=\" p\\left( {{w_j}|{z_k}} \\right) \" eeimg=\"1\"/> 是模型的参数，一切参数都是随机变量，模型中 <img src=\"https://www.zhihu.com/equation?tex=+p%5Cleft%28+%7B%7Bz_k%7D%7C%7Bd_i%7D%7D+%5Cright%29+\" alt=\" p\\left( {{z_k}|{d_i}} \\right) \" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=+p%5Cleft%28+%7B%7Bw_j%7D%7C%7Bz_k%7D%7D+%5Cright%29+\" alt=\" p\\left( {{w_j}|{z_k}} \\right) \" eeimg=\"1\"/> 不是唯一固定的，类似 2.2 节贝叶斯 Unigram Model 和 2.1 节 Unigram Model 的关系。所以 LDA 游戏规则为：</p><p><b>Game 2.4 LDA Topic Model</b></p><blockquote>1. 上帝有两坛的骰子，第一个坛子装的是 doc-topic 骰子，第二个坛子装的是 topic-word 骰子。<br/> 2. 上帝随机地从第二个坛子中独立的抽取 <img src=\"https://www.zhihu.com/equation?tex=+K+\" alt=\" K \" eeimg=\"1\"/> 个 topic-word 骰子，编号为 <img src=\"https://www.zhihu.com/equation?tex=1\" alt=\"1\" eeimg=\"1\"/> 到 <img src=\"https://www.zhihu.com/equation?tex=+K\" alt=\" K\" eeimg=\"1\"/> 。<br/> 3. 生成每篇文档之前，上帝先从第一个坛子中随机抽取一个 doc-topic 骰子，然后重复地投掷这个骰子，为文档中每个词生成一个 topic 编号为 <img src=\"https://www.zhihu.com/equation?tex=+z+\" alt=\" z \" eeimg=\"1\"/> （这里只是生成每个词的主题，词并未生成），重复生成文档中每个词对应的 topic。<br/> 4. 对语料中每篇文档的每个 topic 编号为 <img src=\"https://www.zhihu.com/equation?tex=+z+\" alt=\" z \" eeimg=\"1\"/> 的主题，选择 <img src=\"https://www.zhihu.com/equation?tex=+K+\" alt=\" K \" eeimg=\"1\"/> 个 topic-word 骰子中编号为 <img src=\"https://www.zhihu.com/equation?tex=+z+\" alt=\" z \" eeimg=\"1\"/> 骰子，投掷这个 topic-word 骰子，生成对应的 word。</blockquote><p>假设我们训练语料有 <img src=\"https://www.zhihu.com/equation?tex=+M+\" alt=\" M \" eeimg=\"1\"/> 篇 doc，词典中有 <img src=\"https://www.zhihu.com/equation?tex=+V+\" alt=\" V \" eeimg=\"1\"/> 个word， <img src=\"https://www.zhihu.com/equation?tex=K+\" alt=\"K \" eeimg=\"1\"/> 个topic。对于第 <img src=\"https://www.zhihu.com/equation?tex=+m+\" alt=\" m \" eeimg=\"1\"/> 篇文档有 <img src=\"https://www.zhihu.com/equation?tex=%7BN_m%7D\" alt=\"{N_m}\" eeimg=\"1\"/> 个词。</p><ul><li><img src=\"https://www.zhihu.com/equation?tex=%7B%5Cvec+%5Cvartheta+_m%7D\" alt=\"{\\vec \\vartheta _m}\" eeimg=\"1\"/> ： <img src=\"https://www.zhihu.com/equation?tex=p%5Cleft%28+%7Bz%7C%7Bd_m%7D%7D+%5Cright%29\" alt=\"p\\left( {z|{d_m}} \\right)\" eeimg=\"1\"/> ，第 <img src=\"https://www.zhihu.com/equation?tex=+m+\" alt=\" m \" eeimg=\"1\"/> 篇文档的主题分布概率， <img src=\"https://www.zhihu.com/equation?tex=%5Cvec+%5CTheta+%3D+%5Cleft%5C%7B+%7B%7B%7B%5Cvec+%5Cvartheta+%7D_m%7D%7D+%5Cright%5C%7D_%7Bm+%3D+1%7D%5EM+%5Cin+%7BR%5E%7BM+%5Ctimes+K%7D%7D\" alt=\"\\vec \\Theta = \\left\\{ {{{\\vec \\vartheta }_m}} \\right\\}_{m = 1}^M \\in {R^{M \\times K}}\" eeimg=\"1\"/> ；</li><li><img src=\"https://www.zhihu.com/equation?tex=%7B%5Cvec+%5Cvarphi+_k%7D\" alt=\"{\\vec \\varphi _k}\" eeimg=\"1\"/> ： <img src=\"https://www.zhihu.com/equation?tex=p%5Cleft%28+%7Bw%7C%7Bz_k%7D%7D+%5Cright%29\" alt=\"p\\left( {w|{z_k}} \\right)\" eeimg=\"1\"/> ，主题为 <img src=\"https://www.zhihu.com/equation?tex=+k+\" alt=\" k \" eeimg=\"1\"/> 的词的概率分布， <img src=\"https://www.zhihu.com/equation?tex=%5Cvec+%5CPhi+%3D+%5Cleft%5C%7B+%7B%7B%7B%5Cvec+%5Cvarphi+%7D_k%7D%7D+%5Cright%5C%7D_%7Bk+%3D+1%7D%5EK+%5Cin+%7BR%5E%7BK+%5Ctimes+V%7D%7D\" alt=\"\\vec \\Phi = \\left\\{ {{{\\vec \\varphi }_k}} \\right\\}_{k = 1}^K \\in {R^{K \\times V}}\" eeimg=\"1\"/> ；</li><li><img src=\"https://www.zhihu.com/equation?tex=n_m%5E%7B%5Cleft%28+k+%5Cright%29%7D\" alt=\"n_m^{\\left( k \\right)}\" eeimg=\"1\"/> ：第 <img src=\"https://www.zhihu.com/equation?tex=+m+\" alt=\" m \" eeimg=\"1\"/> 篇文档中属于 <img src=\"https://www.zhihu.com/equation?tex=+topic+k+\" alt=\" topic k \" eeimg=\"1\"/> 的词的个数， <img src=\"https://www.zhihu.com/equation?tex=%7B%5Cvec+n_m%7D+%3D+%5Cleft%28+%7Bn_m%5E%7B%5Cleft%28+1+%5Cright%29%7D%2Cn_m%5E%7B%5Cleft%28+2+%5Cright%29%7D%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2Cn_m%5E%7B%5Cleft%28+K+%5Cright%29%7D%7D+%5Cright%29\" alt=\"{\\vec n_m} = \\left( {n_m^{\\left( 1 \\right)},n_m^{\\left( 2 \\right)}, \\cdot \\cdot \\cdot ,n_m^{\\left( K \\right)}} \\right)\" eeimg=\"1\"/> ；</li><li><img src=\"https://www.zhihu.com/equation?tex=n_k%5E%7B%5Cleft%28+t+%5Cright%29%7D\" alt=\"n_k^{\\left( t \\right)}\" eeimg=\"1\"/> ：topic <img src=\"https://www.zhihu.com/equation?tex=+k+\" alt=\" k \" eeimg=\"1\"/> 产生词 <img src=\"https://www.zhihu.com/equation?tex=+t+\" alt=\" t \" eeimg=\"1\"/> 的个数， <img src=\"https://www.zhihu.com/equation?tex=%7B%5Cvec+n_k%7D+%3D+%5Cleft%28+%7Bn_k%5E%7B%5Cleft%28+1+%5Cright%29%7D%2Cn_k%5E%7B%5Cleft%28+2+%5Cright%29%7D%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2Cn_k%5E%7B%5Cleft%28+V+%5Cright%29%7D%7D+%5Cright%29\" alt=\"{\\vec n_k} = \\left( {n_k^{\\left( 1 \\right)},n_k^{\\left( 2 \\right)}, \\cdot \\cdot \\cdot ,n_k^{\\left( V \\right)}} \\right)\" eeimg=\"1\"/> ；</li><li><img src=\"https://www.zhihu.com/equation?tex=%5Cvec+%5Calpha+%5Cin+%7BR%5E%7BK+%5Ctimes+1%7D%7D\" alt=\"\\vec \\alpha \\in {R^{K \\times 1}}\" eeimg=\"1\"/> ： <img src=\"https://www.zhihu.com/equation?tex=%7B%5Cvec+%5Cvartheta+_m%7D\" alt=\"{\\vec \\vartheta _m}\" eeimg=\"1\"/> 先验分布超参数；</li><li><img src=\"https://www.zhihu.com/equation?tex=%5Cvec+%5Cbeta+%5Cin+%7BR%5E%7BV+%5Ctimes+1%7D%7D\" alt=\"\\vec \\beta \\in {R^{V \\times 1}}\" eeimg=\"1\"/> ： <img src=\"https://www.zhihu.com/equation?tex=%7B%5Cvec+%5Cvarphi+_k%7D+\" alt=\"{\\vec \\varphi _k} \" eeimg=\"1\"/> 先验分布超参数；</li><li><img src=\"https://www.zhihu.com/equation?tex=%7Bz_%7Bm%2Cn%7D%7D\" alt=\"{z_{m,n}}\" eeimg=\"1\"/> ：第 <img src=\"https://www.zhihu.com/equation?tex=+m+\" alt=\" m \" eeimg=\"1\"/> 篇文档中第 <img src=\"https://www.zhihu.com/equation?tex=+n+\" alt=\" n \" eeimg=\"1\"/> 个词的主题；</li><li><img src=\"https://www.zhihu.com/equation?tex=+%7Bw_%7Bm%2Cn%7D%7D\" alt=\" {w_{m,n}}\" eeimg=\"1\"/> ：第 <img src=\"https://www.zhihu.com/equation?tex=+m+\" alt=\" m \" eeimg=\"1\"/> 篇文档中第 <img src=\"https://www.zhihu.com/equation?tex=+n+\" alt=\" n \" eeimg=\"1\"/> 个词。</li></ul><p>LDA的概率图模型表示如图2.4所示。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-ede743e2ea592409a806cdce8a5adb4e_b.jpg\" data-size=\"normal\" data-rawwidth=\"341\" data-rawheight=\"293\" class=\"content_image\" width=\"341\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;341&#39; height=&#39;293&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"341\" data-rawheight=\"293\" class=\"content_image lazy\" width=\"341\" data-actualsrc=\"https://pic3.zhimg.com/v2-ede743e2ea592409a806cdce8a5adb4e_b.jpg\"/><figcaption>图 2.4</figcaption></figure><p><b>1. 联合概率分布</b></p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7Dp%5Cleft%28+%7B%5Cvec+w%2C%5Cvec+z%7C%5Cvec+%5Calpha+%2C%5Cvec+%5Cbeta+%7D+%5Cright%29+%3D+p%5Cleft%28+%7B%5Cvec+w%7C%5Cvec+z%2C%5Cvec+%5Cbeta+%7D+%5Cright%29p%5Cleft%28+%7B%5Cvec+z%7C%5Cvec+%5Calpha+%7D+%5Cright%29%5Ctag%7B74%7D%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*}p\\left( {\\vec w,\\vec z|\\vec \\alpha ,\\vec \\beta } \\right) = p\\left( {\\vec w|\\vec z,\\vec \\beta } \\right)p\\left( {\\vec z|\\vec \\alpha } \\right)\\tag{74}\\end{align*}\" eeimg=\"1\"/> 1） <img src=\"https://www.zhihu.com/equation?tex=%5Cvec+%5Calpha+%5Cunderbrace+%7B%5Cxrightarrow%7B%7B%5Cbegin%7Barray%7D%7B%2A%7B20%7D%7Bc%7D%7D+%7B%7D%26%7B%7D+%5Cend%7Barray%7D%7D%7D%7D_%7BDir%7D%7B%5Cvec+%5Cvartheta+_m%7D%5Cunderbrace+%7B%5Cxrightarrow%7B%7B%5Cbegin%7Barray%7D%7B%2A%7B20%7D%7Bc%7D%7D+%7B%7D%26%7B%7D+%5Cend%7Barray%7D%7D%7D%7D_%7BMult%7D%7B%5Cvec+z_m%7D\" alt=\"\\vec \\alpha \\underbrace {\\xrightarrow{{\\begin{array}{*{20}{c}} {}&amp;{} \\end{array}}}}_{Dir}{\\vec \\vartheta _m}\\underbrace {\\xrightarrow{{\\begin{array}{*{20}{c}} {}&amp;{} \\end{array}}}}_{Mult}{\\vec z_m}\" eeimg=\"1\"/> ：第一步对 Dir 分布进行采样得到样本 <img src=\"https://www.zhihu.com/equation?tex=+%7B%5Cvec+%5Cvartheta+_m%7D\" alt=\" {\\vec \\vartheta _m}\" eeimg=\"1\"/> （也就是从第一个坛子中抽取 doc-topic 骰子 <img src=\"https://www.zhihu.com/equation?tex=+%7B%5Cvec+%5Cvartheta+_m%7D\" alt=\" {\\vec \\vartheta _m}\" eeimg=\"1\"/> ）；第二步 doc-topic 骰子有 <img src=\"https://www.zhihu.com/equation?tex=+K+\" alt=\" K \" eeimg=\"1\"/> 个面，每个面表示一个主题，那么在一次投掷骰子过程中，每个主题的概率为 <img src=\"https://www.zhihu.com/equation?tex=+%7B%5Cvec+%5Cvartheta+_m%7D%7B%5Crm%7B+%3D+%7D%7D%5Cleft%28+%7B%5Cvartheta+_m%5E%7B%5Cleft%28+1+%5Cright%29%7D%2C%5Cvartheta+_m%5E%7B%5Cleft%28+2+%5Cright%29%7D%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2C%5Cvartheta+_m%5E%7B%5Cleft%28+K+%5Cright%29%7D%7D+%5Cright%29\" alt=\" {\\vec \\vartheta _m}{\\rm{ = }}\\left( {\\vartheta _m^{\\left( 1 \\right)},\\vartheta _m^{\\left( 2 \\right)}, \\cdot \\cdot \\cdot ,\\vartheta _m^{\\left( K \\right)}} \\right)\" eeimg=\"1\"/> ，第 <img src=\"https://www.zhihu.com/equation?tex=+m+\" alt=\" m \" eeimg=\"1\"/> 篇文档有 <img src=\"https://www.zhihu.com/equation?tex=+%7BN_m%7D+\" alt=\" {N_m} \" eeimg=\"1\"/> 个词，所以需要投掷 <img src=\"https://www.zhihu.com/equation?tex=+%7BN_m%7D+\" alt=\" {N_m} \" eeimg=\"1\"/> 次骰子，为该篇文档中的每个词生成一个主题， 第 <img src=\"https://www.zhihu.com/equation?tex=+n\" alt=\" n\" eeimg=\"1\"/> 个词对应的主题为 <img src=\"https://www.zhihu.com/equation?tex=+%7Bz_%7Bm%2Cn%7D%7D\" alt=\" {z_{m,n}}\" eeimg=\"1\"/> ，整篇文档的主题表示为 <img src=\"https://www.zhihu.com/equation?tex=+%7B%5Cvec+z_m%7D\" alt=\" {\\vec z_m}\" eeimg=\"1\"/> 。在 <img src=\"https://www.zhihu.com/equation?tex=+%7BN_m%7D+\" alt=\" {N_m} \" eeimg=\"1\"/> 次投掷过程中，每个主题出现的次数为 <img src=\"https://www.zhihu.com/equation?tex=%7B%5Cvec+n_m%7D+%3D+%5Cleft%28+%7Bn_m%5E%7B%5Cleft%28+1+%5Cright%29%7D%2Cn_m%5E%7B%5Cleft%28+2+%5Cright%29%7D%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2Cn_m%5E%7B%5Cleft%28+K+%5Cright%29%7D%7D+%5Cright%29\" alt=\"{\\vec n_m} = \\left( {n_m^{\\left( 1 \\right)},n_m^{\\left( 2 \\right)}, \\cdot \\cdot \\cdot ,n_m^{\\left( K \\right)}} \\right)\" eeimg=\"1\"/> ，那么 <img src=\"https://www.zhihu.com/equation?tex=+%7B%5Cvec+n_m%7D+\" alt=\" {\\vec n_m} \" eeimg=\"1\"/> 服从多项式分布 <img src=\"https://www.zhihu.com/equation?tex=Mult%5Cleft%28+%7B%7B%7B%5Cvec+n%7D_m%7D%7C%7B%7B%5Cvec+%5Cvartheta+%7D_m%7D%2C%7BN_m%7D%7D+%5Cright%29\" alt=\"Mult\\left( {{{\\vec n}_m}|{{\\vec \\vartheta }_m},{N_m}} \\right)\" eeimg=\"1\"/> （只生成每个词的主题，并未由主题产生具体的词）。可以采用贝叶斯估计对参数 <img src=\"https://www.zhihu.com/equation?tex=+%7B%5Cvec+%5Cvartheta+_m%7D+\" alt=\" {\\vec \\vartheta _m} \" eeimg=\"1\"/> 进行估计。</p><ul><li><img src=\"https://www.zhihu.com/equation?tex=%7B%5Cvec+%5Cvartheta+_m%7D+\" alt=\"{\\vec \\vartheta _m} \" eeimg=\"1\"/> 的先验分布为 <img src=\"https://www.zhihu.com/equation?tex=+Dir%5Cleft%28+%7B%7B%7B%5Cvec+%5Cvartheta+%7D_m%7D%7C%5Cvec+%5Calpha+%7D+%5Cright%29\" alt=\" Dir\\left( {{{\\vec \\vartheta }_m}|\\vec \\alpha } \\right)\" eeimg=\"1\"/> </li><li>后验分布为（推导过程可以参考1.7节）</li></ul><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7Dp%5Cleft%28+%7B%7B%7B%5Cvec+%5Cvartheta+%7D_m%7D%7C%7B%7B%5Cvec+n%7D_m%7D%2C%5Cvec+%5Calpha+%7D+%5Cright%29+%26%3D+%5Cfrac%7B%7Bp%5Cleft%28+%7B%7B%7B%5Cvec+n%7D_m%7D%7C%7B%7B%5Cvec+%5Cvartheta+%7D_m%7D%7D+%5Cright%29p%5Cleft%28+%7B%7B%7B%5Cvec+%5Cvartheta+%7D_m%7D%7C%5Cvec+%5Calpha+%7D+%5Cright%29%7D%7D%7B%7B%5Cint+%7Bp%5Cleft%28+%7B%7B%7B%5Cvec+n%7D_m%7D%7C%7B%7B%5Cvec+%5Cvartheta+%7D_m%7D%7D+%5Cright%29p%5Cleft%28+%7B%7B%7B%5Cvec+%5Cvartheta+%7D_m%7D%7C%5Cvec+%5Calpha+%7D+%5Cright%29d%7B%7B%5Cvec+%5Cvartheta+%7D_m%7D%7D+%7D%7D%5C%5C+%26%3D+%5Cfrac%7B%7Bmult%5Cleft%28+%7B%7B%7B%5Cvec+n%7D_m%7D%7C%7B%7B%5Cvec+%5Cvartheta+%7D_m%7D%2C%7BN_m%7D%7D+%5Cright%29Dir%5Cleft%28+%7B%7B%7B%5Cvec+%5Cvartheta+%7D_m%7D%7C%5Cvec+%5Calpha+%7D+%5Cright%29%7D%7D%7B%7B%5Cint+%7Bmult%5Cleft%28+%7B%7B%7B%5Cvec+n%7D_m%7D%7C%7B%7B%5Cvec+%5Cvartheta+%7D_m%7D%2C%7BN_m%7D%7D+%5Cright%29Dir%5Cleft%28+%7B%7B%7B%5Cvec+%5Cvartheta+%7D_m%7D%7C%5Cvec+%5Calpha+%7D+%5Cright%29d%7B%7B%5Cvec+%5Cvartheta+%7D_m%7D%7D+%7D%7D%5C%5C+%26%3D+Dir%5Cleft%28+%7B%7B%7B%5Cvec+%5Cvartheta+%7D_m%7D%7C%5Cvec+%5Calpha+%2B+%7B%7B%5Cvec+n%7D_m%7D%7D+%5Cright%29%5Ctag%7B75%7D%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*}p\\left( {{{\\vec \\vartheta }_m}|{{\\vec n}_m},\\vec \\alpha } \\right) &amp;= \\frac{{p\\left( {{{\\vec n}_m}|{{\\vec \\vartheta }_m}} \\right)p\\left( {{{\\vec \\vartheta }_m}|\\vec \\alpha } \\right)}}{{\\int {p\\left( {{{\\vec n}_m}|{{\\vec \\vartheta }_m}} \\right)p\\left( {{{\\vec \\vartheta }_m}|\\vec \\alpha } \\right)d{{\\vec \\vartheta }_m}} }}\\\\ &amp;= \\frac{{mult\\left( {{{\\vec n}_m}|{{\\vec \\vartheta }_m},{N_m}} \\right)Dir\\left( {{{\\vec \\vartheta }_m}|\\vec \\alpha } \\right)}}{{\\int {mult\\left( {{{\\vec n}_m}|{{\\vec \\vartheta }_m},{N_m}} \\right)Dir\\left( {{{\\vec \\vartheta }_m}|\\vec \\alpha } \\right)d{{\\vec \\vartheta }_m}} }}\\\\ &amp;= Dir\\left( {{{\\vec \\vartheta }_m}|\\vec \\alpha + {{\\vec n}_m}} \\right)\\tag{75}\\end{align*}\" eeimg=\"1\"/> </p><ul><li><img src=\"https://www.zhihu.com/equation?tex=%7B%5Cvec+%5Cvartheta+_m%7D+\" alt=\"{\\vec \\vartheta _m} \" eeimg=\"1\"/> 的贝叶斯估计值为</li></ul><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7DE%5Cleft%28+%7B%7B%7B%5Cvec+%5Cvartheta+%7D_m%7D%7D+%5Cright%29+%3D+%5Cleft%28+%7B%5Cfrac%7B%7B%7B%5Calpha+_1%7D+%2B+n_m%5E%7B%5Cleft%28+1+%5Cright%29%7D%7D%7D%7B%7B%5Csum%5Cnolimits_%7Bk+%3D+1%7D%5EK+%7B%5Cleft%28+%7B%7B%5Calpha+_k%7D+%2B+n_m%5E%7B%5Cleft%28+k+%5Cright%29%7D%7D+%5Cright%29%7D+%7D%7D%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2C%5Cfrac%7B%7B%7B%5Calpha+_K%7D+%2B+n_m%5E%7B%5Cleft%28+K+%5Cright%29%7D%7D%7D%7B%7B%5Csum%5Cnolimits_%7Bk+%3D+1%7D%5EK+%7B%5Cleft%28+%7B%7B%5Calpha+_k%7D+%2B+n_m%5E%7B%5Cleft%28+k+%5Cright%29%7D%7D+%5Cright%29%7D+%7D%7D%7D+%5Cright%29+%5Ctag%7B76%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*}E\\left( {{{\\vec \\vartheta }_m}} \\right) = \\left( {\\frac{{{\\alpha _1} + n_m^{\\left( 1 \\right)}}}{{\\sum\\nolimits_{k = 1}^K {\\left( {{\\alpha _k} + n_m^{\\left( k \\right)}} \\right)} }}, \\cdot \\cdot \\cdot ,\\frac{{{\\alpha _K} + n_m^{\\left( K \\right)}}}{{\\sum\\nolimits_{k = 1}^K {\\left( {{\\alpha _k} + n_m^{\\left( k \\right)}} \\right)} }}} \\right) \\tag{76} \\end{align*}\" eeimg=\"1\"/> </p><p>下面我们计算第 <img src=\"https://www.zhihu.com/equation?tex=+m+\" alt=\" m \" eeimg=\"1\"/> 篇文档的主题概率分布：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7Dp%5Cleft%28+%7B%7B%7B%5Cvec+z%7D_m%7D%7C%5Cvec+%5Calpha+%7D+%5Cright%29+%26%3D+%5Cint+%7Bp%5Cleft%28+%7B%7B%7B%5Cvec+z%7D_m%7D%7C%7B%7B%5Cvec+%5Cvartheta+%7D_m%7D%7D+%5Cright%29p%5Cleft%28+%7B%7B%7B%5Cvec+%5Cvartheta+%7D_m%7D%7C%5Cvec+%5Calpha+%7D+%5Cright%29d%7B%7B%5Cvec+%5Cvartheta+%7D_m%7D%7D+%5C%5C+%26%3D+%5Cint+%7B%5Cprod%5Climits_%7Bk+%3D+1%7D%5EK+%7B%7B%7B%5Cleft%28+%7B%5Cvartheta+_m%5E%7B%5Cleft%28+k+%5Cright%29%7D%7D+%5Cright%29%7D%5E%7Bn_m%5E%7B%5Cleft%28+k+%5Cright%29%7D%7D%7D%5Cfrac%7B1%7D%7B%7B%5CDelta+%5Cleft%28+%7B%5Cvec+%5Calpha+%7D+%5Cright%29%7D%7D%5Cprod%5Climits_%7Bk+%3D+1%7D%5EK+%7B%7B%7B%5Cleft%28+%7B%5Cvartheta+_m%5E%7B%5Cleft%28+k+%5Cright%29%7D%7D+%5Cright%29%7D%5E%7B%7B%5Calpha+_k%7D+-+1%7D%7D%7D+%7D+d%7B%7B%5Cvec+%5Cvartheta+%7D_m%7D%7D+%5C%5C+%26%3D+%5Cfrac%7B1%7D%7B%7B%5CDelta+%5Cleft%28+%7B%5Cvec+%5Calpha+%7D+%5Cright%29%7D%7D%5Cint+%7B%5Cprod%5Climits_%7Bk+%3D+1%7D%5EK+%7B%7B%7B%5Cleft%28+%7B%5Cvartheta+_m%5E%7B%5Cleft%28+k+%5Cright%29%7D%7D+%5Cright%29%7D%5E%7Bn_m%5E%7B%5Cleft%28+k+%5Cright%29%7D+%2B+%7B%5Calpha+_k%7D+-+1%7D%7D%7D+d%7B%7B%5Cvec+%5Cvartheta+%7D_m%7D%7D+%5C%5C+%26%3D+%5Cfrac%7B%7B%5CDelta+%5Cleft%28+%7B%5Cvec+%5Calpha+%2B+%7B%7B%5Cvec+n%7D_m%7D%7D+%5Cright%29%7D%7D%7B%7B%5CDelta+%5Cleft%28+%7B%5Cvec+%5Calpha+%7D+%5Cright%29%7D%7D+%5Ctag%7B77%7D%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*}p\\left( {{{\\vec z}_m}|\\vec \\alpha } \\right) &amp;= \\int {p\\left( {{{\\vec z}_m}|{{\\vec \\vartheta }_m}} \\right)p\\left( {{{\\vec \\vartheta }_m}|\\vec \\alpha } \\right)d{{\\vec \\vartheta }_m}} \\\\ &amp;= \\int {\\prod\\limits_{k = 1}^K {{{\\left( {\\vartheta _m^{\\left( k \\right)}} \\right)}^{n_m^{\\left( k \\right)}}}\\frac{1}{{\\Delta \\left( {\\vec \\alpha } \\right)}}\\prod\\limits_{k = 1}^K {{{\\left( {\\vartheta _m^{\\left( k \\right)}} \\right)}^{{\\alpha _k} - 1}}} } d{{\\vec \\vartheta }_m}} \\\\ &amp;= \\frac{1}{{\\Delta \\left( {\\vec \\alpha } \\right)}}\\int {\\prod\\limits_{k = 1}^K {{{\\left( {\\vartheta _m^{\\left( k \\right)}} \\right)}^{n_m^{\\left( k \\right)} + {\\alpha _k} - 1}}} d{{\\vec \\vartheta }_m}} \\\\ &amp;= \\frac{{\\Delta \\left( {\\vec \\alpha + {{\\vec n}_m}} \\right)}}{{\\Delta \\left( {\\vec \\alpha } \\right)}} \\tag{77}\\end{align*}\" eeimg=\"1\"/> </p><p>整个语料中的 <img src=\"https://www.zhihu.com/equation?tex=+M+\" alt=\" M \" eeimg=\"1\"/> 篇文档是相互独立的，所以可以得到语料中主题的概率为：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7Dp%5Cleft%28+%7B%5Cvec+z%7C%5Cvec+%5Calpha+%7D+%5Cright%29+%3D+%5Cprod%5Climits_%7Bm+%3D+1%7D%5EM+%7Bp%5Cleft%28+%7B%7B%7B%5Cvec+z%7D_m%7D%7C%5Cvec+%5Calpha+%7D+%5Cright%29%7D+%3D+%5Cprod%5Climits_%7Bm+%3D+1%7D%5EM+%7B%5Cfrac%7B%7B%5CDelta+%5Cleft%28+%7B%5Cvec+%5Calpha+%2B+%7B%7B%5Cvec+n%7D_m%7D%7D+%5Cright%29%7D%7D%7B%7B%5CDelta+%5Cleft%28+%7B%5Cvec+%5Calpha+%7D+%5Cright%29%7D%7D%7D%5Ctag%7B78%7D%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*}p\\left( {\\vec z|\\vec \\alpha } \\right) = \\prod\\limits_{m = 1}^M {p\\left( {{{\\vec z}_m}|\\vec \\alpha } \\right)} = \\prod\\limits_{m = 1}^M {\\frac{{\\Delta \\left( {\\vec \\alpha + {{\\vec n}_m}} \\right)}}{{\\Delta \\left( {\\vec \\alpha } \\right)}}}\\tag{78}\\end{align*}\" eeimg=\"1\"/> </p><p>2） <img src=\"https://www.zhihu.com/equation?tex=%5Cvec+%5Cbeta+%5Cunderbrace+%7B%5Cxrightarrow%7B%7B%5Cbegin%7Barray%7D%7B%2A%7B20%7D%7Bc%7D%7D+%7B%7D%26%7B%7D+%5Cend%7Barray%7D%7D%7D%7D_%7BDir%7D%7B%5Cvec+%5Cvarphi+_k%7D%5Cunderbrace+%7B%5Cxrightarrow%7B%7B%5Cbegin%7Barray%7D%7B%2A%7B20%7D%7Bc%7D%7D+%7B%7D%26%7B%7D+%5Cend%7Barray%7D%7D%7D%7D_%7BMult%7D%7B%5Cvec+w_k%7D\" alt=\"\\vec \\beta \\underbrace {\\xrightarrow{{\\begin{array}{*{20}{c}} {}&amp;{} \\end{array}}}}_{Dir}{\\vec \\varphi _k}\\underbrace {\\xrightarrow{{\\begin{array}{*{20}{c}} {}&amp;{} \\end{array}}}}_{Mult}{\\vec w_k}\" eeimg=\"1\"/> ：第一步对 Dir 分布进行 <img src=\"https://www.zhihu.com/equation?tex=+K+\" alt=\" K \" eeimg=\"1\"/> 次采样得到样本 <img src=\"https://www.zhihu.com/equation?tex=%5Cleft%5C%7B+%7B%7B%7B%5Cvec+%5Cvarphi+%7D_k%7D%7D+%5Cright%5C%7D_%7Bk+%3D+1%7D%5EK\" alt=\"\\left\\{ {{{\\vec \\varphi }_k}} \\right\\}_{k = 1}^K\" eeimg=\"1\"/> （从第二个坛子中独立地抽取了 <img src=\"https://www.zhihu.com/equation?tex=+K+\" alt=\" K \" eeimg=\"1\"/> 个topic-word骰子 <img src=\"https://www.zhihu.com/equation?tex=%5Cleft%5C%7B+%7B%7B%7B%5Cvec+%5Cvarphi+%7D_k%7D%7D+%5Cright%5C%7D_%7Bk+%3D+1%7D%5EK%EF%BC%89\" alt=\"\\left\\{ {{{\\vec \\varphi }_k}} \\right\\}_{k = 1}^K）\" eeimg=\"1\"/> ；第二步根据之前得到的主题 <img src=\"https://www.zhihu.com/equation?tex=+%5Cvec+z\" alt=\" \\vec z\" eeimg=\"1\"/> ，为每个 <img src=\"https://www.zhihu.com/equation?tex=+%7Bz_%7Bm%2Cn%7D%7D+\" alt=\" {z_{m,n}} \" eeimg=\"1\"/> 生成对应的词 <img src=\"https://www.zhihu.com/equation?tex=+%7Bw_%7Bm%2Cn%7D%7D\" alt=\" {w_{m,n}}\" eeimg=\"1\"/> ， <img src=\"https://www.zhihu.com/equation?tex=%5Cvec+z+\" alt=\"\\vec z \" eeimg=\"1\"/> 中的值有 <img src=\"https://www.zhihu.com/equation?tex=+K+\" alt=\" K \" eeimg=\"1\"/> 种不同的取值（因为我们假设语料有 <img src=\"https://www.zhihu.com/equation?tex=+K+\" alt=\" K \" eeimg=\"1\"/> 个主题），所以可以将 <img src=\"https://www.zhihu.com/equation?tex=+%5Cvec+z+\" alt=\" \\vec z \" eeimg=\"1\"/> 中的元素分为 <img src=\"https://www.zhihu.com/equation?tex=+K+\" alt=\" K \" eeimg=\"1\"/> 类。我们现在为第 <img src=\"https://www.zhihu.com/equation?tex=+k+\" alt=\" k \" eeimg=\"1\"/> 个主题生成对应的词，那么需要选择编号为 <img src=\"https://www.zhihu.com/equation?tex=+k+\" alt=\" k \" eeimg=\"1\"/> 的topic-word骰子，该骰子有 <img src=\"https://www.zhihu.com/equation?tex=+V+\" alt=\" V \" eeimg=\"1\"/> 个面，每个面表示一个词，那么在一次投掷骰子过程中，每个词的概率为 <img src=\"https://www.zhihu.com/equation?tex=+%7B%5Cvec+%5Cvarphi+_k%7D+%3D+%5Cleft%28+%7B%5Cvarphi+_k%5E%7B%5Cleft%28+1+%5Cright%29%7D%2C%5Cvarphi+_k%5E%7B%5Cleft%28+2+%5Cright%29%7D%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2C%5Cvarphi+_k%5E%7B%5Cleft%28+V+%5Cright%29%7D%7D+%5Cright%29+\" alt=\" {\\vec \\varphi _k} = \\left( {\\varphi _k^{\\left( 1 \\right)},\\varphi _k^{\\left( 2 \\right)}, \\cdot \\cdot \\cdot ,\\varphi _k^{\\left( V \\right)}} \\right) \" eeimg=\"1\"/> ，第 <img src=\"https://www.zhihu.com/equation?tex=+k+\" alt=\" k \" eeimg=\"1\"/> 个主题有 <img src=\"https://www.zhihu.com/equation?tex=+%7BN_k%7D+\" alt=\" {N_k} \" eeimg=\"1\"/> 个词，所以需要投掷 <img src=\"https://www.zhihu.com/equation?tex=+%7BN_k%7D+\" alt=\" {N_k} \" eeimg=\"1\"/> 次骰子，为该主题生成 <img src=\"https://www.zhihu.com/equation?tex=+%7BN_k%7D+\" alt=\" {N_k} \" eeimg=\"1\"/> 个词。在 <img src=\"https://www.zhihu.com/equation?tex=+%7BN_k%7D+\" alt=\" {N_k} \" eeimg=\"1\"/> 次投掷过程中，每个词出现的次数为 <img src=\"https://www.zhihu.com/equation?tex=+%7B%5Cvec+n_k%7D+%3D+%5Cleft%28+%7Bn_k%5E%7B%5Cleft%28+1+%5Cright%29%7D%2Cn_k%5E%7B%5Cleft%28+2+%5Cright%29%7D%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2Cn_k%5E%7B%5Cleft%28+V+%5Cright%29%7D%7D+%5Cright%29\" alt=\" {\\vec n_k} = \\left( {n_k^{\\left( 1 \\right)},n_k^{\\left( 2 \\right)}, \\cdot \\cdot \\cdot ,n_k^{\\left( V \\right)}} \\right)\" eeimg=\"1\"/> ，那么 <img src=\"https://www.zhihu.com/equation?tex=+%7B%5Cvec+n_k%7D+\" alt=\" {\\vec n_k} \" eeimg=\"1\"/> 服从多项式分布 <img src=\"https://www.zhihu.com/equation?tex=+Mult%5Cleft%28+%7B%7B%7B%5Cvec+n%7D_k%7D%7C%7B%7B%5Cvec+%5Cvarphi+%7D_k%7D%2C%7BN_k%7D%7D+%5Cright%29\" alt=\" Mult\\left( {{{\\vec n}_k}|{{\\vec \\varphi }_k},{N_k}} \\right)\" eeimg=\"1\"/> ，可以采用贝叶斯估计对参数 <img src=\"https://www.zhihu.com/equation?tex=+%7B%5Cvec+%5Cvarphi+_k%7D+\" alt=\" {\\vec \\varphi _k} \" eeimg=\"1\"/> 进行估计。</p><ul><li><img src=\"https://www.zhihu.com/equation?tex=%7B%5Cvec+%5Cvarphi+_k%7D+\" alt=\"{\\vec \\varphi _k} \" eeimg=\"1\"/> 的先验分布为 <img src=\"https://www.zhihu.com/equation?tex=+Dir%5Cleft%28+%7B%7B%7B%5Cvec+%5Cvarphi+%7D_k%7D%7C%5Cvec+%5Cbeta+%7D+%5Cright%29\" alt=\" Dir\\left( {{{\\vec \\varphi }_k}|\\vec \\beta } \\right)\" eeimg=\"1\"/> </li><li>后验分布为（推导过程可以参考1.7节）</li></ul><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+p%5Cleft%28+%7B%7B%7B%5Cvec+%5Cvarphi+%7D_k%7D%7C%7B%7B%5Cvec+n%7D_k%7D%2C%5Cvec+%5Cbeta+%7D+%5Cright%29+%26%3D+%5Cfrac%7B%7Bp%5Cleft%28+%7B%7B%7B%5Cvec+n%7D_k%7D%7C%7B%7B%5Cvec+%5Cvarphi+%7D_k%7D%7D+%5Cright%29p%5Cleft%28+%7B%7B%7B%5Cvec+%5Cvarphi+%7D_k%7D%7C%5Cvec+%5Cbeta+%7D+%5Cright%29%7D%7D%7B%7B%5Cint+%7Bp%5Cleft%28+%7B%7B%7B%5Cvec+n%7D_k%7D%7C%7B%7B%5Cvec+%5Cvarphi+%7D_k%7D%7D+%5Cright%29p%5Cleft%28+%7B%7B%7B%5Cvec+%5Cvarphi+%7D_k%7D%7C%5Cvec+%5Cbeta+%7D+%5Cright%29d%7B%7B%5Cvec+%5Cvarphi+%7D_k%7D%7D+%7D%7D%5C%5C+%26%3D+%5Cfrac%7B%7Bmult%5Cleft%28+%7B%7B%7B%5Cvec+n%7D_k%7D%7C%7B%7B%5Cvec+%5Cvarphi+%7D_k%7D%2C%7BN_k%7D%7D+%5Cright%29Dir%5Cleft%28+%7B%7B%7B%5Cvec+%5Cvarphi+%7D_k%7D%7C%5Cvec+%5Cbeta+%7D+%5Cright%29%7D%7D%7B%7B%5Cint+%7Bmult%5Cleft%28+%7B%7B%7B%5Cvec+n%7D_k%7D%7C%7B%7B%5Cvec+%5Cvarphi+%7D_k%7D%2C%7BN_k%7D%7D+%5Cright%29Dir%5Cleft%28+%7B%7B%7B%5Cvec+%5Cvarphi+%7D_k%7D%7C%5Cvec+%5Cbeta+%7D+%5Cright%29d%7B%7B%5Cvec+%5Cvarphi+%7D_k%7D%7D+%7D%7D%5C%5C+%26%3D+Dir%5Cleft%28+%7B%7B%7B%5Cvec+%5Cvarphi+%7D_k%7D%7C%5Cvec+%5Cbeta+%2B+%7B%7B%5Cvec+n%7D_k%7D%7D+%5Cright%29+%5Ctag%7B79%7D%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} p\\left( {{{\\vec \\varphi }_k}|{{\\vec n}_k},\\vec \\beta } \\right) &amp;= \\frac{{p\\left( {{{\\vec n}_k}|{{\\vec \\varphi }_k}} \\right)p\\left( {{{\\vec \\varphi }_k}|\\vec \\beta } \\right)}}{{\\int {p\\left( {{{\\vec n}_k}|{{\\vec \\varphi }_k}} \\right)p\\left( {{{\\vec \\varphi }_k}|\\vec \\beta } \\right)d{{\\vec \\varphi }_k}} }}\\\\ &amp;= \\frac{{mult\\left( {{{\\vec n}_k}|{{\\vec \\varphi }_k},{N_k}} \\right)Dir\\left( {{{\\vec \\varphi }_k}|\\vec \\beta } \\right)}}{{\\int {mult\\left( {{{\\vec n}_k}|{{\\vec \\varphi }_k},{N_k}} \\right)Dir\\left( {{{\\vec \\varphi }_k}|\\vec \\beta } \\right)d{{\\vec \\varphi }_k}} }}\\\\ &amp;= Dir\\left( {{{\\vec \\varphi }_k}|\\vec \\beta + {{\\vec n}_k}} \\right) \\tag{79}\\end{align*}\" eeimg=\"1\"/> </p><ul><li><img src=\"https://www.zhihu.com/equation?tex=%7B%5Cvec+%5Cvarphi+_k%7D+\" alt=\"{\\vec \\varphi _k} \" eeimg=\"1\"/> 的贝叶斯估计值为</li></ul><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7DE%5Cleft%28+%7B%7B%7B%5Cvec+%5Cvarphi+%7D_k%7D%7D+%5Cright%29+%3D+%5Cleft%28+%7B%5Cfrac%7B%7B%7B%5Cbeta+_1%7D+%2B+n_k%5E%7B%5Cleft%28+1+%5Cright%29%7D%7D%7D%7B%7B%5Csum%5Cnolimits_%7Bt+%3D+1%7D%5EV+%7B%5Cleft%28+%7B%7B%5Cbeta+_t%7D+%2B+n_k%5E%7B%5Cleft%28+t+%5Cright%29%7D%7D+%5Cright%29%7D+%7D%7D%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2C%5Cfrac%7B%7B%7B%5Cbeta+_V%7D+%2B+n_k%5E%7B%5Cleft%28+V+%5Cright%29%7D%7D%7D%7B%7B%5Csum%5Cnolimits_%7Bt+%3D+1%7D%5EV+%7B%5Cleft%28+%7B%7B%5Cbeta+_t%7D+%2B+n_k%5E%7B%5Cleft%28+t+%5Cright%29%7D%7D+%5Cright%29%7D+%7D%7D%7D+%5Cright%29+%5Ctag%7B80%7D%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*}E\\left( {{{\\vec \\varphi }_k}} \\right) = \\left( {\\frac{{{\\beta _1} + n_k^{\\left( 1 \\right)}}}{{\\sum\\nolimits_{t = 1}^V {\\left( {{\\beta _t} + n_k^{\\left( t \\right)}} \\right)} }}, \\cdot \\cdot \\cdot ,\\frac{{{\\beta _V} + n_k^{\\left( V \\right)}}}{{\\sum\\nolimits_{t = 1}^V {\\left( {{\\beta _t} + n_k^{\\left( t \\right)}} \\right)} }}} \\right) \\tag{80}\\end{align*}\" eeimg=\"1\"/> </p><p>下面我们计算第 <img src=\"https://www.zhihu.com/equation?tex=+k+\" alt=\" k \" eeimg=\"1\"/> 个主题的词概率分布：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7Dp%5Cleft%28+%7B%7B%7B%5Cvec+w%7D_k%7D%7C%5Cvec+%5Cbeta+%7D+%5Cright%29+%26%3D+%5Cint+%7Bp%5Cleft%28+%7B%7B%7B%5Cvec+w%7D_k%7D%7C%7B%7B%5Cvec+%5Cvarphi+%7D_k%7D%7D+%5Cright%29p%5Cleft%28+%7B%7B%7B%5Cvec+%5Cvarphi+%7D_k%7D%7C%5Cvec+%5Cbeta+%7D+%5Cright%29d%7B%7B%5Cvec+%5Cvarphi+%7D_k%7D%7D+%5C%5C+%26%3D+%5Cint+%7B%5Cprod%5Climits_%7Bt+%3D+1%7D%5EV+%7B%7B%7B%5Cleft%28+%7B%5Cvarphi+_k%5E%7B%5Cleft%28+t+%5Cright%29%7D%7D+%5Cright%29%7D%5E%7Bn_k%5E%7B%5Cleft%28+t+%5Cright%29%7D%7D%7D%5Cfrac%7B1%7D%7B%7B%5CDelta+%5Cleft%28+%7B%5Cvec+%5Cbeta+%7D+%5Cright%29%7D%7D%5Cprod%5Climits_%7Bt+%3D+1%7D%5EV+%7B%7B%7B%5Cleft%28+%7B%5Cvarphi+_k%5E%7B%5Cleft%28+t+%5Cright%29%7D%7D+%5Cright%29%7D%5E%7B%7B%5Cbeta+_t%7D+-+1%7D%7D%7D+%7D+d%7B%7B%5Cvec+%5Cvarphi+%7D_k%7D%7D+%5C%5C+%26%3D+%5Cfrac%7B1%7D%7B%7B%5CDelta+%5Cleft%28+%7B%5Cvec+%5Cbeta+%7D+%5Cright%29%7D%7D%5Cint+%7B%5Cprod%5Climits_%7Bt+%3D+1%7D%5EV+%7B%7B%7B%5Cleft%28+%7B%5Cvarphi+_k%5E%7B%5Cleft%28+t+%5Cright%29%7D%7D+%5Cright%29%7D%5E%7Bn_k%5E%7B%5Cleft%28+t+%5Cright%29%7D+%2B+%7B%5Cbeta+_t%7D+-+1%7D%7D%7D+d%7B%7B%5Cvec+%5Cvarphi+%7D_k%7D%7D+%5C%5C+%26%3D+%5Cfrac%7B%7B%5CDelta+%5Cleft%28+%7B%5Cvec+%5Cbeta+%2B+%7B%7B%5Cvec+n%7D_k%7D%7D+%5Cright%29%7D%7D%7B%7B%5CDelta+%5Cleft%28+%7B%5Cvec+%5Cbeta+%7D+%5Cright%29%7D%7D+%5Ctag%7B81%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*}p\\left( {{{\\vec w}_k}|\\vec \\beta } \\right) &amp;= \\int {p\\left( {{{\\vec w}_k}|{{\\vec \\varphi }_k}} \\right)p\\left( {{{\\vec \\varphi }_k}|\\vec \\beta } \\right)d{{\\vec \\varphi }_k}} \\\\ &amp;= \\int {\\prod\\limits_{t = 1}^V {{{\\left( {\\varphi _k^{\\left( t \\right)}} \\right)}^{n_k^{\\left( t \\right)}}}\\frac{1}{{\\Delta \\left( {\\vec \\beta } \\right)}}\\prod\\limits_{t = 1}^V {{{\\left( {\\varphi _k^{\\left( t \\right)}} \\right)}^{{\\beta _t} - 1}}} } d{{\\vec \\varphi }_k}} \\\\ &amp;= \\frac{1}{{\\Delta \\left( {\\vec \\beta } \\right)}}\\int {\\prod\\limits_{t = 1}^V {{{\\left( {\\varphi _k^{\\left( t \\right)}} \\right)}^{n_k^{\\left( t \\right)} + {\\beta _t} - 1}}} d{{\\vec \\varphi }_k}} \\\\ &amp;= \\frac{{\\Delta \\left( {\\vec \\beta + {{\\vec n}_k}} \\right)}}{{\\Delta \\left( {\\vec \\beta } \\right)}} \\tag{81} \\end{align*}\" eeimg=\"1\"/> </p><p>整个语料中的 <img src=\"https://www.zhihu.com/equation?tex=+K+\" alt=\" K \" eeimg=\"1\"/> 个主题是相互独立的，所以可以得到语料中词的概率为：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7Dp%5Cleft%28+%7B%5Cvec+w%7C%5Cvec+z%2C%5Cvec+%5Cbeta+%7D+%5Cright%29+%3D+%5Cprod%5Climits_%7Bk+%3D+1%7D%5EK+%7Bp%5Cleft%28+%7B%7B%7B%5Cvec+w%7D_k%7D%7C%5Cvec+%5Cbeta+%7D+%5Cright%29%7D+%3D+%5Cprod%5Climits_%7Bk+%3D+1%7D%5EK+%7B%5Cfrac%7B%7B%5CDelta+%5Cleft%28+%7B%5Cvec+%5Cbeta+%2B+%7B%7B%5Cvec+n%7D_k%7D%7D+%5Cright%29%7D%7D%7B%7B%5CDelta+%5Cleft%28+%7B%5Cvec+%5Cbeta+%7D+%5Cright%29%7D%7D%7D+%5Ctag%7B82%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*}p\\left( {\\vec w|\\vec z,\\vec \\beta } \\right) = \\prod\\limits_{k = 1}^K {p\\left( {{{\\vec w}_k}|\\vec \\beta } \\right)} = \\prod\\limits_{k = 1}^K {\\frac{{\\Delta \\left( {\\vec \\beta + {{\\vec n}_k}} \\right)}}{{\\Delta \\left( {\\vec \\beta } \\right)}}} \\tag{82} \\end{align*}\" eeimg=\"1\"/> </p><p>由公式(74)、(78)、(82) 可得联合概率分布为：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7Dp%5Cleft%28+%7B%5Cvec+w%2C%5Cvec+z%7C%5Cvec+%5Calpha+%2C%5Cvec+%5Cbeta+%7D+%5Cright%29+%26%3D+p%5Cleft%28+%7B%5Cvec+w%7C%5Cvec+z%2C%5Cvec+%5Cbeta+%7D+%5Cright%29p%5Cleft%28+%7B%5Cvec+z%7C%5Cvec+%5Calpha+%7D+%5Cright%29%5C%5C+%26%3D+%5Cprod%5Climits_%7Bk+%3D+1%7D%5EK+%7B%5Cfrac%7B%7B%5CDelta+%5Cleft%28+%7B%5Cvec+%5Cbeta+%2B+%7B%7B%5Cvec+n%7D_k%7D%7D+%5Cright%29%7D%7D%7B%7B%5CDelta+%5Cleft%28+%7B%5Cvec+%5Cbeta+%7D+%5Cright%29%7D%7D%5Cprod%5Climits_%7Bm+%3D+1%7D%5EM+%7B%5Cfrac%7B%7B%5CDelta+%5Cleft%28+%7B%5Cvec+%5Calpha+%2B+%7B%7B%5Cvec+n%7D_m%7D%7D+%5Cright%29%7D%7D%7B%7B%5CDelta+%5Cleft%28+%7B%5Cvec+%5Calpha+%7D+%5Cright%29%7D%7D%7D+%7D+%5Ctag%7B83%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*}p\\left( {\\vec w,\\vec z|\\vec \\alpha ,\\vec \\beta } \\right) &amp;= p\\left( {\\vec w|\\vec z,\\vec \\beta } \\right)p\\left( {\\vec z|\\vec \\alpha } \\right)\\\\ &amp;= \\prod\\limits_{k = 1}^K {\\frac{{\\Delta \\left( {\\vec \\beta + {{\\vec n}_k}} \\right)}}{{\\Delta \\left( {\\vec \\beta } \\right)}}\\prod\\limits_{m = 1}^M {\\frac{{\\Delta \\left( {\\vec \\alpha + {{\\vec n}_m}} \\right)}}{{\\Delta \\left( {\\vec \\alpha } \\right)}}} } \\tag{83} \\end{align*}\" eeimg=\"1\"/> </p><p><b>2. Gibbs Sampling</b></p><p>上面我们已经推导出参数的贝叶斯估计公式，但是仍然存在一个问题，公式中的 <img src=\"https://www.zhihu.com/equation?tex=+%7B%5Cvec+n_k%7D+\" alt=\" {\\vec n_k} \" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=+%7B%5Cvec+n_m%7D+\" alt=\" {\\vec n_m} \" eeimg=\"1\"/> 无法根据语料直接得到，如果我们知道语料中的每个词的主题，即得到 <img src=\"https://www.zhihu.com/equation?tex=%5Cvec+z\" alt=\"\\vec z\" eeimg=\"1\"/> ，那么就可以推断出 <img src=\"https://www.zhihu.com/equation?tex=+%7B%5Cvec+n_k%7D+\" alt=\" {\\vec n_k} \" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=+%7B%5Cvec+n_m%7D\" alt=\" {\\vec n_m}\" eeimg=\"1\"/> ，进一步就可以得出贝叶斯的参数估计 <img src=\"https://www.zhihu.com/equation?tex=%7B%5Cvec+%5Cvartheta+_m%7D+\" alt=\"{\\vec \\vartheta _m} \" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%7B%5Cvec+%5Cvarphi+_k%7D+\" alt=\"{\\vec \\varphi _k} \" eeimg=\"1\"/> 。</p><p>我们需要利用 Gibbs Sampling 对 <img src=\"https://www.zhihu.com/equation?tex=+p%5Cleft%28+%7B%5Cvec+z%7C%5Cvec+w%7D+%5Cright%29+\" alt=\" p\\left( {\\vec z|\\vec w} \\right) \" eeimg=\"1\"/> 进行采样来得到 <img src=\"https://www.zhihu.com/equation?tex=+%5Cvec+z\" alt=\" \\vec z\" eeimg=\"1\"/> 。根据1.10节 Gibbs Sampling 的原理可知，我们首先需要推导条件概率 <img src=\"https://www.zhihu.com/equation?tex=+p%5Cleft%28+%7B%7Bz_i%7D+%3D+k%7C%7B%7B%5Cvec+z%7D_%7B+-+i%7D%7D%2C%5Cvec+w%7D+%5Cright%29\" alt=\" p\\left( {{z_i} = k|{{\\vec z}_{ - i}},\\vec w} \\right)\" eeimg=\"1\"/> 。 先介绍一些符号定义。</p><ul><li><img src=\"https://www.zhihu.com/equation?tex=i+%3D+%5Cleft%28+%7Bm%2Cn%7D+%5Cright%29\" alt=\"i = \\left( {m,n} \\right)\" eeimg=\"1\"/> ：下标索引；</li><li><img src=\"https://www.zhihu.com/equation?tex=-+i\" alt=\"- i\" eeimg=\"1\"/> ：表示去除下标为 <img src=\"https://www.zhihu.com/equation?tex=+i+\" alt=\" i \" eeimg=\"1\"/> 的词；</li><li><img src=\"https://www.zhihu.com/equation?tex=%7Bw_i%7D+%3D+t\" alt=\"{w_i} = t\" eeimg=\"1\"/> ：第 <img src=\"https://www.zhihu.com/equation?tex=+m+\" alt=\" m \" eeimg=\"1\"/> 篇文档中第 <img src=\"https://www.zhihu.com/equation?tex=+n+\" alt=\" n \" eeimg=\"1\"/> 个词为 <img src=\"https://www.zhihu.com/equation?tex=+t\" alt=\" t\" eeimg=\"1\"/> ；</li><li><img src=\"https://www.zhihu.com/equation?tex=%7Bz_i%7D+%3D+k\" alt=\"{z_i} = k\" eeimg=\"1\"/> ：第 <img src=\"https://www.zhihu.com/equation?tex=+m+\" alt=\" m \" eeimg=\"1\"/> 篇文档中第 <img src=\"https://www.zhihu.com/equation?tex=+n+\" alt=\" n \" eeimg=\"1\"/> 个词的主题为 <img src=\"https://www.zhihu.com/equation?tex=+k\" alt=\" k\" eeimg=\"1\"/> ；</li><li><img src=\"https://www.zhihu.com/equation?tex=n_%7Bk%2C+-+i%7D%5E%7B%5Cleft%28+t+%5Cright%29%7D\" alt=\"n_{k, - i}^{\\left( t \\right)}\" eeimg=\"1\"/> ：除去下标为 <img src=\"https://www.zhihu.com/equation?tex=+i+\" alt=\" i \" eeimg=\"1\"/> 这个词，剩下的所有词中，词 <img src=\"https://www.zhihu.com/equation?tex=+t+\" alt=\" t \" eeimg=\"1\"/> 属于主题 <img src=\"https://www.zhihu.com/equation?tex=+k+\" alt=\" k \" eeimg=\"1\"/> 的统计次数， <img src=\"https://www.zhihu.com/equation?tex=%7B%5Cvec+n_%7Bk%2C+-+i%7D%7D+%3D+%5Cleft%28+%7Bn_k%5E%7B%5Cleft%28+1+%5Cright%29%7D%2Cn_k%5E%7B%5Cleft%28+2+%5Cright%29%7D%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2Cn_k%5E%7B%5Cleft%28+t+%5Cright%29%7D+-+1%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2Cn_k%5E%7B%5Cleft%28+V+%5Cright%29%7D%7D+%5Cright%29\" alt=\"{\\vec n_{k, - i}} = \\left( {n_k^{\\left( 1 \\right)},n_k^{\\left( 2 \\right)}, \\cdot \\cdot \\cdot ,n_k^{\\left( t \\right)} - 1, \\cdot \\cdot \\cdot ,n_k^{\\left( V \\right)}} \\right)\" eeimg=\"1\"/> （这里假设 <img src=\"https://www.zhihu.com/equation?tex=%7Bw_i%7D+%3D+t%2C%7Bz_i%7D+%3D+k\" alt=\"{w_i} = t,{z_i} = k\" eeimg=\"1\"/> ）；</li><li><img src=\"https://www.zhihu.com/equation?tex=n_%7Bm%2C+-+i%7D%5E%7B%5Cleft%28+k+%5Cright%29%7D\" alt=\"n_{m, - i}^{\\left( k \\right)}\" eeimg=\"1\"/> ：除去下标为 <img src=\"https://www.zhihu.com/equation?tex=+i+\" alt=\" i \" eeimg=\"1\"/> 的这个词，第 <img src=\"https://www.zhihu.com/equation?tex=+m\" alt=\" m\" eeimg=\"1\"/> 篇文档中主题 <img src=\"https://www.zhihu.com/equation?tex=+k+\" alt=\" k \" eeimg=\"1\"/> 产生词的个数， <img src=\"https://www.zhihu.com/equation?tex=+%7B%5Cvec+n_%7Bm%2C+-+i%7D%7D+%3D+%5Cleft%28+%7Bn_m%5E%7B%5Cleft%28+1+%5Cright%29%7D%2Cn_m%5E%7B%5Cleft%28+2+%5Cright%29%7D%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2Cn_m%5E%7B%5Cleft%28+k+%5Cright%29%7D+-+1%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2Cn_m%5E%7B%5Cleft%28+K+%5Cright%29%7D%7D+%5Cright%29+\" alt=\" {\\vec n_{m, - i}} = \\left( {n_m^{\\left( 1 \\right)},n_m^{\\left( 2 \\right)}, \\cdot \\cdot \\cdot ,n_m^{\\left( k \\right)} - 1, \\cdot \\cdot \\cdot ,n_m^{\\left( K \\right)}} \\right) \" eeimg=\"1\"/> （这里假设 <img src=\"https://www.zhihu.com/equation?tex=+%7Bz_i%7D+%3D+k+\" alt=\" {z_i} = k \" eeimg=\"1\"/> ）；</li><li><img src=\"https://www.zhihu.com/equation?tex=%5Cvec+z+%3D+%5Cleft%5C%7B+%7B%7Bz_i%7D+%3D+k%2C%7B%7B%5Cvec+z%7D_%7B+-+i%7D%7D%7D+%5Cright%5C%7D\" alt=\"\\vec z = \\left\\{ {{z_i} = k,{{\\vec z}_{ - i}}} \\right\\}\" eeimg=\"1\"/> ：语料的主题；</li><li><img src=\"https://www.zhihu.com/equation?tex=%5Cvec+w+%3D+%5Cleft%5C%7B+%7B%7Bw_i%7D+%3D+t%2C%7B%7B%5Cvec+w%7D_%7B+-+i%7D%7D%7D+%5Cright%5C%7D\" alt=\"\\vec w = \\left\\{ {{w_i} = t,{{\\vec w}_{ - i}}} \\right\\}\" eeimg=\"1\"/> ：语料的单词。</li></ul><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7Dp%5Cleft%28+%7B%7Bz_i%7D+%3D+k%7C%7B%7B%5Cvec+z%7D_%7B+-+i%7D%7D%2C%5Cvec+w%7D+%5Cright%29+%26%3D+%5Cfrac%7B%7Bp%5Cleft%28+%7B%5Cvec+w%2C%5Cvec+z%7D+%5Cright%29%7D%7D%7B%7Bp%5Cleft%28+%7B%5Cvec+w%2C%7B%7B%5Cvec+z%7D_%7B+-+i%7D%7D%7D+%5Cright%29%7D%7D%5C%5C+%26%3D+%5Cfrac%7B%7Bp%5Cleft%28+%7B%5Cvec+w%7C%5Cvec+z%7D+%5Cright%29%7D%7D%7B%7Bp%5Cleft%28+%7B%7B%7B%5Cvec+w%7D_%7B+-+i%7D%7D%7C%7B%7B%5Cvec+z%7D_%7B+-+i%7D%7D%7D+%5Cright%29p%5Cleft%28+%7B%7Bw_i%7D%7D+%5Cright%29%7D%7D+%5Ccdot+%5Cfrac%7B%7Bp%5Cleft%28+%7B%5Cvec+z%7D+%5Cright%29%7D%7D%7B%7Bp%5Cleft%28+%7B%7B%7B%5Cvec+z%7D_%7B+-+i%7D%7D%7D+%5Cright%29%7D%7D%5C%5C+%26%5Cpropto+%5Cfrac%7B%7B%5CDelta+%5Cleft%28+%7B%7B%7B%5Cvec+n%7D_k%7D+%2B+%5Cvec+%5Cbeta+%7D+%5Cright%29%7D%7D%7B%7B%5CDelta+%5Cleft%28+%7B%7B%7B%5Cvec+n%7D_%7Bk%2C+-+i%7D%7D+%2B+%5Cvec+%5Cbeta+%7D+%5Cright%29%7D%7D+%5Ccdot+%5Cfrac%7B%7B%5CDelta+%5Cleft%28+%7B%7B%7B%5Cvec+n%7D_m%7D+%2B+%5Cvec+%5Calpha+%7D+%5Cright%29%7D%7D%7B%7B%5CDelta+%5Cleft%28+%7B%7B%7B%5Cvec+n%7D_%7Bm%2C+-+i%7D%7D+%2B+%5Cvec+%5Calpha+%7D+%5Cright%29%7D%7D%5C%5C+%26%3D+%5Cfrac%7B%7Bn_%7Bk%2C+-+i%7D%5E%7B%5Cleft%28+t+%5Cright%29%7D+%2B+%7B%5Cbeta+%5E%7B%5Cleft%28+t+%5Cright%29%7D%7D%7D%7D%7B%7B%5Csum%5Cnolimits_%7Bv+%3D+1%7D%5EV+%7B%5Cleft%28+%7Bn_%7Bk%2C+-+i%7D%5E%7B%5Cleft%28+v+%5Cright%29%7D+%2B+%7B%5Cbeta+%5E%7B%5Cleft%28+v+%5Cright%29%7D%7D%7D+%5Cright%29%7D+%7D%7D+%5Ccdot+%5Cfrac%7B%7Bn_%7Bm%2C+-+i%7D%5E%7B%5Cleft%28+k+%5Cright%29%7D+%2B+%7B%5Calpha+%5E%7B%5Cleft%28+k+%5Cright%29%7D%7D%7D%7D%7B%7B%5Csum%5Cnolimits_%7Bj+%3D+1%7D%5EK+%7B%5Cleft%28+%7Bn_%7Bm%2C+-+i%7D%5E%7B%5Cleft%28+j+%5Cright%29%7D+%2B+%7B%5Calpha+%5E%7B%5Cleft%28+j+%5Cright%29%7D%7D%7D+%5Cright%29%7D+%7D%7D+%5Ctag%7B84%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*}p\\left( {{z_i} = k|{{\\vec z}_{ - i}},\\vec w} \\right) &amp;= \\frac{{p\\left( {\\vec w,\\vec z} \\right)}}{{p\\left( {\\vec w,{{\\vec z}_{ - i}}} \\right)}}\\\\ &amp;= \\frac{{p\\left( {\\vec w|\\vec z} \\right)}}{{p\\left( {{{\\vec w}_{ - i}}|{{\\vec z}_{ - i}}} \\right)p\\left( {{w_i}} \\right)}} \\cdot \\frac{{p\\left( {\\vec z} \\right)}}{{p\\left( {{{\\vec z}_{ - i}}} \\right)}}\\\\ &amp;\\propto \\frac{{\\Delta \\left( {{{\\vec n}_k} + \\vec \\beta } \\right)}}{{\\Delta \\left( {{{\\vec n}_{k, - i}} + \\vec \\beta } \\right)}} \\cdot \\frac{{\\Delta \\left( {{{\\vec n}_m} + \\vec \\alpha } \\right)}}{{\\Delta \\left( {{{\\vec n}_{m, - i}} + \\vec \\alpha } \\right)}}\\\\ &amp;= \\frac{{n_{k, - i}^{\\left( t \\right)} + {\\beta ^{\\left( t \\right)}}}}{{\\sum\\nolimits_{v = 1}^V {\\left( {n_{k, - i}^{\\left( v \\right)} + {\\beta ^{\\left( v \\right)}}} \\right)} }} \\cdot \\frac{{n_{m, - i}^{\\left( k \\right)} + {\\alpha ^{\\left( k \\right)}}}}{{\\sum\\nolimits_{j = 1}^K {\\left( {n_{m, - i}^{\\left( j \\right)} + {\\alpha ^{\\left( j \\right)}}} \\right)} }} \\tag{84} \\end{align*}\" eeimg=\"1\"/> </p><p>1） <img src=\"https://www.zhihu.com/equation?tex=p%5Cleft%28+%7B%7B%7B%5Cvec+w%7D_%7B+-+i%7D%7D%7C%7B%7B%5Cvec+z%7D_%7B+-+i%7D%7D%7D+%5Cright%29+\" alt=\"p\\left( {{{\\vec w}_{ - i}}|{{\\vec z}_{ - i}}} \\right) \" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=+p%5Cleft%28+%7B%7B%7B%5Cvec+z%7D_%7B+-+i%7D%7D%7D+%5Cright%29+\" alt=\" p\\left( {{{\\vec z}_{ - i}}} \\right) \" eeimg=\"1\"/> 的计算过程类似 <img src=\"https://www.zhihu.com/equation?tex=+p%5Cleft%28+%7B%5Cvec+w%7C%5Cvec+z%7D+%5Cright%29+%E5%92%8C+p%5Cleft%28+%7B%5Cvec+z%7D+%5Cright%29\" alt=\" p\\left( {\\vec w|\\vec z} \\right) 和 p\\left( {\\vec z} \\right)\" eeimg=\"1\"/> ，仅仅在计算的时候不考虑下标为 <img src=\"https://www.zhihu.com/equation?tex=+i+\" alt=\" i \" eeimg=\"1\"/> 的这个词，我们假设 <img src=\"https://www.zhihu.com/equation?tex=+%7Bw_i%7D+%3D+t%2C%7Bz_i%7D+%3D+k+\" alt=\" {w_i} = t,{z_i} = k \" eeimg=\"1\"/> ；当已知语料时， <img src=\"https://www.zhihu.com/equation?tex=p%5Cleft%28+%7B%7Bw_i%7D%7D+%5Cright%29\" alt=\"p\\left( {{w_i}} \\right)\" eeimg=\"1\"/> 可以从语料中统计出来，所以可以认为是常量。</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7Dp%5Cleft%28+%7B%5Cvec+w%7C%5Cvec+z%7D+%5Cright%29+%26%3D+%5CDelta+%5Cleft%28+%7B%5Cvec+%5Cbeta+%2B+%7B%7B%5Cvec+n%7D_k%7D%7D+%5Cright%29%5Cprod%5Climits_%7Bz+%3D+1%2Cz+%5Cne+k%7D%5EK+%7B%5Cfrac%7B%7B%5CDelta+%5Cleft%28+%7B%5Cvec+%5Cbeta+%2B+%7B%7B%5Cvec+n%7D_z%7D%7D+%5Cright%29%7D%7D%7B%7B%5CDelta+%5Cleft%28+%7B%5Cvec+%5Cbeta+%7D+%5Cright%29%7D%7D%7D+%5C%5C+p%5Cleft%28+%7B%7B%7B%5Cvec+w%7D_%7B+-+i%7D%7D%7C%7B%7B%5Cvec+z%7D_%7B+-+i%7D%7D%7D+%5Cright%29+%26%3D+%5CDelta+%5Cleft%28+%7B%5Cvec+%5Cbeta+%2B+%7B%7B%5Cvec+n%7D_%7Bk%2C+-+i%7D%7D%7D+%5Cright%29%5Cprod%5Climits_%7Bz+%3D+1%2Cz+%5Cne+k%7D%5EK+%7B%5Cfrac%7B%7B%5CDelta+%5Cleft%28+%7B%5Cvec+%5Cbeta+%2B+%7B%7B%5Cvec+n%7D_z%7D%7D+%5Cright%29%7D%7D%7B%7B%5CDelta+%5Cleft%28+%7B%5Cvec+%5Cbeta+%7D+%5Cright%29%7D%7D%7D+%5C%5C+p%5Cleft%28+%7B%5Cvec+z%7D+%5Cright%29+%26%3D+%5CDelta+%5Cleft%28+%7B%5Cvec+%5Calpha+%2B+%7B%7B%5Cvec+n%7D_m%7D%7D+%5Cright%29%5Cprod%5Climits_%7Bi+%3D+1%2Ci+%5Cne+m%7D%5EM+%7B%5Cfrac%7B%7B%5CDelta+%5Cleft%28+%7B%5Cvec+%5Calpha+%2B+%7B%7B%5Cvec+n%7D_i%7D%7D+%5Cright%29%7D%7D%7B%7B%5CDelta+%5Cleft%28+%7B%5Cvec+%5Calpha+%7D+%5Cright%29%7D%7D%7D+%5C%5C+p%5Cleft%28+%7B%7B%7B%5Cvec+z%7D_%7B+-+i%7D%7D%7D+%5Cright%29+%26%3D+%5CDelta+%5Cleft%28+%7B%5Cvec+%5Calpha+%2B+%7B%7B%5Cvec+n%7D_%7Bm%2C+-+i%7D%7D%7D+%5Cright%29%5Cprod%5Climits_%7Bi+%3D+1%2Ci+%5Cne+m%7D%5EM+%7B%5Cfrac%7B%7B%5CDelta+%5Cleft%28+%7B%5Cvec+%5Calpha+%2B+%7B%7B%5Cvec+n%7D_i%7D%7D+%5Cright%29%7D%7D%7B%7B%5CDelta+%5Cleft%28+%7B%5Cvec+%5Calpha+%7D+%5Cright%29%7D%7D%7D+%5Ctag%7B85%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*}p\\left( {\\vec w|\\vec z} \\right) &amp;= \\Delta \\left( {\\vec \\beta + {{\\vec n}_k}} \\right)\\prod\\limits_{z = 1,z \\ne k}^K {\\frac{{\\Delta \\left( {\\vec \\beta + {{\\vec n}_z}} \\right)}}{{\\Delta \\left( {\\vec \\beta } \\right)}}} \\\\ p\\left( {{{\\vec w}_{ - i}}|{{\\vec z}_{ - i}}} \\right) &amp;= \\Delta \\left( {\\vec \\beta + {{\\vec n}_{k, - i}}} \\right)\\prod\\limits_{z = 1,z \\ne k}^K {\\frac{{\\Delta \\left( {\\vec \\beta + {{\\vec n}_z}} \\right)}}{{\\Delta \\left( {\\vec \\beta } \\right)}}} \\\\ p\\left( {\\vec z} \\right) &amp;= \\Delta \\left( {\\vec \\alpha + {{\\vec n}_m}} \\right)\\prod\\limits_{i = 1,i \\ne m}^M {\\frac{{\\Delta \\left( {\\vec \\alpha + {{\\vec n}_i}} \\right)}}{{\\Delta \\left( {\\vec \\alpha } \\right)}}} \\\\ p\\left( {{{\\vec z}_{ - i}}} \\right) &amp;= \\Delta \\left( {\\vec \\alpha + {{\\vec n}_{m, - i}}} \\right)\\prod\\limits_{i = 1,i \\ne m}^M {\\frac{{\\Delta \\left( {\\vec \\alpha + {{\\vec n}_i}} \\right)}}{{\\Delta \\left( {\\vec \\alpha } \\right)}}} \\tag{85} \\end{align*}\" eeimg=\"1\"/> </p><p>2）我们是推断 <img src=\"https://www.zhihu.com/equation?tex=+i+%3D+%5Cleft%28+%7Bm%2Cn%7D+%5Cright%29+\" alt=\" i = \\left( {m,n} \\right) \" eeimg=\"1\"/> 词 <img src=\"https://www.zhihu.com/equation?tex=+t+\" alt=\" t \" eeimg=\"1\"/> 的主题为 <img src=\"https://www.zhihu.com/equation?tex=+k+\" alt=\" k \" eeimg=\"1\"/> 的条件概率</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D%5CDelta+%5Cleft%28+%7B%7B%7B%5Cvec+n%7D_k%7D+%2B+%5Cvec+%5Cbeta+%7D+%5Cright%29+%26%3D+%5Cfrac%7B%7B%5Cprod%5Cnolimits_%7Bv+%3D+1%7D%5EV+%7B%5CGamma+%5Cleft%28+%7Bn_k%5E%7B%5Cleft%28+v+%5Cright%29%7D+%2B+%7B%5Cbeta+%5E%7B%5Cleft%28+v+%5Cright%29%7D%7D%7D+%5Cright%29%7D+%7D%7D%7B%7B%5CGamma+%5Cleft%28+%7B%5Csum%5Cnolimits_%7Bv+%3D+1%7D%5EV+%7B%5Cleft%28+%7Bn_k%5E%7B%5Cleft%28+v+%5Cright%29%7D+%2B+%7B%5Cbeta+%5E%7B%5Cleft%28+v+%5Cright%29%7D%7D%7D+%5Cright%29%7D+%7D+%5Cright%29%7D%7D%5C%5C+%5CDelta+%5Cleft%28+%7B%7B%7B%5Cvec+n%7D_%7Bk%2C+-+i%7D%7D+%2B+%5Cvec+%5Cbeta+%7D+%5Cright%29+%26%3D+%5Cfrac%7B%7B%5CGamma+%5Cleft%28+%7Bn_k%5E%7B%5Cleft%28+t+%5Cright%29%7D+%2B+%7B%5Cbeta+%5E%7B%5Cleft%28+t+%5Cright%29%7D%7D+-+1%7D+%5Cright%29%5Cprod%5Cnolimits_%7Bv+%3D+1%2Cv+%5Cne+t%7D%5EV+%7B%5CGamma+%5Cleft%28+%7Bn_k%5E%7B%5Cleft%28+v+%5Cright%29%7D+%2B+%7B%5Cbeta+%5E%7B%5Cleft%28+v+%5Cright%29%7D%7D%7D+%5Cright%29%7D+%7D%7D%7B%7B%5CGamma+%5Cleft%28+%7B%5Csum%5Cnolimits_%7Bv+%3D+1%2Cv+%5Cne+t%7D%5EV+%7B%5Cleft%28+%7Bn_k%5E%7B%5Cleft%28+v+%5Cright%29%7D+%2B+%7B%5Cbeta+%5E%7B%5Cleft%28+v+%5Cright%29%7D%7D%7D+%5Cright%29%7D+%2B+n_k%5E%7B%5Cleft%28+t+%5Cright%29%7D+%2B+%7B%5Cbeta+%5E%7B%5Cleft%28+t+%5Cright%29%7D%7D+-+1%7D+%5Cright%29%7D%7D%5C%5C+%5CDelta+%5Cleft%28+%7B%7B%7B%5Cvec+n%7D_m%7D+%2B+%5Cvec+%5Calpha+%7D+%5Cright%29+%26%3D+%5Cfrac%7B%7B%5Cprod%5Cnolimits_%7Bj+%3D+1%7D%5EK+%7B%5CGamma+%5Cleft%28+%7Bn_m%5E%7B%5Cleft%28+j+%5Cright%29%7D+%2B+%7B%5Calpha+%5E%7B%5Cleft%28+j+%5Cright%29%7D%7D%7D+%5Cright%29%7D+%7D%7D%7B%7B%5CGamma+%5Cleft%28+%7B%5Csum%5Cnolimits_%7Bj+%3D+1%7D%5EK+%7B%5Cleft%28+%7Bn_m%5E%7B%5Cleft%28+j+%5Cright%29%7D+%2B+%7B%5Calpha+%5E%7B%5Cleft%28+j+%5Cright%29%7D%7D%7D+%5Cright%29%7D+%7D+%5Cright%29%7D%7D%5C%5C+%5CDelta+%5Cleft%28+%7B%7B%7B%5Cvec+n%7D_%7Bm%2C+-+i%7D%7D+%2B+%5Cvec+%5Calpha+%7D+%5Cright%29+%26%3D+%5Cfrac%7B%7B%5CGamma+%5Cleft%28+%7Bn_m%5E%7B%5Cleft%28+k+%5Cright%29%7D+%2B+%7B%5Calpha+%5E%7B%5Cleft%28+k+%5Cright%29%7D%7D+-+1%7D+%5Cright%29%5Cprod%5Cnolimits_%7Bj+%3D+1%2Cj+%5Cne+k%7D%5EK+%7B%5CGamma+%5Cleft%28+%7Bn_m%5E%7B%5Cleft%28+j+%5Cright%29%7D+%2B+%7B%5Calpha+%5E%7B%5Cleft%28+j+%5Cright%29%7D%7D%7D+%5Cright%29%7D+%7D%7D%7B%7B%5CGamma+%5Cleft%28+%7B%5Csum%5Cnolimits_%7Bj+%3D+1%2Cj+%5Cne+k%7D%5EK+%7B%5Cleft%28+%7Bn_m%5E%7B%5Cleft%28+j+%5Cright%29%7D+%2B+%7B%5Calpha+%5E%7B%5Cleft%28+j+%5Cright%29%7D%7D%7D+%5Cright%29%7D+%2B+n_m%5E%7B%5Cleft%28+k+%5Cright%29%7D+%2B+%7B%5Calpha+%5E%7B%5Cleft%28+k+%5Cright%29%7D%7D+-+1%7D+%5Cright%29%7D%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*}\\Delta \\left( {{{\\vec n}_k} + \\vec \\beta } \\right) &amp;= \\frac{{\\prod\\nolimits_{v = 1}^V {\\Gamma \\left( {n_k^{\\left( v \\right)} + {\\beta ^{\\left( v \\right)}}} \\right)} }}{{\\Gamma \\left( {\\sum\\nolimits_{v = 1}^V {\\left( {n_k^{\\left( v \\right)} + {\\beta ^{\\left( v \\right)}}} \\right)} } \\right)}}\\\\ \\Delta \\left( {{{\\vec n}_{k, - i}} + \\vec \\beta } \\right) &amp;= \\frac{{\\Gamma \\left( {n_k^{\\left( t \\right)} + {\\beta ^{\\left( t \\right)}} - 1} \\right)\\prod\\nolimits_{v = 1,v \\ne t}^V {\\Gamma \\left( {n_k^{\\left( v \\right)} + {\\beta ^{\\left( v \\right)}}} \\right)} }}{{\\Gamma \\left( {\\sum\\nolimits_{v = 1,v \\ne t}^V {\\left( {n_k^{\\left( v \\right)} + {\\beta ^{\\left( v \\right)}}} \\right)} + n_k^{\\left( t \\right)} + {\\beta ^{\\left( t \\right)}} - 1} \\right)}}\\\\ \\Delta \\left( {{{\\vec n}_m} + \\vec \\alpha } \\right) &amp;= \\frac{{\\prod\\nolimits_{j = 1}^K {\\Gamma \\left( {n_m^{\\left( j \\right)} + {\\alpha ^{\\left( j \\right)}}} \\right)} }}{{\\Gamma \\left( {\\sum\\nolimits_{j = 1}^K {\\left( {n_m^{\\left( j \\right)} + {\\alpha ^{\\left( j \\right)}}} \\right)} } \\right)}}\\\\ \\Delta \\left( {{{\\vec n}_{m, - i}} + \\vec \\alpha } \\right) &amp;= \\frac{{\\Gamma \\left( {n_m^{\\left( k \\right)} + {\\alpha ^{\\left( k \\right)}} - 1} \\right)\\prod\\nolimits_{j = 1,j \\ne k}^K {\\Gamma \\left( {n_m^{\\left( j \\right)} + {\\alpha ^{\\left( j \\right)}}} \\right)} }}{{\\Gamma \\left( {\\sum\\nolimits_{j = 1,j \\ne k}^K {\\left( {n_m^{\\left( j \\right)} + {\\alpha ^{\\left( j \\right)}}} \\right)} + n_m^{\\left( k \\right)} + {\\alpha ^{\\left( k \\right)}} - 1} \\right)}} \\end{align*}\" eeimg=\"1\"/> </p><p>我们再利用另外一种方法推导条件概率：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7Dp%5Cleft%28+%7B%7Bz_i%7D+%3D+k%7C%7B%7B%5Cvec+z%7D_%7B+-+i%7D%7D%2C%5Cvec+w%7D+%5Cright%29+%26%5Cpropto+p%5Cleft%28+%7B%7Bz_i%7D+%3D+k%2C%7Bw_i%7D+%3D+t%7C%7B%7B%5Cvec+z%7D_%7B+-+i%7D%7D%2C%7B%7B%5Cvec+w%7D_%7B+-+i%7D%7D%7D+%5Cright%29%5C%5C+%26%3D+%5Cint+%7Bp%5Cleft%28+%7B%7Bz_i%7D+%3D+k%2C%7Bw_i%7D+%3D+t%2C%7B%7B%5Cvec+%5Cvartheta+%7D_m%7D%2C%7B%7B%5Cvec+%5Cvarphi+%7D_k%7D%7C%7B%7B%5Cvec+z%7D_%7B+-+i%7D%7D%2C%7B%7B%5Cvec+w%7D_%7B+-+i%7D%7D%7D+%5Cright%29%7D+d%7B%7B%5Cvec+%5Cvartheta+%7D_m%7Dd%7B%7B%5Cvec+%5Cvarphi+%7D_k%7D%5C%5C+%26%3D+%5Cint+%7Bp%5Cleft%28+%7B%7Bz_i%7D+%3D+k%2C%7B%7B%5Cvec+%5Cvartheta+%7D_m%7D%7C%7B%7B%5Cvec+z%7D_%7B+-+i%7D%7D%2C%7B%7B%5Cvec+w%7D_%7B+-+i%7D%7D%7D+%5Cright%29+%5Ccdot+%7D+p%5Cleft%28+%7B%7Bw_i%7D+%3D+t%2C%7B%7B%5Cvec+%5Cvarphi+%7D_k%7D%7C%7B%7B%5Cvec+z%7D_%7B+-+i%7D%7D%2C%7B%7B%5Cvec+w%7D_%7B+-+i%7D%7D%7D+%5Cright%29d%7B%7B%5Cvec+%5Cvartheta+%7D_m%7Dd%7B%7B%5Cvec+%5Cvarphi+%7D_k%7D%5C%5C+%26%3D+%5Cint+%7Bp%5Cleft%28+%7B%7Bz_i%7D+%3D+k%7C%7B%7B%5Cvec+%5Cvartheta+%7D_m%7D%7D+%5Cright%29p%5Cleft%28+%7B%7B%7B%5Cvec+%5Cvartheta+%7D_m%7D%7C%7B%7B%5Cvec+z%7D_%7B+-+i%7D%7D%2C%7B%7B%5Cvec+w%7D_%7B+-+i%7D%7D%7D+%5Cright%29+%5Ccdot+%7D+p%5Cleft%28+%7B%7Bw_i%7D+%3D+t%7C%7B%7B%5Cvec+%5Cvarphi+%7D_k%7D%7D+%5Cright%29p%5Cleft%28+%7B%7B%7B%5Cvec+%5Cvarphi+%7D_k%7D%7C%7B%7B%5Cvec+z%7D_%7B+-+i%7D%7D%2C%7B%7B%5Cvec+w%7D_%7B+-+i%7D%7D%7D+%5Cright%29d%7B%7B%5Cvec+%5Cvartheta+%7D_m%7Dd%7B%7B%5Cvec+%5Cvarphi+%7D_k%7D%5C%5C+%26%3D+%5Cint+%7Bp%5Cleft%28+%7B%7Bz_i%7D+%3D+k%7C%7B%7B%5Cvec+%5Cvartheta+%7D_m%7D%7D+%5Cright%29Dir%5Cleft%28+%7B%7B%7B%5Cvec+%5Cvartheta+%7D_m%7D%7C%7B%7B%5Cvec+n%7D_%7Bm%2C+-+i%7D%7D+%2B+%5Cvec+%5Calpha+%7D+%5Cright%29+%5Ccdot+%7D+p%5Cleft%28+%7B%7Bw_i%7D+%3D+t%7C%7B%7B%5Cvec+%5Cvarphi+%7D_k%7D%7D+%5Cright%29Dir%5Cleft%28+%7B%7B%7B%5Cvec+%5Cvarphi+%7D_k%7D%7C%7B%7B%5Cvec+n%7D_%7Bk%2C+-+i%7D%7D+%2B+%5Cvec+%5Cbeta+%7D+%5Cright%29d%7B%7B%5Cvec+%5Cvartheta+%7D_m%7Dd%7B%7B%5Cvec+%5Cvarphi+%7D_k%7D%5C%5C+%26%3D+%5Cint+%7B%5Cvartheta+_m%5E%7B%5Cleft%28+k+%5Cright%29%7DDir%5Cleft%28+%7B%7B%7B%5Cvec+%5Cvartheta+%7D_m%7D%7C%7B%7B%5Cvec+n%7D_%7Bm%2C+-+i%7D%7D+%2B+%5Cvec+%5Calpha+%7D+%5Cright%29d%7B%7B%5Cvec+%5Cvartheta+%7D_m%7D%7D+%5Ccdot+%5Cint+%7B%5Cvarphi+_k%5E%7B%5Cleft%28+t+%5Cright%29%7DDir%5Cleft%28+%7B%7B%7B%5Cvec+%5Cvarphi+%7D_k%7D%7C%7B%7B%5Cvec+n%7D_%7Bk%2C+-+i%7D%7D+%2B+%5Cvec+%5Cbeta+%7D+%5Cright%29d%7B%7B%5Cvec+%5Cvarphi+%7D_k%7D%7D+%5C%5C+%26%3D+E%5Cleft%28+%7B%5Cvartheta+_m%5E%7B%5Cleft%28+k+%5Cright%29%7D%7D+%5Cright%29+%5Ccdot+E%5Cleft%28+%7B%5Cvarphi+_k%5E%7B%5Cleft%28+t+%5Cright%29%7D%7D+%5Cright%29%5C%5C+%26%3D+%5Cfrac%7B%7Bn_%7Bm%2C+-+i%7D%5E%7B%5Cleft%28+k+%5Cright%29%7D+%2B+%7B%5Calpha+%5E%7B%5Cleft%28+k+%5Cright%29%7D%7D%7D%7D%7B%7B%5Csum%5Cnolimits_%7Bj+%3D+1%7D%5EK+%7B%5Cleft%28+%7Bn_%7Bm%2C+-+i%7D%5E%7B%5Cleft%28+j+%5Cright%29%7D+%2B+%7B%5Calpha+%5E%7B%5Cleft%28+j+%5Cright%29%7D%7D%7D+%5Cright%29%7D+%7D%7D+%5Ccdot+%5Cfrac%7B%7Bn_%7Bk%2C+-+i%7D%5E%7B%5Cleft%28+t+%5Cright%29%7D+%2B+%7B%5Cbeta+%5E%7B%5Cleft%28+t+%5Cright%29%7D%7D%7D%7D%7B%7B%5Csum%5Cnolimits_%7Bv+%3D+1%7D%5EV+%7B%5Cleft%28+%7Bn_%7Bk%2C+-+i%7D%5E%7B%5Cleft%28+v+%5Cright%29%7D+%2B+%7B%5Cbeta+%5E%7B%5Cleft%28+v+%5Cright%29%7D%7D%7D+%5Cright%29%7D+%7D%7D%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*}p\\left( {{z_i} = k|{{\\vec z}_{ - i}},\\vec w} \\right) &amp;\\propto p\\left( {{z_i} = k,{w_i} = t|{{\\vec z}_{ - i}},{{\\vec w}_{ - i}}} \\right)\\\\ &amp;= \\int {p\\left( {{z_i} = k,{w_i} = t,{{\\vec \\vartheta }_m},{{\\vec \\varphi }_k}|{{\\vec z}_{ - i}},{{\\vec w}_{ - i}}} \\right)} d{{\\vec \\vartheta }_m}d{{\\vec \\varphi }_k}\\\\ &amp;= \\int {p\\left( {{z_i} = k,{{\\vec \\vartheta }_m}|{{\\vec z}_{ - i}},{{\\vec w}_{ - i}}} \\right) \\cdot } p\\left( {{w_i} = t,{{\\vec \\varphi }_k}|{{\\vec z}_{ - i}},{{\\vec w}_{ - i}}} \\right)d{{\\vec \\vartheta }_m}d{{\\vec \\varphi }_k}\\\\ &amp;= \\int {p\\left( {{z_i} = k|{{\\vec \\vartheta }_m}} \\right)p\\left( {{{\\vec \\vartheta }_m}|{{\\vec z}_{ - i}},{{\\vec w}_{ - i}}} \\right) \\cdot } p\\left( {{w_i} = t|{{\\vec \\varphi }_k}} \\right)p\\left( {{{\\vec \\varphi }_k}|{{\\vec z}_{ - i}},{{\\vec w}_{ - i}}} \\right)d{{\\vec \\vartheta }_m}d{{\\vec \\varphi }_k}\\\\ &amp;= \\int {p\\left( {{z_i} = k|{{\\vec \\vartheta }_m}} \\right)Dir\\left( {{{\\vec \\vartheta }_m}|{{\\vec n}_{m, - i}} + \\vec \\alpha } \\right) \\cdot } p\\left( {{w_i} = t|{{\\vec \\varphi }_k}} \\right)Dir\\left( {{{\\vec \\varphi }_k}|{{\\vec n}_{k, - i}} + \\vec \\beta } \\right)d{{\\vec \\vartheta }_m}d{{\\vec \\varphi }_k}\\\\ &amp;= \\int {\\vartheta _m^{\\left( k \\right)}Dir\\left( {{{\\vec \\vartheta }_m}|{{\\vec n}_{m, - i}} + \\vec \\alpha } \\right)d{{\\vec \\vartheta }_m}} \\cdot \\int {\\varphi _k^{\\left( t \\right)}Dir\\left( {{{\\vec \\varphi }_k}|{{\\vec n}_{k, - i}} + \\vec \\beta } \\right)d{{\\vec \\varphi }_k}} \\\\ &amp;= E\\left( {\\vartheta _m^{\\left( k \\right)}} \\right) \\cdot E\\left( {\\varphi _k^{\\left( t \\right)}} \\right)\\\\ &amp;= \\frac{{n_{m, - i}^{\\left( k \\right)} + {\\alpha ^{\\left( k \\right)}}}}{{\\sum\\nolimits_{j = 1}^K {\\left( {n_{m, - i}^{\\left( j \\right)} + {\\alpha ^{\\left( j \\right)}}} \\right)} }} \\cdot \\frac{{n_{k, - i}^{\\left( t \\right)} + {\\beta ^{\\left( t \\right)}}}}{{\\sum\\nolimits_{v = 1}^V {\\left( {n_{k, - i}^{\\left( v \\right)} + {\\beta ^{\\left( v \\right)}}} \\right)} }}\\end{align*}\" eeimg=\"1\"/> </p><p>已经推导出条件概率，可以用 Gibbs Sampling 公式进行采样了。</p><p><b>Algorithm 2.1 LDA Gibbs sampling</b></p><blockquote>1. Initialisation<br/> <img src=\"https://www.zhihu.com/equation?tex=n_m%5E%7B%5Cleft%28+k+%5Cright%29%7D+%3D+0%2C%7Bn_m%7D+%3D+0%2Cn_k%5E%7B%5Cleft%28+t+%5Cright%29%7D+%3D+0%2C%7Bn_k%7D+%3D+0\" alt=\"n_m^{\\left( k \\right)} = 0,{n_m} = 0,n_k^{\\left( t \\right)} = 0,{n_k} = 0\" eeimg=\"1\"/> <br/>     For all documents <img src=\"https://www.zhihu.com/equation?tex=m+%5Cin+%5Cleft%5B+%7B1%2CM%7D+%5Cright%5D\" alt=\"m \\in \\left[ {1,M} \\right]\" eeimg=\"1\"/> do:<br/>     For all words <img src=\"https://www.zhihu.com/equation?tex=+n+%5Cin+%5Cleft%5B+%7B1%2C%7BN_m%7D%7D+%5Cright%5D+\" alt=\" n \\in \\left[ {1,{N_m}} \\right] \" eeimg=\"1\"/> in document m do:<br/>            1）Sample <img src=\"https://www.zhihu.com/equation?tex=+%7Bz_%7Bm%2Cn%7D%7D+%3D+k+%5Csim+Mult%5Cleft%28+%7B%5Cfrac%7B1%7D%7BK%7D%7D+%5Cright%29%2C%7Bw_%7Bm%2Cn%7D%7D+%3D+t\" alt=\" {z_{m,n}} = k \\sim Mult\\left( {\\frac{1}{K}} \\right),{w_{m,n}} = t\" eeimg=\"1\"/> <br/>            2） <img src=\"https://www.zhihu.com/equation?tex=+%5Cbegin%7Balign%2A%7Dn_m%5E%7B%5Cleft%28+k+%5Cright%29%7D%7B%5Crm%7B+%3D+%7D%7Dn_m%5E%7B%5Cleft%28+k+%5Cright%29%7D%7B%5Crm%7B+%2B+%7D%7D1%2C%7Bn_m%7D+%3D+%7Bn_m%7D+%2B+1%5C%5C+n_k%5E%7B%5Cleft%28+t+%5Cright%29%7D+%3D+n_k%5E%7B%5Cleft%28+t+%5Cright%29%7D+%2B+1%2C%7Bn_k%7D+%3D+%7Bn_k%7D+%2B+1%5Cend%7Balign%2A%7D\" alt=\" \\begin{align*}n_m^{\\left( k \\right)}{\\rm{ = }}n_m^{\\left( k \\right)}{\\rm{ + }}1,{n_m} = {n_m} + 1\\\\ n_k^{\\left( t \\right)} = n_k^{\\left( t \\right)} + 1,{n_k} = {n_k} + 1\\end{align*}\" eeimg=\"1\"/> <br/>2. Gibbs Sampling<br/>    While not finished do:<br/>    For all documents <img src=\"https://www.zhihu.com/equation?tex=m+%5Cin+%5Cleft%5B+%7B1%2CM%7D+%5Cright%5D\" alt=\"m \\in \\left[ {1,M} \\right]\" eeimg=\"1\"/> do:<br/>    For all words <img src=\"https://www.zhihu.com/equation?tex=n+%5Cin+%5Cleft%5B+%7B1%2C%7BN_m%7D%7D+%5Cright%5D\" alt=\"n \\in \\left[ {1,{N_m}} \\right]\" eeimg=\"1\"/> in document m do:<br/>            1） <img src=\"https://www.zhihu.com/equation?tex=%7Bz_%7Bm%2Cn%7D%7D+%3D+k%2C%7Bw_%7Bm%2Cn%7D%7D+%3D+t+%5CRightarrow+%5Cbegin%7Barray%7D%7B%2A%7B20%7D%7Bc%7D%7D+%7Bn_m%5E%7B%5Cleft%28+k+%5Cright%29%7D%7B%5Crm%7B+%3D+%7D%7Dn_m%5E%7B%5Cleft%28+k+%5Cright%29%7D+-+1%2C%7Bn_m%7D+%3D+%7Bn_m%7D+-+1%7D%5C%5C+%7Bn_k%5E%7B%5Cleft%28+t+%5Cright%29%7D+%3D+n_k%5E%7B%5Cleft%28+t+%5Cright%29%7D+-+1%2C%7Bn_k%7D+%3D+%7Bn_k%7D+-+1%7D+%5Cend%7Barray%7D\" alt=\"{z_{m,n}} = k,{w_{m,n}} = t \\Rightarrow \\begin{array}{*{20}{c}} {n_m^{\\left( k \\right)}{\\rm{ = }}n_m^{\\left( k \\right)} - 1,{n_m} = {n_m} - 1}\\\\ {n_k^{\\left( t \\right)} = n_k^{\\left( t \\right)} - 1,{n_k} = {n_k} - 1} \\end{array}\" eeimg=\"1\"/> <br/>            2）Mult. sampling acc. to Eq. 85 sample <img src=\"https://www.zhihu.com/equation?tex=+%7Bz_%7Bm%2Cn%7D%7D+%3D+%5Ctilde+k+%5Csim+p%5Cleft%28+%7B%7Bz_i%7D%7C%7B%7B%5Cvec+z%7D_%7B+-+i%7D%7D%2C%5Cvec+w%7D+%5Cright%29\" alt=\" {z_{m,n}} = \\tilde k \\sim p\\left( {{z_i}|{{\\vec z}_{ - i}},\\vec w} \\right)\" eeimg=\"1\"/> <br/> <img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Barray%7D%7Bl%7D+n_m%5E%7B%5Cleft%28+%7B%5Ctilde+k%7D+%5Cright%29%7D%7B%5Crm%7B+%3D+%7D%7Dn_m%5E%7B%5Cleft%28+%7B%5Ctilde+k%7D+%5Cright%29%7D%7B%5Crm%7B+%2B+%7D%7D1%2C%7Bn_m%7D+%3D+%7Bn_m%7D+%2B+1%5C%5C+n_%7B%5Ctilde+k%7D%5E%7B%5Cleft%28+t+%5Cright%29%7D+%3D+n_%7B%5Ctilde+k%7D%5E%7B%5Cleft%28+t+%5Cright%29%7D+%2B+1%2C%7Bn_%7B%5Ctilde+k%7D%7D+%3D+%7Bn_%7B%5Ctilde+k%7D%7D+%2B+1+%5Cend%7Barray%7D\" alt=\"\\begin{array}{l} n_m^{\\left( {\\tilde k} \\right)}{\\rm{ = }}n_m^{\\left( {\\tilde k} \\right)}{\\rm{ + }}1,{n_m} = {n_m} + 1\\\\ n_{\\tilde k}^{\\left( t \\right)} = n_{\\tilde k}^{\\left( t \\right)} + 1,{n_{\\tilde k}} = {n_{\\tilde k}} + 1 \\end{array}\" eeimg=\"1\"/> <br/>3. Parameter estimation<br/>        according to Eq. 76 and Eq. 80 estimate <img src=\"https://www.zhihu.com/equation?tex=+%7B%5Cvec+%5Cvartheta+_m%7D%2C%7B%5Cvec+%5Cvarphi+_k%7D\" alt=\" {\\vec \\vartheta _m},{\\vec \\varphi _k}\" eeimg=\"1\"/> .</blockquote><p>有了 LDA 模型，对于新来的文档 <img src=\"https://www.zhihu.com/equation?tex=%5C%5Bdo%7Bc_%7Bnew%7D%7D%5C%5D\" alt=\"\\[do{c_{new}}\\]\" eeimg=\"1\"/> ，如何计算其主题分布呢？基本上 inference 的过程和 training 的过程完全类似，对于新的文档，Gibbs Sampling 公式中 <img src=\"https://www.zhihu.com/equation?tex=%5C%5B%5Cvarphi+_k%5E%7B%28t%29%7D%5C%5D\" alt=\"\\[\\varphi _k^{(t)}\\]\" eeimg=\"1\"/> 固定不变，由训练语料得到的模型提供，所以采样过程只要估计新来文档的 <img src=\"https://www.zhihu.com/equation?tex=%7B%5Cvec+%5Cvartheta+_m%7D+\" alt=\"{\\vec \\vartheta _m} \" eeimg=\"1\"/> 就好。</p><p>根据 Algorithm 2.1，实现一个简单的LDA，直接把算法中的数学公式翻译成代码即可。</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"kn\">import</span> <span class=\"nn\">sys</span>\n<span class=\"kn\">import</span> <span class=\"nn\">random</span>\n<span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n\n<span class=\"n\">TOPIC_NUM</span> <span class=\"o\">=</span> <span class=\"mi\">2</span>\n<span class=\"n\">ITER_NUM</span> <span class=\"o\">=</span> <span class=\"mi\">10000</span>\n<span class=\"n\">ALPHA</span> <span class=\"o\">=</span> <span class=\"mf\">0.01</span>\n<span class=\"n\">BETA</span> <span class=\"o\">=</span> <span class=\"mf\">0.01</span>\n<span class=\"n\">d_w</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>\n<span class=\"n\">id2word</span><span class=\"o\">=</span><span class=\"p\">{}</span>\n<span class=\"n\">word2id</span><span class=\"o\">=</span><span class=\"p\">{}</span>\n<span class=\"n\">n_m_k</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([],</span><span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">int32</span><span class=\"p\">)</span> <span class=\"c1\"># document-topic</span>\n<span class=\"n\">n_m</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([],</span><span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">int32</span><span class=\"p\">)</span>   <span class=\"c1\"># document</span>\n<span class=\"n\">n_k_t</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([],</span><span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">int32</span><span class=\"p\">)</span> <span class=\"c1\"># topic-word</span>\n<span class=\"n\">n_k</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([],</span><span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">int32</span><span class=\"p\">)</span>   <span class=\"c1\"># topic</span>\n<span class=\"n\">d_w_t</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([],</span><span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">int32</span><span class=\"p\">)</span> <span class=\"c1\"># doc-word-topic</span>\n<span class=\"n\">p_k</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"n\">TOPIC_NUM</span><span class=\"p\">)</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">input</span><span class=\"p\">(</span><span class=\"n\">docs</span><span class=\"p\">):</span>\n    <span class=\"n\">words</span> <span class=\"o\">=</span> <span class=\"nb\">set</span><span class=\"p\">()</span>\n    <span class=\"k\">for</span> <span class=\"n\">line</span> <span class=\"ow\">in</span> <span class=\"n\">docs</span><span class=\"p\">:</span>\n        <span class=\"n\">items</span> <span class=\"o\">=</span> <span class=\"n\">line</span><span class=\"o\">.</span><span class=\"n\">strip</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">(</span><span class=\"s1\">&#39; &#39;</span><span class=\"p\">)</span>\n        <span class=\"n\">words</span><span class=\"o\">.</span><span class=\"n\">update</span><span class=\"p\">(</span><span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"n\">items</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">:]))</span>\n    <span class=\"n\">id2word</span><span class=\"o\">.</span><span class=\"n\">update</span><span class=\"p\">({</span><span class=\"n\">k</span><span class=\"p\">:</span><span class=\"n\">v</span> <span class=\"k\">for</span> <span class=\"n\">k</span><span class=\"p\">,</span><span class=\"n\">v</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">words</span><span class=\"p\">)})</span>\n    <span class=\"n\">word2id</span><span class=\"o\">.</span><span class=\"n\">update</span><span class=\"p\">({</span><span class=\"n\">v</span><span class=\"p\">:</span><span class=\"n\">k</span> <span class=\"k\">for</span> <span class=\"n\">k</span><span class=\"p\">,</span><span class=\"n\">v</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">words</span><span class=\"p\">)})</span>\n    <span class=\"k\">for</span> <span class=\"n\">line</span> <span class=\"ow\">in</span> <span class=\"n\">docs</span><span class=\"p\">:</span>\n        <span class=\"n\">items</span> <span class=\"o\">=</span> <span class=\"n\">line</span><span class=\"o\">.</span><span class=\"n\">strip</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">(</span><span class=\"s1\">&#39; &#39;</span><span class=\"p\">)</span>\n        <span class=\"n\">doc</span> <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">items</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">])</span>\n        <span class=\"n\">d_w</span><span class=\"p\">[</span><span class=\"n\">doc</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">word2id</span><span class=\"p\">[</span><span class=\"n\">word</span><span class=\"p\">]</span> <span class=\"k\">for</span> <span class=\"n\">word</span> <span class=\"ow\">in</span> <span class=\"n\">items</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">:]]</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">init</span><span class=\"p\">():</span>\n    <span class=\"k\">global</span> <span class=\"n\">n_m_k</span><span class=\"p\">,</span><span class=\"n\">n_m</span><span class=\"p\">,</span><span class=\"n\">n_k_t</span><span class=\"p\">,</span><span class=\"n\">n_k</span><span class=\"p\">,</span><span class=\"n\">d_w_t</span>\n    <span class=\"n\">n_m_k</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">d_w</span><span class=\"p\">),</span><span class=\"n\">TOPIC_NUM</span><span class=\"p\">))</span>\n    <span class=\"n\">n_m</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">d_w</span><span class=\"p\">))</span>\n    <span class=\"n\">n_k_t</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">TOPIC_NUM</span><span class=\"p\">,</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">word2id</span><span class=\"p\">)))</span>\n    <span class=\"n\">n_k</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"n\">TOPIC_NUM</span><span class=\"p\">)</span>\n    <span class=\"n\">d_w_t</span> <span class=\"o\">=</span> <span class=\"o\">-</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">ones</span><span class=\"p\">((</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">d_w</span><span class=\"p\">),</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">word2id</span><span class=\"p\">)),</span><span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">int32</span><span class=\"p\">)</span>\n    \n    <span class=\"k\">for</span> <span class=\"n\">d</span><span class=\"p\">,</span><span class=\"n\">w_L</span> <span class=\"ow\">in</span> <span class=\"n\">d_w</span><span class=\"o\">.</span><span class=\"n\">items</span><span class=\"p\">():</span>\n        <span class=\"k\">for</span> <span class=\"n\">w</span> <span class=\"ow\">in</span> <span class=\"n\">w_L</span><span class=\"p\">:</span>\n            <span class=\"n\">r</span> <span class=\"o\">=</span> <span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"p\">()</span>\n            <span class=\"k\">if</span> <span class=\"n\">r</span> <span class=\"o\">&lt;</span> <span class=\"mf\">0.5</span><span class=\"p\">:</span>\n                <span class=\"n\">t</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n            <span class=\"k\">else</span><span class=\"p\">:</span>\n                <span class=\"n\">t</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>\n            <span class=\"n\">n_m_k</span><span class=\"p\">[</span><span class=\"n\">d</span><span class=\"p\">,</span><span class=\"n\">t</span><span class=\"p\">]</span> <span class=\"o\">+=</span> <span class=\"mi\">1</span>\n            <span class=\"n\">n_m</span><span class=\"p\">[</span><span class=\"n\">d</span><span class=\"p\">]</span> <span class=\"o\">+=</span> <span class=\"mi\">1</span>\n            <span class=\"n\">n_k_t</span><span class=\"p\">[</span><span class=\"n\">t</span><span class=\"p\">,</span><span class=\"n\">w</span><span class=\"p\">]</span> <span class=\"o\">+=</span> <span class=\"mi\">1</span>\n            <span class=\"n\">n_k</span><span class=\"p\">[</span><span class=\"n\">t</span><span class=\"p\">]</span> <span class=\"o\">+=</span> <span class=\"mi\">1</span>\n            <span class=\"n\">d_w_t</span><span class=\"p\">[</span><span class=\"n\">d</span><span class=\"p\">,</span><span class=\"n\">w</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">t</span>\n<span class=\"k\">def</span> <span class=\"nf\">sampling</span><span class=\"p\">():</span>\n    <span class=\"k\">global</span> <span class=\"n\">p_k</span>\n    <span class=\"k\">for</span> <span class=\"n\">step</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">ITER_NUM</span><span class=\"p\">):</span>\n        <span class=\"k\">if</span> <span class=\"n\">step</span> <span class=\"o\">%</span> <span class=\"mi\">100</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n            <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;step:</span><span class=\"si\">%s</span><span class=\"s1\">&#39;</span> <span class=\"o\">%</span> <span class=\"n\">step</span><span class=\"p\">)</span>\n        <span class=\"k\">for</span> <span class=\"n\">d</span><span class=\"p\">,</span><span class=\"n\">w_L</span> <span class=\"ow\">in</span> <span class=\"n\">d_w</span><span class=\"o\">.</span><span class=\"n\">items</span><span class=\"p\">():</span>\n            <span class=\"k\">for</span> <span class=\"n\">w</span> <span class=\"ow\">in</span> <span class=\"n\">w_L</span><span class=\"p\">:</span>\n                <span class=\"n\">t</span> <span class=\"o\">=</span> <span class=\"n\">d_w_t</span><span class=\"p\">[</span><span class=\"n\">d</span><span class=\"p\">,</span><span class=\"n\">w</span><span class=\"p\">]</span>\n                <span class=\"n\">n_m_k</span><span class=\"p\">[</span><span class=\"n\">d</span><span class=\"p\">,</span><span class=\"n\">t</span><span class=\"p\">]</span> <span class=\"o\">-=</span> <span class=\"mi\">1</span>\n                <span class=\"n\">n_m</span><span class=\"p\">[</span><span class=\"n\">d</span><span class=\"p\">]</span> <span class=\"o\">-=</span> <span class=\"mi\">1</span>\n                <span class=\"n\">n_k_t</span><span class=\"p\">[</span><span class=\"n\">t</span><span class=\"p\">,</span><span class=\"n\">w</span><span class=\"p\">]</span> <span class=\"o\">-=</span> <span class=\"mi\">1</span>\n                <span class=\"n\">n_k</span><span class=\"p\">[</span><span class=\"n\">t</span><span class=\"p\">]</span> <span class=\"o\">-=</span> <span class=\"mi\">1</span>\n                <span class=\"k\">for</span> <span class=\"n\">t</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">TOPIC_NUM</span><span class=\"p\">):</span>\n                    <span class=\"n\">p_k</span><span class=\"p\">[</span><span class=\"n\">t</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">n_k_t</span><span class=\"p\">[</span><span class=\"n\">t</span><span class=\"p\">,</span><span class=\"n\">w</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"n\">BETA</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"n\">n_m_k</span><span class=\"p\">[</span><span class=\"n\">d</span><span class=\"p\">,</span><span class=\"n\">t</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"n\">ALPHA</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"mf\">1.0</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"n\">n_k</span><span class=\"p\">[</span><span class=\"n\">t</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"n\">BETA</span><span class=\"o\">*</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">word2id</span><span class=\"p\">))</span>\n                <span class=\"n\">p_k</span> <span class=\"o\">/=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">p_k</span><span class=\"p\">)</span>\n                <span class=\"k\">for</span> <span class=\"n\">k</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">TOPIC_NUM</span><span class=\"p\">):</span>\n                    <span class=\"n\">p_k</span><span class=\"p\">[</span><span class=\"n\">k</span><span class=\"p\">]</span> <span class=\"o\">+=</span> <span class=\"n\">p_k</span><span class=\"p\">[</span><span class=\"n\">k</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n                <span class=\"n\">r</span> <span class=\"o\">=</span> <span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"p\">()</span>\n                <span class=\"k\">for</span> <span class=\"n\">k</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">TOPIC_NUM</span><span class=\"p\">):</span>\n                    <span class=\"k\">if</span><span class=\"p\">(</span><span class=\"n\">r</span><span class=\"o\">&lt;=</span><span class=\"n\">p_k</span><span class=\"p\">[</span><span class=\"n\">k</span><span class=\"p\">]):</span>\n                        <span class=\"n\">t</span> <span class=\"o\">=</span> <span class=\"n\">k</span>\n                        <span class=\"k\">break</span>\n                <span class=\"n\">d_w_t</span><span class=\"p\">[</span><span class=\"n\">d</span><span class=\"p\">,</span><span class=\"n\">w</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">t</span>\n                <span class=\"n\">n_m_k</span><span class=\"p\">[</span><span class=\"n\">d</span><span class=\"p\">,</span><span class=\"n\">t</span><span class=\"p\">]</span> <span class=\"o\">+=</span> <span class=\"mi\">1</span>\n                <span class=\"n\">n_m</span><span class=\"p\">[</span><span class=\"n\">d</span><span class=\"p\">]</span> <span class=\"o\">+=</span> <span class=\"mi\">1</span>                                                                 \n                <span class=\"n\">n_k_t</span><span class=\"p\">[</span><span class=\"n\">t</span><span class=\"p\">,</span><span class=\"n\">w</span><span class=\"p\">]</span> <span class=\"o\">+=</span> <span class=\"mi\">1</span>\n                <span class=\"n\">n_k</span><span class=\"p\">[</span><span class=\"n\">t</span><span class=\"p\">]</span> <span class=\"o\">+=</span> <span class=\"mi\">1</span>\n<span class=\"k\">def</span> <span class=\"nf\">output</span><span class=\"p\">():</span>\n    <span class=\"k\">for</span> <span class=\"n\">d</span><span class=\"p\">,</span> <span class=\"n\">w_L</span> <span class=\"ow\">in</span> <span class=\"n\">d_w</span><span class=\"o\">.</span><span class=\"n\">items</span><span class=\"p\">():</span>\n        <span class=\"k\">for</span> <span class=\"n\">w</span> <span class=\"ow\">in</span> <span class=\"n\">w_L</span><span class=\"p\">:</span>\n            <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;doc:</span><span class=\"si\">%s</span><span class=\"s1\">,word:</span><span class=\"si\">%s</span><span class=\"s1\">,topic:</span><span class=\"si\">%s</span><span class=\"s1\">&#39;</span> <span class=\"o\">%</span> <span class=\"p\">(</span><span class=\"n\">d</span><span class=\"p\">,</span><span class=\"n\">id2word</span><span class=\"p\">[</span><span class=\"n\">w</span><span class=\"p\">],</span><span class=\"n\">d_w_t</span><span class=\"p\">[</span><span class=\"n\">d</span><span class=\"p\">,</span><span class=\"n\">w</span><span class=\"p\">]))</span>\n                \n<span class=\"n\">docs</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"s2\">&#34;0 枪 游戏 计算机 dota 电脑&#34;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&#34;1 娃娃 美丽 面膜 高跟鞋 裙子&#34;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&#34;2 购物 娃娃 裙子 SPA 指甲&#34;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&#34;3 枪 帅 电脑 坦克 飞机&#34;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&#34;4 游戏 坦克 飞机 数学 美丽&#34;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&#34;5 计算机 帅 枪 dota&#34;</span><span class=\"p\">,</span>\n        <span class=\"s2\">&#34;6 美丽 购物 面膜 SPA 飘柔&#34;</span><span class=\"p\">]</span>\n<span class=\"nb\">input</span><span class=\"p\">(</span><span class=\"n\">docs</span><span class=\"p\">)</span>\n<span class=\"n\">init</span><span class=\"p\">()</span>\n<span class=\"n\">sampling</span><span class=\"p\">()</span>\n<span class=\"n\">output</span><span class=\"p\">()</span></code></pre></div><p>结果：</p><blockquote>doc:0,word:枪,topic:1<br/>doc:0,word:游戏,topic:1<br/>doc:0,word:计算机,topic:1<br/>doc:0,word:dota,topic:1<br/>doc:0,word:电脑,topic:1<br/>doc:1,word:娃娃,topic:0<br/>doc:1,word:美丽,topic:0<br/>doc:1,word:面膜,topic:0<br/>doc:1,word:高跟鞋,topic:0<br/>doc:1,word:裙子,topic:0<br/>doc:2,word:购物,topic:0<br/>doc:2,word:娃娃,topic:0<br/>doc:2,word:裙子,topic:0<br/>doc:2,word:SPA,topic:0<br/>doc:2,word:指甲,topic:0<br/>doc:3,word:枪,topic:1<br/>doc:3,word:帅,topic:1<br/>doc:3,word:电脑,topic:1<br/>doc:3,word:坦克,topic:1<br/>doc:3,word:飞机,topic:1<br/>doc:4,word:游戏,topic:1<br/>doc:4,word:坦克,topic:1<br/>doc:4,word:飞机,topic:1<br/>doc:4,word:数学,topic:1<br/>doc:4,word:美丽,topic:1<br/>doc:5,word:计算机,topic:1<br/>doc:5,word:帅,topic:1<br/>doc:5,word:枪,topic:1<br/>doc:5,word:dota,topic:1<br/>doc:6,word:美丽,topic:0<br/>doc:6,word:购物,topic:0<br/>doc:6,word:面膜,topic:0<br/>doc:6,word:SPA,topic:0<br/>doc:6,word:飘柔,topic:0</blockquote><p><a href=\"https://zhuanlan.zhihu.com/p/42714332\" class=\"internal\">上篇</a></p>", 
            "topic": [
                {
                    "tag": "LDA", 
                    "tagLink": "https://api.zhihu.com/topics/19565464"
                }, 
                {
                    "tag": "主题模型", 
                    "tagLink": "https://api.zhihu.com/topics/19565468"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/42714332", 
            "userName": "徽州风韵", 
            "userLink": "https://www.zhihu.com/people/53bd6f8b93873b03c5edb714aea2a456", 
            "upvote": 3, 
            "title": "Latent Dirichlet Allocation-上篇", 
            "content": "<h2><b>一、简介</b></h2><p>隐含狄利克雷分布（Latent Dirichlet Allocation，简称LDA）是由 David M. Blei、Andrew Y. Ng、Michael I. Jordan 在2003年提出的，是一种词袋模型，它认为文档是一组词构成的集合，词与词之间是无序的。一篇文档可以包含多个主题，文档中的每个词都是由某个主题生成的，LDA给出文档属于每个主题的概率分布，同时给出每个主题上词的概率分布。LDA是一种无监督学习，在文本主题识别、文本分类、文本相似度计算和文章相似推荐等方面都有应用。</p><h2><b>二、基础知识</b></h2><h2><b>1.1 贝叶公式</b></h2><p>贝叶斯学派的最基本的观点是：任一个未知量 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 都可看作一个随机变量，应该用一个概率分布去描述对 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 的未知状况，这个概率分布是在抽样前就有关于 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 的先验信息的概率陈述，这个概率分布被称为先验分布。</p><p>从贝叶斯观点看，样本 <img src=\"https://www.zhihu.com/equation?tex=X+%3D+%5Cleft%28+%7B%7Bx_1%7D%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2C%7Bx_n%7D%7D+%5Cright%29\" alt=\"X = \\left( {{x_1}, \\cdot \\cdot \\cdot ,{x_n}} \\right)\" eeimg=\"1\"/> 的产生要分两步进行，首先设想从先验分布 <img src=\"https://www.zhihu.com/equation?tex=p%5Cleft%28+%5Ctheta+%5Cright%29\" alt=\"p\\left( \\theta \\right)\" eeimg=\"1\"/> 产生一个样本 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta+%27\" alt=\"\\theta &#39;\" eeimg=\"1\"/> ，这一步是“老天爷”做的，人们是看不到的，故用“设想”二字。第二步是从总体分布 <img src=\"https://www.zhihu.com/equation?tex=p%5Cleft%28+%7BX%7C%5Ctheta+%27%7D+%5Cright%29\" alt=\"p\\left( {X|\\theta &#39;} \\right)\" eeimg=\"1\"/> 产生一个样本 <img src=\"https://www.zhihu.com/equation?tex=X+%3D+%5Cleft%28+%7B%7Bx_1%7D%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2C%7Bx_n%7D%7D+%5Cright%29\" alt=\"X = \\left( {{x_1}, \\cdot \\cdot \\cdot ,{x_n}} \\right)\" eeimg=\"1\"/> ，这个样本是具体的，人们能看得到的，此样本 <img src=\"https://www.zhihu.com/equation?tex=X\" alt=\"X\" eeimg=\"1\"/> 发生的概率是与如下联合密度函数成正比。</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+p%5Cleft%28+%7BX%7C%5Ctheta+%27%7D+%5Cright%29+%3D+%5Cprod%5Climits_%7Bi+%3D+1%7D%5En+%7Bp%5Cleft%28+%7B%7Bx_i%7D%7C%5Ctheta+%27%7D+%5Cright%29%7D+%5Ctag%7B1%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} p\\left( {X|\\theta &#39;} \\right) = \\prod\\limits_{i = 1}^n {p\\left( {{x_i}|\\theta &#39;} \\right)} \\tag{1} \\end{align*}\" eeimg=\"1\"/> </p><p>这个联合密度函数是综合了总体信息和样本信息，常称为似然函数，记为 <img src=\"https://www.zhihu.com/equation?tex=L%5Cleft%28+%7B%5Ctheta+%27%7D+%5Cright%29\" alt=\"L\\left( {\\theta &#39;} \\right)\" eeimg=\"1\"/> 。</p><p>由于 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta+%27\" alt=\"\\theta &#39;\" eeimg=\"1\"/> 是设想出来的，它仍然是未知的，它是按先验分布 <img src=\"https://www.zhihu.com/equation?tex=p%5Cleft%28+%5Ctheta+%5Cright%29\" alt=\"p\\left( \\theta \\right)\" eeimg=\"1\"/> 产生的，要把先验信息进行综合，不能只考虑 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta+%27\" alt=\"\\theta &#39;\" eeimg=\"1\"/> ，而应对 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 的一切可能加以考虑，故要用 <img src=\"https://www.zhihu.com/equation?tex=p%5Cleft%28+%5Ctheta+%5Cright%29\" alt=\"p\\left( \\theta \\right)\" eeimg=\"1\"/> 参与进一步综合，所以样本 <img src=\"https://www.zhihu.com/equation?tex=X\" alt=\"X\" eeimg=\"1\"/> 和参数 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 的联合分布（三种可用的信息都综合进去了）：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+p%5Cleft%28+%7BX%2C%5Ctheta+%7D+%5Cright%29+%3D+p%5Cleft%28+%7BX%7C%5Ctheta+%7D+%5Cright%29p%5Cleft%28+%5Ctheta+%5Cright%29+%5Ctag%7B2%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} p\\left( {X,\\theta } \\right) = p\\left( {X|\\theta } \\right)p\\left( \\theta \\right) \\tag{2} \\end{align*}\" eeimg=\"1\"/> </p><p>我们的任务是要对未知数 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta+\" alt=\"\\theta \" eeimg=\"1\"/> 作出统计推断，在没有样本信息时，人们只能根据先验分布对 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 作出推断。在有样本观察值 <img src=\"https://www.zhihu.com/equation?tex=X+%3D+%5Cleft%28+%7B%7Bx_1%7D%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2C%7Bx_n%7D%7D+%5Cright%29\" alt=\"X = \\left( {{x_1}, \\cdot \\cdot \\cdot ,{x_n}} \\right)\" eeimg=\"1\"/> 之后，我们应该依据 <img src=\"https://www.zhihu.com/equation?tex=p%5Cleft%28+%7BX%2C%5Ctheta+%7D+%5Cright%29\" alt=\"p\\left( {X,\\theta } \\right)\" eeimg=\"1\"/> 对 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 作出推断，为此我们把 <img src=\"https://www.zhihu.com/equation?tex=p%5Cleft%28+%7BX%2C%5Ctheta+%7D+%5Cright%29\" alt=\"p\\left( {X,\\theta } \\right)\" eeimg=\"1\"/> 作如下分解：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+p%5Cleft%28+%7BX%2C%5Ctheta+%7D+%5Cright%29%7B%5Crm%7B+%3D+%7D%7Dp%5Cleft%28+%7B%5Ctheta+%7CX%7D+%5Cright%29p%5Cleft%28+X+%5Cright%29+%5Ctag%7B3%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} p\\left( {X,\\theta } \\right){\\rm{ = }}p\\left( {\\theta |X} \\right)p\\left( X \\right) \\tag{3} \\end{align*}\" eeimg=\"1\"/> </p><p>其中 <img src=\"https://www.zhihu.com/equation?tex=p%5Cleft%28+X+%5Cright%29\" alt=\"p\\left( X \\right)\" eeimg=\"1\"/> 是 <img src=\"https://www.zhihu.com/equation?tex=X\" alt=\"X\" eeimg=\"1\"/> 的边缘密度函数。</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+p%5Cleft%28+X+%5Cright%29+%3D+%5Cint_%5CTheta+%7Bp%5Cleft%28+%7BX%2C%5Ctheta+%7D+%5Cright%29%7D+d%5Ctheta+%3D+%5Cint_%5CTheta+%7Bp%5Cleft%28+%7BX%7C%5Ctheta+%7D+%5Cright%29p%5Cleft%28+%5Ctheta+%5Cright%29%7D+d%5Ctheta+%5Ctag%7B4%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} p\\left( X \\right) = \\int_\\Theta {p\\left( {X,\\theta } \\right)} d\\theta = \\int_\\Theta {p\\left( {X|\\theta } \\right)p\\left( \\theta \\right)} d\\theta \\tag{4} \\end{align*}\" eeimg=\"1\"/> </p><p>它与 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 无关 <img src=\"https://www.zhihu.com/equation?tex=p%5Cleft%28+X+%5Cright%29\" alt=\"p\\left( X \\right)\" eeimg=\"1\"/> 中不含 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 的任何信息。因此能用来对 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 作出推断的仅是条件分布 <img src=\"https://www.zhihu.com/equation?tex=p%5Cleft%28+%7B%5Ctheta+%7CX%7D+%5Cright%29\" alt=\"p\\left( {\\theta |X} \\right)\" eeimg=\"1\"/> ：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+p%5Cleft%28+%7B%5Ctheta+%7CX%7D+%5Cright%29+%26%3D+%5Cfrac%7B%7Bp%5Cleft%28+%7BX%2C%5Ctheta+%7D+%5Cright%29%7D%7D%7B%7Bp%5Cleft%28+X+%5Cright%29%7D%7D+%3D+%5Cfrac%7B%7Bp%5Cleft%28+%7BX%7C%5Ctheta+%7D+%5Cright%29p%5Cleft%28+%5Ctheta+%5Cright%29%7D%7D%7B%7B%5Cint_%5CTheta+%7Bp%5Cleft%28+%7BX%7C%5Ctheta+%7D+%5Cright%29p%5Cleft%28+%5Ctheta+%5Cright%29%7D+d%5Ctheta+%7D%7D+%5C%5C+posterior+%26%3D+%5Cfrac%7B%7B%7B%5Crm%7Blikelihood%7D%7D+%5Ccdot+%7B%5Crm%7Bprior%7D%7D%7D%7D%7B%7B%7B%5Crm%7Bevidence%7D%7D%7D%7D+%5Ctag%7B5%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} p\\left( {\\theta |X} \\right) &amp;= \\frac{{p\\left( {X,\\theta } \\right)}}{{p\\left( X \\right)}} = \\frac{{p\\left( {X|\\theta } \\right)p\\left( \\theta \\right)}}{{\\int_\\Theta {p\\left( {X|\\theta } \\right)p\\left( \\theta \\right)} d\\theta }} \\\\ posterior &amp;= \\frac{{{\\rm{likelihood}} \\cdot {\\rm{prior}}}}{{{\\rm{evidence}}}} \\tag{5} \\end{align*}\" eeimg=\"1\"/> </p><p>这就是贝叶斯公式的密度函数形式，在样本 <img src=\"https://www.zhihu.com/equation?tex=X\" alt=\"X\" eeimg=\"1\"/> 给定下， <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 的条件分布被称为 <img src=\"https://www.zhihu.com/equation?tex=+%5Ctheta\" alt=\" \\theta\" eeimg=\"1\"/> 的后验分布。它是集中了总体、样本和先验等三种信息中有关 <img src=\"https://www.zhihu.com/equation?tex=+%5Ctheta\" alt=\" \\theta\" eeimg=\"1\"/> 的一切信息，而又是排除一切与 <img src=\"https://www.zhihu.com/equation?tex=+%5Ctheta+\" alt=\" \\theta \" eeimg=\"1\"/> 无关的信息之后得到的结果，故基于后验分布 <img src=\"https://www.zhihu.com/equation?tex=+p%5Cleft%28+%7B%5Ctheta+%7CX%7D+%5Cright%29+\" alt=\" p\\left( {\\theta |X} \\right) \" eeimg=\"1\"/> 对 <img src=\"https://www.zhihu.com/equation?tex=+%5Ctheta+\" alt=\" \\theta \" eeimg=\"1\"/> 进行统计推断是更合理的。</p><p>一般说来，先验分布 <img src=\"https://www.zhihu.com/equation?tex=+p%5Cleft%28+%5Ctheta+%5Cright%29+\" alt=\" p\\left( \\theta \\right) \" eeimg=\"1\"/> 是反映人们在抽样前对 <img src=\"https://www.zhihu.com/equation?tex=+%5Ctheta+\" alt=\" \\theta \" eeimg=\"1\"/> 的认识，后验分布 <img src=\"https://www.zhihu.com/equation?tex=+p%5Cleft%28+%7B%5Ctheta+%7CX%7D+%5Cright%29+\" alt=\" p\\left( {\\theta |X} \\right) \" eeimg=\"1\"/> 是反映人们在抽样后对 <img src=\"https://www.zhihu.com/equation?tex=+%5Ctheta+\" alt=\" \\theta \" eeimg=\"1\"/> 的认识，之间的差异是由于样本 <img src=\"https://www.zhihu.com/equation?tex=X\" alt=\"X\" eeimg=\"1\"/> 的出现后人们对 <img src=\"https://www.zhihu.com/equation?tex=+%5Ctheta+\" alt=\" \\theta \" eeimg=\"1\"/> 认识的一种调整，所以后验分布 <img src=\"https://www.zhihu.com/equation?tex=+p%5Cleft%28+%7B%5Ctheta+%7CX%7D+%5Cright%29+\" alt=\" p\\left( {\\theta |X} \\right) \" eeimg=\"1\"/> 可以看作是人们用总体信息和样本信息（抽样信息）对先验分布 <img src=\"https://www.zhihu.com/equation?tex=+p%5Cleft%28+%5Ctheta+%5Cright%29+\" alt=\" p\\left( \\theta \\right) \" eeimg=\"1\"/> 作调整的结果。下面我们介绍三种估计方法：</p><p><b>1. 最大似然估计（ML）</b></p><p>最大似然估计是找到参数 <img src=\"https://www.zhihu.com/equation?tex=+%5Ctheta+\" alt=\" \\theta \" eeimg=\"1\"/> 使得样本 <img src=\"https://www.zhihu.com/equation?tex=+X+\" alt=\" X \" eeimg=\"1\"/> 的联合概率最大，并不会考虑先验知识，频率学派和贝叶斯学派都承认似然函数，频率学派认为参数 <img src=\"https://www.zhihu.com/equation?tex=+%5Ctheta+\" alt=\" \\theta \" eeimg=\"1\"/> 是客观存在的，只是未知。求参数 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 使似然函数最大，ML估计问题可以用下面公式表示：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Chat+%5Ctheta+%7B%5Crm%7B+%3D+%7D%7D%5Cmathop+%7B%5Carg+%5Cmax+%7D%5Climits_%5Ctheta+%5Csum%5Climits_%7Bi+%3D+1%7D%5En+%7B%5Clog+p%5Cleft%28+%7B%7Bx_i%7D%7C%5Ctheta+%7D+%5Cright%29%7D+%5Ctag%7B6%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} \\hat \\theta {\\rm{ = }}\\mathop {\\arg \\max }\\limits_\\theta \\sum\\limits_{i = 1}^n {\\log p\\left( {{x_i}|\\theta } \\right)} \\tag{6} \\end{align*}\" eeimg=\"1\"/> </p><p>通常可以令导数为 <img src=\"https://www.zhihu.com/equation?tex=+0+\" alt=\" 0 \" eeimg=\"1\"/> 求得 <img src=\"https://www.zhihu.com/equation?tex=+%5Ctheta+\" alt=\" \\theta \" eeimg=\"1\"/> 的值。ML估计不会把先验知识考虑进去，很容易出现过拟合的现象。我们举个例子，抛一枚硬币，假设正面向上的概率为 <img src=\"https://www.zhihu.com/equation?tex=p+\" alt=\"p \" eeimg=\"1\"/> ，抛了 <img src=\"https://www.zhihu.com/equation?tex=+N+\" alt=\" N \" eeimg=\"1\"/> 次，正面出现 <img src=\"https://www.zhihu.com/equation?tex=%7Bn%5E%7B%5Cleft%28+1+%5Cright%29%7D%7D+\" alt=\"{n^{\\left( 1 \\right)}} \" eeimg=\"1\"/> 次，反面出现 <img src=\"https://www.zhihu.com/equation?tex=+%7Bn%5E%7B%5Cleft%28+0+%5Cright%29%7D%7D+\" alt=\" {n^{\\left( 0 \\right)}} \" eeimg=\"1\"/> 次， <img src=\"https://www.zhihu.com/equation?tex=+c+%3D+1+\" alt=\" c = 1 \" eeimg=\"1\"/> 表示正面， <img src=\"https://www.zhihu.com/equation?tex=+c+%3D+0+\" alt=\" c = 0 \" eeimg=\"1\"/> 表示反面，我们用ML估计：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+L%5Cleft%28+p+%5Cright%29+%26%3D+%5Csum%5Climits_%7Bi+%3D+1%7D%5EN+%7B%5Clog+%7D+p%5Cleft%28+%7BC+%3D+%7Bc_i%7D%7Cp%7D+%5Cright%29%5C%5C+%26%3D+%7Bn%5E%7B%5Cleft%28+1+%5Cright%29%7D%7D%5Clog+p%5Cleft%28+%7BC+%3D+1%7Cp%7D+%5Cright%29+%2B+%7Bn%5E%7B%5Cleft%28+0+%5Cright%29%7D%7D%5Clog+p%5Cleft%28+%7BC+%3D+0%7Cp%7D+%5Cright%29%5C%5C+%26%3D+%7Bn%5E%7B%5Cleft%28+1+%5Cright%29%7D%7D%5Clog+p+%2B+%7Bn%5E%7B%5Cleft%28+0+%5Cright%29%7D%7D%5Clog+%5Cleft%28+%7B1+-+p%7D+%5Cright%29%5C%5C+%5Cfrac%7B%7B%5Cpartial+L%7D%7D%7B%7B%5Cpartial+p%7D%7D+%26%3D+0+%5CRightarrow+%7B%7B%5Chat+p%7D_%7BML%7D%7D+%3D+%5Cfrac%7B%7B%7Bn%5E%7B%5Cleft%28+1+%5Cright%29%7D%7D%7D%7D%7B%7B%7Bn%5E%7B%5Cleft%28+1+%5Cright%29%7D%7D+%2B+%7Bn%5E%7B%5Cleft%28+0+%5Cright%29%7D%7D%7D%7D+%5Ctag%7B7%7D+%5Cend%7Balign%2A%7D+\" alt=\"\\begin{align*} L\\left( p \\right) &amp;= \\sum\\limits_{i = 1}^N {\\log } p\\left( {C = {c_i}|p} \\right)\\\\ &amp;= {n^{\\left( 1 \\right)}}\\log p\\left( {C = 1|p} \\right) + {n^{\\left( 0 \\right)}}\\log p\\left( {C = 0|p} \\right)\\\\ &amp;= {n^{\\left( 1 \\right)}}\\log p + {n^{\\left( 0 \\right)}}\\log \\left( {1 - p} \\right)\\\\ \\frac{{\\partial L}}{{\\partial p}} &amp;= 0 \\Rightarrow {{\\hat p}_{ML}} = \\frac{{{n^{\\left( 1 \\right)}}}}{{{n^{\\left( 1 \\right)}} + {n^{\\left( 0 \\right)}}}} \\tag{7} \\end{align*} \" eeimg=\"1\"/> </p><p>如果 <img src=\"https://www.zhihu.com/equation?tex=%7Bn%5E%7B%5Cleft%28+1+%5Cright%29%7D%7D+%3D+14+%2C+%7Bn%5E%7B%5Cleft%28+0+%5Cright%29%7D%7D+%3D+6+\" alt=\"{n^{\\left( 1 \\right)}} = 14 , {n^{\\left( 0 \\right)}} = 6 \" eeimg=\"1\"/> ，则 <img src=\"https://www.zhihu.com/equation?tex=+%7B%5Chat+p_%7BML%7D%7D+%3D+0.7+\" alt=\" {\\hat p_{ML}} = 0.7 \" eeimg=\"1\"/> ，似乎比我们认知的 <img src=\"https://www.zhihu.com/equation?tex=+0.5+\" alt=\" 0.5 \" eeimg=\"1\"/> 高了很多。</p><p><b>2. 最大后验估计（MAP）</b></p><p>MAP 是为了解决 ML 缺少先验知识的缺点，刚好公式 (5) 后验概率集中了样本信息和先验信息，所以 MAP 估计问题可以用下面公式表示：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Chat+%5Ctheta+%26%3D+%5Cmathop+%7B%5Carg+%5Cmax+%7D%5Climits_%5Ctheta+p%5Cleft%28+%7B%5Ctheta+%7CX%7D+%5Cright%29%5C%5C+%26%3D+%5Cmathop+%7B%5Carg+%5Cmax+%7D%5Climits_%5Ctheta+%5Cfrac%7B%7Bp%5Cleft%28+%7BX%7C%5Ctheta+%7D+%5Cright%29p%5Cleft%28+%5Ctheta+%5Cright%29%7D%7D%7B%7Bp%5Cleft%28+X+%5Cright%29%7D%7D%5C%5C+%26%3D+%5Cmathop+%7B%5Carg+%5Cmax+%7D%5Climits_%5Ctheta+p%5Cleft%28+%7BX%7C%5Ctheta+%7D+%5Cright%29p%5Cleft%28+%5Ctheta+%5Cright%29%5C%5C+%26%3D+%5Cmathop+%7B%5Carg+%5Cmax+%7D%5Climits_%5Ctheta+%5Cleft%5C%7B+%7B%5Csum%5Climits_%7Bi+%3D+1%7D%5En+%7B%5Clog+p%5Cleft%28+%7B%7Bx_i%7D%7C%5Ctheta+%7D+%5Cright%29%7D+%2B+%5Clog+p%5Cleft%28+%5Ctheta+%5Cright%29%7D+%5Cright%5C%7D+%5Ctag%7B8%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} \\hat \\theta &amp;= \\mathop {\\arg \\max }\\limits_\\theta p\\left( {\\theta |X} \\right)\\\\ &amp;= \\mathop {\\arg \\max }\\limits_\\theta \\frac{{p\\left( {X|\\theta } \\right)p\\left( \\theta \\right)}}{{p\\left( X \\right)}}\\\\ &amp;= \\mathop {\\arg \\max }\\limits_\\theta p\\left( {X|\\theta } \\right)p\\left( \\theta \\right)\\\\ &amp;= \\mathop {\\arg \\max }\\limits_\\theta \\left\\{ {\\sum\\limits_{i = 1}^n {\\log p\\left( {{x_i}|\\theta } \\right)} + \\log p\\left( \\theta \\right)} \\right\\} \\tag{8} \\end{align*}\" eeimg=\"1\"/> </p><p>MAP 不仅希望似然函数最大，还希望自己出现的先验概率也最大，加入先验概率，起到正则化的作用，如果 <img src=\"https://www.zhihu.com/equation?tex=+%5Ctheta+\" alt=\" \\theta \" eeimg=\"1\"/> 服从高斯分布，相当于加一个 <img src=\"https://www.zhihu.com/equation?tex=L2\" alt=\"L2\" eeimg=\"1\"/> 范数正则化，如果 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta+\" alt=\"\\theta \" eeimg=\"1\"/> 服从拉普拉斯分布，相当于加一个 <img src=\"https://www.zhihu.com/equation?tex=L1\" alt=\"L1\" eeimg=\"1\"/> 范数正则化。我们继续前面抛硬币的例子，大部分人认为 应该等于 <img src=\"https://www.zhihu.com/equation?tex=0.5\" alt=\"0.5\" eeimg=\"1\"/> ，那么还有少数人认为 <img src=\"https://www.zhihu.com/equation?tex=+p+\" alt=\" p \" eeimg=\"1\"/> 取其他值，我们认为 <img src=\"https://www.zhihu.com/equation?tex=+p\" alt=\" p\" eeimg=\"1\"/> 的取值服从 Beta 分布。</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+p%5Cleft%28+%7Bp%7C%5Calpha+%2C%5Cbeta+%7D+%5Cright%29+%26%3D+%5Cfrac%7B1%7D%7B%7BB%5Cleft%28+%7B%5Calpha+%2C%5Cbeta+%7D+%5Cright%29%7D%7D%7Bp%5E%7B%5Calpha+-+1%7D%7D%7B%5Cleft%28+%7B1+-+p%7D+%5Cright%29%5E%7B%5Cbeta+-+1%7D%7D%5C%5C+L%5Cleft%28+p+%5Cright%29+%26%3D+%7Bn%5E%7B%5Cleft%28+1+%5Cright%29%7D%7D%5Clog+p+%2B+%7Bn%5E%7B%5Cleft%28+0+%5Cright%29%7D%7D%5Clog+%5Cleft%28+%7B1+-+p%7D+%5Cright%29+%2B+%5Cleft%28+%7B%5Calpha+-+1%7D+%5Cright%29%5Clog+p+%2B+%5Cleft%28+%7B%5Cbeta+-+1%7D+%5Cright%29%5Clog+%5Cleft%28+%7B1+-+p%7D+%5Cright%29+-+%5Clog+B%5Cleft%28+%7B%5Calpha+%2C%5Cbeta+%7D+%5Cright%29%5C%5C+%5Cfrac%7B%7B%5Cpartial+L%7D%7D%7B%7B%5Cpartial+p%7D%7D+%26%3D+%5Cfrac%7B%7B%7Bn%5E%7B%5Cleft%28+1+%5Cright%29%7D%7D%7D%7D%7Bp%7D+-+%5Cfrac%7B%7B%7Bn%5E%7B%5Cleft%28+0+%5Cright%29%7D%7D%7D%7D%7B%7B1+-+p%7D%7D+%2B+%5Cfrac%7B%7B%5Calpha+-+1%7D%7D%7Bp%7D+-+%5Cfrac%7B%7B%5Cbeta+-+1%7D%7D%7B%7B1+-+p%7D%7D+%5CRightarrow+%7B%7B%5Chat+p%7D_%7BMAP%7D%7D+%3D+%5Cfrac%7B%7B%7Bn%5E%7B%5Cleft%28+1+%5Cright%29%7D%7D+%2B+%5Calpha+-+1%7D%7D%7B%7B%7Bn%5E%7B%5Cleft%28+1+%5Cright%29%7D%7D+%2B+%7Bn%5E%7B%5Cleft%28+0+%5Cright%29%7D%7D+%2B+%5Calpha+%7B%5Crm%7B+%2B+%7D%7D%5Cbeta+-+2%7D%7D+%5Ctag%7B9%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} p\\left( {p|\\alpha ,\\beta } \\right) &amp;= \\frac{1}{{B\\left( {\\alpha ,\\beta } \\right)}}{p^{\\alpha - 1}}{\\left( {1 - p} \\right)^{\\beta - 1}}\\\\ L\\left( p \\right) &amp;= {n^{\\left( 1 \\right)}}\\log p + {n^{\\left( 0 \\right)}}\\log \\left( {1 - p} \\right) + \\left( {\\alpha - 1} \\right)\\log p + \\left( {\\beta - 1} \\right)\\log \\left( {1 - p} \\right) - \\log B\\left( {\\alpha ,\\beta } \\right)\\\\ \\frac{{\\partial L}}{{\\partial p}} &amp;= \\frac{{{n^{\\left( 1 \\right)}}}}{p} - \\frac{{{n^{\\left( 0 \\right)}}}}{{1 - p}} + \\frac{{\\alpha - 1}}{p} - \\frac{{\\beta - 1}}{{1 - p}} \\Rightarrow {{\\hat p}_{MAP}} = \\frac{{{n^{\\left( 1 \\right)}} + \\alpha - 1}}{{{n^{\\left( 1 \\right)}} + {n^{\\left( 0 \\right)}} + \\alpha {\\rm{ + }}\\beta - 2}} \\tag{9} \\end{align*}\" eeimg=\"1\"/> </p><p>我们取 <img src=\"https://www.zhihu.com/equation?tex=+%5Calpha+%7B%5Crm%7B+%3D+%7D%7D5%2C%5Cbeta+%3D+5\" alt=\" \\alpha {\\rm{ = }}5,\\beta = 5\" eeimg=\"1\"/> ，即 <img src=\"https://www.zhihu.com/equation?tex=+p+\" alt=\" p \" eeimg=\"1\"/> 以最大的概率取 <img src=\"https://www.zhihu.com/equation?tex=0.5\" alt=\"0.5\" eeimg=\"1\"/> ，得到 <img src=\"https://www.zhihu.com/equation?tex=+%7B%5Chat+p_%7BMAP%7D%7D+%3D+0.64\" alt=\" {\\hat p_{MAP}} = 0.64\" eeimg=\"1\"/> 。</p><p><b>3. 贝叶斯估计</b></p><p>前面介绍的 ML 和 MAP 属于点估计，贝叶斯估计不再把参数 <img src=\"https://www.zhihu.com/equation?tex=+%5Ctheta+\" alt=\" \\theta \" eeimg=\"1\"/> 看成一个未知的确定值，而是看成未知的随机变量，利用贝叶斯定理结合新的样本信息和参数 <img src=\"https://www.zhihu.com/equation?tex=+%5Ctheta+\" alt=\" \\theta \" eeimg=\"1\"/> 的先验分布，来得到 <img src=\"https://www.zhihu.com/equation?tex=+%5Ctheta\" alt=\" \\theta\" eeimg=\"1\"/> 的新的概率分布（后验分布）。贝叶斯估计的本质是通过贝叶斯决策得到参数 <img src=\"https://www.zhihu.com/equation?tex=+%5Ctheta+\" alt=\" \\theta \" eeimg=\"1\"/> 的最优估计 <img src=\"https://www.zhihu.com/equation?tex=+%5Chat+%5Ctheta+\" alt=\" \\hat \\theta \" eeimg=\"1\"/> ，使得贝叶斯期望损失最小。贝叶斯期望损失为：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+R%5Cleft%28+%7B%5Chat+%5Ctheta+%7CX%7D+%5Cright%29+%3D+%5Cint_%5CTheta+%7B%5Clambda+%5Cleft%28+%7B%5Chat+%5Ctheta+%2C%5Ctheta+%7D+%5Cright%29%7D+p%5Cleft%28+%7B%5Ctheta+%7CX%7D+%5Cright%29d%5Ctheta+%5Ctag%7B10%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} R\\left( {\\hat \\theta |X} \\right) = \\int_\\Theta {\\lambda \\left( {\\hat \\theta ,\\theta } \\right)} p\\left( {\\theta |X} \\right)d\\theta \\tag{10} \\end{align*}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%5Clambda+%5Cleft%28+%7B%5Chat+%5Ctheta+%2C%5Ctheta+%7D+%5Cright%29+\" alt=\"\\lambda \\left( {\\hat \\theta ,\\theta } \\right) \" eeimg=\"1\"/> 是损失函数，我们希望 <img src=\"https://www.zhihu.com/equation?tex=R%5Cleft%28+%7B%5Chat+%5Ctheta+%7CX%7D+%5Cright%29\" alt=\"R\\left( {\\hat \\theta |X} \\right)\" eeimg=\"1\"/> 最小。如果 <img src=\"https://www.zhihu.com/equation?tex=+%5Clambda+%5Cleft%28+%7B%5Chat+%5Ctheta+%2C%5Ctheta+%7D+%5Cright%29+%3D+%5Cleft%5C%7C+%7B%5Chat+%5Ctheta+-+%5Ctheta+%7D+%5Cright%5C%7C_2%5E2+\" alt=\" \\lambda \\left( {\\hat \\theta ,\\theta } \\right) = \\left\\| {\\hat \\theta - \\theta } \\right\\|_2^2 \" eeimg=\"1\"/> ，则：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Cfrac%7B%7B%5Cpartial+R%5Cleft%28+%7B%5Chat+%5Ctheta+%7CX%7D+%5Cright%29%7D%7D%7B%7B%5Cpartial+%5Chat+%5Ctheta+%7D%7D+%26%3D+%5Cint_%5CTheta+%7B2%5Cleft%28+%7B%5Chat+%5Ctheta+-+%5Ctheta+%7D+%5Cright%29%7D+p%5Cleft%28+%7B%5Ctheta+%7CX%7D+%5Cright%29d%5Ctheta+%3D+0%5C%5C+%5CRightarrow+%5Chat+%5Ctheta+%26%3D+%5Cint_%5CTheta+%5Ctheta+p%5Cleft%28+%7B%5Ctheta+%7CX%7D+%5Cright%29d%5Ctheta+%5Ctag%7B11%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} \\frac{{\\partial R\\left( {\\hat \\theta |X} \\right)}}{{\\partial \\hat \\theta }} &amp;= \\int_\\Theta {2\\left( {\\hat \\theta - \\theta } \\right)} p\\left( {\\theta |X} \\right)d\\theta = 0\\\\ \\Rightarrow \\hat \\theta &amp;= \\int_\\Theta \\theta p\\left( {\\theta |X} \\right)d\\theta \\tag{11} \\end{align*}\" eeimg=\"1\"/> </p><p>所以贝叶斯估计值为在样本 <img src=\"https://www.zhihu.com/equation?tex=+X+\" alt=\" X \" eeimg=\"1\"/> 条件下 <img src=\"https://www.zhihu.com/equation?tex=+%5Ctheta+\" alt=\" \\theta \" eeimg=\"1\"/> 的期望值，贝叶斯估计的步骤为：</p><ul><li>确定参数 <img src=\"https://www.zhihu.com/equation?tex=+%5Ctheta+\" alt=\" \\theta \" eeimg=\"1\"/> 的先验分布 <img src=\"https://www.zhihu.com/equation?tex=+p%5Cleft%28+%5Ctheta+%5Cright%29\" alt=\" p\\left( \\theta \\right)\" eeimg=\"1\"/> </li><li>利用贝叶斯公式，求 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta+\" alt=\"\\theta \" eeimg=\"1\"/> 的后验分布：</li></ul><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+p%5Cleft%28+%7B%5Ctheta+%7CX%7D+%5Cright%29+%3D+%5Cfrac%7B%7Bp%5Cleft%28+%7BX%7C%5Ctheta+%7D+%5Cright%29p%5Cleft%28+%5Ctheta+%5Cright%29%7D%7D%7B%7B%5Cint_%5CTheta+%7Bp%5Cleft%28+%7BX%7C%5Ctheta+%7D+%5Cright%29p%5Cleft%28+%5Ctheta+%5Cright%29d%5Ctheta+%7D+%7D%7D+%5Ctag%7B12%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} p\\left( {\\theta |X} \\right) = \\frac{{p\\left( {X|\\theta } \\right)p\\left( \\theta \\right)}}{{\\int_\\Theta {p\\left( {X|\\theta } \\right)p\\left( \\theta \\right)d\\theta } }} \\tag{12} \\end{align*}\" eeimg=\"1\"/> </p><ul><li>求出贝叶斯的估计值：</li></ul><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Chat+%5Ctheta+%3D+%5Cint_%5CTheta+%5Ctheta+p%5Cleft%28+%7B%5Ctheta+%7CX%7D+%5Cright%29d%5Ctheta+%5Ctag%7B13%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} \\hat \\theta = \\int_\\Theta \\theta p\\left( {\\theta |X} \\right)d\\theta \\tag{13} \\end{align*}\" eeimg=\"1\"/> </p><p>我们继续前面的抛硬币的例子，后验概率：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+p%5Cleft%28+%7Bp%7CC%2C%5Calpha+%2C%5Cbeta+%7D+%5Cright%29+%26%3D+%5Cfrac%7B%7B%5Cprod%5Cnolimits_%7Bi+%3D+1%7D%5EN+%7Bp%5Cleft%28+%7BC+%3D+%7Bc_i%7D%7Cp%7D+%5Cright%29p%5Cleft%28+%7Bp%7C%5Calpha+%2C%5Cbeta+%7D+%5Cright%29%7D+%7D%7D%7B%7B%5Cint_0%5E1+%7B%5Cprod%5Cnolimits_%7Bi+%3D+1%7D%5EN+%7Bp%5Cleft%28+%7BC+%3D+%7Bc_i%7D%7Cp%7D+%5Cright%29p%5Cleft%28+%7Bp%7C%5Calpha+%2C%5Cbeta+%7D+%5Cright%29%7D+dp%7D+%7D%7D%5C%5C+%26%3D+%5Cfrac%7B%7B%7Bp%5E%7Bn%5Cleft%28+1+%5Cright%29%7D%7D%7B%7B%5Cleft%28+%7B1+-+p%7D+%5Cright%29%7D%5E%7Bn%5Cleft%28+0+%5Cright%29%7D%7D%5Cfrac%7B1%7D%7B%7BB%5Cleft%28+%7B%5Calpha+%2C%5Cbeta+%7D+%5Cright%29%7D%7D%7Bp%5E%7B%5Calpha+-+1%7D%7D%7B%7B%5Cleft%28+%7B1+-+p%7D+%5Cright%29%7D%5E%7B%5Cbeta+-+1%7D%7D%7D%7D%7B%7B%5Cint_0%5E1+%7B%7Bp%5E%7Bn%5Cleft%28+1+%5Cright%29%7D%7D%7B%7B%5Cleft%28+%7B1+-+p%7D+%5Cright%29%7D%5E%7Bn%5Cleft%28+0+%5Cright%29%7D%7D%5Cfrac%7B1%7D%7B%7BB%5Cleft%28+%7B%5Calpha+%2C%5Cbeta+%7D+%5Cright%29%7D%7D%7Bp%5E%7B%5Calpha+-+1%7D%7D%7B%7B%5Cleft%28+%7B1+-+p%7D+%5Cright%29%7D%5E%7B%5Cbeta+-+1%7D%7Ddp%7D+%7D%7D%5C%5C+%26%3D+%5Cfrac%7B%7B%7Bp%5E%7Bn%5Cleft%28+1+%5Cright%29+%2B+%5Calpha+-+1%7D%7D%7B%7B%5Cleft%28+%7B1+-+p%7D+%5Cright%29%7D%5E%7Bn%5Cleft%28+0+%5Cright%29+%2B+%5Cbeta+-+1%7D%7D%7D%7D%7B%7B%5Cint_0%5E1+%7B%7Bp%5E%7Bn%5Cleft%28+1+%5Cright%29+%2B+%5Calpha+-+1%7D%7D%7B%7B%5Cleft%28+%7B1+-+p%7D+%5Cright%29%7D%5E%7Bn%5Cleft%28+0+%5Cright%29+%2B+%5Cbeta+-+1%7D%7Ddp%7D+%7D%7D%5C%5C+%26%3D+Beta%5Cleft%28+%7Bp%7Cn%5Cleft%28+1+%5Cright%29+%2B+%5Calpha+%2Cn%5Cleft%28+0+%5Cright%29+%2B+%5Cbeta+%7D+%5Cright%29+%5Ctag%7B14%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} p\\left( {p|C,\\alpha ,\\beta } \\right) &amp;= \\frac{{\\prod\\nolimits_{i = 1}^N {p\\left( {C = {c_i}|p} \\right)p\\left( {p|\\alpha ,\\beta } \\right)} }}{{\\int_0^1 {\\prod\\nolimits_{i = 1}^N {p\\left( {C = {c_i}|p} \\right)p\\left( {p|\\alpha ,\\beta } \\right)} dp} }}\\\\ &amp;= \\frac{{{p^{n\\left( 1 \\right)}}{{\\left( {1 - p} \\right)}^{n\\left( 0 \\right)}}\\frac{1}{{B\\left( {\\alpha ,\\beta } \\right)}}{p^{\\alpha - 1}}{{\\left( {1 - p} \\right)}^{\\beta - 1}}}}{{\\int_0^1 {{p^{n\\left( 1 \\right)}}{{\\left( {1 - p} \\right)}^{n\\left( 0 \\right)}}\\frac{1}{{B\\left( {\\alpha ,\\beta } \\right)}}{p^{\\alpha - 1}}{{\\left( {1 - p} \\right)}^{\\beta - 1}}dp} }}\\\\ &amp;= \\frac{{{p^{n\\left( 1 \\right) + \\alpha - 1}}{{\\left( {1 - p} \\right)}^{n\\left( 0 \\right) + \\beta - 1}}}}{{\\int_0^1 {{p^{n\\left( 1 \\right) + \\alpha - 1}}{{\\left( {1 - p} \\right)}^{n\\left( 0 \\right) + \\beta - 1}}dp} }}\\\\ &amp;= Beta\\left( {p|n\\left( 1 \\right) + \\alpha ,n\\left( 0 \\right) + \\beta } \\right) \\tag{14} \\end{align*}\" eeimg=\"1\"/> </p><p>其中 <img src=\"https://www.zhihu.com/equation?tex=+B%5Cleft%28+%7B%5Calpha+%2C%5Cbeta+%7D+%5Cright%29+%3D+%5Cint_0%5E1+%7B%7Bp%5E%7B%5Calpha+-+1%7D%7D%7B%7B%5Cleft%28+%7B1+-+p%7D+%5Cright%29%7D%5E%7B%5Cbeta+-+1%7D%7Ddp%7D\" alt=\" B\\left( {\\alpha ,\\beta } \\right) = \\int_0^1 {{p^{\\alpha - 1}}{{\\left( {1 - p} \\right)}^{\\beta - 1}}dp}\" eeimg=\"1\"/> ，所以可以得：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Chat+p+%3D+%5Cfrac%7B%7B%7Bn%5E%7B%5Cleft%28+1+%5Cright%29%7D%7D+%2B+%5Calpha+%7D%7D%7B%7B%7Bn%5E%7B%5Cleft%28+1+%5Cright%29%7D%7D+%2B+%7Bn%5E%7B%5Cleft%28+0+%5Cright%29%7D%7D+%2B+%5Calpha+%2B+%5Cbeta+%7D%7D+%5Ctag%7B15%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} \\hat p = \\frac{{{n^{\\left( 1 \\right)}} + \\alpha }}{{{n^{\\left( 1 \\right)}} + {n^{\\left( 0 \\right)}} + \\alpha + \\beta }} \\tag{15} \\end{align*}\" eeimg=\"1\"/> </p><h2><b>1.2 Gamma函数</b></h2><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5CGamma+%5Cleft%28+x+%5Cright%29+%3D+%5Cint_0%5E%5Cinfty+%7B%7Bt%5E%7Bx+-+1%7D%7D%7Be%5E%7B+-+t%7D%7Ddt%7D+%5Ctag%7B16%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} \\Gamma \\left( x \\right) = \\int_0^\\infty {{t^{x - 1}}{e^{ - t}}dt} \\tag{16} \\end{align*}\" eeimg=\"1\"/> </p><p>通过分部积分的方法，可以得到一个递归性质。</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5CGamma+%5Cleft%28+%7Bx+%2B+1%7D+%5Cright%29+%26%3D+%5Cint_0%5E%5Cinfty+%7B%7Bt%5Ex%7D%7Be%5E%7B+-+t%7D%7Ddt%7D+%5C%5C+%26%3D+-+%5Cint_0%5E%5Cinfty+%7B%7Bt%5Ex%7Dd%7Be%5E%7B+-+t%7D%7D%7D+%5C%5C+%26%3D+-+%5Cleft%5B+%7B%7Bt%5Ex%7D%7Be%5E%7B+-+t%7D%7D%7D+%5Cright%5D_0%5E%5Cinfty+%2B+%5Cint_0%5E%5Cinfty+%7B%7Be%5E%7B+-+t%7D%7Dd%7Bt%5Ex%7D%7D+%5C%5C+%26%3D+x%5Cint_0%5E%5Cinfty+%7B%7Bt%5E%7Bx+-+1%7D%7D%7Be%5E%7B+-+t%7D%7Dd%7D+t+%3D+x%5CGamma+%5Cleft%28+x+%5Cright%29+%5Ctag%7B17%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} \\Gamma \\left( {x + 1} \\right) &amp;= \\int_0^\\infty {{t^x}{e^{ - t}}dt} \\\\ &amp;= - \\int_0^\\infty {{t^x}d{e^{ - t}}} \\\\ &amp;= - \\left[ {{t^x}{e^{ - t}}} \\right]_0^\\infty + \\int_0^\\infty {{e^{ - t}}d{t^x}} \\\\ &amp;= x\\int_0^\\infty {{t^{x - 1}}{e^{ - t}}d} t = x\\Gamma \\left( x \\right) \\tag{17} \\end{align*}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%5CGamma+%5Cleft%28+x+%5Cright%29+\" alt=\"\\Gamma \\left( x \\right) \" eeimg=\"1\"/> 函数可以当成是阶乘在实数集上的延拓， <img src=\"https://www.zhihu.com/equation?tex=%5CGamma+%5Cleft%28+n+%5Cright%29+%3D+%5Cleft%28+%7Bn+-+1%7D+%5Cright%29%21\" alt=\"\\Gamma \\left( n \\right) = \\left( {n - 1} \\right)!\" eeimg=\"1\"/> 。</p><h2><b>1.3 二项分布</b></h2><p>在概率论中，试验 <img src=\"https://www.zhihu.com/equation?tex=+E+\" alt=\" E \" eeimg=\"1\"/> 只有两个可能结果： <img src=\"https://www.zhihu.com/equation?tex=A\" alt=\"A\" eeimg=\"1\"/> 及 <img src=\"https://www.zhihu.com/equation?tex=+%5Cbar+A\" alt=\" \\bar A\" eeimg=\"1\"/> ，则称 <img src=\"https://www.zhihu.com/equation?tex=+E+\" alt=\" E \" eeimg=\"1\"/> 为伯努利 (Bernoulli) 试验。设 <img src=\"https://www.zhihu.com/equation?tex=p%5Cleft%28+A+%5Cright%29+%3D+p\" alt=\"p\\left( A \\right) = p\" eeimg=\"1\"/> ，则 <img src=\"https://www.zhihu.com/equation?tex=+p%5Cleft%28+%7B%5Cbar+A%7D+%5Cright%29+%3D+1+-+p\" alt=\" p\\left( {\\bar A} \\right) = 1 - p\" eeimg=\"1\"/> 。将 <img src=\"https://www.zhihu.com/equation?tex=+E+\" alt=\" E \" eeimg=\"1\"/> 独立重复地进行 <img src=\"https://www.zhihu.com/equation?tex=+n+\" alt=\" n \" eeimg=\"1\"/> 次，则称这一串重复的独立试验为 <img src=\"https://www.zhihu.com/equation?tex=+n+\" alt=\" n \" eeimg=\"1\"/> 重伯努利试验，这里重复是指在每次试验中 <img src=\"https://www.zhihu.com/equation?tex=+p%5Cleft%28+A+%5Cright%29+%3D+p+\" alt=\" p\\left( A \\right) = p \" eeimg=\"1\"/> 保持不变，独立是指各次试验的结果互不影响。以 <img src=\"https://www.zhihu.com/equation?tex=+X+\" alt=\" X \" eeimg=\"1\"/> 表示 <img src=\"https://www.zhihu.com/equation?tex=+n+\" alt=\" n \" eeimg=\"1\"/> 重伯努利试验中事件 <img src=\"https://www.zhihu.com/equation?tex=+A+\" alt=\" A \" eeimg=\"1\"/> 发生的次数，称随机变量 <img src=\"https://www.zhihu.com/equation?tex=+X+\" alt=\" X \" eeimg=\"1\"/> 服从参数为 <img src=\"https://www.zhihu.com/equation?tex=+n%2Cp+\" alt=\" n,p \" eeimg=\"1\"/> 的二项分布，记为 <img src=\"https://www.zhihu.com/equation?tex=+X+%5Csim+B%5Cleft%28+%7Bn%2Cp%7D+%5Cright%29\" alt=\" X \\sim B\\left( {n,p} \\right)\" eeimg=\"1\"/> 。</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+p%5Cleft%28+%7BX+%3D+k%7D+%5Cright%29+%3D+%5Cleft%28+%7B%5Cbegin%7Barray%7D%7B%2A%7B20%7D%7Bc%7D%7D+n%5C%5C+k+%5Cend%7Barray%7D%7D+%5Cright%29%7Bp%5Ek%7D%7B%5Cleft%28+%7B1+-+p%7D+%5Cright%29%5E%7Bn+-+k%7D%7D+%5Ctag%7B18%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} p\\left( {X = k} \\right) = \\left( {\\begin{array}{*{20}{c}} n\\\\ k \\end{array}} \\right){p^k}{\\left( {1 - p} \\right)^{n - k}} \\tag{18} \\end{align*}\" eeimg=\"1\"/> </p><h2><b>1.4 Beta分布</b></h2><p>Beta 分布是指一组定义在 <img src=\"https://www.zhihu.com/equation?tex=+%5Cleft%28+%7B0%2C1%7D+%5Cright%29+\" alt=\" \\left( {0,1} \\right) \" eeimg=\"1\"/> 区间的连续概率分布，其概率密度函数是：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+Beta%5Cleft%28+%7Bp%7C%5Calpha+%2C%5Cbeta+%7D+%5Cright%29+%26%3D+%5Cfrac%7B%7B%7Bp%5E%7B%5Calpha+-+1%7D%7D%7B%7B%5Cleft%28+%7B1+-+p%7D+%5Cright%29%7D%5E%7B%5Cbeta+-+1%7D%7D%7D%7D%7B%7B%5Cint_0%5E1+%7B%7Bp%5E%7B%5Calpha+-+1%7D%7D%7B%7B%5Cleft%28+%7B1+-+p%7D+%5Cright%29%7D%5E%7B%5Cbeta+-+1%7D%7Ddp%7D+%7D%7D%5C%5C+%26%3D+%5Cfrac%7B%7B%5CGamma+%5Cleft%28+%7B%5Calpha+%2B+%5Cbeta+%7D+%5Cright%29%7D%7D%7B%7B%5CGamma+%5Cleft%28+%5Calpha+%5Cright%29%5CGamma+%5Cleft%28+%5Cbeta+%5Cright%29%7D%7D%7Bp%5E%7B%5Calpha+-+1%7D%7D%7B%5Cleft%28+%7B1+-+p%7D+%5Cright%29%5E%7B%5Cbeta+-+1%7D%7D+%5Ctag%7B19%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} Beta\\left( {p|\\alpha ,\\beta } \\right) &amp;= \\frac{{{p^{\\alpha - 1}}{{\\left( {1 - p} \\right)}^{\\beta - 1}}}}{{\\int_0^1 {{p^{\\alpha - 1}}{{\\left( {1 - p} \\right)}^{\\beta - 1}}dp} }}\\\\ &amp;= \\frac{{\\Gamma \\left( {\\alpha + \\beta } \\right)}}{{\\Gamma \\left( \\alpha \\right)\\Gamma \\left( \\beta \\right)}}{p^{\\alpha - 1}}{\\left( {1 - p} \\right)^{\\beta - 1}} \\tag{19} \\end{align*}\" eeimg=\"1\"/> </p><p>1） <img src=\"https://www.zhihu.com/equation?tex=B%5Cleft%28+%7B%5Calpha+%2C%5Cbeta+%7D+%5Cright%29+%3D+%5Cfrac%7B%7B%5CGamma+%5Cleft%28+%5Calpha+%5Cright%29%5CGamma+%5Cleft%28+%5Cbeta+%5Cright%29%7D%7D%7B%7B%5CGamma+%5Cleft%28+%7B%5Calpha+%2B+%5Cbeta+%7D+%5Cright%29%7D%7D+%3D+%5Cint_0%5E1+%7B%7Bp%5E%7B%5Calpha+-+1%7D%7D%7B%7B%5Cleft%28+%7B1+-+p%7D+%5Cright%29%7D%5E%7B%5Cbeta+-+1%7D%7Ddp%7D+%E3%80%82%5CGamma+%5Cleft%28+%5Calpha+%5Cright%29+%3D+%5Cint_0%5E%5Cinfty+%7B%7Bx%5E%7B%5Calpha+-+1%7D%7D%7Be%5E%7B+-+x%7D%7Ddx%7B%5Crm%7B+%7D%7D%7D+%2C%5CGamma+%5Cleft%28+%5Cbeta+%5Cright%29+%3D+%5Cint_0%5E%5Cinfty+%7B%7By%5E%7B%5Cbeta+-+1%7D%7D%7Be%5E%7B+-+y%7D%7Ddy%7D+\" alt=\"B\\left( {\\alpha ,\\beta } \\right) = \\frac{{\\Gamma \\left( \\alpha \\right)\\Gamma \\left( \\beta \\right)}}{{\\Gamma \\left( {\\alpha + \\beta } \\right)}} = \\int_0^1 {{p^{\\alpha - 1}}{{\\left( {1 - p} \\right)}^{\\beta - 1}}dp} 。\\Gamma \\left( \\alpha \\right) = \\int_0^\\infty {{x^{\\alpha - 1}}{e^{ - x}}dx{\\rm{ }}} ,\\Gamma \\left( \\beta \\right) = \\int_0^\\infty {{y^{\\beta - 1}}{e^{ - y}}dy} \" eeimg=\"1\"/> </p><p>证明：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5CGamma+%5Cleft%28+%5Calpha+%5Cright%29%5CGamma+%5Cleft%28+%5Cbeta+%5Cright%29+%26%3D+%5Cint_0%5E%5Cinfty+%7B%7Bx%5E%7B%5Calpha+-+1%7D%7D%7Be%5E%7B+-+x%7D%7Ddx%7B%5Crm%7B+%7D%7D%7D+%5Cint_0%5E%5Cinfty+%7B%7By%5E%7B%5Cbeta+-+1%7D%7D%7Be%5E%7B+-+y%7D%7Ddy%7D+%5C%5C+%26%3D+%5Cint_0%5E%5Cinfty+%7B%5Cint_0%5E%5Cinfty+%7B%7Bx%5E%7B%5Calpha+-+1%7D%7D%7By%5E%7B%5Cbeta+-+1%7D%7D%7Be%5E%7B+-+%5Cleft%28+%7Bx+%2B+y%7D+%5Cright%29%7D%7Ddydx%7D+%7D+%5Ctag%7B20%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} \\Gamma \\left( \\alpha \\right)\\Gamma \\left( \\beta \\right) &amp;= \\int_0^\\infty {{x^{\\alpha - 1}}{e^{ - x}}dx{\\rm{ }}} \\int_0^\\infty {{y^{\\beta - 1}}{e^{ - y}}dy} \\\\ &amp;= \\int_0^\\infty {\\int_0^\\infty {{x^{\\alpha - 1}}{y^{\\beta - 1}}{e^{ - \\left( {x + y} \\right)}}dydx} } \\tag{20} \\end{align*}\" eeimg=\"1\"/> </p><p>令 <img src=\"https://www.zhihu.com/equation?tex=t+%3D+x+%2B+y\" alt=\"t = x + y\" eeimg=\"1\"/> ，当 <img src=\"https://www.zhihu.com/equation?tex=y+%3D+0%2Ct+%3D+x\" alt=\"y = 0,t = x\" eeimg=\"1\"/> ； <img src=\"https://www.zhihu.com/equation?tex=y+%3D+%5Cinfty+%2Ct+%3D+%5Cinfty+\" alt=\"y = \\infty ,t = \\infty \" eeimg=\"1\"/> ，可得：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5CGamma+%5Cleft%28+%5Calpha+%5Cright%29%5CGamma+%5Cleft%28+%5Cbeta+%5Cright%29+%26%3D+%5Cint_0%5E%5Cinfty+%7B%5Cint_x%5E%5Cinfty+%7B%7Bx%5E%7B%5Calpha+-+1%7D%7D%7B%7B%5Cleft%28+%7Bt+-+x%7D+%5Cright%29%7D%5E%7B%5Cbeta+-+1%7D%7D%7Be%5E%7B+-+t%7D%7Ddtdx%7D+%7D+%5C%5C+%26%3D+%5Cint_0%5E%5Cinfty+%7B%5Cint_0%5Et+%7B%7Bx%5E%7B%5Calpha+-+1%7D%7D%7B%7B%5Cleft%28+%7Bt+-+x%7D+%5Cright%29%7D%5E%7B%5Cbeta+-+1%7D%7D%7Be%5E%7B+-+t%7D%7Ddxdt%7D+%7D+%5Ctag%7B21%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} \\Gamma \\left( \\alpha \\right)\\Gamma \\left( \\beta \\right) &amp;= \\int_0^\\infty {\\int_x^\\infty {{x^{\\alpha - 1}}{{\\left( {t - x} \\right)}^{\\beta - 1}}{e^{ - t}}dtdx} } \\\\ &amp;= \\int_0^\\infty {\\int_0^t {{x^{\\alpha - 1}}{{\\left( {t - x} \\right)}^{\\beta - 1}}{e^{ - t}}dxdt} } \\tag{21} \\end{align*}\" eeimg=\"1\"/> </p><p>令 <img src=\"https://www.zhihu.com/equation?tex=x+%3D+%5Cmu+t%2C%5Cmu+%5Cin+%5Cleft%5B+%7B0%2C1%7D+%5Cright%5D\" alt=\"x = \\mu t,\\mu \\in \\left[ {0,1} \\right]\" eeimg=\"1\"/> ，可得：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5CGamma+%5Cleft%28+%5Calpha+%5Cright%29%5CGamma+%5Cleft%28+%5Cbeta+%5Cright%29+%26%3D+%5Cint_0%5E%5Cinfty+%7B%5Cint_0%5E1+%7B%7B%7B%5Cleft%28+%7B%5Cmu+t%7D+%5Cright%29%7D%5E%7B%5Calpha+-+1%7D%7D%7B%7B%5Cleft%28+%7Bt+-+%5Cmu+t%7D+%5Cright%29%7D%5E%7B%5Cbeta+-+1%7D%7D%7Be%5E%7B+-+t%7D%7Dd%5Cmu+tdt%7D+%7D+%5C%5C+%26+%3D+%5Cint_0%5E%5Cinfty+%7B%7Bt%5E%7B%5Calpha+%2B+%5Cbeta+-+1%7D%7D%7Be%5E%7B+-+t%7D%7Ddt%7D+%5Cint_0%5E1+%7B%7B%5Cmu+%5E%7B%5Calpha+-+1%7D%7D%7B%7B%5Cleft%28+%7B1+-+%5Cmu+%7D+%5Cright%29%7D%5E%7B%5Cbeta+-+1%7D%7Dd%5Cmu+%7D+%5C%5C+%26+%3D+%5CGamma+%5Cleft%28+%7B%5Calpha+%2B+%5Cbeta+%7D+%5Cright%29%5Cint_0%5E1+%7B%7B%5Cmu+%5E%7B%5Calpha+-+1%7D%7D%7B%7B%5Cleft%28+%7B1+-+%5Cmu+%7D+%5Cright%29%7D%5E%7B%5Cbeta+-+1%7D%7Dd%5Cmu+%7D+%5C%5C+%5Cfrac%7B%7B%5CGamma+%5Cleft%28+%5Calpha+%5Cright%29%5CGamma+%5Cleft%28+%5Cbeta+%5Cright%29%7D%7D%7B%7B%5CGamma+%5Cleft%28+%7B%5Calpha+%2B+%5Cbeta+%7D+%5Cright%29%7D%7D+%26%3D+%5Cint_0%5E1+%7B%7B%5Cmu+%5E%7B%5Calpha+-+1%7D%7D%7B%7B%5Cleft%28+%7B1+-+%5Cmu+%7D+%5Cright%29%7D%5E%7B%5Cbeta+-+1%7D%7Dd%5Cmu+%7D+%5Ctag%7B22%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} \\Gamma \\left( \\alpha \\right)\\Gamma \\left( \\beta \\right) &amp;= \\int_0^\\infty {\\int_0^1 {{{\\left( {\\mu t} \\right)}^{\\alpha - 1}}{{\\left( {t - \\mu t} \\right)}^{\\beta - 1}}{e^{ - t}}d\\mu tdt} } \\\\ &amp; = \\int_0^\\infty {{t^{\\alpha + \\beta - 1}}{e^{ - t}}dt} \\int_0^1 {{\\mu ^{\\alpha - 1}}{{\\left( {1 - \\mu } \\right)}^{\\beta - 1}}d\\mu } \\\\ &amp; = \\Gamma \\left( {\\alpha + \\beta } \\right)\\int_0^1 {{\\mu ^{\\alpha - 1}}{{\\left( {1 - \\mu } \\right)}^{\\beta - 1}}d\\mu } \\\\ \\frac{{\\Gamma \\left( \\alpha \\right)\\Gamma \\left( \\beta \\right)}}{{\\Gamma \\left( {\\alpha + \\beta } \\right)}} &amp;= \\int_0^1 {{\\mu ^{\\alpha - 1}}{{\\left( {1 - \\mu } \\right)}^{\\beta - 1}}d\\mu } \\tag{22} \\end{align*}\" eeimg=\"1\"/> </p><p>2）期望 <img src=\"https://www.zhihu.com/equation?tex=E%5Cleft%28+p+%5Cright%29+%3D+%5Cfrac%7B%5Calpha+%7D%7B%7B%5Calpha+%2B+%5Cbeta+%7D%7D\" alt=\"E\\left( p \\right) = \\frac{\\alpha }{{\\alpha + \\beta }}\" eeimg=\"1\"/> </p><p>证明：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+E%5Cleft%28+p+%5Cright%29+%26%3D+%5Cint_0%5E1+%7Bp%5Cfrac%7B%7B%5CGamma+%5Cleft%28+%7B%5Calpha+%2B+%5Cbeta+%7D+%5Cright%29%7D%7D%7B%7B%5CGamma+%5Cleft%28+%5Calpha+%5Cright%29%5CGamma+%5Cleft%28+%5Cbeta+%5Cright%29%7D%7D%7Bp%5E%7B%5Calpha+-+1%7D%7D%7B%7B%5Cleft%28+%7B1+-+p%7D+%5Cright%29%7D%5E%7B%5Cbeta+-+1%7D%7Ddp%7D+%5C%5C+%26+%3D+%5Cfrac%7B%7B%5CGamma+%5Cleft%28+%7B%5Calpha+%2B+%5Cbeta+%7D+%5Cright%29%7D%7D%7B%7B%5CGamma+%5Cleft%28+%5Calpha+%5Cright%29%5CGamma+%5Cleft%28+%5Cbeta+%5Cright%29%7D%7D%5Cint_0%5E1+%7B%7Bp%5E%5Calpha+%7D%7B%7B%5Cleft%28+%7B1+-+p%7D+%5Cright%29%7D%5E%7B%5Cbeta+-+1%7D%7Ddp%7D+%5C%5C+%26+%3D+%5Cfrac%7B%7B%5CGamma+%5Cleft%28+%7B%5Calpha+%2B+%5Cbeta+%7D+%5Cright%29%7D%7D%7B%7B%5CGamma+%5Cleft%28+%5Calpha+%5Cright%29%5CGamma+%5Cleft%28+%5Cbeta+%5Cright%29%7D%7D%5Cfrac%7B%7B%5CGamma+%5Cleft%28+%7B%5Calpha+%7B%5Crm%7B+%2B+%7D%7D1%7D+%5Cright%29%5CGamma+%5Cleft%28+%5Cbeta+%5Cright%29%7D%7D%7B%7B%5CGamma+%5Cleft%28+%7B%5Calpha+%2B+%5Cbeta+%7B%5Crm%7B+%2B+%7D%7D1%7D+%5Cright%29%7D%7D%5Cint_0%5E1+%7B%5Cfrac%7B%7B%5CGamma+%5Cleft%28+%7B%5Calpha+%2B+%5Cbeta+%7B%5Crm%7B+%2B+%7D%7D1%7D+%5Cright%29%7D%7D%7B%7B%5CGamma+%5Cleft%28+%7B%5Calpha+%7B%5Crm%7B+%2B+%7D%7D1%7D+%5Cright%29%5CGamma+%5Cleft%28+%5Cbeta+%5Cright%29%7D%7D%7Bp%5E%5Calpha+%7D%7B%7B%5Cleft%28+%7B1+-+p%7D+%5Cright%29%7D%5E%7B%5Cbeta+-+1%7D%7Ddp%7D+%5C%5C+%26+%3D+%5Cfrac%7B%7B%5CGamma+%5Cleft%28+%7B%5Calpha+%2B+%5Cbeta+%7D+%5Cright%29%7D%7D%7B%7B%5CGamma+%5Cleft%28+%5Calpha+%5Cright%29%5CGamma+%5Cleft%28+%5Cbeta+%5Cright%29%7D%7D%5Cfrac%7B%7B%5CGamma+%5Cleft%28+%7B%5Calpha+%7B%5Crm%7B+%2B+%7D%7D1%7D+%5Cright%29%5CGamma+%5Cleft%28+%5Cbeta+%5Cright%29%7D%7D%7B%7B%5CGamma+%5Cleft%28+%7B%5Calpha+%2B+%5Cbeta+%7B%5Crm%7B+%2B+%7D%7D1%7D+%5Cright%29%7D%7D%5Cint_0%5E1+%7BBeta%5Cleft%28+%7Bp%7C%5Calpha+%2B+1%2C%5Cbeta+%7D+%5Cright%29dp%7D+%5C%5C+%26%3D+%5Cfrac%7B%7B%5CGamma+%5Cleft%28+%7B%5Calpha+%2B+%5Cbeta+%7D+%5Cright%29%7D%7D%7B%7B%5CGamma+%5Cleft%28+%5Calpha+%5Cright%29%5CGamma+%5Cleft%28+%5Cbeta+%5Cright%29%7D%7D%5Cfrac%7B%7B%5Calpha+%5CGamma+%5Cleft%28+%5Calpha+%5Cright%29%5CGamma+%5Cleft%28+%5Cbeta+%5Cright%29%7D%7D%7B%7B%5Cleft%28+%7B%5Calpha+%2B+%5Cbeta+%7D+%5Cright%29%5CGamma+%5Cleft%28+%7B%5Calpha+%2B+%5Cbeta+%7D+%5Cright%29%7D%7D%5C%5C+%26%3D+%5Cfrac%7B%5Calpha+%7D%7B%7B%5Calpha+%2B+%5Cbeta+%7D%7D+%5Ctag%7B23%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} E\\left( p \\right) &amp;= \\int_0^1 {p\\frac{{\\Gamma \\left( {\\alpha + \\beta } \\right)}}{{\\Gamma \\left( \\alpha \\right)\\Gamma \\left( \\beta \\right)}}{p^{\\alpha - 1}}{{\\left( {1 - p} \\right)}^{\\beta - 1}}dp} \\\\ &amp; = \\frac{{\\Gamma \\left( {\\alpha + \\beta } \\right)}}{{\\Gamma \\left( \\alpha \\right)\\Gamma \\left( \\beta \\right)}}\\int_0^1 {{p^\\alpha }{{\\left( {1 - p} \\right)}^{\\beta - 1}}dp} \\\\ &amp; = \\frac{{\\Gamma \\left( {\\alpha + \\beta } \\right)}}{{\\Gamma \\left( \\alpha \\right)\\Gamma \\left( \\beta \\right)}}\\frac{{\\Gamma \\left( {\\alpha {\\rm{ + }}1} \\right)\\Gamma \\left( \\beta \\right)}}{{\\Gamma \\left( {\\alpha + \\beta {\\rm{ + }}1} \\right)}}\\int_0^1 {\\frac{{\\Gamma \\left( {\\alpha + \\beta {\\rm{ + }}1} \\right)}}{{\\Gamma \\left( {\\alpha {\\rm{ + }}1} \\right)\\Gamma \\left( \\beta \\right)}}{p^\\alpha }{{\\left( {1 - p} \\right)}^{\\beta - 1}}dp} \\\\ &amp; = \\frac{{\\Gamma \\left( {\\alpha + \\beta } \\right)}}{{\\Gamma \\left( \\alpha \\right)\\Gamma \\left( \\beta \\right)}}\\frac{{\\Gamma \\left( {\\alpha {\\rm{ + }}1} \\right)\\Gamma \\left( \\beta \\right)}}{{\\Gamma \\left( {\\alpha + \\beta {\\rm{ + }}1} \\right)}}\\int_0^1 {Beta\\left( {p|\\alpha + 1,\\beta } \\right)dp} \\\\ &amp;= \\frac{{\\Gamma \\left( {\\alpha + \\beta } \\right)}}{{\\Gamma \\left( \\alpha \\right)\\Gamma \\left( \\beta \\right)}}\\frac{{\\alpha \\Gamma \\left( \\alpha \\right)\\Gamma \\left( \\beta \\right)}}{{\\left( {\\alpha + \\beta } \\right)\\Gamma \\left( {\\alpha + \\beta } \\right)}}\\\\ &amp;= \\frac{\\alpha }{{\\alpha + \\beta }} \\tag{23} \\end{align*}\" eeimg=\"1\"/> </p><h2><b>1.5 多项式分布</b></h2><p>多项式分布是二项式分布的推广，二项式分布做 <img src=\"https://www.zhihu.com/equation?tex=+n+\" alt=\" n \" eeimg=\"1\"/> 次伯努利试验，规定每次试验的结果只有两个，而多项式分布在 <img src=\"https://www.zhihu.com/equation?tex=+N+\" alt=\" N \" eeimg=\"1\"/> 次独立试验中结果有 <img src=\"https://www.zhihu.com/equation?tex=+K+\" alt=\" K \" eeimg=\"1\"/> 种，且每种结果都有一个确定的概率 <img src=\"https://www.zhihu.com/equation?tex=+p\" alt=\" p\" eeimg=\"1\"/> ，仍骰子是典型的多项式分布。</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+Mult%5Cleft%28+%7B%5Cvec+n%7C%5Cvec+p%2CN%7D+%5Cright%29+%3D+%5Cleft%28+%7B%5Cbegin%7Barray%7D%7B%2A%7B20%7D%7Bc%7D%7D+N%5C%5C+%7B%5Cvec+n%7D+%5Cend%7Barray%7D%7D+%5Cright%29%5Cprod%5Climits_%7Bk+%3D+1%7D%5EK+%7Bp_k%5E%7B%7Bn_k%7D%7D%7D+%5Ctag%7B24%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} Mult\\left( {\\vec n|\\vec p,N} \\right) = \\left( {\\begin{array}{*{20}{c}} N\\\\ {\\vec n} \\end{array}} \\right)\\prod\\limits_{k = 1}^K {p_k^{{n_k}}} \\tag{24} \\end{align*}\" eeimg=\"1\"/> </p><p>其中 <img src=\"https://www.zhihu.com/equation?tex=%5Csum%5Climits_%7Bk+%3D+1%7D%5EK+%7B%7Bn_k%7D%7D+%3D+N%2C%5Csum%5Climits_%7Bk+%3D+1%7D%5EK+%7B%7Bp_k%7D%7D+%3D+1%5Cleft%28+%7B%5Cbegin%7Barray%7D%7B%2A%7B20%7D%7Bc%7D%7D+N%5C%5C+%7B%5Cvec+n%7D+%5Cend%7Barray%7D%7D+%5Cright%29+%3D+%5Cfrac%7B%7BN%21%7D%7D%7B%7B%5Cprod%5Cnolimits_k+%7B%7Bn_k%7D%21%7D+%7D%7D\" alt=\"\\sum\\limits_{k = 1}^K {{n_k}} = N,\\sum\\limits_{k = 1}^K {{p_k}} = 1\\left( {\\begin{array}{*{20}{c}} N\\\\ {\\vec n} \\end{array}} \\right) = \\frac{{N!}}{{\\prod\\nolimits_k {{n_k}!} }}\" eeimg=\"1\"/> 。</p><h2><b>1.6 Dirichlet分布</b></h2><p>Dirichlet 分布是 Beta 分布在高维度上的推广，概率密度函数是：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+Dir%5Cleft%28+%7B%5Cvec+p%7C%5Cvec+%5Calpha+%7D+%5Cright%29+%26%3D+%5Cfrac%7B%7B%5CGamma+%5Cleft%28+%7B%5Csum%5Cnolimits_%7Bk+%3D+1%7D%5EK+%7B%7B%5Calpha+_k%7D%7D+%7D+%5Cright%29%7D%7D%7B%7B%5Cprod%5Cnolimits_%7Bk+%3D+1%7D%5EK+%7B%5CGamma+%5Cleft%28+%7B%7B%5Calpha+_k%7D%7D+%5Cright%29%7D+%7D%7D%5Cprod%5Climits_%7Bk+%3D+1%7D%5EK+%7Bp_k%5E%7B%7B%5Calpha+_k%7D+-+1%7D%7D+%5C%5C+%26%3D+%5Cfrac%7B1%7D%7B%7B%5CDelta+%5Cleft%28+%7B%5Cvec+%5Calpha+%7D+%5Cright%29%7D%7D%5Cprod%5Climits_%7Bk+%3D+1%7D%5EK+%7Bp_k%5E%7B%7B%5Calpha+_k%7D+-+1%7D%7D+%5Ctag%7B25%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} Dir\\left( {\\vec p|\\vec \\alpha } \\right) &amp;= \\frac{{\\Gamma \\left( {\\sum\\nolimits_{k = 1}^K {{\\alpha _k}} } \\right)}}{{\\prod\\nolimits_{k = 1}^K {\\Gamma \\left( {{\\alpha _k}} \\right)} }}\\prod\\limits_{k = 1}^K {p_k^{{\\alpha _k} - 1}} \\\\ &amp;= \\frac{1}{{\\Delta \\left( {\\vec \\alpha } \\right)}}\\prod\\limits_{k = 1}^K {p_k^{{\\alpha _k} - 1}} \\tag{25} \\end{align*}\" eeimg=\"1\"/> </p><p>1） <img src=\"https://www.zhihu.com/equation?tex=%5CDelta+%5Cleft%28+%7B%5Cvec+%5Calpha+%7D+%5Cright%29%7B%5Crm%7B+%3D+%7D%7D%5Cfrac%7B%7B%5Cprod%5Cnolimits_%7Bk+%3D+1%7D%5EK+%7B%5CGamma+%5Cleft%28+%7B%7B%5Calpha+_k%7D%7D+%5Cright%29%7D+%7D%7D%7B%7B%5CGamma+%5Cleft%28+%7B%5Csum%5Cnolimits_%7Bk+%3D+1%7D%5EK+%7B%7B%5Calpha+_k%7D%7D+%7D+%5Cright%29%7D%7D%7B%5Crm%7B+%3D+%7D%7D%5Cint_0%5E1+%7B%5Cprod%5Climits_%7Bk+%3D+1%7D%5EK+%7Bp_k%5E%7B%7B%5Calpha+_k%7D+-+1%7D%7D+d%5Cvec+p%7D+\" alt=\"\\Delta \\left( {\\vec \\alpha } \\right){\\rm{ = }}\\frac{{\\prod\\nolimits_{k = 1}^K {\\Gamma \\left( {{\\alpha _k}} \\right)} }}{{\\Gamma \\left( {\\sum\\nolimits_{k = 1}^K {{\\alpha _k}} } \\right)}}{\\rm{ = }}\\int_0^1 {\\prod\\limits_{k = 1}^K {p_k^{{\\alpha _k} - 1}} d\\vec p} \" eeimg=\"1\"/> </p><p class=\"ztext-empty-paragraph\"><br/></p><p>2）期望 <img src=\"https://www.zhihu.com/equation?tex=E%5Cleft%28+%7B%5Cvec+p%7D+%5Cright%29+%3D+%5Cleft%28+%7B%5Cfrac%7B%7B%7B%5Calpha+_1%7D%7D%7D%7B%7B%5Csum%5Cnolimits_%7Bk+%3D+1%7D%5EK+%7B%7B%5Calpha+_k%7D%7D+%7D%7D%2C%5Cfrac%7B%7B%7B%5Calpha+_2%7D%7D%7D%7B%7B%5Csum%5Cnolimits_%7Bk+%3D+1%7D%5EK+%7B%7B%5Calpha+_k%7D%7D+%7D%7D%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2C%5Cfrac%7B%7B%7B%5Calpha+_K%7D%7D%7D%7B%7B%5Csum%5Cnolimits_%7Bk+%3D+1%7D%5EK+%7B%7B%5Calpha+_k%7D%7D+%7D%7D%7D+%5Cright%29\" alt=\"E\\left( {\\vec p} \\right) = \\left( {\\frac{{{\\alpha _1}}}{{\\sum\\nolimits_{k = 1}^K {{\\alpha _k}} }},\\frac{{{\\alpha _2}}}{{\\sum\\nolimits_{k = 1}^K {{\\alpha _k}} }}, \\cdot \\cdot \\cdot ,\\frac{{{\\alpha _K}}}{{\\sum\\nolimits_{k = 1}^K {{\\alpha _k}} }}} \\right)\" eeimg=\"1\"/> 。</p><h2><b>1.7 共轭先验分布</b></h2><p>在贝叶斯中，如果后验分布与先验分布属于同类分布，则先验分布与后验分布被称为共轭分布，而先验分布被称为似然函数的共轭先验。</p><p><b>1．Beta-Binomial共轭</b></p><p>1）先验分布</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+Beta%5Cleft%28+%7Bp%7C%5Calpha+%2C%5Cbeta+%7D+%5Cright%29+%3D+%5Cfrac%7B1%7D%7B%7BB%5Cleft%28+%7B%5Calpha+%2C%5Cbeta+%7D+%5Cright%29%7D%7D%7Bp%5E%7B%5Calpha+-+1%7D%7D%7B%5Cleft%28+%7B1+-+p%7D+%5Cright%29%5E%7B%5Cbeta+-+1%7D%7D+%5Ctag%7B26%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} Beta\\left( {p|\\alpha ,\\beta } \\right) = \\frac{1}{{B\\left( {\\alpha ,\\beta } \\right)}}{p^{\\alpha - 1}}{\\left( {1 - p} \\right)^{\\beta - 1}} \\tag{26} \\end{align*}\" eeimg=\"1\"/> </p><p>2）二项式似然函数</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+B%5Cleft%28+%7B%7Bn_1%7D%2C%7Bn_2%7D%7Cp%7D+%5Cright%29+%3D+%5Cleft%28+%7B%5Cbegin%7Barray%7D%7B%2A%7B20%7D%7Bc%7D%7D+n%5C%5C+%7B%7Bn_1%7D%7D+%5Cend%7Barray%7D%7D+%5Cright%29%7Bp%5E%7B%7Bn_1%7D%7D%7D%7B%5Cleft%28+%7B1+-+p%7D+%5Cright%29%5E%7B%7Bn_2%7D%7D%7D+%5Ctag%7B27%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} B\\left( {{n_1},{n_2}|p} \\right) = \\left( {\\begin{array}{*{20}{c}} n\\\\ {{n_1}} \\end{array}} \\right){p^{{n_1}}}{\\left( {1 - p} \\right)^{{n_2}}} \\tag{27} \\end{align*}\" eeimg=\"1\"/> </p><p>3）后验分布</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Cfrac%7B%7BB%5Cleft%28+%7B%7Bn_1%7D%2C%7Bn_2%7D%7Cp%7D+%5Cright%29Beta%5Cleft%28+%7Bp%7C%5Calpha+%2C%5Cbeta+%7D+%5Cright%29%7D%7D%7B%7B%5Cint_0%5E1+%7BB%5Cleft%28+%7B%7Bn_1%7D%2C%7Bn_2%7D%7Cp%7D+%5Cright%29Beta%5Cleft%28+%7Bp%7C%5Calpha+%2C%5Cbeta+%7D+%5Cright%29dp%7D+%7D%7D%26%3D+%5Cfrac%7B%7B%7Bp%5E%7B%5Calpha+%2B+%7Bn_1%7D+-+1%7D%7D%7B%7B%5Cleft%28+%7B1+-+p%7D+%5Cright%29%7D%5E%7B%5Cbeta+%2B+%7Bn_2%7D+-+1%7D%7D%7D%7D%7B%7B%5Cint_0%5E1+%7B%7Bp%5E%7B%5Calpha+%2B+%7Bn_1%7D+-+1%7D%7D%7B%7B%5Cleft%28+%7B1+-+p%7D+%5Cright%29%7D%5E%7B%5Cbeta+%2B+%7Bn_2%7D+-+1%7D%7Ddp%7D+%7D%7D%5C%5C+%26%3D+%5Cfrac%7B%7B%7Bp%5E%7B%5Calpha+%2B+%7Bn_1%7D+-+1%7D%7D%7B%7B%5Cleft%28+%7B1+-+p%7D+%5Cright%29%7D%5E%7B%5Cbeta+%2B+%7Bn_2%7D+-+1%7D%7D%7D%7D%7B%7BB%5Cleft%28+%7B%5Calpha+%7B%5Crm%7B+%2B+%7D%7D%7Bn_1%7D%2C%5Cbeta+%2B+%7Bn_2%7D%7D+%5Cright%29%7D%7D+%5Csim+Beta%5Cleft%28+%7B%5Calpha+%7B%5Crm%7B+%2B+%7D%7D%7Bn_1%7D%2C%5Cbeta+%2B+%7Bn_2%7D%7D+%5Cright%29+%5Ctag%7B28%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} \\frac{{B\\left( {{n_1},{n_2}|p} \\right)Beta\\left( {p|\\alpha ,\\beta } \\right)}}{{\\int_0^1 {B\\left( {{n_1},{n_2}|p} \\right)Beta\\left( {p|\\alpha ,\\beta } \\right)dp} }}&amp;= \\frac{{{p^{\\alpha + {n_1} - 1}}{{\\left( {1 - p} \\right)}^{\\beta + {n_2} - 1}}}}{{\\int_0^1 {{p^{\\alpha + {n_1} - 1}}{{\\left( {1 - p} \\right)}^{\\beta + {n_2} - 1}}dp} }}\\\\ &amp;= \\frac{{{p^{\\alpha + {n_1} - 1}}{{\\left( {1 - p} \\right)}^{\\beta + {n_2} - 1}}}}{{B\\left( {\\alpha {\\rm{ + }}{n_1},\\beta + {n_2}} \\right)}} \\sim Beta\\left( {\\alpha {\\rm{ + }}{n_1},\\beta + {n_2}} \\right) \\tag{28} \\end{align*}\" eeimg=\"1\"/> </p><p>即可以表达为 <img src=\"https://www.zhihu.com/equation?tex=Beta%5Cleft%28+%7Bp%7C%5Calpha+%2C%5Cbeta+%7D+%5Cright%29%7B%5Crm%7B+%2B+%7D%7DB%5Cleft%28+%7B%7Bn_1%7D%2C%7Bn_2%7D%7Cp%7D+%5Cright%29%7B%5Crm%7B+%3D+%7D%7DBeta%5Cleft%28+%7Bp%7C%5Calpha+%7B%5Crm%7B+%2B+%7D%7D%7Bn_1%7D%2C%5Cbeta+%2B+%7Bn_2%7D%7D+%5Cright%29\" alt=\"Beta\\left( {p|\\alpha ,\\beta } \\right){\\rm{ + }}B\\left( {{n_1},{n_2}|p} \\right){\\rm{ = }}Beta\\left( {p|\\alpha {\\rm{ + }}{n_1},\\beta + {n_2}} \\right)\" eeimg=\"1\"/> ，取一个特殊情况理解<img src=\"https://www.zhihu.com/equation?tex=Beta%5Cleft%28+%7Bp%7C1%2C1%7D+%5Cright%29%7B%5Crm%7B+%2B+%7D%7DB%5Cleft%28+%7B%5Calpha+-+1%2C%5Cbeta+-+1%7Cp%7D+%5Cright%29%7B%5Crm%7B+%3D+%7D%7DBeta%5Cleft%28+%7Bp%7C%5Calpha+%2C%5Cbeta+%7D+%5Cright%29%EF%BC%8CBeta%5Cleft%28+%7Bp%7C1%2C1%7D+%5Cright%29+\" alt=\"Beta\\left( {p|1,1} \\right){\\rm{ + }}B\\left( {\\alpha - 1,\\beta - 1|p} \\right){\\rm{ = }}Beta\\left( {p|\\alpha ,\\beta } \\right)，Beta\\left( {p|1,1} \\right) \" eeimg=\"1\"/> 恰好是均匀分布 <img src=\"https://www.zhihu.com/equation?tex=uniform%5Cleft%28+%7B0%2C1%7D+%5Cright%29\" alt=\"uniform\\left( {0,1} \\right)\" eeimg=\"1\"/> ，假设有一个不均匀的硬币抛出正面的概率为 <img src=\"https://www.zhihu.com/equation?tex=+p\" alt=\" p\" eeimg=\"1\"/> ，抛出 <img src=\"https://www.zhihu.com/equation?tex=+n+\" alt=\" n \" eeimg=\"1\"/> 次后出现正面和反面的次数分别是 <img src=\"https://www.zhihu.com/equation?tex=+%7Bn_1%7D+\" alt=\" {n_1} \" eeimg=\"1\"/>和 <img src=\"https://www.zhihu.com/equation?tex=+%7Bn_2%7D+\" alt=\" {n_2} \" eeimg=\"1\"/> ，开始我们对硬币不均匀性一无所知，所以应该假设 <img src=\"https://www.zhihu.com/equation?tex=+p+%5Csim+uniform%5Cleft%28+%7B0%2C1%7D+%5Cright%29+\" alt=\" p \\sim uniform\\left( {0,1} \\right) \" eeimg=\"1\"/> ，当有了试验样本，我们加入样本信息对 <img src=\"https://www.zhihu.com/equation?tex=+p+\" alt=\" p \" eeimg=\"1\"/> 的分布进行修正, <img src=\"https://www.zhihu.com/equation?tex=p+\" alt=\"p \" eeimg=\"1\"/> 的分布由均匀分布变为 Beta 分布。</p><p><b>2．Dirichlet-Multinomial共轭</b></p><p>1）先验分布</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+Dir%5Cleft%28+%7B%5Cvec+p%7C%5Cvec+%5Calpha+%7D+%5Cright%29+%3D+%5Cfrac%7B1%7D%7B%7B%5CDelta+%5Cleft%28+%7B%5Cvec+%5Calpha+%7D+%5Cright%29%7D%7D%5Cprod%5Climits_%7Bk+%3D+1%7D%5EK+%7Bp_k%5E%7B%7B%5Calpha+_k%7D+-+1%7D%7D+%5Ctag%7B29%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} Dir\\left( {\\vec p|\\vec \\alpha } \\right) = \\frac{1}{{\\Delta \\left( {\\vec \\alpha } \\right)}}\\prod\\limits_{k = 1}^K {p_k^{{\\alpha _k} - 1}} \\tag{29} \\end{align*}\" eeimg=\"1\"/> </p><p>2）多项分布似然函数</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+Mult%5Cleft%28+%7B%5Cvec+n%7C%5Cvec+p%2CN%7D+%5Cright%29+%3D+%5Cleft%28+%7B%5Cbegin%7Barray%7D%7B%2A%7B20%7D%7Bc%7D%7D+N%5C%5C+%7B%5Cvec+n%7D+%5Cend%7Barray%7D%7D+%5Cright%29%5Cprod%5Climits_%7Bk+%3D+1%7D%5EK+%7Bp_k%5E%7B%7Bn_k%7D%7D%7D+%5Ctag%7B30%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} Mult\\left( {\\vec n|\\vec p,N} \\right) = \\left( {\\begin{array}{*{20}{c}} N\\\\ {\\vec n} \\end{array}} \\right)\\prod\\limits_{k = 1}^K {p_k^{{n_k}}} \\tag{30} \\end{align*}\" eeimg=\"1\"/> </p><p>3）后验分布</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Cfrac%7B%7BDir%5Cleft%28+%7B%5Cvec+p%7C%5Cvec+%5Calpha+%7D+%5Cright%29Mult%5Cleft%28+%7B%5Cvec+n%7C%5Cvec+p%2CN%7D+%5Cright%29%7D%7D%7B%7B%5Cint_0%5E1+%7BDir%5Cleft%28+%7B%5Cvec+p%7C%5Cvec+%5Calpha+%7D+%5Cright%29Mult%5Cleft%28+%7B%5Cvec+n%7C%5Cvec+p%2CN%7D+%5Cright%29d%5Cvec+p%7D+%7D%7D%26%3D+%5Cfrac%7B%7B%5Cprod%5Climits_%7Bk+%3D+1%7D%5EK+%7Bp_k%5E%7B%7B%5Calpha+_k%7D+%2B+%7Bn_k%7D+-+1%7D%7D+%7D%7D%7B%7B%5Cint_0%5E1+%7B%5Cprod%5Climits_%7Bk+%3D+1%7D%5EK+%7Bp_k%5E%7B%7B%5Calpha+_k%7D+%2B+%7Bn_k%7D+-+1%7D%7D+d%5Cvec+p%7D+%7D%7D%5C%5C+%26%3D+%5Cfrac%7B%7B%5Cprod%5Climits_%7Bk+%3D+1%7D%5EK+%7Bp_k%5E%7B%7B%5Calpha+_k%7D+%2B+%7Bn_k%7D+-+1%7D%7D+%7D%7D%7B%7B%5CDelta+%5Cleft%28+%7B%5Cvec+%5Calpha+%2B+%5Cvec+n%7D+%5Cright%29%7D%7D+%5Csim+Dir%5Cleft%28+%7B%5Cvec+p%7C%5Cvec+%5Calpha+%2B+%5Cvec+n%7D+%5Cright%29+%5Ctag%7B31%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} \\frac{{Dir\\left( {\\vec p|\\vec \\alpha } \\right)Mult\\left( {\\vec n|\\vec p,N} \\right)}}{{\\int_0^1 {Dir\\left( {\\vec p|\\vec \\alpha } \\right)Mult\\left( {\\vec n|\\vec p,N} \\right)d\\vec p} }}&amp;= \\frac{{\\prod\\limits_{k = 1}^K {p_k^{{\\alpha _k} + {n_k} - 1}} }}{{\\int_0^1 {\\prod\\limits_{k = 1}^K {p_k^{{\\alpha _k} + {n_k} - 1}} d\\vec p} }}\\\\ &amp;= \\frac{{\\prod\\limits_{k = 1}^K {p_k^{{\\alpha _k} + {n_k} - 1}} }}{{\\Delta \\left( {\\vec \\alpha + \\vec n} \\right)}} \\sim Dir\\left( {\\vec p|\\vec \\alpha + \\vec n} \\right) \\tag{31} \\end{align*}\" eeimg=\"1\"/> </p><p>即可以表达为 <img src=\"https://www.zhihu.com/equation?tex=Dir%5Cleft%28+%7B%5Cvec+p%7C%5Cvec+%5Calpha+%7D+%5Cright%29%7B%5Crm%7B+%2B+%7D%7DMult%5Cleft%28+%7B%5Cvec+n%7C%5Cvec+p%2CN%7D+%5Cright%29%7B%5Crm%7B+%3D+%7D%7DDir%5Cleft%28+%7B%5Cvec+p%7C%5Cvec+%5Calpha+%7B%5Crm%7B+%2B+%7D%7D%5Cvec+n%7D+%5Cright%29\" alt=\"Dir\\left( {\\vec p|\\vec \\alpha } \\right){\\rm{ + }}Mult\\left( {\\vec n|\\vec p,N} \\right){\\rm{ = }}Dir\\left( {\\vec p|\\vec \\alpha {\\rm{ + }}\\vec n} \\right)\" eeimg=\"1\"/> </p><h2><b>1.8 马氏链及其平稳分布</b></h2><p>马氏链的数学定义很简单，状态转移的概率只依赖于前一个状态。</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+P%5Cleft%28+%7B%7BX_%7Bt+%2B+1%7D%7D+%3D+x%7C%7BX_t%7D%2C%7BX_%7Bt+-+1%7D%7D%2C+%5Ccdot+%5Ccdot+%5Ccdot+%7D+%5Cright%29+%3D+P%5Cleft%28+%7B%7BX_%7Bt+%2B+1%7D%7D+%3D+x%7C%7BX_t%7D%7D+%5Cright%29+%5Ctag%7B32%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} P\\left( {{X_{t + 1}} = x|{X_t},{X_{t - 1}}, \\cdot \\cdot \\cdot } \\right) = P\\left( {{X_{t + 1}} = x|{X_t}} \\right) \\tag{32} \\end{align*}\" eeimg=\"1\"/> </p><p>看一个马氏链的具体例子，马氏链表示股市模型，共有三种状态：牛市(Bull market)、熊市(Bear market)、横盘(Stagnant market)，每一个转态都以一定的概率转化到下一个状态，如图1.1所示。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-23c682c973c408ccb96fd306c42700cb_b.jpg\" data-size=\"normal\" data-rawwidth=\"398\" data-rawheight=\"288\" class=\"content_image\" width=\"398\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;398&#39; height=&#39;288&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"398\" data-rawheight=\"288\" class=\"content_image lazy\" width=\"398\" data-actualsrc=\"https://pic4.zhimg.com/v2-23c682c973c408ccb96fd306c42700cb_b.jpg\"/><figcaption>图 1.1</figcaption></figure><p>这个概率转化图可以以矩阵的形式表示，如果我们定义矩阵 <img src=\"https://www.zhihu.com/equation?tex=+P+\" alt=\" P \" eeimg=\"1\"/> 某一位置 <img src=\"https://www.zhihu.com/equation?tex=+%5Cleft%28+%7Bi%2Cj%7D+%5Cright%29\" alt=\" \\left( {i,j} \\right)\" eeimg=\"1\"/> 的值为 <img src=\"https://www.zhihu.com/equation?tex=+P%5Cleft%28+%7Bj%7Ci%7D+%5Cright%29\" alt=\" P\\left( {j|i} \\right)\" eeimg=\"1\"/> ，表示从状态 <img src=\"https://www.zhihu.com/equation?tex=+i+\" alt=\" i \" eeimg=\"1\"/> 转化到状态 <img src=\"https://www.zhihu.com/equation?tex=+j+\" alt=\" j \" eeimg=\"1\"/> 的概率，这样我们可以得到马尔科夫链模型的状态转移矩阵为：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+P+%3D+%5Cleft%28+%7B%5Cbegin%7Barray%7D%7B%2A%7B20%7D%7Bc%7D%7D+%7B0.9%7D%26%7B0.075%7D%26%7B0.025%7D%5C%5C+%7B0.15%7D%26%7B0.8%7D%26%7B0.05%7D%5C%5C+%7B0.25%7D%26%7B0.25%7D%26%7B0.5%7D+%5Cend%7Barray%7D%7D+%5Cright%29+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} P = \\left( {\\begin{array}{*{20}{c}} {0.9}&amp;{0.075}&amp;{0.025}\\\\ {0.15}&amp;{0.8}&amp;{0.05}\\\\ {0.25}&amp;{0.25}&amp;{0.5} \\end{array}} \\right) \\end{align*}\" eeimg=\"1\"/> </p><p>假设初始概率分布为 <img src=\"https://www.zhihu.com/equation?tex=+%7B%5Cpi+_0%7D%7B%5Crm%7B+%3D+%7D%7D%5Cleft%5B+%7B%5Cbegin%7Barray%7D%7B%2A%7B20%7D%7Bc%7D%7D+%7B0.3%7D%26%7B0.4%7D%26%7B0.3%7D+%5Cend%7Barray%7D%7D+%5Cright%5D%EF%BC%8C%7B%5Cpi+_1%7D%7B%5Crm%7B+%3D+%7D%7D%7B%5Cpi+_0%7DP%2C%7B%5Cpi+_2%7D%7B%5Crm%7B+%3D+%7D%7D%7B%5Cpi+_1%7DP+%3D+%7B%5Cpi+_0%7D%7BP%5E2%7D%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2C%7B%5Cpi+_n%7D%7B%5Crm%7B+%3D+%7D%7D%7B%5Cpi+_%7Bn+-+1%7D%7DP+%3D+%7B%5Cpi+_0%7D%7BP%5En%7D\" alt=\" {\\pi _0}{\\rm{ = }}\\left[ {\\begin{array}{*{20}{c}} {0.3}&amp;{0.4}&amp;{0.3} \\end{array}} \\right]，{\\pi _1}{\\rm{ = }}{\\pi _0}P,{\\pi _2}{\\rm{ = }}{\\pi _1}P = {\\pi _0}{P^2}, \\cdot \\cdot \\cdot ,{\\pi _n}{\\rm{ = }}{\\pi _{n - 1}}P = {\\pi _0}{P^n}\" eeimg=\"1\"/> 。从第 <img src=\"https://www.zhihu.com/equation?tex=+60+\" alt=\" 60 \" eeimg=\"1\"/> 轮开始的 <img src=\"https://www.zhihu.com/equation?tex=+%7B%5Cpi+_%7B60%7D%7D%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2C%7B%5Cpi+_n%7D+\" alt=\" {\\pi _{60}}, \\cdot \\cdot \\cdot ,{\\pi _n} \" eeimg=\"1\"/> 值保持不变，为 <img src=\"https://www.zhihu.com/equation?tex=+%5Cleft%5B+%7B%5Cbegin%7Barray%7D%7B%2A%7B20%7D%7Bc%7D%7D+%7B0.625%7D%26%7B0.3125%7D%26%7B0.0625%7D+%5Cend%7Barray%7D%7D+%5Cright%5D+\" alt=\" \\left[ {\\begin{array}{*{20}{c}} {0.625}&amp;{0.3125}&amp;{0.0625} \\end{array}} \\right] \" eeimg=\"1\"/> 。我们更改初始概率， <img src=\"https://www.zhihu.com/equation?tex=%7B%5Cpi+_0%7D%7B%5Crm%7B+%3D+%7D%7D%5Cleft%5B+%7B%5Cbegin%7Barray%7D%7B%2A%7B20%7D%7Bc%7D%7D+%7B0.7%7D%26%7B0.2%7D%26%7B0.1%7D+%5Cend%7Barray%7D%7D+%5Cright%5D\" alt=\"{\\pi _0}{\\rm{ = }}\\left[ {\\begin{array}{*{20}{c}} {0.7}&amp;{0.2}&amp;{0.1} \\end{array}} \\right]\" eeimg=\"1\"/> ，从 <img src=\"https://www.zhihu.com/equation?tex=55\" alt=\"55\" eeimg=\"1\"/> 轮开始 <img src=\"https://www.zhihu.com/equation?tex=%7B%5Cpi+_%7B55%7D%7D%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2C%7B%5Cpi+_n%7D+\" alt=\"{\\pi _{55}}, \\cdot \\cdot \\cdot ,{\\pi _n} \" eeimg=\"1\"/> 的值保持不变，为 <img src=\"https://www.zhihu.com/equation?tex=%5Cleft%5B+%7B%5Cbegin%7Barray%7D%7B%2A%7B20%7D%7Bc%7D%7D+%7B0.625%7D%26%7B0.3125%7D%26%7B0.0625%7D+%5Cend%7Barray%7D%7D+%5Cright%5D\" alt=\"\\left[ {\\begin{array}{*{20}{c}} {0.625}&amp;{0.3125}&amp;{0.0625} \\end{array}} \\right]\" eeimg=\"1\"/> 。两次给定不同的初始概率分布，最终都收敛到概率分布 <img src=\"https://www.zhihu.com/equation?tex=+%5Cpi+%7B%5Crm%7B+%3D+%7D%7D%5Cleft%5B+%7B%5Cbegin%7Barray%7D%7B%2A%7B20%7D%7Bc%7D%7D+%7B0.625%7D%26%7B0.3125%7D%26%7B0.0625%7D+%5Cend%7Barray%7D%7D+%5Cright%5D+\" alt=\" \\pi {\\rm{ = }}\\left[ {\\begin{array}{*{20}{c}} {0.625}&amp;{0.3125}&amp;{0.0625} \\end{array}} \\right] \" eeimg=\"1\"/> ，也就是说收敛的行为和初始概率分布 <img src=\"https://www.zhihu.com/equation?tex=+%7B%5Cpi+_0%7D+\" alt=\" {\\pi _0} \" eeimg=\"1\"/> 无关，这个收敛的行为主要是由概率转移矩阵 <img src=\"https://www.zhihu.com/equation?tex=+P+\" alt=\" P \" eeimg=\"1\"/> 决定的，可以计算下 <img src=\"https://www.zhihu.com/equation?tex=+%7BP%5En%7D\" alt=\" {P^n}\" eeimg=\"1\"/> 。</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%7BP%5E%7B63%7D%7D+%3D+%7BP%5E%7B64%7D%7D+%3D+%5Ccdot+%5Ccdot+%5Ccdot+%3D+%5Cleft%5B+%7B%5Cbegin%7Barray%7D%7B%2A%7B20%7D%7Bc%7D%7D+%7B0.625%7D%26%7B0.3125%7D%26%7B0.0625%7D%5C%5C+%7B0.625%7D%26%7B0.3125%7D%26%7B0.0625%7D%5C%5C+%7B0.625%7D%26%7B0.3125%7D%26%7B0.0625%7D+%5Cend%7Barray%7D%7D+%5Cright%5D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} {P^{63}} = {P^{64}} = \\cdot \\cdot \\cdot = \\left[ {\\begin{array}{*{20}{c}} {0.625}&amp;{0.3125}&amp;{0.0625}\\\\ {0.625}&amp;{0.3125}&amp;{0.0625}\\\\ {0.625}&amp;{0.3125}&amp;{0.0625} \\end{array}} \\right] \\end{align*}\" eeimg=\"1\"/> </p><p>当 <img src=\"https://www.zhihu.com/equation?tex=+n+\" alt=\" n \" eeimg=\"1\"/> 足够大的时候， <img src=\"https://www.zhihu.com/equation?tex=%7BP%5En%7D+\" alt=\"{P^n} \" eeimg=\"1\"/> 矩阵的每一行都是稳定地收敛到 <img src=\"https://www.zhihu.com/equation?tex=+%5Cpi+%7B%5Crm%7B+%3D+%7D%7D%5Cleft%5B+%7B%5Cbegin%7Barray%7D%7B%2A%7B20%7D%7Bc%7D%7D+%7B0.625%7D%26%7B0.3125%7D%26%7B0.0625%7D+%5Cend%7Barray%7D%7D+%5Cright%5D+\" alt=\" \\pi {\\rm{ = }}\\left[ {\\begin{array}{*{20}{c}} {0.625}&amp;{0.3125}&amp;{0.0625} \\end{array}} \\right] \" eeimg=\"1\"/> 这个概率分布。这个收敛现象并不是这个马氏链独有的，而是绝大多数马氏链独有的。关于马氏链的收敛有如下定理：</p><p><b>定理1.1 </b>如果一个非周期马氏链具有转移概率矩阵 <img src=\"https://www.zhihu.com/equation?tex=+P\" alt=\" P\" eeimg=\"1\"/> ，且它的任何两个状态是连通的，那么 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathop+%7B%5Clim+%7D%5Climits_%7Bn+%5Cto+%5Cinfty+%7D+P_%7Bij%7D%5En+\" alt=\"\\mathop {\\lim }\\limits_{n \\to \\infty } P_{ij}^n \" eeimg=\"1\"/> 存在且与 <img src=\"https://www.zhihu.com/equation?tex=+i+\" alt=\" i \" eeimg=\"1\"/> 无关，我们有：</p><p>1） <img src=\"https://www.zhihu.com/equation?tex=%5Cmathop+%7B%5Clim+%7D%5Climits_%7Bn+%5Cto+%5Cinfty+%7D+P_%7Bij%7D%5En%7B%5Crm%7B+%3D+%7D%7D%5Cpi+%5Cleft%28+j+%5Cright%29\" alt=\"\\mathop {\\lim }\\limits_{n \\to \\infty } P_{ij}^n{\\rm{ = }}\\pi \\left( j \\right)\" eeimg=\"1\"/> </p><p>2） <img src=\"https://www.zhihu.com/equation?tex=%5Cmathop+%7B%5Clim+%7D%5Climits_%7Bn+%5Cto+%5Cinfty+%7D+%7BP%5En%7D+%3D+%5Cleft%5B+%7B%5Cbegin%7Barray%7D%7B%2A%7B20%7D%7Bc%7D%7D+%7B%5Cpi+%5Cleft%28+1+%5Cright%29%7D%26%7B%5Cpi+%5Cleft%28+2+%5Cright%29%7D%26%7B+%5Ccdot+%5Ccdot+%5Ccdot+%7D%26%7B%5Cpi+%5Cleft%28+j+%5Cright%29%7D%26%7B+%5Ccdot+%5Ccdot+%5Ccdot+%7D%5C%5C+%7B%5Cpi+%5Cleft%28+1+%5Cright%29%7D%26%7B%5Cpi+%5Cleft%28+2+%5Cright%29%7D%26%7B+%5Ccdot+%5Ccdot+%5Ccdot+%7D%26%7B%5Cpi+%5Cleft%28+j+%5Cright%29%7D%26%7B+%5Ccdot+%5Ccdot+%5Ccdot+%7D%5C%5C+%7B+%5Ccdot+%5Ccdot+%5Ccdot+%7D%26%7B+%5Ccdot+%5Ccdot+%5Ccdot+%7D%26%7B+%5Ccdot+%5Ccdot+%5Ccdot+%7D%26%7B+%5Ccdot+%5Ccdot+%5Ccdot+%7D%26%7B+%5Ccdot+%5Ccdot+%5Ccdot+%7D%5C%5C+%7B%5Cpi+%5Cleft%28+1+%5Cright%29%7D%26%7B%5Cpi+%5Cleft%28+2+%5Cright%29%7D%26%7B+%5Ccdot+%5Ccdot+%5Ccdot+%7D%26%7B%5Cpi+%5Cleft%28+j+%5Cright%29%7D%26%7B+%5Ccdot+%5Ccdot+%5Ccdot+%7D%5C%5C+%7B+%5Ccdot+%5Ccdot+%5Ccdot+%7D%26%7B+%5Ccdot+%5Ccdot+%5Ccdot+%7D%26%7B+%5Ccdot+%5Ccdot+%5Ccdot+%7D%26%7B+%5Ccdot+%5Ccdot+%5Ccdot+%7D%26%7B+%5Ccdot+%5Ccdot+%5Ccdot+%7D+%5Cend%7Barray%7D%7D+%5Cright%5D\" alt=\"\\mathop {\\lim }\\limits_{n \\to \\infty } {P^n} = \\left[ {\\begin{array}{*{20}{c}} {\\pi \\left( 1 \\right)}&amp;{\\pi \\left( 2 \\right)}&amp;{ \\cdot \\cdot \\cdot }&amp;{\\pi \\left( j \\right)}&amp;{ \\cdot \\cdot \\cdot }\\\\ {\\pi \\left( 1 \\right)}&amp;{\\pi \\left( 2 \\right)}&amp;{ \\cdot \\cdot \\cdot }&amp;{\\pi \\left( j \\right)}&amp;{ \\cdot \\cdot \\cdot }\\\\ { \\cdot \\cdot \\cdot }&amp;{ \\cdot \\cdot \\cdot }&amp;{ \\cdot \\cdot \\cdot }&amp;{ \\cdot \\cdot \\cdot }&amp;{ \\cdot \\cdot \\cdot }\\\\ {\\pi \\left( 1 \\right)}&amp;{\\pi \\left( 2 \\right)}&amp;{ \\cdot \\cdot \\cdot }&amp;{\\pi \\left( j \\right)}&amp;{ \\cdot \\cdot \\cdot }\\\\ { \\cdot \\cdot \\cdot }&amp;{ \\cdot \\cdot \\cdot }&amp;{ \\cdot \\cdot \\cdot }&amp;{ \\cdot \\cdot \\cdot }&amp;{ \\cdot \\cdot \\cdot } \\end{array}} \\right]\" eeimg=\"1\"/> </p><p>3） <img src=\"https://www.zhihu.com/equation?tex=%5Cpi+%5Cleft%28+j+%5Cright%29+%3D+%5Csum%5Climits_%7Bi+%3D+0%7D%5E%5Cinfty+%7B%5Cpi+%5Cleft%28+i+%5Cright%29%7BP_%7Bij%7D%7D%7D+\" alt=\"\\pi \\left( j \\right) = \\sum\\limits_{i = 0}^\\infty {\\pi \\left( i \\right){P_{ij}}} \" eeimg=\"1\"/> </p><p>4） <img src=\"https://www.zhihu.com/equation?tex=%5Cpi+\" alt=\"\\pi \" eeimg=\"1\"/> 是方程 <img src=\"https://www.zhihu.com/equation?tex=+%5Cpi+P+%3D+%5Cpi+\" alt=\" \\pi P = \\pi \" eeimg=\"1\"/> 的唯一非负解，其中 <img src=\"https://www.zhihu.com/equation?tex=%5Cpi+%7B%5Crm%7B+%3D+%7D%7D%5Cleft%5B+%7B%5Cbegin%7Barray%7D%7B%2A%7B20%7D%7Bc%7D%7D+%7B%5Cpi+%5Cleft%28+1+%5Cright%29%7D%26%7B%5Cpi+%5Cleft%28+2+%5Cright%29%7D%26%7B+%5Ccdot+%5Ccdot+%5Ccdot+%7D%26%7B%5Cpi+%5Cleft%28+j+%5Cright%29%7D%26%7B+%5Ccdot+%5Ccdot+%5Ccdot+%7D+%5Cend%7Barray%7D%7D+%5Cright%5D%2C%5Csum%5Climits_%7Bj+%3D+1%7D%5E%5Cinfty+%7B%5Cpi+%5Cleft%28+j+%5Cright%29%7D+%3D+1\" alt=\"\\pi {\\rm{ = }}\\left[ {\\begin{array}{*{20}{c}} {\\pi \\left( 1 \\right)}&amp;{\\pi \\left( 2 \\right)}&amp;{ \\cdot \\cdot \\cdot }&amp;{\\pi \\left( j \\right)}&amp;{ \\cdot \\cdot \\cdot } \\end{array}} \\right],\\sum\\limits_{j = 1}^\\infty {\\pi \\left( j \\right)} = 1\" eeimg=\"1\"/> 。</p><p>关于上述定理，给出几点解释：</p><p>1）\t马氏链的状态数可以是有限的，也可以是无限的，因此可以用于连续概率分布和离散概率分布。</p><p>2）\t非周期马氏链：马氏链的状态转化不是循环的，如果是循环的则永远不会收敛，我们遇到的一般都是非周期马氏链。对于任意某一状态 <img src=\"https://www.zhihu.com/equation?tex=+i\" alt=\" i\" eeimg=\"1\"/> ， <img src=\"https://www.zhihu.com/equation?tex=d+\" alt=\"d \" eeimg=\"1\"/> 为集合 <img src=\"https://www.zhihu.com/equation?tex=%5Cleft%5C%7B+%7Bn%7Cn+%5Cge+1%2CP_%7Bii%7D%5En+%3E+0%7D+%5Cright%5C%7D+\" alt=\"\\left\\{ {n|n \\ge 1,P_{ii}^n &gt; 0} \\right\\} \" eeimg=\"1\"/> 的最大公约数，如果 <img src=\"https://www.zhihu.com/equation?tex=d+%3D+1\" alt=\"d = 1\" eeimg=\"1\"/> ，则该状态为非周期。</p><p>3）\t任何两个状态是连通的：从任意一个状态可以通过有限步到达其他的任意状态，不会出现条件概率一直为 <img src=\"https://www.zhihu.com/equation?tex=0\" alt=\"0\" eeimg=\"1\"/> 导致不可达的情况。</p><p>4） <img src=\"https://www.zhihu.com/equation?tex=+%5Cpi+\" alt=\" \\pi \" eeimg=\"1\"/> 称为马氏链的平稳分布。</p><p>如果从一个具体的初始状态 <img src=\"https://www.zhihu.com/equation?tex=+%7Bx_0%7D+\" alt=\" {x_0} \" eeimg=\"1\"/> 开始，沿着马氏链按照概率转移矩阵做跳转，那么可以得到一个转移序列 <img src=\"https://www.zhihu.com/equation?tex=+%7Bx_0%7D%2C%7Bx_1%7D%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2C%7Bx_n%7D%2C%7Bx_%7Bn+%2B+1%7D%7D%2C+%5Ccdot+%5Ccdot+%5Ccdot+\" alt=\" {x_0},{x_1}, \\cdot \\cdot \\cdot ,{x_n},{x_{n + 1}}, \\cdot \\cdot \\cdot \" eeimg=\"1\"/> ，由于马氏链的收敛行为， <img src=\"https://www.zhihu.com/equation?tex=%7Bx_n%7D%2C%7Bx_%7Bn+%2B+1%7D%7D%2C+%5Ccdot+%5Ccdot+%5Ccdot+\" alt=\"{x_n},{x_{n + 1}}, \\cdot \\cdot \\cdot \" eeimg=\"1\"/> 都将是平稳分布 <img src=\"https://www.zhihu.com/equation?tex=+%5Cpi+%5Cleft%28+x+%5Cright%29+\" alt=\" \\pi \\left( x \\right) \" eeimg=\"1\"/> 的样本。</p><h2><b>1.9 MCMC</b></h2><p><b>1. 接受-拒绝采样</b></p><p>对于不常见的概率分布 <img src=\"https://www.zhihu.com/equation?tex=+%5Cpi+%5Cleft%28+x+%5Cright%29+\" alt=\" \\pi \\left( x \\right) \" eeimg=\"1\"/> 样本，使用接受-拒绝采样对可采样的分布 <img src=\"https://www.zhihu.com/equation?tex=+q%5Cleft%28+x+%5Cright%29+\" alt=\" q\\left( x \\right) \" eeimg=\"1\"/> 进行采样得到，如图1.2所示，采样得到 <img src=\"https://www.zhihu.com/equation?tex=+Mq%5Cleft%28+x+%5Cright%29+\" alt=\" Mq\\left( x \\right) \" eeimg=\"1\"/> 的一个样本 <img src=\"https://www.zhihu.com/equation?tex=+%7Bx_0%7D\" alt=\" {x_0}\" eeimg=\"1\"/> ，从均匀分布 <img src=\"https://www.zhihu.com/equation?tex=+%5Cleft%28+%7B0%2CMq%5Cleft%28+%7B%7Bx_0%7D%7D+%5Cright%29%7D+%5Cright%29+\" alt=\" \\left( {0,Mq\\left( {{x_0}} \\right)} \\right) \" eeimg=\"1\"/> 中采样得到一个值 <img src=\"https://www.zhihu.com/equation?tex=+%7Bu_0%7D\" alt=\" {u_0}\" eeimg=\"1\"/> ，如果 <img src=\"https://www.zhihu.com/equation?tex=+%7Bu_0%7D+\" alt=\" {u_0} \" eeimg=\"1\"/> 落在图中灰色区域则拒绝这次采样，否则接受样本 <img src=\"https://www.zhihu.com/equation?tex=+%7Bx_0%7D\" alt=\" {x_0}\" eeimg=\"1\"/> ，重复上面过程得到 <img src=\"https://www.zhihu.com/equation?tex=+n+\" alt=\" n \" eeimg=\"1\"/> 个接受的样本，则这些样本服从 <img src=\"https://www.zhihu.com/equation?tex=+%5Cpi+%5Cleft%28+x+%5Cright%29+\" alt=\" \\pi \\left( x \\right) \" eeimg=\"1\"/> 分布，具体过程见算法1.1。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-f577bfed48c562e43c84b5ce601149ef_b.jpg\" data-size=\"normal\" data-rawwidth=\"589\" data-rawheight=\"281\" class=\"origin_image zh-lightbox-thumb\" width=\"589\" data-original=\"https://pic4.zhimg.com/v2-f577bfed48c562e43c84b5ce601149ef_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;589&#39; height=&#39;281&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"589\" data-rawheight=\"281\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"589\" data-original=\"https://pic4.zhimg.com/v2-f577bfed48c562e43c84b5ce601149ef_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-f577bfed48c562e43c84b5ce601149ef_b.jpg\"/><figcaption>图 1.2</figcaption></figure><p><b>Algorithm 1.1 接受-拒绝采样算法</b></p><blockquote>1. 目标分布 <img src=\"https://www.zhihu.com/equation?tex=+%5Cpi+%5Cleft%28+x+%5Cright%29+\" alt=\" \\pi \\left( x \\right) \" eeimg=\"1\"/> ，分布 <img src=\"https://www.zhihu.com/equation?tex=+q%5Cleft%28+x+%5Cright%29+\" alt=\" q\\left( x \\right) \" eeimg=\"1\"/> 和常数 <img src=\"https://www.zhihu.com/equation?tex=+M\" alt=\" M\" eeimg=\"1\"/> ，通过对 <img src=\"https://www.zhihu.com/equation?tex=+q%5Cleft%28+x+%5Cright%29+\" alt=\" q\\left( x \\right) \" eeimg=\"1\"/> 的采样实现对 <img src=\"https://www.zhihu.com/equation?tex=+%5Cpi+%5Cleft%28+x+%5Cright%29+\" alt=\" \\pi \\left( x \\right) \" eeimg=\"1\"/> 采样，满足：<br/>  1）对 <img src=\"https://www.zhihu.com/equation?tex=q%5Cleft%28+x+%5Cright%29+\" alt=\"q\\left( x \\right) \" eeimg=\"1\"/> 采样比较容易；<br/>  2） <img src=\"https://www.zhihu.com/equation?tex=q%5Cleft%28+x+%5Cright%29+\" alt=\"q\\left( x \\right) \" eeimg=\"1\"/> 的形状接近 <img src=\"https://www.zhihu.com/equation?tex=+%5Cpi+%5Cleft%28+x+%5Cright%29\" alt=\" \\pi \\left( x \\right)\" eeimg=\"1\"/> ，且 <img src=\"https://www.zhihu.com/equation?tex=+%5Cforall+x%EF%BC%8C%5Cpi+%5Cleft%28+x+%5Cright%29+%5Cle+Mq%5Cleft%28+x+%5Cright%29\" alt=\" \\forall x，\\pi \\left( x \\right) \\le Mq\\left( x \\right)\" eeimg=\"1\"/> 。<br/>2. 采样过程<br/>  1）产生样本 <img src=\"https://www.zhihu.com/equation?tex=+x+%5Csim+q%5Cleft%28+x+%5Cright%29\" alt=\" x \\sim q\\left( x \\right)\" eeimg=\"1\"/> ，和 <img src=\"https://www.zhihu.com/equation?tex=u+%5Csim+Uniform%5Cleft%5B+%7B0%2C1%7D+%5Cright%5D\" alt=\"u \\sim Uniform\\left[ {0,1} \\right]\" eeimg=\"1\"/> <br/>  2）若 <img src=\"https://www.zhihu.com/equation?tex=u+%5Cle+%5Cfrac%7B%7B%5Cpi+%5Cleft%28+x+%5Cright%29%7D%7D%7B%7BMq%5Cleft%28+x+%5Cright%29%7D%7D\" alt=\"u \\le \\frac{{\\pi \\left( x \\right)}}{{Mq\\left( x \\right)}}\" eeimg=\"1\"/> ，则接受样本 <img src=\"https://www.zhihu.com/equation?tex=x\" alt=\"x\" eeimg=\"1\"/> <br/>  3） 则接受的样本服从 <img src=\"https://www.zhihu.com/equation?tex=%5Cpi+%5Cleft%28+x+%5Cright%29\" alt=\"\\pi \\left( x \\right)\" eeimg=\"1\"/> 分布</blockquote><p>下面我们来证明下接受-拒绝方法采样得到的样本服从 <img src=\"https://www.zhihu.com/equation?tex=+%5Cpi+%5Cleft%28+x+%5Cright%29+\" alt=\" \\pi \\left( x \\right) \" eeimg=\"1\"/> 分布。</p><p>证明： <img src=\"https://www.zhihu.com/equation?tex=accept\" alt=\"accept\" eeimg=\"1\"/> <img src=\"https://www.zhihu.com/equation?tex=x\" alt=\"x\" eeimg=\"1\"/> 服从 <img src=\"https://www.zhihu.com/equation?tex=+%5Cpi+%5Cleft%28+x+%5Cright%29+\" alt=\" \\pi \\left( x \\right) \" eeimg=\"1\"/> 分布，即 <img src=\"https://www.zhihu.com/equation?tex=+p%5Cleft%28+%7Bx%7Caccept%7D+%5Cright%29+%3D+%5Cpi+%5Cleft%28+x+%5Cright%29+\" alt=\" p\\left( {x|accept} \\right) = \\pi \\left( x \\right) \" eeimg=\"1\"/> 。</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+p%5Cleft%28+%7Bx%7Caccept%7D+%5Cright%29+%26%3D+%5Cfrac%7B%7Bp%5Cleft%28+%7Baccept%7Cx%7D+%5Cright%29p%5Cleft%28+x+%5Cright%29%7D%7D%7B%7Bp%5Cleft%28+%7Baccept%7D+%5Cright%29%7D%7D%5C%5C++p%5Cleft%28+%7Baccept%7Cx%7D+%5Cright%29+%26%3D+p%5Cleft%28+%7Bu+%5Cle+%5Cfrac%7B%7B%5Cpi+%5Cleft%28+x+%5Cright%29%7D%7D%7B%7BMq%5Cleft%28+x+%5Cright%29%7D%7D%7D+%5Cright%29+%3D+%5Cfrac%7B%7B%5Cpi+%5Cleft%28+x+%5Cright%29%7D%7D%7B%7BMq%5Cleft%28+x+%5Cright%29%7D%7D%5C%5C+p%5Cleft%28+x+%5Cright%29+%26%3D+q%5Cleft%28+x+%5Cright%29%5C%5C+p%5Cleft%28+%7Baccept%7D+%5Cright%29+%26%3D+%5Cint_x+%7Bp%5Cleft%28+%7Baccept%7Cx%7D+%5Cright%29p%5Cleft%28+x+%5Cright%29dx%7D+%5C%5C+%26%3D+%5Cint_x+%7B%5Cfrac%7B%7B%5Cpi+%5Cleft%28+x+%5Cright%29%7D%7D%7B%7BMq%5Cleft%28+x+%5Cright%29%7D%7Dq%5Cleft%28+x+%5Cright%29dx%7D+%5C%5C+%26%3D+%5Cfrac%7B1%7D%7BM%7D%5C%5C+p%5Cleft%28+%7Bx%7Caccept%7D+%5Cright%29+%26%3D+%5Cpi+%5Cleft%28+x+%5Cright%29+%5Ctag%7B33%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} p\\left( {x|accept} \\right) &amp;= \\frac{{p\\left( {accept|x} \\right)p\\left( x \\right)}}{{p\\left( {accept} \\right)}}\\\\  p\\left( {accept|x} \\right) &amp;= p\\left( {u \\le \\frac{{\\pi \\left( x \\right)}}{{Mq\\left( x \\right)}}} \\right) = \\frac{{\\pi \\left( x \\right)}}{{Mq\\left( x \\right)}}\\\\ p\\left( x \\right) &amp;= q\\left( x \\right)\\\\ p\\left( {accept} \\right) &amp;= \\int_x {p\\left( {accept|x} \\right)p\\left( x \\right)dx} \\\\ &amp;= \\int_x {\\frac{{\\pi \\left( x \\right)}}{{Mq\\left( x \\right)}}q\\left( x \\right)dx} \\\\ &amp;= \\frac{1}{M}\\\\ p\\left( {x|accept} \\right) &amp;= \\pi \\left( x \\right) \\tag{33} \\end{align*}\" eeimg=\"1\"/> </p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>2. MCMC</b></p><p>给定概率分布 <img src=\"https://www.zhihu.com/equation?tex=+p%5Cleft%28+x+%5Cright%29\" alt=\" p\\left( x \\right)\" eeimg=\"1\"/> ，希望能够生成它对应的样本，由于马氏链能收敛到平稳分布，有一个很好的想法：如果我们能构造一个转移矩阵为 P 的马氏链，使得该马氏链的平稳分布恰好是 <img src=\"https://www.zhihu.com/equation?tex=p%5Cleft%28+x+%5Cright%29+\" alt=\"p\\left( x \\right) \" eeimg=\"1\"/> ，那么我们从任何一个初始状态出发沿着马氏链转移，得到一个转移序列, <img src=\"https://www.zhihu.com/equation?tex=%5Ccdot+%5Ccdot+%5Ccdot+%2C%7Bx_n%7D%2C%7Bx_%7Bn+%2B+1%7D%7D%2C+%5Ccdot+%5Ccdot+%5Ccdot+\" alt=\"\\cdot \\cdot \\cdot ,{x_n},{x_{n + 1}}, \\cdot \\cdot \\cdot \" eeimg=\"1\"/> ，如果马氏链在第 <img src=\"https://www.zhihu.com/equation?tex=+n+\" alt=\" n \" eeimg=\"1\"/> 步已经收敛了，于是我们可以得到 <img src=\"https://www.zhihu.com/equation?tex=+p%5Cleft%28+x+%5Cright%29+\" alt=\" p\\left( x \\right) \" eeimg=\"1\"/> 的样本 <img src=\"https://www.zhihu.com/equation?tex=%7Bx_n%7D%2C%7Bx_%7Bn+%2B+1%7D%7D%2C+%5Ccdot+%5Ccdot+%5Ccdot+\" alt=\"{x_n},{x_{n + 1}}, \\cdot \\cdot \\cdot \" eeimg=\"1\"/> ，所以关键问题是如何构造转移矩阵 <img src=\"https://www.zhihu.com/equation?tex=+P\" alt=\" P\" eeimg=\"1\"/> ，我们是基于下面的定理。</p><p><b>定理1.2（细致平稳条件）</b> 如果非周期马氏链的转移矩阵 <img src=\"https://www.zhihu.com/equation?tex=+P+\" alt=\" P \" eeimg=\"1\"/> 和分布 <img src=\"https://www.zhihu.com/equation?tex=+%5Cpi+%5Cleft%28+x+%5Cright%29+\" alt=\" \\pi \\left( x \\right) \" eeimg=\"1\"/> 满足：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Cpi+%5Cleft%28+i+%5Cright%29%7BP_%7Bij%7D%7D+%3D+%5Cpi+%5Cleft%28+j+%5Cright%29%7BP_%7Bji%7D%7D+%5Ctag%7B34%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} \\pi \\left( i \\right){P_{ij}} = \\pi \\left( j \\right){P_{ji}} \\tag{34} \\end{align*}\" eeimg=\"1\"/> </p><p>则 <img src=\"https://www.zhihu.com/equation?tex=+%5Cpi+%5Cleft%28+x+%5Cright%29+\" alt=\" \\pi \\left( x \\right) \" eeimg=\"1\"/> 是马氏链的平稳分布。</p><p>证明很简单，有公式 (34) 得：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Csum%5Climits_%7Bi+%3D+1%7D%5E%5Cinfty+%7B%5Cpi+%5Cleft%28+i+%5Cright%29%7BP_%7Bij%7D%7D%7D+%3D+%5Csum%5Climits_%7Bi+%3D+1%7D%5E%5Cinfty+%7B%5Cpi+%5Cleft%28+j+%5Cright%29%7BP_%7Bji%7D%7D%7D+%3D+%5Cpi+%5Cleft%28+j+%5Cright%29%5Csum%5Climits_%7Bi+%3D+1%7D%5E%5Cinfty+%7B%7BP_%7Bji%7D%7D%7D+%3D+%5Cpi+%5Cleft%28+j+%5Cright%29+%5Ctag%7B35%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} \\sum\\limits_{i = 1}^\\infty {\\pi \\left( i \\right){P_{ij}}} = \\sum\\limits_{i = 1}^\\infty {\\pi \\left( j \\right){P_{ji}}} = \\pi \\left( j \\right)\\sum\\limits_{i = 1}^\\infty {{P_{ji}}} = \\pi \\left( j \\right) \\tag{35} \\end{align*}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cpi+P+%3D+%5Cpi+\" alt=\"\\pi P = \\pi \" eeimg=\"1\"/> ，满足马氏链的收敛性质。这样我们就有了新的思路寻找转移矩阵 <img src=\"https://www.zhihu.com/equation?tex=+P\" alt=\" P\" eeimg=\"1\"/> ，即只要我们找到矩阵 <img src=\"https://www.zhihu.com/equation?tex=+P+\" alt=\" P \" eeimg=\"1\"/> 使得概率分布 <img src=\"https://www.zhihu.com/equation?tex=+%5Cpi+%5Cleft%28+x+%5Cright%29+\" alt=\" \\pi \\left( x \\right) \" eeimg=\"1\"/> 满足细致平稳条件即可。</p><p>假设有一个转移矩阵为 <img src=\"https://www.zhihu.com/equation?tex=+Q+\" alt=\" Q \" eeimg=\"1\"/> 的马氏链 <img src=\"https://www.zhihu.com/equation?tex=%28Q%5Cleft%28+%7Bi%2Cj%7D+%5Cright%29+\" alt=\"(Q\\left( {i,j} \\right) \" eeimg=\"1\"/> 表示从状态 <img src=\"https://www.zhihu.com/equation?tex=+i+\" alt=\" i \" eeimg=\"1\"/> 转移到状态 <img src=\"https://www.zhihu.com/equation?tex=+j+\" alt=\" j \" eeimg=\"1\"/> 的概率），通常情况下很难满足细致平稳条件的，即：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Cpi+%5Cleft%28+i+%5Cright%29Q%5Cleft%28+%7Bi%2Cj%7D+%5Cright%29+%5Cne+%5Cpi+%5Cleft%28+j+%5Cright%29Q%5Cleft%28+%7Bj%2Ci%7D+%5Cright%29+%5Ctag%7B36%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} \\pi \\left( i \\right)Q\\left( {i,j} \\right) \\ne \\pi \\left( j \\right)Q\\left( {j,i} \\right) \\tag{36} \\end{align*}\" eeimg=\"1\"/> </p><p>我们对公式(36)进行改造，使细致平稳条件成立，引入 <img src=\"https://www.zhihu.com/equation?tex=+%5Calpha+%5Cleft%28+%7Bi%2Cj%7D+%5Cright%29\" alt=\" \\alpha \\left( {i,j} \\right)\" eeimg=\"1\"/> 。</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Cpi+%5Cleft%28+i+%5Cright%29Q%5Cleft%28+%7Bi%2Cj%7D+%5Cright%29%5Calpha+%5Cleft%28+%7Bi%2Cj%7D+%5Cright%29+%3D+%5Cpi+%5Cleft%28+j+%5Cright%29Q%5Cleft%28+%7Bj%2Ci%7D+%5Cright%29%5Calpha+%5Cleft%28+%7Bj%2Ci%7D+%5Cright%29+%5Ctag%7B37%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} \\pi \\left( i \\right)Q\\left( {i,j} \\right)\\alpha \\left( {i,j} \\right) = \\pi \\left( j \\right)Q\\left( {j,i} \\right)\\alpha \\left( {j,i} \\right) \\tag{37} \\end{align*}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%5Calpha+%5Cleft%28+%7Bi%2Cj%7D+%5Cright%29\" alt=\"\\alpha \\left( {i,j} \\right)\" eeimg=\"1\"/> 如何取值才能使公式(37)成立？最简单的我们可以取：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Calpha+%5Cleft%28+%7Bi%2Cj%7D+%5Cright%29+%26%3D+%5Cpi+%5Cleft%28+j+%5Cright%29Q%5Cleft%28+%7Bj%2Ci%7D+%5Cright%29%5C%5C+%5Calpha+%5Cleft%28+%7Bj%2Ci%7D+%5Cright%29+%26%3D+%5Cpi+%5Cleft%28+i+%5Cright%29Q%5Cleft%28+%7Bi%2Cj%7D+%5Cright%29+%5Ctag%7B38%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} \\alpha \\left( {i,j} \\right) &amp;= \\pi \\left( j \\right)Q\\left( {j,i} \\right)\\\\ \\alpha \\left( {j,i} \\right) &amp;= \\pi \\left( i \\right)Q\\left( {i,j} \\right) \\tag{38} \\end{align*}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=Q%27%5Cleft%28+%7Bi%2Cj%7D+%5Cright%29+%3D+Q%5Cleft%28+%7Bi%2Cj%7D+%5Cright%29%5Calpha+%5Cleft%28+%7Bi%2Cj%7D+%5Cright%29%EF%BC%8CQ%27%5Cleft%28+%7Bj%2Ci%7D+%5Cright%29+%3D+Q%5Cleft%28+%7Bj%2Ci%7D+%5Cright%29%5Calpha+%5Cleft%28+%7Bj%2Ci%7D+%5Cright%29+\" alt=\"Q&#39;\\left( {i,j} \\right) = Q\\left( {i,j} \\right)\\alpha \\left( {i,j} \\right)，Q&#39;\\left( {j,i} \\right) = Q\\left( {j,i} \\right)\\alpha \\left( {j,i} \\right) \" eeimg=\"1\"/> 所以我们有:</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Cpi+%5Cleft%28+i+%5Cright%29Q%27%5Cleft%28+%7Bi%2Cj%7D+%5Cright%29+%3D+%5Cpi+%5Cleft%28+j+%5Cright%29Q%27%5Cleft%28+%7Bj%2Ci%7D+%5Cright%29+%5Ctag%7B39%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} \\pi \\left( i \\right)Q&#39;\\left( {i,j} \\right) = \\pi \\left( j \\right)Q&#39;\\left( {j,i} \\right) \\tag{39} \\end{align*}\" eeimg=\"1\"/> </p><p>转移矩阵 <img src=\"https://www.zhihu.com/equation?tex=+Q%27\" alt=\" Q&#39;\" eeimg=\"1\"/> 满足细致平稳条件，因此马氏链 <img src=\"https://www.zhihu.com/equation?tex=+Q%27+\" alt=\" Q&#39; \" eeimg=\"1\"/> 的平稳分布就是 <img src=\"https://www.zhihu.com/equation?tex=+%5Cpi+%5Cleft%28+x+%5Cright%29\" alt=\" \\pi \\left( x \\right)\" eeimg=\"1\"/>！</p><p>我们可以得到一个非常好的结论，转移矩阵 <img src=\"https://www.zhihu.com/equation?tex=+Q%27+\" alt=\" Q&#39; \" eeimg=\"1\"/> 可以通过任意一个马氏链转移矩阵 <img src=\"https://www.zhihu.com/equation?tex=+Q+\" alt=\" Q \" eeimg=\"1\"/> 乘以 <img src=\"https://www.zhihu.com/equation?tex=%5Calpha+%5Cleft%28+%7Bi%2Cj%7D+%5Cright%29\" alt=\"\\alpha \\left( {i,j} \\right)\" eeimg=\"1\"/> 得到， <img src=\"https://www.zhihu.com/equation?tex=%5Calpha+%5Cleft%28+%7Bi%2Cj%7D+%5Cright%29+\" alt=\"\\alpha \\left( {i,j} \\right) \" eeimg=\"1\"/> 一般称为接受率，其取值范围为 <img src=\"https://www.zhihu.com/equation?tex=+%5Cleft%5B+%7B0%2C1%7D+%5Cright%5D\" alt=\" \\left[ {0,1} \\right]\" eeimg=\"1\"/> ，可以理解为一个概率值，在原来的马氏链上，从状态 <img src=\"https://www.zhihu.com/equation?tex=+i+\" alt=\" i \" eeimg=\"1\"/> 以 <img src=\"https://www.zhihu.com/equation?tex=+Q%5Cleft%28+%7Bi%2Cj%7D+%5Cright%29+\" alt=\" Q\\left( {i,j} \\right) \" eeimg=\"1\"/> 的概率跳转到状态 <img src=\"https://www.zhihu.com/equation?tex=+j+\" alt=\" j \" eeimg=\"1\"/> 的时候，我们以一定的概率 <img src=\"https://www.zhihu.com/equation?tex=%5Calpha+%5Cleft%28+%7Bi%2Cj%7D+%5Cright%29+\" alt=\"\\alpha \\left( {i,j} \\right) \" eeimg=\"1\"/> 接受这个转移，很像前面介绍的接受-拒绝采样，那里以一个常见的分布通过一定的接受-拒绝概率得到一个不常见的分布，这里以一个常见的马氏链状态转移矩阵 <img src=\"https://www.zhihu.com/equation?tex=+Q+\" alt=\" Q \" eeimg=\"1\"/> 通过一定的接受-拒绝概率得到新的马氏链状态转移矩阵 <img src=\"https://www.zhihu.com/equation?tex=+Q%27\" alt=\" Q&#39;\" eeimg=\"1\"/> 。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-93fc0e8b050b6fceaa27b54125de073a_b.jpg\" data-size=\"normal\" data-rawwidth=\"700\" data-rawheight=\"280\" class=\"origin_image zh-lightbox-thumb\" width=\"700\" data-original=\"https://pic3.zhimg.com/v2-93fc0e8b050b6fceaa27b54125de073a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;700&#39; height=&#39;280&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"700\" data-rawheight=\"280\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"700\" data-original=\"https://pic3.zhimg.com/v2-93fc0e8b050b6fceaa27b54125de073a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-93fc0e8b050b6fceaa27b54125de073a_b.jpg\"/><figcaption>图 1.3</figcaption></figure><p>总结下 <b>MCMC </b>的采样过程。</p><p><b>Algorithm 1.2 MCMC采样算法</b></p><blockquote>1. 初始化马氏链初始状态 <img src=\"https://www.zhihu.com/equation?tex=%7BX_0%7D+%3D+%7Bx_0%7D\" alt=\"{X_0} = {x_0}\" eeimg=\"1\"/> <br/>2. 对 <img src=\"https://www.zhihu.com/equation?tex=+t+%3D+0%2C1%2C2%2C+%5Ccdot+%5Ccdot+%5Ccdot+\" alt=\" t = 0,1,2, \\cdot \\cdot \\cdot \" eeimg=\"1\"/> 循环以下过程进行采样<br/>  1）第 <img src=\"https://www.zhihu.com/equation?tex=+t+\" alt=\" t \" eeimg=\"1\"/> 时刻马氏链状态为 <img src=\"https://www.zhihu.com/equation?tex=+%7BX_t%7D+%3D+%7Bx_t%7D+\" alt=\" {X_t} = {x_t} \" eeimg=\"1\"/> ，从条件概率 <img src=\"https://www.zhihu.com/equation?tex=+Q%5Cleft%28+%7Bx%7C%7Bx_t%7D%7D+%5Cright%29+\" alt=\" Q\\left( {x|{x_t}} \\right) \" eeimg=\"1\"/> 采样得到样本 <img src=\"https://www.zhihu.com/equation?tex=+x%27\" alt=\" x&#39;\" eeimg=\"1\"/> <br/>  2）从均匀分布采样 <img src=\"https://www.zhihu.com/equation?tex=+u+%5Csim+uniform%5Cleft%28+%7B0%2C1%7D+%5Cright%29\" alt=\" u \\sim uniform\\left( {0,1} \\right)\" eeimg=\"1\"/> <br/>  3） 如果 <img src=\"https://www.zhihu.com/equation?tex=+u+%3C+%5Calpha+%5Cleft%28+%7B%7Bx_t%7D%2Cx%27%7D+%5Cright%29+%3D+%5Cpi+%5Cleft%28+%7Bx%27%7D+%5Cright%29Q%5Cleft%28+%7B%7Bx_t%7D%7Cx%27%7D+%5Cright%29\" alt=\" u &lt; \\alpha \\left( {{x_t},x&#39;} \\right) = \\pi \\left( {x&#39;} \\right)Q\\left( {{x_t}|x&#39;} \\right)\" eeimg=\"1\"/> 则接受转移 <img src=\"https://www.zhihu.com/equation?tex=+%7Bx_t%7D+%5Cto+x%27+\" alt=\" {x_t} \\to x&#39; \" eeimg=\"1\"/> ，即 <img src=\"https://www.zhihu.com/equation?tex=%7BX_%7Bt+%2B+1%7D%7D+%3D+x%27\" alt=\"{X_{t + 1}} = x&#39;\" eeimg=\"1\"/> <br/>  4）否则不接受转移，即 <img src=\"https://www.zhihu.com/equation?tex=+%7BX_%7Bt+%2B+1%7D%7D+%3D+%7Bx_t%7D\" alt=\" {X_{t + 1}} = {x_t}\" eeimg=\"1\"/> </blockquote><p>MCMC 采样算法有一个问题，如果接受率 <img src=\"https://www.zhihu.com/equation?tex=+%5Calpha+%5Cleft%28+%7B%7Bx_t%7D%2Cx%27%7D+%5Cright%29+\" alt=\" \\alpha \\left( {{x_t},x&#39;} \\right) \" eeimg=\"1\"/> 比较小，马氏链容易原地踏步，拒绝大量的跳转，收敛到平稳分布 <img src=\"https://www.zhihu.com/equation?tex=+%5Cpi+%5Cleft%28+x+%5Cright%29+\" alt=\" \\pi \\left( x \\right) \" eeimg=\"1\"/> 的速度很慢，有没有办法可以使 <img src=\"https://www.zhihu.com/equation?tex=+%5Calpha+%5Cleft%28+%7B%7Bx_t%7D%2Cx%27%7D+%5Cright%29+\" alt=\" \\alpha \\left( {{x_t},x&#39;} \\right) \" eeimg=\"1\"/> 变大？</p><p><b>3. M-H采样</b></p><p>M-H 采样可以解决 MCMC 采样接受概率过低问题，回到公式(37)，若 <img src=\"https://www.zhihu.com/equation?tex=+%5Calpha+%5Cleft%28+%7Bi%2Cj%7D+%5Cright%29+%3D+0.1%EF%BC%8C%5Calpha+%5Cleft%28+%7Bj%2Ci%7D+%5Cright%29+%3D+0.2\" alt=\" \\alpha \\left( {i,j} \\right) = 0.1，\\alpha \\left( {j,i} \\right) = 0.2\" eeimg=\"1\"/> ，即：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Cpi+%5Cleft%28+i+%5Cright%29Q%5Cleft%28+%7Bi%2Cj%7D+%5Cright%29+%5Ctimes+0.1+%3D+%5Cpi+%5Cleft%28+j+%5Cright%29Q%5Cleft%28+%7Bj%2Ci%7D+%5Cright%29+%5Ctimes+0.2+%5Ctag%7B40%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} \\pi \\left( i \\right)Q\\left( {i,j} \\right) \\times 0.1 = \\pi \\left( j \\right)Q\\left( {j,i} \\right) \\times 0.2 \\tag{40} \\end{align*}\" eeimg=\"1\"/> </p><p>公式(40)两边同时扩大5倍，仍然满足细致平稳条件，即：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Cpi+%5Cleft%28+i+%5Cright%29Q%5Cleft%28+%7Bi%2Cj%7D+%5Cright%29+%5Ctimes+0.5+%3D+%5Cpi+%5Cleft%28+j+%5Cright%29Q%5Cleft%28+%7Bj%2Ci%7D+%5Cright%29+%5Ctimes+1+%5Ctag%7B41%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} \\pi \\left( i \\right)Q\\left( {i,j} \\right) \\times 0.5 = \\pi \\left( j \\right)Q\\left( {j,i} \\right) \\times 1 \\tag{41} \\end{align*}\" eeimg=\"1\"/> </p><p>所以我们可以把公式(37)中的 <img src=\"https://www.zhihu.com/equation?tex=+%5Calpha+%5Cleft%28+%7Bi%2Cj%7D+%5Cright%29+%E5%92%8C+%5Calpha+%5Cleft%28+%7Bj%2Ci%7D+%5Cright%29+\" alt=\" \\alpha \\left( {i,j} \\right) 和 \\alpha \\left( {j,i} \\right) \" eeimg=\"1\"/> 同比例放大，使得其中最大的放大到 1，这样提高了采样中的接受率，细致平稳条件也没有打破，所以可以取：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Calpha+%5Cleft%28+%7Bi%2Cj%7D+%5Cright%29+%3D+%5Cmin+%5Cleft%5C%7B+%7B%5Cfrac%7B%7B%5Cpi+%5Cleft%28+j+%5Cright%29Q%5Cleft%28+%7Bj%2Ci%7D+%5Cright%29%7D%7D%7B%7B%5Cpi+%5Cleft%28+i+%5Cright%29Q%5Cleft%28+%7Bi%2Cj%7D+%5Cright%29%7D%7D%2C1%7D+%5Cright%5C%7D+%5Ctag%7B42%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} \\alpha \\left( {i,j} \\right) = \\min \\left\\{ {\\frac{{\\pi \\left( j \\right)Q\\left( {j,i} \\right)}}{{\\pi \\left( i \\right)Q\\left( {i,j} \\right)}},1} \\right\\} \\tag{42} \\end{align*}\" eeimg=\"1\"/> </p><p><b>Algorithm 1.3 M-H采样算法</b></p><blockquote>1. 初始化马氏链初始状态 <img src=\"https://www.zhihu.com/equation?tex=+%7BX_0%7D+%3D+%7Bx_0%7D\" alt=\" {X_0} = {x_0}\" eeimg=\"1\"/> <br/>2. 对 <img src=\"https://www.zhihu.com/equation?tex=+t+%3D+0%2C1%2C2%2C+%5Ccdot+%5Ccdot+%5Ccdot+\" alt=\" t = 0,1,2, \\cdot \\cdot \\cdot \" eeimg=\"1\"/> 循环以下过程进行采样<br/>  1）第 <img src=\"https://www.zhihu.com/equation?tex=+%5C%5Bt%5C%5D+\" alt=\" \\[t\\] \" eeimg=\"1\"/> 时刻马氏链状态为 <img src=\"https://www.zhihu.com/equation?tex=+%7BX_t%7D+%3D+%7Bx_t%7D+\" alt=\" {X_t} = {x_t} \" eeimg=\"1\"/> ，从条件概率 <img src=\"https://www.zhihu.com/equation?tex=+Q%5Cleft%28+%7Bx%7C%7Bx_t%7D%7D+%5Cright%29+\" alt=\" Q\\left( {x|{x_t}} \\right) \" eeimg=\"1\"/> 采样得到样本 <img src=\"https://www.zhihu.com/equation?tex=+x%27\" alt=\" x&#39;\" eeimg=\"1\"/> <br/>  2）从均匀分布采样 <img src=\"https://www.zhihu.com/equation?tex=+u+%5Csim+uniform%5Cleft%28+%7B0%2C1%7D+%5Cright%29\" alt=\" u \\sim uniform\\left( {0,1} \\right)\" eeimg=\"1\"/> <br/>  3）如果 <img src=\"https://www.zhihu.com/equation?tex=+u+%3C+%5Calpha+%5Cleft%28+%7B%7Bx_t%7D%2Cx%27%7D+%5Cright%29+%3D+%5Cmin+%5Cleft%5C%7B+%7B%5Cfrac%7B%7B%5Cpi+%5Cleft%28+%7Bx%27%7D+%5Cright%29Q%5Cleft%28+%7B%7Bx_t%7D%7Cx%27%7D+%5Cright%29%7D%7D%7B%7B%5Cpi+%5Cleft%28+%7B%7Bx_t%7D%7D+%5Cright%29Q%5Cleft%28+%7Bx%27%7C%7Bx_t%7D%7D+%5Cright%29%7D%7D%2C1%7D+%5Cright%5C%7D+\" alt=\" u &lt; \\alpha \\left( {{x_t},x&#39;} \\right) = \\min \\left\\{ {\\frac{{\\pi \\left( {x&#39;} \\right)Q\\left( {{x_t}|x&#39;} \\right)}}{{\\pi \\left( {{x_t}} \\right)Q\\left( {x&#39;|{x_t}} \\right)}},1} \\right\\} \" eeimg=\"1\"/> 则接受转移 <img src=\"https://www.zhihu.com/equation?tex=+%7Bx_t%7D+%5Cto+x%27\" alt=\" {x_t} \\to x&#39;\" eeimg=\"1\"/> ，即 <img src=\"https://www.zhihu.com/equation?tex=+%7BX_%7Bt+%2B+1%7D%7D+%3D+x%27\" alt=\" {X_{t + 1}} = x&#39;\" eeimg=\"1\"/> <br/>  4） 否则不接受转移，即 <img src=\"https://www.zhihu.com/equation?tex=+%7BX_%7Bt+%2B+1%7D%7D+%3D+%7Bx_t%7D\" alt=\" {X_{t + 1}} = {x_t}\" eeimg=\"1\"/> </blockquote><p>提出一个问题：按照 MCMC 中介绍的方法把 <img src=\"https://www.zhihu.com/equation?tex=+Q+%5Cto+Q%27+\" alt=\" Q \\to Q&#39; \" eeimg=\"1\"/> ，是否可以保证 <img src=\"https://www.zhihu.com/equation?tex=+Q%27+\" alt=\" Q&#39; \" eeimg=\"1\"/> 每行加和为1？</p><ul><li>当 <img src=\"https://www.zhihu.com/equation?tex=i+%5Cne+j%EF%BC%8CQ%27%5Cleft%28+%7Bi%2Cj%7D+%5Cright%29+%3D+Q%5Cleft%28+%7Bi%2Cj%7D+%5Cright%29%5Calpha+%5Cleft%28+%7Bi%2Cj%7D+%5Cright%29\" alt=\"i \\ne j，Q&#39;\\left( {i,j} \\right) = Q\\left( {i,j} \\right)\\alpha \\left( {i,j} \\right)\" eeimg=\"1\"/> ；</li><li>当 <img src=\"https://www.zhihu.com/equation?tex=+i+%3D+j\" alt=\" i = j\" eeimg=\"1\"/> ，考虑拒绝转移概率， <img src=\"https://www.zhihu.com/equation?tex=Q%27%5Cleft%28+%7Bi%2Ci%7D+%5Cright%29+%3D+Q%5Cleft%28+%7Bi%2Ci%7D+%5Cright%29%5Calpha+%5Cleft%28+%7Bi%2Ci%7D+%5Cright%29+%2B+%5Csum%5Climits_j+%7BQ%5Cleft%28+%7Bi%2Cj%7D+%5Cright%29%5Cleft%28+%7B1+-+%5Calpha+%5Cleft%28+%7Bi%2Cj%7D+%5Cright%29%7D+%5Cright%29%7D\" alt=\"Q&#39;\\left( {i,i} \\right) = Q\\left( {i,i} \\right)\\alpha \\left( {i,i} \\right) + \\sum\\limits_j {Q\\left( {i,j} \\right)\\left( {1 - \\alpha \\left( {i,j} \\right)} \\right)}\" eeimg=\"1\"/> 。</li></ul><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Csum%5Climits_j+%7BQ%27%5Cleft%28+%7Bi%2Cj%7D+%5Cright%29%7D+%26%3D+Q%27%5Cleft%28+%7Bi%2Ci%7D+%5Cright%29+%2B+%5Csum%5Climits_%7Bi+%5Cne+j%7D+%7BQ%27%5Cleft%28+%7Bi%2Cj%7D+%5Cright%29%7D+%5C%5C+%26%3D+Q%5Cleft%28+%7Bi%2Ci%7D+%5Cright%29%5Calpha+%5Cleft%28+%7Bi%2Ci%7D+%5Cright%29+%2B+%5Csum%5Climits_j+%7BQ%5Cleft%28+%7Bi%2Cj%7D+%5Cright%29%5Cleft%28+%7B1+-+%5Calpha+%5Cleft%28+%7Bi%2Cj%7D+%5Cright%29%7D+%5Cright%29%7D+%2B+%5Csum%5Climits_%7Bi+%5Cne+j%7D+%7BQ%5Cleft%28+%7Bi%2Cj%7D+%5Cright%29%5Calpha+%5Cleft%28+%7Bi%2Cj%7D+%5Cright%29%7D+%5C%5C+%26%3D+%5Csum%5Climits_j+%7BQ%5Cleft%28+%7Bi%2Cj%7D+%5Cright%29%7D+%3D+1+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} \\sum\\limits_j {Q&#39;\\left( {i,j} \\right)} &amp;= Q&#39;\\left( {i,i} \\right) + \\sum\\limits_{i \\ne j} {Q&#39;\\left( {i,j} \\right)} \\\\ &amp;= Q\\left( {i,i} \\right)\\alpha \\left( {i,i} \\right) + \\sum\\limits_j {Q\\left( {i,j} \\right)\\left( {1 - \\alpha \\left( {i,j} \\right)} \\right)} + \\sum\\limits_{i \\ne j} {Q\\left( {i,j} \\right)\\alpha \\left( {i,j} \\right)} \\\\ &amp;= \\sum\\limits_j {Q\\left( {i,j} \\right)} = 1 \\end{align*}\" eeimg=\"1\"/> </p><h2><b>1.10 Gibbs Sampling</b></h2><p>对于高维的情形，由于接受率 <img src=\"https://www.zhihu.com/equation?tex=+%5Calpha+%5Cle+1\" alt=\" \\alpha \\le 1\" eeimg=\"1\"/> ，M-H 算法效率不够高，我们能否找到一个转移矩阵 Q 使得接受率 <img src=\"https://www.zhihu.com/equation?tex=+%5Calpha+%7B%5Crm%7B+%3D+%7D%7D1+\" alt=\" \\alpha {\\rm{ = }}1 \" eeimg=\"1\"/> 呢？从二维分布开始，假设 <img src=\"https://www.zhihu.com/equation?tex=+p%5Cleft%28+%7Bx%2Cy%7D+%5Cright%29+\" alt=\" p\\left( {x,y} \\right) \" eeimg=\"1\"/> 是一个二维联合概率分布，考察某个特征维度相同的两个点 <img src=\"https://www.zhihu.com/equation?tex=+A%5Cleft%28+%7B%7Bx_1%7D%2C%7By_1%7D%7D+%5Cright%29+%E5%92%8C+B%5Cleft%28+%7B%7Bx_1%7D%2C%7By_2%7D%7D+%5Cright%29+\" alt=\" A\\left( {{x_1},{y_1}} \\right) 和 B\\left( {{x_1},{y_2}} \\right) \" eeimg=\"1\"/> ，容易发现下面等式成立：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+p%5Cleft%28+%7B%7Bx_1%7D%2C%7By_1%7D%7D+%5Cright%29p%5Cleft%28+%7B%7By_2%7D%7C%7Bx_1%7D%7D+%5Cright%29+%3D+p%5Cleft%28+%7B%7Bx_1%7D%7D+%5Cright%29p%5Cleft%28+%7B%7By_1%7D%7C%7Bx_1%7D%7D+%5Cright%29p%5Cleft%28+%7B%7By_2%7D%7C%7Bx_1%7D%7D+%5Cright%29%5C%5C+p%5Cleft%28+%7B%7Bx_1%7D%2C%7By_2%7D%7D+%5Cright%29p%5Cleft%28+%7B%7By_1%7D%7C%7Bx_1%7D%7D+%5Cright%29+%3D+p%5Cleft%28+%7B%7Bx_1%7D%7D+%5Cright%29p%5Cleft%28+%7B%7By_2%7D%7C%7Bx_1%7D%7D+%5Cright%29p%5Cleft%28+%7B%7By_1%7D%7C%7Bx_1%7D%7D+%5Cright%29+%5Ctag%7B43%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} p\\left( {{x_1},{y_1}} \\right)p\\left( {{y_2}|{x_1}} \\right) = p\\left( {{x_1}} \\right)p\\left( {{y_1}|{x_1}} \\right)p\\left( {{y_2}|{x_1}} \\right)\\\\ p\\left( {{x_1},{y_2}} \\right)p\\left( {{y_1}|{x_1}} \\right) = p\\left( {{x_1}} \\right)p\\left( {{y_2}|{x_1}} \\right)p\\left( {{y_1}|{x_1}} \\right) \\tag{43} \\end{align*}\" eeimg=\"1\"/> </p><p>所以可得：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+p%5Cleft%28+%7B%7Bx_1%7D%2C%7By_1%7D%7D+%5Cright%29p%5Cleft%28+%7B%7By_2%7D%7C%7Bx_1%7D%7D+%5Cright%29%7B%5Crm%7B+%3D+%7D%7Dp%5Cleft%28+%7B%7Bx_1%7D%2C%7By_2%7D%7D+%5Cright%29p%5Cleft%28+%7B%7By_1%7D%7C%7Bx_1%7D%7D+%5Cright%29+%5Ctag%7B44%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} p\\left( {{x_1},{y_1}} \\right)p\\left( {{y_2}|{x_1}} \\right){\\rm{ = }}p\\left( {{x_1},{y_2}} \\right)p\\left( {{y_1}|{x_1}} \\right) \\tag{44} \\end{align*}\" eeimg=\"1\"/> </p><p>也就是：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+p%5Cleft%28+A+%5Cright%29p%5Cleft%28+%7B%7By_2%7D%7C%7Bx_1%7D%7D+%5Cright%29%7B%5Crm%7B+%3D+%7D%7Dp%5Cleft%28+B+%5Cright%29p%5Cleft%28+%7B%7By_1%7D%7C%7Bx_1%7D%7D+%5Cright%29+%5Ctag%7B45%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} p\\left( A \\right)p\\left( {{y_2}|{x_1}} \\right){\\rm{ = }}p\\left( B \\right)p\\left( {{y_1}|{x_1}} \\right) \\tag{45} \\end{align*}\" eeimg=\"1\"/> </p><p>观察细致平稳条件公式，我们发现在 <img src=\"https://www.zhihu.com/equation?tex=+x+%3D+%7Bx_1%7D+\" alt=\" x = {x_1} \" eeimg=\"1\"/> 这条直线上，如果用条件分布 <img src=\"https://www.zhihu.com/equation?tex=+p%5Cleft%28+%7By%7C%7Bx_1%7D%7D+%5Cright%29+\" alt=\" p\\left( {y|{x_1}} \\right) \" eeimg=\"1\"/>作为任何两点之间的转移概率，那么任何两点之间的转移都满足细致平稳条件。同样的，在 <img src=\"https://www.zhihu.com/equation?tex=+y+%3D+%7By_1%7D+\" alt=\" y = {y_1} \" eeimg=\"1\"/> 这条直线上任取两点 <img src=\"https://www.zhihu.com/equation?tex=A%5Cleft%28+%7B%7Bx_1%7D%2C%7By_1%7D%7D+%5Cright%29+\" alt=\"A\\left( {{x_1},{y_1}} \\right) \" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=C%5Cleft%28+%7B%7Bx_2%7D%2C%7By_1%7D%7D+%5Cright%29+\" alt=\"C\\left( {{x_2},{y_1}} \\right) \" eeimg=\"1\"/> ，我们可以得到：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+p%5Cleft%28+A+%5Cright%29p%5Cleft%28+%7B%7Bx_2%7D%7C%7By_1%7D%7D+%5Cright%29%7B%5Crm%7B+%3D+%7D%7Dp%5Cleft%28+C+%5Cright%29p%5Cleft%28+%7B%7Bx_1%7D%7C%7By_1%7D%7D+%5Cright%29+%5Ctag%7B46%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} p\\left( A \\right)p\\left( {{x_2}|{y_1}} \\right){\\rm{ = }}p\\left( C \\right)p\\left( {{x_1}|{y_1}} \\right) \\tag{46} \\end{align*}\" eeimg=\"1\"/> </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-49c9a6895d29452b7955c3b7a7fb85c7_b.jpg\" data-size=\"normal\" data-rawwidth=\"283\" data-rawheight=\"235\" class=\"content_image\" width=\"283\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;283&#39; height=&#39;235&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"283\" data-rawheight=\"235\" class=\"content_image lazy\" width=\"283\" data-actualsrc=\"https://pic4.zhimg.com/v2-49c9a6895d29452b7955c3b7a7fb85c7_b.jpg\"/><figcaption>图 1.4</figcaption></figure><p>基于上面的发现，我们可以构造分布 <img src=\"https://www.zhihu.com/equation?tex=+p%5Cleft%28+%7Bx%2Cy%7D+%5Cright%29+\" alt=\" p\\left( {x,y} \\right) \" eeimg=\"1\"/>的马氏链的状态转移矩阵 <img src=\"https://www.zhihu.com/equation?tex=+Q\" alt=\" Q\" eeimg=\"1\"/> 。</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+Q%5Cleft%28+%7BA+%5Cto+B%7D+%5Cright%29+%26%3D+p%5Cleft%28+%7B%7By_B%7D%7C%7Bx_1%7D%7D+%5Cright%29+%26if%5Cleft%28+%7B%7Bx_A%7D+%3D+%7Bx_B%7D+%3D+%7Bx_1%7D%7D+%5Cright%29+%5C%5C+Q%5Cleft%28+%7BA+%5Cto+C%7D+%5Cright%29+%26%3D+p%5Cleft%28+%7B%7Bx_C%7D%7C%7By_1%7D%7D+%5Cright%29+%26if%5Cleft%28+%7B%7By_A%7D+%3D+%7By_C%7D+%3D+%7By_1%7D%7D+%5Cright%29%5C%5C+Q%5Cleft%28+%7BA+%5Cto+D%7D+%5Cright%29+%26%3D+0+%26others+%5Ctag%7B47%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} Q\\left( {A \\to B} \\right) &amp;= p\\left( {{y_B}|{x_1}} \\right) &amp;if\\left( {{x_A} = {x_B} = {x_1}} \\right) \\\\ Q\\left( {A \\to C} \\right) &amp;= p\\left( {{x_C}|{y_1}} \\right) &amp;if\\left( {{y_A} = {y_C} = {y_1}} \\right)\\\\ Q\\left( {A \\to D} \\right) &amp;= 0 &amp;others \\tag{47} \\end{align*}\" eeimg=\"1\"/> </p><p>有了上面的转移矩阵 <img src=\"https://www.zhihu.com/equation?tex=+Q+\" alt=\" Q \" eeimg=\"1\"/> ，很容易验证对于平面任意两点 <img src=\"https://www.zhihu.com/equation?tex=+X%2CY+\" alt=\" X,Y \" eeimg=\"1\"/> ，都满足细致平稳条件。</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+P%5Cleft%28+X+%5Cright%29Q%5Cleft%28+%7BX+%5Cto+Y%7D+%5Cright%29+%3D+P%5Cleft%28+Y+%5Cright%29Q%5Cleft%28+%7BY+%5Cto+X%7D+%5Cright%29+%5Ctag%7B48%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} P\\left( X \\right)Q\\left( {X \\to Y} \\right) = P\\left( Y \\right)Q\\left( {Y \\to X} \\right) \\tag{48} \\end{align*}\" eeimg=\"1\"/> </p><p>所以这个二维空间上的马氏链将收敛到平稳分布 <img src=\"https://www.zhihu.com/equation?tex=+p%5Cleft%28+%7Bx%2Cy%7D+%5Cright%29\" alt=\" p\\left( {x,y} \\right)\" eeimg=\"1\"/> ，称为 Gibbs Sampling 算法。</p><p><b>Algorithm 1.4 Gibbs Sampling算法</b></p><blockquote>1. 随机初始化 <img src=\"https://www.zhihu.com/equation?tex=+%7BX_0%7D+%3D+%7Bx_0%7D%2C%7BY_0%7D+%3D+%7By_0%7D\" alt=\" {X_0} = {x_0},{Y_0} = {y_0}\" eeimg=\"1\"/> <br/>2. 对 <img src=\"https://www.zhihu.com/equation?tex=+t+%3D+0%2C1%2C2%2C+%5Ccdot+%5Ccdot+%5Ccdot+\" alt=\" t = 0,1,2, \\cdot \\cdot \\cdot \" eeimg=\"1\"/> 循环以下过程进行采样<br/>  1）从条件概率分布 <img src=\"https://www.zhihu.com/equation?tex=+p%5Cleft%28+%7By%7C%7Bx_t%7D%7D+%5Cright%29+\" alt=\" p\\left( {y|{x_t}} \\right) \" eeimg=\"1\"/> 中采样得到 <img src=\"https://www.zhihu.com/equation?tex=+%7By_%7Bt+%2B+1%7D%7D+%5Csim+p%5Cleft%28+%7By%7C%7Bx_t%7D%7D+%5Cright%29\" alt=\" {y_{t + 1}} \\sim p\\left( {y|{x_t}} \\right)\" eeimg=\"1\"/> <br/>  2） 从条件概率分布 <img src=\"https://www.zhihu.com/equation?tex=+p%5Cleft%28+%7Bx%7C%7By_%7Bt+%2B+1%7D%7D%7D+%5Cright%29+\" alt=\" p\\left( {x|{y_{t + 1}}} \\right) \" eeimg=\"1\"/> 中采样得到 <img src=\"https://www.zhihu.com/equation?tex=%7Bx_%7Bt+%2B+1%7D%7D+%5Csim+p%5Cleft%28+%7Bx%7C%7By_%7Bt+%2B+1%7D%7D%7D+%5Cright%29\" alt=\"{x_{t + 1}} \\sim p\\left( {x|{y_{t + 1}}} \\right)\" eeimg=\"1\"/> </blockquote><p>整个采样过程中，我们通过轮换坐标轴，得到样本 <img src=\"https://www.zhihu.com/equation?tex=+%5Cleft%28+%7B%7Bx_0%7D%2C%7By_0%7D%7D+%5Cright%29%2C%5Cleft%28+%7B%7Bx_0%7D%2C%7By_1%7D%7D+%5Cright%29%2C%5Cleft%28+%7B%7Bx_1%7D%2C%7By_1%7D%7D+%5Cright%29%2C+%5Ccdot+%5Ccdot+%5Ccdot+\" alt=\" \\left( {{x_0},{y_0}} \\right),\\left( {{x_0},{y_1}} \\right),\\left( {{x_1},{y_1}} \\right), \\cdot \\cdot \\cdot \" eeimg=\"1\"/> ，马氏链收敛后，最终得到的样本就是 <img src=\"https://www.zhihu.com/equation?tex=+p%5Cleft%28+%7Bx%2Cy%7D+%5Cright%29+\" alt=\" p\\left( {x,y} \\right) \" eeimg=\"1\"/> 的样本。当然坐标轴轮换不是必须的，我们也可以每次随机选择一个坐标轴进行采样，在 <img src=\"https://www.zhihu.com/equation?tex=+t+\" alt=\" t \" eeimg=\"1\"/> 时刻，可以在 <img src=\"https://www.zhihu.com/equation?tex=+x+\" alt=\" x \" eeimg=\"1\"/> 轴和 <img src=\"https://www.zhihu.com/equation?tex=+y+\" alt=\" y \" eeimg=\"1\"/> 轴之间随机的选择一个坐标轴，然后按照条件概率做转移。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-842584f4f1bc44f44879dfb842c14a48_b.jpg\" data-size=\"normal\" data-rawwidth=\"300\" data-rawheight=\"298\" class=\"content_image\" width=\"300\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;300&#39; height=&#39;298&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"300\" data-rawheight=\"298\" class=\"content_image lazy\" width=\"300\" data-actualsrc=\"https://pic1.zhimg.com/v2-842584f4f1bc44f44879dfb842c14a48_b.jpg\"/><figcaption>图 1.5</figcaption></figure><p>二维可以很容易推广到高维的情况，在 <img src=\"https://www.zhihu.com/equation?tex=+n+\" alt=\" n \" eeimg=\"1\"/> 维空间中对于概率分布 <img src=\"https://www.zhihu.com/equation?tex=+p%5Cleft%28+%7B%7Bx_1%7D%2C%7Bx_2%7D%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2C%7Bx_n%7D%7D+%5Cright%29\" alt=\" p\\left( {{x_1},{x_2}, \\cdot \\cdot \\cdot ,{x_n}} \\right)\" eeimg=\"1\"/> 。</p><p><b>Algorithm 1.5 n维Gibbs Sampling算法</b></p><blockquote>1. 随机初始化 <img src=\"https://www.zhihu.com/equation?tex=+%5Cleft%5C%7B+%7B%7Bx_i%7D%3Ai+%3D+1%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2Cn%7D+%5Cright%5C%7D\" alt=\" \\left\\{ {{x_i}:i = 1, \\cdot \\cdot \\cdot ,n} \\right\\}\" eeimg=\"1\"/> <br/>2. 对 <img src=\"https://www.zhihu.com/equation?tex=+t+%3D+0%2C1%2C2%2C+%5Ccdot+%5Ccdot+%5Ccdot+\" alt=\" t = 0,1,2, \\cdot \\cdot \\cdot \" eeimg=\"1\"/> 循环以下过程进行采样<br/>  1） <img src=\"https://www.zhihu.com/equation?tex=x_1%5E%7B%5Cleft%28+%7Bt+%2B+1%7D+%5Cright%29%7D+%5Csim+p%5Cleft%28+%7B%7Bx_1%7D%7Cx_2%5E%7B%5Cleft%28+t+%5Cright%29%7D%2Cx_3%5E%7B%5Cleft%28+t+%5Cright%29%7D%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2Cx_n%5E%7B%5Cleft%28+t+%5Cright%29%7D%7D+%5Cright%29\" alt=\"x_1^{\\left( {t + 1} \\right)} \\sim p\\left( {{x_1}|x_2^{\\left( t \\right)},x_3^{\\left( t \\right)}, \\cdot \\cdot \\cdot ,x_n^{\\left( t \\right)}} \\right)\" eeimg=\"1\"/> <br/>  2） <img src=\"https://www.zhihu.com/equation?tex=x_2%5E%7B%5Cleft%28+%7Bt+%2B+1%7D+%5Cright%29%7D+%5Csim+p%5Cleft%28+%7B%7Bx_2%7D%7Cx_1%5E%7B%5Cleft%28+%7Bt%7B%5Crm%7B+%2B+%7D%7D1%7D+%5Cright%29%7D%2Cx_3%5E%7B%5Cleft%28+t+%5Cright%29%7D%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2Cx_n%5E%7B%5Cleft%28+t+%5Cright%29%7D%7D+%5Cright%29\" alt=\"x_2^{\\left( {t + 1} \\right)} \\sim p\\left( {{x_2}|x_1^{\\left( {t{\\rm{ + }}1} \\right)},x_3^{\\left( t \\right)}, \\cdot \\cdot \\cdot ,x_n^{\\left( t \\right)}} \\right)\" eeimg=\"1\"/> <br/>  3） <img src=\"https://www.zhihu.com/equation?tex=%5Ccdot+%5Ccdot+%5Ccdot+\" alt=\"\\cdot \\cdot \\cdot \" eeimg=\"1\"/> <br/>  4） <img src=\"https://www.zhihu.com/equation?tex=+x_j%5E%7B%5Cleft%28+%7Bt+%2B+1%7D+%5Cright%29%7D+%5Csim+p%5Cleft%28+%7B%7Bx_j%7D%7Cx_1%5E%7B%5Cleft%28+%7Bt%7B%5Crm%7B+%2B+%7D%7D1%7D+%5Cright%29%7D%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2Cx_%7Bj+-+1%7D%5E%7B%5Cleft%28+%7Bt+%2B+1%7D+%5Cright%29%7D%2Cx_j%5E%7B%5Cleft%28+t+%5Cright%29%7D+%5Ccdot+%5Ccdot+%5Ccdot+%2Cx_n%5E%7B%5Cleft%28+t+%5Cright%29%7D%7D+%5Cright%29\" alt=\" x_j^{\\left( {t + 1} \\right)} \\sim p\\left( {{x_j}|x_1^{\\left( {t{\\rm{ + }}1} \\right)}, \\cdot \\cdot \\cdot ,x_{j - 1}^{\\left( {t + 1} \\right)},x_j^{\\left( t \\right)} \\cdot \\cdot \\cdot ,x_n^{\\left( t \\right)}} \\right)\" eeimg=\"1\"/> <br/>  5） <img src=\"https://www.zhihu.com/equation?tex=+%5Ccdot+%5Ccdot+%5Ccdot\" alt=\" \\cdot \\cdot \\cdot\" eeimg=\"1\"/> <br/>  6） <img src=\"https://www.zhihu.com/equation?tex=x_n%5E%7B%5Cleft%28+%7Bt+%2B+1%7D+%5Cright%29%7D+%5Csim+p%5Cleft%28+%7B%7Bx_n%7D%7Cx_1%5E%7B%5Cleft%28+%7Bt+%2B+1%7D+%5Cright%29%7D%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2Cx_%7Bn+-+1%7D%5E%7B%5Cleft%28+%7Bt+%2B+1%7D+%5Cright%29%7D%7D+%5Cright%29\" alt=\"x_n^{\\left( {t + 1} \\right)} \\sim p\\left( {{x_n}|x_1^{\\left( {t + 1} \\right)}, \\cdot \\cdot \\cdot ,x_{n - 1}^{\\left( {t + 1} \\right)}} \\right)\" eeimg=\"1\"/> </blockquote><p>给一个<a href=\"https://zhuanlan.zhihu.com/p/45164073\" class=\"internal\">二维高斯分布的Gibbs Sampling</a>例子。</p><h2><b>1.11 EM算法</b></h2><p>我们先介绍凸函数的概念， <img src=\"https://www.zhihu.com/equation?tex=f+\" alt=\"f \" eeimg=\"1\"/> 的定义域是实数集，若 <img src=\"https://www.zhihu.com/equation?tex=+x+%5Cin+R\" alt=\" x \\in R\" eeimg=\"1\"/> 且 <img src=\"https://www.zhihu.com/equation?tex=+f%27%27%5Cleft%28+x+%5Cright%29+%5Cge+0+\" alt=\" f&#39;&#39;\\left( x \\right) \\ge 0 \" eeimg=\"1\"/> ，则 <img src=\"https://www.zhihu.com/equation?tex=+f+\" alt=\" f \" eeimg=\"1\"/> 是凸函数，若 <img src=\"https://www.zhihu.com/equation?tex=+f%27%27%5Cleft%28+x+%5Cright%29+%3E+0+\" alt=\" f&#39;&#39;\\left( x \\right) &gt; 0 \" eeimg=\"1\"/> ，则 <img src=\"https://www.zhihu.com/equation?tex=+f+\" alt=\" f \" eeimg=\"1\"/> 是严格凸函数；若 <img src=\"https://www.zhihu.com/equation?tex=%7B%5Crm%7Bx%7D%7D+\" alt=\"{\\rm{x}} \" eeimg=\"1\"/> 是向量且 hessian 矩阵 <img src=\"https://www.zhihu.com/equation?tex=+H+\" alt=\" H \" eeimg=\"1\"/> 是半正定矩阵，则 <img src=\"https://www.zhihu.com/equation?tex=+f+\" alt=\" f \" eeimg=\"1\"/> 是凸函数，若 <img src=\"https://www.zhihu.com/equation?tex=+H+\" alt=\" H \" eeimg=\"1\"/> 是正定矩阵，则 <img src=\"https://www.zhihu.com/equation?tex=+f+\" alt=\" f \" eeimg=\"1\"/> 是严格凸函数。</p><p><b>定理1.3（Jensen不等式）</b> <img src=\"https://www.zhihu.com/equation?tex=f+\" alt=\"f \" eeimg=\"1\"/> 的定义域是实数集，且是凸函数，则有：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+E%5Cleft%5B+%7Bf%5Cleft%28+X+%5Cright%29%7D+%5Cright%5D+%5Cge+f%5Cleft%28+%7BE%5Cleft%5B+X+%5Cright%5D%7D+%5Cright%29+%5Ctag%7B49%7D+%5Cend%7Balign%2A%7D+\" alt=\"\\begin{align*} E\\left[ {f\\left( X \\right)} \\right] \\ge f\\left( {E\\left[ X \\right]} \\right) \\tag{49} \\end{align*} \" eeimg=\"1\"/> </p><p>如果 <img src=\"https://www.zhihu.com/equation?tex=+f+\" alt=\" f \" eeimg=\"1\"/> 是严格凸函数，只有当 <img src=\"https://www.zhihu.com/equation?tex=+X+\" alt=\" X \" eeimg=\"1\"/> 是常量，公式 (49) 等式成立即 <img src=\"https://www.zhihu.com/equation?tex=+E%5Cleft%5B+%7Bf%5Cleft%28+X+%5Cright%29%7D+%5Cright%5D%7B%5Crm%7B+%3D+%7D%7Df%5Cleft%28+%7BE%5Cleft%5B+X+%5Cright%5D%7D+%5Cright%29\" alt=\" E\\left[ {f\\left( X \\right)} \\right]{\\rm{ = }}f\\left( {E\\left[ X \\right]} \\right)\" eeimg=\"1\"/> 。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-b1f111a136774ae4210c5cd9befc861e_b.jpg\" data-size=\"normal\" data-rawwidth=\"457\" data-rawheight=\"378\" class=\"origin_image zh-lightbox-thumb\" width=\"457\" data-original=\"https://pic3.zhimg.com/v2-b1f111a136774ae4210c5cd9befc861e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;457&#39; height=&#39;378&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"457\" data-rawheight=\"378\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"457\" data-original=\"https://pic3.zhimg.com/v2-b1f111a136774ae4210c5cd9befc861e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-b1f111a136774ae4210c5cd9befc861e_b.jpg\"/><figcaption>图 1.6</figcaption></figure><p>假设训练集 <img src=\"https://www.zhihu.com/equation?tex=+%5Cleft%5C%7B+%7B%7Bx%5E%7B%5Cleft%28+1+%5Cright%29%7D%7D%2C%7Bx%5E%7B%5Cleft%28+2+%5Cright%29%7D%7D%2C+%5Ccdot+%5Ccdot+%5Ccdot+%2C%7Bx%5E%7B%5Cleft%28+m+%5Cright%29%7D%7D%7D+%5Cright%5C%7D\" alt=\" \\left\\{ {{x^{\\left( 1 \\right)}},{x^{\\left( 2 \\right)}}, \\cdot \\cdot \\cdot ,{x^{\\left( m \\right)}}} \\right\\}\" eeimg=\"1\"/> ，每个样本相互独立，我们需要估计模型 <img src=\"https://www.zhihu.com/equation?tex=+p%5Cleft%28+%7Bx%2Cz%7D+%5Cright%29+\" alt=\" p\\left( {x,z} \\right) \" eeimg=\"1\"/> 的参数 <img src=\"https://www.zhihu.com/equation?tex=+%5Ctheta\" alt=\" \\theta\" eeimg=\"1\"/> ，由于含有隐变量 <img src=\"https://www.zhihu.com/equation?tex=+z\" alt=\" z\" eeimg=\"1\"/> ，所以很难直接用最大似然求解，如果 <img src=\"https://www.zhihu.com/equation?tex=+z+\" alt=\" z \" eeimg=\"1\"/> 已知，那么就可以用最大似然求解。</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+l%5Cleft%28+%5Ctheta+%5Cright%29+%26%3D+%5Csum%5Climits_%7Bi+%3D+1%7D%5Em+%7B%5Clog+p%5Cleft%28+%7Bx%3B%5Ctheta+%7D+%5Cright%29%7D+%5C%5C+%26%3D+%5Csum%5Climits_%7Bi+%3D+1%7D%5Em+%7B%5Clog+%5Csum%5Climits_z+%7Bp%5Cleft%28+%7Bx%2Cz%3B%5Ctheta+%7D+%5Cright%29%7D+%7D+%5Ctag%7B50%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} l\\left( \\theta \\right) &amp;= \\sum\\limits_{i = 1}^m {\\log p\\left( {x;\\theta } \\right)} \\\\ &amp;= \\sum\\limits_{i = 1}^m {\\log \\sum\\limits_z {p\\left( {x,z;\\theta } \\right)} } \\tag{50} \\end{align*}\" eeimg=\"1\"/> </p><p>其实我们的目标是找到 <img src=\"https://www.zhihu.com/equation?tex=+z+\" alt=\" z \" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=+%5Ctheta+\" alt=\" \\theta \" eeimg=\"1\"/> 使 <img src=\"https://www.zhihu.com/equation?tex=+l%5Cleft%28+%5Ctheta+%5Cright%29+\" alt=\" l\\left( \\theta \\right) \" eeimg=\"1\"/> 最大，也就是分别对 <img src=\"https://www.zhihu.com/equation?tex=+z\" alt=\" z\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=+%5Ctheta+\" alt=\" \\theta \" eeimg=\"1\"/> 求偏导，然后令其为 <img src=\"https://www.zhihu.com/equation?tex=0\" alt=\"0\" eeimg=\"1\"/> ，理想是美好的，现实是残酷的，公式 (50) 求偏导后变的很复杂，求导前要是能把求和符号从对数函数中提出来就好了。EM 算法可以有效地解决这个问题，引入 <img src=\"https://www.zhihu.com/equation?tex=%7BQ_i%7D%5Cleft%28+%7B%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%7D+%5Cright%29+\" alt=\"{Q_i}\\left( {{z^{\\left( i \\right)}}} \\right) \" eeimg=\"1\"/> 表示 <img src=\"https://www.zhihu.com/equation?tex=%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D\" alt=\"{z^{\\left( i \\right)}}\" eeimg=\"1\"/> 的概率分布（ <img src=\"https://www.zhihu.com/equation?tex=%5Csum%5Cnolimits_z+%7B%7BQ_i%7D%5Cleft%28+%7B%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%7D+%5Cright%29%7D+%3D+1%2C%7BQ_i%7D%5Cleft%28+%7B%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%7D+%5Cright%29+%5Cge+0\" alt=\"\\sum\\nolimits_z {{Q_i}\\left( {{z^{\\left( i \\right)}}} \\right)} = 1,{Q_i}\\left( {{z^{\\left( i \\right)}}} \\right) \\ge 0\" eeimg=\"1\"/> ）。由公式(50)可得：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Csum%5Climits_i+%7B%5Clog+p%5Cleft%28+%7B%7Bx%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%3B%5Ctheta+%7D+%5Cright%29%7D+%26%3D+%5Csum%5Climits_i+%7B%5Clog+%5Csum%5Climits_%7B%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%7D+%7Bp%5Cleft%28+%7B%7Bx%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%2C%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%3B%5Ctheta+%7D+%5Cright%29%7D+%7D+%5C%5C+%26%3D+%5Csum%5Climits_i+%7B%5Clog+%5Csum%5Climits_%7B%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%7D+%7B%7BQ_i%7D%5Cleft%28+%7B%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%7D+%5Cright%29%5Cfrac%7B%7Bp%5Cleft%28+%7B%7Bx%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%2C%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%3B%5Ctheta+%7D+%5Cright%29%7D%7D%7B%7B%7BQ_i%7D%5Cleft%28+%7B%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%7D+%5Cright%29%7D%7D%7D+%7D+%5C%5C+%26+%5Cge+%5Csum%5Climits_i+%7B%5Csum%5Climits_%7B%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%7D+%7B%7BQ_i%7D%5Cleft%28+%7B%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%7D+%5Cright%29%5Clog+%5Cfrac%7B%7Bp%5Cleft%28+%7B%7Bx%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%2C%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%3B%5Ctheta+%7D+%5Cright%29%7D%7D%7B%7B%7BQ_i%7D%5Cleft%28+%7B%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%7D+%5Cright%29%7D%7D%7D+%7D+%5Ctag%7B51%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} \\sum\\limits_i {\\log p\\left( {{x^{\\left( i \\right)}};\\theta } \\right)} &amp;= \\sum\\limits_i {\\log \\sum\\limits_{{z^{\\left( i \\right)}}} {p\\left( {{x^{\\left( i \\right)}},{z^{\\left( i \\right)}};\\theta } \\right)} } \\\\ &amp;= \\sum\\limits_i {\\log \\sum\\limits_{{z^{\\left( i \\right)}}} {{Q_i}\\left( {{z^{\\left( i \\right)}}} \\right)\\frac{{p\\left( {{x^{\\left( i \\right)}},{z^{\\left( i \\right)}};\\theta } \\right)}}{{{Q_i}\\left( {{z^{\\left( i \\right)}}} \\right)}}} } \\\\ &amp; \\ge \\sum\\limits_i {\\sum\\limits_{{z^{\\left( i \\right)}}} {{Q_i}\\left( {{z^{\\left( i \\right)}}} \\right)\\log \\frac{{p\\left( {{x^{\\left( i \\right)}},{z^{\\left( i \\right)}};\\theta } \\right)}}{{{Q_i}\\left( {{z^{\\left( i \\right)}}} \\right)}}} } \\tag{51} \\end{align*}\" eeimg=\"1\"/> </p><p>最后一步是利用 Jensen 不等式， <img src=\"https://www.zhihu.com/equation?tex=f+%3D+%5Clog+%5Cleft%28+x+%5Cright%29%EF%BC%8Cf%27%27+%3D+-+%5Cfrac%7B1%7D%7B%7B%7Bx%5E2%7D%7D%7D+%3C+0\" alt=\"f = \\log \\left( x \\right)，f&#39;&#39; = - \\frac{1}{{{x^2}}} &lt; 0\" eeimg=\"1\"/> ，所以 <img src=\"https://www.zhihu.com/equation?tex=+f+\" alt=\" f \" eeimg=\"1\"/> 是凹函数， <img src=\"https://www.zhihu.com/equation?tex=%5Csum%5Climits_%7B%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%7D+%7B%7BQ_i%7D%5Cleft%28+%7B%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%7D+%5Cright%29%5Cfrac%7B%7Bp%5Cleft%28+%7B%7Bx%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%2C%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%3B%5Ctheta+%7D+%5Cright%29%7D%7D%7B%7B%7BQ_i%7D%5Cleft%28+%7B%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%7D+%5Cright%29%7D%7D%7D+\" alt=\"\\sum\\limits_{{z^{\\left( i \\right)}}} {{Q_i}\\left( {{z^{\\left( i \\right)}}} \\right)\\frac{{p\\left( {{x^{\\left( i \\right)}},{z^{\\left( i \\right)}};\\theta } \\right)}}{{{Q_i}\\left( {{z^{\\left( i \\right)}}} \\right)}}} \" eeimg=\"1\"/> 是 <img src=\"https://www.zhihu.com/equation?tex=+%5Cfrac%7B%7Bp%5Cleft%28+%7B%7Bx%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%2C%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%3B%5Ctheta+%7D+%5Cright%29%7D%7D%7B%7B%7BQ_i%7D%5Cleft%28+%7B%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%7D+%5Cright%29%7D%7D\" alt=\" \\frac{{p\\left( {{x^{\\left( i \\right)}},{z^{\\left( i \\right)}};\\theta } \\right)}}{{{Q_i}\\left( {{z^{\\left( i \\right)}}} \\right)}}\" eeimg=\"1\"/> 的期望，所以有：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+f%5Cleft%28+%7BE%5Cleft%5B+%7B%5Cfrac%7B%7Bp%5Cleft%28+%7B%7Bx%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%2C%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%3B%5Ctheta+%7D+%5Cright%29%7D%7D%7B%7B%7BQ_i%7D%5Cleft%28+%7B%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%7D+%5Cright%29%7D%7D%7D+%5Cright%5D%7D+%5Cright%29+%5Cge+E%5Cleft%5B+%7Bf%5Cleft%28+%7B%5Cfrac%7B%7Bp%5Cleft%28+%7B%7Bx%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%2C%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%3B%5Ctheta+%7D+%5Cright%29%7D%7D%7B%7B%7BQ_i%7D%5Cleft%28+%7B%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%7D+%5Cright%29%7D%7D%7D+%5Cright%29%7D+%5Cright%5D+%5Ctag%7B52%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} f\\left( {E\\left[ {\\frac{{p\\left( {{x^{\\left( i \\right)}},{z^{\\left( i \\right)}};\\theta } \\right)}}{{{Q_i}\\left( {{z^{\\left( i \\right)}}} \\right)}}} \\right]} \\right) \\ge E\\left[ {f\\left( {\\frac{{p\\left( {{x^{\\left( i \\right)}},{z^{\\left( i \\right)}};\\theta } \\right)}}{{{Q_i}\\left( {{z^{\\left( i \\right)}}} \\right)}}} \\right)} \\right] \\tag{52} \\end{align*}\" eeimg=\"1\"/> </p><p>由公式 (51) 可知，我们可以不断地最大化下界，以提高 <img src=\"https://www.zhihu.com/equation?tex=+l%5Cleft%28+%5Ctheta+%5Cright%29\" alt=\" l\\left( \\theta \\right)\" eeimg=\"1\"/> ，最终达到最大值。如果固定 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta+\" alt=\"\\theta \" eeimg=\"1\"/> ，那么 <img src=\"https://www.zhihu.com/equation?tex=l%5Cleft%28+%5Ctheta+%5Cright%29+\" alt=\"l\\left( \\theta \\right) \" eeimg=\"1\"/> 的下界就取决于 <img src=\"https://www.zhihu.com/equation?tex=+%7BQ_i%7D%5Cleft%28+%7B%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%7D+%5Cright%29\" alt=\" {Q_i}\\left( {{z^{\\left( i \\right)}}} \\right)\" eeimg=\"1\"/> ，可以通过调整这个概率，使得下界不断地上升逼近 <img src=\"https://www.zhihu.com/equation?tex=+l%5Cleft%28+%5Ctheta+%5Cright%29\" alt=\" l\\left( \\theta \\right)\" eeimg=\"1\"/> ，最终相等，然后固定 <img src=\"https://www.zhihu.com/equation?tex=+%7BQ_i%7D%5Cleft%28+%7B%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%7D+%5Cright%29\" alt=\" {Q_i}\\left( {{z^{\\left( i \\right)}}} \\right)\" eeimg=\"1\"/> ，调整 <img src=\"https://www.zhihu.com/equation?tex=+%5Ctheta+\" alt=\" \\theta \" eeimg=\"1\"/> ，使下界达到最大值，此时 <img src=\"https://www.zhihu.com/equation?tex=+%5Ctheta+\" alt=\" \\theta \" eeimg=\"1\"/> 为新的值，再固定 <img src=\"https://www.zhihu.com/equation?tex=+%5Ctheta+\" alt=\" \\theta \" eeimg=\"1\"/> ，调整 <img src=\"https://www.zhihu.com/equation?tex=+%7BQ_i%7D%5Cleft%28+%7B%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%7D+%5Cright%29\" alt=\" {Q_i}\\left( {{z^{\\left( i \\right)}}} \\right)\" eeimg=\"1\"/> ，反复直到收敛到 <img src=\"https://www.zhihu.com/equation?tex=+l%5Cleft%28+%5Ctheta+%5Cright%29\" alt=\" l\\left( \\theta \\right)\" eeimg=\"1\"/> 的最大值。现在我们有两个问题需要证明，1. 下界何时等于 <img src=\"https://www.zhihu.com/equation?tex=+l%5Cleft%28+%5Ctheta+%5Cright%29\" alt=\" l\\left( \\theta \\right)\" eeimg=\"1\"/> ；2. 为什么可以收敛到最大值。</p><p>第一个问题，由 Jensen 不等式定理中等式成立条件可知， <img src=\"https://www.zhihu.com/equation?tex=X+\" alt=\"X \" eeimg=\"1\"/> 为常量，即：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Cfrac%7B%7Bp%5Cleft%28+%7B%7Bx%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%2C%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%3B%5Ctheta+%7D+%5Cright%29%7D%7D%7B%7B%7BQ_i%7D%5Cleft%28+%7B%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%7D+%5Cright%29%7D%7D+%3D+c+%5Ctag%7B53%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} \\frac{{p\\left( {{x^{\\left( i \\right)}},{z^{\\left( i \\right)}};\\theta } \\right)}}{{{Q_i}\\left( {{z^{\\left( i \\right)}}} \\right)}} = c \\tag{53} \\end{align*}\" eeimg=\"1\"/> </p><p>再由 <img src=\"https://www.zhihu.com/equation?tex=+%5Csum%5Cnolimits_z+%7B%7BQ_i%7D%5Cleft%28+%7B%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%7D+%5Cright%29%7D+%3D+1+\" alt=\" \\sum\\nolimits_z {{Q_i}\\left( {{z^{\\left( i \\right)}}} \\right)} = 1 \" eeimg=\"1\"/> 得：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Csum%5Cnolimits_z+%7B%5Cfrac%7B%7Bp%5Cleft%28+%7B%7Bx%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%2C%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%3B%5Ctheta+%7D+%5Cright%29%7D%7D%7Bc%7D%7D+%26%3D+1+%5CRightarrow+c+%3D+%5Csum%5Cnolimits_z+%7Bp%5Cleft%28+%7B%7Bx%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%2C%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%3B%5Ctheta+%7D+%5Cright%29%7D+%5C%5C+%7BQ_i%7D%5Cleft%28+%7B%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%7D+%5Cright%29+%26%3D+%5Cfrac%7B%7Bp%5Cleft%28+%7B%7Bx%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%2C%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%3B%5Ctheta+%7D+%5Cright%29%7D%7D%7B%7B%5Csum%5Cnolimits_z+%7Bp%5Cleft%28+%7B%7Bx%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%2C%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%3B%5Ctheta+%7D+%5Cright%29%7D+%7D%7D%7B%5Crm%7B+%3D+%7D%7D%5Cfrac%7B%7Bp%5Cleft%28+%7B%7Bx%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%2C%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%3B%5Ctheta+%7D+%5Cright%29%7D%7D%7B%7Bp%5Cleft%28+%7B%7Bx%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%3B%5Ctheta+%7D+%5Cright%29%7D%7D%7B%5Crm%7B+%3D+%7D%7Dp%5Cleft%28+%7B%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%7C%7Bx%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%2C%5Ctheta+%7D+%5Cright%29+%5Ctag%7B54%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} \\sum\\nolimits_z {\\frac{{p\\left( {{x^{\\left( i \\right)}},{z^{\\left( i \\right)}};\\theta } \\right)}}{c}} &amp;= 1 \\Rightarrow c = \\sum\\nolimits_z {p\\left( {{x^{\\left( i \\right)}},{z^{\\left( i \\right)}};\\theta } \\right)} \\\\ {Q_i}\\left( {{z^{\\left( i \\right)}}} \\right) &amp;= \\frac{{p\\left( {{x^{\\left( i \\right)}},{z^{\\left( i \\right)}};\\theta } \\right)}}{{\\sum\\nolimits_z {p\\left( {{x^{\\left( i \\right)}},{z^{\\left( i \\right)}};\\theta } \\right)} }}{\\rm{ = }}\\frac{{p\\left( {{x^{\\left( i \\right)}},{z^{\\left( i \\right)}};\\theta } \\right)}}{{p\\left( {{x^{\\left( i \\right)}};\\theta } \\right)}}{\\rm{ = }}p\\left( {{z^{\\left( i \\right)}}|{x^{\\left( i \\right)}},\\theta } \\right) \\tag{54} \\end{align*}\" eeimg=\"1\"/> </p><p>下面我们先给出 EM 算法，然后再讨论第二个问题，E步：固定 <img src=\"https://www.zhihu.com/equation?tex=+%5Ctheta+\" alt=\" \\theta \" eeimg=\"1\"/> ，根据公式(53)选择 <img src=\"https://www.zhihu.com/equation?tex=+%7BQ_i%7D+\" alt=\" {Q_i} \" eeimg=\"1\"/> 使得下界等于 <img src=\"https://www.zhihu.com/equation?tex=+l%5Cleft%28+%5Ctheta+%5Cright%29\" alt=\" l\\left( \\theta \\right)\" eeimg=\"1\"/> ，M步：最大化下界，得到新的 <img src=\"https://www.zhihu.com/equation?tex=+%5Ctheta+\" alt=\" \\theta \" eeimg=\"1\"/> 值。EM算法如下：</p><p><b>Algorithm 1.6 EM算法</b></p><blockquote>1. 初始化 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> <br/>2. Repeat until convergence<br/>  1） (E-step) For each i,set<br/> <img src=\"https://www.zhihu.com/equation?tex=%7BQ_i%7D%5Cleft%28+%7B%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%7D+%5Cright%29%7B%5Crm%7B+%3D+%7D%7Dp%5Cleft%28+%7B%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%7C%7Bx%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%2C%5Ctheta+%7D+%5Cright%29\" alt=\"{Q_i}\\left( {{z^{\\left( i \\right)}}} \\right){\\rm{ = }}p\\left( {{z^{\\left( i \\right)}}|{x^{\\left( i \\right)}},\\theta } \\right)\" eeimg=\"1\"/> <br/>  2） (M-step) Set<br/> <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta+%3A+%3D+%5Cmathop+%7B%5Carg+%5Cmax+%7D%5Climits_%5Ctheta+%5Csum%5Climits_i+%7B%5Csum%5Climits_%7B%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%7D+%7B%7BQ_i%7D%5Cleft%28+%7B%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%7D+%5Cright%29%5Clog+%5Cfrac%7B%7Bp%5Cleft%28+%7B%7Bx%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%2C%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%3B%5Ctheta+%7D+%5Cright%29%7D%7D%7B%7B%7BQ_i%7D%5Cleft%28+%7B%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%7D+%5Cright%29%7D%7D%7D+%7D+\" alt=\"\\theta : = \\mathop {\\arg \\max }\\limits_\\theta \\sum\\limits_i {\\sum\\limits_{{z^{\\left( i \\right)}}} {{Q_i}\\left( {{z^{\\left( i \\right)}}} \\right)\\log \\frac{{p\\left( {{x^{\\left( i \\right)}},{z^{\\left( i \\right)}};\\theta } \\right)}}{{{Q_i}\\left( {{z^{\\left( i \\right)}}} \\right)}}} } \" eeimg=\"1\"/> </blockquote><p>现在我们开始讨论第二个问题， <img src=\"https://www.zhihu.com/equation?tex=%7B%5Ctheta+%5E%7B%5Cleft%28+t+%5Cright%29%7D%7D+%E5%92%8C+%7B%5Ctheta+%5E%7B%5Cleft%28+%7Bt%7B%5Crm%7B+%2B+%7D%7D1%7D+%5Cright%29%7D%7D+\" alt=\"{\\theta ^{\\left( t \\right)}} 和 {\\theta ^{\\left( {t{\\rm{ + }}1} \\right)}} \" eeimg=\"1\"/> 是EM迭代过程的参数估计，我们需要证明 <img src=\"https://www.zhihu.com/equation?tex=l%5Cleft%28+%7B%7B%5Ctheta+%5E%7B%5Cleft%28+t+%5Cright%29%7D%7D%7D+%5Cright%29+%5Cle+l%5Cleft%28+%7B%7B%5Ctheta+%5E%7B%5Cleft%28+%7Bt+%2B+1%7D+%5Cright%29%7D%7D%7D+%5Cright%29+\" alt=\"l\\left( {{\\theta ^{\\left( t \\right)}}} \\right) \\le l\\left( {{\\theta ^{\\left( {t + 1} \\right)}}} \\right) \" eeimg=\"1\"/> ，也就是EM算法是单调地提高 <img src=\"https://www.zhihu.com/equation?tex=l%5Cleft%28+%5Ctheta+%5Cright%29+%EF%BC%8CQ_i%5E%7B%5Cleft%28+t+%5Cright%29%7D%5Cleft%28+%7B%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%7D+%5Cright%29%7B%5Crm%7B+%3D+%7D%7Dp%5Cleft%28+%7B%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%7C%7Bx%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%2C%7B%5Ctheta+%5E%7B%5Cleft%28+t+%5Cright%29%7D%7D%7D+%5Cright%29\" alt=\"l\\left( \\theta \\right) ，Q_i^{\\left( t \\right)}\\left( {{z^{\\left( i \\right)}}} \\right){\\rm{ = }}p\\left( {{z^{\\left( i \\right)}}|{x^{\\left( i \\right)}},{\\theta ^{\\left( t \\right)}}} \\right)\" eeimg=\"1\"/> 。</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+l%5Cleft%28+%7B%7B%5Ctheta+%5E%7B%5Cleft%28+t+%5Cright%29%7D%7D%7D+%5Cright%29+%3D+%5Csum%5Climits_i+%7B%5Csum%5Climits_%7B%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%7D+%7BQ_i%5E%7B%5Cleft%28+t+%5Cright%29%7D%5Cleft%28+%7B%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%7D+%5Cright%29%5Clog+%5Cfrac%7B%7Bp%5Cleft%28+%7B%7Bx%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%2C%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%3B%7B%5Ctheta+%5E%7B%5Cleft%28+t+%5Cright%29%7D%7D%7D+%5Cright%29%7D%7D%7B%7BQ_i%5E%7B%5Cleft%28+t+%5Cright%29%7D%5Cleft%28+%7B%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%7D+%5Cright%29%7D%7D%7D+%7D+%5Ctag%7B55%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} l\\left( {{\\theta ^{\\left( t \\right)}}} \\right) = \\sum\\limits_i {\\sum\\limits_{{z^{\\left( i \\right)}}} {Q_i^{\\left( t \\right)}\\left( {{z^{\\left( i \\right)}}} \\right)\\log \\frac{{p\\left( {{x^{\\left( i \\right)}},{z^{\\left( i \\right)}};{\\theta ^{\\left( t \\right)}}} \\right)}}{{Q_i^{\\left( t \\right)}\\left( {{z^{\\left( i \\right)}}} \\right)}}} } \\tag{55} \\end{align*}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+l%5Cleft%28+%7B%7B%5Ctheta+%5E%7B%5Cleft%28+%7Bt%7B%5Crm%7B+%2B+%7D%7D1%7D+%5Cright%29%7D%7D%7D+%5Cright%29+%26%5Cge+%5Csum%5Climits_i+%7B%5Csum%5Climits_%7B%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%7D+%7BQ_i%5E%7B%5Cleft%28+t+%5Cright%29%7D%5Cleft%28+%7B%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%7D+%5Cright%29%5Clog+%5Cfrac%7B%7Bp%5Cleft%28+%7B%7Bx%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%2C%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%3B%7B%5Ctheta+%5E%7B%5Cleft%28+%7Bt%7B%5Crm%7B+%2B+%7D%7D1%7D+%5Cright%29%7D%7D%7D+%5Cright%29%7D%7D%7B%7BQ_i%5E%7B%5Cleft%28+t+%5Cright%29%7D%5Cleft%28+%7B%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%7D+%5Cright%29%7D%7D%7D+%7D+%5C%5C+%26%5Cge+%5Csum%5Climits_i+%7B%5Csum%5Climits_%7B%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%7D+%7BQ_i%5E%7B%5Cleft%28+t+%5Cright%29%7D%5Cleft%28+%7B%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%7D+%5Cright%29%5Clog+%5Cfrac%7B%7Bp%5Cleft%28+%7B%7Bx%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%2C%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%3B%7B%5Ctheta+%5E%7B%5Cleft%28+t+%5Cright%29%7D%7D%7D+%5Cright%29%7D%7D%7B%7BQ_i%5E%7B%5Cleft%28+t+%5Cright%29%7D%5Cleft%28+%7B%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%7D+%5Cright%29%7D%7D%7D+%7D+%5C%5C+%26%3D+l%5Cleft%28+%7B%7B%5Ctheta+%5E%7B%5Cleft%28+t+%5Cright%29%7D%7D%7D+%5Cright%29+%5Ctag%7B56%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} l\\left( {{\\theta ^{\\left( {t{\\rm{ + }}1} \\right)}}} \\right) &amp;\\ge \\sum\\limits_i {\\sum\\limits_{{z^{\\left( i \\right)}}} {Q_i^{\\left( t \\right)}\\left( {{z^{\\left( i \\right)}}} \\right)\\log \\frac{{p\\left( {{x^{\\left( i \\right)}},{z^{\\left( i \\right)}};{\\theta ^{\\left( {t{\\rm{ + }}1} \\right)}}} \\right)}}{{Q_i^{\\left( t \\right)}\\left( {{z^{\\left( i \\right)}}} \\right)}}} } \\\\ &amp;\\ge \\sum\\limits_i {\\sum\\limits_{{z^{\\left( i \\right)}}} {Q_i^{\\left( t \\right)}\\left( {{z^{\\left( i \\right)}}} \\right)\\log \\frac{{p\\left( {{x^{\\left( i \\right)}},{z^{\\left( i \\right)}};{\\theta ^{\\left( t \\right)}}} \\right)}}{{Q_i^{\\left( t \\right)}\\left( {{z^{\\left( i \\right)}}} \\right)}}} } \\\\ &amp;= l\\left( {{\\theta ^{\\left( t \\right)}}} \\right) \\tag{56} \\end{align*}\" eeimg=\"1\"/> </p><p>第一个不等式是因为：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+l%5Cleft%28+%5Ctheta+%5Cright%29+%5Cge+%5Csum%5Climits_i+%7B%5Csum%5Climits_%7B%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%7D+%7B%7BQ_i%7D%5Cleft%28+%7B%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%7D+%5Cright%29%5Clog+%5Cfrac%7B%7Bp%5Cleft%28+%7B%7Bx%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%2C%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%3B%5Ctheta+%7D+%5Cright%29%7D%7D%7B%7B%7BQ_i%7D%5Cleft%28+%7B%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%7D+%5Cright%29%7D%7D%7D+%7D+%5Ctag%7B57%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} l\\left( \\theta \\right) \\ge \\sum\\limits_i {\\sum\\limits_{{z^{\\left( i \\right)}}} {{Q_i}\\left( {{z^{\\left( i \\right)}}} \\right)\\log \\frac{{p\\left( {{x^{\\left( i \\right)}},{z^{\\left( i \\right)}};\\theta } \\right)}}{{{Q_i}\\left( {{z^{\\left( i \\right)}}} \\right)}}} } \\tag{57} \\end{align*}\" eeimg=\"1\"/> </p><p>公式(57)中， <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta+%3D+%7B%5Ctheta+%5E%7B%5Cleft%28+%7Bt+%2B+1%7D+%5Cright%29%7D%7D%EF%BC%8C%7BQ_i%7D+%3D+Q_i%5E%7B%5Cleft%28+t+%5Cright%29%7D\" alt=\"\\theta = {\\theta ^{\\left( {t + 1} \\right)}}，{Q_i} = Q_i^{\\left( t \\right)}\" eeimg=\"1\"/> 。</p><p>第二个不等式是因为 <img src=\"https://www.zhihu.com/equation?tex=%7B%5Ctheta+%5E%7B%5Cleft%28+%7Bt+%2B+1%7D+%5Cright%29%7D%7D\" alt=\"{\\theta ^{\\left( {t + 1} \\right)}}\" eeimg=\"1\"/> 是为了</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+%5Cmathop+%7B%5Carg+%5Cmax+%7D%5Climits_%5Ctheta+%5Csum%5Climits_i+%7B%5Csum%5Climits_%7B%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%7D+%7B%7BQ_i%7D%5Cleft%28+%7B%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%7D+%5Cright%29%5Clog+%5Cfrac%7B%7Bp%5Cleft%28+%7B%7Bx%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%2C%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%3B%5Ctheta+%7D+%5Cright%29%7D%7D%7B%7B%7BQ_i%7D%5Cleft%28+%7B%7Bz%5E%7B%5Cleft%28+i+%5Cright%29%7D%7D%7D+%5Cright%29%7D%7D%7D+%7D+%5Ctag%7B58%7D+%5Cend%7Balign%2A%7D\" alt=\"\\begin{align*} \\mathop {\\arg \\max }\\limits_\\theta \\sum\\limits_i {\\sum\\limits_{{z^{\\left( i \\right)}}} {{Q_i}\\left( {{z^{\\left( i \\right)}}} \\right)\\log \\frac{{p\\left( {{x^{\\left( i \\right)}},{z^{\\left( i \\right)}};\\theta } \\right)}}{{{Q_i}\\left( {{z^{\\left( i \\right)}}} \\right)}}} } \\tag{58} \\end{align*}\" eeimg=\"1\"/> </p><p><a href=\"https://zhuanlan.zhihu.com/p/42860420\" class=\"internal\">下篇</a></p><p></p><p></p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "主题模型", 
                    "tagLink": "https://api.zhihu.com/topics/19565468"
                }, 
                {
                    "tag": "LDA", 
                    "tagLink": "https://api.zhihu.com/topics/19565464"
                }
            ], 
            "comments": []
        }
    ], 
    "url": "https://zhuanlan.zhihu.com/c_1011189203052376064"
}
