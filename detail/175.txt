{
    "title": "深度强化学习前沿技术解读", 
    "description": "认知智能的必经之路《深度强化学习》修炼记", 
    "followers": [
        "https://www.zhihu.com/people/ban-ge-wang-zi-88", 
        "https://www.zhihu.com/people/feng-ye-67-29", 
        "https://www.zhihu.com/people/li-hao-97-7-38", 
        "https://www.zhihu.com/people/zhang-wei-tao-92-11", 
        "https://www.zhihu.com/people/feiyudelei", 
        "https://www.zhihu.com/people/zheng-xiaosheng", 
        "https://www.zhihu.com/people/ljmifzu", 
        "https://www.zhihu.com/people/wang-zai-shou", 
        "https://www.zhihu.com/people/he-zheng-e", 
        "https://www.zhihu.com/people/wang-tao-5-60", 
        "https://www.zhihu.com/people/ge-zi-mu", 
        "https://www.zhihu.com/people/li.zhao.hua", 
        "https://www.zhihu.com/people/man-man-feng-diao", 
        "https://www.zhihu.com/people/ronald-xie", 
        "https://www.zhihu.com/people/zake-25", 
        "https://www.zhihu.com/people/yangkang-78", 
        "https://www.zhihu.com/people/zhang-cso", 
        "https://www.zhihu.com/people/drdh", 
        "https://www.zhihu.com/people/alphasue", 
        "https://www.zhihu.com/people/dd-hold", 
        "https://www.zhihu.com/people/pian-wei-fen", 
        "https://www.zhihu.com/people/shawn-52-23", 
        "https://www.zhihu.com/people/chris-10-37-33", 
        "https://www.zhihu.com/people/xiaojidan", 
        "https://www.zhihu.com/people/wang-han-qing-11", 
        "https://www.zhihu.com/people/15150575882", 
        "https://www.zhihu.com/people/mo-mo-51-47-11", 
        "https://www.zhihu.com/people/xue-chen-5-41", 
        "https://www.zhihu.com/people/liu-yuan-99-92", 
        "https://www.zhihu.com/people/di-zhuan-tian-xuan", 
        "https://www.zhihu.com/people/longer-9-14", 
        "https://www.zhihu.com/people/hac-73", 
        "https://www.zhihu.com/people/zhaolinfeng", 
        "https://www.zhihu.com/people/zhang-xiao-kai-93-92", 
        "https://www.zhihu.com/people/herman-25-50", 
        "https://www.zhihu.com/people/xiao-yu-22-71-20", 
        "https://www.zhihu.com/people/chiling-15", 
        "https://www.zhihu.com/people/zhang-jia-yi-6-82", 
        "https://www.zhihu.com/people/li-yi-chen-13-2", 
        "https://www.zhihu.com/people/alreadydone", 
        "https://www.zhihu.com/people/huang-shuai-4", 
        "https://www.zhihu.com/people/hujinzhao", 
        "https://www.zhihu.com/people/jerry-jerry-18-76", 
        "https://www.zhihu.com/people/jordan-49-83", 
        "https://www.zhihu.com/people/lang-wei-xian-he-lu-ye", 
        "https://www.zhihu.com/people/leng-mian-87", 
        "https://www.zhihu.com/people/wang-qiu-feng-75", 
        "https://www.zhihu.com/people/hey_free", 
        "https://www.zhihu.com/people/hu-bin-36-78", 
        "https://www.zhihu.com/people/xi-cheng-27-96", 
        "https://www.zhihu.com/people/ni-fei-84", 
        "https://www.zhihu.com/people/moolighty", 
        "https://www.zhihu.com/people/zheng-zhang-42", 
        "https://www.zhihu.com/people/yang-yang-17-6666", 
        "https://www.zhihu.com/people/zai-zhui-meng-39", 
        "https://www.zhihu.com/people/hua-qing-68-67", 
        "https://www.zhihu.com/people/wang-run-zhe-86", 
        "https://www.zhihu.com/people/yu-er-bu-rao-shu", 
        "https://www.zhihu.com/people/han-yang-16-69", 
        "https://www.zhihu.com/people/wu-lei-75", 
        "https://www.zhihu.com/people/bao-bei-50-52-83", 
        "https://www.zhihu.com/people/liu-shan-qi-41", 
        "https://www.zhihu.com/people/huan-shi-5-53", 
        "https://www.zhihu.com/people/ziran-91", 
        "https://www.zhihu.com/people/zjh10288", 
        "https://www.zhihu.com/people/boyuezh", 
        "https://www.zhihu.com/people/csxz", 
        "https://www.zhihu.com/people/wht-buaa", 
        "https://www.zhihu.com/people/re-ai-ke-xue", 
        "https://www.zhihu.com/people/saikusyo", 
        "https://www.zhihu.com/people/lao-tang-bao-zi", 
        "https://www.zhihu.com/people/bu-ceng-yi-wang-de-yi-wang", 
        "https://www.zhihu.com/people/chen-yi-17-34", 
        "https://www.zhihu.com/people/chi-xian-sheng-25", 
        "https://www.zhihu.com/people/liu-jie-77-40-17", 
        "https://www.zhihu.com/people/shen-mi-miao", 
        "https://www.zhihu.com/people/Jone8541", 
        "https://www.zhihu.com/people/liu-yi-25-9", 
        "https://www.zhihu.com/people/stochastics", 
        "https://www.zhihu.com/people/zhang-xiao-54-23", 
        "https://www.zhihu.com/people/xuxiao-looper", 
        "https://www.zhihu.com/people/niceworld-49-6", 
        "https://www.zhihu.com/people/zeavan", 
        "https://www.zhihu.com/people/yang-rui-33-68", 
        "https://www.zhihu.com/people/wang-lang-96-64", 
        "https://www.zhihu.com/people/lijiankou", 
        "https://www.zhihu.com/people/zhang-xiao-hui-66-53", 
        "https://www.zhihu.com/people/magicly", 
        "https://www.zhihu.com/people/hum-75", 
        "https://www.zhihu.com/people/yu-ji-shi-17", 
        "https://www.zhihu.com/people/cajvon", 
        "https://www.zhihu.com/people/jia-tong-92", 
        "https://www.zhihu.com/people/zhuang-ng-han", 
        "https://www.zhihu.com/people/wen-tong-14-31", 
        "https://www.zhihu.com/people/oqi-yuan-o", 
        "https://www.zhihu.com/people/duan-xing-99", 
        "https://www.zhihu.com/people/willwinworld", 
        "https://www.zhihu.com/people/chen-wen-jing-41", 
        "https://www.zhihu.com/people/bread-finding", 
        "https://www.zhihu.com/people/inextime", 
        "https://www.zhihu.com/people/yu-yong-sheng-14", 
        "https://www.zhihu.com/people/gu-dao-xi-yang", 
        "https://www.zhihu.com/people/xiao-65-50", 
        "https://www.zhihu.com/people/ma-zhi-hao-65", 
        "https://www.zhihu.com/people/wo-zai-di-shang-pao", 
        "https://www.zhihu.com/people/lianyuhu", 
        "https://www.zhihu.com/people/troy-loop", 
        "https://www.zhihu.com/people/18030226046", 
        "https://www.zhihu.com/people/BruceWDZ", 
        "https://www.zhihu.com/people/lucas_liu", 
        "https://www.zhihu.com/people/lan-zi-yuan-34", 
        "https://www.zhihu.com/people/rain-9-76", 
        "https://www.zhihu.com/people/genghanqiang", 
        "https://www.zhihu.com/people/its-zhi-guang", 
        "https://www.zhihu.com/people/jacymdk", 
        "https://www.zhihu.com/people/dcq-44", 
        "https://www.zhihu.com/people/yang-jing-lin-16", 
        "https://www.zhihu.com/people/liu-yu-qi-5-70", 
        "https://www.zhihu.com/people/lingshuo-81", 
        "https://www.zhihu.com/people/wei-shun-yi-61", 
        "https://www.zhihu.com/people/a-bu-40-78", 
        "https://www.zhihu.com/people/qin-xiao-yu-35-72", 
        "https://www.zhihu.com/people/cen-bing-18", 
        "https://www.zhihu.com/people/chen-ruo-yu-47-83", 
        "https://www.zhihu.com/people/song-zhi-17", 
        "https://www.zhihu.com/people/liu-qi-52-45", 
        "https://www.zhihu.com/people/fung-chow", 
        "https://www.zhihu.com/people/bailingnan", 
        "https://www.zhihu.com/people/garfield-guo", 
        "https://www.zhihu.com/people/luo-wei-bin-35", 
        "https://www.zhihu.com/people/jin-jia-rui-79", 
        "https://www.zhihu.com/people/herui112112", 
        "https://www.zhihu.com/people/jiaxian-17", 
        "https://www.zhihu.com/people/xiyao-lin", 
        "https://www.zhihu.com/people/lan-lan-lan-lan-96-82", 
        "https://www.zhihu.com/people/hyq-71-40", 
        "https://www.zhihu.com/people/liu-qiang-91-74", 
        "https://www.zhihu.com/people/han-feng-90-52", 
        "https://www.zhihu.com/people/lee-lee-84-78", 
        "https://www.zhihu.com/people/nlpxue-xi-zhe", 
        "https://www.zhihu.com/people/bei-ming-you-yu-58-74", 
        "https://www.zhihu.com/people/zhang-zhuo-24-35", 
        "https://www.zhihu.com/people/liang-ying-qu", 
        "https://www.zhihu.com/people/li-ran110", 
        "https://www.zhihu.com/people/liu-hao-hao-46", 
        "https://www.zhihu.com/people/ke-xue-jia-63", 
        "https://www.zhihu.com/people/moonbunnyzzz", 
        "https://www.zhihu.com/people/zhao-rong-wen", 
        "https://www.zhihu.com/people/sun-yang-ting", 
        "https://www.zhihu.com/people/ORA-09527", 
        "https://www.zhihu.com/people/guansuo666", 
        "https://www.zhihu.com/people/nata-26", 
        "https://www.zhihu.com/people/xuwei0221", 
        "https://www.zhihu.com/people/jax-59", 
        "https://www.zhihu.com/people/lchen-57", 
        "https://www.zhihu.com/people/morny_dew", 
        "https://www.zhihu.com/people/wenbin-yan-007", 
        "https://www.zhihu.com/people/liu-qiang-59-27", 
        "https://www.zhihu.com/people/amusi1994", 
        "https://www.zhihu.com/people/youngfish42", 
        "https://www.zhihu.com/people/caifengsteven", 
        "https://www.zhihu.com/people/yousaid0927", 
        "https://www.zhihu.com/people/dhg-18", 
        "https://www.zhihu.com/people/miao-tian-yi-73", 
        "https://www.zhihu.com/people/dong-yang-47-31", 
        "https://www.zhihu.com/people/learning-21-16", 
        "https://www.zhihu.com/people/explorerhiro", 
        "https://www.zhihu.com/people/long-ting-47-2", 
        "https://www.zhihu.com/people/xu-jian-feng-76-86", 
        "https://www.zhihu.com/people/wu-ri-san-sheng-wu-shen-2", 
        "https://www.zhihu.com/people/ha-ke-da", 
        "https://www.zhihu.com/people/blueman", 
        "https://www.zhihu.com/people/leng-jun-cheng-93", 
        "https://www.zhihu.com/people/lu-chen-xu-8", 
        "https://www.zhihu.com/people/kong-chui-shun", 
        "https://www.zhihu.com/people/ma-xiao-92-80", 
        "https://www.zhihu.com/people/eric-30", 
        "https://www.zhihu.com/people/ai-mu-87", 
        "https://www.zhihu.com/people/wang-zhi-fu-15", 
        "https://www.zhihu.com/people/yang-tuo-220-17", 
        "https://www.zhihu.com/people/li-zhu-7-39", 
        "https://www.zhihu.com/people/shi-hao-sen-87", 
        "https://www.zhihu.com/people/hou-chun-zhu-37", 
        "https://www.zhihu.com/people/diao-pi-de-xiao-jack", 
        "https://www.zhihu.com/people/lorinc", 
        "https://www.zhihu.com/people/blackclover", 
        "https://www.zhihu.com/people/chen-hao-ran-86", 
        "https://www.zhihu.com/people/haiyinpiao", 
        "https://www.zhihu.com/people/yang-sa-zai-chen-shang", 
        "https://www.zhihu.com/people/uuummmmiiii", 
        "https://www.zhihu.com/people/liu-tao-65-52", 
        "https://www.zhihu.com/people/jiu-ge-27-82", 
        "https://www.zhihu.com/people/aidiming", 
        "https://www.zhihu.com/people/huyichuan", 
        "https://www.zhihu.com/people/wu-ye-nan-30", 
        "https://www.zhihu.com/people/shawn-66-8", 
        "https://www.zhihu.com/people/liu-xiao-yong-80-42", 
        "https://www.zhihu.com/people/shui-ma-xi-mu-dong", 
        "https://www.zhihu.com/people/she-liang", 
        "https://www.zhihu.com/people/chen-wu-47-39", 
        "https://www.zhihu.com/people/HONTACY", 
        "https://www.zhihu.com/people/wang-cong-1-22", 
        "https://www.zhihu.com/people/cerena-8", 
        "https://www.zhihu.com/people/hao-tian-87", 
        "https://www.zhihu.com/people/depmeditator", 
        "https://www.zhihu.com/people/tang-li-78-82", 
        "https://www.zhihu.com/people/tian-lei-lei-14", 
        "https://www.zhihu.com/people/von-lynn-shawn", 
        "https://www.zhihu.com/people/xu-bin-2-24", 
        "https://www.zhihu.com/people/sunyong2012", 
        "https://www.zhihu.com/people/bai-chen-78", 
        "https://www.zhihu.com/people/a-ti-10", 
        "https://www.zhihu.com/people/chen-hong-qing-63", 
        "https://www.zhihu.com/people/li-xin-60-33", 
        "https://www.zhihu.com/people/idionil", 
        "https://www.zhihu.com/people/huang-chao-lin-6", 
        "https://www.zhihu.com/people/wang-kuan-57-47", 
        "https://www.zhihu.com/people/ni-hui-51-23", 
        "https://www.zhihu.com/people/zyfufeng", 
        "https://www.zhihu.com/people/ticktock-47", 
        "https://www.zhihu.com/people/dao-dao-90", 
        "https://www.zhihu.com/people/kai-xu-37-40", 
        "https://www.zhihu.com/people/xu-kai-71-15", 
        "https://www.zhihu.com/people/wang-chong-43-89", 
        "https://www.zhihu.com/people/yong-ge-16-26", 
        "https://www.zhihu.com/people/mel-tor", 
        "https://www.zhihu.com/people/hua-fei-29", 
        "https://www.zhihu.com/people/xiangguangyan", 
        "https://www.zhihu.com/people/Jade_Cong", 
        "https://www.zhihu.com/people/liu-peng-yun-40", 
        "https://www.zhihu.com/people/xiao-fa-94-94"
    ], 
    "article": [
        {
            "url": "https://zhuanlan.zhihu.com/p/84441093", 
            "userName": "DRLearner", 
            "userLink": "https://www.zhihu.com/people/2a0976472ac2864fa905e4ef0e8e5a36", 
            "upvote": 19, 
            "title": "深度强化学习岗位招聘汇总", 
            "content": "<p>本文汇总了13家企业关于强化学习招聘的信息，包含网易，头条，快手，华为诺亚等企业，有实习有工作（注：没有链接的自行查看官网或者搜索投递简历）,同时根据招聘要求，提升对应能力。</p><hr/><p><b>网易(伏羲实验室004)</b></p><p>职位：强化学习研究员</p><p>本科及以上||经验不限||语言不限||20-50岁</p><p>地址：杭州</p><p><b>​职位描述：</b></p><p>工作职责：<br/>1. 在（深度）强化学习领域进行创新性研究，发表高质量论文或撰写专利；<br/>2. 建模（深度）强化学习在游戏等产业落地时遇到的问题， 能够独立研究并解决；<br/>3. 具有以下相关项目研究积累尤佳：<br/>模仿学习：利用游戏玩家数据生成风格化各异的游戏AI角色；利用玩家数据学习出具有一定智能的游戏AI角色。（实际项目：玩家离线挂机AI， 多人副本测试AI, 线上对战AI等）<br/>强化学习：利用游戏等仿真环境训练过强化学习AI智能体；能够生成难度可控的游戏AI角色；设计新的强化学习算法，提高强化学习效率和效果；（实际项目：难度可变的副本boss AI；强化学习自动化测试等）<br/>多智能体：在竞争或合作的仿真环境训练过多智能体AI；设计新的多智能体算法， 提高多智能体算法的效果；（实际项目：篮球游戏AI， 多人副本游戏AI， 多人对战棋牌类游戏AI）<br/>AutoML: 有神经网络自动调参相关的经验；利用进化算法解决过实际问题；（实际项目：网络结构自动调整项目，算法超参自动调整项目， 风格多样化AI生成项目 ）</p><p><br/><b>任职要求：</b><br/>1. 有一定的游戏经历，热爱人工智能；<br/>2. 计算机或相关专业硕士以上学历，保持对领域最前沿技术的追踪；<br/>3. 能熟练使用主流深度学习框架，如tensorflow、mxnet、caffe、theano、keras等，具备实现常用的（深度）强化学习算法能力；<br/>4. 在人工智能会议和期刊发表过优秀论文，有顶级会议期刊发表经历者优先（NIPS, IJCAI, AAAI, ICML, ICLR，AAMAS等）；<br/>5. 在以下（深度）强化学习领域有一定积累：<br/>无模型学习（Model-Free RL）:</p><p><i>Value-based Algorithm, Policy Gradients, Deterministic Policy Gradients, Distributional RL, Evolutionary Algorithms;</i><br/>模仿学习（Imitation Learning and Inverse Reinforcement Learning）: <i>Behavior Clone, GAIL</i><br/>多智能体学习（Multi-agent learning）<br/>基于模型的学习(Model-Based RL): <i>Model is Learned, Model is Given</i><br/>Scaling RL（分布式强化学习）：<i>Ape-x, R2D2, IMPALA</i><br/>探索（Exportation）: <i>Intrinsic Motivation, Unsupervised RL</i><br/>迁移和多任务（Transfer and Multitask RL）: <i>Progressive Networks, UVFA, UNREAL, HER</i><br/>分层（Hierarchy）: <i>STRAW， Feudal Networks， HIRO</i><br/>Meta-RL（元学习）</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-d54a6bde504a74b444a38caa70fd012c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"256\" data-rawheight=\"256\" class=\"content_image\" width=\"256\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;256&#39; height=&#39;256&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"256\" data-rawheight=\"256\" class=\"content_image lazy\" width=\"256\" data-actualsrc=\"https://pic1.zhimg.com/v2-d54a6bde504a74b444a38caa70fd012c_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p><b>今日头条</b></p><p>职位：强化学习研究员</p><p>硕士及以上||3年以上||普通话||年龄不限</p><p>地址：深圳-南山区</p><p><b>职位描述：</b></p><p>工作职责：<br/>1、 负责强化学习前沿算法的创新研究与探索，发表顶会论文和申请专利；<br/>2、探索不同类游戏的AI对弈算法，包括拟人化学习和强化学习框架搭建等。<br/><b>任职要求：</b><br/>1、数学、计算机、人工智能、自动控制、模式识别等相关专业，硕士及以上学历；<br/>2、3年以上强化学习研究经验，对优化理论、多智能体学习、模仿学习、分布式强化学习等有积累者优先考虑；<br/>3、发表过期刊或会议论文，在ICML/ NIPS/ IJCAI/ AAAI/ ICLR/ ICRA等国际顶级会议上发表过学术论文者优先考虑；<br/>4、熟练使用TensorFlow/ PyTorch等至少一种深度学习框架，熟练使用Python/ C++等至少一种编程语。</p><hr/><p><b>今日头条</b></p><p>职位：强化学习研究员 (职位编号：46326)</p><p>北京 | 3-4年经验 | 本科 | 招2人 | 09-24发布</p><p>地点：海淀北二街10号泰鹏大厦10层</p><p><b>职位描述：</b></p><p>负责强化学习的游戏AI应用场景商业化落地，包括特征提取、模型训练、强化学习框架开发、在线效果优化等，完成项目交付。</p><p><br/><b>职位要求:</b><br/>1、计算机、人工智能、自动控制、模式识别等相关专业硕士及以上学位；<br/>2、具有1年以上强化学习研究经验，对多智能体学习、模仿学习、分布式强化学习等有积累者优先考虑；<br/>3、熟练使用TensorFlow/ PyTorch等至少一种深度学习框架，熟练使用Python/ C++等至少一种编程语。</p><hr/><p><b>华为-诺亚方舟实验室</b></p><p>职位：强化学习方向实习生</p><p>实习地点：深圳/北京</p><p>实习时间：不少于3个月</p><p><b>职位描述：</b></p><p>岗位要求：</p><p>机器学习、应用统计/数学、计算机、电子工程、自动化等相关方向在校硕士/博士，对主流强化学习算法有一定了解，熟练使用Tensorflow/Pytorch等，有实际应用经验或发表过相关论文者，并有意愿结合实际应用问题做研究者优先考虑。</p><p>有意向者请发简历至：</p><p>zhuangyuzheng@huawei.com</p><hr/><p><b>网易</b></p><p>职位：强化学习研究工程师</p><p>硕士及以上||经验不限||语言不限||年龄不限</p><p>广州-珠江新城</p><p><b>职位描述：</b></p><p>岗位描述<br/>网易互娱AI实验室招募强化学习研究工程师，利用深度学习和强化学习学习游戏中的智能体。目标是实现AI算法在游戏中的落地，从而提高游戏策划制作效率、提升游戏中NPC角色智能化水平以及提升游戏玩家体验。<br/><br/><b>岗位要求</b><br/>1. 计算机和机器学习等相关专业硕士及以上学历；<br/>2. 在机器学习、深度学习和强化学习方向具有扎实的理论和实践基础，保持对领域最前沿技术的追踪；<br/>3. 具有上述方向的工业界经历，有实际项目经验；<br/>4. 熟练掌握一种常见的深度学习框架，譬如 TensorFlow/pytorch/MXNet/Caffe；<br/>5. 喜欢游戏及其开发，了解游戏基本逻辑，对竞技类游戏、手游开发有经验者优先；<br/>6. 在顶级会议（AAAI/IJCAI/ICML /NIPS等）或期刊有论文发表者优先;<br/>7. 能够独立分析并解决问题</p><hr/><p><b>阿里巴巴(社招)</b></p><p>职位：自动驾驶实验室-行为决策方向-强化学习算法专家</p><p>地点：北京/杭州</p><h3><b>岗位描述：</b></h3><p>1、利用强化学习等机器学习方法优化自动驾驶行为决策算法；<br/>2、对接自动驾驶仿真平台, 搭建基于迭代学习的自动驾驶行为决策优化工具平台；<br/>3、对接自动驾驶实测数据, 挖掘最优行为决策逻辑, 优化自动驾驶系统的决策算法；<br/>4、有机器人／无人驾驶相关强化学习或行为优化开发经验者优先。</p><p>岗位要求：</p><p>1、机器人、人工智能等相关专业<br/>2、熟悉强化学习, 并有强化学习在实际自主系统中应用的项目经验<br/>3、熟悉自动驾驶决策, 最优化交通行为决策等算法者优先<br/>4、 有在Linux系统下开发经验<br/>5、 多年的C++/Python的开发经验，1～3年的实际项目工作经验<br/>6、 善于团队协作完成既定任务</p><hr/><p><b>VIVO</b></p><p>简介：vivo是一家专注连接产业生态，为追求科技与时尚的前端用户提供智慧终端产品和服务的科技公司，我们致力于整合产业创新能力，并拥有完善的自研自产体系，在中国东莞，深圳，南京，北京，杭州，上海，台北，美国圣地亚哥，日本东京设有研发中心，研发范围包括5G、人工智能、拍照、设计等众多消费级前沿领域。</p><p><b>职位：强化学习算法研究员</b></p><h3><b>职位描述：</b></h3><p>1. 根据项目需求，设计智能Agent；<br/>2. 根据项目需求，设计策略算法</p><p><br/><b>任职资格：</b><br/>1. 硕士或以上学历，计算机、数学等相关专业；<br/>2. 深度理解强化学习，模仿学习，有在实际项目中应用与优化过相关算法者优先；<br/>3. 熟悉强化学习基本算法，使用过DQN，DDPG，GPS，TRPO等算法者优先<br/>4. 精通C/C++、python编程，有一定的项目开发经验 ；<br/>5. 具有较强的英文阅读能力，能够阅读英文论文和专利；<br/>6. 具有良好的语言沟通能力，团队合作精神，上进心强，具有快速学习能力；<br/>7. 具有论文、专利、竞赛成功的优先考虑。</p><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p><b>美团点评</b></p><p>职位：无人配送-强化学习算法专家</p><p>博士||2年以上||普通话||年龄不限</p><p>地址：北京</p><h3><b>职位描述：</b></h3><p>工作职责 ：<br/>1. 融合感知信息、定位数据、地图信息，研发真实道路和交通条件下的无人车驾驶规划算法。<br/>2. 研究前沿的深度学习和强化学习相关算法，提供开放复杂场景下的无人车决策规划解决方案。<br/><br/><b>任职要求：</b><br/>1. 具有无人车驾驶规划或机器人规划工作经历。<br/>2. 具有良好的编程基础，至少精通一门编程语言。<br/>3. 具有抽象建模能力，能够根据实际问题合理建模，制定解决方案。<br/>4. 熟悉常用的无人车规划算法。<br/>5. 熟悉ROS等机器人操作系统。<br/>6. 熟悉马尔科夫过程，概率图模型。<br/>7. 熟悉强化学习和深度学习算法。<br/><br/><b>优先考虑：</b><br/>1. 硕士或博士在自动化，车辆工程，机械工程，计算机，数学等专业。<br/>2. 有无人车算法开发经验，熟悉无人车软件架构。<br/>3. 有无人车相关比赛经历且取得优异成绩。<br/>4. 在规划算法、深度学习或强化学习方法有深入研究，有落地经验或者发表过高水平论文。<br/>5. 有扎实的数学基础，熟悉常用的优化算法。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-e611f1835c334d7515bd11b65ea41163_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"239\" data-rawheight=\"243\" class=\"content_image\" width=\"239\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;239&#39; height=&#39;243&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"239\" data-rawheight=\"243\" class=\"content_image lazy\" width=\"239\" data-actualsrc=\"https://pic4.zhimg.com/v2-e611f1835c334d7515bd11b65ea41163_b.jpg\"/></figure><hr/><p class=\"ztext-empty-paragraph\"><br/></p><p><b>北京快手科技有限公司</b></p><p>简介： 快手是一个记录和分享生活的短视频社交平台，公司自2011年创立，在过去8年时间里，稳步成长为众多用户的生活分享社区。快手坚持每个人都值得被记录，每个人都有被看到的权利和机会，致力于用技术提升每一个人独特的幸福感。</p><p>职位：推荐算法工程师-深度/强化学习算法</p><p>统招本科|| 1年以上|| 语言不限|| 年龄不限</p><p>地址：北京</p><p><b>职位描述：</b></p><p>工作职责<br/>1. 跟踪机器学习领域前沿技术方向的最新进展，包括强化学习、迁移学习、深度学习等；<br/>2. 将强化学习等机器学习领域前沿技术应用到快手推荐系统中，优化留存率等核心业务指标；<br/>3. 系统框架的迭代和优化，提供可靠的线上服务。<br/>任职资格<br/>1. 机器学习、深度学习相关方向背景扎实，有实际应用经验；<br/>2. 优秀的编程能力C++/Python, 熟悉常见的数据结构和算法；<br/>3. 熟悉Linux开发环境和常用开发工具；熟悉至少一种常用深度学习框架Tensorflow/PyTorch/MxNet等；<br/>4. 较强的文献阅读和理解能力，良好的逻辑思维、沟通表达能力，良好的技术视野和深度，对前沿技术的实际应用有浓厚兴趣<br/>加分项：<br/>1. 高质量技术博客/Github，或知名开源项目contributor；<br/>2. 机器学习竞赛成绩优异者，如Kaggle/KDD等；或有ACM/NOI获奖经历；<br/>3. 顶级会议论文发表者，如NIPS/ICML/IJCAI/CVPR/ACL/SIGKDD等；<br/>4. 在强化学习、迁移学习等前沿领域，有较深入的研究经验；或在推荐系统/搜索排序系统/大型分布式系统 有较深入的工作经验。</p><hr/><p><b>深圳星行科技有限公司</b></p><p>简介：Roadstar.ai是一家主打L4自动驾驶的科技公司，选择以多传感器融合的方案切入自动驾驶，即通过算法+成本可控的传感器，进行L4级别自动驾驶的研发。</p><p>职位：强化学习算法工程师</p><p>学历不限|| 1年以上|| 语言不限|| 年龄不限</p><p>深圳-龙岗区</p><h3><b>职位描述：：</b></h3><p>用强化学习方法优化物体追踪，预测性能<br/><br/><b>基本要求</b><br/>1. 数理、工程类专业，本科及以上学历 <br/>2. 熟悉强化学习和深度强化学习<br/>3. 熟悉深度学习基本方法，熟练使用一种深度学习工具(Tensorflow/Pytorch/Caffe等) <br/>4. 熟悉基本算法与数据结构，熟练使用Python与C++<br/><br/><b>优先考虑</b><br/>1. 相关领域（在读）硕士/博士，或等同的实践经验<br/>2. 有自己的开源代码项目(Github)或在其他开源代码库有贡献<br/>3. 在知名强化学习benchmark(Atari game, OpenAI Gym)取得优异成绩<br/>4. 在人工智能/深度学习/机器人领域顶级会议或期刊发表文章<br/>5. 来自知名大学且绩点排名靠前<br/>6. 突出的编程能力(ACM等比赛获奖)</p><hr/><p><b>Testin云测</b></p><p>简介：Testin云测（<a href=\"https://link.zhihu.com/?target=http%3A//www.testin.cn\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">testin.cn</span><span class=\"invisible\"></span></a>) 创立于2011年，作为先进的应用服务平台，为全球超过百万的开发者和企业提供测试、安全、推广、产品优化、流量变现，及AI大数据解决方案。</p><p>职位：强化学习工程师</p><p>地址：北京</p><h3><b>职位描述：</b></h3><p>1、参与公司核心项目策略模型的开发和集成，在生成对抗网络中应用各类强化学习策略模型生成App测试序列集；<br/>2、配合并支持自动化测试执行团队；</p><p><b>岗位要求：</b><br/>1、拥有计算机、数学或相关学科本科及以上学位；<br/>2、熟悉各类主流深度学习算法，包括不限于强化学习（DQN、AC等）、GAN、深度学习（CNN、RNN等）、迁移学习等；<br/>3、熟练掌握Tensorflow、Keras、pytorch等至少一种深度学习框架，熟练掌握Python、R或C++其中一门开发语言；<br/>4、对前沿算法有强烈的兴趣，思辨能力强，沟通协作意识好，责任心及结果导向意识强，自驱力强；<br/>5、加分项：在无人驾驶、机器人控制、游戏AI、或其他强化学习相关场景2年以上算法工程经验；</p><hr/><p><b>超参数科技(深圳)有限公司</b></p><p>简介：超参数科技是一家专注于游戏AI探索的初创公司，主攻机器学习、强化学习、大系统工程等技术领域，通过将AI能力和游戏场景进行深度结合，为游戏公司提供人工智能解决方案(<a href=\"https://link.zhihu.com/?target=https%3A//chaocanshu.cn/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">chaocanshu.cn/</span><span class=\"invisible\"></span></a>)</p><p>职位：强化学习高级研究员</p><p>博士||3年以上||语言不限||年龄不限</p><p>深圳-南山区</p><h3><b>职位描述：</b></h3><p>职责描述：<br/>• 负责强化学习前沿算法的创新研究与探索，发表顶会论文和申请专利；<br/>• 针对游戏AI的应用场景，提供算法和解决方案。<br/><br/><b>任职要求：</b><br/>• 计算机、人工智能、自动控制、模式识别等相关专业的博士；<br/>• 具有3年以上强化学习研究经验，对多智能体学习、模仿学习、分布式强化学习等有积累者优先考虑；<br/>• 发表过期刊或会议论文，在ICML/NIPS/IJCAI/AAAI/ICLR/ICRA等国际顶级会议上发表过学术论文者优先考虑；<br/>• 熟练使用TensorFlow/PyTorch等至少一种深度学习框架，熟练使用Python/C++等至少一种编程语言。</p><hr/><p><b>北京初速度科技有限公司</b></p><p>职位：强化学习工程师</p><p>学历不限||经验不限||语言不限||年龄不限</p><p>地址：苏州-元和</p><h3><b>职位描述：</b></h3><p>1. 利用强化学习，深度学习实现一些场景下的决策算法。<br/>2. 搭建决策系统所需要的虚拟环境。<br/>3. 实现交通管理决策系统和算法，并与系统团队深度合作，确保系统的运行决策过程高效、合理。<br/><b>岗位要求：</b><br/>1. 3年以上机器学习经验，熟悉机器学习/强化学习算法，对神经网络有较深刻的理解尤佳。<br/>2. 扎实的编程功底，熟练掌握至少一种常见的深度学习框架，如 TensorFlow/Keras, Caffe, Mxnet，熟练掌握C/C++,JAVA,Python,golang中的一种或多种语言。<br/>3. 在强化学习方向具有扎实的理论基础和相关项目或课题经验<br/>4. 较强的独立工作能力，有责任心，较强的问题分析和解决能力、团队沟通和合作能力<br/>5. 有cityflow ，sumo等交通模拟器实践经验的优先，有搭建虚拟环境训练强化学习算法经验的优先。</p><hr/><p><b>GitHub仓库</b></p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/NeuronDance/DeepRL\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic4.zhimg.com/v2-d38ee0c1d4d832dea1406f1ca51f8dbb_ipico.jpg\" data-image-width=\"420\" data-image-height=\"420\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">NeuronDance/DeepRL</a><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-b6f81e5863e926c731272726198258ae_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"430\" data-rawheight=\"430\" class=\"origin_image zh-lightbox-thumb\" width=\"430\" data-original=\"https://pic3.zhimg.com/v2-b6f81e5863e926c731272726198258ae_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;430&#39; height=&#39;430&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"430\" data-rawheight=\"430\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"430\" data-original=\"https://pic3.zhimg.com/v2-b6f81e5863e926c731272726198258ae_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-b6f81e5863e926c731272726198258ae_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "招聘", 
                    "tagLink": "https://api.zhihu.com/topics/19578758"
                }, 
                {
                    "tag": "人力资源（HR）", 
                    "tagLink": "https://api.zhihu.com/topics/19555189"
                }, 
                {
                    "tag": "强化学习 (Reinforcement Learning)", 
                    "tagLink": "https://api.zhihu.com/topics/20039099"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/82659139", 
            "userName": "DRLearner", 
            "userLink": "https://www.zhihu.com/people/2a0976472ac2864fa905e4ef0e8e5a36", 
            "upvote": 4, 
            "title": "AI领域：如何做优秀研究并写高水平论文？", 
            "content": "<p></p><blockquote>每个人从本科到硕士，再到博士、博士后，甚至工作以后，都会遇到做研究、写论文这个差事。论文通常是对现有工作的一个总结和展示，特别对于博士和做研究的人来说，论文则显得更加重要。<br/><br/><b>那么该如果做突出研究，并写出高水平的论文呢？</b><br/><b>那么该如果做突出研究，并写出高水平的论文呢？</b><br/><b>那么该如果做突出研究，并写出高水平的论文呢？</b><br/><br/>本文整理了著名人工智能学者<b>周志华</b>教授《做研究与写论文》的PPT(时间比较早，但方法永远不会过时)。其详细介绍了关于为什么要做研究？如何做研究，选择研究方向、选择研究课题(Topic)，学习领域知识、选期刊投稿、稿件处理过程、写高水平论文的方法与技巧等方面的知识，是一份非常优秀的做研究和写论文指南，值得每个人细读<b>（推荐指数：★★★★）</b><br/>周志华个人简介<br/>他于2001年1月留校任教，2002年破格晋升副教授，2003年获 国家杰出青年科学基金，随后被聘任为教授，2004年获博士生导师资格，2006年入选教育部长江学者特聘教授。现任南京大学 校学术委员会委员、计算机科学与技术系 主任、人工智能学院 院长，主要从事人工智能、机器学习、数据挖掘等领域的研究工作。主持多项科研课题，出版《机器学习》(2016)、《Ensemble Methods: Foundations and Algorithms》(2012)、《Evolutionary Learning: Advances in Theories and Algorithms》(2019)。<br/>更多详见：<a href=\"https://link.zhihu.com/?target=https%3A//cs.nju.edu.cn/zhouzh/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">cs.nju.edu.cn/zhouzh/</span><span class=\"invisible\"></span></a></blockquote><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-423ecebf06ef80964427270b7ac19150_b.gif\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"100\" data-rawheight=\"100\" data-thumbnail=\"https://pic1.zhimg.com/v2-423ecebf06ef80964427270b7ac19150_b.jpg\" class=\"content_image\" width=\"100\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;100&#39; height=&#39;100&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"100\" data-rawheight=\"100\" data-thumbnail=\"https://pic1.zhimg.com/v2-423ecebf06ef80964427270b7ac19150_b.jpg\" class=\"content_image lazy\" width=\"100\" data-actualsrc=\"https://pic1.zhimg.com/v2-423ecebf06ef80964427270b7ac19150_b.gif\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-225013b151825327fc02b64639bdf780_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"814\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-225013b151825327fc02b64639bdf780_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;814&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"814\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-225013b151825327fc02b64639bdf780_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-225013b151825327fc02b64639bdf780_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ce258d89c6bfd89db55e9a859894892c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"812\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-ce258d89c6bfd89db55e9a859894892c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;812&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"812\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-ce258d89c6bfd89db55e9a859894892c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-ce258d89c6bfd89db55e9a859894892c_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-4b4cc280bef59b96b3a1c7c3f9ab79d2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"806\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-4b4cc280bef59b96b3a1c7c3f9ab79d2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;806&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"806\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-4b4cc280bef59b96b3a1c7c3f9ab79d2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-4b4cc280bef59b96b3a1c7c3f9ab79d2_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-36508e335e0cb15f6aabca4fec3aac40_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"807\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-36508e335e0cb15f6aabca4fec3aac40_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;807&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"807\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-36508e335e0cb15f6aabca4fec3aac40_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-36508e335e0cb15f6aabca4fec3aac40_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-bafbc8e646fad1a73dfbc0b3fd28b5a4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"811\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-bafbc8e646fad1a73dfbc0b3fd28b5a4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;811&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"811\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-bafbc8e646fad1a73dfbc0b3fd28b5a4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-bafbc8e646fad1a73dfbc0b3fd28b5a4_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-7c6ec0b523406e11a09d9a92a1e8a294_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"806\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-7c6ec0b523406e11a09d9a92a1e8a294_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;806&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"806\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-7c6ec0b523406e11a09d9a92a1e8a294_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-7c6ec0b523406e11a09d9a92a1e8a294_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-15a31d550d069aba474e6f6bd1345ee1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"808\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-15a31d550d069aba474e6f6bd1345ee1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;808&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"808\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-15a31d550d069aba474e6f6bd1345ee1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-15a31d550d069aba474e6f6bd1345ee1_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-53832c2c123e04b92c0a803a234c3020_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"809\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-53832c2c123e04b92c0a803a234c3020_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;809&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"809\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-53832c2c123e04b92c0a803a234c3020_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-53832c2c123e04b92c0a803a234c3020_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-0992d9c6bd68027419dae246657fae45_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"795\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-0992d9c6bd68027419dae246657fae45_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;795&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"795\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-0992d9c6bd68027419dae246657fae45_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-0992d9c6bd68027419dae246657fae45_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-163d606284a13dcfe21589818440654e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"812\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-163d606284a13dcfe21589818440654e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;812&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"812\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-163d606284a13dcfe21589818440654e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-163d606284a13dcfe21589818440654e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-1cbb5082a2cebb5a35e25e84fad37b70_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"809\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-1cbb5082a2cebb5a35e25e84fad37b70_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;809&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"809\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-1cbb5082a2cebb5a35e25e84fad37b70_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-1cbb5082a2cebb5a35e25e84fad37b70_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-1a752af03b2900fe3dfc190010024aff_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"815\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-1a752af03b2900fe3dfc190010024aff_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;815&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"815\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-1a752af03b2900fe3dfc190010024aff_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-1a752af03b2900fe3dfc190010024aff_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-b9bf6a94bfb71de8a1e2f9e4643fbac3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"797\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-b9bf6a94bfb71de8a1e2f9e4643fbac3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;797&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"797\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-b9bf6a94bfb71de8a1e2f9e4643fbac3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-b9bf6a94bfb71de8a1e2f9e4643fbac3_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-7962a4d8ce0846e7d65fe53df69bf98a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"804\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-7962a4d8ce0846e7d65fe53df69bf98a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;804&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"804\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-7962a4d8ce0846e7d65fe53df69bf98a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-7962a4d8ce0846e7d65fe53df69bf98a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-b9aebe8300c1d661b13cc14ef016126b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"802\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-b9aebe8300c1d661b13cc14ef016126b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;802&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"802\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-b9aebe8300c1d661b13cc14ef016126b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-b9aebe8300c1d661b13cc14ef016126b_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-48ae7fffe6babba97ba2a6f8a3f8f7e2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"806\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-48ae7fffe6babba97ba2a6f8a3f8f7e2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;806&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"806\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-48ae7fffe6babba97ba2a6f8a3f8f7e2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-48ae7fffe6babba97ba2a6f8a3f8f7e2_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-9c79da5854e688e5406b00d6b82a539a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"808\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-9c79da5854e688e5406b00d6b82a539a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;808&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"808\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-9c79da5854e688e5406b00d6b82a539a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-9c79da5854e688e5406b00d6b82a539a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-1e974b29d568f6d3cb8b807edb4acf82_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"793\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-1e974b29d568f6d3cb8b807edb4acf82_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;793&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"793\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-1e974b29d568f6d3cb8b807edb4acf82_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-1e974b29d568f6d3cb8b807edb4acf82_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-5310b77a9d63b95091c41b05ce1127a2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"797\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-5310b77a9d63b95091c41b05ce1127a2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;797&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"797\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-5310b77a9d63b95091c41b05ce1127a2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-5310b77a9d63b95091c41b05ce1127a2_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-1305654417ee61005e69881764408832_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"806\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-1305654417ee61005e69881764408832_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;806&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"806\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-1305654417ee61005e69881764408832_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-1305654417ee61005e69881764408832_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-86e4c39e96831f02a408e374bae0756d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"788\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-86e4c39e96831f02a408e374bae0756d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;788&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"788\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-86e4c39e96831f02a408e374bae0756d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-86e4c39e96831f02a408e374bae0756d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-51fdd9fa38cac2df08b9b11957e00715_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"806\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-51fdd9fa38cac2df08b9b11957e00715_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;806&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"806\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-51fdd9fa38cac2df08b9b11957e00715_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-51fdd9fa38cac2df08b9b11957e00715_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-c6d1345479005ca62b71fb9ea198f5ac_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"802\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-c6d1345479005ca62b71fb9ea198f5ac_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;802&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"802\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-c6d1345479005ca62b71fb9ea198f5ac_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-c6d1345479005ca62b71fb9ea198f5ac_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-66aaa3cd29660a6f54a75b38cf7ed720_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"804\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-66aaa3cd29660a6f54a75b38cf7ed720_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;804&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"804\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-66aaa3cd29660a6f54a75b38cf7ed720_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-66aaa3cd29660a6f54a75b38cf7ed720_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-ad3a48a52e5672d53bfdef5b2fe05e92_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"806\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-ad3a48a52e5672d53bfdef5b2fe05e92_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;806&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"806\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-ad3a48a52e5672d53bfdef5b2fe05e92_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-ad3a48a52e5672d53bfdef5b2fe05e92_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-4991c963098c78b0d0fde88fa3fc395a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"804\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-4991c963098c78b0d0fde88fa3fc395a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;804&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"804\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-4991c963098c78b0d0fde88fa3fc395a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-4991c963098c78b0d0fde88fa3fc395a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-14320bf51198dbde89e34e37030a7e18_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"805\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-14320bf51198dbde89e34e37030a7e18_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;805&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"805\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-14320bf51198dbde89e34e37030a7e18_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-14320bf51198dbde89e34e37030a7e18_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-4fad7add8df5c189ac3cafa81991e7f5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"803\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-4fad7add8df5c189ac3cafa81991e7f5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;803&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"803\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-4fad7add8df5c189ac3cafa81991e7f5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-4fad7add8df5c189ac3cafa81991e7f5_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-acfc6d00e2f8035686a74049d8c38777_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"801\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-acfc6d00e2f8035686a74049d8c38777_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;801&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"801\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-acfc6d00e2f8035686a74049d8c38777_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-acfc6d00e2f8035686a74049d8c38777_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-01bb009d80cc169b50d13ce752519b6e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"799\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-01bb009d80cc169b50d13ce752519b6e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;799&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"799\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-01bb009d80cc169b50d13ce752519b6e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-01bb009d80cc169b50d13ce752519b6e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-a47b9f6ab78e991d3ea2ca07f86165a7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"810\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-a47b9f6ab78e991d3ea2ca07f86165a7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;810&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"810\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-a47b9f6ab78e991d3ea2ca07f86165a7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-a47b9f6ab78e991d3ea2ca07f86165a7_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-a41894b866987d66921f5bc077c2a381_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"804\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-a41894b866987d66921f5bc077c2a381_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;804&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"804\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-a41894b866987d66921f5bc077c2a381_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-a41894b866987d66921f5bc077c2a381_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-23120db2ea1525084702e77e5f7cf911_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"807\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-23120db2ea1525084702e77e5f7cf911_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;807&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"807\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-23120db2ea1525084702e77e5f7cf911_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-23120db2ea1525084702e77e5f7cf911_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-9ead4408bc803855306e5d7507d0c951_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"806\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-9ead4408bc803855306e5d7507d0c951_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;806&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"806\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-9ead4408bc803855306e5d7507d0c951_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-9ead4408bc803855306e5d7507d0c951_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-6743f85751b227fb9ea746596696bc74_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"805\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-6743f85751b227fb9ea746596696bc74_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;805&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"805\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-6743f85751b227fb9ea746596696bc74_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-6743f85751b227fb9ea746596696bc74_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-bc205b6f400732783692c894e4073bd1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"804\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-bc205b6f400732783692c894e4073bd1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;804&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"804\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-bc205b6f400732783692c894e4073bd1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-bc205b6f400732783692c894e4073bd1_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-5ffe956e70b4f8a2e92801684c357bd5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"792\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-5ffe956e70b4f8a2e92801684c357bd5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;792&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"792\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-5ffe956e70b4f8a2e92801684c357bd5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-5ffe956e70b4f8a2e92801684c357bd5_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-901edb6e6ba7011b6fbda54a8ea29ad9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"811\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-901edb6e6ba7011b6fbda54a8ea29ad9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;811&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"811\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-901edb6e6ba7011b6fbda54a8ea29ad9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-901edb6e6ba7011b6fbda54a8ea29ad9_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-a88deea158774c415727120cad266880_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"800\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-a88deea158774c415727120cad266880_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;800&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"800\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-a88deea158774c415727120cad266880_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-a88deea158774c415727120cad266880_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-4b2434a1e061022f2e87f7eb9809e53d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"800\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-4b2434a1e061022f2e87f7eb9809e53d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;800&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"800\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-4b2434a1e061022f2e87f7eb9809e53d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-4b2434a1e061022f2e87f7eb9809e53d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-8edb82a73e79997684e9c57b4832746e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"806\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-8edb82a73e79997684e9c57b4832746e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;806&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"806\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-8edb82a73e79997684e9c57b4832746e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-8edb82a73e79997684e9c57b4832746e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-2b1e9dbbc87afb898fafbc91e9504b61_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"803\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-2b1e9dbbc87afb898fafbc91e9504b61_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;803&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"803\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-2b1e9dbbc87afb898fafbc91e9504b61_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-2b1e9dbbc87afb898fafbc91e9504b61_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-dff68a0166e46869cb39d157b788f5ad_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"803\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-dff68a0166e46869cb39d157b788f5ad_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;803&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"803\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-dff68a0166e46869cb39d157b788f5ad_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-dff68a0166e46869cb39d157b788f5ad_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-8df14c7b2109aa267e0983ce0c436947_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"801\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-8df14c7b2109aa267e0983ce0c436947_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;801&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"801\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-8df14c7b2109aa267e0983ce0c436947_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-8df14c7b2109aa267e0983ce0c436947_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-b20ce82b603590e80cb826d74a05cae6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"806\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-b20ce82b603590e80cb826d74a05cae6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;806&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"806\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-b20ce82b603590e80cb826d74a05cae6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-b20ce82b603590e80cb826d74a05cae6_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-e1dd2e23f2ccf0dc7b9957e42a372bc7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"797\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-e1dd2e23f2ccf0dc7b9957e42a372bc7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;797&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"797\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-e1dd2e23f2ccf0dc7b9957e42a372bc7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-e1dd2e23f2ccf0dc7b9957e42a372bc7_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-c5a796809bd574e4ae6c04a1fbc1358f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"802\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-c5a796809bd574e4ae6c04a1fbc1358f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;802&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"802\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-c5a796809bd574e4ae6c04a1fbc1358f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-c5a796809bd574e4ae6c04a1fbc1358f_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-a23f14039c7926b093d664ed394f58af_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"795\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-a23f14039c7926b093d664ed394f58af_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;795&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"795\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-a23f14039c7926b093d664ed394f58af_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-a23f14039c7926b093d664ed394f58af_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-19af8671a144412fe10f983cbb5952b7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"807\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-19af8671a144412fe10f983cbb5952b7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;807&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"807\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-19af8671a144412fe10f983cbb5952b7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-19af8671a144412fe10f983cbb5952b7_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-863922b44586f10a79922d865f04ed06_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"800\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-863922b44586f10a79922d865f04ed06_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;800&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"800\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-863922b44586f10a79922d865f04ed06_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-863922b44586f10a79922d865f04ed06_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-5c6ab7177b4d4feaece3183a5c8d51fb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"810\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-5c6ab7177b4d4feaece3183a5c8d51fb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;810&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"810\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-5c6ab7177b4d4feaece3183a5c8d51fb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-5c6ab7177b4d4feaece3183a5c8d51fb_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-425e262b626519816414cc92104c61d2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"795\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-425e262b626519816414cc92104c61d2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;795&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"795\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-425e262b626519816414cc92104c61d2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-425e262b626519816414cc92104c61d2_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-58553426180de8272c1881fbd1050b43_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"793\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-58553426180de8272c1881fbd1050b43_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;793&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"793\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-58553426180de8272c1881fbd1050b43_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-58553426180de8272c1881fbd1050b43_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-9e86932893ac256ee0ad5a44a470058f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"797\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-9e86932893ac256ee0ad5a44a470058f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;797&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"797\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-9e86932893ac256ee0ad5a44a470058f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-9e86932893ac256ee0ad5a44a470058f_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-535ce69205b5f55a451dc4690d9c3dcb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"811\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-535ce69205b5f55a451dc4690d9c3dcb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;811&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"811\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-535ce69205b5f55a451dc4690d9c3dcb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-535ce69205b5f55a451dc4690d9c3dcb_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-8830e5f517ca995e3782ffe650a2cec9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"803\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-8830e5f517ca995e3782ffe650a2cec9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;803&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"803\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-8830e5f517ca995e3782ffe650a2cec9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-8830e5f517ca995e3782ffe650a2cec9_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-c727868358d904293e1f6c782248fd9c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"796\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-c727868358d904293e1f6c782248fd9c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;796&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"796\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-c727868358d904293e1f6c782248fd9c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-c727868358d904293e1f6c782248fd9c_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-9e32917a39f329a228010e40defdd8d4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"805\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-9e32917a39f329a228010e40defdd8d4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;805&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"805\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-9e32917a39f329a228010e40defdd8d4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-9e32917a39f329a228010e40defdd8d4_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-953176f03e6ae4e42544fb6e544cc6dc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"794\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-953176f03e6ae4e42544fb6e544cc6dc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;794&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"794\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-953176f03e6ae4e42544fb6e544cc6dc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-953176f03e6ae4e42544fb6e544cc6dc_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-46a95cab12311b3f16a028bca19bf66a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"798\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-46a95cab12311b3f16a028bca19bf66a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;798&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"798\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-46a95cab12311b3f16a028bca19bf66a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-46a95cab12311b3f16a028bca19bf66a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><blockquote>郑重声明：本文内容来源于周志华教授报告PPT，解释权系原作者所有，本文分享仅供大家学习参考使用，切勿做其他用途，如有版权问题，请联系私信本专栏。<br/>希望大家看完本文，能够对自己有所启发，并祝愿各位能做出突出研究。</blockquote><p class=\"ztext-empty-paragraph\"><br/></p><p>仓库： <a href=\"https://link.zhihu.com/?target=https%3A//github.com/NeuronDance/DeepRL\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/NeuronDance/</span><span class=\"invisible\">DeepRL</span><span class=\"ellipsis\"></span></a></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-013b791a50891bde70eb8b4561d34885_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"430\" data-rawheight=\"430\" class=\"origin_image zh-lightbox-thumb\" width=\"430\" data-original=\"https://pic2.zhimg.com/v2-013b791a50891bde70eb8b4561d34885_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;430&#39; height=&#39;430&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"430\" data-rawheight=\"430\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"430\" data-original=\"https://pic2.zhimg.com/v2-013b791a50891bde70eb8b4561d34885_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-013b791a50891bde70eb8b4561d34885_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "论文写作", 
                    "tagLink": "https://api.zhihu.com/topics/19664288"
                }, 
                {
                    "tag": "论文", 
                    "tagLink": "https://api.zhihu.com/topics/19572442"
                }, 
                {
                    "tag": "SCI期刊", 
                    "tagLink": "https://api.zhihu.com/topics/20666238"
                }
            ], 
            "comments": [
                {
                    "userName": "知乎用户", 
                    "userLink": "https://www.zhihu.com/people/0", 
                    "content": "感谢分享，非常不错的资料！收藏了", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/81761367", 
            "userName": "DRLearner", 
            "userLink": "https://www.zhihu.com/people/2a0976472ac2864fa905e4ef0e8e5a36", 
            "upvote": 118, 
            "title": "61篇NIPS2019顶会深度强化学习论文汇总与部分解读", 
            "content": "<p>NeurIPS（前称NIPS）可谓人工智能年度最大盛会。每年圣诞前夕，全球的人工智能爱好者和科学家都会在这里聚集，发布最新研究，并进行热烈探讨。这不仅是一次大的party，也是一次重要的技术发展指向，大会的技术往往这未来几年就会演变成真正的研究甚至应用成果。今年的大会将在12月8日-14日在加拿大温哥华举行，据官方消息，NeurIPS今年共收到投稿6743篇，再次打破了历年来的接收记录。今年接收论文1429篇，其中，Oral论文36篇，占比0.5%；Spotlight论文接收量为164篇，占比2.4%。</p><p>今年NeurIPS接受的论文中，接收论文数量最多的机构还是<b>Google，共179篇</b>，其中<b>Deepmind参与的有53篇</b>，Google/Google Brain/Google Research有126篇，远远超出了其他家：Facebook上榜39篇，NVIDIA上榜9篇。高校方面：斯坦福上榜79篇，MIT上榜77篇，卡耐基梅隆上榜75篇。国内，高校方面清华参与的共有35篇，北大26篇，中科大7篇，西安交通大学5篇，还有大连理工也有2篇论文被收录。国内企业投稿也很多，腾讯上榜18篇，阿里巴巴上榜10篇，百度5篇，而最近准备赴港上市的旷视也上榜两篇。</p><p>而从论文题目来看 + <b>强化学习61篇，占比：4.2%</b> +</p><p> &gt;理论大约（21）篇 ，强化学习技巧（3）篇 ，框架大约（3）篇 ，探索和利用（1）篇 ，元强化学习（4）篇 ，分层强化学习（2）篇， 逆强化学习（2）篇 ，多智能体（6）篇， 奖励函数（2）篇 ，应用（6）篇 ，其他（4）篇 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-f519afcc034353f19bb56fa962ae7a1a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1352\" data-rawheight=\"814\" class=\"origin_image zh-lightbox-thumb\" width=\"1352\" data-original=\"https://pic3.zhimg.com/v2-f519afcc034353f19bb56fa962ae7a1a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1352&#39; height=&#39;814&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1352\" data-rawheight=\"814\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1352\" data-original=\"https://pic3.zhimg.com/v2-f519afcc034353f19bb56fa962ae7a1a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-f519afcc034353f19bb56fa962ae7a1a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p class=\"ztext-empty-paragraph\"><br/></p><blockquote> 内容总共分为两部分：      第一部分：论文标题与链接       第二部分：论文详细解读</blockquote><h2>第一部分：论文标题与链接</h2><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p class=\"ztext-empty-paragraph\"><br/></p><h3>理论</h3><ul><li><b>Multi-View Reinforcement Learning</b> </li><li><b>Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update </b><a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1805.12375.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/pdf/1805.1237</span><span class=\"invisible\">5.pdf</span><span class=\"ellipsis\"></span></a>  </li><li><b>Information-Theoretic Confidence Bounds for Reinforcement Learning</b> </li><li><b>Regret Minimization for Reinforcement Learning by Evaluating the Optimal Bias Function </b><a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1906.05110.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/pdf/1906.0511</span><span class=\"invisible\">0.pdf</span><span class=\"ellipsis\"></span></a>  </li><li><b> Real-Time Reinforcement Learning</b> </li><li><b>Convergent Policy Optimization for Safe Reinforcement Learning</b> </li><li><b>Intrinsically Efficient, Stable, and Bounded Off-Policy Evaluation for Reinforcement Learning </b><a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1906.03735.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/pdf/1906.0373</span><span class=\"invisible\">5.pdf</span><span class=\"ellipsis\"></span></a>  </li><li><b>Propagating Uncertainty in Reinforcement Learning via Wasserstein Barycenters</b> </li><li><b>A Geometric Perspective on Optimal Representations for Reinforcement Learning </b><a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1901.11530.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/pdf/1901.1153</span><span class=\"invisible\">0.pdf</span><span class=\"ellipsis\"></span></a>  </li><li><b>Finite-Time Performance Bounds and Adaptive Learning Rate Selection for Two Time-Scale Reinforcement Learning</b> <a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1907.06290.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/pdf/1907.0629</span><span class=\"invisible\">0.pdf</span><span class=\"ellipsis\"></span></a>  </li><li><b>Interval Timing in Deep Reinforcement Learning Agents </b><a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1905.13469v1.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/pdf/1905.1346</span><span class=\"invisible\">9v1.pdf</span><span class=\"ellipsis\"></span></a>  </li><li><b>Non-Stationary Markov Decision Processes, a Worst-Case Approach using Model-Based Reinforcement Learning</b> <a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1904.10090.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/pdf/1904.1009</span><span class=\"invisible\">0.pdf</span><span class=\"ellipsis\"></span></a>  </li><li><b>Budgeted Reinforcement Learning in Continuous State Space </b><a href=\"https://link.zhihu.com/?target=http%3A//ncarrara.fr/others/ncarrara_budgeted_rl.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">ncarrara.fr/others/ncar</span><span class=\"invisible\">rara_budgeted_rl.pdf</span><span class=\"ellipsis\"></span></a>  </li><li><b>Marginalized Off-Policy Evaluation for Reinforcement Learning</b> <a href=\"https://link.zhihu.com/?target=https%3A//tengyangxie.github.io/papers/xie2018nips_cl.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">tengyangxie.github.io/p</span><span class=\"invisible\">apers/xie2018nips_cl.pdf</span><span class=\"ellipsis\"></span></a>  </li><li><b>Regularized Anderson Acceleration for Off-Policy Deep Reinforcement Learning</b> </li><li><b>Tight Regret Bounds for Model-Based Reinforcement Learning with Greedy Policies </b><a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1905.11527.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/pdf/1905.1152</span><span class=\"invisible\">7.pdf</span><span class=\"ellipsis\"></span></a>  </li><li><b>Regret Bounds for Learning State Representations in Reinforcement Learning</b> </li><li><b> Reinforcement Learning with Convex Constraints</b> <a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1906.09323.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/pdf/1906.0932</span><span class=\"invisible\">3.pdf</span><span class=\"ellipsis\"></span></a>  </li><li><b> Correlation Priors for Reinforcement Learning</b> </li><li><b> Policy Poisoning in Batch Reinforcement Learning and Control</b> </li><li><b>Imitation-Projected Policy Gradient for Programmatic Reinforcement Learning </b><a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1907.05431.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/pdf/1907.0543</span><span class=\"invisible\">1.pdf</span><span class=\"ellipsis\"></span></a>  </li></ul><h3>强化学习技巧</h3><ul><li><b>The Option Keyboard: Combining Skills in Reinforcement Learning</b> </li><li><b> When to use parametric models in reinforcement learning? </b><a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1906.05243.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/pdf/1906.0524</span><span class=\"invisible\">3.pdf</span><span class=\"ellipsis\"></span></a>  </li><li><b>Robust exploration in linear quadratic reinforcement learning  </b><a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1906.01584.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/pdf/1906.0158</span><span class=\"invisible\">4.pdf</span><span class=\"ellipsis\"></span></a>  </li></ul><h3>框架</h3><ul><li><b>A Regularized Approach to Sparse Optimal Policy in Reinforcement Learning </b><a href=\"https://link.zhihu.com/?target=http%3A//export.arxiv.org/pdf/1903.00725\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">export.arxiv.org/pdf/19</span><span class=\"invisible\">03.00725</span><span class=\"ellipsis\"></span></a>  </li><li><b> A Variational Inference Framework for Reinforcement Learning </b><a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1811.01132.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/pdf/1811.0113</span><span class=\"invisible\">2.pdf</span><span class=\"ellipsis\"></span></a>  </li><li><b> Gossip-based Actor-Learner Architectures for Deep Reinforcement Learning</b> <a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1906.04585.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/pdf/1906.0458</span><span class=\"invisible\">5.pdf</span><span class=\"ellipsis\"></span></a>  </li></ul><h3>探索和利用</h3><ul><li><b>Explicit Planning for Efficient Exploration in Reinforcement Learning</b></li></ul><h3>元强化学习</h3><ul><li><b>A Meta-MDP Approach to Exploration for Lifelong Reinforcement Learning </b><a href=\"https://link.zhihu.com/?target=https%3A//people.cs.umass.edu/~fmgarcia/Papers/MetaMDP_Paper.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">people.cs.umass.edu/~fm</span><span class=\"invisible\">garcia/Papers/MetaMDP_Paper.pdf</span><span class=\"ellipsis\"></span></a>  </li><li><b>SMILe: Scalable Meta Inverse Reinforcement Learning through Context-Conditional Policies</b> </li><li><b> Unsupervised Curricula for Visual Meta-Reinforcement Learning </b><a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1806.04640.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/pdf/1806.0464</span><span class=\"invisible\">0.pdf</span><span class=\"ellipsis\"></span></a>  </li><li><b>Meta-Inverse Reinforcement Learning with Probabilistic Context Variables</b> </li></ul><h3>分层强化学习</h3><ul><li><b>Hierarchical Reinforcement Learning with Advantage-Based Auxiliary Rewards</b> </li><li><b> Language as an Abstraction for Hierarchical Deep Reinforcement Learning(Google) </b><a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1906.07343.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/pdf/1906.0734</span><span class=\"invisible\">3.pdf</span><span class=\"ellipsis\"></span></a>  </li></ul><h3>逆强化学习</h3><ul><li><b> Learner-aware Teaching: Inverse Reinforcement Learning with Preferences and Constraints</b> <a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1906.00429.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/pdf/1906.0042</span><span class=\"invisible\">9.pdf</span><span class=\"ellipsis\"></span></a>  </li><li><b>On the Correctness and Sample Complexity of Inverse Reinforcement Learning </b><a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1906.00422.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/pdf/1906.0042</span><span class=\"invisible\">2.pdf</span><span class=\"ellipsis\"></span></a>  </li></ul><h3>多智能体</h3><ul><li><b>Regret Minimization for Reinforcement Learning on Multi-Objective Online Markov Decision Processes</b> </li><li><b> Value Propagation for Decentralized Networked Deep Multi-agent Reinforcement </b>Learning <a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1901.09326\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/abs/1901.0932</span><span class=\"invisible\">6</span><span class=\"ellipsis\"></span></a>  </li><li><b> Efficient Communication in Multi-Agent Reinforcement Learning via Variance Based Control</b> </li><li><b> LIIR: Learning Individual Intrinsic Reward in Multi-Agent Reinforcement Learning</b> </li><li><b> A Structured Prediction Approach for Generalization in Cooperative Multi-Agent Reinforcement Learning</b> </li><li><b> Multi-Agent Common Knowledge Reinforcement Learning </b><a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1810.11702.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/pdf/1810.1170</span><span class=\"invisible\">2.pdf</span><span class=\"ellipsis\"></span></a>  </li></ul><h3>奖励函数</h3><ul><li><b>Distributional Reward Decomposition for Reinforcement Learning</b> </li><li><b> Learning Reward Machines for Partially Observable Reinforcement Learning</b> </li></ul><h3>应用</h3><ul><li><b>Staying up to Date with Online Content Changes Using Reinforcement Learning for Scheduling </b>   <a href=\"https://link.zhihu.com/?target=https%3A//openreview.net/pdf%3Fid%3DByxjGvuKoE\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">openreview.net/pdf?</span><span class=\"invisible\">id=ByxjGvuKoE</span><span class=\"ellipsis\"></span></a>  </li><li><b>Loaded DiCE: Trading off Bias and Variance in Any-Order Score Function Gradient Estimators for Reinforcement Learning</b> </li><li><b> InteractiveRecGAN: a Model Based Reinforcement Learning Method with Adversarial Training for Online Recommendation</b> </li><li><b> A Composable Specification Language for Reinforcement Learning Tasks</b> </li><li><b> Near-Optimal Reinforcement Learning in Dynamic Treatment Regimes</b> </li><li><b> Constraint Augmented Reinforcement Learning for Text-based Recommendation and Generation</b> </li></ul><h3>其他</h3><ul><li><b>Generalization in Reinforcement Learning with Selective Noise Injection and Information Bottleneck</b> </li><li><b> Using a Logarithmic Mapping to Enable Lower Discount Factors in Reinforcement Learning</b> </li><li><b> Search on the Replay Buffer: Bridging Planning and Reinforcement Learnin</b></li><li><b>A Family of Robust Stochastic Operators for Reinforcement Learning</b></li></ul><h2>第二部分：论文详细解读</h2><p>理论方法 + <b>Multi-View Reinforcement Learning(UCL出的论文，很遗憾没找到paper)</b> + <b>Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update</b></p><blockquote> 该文章提出了一种具有直接值传播的新型深度强化学习算法Episodic Backward Update（EBU）。它的代理对整个事件进行采样并且连续地将状态的值传播到其先前的状态。通过计算效率高的递归算法允许稀疏和延迟奖励直接传播到采样情节的所有转换中。 作者在理论上证明了EBU方法的收敛性，并在确定性和随机环境中实验证明了它的性能。 特别是在Atari 2600域的49场比赛中，EBU分别仅使用5％和10％的样本，达到了相同的DQN均值和中值人归一化性能。 </blockquote><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-3c72cea3f5f746d86d0e53f5fd74afb8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"415\" data-rawheight=\"216\" class=\"content_image\" width=\"415\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;415&#39; height=&#39;216&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"415\" data-rawheight=\"216\" class=\"content_image lazy\" width=\"415\" data-actualsrc=\"https://pic1.zhimg.com/v2-3c72cea3f5f746d86d0e53f5fd74afb8_b.jpg\"/></figure><ul><li><b>Information-Theoretic Confidence Bounds for Reinforcement Learning(强化学习的信息理论置信度)</b> </li><li><b>Regret Minimization for Reinforcement Learning by Evaluating the Optimal Bias Function</b> 该论文提出了一种基于面对不确定性（OFU）原理的算法，该算法能够有效地学习具有有限状态作用空间的马尔可夫决策过程（MDP）建模的强化学习（RL）,其通过评估最优偏差函数的状态对差异，在h*跨度的上限H的情况下，提出的算法实现了具有S状态和A动作的MDP的后悔界限，其边界如下。【这是清华大学出的一篇包含大量数学证明的论文】。 </li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-f4f4d2a6b8b2786e0b6d596047782426_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"414\" data-rawheight=\"215\" class=\"content_image\" width=\"414\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;414&#39; height=&#39;215&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"414\" data-rawheight=\"215\" class=\"content_image lazy\" width=\"414\" data-actualsrc=\"https://pic3.zhimg.com/v2-f4f4d2a6b8b2786e0b6d596047782426_b.jpg\"/></figure><ul><li><b>Real-Time Reinforcement Learning(实时强化学习)</b> </li><li><b>Convergent Policy Optimization for Safe Reinforcement Learning</b> </li><li><b>Intrinsically Efficient, Stable, and Bounded Off-Policy Evaluation for Reinforcement Learning</b>   在非策略评估（OPE）中允许人们在不需要进行探索的情况下评估新颖的决策政策，这通常是昂贵的或不可行的。该问题的重要性吸引了许多提出的解决方案，包括重要性抽样（IS），自标准化IS（SNIS）和双重鲁棒（DR）估计。作者基于经验似然提出了新的OPE估计，这些估计总是比IS，SNIS和DR更有效，并且满足与SNIS相同的稳定性和有界性。通过它们对现有的估算器进行分类，除了理论上的保证外，实证研究表明新的估算器具有优势。    </li><li><b>Propagating Uncertainty in Reinforcement Learning via Wasserstein Barycenters</b> </li><li><b>A Geometric Perspective on Optimal Representations for Reinforcement Learning（★★★★，推荐看）</b>   本文是一篇由DeepMind, GoogleBrain,牛津大学等共同发表的文章，作者基于价值函数空间的几何性质，提出了强化学习中表征学习的新视角。其利用这种观点提供关于价值函数作为辅助任务的有用性的正式证据。 并表述考虑调整表示以最小化给定环境的所有固定策略的值函数的（线性）近似，这种优化减少了对一类特殊值函数的准确预测，他们将其称为对抗值函数（AVF）。 结果证明使用值函数作为辅助任务对应于我公式的预期误差放宽，AVF是一个自然的候选者，并确定与原始值函数的密切关系（Mahadevan，2005）。 其强调了AVF的特征及其在四室域系列实验中作为辅助任务的实用性。 </li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-df7944935fbd85c8d72caa73d7a8035a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"432\" data-rawheight=\"88\" class=\"origin_image zh-lightbox-thumb\" width=\"432\" data-original=\"https://pic3.zhimg.com/v2-df7944935fbd85c8d72caa73d7a8035a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;432&#39; height=&#39;88&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"432\" data-rawheight=\"88\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"432\" data-original=\"https://pic3.zhimg.com/v2-df7944935fbd85c8d72caa73d7a8035a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-df7944935fbd85c8d72caa73d7a8035a_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-297ebed01743e7e952d58021e8ba08f8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"415\" data-rawheight=\"131\" class=\"content_image\" width=\"415\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;415&#39; height=&#39;131&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"415\" data-rawheight=\"131\" class=\"content_image lazy\" width=\"415\" data-actualsrc=\"https://pic1.zhimg.com/v2-297ebed01743e7e952d58021e8ba08f8_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-6606da0ac3898e3f6340d39e4f8358da_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"414\" data-rawheight=\"87\" class=\"content_image\" width=\"414\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;414&#39; height=&#39;87&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"414\" data-rawheight=\"87\" class=\"content_image lazy\" width=\"414\" data-actualsrc=\"https://pic3.zhimg.com/v2-6606da0ac3898e3f6340d39e4f8358da_b.jpg\"/></figure><ul><li><b>Finite-Time Performance Bounds and Adaptive Learning Rate Selection for Two Time-Scale Reinforcement Learning</b>   作者研究了两种时间尺度的线性随机近似算法，可用于模拟众所周知的强化学习算法，如GTD，GTD2和TDC。 其给出了学习率固定的情况下的有限时间性能界限，并获得这些界限的关键思想是使用由奇异摄动理论驱动的线性微分方程的Lyapunov函数（Lyapunov function motivated by singular perturbation theory for linear differential equations）。 并使用边界来设计自适应学习速率方案，该方案在实验中显着提高了已知最优多项式衰减规则的收敛速度，并且可用于潜在地改善学习速率在前期改变的任何其他时间表的性能。 确定的时间瞬间。   </li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-0591cf4424af8afa4b34a5d8eb2b8baf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"415\" data-rawheight=\"194\" class=\"content_image\" width=\"415\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;415&#39; height=&#39;194&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"415\" data-rawheight=\"194\" class=\"content_image lazy\" width=\"415\" data-actualsrc=\"https://pic4.zhimg.com/v2-0591cf4424af8afa4b34a5d8eb2b8baf_b.jpg\"/></figure><ul><li><b>Interval timing in deep reinforcement learning agents（DeepMind）</b>   作者认为时间的测量是智能行为的核心。一般知道动物和人工代理都可以成功地使用时间依赖来选择动作。在人工智能体中，很少有工作直接解决（1）哪些架构组件是成功开发此能力所必需的，（2）如何在代理的单元和动作中表示这种时序能力，以及（3）是否系统的最终行为会集中在类似于生物学的解决方案上。在这里，我们研究了深度强化学习智能体的区间定时能力，这些代理能力是在间隔再生范式上进行端到端训练，这种范式受到关于时间机制的实验文献的启发。并描述了由经常性和前馈性代理人开发的策略，这些策略都使用不同的机制在时间再生上取得成功，其中一些机制与生物系统具有特定且有趣的相似性。这些发现推动了我们对代理人如何代表时间的理解，并突出了实验启发的方法来表征代理人能力的价值。 </li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-3aff3973f4416fcafaa3c5f79ffadee0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"415\" data-rawheight=\"98\" class=\"content_image\" width=\"415\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;415&#39; height=&#39;98&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"415\" data-rawheight=\"98\" class=\"content_image lazy\" width=\"415\" data-actualsrc=\"https://pic1.zhimg.com/v2-3aff3973f4416fcafaa3c5f79ffadee0_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-0ac2495106fea724ba2540f0ba97e9d5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"265\" data-rawheight=\"88\" class=\"content_image\" width=\"265\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;265&#39; height=&#39;88&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"265\" data-rawheight=\"88\" class=\"content_image lazy\" width=\"265\" data-actualsrc=\"https://pic2.zhimg.com/v2-0ac2495106fea724ba2540f0ba97e9d5_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-9b9139c4190f8197ce5bb2aa07872ad3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"331\" data-rawheight=\"349\" class=\"content_image\" width=\"331\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;331&#39; height=&#39;349&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"331\" data-rawheight=\"349\" class=\"content_image lazy\" width=\"331\" data-actualsrc=\"https://pic4.zhimg.com/v2-9b9139c4190f8197ce5bb2aa07872ad3_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><ul><li><b>Non-Stationary Markov Decision Processes, a Worst-Case Approach using Model-Based Reinforcement Learning</b>   这项工作解决了非平稳随机环境中强大的零射击规划问题。作者研究马尔可夫决策过程（MDP）随时间演变并在此设置中考虑基于模型的强化学习算法。提出两个假设：1）环境以有限的进化速率不断演变; 2）当前模型在每个决策时期都是已知的，但不是它的演化。我们的贡献可以分为四点。 1）定义了一类特定的MDP，称之为非固定MDP（NSMDP）。作者通过对过渡和奖励函数w.r.t做出Lipschitz-Continuity的假设来介绍常规进化的概念。时间; 2）考虑使用当前环境模型但未意识到其未来发展的计划代理。这导致我们考虑一种最坏情况的方法，其中环境被视为对抗剂; 3）遵循这种方法，最后作者提出风险反向树搜索（RATS）算法，一种类似于Minimax搜索的基于模型的零射击方法; 4）通过经验证明了RATS带来的好处，并将其性能与基于模型的参考算法进行了比较。 </li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-a2778293b6bb4c0771e4497d71cb70d1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"415\" data-rawheight=\"207\" class=\"content_image\" width=\"415\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;415&#39; height=&#39;207&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"415\" data-rawheight=\"207\" class=\"content_image lazy\" width=\"415\" data-actualsrc=\"https://pic2.zhimg.com/v2-a2778293b6bb4c0771e4497d71cb70d1_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-3a659b9d1fa827829c8ec13e89d1cf3b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"415\" data-rawheight=\"303\" class=\"content_image\" width=\"415\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;415&#39; height=&#39;303&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"415\" data-rawheight=\"303\" class=\"content_image lazy\" width=\"415\" data-actualsrc=\"https://pic4.zhimg.com/v2-3a659b9d1fa827829c8ec13e89d1cf3b_b.jpg\"/></figure><ul><li><b>Budgeted Reinforcement Learning in Continuous State Space</b>   预算马尔可夫决策过程（BMDP）是马尔可夫决策过程对需要安全约束的关键应用的扩展。 它依赖于以成本信号的形式实施的风险概念，该成本信号被限制在低于-可调-阈值。 到目前为止，BMDP只能在具有已知动态的有限状态空间的情况下求解。 这项工作将最先进的技术扩展到连续的空间环境和未知的动态。 作者证明了BMDP的解决方案是一个新的预算贝尔曼最优性算子的固定点。 这一观察结果使我们能够引入Deep Reinforcement Learning算法的自然扩展来解决大规模BMDP问题，并在口语对话和自动驾驶两个模拟应用上验证了方法： </li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-3f21d1c89f564c536d3fde569d48b90b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"415\" data-rawheight=\"195\" class=\"content_image\" width=\"415\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;415&#39; height=&#39;195&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"415\" data-rawheight=\"195\" class=\"content_image lazy\" width=\"415\" data-actualsrc=\"https://pic4.zhimg.com/v2-3f21d1c89f564c536d3fde569d48b90b_b.jpg\"/></figure><ul><li><b>Marginalized Off-Policy Evaluation for Reinforcement Learning</b>   非策略评估涉及使用不同行为政策获得的历史数据评估政策的绩效。 在强化学习的现实应用中，策略政策可能成本高昂且危险，而策略外评估通常是关键步骤。 目前，离线策略评估的现有方法主要基于离散树MDP的马尔可夫决策过程（MDP）模型，并且由于重要性权重的累积乘积而具有高方差。 在本文中，作者直接基于离散有向无环图（DAG）MDP提出了一种新的非策略评估方法。方法可以应用于大多数非策略评估的估算，无需修改，可以显着减少差异。 作者还对方法进行了理论分析，并通过实证结果对其进行了评估。 </li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-99e562ea0cdf72e574b9ff5d5ac3ed2d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"415\" data-rawheight=\"140\" class=\"content_image\" width=\"415\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;415&#39; height=&#39;140&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"415\" data-rawheight=\"140\" class=\"content_image lazy\" width=\"415\" data-actualsrc=\"https://pic2.zhimg.com/v2-99e562ea0cdf72e574b9ff5d5ac3ed2d_b.jpg\"/></figure><ul><li><b>Regularized Anderson Acceleration for Off-Policy Deep Reinforcement Learning</b> </li><li><b>Tight Regret Bounds for Model-Based Reinforcement Learning with Greedy Policies</b>   最先进的基于模型的有效强化学习（RL）算法通常通过迭代求解经验模型来行动，即，通过对所收集的经验构建的马尔可夫决策过程（MDP）执行完全规划。在本文中，作者关注有限状态有限时间MDP设置中基于模型的RL，并建立用贪婪策略进行探索 - 通过one-step planning行动,可以实现紧密的极小极大表现为reget。因此，可以完全避免基于模型的RL中的完全规划而不会降低性能，并且通过这样做，计算复杂度降低了S因子。结果基于对实时动态编程的新颖分析，然后扩展到基于模型的RL。具体而言，作者概括了执行完整计划的现有算法，以便通过一步规划来实现。对于这些概括，最后以与其全规划对应方相同的速率证明了后悔。    </li><li><b>Regret Bounds for Learning State Representations in Reinforcement Learning</b> </li><li><b>Reinforcement Learning with Convex Constraints （普林斯顿、微软）</b>   在标准强化学习（RL）中，学习智能体寻求优化整体奖励。然而，期望行为的许多关键方面更自然地表达为约束。例如，设计者可能想要限制不安全动作的使用，增加轨迹的多样性以实现探索，或者在奖励稀疏时近似专家轨迹。在本文中，作者提出了一种算法方案，可以处理RL任务中的一大类约束：具体而言，任何需要某些向量测量的期望值（例如使用动作）的约束都位于凸集中。这捕获了先前研究的约束（例如安全性和与专家的接近度），但也实现了新的约束类（例如多样性）。作者的方法具有严格的理论保证，并且仅依赖于近似解决标准RL任务的能力。因此，它可以很容易地适用于任何无模型或基于模型的RL。在实验中，结果表明它匹配以前通过约束强制实施安全性的算法，但也可以强制执行这些算法不包含的新属性，例如多样性。 </li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-49529852bd953e9da4caa8e16822b6d4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"414\" data-rawheight=\"177\" class=\"content_image\" width=\"414\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;414&#39; height=&#39;177&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"414\" data-rawheight=\"177\" class=\"content_image lazy\" width=\"414\" data-actualsrc=\"https://pic1.zhimg.com/v2-49529852bd953e9da4caa8e16822b6d4_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-f932e375bfe4b5b6e18ed2dd10e48b26_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"234\" data-rawheight=\"191\" class=\"content_image\" width=\"234\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;234&#39; height=&#39;191&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"234\" data-rawheight=\"191\" class=\"content_image lazy\" width=\"234\" data-actualsrc=\"https://pic3.zhimg.com/v2-f932e375bfe4b5b6e18ed2dd10e48b26_b.jpg\"/></figure><ul><li><b>Correlation Priors for Reinforcement Learning</b> </li><li><b>Policy Poisoning in Batch Reinforcement Learning and Control</b> </li><li><b>Imitation-Projected Policy Gradient for Programmatic Reinforcement Learning</b>   作者提出了模仿预测策略梯度（IPPG），这是一种用于学习策略的算法框架，这些策略以结构化编程语言简洁地表示。与神经政策相比，此类计划政策可以更具解释性，可推广性，并且易于进行形式验证;然而，为计划政策设计严格的学习方法仍然是一项挑战。 IPPG，作者对这一挑战的回应，基于三个见解。首先，他将学习任务视为策略空间中的优化，模拟所需策略具有编程表示的约束，并使用“提升和项目”视角解决此优化问题，该视角采用渐变步骤进入无约束策略空间然后投射到受约束的空间。其次，将无约束的政策空间视为混合神经和程序化表示，这使得能够采用最先进的深度政策梯度方法。第三，通过模仿学习将投射步骤作为程序综合，并利用当代组合方法完成这项任务。最后提出了IPPG的理论收敛结果，以及三个连续控制域的经验评估。实验表明，IPPG可以明显优于现有技术 </li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-eac509a624ea14da9b928cb573b2f9e5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"414\" data-rawheight=\"143\" class=\"content_image\" width=\"414\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;414&#39; height=&#39;143&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"414\" data-rawheight=\"143\" class=\"content_image lazy\" width=\"414\" data-actualsrc=\"https://pic2.zhimg.com/v2-eac509a624ea14da9b928cb573b2f9e5_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-cbe1b4b58a16d61051a270dbbe119404_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"170\" data-rawheight=\"135\" class=\"content_image\" width=\"170\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;170&#39; height=&#39;135&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"170\" data-rawheight=\"135\" class=\"content_image lazy\" width=\"170\" data-actualsrc=\"https://pic1.zhimg.com/v2-cbe1b4b58a16d61051a270dbbe119404_b.png\"/></figure><h3>强化学习优化技巧</h3><ul><li><b>The Option Keyboard: Combining Skills in Reinforcement Learning</b> </li><li><b>When to use parametric models in reinforcement learning?（DeepMind, ★★★★）</b>   作者研究了参数化模型何时以及如何在强化学习中最有用的问题。 特别是，我们研究参数模型和体验重放之间的共性和差异。 基于重放的学习算法与基于模型的方法共享重要特征，包括计划：使用更多计算而无需额外数据来改进预测和行为的能力。 我们讨论何时期望从这两种方法中获益，并在此背景下解释先前的工作。 我们假设，在适当的条件下，基于重放的算法应该比基于模型的算法具有竞争力或者更好，如果该模型仅用于从观察状态生成虚构的过渡，则更新规则是无模型的。 我们在Atari 2600视频游戏中验证了这一假设。 基于重放的算法获得了最先进的数据效率，与参数模型的先前结果相比有所改进。    </li><li><b>Robust exploration in linear quadratic reinforcement learning</b>   该论文讨论了学习未知线性动力系统控制策略的问题，以最小化二次成本函数。作者提出了一种基于凸优化的方法，它可以稳健地完成这项任务：即最小化最坏情况成本，考虑到观测数据给出的系统不确定性。 该方法平衡了开发和探索，以这种方式激励系统，以减少最坏情况成本最敏感的模型参数的不确定性。 硬件在环伺服机构的数值模拟和应用证明了这种方法，与两者中观察到的替代方法相比，具有可观的性能和鲁棒性。    </li></ul><h3>框架</h3><ul><li><b>A Regularized Approach to Sparse Optimal Policy in Reinforcement Learning（北京大学）</b>   我们提出并研究正则化马尔可夫决策过程（MDP）的一般框架，其目标是找到最大化预期贴现总奖励加上政策正规化期限的最优政策。现存的熵正则化MDP可以投射到我们的框架中。此外，在我们的框架下，许多正则化术语可以带来多模态和稀疏性，这在强化学习中可能是有用的。特别是，我们提出了足够和必要的条件，导致稀疏的最优政策。我们还对所提出的正则化MDP进行了全面的数学分析，包括最优性条件，性能误差和稀疏度控制。我们提供了一种通用的方法来设计正规化形式，并在复杂的环境设置中提出非策略行为者批评算法。我们实证分析了最优策略的数值性质，并比较了离散和连续环境中不同稀疏正则化形式的性能 </li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-3b65052f59c97d6d6c670742eece59d2_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"163\" data-rawheight=\"162\" class=\"content_image\" width=\"163\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;163&#39; height=&#39;162&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"163\" data-rawheight=\"162\" class=\"content_image lazy\" width=\"163\" data-actualsrc=\"https://pic3.zhimg.com/v2-3b65052f59c97d6d6c670742eece59d2_b.png\"/></figure><ul><li><b>A Variational Inference Framework for Reinforcement Learning（牛津大学）</b>   作者试图将概率模型应用于强化学习（RL）使得能够应用强大的优化工具，例如对RL的变分推理。然而，现有的推理框架及其算法对学习最优策略提出了重大挑战，例如，在伪似然方法中缺少模式捕获行为以及在基于最大熵RL的方法中学习确定性策略的困难。于是提出了VIREL，一种新的，理论上基于RL的概率推理框架，它利用参数化的动作-值函数来总结底层MDP的未来动态。这使得VIREL成为一种模式寻求形式的KL分歧，能够自然地从推理中学习确定性最优策略，以及在单独的迭代步骤中优化价值函数和策略的能力，在将变分期望最大化应用于VIREL时，结果表明，演员 -评论者算法可以减少到期望最大化，政策改进等同于E步骤和政策评估到M步骤。然后从VIREL推导出一系列演员评论方法，包括一个适应性探索方案。最后，作者证明了来自这个家族的演员评论算法在几个领域的表现优于基于软值函数的最新方法 </li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-1fa6d51f48b0b7cd48de363fa4e8b514_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"190\" data-rawheight=\"143\" class=\"content_image\" width=\"190\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;190&#39; height=&#39;143&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"190\" data-rawheight=\"143\" class=\"content_image lazy\" width=\"190\" data-actualsrc=\"https://pic1.zhimg.com/v2-1fa6d51f48b0b7cd48de363fa4e8b514_b.png\"/></figure><ul><li><b>Gossip-based Actor-Learner Architectures for Deep Reinforcement Learning（FaceBook）</b>   多模拟器培训通过稳定学习和提高培训吞吐量，为最近深度强化学习的成功做出了贡献。作者提出了基于Gossip的Actor-Learner Architectures（GALA），其中几个演员学习者（如A2C代理人）以对等通信拓扑结构组织，并通过异步八卦交换信息以利用大量分布式模拟器。并证明在使用松散耦合的异步通信时，GALA代理在训练期间保持在一个ε-球之间。通过减少代理之间的同步量，与A2C（其完全同步的对应物）相比，GALA在计算上更有效且可扩展。 GALA也优于A2C，更加强大，样品效率更高。最后作者展示了可以在单个GPU上并行运行几个松散耦合的GALA代理，并且在可比功耗方面实现了比朴素A2C更高的硬件利用率和帧速率。 </li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-91a9d5935a530474231e06f9088db2ea_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"415\" data-rawheight=\"171\" class=\"content_image\" width=\"415\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;415&#39; height=&#39;171&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"415\" data-rawheight=\"171\" class=\"content_image lazy\" width=\"415\" data-actualsrc=\"https://pic3.zhimg.com/v2-91a9d5935a530474231e06f9088db2ea_b.jpg\"/></figure><h3>探索与利用</h3><ul><li><b>Explicit Planning for Efficient Exploration in Reinforcement Learning</b></li></ul><h3>元强化学习</h3><ul><li><b>A Meta-MDP Approach to Exploration for Lifelong Reinforcement Learning</b>   在该论文中，作者考虑一个强化学习代理如何解决一系列强化学习问题（一系列马尔可夫决策过程）可以使用在其生命早期获得的知识来提高其解决新问题的能力的问题。 具体而言，他们关注的是智能体在面对新环境时应该如何探索的问题。尽管时间尺度不同，但寻找最优勘探策略本身可以作为强化学习问题。 作者通过实验得出结论，这些实验显示了使用我们提出的方法优化勘探策略的好处。    </li><li><b>SMILe: Scalable Meta Inverse Reinforcement Learning through Context-Conditional Policies</b> </li><li><b>Unsupervised Curricula for Visual Meta-Reinforcement Learning（UCB，Google，★★★）</b>   UCB对元学习的研究可以说是出于前沿，本文基于元学习是一种强大的工具，它建立在多任务学习的基础上，以学习如何快速地将模型适应新任务。在强化学习的背景下，元学习算法可以获得强化学习过程，通过元学习先验任务更有效地解决新问题。元学习算法的性能关键取决于可用于元训练的任务：与监督学习算法最佳地概括为从与训练点相同的分布中绘制的测试点一样，元学习方法最好地概括为来自与元训练任务相同的分布。实际上，元强化学习可以减轻从算法设计到任务设计的设计负担。如果我们也可以自动化任务设计过程，我们可以设计一个真正自动化的元学习算法。在这项工作中，作者朝这个方向迈出了一步，提出了一系列无监督的元学习算法，用于强化学习。描述了无监督元强化学习的一般方法，并基于最近提出的无监督探索技术和模型无关的元学习描述了该方法的有效实例化。还讨论了开发无监督元学习方法的实践和概念考虑。实验结果表明，无监督的元强化学习有效地获得了加速强化学习过程，而不需要手动任务设计，显着超过了从头学习的性能，甚至匹配使用手工指定任务分布的元学习方法的性能。 </li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-59da568755afd0b3909de44b01f13c4f_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"191\" data-rawheight=\"131\" class=\"content_image\" width=\"191\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;191&#39; height=&#39;131&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"191\" data-rawheight=\"131\" class=\"content_image lazy\" width=\"191\" data-actualsrc=\"https://pic4.zhimg.com/v2-59da568755afd0b3909de44b01f13c4f_b.png\"/></figure><ul><li><b>Meta-Inverse Reinforcement Learning with Probabilistic Context Variables</b> </li></ul><h3>分层强化学习</h3><ul><li><b>Hierarchical Reinforcement Learning with Advantage-Based Auxiliary Rewards</b> </li><li><b>Language as an Abstraction for Hierarchical Deep Reinforcement Learning(Google)</b>   解决复杂的，时间延长的任务是强化学习（RL）中长期存在的问题。我们假设解决这些问题的一个关键因素是组合性概念。通过学习可以组成以解决更长任务的概念和子技能的能力，即分级RL，我们可以获得时间延长的行为。然而，获得有层次的RL的有效但一般的抽象是非常具有挑战性的。在本文中，我们建议使用语言作为抽象，因为它提供了独特的组合结构，实现了快速学习和组合泛化，同时保留了极大的灵活性，使其适用于各种问题。我们的方法学习了一个遵循指令的低级策略和一个高级策略，可以重复跨任务的抽象，实质上是允许代理使用结构化语言进行推理。为了研究组合任务学习，我们介绍了使用MuJoCo物理引擎和CLEVR引擎构建的开源对象交互环境。我们发现，使用我们的方法，代理可以学习解决各种时间扩展的任务，例如对象排序和多对象重新排列，包括原始像素观察。我们的分析发现，与使用相同监督的非组合抽象相比，语言的组成性质对于学习各种子技能和系统地推广到新的子技能至关重要。    </li></ul><h3>逆强化学习</h3><ul><li><b>Learner-aware Teaching: Inverse Reinforcement Learning with Preferences and Constraints（微软）</b>   反向强化学习（IRL）使智能体能够通过观察（近似）最优策略的演示来学习复杂行为。 典型的假设是学习者的目标是匹配教师所展示的行为。 在本文中，作者考虑了学习者有自己喜好的环境，并将其考虑在内。 这些偏好可以例如捕获行为偏差，不匹配的世界观或物理约束。 他们研究了两种教学方法：学习者不可知教学，教师通过忽略学习者偏好的最优政策提供示范，以及教师考虑学习者偏好的学习者意识教学。 最后设计了学习者感知的教学算法，并表明与学习者无关的教学可以实现显着的性能提升。 </li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-d30a6961b2ab40d4539f6f08ed370da2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"401\" data-rawheight=\"235\" class=\"content_image\" width=\"401\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;401&#39; height=&#39;235&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"401\" data-rawheight=\"235\" class=\"content_image lazy\" width=\"401\" data-actualsrc=\"https://pic3.zhimg.com/v2-d30a6961b2ab40d4539f6f08ed370da2_b.jpg\"/></figure><ul><li><b>On the Correctness and Sample Complexity of Inverse Reinforcement Learning</b>   逆强化学习（IRL）是找到奖励函数的问题，该奖励函数为给定的马尔可夫决策过程生成给定的最优策略。 该论文着眼于有限状态和动作的IRL问题的算法无关几何分析。 然后提出了由几何分析驱动的IRL问题的L1正则化支持向量机公式，其中考虑了反向强化问题的基本目标：找到生成指定最优策略的奖励函数。 同时进一步分析了具有n个状态和k个动作的逆强化学习的拟议公式，并且显示了用于恢复奖励函数的O（n2log（nk））的样本复杂度，该奖励函数生成满足贝尔曼关于真实性的最优性条件的策略转换概率。  </li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-b69f46ba2685a40146d80f24ba9cd2ae_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"356\" data-rawheight=\"197\" class=\"content_image\" width=\"356\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;356&#39; height=&#39;197&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"356\" data-rawheight=\"197\" class=\"content_image lazy\" width=\"356\" data-actualsrc=\"https://pic3.zhimg.com/v2-b69f46ba2685a40146d80f24ba9cd2ae_b.jpg\"/></figure><h3>多智能体</h3><ul><li><b>Regret Minimization for Reinforcement Learning on Multi-Objective Online Markov Decision Processes</b> </li><li><b>Value Propagation for Decentralized Networked Deep Multi-agent Reinforcement Learning</b>   该论文使用了softmax时间一致性和分散优化方法，获得了原理和数据有效的迭代算法。在每次迭代的第一步中，代理计算其本地策略和值渐变，然后仅更新策略参数。并且代理根据其值函数将消息传播给其邻居，然后更新其自己的值函数。同时使用非线性函数逼近证明了非渐近收敛速度的过程。 </li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-3221229629768d75444d83265baf1b2c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"415\" data-rawheight=\"445\" class=\"content_image\" width=\"415\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;415&#39; height=&#39;445&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"415\" data-rawheight=\"445\" class=\"content_image lazy\" width=\"415\" data-actualsrc=\"https://pic1.zhimg.com/v2-3221229629768d75444d83265baf1b2c_b.jpg\"/></figure><ul><li><b>Efficient Communication in Multi-Agent Reinforcement Learning via Variance Based Control</b> </li><li><b>LIIR: Learning Individual Intrinsic Reward in Multi-Agent Reinforcement Learning</b> </li><li><b>A Structured Prediction Approach for Generalization in Cooperative Multi-Agent Reinforcement Learning</b> </li><li><b>Multi-Agent Common Knowledge Reinforcement Learning（牛津大学）</b>   协作多智能体强化学习通常需要分散的政策，这严重限制了智能体协调行为的能力。在本文中，作者表明代理之间的常识允许复杂的分散协调。在大量分散的协作多智能体任务中自然产生了常识，例如，当智能体可以重建彼此观察的部分时。由于智能体可以独立地就他们的共同知识达成一致，他们可以执行复杂的协调政策，以完全分散的方式对这些知识进行调整。作者提出了多智能体常识知识强化学习（MACKRL），一种学习分层策略树的新型随机行为者 - 批评算法。层次结构中的较高级别通过调整其常识来协调代理组，或者通过较小的子组委托较低级别但可能具有更丰富的常识。整个策略树可以完全分散的方式执行。由于最低的策略树级别由每个代理的独立策略组成，因此MACKRL将独立学习的分散策略简化为特殊情况。作者证明了方法可以利用常见知识在复杂的分散协调任务中获得卓越的性能，包括随机矩阵游戏和星际争霸II单元微观管理中的挑战性问题。 </li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-08f84947cf53272ef79dd583b98157e4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"415\" data-rawheight=\"192\" class=\"content_image\" width=\"415\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;415&#39; height=&#39;192&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"415\" data-rawheight=\"192\" class=\"content_image lazy\" width=\"415\" data-actualsrc=\"https://pic1.zhimg.com/v2-08f84947cf53272ef79dd583b98157e4_b.jpg\"/></figure><ul><li><b>Biases for Emergent Communication in Multi-agent Reinforcement Learning</b> </li></ul><h3>奖励函数</h3><ul><li><b>Distributional Reward Decomposition for Reinforcement Learning</b> </li><li><b>Learning Reward Machines for Partially Observable Reinforcement Learning</b> </li></ul><h3>应用</h3><ul><li><b>Staying up to Date with Online Content Changes Using Reinforcement Learning for Scheduling</b>   该论文阐述了一种关于Web页面的优化目标，并在每周大约18M URL数据的基础上进行试验。 </li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ef44e4d80fbe0e3bc6b07975be290b88_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"415\" data-rawheight=\"164\" class=\"content_image\" width=\"415\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;415&#39; height=&#39;164&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"415\" data-rawheight=\"164\" class=\"content_image lazy\" width=\"415\" data-actualsrc=\"https://pic1.zhimg.com/v2-ef44e4d80fbe0e3bc6b07975be290b88_b.jpg\"/></figure><ul><li><b>Loaded DiCE: Trading off Bias and Variance in Any-Order Score Function Gradient Estimators for Reinforcement Learning</b> </li><li><b>InteractiveRecGAN: a Model Based Reinforcement Learning Method with Adversarial Training for Online Recommendation</b> </li><li><b>A Composable Specification Language for Reinforcement Learning Tasks</b> </li><li><b>Near-Optimal Reinforcement Learning in Dynamic Treatment Regimes</b> </li><li><b>Constraint Augmented Reinforcement Learning for Text-based Recommendation and Generation</b> </li></ul><h3>其他</h3><ul><li><b>Generalization in Reinforcement Learning with Selective Noise Injection and Information Bottleneck</b> </li><li><b>Using a Logarithmic Mapping to Enable Lower Discount Factors in Reinforcement Learning</b> </li><li><b>Search on the Replay Buffer: Bridging Planning and Reinforcement Learning</b> </li><li><b>A Family of Robust Stochastic Operators for Reinforcement Learning</b></li></ul><h3>参考文献</h3><p>[1]. <a href=\"https://link.zhihu.com/?target=https%3A//neurips.cc/Conferences/2019/AcceptedPapersInitial\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">neurips.cc/Conferences/</span><span class=\"invisible\">2019/AcceptedPapersInitial</span><span class=\"ellipsis\"></span></a></p>", 
            "topic": [
                {
                    "tag": "NIPS", 
                    "tagLink": "https://api.zhihu.com/topics/20033808"
                }, 
                {
                    "tag": "强化学习 (Reinforcement Learning)", 
                    "tagLink": "https://api.zhihu.com/topics/20039099"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": [
                {
                    "userName": "vector", 
                    "userLink": "https://www.zhihu.com/people/320b2a7e5c663ccd0b0b35fc921dd4e2", 
                    "content": "<p>感谢整理，就是能不能别用机翻</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "DRLearner", 
                            "userLink": "https://www.zhihu.com/people/2a0976472ac2864fa905e4ef0e8e5a36", 
                            "content": "<p>感谢阅读，时间短，数量大，以后争取更高的质量！</p>", 
                            "likes": 0, 
                            "replyToAuthor": "vector"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/60047479", 
            "userName": "DRLearner", 
            "userLink": "https://www.zhihu.com/people/2a0976472ac2864fa905e4ef0e8e5a36", 
            "upvote": 1, 
            "title": "深度强化学习系列(3): “超参数”与“网络结构”自动化设置方法---DeepHyper", 
            "content": "<h2>DeepHyper</h2><p>可扩展的异步神经网络和超参数搜索深度神经网络 </p><blockquote> 前言：<br/> 在深度学习和机器学习算法学习和训练的过程中，有两个非常让人头疼的问题 <br/>1. 超参数的设置 <br/>2. 神经网络结构的设计</blockquote><p>可以说这两个问题一直困扰每一个学习者，为了解决这些问题，谷歌公司开源了AutoML(貌似收费)。此外还有Keras（后期详解），本篇文章介绍一个自动化学习包： <b>DeepHyper</b></p><p><b>DeepHyper</b>是一种用于深度神经网络的自动化机器学习（AutoML）软件包。 它包括两个组成部分: </p><p>(1）神经架构搜索是一种自动搜索高性能深度神经网络架构的方法。</p><p> (2）超参数搜索是一种自动搜索给定深度神经网络的高性能超参数的方法。</p><p>DeepHyper提供了一个基础架构，旨在针对HPC(Hyper Parameters Search)系统中的神经架构和超参数搜索方法，可扩展性和可移植性进行实验研究。 为可扩展的超参数和神经架构搜索方法的实现和研究提供了一个通用接口。 在这个包中，其为用户提供了不同的模块：</p><p> + <b>基准（benchmark）</b>：超参数或神经架构搜索的一组问题，用户可以使用它来比较我们的不同搜索算法或作为构建自己问题的示例。</p><ul><li><b>评估者（evaluator）</b>：一组有助于在不同系统和不同情况下运行搜索的对象，例如快速和轻型实验或长时间和重度运行。</li><li><b>搜索（search）</b>：一组用于超参数和神经架构搜索的算法。 您还将找到一种模块化方法来定义新的搜索算法和用于超参数或神经架构搜索的特定子模块。<br/> </li></ul><p>其结构如下：</p><figure data-size=\"small\"><noscript><img src=\"https://pic1.zhimg.com/v2-897877e589828492519d79cfac27730c_b.jpg\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"970\" data-rawheight=\"610\" class=\"origin_image zh-lightbox-thumb\" width=\"970\" data-original=\"https://pic1.zhimg.com/v2-897877e589828492519d79cfac27730c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;970&#39; height=&#39;610&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"970\" data-rawheight=\"610\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"970\" data-original=\"https://pic1.zhimg.com/v2-897877e589828492519d79cfac27730c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-897877e589828492519d79cfac27730c_b.jpg\"/></figure><hr/><h2>一、<b>Hyperparameter Search (HPS)搜索</b></h2><h3>（1）定义超参数问题</h3><p>首先导入deephyper包，并设置问题和纬度</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">from</span> <span class=\"nn\">deephyper.benchmark</span> <span class=\"kn\">import</span> <span class=\"n\">HpProblem</span>\n<span class=\"n\">Problem</span> <span class=\"o\">=</span> <span class=\"n\">HpProblem</span><span class=\"p\">()</span>\n<span class=\"n\">Problem</span><span class=\"o\">.</span><span class=\"n\">add_dim</span><span class=\"p\">(</span><span class=\"s1\">&#39;nunits&#39;</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">20</span><span class=\"p\">),</span> <span class=\"mi\">10</span><span class=\"p\">)</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">Problem</span><span class=\"p\">)</span>\n<span class=\"n\">Problem</span>\n<span class=\"p\">{</span><span class=\"s1\">&#39;nunits&#39;</span><span class=\"p\">:</span> <span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">20</span><span class=\"p\">)}</span>\n\n<span class=\"n\">Starting</span> <span class=\"n\">Point</span>\n<span class=\"p\">{</span><span class=\"s1\">&#39;nunits&#39;</span><span class=\"p\">:</span> <span class=\"mi\">10</span><span class=\"p\">}</span></code></pre></div><p>通过运行模型的函数，结果为类似{&#39;nunits&#39;：10}的字典，但每个键的值将根据搜索的选择而改变。 下面看看如何为mnist数据上的多层Perceptron模型训练定义一个简单的运行函数。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"s1\">&#39;&#39;&#39;Trains a simple deep NN on the MNIST dataset.\n</span><span class=\"s1\">Gets to 98.40% test accuracy after 20 epochs\n</span><span class=\"s1\">(there is *a lot* of margin for parameter tuning).\n</span><span class=\"s1\">2 seconds per epoch on a K520 GPU.\n</span><span class=\"s1\">&#39;&#39;&#39;</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">__future__</span> <span class=\"kn\">import</span> <span class=\"n\">print_function</span>\n\n<span class=\"kn\">import</span> <span class=\"nn\">keras</span>\n<span class=\"kn\">from</span> <span class=\"nn\">keras.datasets</span> <span class=\"kn\">import</span> <span class=\"n\">mnist</span>\n<span class=\"kn\">from</span> <span class=\"nn\">keras.models</span> <span class=\"kn\">import</span> <span class=\"n\">Sequential</span>\n<span class=\"kn\">from</span> <span class=\"nn\">keras.layers</span> <span class=\"kn\">import</span> <span class=\"n\">Dense</span><span class=\"p\">,</span> <span class=\"n\">Dropout</span>\n<span class=\"kn\">from</span> <span class=\"nn\">keras.optimizers</span> <span class=\"kn\">import</span> <span class=\"n\">RMSprop</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">run</span><span class=\"p\">(</span><span class=\"n\">params</span><span class=\"p\">):</span>\n    <span class=\"n\">nunits</span> <span class=\"o\">=</span> <span class=\"n\">params</span><span class=\"p\">[</span><span class=\"s1\">&#39;nunits]</span><span class=\"err\">\n</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"s1\">    batch_size = 128</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"s1\">    num_classes = 10</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"s1\">    epochs = 20</span><span class=\"err\">\n</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"s1\">    # the data, split between train and test sets</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"s1\">    (x_train, y_train), (x_test, y_test) = mnist.load_data()</span><span class=\"err\">\n</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"s1\">    x_train = x_train.reshape(60000, 784)</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"s1\">    x_test = x_test.reshape(10000, 784)</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"s1\">    x_train = x_train.astype(&#39;</span><span class=\"n\">float32</span><span class=\"s1\">&#39;)</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"s1\">    x_test = x_test.astype(&#39;</span><span class=\"n\">float32</span><span class=\"s1\">&#39;)</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"s1\">    x_train /= 255</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"s1\">    x_test /= 255</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"s1\">    print(x_train.shape[0], &#39;</span><span class=\"n\">train</span> <span class=\"n\">samples</span><span class=\"s1\">&#39;)</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"s1\">    print(x_test.shape[0], &#39;</span><span class=\"n\">test</span> <span class=\"n\">samples</span><span class=\"s1\">&#39;)</span><span class=\"err\">\n</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"s1\">    # convert class vectors to binary class matrices</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"s1\">    y_train = keras.utils.to_categorical(y_train, num_classes)</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"s1\">    y_test = keras.utils.to_categorical(y_test, num_classes)</span><span class=\"err\">\n</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"s1\">    model = Sequential()</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"s1\">    model.add(Dense(nunits, activation=&#39;</span><span class=\"n\">relu</span><span class=\"s1\">&#39;, input_shape=(784,)))</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"s1\">    model.add(Dropout(0.2))</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"s1\">    model.add(Dense(512, activation=&#39;</span><span class=\"n\">relu</span><span class=\"s1\">&#39;))</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"s1\">    model.add(Dropout(0.2))</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"s1\">    model.add(Dense(num_classes, activation=&#39;</span><span class=\"n\">softmax</span><span class=\"s1\">&#39;))</span><span class=\"err\">\n</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"s1\">    model.summary()</span><span class=\"err\">\n</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"s1\">    model.compile(loss=&#39;</span><span class=\"n\">categorical_crossentropy</span><span class=\"s1\">&#39;,</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"s1\">                optimizer=RMSprop(),</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"s1\">                metrics=[&#39;</span><span class=\"n\">accuracy</span><span class=\"s1\">&#39;])</span><span class=\"err\">\n</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"s1\">    history = model.fit(x_train, y_train,</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"s1\">                        batch_size=batch_size,</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"s1\">                        epochs=epochs,</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"s1\">                        verbose=1,</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"s1\">                        validation_data=(x_test, y_test))</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"s1\">    score = model.evaluate(x_test, y_test, verbose=0)</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"s1\">    print(&#39;</span><span class=\"n\">Test</span> <span class=\"n\">loss</span><span class=\"p\">:</span><span class=\"s1\">&#39;, score[0])</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"s1\">    print(&#39;</span><span class=\"n\">Test</span> <span class=\"n\">accuracy</span><span class=\"p\">:</span><span class=\"s1\">&#39;, score[1])</span><span class=\"err\">\n</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"s1\">    return -score[1]</span></code></pre></div><p>现在，如果您想要搜索上一个问题和模型。 假设问题是在“package_name/problem.py” 中定义，模型在package_name/mnist_mlp.py 中定义。 如果使用命令行运行AMBS之类的搜索：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">python</span> <span class=\"n\">ambs</span><span class=\"o\">.</span><span class=\"n\">py</span> <span class=\"o\">--</span><span class=\"n\">problem</span> <span class=\"n\">package_name</span><span class=\"o\">.</span><span class=\"n\">problem</span><span class=\"o\">.</span><span class=\"n\">Problem</span> <span class=\"o\">--</span><span class=\"n\">run</span> <span class=\"n\">package_name</span><span class=\"o\">.</span><span class=\"n\">mnist_mlp</span><span class=\"o\">.</span><span class=\"n\">run</span></code></pre></div><h3>（2）Asynchronous Model-Base Search (AMBS)</h3><p>论文：<a href=\"https://link.zhihu.com/?target=https%3A//deephyper.readthedocs.io/en/latest/_downloads/f0b70a4a149bb84e6f5ad67cce57ed88/deephyper_final.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">点击阅读</a></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">class</span> <span class=\"nc\">deephyper</span><span class=\"o\">.</span><span class=\"n\">search</span><span class=\"o\">.</span><span class=\"n\">hps</span><span class=\"o\">.</span><span class=\"n\">ambs</span><span class=\"o\">.</span><span class=\"n\">AMBS</span><span class=\"p\">(</span><span class=\"n\">problem</span><span class=\"p\">,</span> <span class=\"n\">run</span><span class=\"p\">,</span> <span class=\"n\">evaluator</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">)</span></code></pre></div><h3>(3) Genetic Algorithm (GA)</h3><p>接口类</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">class</span> <span class=\"nc\">deephyper</span><span class=\"o\">.</span><span class=\"n\">search</span><span class=\"o\">.</span><span class=\"n\">hps</span><span class=\"o\">.</span><span class=\"n\">ga</span><span class=\"o\">.</span><span class=\"n\">GA</span><span class=\"p\">(</span><span class=\"n\">problem</span><span class=\"p\">,</span> <span class=\"n\">run</span><span class=\"p\">,</span> <span class=\"n\">evaluator</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">)</span></code></pre></div><p>完整代码</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">import</span> <span class=\"nn\">signal</span>\n<span class=\"kn\">import</span> <span class=\"nn\">random</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">deephyper.search.hps.optimizer</span> <span class=\"kn\">import</span> <span class=\"n\">GAOptimizer</span>\n<span class=\"kn\">from</span> <span class=\"nn\">deephyper.search</span> <span class=\"kn\">import</span> <span class=\"n\">Search</span>\n<span class=\"kn\">from</span> <span class=\"nn\">deephyper.search</span> <span class=\"kn\">import</span> <span class=\"n\">util</span>\n\n<span class=\"n\">logger</span> <span class=\"o\">=</span> <span class=\"n\">util</span><span class=\"o\">.</span><span class=\"n\">conf_logger</span><span class=\"p\">(</span><span class=\"s1\">&#39;deephyper.search.hps.ga&#39;</span><span class=\"p\">)</span>\n\n<span class=\"n\">SERVICE_PERIOD</span> <span class=\"o\">=</span> <span class=\"mi\">2</span>          <span class=\"c1\"># Delay (seconds) between main loop iterations</span>\n<span class=\"n\">CHECKPOINT_INTERVAL</span> <span class=\"o\">=</span> <span class=\"mi\">10</span>    <span class=\"c1\"># How many jobs to complete between optimizer checkpoints</span>\n<span class=\"n\">EXIT_FLAG</span> <span class=\"o\">=</span> <span class=\"bp\">False</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">on_exit</span><span class=\"p\">(</span><span class=\"n\">signum</span><span class=\"p\">,</span> <span class=\"n\">stack</span><span class=\"p\">):</span>\n    <span class=\"k\">global</span> <span class=\"n\">EXIT_FLAG</span>\n    <span class=\"n\">EXIT_FLAG</span> <span class=\"o\">=</span> <span class=\"bp\">True</span>\n\n<span class=\"p\">[</span><span class=\"n\">docs</span><span class=\"p\">]</span><span class=\"k\">class</span> <span class=\"nc\">GA</span><span class=\"p\">(</span><span class=\"n\">Search</span><span class=\"p\">):</span>\n    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">problem</span><span class=\"p\">,</span> <span class=\"n\">run</span><span class=\"p\">,</span> <span class=\"n\">evaluator</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">):</span>\n        <span class=\"nb\">super</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"n\">problem</span><span class=\"p\">,</span> <span class=\"n\">run</span><span class=\"p\">,</span> <span class=\"n\">evaluator</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">)</span>\n        <span class=\"n\">logger</span><span class=\"o\">.</span><span class=\"n\">info</span><span class=\"p\">(</span><span class=\"s2\">&#34;Initializing GA&#34;</span><span class=\"p\">)</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">GAOptimizer</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">problem</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">num_workers</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">args</span><span class=\"p\">)</span>\n\n    <span class=\"nd\">@staticmethod</span>\n    <span class=\"k\">def</span> <span class=\"nf\">_extend_parser</span><span class=\"p\">(</span><span class=\"n\">parser</span><span class=\"p\">):</span>\n        <span class=\"n\">parser</span><span class=\"o\">.</span><span class=\"n\">add_argument</span><span class=\"p\">(</span><span class=\"s1\">&#39;--ga_num_gen&#39;</span><span class=\"p\">,</span>\n            <span class=\"n\">default</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">,</span>\n            <span class=\"nb\">type</span><span class=\"o\">=</span><span class=\"nb\">int</span><span class=\"p\">,</span>\n            <span class=\"n\">help</span><span class=\"o\">=</span><span class=\"s1\">&#39;number of generation for genetic algorithm&#39;</span>\n        <span class=\"p\">)</span>\n        <span class=\"n\">parser</span><span class=\"o\">.</span><span class=\"n\">add_argument</span><span class=\"p\">(</span><span class=\"s1\">&#39;--individuals_per_worker&#39;</span><span class=\"p\">,</span>\n            <span class=\"n\">default</span><span class=\"o\">=</span><span class=\"mi\">5</span><span class=\"p\">,</span>\n            <span class=\"nb\">type</span><span class=\"o\">=</span><span class=\"nb\">int</span><span class=\"p\">,</span>\n            <span class=\"n\">help</span><span class=\"o\">=</span><span class=\"s1\">&#39;number of individuals per worker&#39;</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"n\">parser</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">run</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"c1\"># opt = GAOptimizer(cfg)</span>\n        <span class=\"c1\"># evaluator = evaluate.create_evaluator(cfg)</span>\n        <span class=\"n\">logger</span><span class=\"o\">.</span><span class=\"n\">info</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"s2\">&#34;Starting new run&#34;</span><span class=\"p\">)</span>\n\n\n        <span class=\"n\">timer</span> <span class=\"o\">=</span> <span class=\"n\">util</span><span class=\"o\">.</span><span class=\"n\">DelayTimer</span><span class=\"p\">(</span><span class=\"n\">max_minutes</span><span class=\"o\">=</span><span class=\"bp\">None</span><span class=\"p\">,</span> <span class=\"n\">period</span><span class=\"o\">=</span><span class=\"n\">SERVICE_PERIOD</span><span class=\"p\">)</span>\n        <span class=\"n\">timer</span> <span class=\"o\">=</span> <span class=\"nb\">iter</span><span class=\"p\">(</span><span class=\"n\">timer</span><span class=\"p\">)</span>\n        <span class=\"n\">elapsed_str</span> <span class=\"o\">=</span> <span class=\"nb\">next</span><span class=\"p\">(</span><span class=\"n\">timer</span><span class=\"p\">)</span>\n\n        <span class=\"n\">logger</span><span class=\"o\">.</span><span class=\"n\">info</span><span class=\"p\">(</span><span class=\"s2\">&#34;Hyperopt GA driver starting&#34;</span><span class=\"p\">)</span>\n        <span class=\"n\">logger</span><span class=\"o\">.</span><span class=\"n\">info</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"s2\">&#34;Elapsed time: {elapsed_str}&#34;</span><span class=\"p\">)</span>\n\n\n        <span class=\"k\">if</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">pop</span> <span class=\"ow\">is</span> <span class=\"bp\">None</span><span class=\"p\">:</span>\n            <span class=\"n\">logger</span><span class=\"o\">.</span><span class=\"n\">info</span><span class=\"p\">(</span><span class=\"s2\">&#34;Generating initial population&#34;</span><span class=\"p\">)</span>\n            <span class=\"n\">logger</span><span class=\"o\">.</span><span class=\"n\">info</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"s2\">&#34;{self.optimizer.INIT_POP_SIZE} individuals&#34;</span><span class=\"p\">)</span>\n            <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">pop</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">toolbox</span><span class=\"o\">.</span><span class=\"n\">population</span><span class=\"p\">(</span><span class=\"n\">n</span><span class=\"o\">=</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">INIT_POP_SIZE</span><span class=\"p\">)</span>\n            <span class=\"n\">individuals</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">pop</span>\n            <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">evaluate_fitnesses</span><span class=\"p\">(</span><span class=\"n\">individuals</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">optimizer</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">evaluator</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">args</span><span class=\"o\">.</span><span class=\"n\">eval_timeout_minutes</span><span class=\"p\">)</span>\n            <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">record_generation</span><span class=\"p\">(</span><span class=\"n\">num_evals</span><span class=\"o\">=</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">pop</span><span class=\"p\">))</span>\n\n            <span class=\"k\">with</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"s1\">&#39;ga_logbook.log&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;w&#39;</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">fp</span><span class=\"p\">:</span>\n                <span class=\"n\">fp</span><span class=\"o\">.</span><span class=\"n\">write</span><span class=\"p\">(</span><span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">logbook</span><span class=\"p\">))</span>\n            <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s2\">&#34;best:&#34;</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">halloffame</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">])</span>\n\n        <span class=\"k\">while</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">current_gen</span> <span class=\"o\">&lt;</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">NGEN</span><span class=\"p\">:</span>\n            <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">current_gen</span> <span class=\"o\">+=</span> <span class=\"mi\">1</span>\n            <span class=\"n\">logger</span><span class=\"o\">.</span><span class=\"n\">info</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"s2\">&#34;Generation {self.optimizer.current_gen} out of {self.optimizer.NGEN}&#34;</span><span class=\"p\">)</span>\n            <span class=\"n\">logger</span><span class=\"o\">.</span><span class=\"n\">info</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"s2\">&#34;Elapsed time: {elapsed_str}&#34;</span><span class=\"p\">)</span>\n\n            <span class=\"c1\"># Select the next generation individuals</span>\n            <span class=\"n\">offspring</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">toolbox</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">pop</span><span class=\"p\">,</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">pop</span><span class=\"p\">))</span>\n            <span class=\"c1\"># Clone the selected individuals</span>\n            <span class=\"n\">offspring</span> <span class=\"o\">=</span> <span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"nb\">map</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">toolbox</span><span class=\"o\">.</span><span class=\"n\">clone</span><span class=\"p\">,</span> <span class=\"n\">offspring</span><span class=\"p\">))</span>\n\n            <span class=\"c1\"># Apply crossover and mutation on the offspring</span>\n            <span class=\"k\">for</span> <span class=\"n\">child1</span><span class=\"p\">,</span> <span class=\"n\">child2</span> <span class=\"ow\">in</span> <span class=\"nb\">zip</span><span class=\"p\">(</span><span class=\"n\">offspring</span><span class=\"p\">[::</span><span class=\"mi\">2</span><span class=\"p\">],</span> <span class=\"n\">offspring</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">::</span><span class=\"mi\">2</span><span class=\"p\">]):</span>\n                <span class=\"k\">if</span> <span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"p\">()</span> <span class=\"o\">&lt;</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">CXPB</span><span class=\"p\">:</span>\n                    <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">toolbox</span><span class=\"o\">.</span><span class=\"n\">mate</span><span class=\"p\">(</span><span class=\"n\">child1</span><span class=\"p\">,</span> <span class=\"n\">child2</span><span class=\"p\">)</span>\n                    <span class=\"k\">del</span> <span class=\"n\">child1</span><span class=\"o\">.</span><span class=\"n\">fitness</span><span class=\"o\">.</span><span class=\"n\">values</span>\n                    <span class=\"k\">del</span> <span class=\"n\">child2</span><span class=\"o\">.</span><span class=\"n\">fitness</span><span class=\"o\">.</span><span class=\"n\">values</span>\n\n            <span class=\"k\">for</span> <span class=\"n\">mutant</span> <span class=\"ow\">in</span> <span class=\"n\">offspring</span><span class=\"p\">:</span>\n                <span class=\"k\">if</span> <span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"p\">()</span> <span class=\"o\">&lt;</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">MUTPB</span><span class=\"p\">:</span>\n                    <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">toolbox</span><span class=\"o\">.</span><span class=\"n\">mutate</span><span class=\"p\">(</span><span class=\"n\">mutant</span><span class=\"p\">)</span>\n                    <span class=\"k\">del</span> <span class=\"n\">mutant</span><span class=\"o\">.</span><span class=\"n\">fitness</span><span class=\"o\">.</span><span class=\"n\">values</span>\n\n            <span class=\"c1\"># Evaluate the individuals with an invalid fitness</span>\n            <span class=\"n\">invalid_ind</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">ind</span> <span class=\"k\">for</span> <span class=\"n\">ind</span> <span class=\"ow\">in</span> <span class=\"n\">offspring</span> <span class=\"k\">if</span> <span class=\"ow\">not</span> <span class=\"n\">ind</span><span class=\"o\">.</span><span class=\"n\">fitness</span><span class=\"o\">.</span><span class=\"n\">valid</span><span class=\"p\">]</span>\n            <span class=\"n\">logger</span><span class=\"o\">.</span><span class=\"n\">info</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"s2\">&#34;Evaluating {len(invalid_ind)} invalid individuals&#34;</span><span class=\"p\">)</span>\n            <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">evaluate_fitnesses</span><span class=\"p\">(</span><span class=\"n\">invalid_ind</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">optimizer</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">evaluator</span><span class=\"p\">,</span>\n                    <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">args</span><span class=\"o\">.</span><span class=\"n\">eval_timeout_minutes</span><span class=\"p\">)</span>\n\n            <span class=\"c1\"># The population is entirely replaced by the offspring</span>\n            <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">pop</span><span class=\"p\">[:]</span> <span class=\"o\">=</span> <span class=\"n\">offspring</span>\n\n            <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">record_generation</span><span class=\"p\">(</span><span class=\"n\">num_evals</span><span class=\"o\">=</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">invalid_ind</span><span class=\"p\">))</span>\n\n            <span class=\"k\">with</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"s1\">&#39;ga_logbook.log&#39;</span><span class=\"p\">,</span> <span class=\"s1\">&#39;w&#39;</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">fp</span><span class=\"p\">:</span>\n                <span class=\"n\">fp</span><span class=\"o\">.</span><span class=\"n\">write</span><span class=\"p\">(</span><span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">logbook</span><span class=\"p\">))</span>\n            <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s2\">&#34;best:&#34;</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">halloffame</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">])</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">evaluate_fitnesses</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">individuals</span><span class=\"p\">,</span> <span class=\"n\">opt</span><span class=\"p\">,</span> <span class=\"n\">evaluator</span><span class=\"p\">,</span> <span class=\"n\">timeout_minutes</span><span class=\"p\">):</span>\n        <span class=\"n\">points</span> <span class=\"o\">=</span> <span class=\"nb\">map</span><span class=\"p\">(</span><span class=\"n\">opt</span><span class=\"o\">.</span><span class=\"n\">space_encoder</span><span class=\"o\">.</span><span class=\"n\">decode_point</span><span class=\"p\">,</span> <span class=\"n\">individuals</span><span class=\"p\">)</span>\n        <span class=\"n\">points</span> <span class=\"o\">=</span> <span class=\"p\">[{</span><span class=\"n\">key</span><span class=\"p\">:</span><span class=\"n\">x</span> <span class=\"k\">for</span> <span class=\"n\">key</span><span class=\"p\">,</span><span class=\"n\">x</span> <span class=\"ow\">in</span> <span class=\"nb\">zip</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">problem</span><span class=\"o\">.</span><span class=\"n\">space</span><span class=\"o\">.</span><span class=\"n\">keys</span><span class=\"p\">(),</span> <span class=\"n\">point</span><span class=\"p\">)}</span>\n                  <span class=\"k\">for</span> <span class=\"n\">point</span> <span class=\"ow\">in</span> <span class=\"n\">points</span><span class=\"p\">]</span>\n        <span class=\"n\">evaluator</span><span class=\"o\">.</span><span class=\"n\">add_eval_batch</span><span class=\"p\">(</span><span class=\"n\">points</span><span class=\"p\">)</span>\n        <span class=\"n\">logger</span><span class=\"o\">.</span><span class=\"n\">info</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"s2\">&#34;Waiting on {len(points)} individual fitness evaluations&#34;</span><span class=\"p\">)</span>\n        <span class=\"n\">results</span> <span class=\"o\">=</span> <span class=\"n\">evaluator</span><span class=\"o\">.</span><span class=\"n\">await_evals</span><span class=\"p\">(</span><span class=\"n\">points</span><span class=\"p\">,</span> <span class=\"n\">timeout</span><span class=\"o\">=</span><span class=\"n\">timeout_minutes</span><span class=\"o\">*</span><span class=\"mi\">60</span><span class=\"p\">)</span>\n\n        <span class=\"k\">for</span> <span class=\"n\">ind</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span><span class=\"n\">fit</span><span class=\"p\">)</span> <span class=\"ow\">in</span> <span class=\"nb\">zip</span><span class=\"p\">(</span><span class=\"n\">individuals</span><span class=\"p\">,</span> <span class=\"n\">results</span><span class=\"p\">):</span>\n            <span class=\"n\">ind</span><span class=\"o\">.</span><span class=\"n\">fitness</span><span class=\"o\">.</span><span class=\"n\">values</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">fit</span><span class=\"p\">,)</span>\n\n\n\n<span class=\"k\">if</span> <span class=\"vm\">__name__</span> <span class=\"o\">==</span> <span class=\"s2\">&#34;__main__&#34;</span><span class=\"p\">:</span>\n    <span class=\"n\">args</span> <span class=\"o\">=</span> <span class=\"n\">GA</span><span class=\"o\">.</span><span class=\"n\">parse_args</span><span class=\"p\">()</span>\n    <span class=\"n\">search</span> <span class=\"o\">=</span> <span class=\"n\">GA</span><span class=\"p\">(</span><span class=\"o\">**</span><span class=\"nb\">vars</span><span class=\"p\">(</span><span class=\"n\">args</span><span class=\"p\">))</span>\n    <span class=\"c1\">#signal.signal(signal.SIGINT, on_exit)</span>\n    <span class=\"c1\">#signal.signal(signal.SIGTERM, on_exit)</span>\n    <span class=\"n\">search</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">()</span></code></pre></div><hr/><h2><b>二、神经网络搜索</b></h2><p>Neural Architecture Search (NAS)</p><h3>（1）异步搜索 NAS A3C (PPO) Asynchronous</h3><figure data-size=\"small\"><noscript><img src=\"https://pic4.zhimg.com/v2-0088bc71f803e042b4e0df4f313e429b_b.jpg\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"1390\" data-rawheight=\"1014\" class=\"origin_image zh-lightbox-thumb\" width=\"1390\" data-original=\"https://pic4.zhimg.com/v2-0088bc71f803e042b4e0df4f313e429b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1390&#39; height=&#39;1014&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"1390\" data-rawheight=\"1014\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1390\" data-original=\"https://pic4.zhimg.com/v2-0088bc71f803e042b4e0df4f313e429b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-0088bc71f803e042b4e0df4f313e429b_b.jpg\"/></figure><p>搜索接口类</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">class</span> <span class=\"nc\">deephyper</span><span class=\"o\">.</span><span class=\"n\">search</span><span class=\"o\">.</span><span class=\"n\">nas</span><span class=\"o\">.</span><span class=\"n\">ppo_a3c_async</span><span class=\"o\">.</span><span class=\"n\">NasPPOAsyncA3C</span><span class=\"p\">(</span><span class=\"n\">problem</span><span class=\"p\">,</span> <span class=\"n\">run</span><span class=\"p\">,</span> <span class=\"n\">evaluator</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">)</span></code></pre></div><p>具体搜索运行代码：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">mpirun</span> <span class=\"o\">-</span><span class=\"n\">np</span> <span class=\"mi\">2</span> <span class=\"n\">python</span> <span class=\"n\">ppo_a3c_async</span><span class=\"o\">.</span><span class=\"n\">py</span> <span class=\"o\">--</span><span class=\"n\">problem</span> <span class=\"n\">deephyper</span><span class=\"o\">.</span><span class=\"n\">benchmark</span><span class=\"o\">.</span><span class=\"n\">nas</span><span class=\"o\">.</span><span class=\"n\">mnist1D</span><span class=\"o\">.</span><span class=\"n\">problem</span><span class=\"o\">.</span><span class=\"n\">Problem</span> <span class=\"o\">--</span><span class=\"n\">run</span> <span class=\"n\">deephyper</span><span class=\"o\">.</span><span class=\"n\">search</span><span class=\"o\">.</span><span class=\"n\">nas</span><span class=\"o\">.</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"o\">.</span><span class=\"n\">alpha</span><span class=\"o\">.</span><span class=\"n\">run</span> <span class=\"o\">--</span><span class=\"n\">evaluator</span> <span class=\"n\">subprocess</span></code></pre></div><p>完整代码：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">import</span> <span class=\"nn\">os</span>\n<span class=\"kn\">import</span> <span class=\"nn\">json</span>\n<span class=\"kn\">from</span> <span class=\"nn\">math</span> <span class=\"kn\">import</span> <span class=\"n\">ceil</span><span class=\"p\">,</span> <span class=\"n\">log</span>\n<span class=\"kn\">from</span> <span class=\"nn\">pprint</span> <span class=\"kn\">import</span> <span class=\"n\">pprint</span><span class=\"p\">,</span> <span class=\"n\">pformat</span>\n<span class=\"kn\">from</span> <span class=\"nn\">mpi4py</span> <span class=\"kn\">import</span> <span class=\"n\">MPI</span>\n<span class=\"kn\">import</span> <span class=\"nn\">math</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">deephyper.evaluator</span> <span class=\"kn\">import</span> <span class=\"n\">Evaluator</span>\n<span class=\"kn\">from</span> <span class=\"nn\">deephyper.search</span> <span class=\"kn\">import</span> <span class=\"n\">util</span><span class=\"p\">,</span> <span class=\"n\">Search</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">deephyper.search.nas.agent</span> <span class=\"kn\">import</span> <span class=\"n\">nas_ppo_async_a3c</span>\n\n<span class=\"n\">logger</span> <span class=\"o\">=</span> <span class=\"n\">util</span><span class=\"o\">.</span><span class=\"n\">conf_logger</span><span class=\"p\">(</span><span class=\"s1\">&#39;deephyper.search.nas.ppo_a3c_async&#39;</span><span class=\"p\">)</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">print_logs</span><span class=\"p\">(</span><span class=\"n\">runner</span><span class=\"p\">):</span>\n    <span class=\"n\">logger</span><span class=\"o\">.</span><span class=\"n\">debug</span><span class=\"p\">(</span><span class=\"s1\">&#39;num_episodes = {}&#39;</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span><span class=\"n\">runner</span><span class=\"o\">.</span><span class=\"n\">global_episode</span><span class=\"p\">))</span>\n    <span class=\"n\">logger</span><span class=\"o\">.</span><span class=\"n\">debug</span><span class=\"p\">(</span><span class=\"s1\">&#39; workers = {}&#39;</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span><span class=\"n\">runner</span><span class=\"o\">.</span><span class=\"n\">workers</span><span class=\"p\">))</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">key</span><span class=\"p\">(</span><span class=\"n\">d</span><span class=\"p\">):</span>\n    <span class=\"k\">return</span> <span class=\"n\">json</span><span class=\"o\">.</span><span class=\"n\">dumps</span><span class=\"p\">(</span><span class=\"nb\">dict</span><span class=\"p\">(</span><span class=\"n\">arch_seq</span><span class=\"o\">=</span><span class=\"n\">d</span><span class=\"p\">[</span><span class=\"s1\">&#39;arch_seq&#39;</span><span class=\"p\">]))</span>\n\n<span class=\"n\">LAUNCHER_NODES</span> <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s1\">&#39;BALSAM_LAUNCHER_NODES&#39;</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n<span class=\"n\">WORKERS_PER_NODE</span> <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s1\">&#39;DEEPHYPER_WORKERS_PER_NODE&#39;</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n\n<span class=\"p\">[</span><span class=\"n\">docs</span><span class=\"p\">]</span><span class=\"k\">class</span> <span class=\"nc\">NasPPOAsyncA3C</span><span class=\"p\">(</span><span class=\"n\">Search</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;Neural Architecture search using proximal policy gradient with asynchronous optimization.\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n\n    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">problem</span><span class=\"p\">,</span> <span class=\"n\">run</span><span class=\"p\">,</span> <span class=\"n\">evaluator</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">):</span>\n        <span class=\"nb\">super</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"n\">problem</span><span class=\"p\">,</span> <span class=\"n\">run</span><span class=\"p\">,</span> <span class=\"n\">evaluator</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">)</span>\n        <span class=\"c1\"># set in super : self.problem</span>\n        <span class=\"c1\"># set in super : self.run_func</span>\n        <span class=\"c1\"># set in super : self.evaluator</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">evaluator</span> <span class=\"o\">=</span> <span class=\"n\">Evaluator</span><span class=\"o\">.</span><span class=\"n\">create</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">run_func</span><span class=\"p\">,</span>\n                                          <span class=\"n\">cache_key</span><span class=\"o\">=</span><span class=\"n\">key</span><span class=\"p\">,</span>\n                                          <span class=\"n\">method</span><span class=\"o\">=</span><span class=\"n\">evaluator</span><span class=\"p\">)</span>\n\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">num_episodes</span> <span class=\"o\">=</span> <span class=\"n\">kwargs</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s1\">&#39;num_episodes&#39;</span><span class=\"p\">)</span>\n        <span class=\"k\">if</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">num_episodes</span> <span class=\"ow\">is</span> <span class=\"bp\">None</span><span class=\"p\">:</span>\n            <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">num_episodes</span> <span class=\"o\">=</span> <span class=\"n\">math</span><span class=\"o\">.</span><span class=\"n\">inf</span>\n\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">reward_rule</span> <span class=\"o\">=</span> <span class=\"n\">util</span><span class=\"o\">.</span><span class=\"n\">load_attr_from</span><span class=\"p\">(</span><span class=\"s1\">&#39;deephyper.search.nas.agent.utils.&#39;</span><span class=\"o\">+</span><span class=\"n\">kwargs</span><span class=\"p\">[</span><span class=\"s1\">&#39;reward_rule&#39;</span><span class=\"p\">])</span>\n\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">space</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">problem</span><span class=\"o\">.</span><span class=\"n\">space</span>\n\n        <span class=\"n\">logger</span><span class=\"o\">.</span><span class=\"n\">debug</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"s1\">&#39;evaluator: {type(self.evaluator)}&#39;</span><span class=\"p\">)</span>\n\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">num_agents</span> <span class=\"o\">=</span> <span class=\"n\">MPI</span><span class=\"o\">.</span><span class=\"n\">COMM_WORLD</span><span class=\"o\">.</span><span class=\"n\">Get_size</span><span class=\"p\">()</span> <span class=\"o\">-</span> <span class=\"mi\">1</span> <span class=\"c1\"># one is  the parameter server</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">rank</span> <span class=\"o\">=</span> <span class=\"n\">MPI</span><span class=\"o\">.</span><span class=\"n\">COMM_WORLD</span><span class=\"o\">.</span><span class=\"n\">Get_rank</span><span class=\"p\">()</span>\n\n        <span class=\"n\">logger</span><span class=\"o\">.</span><span class=\"n\">debug</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"s1\">&#39;num_agents: {self.num_agents}&#39;</span><span class=\"p\">)</span>\n        <span class=\"n\">logger</span><span class=\"o\">.</span><span class=\"n\">debug</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"s1\">&#39;rank: {self.rank}&#39;</span><span class=\"p\">)</span>\n\n    <span class=\"nd\">@staticmethod</span>\n    <span class=\"k\">def</span> <span class=\"nf\">_extend_parser</span><span class=\"p\">(</span><span class=\"n\">parser</span><span class=\"p\">):</span>\n        <span class=\"n\">parser</span><span class=\"o\">.</span><span class=\"n\">add_argument</span><span class=\"p\">(</span><span class=\"s1\">&#39;--num-episodes&#39;</span><span class=\"p\">,</span> <span class=\"nb\">type</span><span class=\"o\">=</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">default</span><span class=\"o\">=</span><span class=\"bp\">None</span><span class=\"p\">,</span>\n                            <span class=\"n\">help</span><span class=\"o\">=</span><span class=\"s1\">&#39;maximum number of episodes&#39;</span><span class=\"p\">)</span>\n        <span class=\"n\">parser</span><span class=\"o\">.</span><span class=\"n\">add_argument</span><span class=\"p\">(</span><span class=\"s1\">&#39;--reward-rule&#39;</span><span class=\"p\">,</span> <span class=\"nb\">type</span><span class=\"o\">=</span><span class=\"nb\">str</span><span class=\"p\">,</span>\n            <span class=\"n\">default</span><span class=\"o\">=</span><span class=\"s1\">&#39;final_reward_for_all_timesteps&#39;</span><span class=\"p\">,</span>\n            <span class=\"n\">choices</span><span class=\"o\">=</span><span class=\"p\">[</span>\n                <span class=\"s1\">&#39;final_reward_for_all_timesteps&#39;</span><span class=\"p\">,</span>\n                <span class=\"s1\">&#39;episode_reward_for_final_timestep&#39;</span>\n            <span class=\"p\">],</span>\n            <span class=\"n\">help</span><span class=\"o\">=</span><span class=\"s1\">&#39;A function which describe how to spread the episodic reward on all timesteps of the corresponding episode.&#39;</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"n\">parser</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">main</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"c1\"># Settings</span>\n        <span class=\"c1\">#num_parallel = self.evaluator.num_workers - 4 #balsam launcher &amp; controller of search for cooley</span>\n        <span class=\"c1\"># num_nodes = self.evaluator.num_workers - 1 #balsam launcher &amp; controller of search for theta</span>\n        <span class=\"n\">num_nodes</span> <span class=\"o\">=</span> <span class=\"n\">LAUNCHER_NODES</span> <span class=\"o\">*</span> <span class=\"n\">WORKERS_PER_NODE</span> <span class=\"o\">-</span> <span class=\"mi\">1</span> <span class=\"c1\"># balsam launcher</span>\n        <span class=\"n\">num_nodes</span> <span class=\"o\">-=</span> <span class=\"mi\">1</span> <span class=\"c1\"># parameter server is neither an agent nor a worker</span>\n        <span class=\"k\">if</span> <span class=\"n\">num_nodes</span> <span class=\"o\">&gt;</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">num_agents</span><span class=\"p\">:</span>\n            <span class=\"n\">num_episodes_per_batch</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">num_nodes</span><span class=\"o\">-</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">num_agents</span><span class=\"p\">)</span><span class=\"o\">//</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">num_agents</span>\n        <span class=\"k\">else</span><span class=\"p\">:</span>\n            <span class=\"n\">num_episodes_per_batch</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>\n\n        <span class=\"k\">if</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">rank</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n            <span class=\"n\">logger</span><span class=\"o\">.</span><span class=\"n\">debug</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"s1\">&#39;&lt;Rank={self.rank}&gt; num_nodes: {num_nodes}&#39;</span><span class=\"p\">)</span>\n            <span class=\"n\">logger</span><span class=\"o\">.</span><span class=\"n\">debug</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"s1\">&#39;&lt;Rank={self.rank}&gt; num_episodes_per_batch: {num_episodes_per_batch}&#39;</span><span class=\"p\">)</span>\n\n        <span class=\"n\">logger</span><span class=\"o\">.</span><span class=\"n\">debug</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"s1\">&#39;&lt;Rank={self.rank}&gt; starting training...&#39;</span><span class=\"p\">)</span>\n        <span class=\"n\">nas_ppo_async_a3c</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"p\">(</span>\n            <span class=\"n\">num_episodes</span><span class=\"o\">=</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">num_episodes</span><span class=\"p\">,</span>\n            <span class=\"n\">seed</span><span class=\"o\">=</span><span class=\"mi\">2018</span><span class=\"p\">,</span>\n            <span class=\"n\">space</span><span class=\"o\">=</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">problem</span><span class=\"o\">.</span><span class=\"n\">space</span><span class=\"p\">,</span>\n            <span class=\"n\">evaluator</span><span class=\"o\">=</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">evaluator</span><span class=\"p\">,</span>\n            <span class=\"n\">num_episodes_per_batch</span><span class=\"o\">=</span><span class=\"n\">num_episodes_per_batch</span><span class=\"p\">,</span>\n            <span class=\"n\">reward_rule</span><span class=\"o\">=</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">reward_rule</span>\n        <span class=\"p\">)</span>\n\n\n<span class=\"k\">if</span> <span class=\"vm\">__name__</span> <span class=\"o\">==</span> <span class=\"s2\">&#34;__main__&#34;</span><span class=\"p\">:</span>\n    <span class=\"n\">args</span> <span class=\"o\">=</span> <span class=\"n\">NasPPOAsyncA3C</span><span class=\"o\">.</span><span class=\"n\">parse_args</span><span class=\"p\">()</span>\n    <span class=\"n\">search</span> <span class=\"o\">=</span> <span class=\"n\">NasPPOAsyncA3C</span><span class=\"p\">(</span><span class=\"o\">**</span><span class=\"nb\">vars</span><span class=\"p\">(</span><span class=\"n\">args</span><span class=\"p\">))</span>\n    <span class=\"n\">search</span><span class=\"o\">.</span><span class=\"n\">main</span><span class=\"p\">()</span></code></pre></div><h3>（2）同步更新 NAS A3C (PPO) Synchronous</h3><figure data-size=\"small\"><noscript><img src=\"https://pic3.zhimg.com/v2-eb191a94851693277d0b8c3f33b23902_b.jpg\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"1556\" data-rawheight=\"1012\" class=\"origin_image zh-lightbox-thumb\" width=\"1556\" data-original=\"https://pic3.zhimg.com/v2-eb191a94851693277d0b8c3f33b23902_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1556&#39; height=&#39;1012&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"1556\" data-rawheight=\"1012\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1556\" data-original=\"https://pic3.zhimg.com/v2-eb191a94851693277d0b8c3f33b23902_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-eb191a94851693277d0b8c3f33b23902_b.jpg\"/></figure><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">class</span> <span class=\"nc\">deephyper</span><span class=\"o\">.</span><span class=\"n\">search</span><span class=\"o\">.</span><span class=\"n\">nas</span><span class=\"o\">.</span><span class=\"n\">ppo_a3c_sync</span><span class=\"o\">.</span><span class=\"n\">NasPPOSyncA3C</span><span class=\"p\">(</span><span class=\"n\">problem</span><span class=\"p\">,</span> <span class=\"n\">run</span><span class=\"p\">,</span> <span class=\"n\">evaluator</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">)</span></code></pre></div><p>使用近端策略梯度进行神经结构搜索和同步优化</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">python</span> <span class=\"o\">-</span><span class=\"n\">m</span> <span class=\"n\">deephyper</span><span class=\"o\">.</span><span class=\"n\">search</span><span class=\"o\">.</span><span class=\"n\">nas</span><span class=\"o\">.</span><span class=\"n\">ppo_a3c_sync</span> <span class=\"o\">--</span><span class=\"n\">evaluator</span> <span class=\"n\">subprocess</span> <span class=\"o\">--</span><span class=\"n\">problem</span> <span class=\"s1\">&#39;deephyper.benchmark.nas.linearReg.problem.Problem&#39;</span> <span class=\"o\">--</span><span class=\"n\">run</span> <span class=\"s1\">&#39;deephyper.search.nas.model.run.alpha.run&#39;</span></code></pre></div><p>或者使用MPI来启动n个代理，其中n = np，因为所有代理都是将与第一个代理同步：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">mpirun</span> <span class=\"o\">-</span><span class=\"n\">np</span> <span class=\"mi\">2</span> <span class=\"n\">python</span> <span class=\"n\">ppo_a3c_async</span><span class=\"o\">.</span><span class=\"n\">py</span> <span class=\"o\">--</span><span class=\"n\">problem</span> <span class=\"n\">deephyper</span><span class=\"o\">.</span><span class=\"n\">benchmark</span><span class=\"o\">.</span><span class=\"n\">nas</span><span class=\"o\">.</span><span class=\"n\">mnist1D</span><span class=\"o\">.</span><span class=\"n\">problem</span><span class=\"o\">.</span><span class=\"n\">Problem</span> <span class=\"o\">--</span><span class=\"n\">run</span> <span class=\"n\">deephyper</span><span class=\"o\">.</span><span class=\"n\">search</span><span class=\"o\">.</span><span class=\"n\">nas</span><span class=\"o\">.</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"o\">.</span><span class=\"n\">alpha</span><span class=\"o\">.</span><span class=\"n\">run</span> <span class=\"o\">--</span><span class=\"n\">evaluator</span> <span class=\"n\">subprocess</span></code></pre></div><p>完整代码实现</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">import</span> <span class=\"nn\">os</span>\n<span class=\"kn\">import</span> <span class=\"nn\">json</span>\n<span class=\"kn\">from</span> <span class=\"nn\">pprint</span> <span class=\"kn\">import</span> <span class=\"n\">pprint</span><span class=\"p\">,</span> <span class=\"n\">pformat</span>\n<span class=\"kn\">from</span> <span class=\"nn\">mpi4py</span> <span class=\"kn\">import</span> <span class=\"n\">MPI</span>\n<span class=\"kn\">import</span> <span class=\"nn\">math</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">deephyper.evaluator</span> <span class=\"kn\">import</span> <span class=\"n\">Evaluator</span>\n<span class=\"kn\">from</span> <span class=\"nn\">deephyper.search</span> <span class=\"kn\">import</span> <span class=\"n\">util</span><span class=\"p\">,</span> <span class=\"n\">Search</span>\n\n<span class=\"kn\">from</span> <span class=\"nn\">deephyper.search.nas.agent</span> <span class=\"kn\">import</span> <span class=\"n\">nas_ppo_sync_a3c</span>\n\n<span class=\"n\">logger</span> <span class=\"o\">=</span> <span class=\"n\">util</span><span class=\"o\">.</span><span class=\"n\">conf_logger</span><span class=\"p\">(</span><span class=\"s1\">&#39;deephyper.search.nas.ppo_a3c_sync&#39;</span><span class=\"p\">)</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">print_logs</span><span class=\"p\">(</span><span class=\"n\">runner</span><span class=\"p\">):</span>\n    <span class=\"n\">logger</span><span class=\"o\">.</span><span class=\"n\">debug</span><span class=\"p\">(</span><span class=\"s1\">&#39;num_episodes = {}&#39;</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span><span class=\"n\">runner</span><span class=\"o\">.</span><span class=\"n\">global_episode</span><span class=\"p\">))</span>\n    <span class=\"n\">logger</span><span class=\"o\">.</span><span class=\"n\">debug</span><span class=\"p\">(</span><span class=\"s1\">&#39; workers = {}&#39;</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span><span class=\"n\">runner</span><span class=\"o\">.</span><span class=\"n\">workers</span><span class=\"p\">))</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">key</span><span class=\"p\">(</span><span class=\"n\">d</span><span class=\"p\">):</span>\n    <span class=\"k\">return</span> <span class=\"n\">json</span><span class=\"o\">.</span><span class=\"n\">dumps</span><span class=\"p\">(</span><span class=\"nb\">dict</span><span class=\"p\">(</span><span class=\"n\">arch_seq</span><span class=\"o\">=</span><span class=\"n\">d</span><span class=\"p\">[</span><span class=\"s1\">&#39;arch_seq&#39;</span><span class=\"p\">]))</span>\n\n<span class=\"n\">LAUNCHER_NODES</span> <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s1\">&#39;BALSAM_LAUNCHER_NODES&#39;</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n<span class=\"n\">WORKERS_PER_NODE</span> <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">environ</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s1\">&#39;DEEPHYPER_WORKERS_PER_NODE&#39;</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n\n<span class=\"p\">[</span><span class=\"n\">docs</span><span class=\"p\">]</span><span class=\"k\">class</span> <span class=\"nc\">NasPPOSyncA3C</span><span class=\"p\">(</span><span class=\"n\">Search</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;Neural Architecture search using proximal policy gradient with synchronous optimization.\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n\n    <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">problem</span><span class=\"p\">,</span> <span class=\"n\">run</span><span class=\"p\">,</span> <span class=\"n\">evaluator</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">):</span>\n        <span class=\"nb\">super</span><span class=\"p\">()</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"n\">problem</span><span class=\"p\">,</span> <span class=\"n\">run</span><span class=\"p\">,</span> <span class=\"n\">evaluator</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">)</span>\n        <span class=\"c1\"># set in super : self.problem</span>\n        <span class=\"c1\"># set in super : self.run_func</span>\n        <span class=\"c1\"># set in super : self.evaluator</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">evaluator</span> <span class=\"o\">=</span> <span class=\"n\">Evaluator</span><span class=\"o\">.</span><span class=\"n\">create</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">run_func</span><span class=\"p\">,</span>\n                                          <span class=\"n\">cache_key</span><span class=\"o\">=</span><span class=\"n\">key</span><span class=\"p\">,</span>\n                                          <span class=\"n\">method</span><span class=\"o\">=</span><span class=\"n\">evaluator</span><span class=\"p\">)</span>\n\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">num_episodes</span> <span class=\"o\">=</span> <span class=\"n\">kwargs</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"p\">(</span><span class=\"s1\">&#39;num_episodes&#39;</span><span class=\"p\">)</span>\n        <span class=\"k\">if</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">num_episodes</span> <span class=\"ow\">is</span> <span class=\"bp\">None</span><span class=\"p\">:</span>\n            <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">num_episodes</span> <span class=\"o\">=</span> <span class=\"n\">math</span><span class=\"o\">.</span><span class=\"n\">inf</span>\n\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">reward_rule</span> <span class=\"o\">=</span> <span class=\"n\">util</span><span class=\"o\">.</span><span class=\"n\">load_attr_from</span><span class=\"p\">(</span><span class=\"s1\">&#39;deephyper.search.nas.agent.utils.&#39;</span><span class=\"o\">+</span><span class=\"n\">kwargs</span><span class=\"p\">[</span><span class=\"s1\">&#39;reward_rule&#39;</span><span class=\"p\">])</span>\n\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">space</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">problem</span><span class=\"o\">.</span><span class=\"n\">space</span>\n\n        <span class=\"n\">logger</span><span class=\"o\">.</span><span class=\"n\">debug</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"s1\">&#39;evaluator: {type(self.evaluator)}&#39;</span><span class=\"p\">)</span>\n\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">num_agents</span> <span class=\"o\">=</span> <span class=\"n\">MPI</span><span class=\"o\">.</span><span class=\"n\">COMM_WORLD</span><span class=\"o\">.</span><span class=\"n\">Get_size</span><span class=\"p\">()</span> <span class=\"o\">-</span> <span class=\"mi\">1</span> <span class=\"c1\"># one is  the parameter server</span>\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">rank</span> <span class=\"o\">=</span> <span class=\"n\">MPI</span><span class=\"o\">.</span><span class=\"n\">COMM_WORLD</span><span class=\"o\">.</span><span class=\"n\">Get_rank</span><span class=\"p\">()</span>\n\n        <span class=\"n\">logger</span><span class=\"o\">.</span><span class=\"n\">debug</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"s1\">&#39;num_agents: {self.num_agents}&#39;</span><span class=\"p\">)</span>\n        <span class=\"n\">logger</span><span class=\"o\">.</span><span class=\"n\">debug</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"s1\">&#39;rank: {self.rank}&#39;</span><span class=\"p\">)</span>\n\n    <span class=\"nd\">@staticmethod</span>\n    <span class=\"k\">def</span> <span class=\"nf\">_extend_parser</span><span class=\"p\">(</span><span class=\"n\">parser</span><span class=\"p\">):</span>\n        <span class=\"n\">parser</span><span class=\"o\">.</span><span class=\"n\">add_argument</span><span class=\"p\">(</span><span class=\"s1\">&#39;--num-episodes&#39;</span><span class=\"p\">,</span> <span class=\"nb\">type</span><span class=\"o\">=</span><span class=\"nb\">int</span><span class=\"p\">,</span> <span class=\"n\">default</span><span class=\"o\">=</span><span class=\"bp\">None</span><span class=\"p\">,</span>\n                            <span class=\"n\">help</span><span class=\"o\">=</span><span class=\"s1\">&#39;maximum number of episodes&#39;</span><span class=\"p\">)</span>\n        <span class=\"n\">parser</span><span class=\"o\">.</span><span class=\"n\">add_argument</span><span class=\"p\">(</span><span class=\"s1\">&#39;--reward-rule&#39;</span><span class=\"p\">,</span> <span class=\"nb\">type</span><span class=\"o\">=</span><span class=\"nb\">str</span><span class=\"p\">,</span>\n            <span class=\"n\">default</span><span class=\"o\">=</span><span class=\"s1\">&#39;reward_for_final_timestep&#39;</span><span class=\"p\">,</span>\n            <span class=\"n\">choices</span><span class=\"o\">=</span><span class=\"p\">[</span>\n                <span class=\"s1\">&#39;reward_for_all_timesteps&#39;</span><span class=\"p\">,</span>\n                <span class=\"s1\">&#39;reward_for_final_timestep&#39;</span>\n            <span class=\"p\">],</span>\n            <span class=\"n\">help</span><span class=\"o\">=</span><span class=\"s1\">&#39;A function which describe how to spread the episodic reward on all timesteps of the corresponding episode.&#39;</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"n\">parser</span>\n\n    <span class=\"k\">def</span> <span class=\"nf\">main</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n        <span class=\"c1\"># Settings</span>\n        <span class=\"c1\">#num_parallel = self.evaluator.num_workers - 4 #balsam launcher &amp; controller of search for cooley</span>\n        <span class=\"c1\"># num_nodes = self.evaluator.num_workers - 1 #balsam launcher &amp; controller of search for theta</span>\n        <span class=\"n\">num_nodes</span> <span class=\"o\">=</span> <span class=\"n\">LAUNCHER_NODES</span> <span class=\"o\">*</span> <span class=\"n\">WORKERS_PER_NODE</span> <span class=\"o\">-</span> <span class=\"mi\">1</span> <span class=\"c1\"># balsam launcher</span>\n        <span class=\"k\">if</span> <span class=\"n\">num_nodes</span> <span class=\"o\">&gt;</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">num_agents</span><span class=\"p\">:</span>\n            <span class=\"n\">num_episodes_per_batch</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">num_nodes</span><span class=\"o\">-</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">num_agents</span><span class=\"p\">)</span><span class=\"o\">//</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">num_agents</span>\n        <span class=\"k\">else</span><span class=\"p\">:</span>\n            <span class=\"n\">num_episodes_per_batch</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>\n\n        <span class=\"k\">if</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">rank</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n            <span class=\"n\">logger</span><span class=\"o\">.</span><span class=\"n\">debug</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"s1\">&#39;&lt;Rank={self.rank}&gt; num_nodes: {num_nodes}&#39;</span><span class=\"p\">)</span>\n            <span class=\"n\">logger</span><span class=\"o\">.</span><span class=\"n\">debug</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"s1\">&#39;&lt;Rank={self.rank}&gt; num_episodes_per_batch: {num_episodes_per_batch}&#39;</span><span class=\"p\">)</span>\n\n        <span class=\"n\">logger</span><span class=\"o\">.</span><span class=\"n\">debug</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"s1\">&#39;&lt;Rank={self.rank}&gt; starting training...&#39;</span><span class=\"p\">)</span>\n        <span class=\"n\">nas_ppo_sync_a3c</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"p\">(</span>\n            <span class=\"n\">num_episodes</span><span class=\"o\">=</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">num_episodes</span><span class=\"p\">,</span>\n            <span class=\"n\">seed</span><span class=\"o\">=</span><span class=\"mi\">2018</span><span class=\"p\">,</span>\n            <span class=\"n\">space</span><span class=\"o\">=</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">problem</span><span class=\"o\">.</span><span class=\"n\">space</span><span class=\"p\">,</span>\n            <span class=\"n\">evaluator</span><span class=\"o\">=</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">evaluator</span><span class=\"p\">,</span>\n            <span class=\"n\">num_episodes_per_batch</span><span class=\"o\">=</span><span class=\"n\">num_episodes_per_batch</span><span class=\"p\">,</span>\n            <span class=\"n\">reward_rule</span><span class=\"o\">=</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">reward_rule</span>\n        <span class=\"p\">)</span>\n\n\n<span class=\"k\">if</span> <span class=\"vm\">__name__</span> <span class=\"o\">==</span> <span class=\"s2\">&#34;__main__&#34;</span><span class=\"p\">:</span>\n    <span class=\"n\">args</span> <span class=\"o\">=</span> <span class=\"n\">NasPPOSyncA3C</span><span class=\"o\">.</span><span class=\"n\">parse_args</span><span class=\"p\">()</span>\n    <span class=\"n\">search</span> <span class=\"o\">=</span> <span class=\"n\">NasPPOSyncA3C</span><span class=\"p\">(</span><span class=\"o\">**</span><span class=\"nb\">vars</span><span class=\"p\">(</span><span class=\"n\">args</span><span class=\"p\">))</span>\n    <span class=\"n\">search</span><span class=\"o\">.</span><span class=\"n\">main</span><span class=\"p\">()</span></code></pre></div><h3>（3）随机搜索 NAS Random</h3><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">class</span> <span class=\"nc\">deephyper</span><span class=\"o\">.</span><span class=\"n\">search</span><span class=\"o\">.</span><span class=\"n\">nas</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">NasRandom</span><span class=\"p\">(</span><span class=\"n\">problem</span><span class=\"p\">,</span> <span class=\"n\">run</span><span class=\"p\">,</span> <span class=\"n\">evaluator</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">)</span></code></pre></div><p>好了，下面先安装</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">From</span> <span class=\"n\">pip</span><span class=\"p\">:</span>\n\n<span class=\"n\">pip</span> <span class=\"n\">install</span> <span class=\"n\">deephyper</span>\n\n<span class=\"n\">git</span> <span class=\"n\">clone</span> <span class=\"n\">https</span><span class=\"p\">:</span><span class=\"o\">//</span><span class=\"n\">github</span><span class=\"o\">.</span><span class=\"n\">com</span><span class=\"o\">/</span><span class=\"n\">deephyper</span><span class=\"o\">/</span><span class=\"n\">deephyper</span><span class=\"o\">.</span><span class=\"n\">git</span>\n<span class=\"n\">cd</span> <span class=\"n\">deephyper</span><span class=\"o\">/</span>\n<span class=\"n\">pip</span> <span class=\"n\">install</span> <span class=\"o\">-</span><span class=\"n\">e</span> <span class=\"o\">.</span>\n<span class=\"c1\">#if you want to install deephyper with test and documentation packages:</span>\n\n<span class=\"c1\"># From Pypi</span>\n<span class=\"n\">pip</span> <span class=\"n\">install</span> <span class=\"s1\">&#39;deephyper[tests,docs]&#39;</span>\n\n<span class=\"c1\"># From github</span>\n<span class=\"n\">git</span> <span class=\"n\">clone</span> <span class=\"n\">https</span><span class=\"p\">:</span><span class=\"o\">//</span><span class=\"n\">github</span><span class=\"o\">.</span><span class=\"n\">com</span><span class=\"o\">/</span><span class=\"n\">deephyper</span><span class=\"o\">/</span><span class=\"n\">deephyper</span><span class=\"o\">.</span><span class=\"n\">git</span>\n<span class=\"n\">cd</span> <span class=\"n\">deephyper</span><span class=\"o\">/</span>\n<span class=\"n\">pip</span> <span class=\"n\">install</span> <span class=\"o\">-</span><span class=\"n\">e</span> <span class=\"s1\">&#39;.[tests,docs]&#39;</span></code></pre></div><p>以上为本文内容，详细内容请学习文档内容 </p><p>deephyper DocumentationRelease alpha v0.0.5 ：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/NeuronDance/DeepRL/blob/master/DRL%25E4%25B9%25A6%25E7%25B1%258D/%25E7%2594%25B5%25E5%25AD%2590%25E4%25B9%25A6%25E5%258E%259F%25E7%2589%2588/deephyper.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/NeuronDance/</span><span class=\"invisible\">DeepRL/blob/master/DRL%E4%B9%A6%E7%B1%8D/%E7%94%B5%E5%AD%90%E4%B9%A6%E5%8E%9F%E7%89%88/deephyper.pdf</span><span class=\"ellipsis\"></span></a></p><p>Github仓库：<a href=\"https://link.zhihu.com/?target=https%3A//deephyper.readthedocs.io/en/latest/%3Fbadge%3Dlatest\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">deephyper.readthedocs.io</span><span class=\"invisible\">/en/latest/?badge=latest</span><span class=\"ellipsis\"></span></a></p><p>参考文献：<a href=\"https://link.zhihu.com/?target=https%3A//deephyper.readthedocs.io/en/latest/%3Fbadge%3Dlatest\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">deephyper.readthedocs.io</span><span class=\"invisible\">/en/latest/?badge=latest</span><span class=\"ellipsis\"></span></a></p><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p>本文同步至CSDN博客：</p><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/gsww404\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">blog.csdn.net/gsww404</span><span class=\"invisible\"></span></a><p>深度强化学习微信公众号：Deep-RL</p><p></p>", 
            "topic": [
                {
                    "tag": "超参数", 
                    "tagLink": "https://api.zhihu.com/topics/20687672"
                }, 
                {
                    "tag": "模型", 
                    "tagLink": "https://api.zhihu.com/topics/19579715"
                }, 
                {
                    "tag": "AutoML", 
                    "tagLink": "https://api.zhihu.com/topics/20173754"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/56085913", 
            "userName": "DRLearner", 
            "userLink": "https://www.zhihu.com/people/2a0976472ac2864fa905e4ef0e8e5a36", 
            "upvote": 24, 
            "title": "深度强化学习系列(2):（重磅）深度强化学习的加速方法", 
            "content": "<h2>深度强化学习一直以来都以智能体训练时间长、计算力需求大而限制很多的人去学习，比如：AlphaZero训练3天的时间等，因此缩短训练周转时间成为一个重要话题。深度强化学习大神Pieter Abbeel最近发表了深度强化学习的加速方法，他从整体上提出了一个加速深度强化学习周转时间的方法，说起Pieter Abbeel，他是伯克利大学教授，也是强化学习的重要科学家。</h2><p>继续本篇文章的主题《深度强化学习的加速方法》，还是惯例，先放出paper的首页摘要部分。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-09c8f1ae739639fbddb5bec378c79331_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2174\" data-rawheight=\"1048\" class=\"origin_image zh-lightbox-thumb\" width=\"2174\" data-original=\"https://pic2.zhimg.com/v2-09c8f1ae739639fbddb5bec378c79331_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2174&#39; height=&#39;1048&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2174\" data-rawheight=\"1048\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2174\" data-original=\"https://pic2.zhimg.com/v2-09c8f1ae739639fbddb5bec378c79331_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-09c8f1ae739639fbddb5bec378c79331_b.jpg\"/></figure><p>论文地址： <a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1803.02811.pdf\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/pdf/1803.0281</span><span class=\"invisible\">1.pdf</span><span class=\"ellipsis\"></span></a></p><p>最近几年，深度强化学习在各行各业已经有了很成功的应用，但实验的周转时间（turn-around time）仍然是研究和实践中的一个关键瓶颈。 该论文研究如何在现有计算机上优化现有深度RL算法，特别是CPU和GPU的组合。 且作者确认可以调整策略梯度和Q值学习算法以学习使用许多并行模拟器实例。 通过他们进一步发现可以使用比标准尺寸大得多的批量进行训练，而不会对样品复杂性或最终性能产生负面影响。 同时他们利用这些事实来构建一个统一的并行化框架，从而大大加快了两类算法的实验。 所有神经网络计算都使用GPU，加速数据收集和训练。在使用同步和异步算法的基础上，结果标明在使用整个DGX-1在几分钟内学习Atari游戏中的成功策略。</p><blockquote> 注： 1. 周转时间（turnaround time）：训练模型的时间 2. Nvidia DGX-1是Nvidia生产的服务器和工作站系列，专门用于使用GPGPU加速深度学习应用程序。这些服务器具有8个GPU，基于带有HBM 2内存的Pascal或Volta 子卡，通过NVLink 网状网络连接。该产品系列旨在弥合GPU和AI加速器之间的差距，因为该设备具有专门用于深度学习工作负载的特定功能。最初的基于Pascal的DGX-1提供了170 teraflops的半精度处理，而基于Volta的升级将其提高到960 teraflops。<a href=\"https://link.zhihu.com/?target=https%3A//www.nvidia.com/en-us/data-center/dgx-1/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">更多信息，点击查看</a><br/> </blockquote><h2><b>1、背景和相关内容</b></h2><p>目前的深度强化学习严重依赖于经验评估，因此turnaround 时间成为一个关键的限制因素，尽管存在这一重要瓶颈，但许多参考实施方案不能满足现代计算机的吞吐量潜力,在这项工作中，作者研究如何在不改变其基本公式的情况下调整深度RL算法,并在一台机器中更好地利用多个CPU和GPU进行实验。 结果标明，显着提高了硬件利用率的效率和规模，从而提高了学习速度。</p><p>今天比较领先的深度RL算法大致分为两类：</p><p>（i）<b>策略梯度方法</b> ，以Asynchronous Advantage Actor-Critic（A3C）（Mnih et al 2016）是一个代表性的例子， </p><p>（ii）<b>Q值学习方法</b> ，一个代表性的例子是Deep Q-Networks（DQN）（Mnih等，2015）。</p><p>传统上，这两个系列出现在不同的实现中并使用不同的硬件资源，该篇paper作者将它们统一在相同的扩展框架下。作者贡献了并行化深度RL的框架，包括用于推理和训练的GPU加速的新技术。演示了以下算法的多GPU版本：Advantage Actor-Critic（A3C），Proximal Policy Optimization(PPO)，DQN，Categorical DQN和Rainbow。为了提供校准结果，作者通过Arcade学习环境（ALE）测试我们在重度基准测试的Atari-2600域中的实现。同时使用批量推断的高度并行采样可以加速所有实验的（turnaround）周转时间，同时发现神经网络可以使用比标准大得多的批量大小来学习，而不会损害样本复杂性或最终游戏分数。除了探索这些新的学习方式之外，作者还利用它们来大大加快学习速度。例如， 策略梯度算法在8-GPU服务器上运行，在10分钟内学会成功的游戏策略，而不是数小时。他们同样将一些标准Q值学习的持续时间从10天减少到2小时以下。或者，独立的RL实验可以与每台计算机的高聚合吞吐量并行运行。相信这些结果有望加速深度研究，并为进一步研究和发展提出建议。</p><p>另外，作者对演员评论方法的贡献在很多方面超越了目前的很多人做法，他们主要做了：“<b>”改进抽样组织，使用多个GPU大大提高规模和速度，以及包含异步优化。</b></p><h2><b>2、并行，加速的RL框架</b></h2><p>作者考虑使用深度神经网络来实验基于CPU的模拟器环境和策略，在这里描述了一套完整的深度RL并行化技术，可以在采样和优化过程中实现高吞吐量。同时并对GPU进行均匀处理，每个都执行相同的抽样学习过程，该策略可以直接扩展到各种数量的GPU。</p><h2><b>2.1 、同步采样（Synchronized Sampling）</b></h2><p>首先将多个 <b>CPU核心</b> 与 <b>单个GPU</b> 相关联。多个模拟器在CPU内核上以并行进程运行，并且这些进程以同步方式执行环境步骤。在每个步骤中，将所有单独的观察结果收集到批处理中以进行推理，在提交最后一个观察结果后在GPU上调用该批处理。 一旦动作返回，模拟器再次步骤，依此类推，系统共享内存阵列提供了动作服务器和模拟器进程之间的快速通信。</p><p>由于落后效应等同于每一步的最慢过程，同步采样可能会减速。步进时间的变化源于不同模拟器状态的不同计算负载和其他随机波动。随着并行进程数量的增加，落后者效应会恶化，但通过在每个进程中堆叠多个独立的模拟器实例来缓解它。每个进程为每个推理批处理步骤（顺序）执行所有模拟器。这种安排还允许用于推断的批量大小增加超过进程数（即CPU核心），其原理如图1（a）所示。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3d099ba18e8bfe20cc5a13fdaad3ee19_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"744\" data-rawheight=\"1174\" class=\"origin_image zh-lightbox-thumb\" width=\"744\" data-original=\"https://pic2.zhimg.com/v2-3d099ba18e8bfe20cc5a13fdaad3ee19_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;744&#39; height=&#39;1174&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"744\" data-rawheight=\"1174\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"744\" data-original=\"https://pic2.zhimg.com/v2-3d099ba18e8bfe20cc5a13fdaad3ee19_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-3d099ba18e8bfe20cc5a13fdaad3ee19_b.jpg\"/></figure><p>通过仅在优化暂停期间重置可以避免由长环境重置引起的减速，如果模拟和推理负载平衡，则每个组件将在一半的时间内处于空闲状态，因此我们形成两组交替的模拟器过程。当一个组等待其下一个动作时，其他步骤和GPU在为每个组服务之间交替。交替保持高利用率，并且进一步隐藏两者中较快的计算的执行时间。我们通过重复模板组织多个GPU，均匀分配可用的CPU核心。我们发现修复每个模拟器进程的CPU分配是有益的，其中一个核心保留用于运行每个GPU。实验部分包含采样速度的测量值，该测量值随环境实例的数量而增加。</p><h2><b>2.2、同步多GPU优化（Synchronous Multi-GPU Optimization）</b></h2><p>在同步算法中，所有GPU都保持相同的参数值，利用随机梯度估计的数据并行性并在每个GPU上使用众所周知的更新程序： + 1）使用本地采集的样本计算梯度， + 2）全部减少GPU之间的梯度， + 3）使用组合梯度 更新本地参数。 我们使用NVIDIA集体通信库在GPU之间进行快速通信。</p><h2><b>2.3、异步多GPU优化（Asynchronous Multi-GPU Optimization）</b></h2><p>在异步优化中，每个GPU充当其自己的采样器-学习器单元，并将更新应用于CPU内存中保存的中央参数存储。使用加速器会强制选择执行参数更新的位置。根据经验，在GPU上将更快的常见规则应用于网络更快。一般更新程序包括三个步骤： + 步骤（1）: 在本地计算梯度并将其存储在GPU上 + 步骤（2）: 将当前中心参数拉到GPU上并使用预先计算的梯度将更新规则应用于它们 + 步骤（3）: 写入更新的参数回到中央CPU商店。</p><p>在此序列之后，本地GPU参数与中心值同步，并且再次进行采样，集中更新规则参数。其不会将更新增量添加到需要CPU计算的中心参数，而是覆盖这些值。因此，采用上述步骤（2）和（3）的锁定，防止其他进程同时读取或写入参数值。同时将参数分成少量不相交的块，这些块分别更新，每个块都有自己的锁（步骤2-3成为块上的循环）。这可以平衡更新调用效率与锁争用，并可以提供良好的性能。</p><h2><b>3、实验</b></h2><p>使用Atari-2600域来研究高度并行化RL的缩放特性，研究如下： + 1）同步采样的效率如何，它可以达到什么速度？ + 2）策略梯度和Qlearning算法是否可以适应学习使用许多并行模拟器实例而不会降低学习成绩 + 3）大批量培训和/或异步方法能否加快优化速度而不会降低样品的复杂性？</p><p>在所有学习实验中，作者保持原始训练强度，意味着每个采样数据点的平均训练使用次数。对于A3C，PPO和DQN+变体，参考训练强度分别为1，4和8。此处显示的所有学习曲线均为至少两个随机种子的平均值。对于策略梯度方法，我们跟踪在线分数，对最近100个完成的轨迹进行平均。对于DQN和变体，我们每100万步暂停以评估，直到达到125,000步，最大路径长度为27,000步。</p><h2><b>3.1、Sampling（采样）</b></h2><p>一系列仅采样测量表明，尽管存在潜在的落后者，同步采样方案可以实现良好的硬件利用率。首先，我们研究了 <b>单个GPU</b> 在为多个环境提供推理时的容量。图1（b）显示了在播放BREAKOUT时在P100 GPU上运行训练有素的A3C-Net策略的测量结果。通过CPU核心计数归一化的聚合采样速度被绘制为在每个核心上运行的（顺序）Atari模拟器的数量的函数，交替方案的最小值是每个核心2个模拟器。不同的曲线表示运行模拟的不同数量的CPU核心。作为参考，我们包括在没有推断的情况下运行的单个核心的采样速度--单个过程的虚线，以及两个超线程中的每一个的虚线一个过程。使用推理和单核运行，采样速度随着模拟器计数而增加，直到推断时间完全隐藏。出现更高核心数的同步丢失。但是，每个核心只有8个环境，GPU甚至支持16个CPU内核，运行速度大约为无推理速度的80％。</p><p>接下来，测量了在整个8-GPU，40核服务器上并行播放BREAKOUT的同一A3C-Net的仅采样速度。 在模拟器计数为256（每个核心8个）及以上时，服务器每秒实现大于35,000个样本，或每小时5亿个仿真器帧，其结果如图：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-827b7f5e5e2d20477a71b8421e841d09_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1834\" data-rawheight=\"1056\" class=\"origin_image zh-lightbox-thumb\" width=\"1834\" data-original=\"https://pic2.zhimg.com/v2-827b7f5e5e2d20477a71b8421e841d09_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1834&#39; height=&#39;1056&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1834\" data-rawheight=\"1056\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1834\" data-original=\"https://pic2.zhimg.com/v2-827b7f5e5e2d20477a71b8421e841d09_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-827b7f5e5e2d20477a71b8421e841d09_b.jpg\"/></figure><h2><b>3.2、许多模拟器实例（Learning with Many Simulator Instances）</b></h2><p>为了利用并行采样的高吞吐量，同时研究了如何使用现有的深度RL算法来学习许多模拟器实例。以下研究结果表明，只有微小的变化才能适应所有算法并保持性能。我们为每种算法尝试了不同的技术。有趣的是，缩放对同步和异步学习的影响有所不同。</p><p><b>开始状态解相关（Starting State Decorrelation）</b> 在许多模拟器的一些策略梯度实验中，学习很早就失败了。我们发现在起始游戏状态中的相关性导致大的但知情度不足的学习信号，从而破坏了早期学习的稳定性。通过在实验初始化期间通过随机数量的均匀随机动作步进每个模拟器来纠正此问题。采取这一措施时，发现学习率升温没有进一步的效果。在训练时，游戏重置照常进行。</p><p><b>A2C</b> ：优化批量大小随着模拟器的数量而增加（保持采样范围固定）。相应地，每个采集的样本进行的参数更新步骤更少，我们发现，在一组测试游戏中，以批量大小的平方根增加学习率是最佳的。图2的上图显示了学习曲线与总样本数，模拟器计数范围为16到512（批量大小为80到2,560）。虽然大型模拟器计数的样本效率逐渐下降，但游戏分数基本没有变化。</p><p><b>A3C</b>：我们测试的异步适应使用16环境A2C代理作为基础采样器 - 学习器单元。图2显示学习曲线与学习者数量的总样本计数，范围从1到32,4，对应于16到512个模拟器。在大多数情况下，由此产生的学习曲线几乎无法区分，尽管有些学习曲线在最大范围内降级。</p><p><b>PPO</b>：已经用于基准PPO的大批量（8模拟器x 256-horizo​​n = 2,048）提供了与许多模拟器学习的不同途径：我们减少了采样范围，使得总批量大小保持固定。图2显示了模拟器计数从8到512的学习曲线与样本计数，相应的采样范围从256到4步。成功的学习继续保持最大规模。</p><p><b>APPO</b>：尝试了PPO的异步版本，使用8模拟器PPO代理作为基础学习器单元。图2中的底部面板显示了对8个GPU运行的8个学习者的研究的学习曲线，其中通信频率不同。标准PPO每个时期使用4个梯度更新，每个优化使用4个时期;我们在同步之间进行了1-4次渐变更新（补充材料中提供了更新规则）。我们发现在采样期间定期从中心参数中提取新值是有帮助的，并且在所有情况下都采用64步的范围（因此减少了异步技术中固有的策略滞后，PPO的频率降低，但更频繁，但更实质性的更新） 。在几个游戏中，学习保持一致，表明可以减少某些异步设置中的通信。</p><p><b>DQN + Variants</b>：我们通过模拟器组织了体验重放缓冲区。总缓冲区大小保持在1百万次转换，因此每个模拟器保持相应较短的历史记录。我们观察到学习性能在很大程度上独立于模拟器计数高达200以上，前提是每个优化周期的更新步骤数不是太多（大批量大小可以改善这一点）。</p><h2><b>3.3、Q-Value Learning with Large Training Batches</b></h2><p>DQN：我们试验了从标准32到2,048的批量大小。我们发现一致的学习成绩高达512，超过这一点，很难找到在所有测试游戏中表现良好的单一（缩放）学习率。在几个游戏中，更大的批量大小改进了学习，如图3所示。 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-b1d0def61194deffc1958410093c0613_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1758\" data-rawheight=\"650\" class=\"origin_image zh-lightbox-thumb\" width=\"1758\" data-original=\"https://pic4.zhimg.com/v2-b1d0def61194deffc1958410093c0613_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1758&#39; height=&#39;650&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1758\" data-rawheight=\"650\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1758\" data-original=\"https://pic4.zhimg.com/v2-b1d0def61194deffc1958410093c0613_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-b1d0def61194deffc1958410093c0613_b.jpg\"/></figure><p>发现异步DQN可以使用多达4个GPU学习者很好地学习，每个学习者使用批量大小512。分类DQN：我们发现分类DQN比DQN进一步扩展。图3的下显示批量大小高达2,048的学习曲线，最大分数没有减少。这可能是由于梯度信号的内容更丰富。值得注意的是，SEAQUEST游戏中最大批量的学习被延迟，但最终达到了更高的最高分。由于使用了Adam优化器，因此没有必要缩放学习率。</p><p>e-Rainbow：尽管使用了分布式学习，但在某些游戏中，e-Rainbow的性能却超过批量512。该批次大小的分数大致与批量大小为32的文献中报道的分数相符（Hessel等，2017）（曲线显示在附录中）。</p><h2><b>3.4、学习速度（Learning Speed）</b></h2><p>研究运行8-GPU，40核服务器（P100 DGX-1），以学习单个游戏时可获得的学习速度，作为大规模实施的示例。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-cce10f3762dd62cd22d7b7cf29339b24_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1802\" data-rawheight=\"400\" class=\"origin_image zh-lightbox-thumb\" width=\"1802\" data-original=\"https://pic1.zhimg.com/v2-cce10f3762dd62cd22d7b7cf29339b24_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1802&#39; height=&#39;400&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1802\" data-rawheight=\"400\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1802\" data-original=\"https://pic1.zhimg.com/v2-cce10f3762dd62cd22d7b7cf29339b24_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-cce10f3762dd62cd22d7b7cf29339b24_b.jpg\"/></figure><p>图4显示了策略梯度方法A2C，A3C，PPO和APPO的良好性能配置的结果。几场比赛表现出陡峭的初始学习阶段;所有算法都在10分钟内完成了该阶段。值得注意的是，PPO在4分钟内掌握了Pong。具有256个环境的A2C每秒处理超过25,000个样本，相当于每小时超过9000万步（3.6亿帧）。表2列出了缩放测量，显示使用8个GPU相对于1的加速比大于6倍。</p><p>我们运行了DQN及其变体的同步版本，训练时间如表2所示。使用1个GPU和5个CPU核心，DQN和e-Rainbow分别在8小时和14小时内完成了5000万步（2亿帧），一个重要的获得超过10天的参考时间。后者使用1个GPU和376个CPU核心（参见例如图2中的10小时学习曲线）。使用多个GPU和更多内核加速了我们的实施。凭借更大的批量大小，Categorical-DQN使用整个服务器在2小时内完成最佳扩展和完成培训，相对于1 GPU，速度超过6倍。然而，DQN和e-Rainbow的回报减少超过2个GPU。我们无法找到进一步提高学习速度而不会在某些游戏中降低性能的异步配置（我们只测试了完全通信的算法）。可能存在改善我们扩展的机会。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b8ab994f5ffcc94ac1703aa91e1fb981_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1040\" data-rawheight=\"712\" class=\"origin_image zh-lightbox-thumb\" width=\"1040\" data-original=\"https://pic2.zhimg.com/v2-b8ab994f5ffcc94ac1703aa91e1fb981_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1040&#39; height=&#39;712&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1040\" data-rawheight=\"712\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1040\" data-original=\"https://pic2.zhimg.com/v2-b8ab994f5ffcc94ac1703aa91e1fb981_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-b8ab994f5ffcc94ac1703aa91e1fb981_b.jpg\"/></figure><h2><b>4、批量大小对优化的影响（Effects of Batch Size on Optimization）</b></h2><p>限制培训批量大小的可能因素包括： + 1、减少探索，因为在环境中运行的网络较少， + 2、网络权重的数值优化存在困难。我们进行了实验以开始识别这些因素。</p><h2><b>4.1、 二级学习者实验（Secondary-Learner Experiment）</b></h2><p>我们配置了一个辅助DQN学习器，仅使用普通DQN代理的重放缓冲区进行训练。初级学习者使用与主要参数值相同的参数值进行初始化， “采样器 - 学习器”，两个网络同时训练，数据消耗速率相同。每个人都抽样自己的培训批次。在BREAKOUT的游戏中，64和2048采样器学习者获得了相同的分数，但是2048学习者需要更多的样本，尽管使用最快的稳定学习率（数字指的是训练批量大小）。当使用2048样本学习者训练64中学习者时，中学习者的分数跟踪了初级学习者的分数。然而，在相反的情况下，2048中学生无法学习。我们认为这是由于参数更新数量减少的优化速度较慢 - 它无法跟踪初始化附近的Q值估计的快速变化，并且变得过于偏离策略学习。在使用两个256学习者的相同测试中，他们的分数相匹配。如果2048年的二级学习者超过了2048年的样本学习者，那么就会认为探索是一个比优化更重要的因素。有关数据，请参阅补充材料。</p><h2><b>4.2、更新规则（Update Rule）</b></h2><p>我们进行了一项实验，以确定更新规则对分类DQN中的优化的影响。我们发现Adam公式优于RMSProp，为大批量学习者提供了在学习过程中遍历参数空间的能力。当比较实现相同学习曲线的代理时，那些使用较小批量（并因此执行更多更新步骤）的代理倾向于在训练中的所有点具有更大的参数矢量规范。与RMSProp不同，Adam规则导致批量大小之间参数规范的相当紧密的传播，而不会改变学习率。这解释了在分类DQN和e-Rainbow中不需要缩放学习率，并且表明更新规则在缩放中起着重要作用。更多细节，包括卷积和完全连通层的趋势.</p><p>关于更新规则和批量大小规模的观察的细节</p><p>我们在两个不同的参数更新规则下提出了缩放训练批量大小对神经网络优化的影响的观察结果：Adam和RMSProp（没有动量的RMSProp，只有平方梯度的直接累积，参见例如(<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Lasagne/Lasagne/blob/master/lasagne/updates.py\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/Lasagne/Lasa</span><span class=\"invisible\">gne/blob/master/lasagne/updates.py</span><span class=\"ellipsis\"></span></a>)。我们在游戏Q * BERT上培训代理，调整学习率以产生非常相似的所有设置的性能曲线，并且我们在学习期间跟踪了几个量的L-2矢量规范。这些包括渐变，参数更新步骤和参数值本身。与本文中的所有DQN实验一样，训练强度固定为8，因此学习期间参数更新步骤的数量与批量大小成反比。每个设置运行两个随机种子。</p><p>尽管整个训练过程中游戏分数大致相同，但在任何一点上找到的确切解决方案都没有，正如不同的参数规范所证明的那样。没有使用正规化。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-8cdcd7e248c458767abb8aad2ac38b18_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1370\" data-rawheight=\"1624\" class=\"origin_image zh-lightbox-thumb\" width=\"1370\" data-original=\"https://pic1.zhimg.com/v2-8cdcd7e248c458767abb8aad2ac38b18_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1370&#39; height=&#39;1624&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1370\" data-rawheight=\"1624\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1370\" data-original=\"https://pic1.zhimg.com/v2-8cdcd7e248c458767abb8aad2ac38b18_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-8cdcd7e248c458767abb8aad2ac38b18_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>根据图-8，其中曲线由批量大小和学习率标记。当整体观察网络（即所有权重和偏差的规范作为单个向量）时，趋势反映了在大多数权重为的FC-0中看到的趋势。</p><p>i）学习曲线：我们控制游戏得分，根据需要调整学习率。对于批量大小为64的RMSProp，我们考虑的学习率略低（1×10^(-4)），学习速度慢，最终得分较低，学习率略高（5*10^-4） ），由于不稳定性而产生较低的最终得分 - 这些是所有面板中的虚线。</p><p>ii）完全连接-0 Weights-Norm：尽管对所有设置使用相同的学习速率，但Adam优化器产生了相当紧密的分组。另一方面，RMSProp学习者需要在批量大小为64到1,024之间将学习率提高20倍，然后产生非常相似的规范。在批量大小64处，慢/不稳定学习分别以小/大规范为特征。批量大小256次运行的大规范表明这种学习率可能接近稳定性的上限。</p><p>iii）完全连接-0梯度 - 范数：在两个更新规则下，大批量大小总是产生较小的梯度向量 - 减小的方差导致减小的量值。在查看总梯度范数时，我们还在策略梯度方法中观察到了这种模式。这里，梯度的大小与参数范数成反比;请参阅RMSProp 64批量大小的曲线。这种效果与批量大小的影响相反并且被抵消了。 iv）完全连接-0步骤规范：尽管梯度较小，但Adam优化器为较大批量学习者产生了明显更大的步长。 RMSProp需要调整学习率才能产生相同的效果。在两个更新规则下，步长增加量并未完全补偿步数的减少，这表明较大的批量学习者通过参数空间遵循更直的轨迹。 RMSProp总体上导致了更大的步骤，但尽管以较小的权重结束了学习 - 其学习轨迹显然不那么直接，更蜿蜒。</p><p>v）卷积-0权重 - 范数：亚当优化器在这里的规范中比在FC-0层中扩展得更多;随着批量增加，学习重点从Conv-0转移。但是在RMSProp中，学习率的提高导致第一个卷积层对于更大的批量大小变大，更加强调这一层。</p><p>vi）卷积-0梯度 - 范数：亚当更新规则在梯度范数中产生了一个有趣的交叉;大批量学习者实际上开始走高，抵消了其他情况下看到的趋势。 RMSProp下的模式与FC-0的模式相匹配。</p><p>vii）卷积-0步骤范数：与FC-0不同，步骤范数在Adam下的批量大小没有显着变化。 RMSProp产生了与FC-0类似的模式。总体而言，Adam优化器似乎可以补偿FC-0层中的批量大小，但在Conv-0层中则较少，导致Conv-0中大批量学习时不再强调。 RMSProp中学习率的提高补偿了FC-0层中的批量大小，并增加了对Conv-0学习的重视程度。这种模式可能会对学习表示与游戏策略产生影响。对这些明显趋势的进一步研究可以深入了解学习退化的原因以及大批量RL的可能解决方案。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-95320d3441a4b4c90cc5928529ea83b9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1456\" data-rawheight=\"1196\" class=\"origin_image zh-lightbox-thumb\" width=\"1456\" data-original=\"https://pic2.zhimg.com/v2-95320d3441a4b4c90cc5928529ea83b9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1456&#39; height=&#39;1196&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1456\" data-rawheight=\"1196\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1456\" data-original=\"https://pic2.zhimg.com/v2-95320d3441a4b4c90cc5928529ea83b9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-95320d3441a4b4c90cc5928529ea83b9_b.jpg\"/></figure><h2><b>4.3、梯度估计饱和度（Gradient Estimate Saturation）</b></h2><p>使用A2C，我们在每次迭代时测量正常，全批次梯度和仅使用批次的一半计算的梯度之间的关系。对于小批量试剂，测量的全批次和半批次梯度之间的平均余弦相似度接近1 =P2。 这意味着两个半批渐变是正交的，高维空间中的零中心随机向量也是正交的。 然而，对于大批量学习者（例如256个环境），余弦相似性在1 = p2之上显着增加。 梯度估计的饱和度明显与较差的样本效率相关，如图2顶部的学习曲线所示。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-42e33086dd758aeff4f7412a75d0c8ba_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1726\" data-rawheight=\"1216\" class=\"origin_image zh-lightbox-thumb\" width=\"1726\" data-original=\"https://pic3.zhimg.com/v2-42e33086dd758aeff4f7412a75d0c8ba_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1726&#39; height=&#39;1216&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1726\" data-rawheight=\"1216\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1726\" data-original=\"https://pic3.zhimg.com/v2-42e33086dd758aeff4f7412a75d0c8ba_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-42e33086dd758aeff4f7412a75d0c8ba_b.jpg\"/></figure><h2>总结：</h2><p>我们引入了一个统一的框架来并行化深度RL，它使用硬件加速器来实现快速学习。该框架适用于一系列算法，包括策略梯度和Q值学习方法。我们的实验表明，几种领先的算法可以高度并行的方式学习各种Atari游戏，而不会损失样本复杂性和前所未有的挂钟时间。该结果表明了显着提高实验规模的有希望的方向。我们将发布代码库。我们注意到扩展该框架的几个方向。首先是将其应用于Atari以外的领域，尤其是涉及感知的领域。其次，由于GPU加速推理和训练，我们的框架很可能有利地扩展到更复杂的神经网络代理。此外，随着网络复杂性的增加，扩展可能变得更容易，因为GPU可以以较小的批量大小有效地运行，尽管通信开销可能会恶化。降低精度算术可以加速学习 - 由于使用基于CPU的推理，在深度RL中尚待探索的主题。当前的单节点实现可以是用于分布式算法的构建块。关于深度RL中可能的并行化程度的问题仍然存在。我们还没有最终确定缩放的限制因素，也没有确定每个游戏和算法是否相同。虽然我们已经看到大批量学习中的优化效果，但其他因素仍然存在。异步扩展的限制仍未得到探索;我们没有明确确定这些算法的最佳配置，但只提供了一些成功的版本。更好的理解可以进一步提高缩放率，这是推动深度RL的一个有希望的方向。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>-----------------</p><p>至此，以上是对深度强化学习的加速方法的部分解读，受能力有限，文中难免有错误之处，还望大家多提意见、以便做的更好！</p><h2>参考文献：</h2><p>ACCELERATED METHODS FOR DEEP REINFORCEMENT LEARNING</p><p class=\"ztext-empty-paragraph\"><br/></p><p>号外号外！</p><p><b>1、欢迎大家踊跃投稿--深度强化学习论文解读！</b></p><p>2、请尊重每一位创作者的汗水，转载请注明出处！</p><p>更多最新方法和解读的实时更新请关注公众号！</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-4f3ec2b55029788d646d99517425d670_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"430\" data-rawheight=\"430\" class=\"origin_image zh-lightbox-thumb\" width=\"430\" data-original=\"https://pic1.zhimg.com/v2-4f3ec2b55029788d646d99517425d670_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;430&#39; height=&#39;430&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"430\" data-rawheight=\"430\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"430\" data-original=\"https://pic1.zhimg.com/v2-4f3ec2b55029788d646d99517425d670_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-4f3ec2b55029788d646d99517425d670_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "强化学习 (Reinforcement Learning)", 
                    "tagLink": "https://api.zhihu.com/topics/20039099"
                }, 
                {
                    "tag": "加速", 
                    "tagLink": "https://api.zhihu.com/topics/19609182"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/54519793", 
            "userName": "DRLearner", 
            "userLink": "https://www.zhihu.com/people/2a0976472ac2864fa905e4ef0e8e5a36", 
            "upvote": 5, 
            "title": "深度强化学习系列(1): 深入浅出“多巴胺”（Dopamine）论文解读、环境配置、实例分析", 
            "content": "<p></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-64b50ace0c7b1a9654e69ae617244e3b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1908\" data-rawheight=\"1242\" class=\"origin_image zh-lightbox-thumb\" width=\"1908\" data-original=\"https://pic4.zhimg.com/v2-64b50ace0c7b1a9654e69ae617244e3b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1908&#39; height=&#39;1242&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1908\" data-rawheight=\"1242\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1908\" data-original=\"https://pic4.zhimg.com/v2-64b50ace0c7b1a9654e69ae617244e3b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-64b50ace0c7b1a9654e69ae617244e3b_b.jpg\"/></figure><p>Paper:   Dopamine--a research framework for deep reinforcement Learning</p><p>Github: <a href=\"https://link.zhihu.com/?target=https%3A//github.com/google/dopamine\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/google/dopam</span><span class=\"invisible\">ine</span><span class=\"ellipsis\"></span></a></p><p class=\"ztext-empty-paragraph\"><br/></p><p>论文的首页明显告诉我们，这是一篇Google出的论文（所以值得一读），该文作者提出了一种新的深度强化学习研究框架： 多巴胺（Dopamine），旨在于丰富DRL的多样性，该框架是一个开源的，基于tensorflow平台的的最先进的智能体实现平台，并通过深入研究RL中不同研究目标的分类来补充这一产品。虽然并非详尽无遗，但分析强调了该领域研究的异质性以及框架的价值。</p><h2><b>Introduction</b></h2><p>目前很多强化学习的研究主要集中在特定领域的决策，比如视觉识别后的控制（atari）等，由于复杂的相互作用的转变，为深度RL研究编写可重复使用的软件也变得更具挑战性。首先，写一个智能体需要一个architecure，比如用openai的baseline实现DQN，它的智能体由6个模块组成，其次，现在有很多算法可供选择，因此在一个实现中全面实现通常需要牺牲简单性。 最重要的是深度RL研究的日益多样化使得很难预见下一个研究项目可能具备的软件需求。</p><p>这篇文章提出的多巴胺的目的是为深度强化学习提供一个基础研究，它强调的是紧凑，而不是全面，第一个版本由12个python文件，这些为Arcade学习环境提供了最先进的，基于价值的智能体的测试实现。该代码旨在为该领域的新手容易理解，同时为所有智能体提供交互式笔记本，训练模型和可下载的训练数据，以及包括以前发布的学习曲线的源。</p><p>同时作者确定了不同的研究目标，并讨论了支持这些目标的代码的期望：architecture research, comprehensive studies, visualization, algorithmic research, and instruction，并在设计Dopamine时考虑了最后两个目标，他们的研究只在于在深层RL框架生态系统中扮演着独特的角色。</p><h2><b>深度强化学习的软件研究</b></h2><p>深度学习社区现已确定了许多对其研究目标至关重要的操作：组件模块化，自动区分和可视化等，由于强化学习是一个比较新的领域，导致在对软件的共识一直很elusive（难懂的；难捉摸的），该文不会详尽的去解释，但强调了该研究自然属性的多样性。为了能够对重点突出研究，作者仅从下面两个方面研究：</p><ol><li>深度强化学习的基础研究。</li><li>应用于或评估模拟环境。</li></ol><p>同时，该文的研究环境为：Arcade Learning Environment(该环境是一个为很多研究者提供实现atari游戏智能体的简单的框架：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/mgbellemare/Arcade-Learning-Environment\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Arcade Learning Environment</a>,</p><h2><b>case研究</b></h2><p>所谓的架构研究涉及组件之间的交互，包括网络拓扑，以创建深度RL代理。 DQN在使用代理体系结构方面具有创新性，包括目标网络，重放内存和Atari特定的预处理。 从那时起，如果没有预料到代理由多个交互组件组成就变得司空见惯了，通常情况下从以下三个方面着手：</p><ol><li>Algorithmic research 架构研究</li><li>Comprehensive studies</li><li>Visualization</li></ol><h2><b>不同目标的不同软件的研究从下面四个角度开始</b></h2><ol><li>Comprehensive studies</li><li>Architecture research</li><li>Algorithmic research</li><li>Conclusions.</li></ol><h2><b>多巴胺（Dopamine）</b></h2><p>在多巴胺的设计中，google 设计它满足了两个条件：自给自足且紧凑、可靠且可重复：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-b1d8320bfac6e6ee31a386d24e388a3f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1218\" data-rawheight=\"384\" class=\"origin_image zh-lightbox-thumb\" width=\"1218\" data-original=\"https://pic4.zhimg.com/v2-b1d8320bfac6e6ee31a386d24e388a3f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1218&#39; height=&#39;384&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1218\" data-rawheight=\"384\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1218\" data-original=\"https://pic4.zhimg.com/v2-b1d8320bfac6e6ee31a386d24e388a3f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-b1d8320bfac6e6ee31a386d24e388a3f_b.jpg\"/></figure><p>自给自足且紧凑可以帮助研究者实现一个简单的框架，并且可以集中进行算法研究，同时可靠则保证了实验、算法的结果具有trust。</p><h2><b>多巴胺的整体结构设计</b></h2><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-006f60153940429c32a04339e6829884_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1256\" data-rawheight=\"552\" class=\"origin_image zh-lightbox-thumb\" width=\"1256\" data-original=\"https://pic1.zhimg.com/v2-006f60153940429c32a04339e6829884_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1256&#39; height=&#39;552&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1256\" data-rawheight=\"552\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1256\" data-original=\"https://pic1.zhimg.com/v2-006f60153940429c32a04339e6829884_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-006f60153940429c32a04339e6829884_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>其中：蓝盒子是软件组件。 黄色框表示组成软件组件的文件名; 有向箭头表示类继承，而括号中的数字表示非注释Python行的数量。</p><ol><li>Runner类管理代理和ALE之间的交互（例如，采取步骤和接收观察）以及簿记（例如，分别通过检查指针和记录器进行检查点和记录）。</li><li>Checkpointer负责定期保存实验状态，使故障后能够正常恢复，并重新学习重量。</li><li>记录器负责将实验统计（例如，累积的培训或评估奖励）保存到磁盘以进行可视化。</li><li>Colab提供交互式笔记本，以便于这些统计数据的可视化。</li></ol><h2><b>可靠且可重复：</b></h2><p>作者为所有代码库提供了一整套测试，代码覆盖率超过98％。除了帮助确保代码的正确性之外，这些测试还提供了另一种形式的文档，补充了随框架提供的常规文档和交互式笔记本。</p><p>多巴胺使用gin-config 【<a href=\"https://zhuanlan.zhihu.com/p/54519793/github.com/google/gin-config\" class=\"internal\">config过程</a>】来配置不同的模块。 Gin-config是参数注入的简单方案，即动态更改方法的默认参数。 在多巴胺中，在单个文件中指定实验的所有参数。 下面代码显示了默认DQN代理设置的配置示例（附录D中提供了所有代理的完整gin-config文件）</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">DQNAgent</span><span class=\"o\">.</span><span class=\"n\">gamma</span> <span class=\"o\">=</span> <span class=\"mf\">0.99</span>\n<span class=\"n\">DQNAgent</span><span class=\"o\">.</span><span class=\"n\">epsilon_train</span> <span class=\"o\">=</span> <span class=\"mf\">0.01</span>\n<span class=\"n\">DQNAgent</span><span class=\"o\">.</span><span class=\"n\">epsilon_decay_period</span> <span class=\"o\">=</span> <span class=\"mi\">250000</span>  <span class=\"c1\"># agent steps</span>\n<span class=\"n\">DQNAgent</span><span class=\"o\">.</span><span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"nd\">@tf.train.RMSPropOptimizer</span><span class=\"p\">()</span>\n<span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"o\">.</span><span class=\"n\">RMSPropOptimizer</span><span class=\"o\">.</span><span class=\"n\">learning_rate</span> <span class=\"o\">=</span> <span class=\"mf\">0.00025</span>\n<span class=\"n\">Runner</span><span class=\"o\">.</span><span class=\"n\">sticky_actions</span> <span class=\"o\">=</span> <span class=\"bp\">True</span>\n<span class=\"n\">WrappedReplayBuffer</span><span class=\"o\">.</span><span class=\"n\">replay_capacity</span> <span class=\"o\">=</span> <span class=\"mi\">1000000</span>\n<span class=\"n\">WrappedReplayBuffer</span><span class=\"o\">.</span><span class=\"n\">batch_size</span> <span class=\"o\">=</span> <span class=\"mi\">32</span></code></pre></div><h2><b>基准线对比</b></h2><p>作者在探索的目的不是提供一组最佳的超参数，而是提供一致的集合作为基线，同时促进超参数探索的过程。下图是实验的数据显示： </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-a136cdc1b595bcefb399ed534cc059c7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1484\" data-rawheight=\"476\" class=\"origin_image zh-lightbox-thumb\" width=\"1484\" data-original=\"https://pic4.zhimg.com/v2-a136cdc1b595bcefb399ed534cc059c7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1484&#39; height=&#39;476&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1484\" data-rawheight=\"476\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1484\" data-original=\"https://pic4.zhimg.com/v2-a136cdc1b595bcefb399ed534cc059c7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-a136cdc1b595bcefb399ed534cc059c7_b.jpg\"/></figure><p>同时作者也将已发布设置的结果与默认设置进行比较(下图)，平均超过5次运行。 在每个游戏中，已发布和默认设置之间的y-scales是不同的; 这主要是因为默认设置中使用了粘滞action。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-b585144dd991398ebb2309fe3383cf22_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1816\" data-rawheight=\"416\" class=\"origin_image zh-lightbox-thumb\" width=\"1816\" data-original=\"https://pic3.zhimg.com/v2-b585144dd991398ebb2309fe3383cf22_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1816&#39; height=&#39;416&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1816\" data-rawheight=\"416\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1816\" data-original=\"https://pic3.zhimg.com/v2-b585144dd991398ebb2309fe3383cf22_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-b585144dd991398ebb2309fe3383cf22_b.jpg\"/></figure><p>重新审视ARCADE学习环境：一个测试案例</p><h2><b>Episode终止</b></h2><p>当一个人正常停止游戏时，ALE认为一个episode完成：当他们完成游戏或者用尽生命时。 将这种终止条件称为“游戏结束”。 Mnih等引入了一种称为生命损失的启发式方法，当玩家失去生命时，它会在重放记忆中添加人工插曲边界。 在最近的文献中已经使用了episode终止的两种定义。 在多巴胺中运行此实验包括修改以下gin-config选项：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">AtariPreprocessing</span><span class=\"o\">.</span><span class=\"n\">terminal_on_life_loss</span> <span class=\"o\">=</span> <span class=\"bp\">True</span></code></pre></div><p>下图显示了两种情况下报告的性能差异。虽然episode表明，Life Loss启发式技术可以提高一些简单游戏的性能，但是Bellemare等人指出，它阻碍了其他人的表现，特别是因为智能体无法了解失去生命的真实后果。根据Machado等人的指南，在默认设置中禁用了Life Loss启发式。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-206384e406408d205dbf523853fc904e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1718\" data-rawheight=\"476\" class=\"origin_image zh-lightbox-thumb\" width=\"1718\" data-original=\"https://pic3.zhimg.com/v2-206384e406408d205dbf523853fc904e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1718&#39; height=&#39;476&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1718\" data-rawheight=\"476\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1718\" data-original=\"https://pic3.zhimg.com/v2-206384e406408d205dbf523853fc904e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-206384e406408d205dbf523853fc904e_b.jpg\"/></figure><h2><b>测量训练数据和总结学习性能</b></h2><p>多巴胺支持两种运行方式：train和train_and_eval。 前者仅测量训练期间的平均分数，而后者则与评估运行相交学习。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-09a5cadca1d7be0d64ce8bc4fe20c927_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1694\" data-rawheight=\"474\" class=\"origin_image zh-lightbox-thumb\" width=\"1694\" data-original=\"https://pic4.zhimg.com/v2-09a5cadca1d7be0d64ce8bc4fe20c927_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1694&#39; height=&#39;474&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1694\" data-rawheight=\"474\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1694\" data-original=\"https://pic4.zhimg.com/v2-09a5cadca1d7be0d64ce8bc4fe20c927_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-09a5cadca1d7be0d64ce8bc4fe20c927_b.jpg\"/></figure><p><b>粘性动作影响在智能体上的性能</b></p><p>原始ALE具有确定性转换，其奖励可以记忆行动序列以获得高分的代理。为缓解此问题，最新版本的ALE实现了粘性操作。粘性动作使用粘性参数ς，这是环境执行代理程序之前操作的概率，而不是代理程序刚刚选择的那种 - 有效地实现了一种动作形式。 在多巴胺中运行此实验包括修改以下gin-config选项：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">Runner</span><span class=\"o\">.</span><span class=\"n\">sticky_actions</span> <span class=\"o\">=</span> <span class=\"bp\">False</span></code></pre></div><p>下图演示了使用或不使用粘性操作运行时性能上的差异。虽然在某些情况下（Rainbow播放SPACE INVADERS）粘性动作似乎可以提高性能，但它们通常会降低性能。同时平均分数学习曲线（rainbow超越DQN）; 因此，并根据Machado等人提出的建议，多普胺默认启用粘滞动作。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-2800b897b7fc0dcfb4ccd99c45f2c1d6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1736\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb\" width=\"1736\" data-original=\"https://pic3.zhimg.com/v2-2800b897b7fc0dcfb4ccd99c45f2c1d6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1736&#39; height=&#39;606&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1736\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1736\" data-original=\"https://pic3.zhimg.com/v2-2800b897b7fc0dcfb4ccd99c45f2c1d6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-2800b897b7fc0dcfb4ccd99c45f2c1d6_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><hr/><h2><b>环境安装</b></h2><h2><b>Ubuntu</b></h2><p>1、设置初始化虚拟环境</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">sudo</span> <span class=\"n\">apt</span><span class=\"o\">-</span><span class=\"n\">get</span> <span class=\"n\">update</span> <span class=\"o\">&amp;&amp;</span> <span class=\"n\">sudo</span> <span class=\"n\">apt</span><span class=\"o\">-</span><span class=\"n\">get</span> <span class=\"n\">install</span> <span class=\"n\">virtualenv</span>\n<span class=\"n\">virtualenv</span> <span class=\"o\">--</span><span class=\"n\">python</span><span class=\"o\">=</span><span class=\"n\">python2</span><span class=\"o\">.</span><span class=\"mi\">7</span> <span class=\"n\">dopamine</span><span class=\"o\">-</span><span class=\"n\">env</span>\n<span class=\"n\">source</span> <span class=\"n\">dopamine</span><span class=\"o\">-</span><span class=\"n\">env</span><span class=\"o\">/</span><span class=\"nb\">bin</span><span class=\"o\">/</span><span class=\"n\">activate</span></code></pre></div><p>这将创建一个名为dopamine-env的目录，其中包含虚拟环境。 最后一个命令激活环境。</p><p>2、将依赖项安装到多巴胺。如果无法访问GPU，请在下面的行中将tensorflow-gpu替换为tensorflow（有关详细信息，请参阅Tensorflow说明）。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">sudo</span> <span class=\"n\">apt</span><span class=\"o\">-</span><span class=\"n\">get</span> <span class=\"n\">update</span> <span class=\"o\">&amp;&amp;</span> <span class=\"n\">sudo</span> <span class=\"n\">apt</span><span class=\"o\">-</span><span class=\"n\">get</span> <span class=\"n\">install</span> <span class=\"n\">cmake</span> <span class=\"n\">zlib1g</span><span class=\"o\">-</span><span class=\"n\">dev</span>\n<span class=\"n\">pip</span> <span class=\"n\">install</span> <span class=\"n\">absl</span><span class=\"o\">-</span><span class=\"n\">py</span> <span class=\"n\">atari</span><span class=\"o\">-</span><span class=\"n\">py</span> <span class=\"n\">gin</span><span class=\"o\">-</span><span class=\"n\">config</span> <span class=\"n\">gym</span> <span class=\"n\">opencv</span><span class=\"o\">-</span><span class=\"n\">python</span> <span class=\"n\">tensorflow</span><span class=\"o\">-</span><span class=\"n\">gpu</span></code></pre></div><blockquote> 注：在安装过程中，您可以安全地忽略以下错误消息：tensorflow 1.10.1要求numpy &lt;= 1.14.5，&gt; = 1.13.3，但是您将拥有不兼容的numpy 1.15.1。<br/> </blockquote><p>3、下载多巴胺源文件</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">git</span> <span class=\"n\">clone</span> <span class=\"n\">https</span><span class=\"p\">:</span><span class=\"o\">//</span><span class=\"n\">github</span><span class=\"o\">.</span><span class=\"n\">com</span><span class=\"o\">/</span><span class=\"n\">google</span><span class=\"o\">/</span><span class=\"n\">dopamine</span><span class=\"o\">.</span><span class=\"n\">git</span></code></pre></div><h2><b>Mac OS</b></h2><p>同样的道理，和以上以上的过程一样，代码为：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">pip</span> <span class=\"n\">install</span> <span class=\"n\">virtualenv</span>\n<span class=\"n\">virtualenv</span> <span class=\"o\">--</span><span class=\"n\">python</span><span class=\"o\">=</span><span class=\"n\">python2</span><span class=\"o\">.</span><span class=\"mi\">7</span> <span class=\"n\">dopamine</span><span class=\"o\">-</span><span class=\"n\">env</span>\n<span class=\"n\">source</span> <span class=\"n\">dopamine</span><span class=\"o\">-</span><span class=\"n\">env</span><span class=\"o\">/</span><span class=\"nb\">bin</span><span class=\"o\">/</span><span class=\"n\">activate</span>\n\n<span class=\"n\">brew</span> <span class=\"n\">install</span> <span class=\"n\">cmake</span> <span class=\"n\">zlib</span>\n<span class=\"n\">pip</span> <span class=\"n\">install</span> <span class=\"n\">absl</span><span class=\"o\">-</span><span class=\"n\">py</span> <span class=\"n\">atari</span><span class=\"o\">-</span><span class=\"n\">py</span> <span class=\"n\">gin</span><span class=\"o\">-</span><span class=\"n\">config</span> <span class=\"n\">gym</span> <span class=\"n\">opencv</span><span class=\"o\">-</span><span class=\"n\">python</span> <span class=\"n\">tensorflow</span>\n\n<span class=\"n\">git</span> <span class=\"n\">clone</span> <span class=\"n\">https</span><span class=\"p\">:</span><span class=\"o\">//</span><span class=\"n\">github</span><span class=\"o\">.</span><span class=\"n\">com</span><span class=\"o\">/</span><span class=\"n\">google</span><span class=\"o\">/</span><span class=\"n\">dopamine</span><span class=\"o\">.</span><span class=\"n\">git</span></code></pre></div><p>在获得源后便是测试初始化可以成功运行，通过以下命令：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">cd</span> <span class=\"n\">dopamine</span>\n<span class=\"n\">export</span> <span class=\"n\">PYTHONPATH</span><span class=\"o\">=</span><span class=\"err\">$</span><span class=\"p\">{</span><span class=\"n\">PYTHONPATH</span><span class=\"p\">}:</span><span class=\"o\">.</span>\n<span class=\"n\">python</span> <span class=\"n\">tests</span><span class=\"o\">/</span><span class=\"n\">atari_init_test</span><span class=\"o\">.</span><span class=\"n\">py</span></code></pre></div><p>标准Atari 2600实验的切入点是dopamine/atari/train.py。 要运行基本DQN代理，</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">python</span> <span class=\"o\">-</span><span class=\"n\">um</span> <span class=\"n\">dopamine</span><span class=\"o\">.</span><span class=\"n\">atari</span><span class=\"o\">.</span><span class=\"n\">train</span> \\\n  <span class=\"o\">--</span><span class=\"n\">agent_name</span><span class=\"o\">=</span><span class=\"n\">dqn</span> \\\n  <span class=\"o\">--</span><span class=\"n\">base_dir</span><span class=\"o\">=/</span><span class=\"n\">tmp</span><span class=\"o\">/</span><span class=\"n\">dopamine</span> \\\n  <span class=\"o\">--</span><span class=\"n\">gin_files</span><span class=\"o\">=</span><span class=\"s1\">&#39;dopamine/agents/dqn/configs/dqn.gin&#39;</span></code></pre></div><p>默认情况下，这将启动一个持续2亿帧的实验。 命令行界面将输出有关最新训练集的统计信息：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"p\">[</span><span class=\"o\">...</span><span class=\"p\">]</span>\n<span class=\"n\">I0824</span> <span class=\"mi\">17</span><span class=\"p\">:</span><span class=\"mi\">13</span><span class=\"p\">:</span><span class=\"mf\">33.078342</span> <span class=\"mi\">140196395337472</span> <span class=\"n\">tf_logging</span><span class=\"o\">.</span><span class=\"n\">py</span><span class=\"p\">:</span><span class=\"mi\">115</span><span class=\"p\">]</span> <span class=\"n\">gamma</span><span class=\"p\">:</span> <span class=\"mf\">0.990000</span>\n<span class=\"n\">I0824</span> <span class=\"mi\">17</span><span class=\"p\">:</span><span class=\"mi\">13</span><span class=\"p\">:</span><span class=\"mf\">33.795608</span> <span class=\"mi\">140196395337472</span> <span class=\"n\">tf_logging</span><span class=\"o\">.</span><span class=\"n\">py</span><span class=\"p\">:</span><span class=\"mi\">115</span><span class=\"p\">]</span> <span class=\"n\">Beginning</span> <span class=\"n\">training</span><span class=\"o\">...</span>\n<span class=\"n\">Steps</span> <span class=\"n\">executed</span><span class=\"p\">:</span> <span class=\"mi\">5903</span> <span class=\"n\">Episode</span> <span class=\"n\">length</span><span class=\"p\">:</span> <span class=\"mi\">1203</span> <span class=\"n\">Return</span><span class=\"p\">:</span> <span class=\"o\">-</span><span class=\"mf\">19.</span></code></pre></div><p>通常情况下，多巴胺通过gin配置是非常简单的</p><p>安装依赖包 安装多巴胺的一种简单的替代方法是作为Python库：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># Alternatively brew install, see Mac OS X instructions above.</span>\n<span class=\"n\">sudo</span> <span class=\"n\">apt</span><span class=\"o\">-</span><span class=\"n\">get</span> <span class=\"n\">update</span> <span class=\"o\">&amp;&amp;</span> <span class=\"n\">sudo</span> <span class=\"n\">apt</span><span class=\"o\">-</span><span class=\"n\">get</span> <span class=\"n\">install</span> <span class=\"n\">cmake</span>\n<span class=\"n\">pip</span> <span class=\"n\">install</span> <span class=\"n\">dopamine</span><span class=\"o\">-</span><span class=\"n\">rl</span>\n<span class=\"n\">pip</span> <span class=\"n\">install</span> <span class=\"n\">atari</span><span class=\"o\">-</span><span class=\"n\">py</span></code></pre></div><p>从root目录下，通过以下命令测试：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">python</span> <span class=\"o\">-</span><span class=\"n\">um</span> <span class=\"n\">tests</span><span class=\"o\">.</span><span class=\"n\">agents</span><span class=\"o\">.</span><span class=\"n\">rainbow</span><span class=\"o\">.</span><span class=\"n\">rainbow_agent_test</span></code></pre></div><hr/><h2><b>实例创建</b></h2><p>在本节中，将演示如何通过继承其中一个提供的代理来创建新代理。 此代码仅用于说明目的。 首先设置默认信息</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">DQNAgent</span><span class=\"o\">.</span><span class=\"n\">gamma</span> <span class=\"o\">=</span> <span class=\"mf\">0.99</span>  <span class=\"c1\"># 衰减系数</span>\n<span class=\"n\">DQNAgent</span><span class=\"o\">.</span><span class=\"n\">update_horizon</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>\n<span class=\"n\">DQNAgent</span><span class=\"o\">.</span><span class=\"n\">min_replay_history</span> <span class=\"o\">=</span> <span class=\"mi\">20000</span>  <span class=\"c1\"># agent steps</span>\n<span class=\"n\">DQNAgent</span><span class=\"o\">.</span><span class=\"n\">update_period</span> <span class=\"o\">=</span> <span class=\"mi\">4</span>\n<span class=\"n\">DQNAgent</span><span class=\"o\">.</span><span class=\"n\">target_update_period</span> <span class=\"o\">=</span> <span class=\"mi\">8000</span>  <span class=\"c1\"># agent steps</span>\n<span class=\"n\">DQNAgent</span><span class=\"o\">.</span><span class=\"n\">epsilon_train</span> <span class=\"o\">=</span> <span class=\"mf\">0.01</span>\n<span class=\"n\">DQNAgent</span><span class=\"o\">.</span><span class=\"n\">epsilon_eval</span> <span class=\"o\">=</span> <span class=\"mf\">0.001</span>\n<span class=\"n\">DQNAgent</span><span class=\"o\">.</span><span class=\"n\">epsilon_decay_period</span> <span class=\"o\">=</span> <span class=\"mi\">250000</span>  <span class=\"c1\"># agent steps</span>\n<span class=\"n\">DQNAgent</span><span class=\"o\">.</span><span class=\"n\">tf_device</span> <span class=\"o\">=</span> <span class=\"err\">’</span><span class=\"o\">/</span><span class=\"n\">gpu</span><span class=\"p\">:</span><span class=\"mi\">0</span><span class=\"err\">’</span>  <span class=\"c1\"># use ’/cpu:*’ for non-GPU version</span>\n<span class=\"n\">DQNAgent</span><span class=\"o\">.</span><span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"nd\">@tf.train.RMSPropOptimizer</span><span class=\"p\">()</span>\n<span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"o\">.</span><span class=\"n\">RMSPropOptimizer</span><span class=\"o\">.</span><span class=\"n\">learning_rate</span> <span class=\"o\">=</span> <span class=\"mf\">0.00025</span>\n<span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"o\">.</span><span class=\"n\">RMSPropOptimizer</span><span class=\"o\">.</span><span class=\"n\">decay</span> <span class=\"o\">=</span> <span class=\"mf\">0.95</span>\n<span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"o\">.</span><span class=\"n\">RMSPropOptimizer</span><span class=\"o\">.</span><span class=\"n\">momentum</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span>\n<span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"o\">.</span><span class=\"n\">RMSPropOptimizer</span><span class=\"o\">.</span><span class=\"n\">epsilon</span> <span class=\"o\">=</span> <span class=\"mf\">0.00001</span>\n<span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"o\">.</span><span class=\"n\">RMSPropOptimizer</span><span class=\"o\">.</span><span class=\"n\">centered</span> <span class=\"o\">=</span> <span class=\"bp\">True</span>\n<span class=\"n\">Runner</span><span class=\"o\">.</span><span class=\"n\">game_name</span> <span class=\"o\">=</span> <span class=\"err\">’</span><span class=\"n\">Pong</span><span class=\"err\">’</span>\n<span class=\"n\">Runner</span><span class=\"o\">.</span><span class=\"n\">sticky_actions</span> <span class=\"o\">=</span> <span class=\"bp\">True</span>\n<span class=\"n\">Runner</span><span class=\"o\">.</span><span class=\"n\">num_iterations</span> <span class=\"o\">=</span> <span class=\"mi\">200</span>\n<span class=\"n\">Runner</span><span class=\"o\">.</span><span class=\"n\">training_steps</span> <span class=\"o\">=</span> <span class=\"mi\">250000</span>  <span class=\"c1\"># agent steps</span>\n<span class=\"n\">Runner</span><span class=\"o\">.</span><span class=\"n\">evaluation_steps</span> <span class=\"o\">=</span> <span class=\"mi\">125000</span>  <span class=\"c1\"># agent steps</span>\n<span class=\"n\">Runner</span><span class=\"o\">.</span><span class=\"n\">max_steps_per_episode</span> <span class=\"o\">=</span> <span class=\"mi\">27000</span>  <span class=\"c1\"># agent steps</span>\n<span class=\"n\">WrappedReplayBuffer</span><span class=\"o\">.</span><span class=\"n\">replay_capacity</span> <span class=\"o\">=</span> <span class=\"mi\">1000000</span>\n<span class=\"n\">WrappedReplayBuffer</span><span class=\"o\">.</span><span class=\"n\">batch_size</span> <span class=\"o\">=</span> <span class=\"mi\">32</span></code></pre></div><p>继续开始代部分：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"kn\">as</span> <span class=\"nn\">np</span>\n<span class=\"kn\">import</span> <span class=\"nn\">os</span>\n<span class=\"kn\">from</span> <span class=\"nn\">dopamine.agents.dqn</span> <span class=\"kn\">import</span> <span class=\"n\">dqn_agent</span>\n<span class=\"kn\">from</span> <span class=\"nn\">dopamine.atari</span> <span class=\"kn\">import</span> <span class=\"n\">run_experiment</span>\n<span class=\"kn\">from</span> <span class=\"nn\">dopamine.colab</span> <span class=\"kn\">import</span> <span class=\"n\">utils</span> <span class=\"k\">as</span> <span class=\"n\">colab_utils</span>\n<span class=\"kn\">from</span> <span class=\"nn\">absl</span> <span class=\"kn\">import</span> <span class=\"n\">flags</span>\n<span class=\"n\">BASE_PATH</span> <span class=\"o\">=</span> <span class=\"err\">’</span><span class=\"o\">/</span><span class=\"n\">tmp</span><span class=\"o\">/</span><span class=\"n\">colab_dope_run</span><span class=\"err\">’</span>\n<span class=\"n\">GAME</span> <span class=\"o\">=</span> <span class=\"err\">’</span><span class=\"n\">Asterix</span><span class=\"err\">’</span>\n<span class=\"n\">LOG_PATH</span> <span class=\"o\">=</span> <span class=\"n\">os</span><span class=\"o\">.</span><span class=\"n\">path</span><span class=\"o\">.</span><span class=\"n\">join</span><span class=\"p\">(</span><span class=\"n\">BASE_PATH</span><span class=\"p\">,</span> <span class=\"err\">’</span><span class=\"n\">random_dqn</span><span class=\"err\">’</span><span class=\"p\">,</span> <span class=\"n\">GAME</span><span class=\"p\">)</span>\n<span class=\"k\">class</span> <span class=\"nc\">MyRandomDQNAgent</span><span class=\"p\">(</span><span class=\"n\">dqn_agent</span><span class=\"o\">.</span><span class=\"n\">DQNAgent</span><span class=\"p\">):</span>\n  <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">sess</span><span class=\"p\">,</span> <span class=\"n\">num_actions</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;This maintains all the DQN default argument values.&#34;&#34;&#34;</span>\n    <span class=\"nb\">super</span><span class=\"p\">(</span><span class=\"n\">MyRandomDQNAgent</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"n\">sess</span><span class=\"p\">,</span> <span class=\"n\">num_actions</span><span class=\"p\">)</span>\n  <span class=\"k\">def</span> <span class=\"nf\">step</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">reward</span><span class=\"p\">,</span> <span class=\"n\">observation</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;Calls the step function of the parent class, but returns a random\n</span><span class=\"s2\">    action.\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    <span class=\"n\">_</span> <span class=\"o\">=</span> <span class=\"nb\">super</span><span class=\"p\">(</span><span class=\"n\">MyRandomDQNAgent</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">step</span><span class=\"p\">(</span><span class=\"n\">reward</span><span class=\"p\">,</span> <span class=\"n\">observation</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randint</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">num_actions</span><span class=\"p\">)</span>\n  <span class=\"k\">def</span> <span class=\"nf\">create_random_dqn_agent</span><span class=\"p\">(</span><span class=\"n\">sess</span><span class=\"p\">,</span> <span class=\"n\">environment</span><span class=\"p\">):</span>\n  <span class=\"s2\">&#34;&#34;&#34;The Runner class will expect a function of this type to create an\n</span><span class=\"s2\">agent.&#34;&#34;&#34;</span>\n    <span class=\"k\">return</span> <span class=\"n\">MyRandomDQNAgent</span><span class=\"p\">(</span><span class=\"n\">sess</span><span class=\"p\">,</span> <span class=\"n\">num_actions</span><span class=\"o\">=</span><span class=\"n\">environment</span><span class=\"o\">.</span><span class=\"n\">action_space</span><span class=\"o\">.</span><span class=\"n\">n</span><span class=\"p\">)</span>\n<span class=\"c1\"># Create the runner class with this agent. We use very small numbers of</span>\n<span class=\"c1\">#    steps</span>\n<span class=\"c1\"># to terminate quickly, as this is mostly meant for demonstrating how one</span>\n<span class=\"c1\">#     can</span>\n<span class=\"c1\"># use the framework. We also explicitly terminate after 110 iterations (</span>\n<span class=\"c1\">#    instead</span>\n<span class=\"c1\"># of the standard 200) to demonstrate the plotting of partial runs.</span>\n<span class=\"n\">random_dqn_runner</span> <span class=\"o\">=</span> <span class=\"n\">run_experiment</span><span class=\"o\">.</span><span class=\"n\">Runner</span><span class=\"p\">(</span><span class=\"n\">LOG_PATH</span><span class=\"p\">,</span>\n                                          <span class=\"n\">create_random_dqn_agent</span><span class=\"p\">,</span>\n                                          <span class=\"n\">game_name</span><span class=\"o\">=</span><span class=\"n\">GAME</span><span class=\"p\">,</span>\n                                          <span class=\"n\">num_iterations</span><span class=\"o\">=</span><span class=\"mi\">200</span><span class=\"p\">,</span>\n                                          <span class=\"n\">training_steps</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">,</span>\n                                          <span class=\"n\">evaluation_steps</span><span class=\"o\">=</span><span class=\"mi\">10</span><span class=\"p\">,</span>\n                                          <span class=\"n\">max_steps_per_episode</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">)</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"err\">’</span><span class=\"n\">Will</span> <span class=\"n\">train</span> <span class=\"n\">agent</span><span class=\"p\">,</span> <span class=\"n\">please</span> <span class=\"n\">be</span> <span class=\"n\">patient</span><span class=\"p\">,</span> <span class=\"n\">may</span> <span class=\"n\">be</span> <span class=\"n\">a</span> <span class=\"k\">while</span><span class=\"o\">...</span><span class=\"err\">’</span><span class=\"p\">)</span>\n<span class=\"n\">random_dqn_runner</span><span class=\"o\">.</span><span class=\"n\">run_experiment</span><span class=\"p\">()</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"err\">’</span><span class=\"n\">Done</span> <span class=\"n\">training</span><span class=\"err\">!’</span><span class=\"p\">)</span></code></pre></div><p>此代码段提供了一个交互式显示，用户可以根据训练基线训练和可视化代理的性能，如下所示：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">import</span> <span class=\"nn\">seaborn</span> <span class=\"kn\">as</span> <span class=\"nn\">sns</span>\n<span class=\"kn\">import</span> <span class=\"nn\">matplotlib.pyplot</span> <span class=\"kn\">as</span> <span class=\"nn\">plt</span>\n<span class=\"n\">random_dqn_data</span> <span class=\"o\">=</span> <span class=\"n\">colab_utils</span><span class=\"o\">.</span><span class=\"n\">read_experiment</span><span class=\"p\">(</span><span class=\"n\">LOG_PATH</span><span class=\"p\">,</span> <span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n<span class=\"n\">random_dqn_data</span><span class=\"p\">[</span><span class=\"err\">’</span><span class=\"n\">agent</span><span class=\"err\">’</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"err\">’</span><span class=\"n\">MyRandomDQN</span><span class=\"err\">’</span>\n<span class=\"n\">random_dqn_data</span><span class=\"p\">[</span><span class=\"err\">’</span><span class=\"n\">run_number</span><span class=\"err\">’</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>\n<span class=\"n\">experimental_data</span><span class=\"p\">[</span><span class=\"n\">GAME</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">experimental_data</span><span class=\"p\">[</span><span class=\"n\">GAME</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">merge</span><span class=\"p\">(</span><span class=\"n\">random_dqn_data</span><span class=\"p\">,</span><span class=\"n\">fig</span><span class=\"p\">,</span> <span class=\"n\">ax</span> <span class=\"o\">=</span> <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplots</span><span class=\"p\">(</span><span class=\"n\">figsize</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">16</span><span class=\"p\">,</span><span class=\"mi\">8</span><span class=\"p\">))</span>\n<span class=\"n\">sns</span><span class=\"o\">.</span><span class=\"n\">tsplot</span><span class=\"p\">(</span><span class=\"n\">data</span><span class=\"o\">=</span><span class=\"n\">experimental_data</span><span class=\"p\">[</span><span class=\"n\">GAME</span><span class=\"p\">],</span> <span class=\"n\">time</span><span class=\"o\">=</span><span class=\"err\">’</span><span class=\"n\">iteration</span><span class=\"err\">’</span><span class=\"p\">,</span> <span class=\"n\">unit</span><span class=\"o\">=</span><span class=\"err\">’</span>\n    <span class=\"n\">run_number</span><span class=\"err\">’</span><span class=\"p\">,</span>\n           <span class=\"n\">condition</span><span class=\"o\">=</span><span class=\"err\">’</span><span class=\"n\">agent</span><span class=\"err\">’</span><span class=\"p\">,</span> <span class=\"n\">value</span><span class=\"o\">=</span><span class=\"err\">’</span><span class=\"n\">train_episode_returns</span><span class=\"err\">’</span><span class=\"p\">,</span> <span class=\"n\">ax</span><span class=\"o\">=</span><span class=\"n\">ax</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">title</span><span class=\"p\">(</span><span class=\"n\">GAME</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span></code></pre></div><blockquote> 注：其中import seabore as sns  是个可视化工具<br/> </blockquote><p>最终结果如图： </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-ddcad65be160432335e8bd2b40ce3055_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1738\" data-rawheight=\"892\" class=\"origin_image zh-lightbox-thumb\" width=\"1738\" data-original=\"https://pic2.zhimg.com/v2-ddcad65be160432335e8bd2b40ce3055_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1738&#39; height=&#39;892&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1738\" data-rawheight=\"892\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1738\" data-original=\"https://pic2.zhimg.com/v2-ddcad65be160432335e8bd2b40ce3055_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-ddcad65be160432335e8bd2b40ce3055_b.jpg\"/></figure><p>至此，以上是对多巴胺的部分解读，关于更多的c51等请自行阅读和安装。部分代码请学习github源码，受能力有限，文中难免有错误之处，还望大家多提意见、以便做的更好！</p><h2>参考文献：</h2><p>1、DOPAMINE: A RESEARCH FRAMEWORK FOR DEEP REINFORCEMENT LEARNING</p><p class=\"ztext-empty-paragraph\"><br/></p><p>另外</p><p>1、欢迎大家踊跃投稿--深度强化学习论文解读！</p><p>2、请尊重每一位创作者的汗水，转载请注明出处！</p>", 
            "topic": [
                {
                    "tag": "多巴胺", 
                    "tagLink": "https://api.zhihu.com/topics/19654434"
                }, 
                {
                    "tag": "强化学习 (Reinforcement Learning)", 
                    "tagLink": "https://api.zhihu.com/topics/20039099"
                }
            ], 
            "comments": []
        }
    ], 
    "url": "https://zhuanlan.zhihu.com/drl-paper"
}
