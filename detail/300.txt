{
    "title": "Spark", 
    "description": "分享Spark和大数据相关的经验", 
    "followers": [
        "https://www.zhihu.com/people/zhu-zhi-bi-bu-yu-hui-xu-wang", 
        "https://www.zhihu.com/people/NJU04", 
        "https://www.zhihu.com/people/xu-ren-he-88", 
        "https://www.zhihu.com/people/returning-youth", 
        "https://www.zhihu.com/people/li-xiao-gang-13-25", 
        "https://www.zhihu.com/people/yang-ping-fang-54", 
        "https://www.zhihu.com/people/he-jia-yuan-96", 
        "https://www.zhihu.com/people/kang-zi-jian-81", 
        "https://www.zhihu.com/people/nai-men-mo-er-77", 
        "https://www.zhihu.com/people/zzfnohell", 
        "https://www.zhihu.com/people/he-hui-hui-32", 
        "https://www.zhihu.com/people/HannanKan", 
        "https://www.zhihu.com/people/win-winfj2889", 
        "https://www.zhihu.com/people/jess10", 
        "https://www.zhihu.com/people/hwanji", 
        "https://www.zhihu.com/people/gameending", 
        "https://www.zhihu.com/people/hu-ke-70-4", 
        "https://www.zhihu.com/people/wu-shi-jun-97-31", 
        "https://www.zhihu.com/people/sara-liu-86", 
        "https://www.zhihu.com/people/da-xue-wu-hen-7", 
        "https://www.zhihu.com/people/chu-tian-75", 
        "https://www.zhihu.com/people/han-fang-yuan", 
        "https://www.zhihu.com/people/javafans", 
        "https://www.zhihu.com/people/kai-sa-er-94", 
        "https://www.zhihu.com/people/chen-jie-fang-59", 
        "https://www.zhihu.com/people/da-xin-xin-25-40", 
        "https://www.zhihu.com/people/zhi-zhe-71-44", 
        "https://www.zhihu.com/people/chen-qiang-12-59", 
        "https://www.zhihu.com/people/long-li-80", 
        "https://www.zhihu.com/people/landon-guo", 
        "https://www.zhihu.com/people/yimei-su-ren-14", 
        "https://www.zhihu.com/people/ben-ben-42-32", 
        "https://www.zhihu.com/people/rossi0128", 
        "https://www.zhihu.com/people/wang-zhi-67-39", 
        "https://www.zhihu.com/people/chang-men-76", 
        "https://www.zhihu.com/people/latch-shun-11", 
        "https://www.zhihu.com/people/liu-dao-xian-ren-67", 
        "https://www.zhihu.com/people/icloudate", 
        "https://www.zhihu.com/people/lu-gui-lin-32", 
        "https://www.zhihu.com/people/tomcat-zf", 
        "https://www.zhihu.com/people/wang-fan-ji-85", 
        "https://www.zhihu.com/people/li-an-zeng", 
        "https://www.zhihu.com/people/s6031207", 
        "https://www.zhihu.com/people/huang-ji-cong-86", 
        "https://www.zhihu.com/people/xuan-a-76-44", 
        "https://www.zhihu.com/people/zuo-liang-90", 
        "https://www.zhihu.com/people/kevin-59-27", 
        "https://www.zhihu.com/people/ma-li-jian-86-65", 
        "https://www.zhihu.com/people/wu-ji-15-56", 
        "https://www.zhihu.com/people/yu-yu-40-99-28", 
        "https://www.zhihu.com/people/wang-zhe-68-63", 
        "https://www.zhihu.com/people/lin-yong-hong-39", 
        "https://www.zhihu.com/people/jack-cheung-15", 
        "https://www.zhihu.com/people/wu-ming-30-7-29", 
        "https://www.zhihu.com/people/he-huan-35-38-15", 
        "https://www.zhihu.com/people/mu-zi-13-91-44", 
        "https://www.zhihu.com/people/jianxin136", 
        "https://www.zhihu.com/people/lei-xiao-yu-49", 
        "https://www.zhihu.com/people/scnu-arain", 
        "https://www.zhihu.com/people/li-xiao-jian-51-59", 
        "https://www.zhihu.com/people/wu-ming-20-70-71", 
        "https://www.zhihu.com/people/mo-mo-ha-ha-91", 
        "https://www.zhihu.com/people/xinjian-47", 
        "https://www.zhihu.com/people/da-cong-ban-dou-fu-89", 
        "https://www.zhihu.com/people/xuan-lu-80-62", 
        "https://www.zhihu.com/people/666677-13", 
        "https://www.zhihu.com/people/qwert-37-18", 
        "https://www.zhihu.com/people/wu-si-si-85-64", 
        "https://www.zhihu.com/people/dong-feng-zao-ji", 
        "https://www.zhihu.com/people/ben-pao-de-shu-shen", 
        "https://www.zhihu.com/people/yizhao-xiao-jin-lang-yan-po", 
        "https://www.zhihu.com/people/peng-xiao-chu-40", 
        "https://www.zhihu.com/people/wu-wei-long-63", 
        "https://www.zhihu.com/people/che-2-49", 
        "https://www.zhihu.com/people/zheng-jian-yang-56", 
        "https://www.zhihu.com/people/he-wen-hui-34", 
        "https://www.zhihu.com/people/hiwendi", 
        "https://www.zhihu.com/people/shi-dong-44-72", 
        "https://www.zhihu.com/people/rliao-45", 
        "https://www.zhihu.com/people/sun-wu-kong-5-18-83", 
        "https://www.zhihu.com/people/liushu-76", 
        "https://www.zhihu.com/people/jiang-xin-bi-xin-2-19", 
        "https://www.zhihu.com/people/hong-jiang-16", 
        "https://www.zhihu.com/people/wj2014-59", 
        "https://www.zhihu.com/people/qia-qia-nihen-shuai", 
        "https://www.zhihu.com/people/0413ajie", 
        "https://www.zhihu.com/people/aryiaa", 
        "https://www.zhihu.com/people/ji-yue-53", 
        "https://www.zhihu.com/people/dawinia-lo", 
        "https://www.zhihu.com/people/liu-yu-chen-66-65", 
        "https://www.zhihu.com/people/wang-wei-wei-92-15", 
        "https://www.zhihu.com/people/zhu-hai-wei-97", 
        "https://www.zhihu.com/people/hong-yan-nu-xia", 
        "https://www.zhihu.com/people/qian-qian-76-85", 
        "https://www.zhihu.com/people/tolkien", 
        "https://www.zhihu.com/people/zhang-jia-wang-38-94", 
        "https://www.zhihu.com/people/kuang-tao-77", 
        "https://www.zhihu.com/people/huang-none", 
        "https://www.zhihu.com/people/li-yu-zhu-99", 
        "https://www.zhihu.com/people/ji-jian-nan", 
        "https://www.zhihu.com/people/li-hao-34-94-13", 
        "https://www.zhihu.com/people/poemcode", 
        "https://www.zhihu.com/people/upcsjt", 
        "https://www.zhihu.com/people/wang-qifei", 
        "https://www.zhihu.com/people/tian-qi-rui-33", 
        "https://www.zhihu.com/people/xiao-peng-zi-57", 
        "https://www.zhihu.com/people/yao-bu-neng-ting-92-2", 
        "https://www.zhihu.com/people/chun-ri-shan-cheng", 
        "https://www.zhihu.com/people/lercy81", 
        "https://www.zhihu.com/people/sai-ba-si-wan-long", 
        "https://www.zhihu.com/people/xu-hua-qing-6", 
        "https://www.zhihu.com/people/jiang-nan-shui-yun", 
        "https://www.zhihu.com/people/an-ran-57", 
        "https://www.zhihu.com/people/summer-75-24-1", 
        "https://www.zhihu.com/people/coderex2522", 
        "https://www.zhihu.com/people/liu-zhuang-71-48", 
        "https://www.zhihu.com/people/zzz-7-43", 
        "https://www.zhihu.com/people/wang-xiao-78-41-62", 
        "https://www.zhihu.com/people/han-1-18", 
        "https://www.zhihu.com/people/hawhaw", 
        "https://www.zhihu.com/people/ape-naked", 
        "https://www.zhihu.com/people/videox", 
        "https://www.zhihu.com/people/wang-ye-27-20-50", 
        "https://www.zhihu.com/people/zhang-ming-feng-91", 
        "https://www.zhihu.com/people/liangwang-69", 
        "https://www.zhihu.com/people/xin-su-ru-jian-16-30", 
        "https://www.zhihu.com/people/huihui-89-62", 
        "https://www.zhihu.com/people/hello-world-71-32", 
        "https://www.zhihu.com/people/zhao-yi-22-57", 
        "https://www.zhihu.com/people/vyxf", 
        "https://www.zhihu.com/people/man-man-zou-xin-shang-a-3", 
        "https://www.zhihu.com/people/spark-69-14", 
        "https://www.zhihu.com/people/haotianhaq", 
        "https://www.zhihu.com/people/lain01", 
        "https://www.zhihu.com/people/ticktock-47", 
        "https://www.zhihu.com/people/zhong-robin", 
        "https://www.zhihu.com/people/xu-xiao-15-65", 
        "https://www.zhihu.com/people/guz-gegeroufv", 
        "https://www.zhihu.com/people/chen-chen-2-38-52", 
        "https://www.zhihu.com/people/jack-nee", 
        "https://www.zhihu.com/people/jerry-39-30-63", 
        "https://www.zhihu.com/people/chen-cheng-31-63", 
        "https://www.zhihu.com/people/zongy", 
        "https://www.zhihu.com/people/bai-zhi-wei", 
        "https://www.zhihu.com/people/data11", 
        "https://www.zhihu.com/people/lishiyu", 
        "https://www.zhihu.com/people/jing-gong-zhu-58", 
        "https://www.zhihu.com/people/orangleliu", 
        "https://www.zhihu.com/people/tinker-34-37", 
        "https://www.zhihu.com/people/willwinworld", 
        "https://www.zhihu.com/people/yu-zi-jun-49", 
        "https://www.zhihu.com/people/tian-xia-di-yishuai-9", 
        "https://www.zhihu.com/people/tenyun", 
        "https://www.zhihu.com/people/chi-chi-he-he-qiu-da-dao", 
        "https://www.zhihu.com/people/cai-gen-tan-sama", 
        "https://www.zhihu.com/people/lorraine-46-17", 
        "https://www.zhihu.com/people/wu-ji-81", 
        "https://www.zhihu.com/people/zha-zha-43-33", 
        "https://www.zhihu.com/people/aajjib", 
        "https://www.zhihu.com/people/li-tian-84", 
        "https://www.zhihu.com/people/bingo-69-48", 
        "https://www.zhihu.com/people/lmz-54-1", 
        "https://www.zhihu.com/people/deng-biao-8-2", 
        "https://www.zhihu.com/people/han-wu-ji-de-shi-dai", 
        "https://www.zhihu.com/people/lvcaihong", 
        "https://www.zhihu.com/people/whirlys", 
        "https://www.zhihu.com/people/yishan-xiao-he-shang-77", 
        "https://www.zhihu.com/people/nuan-feng-79-73", 
        "https://www.zhihu.com/people/xiao-yi-34-84", 
        "https://www.zhihu.com/people/lifzhe", 
        "https://www.zhihu.com/people/shui-mu-48-80", 
        "https://www.zhihu.com/people/xu-xiao-sheng-18", 
        "https://www.zhihu.com/people/frankccshen", 
        "https://www.zhihu.com/people/shurato-wang", 
        "https://www.zhihu.com/people/dsc-debian", 
        "https://www.zhihu.com/people/lan-shu-sama", 
        "https://www.zhihu.com/people/xiao-xiang-ri-kui-78", 
        "https://www.zhihu.com/people/bianliang", 
        "https://www.zhihu.com/people/li-ming-88-24", 
        "https://www.zhihu.com/people/dzhishi", 
        "https://www.zhihu.com/people/huang-xin-12-50", 
        "https://www.zhihu.com/people/island_", 
        "https://www.zhihu.com/people/he-lin-yu-lu-50", 
        "https://www.zhihu.com/people/smile-77-3", 
        "https://www.zhihu.com/people/li-yong-dong-55", 
        "https://www.zhihu.com/people/shi-wang-dan-bu-ma-mu", 
        "https://www.zhihu.com/people/huang-jia-88-83", 
        "https://www.zhihu.com/people/tang-di-zhi-hua-55-75", 
        "https://www.zhihu.com/people/mr-liu-80-72", 
        "https://www.zhihu.com/people/alix-8080", 
        "https://www.zhihu.com/people/iconsider", 
        "https://www.zhihu.com/people/liu-yong-69", 
        "https://www.zhihu.com/people/hua-kai-kan-zhe-zhi-xu-zhe-87-9", 
        "https://www.zhihu.com/people/xiaobeimi", 
        "https://www.zhihu.com/people/kuang-peng-30", 
        "https://www.zhihu.com/people/mo-ran-ru-chen", 
        "https://www.zhihu.com/people/tuscany", 
        "https://www.zhihu.com/people/kensuke-hinata", 
        "https://www.zhihu.com/people/LzyRapx", 
        "https://www.zhihu.com/people/yi-jiao-16", 
        "https://www.zhihu.com/people/clancy_liu", 
        "https://www.zhihu.com/people/feng-41", 
        "https://www.zhihu.com/people/chang-wen-29-18", 
        "https://www.zhihu.com/people/hai-kuo-tian-kong-34-71-58", 
        "https://www.zhihu.com/people/txhsj", 
        "https://www.zhihu.com/people/huangzheng", 
        "https://www.zhihu.com/people/liu-qi-qi-73-94", 
        "https://www.zhihu.com/people/she-liang", 
        "https://www.zhihu.com/people/bu-dai-yan-jing-18", 
        "https://www.zhihu.com/people/cai-niao-nu-li", 
        "https://www.zhihu.com/people/wayne-26-48", 
        "https://www.zhihu.com/people/huang-sheng-29-6", 
        "https://www.zhihu.com/people/cai-cai-zi-98", 
        "https://www.zhihu.com/people/cui-xiao-chen-82", 
        "https://www.zhihu.com/people/zhan-jian-guo-67", 
        "https://www.zhihu.com/people/kevin-hill", 
        "https://www.zhihu.com/people/min-zh-24", 
        "https://www.zhihu.com/people/wei-han-88", 
        "https://www.zhihu.com/people/ji-ling-72", 
        "https://www.zhihu.com/people/jin-yuan-32-36", 
        "https://www.zhihu.com/people/dong-fang-dan-82", 
        "https://www.zhihu.com/people/BruceWDZ", 
        "https://www.zhihu.com/people/chen-kun-1994", 
        "https://www.zhihu.com/people/liu-fei-94-95", 
        "https://www.zhihu.com/people/lan-lan-lan-lan-96-82", 
        "https://www.zhihu.com/people/zhu-jia-94-76", 
        "https://www.zhihu.com/people/xia-li-qiao", 
        "https://www.zhihu.com/people/li-peng-3-74", 
        "https://www.zhihu.com/people/yin-zuo-jie", 
        "https://www.zhihu.com/people/di-cheng-39-71"
    ], 
    "article": [
        {
            "url": "https://zhuanlan.zhihu.com/p/76016941", 
            "userName": "董可伦", 
            "userLink": "https://www.zhihu.com/people/de888cc734ddbfd918269a8e5fb196ba", 
            "upvote": 2, 
            "title": "Spark 通过 spark-submit 设置日志级别", 
            "content": "<p></p><p>我的原创地址：</p><a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/11/16/sparkSubmitLogLevel/\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">dongkelun.com/2018/11/1</span><span class=\"invisible\">6/sparkSubmitLogLevel/</span><span class=\"ellipsis\"></span></a><h2>前言</h2><p>Spark有多种方式设置日志级别，这次主要记录一下如何在spark-submit设置Spark的日志级别。</p><h2>1、需求</h2><p>因为Spark的日志级别默认为INFO(log4j.rootCategory=INFO, console),这样在运行程序的时候有很多我不需要的日志信息都打印出来了，看起来比较乱，比较烦，抓不住重点，而我只想把warn和error打印出来。 之前在测试环境或者在eclipse我是通过其他几种方式（下面会介绍）设置的，但是在生产环境下不允许我修改集群的配置文件（不是我负责~），而在代码里设置日志级别却不生效（原因还没找到），最后通过spark-submit里设置日志级别搞定的。</p><h2>2、spark-submit 设置</h2><div class=\"highlight\"><pre><code class=\"language-bash\">spark-submit --conf <span class=\"s2\">&#34;spark.driver.extraJavaOptions=-Dlog4j.configuration=file:log4j.properties&#34;</span></code></pre></div><p>其中log4j.properties为我将本地的日志文件，拷贝到执行spark-submit的机器上 参考：<a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/xueba207/article/details/50436684\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">blog.csdn.net/xueba207/</span><span class=\"invisible\">article/details/50436684</span><span class=\"ellipsis\"></span></a> </p><h2>3、其他几种设置方法</h2><h3>3.1 修改集群的配置文件</h3><div class=\"highlight\"><pre><code class=\"language-bash\"><span class=\"nb\">cd</span> <span class=\"nv\">$SPARK_HOME</span>/conf \ncp log4j.properties.template log4j.properties\nvim log4j.properties</code></pre></div><p>将log4j.rootCategory=INFO, console改为log4j.rootCategory=WARN, console</p><h3>3.2 在Eclipse里设置</h3><p>将log4j.properties放在项目的src/main/resources即可 * Spark 默认日志文件：org/apache/spark/log4j-defaults.properties</p><h3>3.3 代码里配置（未生效）</h3><div class=\"highlight\"><pre><code class=\"language-text\">spark.sparkContext.setLogLevel(&#34;WARN&#34;)</code></pre></div><ul><li>在代码里设置，不生效原因未知 </li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-62b19f99f0e7ef3374a78469eee3a006_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1864\" data-rawheight=\"887\" class=\"origin_image zh-lightbox-thumb\" width=\"1864\" data-original=\"https://pic3.zhimg.com/v2-62b19f99f0e7ef3374a78469eee3a006_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1864&#39; height=&#39;887&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1864\" data-rawheight=\"887\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1864\" data-original=\"https://pic3.zhimg.com/v2-62b19f99f0e7ef3374a78469eee3a006_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-62b19f99f0e7ef3374a78469eee3a006_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>4、 总结</h2><ul><li>1、如果在自己的测试集群上，直接修改$SPARK_HOME/conf下的log4j.properties即可</li><li>2、如果在Eclipse里，将log4j.properties放在项目的src/main/resources即可</li><li>3、如果在生产环境的集群，又不允许修改配置文件的话，用上面讲的spark-submit --conf 即可</li></ul>", 
            "topic": [
                {
                    "tag": "Spark", 
                    "tagLink": "https://api.zhihu.com/topics/19942170"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/54328102", 
            "userName": "董可伦", 
            "userLink": "https://www.zhihu.com/people/de888cc734ddbfd918269a8e5fb196ba", 
            "upvote": 2, 
            "title": "Spark性能优化：基于分区进行操作", 
            "content": "<p>我的原创地址：</p><a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/09/02/sparkMapPartitions/\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic4.zhimg.com/v2-9eda5a2579f355ba882830d161526de3_180x120.jpg\" data-image-width=\"900\" data-image-height=\"704\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Spark性能优化：基于分区进行操作</a><p class=\"ztext-empty-paragraph\"><br/></p><h2>前言（摘自Spark快速大数据分析）</h2><p>基于分区对数据进行操作可以让我们避免为每个数据元素进行重复的配置工作。诸如打开数据库连接或创建随机数生成器等操作，都是我们应当尽量避免为每个元素都配置一次的工作。Spark 提供基于分区的map 和foreach，让你的部分代码只对RDD 的每个分区运行一次，这样可以帮助降低这些操作的代价。 当基于分区操作RDD 时，Spark 会为函数提供该分区中的元素的迭代器。返回值方面，也返回一个迭代器。除mapPartitions() 外，Spark 还有一些别的基于分区的操作符，见下表：</p><p>函数名 | 调用所提供的 | 返回的 | 对于RDD[T]的函数签名  - | :-:  | :-:  | :-:<br/> mapPartitions() |  该分区中元素的迭代器   |  返回的元素的迭代器    |  f: (Iterator[T]) → Iterator[U]  mapPartitionsWithIndex()    |  分区序号，以及每个分区中的元素的迭代器  |  返回的元素的迭代器    |  f: (Int, Iterator[T]) → Iterator[U] foreachPartitions()      |  元素迭代器   |  无    |  f: (Iterator[T]) → Unit <br/> </p><p>首先给出上面三个算子的具体代码示例。</p><h2>1、mapPartitions</h2><p>与map类似，不同点是map是对RDD的里的每一个元素进行操作，而mapPartitions是对每一个分区的数据（迭代器）进行操作，具体可以看上面的表格。 下面同时用map和mapPartitions实现WordCount，看一下mapPartitions的用法以及与map的区别</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">package</span> <span class=\"nn\">com.dkl.leanring.spark.test</span>\n\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.SparkSession</span>\n\n<span class=\"k\">object</span> <span class=\"nc\">WordCount</span> <span class=\"o\">{</span>\n  <span class=\"k\">def</span> <span class=\"n\">main</span><span class=\"o\">(</span><span class=\"n\">args</span><span class=\"k\">:</span> <span class=\"kt\">Array</span><span class=\"o\">[</span><span class=\"kt\">String</span><span class=\"o\">])</span><span class=\"k\">:</span> <span class=\"kt\">Unit</span> <span class=\"o\">=</span> <span class=\"o\">{</span>\n    <span class=\"k\">val</span> <span class=\"n\">spark</span> <span class=\"k\">=</span> <span class=\"nc\">SparkSession</span><span class=\"o\">.</span><span class=\"n\">builder</span><span class=\"o\">().</span><span class=\"n\">master</span><span class=\"o\">(</span><span class=\"s\">&#34;local&#34;</span><span class=\"o\">).</span><span class=\"n\">appName</span><span class=\"o\">(</span><span class=\"s\">&#34;WordCount&#34;</span><span class=\"o\">).</span><span class=\"n\">getOrCreate</span><span class=\"o\">()</span>\n    <span class=\"k\">val</span> <span class=\"n\">sc</span> <span class=\"k\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">sparkContext</span>\n\n    <span class=\"k\">val</span> <span class=\"n\">input</span> <span class=\"k\">=</span> <span class=\"n\">sc</span><span class=\"o\">.</span><span class=\"n\">parallelize</span><span class=\"o\">(</span><span class=\"nc\">Seq</span><span class=\"o\">(</span><span class=\"s\">&#34;Spark Hive Kafka&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;Hadoop Kafka Hive Hbase&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;Java Scala Spark&#34;</span><span class=\"o\">))</span>\n    <span class=\"k\">val</span> <span class=\"n\">words</span> <span class=\"k\">=</span> <span class=\"n\">input</span><span class=\"o\">.</span><span class=\"n\">flatMap</span><span class=\"o\">(</span><span class=\"n\">line</span> <span class=\"k\">=&gt;</span> <span class=\"n\">line</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"o\">(</span><span class=\"s\">&#34; &#34;</span><span class=\"o\">))</span>\n    <span class=\"k\">val</span> <span class=\"n\">counts</span> <span class=\"k\">=</span> <span class=\"n\">words</span><span class=\"o\">.</span><span class=\"n\">map</span><span class=\"o\">(</span><span class=\"n\">word</span> <span class=\"k\">=&gt;</span> <span class=\"o\">(</span><span class=\"n\">word</span><span class=\"o\">,</span> <span class=\"mi\">1</span><span class=\"o\">)).</span><span class=\"n\">reduceByKey</span> <span class=\"o\">{</span> <span class=\"o\">(</span><span class=\"n\">x</span><span class=\"o\">,</span> <span class=\"n\">y</span><span class=\"o\">)</span> <span class=\"k\">=&gt;</span> <span class=\"n\">x</span> <span class=\"o\">+</span> <span class=\"n\">y</span> <span class=\"o\">}</span>\n    <span class=\"n\">println</span><span class=\"o\">(</span><span class=\"n\">counts</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"o\">().</span><span class=\"n\">mkString</span><span class=\"o\">(</span><span class=\"s\">&#34;,&#34;</span><span class=\"o\">))</span>\n    <span class=\"k\">val</span> <span class=\"n\">counts1</span> <span class=\"k\">=</span> <span class=\"n\">words</span><span class=\"o\">.</span><span class=\"n\">mapPartitions</span><span class=\"o\">(</span><span class=\"n\">it</span> <span class=\"k\">=&gt;</span> <span class=\"n\">it</span><span class=\"o\">.</span><span class=\"n\">map</span><span class=\"o\">(</span><span class=\"n\">word</span> <span class=\"k\">=&gt;</span> <span class=\"o\">(</span><span class=\"n\">word</span><span class=\"o\">,</span> <span class=\"mi\">1</span><span class=\"o\">))).</span><span class=\"n\">reduceByKey</span> <span class=\"o\">{</span> <span class=\"o\">(</span><span class=\"n\">x</span><span class=\"o\">,</span> <span class=\"n\">y</span><span class=\"o\">)</span> <span class=\"k\">=&gt;</span> <span class=\"n\">x</span> <span class=\"o\">+</span> <span class=\"n\">y</span> <span class=\"o\">}</span>\n    <span class=\"n\">println</span><span class=\"o\">(</span><span class=\"n\">counts1</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"o\">().</span><span class=\"n\">mkString</span><span class=\"o\">(</span><span class=\"s\">&#34;,&#34;</span><span class=\"o\">))</span>\n\n    <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">stop</span><span class=\"o\">()</span>\n\n  <span class=\"o\">}</span>\n<span class=\"o\">}</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-fcbb2e4b4b8b93b1c80821291b40a001_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1450\" data-rawheight=\"605\" class=\"origin_image zh-lightbox-thumb\" width=\"1450\" data-original=\"https://pic2.zhimg.com/v2-fcbb2e4b4b8b93b1c80821291b40a001_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1450&#39; height=&#39;605&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1450\" data-rawheight=\"605\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1450\" data-original=\"https://pic2.zhimg.com/v2-fcbb2e4b4b8b93b1c80821291b40a001_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-fcbb2e4b4b8b93b1c80821291b40a001_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>2、mapPartitionsWithIndex</h2><p>和mapPartitions一样，只是多了一个分区的序号，下面的代码实现了将Rdd的元素数字n变为(分区序号,n*n)</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">val</span> <span class=\"n\">rdd</span> <span class=\"k\">=</span> <span class=\"n\">sc</span><span class=\"o\">.</span><span class=\"n\">parallelize</span><span class=\"o\">(</span><span class=\"mi\">1</span> <span class=\"n\">to</span> <span class=\"mi\">10</span><span class=\"o\">,</span> <span class=\"mi\">5</span><span class=\"o\">)</span>\n<span class=\"k\">val</span> <span class=\"n\">res</span> <span class=\"k\">=</span> <span class=\"n\">rdd</span><span class=\"o\">.</span><span class=\"n\">mapPartitionsWithIndex</span><span class=\"o\">((</span><span class=\"n\">index</span><span class=\"o\">,</span> <span class=\"n\">it</span><span class=\"o\">)</span> <span class=\"k\">=&gt;</span> <span class=\"o\">{</span>\n  <span class=\"n\">it</span><span class=\"o\">.</span><span class=\"n\">map</span><span class=\"o\">(</span><span class=\"n\">n</span> <span class=\"k\">=&gt;</span> <span class=\"o\">(</span><span class=\"n\">index</span><span class=\"o\">,</span> <span class=\"n\">n</span> <span class=\"o\">*</span> <span class=\"n\">n</span><span class=\"o\">))</span>\n<span class=\"o\">})</span>\n<span class=\"n\">println</span><span class=\"o\">(</span><span class=\"n\">res</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"o\">().</span><span class=\"n\">mkString</span><span class=\"o\">(</span><span class=\"s\">&#34; &#34;</span><span class=\"o\">))</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-da686274aca0573149d294c02cb6b7d8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"900\" data-rawheight=\"187\" class=\"origin_image zh-lightbox-thumb\" width=\"900\" data-original=\"https://pic1.zhimg.com/v2-da686274aca0573149d294c02cb6b7d8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;900&#39; height=&#39;187&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"900\" data-rawheight=\"187\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"900\" data-original=\"https://pic1.zhimg.com/v2-da686274aca0573149d294c02cb6b7d8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-da686274aca0573149d294c02cb6b7d8_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>3、foreachPartitions</h2><p>foreachPartitions和foreach类似，不同点也是foreachPartitions基于分区进行操作的</p><div class=\"highlight\"><pre><code class=\"language-text\">rdd.foreachPartition(it =&gt; it.foreach(println))</code></pre></div><h2>4、关于如何避免重复配置</h2><p>下面以打开数据库连接举例，需求是这样的： 读取mysql表里的数据，做了一系列数据处理得到结果之后，需要修改我们mysql表里的每一条数据的状态，代表程序已经处理过了，下次不需要处理了。</p><h2>4.1 表</h2><p>以最简单表结构示例</p><p>字段名 | 注释 |  - | :-:<br/> ID | 主键、唯一标识 ISDEAL | 程序是否处理过</p><p>建表语句</p><div class=\"highlight\"><pre><code class=\"language-sql\"><span class=\"k\">CREATE</span> <span class=\"k\">TABLE</span> <span class=\"n\">test</span> <span class=\"p\">(</span>\n    <span class=\"n\">id</span> <span class=\"nb\">INTEGER</span> <span class=\"k\">NOT</span> <span class=\"k\">NULL</span> <span class=\"n\">AUTO_INCREMENT</span><span class=\"p\">,</span>\n    <span class=\"n\">isdeal</span> <span class=\"nb\">INTEGER</span> <span class=\"k\">DEFAULT</span> <span class=\"mi\">0</span> <span class=\"k\">NOT</span> <span class=\"k\">NULL</span><span class=\"p\">,</span>\n    <span class=\"k\">primary</span> <span class=\"k\">key</span><span class=\"p\">(</span><span class=\"n\">id</span><span class=\"p\">)</span> \n<span class=\"p\">)</span>\n<span class=\"n\">ENGINE</span><span class=\"o\">=</span><span class=\"n\">InnoDB</span>\n<span class=\"k\">DEFAULT</span> <span class=\"n\">CHARSET</span><span class=\"o\">=</span><span class=\"n\">utf8</span>\n<span class=\"k\">COLLATE</span><span class=\"o\">=</span><span class=\"n\">utf8_general_ci</span><span class=\"p\">;</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ce71838e1a67611092ce1056f682a4fc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"900\" data-rawheight=\"752\" class=\"origin_image zh-lightbox-thumb\" width=\"900\" data-original=\"https://pic1.zhimg.com/v2-ce71838e1a67611092ce1056f682a4fc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;900&#39; height=&#39;752&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"900\" data-rawheight=\"752\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"900\" data-original=\"https://pic1.zhimg.com/v2-ce71838e1a67611092ce1056f682a4fc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-ce71838e1a67611092ce1056f682a4fc_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>4.2 不基于分区操作</h2><p>一共用两种方法</p><h2>4.2.1 第一种</h2><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">package</span> <span class=\"nn\">com.dkl.leanring.spark.sql.mysql</span>\n\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.SparkSession</span>\n\n<span class=\"k\">object</span> <span class=\"nc\">UpdateMysqlDemo</span> <span class=\"o\">{</span>\n  <span class=\"k\">def</span> <span class=\"n\">main</span><span class=\"o\">(</span><span class=\"n\">args</span><span class=\"k\">:</span> <span class=\"kt\">Array</span><span class=\"o\">[</span><span class=\"kt\">String</span><span class=\"o\">])</span><span class=\"k\">:</span> <span class=\"kt\">Unit</span> <span class=\"o\">=</span> <span class=\"o\">{</span>\n    <span class=\"k\">val</span> <span class=\"n\">spark</span> <span class=\"k\">=</span> <span class=\"nc\">SparkSession</span><span class=\"o\">.</span><span class=\"n\">builder</span><span class=\"o\">().</span><span class=\"n\">appName</span><span class=\"o\">(</span><span class=\"s\">&#34;UpdateMysqlDemo&#34;</span><span class=\"o\">).</span><span class=\"n\">master</span><span class=\"o\">(</span><span class=\"s\">&#34;local&#34;</span><span class=\"o\">).</span><span class=\"n\">getOrCreate</span><span class=\"o\">()</span>\n\n    <span class=\"k\">val</span> <span class=\"n\">database_url</span> <span class=\"k\">=</span> <span class=\"s\">&#34;jdbc:mysql://192.168.44.128:3306/test?useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=false&#34;</span>\n    <span class=\"k\">val</span> <span class=\"n\">user</span> <span class=\"k\">=</span> <span class=\"s\">&#34;root&#34;</span>\n    <span class=\"k\">val</span> <span class=\"n\">password</span> <span class=\"k\">=</span> <span class=\"s\">&#34;Root-123456&#34;</span>\n    <span class=\"k\">val</span> <span class=\"n\">df</span> <span class=\"k\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">read</span>\n      <span class=\"o\">.</span><span class=\"n\">format</span><span class=\"o\">(</span><span class=\"s\">&#34;jdbc&#34;</span><span class=\"o\">)</span>\n      <span class=\"o\">.</span><span class=\"n\">option</span><span class=\"o\">(</span><span class=\"s\">&#34;url&#34;</span><span class=\"o\">,</span> <span class=\"n\">database_url</span><span class=\"o\">)</span>\n      <span class=\"o\">.</span><span class=\"n\">option</span><span class=\"o\">(</span><span class=\"s\">&#34;dbtable&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;(select * from test where isDeal=0 limit 5)a&#34;</span><span class=\"o\">)</span>\n      <span class=\"o\">.</span><span class=\"n\">option</span><span class=\"o\">(</span><span class=\"s\">&#34;user&#34;</span><span class=\"o\">,</span> <span class=\"n\">user</span><span class=\"o\">)</span>\n      <span class=\"o\">.</span><span class=\"n\">option</span><span class=\"o\">(</span><span class=\"s\">&#34;password&#34;</span><span class=\"o\">,</span> <span class=\"n\">password</span><span class=\"o\">)</span>\n      <span class=\"o\">.</span><span class=\"n\">option</span><span class=\"o\">(</span><span class=\"s\">&#34;driver&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;com.mysql.jdbc.Driver&#34;</span><span class=\"o\">)</span>\n      <span class=\"o\">.</span><span class=\"n\">option</span><span class=\"o\">(</span><span class=\"s\">&#34;numPartitions&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;5&#34;</span><span class=\"o\">)</span>\n      <span class=\"o\">.</span><span class=\"n\">option</span><span class=\"o\">(</span><span class=\"s\">&#34;partitionColumn&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;ID&#34;</span><span class=\"o\">)</span>\n      <span class=\"o\">.</span><span class=\"n\">option</span><span class=\"o\">(</span><span class=\"s\">&#34;lowerBound&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;1&#34;</span><span class=\"o\">)</span>\n      <span class=\"o\">.</span><span class=\"n\">option</span><span class=\"o\">(</span><span class=\"s\">&#34;upperBound&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;10&#34;</span><span class=\"o\">)</span>\n      <span class=\"o\">.</span><span class=\"n\">load</span><span class=\"o\">()</span>\n\n    <span class=\"k\">import</span> <span class=\"nn\">java.sql.</span><span class=\"o\">{</span> <span class=\"nc\">Connection</span><span class=\"o\">,</span> <span class=\"nc\">DriverManager</span><span class=\"o\">,</span> <span class=\"nc\">ResultSet</span> <span class=\"o\">};</span>\n    <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">rdd</span><span class=\"o\">.</span><span class=\"n\">foreach</span><span class=\"o\">(</span><span class=\"n\">row</span> <span class=\"k\">=&gt;</span> <span class=\"o\">{</span>\n      <span class=\"k\">val</span> <span class=\"n\">conn</span> <span class=\"k\">=</span> <span class=\"nc\">DriverManager</span><span class=\"o\">.</span><span class=\"n\">getConnection</span><span class=\"o\">(</span><span class=\"n\">database_url</span><span class=\"o\">,</span> <span class=\"n\">user</span><span class=\"o\">,</span> <span class=\"n\">password</span><span class=\"o\">)</span>\n      <span class=\"k\">try</span> <span class=\"o\">{</span>\n        <span class=\"c1\">// Configure to be Read Only\n</span><span class=\"c1\"></span>        <span class=\"k\">val</span> <span class=\"n\">statement</span> <span class=\"k\">=</span> <span class=\"n\">conn</span><span class=\"o\">.</span><span class=\"n\">createStatement</span><span class=\"o\">(</span><span class=\"nc\">ResultSet</span><span class=\"o\">.</span><span class=\"nc\">TYPE_FORWARD_ONLY</span><span class=\"o\">,</span> <span class=\"nc\">ResultSet</span><span class=\"o\">.</span><span class=\"nc\">CONCUR_READ_ONLY</span><span class=\"o\">)</span>\n        <span class=\"k\">val</span> <span class=\"n\">prep</span> <span class=\"k\">=</span> <span class=\"n\">conn</span><span class=\"o\">.</span><span class=\"n\">prepareStatement</span><span class=\"o\">(</span><span class=\"s\">s&#34;update test set isDeal=1 where id=?&#34;</span><span class=\"o\">)</span>\n\n        <span class=\"k\">val</span> <span class=\"n\">id</span> <span class=\"k\">=</span> <span class=\"n\">row</span><span class=\"o\">.</span><span class=\"n\">getAs</span><span class=\"o\">[</span><span class=\"kt\">Int</span><span class=\"o\">](</span><span class=\"s\">&#34;id&#34;</span><span class=\"o\">)</span>\n        <span class=\"n\">prep</span><span class=\"o\">.</span><span class=\"n\">setInt</span><span class=\"o\">(</span><span class=\"mi\">1</span><span class=\"o\">,</span> <span class=\"n\">id</span><span class=\"o\">)</span>\n        <span class=\"n\">prep</span><span class=\"o\">.</span><span class=\"n\">executeUpdate</span>\n\n      <span class=\"o\">}</span> <span class=\"k\">catch</span> <span class=\"o\">{</span>\n        <span class=\"k\">case</span> <span class=\"n\">e</span><span class=\"k\">:</span> <span class=\"kt\">Exception</span> <span class=\"o\">=&gt;</span> <span class=\"n\">e</span><span class=\"o\">.</span><span class=\"n\">printStackTrace</span>\n      <span class=\"o\">}</span> <span class=\"k\">finally</span> <span class=\"o\">{</span>\n        <span class=\"n\">conn</span><span class=\"o\">.</span><span class=\"n\">close</span><span class=\"o\">()</span>\n      <span class=\"o\">}</span>\n\n    <span class=\"o\">})</span>\n\n    <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">stop</span><span class=\"o\">()</span>\n  <span class=\"o\">}</span>\n<span class=\"o\">}</span></code></pre></div><ul><li>上面的代码，取isDeal=0的前五条，因为造的数据量少，所以只取了前五条，然后指定了五个分区，这里只是一个代码示例，实际工作中应该数据量很大，每个分区肯定不止一条数据</li></ul><p>根据上面的代码，看到用这种方式的缺点是每一个元素都要创建一个数据库连接，这样频繁创建连接、关闭连接，在数据量很大的情况下，势必会对性能产生影响，但是优点是不用担心内存不够。</p><h2>4.2.2 第二种</h2><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">val</span> <span class=\"n\">conn</span> <span class=\"k\">=</span> <span class=\"nc\">DriverManager</span><span class=\"o\">.</span><span class=\"n\">getConnection</span><span class=\"o\">(</span><span class=\"n\">database_url</span><span class=\"o\">,</span> <span class=\"n\">user</span><span class=\"o\">,</span> <span class=\"n\">password</span><span class=\"o\">)</span>\n<span class=\"k\">try</span> <span class=\"o\">{</span>\n  <span class=\"k\">val</span> <span class=\"n\">statement</span> <span class=\"k\">=</span> <span class=\"n\">conn</span><span class=\"o\">.</span><span class=\"n\">createStatement</span><span class=\"o\">(</span><span class=\"nc\">ResultSet</span><span class=\"o\">.</span><span class=\"nc\">TYPE_FORWARD_ONLY</span><span class=\"o\">,</span> <span class=\"nc\">ResultSet</span><span class=\"o\">.</span><span class=\"nc\">CONCUR_READ_ONLY</span><span class=\"o\">)</span>\n  <span class=\"k\">val</span> <span class=\"n\">prep</span> <span class=\"k\">=</span> <span class=\"n\">conn</span><span class=\"o\">.</span><span class=\"n\">prepareStatement</span><span class=\"o\">(</span><span class=\"s\">s&#34;update test set isDeal=1 where id=?&#34;</span><span class=\"o\">)</span>\n\n  <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"o\">(</span><span class=\"s\">&#34;id&#34;</span><span class=\"o\">).</span><span class=\"n\">collect</span><span class=\"o\">().</span><span class=\"n\">foreach</span><span class=\"o\">(</span><span class=\"n\">row</span> <span class=\"k\">=&gt;</span> <span class=\"o\">{</span>\n    <span class=\"k\">val</span> <span class=\"n\">id</span> <span class=\"k\">=</span> <span class=\"n\">row</span><span class=\"o\">.</span><span class=\"n\">getAs</span><span class=\"o\">[</span><span class=\"kt\">Int</span><span class=\"o\">](</span><span class=\"s\">&#34;id&#34;</span><span class=\"o\">)</span>\n    <span class=\"n\">prep</span><span class=\"o\">.</span><span class=\"n\">setInt</span><span class=\"o\">(</span><span class=\"mi\">1</span><span class=\"o\">,</span> <span class=\"n\">id</span><span class=\"o\">)</span>\n    <span class=\"n\">prep</span><span class=\"o\">.</span><span class=\"n\">executeUpdate</span>\n\n <span class=\"o\">})</span>\n\n<span class=\"o\">}</span> <span class=\"k\">catch</span> <span class=\"o\">{</span>\n  <span class=\"k\">case</span> <span class=\"n\">e</span><span class=\"k\">:</span> <span class=\"kt\">Exception</span> <span class=\"o\">=&gt;</span> <span class=\"n\">e</span><span class=\"o\">.</span><span class=\"n\">printStackTrace</span>\n<span class=\"o\">}</span></code></pre></div><p>这种方式的缺点是把要操作的数据全部转成scala数组，仅在Driver端执行，但是如果数据量很大的话，可能因为Driver内存不够大而抛出异常，优点是只建立一次数据库连接，在数据量不是特别大，且确定Driver的内存足够的时候，可以采取这种方式。</p><h2>4.3 基于分区的方式</h2><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">rdd</span><span class=\"o\">.</span><span class=\"n\">foreachPartition</span><span class=\"o\">(</span><span class=\"n\">it</span> <span class=\"k\">=&gt;</span> <span class=\"o\">{</span>\n  <span class=\"k\">val</span> <span class=\"n\">conn</span> <span class=\"k\">=</span> <span class=\"nc\">DriverManager</span><span class=\"o\">.</span><span class=\"n\">getConnection</span><span class=\"o\">(</span><span class=\"n\">database_url</span><span class=\"o\">,</span> <span class=\"n\">user</span><span class=\"o\">,</span> <span class=\"n\">password</span><span class=\"o\">)</span>\n  <span class=\"k\">try</span> <span class=\"o\">{</span>\n    <span class=\"k\">val</span> <span class=\"n\">statement</span> <span class=\"k\">=</span> <span class=\"n\">conn</span><span class=\"o\">.</span><span class=\"n\">createStatement</span><span class=\"o\">(</span><span class=\"nc\">ResultSet</span><span class=\"o\">.</span><span class=\"nc\">TYPE_FORWARD_ONLY</span><span class=\"o\">,</span> <span class=\"nc\">ResultSet</span><span class=\"o\">.</span><span class=\"nc\">CONCUR_READ_ONLY</span><span class=\"o\">)</span>\n    <span class=\"k\">val</span> <span class=\"n\">prep</span> <span class=\"k\">=</span> <span class=\"n\">conn</span><span class=\"o\">.</span><span class=\"n\">prepareStatement</span><span class=\"o\">(</span><span class=\"s\">s&#34;update test set isDeal=1 where id=?&#34;</span><span class=\"o\">)</span>\n    <span class=\"n\">it</span><span class=\"o\">.</span><span class=\"n\">foreach</span><span class=\"o\">(</span><span class=\"n\">row</span> <span class=\"k\">=&gt;</span> <span class=\"o\">{</span>\n      <span class=\"k\">val</span> <span class=\"n\">id</span> <span class=\"k\">=</span> <span class=\"n\">row</span><span class=\"o\">.</span><span class=\"n\">getAs</span><span class=\"o\">[</span><span class=\"kt\">Int</span><span class=\"o\">](</span><span class=\"s\">&#34;id&#34;</span><span class=\"o\">)</span>\n      <span class=\"n\">prep</span><span class=\"o\">.</span><span class=\"n\">setInt</span><span class=\"o\">(</span><span class=\"mi\">1</span><span class=\"o\">,</span> <span class=\"n\">id</span><span class=\"o\">)</span>\n      <span class=\"n\">prep</span><span class=\"o\">.</span><span class=\"n\">executeUpdate</span>\n    <span class=\"o\">})</span>\n\n  <span class=\"o\">}</span> <span class=\"k\">catch</span> <span class=\"o\">{</span>\n    <span class=\"k\">case</span> <span class=\"n\">e</span><span class=\"k\">:</span> <span class=\"kt\">Exception</span> <span class=\"o\">=&gt;</span> <span class=\"n\">e</span><span class=\"o\">.</span><span class=\"n\">printStackTrace</span>\n  <span class=\"o\">}</span> <span class=\"k\">finally</span> <span class=\"o\">{</span>\n    <span class=\"n\">conn</span><span class=\"o\">.</span><span class=\"n\">close</span><span class=\"o\">()</span>\n  <span class=\"o\">}</span>\n\n<span class=\"o\">})</span></code></pre></div><p>这种方式就结合了上面两种方式的优点，基于分区的方式使得创建连接的次数不会那么多，然后每个分区的数据也可以平均分到每个节点的executor上，避免了内存不足产生的异常，当然前提是要合理的分配分区数，既不能让分区数太多，也不能让每个分区的数据太多，还有要注意数据倾斜的问题，因为当数据倾斜造成某个分区数据量太大同样造成OOM（内存溢出）。</p><h2>4.4 其他</h2><p>上面只是列举了一个例子，且只是在foreach这样的action算子里体现的，当然肯定也有需求需是在transformation里进行如数据库的连接这样的操作，大家可类比的使用mapPartitions即可</p><h2>5、其他优点（未证实）</h2><p>网上有很多博客提到mapPartitions还有其他优点，就是mapPartitions比map快，性能高，原因是因为map的function会执行rdd.count次，而mapPartitions的function则执行rdd.numPartitions次。 但我并这么认为，因mapPartitions的function和map的function是不一样的，mapPartitions里的迭代器的每个元素还是都要执行一遍的，实际上也是执行rdd.count次。 下面以其中一篇博客举例（只列出优点，大部分博客上的写的都一样的，应该出自同一篇博客吧~） </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-126e1cb04e87f25c882092733aa2296c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"900\" data-rawheight=\"704\" class=\"origin_image zh-lightbox-thumb\" width=\"900\" data-original=\"https://pic1.zhimg.com/v2-126e1cb04e87f25c882092733aa2296c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;900&#39; height=&#39;704&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"900\" data-rawheight=\"704\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"900\" data-original=\"https://pic1.zhimg.com/v2-126e1cb04e87f25c882092733aa2296c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-126e1cb04e87f25c882092733aa2296c_b.jpg\"/></figure><p> 博客地址：<a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/tian_qing_lei/article/details/77940504\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Spark---算子调优之MapPartitions提升Map类操作性能</a></p><ul><li>至于mapPartitions是否真的比map处理速度快,如果我有时间验证得到结果的话，我再更新一下这个地方~</li></ul><h2>相关阅读</h2><ul><li><a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/03/21/sparkMysql/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Spark Sql 连接mysql</a></li><li><a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/08/13/sparkDefaultPartitionNums/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Spark 创建RDD、DataFrame各种情况的默认分区数</a></li></ul>", 
            "topic": [
                {
                    "tag": "Spark", 
                    "tagLink": "https://api.zhihu.com/topics/19942170"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/52840928", 
            "userName": "java架构师", 
            "userLink": "https://www.zhihu.com/people/d046409c3a516c2640c38bdf143a247d", 
            "upvote": 4, 
            "title": "Hive系列之HSQL转换成MapReduce过程", 
            "content": "<p>hive的库、表等数据实际是hdfs系统中的目录和文件，让开发者可以通过sql语句， 像操作关系数据库一样操作文件内容， 比如执行查询，统计，插入等操作。一直很好奇hive是如何做到这些的。</p><p>hive的整体架构图如下所示， compiler部分负责把HiveSQL转换成MapReduce任务。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-64e6bf09897b832b94505c1e2f0df863_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"381\" data-rawheight=\"382\" class=\"content_image\" width=\"381\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;381&#39; height=&#39;382&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"381\" data-rawheight=\"382\" class=\"content_image lazy\" width=\"381\" data-actualsrc=\"https://pic4.zhimg.com/v2-64e6bf09897b832b94505c1e2f0df863_b.jpg\"/></figure><p>基本转换步骤</p><p>hiveSQL转换成MapReduce的执行计划包括如下几个步骤：</p><p>HiveSQL -&gt;AST(抽象语法树) -&gt; QB(查询块) -&gt;OperatorTree（操作树）-&gt;优化后的操作树-&gt;mapreduce任务树-&gt;优化后的mapreduce任务树</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-3750a6409bc17ee1d3b6397b7549ccf6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"591\" data-rawheight=\"329\" class=\"origin_image zh-lightbox-thumb\" width=\"591\" data-original=\"https://pic3.zhimg.com/v2-3750a6409bc17ee1d3b6397b7549ccf6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;591&#39; height=&#39;329&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"591\" data-rawheight=\"329\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"591\" data-original=\"https://pic3.zhimg.com/v2-3750a6409bc17ee1d3b6397b7549ccf6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-3750a6409bc17ee1d3b6397b7549ccf6_b.jpg\"/></figure><p>SQL Parser：Antlr定义SQL的语法规则，完成SQL词法，语法解析，将SQL转化为抽象 语法树AST Tree；<br/>Semantic Analyzer：遍历AST Tree，抽象出查询的基本组成单元QueryBlock；<br/>Logical plan：遍历QueryBlock，翻译为执行操作树OperatorTree；<br/>Logical plan optimizer: 逻辑层优化器进行OperatorTree变换，合并不必要的ReduceSinkOperator，减少shuffle数据量；<br/>Physical plan：遍历OperatorTree，翻译为MapReduce任务；<br/>Logical plan optimizer：物理层优化器进行MapReduce任务的变换，生成最终的执行计划；<br/> step1: SQL Parser<br/>如下图所示， sql语句可以解析为三个部分<br/>AST中第一个部分对应SQL语句中FROM access_log_hbase a JOIN product_hbase p ON (a.prono=p.prono)。<br/>insert overwrite table对应第二部分。<br/>select a.user, a.prono, p.maker, p.price对应第三部分。<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-819e5a7218dfe5b5e83e77cda2cfed0f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"615\" data-rawheight=\"463\" class=\"origin_image zh-lightbox-thumb\" width=\"615\" data-original=\"https://pic4.zhimg.com/v2-819e5a7218dfe5b5e83e77cda2cfed0f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;615&#39; height=&#39;463&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"615\" data-rawheight=\"463\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"615\" data-original=\"https://pic4.zhimg.com/v2-819e5a7218dfe5b5e83e77cda2cfed0f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-819e5a7218dfe5b5e83e77cda2cfed0f_b.jpg\"/></figure><p>step2: Semantic Analyzer<br/>这个步骤把AST转换成基本的查询块QB，如下图所示<br/>QB的对象包括如下属性：<br/>aliasToTabs:保存表格别名的信息<br/>aliasToSubq:保存子查询的信息<br/>qbm：保存每个输入表的元信息，比如表在HDFS上的路径，保存表数据的文件格式等<br/> QBParseInfo对象包括如下属性：<br/>joinExpr: 保存TOK_JOIN节点信息<br/>destToxx:保存输出和各个操作的ASTNode节点的对应关系。<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-3fc5b85031b859c3546001c216dbc782_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"746\" data-rawheight=\"356\" class=\"origin_image zh-lightbox-thumb\" width=\"746\" data-original=\"https://pic3.zhimg.com/v2-3fc5b85031b859c3546001c216dbc782_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;746&#39; height=&#39;356&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"746\" data-rawheight=\"356\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"746\" data-original=\"https://pic3.zhimg.com/v2-3fc5b85031b859c3546001c216dbc782_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-3fc5b85031b859c3546001c216dbc782_b.jpg\"/></figure><p>如下图中， 表格别名a, p保存到aliasTotabs, 分别对应“access_log_hbase&#34;, &#34; product_hbase&#34;。</p><p>TOK_JOIN信息保存到ParseInfo对象:joinExpr</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-ed450bb25e5f32da6e1b475b2ad1fac9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"627\" data-rawheight=\"494\" class=\"origin_image zh-lightbox-thumb\" width=\"627\" data-original=\"https://pic2.zhimg.com/v2-ed450bb25e5f32da6e1b475b2ad1fac9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;627&#39; height=&#39;494&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"627\" data-rawheight=\"494\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"627\" data-original=\"https://pic2.zhimg.com/v2-ed450bb25e5f32da6e1b475b2ad1fac9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-ed450bb25e5f32da6e1b475b2ad1fac9_b.jpg\"/></figure><p>下图所示，TOK_DESTINATION节点保存到nameToDest属性中。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-0394f6e24edc29c914298af8cfcba5e3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"627\" data-rawheight=\"468\" class=\"origin_image zh-lightbox-thumb\" width=\"627\" data-original=\"https://pic4.zhimg.com/v2-0394f6e24edc29c914298af8cfcba5e3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;627&#39; height=&#39;468&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"627\" data-rawheight=\"468\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"627\" data-original=\"https://pic4.zhimg.com/v2-0394f6e24edc29c914298af8cfcba5e3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-0394f6e24edc29c914298af8cfcba5e3_b.jpg\"/></figure><p>step3:Logical Plan<br/>该步骤是把查询块QB转换操作树。<br/>操作树基本的操作符包括TableScanOperator，SelectOperator，FilterOperator，JoinOperator，GroupByOperator，ReduceSinkOperator。<br/>TableScanOperator： 扫描数据表中数据，从原表中取数据。<br/>JoinOperator完成Join操作。<br/>FilterOperator完成过滤操作， 对应sql里面的where语句功能<br/>ReduceSinkOperator：标志着Hive Map阶段的结束， Reduce阶段的开始。<br/>SelectOperator：reduce阶段输出select中的列<br/>FileSinkOperator: 生成结果数据到输出文件。<br/> 从两个输入表格中读入数据， 用operator树表示为两个TableScanOperator节点<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-46961ab0228c30c4acbd670027cf4aa0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"614\" data-rawheight=\"469\" class=\"origin_image zh-lightbox-thumb\" width=\"614\" data-original=\"https://pic1.zhimg.com/v2-46961ab0228c30c4acbd670027cf4aa0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;614&#39; height=&#39;469&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"614\" data-rawheight=\"469\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"614\" data-original=\"https://pic1.zhimg.com/v2-46961ab0228c30c4acbd670027cf4aa0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-46961ab0228c30c4acbd670027cf4aa0_b.jpg\"/></figure><p>Join放在reduce阶段执行， 执行join节点前，加入两个ReduceSinkOperator节点，表示当前map阶段结束， 进入到reduce阶段。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-28dcd4a0d7bf3c25acdb6850bf2a9850_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"622\" data-rawheight=\"467\" class=\"origin_image zh-lightbox-thumb\" width=\"622\" data-original=\"https://pic1.zhimg.com/v2-28dcd4a0d7bf3c25acdb6850bf2a9850_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;622&#39; height=&#39;467&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"622\" data-rawheight=\"467\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"622\" data-original=\"https://pic1.zhimg.com/v2-28dcd4a0d7bf3c25acdb6850bf2a9850_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-28dcd4a0d7bf3c25acdb6850bf2a9850_b.jpg\"/></figure><p>selectoperator节点，从reduce节点获取select指定的列值。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-2f59ef101d828a78d8e090754ec209bb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"615\" data-rawheight=\"460\" class=\"origin_image zh-lightbox-thumb\" width=\"615\" data-original=\"https://pic4.zhimg.com/v2-2f59ef101d828a78d8e090754ec209bb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;615&#39; height=&#39;460&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"615\" data-rawheight=\"460\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"615\" data-original=\"https://pic4.zhimg.com/v2-2f59ef101d828a78d8e090754ec209bb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-2f59ef101d828a78d8e090754ec209bb_b.jpg\"/></figure><p>nameToDest ASTNode节点，转换为FileSinkOperator节点， 把结果写入到目标文件。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-712f4b4c771f1ea59c78e3ce8d2ac8df_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"607\" data-rawheight=\"459\" class=\"origin_image zh-lightbox-thumb\" width=\"607\" data-original=\"https://pic4.zhimg.com/v2-712f4b4c771f1ea59c78e3ce8d2ac8df_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;607&#39; height=&#39;459&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"607\" data-rawheight=\"459\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"607\" data-original=\"https://pic4.zhimg.com/v2-712f4b4c771f1ea59c78e3ce8d2ac8df_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-712f4b4c771f1ea59c78e3ce8d2ac8df_b.jpg\"/></figure><p>通过上面几个转换步骤， 最终生成的logical计划树。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-b8f6c50546115af9c25995c331192556_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"615\" data-rawheight=\"486\" class=\"origin_image zh-lightbox-thumb\" width=\"615\" data-original=\"https://pic3.zhimg.com/v2-b8f6c50546115af9c25995c331192556_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;615&#39; height=&#39;486&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"615\" data-rawheight=\"486\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"615\" data-original=\"https://pic3.zhimg.com/v2-b8f6c50546115af9c25995c331192556_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-b8f6c50546115af9c25995c331192556_b.jpg\"/></figure><p>logical plan tree还可以通过logical plan optimizer进一步优化， 优化完成的逻辑优化树还有转换成物理执行计划和物理执行计划优化。本文不做详细介绍， 后续有时间再补充。<br/> PS: 查看hive sql编译后的执行计划<br/>hive&gt; explain select * from tablename;</p><p><br/>感兴趣可以加Java架构师群获取Java工程化、高性能及分布式、高性能、深入浅出。高架构。性能调优、Spring，MyBatis，Netty源码分析和大数据等多个知识点高级进阶干货的直播免费学习权限 都是大牛带飞 让你少走很多的弯路的 群..号是：855801563 对了 小白勿进 最好是有开发经验</p><p>注：加群要求</p><p>1、具有工作经验的，面对目前流行的技术不知从何下手，需要突破技术瓶颈的可以加。</p><p>2、在公司待久了，过得很安逸，但跳槽时面试碰壁。需要在短时间内进修、跳槽拿高薪的可以加。</p><p>3、如果没有工作经验，但基础非常扎实，对java工作机制，常用设计思想，常用java开发框架掌握熟练的，可以加。</p><p>4、觉得自己很牛B，一般需求都能搞定。但是所学的知识点没有系统化，很难在技术领域继续突破的可以加。</p><p>5.阿里Java高级大牛直播讲解知识点，分享知识，多年工作经验的梳理和总结，带着大家全面、科学地建立自己的技术体系和技术认知！</p>", 
            "topic": [
                {
                    "tag": "Kafka", 
                    "tagLink": "https://api.zhihu.com/topics/20012159"
                }, 
                {
                    "tag": "Spark", 
                    "tagLink": "https://api.zhihu.com/topics/19942170"
                }, 
                {
                    "tag": "数据", 
                    "tagLink": "https://api.zhihu.com/topics/19554449"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/52385273", 
            "userName": "董可伦", 
            "userLink": "https://www.zhihu.com/people/de888cc734ddbfd918269a8e5fb196ba", 
            "upvote": 5, 
            "title": "利用Spark实现Oracle到Hive的历史数据同步", 
            "content": "<p>我的原创地址：</p><a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/08/27/sparkOracle2Hive/\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic2.zhimg.com/v2-efeb858e8cde527992b2f00ea6ba2ad5_180x120.jpg\" data-image-width=\"1200\" data-image-height=\"653\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">利用Spark实现Oracle到Hive的历史数据同步</a><h2>1、需求背景</h2><p>和上一篇文章<a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/08/20/sparkDfAddComments/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Spark通过修改DataFrame的schema给表字段添加注释</a>一样，通过Spark将关系型数据库（以Oracle为例）的表同步的Hive，这里讲的只是同步历史数据，不包括同步增量数据。</p><h2>2、Oracle和Hive的字段类型对应</h2><p>利用Spark的字段类型自动匹配，本来以为Spark匹配的不是很好，只是简单的判断一下是否为数字、字符串，结果经验证，Spark可以获取到Oracle的小数点精度，Spark的字段类型对应和我自己整理的差不多，所以就索性用Spark自带的字段类型匹配，而不是自己去Oracle相关表获取每个字段类型，然后一一转化为Hive对应的字段类型，下面是Oracle和Hive的字段类型对应，只是整理了大概：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-55b62bc6e3bf9e660e51207b2bdaeae6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"991\" data-rawheight=\"468\" class=\"origin_image zh-lightbox-thumb\" width=\"991\" data-original=\"https://pic3.zhimg.com/v2-55b62bc6e3bf9e660e51207b2bdaeae6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;991&#39; height=&#39;468&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"991\" data-rawheight=\"468\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"991\" data-original=\"https://pic3.zhimg.com/v2-55b62bc6e3bf9e660e51207b2bdaeae6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-55b62bc6e3bf9e660e51207b2bdaeae6_b.jpg\"/></figure><h2>2.1 看一下Spark字段类型对应</h2><p>首先建一张包含大部分字段类型的Oracle表</p><div class=\"highlight\"><pre><code class=\"language-sql\"><span class=\"k\">CREATE</span> <span class=\"k\">TABLE</span> <span class=\"n\">TEST</span> <span class=\"p\">(</span>\n    <span class=\"n\">COL1</span> <span class=\"n\">VARCHAR2</span><span class=\"p\">(</span><span class=\"mi\">25</span><span class=\"p\">),</span>\n    <span class=\"n\">COL2</span> <span class=\"n\">NVARCHAR2</span><span class=\"p\">(</span><span class=\"mi\">18</span><span class=\"p\">),</span>\n    <span class=\"n\">COL3</span> <span class=\"nb\">INTEGER</span><span class=\"p\">,</span>\n    <span class=\"n\">COL4</span> <span class=\"nb\">NUMBER</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">,</span><span class=\"mi\">4</span><span class=\"p\">),</span>\n    <span class=\"n\">COL5</span> <span class=\"nb\">NUMBER</span><span class=\"p\">(</span><span class=\"mi\">30</span><span class=\"p\">,</span><span class=\"mi\">7</span><span class=\"p\">),</span>\n    <span class=\"n\">COL6</span> <span class=\"nb\">NUMBER</span><span class=\"p\">,</span>\n    <span class=\"n\">COL7</span> <span class=\"nb\">DATE</span><span class=\"p\">,</span>\n    <span class=\"n\">COL8</span> <span class=\"k\">TIMESTAMP</span><span class=\"p\">,</span>\n    <span class=\"n\">COL9</span> <span class=\"nb\">CHAR</span><span class=\"p\">(</span><span class=\"mi\">30</span><span class=\"p\">),</span>\n    <span class=\"n\">COL10</span> <span class=\"k\">CLOB</span><span class=\"p\">,</span>\n    <span class=\"n\">COL11</span> <span class=\"nb\">BLOB</span><span class=\"p\">,</span>\n    <span class=\"n\">COL12</span> <span class=\"n\">RAW</span><span class=\"p\">(</span><span class=\"mi\">12</span><span class=\"p\">)</span>\n<span class=\"p\">)</span> <span class=\"p\">;</span>\n<span class=\"k\">COMMENT</span> <span class=\"k\">ON</span> <span class=\"k\">COLUMN</span> <span class=\"n\">TEST</span><span class=\"p\">.</span><span class=\"n\">COL2</span> <span class=\"k\">IS</span> <span class=\"s1\">&#39;注释2&#39;</span> <span class=\"p\">;</span>\n<span class=\"k\">COMMENT</span> <span class=\"k\">ON</span> <span class=\"k\">COLUMN</span> <span class=\"n\">TEST</span><span class=\"p\">.</span><span class=\"n\">COL7</span> <span class=\"k\">IS</span> <span class=\"s1\">&#39;注释7&#39;</span> <span class=\"p\">;</span>\n<span class=\"k\">COMMENT</span> <span class=\"k\">ON</span> <span class=\"k\">COLUMN</span> <span class=\"n\">TEST</span><span class=\"p\">.</span><span class=\"n\">COL10</span> <span class=\"k\">IS</span> <span class=\"s1\">&#39;注释10&#39;</span> <span class=\"p\">;</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ce384133e8407586240141cefa8d019b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1430\" data-rawheight=\"388\" class=\"origin_image zh-lightbox-thumb\" width=\"1430\" data-original=\"https://pic4.zhimg.com/v2-ce384133e8407586240141cefa8d019b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1430&#39; height=&#39;388&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1430\" data-rawheight=\"388\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1430\" data-original=\"https://pic4.zhimg.com/v2-ce384133e8407586240141cefa8d019b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-ce384133e8407586240141cefa8d019b_b.jpg\"/></figure><p> 然后用Spark打印一下获取到的字段类型。 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-0f2a62fdd6da6237b275c42a8c23ed1b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1350\" data-rawheight=\"866\" class=\"origin_image zh-lightbox-thumb\" width=\"1350\" data-original=\"https://pic4.zhimg.com/v2-0f2a62fdd6da6237b275c42a8c23ed1b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1350&#39; height=&#39;866&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1350\" data-rawheight=\"866\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1350\" data-original=\"https://pic4.zhimg.com/v2-0f2a62fdd6da6237b275c42a8c23ed1b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-0f2a62fdd6da6237b275c42a8c23ed1b_b.jpg\"/></figure><p> 可以看到Spark成功的完成上述表格的字段类型转化，小数的精度和是否为空都可以获取到，但是不完美的一点是没有将NUMBER标度为零的转换为Int,而还是以DECIMAL(38,0)的形式表示，虽然都是表示的整数，但是在后面Spark读取hive的时候，还需要将DECIMAL转为Int。</p><h2>2.2 按需修改字段类型对应</h2><p>以上面讲的将DECIMAL(38,0)转为Int为例: 先尝试通过修改schema实现</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.types._</span>\n<span class=\"k\">val</span> <span class=\"n\">schema</span> <span class=\"k\">=</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">schema</span><span class=\"o\">.</span><span class=\"n\">map</span><span class=\"o\">(</span><span class=\"n\">s</span> <span class=\"k\">=&gt;</span> <span class=\"o\">{</span>\n  <span class=\"k\">if</span> <span class=\"o\">(</span><span class=\"n\">s</span><span class=\"o\">.</span><span class=\"n\">dataType</span><span class=\"o\">.</span><span class=\"n\">equals</span><span class=\"o\">(</span><span class=\"nc\">DecimalType</span><span class=\"o\">(</span><span class=\"mi\">38</span><span class=\"o\">,</span> <span class=\"mi\">0</span><span class=\"o\">)))</span> <span class=\"o\">{</span>\n    <span class=\"k\">new</span> <span class=\"nc\">StructField</span><span class=\"o\">(</span><span class=\"n\">s</span><span class=\"o\">.</span><span class=\"n\">name</span><span class=\"o\">,</span> <span class=\"nc\">IntegerType</span><span class=\"o\">,</span> <span class=\"n\">s</span><span class=\"o\">.</span><span class=\"n\">nullable</span><span class=\"o\">)</span>\n  <span class=\"o\">}</span> <span class=\"k\">else</span> <span class=\"o\">{</span>\n    <span class=\"n\">s</span>\n  <span class=\"o\">}</span>\n<span class=\"o\">})</span>\n<span class=\"c1\">//根据添加了注释的schema，新建DataFrame\n</span><span class=\"c1\"></span><span class=\"k\">val</span> <span class=\"n\">new_df</span> <span class=\"k\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"o\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">rdd</span><span class=\"o\">,</span> <span class=\"nc\">StructType</span><span class=\"o\">(</span><span class=\"n\">schema</span><span class=\"o\">)).</span><span class=\"n\">repartition</span><span class=\"o\">(</span><span class=\"mi\">160</span><span class=\"o\">)</span>\n<span class=\"n\">new_df</span><span class=\"o\">.</span><span class=\"n\">printSchema</span><span class=\"o\">()</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-fb027a4851fcceca77f338586733f2b7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1350\" data-rawheight=\"887\" class=\"origin_image zh-lightbox-thumb\" width=\"1350\" data-original=\"https://pic4.zhimg.com/v2-fb027a4851fcceca77f338586733f2b7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1350&#39; height=&#39;887&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1350\" data-rawheight=\"887\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1350\" data-original=\"https://pic4.zhimg.com/v2-fb027a4851fcceca77f338586733f2b7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-fb027a4851fcceca77f338586733f2b7_b.jpg\"/></figure><p> 可以看到，已经成功的将COL3的字段转为了Int。但是这样构造的DataFrame是不能用的，如执行new_df.show会报如下错误：</p><div class=\"highlight\"><pre><code class=\"language-vim\"><span class=\"nx\">java</span>.<span class=\"nx\">lang</span>.<span class=\"nx\">RuntimeException</span><span class=\"p\">:</span> <span class=\"nx\">Error</span> <span class=\"nx\">while</span> <span class=\"nx\">encoding</span><span class=\"p\">:</span> <span class=\"nx\">java</span>.<span class=\"nx\">lang</span>.<span class=\"nx\">RuntimeException</span><span class=\"p\">:</span> <span class=\"nx\">java</span>.<span class=\"nx\">math</span>.<span class=\"nx\">BigDecimal</span> <span class=\"nx\">is</span> <span class=\"nx\">not</span> <span class=\"nx\">a</span> <span class=\"nx\">valid</span> <span class=\"nx\">external</span> <span class=\"nx\">type</span> <span class=\"nx\">for</span> <span class=\"nx\">schema</span> <span class=\"nx\">of</span> <span class=\"nx\">int</span></code></pre></div><p>原因是rdd的数据类型和schema的数据类型不匹配。 最后可以通过如下方式实现：</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">import</span> <span class=\"nn\">scala.collection.mutable.ArrayBuffer</span>\n<span class=\"c1\">//需要转换的列名\n</span><span class=\"c1\"></span><span class=\"k\">val</span> <span class=\"n\">colName</span> <span class=\"k\">=</span> <span class=\"nc\">ArrayBuffer</span><span class=\"o\">[</span><span class=\"kt\">String</span><span class=\"o\">]()</span>\n<span class=\"k\">val</span> <span class=\"n\">schema</span> <span class=\"k\">=</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">schema</span><span class=\"o\">.</span><span class=\"n\">foreach</span><span class=\"o\">(</span><span class=\"n\">s</span> <span class=\"k\">=&gt;</span> <span class=\"o\">{</span>\n  <span class=\"k\">if</span> <span class=\"o\">(</span><span class=\"n\">s</span><span class=\"o\">.</span><span class=\"n\">dataType</span><span class=\"o\">.</span><span class=\"n\">equals</span><span class=\"o\">(</span><span class=\"nc\">DecimalType</span><span class=\"o\">(</span><span class=\"mi\">38</span><span class=\"o\">,</span> <span class=\"mi\">0</span><span class=\"o\">)))</span> <span class=\"o\">{</span>\n    <span class=\"n\">colName</span> <span class=\"o\">+=</span> <span class=\"n\">s</span><span class=\"o\">.</span><span class=\"n\">name</span>\n  <span class=\"o\">}</span>\n<span class=\"o\">})</span>\n\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.functions._</span>\n<span class=\"k\">var</span> <span class=\"n\">df_int</span> <span class=\"k\">=</span> <span class=\"n\">df</span>\n<span class=\"n\">colName</span><span class=\"o\">.</span><span class=\"n\">foreach</span><span class=\"o\">(</span><span class=\"n\">name</span> <span class=\"k\">=&gt;</span> <span class=\"o\">{</span>\n  <span class=\"n\">df_int</span> <span class=\"k\">=</span> <span class=\"n\">df_int</span><span class=\"o\">.</span><span class=\"n\">withColumn</span><span class=\"o\">(</span><span class=\"n\">name</span><span class=\"o\">,</span> <span class=\"n\">col</span><span class=\"o\">(</span><span class=\"n\">name</span><span class=\"o\">).</span><span class=\"n\">cast</span><span class=\"o\">(</span><span class=\"nc\">IntegerType</span><span class=\"o\">))</span>\n<span class=\"o\">})</span>\n<span class=\"n\">df_int</span><span class=\"o\">.</span><span class=\"n\">printSchema</span><span class=\"o\">()</span>\n<span class=\"n\">df_int</span><span class=\"o\">.</span><span class=\"n\">show</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-ae597f26dc0f4d6ba86bc51cd0cba149_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1350\" data-rawheight=\"911\" class=\"origin_image zh-lightbox-thumb\" width=\"1350\" data-original=\"https://pic2.zhimg.com/v2-ae597f26dc0f4d6ba86bc51cd0cba149_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1350&#39; height=&#39;911&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1350\" data-rawheight=\"911\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1350\" data-original=\"https://pic2.zhimg.com/v2-ae597f26dc0f4d6ba86bc51cd0cba149_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-ae597f26dc0f4d6ba86bc51cd0cba149_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>3、Oracle全部历史数据同步Hive</h2><h2>3.1 再新建一张表</h2><p>这里的目的是表示多个表，而不是一个表，上面已经建了一张表，再建一张表，以验证代码可以将所有的表都同步过去,这里用上一篇博客上的建表Sql即可</p><div class=\"highlight\"><pre><code class=\"language-sql\"><span class=\"k\">CREATE</span> <span class=\"k\">TABLE</span> <span class=\"n\">ORA_TEST</span> <span class=\"p\">(</span>\n<span class=\"n\">ID</span> <span class=\"n\">VARCHAR2</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">),</span> \n<span class=\"n\">NAME</span> <span class=\"n\">VARCHAR2</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">)</span>\n<span class=\"p\">);</span>\n<span class=\"k\">COMMENT</span> <span class=\"k\">ON</span> <span class=\"k\">COLUMN</span> <span class=\"n\">ORA_TEST</span><span class=\"p\">.</span><span class=\"n\">ID</span> <span class=\"k\">IS</span> <span class=\"s1\">&#39;ID&#39;</span><span class=\"p\">;</span>\n<span class=\"k\">COMMENT</span> <span class=\"k\">ON</span> <span class=\"k\">COLUMN</span> <span class=\"n\">ORA_TEST</span><span class=\"p\">.</span><span class=\"n\">NAME</span> <span class=\"k\">IS</span> <span class=\"s1\">&#39;名字&#39;</span><span class=\"p\">;</span>\n<span class=\"k\">COMMENT</span> <span class=\"k\">ON</span> <span class=\"k\">TABLE</span> <span class=\"n\">ORA_TEST</span> <span class=\"k\">IS</span>  <span class=\"s1\">&#39;测试&#39;</span><span class=\"p\">;</span></code></pre></div><p>再在每张表里造点数据，这里就不截图了。</p><h2>3.2 代码</h2><p>上篇博客里用到的注释，是在程序里手工添加的注释，下面的代码是从Oracle里取的，且同步的是一个用户下所有的表。</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">package</span> <span class=\"nn\">com.dkl.leanring.spark.sql.Oracle</span>\n\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.SparkSession</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.types._</span>\n<span class=\"k\">import</span> <span class=\"nn\">java.util.Properties</span>\n<span class=\"k\">import</span> <span class=\"nn\">scala.collection.mutable.ArrayBuffer</span>\n\n<span class=\"cm\">/**\n</span><span class=\"cm\"> *\n</span><span class=\"cm\"> * Spark自动建表（带字段注释、暂无表注释）\n</span><span class=\"cm\"> * 并将中间库Oracle的历史数据全部初始化到hive\n</span><span class=\"cm\"> *\n</span><span class=\"cm\"> * 实现方案：\n</span><span class=\"cm\"> * 1、 利用Spark的自动字段类型匹配\n</span><span class=\"cm\"> * 2、 读取Oracle表字段注释，添加到DataFrame的元数据\n</span><span class=\"cm\"> * 3、按需修改Spark默认的字段类型转换\n</span><span class=\"cm\"> *\n</span><span class=\"cm\"> * 注：需要提前建好对应的hive数据库\n</span><span class=\"cm\"> */</span>\n<span class=\"k\">object</span> <span class=\"nc\">Oracle2Hive</span> <span class=\"o\">{</span>\n\n  <span class=\"k\">def</span> <span class=\"n\">main</span><span class=\"o\">(</span><span class=\"n\">args</span><span class=\"k\">:</span> <span class=\"kt\">Array</span><span class=\"o\">[</span><span class=\"kt\">String</span><span class=\"o\">])</span><span class=\"k\">:</span> <span class=\"kt\">Unit</span> <span class=\"o\">=</span> <span class=\"o\">{</span>\n    <span class=\"k\">val</span> <span class=\"n\">spark</span> <span class=\"k\">=</span> <span class=\"nc\">SparkSession</span>\n      <span class=\"o\">.</span><span class=\"n\">builder</span><span class=\"o\">()</span>\n      <span class=\"o\">.</span><span class=\"n\">appName</span><span class=\"o\">(</span><span class=\"s\">&#34;Oracle2Hive&#34;</span><span class=\"o\">)</span>\n      <span class=\"o\">.</span><span class=\"n\">master</span><span class=\"o\">(</span><span class=\"s\">&#34;local&#34;</span><span class=\"o\">)</span>\n      <span class=\"o\">.</span><span class=\"n\">config</span><span class=\"o\">(</span><span class=\"s\">&#34;spark.sql.parquet.writeLegacyFormat&#34;</span><span class=\"o\">,</span> <span class=\"kc\">true</span><span class=\"o\">)</span>\n      <span class=\"o\">.</span><span class=\"n\">enableHiveSupport</span><span class=\"o\">()</span>\n      <span class=\"o\">.</span><span class=\"n\">getOrCreate</span><span class=\"o\">()</span>\n\n    <span class=\"c1\">//oracle的连接信息\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">p</span> <span class=\"k\">=</span> <span class=\"k\">new</span> <span class=\"nc\">Properties</span><span class=\"o\">()</span>\n    <span class=\"n\">p</span><span class=\"o\">.</span><span class=\"n\">put</span><span class=\"o\">(</span><span class=\"s\">&#34;driver&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;oracle.jdbc.driver.OracleDriver&#34;</span><span class=\"o\">)</span>\n    <span class=\"n\">p</span><span class=\"o\">.</span><span class=\"n\">put</span><span class=\"o\">(</span><span class=\"s\">&#34;url&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;jdbc:oracle:thin:@192.168.44.128:1521:orcl&#34;</span><span class=\"o\">)</span>\n    <span class=\"n\">p</span><span class=\"o\">.</span><span class=\"n\">put</span><span class=\"o\">(</span><span class=\"s\">&#34;user&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;bigdata&#34;</span><span class=\"o\">)</span>\n    <span class=\"n\">p</span><span class=\"o\">.</span><span class=\"n\">put</span><span class=\"o\">(</span><span class=\"s\">&#34;password&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;bigdata&#34;</span><span class=\"o\">)</span>\n\n    <span class=\"k\">import</span> <span class=\"nn\">scala.collection.JavaConversions._</span>\n    <span class=\"k\">val</span> <span class=\"n\">database_conf</span><span class=\"k\">:</span> <span class=\"kt\">scala.collection.mutable.Map</span><span class=\"o\">[</span><span class=\"kt\">String</span>, <span class=\"kt\">String</span><span class=\"o\">]</span> <span class=\"k\">=</span> <span class=\"n\">p</span>\n\n    <span class=\"c1\">//Oracle是分用户的，这里以用户BIGDATA为例\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">owner</span> <span class=\"k\">=</span> <span class=\"s\">&#34;BIGDATA&#34;</span>\n    <span class=\"k\">val</span> <span class=\"n\">sql_in_owner</span> <span class=\"k\">=</span> <span class=\"s\">s&#34;(&#39;</span><span class=\"si\">${</span><span class=\"n\">owner</span><span class=\"si\">}</span><span class=\"s\">&#39;)&#34;</span>\n\n    <span class=\"n\">database_conf</span><span class=\"o\">.</span><span class=\"n\">put</span><span class=\"o\">(</span><span class=\"s\">&#34;dbtable&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;TEST&#34;</span><span class=\"o\">)</span>\n\n    <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">(</span><span class=\"s\">s&#34;use </span><span class=\"si\">${</span><span class=\"n\">owner</span><span class=\"si\">}</span><span class=\"s\">&#34;</span><span class=\"o\">)</span>\n\n    <span class=\"n\">database_conf</span><span class=\"o\">.</span><span class=\"n\">put</span><span class=\"o\">(</span><span class=\"s\">&#34;dbtable&#34;</span><span class=\"o\">,</span> <span class=\"s\">s&#34;(select table_name from all_tables where owner in </span><span class=\"si\">${</span><span class=\"n\">sql_in_owner</span><span class=\"si\">}</span><span class=\"s\">)a&#34;</span><span class=\"o\">)</span>\n    <span class=\"c1\">//所有的表名\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">allTableNames</span> <span class=\"k\">=</span> <span class=\"n\">getDataFrame</span><span class=\"o\">(</span><span class=\"n\">spark</span><span class=\"o\">,</span> <span class=\"n\">database_conf</span><span class=\"o\">)</span>\n\n    <span class=\"n\">database_conf</span><span class=\"o\">.</span><span class=\"n\">put</span><span class=\"o\">(</span><span class=\"s\">&#34;dbtable&#34;</span><span class=\"o\">,</span> <span class=\"s\">s&#34;(select * from all_col_comments where owner in </span><span class=\"si\">${</span><span class=\"n\">sql_in_owner</span><span class=\"si\">}</span><span class=\"s\">)a&#34;</span><span class=\"o\">)</span>\n    <span class=\"c1\">//所有的表字段对应的注释\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">allColComments</span> <span class=\"k\">=</span> <span class=\"n\">getDataFrame</span><span class=\"o\">(</span><span class=\"n\">spark</span><span class=\"o\">,</span> <span class=\"n\">database_conf</span><span class=\"o\">).</span><span class=\"n\">repartition</span><span class=\"o\">(</span><span class=\"mi\">160</span><span class=\"o\">).</span><span class=\"n\">cache</span>\n\n    <span class=\"n\">allTableNames</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"o\">(</span><span class=\"s\">&#34;table_name&#34;</span><span class=\"o\">).</span><span class=\"n\">collect</span><span class=\"o\">().</span><span class=\"n\">foreach</span><span class=\"o\">(</span><span class=\"n\">row</span> <span class=\"k\">=&gt;</span> <span class=\"o\">{</span>\n      <span class=\"c1\">//表名\n</span><span class=\"c1\"></span>      <span class=\"k\">val</span> <span class=\"n\">table_name</span> <span class=\"k\">=</span> <span class=\"n\">row</span><span class=\"o\">.</span><span class=\"n\">getAs</span><span class=\"o\">[</span><span class=\"kt\">String</span><span class=\"o\">](</span><span class=\"s\">&#34;table_name&#34;</span><span class=\"o\">)</span>\n      <span class=\"n\">database_conf</span><span class=\"o\">.</span><span class=\"n\">put</span><span class=\"o\">(</span><span class=\"s\">&#34;dbtable&#34;</span><span class=\"o\">,</span> <span class=\"n\">table_name</span><span class=\"o\">)</span>\n      <span class=\"c1\">//根据表名从Oracle取数\n</span><span class=\"c1\"></span>      <span class=\"k\">val</span> <span class=\"n\">df</span> <span class=\"k\">=</span> <span class=\"n\">getDataFrame</span><span class=\"o\">(</span><span class=\"n\">spark</span><span class=\"o\">,</span> <span class=\"n\">database_conf</span><span class=\"o\">)</span>\n      <span class=\"c1\">//字段名 和注 对应的map\n</span><span class=\"c1\"></span>      <span class=\"k\">val</span> <span class=\"n\">colName_comments_map</span> <span class=\"k\">=</span> <span class=\"n\">allColComments</span><span class=\"o\">.</span><span class=\"n\">where</span><span class=\"o\">(</span><span class=\"s\">s&#34;TABLE_NAME=&#39;</span><span class=\"si\">${</span><span class=\"n\">table_name</span><span class=\"si\">}</span><span class=\"s\">&#39;&#34;</span><span class=\"o\">)</span>\n        <span class=\"o\">.</span><span class=\"n\">select</span><span class=\"o\">(</span><span class=\"s\">&#34;COLUMN_NAME&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;COMMENTS&#34;</span><span class=\"o\">)</span>\n        <span class=\"o\">.</span><span class=\"n\">na</span><span class=\"o\">.</span><span class=\"n\">fill</span><span class=\"o\">(</span><span class=\"s\">&#34;&#34;</span><span class=\"o\">,</span> <span class=\"nc\">Array</span><span class=\"o\">(</span><span class=\"s\">&#34;COMMENTS&#34;</span><span class=\"o\">))</span>\n        <span class=\"o\">.</span><span class=\"n\">rdd</span><span class=\"o\">.</span><span class=\"n\">map</span><span class=\"o\">(</span><span class=\"n\">row</span> <span class=\"k\">=&gt;</span> <span class=\"o\">(</span><span class=\"n\">row</span><span class=\"o\">.</span><span class=\"n\">getAs</span><span class=\"o\">[</span><span class=\"kt\">String</span><span class=\"o\">](</span><span class=\"s\">&#34;COLUMN_NAME&#34;</span><span class=\"o\">),</span> <span class=\"n\">row</span><span class=\"o\">.</span><span class=\"n\">getAs</span><span class=\"o\">[</span><span class=\"kt\">String</span><span class=\"o\">](</span><span class=\"s\">&#34;COMMENTS&#34;</span><span class=\"o\">)))</span>\n        <span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"o\">()</span>\n        <span class=\"o\">.</span><span class=\"n\">toMap</span>\n\n      <span class=\"k\">val</span> <span class=\"n\">colName</span> <span class=\"k\">=</span> <span class=\"nc\">ArrayBuffer</span><span class=\"o\">[</span><span class=\"kt\">String</span><span class=\"o\">]()</span>\n      <span class=\"c1\">//为schema添加注释信息\n</span><span class=\"c1\"></span>      <span class=\"k\">val</span> <span class=\"n\">schema</span> <span class=\"k\">=</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">schema</span><span class=\"o\">.</span><span class=\"n\">map</span><span class=\"o\">(</span><span class=\"n\">s</span> <span class=\"k\">=&gt;</span> <span class=\"o\">{</span>\n        <span class=\"k\">if</span> <span class=\"o\">(</span><span class=\"n\">s</span><span class=\"o\">.</span><span class=\"n\">dataType</span><span class=\"o\">.</span><span class=\"n\">equals</span><span class=\"o\">(</span><span class=\"nc\">DecimalType</span><span class=\"o\">(</span><span class=\"mi\">38</span><span class=\"o\">,</span> <span class=\"mi\">0</span><span class=\"o\">)))</span> <span class=\"o\">{</span>\n          <span class=\"n\">colName</span> <span class=\"o\">+=</span> <span class=\"n\">s</span><span class=\"o\">.</span><span class=\"n\">name</span>\n          <span class=\"k\">new</span> <span class=\"nc\">StructField</span><span class=\"o\">(</span><span class=\"n\">s</span><span class=\"o\">.</span><span class=\"n\">name</span><span class=\"o\">,</span> <span class=\"nc\">IntegerType</span><span class=\"o\">,</span> <span class=\"n\">s</span><span class=\"o\">.</span><span class=\"n\">nullable</span><span class=\"o\">,</span> <span class=\"n\">s</span><span class=\"o\">.</span><span class=\"n\">metadata</span><span class=\"o\">).</span><span class=\"n\">withComment</span><span class=\"o\">(</span><span class=\"n\">colName_comments_map</span><span class=\"o\">(</span><span class=\"n\">s</span><span class=\"o\">.</span><span class=\"n\">name</span><span class=\"o\">))</span>\n        <span class=\"o\">}</span> <span class=\"k\">else</span> <span class=\"o\">{</span>\n          <span class=\"n\">s</span><span class=\"o\">.</span><span class=\"n\">withComment</span><span class=\"o\">(</span><span class=\"n\">colName_comments_map</span><span class=\"o\">(</span><span class=\"n\">s</span><span class=\"o\">.</span><span class=\"n\">name</span><span class=\"o\">))</span>\n        <span class=\"o\">}</span>\n      <span class=\"o\">})</span>\n\n      <span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.functions._</span>\n      <span class=\"k\">var</span> <span class=\"n\">df_int</span> <span class=\"k\">=</span> <span class=\"n\">df</span>\n      <span class=\"n\">colName</span><span class=\"o\">.</span><span class=\"n\">foreach</span><span class=\"o\">(</span><span class=\"n\">name</span> <span class=\"k\">=&gt;</span> <span class=\"o\">{</span>\n        <span class=\"n\">df_int</span> <span class=\"k\">=</span> <span class=\"n\">df_int</span><span class=\"o\">.</span><span class=\"n\">withColumn</span><span class=\"o\">(</span><span class=\"n\">name</span><span class=\"o\">,</span> <span class=\"n\">col</span><span class=\"o\">(</span><span class=\"n\">name</span><span class=\"o\">).</span><span class=\"n\">cast</span><span class=\"o\">(</span><span class=\"nc\">IntegerType</span><span class=\"o\">))</span>\n\n      <span class=\"o\">})</span>\n      <span class=\"c1\">//根据添加了注释的schema，新建DataFrame\n</span><span class=\"c1\"></span>      <span class=\"k\">val</span> <span class=\"n\">new_df</span> <span class=\"k\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"o\">(</span><span class=\"n\">df_int</span><span class=\"o\">.</span><span class=\"n\">rdd</span><span class=\"o\">,</span> <span class=\"nc\">StructType</span><span class=\"o\">(</span><span class=\"n\">schema</span><span class=\"o\">))</span>\n      <span class=\"n\">new_df</span><span class=\"o\">.</span><span class=\"n\">write</span><span class=\"o\">.</span><span class=\"n\">mode</span><span class=\"o\">(</span><span class=\"s\">&#34;overwrite&#34;</span><span class=\"o\">).</span><span class=\"n\">saveAsTable</span><span class=\"o\">(</span><span class=\"n\">table_name</span><span class=\"o\">)</span>\n\n      <span class=\"c1\">//      new_df.schema.foreach(s =&gt; println(s.metadata))\n</span><span class=\"c1\"></span>      <span class=\"c1\">//      new_df.printSchema()\n</span><span class=\"c1\"></span>\n    <span class=\"o\">})</span>\n\n    <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">stop</span>\n\n  <span class=\"o\">}</span>\n  <span class=\"cm\">/**\n</span><span class=\"cm\">   * @param spark SparkSession\n</span><span class=\"cm\">   * @param database_conf 数据库配置项Map，包括driver,url,username,password,dbtable等内容，提交程序时需用--jars选项引用相关jar包\n</span><span class=\"cm\">   * @return 返回DataFrame对象\n</span><span class=\"cm\">   */</span>\n  <span class=\"k\">def</span> <span class=\"n\">getDataFrame</span><span class=\"o\">(</span><span class=\"n\">spark</span><span class=\"k\">:</span> <span class=\"kt\">SparkSession</span><span class=\"o\">,</span> <span class=\"n\">database_conf</span><span class=\"k\">:</span> <span class=\"kt\">scala.collection.Map</span><span class=\"o\">[</span><span class=\"kt\">String</span>, <span class=\"kt\">String</span><span class=\"o\">])</span> <span class=\"k\">=</span> <span class=\"o\">{</span>\n    <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">read</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"o\">(</span><span class=\"s\">&#34;jdbc&#34;</span><span class=\"o\">).</span><span class=\"n\">options</span><span class=\"o\">(</span><span class=\"n\">database_conf</span><span class=\"o\">).</span><span class=\"n\">load</span><span class=\"o\">()</span>\n  <span class=\"o\">}</span>\n\n<span class=\"o\">}</span></code></pre></div><h2>3.3 看一下Hive里的结果</h2><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-08f633aedf4f47b9c96fecf6d278618b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"653\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic4.zhimg.com/v2-08f633aedf4f47b9c96fecf6d278618b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;653&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"653\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic4.zhimg.com/v2-08f633aedf4f47b9c96fecf6d278618b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-08f633aedf4f47b9c96fecf6d278618b_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>这样就成功的完成了Oracle历史数据到Hive的同步！</p><h2>4、关于增量数据的同步</h2><h2>4.1 实时同步</h2><p>可以考虑这样，先用ogg将Oracle的增量数据实时同步到kafka,再用Spark Streaming实现kafka到hive的实时同步。 * 下面两篇文章提供参考：<a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/05/23/oggOracle2Kafka/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">利用ogg实现oracle到kafka的增量数据实时同步</a>、<a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/06/20/sparkStreamingOffsetOnlyOnce/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Spark Streamming+Kafka提交offset实现有且仅有一次</a>,其中Spark Streaming的代码并没有实现写入hive的功能，但是实时读取kafka的功能已经实现，只要自己处理一下解析kafka里json格式的增量数据，转成DataFrame保存到hive表里即可。</p><h2>4.2 非实时</h2><p>如果Oracle的每个表里都有时间字段，那么可以通过时间字段来过滤增量数据，用上面的Spark程序去定时的跑，如果没有时间字段的话，可以用ogg的colmap函数增加时间字段，先实时同步到中间的Oracle库，再根据时间字段来同步。</p>", 
            "topic": [
                {
                    "tag": "Spark", 
                    "tagLink": "https://api.zhihu.com/topics/19942170"
                }, 
                {
                    "tag": "Oracle 数据库", 
                    "tagLink": "https://api.zhihu.com/topics/19660156"
                }, 
                {
                    "tag": "Hive", 
                    "tagLink": "https://api.zhihu.com/topics/19655283"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/51777949", 
            "userName": "董可伦", 
            "userLink": "https://www.zhihu.com/people/de888cc734ddbfd918269a8e5fb196ba", 
            "upvote": 2, 
            "title": "Spark通过修改DataFrame的schema给表字段添加注释", 
            "content": "<p></p><p>我的原创地址：</p><a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/08/20/sparkDfAddComments/\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic2.zhimg.com/v2-f66a6d86c4c5a333a9775ea8e9e3c9cd_180x120.jpg\" data-image-width=\"1000\" data-image-height=\"132\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Spark通过修改DataFrame的schema给表字段添加注释</a><h2>1、需求背景</h2><p>通过Spark将关系型数据库（以Oracle为例）的表同步的Hive表，要求用Spark建表，有字段注释的也要加上注释。Spark建表，有两种方法： <i> 用Spark Sql，在程序里组建表语句，然后用Spark.sql(&#34;建表语句&#34;)建表，这种方法麻烦的地方在于你要读取Oracle表的详细的表结构信息，且要进行Oracle和Hive的字段类型进行一一对应 </i> 用DataFrame 的saveAsTable方法，这种方法如果对应的数据库里没有表，则Spark会根据DataFrame的schema自动建表，比较简单，不用考虑字段类型匹配转化问题，但是这种方法有一个问题，Spark读取Oracle的表为DataFrame时，并不能将表字段的注释读进来，所以就有了如标题所示的需求。（一开始以为DataFrame不能加注释，经过研究，发现是可以的！）</p><h2>2、如何查看DataFrame是否有注释</h2><p>前面讲到DataFrame里没有Oracle的注释信息，但是如果数据源为Hive的话，是可以将注释获取到的。</p><h2>2.1 新建Hive测试表（带注释）</h2><div class=\"highlight\"><pre><code class=\"language-sql\"><span class=\"k\">create</span> <span class=\"k\">table</span> <span class=\"o\">`</span><span class=\"n\">test</span><span class=\"o\">`</span> <span class=\"p\">(</span>\n<span class=\"o\">`</span><span class=\"n\">id</span><span class=\"o\">`</span> <span class=\"n\">string</span> <span class=\"k\">comment</span> <span class=\"s1\">&#39;ID&#39;</span><span class=\"p\">,</span> \n<span class=\"o\">`</span><span class=\"n\">Name</span><span class=\"o\">`</span> <span class=\"n\">string</span> <span class=\"k\">comment</span> <span class=\"s1\">&#39;名字&#39;</span>\n<span class=\"p\">)</span>\n<span class=\"k\">comment</span> <span class=\"s1\">&#39;测试&#39;</span><span class=\"p\">;</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3189dda38317082db44bd1b4f37b0781_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1000\" data-rawheight=\"243\" class=\"origin_image zh-lightbox-thumb\" width=\"1000\" data-original=\"https://pic2.zhimg.com/v2-3189dda38317082db44bd1b4f37b0781_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1000&#39; height=&#39;243&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1000\" data-rawheight=\"243\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1000\" data-original=\"https://pic2.zhimg.com/v2-3189dda38317082db44bd1b4f37b0781_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-3189dda38317082db44bd1b4f37b0781_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>2.2 Spark读取hive表并打印注释（在spark-shell里执行）</h2><p>若不清楚Spark如何连接hive，可以参考：<a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/03/25/sparkHive/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">spark连接hive（spark-shell和eclipse两种方式）</a> 首先看一下df.printSchema里并没有注释信息</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"n\">sql</span><span class=\"o\">(</span><span class=\"s\">&#34;use test&#34;</span><span class=\"o\">)</span>\n<span class=\"k\">val</span> <span class=\"n\">df</span> <span class=\"k\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">table</span><span class=\"o\">(</span><span class=\"s\">&#34;test&#34;</span><span class=\"o\">)</span>\n<span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">printSchema</span>\n<span class=\"n\">root</span>\n <span class=\"o\">|--</span> <span class=\"n\">id</span><span class=\"k\">:</span> <span class=\"kt\">string</span> <span class=\"o\">(</span><span class=\"kt\">nullable</span> <span class=\"o\">=</span> <span class=\"kt\">true</span><span class=\"o\">)</span>\n <span class=\"o\">|--</span> <span class=\"n\">name</span><span class=\"k\">:</span> <span class=\"kt\">string</span> <span class=\"o\">(</span><span class=\"kt\">nullable</span> <span class=\"o\">=</span> <span class=\"kt\">true</span><span class=\"o\">)</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ee0708cb52d8f919de37a5ea235010e3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1000\" data-rawheight=\"420\" class=\"origin_image zh-lightbox-thumb\" width=\"1000\" data-original=\"https://pic4.zhimg.com/v2-ee0708cb52d8f919de37a5ea235010e3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1000&#39; height=&#39;420&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1000\" data-rawheight=\"420\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1000\" data-original=\"https://pic4.zhimg.com/v2-ee0708cb52d8f919de37a5ea235010e3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-ee0708cb52d8f919de37a5ea235010e3_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>用下面这行代码便可以打印注释信息：</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">schema</span><span class=\"o\">.</span><span class=\"n\">foreach</span><span class=\"o\">(</span><span class=\"n\">s</span><span class=\"k\">=&gt;</span><span class=\"n\">println</span><span class=\"o\">(</span><span class=\"n\">s</span><span class=\"o\">.</span><span class=\"n\">name</span><span class=\"o\">,</span><span class=\"n\">s</span><span class=\"o\">.</span><span class=\"n\">metadata</span><span class=\"o\">))</span>\n<span class=\"o\">(</span><span class=\"n\">id</span><span class=\"o\">,{</span><span class=\"s\">&#34;comment&#34;</span><span class=\"k\">:</span><span class=\"err\">&#34;</span><span class=\"kt\">ID</span><span class=\"err\">&#34;</span><span class=\"o\">,</span><span class=\"s\">&#34;HIVE_TYPE_STRING&#34;</span><span class=\"k\">:</span><span class=\"err\">&#34;</span><span class=\"kt\">string</span><span class=\"err\">&#34;</span><span class=\"o\">})</span>\n<span class=\"o\">(</span><span class=\"n\">name</span><span class=\"o\">,{</span><span class=\"s\">&#34;comment&#34;</span><span class=\"k\">:</span><span class=\"err\">&#34;</span><span class=\"kt\">名字</span><span class=\"err\">&#34;</span><span class=\"o\">,</span><span class=\"s\">&#34;HIVE_TYPE_STRING&#34;</span><span class=\"k\">:</span><span class=\"err\">&#34;</span><span class=\"kt\">string</span><span class=\"err\">&#34;</span><span class=\"o\">})</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-311ac66fa07d8b3a4b21ae90d240489d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1000\" data-rawheight=\"60\" class=\"origin_image zh-lightbox-thumb\" width=\"1000\" data-original=\"https://pic2.zhimg.com/v2-311ac66fa07d8b3a4b21ae90d240489d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1000&#39; height=&#39;60&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1000\" data-rawheight=\"60\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1000\" data-original=\"https://pic2.zhimg.com/v2-311ac66fa07d8b3a4b21ae90d240489d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-311ac66fa07d8b3a4b21ae90d240489d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>3、读取Oracle表并打印DataFrmae的元数据信息</h2><h2>3.1 新建Oracle测试表（带注释）</h2><div class=\"highlight\"><pre><code class=\"language-sql\"><span class=\"k\">CREATE</span> <span class=\"k\">TABLE</span> <span class=\"n\">ORA_TEST</span> <span class=\"p\">(</span>\n<span class=\"n\">ID</span> <span class=\"n\">VARCHAR2</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">),</span> \n<span class=\"n\">NAME</span> <span class=\"n\">VARCHAR2</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">)</span>\n<span class=\"p\">);</span>\n<span class=\"k\">COMMENT</span> <span class=\"k\">ON</span> <span class=\"k\">COLUMN</span> <span class=\"n\">ORA_TEST</span><span class=\"p\">.</span><span class=\"n\">ID</span> <span class=\"k\">IS</span> <span class=\"s1\">&#39;ID&#39;</span><span class=\"p\">;</span>\n<span class=\"k\">COMMENT</span> <span class=\"k\">ON</span> <span class=\"k\">COLUMN</span> <span class=\"n\">ORA_TEST</span><span class=\"p\">.</span><span class=\"n\">NAME</span> <span class=\"k\">IS</span> <span class=\"s1\">&#39;名字&#39;</span><span class=\"p\">;</span>\n<span class=\"k\">COMMENT</span> <span class=\"k\">ON</span> <span class=\"k\">TABLE</span> <span class=\"n\">ORA_TEST</span> <span class=\"k\">IS</span>  <span class=\"s1\">&#39;测试&#39;</span><span class=\"p\">;</span></code></pre></div><ul><li>注：上面的注释语句和建表语句需要分开执行，或者也可以在数据库工具执行脚本，比如我用的DBeaver用快捷键Alt+x即可。当然也可以在工具的界面直接建表均可。</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-25cc58a1aeff621d22c285a2cea6b3c5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1648\" data-rawheight=\"284\" class=\"origin_image zh-lightbox-thumb\" width=\"1648\" data-original=\"https://pic2.zhimg.com/v2-25cc58a1aeff621d22c285a2cea6b3c5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1648&#39; height=&#39;284&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1648\" data-rawheight=\"284\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1648\" data-original=\"https://pic2.zhimg.com/v2-25cc58a1aeff621d22c285a2cea6b3c5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-25cc58a1aeff621d22c285a2cea6b3c5_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>3.2 读取Oracle表，并打印元数据</h2><p>代码：</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">package</span> <span class=\"nn\">com.dkl.leanring.spark.sql.Oracle</span>\n\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.SparkSession</span>\n\n<span class=\"k\">object</span> <span class=\"nc\">OracleSchemaDemo</span> <span class=\"o\">{</span>\n  <span class=\"k\">def</span> <span class=\"n\">main</span><span class=\"o\">(</span><span class=\"n\">args</span><span class=\"k\">:</span> <span class=\"kt\">Array</span><span class=\"o\">[</span><span class=\"kt\">String</span><span class=\"o\">])</span><span class=\"k\">:</span> <span class=\"kt\">Unit</span> <span class=\"o\">=</span> <span class=\"o\">{</span>\n    <span class=\"k\">val</span> <span class=\"n\">spark</span> <span class=\"k\">=</span> <span class=\"nc\">SparkSession</span><span class=\"o\">.</span><span class=\"n\">builder</span><span class=\"o\">().</span><span class=\"n\">appName</span><span class=\"o\">(</span><span class=\"s\">&#34;OracleSchemaDemo&#34;</span><span class=\"o\">).</span><span class=\"n\">master</span><span class=\"o\">(</span><span class=\"s\">&#34;local&#34;</span><span class=\"o\">).</span><span class=\"n\">getOrCreate</span><span class=\"o\">()</span>\n    <span class=\"k\">val</span> <span class=\"n\">df</span> <span class=\"k\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">read</span>\n      <span class=\"o\">.</span><span class=\"n\">format</span><span class=\"o\">(</span><span class=\"s\">&#34;jdbc&#34;</span><span class=\"o\">)</span>\n      <span class=\"o\">.</span><span class=\"n\">option</span><span class=\"o\">(</span><span class=\"s\">&#34;url&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;jdbc:oracle:thin:@192.168.44.128:1521:orcl&#34;</span><span class=\"o\">)</span>\n      <span class=\"o\">.</span><span class=\"n\">option</span><span class=\"o\">(</span><span class=\"s\">&#34;dbtable&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;ORA_TEST&#34;</span><span class=\"o\">)</span>\n      <span class=\"o\">.</span><span class=\"n\">option</span><span class=\"o\">(</span><span class=\"s\">&#34;user&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;bigdata&#34;</span><span class=\"o\">)</span>\n      <span class=\"o\">.</span><span class=\"n\">option</span><span class=\"o\">(</span><span class=\"s\">&#34;password&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;bigdata&#34;</span><span class=\"o\">)</span>\n      <span class=\"o\">.</span><span class=\"n\">option</span><span class=\"o\">(</span><span class=\"s\">&#34;driver&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;oracle.jdbc.driver.OracleDriver&#34;</span><span class=\"o\">)</span>\n      <span class=\"o\">.</span><span class=\"n\">load</span><span class=\"o\">()</span>\n    <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">schema</span><span class=\"o\">.</span><span class=\"n\">foreach</span><span class=\"o\">(</span><span class=\"n\">s</span> <span class=\"k\">=&gt;</span> <span class=\"n\">println</span><span class=\"o\">(</span><span class=\"n\">s</span><span class=\"o\">.</span><span class=\"n\">name</span><span class=\"o\">,</span> <span class=\"n\">s</span><span class=\"o\">.</span><span class=\"n\">metadata</span><span class=\"o\">))</span>\n\n    <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">stop</span>\n\n  <span class=\"o\">}</span>\n<span class=\"o\">}</span>\n<span class=\"o\">(</span><span class=\"nc\">ID</span><span class=\"o\">,{</span><span class=\"s\">&#34;name&#34;</span><span class=\"k\">:</span><span class=\"err\">&#34;</span><span class=\"kt\">ID</span><span class=\"err\">&#34;</span><span class=\"o\">,</span><span class=\"s\">&#34;scale&#34;</span><span class=\"k\">:</span><span class=\"err\">0</span><span class=\"o\">})</span>\n<span class=\"o\">(</span><span class=\"nc\">NAME</span><span class=\"o\">,{</span><span class=\"s\">&#34;name&#34;</span><span class=\"k\">:</span><span class=\"err\">&#34;</span><span class=\"kt\">NAME</span><span class=\"err\">&#34;</span><span class=\"o\">,</span><span class=\"s\">&#34;scale&#34;</span><span class=\"k\">:</span><span class=\"err\">0</span><span class=\"o\">})</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-60bf1b65266f65832afd3983b0b0d19a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1350\" data-rawheight=\"681\" class=\"origin_image zh-lightbox-thumb\" width=\"1350\" data-original=\"https://pic3.zhimg.com/v2-60bf1b65266f65832afd3983b0b0d19a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1350&#39; height=&#39;681&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1350\" data-rawheight=\"681\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1350\" data-original=\"https://pic3.zhimg.com/v2-60bf1b65266f65832afd3983b0b0d19a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-60bf1b65266f65832afd3983b0b0d19a_b.jpg\"/></figure><p> 注：Spark2.3.0和Spark2.2.1的元数据不太一样，上面的结果是Spark2.2.1(也是我写博客测试用的)，项目中用的Spark2.3.0，2.3.0的元数据是空的,如下</p><div class=\"highlight\"><pre><code class=\"language-vim\"><span class=\"p\">(</span><span class=\"nx\">ID</span><span class=\"p\">,</span>{}<span class=\"p\">)</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"p\">(</span><span class=\"nx\">NAME</span><span class=\"p\">,</span>{}<span class=\"p\">)</span></code></pre></div><p>可见并没有注释信息</p><h2>3.3 给DataFrame添加注释</h2><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.types._</span>\n<span class=\"k\">val</span> <span class=\"n\">commentMap</span> <span class=\"k\">=</span> <span class=\"nc\">Map</span><span class=\"o\">(</span><span class=\"s\">&#34;ID&#34;</span> <span class=\"o\">-&gt;</span> <span class=\"s\">&#34;ID&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;NAME&#34;</span> <span class=\"o\">-&gt;</span> <span class=\"s\">&#34;名字&#34;</span><span class=\"o\">)</span>\n\n<span class=\"k\">val</span> <span class=\"n\">schema</span> <span class=\"k\">=</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">schema</span><span class=\"o\">.</span><span class=\"n\">map</span><span class=\"o\">(</span><span class=\"n\">s</span> <span class=\"k\">=&gt;</span> <span class=\"o\">{</span>\n  <span class=\"n\">s</span><span class=\"o\">.</span><span class=\"n\">withComment</span><span class=\"o\">(</span><span class=\"n\">commentMap</span><span class=\"o\">(</span><span class=\"n\">s</span><span class=\"o\">.</span><span class=\"n\">name</span><span class=\"o\">))</span>\n<span class=\"o\">})</span>\n\n<span class=\"c1\">//根据添加了注释的schema，新建DataFrame\n</span><span class=\"c1\"></span><span class=\"k\">val</span> <span class=\"n\">new_df</span> <span class=\"k\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"o\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">rdd</span><span class=\"o\">,</span> <span class=\"nc\">StructType</span><span class=\"o\">(</span><span class=\"n\">schema</span><span class=\"o\">)).</span><span class=\"n\">repartition</span><span class=\"o\">(</span><span class=\"mi\">160</span><span class=\"o\">)</span>\n\n<span class=\"n\">new_df</span><span class=\"o\">.</span><span class=\"n\">schema</span><span class=\"o\">.</span><span class=\"n\">foreach</span><span class=\"o\">(</span><span class=\"n\">s</span> <span class=\"k\">=&gt;</span> <span class=\"n\">println</span><span class=\"o\">(</span><span class=\"n\">s</span><span class=\"o\">.</span><span class=\"n\">name</span><span class=\"o\">,</span> <span class=\"n\">s</span><span class=\"o\">.</span><span class=\"n\">metadata</span><span class=\"o\">))</span>\n<span class=\"o\">(</span><span class=\"nc\">ID</span><span class=\"o\">,{</span><span class=\"s\">&#34;comment&#34;</span><span class=\"k\">:</span><span class=\"err\">&#34;</span><span class=\"kt\">ID</span><span class=\"err\">&#34;</span><span class=\"o\">,</span><span class=\"s\">&#34;name&#34;</span><span class=\"k\">:</span><span class=\"err\">&#34;</span><span class=\"kt\">ID</span><span class=\"err\">&#34;</span><span class=\"o\">,</span><span class=\"s\">&#34;scale&#34;</span><span class=\"k\">:</span><span class=\"err\">0</span><span class=\"o\">})</span>\n<span class=\"o\">(</span><span class=\"nc\">NAME</span><span class=\"o\">,{</span><span class=\"s\">&#34;comment&#34;</span><span class=\"k\">:</span><span class=\"err\">&#34;</span><span class=\"kt\">名字</span><span class=\"err\">&#34;</span><span class=\"o\">,</span><span class=\"s\">&#34;name&#34;</span><span class=\"k\">:</span><span class=\"err\">&#34;</span><span class=\"kt\">NAME</span><span class=\"err\">&#34;</span><span class=\"o\">,</span><span class=\"s\">&#34;scale&#34;</span><span class=\"k\">:</span><span class=\"err\">0</span><span class=\"o\">})</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-093160a4c0fb327ad5bb2de2c2e75874_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1350\" data-rawheight=\"789\" class=\"origin_image zh-lightbox-thumb\" width=\"1350\" data-original=\"https://pic1.zhimg.com/v2-093160a4c0fb327ad5bb2de2c2e75874_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1350&#39; height=&#39;789&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1350\" data-rawheight=\"789\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1350\" data-original=\"https://pic1.zhimg.com/v2-093160a4c0fb327ad5bb2de2c2e75874_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-093160a4c0fb327ad5bb2de2c2e75874_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>4、 测试写到Hive表有没有注释</h2><p>需将前面代码中的spark改为支持hive，即加上enableHiveSupport()</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">(</span><span class=\"s\">&#34;use test&#34;</span><span class=\"o\">)</span>\n<span class=\"n\">new_df</span><span class=\"o\">.</span><span class=\"n\">write</span><span class=\"o\">.</span><span class=\"n\">mode</span><span class=\"o\">(</span><span class=\"s\">&#34;overwrite&#34;</span><span class=\"o\">).</span><span class=\"n\">saveAsTable</span><span class=\"o\">(</span><span class=\"s\">&#34;ORA_TEST&#34;</span><span class=\"o\">)</span></code></pre></div><p>然后在hive里看一下，是否有注释 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-f66a6d86c4c5a333a9775ea8e9e3c9cd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1000\" data-rawheight=\"132\" class=\"origin_image zh-lightbox-thumb\" width=\"1000\" data-original=\"https://pic2.zhimg.com/v2-f66a6d86c4c5a333a9775ea8e9e3c9cd_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1000&#39; height=&#39;132&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1000\" data-rawheight=\"132\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1000\" data-original=\"https://pic2.zhimg.com/v2-f66a6d86c4c5a333a9775ea8e9e3c9cd_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-f66a6d86c4c5a333a9775ea8e9e3c9cd_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>可以看到，成功的把注释也保存到里hive里</p><h2>5、附录</h2><p>附上在Eclipse运行的完整代码</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">package</span> <span class=\"nn\">com.dkl.leanring.spark.sql.Oracle</span>\n\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.SparkSession</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.types._</span>\n\n<span class=\"k\">object</span> <span class=\"nc\">OracleSchemaDemo</span> <span class=\"o\">{</span>\n  <span class=\"k\">def</span> <span class=\"n\">main</span><span class=\"o\">(</span><span class=\"n\">args</span><span class=\"k\">:</span> <span class=\"kt\">Array</span><span class=\"o\">[</span><span class=\"kt\">String</span><span class=\"o\">])</span><span class=\"k\">:</span> <span class=\"kt\">Unit</span> <span class=\"o\">=</span> <span class=\"o\">{</span>\n    <span class=\"k\">val</span> <span class=\"n\">spark</span> <span class=\"k\">=</span> <span class=\"nc\">SparkSession</span><span class=\"o\">.</span><span class=\"n\">builder</span><span class=\"o\">().</span><span class=\"n\">appName</span><span class=\"o\">(</span><span class=\"s\">&#34;OracleSchemaDemo&#34;</span><span class=\"o\">).</span><span class=\"n\">master</span><span class=\"o\">(</span><span class=\"s\">&#34;local&#34;</span><span class=\"o\">).</span><span class=\"n\">enableHiveSupport</span><span class=\"o\">().</span><span class=\"n\">getOrCreate</span><span class=\"o\">()</span>\n    <span class=\"k\">val</span> <span class=\"n\">df</span> <span class=\"k\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">read</span>\n      <span class=\"o\">.</span><span class=\"n\">format</span><span class=\"o\">(</span><span class=\"s\">&#34;jdbc&#34;</span><span class=\"o\">)</span>\n      <span class=\"o\">.</span><span class=\"n\">option</span><span class=\"o\">(</span><span class=\"s\">&#34;url&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;jdbc:oracle:thin:@192.168.44.128:1521:orcl&#34;</span><span class=\"o\">)</span>\n      <span class=\"o\">.</span><span class=\"n\">option</span><span class=\"o\">(</span><span class=\"s\">&#34;dbtable&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;ORA_TEST&#34;</span><span class=\"o\">)</span>\n      <span class=\"o\">.</span><span class=\"n\">option</span><span class=\"o\">(</span><span class=\"s\">&#34;user&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;bigdata&#34;</span><span class=\"o\">)</span>\n      <span class=\"o\">.</span><span class=\"n\">option</span><span class=\"o\">(</span><span class=\"s\">&#34;password&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;bigdata&#34;</span><span class=\"o\">)</span>\n      <span class=\"o\">.</span><span class=\"n\">option</span><span class=\"o\">(</span><span class=\"s\">&#34;driver&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;oracle.jdbc.driver.OracleDriver&#34;</span><span class=\"o\">)</span>\n      <span class=\"o\">.</span><span class=\"n\">load</span><span class=\"o\">()</span>\n    <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">schema</span><span class=\"o\">.</span><span class=\"n\">foreach</span><span class=\"o\">(</span><span class=\"n\">s</span> <span class=\"k\">=&gt;</span> <span class=\"n\">println</span><span class=\"o\">(</span><span class=\"n\">s</span><span class=\"o\">.</span><span class=\"n\">name</span><span class=\"o\">,</span> <span class=\"n\">s</span><span class=\"o\">.</span><span class=\"n\">metadata</span><span class=\"o\">))</span>\n\n    <span class=\"k\">val</span> <span class=\"n\">commentMap</span> <span class=\"k\">=</span> <span class=\"nc\">Map</span><span class=\"o\">(</span><span class=\"s\">&#34;ID&#34;</span> <span class=\"o\">-&gt;</span> <span class=\"s\">&#34;ID&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;NAME&#34;</span> <span class=\"o\">-&gt;</span> <span class=\"s\">&#34;名字&#34;</span><span class=\"o\">)</span>\n\n    <span class=\"k\">val</span> <span class=\"n\">schema</span> <span class=\"k\">=</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">schema</span><span class=\"o\">.</span><span class=\"n\">map</span><span class=\"o\">(</span><span class=\"n\">s</span> <span class=\"k\">=&gt;</span> <span class=\"o\">{</span>\n      <span class=\"n\">s</span><span class=\"o\">.</span><span class=\"n\">withComment</span><span class=\"o\">(</span><span class=\"n\">commentMap</span><span class=\"o\">(</span><span class=\"n\">s</span><span class=\"o\">.</span><span class=\"n\">name</span><span class=\"o\">))</span>\n    <span class=\"o\">})</span>\n\n    <span class=\"c1\">//根据添加了注释的schema，新建DataFrame\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">new_df</span> <span class=\"k\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"o\">(</span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">rdd</span><span class=\"o\">,</span> <span class=\"nc\">StructType</span><span class=\"o\">(</span><span class=\"n\">schema</span><span class=\"o\">)).</span><span class=\"n\">repartition</span><span class=\"o\">(</span><span class=\"mi\">160</span><span class=\"o\">)</span>\n\n    <span class=\"n\">new_df</span><span class=\"o\">.</span><span class=\"n\">schema</span><span class=\"o\">.</span><span class=\"n\">foreach</span><span class=\"o\">(</span><span class=\"n\">s</span> <span class=\"k\">=&gt;</span> <span class=\"n\">println</span><span class=\"o\">(</span><span class=\"n\">s</span><span class=\"o\">.</span><span class=\"n\">name</span><span class=\"o\">,</span> <span class=\"n\">s</span><span class=\"o\">.</span><span class=\"n\">metadata</span><span class=\"o\">))</span>\n\n    <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">(</span><span class=\"s\">&#34;use test&#34;</span><span class=\"o\">)</span>\n    <span class=\"c1\">//保存到hive\n</span><span class=\"c1\"></span>    <span class=\"n\">new_df</span><span class=\"o\">.</span><span class=\"n\">write</span><span class=\"o\">.</span><span class=\"n\">mode</span><span class=\"o\">(</span><span class=\"s\">&#34;overwrite&#34;</span><span class=\"o\">).</span><span class=\"n\">saveAsTable</span><span class=\"o\">(</span><span class=\"s\">&#34;ORA_TEST&#34;</span><span class=\"o\">)</span>\n\n    <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">stop</span>\n\n  <span class=\"o\">}</span>\n<span class=\"o\">}</span></code></pre></div><p></p>", 
            "topic": [
                {
                    "tag": "Spark", 
                    "tagLink": "https://api.zhihu.com/topics/19942170"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/51682275", 
            "userName": "董可伦", 
            "userLink": "https://www.zhihu.com/people/de888cc734ddbfd918269a8e5fb196ba", 
            "upvote": 1, 
            "title": "Spark创建空的DataFrame", 
            "content": "<p>我的原创地址：</p><a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/08/14/sparkEmptyDataFrame/\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-fd2abddc815cee303039344a5032d1be_180x120.jpg\" data-image-width=\"1667\" data-image-height=\"920\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Spark创建空的DataFrame</a><h2>前言</h2><p>本文主要给出Spark创建空的DataFrame的代码示例，这里讲的空的DataFrame主要指有列名（可以自己随意指定），但是没有行的DataFrame，因为自己在开发过程中有这个需求，之前并不知道怎么创建，就查了一下，发现资料并不多，不知道因为太简单还是用的人少~，至于具体什么需求就不阐述了，主要给有这方面需求的小伙伴参考一下。还有另一种空的DataFrame就是没有任何行任何列的DataFrame，不知道有什么用，反正贴在代码里，万一有人用呢~</p><h2>1、代码</h2><p>代码较简单，如下</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">package</span> <span class=\"nn\">com.dkl.leanring.spark.df</span>\n\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.SparkSession</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.types._</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.Row</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.rdd.EmptyRDD</span>\n\n<span class=\"cm\">/**\n</span><span class=\"cm\"> * Spark创建空DataFrame示例\n</span><span class=\"cm\"> */</span>\n<span class=\"k\">object</span> <span class=\"nc\">EmptyDataFrame</span> <span class=\"o\">{</span>\n\n  <span class=\"k\">def</span> <span class=\"n\">main</span><span class=\"o\">(</span><span class=\"n\">args</span><span class=\"k\">:</span> <span class=\"kt\">Array</span><span class=\"o\">[</span><span class=\"kt\">String</span><span class=\"o\">])</span><span class=\"k\">:</span> <span class=\"kt\">Unit</span> <span class=\"o\">=</span> <span class=\"o\">{</span>\n\n    <span class=\"k\">val</span> <span class=\"n\">spark</span> <span class=\"k\">=</span> <span class=\"nc\">SparkSession</span><span class=\"o\">.</span><span class=\"n\">builder</span><span class=\"o\">().</span><span class=\"n\">appName</span><span class=\"o\">(</span><span class=\"s\">&#34;EmptyDataFrame&#34;</span><span class=\"o\">).</span><span class=\"n\">master</span><span class=\"o\">(</span><span class=\"s\">&#34;local&#34;</span><span class=\"o\">).</span><span class=\"n\">getOrCreate</span><span class=\"o\">()</span>\n\n    <span class=\"cm\">/**\n</span><span class=\"cm\">     * 创建一个空的DataFrame，代表用户\n</span><span class=\"cm\">     * 有四列，分别代表ID、名字、年龄、生日\n</span><span class=\"cm\">     */</span>\n    <span class=\"k\">val</span> <span class=\"n\">colNames</span> <span class=\"k\">=</span> <span class=\"nc\">Array</span><span class=\"o\">(</span><span class=\"s\">&#34;id&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;name&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;age&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;birth&#34;</span><span class=\"o\">)</span>\n    <span class=\"c1\">//为了简单起见，字段类型都为String\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">schema</span> <span class=\"k\">=</span> <span class=\"nc\">StructType</span><span class=\"o\">(</span><span class=\"n\">colNames</span><span class=\"o\">.</span><span class=\"n\">map</span><span class=\"o\">(</span><span class=\"n\">fieldName</span> <span class=\"k\">=&gt;</span> <span class=\"nc\">StructField</span><span class=\"o\">(</span><span class=\"n\">fieldName</span><span class=\"o\">,</span> <span class=\"nc\">StringType</span><span class=\"o\">,</span> <span class=\"kc\">true</span><span class=\"o\">)))</span>\n    <span class=\"c1\">//主要是利用了spark.sparkContext.emptyRDD\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">emptyDf</span> <span class=\"k\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"o\">(</span><span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">sparkContext</span><span class=\"o\">.</span><span class=\"n\">emptyRDD</span><span class=\"o\">[</span><span class=\"kt\">Row</span><span class=\"o\">],</span> <span class=\"n\">schema</span><span class=\"o\">)</span>\n\n    <span class=\"n\">emptyDf</span><span class=\"o\">.</span><span class=\"n\">show</span>\n\n    <span class=\"cm\">/**\n</span><span class=\"cm\">     * 也可以给每列指定相对应的类型\n</span><span class=\"cm\">     */</span>\n    <span class=\"k\">val</span> <span class=\"n\">schema1</span> <span class=\"k\">=</span> <span class=\"nc\">StructType</span><span class=\"o\">(</span>\n      <span class=\"nc\">Seq</span><span class=\"o\">(</span>\n        <span class=\"nc\">StructField</span><span class=\"o\">(</span><span class=\"s\">&#34;id&#34;</span><span class=\"o\">,</span> <span class=\"nc\">IntegerType</span><span class=\"o\">,</span> <span class=\"kc\">true</span><span class=\"o\">),</span>\n        <span class=\"nc\">StructField</span><span class=\"o\">(</span><span class=\"s\">&#34;name&#34;</span><span class=\"o\">,</span> <span class=\"nc\">StringType</span><span class=\"o\">,</span> <span class=\"kc\">true</span><span class=\"o\">),</span>\n        <span class=\"nc\">StructField</span><span class=\"o\">(</span><span class=\"s\">&#34;age&#34;</span><span class=\"o\">,</span> <span class=\"nc\">IntegerType</span><span class=\"o\">,</span> <span class=\"kc\">true</span><span class=\"o\">),</span>\n        <span class=\"nc\">StructField</span><span class=\"o\">(</span><span class=\"s\">&#34;birth&#34;</span><span class=\"o\">,</span> <span class=\"nc\">StringType</span><span class=\"o\">,</span> <span class=\"kc\">true</span><span class=\"o\">)))</span>\n    <span class=\"k\">val</span> <span class=\"n\">emptyDf1</span> <span class=\"k\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"o\">(</span><span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">sparkContext</span><span class=\"o\">.</span><span class=\"n\">emptyRDD</span><span class=\"o\">[</span><span class=\"kt\">Row</span><span class=\"o\">],</span> <span class=\"n\">schema1</span><span class=\"o\">)</span>\n    <span class=\"n\">emptyDf1</span><span class=\"o\">.</span><span class=\"n\">show</span>\n\n    <span class=\"c1\">//还有一种空的DataFrame，没有任何行任何列\n</span><span class=\"c1\"></span>    <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">emptyDataFrame</span><span class=\"o\">.</span><span class=\"n\">show</span>\n\n    <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">stop</span><span class=\"o\">()</span>\n  <span class=\"o\">}</span>\n\n<span class=\"o\">}</span></code></pre></div><h2>2、结果</h2><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-be5143b1fd0084ef55ed35dc19dd9146_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1667\" data-rawheight=\"920\" class=\"origin_image zh-lightbox-thumb\" width=\"1667\" data-original=\"https://pic3.zhimg.com/v2-be5143b1fd0084ef55ed35dc19dd9146_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1667&#39; height=&#39;920&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1667\" data-rawheight=\"920\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1667\" data-original=\"https://pic3.zhimg.com/v2-be5143b1fd0084ef55ed35dc19dd9146_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-be5143b1fd0084ef55ed35dc19dd9146_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "Spark", 
                    "tagLink": "https://api.zhihu.com/topics/19942170"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/51585745", 
            "userName": "董可伦", 
            "userLink": "https://www.zhihu.com/people/de888cc734ddbfd918269a8e5fb196ba", 
            "upvote": 2, 
            "title": "Spark 创建RDD、DataFrame各种情况的默认分区数", 
            "content": "<p>我的原创地址：</p><a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/08/13/sparkDefaultPartitionNums/\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-42ef0fb85289687c04af68404c3b3e3c_180x120.jpg\" data-image-width=\"900\" data-image-height=\"171\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Spark 创建RDD、DataFrame各种情况的默认分区数</a><h2>前言</h2><p>熟悉Spark的分区对于Spark性能调优很重要，本文总结Spark通过各种函数创建RDD、DataFrame时默认的分区数，其中主要和sc.defaultParallelism、sc.defaultMinPartitions以及HDFS文件的Block数量有关，还有很坑的某些情况的默认分区数为1。</p><ul><li>如果分区数少，那么并行执行的task就少，特别情况下，分区数为1，即使你分配的Executor很多，而实际执行的Executor只有1个，如果数据很大的话，那么任务执行的就很慢，好像是卡死了~，所以熟悉各种情况下默认的分区数对于Spark调优就很有必要了，特别是执行完算子返回的结果分区数为1的情况，更需要特别注意。（我就被坑过，我已经分配了足够多的Executor、默认的并行度、以及执行之前的数据集分区数，但分区数依然为1）</li></ul><h2>1、关于 sc.defaultMinPartitions</h2><p>sc.defaultMinPartitions=min(sc.defaultParallelism,2) 也就是sc.defaultMinPartitions只有两个值1和2，当sc.defaultParallelism&gt;1时值为2，当sc.defaultParallelism=1时，值为1 上面的公式是在源码里定义的（均在类SparkContext里）：</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">def</span> <span class=\"n\">defaultMinPartitions</span><span class=\"k\">:</span> <span class=\"kt\">Int</span> <span class=\"o\">=</span> <span class=\"n\">math</span><span class=\"o\">.</span><span class=\"n\">min</span><span class=\"o\">(</span><span class=\"n\">defaultParallelism</span><span class=\"o\">,</span> <span class=\"mi\">2</span><span class=\"o\">)</span>\n<span class=\"k\">def</span> <span class=\"n\">defaultParallelism</span><span class=\"k\">:</span> <span class=\"kt\">Int</span> <span class=\"o\">=</span> <span class=\"o\">{</span>\n    <span class=\"n\">assertNotStopped</span><span class=\"o\">()</span>\n    <span class=\"n\">taskScheduler</span><span class=\"o\">.</span><span class=\"n\">defaultParallelism</span>\n  <span class=\"o\">}</span></code></pre></div><h2>2、关于sc.defaultParallelism</h2><h2>2.1 首先可通过spark.default.parallelism设置sc.defaultParallelism的值</h2><h2>2.1.1 在文件中配置</h2><p>在文件spark-defaults.conf添加一行（这里用的我的windows环境） * spark.default.parallelism=20 验证： 在spark-shell里输入sc.defaultParallelism，输出结果为20 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-15d9c2d5202be60224677cb3821cedcf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"966\" data-rawheight=\"389\" class=\"origin_image zh-lightbox-thumb\" width=\"966\" data-original=\"https://pic4.zhimg.com/v2-15d9c2d5202be60224677cb3821cedcf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;966&#39; height=&#39;389&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"966\" data-rawheight=\"389\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"966\" data-original=\"https://pic4.zhimg.com/v2-15d9c2d5202be60224677cb3821cedcf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-15d9c2d5202be60224677cb3821cedcf_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>2.1.2 在代码里配置</h2><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">val</span> <span class=\"n\">spark</span> <span class=\"k\">=</span> <span class=\"nc\">SparkSession</span><span class=\"o\">.</span><span class=\"n\">builder</span><span class=\"o\">()</span>\n  <span class=\"o\">.</span><span class=\"n\">appName</span><span class=\"o\">(</span><span class=\"s\">&#34;TestPartitionNums&#34;</span><span class=\"o\">)</span>\n  <span class=\"o\">.</span><span class=\"n\">master</span><span class=\"o\">(</span><span class=\"s\">&#34;local&#34;</span><span class=\"o\">)</span>\n  <span class=\"o\">.</span><span class=\"n\">config</span><span class=\"o\">(</span><span class=\"s\">&#34;spark.default.parallelism&#34;</span><span class=\"o\">,</span> <span class=\"mi\">20</span><span class=\"o\">)</span>\n  <span class=\"o\">.</span><span class=\"n\">getOrCreate</span><span class=\"o\">()</span>\n\n<span class=\"k\">val</span> <span class=\"n\">sc</span> <span class=\"k\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">sparkContext</span>\n<span class=\"n\">println</span><span class=\"o\">(</span><span class=\"n\">sc</span><span class=\"o\">.</span><span class=\"n\">defaultParallelism</span><span class=\"o\">)</span>\n\n<span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">stop</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-924cc2d9edf164e29ba7ddd91376cfc8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1136\" data-rawheight=\"579\" class=\"origin_image zh-lightbox-thumb\" width=\"1136\" data-original=\"https://pic1.zhimg.com/v2-924cc2d9edf164e29ba7ddd91376cfc8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1136&#39; height=&#39;579&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1136\" data-rawheight=\"579\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1136\" data-original=\"https://pic1.zhimg.com/v2-924cc2d9edf164e29ba7ddd91376cfc8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-924cc2d9edf164e29ba7ddd91376cfc8_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>2.1.3 spark-submit配置</h2><p>通过--conf spark.default.parallelism=20即可</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"n\">spark</span><span class=\"o\">-</span><span class=\"n\">submit</span> <span class=\"o\">--</span><span class=\"n\">conf</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">default</span><span class=\"o\">.</span><span class=\"n\">parallelism</span><span class=\"k\">=</span><span class=\"mi\">160</span> <span class=\"o\">...</span></code></pre></div><h2>2.2 没有配置spark.default.parallelism时的默认值</h2><h2>2.2.1 spark-shell</h2><p>spark-shell里的值等于cpu的核数，比如我的windows的cpu的核数为4 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b1329ef35c9e05e222ff47115303462d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1340\" data-rawheight=\"588\" class=\"origin_image zh-lightbox-thumb\" width=\"1340\" data-original=\"https://pic2.zhimg.com/v2-b1329ef35c9e05e222ff47115303462d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1340&#39; height=&#39;588&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1340\" data-rawheight=\"588\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1340\" data-original=\"https://pic2.zhimg.com/v2-b1329ef35c9e05e222ff47115303462d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-b1329ef35c9e05e222ff47115303462d_b.jpg\"/></figure><p> 再比如测试机的核数为8 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-38347276fb559ee64f3755098645d7d1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"595\" data-rawheight=\"325\" class=\"origin_image zh-lightbox-thumb\" width=\"595\" data-original=\"https://pic2.zhimg.com/v2-38347276fb559ee64f3755098645d7d1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;595&#39; height=&#39;325&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"595\" data-rawheight=\"325\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"595\" data-original=\"https://pic2.zhimg.com/v2-38347276fb559ee64f3755098645d7d1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-38347276fb559ee64f3755098645d7d1_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>2.2.2 指定master为local</h2><ul><li>注：在spark-shell里通过--master local和在代码里通过.master(&#34;local&#34;)的结果是一样的，这里以spark-shell为例 当master为local时，值为1，当master为local[n]时，值为n </li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-8d8ffa8f15e9bc8ff8ecc276c0a51960_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"684\" data-rawheight=\"583\" class=\"origin_image zh-lightbox-thumb\" width=\"684\" data-original=\"https://pic1.zhimg.com/v2-8d8ffa8f15e9bc8ff8ecc276c0a51960_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;684&#39; height=&#39;583&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"684\" data-rawheight=\"583\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"684\" data-original=\"https://pic1.zhimg.com/v2-8d8ffa8f15e9bc8ff8ecc276c0a51960_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-8d8ffa8f15e9bc8ff8ecc276c0a51960_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>2.2.3 master为local[*]和不指定master(2.2.1)一样，都为cpu核数</h2><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-5b730164e52d9945329a6797b875e73f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"630\" data-rawheight=\"297\" class=\"origin_image zh-lightbox-thumb\" width=\"630\" data-original=\"https://pic4.zhimg.com/v2-5b730164e52d9945329a6797b875e73f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;630&#39; height=&#39;297&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"630\" data-rawheight=\"297\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"630\" data-original=\"https://pic4.zhimg.com/v2-5b730164e52d9945329a6797b875e73f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-5b730164e52d9945329a6797b875e73f_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>2.2.4 master为yarn</h2><p>master为yarn模式时为分配的所有的Executor的cpu核数的总和或者2，两者取最大值，将2.1.2的代码的master注释掉并打包，然后用下面的脚本执行测试 test.sh</p><div class=\"highlight\"><pre><code class=\"language-text\">spark-submit --num-executors $1 --executor-cores 1 --executor-memory 640M --master yarn   --class  com.dkl.leanring.spark.TestPartitionNums   spark-scala_2.11-1.0.jar</code></pre></div><ul><li>之所用这种方式不用spark-shell是因为这种方式截图的话，占得空间比较小</li><li>因为yarn模式时使用的cpu核数为虚拟的cpu核数，和实际cpu的核数有偏差，具体应该和yarn的配置有关，而且根据结果，每次申请的实际的cpu核数不完全一样，这里没有去深究原因 </li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-15ebc79510148fe166402359c4c6aa32_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"572\" data-rawheight=\"119\" class=\"origin_image zh-lightbox-thumb\" width=\"572\" data-original=\"https://pic3.zhimg.com/v2-15ebc79510148fe166402359c4c6aa32_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;572&#39; height=&#39;119&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"572\" data-rawheight=\"119\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"572\" data-original=\"https://pic3.zhimg.com/v2-15ebc79510148fe166402359c4c6aa32_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-15ebc79510148fe166402359c4c6aa32_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>2.2.5 Standalone、其他集群模式</h2><p>因本人工作用yarn模式，Standalone和其他模式没法在这里截图验证了，根据网上的资料，应该和yarn模式默认值是一样的。</p><h2>3、HDFS文件的默认分区</h2><p>这里及后面讨论的是rdd和dataframe的分区，也就是读取hdfs文件并不会改变前面讲的sc.defaultParallelism和sc.defaultMinPartitions的值。</p><h2>3.1 sc.textFile()</h2><p>rdd的分区数 = max(hdfs文件的block数目, sc.defaultMinPartitions)</p><h2>3.1.1 测试大文件（block的数量大于2）</h2><p>这里我上传了一个1.52G的txt到hdfs上用来测试，其中每个block的大小为默认的128M，那么该文件有13个分区 <i> 1.52</i>1024/128=12.16 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-cd665c5467346ae15afbf0f4d1966aff_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1276\" data-rawheight=\"360\" class=\"origin_image zh-lightbox-thumb\" width=\"1276\" data-original=\"https://pic4.zhimg.com/v2-cd665c5467346ae15afbf0f4d1966aff_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1276&#39; height=&#39;360&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1276\" data-rawheight=\"360\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1276\" data-original=\"https://pic4.zhimg.com/v2-cd665c5467346ae15afbf0f4d1966aff_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-cd665c5467346ae15afbf0f4d1966aff_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-71f734a8ef35b8f04929ec4c00263915_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1187\" data-rawheight=\"564\" class=\"origin_image zh-lightbox-thumb\" width=\"1187\" data-original=\"https://pic2.zhimg.com/v2-71f734a8ef35b8f04929ec4c00263915_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1187&#39; height=&#39;564&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1187\" data-rawheight=\"564\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1187\" data-original=\"https://pic2.zhimg.com/v2-71f734a8ef35b8f04929ec4c00263915_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-71f734a8ef35b8f04929ec4c00263915_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>用下面代码可以测试读取hdfs文件的分区数</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">val</span> <span class=\"n\">rdd</span> <span class=\"k\">=</span> <span class=\"n\">sc</span><span class=\"o\">.</span><span class=\"n\">textFile</span><span class=\"o\">(</span><span class=\"s\">&#34;hdfs://ambari.master.com/data/egaosu/txt/20180416.txt&#34;</span><span class=\"o\">)</span>\n<span class=\"n\">rdd</span><span class=\"o\">.</span><span class=\"n\">rdd</span><span class=\"o\">.</span><span class=\"n\">getNumPartitions</span></code></pre></div><p>这种方式无论是sc.defaultParallelism大于block数还是sc.defaultParallelism小于block数，rdd的默认分区数都为block数 * 注：之所以说是默认分区，因为textFile可以指定分区数，sc.textFile(path, minPartitions)，通过第二个参数可以指定分区数 sc.defaultMinPartitions大于block数 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-abb49842de62c65058b903e615aa5c02_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"998\" data-rawheight=\"428\" class=\"origin_image zh-lightbox-thumb\" width=\"998\" data-original=\"https://pic3.zhimg.com/v2-abb49842de62c65058b903e615aa5c02_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;998&#39; height=&#39;428&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"998\" data-rawheight=\"428\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"998\" data-original=\"https://pic3.zhimg.com/v2-abb49842de62c65058b903e615aa5c02_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-abb49842de62c65058b903e615aa5c02_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>sc.defaultMinPartitions小于block数 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-dbe60a637e2a4b5903d6832f95ea551a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1011\" data-rawheight=\"426\" class=\"origin_image zh-lightbox-thumb\" width=\"1011\" data-original=\"https://pic3.zhimg.com/v2-dbe60a637e2a4b5903d6832f95ea551a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1011&#39; height=&#39;426&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1011\" data-rawheight=\"426\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1011\" data-original=\"https://pic3.zhimg.com/v2-dbe60a637e2a4b5903d6832f95ea551a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-dbe60a637e2a4b5903d6832f95ea551a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>当用参数指定分区数时，有两种情况，当参数大于block数时，则rdd的分区数为指定的参数值，否则分区数为block数 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-111a6f6b3d82d792defdee7544948272_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1166\" data-rawheight=\"182\" class=\"origin_image zh-lightbox-thumb\" width=\"1166\" data-original=\"https://pic3.zhimg.com/v2-111a6f6b3d82d792defdee7544948272_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1166&#39; height=&#39;182&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1166\" data-rawheight=\"182\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1166\" data-original=\"https://pic3.zhimg.com/v2-111a6f6b3d82d792defdee7544948272_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-111a6f6b3d82d792defdee7544948272_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>3.1.2 测试小文件（block数量等于1）</h2><p>这种情况的默认分区数为sc.defaultMinPartitions，下面是对应的hdfs文件： </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-afc0d4ba3b0e124308ade816b95c912a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1330\" data-rawheight=\"719\" class=\"origin_image zh-lightbox-thumb\" width=\"1330\" data-original=\"https://pic3.zhimg.com/v2-afc0d4ba3b0e124308ade816b95c912a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1330&#39; height=&#39;719&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1330\" data-rawheight=\"719\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1330\" data-original=\"https://pic3.zhimg.com/v2-afc0d4ba3b0e124308ade816b95c912a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-afc0d4ba3b0e124308ade816b95c912a_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-cfa470dd47707dc79201e2d5f8e17acb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1213\" data-rawheight=\"568\" class=\"origin_image zh-lightbox-thumb\" width=\"1213\" data-original=\"https://pic4.zhimg.com/v2-cfa470dd47707dc79201e2d5f8e17acb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1213&#39; height=&#39;568&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1213\" data-rawheight=\"568\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1213\" data-original=\"https://pic4.zhimg.com/v2-cfa470dd47707dc79201e2d5f8e17acb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-cfa470dd47707dc79201e2d5f8e17acb_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>将上面的hdfs路径改为：hdfs://<a href=\"https://link.zhihu.com/?target=http%3A//ambari.master.com/tmp/dkl/data.txt\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">ambari.master.com/tmp/d</span><span class=\"invisible\">kl/data.txt</span><span class=\"ellipsis\"></span></a>，结果： </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-8c57481233259391910c8886c21accb3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"922\" data-rawheight=\"126\" class=\"origin_image zh-lightbox-thumb\" width=\"922\" data-original=\"https://pic4.zhimg.com/v2-8c57481233259391910c8886c21accb3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;922&#39; height=&#39;126&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"922\" data-rawheight=\"126\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"922\" data-original=\"https://pic4.zhimg.com/v2-8c57481233259391910c8886c21accb3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-8c57481233259391910c8886c21accb3_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-a63ba0b59d0ba0195e869fa1a45ad821_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"922\" data-rawheight=\"126\" class=\"origin_image zh-lightbox-thumb\" width=\"922\" data-original=\"https://pic2.zhimg.com/v2-a63ba0b59d0ba0195e869fa1a45ad821_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;922&#39; height=&#39;126&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"922\" data-rawheight=\"126\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"922\" data-original=\"https://pic2.zhimg.com/v2-a63ba0b59d0ba0195e869fa1a45ad821_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-a63ba0b59d0ba0195e869fa1a45ad821_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>当用参数指定分区数时，rdd的分区数大于等于参数值，本次测试为等于参数值或参数值+1 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-41ea0fe0bf11aa94cc3caa674244fc63_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1145\" data-rawheight=\"256\" class=\"origin_image zh-lightbox-thumb\" width=\"1145\" data-original=\"https://pic4.zhimg.com/v2-41ea0fe0bf11aa94cc3caa674244fc63_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1145&#39; height=&#39;256&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1145\" data-rawheight=\"256\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1145\" data-original=\"https://pic4.zhimg.com/v2-41ea0fe0bf11aa94cc3caa674244fc63_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-41ea0fe0bf11aa94cc3caa674244fc63_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>3.2 spark.read.csv()</h2><p>大文件（block较多）：df的分区数 = max(hdfs文件的block数目, sc.defaultParallelism) 小文件（本次测试的block为1）：df的分区数=1，也就是和sc.defaultParallelism无关(一般小文件也没必要用很多分区，一个分区很快就可以处理完成)</p><h2>3.2.1 大文件</h2><p>文件大小8.98G,block数为72 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-eb9b8ff11206ee71ab778a689b787a7b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"291\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic4.zhimg.com/v2-eb9b8ff11206ee71ab778a689b787a7b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;291&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"291\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic4.zhimg.com/v2-eb9b8ff11206ee71ab778a689b787a7b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-eb9b8ff11206ee71ab778a689b787a7b_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-7359bd1dd40ba5af90ea95b458e3cd76_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"585\" class=\"origin_image zh-lightbox-thumb\" width=\"1200\" data-original=\"https://pic3.zhimg.com/v2-7359bd1dd40ba5af90ea95b458e3cd76_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1200&#39; height=&#39;585&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1200\" data-rawheight=\"585\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1200\" data-original=\"https://pic3.zhimg.com/v2-7359bd1dd40ba5af90ea95b458e3cd76_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-7359bd1dd40ba5af90ea95b458e3cd76_b.jpg\"/></figure><p> 读取代码：</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">val</span> <span class=\"n\">df</span> <span class=\"k\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">read</span><span class=\"o\">.</span><span class=\"n\">option</span><span class=\"o\">(</span><span class=\"s\">&#34;header&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;true&#34;</span><span class=\"o\">).</span><span class=\"n\">csv</span><span class=\"o\">(</span><span class=\"s\">&#34;hdfs://ambari.master.com//data/etc_t/etc_t_consumewaste201801.csv&#34;</span><span class=\"o\">)</span></code></pre></div><p>分区数 1、当sc.defaultParallelism小于block，分区数默认为block数：72 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-6e61eeea95c55a943a01157175c98539_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"900\" data-rawheight=\"131\" class=\"origin_image zh-lightbox-thumb\" width=\"900\" data-original=\"https://pic2.zhimg.com/v2-6e61eeea95c55a943a01157175c98539_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;900&#39; height=&#39;131&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"900\" data-rawheight=\"131\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"900\" data-original=\"https://pic2.zhimg.com/v2-6e61eeea95c55a943a01157175c98539_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-6e61eeea95c55a943a01157175c98539_b.jpg\"/></figure><p> 2、当sc.defaultParallelism大于于block，分区数默认为sc.defaultParallelism </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-59c3d3aeedac0ceffa43f55280d040c2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"900\" data-rawheight=\"135\" class=\"origin_image zh-lightbox-thumb\" width=\"900\" data-original=\"https://pic3.zhimg.com/v2-59c3d3aeedac0ceffa43f55280d040c2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;900&#39; height=&#39;135&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"900\" data-rawheight=\"135\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"900\" data-original=\"https://pic3.zhimg.com/v2-59c3d3aeedac0ceffa43f55280d040c2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-59c3d3aeedac0ceffa43f55280d040c2_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>3.2.2 小文件（1个block）</h2><p>分区数为1 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-2dd36eade67fbf7b38d7a6739bc2b680_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1198\" data-rawheight=\"251\" class=\"origin_image zh-lightbox-thumb\" width=\"1198\" data-original=\"https://pic1.zhimg.com/v2-2dd36eade67fbf7b38d7a6739bc2b680_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1198&#39; height=&#39;251&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1198\" data-rawheight=\"251\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1198\" data-original=\"https://pic1.zhimg.com/v2-2dd36eade67fbf7b38d7a6739bc2b680_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-2dd36eade67fbf7b38d7a6739bc2b680_b.jpg\"/></figure><p> 读取代码：</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">val</span> <span class=\"n\">df</span> <span class=\"k\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">read</span><span class=\"o\">.</span><span class=\"n\">option</span><span class=\"o\">(</span><span class=\"s\">&#34;header&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;true&#34;</span><span class=\"o\">).</span><span class=\"n\">csv</span><span class=\"o\">(</span><span class=\"s\">&#34;hdfs://ambari.master.com//data/etc_t/etc_sale_desc.csv&#34;</span><span class=\"o\">)</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-fa1cd76271869d5acc7fd4f8aeada109_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"900\" data-rawheight=\"131\" class=\"origin_image zh-lightbox-thumb\" width=\"900\" data-original=\"https://pic2.zhimg.com/v2-fa1cd76271869d5acc7fd4f8aeada109_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;900&#39; height=&#39;131&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"900\" data-rawheight=\"131\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"900\" data-original=\"https://pic2.zhimg.com/v2-fa1cd76271869d5acc7fd4f8aeada109_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-fa1cd76271869d5acc7fd4f8aeada109_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>3.3 测试读取hive表创建的DataFrame的分区数</h2><p>下面是该表的hdfs路径，从下面的图可以看出该表对应的hdfs文件的block的数目为10（2*5） </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-4f98a088566d210d89953c7d11fb59cf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1325\" data-rawheight=\"586\" class=\"origin_image zh-lightbox-thumb\" width=\"1325\" data-original=\"https://pic4.zhimg.com/v2-4f98a088566d210d89953c7d11fb59cf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1325&#39; height=&#39;586&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1325\" data-rawheight=\"586\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1325\" data-original=\"https://pic4.zhimg.com/v2-4f98a088566d210d89953c7d11fb59cf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-4f98a088566d210d89953c7d11fb59cf_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>用下面的代码测试：</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"c1\">//切换数据库\n</span><span class=\"c1\"></span><span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">(</span><span class=\"s\">&#34;use route_analysis&#34;</span><span class=\"o\">)</span>\n<span class=\"c1\">//读取该数据库下的egaosu表为df\n</span><span class=\"c1\"></span><span class=\"k\">val</span> <span class=\"n\">df</span> <span class=\"k\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">table</span><span class=\"o\">(</span><span class=\"s\">&#34;egaosu&#34;</span><span class=\"o\">)</span>\n<span class=\"c1\">//打印df对应的rdd的分区数\n</span><span class=\"c1\"></span><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">rdd</span><span class=\"o\">.</span><span class=\"n\">getNumPartitions</span></code></pre></div><p>测试发现，当sc.defaultParallelism大于block时，df的分区是等于sc.defaultParallelism，当小于block时，df的分区数介于sc.defaultParallelism和block之间，至于详细的分配策略，我还没有查到~ </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b32ecf56f93c804cb519aab47574657d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"687\" data-rawheight=\"397\" class=\"origin_image zh-lightbox-thumb\" width=\"687\" data-original=\"https://pic2.zhimg.com/v2-b32ecf56f93c804cb519aab47574657d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;687&#39; height=&#39;397&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"687\" data-rawheight=\"397\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"687\" data-original=\"https://pic2.zhimg.com/v2-b32ecf56f93c804cb519aab47574657d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-b32ecf56f93c804cb519aab47574657d_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-316f9db548a85afbde8789340b0692fa_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"670\" data-rawheight=\"405\" class=\"origin_image zh-lightbox-thumb\" width=\"670\" data-original=\"https://pic3.zhimg.com/v2-316f9db548a85afbde8789340b0692fa_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;670&#39; height=&#39;405&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"670\" data-rawheight=\"405\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"670\" data-original=\"https://pic3.zhimg.com/v2-316f9db548a85afbde8789340b0692fa_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-316f9db548a85afbde8789340b0692fa_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-c677e713777049ec7603b015297907c5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"676\" data-rawheight=\"394\" class=\"origin_image zh-lightbox-thumb\" width=\"676\" data-original=\"https://pic2.zhimg.com/v2-c677e713777049ec7603b015297907c5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;676&#39; height=&#39;394&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"676\" data-rawheight=\"394\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"676\" data-original=\"https://pic2.zhimg.com/v2-c677e713777049ec7603b015297907c5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-c677e713777049ec7603b015297907c5_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><ul><li> 用spark.sql(&#34;select * from egaosu&#34;)这种方式得到df和上面的分区数是一样的<br/> </li><li> 上面讲的是我经常使用的几种读取hdfs文件的方法，至于利用其他方法读取hdfs文件的默认的分区，大家可以自己测试（比如json文件，因我没有比较大的hdfs文件就不做测试了）<br/> </li></ul><h2>4、非HDFS文件的默认分区（本地文件）</h2><ul><li>实际工作中用到本地文件的情况很少，一般都是hdfs、关系型数据库和代码里的集合</li></ul><h2>4.1 sc.textFile()</h2><h2>4.1.1 大文件</h2><p>文件大小为1142M，经测试本地文件也会像hdfs一样进行类似于block的划分，固定按32M来分片（这里的32M参考<a href=\"https://link.zhihu.com/?target=https%3A//www.jianshu.com/p/4b7d07e754fa\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Spark RDD的默认分区数：（spark 2.1.0）</a>） </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-bb612f256e099856e43afba5a67b8850_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"690\" data-rawheight=\"35\" class=\"origin_image zh-lightbox-thumb\" width=\"690\" data-original=\"https://pic1.zhimg.com/v2-bb612f256e099856e43afba5a67b8850_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;690&#39; height=&#39;35&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"690\" data-rawheight=\"35\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"690\" data-original=\"https://pic1.zhimg.com/v2-bb612f256e099856e43afba5a67b8850_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-bb612f256e099856e43afba5a67b8850_b.jpg\"/></figure><p> 所以应该默认有36个分区（1142/32=35.6875） 当用参数值指定时，参数小于block时，分区数为block的数目，大于block时，分区数为参数值，即分区数 = max(本地文件的block数目, 参数值) 读取代码：</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">val</span> <span class=\"n\">rdd</span> <span class=\"k\">=</span> <span class=\"n\">sc</span><span class=\"o\">.</span><span class=\"n\">textFile</span><span class=\"o\">(</span><span class=\"s\">&#34;file:///root/dkl/170102.txt&#34;</span><span class=\"o\">)</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-dcb24dcb1e1ff1d4dfb7ef4f6c03c9aa_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"690\" data-rawheight=\"205\" class=\"origin_image zh-lightbox-thumb\" width=\"690\" data-original=\"https://pic3.zhimg.com/v2-dcb24dcb1e1ff1d4dfb7ef4f6c03c9aa_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;690&#39; height=&#39;205&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"690\" data-rawheight=\"205\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"690\" data-original=\"https://pic3.zhimg.com/v2-dcb24dcb1e1ff1d4dfb7ef4f6c03c9aa_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-dcb24dcb1e1ff1d4dfb7ef4f6c03c9aa_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>4.1.2 小文件</h2><p>默认的分区数为sc.defaultMinPartitions 新建一个测试文件text.txt,内容自己造几行 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-74e65f3092423b4fafe5cbca50c06ff4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"900\" data-rawheight=\"356\" class=\"origin_image zh-lightbox-thumb\" width=\"900\" data-original=\"https://pic1.zhimg.com/v2-74e65f3092423b4fafe5cbca50c06ff4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;900&#39; height=&#39;356&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"900\" data-rawheight=\"356\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"900\" data-original=\"https://pic1.zhimg.com/v2-74e65f3092423b4fafe5cbca50c06ff4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-74e65f3092423b4fafe5cbca50c06ff4_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-1e5cc04e1b15c1b55accf0e12ec9aea5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"900\" data-rawheight=\"356\" class=\"origin_image zh-lightbox-thumb\" width=\"900\" data-original=\"https://pic2.zhimg.com/v2-1e5cc04e1b15c1b55accf0e12ec9aea5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;900&#39; height=&#39;356&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"900\" data-rawheight=\"356\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"900\" data-original=\"https://pic2.zhimg.com/v2-1e5cc04e1b15c1b55accf0e12ec9aea5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-1e5cc04e1b15c1b55accf0e12ec9aea5_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>当用参数指定分区数时，本以为分区数为指定的参数值，结果经测试，当参数值在一定的范围内分区数确实为指定的参数值，当参数值大于某个数值时，分区数实际比参数值大一点，不知道是不是Spark的bug还是有自己的策略。 读取代码：</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">val</span> <span class=\"n\">rdd</span> <span class=\"k\">=</span> <span class=\"n\">sc</span><span class=\"o\">.</span><span class=\"n\">textFile</span><span class=\"o\">(</span><span class=\"s\">&#34;file:///root/dkl/sh/test/test.txt&#34;</span><span class=\"o\">)</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-1ccedb1d618cedb6e86af52b4ce4be5a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"900\" data-rawheight=\"729\" class=\"origin_image zh-lightbox-thumb\" width=\"900\" data-original=\"https://pic3.zhimg.com/v2-1ccedb1d618cedb6e86af52b4ce4be5a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;900&#39; height=&#39;729&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"900\" data-rawheight=\"729\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"900\" data-original=\"https://pic3.zhimg.com/v2-1ccedb1d618cedb6e86af52b4ce4be5a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-1ccedb1d618cedb6e86af52b4ce4be5a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>4.2 spark.read.csv()</h2><p>规律和HDFS文件是一样的（见3.2），且按128M来分block，这里和上面讲的txt不一样，txt是按32M</p><h2>4.2.1 大文件</h2><p>1081M，那么block为9（1081/128），分区数 = max(本地文件的block数目, sc.defaultParallelism) </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-b80122aed751598a20414f51f5275e32_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"900\" data-rawheight=\"79\" class=\"origin_image zh-lightbox-thumb\" width=\"900\" data-original=\"https://pic3.zhimg.com/v2-b80122aed751598a20414f51f5275e32_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;900&#39; height=&#39;79&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"900\" data-rawheight=\"79\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"900\" data-original=\"https://pic3.zhimg.com/v2-b80122aed751598a20414f51f5275e32_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-b80122aed751598a20414f51f5275e32_b.jpg\"/></figure><p> 读取代码：</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">val</span> <span class=\"n\">df</span> <span class=\"k\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">read</span><span class=\"o\">.</span><span class=\"n\">option</span><span class=\"o\">(</span><span class=\"s\">&#34;header&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;true&#34;</span><span class=\"o\">).</span><span class=\"n\">csv</span><span class=\"o\">(</span><span class=\"s\">&#34;file:///root/dir/etc_t/etc_t_consumewaste20180614-0616.csv&#34;</span><span class=\"o\">)</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-070f6b538743097ff4f23e5433a0743b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"900\" data-rawheight=\"131\" class=\"origin_image zh-lightbox-thumb\" width=\"900\" data-original=\"https://pic4.zhimg.com/v2-070f6b538743097ff4f23e5433a0743b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;900&#39; height=&#39;131&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"900\" data-rawheight=\"131\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"900\" data-original=\"https://pic4.zhimg.com/v2-070f6b538743097ff4f23e5433a0743b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-070f6b538743097ff4f23e5433a0743b_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-eb2270d35c7a413b2a636ad8fd561404_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"900\" data-rawheight=\"121\" class=\"origin_image zh-lightbox-thumb\" width=\"900\" data-original=\"https://pic1.zhimg.com/v2-eb2270d35c7a413b2a636ad8fd561404_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;900&#39; height=&#39;121&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"900\" data-rawheight=\"121\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"900\" data-original=\"https://pic1.zhimg.com/v2-eb2270d35c7a413b2a636ad8fd561404_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-eb2270d35c7a413b2a636ad8fd561404_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>4.2.2 小文件</h2><p>大小6K，block为1，分区数为1 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-f0e2adeb1a43d82c3eb5be24c09bc6f4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"900\" data-rawheight=\"117\" class=\"origin_image zh-lightbox-thumb\" width=\"900\" data-original=\"https://pic1.zhimg.com/v2-f0e2adeb1a43d82c3eb5be24c09bc6f4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;900&#39; height=&#39;117&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"900\" data-rawheight=\"117\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"900\" data-original=\"https://pic1.zhimg.com/v2-f0e2adeb1a43d82c3eb5be24c09bc6f4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-f0e2adeb1a43d82c3eb5be24c09bc6f4_b.jpg\"/></figure><p> 读取代码：</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">val</span> <span class=\"n\">df</span> <span class=\"k\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">read</span><span class=\"o\">.</span><span class=\"n\">option</span><span class=\"o\">(</span><span class=\"s\">&#34;header&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;true&#34;</span><span class=\"o\">).</span><span class=\"n\">csv</span><span class=\"o\">(</span><span class=\"s\">&#34;file:///root/dkl/sh/test/test.csv&#34;</span><span class=\"o\">)</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-59ab320ae36ff8186f80bf2ee9b45e22_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"900\" data-rawheight=\"403\" class=\"origin_image zh-lightbox-thumb\" width=\"900\" data-original=\"https://pic3.zhimg.com/v2-59ab320ae36ff8186f80bf2ee9b45e22_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;900&#39; height=&#39;403&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"900\" data-rawheight=\"403\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"900\" data-original=\"https://pic3.zhimg.com/v2-59ab320ae36ff8186f80bf2ee9b45e22_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-59ab320ae36ff8186f80bf2ee9b45e22_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>5、关系型数据库</h2><p>从关系型数据库表读取的df的分区数为1，以mysql为例，我这里拿一张1000万条数据的表进行测试 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-0bfe5dc31587f10e34b14e29d426bf90_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"900\" data-rawheight=\"160\" class=\"origin_image zh-lightbox-thumb\" width=\"900\" data-original=\"https://pic1.zhimg.com/v2-0bfe5dc31587f10e34b14e29d426bf90_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;900&#39; height=&#39;160&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"900\" data-rawheight=\"160\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"900\" data-original=\"https://pic1.zhimg.com/v2-0bfe5dc31587f10e34b14e29d426bf90_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-0bfe5dc31587f10e34b14e29d426bf90_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-6401515e52d4054efbcc56b6cfe34eeb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1315\" data-rawheight=\"825\" class=\"origin_image zh-lightbox-thumb\" width=\"1315\" data-original=\"https://pic4.zhimg.com/v2-6401515e52d4054efbcc56b6cfe34eeb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1315&#39; height=&#39;825&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1315\" data-rawheight=\"825\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1315\" data-original=\"https://pic4.zhimg.com/v2-6401515e52d4054efbcc56b6cfe34eeb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-6401515e52d4054efbcc56b6cfe34eeb_b.jpg\"/></figure><p> Spark连接mysql的代码都可以参考<a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/03/21/sparkMysql/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Spark Sql 连接mysql</a> * 设置df的默认分区数也可以参考<a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/03/21/sparkMysql/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Spark Sql 连接mysql</a></p><h2>6、从代码里的内部数据集创建</h2><h2>6.1 sc.parallelize()创建RDD</h2><p>默认分区数等于sc.defaultParallelism，指定参数时分区数值等于参数值。</p><h2>6.2 spark.createDataFrame(data)创建DataFrame</h2><p>当data的长度小于sc.defaultParallelism，分区数等于data长度，否则分区数等于sc.defaultParallelism</p><p>如图： </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ef9e9539d5f3c6836bb7b1e39d934d2b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1618\" data-rawheight=\"895\" class=\"origin_image zh-lightbox-thumb\" width=\"1618\" data-original=\"https://pic4.zhimg.com/v2-ef9e9539d5f3c6836bb7b1e39d934d2b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1618&#39; height=&#39;895&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1618\" data-rawheight=\"895\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1618\" data-original=\"https://pic4.zhimg.com/v2-ef9e9539d5f3c6836bb7b1e39d934d2b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-ef9e9539d5f3c6836bb7b1e39d934d2b_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>6.3 代码</h2><p>下面是上面图中的代码：</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">package</span> <span class=\"nn\">com.dkl.leanring.spark</span>\n\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.SparkSession</span>\n\n<span class=\"k\">object</span> <span class=\"nc\">TestPartitionNums</span> <span class=\"o\">{</span>\n  <span class=\"k\">def</span> <span class=\"n\">main</span><span class=\"o\">(</span><span class=\"n\">args</span><span class=\"k\">:</span> <span class=\"kt\">Array</span><span class=\"o\">[</span><span class=\"kt\">String</span><span class=\"o\">])</span><span class=\"k\">:</span> <span class=\"kt\">Unit</span> <span class=\"o\">=</span> <span class=\"o\">{</span>\n    <span class=\"k\">val</span> <span class=\"n\">spark</span> <span class=\"k\">=</span> <span class=\"nc\">SparkSession</span><span class=\"o\">.</span><span class=\"n\">builder</span><span class=\"o\">()</span>\n      <span class=\"o\">.</span><span class=\"n\">appName</span><span class=\"o\">(</span><span class=\"s\">&#34;TestPartitionNums&#34;</span><span class=\"o\">)</span>\n      <span class=\"o\">.</span><span class=\"n\">master</span><span class=\"o\">(</span><span class=\"s\">&#34;local&#34;</span><span class=\"o\">)</span>\n      <span class=\"o\">.</span><span class=\"n\">config</span><span class=\"o\">(</span><span class=\"s\">&#34;spark.default.parallelism&#34;</span><span class=\"o\">,</span> <span class=\"mi\">8</span><span class=\"o\">)</span>\n      <span class=\"o\">.</span><span class=\"n\">getOrCreate</span><span class=\"o\">()</span>\n\n    <span class=\"k\">val</span> <span class=\"n\">sc</span> <span class=\"k\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">sparkContext</span>\n\n    <span class=\"n\">println</span><span class=\"o\">(</span><span class=\"s\">&#34;默认的并行度: &#34;</span> <span class=\"o\">+</span> <span class=\"n\">sc</span><span class=\"o\">.</span><span class=\"n\">defaultParallelism</span><span class=\"o\">)</span>\n\n    <span class=\"n\">println</span><span class=\"o\">(</span><span class=\"s\">&#34;sc.parallelize 默认分区：&#34;</span> <span class=\"o\">+</span> <span class=\"n\">sc</span><span class=\"o\">.</span><span class=\"n\">parallelize</span><span class=\"o\">(</span><span class=\"mi\">1</span> <span class=\"n\">to</span> <span class=\"mi\">30</span><span class=\"o\">).</span><span class=\"n\">getNumPartitions</span><span class=\"o\">)</span>\n    <span class=\"n\">println</span><span class=\"o\">(</span><span class=\"s\">&#34;sc.parallelize 参数指定，参数大于sc.defaultParallelism时：&#34;</span> <span class=\"o\">+</span> <span class=\"n\">sc</span><span class=\"o\">.</span><span class=\"n\">parallelize</span><span class=\"o\">(</span><span class=\"mi\">1</span> <span class=\"n\">to</span> <span class=\"mi\">30</span><span class=\"o\">,</span> <span class=\"mi\">100</span><span class=\"o\">).</span><span class=\"n\">getNumPartitions</span><span class=\"o\">)</span>\n    <span class=\"n\">println</span><span class=\"o\">(</span><span class=\"s\">&#34;sc.parallelize  参数指定，参数小于sc.defaultParallelism时：&#34;</span> <span class=\"o\">+</span> <span class=\"n\">sc</span><span class=\"o\">.</span><span class=\"n\">parallelize</span><span class=\"o\">(</span><span class=\"mi\">1</span> <span class=\"n\">to</span> <span class=\"mi\">30</span><span class=\"o\">,</span> <span class=\"mi\">3</span><span class=\"o\">).</span><span class=\"n\">getNumPartitions</span><span class=\"o\">)</span>\n\n    <span class=\"k\">var</span> <span class=\"n\">data</span> <span class=\"k\">=</span> <span class=\"nc\">Seq</span><span class=\"o\">((</span><span class=\"mi\">1</span><span class=\"o\">,</span> <span class=\"mi\">2</span><span class=\"o\">),</span> <span class=\"o\">(</span><span class=\"mi\">1</span><span class=\"o\">,</span> <span class=\"mi\">2</span><span class=\"o\">),</span> <span class=\"o\">(</span><span class=\"mi\">1</span><span class=\"o\">,</span> <span class=\"mi\">2</span><span class=\"o\">),</span> <span class=\"o\">(</span><span class=\"mi\">1</span><span class=\"o\">,</span> <span class=\"mi\">2</span><span class=\"o\">),</span> <span class=\"o\">(</span><span class=\"mi\">1</span><span class=\"o\">,</span> <span class=\"mi\">2</span><span class=\"o\">))</span>\n\n    <span class=\"n\">println</span><span class=\"o\">(</span><span class=\"s\">&#34;spark.createDataFrame data的长度小于sc.defaultParallelism时，长度：&#34;</span> <span class=\"o\">+</span> <span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">length</span> <span class=\"o\">+</span> <span class=\"s\">&#34; 分区数：&#34;</span> <span class=\"o\">+</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"o\">(</span><span class=\"n\">data</span><span class=\"o\">).</span><span class=\"n\">rdd</span><span class=\"o\">.</span><span class=\"n\">getNumPartitions</span><span class=\"o\">)</span>\n    <span class=\"n\">data</span> <span class=\"k\">=</span> <span class=\"nc\">Seq</span><span class=\"o\">((</span><span class=\"mi\">1</span><span class=\"o\">,</span> <span class=\"mi\">2</span><span class=\"o\">),</span> <span class=\"o\">(</span><span class=\"mi\">1</span><span class=\"o\">,</span> <span class=\"mi\">2</span><span class=\"o\">),</span> <span class=\"o\">(</span><span class=\"mi\">1</span><span class=\"o\">,</span> <span class=\"mi\">2</span><span class=\"o\">),</span> <span class=\"o\">(</span><span class=\"mi\">1</span><span class=\"o\">,</span> <span class=\"mi\">2</span><span class=\"o\">),</span> <span class=\"o\">(</span><span class=\"mi\">1</span><span class=\"o\">,</span> <span class=\"mi\">2</span><span class=\"o\">),</span> <span class=\"o\">(</span><span class=\"mi\">1</span><span class=\"o\">,</span> <span class=\"mi\">2</span><span class=\"o\">),</span> <span class=\"o\">(</span><span class=\"mi\">1</span><span class=\"o\">,</span> <span class=\"mi\">2</span><span class=\"o\">),</span> <span class=\"o\">(</span><span class=\"mi\">1</span><span class=\"o\">,</span> <span class=\"mi\">2</span><span class=\"o\">),</span> <span class=\"o\">(</span><span class=\"mi\">1</span><span class=\"o\">,</span> <span class=\"mi\">2</span><span class=\"o\">),</span> <span class=\"o\">(</span><span class=\"mi\">1</span><span class=\"o\">,</span> <span class=\"mi\">2</span><span class=\"o\">),</span> <span class=\"o\">(</span><span class=\"mi\">1</span><span class=\"o\">,</span> <span class=\"mi\">2</span><span class=\"o\">),</span> <span class=\"o\">(</span><span class=\"mi\">1</span><span class=\"o\">,</span> <span class=\"mi\">2</span><span class=\"o\">))</span>\n    <span class=\"n\">println</span><span class=\"o\">(</span><span class=\"s\">&#34;spark.createDataFrame data的长度大于sc.defaultParallelism时，长度：&#34;</span> <span class=\"o\">+</span> <span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">length</span> <span class=\"o\">+</span> <span class=\"s\">&#34; 分区数：&#34;</span> <span class=\"o\">+</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"o\">(</span><span class=\"n\">data</span><span class=\"o\">).</span><span class=\"n\">rdd</span><span class=\"o\">.</span><span class=\"n\">getNumPartitions</span><span class=\"o\">)</span>\n\n    <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">stop</span>\n  <span class=\"o\">}</span>\n<span class=\"o\">}</span></code></pre></div><h2>7、其他改变分区数的算子</h2><h2>7.1 分区数为1</h2><p>上面已经讲过几个分区数为1（当默认的并行度大于1时）的情况： 1、spark.read.csv()读取本地文件 2、读取关系型数据库表 上面是从外部数据源加载进来就为1的情况，还有就是对df或rdd进行转换操作之后的分区数为1的情况： 1、df.limit(n)</p><h2>7.2 分区数不为1的情况</h2><p>df.distinct()分区数为200</p><p>如图： </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ab0845db66b290629f98744974b661a0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"900\" data-rawheight=\"171\" class=\"origin_image zh-lightbox-thumb\" width=\"900\" data-original=\"https://pic1.zhimg.com/v2-ab0845db66b290629f98744974b661a0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;900&#39; height=&#39;171&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"900\" data-rawheight=\"171\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"900\" data-original=\"https://pic1.zhimg.com/v2-ab0845db66b290629f98744974b661a0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-ab0845db66b290629f98744974b661a0_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>8、合理的设置分区数</h2><p>根据自己集群的情况和数据大小等合理设置分区的数目，对于Spark性能调优很有必要，根据前面讲的可知，可通过配置spark.default.parallelism、传参设置分区数，遇到那些分区数为1的特殊算子可以利用repartition()进行重新分区即可。</p><h2>9、总结</h2><p>本文首先讲了各种情况下的sc.defaultParallelism，defaultMinPartitions，然后讲了各种情况下创建以及转化RDD、DataFrame的分区数，因为Spark的外部数据源很多，创建以及转化RDD、DataFrame的方法和算子也很多，所以主要是讲了我个人常用的各种情况，并不能包含所有情况，至于其他情况，大家可以自己测试总结。还有一点就是本文并没有从源码的层次去分析，只是总结一些规律，对于前面提到的一些还不太清楚的规律，以后如果有时间的话可以从源码的层次去分析为什么~</p>", 
            "topic": [
                {
                    "tag": "Spark", 
                    "tagLink": "https://api.zhihu.com/topics/19942170"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/51501349", 
            "userName": "董可伦", 
            "userLink": "https://www.zhihu.com/people/de888cc734ddbfd918269a8e5fb196ba", 
            "upvote": 2, 
            "title": "Spark UDF使用详解及代码示例", 
            "content": "<p>我的原创地址：</p><a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/08/02/sparkUDF/\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">dongkelun.com/2018/08/0</span><span class=\"invisible\">2/sparkUDF/</span><span class=\"ellipsis\"></span></a><h2>前言</h2><p>本文介绍如何在Spark Sql和DataFrame中使用UDF，如何利用UDF给一个表或者一个DataFrame根据需求添加几列，并给出了旧版（Spark1.x）和新版（Spark2.x）完整的代码示例。</p><ul><li>关于UDF：UDF：User Defined Function，用户自定义函数。</li></ul><h2>1、创建测试用DataFrame</h2><p>下面以Spark2.x为例给出代码，关于Spark1.x创建DataFrame可在最后的完整代码里查看。</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"c1\">// 构造测试数据，有两个字段、名字和年龄\n</span><span class=\"c1\"></span><span class=\"k\">val</span> <span class=\"n\">userData</span> <span class=\"k\">=</span> <span class=\"nc\">Array</span><span class=\"o\">((</span><span class=\"s\">&#34;Leo&#34;</span><span class=\"o\">,</span> <span class=\"mi\">16</span><span class=\"o\">),</span> <span class=\"o\">(</span><span class=\"s\">&#34;Marry&#34;</span><span class=\"o\">,</span> <span class=\"mi\">21</span><span class=\"o\">),</span> <span class=\"o\">(</span><span class=\"s\">&#34;Jack&#34;</span><span class=\"o\">,</span> <span class=\"mi\">14</span><span class=\"o\">),</span> <span class=\"o\">(</span><span class=\"s\">&#34;Tom&#34;</span><span class=\"o\">,</span> <span class=\"mi\">18</span><span class=\"o\">))</span>\n\n<span class=\"c1\">//创建测试df\n</span><span class=\"c1\"></span><span class=\"k\">val</span> <span class=\"n\">userDF</span> <span class=\"k\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"o\">(</span><span class=\"n\">userData</span><span class=\"o\">).</span><span class=\"n\">toDF</span><span class=\"o\">(</span><span class=\"s\">&#34;name&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;age&#34;</span><span class=\"o\">)</span>\n<span class=\"n\">userDF</span><span class=\"o\">.</span><span class=\"n\">show</span>\n<span class=\"o\">+-----+---+</span>\n<span class=\"o\">|</span> <span class=\"n\">name</span><span class=\"o\">|</span><span class=\"n\">age</span><span class=\"o\">|</span>\n<span class=\"o\">+-----+---+</span>\n<span class=\"o\">|</span>  <span class=\"nc\">Leo</span><span class=\"o\">|</span> <span class=\"mi\">16</span><span class=\"o\">|</span>\n<span class=\"o\">|</span><span class=\"nc\">Marry</span><span class=\"o\">|</span> <span class=\"mi\">21</span><span class=\"o\">|</span>\n<span class=\"o\">|</span> <span class=\"nc\">Jack</span><span class=\"o\">|</span> <span class=\"mi\">14</span><span class=\"o\">|</span>\n<span class=\"o\">|</span>  <span class=\"nc\">Tom</span><span class=\"o\">|</span> <span class=\"mi\">18</span><span class=\"o\">|</span>\n<span class=\"o\">+-----+---+</span>\n<span class=\"c1\">// 注册一张user表\n</span><span class=\"c1\"></span><span class=\"n\">userDF</span><span class=\"o\">.</span><span class=\"n\">createOrReplaceTempView</span><span class=\"o\">(</span><span class=\"s\">&#34;user&#34;</span><span class=\"o\">)</span></code></pre></div><h2>2、Spark Sql用法</h2><h2>2.1 通过匿名函数注册UDF</h2><p>下面的UDF的功能是计算某列的长度，该列的类型为String</p><h2>2.1.1 注册</h2><ul><li>Spark2.x:</li></ul><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">udf</span><span class=\"o\">.</span><span class=\"n\">register</span><span class=\"o\">(</span><span class=\"s\">&#34;strLen&#34;</span><span class=\"o\">,</span> <span class=\"o\">(</span><span class=\"n\">str</span><span class=\"k\">:</span> <span class=\"kt\">String</span><span class=\"o\">)</span> <span class=\"k\">=&gt;</span> <span class=\"n\">str</span><span class=\"o\">.</span><span class=\"n\">length</span><span class=\"o\">())</span></code></pre></div><ul><li>Spark1.x:</li></ul><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"n\">sqlContext</span><span class=\"o\">.</span><span class=\"n\">udf</span><span class=\"o\">.</span><span class=\"n\">register</span><span class=\"o\">(</span><span class=\"s\">&#34;strLen&#34;</span><span class=\"o\">,</span> <span class=\"o\">(</span><span class=\"n\">str</span><span class=\"k\">:</span> <span class=\"kt\">String</span><span class=\"o\">)</span> <span class=\"k\">=&gt;</span> <span class=\"n\">str</span><span class=\"o\">.</span><span class=\"n\">length</span><span class=\"o\">())</span></code></pre></div><h2>2.2.2 使用</h2><p>仅以Spark2.x为例</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">(</span><span class=\"s\">&#34;select name,strLen(name) as name_len from user&#34;</span><span class=\"o\">).</span><span class=\"n\">show</span>\n<span class=\"o\">+-----+--------+</span>\n<span class=\"o\">|</span> <span class=\"n\">name</span><span class=\"o\">|</span><span class=\"n\">name_len</span><span class=\"o\">|</span>\n<span class=\"o\">+-----+--------+</span>\n<span class=\"o\">|</span>  <span class=\"nc\">Leo</span><span class=\"o\">|</span>       <span class=\"mi\">3</span><span class=\"o\">|</span>\n<span class=\"o\">|</span><span class=\"nc\">Marry</span><span class=\"o\">|</span>       <span class=\"mi\">5</span><span class=\"o\">|</span>\n<span class=\"o\">|</span> <span class=\"nc\">Jack</span><span class=\"o\">|</span>       <span class=\"mi\">4</span><span class=\"o\">|</span>\n<span class=\"o\">|</span>  <span class=\"nc\">Tom</span><span class=\"o\">|</span>       <span class=\"mi\">3</span><span class=\"o\">|</span>\n<span class=\"o\">+-----+--------+</span></code></pre></div><h2>2.2 通过实名函数注册UDF</h2><p>实名函数的注册有点不同，要在后面加 _(注意前面有个空格) 定义一个实名函数</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"cm\">/**\n</span><span class=\"cm\"> * 根据年龄大小返回是否成年 成年：true,未成年：false\n</span><span class=\"cm\">*/</span>\n<span class=\"k\">def</span> <span class=\"n\">isAdult</span><span class=\"o\">(</span><span class=\"n\">age</span><span class=\"k\">:</span> <span class=\"kt\">Int</span><span class=\"o\">)</span> <span class=\"k\">=</span> <span class=\"o\">{</span>\n  <span class=\"k\">if</span> <span class=\"o\">(</span><span class=\"n\">age</span> <span class=\"o\">&lt;</span> <span class=\"mi\">18</span><span class=\"o\">)</span> <span class=\"o\">{</span>\n    <span class=\"kc\">false</span>\n  <span class=\"o\">}</span> <span class=\"k\">else</span> <span class=\"o\">{</span>\n    <span class=\"kc\">true</span>\n  <span class=\"o\">}</span>\n\n<span class=\"o\">}</span></code></pre></div><p>注册（仅以Spark2.x为例）</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">udf</span><span class=\"o\">.</span><span class=\"n\">register</span><span class=\"o\">(</span><span class=\"s\">&#34;isAdult&#34;</span><span class=\"o\">,</span> <span class=\"n\">isAdult</span> <span class=\"k\">_</span><span class=\"o\">)</span></code></pre></div><p>至于使用都是一样的</p><h2>2.3 关于spark.udf和sqlContext.udf</h2><p>在Spark2.x里，两者实际最终都是调用的spark.udf sqlContext.udf源码</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">def</span> <span class=\"n\">udf</span><span class=\"k\">:</span> <span class=\"kt\">UDFRegistration</span> <span class=\"o\">=</span> <span class=\"n\">sparkSession</span><span class=\"o\">.</span><span class=\"n\">udf</span></code></pre></div><p>可以看到调用的是sparkSession的udf，即spark.udf</p><h2>3、DataFrame用法</h2><p>DataFrame的udf方法虽然和Spark Sql的名字一样，但是属于不同的类，它在org.apache.spark.sql.functions里，下面是它的用法</p><h2>3.1注册</h2><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.functions._</span>\n<span class=\"c1\">//注册自定义函数（通过匿名函数）\n</span><span class=\"c1\"></span><span class=\"k\">val</span> <span class=\"n\">strLen</span> <span class=\"k\">=</span> <span class=\"n\">udf</span><span class=\"o\">((</span><span class=\"n\">str</span><span class=\"k\">:</span> <span class=\"kt\">String</span><span class=\"o\">)</span> <span class=\"k\">=&gt;</span> <span class=\"n\">str</span><span class=\"o\">.</span><span class=\"n\">length</span><span class=\"o\">())</span>\n<span class=\"c1\">//注册自定义函数（通过实名函数）\n</span><span class=\"c1\"></span><span class=\"k\">val</span> <span class=\"n\">udf_isAdult</span> <span class=\"k\">=</span> <span class=\"n\">udf</span><span class=\"o\">(</span><span class=\"n\">isAdult</span> <span class=\"k\">_</span><span class=\"o\">)</span></code></pre></div><h2>3.2 使用</h2><p>可通过withColumn和select使用,下面的代码已经实现了给user表添加两列的功能 * 通过看源码，下面的withColumn和select方法Spark2.0.0之后才有的，关于spark1.xDataFrame怎么使用注册好的UDF没有研究</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"c1\">//通过withColumn添加列\n</span><span class=\"c1\"></span><span class=\"n\">userDF</span><span class=\"o\">.</span><span class=\"n\">withColumn</span><span class=\"o\">(</span><span class=\"s\">&#34;name_len&#34;</span><span class=\"o\">,</span> <span class=\"n\">strLen</span><span class=\"o\">(</span><span class=\"n\">col</span><span class=\"o\">(</span><span class=\"s\">&#34;name&#34;</span><span class=\"o\">))).</span><span class=\"n\">withColumn</span><span class=\"o\">(</span><span class=\"s\">&#34;isAdult&#34;</span><span class=\"o\">,</span> <span class=\"n\">udf_isAdult</span><span class=\"o\">(</span><span class=\"n\">col</span><span class=\"o\">(</span><span class=\"s\">&#34;age&#34;</span><span class=\"o\">))).</span><span class=\"n\">show</span>\n<span class=\"c1\">//通过select添加列\n</span><span class=\"c1\"></span><span class=\"n\">userDF</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"o\">(</span><span class=\"n\">col</span><span class=\"o\">(</span><span class=\"s\">&#34;*&#34;</span><span class=\"o\">),</span> <span class=\"n\">strLen</span><span class=\"o\">(</span><span class=\"n\">col</span><span class=\"o\">(</span><span class=\"s\">&#34;name&#34;</span><span class=\"o\">))</span> <span class=\"n\">as</span> <span class=\"s\">&#34;name_len&#34;</span><span class=\"o\">,</span> <span class=\"n\">udf_isAdult</span><span class=\"o\">(</span><span class=\"n\">col</span><span class=\"o\">(</span><span class=\"s\">&#34;age&#34;</span><span class=\"o\">))</span> <span class=\"n\">as</span> <span class=\"s\">&#34;isAdult&#34;</span><span class=\"o\">).</span><span class=\"n\">show</span></code></pre></div><p>结果均为</p><div class=\"highlight\"><pre><code class=\"language-vim\"><span class=\"p\">+-----+---+--------+-------+</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"p\">|</span> <span class=\"nx\">name</span><span class=\"p\">|</span><span class=\"nx\">age</span><span class=\"p\">|</span><span class=\"nx\">name_len</span><span class=\"p\">|</span><span class=\"nx\">isAdult</span><span class=\"p\">|</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"p\">+-----+---+--------+-------+</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"p\">|</span>  <span class=\"nx\">Leo</span><span class=\"p\">|</span> <span class=\"m\">16</span><span class=\"p\">|</span>       <span class=\"m\">3</span><span class=\"p\">|</span>  <span class=\"nx\">false</span><span class=\"p\">|</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"p\">|</span><span class=\"nx\">Marry</span><span class=\"p\">|</span> <span class=\"m\">21</span><span class=\"p\">|</span>       <span class=\"m\">5</span><span class=\"p\">|</span>   <span class=\"nx\">true</span><span class=\"p\">|</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"p\">|</span> <span class=\"nx\">Jack</span><span class=\"p\">|</span> <span class=\"m\">14</span><span class=\"p\">|</span>       <span class=\"m\">4</span><span class=\"p\">|</span>  <span class=\"nx\">false</span><span class=\"p\">|</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"p\">|</span>  <span class=\"nx\">Tom</span><span class=\"p\">|</span> <span class=\"m\">18</span><span class=\"p\">|</span>       <span class=\"m\">3</span><span class=\"p\">|</span>   <span class=\"nx\">true</span><span class=\"p\">|</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"p\">+-----+---+--------+-------+</span></code></pre></div><h2>3.3 withColumn和select的区别</h2><p>可通过withColumn的源码看出withColumn的功能是实现增加一列，或者替换一个已存在的列，他会先判断DataFrame里有没有这个列名，如果有的话就会替换掉原来的列，没有的话就用调用select方法增加一列，所以如果我们的需求是增加一列的话，两者实现的功能一样，且最终都是调用select方法，但是withColumn会提前做一些判断处理，所以withColumn的性能不如select好。 * 注：select方法和sql 里的select一样，如果新增的列名在表里已经存在，那么结果里允许出现两列列名相同但数据不一样，大家可以自己试一下。</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"cm\">/**\n</span><span class=\"cm\"> * Returns a new Dataset by adding a column or replacing the existing column that has\n</span><span class=\"cm\"> * the same name.\n</span><span class=\"cm\"> *\n</span><span class=\"cm\"> * @group untypedrel\n</span><span class=\"cm\"> * @since 2.0.0\n</span><span class=\"cm\">*/</span>\n<span class=\"k\">def</span> <span class=\"n\">withColumn</span><span class=\"o\">(</span><span class=\"n\">colName</span><span class=\"k\">:</span> <span class=\"kt\">String</span><span class=\"o\">,</span> <span class=\"n\">col</span><span class=\"k\">:</span> <span class=\"kt\">Column</span><span class=\"o\">)</span><span class=\"k\">:</span> <span class=\"kt\">DataFrame</span> <span class=\"o\">=</span> <span class=\"o\">{</span>\n  <span class=\"k\">val</span> <span class=\"n\">resolver</span> <span class=\"k\">=</span> <span class=\"n\">sparkSession</span><span class=\"o\">.</span><span class=\"n\">sessionState</span><span class=\"o\">.</span><span class=\"n\">analyzer</span><span class=\"o\">.</span><span class=\"n\">resolver</span>\n  <span class=\"k\">val</span> <span class=\"n\">output</span> <span class=\"k\">=</span> <span class=\"n\">queryExecution</span><span class=\"o\">.</span><span class=\"n\">analyzed</span><span class=\"o\">.</span><span class=\"n\">output</span>\n  <span class=\"k\">val</span> <span class=\"n\">shouldReplace</span> <span class=\"k\">=</span> <span class=\"n\">output</span><span class=\"o\">.</span><span class=\"n\">exists</span><span class=\"o\">(</span><span class=\"n\">f</span> <span class=\"k\">=&gt;</span> <span class=\"n\">resolver</span><span class=\"o\">(</span><span class=\"n\">f</span><span class=\"o\">.</span><span class=\"n\">name</span><span class=\"o\">,</span> <span class=\"n\">colName</span><span class=\"o\">))</span>\n  <span class=\"k\">if</span> <span class=\"o\">(</span><span class=\"n\">shouldReplace</span><span class=\"o\">)</span> <span class=\"o\">{</span>\n    <span class=\"k\">val</span> <span class=\"n\">columns</span> <span class=\"k\">=</span> <span class=\"n\">output</span><span class=\"o\">.</span><span class=\"n\">map</span> <span class=\"o\">{</span> <span class=\"n\">field</span> <span class=\"k\">=&gt;</span>\n      <span class=\"k\">if</span> <span class=\"o\">(</span><span class=\"n\">resolver</span><span class=\"o\">(</span><span class=\"n\">field</span><span class=\"o\">.</span><span class=\"n\">name</span><span class=\"o\">,</span> <span class=\"n\">colName</span><span class=\"o\">))</span> <span class=\"o\">{</span>\n        <span class=\"n\">col</span><span class=\"o\">.</span><span class=\"n\">as</span><span class=\"o\">(</span><span class=\"n\">colName</span><span class=\"o\">)</span>\n      <span class=\"o\">}</span> <span class=\"k\">else</span> <span class=\"o\">{</span>\n        <span class=\"nc\">Column</span><span class=\"o\">(</span><span class=\"n\">field</span><span class=\"o\">)</span>\n      <span class=\"o\">}</span>\n    <span class=\"o\">}</span>\n    <span class=\"n\">select</span><span class=\"o\">(</span><span class=\"n\">columns</span> <span class=\"k\">:</span> <span class=\"k\">_</span><span class=\"kt\">*</span><span class=\"o\">)</span>\n  <span class=\"o\">}</span> <span class=\"k\">else</span> <span class=\"o\">{</span>\n    <span class=\"n\">select</span><span class=\"o\">(</span><span class=\"nc\">Column</span><span class=\"o\">(</span><span class=\"s\">&#34;*&#34;</span><span class=\"o\">),</span> <span class=\"n\">col</span><span class=\"o\">.</span><span class=\"n\">as</span><span class=\"o\">(</span><span class=\"n\">colName</span><span class=\"o\">))</span>\n  <span class=\"o\">}</span>\n<span class=\"o\">}</span></code></pre></div><h2>4、完整代码</h2><p>下面的代码的功能是使用UDF给user表添加两列:name_len、isAdult，每个输出结果都是一样的</p><div class=\"highlight\"><pre><code class=\"language-vim\"><span class=\"p\">+-----+---+--------+-------+</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"p\">|</span> <span class=\"nx\">name</span><span class=\"p\">|</span><span class=\"nx\">age</span><span class=\"p\">|</span><span class=\"nx\">name_len</span><span class=\"p\">|</span><span class=\"nx\">isAdult</span><span class=\"p\">|</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"p\">+-----+---+--------+-------+</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"p\">|</span>  <span class=\"nx\">Leo</span><span class=\"p\">|</span> <span class=\"m\">16</span><span class=\"p\">|</span>       <span class=\"m\">3</span><span class=\"p\">|</span>  <span class=\"nx\">false</span><span class=\"p\">|</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"p\">|</span><span class=\"nx\">Marry</span><span class=\"p\">|</span> <span class=\"m\">21</span><span class=\"p\">|</span>       <span class=\"m\">5</span><span class=\"p\">|</span>   <span class=\"nx\">true</span><span class=\"p\">|</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"p\">|</span> <span class=\"nx\">Jack</span><span class=\"p\">|</span> <span class=\"m\">14</span><span class=\"p\">|</span>       <span class=\"m\">4</span><span class=\"p\">|</span>  <span class=\"nx\">false</span><span class=\"p\">|</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"p\">|</span>  <span class=\"nx\">Tom</span><span class=\"p\">|</span> <span class=\"m\">18</span><span class=\"p\">|</span>       <span class=\"m\">3</span><span class=\"p\">|</span>   <span class=\"nx\">true</span><span class=\"p\">|</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"p\">+-----+---+--------+-------+</span></code></pre></div><p>代码：</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">package</span> <span class=\"nn\">com.dkl.leanring.spark.sql</span>\n\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.SparkContext</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.SQLContext</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.SparkConf</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.SparkSession</span>\n\n<span class=\"cm\">/**\n</span><span class=\"cm\"> * Spark Sql 用户自定义函数示例\n</span><span class=\"cm\"> */</span>\n<span class=\"k\">object</span> <span class=\"nc\">UdfDemo</span> <span class=\"o\">{</span>\n\n  <span class=\"k\">def</span> <span class=\"n\">main</span><span class=\"o\">(</span><span class=\"n\">args</span><span class=\"k\">:</span> <span class=\"kt\">Array</span><span class=\"o\">[</span><span class=\"kt\">String</span><span class=\"o\">])</span><span class=\"k\">:</span> <span class=\"kt\">Unit</span> <span class=\"o\">=</span> <span class=\"o\">{</span>\n    <span class=\"n\">oldUdf</span>\n    <span class=\"n\">newUdf</span>\n    <span class=\"n\">newDfUdf</span>\n    <span class=\"n\">oldDfUdf</span>\n  <span class=\"o\">}</span>\n\n  <span class=\"cm\">/**\n</span><span class=\"cm\">   * 根据年龄大小返回是否成年 成年：true,未成年：false\n</span><span class=\"cm\">   */</span>\n  <span class=\"k\">def</span> <span class=\"n\">isAdult</span><span class=\"o\">(</span><span class=\"n\">age</span><span class=\"k\">:</span> <span class=\"kt\">Int</span><span class=\"o\">)</span> <span class=\"k\">=</span> <span class=\"o\">{</span>\n    <span class=\"k\">if</span> <span class=\"o\">(</span><span class=\"n\">age</span> <span class=\"o\">&lt;</span> <span class=\"mi\">18</span><span class=\"o\">)</span> <span class=\"o\">{</span>\n      <span class=\"kc\">false</span>\n    <span class=\"o\">}</span> <span class=\"k\">else</span> <span class=\"o\">{</span>\n      <span class=\"kc\">true</span>\n    <span class=\"o\">}</span>\n\n  <span class=\"o\">}</span>\n\n  <span class=\"cm\">/**\n</span><span class=\"cm\">   * 旧版本(Spark1.x)Spark Sql udf示例\n</span><span class=\"cm\">   */</span>\n  <span class=\"k\">def</span> <span class=\"n\">oldUdf</span><span class=\"o\">()</span> <span class=\"o\">{</span>\n\n    <span class=\"c1\">//spark 初始化\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">conf</span> <span class=\"k\">=</span> <span class=\"k\">new</span> <span class=\"nc\">SparkConf</span><span class=\"o\">()</span>\n      <span class=\"o\">.</span><span class=\"n\">setMaster</span><span class=\"o\">(</span><span class=\"s\">&#34;local&#34;</span><span class=\"o\">)</span>\n      <span class=\"o\">.</span><span class=\"n\">setAppName</span><span class=\"o\">(</span><span class=\"s\">&#34;oldUdf&#34;</span><span class=\"o\">)</span>\n    <span class=\"k\">val</span> <span class=\"n\">sc</span> <span class=\"k\">=</span> <span class=\"k\">new</span> <span class=\"nc\">SparkContext</span><span class=\"o\">(</span><span class=\"n\">conf</span><span class=\"o\">)</span>\n    <span class=\"k\">val</span> <span class=\"n\">sqlContext</span> <span class=\"k\">=</span> <span class=\"k\">new</span> <span class=\"nc\">SQLContext</span><span class=\"o\">(</span><span class=\"n\">sc</span><span class=\"o\">)</span>\n    <span class=\"k\">import</span> <span class=\"nn\">sqlContext.implicits._</span>\n\n    <span class=\"c1\">// 构造测试数据，有两个字段、名字和年龄\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">userData</span> <span class=\"k\">=</span> <span class=\"nc\">Array</span><span class=\"o\">((</span><span class=\"s\">&#34;Leo&#34;</span><span class=\"o\">,</span> <span class=\"mi\">16</span><span class=\"o\">),</span> <span class=\"o\">(</span><span class=\"s\">&#34;Marry&#34;</span><span class=\"o\">,</span> <span class=\"mi\">21</span><span class=\"o\">),</span> <span class=\"o\">(</span><span class=\"s\">&#34;Jack&#34;</span><span class=\"o\">,</span> <span class=\"mi\">14</span><span class=\"o\">),</span> <span class=\"o\">(</span><span class=\"s\">&#34;Tom&#34;</span><span class=\"o\">,</span> <span class=\"mi\">18</span><span class=\"o\">))</span>\n    <span class=\"c1\">//创建测试df\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">userDF</span> <span class=\"k\">=</span> <span class=\"n\">sc</span><span class=\"o\">.</span><span class=\"n\">parallelize</span><span class=\"o\">(</span><span class=\"n\">userData</span><span class=\"o\">).</span><span class=\"n\">toDF</span><span class=\"o\">(</span><span class=\"s\">&#34;name&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;age&#34;</span><span class=\"o\">)</span>\n    <span class=\"c1\">// 注册一张user表\n</span><span class=\"c1\"></span>    <span class=\"n\">userDF</span><span class=\"o\">.</span><span class=\"n\">registerTempTable</span><span class=\"o\">(</span><span class=\"s\">&#34;user&#34;</span><span class=\"o\">)</span>\n\n    <span class=\"c1\">// 注册自定义函数（通过匿名函数）\n</span><span class=\"c1\"></span>    <span class=\"n\">sqlContext</span><span class=\"o\">.</span><span class=\"n\">udf</span><span class=\"o\">.</span><span class=\"n\">register</span><span class=\"o\">(</span><span class=\"s\">&#34;strLen&#34;</span><span class=\"o\">,</span> <span class=\"o\">(</span><span class=\"n\">str</span><span class=\"k\">:</span> <span class=\"kt\">String</span><span class=\"o\">)</span> <span class=\"k\">=&gt;</span> <span class=\"n\">str</span><span class=\"o\">.</span><span class=\"n\">length</span><span class=\"o\">())</span>\n\n    <span class=\"n\">sqlContext</span><span class=\"o\">.</span><span class=\"n\">udf</span><span class=\"o\">.</span><span class=\"n\">register</span><span class=\"o\">(</span><span class=\"s\">&#34;isAdult&#34;</span><span class=\"o\">,</span> <span class=\"n\">isAdult</span> <span class=\"k\">_</span><span class=\"o\">)</span>\n    <span class=\"c1\">// 使用自定义函数\n</span><span class=\"c1\"></span>    <span class=\"n\">sqlContext</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">(</span><span class=\"s\">&#34;select *,strLen(name)as name_len,isAdult(age) as isAdult from user&#34;</span><span class=\"o\">).</span><span class=\"n\">show</span>\n    <span class=\"c1\">//关闭\n</span><span class=\"c1\"></span>    <span class=\"n\">sc</span><span class=\"o\">.</span><span class=\"n\">stop</span><span class=\"o\">()</span>\n\n  <span class=\"o\">}</span>\n\n  <span class=\"cm\">/**\n</span><span class=\"cm\">   * 新版本(Spark2.x)Spark Sql udf示例\n</span><span class=\"cm\">   */</span>\n  <span class=\"k\">def</span> <span class=\"n\">newUdf</span><span class=\"o\">()</span> <span class=\"o\">{</span>\n    <span class=\"c1\">//spark初始化\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">spark</span> <span class=\"k\">=</span> <span class=\"nc\">SparkSession</span><span class=\"o\">.</span><span class=\"n\">builder</span><span class=\"o\">().</span><span class=\"n\">appName</span><span class=\"o\">(</span><span class=\"s\">&#34;newUdf&#34;</span><span class=\"o\">).</span><span class=\"n\">master</span><span class=\"o\">(</span><span class=\"s\">&#34;local&#34;</span><span class=\"o\">).</span><span class=\"n\">getOrCreate</span><span class=\"o\">()</span>\n\n    <span class=\"c1\">// 构造测试数据，有两个字段、名字和年龄\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">userData</span> <span class=\"k\">=</span> <span class=\"nc\">Array</span><span class=\"o\">((</span><span class=\"s\">&#34;Leo&#34;</span><span class=\"o\">,</span> <span class=\"mi\">16</span><span class=\"o\">),</span> <span class=\"o\">(</span><span class=\"s\">&#34;Marry&#34;</span><span class=\"o\">,</span> <span class=\"mi\">21</span><span class=\"o\">),</span> <span class=\"o\">(</span><span class=\"s\">&#34;Jack&#34;</span><span class=\"o\">,</span> <span class=\"mi\">14</span><span class=\"o\">),</span> <span class=\"o\">(</span><span class=\"s\">&#34;Tom&#34;</span><span class=\"o\">,</span> <span class=\"mi\">18</span><span class=\"o\">))</span>\n\n    <span class=\"c1\">//创建测试df\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">userDF</span> <span class=\"k\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"o\">(</span><span class=\"n\">userData</span><span class=\"o\">).</span><span class=\"n\">toDF</span><span class=\"o\">(</span><span class=\"s\">&#34;name&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;age&#34;</span><span class=\"o\">)</span>\n\n    <span class=\"c1\">// 注册一张user表\n</span><span class=\"c1\"></span>    <span class=\"n\">userDF</span><span class=\"o\">.</span><span class=\"n\">createOrReplaceTempView</span><span class=\"o\">(</span><span class=\"s\">&#34;user&#34;</span><span class=\"o\">)</span>\n\n    <span class=\"c1\">//注册自定义函数（通过匿名函数）\n</span><span class=\"c1\"></span>    <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">udf</span><span class=\"o\">.</span><span class=\"n\">register</span><span class=\"o\">(</span><span class=\"s\">&#34;strLen&#34;</span><span class=\"o\">,</span> <span class=\"o\">(</span><span class=\"n\">str</span><span class=\"k\">:</span> <span class=\"kt\">String</span><span class=\"o\">)</span> <span class=\"k\">=&gt;</span> <span class=\"n\">str</span><span class=\"o\">.</span><span class=\"n\">length</span><span class=\"o\">())</span>\n    <span class=\"c1\">//注册自定义函数（通过实名函数）\n</span><span class=\"c1\"></span>    <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">udf</span><span class=\"o\">.</span><span class=\"n\">register</span><span class=\"o\">(</span><span class=\"s\">&#34;isAdult&#34;</span><span class=\"o\">,</span> <span class=\"n\">isAdult</span> <span class=\"k\">_</span><span class=\"o\">)</span>\n    <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">(</span><span class=\"s\">&#34;select *,strLen(name) as name_len,isAdult(age) as isAdult from user&#34;</span><span class=\"o\">).</span><span class=\"n\">show</span>\n\n    <span class=\"c1\">//关闭\n</span><span class=\"c1\"></span>    <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">stop</span><span class=\"o\">()</span>\n\n  <span class=\"o\">}</span>\n\n  <span class=\"cm\">/**\n</span><span class=\"cm\">   * 新版本(Spark2.x)DataFrame udf示例\n</span><span class=\"cm\">   */</span>\n  <span class=\"k\">def</span> <span class=\"n\">newDfUdf</span><span class=\"o\">()</span> <span class=\"o\">{</span>\n    <span class=\"k\">val</span> <span class=\"n\">spark</span> <span class=\"k\">=</span> <span class=\"nc\">SparkSession</span><span class=\"o\">.</span><span class=\"n\">builder</span><span class=\"o\">().</span><span class=\"n\">appName</span><span class=\"o\">(</span><span class=\"s\">&#34;newDfUdf&#34;</span><span class=\"o\">).</span><span class=\"n\">master</span><span class=\"o\">(</span><span class=\"s\">&#34;local&#34;</span><span class=\"o\">).</span><span class=\"n\">getOrCreate</span><span class=\"o\">()</span>\n\n    <span class=\"c1\">// 构造测试数据，有两个字段、名字和年龄\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">userData</span> <span class=\"k\">=</span> <span class=\"nc\">Array</span><span class=\"o\">((</span><span class=\"s\">&#34;Leo&#34;</span><span class=\"o\">,</span> <span class=\"mi\">16</span><span class=\"o\">),</span> <span class=\"o\">(</span><span class=\"s\">&#34;Marry&#34;</span><span class=\"o\">,</span> <span class=\"mi\">21</span><span class=\"o\">),</span> <span class=\"o\">(</span><span class=\"s\">&#34;Jack&#34;</span><span class=\"o\">,</span> <span class=\"mi\">14</span><span class=\"o\">),</span> <span class=\"o\">(</span><span class=\"s\">&#34;Tom&#34;</span><span class=\"o\">,</span> <span class=\"mi\">18</span><span class=\"o\">))</span>\n\n    <span class=\"c1\">//创建测试df\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">userDF</span> <span class=\"k\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"o\">(</span><span class=\"n\">userData</span><span class=\"o\">).</span><span class=\"n\">toDF</span><span class=\"o\">(</span><span class=\"s\">&#34;name&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;age&#34;</span><span class=\"o\">)</span>\n    <span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.functions._</span>\n    <span class=\"c1\">//注册自定义函数（通过匿名函数）\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">strLen</span> <span class=\"k\">=</span> <span class=\"n\">udf</span><span class=\"o\">((</span><span class=\"n\">str</span><span class=\"k\">:</span> <span class=\"kt\">String</span><span class=\"o\">)</span> <span class=\"k\">=&gt;</span> <span class=\"n\">str</span><span class=\"o\">.</span><span class=\"n\">length</span><span class=\"o\">())</span>\n    <span class=\"c1\">//注册自定义函数（通过实名函数）\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">udf_isAdult</span> <span class=\"k\">=</span> <span class=\"n\">udf</span><span class=\"o\">(</span><span class=\"n\">isAdult</span> <span class=\"k\">_</span><span class=\"o\">)</span>\n\n    <span class=\"c1\">//通过withColumn添加列\n</span><span class=\"c1\"></span>    <span class=\"n\">userDF</span><span class=\"o\">.</span><span class=\"n\">withColumn</span><span class=\"o\">(</span><span class=\"s\">&#34;name_len&#34;</span><span class=\"o\">,</span> <span class=\"n\">strLen</span><span class=\"o\">(</span><span class=\"n\">col</span><span class=\"o\">(</span><span class=\"s\">&#34;name&#34;</span><span class=\"o\">))).</span><span class=\"n\">withColumn</span><span class=\"o\">(</span><span class=\"s\">&#34;isAdult&#34;</span><span class=\"o\">,</span> <span class=\"n\">udf_isAdult</span><span class=\"o\">(</span><span class=\"n\">col</span><span class=\"o\">(</span><span class=\"s\">&#34;age&#34;</span><span class=\"o\">))).</span><span class=\"n\">show</span>\n    <span class=\"c1\">//通过select添加列\n</span><span class=\"c1\"></span>    <span class=\"n\">userDF</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"o\">(</span><span class=\"n\">col</span><span class=\"o\">(</span><span class=\"s\">&#34;*&#34;</span><span class=\"o\">),</span> <span class=\"n\">strLen</span><span class=\"o\">(</span><span class=\"n\">col</span><span class=\"o\">(</span><span class=\"s\">&#34;name&#34;</span><span class=\"o\">))</span> <span class=\"n\">as</span> <span class=\"s\">&#34;name_len&#34;</span><span class=\"o\">,</span> <span class=\"n\">udf_isAdult</span><span class=\"o\">(</span><span class=\"n\">col</span><span class=\"o\">(</span><span class=\"s\">&#34;age&#34;</span><span class=\"o\">))</span> <span class=\"n\">as</span> <span class=\"s\">&#34;isAdult&#34;</span><span class=\"o\">).</span><span class=\"n\">show</span>\n\n    <span class=\"c1\">//关闭\n</span><span class=\"c1\"></span>    <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">stop</span><span class=\"o\">()</span>\n  <span class=\"o\">}</span>\n  <span class=\"cm\">/**\n</span><span class=\"cm\">   * 旧版本(Spark1.x)DataFrame udf示例\n</span><span class=\"cm\">   * 注意，这里只是用的Spark1.x创建sc的和df的语法，其中注册udf在Spark1.x也是可以使用的的\n</span><span class=\"cm\">   * 但是withColumn和select方法Spark2.0.0之后才有的，关于spark1.xDataFrame怎么使用注册好的UDF没有研究\n</span><span class=\"cm\">   */</span>\n  <span class=\"k\">def</span> <span class=\"n\">oldDfUdf</span><span class=\"o\">()</span> <span class=\"o\">{</span>\n    <span class=\"c1\">//spark 初始化\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">conf</span> <span class=\"k\">=</span> <span class=\"k\">new</span> <span class=\"nc\">SparkConf</span><span class=\"o\">()</span>\n      <span class=\"o\">.</span><span class=\"n\">setMaster</span><span class=\"o\">(</span><span class=\"s\">&#34;local&#34;</span><span class=\"o\">)</span>\n      <span class=\"o\">.</span><span class=\"n\">setAppName</span><span class=\"o\">(</span><span class=\"s\">&#34;oldDfUdf&#34;</span><span class=\"o\">)</span>\n    <span class=\"k\">val</span> <span class=\"n\">sc</span> <span class=\"k\">=</span> <span class=\"k\">new</span> <span class=\"nc\">SparkContext</span><span class=\"o\">(</span><span class=\"n\">conf</span><span class=\"o\">)</span>\n    <span class=\"k\">val</span> <span class=\"n\">sqlContext</span> <span class=\"k\">=</span> <span class=\"k\">new</span> <span class=\"nc\">SQLContext</span><span class=\"o\">(</span><span class=\"n\">sc</span><span class=\"o\">)</span>\n    <span class=\"k\">import</span> <span class=\"nn\">sqlContext.implicits._</span>\n\n    <span class=\"c1\">// 构造测试数据，有两个字段、名字和年龄\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">userData</span> <span class=\"k\">=</span> <span class=\"nc\">Array</span><span class=\"o\">((</span><span class=\"s\">&#34;Leo&#34;</span><span class=\"o\">,</span> <span class=\"mi\">16</span><span class=\"o\">),</span> <span class=\"o\">(</span><span class=\"s\">&#34;Marry&#34;</span><span class=\"o\">,</span> <span class=\"mi\">21</span><span class=\"o\">),</span> <span class=\"o\">(</span><span class=\"s\">&#34;Jack&#34;</span><span class=\"o\">,</span> <span class=\"mi\">14</span><span class=\"o\">),</span> <span class=\"o\">(</span><span class=\"s\">&#34;Tom&#34;</span><span class=\"o\">,</span> <span class=\"mi\">18</span><span class=\"o\">))</span>\n    <span class=\"c1\">//创建测试df\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">userDF</span> <span class=\"k\">=</span> <span class=\"n\">sc</span><span class=\"o\">.</span><span class=\"n\">parallelize</span><span class=\"o\">(</span><span class=\"n\">userData</span><span class=\"o\">).</span><span class=\"n\">toDF</span><span class=\"o\">(</span><span class=\"s\">&#34;name&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;age&#34;</span><span class=\"o\">)</span>\n    <span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.functions._</span>\n    <span class=\"c1\">//注册自定义函数（通过匿名函数）\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">strLen</span> <span class=\"k\">=</span> <span class=\"n\">udf</span><span class=\"o\">((</span><span class=\"n\">str</span><span class=\"k\">:</span> <span class=\"kt\">String</span><span class=\"o\">)</span> <span class=\"k\">=&gt;</span> <span class=\"n\">str</span><span class=\"o\">.</span><span class=\"n\">length</span><span class=\"o\">())</span>\n    <span class=\"c1\">//注册自定义函数（通过实名函数）\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">udf_isAdult</span> <span class=\"k\">=</span> <span class=\"n\">udf</span><span class=\"o\">(</span><span class=\"n\">isAdult</span> <span class=\"k\">_</span><span class=\"o\">)</span>\n\n    <span class=\"c1\">//通过withColumn添加列\n</span><span class=\"c1\"></span>    <span class=\"n\">userDF</span><span class=\"o\">.</span><span class=\"n\">withColumn</span><span class=\"o\">(</span><span class=\"s\">&#34;name_len&#34;</span><span class=\"o\">,</span> <span class=\"n\">strLen</span><span class=\"o\">(</span><span class=\"n\">col</span><span class=\"o\">(</span><span class=\"s\">&#34;name&#34;</span><span class=\"o\">))).</span><span class=\"n\">withColumn</span><span class=\"o\">(</span><span class=\"s\">&#34;isAdult&#34;</span><span class=\"o\">,</span> <span class=\"n\">udf_isAdult</span><span class=\"o\">(</span><span class=\"n\">col</span><span class=\"o\">(</span><span class=\"s\">&#34;age&#34;</span><span class=\"o\">))).</span><span class=\"n\">show</span>\n    <span class=\"c1\">//通过select添加列\n</span><span class=\"c1\"></span>    <span class=\"n\">userDF</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"o\">(</span><span class=\"n\">col</span><span class=\"o\">(</span><span class=\"s\">&#34;*&#34;</span><span class=\"o\">),</span> <span class=\"n\">strLen</span><span class=\"o\">(</span><span class=\"n\">col</span><span class=\"o\">(</span><span class=\"s\">&#34;name&#34;</span><span class=\"o\">))</span> <span class=\"n\">as</span> <span class=\"s\">&#34;name_len&#34;</span><span class=\"o\">,</span> <span class=\"n\">udf_isAdult</span><span class=\"o\">(</span><span class=\"n\">col</span><span class=\"o\">(</span><span class=\"s\">&#34;age&#34;</span><span class=\"o\">))</span> <span class=\"n\">as</span> <span class=\"s\">&#34;isAdult&#34;</span><span class=\"o\">).</span><span class=\"n\">show</span>\n\n    <span class=\"c1\">//关闭\n</span><span class=\"c1\"></span>    <span class=\"n\">sc</span><span class=\"o\">.</span><span class=\"n\">stop</span><span class=\"o\">()</span>\n  <span class=\"o\">}</span>\n\n<span class=\"o\">}</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-6ba805f6655ddb24e9035be3160267a6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1510\" data-rawheight=\"920\" class=\"origin_image zh-lightbox-thumb\" width=\"1510\" data-original=\"https://pic3.zhimg.com/v2-6ba805f6655ddb24e9035be3160267a6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1510&#39; height=&#39;920&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1510\" data-rawheight=\"920\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1510\" data-original=\"https://pic3.zhimg.com/v2-6ba805f6655ddb24e9035be3160267a6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-6ba805f6655ddb24e9035be3160267a6_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "Spark", 
                    "tagLink": "https://api.zhihu.com/topics/19942170"
                }, 
                {
                    "tag": "UDF", 
                    "tagLink": "https://api.zhihu.com/topics/20166336"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/51448167", 
            "userName": "董可伦", 
            "userLink": "https://www.zhihu.com/people/de888cc734ddbfd918269a8e5fb196ba", 
            "upvote": 3, 
            "title": "通过数据库客户端界面工具DBeaver连接Hive", 
            "content": "<p>我的原创地址：</p><a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/07/13/dbeaverConnectHive/\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-46caaaf97059932255aa69e8d0af98ea_180x120.jpg\" data-image-width=\"1293\" data-image-height=\"698\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">通过数据库客户端界面工具DBeaver连接Hive</a><p class=\"ztext-empty-paragraph\"><br/></p><h2>前言</h2><p>本文讲解如何通过数据库客户端界面工具DBeaver连接hive，并解决驱动下载不下来的问题。</p><h2>1、为什么使用客户端界面工具</h2><p>为什么使用客户端界面工具而不用命令行使用hive <i> 通过界面工具查看分析hive里的数据要方便很多 </i> 业务人员没有权限通过命令行连接hive * 领导喜欢在界面工具上查看hive里的数据</p><h2>2、为什么使用DBeaver</h2><p>其实在网上搜一下，连接hive的工具还有很多，使用DBeaver的原因是因为我之前连接关系型数据库使用的就是DBeaver，正好DBeaver支持连接hive，且个人认为DBeaver确实挺好用的，支持各种关系型数据库，如连接Oracle数据库不需要像plsql那样自己配置连接文件，只需要在界面上输入url、用户名、密码即可，还有就是DBeaver的快捷键和Eclipse是一样的，比如注释、删除一行、复制一行到下一行等。</p><h2>3、DBeaver下载、安装</h2><p>之前我一直用的旧版的，现在在官网上下载了最新版的DBeaver，发现界面功能比旧版好用了很多，亲测连hive没有问题。 下载地址：<a href=\"https://link.zhihu.com/?target=https%3A//dbeaver.io/download/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">dbeaver.io/download/</span><span class=\"invisible\"></span></a> 我下载的免安装版（不带jre），windows64位，大家可以根据自己情况下载对应版本。 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-fbfc28a409a6c94cb64bec7754690cad_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1039\" data-rawheight=\"744\" class=\"origin_image zh-lightbox-thumb\" width=\"1039\" data-original=\"https://pic2.zhimg.com/v2-fbfc28a409a6c94cb64bec7754690cad_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1039&#39; height=&#39;744&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1039\" data-rawheight=\"744\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1039\" data-original=\"https://pic2.zhimg.com/v2-fbfc28a409a6c94cb64bec7754690cad_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-fbfc28a409a6c94cb64bec7754690cad_b.jpg\"/></figure><p>  因为我下载的免安装版，所以解压后，直接点击dbeaver.exe就可以使用了！</p><h2>4、启动hive相关</h2><p>测试连接前先启动hive相关的服务</p><p>1、启动hdfs、yarn</p><div class=\"highlight\"><pre><code class=\"language-bash\">opt/hadoop-2.7.5/sbin/start-dfs.sh\n/opt/hadoop-2.7.5/sbin/start-yarn.sh</code></pre></div><p>2、启动hiveserver2(hive-0.11.0以后的版本) 如果想远程连接hive,则需要启动hiveserver2</p><div class=\"highlight\"><pre><code class=\"language-text\">/opt/apache-hive-2.3.2-bin/bin/hive --service hiveserver2</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-d5a99d4da48e887c6949ed538d10bd18_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1226\" data-rawheight=\"380\" class=\"origin_image zh-lightbox-thumb\" width=\"1226\" data-original=\"https://pic1.zhimg.com/v2-d5a99d4da48e887c6949ed538d10bd18_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1226&#39; height=&#39;380&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1226\" data-rawheight=\"380\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1226\" data-original=\"https://pic1.zhimg.com/v2-d5a99d4da48e887c6949ed538d10bd18_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-d5a99d4da48e887c6949ed538d10bd18_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>关于hadoop和hive的配置可以参考<a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/03/23/hadoopConf/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">centos7 hadoop 单机模式安装配置</a>、<a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/03/24/hiveConf/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">centos7 hive 单机模式安装配置</a> * 注：图中的startall.sh就是上面hdfs、yarn的启动命令</p><h2>5、创建hive测试表</h2><div class=\"highlight\"><pre><code class=\"language-sql\"><span class=\"k\">CREATE</span> <span class=\"k\">TABLE</span> <span class=\"k\">IF</span> <span class=\"k\">NOT</span> <span class=\"k\">EXISTS</span> <span class=\"n\">test_20180713</span> <span class=\"p\">(</span><span class=\"n\">id</span> <span class=\"nb\">INT</span><span class=\"p\">,</span><span class=\"n\">name</span> <span class=\"n\">STRING</span><span class=\"p\">)</span><span class=\"k\">ROW</span> <span class=\"n\">FORMAT</span> <span class=\"n\">DELIMITED</span> <span class=\"n\">FIELDS</span> <span class=\"n\">TERMINATED</span> <span class=\"k\">BY</span> <span class=\"s2\">&#34; &#34;</span> <span class=\"n\">LINES</span> <span class=\"n\">TERMINATED</span> <span class=\"k\">BY</span> <span class=\"s2\">&#34;\\n&#34;</span><span class=\"p\">;</span>\n\n<span class=\"k\">insert</span> <span class=\"k\">into</span> <span class=\"n\">test_20180713</span> <span class=\"k\">values</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"s1\">&#39;tom&#39;</span><span class=\"p\">);</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-20e13b6d1c4a389a815f434a93f45f08_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1384\" data-rawheight=\"566\" class=\"origin_image zh-lightbox-thumb\" width=\"1384\" data-original=\"https://pic1.zhimg.com/v2-20e13b6d1c4a389a815f434a93f45f08_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1384&#39; height=&#39;566&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1384\" data-rawheight=\"566\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1384\" data-original=\"https://pic1.zhimg.com/v2-20e13b6d1c4a389a815f434a93f45f08_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-20e13b6d1c4a389a815f434a93f45f08_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>6、DBeaver连接hive</h2><p>DBeaver连接关系型数据库比较的简单，连接hive的话因为要配置下载驱动，所以这里详细说明一下。</p><h2>6.1 文件-&gt;新建-&gt;数据库连接（新版是中文的，而之前旧版的是英文的，这点我还是比较喜欢的~）</h2><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-6f8e1624672c583e9d1cdc66feed9d73_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"513\" data-rawheight=\"486\" class=\"origin_image zh-lightbox-thumb\" width=\"513\" data-original=\"https://pic4.zhimg.com/v2-6f8e1624672c583e9d1cdc66feed9d73_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;513&#39; height=&#39;486&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"513\" data-rawheight=\"486\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"513\" data-original=\"https://pic4.zhimg.com/v2-6f8e1624672c583e9d1cdc66feed9d73_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-6f8e1624672c583e9d1cdc66feed9d73_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>6.2 选择新连接类型-&gt;Apache Hive(从这里看到，DBeaver支持的数据库还是很多的)</h2><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-2aa9c7e8dc06825e1bc5adc74244081c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"515\" data-rawheight=\"682\" class=\"origin_image zh-lightbox-thumb\" width=\"515\" data-original=\"https://pic1.zhimg.com/v2-2aa9c7e8dc06825e1bc5adc74244081c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;515&#39; height=&#39;682&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"515\" data-rawheight=\"682\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"515\" data-original=\"https://pic1.zhimg.com/v2-2aa9c7e8dc06825e1bc5adc74244081c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-2aa9c7e8dc06825e1bc5adc74244081c_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>6.3 填一下hiveserver2的ip和hive的数据库名</h2><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-f63381f96f3e0d75018b786cbf720b18_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"512\" data-rawheight=\"682\" class=\"origin_image zh-lightbox-thumb\" width=\"512\" data-original=\"https://pic1.zhimg.com/v2-f63381f96f3e0d75018b786cbf720b18_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;512&#39; height=&#39;682&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"512\" data-rawheight=\"682\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"512\" data-original=\"https://pic1.zhimg.com/v2-f63381f96f3e0d75018b786cbf720b18_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-f63381f96f3e0d75018b786cbf720b18_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>6.4 编辑驱动设置-下载/更新（第一次打开需要下载maven的依赖配置，需等待一会）</h2><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-e9324e196283e4e295f9a1fa9118c2a5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"504\" data-rawheight=\"656\" class=\"origin_image zh-lightbox-thumb\" width=\"504\" data-original=\"https://pic2.zhimg.com/v2-e9324e196283e4e295f9a1fa9118c2a5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;504&#39; height=&#39;656&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"504\" data-rawheight=\"656\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"504\" data-original=\"https://pic2.zhimg.com/v2-e9324e196283e4e295f9a1fa9118c2a5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-e9324e196283e4e295f9a1fa9118c2a5_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>这里需等待一会，截图太多，记不清楚是不是这个图了~</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-1f57347281065498c789f1eef7214ea5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"516\" data-rawheight=\"481\" class=\"origin_image zh-lightbox-thumb\" width=\"516\" data-original=\"https://pic2.zhimg.com/v2-1f57347281065498c789f1eef7214ea5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;516&#39; height=&#39;481&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"516\" data-rawheight=\"481\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"516\" data-original=\"https://pic2.zhimg.com/v2-1f57347281065498c789f1eef7214ea5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-1f57347281065498c789f1eef7214ea5_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>6.5 选择hive的版本，我这里的版本为2.3.2</h2><p>到这里就有问题了，之前我在公司的测试机用ambari装的hive的版本是1.2.1,然后下载1.2.1需要的hive的jar包，很快就下载下来了，之后就可以查询hive里的数据了，但是在自己的虚拟机版本为2.3.2，2.3.2的jar就下不下来了 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-9c72b76c008e9f424aec79485edb05fd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"513\" data-rawheight=\"724\" class=\"origin_image zh-lightbox-thumb\" width=\"513\" data-original=\"https://pic2.zhimg.com/v2-9c72b76c008e9f424aec79485edb05fd_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;513&#39; height=&#39;724&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"513\" data-rawheight=\"724\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"513\" data-original=\"https://pic2.zhimg.com/v2-9c72b76c008e9f424aec79485edb05fd_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-9c72b76c008e9f424aec79485edb05fd_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-52455a2bd0cf58b84775fc32f3de12f4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"535\" data-rawheight=\"430\" class=\"origin_image zh-lightbox-thumb\" width=\"535\" data-original=\"https://pic1.zhimg.com/v2-52455a2bd0cf58b84775fc32f3de12f4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;535&#39; height=&#39;430&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"535\" data-rawheight=\"430\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"535\" data-original=\"https://pic1.zhimg.com/v2-52455a2bd0cf58b84775fc32f3de12f4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-52455a2bd0cf58b84775fc32f3de12f4_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>尝试将maven库改为阿里云的无效，且测试和DBeaver的版本无关，旧版和最新版的都下载不下来</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-facb3b0e044ce083d42b1ec8afc2d357_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"614\" data-rawheight=\"555\" class=\"origin_image zh-lightbox-thumb\" width=\"614\" data-original=\"https://pic4.zhimg.com/v2-facb3b0e044ce083d42b1ec8afc2d357_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;614&#39; height=&#39;555&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"614\" data-rawheight=\"555\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"614\" data-original=\"https://pic4.zhimg.com/v2-facb3b0e044ce083d42b1ec8afc2d357_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-facb3b0e044ce083d42b1ec8afc2d357_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>7、用hive自带的jdbc jar</h2><p>多次尝试依然解决不了驱动下载不下来的问题，只能尝试用hive安装包自带的jdbc jar了，幸好DBeaver和hive很好的支持了这种方法。hive帮我们集成了jdbc相关的一个jar包，这样我们就不用一个一个去lib下找相关的jar包了~ 首先将hive安装包里的jdbc包下载到本地，jar包位置：</p><div class=\"highlight\"><pre><code class=\"language-vim\"><span class=\"sr\">/opt/</span><span class=\"nx\">apache</span><span class=\"p\">-</span><span class=\"nx\">hive</span><span class=\"m\">-2</span>.<span class=\"m\">3</span>.<span class=\"m\">2</span><span class=\"p\">-</span><span class=\"nx\">bin</span><span class=\"sr\">/jdbc/</span><span class=\"nx\">hive</span><span class=\"p\">-</span><span class=\"nx\">jdbc</span><span class=\"m\">-2</span>.<span class=\"m\">3</span>.<span class=\"m\">2</span><span class=\"p\">-</span><span class=\"nx\">standalone</span>.<span class=\"nx\">jar</span></code></pre></div><p>然后将DBeaver默认的jar删除，再添加上我们刚才下载的jar包，然后下一步，命名我们的连接。 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-9f30c90fa830022b0d7013c2c369065e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"496\" data-rawheight=\"659\" class=\"origin_image zh-lightbox-thumb\" width=\"496\" data-original=\"https://pic3.zhimg.com/v2-9f30c90fa830022b0d7013c2c369065e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;496&#39; height=&#39;659&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"496\" data-rawheight=\"659\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"496\" data-original=\"https://pic3.zhimg.com/v2-9f30c90fa830022b0d7013c2c369065e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-9f30c90fa830022b0d7013c2c369065e_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-4bc1dddbca72c7b2edee10d3be80313d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"805\" data-rawheight=\"662\" class=\"origin_image zh-lightbox-thumb\" width=\"805\" data-original=\"https://pic2.zhimg.com/v2-4bc1dddbca72c7b2edee10d3be80313d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;805&#39; height=&#39;662&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"805\" data-rawheight=\"662\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"805\" data-original=\"https://pic2.zhimg.com/v2-4bc1dddbca72c7b2edee10d3be80313d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-4bc1dddbca72c7b2edee10d3be80313d_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-1c8debb324ff70d547e616875e6eba11_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"513\" data-rawheight=\"685\" class=\"origin_image zh-lightbox-thumb\" width=\"513\" data-original=\"https://pic2.zhimg.com/v2-1c8debb324ff70d547e616875e6eba11_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;513&#39; height=&#39;685&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"513\" data-rawheight=\"685\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"513\" data-original=\"https://pic2.zhimg.com/v2-1c8debb324ff70d547e616875e6eba11_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-1c8debb324ff70d547e616875e6eba11_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>8、测试连接</h2><p>右键连接名-&gt;编辑连接-&gt;测试连接，然后发现报了一个异常 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-7b9b4e4cef5b90ab0de7a7a48b23f7e3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"510\" data-rawheight=\"491\" class=\"origin_image zh-lightbox-thumb\" width=\"510\" data-original=\"https://pic4.zhimg.com/v2-7b9b4e4cef5b90ab0de7a7a48b23f7e3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;510&#39; height=&#39;491&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"510\" data-rawheight=\"491\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"510\" data-original=\"https://pic4.zhimg.com/v2-7b9b4e4cef5b90ab0de7a7a48b23f7e3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-7b9b4e4cef5b90ab0de7a7a48b23f7e3_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-33a99b973e99fd61b91de59361c0eef6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"608\" data-rawheight=\"573\" class=\"origin_image zh-lightbox-thumb\" width=\"608\" data-original=\"https://pic3.zhimg.com/v2-33a99b973e99fd61b91de59361c0eef6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;608&#39; height=&#39;573&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"608\" data-rawheight=\"573\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"608\" data-original=\"https://pic3.zhimg.com/v2-33a99b973e99fd61b91de59361c0eef6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-33a99b973e99fd61b91de59361c0eef6_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>9、解决异常</h2><p>但是在ambari安装的hive上测试这种方法是没有问题的，经过网上查询该异常，发现是我的hdfs配置问题，修改hdfs的core-site.xml即可 先关掉hdfs、yarn、hiveserver2服务</p><div class=\"highlight\"><pre><code class=\"language-bash\">vim /opt/hadoop-2.7.5/etc/hadoop/core-site.xml</code></pre></div><p>添加</p><div class=\"highlight\"><pre><code class=\"language-vim\"><span class=\"p\">&lt;</span><span class=\"nx\">property</span><span class=\"p\">&gt;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">&lt;</span><span class=\"nx\">name</span><span class=\"p\">&gt;</span><span class=\"nx\">hadoop</span>.<span class=\"nx\">proxyuser</span>.<span class=\"nx\">root</span>.<span class=\"nx\">hosts</span><span class=\"p\">&lt;</span>/<span class=\"nx\">name</span><span class=\"p\">&gt;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">&lt;</span><span class=\"nx\">value</span><span class=\"p\">&gt;</span>*<span class=\"p\">&lt;</span>/<span class=\"nx\">value</span><span class=\"p\">&gt;</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"p\">&lt;</span>/<span class=\"nx\">property</span><span class=\"p\">&gt;</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"p\">&lt;</span><span class=\"nx\">property</span><span class=\"p\">&gt;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">&lt;</span><span class=\"nx\">name</span><span class=\"p\">&gt;</span><span class=\"nx\">hadoop</span>.<span class=\"nx\">proxyuser</span>.<span class=\"nx\">root</span>.<span class=\"nx\">groups</span><span class=\"p\">&lt;</span>/<span class=\"nx\">name</span><span class=\"p\">&gt;</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"p\">&lt;</span><span class=\"nx\">value</span><span class=\"p\">&gt;</span>*<span class=\"p\">&lt;</span>/<span class=\"nx\">value</span><span class=\"p\">&gt;</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"p\">&lt;</span>/<span class=\"nx\">property</span><span class=\"p\">&gt;</span></code></pre></div><p>然后重启hdfs、yarn、hiveserver2 具体可参考<a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/sunnyyoona/article/details/51648871\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">[Hive]那些年我们踩过的Hive坑</a>里的第十个异常。</p><h2>10、再次测试</h2><p>再次测试连接，成功！ </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-091fb32c7cdd655c7a40be3c1a49d18c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1293\" data-rawheight=\"698\" class=\"origin_image zh-lightbox-thumb\" width=\"1293\" data-original=\"https://pic1.zhimg.com/v2-091fb32c7cdd655c7a40be3c1a49d18c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1293&#39; height=&#39;698&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1293\" data-rawheight=\"698\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1293\" data-original=\"https://pic1.zhimg.com/v2-091fb32c7cdd655c7a40be3c1a49d18c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-091fb32c7cdd655c7a40be3c1a49d18c_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>从这里可以看出DBeaver的界面功能还是挺丰富的，使用起来也比较方便</p>", 
            "topic": [
                {
                    "tag": "客户端", 
                    "tagLink": "https://api.zhihu.com/topics/19555413"
                }, 
                {
                    "tag": "界面", 
                    "tagLink": "https://api.zhihu.com/topics/19551741"
                }, 
                {
                    "tag": "Hive", 
                    "tagLink": "https://api.zhihu.com/topics/19655283"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/51373908", 
            "userName": "董可伦", 
            "userLink": "https://www.zhihu.com/people/de888cc734ddbfd918269a8e5fb196ba", 
            "upvote": 1, 
            "title": "HDFS DataNode启动异常:/opt/jdk1.8.0_151/bin/java:权限不够", 
            "content": "<p></p><p>我的原创地址：</p><a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/07/10/HadoopException/\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic4.zhimg.com/v2-40803fbd569ff6833210d4cf684c285b_120x160.jpg\" data-image-width=\"217\" data-image-height=\"307\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">HDFS DataNode启动异常:/opt/jdk1.8.0_151/bin/java:权限不够</a><h2>前言</h2><p>这个异常是在在ambari里启动DataNode产生的，其实这个问题很久就发现了，只是没时间去处理，所以之前把发生问题的slave1节点给移除了，现在有时间处理，就又把slave1加上了，所以就有了<a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/07/10/ambariExceptions/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">ambari 异常总结及解决办法</a>里面的问题，这个java权限不够的问题，在添加slave1之后，并安装datanode和nodemanager之后启动服务依然报错。</p><p>异常信息：</p><div class=\"highlight\"><pre><code class=\"language-text\">/usr/hdp/2.6.4.0-91//hadoop-hdfs/bin/hdfs.distro:行317: /opt/jdk1.8.0_151/bin/java: 权限不够\n/usr/hdp/2.6.4.0-91//hadoop-hdfs/bin/hdfs.distro: 第 317 行:exec: /opt/jdk1.8.0_151/bin/java: 无法执行: 权限不够</code></pre></div><p>这个问题在网上找也找不到对应的解决办法，所以有必要单独记录一下，没有放在<a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/07/10/ambariExceptions/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">ambari 异常总结及解决办法</a>，且我认为即使不用ambari，该问题依然可能会发生。</p><h2>1、详细异常信息</h2><p>部分截图: </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-b10d09e5383c3541614fb4d119d029a3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1117\" data-rawheight=\"348\" class=\"origin_image zh-lightbox-thumb\" width=\"1117\" data-original=\"https://pic4.zhimg.com/v2-b10d09e5383c3541614fb4d119d029a3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1117&#39; height=&#39;348&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1117\" data-rawheight=\"348\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1117\" data-original=\"https://pic4.zhimg.com/v2-b10d09e5383c3541614fb4d119d029a3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-b10d09e5383c3541614fb4d119d029a3_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-395915520643d5f32db161345441e53f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1112\" data-rawheight=\"672\" class=\"origin_image zh-lightbox-thumb\" width=\"1112\" data-original=\"https://pic4.zhimg.com/v2-395915520643d5f32db161345441e53f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1112&#39; height=&#39;672&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1112\" data-rawheight=\"672\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1112\" data-original=\"https://pic4.zhimg.com/v2-395915520643d5f32db161345441e53f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-395915520643d5f32db161345441e53f_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>2、解决思路</h2><p>首先确定java是否有问题，经过简单的测试，没发现问题，然后在/usr/hdp/2.6.4.0-91//hadoop-hdfs/bin/hdfs.distro查看第317行的内容，发现hdfs命令实际调用的java命令，但是一开始并不知道怎么解决这个问题，通过查看ambari的日志也并没有解决问题，最后回到jdk目录的权限问题。 首先有问题的时候</p><div class=\"highlight\"><pre><code class=\"language-text\">su - hdfs \nhdfs -ls</code></pre></div><p>执行上面的命令也是会报同样的问题，所以就不用在ambari里启动datanode服务就可以测试有没有问题了</p><h2>3、解决办法</h2><h2>3.1 改变/opt的所有者给hdfs（只将jdk目录的给hdfs没有解决问题，没有深究其原因，可能因为还有其他目录和jdk有关联）</h2><div class=\"highlight\"><pre><code class=\"language-bash\">chown -R hdfs:hdfs /opt/</code></pre></div><p>然后用hdfs -ls测试，发现没有上面的异常，这样就可以在在ambari里启动datanode了，启动成功！</p><h2>3.2 启动nodemanager报错</h2><div class=\"highlight\"><pre><code class=\"language-vim\"><span class=\"sr\">/usr/</span><span class=\"nx\">hdp</span><span class=\"sr\">/2.6.4.0-91//hadoop-yarn/</span><span class=\"nx\">bin</span><span class=\"sr\">/yarn.distro:行376: /</span><span class=\"nx\">opt</span><span class=\"sr\">/jdk1.8.0_151/</span><span class=\"nx\">bin</span>/<span class=\"nx\">java</span><span class=\"p\">:</span> <span class=\"nx\">权限不够</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"sr\">/usr/</span><span class=\"nx\">hdp</span><span class=\"sr\">/2.6.4.0-91//hadoop-yarn/</span><span class=\"nx\">bin</span><span class=\"sr\">/yarn.distro: 第 376 行:exec: /</span><span class=\"nx\">opt</span><span class=\"sr\">/jdk1.8.0_151/</span><span class=\"nx\">bin</span>/<span class=\"nx\">java</span><span class=\"p\">:</span> <span class=\"nx\">无法执行</span><span class=\"p\">:</span> <span class=\"nx\">权限不够</span></code></pre></div><p>异常信息一样，只是文件名不一样，同样的方法解决</p><div class=\"highlight\"><pre><code class=\"language-bash\">chown -R yarn:yarn /opt/</code></pre></div><p>启动nodemanager成功！ 然后重启ambari所有服务，看一下是否还有问题，结果slave1启动datanode时报了同样的错误，问了一下同事，发现我对chown -R上面的命令有所误解，我开始认为是让hdfs拥有opt的权限，只想上面两步后，hdfs和yarn都有了opt的权限，但是其实该命令是更改opt的所有者，执行上面的两步之后，只有yarn拥有opt的权限了，也就是将hdfs覆盖了。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-6e0725777ab4da359ce75902fe0ddbdc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1101\" data-rawheight=\"433\" class=\"origin_image zh-lightbox-thumb\" width=\"1101\" data-original=\"https://pic1.zhimg.com/v2-6e0725777ab4da359ce75902fe0ddbdc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1101&#39; height=&#39;433&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1101\" data-rawheight=\"433\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1101\" data-original=\"https://pic1.zhimg.com/v2-6e0725777ab4da359ce75902fe0ddbdc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-6e0725777ab4da359ce75902fe0ddbdc_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>3.3 让hadoop组都拥有opt的权限</h2><div class=\"highlight\"><pre><code class=\"language-bash\">chown -R yarn:hadoop /opt/\nchmod -R <span class=\"m\">770</span> /opt</code></pre></div><p>大概解释一下上面的命令，首先更改opt的所有者为hadoop组下面的yarn用户（yarn和hdfs都属于hadoop组） 然后770的意思是用户和用户组对opt目录具有rwx的权限，其他用户没有任何权限，这样hdfs用户也有opt的权限了，这样重启ambari的所有服务就会成功了！</p><h2>3.4 最后的方法</h2><p>上面命令是同事帮忙操作的，然后发现自己对Linux权限不太了解，就大概查了一下权限相关的资料，因为之前已经把opt的权限弄乱了，所以最后先这样解决（测试环境，且opt目录一般就我们几个人用） 首先将opt的所有者改回为root</p><div class=\"highlight\"><pre><code class=\"language-bash\">chown -R root:root /opt/</code></pre></div><p>然后更改opt的权限为755</p><div class=\"highlight\"><pre><code class=\"language-bash\">chmod -R <span class=\"m\">755</span> /opt</code></pre></div><p>755的意思为用户的权限为rwx,用户组和其他人的权限均为r-x,即没有写权限，一般文件权限也是755（应该也是默认的，可以自己新建一个文件夹进行测试）,然后这样重启ambari的所有服务也是没有问题的 因为-R是递归的，这样其实不好，因为有些txt,readme的权限也改了，所以我将opt下面的jdk目录删除并重新解压了一份。 opt各目录权限截图：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b7ecde4470c9d5bc8a78a4d9b01901e5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"217\" data-rawheight=\"307\" class=\"content_image\" width=\"217\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;217&#39; height=&#39;307&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"217\" data-rawheight=\"307\" class=\"content_image lazy\" width=\"217\" data-actualsrc=\"https://pic2.zhimg.com/v2-b7ecde4470c9d5bc8a78a4d9b01901e5_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>权限问题参考：<a href=\"https://link.zhihu.com/?target=http%3A//justcode.ikeepstudying.com/2016/08/linux-chmod-%25E5%2592%258C-chown%25E7%2594%25A8%25E6%25B3%2595%25E5%25B0%258F%25E7%25BB%2593/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">http://justcode.ikeepstudying.com/2016/08/linux-chmod-和-chown用法小结/</a></p><h2>4、总结</h2><p>其实该问题只修改jdk相关的目录的权限即可，但是之前已经递归修改opt的目录，所以暂时只能按照3.4的办法解决。该异常出现的原因应该是才平时的操作中，误修改了jdk的权限，为了防止出现这种问题，平时在操作中关于chmod -R等相关的操作一定要慎用。</p>", 
            "topic": [
                {
                    "tag": "HDFS", 
                    "tagLink": "https://api.zhihu.com/topics/19657742"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/51320976", 
            "userName": "董可伦", 
            "userLink": "https://www.zhihu.com/people/de888cc734ddbfd918269a8e5fb196ba", 
            "upvote": 2, 
            "title": "ambari 异常总结及解决办法", 
            "content": "<p>我的原创地址：</p><a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/07/10/ambariExceptions/\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic4.zhimg.com/v2-a1a0753d30308ceac586e11efc383ed3_180x120.jpg\" data-image-width=\"1598\" data-image-height=\"850\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">ambari 异常总结及解决办法</a><h2>前言</h2><p>本文总结在使用ambari时产生的异常，以及如何解决的。 如果发生了异常，在界面上不能直观的看出异常的原因，那么我一般通过查看日志的方法解决。 通过下面的命令查看</p><div class=\"highlight\"><pre><code class=\"language-bash\">vim /var/log/ambari-server/ambari-server.log</code></pre></div><ul><li>若该日志文件没有异常信息，可在其他日志文件里查找</li></ul><h2>1、异常一</h2><p>异常发生在add host并confirm host的时候，会卡住不动，然后看日志会发现异常信息：</p><div class=\"highlight\"><pre><code class=\"language-vim\"><span class=\"nx\">Error</span> <span class=\"nx\">executing</span> <span class=\"nx\">bootstrap</span> <span class=\"nx\">Cannot</span> <span class=\"nx\">create</span> <span class=\"sr\">/var/</span><span class=\"nx\">run</span><span class=\"sr\">/ambari-server/</span><span class=\"nx\">bootstrap</span></code></pre></div><h2>1.1 具体的异常</h2><div class=\"highlight\"><pre><code class=\"language-vim\"><span class=\"nx\">七月</span> <span class=\"m\">2018</span> <span class=\"m\">14</span><span class=\"p\">:</span><span class=\"m\">29</span><span class=\"p\">:</span><span class=\"m\">08</span><span class=\"p\">,</span><span class=\"m\">339</span>  <span class=\"nx\">INFO</span> [<span class=\"nx\">ambari</span><span class=\"p\">-</span><span class=\"nx\">client</span><span class=\"p\">-</span><span class=\"nx\">thread</span><span class=\"m\">-22654</span>] <span class=\"nx\">BootStrapImpl</span><span class=\"p\">:</span><span class=\"m\">108</span> <span class=\"p\">-</span> <span class=\"nx\">BootStrapping</span> <span class=\"nx\">hosts</span> <span class=\"nx\">ambari</span>.<span class=\"nx\">slave1</span>.<span class=\"nx\">com</span><span class=\"p\">:</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"m\">09</span> <span class=\"nx\">七月</span> <span class=\"m\">2018</span> <span class=\"m\">14</span><span class=\"p\">:</span><span class=\"m\">29</span><span class=\"p\">:</span><span class=\"m\">08</span><span class=\"p\">,</span><span class=\"m\">340</span>  <span class=\"nx\">INFO</span> [<span class=\"nx\">Thread</span><span class=\"m\">-4609</span>] <span class=\"nx\">BSRunner</span><span class=\"p\">:</span><span class=\"m\">189</span> <span class=\"p\">-</span> <span class=\"nx\">Kicking</span> <span class=\"nx\">off</span> <span class=\"nx\">the</span> <span class=\"nx\">scheduler</span> <span class=\"nx\">for</span> <span class=\"nx\">polling</span> <span class=\"nx\">on</span> <span class=\"nx\">logs</span> <span class=\"nx\">in</span> <span class=\"sr\">/var/</span><span class=\"nx\">run</span><span class=\"sr\">/ambari-server/</span><span class=\"nx\">bootstrap</span>/<span class=\"m\">3</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"m\">09</span> <span class=\"nx\">七月</span> <span class=\"m\">2018</span> <span class=\"m\">14</span><span class=\"p\">:</span><span class=\"m\">29</span><span class=\"p\">:</span><span class=\"m\">08</span><span class=\"p\">,</span><span class=\"m\">340</span>  <span class=\"nx\">INFO</span> [<span class=\"nx\">Thread</span><span class=\"m\">-4609</span>] <span class=\"nx\">BSRunner</span><span class=\"p\">:</span><span class=\"m\">372</span> <span class=\"p\">-</span> <span class=\"nx\">Error</span> <span class=\"nx\">executing</span> <span class=\"nx\">bootstrap</span> <span class=\"nx\">Cannot</span> <span class=\"nx\">create</span> <span class=\"sr\">/var/</span><span class=\"nx\">run</span><span class=\"sr\">/ambari-server/</span><span class=\"nx\">bootstrap</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"m\">09</span> <span class=\"nx\">七月</span> <span class=\"m\">2018</span> <span class=\"m\">14</span><span class=\"p\">:</span><span class=\"m\">29</span><span class=\"p\">:</span><span class=\"m\">08</span><span class=\"p\">,</span><span class=\"m\">343</span> <span class=\"nx\">ERROR</span> [<span class=\"nx\">Thread</span><span class=\"m\">-4609</span>] <span class=\"nx\">BSRunner</span><span class=\"p\">:</span><span class=\"m\">441</span> <span class=\"p\">-</span> <span class=\"nx\">java</span>.<span class=\"nx\">io</span>.<span class=\"nx\">FileNotFoundException</span><span class=\"p\">:</span> <span class=\"sr\">/var/</span><span class=\"nx\">run</span><span class=\"sr\">/ambari-server/</span><span class=\"nx\">bootstrap</span><span class=\"sr\">/3/</span><span class=\"nx\">ambari</span>.<span class=\"nx\">slave1</span>.<span class=\"nx\">com</span>.<span class=\"nx\">done</span> <span class=\"p\">(</span><span class=\"nx\">没有那个文件或目录</span><span class=\"p\">)</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"m\">09</span> <span class=\"nx\">七月</span> <span class=\"m\">2018</span> <span class=\"m\">14</span><span class=\"p\">:</span><span class=\"m\">29</span><span class=\"p\">:</span><span class=\"m\">08</span><span class=\"p\">,</span><span class=\"m\">344</span>  <span class=\"nx\">WARN</span> [<span class=\"nx\">Thread</span><span class=\"m\">-4609</span>] <span class=\"nx\">BSRunner</span><span class=\"p\">:</span><span class=\"m\">401</span> <span class=\"p\">-</span> <span class=\"nx\">File</span> <span class=\"nx\">does</span> <span class=\"nx\">not</span> <span class=\"nx\">exist</span><span class=\"p\">:</span> <span class=\"sr\">/var/</span><span class=\"nx\">run</span><span class=\"sr\">/ambari-server/</span><span class=\"nx\">bootstrap</span><span class=\"sr\">/3/</span><span class=\"nx\">sshKey</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"m\">09</span> <span class=\"nx\">七月</span> <span class=\"m\">2018</span> <span class=\"m\">14</span><span class=\"p\">:</span><span class=\"m\">40</span><span class=\"p\">:</span><span class=\"m\">28</span><span class=\"p\">,</span><span class=\"m\">033</span>  <span class=\"nx\">INFO</span> [<span class=\"nx\">ambari</span><span class=\"p\">-</span><span class=\"nx\">client</span><span class=\"p\">-</span><span class=\"nx\">thread</span><span class=\"m\">-22676</span>] <span class=\"nx\">BootStrapImpl</span><span class=\"p\">:</span><span class=\"m\">108</span> <span class=\"p\">-</span> <span class=\"nx\">BootStrapping</span> <span class=\"nx\">hosts</span> <span class=\"nx\">amabri</span>.<span class=\"nx\">slave1</span>.<span class=\"nx\">com</span><span class=\"p\">:</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"m\">09</span> <span class=\"nx\">七月</span> <span class=\"m\">2018</span> <span class=\"m\">14</span><span class=\"p\">:</span><span class=\"m\">40</span><span class=\"p\">:</span><span class=\"m\">28</span><span class=\"p\">,</span><span class=\"m\">034</span>  <span class=\"nx\">INFO</span> [<span class=\"nx\">Thread</span><span class=\"m\">-4635</span>] <span class=\"nx\">BSRunner</span><span class=\"p\">:</span><span class=\"m\">189</span> <span class=\"p\">-</span> <span class=\"nx\">Kicking</span> <span class=\"nx\">off</span> <span class=\"nx\">the</span> <span class=\"nx\">scheduler</span> <span class=\"nx\">for</span> <span class=\"nx\">polling</span> <span class=\"nx\">on</span> <span class=\"nx\">logs</span> <span class=\"nx\">in</span> <span class=\"sr\">/var/</span><span class=\"nx\">run</span><span class=\"sr\">/ambari-server/</span><span class=\"nx\">bootstrap</span>/<span class=\"m\">4</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"m\">09</span> <span class=\"nx\">七月</span> <span class=\"m\">2018</span> <span class=\"m\">14</span><span class=\"p\">:</span><span class=\"m\">40</span><span class=\"p\">:</span><span class=\"m\">28</span><span class=\"p\">,</span><span class=\"m\">035</span>  <span class=\"nx\">INFO</span> [<span class=\"nx\">Thread</span><span class=\"m\">-4635</span>] <span class=\"nx\">BSRunner</span><span class=\"p\">:</span><span class=\"m\">372</span> <span class=\"p\">-</span> <span class=\"nx\">Error</span> <span class=\"nx\">executing</span> <span class=\"nx\">bootstrap</span> <span class=\"nx\">Cannot</span> <span class=\"nx\">create</span> <span class=\"sr\">/var/</span><span class=\"nx\">run</span><span class=\"sr\">/ambari-server/</span><span class=\"nx\">bootstrap</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"m\">09</span> <span class=\"nx\">七月</span> <span class=\"m\">2018</span> <span class=\"m\">14</span><span class=\"p\">:</span><span class=\"m\">40</span><span class=\"p\">:</span><span class=\"m\">28</span><span class=\"p\">,</span><span class=\"m\">036</span> <span class=\"nx\">ERROR</span> [<span class=\"nx\">Thread</span><span class=\"m\">-4635</span>] <span class=\"nx\">BSRunner</span><span class=\"p\">:</span><span class=\"m\">441</span> <span class=\"p\">-</span> <span class=\"nx\">java</span>.<span class=\"nx\">io</span>.<span class=\"nx\">FileNotFoundException</span><span class=\"p\">:</span> <span class=\"sr\">/var/</span><span class=\"nx\">run</span><span class=\"sr\">/ambari-server/</span><span class=\"nx\">bootstrap</span><span class=\"sr\">/4/</span><span class=\"nx\">amabri</span>.<span class=\"nx\">slave1</span>.<span class=\"nx\">com</span>.<span class=\"nx\">done</span> <span class=\"p\">(</span><span class=\"nx\">没有那个文件或目录</span><span class=\"p\">)</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"m\">09</span> <span class=\"nx\">七月</span> <span class=\"m\">2018</span> <span class=\"m\">14</span><span class=\"p\">:</span><span class=\"m\">40</span><span class=\"p\">:</span><span class=\"m\">28</span><span class=\"p\">,</span><span class=\"m\">037</span>  <span class=\"nx\">WARN</span> [<span class=\"nx\">Thread</span><span class=\"m\">-4635</span>] <span class=\"nx\">BSRunner</span><span class=\"p\">:</span><span class=\"m\">401</span> <span class=\"p\">-</span> <span class=\"nx\">File</span> <span class=\"nx\">does</span> <span class=\"nx\">not</span> <span class=\"nx\">exist</span><span class=\"p\">:</span> <span class=\"sr\">/var/</span><span class=\"nx\">run</span><span class=\"sr\">/ambari-server/</span><span class=\"nx\">bootstrap</span><span class=\"sr\">/4/</span><span class=\"nx\">sshKey</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"m\">09</span> <span class=\"nx\">七月</span> <span class=\"m\">2018</span> <span class=\"m\">14</span><span class=\"p\">:</span><span class=\"m\">44</span><span class=\"p\">:</span><span class=\"m\">02</span><span class=\"p\">,</span><span class=\"m\">863</span>  <span class=\"nx\">INFO</span> [<span class=\"nx\">ambari</span><span class=\"p\">-</span><span class=\"nx\">client</span><span class=\"p\">-</span><span class=\"nx\">thread</span><span class=\"m\">-22659</span>] <span class=\"nx\">BootStrapImpl</span><span class=\"p\">:</span><span class=\"m\">108</span> <span class=\"p\">-</span> <span class=\"nx\">BootStrapping</span> <span class=\"nx\">hosts</span> <span class=\"nx\">ambari</span>.<span class=\"nx\">slave1</span>.<span class=\"nx\">com</span><span class=\"p\">:</span></code></pre></div><h2>1.2 解决办法</h2><p>通过查看，/var/run/ambari-server目录下确实没有bootstrap文件夹，手动添加即可</p><div class=\"highlight\"><pre><code class=\"language-text\">mkdir bootstrap</code></pre></div><p>但是，如果只创建该文件夹的话，ambari依然报错，大概是没有权限相关的，就不贴出详细的异常了</p><p>为ambari用户添加权限即可</p><div class=\"highlight\"><pre><code class=\"language-text\">chown -R ambari:ambari /var/run/ambari-server/bootstrap/</code></pre></div><h2>1.3 解决添加host时给出的警告</h2><p>在添加host时，如果没有异常，最后会检查该host相关的配置，最后会给出警告，点击警告的详细信息，会看到下面的界面 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-7ea8afc3b8c79515f4d65c3b133be48d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"549\" data-rawheight=\"434\" class=\"origin_image zh-lightbox-thumb\" width=\"549\" data-original=\"https://pic2.zhimg.com/v2-7ea8afc3b8c79515f4d65c3b133be48d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;549&#39; height=&#39;434&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"549\" data-rawheight=\"434\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"549\" data-original=\"https://pic2.zhimg.com/v2-7ea8afc3b8c79515f4d65c3b133be48d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-7ea8afc3b8c79515f4d65c3b133be48d_b.jpg\"/></figure><p> 根绝他的提示，在对应的host上执行下面的命令</p><div class=\"highlight\"><pre><code class=\"language-text\">python /usr/lib/python2.6/site-packages/ambari_agent/HostCleanup.py --silent --skip=users</code></pre></div><p>在重新检查一下，可能还有警告，且警告为，该host存在一些用户，根据界面上的提示，将--skip=users去掉，即可删除这些用户</p><div class=\"highlight\"><pre><code class=\"language-bash\">python /usr/lib/python2.6/site-packages/ambari_agent/HostCleanup.py</code></pre></div><h2>2、异常二</h2><p>异常体现在，添加完host，选择要安装的组件的时候，next为灰色，不能点击，检查网页源代码为disabled(不可用)，重试几次发现结果一样，然后尝试修改界面上的代码disabled删掉，然后next按钮可用，点击next，但是依然会卡住，确定不是ambari的bug </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-70c58bf4d7c7df6fa16994ee4372c515_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1172\" data-rawheight=\"519\" class=\"origin_image zh-lightbox-thumb\" width=\"1172\" data-original=\"https://pic2.zhimg.com/v2-70c58bf4d7c7df6fa16994ee4372c515_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1172&#39; height=&#39;519&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1172\" data-rawheight=\"519\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1172\" data-original=\"https://pic2.zhimg.com/v2-70c58bf4d7c7df6fa16994ee4372c515_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-70c58bf4d7c7df6fa16994ee4372c515_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-1d4e0a60d8367c806163442e40c0a506_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1884\" data-rawheight=\"919\" class=\"origin_image zh-lightbox-thumb\" width=\"1884\" data-original=\"https://pic3.zhimg.com/v2-1d4e0a60d8367c806163442e40c0a506_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1884&#39; height=&#39;919&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1884\" data-rawheight=\"919\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1884\" data-original=\"https://pic3.zhimg.com/v2-1d4e0a60d8367c806163442e40c0a506_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-1d4e0a60d8367c806163442e40c0a506_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-308c1140495ecec44a25c06ac2b68851_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1598\" data-rawheight=\"850\" class=\"origin_image zh-lightbox-thumb\" width=\"1598\" data-original=\"https://pic2.zhimg.com/v2-308c1140495ecec44a25c06ac2b68851_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1598&#39; height=&#39;850&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1598\" data-rawheight=\"850\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1598\" data-original=\"https://pic2.zhimg.com/v2-308c1140495ecec44a25c06ac2b68851_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-308c1140495ecec44a25c06ac2b68851_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>2.1 具体的异常</h2><p>然后在浏览器的console发现ambari打印出了异常，大概是哪个目录或地址找不到，没有截图保存~，发现ambari报了异常之后，继续在ambari-server.log查看，发现确实有异常，且该异常和第一个异常类似</p><div class=\"highlight\"><pre><code class=\"language-text\">org.apache.ambari.server.api.services.stackadvisor.StackAdvisorException: Error occured during stack advisor command invocation: Cannot create /var/run/ambari-server/stack-recommendations</code></pre></div><h2>2.2 解决办法</h2><p>和第一个异常解决方法一样</p><div class=\"highlight\"><pre><code class=\"language-bash\"><span class=\"nb\">cd</span> /var/run/ambari-server\nmkdir stack-recommendations\nchown -R ambari:ambari /var/run/ambari-server/stack-recommendations/</code></pre></div><h2>2、3 重新添加host</h2><p>重新执行添加host的操作，到这一步，就可以点击next继续后面的操作了！</p>", 
            "topic": [
                {
                    "tag": "大数据", 
                    "tagLink": "https://api.zhihu.com/topics/19740929"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/51207608", 
            "userName": "董可伦", 
            "userLink": "https://www.zhihu.com/people/de888cc734ddbfd918269a8e5fb196ba", 
            "upvote": 1, 
            "title": "Spark异常：Application finished with failed status", 
            "content": "<p>我的原创地址：</p><a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/07/06/sparkSubmitException1/\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-0b4e5faa279a5e545d79e754b3dba862_ipico.jpg\" data-image-width=\"640\" data-image-height=\"640\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">spark-submit报错:Application application_1529650293575_0148 finished with failed status</a><h2>前言</h2><p>记录spark-submit提交Spark程序出现的一个异常，以供第一次出现这种异常且不知道原因，该怎么解决的的同学参考。</p><h2>1、异常信息</h2><div class=\"highlight\"><pre><code class=\"language-vim\"><span class=\"nx\">Exception</span> <span class=\"nx\">in</span> <span class=\"nx\">thread</span><span class=\"c\"> &#34;main&#34; org.apache.spark.SparkException: Application application_1529650293575_0148 finished with failed status</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"nx\">at</span> <span class=\"nx\">org</span>.<span class=\"nx\">apache</span>.<span class=\"nx\">spark</span>.<span class=\"nx\">deploy</span>.<span class=\"nx\">yarn</span>.<span class=\"nx\">Client</span>.<span class=\"nx\">run</span><span class=\"p\">(</span><span class=\"nx\">Client</span>.<span class=\"nx\">scala</span><span class=\"p\">:</span><span class=\"m\">1187</span><span class=\"p\">)</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"nx\">at</span> <span class=\"nx\">org</span>.<span class=\"nx\">apache</span>.<span class=\"nx\">spark</span>.<span class=\"nx\">deploy</span>.<span class=\"nx\">yarn</span>.<span class=\"nx\">Client</span>$.<span class=\"nx\">main</span><span class=\"p\">(</span><span class=\"nx\">Client</span>.<span class=\"nx\">scala</span><span class=\"p\">:</span><span class=\"m\">1233</span><span class=\"p\">)</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"nx\">at</span> <span class=\"nx\">org</span>.<span class=\"nx\">apache</span>.<span class=\"nx\">spark</span>.<span class=\"nx\">deploy</span>.<span class=\"nx\">yarn</span>.<span class=\"nx\">Client</span>.<span class=\"nx\">main</span><span class=\"p\">(</span><span class=\"nx\">Client</span>.<span class=\"nx\">scala</span><span class=\"p\">)</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"nx\">at</span> <span class=\"nx\">sun</span>.<span class=\"nx\">reflect</span>.<span class=\"nx\">NativeMethodAccessorImpl</span>.<span class=\"nx\">invoke0</span><span class=\"p\">(</span><span class=\"nx\">Native</span> <span class=\"nx\">Method</span><span class=\"p\">)</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"nx\">at</span> <span class=\"nx\">sun</span>.<span class=\"nx\">reflect</span>.<span class=\"nx\">NativeMethodAccessorImpl</span>.<span class=\"nx\">invoke</span><span class=\"p\">(</span><span class=\"nx\">NativeMethodAccessorImpl</span>.<span class=\"nx\">java</span><span class=\"p\">:</span><span class=\"m\">62</span><span class=\"p\">)</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"nx\">at</span> <span class=\"nx\">sun</span>.<span class=\"nx\">reflect</span>.<span class=\"nx\">DelegatingMethodAccessorImpl</span>.<span class=\"nx\">invoke</span><span class=\"p\">(</span><span class=\"nx\">DelegatingMethodAccessorImpl</span>.<span class=\"nx\">java</span><span class=\"p\">:</span><span class=\"m\">43</span><span class=\"p\">)</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"nx\">at</span> <span class=\"nx\">java</span>.<span class=\"nx\">lang</span>.<span class=\"nx\">reflect</span>.<span class=\"nx\">Method</span>.<span class=\"nx\">invoke</span><span class=\"p\">(</span><span class=\"nx\">Method</span>.<span class=\"nx\">java</span><span class=\"p\">:</span><span class=\"m\">498</span><span class=\"p\">)</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"nx\">at</span> <span class=\"nx\">org</span>.<span class=\"nx\">apache</span>.<span class=\"nx\">spark</span>.<span class=\"nx\">deploy</span>.<span class=\"nx\">SparkSubmit</span>$.<span class=\"nx\">org</span>$<span class=\"nx\">apache</span>$<span class=\"nx\">spark</span>$<span class=\"nx\">deploy</span>$<span class=\"nx\">SparkSubmit</span>$$<span class=\"nx\">runMain</span><span class=\"p\">(</span><span class=\"nx\">SparkSubmit</span>.<span class=\"nx\">scala</span><span class=\"p\">:</span><span class=\"m\">782</span><span class=\"p\">)</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"nx\">at</span> <span class=\"nx\">org</span>.<span class=\"nx\">apache</span>.<span class=\"nx\">spark</span>.<span class=\"nx\">deploy</span>.<span class=\"nx\">SparkSubmit</span>$.<span class=\"nx\">doRunMain</span>$<span class=\"m\">1</span><span class=\"p\">(</span><span class=\"nx\">SparkSubmit</span>.<span class=\"nx\">scala</span><span class=\"p\">:</span><span class=\"m\">180</span><span class=\"p\">)</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"nx\">at</span> <span class=\"nx\">org</span>.<span class=\"nx\">apache</span>.<span class=\"nx\">spark</span>.<span class=\"nx\">deploy</span>.<span class=\"nx\">SparkSubmit</span>$.<span class=\"nx\">submit</span><span class=\"p\">(</span><span class=\"nx\">SparkSubmit</span>.<span class=\"nx\">scala</span><span class=\"p\">:</span><span class=\"m\">205</span><span class=\"p\">)</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"nx\">at</span> <span class=\"nx\">org</span>.<span class=\"nx\">apache</span>.<span class=\"nx\">spark</span>.<span class=\"nx\">deploy</span>.<span class=\"nx\">SparkSubmit</span>$.<span class=\"nx\">main</span><span class=\"p\">(</span><span class=\"nx\">SparkSubmit</span>.<span class=\"nx\">scala</span><span class=\"p\">:</span><span class=\"m\">119</span><span class=\"p\">)</span><span class=\"err\">\n</span><span class=\"err\"></span>    <span class=\"nx\">at</span> <span class=\"nx\">org</span>.<span class=\"nx\">apache</span>.<span class=\"nx\">spark</span>.<span class=\"nx\">deploy</span>.<span class=\"nx\">SparkSubmit</span>.<span class=\"nx\">main</span><span class=\"p\">(</span><span class=\"nx\">SparkSubmit</span>.<span class=\"nx\">scala</span><span class=\"p\">)</span></code></pre></div><h2>2、异常原因</h2><p>出现该异常场景是spark-submit提交master为yarn cluster，yarn client没有这种问题，原因是因为，在代码里指定了master为local（本地测试用），在spark-submit提交程序时忘记删除了 * 注：上面说的是我碰到的一种情况，其实只要有错误，yarn cluster模式都会报这种异常，具体可以看目录7</p><h2>3、异常再现</h2><p>代码</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">package</span> <span class=\"nn\">com.dkl.leanring.spark.exception</span>\n\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.SparkSession</span>\n\n<span class=\"k\">object</span> <span class=\"nc\">YarnClusterDemo</span> <span class=\"o\">{</span>\n\n  <span class=\"k\">def</span> <span class=\"n\">main</span><span class=\"o\">(</span><span class=\"n\">args</span><span class=\"k\">:</span> <span class=\"kt\">Array</span><span class=\"o\">[</span><span class=\"kt\">String</span><span class=\"o\">])</span><span class=\"k\">:</span> <span class=\"kt\">Unit</span> <span class=\"o\">=</span> <span class=\"o\">{</span>\n\n    <span class=\"k\">val</span> <span class=\"n\">spark</span> <span class=\"k\">=</span> <span class=\"nc\">SparkSession</span><span class=\"o\">.</span><span class=\"n\">builder</span><span class=\"o\">().</span><span class=\"n\">appName</span><span class=\"o\">(</span><span class=\"s\">&#34;YarnClusterDemo&#34;</span><span class=\"o\">).</span><span class=\"n\">master</span><span class=\"o\">(</span><span class=\"s\">&#34;local&#34;</span><span class=\"o\">).</span><span class=\"n\">getOrCreate</span><span class=\"o\">()</span>\n    <span class=\"k\">val</span> <span class=\"n\">sc</span> <span class=\"k\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">sparkContext</span>\n\n    <span class=\"k\">val</span> <span class=\"n\">rdd</span> <span class=\"k\">=</span> <span class=\"n\">sc</span><span class=\"o\">.</span><span class=\"n\">parallelize</span><span class=\"o\">(</span><span class=\"nc\">Seq</span><span class=\"o\">(</span><span class=\"mi\">1</span><span class=\"o\">,</span> <span class=\"mi\">2</span><span class=\"o\">,</span> <span class=\"mi\">3</span><span class=\"o\">))</span>\n    <span class=\"n\">println</span><span class=\"o\">(</span><span class=\"n\">rdd</span><span class=\"o\">.</span><span class=\"n\">count</span><span class=\"o\">)</span>\n\n    <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">stop</span><span class=\"o\">()</span>\n  <span class=\"o\">}</span>\n<span class=\"o\">}</span></code></pre></div><p>这段代码在本地是没有问题，输出结果为3，把它打包，然后放在集群上，用spark-submit提交测试一下</p><h2>4、yarn client</h2><p>--master yarn 默认的就是client模式，所以用下面的命令</p><div class=\"highlight\"><pre><code class=\"language-bash\">spark-submit --master yarn --class com.dkl.leanring.spark.exception.YarnClusterDemo spark-scala_2.11-1.0.jar</code></pre></div><p>等价于</p><div class=\"highlight\"><pre><code class=\"language-text\">spark-submit --master yarn --deploy-mode client --class com.dkl.leanring.spark.exception.YarnClusterDemo spark-scala_2.11-1.0.jar</code></pre></div><p>结果也会正常打印出来，因为在代码里指定了master为local所以实际上应该还是用的local，但是没有研究client模式不报错，可能是用的client模式用提交代码的那台机器为Driver，然后再用local模式吧</p><h2>5、yarn cluster</h2><p>用下面的命令，即可再现异常</p><div class=\"highlight\"><pre><code class=\"language-text\">spark-submit --master yarn --deploy-mode cluster --class com.dkl.leanring.spark.exception.YarnClusterDemo spark-scala_2.11-1.0.jar</code></pre></div><p>所以实际用spark-submit提交程序的时候，将master在代码里删掉，然后用命令行--master指定即可</p><div class=\"highlight\"><pre><code class=\"language-text\">val spark = SparkSession.builder().appName(&#34;YarnClusterDemo&#34;).getOrCreate()</code></pre></div><h2>6、附图</h2><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-6c13d4a336babfe2b55a497e0c83b3cf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1672\" data-rawheight=\"413\" class=\"origin_image zh-lightbox-thumb\" width=\"1672\" data-original=\"https://pic4.zhimg.com/v2-6c13d4a336babfe2b55a497e0c83b3cf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1672&#39; height=&#39;413&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1672\" data-rawheight=\"413\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1672\" data-original=\"https://pic4.zhimg.com/v2-6c13d4a336babfe2b55a497e0c83b3cf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-6c13d4a336babfe2b55a497e0c83b3cf_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-177cb5b6ac0ad6df125ce0c0a83b106a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1135\" data-rawheight=\"284\" class=\"origin_image zh-lightbox-thumb\" width=\"1135\" data-original=\"https://pic3.zhimg.com/v2-177cb5b6ac0ad6df125ce0c0a83b106a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1135&#39; height=&#39;284&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1135\" data-rawheight=\"284\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1135\" data-original=\"https://pic3.zhimg.com/v2-177cb5b6ac0ad6df125ce0c0a83b106a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-177cb5b6ac0ad6df125ce0c0a83b106a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>7、更新：其他情况</h2><p>在我后面用yarn cluster模式的时候，又碰到了一样的异常信息，检查了一下master没有在代码里指定为local，经过一步步的测试，最后发现，是由于有一行代码读取的本地文件，在yarn client模式的时候，提交程序的那台机器即为Driver，会从Driver的机器上对应的目录找本地文件，所以在yarn client的时候不会报错（Driver有该文件）在yarn cluster会随机选择集群上一个节点作为Driver，那么Driver没有该文件，就会产生异常，而yarn cluster模式返回的异常不很明确，只是返回</p><div class=\"highlight\"><pre><code class=\"language-vim\"><span class=\"nx\">Exception</span> <span class=\"nx\">in</span> <span class=\"nx\">thread</span><span class=\"c\"> &#34;main&#34; org.apache.spark.SparkException: Application application_*** finished with failed status</span></code></pre></div><p>这样的异常，所以不知道是哪里出错了，可以在代码里加上捕获异常的代码，这样就可以在yarn的日志里看到具体的异常信息了，具体的代码我就不贴了，贴一下捕获异常的</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">try</span> <span class=\"o\">{</span>\n      <span class=\"c1\">//此处为你的可能产生异常的代码\n</span><span class=\"c1\"></span>   <span class=\"o\">}</span> <span class=\"k\">catch</span> <span class=\"o\">{</span>\n     <span class=\"k\">case</span> <span class=\"n\">e</span><span class=\"k\">:</span> <span class=\"kt\">Exception</span> <span class=\"o\">=&gt;</span> <span class=\"n\">println</span><span class=\"o\">(</span><span class=\"n\">e</span><span class=\"o\">)</span>\n   <span class=\"o\">}</span></code></pre></div><p>在加了捕获异常代码后，yarn日志 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-1382a88354f8f91c7df9213b7ef23306_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1456\" data-rawheight=\"260\" class=\"origin_image zh-lightbox-thumb\" width=\"1456\" data-original=\"https://pic3.zhimg.com/v2-1382a88354f8f91c7df9213b7ef23306_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1456&#39; height=&#39;260&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1456\" data-rawheight=\"260\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1456\" data-original=\"https://pic3.zhimg.com/v2-1382a88354f8f91c7df9213b7ef23306_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-1382a88354f8f91c7df9213b7ef23306_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "Spark", 
                    "tagLink": "https://api.zhihu.com/topics/19942170"
                }, 
                {
                    "tag": "Hadoop", 
                    "tagLink": "https://api.zhihu.com/topics/19563390"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/51117565", 
            "userName": "董可伦", 
            "userLink": "https://www.zhihu.com/people/de888cc734ddbfd918269a8e5fb196ba", 
            "upvote": 1, 
            "title": "Spark DataFrame按某列降序排序", 
            "content": "<p>我的原创地址：</p><a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/07/04/sparkDfSortDesc/\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-0b4e5faa279a5e545d79e754b3dba862_ipico.jpg\" data-image-width=\"640\" data-image-height=\"640\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Spark DataFrame按某列降序排序</a><h2>前言</h2><p>本文总结如何将DataFrame按某列降序排序，因为Spark默认的排序方式为升序，而降序的用法和java语言等又不一样，所以需要特地总结记录一下其用法。</p><h2>1、创建测试用DataFrame</h2><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">val</span> <span class=\"n\">data</span> <span class=\"k\">=</span> <span class=\"nc\">Array</span><span class=\"o\">((</span><span class=\"mi\">7</span><span class=\"o\">,</span> <span class=\"mi\">2</span><span class=\"o\">,</span> <span class=\"mi\">3</span><span class=\"o\">),</span> <span class=\"o\">(</span><span class=\"mi\">1</span><span class=\"o\">,</span> <span class=\"mi\">8</span><span class=\"o\">,</span> <span class=\"mi\">6</span><span class=\"o\">),</span> <span class=\"o\">(</span><span class=\"mi\">4</span><span class=\"o\">,</span> <span class=\"mi\">5</span><span class=\"o\">,</span> <span class=\"mi\">9</span><span class=\"o\">))</span>\n<span class=\"k\">val</span> <span class=\"n\">df</span> <span class=\"k\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"o\">(</span><span class=\"n\">data</span><span class=\"o\">).</span><span class=\"n\">toDF</span><span class=\"o\">(</span><span class=\"s\">&#34;col1&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;col2&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;col3&#34;</span><span class=\"o\">)</span>\n<span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"o\">()</span>\n<span class=\"o\">+----+----+----+</span>\n<span class=\"o\">|</span><span class=\"n\">col1</span><span class=\"o\">|</span><span class=\"n\">col2</span><span class=\"o\">|</span><span class=\"n\">col3</span><span class=\"o\">|</span>\n<span class=\"o\">+----+----+----+</span>\n<span class=\"o\">|</span>   <span class=\"mi\">7</span><span class=\"o\">|</span>   <span class=\"mi\">2</span><span class=\"o\">|</span>   <span class=\"mi\">3</span><span class=\"o\">|</span>\n<span class=\"o\">|</span>   <span class=\"mi\">1</span><span class=\"o\">|</span>   <span class=\"mi\">8</span><span class=\"o\">|</span>   <span class=\"mi\">6</span><span class=\"o\">|</span>\n<span class=\"o\">|</span>   <span class=\"mi\">4</span><span class=\"o\">|</span>   <span class=\"mi\">5</span><span class=\"o\">|</span>   <span class=\"mi\">9</span><span class=\"o\">|</span>\n<span class=\"o\">+----+----+----+</span></code></pre></div><h2>2、默认的升序排序效果(按col2排序，以下都是)</h2><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">orderBy</span><span class=\"o\">(</span><span class=\"s\">&#34;col2&#34;</span><span class=\"o\">).</span><span class=\"n\">show</span><span class=\"o\">()</span>\n<span class=\"o\">+----+----+----+</span>\n<span class=\"o\">|</span><span class=\"n\">col1</span><span class=\"o\">|</span><span class=\"n\">col2</span><span class=\"o\">|</span><span class=\"n\">col3</span><span class=\"o\">|</span>\n<span class=\"o\">+----+----+----+</span>\n<span class=\"o\">|</span>   <span class=\"mi\">7</span><span class=\"o\">|</span>   <span class=\"mi\">2</span><span class=\"o\">|</span>   <span class=\"mi\">3</span><span class=\"o\">|</span>\n<span class=\"o\">|</span>   <span class=\"mi\">4</span><span class=\"o\">|</span>   <span class=\"mi\">5</span><span class=\"o\">|</span>   <span class=\"mi\">9</span><span class=\"o\">|</span>\n<span class=\"o\">|</span>   <span class=\"mi\">1</span><span class=\"o\">|</span>   <span class=\"mi\">8</span><span class=\"o\">|</span>   <span class=\"mi\">6</span><span class=\"o\">|</span>\n<span class=\"o\">+----+----+----+</span></code></pre></div><h2>3、降序方法一</h2><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">orderBy</span><span class=\"o\">(-</span><span class=\"n\">df</span><span class=\"o\">(</span><span class=\"s\">&#34;col2&#34;</span><span class=\"o\">)).</span><span class=\"n\">show</span>\n<span class=\"o\">+----+----+----+</span>\n<span class=\"o\">|</span><span class=\"n\">col1</span><span class=\"o\">|</span><span class=\"n\">col2</span><span class=\"o\">|</span><span class=\"n\">col3</span><span class=\"o\">|</span>\n<span class=\"o\">+----+----+----+</span>\n<span class=\"o\">|</span>   <span class=\"mi\">1</span><span class=\"o\">|</span>   <span class=\"mi\">8</span><span class=\"o\">|</span>   <span class=\"mi\">6</span><span class=\"o\">|</span>\n<span class=\"o\">|</span>   <span class=\"mi\">4</span><span class=\"o\">|</span>   <span class=\"mi\">5</span><span class=\"o\">|</span>   <span class=\"mi\">9</span><span class=\"o\">|</span>\n<span class=\"o\">|</span>   <span class=\"mi\">7</span><span class=\"o\">|</span>   <span class=\"mi\">2</span><span class=\"o\">|</span>   <span class=\"mi\">3</span><span class=\"o\">|</span>\n<span class=\"o\">+----+----+----+</span></code></pre></div><p>这个方法在前面加上负号-即可，看起来挺简单的，但是其实这种方法不能在第一次构建df的时候进行排序，必须先创建好一个df，再用创建好的df生成新的df。</p><h2>4、降序方法二</h2><p>下面的方法和方法一是一样的</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">orderBy</span><span class=\"o\">(</span><span class=\"n\">df</span><span class=\"o\">(</span><span class=\"s\">&#34;col2&#34;</span><span class=\"o\">).</span><span class=\"n\">desc</span><span class=\"o\">).</span><span class=\"n\">show</span>\n<span class=\"o\">+----+----+----+</span>\n<span class=\"o\">|</span><span class=\"n\">col1</span><span class=\"o\">|</span><span class=\"n\">col2</span><span class=\"o\">|</span><span class=\"n\">col3</span><span class=\"o\">|</span>\n<span class=\"o\">+----+----+----+</span>\n<span class=\"o\">|</span>   <span class=\"mi\">1</span><span class=\"o\">|</span>   <span class=\"mi\">8</span><span class=\"o\">|</span>   <span class=\"mi\">6</span><span class=\"o\">|</span>\n<span class=\"o\">|</span>   <span class=\"mi\">4</span><span class=\"o\">|</span>   <span class=\"mi\">5</span><span class=\"o\">|</span>   <span class=\"mi\">9</span><span class=\"o\">|</span>\n<span class=\"o\">|</span>   <span class=\"mi\">7</span><span class=\"o\">|</span>   <span class=\"mi\">2</span><span class=\"o\">|</span>   <span class=\"mi\">3</span><span class=\"o\">|</span>\n<span class=\"o\">+----+----+----+</span></code></pre></div><h2>5、降序方法三</h2><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.functions._</span>\n<span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">orderBy</span><span class=\"o\">(</span><span class=\"n\">desc</span><span class=\"o\">(</span><span class=\"s\">&#34;col2&#34;</span><span class=\"o\">)).</span><span class=\"n\">show</span>\n<span class=\"o\">+----+----+----+</span>\n<span class=\"o\">|</span><span class=\"n\">col1</span><span class=\"o\">|</span><span class=\"n\">col2</span><span class=\"o\">|</span><span class=\"n\">col3</span><span class=\"o\">|</span>\n<span class=\"o\">+----+----+----+</span>\n<span class=\"o\">|</span>   <span class=\"mi\">1</span><span class=\"o\">|</span>   <span class=\"mi\">8</span><span class=\"o\">|</span>   <span class=\"mi\">6</span><span class=\"o\">|</span>\n<span class=\"o\">|</span>   <span class=\"mi\">4</span><span class=\"o\">|</span>   <span class=\"mi\">5</span><span class=\"o\">|</span>   <span class=\"mi\">9</span><span class=\"o\">|</span>\n<span class=\"o\">|</span>   <span class=\"mi\">7</span><span class=\"o\">|</span>   <span class=\"mi\">2</span><span class=\"o\">|</span>   <span class=\"mi\">3</span><span class=\"o\">|</span>\n<span class=\"o\">+----+----+----+</span></code></pre></div><p>这种方法是我比较喜欢的，因为在第一次创建的时候就可以排序了，且使用起来也很简洁。 可以使用下面的代码测试一下</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"o\">(</span><span class=\"n\">data</span><span class=\"o\">).</span><span class=\"n\">toDF</span><span class=\"o\">(</span><span class=\"s\">&#34;col1&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;col2&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;col3&#34;</span><span class=\"o\">).</span><span class=\"n\">orderBy</span><span class=\"o\">(</span><span class=\"n\">desc</span><span class=\"o\">(</span><span class=\"s\">&#34;col2&#34;</span><span class=\"o\">)).</span><span class=\"n\">show</span></code></pre></div><ul><li>注：上面导入的包，在spark-shell里执行的时候是不需要的</li></ul><h2>6、降序方法四</h2><p>下面的方法和方法三是一样的，由于结果一样，就不贴上了</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">orderBy</span><span class=\"o\">(-</span><span class=\"n\">col</span><span class=\"o\">(</span><span class=\"s\">&#34;col2&#34;</span><span class=\"o\">)).</span><span class=\"n\">show</span></code></pre></div><h2>7、降序方法五</h2><p>下面的方法和方法四是一样的，由于结果一样，就不贴上了</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">orderBy</span><span class=\"o\">(</span><span class=\"n\">col</span><span class=\"o\">(</span><span class=\"s\">&#34;col2&#34;</span><span class=\"o\">).</span><span class=\"n\">desc</span><span class=\"o\">).</span><span class=\"n\">show</span></code></pre></div><h2>8、sort函数</h2><p>sort函数和orderBy用法和结果是一样的，因为orderBy和sql语法里的order by名字一样，所以我首先想到这个方法，就把orderBy放在前面介绍了（sort比orderBy短一点哈~）</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">sort</span><span class=\"o\">(</span><span class=\"n\">desc</span><span class=\"o\">(</span><span class=\"s\">&#34;col2&#34;</span><span class=\"o\">)).</span><span class=\"n\">show</span></code></pre></div><h2>附录</h2><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">package</span> <span class=\"nn\">com.dkl.leanring.spark.df</span>\n\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.SparkSession</span>\n\n<span class=\"k\">object</span> <span class=\"nc\">DfSortDesc</span> <span class=\"o\">{</span>\n  <span class=\"k\">def</span> <span class=\"n\">main</span><span class=\"o\">(</span><span class=\"n\">args</span><span class=\"k\">:</span> <span class=\"kt\">Array</span><span class=\"o\">[</span><span class=\"kt\">String</span><span class=\"o\">])</span><span class=\"k\">:</span> <span class=\"kt\">Unit</span> <span class=\"o\">=</span> <span class=\"o\">{</span>\n    <span class=\"k\">val</span> <span class=\"n\">spark</span> <span class=\"k\">=</span> <span class=\"nc\">SparkSession</span><span class=\"o\">.</span><span class=\"n\">builder</span><span class=\"o\">().</span><span class=\"n\">appName</span><span class=\"o\">(</span><span class=\"s\">&#34;DfSortDesc&#34;</span><span class=\"o\">).</span><span class=\"n\">master</span><span class=\"o\">(</span><span class=\"s\">&#34;local&#34;</span><span class=\"o\">).</span><span class=\"n\">getOrCreate</span><span class=\"o\">()</span>\n\n    <span class=\"k\">val</span> <span class=\"n\">data</span> <span class=\"k\">=</span> <span class=\"nc\">Array</span><span class=\"o\">((</span><span class=\"mi\">7</span><span class=\"o\">,</span> <span class=\"mi\">2</span><span class=\"o\">,</span> <span class=\"mi\">3</span><span class=\"o\">),</span> <span class=\"o\">(</span><span class=\"mi\">1</span><span class=\"o\">,</span> <span class=\"mi\">8</span><span class=\"o\">,</span> <span class=\"mi\">6</span><span class=\"o\">),</span> <span class=\"o\">(</span><span class=\"mi\">4</span><span class=\"o\">,</span> <span class=\"mi\">5</span><span class=\"o\">,</span> <span class=\"mi\">9</span><span class=\"o\">))</span>\n    <span class=\"k\">val</span> <span class=\"n\">df</span> <span class=\"k\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"o\">(</span><span class=\"n\">data</span><span class=\"o\">).</span><span class=\"n\">toDF</span><span class=\"o\">(</span><span class=\"s\">&#34;col1&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;col2&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;col3&#34;</span><span class=\"o\">)</span>\n    <span class=\"c1\">//打印 df\n</span><span class=\"c1\"></span>    <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"o\">()</span>\n    <span class=\"c1\">// 默认的升序\n</span><span class=\"c1\"></span>    <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">orderBy</span><span class=\"o\">(</span><span class=\"s\">&#34;col2&#34;</span><span class=\"o\">).</span><span class=\"n\">show</span><span class=\"o\">()</span>\n    <span class=\"c1\">//降序方法一\n</span><span class=\"c1\"></span>    <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">orderBy</span><span class=\"o\">(-</span><span class=\"n\">df</span><span class=\"o\">(</span><span class=\"s\">&#34;col2&#34;</span><span class=\"o\">)).</span><span class=\"n\">show</span>\n    <span class=\"c1\">//降序方法二同上\n</span><span class=\"c1\"></span>    <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">orderBy</span><span class=\"o\">(</span><span class=\"n\">df</span><span class=\"o\">(</span><span class=\"s\">&#34;col2&#34;</span><span class=\"o\">).</span><span class=\"n\">desc</span><span class=\"o\">).</span><span class=\"n\">show</span>\n\n    <span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.functions._</span>\n    <span class=\"c1\">//降序方法三\n</span><span class=\"c1\"></span>    <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">orderBy</span><span class=\"o\">(</span><span class=\"n\">desc</span><span class=\"o\">(</span><span class=\"s\">&#34;col2&#34;</span><span class=\"o\">)).</span><span class=\"n\">show</span>\n    <span class=\"c1\">//测试方法三\n</span><span class=\"c1\"></span>    <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"o\">(</span><span class=\"n\">data</span><span class=\"o\">).</span><span class=\"n\">toDF</span><span class=\"o\">(</span><span class=\"s\">&#34;col1&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;col2&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;col3&#34;</span><span class=\"o\">).</span><span class=\"n\">orderBy</span><span class=\"o\">(</span><span class=\"n\">desc</span><span class=\"o\">(</span><span class=\"s\">&#34;col2&#34;</span><span class=\"o\">)).</span><span class=\"n\">show</span>\n\n    <span class=\"c1\">//降序方法四\n</span><span class=\"c1\"></span>    <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">orderBy</span><span class=\"o\">(-</span><span class=\"n\">col</span><span class=\"o\">(</span><span class=\"s\">&#34;col2&#34;</span><span class=\"o\">)).</span><span class=\"n\">show</span>\n    <span class=\"c1\">//降序方法五\n</span><span class=\"c1\"></span>    <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">orderBy</span><span class=\"o\">(</span><span class=\"n\">col</span><span class=\"o\">(</span><span class=\"s\">&#34;col2&#34;</span><span class=\"o\">).</span><span class=\"n\">desc</span><span class=\"o\">).</span><span class=\"n\">show</span>\n    <span class=\"c1\">//sort函数和orderBy用法和结果是一样的\n</span><span class=\"c1\"></span>    <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">sort</span><span class=\"o\">(</span><span class=\"n\">desc</span><span class=\"o\">(</span><span class=\"s\">&#34;col2&#34;</span><span class=\"o\">)).</span><span class=\"n\">show</span>\n    <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">stop</span><span class=\"o\">()</span>\n\n  <span class=\"o\">}</span>\n\n<span class=\"o\">}</span></code></pre></div><h2>参考资料</h2><p><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/dabokele/article/details/52802150\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Spark-SQL之DataFrame操作大全</a> <a href=\"https://link.zhihu.com/?target=https%3A//www.sohu.com/a/143978243_692906\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">在Apache Spark 2.0中使用DataFrames和SQL</a></p>", 
            "topic": [
                {
                    "tag": "Spark", 
                    "tagLink": "https://api.zhihu.com/topics/19942170"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/51007461", 
            "userName": "董可伦", 
            "userLink": "https://www.zhihu.com/people/de888cc734ddbfd918269a8e5fb196ba", 
            "upvote": 2, 
            "title": "Spark获取当前分区的partitionId", 
            "content": "<p></p><p>我的原创地址：</p><a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/06/28/sparkGetPartitionId/\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic4.zhimg.com/v2-08dd336023ebf8e4b7463a8518b72a37_180x120.jpg\" data-image-width=\"1819\" data-image-height=\"920\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Spark获取当前分区的partitionId</a><h2>前言</h2><p>本文讲解Spark如何获取当前分区的partitionId，这是一位群友提出的问题，其实只要通过TaskContext.get.partitionId（我是在官网上看到的），下面给出一些示例。</p><h2>1、代码</h2><p>下面的代码主要测试SparkSession，SparkContext创建的rdd和df是否都支持。</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">package</span> <span class=\"nn\">com.dkl.leanring.partition</span>\n\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.SparkSession</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.TaskContext</span>\n\n<span class=\"cm\">/**\n</span><span class=\"cm\"> * 获取当前分区的partitionId\n</span><span class=\"cm\"> */</span>\n<span class=\"k\">object</span> <span class=\"nc\">GetPartitionIdDemo</span> <span class=\"o\">{</span>\n  <span class=\"k\">def</span> <span class=\"n\">main</span><span class=\"o\">(</span><span class=\"n\">args</span><span class=\"k\">:</span> <span class=\"kt\">Array</span><span class=\"o\">[</span><span class=\"kt\">String</span><span class=\"o\">])</span><span class=\"k\">:</span> <span class=\"kt\">Unit</span> <span class=\"o\">=</span> <span class=\"o\">{</span>\n    <span class=\"k\">val</span> <span class=\"n\">spark</span> <span class=\"k\">=</span> <span class=\"nc\">SparkSession</span><span class=\"o\">.</span><span class=\"n\">builder</span><span class=\"o\">().</span><span class=\"n\">appName</span><span class=\"o\">(</span><span class=\"s\">&#34;GetPartitionIdDemo&#34;</span><span class=\"o\">).</span><span class=\"n\">master</span><span class=\"o\">(</span><span class=\"s\">&#34;local&#34;</span><span class=\"o\">).</span><span class=\"n\">getOrCreate</span><span class=\"o\">()</span>\n    <span class=\"k\">val</span> <span class=\"n\">sc</span> <span class=\"k\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">sparkContext</span>\n    <span class=\"k\">val</span> <span class=\"n\">data</span> <span class=\"k\">=</span> <span class=\"nc\">Seq</span><span class=\"o\">(</span><span class=\"mi\">1</span><span class=\"o\">,</span> <span class=\"mi\">2</span><span class=\"o\">,</span> <span class=\"mi\">3</span><span class=\"o\">,</span> <span class=\"mi\">4</span><span class=\"o\">)</span>\n\n    <span class=\"c1\">// 测试rdd,三个分区\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">rdd</span> <span class=\"k\">=</span> <span class=\"n\">sc</span><span class=\"o\">.</span><span class=\"n\">parallelize</span><span class=\"o\">(</span><span class=\"n\">data</span><span class=\"o\">,</span> <span class=\"mi\">3</span><span class=\"o\">)</span>\n    <span class=\"n\">rdd</span><span class=\"o\">.</span><span class=\"n\">foreach</span><span class=\"o\">(</span><span class=\"n\">i</span> <span class=\"k\">=&gt;</span> <span class=\"o\">{</span>\n      <span class=\"n\">println</span><span class=\"o\">(</span><span class=\"s\">&#34;partitionId：&#34;</span> <span class=\"o\">+</span> <span class=\"nc\">TaskContext</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"o\">.</span><span class=\"n\">partitionId</span><span class=\"o\">)</span>\n    <span class=\"o\">})</span>\n\n    <span class=\"k\">import</span> <span class=\"nn\">spark.implicits._</span>\n    <span class=\"c1\">// 测试df,三个分区\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">df</span> <span class=\"k\">=</span> <span class=\"n\">rdd</span><span class=\"o\">.</span><span class=\"n\">toDF</span><span class=\"o\">(</span><span class=\"s\">&#34;id&#34;</span><span class=\"o\">)</span>\n    <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">show</span>\n    <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">foreach</span><span class=\"o\">(</span><span class=\"n\">row</span> <span class=\"k\">=&gt;</span> <span class=\"o\">{</span>\n      <span class=\"n\">println</span><span class=\"o\">(</span><span class=\"s\">&#34;partitionId：&#34;</span> <span class=\"o\">+</span> <span class=\"nc\">TaskContext</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"o\">.</span><span class=\"n\">partitionId</span><span class=\"o\">)</span>\n    <span class=\"o\">})</span>\n    <span class=\"c1\">// 测试df,两个分区\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">data1</span> <span class=\"k\">=</span> <span class=\"nc\">Array</span><span class=\"o\">((</span><span class=\"mi\">1</span><span class=\"o\">,</span> <span class=\"mi\">2</span><span class=\"o\">),</span> <span class=\"o\">(</span><span class=\"mi\">3</span><span class=\"o\">,</span> <span class=\"mi\">4</span><span class=\"o\">))</span>\n    <span class=\"k\">val</span> <span class=\"n\">df1</span> <span class=\"k\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"o\">(</span><span class=\"n\">data1</span><span class=\"o\">).</span><span class=\"n\">repartition</span><span class=\"o\">(</span><span class=\"mi\">2</span><span class=\"o\">)</span>\n    <span class=\"n\">df1</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"o\">()</span>\n    <span class=\"n\">df1</span><span class=\"o\">.</span><span class=\"n\">foreach</span><span class=\"o\">(</span><span class=\"n\">row</span> <span class=\"k\">=&gt;</span> <span class=\"o\">{</span>\n      <span class=\"n\">println</span><span class=\"o\">(</span><span class=\"s\">&#34;partitionId：&#34;</span> <span class=\"o\">+</span> <span class=\"nc\">TaskContext</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"o\">.</span><span class=\"n\">partitionId</span><span class=\"o\">)</span>\n    <span class=\"o\">})</span>\n\n  <span class=\"o\">}</span>\n<span class=\"o\">}</span></code></pre></div><h2>2、结果</h2><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-4dd2b08c94bcb840138204a25c3bc132_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1819\" data-rawheight=\"920\" class=\"origin_image zh-lightbox-thumb\" width=\"1819\" data-original=\"https://pic3.zhimg.com/v2-4dd2b08c94bcb840138204a25c3bc132_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1819&#39; height=&#39;920&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1819\" data-rawheight=\"920\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1819\" data-original=\"https://pic3.zhimg.com/v2-4dd2b08c94bcb840138204a25c3bc132_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-4dd2b08c94bcb840138204a25c3bc132_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "Spark", 
                    "tagLink": "https://api.zhihu.com/topics/19942170"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50861742", 
            "userName": "董可伦", 
            "userLink": "https://www.zhihu.com/people/de888cc734ddbfd918269a8e5fb196ba", 
            "upvote": 3, 
            "title": "SparkStreaming+Kafka 实现统计基于缓存的实时uv", 
            "content": "<p>我的原创地址：</p><a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/06/25/KafkaUV/\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic2.zhimg.com/v2-d7c3d533bd3137656ed2641617d3de39_180x120.jpg\" data-image-width=\"1849\" data-image-height=\"919\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">SparkStreaming+Kafka 实现统计基于缓存的实时uv</a><p class=\"ztext-empty-paragraph\"><br/></p><h2>前言</h2><p>本文利用SparkStreaming+Kafka实现实时的统计uv，即独立访客，一个用户一天内访问多次算一次，这个看起来要对用户去重，其实只要按照WordCount的思路，最后输出key的数量即可，所以可以利用<a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/06/14/updateStateBykeyWordCount/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">SparkStreaming+Kafka 实现基于缓存的实时wordcount程序</a>，这里稍加改动，如果uv数量增加的话就打印uv的数量(key的数量)。</p><h2>1、数据</h2><p>数据是我随机在kafka里生产的几条，用户以空格区分开（因为用的之前单词统计的程序）</p><h2>2、kafka topic</h2><p>首先在kafka建一个程序用到topic:KafkaUV</p><div class=\"highlight\"><pre><code class=\"language-bash\">bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor <span class=\"m\">1</span> --partitions <span class=\"m\">1</span> --topic KafkaUV</code></pre></div><h2>3、创建checkpoint的hdfs目录</h2><p>我的目录为：/spark/dkl/kafka/UV_checkpoint</p><div class=\"highlight\"><pre><code class=\"language-bash\">hadoop fs -mkdir -p /spark/dkl/kafka/UV_checkpoint</code></pre></div><h2>4、Spark代码</h2><p>启动下面的程序</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">package</span> <span class=\"nn\">com.dkl.leanring.spark.kafka</span>\n\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.streaming.StreamingContext</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.SparkSession</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.streaming.Seconds</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.kafka.common.serialization.StringDeserializer</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.streaming.kafka010.KafkaUtils</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe</span>\n\n<span class=\"k\">object</span> <span class=\"nc\">KafkaUV</span> <span class=\"o\">{</span>\n  <span class=\"k\">def</span> <span class=\"n\">main</span><span class=\"o\">(</span><span class=\"n\">args</span><span class=\"k\">:</span> <span class=\"kt\">Array</span><span class=\"o\">[</span><span class=\"kt\">String</span><span class=\"o\">])</span><span class=\"k\">:</span> <span class=\"kt\">Unit</span> <span class=\"o\">=</span> <span class=\"o\">{</span>\n    <span class=\"c1\">//初始化，创建SparkSession\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">spark</span> <span class=\"k\">=</span> <span class=\"nc\">SparkSession</span><span class=\"o\">.</span><span class=\"n\">builder</span><span class=\"o\">().</span><span class=\"n\">appName</span><span class=\"o\">(</span><span class=\"s\">&#34;KafkaUV&#34;</span><span class=\"o\">).</span><span class=\"n\">master</span><span class=\"o\">(</span><span class=\"s\">&#34;local[2]&#34;</span><span class=\"o\">).</span><span class=\"n\">enableHiveSupport</span><span class=\"o\">().</span><span class=\"n\">getOrCreate</span><span class=\"o\">()</span>\n    <span class=\"c1\">//初始化，创建sparkContext\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">sc</span> <span class=\"k\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">sparkContext</span>\n    <span class=\"c1\">//初始化，创建StreamingContext，batchDuration为5秒\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">ssc</span> <span class=\"k\">=</span> <span class=\"k\">new</span> <span class=\"nc\">StreamingContext</span><span class=\"o\">(</span><span class=\"n\">sc</span><span class=\"o\">,</span> <span class=\"nc\">Seconds</span><span class=\"o\">(</span><span class=\"mi\">5</span><span class=\"o\">))</span>\n\n    <span class=\"c1\">//开启checkpoint机制\n</span><span class=\"c1\"></span>    <span class=\"n\">ssc</span><span class=\"o\">.</span><span class=\"n\">checkpoint</span><span class=\"o\">(</span><span class=\"s\">&#34;hdfs://ambari.master.com:8020/spark/dkl/kafka/UV_checkpoint&#34;</span><span class=\"o\">)</span>\n\n    <span class=\"c1\">//kafka集群地址\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">server</span> <span class=\"k\">=</span> <span class=\"s\">&#34;ambari.master.com:6667&#34;</span>\n\n    <span class=\"c1\">//配置消费者\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">kafkaParams</span> <span class=\"k\">=</span> <span class=\"nc\">Map</span><span class=\"o\">[</span><span class=\"kt\">String</span>, <span class=\"kt\">Object</span><span class=\"o\">](</span>\n      <span class=\"s\">&#34;bootstrap.servers&#34;</span> <span class=\"o\">-&gt;</span> <span class=\"n\">server</span><span class=\"o\">,</span> <span class=\"c1\">//kafka集群地址\n</span><span class=\"c1\"></span>      <span class=\"s\">&#34;key.deserializer&#34;</span> <span class=\"o\">-&gt;</span> <span class=\"n\">classOf</span><span class=\"o\">[</span><span class=\"kt\">StringDeserializer</span><span class=\"o\">],</span>\n      <span class=\"s\">&#34;value.deserializer&#34;</span> <span class=\"o\">-&gt;</span> <span class=\"n\">classOf</span><span class=\"o\">[</span><span class=\"kt\">StringDeserializer</span><span class=\"o\">],</span>\n      <span class=\"s\">&#34;group.id&#34;</span> <span class=\"o\">-&gt;</span> <span class=\"s\">&#34;UpdateStateBykeyWordCount&#34;</span><span class=\"o\">,</span> <span class=\"c1\">//消费者组名\n</span><span class=\"c1\"></span>      <span class=\"s\">&#34;auto.offset.reset&#34;</span> <span class=\"o\">-&gt;</span> <span class=\"s\">&#34;latest&#34;</span><span class=\"o\">,</span> <span class=\"c1\">//latest自动重置偏移量为最新的偏移量   earliest 、none\n</span><span class=\"c1\"></span>      <span class=\"s\">&#34;enable.auto.commit&#34;</span> <span class=\"o\">-&gt;</span> <span class=\"o\">(</span><span class=\"kc\">false</span><span class=\"k\">:</span> <span class=\"kt\">java.lang.Boolean</span><span class=\"o\">))</span> <span class=\"c1\">//如果是true，则这个消费者的偏移量会在后台自动提交\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">topics</span> <span class=\"k\">=</span> <span class=\"nc\">Array</span><span class=\"o\">(</span><span class=\"s\">&#34;KafkaUV&#34;</span><span class=\"o\">)</span> <span class=\"c1\">//消费主题\n</span><span class=\"c1\"></span>\n    <span class=\"c1\">//基于Direct方式创建DStream\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">stream</span> <span class=\"k\">=</span> <span class=\"nc\">KafkaUtils</span><span class=\"o\">.</span><span class=\"n\">createDirectStream</span><span class=\"o\">(</span><span class=\"n\">ssc</span><span class=\"o\">,</span> <span class=\"nc\">PreferConsistent</span><span class=\"o\">,</span> <span class=\"nc\">Subscribe</span><span class=\"o\">[</span><span class=\"kt\">String</span>, <span class=\"kt\">String</span><span class=\"o\">](</span><span class=\"n\">topics</span><span class=\"o\">,</span> <span class=\"n\">kafkaParams</span><span class=\"o\">))</span>\n\n    <span class=\"c1\">//开始执行WordCount程序\n</span><span class=\"c1\"></span>\n    <span class=\"c1\">//以空格为切分符切分单词，并转化为 (word,1)形式\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">words</span> <span class=\"k\">=</span> <span class=\"n\">stream</span><span class=\"o\">.</span><span class=\"n\">flatMap</span><span class=\"o\">(</span><span class=\"k\">_</span><span class=\"o\">.</span><span class=\"n\">value</span><span class=\"o\">().</span><span class=\"n\">split</span><span class=\"o\">(</span><span class=\"s\">&#34; &#34;</span><span class=\"o\">)).</span><span class=\"n\">map</span><span class=\"o\">((</span><span class=\"k\">_</span><span class=\"o\">,</span> <span class=\"mi\">1</span><span class=\"o\">))</span>\n    <span class=\"k\">val</span> <span class=\"n\">wordCounts</span> <span class=\"k\">=</span> <span class=\"n\">words</span><span class=\"o\">.</span><span class=\"n\">updateStateByKey</span><span class=\"o\">(</span>\n      <span class=\"c1\">//每个单词每次batch计算的时候都会调用这个函数\n</span><span class=\"c1\"></span>      <span class=\"c1\">//第一个参数为每个key对应的新的值，可能有多个，比如(hello,1)(hello,1),那么values为(1,1)\n</span><span class=\"c1\"></span>      <span class=\"c1\">//第二个参数为这个key对应的之前的状态\n</span><span class=\"c1\"></span>      <span class=\"o\">(</span><span class=\"n\">values</span><span class=\"k\">:</span> <span class=\"kt\">Seq</span><span class=\"o\">[</span><span class=\"kt\">Int</span><span class=\"o\">],</span> <span class=\"n\">state</span><span class=\"k\">:</span> <span class=\"kt\">Option</span><span class=\"o\">[</span><span class=\"kt\">Int</span><span class=\"o\">])</span> <span class=\"k\">=&gt;</span> <span class=\"o\">{</span>\n\n        <span class=\"k\">var</span> <span class=\"n\">newValue</span> <span class=\"k\">=</span> <span class=\"n\">state</span><span class=\"o\">.</span><span class=\"n\">getOrElse</span><span class=\"o\">(</span><span class=\"mi\">0</span><span class=\"o\">)</span>\n        <span class=\"n\">values</span><span class=\"o\">.</span><span class=\"n\">foreach</span><span class=\"o\">(</span><span class=\"n\">newValue</span> <span class=\"o\">+=</span> <span class=\"k\">_</span><span class=\"o\">)</span>\n        <span class=\"nc\">Option</span><span class=\"o\">(</span><span class=\"n\">newValue</span><span class=\"o\">)</span>\n\n      <span class=\"o\">})</span>\n\n    <span class=\"c1\">//共享变量，便于后面的比较是否用新的uv\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">accum</span> <span class=\"k\">=</span> <span class=\"n\">sc</span><span class=\"o\">.</span><span class=\"n\">longAccumulator</span><span class=\"o\">(</span><span class=\"s\">&#34;uv&#34;</span><span class=\"o\">)</span>\n\n    <span class=\"n\">wordCounts</span><span class=\"o\">.</span><span class=\"n\">foreachRDD</span><span class=\"o\">(</span><span class=\"n\">rdd</span> <span class=\"k\">=&gt;</span> <span class=\"o\">{</span>\n\n      <span class=\"c1\">//如果uv增加\n</span><span class=\"c1\"></span>      <span class=\"k\">if</span> <span class=\"o\">(</span><span class=\"n\">rdd</span><span class=\"o\">.</span><span class=\"n\">count</span> <span class=\"o\">&gt;</span> <span class=\"n\">accum</span><span class=\"o\">.</span><span class=\"n\">value</span><span class=\"o\">)</span> <span class=\"o\">{</span>\n        <span class=\"c1\">//打印uv\n</span><span class=\"c1\"></span>        <span class=\"n\">println</span><span class=\"o\">(</span><span class=\"n\">rdd</span><span class=\"o\">.</span><span class=\"n\">count</span><span class=\"o\">)</span>\n        <span class=\"c1\">//将共享变量的值更新为新的uv\n</span><span class=\"c1\"></span>        <span class=\"n\">accum</span><span class=\"o\">.</span><span class=\"n\">add</span><span class=\"o\">(</span><span class=\"n\">rdd</span><span class=\"o\">.</span><span class=\"n\">count</span> <span class=\"o\">-</span> <span class=\"n\">accum</span><span class=\"o\">.</span><span class=\"n\">value</span><span class=\"o\">)</span>\n      <span class=\"o\">}</span>\n    <span class=\"o\">})</span>\n\n    <span class=\"n\">ssc</span><span class=\"o\">.</span><span class=\"n\">start</span><span class=\"o\">()</span>\n    <span class=\"n\">ssc</span><span class=\"o\">.</span><span class=\"n\">awaitTermination</span><span class=\"o\">()</span>\n\n  <span class=\"o\">}</span>\n\n<span class=\"o\">}</span></code></pre></div><h2>5、生产几条数据</h2><p>随便写几条即可</p><div class=\"highlight\"><pre><code class=\"language-bash\">bin/kafka-console-producer.sh --broker-list ambari.master.com:6667 --topic KafkaUV</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-8c7021a6bb98e972e3d8fab6c031fa8e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"861\" data-rawheight=\"106\" class=\"origin_image zh-lightbox-thumb\" width=\"861\" data-original=\"https://pic3.zhimg.com/v2-8c7021a6bb98e972e3d8fab6c031fa8e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;861&#39; height=&#39;106&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"861\" data-rawheight=\"106\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"861\" data-original=\"https://pic3.zhimg.com/v2-8c7021a6bb98e972e3d8fab6c031fa8e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-8c7021a6bb98e972e3d8fab6c031fa8e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>6、结果</h2><p>根据结果可以看到，既做到了历史消息用户的累计，也做到了用户的去重 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-362d2a7c56b6552debbd36b4a62eb522_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1849\" data-rawheight=\"919\" class=\"origin_image zh-lightbox-thumb\" width=\"1849\" data-original=\"https://pic3.zhimg.com/v2-362d2a7c56b6552debbd36b4a62eb522_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1849&#39; height=&#39;919&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1849\" data-rawheight=\"919\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1849\" data-original=\"https://pic3.zhimg.com/v2-362d2a7c56b6552debbd36b4a62eb522_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-362d2a7c56b6552debbd36b4a62eb522_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "Spark Streaming", 
                    "tagLink": "https://api.zhihu.com/topics/20207375"
                }, 
                {
                    "tag": "Kafka", 
                    "tagLink": "https://api.zhihu.com/topics/20012159"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50669520", 
            "userName": "董可伦", 
            "userLink": "https://www.zhihu.com/people/de888cc734ddbfd918269a8e5fb196ba", 
            "upvote": 3, 
            "title": "通过offsets.retention.minutes设置kafka offset的过期时间", 
            "content": "<p>我的原创地址：</p><a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/06/21/modifyKafkaOffsetTime/\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-0b4e5faa279a5e545d79e754b3dba862_ipico.jpg\" data-image-width=\"640\" data-image-height=\"640\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">通过offsets.retention.minutes设置kafka offset的过期时间</a><h2>前言</h2><p>本文记录博主如何设置kafka的offset过期时间并测试其效果</p><h2>1、offsets.retention.minutes</h2><p>通过修改offsets.retention.minutes的值即可改变kafka offset的过期时间，单位为分钟，改完之后需要重启kafka。具体的配置文件为$KAFKA_HOME/config/server.properties,原生的kafka配置文件里可能没有这个配置项，自己添加上即可，比如设置过期时间为一小时，那么按如下配置即可</p><div class=\"highlight\"><pre><code class=\"language-vim\"><span class=\"nx\">offsets</span>.<span class=\"nx\">retention</span>.<span class=\"nx\">minutes</span><span class=\"p\">=</span><span class=\"m\">60</span></code></pre></div><h2>2、官方文档</h2><p>网上有的博客说官网文档对于这个配置的说明有点错误，将offsets.retention.minutes错写成了offsets.topic.retention.minutes，但是我查看了一下，官方文档上并没有写错，可能是之前的版本写错了，而且很多博客按之前的版本写的，大家注意一下。官网文档地址<a href=\"https://link.zhihu.com/?target=http%3A//kafka.apache.org/documentation/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">kafka.apache.org/docume</span><span class=\"invisible\">ntation/</span><span class=\"ellipsis\"></span></a></p><h2>3、ambari的bug</h2><p>因本人用ambari管理大数据集群的各个组件，所以在界面上直接修改kafka的配置，在界面上查看kafka的配置offsets.retention.minutes为86400000，因为kafka offset默认过期时间为一天，那么根据这个86400000来看offsets.retention.minutes的单位为毫秒才对，所以一开始误认为单位为毫秒，所以修改配置后的时间设置的很大，导致一开始测试不成功，经过一点点的验证，发现单位实际上为分钟，而ambari上显示的86400000应该是个bug，因为kafka默认的配置文件里是没有这个配置项的，所以我估计ambari一开始也没有配置只是搜索的时候将其显示为86400000，而并没有真正的生效，只有将这个配置项修改之后，才会生效，并且单位为分钟（看了一下ambari的大部分默认时间单位都是毫秒~）。 后来在官网上看到offsets.retention.minutes的default为1440也证实了这一点。</p><h2>4、测试效果</h2><p>虽然本人的需求是将默认的一天的时间改长一点，但是时间长了测试太慢，所以将时间改短一点测试效果即可，测试代码见<a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/06/20/sparkStreamingOffsetOnlyOnce/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Spark Streamming+Kafka提交offset实现有且仅有一次</a>,经过多次测试，得出结论，在修改重启之后，不管是新增加的topic还是之前的topic，只要是新保存的offset都会生效，而之前保存的offset，比如之前是一天才会删除，那么修改重启后，之前保存的offset还是会一天后才能删掉。  注：spark保存offset代码</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"n\">stream</span><span class=\"o\">.</span><span class=\"n\">foreachRDD</span> <span class=\"o\">{</span> <span class=\"n\">rdd</span> <span class=\"k\">=&gt;</span>\n  <span class=\"k\">val</span> <span class=\"n\">offsetRanges</span> <span class=\"k\">=</span> <span class=\"n\">rdd</span><span class=\"o\">.</span><span class=\"n\">asInstanceOf</span><span class=\"o\">[</span><span class=\"kt\">HasOffsetRanges</span><span class=\"o\">].</span><span class=\"n\">offsetRanges</span>\n\n  <span class=\"c1\">// some time later, after outputs have completed\n</span><span class=\"c1\"></span>  <span class=\"n\">stream</span><span class=\"o\">.</span><span class=\"n\">asInstanceOf</span><span class=\"o\">[</span><span class=\"kt\">CanCommitOffsets</span><span class=\"o\">].</span><span class=\"n\">commitAsync</span><span class=\"o\">(</span><span class=\"n\">offsetRanges</span><span class=\"o\">)</span>\n<span class=\"o\">}</span></code></pre></div><h2>5、注意</h2><p>offset的过期时间是不精确的，实际上大于等于你设置的时间，假如设置的时间为10分钟，那么可能在10-20之后才会删掉，原因我想应该是kafka会定期的检查offset被标记为应该清理的offset，可能offsets.retention.check.interval.ms这个配置项有关，因为其默认时间为十分钟，但是没有去验证这一点。 * offsets.retention.check.interval.ms 600000 offset管理器检查陈旧offsets的频率</p>", 
            "topic": [
                {
                    "tag": "Kafka", 
                    "tagLink": "https://api.zhihu.com/topics/20012159"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50585703", 
            "userName": "董可伦", 
            "userLink": "https://www.zhihu.com/people/de888cc734ddbfd918269a8e5fb196ba", 
            "upvote": 3, 
            "title": "Spark Streamming+Kafka提交offset实现有且仅有一次", 
            "content": "<p>我的原创地址：</p><a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/06/20/sparkStreamingOffsetOnlyOnce/\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-bbcb0b54d37dfe1a26ea6edae19fb442_180x120.jpg\" data-image-width=\"1872\" data-image-height=\"925\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Spark Streaming+Kafka提交offset实现有且仅有一次(exactly-once)</a><h2>前言<a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/06/20/sparkStreamingOffsetOnlyOnce/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Spark Streaming+Kafka提交offset实现有且仅有一次(exactly-once)</a>前言</h2><p>本文讲Spark Streamming使用Direct方式读取Kafka，并在输出（存储）操作之后提交offset到Kafka里实现程序读写操作有且仅有一次，即程序重启之后之前消费并且输出过的数据不再重复消费，接着上次消费的位置继续消费Kafka里的数据。 Spark Streamming+Kafka官方文档：<a href=\"https://link.zhihu.com/?target=http%3A//spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">spark.apache.org/docs/l</span><span class=\"invisible\">atest/streaming-kafka-0-10-integration.html</span><span class=\"ellipsis\"></span></a></p><h2>1、提交offset的程序</h2><p>根据官方文档可知，在spark代码里可以获取对应的offset信息，并且可以提交offset存储到kafka中。 代码：</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">package</span> <span class=\"nn\">com.dkl.leanring.spark.kafka</span>\n\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.kafka.clients.consumer.ConsumerRecord</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.kafka.common.serialization.StringDeserializer</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.streaming.kafka010._</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.streaming.StreamingContext</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.SparkConf</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.streaming.Seconds</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.TaskContext</span>\n\n<span class=\"k\">object</span> <span class=\"nc\">KafkaOffsetDemo</span> <span class=\"o\">{</span>\n  <span class=\"k\">def</span> <span class=\"n\">main</span><span class=\"o\">(</span><span class=\"n\">args</span><span class=\"k\">:</span> <span class=\"kt\">Array</span><span class=\"o\">[</span><span class=\"kt\">String</span><span class=\"o\">])</span> <span class=\"o\">{</span>\n\n    <span class=\"c1\">//创建sparkConf\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">sparkConf</span> <span class=\"k\">=</span> <span class=\"k\">new</span> <span class=\"nc\">SparkConf</span><span class=\"o\">().</span><span class=\"n\">setAppName</span><span class=\"o\">(</span><span class=\"s\">&#34;KafkaOffsetDemo&#34;</span><span class=\"o\">).</span><span class=\"n\">setMaster</span><span class=\"o\">(</span><span class=\"s\">&#34;local[2]&#34;</span><span class=\"o\">)</span>\n    <span class=\"c1\">// 创建StreamingContext batch size 为 1秒\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">ssc</span> <span class=\"k\">=</span> <span class=\"k\">new</span> <span class=\"nc\">StreamingContext</span><span class=\"o\">(</span><span class=\"n\">sparkConf</span><span class=\"o\">,</span> <span class=\"nc\">Seconds</span><span class=\"o\">(</span><span class=\"mi\">1</span><span class=\"o\">))</span>\n    <span class=\"k\">val</span> <span class=\"n\">kafkaParams</span> <span class=\"k\">=</span> <span class=\"nc\">Map</span><span class=\"o\">[</span><span class=\"kt\">String</span>, <span class=\"kt\">Object</span><span class=\"o\">](</span>\n      <span class=\"s\">&#34;bootstrap.servers&#34;</span> <span class=\"o\">-&gt;</span> <span class=\"s\">&#34;ambari.master.com:6667&#34;</span><span class=\"o\">,</span> <span class=\"c1\">//kafka集群地址\n</span><span class=\"c1\"></span>      <span class=\"s\">&#34;key.deserializer&#34;</span> <span class=\"o\">-&gt;</span> <span class=\"n\">classOf</span><span class=\"o\">[</span><span class=\"kt\">StringDeserializer</span><span class=\"o\">],</span>\n      <span class=\"s\">&#34;value.deserializer&#34;</span> <span class=\"o\">-&gt;</span> <span class=\"n\">classOf</span><span class=\"o\">[</span><span class=\"kt\">StringDeserializer</span><span class=\"o\">],</span>\n      <span class=\"s\">&#34;group.id&#34;</span> <span class=\"o\">-&gt;</span> <span class=\"s\">&#34;KafkaOffsetDemo&#34;</span><span class=\"o\">,</span> <span class=\"c1\">//消费者组名\n</span><span class=\"c1\"></span>      <span class=\"s\">&#34;auto.offset.reset&#34;</span> <span class=\"o\">-&gt;</span> <span class=\"s\">&#34;earliest&#34;</span><span class=\"o\">,</span> <span class=\"c1\">//当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，从头开始消费\n</span><span class=\"c1\"></span>      <span class=\"s\">&#34;enable.auto.commit&#34;</span> <span class=\"o\">-&gt;</span> <span class=\"o\">(</span><span class=\"kc\">false</span><span class=\"k\">:</span> <span class=\"kt\">java.lang.Boolean</span><span class=\"o\">))</span> <span class=\"c1\">//如果是true，则这个消费者的偏移量会在后台自动提交\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">topics</span> <span class=\"k\">=</span> <span class=\"nc\">Array</span><span class=\"o\">(</span><span class=\"s\">&#34;top1&#34;</span><span class=\"o\">)</span> <span class=\"c1\">//消费主题\n</span><span class=\"c1\"></span>    <span class=\"c1\">//创建DStream，返回接收到的输入数据\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">stream</span> <span class=\"k\">=</span> <span class=\"nc\">KafkaUtils</span><span class=\"o\">.</span><span class=\"n\">createDirectStream</span><span class=\"o\">[</span><span class=\"kt\">String</span>, <span class=\"kt\">String</span><span class=\"o\">](</span>\n      <span class=\"n\">ssc</span><span class=\"o\">,</span>\n      <span class=\"nc\">PreferConsistent</span><span class=\"o\">,</span>\n      <span class=\"nc\">Subscribe</span><span class=\"o\">[</span><span class=\"kt\">String</span>, <span class=\"kt\">String</span><span class=\"o\">](</span><span class=\"n\">topics</span><span class=\"o\">,</span> <span class=\"n\">kafkaParams</span><span class=\"o\">))</span>\n    <span class=\"c1\">// 打印获取到的数据，因为1秒刷新一次，所以数据长度大于0时才打印\n</span><span class=\"c1\"></span>    <span class=\"n\">stream</span><span class=\"o\">.</span><span class=\"n\">foreachRDD</span><span class=\"o\">(</span><span class=\"n\">f</span> <span class=\"k\">=&gt;</span> <span class=\"o\">{</span>\n\n      <span class=\"k\">if</span> <span class=\"o\">(</span><span class=\"n\">f</span><span class=\"o\">.</span><span class=\"n\">count</span> <span class=\"o\">&gt;</span> <span class=\"mi\">0</span><span class=\"o\">)</span> <span class=\"o\">{</span>\n        <span class=\"n\">println</span><span class=\"o\">(</span><span class=\"s\">&#34;=============================&#34;</span><span class=\"o\">)</span>\n        <span class=\"n\">println</span><span class=\"o\">(</span><span class=\"s\">&#34;打印获取到的kafka里的内容&#34;</span><span class=\"o\">)</span>\n        <span class=\"n\">f</span><span class=\"o\">.</span><span class=\"n\">foreach</span><span class=\"o\">(</span><span class=\"n\">f</span> <span class=\"k\">=&gt;</span> <span class=\"o\">{</span>\n          <span class=\"k\">val</span> <span class=\"n\">value</span> <span class=\"k\">=</span> <span class=\"n\">f</span><span class=\"o\">.</span><span class=\"n\">value</span><span class=\"o\">()</span>\n          <span class=\"n\">println</span><span class=\"o\">(</span><span class=\"n\">value</span><span class=\"o\">)</span>\n\n        <span class=\"o\">})</span>\n        <span class=\"n\">println</span><span class=\"o\">(</span><span class=\"s\">&#34;=============================&#34;</span><span class=\"o\">)</span>\n        <span class=\"n\">println</span><span class=\"o\">(</span><span class=\"s\">&#34;打印offset的信息&#34;</span><span class=\"o\">)</span>\n        <span class=\"c1\">// offset\n</span><span class=\"c1\"></span>        <span class=\"k\">val</span> <span class=\"n\">offsetRanges</span> <span class=\"k\">=</span> <span class=\"n\">f</span><span class=\"o\">.</span><span class=\"n\">asInstanceOf</span><span class=\"o\">[</span><span class=\"kt\">HasOffsetRanges</span><span class=\"o\">].</span><span class=\"n\">offsetRanges</span>\n\n        <span class=\"c1\">//打印offset\n</span><span class=\"c1\"></span>        <span class=\"n\">f</span><span class=\"o\">.</span><span class=\"n\">foreachPartition</span> <span class=\"o\">{</span> <span class=\"n\">iter</span> <span class=\"k\">=&gt;</span>\n          <span class=\"k\">val</span> <span class=\"n\">o</span><span class=\"k\">:</span> <span class=\"kt\">OffsetRange</span> <span class=\"o\">=</span> <span class=\"n\">offsetRanges</span><span class=\"o\">(</span><span class=\"nc\">TaskContext</span><span class=\"o\">.</span><span class=\"n\">get</span><span class=\"o\">.</span><span class=\"n\">partitionId</span><span class=\"o\">)</span>\n          <span class=\"n\">println</span><span class=\"o\">(</span><span class=\"s\">s&#34;</span><span class=\"si\">${</span><span class=\"n\">o</span><span class=\"o\">.</span><span class=\"n\">topic</span><span class=\"si\">}</span><span class=\"s\"> </span><span class=\"si\">${</span><span class=\"n\">o</span><span class=\"o\">.</span><span class=\"n\">partition</span><span class=\"si\">}</span><span class=\"s\"> </span><span class=\"si\">${</span><span class=\"n\">o</span><span class=\"o\">.</span><span class=\"n\">fromOffset</span><span class=\"si\">}</span><span class=\"s\"> </span><span class=\"si\">${</span><span class=\"n\">o</span><span class=\"o\">.</span><span class=\"n\">untilOffset</span><span class=\"si\">}</span><span class=\"s\">&#34;</span><span class=\"o\">)</span>\n        <span class=\"o\">}</span>\n        <span class=\"n\">println</span><span class=\"o\">(</span><span class=\"s\">&#34;=============================&#34;</span><span class=\"o\">)</span>\n        <span class=\"c1\">// 等输出操作完成后提交offset\n</span><span class=\"c1\"></span>        <span class=\"n\">stream</span><span class=\"o\">.</span><span class=\"n\">asInstanceOf</span><span class=\"o\">[</span><span class=\"kt\">CanCommitOffsets</span><span class=\"o\">].</span><span class=\"n\">commitAsync</span><span class=\"o\">(</span><span class=\"n\">offsetRanges</span><span class=\"o\">)</span>\n\n      <span class=\"o\">}</span>\n    <span class=\"o\">})</span>\n    <span class=\"c1\">//启动\n</span><span class=\"c1\"></span>    <span class=\"n\">ssc</span><span class=\"o\">.</span><span class=\"n\">start</span><span class=\"o\">()</span>\n    <span class=\"c1\">//等待停止\n</span><span class=\"c1\"></span>    <span class=\"n\">ssc</span><span class=\"o\">.</span><span class=\"n\">awaitTermination</span><span class=\"o\">()</span>\n  <span class=\"o\">}</span>\n<span class=\"o\">}</span></code></pre></div><p>说明： <i> auto.offset.reset设置为earliest，即当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，从头开始，这样设置的目的是为了一开始可以获取到kafka对应主题下的所有的历史消息。 </i> enable.auto.commit 设置为false，如果是true，则这个消费者的偏移量会在后台自动提交，这样设置目的是为了后面自己提交offset，因为如果虽然获取到了消息，但是后面的转化操作并将结果写到如hive中并没有完成程序就挂了的话，这样是不能将这次的offset提交的，这样就可以等程序重启之后接着上次失败的地方继续消费 <i> group.id 是不能变得，也就是offset是和topic和group绑定的，如果换一个group的话，程序将从头消费所有的历史数据 </i> 这个api是将offset存储到kakfa的一个指定的topic里，名字为__consumer_offsets,而不是zookeeper中</p><h2>2、测试程序</h2><p>1、首先创建对应的topic 2、生产几条数据作为历史消息</p><div class=\"highlight\"><pre><code class=\"language-bash\">bin/kafka-console-producer.sh --broker-list ambari.master.com:6667 --topic top1</code></pre></div><p>3、启动上面的程序 4、继续生产几条数据 接下来先看一下结果：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-58bb08af46bbd23cfe8809c19bab07fb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"769\" data-rawheight=\"91\" class=\"origin_image zh-lightbox-thumb\" width=\"769\" data-original=\"https://pic4.zhimg.com/v2-58bb08af46bbd23cfe8809c19bab07fb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;769&#39; height=&#39;91&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"769\" data-rawheight=\"91\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"769\" data-original=\"https://pic4.zhimg.com/v2-58bb08af46bbd23cfe8809c19bab07fb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-58bb08af46bbd23cfe8809c19bab07fb_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-9aa1ef540f536dcf2b931ef41af28937_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1836\" data-rawheight=\"896\" class=\"origin_image zh-lightbox-thumb\" width=\"1836\" data-original=\"https://pic4.zhimg.com/v2-9aa1ef540f536dcf2b931ef41af28937_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1836&#39; height=&#39;896&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1836\" data-rawheight=\"896\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1836\" data-original=\"https://pic4.zhimg.com/v2-9aa1ef540f536dcf2b931ef41af28937_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-9aa1ef540f536dcf2b931ef41af28937_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>由图可得，这样可以将历史数据全部打印出来，并且后面实时增加的数据，也打印出来了，且可以看到offset是在增加的，最后一个offset是202，那么接下来测试一下程序重启之后是否会接着之前的数据继续消费呢 5、停止程序 6、生产几条数据 7、启动程序 看一下结果: </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-58be313f8b5d8d71591b00bc766852be_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"766\" data-rawheight=\"171\" class=\"origin_image zh-lightbox-thumb\" width=\"766\" data-original=\"https://pic3.zhimg.com/v2-58be313f8b5d8d71591b00bc766852be_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;766&#39; height=&#39;171&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"766\" data-rawheight=\"171\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"766\" data-original=\"https://pic3.zhimg.com/v2-58be313f8b5d8d71591b00bc766852be_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-58be313f8b5d8d71591b00bc766852be_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-b9a920e6cefaa9542cff6101638be1bb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1872\" data-rawheight=\"925\" class=\"origin_image zh-lightbox-thumb\" width=\"1872\" data-original=\"https://pic4.zhimg.com/v2-b9a920e6cefaa9542cff6101638be1bb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1872&#39; height=&#39;925&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1872\" data-rawheight=\"925\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1872\" data-original=\"https://pic4.zhimg.com/v2-b9a920e6cefaa9542cff6101638be1bb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-b9a920e6cefaa9542cff6101638be1bb_b.jpg\"/></figure><p> 可以看出，程序确实是接着上次消费的地方消费的，为了证实这一点，我将earliest和offset圈了起来，从offset可以看到是从上次的202开始消费的。</p><h2>3、关于offset过期时间</h2><p>kafka offset默认的过期时间是一天，当上面的程序挂掉，一天之内没有重启，也就是一天之内没有保存新的offset的话，那么之前的offset就会被删除，再重启程序，就会从头开始消费kafka里的所有历史数据，这种情况是有问题的，所以可以通过设置offsets.retention.minutes自定义offset过期时间，该设置单位为分钟，默认为1440。 修改kafka的offset过期时间详细信息见：<a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/06/21/modifyKafkaOffsetTime/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">dongkelun.com/2018/06/2</span><span class=\"invisible\">1/modifyKafkaOffsetTime/</span><span class=\"ellipsis\"></span></a></p><h2>4、自己保存offset</h2><p>可以通过自己保存offset的信息到数据库里，然后需要时再取出来，根据得到的offset信息消费kafka里的数据，这样就不用担心offset的过期的问题了，因为没有自己写代码实现，所以先给出官网的示例代码：</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"c1\">// The details depend on your data store, but the general idea looks like this\n</span><span class=\"c1\"></span>\n<span class=\"c1\">// begin from the the offsets committed to the database\n</span><span class=\"c1\"></span><span class=\"k\">val</span> <span class=\"n\">fromOffsets</span> <span class=\"k\">=</span> <span class=\"n\">selectOffsetsFromYourDatabase</span><span class=\"o\">.</span><span class=\"n\">map</span> <span class=\"o\">{</span> <span class=\"n\">resultSet</span> <span class=\"k\">=&gt;</span>\n  <span class=\"k\">new</span> <span class=\"nc\">TopicPartition</span><span class=\"o\">(</span><span class=\"n\">resultSet</span><span class=\"o\">.</span><span class=\"n\">string</span><span class=\"o\">(</span><span class=\"s\">&#34;topic&#34;</span><span class=\"o\">),</span> <span class=\"n\">resultSet</span><span class=\"o\">.</span><span class=\"n\">int</span><span class=\"o\">(</span><span class=\"s\">&#34;partition&#34;</span><span class=\"o\">))</span> <span class=\"o\">-&gt;</span> <span class=\"n\">resultSet</span><span class=\"o\">.</span><span class=\"n\">long</span><span class=\"o\">(</span><span class=\"s\">&#34;offset&#34;</span><span class=\"o\">)</span>\n<span class=\"o\">}.</span><span class=\"n\">toMap</span>\n\n<span class=\"k\">val</span> <span class=\"n\">stream</span> <span class=\"k\">=</span> <span class=\"nc\">KafkaUtils</span><span class=\"o\">.</span><span class=\"n\">createDirectStream</span><span class=\"o\">[</span><span class=\"kt\">String</span>, <span class=\"kt\">String</span><span class=\"o\">](</span>\n  <span class=\"n\">streamingContext</span><span class=\"o\">,</span>\n  <span class=\"nc\">PreferConsistent</span><span class=\"o\">,</span>\n  <span class=\"nc\">Assign</span><span class=\"o\">[</span><span class=\"kt\">String</span>, <span class=\"kt\">String</span><span class=\"o\">](</span><span class=\"n\">fromOffsets</span><span class=\"o\">.</span><span class=\"n\">keys</span><span class=\"o\">.</span><span class=\"n\">toList</span><span class=\"o\">,</span> <span class=\"n\">kafkaParams</span><span class=\"o\">,</span> <span class=\"n\">fromOffsets</span><span class=\"o\">)</span>\n<span class=\"o\">)</span>\n\n<span class=\"n\">stream</span><span class=\"o\">.</span><span class=\"n\">foreachRDD</span> <span class=\"o\">{</span> <span class=\"n\">rdd</span> <span class=\"k\">=&gt;</span>\n  <span class=\"k\">val</span> <span class=\"n\">offsetRanges</span> <span class=\"k\">=</span> <span class=\"n\">rdd</span><span class=\"o\">.</span><span class=\"n\">asInstanceOf</span><span class=\"o\">[</span><span class=\"kt\">HasOffsetRanges</span><span class=\"o\">].</span><span class=\"n\">offsetRanges</span>\n\n  <span class=\"k\">val</span> <span class=\"n\">results</span> <span class=\"k\">=</span> <span class=\"n\">yourCalculation</span><span class=\"o\">(</span><span class=\"n\">rdd</span><span class=\"o\">)</span>\n\n  <span class=\"c1\">// begin your transaction\n</span><span class=\"c1\"></span>\n  <span class=\"c1\">// update results\n</span><span class=\"c1\"></span>  <span class=\"c1\">// update offsets where the end of existing offsets matches the beginning of this batch of offsets\n</span><span class=\"c1\"></span>  <span class=\"c1\">// assert that offsets were updated correctly\n</span><span class=\"c1\"></span>\n  <span class=\"c1\">// end your transaction\n</span><span class=\"c1\"></span><span class=\"o\">}</span></code></pre></div><p></p>", 
            "topic": [
                {
                    "tag": "Spark Streaming", 
                    "tagLink": "https://api.zhihu.com/topics/20207375"
                }, 
                {
                    "tag": "Kafka", 
                    "tagLink": "https://api.zhihu.com/topics/20012159"
                }
            ], 
            "comments": [
                {
                    "userName": "房东的狗", 
                    "userLink": "https://www.zhihu.com/people/26a88e420afd451c2aaab55ac5747b59", 
                    "content": "<p>博主你好，请问在消息处理完之后，提交偏移量到Kafka之前，如果程序出异常了。那么这部分已经消费的消息不就重复消费了么？ 怎么保证消费和提交偏移量是原子操作呢？</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50496380", 
            "userName": "董可伦", 
            "userLink": "https://www.zhihu.com/people/de888cc734ddbfd918269a8e5fb196ba", 
            "upvote": 1, 
            "title": "spark-submit提交Spark Streamming+Kafka程序", 
            "content": "<p>我的原创地址：</p><a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/06/19/sparkSubmitKafka/\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-0b4e5faa279a5e545d79e754b3dba862_ipico.jpg\" data-image-width=\"640\" data-image-height=\"640\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">spark-submit提交Spark Streaming+Kafka程序</a><h2>前言<a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/06/19/sparkSubmitKafka/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">spark-submit提交Spark Streaming+Kafka程序</a>前言</h2><p>Spark Streaming本身是没有Kafka相关的jar包和API的，如果想利用Spark Streaming获取Kafka里的数据，需要自己将依赖添加SBT或Maven项目中，添加依赖更新项目之后，就可以在Eclipse等IDE里直接运行Spark Streamming+Kafka的程序了，可参考<a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/05/17/sparkKafka/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Spark Streaming连接Kafka入门教程</a>，但是如果需要在集群通过spark-submit提交jar包的方式来运行程序的话，会抛出异常：</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"nc\">Exception</span> <span class=\"n\">in</span> <span class=\"n\">thread</span> <span class=\"s\">&#34;main&#34;</span> <span class=\"n\">java</span><span class=\"o\">.</span><span class=\"n\">lang</span><span class=\"o\">.</span><span class=\"nc\">NoClassDefFoundError</span><span class=\"k\">:</span> <span class=\"kt\">org/apache/kafka/common/serialization/StringDeserializer</span></code></pre></div><p>这是因为Spark本身没有Kafka相关的jar，所以需要将与Kafka相关的jar添加Spark环境中</p><h2>1、相关jar包</h2><p>总共需要两个jar，可以在SBT和Maven下载的目录里找到，分别为org.apache.spark/spark-streaming-kafka-0-10_2.11/jars/spark-streaming-kafka-0-10_2.11-2.3.0.jar和org.apache.kafka/kafka-clients/jars/kafka-clients-0.10.0.1.jar,具体的名字会因你的kafka版本和spark版本而有所不同 </p><h2>2、方法一(仅在执行spark-submit的节点上即可)</h2><p>第一个方法也是我觉得最简单的方法，将这两个jar拷贝到$SPARK_HOME/jars目下即可 如果用ambari安装的spark2,则对应的目录为</p><div class=\"highlight\"><pre><code class=\"language-bash\">/usr/hdp/current/spark2-client/jars/</code></pre></div><p>拷贝命令</p><div class=\"highlight\"><pre><code class=\"language-text\">cp spark-streaming-kafka-0-10_2.11-2.3.0.jar kafka-clients-0.10.0.1.jar /usr/hdp/current/spark2-client/jars/</code></pre></div><p>然后通过如下命令提交jar即可(只是最简单的配置，实际还需要配置Executor的数量，内存大小等等)</p><div class=\"highlight\"><pre><code class=\"language-text\">spark-submit --master yarn --class com.dkl.leanring.spark.kafka.KafaDemo spark-scala_2.11-1.0.jar</code></pre></div><h2>3、方法二</h2><p>通过--jars指定需要的jar，多个jar需要以逗号分隔</p><div class=\"highlight\"><pre><code class=\"language-text\">spark-submit --master yarn --class com.dkl.leanring.spark.kafka.KafaDemo --jars jars/kafka-clients-0.10.0.1.jar,jars/spark-streaming-kafka-0-10_2.11-2.3.0.jar spark-scala_2.11-1.0.jar</code></pre></div><p>这种方式如果依赖的jar很多的话就不方便了</p><p>关于spark-submit添加依赖jar更详细说明可参考<a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/05/06/sparkSubmitException/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">dongkelun.com/2018/05/0</span><span class=\"invisible\">6/sparkSubmitException/</span><span class=\"ellipsis\"></span></a></p><h2>参考</h2><p><a href=\"https://link.zhihu.com/?target=https%3A//www.cnblogs.com/zhangXingSheng/p/6646879.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">cnblogs.com/zhangXingSh</span><span class=\"invisible\">eng/p/6646879.html</span><span class=\"ellipsis\"></span></a></p>", 
            "topic": [
                {
                    "tag": "Spark", 
                    "tagLink": "https://api.zhihu.com/topics/19942170"
                }, 
                {
                    "tag": "Spark Streaming", 
                    "tagLink": "https://api.zhihu.com/topics/20207375"
                }, 
                {
                    "tag": "Kafka", 
                    "tagLink": "https://api.zhihu.com/topics/20012159"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50455684", 
            "userName": "董可伦", 
            "userLink": "https://www.zhihu.com/people/de888cc734ddbfd918269a8e5fb196ba", 
            "upvote": 2, 
            "title": "SparkStreaming+Kafka 实现基于缓存的实时wordcount程序", 
            "content": "<p>我的原创地址：</p><a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/06/14/updateStateBykeyWordCount/\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic4.zhimg.com/v2-2fe120920b0d4e5c256e414011d2a0f3_180x120.jpg\" data-image-width=\"1831\" data-image-height=\"987\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">SparkStreaming+Kafka 实现基于缓存的实时wordcount程序</a><h2>前言</h2><p>本文利用SparkStreaming和Kafka实现基于缓存的实时wordcount程序，什么意思呢，因为一般的SparkStreaming的wordcount程序比如官网上的，只能统计最新时间间隔内的每个单词的数量，而不能将历史的累加起来，本文是看了教程之后，自己实现了一下kafka的程序，记录在这里。其实没什么难度，只是用了一个updateStateByKey算子就能实现，因为第一次用这个算子，所以正好学习一下。</p><h2>1、数据</h2><p>数据是我随机在kafka里生产的几条，单词以空格区分开</p><h2>2、kafka topic</h2><p>首先在kafka建一个程序用到topic:UpdateStateBykeyWordCount</p><div class=\"highlight\"><pre><code class=\"language-bash\">bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor <span class=\"m\">1</span> --partitions <span class=\"m\">1</span> --topic UpdateStateBykeyWordCount</code></pre></div><h2>3、创建checkpoint的hdfs目录</h2><p>我的目录为：/spark/dkl/kafka/wordcount_checkpoint</p><div class=\"highlight\"><pre><code class=\"language-bash\">hadoop fs -mkdir -p /spark/dkl/kafka/wordcount_checkpoint</code></pre></div><h2>4、Spark代码</h2><p>启动下面的程序</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">package</span> <span class=\"nn\">com.dkl.leanring.spark.kafka</span>\n\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.streaming.StreamingContext</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.SparkSession</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.streaming.Seconds</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.kafka.common.serialization.StringDeserializer</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.streaming.kafka010.KafkaUtils</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe</span>\n<span class=\"k\">object</span> <span class=\"nc\">UpdateStateBykeyWordCount</span> <span class=\"o\">{</span>\n\n  <span class=\"k\">def</span> <span class=\"n\">main</span><span class=\"o\">(</span><span class=\"n\">args</span><span class=\"k\">:</span> <span class=\"kt\">Array</span><span class=\"o\">[</span><span class=\"kt\">String</span><span class=\"o\">])</span><span class=\"k\">:</span> <span class=\"kt\">Unit</span> <span class=\"o\">=</span> <span class=\"o\">{</span>\n    <span class=\"c1\">//初始化，创建SparkSession\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">spark</span> <span class=\"k\">=</span> <span class=\"nc\">SparkSession</span><span class=\"o\">.</span><span class=\"n\">builder</span><span class=\"o\">().</span><span class=\"n\">appName</span><span class=\"o\">(</span><span class=\"s\">&#34;sskt&#34;</span><span class=\"o\">).</span><span class=\"n\">master</span><span class=\"o\">(</span><span class=\"s\">&#34;local[2]&#34;</span><span class=\"o\">).</span><span class=\"n\">enableHiveSupport</span><span class=\"o\">().</span><span class=\"n\">getOrCreate</span><span class=\"o\">()</span>\n    <span class=\"c1\">//初始化，创建sparkContext\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">sc</span> <span class=\"k\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">sparkContext</span>\n    <span class=\"c1\">//初始化，创建StreamingContext，batchDuration为1秒\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">ssc</span> <span class=\"k\">=</span> <span class=\"k\">new</span> <span class=\"nc\">StreamingContext</span><span class=\"o\">(</span><span class=\"n\">sc</span><span class=\"o\">,</span> <span class=\"nc\">Seconds</span><span class=\"o\">(</span><span class=\"mi\">5</span><span class=\"o\">))</span>\n\n    <span class=\"c1\">//开启checkpoint机制\n</span><span class=\"c1\"></span>    <span class=\"n\">ssc</span><span class=\"o\">.</span><span class=\"n\">checkpoint</span><span class=\"o\">(</span><span class=\"s\">&#34;hdfs://ambari.master.com:8020/spark/dkl/kafka/wordcount_checkpoint&#34;</span><span class=\"o\">)</span>\n\n    <span class=\"c1\">//kafka集群地址\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">server</span> <span class=\"k\">=</span> <span class=\"s\">&#34;ambari.master.com:6667&#34;</span>\n\n    <span class=\"c1\">//配置消费者\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">kafkaParams</span> <span class=\"k\">=</span> <span class=\"nc\">Map</span><span class=\"o\">[</span><span class=\"kt\">String</span>, <span class=\"kt\">Object</span><span class=\"o\">](</span>\n      <span class=\"s\">&#34;bootstrap.servers&#34;</span> <span class=\"o\">-&gt;</span> <span class=\"n\">server</span><span class=\"o\">,</span> <span class=\"c1\">//kafka集群地址\n</span><span class=\"c1\"></span>      <span class=\"s\">&#34;key.deserializer&#34;</span> <span class=\"o\">-&gt;</span> <span class=\"n\">classOf</span><span class=\"o\">[</span><span class=\"kt\">StringDeserializer</span><span class=\"o\">],</span>\n      <span class=\"s\">&#34;value.deserializer&#34;</span> <span class=\"o\">-&gt;</span> <span class=\"n\">classOf</span><span class=\"o\">[</span><span class=\"kt\">StringDeserializer</span><span class=\"o\">],</span>\n      <span class=\"s\">&#34;group.id&#34;</span> <span class=\"o\">-&gt;</span> <span class=\"s\">&#34;UpdateStateBykeyWordCount&#34;</span><span class=\"o\">,</span> <span class=\"c1\">//消费者组名\n</span><span class=\"c1\"></span>      <span class=\"s\">&#34;auto.offset.reset&#34;</span> <span class=\"o\">-&gt;</span> <span class=\"s\">&#34;latest&#34;</span><span class=\"o\">,</span> <span class=\"c1\">//latest自动重置偏移量为最新的偏移量   earliest 、none\n</span><span class=\"c1\"></span>      <span class=\"s\">&#34;enable.auto.commit&#34;</span> <span class=\"o\">-&gt;</span> <span class=\"o\">(</span><span class=\"kc\">false</span><span class=\"k\">:</span> <span class=\"kt\">java.lang.Boolean</span><span class=\"o\">))</span> <span class=\"c1\">//如果是true，则这个消费者的偏移量会在后台自动提交\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">topics</span> <span class=\"k\">=</span> <span class=\"nc\">Array</span><span class=\"o\">(</span><span class=\"s\">&#34;UpdateStateBykeyWordCount&#34;</span><span class=\"o\">)</span> <span class=\"c1\">//消费主题\n</span><span class=\"c1\"></span>\n    <span class=\"c1\">//基于Direct方式创建DStream\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">stream</span> <span class=\"k\">=</span> <span class=\"nc\">KafkaUtils</span><span class=\"o\">.</span><span class=\"n\">createDirectStream</span><span class=\"o\">(</span><span class=\"n\">ssc</span><span class=\"o\">,</span> <span class=\"nc\">PreferConsistent</span><span class=\"o\">,</span> <span class=\"nc\">Subscribe</span><span class=\"o\">[</span><span class=\"kt\">String</span>, <span class=\"kt\">String</span><span class=\"o\">](</span><span class=\"n\">topics</span><span class=\"o\">,</span> <span class=\"n\">kafkaParams</span><span class=\"o\">))</span>\n\n    <span class=\"c1\">//开始执行WordCount程序\n</span><span class=\"c1\"></span>\n    <span class=\"c1\">//以空格为切分符切分单词，并转化为 (word,1)形式\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">words</span> <span class=\"k\">=</span> <span class=\"n\">stream</span><span class=\"o\">.</span><span class=\"n\">flatMap</span><span class=\"o\">(</span><span class=\"k\">_</span><span class=\"o\">.</span><span class=\"n\">value</span><span class=\"o\">().</span><span class=\"n\">split</span><span class=\"o\">(</span><span class=\"s\">&#34; &#34;</span><span class=\"o\">)).</span><span class=\"n\">map</span><span class=\"o\">((</span><span class=\"k\">_</span><span class=\"o\">,</span> <span class=\"mi\">1</span><span class=\"o\">))</span>\n    <span class=\"k\">val</span> <span class=\"n\">wordCounts</span> <span class=\"k\">=</span> <span class=\"n\">words</span><span class=\"o\">.</span><span class=\"n\">updateStateByKey</span><span class=\"o\">(</span>\n      <span class=\"c1\">//每个单词每次batch计算的时候都会调用这个函数\n</span><span class=\"c1\"></span>      <span class=\"c1\">//第一个参数为每个key对应的新的值，可能有多个，比如(hello,1)(hello,1),那么values为(1,1)\n</span><span class=\"c1\"></span>      <span class=\"c1\">//第二个参数为这个key对应的之前的状态\n</span><span class=\"c1\"></span>      <span class=\"o\">(</span><span class=\"n\">values</span><span class=\"k\">:</span> <span class=\"kt\">Seq</span><span class=\"o\">[</span><span class=\"kt\">Int</span><span class=\"o\">],</span> <span class=\"n\">state</span><span class=\"k\">:</span> <span class=\"kt\">Option</span><span class=\"o\">[</span><span class=\"kt\">Int</span><span class=\"o\">])</span> <span class=\"k\">=&gt;</span> <span class=\"o\">{</span>\n\n        <span class=\"k\">var</span> <span class=\"n\">newValue</span> <span class=\"k\">=</span> <span class=\"n\">state</span><span class=\"o\">.</span><span class=\"n\">getOrElse</span><span class=\"o\">(</span><span class=\"mi\">0</span><span class=\"o\">)</span>\n        <span class=\"n\">values</span><span class=\"o\">.</span><span class=\"n\">foreach</span><span class=\"o\">(</span><span class=\"n\">newValue</span> <span class=\"o\">+=</span> <span class=\"k\">_</span><span class=\"o\">)</span>\n        <span class=\"nc\">Option</span><span class=\"o\">(</span><span class=\"n\">newValue</span><span class=\"o\">)</span>\n\n      <span class=\"o\">})</span>\n    <span class=\"n\">wordCounts</span><span class=\"o\">.</span><span class=\"n\">print</span><span class=\"o\">()</span>\n\n    <span class=\"n\">ssc</span><span class=\"o\">.</span><span class=\"n\">start</span><span class=\"o\">()</span>\n    <span class=\"n\">ssc</span><span class=\"o\">.</span><span class=\"n\">awaitTermination</span><span class=\"o\">()</span>\n\n  <span class=\"o\">}</span>\n\n<span class=\"o\">}</span></code></pre></div><h2>5、生产几条数据</h2><p>随便写几条即可</p><div class=\"highlight\"><pre><code class=\"language-bash\">bin/kafka-console-producer.sh --broker-list ambari.master.com:6667 --topic UpdateStateBykeyWordCount</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-45fbc0a1f8ca6144a82e2c035fb9b42d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"923\" data-rawheight=\"64\" class=\"origin_image zh-lightbox-thumb\" width=\"923\" data-original=\"https://pic2.zhimg.com/v2-45fbc0a1f8ca6144a82e2c035fb9b42d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;923&#39; height=&#39;64&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"923\" data-rawheight=\"64\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"923\" data-original=\"https://pic2.zhimg.com/v2-45fbc0a1f8ca6144a82e2c035fb9b42d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-45fbc0a1f8ca6144a82e2c035fb9b42d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>6、结果</h2><p>根据结果可以看到，历史的单词也被统计打印出来了 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-6df3dee9c0ebf7fd5044a8e80c2e2da7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1831\" data-rawheight=\"987\" class=\"origin_image zh-lightbox-thumb\" width=\"1831\" data-original=\"https://pic4.zhimg.com/v2-6df3dee9c0ebf7fd5044a8e80c2e2da7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1831&#39; height=&#39;987&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1831\" data-rawheight=\"987\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1831\" data-original=\"https://pic4.zhimg.com/v2-6df3dee9c0ebf7fd5044a8e80c2e2da7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-6df3dee9c0ebf7fd5044a8e80c2e2da7_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "Kafka", 
                    "tagLink": "https://api.zhihu.com/topics/20012159"
                }, 
                {
                    "tag": "缓存", 
                    "tagLink": "https://api.zhihu.com/topics/19564998"
                }, 
                {
                    "tag": "Spark", 
                    "tagLink": "https://api.zhihu.com/topics/19942170"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50416326", 
            "userName": "董可伦", 
            "userLink": "https://www.zhihu.com/people/de888cc734ddbfd918269a8e5fb196ba", 
            "upvote": 10, 
            "title": "Spark架构原理", 
            "content": "<p>我的原创地址：</p><a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/06/09/sparkArchitecturePrinciples/\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic4.zhimg.com/v2-864de4438aae228899a2f0de1c72f41b_180x120.jpg\" data-image-width=\"1335\" data-image-height=\"744\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Spark架构原理</a><h2>前言</h2><p>本文总结了Spark架构原理,其中主要包括五个组件：Driver、Master、Worker、Executor和Task，简要概括了每个组件是干啥的，并总结提交spark程序之后，这五个组件运行的详细步骤。</p><h2>1、流程图</h2><p>为了直观，就把流程图放在最前面了 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-d146927f55be1b7e485f31f2e809a47f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1335\" data-rawheight=\"744\" class=\"origin_image zh-lightbox-thumb\" width=\"1335\" data-original=\"https://pic4.zhimg.com/v2-d146927f55be1b7e485f31f2e809a47f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1335&#39; height=&#39;744&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1335\" data-rawheight=\"744\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1335\" data-original=\"https://pic4.zhimg.com/v2-d146927f55be1b7e485f31f2e809a47f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-d146927f55be1b7e485f31f2e809a47f_b.jpg\"/></figure><h2>2、Driver</h2><p>driver是一个进程，我们编写的spark程序运行在driver上，由dirver进程执行，driver是作业的主进程，具有main函数，是程序的入口点，driver进程启动后，向master发送请求，进行注册，申请资源，在后面的executor启动后，会向dirver进行反注册，dirver注册了executor后，正式执行spark程序，读取数据源，创建rdd或dataframe，生成stage,提交task到executor</p><h2>3、Master</h2><p>Master是个进程，主要负责资源的调度和分配，集群的监控等。</p><h2>4、Worker</h2><p>worke是个进程，主要负责两个，一个是用自己的内存存储RDD的某个或某些partition，另一个是启动其他进程和线程，对RDD上的partition进行处理和计算。</p><h2>5、Executor</h2><p>Executor是个进程，一个Executor执行多个Task，多个Executor可以并行执行，可以通过--num-executors来指定Executor的数量，但是经过我的测试，Executor最大为集群可用的cpu核数减1，另一个core可能是用来作为master，另外如果master为yarn，则实际可用cpu核数为yarn的虚拟核数，可以通过yarn.nodemanager.resource.cpu-vcores设定，虚拟核数可以大于物理核数。</p><h2>6、Task</h2><p>Task是个线程，具体的spark任务是在Task上运行的，某些并行的算子，有多少个分区就有多少个task，但是有些算子像take这样的只有一个task。</p><h2>7、详细的流程</h2><p>1、Driver进程启动之后，会进行一些初始化的操作，在这个过程中，会发送请求到master 2、Master，接收到Driver的注册之后，发送请求给Worker,进行资源的调度和分配，也就是Executor的分配 3、Worker接收到master的请求，启动Executor 4、Executor启动之后，会向Driver进行反注册 5、Driver注册了Executor之后，正式开始执行Spark程序，首先读取数据源，创建RDD 6、HDFS文件被读取到多个Worker节点，形成RDD 7、在worker上生成RDD之后，Driver会根据我们对RDD定义的操作，提交相应数量的Task到Executor上</p>", 
            "topic": [
                {
                    "tag": "Spark", 
                    "tagLink": "https://api.zhihu.com/topics/19942170"
                }, 
                {
                    "tag": "原理", 
                    "tagLink": "https://api.zhihu.com/topics/19808471"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50089958", 
            "userName": "董可伦", 
            "userLink": "https://www.zhihu.com/people/de888cc734ddbfd918269a8e5fb196ba", 
            "upvote": 2, 
            "title": "Spark 持久化（cache和persist的区别）", 
            "content": "<p>我的原创地址：</p><a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/06/03/sparkCacheAndPersist/\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-0b4e5faa279a5e545d79e754b3dba862_ipico.jpg\" data-image-width=\"640\" data-image-height=\"640\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Spark 持久化（cache和persist的区别）</a><h2>1、RDD 持久化</h2><p>Spark 中一个很重要的能力是将数据持久化（或称为缓存），在多个操作间都可以访问这些持久化的数据。当持久化一个 RDD 时，每个节点的其它分区都可以使用 RDD 在内存中进行计算，在该数据上的其他 action 操作将直接使用内存中的数据。这样会让以后的 action 操作计算速度加快（通常运行速度会加速 10 倍）。缓存是迭代算法和快速的交互式使用的重要工具。</p><p>RDD 可以使用 persist() 方法或 cache() 方法进行持久化。数据将会在第一次 action 操作时进行计算，并缓存在节点的内存中。Spark 的缓存具有容错机制，如果一个缓存的 RDD 的某个分区丢失了，Spark 将按照原来的计算过程，自动重新计算并进行缓存。</p><p>在 shuffle 操作中（例如 reduceByKey），即便是用户没有调用 persist 方法，Spark 也会自动缓存部分中间数据。这么做的目的是，在 shuffle 的过程中某个节点运行失败时，不需要重新计算所有的输入数据。如果用户想多次使用某个 RDD，强烈推荐在该 RDD 上调用 persist 方法。</p><h2>2、存储级别</h2><p>每个持久化的 RDD 可以使用不同的存储级别进行缓存，例如，持久化到磁盘、已序列化的 Java 对象形式持久化到内存（可以节省空间）、跨节点间复制、以 off-heap 的方式存储在 Tachyon。这些存储级别通过传递一个 StorageLevel 对象给 persist() 方法进行设置。 详细的存储级别介绍如下：</p><ul><li>MEMORY_ONLY : 将 RDD 以反序列化 Java 对象的形式存储在 JVM 中。如果内存空间不够，部分数据分区将不再缓存，在每次需要用到这些数据时重新进行计算。这是默认的级别。</li><li>MEMORY_AND_DISK : 将 RDD 以反序列化 Java 对象的形式存储在 JVM 中。如果内存空间不够，将未缓存的数据分区存储到磁盘，在需要使用这些分区时从磁盘读取。</li><li>MEMORY_ONLY_SER : 将 RDD 以序列化的 Java 对象的形式进行存储（每个分区为一个 byte 数组）。这种方式会比反序列化对象的方式节省很多空间，尤其是在使用 fast serializer时会节省更多的空间，但是在读取时会增加 CPU 的计算负担。</li><li>MEMORY_AND_DISK_SER : 类似于 MEMORY_ONLY_SER ，但是溢出的分区会存储到磁盘，而不是在用到它们时重新计算。</li><li>DISK_ONLY : 只在磁盘上缓存 RDD。</li><li>MEMORY_ONLY_2，MEMORY_AND_DISK_2，等等 : 与上面的级别功能相同，只不过每个分区在集群中两个节点上建立副本。</li><li>OFF_HEAP（实验中）: 类似于 MEMORY_ONLY_SER ，但是将数据存储在 off-heap memory，这需要启动 off-heap 内存。</li></ul><p>注意，在 Python 中，缓存的对象总是使用 Pickle 进行序列化，所以在 Python 中不关心你选择的是哪一种序列化级别。python 中的存储级别包括 MEMORY_ONLY，MEMORY_ONLY_2，MEMORY_AND_DISK，MEMORY_AND_DISK_2，DISK_ONLY 和 DISK_ONLY_2 。  上面的几个缓存级别是官网给出的，但是通过源码看，实际上一共有12种缓存级别</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">object</span> <span class=\"nc\">StorageLevel</span> <span class=\"o\">{</span>\n  <span class=\"k\">val</span> <span class=\"nc\">NONE</span> <span class=\"k\">=</span> <span class=\"k\">new</span> <span class=\"nc\">StorageLevel</span><span class=\"o\">(</span><span class=\"kc\">false</span><span class=\"o\">,</span> <span class=\"kc\">false</span><span class=\"o\">,</span> <span class=\"kc\">false</span><span class=\"o\">,</span> <span class=\"kc\">false</span><span class=\"o\">)</span>\n  <span class=\"k\">val</span> <span class=\"nc\">DISK_ONLY</span> <span class=\"k\">=</span> <span class=\"k\">new</span> <span class=\"nc\">StorageLevel</span><span class=\"o\">(</span><span class=\"kc\">true</span><span class=\"o\">,</span> <span class=\"kc\">false</span><span class=\"o\">,</span> <span class=\"kc\">false</span><span class=\"o\">,</span> <span class=\"kc\">false</span><span class=\"o\">)</span>\n  <span class=\"k\">val</span> <span class=\"nc\">DISK_ONLY_2</span> <span class=\"k\">=</span> <span class=\"k\">new</span> <span class=\"nc\">StorageLevel</span><span class=\"o\">(</span><span class=\"kc\">true</span><span class=\"o\">,</span> <span class=\"kc\">false</span><span class=\"o\">,</span> <span class=\"kc\">false</span><span class=\"o\">,</span> <span class=\"kc\">false</span><span class=\"o\">,</span> <span class=\"mi\">2</span><span class=\"o\">)</span>\n  <span class=\"k\">val</span> <span class=\"nc\">MEMORY_ONLY</span> <span class=\"k\">=</span> <span class=\"k\">new</span> <span class=\"nc\">StorageLevel</span><span class=\"o\">(</span><span class=\"kc\">false</span><span class=\"o\">,</span> <span class=\"kc\">true</span><span class=\"o\">,</span> <span class=\"kc\">false</span><span class=\"o\">,</span> <span class=\"kc\">true</span><span class=\"o\">)</span>\n  <span class=\"k\">val</span> <span class=\"nc\">MEMORY_ONLY_2</span> <span class=\"k\">=</span> <span class=\"k\">new</span> <span class=\"nc\">StorageLevel</span><span class=\"o\">(</span><span class=\"kc\">false</span><span class=\"o\">,</span> <span class=\"kc\">true</span><span class=\"o\">,</span> <span class=\"kc\">false</span><span class=\"o\">,</span> <span class=\"kc\">true</span><span class=\"o\">,</span> <span class=\"mi\">2</span><span class=\"o\">)</span>\n  <span class=\"k\">val</span> <span class=\"nc\">MEMORY_ONLY_SER</span> <span class=\"k\">=</span> <span class=\"k\">new</span> <span class=\"nc\">StorageLevel</span><span class=\"o\">(</span><span class=\"kc\">false</span><span class=\"o\">,</span> <span class=\"kc\">true</span><span class=\"o\">,</span> <span class=\"kc\">false</span><span class=\"o\">,</span> <span class=\"kc\">false</span><span class=\"o\">)</span>\n  <span class=\"k\">val</span> <span class=\"nc\">MEMORY_ONLY_SER_2</span> <span class=\"k\">=</span> <span class=\"k\">new</span> <span class=\"nc\">StorageLevel</span><span class=\"o\">(</span><span class=\"kc\">false</span><span class=\"o\">,</span> <span class=\"kc\">true</span><span class=\"o\">,</span> <span class=\"kc\">false</span><span class=\"o\">,</span> <span class=\"kc\">false</span><span class=\"o\">,</span> <span class=\"mi\">2</span><span class=\"o\">)</span>\n  <span class=\"k\">val</span> <span class=\"nc\">MEMORY_AND_DISK</span> <span class=\"k\">=</span> <span class=\"k\">new</span> <span class=\"nc\">StorageLevel</span><span class=\"o\">(</span><span class=\"kc\">true</span><span class=\"o\">,</span> <span class=\"kc\">true</span><span class=\"o\">,</span> <span class=\"kc\">false</span><span class=\"o\">,</span> <span class=\"kc\">true</span><span class=\"o\">)</span>\n  <span class=\"k\">val</span> <span class=\"nc\">MEMORY_AND_DISK_2</span> <span class=\"k\">=</span> <span class=\"k\">new</span> <span class=\"nc\">StorageLevel</span><span class=\"o\">(</span><span class=\"kc\">true</span><span class=\"o\">,</span> <span class=\"kc\">true</span><span class=\"o\">,</span> <span class=\"kc\">false</span><span class=\"o\">,</span> <span class=\"kc\">true</span><span class=\"o\">,</span> <span class=\"mi\">2</span><span class=\"o\">)</span>\n  <span class=\"k\">val</span> <span class=\"nc\">MEMORY_AND_DISK_SER</span> <span class=\"k\">=</span> <span class=\"k\">new</span> <span class=\"nc\">StorageLevel</span><span class=\"o\">(</span><span class=\"kc\">true</span><span class=\"o\">,</span> <span class=\"kc\">true</span><span class=\"o\">,</span> <span class=\"kc\">false</span><span class=\"o\">,</span> <span class=\"kc\">false</span><span class=\"o\">)</span>\n  <span class=\"k\">val</span> <span class=\"nc\">MEMORY_AND_DISK_SER_2</span> <span class=\"k\">=</span> <span class=\"k\">new</span> <span class=\"nc\">StorageLevel</span><span class=\"o\">(</span><span class=\"kc\">true</span><span class=\"o\">,</span> <span class=\"kc\">true</span><span class=\"o\">,</span> <span class=\"kc\">false</span><span class=\"o\">,</span> <span class=\"kc\">false</span><span class=\"o\">,</span> <span class=\"mi\">2</span><span class=\"o\">)</span>\n  <span class=\"k\">val</span> <span class=\"nc\">OFF_HEAP</span> <span class=\"k\">=</span> <span class=\"k\">new</span> <span class=\"nc\">StorageLevel</span><span class=\"o\">(</span><span class=\"kc\">true</span><span class=\"o\">,</span> <span class=\"kc\">true</span><span class=\"o\">,</span> <span class=\"kc\">true</span><span class=\"o\">,</span> <span class=\"kc\">false</span><span class=\"o\">,</span> <span class=\"mi\">1</span><span class=\"o\">)</span>\n<span class=\"o\">.....</span>\n\n<span class=\"o\">}</span></code></pre></div><p>关于构造函数的几个参数</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">class</span> <span class=\"nc\">StorageLevel</span> <span class=\"k\">private</span><span class=\"o\">(</span>\n    <span class=\"k\">private</span> <span class=\"k\">var</span> <span class=\"nc\">_useDisk</span><span class=\"k\">:</span> <span class=\"kt\">Boolean</span><span class=\"o\">,</span>\n    <span class=\"k\">private</span> <span class=\"k\">var</span> <span class=\"nc\">_useMemory</span><span class=\"k\">:</span> <span class=\"kt\">Boolean</span><span class=\"o\">,</span>\n    <span class=\"k\">private</span> <span class=\"k\">var</span> <span class=\"nc\">_useOffHeap</span><span class=\"k\">:</span> <span class=\"kt\">Boolean</span><span class=\"o\">,</span>\n    <span class=\"k\">private</span> <span class=\"k\">var</span> <span class=\"nc\">_deserialized</span><span class=\"k\">:</span> <span class=\"kt\">Boolean</span><span class=\"o\">,</span>\n    <span class=\"k\">private</span> <span class=\"k\">var</span> <span class=\"nc\">_replication</span><span class=\"k\">:</span> <span class=\"kt\">Int</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"o\">)</span>\n  <span class=\"k\">extends</span> <span class=\"nc\">Externalizable</span></code></pre></div><ul><li>_useDisk：使用磁盘</li><li>_useMemory：使用内存</li><li>_useOffHeap：使用堆外存，这是Java虚拟机里面的概念，堆外内存意味着把内存对象分配在Java虚拟机的堆以外的内存，这些内存直接受操作系统管理（而不是虚拟机）。这样做的结果就是能保持一个较小的堆，以减少垃圾收集对应用的影响。</li><li>_deserialized：使用反序列化，其逆过程序列化（Serialization）是java提供的一种机制，将对象表示成一连串的字节；而反序列化就表示将字节恢复为对象的过程。序列化是对象永久化的一种机制，可以将对象及其属性保存起来，并能在反序列化后直接恢复这个对象</li><li>_replication：副本数，默认是一个</li></ul><h2>3、如何选择存储级别</h2><p>Spark 的存储级别的选择，核心问题是在内存使用率和 CPU 效率之间进行权衡。建议按下面的过程进行存储级别的选择 : </p><ul><li>如果使用默认的存储级别（MEMORY_ONLY），存储在内存中的 RDD 没有发生溢出，那么就选择默认的存储级别。默认存储级别可以最大程度的提高 CPU 的效率,可以使在 RDD 上的操作以最快的速度运行。</li><li>如果内存不能全部存储 RDD，那么使用 MEMORY_ONLY_SER，并挑选一个快速序列化库将对象序列化，以节省内存空间。使用这种存储级别，计算速度仍然很快。</li><li>除了在计算该数据集的代价特别高，或者在需要过滤大量数据的情况下，尽量不要将溢出的数据存储到磁盘。因为，重新计算这个数据分区的耗时与从磁盘读取这些数据的耗时差不多。</li><li>如果想快速还原故障，建议使用多副本存储级别（例如，使用 Spark 作为 web 应用的后台服务，在服务出故障时需要快速恢复的场景下）。所有的存储级别都通过重新计算丢失的数据的方式，提供了完全容错机制。但是多副本级别在发生数据丢失时，不需要重新计算对应的数据库，可以让任务继续运行。</li></ul><h2>4、删除数据</h2><p>Spark 自动监控各个节点上的缓存使用率，并以最近最少使用的方式（LRU）将旧数据块移除内存。如果想手动移除一个 RDD，而不是等待该 RDD 被 Spark 自动移除，可以使用 RDD.unpersist() 方法</p><h2>5、RDD的cache和persist的区别</h2><p>cache()调用的persist()，是使用默认存储级别的快捷设置方法 看一下源码</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"cm\">/**\n</span><span class=\"cm\"> * Persist this RDD with the default storage level (`MEMORY_ONLY`).\n</span><span class=\"cm\"> */</span>\n<span class=\"k\">def</span> <span class=\"n\">cache</span><span class=\"o\">()</span><span class=\"k\">:</span> <span class=\"kt\">this.</span><span class=\"k\">type</span> <span class=\"o\">=</span> <span class=\"n\">persist</span><span class=\"o\">()</span>\n\n<span class=\"cm\">/**\n</span><span class=\"cm\"> * Persist this RDD with the default storage level (`MEMORY_ONLY`).\n</span><span class=\"cm\">*/</span>\n<span class=\"k\">def</span> <span class=\"n\">persist</span><span class=\"o\">()</span><span class=\"k\">:</span> <span class=\"kt\">this.</span><span class=\"k\">type</span> <span class=\"o\">=</span> <span class=\"n\">persist</span><span class=\"o\">(</span><span class=\"nc\">StorageLevel</span><span class=\"o\">.</span><span class=\"nc\">MEMORY_ONLY</span><span class=\"o\">)</span></code></pre></div><p>通过源码可以看出cache()是persist()的简化方式，调用persist的无参版本，也就是调用persist(StorageLevel.MEMORY_ONLY)，cache只有一个默认的缓存级别MEMORY_ONLY，即将数据持久化到内存中，而persist可以通过传递一个 StorageLevel 对象来设置缓存的存储级别。</p><h2>6、DataFrame的cache和persist的区别</h2><p>官网和上的教程说的都是RDD,但是没有讲df的缓存，通过源码发现df和rdd还是不太一样的。</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"cm\">/**\n</span><span class=\"cm\"> * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`).\n</span><span class=\"cm\"> *\n</span><span class=\"cm\"> * @group basic\n</span><span class=\"cm\"> * @since 1.6.0\n</span><span class=\"cm\"> */</span>\n<span class=\"k\">def</span> <span class=\"n\">cache</span><span class=\"o\">()</span><span class=\"k\">:</span> <span class=\"kt\">this.</span><span class=\"k\">type</span> <span class=\"o\">=</span> <span class=\"n\">persist</span><span class=\"o\">()</span>\n\n<span class=\"cm\">/**\n</span><span class=\"cm\"> * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`).\n</span><span class=\"cm\"> *\n</span><span class=\"cm\"> * @group basic\n</span><span class=\"cm\"> * @since 1.6.0\n</span><span class=\"cm\"> */</span>\n<span class=\"k\">def</span> <span class=\"n\">persist</span><span class=\"o\">()</span><span class=\"k\">:</span> <span class=\"kt\">this.</span><span class=\"k\">type</span> <span class=\"o\">=</span> <span class=\"o\">{</span>\n  <span class=\"n\">sparkSession</span><span class=\"o\">.</span><span class=\"n\">sharedState</span><span class=\"o\">.</span><span class=\"n\">cacheManager</span><span class=\"o\">.</span><span class=\"n\">cacheQuery</span><span class=\"o\">(</span><span class=\"k\">this</span><span class=\"o\">)</span>\n  <span class=\"k\">this</span>\n<span class=\"o\">}</span>\n\n<span class=\"cm\">/**\n</span><span class=\"cm\"> * Persist this Dataset with the given storage level.\n</span><span class=\"cm\"> * @param newLevel One of: `MEMORY_ONLY`, `MEMORY_AND_DISK`, `MEMORY_ONLY_SER`,\n</span><span class=\"cm\"> *                 `MEMORY_AND_DISK_SER`, `DISK_ONLY`, `MEMORY_ONLY_2`,\n</span><span class=\"cm\"> *                 `MEMORY_AND_DISK_2`, etc.\n</span><span class=\"cm\"> *\n</span><span class=\"cm\"> * @group basic\n</span><span class=\"cm\"> * @since 1.6.0\n</span><span class=\"cm\"> */</span>\n<span class=\"k\">def</span> <span class=\"n\">persist</span><span class=\"o\">(</span><span class=\"n\">newLevel</span><span class=\"k\">:</span> <span class=\"kt\">StorageLevel</span><span class=\"o\">)</span><span class=\"k\">:</span> <span class=\"kt\">this.</span><span class=\"k\">type</span> <span class=\"o\">=</span> <span class=\"o\">{</span>\n  <span class=\"n\">sparkSession</span><span class=\"o\">.</span><span class=\"n\">sharedState</span><span class=\"o\">.</span><span class=\"n\">cacheManager</span><span class=\"o\">.</span><span class=\"n\">cacheQuery</span><span class=\"o\">(</span><span class=\"k\">this</span><span class=\"o\">,</span> <span class=\"nc\">None</span><span class=\"o\">,</span> <span class=\"n\">newLevel</span><span class=\"o\">)</span>\n  <span class=\"k\">this</span>\n<span class=\"o\">}</span>\n\n<span class=\"k\">def</span> <span class=\"n\">cacheQuery</span><span class=\"o\">(</span>\n    <span class=\"n\">query</span><span class=\"k\">:</span> <span class=\"kt\">Dataset</span><span class=\"o\">[</span><span class=\"k\">_</span><span class=\"o\">],</span>\n    <span class=\"n\">tableName</span><span class=\"k\">:</span> <span class=\"kt\">Option</span><span class=\"o\">[</span><span class=\"kt\">String</span><span class=\"o\">]</span> <span class=\"k\">=</span> <span class=\"nc\">None</span><span class=\"o\">,</span>\n    <span class=\"n\">storageLevel</span><span class=\"k\">:</span> <span class=\"kt\">StorageLevel</span> <span class=\"o\">=</span> <span class=\"nc\">MEMORY_AND_DISK</span><span class=\"o\">)</span><span class=\"k\">:</span> <span class=\"kt\">Unit</span> <span class=\"o\">=</span> <span class=\"n\">writeLock</span></code></pre></div><p>可以到cache()依然调用的persist()，但是persist调用cacheQuery，而cacheQuery的默认存储级别为MEMORY_AND_DISK，这点和rdd是不一样的。</p><h2>7、代码测试</h2><p>新建一个测试的txt,文件越大越好，如果文件比较小，可能cache的效果还不如不cache的好。</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.SparkSession</span>\n\n<span class=\"k\">object</span> <span class=\"nc\">Test</span> <span class=\"o\">{</span>\n\n  <span class=\"k\">def</span> <span class=\"n\">main</span><span class=\"o\">(</span><span class=\"n\">args</span><span class=\"k\">:</span> <span class=\"kt\">Array</span><span class=\"o\">[</span><span class=\"kt\">String</span><span class=\"o\">])</span><span class=\"k\">:</span> <span class=\"kt\">Unit</span> <span class=\"o\">=</span> <span class=\"o\">{</span>\n\n    <span class=\"k\">val</span> <span class=\"n\">spark</span> <span class=\"k\">=</span> <span class=\"nc\">SparkSession</span><span class=\"o\">.</span><span class=\"n\">builder</span><span class=\"o\">().</span><span class=\"n\">master</span><span class=\"o\">(</span><span class=\"s\">&#34;local&#34;</span><span class=\"o\">).</span><span class=\"n\">getOrCreate</span><span class=\"o\">()</span>\n    <span class=\"k\">val</span> <span class=\"n\">sc</span> <span class=\"k\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">sparkContext</span>\n\n    <span class=\"k\">val</span> <span class=\"n\">rdd</span> <span class=\"k\">=</span> <span class=\"n\">sc</span><span class=\"o\">.</span><span class=\"n\">textFile</span><span class=\"o\">(</span><span class=\"s\">&#34;files/test.txt&#34;</span><span class=\"o\">)</span>\n    <span class=\"k\">var</span> <span class=\"n\">beiginTime</span> <span class=\"k\">=</span> <span class=\"nc\">System</span><span class=\"o\">.</span><span class=\"n\">currentTimeMillis</span>\n    <span class=\"n\">println</span><span class=\"o\">(</span><span class=\"n\">rdd</span><span class=\"o\">.</span><span class=\"n\">count</span><span class=\"o\">)</span>\n    <span class=\"k\">var</span> <span class=\"n\">endTime</span> <span class=\"k\">=</span> <span class=\"nc\">System</span><span class=\"o\">.</span><span class=\"n\">currentTimeMillis</span>\n    <span class=\"n\">println</span><span class=\"o\">(</span><span class=\"s\">&#34;cost &#34;</span> <span class=\"o\">+</span> <span class=\"o\">(</span><span class=\"n\">endTime</span> <span class=\"o\">-</span> <span class=\"n\">beiginTime</span><span class=\"o\">)</span> <span class=\"o\">+</span> <span class=\"s\">&#34; milliseconds.&#34;</span><span class=\"o\">)</span>\n\n    <span class=\"n\">beiginTime</span> <span class=\"k\">=</span> <span class=\"nc\">System</span><span class=\"o\">.</span><span class=\"n\">currentTimeMillis</span>\n    <span class=\"n\">println</span><span class=\"o\">(</span><span class=\"n\">rdd</span><span class=\"o\">.</span><span class=\"n\">count</span><span class=\"o\">)</span>\n    <span class=\"n\">endTime</span> <span class=\"k\">=</span> <span class=\"nc\">System</span><span class=\"o\">.</span><span class=\"n\">currentTimeMillis</span>\n    <span class=\"n\">println</span><span class=\"o\">(</span><span class=\"s\">&#34;cost &#34;</span> <span class=\"o\">+</span> <span class=\"o\">(</span><span class=\"n\">endTime</span> <span class=\"o\">-</span> <span class=\"n\">beiginTime</span><span class=\"o\">)</span> <span class=\"o\">+</span> <span class=\"s\">&#34; milliseconds.&#34;</span><span class=\"o\">)</span>\n    <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">stop</span>\n  <span class=\"o\">}</span>\n<span class=\"o\">}</span></code></pre></div><p>先看一下没有cache的时间</p><div class=\"highlight\"><pre><code class=\"language-vim\"><span class=\"m\">79391</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"nx\">cost</span> <span class=\"m\">575</span> <span class=\"nx\">milliseconds</span>.<span class=\"err\">\n</span><span class=\"err\"></span><span class=\"m\">79391</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"nx\">cost</span> <span class=\"m\">124</span> <span class=\"nx\">milliseconds</span>.</code></pre></div><p>然后将代码val rdd = sc.textFile(&#34;files/test.txt&#34;)替换为val rdd = sc.textFile(&#34;files/test.txt&#34;).cache，再进行测试</p><div class=\"highlight\"><pre><code class=\"language-vim\"><span class=\"m\">79391</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"nx\">cost</span> <span class=\"m\">635</span> <span class=\"nx\">milliseconds</span>.<span class=\"err\">\n</span><span class=\"err\"></span><span class=\"m\">79391</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"nx\">cost</span> <span class=\"m\">44</span> <span class=\"nx\">milliseconds</span>.</code></pre></div><p>可以看到cache后第二次count的时间明显比没有cache第二次count的时间少很多，第一次count时间增加是因为要进行持久化，如果看总时间的话，只有多次使用该rdd的时候，效果才明显。</p><h2>8、注意</h2><p>cache()和persist()的使用是有规则的： 必须在transformation或者textfile等创建一个rdd之后，直接连续调用cache()或者persist()才可以，如果先创建一个rdd,再单独另起一行执行cache()或者persist()，是没有用的，而且会报错，大量的文件会丢失。</p><h2>参考资料</h2><p><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/houmou/article/details/52491419\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">blog.csdn.net/houmou/ar</span><span class=\"invisible\">ticle/details/52491419</span><span class=\"ellipsis\"></span></a></p>", 
            "topic": [
                {
                    "tag": "缓存", 
                    "tagLink": "https://api.zhihu.com/topics/19564998"
                }, 
                {
                    "tag": "Spark", 
                    "tagLink": "https://api.zhihu.com/topics/19942170"
                }
            ], 
            "comments": [
                {
                    "userName": "Kikinger", 
                    "userLink": "https://www.zhihu.com/people/48877a28793b6d8057ea2e5ea4276f7c", 
                    "content": "<p>请问第8点对于cache的规则的说明是如何得出的呢？</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50088687", 
            "userName": "董可伦", 
            "userLink": "https://www.zhihu.com/people/de888cc734ddbfd918269a8e5fb196ba", 
            "upvote": 0, 
            "title": "Scala日期操作", 
            "content": "<p>我的原创地址：</p><a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/06/01/scalaDate//\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-0b4e5faa279a5e545d79e754b3dba862_ipico.jpg\" data-image-width=\"640\" data-image-height=\"640\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Scala日期操作</a><h2>前言</h2><p>本文主要记录我自己对日期格式数据的一些常用操作，主要目的是备忘，方便随时查阅。本文没有将代码封装为函数，如果有需要的可以自行封装，注意每一部分的代码会依赖前面代码里的变量。</p><p>代码可以直接在spark-shell里运行（在scala里有的包没有）</p><h2>1、字符串转日期</h2><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">import</span> <span class=\"nn\">java.text.SimpleDateFormat</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.joda.time.DateTime</span>\n<span class=\"k\">val</span> <span class=\"n\">dateStr</span> <span class=\"k\">=</span> <span class=\"s\">&#34;2018-06-01&#34;</span>\n<span class=\"k\">val</span> <span class=\"n\">pattern</span> <span class=\"k\">=</span> <span class=\"s\">&#34;yyyy-MM-dd&#34;</span>\n<span class=\"k\">val</span> <span class=\"n\">date</span> <span class=\"k\">=</span> <span class=\"k\">new</span> <span class=\"nc\">SimpleDateFormat</span><span class=\"o\">(</span><span class=\"n\">pattern</span><span class=\"o\">).</span><span class=\"n\">parse</span><span class=\"o\">(</span><span class=\"n\">dateStr</span><span class=\"o\">)</span>\n<span class=\"k\">val</span> <span class=\"n\">dateTime</span> <span class=\"k\">=</span> <span class=\"k\">new</span> <span class=\"nc\">DateTime</span><span class=\"o\">(</span><span class=\"n\">date</span><span class=\"o\">)</span>\n<span class=\"n\">println</span><span class=\"o\">(</span><span class=\"n\">date</span><span class=\"o\">)</span>\n<span class=\"n\">println</span><span class=\"o\">(</span><span class=\"n\">dateTime</span><span class=\"o\">)</span>\n<span class=\"nc\">Fri</span> <span class=\"nc\">Jun</span> <span class=\"mi\">01</span> <span class=\"mi\">00</span><span class=\"k\">:</span><span class=\"err\">00</span><span class=\"kt\">:</span><span class=\"err\">00</span> <span class=\"kt\">CST</span> <span class=\"err\">2018</span>\n<span class=\"err\">2018</span><span class=\"kt\">-</span><span class=\"err\">06</span><span class=\"kt\">-</span><span class=\"err\">01</span><span class=\"kt\">T00:</span><span class=\"err\">00</span><span class=\"kt\">:</span><span class=\"err\">00</span><span class=\"kt\">.</span><span class=\"err\">000</span><span class=\"kt\">+</span><span class=\"err\">08</span><span class=\"kt\">:</span><span class=\"err\">00</span></code></pre></div><h2>2、日期转字符串</h2><p>将上面的日期转成其他格式的字符串</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"n\">println</span><span class=\"o\">(</span><span class=\"k\">new</span> <span class=\"nc\">SimpleDateFormat</span><span class=\"o\">(</span><span class=\"s\">&#34;yyyyMMdd&#34;</span><span class=\"o\">).</span><span class=\"n\">format</span><span class=\"o\">(</span><span class=\"n\">date</span><span class=\"o\">))</span>\n<span class=\"mi\">20180601</span></code></pre></div><h2>3、字符串转时间戳</h2><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"n\">println</span><span class=\"o\">(</span><span class=\"n\">date</span><span class=\"o\">.</span><span class=\"n\">getTime</span><span class=\"o\">)</span>\n<span class=\"n\">println</span><span class=\"o\">(</span><span class=\"n\">date</span><span class=\"o\">.</span><span class=\"n\">getTime</span><span class=\"o\">)</span></code></pre></div><h2>4、计算时间差</h2><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">val</span> <span class=\"n\">startDateStr</span> <span class=\"k\">=</span> <span class=\"s\">&#34;2018-03-21&#34;</span>\n<span class=\"k\">val</span> <span class=\"n\">endDateStr</span> <span class=\"k\">=</span> <span class=\"s\">&#34;2018-03-22&#34;</span>\n<span class=\"k\">val</span> <span class=\"n\">startDate</span> <span class=\"k\">=</span> <span class=\"k\">new</span> <span class=\"nc\">SimpleDateFormat</span><span class=\"o\">(</span><span class=\"n\">pattern</span><span class=\"o\">).</span><span class=\"n\">parse</span><span class=\"o\">(</span><span class=\"n\">startDateStr</span><span class=\"o\">)</span>\n<span class=\"k\">val</span> <span class=\"n\">endDate</span> <span class=\"k\">=</span> <span class=\"k\">new</span> <span class=\"nc\">SimpleDateFormat</span><span class=\"o\">(</span><span class=\"n\">pattern</span><span class=\"o\">).</span><span class=\"n\">parse</span><span class=\"o\">(</span><span class=\"n\">endDateStr</span><span class=\"o\">)</span>\n<span class=\"k\">val</span> <span class=\"n\">between</span> <span class=\"k\">=</span> <span class=\"n\">endDate</span><span class=\"o\">.</span><span class=\"n\">getTime</span> <span class=\"o\">-</span> <span class=\"n\">startDate</span><span class=\"o\">.</span><span class=\"n\">getTime</span>\n<span class=\"k\">val</span> <span class=\"n\">second</span> <span class=\"k\">=</span> <span class=\"n\">between</span> <span class=\"o\">/</span> <span class=\"mi\">1000</span>\n<span class=\"k\">val</span> <span class=\"n\">hour</span> <span class=\"k\">=</span> <span class=\"n\">between</span> <span class=\"o\">/</span> <span class=\"mi\">1000</span> <span class=\"o\">/</span> <span class=\"mi\">3600</span>\n<span class=\"k\">val</span> <span class=\"n\">day</span> <span class=\"k\">=</span> <span class=\"n\">between</span> <span class=\"o\">/</span> <span class=\"mi\">1000</span> <span class=\"o\">/</span> <span class=\"mi\">3600</span> <span class=\"o\">/</span> <span class=\"mi\">24</span>\n<span class=\"k\">val</span> <span class=\"n\">year</span> <span class=\"k\">=</span> <span class=\"n\">between</span> <span class=\"o\">/</span> <span class=\"mi\">1000</span> <span class=\"o\">/</span> <span class=\"mi\">3600</span> <span class=\"o\">/</span> <span class=\"mi\">24</span> <span class=\"o\">/</span> <span class=\"mi\">365</span></code></pre></div><p>如果需要结果为小数，以hour举例</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">import</span> <span class=\"nn\">java.text.DecimalFormat</span>\n<span class=\"k\">val</span> <span class=\"n\">hour</span><span class=\"k\">:</span> <span class=\"kt\">Float</span> <span class=\"o\">=</span> <span class=\"n\">between</span><span class=\"o\">.</span><span class=\"n\">toFloat</span> <span class=\"o\">/</span> <span class=\"mi\">1000</span> <span class=\"o\">/</span> <span class=\"mi\">3600</span>\n<span class=\"k\">val</span> <span class=\"n\">decf</span><span class=\"k\">:</span> <span class=\"kt\">DecimalFormat</span> <span class=\"o\">=</span> <span class=\"k\">new</span> <span class=\"nc\">DecimalFormat</span><span class=\"o\">(</span><span class=\"s\">&#34;#.00&#34;</span><span class=\"o\">)</span>\n<span class=\"n\">println</span><span class=\"o\">(</span><span class=\"n\">hour</span><span class=\"o\">)</span>\n<span class=\"n\">println</span><span class=\"o\">(</span><span class=\"n\">decf</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"o\">(</span><span class=\"n\">hour</span><span class=\"o\">))</span> <span class=\"c1\">//格式化为两位小数\n</span><span class=\"c1\"></span><span class=\"mf\">24.0</span>\n<span class=\"mf\">24.00</span></code></pre></div><h2>参考资料</h2><p><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/qq_16038125/article/details/72834270\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">blog.csdn.net/qq_160381</span><span class=\"invisible\">25/article/details/72834270</span><span class=\"ellipsis\"></span></a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/springlustre/article/details/47273353\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">blog.csdn.net/springlus</span><span class=\"invisible\">tre/article/details/47273353</span><span class=\"ellipsis\"></span></a></p>", 
            "topic": [
                {
                    "tag": "Scala", 
                    "tagLink": "https://api.zhihu.com/topics/19566465"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/45383748", 
            "userName": "董可伦", 
            "userLink": "https://www.zhihu.com/people/de888cc734ddbfd918269a8e5fb196ba", 
            "upvote": 3, 
            "title": "Spark读取压缩文件", 
            "content": "<p>我的原创地址：</p><a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/05/30/sparkGZ/\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-0b4e5faa279a5e545d79e754b3dba862_ipico.jpg\" data-image-width=\"640\" data-image-height=\"640\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Spark读取压缩文件</a><h2>前言</h2><p>本文讲如何用spark读取gz类型的压缩文件，以及如何解决我遇到的各种问题。</p><h2>1、文件压缩</h2><p>下面这一部分摘自Spark快速大数据分析：   在大数据工作中，我们经常需要对数据进行压缩以节省存储空间和网络传输开销。对于大多数Hadoop输出格式来说，我们可以指定一种压缩编解码器来压缩数据。   选择一个输出压缩编解码器可能会对这些数据以后的用户产生巨大影响。对于像Spark 这样的分布式系统，我们通常会尝试从多个不同机器上一起读入数据。要实现这种情况，每个工作节点都必须能够找到一条新记录的开端。有些压缩格式会使这变得不可能，而必须要单个节点来读入所有数据，这就很容易产生性能瓶颈。可以很容易地从多个节点上并行读取的格式被称为“可分割”的格式。下表列出了可用的压缩选项。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>格式 | 可分割 | 平均压缩速度| 文本文件压缩效率 | Hadoop压缩编解码器 | 纯Java实现 | 原生 | 备注 - | :-:  | :-:  | :-: | :-:  gzip | 否 | 快 | 高 | org.apache.hadoop.io.compress.GzipCodec | 是 | 是 | lzo |   是（取决于所使用的库） | 非常快 | 中等 | com.hadoop.compression.lzo.LzoCodec | 是 | 是 | 需要在每个节点上安装LZO bzip2 |  是 | 慢 | 非常高 | org.apache.hadoop.io.compress.Bzip2Codec | 是 | 是 | 为可分割版本使用纯Java zlib  |  否 |  慢 |  中等 |  org.apache.hadoop.io.compress.DefaultCodec |  是 |  是 |  Hadoop 的默认压缩编解码器 Snappy  | 否 | 非常快 | 低 | org.apache.hadoop.io.compress.SnappyCodec | 否 | 是 | Snappy 有纯Java的移植版，但是在Spark/Hadoop中不能用</p><p>尽管Spark 的textFile() 方法可以处理压缩过的输入，但即使输入数据被以可分割读取的方式压缩，Spark 也不会打开splittable。因此，如果你要读取单个压缩过的输入，最好不要考虑使用Spark 的封装，而是使用newAPIHadoopFile 或者hadoopFile，并指定正确的压缩编解码器。</p><p>关于上面一段话的个人测试：选取一个大文件txt，大小为1.5G，写spark程序读取hdfs上的该文件然后写入hive，经测试在多个分区的情况下，txt执行时间最短，因为在多个机器并行执行，而gz文件是不可分割的，即使指定分区数目，但依然是一个分区，一个task，即在一个机器上执行，bzip2格式的文件虽然是可分割的，即可以按照指定的分区分为不同的task在多个机器上执行，但是执行时间长，比gz时间还长，经过四次改变bzip2的分区，发现最快的时间和gz时间是一样的，如果指定一个分区的话，比gz要慢很多，我想这样就可以更好的理解：&#34;尽管Spark 的textFile() 方法可以处理压缩过的输入，但即使输入数据被以可分割读取的方式压缩，Spark 也不会打开splittable&#34;这句话了。 * 注：上面主要证明不管是可分割的bzip2还是不可分割的gz都比txt慢，至于bzip2比gz慢的原因，是因为我分配的Executor数量较少，经后续测试，根据集群的cpu合理分配executor的个数的情况下，txt的时间缩短到1分钟，bzip2缩短到1.3分钟，而对gz重新分区(reparation)缩短到2分钟，可以看到在合理分配资源的情况下，bzip2比gz快不少，但依然赶不上txt，当然这也的结果可能受文件大小和集群资源的限制，所以根据自己的实际需求测试再决定用哪个即可。 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-d189d2be50f16fc851bb36c265572941_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1207\" data-rawheight=\"219\" class=\"origin_image zh-lightbox-thumb\" width=\"1207\" data-original=\"https://pic2.zhimg.com/v2-d189d2be50f16fc851bb36c265572941_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1207&#39; height=&#39;219&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1207\" data-rawheight=\"219\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1207\" data-original=\"https://pic2.zhimg.com/v2-d189d2be50f16fc851bb36c265572941_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-d189d2be50f16fc851bb36c265572941_b.jpg\"/></figure><h2>2、代码</h2><p>代码很简单，用textFile()即可，假设，我的数据名为data.txt.gz,我把它放在hdfs上的/tmp/dkl路径下那么代码为：</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">val</span> <span class=\"n\">path</span> <span class=\"k\">=</span> <span class=\"s\">&#34;hdfs://ambari.master.com:8020/tmp/dkl/data.txt.gz&#34;</span>\n<span class=\"k\">val</span> <span class=\"n\">data</span> <span class=\"k\">=</span> <span class=\"n\">sc</span><span class=\"o\">.</span><span class=\"n\">textFile</span><span class=\"o\">(</span><span class=\"n\">path</span><span class=\"o\">)</span></code></pre></div><p>注：把数据放在hdfs的命令为</p><div class=\"highlight\"><pre><code class=\"language-bash\">hadoop fs -put data.tar.gz /tml/dkl</code></pre></div><h2>3、一些小问题</h2><h2>3.1 数据</h2><p>首先造几个数据吧,先创建一个txt,名字为data.txt,内容如下</p><div class=\"highlight\"><pre><code class=\"language-vim\"><span class=\"m\">1</span>            <span class=\"nx\">张三</span>            <span class=\"nx\">上海</span>        <span class=\"m\">2018-05-25</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"m\">2</span>            <span class=\"nx\">张三</span>            <span class=\"nx\">上海</span>        <span class=\"m\">2018-05-25</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"m\">3</span>            <span class=\"nx\">张三</span>            <span class=\"nx\">上海</span>        <span class=\"m\">2018-05-25</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"m\">4</span>            <span class=\"nx\">张三</span>            <span class=\"nx\">上海</span>        <span class=\"m\">2018-05-25</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"m\">5</span>            <span class=\"nx\">张三</span>            <span class=\"nx\">上海</span>        <span class=\"m\">2018-05-25</span></code></pre></div><h2>3.2 如何压缩</h2><p>那么如如何打包为gz格式的压缩文件呢，分两种 一、 在windows上打包，如果不想在Linux服务器上用命令打包，那么可以直接用windows上的软件打包（win上常见的zip,rar格式，spark是不支持的），我用7-zip软件压缩，大家可百度7-zip或直接在<a href=\"https://link.zhihu.com/?target=https%3A//www.7-zip.org/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">7-zip.org/</span><span class=\"invisible\"></span></a>下载安装,压缩格式选gzip即可。 二、 在Linux上压缩，可通过下面的命令 1、保留原文件</p><div class=\"highlight\"><pre><code class=\"language-bash\">gzip –c data.txt &gt; data.txt.gz</code></pre></div><p>2、不保留原文件，默认生成的文件名为原文件名.gz,即data.txt.gz</p><div class=\"highlight\"><pre><code class=\"language-text\">gzip data.txt</code></pre></div><p>压缩完了之后，跑一下程序测试一下</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">take</span><span class=\"o\">(</span><span class=\"mi\">3</span><span class=\"o\">).</span><span class=\"n\">foreach</span><span class=\"o\">(</span><span class=\"n\">println</span><span class=\"o\">)</span>\n<span class=\"mi\">1</span>            <span class=\"n\">张三</span>            <span class=\"n\">上海</span>        <span class=\"mi\">2018</span><span class=\"o\">-</span><span class=\"mi\">05</span><span class=\"o\">-</span><span class=\"mi\">25</span>\n<span class=\"mi\">2</span>            <span class=\"n\">张三</span>            <span class=\"n\">上海</span>        <span class=\"mi\">2018</span><span class=\"o\">-</span><span class=\"mi\">05</span><span class=\"o\">-</span><span class=\"mi\">25</span>\n<span class=\"mi\">3</span>            <span class=\"n\">张三</span>            <span class=\"n\">上海</span>        <span class=\"mi\">2018</span><span class=\"o\">-</span><span class=\"mi\">05</span><span class=\"o\">-</span><span class=\"mi\">25</span></code></pre></div><p>根据结果看没问题。 三、 说明 在Linux上用tar命令压缩，spark虽然可以读，但是第一行会有文件信息</p><div class=\"highlight\"><pre><code class=\"language-bash\">tar -zcvf data.tar.gz data.txt</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-a1337902c4d69b721bdb1ba4a319c4f5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1463\" data-rawheight=\"62\" class=\"origin_image zh-lightbox-thumb\" width=\"1463\" data-original=\"https://pic2.zhimg.com/v2-a1337902c4d69b721bdb1ba4a319c4f5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1463&#39; height=&#39;62&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1463\" data-rawheight=\"62\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1463\" data-original=\"https://pic2.zhimg.com/v2-a1337902c4d69b721bdb1ba4a319c4f5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-a1337902c4d69b721bdb1ba4a319c4f5_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>3.3 文件编码问题</h2><p>别人给我的原文件是.rar,那我需要将其解压之后得到txt，然后按照上述方式压缩为.gz，然后上传到hdfs，进行代码测试，打印前几条发现乱码，查了一下发现原文件是gbk编码的，且sc.textFile()不能指定编码，只能读取utf8格式，其他格式就会乱码。</p><p>注意：因为实际情况下解压后的txt文件很大，windows是直接打不开的，所以不能通过打开文件修改编码的方法去解决。</p><h2>3.3.1 构建测试gbk格式的文件</h2><p>1、windows上可以用记事本打开，另存为，编码选择ANSI即可</p><p>2、Linux可以通过下面的命令修改</p><div class=\"highlight\"><pre><code class=\"language-bash\">iconv -f utf8 -t gbk data.txt &gt; data_gbk.txt</code></pre></div><p>测试一下输出,发现确实乱码了（直接测试txt即可）</p><div class=\"highlight\"><pre><code class=\"language-vim\"><span class=\"m\">1</span>            ����            �<span class=\"nx\">Ϻ</span>�        <span class=\"m\">2018-05-25</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"m\">2</span>            ����            �<span class=\"nx\">Ϻ</span>�        <span class=\"m\">2018-05-25</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"m\">3</span>            ����            �<span class=\"nx\">Ϻ</span>�        <span class=\"m\">2018-05-25</span></code></pre></div><h2>3.3.2 代码解决</h2><p>通过如下代码测试即可 定义方法</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.rdd.RDD</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.SparkContext</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.hadoop.io.LongWritable</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.hadoop.mapred.TextInputFormat</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.hadoop.io.Text</span>\n<span class=\"k\">def</span> <span class=\"n\">transfer</span><span class=\"o\">(</span><span class=\"n\">sc</span><span class=\"k\">:</span> <span class=\"kt\">SparkContext</span><span class=\"o\">,</span> <span class=\"n\">path</span><span class=\"k\">:</span> <span class=\"kt\">String</span><span class=\"o\">)</span><span class=\"k\">:</span> <span class=\"kt\">RDD</span><span class=\"o\">[</span><span class=\"kt\">String</span><span class=\"o\">]</span> <span class=\"k\">=</span> <span class=\"o\">{</span>\n  <span class=\"n\">sc</span><span class=\"o\">.</span><span class=\"n\">hadoopFile</span><span class=\"o\">(</span><span class=\"n\">path</span><span class=\"o\">,</span> <span class=\"n\">classOf</span><span class=\"o\">[</span><span class=\"kt\">TextInputFormat</span><span class=\"o\">],</span> <span class=\"n\">classOf</span><span class=\"o\">[</span><span class=\"kt\">LongWritable</span><span class=\"o\">],</span> <span class=\"n\">classOf</span><span class=\"o\">[</span><span class=\"kt\">Text</span><span class=\"o\">],</span> <span class=\"mi\">1</span><span class=\"o\">)</span>\n    <span class=\"o\">.</span><span class=\"n\">map</span><span class=\"o\">(</span><span class=\"n\">p</span> <span class=\"k\">=&gt;</span> <span class=\"k\">new</span> <span class=\"nc\">String</span><span class=\"o\">(</span><span class=\"n\">p</span><span class=\"o\">.</span><span class=\"n\">_2</span><span class=\"o\">.</span><span class=\"n\">getBytes</span><span class=\"o\">,</span> <span class=\"mi\">0</span><span class=\"o\">,</span> <span class=\"n\">p</span><span class=\"o\">.</span><span class=\"n\">_2</span><span class=\"o\">.</span><span class=\"n\">getLength</span><span class=\"o\">,</span> <span class=\"s\">&#34;GBK&#34;</span><span class=\"o\">))</span>\n<span class=\"o\">}</span></code></pre></div><p>测试方法</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"n\">transfer</span><span class=\"o\">(</span><span class=\"n\">sc</span><span class=\"o\">,</span> <span class=\"n\">path3</span><span class=\"o\">).</span><span class=\"n\">take</span><span class=\"o\">(</span><span class=\"mi\">3</span><span class=\"o\">).</span><span class=\"n\">foreach</span><span class=\"o\">(</span><span class=\"n\">println</span><span class=\"o\">)</span></code></pre></div><p>参考：<a href=\"https://link.zhihu.com/?target=https%3A//www.cnblogs.com/bonnienote/p/6139671.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Spark Scala 读取GBK文件的方法</a></p><h2>3.3.3 Linux命令</h2><p>可直接通过Linux命令转换txt的编码格式，再压缩，这样代码就不用修改 其实在3.2.1中已经涉及到了 1、通过Linux自带的命令iconv iconv不能覆盖原来的文件，只能生成新的文件之后，再通过mv命令去覆盖</p><div class=\"highlight\"><pre><code class=\"language-bash\">iconv -f gbk -t utf8 data_gbk.txt &gt; data_new.txt</code></pre></div><p>2、通过enca enca可以直接覆盖原来的文件，这样如果不想改变来的文件名，就少一步mv操作了，enca不是子系统自带的，需要自己下载安装，可在<a href=\"https://link.zhihu.com/?target=http%3A//dl.cihar.com/enca/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">dl.cihar.com/enca/</span><span class=\"invisible\"></span></a>下载最新版本。</p><div class=\"highlight\"><pre><code class=\"language-bash\"><span class=\"c1\">#下载&amp;解压</span>\nwget http://dl.cihar.com/enca/enca-1.19.tar.gz\ntar -zxvf enca-1.19.tar.gz\n<span class=\"nb\">cd</span> enca-1.19\n<span class=\"c1\">#编译安装</span>\n./configure\nmake\nmake install</code></pre></div><p>安装好了之后通过下面的命令转换即可</p><div class=\"highlight\"><pre><code class=\"language-text\">enca -L zh_CN -x UTF-8 data_gbk.txt</code></pre></div><p>转换编码格式之后，在通过程序测试即可。</p><p>参考:<a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/houmou/article/details/51064579\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">linux 下的文件编码格式转换</a></p><h2>3.4 rdd换df</h2><p>由于文件过大，不能直接打开看也没用垃圾数据，造成格式问题，如果有垃圾数据，在rdd转df的过程中会产生异常，这里记录一下我碰见的问题。</p><p>1、首先可以先打印出前几行数据查看一下该文件的大体格式</p><p>2、碰到的一个一个异常 代码用的<a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/05/11/rdd2df/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">旧版spark（1.6版本） 将rdd动态转为dataframe</a>里面的方法。</p><div class=\"highlight\"><pre><code class=\"language-text\">if (assertnotnull(input[0, org.apache.spark.sql.Row, true]).isNullAt) null else staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, validateexternaltype(getexternalrowfield(assertnotnull(input[0, org.apache.spark.sql.Row, true])....</code></pre></div><p>原因是因为文件里有一行数据为垃圾数据，这行数据的列数和列名的个数不一样导致的，可以在代码中过滤掉这样数据即可。</p><div class=\"highlight\"><pre><code class=\"language-text\">.filter(_.length == colName.length)</code></pre></div><p></p>", 
            "topic": [
                {
                    "tag": "Hadoop", 
                    "tagLink": "https://api.zhihu.com/topics/19563390"
                }, 
                {
                    "tag": "Spark", 
                    "tagLink": "https://api.zhihu.com/topics/19942170"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/45334040", 
            "userName": "董可伦", 
            "userLink": "https://www.zhihu.com/people/de888cc734ddbfd918269a8e5fb196ba", 
            "upvote": 1, 
            "title": "如何解决spark开发中遇到需要去掉文件前几行数据的问题", 
            "content": "<p>我的原创地址：</p><a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/05/27/sparkDelFirstNLines/\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-0b4e5faa279a5e545d79e754b3dba862_ipico.jpg\" data-image-width=\"640\" data-image-height=\"640\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">如何解决spark开发中遇到需要去掉文件前几行数据的问题</a><h2>前言</h2><p>我碰到的问题是这样的，我需要读取压缩文件里的数据存到hive表里，压缩文件解压之后是一个txt，这个txt里前几行的数据是垃圾数据，而这个txt文件太大，txt是直接打不开的，所以不能手动打开删除前几行数据，而这个文件是业务人员从别人那拿到的所以也不能改，本文就是讲如何解决这个问题。</p><h2>1、数据</h2><p>首先造几条数据，以理解我的需求 data.txt</p><div class=\"highlight\"><pre><code class=\"language-vim\"><span class=\"nx\">id</span>           <span class=\"nx\">name</span>                <span class=\"nx\">addr</span>            <span class=\"nx\">time</span>                                      <span class=\"err\">\n</span><span class=\"err\"></span><span class=\"p\">------------</span> <span class=\"p\">-------------------</span> <span class=\"p\">---------------</span> <span class=\"p\">--------------------</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"m\">1</span>            <span class=\"nx\">zhangsan</span>            <span class=\"nx\">shanghai</span>        <span class=\"m\">2018-05-25</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"m\">2</span>            <span class=\"nx\">zhangsan</span>            <span class=\"nx\">shanghai</span>        <span class=\"m\">2018-05-25</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"m\">3</span>            <span class=\"nx\">zhangsan</span>            <span class=\"nx\">shanghai</span>        <span class=\"m\">2018-05-25</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"m\">4</span>            <span class=\"nx\">zhangsan</span>            <span class=\"nx\">shanghai</span>        <span class=\"m\">2018-05-25</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"m\">5</span>            <span class=\"nx\">zhangsan</span>            <span class=\"nx\">shanghai</span>        <span class=\"m\">2018-05-25</span></code></pre></div><p>其中前三行是我不想要的数据，第一行为空，第二行为字段名，第三行应该是为了美观单独加了一行。 </p><h2>2、尝试用代码解决</h2><h2>2.1 思路一</h2><p>用zipWithIndex给rdd加上索引，索引从0开始依次次递增1</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">val</span> <span class=\"n\">path</span> <span class=\"k\">=</span> <span class=\"s\">&#34;files/data.txt&#34;</span>\n<span class=\"k\">val</span> <span class=\"n\">rdd</span> <span class=\"k\">=</span> <span class=\"n\">sc</span><span class=\"o\">.</span><span class=\"n\">textFile</span><span class=\"o\">(</span><span class=\"n\">path</span><span class=\"o\">)</span>\n<span class=\"n\">println</span><span class=\"o\">(</span><span class=\"s\">&#34;分区数：&#34;</span> <span class=\"o\">+</span> <span class=\"n\">rdd</span><span class=\"o\">.</span><span class=\"n\">getNumPartitions</span><span class=\"o\">)</span>\n<span class=\"k\">val</span> <span class=\"n\">rdd1</span> <span class=\"k\">=</span> <span class=\"n\">rdd</span><span class=\"o\">.</span><span class=\"n\">zipWithIndex</span><span class=\"o\">()</span>\n<span class=\"c1\">//过滤掉索引小于等于2的\n</span><span class=\"c1\"></span><span class=\"k\">val</span> <span class=\"n\">rdd2</span> <span class=\"k\">=</span> <span class=\"n\">rdd1</span><span class=\"o\">.</span><span class=\"n\">filter</span><span class=\"o\">(</span><span class=\"k\">_</span><span class=\"o\">.</span><span class=\"n\">_2</span> <span class=\"o\">&gt;</span> <span class=\"mi\">2</span><span class=\"o\">)</span>\n<span class=\"n\">rdd1</span><span class=\"o\">.</span><span class=\"n\">foreach</span><span class=\"o\">(</span><span class=\"n\">println</span><span class=\"o\">)</span>\n<span class=\"n\">println</span><span class=\"o\">(</span><span class=\"s\">&#34;**********分割线***********&#34;</span><span class=\"o\">)</span>\n<span class=\"n\">rdd2</span><span class=\"o\">.</span><span class=\"n\">map</span><span class=\"o\">(</span><span class=\"n\">kv</span> <span class=\"k\">=&gt;</span> <span class=\"n\">kv</span><span class=\"o\">.</span><span class=\"n\">_1</span><span class=\"o\">).</span><span class=\"n\">foreach</span><span class=\"o\">(</span><span class=\"n\">println</span><span class=\"o\">)</span>\n<span class=\"n\">分区数</span><span class=\"err\">：</span><span class=\"mi\">1</span>\n<span class=\"o\">(,</span><span class=\"mi\">0</span><span class=\"o\">)</span>\n\n<span class=\"o\">(</span><span class=\"n\">id</span>           <span class=\"n\">name</span>                <span class=\"n\">addr</span>            <span class=\"n\">time</span>                                      <span class=\"o\">,</span><span class=\"mi\">1</span><span class=\"o\">)</span>\n<span class=\"o\">(------------</span> <span class=\"o\">-------------------</span> <span class=\"o\">---------------</span> <span class=\"o\">--------------------,</span><span class=\"mi\">2</span><span class=\"o\">)</span>\n<span class=\"o\">(</span><span class=\"mi\">1</span>            <span class=\"n\">zhangsan</span>            <span class=\"n\">shanghai</span>        <span class=\"mi\">2018</span><span class=\"o\">-</span><span class=\"mi\">05</span><span class=\"o\">-</span><span class=\"mi\">25</span><span class=\"o\">,</span><span class=\"mi\">3</span><span class=\"o\">)</span>\n<span class=\"o\">(</span><span class=\"mi\">2</span>            <span class=\"n\">zhangsan</span>            <span class=\"n\">shanghai</span>        <span class=\"mi\">2018</span><span class=\"o\">-</span><span class=\"mi\">05</span><span class=\"o\">-</span><span class=\"mi\">25</span><span class=\"o\">,</span><span class=\"mi\">4</span><span class=\"o\">)</span>\n<span class=\"o\">(</span><span class=\"mi\">3</span>            <span class=\"n\">zhangsan</span>            <span class=\"n\">shanghai</span>        <span class=\"mi\">2018</span><span class=\"o\">-</span><span class=\"mi\">05</span><span class=\"o\">-</span><span class=\"mi\">25</span><span class=\"o\">,</span><span class=\"mi\">5</span><span class=\"o\">)</span>\n<span class=\"o\">(</span><span class=\"mi\">4</span>            <span class=\"n\">zhangsan</span>            <span class=\"n\">shanghai</span>        <span class=\"mi\">2018</span><span class=\"o\">-</span><span class=\"mi\">05</span><span class=\"o\">-</span><span class=\"mi\">25</span><span class=\"o\">,</span><span class=\"mi\">6</span><span class=\"o\">)</span>\n<span class=\"o\">(</span><span class=\"mi\">5</span>            <span class=\"n\">zhangsan</span>            <span class=\"n\">shanghai</span>        <span class=\"mi\">2018</span><span class=\"o\">-</span><span class=\"mi\">05</span><span class=\"o\">-</span><span class=\"mi\">25</span><span class=\"o\">,</span><span class=\"mi\">7</span><span class=\"o\">)</span>\n<span class=\"o\">**********</span><span class=\"n\">分割线</span><span class=\"o\">***********</span>\n<span class=\"mi\">1</span>            <span class=\"n\">zhangsan</span>            <span class=\"n\">shanghai</span>        <span class=\"mi\">2018</span><span class=\"o\">-</span><span class=\"mi\">05</span><span class=\"o\">-</span><span class=\"mi\">25</span>\n<span class=\"mi\">2</span>            <span class=\"n\">zhangsan</span>            <span class=\"n\">shanghai</span>        <span class=\"mi\">2018</span><span class=\"o\">-</span><span class=\"mi\">05</span><span class=\"o\">-</span><span class=\"mi\">25</span>\n<span class=\"mi\">3</span>            <span class=\"n\">zhangsan</span>            <span class=\"n\">shanghai</span>        <span class=\"mi\">2018</span><span class=\"o\">-</span><span class=\"mi\">05</span><span class=\"o\">-</span><span class=\"mi\">25</span>\n<span class=\"mi\">4</span>            <span class=\"n\">zhangsan</span>            <span class=\"n\">shanghai</span>        <span class=\"mi\">2018</span><span class=\"o\">-</span><span class=\"mi\">05</span><span class=\"o\">-</span><span class=\"mi\">25</span>\n<span class=\"mi\">5</span>            <span class=\"n\">zhangsan</span>            <span class=\"n\">shanghai</span>        <span class=\"mi\">2018</span><span class=\"o\">-</span><span class=\"mi\">05</span><span class=\"o\">-</span><span class=\"mi\">25</span></code></pre></div><p>将rdd的分区改为8（大于1即可）测试一下 将</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">val</span> <span class=\"n\">rdd</span> <span class=\"k\">=</span> <span class=\"n\">sc</span><span class=\"o\">.</span><span class=\"n\">textFile</span><span class=\"o\">(</span><span class=\"n\">path</span><span class=\"o\">)</span></code></pre></div><p>改为</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">val</span> <span class=\"n\">rdd</span> <span class=\"k\">=</span> <span class=\"n\">sc</span><span class=\"o\">.</span><span class=\"n\">textFile</span><span class=\"o\">(</span><span class=\"n\">path</span><span class=\"o\">,</span> <span class=\"mi\">8</span><span class=\"o\">)</span></code></pre></div><p>发现结果是一样的 因为我不太熟悉读取本地数据分区和读取hdfs数据分区的是否一样，所以将数据放在分布式的hdfs上测试一下 首先将data.txt上传的hdfs上</p><div class=\"highlight\"><pre><code class=\"language-vim\"><span class=\"nx\">hadoop</span> <span class=\"nx\">fs</span> <span class=\"p\">-</span><span class=\"nx\">put</span> <span class=\"nx\">data</span>.<span class=\"nx\">txt</span> <span class=\"sr\">/tmp/</span><span class=\"nx\">dkl</span>/</code></pre></div><p>将代码中的path改为</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">val</span> <span class=\"n\">path</span> <span class=\"k\">=</span> <span class=\"s\">&#34;hdfs://ambari.master.com:8020/tmp/dkl/data.txt&#34;</span></code></pre></div><p>发现最后的结果也是一样的（分区数可能不一样） 那么这样看来这个思路是可以解决这个问题的，但是我举得例子数据量比较少，在实际工作中数据量大的话，用zipWithIndex会有性能问题。</p><h2>2.2 思路2</h2><p>尝试直接获取rdd的前几行数据，然后过滤掉这几行数据，但是这个前提是前几行数据在rdd里是唯一的，否在会过滤掉其他行一样的数据，我的使用场景是删掉垃圾数据，如果其他行也有一样的数据，那么正好删掉了~</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">val</span> <span class=\"n\">path</span> <span class=\"k\">=</span> <span class=\"s\">&#34;files/data.txt&#34;</span>\n<span class=\"k\">val</span> <span class=\"n\">rdd</span> <span class=\"k\">=</span> <span class=\"n\">sc</span><span class=\"o\">.</span><span class=\"n\">textFile</span><span class=\"o\">(</span><span class=\"n\">path</span><span class=\"o\">,</span> <span class=\"mi\">8</span><span class=\"o\">)</span>\n<span class=\"n\">println</span><span class=\"o\">(</span><span class=\"s\">&#34;分区数：&#34;</span> <span class=\"o\">+</span> <span class=\"n\">rdd</span><span class=\"o\">.</span><span class=\"n\">getNumPartitions</span><span class=\"o\">)</span>\n<span class=\"c1\">//前三条\n</span><span class=\"c1\"></span><span class=\"k\">val</span> <span class=\"n\">arr</span> <span class=\"k\">=</span> <span class=\"n\">rdd</span><span class=\"o\">.</span><span class=\"n\">take</span><span class=\"o\">(</span><span class=\"mi\">3</span><span class=\"o\">)</span>\n<span class=\"c1\">//过滤掉arr里的数据\n</span><span class=\"c1\"></span><span class=\"k\">val</span> <span class=\"n\">rdd3</span> <span class=\"k\">=</span> <span class=\"n\">rdd</span><span class=\"o\">.</span><span class=\"n\">filter</span><span class=\"o\">(!</span><span class=\"n\">arr</span><span class=\"o\">.</span><span class=\"n\">contains</span><span class=\"o\">(</span><span class=\"k\">_</span><span class=\"o\">))</span>\n<span class=\"n\">rdd3</span><span class=\"o\">.</span><span class=\"n\">foreach</span><span class=\"o\">(</span><span class=\"n\">println</span><span class=\"o\">)</span></code></pre></div><p>结果</p><div class=\"highlight\"><pre><code class=\"language-text\">分区数：8\n1            zhangsan            shanghai        2018-05-25\n2            zhangsan            shanghai        2018-05-25\n3            zhangsan            shanghai        2018-05-25\n4            zhangsan            shanghai        2018-05-25\n5            zhangsan            shanghai        2018-05-25</code></pre></div><p>从结果看是可以解决我的问题，且和分区多少无关，大家可以试一下。</p><h2>2.3 关于rdd前几行的定义</h2><p>一开始我对于rdd前几行的数据的定义是有疑惑的，不知道改变rdd分区的数目，通过rdd.first或者rdd.take获取到的前几条数据是否是固定的，此次通过写代码测试，确定是和分区无关的，而且我担心测试数据量过小，将1.5G的txt放在hdfs测试，并且不指定分区数目（默认）进行测试，发现分区默认大小13，前几行数据依旧是固定，由此更加确认和rdd分区无关。</p><h2>2.4 关于rdd重新分区</h2><p>可通过repartition和coalesce对rdd进行重新分区，通过如下代码测试</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"n\">rdd</span><span class=\"o\">.</span><span class=\"n\">repartition</span><span class=\"o\">(</span><span class=\"mi\">3</span><span class=\"o\">).</span><span class=\"n\">take</span><span class=\"o\">(</span><span class=\"mi\">3</span><span class=\"o\">).</span><span class=\"n\">foreach</span><span class=\"o\">(</span><span class=\"n\">println</span><span class=\"o\">)</span>\n<span class=\"n\">rdd</span><span class=\"o\">.</span><span class=\"n\">coalesce</span><span class=\"o\">(</span><span class=\"mi\">3</span><span class=\"o\">).</span><span class=\"n\">take</span><span class=\"o\">(</span><span class=\"mi\">3</span><span class=\"o\">).</span><span class=\"n\">foreach</span><span class=\"o\">(</span><span class=\"n\">println</span><span class=\"o\">)</span></code></pre></div><p>通过结果得出coalesce之后顺序和之前的顺序是一样的，而repartition之后的顺序和之前不一样了，也就是如果rdd进行repartition之后的前几行和原来的前几行是不一样的，但是重新分区的数目固定的话，每次repartition之后的顺序是一样的。 注：repartition 内部实现调用的 coalesce 且为coalesce中  shuffle = true的实现 关于Spark中repartition和coalesce的使用场景，参考<a href=\"https://link.zhihu.com/?target=http%3A//www.lishiyu.cn/post/104.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Spark中repartition和coalesce的区别与使用场景解析</a> 因为我没有repartition的需求，所以可以通过2.2的代码解决我的问题，且如果有repartition的需求，可以在删掉前几行之后再repartition~</p><h2>3、通过Linux命令删除文件前几行</h2><p>命令如下：</p><div class=\"highlight\"><pre><code class=\"language-bash\">cat data.txt \n\nid           name                addr            <span class=\"nb\">time</span>                                      \n------------ ------------------- --------------- --------------------\n<span class=\"m\">1</span>            zhangsan            shanghai        <span class=\"m\">2018</span>-05-25\n<span class=\"m\">2</span>            zhangsan            shanghai        <span class=\"m\">2018</span>-05-25\n<span class=\"m\">3</span>            zhangsan            shanghai        <span class=\"m\">2018</span>-05-25\n<span class=\"m\">4</span>            zhangsan            shanghai        <span class=\"m\">2018</span>-05-25\n<span class=\"m\">5</span>            zhangsan            shanghai        <span class=\"m\">2018</span>-05-25\ntail -n+4 data.txt &gt; data_new.txt\ncat data_new.txt \n<span class=\"m\">1</span>            zhangsan            shanghai        <span class=\"m\">2018</span>-05-25\n<span class=\"m\">2</span>            zhangsan            shanghai        <span class=\"m\">2018</span>-05-25\n<span class=\"m\">3</span>            zhangsan            shanghai        <span class=\"m\">2018</span>-05-25\n<span class=\"m\">4</span>            zhangsan            shanghai        <span class=\"m\">2018</span>-05-25\n<span class=\"m\">5</span>            zhangsan            shanghai        <span class=\"m\">2018</span>-05-25\nmv data_new.txt data.txt \nmv：是否覆盖<span class=\"s2\">&#34;data.txt&#34;</span>？ y\ncat data.txt \n<span class=\"m\">1</span>            zhangsan            shanghai        <span class=\"m\">2018</span>-05-25\n<span class=\"m\">2</span>            zhangsan            shanghai        <span class=\"m\">2018</span>-05-25\n<span class=\"m\">3</span>            zhangsan            shanghai        <span class=\"m\">2018</span>-05-25\n<span class=\"m\">4</span>            zhangsan            shanghai        <span class=\"m\">2018</span>-05-25\n<span class=\"m\">5</span>            zhangsan            shanghai        <span class=\"m\">2018</span>-05-25</code></pre></div><p>注：其中的cat只是为了便于理解每个操作步骤之后的结果 这样就可以删除文件前三条数据了，之后压缩，上传到hdfs再用spark程序处理即可 参考：<a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/cugb1004101218/article/details/44022571\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">linux删除大文件的前n行</a></p><h2>4、最后</h2><p>不知道我对spark的分区的理解是否正确，如果不对的话，欢迎大家提出指正~</p><h2>附录</h2><p>最后附上将此格式的txt文件读取为rdd并转为df的示例程序</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">package</span> <span class=\"nn\">com.dkl.leanring.spark.sql</span>\n\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.Row</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.SparkSession</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.types.StringType</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.types.StructField</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.types.StructType</span>\n\n<span class=\"k\">object</span> <span class=\"nc\">Demo</span> <span class=\"o\">{</span>\n  <span class=\"k\">def</span> <span class=\"n\">main</span><span class=\"o\">(</span><span class=\"n\">args</span><span class=\"k\">:</span> <span class=\"kt\">Array</span><span class=\"o\">[</span><span class=\"kt\">String</span><span class=\"o\">])</span><span class=\"k\">:</span> <span class=\"kt\">Unit</span> <span class=\"o\">=</span> <span class=\"o\">{</span>\n    <span class=\"k\">val</span> <span class=\"n\">spark</span> <span class=\"k\">=</span> <span class=\"nc\">SparkSession</span><span class=\"o\">.</span><span class=\"n\">builder</span><span class=\"o\">().</span><span class=\"n\">appName</span><span class=\"o\">(</span><span class=\"s\">&#34;DelFirstNLines&#34;</span><span class=\"o\">).</span><span class=\"n\">master</span><span class=\"o\">(</span><span class=\"s\">&#34;local&#34;</span><span class=\"o\">).</span><span class=\"n\">getOrCreate</span><span class=\"o\">()</span>\n\n    <span class=\"k\">val</span> <span class=\"n\">sc</span> <span class=\"k\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">sparkContext</span>\n    <span class=\"k\">val</span> <span class=\"n\">path</span> <span class=\"k\">=</span> <span class=\"s\">&#34;files/data.txt&#34;</span>\n    <span class=\"k\">val</span> <span class=\"n\">data</span> <span class=\"k\">=</span> <span class=\"n\">sc</span><span class=\"o\">.</span><span class=\"n\">textFile</span><span class=\"o\">(</span><span class=\"n\">path</span><span class=\"o\">,</span> <span class=\"mi\">8</span><span class=\"o\">)</span>\n    <span class=\"k\">val</span> <span class=\"n\">arr</span> <span class=\"k\">=</span> <span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">take</span><span class=\"o\">(</span><span class=\"mi\">3</span><span class=\"o\">)</span>\n    <span class=\"k\">val</span> <span class=\"n\">rdd</span> <span class=\"k\">=</span> <span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">filter</span><span class=\"o\">(!</span><span class=\"n\">arr</span><span class=\"o\">.</span><span class=\"n\">contains</span><span class=\"o\">(</span><span class=\"k\">_</span><span class=\"o\">))</span>\n    <span class=\"c1\">//第二行为列名\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">colName</span> <span class=\"k\">=</span> <span class=\"n\">arr</span><span class=\"o\">(</span><span class=\"mi\">1</span><span class=\"o\">).</span><span class=\"n\">split</span><span class=\"o\">(</span><span class=\"s\">&#34; +&#34;</span><span class=\"o\">)</span> <span class=\"c1\">// +表示根据一个或多个空格进行分割\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">schema</span> <span class=\"k\">=</span> <span class=\"nc\">StructType</span><span class=\"o\">(</span><span class=\"n\">colName</span><span class=\"o\">.</span><span class=\"n\">map</span><span class=\"o\">(</span><span class=\"n\">fieldName</span> <span class=\"k\">=&gt;</span> <span class=\"nc\">StructField</span><span class=\"o\">(</span><span class=\"n\">fieldName</span><span class=\"o\">,</span> <span class=\"nc\">StringType</span><span class=\"o\">,</span> <span class=\"kc\">true</span><span class=\"o\">)))</span>\n    <span class=\"k\">val</span> <span class=\"n\">rowRDD</span> <span class=\"k\">=</span> <span class=\"n\">rdd</span><span class=\"o\">.</span><span class=\"n\">map</span><span class=\"o\">(</span><span class=\"k\">_</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"o\">(</span><span class=\"s\">&#34; +&#34;</span><span class=\"o\">)).</span><span class=\"n\">map</span><span class=\"o\">(</span><span class=\"n\">p</span> <span class=\"k\">=&gt;</span> <span class=\"nc\">Row</span><span class=\"o\">(</span><span class=\"n\">p</span><span class=\"k\">:</span> <span class=\"k\">_</span><span class=\"kt\">*</span><span class=\"o\">))</span>\n    <span class=\"k\">val</span> <span class=\"n\">df</span> <span class=\"k\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"o\">(</span><span class=\"n\">rowRDD</span><span class=\"o\">,</span> <span class=\"n\">schema</span><span class=\"o\">)</span>\n    <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"o\">()</span>\n\n    <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">stop</span><span class=\"o\">()</span>\n  <span class=\"o\">}</span>\n<span class=\"o\">}</span>\n<span class=\"o\">+---+--------+--------+----------+</span>\n<span class=\"o\">|</span> <span class=\"n\">id</span><span class=\"o\">|</span>    <span class=\"n\">name</span><span class=\"o\">|</span>    <span class=\"n\">addr</span><span class=\"o\">|</span>      <span class=\"n\">time</span><span class=\"o\">|</span>\n<span class=\"o\">+---+--------+--------+----------+</span>\n<span class=\"o\">|</span>  <span class=\"mi\">1</span><span class=\"o\">|</span><span class=\"n\">zhangsan</span><span class=\"o\">|</span><span class=\"n\">shanghai</span><span class=\"o\">|</span><span class=\"mi\">2018</span><span class=\"o\">-</span><span class=\"mi\">05</span><span class=\"o\">-</span><span class=\"mi\">25</span><span class=\"o\">|</span>\n<span class=\"o\">|</span>  <span class=\"mi\">2</span><span class=\"o\">|</span><span class=\"n\">zhangsan</span><span class=\"o\">|</span><span class=\"n\">shanghai</span><span class=\"o\">|</span><span class=\"mi\">2018</span><span class=\"o\">-</span><span class=\"mi\">05</span><span class=\"o\">-</span><span class=\"mi\">25</span><span class=\"o\">|</span>\n<span class=\"o\">|</span>  <span class=\"mi\">3</span><span class=\"o\">|</span><span class=\"n\">zhangsan</span><span class=\"o\">|</span><span class=\"n\">shanghai</span><span class=\"o\">|</span><span class=\"mi\">2018</span><span class=\"o\">-</span><span class=\"mi\">05</span><span class=\"o\">-</span><span class=\"mi\">25</span><span class=\"o\">|</span>\n<span class=\"o\">|</span>  <span class=\"mi\">4</span><span class=\"o\">|</span><span class=\"n\">zhangsan</span><span class=\"o\">|</span><span class=\"n\">shanghai</span><span class=\"o\">|</span><span class=\"mi\">2018</span><span class=\"o\">-</span><span class=\"mi\">05</span><span class=\"o\">-</span><span class=\"mi\">25</span><span class=\"o\">|</span>\n<span class=\"o\">|</span>  <span class=\"mi\">5</span><span class=\"o\">|</span><span class=\"n\">zhangsan</span><span class=\"o\">|</span><span class=\"n\">shanghai</span><span class=\"o\">|</span><span class=\"mi\">2018</span><span class=\"o\">-</span><span class=\"mi\">05</span><span class=\"o\">-</span><span class=\"mi\">25</span><span class=\"o\">|</span>\n<span class=\"o\">+---+--------+--------+----------+</span></code></pre></div><p></p>", 
            "topic": [
                {
                    "tag": "Spark", 
                    "tagLink": "https://api.zhihu.com/topics/19942170"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/45229241", 
            "userName": "董可伦", 
            "userLink": "https://www.zhihu.com/people/de888cc734ddbfd918269a8e5fb196ba", 
            "upvote": 3, 
            "title": "利用ogg实现oracle到kafka的增量数据实时同步", 
            "content": "<p>我的原创地址：</p><a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/05/23/oggOracle2Kafka/\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-6c6b85c05445eb4fb16801c026ccf0e4_180x120.jpg\" data-image-width=\"1858\" data-image-height=\"510\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">利用ogg实现oracle到kafka的增量数据实时同步</a><h2>前言</h2><p>ogg即Oracle GoldenGate是Oracle的同步工具，本文讲如何配置ogg以实现Oracle数据库增量数据实时同步到kafka中，其中同步消息格式为json。 下面是我的源端和目标端的一些配置信息：</p><ul><li>| 版本 | OGG版本| ip | 别名</li><li>| :-:  | :-:  | :-: | :-:  源端 | OracleRelease 11.2.0.1.0 | Oracle GoldenGate 11.2.1.0.3 for Oracle on Linux x86-64   | 192.168.44.128 | master 目标端 | kafka_2.11-1.1.0 | Oracle GoldenGate for Big Data 12.3.1.1.1 on Linux x86-64 | 192.168.44.129 | slave1</li></ul><h2>1、下载</h2><p>可在<a href=\"https://link.zhihu.com/?target=http%3A//www.oracle.com/technetwork/middleware/goldengate/downloads/index.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">这里</a>或<a href=\"https://link.zhihu.com/?target=https%3A//edelivery.oracle.com/osdc/faces/SoftwareDelivery\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">旧版本</a>查询下载 注意：源端和目标端的文件不一样，目标端需要下载Oracle GoldenGate for Big Data,源端需要下载Oracle GoldenGate for Oracle具体下载方法见最后的附录截图。</p><h2>2、源端（Oracle）配置</h2><p>注意：源端是安装了oracle的机器，oracle环境变量之前都配置好了</p><h2>2.1 解压</h2><p>先建立ogg目录</p><div class=\"highlight\"><pre><code class=\"language-bash\">mkdir -p /opt/ogg\nunzip V34339-01.zip</code></pre></div><p>解压后得到一个tar包，再解压这个tar</p><div class=\"highlight\"><pre><code class=\"language-bash\">tar xf fbo_ggs_Linux_x64_ora11g_64bit.tar -C /opt/ogg\nchown -R oracle:oinstall /opt/ogg （使oracle用户有ogg的权限，后面有些需要在oracle用户下执行才能成功）</code></pre></div><h2>2.2 配置ogg环境变量</h2><p>为了简单方便起见，我在/etc/profile里配置的，建议在生产中配置oracle的环境变量文件/home/oracle/.bash_profile里配置，为了怕出问题，我把OGG_HOME等环境变量在/etc/profile配置了一份，不知道这是否是必须的。</p><div class=\"highlight\"><pre><code class=\"language-bash\">vim /etc/profile\n<span class=\"nb\">export</span> <span class=\"nv\">OGG_HOME</span><span class=\"o\">=</span>/opt/ogg\n<span class=\"nb\">export</span> <span class=\"nv\">LD_LIBRARY_PATH</span><span class=\"o\">=</span><span class=\"nv\">$ORACLE_HOME</span>/lib:/usr/lib\n<span class=\"nb\">export</span> <span class=\"nv\">PATH</span><span class=\"o\">=</span><span class=\"nv\">$OGG_HOME</span>:<span class=\"nv\">$PATH</span></code></pre></div><p>使之生效</p><div class=\"highlight\"><pre><code class=\"language-bash\"><span class=\"nb\">source</span> /etc/profile</code></pre></div><p>测试一下ogg命令</p><div class=\"highlight\"><pre><code class=\"language-text\">ggsci</code></pre></div><p>如果命令成功即可进行下一步，不成功请检查前面的步骤。</p><h2>2.3 oracle打开归档模式</h2><div class=\"highlight\"><pre><code class=\"language-text\">su - oracle\nsqlplus / as sysdba</code></pre></div><p>执行下面的命令查看当前是否为归档模式</p><div class=\"highlight\"><pre><code class=\"language-sql\"><span class=\"n\">archive</span> <span class=\"n\">log</span> <span class=\"n\">list</span>\n<span class=\"k\">SQL</span><span class=\"o\">&gt;</span> <span class=\"n\">archive</span> <span class=\"n\">log</span> <span class=\"n\">list</span> \n<span class=\"k\">Database</span> <span class=\"n\">log</span> <span class=\"k\">mode</span>          <span class=\"k\">No</span> <span class=\"n\">Archive</span> <span class=\"k\">Mode</span>\n<span class=\"n\">Automatic</span> <span class=\"n\">archival</span>         <span class=\"n\">Disabled</span>\n<span class=\"n\">Archive</span> <span class=\"n\">destination</span>        <span class=\"n\">USE_DB_RECOVERY_FILE_DEST</span>\n<span class=\"n\">Oldest</span> <span class=\"n\">online</span> <span class=\"n\">log</span> <span class=\"n\">sequence</span>     <span class=\"mi\">12</span>\n<span class=\"k\">Current</span> <span class=\"n\">log</span> <span class=\"n\">sequence</span>           <span class=\"mi\">14</span></code></pre></div><p>若为Disabled，手动打开即可</p><div class=\"highlight\"><pre><code class=\"language-sql\"><span class=\"n\">conn</span> <span class=\"o\">/</span> <span class=\"k\">as</span> <span class=\"n\">sysdba</span> <span class=\"p\">(</span><span class=\"err\">以</span><span class=\"n\">DBA身份连接数据库</span><span class=\"p\">)</span> \n<span class=\"n\">shutdown</span> <span class=\"k\">immediate</span> <span class=\"p\">(</span><span class=\"err\">立即关闭数据库</span><span class=\"p\">)</span>\n<span class=\"n\">startup</span> <span class=\"n\">mount</span> <span class=\"p\">(</span><span class=\"err\">启动实例并加载数据库，但不打开</span><span class=\"p\">)</span>\n<span class=\"k\">alter</span> <span class=\"k\">database</span> <span class=\"n\">archivelog</span><span class=\"p\">;</span> <span class=\"p\">(</span><span class=\"err\">更改数据库为归档模式</span><span class=\"p\">)</span>\n<span class=\"k\">alter</span> <span class=\"k\">database</span> <span class=\"k\">open</span><span class=\"p\">;</span> <span class=\"p\">(</span><span class=\"err\">打开数据库</span><span class=\"p\">)</span>\n<span class=\"k\">alter</span> <span class=\"k\">system</span> <span class=\"n\">archive</span> <span class=\"n\">log</span> <span class=\"k\">start</span><span class=\"p\">;</span> <span class=\"p\">(</span><span class=\"err\">启用自动归档</span><span class=\"p\">)</span></code></pre></div><p>再执行一下</p><div class=\"highlight\"><pre><code class=\"language-sql\"><span class=\"n\">archive</span> <span class=\"n\">log</span> <span class=\"n\">list</span>\n<span class=\"k\">Database</span> <span class=\"n\">log</span> <span class=\"k\">mode</span>          <span class=\"n\">Archive</span> <span class=\"k\">Mode</span>\n<span class=\"n\">Automatic</span> <span class=\"n\">archival</span>         <span class=\"n\">Enabled</span>\n<span class=\"n\">Archive</span> <span class=\"n\">destination</span>        <span class=\"n\">USE_DB_RECOVERY_FILE_DEST</span>\n<span class=\"n\">Oldest</span> <span class=\"n\">online</span> <span class=\"n\">log</span> <span class=\"n\">sequence</span>     <span class=\"mi\">12</span>\n<span class=\"k\">Next</span> <span class=\"n\">log</span> <span class=\"n\">sequence</span> <span class=\"k\">to</span> <span class=\"n\">archive</span>   <span class=\"mi\">14</span>\n<span class=\"k\">Current</span> <span class=\"n\">log</span> <span class=\"n\">sequence</span>           <span class=\"mi\">14</span></code></pre></div><p>可以看到为Enabled，则成功打开归档模式。</p><h2>2.4 Oracle打开日志相关</h2><p>OGG基于辅助日志等进行实时传输，故需要打开相关日志确保可获取事务内容，通过下面的命令查看该状态</p><div class=\"highlight\"><pre><code class=\"language-sql\"><span class=\"k\">select</span> <span class=\"n\">force_logging</span><span class=\"p\">,</span> <span class=\"n\">supplemental_log_data_min</span> <span class=\"k\">from</span> <span class=\"n\">v$database</span><span class=\"p\">;</span>\n<span class=\"n\">FORCE_</span> <span class=\"n\">SUPPLEMENTAL_LOG</span>\n<span class=\"c1\">------ ----------------\n</span><span class=\"c1\"></span><span class=\"k\">NO</span>     <span class=\"k\">NO</span></code></pre></div><p>若为NO，则需要通过命令修改</p><div class=\"highlight\"><pre><code class=\"language-text\">alter database force logging;\nalter database add supplemental log data;</code></pre></div><p>再查看一下为YES即可</p><div class=\"highlight\"><pre><code class=\"language-sql\"><span class=\"k\">SQL</span><span class=\"o\">&gt;</span> <span class=\"k\">select</span> <span class=\"n\">force_logging</span><span class=\"p\">,</span> <span class=\"n\">supplemental_log_data_min</span> <span class=\"k\">from</span> <span class=\"n\">v$database</span><span class=\"p\">;</span>\n\n<span class=\"n\">FORCE_</span> <span class=\"n\">SUPPLEMENTAL_LOG</span>\n<span class=\"c1\">------ ----------------\n</span><span class=\"c1\"></span><span class=\"n\">YES</span>    <span class=\"n\">YES</span></code></pre></div><h2>2.5 oracle创建复制用户</h2><p>首先root用户建立相关文件夹，并赋予权限</p><div class=\"highlight\"><pre><code class=\"language-text\">mkdir -p /u01/app/oracle/oggdata/orcl\nchown -R oracle:oinstall /u01/app/oracle/oggdata/orcl</code></pre></div><p>然后执行下面sql</p><div class=\"highlight\"><pre><code class=\"language-sql\"><span class=\"k\">SQL</span><span class=\"o\">&gt;</span> <span class=\"k\">create</span> <span class=\"n\">tablespace</span> <span class=\"n\">oggtbs</span> <span class=\"n\">datafile</span> <span class=\"s1\">&#39;/u01/app/oracle/oggdata/orcl/oggtbs01.dbf&#39;</span> <span class=\"k\">size</span> <span class=\"mi\">1000</span><span class=\"n\">M</span> <span class=\"n\">autoextend</span> <span class=\"k\">on</span><span class=\"p\">;</span>\n\n<span class=\"n\">Tablespace</span> <span class=\"n\">created</span><span class=\"p\">.</span>\n\n<span class=\"k\">SQL</span><span class=\"o\">&gt;</span>  <span class=\"k\">create</span> <span class=\"k\">user</span> <span class=\"n\">ogg</span> <span class=\"n\">identified</span> <span class=\"k\">by</span> <span class=\"n\">ogg</span> <span class=\"k\">default</span> <span class=\"n\">tablespace</span> <span class=\"n\">oggtbs</span><span class=\"p\">;</span>\n\n<span class=\"k\">User</span> <span class=\"n\">created</span><span class=\"p\">.</span>\n\n<span class=\"k\">SQL</span><span class=\"o\">&gt;</span> <span class=\"k\">grant</span> <span class=\"n\">dba</span> <span class=\"k\">to</span> <span class=\"n\">ogg</span><span class=\"p\">;</span>\n\n<span class=\"k\">Grant</span> <span class=\"n\">succeeded</span><span class=\"p\">.</span></code></pre></div><h2>2.6 OGG初始化</h2><div class=\"highlight\"><pre><code class=\"language-text\">ggsci\ncreate subdirs\nggsci\n\nOracle GoldenGate Command Interpreter for Oracle\nVersion 11.2.1.0.3 14400833 OGGCORE_11.2.1.0.3_PLATFORMS_120823.1258_FBO\nLinux, x64, 64bit (optimized), Oracle 11g on Aug 23 2012 20:20:21\n\nCopyright (C) 1995, 2012, Oracle and/or its affiliates. All rights reserved.\n\n\n\nGGSCI (ambari.master.com) 1&gt; create subdirs\n\nCreating subdirectories under current directory /root\n\nParameter files                /root/dirprm: created\nReport files                   /root/dirrpt: created\nCheckpoint files               /root/dirchk: created\nProcess status files           /root/dirpcs: created\nSQL script files               /root/dirsql: created\nDatabase definitions files     /root/dirdef: created\nExtract data files             /root/dirdat: created\nTemporary files                /root/dirtmp: created\nStdout files                   /root/dirout: created\n\n\nGGSCI (ambari.master.com) 2&gt;</code></pre></div><h2>2.7 Oracle创建测试表</h2><p>创建一个用户,在该用户下新建测试表，用户名、密码、表名均为 test_ogg。</p><div class=\"highlight\"><pre><code class=\"language-sql\"><span class=\"k\">create</span> <span class=\"k\">user</span> <span class=\"n\">test_ogg</span>  <span class=\"n\">identified</span> <span class=\"k\">by</span> <span class=\"n\">test_ogg</span> <span class=\"k\">default</span> <span class=\"n\">tablespace</span> <span class=\"n\">users</span><span class=\"p\">;</span>\n<span class=\"k\">grant</span> <span class=\"n\">dba</span> <span class=\"k\">to</span> <span class=\"n\">test_ogg</span><span class=\"p\">;</span>\n<span class=\"n\">conn</span> <span class=\"n\">test_ogg</span><span class=\"o\">/</span><span class=\"n\">test_ogg</span><span class=\"p\">;</span>\n<span class=\"k\">create</span> <span class=\"k\">table</span> <span class=\"n\">test_ogg</span><span class=\"p\">(</span><span class=\"n\">id</span> <span class=\"nb\">int</span> <span class=\"p\">,</span><span class=\"n\">name</span> <span class=\"nb\">varchar</span><span class=\"p\">(</span><span class=\"mi\">20</span><span class=\"p\">),</span><span class=\"k\">primary</span> <span class=\"k\">key</span><span class=\"p\">(</span><span class=\"n\">id</span><span class=\"p\">));</span></code></pre></div><h2>3 目标端（kafka）配置</h2><div class=\"highlight\"><pre><code class=\"language-bash\">mkdir -p /opt/ogg\nunzip 123111_ggs_Adapters_Linux_x64.zip \ntar xf ggs_Adapters_Linux_x64.tar  -C /opt/ogg/</code></pre></div><h2>3.2 环境变量</h2><div class=\"highlight\"><pre><code class=\"language-bash\">vim /etc/profile\n<span class=\"nb\">export</span> <span class=\"nv\">OGG_HOME</span><span class=\"o\">=</span>/opt/ogg\n<span class=\"nb\">export</span> <span class=\"nv\">LD_LIBRARY_PATH</span><span class=\"o\">=</span><span class=\"nv\">$JAVA_HOME</span>/jre/lib/amd64:<span class=\"nv\">$JAVA_HOME</span>/jre/lib/amd64/server:<span class=\"nv\">$JAVA_HOME</span>/jre/lib/amd64/libjsig.so:<span class=\"nv\">$JAVA_HOME</span>/jre/lib/amd64/server/libjvm.so:<span class=\"nv\">$OGG_HOME</span>/lib\n<span class=\"nb\">export</span> <span class=\"nv\">PATH</span><span class=\"o\">=</span><span class=\"nv\">$OGG_HOME</span>:<span class=\"nv\">$PATH</span>\n<span class=\"nb\">source</span> /etc/profile</code></pre></div><p>同样测试一下ogg命令</p><div class=\"highlight\"><pre><code class=\"language-text\">ggsci</code></pre></div><h2>3.3 初始化目录</h2><div class=\"highlight\"><pre><code class=\"language-text\">create subdirs</code></pre></div><h2>4、OGG源端配置</h2><h2>4.1 配置OGG的全局变量</h2><p>先切换到oracle用户下</p><div class=\"highlight\"><pre><code class=\"language-bash\">su oracle\n<span class=\"nb\">cd</span> /opt/ogg\nggsci\nGGSCI <span class=\"o\">(</span>ambari.master.com<span class=\"o\">)</span> <span class=\"m\">1</span>&gt; dblogin userid ogg password ogg\nSuccessfully logged into database.\n\nGGSCI <span class=\"o\">(</span>ambari.master.com<span class=\"o\">)</span> <span class=\"m\">2</span>&gt; edit param ./globals</code></pre></div><p>然后和用vim编辑一样添加</p><div class=\"highlight\"><pre><code class=\"language-vim\"><span class=\"nx\">oggschema</span> <span class=\"nx\">ogg</span></code></pre></div><h2>4.2 配置管理器mgr</h2><div class=\"highlight\"><pre><code class=\"language-text\">GGSCI (ambari.master.com) 3&gt; edit param mgr\nPORT 7809\nDYNAMICPORTLIST 7810-7909\nAUTORESTART EXTRACT *,RETRIES 5,WAITMINUTES 3\nPURGEOLDEXTRACTS ./dirdat/*,usecheckpoints, minkeepdays 3</code></pre></div><p>说明：PORT即mgr的默认监听端口；DYNAMICPORTLIST动态端口列表，当指定的mgr端口不可用时，会在这个端口列表中选择一个，最大指定范围为256个；AUTORESTART重启参数设置表示重启所有EXTRACT进程，最多5次，每次间隔3分钟；PURGEOLDEXTRACTS即TRAIL文件的定期清理</p><h2>4.3 添加复制表</h2><div class=\"highlight\"><pre><code class=\"language-text\">GGSCI (ambari.master.com) 4&gt; add trandata test_ogg.test_ogg\n\nLogging of supplemental redo data enabled for table TEST_OGG.TEST_OGG.\n\nGGSCI (ambari.master.com) 5&gt; info trandata test_ogg.test_ogg\n\nLogging of supplemental redo log data is enabled for table TEST_OGG.TEST_OGG.\n\nColumns supplementally logged for table TEST_OGG.TEST_OGG: ID</code></pre></div><h2>4.4 配置extract进程</h2><div class=\"highlight\"><pre><code class=\"language-text\">GGSCI (ambari.master.com) 6&gt; edit param extkafka\nextract extkafka\ndynamicresolution\nSETENV (ORACLE_SID = &#34;orcl&#34;)\nSETENV (NLS_LANG = &#34;american_america.AL32UTF8&#34;)\nuserid ogg,password ogg\nexttrail /opt/ogg/dirdat/to\ntable test_ogg.test_ogg;</code></pre></div><p>说明：第一行指定extract进程名称；dynamicresolution动态解析；SETENV设置环境变量，这里分别设置了Oracle数据库以及字符集；userid ggs,password ggs即OGG连接Oracle数据库的帐号密码，这里使用2.5中特意创建的复制帐号；exttrail定义trail文件的保存位置以及文件名，注意这里文件名只能是2个字母，其余部分OGG会补齐；table即复制表的表名，支持*通配，必须以;结尾</p><p>添加extract进程：</p><div class=\"highlight\"><pre><code class=\"language-text\">GGSCI (ambari.master.com) 16&gt; add extract extkafka,tranlog,begin now\nEXTRACT added.</code></pre></div><p>(注：若报错</p><div class=\"highlight\"><pre><code class=\"language-text\">ERROR: Could not create checkpoint file /opt/ogg/dirchk/EXTKAFKA.cpe (error 2, No such file or directory).</code></pre></div><p>执行下面的命令再重新添加即可。</p><div class=\"highlight\"><pre><code class=\"language-text\">create subdirs</code></pre></div><p>)</p><p>添加trail文件的定义与extract进程绑定：</p><div class=\"highlight\"><pre><code class=\"language-text\">GGSCI (ambari.master.com) 17&gt; add exttrail /opt/ogg/dirdat/to,extract extkafka\nEXTTRAIL added.</code></pre></div><h2>4.5 配置pump进程</h2><p>pump进程本质上来说也是一个extract，只不过他的作用仅仅是把trail文件传递到目标端，配置过程和extract进程类似，只是逻辑上称之为pump进程</p><div class=\"highlight\"><pre><code class=\"language-text\">GGSCI (ambari.master.com) 18&gt; edit param pukafka\nextract pukafka\npassthru\ndynamicresolution\nuserid ogg,password ogg\nrmthost 192.168.44.129 mgrport 7809\nrmttrail /opt/ogg/dirdat/to\ntable test_ogg.test_ogg;</code></pre></div><p>说明：第一行指定extract进程名称；passthru即禁止OGG与Oracle交互，我们这里使用pump逻辑传输，故禁止即可；dynamicresolution动态解析；userid ogg,password ogg即OGG连接Oracle数据库的帐号密码rmthost和mgrhost即目标端(kafka)OGG的mgr服务的地址以及监听端口；rmttrail即目标端trail文件存储位置以及名称。</p><p>分别将本地trail文件和目标端的trail文件绑定到extract进程：</p><div class=\"highlight\"><pre><code class=\"language-bash\">GGSCI <span class=\"o\">(</span>ambari.master.com<span class=\"o\">)</span> <span class=\"m\">1</span>&gt; add extract pukafka,exttrailsource /opt/ogg/dirdat/to\nEXTRACT added.\nGGSCI <span class=\"o\">(</span>ambari.master.com<span class=\"o\">)</span> <span class=\"m\">2</span>&gt; add rmttrail /opt/ogg/dirdat/to,extract pukafka\nRMTTRAIL added.</code></pre></div><h2>4.6 配置define文件</h2><p>Oracle与MySQL，Hadoop集群（HDFS，Hive，kafka等）等之间数据传输可以定义为异构数据类型的传输，故需要定义表之间的关系映射，在OGG命令行执行：</p><div class=\"highlight\"><pre><code class=\"language-text\">GGSCI (ambari.master.com) 3&gt; edit param test_ogg\ndefsfile /opt/ogg/dirdef/test_ogg.test_ogg\nuserid ogg,password ogg\ntable test_ogg.test_ogg;</code></pre></div><p>在OGG主目录下执行(oracle用户)：</p><div class=\"highlight\"><pre><code class=\"language-bash\">./defgen paramfile dirprm/test_ogg.prm\n\n***********************************************************************\n        Oracle GoldenGate Table Definition Generator <span class=\"k\">for</span> Oracle\n Version <span class=\"m\">11</span>.2.1.0.3 <span class=\"m\">14400833</span> OGGCORE_11.2.1.0.3_PLATFORMS_120823.1258\n   Linux, x64, 64bit <span class=\"o\">(</span>optimized<span class=\"o\">)</span>, Oracle 11g on Aug <span class=\"m\">23</span> <span class=\"m\">2012</span> <span class=\"m\">16</span>:58:29\n\nCopyright <span class=\"o\">(</span>C<span class=\"o\">)</span> <span class=\"m\">1995</span>, <span class=\"m\">2012</span>, Oracle and/or its affiliates. All rights reserved.\n\n\n                    Starting at <span class=\"m\">2018</span>-05-23 <span class=\"m\">05</span>:03:04\n***********************************************************************\n\nOperating System Version:\nLinux\nVersion <span class=\"c1\">#1 SMP Wed Apr 12 15:04:24 UTC 2017, Release 3.10.0-514.16.1.el7.x86_64</span>\nNode: ambari.master.com\nMachine: x86_64\n                         soft limit   hard limit\nAddress Space Size   :    unlimited    unlimited\nHeap Size            :    unlimited    unlimited\nFile Size            :    unlimited    unlimited\nCPU Time             :    unlimited    unlimited\n\nProcess id: <span class=\"m\">13126</span>\n\n***********************************************************************\n**            Running with the following parameters                  **\n***********************************************************************\ndefsfile /opt/ogg/dirdef/test_ogg.test_ogg\nuserid ogg,password ***\ntable test_ogg.test_ogg<span class=\"p\">;</span>\nRetrieving definition <span class=\"k\">for</span> TEST_OGG.TEST_OGG\n\n\n\nDefinitions generated <span class=\"k\">for</span> <span class=\"m\">1</span> table in /opt/ogg/dirdef/test_ogg.test_ogg</code></pre></div><p>将生成的/opt/ogg/dirdef/test_ogg.test_ogg发送的目标端ogg目录下的dirdef里：</p><div class=\"highlight\"><pre><code class=\"language-text\">scp -r /opt/ogg/dirdef/test_ogg.test_ogg root@slave1:/opt/ogg/dirdef/</code></pre></div><h2>5、OGG目标端配置</h2><h2>5.1 开启kafka服务</h2><div class=\"highlight\"><pre><code class=\"language-text\">cd /opt/kafka_2.11-1.1.0/\nbin/zookeeper-server-start.sh config/zookeeper.properties\nbin/kafka-server-start.sh config/server.properties</code></pre></div><h2>5.2 配置管理器mgr</h2><div class=\"highlight\"><pre><code class=\"language-text\">GGSCI (ambari.slave1.com) 1&gt;  edit param mgr\nPORT 7809\nDYNAMICPORTLIST 7810-7909\nAUTORESTART EXTRACT *,RETRIES 5,WAITMINUTES 3\nPURGEOLDEXTRACTS ./dirdat/*,usecheckpoints, minkeepdays 3</code></pre></div><h2>5.3 配置checkpoint</h2><p>checkpoint即复制可追溯的一个偏移量记录，在全局配置里添加checkpoint表即可。</p><div class=\"highlight\"><pre><code class=\"language-text\">edit  param  ./GLOBALS\nCHECKPOINTTABLE test_ogg.checkpoint</code></pre></div><h2>5.4 配置replicate进程</h2><div class=\"highlight\"><pre><code class=\"language-text\">GGSCI (ambari.slave1.com) 4&gt; edit param rekafka\nREPLICAT rekafka\nsourcedefs /opt/ogg/dirdef/test_ogg.test_ogg\nTARGETDB LIBFILE libggjava.so SET property=dirprm/kafka.props\nREPORTCOUNT EVERY 1 MINUTES, RATE \nGROUPTRANSOPS 10000\nMAP test_ogg.test_ogg, TARGET test_ogg.test_ogg;</code></pre></div><p>说明：REPLICATE rekafka定义rep进程名称；sourcedefs即在4.6中在源服务器上做的表映射文件；TARGETDB LIBFILE即定义kafka一些适配性的库文件以及配置文件，配置文件位于OGG主目录下的dirprm/kafka.props；REPORTCOUNT即复制任务的报告生成频率；GROUPTRANSOPS为以事务传输时，事务合并的单位，减少IO操作；MAP即源端与目标端的映射关系</p><h2>5.5 配置kafka.props</h2><div class=\"highlight\"><pre><code class=\"language-bash\"><span class=\"nb\">cd</span> /opt/ogg/dirprm/\nvim kafka.props\ngg.handlerlist<span class=\"o\">=</span>kafkahandler //handler类型\ngg.handler.kafkahandler.type<span class=\"o\">=</span>kafka\ngg.handler.kafkahandler.KafkaProducerConfigFile<span class=\"o\">=</span>custom_kafka_producer.properties //kafka相关配置\ngg.handler.kafkahandler.topicMappingTemplate<span class=\"o\">=</span>test_ogg //kafka的topic名称，无需手动创建\ngg.handler.kafkahandler.format<span class=\"o\">=</span>json //传输文件的格式，支持json，xml等\ngg.handler.kafkahandler.mode<span class=\"o\">=</span>op  //OGG <span class=\"k\">for</span> Big Data中传输模式，即op为一次SQL传输一次，tx为一次事务传输一次\ngg.classpath<span class=\"o\">=</span>dirprm/:/opt/kafka_2.11-1.1.0/libs/*:/opt/ogg/:/opt/ogg/lib/*\nvim custom_kafka_producer.properties\nbootstrap.servers<span class=\"o\">=</span><span class=\"m\">192</span>.168.44.129:9092 //kafkabroker的地址\n<span class=\"nv\">acks</span><span class=\"o\">=</span><span class=\"m\">1</span>\ncompression.type<span class=\"o\">=</span>gzip //压缩类型\nreconnect.backoff.ms<span class=\"o\">=</span><span class=\"m\">1000</span> //重连延时\nvalue.serializer<span class=\"o\">=</span>org.apache.kafka.common.serialization.ByteArraySerializer\nkey.serializer<span class=\"o\">=</span>org.apache.kafka.common.serialization.ByteArraySerializer\nbatch.size<span class=\"o\">=</span><span class=\"m\">102400</span>\nlinger.ms<span class=\"o\">=</span><span class=\"m\">10000</span></code></pre></div><p>其中需要将后面的注释去掉，ogg不识别注释，如果不去掉会报错</p><h2>5.6 添加trail文件到replicate进程</h2><div class=\"highlight\"><pre><code class=\"language-text\">GGSCI (ambari.slave1.com) 2&gt; add replicat rekafka exttrail /opt/ogg/dirdat/to,checkpointtable test_ogg.checkpoint\nREPLICAT added.</code></pre></div><h2>6、测试</h2><h2>6.1 启动所有进程</h2><p>在源端和目标端的OGG命令行下使用start [进程名]的形式启动所有进程。 启动顺序按照源mgr——目标mgr——源extract——源pump——目标replicate来完成。 全部需要在ogg目录下执行ggsci目录进入ogg命令行。 源端依次是</p><div class=\"highlight\"><pre><code class=\"language-text\">start mgr\nstart extkafka\nstart pukafka</code></pre></div><p>目标端</p><div class=\"highlight\"><pre><code class=\"language-text\">start mgr\nstart rekafka</code></pre></div><p>可以通过info all 或者info [进程名] 查看状态，所有的进程都为RUNNING才算成功 源端</p><div class=\"highlight\"><pre><code class=\"language-text\">GGSCI (ambari.master.com) 5&gt; info all\n\nProgram     Status      Group       Lag at Chkpt  Time Since Chkpt\n\nMANAGER     RUNNING                                           \nEXTRACT     RUNNING     EXTKAFKA    04:50:21      00:00:03    \nEXTRACT     RUNNING     PUKAFKA     00:00:00      00:00:03</code></pre></div><p>目标端</p><div class=\"highlight\"><pre><code class=\"language-text\">GGSCI (ambari.slave1.com) 3&gt; info all\n\nProgram     Status      Group       Lag at Chkpt  Time Since Chkpt\n\nMANAGER     RUNNING                                           \nREPLICAT    RUNNING     REKAFKA     00:00:00      00:00:01</code></pre></div><h2>6.2 异常解决</h2><p>如果有不是RUNNING可通过查看日志的方法检查解决问题，具体通过下面两种方法</p><div class=\"highlight\"><pre><code class=\"language-bash\">vim ggser.log</code></pre></div><p>或者ogg命令行,以rekafka进程为例</p><div class=\"highlight\"><pre><code class=\"language-text\">GGSCI (ambari.slave1.com) 2&gt; view report rekafka</code></pre></div><p>列举其中我遇到的一个问题： 异常信息</p><div class=\"highlight\"><pre><code class=\"language-text\">SEVERE: Unable to set property on handler &#39;kafkahandler&#39; (oracle.goldengate.handler.kafka.KafkaHandler). Failed to set property: TopicName:=&#34;test_ogg&#34; (class: oracle.goldengate.handler.kafka.KafkaHandler).\noracle.goldengate.util.ConfigException: Failed to set property: TopicName:=&#34;test_ogg&#34; (class: oracle.goldengate.handler.kafka.KafkaHandler).\nat ......</code></pre></div><p>具体原因是网上的教程是旧版的，设置topicName的属性为:</p><div class=\"highlight\"><pre><code class=\"language-text\">gg.handler.kafkahandler.topicName=test_ogg</code></pre></div><p>新版的这样设置</p><div class=\"highlight\"><pre><code class=\"language-text\">gg.handler.kafkahandler.topicMappingTemplate=test_ogg</code></pre></div><p>大家可根据自己的版本进行设置，附上stackoverflow原答案</p><div class=\"highlight\"><pre><code class=\"language-text\">I tried to move data from Oracle Database to Kafka using Golden gate adapter Version 12.3.0.1.0\n\nIn new version there is no topicname\n\nThe following resolves the topic name using the short table name\ngg.handler.kafkahandler.topicMappingTemplate=test\n\nIn previous version we have gg.handler.kafkahandler.topicName=test</code></pre></div><h2>6.3 测试同步更新效果</h2><p>现在源端执行sql语句</p><div class=\"highlight\"><pre><code class=\"language-text\">conn test_ogg/test_ogg\ninsert into test_ogg values(1,&#39;test&#39;);\ncommit;\nupdate test_ogg set name=&#39;zhangsan&#39; where id=1;\ncommit;\ndelete test_ogg where id=1;\ncommit;</code></pre></div><p>查看源端trail文件状态</p><div class=\"highlight\"><pre><code class=\"language-bash\">ls -l /opt/ogg/dirdat/to*\n-rw-rw-rw- <span class=\"m\">1</span> oracle oinstall <span class=\"m\">1464</span> May <span class=\"m\">23</span> <span class=\"m\">10</span>:31 /opt/ogg/dirdat/to000000</code></pre></div><p>查看目标端trail文件状态</p><div class=\"highlight\"><pre><code class=\"language-bash\">ls -l /opt/ogg/dirdat/to*\n-rw-r----- <span class=\"m\">1</span> root root <span class=\"m\">1504</span> May <span class=\"m\">23</span> <span class=\"m\">10</span>:31 /opt/ogg/dirdat/to000000</code></pre></div><p>查看kafka是否自动建立对应的主题</p><div class=\"highlight\"><pre><code class=\"language-text\">bin/kafka-topics.sh --list --zookeeper localhost:2181</code></pre></div><p>在列表中显示有test_ogg则表示没问题 通过消费者看是否有同步消息</p><div class=\"highlight\"><pre><code class=\"language-text\">bin/kafka-console-consumer.sh --bootstrap-server 192.168.44.129:9092 --topic test_ogg --from-beginning\n{&#34;table&#34;:&#34;TEST_OGG.TEST_OGG&#34;,&#34;op_type&#34;:&#34;I&#34;,&#34;op_ts&#34;:&#34;2018-05-23 10:31:28.000078&#34;,&#34;current_ts&#34;:&#34;2018-05-23T10:36:48.525000&#34;,&#34;pos&#34;:&#34;00000000000000001093&#34;,&#34;after&#34;:{&#34;ID&#34;:1,&#34;NAME&#34;:&#34;test&#34;}}\n{&#34;table&#34;:&#34;TEST_OGG.TEST_OGG&#34;,&#34;op_type&#34;:&#34;U&#34;,&#34;op_ts&#34;:&#34;2018-05-23 10:31:36.000073&#34;,&#34;current_ts&#34;:&#34;2018-05-23T10:36:48.874000&#34;,&#34;pos&#34;:&#34;00000000000000001233&#34;,&#34;before&#34;:{},&#34;after&#34;:{&#34;ID&#34;:1,&#34;NAME&#34;:&#34;zhangsan&#34;}}\n{&#34;table&#34;:&#34;TEST_OGG.TEST_OGG&#34;,&#34;op_type&#34;:&#34;D&#34;,&#34;op_ts&#34;:&#34;2018-05-23 10:31:43.000107&#34;,&#34;current_ts&#34;:&#34;2018-05-23T10:36:48.875000&#34;,&#34;pos&#34;:&#34;00000000000000001376&#34;,&#34;before&#34;:{&#34;ID&#34;:1}}</code></pre></div><p>显然，Oracle的数据已准实时同步到Kafka,格式为json,其中op_type代表操作类型，这个可配置，我没有配置则按默认的来，默认为</p><div class=\"highlight\"><pre><code class=\"language-text\">gg.handler.kafkahandler.format.insertOpKey = I  \ngg.handler.kafkahandler.format.updateOpKey = U  \ngg.handler.kafkahandler.format.deleteOpKey = D</code></pre></div><p>before代表操作之前的数据，after代表操作后的数据，现在已经可以从kafka获取到同步的json数据了，后面可以用SparkStreaming和Storm等解析然后存到hadoop等大数据平台里</p><h2>6.4 SparkStreaming测试消费同步消息</h2><p>具体代码可参考<a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/05/17/sparkKafka/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Spark Streaming连接Kafka入门教程</a> 下面附上消费成功的结果图 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ad271bad54f54310accfd41a68f8cd70_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1551\" data-rawheight=\"874\" class=\"origin_image zh-lightbox-thumb\" width=\"1551\" data-original=\"https://pic1.zhimg.com/v2-ad271bad54f54310accfd41a68f8cd70_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1551&#39; height=&#39;874&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1551\" data-rawheight=\"874\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1551\" data-original=\"https://pic1.zhimg.com/v2-ad271bad54f54310accfd41a68f8cd70_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-ad271bad54f54310accfd41a68f8cd70_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>7、更新：后续遇到的问题</h2><p>在后面的使用过程中发现上面同步到kafka的json数据中少一些我们想要的一些，下面讲一下我是如何解决的 首先建表：</p><div class=\"highlight\"><pre><code class=\"language-sql\"><span class=\"k\">CREATE</span> <span class=\"k\">TABLE</span> <span class=\"s2\">&#34;TCLOUD&#34;</span><span class=\"p\">.</span><span class=\"s2\">&#34;T_OGG2&#34;</span> \n   <span class=\"p\">(</span>    <span class=\"s2\">&#34;ID&#34;</span> <span class=\"nb\">NUMBER</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"p\">,</span><span class=\"mi\">0</span><span class=\"p\">),</span> \n    <span class=\"s2\">&#34;TEXT_NAME&#34;</span> <span class=\"n\">VARCHAR2</span><span class=\"p\">(</span><span class=\"mi\">20</span><span class=\"p\">),</span> \n    <span class=\"s2\">&#34;AGE&#34;</span> <span class=\"nb\">NUMBER</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"p\">,</span><span class=\"mi\">0</span><span class=\"p\">),</span> \n    <span class=\"s2\">&#34;ADD&#34;</span> <span class=\"n\">VARCHAR2</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">),</span> \n    <span class=\"s2\">&#34;IDD&#34;</span> <span class=\"n\">VARCHAR2</span><span class=\"p\">(</span><span class=\"mi\">100</span><span class=\"p\">),</span> \n     <span class=\"k\">CONSTRAINT</span> <span class=\"s2\">&#34;T_OGG2_PK&#34;</span> <span class=\"k\">PRIMARY</span> <span class=\"k\">KEY</span> <span class=\"p\">(</span><span class=\"s2\">&#34;ID&#34;</span><span class=\"p\">,</span> <span class=\"s2\">&#34;IDD&#34;</span><span class=\"p\">)</span>\n\n   <span class=\"p\">)</span></code></pre></div><p>为什么不用之前建的表，主要是之前的字段太少，不容易看出问题，现在主要是增加几个字段，然后id,idd是联合主键。 看一下按照之前的配置，同步到kafka的数据(截取部分数据)</p><div class=\"highlight\"><pre><code class=\"language-vim\">{<span class=\"c\">&#34;table&#34;:&#34;TCLOUD.T_OGG2&#34;,&#34;op_type&#34;:&#34;I&#34;,&#34;op_ts&#34;:&#34;2018-05-31 11:46:09.512672&#34;,&#34;current_ts&#34;:&#34;2018-05-31T11:46:15.292000&#34;,&#34;pos&#34;:&#34;00000000000000001903&#34;,&#34;after&#34;:{&#34;ID&#34;:4,&#34;TEXT_NAME&#34;:null,&#34;AGE&#34;:0,&#34;ADD&#34;:null,&#34;IDD&#34;:&#34;8&#34;}}</span><span class=\"err\">\n</span><span class=\"err\"></span>{<span class=\"c\">&#34;table&#34;:&#34;TCLOUD.T_OGG2&#34;,&#34;op_type&#34;:&#34;U&#34;,&#34;op_ts&#34;:&#34;2018-05-31 11:49:10.514549&#34;,&#34;current_ts&#34;:&#34;2018-05-31T11:49:16.450000&#34;,&#34;pos&#34;:&#34;00000000000000002227&#34;,&#34;before&#34;:{},&#34;after&#34;:{&#34;ID&#34;:4,&#34;TEXT_NAME&#34;:&#34;lisi&#34;,&#34;IDD&#34;:&#34;7&#34;}}</span><span class=\"err\">\n</span><span class=\"err\"></span>{<span class=\"c\">&#34;table&#34;:&#34;TCLOUD.T_OGG2&#34;,&#34;op_type&#34;:&#34;U&#34;,&#34;op_ts&#34;:&#34;2018-05-31 11:49:48.514869&#34;,&#34;current_ts&#34;:&#34;2018-05-31T11:49:54.481000&#34;,&#34;pos&#34;:&#34;00000000000000002373&#34;,&#34;before&#34;:{&#34;ID&#34;:4,&#34;IDD&#34;:&#34;7&#34;},&#34;after&#34;:{&#34;ID&#34;:1,&#34;IDD&#34;:&#34;7&#34;}}</span><span class=\"err\">\n</span><span class=\"err\">\n</span><span class=\"err\"></span>{<span class=\"c\">&#34;table&#34;:&#34;TCLOUD.T_OGG2&#34;,&#34;op_type&#34;:&#34;D&#34;,&#34;op_ts&#34;:&#34;2018-05-31 11:52:38.516877&#34;,&#34;current_ts&#34;:&#34;2018-05-31T11:52:45.633000&#34;,&#34;pos&#34;:&#34;00000000000000003161&#34;,&#34;before&#34;:{&#34;ID&#34;:1,&#34;IDD&#34;:&#34;7&#34;}}</span></code></pre></div><p>现在只有insert的数据是全的，update更新非主键字段before是没有数据的，更新主键before只有主键的数据，delete只有before的主键字段，也就是update和delete的信息是不全的，且没有主键信息（程序里是不能判断哪一个是主键的），这样对于程序自动解析同步数据是不利的（不同的需求可能不一样），具体自己可以分析，就不啰嗦了，这里主要解决，有需要before和after全部信息和主键信息的需求。</p><h2>7.1 添加before</h2><p>在源端extract里添加下面几行</p><div class=\"highlight\"><pre><code class=\"language-text\">GGSCI (ambari.master.com) 33&gt; edit param extkafka\nGETUPDATEBEFORES\nNOCOMPRESSDELETES\nNOCOMPRESSUPDATES</code></pre></div><p>重启 extkafka</p><div class=\"highlight\"><pre><code class=\"language-text\">stop extkafka\nstart extkafka</code></pre></div><p>然后测试</p><div class=\"highlight\"><pre><code class=\"language-vim\">{<span class=\"c\">&#34;table&#34;:&#34;TCLOUD.T_OGG2&#34;,&#34;op_type&#34;:&#34;U&#34;,&#34;op_ts&#34;:&#34;2018-05-31 14:48:55.630340&#34;,&#34;current_ts&#34;:&#34;2018-05-31T14:49:01.709000&#34;,&#34;pos&#34;:&#34;00000000000000003770&#34;,&#34;before&#34;:{&#34;ID&#34;:1,&#34;AGE&#34;:20,&#34;IDD&#34;:&#34;1&#34;},&#34;after&#34;:{&#34;ID&#34;:1,&#34;AGE&#34;:1,&#34;IDD&#34;:&#34;1&#34;}}</span><span class=\"err\">\n</span><span class=\"err\"></span>{<span class=\"c\">&#34;table&#34;:&#34;TCLOUD.T_OGG2&#34;,&#34;op_type&#34;:&#34;U&#34;,&#34;op_ts&#34;:&#34;2018-05-31 14:48:55.630340&#34;,&#34;current_ts&#34;:&#34;2018-05-31T14:49:01.714000&#34;,&#34;pos&#34;:&#34;00000000000000004009&#34;,&#34;before&#34;:{&#34;ID&#34;:1,&#34;AGE&#34;:20,&#34;IDD&#34;:&#34;2&#34;},&#34;after&#34;:{&#34;ID&#34;:1,&#34;AGE&#34;:1,&#34;IDD&#34;:&#34;2&#34;}}</span><span class=\"err\">\n</span><span class=\"err\"></span>{<span class=\"c\">&#34;table&#34;:&#34;TCLOUD.T_OGG2&#34;,&#34;op_type&#34;:&#34;U&#34;,&#34;op_ts&#34;:&#34;2018-05-31 14:48:55.630340&#34;,&#34;current_ts&#34;:&#34;2018-05-31T14:49:01.715000&#34;,&#34;pos&#34;:&#34;00000000000000004248&#34;,&#34;before&#34;:{&#34;ID&#34;:1,&#34;AGE&#34;:20,&#34;IDD&#34;:&#34;8&#34;},&#34;after&#34;:{&#34;ID&#34;:1,&#34;AGE&#34;:1,&#34;IDD&#34;:&#34;8&#34;}}</span></code></pre></div><p>发现update之后before里有数据即可，但是现在before和after的数据都不全（只有部分字段）</p><p>网上有的说只添加GETUPDATES即可，但我测试了没有成功，关于每个配置项什么含义可以参考<a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/linucle/article/details/13505939\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">blog.csdn.net/linucle/a</span><span class=\"invisible\">rticle/details/13505939</span><span class=\"ellipsis\"></span></a>（有些配置的含义里面也没有给出） 参考：<a href=\"https://link.zhihu.com/?target=http%3A//www.itpub.net/thread-2083473-1-1.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">itpub.net/thread-208347</span><span class=\"invisible\">3-1-1.html</span><span class=\"ellipsis\"></span></a></p><h2>7.2 添加主键</h2><p>在kafka.props添加 </p><div class=\"highlight\"><pre><code class=\"language-vim\"><span class=\"nx\">gg</span>.<span class=\"nx\">handler</span>.<span class=\"nx\">kafkahandler</span>.<span class=\"nx\">format</span>.<span class=\"nx\">includePrimaryKeys</span><span class=\"p\">=</span><span class=\"nx\">true</span></code></pre></div><p>重启 rekafka</p><div class=\"highlight\"><pre><code class=\"language-bash\">stop rekafka\nstart rekafka</code></pre></div><p>测试：</p><div class=\"highlight\"><pre><code class=\"language-vim\">{<span class=\"c\">&#34;table&#34;:&#34;TCLOUD.T_OGG2&#34;,&#34;op_type&#34;:&#34;U&#34;,&#34;op_ts&#34;:&#34;2018-05-31 14:58:57.637035&#34;,&#34;current_ts&#34;:&#34;2018-05-31T14:59:03.401000&#34;,&#34;pos&#34;:&#34;00000000000000004510&#34;,&#34;primary_keys&#34;:[&#34;ID&#34;,&#34;IDD&#34;],&#34;before&#34;:{&#34;ID&#34;:1,&#34;AGE&#34;:1,&#34;IDD&#34;:&#34;1&#34;},&#34;after&#34;:{&#34;ID&#34;:1,&#34;AGE&#34;:20,&#34;IDD&#34;:&#34;1&#34;}}</span></code></pre></div><p>发现有primary_keys，不错~</p><p>参考：<a href=\"https://link.zhihu.com/?target=http%3A//blog.51cto.com/lyzbg/2088409\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">blog.51cto.com/lyzbg/20</span><span class=\"invisible\">88409</span><span class=\"ellipsis\"></span></a></p><h2>7.3 补全全部字段</h2><p>如果字段补全应该是Oracle没有开启全列补充日志</p><div class=\"highlight\"><pre><code class=\"language-sql\"><span class=\"k\">SQL</span><span class=\"o\">&gt;</span> <span class=\"k\">select</span> <span class=\"n\">supplemental_log_data_all</span> <span class=\"k\">from</span> <span class=\"n\">v$database</span><span class=\"p\">;</span>  \n\n<span class=\"n\">SUPPLE</span>\n<span class=\"c1\">------\n</span><span class=\"c1\"></span><span class=\"k\">NO</span></code></pre></div><p>通过以下命令开启</p><div class=\"highlight\"><pre><code class=\"language-sql\"><span class=\"k\">SQL</span><span class=\"o\">&gt;</span> <span class=\"k\">alter</span> <span class=\"k\">database</span> <span class=\"k\">add</span> <span class=\"n\">supplemental</span> <span class=\"n\">log</span> <span class=\"k\">data</span><span class=\"p\">(</span><span class=\"k\">all</span><span class=\"p\">)</span> <span class=\"n\">columns</span><span class=\"p\">;</span>\n\n<span class=\"k\">Database</span> <span class=\"n\">altered</span><span class=\"p\">.</span>\n\n<span class=\"k\">SQL</span><span class=\"o\">&gt;</span> <span class=\"k\">select</span> <span class=\"n\">supplemental_log_data_all</span> <span class=\"k\">from</span> <span class=\"n\">v$database</span><span class=\"p\">;</span>\n\n<span class=\"n\">SUPPLE</span>\n<span class=\"c1\">------\n</span><span class=\"c1\"></span><span class=\"n\">YES</span>\n\n<span class=\"k\">SQL</span><span class=\"o\">&gt;</span></code></pre></div><p>测试一下</p><div class=\"highlight\"><pre><code class=\"language-vim\">{<span class=\"c\">&#34;table&#34;:&#34;TCLOUD.T_OGG2&#34;,&#34;op_type&#34;:&#34;U&#34;,&#34;op_ts&#34;:&#34;2018-05-31 15:27:45.655518&#34;,&#34;current_ts&#34;:&#34;2018-05-31T15:27:52.891000&#34;,&#34;pos&#34;:&#34;00000000000000006070&#34;,&#34;primary_keys&#34;:[&#34;ID&#34;,&#34;IDD&#34;],&#34;before&#34;:{&#34;ID&#34;:1,&#34;TEXT_NAME&#34;:null,&#34;AGE&#34;:1,&#34;ADD&#34;:null,&#34;IDD&#34;:&#34;1&#34;},&#34;after&#34;:{&#34;ID&#34;:1,&#34;TEXT_NAME&#34;:null,&#34;AGE&#34;:20,&#34;ADD&#34;:null,&#34;IDD&#34;:&#34;1&#34;}}</span><span class=\"err\">\n</span><span class=\"err\"></span>{<span class=\"c\">&#34;table&#34;:&#34;TCLOUD.T_OGG2&#34;,&#34;op_type&#34;:&#34;U&#34;,&#34;op_ts&#34;:&#34;2018-05-31 15:27:45.655518&#34;,&#34;current_ts&#34;:&#34;2018-05-31T15:27:52.893000&#34;,&#34;pos&#34;:&#34;00000000000000006341&#34;,&#34;primary_keys&#34;:[&#34;ID&#34;,&#34;IDD&#34;],&#34;before&#34;:{&#34;ID&#34;:1,&#34;TEXT_NAME&#34;:null,&#34;AGE&#34;:1,&#34;ADD&#34;:null,&#34;IDD&#34;:&#34;2&#34;},&#34;after&#34;:{&#34;ID&#34;:1,&#34;TEXT_NAME&#34;:null,&#34;AGE&#34;:20,&#34;ADD&#34;:null,&#34;IDD&#34;:&#34;2&#34;}}</span><span class=\"err\">\n</span><span class=\"err\"></span>{<span class=\"c\">&#34;table&#34;:&#34;TCLOUD.T_OGG2&#34;,&#34;op_type&#34;:&#34;U&#34;,&#34;op_ts&#34;:&#34;2018-05-31 15:27:45.655518&#34;,&#34;current_ts&#34;:&#34;2018-05-31T15:27:52.895000&#34;,&#34;pos&#34;:&#34;00000000000000006612&#34;,&#34;primary_keys&#34;:[&#34;ID&#34;,&#34;IDD&#34;],&#34;before&#34;:{&#34;ID&#34;:1,&#34;TEXT_NAME&#34;:null,&#34;AGE&#34;:1,&#34;ADD&#34;:null,&#34;IDD&#34;:&#34;8&#34;},&#34;after&#34;:{&#34;ID&#34;:1,&#34;TEXT_NAME&#34;:null,&#34;AGE&#34;:20,&#34;ADD&#34;:null,&#34;IDD&#34;:&#34;8&#34;}}</span></code></pre></div><p>到现在json信息里的内容已经很全了，基本满足了我想要的，附图： </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-52b43b9f619afce47aee0111112cbb50_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1895\" data-rawheight=\"500\" class=\"origin_image zh-lightbox-thumb\" width=\"1895\" data-original=\"https://pic1.zhimg.com/v2-52b43b9f619afce47aee0111112cbb50_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1895&#39; height=&#39;500&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1895\" data-rawheight=\"500\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1895\" data-original=\"https://pic1.zhimg.com/v2-52b43b9f619afce47aee0111112cbb50_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-52b43b9f619afce47aee0111112cbb50_b.jpg\"/></figure><p> 启发我发现和Oracle全列补充日志没有开启有关的博客：<a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/huoshuyinhua/article/details/79013387\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">blog.csdn.net/huoshuyin</span><span class=\"invisible\">hua/article/details/79013387</span><span class=\"ellipsis\"></span></a> 开启命令参考：<a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/aaron8219/article/details/16825963\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">blog.csdn.net/aaron8219</span><span class=\"invisible\">/article/details/16825963</span><span class=\"ellipsis\"></span></a></p><p>注：博客上讲到，开启全列补充日志会导致磁盘快速增长，LGWR进程繁忙，不建议使用。大家可根据自己的情况使用。</p><h2>8、关于通配</h2><div class=\"highlight\"><pre><code class=\"language-text\">如果想通配整个库的话，只需要把上面的配置所有表名的改为*，如test_ogg.test_ogg改为 test_ogg.*,但是kafka的topic不能通配，所以需要把所有的表的数据放在一个topic即可，后面再用程序解析表名即可。</code></pre></div><h2>9、附录</h2><p>目标端在<a href=\"https://link.zhihu.com/?target=http%3A//www.oracle.com/technetwork/middleware/goldengate/downloads/index.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">这里</a>，下载下来后文件名123111_ggs_Adapters_Linux_x64.zip</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-381f3e0769fed0b79f11208c5d8b0101_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1560\" data-rawheight=\"592\" class=\"origin_image zh-lightbox-thumb\" width=\"1560\" data-original=\"https://pic2.zhimg.com/v2-381f3e0769fed0b79f11208c5d8b0101_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1560&#39; height=&#39;592&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1560\" data-rawheight=\"592\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1560\" data-original=\"https://pic2.zhimg.com/v2-381f3e0769fed0b79f11208c5d8b0101_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-381f3e0769fed0b79f11208c5d8b0101_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>源端在<a href=\"https://link.zhihu.com/?target=https%3A//edelivery.oracle.com/osdc/faces/SoftwareDelivery\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">旧版本</a>查询下载，下载后文件名为V34339-01.zip</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-b7f9009e25a955d2d284028fa2fedbaf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1644\" data-rawheight=\"565\" class=\"origin_image zh-lightbox-thumb\" width=\"1644\" data-original=\"https://pic4.zhimg.com/v2-b7f9009e25a955d2d284028fa2fedbaf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1644&#39; height=&#39;565&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1644\" data-rawheight=\"565\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1644\" data-original=\"https://pic4.zhimg.com/v2-b7f9009e25a955d2d284028fa2fedbaf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-b7f9009e25a955d2d284028fa2fedbaf_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-08e959e64f7aa7079aa3bdedaf88989a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1565\" data-rawheight=\"419\" class=\"origin_image zh-lightbox-thumb\" width=\"1565\" data-original=\"https://pic3.zhimg.com/v2-08e959e64f7aa7079aa3bdedaf88989a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1565&#39; height=&#39;419&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1565\" data-rawheight=\"419\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1565\" data-original=\"https://pic3.zhimg.com/v2-08e959e64f7aa7079aa3bdedaf88989a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-08e959e64f7aa7079aa3bdedaf88989a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-80c777b36e5a9f206a7c9dfb473e65dc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1753\" data-rawheight=\"757\" class=\"origin_image zh-lightbox-thumb\" width=\"1753\" data-original=\"https://pic1.zhimg.com/v2-80c777b36e5a9f206a7c9dfb473e65dc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1753&#39; height=&#39;757&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1753\" data-rawheight=\"757\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1753\" data-original=\"https://pic1.zhimg.com/v2-80c777b36e5a9f206a7c9dfb473e65dc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-80c777b36e5a9f206a7c9dfb473e65dc_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ffeef6bb573260c3cd7e1a92e36bde58_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1858\" data-rawheight=\"510\" class=\"origin_image zh-lightbox-thumb\" width=\"1858\" data-original=\"https://pic1.zhimg.com/v2-ffeef6bb573260c3cd7e1a92e36bde58_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1858&#39; height=&#39;510&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1858\" data-rawheight=\"510\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1858\" data-original=\"https://pic1.zhimg.com/v2-ffeef6bb573260c3cd7e1a92e36bde58_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-ffeef6bb573260c3cd7e1a92e36bde58_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>参考资料</h2><p><a href=\"https://link.zhihu.com/?target=https%3A//www.cnblogs.com/purpleraintear/p/6071038.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">基于OGG的Oracle与Hadoop集群准实时同步介绍</a></p>", 
            "topic": [
                {
                    "tag": "Kafka", 
                    "tagLink": "https://api.zhihu.com/topics/20012159"
                }, 
                {
                    "tag": "Ogg", 
                    "tagLink": "https://api.zhihu.com/topics/19719364"
                }, 
                {
                    "tag": "Oracle 数据库", 
                    "tagLink": "https://api.zhihu.com/topics/19660156"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/45229112", 
            "userName": "董可伦", 
            "userLink": "https://www.zhihu.com/people/de888cc734ddbfd918269a8e5fb196ba", 
            "upvote": 2, 
            "title": "Kafka安装启动入门教程", 
            "content": "<p>我的原创地址：</p><a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/05/21/kafkaConf/\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-0b4e5faa279a5e545d79e754b3dba862_ipico.jpg\" data-image-width=\"640\" data-image-height=\"640\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Kafka安装启动入门教程</a><h2>前言</h2><p>本文讲如何安装启动kafka,并进行测试，其中zookeepr是kafka自带的，本文基本按照官网文档进行安装启动的，并提出可能会出现的问题。官方文档：<a href=\"https://link.zhihu.com/?target=http%3A//kafka.apache.org/quickstart\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">kafka.apache.org/quicks</span><span class=\"invisible\">tart</span><span class=\"ellipsis\"></span></a> 本文虚拟机系统：centos7，不过其他版本的Linux系统是一样的~</p><h2>1、下载</h2><p>可直接在官网下载对应的版本<a href=\"https://link.zhihu.com/?target=http%3A//kafka.apache.org/downloads\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">kafka.apache.org/downlo</span><span class=\"invisible\">ads</span><span class=\"ellipsis\"></span></a>，我下载的是二进制版的，由于我的scala版本是2.11，所以下载kafka_2.11-1.1.0.tgz，大家可以根据自己的实际情况选择对应的版本。执行以下命令即可下载到本地了。</p><div class=\"highlight\"><pre><code class=\"language-bash\">wget http://mirrors.tuna.tsinghua.edu.cn/apache/kafka/1.1.0/kafka_2.11-1.1.0.tgz</code></pre></div><h2>2、解压到指定目录</h2><div class=\"highlight\"><pre><code class=\"language-bash\">tar -xzf kafka_2.11-1.1.0.tgz -C /opt/</code></pre></div><h2>3、启动服务</h2><h2>3.1 启动zookeeper</h2><p>kafka用到zookeeper，因此如果您的机器上没有zookeeper服务，则需要先启动zookpeer服务，本文使用kafka自带的zookeeper。</p><div class=\"highlight\"><pre><code class=\"language-bash\"><span class=\"nb\">cd</span> /opt/kafka_2.11-1.1.0/\nbin/zookeeper-server-start.sh config/zookeeper.properties</code></pre></div><p>因日志较多，所以只给出前几行信息和最后一行成功的信息</p><div class=\"highlight\"><pre><code class=\"language-text\">[2018-05-21 12:17:44,461] INFO Reading configuration from: config/zookeeper.properties (org.apache.zookeeper.server.quorum.QuorumPeerConfig)\n[2018-05-21 12:17:44,472] INFO autopurge.snapRetainCount set to 3 (org.apache.zookeeper.server.DatadirCleanupManager)\n...\n[2018-05-21 12:17:44,844] INFO binding to port 0.0.0.0/0.0.0.0:2181 (org.apache.zookeeper.server.NIOServerCnxnFactory)</code></pre></div><p>可以看到zookeeper服务的端口为2181</p><h2>3.2 启动kafka服务</h2><p>打开第二个终端</p><div class=\"highlight\"><pre><code class=\"language-bash\">bin/kafka-server-start.sh config/server.properties\n<span class=\"o\">[</span><span class=\"m\">2018</span>-05-21 <span class=\"m\">12</span>:21:07,901<span class=\"o\">]</span> INFO Registered kafka:type<span class=\"o\">=</span>kafka.Log4jController MBean <span class=\"o\">(</span>kafka.utils.Log4jControllerRegistration$<span class=\"o\">)</span>\n<span class=\"o\">[</span><span class=\"m\">2018</span>-05-21 <span class=\"m\">12</span>:21:09,417<span class=\"o\">]</span> INFO starting <span class=\"o\">(</span>kafka.server.KafkaServer<span class=\"o\">)</span>\n<span class=\"o\">[</span><span class=\"m\">2018</span>-05-21 <span class=\"m\">12</span>:21:09,419<span class=\"o\">]</span> INFO Connecting to zookeeper on localhost:2181 <span class=\"o\">(</span>kafka.server.KafkaServer<span class=\"o\">)</span>\n...\n<span class=\"o\">[</span><span class=\"m\">2018</span>-05-21 <span class=\"m\">12</span>:21:15,955<span class=\"o\">]</span> INFO <span class=\"o\">[</span>KafkaServer <span class=\"nv\">id</span><span class=\"o\">=</span><span class=\"m\">0</span><span class=\"o\">]</span> started <span class=\"o\">(</span>kafka.server.KafkaServer<span class=\"o\">)</span></code></pre></div><p>其中kafka的端口为9092,在下面这条信息可以看到</p><div class=\"highlight\"><pre><code class=\"language-text\">INFO Registered broker 0 at path /brokers/ids/0 with addresses: ArrayBuffer(EndPoint(ambari.master.com,9092,ListenerName(PLAINTEXT),PLAINTEXT)) (kafka.zk.KafkaZkClient)</code></pre></div><h2>4、创建一个主题</h2><p>打开第三个个终端</p><div class=\"highlight\"><pre><code class=\"language-bash\">bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor <span class=\"m\">1</span> --partitions <span class=\"m\">1</span> --topic <span class=\"nb\">test</span>\nCreated topic <span class=\"s2\">&#34;test&#34;</span>.</code></pre></div><p>可以通过list topic命令查看所有的主题</p><div class=\"highlight\"><pre><code class=\"language-bash\">bin/kafka-topics.sh --list --zookeeper localhost:2181\ntest</code></pre></div><p>或者，您也可以将代理配置为在发布不存在的主题时自动创建主题，而不是手动创建主题。</p><h2>5、发送消息</h2><p>Kafka带有一个命令行客户端，它将从文件或标准输入中获取输入，并将其作为消息发送到Kafka集群。默认情况下，每行将作为单独的消息发送。 启动生产者</p><div class=\"highlight\"><pre><code class=\"language-bash\">bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test</code></pre></div><p>输入几条消息发送到服务器</p><div class=\"highlight\"><pre><code class=\"language-text\">&gt;This is a message\n&gt;This is another message</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-926e6d88b45f35112c4ac8fce99f25fb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"855\" data-rawheight=\"125\" class=\"origin_image zh-lightbox-thumb\" width=\"855\" data-original=\"https://pic4.zhimg.com/v2-926e6d88b45f35112c4ac8fce99f25fb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;855&#39; height=&#39;125&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"855\" data-rawheight=\"125\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"855\" data-original=\"https://pic4.zhimg.com/v2-926e6d88b45f35112c4ac8fce99f25fb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-926e6d88b45f35112c4ac8fce99f25fb_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>6、启动消费者</h2><p>消费者可以将消息转储到标准输出 打开第四个个终端</p><div class=\"highlight\"><pre><code class=\"language-bash\">bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic <span class=\"nb\">test</span> --from-beginning</code></pre></div><p>然后就可以在命令行看到生产者发送的消息了</p><div class=\"highlight\"><pre><code class=\"language-text\">This is a message\nThis is another message</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-60d5f0909d7c0363c4ec596d0f69875b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"766\" data-rawheight=\"92\" class=\"origin_image zh-lightbox-thumb\" width=\"766\" data-original=\"https://pic4.zhimg.com/v2-60d5f0909d7c0363c4ec596d0f69875b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;766&#39; height=&#39;92&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"766\" data-rawheight=\"92\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"766\" data-original=\"https://pic4.zhimg.com/v2-60d5f0909d7c0363c4ec596d0f69875b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-60d5f0909d7c0363c4ec596d0f69875b_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>7、spark远程连接kafka</h2><p>因为我是用spark进行开发的，所以此时就想测试一下，这样配置，用spark程序在celipse里是否可以获取到kafka的数据（我之前使用ambari搭建的kafka进行测试的），程序可参考<a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/05/17/sparkKafka/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Spark Streaming连接Kafka入门教程</a>,但是运行程序发现并不能获取到kafka对应topic里的消息。 几次尝试，发现修改下面的配置即可</p><div class=\"highlight\"><pre><code class=\"language-bash\">vim config/server.properties\n<span class=\"nv\">listeners</span><span class=\"o\">=</span>PLAINTEXT://192.168.44.129:9092</code></pre></div><p>其中kafka默认的端口就是9092，不同的地方是端口前面加上了ip（192.168.44.129）,我想默认的是localhost，spark程序远程通过ip地址和localhost对应不上，所以获取不到kafka的消息，注意在第8部分：配置多个broker的集群，为了和官方文档一致，并没有加上ip，大家可根据需要自行修改。 这样的话就可以在程序里获取到历史消息了，但是如果想新产生几条数据的话，启动命令需要将localhost改为192.168.44.129，否则会产生异常，即：</p><div class=\"highlight\"><pre><code class=\"language-bash\">bin/kafka-console-producer.sh --broker-list <span class=\"m\">192</span>.168.44.129:9092 --topic <span class=\"nb\">test</span>\nbin/kafka-console-consumer.sh --bootstrap-server <span class=\"m\">192</span>.168.44.129:9092 --topic <span class=\"nb\">test</span> --from-beginning</code></pre></div><h2>8、设置多个broker的集群</h2><p>到目前为止，我们设置的是单个broker，这样并不好，下面我们还是在这一台机器上设置三个节点。</p><h2>8.1 为每个broker创建一个配置文件</h2><div class=\"highlight\"><pre><code class=\"language-text\">cp config/server.properties config/server-1.properties\ncp config/server.properties config/server-2.properties</code></pre></div><p>然后用vim修改 config/server-1.properties:</p><div class=\"highlight\"><pre><code class=\"language-vim\"><span class=\"nx\">broker</span>.<span class=\"nx\">id</span><span class=\"p\">=</span><span class=\"m\">1</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"nx\">listeners</span><span class=\"p\">=</span><span class=\"nx\">PLAINTEXT</span><span class=\"p\">:</span><span class=\"sr\">//</span><span class=\"p\">:</span><span class=\"m\">9093</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"nx\">log</span>.<span class=\"nx\">dir</span><span class=\"p\">=</span><span class=\"sr\">/tmp/</span><span class=\"nx\">kafka</span><span class=\"p\">-</span><span class=\"nx\">logs</span><span class=\"m\">-1</span></code></pre></div><p>config/server-2.properties:</p><div class=\"highlight\"><pre><code class=\"language-vim\"><span class=\"nx\">broker</span>.<span class=\"nx\">id</span><span class=\"p\">=</span><span class=\"m\">2</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"nx\">listeners</span><span class=\"p\">=</span><span class=\"nx\">PLAINTEXT</span><span class=\"p\">:</span><span class=\"sr\">//</span><span class=\"p\">:</span><span class=\"m\">9094</span><span class=\"err\">\n</span><span class=\"err\"></span><span class=\"nx\">log</span>.<span class=\"nx\">dir</span><span class=\"p\">=</span><span class=\"sr\">/tmp/</span><span class=\"nx\">kafka</span><span class=\"p\">-</span><span class=\"nx\">logs</span><span class=\"m\">-2</span></code></pre></div><p>（其中listeners需要把前面的注释也就是#去掉） broker.id是集群中每个节点唯一且永久的名称，因为我们实在同一个机器上运行这些文件，所以为了避免端口冲突和数据彼此覆盖，我们必须重写它的端口和日志目录。</p><h2>8.2 启动新节点</h2><p>我们已经启动了一个节点了（broker.id=0），现在启动两个新节点</p><div class=\"highlight\"><pre><code class=\"language-text\">bin/kafka-server-start.sh config/server-1.properties &amp;\nbin/kafka-server-start.sh config/server-2.properties &amp;</code></pre></div><h2>8.3 创建三个副本的新主题：</h2><div class=\"highlight\"><pre><code class=\"language-text\">bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 3 --partitions 1 --topic my-replicated-topic</code></pre></div><p>运行describe topics查看主题的信息</p><div class=\"highlight\"><pre><code class=\"language-text\">bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topic\nTopic:my-replicated-topic   PartitionCount:1    ReplicationFactor:3 Configs:\n    Topic: my-replicated-topic  Partition: 0    Leader: 0   Replicas: 0,1,2 Isr: 0,1,2</code></pre></div><p>下面解释一下这些输出，第一行是所有分区的摘要，另外的每一行是一个分区的信息，因为这个主题只有一个分区，所以只有一行。 leader：负责所有读和写，是这个分区从所有节点随机选择的。 replicas：是为这个分区复制日志的节点列表，无论他们是领导者还是他们现在还活着。 isr：是同步副本的集合，是还活着的副本的自己并被leader捕获（caught-up）。</p><p>在我的示例中节点0是该主题唯一分区的leader。 我们可以用相同的命令查看之前创建的主题 test</p><div class=\"highlight\"><pre><code class=\"language-bash\">bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic <span class=\"nb\">test</span>\nTopic:test  PartitionCount:1    ReplicationFactor:1 Configs:\n    Topic: <span class=\"nb\">test</span> Partition: <span class=\"m\">0</span>    Leader: <span class=\"m\">0</span>   Replicas: <span class=\"m\">0</span> Isr: <span class=\"m\">0</span></code></pre></div><h2>8.4 发送消息</h2><div class=\"highlight\"><pre><code class=\"language-bash\">bin/kafka-console-producer.sh --broker-list localhost:9092 --topic my-replicated-topic\n&gt;my <span class=\"nb\">test</span> message <span class=\"m\">1</span>\n&gt;my <span class=\"nb\">test</span> message <span class=\"m\">2</span></code></pre></div><h2>8、5 消费消息</h2><div class=\"highlight\"><pre><code class=\"language-bash\">bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --from-beginning --topic my-replicated-topic\nmy <span class=\"nb\">test</span> message <span class=\"m\">1</span>\nmy <span class=\"nb\">test</span> message <span class=\"m\">2</span></code></pre></div><h2>8.6 测试容错及发现的问题</h2><p>我的leader是节点0，现在kill掉</p><div class=\"highlight\"><pre><code class=\"language-bash\">ps aux <span class=\"p\">|</span> grep server.properties</code></pre></div><p>得到对应的进程号</p><div class=\"highlight\"><pre><code class=\"language-text\">root      16557 34.6  8.4 4322904 326472 pts/1  Sl+  05:30 ...\n...\n2.11-1.1.0/bin/../libs/zookeeper-3.4.10.jar kafka.Kafka config/server.properties\n\nroot      11040  0.0  0.0 112648   976 pts/4    R+   03:04   0:00 grep --color=auto server-0.properties</code></pre></div><p>然后kill掉第一个（第二个是grep命令本身的）</p><div class=\"highlight\"><pre><code class=\"language-bash\"><span class=\"nb\">kill</span> -9 <span class=\"m\">16557</span></code></pre></div><p>kill掉之后再描述下topic</p><div class=\"highlight\"><pre><code class=\"language-text\">bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topic\nTopic:my-replicated-topic   PartitionCount:1    ReplicationFactor:3 Configs:\n    Topic: my-replicated-topic  Partition: 0    Leader: 1   Replicas: 0,1,2 Isr: 1,2</code></pre></div><p>现在leader已经切换到节点1，Isr也只有节点1和2了</p><p>官网上说：但是即使原来的leader失败，这些消息仍然可用于消费，但是下面又有一些坑 首先执行下面这句</p><div class=\"highlight\"><pre><code class=\"language-bash\">bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --from-beginning --topic my-replicated-topic</code></pre></div><p>发现会报错</p><div class=\"highlight\"><pre><code class=\"language-text\">[2018-05-22 03:55:13,304] WARN [Consumer clientId=consumer-1, groupId=console-consumer-29320] Connection to node -1 could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)</code></pre></div><p>原因是节点0已经kill掉了，也就是端口9092已经不能用了，要换成别的端口，所以把上面的命令的端口换成节点1或者2，再执行，发现不报错了，但是没有得任何数据，即使再生成几条消息，也是不报错，但没有数据。 于是想是不是因为节点0没有加 &amp;，加上 &amp;重新启动节点0，新建主题，直到找到leader为节点0的，再kill掉节点0发现还是同样的问题，但是官网上说是可以的... 找一个leader不是节点0的主题进行测试，比如官网例子上的节点1，kill掉节点1，之前的leader由1变为0了，再消费测试，发现不管是新增的消息还是历史消息都可以得到，至于这是不是官网的bug及如何解决这个bug（可能还需要修改其他配置），目前我还没找到，如果找到我会更新的。</p>", 
            "topic": [
                {
                    "tag": "Kafka", 
                    "tagLink": "https://api.zhihu.com/topics/20012159"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/44560572", 
            "userName": "Paco", 
            "userLink": "https://www.zhihu.com/people/78be1eb7dc54c0817aa1b182a7a1ebf8", 
            "upvote": 0, 
            "title": "spark读parquet目录遇到的元数据文件不完整的问题", 
            "content": "<p>有个在线系统，Spark1.6.3，有一个spark streaming程序定期产生一个parquet目录， 后面一个spark定期批处理检测目录下_SUCCESS文件是否生成结束，然后读入dataframe处理。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>大部分情况下没有问题，但是每天总会遇到几个批次后面读取失败的，一般都是报错_metadata和_common_metadata读取的问题。</p><p class=\"ztext-empty-paragraph\"><br/></p><blockquote>18/09/14 16:58:00 WARN TaskSetManager: Lost task 7.0 in stage 0.0 (TID 7, localhost): java.io.IOException: Could not read footer: java.lang.RuntimeException: file:/data/parquet/_common_metadata is not a Parquet file (too small)<br/>\tat org.apache.parquet.hadoop.ParquetFileReader.readAllFootersInParallel(<a href=\"https://link.zhihu.com/?target=http%3A//parquetfilereader.java%3A247/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">ParquetFileReader.java:247</a>)<br/>\tat org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$$anonfun$27.apply(ParquetRelation.scala:786)<br/>\tat org.apache.spark.sql.execution.datasources.parquet.ParquetRelation$$anonfun$27.apply(ParquetRelation.scala:775)<br/>\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710)<br/>\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710)<br/>\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)<br/>\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)<br/>\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)<br/>\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)<br/>\tat <a href=\"https://link.zhihu.com/?target=http%3A//org.apache.spark.scheduler.task.run/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">org.apache.spark.scheduler.Task.run</a>(Task.scala:89)<br/>\tat <a href=\"https://link.zhihu.com/?target=http%3A//org.apache.spark.executor.executor%2524taskrunner.run/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">org.apache.spark.executor.Executor$TaskRunner.run</a>(Executor.scala:227)<br/>\tat java.util.concurrent.ThreadPoolExecutor.runWorker(<a href=\"https://link.zhihu.com/?target=http%3A//threadpoolexecutor.java%3A1149/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">ThreadPoolExecutor.java:1149</a>)<br/>\tat <a href=\"https://link.zhihu.com/?target=http%3A//java.util.concurrent.threadpoolexecutor%2524worker.run/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">java.util.concurrent.ThreadPoolExecutor$Worker.run</a>(<a href=\"https://link.zhihu.com/?target=http%3A//threadpoolexecutor.java%3A624/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">ThreadPoolExecutor.java:624</a>)<br/>\tat <a href=\"https://link.zhihu.com/?target=http%3A//java.lang.thread.run/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">java.lang.Thread.run</a>(<a href=\"https://link.zhihu.com/?target=http%3A//thread.java%3A748/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Thread.java:748</a>)<br/>Caused by: java.lang.RuntimeException: file:/data/parquet/_common_metadata is not a Parquet file (too small)<br/>\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(<a href=\"https://link.zhihu.com/?target=http%3A//parquetfilereader.java%3A412/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">ParquetFileReader.java:412</a>)<br/>\tat <a href=\"https://link.zhihu.com/?target=http%3A//org.apache.parquet.hadoop.parquetfilereader%25242.call/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">org.apache.parquet.hadoop.ParquetFileReader$2.call</a>(<a href=\"https://link.zhihu.com/?target=http%3A//parquetfilereader.java%3A237/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">ParquetFileReader.java:237</a>)<br/>\tat <a href=\"https://link.zhihu.com/?target=http%3A//org.apache.parquet.hadoop.parquetfilereader%25242.call/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">org.apache.parquet.hadoop.ParquetFileReader$2.call</a>(<a href=\"https://link.zhihu.com/?target=http%3A//parquetfilereader.java%3A233/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">ParquetFileReader.java:233</a>)<br/>\tat <a href=\"https://link.zhihu.com/?target=http%3A//java.util.concurrent.futuretask.run/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">java.util.concurrent.FutureTask.run</a>(<a href=\"https://link.zhihu.com/?target=http%3A//futuretask.java%3A266/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">FutureTask.java:266</a>)<br/>\t... 3 more</blockquote><p>怀疑是<i>SUCCESS比两个metadata文件先生成，导致后面开始读数据了但是读到了空的或是未完整的_metadata， _common_metadata文件了。</i></p><p><i>为了验证猜测，直接在本地spark-shell环境做个实验</i></p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.</span><span class=\"o\">{</span><span class=\"nc\">SQLContext</span><span class=\"o\">,</span> <span class=\"nc\">SaveMode</span><span class=\"o\">}</span>\n<span class=\"k\">val</span> <span class=\"n\">path</span> <span class=\"k\">=</span> <span class=\"s\">&#34;file:///data/parquet&#34;</span>\n<span class=\"k\">case</span> <span class=\"k\">class</span> <span class=\"nc\">A</span><span class=\"o\">(</span><span class=\"n\">id</span><span class=\"k\">:</span><span class=\"kt\">String</span><span class=\"o\">)</span>\n<span class=\"k\">val</span> <span class=\"n\">df</span> <span class=\"k\">=</span> <span class=\"nc\">Seq</span><span class=\"o\">(</span><span class=\"s\">&#34;aaa&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;bbbbb&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;abcdfd&#34;</span><span class=\"o\">).</span><span class=\"n\">map</span><span class=\"o\">(</span><span class=\"n\">A</span><span class=\"o\">(</span><span class=\"k\">_</span><span class=\"o\">)).</span><span class=\"n\">toDF</span><span class=\"o\">()</span>\n<span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">write</span><span class=\"o\">.</span><span class=\"n\">mode</span><span class=\"o\">(</span><span class=\"nc\">SaveMode</span><span class=\"o\">.</span><span class=\"nc\">Append</span><span class=\"o\">).</span><span class=\"n\">parquet</span><span class=\"o\">(</span><span class=\"n\">path</span><span class=\"o\">)</span></code></pre></div><p>看看输出目录的情况, stat一下文件时间，证实了我的猜想。</p><blockquote>➜  parquet ls -lt /data/parquet/*<br/>-rw-r--r-- 1 paco paco  211 9月  14 17:06 /data/parquet/_common_metadata<br/>-rw-r--r-- 1 paco paco 5844 9月  14 17:06 /data/parquet/_metadata<br/>-rw-r--r-- 1 paco paco    0 9月  14 17:06 /data/parquet/_SUCCESS<br/>-rw-r--r-- 1 paco paco  211 9月  14 17:06 /data/parquet/part-r-00063-6a9d51c6-b204-4807-9bbb-9c202f1cd131.snappy.parquet<br/>...<br/>➜  parquet stat _*<br/>  文件：_common_metadata<br/>  大小：211       \t块：8          IO 块：4096   普通文件<br/>设备：806h/2054d\tInode：8140744     硬链接：1<br/>权限：(0644/-rw-r--r--)  Uid：( 1000/    paco)   Gid：( 1000/    paco)<br/>最近访问：2018-09-14 17:06:56.836039763 +0800<br/>最近更改：2018-09-14 17:06:56.848039475 +0800<br/>最近改动：2018-09-14 17:06:<b>56.848039475</b> +0800<br/>创建时间：-<br/>  文件：_metadata<br/>  大小：5844      \t块：16         IO 块：4096   普通文件<br/>设备：806h/2054d\tInode：8140738     硬链接：1<br/>权限：(0644/-rw-r--r--)  Uid：( 1000/    paco)   Gid：( 1000/    paco)<br/>最近访问：2018-09-14 17:06:56.820040147 +0800<br/>最近更改：2018-09-14 17:06:56.836039763 +0800<br/>最近改动：2018-09-14 17:06:<b>56.836039763</b> +0800<br/>创建时间：-<br/>  文件：_SUCCESS<br/>  大小：0         \t块：0          IO 块：4096   普通空文件<br/>设备：806h/2054d\tInode：8140732     硬链接：1<br/>权限：(0644/-rw-r--r--)  Uid：( 1000/    paco)   Gid：( 1000/    paco)<br/>最近访问：2018-09-14 17:06:56.740042067 +0800<br/>最近更改：2018-09-14 17:06:56.740042067 +0800<br/>最近改动：2018-09-14 17:06:<b>56.740042067</b> +0800<br/>创建时间：-</blockquote><p>实际上，这俩文件删掉的话也是可以被sqlContext正常读取parquet数据的，</p><blockquote>sqlContext.read.parquet(path) </blockquote><p>然而如果是内容为空或者不完成，比如删掉后，touch一个空的，上面的读取就失败了，重现了上面的Exception了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>解决办法:</p><p>治本应该是找到某些配置，让读取parquet目录的时候忽略掉这俩文件。暂时没找到，有知道的请告诉我。 </p><p>work around1： 则是，在detect到_SUCCESS文件之后，sleep一个安全的时间段，比如1s之后，再开始处理, 这个略显土鳖。</p><p>work around2：在读取的时候，扫描目录，只读取*.parquet 文件名的，经过观察这些文件先于_SUCCESS生成，目前采用了这个方法，解决了问题，代码如下</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"c1\">//过滤得到输出目录下所有*.parquet文件列表，避免处理到空的_*metadata文件造成读取失败\n</span><span class=\"c1\"></span><span class=\"k\">val</span> <span class=\"n\">parquetFiles</span> <span class=\"k\">=</span> <span class=\"n\">hdfs</span><span class=\"o\">.</span><span class=\"n\">list</span><span class=\"o\">(</span><span class=\"n\">inPath</span><span class=\"o\">)</span>\n         <span class=\"o\">.</span><span class=\"n\">map</span><span class=\"o\">(</span><span class=\"k\">_</span><span class=\"o\">.</span><span class=\"n\">getPath</span><span class=\"o\">.</span><span class=\"n\">toString</span><span class=\"o\">)</span>\n         <span class=\"o\">.</span><span class=\"n\">filter</span><span class=\"o\">(</span><span class=\"k\">_</span><span class=\"o\">.</span><span class=\"n\">endsWith</span><span class=\"o\">(</span><span class=\"s\">&#34;.parquet&#34;</span><span class=\"o\">))</span>\n<span class=\"k\">val</span> <span class=\"n\">df_user_pos</span> <span class=\"k\">=</span> <span class=\"n\">sqlContext</span><span class=\"o\">.</span><span class=\"n\">read</span><span class=\"o\">.</span><span class=\"n\">parquet</span><span class=\"o\">(</span><span class=\"n\">parquetFiles</span><span class=\"k\">:_</span><span class=\"kt\">*</span><span class=\"o\">)</span></code></pre></div><p></p>", 
            "topic": [
                {
                    "tag": "Spark", 
                    "tagLink": "https://api.zhihu.com/topics/19942170"
                }, 
                {
                    "tag": "大数据", 
                    "tagLink": "https://api.zhihu.com/topics/19740929"
                }, 
                {
                    "tag": "Hadoop", 
                    "tagLink": "https://api.zhihu.com/topics/19563390"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/43680178", 
            "userName": "董可伦", 
            "userLink": "https://www.zhihu.com/people/de888cc734ddbfd918269a8e5fb196ba", 
            "upvote": 0, 
            "title": "旧版spark（1.6） 将rdd动态转为dataframe", 
            "content": "<p>我的原创地址：</p><a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/05/11/rdd2df/\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">dongkelun.com/2018/05/1</span><span class=\"invisible\">1/rdd2df/</span><span class=\"ellipsis\"></span></a><h2>前言</h2><p>旧版本spark不能直接读取csv转为df,没有spark.read.option(&#34;header&#34;, &#34;true&#34;).csv这么简单的方法直接将第一行作为df的列名，只能现将数据读取为rdd,然后通过map和todf方法转为df,如果csv(txt)的列数很多的话用如(1,2,...,n)，即创建元组很麻烦，本文解决如何用旧版spark读取多列txt文件转为df</p><h2>1、新版</h2><p>为了直观明白本文的目的，先看一下新版spark如何实现 </p><h2>1.1 数据</h2><p>data.csv，如图： </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-214424e63e67abbd79996871d70531e5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1118\" data-rawheight=\"358\" class=\"origin_image zh-lightbox-thumb\" width=\"1118\" data-original=\"https://pic2.zhimg.com/v2-214424e63e67abbd79996871d70531e5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1118&#39; height=&#39;358&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1118\" data-rawheight=\"358\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1118\" data-original=\"https://pic2.zhimg.com/v2-214424e63e67abbd79996871d70531e5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-214424e63e67abbd79996871d70531e5_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>1.2 代码</h2><p>新版代码较简单，直接通过spark.read.option(&#34;header&#34;, &#34;true&#34;).csv(data_path)即可实现！</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">package</span> <span class=\"nn\">com.dkl.leanring.spark.sql</span>\n\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.SparkSession</span>\n\n<span class=\"k\">object</span> <span class=\"nc\">Txt2Df</span> <span class=\"o\">{</span>\n  <span class=\"k\">def</span> <span class=\"n\">main</span><span class=\"o\">(</span><span class=\"n\">args</span><span class=\"k\">:</span> <span class=\"kt\">Array</span><span class=\"o\">[</span><span class=\"kt\">String</span><span class=\"o\">])</span><span class=\"k\">:</span> <span class=\"kt\">Unit</span> <span class=\"o\">=</span> <span class=\"o\">{</span>\n    <span class=\"k\">val</span> <span class=\"n\">spark</span> <span class=\"k\">=</span> <span class=\"nc\">SparkSession</span><span class=\"o\">.</span><span class=\"n\">builder</span><span class=\"o\">().</span><span class=\"n\">appName</span><span class=\"o\">(</span><span class=\"s\">&#34;Txt2Df&#34;</span><span class=\"o\">).</span><span class=\"n\">master</span><span class=\"o\">(</span><span class=\"s\">&#34;local&#34;</span><span class=\"o\">).</span><span class=\"n\">getOrCreate</span><span class=\"o\">()</span>\n    <span class=\"k\">val</span> <span class=\"n\">data_path</span> <span class=\"k\">=</span> <span class=\"s\">&#34;files/data.csv&#34;</span>\n    <span class=\"k\">val</span> <span class=\"n\">df</span> <span class=\"k\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">read</span><span class=\"o\">.</span><span class=\"n\">option</span><span class=\"o\">(</span><span class=\"s\">&#34;header&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;true&#34;</span><span class=\"o\">).</span><span class=\"n\">csv</span><span class=\"o\">(</span><span class=\"n\">data_path</span><span class=\"o\">)</span>\n    <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"o\">()</span>\n  <span class=\"o\">}</span>\n<span class=\"o\">}</span></code></pre></div><h2>1.3 结果</h2><div class=\"highlight\"><pre><code class=\"language-text\">+----+----+----+----+----+\n|col1|col2|col3|col4|col5|\n+----+----+----+----+----+\n|  11|  12|  13|  14|  15|\n|  21|  22|  23|  24|  25|\n|  31|  32|  33|  34|  35|\n|  41|  42|  43|  44|  45|\n+----+----+----+----+----+</code></pre></div><h2>2、旧版</h2><h2>2.1 数据</h2><p>data.txt</p><div class=\"highlight\"><pre><code class=\"language-text\">col1,col2,col3,col4,col5\n11,12,13,14,15\n21,22,23,24,25\n31,32,33,34,35\n41,42,43,44,45</code></pre></div><p>其中列数可任意指定</p><h2>2.2 代码</h2><p>先看一下，利用rdd.map().toDF()遇到的问题</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">val</span> <span class=\"n\">sqlContext</span> <span class=\"k\">=</span> <span class=\"k\">new</span> <span class=\"nc\">SQLContext</span><span class=\"o\">(</span><span class=\"n\">sc</span><span class=\"o\">)</span>\n<span class=\"k\">import</span> <span class=\"nn\">sqlContext.implicits._</span>\n<span class=\"k\">val</span> <span class=\"n\">data_path</span> <span class=\"k\">=</span> <span class=\"s\">&#34;files/data.txt&#34;</span>\n<span class=\"k\">val</span> <span class=\"n\">data</span> <span class=\"k\">=</span> <span class=\"n\">sc</span><span class=\"o\">.</span><span class=\"n\">textFile</span><span class=\"o\">(</span><span class=\"n\">data_path</span><span class=\"o\">)</span>\n<span class=\"k\">val</span> <span class=\"n\">first</span> <span class=\"k\">=</span> <span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">first</span> <span class=\"c1\">//第一行作为列名\n</span><span class=\"c1\"></span><span class=\"k\">val</span> <span class=\"n\">colName</span> <span class=\"k\">=</span> <span class=\"n\">first</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"o\">(</span><span class=\"s\">&#34;,&#34;</span><span class=\"o\">)</span>\n<span class=\"k\">val</span> <span class=\"n\">rdd</span> <span class=\"k\">=</span> <span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">filter</span><span class=\"o\">(</span><span class=\"k\">_</span> <span class=\"o\">!=</span> <span class=\"n\">first</span><span class=\"o\">)</span> <span class=\"c1\">//注意first是列名，在这里的txt里是唯一的，否则会过滤掉多行\n</span><span class=\"c1\"></span><span class=\"k\">val</span> <span class=\"n\">df</span> <span class=\"k\">=</span> <span class=\"n\">rdd</span><span class=\"o\">.</span><span class=\"n\">map</span><span class=\"o\">(</span><span class=\"k\">_</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"o\">(</span><span class=\"s\">&#34;,&#34;</span><span class=\"o\">)).</span><span class=\"n\">map</span><span class=\"o\">(</span><span class=\"n\">p</span> <span class=\"k\">=&gt;</span> <span class=\"o\">(</span><span class=\"n\">p</span><span class=\"o\">(</span><span class=\"mi\">0</span><span class=\"o\">),</span> <span class=\"n\">p</span><span class=\"o\">(</span><span class=\"mi\">1</span><span class=\"o\">),</span> <span class=\"n\">p</span><span class=\"o\">(</span><span class=\"mi\">2</span><span class=\"o\">),</span> <span class=\"n\">p</span><span class=\"o\">(</span><span class=\"mi\">3</span><span class=\"o\">),</span> <span class=\"n\">p</span><span class=\"o\">(</span><span class=\"mi\">4</span><span class=\"o\">))).</span><span class=\"n\">toDF</span><span class=\"o\">(</span><span class=\"n\">colName</span><span class=\"k\">:</span> <span class=\"k\">_</span><span class=\"kt\">*</span><span class=\"o\">)</span>\n<span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">show</span></code></pre></div><p>通过上面的代码可以完成rdd转df,但是在构造元组的时候：(p(0), p(1), p(2), p(3), p(4))，只能通过()来构造，而每多一列就要手写加一列，不能通过:_*给构造函数传数组的方式来完成（目前我没有找到~），所以当列数很多的时候如上百列，这种方式很麻烦，可通过下面的代码解决。</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">package</span> <span class=\"nn\">com.dkl.leanring.spark.sql</span>\n\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.SparkConf</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.SparkContext</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.SQLContext</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.types._</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.Row</span>\n<span class=\"k\">object</span> <span class=\"nc\">Rdd2Df</span> <span class=\"o\">{</span>\n  <span class=\"k\">def</span> <span class=\"n\">main</span><span class=\"o\">(</span><span class=\"n\">args</span><span class=\"k\">:</span> <span class=\"kt\">Array</span><span class=\"o\">[</span><span class=\"kt\">String</span><span class=\"o\">])</span><span class=\"k\">:</span> <span class=\"kt\">Unit</span> <span class=\"o\">=</span> <span class=\"o\">{</span>\n    <span class=\"k\">val</span> <span class=\"n\">conf</span> <span class=\"k\">=</span> <span class=\"k\">new</span> <span class=\"nc\">SparkConf</span><span class=\"o\">().</span><span class=\"n\">setAppName</span><span class=\"o\">(</span><span class=\"s\">&#34;Rdd2Df&#34;</span><span class=\"o\">).</span><span class=\"n\">setMaster</span><span class=\"o\">(</span><span class=\"s\">&#34;local&#34;</span><span class=\"o\">)</span>\n    <span class=\"k\">val</span> <span class=\"n\">sc</span> <span class=\"k\">=</span> <span class=\"k\">new</span> <span class=\"nc\">SparkContext</span><span class=\"o\">(</span><span class=\"n\">conf</span><span class=\"o\">)</span>\n    <span class=\"k\">val</span> <span class=\"n\">sqlContext</span> <span class=\"k\">=</span> <span class=\"k\">new</span> <span class=\"nc\">SQLContext</span><span class=\"o\">(</span><span class=\"n\">sc</span><span class=\"o\">)</span>\n    <span class=\"k\">val</span> <span class=\"n\">data_path</span> <span class=\"k\">=</span> <span class=\"s\">&#34;files/data.txt&#34;</span>\n    <span class=\"k\">val</span> <span class=\"n\">data</span> <span class=\"k\">=</span> <span class=\"n\">sc</span><span class=\"o\">.</span><span class=\"n\">textFile</span><span class=\"o\">(</span><span class=\"n\">data_path</span><span class=\"o\">)</span>\n    <span class=\"k\">val</span> <span class=\"n\">first</span> <span class=\"k\">=</span> <span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">first</span> <span class=\"c1\">//第一行作为列名\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">colName</span> <span class=\"k\">=</span> <span class=\"n\">first</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"o\">(</span><span class=\"s\">&#34;,&#34;</span><span class=\"o\">)</span>\n    <span class=\"k\">val</span> <span class=\"n\">rdd</span> <span class=\"k\">=</span> <span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">filter</span><span class=\"o\">(</span><span class=\"k\">_</span> <span class=\"o\">!=</span> <span class=\"n\">first</span><span class=\"o\">)</span> <span class=\"c1\">//注意first是列名，在这里的txt里是唯一的，否则会过滤掉多行\n</span><span class=\"c1\"></span>    <span class=\"c1\">//列名\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">schema</span> <span class=\"k\">=</span> <span class=\"nc\">StructType</span><span class=\"o\">(</span><span class=\"n\">colName</span><span class=\"o\">.</span><span class=\"n\">map</span><span class=\"o\">(</span><span class=\"n\">fieldName</span> <span class=\"k\">=&gt;</span> <span class=\"nc\">StructField</span><span class=\"o\">(</span><span class=\"n\">fieldName</span><span class=\"o\">,</span> <span class=\"nc\">StringType</span><span class=\"o\">,</span> <span class=\"kc\">true</span><span class=\"o\">)))</span>\n    <span class=\"k\">val</span> <span class=\"n\">rowRDD</span> <span class=\"k\">=</span> <span class=\"n\">rdd</span><span class=\"o\">.</span><span class=\"n\">map</span><span class=\"o\">(</span><span class=\"k\">_</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"o\">(</span><span class=\"s\">&#34;,&#34;</span><span class=\"o\">)).</span><span class=\"n\">map</span><span class=\"o\">(</span><span class=\"n\">p</span> <span class=\"k\">=&gt;</span> <span class=\"nc\">Row</span><span class=\"o\">(</span><span class=\"n\">p</span><span class=\"k\">:</span> <span class=\"k\">_</span><span class=\"kt\">*</span><span class=\"o\">))</span>\n    <span class=\"k\">val</span> <span class=\"n\">df</span> <span class=\"k\">=</span> <span class=\"n\">sqlContext</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"o\">(</span><span class=\"n\">rowRDD</span><span class=\"o\">,</span> <span class=\"n\">schema</span><span class=\"o\">)</span>\n    <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">show</span>\n\n  <span class=\"o\">}</span>\n<span class=\"o\">}</span></code></pre></div><h2>2.3 结果</h2><div class=\"highlight\"><pre><code class=\"language-text\">+----+----+----+----+----+\n|col1|col2|col3|col4|col5|\n+----+----+----+----+----+\n|  11|  12|  13|  14|  15|\n|  21|  22|  23|  24|  25|\n|  31|  32|  33|  34|  35|\n|  41|  42|  43|  44|  45|\n+----+----+----+----+----+</code></pre></div><p>根据结果看，符合预期的效果！</p>", 
            "topic": [
                {
                    "tag": "Spark", 
                    "tagLink": "https://api.zhihu.com/topics/19942170"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/43499748", 
            "userName": "董可伦", 
            "userLink": "https://www.zhihu.com/people/de888cc734ddbfd918269a8e5fb196ba", 
            "upvote": 1, 
            "title": "spark ML算法之线性回归使用", 
            "content": "<p>我的原创地址：</p><a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/04/09/sparkMlLinearRegressionUsing/\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-0b4e5faa279a5e545d79e754b3dba862_ipico.jpg\" data-image-width=\"640\" data-image-height=\"640\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">spark ML算法之线性回归使用</a><h2>前言</h2><p>本文是讲如何使用spark ml进行线性回归，不涉及线性回归的原理。</p><h2>1、数据格式</h2><h2>1.1 普通标签格式</h2><h2>1.1.1 格式为：</h2><div class=\"highlight\"><pre><code class=\"language-bash\">标签,特征值1 特征值2 特征值3...\n<span class=\"m\">1</span>,1.9\n<span class=\"m\">2</span>,3.1\n<span class=\"m\">3</span>,4\n<span class=\"m\">3</span>.5,4.45\n<span class=\"m\">4</span>,5.02\n<span class=\"m\">9</span>,9.97\n-2,-0.98</code></pre></div><h2>1.1.2 spark 读取</h2><p>1、Rdd  旧版（mllib）的线性回归要求传入的参数类型为RDD[LabeledPoint] </p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.SparkConf</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.SparkContext</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.mllib.linalg.Vectors</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.mllib.regression.LabeledPoint</span>\n\n<span class=\"k\">val</span> <span class=\"n\">data_path</span> <span class=\"k\">=</span> <span class=\"s\">&#34;files/ml/linear_regression_data1.txt&#34;</span>\n<span class=\"k\">val</span> <span class=\"n\">data</span> <span class=\"k\">=</span> <span class=\"n\">sc</span><span class=\"o\">.</span><span class=\"n\">textFile</span><span class=\"o\">(</span><span class=\"n\">data_path</span><span class=\"o\">)</span>\n<span class=\"k\">val</span> <span class=\"n\">training</span> <span class=\"k\">=</span> <span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">map</span> <span class=\"o\">{</span> <span class=\"n\">line</span> <span class=\"k\">=&gt;</span>\n  <span class=\"k\">val</span> <span class=\"n\">arr</span> <span class=\"k\">=</span> <span class=\"n\">line</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"o\">(</span><span class=\"sc\">&#39;,&#39;</span><span class=\"o\">)</span>\n  <span class=\"nc\">LabeledPoint</span><span class=\"o\">(</span><span class=\"n\">arr</span><span class=\"o\">(</span><span class=\"mi\">0</span><span class=\"o\">).</span><span class=\"n\">toDouble</span><span class=\"o\">,</span> <span class=\"nc\">Vectors</span><span class=\"o\">.</span><span class=\"n\">dense</span><span class=\"o\">(</span><span class=\"n\">arr</span><span class=\"o\">(</span><span class=\"mi\">1</span><span class=\"o\">).</span><span class=\"n\">split</span><span class=\"o\">(</span><span class=\"sc\">&#39; &#39;</span><span class=\"o\">).</span><span class=\"n\">map</span><span class=\"o\">(</span><span class=\"k\">_</span><span class=\"o\">.</span><span class=\"n\">toDouble</span><span class=\"o\">)))</span>\n<span class=\"o\">}.</span><span class=\"n\">cache</span><span class=\"o\">()</span>\n<span class=\"n\">training</span><span class=\"o\">.</span><span class=\"n\">foreach</span><span class=\"o\">(</span><span class=\"n\">println</span><span class=\"o\">)</span></code></pre></div><p>结果：</p><div class=\"highlight\"><pre><code class=\"language-text\">(1.0,[1.9])\n(2.0,[3.1])\n(3.0,[4.0])\n(3.5,[4.45])\n(4.0,[5.02])\n(9.0,[9.97])\n(-2.0,[-0.98])</code></pre></div><p>一共有两列，第一列可以通过.label获得（类型为Double），第二列可以通过.features获得（类型为Vector[Double]） 2、 DataFrame 新版（ml）的线性回归要求传入的参数类型为Dataset[_]</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.ml.linalg.Vectors</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.Row</span>\n<span class=\"k\">import</span> <span class=\"nn\">spark.implicits._</span>\n<span class=\"k\">val</span> <span class=\"n\">data_path</span> <span class=\"k\">=</span> <span class=\"s\">&#34;files/ml/linear_regression_data1.txt&#34;</span>\n<span class=\"k\">val</span> <span class=\"n\">data</span> <span class=\"k\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">read</span><span class=\"o\">.</span><span class=\"n\">text</span><span class=\"o\">(</span><span class=\"n\">data_path</span><span class=\"o\">)</span>\n<span class=\"k\">val</span> <span class=\"n\">training</span> <span class=\"k\">=</span> <span class=\"n\">data</span><span class=\"o\">.</span><span class=\"n\">map</span> <span class=\"o\">{</span>\n  <span class=\"k\">case</span> <span class=\"nc\">Row</span><span class=\"o\">(</span><span class=\"n\">line</span><span class=\"k\">:</span> <span class=\"kt\">String</span><span class=\"o\">)</span> <span class=\"k\">=&gt;</span>\n    <span class=\"k\">var</span> <span class=\"n\">arr</span> <span class=\"k\">=</span> <span class=\"n\">line</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"o\">(</span><span class=\"sc\">&#39;,&#39;</span><span class=\"o\">)</span>\n    <span class=\"o\">(</span><span class=\"n\">arr</span><span class=\"o\">(</span><span class=\"mi\">0</span><span class=\"o\">).</span><span class=\"n\">toDouble</span><span class=\"o\">,</span> <span class=\"nc\">Vectors</span><span class=\"o\">.</span><span class=\"n\">dense</span><span class=\"o\">(</span><span class=\"n\">arr</span><span class=\"o\">(</span><span class=\"mi\">1</span><span class=\"o\">).</span><span class=\"n\">split</span><span class=\"o\">(</span><span class=\"sc\">&#39; &#39;</span><span class=\"o\">).</span><span class=\"n\">map</span><span class=\"o\">(</span><span class=\"k\">_</span><span class=\"o\">.</span><span class=\"n\">toDouble</span><span class=\"o\">)))</span>\n<span class=\"o\">}.</span><span class=\"n\">toDF</span><span class=\"o\">(</span><span class=\"s\">&#34;label&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;features&#34;</span><span class=\"o\">)</span>\n<span class=\"n\">training</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"o\">()</span></code></pre></div><p>结果：</p><div class=\"highlight\"><pre><code class=\"language-text\">+-----+--------+\n|label|features|\n+-----+--------+\n|  1.0|   [1.9]|\n|  2.0|   [3.1]|\n|  3.0|   [4.0]|\n|  3.5|  [4.45]|\n|  4.0|  [5.02]|\n|  9.0|  [9.97]|\n| -2.0| [-0.98]|\n+-----+--------+</code></pre></div><p>其中列名&#34;label&#34;, &#34;features&#34;固定，不能改为其他列名。</p><h2>1.2 LIBSVM格式</h2><h2>1.2.1 格式为：</h2><div class=\"highlight\"><pre><code class=\"language-bash\">label index1:value1 index2:value2 ...</code></pre></div><p>其中每一行的index必须为升序 为了便于理解，造几条多维数据：</p><div class=\"highlight\"><pre><code class=\"language-text\">1 1:1.9 2:2 4:2 100:3 101:6\n2 1:3.1 2:2 4:2 100:3 101:6\n3 1:4 2:2 4:2 100:3 101:6\n3.5 1:4.45 2:2 4:2 100:3 101:6\n4 1:5.02 2:2 4:2 100:3 101:6\n9 1:9.97 4:2 100:3 101:6\n-2 1:-0.98 2:2 4:2 100:3 201:6</code></pre></div><h2>1.2.2 spark 读取</h2><p>1、Rdd </p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.SparkConf</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.SparkContext</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.mllib.util.MLUtils</span>\n<span class=\"k\">val</span> <span class=\"n\">data_path</span> <span class=\"k\">=</span> <span class=\"s\">&#34;files/ml/linear_regression_data2.txt&#34;</span>\n<span class=\"k\">val</span> <span class=\"n\">training</span> <span class=\"k\">=</span> <span class=\"nc\">MLUtils</span><span class=\"o\">.</span><span class=\"n\">loadLibSVMFile</span><span class=\"o\">(</span><span class=\"n\">sc</span><span class=\"o\">,</span> <span class=\"n\">data_path</span><span class=\"o\">)</span>\n<span class=\"n\">training</span><span class=\"o\">.</span><span class=\"n\">foreach</span><span class=\"o\">(</span><span class=\"n\">println</span><span class=\"o\">)</span></code></pre></div><p>结果：</p><div class=\"highlight\"><pre><code class=\"language-text\">(1.0,(201,[0,1,3,99,100],[1.9,2.0,2.0,3.0,6.0]))\n(2.0,(201,[0,1,3,99,100],[3.1,2.0,2.0,3.0,6.0]))\n(3.0,(201,[0,1,3,99,100],[4.0,2.0,2.0,3.0,6.0]))\n(3.5,(201,[0,1,3,99,100],[4.45,2.0,2.0,3.0,6.0]))\n(4.0,(201,[0,1,3,99,100],[5.02,2.0,2.0,3.0,6.0]))\n(9.0,(201,[0,3,99,100],[9.97,2.0,3.0,6.0]))\n(-2.0,(201,[0,1,3,99,200],[-0.98,2.0,2.0,3.0,6.0]))</code></pre></div><p>返回类型为RDD[LabeledPoint]，其中第一列为label,第二列vector的第一个值为max(index),第二个index-1组成的数组，第三个为value组成的数组。 2、DataFrame</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">val</span> <span class=\"n\">data_path</span> <span class=\"k\">=</span> <span class=\"s\">&#34;files/ml/linear_regression_data2.txt&#34;</span>\n<span class=\"k\">val</span> <span class=\"n\">data</span> <span class=\"k\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">read</span><span class=\"o\">.</span><span class=\"n\">text</span><span class=\"o\">(</span><span class=\"n\">data_path</span><span class=\"o\">)</span>\n<span class=\"k\">val</span> <span class=\"n\">training</span> <span class=\"k\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">read</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"o\">(</span><span class=\"s\">&#34;libsvm&#34;</span><span class=\"o\">).</span><span class=\"n\">load</span><span class=\"o\">(</span><span class=\"n\">data_path</span><span class=\"o\">)</span>\n<span class=\"n\">training</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"o\">(</span><span class=\"kc\">false</span><span class=\"o\">)</span></code></pre></div><p>结果：</p><div class=\"highlight\"><pre><code class=\"language-text\">+-----+--------------------------------------------+\n|label|features                                    |\n+-----+--------------------------------------------+\n|1.0  |(201,[0,1,3,99,100],[1.9,2.0,2.0,3.0,6.0])  |\n|2.0  |(201,[0,1,3,99,100],[3.1,2.0,2.0,3.0,6.0])  |\n|3.0  |(201,[0,1,3,99,100],[4.0,2.0,2.0,3.0,6.0])  |\n|3.5  |(201,[0,1,3,99,100],[4.45,2.0,2.0,3.0,6.0]) |\n|4.0  |(201,[0,1,3,99,100],[5.02,2.0,2.0,3.0,6.0]) |\n|9.0  |(201,[0,3,99,100],[9.97,2.0,3.0,6.0])       |\n|-2.0 |(201,[0,1,3,99,200],[-0.98,2.0,2.0,3.0,6.0])|\n+-----+--------------------------------------------+</code></pre></div><h2>2、线性回归代码</h2><h2>2.1 数据</h2><p>用libsvm格式的数据：</p><div class=\"highlight\"><pre><code class=\"language-text\">1 1:1.9\n2 1:3.1\n3 1:4\n3.5 1:4.45\n4 1:5.02\n9 1:9.97\n-2 1:-0.98</code></pre></div><h2>2.2 旧版代码</h2><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">package</span> <span class=\"nn\">com.dkl.leanring.spark.ml</span>\n\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.log4j.</span><span class=\"o\">{</span> <span class=\"nc\">Level</span><span class=\"o\">,</span> <span class=\"nc\">Logger</span> <span class=\"o\">}</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.</span><span class=\"o\">{</span> <span class=\"nc\">SparkConf</span><span class=\"o\">,</span> <span class=\"nc\">SparkContext</span> <span class=\"o\">}</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.mllib.regression.LinearRegressionWithSGD</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.mllib.util.MLUtils</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.mllib.regression.LabeledPoint</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.mllib.linalg.Vectors</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.mllib.regression.LinearRegressionModel</span>\n\n<span class=\"k\">object</span> <span class=\"nc\">OldLinearRegression</span> <span class=\"o\">{</span>\n\n  <span class=\"k\">def</span> <span class=\"n\">main</span><span class=\"o\">(</span><span class=\"n\">args</span><span class=\"k\">:</span> <span class=\"kt\">Array</span><span class=\"o\">[</span><span class=\"kt\">String</span><span class=\"o\">])</span> <span class=\"o\">{</span>\n    <span class=\"c1\">// 构建Spark对象\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">conf</span> <span class=\"k\">=</span> <span class=\"k\">new</span> <span class=\"nc\">SparkConf</span><span class=\"o\">().</span><span class=\"n\">setAppName</span><span class=\"o\">(</span><span class=\"s\">&#34;OldLinearRegression&#34;</span><span class=\"o\">).</span><span class=\"n\">setMaster</span><span class=\"o\">(</span><span class=\"s\">&#34;local&#34;</span><span class=\"o\">)</span>\n    <span class=\"k\">val</span> <span class=\"n\">sc</span> <span class=\"k\">=</span> <span class=\"k\">new</span> <span class=\"nc\">SparkContext</span><span class=\"o\">(</span><span class=\"n\">conf</span><span class=\"o\">)</span>\n    <span class=\"nc\">Logger</span><span class=\"o\">.</span><span class=\"n\">getRootLogger</span><span class=\"o\">.</span><span class=\"n\">setLevel</span><span class=\"o\">(</span><span class=\"nc\">Level</span><span class=\"o\">.</span><span class=\"nc\">WARN</span><span class=\"o\">)</span>\n\n    <span class=\"c1\">//读取样本数据\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">data_path</span> <span class=\"k\">=</span> <span class=\"s\">&#34;files/ml/linear_regression_data3.txt&#34;</span>\n    <span class=\"k\">val</span> <span class=\"n\">training</span> <span class=\"k\">=</span> <span class=\"nc\">MLUtils</span><span class=\"o\">.</span><span class=\"n\">loadLibSVMFile</span><span class=\"o\">(</span><span class=\"n\">sc</span><span class=\"o\">,</span> <span class=\"n\">data_path</span><span class=\"o\">)</span>\n    <span class=\"k\">val</span> <span class=\"n\">numTraing</span> <span class=\"k\">=</span> <span class=\"n\">training</span><span class=\"o\">.</span><span class=\"n\">count</span><span class=\"o\">()</span>\n\n    <span class=\"c1\">// 新建线性回归模型，并设置训练参数\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">numIterations</span> <span class=\"k\">=</span> <span class=\"mi\">10000</span>\n    <span class=\"k\">val</span> <span class=\"n\">stepSize</span> <span class=\"k\">=</span> <span class=\"mf\">0.5</span>\n    <span class=\"k\">val</span> <span class=\"n\">miniBatchFraction</span> <span class=\"k\">=</span> <span class=\"mf\">1.0</span>\n\n    <span class=\"c1\">//书上的代码 intercept 永远为0\n</span><span class=\"c1\"></span>    <span class=\"c1\">//val model = LinearRegressionWithSGD.train(examples, numIterations, stepSize, miniBatchFraction)\n</span><span class=\"c1\"></span>    <span class=\"k\">var</span> <span class=\"n\">lr</span> <span class=\"k\">=</span> <span class=\"k\">new</span> <span class=\"nc\">LinearRegressionWithSGD</span><span class=\"o\">().</span><span class=\"n\">setIntercept</span><span class=\"o\">(</span><span class=\"kc\">true</span><span class=\"o\">)</span>\n    <span class=\"n\">lr</span><span class=\"o\">.</span><span class=\"n\">optimizer</span><span class=\"o\">.</span><span class=\"n\">setNumIterations</span><span class=\"o\">(</span><span class=\"n\">numIterations</span><span class=\"o\">).</span><span class=\"n\">setStepSize</span><span class=\"o\">(</span><span class=\"n\">stepSize</span><span class=\"o\">).</span><span class=\"n\">setMiniBatchFraction</span><span class=\"o\">(</span><span class=\"n\">miniBatchFraction</span><span class=\"o\">)</span>\n    <span class=\"k\">val</span> <span class=\"n\">model</span> <span class=\"k\">=</span> <span class=\"n\">lr</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"o\">(</span><span class=\"n\">training</span><span class=\"o\">)</span>\n    <span class=\"n\">println</span><span class=\"o\">(</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">weights</span><span class=\"o\">)</span>\n    <span class=\"n\">println</span><span class=\"o\">(</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">intercept</span><span class=\"o\">)</span>\n\n    <span class=\"c1\">// 对样本进行测试\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">prediction</span> <span class=\"k\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"o\">(</span><span class=\"n\">training</span><span class=\"o\">.</span><span class=\"n\">map</span><span class=\"o\">(</span><span class=\"k\">_</span><span class=\"o\">.</span><span class=\"n\">features</span><span class=\"o\">))</span>\n    <span class=\"k\">val</span> <span class=\"n\">predictionAndLabel</span> <span class=\"k\">=</span> <span class=\"n\">prediction</span><span class=\"o\">.</span><span class=\"n\">zip</span><span class=\"o\">(</span><span class=\"n\">training</span><span class=\"o\">.</span><span class=\"n\">map</span><span class=\"o\">(</span><span class=\"k\">_</span><span class=\"o\">.</span><span class=\"n\">label</span><span class=\"o\">))</span>\n    <span class=\"k\">val</span> <span class=\"n\">print_predict</span> <span class=\"k\">=</span> <span class=\"n\">predictionAndLabel</span><span class=\"o\">.</span><span class=\"n\">take</span><span class=\"o\">(</span><span class=\"mi\">20</span><span class=\"o\">)</span>\n    <span class=\"n\">println</span><span class=\"o\">(</span><span class=\"s\">&#34;prediction&#34;</span> <span class=\"o\">+</span> <span class=\"s\">&#34;\\t&#34;</span> <span class=\"o\">+</span> <span class=\"s\">&#34;label&#34;</span><span class=\"o\">)</span>\n    <span class=\"k\">for</span> <span class=\"o\">(</span><span class=\"n\">i</span> <span class=\"k\">&lt;-</span> <span class=\"mi\">0</span> <span class=\"n\">to</span> <span class=\"n\">print_predict</span><span class=\"o\">.</span><span class=\"n\">length</span> <span class=\"o\">-</span> <span class=\"mi\">1</span><span class=\"o\">)</span> <span class=\"o\">{</span>\n      <span class=\"n\">println</span><span class=\"o\">(</span><span class=\"n\">print_predict</span><span class=\"o\">(</span><span class=\"n\">i</span><span class=\"o\">).</span><span class=\"n\">_1</span> <span class=\"o\">+</span> <span class=\"s\">&#34;\\t&#34;</span> <span class=\"o\">+</span> <span class=\"n\">print_predict</span><span class=\"o\">(</span><span class=\"n\">i</span><span class=\"o\">).</span><span class=\"n\">_2</span><span class=\"o\">)</span>\n    <span class=\"o\">}</span>\n    <span class=\"c1\">// 计算测试误差\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">loss</span> <span class=\"k\">=</span> <span class=\"n\">predictionAndLabel</span><span class=\"o\">.</span><span class=\"n\">map</span> <span class=\"o\">{</span>\n      <span class=\"k\">case</span> <span class=\"o\">(</span><span class=\"n\">p</span><span class=\"o\">,</span> <span class=\"n\">l</span><span class=\"o\">)</span> <span class=\"k\">=&gt;</span>\n        <span class=\"k\">val</span> <span class=\"n\">err</span> <span class=\"k\">=</span> <span class=\"n\">p</span> <span class=\"o\">-</span> <span class=\"n\">l</span>\n        <span class=\"n\">err</span> <span class=\"o\">*</span> <span class=\"n\">err</span>\n    <span class=\"o\">}.</span><span class=\"n\">reduce</span><span class=\"o\">(</span><span class=\"k\">_</span> <span class=\"o\">+</span> <span class=\"k\">_</span><span class=\"o\">)</span>\n    <span class=\"k\">val</span> <span class=\"n\">rmse</span> <span class=\"k\">=</span> <span class=\"n\">math</span><span class=\"o\">.</span><span class=\"n\">sqrt</span><span class=\"o\">(</span><span class=\"n\">loss</span> <span class=\"o\">/</span> <span class=\"n\">numTraing</span><span class=\"o\">)</span>\n    <span class=\"n\">println</span><span class=\"o\">(</span><span class=\"s\">s&#34;Test RMSE = </span><span class=\"si\">$rmse</span><span class=\"s\">.&#34;</span><span class=\"o\">)</span>\n\n  <span class=\"o\">}</span>\n\n<span class=\"o\">}</span></code></pre></div><p>其中注释的第30行代码为书上的写法，但这样写intercept一直为0，也就是只适用于y=a*x的形式，不适用于y=ax+b,改为31、32替代即可。</p><p>结果：</p><div class=\"highlight\"><pre><code class=\"language-text\">[0.992894785953067]\n-0.9446037936869749\nprediction  label\n0.9418962996238525  1.0\n2.133370042767533   2.0\n3.0269753501252934  3.0\n3.473778003804174   3.5\n4.039728031797421   4.0\n8.954557222265104   9.0\n-1.9176406839209805 -2.0\nTest RMSE = 0.06866615969192089.</code></pre></div><p>即a=0.992894785953067,b=-0.9446037936869749,y=0.992894785953067*x-0.9446037936869749</p><h2>2.2 新版代码</h2><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">package</span> <span class=\"nn\">com.dkl.leanring.spark.ml</span>\n\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.ml.regression.LinearRegression</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.SparkSession</span>\n\n<span class=\"k\">object</span> <span class=\"nc\">NewLinearRegression</span> <span class=\"o\">{</span>\n\n  <span class=\"k\">def</span> <span class=\"n\">main</span><span class=\"o\">(</span><span class=\"n\">args</span><span class=\"k\">:</span> <span class=\"kt\">Array</span><span class=\"o\">[</span><span class=\"kt\">String</span><span class=\"o\">])</span><span class=\"k\">:</span> <span class=\"kt\">Unit</span> <span class=\"o\">=</span> <span class=\"o\">{</span>\n    <span class=\"k\">val</span> <span class=\"n\">spark</span> <span class=\"k\">=</span> <span class=\"nc\">SparkSession</span>\n      <span class=\"o\">.</span><span class=\"n\">builder</span>\n      <span class=\"o\">.</span><span class=\"n\">appName</span><span class=\"o\">(</span><span class=\"s\">&#34;NewLinearRegression&#34;</span><span class=\"o\">)</span>\n      <span class=\"o\">.</span><span class=\"n\">master</span><span class=\"o\">(</span><span class=\"s\">&#34;local&#34;</span><span class=\"o\">)</span>\n      <span class=\"o\">.</span><span class=\"n\">getOrCreate</span><span class=\"o\">()</span>\n    <span class=\"k\">val</span> <span class=\"n\">data_path</span> <span class=\"k\">=</span> <span class=\"s\">&#34;files/ml/linear_regression_data3.txt&#34;</span>\n\n    <span class=\"k\">import</span> <span class=\"nn\">spark.implicits._</span>\n    <span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.ml.linalg.Vectors</span>\n    <span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.Row</span>\n    <span class=\"k\">val</span> <span class=\"n\">training</span> <span class=\"k\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">read</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"o\">(</span><span class=\"s\">&#34;libsvm&#34;</span><span class=\"o\">).</span><span class=\"n\">load</span><span class=\"o\">(</span><span class=\"n\">data_path</span><span class=\"o\">)</span>\n\n    <span class=\"k\">val</span> <span class=\"n\">lr</span> <span class=\"k\">=</span> <span class=\"k\">new</span> <span class=\"nc\">LinearRegression</span><span class=\"o\">()</span>\n      <span class=\"o\">.</span><span class=\"n\">setMaxIter</span><span class=\"o\">(</span><span class=\"mi\">10000</span><span class=\"o\">)</span>\n      <span class=\"o\">.</span><span class=\"n\">setRegParam</span><span class=\"o\">(</span><span class=\"mf\">0.3</span><span class=\"o\">)</span>\n      <span class=\"o\">.</span><span class=\"n\">setElasticNetParam</span><span class=\"o\">(</span><span class=\"mf\">0.8</span><span class=\"o\">)</span>\n\n    <span class=\"k\">val</span> <span class=\"n\">lrModel</span> <span class=\"k\">=</span> <span class=\"n\">lr</span><span class=\"o\">.</span><span class=\"n\">fit</span><span class=\"o\">(</span><span class=\"n\">training</span><span class=\"o\">)</span>\n\n    <span class=\"n\">println</span><span class=\"o\">(</span><span class=\"s\">s&#34;Coefficients: </span><span class=\"si\">${</span><span class=\"n\">lrModel</span><span class=\"o\">.</span><span class=\"n\">coefficients</span><span class=\"si\">}</span><span class=\"s\"> Intercept: </span><span class=\"si\">${</span><span class=\"n\">lrModel</span><span class=\"o\">.</span><span class=\"n\">intercept</span><span class=\"si\">}</span><span class=\"s\">&#34;</span><span class=\"o\">)</span>\n\n    <span class=\"k\">val</span> <span class=\"n\">trainingSummary</span> <span class=\"k\">=</span> <span class=\"n\">lrModel</span><span class=\"o\">.</span><span class=\"n\">summary</span>\n    <span class=\"n\">println</span><span class=\"o\">(</span><span class=\"s\">s&#34;numIterations: </span><span class=\"si\">${</span><span class=\"n\">trainingSummary</span><span class=\"o\">.</span><span class=\"n\">totalIterations</span><span class=\"si\">}</span><span class=\"s\">&#34;</span><span class=\"o\">)</span>\n    <span class=\"n\">println</span><span class=\"o\">(</span><span class=\"s\">s&#34;objectiveHistory: [</span><span class=\"si\">${</span><span class=\"n\">trainingSummary</span><span class=\"o\">.</span><span class=\"n\">objectiveHistory</span><span class=\"o\">.</span><span class=\"n\">mkString</span><span class=\"o\">(</span><span class=\"s\">&#34;,&#34;</span><span class=\"o\">)</span><span class=\"si\">}</span><span class=\"s\">]&#34;</span><span class=\"o\">)</span>\n    <span class=\"n\">trainingSummary</span><span class=\"o\">.</span><span class=\"n\">residuals</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"o\">()</span>\n    <span class=\"n\">println</span><span class=\"o\">(</span><span class=\"s\">s&#34;RMSE: </span><span class=\"si\">${</span><span class=\"n\">trainingSummary</span><span class=\"o\">.</span><span class=\"n\">rootMeanSquaredError</span><span class=\"si\">}</span><span class=\"s\">&#34;</span><span class=\"o\">)</span>\n    <span class=\"n\">println</span><span class=\"o\">(</span><span class=\"s\">s&#34;r2: </span><span class=\"si\">${</span><span class=\"n\">trainingSummary</span><span class=\"o\">.</span><span class=\"n\">r2</span><span class=\"si\">}</span><span class=\"s\">&#34;</span><span class=\"o\">)</span>\n    <span class=\"n\">trainingSummary</span><span class=\"o\">.</span><span class=\"n\">predictions</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"o\">()</span>\n\n    <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">stop</span><span class=\"o\">()</span>\n  <span class=\"o\">}</span>\n<span class=\"o\">}</span></code></pre></div><p>结果：</p><div class=\"highlight\"><pre><code class=\"language-text\">Coefficients: [0.9072296333951224] Intercept: -0.630360819004294\nnumIterations: 3\nobjectiveHistory: [0.5,0.41543560544030766,0.08269406021049913]\n+--------------------+\n|           residuals|\n+--------------------+\n| -0.0933754844464385|\n|-0.18205104452058585|\n|0.001442285423804...|\n| 0.09318895039599973|\n| 0.07606805936077965|\n|  0.5852813740549223|\n| -0.4805541402684861|\n+--------------------+\n\nRMSE: 0.2999573166705823\nr2: 0.9906296595124621\n+-----+---------------+------------------+\n|label|       features|        prediction|\n+-----+---------------+------------------+\n|  1.0|  (1,[0],[1.9])|1.0933754844464385|\n|  2.0|  (1,[0],[3.1])| 2.182051044520586|\n|  3.0|  (1,[0],[4.0])|2.9985577145761955|\n|  3.5| (1,[0],[4.45])|3.4068110496040003|\n|  4.0| (1,[0],[5.02])|3.9239319406392204|\n|  9.0| (1,[0],[9.97])| 8.414718625945078|\n| -2.0|(1,[0],[-0.98])|-1.519445859731514|\n+-----+---------------+------------------+</code></pre></div><p></p>", 
            "topic": [
                {
                    "tag": "Spark", 
                    "tagLink": "https://api.zhihu.com/topics/19942170"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "线性回归", 
                    "tagLink": "https://api.zhihu.com/topics/19650500"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/43499624", 
            "userName": "董可伦", 
            "userLink": "https://www.zhihu.com/people/de888cc734ddbfd918269a8e5fb196ba", 
            "upvote": 1, 
            "title": "spark 统计每天新增用户数", 
            "content": "<p></p><p>我的原创地址：</p><a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/04/11/sparkNewUV/\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic4.zhimg.com/v2-dcd2fc183e08361aec84037c75953643_180x120.jpg\" data-image-width=\"1448\" data-image-height=\"663\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">spark 统计每天新增用户数</a><h2>前言</h2><p>本文源自一位群友的一道美团面试题，解题思路（基于倒排索引）和代码都是这位大佬（相对于尚处于小白阶段的我）写的，我只是在基于倒排索引的基础上帮忙想出了最后一步思路，感觉这个解题思路不错，值得记录一下。</p><h2>1、原始数据</h2><div class=\"highlight\"><pre><code class=\"language-text\">2017-01-01  a\n2017-01-01  b\n2017-01-01  c\n2017-01-02  a\n2017-01-02  b\n2017-01-02  d\n2017-01-03  b\n2017-01-03  e\n2017-01-03  f</code></pre></div><p>根据数据可以看出我们要求的结果为： 2017-01-01 新增三个用户（a,b,c） 2017-01-02 新增一个用户（d） 2017-01-03 新增两个用户（e，f） </p><h2>2、解题思路</h2><h2>2.1 对原始数据进行倒排索引</h2><p>结果如下：</p><p>用户名 | 列一 | 列二 | 列三 - | :-:  | :-:  | :-:  a | 2017-01-01 | 2017-01-02 | b | 2017-01-01 | 2017-01-02 | 2017-01-03 c | 2017-01-01 |            | d | 2017-01-02 |            |<br/> e | 2017-01-03 |            | f | 2017-01-03 |            |</p><h2>2.2 统计列一中每个日期出现的次数</h2><p>这样我们只看列一，统计每个日期在列一出现的次数，即为对应日期新增用户数。</p><h2>3、代码</h2><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">package</span> <span class=\"nn\">com.dkl.leanring.spark.test</span>\n\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.SparkSession</span>\n\n<span class=\"k\">object</span> <span class=\"nc\">NewUVDemo</span> <span class=\"o\">{</span>\n  <span class=\"k\">def</span> <span class=\"n\">main</span><span class=\"o\">(</span><span class=\"n\">args</span><span class=\"k\">:</span> <span class=\"kt\">Array</span><span class=\"o\">[</span><span class=\"kt\">String</span><span class=\"o\">])</span><span class=\"k\">:</span> <span class=\"kt\">Unit</span> <span class=\"o\">=</span> <span class=\"o\">{</span>\n    <span class=\"k\">val</span> <span class=\"n\">spark</span> <span class=\"k\">=</span> <span class=\"nc\">SparkSession</span><span class=\"o\">.</span><span class=\"n\">builder</span><span class=\"o\">().</span><span class=\"n\">appName</span><span class=\"o\">(</span><span class=\"s\">&#34;NewUVDemo&#34;</span><span class=\"o\">).</span><span class=\"n\">master</span><span class=\"o\">(</span><span class=\"s\">&#34;local&#34;</span><span class=\"o\">).</span><span class=\"n\">getOrCreate</span><span class=\"o\">()</span>\n    <span class=\"k\">val</span> <span class=\"n\">rdd1</span> <span class=\"k\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">sparkContext</span><span class=\"o\">.</span><span class=\"n\">parallelize</span><span class=\"o\">(</span>\n      <span class=\"nc\">Array</span><span class=\"o\">(</span>\n        <span class=\"o\">(</span><span class=\"s\">&#34;2017-01-01&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;a&#34;</span><span class=\"o\">),</span> <span class=\"o\">(</span><span class=\"s\">&#34;2017-01-01&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;b&#34;</span><span class=\"o\">),</span> <span class=\"o\">(</span><span class=\"s\">&#34;2017-01-01&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;c&#34;</span><span class=\"o\">),</span>\n        <span class=\"o\">(</span><span class=\"s\">&#34;2017-01-02&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;a&#34;</span><span class=\"o\">),</span> <span class=\"o\">(</span><span class=\"s\">&#34;2017-01-02&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;b&#34;</span><span class=\"o\">),</span> <span class=\"o\">(</span><span class=\"s\">&#34;2017-01-02&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;d&#34;</span><span class=\"o\">),</span>\n        <span class=\"o\">(</span><span class=\"s\">&#34;2017-01-03&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;b&#34;</span><span class=\"o\">),</span> <span class=\"o\">(</span><span class=\"s\">&#34;2017-01-03&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;e&#34;</span><span class=\"o\">),</span> <span class=\"o\">(</span><span class=\"s\">&#34;2017-01-03&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;f&#34;</span><span class=\"o\">)))</span>\n    <span class=\"c1\">//倒排\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">rdd2</span> <span class=\"k\">=</span> <span class=\"n\">rdd1</span><span class=\"o\">.</span><span class=\"n\">map</span><span class=\"o\">(</span><span class=\"n\">kv</span> <span class=\"k\">=&gt;</span> <span class=\"o\">(</span><span class=\"n\">kv</span><span class=\"o\">.</span><span class=\"n\">_2</span><span class=\"o\">,</span> <span class=\"n\">kv</span><span class=\"o\">.</span><span class=\"n\">_1</span><span class=\"o\">))</span>\n    <span class=\"c1\">//倒排后的key分组\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">rdd3</span> <span class=\"k\">=</span> <span class=\"n\">rdd2</span><span class=\"o\">.</span><span class=\"n\">groupByKey</span><span class=\"o\">()</span>\n    <span class=\"c1\">//取最小时间\n</span><span class=\"c1\"></span>    <span class=\"k\">val</span> <span class=\"n\">rdd4</span> <span class=\"k\">=</span> <span class=\"n\">rdd3</span><span class=\"o\">.</span><span class=\"n\">map</span><span class=\"o\">(</span><span class=\"n\">kv</span> <span class=\"k\">=&gt;</span> <span class=\"o\">(</span><span class=\"n\">kv</span><span class=\"o\">.</span><span class=\"n\">_2</span><span class=\"o\">.</span><span class=\"n\">min</span><span class=\"o\">,</span> <span class=\"mi\">1</span><span class=\"o\">))</span>\n    <span class=\"n\">rdd4</span><span class=\"o\">.</span><span class=\"n\">countByKey</span><span class=\"o\">().</span><span class=\"n\">foreach</span><span class=\"o\">(</span><span class=\"n\">println</span><span class=\"o\">)</span>\n  <span class=\"o\">}</span>\n<span class=\"o\">}</span></code></pre></div><p>结果：</p><div class=\"highlight\"><pre><code class=\"language-text\">(2017-01-03,2)\n(2017-01-02,1)\n(2017-01-01,3)</code></pre></div><p>附图： </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-d2f85adeca22b2782541a7d44553b375_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1448\" data-rawheight=\"663\" class=\"origin_image zh-lightbox-thumb\" width=\"1448\" data-original=\"https://pic2.zhimg.com/v2-d2f85adeca22b2782541a7d44553b375_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1448&#39; height=&#39;663&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1448\" data-rawheight=\"663\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1448\" data-original=\"https://pic2.zhimg.com/v2-d2f85adeca22b2782541a7d44553b375_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-d2f85adeca22b2782541a7d44553b375_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "Spark", 
                    "tagLink": "https://api.zhihu.com/topics/19942170"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/43499169", 
            "userName": "董可伦", 
            "userLink": "https://www.zhihu.com/people/de888cc734ddbfd918269a8e5fb196ba", 
            "upvote": 0, 
            "title": "spark-submit报错：没有合适驱动", 
            "content": "<p>我的原创地址：</p><a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/05/06/sparkSubmitException/\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-0b4e5faa279a5e545d79e754b3dba862_ipico.jpg\" data-image-width=\"640\" data-image-height=\"640\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">spark-submit报错:Exception in thread &#34;main&#34; java.sql.SQLException:No suitable driver</a><h2>前言</h2><p>最近写了一个用spark连接oracle,然后将mysql所有的表保存到hive中的程序，在本地eclipse里运行没有问题，想在集群上跑一下，看看在集群上性能如何，但是用spark-submit 提交程序时抛出一个异常Exception in thread &#34;main&#34; java.sql.SQLException: No suitable driver，一开始以为spark-submit提交时找不到oracle 驱动jar,折腾了半天才发现是代码问题。</p><h2>1、猜测是否是缺失oracle驱动</h2><p>由于在本地没有问题，所以不会想到是代码问题，根据提示想到的是spark-submit找不到oracle驱动，因为maven或sbt仓库里没有oracle驱动，在本地跑的时候，是将oracle驱动下载到本地，然后在eclipse设置build path就可以了。</p><p>但是我在spark-submit 里已经通过--jars 加载oracle驱动了：</p><div class=\"highlight\"><pre><code class=\"language-bash\">spark-submit --class com.dkl.leanring.spark.sql.Oracle2HiveDemo --jars ojdbc5-11.2.0.3.jar spark-scala_2.11-1.0.jar</code></pre></div><p>开始以为自己用法不对，但是上网搜了一下，发现就是这么用的~ 然后尝试用--driver-class-path、--driver-library-path等都没成功。 </p><h2>2、sbt-assembly打包</h2><p>网上查的sbt-assembly打包可以将所有依赖的jar包包括你写的代码全部打包在一起，于是尝试了一下 首先在项目目录中project/plugins.sbt添加</p><div class=\"highlight\"><pre><code class=\"language-text\">addSbtPlugin(&#34;com.eed3si9n&#34; % &#34;sbt-assembly&#34; % &#34;0.14.5&#34;)\nresolvers += Resolver.url(&#34;bintray-sbt-plugins&#34;, url(&#34;http://dl.bintray.com/sbt/sbt-plugin-releases&#34;))(Resolver.ivyStylePatterns)</code></pre></div><p>其中0.14.5为版本号，需要与自己sbt对应上，否则报错，可在<a href=\"https://link.zhihu.com/?target=http%3A//dl.bintray.com/sbt/sbt-plugin-releases/com.eed3si9n/sbt-assembly/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">dl.bintray.com/sbt/sbt-</span><span class=\"invisible\">plugin-releases/com.eed3si9n/sbt-assembly/</span><span class=\"ellipsis\"></span></a>查看版本 然后在项目对应目录下执行sbt,然后输入plugins，即可看到sbtassembly插件了，如下：</p><div class=\"highlight\"><pre><code class=\"language-text\">$ sbt\n[info] Loading settings from plugins.sbt ...\n[info] Loading global plugins from C:\\Users\\14123\\.sbt\\1.0\\plugins\n[info] Loading settings from assembly.sbt,plugins.sbt ...\n[info] Loading project definition from D:\\workspace\\spark-scala\\project\n[info] Loading settings from spark-scala.sbt ...\n[info] Set current project to spark-scala (in build file:/D:/workspace/spark-scala/)\n[info] sbt server started at local:sbt-server-8b6c904d40b181717b3f\nsbt:spark-scala&gt; plugins\nIn file:/D:/workspace/spark-scala/\n        sbt.plugins.IvyPlugin: enabled in spark-scala\n        sbt.plugins.JvmPlugin: enabled in spark-scala\n        sbt.plugins.CorePlugin: enabled in spark-scala\n        sbt.plugins.JUnitXmlReportPlugin: enabled in spark-scala\n        sbt.plugins.Giter8TemplatePlugin: enabled in spark-scala\n        com.typesafe.sbteclipse.plugin.EclipsePlugin: enabled in spark-scala\n        sbtassembly.AssemblyPlugin: enabled in spark-scala\nsbt:spark-scala&gt;</code></pre></div><p>但是这样执行sbt-assembly打包会报错，需要解决jar包冲突(deduplicate)问题 在项目的bulid.sbt里添加如下即可（只是其中一种解决策略，可根据自己项目实际情况自己设定）</p><div class=\"highlight\"><pre><code class=\"language-text\">assemblyMergeStrategy in assembly := {\n    case m if m.toLowerCase.endsWith(&#34;manifest.mf&#34;) =&gt; MergeStrategy.discard\n    case m if m.startsWith(&#34;META-INF&#34;) =&gt; MergeStrategy.discard\n    case PathList(&#34;javax&#34;, &#34;servlet&#34;, xs @ _*) =&gt; MergeStrategy.first\n    case PathList(&#34;org&#34;, &#34;apache&#34;, xs @ _*) =&gt; MergeStrategy.first\n    case PathList(&#34;org&#34;, &#34;jboss&#34;, xs @ _*) =&gt; MergeStrategy.first\n    case &#34;about.html&#34;  =&gt; MergeStrategy.rename\n    case &#34;reference.conf&#34; =&gt; MergeStrategy.concat\n    case _ =&gt; MergeStrategy.first\n}</code></pre></div><p>其中不同旧版本和新版本sbt写法不一样，具体可上网看一下别人的博客或者在官网查看。</p><p>这样就可以sbt-assembly进行打包了，发现这样打的jar包确实很大，用sbt package打的jar包大小1.48MB,sbt-assembly打的jar包大小194MB,将spark-scala-assembly-1.0.jar上传到服务器，然后执行submit,发现还是报同样的错，查看一下sbt-assembly日志，发现确实将oracle驱动加载上了~</p><h2>3、真正原因</h2><p>这样就猜想不是缺少oracle驱动，于是上网查了好多，偶然发现可能是代码问题，下面是我写的从oracle取数的部分代码</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">val</span> <span class=\"n\">allTablesDF</span> <span class=\"k\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">read</span>\n  <span class=\"o\">.</span><span class=\"n\">format</span><span class=\"o\">(</span><span class=\"s\">&#34;jdbc&#34;</span><span class=\"o\">)</span>\n  <span class=\"o\">.</span><span class=\"n\">option</span><span class=\"o\">(</span><span class=\"s\">&#34;url&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;jdbc:oracle:thin:@192.168.44.128:1521:orcl&#34;</span><span class=\"o\">)</span>\n  <span class=\"o\">.</span><span class=\"n\">option</span><span class=\"o\">(</span><span class=\"s\">&#34;dbtable&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;(select table_name,owner from all_tables where  owner  in(&#39;BIGDATA&#39;))a&#34;</span><span class=\"o\">)</span>\n  <span class=\"o\">.</span><span class=\"n\">option</span><span class=\"o\">(</span><span class=\"s\">&#34;user&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;bigdata&#34;</span><span class=\"o\">)</span>\n  <span class=\"o\">.</span><span class=\"n\">option</span><span class=\"o\">(</span><span class=\"s\">&#34;password&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;bigdata&#34;</span><span class=\"o\">)</span>\n  <span class=\"o\">.</span><span class=\"n\">load</span><span class=\"o\">()</span></code></pre></div><p>写法和我之前写的spark连接mysql的博客里的写法是一样的：<a href=\"https://link.zhihu.com/?target=http%3A//dongkelun.com/2018/03/21/sparkMysql/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Spark Sql 连接mysql</a> 这样写在eclipse运行是没问题的，但是在spark-submit提交时是不行的，需要加上驱动信息</p><div class=\"highlight\"><pre><code class=\"language-text\">.option(&#34;driver&#34;, &#34;oracle.jdbc.driver.OracleDriver&#34;)</code></pre></div><p>重新打包，再运行，发现果然没问题</p><h2>4、总结</h2><h2>4.1</h2><p>其实在用spark提交之前写的spark连接mysql的程序也会报统一的错(如果$SPARK_HOME/jars没有mysql驱动)，和oracle驱动不在sbt仓库里没关系。 但是之前在spark-shell里测试spark连接hive时已经将mysql驱动拷贝过去了，所以mysql没有报错</p><h2>4.2</h2><p>在代码里加上driver之后再提交如果没有oracle驱动会报不同的错</p><div class=\"highlight\"><pre><code class=\"language-text\">Exception in thread &#34;main&#34; java.lang.ClassNotFoundException: oracle.jdbc.driver.OracleDriver</code></pre></div><h2>4.3</h2><p>通过--jars指定jar和sbt assembly打包都可以，看自己习惯，但通过--jars需要注意先后顺序，如果是多个jar以逗号隔开即可 正确：</p><div class=\"highlight\"><pre><code class=\"language-bash\">spark-submit --class com.dkl.leanring.spark.sql.Oracle2HiveDemo --jars ojdbc5-11.2.0.3.jar spark-scala_2.11-1.0.jar \nspark-submit --jars ojdbc5-11.2.0.3.jar --class com.dkl.leanring.spark.sql.Oracle2HiveDemo  spark-scala_2.11-1.0.jar</code></pre></div><p>错误：</p><div class=\"highlight\"><pre><code class=\"language-bash\">spark-submit --class com.dkl.leanring.spark.sql.Oracle2HiveDemo  spark-scala_2.11-1.0.jar --jars ojdbc5-11.2.0.3.jar\nspark-submit --jars ojdbc5-11.2.0.3.jar spark-scala_2.11-1.0.jar --class com.dkl.leanring.spark.sql.Oracle2HiveDemo \n...</code></pre></div><p>也就是通过sbt package和sbt assembly生成的项目jar包一定要放在最后面</p><h2>4.4</h2><p>通过--driver-class-path也可以实现加载额外的jar</p><div class=\"highlight\"><pre><code class=\"language-bash\">spark-submit --class com.dkl.leanring.spark.sql.Oracle2HiveDemo --driver-class-path lib/*  spark-scala_2.11-1.0.jar</code></pre></div><p>但是这样lib下只能一个jar,两个jar就会报错，不知道什么原因~</p><h2>4.5</h2><p>将oracle驱动拷贝到$SPARK_HOME/jars,就可以不在代码里指定driver选项了，而且也不用通过--jars添加oracle驱动，一劳永逸.</p><div class=\"highlight\"><pre><code class=\"language-bash\">cp ojdbc5-11.2.0.3.jar <span class=\"nv\">$SPARK_HOME</span>/jars\nspark-submit --class com.dkl.leanring.spark.sql.Oracle2HiveDemo  spark-scala_2.11-1.0.jar</code></pre></div><p>具体这样设置可根据实际情况和偏好习惯使用。</p><h2>附完整代码（测试用）</h2><p>比较简单就不加注释~</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">package</span> <span class=\"nn\">com.dkl.leanring.spark.sql</span>\n\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.SparkSession</span>\n\n<span class=\"k\">object</span> <span class=\"nc\">Oracle2HiveDemo</span> <span class=\"o\">{</span>\n\n  <span class=\"k\">def</span> <span class=\"n\">main</span><span class=\"o\">(</span><span class=\"n\">args</span><span class=\"k\">:</span> <span class=\"kt\">Array</span><span class=\"o\">[</span><span class=\"kt\">String</span><span class=\"o\">])</span><span class=\"k\">:</span> <span class=\"kt\">Unit</span> <span class=\"o\">=</span> <span class=\"o\">{</span>\n    <span class=\"k\">val</span> <span class=\"n\">spark</span> <span class=\"k\">=</span> <span class=\"nc\">SparkSession</span>\n      <span class=\"o\">.</span><span class=\"n\">builder</span><span class=\"o\">()</span>\n      <span class=\"o\">.</span><span class=\"n\">appName</span><span class=\"o\">(</span><span class=\"s\">&#34;Oracle2HiveDemo&#34;</span><span class=\"o\">)</span>\n      <span class=\"o\">.</span><span class=\"n\">master</span><span class=\"o\">(</span><span class=\"s\">&#34;local&#34;</span><span class=\"o\">)</span>\n      <span class=\"o\">.</span><span class=\"n\">enableHiveSupport</span><span class=\"o\">()</span>\n      <span class=\"o\">.</span><span class=\"n\">getOrCreate</span><span class=\"o\">()</span>\n\n    <span class=\"k\">val</span> <span class=\"n\">allTablesDF</span> <span class=\"k\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">read</span>\n      <span class=\"o\">.</span><span class=\"n\">format</span><span class=\"o\">(</span><span class=\"s\">&#34;jdbc&#34;</span><span class=\"o\">)</span>\n      <span class=\"o\">.</span><span class=\"n\">option</span><span class=\"o\">(</span><span class=\"s\">&#34;url&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;jdbc:oracle:thin:@192.168.44.128:1521:orcl&#34;</span><span class=\"o\">)</span>\n      <span class=\"o\">.</span><span class=\"n\">option</span><span class=\"o\">(</span><span class=\"s\">&#34;dbtable&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;(select table_name,owner from all_tables where  owner  in(&#39;BIGDATA&#39;))a&#34;</span><span class=\"o\">)</span>\n      <span class=\"o\">.</span><span class=\"n\">option</span><span class=\"o\">(</span><span class=\"s\">&#34;user&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;bigdata&#34;</span><span class=\"o\">)</span>\n      <span class=\"o\">.</span><span class=\"n\">option</span><span class=\"o\">(</span><span class=\"s\">&#34;password&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;bigdata&#34;</span><span class=\"o\">)</span>\n      <span class=\"o\">.</span><span class=\"n\">option</span><span class=\"o\">(</span><span class=\"s\">&#34;driver&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;oracle.jdbc.driver.OracleDriver&#34;</span><span class=\"o\">)</span>\n      <span class=\"o\">.</span><span class=\"n\">load</span><span class=\"o\">()</span>\n    <span class=\"k\">import</span> <span class=\"nn\">spark.implicits._</span>\n    <span class=\"k\">import</span> <span class=\"nn\">spark.sql</span>\n    <span class=\"n\">sql</span><span class=\"o\">(</span><span class=\"s\">&#34;use oracle_test&#34;</span><span class=\"o\">)</span>\n    <span class=\"n\">allTablesDF</span><span class=\"o\">.</span><span class=\"n\">rdd</span><span class=\"o\">.</span><span class=\"n\">collect</span><span class=\"o\">().</span><span class=\"n\">foreach</span><span class=\"o\">(</span><span class=\"n\">row</span> <span class=\"k\">=&gt;</span> <span class=\"o\">{</span>\n      <span class=\"k\">val</span> <span class=\"n\">tableName</span><span class=\"k\">:</span> <span class=\"kt\">String</span> <span class=\"o\">=</span> <span class=\"n\">row</span><span class=\"o\">(</span><span class=\"mi\">0</span><span class=\"o\">).</span><span class=\"n\">toString</span><span class=\"o\">()</span>\n      <span class=\"k\">val</span> <span class=\"n\">dataBase</span><span class=\"k\">:</span> <span class=\"kt\">String</span> <span class=\"o\">=</span> <span class=\"n\">row</span><span class=\"o\">(</span><span class=\"mi\">1</span><span class=\"o\">).</span><span class=\"n\">toString</span><span class=\"o\">()</span>\n\n      <span class=\"n\">println</span><span class=\"o\">(</span><span class=\"n\">dataBase</span> <span class=\"o\">+</span> <span class=\"s\">&#34;.&#34;</span> <span class=\"o\">+</span> <span class=\"n\">tableName</span><span class=\"o\">)</span>\n      <span class=\"k\">val</span> <span class=\"n\">df</span> <span class=\"k\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">read</span>\n        <span class=\"o\">.</span><span class=\"n\">format</span><span class=\"o\">(</span><span class=\"s\">&#34;jdbc&#34;</span><span class=\"o\">)</span>\n        <span class=\"o\">.</span><span class=\"n\">option</span><span class=\"o\">(</span><span class=\"s\">&#34;url&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;jdbc:oracle:thin:@192.168.44.128:1521:orcl&#34;</span><span class=\"o\">)</span>\n        <span class=\"o\">.</span><span class=\"n\">option</span><span class=\"o\">(</span><span class=\"s\">&#34;dbtable&#34;</span><span class=\"o\">,</span> <span class=\"n\">dataBase</span> <span class=\"o\">+</span> <span class=\"s\">&#34;.&#34;</span> <span class=\"o\">+</span> <span class=\"n\">tableName</span><span class=\"o\">)</span>\n        <span class=\"o\">.</span><span class=\"n\">option</span><span class=\"o\">(</span><span class=\"s\">&#34;user&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;bigdata&#34;</span><span class=\"o\">)</span>\n        <span class=\"o\">.</span><span class=\"n\">option</span><span class=\"o\">(</span><span class=\"s\">&#34;password&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;bigdata&#34;</span><span class=\"o\">)</span>\n        <span class=\"o\">.</span><span class=\"n\">option</span><span class=\"o\">(</span><span class=\"s\">&#34;driver&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;oracle.jdbc.driver.OracleDriver&#34;</span><span class=\"o\">)</span>\n        <span class=\"o\">.</span><span class=\"n\">load</span><span class=\"o\">()</span>\n      <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">write</span><span class=\"o\">.</span><span class=\"n\">mode</span><span class=\"o\">(</span><span class=\"s\">&#34;overwrite&#34;</span><span class=\"o\">).</span><span class=\"n\">saveAsTable</span><span class=\"o\">(</span><span class=\"n\">tableName</span><span class=\"o\">)</span>\n\n    <span class=\"o\">})</span>\n\n    <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">stop</span>\n\n  <span class=\"o\">}</span>\n<span class=\"o\">}</span></code></pre></div><p></p>", 
            "topic": [
                {
                    "tag": "Spark", 
                    "tagLink": "https://api.zhihu.com/topics/19942170"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/43450416", 
            "userName": "董可伦", 
            "userLink": "https://www.zhihu.com/people/de888cc734ddbfd918269a8e5fb196ba", 
            "upvote": 0, 
            "title": "spark将DataFrame所有的列类型改为double", 
            "content": "<p></p><p>我的原创地址：</p><a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/04/27/dfChangeAllColDatatypes/\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-0b4e5faa279a5e545d79e754b3dba862_ipico.jpg\" data-image-width=\"640\" data-image-height=\"640\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">spark 将DataFrame所有的列类型改为double</a><h2>前言</h2><p>由于spark机器学习要求输入的DataFrame类型为数值类型，所以如果原始数据读进来的列为string类型，需要一一转化，而如果列很多的情况下一个转化很麻烦，所以能不能一个循环或者一个函数去解决呢。</p><h2>1、单列转化方法</h2><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.types._</span>\n<span class=\"k\">val</span> <span class=\"n\">data</span> <span class=\"k\">=</span> <span class=\"nc\">Array</span><span class=\"o\">((</span><span class=\"s\">&#34;1&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;2&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;3&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;4&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;5&#34;</span><span class=\"o\">),</span> <span class=\"o\">(</span><span class=\"s\">&#34;6&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;7&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;8&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;9&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;10&#34;</span><span class=\"o\">))</span>\n<span class=\"k\">val</span> <span class=\"n\">df</span> <span class=\"k\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"o\">(</span><span class=\"n\">data</span><span class=\"o\">).</span><span class=\"n\">toDF</span><span class=\"o\">(</span><span class=\"s\">&#34;col1&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;col2&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;col3&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;col4&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;col5&#34;</span><span class=\"o\">)</span>\n\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.functions._</span>\n<span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"o\">(</span><span class=\"n\">col</span><span class=\"o\">(</span><span class=\"s\">&#34;col1&#34;</span><span class=\"o\">).</span><span class=\"n\">cast</span><span class=\"o\">(</span><span class=\"nc\">DoubleType</span><span class=\"o\">)).</span><span class=\"n\">show</span><span class=\"o\">()</span>\n<span class=\"o\">+----+</span>\n<span class=\"o\">|</span><span class=\"n\">col1</span><span class=\"o\">|</span>\n<span class=\"o\">+----+</span>\n<span class=\"o\">|</span> <span class=\"mf\">1.0</span><span class=\"o\">|</span>\n<span class=\"o\">|</span> <span class=\"mf\">6.0</span><span class=\"o\">|</span>\n<span class=\"o\">+----+</span></code></pre></div><h2>2、循环转变</h2><p>然后就想能不能用这个方法循环把每一列转成double，但没想到怎么实现，可以用withColumn循环实现。</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">val</span> <span class=\"n\">colNames</span> <span class=\"k\">=</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">columns</span>\n\n<span class=\"k\">var</span> <span class=\"n\">df1</span> <span class=\"k\">=</span> <span class=\"n\">df</span>\n<span class=\"k\">for</span> <span class=\"o\">(</span><span class=\"n\">colName</span> <span class=\"k\">&lt;-</span> <span class=\"n\">colNames</span><span class=\"o\">)</span> <span class=\"o\">{</span>\n  <span class=\"n\">df1</span> <span class=\"k\">=</span> <span class=\"n\">df1</span><span class=\"o\">.</span><span class=\"n\">withColumn</span><span class=\"o\">(</span><span class=\"n\">colName</span><span class=\"o\">,</span> <span class=\"n\">col</span><span class=\"o\">(</span><span class=\"n\">colName</span><span class=\"o\">).</span><span class=\"n\">cast</span><span class=\"o\">(</span><span class=\"nc\">DoubleType</span><span class=\"o\">))</span>\n<span class=\"o\">}</span>\n<span class=\"n\">df1</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"o\">()</span>\n<span class=\"o\">+----+----+----+----+----+</span>\n<span class=\"o\">|</span><span class=\"n\">col1</span><span class=\"o\">|</span><span class=\"n\">col2</span><span class=\"o\">|</span><span class=\"n\">col3</span><span class=\"o\">|</span><span class=\"n\">col4</span><span class=\"o\">|</span><span class=\"n\">col5</span><span class=\"o\">|</span>\n<span class=\"o\">+----+----+----+----+----+</span>\n<span class=\"o\">|</span> <span class=\"mf\">1.0</span><span class=\"o\">|</span> <span class=\"mf\">2.0</span><span class=\"o\">|</span> <span class=\"mf\">3.0</span><span class=\"o\">|</span> <span class=\"mf\">4.0</span><span class=\"o\">|</span> <span class=\"mf\">5.0</span><span class=\"o\">|</span>\n<span class=\"o\">|</span> <span class=\"mf\">6.0</span><span class=\"o\">|</span> <span class=\"mf\">7.0</span><span class=\"o\">|</span> <span class=\"mf\">8.0</span><span class=\"o\">|</span> <span class=\"mf\">9.0</span><span class=\"o\">|</span><span class=\"mf\">10.0</span><span class=\"o\">|</span>\n<span class=\"o\">+----+----+----+----+----+</span></code></pre></div><h2>3、通过:_*</h2><p>但是上面这个方法效率比较低，然后问了一下别人，发现scala 有array:_*这样传参这种语法，而df的select方法也支持这样传，于是最终可以按下面的这样写</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">val</span> <span class=\"n\">cols</span> <span class=\"k\">=</span> <span class=\"n\">colNames</span><span class=\"o\">.</span><span class=\"n\">map</span><span class=\"o\">(</span><span class=\"n\">f</span> <span class=\"k\">=&gt;</span> <span class=\"n\">col</span><span class=\"o\">(</span><span class=\"n\">f</span><span class=\"o\">).</span><span class=\"n\">cast</span><span class=\"o\">(</span><span class=\"nc\">DoubleType</span><span class=\"o\">))</span>\n<span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"o\">(</span><span class=\"n\">cols</span><span class=\"k\">:</span> <span class=\"k\">_</span><span class=\"kt\">*</span><span class=\"o\">).</span><span class=\"n\">show</span><span class=\"o\">()</span>\n<span class=\"o\">+----+----+----+----+----+</span>\n<span class=\"o\">|</span><span class=\"n\">col1</span><span class=\"o\">|</span><span class=\"n\">col2</span><span class=\"o\">|</span><span class=\"n\">col3</span><span class=\"o\">|</span><span class=\"n\">col4</span><span class=\"o\">|</span><span class=\"n\">col5</span><span class=\"o\">|</span>\n<span class=\"o\">+----+----+----+----+----+</span>\n<span class=\"o\">|</span> <span class=\"mf\">1.0</span><span class=\"o\">|</span> <span class=\"mf\">2.0</span><span class=\"o\">|</span> <span class=\"mf\">3.0</span><span class=\"o\">|</span> <span class=\"mf\">4.0</span><span class=\"o\">|</span> <span class=\"mf\">5.0</span><span class=\"o\">|</span>\n<span class=\"o\">|</span> <span class=\"mf\">6.0</span><span class=\"o\">|</span> <span class=\"mf\">7.0</span><span class=\"o\">|</span> <span class=\"mf\">8.0</span><span class=\"o\">|</span> <span class=\"mf\">9.0</span><span class=\"o\">|</span><span class=\"mf\">10.0</span><span class=\"o\">|</span>\n<span class=\"o\">+----+----+----+----+----+</span></code></pre></div><p>这样就可以很方便的查询指定多列和转变指定列的类型了：</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">val</span> <span class=\"n\">name</span> <span class=\"k\">=</span> <span class=\"s\">&#34;col1,col3,col5&#34;</span>\n<span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"o\">(</span><span class=\"n\">name</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"o\">(</span><span class=\"s\">&#34;,&#34;</span><span class=\"o\">).</span><span class=\"n\">map</span><span class=\"o\">(</span><span class=\"n\">name</span> <span class=\"k\">=&gt;</span> <span class=\"n\">col</span><span class=\"o\">(</span><span class=\"n\">name</span><span class=\"o\">))</span><span class=\"k\">:</span> <span class=\"k\">_</span><span class=\"kt\">*</span><span class=\"o\">).</span><span class=\"n\">show</span><span class=\"o\">()</span>\n<span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"o\">(</span><span class=\"n\">name</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"o\">(</span><span class=\"s\">&#34;,&#34;</span><span class=\"o\">).</span><span class=\"n\">map</span><span class=\"o\">(</span><span class=\"n\">name</span> <span class=\"k\">=&gt;</span> <span class=\"n\">col</span><span class=\"o\">(</span><span class=\"n\">name</span><span class=\"o\">).</span><span class=\"n\">cast</span><span class=\"o\">(</span><span class=\"nc\">DoubleType</span><span class=\"o\">))</span><span class=\"k\">:</span> <span class=\"k\">_</span><span class=\"kt\">*</span><span class=\"o\">).</span><span class=\"n\">show</span><span class=\"o\">()</span>\n<span class=\"o\">+----+----+----+</span>\n<span class=\"o\">|</span><span class=\"n\">col1</span><span class=\"o\">|</span><span class=\"n\">col3</span><span class=\"o\">|</span><span class=\"n\">col5</span><span class=\"o\">|</span>\n<span class=\"o\">+----+----+----+</span>\n<span class=\"o\">|</span>   <span class=\"mi\">1</span><span class=\"o\">|</span>   <span class=\"mi\">3</span><span class=\"o\">|</span>   <span class=\"mi\">5</span><span class=\"o\">|</span>\n<span class=\"o\">|</span>   <span class=\"mi\">6</span><span class=\"o\">|</span>   <span class=\"mi\">8</span><span class=\"o\">|</span>  <span class=\"mi\">10</span><span class=\"o\">|</span>\n<span class=\"o\">+----+----+----+</span>\n\n<span class=\"o\">+----+----+----+</span>\n<span class=\"o\">|</span><span class=\"n\">col1</span><span class=\"o\">|</span><span class=\"n\">col3</span><span class=\"o\">|</span><span class=\"n\">col5</span><span class=\"o\">|</span>\n<span class=\"o\">+----+----+----+</span>\n<span class=\"o\">|</span> <span class=\"mf\">1.0</span><span class=\"o\">|</span> <span class=\"mf\">3.0</span><span class=\"o\">|</span> <span class=\"mf\">5.0</span><span class=\"o\">|</span>\n<span class=\"o\">|</span> <span class=\"mf\">6.0</span><span class=\"o\">|</span> <span class=\"mf\">8.0</span><span class=\"o\">|</span><span class=\"mf\">10.0</span><span class=\"o\">|</span>\n<span class=\"o\">+----+----+----+</span></code></pre></div><p>附完整代码：</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">package</span> <span class=\"nn\">com.dkl.leanring.spark.test</span>\n\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.SparkSession</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.types._</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.DataFrame</span>\n<span class=\"k\">object</span> <span class=\"nc\">DfDemo</span> <span class=\"o\">{</span>\n\n  <span class=\"k\">def</span> <span class=\"n\">main</span><span class=\"o\">(</span><span class=\"n\">args</span><span class=\"k\">:</span> <span class=\"kt\">Array</span><span class=\"o\">[</span><span class=\"kt\">String</span><span class=\"o\">])</span><span class=\"k\">:</span> <span class=\"kt\">Unit</span> <span class=\"o\">=</span> <span class=\"o\">{</span>\n    <span class=\"k\">val</span> <span class=\"n\">spark</span> <span class=\"k\">=</span> <span class=\"nc\">SparkSession</span><span class=\"o\">.</span><span class=\"n\">builder</span><span class=\"o\">().</span><span class=\"n\">appName</span><span class=\"o\">(</span><span class=\"s\">&#34;DfDemo&#34;</span><span class=\"o\">).</span><span class=\"n\">master</span><span class=\"o\">(</span><span class=\"s\">&#34;local&#34;</span><span class=\"o\">).</span><span class=\"n\">getOrCreate</span><span class=\"o\">()</span>\n    <span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.types._</span>\n    <span class=\"k\">val</span> <span class=\"n\">data</span> <span class=\"k\">=</span> <span class=\"nc\">Array</span><span class=\"o\">((</span><span class=\"s\">&#34;1&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;2&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;3&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;4&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;5&#34;</span><span class=\"o\">),</span> <span class=\"o\">(</span><span class=\"s\">&#34;6&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;7&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;8&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;9&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;10&#34;</span><span class=\"o\">))</span>\n    <span class=\"k\">val</span> <span class=\"n\">df</span> <span class=\"k\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"o\">(</span><span class=\"n\">data</span><span class=\"o\">).</span><span class=\"n\">toDF</span><span class=\"o\">(</span><span class=\"s\">&#34;col1&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;col2&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;col3&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;col4&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;col5&#34;</span><span class=\"o\">)</span>\n\n    <span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.functions._</span>\n    <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"o\">(</span><span class=\"n\">col</span><span class=\"o\">(</span><span class=\"s\">&#34;col1&#34;</span><span class=\"o\">).</span><span class=\"n\">cast</span><span class=\"o\">(</span><span class=\"nc\">DoubleType</span><span class=\"o\">)).</span><span class=\"n\">show</span><span class=\"o\">()</span>\n\n    <span class=\"k\">val</span> <span class=\"n\">colNames</span> <span class=\"k\">=</span> <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">columns</span>\n\n    <span class=\"k\">var</span> <span class=\"n\">df1</span> <span class=\"k\">=</span> <span class=\"n\">df</span>\n    <span class=\"k\">for</span> <span class=\"o\">(</span><span class=\"n\">colName</span> <span class=\"k\">&lt;-</span> <span class=\"n\">colNames</span><span class=\"o\">)</span> <span class=\"o\">{</span>\n      <span class=\"n\">df1</span> <span class=\"k\">=</span> <span class=\"n\">df1</span><span class=\"o\">.</span><span class=\"n\">withColumn</span><span class=\"o\">(</span><span class=\"n\">colName</span><span class=\"o\">,</span> <span class=\"n\">col</span><span class=\"o\">(</span><span class=\"n\">colName</span><span class=\"o\">).</span><span class=\"n\">cast</span><span class=\"o\">(</span><span class=\"nc\">DoubleType</span><span class=\"o\">))</span>\n    <span class=\"o\">}</span>\n    <span class=\"n\">df1</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"o\">()</span>\n\n    <span class=\"k\">val</span> <span class=\"n\">cols</span> <span class=\"k\">=</span> <span class=\"n\">colNames</span><span class=\"o\">.</span><span class=\"n\">map</span><span class=\"o\">(</span><span class=\"n\">f</span> <span class=\"k\">=&gt;</span> <span class=\"n\">col</span><span class=\"o\">(</span><span class=\"n\">f</span><span class=\"o\">).</span><span class=\"n\">cast</span><span class=\"o\">(</span><span class=\"nc\">DoubleType</span><span class=\"o\">))</span>\n    <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"o\">(</span><span class=\"n\">cols</span><span class=\"k\">:</span> <span class=\"k\">_</span><span class=\"kt\">*</span><span class=\"o\">).</span><span class=\"n\">show</span><span class=\"o\">()</span>\n    <span class=\"k\">val</span> <span class=\"n\">name</span> <span class=\"k\">=</span> <span class=\"s\">&#34;col1,col3,col5&#34;</span>\n    <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"o\">(</span><span class=\"n\">name</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"o\">(</span><span class=\"s\">&#34;,&#34;</span><span class=\"o\">).</span><span class=\"n\">map</span><span class=\"o\">(</span><span class=\"n\">name</span> <span class=\"k\">=&gt;</span> <span class=\"n\">col</span><span class=\"o\">(</span><span class=\"n\">name</span><span class=\"o\">))</span><span class=\"k\">:</span> <span class=\"k\">_</span><span class=\"kt\">*</span><span class=\"o\">).</span><span class=\"n\">show</span><span class=\"o\">()</span>\n    <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">select</span><span class=\"o\">(</span><span class=\"n\">name</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"o\">(</span><span class=\"s\">&#34;,&#34;</span><span class=\"o\">).</span><span class=\"n\">map</span><span class=\"o\">(</span><span class=\"n\">name</span> <span class=\"k\">=&gt;</span> <span class=\"n\">col</span><span class=\"o\">(</span><span class=\"n\">name</span><span class=\"o\">).</span><span class=\"n\">cast</span><span class=\"o\">(</span><span class=\"nc\">DoubleType</span><span class=\"o\">))</span><span class=\"k\">:</span> <span class=\"k\">_</span><span class=\"kt\">*</span><span class=\"o\">).</span><span class=\"n\">show</span><span class=\"o\">()</span>\n\n  <span class=\"o\">}</span></code></pre></div><p></p>", 
            "topic": [
                {
                    "tag": "Spark", 
                    "tagLink": "https://api.zhihu.com/topics/19942170"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/43450014", 
            "userName": "董可伦", 
            "userLink": "https://www.zhihu.com/people/de888cc734ddbfd918269a8e5fb196ba", 
            "upvote": 0, 
            "title": "spark on yarn 配置及异常解决", 
            "content": "<p>我的原创地址：</p><a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/04/16/sparkOnYarnConf/\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic2.zhimg.com/v2-80454507c6783633e9a0e2f0b663827d_180x120.jpg\" data-image-width=\"1157\" data-image-height=\"255\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">spark on yarn 配置及异常解决</a><h2>前言</h2><p>YARN 是在Hadoop 2.0 中引入的集群管理器，它可以让多种数据处理框架运行在一个共享的资源池上，并且通常安装在与Hadoop 文件系统（简称HDFS）相同的物理节点上。在这样配置的YARN 集群上运行Spark 是很有意义的，它可以让Spark 在存储数据的物理节点上运行，以快速访问HDFS 中的数据。</p><h2>1、配置</h2><h2>1.1 配置HADOOP_CONF_DIR</h2><div class=\"highlight\"><pre><code class=\"language-bash\">vim /etc/profile\n<span class=\"nb\">export</span> <span class=\"nv\">HADOOP_CONF_DIR</span><span class=\"o\">=</span>/opt/hadoop-2.7.5/etc/hadoop\n<span class=\"nb\">source</span> /etc/profile</code></pre></div><h2>1.2 命令行启动</h2><div class=\"highlight\"><pre><code class=\"language-bash\">spark-shell --master yarn</code></pre></div><p>但是在spark2.x里会报一个错误</p><div class=\"highlight\"><pre><code class=\"language-text\">18/04/16 07:59:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n18/04/16 07:59:27 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n18/04/16 07:59:54 ERROR SparkContext: Error initializing SparkContext.\norg.apache.spark.SparkException: Yarn application has already ended! It might have been killed or unable to launch application master.\n    at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.waitForApplication(YarnClientSchedulerBackend.scala:85)\n    at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:62)\n    at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:173)\n    at org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:509)\n    at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2516)\n    at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:918)\n    at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:910)\n    at scala.Option.getOrElse(Option.scala:121)\n    at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:910)\n    at org.apache.spark.repl.Main$.createSparkSession(Main.scala:101)\n    at $line3.$read$$iw$$iw.&lt;init&gt;(&lt;console&gt;:15)\n    at $line3.$read$$iw.&lt;init&gt;(&lt;console&gt;:42)\n    at $line3.$read.&lt;init&gt;(&lt;console&gt;:44)\n    at $line3.$read$.&lt;init&gt;(&lt;console&gt;:48)\n    at $line3.$read$.&lt;clinit&gt;(&lt;console&gt;)\n    at $line3.$eval$.$print$lzycompute(&lt;console&gt;:7)\n    at $line3.$eval$.$print(&lt;console&gt;:6)\n    at $line3.$eval.$print(&lt;console&gt;)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:497)\n    at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:786)\n    at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1047)\n    at scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:638)\n    at scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:637)\n    at scala.reflect.internal.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:31)\n    at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:19)\n    at scala.tools.nsc.interpreter.IMain$WrappedRequest.loadAndRunReq(IMain.scala:637)\n    at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:569)\n    at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:565)\n    at scala.tools.nsc.interpreter.ILoop.interpretStartingWith(ILoop.scala:807)\n    at scala.tools.nsc.interpreter.ILoop.command(ILoop.scala:681)\n    at scala.tools.nsc.interpreter.ILoop.processLine(ILoop.scala:395)\n    at org.apache.spark.repl.SparkILoop$$anonfun$initializeSpark$1.apply$mcV$sp(SparkILoop.scala:38)\n    at org.apache.spark.repl.SparkILoop$$anonfun$initializeSpark$1.apply(SparkILoop.scala:37)\n    at org.apache.spark.repl.SparkILoop$$anonfun$initializeSpark$1.apply(SparkILoop.scala:37)\n    at scala.tools.nsc.interpreter.IMain.beQuietDuring(IMain.scala:214)\n    at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:37)\n    at org.apache.spark.repl.SparkILoop.loadFiles(SparkILoop.scala:98)\n    at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply$mcZ$sp(ILoop.scala:920)\n    at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply(ILoop.scala:909)\n    at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply(ILoop.scala:909)\n    at scala.reflect.internal.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:97)\n    at scala.tools.nsc.interpreter.ILoop.process(ILoop.scala:909)\n    at org.apache.spark.repl.Main$.doMain(Main.scala:74)\n    at org.apache.spark.repl.Main$.main(Main.scala:54)\n    at org.apache.spark.repl.Main.main(Main.scala)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:497)\n    at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:775)\n    at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)\n    at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)\n    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)\n    at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n18/04/16 07:59:54 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!\n18/04/16 07:59:54 WARN MetricsSystem: Stopping a MetricsSystem that is not running\norg.apache.spark.SparkException: Yarn application has already ended! It might have been killed or unable to launch application master.\n  at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.waitForApplication(YarnClientSchedulerBackend.scala:85)\n  at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:62)\n  at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:173)\n  at org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:509)\n  at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2516)\n  at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:918)\n  at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:910)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:910)\n  at org.apache.spark.repl.Main$.createSparkSession(Main.scala:101)\n  ... 47 elided\n&lt;console&gt;:14: error: not found: value spark\n       import spark.implicits._\n              ^\n&lt;console&gt;:14: error: not found: value spark\n       import spark.sql\n              ^\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  &#39;_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 2.2.1\n      /_/\n\nUsing Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_45)\nType in expressions to have them evaluated.\nType :help for more information.\n\nscala&gt;</code></pre></div><h2>2、错误解决</h2><h2>2.1 添加spark.yarn.jars</h2><p>首先看到第二条warn</p><div class=\"highlight\"><pre><code class=\"language-bash\">Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.</code></pre></div><p>联想到是不是这条warn信息导致的，然后根据这条warn信息上网查了一下，再根据错误信息也查了一下</p><div class=\"highlight\"><pre><code class=\"language-text\">Yarn application has already ended! It might have been killed or unable to ...</code></pre></div><p>发现，都是说要配置spark.yarn.jars，于是按照如下命令配置</p><div class=\"highlight\"><pre><code class=\"language-bash\">hdfs dfs -mkdir /hadoop\nhdfs dfs -mkdir /hadoop/spark_jars\nhdfs dfs -put /opt/spark-2.2.1-bin-hadoop2.7/jars/* /hadoop/spark_jars\n<span class=\"nb\">cd</span> /opt/spark-2.2.1-bin-hadoop2.7/conf/\ncp spark-defaults.conf.template spark-defaults.conf\nvim spark-defaults.conf</code></pre></div><p>在最下面添加：</p><div class=\"highlight\"><pre><code class=\"language-text\">spark.yarn.jars hdfs://192.168.44.128:8888/hadoop/spark_jars/*</code></pre></div><p>(注意后面的*不能去掉) 然后启动spark-shell,发现还是报相似错误（没了warn）</p><h2>2.2 配置hadoop的yarn-site.xml</h2><p>因为java8导致的问题</p><div class=\"highlight\"><pre><code class=\"language-bash\">vim /opt/hadoop-2.7.5/etc/hadoop/yarn-site.xml</code></pre></div><p>添加：</p><div class=\"highlight\"><pre><code class=\"language-text\">&lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt;\n    &lt;value&gt;false&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;\n    &lt;value&gt;false&lt;/value&gt;\n&lt;/property&gt;</code></pre></div><p>再次启动spark-shell,成功！ </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-ae39d772b60e7f34fd5ac252fdd656ce_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1100\" data-rawheight=\"263\" class=\"origin_image zh-lightbox-thumb\" width=\"1100\" data-original=\"https://pic3.zhimg.com/v2-ae39d772b60e7f34fd5ac252fdd656ce_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1100&#39; height=&#39;263&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1100\" data-rawheight=\"263\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1100\" data-original=\"https://pic3.zhimg.com/v2-ae39d772b60e7f34fd5ac252fdd656ce_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-ae39d772b60e7f34fd5ac252fdd656ce_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>3、意外之喜</h2><p>由于要写博客记录，所以需要将错误还原，第一次只将spark.yarn.jars注释掉，启动spark-shell,发现是成功的，只是会有条warn而已，也就是说，这个错误的根本原因，是java8导致没有配置2.2中的yarn-site.xml！！ </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-cb5d84d49f2b6ad35c6c18578e18c07a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1157\" data-rawheight=\"255\" class=\"origin_image zh-lightbox-thumb\" width=\"1157\" data-original=\"https://pic3.zhimg.com/v2-cb5d84d49f2b6ad35c6c18578e18c07a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1157&#39; height=&#39;255&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1157\" data-rawheight=\"255\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1157\" data-original=\"https://pic3.zhimg.com/v2-cb5d84d49f2b6ad35c6c18578e18c07a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-cb5d84d49f2b6ad35c6c18578e18c07a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>参考资料</h2><p><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/lxhandlbb/article/details/54410644\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">blog.csdn.net/lxhandlbb</span><span class=\"invisible\">/article/details/54410644</span><span class=\"ellipsis\"></span></a> <a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/gg584741/article/details/72825713\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">blog.csdn.net/gg584741/</span><span class=\"invisible\">article/details/72825713</span><span class=\"ellipsis\"></span></a></p>", 
            "topic": [
                {
                    "tag": "Spark", 
                    "tagLink": "https://api.zhihu.com/topics/19942170"
                }, 
                {
                    "tag": "Hadoop", 
                    "tagLink": "https://api.zhihu.com/topics/19563390"
                }, 
                {
                    "tag": "YARN", 
                    "tagLink": "https://api.zhihu.com/topics/20041614"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/43449758", 
            "userName": "董可伦", 
            "userLink": "https://www.zhihu.com/people/de888cc734ddbfd918269a8e5fb196ba", 
            "upvote": 0, 
            "title": "spark连接hive", 
            "content": "<p>我的原创地址：</p><a href=\"https://link.zhihu.com/?target=http%3A//dongkelun.com/2018/03/25/sparkHive/\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">dongkelun.com/2018/03/2</span><span class=\"invisible\">5/sparkHive/</span><span class=\"ellipsis\"></span></a><h2>1、在服务器（虚拟机）spark-shell连接hive</h2><h2>1.1 将hive-site.xml拷贝到spark/conf里</h2><div class=\"highlight\"><pre><code class=\"language-bash\">cp /opt/apache-hive-2.3.2-bin/conf/hive-site.xml /opt/spark-2.2.1-bin-hadoop2.7/conf/</code></pre></div><h2>1.2 将mysql驱动拷贝到spark/jar里</h2><div class=\"highlight\"><pre><code class=\"language-text\">cp /opt/apache-hive-2.3.2-bin/bin/mysql-connector-java-5.1.46-bin.jar /opt/spark-2.2.1-bin-hadoop2.7/jars/</code></pre></div><h2>1.3 启动spark-shell,输入代码测试</h2><div class=\"highlight\"><pre><code class=\"language-bash\">spark-shell\nimport org.apache.spark.sql.hive.HiveContext\nval <span class=\"nv\">hiveContext</span> <span class=\"o\">=</span> new HiveContext<span class=\"o\">(</span>sc<span class=\"o\">)</span>\nhiveContext.sql<span class=\"o\">(</span><span class=\"s2\">&#34;select * from test&#34;</span><span class=\"o\">)</span>.show<span class=\"o\">()</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-a4d7df8bf3a60d8e1a3eaec9955b6ff1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1101\" data-rawheight=\"495\" class=\"origin_image zh-lightbox-thumb\" width=\"1101\" data-original=\"https://pic2.zhimg.com/v2-a4d7df8bf3a60d8e1a3eaec9955b6ff1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1101&#39; height=&#39;495&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1101\" data-rawheight=\"495\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1101\" data-original=\"https://pic2.zhimg.com/v2-a4d7df8bf3a60d8e1a3eaec9955b6ff1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-a4d7df8bf3a60d8e1a3eaec9955b6ff1_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>1.4 异常及解决</h2><p>在执行hiveContext.sql(&#34;select * from test&#34;).show() 报了一个异常：</p><div class=\"highlight\"><pre><code class=\"language-bash\">The root scratch dir: /tmp/hive on HDFS should be writable. Current permissions are: rwxr-xr-x<span class=\"p\">;</span></code></pre></div><p>解决办法：</p><h2>1.4.1 更改HDFS目录/tmp/hive的权限：</h2><div class=\"highlight\"><pre><code class=\"language-bash\">hadoop fs -chmod <span class=\"m\">777</span> /tmp/hive</code></pre></div><h2>1.4.2 同时删HDFS与本地的目录/tmp/hive：</h2><div class=\"highlight\"><pre><code class=\"language-bash\">hadoop fs -rm -r /tmp/hive \nrm -rf /tmp/hive</code></pre></div><p>这次错误采用的是第二种解决办法，有的情况下用第一种方法，比如一次在启动hive时候报这种错误~。 错误截图： </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-97ef7aed50937894648b0f7823f35713_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1527\" data-rawheight=\"666\" class=\"origin_image zh-lightbox-thumb\" width=\"1527\" data-original=\"https://pic4.zhimg.com/v2-97ef7aed50937894648b0f7823f35713_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1527&#39; height=&#39;666&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1527\" data-rawheight=\"666\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1527\" data-original=\"https://pic4.zhimg.com/v2-97ef7aed50937894648b0f7823f35713_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-97ef7aed50937894648b0f7823f35713_b.jpg\"/></figure><p> 参考：<a href=\"https://link.zhihu.com/?target=http%3A//www.cnblogs.com/czm1032851561/p/5751722.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">cnblogs.com/czm10328515</span><span class=\"invisible\">61/p/5751722.html</span><span class=\"ellipsis\"></span></a></p><h2>2、win10+eclipse上连接hive</h2><h2>2.1 将hive-site.xml拷贝到项目中的resources文件夹下</h2><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-1a948f83985e3c6f7c803b8466a678a0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"364\" data-rawheight=\"105\" class=\"content_image\" width=\"364\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;364&#39; height=&#39;105&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"364\" data-rawheight=\"105\" class=\"content_image lazy\" width=\"364\" data-actualsrc=\"https://pic1.zhimg.com/v2-1a948f83985e3c6f7c803b8466a678a0_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>2.2 在sbt里添加对应版本的mysql依赖</h2><div class=\"highlight\"><pre><code class=\"language-bash\"><span class=\"s2\">&#34;mysql&#34;</span> % <span class=\"s2\">&#34;mysql-connector-java&#34;</span> % <span class=\"s2\">&#34;5.1.46&#34;</span></code></pre></div><h2>2.3 代码</h2><h2>2.3.1 旧版api(1.6以上)</h2><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">package</span> <span class=\"nn\">com.dkl.leanring.spark.sql</span>\n\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.SparkConf</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.SQLContext</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.hive.HiveContext</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.SparkContext</span>\n\n<span class=\"cm\">/**\n</span><span class=\"cm\"> * 旧版本spark-hive测试\n</span><span class=\"cm\"> */</span>\n<span class=\"k\">object</span> <span class=\"nc\">OldSparkHiveDemo</span> <span class=\"o\">{</span>\n  <span class=\"k\">def</span> <span class=\"n\">main</span><span class=\"o\">(</span><span class=\"n\">args</span><span class=\"k\">:</span> <span class=\"kt\">Array</span><span class=\"o\">[</span><span class=\"kt\">String</span><span class=\"o\">])</span><span class=\"k\">:</span> <span class=\"kt\">Unit</span> <span class=\"o\">=</span> <span class=\"o\">{</span>\n    <span class=\"k\">val</span> <span class=\"n\">conf</span> <span class=\"k\">=</span> <span class=\"k\">new</span> <span class=\"nc\">SparkConf</span><span class=\"o\">().</span><span class=\"n\">setAppName</span><span class=\"o\">(</span><span class=\"s\">&#34;OldSparkHiveDemo&#34;</span><span class=\"o\">).</span><span class=\"n\">setMaster</span><span class=\"o\">(</span><span class=\"s\">&#34;local&#34;</span><span class=\"o\">)</span>\n    <span class=\"k\">val</span> <span class=\"n\">sc</span> <span class=\"k\">=</span> <span class=\"k\">new</span> <span class=\"nc\">SparkContext</span><span class=\"o\">(</span><span class=\"n\">conf</span><span class=\"o\">)</span>\n    <span class=\"k\">val</span> <span class=\"n\">sqlContext</span> <span class=\"k\">=</span> <span class=\"k\">new</span> <span class=\"nc\">SQLContext</span><span class=\"o\">(</span><span class=\"n\">sc</span><span class=\"o\">)</span>\n    <span class=\"k\">import</span> <span class=\"nn\">sqlContext.implicits._</span>\n    <span class=\"k\">val</span> <span class=\"n\">hiveCtx</span> <span class=\"k\">=</span> <span class=\"k\">new</span> <span class=\"nc\">HiveContext</span><span class=\"o\">(</span><span class=\"n\">sc</span><span class=\"o\">)</span>\n\n    <span class=\"n\">hiveCtx</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">(</span><span class=\"s\">&#34;select * from test&#34;</span><span class=\"o\">).</span><span class=\"n\">show</span><span class=\"o\">()</span>\n    <span class=\"k\">val</span> <span class=\"n\">data</span> <span class=\"k\">=</span> <span class=\"nc\">Array</span><span class=\"o\">((</span><span class=\"mi\">3</span><span class=\"o\">,</span> <span class=\"s\">&#34;name3&#34;</span><span class=\"o\">),</span> <span class=\"o\">(</span><span class=\"mi\">4</span><span class=\"o\">,</span> <span class=\"s\">&#34;name4&#34;</span><span class=\"o\">),</span> <span class=\"o\">(</span><span class=\"mi\">5</span><span class=\"o\">,</span> <span class=\"s\">&#34;name5&#34;</span><span class=\"o\">))</span>\n    <span class=\"k\">val</span> <span class=\"n\">df</span> <span class=\"k\">=</span> <span class=\"n\">sc</span><span class=\"o\">.</span><span class=\"n\">parallelize</span><span class=\"o\">(</span><span class=\"n\">data</span><span class=\"o\">).</span><span class=\"n\">toDF</span><span class=\"o\">(</span><span class=\"s\">&#34;id&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;name&#34;</span><span class=\"o\">)</span>\n    <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">createOrReplaceTempView</span><span class=\"o\">(</span><span class=\"s\">&#34;user&#34;</span><span class=\"o\">)</span>\n    <span class=\"n\">hiveCtx</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">(</span><span class=\"s\">&#34;insert into test select id,name from user&#34;</span><span class=\"o\">)</span>\n    <span class=\"n\">hiveCtx</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">(</span><span class=\"s\">&#34;select * from test&#34;</span><span class=\"o\">).</span><span class=\"n\">show</span><span class=\"o\">()</span>\n  <span class=\"o\">}</span>\n\n<span class=\"o\">}</span></code></pre></div><p>（注：其中df.createOrReplaceTempView(&#34;user&#34;)改为df.registerTempTable(&#34;user&#34;)，因为createOrReplaceTempView方法是2.0.0才有的，registerTempTable是旧版的方法，1.6.0就有了，嫌麻烦就不改代码重新贴图了） </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-d8f765a1aaf02bf864321d6faf7da6e8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1341\" data-rawheight=\"901\" class=\"origin_image zh-lightbox-thumb\" width=\"1341\" data-original=\"https://pic1.zhimg.com/v2-d8f765a1aaf02bf864321d6faf7da6e8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1341&#39; height=&#39;901&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1341\" data-rawheight=\"901\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1341\" data-original=\"https://pic1.zhimg.com/v2-d8f765a1aaf02bf864321d6faf7da6e8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-d8f765a1aaf02bf864321d6faf7da6e8_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>2.3.2 新版api</h2><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">package</span> <span class=\"nn\">com.dkl.leanring.spark.sql</span>\n\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.SparkSession</span>\n\n<span class=\"cm\">/**\n</span><span class=\"cm\"> * 新版本spark-hive测试\n</span><span class=\"cm\"> */</span>\n<span class=\"k\">object</span> <span class=\"nc\">NewSparkHiveDemo</span> <span class=\"o\">{</span>\n  <span class=\"k\">def</span> <span class=\"n\">main</span><span class=\"o\">(</span><span class=\"n\">args</span><span class=\"k\">:</span> <span class=\"kt\">Array</span><span class=\"o\">[</span><span class=\"kt\">String</span><span class=\"o\">])</span><span class=\"k\">:</span> <span class=\"kt\">Unit</span> <span class=\"o\">=</span> <span class=\"o\">{</span>\n    <span class=\"k\">val</span> <span class=\"n\">spark</span> <span class=\"k\">=</span> <span class=\"nc\">SparkSession</span>\n      <span class=\"o\">.</span><span class=\"n\">builder</span><span class=\"o\">()</span>\n      <span class=\"o\">.</span><span class=\"n\">appName</span><span class=\"o\">(</span><span class=\"s\">&#34;Spark Hive Example&#34;</span><span class=\"o\">)</span>\n      <span class=\"o\">.</span><span class=\"n\">master</span><span class=\"o\">(</span><span class=\"s\">&#34;local&#34;</span><span class=\"o\">)</span>\n      <span class=\"o\">.</span><span class=\"n\">config</span><span class=\"o\">(</span><span class=\"s\">&#34;spark.sql.warehouse.dir&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;/user/hive/warehouse/&#34;</span><span class=\"o\">)</span>\n      <span class=\"o\">.</span><span class=\"n\">enableHiveSupport</span><span class=\"o\">()</span>\n      <span class=\"o\">.</span><span class=\"n\">getOrCreate</span><span class=\"o\">()</span>\n\n    <span class=\"k\">import</span> <span class=\"nn\">spark.implicits._</span>\n    <span class=\"k\">import</span> <span class=\"nn\">spark.sql</span>\n    <span class=\"n\">sql</span><span class=\"o\">(</span><span class=\"s\">&#34;CREATE TABLE IF NOT EXISTS src (key INT, value STRING)&#34;</span><span class=\"o\">)</span>\n    <span class=\"k\">val</span> <span class=\"n\">data</span> <span class=\"k\">=</span> <span class=\"nc\">Array</span><span class=\"o\">((</span><span class=\"mi\">1</span><span class=\"o\">,</span> <span class=\"s\">&#34;val1&#34;</span><span class=\"o\">),</span> <span class=\"o\">(</span><span class=\"mi\">2</span><span class=\"o\">,</span> <span class=\"s\">&#34;val2&#34;</span><span class=\"o\">),</span> <span class=\"o\">(</span><span class=\"mi\">3</span><span class=\"o\">,</span> <span class=\"s\">&#34;val3&#34;</span><span class=\"o\">))</span>\n    <span class=\"k\">var</span> <span class=\"n\">df</span> <span class=\"k\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">createDataFrame</span><span class=\"o\">(</span><span class=\"n\">data</span><span class=\"o\">).</span><span class=\"n\">toDF</span><span class=\"o\">(</span><span class=\"s\">&#34;key&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;value&#34;</span><span class=\"o\">)</span>\n    <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">createOrReplaceTempView</span><span class=\"o\">(</span><span class=\"s\">&#34;temp_src&#34;</span><span class=\"o\">)</span>\n    <span class=\"n\">sql</span><span class=\"o\">(</span><span class=\"s\">&#34;insert into src select key,value from temp_src&#34;</span><span class=\"o\">)</span>\n    <span class=\"n\">sql</span><span class=\"o\">(</span><span class=\"s\">&#34;SELECT * FROM src&#34;</span><span class=\"o\">).</span><span class=\"n\">show</span><span class=\"o\">()</span>\n  <span class=\"o\">}</span>\n<span class=\"o\">}</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-b3f5e09c2a45fe13630335c37df75982_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1368\" data-rawheight=\"888\" class=\"origin_image zh-lightbox-thumb\" width=\"1368\" data-original=\"https://pic3.zhimg.com/v2-b3f5e09c2a45fe13630335c37df75982_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1368&#39; height=&#39;888&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1368\" data-rawheight=\"888\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1368\" data-original=\"https://pic3.zhimg.com/v2-b3f5e09c2a45fe13630335c37df75982_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-b3f5e09c2a45fe13630335c37df75982_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>2.4 异常及解决方法</h2><p>在执行insert语句时会出现如下异常信息：</p><div class=\"highlight\"><pre><code class=\"language-text\">org.apache.hadoop.security.AccessControlException: Permission denied: user=dongkelun, access=EXECUTE, inode=&#34;/user/hive/warehouse&#34;:root...</code></pre></div><p>原因是：启动 Spark 应用程序的win用户对spark.sql.warehouse.dir没有写权限 解决办法：</p><div class=\"highlight\"><pre><code class=\"language-bash\">hadoop fs -chmod <span class=\"m\">777</span> /user/hive/warehouse/</code></pre></div><p>附异常信息截图： </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3cfbc33742915eacf7189d34a26a5009_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1608\" data-rawheight=\"828\" class=\"origin_image zh-lightbox-thumb\" width=\"1608\" data-original=\"https://pic2.zhimg.com/v2-3cfbc33742915eacf7189d34a26a5009_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1608&#39; height=&#39;828&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1608\" data-rawheight=\"828\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1608\" data-original=\"https://pic2.zhimg.com/v2-3cfbc33742915eacf7189d34a26a5009_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-3cfbc33742915eacf7189d34a26a5009_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>3、更新：写hive方法</h2><p>直接用下面这句代码即可将df里的全部数据存到hive表里</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">write</span><span class=\"o\">.</span><span class=\"n\">mode</span><span class=\"o\">(</span><span class=\"nc\">SaveMode</span><span class=\"o\">.</span><span class=\"nc\">Append</span><span class=\"o\">).</span><span class=\"n\">saveAsTable</span><span class=\"o\">(</span><span class=\"n\">tableName</span><span class=\"o\">)</span></code></pre></div><p></p>", 
            "topic": [
                {
                    "tag": "Hive", 
                    "tagLink": "https://api.zhihu.com/topics/19655283"
                }, 
                {
                    "tag": "Spark", 
                    "tagLink": "https://api.zhihu.com/topics/19942170"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/43374434", 
            "userName": "董可伦", 
            "userLink": "https://www.zhihu.com/people/de888cc734ddbfd918269a8e5fb196ba", 
            "upvote": 0, 
            "title": "Spark Sql 连接mysql", 
            "content": "<p>我的原创地址：</p><a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/03/21/sparkMysql/\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-b64a3689ebf38076f82fdab961ec62d4_180x120.jpg\" data-image-width=\"1499\" data-image-height=\"279\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Spark Sql 连接mysql</a><p class=\"ztext-empty-paragraph\"><br/></p><h2>1、基本概念和用法（摘自spark官方文档中文版）</h2><p>Spark SQL 还有一个能够使用 JDBC 从其他数据库读取数据的数据源。当使用 JDBC 访问其它数据库时，应该首选 JdbcRDD。这是因为结果是以数据框（DataFrame）返回的，且这样 Spark SQL操作轻松或便于连接其它数据源。因为这种 JDBC 数据源不需要用户提供 ClassTag，所以它也更适合使用 Java 或 Python 操作。（注意，这与允许其它应用使用 Spark SQL 执行查询操作的 Spark SQL JDBC 服务器是不同的）。</p><p>使用 JDBC 访问特定数据库时，需要在 spark classpath 上添加对应的 JDBC 驱动配置。例如，为了从 Spark Shell 连接 postgres，你需要运行如下命令 : </p><div class=\"highlight\"><pre><code class=\"language-bash\">bin/spark-shell --driver-class-path postgresql-9.4.1207.jar --jars postgresql-9.4.1207.jar</code></pre></div><p>通过调用数据源API，远程数据库的表可以被加载为DataFrame 或Spark SQL临时表。支持的参数有 : </p><p>属性名 | 含义 - | :-:  url | 要连接的 JDBC URL。 dbtable | 要读取的 JDBC 表。 注意，一个 SQL 查询的 From 分语句中的任何有效表都能被使用。例如，既可以是完整表名，也可以是括号括起来的子查询语句。  driver | 用于连接 URL 的 JDBC 驱动的类名。 partitionColumn, lowerBound, upperBound, numPartitions | 这几个选项，若有一个被配置，则必须全部配置。它们描述了当从多个 worker 中并行的读取表时，如何对它分区。partitionColumn 必须是所查询表的一个数值字段。注意，lowerBound 和 upperBound 都只是用于决定分区跨度的，而不是过滤表中的行。因此，表中的所有行将被分区并返回。 fetchSize | JDBC fetch size，决定每次读取多少行数据。 默认将它设为较小值（如，Oracle上设为 10）有助于 JDBC 驱动上的性能优化。</p><ul><li>其实该部分翻译自Spark官方文档，所以对于翻译有疑问的可直接看官方文档 </li></ul><h2>2、scala代码实现连接mysql</h2><h2>2.1 添加mysql 依赖</h2><p>在sbt 配置文件里添加：</p><div class=\"highlight\"><pre><code class=\"language-text\">&#34;mysql&#34; % &#34;mysql-connector-java&#34; % &#34;6.0.6&#34;</code></pre></div><p>然后执行：</p><div class=\"highlight\"><pre><code class=\"language-bash\">sbt eclipse</code></pre></div><h2>2.2 建表并初始化数据</h2><div class=\"highlight\"><pre><code class=\"language-sql\"><span class=\"k\">DROP</span> <span class=\"k\">TABLE</span> <span class=\"k\">IF</span> <span class=\"k\">EXISTS</span> <span class=\"o\">`</span><span class=\"n\">USER_T</span><span class=\"o\">`</span><span class=\"p\">;</span>  \n<span class=\"k\">CREATE</span> <span class=\"k\">TABLE</span> <span class=\"o\">`</span><span class=\"n\">USER_T</span><span class=\"o\">`</span> <span class=\"p\">(</span>  \n  <span class=\"o\">`</span><span class=\"n\">ID</span><span class=\"o\">`</span> <span class=\"nb\">INT</span><span class=\"p\">(</span><span class=\"mi\">11</span><span class=\"p\">)</span> <span class=\"k\">NOT</span> <span class=\"k\">NULL</span><span class=\"p\">,</span>  \n  <span class=\"o\">`</span><span class=\"n\">USER_NAME</span><span class=\"o\">`</span> <span class=\"nb\">VARCHAR</span><span class=\"p\">(</span><span class=\"mi\">40</span><span class=\"p\">)</span> <span class=\"k\">NOT</span> <span class=\"k\">NULL</span><span class=\"p\">,</span>  \n  <span class=\"k\">PRIMARY</span> <span class=\"k\">KEY</span> <span class=\"p\">(</span><span class=\"o\">`</span><span class=\"n\">ID</span><span class=\"o\">`</span><span class=\"p\">)</span>  \n<span class=\"p\">)</span> <span class=\"n\">ENGINE</span><span class=\"o\">=</span><span class=\"n\">INNODB</span>  <span class=\"k\">DEFAULT</span> <span class=\"n\">CHARSET</span><span class=\"o\">=</span><span class=\"n\">UTF8</span><span class=\"p\">;</span>\n<span class=\"k\">INSERT</span>  <span class=\"k\">INTO</span> <span class=\"o\">`</span><span class=\"n\">USER_T</span><span class=\"o\">`</span><span class=\"p\">(</span><span class=\"o\">`</span><span class=\"n\">ID</span><span class=\"o\">`</span><span class=\"p\">,</span><span class=\"o\">`</span><span class=\"n\">USER_NAME</span><span class=\"o\">`</span><span class=\"p\">)</span> <span class=\"k\">VALUES</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"s1\">&#39;测试1&#39;</span><span class=\"p\">);</span>\n<span class=\"k\">INSERT</span>  <span class=\"k\">INTO</span> <span class=\"o\">`</span><span class=\"n\">USER_T</span><span class=\"o\">`</span><span class=\"p\">(</span><span class=\"o\">`</span><span class=\"n\">ID</span><span class=\"o\">`</span><span class=\"p\">,</span><span class=\"o\">`</span><span class=\"n\">USER_NAME</span><span class=\"o\">`</span><span class=\"p\">)</span> <span class=\"k\">VALUES</span> <span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">,</span><span class=\"s1\">&#39;测试2&#39;</span><span class=\"p\">);</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-32c4b7d81951e979b84151d20cd52d47_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"302\" data-rawheight=\"167\" class=\"content_image\" width=\"302\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;302&#39; height=&#39;167&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"302\" data-rawheight=\"167\" class=\"content_image lazy\" width=\"302\" data-actualsrc=\"https://pic4.zhimg.com/v2-32c4b7d81951e979b84151d20cd52d47_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>2.3 代码</h2><h2>2.3.1 查询</h2><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">package</span> <span class=\"nn\">com.dkl.leanring.spark.sql</span>\n\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.SparkSession</span>\n\n<span class=\"cm\">/**\n</span><span class=\"cm\"> * spark查询mysql测试\n</span><span class=\"cm\"> */</span>\n<span class=\"k\">object</span> <span class=\"nc\">MysqlQueryDemo</span> <span class=\"o\">{</span>\n\n  <span class=\"k\">def</span> <span class=\"n\">main</span><span class=\"o\">(</span><span class=\"n\">args</span><span class=\"k\">:</span> <span class=\"kt\">Array</span><span class=\"o\">[</span><span class=\"kt\">String</span><span class=\"o\">])</span><span class=\"k\">:</span> <span class=\"kt\">Unit</span> <span class=\"o\">=</span> <span class=\"o\">{</span>\n    <span class=\"k\">val</span> <span class=\"n\">spark</span> <span class=\"k\">=</span> <span class=\"nc\">SparkSession</span><span class=\"o\">.</span><span class=\"n\">builder</span><span class=\"o\">().</span><span class=\"n\">appName</span><span class=\"o\">(</span><span class=\"s\">&#34;MysqlQueryDemo&#34;</span><span class=\"o\">).</span><span class=\"n\">master</span><span class=\"o\">(</span><span class=\"s\">&#34;local&#34;</span><span class=\"o\">).</span><span class=\"n\">getOrCreate</span><span class=\"o\">()</span>\n    <span class=\"k\">val</span> <span class=\"n\">jdbcDF</span> <span class=\"k\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">read</span>\n      <span class=\"o\">.</span><span class=\"n\">format</span><span class=\"o\">(</span><span class=\"s\">&#34;jdbc&#34;</span><span class=\"o\">)</span>\n      <span class=\"o\">.</span><span class=\"n\">option</span><span class=\"o\">(</span><span class=\"s\">&#34;url&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;jdbc:mysql://192.168.44.128:3306/hive?useUnicode=true&amp;characterEncoding=utf-8&#34;</span><span class=\"o\">)</span>\n      <span class=\"o\">.</span><span class=\"n\">option</span><span class=\"o\">(</span><span class=\"s\">&#34;dbtable&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;USER_T&#34;</span><span class=\"o\">)</span>\n      <span class=\"o\">.</span><span class=\"n\">option</span><span class=\"o\">(</span><span class=\"s\">&#34;user&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;root&#34;</span><span class=\"o\">)</span>\n      <span class=\"o\">.</span><span class=\"n\">option</span><span class=\"o\">(</span><span class=\"s\">&#34;password&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;Root-123456&#34;</span><span class=\"o\">)</span>\n      <span class=\"o\">.</span><span class=\"n\">load</span><span class=\"o\">()</span>\n    <span class=\"n\">jdbcDF</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"o\">()</span>\n  <span class=\"o\">}</span>\n<span class=\"o\">}</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-58ed02ed139ce3809b159b44af3f5673_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1438\" data-rawheight=\"735\" class=\"origin_image zh-lightbox-thumb\" width=\"1438\" data-original=\"https://pic4.zhimg.com/v2-58ed02ed139ce3809b159b44af3f5673_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1438&#39; height=&#39;735&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1438\" data-rawheight=\"735\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1438\" data-original=\"https://pic4.zhimg.com/v2-58ed02ed139ce3809b159b44af3f5673_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-58ed02ed139ce3809b159b44af3f5673_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>2.3.2 插入数据</h2><p>新建USER_T.csv,造几条数据如图： （需将csv的编码格式转为utf-8,否则spark读取中文乱码，转码方法见：<a href=\"https://link.zhihu.com/?target=https%3A//jingyan.baidu.com/article/fea4511a092e53f7bb912528.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">jingyan.baidu.com/artic</span><span class=\"invisible\">le/fea4511a092e53f7bb912528.html</span><span class=\"ellipsis\"></span></a>） </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-014723ec1b83d9a2e10b9f9b99e1f595_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1151\" data-rawheight=\"359\" class=\"origin_image zh-lightbox-thumb\" width=\"1151\" data-original=\"https://pic2.zhimg.com/v2-014723ec1b83d9a2e10b9f9b99e1f595_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1151&#39; height=&#39;359&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1151\" data-rawheight=\"359\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1151\" data-original=\"https://pic2.zhimg.com/v2-014723ec1b83d9a2e10b9f9b99e1f595_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-014723ec1b83d9a2e10b9f9b99e1f595_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">package</span> <span class=\"nn\">com.dkl.leanring.spark.sql</span>\n\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.SparkSession</span>\n<span class=\"k\">import</span> <span class=\"nn\">org.apache.spark.sql.SaveMode</span>\n<span class=\"k\">import</span> <span class=\"nn\">java.util.Properties</span>\n\n<span class=\"cm\">/**\n</span><span class=\"cm\"> * 从USER_T.csv读取数据并插入的mysql表中\n</span><span class=\"cm\"> */</span>\n<span class=\"k\">object</span> <span class=\"nc\">MysqlInsertDemo</span> <span class=\"o\">{</span>\n  <span class=\"k\">def</span> <span class=\"n\">main</span><span class=\"o\">(</span><span class=\"n\">args</span><span class=\"k\">:</span> <span class=\"kt\">Array</span><span class=\"o\">[</span><span class=\"kt\">String</span><span class=\"o\">])</span><span class=\"k\">:</span> <span class=\"kt\">Unit</span> <span class=\"o\">=</span> <span class=\"o\">{</span>\n    <span class=\"k\">val</span> <span class=\"n\">spark</span> <span class=\"k\">=</span> <span class=\"nc\">SparkSession</span><span class=\"o\">.</span><span class=\"n\">builder</span><span class=\"o\">().</span><span class=\"n\">appName</span><span class=\"o\">(</span><span class=\"s\">&#34;MysqlInsertDemo&#34;</span><span class=\"o\">).</span><span class=\"n\">master</span><span class=\"o\">(</span><span class=\"s\">&#34;local&#34;</span><span class=\"o\">).</span><span class=\"n\">getOrCreate</span><span class=\"o\">()</span>\n    <span class=\"k\">val</span> <span class=\"n\">df</span> <span class=\"k\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">read</span><span class=\"o\">.</span><span class=\"n\">option</span><span class=\"o\">(</span><span class=\"s\">&#34;header&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;true&#34;</span><span class=\"o\">).</span><span class=\"n\">csv</span><span class=\"o\">(</span><span class=\"s\">&#34;src/main/resources/scala/USER_T.csv&#34;</span><span class=\"o\">)</span>\n    <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"o\">()</span>\n    <span class=\"k\">val</span> <span class=\"n\">url</span> <span class=\"k\">=</span> <span class=\"s\">&#34;jdbc:mysql://192.168.44.128:3306/hive?useUnicode=true&amp;characterEncoding=utf-8&#34;</span>\n    <span class=\"k\">val</span> <span class=\"n\">prop</span> <span class=\"k\">=</span> <span class=\"k\">new</span> <span class=\"nc\">Properties</span><span class=\"o\">()</span>\n    <span class=\"n\">prop</span><span class=\"o\">.</span><span class=\"n\">put</span><span class=\"o\">(</span><span class=\"s\">&#34;user&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;root&#34;</span><span class=\"o\">)</span>\n    <span class=\"n\">prop</span><span class=\"o\">.</span><span class=\"n\">put</span><span class=\"o\">(</span><span class=\"s\">&#34;password&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;Root-123456&#34;</span><span class=\"o\">)</span>\n    <span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">write</span><span class=\"o\">.</span><span class=\"n\">mode</span><span class=\"o\">(</span><span class=\"nc\">SaveMode</span><span class=\"o\">.</span><span class=\"nc\">Append</span><span class=\"o\">).</span><span class=\"n\">jdbc</span><span class=\"o\">(</span><span class=\"n\">url</span><span class=\"o\">,</span> <span class=\"s\">&#34;USER_T&#34;</span><span class=\"o\">,</span> <span class=\"n\">prop</span><span class=\"o\">)</span>\n  <span class=\"o\">}</span>\n<span class=\"o\">}</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-703c6da7f9ab961ec05441a9b32be99b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1435\" data-rawheight=\"865\" class=\"origin_image zh-lightbox-thumb\" width=\"1435\" data-original=\"https://pic4.zhimg.com/v2-703c6da7f9ab961ec05441a9b32be99b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1435&#39; height=&#39;865&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1435\" data-rawheight=\"865\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1435\" data-original=\"https://pic4.zhimg.com/v2-703c6da7f9ab961ec05441a9b32be99b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-703c6da7f9ab961ec05441a9b32be99b_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>再查询一次，就会发现表里多了几条数据</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-84e6df3e677e13b9cbe159f93b8c5ae4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1499\" data-rawheight=\"279\" class=\"origin_image zh-lightbox-thumb\" width=\"1499\" data-original=\"https://pic1.zhimg.com/v2-84e6df3e677e13b9cbe159f93b8c5ae4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1499&#39; height=&#39;279&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1499\" data-rawheight=\"279\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1499\" data-original=\"https://pic1.zhimg.com/v2-84e6df3e677e13b9cbe159f93b8c5ae4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-84e6df3e677e13b9cbe159f93b8c5ae4_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>3、注意（更新）</h2><p>上面的代码在本地eclipse运行是没有问题的,如果放在服务器上用spark-submit提交的话，可能会报异常</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"n\">java</span><span class=\"o\">.</span><span class=\"n\">sql</span><span class=\"o\">.</span><span class=\"nc\">SQLException</span><span class=\"k\">:</span><span class=\"kt\">No</span> <span class=\"kt\">suitable</span> <span class=\"kt\">driver</span></code></pre></div><p>解决方法是在代码里添加 mysql:</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"o\">.</span><span class=\"n\">option</span><span class=\"o\">(</span><span class=\"s\">&#34;driver&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;com.mysql.jdbc.Driver&#34;</span><span class=\"o\">)</span></code></pre></div><p>oracle:</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"o\">.</span><span class=\"n\">option</span><span class=\"o\">(</span><span class=\"s\">&#34;driver&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;oracle.jdbc.driver.OracleDriver&#34;</span><span class=\"o\">)</span></code></pre></div><p>具体可参考我的另一篇博客：<a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/05/06/sparkSubmitException/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">spark-submit报错:Exception in thread &#34;main&#34; java.sql.SQLException:No suitable driver</a></p><h2>4、其他读取mysql的方法（更新于2018.08.22）</h2><p>发现还有Spark还有其他读取mysql的方法，其实只是上面讲的快捷方式（2.3.1），只附上用法，具体看以看api或参考其他博客，如<a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/liuzongxi/article/details/51764104\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">spark：scala读取mysql的4种方法</a> (我就是在这篇博客看到的，该博客是Spark1.x的写法，若用Spark2.x将sqlContext改为本文的spark即可)</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">read</span><span class=\"o\">.</span><span class=\"n\">jdbc</span><span class=\"o\">(</span><span class=\"n\">url</span><span class=\"o\">,</span> <span class=\"n\">table</span><span class=\"o\">,</span> <span class=\"n\">columnName</span><span class=\"o\">,</span> <span class=\"n\">lowerBound</span><span class=\"o\">,</span> <span class=\"n\">upperBound</span><span class=\"o\">,</span> <span class=\"n\">numPartitions</span><span class=\"o\">,</span> <span class=\"n\">connectionProperties</span><span class=\"o\">)</span>\n<span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">read</span><span class=\"o\">.</span><span class=\"n\">jdbc</span><span class=\"o\">(</span><span class=\"n\">url</span><span class=\"o\">,</span> <span class=\"n\">table</span><span class=\"o\">,</span> <span class=\"n\">predicates</span><span class=\"o\">,</span> <span class=\"n\">connectionProperties</span><span class=\"o\">)</span>\n<span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">read</span><span class=\"o\">.</span><span class=\"n\">jdbc</span><span class=\"o\">(</span><span class=\"n\">url</span><span class=\"o\">,</span> <span class=\"n\">table</span><span class=\"o\">,</span> <span class=\"n\">properties</span><span class=\"o\">)</span></code></pre></div><ul><li>只要在2.3.1的代码里用.option(key,value)即可</li></ul><h2>5、关于读取mysql的分区设置（更新于2018.08.22）</h2><p>按照2.3.1的代码读取的DataFrame的分区数为1，若想改变分区数，一种方法是重分区</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"n\">df</span><span class=\"o\">.</span><span class=\"n\">repartition</span><span class=\"o\">(</span><span class=\"n\">numPartitions</span><span class=\"o\">)</span></code></pre></div><p>另一种是在读的时候设置分区数，在第一部分可以看到，通过numPartitions可以设置分区数，但是注意partitionColumn, lowerBound, upperBound, numPartitions需要同时设置 如：</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">val</span> <span class=\"n\">jdbcDF</span> <span class=\"k\">=</span> <span class=\"n\">spark</span><span class=\"o\">.</span><span class=\"n\">read</span>\n     <span class=\"o\">.</span><span class=\"n\">format</span><span class=\"o\">(</span><span class=\"s\">&#34;jdbc&#34;</span><span class=\"o\">)</span>\n     <span class=\"o\">.</span><span class=\"n\">option</span><span class=\"o\">(</span><span class=\"s\">&#34;url&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;jdbc:mysql://192.168.44.128:3306/hive?useUnicode=true&amp;characterEncoding=utf-8&#34;</span><span class=\"o\">)</span>\n     <span class=\"o\">.</span><span class=\"n\">option</span><span class=\"o\">(</span><span class=\"s\">&#34;dbtable&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;USER_T&#34;</span><span class=\"o\">)</span>\n     <span class=\"o\">.</span><span class=\"n\">option</span><span class=\"o\">(</span><span class=\"s\">&#34;user&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;root&#34;</span><span class=\"o\">)</span>\n     <span class=\"o\">.</span><span class=\"n\">option</span><span class=\"o\">(</span><span class=\"s\">&#34;password&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;Root-123456&#34;</span><span class=\"o\">)</span>\n     <span class=\"o\">.</span><span class=\"n\">option</span><span class=\"o\">(</span><span class=\"s\">&#34;numPartitions&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;160&#34;</span><span class=\"o\">)</span>\n     <span class=\"o\">.</span><span class=\"n\">option</span><span class=\"o\">(</span><span class=\"s\">&#34;partitionColumn&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;ID&#34;</span><span class=\"o\">)</span>\n     <span class=\"o\">.</span><span class=\"n\">option</span><span class=\"o\">(</span><span class=\"s\">&#34;lowerBound&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;1&#34;</span><span class=\"o\">)</span>\n     <span class=\"o\">.</span><span class=\"n\">option</span><span class=\"o\">(</span><span class=\"s\">&#34;upperBound&#34;</span><span class=\"o\">,</span> <span class=\"s\">&#34;1000&#34;</span><span class=\"o\">)</span>\n     <span class=\"o\">.</span><span class=\"n\">load</span><span class=\"o\">()</span></code></pre></div><p></p>", 
            "topic": [
                {
                    "tag": "MySQL", 
                    "tagLink": "https://api.zhihu.com/topics/19554128"
                }, 
                {
                    "tag": "Spark", 
                    "tagLink": "https://api.zhihu.com/topics/19942170"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/43374331", 
            "userName": "董可伦", 
            "userLink": "https://www.zhihu.com/people/de888cc734ddbfd918269a8e5fb196ba", 
            "upvote": 0, 
            "title": "scala 两个map合并，key相同时value相加", 
            "content": "<p>我的原创地址：</p><a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/04/01/scalaMapAdd/\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-0b4e5faa279a5e545d79e754b3dba862_ipico.jpg\" data-image-width=\"640\" data-image-height=\"640\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">scala 两个map合并，key相同时value相加</a><p class=\"ztext-empty-paragraph\"><br/></p><h2>1、先看一下map自带的合并操作的效果</h2><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">val</span> <span class=\"n\">map1</span> <span class=\"k\">=</span> <span class=\"nc\">Map</span><span class=\"o\">(</span><span class=\"s\">&#34;key1&#34;</span> <span class=\"o\">-&gt;</span> <span class=\"mi\">1</span><span class=\"o\">,</span> <span class=\"s\">&#34;key2&#34;</span> <span class=\"o\">-&gt;</span> <span class=\"mi\">3</span><span class=\"o\">,</span> <span class=\"s\">&#34;key3&#34;</span> <span class=\"o\">-&gt;</span> <span class=\"mi\">5</span><span class=\"o\">)</span>\n<span class=\"k\">val</span> <span class=\"n\">map2</span> <span class=\"k\">=</span> <span class=\"nc\">Map</span><span class=\"o\">(</span><span class=\"s\">&#34;key2&#34;</span> <span class=\"o\">-&gt;</span> <span class=\"mi\">4</span><span class=\"o\">,</span> <span class=\"s\">&#34;key3&#34;</span> <span class=\"o\">-&gt;</span> <span class=\"mi\">6</span><span class=\"o\">,</span> <span class=\"s\">&#34;key5&#34;</span> <span class=\"o\">-&gt;</span> <span class=\"mi\">10</span><span class=\"o\">)</span>\n<span class=\"n\">println</span><span class=\"o\">(</span><span class=\"n\">map1</span> <span class=\"o\">+</span> <span class=\"o\">(</span><span class=\"s\">&#34;key1&#34;</span> <span class=\"o\">-&gt;</span> <span class=\"mi\">3</span><span class=\"o\">))</span>\n<span class=\"n\">println</span><span class=\"o\">(</span><span class=\"n\">map1</span> <span class=\"o\">++</span> <span class=\"n\">map2</span><span class=\"o\">)</span></code></pre></div><p>结果：</p><div class=\"highlight\"><pre><code class=\"language-text\">Map(key1 -&gt; 3, key2 -&gt; 3, key3 -&gt; 5)\nMap(key1 -&gt; 1, key2 -&gt; 4, key3 -&gt; 6, key5 -&gt; 10)</code></pre></div><p>可以看到现有的方法在key相同时，没有将value相加，而是操作符右边的值把左边的值覆盖掉了。 </p><h2>2、利用map函数</h2><h2>2.1 为了便于理解先看如下代码</h2><p>代码：</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">val</span> <span class=\"n\">map1</span> <span class=\"k\">=</span> <span class=\"nc\">Map</span><span class=\"o\">(</span><span class=\"s\">&#34;key1&#34;</span> <span class=\"o\">-&gt;</span> <span class=\"mi\">1</span><span class=\"o\">,</span> <span class=\"s\">&#34;key2&#34;</span> <span class=\"o\">-&gt;</span> <span class=\"mi\">3</span><span class=\"o\">,</span> <span class=\"s\">&#34;key3&#34;</span> <span class=\"o\">-&gt;</span> <span class=\"mi\">5</span><span class=\"o\">)</span>\n<span class=\"n\">map1</span><span class=\"o\">.</span><span class=\"n\">map</span> <span class=\"o\">{</span> <span class=\"n\">t</span> <span class=\"k\">=&gt;</span> <span class=\"n\">println</span><span class=\"o\">(</span><span class=\"n\">t</span><span class=\"o\">.</span><span class=\"n\">_1</span><span class=\"o\">,</span> <span class=\"n\">t</span><span class=\"o\">.</span><span class=\"n\">_2</span><span class=\"o\">)</span> <span class=\"o\">}</span></code></pre></div><p>结果：</p><div class=\"highlight\"><pre><code class=\"language-text\">(key1,1)\n(key2,3)\n(key3,5)</code></pre></div><p>可以看出map函数会遍历集合中的每个元素 可以为其指定返回结果：</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"n\">println</span><span class=\"o\">(</span><span class=\"n\">map1</span><span class=\"o\">.</span><span class=\"n\">map</span> <span class=\"o\">{</span> <span class=\"n\">t</span> <span class=\"k\">=&gt;</span> <span class=\"mi\">2</span> <span class=\"o\">})</span>\n<span class=\"n\">println</span><span class=\"o\">(</span><span class=\"n\">map1</span><span class=\"o\">.</span><span class=\"n\">map</span> <span class=\"o\">{</span> <span class=\"n\">t</span> <span class=\"k\">=&gt;</span> <span class=\"n\">t</span><span class=\"o\">.</span><span class=\"n\">_1</span> <span class=\"o\">-&gt;</span> <span class=\"n\">t</span><span class=\"o\">.</span><span class=\"n\">_2</span> <span class=\"o\">})</span></code></pre></div><p>结果：</p><div class=\"highlight\"><pre><code class=\"language-text\">List(2, 2, 2)\nMap(key1 -&gt; 1, key2 -&gt; 3, key3 -&gt; 5)</code></pre></div><p>可以看出，该函数返回结果类型可以不同，并且会将我们指定的值，组成一个集合，并自动判断返回类型。</p><h2>2.2 合并两个map</h2><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">val</span> <span class=\"n\">map1</span> <span class=\"k\">=</span> <span class=\"nc\">Map</span><span class=\"o\">(</span><span class=\"s\">&#34;key1&#34;</span> <span class=\"o\">-&gt;</span> <span class=\"mi\">1</span><span class=\"o\">,</span> <span class=\"s\">&#34;key2&#34;</span> <span class=\"o\">-&gt;</span> <span class=\"mi\">3</span><span class=\"o\">,</span> <span class=\"s\">&#34;key3&#34;</span> <span class=\"o\">-&gt;</span> <span class=\"mi\">5</span><span class=\"o\">)</span>\n<span class=\"k\">val</span> <span class=\"n\">map2</span> <span class=\"k\">=</span> <span class=\"nc\">Map</span><span class=\"o\">(</span><span class=\"s\">&#34;key2&#34;</span> <span class=\"o\">-&gt;</span> <span class=\"mi\">4</span><span class=\"o\">,</span> <span class=\"s\">&#34;key3&#34;</span> <span class=\"o\">-&gt;</span> <span class=\"mi\">6</span><span class=\"o\">,</span> <span class=\"s\">&#34;key5&#34;</span> <span class=\"o\">-&gt;</span> <span class=\"mi\">10</span><span class=\"o\">)</span>\n<span class=\"k\">val</span> <span class=\"n\">mapAdd1</span> <span class=\"k\">=</span> <span class=\"n\">map1</span> <span class=\"o\">++</span> <span class=\"n\">map2</span><span class=\"o\">.</span><span class=\"n\">map</span><span class=\"o\">(</span><span class=\"n\">t</span> <span class=\"k\">=&gt;</span> <span class=\"n\">t</span><span class=\"o\">.</span><span class=\"n\">_1</span> <span class=\"o\">-&gt;</span> <span class=\"o\">(</span><span class=\"n\">t</span><span class=\"o\">.</span><span class=\"n\">_2</span> <span class=\"o\">+</span> <span class=\"n\">map1</span><span class=\"o\">.</span><span class=\"n\">getOrElse</span><span class=\"o\">(</span><span class=\"n\">t</span><span class=\"o\">.</span><span class=\"n\">_1</span><span class=\"o\">,</span> <span class=\"mi\">0</span><span class=\"o\">)))</span>\n<span class=\"n\">println</span><span class=\"o\">(</span><span class=\"n\">mapAdd1</span><span class=\"o\">)</span></code></pre></div><p>其中map.getOrElse(key,default):如果map中有这个key,则返回对应的value,否在返回default 结果：</p><div class=\"highlight\"><pre><code class=\"language-text\">Map(key1 -&gt; 1, key2 -&gt; 7, key3 -&gt; 11, key5 -&gt; 10)</code></pre></div><h2>3、用foldLeft</h2><h2>3.1 语法</h2><p>以下三种写法是相等的：</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"nc\">List</span><span class=\"o\">(</span><span class=\"mi\">1</span><span class=\"o\">,</span> <span class=\"mi\">2</span><span class=\"o\">,</span> <span class=\"mi\">3</span><span class=\"o\">,</span> <span class=\"mi\">4</span><span class=\"o\">).</span><span class=\"n\">foldLeft</span><span class=\"o\">(</span><span class=\"mi\">0</span><span class=\"o\">)((</span><span class=\"n\">sum</span><span class=\"o\">,</span> <span class=\"n\">i</span><span class=\"o\">)</span> <span class=\"k\">=&gt;</span> <span class=\"n\">sum</span> <span class=\"o\">+</span> <span class=\"n\">i</span><span class=\"o\">)</span>\n<span class=\"o\">(</span><span class=\"nc\">List</span><span class=\"o\">(</span><span class=\"mi\">1</span><span class=\"o\">,</span> <span class=\"mi\">2</span><span class=\"o\">,</span> <span class=\"mi\">3</span><span class=\"o\">,</span> <span class=\"mi\">4</span><span class=\"o\">)</span> <span class=\"n\">foldLeft</span> <span class=\"mi\">0</span><span class=\"o\">)((</span><span class=\"n\">sum</span><span class=\"o\">,</span> <span class=\"n\">i</span><span class=\"o\">)</span> <span class=\"k\">=&gt;</span> <span class=\"n\">sum</span> <span class=\"o\">+</span> <span class=\"n\">i</span><span class=\"o\">)</span>\n<span class=\"o\">(</span><span class=\"mi\">0</span> <span class=\"o\">/:</span> <span class=\"nc\">List</span><span class=\"o\">(</span><span class=\"mi\">1</span><span class=\"o\">,</span> <span class=\"mi\">2</span><span class=\"o\">,</span> <span class=\"mi\">3</span><span class=\"o\">,</span> <span class=\"mi\">4</span><span class=\"o\">))(</span><span class=\"k\">_</span> <span class=\"o\">+</span> <span class=\"k\">_</span><span class=\"o\">)</span></code></pre></div><p>为了便于理解，先看下面代码：</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"o\">(</span><span class=\"mi\">0</span> <span class=\"o\">/:</span> <span class=\"nc\">List</span><span class=\"o\">(</span><span class=\"mi\">1</span><span class=\"o\">,</span> <span class=\"mi\">2</span><span class=\"o\">,</span> <span class=\"mi\">3</span><span class=\"o\">,</span> <span class=\"mi\">4</span><span class=\"o\">))((</span><span class=\"n\">sum</span><span class=\"o\">,</span> <span class=\"n\">i</span><span class=\"o\">)</span> <span class=\"k\">=&gt;</span> <span class=\"o\">{</span>\n  <span class=\"n\">println</span><span class=\"o\">(</span><span class=\"s\">s&#34;sum=</span><span class=\"si\">${</span><span class=\"n\">sum</span><span class=\"si\">}</span><span class=\"s\"> i=</span><span class=\"si\">${</span><span class=\"n\">i</span><span class=\"si\">}</span><span class=\"s\">&#34;</span><span class=\"o\">)</span>\n  <span class=\"n\">sum</span>\n<span class=\"o\">})</span></code></pre></div><p>结果：</p><div class=\"highlight\"><pre><code class=\"language-text\">sum=0 i=1\nsum=0 i=2\nsum=0 i=3\nsum=0 i=4</code></pre></div><p>该函数的功能是从左往右遍历右边操作类型List,而sum对应的是对应的左边的0，该函数要求返回值类型和左边类型一致，上面的例子中返回值是Int。 同理，两个Map的代码如下：</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">val</span> <span class=\"n\">map1</span> <span class=\"k\">=</span> <span class=\"nc\">Map</span><span class=\"o\">(</span><span class=\"s\">&#34;key1&#34;</span> <span class=\"o\">-&gt;</span> <span class=\"mi\">1</span><span class=\"o\">,</span> <span class=\"s\">&#34;key2&#34;</span> <span class=\"o\">-&gt;</span> <span class=\"mi\">3</span><span class=\"o\">,</span> <span class=\"s\">&#34;key3&#34;</span> <span class=\"o\">-&gt;</span> <span class=\"mi\">5</span><span class=\"o\">)</span>\n<span class=\"k\">val</span> <span class=\"n\">map2</span> <span class=\"k\">=</span> <span class=\"nc\">Map</span><span class=\"o\">(</span><span class=\"s\">&#34;key2&#34;</span> <span class=\"o\">-&gt;</span> <span class=\"mi\">4</span><span class=\"o\">,</span> <span class=\"s\">&#34;key3&#34;</span> <span class=\"o\">-&gt;</span> <span class=\"mi\">6</span><span class=\"o\">,</span> <span class=\"s\">&#34;key5&#34;</span> <span class=\"o\">-&gt;</span> <span class=\"mi\">10</span><span class=\"o\">)</span>\n<span class=\"o\">(</span><span class=\"n\">map1</span> <span class=\"o\">/:</span> <span class=\"n\">map2</span><span class=\"o\">)((</span><span class=\"n\">map</span><span class=\"o\">,</span> <span class=\"n\">kv</span><span class=\"o\">)</span> <span class=\"k\">=&gt;</span> <span class=\"o\">{</span>\n  <span class=\"n\">println</span><span class=\"o\">(</span><span class=\"s\">s&#34;map=</span><span class=\"si\">${</span><span class=\"n\">map</span><span class=\"si\">}</span><span class=\"s\"> kv=</span><span class=\"si\">${</span><span class=\"n\">kv</span><span class=\"si\">}</span><span class=\"s\">&#34;</span><span class=\"o\">)</span>\n  <span class=\"n\">map</span>\n<span class=\"o\">})</span></code></pre></div><p>结果：</p><div class=\"highlight\"><pre><code class=\"language-text\">map=Map(key1 -&gt; 1, key2 -&gt; 3, key3 -&gt; 5) kv=(key2,4)\nmap=Map(key1 -&gt; 1, key2 -&gt; 3, key3 -&gt; 5) kv=(key3,6)\nmap=Map(key1 -&gt; 1, key2 -&gt; 3, key3 -&gt; 5) kv=(key5,10)</code></pre></div><p>从结果中可以看出左边map对应的是map1整体，而不是遍历map1</p><h2>3.2 合并两个map：</h2><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">val</span> <span class=\"n\">map1</span> <span class=\"k\">=</span> <span class=\"nc\">Map</span><span class=\"o\">(</span><span class=\"s\">&#34;key1&#34;</span> <span class=\"o\">-&gt;</span> <span class=\"mi\">1</span><span class=\"o\">,</span> <span class=\"s\">&#34;key2&#34;</span> <span class=\"o\">-&gt;</span> <span class=\"mi\">3</span><span class=\"o\">,</span> <span class=\"s\">&#34;key3&#34;</span> <span class=\"o\">-&gt;</span> <span class=\"mi\">5</span><span class=\"o\">)</span>\n<span class=\"k\">val</span> <span class=\"n\">map2</span> <span class=\"k\">=</span> <span class=\"nc\">Map</span><span class=\"o\">(</span><span class=\"s\">&#34;key2&#34;</span> <span class=\"o\">-&gt;</span> <span class=\"mi\">4</span><span class=\"o\">,</span> <span class=\"s\">&#34;key3&#34;</span> <span class=\"o\">-&gt;</span> <span class=\"mi\">6</span><span class=\"o\">,</span> <span class=\"s\">&#34;key5&#34;</span> <span class=\"o\">-&gt;</span> <span class=\"mi\">10</span><span class=\"o\">)</span>\n<span class=\"k\">val</span> <span class=\"n\">mapAdd2</span> <span class=\"k\">=</span> <span class=\"o\">(</span><span class=\"n\">map1</span> <span class=\"o\">/:</span> <span class=\"n\">map2</span><span class=\"o\">)((</span><span class=\"n\">map</span><span class=\"o\">,</span> <span class=\"n\">kv</span><span class=\"o\">)</span> <span class=\"k\">=&gt;</span> <span class=\"o\">{</span>\n  <span class=\"n\">map</span> <span class=\"o\">+</span> <span class=\"o\">(</span><span class=\"n\">kv</span><span class=\"o\">.</span><span class=\"n\">_1</span> <span class=\"o\">-&gt;</span> <span class=\"o\">(</span><span class=\"n\">kv</span><span class=\"o\">.</span><span class=\"n\">_2</span> <span class=\"o\">+</span> <span class=\"n\">map</span><span class=\"o\">.</span><span class=\"n\">getOrElse</span><span class=\"o\">(</span><span class=\"n\">kv</span><span class=\"o\">.</span><span class=\"n\">_1</span><span class=\"o\">,</span> <span class=\"mi\">0</span><span class=\"o\">)))</span>\n<span class=\"o\">})</span>\n<span class=\"n\">println</span><span class=\"o\">(</span><span class=\"n\">mapAdd2</span><span class=\"o\">)</span></code></pre></div><p>其中map.getOrElse(key,default):如果map中有这个key,则返回对应的value,否在返回default 结果：</p><div class=\"highlight\"><pre><code class=\"language-text\">Map(key1 -&gt; 1, key2 -&gt; 7, key3 -&gt; 11, key5 -&gt; 10)</code></pre></div><h2>4、用模式匹配</h2><p>在网上查的有的用的模式匹配，我感觉在这里这样用就是多余~ 附上代码：</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"k\">val</span> <span class=\"n\">mapAdd2</span> <span class=\"k\">=</span> <span class=\"n\">map1</span> <span class=\"o\">++</span> <span class=\"n\">map2</span><span class=\"o\">.</span><span class=\"n\">map</span> <span class=\"o\">{</span> <span class=\"k\">case</span> <span class=\"o\">(</span><span class=\"n\">key</span><span class=\"o\">,</span> <span class=\"n\">value</span><span class=\"o\">)</span> <span class=\"k\">=&gt;</span> <span class=\"n\">key</span> <span class=\"o\">-&gt;</span> <span class=\"o\">(</span><span class=\"n\">value</span> <span class=\"o\">+</span> <span class=\"n\">map1</span><span class=\"o\">.</span><span class=\"n\">getOrElse</span><span class=\"o\">(</span><span class=\"n\">key</span><span class=\"o\">,</span> <span class=\"mi\">0</span><span class=\"o\">))</span> <span class=\"o\">}</span>\n<span class=\"n\">println</span><span class=\"o\">(</span><span class=\"n\">mapAdd2</span><span class=\"o\">)</span>\n<span class=\"k\">val</span> <span class=\"n\">mapAdd3</span> <span class=\"k\">=</span> <span class=\"o\">(</span><span class=\"n\">map1</span> <span class=\"o\">/:</span> <span class=\"n\">map2</span><span class=\"o\">)</span> <span class=\"o\">{</span>\n  <span class=\"k\">case</span> <span class=\"o\">(</span><span class=\"n\">map</span><span class=\"o\">,</span> <span class=\"n\">kv</span><span class=\"o\">)</span> <span class=\"k\">=&gt;</span> <span class=\"o\">{</span>\n    <span class=\"n\">map</span> <span class=\"o\">+</span> <span class=\"o\">(</span><span class=\"n\">kv</span><span class=\"o\">.</span><span class=\"n\">_1</span> <span class=\"o\">-&gt;</span> <span class=\"o\">(</span><span class=\"n\">kv</span><span class=\"o\">.</span><span class=\"n\">_2</span> <span class=\"o\">+</span> <span class=\"n\">map</span><span class=\"o\">.</span><span class=\"n\">getOrElse</span><span class=\"o\">(</span><span class=\"n\">kv</span><span class=\"o\">.</span><span class=\"n\">_1</span><span class=\"o\">,</span> <span class=\"mi\">0</span><span class=\"o\">)))</span>\n  <span class=\"o\">}</span>\n<span class=\"o\">}</span>\n<span class=\"n\">println</span><span class=\"o\">(</span><span class=\"n\">mapAdd3</span><span class=\"o\">)</span>\n<span class=\"k\">val</span> <span class=\"n\">mapAdd4</span> <span class=\"k\">=</span> <span class=\"o\">(</span><span class=\"n\">map1</span> <span class=\"o\">/:</span> <span class=\"n\">map2</span><span class=\"o\">)</span> <span class=\"o\">{</span>\n  <span class=\"k\">case</span> <span class=\"o\">(</span><span class=\"n\">map</span><span class=\"o\">,</span> <span class=\"o\">(</span><span class=\"n\">k</span><span class=\"o\">,</span> <span class=\"n\">v</span><span class=\"o\">))</span> <span class=\"k\">=&gt;</span> <span class=\"o\">{</span>\n    <span class=\"n\">map</span> <span class=\"o\">+</span> <span class=\"o\">(</span><span class=\"n\">k</span> <span class=\"o\">-&gt;</span> <span class=\"o\">(</span><span class=\"n\">v</span> <span class=\"o\">+</span> <span class=\"n\">map</span><span class=\"o\">.</span><span class=\"n\">getOrElse</span><span class=\"o\">(</span><span class=\"n\">k</span><span class=\"o\">,</span> <span class=\"mi\">0</span><span class=\"o\">)))</span>\n  <span class=\"o\">}</span>\n<span class=\"o\">}</span>\n<span class=\"n\">println</span><span class=\"o\">(</span><span class=\"n\">mapAdd4</span><span class=\"o\">)</span></code></pre></div><p>结果是一样的：</p><div class=\"highlight\"><pre><code class=\"language-text\">Map(key1 -&gt; 1, key2 -&gt; 7, key3 -&gt; 11, key5 -&gt; 10)\nMap(key1 -&gt; 1, key2 -&gt; 7, key3 -&gt; 11, key5 -&gt; 10)\nMap(key1 -&gt; 1, key2 -&gt; 7, key3 -&gt; 11, key5 -&gt; 10)</code></pre></div><h2>参考</h2><p><a href=\"https://link.zhihu.com/?target=https%3A//www.cnblogs.com/tugeler/p/5134862.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">cnblogs.com/tugeler/p/5</span><span class=\"invisible\">134862.html</span><span class=\"ellipsis\"></span></a></p>", 
            "topic": [
                {
                    "tag": "Scala", 
                    "tagLink": "https://api.zhihu.com/topics/19566465"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/43374229", 
            "userName": "董可伦", 
            "userLink": "https://www.zhihu.com/people/de888cc734ddbfd918269a8e5fb196ba", 
            "upvote": 5, 
            "title": "centos7 安装oracle11", 
            "content": "<p>我的原创地址：</p><a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/05/05/oracleConf/\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic2.zhimg.com/v2-bbfac3166358f5b79b9ebdd368b73309_180x120.jpg\" data-image-width=\"911\" data-image-height=\"275\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">centos7 安装oracle11</a><h2>前言</h2><p>由于需要学习配置oracle goldengate(ogg),奈何没有oracle环境，所以想自己装一个oracle，搜了一下相关文档，跟着安装了一下，发现oracle安装比mysql安装麻烦多了，而且出现了很多博客上没有提到的错误，所以特此记录一下~</p><h2>1、下载</h2><p>下载地址：<a href=\"https://link.zhihu.com/?target=http%3A//www.oracle.com/technetwork/database/enterprise-edition/downloads/index.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">oracle.com/technetwork/</span><span class=\"invisible\">database/enterprise-edition/downloads/index.html</span><span class=\"ellipsis\"></span></a>，我下载的是Oracle Database 11g Release 2 (11.2.0.1.0) Linux x86-64，注意File1和File2都要下载</p><h2>2、为host添加映射</h2><p>我的虚拟机之前已经配好</p><div class=\"highlight\"><pre><code class=\"language-bash\"><span class=\"m\">192</span>.168.44.128 ambari.master.com</code></pre></div><h2>3、安装依赖</h2><h2>3.1 先安装pdksh</h2><p>centos7没有相关安装包可用，可下载pdksh的rpm包</p><div class=\"highlight\"><pre><code class=\"language-bash\">wget  http://vault.centos.org/5.11/os/x86_64/CentOS/pdksh-5.2.14-37.el5_8.1.x86_64.rpm\nrpm -ivh pdksh-5.2.14-37.el5_8.1.x86_64.rpm</code></pre></div><h2>3.2 安装其他依赖</h2><div class=\"highlight\"><pre><code class=\"language-text\">yum -y install binutils compat-libstdc++-33 elfutils-libelf elfutils-libelf-devel expat gcc gcc-c++ glibc glibc-common glibc-devel glibc-headers libaio libaio-devel libgcc libstdc++ libstdc++-devel make pdksh sysstat unixODBC unixODBC-devel</code></pre></div><h2>3.3 检查所有依赖是否安装完整</h2><div class=\"highlight\"><pre><code class=\"language-text\">rpm -q binutils compat-libstdc++-33 elfutils-libelf elfutils-libelf-devel expat gcc gcc-c++ glibc glibc-common glibc-devel glibc-headers libaio libaio-devel libgcc libstdc++ libstdc++-devel make pdksh sysstat unixODBC unixODBC-devel | grep &#34;not installed&#34;</code></pre></div><p>其中中文系统&#34;not installed&#34; 可能需要替换成中文相关的 </p><h2>4、添加oracle用户组和用户</h2><div class=\"highlight\"><pre><code class=\"language-bash\">groupadd oinstall\ngroupadd dba\ngroupadd asmadmin\ngroupadd asmdba\nuseradd -g oinstall -G dba,asmdba oracle -d /home/oracle</code></pre></div><p>查看oracle用户</p><div class=\"highlight\"><pre><code class=\"language-text\">id oracle</code></pre></div><p>为oracle 用户设置密码</p><div class=\"highlight\"><pre><code class=\"language-text\">passwd oracle</code></pre></div><h2>5、优化系统内核</h2><div class=\"highlight\"><pre><code class=\"language-bash\">vim /etc/sysctl.conf\nfs.aio-max-nr<span class=\"o\">=</span><span class=\"m\">1048576</span>\nfs.file-max<span class=\"o\">=</span><span class=\"m\">6815744</span>\nkernel.shmall<span class=\"o\">=</span><span class=\"m\">2097152</span>\nkernel.shmmni<span class=\"o\">=</span><span class=\"m\">4096</span>\nkernel.shmmax <span class=\"o\">=</span> <span class=\"m\">2147483648</span>\nkernel.sem<span class=\"o\">=</span><span class=\"m\">250</span> <span class=\"m\">32000</span> <span class=\"m\">100</span> <span class=\"m\">128</span>\nnet.ipv4.ip_local_port_range<span class=\"o\">=</span><span class=\"m\">9000</span> <span class=\"m\">65500</span>\nnet.core.rmem_default<span class=\"o\">=</span><span class=\"m\">262144</span>\nnet.core.rmem_max<span class=\"o\">=</span><span class=\"m\">4194304</span>\nnet.core.wmem_default<span class=\"o\">=</span><span class=\"m\">262144</span>\nnet.core.wmem_max<span class=\"o\">=</span><span class=\"m\">1048586</span></code></pre></div><p>其中kernel.shmmax为内存的一半，比如内存为4G，则kernel.shmmax=2<i>1024</i>1024*1024=2147483648 使参数生效</p><div class=\"highlight\"><pre><code class=\"language-bash\">sysctl -p</code></pre></div><h2>6、限制oracle用户的shell权限</h2><div class=\"highlight\"><pre><code class=\"language-bash\">vim /etc/security/limits.conf\noracle              soft    nproc   <span class=\"m\">2047</span>\noracle              hard    nproc   <span class=\"m\">16384</span>\noracle              soft    nofile  <span class=\"m\">1024</span>\noracle              hard    nofile  <span class=\"m\">65536</span>\nvim /etc/pam.d/login\nsession  required   /lib64/security/pam_limits.so\nsession  required   pam_limits.so\nvim /etc/profile\n<span class=\"k\">if</span> <span class=\"o\">[</span> <span class=\"nv\">$USER</span> <span class=\"o\">=</span> <span class=\"s2\">&#34;oracle&#34;</span> <span class=\"o\">]</span><span class=\"p\">;</span> <span class=\"k\">then</span>\n<span class=\"k\">if</span> <span class=\"o\">[</span> <span class=\"nv\">$SHELL</span> <span class=\"o\">=</span> <span class=\"s2\">&#34;/bin/ksh&#34;</span> <span class=\"o\">]</span><span class=\"p\">;</span> <span class=\"k\">then</span>\n<span class=\"nb\">ulimit</span> -p <span class=\"m\">16384</span>\n<span class=\"nb\">ulimit</span> -n <span class=\"m\">65536</span>\n<span class=\"k\">else</span>\n<span class=\"nb\">ulimit</span> -u <span class=\"m\">16384</span> -n <span class=\"m\">65536</span>\n<span class=\"k\">fi</span>\n<span class=\"k\">fi</span></code></pre></div><h2>7、创建oracle相关目录</h2><div class=\"highlight\"><pre><code class=\"language-bash\">mkdir -p /db/app/oracle/product/11.2.0\nmkdir /db/app/oracle/oradata\nmkdir /db/app/oracle/inventory\nmkdir /db/app/oracle/fast_recovery_area\nchown -R oracle:oinstall /db/app/oracle\nchmod -R <span class=\"m\">775</span> /db/app/oracle\nmkdir -p /u01/app/oracle/inventory\nchown -R oracle:oinstall /u01/app/oracle/inventory</code></pre></div><h2>8、配置oracle用户环境变量</h2><div class=\"highlight\"><pre><code class=\"language-bash\">su oracle\nvim .bash_profile\n<span class=\"nb\">umask</span> <span class=\"m\">022</span>\n<span class=\"nb\">export</span> <span class=\"nv\">ORACLE_HOSTNAME</span><span class=\"o\">=</span>ambari.master.com\n<span class=\"nb\">export</span> <span class=\"nv\">ORACLE_BASE</span><span class=\"o\">=</span>/db/app/oracle\n<span class=\"nb\">export</span> <span class=\"nv\">ORACLE_HOME</span><span class=\"o\">=</span><span class=\"nv\">$ORACLE_BASE</span>/product/11.2.0/\n<span class=\"nb\">export</span> <span class=\"nv\">ORACLE_SID</span><span class=\"o\">=</span>ORCL\n<span class=\"nb\">export</span> <span class=\"nv\">PATH</span><span class=\"o\">=</span>.:<span class=\"nv\">$ORACLE_HOME</span>/bin:<span class=\"nv\">$ORACLE_HOME</span>/OPatch:<span class=\"nv\">$ORACLE_HOME</span>/jdk/bin:<span class=\"nv\">$PATH</span>\n<span class=\"nb\">export</span> <span class=\"nv\">LC_ALL</span><span class=\"o\">=</span><span class=\"s2\">&#34;en_US&#34;</span>\n<span class=\"nb\">export</span> <span class=\"nv\">LANG</span><span class=\"o\">=</span><span class=\"s2\">&#34;en_US&#34;</span>\n<span class=\"nb\">export</span> <span class=\"nv\">NLS_LANG</span><span class=\"o\">=</span><span class=\"s2\">&#34;AMERICAN_AMERICA.ZHS16GBK&#34;</span>\n<span class=\"nb\">export</span> <span class=\"nv\">NLS_DATE_FORMAT</span><span class=\"o\">=</span><span class=\"s2\">&#34;YYYY-MM-DD HH24:MI:SS&#34;</span></code></pre></div><p>使之生效</p><div class=\"highlight\"><pre><code class=\"language-bash\"><span class=\"nb\">source</span> ~/.bash_profile</code></pre></div><p>关于Linux权限的小问题，此处网上资料是配置在.bash_profile即/home/oracle/.bash_profile文件里，我开始也是这样设置的，但每次重新登录切换到oracle用户发现每次都要重新source一下，后来发现必须以oracle用户登录才能每次都生效，不用重新source,如果用root用户登录，再切换到oracle用户的话，需要重新source的，为了方便，我把环境变量在/home/oracle/.bashrc里有配置了一份，这样我从root切换到oracle用户的也不需要重新source了，方便了我后面的使用，当然在生产上还是建议只配置在/home/oracle/.bash_profile，具体的Linux文件权限问题可自行在网上查一下相关资料，希望能对Linux文件权限不太熟悉的同学有所帮助，因为这也困扰了我好多天~</p><h2>9、解压安装包</h2><p>如果安装包在root用户下，现切换到root用户</p><div class=\"highlight\"><pre><code class=\"language-bash\">su\nunzip linux.x64_11gR2_database_1of2.zip -d /db\nunzip linux.x64_11gR2_database_2of2.zip -d /db</code></pre></div><p>然后执行</p><div class=\"highlight\"><pre><code class=\"language-bash\">mkdir /db/etc/\ncp /db/database/response/* /db/etc/\nvim /db/etc/db_install.rsp\noracle.install.option<span class=\"o\">=</span>INSTALL_DB_SWONLY\n<span class=\"nv\">DECLINE_SECURITY_UPDATES</span><span class=\"o\">=</span><span class=\"nb\">true</span>\n<span class=\"nv\">UNIX_GROUP_NAME</span><span class=\"o\">=</span>oinstall\n<span class=\"nv\">INVENTORY_LOCATION</span><span class=\"o\">=</span>/u01/app/oracle/inventory\n<span class=\"nv\">SELECTED_LANGUAGES</span><span class=\"o\">=</span>en,zh_CN\n<span class=\"nv\">ORACLE_HOSTNAME</span><span class=\"o\">=</span>ambari.master.com\n<span class=\"nv\">ORACLE_HOME</span><span class=\"o\">=</span>/db/app/oracle/product/11.2.0\n<span class=\"nv\">ORACLE_BASE</span><span class=\"o\">=</span>/db/app/oracle\noracle.install.db.InstallEdition<span class=\"o\">=</span>EE\noracle.install.db.isCustomInstall<span class=\"o\">=</span><span class=\"nb\">true</span>\noracle.install.db.DBA_GROUP<span class=\"o\">=</span>dba\noracle.install.db.OPER_GROUP<span class=\"o\">=</span>dba</code></pre></div><h2>10、安装</h2><p>先切换到oracle</p><div class=\"highlight\"><pre><code class=\"language-text\">su oracle\ncd /db/database/\n./runInstaller -silent -ignorePrereq -responseFile /db/etc/response/db_install.rsp</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-df4ba1e9345b2d44a6d8ec6b5e5bd049_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1310\" data-rawheight=\"142\" class=\"origin_image zh-lightbox-thumb\" width=\"1310\" data-original=\"https://pic2.zhimg.com/v2-df4ba1e9345b2d44a6d8ec6b5e5bd049_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1310&#39; height=&#39;142&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1310\" data-rawheight=\"142\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1310\" data-original=\"https://pic2.zhimg.com/v2-df4ba1e9345b2d44a6d8ec6b5e5bd049_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-df4ba1e9345b2d44a6d8ec6b5e5bd049_b.jpg\"/></figure><p> 可按他提示的查看日志，新增一个命令窗口，执行</p><div class=\"highlight\"><pre><code class=\"language-text\">tail -f  /u01/app/oracle/inventory/logs/installActions2018-05-04_11-48-18AM.log</code></pre></div><p>安装成功： </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-6530054770bf9ce981921b914a0d9724_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"892\" data-rawheight=\"315\" class=\"origin_image zh-lightbox-thumb\" width=\"892\" data-original=\"https://pic1.zhimg.com/v2-6530054770bf9ce981921b914a0d9724_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;892&#39; height=&#39;315&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"892\" data-rawheight=\"315\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"892\" data-original=\"https://pic1.zhimg.com/v2-6530054770bf9ce981921b914a0d9724_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-6530054770bf9ce981921b914a0d9724_b.jpg\"/></figure><p> 根据提示，执行</p><div class=\"highlight\"><pre><code class=\"language-bash\">su\nsh /u01/app/oracle/inventory/orainstRoot.sh\nsh /db/app/oracle/product/11.2.0/root.sh</code></pre></div><h2>11配置静默监听</h2><div class=\"highlight\"><pre><code class=\"language-text\">su oracle\nnetca /silent /responsefile /db/etc/netca.rsp</code></pre></div><p>查看监听端口</p><div class=\"highlight\"><pre><code class=\"language-text\">netstat -tnulp | grep 1521</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-d7bab2405ce5665c3451f2331cdc216e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"830\" data-rawheight=\"66\" class=\"origin_image zh-lightbox-thumb\" width=\"830\" data-original=\"https://pic3.zhimg.com/v2-d7bab2405ce5665c3451f2331cdc216e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;830&#39; height=&#39;66&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"830\" data-rawheight=\"66\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"830\" data-original=\"https://pic3.zhimg.com/v2-d7bab2405ce5665c3451f2331cdc216e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-d7bab2405ce5665c3451f2331cdc216e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>11、静默创建数据库</h2><div class=\"highlight\"><pre><code class=\"language-text\">vim /db/etc/dbca.rsp\nGDBNAME = &#34;orcl&#34;\nSID = &#34;orcl&#34;\nSYSPASSWORD = &#34;oracle&#34;\nSYSTEMPASSWORD = &#34;oracle&#34;\nSYSMANPASSWORD = &#34;oracle&#34;\nDBSNMPPASSWORD = &#34;oracle&#34;\nDATAFILEDESTINATION =/db/app/oracle/oradata\nRECOVERYAREADESTINATION=/db/app/oracle/fast_recovery_area\nCHARACTERSET = &#34;AL32UTF8&#34;\nTOTALMEMORY = &#34;3277&#34;</code></pre></div><p>其中TOTALMEMORY 设置为总内存的80%（4<i>1024</i>0.8） 在root用户下执行(如果没有权限)</p><div class=\"highlight\"><pre><code class=\"language-text\">chown -R oracle:oinstall /db/etc/dbca.rsp</code></pre></div><p>执行静默建库</p><div class=\"highlight\"><pre><code class=\"language-bash\">dbca -silent -responseFile /db/etc/dbca.rsp</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-473bc6a484c3fb303a7a9fafb7571d21_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"910\" data-rawheight=\"365\" class=\"origin_image zh-lightbox-thumb\" width=\"910\" data-original=\"https://pic2.zhimg.com/v2-473bc6a484c3fb303a7a9fafb7571d21_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;910&#39; height=&#39;365&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"910\" data-rawheight=\"365\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"910\" data-original=\"https://pic2.zhimg.com/v2-473bc6a484c3fb303a7a9fafb7571d21_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-473bc6a484c3fb303a7a9fafb7571d21_b.jpg\"/></figure><p> 然后查看一下日志看看有没有报错</p><div class=\"highlight\"><pre><code class=\"language-bash\">vim /db/app/oracle/cfgtoollogs/dbca/orcl/orcl.log</code></pre></div><p>如下</p><div class=\"highlight\"><pre><code class=\"language-text\">Copying database files\nDBCA_PROGRESS : 1%\nDBCA_PROGRESS : 3%\nDBCA_PROGRESS : 11%\nDBCA_PROGRESS : 18%\nDBCA_PROGRESS : 26%\nDBCA_PROGRESS : 37%\nCreating and starting Oracle instance\nDBCA_PROGRESS : 40%\nDBCA_PROGRESS : 45%\nDBCA_PROGRESS : 50%\nDBCA_PROGRESS : 55%\nDBCA_PROGRESS : 56%\nDBCA_PROGRESS : 60%\nDBCA_PROGRESS : 62%\nCompleting Database Creation\nDBCA_PROGRESS : 66%\nDBCA_PROGRESS : 70%\nDBCA_PROGRESS : 73%\nDBCA_PROGRESS : 85%\nDBCA_PROGRESS : 96%\nDBCA_PROGRESS : 100%\nDatabase creation complete. For details check the logfiles at:\n /db/app/oracle/cfgtoollogs/dbca/orcl.\nDatabase Information:\nGlobal Database Name:orcl\nSystem Identifier(SID):orcl</code></pre></div><p>查看oracle实例进程</p><div class=\"highlight\"><pre><code class=\"language-text\">ps -ef | grep ora_ | grep -v grep\nroot@ambari:~# ps -ef | grep ora_ | grep -v grep\noracle     3531      1  0 05:48 ?        00:00:00 ora_pmon_orcl\noracle     3533      1 11 05:48 ?        00:00:12 ora_vktm_orcl\noracle     3537      1  0 05:48 ?        00:00:00 ora_gen0_orcl\noracle     3539      1  0 05:48 ?        00:00:00 ora_diag_orcl\noracle     3541      1  0 05:48 ?        00:00:00 ora_dbrm_orcl\noracle     3543      1  0 05:48 ?        00:00:00 ora_psp0_orcl\noracle     3545      1  0 05:48 ?        00:00:00 ora_dia0_orcl\noracle     3547      1 16 05:48 ?        00:00:17 ora_mman_orcl\noracle     3549      1  0 05:48 ?        00:00:00 ora_dbw0_orcl\noracle     3551      1  0 05:48 ?        00:00:00 ora_lgwr_orcl\noracle     3553      1  0 05:48 ?        00:00:00 ora_ckpt_orcl\noracle     3555      1  0 05:48 ?        00:00:00 ora_smon_orcl\noracle     3557      1  0 05:48 ?        00:00:00 ora_reco_orcl\noracle     3559      1  1 05:48 ?        00:00:01 ora_mmon_orcl\noracle     3561      1  0 05:48 ?        00:00:00 ora_mmnl_orcl\noracle     3563      1  0 05:48 ?        00:00:00 ora_d000_orcl\noracle     3565      1  0 05:48 ?        00:00:00 ora_s000_orcl\noracle     3615      1  0 05:48 ?        00:00:00 ora_qmnc_orcl\noracle     4088      1  1 05:48 ?        00:00:00 ora_cjq0_orcl\noracle     4121      1  0 05:48 ?        00:00:00 ora_q000_orcl\noracle     4134      1  0 05:48 ?        00:00:00 ora_q001_orcl</code></pre></div><p>查看监听状态</p><div class=\"highlight\"><pre><code class=\"language-bash\">lsnrctl status</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-c5c88ca76ae04221477d03796abafd59_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"721\" data-rawheight=\"409\" class=\"origin_image zh-lightbox-thumb\" width=\"721\" data-original=\"https://pic2.zhimg.com/v2-c5c88ca76ae04221477d03796abafd59_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;721&#39; height=&#39;409&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"721\" data-rawheight=\"409\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"721\" data-original=\"https://pic2.zhimg.com/v2-c5c88ca76ae04221477d03796abafd59_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-c5c88ca76ae04221477d03796abafd59_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>12、登录到oracle，测试</h2><div class=\"highlight\"><pre><code class=\"language-bash\">sqlplus / as sysdba\n<span class=\"k\">select</span> status from v<span class=\"nv\">$instance</span><span class=\"p\">;</span></code></pre></div><p>这是发现oracle执行任何语句报错如图： </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-03c014b82e1a345299341eed24dd730a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"911\" data-rawheight=\"275\" class=\"origin_image zh-lightbox-thumb\" width=\"911\" data-original=\"https://pic3.zhimg.com/v2-03c014b82e1a345299341eed24dd730a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;911&#39; height=&#39;275&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"911\" data-rawheight=\"275\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"911\" data-original=\"https://pic3.zhimg.com/v2-03c014b82e1a345299341eed24dd730a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-03c014b82e1a345299341eed24dd730a_b.jpg\"/></figure><p> 崩溃~</p><h2>13、各种错误及解决</h2><h2>13.1 首先检查前面的步骤有没有错的</h2><p>如果没有，则执行后面，一开始我发现前面日志异常，第一次装没有经验，试了几下干脆卸载重装。</p><h2>13.2 ORACLE not available</h2><p>先根据ORACLE not available上网查了一下，解决方法：startup</p><h2>13.3 startup 报错</h2><p>错误：</p><div class=\"highlight\"><pre><code class=\"language-text\">could not open parameter file &#39;/db/app/oracle/product/11.2.0/dbs/initORCL.ora&#39;</code></pre></div><h2>13.4解决could not open parameter</h2><p>执行以下命令即可（确保oracle用户对下面的文件夹有权限，前面已经执行过）</p><div class=\"highlight\"><pre><code class=\"language-text\">cp $ORACLE_BASE/admin/orcl/pfile/init.ora.43201822553 $ORACLE_HOME/dbs/initORCL.ora</code></pre></div><p>参考：<a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/guchuanlong/article/details/7299079\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Linux下无法启动oracle could not open parameter file 解决方法</a> 继续startup,又报错：MEMORY_TARGET not supported on this system</p><div class=\"highlight\"><pre><code class=\"language-text\">SQL&gt; startup\nORA-00845: MEMORY_TARGET not supported on this system</code></pre></div><h2>13.5 解决 MEMORY_TARGET not supported on this system</h2><p>root 用户下执行</p><div class=\"highlight\"><pre><code class=\"language-text\">mount -t tmpfs shmfs -o size=7g /dev/shm</code></pre></div><p>参考：<a href=\"https://link.zhihu.com/?target=https%3A//www.linuxidc.com/Linux/2012-12/76976.htm\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">ORA-00845: MEMORY_TARGET not supported on this system报错解决</a> 继续startup</p><div class=\"highlight\"><pre><code class=\"language-text\">SQL&gt; startup\nORACLE instance started.\n\nTotal System Global Area 1720328192 bytes\nFixed Size          2214056 bytes\nVariable Size        1006634840 bytes\nDatabase Buffers      704643072 bytes\nRedo Buffers            6836224 bytes\nORA-01102: cannot mount database in EXCLUSIVE mode</code></pre></div><p>如果执行查询会报错：database not mounted，因为上面已经报错ORA-01102: cannot mount database in EXCLUSIVE mode</p><h2>13.6 解决 cannot mount database in EXCLUSIVE mode</h2><p>先关闭数据库</p><div class=\"highlight\"><pre><code class=\"language-text\">shutdown immediate\nSQL&gt; shutdown immediate\nORA-01507: database not mounted\n\n\nORACLE instance shut down.</code></pre></div><p>然后在root用户执行</p><div class=\"highlight\"><pre><code class=\"language-text\">cd $ORACLE_HOME/dbs\nfuser -k lkORCL\n/db/app/oracle/product/11.2.0/dbs/lkORCL:  2933  2939  2943  2945  2949  2951  2953  2955  2957  2959  2961  2963  3138  3140  3142  3144  3341  3343  3345  3869  3961</code></pre></div><p>其中lkORCL 为自己设置oracle实例名的大写。 再执行fuser -u lkORCL没有任何输出即可 参考：<a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/qq_27966627/article/details/51062822\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">ORA-01507: database not mounted （转）</a></p><p>这时再执行startup就可以了</p><div class=\"highlight\"><pre><code class=\"language-text\">SQL&gt; startup\nORACLE instance started.\n\nTotal System Global Area 1720328192 bytes\nFixed Size          2214056 bytes\nVariable Size        1006634840 bytes\nDatabase Buffers      704643072 bytes\nRedo Buffers            6836224 bytes\nDatabase mounted.\nDatabase opened.\nSQL&gt; select * from v$version;\n\nBANNER\n--------------------------------------------------------------------------------\nOracle Database 11g Enterprise Edition Release 11.2.0.1.0 - 64bit Production\nPL/SQL Release 11.2.0.1.0 - Production\nCORE    11.2.0.1.0  Production\nTNS for Linux: Version 11.2.0.1.0 - Production\nNLSRTL Version 11.2.0.1.0 - Production\n\nSQL&gt;</code></pre></div><p>再执行其他查询语句测试一下即可</p><h2>14、创建用户供远程连接</h2><p>开放1521端口</p><div class=\"highlight\"><pre><code class=\"language-bash\">firewall-cmd --zone<span class=\"o\">=</span>public --add-port<span class=\"o\">=</span><span class=\"m\">1521</span>/tcp --permanent\nfirewall-cmd --reload\ncreate user bigdata identified by bigdata<span class=\"p\">;</span>\ngrant connect, resource to bigdata<span class=\"p\">;</span></code></pre></div><p>利用连接数据库的工具就可以远程连接oracle，如DBeaver,然后建表，插入几条记录，查询测试一下，具体方法不再赘述。</p><h2>参考资料</h2><p><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/Kenny1993/article/details/75038670\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">CentOS 7静默（无图形化界面）安装Oracle 11g</a></p>", 
            "topic": [
                {
                    "tag": "CentOS", 
                    "tagLink": "https://api.zhihu.com/topics/19577255"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/42833240", 
            "userName": "董可伦", 
            "userLink": "https://www.zhihu.com/people/de888cc734ddbfd918269a8e5fb196ba", 
            "upvote": 1, 
            "title": "Redis Cluster 安装配置", 
            "content": "<p>我的原创地址：</p><a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/01/09/redisClusterDeployment/\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-0b4e5faa279a5e545d79e754b3dba862_ipico.jpg\" data-image-width=\"640\" data-image-height=\"640\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Redis Cluster 安装配置</a><h2>服务器</h2><p>CentOS Centos 服务器初始环境配置最好先配置好，服务器时间最好配置为一致 我用的是6个服务器，一个服务器一个端口，便于配置文件的修改</p><h2>首先下载redis3 到本地(需要3以后的版本，我下载的最新的版本:3.0.4)</h2><div class=\"highlight\"><pre><code class=\"language-text\">wget http://download.redis.io/releases/redis-3.0.4.tar.gz</code></pre></div><h2>分别上传到每个服务器</h2><div class=\"highlight\"><pre><code class=\"language-text\">scp -r redis-3.0.4.tar.gz redis@redis1:~/</code></pre></div><h2>分别在每个服务器上安装 gcc tcl ruby rubygems gem_redis</h2><div class=\"highlight\"><pre><code class=\"language-text\">sudo yum -y install gcc\nsudo yum -y install tcl\nsudo yum -y install ruby rubygems\nsudo gem install redis --version 3.0.4</code></pre></div><p>(gem_redis如果装不上，可多试几次，若是ubuntu，换源，换成taobao,建议服务器用centOS)</p><h2>解压安装redis</h2><div class=\"highlight\"><pre><code class=\"language-text\">tar -zxvf redis-3.0.4.tar.gz\ncd redis-3.0.4/\nsudo make test\nsudo make install</code></pre></div><h2>修改配置文件redis.conf 以端口9000为例</h2><div class=\"highlight\"><pre><code class=\"language-text\">sudo vim redis.conf\n\nport 9000\npidfile /var/run/redis-9000.pid\ndbfilename dump-9000.rdb\nappendfilename &#34;appendonly-9000.aof&#34;\ncluster-config-file nodes-9000.conf\ncluster-enabled yes\ncluster-node-timeout 5000\nappendonly yes</code></pre></div><h2>然后分别在每个服务器上启动redis</h2><div class=\"highlight\"><pre><code class=\"language-text\">src/redis-server redis.conf</code></pre></div><h2>查看每个端口是否成功开启</h2><div class=\"highlight\"><pre><code class=\"language-text\">ps -aux | grep redis</code></pre></div><p>结果如下：</p><div class=\"highlight\"><pre><code class=\"language-text\">redis 26674 1 0 Sep15 ? 00:01:21 src/redis-server *:9000 [cluster]</code></pre></div><p>如果后面有[cluster] 证明开启成功</p><h2>然后建立集群</h2><div class=\"highlight\"><pre><code class=\"language-text\">src/redis-trib.rb create --replicas 1 192.168.32.195:9000 192.168.32.196:9000\n192.168.32.197:9000 192.168.32.198:9000 192.168.32.199:9000 192.168.6.214:9000</code></pre></div><p>(master 至少有三个 1 代表 一个master 对应一个slave)</p><h2>如果下面这样就代表成功(端口 9000 )</h2><div class=\"highlight\"><pre><code class=\"language-text\">&gt;&gt;&gt; Creating cluster\nConnecting to node 192.168.32.195:9000: OK\nConnecting to node 192.168.32.196:9000: OK\nConnecting to node 192.168.32.197:9000: OK\nConnecting to node 192.168.32.198:9000: OK\nConnecting to node 192.168.32.199:9000: OK\nConnecting to node 192.168.6.214:9000: OK\n&gt;&gt;&gt; Performing hash slots allocation on 6 nodes...\nUsing 3 masters:\n192.168.32.199:9000\n192.168.32.198:9000\n192.168.32.197:9000\nAdding replica 192.168.32.196:9000 to 192.168.32.199:9000\nAdding replica 192.168.32.195:9000 to 192.168.32.198:9000\nAdding replica 192.168.6.214:9000 to 192.168.32.197:9000\nS: f4c625f49b6c1b073a379c41073b9ff9786f45f6 192.168.32.195:9000\nreplicates 20ecac926dba10a056f4d36a72bed35bbb7fcd57\nS: 77e68bf6eb86d8bf6cb993b32d726e2cd4d05a54 192.168.32.196:9000\nreplicates 3276d09525fff0195dd366c8d05f80773d7d9af3\nM: c6f4504b2ceb8bcffb682483168b8016e6f5973e 192.168.32.197:9000\nslots:10923-16383 (5461 slots) master\nM: 20ecac926dba10a056f4d36a72bed35bbb7fcd57 192.168.32.198:9000\nslots:5461-10922 (5462 slots) master\nM: 3276d09525fff0195dd366c8d05f80773d7d9af3 192.168.32.199:9000\nslots:0-5460 (5461 slots) master\nS: 627c0ae61d75570dc15a03cbdb03b1a8abd56f58 192.168.6.214:9000\nreplicates c6f4504b2ceb8bcffb682483168b8016e6f5973e\nCan I set the above configuration? (type &#39;yes&#39; to accept): yes\n&gt;&gt;&gt; Nodes configuration updated\n&gt;&gt;&gt; Assign a different config epoch to each node\n&gt;&gt;&gt; Sending CLUSTER MEET messages to join the cluster\nWaiting for the cluster to join...\n&gt;&gt;&gt; Performing Cluster Check (using node 192.168.32.195:9000)\nM: f4c625f49b6c1b073a379c41073b9ff9786f45f6 192.168.32.195:9000\nslots: (0 slots) master\nreplicates 20ecac926dba10a056f4d36a72bed35bbb7fcd57\nM: 77e68bf6eb86d8bf6cb993b32d726e2cd4d05a54 192.168.32.196:9000\nslots: (0 slots) master\nreplicates 3276d09525fff0195dd366c8d05f80773d7d9af3\nM: c6f4504b2ceb8bcffb682483168b8016e6f5973e 192.168.32.197:9000\nslots:10923-16383 (5461 slots) master\nM: 20ecac926dba10a056f4d36a72bed35bbb7fcd57 192.168.32.198:9000\nslots:5461-10922 (5462 slots) master\nM: 3276d09525fff0195dd366c8d05f80773d7d9af3 192.168.32.199:9000\nslots:0-5460 (5461 slots) master\nM: 627c0ae61d75570dc15a03cbdb03b1a8abd56f58 192.168.6.214:9000\nslots: (0 slots) master\nreplicates c6f4504b2ceb8bcffb682483168b8016e6f5973e\n[OK] All nodes agree about slots configuration.\n&gt;&gt;&gt; Check for open slots...\n&gt;&gt;&gt; Check slots coverage...\n[OK] All 16384 slots covered.</code></pre></div><h2>以集群的方式登陆(9000为例）</h2><div class=\"highlight\"><pre><code class=\"language-text\">redis-cli -c -p 9000</code></pre></div><h2>查看 主从关系</h2><div class=\"highlight\"><pre><code class=\"language-text\">$ 127.0.0.1:9000&gt; cluster nodes\nbaa1779a73a8d329dfc96104ac3116a24b16a681 192.168.32.199:9000 myself,master - 0 0\n5 connected 0-5460\n140a8c76dfd44a6e43045d43a4d853fa330dfd42 192.168.32.198:9000 master - 0\n1442403482170 12 connected 5461-10922\nc99974ae9a92ed18166d580e4d7bb438a26ad3a2 192.168.32.195:9000 slave\n140a8c76dfd44a6e43045d43a4d853fa330dfd42 0 1442403481669 12 connected\ne07467f74f81f31c024bb5626c37e550396eebe5 192.168.6.214:9000 master - 0\n1442403481168 9 connected 10923-16383\n04f3bcd569fb325e77a109a530fd376eb84512ce 192.168.32.196:9000 slave\nbaa1779a73a8d329dfc96104ac3116a24b16a681 0 1442403480667 5 connected\nfd383105b93797d7502a7d6bcaa77308cccf2e71 192.168.32.197:9000 slave\ne07467f74f81f31c024bb5626c37e550396eebe5 0 1442403482671 9 connected\n$127.0.0.1:9000&gt;</code></pre></div><p></p>", 
            "topic": [
                {
                    "tag": "分布式系统", 
                    "tagLink": "https://api.zhihu.com/topics/19570823"
                }, 
                {
                    "tag": "Redis", 
                    "tagLink": "https://api.zhihu.com/topics/19557280"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/42832737", 
            "userName": "董可伦", 
            "userLink": "https://www.zhihu.com/people/de888cc734ddbfd918269a8e5fb196ba", 
            "upvote": 0, 
            "title": "network is unreachable", 
            "content": "<p>我的原创地址：</p><a href=\"https://link.zhihu.com/?target=http%3A//dongkelun.com/2018/01/17/networkIsUnreachable/\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-0b4e5faa279a5e545d79e754b3dba862_ipico.jpg\" data-image-width=\"640\" data-image-height=\"640\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">network is unreachable centos无法连接外网（或unknown host baidu.com）</a><h2>前言</h2><p>在虚拟机上新装的系统设置固定ip、重启系统之后，可能ping不通外网，出现如标题所示错误。</p><h2>1、执行以下命令即可（临时添加网关）</h2><div class=\"highlight\"><pre><code class=\"language-bash\">sudo route add default gw <span class=\"m\">192</span>.168.44.2</code></pre></div><h2>2、永久性修改网关</h2><h2>2.1 修改配置文件</h2><div class=\"highlight\"><pre><code class=\"language-bash\">vim /etc/sysconfig/network</code></pre></div><p>在最下面添加如下内容：</p><div class=\"highlight\"><pre><code class=\"language-text\">GATEWAY=192.168.44.2</code></pre></div><h2>2.2 重启网卡</h2><div class=\"highlight\"><pre><code class=\"language-bash\">service network restart</code></pre></div><p></p>", 
            "topic": [
                {
                    "tag": "CentOS", 
                    "tagLink": "https://api.zhihu.com/topics/19577255"
                }, 
                {
                    "tag": "VMware（威睿）", 
                    "tagLink": "https://api.zhihu.com/topics/19575633"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/41785450", 
            "userName": "董可伦", 
            "userLink": "https://www.zhihu.com/people/de888cc734ddbfd918269a8e5fb196ba", 
            "upvote": 2, 
            "title": "Centos7 Ambari+HDP 大数据集群安装部署", 
            "content": "<p></p><p>我的原创地址：<u><a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/04/25/ambariConf/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">dongkelun.com/2018/04/2</span><span class=\"invisible\">5/ambariConf/</span><span class=\"ellipsis\"></span></a></u></p><h2>前言</h2><p>本文是讲如何在centos7（64位） 安装ambari+hdp,如果在装有原生hadoop等集群的机器上安装，需要先将集群服务停掉，然后将不需要的环境变量注释掉即可，如果不注释掉，后面虽然可以安装成功，但是在启动某些服务的时候可能会有异常，比如最后提到的hive启动异常。本文适合系统： RedHat7、CentOS7、Oracle Linux7(都是64位) 注意:centos7中文系统有bug（python脚本中文识别问题）,需要使用英文系统。 本文仅作参考（基本每个配置博客都有局限性和坑~），推荐先参考官方文档： <a href=\"https://link.zhihu.com/?target=https%3A//docs.hortonworks.com/HDPDocuments/Ambari-2.6.1.5/bk_ambari-installation/content/ch_Getting_Ready.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">docs.hortonworks.com/HD</span><span class=\"invisible\">PDocuments/Ambari-2.6.1.5/bk_ambari-installation/content/ch_Getting_Ready.html</span><span class=\"ellipsis\"></span></a> 以下均在root用户下执行。</p><h2>1、满足最低系统要求</h2><h2>1.1 浏览器</h2><p>建议您将浏览器(自己使用的windows既可)更新至最新的稳定版本</p><h2>1.2 软件要求（在每台主机上）</h2><div class=\"highlight\"><pre><code class=\"language-bash\"><span class=\"m\">1</span>.2.1 yum和rpm\n<span class=\"m\">1</span>.2.2 scp, curl, unzip, tar、 wget\n<span class=\"m\">1</span>.2.3 OpenSSL（v1.01，build 16或更高版本）\n<span class=\"m\">1</span>.2.4 python：2.7<span class=\"o\">(</span>注意如果有使用python3.x的需求，不要改变python环境变量，否则3.x会报错<span class=\"o\">)</span>\n<span class=\"m\">1</span>.2.5 jdk：1.8\n<span class=\"m\">1</span>.2.6 mysql：5.6（官网上写的5.6，不确定更高版本有没有问题，也可以使用其他数据库，根据自己习惯）\n<span class=\"m\">1</span>.2.7 内存要求：Ambari主机应该至少有1 GB RAM，500 MB空闲，（但如果使用的话，建议内存8g以上，我自己的虚拟机内存4g搭好后跑起来会很卡，配置低的话警告也会很多）\n<span class=\"m\">1</span>.2.8 检查最大打开文件描述符,推荐的最大打开文件描述符数为10000或更多\n<span class=\"m\">1</span>.2.9 mysql-connector-java</code></pre></div><p>以上软件大部分系统自带，其余可参考：<a href=\"https://link.zhihu.com/?target=http%3A//dongkelun.com/2018/04/05/centosInitialConf/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">CentOS 初始环境配置</a></p><h2>2、环境准备（在每台主机上）</h2><h2>2.1 ssh 免密</h2><p>只需master 免密到其他节点（包含自身），不需要互通，参考：<a href=\"https://link.zhihu.com/?target=http%3A//dongkelun.com/2018/04/05/sshConf/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">linux ssh 免密登录</a></p><h2>2.2 启用NTP</h2><div class=\"highlight\"><pre><code class=\"language-bash\">yum install -y ntp\nsystemctl <span class=\"nb\">enable</span> ntpd</code></pre></div><h2>2.3 编辑主机文件</h2><div class=\"highlight\"><pre><code class=\"language-bash\">vim /etc/hosts</code></pre></div><p>本文只是在个人虚拟机上进行安装测试，所以只选择两个节点，在公司真实环境下多个节点安装是一样的，ambari对内存要求较高，如果个人电脑配置不高的话，建议学习一下即可。</p><div class=\"highlight\"><pre><code class=\"language-bash\"><span class=\"m\">192</span>.168.44.138 ambari.master.com\n<span class=\"m\">192</span>.168.44.139 ambari.slave1.com</code></pre></div><p>其中后面的如<a href=\"https://link.zhihu.com/?target=http%3A//ambari.master.com\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">ambari.master.com</span><span class=\"invisible\"></span></a>为完全限定域名（FQDN)（通过符号“.”）,不能简单的设为master等，如果该文件里有其他映射，如上面的配置必须要在最前面（自带的localhost下面一行），否则后面安装会报错。</p><h2>2.4 设置主机名</h2><p>以<a href=\"https://link.zhihu.com/?target=http%3A//ambari.master.com\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">ambari.master.com</span><span class=\"invisible\"></span></a>为例 2.4.1</p><div class=\"highlight\"><pre><code class=\"language-bash\">hostname ambari.master.com</code></pre></div><p>2.4.2</p><div class=\"highlight\"><pre><code class=\"language-bash\">vim /etc/hostname\nambari.master.com</code></pre></div><p>两步缺一不可，通过命令验证</p><div class=\"highlight\"><pre><code class=\"language-bash\">hostname\nhostname -f</code></pre></div><p>两个必须都为<a href=\"https://link.zhihu.com/?target=http%3A//ambari.master.com\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">ambari.master.com</span><span class=\"invisible\"></span></a>才行</p><h2>2.5 编辑网络配置文件</h2><div class=\"highlight\"><pre><code class=\"language-bash\">vim /etc/sysconfig/network</code></pre></div><p>修改HOSTNAME属性为FQDN</p><div class=\"highlight\"><pre><code class=\"language-text\">NETWORKING=yes\nHOSTNAME=ambari.master.com</code></pre></div><h2>2.6 禁用iptables</h2><div class=\"highlight\"><pre><code class=\"language-bash\">systemctl disable firewalld\nservice firewalld stop</code></pre></div><h2>2.7 禁用SELinux</h2><p>2.7.1 临时禁用</p><div class=\"highlight\"><pre><code class=\"language-bash\">setenforce <span class=\"m\">0</span></code></pre></div><p>2.7.2 永久禁用(重启机器)</p><div class=\"highlight\"><pre><code class=\"language-bash\">vim /etc/sysconfig/selinux</code></pre></div><p>将SELINUX改为disabled</p><div class=\"highlight\"><pre><code class=\"language-text\">SELINUX=disabled</code></pre></div><p>这样服务器或虚拟机重启也没有问题。</p><h2>3、制作本地源（仅在master）</h2><p>因为ambari 和 hdp 安装文件比较大，如果在线安装的话会很慢，所以最好选择本地源。 （可以在集群可以访问的任何机器上制作本地源）</p><h2>3.1 安装制作本地源工具</h2><div class=\"highlight\"><pre><code class=\"language-bash\">yum install yum-utils createrepo</code></pre></div><h2>3.2 创建一个HTTP服务器</h2><div class=\"highlight\"><pre><code class=\"language-text\">yum install httpd -y\nsystemctl enable httpd &amp;&amp; systemctl start httpd</code></pre></div><h2>3.3 为Web服务器创建目录</h2><div class=\"highlight\"><pre><code class=\"language-text\">mkdir -p /var/www/html/hdp/HDP-UTILS</code></pre></div><h2>3.4 下载系统对应的最新版相关安装包</h2><p>其中包括Ambari、HDP、HDP-UTILS,由于HDP-GPL较小只有几百k，所以没有配置为本地源。</p><h2>3.4.1 下载</h2><div class=\"highlight\"><pre><code class=\"language-bash\">wget http://public-repo-1.hortonworks.com/ambari/centos7/2.x/updates/2.6.1.5/ambari-2.6.1.5-centos7.tar.gz\nwget http://public-repo-1.hortonworks.com/HDP/centos7/2.x/updates/2.6.4.0/HDP-2.6.4.0-centos7-rpm.tar.gz\nwget http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.22/repos/centos7/HDP-UTILS-1.1.0.22-centos7.tar.gz</code></pre></div><p>下载地址见官方文档：<a href=\"https://link.zhihu.com/?target=https%3A//docs.hortonworks.com/HDPDocuments/Ambari-2.6.1.5/bk_ambari-installation/content/ch_obtaining-public-repos.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">docs.hortonworks.com/HD</span><span class=\"invisible\">PDocuments/Ambari-2.6.1.5/bk_ambari-installation/content/ch_obtaining-public-repos.html</span><span class=\"ellipsis\"></span></a></p><h2>3.4.2 解压</h2><div class=\"highlight\"><pre><code class=\"language-text\">tar -zxvf ambari-2.6.1.5-centos7.tar.gz -C /var/www/html\ntar -zxvf HDP-2.6.4.0-centos7-rpm.tar.gz -C /var/www/html/hdp/\ntar -zxvf HDP-UTILS-1.1.0.22-centos7.tar.gz  -C /var/www/html/hdp/HDP-UTILS/</code></pre></div><h2>3.4.3 解决在浏览器访问<a href=\"https://link.zhihu.com/?target=http%3A//ambari.master.com/hdp/HDP/centos7/2.6.4.0-91\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">ambari.master.com/hdp/H</span><span class=\"invisible\">DP/centos7/2.6.4.0-91</span><span class=\"ellipsis\"></span></a> 为空白</h2><p>原因：该目录下index.xml使用了 <a href=\"https://link.zhihu.com/?target=https%3A//ajax.googleapis.com/ajax/libs/jquery/3.1.1/jquery.min.js\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">ajax.googleapis.com/aja</span><span class=\"invisible\">x/libs/jquery/3.1.1/jquery.min.js</span><span class=\"ellipsis\"></span></a>  国内访问不了谷歌，将index.xml注释掉即可</p><div class=\"highlight\"><pre><code class=\"language-text\">cd /var/www/html/hdp/HDP/centos7/2.6.4.0-91\nmv index.xml index.xml.bak</code></pre></div><p>此时应该可以在浏览器访问下面的地址了,可以验证一下</p><div class=\"highlight\"><pre><code class=\"language-text\">http://ambari.master.com/ambari/centos7/2.6.1.5-3/\nhttp://ambari.master.com/hdp/HDP/centos7/2.6.4.0-91\nhttp://ambari.master.com/hdp/HDP-UTILS</code></pre></div><h2>3.5 配置ambari、HDP、HDP-UTILS的本地源</h2><div class=\"highlight\"><pre><code class=\"language-bash\">cp /var/www/html/ambari/centos7/2.6.1.5-3/ambari.repo /etc/yum.repos.d/\ncp /var/www/html/hdp/HDP/centos7/2.6.4.0-91/hdp.repo /etc/yum.repos.d/</code></pre></div><p>将每个repo里的baseurl和gpgkey的地址修改为本地的</p><div class=\"highlight\"><pre><code class=\"language-text\">vim /etc/yum.repos.d/ambari.repo\n#VERSION_NUMBER=2.6.1.5-3\n[ambari-2.6.1.5]\nname=ambari Version - ambari-2.6.1.5\nbaseurl=http://ambari.master.com/ambari/centos7/2.6.1.5-3\ngpgcheck=1\ngpgkey=http://ambari.master.com/ambari/centos7/2.6.1.5-3/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins\nenabled=1\npriority=1\nvim /etc/yum.repos.d/hdp.repo\n#VERSION_NUMBER=2.6.4.0-91\n[HDP-2.6.4.0]\nname=HDP Version - HDP-2.6.4.0\nbaseurl=http://ambari.master.com/hdp/HDP/centos7/2.6.4.0-91\ngpgcheck=1\ngpgkey=http://ambari.master.com/hdp/HDP/centos7/2.6.4.0-91/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins\nenabled=1\npriority=1\n\n\n[HDP-UTILS-1.1.0.22]\nname=HDP-UTILS Version - HDP-UTILS-1.1.0.22\nbaseurl=http://ambari.master.com/hdp/HDP-UTILS\ngpgcheck=1\ngpgkey=http://ambari.master.com/hdp/HDP/centos7/2.6.4.0-91/RPM-GPG-KEY/RPM-GPG-KEY-Jenkins\nenabled=1\npriority=1\nyum clean all\nyum list update\nyum makecache\nyum repolist</code></pre></div><h2>3.6 （可选）如果您的环境中配置了多个存储库，请在集群中的所有节点上部署以下插件</h2><div class=\"highlight\"><pre><code class=\"language-bash\">yum install yum-plugin-priorities -y\nvim /etc/yum/pluginconf.d/priorities.conf\n<span class=\"o\">[</span>main<span class=\"o\">]</span>\n<span class=\"nv\">enabled</span> <span class=\"o\">=</span> <span class=\"m\">1</span>\n<span class=\"nv\">gpgcheck</span><span class=\"o\">=</span><span class=\"m\">0</span></code></pre></div><h2>4、安装ambari（仅在master）</h2><h2>4.1安装ambari-server</h2><div class=\"highlight\"><pre><code class=\"language-bash\">yum install ambari-server -y</code></pre></div><h2>4.2 设置mysql连接器</h2><div class=\"highlight\"><pre><code class=\"language-bash\">ambari-server setup --jdbc-db<span class=\"o\">=</span>mysql --jdbc-driver<span class=\"o\">=</span>/usr/share/java/mysql-connector-java.jar</code></pre></div><p>（如果使用mysql作为hive的元数据库）</p><h2>4.3 创建相关的mysql数据库</h2><p>创建ambari数据库及用户，登录root用户执行下面语句：</p><div class=\"highlight\"><pre><code class=\"language-bash\">mysql -uroot -pRoot-123\ncreate database ambari character <span class=\"nb\">set</span> utf8 <span class=\"p\">;</span>  \nCREATE USER <span class=\"s1\">&#39;ambari&#39;</span>@<span class=\"s1\">&#39;%&#39;</span>IDENTIFIED BY <span class=\"s1\">&#39;Ambari-123&#39;</span><span class=\"p\">;</span>\nGRANT ALL PRIVILEGES ON *.* TO <span class=\"s1\">&#39;ambari&#39;</span>@<span class=\"s1\">&#39;%&#39;</span><span class=\"p\">;</span>\nFLUSH PRIVILEGES<span class=\"p\">;</span></code></pre></div><p>如果要安装Hive,再创建Hive数据库和用户,再执行下面的语句：</p><div class=\"highlight\"><pre><code class=\"language-text\">create database hive character set utf8 ;  \nCREATE USER &#39;hive&#39;@&#39;%&#39;IDENTIFIED BY &#39;Hive-123&#39;;\nGRANT ALL PRIVILEGES ON *.* TO &#39;hive&#39;@&#39;%&#39;;\nFLUSH PRIVILEGES;</code></pre></div><p>hive用户可以不用指定全部库的权限。</p><h2>4.4 配置ambari-server</h2><h2>4.4.1 setup</h2><div class=\"highlight\"><pre><code class=\"language-bash\">ambari-server setup</code></pre></div><h2>4.4.2 配置流程</h2><p>以下为全部的配置过程，其中主要是自定义jdk,输入JAVA_HOME路径，自定义数据库选mysql，输入数据库用户名，密码等</p><div class=\"highlight\"><pre><code class=\"language-text\">ambari-server setup\nUsing python  /usr/bin/python2\nSetup ambari-server\nChecking SELinux...\nSELinux status is &#39;enabled&#39;\nSELinux mode is &#39;permissive&#39;\nWARNING: SELinux is set to &#39;permissive&#39; mode and temporarily disabled.\nOK to continue [y/n] (y)? y\nCustomize user account for ambari-server daemon [y/n] (n)? y\nEnter user account for ambari-server daemon (root):ambari\nAdjusting ambari-server permissions and ownership...\nChecking firewall status...\nChecking JDK...\n[1] Oracle JDK 1.8 + Java Cryptography Extension (JCE) Policy Files 8\n[2] Oracle JDK 1.7 + Java Cryptography Extension (JCE) Policy Files 7\n[3] Custom JDK\n==============================================================================\nEnter choice (1): 3\nWARNING: JDK must be installed on all hosts and JAVA_HOME must be valid on all hosts.\nWARNING: JCE Policy files are required for configuring Kerberos security. If you plan to use Kerberos,please make sure JCE Unlimited Strength Jurisdiction Policy Files are valid on all hosts.\nPath to JAVA_HOME: /opt/jdk1.8.0_151\nValidating JDK on Ambari Server...done.\nChecking GPL software agreement...\nGPL License for LZO: https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.html\nEnable Ambari Server to download and install GPL Licensed LZO packages [y/n] (n)? y\nCompleting setup...\nConfiguring database...\nEnter advanced database configuration [y/n] (n)? y\nConfiguring database...\n==============================================================================\nChoose one of the following options:\n[1] - PostgreSQL (Embedded)\n[2] - Oracle\n[3] - MySQL / MariaDB\n[4] - PostgreSQL\n[5] - Microsoft SQL Server (Tech Preview)\n[6] - SQL Anywhere\n[7] - BDB\n==============================================================================\nEnter choice (1): 3\nHostname (localhost): \nPort (3306): \nDatabase name (ambari): \nUsername (ambari): \nEnter Database Password (bigdata): \nRe-enter password: \nConfiguring ambari database...\nConfiguring remote database connection properties...\nWARNING: Before starting Ambari Server, you must run the following DDL against the database to create the schema: /var/lib/ambari-server/resources/Ambari-DDL-MySQL-CREATE.sql\nProceed with configuring remote database connection properties [y/n] (y)? y\nExtracting system views...\nambari-admin-2.6.1.5.3.jar\n...........\nAdjusting ambari-server permissions and ownership...\nAmbari Server &#39;setup&#39; completed successfully.</code></pre></div><h2>4.4.3将Ambari数据库脚本导入到数据库</h2><div class=\"highlight\"><pre><code class=\"language-bash\">mysql -uambari -pAmbari-123\nuse ambari<span class=\"p\">;</span>\n<span class=\"nb\">source</span> /var/lib/ambari-server/resources/Ambari-DDL-MySQL-CREATE.sql</code></pre></div><h2>4.4.4 启动ambari</h2><div class=\"highlight\"><pre><code class=\"language-bash\">ambari-server start</code></pre></div><h2>4.4.5 启动成功，可以通过如下地址访问：</h2><p><a href=\"https://link.zhihu.com/?target=http%3A//ambari.master.com%3A8080/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">http://ambari.master.com:8080</a></p><p>用户名，密码为admin admin</p><h2>5、使用ambari浏览器界面安装hadoop,hive等组件</h2><h2>5.1 登录到ambari管理界面</h2><p><a href=\"https://link.zhihu.com/?target=http%3A//ambari.master.com%3A8080/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">http://ambari.master.com:8080</a></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-4dea9bfbae8c49a1656a99b8b66654f3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1416\" data-rawheight=\"597\" class=\"origin_image zh-lightbox-thumb\" width=\"1416\" data-original=\"https://pic4.zhimg.com/v2-4dea9bfbae8c49a1656a99b8b66654f3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1416&#39; height=&#39;597&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1416\" data-rawheight=\"597\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1416\" data-original=\"https://pic4.zhimg.com/v2-4dea9bfbae8c49a1656a99b8b66654f3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-4dea9bfbae8c49a1656a99b8b66654f3_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>5.2 安装hdp集群，点击Launch Install Wizard</h2><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-6b3ba689f47ac0b854ef75c90689ce48_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1528\" data-rawheight=\"820\" class=\"origin_image zh-lightbox-thumb\" width=\"1528\" data-original=\"https://pic1.zhimg.com/v2-6b3ba689f47ac0b854ef75c90689ce48_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1528&#39; height=&#39;820&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1528\" data-rawheight=\"820\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1528\" data-original=\"https://pic1.zhimg.com/v2-6b3ba689f47ac0b854ef75c90689ce48_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-6b3ba689f47ac0b854ef75c90689ce48_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>5.3，设置集群名称</h2><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-8b2081b44825bdee2d0dd31de7c63b8f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1622\" data-rawheight=\"696\" class=\"origin_image zh-lightbox-thumb\" width=\"1622\" data-original=\"https://pic4.zhimg.com/v2-8b2081b44825bdee2d0dd31de7c63b8f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1622&#39; height=&#39;696&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1622\" data-rawheight=\"696\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1622\" data-original=\"https://pic4.zhimg.com/v2-8b2081b44825bdee2d0dd31de7c63b8f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-8b2081b44825bdee2d0dd31de7c63b8f_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>5.4 配置本地源</h2><p>其中HDP-GPL较小,用默认的即可 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-11f1fe3199b8751937246198e7201efe_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1836\" data-rawheight=\"1041\" class=\"origin_image zh-lightbox-thumb\" width=\"1836\" data-original=\"https://pic3.zhimg.com/v2-11f1fe3199b8751937246198e7201efe_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1836&#39; height=&#39;1041&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1836\" data-rawheight=\"1041\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1836\" data-original=\"https://pic3.zhimg.com/v2-11f1fe3199b8751937246198e7201efe_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-11f1fe3199b8751937246198e7201efe_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>5.5 设置host</h2><p>其中下面的为master上ssh的私钥（~/.ssh/id_rsa） </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-b3daa8a32acefce75a8ccbf5aa78c48a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1687\" data-rawheight=\"914\" class=\"origin_image zh-lightbox-thumb\" width=\"1687\" data-original=\"https://pic3.zhimg.com/v2-b3daa8a32acefce75a8ccbf5aa78c48a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1687&#39; height=&#39;914&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1687\" data-rawheight=\"914\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1687\" data-original=\"https://pic3.zhimg.com/v2-b3daa8a32acefce75a8ccbf5aa78c48a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-b3daa8a32acefce75a8ccbf5aa78c48a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>5.6 Host确认</h2><p>如果失败或者卡住不动可根据日志解决，如果warn根据提示信息解决，直到全部为Success才可以进行下一步。 注：我一般根据/var/log/ambari-server/ambari-server.log 查看ambari的日志，根据里面的异常解决问题，如果没有异常，再查看ambari的其他日志文件。 下面两个是我在使用ambari时碰到的异常，可以参考：<a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/07/10/HadoopException/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">HDFS DataNode启动异常:/opt/jdk1.8.0_151/bin/java:权限不够</a>和<a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/07/10/ambariExceptions/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">ambari 异常总结及解决办法</a></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-9a3a99b59612191d4bebaa341cc0a59e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1681\" data-rawheight=\"800\" class=\"origin_image zh-lightbox-thumb\" width=\"1681\" data-original=\"https://pic3.zhimg.com/v2-9a3a99b59612191d4bebaa341cc0a59e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1681&#39; height=&#39;800&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1681\" data-rawheight=\"800\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1681\" data-original=\"https://pic3.zhimg.com/v2-9a3a99b59612191d4bebaa341cc0a59e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-9a3a99b59612191d4bebaa341cc0a59e_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-a63d7d8e3d93c65322eed6a2cd2be9b7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1704\" data-rawheight=\"838\" class=\"origin_image zh-lightbox-thumb\" width=\"1704\" data-original=\"https://pic4.zhimg.com/v2-a63d7d8e3d93c65322eed6a2cd2be9b7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1704&#39; height=&#39;838&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1704\" data-rawheight=\"838\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1704\" data-original=\"https://pic4.zhimg.com/v2-a63d7d8e3d93c65322eed6a2cd2be9b7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-a63d7d8e3d93c65322eed6a2cd2be9b7_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>5.7 选择要安装的服务</h2><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-19b4c85b3e2d9723918b48b718eeb6f6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1720\" data-rawheight=\"1039\" class=\"origin_image zh-lightbox-thumb\" width=\"1720\" data-original=\"https://pic3.zhimg.com/v2-19b4c85b3e2d9723918b48b718eeb6f6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1720&#39; height=&#39;1039&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1720\" data-rawheight=\"1039\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1720\" data-original=\"https://pic3.zhimg.com/v2-19b4c85b3e2d9723918b48b718eeb6f6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-19b4c85b3e2d9723918b48b718eeb6f6_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-a0ab9a4b2c79170ac219399afac294fe_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1678\" data-rawheight=\"893\" class=\"origin_image zh-lightbox-thumb\" width=\"1678\" data-original=\"https://pic3.zhimg.com/v2-a0ab9a4b2c79170ac219399afac294fe_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1678&#39; height=&#39;893&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1678\" data-rawheight=\"893\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1678\" data-original=\"https://pic3.zhimg.com/v2-a0ab9a4b2c79170ac219399afac294fe_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-a0ab9a4b2c79170ac219399afac294fe_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>如果有依赖其他组件选择ok即可，如安装hive依赖tez，pig等 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-531024fa8b413b732fa4170bc3d9d69d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1648\" data-rawheight=\"828\" class=\"origin_image zh-lightbox-thumb\" width=\"1648\" data-original=\"https://pic2.zhimg.com/v2-531024fa8b413b732fa4170bc3d9d69d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1648&#39; height=&#39;828&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1648\" data-rawheight=\"828\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1648\" data-original=\"https://pic2.zhimg.com/v2-531024fa8b413b732fa4170bc3d9d69d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-531024fa8b413b732fa4170bc3d9d69d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>5.8 设置各个服务Master</h2><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-ffe6ebd8d7f97406c78e44b32db316d9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1776\" data-rawheight=\"1020\" class=\"origin_image zh-lightbox-thumb\" width=\"1776\" data-original=\"https://pic2.zhimg.com/v2-ffe6ebd8d7f97406c78e44b32db316d9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1776&#39; height=&#39;1020&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1776\" data-rawheight=\"1020\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1776\" data-original=\"https://pic2.zhimg.com/v2-ffe6ebd8d7f97406c78e44b32db316d9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-ffe6ebd8d7f97406c78e44b32db316d9_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>5.9 设置Slaves 和 Clients</h2><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-38a3c1033bce23b4be8c36b818d65ab0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1778\" data-rawheight=\"838\" class=\"origin_image zh-lightbox-thumb\" width=\"1778\" data-original=\"https://pic1.zhimg.com/v2-38a3c1033bce23b4be8c36b818d65ab0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1778&#39; height=&#39;838&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1778\" data-rawheight=\"838\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1778\" data-original=\"https://pic1.zhimg.com/v2-38a3c1033bce23b4be8c36b818d65ab0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-38a3c1033bce23b4be8c36b818d65ab0_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>5.10 自定义配置</h2><p>其中红色的必须要改，大致是设置路径，密码等，如hive要设置hive元数据的数据库信息，我用的master上的mysql </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-fe6e1c8a0cfd8b1662e289cbab27a76b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1856\" data-rawheight=\"1024\" class=\"origin_image zh-lightbox-thumb\" width=\"1856\" data-original=\"https://pic4.zhimg.com/v2-fe6e1c8a0cfd8b1662e289cbab27a76b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1856&#39; height=&#39;1024&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1856\" data-rawheight=\"1024\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1856\" data-original=\"https://pic4.zhimg.com/v2-fe6e1c8a0cfd8b1662e289cbab27a76b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-fe6e1c8a0cfd8b1662e289cbab27a76b_b.jpg\"/></figure><p> 测试一下连接 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-430c84e83c50ee4d12a25c2aa25a3666_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1856\" data-rawheight=\"1010\" class=\"origin_image zh-lightbox-thumb\" width=\"1856\" data-original=\"https://pic3.zhimg.com/v2-430c84e83c50ee4d12a25c2aa25a3666_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1856&#39; height=&#39;1010&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1856\" data-rawheight=\"1010\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1856\" data-original=\"https://pic3.zhimg.com/v2-430c84e83c50ee4d12a25c2aa25a3666_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-430c84e83c50ee4d12a25c2aa25a3666_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>没有了红色的即可进行下一步，如遇到warn，可根据提示信息进行修改配置，也可以忽略警告，等装完以后再改。</p><h2>5.11 review前面的配置</h2><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-e6e48e33ad50dda7eaac9d1f377dc51d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1789\" data-rawheight=\"884\" class=\"origin_image zh-lightbox-thumb\" width=\"1789\" data-original=\"https://pic2.zhimg.com/v2-e6e48e33ad50dda7eaac9d1f377dc51d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1789&#39; height=&#39;884&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1789\" data-rawheight=\"884\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1789\" data-original=\"https://pic2.zhimg.com/v2-e6e48e33ad50dda7eaac9d1f377dc51d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-e6e48e33ad50dda7eaac9d1f377dc51d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>5.12 安装、启动、测试</h2><p>这里因为个人电脑配置较低，浏览器有点卡，进度条没有显示出来。 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-fc00d5fc93cca050c9c930d3e0a433f5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1810\" data-rawheight=\"804\" class=\"origin_image zh-lightbox-thumb\" width=\"1810\" data-original=\"https://pic2.zhimg.com/v2-fc00d5fc93cca050c9c930d3e0a433f5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1810&#39; height=&#39;804&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1810\" data-rawheight=\"804\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1810\" data-original=\"https://pic2.zhimg.com/v2-fc00d5fc93cca050c9c930d3e0a433f5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-fc00d5fc93cca050c9c930d3e0a433f5_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>5.13 安装完成</h2><p>若最后出现警告，可以装完重启所有服务，再检查看看有没有问题，如有警告或启动失败，可根据日志排查原因，一开始安装的的组件较多的话，出现警告的可能性会大一些，所以可以先装几个必要的组件，之后一个一个组件装。 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-a7faea7635de564f23f8725f239258ab_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1830\" data-rawheight=\"838\" class=\"origin_image zh-lightbox-thumb\" width=\"1830\" data-original=\"https://pic4.zhimg.com/v2-a7faea7635de564f23f8725f239258ab_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1830&#39; height=&#39;838&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1830\" data-rawheight=\"838\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1830\" data-original=\"https://pic4.zhimg.com/v2-a7faea7635de564f23f8725f239258ab_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-a7faea7635de564f23f8725f239258ab_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>5.14 概要</h2><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-de7a5bd1773572828e05e20921533117_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1678\" data-rawheight=\"653\" class=\"origin_image zh-lightbox-thumb\" width=\"1678\" data-original=\"https://pic4.zhimg.com/v2-de7a5bd1773572828e05e20921533117_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1678&#39; height=&#39;653&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1678\" data-rawheight=\"653\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1678\" data-original=\"https://pic4.zhimg.com/v2-de7a5bd1773572828e05e20921533117_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-de7a5bd1773572828e05e20921533117_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>5.15 hive启动异常</h2><p>这次安装重启之后发现hive等服务启动不成功，我就把hive等卸载然后重装，本来以为是开始是hive没安装成功，但是重装后hive还是启动不成功，看了一下日志，发现是之前手动安装的原生的hive的环境变量没有注释掉，注释掉，重启ambari之后，再启动所有服务，就成功了（再在hive shell 里建表、插入数据、查询验证一下），所以如果在已经安装好的大数据集群上安装ambari，最好先把之前配的环境变量注释掉。</p><h2>5.16 启动成功</h2><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-22b49636d834d7251f4b76b1f811f15f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1138\" data-rawheight=\"305\" class=\"origin_image zh-lightbox-thumb\" width=\"1138\" data-original=\"https://pic4.zhimg.com/v2-22b49636d834d7251f4b76b1f811f15f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1138&#39; height=&#39;305&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1138\" data-rawheight=\"305\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1138\" data-original=\"https://pic4.zhimg.com/v2-22b49636d834d7251f4b76b1f811f15f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-22b49636d834d7251f4b76b1f811f15f_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-c4b27d647cf522ebe5204c30d77000f9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1798\" data-rawheight=\"921\" class=\"origin_image zh-lightbox-thumb\" width=\"1798\" data-original=\"https://pic2.zhimg.com/v2-c4b27d647cf522ebe5204c30d77000f9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1798&#39; height=&#39;921&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1798\" data-rawheight=\"921\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1798\" data-original=\"https://pic2.zhimg.com/v2-c4b27d647cf522ebe5204c30d77000f9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-c4b27d647cf522ebe5204c30d77000f9_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "大数据", 
                    "tagLink": "https://api.zhihu.com/topics/19740929"
                }, 
                {
                    "tag": "CentOS", 
                    "tagLink": "https://api.zhihu.com/topics/19577255"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/41785243", 
            "userName": "董可伦", 
            "userLink": "https://www.zhihu.com/people/de888cc734ddbfd918269a8e5fb196ba", 
            "upvote": 0, 
            "title": "Spark on yarn 配置及异常解决", 
            "content": "<p>我的原创地址：<u><a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/04/16/sparkOnYarnConf/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">dongkelun.com/2018/04/1</span><span class=\"invisible\">6/sparkOnYarnConf/</span><span class=\"ellipsis\"></span></a></u></p><h2>前言</h2><p>YARN 是在Hadoop 2.0 中引入的集群管理器，它可以让多种数据处理框架运行在一个共享的资源池上，并且通常安装在与Hadoop 文件系统（简称HDFS）相同的物理节点上。在这样配置的YARN 集群上运行Spark 是很有意义的，它可以让Spark 在存储数据的物理节点上运行，以快速访问HDFS 中的数据。</p><h2>1、配置</h2><h2>1.1 配置HADOOP_CONF_DIR</h2><div class=\"highlight\"><pre><code class=\"language-bash\">vim /etc/profile\n<span class=\"nb\">export</span> <span class=\"nv\">HADOOP_CONF_DIR</span><span class=\"o\">=</span>/opt/hadoop-2.7.5/etc/hadoop\n<span class=\"nb\">source</span> /etc/profile</code></pre></div><h2>1.2 命令行启动</h2><div class=\"highlight\"><pre><code class=\"language-bash\">spark-shell --master yarn</code></pre></div><p>但是在spark2.x里会报一个错误</p><div class=\"highlight\"><pre><code class=\"language-text\">18/04/16 07:59:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n18/04/16 07:59:27 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n18/04/16 07:59:54 ERROR SparkContext: Error initializing SparkContext.\norg.apache.spark.SparkException: Yarn application has already ended! It might have been killed or unable to launch application master.\n    at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.waitForApplication(YarnClientSchedulerBackend.scala:85)\n    at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:62)\n    at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:173)\n    at org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:509)\n    at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2516)\n    at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:918)\n    at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:910)\n    at scala.Option.getOrElse(Option.scala:121)\n    at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:910)\n    at org.apache.spark.repl.Main$.createSparkSession(Main.scala:101)\n    at $line3.$read$$iw$$iw.&lt;init&gt;(&lt;console&gt;:15)\n    at $line3.$read$$iw.&lt;init&gt;(&lt;console&gt;:42)\n    at $line3.$read.&lt;init&gt;(&lt;console&gt;:44)\n    at $line3.$read$.&lt;init&gt;(&lt;console&gt;:48)\n    at $line3.$read$.&lt;clinit&gt;(&lt;console&gt;)\n    at $line3.$eval$.$print$lzycompute(&lt;console&gt;:7)\n    at $line3.$eval$.$print(&lt;console&gt;:6)\n    at $line3.$eval.$print(&lt;console&gt;)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:497)\n    at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:786)\n    at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1047)\n    at scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:638)\n    at scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:637)\n    at scala.reflect.internal.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:31)\n    at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:19)\n    at scala.tools.nsc.interpreter.IMain$WrappedRequest.loadAndRunReq(IMain.scala:637)\n    at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:569)\n    at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:565)\n    at scala.tools.nsc.interpreter.ILoop.interpretStartingWith(ILoop.scala:807)\n    at scala.tools.nsc.interpreter.ILoop.command(ILoop.scala:681)\n    at scala.tools.nsc.interpreter.ILoop.processLine(ILoop.scala:395)\n    at org.apache.spark.repl.SparkILoop$$anonfun$initializeSpark$1.apply$mcV$sp(SparkILoop.scala:38)\n    at org.apache.spark.repl.SparkILoop$$anonfun$initializeSpark$1.apply(SparkILoop.scala:37)\n    at org.apache.spark.repl.SparkILoop$$anonfun$initializeSpark$1.apply(SparkILoop.scala:37)\n    at scala.tools.nsc.interpreter.IMain.beQuietDuring(IMain.scala:214)\n    at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:37)\n    at org.apache.spark.repl.SparkILoop.loadFiles(SparkILoop.scala:98)\n    at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply$mcZ$sp(ILoop.scala:920)\n    at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply(ILoop.scala:909)\n    at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply(ILoop.scala:909)\n    at scala.reflect.internal.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:97)\n    at scala.tools.nsc.interpreter.ILoop.process(ILoop.scala:909)\n    at org.apache.spark.repl.Main$.doMain(Main.scala:74)\n    at org.apache.spark.repl.Main$.main(Main.scala:54)\n    at org.apache.spark.repl.Main.main(Main.scala)\n    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n    at java.lang.reflect.Method.invoke(Method.java:497)\n    at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:775)\n    at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)\n    at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)\n    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)\n    at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\n18/04/16 07:59:54 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!\n18/04/16 07:59:54 WARN MetricsSystem: Stopping a MetricsSystem that is not running\norg.apache.spark.SparkException: Yarn application has already ended! It might have been killed or unable to launch application master.\n  at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.waitForApplication(YarnClientSchedulerBackend.scala:85)\n  at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:62)\n  at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:173)\n  at org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:509)\n  at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2516)\n  at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:918)\n  at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:910)\n  at scala.Option.getOrElse(Option.scala:121)\n  at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:910)\n  at org.apache.spark.repl.Main$.createSparkSession(Main.scala:101)\n  ... 47 elided\n&lt;console&gt;:14: error: not found: value spark\n       import spark.implicits._\n              ^\n&lt;console&gt;:14: error: not found: value spark\n       import spark.sql\n              ^\nWelcome to\n      ____              __\n     / __/__  ___ _____/ /__\n    _\\ \\/ _ \\/ _ `/ __/  &#39;_/\n   /___/ .__/\\_,_/_/ /_/\\_\\   version 2.2.1\n      /_/\n\nUsing Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_45)\nType in expressions to have them evaluated.\nType :help for more information.\n\nscala&gt;</code></pre></div><h2>2、错误解决</h2><h2>2.1 添加spark.yarn.jars</h2><p>首先看到第二条warn</p><div class=\"highlight\"><pre><code class=\"language-bash\">Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.</code></pre></div><p>联想到是不是这条warn信息导致的，然后根据这条warn信息上网查了一下，再根据错误信息也查了一下</p><div class=\"highlight\"><pre><code class=\"language-text\">Yarn application has already ended! It might have been killed or unable to ...</code></pre></div><p>发现，都是说要配置spark.yarn.jars，于是按照如下命令配置</p><div class=\"highlight\"><pre><code class=\"language-bash\">hdfs dfs -mkdir /hadoop\nhdfs dfs -mkdir /hadoop/spark_jars\nhdfs dfs -put /opt/spark-2.2.1-bin-hadoop2.7/jars/* /hadoop/spark_jars\n<span class=\"nb\">cd</span> /opt/spark-2.2.1-bin-hadoop2.7/conf/\ncp spark-defaults.conf.template spark-defaults.conf\nvim spark-defaults.conf</code></pre></div><p>在最下面添加：</p><div class=\"highlight\"><pre><code class=\"language-text\">spark.yarn.jars hdfs://192.168.44.128:8888/hadoop/spark_jars/*</code></pre></div><p>(注意后面的*不能去掉) 然后启动spark-shell,发现还是报相似错误（没了warn）</p><h2>2.2 配置hadoop的yarn-site.xml</h2><p>因为java8导致的问题</p><div class=\"highlight\"><pre><code class=\"language-bash\">vim /opt/hadoop-2.7.5/etc/hadoop/yarn-site.xml</code></pre></div><p>添加：</p><div class=\"highlight\"><pre><code class=\"language-text\">&lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt;\n    &lt;value&gt;false&lt;/value&gt;\n&lt;/property&gt;\n\n&lt;property&gt;\n    &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt;\n    &lt;value&gt;false&lt;/value&gt;\n&lt;/property&gt;</code></pre></div><p>再次启动spark-shell,成功！ </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-ae39d772b60e7f34fd5ac252fdd656ce_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1100\" data-rawheight=\"263\" class=\"origin_image zh-lightbox-thumb\" width=\"1100\" data-original=\"https://pic3.zhimg.com/v2-ae39d772b60e7f34fd5ac252fdd656ce_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1100&#39; height=&#39;263&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1100\" data-rawheight=\"263\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1100\" data-original=\"https://pic3.zhimg.com/v2-ae39d772b60e7f34fd5ac252fdd656ce_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-ae39d772b60e7f34fd5ac252fdd656ce_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>3、意外之喜</h2><p>由于要写博客记录，所以需要将错误还原，第一次只将spark.yarn.jars注释掉，启动spark-shell,发现是成功的，只是会有条warn而已，也就是说，这个错误的根本原因，是java8导致没有配置2.2中的yarn-site.xml！！ </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-cb5d84d49f2b6ad35c6c18578e18c07a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1157\" data-rawheight=\"255\" class=\"origin_image zh-lightbox-thumb\" width=\"1157\" data-original=\"https://pic3.zhimg.com/v2-cb5d84d49f2b6ad35c6c18578e18c07a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1157&#39; height=&#39;255&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1157\" data-rawheight=\"255\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1157\" data-original=\"https://pic3.zhimg.com/v2-cb5d84d49f2b6ad35c6c18578e18c07a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-cb5d84d49f2b6ad35c6c18578e18c07a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>参考资料</h2><p><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/lxhandlbb/article/details/54410644\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">blog.csdn.net/lxhandlbb</span><span class=\"invisible\">/article/details/54410644</span><span class=\"ellipsis\"></span></a> <a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/gg584741/article/details/72825713\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">blog.csdn.net/gg584741/</span><span class=\"invisible\">article/details/72825713</span><span class=\"ellipsis\"></span></a></p>", 
            "topic": [
                {
                    "tag": "Spark", 
                    "tagLink": "https://api.zhihu.com/topics/19942170"
                }, 
                {
                    "tag": "Hadoop", 
                    "tagLink": "https://api.zhihu.com/topics/19563390"
                }, 
                {
                    "tag": "YARN", 
                    "tagLink": "https://api.zhihu.com/topics/20041614"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/41784915", 
            "userName": "董可伦", 
            "userLink": "https://www.zhihu.com/people/de888cc734ddbfd918269a8e5fb196ba", 
            "upvote": 4, 
            "title": "centos7 hadoop 集群安装配置", 
            "content": "<p></p><p>我的原创地址：<u><a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/04/05/hadoopClusterConf/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">dongkelun.com/2018/04/0</span><span class=\"invisible\">5/hadoopClusterConf/</span><span class=\"ellipsis\"></span></a></u></p><h2>前言：</h2><p>本文安装配置的hadoop为分布式的集群，单机配置见：<a href=\"https://link.zhihu.com/?target=http%3A//dongkelun.com/2018/03/23/hadoopConf/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">centos7 hadoop 单机模式安装配置</a> 我用的三个centos7, 先将常用环境配置好（<a href=\"https://link.zhihu.com/?target=http%3A//dongkelun.com/2018/04/05/centosInitialConf/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">CentOS 初始环境配置</a>），设置的ip分别为：192.168.44.138、192.168.44.139，192.168.44.140，分别对应别名master、slave1、slave2</p><h2>1、首先安装配置jdk（我安装的1.8）</h2><h2>2、给每个虚拟机的ip起个别名</h2><p>在每个虚拟机上执行</p><div class=\"highlight\"><pre><code class=\"language-text\">vim /etc/hosts</code></pre></div><p>在最下面添加：</p><div class=\"highlight\"><pre><code class=\"language-text\">192.168.44.138 master\n192.168.44.139 slave1\n192.168.44.140 slave2</code></pre></div><p>在每个虚拟机上ping一下，保证都能ping通</p><div class=\"highlight\"><pre><code class=\"language-text\">ping master\nping slave1\nping slave2</code></pre></div><h2>3、SSH免密码登录</h2><p>保证三台机器都可以免密互通，参考：<a href=\"https://link.zhihu.com/?target=http%3A//dongkelun.com/2018/04/05/sshConf/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">linux ssh 免密登录</a></p><h2>3、下载hadoop（每台机器）</h2><p>下载地址：<a href=\"https://link.zhihu.com/?target=http%3A//mirror.bit.edu.cn/apache/hadoop/common/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">mirror.bit.edu.cn/apach</span><span class=\"invisible\">e/hadoop/common/</span><span class=\"ellipsis\"></span></a>，我下载的是hadoop-2.7.5.tar.gz</p><h2>4、解压到/opt目录下（每台机器、目录根据自己习惯）</h2><div class=\"highlight\"><pre><code class=\"language-bash\">tar -zxvf hadoop-2.7.5.tar.gz  -C /opt/</code></pre></div><h2>5、配置hadoop环境变量（每台机器）</h2><div class=\"highlight\"><pre><code class=\"language-bash\">vim /etc/profile\n<span class=\"nb\">export</span> <span class=\"nv\">HADOOP_HOME</span><span class=\"o\">=</span>/opt/hadoop-2.7.5\n<span class=\"nb\">export</span> <span class=\"nv\">PATH</span><span class=\"o\">=</span><span class=\"nv\">$PATH</span>:<span class=\"nv\">$HADOOP_HOME</span>/bin\n<span class=\"nb\">source</span> /etc/profile</code></pre></div><h2>6、配置hadoop（仅master）</h2><p>配置文件里的文件路径和端口随自己习惯配置</p><h2>6.1 配置slaves</h2><p>需要现将slaves1文件中的localhost删掉，本次使用两个slave节点，让master仅作为NameNode使用，也可以让master既作为NameNode也作为 DataNode，在slaves添加master即可</p><div class=\"highlight\"><pre><code class=\"language-text\">vim /opt/hadoop-2.7.5/etc/hadoop/slaves\nslave1\nslave2</code></pre></div><h2>6.2 配置hadoop-env.sh</h2><div class=\"highlight\"><pre><code class=\"language-bash\">vim /opt/hadoop-2.7.5/etc/hadoop/hadoop-env.sh</code></pre></div><p>找到# The java implementation to use.将其下面的一行改为：</p><div class=\"highlight\"><pre><code class=\"language-bash\"><span class=\"nb\">export</span> <span class=\"nv\">JAVA_HOME</span><span class=\"o\">=</span>/opt/jdk1.8.0_45</code></pre></div><h2>6.3 配置core-site.xml</h2><div class=\"highlight\"><pre><code class=\"language-text\">vim /opt/hadoop-2.7.5/etc/hadoop/core-site.xml\n&lt;configuration&gt;\n    &lt;property&gt;\n        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;\n        &lt;value&gt;file:///opt/hadoop-2.7.5&lt;/value&gt;\n        &lt;description&gt;Abase for other temporary directories.&lt;/description&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.defaultFS&lt;/name&gt;\n        &lt;value&gt;hdfs://master:8888&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;</code></pre></div><h2>6.4 配置hdfs-site.xml</h2><div class=\"highlight\"><pre><code class=\"language-text\">vim /opt/hadoop-2.7.5/etc/hadoop/hdfs-site.xml</code></pre></div><p>dfs.replication 一般设为 3，但这次只使用两个slave，所以 dfs.replication 的值设为 2</p><div class=\"highlight\"><pre><code class=\"language-bash\">&lt;configuration&gt;\n    &lt;property&gt;\n        &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;\n        &lt;value&gt;master:50090&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;dfs.replication&lt;/name&gt;\n        &lt;value&gt;2&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;\n        &lt;value&gt;file:///opt/hadoop-2.7.5/tmp/dfs/name&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;\n        &lt;value&gt;file:///opt/hadoop-2.7.5/tmp/dfs/data&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;</code></pre></div><h2>6.5 配置yarn-site.xml</h2><div class=\"highlight\"><pre><code class=\"language-bash\">vim /opt/hadoop-2.7.5/etc/hadoop/yarn-site.xml\n&lt;configuration&gt;\n\n&lt;!-- Site specific YARN configuration properties --&gt;\n    &lt;property&gt;\n        &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;\n        &lt;value&gt;master&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;\n        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;</code></pre></div><h2>6.6 配置mapred-site.xml</h2><div class=\"highlight\"><pre><code class=\"language-bash\"><span class=\"nb\">cd</span> /opt/hadoop-2.7.5/etc/hadoop/\ncp mapred-site.xml.template mapred-site.xml\nvim mapred-site.xml\n&lt;configuration&gt;\n    &lt;property&gt;\n        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;\n        &lt;value&gt;yarn&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;</code></pre></div><h2>6.7 将上述配置的文件传到其他节点的/opt/hadoop-2.7.5/etc/hadoop/目录中</h2><div class=\"highlight\"><pre><code class=\"language-bash\">scp -r slaves hadoop-env.sh core-site.xml  hdfs-site.xml yarn-site.xml hdfs-site.xml root@slave1:/opt/hadoop-2.7.5/etc/hadoop/\nscp -r slaves hadoop-env.sh core-site.xml  hdfs-site.xml yarn-site.xml hdfs-site.xml root@slave2:/opt/hadoop-2.7.5/etc/hadoop/</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-9ccf4052abb21ac29a151d8ec31caaaf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1193\" data-rawheight=\"238\" class=\"origin_image zh-lightbox-thumb\" width=\"1193\" data-original=\"https://pic4.zhimg.com/v2-9ccf4052abb21ac29a151d8ec31caaaf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1193&#39; height=&#39;238&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1193\" data-rawheight=\"238\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1193\" data-original=\"https://pic4.zhimg.com/v2-9ccf4052abb21ac29a151d8ec31caaaf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-9ccf4052abb21ac29a151d8ec31caaaf_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>7、启动与停止（仅master）</h2><h2>7.1 hdfs启动与停止</h2><p>第一次启动hdfs需要先格式化：</p><div class=\"highlight\"><pre><code class=\"language-bash\"><span class=\"nb\">cd</span> /opt/hadoop-2.7.5\n./bin/hdfs namenode -format</code></pre></div><p>启动：</p><div class=\"highlight\"><pre><code class=\"language-bash\">./sbin/start-dfs.sh</code></pre></div><p>停止：</p><div class=\"highlight\"><pre><code class=\"language-bash\">./sbin/stop-dfs.sh</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-73dd1869eaac871816738fd710027d4a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"820\" data-rawheight=\"112\" class=\"origin_image zh-lightbox-thumb\" width=\"820\" data-original=\"https://pic3.zhimg.com/v2-73dd1869eaac871816738fd710027d4a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;820&#39; height=&#39;112&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"820\" data-rawheight=\"112\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"820\" data-original=\"https://pic3.zhimg.com/v2-73dd1869eaac871816738fd710027d4a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-73dd1869eaac871816738fd710027d4a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>验证，浏览器输入：<a href=\"https://link.zhihu.com/?target=http%3A//192.168.44.138%3A50070\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">192.168.44.138:50070</span><span class=\"invisible\"></span></a></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-13811a5136b9803052b161462712b00b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1688\" data-rawheight=\"1012\" class=\"origin_image zh-lightbox-thumb\" width=\"1688\" data-original=\"https://pic4.zhimg.com/v2-13811a5136b9803052b161462712b00b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1688&#39; height=&#39;1012&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1688\" data-rawheight=\"1012\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1688\" data-original=\"https://pic4.zhimg.com/v2-13811a5136b9803052b161462712b00b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-13811a5136b9803052b161462712b00b_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>简单的验证hadoop命令：</p><div class=\"highlight\"><pre><code class=\"language-bash\">hadoop fs -mkdir /test</code></pre></div><p>在浏览器查看，出现如下图所示，即为成功 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-5ba0850100db7c652ee39880be6f1a4c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1558\" data-rawheight=\"486\" class=\"origin_image zh-lightbox-thumb\" width=\"1558\" data-original=\"https://pic1.zhimg.com/v2-5ba0850100db7c652ee39880be6f1a4c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1558&#39; height=&#39;486&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1558\" data-rawheight=\"486\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1558\" data-original=\"https://pic1.zhimg.com/v2-5ba0850100db7c652ee39880be6f1a4c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-5ba0850100db7c652ee39880be6f1a4c_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>7.2 yarn启动与停止</h2><p>启动：</p><div class=\"highlight\"><pre><code class=\"language-bash\"><span class=\"nb\">cd</span> /opt/hadoop-2.7.5\n./sbin/start-yarn.sh\n./sbin/stop-yarn.sh</code></pre></div><p>浏览器查看：<a href=\"https://link.zhihu.com/?target=http%3A//192.168.44.138%3A8088\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">192.168.44.138:8088</span><span class=\"invisible\"></span></a> </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-331d77f7b4d1e320f2d607687a26f761_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1899\" data-rawheight=\"509\" class=\"origin_image zh-lightbox-thumb\" width=\"1899\" data-original=\"https://pic2.zhimg.com/v2-331d77f7b4d1e320f2d607687a26f761_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1899&#39; height=&#39;509&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1899\" data-rawheight=\"509\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1899\" data-original=\"https://pic2.zhimg.com/v2-331d77f7b4d1e320f2d607687a26f761_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-331d77f7b4d1e320f2d607687a26f761_b.jpg\"/></figure><p> jps查看进程 master: </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-9e1ac8a808f2f61624a11fec0a4a44fb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"582\" data-rawheight=\"82\" class=\"origin_image zh-lightbox-thumb\" width=\"582\" data-original=\"https://pic4.zhimg.com/v2-9e1ac8a808f2f61624a11fec0a4a44fb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;582&#39; height=&#39;82&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"582\" data-rawheight=\"82\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"582\" data-original=\"https://pic4.zhimg.com/v2-9e1ac8a808f2f61624a11fec0a4a44fb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-9e1ac8a808f2f61624a11fec0a4a44fb_b.jpg\"/></figure><p> slave1: </p><p> slave2: </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-1011c2bace8fa85f8e66c76a38c227b7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"639\" data-rawheight=\"65\" class=\"origin_image zh-lightbox-thumb\" width=\"639\" data-original=\"https://pic4.zhimg.com/v2-1011c2bace8fa85f8e66c76a38c227b7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;639&#39; height=&#39;65&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"639\" data-rawheight=\"65\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"639\" data-original=\"https://pic4.zhimg.com/v2-1011c2bace8fa85f8e66c76a38c227b7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-1011c2bace8fa85f8e66c76a38c227b7_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>若各节点的进程均如图所示，那么hadoop集群就配置成功！</p><h2>参考资料</h2><p><a href=\"https://link.zhihu.com/?target=http%3A//www.powerxing.com/install-hadoop-cluster/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">powerxing.com/install-h</span><span class=\"invisible\">adoop-cluster/</span><span class=\"ellipsis\"></span></a></p>", 
            "topic": [
                {
                    "tag": "Hadoop", 
                    "tagLink": "https://api.zhihu.com/topics/19563390"
                }, 
                {
                    "tag": "分布式系统", 
                    "tagLink": "https://api.zhihu.com/topics/19570823"
                }, 
                {
                    "tag": "分布式计算", 
                    "tagLink": "https://api.zhihu.com/topics/19552071"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/39544213", 
            "userName": "董可伦", 
            "userLink": "https://www.zhihu.com/people/de888cc734ddbfd918269a8e5fb196ba", 
            "upvote": 0, 
            "title": "centos7 hive 单机模式安装配置", 
            "content": "<p>转载请务必注明原创地址为：<u><a href=\"https://link.zhihu.com/?target=https%3A//dongkelun.com/2018/03/24/hiveConf/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">dongkelun.com/2018/03/2</span><span class=\"invisible\">4/hiveConf/</span><span class=\"ellipsis\"></span></a></u></p><h2>前言：</h2><p>由于只是在自己的虚拟机上进行学习，所以对hive只是进行最简单的配置，其他复杂的配置文件没有配置。</p><h2>1、前提</h2><h2>1.1 安装配置jdk1.8</h2><h2>1.2 安装hadoop2.x</h2><p>hadoop单机模式安装见：<a href=\"https://link.zhihu.com/?target=http%3A//dongkelun.com/2018/03/23/hadoopConf/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">centos7 hadoop 单机模式安装配置</a></p><h2>1.3 安装mysql并配置myql允许远程访问，我的mysql版本5.7.18。</h2><p>mysql数据库安装过程请参考：<a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/lochy/article/details/51721319\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Centos 7.2 安装 Mysql 5.7.13</a></p><h2>2、下载hive</h2><p>下载地址：<a href=\"https://link.zhihu.com/?target=http%3A//mirror.bit.edu.cn/apache/hive/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">mirror.bit.edu.cn/apach</span><span class=\"invisible\">e/hive/</span><span class=\"ellipsis\"></span></a>，我下载的是apache-hive-2.3.2-bin.tar.gz。</p><div class=\"highlight\"><pre><code class=\"language-text\">wget http://mirror.bit.edu.cn/apache/hive/hive-2.3.2/apache-hive-2.3.2-bin.tar.gz\n或者下载到本地，通过工具上传到虚拟机中</code></pre></div><h2>3、解压到/opt目录下（目录根据自己习惯）</h2><div class=\"highlight\"><pre><code class=\"language-bash\">tar -zxvf apache-hive-2.3.2-bin.tar.gz  -C /opt/</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2>4、配置hive环境变量</h2><div class=\"highlight\"><pre><code class=\"language-bash\">vim /etc/profile\n<span class=\"nb\">export</span> <span class=\"nv\">HIVE_HOME</span><span class=\"o\">=</span>/opt/apache-hive-2.3.2-bin\n<span class=\"nb\">export</span> <span class=\"nv\">PATH</span><span class=\"o\">=</span><span class=\"nv\">$PATH</span>:<span class=\"nv\">$HIVE_HOME</span>/bin\n<span class=\"nb\">source</span> /etc/profile</code></pre></div><h2>5、配置hive</h2><h2>5.1 配置hive-site.xml</h2><p>其中ConnectionUserName和ConnectionPassword为mysql远程访问的用户名和密码，hive_metadata为mysql数据库，随自己习惯命名。</p><div class=\"highlight\"><pre><code class=\"language-bash\"><span class=\"nb\">cd</span> /opt/apache-hive-2.3.2-bin/conf/\nvim hive-site.xml\n&lt;?xml <span class=\"nv\">version</span><span class=\"o\">=</span><span class=\"s2\">&#34;1.0&#34;</span> <span class=\"nv\">encoding</span><span class=\"o\">=</span><span class=\"s2\">&#34;UTF-8&#34;</span> <span class=\"nv\">standalone</span><span class=\"o\">=</span><span class=\"s2\">&#34;no&#34;</span>?&gt;\n&lt;?xml-stylesheet <span class=\"nv\">type</span><span class=\"o\">=</span><span class=\"s2\">&#34;text/xsl&#34;</span> <span class=\"nv\">href</span><span class=\"o\">=</span><span class=\"s2\">&#34;configuration.xsl&#34;</span>?&gt;\n&lt;configuration&gt;\n &lt;property&gt;\n    &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;\n    &lt;value&gt;jdbc:mysql://192.168.44.128:3306/hive_metadata?<span class=\"p\">&amp;</span>amp<span class=\"p\">;</span><span class=\"nv\">createDatabaseIfNotExist</span><span class=\"o\">=</span>true<span class=\"p\">&amp;</span>amp<span class=\"p\">;</span><span class=\"nv\">characterEncoding</span><span class=\"o\">=</span>UTF-8<span class=\"p\">&amp;</span>amp<span class=\"p\">;</span><span class=\"nv\">useSSL</span><span class=\"o\">=</span>false&lt;/value&gt;\n &lt;/property&gt;\n&lt;property&gt;\n    &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;\n    &lt;value&gt;root&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n    &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;\n    &lt;value&gt;Root-123456&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n    &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;\n    &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;\n&lt;/property&gt;\n&lt;property&gt;\n    &lt;name&gt;datanucleus.schema.autoCreateAll&lt;/name&gt;\n    &lt;value&gt;true&lt;/value&gt; &lt;/property&gt;\n&lt;property&gt;\n    &lt;name&gt;hive.metastore.schema.verification&lt;/name&gt;\n    &lt;value&gt;false&lt;/value&gt;\n &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre></div><h2>5.2 配置hive-site.xml</h2><div class=\"highlight\"><pre><code class=\"language-text\">cp hive-env.sh.template hive-env.sh\nvim hive-env.sh\nHADOOP_HOME=/opt/hadoop-2.7.5\nexport HIVE_CONF_DIR=/opt/apache-hive-2.3.2-bin/conf</code></pre></div><p>具体位置如图：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-4837152033754f8327c545d118f052b6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"795\" data-rawheight=\"151\" class=\"origin_image zh-lightbox-thumb\" width=\"795\" data-original=\"https://pic3.zhimg.com/v2-4837152033754f8327c545d118f052b6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;795&#39; height=&#39;151&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"795\" data-rawheight=\"151\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"795\" data-original=\"https://pic3.zhimg.com/v2-4837152033754f8327c545d118f052b6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-4837152033754f8327c545d118f052b6_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>6、加载mysql驱动（要与自己安装的mysql版本一致）</h2><p>下载地址：<a href=\"https://link.zhihu.com/?target=http%3A//dev.mysql.com/downloads/connector/j/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">http://dev.mysql.com/downloads/connector/j/ </a>我下载的是：mysql-connector-java-5.1.46.tar.gz，解压并将其中的mysql-connector-java-5.1.46-bin.jar放到hive/lib下<br/>具体路径为：/opt/apache-hive-2.3.2-bin/lib</p><h2>7、初始化数据库</h2><div class=\"highlight\"><pre><code class=\"language-bash\">schematool -initSchema -dbType mysql</code></pre></div><h2>8、启动hive</h2><p>启动hive之前先启动hadoop,不然会报Connection refused异常，在命令行jps看一下hadoop是否启动成功然后启动hive</p><div class=\"highlight\"><pre><code class=\"language-bash\">hive</code></pre></div><p>然后简单的测试:</p><div class=\"highlight\"><pre><code class=\"language-bash\">show databases<span class=\"p\">;</span></code></pre></div><p>出现如下图所示即代表配置成功！</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-6961500b4f94fdc8f5f861564a8f11cb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1262\" data-rawheight=\"249\" class=\"origin_image zh-lightbox-thumb\" width=\"1262\" data-original=\"https://pic4.zhimg.com/v2-6961500b4f94fdc8f5f861564a8f11cb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1262&#39; height=&#39;249&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1262\" data-rawheight=\"249\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1262\" data-original=\"https://pic4.zhimg.com/v2-6961500b4f94fdc8f5f861564a8f11cb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-6961500b4f94fdc8f5f861564a8f11cb_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>9、简单的hive语句测试</h2><p>建表：</p><div class=\"highlight\"><pre><code class=\"language-text\">CREATE TABLE IF NOT EXISTS test (id INT,name STRING)ROW FORMAT DELIMITED FIELDS TERMINATED BY &#34; &#34; LINES TERMINATED BY &#34;\\n&#34;;</code></pre></div><p>插入数据</p><div class=\"highlight\"><pre><code class=\"language-bash\">insert into <span class=\"nb\">test</span> values<span class=\"o\">(</span><span class=\"m\">1</span>,<span class=\"s1\">&#39;张三&#39;</span><span class=\"o\">)</span><span class=\"p\">;</span></code></pre></div><p>查询</p><div class=\"highlight\"><pre><code class=\"language-bash\"><span class=\"k\">select</span> * from test<span class=\"p\">;</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-0e1807dbfb24b63d658d3aea0e071653_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1254\" data-rawheight=\"488\" class=\"origin_image zh-lightbox-thumb\" width=\"1254\" data-original=\"https://pic4.zhimg.com/v2-0e1807dbfb24b63d658d3aea0e071653_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1254&#39; height=&#39;488&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1254\" data-rawheight=\"488\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1254\" data-original=\"https://pic4.zhimg.com/v2-0e1807dbfb24b63d658d3aea0e071653_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-0e1807dbfb24b63d658d3aea0e071653_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "Hadoop", 
                    "tagLink": "https://api.zhihu.com/topics/19563390"
                }, 
                {
                    "tag": "Hive", 
                    "tagLink": "https://api.zhihu.com/topics/19655283"
                }, 
                {
                    "tag": "CentOS", 
                    "tagLink": "https://api.zhihu.com/topics/19577255"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/39543999", 
            "userName": "董可伦", 
            "userLink": "https://www.zhihu.com/people/de888cc734ddbfd918269a8e5fb196ba", 
            "upvote": 0, 
            "title": "centos7 hadoop 单机模式安装配置", 
            "content": "<p>转载请务必注明原创地址为：<u><a href=\"https://link.zhihu.com/?target=http%3A//dongkelun.com/2018/03/23/hadoopConf/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">dongkelun.com/2018/03/2</span><span class=\"invisible\">3/hadoopConf/</span><span class=\"ellipsis\"></span></a></u></p><h2>前言</h2><p>由于现在要用spark,而学习spark会和hdfs和hive打交道，之前在公司服务器配的分布式集群，离开公司之后，自己就不能用了，后来用ambari搭的三台虚拟机的集群太卡了，所以就上网查了一下hadoop+hive的单机部署，以便自己能进行简单的学习，这里记录一下，本来想把hadoop和hive的放在一起写，由于太多，就分成两篇写了。</p><h2>1、首先安装配置jdk（我安装的1.8）</h2><h2>2、下载hadoop</h2><p>下载地址：<a href=\"https://link.zhihu.com/?target=http%3A//mirror.bit.edu.cn/apache/hadoop/common/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">mirror.bit.edu.cn/apach</span><span class=\"invisible\">e/hadoop/common/</span><span class=\"ellipsis\"></span></a>，我下载的是hadoop-2.7.5.tar.gz<br/>（由于我之前用的2.7.1是几年前下载保存在本地的，现在发现之前在配置spark那篇写的那个hadoop下载地址较慢，所以改成这个地址）</p><h2>3、解压到/opt目录下（目录根据自己习惯）</h2><div class=\"highlight\"><pre><code class=\"language-bash\">tar -zxvf hadoop-2.7.5.tar.gz  -C /opt/</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2>4、配置hadoop环境变量</h2><div class=\"highlight\"><pre><code class=\"language-bash\">vim /etc/profile\n<span class=\"nb\">export</span> <span class=\"nv\">HADOOP_HOME</span><span class=\"o\">=</span>/opt/hadoop-2.7.5\n<span class=\"nb\">export</span> <span class=\"nv\">PATH</span><span class=\"o\">=</span><span class=\"nv\">$PATH</span>:<span class=\"nv\">$HADOOP_HOME</span>/bin\n<span class=\"nb\">source</span> /etc/profile</code></pre></div><h2>5、配置hadoop</h2><h2>5.1 配置hadoop-env.sh</h2><div class=\"highlight\"><pre><code class=\"language-bash\">vim /opt/hadoop-2.7.5/etc/hadoop/hadoop-env.sh</code></pre></div><p>找到# The java implementation to use.将其下面的一行改为：</p><div class=\"highlight\"><pre><code class=\"language-bash\"><span class=\"nb\">export</span> <span class=\"nv\">JAVA_HOME</span><span class=\"o\">=</span>/opt/jdk1.8.0_45</code></pre></div><h2>5.2 配置core-site.xml (5.2和5.3中配置文件里的文件路径和端口随自己习惯配置)</h2><p>其中的IP:192.168.44.128为虚拟机ip,不能设置为localhost，如果用localhost,后面在windows上用saprk连接服务器（虚拟机）上的hive会报异常（win读取的配置也是localhost，这样localhost就为win本地ip了~也可以给ip加个映射，不过因为单机的我就没加）。</p><div class=\"highlight\"><pre><code class=\"language-text\">vim /opt/hadoop-2.7.5/etc/hadoop/core-site.xml\n&lt;configuration&gt;\n&lt;property&gt;\n        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;\n        &lt;value&gt;file:///opt/hadoop-2.7.5&lt;/value&gt;\n        &lt;description&gt;Abase for other temporary directories.&lt;/description&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;fs.defaultFS&lt;/name&gt;\n        &lt;value&gt;hdfs://192.168.44.128:8888&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre></div><h2>5.3 配置hdfs-site.xml</h2><div class=\"highlight\"><pre><code class=\"language-text\">vim /opt/hadoop-2.7.5/etc/hadoop/hdfs-site.xml\n&lt;configuration&gt;\n        &lt;property&gt;\n        &lt;name&gt;dfs.replication&lt;/name&gt;\n        &lt;value&gt;1&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;\n        &lt;value&gt;file:///opt/hadoop-2.7.5/tmp/dfs/name&lt;/value&gt;\n    &lt;/property&gt;\n    &lt;property&gt;\n        &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;\n        &lt;value&gt;file:///opt/hadoop-2.7.5/tmp/dfs/data&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;\n</code></pre></div><h2>6、SSH免密码登录</h2><p>参考：<a href=\"https://link.zhihu.com/?target=http%3A//dongkelun.com/2018/04/05/sshConf/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">linux ssh 免密登录</a></p><h2>7、启动与停止</h2><p>第一次启动hdfs需要格式化：</p><div class=\"highlight\"><pre><code class=\"language-bash\"><span class=\"nb\">cd</span> /opt/hadoop-2.7.5\n./bin/hdfs namenode -format</code></pre></div><p>Re-format filesystem in Storage Directory /opt/hadoop-2.7.5/tmp/dfs/name ? (Y or N) <br/>输入：Y<br/>（出现询问输入Y or N,全部输Y即可）<br/>启动：</p><div class=\"highlight\"><pre><code class=\"language-bash\">./sbin/start-dfs.sh</code></pre></div><p>停止：</p><div class=\"highlight\"><pre><code class=\"language-bash\">./sbin/stop-dfs.sh</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-5d08419ccdedbd035b0a5ab6b56ee792_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"905\" data-rawheight=\"90\" class=\"origin_image zh-lightbox-thumb\" width=\"905\" data-original=\"https://pic3.zhimg.com/v2-5d08419ccdedbd035b0a5ab6b56ee792_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;905&#39; height=&#39;90&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"905\" data-rawheight=\"90\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"905\" data-original=\"https://pic3.zhimg.com/v2-5d08419ccdedbd035b0a5ab6b56ee792_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-5d08419ccdedbd035b0a5ab6b56ee792_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>验证，浏览器输入：<a href=\"https://link.zhihu.com/?target=http%3A//192.168.44.128%3A50070\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">192.168.44.128:50070</span><span class=\"invisible\"></span></a></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ba8c38b7c875ef92925f7cbf7c14b3b8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1333\" data-rawheight=\"791\" class=\"origin_image zh-lightbox-thumb\" width=\"1333\" data-original=\"https://pic1.zhimg.com/v2-ba8c38b7c875ef92925f7cbf7c14b3b8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1333&#39; height=&#39;791&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1333\" data-rawheight=\"791\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1333\" data-original=\"https://pic1.zhimg.com/v2-ba8c38b7c875ef92925f7cbf7c14b3b8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-ba8c38b7c875ef92925f7cbf7c14b3b8_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>简单的验证hadoop命令：</p><div class=\"highlight\"><pre><code class=\"language-bash\">hadoop fs -mkdir /test</code></pre></div><p>在浏览器查看，出现如下图所示，即为成功</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-5ba0850100db7c652ee39880be6f1a4c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1558\" data-rawheight=\"486\" class=\"origin_image zh-lightbox-thumb\" width=\"1558\" data-original=\"https://pic1.zhimg.com/v2-5ba0850100db7c652ee39880be6f1a4c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1558&#39; height=&#39;486&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1558\" data-rawheight=\"486\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1558\" data-original=\"https://pic1.zhimg.com/v2-5ba0850100db7c652ee39880be6f1a4c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-5ba0850100db7c652ee39880be6f1a4c_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>8、配置yarn</h2><h2>8.1 配置mapred-site.xml</h2><div class=\"highlight\"><pre><code class=\"language-text\">cd /opt/hadoop-2.7.5/etc/hadoop/\ncp mapred-site.xml.template mapred-site.xml\nvim mapred-site.xml\n&lt;configuration&gt;\n    &lt;!-- 通知框架MR使用YARN --&gt;\n    &lt;property&gt;\n        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;\n        &lt;value&gt;yarn&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;</code></pre></div><h2>8.2 配置yarn-site.xml</h2><div class=\"highlight\"><pre><code class=\"language-bash\">vim yarn-site.xml\n&lt;configuration&gt;\n    &lt;!-- reducer取数据的方式是mapreduce_shuffle --&gt;\n    &lt;property&gt;\n        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;\n        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;\n    &lt;/property&gt;\n&lt;/configuration&gt;</code></pre></div><h2>8.3 yarn启动与停止</h2><p>启动：</p><div class=\"highlight\"><pre><code class=\"language-bash\"><span class=\"nb\">cd</span> /opt/hadoop-2.7.5\n./sbin/start-yarn.sh\n./sbin/stop-yarn.sh</code></pre></div><p>浏览器查看：<a href=\"https://link.zhihu.com/?target=http%3A//192.168.44.128%3A8088\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">192.168.44.128:8088</span><span class=\"invisible\"></span></a></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-c89ad9bfa046f1ec3d792f9d04f188df_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1904\" data-rawheight=\"490\" class=\"origin_image zh-lightbox-thumb\" width=\"1904\" data-original=\"https://pic4.zhimg.com/v2-c89ad9bfa046f1ec3d792f9d04f188df_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1904&#39; height=&#39;490&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1904\" data-rawheight=\"490\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1904\" data-original=\"https://pic4.zhimg.com/v2-c89ad9bfa046f1ec3d792f9d04f188df_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-c89ad9bfa046f1ec3d792f9d04f188df_b.jpg\"/></figure><p><br/>jps查看进程</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-bdb1372129af4590423de87b62caed6c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"471\" data-rawheight=\"122\" class=\"origin_image zh-lightbox-thumb\" width=\"471\" data-original=\"https://pic1.zhimg.com/v2-bdb1372129af4590423de87b62caed6c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;471&#39; height=&#39;122&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"471\" data-rawheight=\"122\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"471\" data-original=\"https://pic1.zhimg.com/v2-bdb1372129af4590423de87b62caed6c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-bdb1372129af4590423de87b62caed6c_b.jpg\"/></figure><p><br/>到此，hadoop单机模式就配置成功了！</p><h2>参考资料</h2><p><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/cafebar123/article/details/73500014\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">blog.csdn.net/cafebar12</span><span class=\"invisible\">3/article/details/73500014</span><span class=\"ellipsis\"></span></a></p>", 
            "topic": [
                {
                    "tag": "Hadoop", 
                    "tagLink": "https://api.zhihu.com/topics/19563390"
                }, 
                {
                    "tag": "CentOS", 
                    "tagLink": "https://api.zhihu.com/topics/19577255"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/36689338", 
            "userName": "董可伦", 
            "userLink": "https://www.zhihu.com/people/de888cc734ddbfd918269a8e5fb196ba", 
            "upvote": 2, 
            "title": "win10 spark+scala+eclipse+sbt 安装配置", 
            "content": "<p></p><h2>1、首先安装配置jdk1.8以上,建议全部的安装路径不要有空格</h2><p class=\"ztext-empty-paragraph\"><br/></p><h2>2、安装spark</h2><p class=\"ztext-empty-paragraph\"><br/></p><h2>2.1 下载</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>下载地址：<a href=\"https://link.zhihu.com/?target=http%3A//spark.apache.org/downloads.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">spark.apache.org/downlo</span><span class=\"invisible\">ads.html</span><span class=\"ellipsis\"></span></a>，我下载的是 spark-2.2.1-bin-hadoop2.7.tgz</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>2.2 安装</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>解压到指定路径下即可，比如 D:\\Company\\bigdata\\spark-2.2.1-bin-hadoop2.7</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>2.3 配置环境变量</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>在系统变量Path添加一条：D:\\Company\\bigdata\\spark-2.2.1-bin-hadoop2.7\\bin 即可<br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2>3、安装hadoop</h2><p class=\"ztext-empty-paragraph\"><br/></p><h2>3.1 下载</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>下载地址：<a href=\"https://link.zhihu.com/?target=https%3A//archive.apache.org/dist/hadoop/common/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">archive.apache.org/dist</span><span class=\"invisible\">/hadoop/common/</span><span class=\"ellipsis\"></span></a>(需要和spark对应的版本保持一致，我选择的hadoop-2.7.1.tar.gz)<br/>（此链接下载较慢，可选择其他镜像下载其他版本如：<a href=\"https://link.zhihu.com/?target=http%3A//mirror.bit.edu.cn/apache/hadoop/common/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">mirror.bit.edu.cn/apach</span><span class=\"invisible\">e/hadoop/common/</span><span class=\"ellipsis\"></span></a>）</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>3.2 安装</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>解压到指定路径下即可，比如 D:\\Company\\bigdata\\hadoop-2.7.1</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>3.3 配置环境变量</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>在系统变量里添加 HADOOP_HOME：D:\\Company\\bigdata\\hadoop-2.7.1</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>3.4 下载winutils.exe</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>1.下载地址：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/steveloughran/winutils\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/steveloughra</span><span class=\"invisible\">n/winutils</span><span class=\"ellipsis\"></span></a>（找到对应的版本下载）<br/>2. 将其复制到 %HADOOP_HOME% 即D:\\Company\\bigdata\\hadoop-2.7.1\\bin</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>3.5 解决/temp/hive 不可写错误</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>执行以下语句：D:\\Company\\bigdata\\hadoop-2.7.1\\bin\\winutils.exe chmod 777 /tmp/hive 即可，参考：<a href=\"https://link.zhihu.com/?target=http%3A//mangocool.com/1473838702533.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">mangocool.com/147383870</span><span class=\"invisible\">2533.html</span><span class=\"ellipsis\"></span></a></p><p class=\"ztext-empty-paragraph\"><br/></p><h2>3.6 运行验证spark</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>在命令行输入：spark-shell，出现如下图所示即为成功(其中warn信息已在日志配置文件里去掉)<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-e7ad89526fe802e1d8f8a2306bafacf6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1177\" data-rawheight=\"331\" class=\"origin_image zh-lightbox-thumb\" width=\"1177\" data-original=\"https://pic3.zhimg.com/v2-e7ad89526fe802e1d8f8a2306bafacf6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1177&#39; height=&#39;331&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1177\" data-rawheight=\"331\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1177\" data-original=\"https://pic3.zhimg.com/v2-e7ad89526fe802e1d8f8a2306bafacf6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-e7ad89526fe802e1d8f8a2306bafacf6_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2>4、安装对应版本的scala（scala-2.11.8.msi）</h2><p class=\"ztext-empty-paragraph\"><br/></p><h2>4.1 下载</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>下载地址：<a href=\"https://link.zhihu.com/?target=https%3A//www.scala-lang.org/download/all.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">scala-lang.org/download</span><span class=\"invisible\">/all.html</span><span class=\"ellipsis\"></span></a></p><p class=\"ztext-empty-paragraph\"><br/></p><h2>4.2 安装</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>一键式安装到指定目录：D:\\Company\\bigdata\\scala</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>4.3 配置环境变量</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>安装过程中已经自动配好</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>4.4 验证</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>输入scala -version 查看版本号 ，输入scala 进入scala的环境<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-3918e52416550cb7ec445019ef691a78_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"712\" data-rawheight=\"180\" class=\"origin_image zh-lightbox-thumb\" width=\"712\" data-original=\"https://pic1.zhimg.com/v2-3918e52416550cb7ec445019ef691a78_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;712&#39; height=&#39;180&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"712\" data-rawheight=\"180\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"712\" data-original=\"https://pic1.zhimg.com/v2-3918e52416550cb7ec445019ef691a78_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-3918e52416550cb7ec445019ef691a78_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2>5、在eclipse上安装scala插件</h2><p class=\"ztext-empty-paragraph\"><br/></p><h2>5.1安装</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>在Eclipse中选择Help-&gt;Install new Software<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-060227480fbc702e78e3b957c5a7d0cc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"880\" data-rawheight=\"731\" class=\"origin_image zh-lightbox-thumb\" width=\"880\" data-original=\"https://pic1.zhimg.com/v2-060227480fbc702e78e3b957c5a7d0cc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;880&#39; height=&#39;731&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"880\" data-rawheight=\"731\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"880\" data-original=\"https://pic1.zhimg.com/v2-060227480fbc702e78e3b957c5a7d0cc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-060227480fbc702e78e3b957c5a7d0cc_b.jpg\"/></figure><p><br/>等待一会儿：<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-221265ffef8a6eb4543541139e3b1c19_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"885\" data-rawheight=\"724\" class=\"origin_image zh-lightbox-thumb\" width=\"885\" data-original=\"https://pic2.zhimg.com/v2-221265ffef8a6eb4543541139e3b1c19_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;885&#39; height=&#39;724&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"885\" data-rawheight=\"724\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"885\" data-original=\"https://pic2.zhimg.com/v2-221265ffef8a6eb4543541139e3b1c19_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-221265ffef8a6eb4543541139e3b1c19_b.jpg\"/></figure><p><br/>然后下一步下一步<br/>中间有一个警告，点ok即可，最后根据提示重启eclipse即可安装完成</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>5.1运行scala程序</h2><p class=\"ztext-empty-paragraph\"><br/></p><h2>5.1.1 新建scala project</h2><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-f1c84132ba9d2e4a2790ecfa1a117bc5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"545\" data-rawheight=\"756\" class=\"origin_image zh-lightbox-thumb\" width=\"545\" data-original=\"https://pic2.zhimg.com/v2-f1c84132ba9d2e4a2790ecfa1a117bc5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;545&#39; height=&#39;756&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"545\" data-rawheight=\"756\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"545\" data-original=\"https://pic2.zhimg.com/v2-f1c84132ba9d2e4a2790ecfa1a117bc5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-f1c84132ba9d2e4a2790ecfa1a117bc5_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-7dc74a55442cec52b014b6c9f40a8702_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"534\" data-rawheight=\"758\" class=\"origin_image zh-lightbox-thumb\" width=\"534\" data-original=\"https://pic3.zhimg.com/v2-7dc74a55442cec52b014b6c9f40a8702_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;534&#39; height=&#39;758&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"534\" data-rawheight=\"758\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"534\" data-original=\"https://pic3.zhimg.com/v2-7dc74a55442cec52b014b6c9f40a8702_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-7dc74a55442cec52b014b6c9f40a8702_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2>5.1.2 将默认的sacala版本改为之前安装的版本</h2><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-bac13ffcfa58fbed8e9730c0cb85ae8c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1799\" data-rawheight=\"817\" class=\"origin_image zh-lightbox-thumb\" width=\"1799\" data-original=\"https://pic1.zhimg.com/v2-bac13ffcfa58fbed8e9730c0cb85ae8c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1799&#39; height=&#39;817&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1799\" data-rawheight=\"817\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1799\" data-original=\"https://pic1.zhimg.com/v2-bac13ffcfa58fbed8e9730c0cb85ae8c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-bac13ffcfa58fbed8e9730c0cb85ae8c_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2>5.1.3 编写salca程序，即可像运行java一样运行scala</h2><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-390854ac691167f4583e9a960cdaad41_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1277\" data-rawheight=\"633\" class=\"origin_image zh-lightbox-thumb\" width=\"1277\" data-original=\"https://pic2.zhimg.com/v2-390854ac691167f4583e9a960cdaad41_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1277&#39; height=&#39;633&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1277\" data-rawheight=\"633\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1277\" data-original=\"https://pic2.zhimg.com/v2-390854ac691167f4583e9a960cdaad41_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-390854ac691167f4583e9a960cdaad41_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-abfb264affa90443be34c1dd2f0c910c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1203\" data-rawheight=\"742\" class=\"origin_image zh-lightbox-thumb\" width=\"1203\" data-original=\"https://pic1.zhimg.com/v2-abfb264affa90443be34c1dd2f0c910c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1203&#39; height=&#39;742&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1203\" data-rawheight=\"742\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1203\" data-original=\"https://pic1.zhimg.com/v2-abfb264affa90443be34c1dd2f0c910c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-abfb264affa90443be34c1dd2f0c910c_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2>6、安装sbt</h2><p class=\"ztext-empty-paragraph\"><br/></p><h2>6.1 下载（sbt-1.1.1.msi）</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>下载地址：<a href=\"https://link.zhihu.com/?target=https%3A//www.scala-sbt.org/download.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">scala-sbt.org/download.</span><span class=\"invisible\">html</span><span class=\"ellipsis\"></span></a></p><p class=\"ztext-empty-paragraph\"><br/></p><h2>6.2 安装</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>一键式安装到指定目录：D:\\Company\\bigdata\\scala-sbt</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>6.3 配置环境变量</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>SBT_HOME=D:\\Company\\bigdata\\scala-sbt<br/>path=%SBT_HOME%\\bin</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>6.3 配置本地仓库</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>编辑：conf/sbtconfig.txt</p><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-text\"># Set the java args to high\n\n-Xmx512M\n\n-XX:MaxPermSize=256m\n\n-XX:ReservedCodeCacheSize=128m\n\n\n\n# Set the extra SBT options\n\n-Dsbt.log.format=true\n-Dsbt.boot.directory=D:/Company/bigdata/scala-sbt/boot/\n-Dsbt.global.base=D:/Company/bigdata/scala-sbt/.sbt\n-Dsbt.ivy.home=D:/Company/bigdata/scala-sbt/.ivy2\n-Dsbt.repository.config=D:/Company/bigdata/scala-sbt/conf/repo.properties\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>增加文件 conf/repo.properties</p><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-text\">[repositories]  \nlocal\nNexus osc : https://code.lds.org/nexus/content/groups/main-repo\nNexus osc thirdparty : https://code.lds.org/nexus/content/groups/plugin-repo/\ntypesafe: http://repo.typesafe.com/typesafe/ivy-releases/, [organization]/[module]/(scala_[scalaVersion]/)(sbt_[sbtVersion]/)[revision]/[type]s/[artifact](-[classifier]).[ext], bootOnly  \ntypesafe2: http://repo.typesafe.com/typesafe/releases/\nsbt-plugin: http://repo.scala-sbt.org/scalasbt/sbt-plugin-releases/\nsonatype: http://oss.sonatype.org/content/repositories/snapshots  \nuk_maven: http://uk.maven.org/maven2/  \nibibli: http://mirrors.ibiblio.org/maven2/  \nrepo2: http://repo2.maven.org/maven2/\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2>6.4  验证</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>输入：sbt<br/>（第一次使用会下载复制一些文件）</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>7、安装eclipse的sbt插件：sbteclipse</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>sbteclipse是eclipse的sbt插件，但与一般eclipse插件的配置及使用并不相同。<br/>sbteclipse项目源码托管在github上：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/typesafehub/sbteclipse\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/typesafehub/</span><span class=\"invisible\">sbteclipse</span><span class=\"ellipsis\"></span></a></p><p>(7.1和7.2不确定是否是必须的,一台机器不需要，另一台因在~/.sbt文件下没有1.0和0.13文件夹，执行这两步即可)</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>7.1 下载项目</h2><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-bash\">git clone https://github.com/sbt/sbteclipse.git\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>或下载zip再解压</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>7.2 编译</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>进入到sbteclipse目录下，输入 </p><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-bash\">sbt compile\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2>7.3 添加全局配置文件</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>新建：~/.sbt/1.0/plugins/plugins.sbt（网上好多说是：~/.sbt/0.13/plugins/plugins.sbt，但我两个电脑都不行）</p><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-text\">addSbtPlugin(&#34;com.typesafe.sbteclipse&#34; % &#34;sbteclipse-plugin&#34; % &#34;5.2.4&#34;)\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2>7.4 进入到之前创建的项目ScalaDemo目录下</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>添加sbt配置文件build.sbt</p><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-text\">name := &#34;ScalaDemo&#34;\n\nversion := &#34;1.0&#34;\n\nscalaVersion := &#34;2.11.8&#34;\n\njavacOptions ++= Seq(&#34;-source&#34;, &#34;1.8&#34;, &#34;-target&#34;, &#34;1.8&#34;)\n\nlibraryDependencies ++= Seq(\n&#34;org.apache.spark&#34; %% &#34;spark-core&#34; % &#34;2.2.1&#34;\n\n)\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>输入 sbt 然后输入eclipse 等待相关的依赖下载完，就可以在eclipse 看到依赖的jar了</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-7c310777193e1f5ca2d2b84723e1cb98_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1381\" data-rawheight=\"663\" class=\"origin_image zh-lightbox-thumb\" width=\"1381\" data-original=\"https://pic1.zhimg.com/v2-7c310777193e1f5ca2d2b84723e1cb98_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1381&#39; height=&#39;663&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1381\" data-rawheight=\"663\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1381\" data-original=\"https://pic1.zhimg.com/v2-7c310777193e1f5ca2d2b84723e1cb98_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-7c310777193e1f5ca2d2b84723e1cb98_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2>7.5 最后将src bulid path 一下，就可以在scala代码里导入spark包了</h2><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-8da52a095018b9e18649764df38a72f4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1166\" data-rawheight=\"542\" class=\"origin_image zh-lightbox-thumb\" width=\"1166\" data-original=\"https://pic1.zhimg.com/v2-8da52a095018b9e18649764df38a72f4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1166&#39; height=&#39;542&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1166\" data-rawheight=\"542\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1166\" data-original=\"https://pic1.zhimg.com/v2-8da52a095018b9e18649764df38a72f4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-8da52a095018b9e18649764df38a72f4_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-7952a60d759dfb503671549061d474d3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1179\" data-rawheight=\"204\" class=\"origin_image zh-lightbox-thumb\" width=\"1179\" data-original=\"https://pic4.zhimg.com/v2-7952a60d759dfb503671549061d474d3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1179&#39; height=&#39;204&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1179\" data-rawheight=\"204\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1179\" data-original=\"https://pic4.zhimg.com/v2-7952a60d759dfb503671549061d474d3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-7952a60d759dfb503671549061d474d3_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2>8、 如果想调用本地spark，在SparkConf或者在SparkSession设置matser为local（本地模式）即可</h2>", 
            "topic": [
                {
                    "tag": "Scala", 
                    "tagLink": "https://api.zhihu.com/topics/19566465"
                }, 
                {
                    "tag": "Spark", 
                    "tagLink": "https://api.zhihu.com/topics/19942170"
                }, 
                {
                    "tag": "Eclipse", 
                    "tagLink": "https://api.zhihu.com/topics/19596718"
                }
            ], 
            "comments": []
        }
    ], 
    "url": "https://zhuanlan.zhihu.com/dongkelun"
}
