{
    "title": "深度学习，谈点儿实在的", 
    "description": "深度学习填坑历程", 
    "followers": [
        "https://www.zhihu.com/people/xun-gu-13", 
        "https://www.zhihu.com/people/zhou-jia-yong-51", 
        "https://www.zhihu.com/people/dufime-liu-9", 
        "https://www.zhihu.com/people/faken93", 
        "https://www.zhihu.com/people/innerpeace-24-25-99", 
        "https://www.zhihu.com/people/bsichrb", 
        "https://www.zhihu.com/people/stephen_chen", 
        "https://www.zhihu.com/people/lu-kai-14-46", 
        "https://www.zhihu.com/people/comeon-cpl", 
        "https://www.zhihu.com/people/tsingcoo", 
        "https://www.zhihu.com/people/lxgend", 
        "https://www.zhihu.com/people/shi-xin-zhe-65", 
        "https://www.zhihu.com/people/thedataman", 
        "https://www.zhihu.com/people/wei-xiao-kui-53", 
        "https://www.zhihu.com/people/ha-ha-8-79-49", 
        "https://www.zhihu.com/people/cai-jin-chao", 
        "https://www.zhihu.com/people/mo-da-la-58", 
        "https://www.zhihu.com/people/yifdu", 
        "https://www.zhihu.com/people/deng-xiao-bing-36", 
        "https://www.zhihu.com/people/zhu-forrest", 
        "https://www.zhihu.com/people/zhang-ming-hao-41", 
        "https://www.zhihu.com/people/gogo-30-43", 
        "https://www.zhihu.com/people/18514203274", 
        "https://www.zhihu.com/people/harbor-liu-hai-bo", 
        "https://www.zhihu.com/people/wu-ya-95-96", 
        "https://www.zhihu.com/people/li-hong-67-14", 
        "https://www.zhihu.com/people/xie-wen-53-59", 
        "https://www.zhihu.com/people/liu-yi-25-9", 
        "https://www.zhihu.com/people/dgjk1010", 
        "https://www.zhihu.com/people/freedom_forever", 
        "https://www.zhihu.com/people/cr777-37", 
        "https://www.zhihu.com/people/gu-dian-pai-liu-mang", 
        "https://www.zhihu.com/people/zhang-liang-34-4", 
        "https://www.zhihu.com/people/tao-ming-87", 
        "https://www.zhihu.com/people/mao-he-lao-shu-jia", 
        "https://www.zhihu.com/people/wfqd", 
        "https://www.zhihu.com/people/yzsun0805", 
        "https://www.zhihu.com/people/wang-bo-78-16-9", 
        "https://www.zhihu.com/people/chen-bo-86-42", 
        "https://www.zhihu.com/people/yu-hai-long-22", 
        "https://www.zhihu.com/people/shinysky", 
        "https://www.zhihu.com/people/zz-ss-68", 
        "https://www.zhihu.com/people/xiao-yu-22-71-20", 
        "https://www.zhihu.com/people/luo-xie-wu-you-jt", 
        "https://www.zhihu.com/people/dreamer-1-81", 
        "https://www.zhihu.com/people/niceworld-49-6", 
        "https://www.zhihu.com/people/yang-zhi-gang-70-1", 
        "https://www.zhihu.com/people/yang-hong-xiao-67", 
        "https://www.zhihu.com/people/cabbage-71", 
        "https://www.zhihu.com/people/zhang-xiao-hui-66-53", 
        "https://www.zhihu.com/people/jiang-wei-42-52", 
        "https://www.zhihu.com/people/wang-yu-3-82", 
        "https://www.zhihu.com/people/wen-xian-sheng-75-39", 
        "https://www.zhihu.com/people/yixi-xi-yang-taurus", 
        "https://www.zhihu.com/people/li-zhong-li-li", 
        "https://www.zhihu.com/people/cherry-zou-37", 
        "https://www.zhihu.com/people/dou-dou-jun-64-36", 
        "https://www.zhihu.com/people/IsingZhang", 
        "https://www.zhihu.com/people/surmount1", 
        "https://www.zhihu.com/people/xu-si-rui-63", 
        "https://www.zhihu.com/people/allen-91-56", 
        "https://www.zhihu.com/people/whsoon", 
        "https://www.zhihu.com/people/DAVIE", 
        "https://www.zhihu.com/people/aiedward-79", 
        "https://www.zhihu.com/people/scottdc-31", 
        "https://www.zhihu.com/people/li-shuang-jiang-17", 
        "https://www.zhihu.com/people/arisosoftware", 
        "https://www.zhihu.com/people/wangjun-55-67", 
        "https://www.zhihu.com/people/pon-22", 
        "https://www.zhihu.com/people/tan-qing-yu-78", 
        "https://www.zhihu.com/people/astronstar", 
        "https://www.zhihu.com/people/diem1987", 
        "https://www.zhihu.com/people/sunshinejack-94", 
        "https://www.zhihu.com/people/zhang-pan-42-25", 
        "https://www.zhihu.com/people/zhu-zi-62-76", 
        "https://www.zhihu.com/people/xiao-shan-94-8", 
        "https://www.zhihu.com/people/lin-hao-xing-57", 
        "https://www.zhihu.com/people/-.-Luffy", 
        "https://www.zhihu.com/people/as-trz", 
        "https://www.zhihu.com/people/sha-ding-yu-8-13", 
        "https://www.zhihu.com/people/strive-55-16", 
        "https://www.zhihu.com/people/qiu-xiao-hu-72", 
        "https://www.zhihu.com/people/kai-xin-jiu-hao-35-42", 
        "https://www.zhihu.com/people/yang-chao-4-50", 
        "https://www.zhihu.com/people/huaze-li", 
        "https://www.zhihu.com/people/bruce-wayne-60", 
        "https://www.zhihu.com/people/chen-lin-64-18-11", 
        "https://www.zhihu.com/people/babao123", 
        "https://www.zhihu.com/people/helixg", 
        "https://www.zhihu.com/people/zai-lu-shang-12-95-74", 
        "https://www.zhihu.com/people/pythonchina", 
        "https://www.zhihu.com/people/devxia", 
        "https://www.zhihu.com/people/cheng-yi-ying", 
        "https://www.zhihu.com/people/qia-bi-xiong-28", 
        "https://www.zhihu.com/people/zhang-mian-66", 
        "https://www.zhihu.com/people/a-piece-of-bread", 
        "https://www.zhihu.com/people/sha-bu-38", 
        "https://www.zhihu.com/people/chen-yi-rong-69-72", 
        "https://www.zhihu.com/people/mu-xi-luo-chen", 
        "https://www.zhihu.com/people/harric", 
        "https://www.zhihu.com/people/wang-xiao-xin-63-62", 
        "https://www.zhihu.com/people/nian-yu-74-83", 
        "https://www.zhihu.com/people/cui-zhen-wei-86", 
        "https://www.zhihu.com/people/hao-xuan-58-70", 
        "https://www.zhihu.com/people/lan-hua-cao-6-22", 
        "https://www.zhihu.com/people/jinshenhehuan", 
        "https://www.zhihu.com/people/nan-fang-you-gu-niang-71", 
        "https://www.zhihu.com/people/zhao-fu-bang-95", 
        "https://www.zhihu.com/people/zhong-er-cheng-xu-yuan", 
        "https://www.zhihu.com/people/xiang-huang-87", 
        "https://www.zhihu.com/people/Ramos4zxj", 
        "https://www.zhihu.com/people/tengfei-57", 
        "https://www.zhihu.com/people/wang-guo-yin-93", 
        "https://www.zhihu.com/people/chen-zhen-hao-19", 
        "https://www.zhihu.com/people/la-geek", 
        "https://www.zhihu.com/people/zzx-47-7", 
        "https://www.zhihu.com/people/libin-sui", 
        "https://www.zhihu.com/people/da-da-18-6-40", 
        "https://www.zhihu.com/people/xiangguangyan", 
        "https://www.zhihu.com/people/zhang-yin-10", 
        "https://www.zhihu.com/people/ni-hui-51-23", 
        "https://www.zhihu.com/people/he-en-dong", 
        "https://www.zhihu.com/people/axibulifu", 
        "https://www.zhihu.com/people/zhang-da-xian-1973", 
        "https://www.zhihu.com/people/zhang-ji-peng-42", 
        "https://www.zhihu.com/people/lucky-45-9-77", 
        "https://www.zhihu.com/people/yildhd-wang", 
        "https://www.zhihu.com/people/cylde-wei-lai-shi-jie", 
        "https://www.zhihu.com/people/huang-pei-song-20", 
        "https://www.zhihu.com/people/song-xiao-yang-56", 
        "https://www.zhihu.com/people/settinghead", 
        "https://www.zhihu.com/people/yyuqing6161", 
        "https://www.zhihu.com/people/YelvZi", 
        "https://www.zhihu.com/people/wzz-74-1", 
        "https://www.zhihu.com/people/chen-xiang-qing-47", 
        "https://www.zhihu.com/people/metfzh", 
        "https://www.zhihu.com/people/tigerbaby-71", 
        "https://www.zhihu.com/people/sundy-sun", 
        "https://www.zhihu.com/people/wang-tian-yuan-77", 
        "https://www.zhihu.com/people/xu-yin-da-58", 
        "https://www.zhihu.com/people/michael-wu-25", 
        "https://www.zhihu.com/people/jin-se-liang-dian-ban", 
        "https://www.zhihu.com/people/she-liang", 
        "https://www.zhihu.com/people/hong-alex", 
        "https://www.zhihu.com/people/wei-yuan-88-25", 
        "https://www.zhihu.com/people/tang-bai-mo", 
        "https://www.zhihu.com/people/wang-xin-60-36", 
        "https://www.zhihu.com/people/whywhy-80-9", 
        "https://www.zhihu.com/people/joyce-58-73-53", 
        "https://www.zhihu.com/people/wangzhanye", 
        "https://www.zhihu.com/people/yeu-yang", 
        "https://www.zhihu.com/people/Nick-Alam", 
        "https://www.zhihu.com/people/wangcheny91", 
        "https://www.zhihu.com/people/wang-xian-long-49", 
        "https://www.zhihu.com/people/ni-ming-83-22", 
        "https://www.zhihu.com/people/diwuming", 
        "https://www.zhihu.com/people/nu-li-nu-li-er", 
        "https://www.zhihu.com/people/ta-you-38", 
        "https://www.zhihu.com/people/gpfvic", 
        "https://www.zhihu.com/people/ping-zi-kuai-le-de", 
        "https://www.zhihu.com/people/afeizai", 
        "https://www.zhihu.com/people/lu-jie-10-70", 
        "https://www.zhihu.com/people/zhang-jia-xin-1203", 
        "https://www.zhihu.com/people/hong-yu-58-71", 
        "https://www.zhihu.com/people/hijackjave", 
        "https://www.zhihu.com/people/zhang-chao-62-57", 
        "https://www.zhihu.com/people/xiao-wang-20-88", 
        "https://www.zhihu.com/people/chen-xiao-ting-80-19", 
        "https://www.zhihu.com/people/cst-6-69", 
        "https://www.zhihu.com/people/ding-bu-fan-18", 
        "https://www.zhihu.com/people/duan-xing-99", 
        "https://www.zhihu.com/people/zhang-ha-ha-29-22", 
        "https://www.zhihu.com/people/li-xue-feng-62", 
        "https://www.zhihu.com/people/ling-ya-juan-67", 
        "https://www.zhihu.com/people/dai-wei-66-30", 
        "https://www.zhihu.com/people/doghu", 
        "https://www.zhihu.com/people/lei-xiao-yu-49", 
        "https://www.zhihu.com/people/tinker-34-37", 
        "https://www.zhihu.com/people/zhang-ting-kai-1", 
        "https://www.zhihu.com/people/canwhut", 
        "https://www.zhihu.com/people/mata-fu", 
        "https://www.zhihu.com/people/pursuit330", 
        "https://www.zhihu.com/people/fenggege", 
        "https://www.zhihu.com/people/moni-gg", 
        "https://www.zhihu.com/people/chen-chun-sheng-39-24", 
        "https://www.zhihu.com/people/cauivy", 
        "https://www.zhihu.com/people/wang-wen-hao-33-75", 
        "https://www.zhihu.com/people/larrick", 
        "https://www.zhihu.com/people/lll-40-85", 
        "https://www.zhihu.com/people/da-fa-yin", 
        "https://www.zhihu.com/people/jiang-dao-dao-72", 
        "https://www.zhihu.com/people/deng-ming-37", 
        "https://www.zhihu.com/people/tian-lan-se-26", 
        "https://www.zhihu.com/people/spirit-22-32", 
        "https://www.zhihu.com/people/xiao-ping-gai-74"
    ], 
    "article": [
        {
            "url": "https://zhuanlan.zhihu.com/p/76936855", 
            "userName": "右左瓜子", 
            "userLink": "https://www.zhihu.com/people/6107fd0900473aae4e5ec6824b4fde4a", 
            "upvote": 5, 
            "title": "机器翻译中训练和推断之间间隔的连接方法", 
            "content": "<p>译注：本文翻译自Bridging the Gapbetween Training and Inference for Neural Machine Translation，微信团队的这篇论文获得了ACL最佳长篇奖，翻译的任务大多数NLPer都不会遇到，毕竟不是每个厂都要做自己的翻译，但是文本生成模型在摘要提取，自动生成标题等领域又比较广泛的使用，在此之前大多数模型对于训练和推理之间的间断要不不处理，要不取一个简单的概率来处理，这篇文章就很系统的论述了如何随着模型的训练选择适当的概率来选择真实数据和模型推理数据，以及模型推理数据选择的方法，希望对做相关领域的同学有所帮助。当然，水平有限，翻译不准确的地方海涵。</p><hr/><h2>Bridging the Gapbetween Training and Inference for Neural Machine Translation</h2><h2>机器翻译中训练和推断之间间隔的连接方法</h2><h3>摘要</h3><p>机器翻译预测目标词汇的时候依赖于它的上下文。在训练的时候，模型会使用训练集中的真实词语作为上下文，而在推断的时候，模型必须在没有上下文的情况下生成整个序列。这种上下文的差异会导致错误一直积累。除此之外，词级别的训练需要生成序列和真实序列很严格的匹配，这会导致合理的但是跟真真实序列不严格匹配的结果被修正。在本文中，我们通过在训练时同时从真实序列和预测序列中采样来解决这个问题，其中预测序列通过选择句子级的最优结果来得到。在中英翻译和英德翻译任务上得到的结果表明我们的方法对多种数据集都会有很大的提升。</p><h3>1. 引言</h3><p>近几年，基于神经网络的机器翻译（NMT）备受关注并且取得了可喜的成果。大多数NMT模型，包括基于RNN的，基于CNN的以及基于注意力机制的模型，使用<b>编码-解码</b>网络，这些模型基于上文词汇，预测接下来的词汇，通过目标词汇建立语言模型。这种情况发生在训练的时候，真实的词汇被用来作为上下文，而在推断的时候，整个序列都是通过模型来生成的，也就是说前一个词汇也是通过模型产生的。这样就导致了训练和推断是在不同的分布上进行的。这种被称为<b>exposure bias</b>的差异导致训练和推断之间存在间隔。随着目标序列的增长，这些误差会一直累积，而模型必须去预测在训练时从没遇到过的上下文。<br/> 直觉上，为了解决这种问题，模型应该在训练时去考虑推断时可能会发生的情况，然后进行预测。受到 <b>DATA AS DEMONSTRATOR(DAD)</b> 的启发，在训练时，把真实词汇和模型预测词汇同时作为上下文输入可能会是一个解决办法。NMT模型通常优化交叉熵损失函数，这种损失函数需要真实数据和预测数据在词级别上严格的匹配，一旦模型生成了偏离真实数据的词汇，交叉熵损失函数会立马纠正这个错误，并且把接下来生成的词重新纠正到正真实的序列上。然而，这种方法会导致新的问题。一个句子通常会有多个合理的翻译，并且就算模型生成了一个不同于真实词汇的词汇也不能说模型出错了，比如：  </p><blockquote> reference: We should comply with the rule.<br/> cand1: We should abide with the rule.<br/> cand2: We should abide by the law.<br/> cand3: We shoud abide by the rule  <br/> </blockquote><p>一旦模型生成了词&#34;abide&#34;作为第三个目标词汇，交叉熵损失函数就会强制模型去生成“with”作为第四个词汇（如cand1）来产生更大的句子级别的似然值并且和真实数据的参考值保持一致，尽管“by”才是正确的选择。然后词“with”将会作为输入成为新的上下文使模型产生完全错误的词“the rule”。cand1的翻译被看作是“过矫正”现象。另一个潜在的错误是就算模型在“abide”之后正确的预测了下一个词“by”，当生成子序列的时候，模型可能生成因为输入“by”而生成不合适的词“the law”（如cand2）。假设参考值和训练的标准数据让模型记忆了短语“the rule”的模式，总是跟在“with”的后面，来帮助模型从两种错误之中复原，产生正确的翻译如cand3所示，我们应该把“with”作为上下文而不是“by”尽管上一个预测短语是“abide by”，我们把这种解决方案成为“过矫正复原”（OR）。<br/> 在本文中，我们呈现了一个方法来链接训练和推断之间的间隔，以此提高NMT的过矫正复原能力。我们的方法首先会从模型的预测词汇中选择一些“oracle”词，然后从这些“oracle”词汇和真是词汇中采样作为上下文。与此同时，oracle词的选择不仅通过一个词接一个词的贪婪搜索得到，也会做句子级别的评估，即<b>BLEU</b>，一种在交叉熵中严格成对匹配的情况下仍然有比较好的灵活性的评估方法。在训练开始时，模型会很大的概率会选择真实数据做为上下文，随着模型逐渐收敛，oracle词则会更频繁的被选中。通过这种方式，训练进程从完全的指导方式逐渐变为奇案指导方式。在这种机制下，模型有机会去学习处理推断时的错误，同时又从过矫正中恢复的能力。我们在RNN搜索模型和更强大的transformer模型中验证了我们的方案。结果显示我们的方案可以显著的提升两种模型的表现。</p><h3>2. 基于RNN的模型</h3><p>我们的方法可以应用在各种NMT模型中。不失一般性，我们使用RNN-based NMT(Bahdanau et al., 2015)作为例子来介绍我们的方法。假设源序列和观察到的翻译序列分别为 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7Bx%7D%3D%7Bx_1%2C...%2Cx_%7B%5Cmathbf%7B%7Cx%7C%7D%7D%7D%2C%5Cmathbf%7By%7D%5E%2A+%3D+%7By_1%5E%2C...%2Cy_%7B%7C%5Cmathbf%7By%7D%5E%2A%7C%7D%7D\" alt=\"\\mathbf{x}={x_1,...,x_{\\mathbf{|x|}}},\\mathbf{y}^* = {y_1^,...,y_{|\\mathbf{y}^*|}}\" eeimg=\"1\"/> <br/> <b>编码器：</b> 一个双向门控循环神经网络单元(GRU)被用来获取两个隐藏序列，对于 <img src=\"https://www.zhihu.com/equation?tex=x_i\" alt=\"x_i\" eeimg=\"1\"/> 对应的序列标注为 <img src=\"https://www.zhihu.com/equation?tex=h_i%3D%5Bh_i%5E+%5Crightarrow%3B+h_i%5E%5Cleftarrow%5D\" alt=\"h_i=[h_i^ \\rightarrow; h_i^\\leftarrow]\" eeimg=\"1\"/> ，我们用 <img src=\"https://www.zhihu.com/equation?tex=e_%7Bx_i%7D\" alt=\"e_{x_i}\" eeimg=\"1\"/> 来代表词 <img src=\"https://www.zhihu.com/equation?tex=x_i\" alt=\"x_i\" eeimg=\"1\"/> 的嵌入向量。 <img src=\"https://www.zhihu.com/equation?tex=h_i%5E%5Crightarrow%3D%5Cmathrm%7BGRU%7D%28e_%7Bx_i%7D%2Ch_%7Bi-1%7D%5E%5Crightarrow%29+%5Ctag%7B1%7D\" alt=\"h_i^\\rightarrow=\\mathrm{GRU}(e_{x_i},h_{i-1}^\\rightarrow) \\tag{1}\" eeimg=\"1\"/> <img src=\"https://www.zhihu.com/equation?tex=h_i%5E%5Cleftarrow%3D%5Cmathrm%7BGRU%7D%28e_%7Bx_i%7D%2Ch_%7Bi%2B1%7D%5E%5Cleftarrow%29+%5Ctag%7B2%7D\" alt=\"h_i^\\leftarrow=\\mathrm{GRU}(e_{x_i},h_{i+1}^\\leftarrow) \\tag{2}\" eeimg=\"1\"/> <b>注意力机制：</b> 注意力用来提取源信息（也叫做源上下文向量，source context vector）。在第 <img src=\"https://www.zhihu.com/equation?tex=j\" alt=\"j\" eeimg=\"1\"/> 步，目标词汇 <img src=\"https://www.zhihu.com/equation?tex=y_j%5E%2A\" alt=\"y_j^*\" eeimg=\"1\"/> 和第 <img src=\"https://www.zhihu.com/equation?tex=i\" alt=\"i\" eeimg=\"1\"/> 个源词汇之间的关联是通过源序列来评估和正则化的 <img src=\"https://www.zhihu.com/equation?tex=r_%7Bij%7D%3D%5Cmathrm%7Bv%7Da%5ET%5Ctanh%28%5Cmathrm%7BW%7D%7Ba%5E%7Bs_j-1%7D%7D%2B%5Cmathrm%7BU%7Dah_i%29%5Ctag%7B3%7D\" alt=\"r_{ij}=\\mathrm{v}a^T\\tanh(\\mathrm{W}{a^{s_j-1}}+\\mathrm{U}ah_i)\\tag{3}\" eeimg=\"1\"/> <img src=\"https://www.zhihu.com/equation?tex=%5Calpha%7Bij%7D%3D%5Cfrac%7B%5Cexp%28r_%7Bij%7D%29%7D%7B%5Csum_%7Bi%5E%7B%27%7D%7D%5E%7B%7C%5Cmathrm%7Bx%7D%7C%7D%5Cexp%28r_%7Bi%5E%7B%27%7Dj%7D%29%7D%5Ctag%7B4%7D\" alt=\"\\alpha{ij}=\\frac{\\exp(r_{ij})}{\\sum_{i^{&#39;}}^{|\\mathrm{x}|}\\exp(r_{i^{&#39;}j})}\\tag{4}\" eeimg=\"1\"/> 源上下文向量是所有源标注的加权求和，使用如下方式计算： <img src=\"https://www.zhihu.com/equation?tex=c_j%3D%5Csum_%7Bi%3D1%7D%5E%7B%7C%5Cmathrm%7Bx%7D%7C%7Da_%7Bij%7Dh_i+%5Ctag%7B5%7D\" alt=\"c_j=\\sum_{i=1}^{|\\mathrm{x}|}a_{ij}h_i \\tag{5}\" eeimg=\"1\"/> (译注：注意力机制部分标记详细含义可以看Bahdanau et al., 2015这篇文章)<br/> <b>解码器：</b> 解码器采用了GRU的一个变种来展开目标信息。在第 <img src=\"https://www.zhihu.com/equation?tex=j\" alt=\"j\" eeimg=\"1\"/> 步，目标隐状态 <img src=\"https://www.zhihu.com/equation?tex=s_j\" alt=\"s_j\" eeimg=\"1\"/> 由如下方式得出： <img src=\"https://www.zhihu.com/equation?tex=s_j%3D%5Cmathrm%7BGRU%7D%28e_%7By_%7Bj-1%7D%5E%2A%7D%2Cs_%7Bj-1%7D%2Cc_j%29%5Ctag%7B6%7D\" alt=\"s_j=\\mathrm{GRU}(e_{y_{j-1}^*},s_{j-1},c_j)\\tag{6}\" eeimg=\"1\"/> 目标词汇的的概率分布 <img src=\"https://www.zhihu.com/equation?tex=P_j\" alt=\"P_j\" eeimg=\"1\"/> 基于上一步的真实数据中的词汇，源上下文向量和隐状态给出： <img src=\"https://www.zhihu.com/equation?tex=t_j%3Dg%28e_%7By_%7Bj-1%7D%5E%2A%7D%2C+c_j%2C+s_j%29%5Ctag%7B7%7D\" alt=\"t_j=g(e_{y_{j-1}^*}, c_j, s_j)\\tag{7}\" eeimg=\"1\"/> <img src=\"https://www.zhihu.com/equation?tex=o_j+%3D+%5Cmathrm%7BW%7D_ot_j%5Ctag%7B8%7D\" alt=\"o_j = \\mathrm{W}_ot_j\\tag{8}\" eeimg=\"1\"/> <img src=\"https://www.zhihu.com/equation?tex=P_j%3D%5Cmathrm%7Bsoftmax%7D%28o_j%29%5Ctag%7B9%7D\" alt=\"P_j=\\mathrm{softmax}(o_j)\\tag{9}\" eeimg=\"1\"/> <img src=\"https://www.zhihu.com/equation?tex=g\" alt=\"g\" eeimg=\"1\"/> 表示一个线性变换， <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7BW_o%7D\" alt=\"\\mathbf{W_o}\" eeimg=\"1\"/> 是 <img src=\"https://www.zhihu.com/equation?tex=t_j\" alt=\"t_j\" eeimg=\"1\"/> 到 <img src=\"https://www.zhihu.com/equation?tex=o_j\" alt=\"o_j\" eeimg=\"1\"/> 之间的映射，这样每一个目标词汇都对应 <img src=\"https://www.zhihu.com/equation?tex=o_j\" alt=\"o_j\" eeimg=\"1\"/> 中的一个维度。</p><h3>3 使用方法</h3><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-b92f24de51d24ffa41f42145eb2621b4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"620\" data-rawheight=\"307\" class=\"origin_image zh-lightbox-thumb\" width=\"620\" data-original=\"https://pic1.zhimg.com/v2-b92f24de51d24ffa41f42145eb2621b4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;620&#39; height=&#39;307&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"620\" data-rawheight=\"307\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"620\" data-original=\"https://pic1.zhimg.com/v2-b92f24de51d24ffa41f42145eb2621b4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-b92f24de51d24ffa41f42145eb2621b4_b.jpg\"/></figure><p> 图一展示了我们的方法的主要部分，真实数据中的词汇或前一个预测词，即“oracle”词，都会以一定的概率被输入作为上下文。通过在训练时处理测试时可能碰到的情况潜在的减少了训练和推断时的间隔。我们将会介绍两种方法来选择“oracle”词。一个方法是使用词级别贪婪搜索算法，另一种是考虑句子级别的最优oracle序列。句子级的oracle使用n-gram为匹配真实数据的序列提供了选项，并且内在的，对于交替的上下文中（groud-truth词汇和predict词汇交替），拥有从过矫正中复原的能力。为了预测第 <img src=\"https://www.zhihu.com/equation?tex=j\" alt=\"j\" eeimg=\"1\"/> 个目标词 <img src=\"https://www.zhihu.com/equation?tex=y_j\" alt=\"y_j\" eeimg=\"1\"/> ，我们的方法包含如下步骤： </p><p>1. 在第 <img src=\"https://www.zhihu.com/equation?tex=j-1\" alt=\"j-1\" eeimg=\"1\"/> 步选择一个oracle词 <img src=\"https://www.zhihu.com/equation?tex=y_%7Bj-1%7D%5E%7B%5Cmathrm%7Boracle%7D%7D\" alt=\"y_{j-1}^{\\mathrm{oracle}}\" eeimg=\"1\"/> (词级别或句子级别) </p><p>2. 以概率 <img src=\"https://www.zhihu.com/equation?tex=p\" alt=\"p\" eeimg=\"1\"/> 从真实数据中采样词 <img src=\"https://www.zhihu.com/equation?tex=y_%7Bj-1%7D%5E%2A\" alt=\"y_{j-1}^*\" eeimg=\"1\"/> 或者以概率 <img src=\"https://www.zhihu.com/equation?tex=1-p\" alt=\"1-p\" eeimg=\"1\"/> 采样oracle词 <img src=\"https://www.zhihu.com/equation?tex=y_%7Bj-1%7D%5E%7B%5Cmathrm%7Boracle%7D%7D\" alt=\"y_{j-1}^{\\mathrm{oracle}}\" eeimg=\"1\"/> </p><p>3. 使用采样得到的词作为 <img src=\"https://www.zhihu.com/equation?tex=y_%7Bi-1%7D\" alt=\"y_{i-1}\" eeimg=\"1\"/> 代替等式(6),(7)中的 <img src=\"https://www.zhihu.com/equation?tex=y_%7Bj-1%7D%5E%2A\" alt=\"y_{j-1}^*\" eeimg=\"1\"/> ，然后使用基于注意力的NMT预测后面的词</p><h3>3.1 oracle词的选择</h3><p>通常在第 <img src=\"https://www.zhihu.com/equation?tex=j\" alt=\"j\" eeimg=\"1\"/> 步，NMT模型需要真实词汇 <img src=\"https://www.zhihu.com/equation?tex=y_%7Bj-1%7D%5E%2A\" alt=\"y_{j-1}^*\" eeimg=\"1\"/> 作为上下文词汇去预测 <img src=\"https://www.zhihu.com/equation?tex=y_j\" alt=\"y_j\" eeimg=\"1\"/> ，因此我们可以选择一个oracle词 <img src=\"https://www.zhihu.com/equation?tex=y_%7Bj-1%7D%5E%5Cmathrm%7Boracle%7D\" alt=\"y_{j-1}^\\mathrm{oracle}\" eeimg=\"1\"/> 去模拟上下文词。oracle词应该是一个和真实词汇相近的词或者同义词。使用不同的策略将会产生不同的oracle词 <img src=\"https://www.zhihu.com/equation?tex=y_%7Bj-1%7D%5E%7B%5Cmathrm%7Boracle%7D%7D\" alt=\"y_{j-1}^{\\mathrm{oracle}}\" eeimg=\"1\"/> 。一个选择是词级别的贪婪搜索输出么一步的oracle词。除此之外，我们可以通过扩大beamsearch的搜索空间，然后使用诸如BLEU,GLEU,ROUGE等方法生成句子级别的指标来对候选翻译排序，优化oracle选择。被选中的翻译被称为<b>oracle句子</b>，翻译中的词汇时<b>句子级的oracle</b>（标注为SO）</p><p><br/> <b>词级别oracle</b><br/>    对于第 <img src=\"https://www.zhihu.com/equation?tex=j-1\" alt=\"j-1\" eeimg=\"1\"/> 步解码，最直接的方式去选择词级别的oracle是根据式(9)得到的 <img src=\"https://www.zhihu.com/equation?tex=P_%7Bj-1%7D\" alt=\"P_{j-1}\" eeimg=\"1\"/> 的分布中选择一个概率最大的词汇，如图二所示。<br/> </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-913a59631afca67cfb4721f46c2d36de_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"513\" data-rawheight=\"311\" class=\"origin_image zh-lightbox-thumb\" width=\"513\" data-original=\"https://pic3.zhimg.com/v2-913a59631afca67cfb4721f46c2d36de_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;513&#39; height=&#39;311&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"513\" data-rawheight=\"311\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"513\" data-original=\"https://pic3.zhimg.com/v2-913a59631afca67cfb4721f46c2d36de_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-913a59631afca67cfb4721f46c2d36de_b.jpg\"/></figure><p> 预测的分数 <img src=\"https://www.zhihu.com/equation?tex=o_%7Bj-1%7D\" alt=\"o_{j-1}\" eeimg=\"1\"/> 是softmax操作之前的值。在实际情况中，通过引入Gumabel-Max技术(Gumbel, 1954; Maddison et al., 2014)，我们可以获取更可信的词级别oracle，他提供了一个简单高效的方法来从确定的分布中采样。<br/> Gumbel噪声被看作是一种正则化的形式，他会与式(8)中的 <img src=\"https://www.zhihu.com/equation?tex=o_%7Bj-1%7D\" alt=\"o_{j-1}\" eeimg=\"1\"/> 相加，如图三所示：<br/> </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-bf2ae38c9fd56ca1ee9800266bdcb0f0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"578\" data-rawheight=\"296\" class=\"origin_image zh-lightbox-thumb\" width=\"578\" data-original=\"https://pic1.zhimg.com/v2-bf2ae38c9fd56ca1ee9800266bdcb0f0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;578&#39; height=&#39;296&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"578\" data-rawheight=\"296\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"578\" data-original=\"https://pic1.zhimg.com/v2-bf2ae38c9fd56ca1ee9800266bdcb0f0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-bf2ae38c9fd56ca1ee9800266bdcb0f0_b.jpg\"/></figure><p> 然后代入softmax函数，词 <img src=\"https://www.zhihu.com/equation?tex=y_%7Bj-1%7D\" alt=\"y_{j-1}\" eeimg=\"1\"/> 的分布由以下式子近似得到： <img src=\"https://www.zhihu.com/equation?tex=%5Ceta%3D-%5Clog%28-%5Clog%28%5Cmu%29%29%5Ctag%7B10%7D\" alt=\"\\eta=-\\log(-\\log(\\mu))\\tag{10}\" eeimg=\"1\"/> <img src=\"https://www.zhihu.com/equation?tex=%5Ctilde%7Bo%7D%7Bj-1%7D%3D%28o%7Bj-1%7D%2B%5Ceta%29%2F%5Ctau%5Ctag%7B11%7D\" alt=\"\\tilde{o}{j-1}=(o{j-1}+\\eta)/\\tau\\tag{11}\" eeimg=\"1\"/> <img src=\"https://www.zhihu.com/equation?tex=%5Ctilde%7BP%7D%7Bj-1%7D%3D%5Cmathrm%7Bsoftmax%7D%28%5Ctilde%7Bo%7D%7Bj-1%7D%29%5Ctag%7B12%7D\" alt=\"\\tilde{P}{j-1}=\\mathrm{softmax}(\\tilde{o}{j-1})\\tag{12}\" eeimg=\"1\"/> <img src=\"https://www.zhihu.com/equation?tex=%5Ceta\" alt=\"\\eta\" eeimg=\"1\"/> 表示从均匀随机变量 <img src=\"https://www.zhihu.com/equation?tex=%5Cmu+%5Cthicksim+%5Cmathcal%7BU%7D+%280%2C1%29\" alt=\"\\mu \\thicksim \\mathcal{U} (0,1)\" eeimg=\"1\"/> 计算得到的Gumbel噪声， <img src=\"https://www.zhihu.com/equation?tex=%5Ctau\" alt=\"\\tau\" eeimg=\"1\"/> 是热度因素。当 <img src=\"https://www.zhihu.com/equation?tex=%5Ctau\" alt=\"\\tau\" eeimg=\"1\"/> 趋近于0的时候， <img src=\"https://www.zhihu.com/equation?tex=%5Cmathrm%7Bsoftmax%7D\" alt=\"\\mathrm{softmax}\" eeimg=\"1\"/> 函数近似于 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathrm%7Bargmax%7D\" alt=\"\\mathrm{argmax}\" eeimg=\"1\"/> 操作，当 <img src=\"https://www.zhihu.com/equation?tex=%5Ctau%5Crightarrow%5Cinfty\" alt=\"\\tau\\rightarrow\\infty\" eeimg=\"1\"/> 时，变为均匀分布。类似的，由 <img src=\"https://www.zhihu.com/equation?tex=%5Ctilde%7BP%7D%7Bj-1%7D\" alt=\"\\tilde{P}{j-1}\" eeimg=\"1\"/> 知，最佳词汇将被选中为词级别的oracle词 <img src=\"https://www.zhihu.com/equation?tex=y_%7Bj-1%7D%5E%7B%5Cmathrm%7Boracle%7D%7D%3Dy_%7Bj-1%7D%5E%7BWO%7D%3D%5Cmathrm%7Bargmax%7D%28%5Ctilde%7BP%7D%7Bj-1%7D%29%5Ctag%7B13%7D\" alt=\"y_{j-1}^{\\mathrm{oracle}}=y_{j-1}^{WO}=\\mathrm{argmax}(\\tilde{P}{j-1})\\tag{13}\" eeimg=\"1\"/> 注意，Gumbel噪声只会用在oracle词的选择上，不会影响训练时的损失函数。</p><p><br/> <b>句子级别Oracle</b><br/> 句子级别的oracle是为了可以通过句子级指标使用n-gram匹配方式，达到更灵活的翻译效果。本文中，我们使用BLEU作为句子级指标。为了选择句子级的oracle，我们首先对每一批句子使用beam search，假设beam-size时k，获取到k个最佳候选翻译。在执行beam search时，对于每一个生成的词我们同样会加入Gumbel噪声。然后我们对每一个翻译使用BLEU进行评估，然后用BLEU得分最高的翻译作为oracle句子，标注为 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7By%7D%5E%7BS%7D%3D%28y_1%5ES%2C...%2Cy_%7B%7C%5Cmathrm%7By%7D%5ES%7C%7D%5E%7BS%7D%29\" alt=\"\\mathbf{y}^{S}=(y_1^S,...,y_{|\\mathrm{y}^S|}^{S})\" eeimg=\"1\"/> ，在第 <img src=\"https://www.zhihu.com/equation?tex=j\" alt=\"j\" eeimg=\"1\"/> 步解码的时候，我们定义句子级的oracle词为 <img src=\"https://www.zhihu.com/equation?tex=+y_%7Bj-1%7D%5E%7B%5Cmathrm+oracle%7D%3Dy_%7Bj-1%7D%5E%7BSO%7D%3Dy_%7Bj-1%7D%5ES%5Ctag%7B14%7D\" alt=\" y_{j-1}^{\\mathrm oracle}=y_{j-1}^{SO}=y_{j-1}^S\\tag{14}\" eeimg=\"1\"/> 但是句子级的oracle也带来一些问题。当模型从真实数据和句子级oracle中采样词汇时，两个序列应该有相同的长度，然而在基于简单的beam search算法的情况下我们并不能保证这种情况一定成立。由于上述问题，我们引入了<b>强制解码</b>来保证两个序列有相同的长度。</p><p><br/> <b>强制解码：</b> 当真是序列的长度是 <img src=\"https://www.zhihu.com/equation?tex=%7C%5Cmathrm%7By%7D%5E%2A%7C\" alt=\"|\\mathrm{y}^*|\" eeimg=\"1\"/> 时，强制解码的目标时生成一个有 <img src=\"https://www.zhihu.com/equation?tex=%7C%5Cmathrm%7By%7D%5E%2A%7C\" alt=\"|\\mathrm{y}^*|\" eeimg=\"1\"/> 个词汇的序列，以一个特殊的句子结束（EOS）标志结尾。这样，在beam search的过程中，一旦候选翻译在句子长度小于或者大于 <img src=\"https://www.zhihu.com/equation?tex=%7C%5Cmathrm%7By%7D%5E%2A%7C\" alt=\"|\\mathrm{y}^*|\" eeimg=\"1\"/> 的时候倾向于选择&#34;EOS&#34;终止翻译，我们将会强制他生成 <img src=\"https://www.zhihu.com/equation?tex=%7C%5Cmathrm%7By%7D%5E%2A%7C\" alt=\"|\\mathrm{y}^*|\" eeimg=\"1\"/> 个词汇，即：</p><ul><li>如果候选翻译在第 <img src=\"https://www.zhihu.com/equation?tex=j\" alt=\"j\" eeimg=\"1\"/> 步获取了词分布 <img src=\"https://www.zhihu.com/equation?tex=P_j\" alt=\"P_j\" eeimg=\"1\"/> ，而 <img src=\"https://www.zhihu.com/equation?tex=j%5Cle%7C%5Cmathrm%7By%7D%5E%2A%7C\" alt=\"j\\le|\\mathrm{y}^*|\" eeimg=\"1\"/> EOS是 <img src=\"https://www.zhihu.com/equation?tex=P_j\" alt=\"P_j\" eeimg=\"1\"/> 中概率最大的词，那么我们会为这个候选翻译选择概率第二大的词作为第 <img src=\"https://www.zhihu.com/equation?tex=j\" alt=\"j\" eeimg=\"1\"/> 步的词。  </li><li>如果候选翻译在第 <img src=\"https://www.zhihu.com/equation?tex=%7C%5Cmathrm%7By%7D%5E%2A%7C%2B1\" alt=\"|\\mathrm{y}^*|+1\" eeimg=\"1\"/> 步获得了词分布 <img src=\"https://www.zhihu.com/equation?tex=P_%7B%7C%5Cmathrm%7By%7D%5E%2A%7C%2B1%7D\" alt=\"P_{|\\mathrm{y}^*|+1}\" eeimg=\"1\"/> ，而EOS不是概率最大的词，那么我们选择EOS作为这个候选翻译的第 <img src=\"https://www.zhihu.com/equation?tex=%7C%5Cmathrm%7By%7D%5E%2A%7C%2B1\" alt=\"|\\mathrm{y}^*|+1\" eeimg=\"1\"/> 个词</li></ul><p>通过这种方法，我们可以保证所有的k个候选翻译都有 <img src=\"https://www.zhihu.com/equation?tex=%7C%5Cmathrm%7By%7D%5E%2A%7C\" alt=\"|\\mathrm{y}^*|\" eeimg=\"1\"/> 个词，然后对这k个候选翻译根据BLEU分数重新排序，并且选择最有可能的oracle句子。至于为句子级的oracle添加Gumabel噪声，我门在强制解码的第 <img src=\"https://www.zhihu.com/equation?tex=j\" alt=\"j\" eeimg=\"1\"/> 步用 <img src=\"https://www.zhihu.com/equation?tex=%5Ctilde%7BP%7D_j\" alt=\"\\tilde{P}_j\" eeimg=\"1\"/> 替换 <img src=\"https://www.zhihu.com/equation?tex=P_j\" alt=\"P_j\" eeimg=\"1\"/> </p><h3>3.2 衰减采样</h3><p>在我们的方法中，我们使用了一种随机的选择真实词汇 <img src=\"https://www.zhihu.com/equation?tex=y_%7Bj-1%7D%5E%2A\" alt=\"y_{j-1}^*\" eeimg=\"1\"/> 或oracle词汇 <img src=\"https://www.zhihu.com/equation?tex=y_%7Bj-1%7D%5E%7B%5Cmathrm%7Boracle%7D%7D\" alt=\"y_{j-1}^{\\mathrm{oracle}}\" eeimg=\"1\"/> 作为 <img src=\"https://www.zhihu.com/equation?tex=y_%7Bj-1%7D\" alt=\"y_{j-1}\" eeimg=\"1\"/> 的采样机制。在训练开始，模型并没有被很好的训练，太频繁的使用 <img src=\"https://www.zhihu.com/equation?tex=y_%7Bj-1%7D%5E%7B%5Cmathrm%7Boracle%7D%7D\" alt=\"y_{j-1}^{\\mathrm{oracle}}\" eeimg=\"1\"/> 作为 <img src=\"https://www.zhihu.com/equation?tex=y_%7Bj-1%7D\" alt=\"y_{j-1}\" eeimg=\"1\"/> 将会导致模型收敛速度很慢，甚至陷入局部极小值。另一方面，在训练的末段，如果上下文 <img src=\"https://www.zhihu.com/equation?tex=y_%7Bj-1%7D\" alt=\"y_{j-1}\" eeimg=\"1\"/> 一九大概率的从真实词汇 <img src=\"https://www.zhihu.com/equation?tex=y_%7Bj-1%7D%5E%2A\" alt=\"y_{j-1}^*\" eeimg=\"1\"/> 中选择，模型不能完全面对推断时所需要处理的情况，在推断时表现不佳。因此，选择真实词汇的概率 <img src=\"https://www.zhihu.com/equation?tex=p\" alt=\"p\" eeimg=\"1\"/> 不能是一个定值，而是应该随着训练的进行逐渐减小。一开始， <img src=\"https://www.zhihu.com/equation?tex=p%3D1\" alt=\"p=1\" eeimg=\"1\"/> ，也就是说模型完全基于真实数据训练，当模型逐渐收敛时，模型更频繁地选择oracle词。<br/> 借鉴但是不同于Bengio让 <img src=\"https://www.zhihu.com/equation?tex=p\" alt=\"p\" eeimg=\"1\"/> 作为mini-bathch索引的函数逐渐衰减想法，我们用一个依赖于训练epoch的索引 <img src=\"https://www.zhihu.com/equation?tex=e\" alt=\"e\" eeimg=\"1\"/> （从0开始计数）的衰减函数定义 <img src=\"https://www.zhihu.com/equation?tex=p\" alt=\"p\" eeimg=\"1\"/> <img src=\"https://www.zhihu.com/equation?tex=p%3D%5Cfrac%7B%5Cmu%7D%7B%5Cmu%2B%5Cexp%28e%2F%5Cmu%29%7D%5Ctag%7B15%7D\" alt=\"p=\\frac{\\mu}{\\mu+\\exp(e/\\mu)}\\tag{15}\" eeimg=\"1\"/> <img src=\"https://www.zhihu.com/equation?tex=%5Cmu\" alt=\"\\mu\" eeimg=\"1\"/> 是一个超参数。函数严格单调递减。随着训练的进行，输入真实词汇的概率 <img src=\"https://www.zhihu.com/equation?tex=p\" alt=\"p\" eeimg=\"1\"/> 会逐渐减小</p><h3>3.3 训练</h3><p>在使用上述方法选择了 <img src=\"https://www.zhihu.com/equation?tex=y_%7Bj-1%7D\" alt=\"y_{j-1}\" eeimg=\"1\"/> 之后，我们可以通过式(6),(7),(8),(9)得到 <img src=\"https://www.zhihu.com/equation?tex=y_j\" alt=\"y_j\" eeimg=\"1\"/> 的词汇分布。在计算训练损失的时候我们不再添加Gumbel噪声。最终目标是基于最大似然估计(MLE)来最大化生成真实序列的概率，也就是最小化如下损失函数： <img src=\"https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D%28%5Ctheta%29%3D-%5Csum_%7Bn%3D1%7D%5EN%5Csum_%7Bj%3D1%7D%5E%7B%7C%5Cmathbf%7By%7D%5En%7C%7D%5Clog+P_j%5En%5By_j%5En%5D%5Ctag%7B16%7D\" alt=\"\\mathcal{L}(\\theta)=-\\sum_{n=1}^N\\sum_{j=1}^{|\\mathbf{y}^n|}\\log P_j^n[y_j^n]\\tag{16}\" eeimg=\"1\"/> <img src=\"https://www.zhihu.com/equation?tex=N\" alt=\"N\" eeimg=\"1\"/> 是训练数据中的句子对， <img src=\"https://www.zhihu.com/equation?tex=%7C%5Cmathbf%7By%7D%5En%7C\" alt=\"|\\mathbf{y}^n|\" eeimg=\"1\"/> 代表第n个真实句子的长度， <img src=\"https://www.zhihu.com/equation?tex=P_j%5En\" alt=\"P_j^n\" eeimg=\"1\"/> 表示在第j步对第n个句子的预测的概率分布， <img src=\"https://www.zhihu.com/equation?tex=P_j%5En%5By_j%5En%5D\" alt=\"P_j^n[y_j^n]\" eeimg=\"1\"/> 即在第j步生成真实词汇 <img src=\"https://www.zhihu.com/equation?tex=y_j%5En\" alt=\"y_j^n\" eeimg=\"1\"/> 的概率。</p><h3>4 相关工作</h3><p>略...</p><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p>原文地址：<a href=\"https://link.zhihu.com/?target=https%3A//www.aclweb.org/anthology/P19-1426\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Bridging the Gap between Training and Inference for NeuralMachine Translation</a></p>", 
            "topic": [
                {
                    "tag": "自然语言处理", 
                    "tagLink": "https://api.zhihu.com/topics/19560026"
                }, 
                {
                    "tag": "机器翻译", 
                    "tagLink": "https://api.zhihu.com/topics/19616892"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/68926263", 
            "userName": "右左瓜子", 
            "userLink": "https://www.zhihu.com/people/6107fd0900473aae4e5ec6824b4fde4a", 
            "upvote": 0, 
            "title": "TF-IDF漫谈", 
            "content": "<h2>TF-IDF漫谈</h2><h2>摘要</h2><p>本篇文章主要谈一谈TF-IDF算法，因为这个算法在几乎处处可见，搜索排序，用户画像，以及事件热点，之所以叫漫谈是因为不涉及到很深的理论知识和算法过程，取而代之的是很多感性上的认知。</p><h3>一. TF-IDF是干嘛用的</h3><p>TF-IDF的核心作用就是关键词提取，而关键词提取可以服务于其他很多业务。TF-IDF算法提出应该有差不多五十个年头了，但是应用至今，因为他是一个简介有效而且理论上很有说服力的算法。接下来我会从感性到理性，从表面到本质来一步一步阐述这个算法是如何产生的。</p><p>首先，提到关键词，什么是关键词，他的定义可能比较模糊，但是他判定却相对来讲比较清晰。比如读完《西游记》，西游记的关键字是什么？“贾宝玉”、“林黛玉”？肯定不是，八杆子打不着的东西。“和尚”、“观音”？有点儿像，但是不够关键，《白蛇传》里边儿也有很多和尚也有观音。“水德星君”、“卯日星君”？好像也不是，虽然跟《西游记》相关但是不具代表性。“孙悟空”、“猪八戒”？是的，毫无疑问。</p><p>基于以上感性的认知，我们来看看关键词是什么样的： </p><p>1. 文章中要有，像“贾宝玉”、“林黛玉”就不行。（文章中没有的但是关键的是属于更高级的概括这里不再讨论范围） </p><p>2. 要有独特，最好别的文章中没有，像“和尚”这样的词大部分古代小说中都有，谁知道你说的是哪一部 </p><p>3. 要有代表性，“水德星君”这样的词虽然够独特，但是只看过一遍的或者看的不仔细的人很可能都忘记了。</p><p>所以感性上的理解，关键词要在文章中，别的文章最好没有，即文档分布（Inverse Document Frequency, <img src=\"https://www.zhihu.com/equation?tex=IDF\" alt=\"IDF\" eeimg=\"1\"/> ），本篇文章最好要多出现，即词频（Term Fraquency, <img src=\"https://www.zhihu.com/equation?tex=TF\" alt=\"TF\" eeimg=\"1\"/> ）。好了，现在我们可以构建一个关键词公式了，：</p><p><img src=\"https://www.zhihu.com/equation?tex=k%3D%5Cfrac%7BTF%7D%7BIDF%7D\" alt=\"k=\\frac{TF}{IDF}\" eeimg=\"1\"/> </p><p>看起来很好，词频越高越关键，其他文档分布越少越关键，起个名字吧，瓜子公式，瓜子理论，但是存在两个问题，一个是在你实验之后会发现效果不是特别好，要调参，这里乘以5那里除以2，甚至开方平方求对数，等等等等，第二个是这是我们的感性认知，有没有理论依据呢？为了创建这个理论我们下了三个定义，牛顿的力学也只有三定律啊。这个如果能成为一个理论的话，自然语言处理的书会不会太厚了。我们尝试一下从信息论里找答案，因为文章的阅读本质上也是一种信息的传输。</p><h3>二、TF-IDF算法的信息论依据</h3><p>我们刚刚为了解决关键词的问题给关键词下了三个定义，构建了一个定性的公式。现在我们来更本质的看下这个问题，而且给一个合理的定量的公式。</p><p>刚刚我们谈到文章的阅读实际上是一种信息的传递，那么我们能不能找到一个能表示信息量大小的东西来衡量词语，信息量越大的词越关键是不是可以解决这个问题？当然可以，而且有人已经为我们找好了，这个人就叫香农，信息论的奠基人，程序员的祖师爷之一，这个信息量的度量的公式也就叫做<b>香农熵</b>。这个公式背后是基于怎样的思考我们不讨论（我不知道）。我们直接看看这个公式的定义：</p><blockquote> 对于任意一个随机变量 X，它的熵定义如下：<br/> <img src=\"https://www.zhihu.com/equation?tex=H%28X%29%3D-%5Csum+P%28x%29%5Clog_2%7BP%28x%29%7D\" alt=\"H(X)=-\\sum P(x)\\log_2{P(x)}\" eeimg=\"1\"/> <br/>变量的不确定性越大，熵也就越大，把它搞清楚所需要的信息量也就越大。<br/> </blockquote><p>这里的随机变量在关键词提取的任务中就是词， <img src=\"https://www.zhihu.com/equation?tex=P%28x%29\" alt=\"P(x)\" eeimg=\"1\"/> 就是词 <img src=\"https://www.zhihu.com/equation?tex=x\" alt=\"x\" eeimg=\"1\"/> 的概率，根据大数定律，我们可以用词在样本中出现的频率也就是词频(Term Frequency,TF)表示。单个词语的信息量我们搞定了，但是上面我们总结的司塔科定理中他还要相对于其他文章，信息论能搞定吗？能，祖师爷们早都已经把工具给我们造好了，这个工具就是<b>互信息</b>，（在吴军老师的《数学之美》中提到逆文档频率实际上是特定情况下关键词分布的交叉熵，因为我没找到这段话相关的证明，另外单单解释逆文档频率的话并不能表明为何是相乘的关系，所以这里就用了互信息的解释，交叉熵相关定义见附录）直接看定义：</p><blockquote> 在概率论和信息论中，两个随机变量的互信息（Mutual Information，简称&gt;MI）或转移信息（transinformation）是变量间相互依赖性的量度。不同于相&gt;关系数，互信息并不局限于实值随机变量，它更加一般且决定着联合分布 p(X,&gt;Y) 和分解的边缘分布的乘积 p(X)p(Y) 的相似程度。互信息是点间互信息 （PMI）的期望值。互信息最常用的单位是<code>bit</code>。<br/> 一般地，两个离散随机变量 X 和 Y 的互信息可以定义为：<br/> <img src=\"https://www.zhihu.com/equation?tex=I%28X%3BY%29%3D%5Csum_%7B%7By%5Cin+Y%7D%7D%5Csum_%7B%7Bx%5Cin+X%7D%7Dp%28x%2Cy%29%5Clog+%7B%5Cleft%28%7B%5Cfrac++%7Bp%28x%2Cy%29%7D%7Bp%28x%29%5C%2Cp%28y%29%7D%7D%5Cright%29%7D%2C%5C%2C%5C%21\" alt=\"I(X;Y)=\\sum_{{y\\in Y}}\\sum_{{x\\in X}}p(x,y)\\log {\\left({\\frac  {p(x,y)}{p(x)\\,p(y)}}\\right)},\\,\\!\" eeimg=\"1\"/> <br/> 其中 <img src=\"https://www.zhihu.com/equation?tex=p%28x%2Cy%29\" alt=\"p(x,y)\" eeimg=\"1\"/> 是 <img src=\"https://www.zhihu.com/equation?tex=X\" alt=\"X\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=Y\" alt=\"Y\" eeimg=\"1\"/> 的联合概率分布函数，而 <img src=\"https://www.zhihu.com/equation?tex=p%28x%29\" alt=\"p(x)\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=p%28y%29\" alt=\"p(y)\" eeimg=\"1\"/> 分别是 <img src=\"https://www.zhihu.com/equation?tex=X\" alt=\"X\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=Y\" alt=\"Y\" eeimg=\"1\"/> 的边缘概率分布函数。<br/> </blockquote><p>我们把互信息公式展开一下</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+I%28X%3BY%29++%26+%3D+%5Csum_%7B%7By%5Cin+Y%7D%7D%5Csum_%7B%7Bx%5Cin+X%7D%7Dp%28x%2Cy%29%5Clog+%7B%5Cleft%28%7B%5Cfrac++%7Bp%28x%2Cy%29%7D%7Bp%28x%29p%28y%29%7D%7D%5Cright%29%7D+%5C%5C+%26+%3D+%5Csum_%7B%7By%5Cin+Y%7D%7D%5Csum_%7B%7Bx%5Cin+X%7D%7Dp%28x%2Cy%29%5Clog+%7B%5Cleft%28%7B%5Cfrac++%7Bp%28x%2Cy%29%7D%7Bp%28x%29%7D+%5Cfrac%7B1%7D%7Bp%28y%29%7D%7D%5Cright%29%7D+%5C%5C+%26+%3D+-+%5Csum_%7B%7By%5Cin+Y%7D%7D%5Csum_%7B%7Bx%5Cin+X%7D%7Dp%28x%2Cy%29%5Clog+%7B%7Bp%28y%29+%7D%7D+-%28-%5Csum_%7B%7By%5Cin+Y%7D%7D%5Csum_%7B%7Bx%5Cin+X%7D%7Dp%28x%2Cy%29%5Clog+%7B%5Cleft%28%7B%5Cfrac++%7Bp%28x%2Cy%29%7D%7Bp%28x%29%7D+%7D%5Cright%29%7D%29+%5C%5C+%26+%3D+H%28X%29+-+H%28X%7CY%29+%5C%5C+%26+%3D+H%28X%29+%2B+H%28Y%29+-+H%28XY%29++%5Cend%7Baligned%7D\" alt=\"\\begin{aligned} I(X;Y)  &amp; = \\sum_{{y\\in Y}}\\sum_{{x\\in X}}p(x,y)\\log {\\left({\\frac  {p(x,y)}{p(x)p(y)}}\\right)} \\\\ &amp; = \\sum_{{y\\in Y}}\\sum_{{x\\in X}}p(x,y)\\log {\\left({\\frac  {p(x,y)}{p(x)} \\frac{1}{p(y)}}\\right)} \\\\ &amp; = - \\sum_{{y\\in Y}}\\sum_{{x\\in X}}p(x,y)\\log {{p(y) }} -(-\\sum_{{y\\in Y}}\\sum_{{x\\in X}}p(x,y)\\log {\\left({\\frac  {p(x,y)}{p(x)} }\\right)}) \\\\ &amp; = H(X) - H(X|Y) \\\\ &amp; = H(X) + H(Y) - H(XY)  \\end{aligned}\" eeimg=\"1\"/> </p><p>这里边涉及到条件熵 <img src=\"https://www.zhihu.com/equation?tex=H%28X%7CY%29\" alt=\"H(X|Y)\" eeimg=\"1\"/> 和联合熵 <img src=\"https://www.zhihu.com/equation?tex=H%28XY%29\" alt=\"H(XY)\" eeimg=\"1\"/> ，我把公式整理了一下，条件熵计算公式就是第三个等号的后半部分，联合熵的计算本文不涉及到所以不展开，这两个熵的定义也不展开了，详情可以去维基看一看，中文维基给了很详细的解释。</p><p>好了，接下来可能需要一点统计学的思维，不然可能会比较绕。我们让 <img src=\"https://www.zhihu.com/equation?tex=D%3D%5C%7Bd_1%2Cd_2%2C...%2Cd_N%5C%7D\" alt=\"D=\\{d_1,d_2,...,d_N\\}\" eeimg=\"1\"/> 作为文章集合， <img src=\"https://www.zhihu.com/equation?tex=W%3D%5C%7Bw_1%2Cw_2%2C...%2Cw_M%5C%7D\" alt=\"W=\\{w_1,w_2,...,w_M\\}\" eeimg=\"1\"/> 作为去重后的词集合， <img src=\"https://www.zhihu.com/equation?tex=N\" alt=\"N\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=M\" alt=\"M\" eeimg=\"1\"/> 分别是文章总数和词总数。用 <img src=\"https://www.zhihu.com/equation?tex=d_j\" alt=\"d_j\" eeimg=\"1\"/> 表示事件从 <img src=\"https://www.zhihu.com/equation?tex=D\" alt=\"D\" eeimg=\"1\"/> 中抽取一篇文档，类似的 <img src=\"https://www.zhihu.com/equation?tex=w_i\" alt=\"w_i\" eeimg=\"1\"/> 表示事件从$<code>W</code>$中选择一个词。 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathcal+D\" alt=\"\\mathcal D\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathcal+W\" alt=\"\\mathcal W\" eeimg=\"1\"/> 作为定义在事件 <img src=\"https://www.zhihu.com/equation?tex=%5C%7Bd_1%2Cd_2%2C...%2Cd_N%5C%7D\" alt=\"\\{d_1,d_2,...,d_N\\}\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5C%7Bw_1%2Cw_2%2C...%2Cw_N%5C%7D\" alt=\"\\{w_1,w_2,...,w_N\\}\" eeimg=\"1\"/> 上的随机变量</p><p>假设所有的文档都会被同等概率的抽取到，即对于 <img src=\"https://www.zhihu.com/equation?tex=d_j+%5Cin+D\" alt=\"d_j \\in D\" eeimg=\"1\"/> ， <img src=\"https://www.zhihu.com/equation?tex=P%28d_j%29%3D%5Cfrac%7B1%7D%7BN%7D\" alt=\"P(d_j)=\\frac{1}{N}\" eeimg=\"1\"/> ，那么 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathcal%7BD%7D\" alt=\"\\mathcal{D}\" eeimg=\"1\"/> 的信息熵就是</p><p><img src=\"https://www.zhihu.com/equation?tex=H%28%5Cmathcal%7BD%7D%29+%3D+-+%5Csum_%7Bd_j+%5Cin+D%7D+P%28d_j%29%5Clog+P%28d_j%29+%3D+-N+%5Cfrac%7B1%7D%7BN%7D%5Clog+%5Cfrac%7B1%7D%7BN%7D+%3D+-%5Clog+%5Cfrac%7B1%7D%7BN%7D+\" alt=\"H(\\mathcal{D}) = - \\sum_{d_j \\in D} P(d_j)\\log P(d_j) = -N \\frac{1}{N}\\log \\frac{1}{N} = -\\log \\frac{1}{N} \" eeimg=\"1\"/> </p><p>接下来我们考虑在已知包含关键词 <img src=\"https://www.zhihu.com/equation?tex=w_i%28%5Cin+W%29\" alt=\"w_i(\\in W)\" eeimg=\"1\"/> 的文档已知的情况。让 <img src=\"https://www.zhihu.com/equation?tex=N_i\" alt=\"N_i\" eeimg=\"1\"/> 表示包含 <img src=\"https://www.zhihu.com/equation?tex=w_i\" alt=\"w_i\" eeimg=\"1\"/> 的文档数，还是假设这 <img src=\"https://www.zhihu.com/equation?tex=N_i\" alt=\"N_i\" eeimg=\"1\"/> 个文档出现的概率相等，这样，对于给定 <img src=\"https://www.zhihu.com/equation?tex=w_i\" alt=\"w_i\" eeimg=\"1\"/> 的随机变量 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathcal+D\" alt=\"\\mathcal D\" eeimg=\"1\"/> 的熵如下所示：</p><p><img src=\"https://www.zhihu.com/equation?tex=H%28%5Cmathcal+%7BD%7D%7Cw_i%29+%3D+-%5Csum_%7Bd_j+%5Cin+D%7DP%28d_j%7Cw_i%29%5Clog+P%28d_j%7Cw_i%29%3D-N_i+%5Cfrac%7B1%7D%7BN_i%7D%5Clog+%5Cfrac+%7B1%7D%7BN_i%7D+%3D+-%5Clog+%5Cfrac+%7B1%7D%7BN_i%7D\" alt=\"H(\\mathcal {D}|w_i) = -\\sum_{d_j \\in D}P(d_j|w_i)\\log P(d_j|w_i)=-N_i \\frac{1}{N_i}\\log \\frac {1}{N_i} = -\\log \\frac {1}{N_i}\" eeimg=\"1\"/> </p><p>有两点需要注意，一个是不包含 <img src=\"https://www.zhihu.com/equation?tex=w_i\" alt=\"w_i\" eeimg=\"1\"/> 的文档因为 <img src=\"https://www.zhihu.com/equation?tex=P%28d_j%7Cw_i%29%3D0\" alt=\"P(d_j|w_i)=0\" eeimg=\"1\"/> ，所以在等式中消失了，另一点是条件概率的计算基于了上面的那个假设。</p><p>好了，基本工作做的差不多了，现在我们假设从整个文章集合中随机的取出一个词 <img src=\"https://www.zhihu.com/equation?tex=w_i\" alt=\"w_i\" eeimg=\"1\"/> ，用 <img src=\"https://www.zhihu.com/equation?tex=f_%7Bij%7D\" alt=\"f_{ij}\" eeimg=\"1\"/> 表示文章 <img src=\"https://www.zhihu.com/equation?tex=d_j\" alt=\"d_j\" eeimg=\"1\"/> 中词 <img src=\"https://www.zhihu.com/equation?tex=w_i\" alt=\"w_i\" eeimg=\"1\"/> 的次数， <img src=\"https://www.zhihu.com/equation?tex=f_%7Bw_i%7D\" alt=\"f_{w_i}\" eeimg=\"1\"/> 表示 <img src=\"https://www.zhihu.com/equation?tex=w_i\" alt=\"w_i\" eeimg=\"1\"/> 在整个文章集合中出现的次数，文章集合中出现的总词数记为 <img src=\"https://www.zhihu.com/equation?tex=F\" alt=\"F\" eeimg=\"1\"/> ，对于给定的 <img src=\"https://www.zhihu.com/equation?tex=w_i\" alt=\"w_i\" eeimg=\"1\"/> ，被选中的概率是 <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac+%7B%5Csum_jf_%7Bij%7D%7D%7BF%7D%3D%5Cfrac+%7Bf_%7Bw_i%7D%7D%7BF%7D\" alt=\"\\frac {\\sum_jf_{ij}}{F}=\\frac {f_{w_i}}{F}\" eeimg=\"1\"/> 。那么文章和词之间的互信息可以计算如下（互信息和条件熵的关系，在互信息的展开公式中已经给出）：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+J%28%5Cmathcal+%7BD%7D%3B+%5Cmathcal+%7BW%7D%29++%26+%3D+H%28%5Cmathcal+%7BD%7D%29-H%28%5Cmathcal+%7BD%7D%7C%5Cmathcal+%7BW%7D%29+%5C%5C+%26+%3D+%5Csum_%7Bw_i+%5Cin+W%7DP%28w_i%29%28H%28%5Cmathcal+%7BD%7D%29-H%28%5Cmathcal+%7BD%7D%7Cw_i%29%29+%5C%5C+%26+%3D+%5Csum_%7Bw_i+%5Cin+W%7D+%5Cfrac+%7Bf_%7Bw_i%7D%7D%7BF%7D%28-%5Clog+%5Cfrac+%7B1%7D%7BN%7D+%2B+%5Clog+%5Cfrac+%7B1%7D%7BN_i%7D%29+%5C%5C+%26+%3D+%5Csum_%7Bw_i+%5Cin+W%7D+%5Cfrac+%7Bf_%7Bw_i%7D%7D+%7BF%7D+%5Clog+%5Cfrac%7BN%7D%7BN_i%7D+%5C%5C+%26+%3D+%5Csum_%7Bw_i+%5Cin+W%7D+%5Csum_%7Bd_j+%5Cin+D%7D+%5Cfrac+%7Bf_%7Bij%7D%7D%7BF%7D%5Clog+%5Cfrac+%7BN%7D%7BN_i%7D++%5Cend%7Baligned%7D\" alt=\"\\begin{aligned} J(\\mathcal {D}; \\mathcal {W})  &amp; = H(\\mathcal {D})-H(\\mathcal {D}|\\mathcal {W}) \\\\ &amp; = \\sum_{w_i \\in W}P(w_i)(H(\\mathcal {D})-H(\\mathcal {D}|w_i)) \\\\ &amp; = \\sum_{w_i \\in W} \\frac {f_{w_i}}{F}(-\\log \\frac {1}{N} + \\log \\frac {1}{N_i}) \\\\ &amp; = \\sum_{w_i \\in W} \\frac {f_{w_i}} {F} \\log \\frac{N}{N_i} \\\\ &amp; = \\sum_{w_i \\in W} \\sum_{d_j \\in D} \\frac {f_{ij}}{F}\\log \\frac {N}{N_i}  \\end{aligned}\" eeimg=\"1\"/> </p><p>最后的两个等式给了我们计算这个互信息的两种方式，展现的是从两个不同的方面来观察互信息，倒数第二个等式表示互信息是所有去重后词语的权重的总和，也就是相对于整个文章集合，关键词权重的总和，这里就提供了单个词语的权重计算方式。倒数第一个等式表明互信息是每篇文章中每个去重后的词的权重的总和，这里的权重就是我们所讲到的TF-IDF值大小，也就是关键程度，两个等式分别给出了相对于单篇文章和相对于整个文档集合的关键程度的计算方式。</p><p>最后提一下这种推理方式成立基于了两个假设，一个是用频率来衡量概率，另一个是对问题作了简化处理，假设</p><p><img src=\"https://www.zhihu.com/equation?tex=P%28d_j%29%3D%5Csum_%7BW%28d_j%29%7D+%5Cfrac+%7Bf_%7Bw_i%7D%7D%7BF%7D+%5Cfrac+%7B1%7D%7BN_i%7D+%5Capprox+%5Cfrac+%7B1%7D%7BN%7D+%5C%5C+P%28w_i%2C+d_j%29+%3D+%5Cfrac+%7Bf_%7Bw_i%7D%7D+%7BF%7D+%5Cfrac+%7B1%7D%7BN_i%7D+%5Capprox+%5Cfrac+%7Bf_%7Bij%7D%7D%7BF%7D\" alt=\"P(d_j)=\\sum_{W(d_j)} \\frac {f_{w_i}}{F} \\frac {1}{N_i} \\approx \\frac {1}{N} \\\\ P(w_i, d_j) = \\frac {f_{w_i}} {F} \\frac {1}{N_i} \\approx \\frac {f_{ij}}{F}\" eeimg=\"1\"/> </p><p>其中$<code>W(d_j)</code>$表示文章$<code>d_j</code>$包含的去重后所有词的集合。第一个假设在数据量大的情况下基本可以符合实际，而第二个假设实际上是一种很强的假设，比如我们看一个满足假设的例子，如果文章满足每个相同的词只出现一次，而且每篇文章大小相同的条件</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+P%28d_j%29+%26+%3D+%5Csum_%7BW%28d_j%29%7D+%5Cfrac+%7Bf_%7Bw_i%7D%7D%7BF%7D+%5Cfrac+%7B1%7D%7BN_i%7D+%5C%5C+%26+%3D+%5Csum_%7BW%28d_j%29%7D+%5Cfrac+%7B1%7D%7BF%7D+%5Cqquad+%E6%AF%8F%E4%B8%AA%E6%96%87%E6%A1%A3%E5%90%8C%E4%B8%80%E4%B8%AA%E8%AF%8D%E5%8F%AA%E5%87%BA%E7%8E%B0%E4%B8%80%E6%AC%A1%EF%BC%8C%E6%89%80%E4%BB%A5f_%7Bw_i%7D+%3D+N_i++%5C%5C+%26+%3D+%7CW%28d_j%29%7C%2A%5Cfrac+%7B1%7D%7BF%7D+%5C%5C+%26+%3D+%7CW%28d_j%29%7C%2A%5Cfrac+%7B1%7D%7BF%7D+%5Cqquad+%E6%AF%8F%E4%B8%AA%E6%96%87%E6%A1%A3%E5%90%8C%E4%B8%80%E4%B8%AA%E8%AF%8D%E5%8F%AA%E5%87%BA%E7%8E%B0%E4%B8%80%E6%AC%A1%EF%BC%8C%E6%AF%8F%E4%B8%AA%E6%96%87%E6%A1%A3%E9%95%BF%E5%BA%A6%E7%9B%B8%E5%90%8C%EF%BC%8C%E6%89%80%E4%BB%A5%7CW%28d_j%29%7C+%3D+%5Cfrac%7BF%7D%7BN%7D+%5C%5C++%26+%3D+%5Cfrac+%7B1%7D%7BN%7D+%5Cend%7Baligned%7D\" alt=\"\\begin{aligned} P(d_j) &amp; = \\sum_{W(d_j)} \\frac {f_{w_i}}{F} \\frac {1}{N_i} \\\\ &amp; = \\sum_{W(d_j)} \\frac {1}{F} \\qquad 每个文档同一个词只出现一次，所以f_{w_i} = N_i  \\\\ &amp; = |W(d_j)|*\\frac {1}{F} \\\\ &amp; = |W(d_j)|*\\frac {1}{F} \\qquad 每个文档同一个词只出现一次，每个文档长度相同，所以|W(d_j)| = \\frac{F}{N} \\\\  &amp; = \\frac {1}{N} \\end{aligned}\" eeimg=\"1\"/> </p><p>同样的，对于假设二的第二个公式：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D+P%28w_i%2C+d_j%29+%26%3D+%5Cfrac+%7Bf_%7Bw_i%7D%7D+%7BF%7D+%5Cfrac+%7B1%7D%7BN_i%7D+%5C%5C+%26%3D+%5Cfrac+%7B1%7D+%7BF%7D+%5Cfrac+%7Bf_%7Bw_i%7D%7D%7BN_i%7D+%5C%5C+%26%3D+%5Cfrac+%7Bf_%7Bij%7D%7D+%7BF%7D++%5Cqquad+f_%7Bij%7D%E5%8F%96%E5%B9%B3%E5%9D%87%E5%80%BC+%5Cend%7Baligned%7D\" alt=\"\\begin{aligned} P(w_i, d_j) &amp;= \\frac {f_{w_i}} {F} \\frac {1}{N_i} \\\\ &amp;= \\frac {1} {F} \\frac {f_{w_i}}{N_i} \\\\ &amp;= \\frac {f_{ij}} {F}  \\qquad f_{ij}取平均值 \\end{aligned}\" eeimg=\"1\"/> </p><p>当然，这种条件满足假设二并不代表假设二必须要是这种条件，吴军老师在推导tf-idf公式的时候用了类似的条件，这是一个看起来很反人类的条件，这么做的原因一个是让公式可以推导下去，因为里边有很多对数计算，如果考虑很复杂的情况下很可能得不到比较好的结果。这也是TF-IDF算法会产生坏结果的原因。此外这里的互信息的计算是针对整个文档集合和整个词集合的，所以条件中 <img src=\"https://www.zhihu.com/equation?tex=f_%7Bij%7D\" alt=\"f_{ij}\" eeimg=\"1\"/> 取均值，当计算指定文档的指定词的权重时，我们依旧会取该文档中的 <img src=\"https://www.zhihu.com/equation?tex=f_%7Bij%7D\" alt=\"f_{ij}\" eeimg=\"1\"/> </p><h3>实验</h3><p>这里我取了《西游记》作为实验数据，结巴分词作为分词工具，没做停用词处理，直接取词长大于2的做计算，本实验代码使用的语料可以从<a href=\"https://link.zhihu.com/?target=http%3A//down.shushu8.com/txt/0/%25E8%25A5%25BF%25E6%25B8%25B8%25E8%25AE%25B0_%25E5%2590%25B4%25E6%2589%25BF%25E6%2581%25A9_shushu8.com.txt\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">这里</a>下载，下载之后可以直接用。上式中的 <img src=\"https://www.zhihu.com/equation?tex=F\" alt=\"F\" eeimg=\"1\"/> 在语料固定的情况下是常数，所以程序中并没有计算它的大小，但这丝毫不会影响到关键词排序结果。</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"kn\">import</span> <span class=\"nn\">re</span>\n<span class=\"kn\">import</span> <span class=\"nn\">pandas</span>\n<span class=\"kn\">import</span> <span class=\"nn\">jieba</span>\n<span class=\"kn\">import</span> <span class=\"nn\">operator</span>\n<span class=\"kn\">import</span> <span class=\"nn\">time</span>\n<span class=\"kn\">import</span> <span class=\"nn\">pickle</span>\n\n<span class=\"n\">start_time</span> <span class=\"o\">=</span> <span class=\"n\">time</span><span class=\"o\">.</span><span class=\"n\">time</span><span class=\"p\">()</span>\n<span class=\"n\">term_freq_dict</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>\n<span class=\"n\">doc_list</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n<span class=\"n\">tmp_list</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n<span class=\"n\">voc_list</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n<span class=\"k\">with</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"s2\">&#34;西游记_吴承恩_shushu8.com.txt&#34;</span><span class=\"p\">,</span> <span class=\"s2\">&#34;r&#34;</span><span class=\"p\">,</span> <span class=\"n\">encoding</span><span class=\"o\">=</span><span class=\"s2\">&#34;utf-8&#34;</span><span class=\"p\">)</span> <span class=\"k\">as</span> <span class=\"n\">f</span><span class=\"p\">:</span>\n    <span class=\"k\">while</span> <span class=\"mi\">1</span><span class=\"p\">:</span>\n        <span class=\"n\">line</span> <span class=\"o\">=</span> <span class=\"n\">f</span><span class=\"o\">.</span><span class=\"n\">readline</span><span class=\"p\">()</span>\n        <span class=\"k\">if</span> <span class=\"n\">line</span><span class=\"p\">:</span>\n            <span class=\"k\">if</span> <span class=\"n\">re</span><span class=\"o\">.</span><span class=\"n\">match</span><span class=\"p\">(</span><span class=\"sa\">r</span><span class=\"s1\">&#39;第[0-9]</span><span class=\"si\">{3}</span><span class=\"s1\">&#39;</span><span class=\"p\">,</span> <span class=\"n\">line</span><span class=\"p\">):</span>\n                <span class=\"n\">doc_list</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">tmp_list</span><span class=\"p\">)</span>\n                <span class=\"n\">voc_list</span> <span class=\"o\">+=</span> <span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"n\">tmp_list</span><span class=\"p\">))</span>\n                <span class=\"n\">voc_list</span> <span class=\"o\">=</span> <span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"nb\">set</span><span class=\"p\">(</span><span class=\"n\">voc_list</span><span class=\"p\">))</span>\n                <span class=\"n\">tmp_list</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n            <span class=\"k\">else</span><span class=\"p\">:</span>\n                <span class=\"n\">tmp_list</span> <span class=\"o\">+=</span> <span class=\"n\">jieba</span><span class=\"o\">.</span><span class=\"n\">lcut</span><span class=\"p\">(</span><span class=\"n\">line</span><span class=\"p\">)</span>\n        <span class=\"k\">else</span><span class=\"p\">:</span>\n            <span class=\"k\">break</span>\n<span class=\"c1\"># 计算IDF值，因为这个过程比较耗时间所以可以计算完之后保存结果用的时候直接从文件加载</span>\n<span class=\"c1\"># for voc in voc_list:</span>\n<span class=\"c1\">#     for doc in doc_list:</span>\n<span class=\"c1\">#         if doc.__contains__(voc):</span>\n<span class=\"c1\">#             try:</span>\n<span class=\"c1\">#                 term_freq_dict[voc] += 1</span>\n<span class=\"c1\">#             except KeyError:</span>\n<span class=\"c1\">#                 term_freq_dict[voc] = 1</span>\n<span class=\"c1\">#             continue</span>\n<span class=\"c1\">#</span>\n<span class=\"c1\"># import math</span>\n<span class=\"c1\"># term_idf_dict = {}</span>\n<span class=\"c1\"># for t in term_freq_dict:</span>\n<span class=\"c1\">#     term_idf_dict[t] = math.log2(term_freq_dict[t]/100)</span>\n<span class=\"c1\">#</span>\n<span class=\"c1\"># 保存结果</span>\n<span class=\"c1\"># idf_pkl = open(&#34;idf_pkl&#34;, &#34;wb&#34;)</span>\n<span class=\"c1\"># pickle.dump(term_idf_dict, idf_pkl, -1)</span>\n<span class=\"c1\"># idf_pkl.close()</span>\n<span class=\"c1\">#</span>\n<span class=\"c1\">#加载结果，第一次运行要去掉上面的代码注释</span>\n<span class=\"n\">idf_pkl_r</span> <span class=\"o\">=</span> <span class=\"nb\">open</span><span class=\"p\">(</span><span class=\"s2\">&#34;idf_pkl&#34;</span><span class=\"p\">,</span> <span class=\"s2\">&#34;rb&#34;</span><span class=\"p\">)</span>\n<span class=\"n\">term_idf_dict</span> <span class=\"o\">=</span> <span class=\"n\">pickle</span><span class=\"o\">.</span><span class=\"n\">load</span><span class=\"p\">(</span><span class=\"n\">idf_pkl_r</span><span class=\"p\">)</span>\n<span class=\"n\">idf_pkl_r</span><span class=\"o\">.</span><span class=\"n\">close</span><span class=\"p\">()</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">get_keyword</span><span class=\"p\">(</span><span class=\"n\">num_doc</span><span class=\"p\">):</span>\n    <span class=\"n\">doc_tf_idf_dict</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>\n    <span class=\"n\">term_freq</span> <span class=\"o\">=</span> <span class=\"n\">pandas</span><span class=\"o\">.</span><span class=\"n\">value_counts</span><span class=\"p\">(</span><span class=\"n\">pandas</span><span class=\"o\">.</span><span class=\"n\">Series</span><span class=\"p\">(</span><span class=\"n\">doc_list</span><span class=\"p\">[</span><span class=\"n\">num_doc</span><span class=\"p\">]))</span>\n    <span class=\"k\">for</span> <span class=\"n\">t</span> <span class=\"ow\">in</span> <span class=\"n\">term_freq</span><span class=\"o\">.</span><span class=\"n\">keys</span><span class=\"p\">():</span>\n        <span class=\"k\">if</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">t</span><span class=\"p\">)</span> <span class=\"o\">&gt;</span> <span class=\"mi\">1</span><span class=\"p\">:</span>\n            <span class=\"k\">try</span><span class=\"p\">:</span>\n                <span class=\"n\">doc_tf_idf_dict</span><span class=\"p\">[</span><span class=\"n\">t</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"o\">-</span><span class=\"n\">term_freq</span><span class=\"p\">[</span><span class=\"n\">t</span><span class=\"p\">]</span><span class=\"o\">*</span><span class=\"n\">term_idf_dict</span><span class=\"p\">[</span><span class=\"n\">t</span><span class=\"p\">]</span> <span class=\"c1\"># TF*IDF</span>\n            <span class=\"k\">except</span> <span class=\"ne\">KeyError</span><span class=\"p\">:</span>\n                <span class=\"k\">pass</span>\n    <span class=\"n\">sorted_list</span> <span class=\"o\">=</span> <span class=\"nb\">sorted</span><span class=\"p\">(</span><span class=\"n\">doc_tf_idf_dict</span><span class=\"o\">.</span><span class=\"n\">items</span><span class=\"p\">(),</span> <span class=\"n\">key</span><span class=\"o\">=</span><span class=\"n\">operator</span><span class=\"o\">.</span><span class=\"n\">itemgetter</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">reverse</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>  \n    <span class=\"k\">return</span> <span class=\"n\">sorted_list</span>\n\n<span class=\"n\">keyword_list</span> <span class=\"o\">=</span> <span class=\"n\">get_keyword</span><span class=\"p\">(</span><span class=\"mi\">72</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&#34;耗时：</span><span class=\"si\">%s</span><span class=\"s2\"> s&#34;</span> <span class=\"o\">%</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">time</span><span class=\"o\">.</span><span class=\"n\">time</span><span class=\"p\">()</span><span class=\"o\">-</span><span class=\"n\">start_time</span><span class=\"p\">))</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"mi\">50</span><span class=\"o\">*</span><span class=\"s2\">&#34;=&#34;</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&#34;词</span><span class=\"se\">\\t\\t\\t\\t</span><span class=\"s2\">tf-idf&#34;</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"mi\">50</span><span class=\"o\">*</span><span class=\"s2\">&#34;=&#34;</span><span class=\"p\">)</span>\n<span class=\"k\">for</span> <span class=\"n\">idx</span><span class=\"p\">,</span> <span class=\"n\">k</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">keyword_list</span><span class=\"p\">):</span>\n    <span class=\"k\">if</span> <span class=\"n\">idx</span> <span class=\"o\">&lt;</span> <span class=\"mi\">100</span><span class=\"p\">:</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&#34;</span><span class=\"si\">%-25s</span><span class=\"se\">\\t</span><span class=\"si\">%10s</span><span class=\"s2\">&#34;</span> <span class=\"o\">%</span> <span class=\"p\">(</span><span class=\"n\">k</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">k</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">])))</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"mi\">50</span><span class=\"o\">*</span><span class=\"s2\">&#34;-&#34;</span><span class=\"p\">)</span>\n    <span class=\"k\">else</span><span class=\"p\">:</span>\n        <span class=\"k\">break</span></code></pre></div><p>这里打印了第六十章的关键字，我先不说第六十章标题是什么，可以根据关键字猜测一下</p><div class=\"highlight\"><pre><code class=\"language-text\">耗时：4 s\n==================================================\n词                              tf-idf\n==================================================\n女子                            63.909833713109634\n--------------------------------------------------\n丝绳                            30.353362134321408\n--------------------------------------------------\n七个                            26.855508874019844\n--------------------------------------------------\n浴池                            26.575424759098897\n--------------------------------------------------\n石桥                            23.21928094887362\n--------------------------------------------------\n盘丝洞                          22.575424759098897\n--------------------------------------------------\n衣架                            20.235574756214273\n--------------------------------------------------\n热水                            19.931568569324174\n--------------------------------------------------\n仙姑                            19.931568569324174\n--------------------------------------------------\n丝篷                            19.931568569324174\n--------------------------------------------------\n亭子                            19.182506338585604\n--------------------------------------------------\n土地                            18.647236713895072\n--------------------------------------------------\n濯垢泉                          16.931568569324174\n--------------------------------------------------\n虫蛭                            16.931568569324174\n--------------------------------------------------\n布施                            15.346005070868483\n--------------------------------------------------\n吊起                            15.176681067160704\n--------------------------------------------------\n跟头                            15.176681067160704\n--------------------------------------------------\n高耸                            14.5754247590989\n--------------------------------------------------\n翠袖                            13.931568569324174\n--------------------------------------------------\n鲇鱼                            13.931568569324174\n--------------------------------------------------\n金莲                            13.287712379549449\n--------------------------------------------------\n如雪                            13.287712379549449\n--------------------------------------------------\n古树森                          13.287712379549449\n--------------------------------------------------\n层厚                            13.287712379549449\n--------------------------------------------------\n迷本                            13.287712379549449\n--------------------------------------------------\n七样                            13.287712379549449\n--------------------------------------------------\n遥长                            13.287712379549449\n--------------------------------------------------\n剥得                            13.287712379549449\n--------------------------------------------------\n牛蜢                            13.287712379549449\n--------------------------------------------------\n化顿                            13.287712379549449\n--------------------------------------------------\n盘丝岭                          13.287712379549449\n--------------------------------------------------\n千个                            13.287712379549449\n--------------------------------------------------\n七套                            13.287712379549449\n--------------------------------------------------\n气球                            13.287712379549449\n--------------------------------------------------\n此泉                            13.287712379549449\n--------------------------------------------------\n玉体                            13.287712379549449\n--------------------------------------------------\n婆儿                            13.287712379549449\n--------------------------------------------------\n第七十二回                      13.287712379549449\n--------------------------------------------------\n汤泉                            13.287712379549449\n--------------------------------------------------\n女怪                            12.176681067160704\n--------------------------------------------------\n佳人                            12.176681067160704\n--------------------------------------------------\n茅屋                            12.176681067160704\n--------------------------------------------------\n八戒                            11.562565014319137\n--------------------------------------------------\n洗澡                            11.509503803151361\n--------------------------------------------------\n钉住                            11.287712379549449\n--------------------------------------------------\n跌得                            11.287712379549449\n--------------------------------------------------\n老鹰                            11.287712379549449\n--------------------------------------------------\n洗洗                            11.287712379549449\n--------------------------------------------------\n路边                            11.287712379549449\n--------------------------------------------------\n七情                            11.287712379549449\n--------------------------------------------------\n飘扬                            11.287712379549449\n--------------------------------------------------\n退步                            11.287712379549449\n--------------------------------------------------\n百个                            11.287712379549449\n--------------------------------------------------\n木香                            11.287712379549449\n--------------------------------------------------\n长老                            10.706634659931115\n--------------------------------------------------\n露出                            10.421793564997238\n--------------------------------------------------\n化缘                            10.117787378107137\n--------------------------------------------------\n绊脚                            10.117787378107137\n--------------------------------------------------\n打断                            10.117787378107137\n--------------------------------------------------\n忘形                            10.117787378107137\n--------------------------------------------------\n女流                            10.117787378107137\n--------------------------------------------------\n化斋                            9.895724753329649\n--------------------------------------------------\n算计                            9.583714705324557\n--------------------------------------------------\n水里                            9.553273713412283\n--------------------------------------------------\n扶师父                          9.287712379549449\n--------------------------------------------------\n山泉                            9.287712379549449\n--------------------------------------------------\n赤条条                          9.287712379549449\n--------------------------------------------------\n古书                            9.287712379549449\n--------------------------------------------------\n衣服                                   9.0\n--------------------------------------------------\n站立                            8.643856189774725\n--------------------------------------------------\n断根                            8.643856189774725\n--------------------------------------------------\n山岭                            8.643856189774725\n--------------------------------------------------\n地脉                            8.643856189774725\n--------------------------------------------------\n洗浴                            8.643856189774725\n--------------------------------------------------\n推开                            8.509503803151361\n--------------------------------------------------\n下水                            8.509503803151361\n--------------------------------------------------\n过桥                            8.117787378107137\n--------------------------------------------------\n桃李                            8.117787378107137\n--------------------------------------------------\n脊背                            8.117787378107137\n--------------------------------------------------\n蛾眉                            7.673002535434241\n--------------------------------------------------\n蜻蜓                            7.673002535434241\n--------------------------------------------------\n肚皮                            7.673002535434241\n--------------------------------------------------\n那怪                            7.547331773069059\n--------------------------------------------------\n名头                            7.28771237954945\n--------------------------------------------------\n解下                            6.947862376664824\n--------------------------------------------------\n不满                            6.947862376664824\n--------------------------------------------------\n主张                            6.947862376664824\n--------------------------------------------------\n洞里                            6.792269854562382\n--------------------------------------------------\n儿子                            6.758639517551398\n--------------------------------------------------\n老儿                            6.754616300987894\n--------------------------------------------------\n十个                            6.643856189774724\n--------------------------------------------------\n斋僧                            6.643856189774724\n--------------------------------------------------\n地积                            6.643856189774724\n--------------------------------------------------\n跌恼                            6.643856189774724\n--------------------------------------------------\n疑粉                            6.643856189774724\n--------------------------------------------------\n动棍                            6.643856189774724\n--------------------------------------------------\n开脚                            6.643856189774724\n--------------------------------------------------\n九处                            6.643856189774724\n--------------------------------------------------\n横远                            6.643856189774724\n--------------------------------------------------\n铺里                            6.643856189774724\n--------------------------------------------------</code></pre></div><p>效果还算不错，最起码看过西游记的，看了前十个词基本上就知道这一章是讲什么的，这只是一个实验性质的程序，实际情况中一般文档数量会不断增加，所以需要不断更新<code>idf.pkl</code>文件，同时停用词处理还是有必要的。</p><hr/><p><br/> </p><p>参考文献：</p><p><a href=\"https://link.zhihu.com/?target=https%3A//ccc.inaoep.mx/~villasen/index_archivos/cursoTL/articulos/Aizawa-tf-idfMeasures.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">An information-theoretic perspective of tf–idf measures, Akiko Aizawa</a></p>", 
            "topic": [
                {
                    "tag": "自然语言处理", 
                    "tagLink": "https://api.zhihu.com/topics/19560026"
                }, 
                {
                    "tag": "信息检索", 
                    "tagLink": "https://api.zhihu.com/topics/19580199"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/56960502", 
            "userName": "右左瓜子", 
            "userLink": "https://www.zhihu.com/people/6107fd0900473aae4e5ec6824b4fde4a", 
            "upvote": 14, 
            "title": "用BERT做情感分析", 
            "content": "<p>诸如情感分析一类的任务比如商品评价正负面分析，敏感内容分析，用户感兴趣内容分析、甚至安全领域的异常访问日志分析等等实际上都可以用文本分类的方式去做，本质上来讲就是一个文本输出一个多个对应的标签。</p><p>这一类任务BERT原文中用的是斯坦福的treebank，在这里我们还是用双向的LSTM网络来实现，因为前面的实体标注的内容中使用的是双向LSTM加CRF，稍加改造就可以很轻松的解决文本分类任务。</p><h2>一、计算过程</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-25ddf44bcb88ee2b2bc1e7d004d81010_b.jpg\" data-rawwidth=\"3106\" data-rawheight=\"2620\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"3106\" data-original=\"https://pic1.zhimg.com/v2-25ddf44bcb88ee2b2bc1e7d004d81010_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;3106&#39; height=&#39;2620&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"3106\" data-rawheight=\"2620\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"3106\" data-original=\"https://pic1.zhimg.com/v2-25ddf44bcb88ee2b2bc1e7d004d81010_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-25ddf44bcb88ee2b2bc1e7d004d81010_b.jpg\"/></figure><p>整个前向传播计算过程如上图所示，和实体识别的程序做了对比，方便看出来如何在实体识别程序的基础上做简单的修改让程序可以处理文本分类的问题。<code>BERT</code>部分和 <code>biLstm</code>部分都没有变化，关键的地方在于<code>BiLstm</code>的输出结果，因为文本分类只需要相对于整句的标签，所以直接把结果展开然后做线性变化，实际上也可以在线性变化上再加一层激活层但是只要你能保证前向传播和后向传播可以顺利进行，得到的结果是一个可以转化为标签的结果，而不是连你自己也不知道是什么东西的结果即可。不过实验证明并没有什么提升，前面的网络已经足够了。</p><h2>二、代码</h2><p>代码上在前面一节的代码的基础上做一些修改，主要是`Bilstm`网络输出的处理以及新的损失函数。</p><div class=\"highlight\"><pre><code class=\"language-python3\">    <span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">transpose</span><span class=\"p\">(</span><span class=\"n\">output</span><span class=\"p\">,</span> <span class=\"n\">perm</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">])</span>\n    <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">info</span><span class=\"p\">(</span><span class=\"n\">output</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span>\n    <span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">dropout</span><span class=\"p\">(</span><span class=\"n\">output</span><span class=\"p\">,</span> <span class=\"n\">rate</span><span class=\"o\">=</span><span class=\"mf\">0.5</span><span class=\"p\">,</span> <span class=\"n\">training</span><span class=\"o\">=</span><span class=\"n\">is_training</span><span class=\"p\">)</span>\n    <span class=\"n\">output</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"n\">output</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">128</span><span class=\"o\">*</span><span class=\"mi\">256</span><span class=\"p\">])</span>\n    <span class=\"n\">output_weights</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">get_variable</span><span class=\"p\">(</span>\n      <span class=\"s2\">&#34;output_weights&#34;</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"n\">num_labels</span><span class=\"p\">,</span> <span class=\"mi\">128</span><span class=\"o\">*</span><span class=\"mi\">256</span><span class=\"p\">],</span>\n      <span class=\"n\">initializer</span><span class=\"o\">=</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">truncated_normal_initializer</span><span class=\"p\">(</span><span class=\"n\">stddev</span><span class=\"o\">=</span><span class=\"mf\">0.02</span><span class=\"p\">))</span>\n\n    <span class=\"n\">output_bias</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">get_variable</span><span class=\"p\">(</span>\n        <span class=\"s2\">&#34;output_bias&#34;</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"n\">num_labels</span><span class=\"p\">],</span> <span class=\"n\">initializer</span><span class=\"o\">=</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">zeros_initializer</span><span class=\"p\">())</span>\n    <span class=\"n\">logits</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">output</span><span class=\"p\">,</span> <span class=\"n\">output_weights</span><span class=\"p\">,</span> <span class=\"n\">transpose_b</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n    <span class=\"n\">logits</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">bias_add</span><span class=\"p\">(</span><span class=\"n\">logits</span><span class=\"p\">,</span> <span class=\"n\">output_bias</span><span class=\"p\">)</span>\n    <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">info</span><span class=\"p\">(</span><span class=\"s2\">&#34;*****shape of label_ids******&#34;</span><span class=\"p\">)</span>\n    <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">info</span><span class=\"p\">(</span><span class=\"n\">label_ids</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span>\n    <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">logging</span><span class=\"o\">.</span><span class=\"n\">info</span><span class=\"p\">(</span><span class=\"n\">logits</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span>\n\n    <span class=\"n\">correctPred</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">equal</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">argmax</span><span class=\"p\">(</span><span class=\"n\">logits</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">argmax</span><span class=\"p\">(</span><span class=\"n\">label_ids</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">))</span> <span class=\"c1\"># tf.argmax: Returns the index with the largest value across axes of a tensor. </span>\n    <span class=\"n\">accuracy</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">reduce_mean</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">cast</span><span class=\"p\">(</span><span class=\"n\">correctPred</span><span class=\"p\">,</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">))</span>\n    <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">summary</span><span class=\"o\">.</span><span class=\"n\">scalar</span><span class=\"p\">(</span><span class=\"s1\">&#39;Accuracy&#39;</span><span class=\"p\">,</span> <span class=\"n\">accuracy</span><span class=\"p\">)</span>\n    <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">reduce_mean</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">softmax_cross_entropy_with_logits</span><span class=\"p\">(</span><span class=\"n\">logits</span><span class=\"o\">=</span><span class=\"n\">logits</span><span class=\"p\">,</span> <span class=\"n\">labels</span><span class=\"o\">=</span><span class=\"n\">label_ids</span><span class=\"p\">))</span></code></pre></div><p> 有一点需要注意的是<code>tf.argmax()</code>函数，我们之前取标签使用的是<code>tensorflow</code>自带的<code>crf</code>的方法，它可以直接输出标签，但是在这里没有crf所以我们要自己把对应的标签取出来，也就是把最后输出的长度为<code>num_len</code>的向量中值最大的索引找出来，这个索引对应的就是标签的索引。当然，除了计算过程改变了之外，前面的输入数据格式整理也是必需的这里就不再赘述，主要是把一个句子对应的多标签改成单个标签，完整代码在这里</p><a data-draft-node=\"block\" data-draft-type=\"link-card\" href=\"https://link.zhihu.com/?target=https%3A//github.com/cedar33/bert_ner/blob/master/bert_senta\" data-image=\"https://pic1.zhimg.com/v2-114a23bf697d72929d55cb0ddb9981a8_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">cedar33/bert_ner</a><p>上一节提到了<code>estimator</code>但是发出来之后会看发现并没有介绍这一块，所以这里补一点。<code>estimator</code>主要是为了方便开发者之关系算法构建的核心部分，把其他的事情交给<code>tensorflow</code>来处理。使用<code>estimator</code>我们只需要写好前期输入数据整理的程序<code>inpu_fn</code> 和模型的计算过程<code>model_fn</code>,摘出上面代码中的片段看一看</p><div class=\"highlight\"><pre><code class=\"language-text\">   estimator = tf.contrib.tpu.TPUEstimator(\n      use_tpu=False,\n      model_fn=model_fn,\n      config=run_config,\n      train_batch_size=FLAGS.train_batch_size,\n      eval_batch_size=FLAGS.eval_batch_size,\n      predict_batch_size=FLAGS.predict_batch_size,\n      params=params)</code></pre></div><p>这是<code>estimator</code>的构造方法，因为谷歌给出的bert的例子中使用了TPU训练，所以这里构建了一个TPU的<code>estimator</code>实际计算的时候如果没有TPU会自动转化为一般的<code>estimator</code>,<code>model_fn</code>方法定义了计算过程 ,其他的参数比较好理解就不多说了，最后一个<code>param</code>参数比较特殊，当<code>model_fn</code>中需要的参数 <code>estimator </code>的方法签名中没有的时候使用，<code>estimator</code>会把这个参数传递给<code>model_fn</code>，注意到这里没有<code>input_fn</code>,因为这里只是在构建计算过程，并没有真正开始训练，<code>tensorflow</code>在训练之前会先构建好计算图，整个计算图前向传播和后向传播能跑通才会输入数据进行计算。</p><div class=\"highlight\"><pre><code class=\"language-text\">     train_input_fn = file_based_input_fn_builder(\n        input_file=train_file,\n        seq_length=FLAGS.max_seq_length,\n        is_training=True,\n        drop_remainder=True)\n    eval_input_fn = file_based_input_fn_builder(\n        input_file=eval_file,\n        seq_length=FLAGS.max_seq_length,\n        is_training=True,\n        drop_remainder=True)\n    # estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n    hook = tf.contrib.estimator.stop_if_no_increase_hook(\n        estimator, &#39;loss&#39;, 100000, min_steps=50000, run_every_secs=300)\n    train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, hooks=[hook])\n    eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_fn, throttle_secs=300)\n    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)</code></pre></div><p><code>estimator</code>构建好之后就可以跑起来训练了，这些方法看方法名基本上都能知道是干嘛的了，有一个要注意的是<code>hook</code>,<code>hook</code>可以看作一个管理训练过程的工具，比如说这里就是设置提前终止的条件，变量<code>loss</code>在100000步以内没有下降即终止，实际上更广泛的用法是用在对测试集的<code>f1</code>值上，有兴趣可以尝试一下。 </p><p>我们花了一些功夫学习<code>estimator</code>除了自动化管理训练过程之外还有一点就是方便使用<code>tensorflowserver</code>部署接口，在使用<code>estimator</code>之前我们把训练好的算法部署到网络端口还是很麻烦的，有多麻烦呢？麻烦到我之前都懒得折腾他直接用<code>flask</code>写了个接口调用算法。我们看看怎么用<code>estimator</code>部署，首先导出模型。 </p><div class=\"highlight\"><pre><code class=\"language-text\">     estimator = tf.contrib.tpu.TPUEstimator(\n      use_tpu=False,\n      model_fn=model_fn,\n      config=run_config,\n      train_batch_size=32,\n      eval_batch_size=32,\n      predict_batch_size=32,\n      params=params)\n    estimator._export_to_tpu = False\n    estimator.export_savedmodel(&#39;senta&#39;, serving_input_receiver_fn)</code></pre></div><p>把之前的<code>estimator</code> 再构造一遍，然后调用方法就可以了，因为在网络端的输入数据和训练时的数据输入方式不同，所以我们还要给他加一个定制的输入方程，定义了如何解析通过接口传入的数据，即<code>serving_input_receiver_fn</code> </p><div class=\"highlight\"><pre><code class=\"language-text\">def serving_input_receiver_fn():\n    &#34;&#34;&#34;Serving input_fn that builds features from placeholders\n    Returns\n    -------\n    tf.estimator.export.ServingInputReceiver\n    &#34;&#34;&#34;\n    # feature = InputFeatures(\n    # input_ids=input_ids,\n    # input_mask=input_mask,\n    # segment_ids=segment_ids,\n    # label_ids=label_id,\n    # seq_length = seq_length,\n    # is_real_example=True)\n\n    input_ids = tf.placeholder(dtype=tf.int32, shape=[None, None], name=&#39;input_ids&#39;)\n    input_mask = tf.placeholder(dtype=tf.int32, shape=[None, None], name=&#39;input_mask&#39;)\n    segment_ids = tf.placeholder(dtype=tf.int32, shape=[None, None], name=&#39;segment_ids&#39;)\n    label_ids = tf.placeholder(dtype=tf.int32, shape=[None], name=&#39;label_ids&#39;)\n    seq_length = tf.placeholder(dtype=tf.int32, shape=[None], name=&#39;seq_length&#39;)\n    is_real_example = tf.placeholder(dtype=tf.string, shape=[None], name=&#39;is_real_example&#39;)\n    receiver_tensors = {&#39;input_ids&#39;: input_ids,\n                            &#39;input_mask&#39;: input_mask,\n                            &#39;segment_ids&#39;: segment_ids,\n                            &#39;label_ids&#39;:label_ids,\n                            &#39;seq_length&#39;:seq_length,\n                            &#39;is_real_example&#39;:is_real_example}\n    features = {&#39;input_ids&#39;: input_ids,\n                            &#39;input_mask&#39;: input_mask,\n                            &#39;segment_ids&#39;: segment_ids,\n                            &#39;label_ids&#39;:label_ids,\n                            &#39;seq_length&#39;:seq_length,\n                            &#39;is_real_example&#39;:is_real_example}\n    return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)</code></pre></div><p>就这么多，然后调用脚本部署就可以了，github链接中有，都是死东西这里不再赘述。部署好之后有grpc的接口也有rest的接口，链接中的<code>client.py</code>展示了如何调用rest接口。源码中给的<code>export.py</code>依旧是实体识别的，如果要导出情感分析的可以自行修改，也比较简单。</p><h2>三、文本分类处理的一些方法</h2><p>文本分类的问题最大的难点应该还是样本的问题，分词和实体词识别一般都能找到公开的比较好的样本，二文本分类问题却不一样，文本分类问题往往是特定领域的针对特定问题的分类，如果是在有钱的大公司还好，出点儿钱，请点儿人标注一批，但是大部分情况都需要工程师和相关领域的专业人员去找样本。记得阿里巴巴在一篇文章提到协助法院处理法律文件的时候也是领域人员和工程师一起去通过各种方式找样本。废话不多说了，找样本一般有三个思路：1.关键词匹配，跟一些同行聊到这些的时候有种感觉就是大部分人都觉得关键词匹配很丢人，这不是做算法的人该干的事儿，实际上，丢人不？丢人！想不想要钱？想！那丢人也得干。通过强特征关键词匹配的样本应该是质量比较高的样本，我们用关键词匹配样本的关键在于算法不仅能学习到关键词信息，而且也能学到关键词以外的语法上的信息，来达到优于直接关键词提取的效果。2.规则匹配，这里的规则匹配不同于关键词匹配，泛指一切能把文本分类的规则，比如再twitter打击假新闻的任务中，所有已知机器人的twitter信息和账号信息都可以作为一个分类的样本，同样的，确认的真实人物或者机构发表的新闻和信息都可以作为另一个分类的样本。3.自然语言生成：一前这种方法只是一种理论上存在的方法，因为一前自然语言生成技术得出的结果比较辣眼睛，但是现在随着新技术一茬接一茬，上一个月BERT刚打败OPENAI-GPT还大红大紫，这个月OPENAI-GPT2就又重新找回了场子，有一个好的语言模型，自然语言生成的结果也应该会有很大提升，所以这一块也能起到一部分帮助。</p>", 
            "topic": [
                {
                    "tag": "自然语言处理", 
                    "tagLink": "https://api.zhihu.com/topics/19560026"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": [
                {
                    "userName": "顿悟的顿", 
                    "userLink": "https://www.zhihu.com/people/241e1e3b00d98c09403f9925730fe296", 
                    "content": "<p>您好我是小白， 有两个问题想请教下。 1。 这个在情感分析的框架中bert起了什么作用 2. 相对于没有用bert 的情感分析流程，这个流程有什么好处或者性能改进的地方。 </p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "才学会飞行", 
                            "userLink": "https://www.zhihu.com/people/cb2770a431c353059af7271dfb4401d1", 
                            "content": "<p>同小白，但是感觉我可以说上些原因。Bert做了一件什么事儿? 如下：input-bert-output，input的是句子token后的结果（one-hot过），output的是对应token后的编码，所以Bert直白来讲就是一个无监督的编码器。为什么用它？个人找代码时看到一个说法（<a href=\"http://link.zhihu.com/?target=https%3A//github.com/Socialbird-AILab/BERT-Classification-Tutorial\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/Socialbird-A</span><span class=\"invisible\">ILab/BERT-Classification-Tutorial</span><span class=\"ellipsis\"></span></a>），很多任务面临人工标注的训练集不够的问题，分类模型加入Bert后，即使在训练数据集较少的情况下，也能取得不错的效果。恩，有错误请指正，刚看完Bert....</p>", 
                            "likes": 0, 
                            "replyToAuthor": "顿悟的顿"
                        }
                    ]
                }, 
                {
                    "userName": "纽约的自行车", 
                    "userLink": "https://www.zhihu.com/people/5eff850c9bcef196258529177eed98fb", 
                    "content": "<p>GitHub链接没了。。。</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/55491448", 
            "userName": "右左瓜子", 
            "userLink": "https://www.zhihu.com/people/6107fd0900473aae4e5ec6824b4fde4a", 
            "upvote": 23, 
            "title": "使用estimator构建基于BERT的实体识别神经网络", 
            "content": "<p>实际上基于bert的实体识别无论理论还是代码实现应该都是比较简单的，bert的代码实现本来是比较复杂的，因为涉及到mask相关的东西，但是google已经给出来了，并且还很良心的给出了一些上游任务的例子，所以基本上没什么难点。所以这里就主要记一下使用estimator构建网络的过程。因为相关主流的算法github上都有开源实现，所以以前基本上都是copy过来（像一个loser）小修小改就行了，这次虽然已经有开源实现了，但是还是想尝试一下自己码一下。代码地址如下：</p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/cedar33/bert_ner\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-114a23bf697d72929d55cb0ddb9981a8_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">cedar33/bert_ner</a><p>## 1. bert部分</p><p>bert前面已经介绍过了，在这里我们只需要拿到它隐状态的输出即可，如何拿到bert源代码已经给我们方法了</p><div class=\"highlight\"><pre><code class=\"language-python3\">  <span class=\"n\">model</span> <span class=\"o\">=</span> <span class=\"n\">modeling</span><span class=\"o\">.</span><span class=\"n\">BertModel</span><span class=\"p\">(</span>\n      <span class=\"n\">config</span><span class=\"o\">=</span><span class=\"n\">bert_config</span><span class=\"p\">,</span>\n      <span class=\"n\">is_training</span><span class=\"o\">=</span><span class=\"n\">is_training</span><span class=\"p\">,</span> <span class=\"c1\"># 这个参数在`bilm+crf`训练的时候也需要训练</span>\n      <span class=\"n\">input_ids</span><span class=\"o\">=</span><span class=\"n\">input_ids</span><span class=\"p\">,</span>\n      <span class=\"n\">input_mask</span><span class=\"o\">=</span><span class=\"n\">input_mask</span><span class=\"p\">,</span>\n      <span class=\"n\">token_type_ids</span><span class=\"o\">=</span><span class=\"n\">segment_ids</span><span class=\"p\">,</span>\n      <span class=\"n\">use_one_hot_embeddings</span><span class=\"o\">=</span><span class=\"n\">use_one_hot_embeddings</span><span class=\"p\">)</span>\n\n  <span class=\"n\">embedding</span> <span class=\"o\">=</span> <span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">get_sequence_output</span><span class=\"p\">()</span> <span class=\"c1\"># 拿到最后一层隐状态的方法</span></code></pre></div><p> 其实看了bert的代码，可以给自己写代码如何分层分包有很大的启示，再看看自己的代码感觉差的还很远，虽然有很多忙啊赶进度啊之类的借口但是这应该是最基本的。好了言归正传，这里有一个地方值得说一下，首先是&#39;&#39;&#39;is_training&#39;&#39;&#39;这个参数，我原以为这个参数在pre-training和fine-tuning的时候训练好了不用再动了，后来看了bert给出的&#39;&#39;&#39;run_squad.py&#39;&#39;&#39;的方法发现这个参数在特定任务中仍然会训练，这就跟之前的word2vec、glove以及elmo稍有区别，当然如果限于硬件原因也可以尝试把所有字的向量形式输出保存然后在特定任务中再加载，不训练bert模型，但是效果没有同时训练好。bert部分的transformer如果有比较好的gpu的话速度会快很多，亲测大概是20倍的加速。</p><p>bert部分输出拿到了，剩下的想怎么折腾就怎么折腾了跟之前的word2vec之类的方法无异，因为在标注任务中是不需要[cls]、[sep]这样的标签的，作者的文章中也没有做position embedding我也没做过实验。</p><h2> 2. bilstm+crf部分</h2><p>虽然bilstm+crf已经是老生常谈了，但是这里还是谈一谈，实际上自己码一遍代码会对整个网络有一个更深刻的认识，数据流在每一个节点的状态，哪些参数需要训练哪些不需要都会了然于胸,顺便介绍一个<code>tensorflow</code>中lstm单元的高级封装<code>LSTMBlockFusedCell</code>以便在CPU上表现得更好，如果GPU空间足够就是用<code>LSTMBlockCell</code>。</p><p>关于理论部分这两篇文章介绍的很好，基本上所有lstm和crf相关的内容都能在这里找到答案，图文并茂，良心至极，放在这里</p><a href=\"https://link.zhihu.com/?target=http%3A//colah.github.io/posts/2015-08-Understanding-LSTMs/\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-684601aa63886d86a1b4dafcf8ab079c_120x160.jpg\" data-image-width=\"458\" data-image-height=\"711\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Understanding LSTM Networks</a><a href=\"https://link.zhihu.com/?target=https%3A//guillaumegenthial.github.io/sequence-tagging-with-tensorflow.html\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-8ff1496ca26e452efe22eee9ef6d33ee_180x120.jpg\" data-image-width=\"1000\" data-image-height=\"775\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Sequence Tagging with Tensorflow</a><p>如果发现链接通向一堵墙的话就勉为其难的看看我的介绍吧。</p><h2>1.lstm</h2><p>这里假定你已经对lstm有一定了解，所以不会对单元的内部情况进行剖析，只是宏观上看一看，主要是看一看代码怎么调用的</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-7cf27cdbeb9fc616eabd29131de50d17_b.jpg\" data-size=\"normal\" data-rawwidth=\"2233\" data-rawheight=\"839\" class=\"origin_image zh-lightbox-thumb\" width=\"2233\" data-original=\"https://pic4.zhimg.com/v2-7cf27cdbeb9fc616eabd29131de50d17_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2233&#39; height=&#39;839&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"2233\" data-rawheight=\"839\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2233\" data-original=\"https://pic4.zhimg.com/v2-7cf27cdbeb9fc616eabd29131de50d17_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-7cf27cdbeb9fc616eabd29131de50d17_b.jpg\"/><figcaption>转载自colah的github.io，lstm网络</figcaption></figure><p>先宏观的看一下，输入即是bert的输出，bert的中文预训练模型给出的输出每一个字由一个长度为768的向量表示，所以bert的输出是<code>shape=[batch_size, max_seq_length, 768]</code>的张量作为lstm的输入，lstm的输出是一个和lstm单元个数相关的张量<code>shape=[batch_size, max_seq_length, lstm_size]</code> 。在这张图中肉眼可见的需要指定的参数有两个，一个是*A*的个数，一个是输出 <img src=\"https://www.zhihu.com/equation?tex=h_t\" alt=\"h_t\" eeimg=\"1\"/> 的形状，<code>LSTMBlockFusedCell</code>的初始化方法和call方法分别指定了这两个参数，把方法签名列一下</p><div class=\"highlight\"><pre><code class=\"language-text\">__init__(\n    num_units,\n    forget_bias=1.0,\n    cell_clip=None,\n    use_peephole=False,\n    reuse=None,\n    dtype=None,\n    name=&#39;lstm_fused_cell&#39;\n)\n\n__call__(\n    inputs,\n    *args,\n    **kwargs\n)</code></pre></div><p>好像并不能明显的看出在哪里指定这两个参数，实际上 <img src=\"https://www.zhihu.com/equation?tex=h_t\" alt=\"h_t\" eeimg=\"1\"/> 的计算公式</p><p><img src=\"https://www.zhihu.com/equation?tex=h_t%3Do_t%2A%5Ctanh%28C_t%29\" alt=\"h_t=o_t*\\tanh(C_t)\" eeimg=\"1\"/> </p><p>告诉我们 <img src=\"https://www.zhihu.com/equation?tex=h_t\" alt=\"h_t\" eeimg=\"1\"/> 的形状是和 <img src=\"https://www.zhihu.com/equation?tex=%5Ctanh\" alt=\"\\tanh\" eeimg=\"1\"/> 函数输入变量的形状相等，在初始化方法中定义了参数<code>num_units</code>这个参数就是来规定 <img src=\"https://www.zhihu.com/equation?tex=%5Ctanh\" alt=\"\\tanh\" eeimg=\"1\"/> 函数的输入变量的大小，所以上面提到的输出 <img src=\"https://www.zhihu.com/equation?tex=h_t\" alt=\"h_t\" eeimg=\"1\"/> 的形状<code>shape=[batch_size, max_seq_length, lstm_size]</code>就由此而来(<code>lstm_size</code>和<code>num_units</code>描述的是同一个东西)，有一个有意思的因果关系是<code>num_units</code>到底是来定义 <img src=\"https://www.zhihu.com/equation?tex=%5Ctanh\" alt=\"\\tanh\" eeimg=\"1\"/> 的属性，然后因为这个属性所以 <img src=\"https://www.zhihu.com/equation?tex=%5Ctanh\" alt=\"\\tanh\" eeimg=\"1\"/> 的输入的大小必须满足这个属性还是这个参数是来定义输入的的大小的，所以 <img src=\"https://www.zhihu.com/equation?tex=%5Ctanh\" alt=\"\\tanh\" eeimg=\"1\"/> 函数具备这个属性。虽然从代码的角度来讲这不重要，因为他们是相等的，所以我定义一个另一个就定了不用管什么因果关系，但是实际上这个因果关系对lstm的理解很重要。<code>num_units</code>字面上看就是神经网络单元的个数，而神经网络单元一般指的是 <img src=\"https://www.zhihu.com/equation?tex=sigmoid\" alt=\"sigmoid\" eeimg=\"1\"/> 或者 <img src=\"https://www.zhihu.com/equation?tex=%5Ctanh\" alt=\"\\tanh\" eeimg=\"1\"/> （当然还有其他种类的神经元，反正不是矩阵或者向量），所以应该定义的是 <img src=\"https://www.zhihu.com/equation?tex=%5Ctanh\" alt=\"\\tanh\" eeimg=\"1\"/> 的一个属性，然后这个属性决定了输入的大小。<code>num_units</code>通常对函数的拟合能力影响很大一般选择128，根据输入特征的大小和样本数量可以做适当的调整。</p><h2>2.bilstm</h2><p>双向的lstm，老生常谈了，序列翻过来同样的方式算一遍然后concat就可以了</p><div class=\"highlight\"><pre><code class=\"language-text\">    t = tf.transpose(embeddings, perm=[1, 0, 2])\n    lstm_cell_fw = tf.contrib.rnn.LSTMBlockFusedCell(FLAGS.max_seq_length) \n    lstm_cell_bw = tf.contrib.rnn.LSTMBlockFusedCell(FLAGS.max_seq_length)\n    lstm_cell_bw = tf.contrib.rnn.TimeReversedFusedRNN(lstm_cell_bw)\n    output_fw, _ = lstm_cell_fw(t, dtype=tf.float32, sequence_length=seq_length)\n    output_bw, _ = lstm_cell_bw(t, dtype=tf.float32, sequence_length=seq_length)\n    output = tf.concat([output_fw, output_bw], axis=-1)\n    output = tf.transpose(output, perm=[1, 0, 2])\n    output = tf.layers.dropout(output, rate=0.5, training=is_training)</code></pre></div><p>有两个地方值得说一下，一个是<code>tf.concat</code>操作，在前面谈到bert的时候我们提到过bert是可以真正意义上实现从上下文推断当前词的模型，bilstm为什么不行呢？bilstm通过上下文推断了啊。然而bilstm的结合只是两个单向推断后简单的concat的结果，实际上每个推断依然是单向的，而bert模型通过mask的方法真正意义上实现了通过上下文推断（bert相关的可以翻一下前面的文章）。</p><p>另一个值得注意的地方是如果使用tensorflow中最基本的lstm单元的话输出是不需要转置的，而<code>LSTMBlockFusedCell</code>得call方法中inputs得shape是 <code>[time_len, batch_size, input_size]</code>所以需要转置。</p><h2>3.crf</h2><p>bilstm+crf和纯种得crf得区别在于bilstm+crf模型中用bilstm取代了纯种crf中的势函数，利用了crf的状态转移概率和解码方法取长补短。代码实现相当简单</p><div class=\"highlight\"><pre><code class=\"language-text\">    logits = tf.layers.dense(output, 5)\n    crf_params = tf.get_variable(&#34;crf&#34;, [5, 5], dtype=tf.float32)\n    trans = tf.get_variable(\n        &#34;transitions&#34;,\n        shape=[num_labels, num_labels],\n        initializer=initializers.xavier_initializer())\n    pred_ids, trans = tf.contrib.crf.crf_decode(logits, crf_params, seq_length)\n    log_likelihood, _ = tf.contrib.crf.crf_log_likelihood(\n        logits, label_ids, seq_length, crf_params)\n    loss = tf.reduce_mean(-log_likelihood)</code></pre></div><p>上面两段代码基本上就是构建整个网络的核心方法，加起来应该不到20行，加上dropout，dense等操作也就30行的样子，然而整个程序代码大概有600多行，大量的代码其实是花在业务逻辑处理和文件读写以及格式整理上。还有一个必须要提到的问题是在构建网络的时候所有的计算都是在一个单独的域中做的</p><div class=\"highlight\"><pre><code class=\"language-text\">with tf.variable_scope(&#39;Graph&#39;, reuse=None, custom_getter=None):</code></pre></div><p>可能比较熟悉tensorflow的认为这不值一提，但是这个问题着实坑了我一把，而且会导致比较可怕的结果。如果忽略这一行，程序不会报错，梯度会下降，但是只会下降到一个很差的结果上。打印出被训练的变量看不出异常，准确率还没有猜得准。之所以说很可怕是因为这个时候程序不报错，一切无异常，梯度还下降只不过没下降到足够低，很容易的去调其他超参数，但是都是徒劳的，然后如果你就很可能会认为你的样本有问题，是不可训练的，不能用这种方法得到一个比较好的结果，然后准备放弃这种方法找寻其他答案，然而实际上只是因为你的代码少了一行，不要问我这个心理活动我是怎么知道的。这个时候最有效的办法是看观察计算图（实际上只要是没有报错的异常都可以看计算图解决）。</p><h2>3. 效果</h2><p>在实体识别方面bert在测试集大概有2%左右的提升对于大公司这是肉眼可见的利润，对于小公司主要是可以在小样本上表现得很好，没有人力做大量数据标注的时候也是很不错的选择。</p>", 
            "topic": [
                {
                    "tag": "自然语言处理", 
                    "tagLink": "https://api.zhihu.com/topics/19560026"
                }, 
                {
                    "tag": "命名实体识别", 
                    "tagLink": "https://api.zhihu.com/topics/19648557"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/52692816", 
            "userName": "右左瓜子", 
            "userLink": "https://www.zhihu.com/people/6107fd0900473aae4e5ec6824b4fde4a", 
            "upvote": 12, 
            "title": "从transformer到OpenAI GPT到BERT", 
            "content": "<h2>从transformer到OpenAI GPT到BERT</h2><p>上一片介绍了<b>BERT</b>，如果之前没有接触过transformer的话可能会看的云里雾里，这一篇就简单的顺着BERT作者的路线，介绍这三个模型。</p><h2>1 transformers</h2><p>先说一下，这里的transformers跟变形金刚没有什么关系。transformer的牛逼之处在于摒弃了之前的CNN和RNN结构，但是效果依然不错，论文中讲在翻译任务中达到了state-of-the-art的结果并且可以通过并行运算显著提升效率。转换器模型和以往的模型不同，没有使用递归神经网络，而且也并不是完全依靠注意力机制来描述输入和输出的依赖关系，这可以更高效的进行并行运算。</p><p>直接看看transformer的结构图 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-07e680192f54919265aa5ed8664d34f3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"654\" data-rawheight=\"914\" class=\"origin_image zh-lightbox-thumb\" width=\"654\" data-original=\"https://pic4.zhimg.com/v2-07e680192f54919265aa5ed8664d34f3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;654&#39; height=&#39;914&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"654\" data-rawheight=\"914\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"654\" data-original=\"https://pic4.zhimg.com/v2-07e680192f54919265aa5ed8664d34f3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-07e680192f54919265aa5ed8664d34f3_b.jpg\"/></figure><p> 转换器具有编码器-解码器的结构，编码器有 <img src=\"https://www.zhihu.com/equation?tex=N%3D6\" alt=\"N=6\" eeimg=\"1\"/> 个完全相同的层组成，每个层有两个子层，第一个是<b>多头自我注意力机制(multi-head self-attention mechanism)</b>，第二层是一个简单的positionwise(这个名词后面会有解释)的全连接前馈网络。在两个子层外添加了残差连接，然后使用了归一化处理。也就是说，每一次子层的输出是 <img src=\"https://www.zhihu.com/equation?tex=LayerNorm%28x%2BSublayer%28x%29%29\" alt=\"LayerNorm(x+Sublayer(x))\" eeimg=\"1\"/> ，而 <img src=\"https://www.zhihu.com/equation?tex=Sublayer%28x%29\" alt=\"Sublayer(x)\" eeimg=\"1\"/> 是一个基于子层自身的函数。为了让残差连接可以计算，所有子层的输出的维度都和嵌入层相同即 <img src=\"https://www.zhihu.com/equation?tex=d_%7Bmodel%7D%3D512\" alt=\"d_{model}=512\" eeimg=\"1\"/> 。至于输入如何一分为三称为海王的三叉戟的后面会谈到。</p><p>解码器也是由 <img src=\"https://www.zhihu.com/equation?tex=N%3D6\" alt=\"N=6\" eeimg=\"1\"/> 个完全相同的块组成，有一点需要注意的是这里编码器和解码器的层是如何叠加在一起的，从图和论文中我都没找到比较明确的答案，所以只能再代码中找。上一个代码片段：</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"k\">def</span> <span class=\"nf\">transformer_decoder</span><span class=\"p\">(</span><span class=\"n\">decoder_input</span><span class=\"p\">,</span>\n                        <span class=\"n\">encoder_output</span><span class=\"p\">,</span>\n                        <span class=\"n\">decoder_self_attention_bias</span><span class=\"p\">,</span>\n                        <span class=\"n\">encoder_decoder_attention_bias</span><span class=\"p\">,</span>\n                        <span class=\"n\">hparams</span><span class=\"p\">,</span>\n                        <span class=\"n\">cache</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span>\n                        <span class=\"n\">decode_loop_step</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span>\n                        <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s2\">&#34;decoder&#34;</span><span class=\"p\">,</span>\n                        <span class=\"n\">nonpadding</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span>\n                        <span class=\"n\">save_weights_to</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span>\n                        <span class=\"n\">make_image_summary</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span>\n                        <span class=\"n\">losses</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">):</span>\n  <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">decoder_input</span>\n  <span class=\"o\">......</span>\n  <span class=\"k\">with</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">variable_scope</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"p\">):</span>\n    <span class=\"k\">for</span> <span class=\"n\">layer</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">hparams</span><span class=\"o\">.</span><span class=\"n\">num_decoder_layers</span> <span class=\"ow\">or</span> <span class=\"n\">hparams</span><span class=\"o\">.</span><span class=\"n\">num_hidden_layers</span><span class=\"p\">):</span>\n      <span class=\"n\">layer_name</span> <span class=\"o\">=</span> <span class=\"s2\">&#34;layer_</span><span class=\"si\">%d</span><span class=\"s2\">&#34;</span> <span class=\"o\">%</span> <span class=\"n\">layer</span>\n      <span class=\"n\">layer_cache</span> <span class=\"o\">=</span> <span class=\"n\">cache</span><span class=\"p\">[</span><span class=\"n\">layer_name</span><span class=\"p\">]</span> <span class=\"k\">if</span> <span class=\"n\">cache</span> <span class=\"ow\">is</span> <span class=\"ow\">not</span> <span class=\"kc\">None</span> <span class=\"k\">else</span> <span class=\"kc\">None</span>\n      <span class=\"k\">with</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">variable_scope</span><span class=\"p\">(</span><span class=\"n\">layer_name</span><span class=\"p\">):</span>\n        <span class=\"k\">with</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">variable_scope</span><span class=\"p\">(</span><span class=\"s2\">&#34;self_attention&#34;</span><span class=\"p\">):</span>\n          <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">common_attention</span><span class=\"o\">.</span><span class=\"n\">multihead_attention</span><span class=\"p\">(</span><span class=\"o\">...</span><span class=\"p\">)</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">transformer_encoder</span><span class=\"p\">(</span><span class=\"n\">encoder_input</span><span class=\"p\">,</span>\n                        <span class=\"n\">encoder_self_attention_bias</span><span class=\"p\">,</span>\n                        <span class=\"n\">hparams</span><span class=\"p\">,</span>\n                        <span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s2\">&#34;encoder&#34;</span><span class=\"p\">,</span>\n                        <span class=\"n\">nonpadding</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span>\n                        <span class=\"n\">save_weights_to</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span>\n                        <span class=\"n\">make_image_summary</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">,</span>\n                        <span class=\"n\">losses</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">):</span>\n  <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">encoder_input</span>\n  <span class=\"o\">......</span>\n  <span class=\"k\">with</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">variable_scope</span><span class=\"p\">(</span><span class=\"n\">name</span><span class=\"p\">):</span>\n    <span class=\"k\">if</span> <span class=\"n\">nonpadding</span> <span class=\"ow\">is</span> <span class=\"ow\">not</span> <span class=\"kc\">None</span><span class=\"p\">:</span>\n      <span class=\"n\">padding</span> <span class=\"o\">=</span> <span class=\"mf\">1.0</span> <span class=\"o\">-</span> <span class=\"n\">nonpadding</span>\n    <span class=\"k\">else</span><span class=\"p\">:</span>\n      <span class=\"n\">padding</span> <span class=\"o\">=</span> <span class=\"n\">common_attention</span><span class=\"o\">.</span><span class=\"n\">attention_bias_to_padding</span><span class=\"p\">(</span>\n          <span class=\"n\">encoder_self_attention_bias</span><span class=\"p\">)</span>\n      <span class=\"n\">nonpadding</span> <span class=\"o\">=</span> <span class=\"mf\">1.0</span> <span class=\"o\">-</span> <span class=\"n\">padding</span>\n    <span class=\"n\">pad_remover</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>\n    <span class=\"k\">if</span> <span class=\"n\">hparams</span><span class=\"o\">.</span><span class=\"n\">use_pad_remover</span> <span class=\"ow\">and</span> <span class=\"ow\">not</span> <span class=\"n\">common_layers</span><span class=\"o\">.</span><span class=\"n\">is_xla_compiled</span><span class=\"p\">():</span>\n      <span class=\"n\">pad_remover</span> <span class=\"o\">=</span> <span class=\"n\">expert_utils</span><span class=\"o\">.</span><span class=\"n\">PadRemover</span><span class=\"p\">(</span><span class=\"n\">padding</span><span class=\"p\">)</span>\n    <span class=\"k\">for</span> <span class=\"n\">layer</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">hparams</span><span class=\"o\">.</span><span class=\"n\">num_encoder_layers</span> <span class=\"ow\">or</span> <span class=\"n\">hparams</span><span class=\"o\">.</span><span class=\"n\">num_hidden_layers</span><span class=\"p\">):</span>\n      <span class=\"k\">with</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">variable_scope</span><span class=\"p\">(</span><span class=\"s2\">&#34;layer_</span><span class=\"si\">%d</span><span class=\"s2\">&#34;</span> <span class=\"o\">%</span> <span class=\"n\">layer</span><span class=\"p\">):</span>\n        <span class=\"k\">with</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">variable_scope</span><span class=\"p\">(</span><span class=\"s2\">&#34;self_attention&#34;</span><span class=\"p\">):</span>\n          <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">common_attention</span><span class=\"o\">.</span><span class=\"n\">multihead_attention</span><span class=\"p\">(</span><span class=\"o\">...</span><span class=\"p\">)</span></code></pre></div><p>原代码比较长，我摘出了能说明问题的部分，注意两个for循环，很容易看出来编码器在独立的执行完6次层运算之后只输出最后结果，同样的解码器也是，所以在编码器与解码器的层与层之间是没有交互的。 除了和编码器相同的两个子层之外，解码器插入了第三个子层用来对编码器的输出做多头注意。。残差连接和归一化同编码器类似。<b>自我关注层也做了修改，避免输出作为输入被输出观察到（这里是BERT在使用transformer时做了优化的地方，BERT中提到这么做会导致输入只能学习到跟前面的token的依赖关系，BERT用了新的MASK方法做了优化，详见上一篇</b>）transformer中的mask是这样实现的，输出比输入偏移了一个位置来保证预测的位置 <img src=\"https://www.zhihu.com/equation?tex=i\" alt=\"i\" eeimg=\"1\"/> 可以只依赖于已知的位置索引在 <img src=\"https://www.zhihu.com/equation?tex=i\" alt=\"i\" eeimg=\"1\"/> 之前的输出。画个简单的示意图对比一下。 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-a5bdefaea59d87f567cae2a1c281e799_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1541\" data-rawheight=\"608\" class=\"origin_image zh-lightbox-thumb\" width=\"1541\" data-original=\"https://pic2.zhimg.com/v2-a5bdefaea59d87f567cae2a1c281e799_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1541&#39; height=&#39;608&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1541\" data-rawheight=\"608\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1541\" data-original=\"https://pic2.zhimg.com/v2-a5bdefaea59d87f567cae2a1c281e799_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-a5bdefaea59d87f567cae2a1c281e799_b.jpg\"/></figure><p> transformer的注意力机制由缩放点积组成的多头注意力机制（Scale Dot-Product Attention, Multi-Head Attention）形成，如下图所示 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ab6f4bec52281d77d287858ad06fb14c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1410\" data-rawheight=\"813\" class=\"origin_image zh-lightbox-thumb\" width=\"1410\" data-original=\"https://pic1.zhimg.com/v2-ab6f4bec52281d77d287858ad06fb14c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1410&#39; height=&#39;813&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1410\" data-rawheight=\"813\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1410\" data-original=\"https://pic1.zhimg.com/v2-ab6f4bec52281d77d287858ad06fb14c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-ab6f4bec52281d77d287858ad06fb14c_b.jpg\"/></figure><p> 我们在前面留了一个输入如何一分为三的问题，在翻译模型中输入可以很自然的由三个部分构成一个query和一组key-value对，而在无监督的语言模型学习中，三个部分是相同的为了说明这点我们把OpenAI GPT的这一部分代码拿出来看看</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">attn</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">scope</span><span class=\"p\">,</span> <span class=\"n\">n_state</span><span class=\"p\">,</span> <span class=\"n\">n_head</span><span class=\"p\">,</span> <span class=\"n\">train</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">,</span> <span class=\"n\">scale</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">):</span>\n    <span class=\"k\">assert</span> <span class=\"n\">n_state</span><span class=\"o\">%</span><span class=\"n\">n_head</span><span class=\"o\">==</span><span class=\"mi\">0</span>\n    <span class=\"k\">with</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">variable_scope</span><span class=\"p\">(</span><span class=\"n\">scope</span><span class=\"p\">):</span>\n        <span class=\"n\">c</span> <span class=\"o\">=</span> <span class=\"n\">conv1d</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"s1\">&#39;c_attn&#39;</span><span class=\"p\">,</span> <span class=\"n\">n_state</span><span class=\"o\">*</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">train</span><span class=\"o\">=</span><span class=\"n\">train</span><span class=\"p\">)</span>\n        <span class=\"n\">q</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">v</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">split</span><span class=\"p\">(</span><span class=\"n\">c</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">)</span>\n        <span class=\"n\">q</span> <span class=\"o\">=</span> <span class=\"n\">split_heads</span><span class=\"p\">(</span><span class=\"n\">q</span><span class=\"p\">,</span> <span class=\"n\">n_head</span><span class=\"p\">)</span>\n        <span class=\"n\">k</span> <span class=\"o\">=</span> <span class=\"n\">split_heads</span><span class=\"p\">(</span><span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">n_head</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n        <span class=\"n\">v</span> <span class=\"o\">=</span> <span class=\"n\">split_heads</span><span class=\"p\">(</span><span class=\"n\">v</span><span class=\"p\">,</span> <span class=\"n\">n_head</span><span class=\"p\">)</span>\n        <span class=\"n\">a</span> <span class=\"o\">=</span> <span class=\"n\">_attn</span><span class=\"p\">(</span><span class=\"n\">q</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">,</span> <span class=\"n\">train</span><span class=\"o\">=</span><span class=\"n\">train</span><span class=\"p\">,</span> <span class=\"n\">scale</span><span class=\"o\">=</span><span class=\"n\">scale</span><span class=\"p\">)</span>\n        <span class=\"n\">a</span> <span class=\"o\">=</span> <span class=\"n\">merge_heads</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">)</span>\n        <span class=\"n\">a</span> <span class=\"o\">=</span> <span class=\"n\">conv1d</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"s1\">&#39;c_proj&#39;</span><span class=\"p\">,</span> <span class=\"n\">n_state</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">train</span><span class=\"o\">=</span><span class=\"n\">train</span><span class=\"p\">)</span>\n        <span class=\"n\">a</span> <span class=\"o\">=</span> <span class=\"n\">dropout</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">resid_pdrop</span><span class=\"p\">,</span> <span class=\"n\">train</span><span class=\"p\">)</span>\n        <span class=\"k\">return</span> <span class=\"n\">a</span></code></pre></div><p>如代码所示，输入通过一个三通道卷积形成q,k,v。</p><p>Scale Dot-Product Attention的计算： </p><p><img src=\"https://www.zhihu.com/equation?tex=%5Crm+Attention%28%5Cit+Q%2CK%2CV%29%3D%5Crm+softmax%28%5Cit+%5Cfrac%7BQK%5E%7BT%7D%7D%7B%5Csqrt%7Bd_%7Bk%7D%7D%7D%29V\" alt=\"\\rm Attention(\\it Q,K,V)=\\rm softmax(\\it \\frac{QK^{T}}{\\sqrt{d_{k}}})V\" eeimg=\"1\"/> </p><p>Multi-Head Attention计算： </p><p><img src=\"https://www.zhihu.com/equation?tex=%5Crm+MultiHead%28%5Cit+Q%2CK%2CV%29%3D%5Crm+Concat%28head_1%2C...%2Chead_n%29+%5Cit+W%5EO\" alt=\"\\rm MultiHead(\\it Q,K,V)=\\rm Concat(head_1,...,head_n) \\it W^O\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=+%5Crm+head_i%3DAttenttion%28%5Cit+QW_i%5EQ%2CKW_i%5EK%2CVW_i%5EV%29\" alt=\" \\rm head_i=Attenttion(\\it QW_i^Q,KW_i^K,VW_i^V)\" eeimg=\"1\"/> </p><p> 这里的计算是可以并行的进行的。</p><p>Position-wise前馈网络：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Crm+FFN%28%5Cit%7Bx%7D+%29+%3D+%5Cmax%280%2C+xW_1%2Bb_1%29W_2%2Bb_2\" alt=\"\\rm FFN(\\it{x} ) = \\max(0, xW_1+b_1)W_2+b_2\" eeimg=\"1\"/> </p><p> 使用了ReLu，没什么好说的，transformer本来是用于翻译任务的，所以在原模型中它要使用训练好的词向量，后来OpenAI GPT把这个方法调整之后用它训练词向量</p><p>Position Encoding部分，也就是输入和输出进入层运算之前做的计算有点儿意思。主要是因为transformer没有使用卷积或者循环神经网络，所以为了利用序列的顺序信息使用了Position Encoding。先看看嵌入函数：</p><p><img src=\"https://www.zhihu.com/equation?tex=+PE_%7B%28pos%2C2i%29%7D+%3D+%5Csin%28pos%2F10000%5E%7B2i%2Fd_%7Bmodel%7D%7D%29\" alt=\" PE_{(pos,2i)} = \\sin(pos/10000^{2i/d_{model}})\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=PE_%7B%28pos%2C2i%2B1%29%7D+%3D+%5Ccos%28pos%2F10000%5E%7B2i%2Fd_%7Bmodel%7D%7D%29\" alt=\"PE_{(pos,2i+1)} = \\cos(pos/10000^{2i/d_{model}})\" eeimg=\"1\"/> </p><p>注意因为维度要相同所以 <img src=\"https://www.zhihu.com/equation?tex=i\" alt=\"i\" eeimg=\"1\"/> 的取值范围是 <img src=\"https://www.zhihu.com/equation?tex=d_%7Bmodel%7D\" alt=\"d_{model}\" eeimg=\"1\"/> 的1/2。我们画个图直观的看一下，下图展示了位置1和位置4的position embedding的向量，截取了前200 dimension。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-b3223b50bc38de8ddddc6e2fe7ef5683_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1117\" data-rawheight=\"840\" class=\"origin_image zh-lightbox-thumb\" width=\"1117\" data-original=\"https://pic4.zhimg.com/v2-b3223b50bc38de8ddddc6e2fe7ef5683_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1117&#39; height=&#39;840&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1117\" data-rawheight=\"840\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1117\" data-original=\"https://pic4.zhimg.com/v2-b3223b50bc38de8ddddc6e2fe7ef5683_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-b3223b50bc38de8ddddc6e2fe7ef5683_b.jpg\"/></figure><p> 为何选用这种方式做位置嵌入又是另一篇论文，在此我们只从图像上感性的理解一下，首先是天然归一化的，其次不同的position差别很大，第三向量波动大，第四各个图像可以通过平移重叠，也就是论文中提到的 <img src=\"https://www.zhihu.com/equation?tex=PE_%7B%28position%2Bk%29%7D\" alt=\"PE_{(position+k)}\" eeimg=\"1\"/> 可以用 <img src=\"https://www.zhihu.com/equation?tex=PE_%7B%28position%2Bk%29%7D\" alt=\"PE_{(position+k)}\" eeimg=\"1\"/> 的线性函数表示，这和位置本身所代表的含义是相通的。</p><p>以上基本上就是transformer的前向计算过程，相关的实验结果不细说了。接下来简短的看一看OpenAI GPT。首先OpenAI GPT是一种fine-tuning模型，它会先在大型语料中做无监督的预训练，然后使用监督方法对特定任务简单学习数量较少的参数然后应用到特定任务中。</p><h2>2 OpenAI GPT</h2><p>transformer理解了OpenAI GPT基本上就没有特别大的障碍了，上面已经提到了OpenAI GPT通过把输入进行一次三通道卷积来做multi-head-attention，这里把相关的式子列出来就可以比较清楚的知道他的计算过程。</p><p>和大部分语言模型建模方式一样，OpenAI GPT的无监督部分也是通过计算句子形成概率的对数似然来作为损失函数。</p><p><img src=\"https://www.zhihu.com/equation?tex=+L_1%28%5Cmathcal%7BU%7D%29+%3D+%5Csum_%7Bi%7D%5Clog+P%28u_i%7Cu_%7Bi-k%7D%2C...%2Cu_%7Bi-1%7D%3B%5CTheta%29\" alt=\" L_1(\\mathcal{U}) = \\sum_{i}\\log P(u_i|u_{i-k},...,u_{i-1};\\Theta)\" eeimg=\"1\"/> </p><p>注意这里还是只是用了当前词前面的词，没有抓住和后文的依赖关系。 无监督的前向计算过程</p><p><img src=\"https://www.zhihu.com/equation?tex=+h_0%3DUW_e%2BW_p\" alt=\" h_0=UW_e+W_p\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=h_l%3D%5Cmathrm%7Btransformer_block%7D%28h_%7Bl-1%7D+%5Cforall+l+%5Cin+%5B1%2Cn%5D%29\" alt=\"h_l=\\mathrm{transformer_block}(h_{l-1} \\forall l \\in [1,n])\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=P%28u%29%3D%5Cmathrm%7Bsoftmax%7D%28h_nW_e%5ET%29\" alt=\"P(u)=\\mathrm{softmax}(h_nW_e^T)\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=W_e\" alt=\"W_e\" eeimg=\"1\"/> 是词嵌入， <img src=\"https://www.zhihu.com/equation?tex=W_p\" alt=\"W_p\" eeimg=\"1\"/> 是位置嵌入。 监督任务下的fine-tuning <img src=\"https://www.zhihu.com/equation?tex=P%28y%7Cx%5E1%2C...%2Cx%5Em%29%3D%5Cmathrm%7Bsoftmax%7D%28h_l%5EmW_y%29\" alt=\"P(y|x^1,...,x^m)=\\mathrm{softmax}(h_l^mW_y)\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=h_l%5Em\" alt=\"h_l^m\" eeimg=\"1\"/> 是transformer最后一层的激活函数输出，目标函数 <img src=\"https://www.zhihu.com/equation?tex=L_2%28%5Cmathcal%7BC%7D%29%3D%5Csum_%7Bx%2Cy%7D%5Clog+P%28y%7Cx%5E1%2C...%2Cx%5Em%29\" alt=\"L_2(\\mathcal{C})=\\sum_{x,y}\\log P(y|x^1,...,x^m)\" eeimg=\"1\"/> </p><p>最终把两个目标函数结合起来就是 <img src=\"https://www.zhihu.com/equation?tex=L_3%28%5Cmathcal%7BC%7D%29%3DL_2%28%5Cmathcal%7BC%7D%29%2B%5Clambda%2AL_1%28%5Cmathcal%7BC%7D%29\" alt=\"L_3(\\mathcal{C})=L_2(\\mathcal{C})+\\lambda*L_1(\\mathcal{C})\" eeimg=\"1\"/> </p><p>OpenAI GPT的基本原理就是这些，需要注意的一点是在对特定任务做fine-tuning的时候词嵌入使用了 <img src=\"https://www.zhihu.com/equation?tex=%5Bz%3Bq%3B%5C%24%3Ba_k%5D\" alt=\"[z;q;\\$;a_k]\" eeimg=\"1\"/> 的嵌入方式，其中 <img src=\"https://www.zhihu.com/equation?tex=z\" alt=\"z\" eeimg=\"1\"/> 是文档， <img src=\"https://www.zhihu.com/equation?tex=q\" alt=\"q\" eeimg=\"1\"/> 是question， <img src=\"https://www.zhihu.com/equation?tex=%7Ba_k%7D\" alt=\"{a_k}\" eeimg=\"1\"/> 是可能的答案，中间有分隔符。BERT扩展了这种嵌入方式，在无监督训练的时候就会把这些信息嵌入进去。所以BERT之所以能在11个自然语言处理任务中都能达到state-of-the-art的结果，它强大的特征工程也是功不可没，再加上双向的语言推断模型，让他成为目前最好的语言模型。</p><p>关于BERT，上一篇理论上的阐述已经比较详细了，在此就不多说了。之所以水了这一篇文章是因为我觉得学习一个模型的时候观察它是如何发展过来的更有帮助自己理解模型，希望也能帮助到大家。</p>", 
            "topic": [
                {
                    "tag": "自然语言处理", 
                    "tagLink": "https://api.zhihu.com/topics/19560026"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": [
                {
                    "userName": "JShen", 
                    "userLink": "https://www.zhihu.com/people/3694137446cf66f20d6b1b7ba559ac18", 
                    "content": "<p>非常好的讲解，特别是阐明了论文中没有提到需要到代码中寻找的信息。感谢分享！</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/52248160", 
            "userName": "右左瓜子", 
            "userLink": "https://www.zhihu.com/people/6107fd0900473aae4e5ec6824b4fde4a", 
            "upvote": 25, 
            "title": "BERT论文翻译", 
            "content": "<p>原标题：BERT:Pre-training of Deep Bidirectional Transformers for  Language Understanding</p><p>地址：</p><a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1810.04805%3Fcontext%3Dcs\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Pre-training of Deep Bidirectional Transformers for Language Understanding</a><p>我翻译的不是很好，主要是自己翻译一遍理解更深刻一点，泛泛而读经常会了解大意而忽略细节，以下是基于原文的翻译，实验部分略过了。另外在阅读前最好了解一下转换器模型和OpenAI GPT模型，不然准会看着云里雾里。</p><p>Transformer：</p><a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1706.03762\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">[1706.03762] Attention Is All You Need</a><p>OpenAI GPT:</p><a href=\"https://link.zhihu.com/?target=https%3A//s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">s3-us-west-2.amazonaws.com</span><span class=\"invisible\">/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf</span><span class=\"ellipsis\"></span></a><h2><b>BERT 语义理解深度双向转换器预训练模型</b></h2><p><br/><b>摘要</b><br/>我们介绍一个叫做<b>BERT</b>的新的语言表示模型，它可以理解为转换器的双向解码表示。不像最近的一些语言表示模型，BERT被设计成通过在所由层联合左边和右边的上下文来做深度双向表示的预训练。预训练的BERT表示可以通过仅仅一个额外的输出层的微调，在很多诸如问答，语义推断等自然语言处理任务上达到state-of-the-art的结果,而且不需要为特定任务在结构上做很大的改动。<br/><br/>BERT在概念上非常简单但是在实际运用中却很有效，他在是一个自然语言处理任务上都达到了state-of-the-art的结果，包括把GLUE benchmark的准确率提升到80.4%（7.6%的绝对提升），MultiNLI的准确率达到86.7%（5.6%的绝对提升），SQuAD v1.1的问答中F1值高达93.2（1.5的绝对提升），比人类的表现高2.0.<br/><br/><b>1 介绍</b><br/>语言模型预训练对很多自然语言处理任务来讲都很有效，这些任务包括句子级的任务比如自然语言推断和解析，通过全面的分析句子来预测句子之间的关系，字词级别的人物比如命名实体识别和SQuAD问答，模型需要产生在字词级别产生很好的输出。<br/><br/>在预训练语言表示的下游任务中有两个现有的策略：基于特征和参数微调，基于特征的方法比如ELMo使用了把图训练表示作为额外特征的任务特定的结构。参数微调的方法诸如生成式预训练转换器，介绍了最少的人物特定的参数，然后再下游任务中通过简单的微调预训练的参数来训练。再前面的工作中，两种方式在与训练时都使用同样的目标函数，用五项语言模型去学习一般的语言表示。<br/><br/>我们认为当前的技术手段严重的限制了预训练表示的力量，特别是微调法。最主要的限制是标准的语言模型是单向的，这限制了可以被用来做预训练的结构的选择。比如，在OpenAI GPT中，作者使用了从左至右的结构，转换其中每一个字词在自我关注层只能关注前一个字词，这种约束对于句子级的任务是一种次优的解决方案。并且当把微调方法应用到诸如SQuAD这样的问答系统中会造成很严重的错误。因为双向的相关上下文对这种系统至关重要。<br/><br/>本文中我们会通过BERT: Bidirectional Encoder Representations from Transformers 来提升基于参数微调的方法。BERT对前面提到的单项模型通过使用新的预训练目标：<b>遮罩语言模型(masked language model)(MLM)来添加新的约束。遮罩语言模型随机的从输入中遮罩一些词，目标是只通过它的上下文去预测这些遮罩词的id</b>。不同于从左至右的预训练语言模型，MLM目标允许表示ro融合左右两个方向的上下文，这让我们可以预训练深度双向转换器。除了遮罩语言模型，我们还会额外的介绍联合了预训练和文本对表示的“下句预测”任务。<br/><br/>本文最主要的贡献在于以下几点：<br/>* 我们阐述了语言表示双向与训练的重要性，BERT使用遮罩语言模型来做深度双向表示预训练。<br/>* 我们展示了预训练表示消出任务特定结构。BERT是第一个在大量句子级和字词级任务中都达到state-of-the-art结果的基于微调的表示模型，比很多任务特定结构的系统表现得更好<br/>* BERT在11个自然语言处理任务上都得到state-of-the-art结果。我们也BERT的扩展能力，对我们模型的双向性的阐述是一个单独的新的重要贡献。预训练模型的代码地址[BERT](<u>goo.gl/language/bert</u>)<br/><br/><b>2 相关工作</b><br/>预训练语言表示历史悠久，这一节我们将简短的回顾一下最流行的几种方法。<br/><b>2.1 特征法</b><br/>近十年间，学习可以广泛应用的词表示在研究领域非常活跃，包括无神经网络的和神经网络式的方法.预训练词嵌入被认为是自然语言处理系统中的一个整体,对从头开始学习的嵌入提供了显着的改进。<br/><br/>这些方法都是粗粒度的，比如句嵌入或者段落嵌入。和传统的词嵌入相比，这些学习到的表示在下游模型中作为特征也很有用。<br/><br/>ELMo把传统的词嵌入研究使用了不同的方式泛化。它提出从语言模型中提取上下文敏感的特征。在遇到使用完整的上下文做词嵌入的特定任务结构时，ELMo在几个主要的自然语言处理任务上获得了state-of-the-art结果包括SQuAD问答，情感分析，和命名实体识别。<br/><br/><b>2.2 微调法</b><br/>最近的从语言模型中做转移学习的一个趋势是微调之前在目标语言模型上预训练一些模型，目标语言模型会在下游任务中做监督训练。这些方法的优势在于只有很少的参数需要学习。就这一点优势让OpenAI GPT在很多句子级的任务中获得了state-of-the-art的结果。<br/><br/><b>2.3 监督训练集的转移学习</b><br/><br/>尽管无监督的预训练的一个优势是有几乎无限多的训练数据，从大量数据集中的有监督任务的转移学习也很有效，比如自然语言推断和机器翻译.除了自然语言处理领域，计算机方面的研究也证明了从大量预训练模型中做转移学习的重要性。一个很有效的诀窍是微调在ImageNet上与训练的模型。<br/><br/><b>3 BERT</b><br/>这一节我们将要介绍BERT和它的详细的实现。首先我们看一看BERT的模型结构和输入表示。接着在3.3节我们会介绍预训练任务，也就是这篇文章的核心思想。预训练和微调的细节将会分别呈现在3.4和3.5中，最后，在3.6节我们会讨论BERT和OpenAI GPT的不同点。<br/><br/><b>3.1 模型结构</b><br/>BERT的模型结构是一个多层双向转换器。因为最近转换器的使用越来越普遍，而且我们的实现和原始的实现基本相同，所以我们将省略掉这一部分的详细描述。<br/><br/>(译者注：在阅读这部分之前最好先搞清楚转换器是什么东西，因为有很多专有名词，不太好翻译，但是看了转换器模型基本上就明白了)<br/>在这个模型中，我们用 <img src=\"https://www.zhihu.com/equation?tex=L\" alt=\"L\" eeimg=\"1\"/> 代表层数， <img src=\"https://www.zhihu.com/equation?tex=H\" alt=\"H\" eeimg=\"1\"/> 代表隐藏层的大小，self-attention heads用 <img src=\"https://www.zhihu.com/equation?tex=A\" alt=\"A\" eeimg=\"1\"/> 表示，在所有的应用中，我们把前馈层的大小设置为 <img src=\"https://www.zhihu.com/equation?tex=4H\" alt=\"4H\" eeimg=\"1\"/> ，也就是说当 <img src=\"https://www.zhihu.com/equation?tex=H%3D768\" alt=\"H=768\" eeimg=\"1\"/> 时前馈层大小为3072， <img src=\"https://www.zhihu.com/equation?tex=H%3D1024\" alt=\"H=1024\" eeimg=\"1\"/> 时，前馈层大小为4096，我们主要会介绍两种尺寸的模型：<br/>* <img src=\"https://www.zhihu.com/equation?tex=%5Crm+BERT_%7BBASE%7D%3AL%3D12%2CH%3D768%2CTotal+Parameters%3D100M\" alt=\"\\rm BERT_{BASE}:L=12,H=768,Total Parameters=100M\" eeimg=\"1\"/> <br/>* <img src=\"https://www.zhihu.com/equation?tex=%5Crm+BERT_%7BLARGE%7D%3AL%3D24%2CH%3D1024%2CA%3D16%2CTotal+Parameters%3D340M\" alt=\"\\rm BERT_{LARGE}:L=24,H=1024,A=16,Total Parameters=340M\" eeimg=\"1\"/> <br/><br/> <img src=\"https://www.zhihu.com/equation?tex=%5Crm+BERT_%7BBASE%7D\" alt=\"\\rm BERT_{BASE}\" eeimg=\"1\"/> 选择了和OpenAI GPT相同的模型大小方便对比。然而严格意义上来讲，BERT转换器使用了双向的自我注意机制，而GPT转换器使用的自我注意约束中每一个字词只注意了它左边的上下文。在这里需要指出的是，在文献中，双向的转换器经常被用来作为“编码转换器”，而只依赖于左边上下文的被称为“编码转换器”，因为他能用于文本生成。BERT,OpenAI GPT和ELMo之前的对比如下图所示。<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-57f0331af735a1eea066086d74c469a1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1535\" data-rawheight=\"367\" class=\"origin_image zh-lightbox-thumb\" width=\"1535\" data-original=\"https://pic2.zhimg.com/v2-57f0331af735a1eea066086d74c469a1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1535&#39; height=&#39;367&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1535\" data-rawheight=\"367\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1535\" data-original=\"https://pic2.zhimg.com/v2-57f0331af735a1eea066086d74c469a1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-57f0331af735a1eea066086d74c469a1_b.jpg\"/></figure><p><br/><br/><b>3.2 输入表示</b><br/>我们的输入表示可以用一个标记序列明确的表示单个文本和成对的语句（如[问题，答案]）。对于一个给定的标记，它的输入由它对应的标记，片段和位置的嵌入来表示，如下图所示<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-32a12c522b7166c696e4590ff395d1ef_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1593\" data-rawheight=\"558\" class=\"origin_image zh-lightbox-thumb\" width=\"1593\" data-original=\"https://pic4.zhimg.com/v2-32a12c522b7166c696e4590ff395d1ef_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1593&#39; height=&#39;558&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1593\" data-rawheight=\"558\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1593\" data-original=\"https://pic4.zhimg.com/v2-32a12c522b7166c696e4590ff395d1ef_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-32a12c522b7166c696e4590ff395d1ef_b.jpg\"/></figure><p><br/>我们的标注的特点在于：<br/>* 我们在30000个词上使用了WordPiece嵌入，把拆分的词片段(word pieces)用&#34;##&#34;标注（译者注：见图中&#34;playing&#34;-&#34;play ##ing&#34;）<br/>* 我们使用了学习过的位置嵌入，支持序列长度达512的标记<br/>* 每一句的句首使用了特殊的分类嵌入([CLS])。这个标记。在最终的隐藏层中（也就是转换器的输出）对应的是分类任务中序列标识的聚合表示。非分类任务中这一标记将被忽略<br/>* 句子对被打包在一起作为一个句子。我们用两种方法区别他们，首先，我们把他们用特殊的标记（[SEP]）区分开，然后我们会给第一句的每一个标记添加一个学习到的句子 A 的嵌入，给第二句的每个标记添加一个学习到的句子 B 的嵌入<br/>* 对于单个句子输入我们只使用句子 A 的嵌入<br/>(译者注：翻译的比较糟糕，直接看图会比较明白)<br/><br/><b>3.3 预训练任务</b><br/>我们没有使用传统的从左至右或者从右至左的语言模型去预训练BERT，而是用了两个新的无监督的预测任务，将在这一节详细的描述<br/><b>3.3.1 Tast #1: Masked LM</b><br/>直觉上，我们相信深度的双向模型会比从左至右模型或者浅层次的从右至左和从左至右的连结要更强大。然而，标准的条件语言模型只能从左至右训练或者从右至左训练，因为双向的条件会让每个词都可以通过多层的上下文看到他自己。<br/>为了训练深度双向表示我们使用了直接随机的遮盖住一定比例的输入标记，然后仅仅预测这些遮住的输入标记。我们把这种方式称为&#34;masked LM&#34;(MLM)。在这种情况下，被遮盖的标记对应的最终的隐藏向量被当作softmax的关于该词的一个输出，和其他标准语言模型中相同。在我们所有的实验中，我们在每一个序列中随机的遮盖了15%的WordPiece标记，和denoising auto-encoders（Vincent et al.,2008）相反，我们只预测被遮盖的词语，而不是重构整个输入。<br/><br/>虽然这允许我们做双向的与训练模型，但是这种方法仍然有两个弊端。第一个是这种方法会让预训练模型和调参法不能相互匹配，因为[MASK]标记在调参法中是不存在的。为了消除这个弊端，我们并不总是把遮盖的词用[MASK]表示，而是在训练数据中对随机的产生15%的标记用[MASK]表示，比如，在句子&#34;my dog is hairy&#34;中选择&#34;hairy&#34;，然后通过以下的方式产生标记：<br/>* 并不总是用[MASK]替换选择的词，数据通过如下方式产生：<br/>* 80%的情况下：把选择的词替换成[MASK]，比如：&#34;my dog is hairy&#34; → &#34;my dog is [MASK]&#34;<br/>* 10%的情况下替换选中的词为随机词，比如：&#34;my dog is hairy&#34; → &#34;my dog is apple&#34;<br/>* 10% 的情况下保持原词不变，比如：&#34;my dog is hairy&#34; → &#34;my dog is hairy&#34;。目的是把表示纠正为实际上观察到的词，所以它被清治的去保留每一个输入标记的分布式上下文表示。除此之外，因为随机的替换仅发生在1.5%的标记上（10% * 15%）,所以这不会折损模型的语言理解能力。<br/><br/>第二个弊端是使用一个MLM意味着每个batch中只有15%的标记会被预测，所以在与训练的时候收敛需要更多步。在5.3中我们会阐述MLM的收敛速度比从左至右的模型（预测每一个标记）慢，但是和MLM带来的巨大提升相比，这么做是值得的。<br/><br/><b>3.3.2 Task #2:Next Sentence Prediction</b><br/><br/>很多重要的下游任务如问答和自然语言推断(NLI)是基于两个句子之间的理解的，语言模型并不能直接捕获到这种关系。为了徐连可以理解句子关系的模型，我们预训练了一个可以从任何单语语料库中轻松产生的二值化的下句预测任务。特别的，当为每个预训练模型选择句子A和句子B时，50%的情况B就是A的下一句，50%的情况下时预料中随机选择的句子。比如：</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"n\">Input</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">CLS</span><span class=\"p\">]</span> <span class=\"n\">the</span> <span class=\"n\">man</span> <span class=\"n\">went</span> <span class=\"n\">to</span> <span class=\"p\">[</span><span class=\"n\">MASK</span><span class=\"p\">]</span> <span class=\"n\">store</span> <span class=\"p\">[</span><span class=\"n\">SEP</span><span class=\"p\">]</span> <span class=\"n\">he</span> <span class=\"n\">bought</span> <span class=\"n\">a</span> <span class=\"n\">gallon</span> <span class=\"p\">[</span><span class=\"n\">MASK</span><span class=\"p\">]</span> <span class=\"n\">milk</span> <span class=\"p\">[</span><span class=\"n\">SEP</span><span class=\"p\">]</span> \n<span class=\"n\">Lable</span> <span class=\"o\">=</span> <span class=\"n\">IsNext</span> \n<span class=\"n\">Input</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">CLS</span><span class=\"p\">]</span> <span class=\"n\">the</span> <span class=\"n\">man</span> <span class=\"n\">went</span> <span class=\"n\">to</span> <span class=\"p\">[</span><span class=\"n\">MASK</span><span class=\"p\">]</span> <span class=\"n\">store</span> <span class=\"p\">[</span><span class=\"n\">SEP</span><span class=\"p\">]</span> <span class=\"n\">penguin</span> <span class=\"p\">[</span><span class=\"n\">MASK</span><span class=\"p\">]</span> <span class=\"n\">are</span> <span class=\"n\">flight</span> <span class=\"c1\">##less birds [SEP] </span>\n<span class=\"n\">Lable</span> <span class=\"o\">=</span> <span class=\"n\">NotNext</span></code></pre></div><p><br/>我们对于NotNext的句子的选择时完全随机的，并且最终预训练模型在这个任务中会达到97%-98%的准确率。除了简单的优点外，我们将会在5.1中阐述这种任务的预训练模型对QA和NLI任务都很有好处。<br/><br/><b>3.4 预训练程序</b><br/>预训练程序主要遵循现有的语言模型与训练文献。对于预训练语料我们把BooksCorpus(800M words)和EnglishWikipedia（2500M words）加在一起使用。对于维基的数据我们仅仅提取了文章部分忽略了列表，表格和头部信息。对于提取长的连续性序列，和使用句子级的文本如Billion Word Benchmark比使用篇章级的文本至关重要。<br/><br/>为了产生每一个训练的输入序列，我们从训练语料中采样了两个spans的文本，我们称之为&#34;sentence&#34;，虽然一般情况下它比单个句子长的多。第一个句子嵌入为A，第二个句子嵌入B。50%的情况下B刚好是A的下一句，另外50%的情况下他是一个随机的句子。这样下局预测的任务就完成了。最终他们被采样总长度小于等于512的标记。LM遮盖被应用于WordPiece的标记化之后，标准的遮盖率是15%，对于不完整的词部m欸有特别考虑。<br/><br/>我们训练的batchsize是256个句子(256 sequence*512 tokens = 128000 token/batch)，训练1,000,000步差不多是40个epochs超过3.3 billion的预料数据。我们使用了Adam优化器，学习率是1e-4, <img src=\"https://www.zhihu.com/equation?tex=%5Cbeta_1%3D0.9%2C%5Cbeta_2%3D0.999\" alt=\"\\beta_1=0.9,\\beta_2=0.999\" eeimg=\"1\"/> ，L2权重衰减是0.01，在前10,000步中学习率warmup，学习率是线性衰减的。所有层的dropout都是0.1，和OpenAI GPT一样，激活函数选择了gelu而不是标准的relu。训练的损失函数是mask LM的似然的平均值加上下据预测的似然。<br/><br/> <img src=\"https://www.zhihu.com/equation?tex=BERT_%7BBASE%7D\" alt=\"BERT_{BASE}\" eeimg=\"1\"/> 的训练是在4个Pod配置的云TPU(总共16个TPU片)上进行的，每一词预训练花费4天完成。</p><p><br/><b>3.5 调参法程序</b><br/>对于序列级别的分类任务，BERT的调参是直接进行的，为了获得固定维度的输入序列表示，我们对输入的第一个标记使用了最终的隐藏状态（也就是转换器的输出），为了和特殊的[CLS]嵌入做匹配。我们把这个向量记住 <img src=\"https://www.zhihu.com/equation?tex=C+%5Cin+%5Cmathbb%7BR%7D%5E%7BH%7D\" alt=\"C \\in \\mathbb{R}^{H}\" eeimg=\"1\"/> ，在调参过程中唯一的新加的参数是给分类层的 <img src=\"https://www.zhihu.com/equation?tex=W+%5Cin+%5Cmathbb%7BR%7D%5E%7BK+%5Ctimes+H%7D\" alt=\"W \\in \\mathbb{R}^{K \\times H}\" eeimg=\"1\"/> ，其中 <img src=\"https://www.zhihu.com/equation?tex=K\" alt=\"K\" eeimg=\"1\"/> 是分类标签的数量。标签的概率 <img src=\"https://www.zhihu.com/equation?tex=P+%5Cin+%5Cmathbb%7BR%7D%5EK\" alt=\"P \\in \\mathbb{R}^K\" eeimg=\"1\"/> 使用标准的softmax来计算， <img src=\"https://www.zhihu.com/equation?tex=P+%3D+%5Crm+softmax%28%5Cit+CW%5ET%29\" alt=\"P = \\rm softmax(\\it CW^T)\" eeimg=\"1\"/> 。BERT所有的参数和W都或通过最大化正确标签的对数概率来调整。对于span-level和token-level的预测任务，上述的过程需要做轻微的修改，我们将在第四节给出详细的说明。<br/><br/>对于调参法，大部分模型的超参数在与训练模型中都是相同的，同样的batchsize，同样的learning rate，以及epochs数量。dropout的概率一直保持在0.1，超参数的优化是任务特定的，但我们发现以下的值在所有任务种豆表现得很好：<br/>* Batch size: 16,32<br/>* Learning rate(Adam):5e-5,3e-5,2e-5<br/>* Number of epochs: 3, 4<br/><br/>我们也观察到在大的数据集中（比如100k+的标注训练数据）对超参数的选择的敏感度远低于小的训练集。参数调整非常快，所以去通过尽量的用更多的参数配置去选择在开发集上表现最好的模型是合理的。<br/><br/><b>3.6 BERT和OpenAI GPT的对比</b><br/><br/>现有的和BERT最有可比性的模型是OpenAI GPT，它在一个大的语料数据上使用了从左至右的Transformer LM。实际上，BERT很多设计上的决定都有意的选择的尽量和GPT相近仪表与两种方法可以最小限度的做对比。这里我们的核心观点是3.3节中提到的这两个新的预训练任务都使用了大量的经验上的提升，大师我们还是注意到这两者之间在如何训练上由一些其他的区别：<br/>* GPT 是在BooksCorpus(800M words)上训练的，而BERT是在BookCorpus和Wikipedia上训练的<br/>* GPT只在调参的时候引入了句子分隔符[SEP]和分类符[CLS]，而BERT在与训练的时候就使用了，而且还使用了句子A/B嵌入。<br/>* GPT训练了1M 步，一个batch size有32000词；BERT也是1M步，但是一个batch size有128000个词<br/>* GPT在所有的调参实验中都使用了5e-5的学习率，而BERT对于不同的任务选择了在开发集上表现得最好的学习率<br/> <br/>为了消除这些不同之处，我们在5.1中做了相关的实验，实验表明大部分的提升实际上来自于新的预训练任务而不是上述的这些因素。<br/><br/><b>4 实验</b><br/>略...</p><p class=\"ztext-empty-paragraph\"><br/></p><p>最后老规矩，代码地址</p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/google-research/bert\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic2.zhimg.com/v2-a773b59dd18af6ec21208560afedd39d_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">google-research/bert</a><p></p>", 
            "topic": [
                {
                    "tag": "自然语言处理", 
                    "tagLink": "https://api.zhihu.com/topics/19560026"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": [
                {
                    "userName": "nihaowhut", 
                    "userLink": "https://www.zhihu.com/people/e925fc10660acc004a08f4cc62637b87", 
                    "content": "这个翻译借助了翻译工具了吧，总感觉有点怪怪的", 
                    "likes": 1, 
                    "childComments": [
                        {
                            "userName": "右左瓜子", 
                            "userLink": "https://www.zhihu.com/people/6107fd0900473aae4e5ec6824b4fde4a", 
                            "content": "<p>没有，有些专有名词我不知道怎么翻，而且我的英文水平应该比我的数学水平还要次😥，所以我只是抛砖引玉，能看懂原文的直接近链接看原文</p>", 
                            "likes": 0, 
                            "replyToAuthor": "nihaowhut"
                        }, 
                        {
                            "userName": "nihaowhut", 
                            "userLink": "https://www.zhihu.com/people/e925fc10660acc004a08f4cc62637b87", 
                            "content": "加油", 
                            "likes": 0, 
                            "replyToAuthor": "右左瓜子"
                        }
                    ]
                }, 
                {
                    "userName": "百本老实人", 
                    "userLink": "https://www.zhihu.com/people/e78f1cb079854cb72ebf77a77e58aa04", 
                    "content": "其实可以看几篇中文综述就可以翻译的更好了", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "右左瓜子", 
                            "userLink": "https://www.zhihu.com/people/6107fd0900473aae4e5ec6824b4fde4a", 
                            "content": "<p>嗯，谢谢指点😀</p>", 
                            "likes": 0, 
                            "replyToAuthor": "百本老实人"
                        }
                    ]
                }, 
                {
                    "userName": "低级炼丹师", 
                    "userLink": "https://www.zhihu.com/people/0ae87f5e0608b6add3ca51f25cebe3cb", 
                    "content": "<p>讲道理，你这个翻译，真的是。。。一言难尽</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/51492037", 
            "userName": "右左瓜子", 
            "userLink": "https://www.zhihu.com/people/6107fd0900473aae4e5ec6824b4fde4a", 
            "upvote": 10, 
            "title": "跟条件随机场来个了断", 
            "content": "<h2>条件随机场，做个了断吧</h2><p>条件随机场之前一直有用过，但是因为有很多开源的实现，所以并没有很关注他的实现细节，当初学的时候看过一点，但是只记得这是一个很头疼的东西，最近做分词和实体词提取的时候发现这真的是个神器，不了解它的话心里不踏实，所以又把李航老师的《统计学习方法》拿出来翻了翻，发现有些公式推导并不是很详细(事实上有几个公式写着“由···式很容易的得出···”，可是我得不出来啊)，又翻阅了一些资料，整理了一下，写下来，以后忘了方便回顾。条件随机场的介绍是一个大工程，李航老师的书里把他放最后一章，因为要用到很多跟前面相关的知识，在这里我也只能尽可能的从基本的开始，好了，闲言少叙，办正事儿。</p><p>放上原文地址，如果你英文水平还可以的话直接看原文就可以了，在md文件导入到知乎文章的时候知乎为了防止脚本注入过虑了很多特殊符号，所以虽然我纠正了一些但是有些地方可能有纰漏，而且，我翻译的····其实也就那样，最后如果你只关心实现的话文章最后有相关实现代码，标题图来自论文中介绍生成式和判别式区别和联系的时候的插图，我觉得起码在看完这篇论文之后，关于判别式和生成式的区别你应该没有疑问了，又讲了许多废话，开始正题吧。</p><a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1011.4088\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">[1011.4088] An Introduction to Conditional Random Fields</a><h2>1. 图模型</h2><h2>1.1.1 无向图</h2><p>考虑一组随机变量 <img src=\"https://www.zhihu.com/equation?tex=Y\" alt=\"Y\" eeimg=\"1\"/> 的概率分布， <img src=\"https://www.zhihu.com/equation?tex=Y_s%5Cin+Y\" alt=\"Y_s\\in Y\" eeimg=\"1\"/> 来自于集合 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathcal%7BY%7D\" alt=\"\\mathcal{Y}\" eeimg=\"1\"/> ，在此我们只看考虑离散的情况，实际上它可以是离散的也可以是连续的。向量 <img src=\"https://www.zhihu.com/equation?tex=%5Crm+%5Cvec+y\" alt=\"\\rm \\vec y\" eeimg=\"1\"/> 是由随机变量 <img src=\"https://www.zhihu.com/equation?tex=Y\" alt=\"Y\" eeimg=\"1\"/> 的组成的一个序列。变量 <img src=\"https://www.zhihu.com/equation?tex=Y_s\" alt=\"Y_s\" eeimg=\"1\"/> 在 <img src=\"https://www.zhihu.com/equation?tex=%5Crm+%5Cvec+y\" alt=\"\\rm \\vec y\" eeimg=\"1\"/> 中的值用 <img src=\"https://www.zhihu.com/equation?tex=y_s\" alt=\"y_s\" eeimg=\"1\"/> 表示。指示函数</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbf+1_%7B%5Cit+y%3Dy%5E%7B%27%7D%7D%3D%5Cbegin%7Bcases%7D+1%2C+y%3Dy%5E%7B%27%7D%5C%5C+0%2C+otherwise%5C%5C+%5Cend%7Bcases%7D+\" alt=\"\\bf 1_{\\it y=y^{&#39;}}=\\begin{cases} 1, y=y^{&#39;}\\\\ 0, otherwise\\\\ \\end{cases} \" eeimg=\"1\"/> </p><p>现在假设随机变量 <img src=\"https://www.zhihu.com/equation?tex=Y\" alt=\"Y\" eeimg=\"1\"/> 的概率分布 <img src=\"https://www.zhihu.com/equation?tex=p\" alt=\"p\" eeimg=\"1\"/> 可以用 <img src=\"https://www.zhihu.com/equation?tex=%5CPsi_a%28%5Crm+y_%7B%5Cit+a%7D%29\" alt=\"\\Psi_a(\\rm y_{\\it a})\" eeimg=\"1\"/> 的乘积来因式(factor)分解，其中 <img src=\"https://www.zhihu.com/equation?tex=a\" alt=\"a\" eeimg=\"1\"/> 是 <img src=\"https://www.zhihu.com/equation?tex=%5B1.A%5D\" alt=\"[1.A]\" eeimg=\"1\"/> 之间的整数,代表因式的数量。因式 <img src=\"https://www.zhihu.com/equation?tex=%5CPsi_a\" alt=\"\\Psi_a\" eeimg=\"1\"/> 仅仅跟随机变量 <img src=\"https://www.zhihu.com/equation?tex=Y\" alt=\"Y\" eeimg=\"1\"/> 的子集 <img src=\"https://www.zhihu.com/equation?tex=Y_a\" alt=\"Y_a\" eeimg=\"1\"/> 相关。 <img src=\"https://www.zhihu.com/equation?tex=%5CPsi_a%28%5Crm+y_%7B%5Cit+a%7D%29\" alt=\"\\Psi_a(\\rm y_{\\it a})\" eeimg=\"1\"/> 的值是非负数，可以被用来衡量 <img src=\"https://www.zhihu.com/equation?tex=%5Crm+%5Cvec+y_%7B%5Cit+a%7D\" alt=\"\\rm \\vec y_{\\it a}\" eeimg=\"1\"/> 中各个值的势（compatibility），这样的因式分解可以让我们用一种更高效的方式来表示概率分布 <img src=\"https://www.zhihu.com/equation?tex=p\" alt=\"p\" eeimg=\"1\"/> ，因为集合 <img src=\"https://www.zhihu.com/equation?tex=Y_a\" alt=\"Y_a\" eeimg=\"1\"/> 的大小远小于 <img src=\"https://www.zhihu.com/equation?tex=Y\" alt=\"Y\" eeimg=\"1\"/> 。</p><p>基于以上假设，给出概率无向图的定义：对于集合 <img src=\"https://www.zhihu.com/equation?tex=Y\" alt=\"Y\" eeimg=\"1\"/> 的一个给定的子集 <img src=\"https://www.zhihu.com/equation?tex=%7BY_a%7D%5EA_%7Ba%3D1%7D\" alt=\"{Y_a}^A_{a=1}\" eeimg=\"1\"/> ，一个无向图模型是所有的可以用 </p><p><img src=\"https://www.zhihu.com/equation?tex=p%28%5Crm+%5Cvec+y%29%3D%5Cit+%5Cfrac%7B1%7D%7BZ%7D%5Cprod_%7Ba%3D1%7D%5EA%5CPsi_a%28%5Crm+%5Cvec+y_%7B%5Cit+a%7D%29%5Ctag%7B1.1%7D\" alt=\"p(\\rm \\vec y)=\\it \\frac{1}{Z}\\prod_{a=1}^A\\Psi_a(\\rm \\vec y_{\\it a})\\tag{1.1}\" eeimg=\"1\"/> </p><p>表示的概率分布组成的集合。对任意因子 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathcal%7BF%7D%3D%7B%5CPsi_a%7D\" alt=\"\\mathcal{F}={\\Psi_a}\" eeimg=\"1\"/> 有 <img src=\"https://www.zhihu.com/equation?tex=%5CPsi_a%28%5Crm+%5Cvec+y_%7B%5Cit+a%7D%29%5Cgeq0\" alt=\"\\Psi_a(\\rm \\vec y_{\\it a})\\geq0\" eeimg=\"1\"/> 对所有的 <img src=\"https://www.zhihu.com/equation?tex=%5Crm+%5Cvec+y_%7B%5Cit+a%7D\" alt=\"\\rm \\vec y_{\\it a}\" eeimg=\"1\"/> 都成立。而这些无向图中的概率分布被称为<b>随机场</b>, <img src=\"https://www.zhihu.com/equation?tex=Z\" alt=\"Z\" eeimg=\"1\"/> 是规范化因子</p><p><img src=\"https://www.zhihu.com/equation?tex=+Z%3D%5Csum_%7B%5Crm+%5Cvec+y%7D%5Cprod_%7Ba%3D1%7D%5E%7BA%7D%5CPsi_a%28%5Crm+%5Cvec+y%28%5Cit+a%29%29+%5Ctag%7B1.2%7D\" alt=\" Z=\\sum_{\\rm \\vec y}\\prod_{a=1}^{A}\\Psi_a(\\rm \\vec y(\\it a)) \\tag{1.2}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=Z\" alt=\"Z\" eeimg=\"1\"/> 很大，所以它的计算比较棘手，绝大多数时候我们是计算如何逼近它。</p><p>之所以称这个模型为“图模型”，是因为因式分解后的式子可以很简洁的用图来表示,一个很自然的形式是因式图(factor graphs)。因子图可以有 <img src=\"https://www.zhihu.com/equation?tex=G%3D%28V%2CF%2CF%29\" alt=\"G=(V,F,F)\" eeimg=\"1\"/> 来表示，其中节点集合 <img src=\"https://www.zhihu.com/equation?tex=V%3D%5C%7B1%2C2%2C...%2C%7CY%7C%5C%7D\" alt=\"V=\\{1,2,...,|Y|\\}\" eeimg=\"1\"/> 表示模型中随机变量的索引，另一个节点集合 <img src=\"https://www.zhihu.com/equation?tex=F%3D%7B1%2C2%2C...%2CA%7D\" alt=\"F={1,2,...,A}\" eeimg=\"1\"/> 表示模型中因子的索引。图模型的意义在于如果一个变量节点 <img src=\"https://www.zhihu.com/equation?tex=Y_s%2Cs%5Cin+V\" alt=\"Y_s,s\\in V\" eeimg=\"1\"/> 连接到了因子节点（注：也就是《统计学习方法》相关章节中的边） <img src=\"https://www.zhihu.com/equation?tex=%5CPsi_a%2Ca%5Cin+F\" alt=\"\\Psi_a,a\\in F\" eeimg=\"1\"/> ，那么 <img src=\"https://www.zhihu.com/equation?tex=Y_s\" alt=\"Y_s\" eeimg=\"1\"/> 就是 <img src=\"https://www.zhihu.com/equation?tex=%5CPsi_a\" alt=\"\\Psi_a\" eeimg=\"1\"/> 的一个参数.所以因式图很直观的展示了概率分布 <img src=\"https://www.zhihu.com/equation?tex=p\" alt=\"p\" eeimg=\"1\"/> 分解为势函数(local function)乘积的方式。</p><p>接下来我们正式定义一下一个因子图是否能描述一个概率分布的概念。首先使 <img src=\"https://www.zhihu.com/equation?tex=N_%7B%28a%29%7D\" alt=\"N_{(a)}\" eeimg=\"1\"/> 代表索引为a的因子的相邻节点(即一组变量)</p><hr/><p><b>定理1.1</b>：如果存在一组势函数(local function) <img src=\"https://www.zhihu.com/equation?tex=%5CPsi_%7Ba%7D\" alt=\"\\Psi_{a}\" eeimg=\"1\"/> 使得概率分布 <img src=\"https://www.zhihu.com/equation?tex=p%28%5Crm+%5Cvec+y%29\" alt=\"p(\\rm \\vec y)\" eeimg=\"1\"/> 可被写为 </p><p><img src=\"https://www.zhihu.com/equation?tex=p%28%5Crm+%5Cvec+y%29%3DZ%5E%7B%28-1%29%7D%5Cprod_%7Ba+%5Cin+F%7D%5CPsi_%7Ba%7D%28%5Crm+%5Cvec+y_%7BN%28a%29%7D%29+%5Ctag%7B1.3%7D\" alt=\"p(\\rm \\vec y)=Z^{(-1)}\\prod_{a \\in F}\\Psi_{a}(\\rm \\vec y_{N(a)}) \\tag{1.3}\" eeimg=\"1\"/> </p><p> 那么 <img src=\"https://www.zhihu.com/equation?tex=p\" alt=\"p\" eeimg=\"1\"/> 就可以通过因子图被因式分解</p><hr/><p>(1.3)式和(1.1)式是对无向图的同一种表述方式，在(1.1)中，如果 <img src=\"https://www.zhihu.com/equation?tex=%5C%7BY_%7BN%28a%29%7D%7C%5Cforall+a+%5Cin+F%5C%7D\" alt=\"\\{Y_{N(a)}|\\forall a \\in F\\}\" eeimg=\"1\"/> ，那么(2.1)表示的无向图模型就刚好是 <img src=\"https://www.zhihu.com/equation?tex=G\" alt=\"G\" eeimg=\"1\"/> 因式分解出的所有的概率分布。</p><p>好了，已经很懵逼了，如果有幸有人能看到这里我真得谢谢了，我翻译的水平比我的数学水平还要差。举个例子吧，说点人话。下图表示有三个随机变量组成的因子图，圆圈代表变量节点，黑色矩形代表因子节点 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-c058a480cc4987797604be16ec18de1b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"892\" data-rawheight=\"192\" class=\"origin_image zh-lightbox-thumb\" width=\"892\" data-original=\"https://pic4.zhimg.com/v2-c058a480cc4987797604be16ec18de1b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;892&#39; height=&#39;192&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"892\" data-rawheight=\"192\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"892\" data-original=\"https://pic4.zhimg.com/v2-c058a480cc4987797604be16ec18de1b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-c058a480cc4987797604be16ec18de1b_b.jpg\"/></figure><p> 这张因子图描述的概率分布</p><p><img src=\"https://www.zhihu.com/equation?tex=p%28y_1%2Cy_2%2Cy_3%29%3D%5CPsi_1%28y_1%2Cy_2%29%5CPsi_2%28y_2%2Cy_3%29%5CPsi_3%28y_1%2Cy_3%29\" alt=\"p(y_1,y_2,y_3)=\\Psi_1(y_1,y_2)\\Psi_2(y_2,y_3)\\Psi_3(y_1,y_3)\" eeimg=\"1\"/> </p><p>这个式子很容易的会让我们联想到条件概率公式，实际上两者确实有一些联系，什么联系呢？马尔可夫网络。马尔可夫网络直观的描述了多变量概率分布的条件独立关系。但是马尔可夫网络中只由随机变量，没有因子，对照上图来讲就是马尔可夫网络中只有圆形节点有意义，而边是没有数量上的意义的。为了更清晰的说明这种联系，我们让 <img src=\"https://www.zhihu.com/equation?tex=G\" alt=\"G\" eeimg=\"1\"/> 为一个无向图，整数 <img src=\"https://www.zhihu.com/equation?tex=V%3D%5C%7B1%2C2%2C3%2C...%2C%7CY%7C%5C%7D\" alt=\"V=\\{1,2,3,...,|Y|\\}\" eeimg=\"1\"/> 是无向图中的随机变量，对于索引 <img src=\"https://www.zhihu.com/equation?tex=s%5Cin+V\" alt=\"s\\in V\" eeimg=\"1\"/> 的随机变量，用 <img src=\"https://www.zhihu.com/equation?tex=N%28s%29\" alt=\"N(s)\" eeimg=\"1\"/> 表示它的相邻节点。那么如果一个分布 <img src=\"https://www.zhihu.com/equation?tex=p\" alt=\"p\" eeimg=\"1\"/> 满足局部马尔可夫性(local Markov property)，即对于任意两个变量 <img src=\"https://www.zhihu.com/equation?tex=Y_s%2CY_t%5Cin+Y\" alt=\"Y_s,Y_t\\in Y\" eeimg=\"1\"/> ， <img src=\"https://www.zhihu.com/equation?tex=Y_s\" alt=\"Y_s\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=Y_t\" alt=\"Y_t\" eeimg=\"1\"/> 在给定 <img src=\"https://www.zhihu.com/equation?tex=Y_%7BN%28s%29%7D\" alt=\"Y_{N(s)}\" eeimg=\"1\"/> 的条件下是独立的，那么就可以说 <img src=\"https://www.zhihu.com/equation?tex=p\" alt=\"p\" eeimg=\"1\"/> 是关于无向图 <img src=\"https://www.zhihu.com/equation?tex=G\" alt=\"G\" eeimg=\"1\"/> 的马尔可夫(注：以上是原文直译，实际上应该说无向图 <img src=\"https://www.zhihu.com/equation?tex=G\" alt=\"G\" eeimg=\"1\"/> 是一个马尔可夫随机场更容易理解一点)。直觉上，上述表述一维着 <img src=\"https://www.zhihu.com/equation?tex=Y_%7BN%28s%29%7D\" alt=\"Y_{N(s)}\" eeimg=\"1\"/> 包含了预测 <img src=\"https://www.zhihu.com/equation?tex=Y_s\" alt=\"Y_s\" eeimg=\"1\"/> 的所有信息。也就是说变量 <img src=\"https://www.zhihu.com/equation?tex=Y_s\" alt=\"Y_s\" eeimg=\"1\"/> 的取值只跟它相邻的节点有关，如果你熟悉马尔可夫定理，现在应该就很明白为什么会有这么多马尔可夫了。</p><p>最后一点关于马尔可夫的不确定性，在了解马尔可夫网络歧义性之前我们先看看如何通过无向图的因式分解构造对应的马尔可夫网络。说来也简单，对于(1.1)式表示的一个概率分布，只需要把共享同一个因子的两个变量连起来就成了一个马尔可夫网络，这也是带来不确定性的原因，看下图。 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-04c57913dceffdda301b85d6a0949a7b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"918\" data-rawheight=\"284\" class=\"origin_image zh-lightbox-thumb\" width=\"918\" data-original=\"https://pic4.zhimg.com/v2-04c57913dceffdda301b85d6a0949a7b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;918&#39; height=&#39;284&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"918\" data-rawheight=\"284\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"918\" data-original=\"https://pic4.zhimg.com/v2-04c57913dceffdda301b85d6a0949a7b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-04c57913dceffdda301b85d6a0949a7b_b.jpg\"/></figure><p> 考虑如图左1所示的由三个节点构成的马尔可夫网络，任意可以分解为 <img src=\"https://www.zhihu.com/equation?tex=p%28y_1%2Cy_2%2Cy_3%29%5Cpropto+f%28y_1%2Cy_2%2Cy_3%29%2Cf+%5Cgeq+0\" alt=\"p(y_1,y_2,y_3)\\propto f(y_1,y_2,y_3),f \\geq 0\" eeimg=\"1\"/> 的分布都是关于这个图的马尔可夫随机场，然而如果我们想要用更严格的参数来限定这个分布，比如 <img src=\"https://www.zhihu.com/equation?tex=p%28y_1%2Cy_2%2Cy_3%29%5Cpropto+f%28y_1%2Cy_2%29g%28y_2%2Cy_3%29h%28y_1%2Cy_3%29%2Cf%2Cg%2Ch+%5Cgeq+0\" alt=\"p(y_1,y_2,y_3)\\propto f(y_1,y_2)g(y_2,y_3)h(y_1,y_3),f,g,h \\geq 0\" eeimg=\"1\"/> ，这个更严格的形式中，我们可以用更少的数据去估计这个分布。也就是说右边和中间的图都可以用来表示左图所示的马尔可夫网络，而这两个无向图又是确确实实有差别的。这就是马尔可夫网络的不确定性。</p><h2>1.1.2 有向图</h2><p>无向图模型中的势函数(local function)不需要方向，而有向图模型中描述了一个分布如何因式分解成局部条件概率分布。设 <img src=\"https://www.zhihu.com/equation?tex=G\" alt=\"G\" eeimg=\"1\"/> 是一个有向无环图， <img src=\"https://www.zhihu.com/equation?tex=%5Cpi%28s%29\" alt=\"\\pi(s)\" eeimg=\"1\"/> 是代表 <img src=\"https://www.zhihu.com/equation?tex=Y_s\" alt=\"Y_s\" eeimg=\"1\"/> 在 <img src=\"https://www.zhihu.com/equation?tex=G\" alt=\"G\" eeimg=\"1\"/> 的父节点，一个有向图模型是一族可以因式分解为下式的分布：</p><p><img src=\"https://www.zhihu.com/equation?tex=+p%28%5Crm+%5Cvec+y%29%3D%5Cprod_%7Bs%3D1%7D%5E%7BS%7Dp%28y_s%7C%5Crm+%5Cvec+y_%7B%5Cit+%5Cpi%28s%29%7D%29\" alt=\" p(\\rm \\vec y)=\\prod_{s=1}^{S}p(y_s|\\rm \\vec y_{\\it \\pi(s)})\" eeimg=\"1\"/> </p><p>其中 <img src=\"https://www.zhihu.com/equation?tex=p%28y_s%7C%5Crm+%5Cvec+y_%7B%5Cit+%5Cpi+%28s%29%7D%29\" alt=\"p(y_s|\\rm \\vec y_{\\it \\pi (s)})\" eeimg=\"1\"/> 是<b>局部条件分布（local conditional distribution）</b>。注意当变量没有父节点时 <img src=\"https://www.zhihu.com/equation?tex=%5Cpi%28s%29\" alt=\"\\pi(s)\" eeimg=\"1\"/> 可以为空，此时 <img src=\"https://www.zhihu.com/equation?tex=p%28y_s%7C%5Crm+%5Cvec+y_%7B%5Cit+%5Cpi%28s%29%7D%29\" alt=\"p(y_s|\\rm \\vec y_{\\it \\pi(s)})\" eeimg=\"1\"/> 变成了 <img src=\"https://www.zhihu.com/equation?tex=p%28y_s%29\" alt=\"p(y_s)\" eeimg=\"1\"/> 。</p><p>和式(1.1)对比一下可以发现这里少了一个正则项，实际上有向图模型可以看作是一种每个因子都被单独的正则化（locally normalized）了的(也就是说每一个因子都等同于随机变量的子集的条件概率分布)无向图。有向图经常被用在生成式模型中，至于生成式和判别式后面将会讲到。</p><h2>1.2 生成式和判别式模型</h2><p>这一小节我们要讨论几个在NLP(natural language processing)中已经使用了的图模型的例子，这里面有些内容是前面提到过的，有些内容是将会在后面条件随机场中再次讨论，强烈建议重点关注隐马尔科夫模型，因为这和后面将要讲到的重点线性链条件随机场(linear-chain CRF)息息相关。 这一节最主要的内容就是对比生成式和判别式模型，会涉及到两个生成式模型（朴素贝叶斯和隐马尔科夫）和一个判别式模型（逻辑回归）。生成式模型是描述标签向量 <img src=\"https://www.zhihu.com/equation?tex=%5Crm+%5Cvec+y\" alt=\"\\rm \\vec y\" eeimg=\"1\"/> 如何按概率生成特征向量 <img src=\"https://www.zhihu.com/equation?tex=%5Crm+%5Cvec+x\" alt=\"\\rm \\vec x\" eeimg=\"1\"/> 。而判别式模型与之相反，描述了如何通过提取向量 <img src=\"https://www.zhihu.com/equation?tex=%5Crm+%5Cvec+x\" alt=\"\\rm \\vec x\" eeimg=\"1\"/> 的特征然后把它分配到对应的标签中去。理论上讲二者可以通过贝叶斯公式互相转化，但是实际上这两个模式各有各的好处。会在后面详细讨论。</p><h2>1.2.1 分类问题</h2><p>首先我们讨论一下分类问题，就是给定一个特征向量 <img src=\"https://www.zhihu.com/equation?tex=%5Crm+%5Cvec+x%3D%5Cit+%28x_1%2Cx_2%2C...%2Cx_k%29\" alt=\"\\rm \\vec x=\\it (x_1,x_2,...,x_k)\" eeimg=\"1\"/> ，预测它的所归属的类别 <img src=\"https://www.zhihu.com/equation?tex=y\" alt=\"y\" eeimg=\"1\"/> 。一个简单的方法就是假设一旦知道了特征向量对应的类别，那么特征向量中的特征就是相互独立的了。这种分类器叫<b>朴素贝叶斯分类器</b>，也就是说特征是相对于标签条件独立的，如下式：</p><p><img src=\"https://www.zhihu.com/equation?tex=+p%28y%2C%5Crm+%5Cvec+x%29%3D%5Cit+p%28y%29%5Cprod_%7Bk%3D1%7D%5EKp%28x_k%7Cy%29+%5Ctag%7B1.4%7D\" alt=\" p(y,\\rm \\vec x)=\\it p(y)\\prod_{k=1}^Kp(x_k|y) \\tag{1.4}\" eeimg=\"1\"/> </p><p> 这种模型可以用下图左中的有向图表示，同样的，我们也可以把这个模型写成因子图的形式。定义 <img src=\"https://www.zhihu.com/equation?tex=%5CPsi+%28y%29%3Dp%28y%29\" alt=\"\\Psi (y)=p(y)\" eeimg=\"1\"/> ，对于特征向量中的每一个特征定义 <img src=\"https://www.zhihu.com/equation?tex=%5CPsi_k%28y%2Cx_k%29%3Dp%28x_k%7Cy%29\" alt=\"\\Psi_k(y,x_k)=p(x_k|y)\" eeimg=\"1\"/> ，这样就可以把有向图模型转化为无向图模型，如下图右所示。 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-9751d0dd4f1b6046c34bc9ca8b1403d3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1261\" data-rawheight=\"322\" class=\"origin_image zh-lightbox-thumb\" width=\"1261\" data-original=\"https://pic4.zhimg.com/v2-9751d0dd4f1b6046c34bc9ca8b1403d3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1261&#39; height=&#39;322&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1261\" data-rawheight=\"322\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1261\" data-original=\"https://pic4.zhimg.com/v2-9751d0dd4f1b6046c34bc9ca8b1403d3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-9751d0dd4f1b6046c34bc9ca8b1403d3_b.jpg\"/></figure><p> 另一个众所周知的可以很自然的用图模型便是的分类器是<b>逻辑回归（logistic regression）</b>（在NLP领域也被叫做最大熵模型）（根据后文看这里作者指的应该是多项逻辑回归）。这个分类器假设每一个类别的对数概率 <img src=\"https://www.zhihu.com/equation?tex=%5Clog+p%28y%7C%5Crm+%5Cvec+x%29\" alt=\"\\log p(y|\\rm \\vec x)\" eeimg=\"1\"/> 是关于特征向量 <img src=\"https://www.zhihu.com/equation?tex=%5Crm+%5Cvec+x\" alt=\"\\rm \\vec x\" eeimg=\"1\"/> 的线性函数加上一个正则项（保证概率和为1），这样就会产生一个条件概率分布：</p><p><img src=\"https://www.zhihu.com/equation?tex=+p%28y%7C%5Crm+%5Cvec+x%29%3D%5Cfrac%7B1%7D%7BZ%28%5Crm+%5Cvec+x%29%7D%5Cexp+%5Cit+%7B%5Ctheta_y%2B%5Csum_%7Bi%3D1%7D%5EK%5Ctheta_%7By%2Cj%7Dx_j%7D%2C+%5Ctag%7B1.5%7D\" alt=\" p(y|\\rm \\vec x)=\\frac{1}{Z(\\rm \\vec x)}\\exp \\it {\\theta_y+\\sum_{i=1}^K\\theta_{y,j}x_j}, \\tag{1.5}\" eeimg=\"1\"/> </p><p>其中 <img src=\"https://www.zhihu.com/equation?tex=Z%28%5Crm+%5Cvec+x%29%3D%5Cit+%5Csum_y+%5Cexp%7B%5Ctheta_y%2B%5Csum_%7Bj%3D1%7D%5EK%5Ctheta_%7By%2Cj%7Dx_j%7D\" alt=\"Z(\\rm \\vec x)=\\it \\sum_y \\exp{\\theta_y+\\sum_{j=1}^K\\theta_{y,j}x_j}\" eeimg=\"1\"/> 是正则常量， <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_y\" alt=\"\\theta_y\" eeimg=\"1\"/> 是偏置权重，类似于朴素贝叶斯公式中的 <img src=\"https://www.zhihu.com/equation?tex=%5Clog+p%28y%29\" alt=\"\\log p(y)\" eeimg=\"1\"/> 。可以通过一些技巧让权重共享，而不是每一个类都使用一个权重。技巧的关键在于定义一组特征函数。在使用这个技巧前先把上式整理一下,把公式展开分子分母消去 <img src=\"https://www.zhihu.com/equation?tex=%5Cexp%28%5Ctheta_y%29\" alt=\"\\exp(\\theta_y)\" eeimg=\"1\"/> 可得 </p><p><img src=\"https://www.zhihu.com/equation?tex=p%28y%7C%5Crm+%5Cvec+x%29%3D%5Cfrac%7B1%7D%7BZ%28%5Crm+%5Cvec+x%29%7D%5Cexp+%5Cit+%7B%5Ctheta_y%2B%5Csum_%7Bi%3D1%7D%5EK%5Ctheta_%7By%2Cj%7Dx_j%7D%3D%5Cfrac%7B%5Cexp%28+%5Cit+%5Csum_%7Bi%3D1%7D%5EK%5Ctheta_%7By%2Cj%7Dx_j%29%7D%7B%5Cit+%5Csum_y+%5Cexp%28%5Csum_%7Bj%3D1%7D%5EK%5Ctheta_%7By%2Cj%7Dx_j%29%7D\" alt=\"p(y|\\rm \\vec x)=\\frac{1}{Z(\\rm \\vec x)}\\exp \\it {\\theta_y+\\sum_{i=1}^K\\theta_{y,j}x_j}=\\frac{\\exp( \\it \\sum_{i=1}^K\\theta_{y,j}x_j)}{\\it \\sum_y \\exp(\\sum_{j=1}^K\\theta_{y,j}x_j)}\" eeimg=\"1\"/> </p><p>然后看看特征函数，对特征权重，特征函数的定义为 <img src=\"https://www.zhihu.com/equation?tex=f_%7By%5E%7B%27%7D%2Cj%28y%2C%5Crm+%5Cvec+x%29%7D%3D%5Cbold1_%7B%5C%7By%5E%7B%27%7D%3Dy%5C%7D%7Dx_j\" alt=\"f_{y^{&#39;},j(y,\\rm \\vec x)}=\\bold1_{\\{y^{&#39;}=y\\}}x_j\" eeimg=\"1\"/> ，对偏置权重的特征函数后面会被约去这里就不说了。用 <img src=\"https://www.zhihu.com/equation?tex=f_k\" alt=\"f_k\" eeimg=\"1\"/> 来索引特征函数 <img src=\"https://www.zhihu.com/equation?tex=f_%7By%5E%7B%27%7D%2Cj%7D%2C%5Ctheta_k\" alt=\"f_{y^{&#39;},j},\\theta_k\" eeimg=\"1\"/> 索引对应的权重 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_%7By%5E%7B%27%7D%2Cj%7D\" alt=\"\\theta_{y^{&#39;},j}\" eeimg=\"1\"/> 。然后带入上式，最终的逻辑回归变为如下形式：</p><p><img src=\"https://www.zhihu.com/equation?tex=p%28y%7C%5Crm+%5Cvec+x%29%3D%5Cit+%5Cfrac%7B1%7D%7BZ%7D%5Cexp%7B%7B%5Csum_%7Bk%3D1%7D%5EK%5Ctheta_kf_k%28y%2C%5Crm+%5Cvec+x%29%7D%7D\" alt=\"p(y|\\rm \\vec x)=\\it \\frac{1}{Z}\\exp{{\\sum_{k=1}^K\\theta_kf_k(y,\\rm \\vec x)}}\" eeimg=\"1\"/> </p><p> 这里介绍这种形式主要是因为后面谈到的CRF用了这种类似的形式。</p><h2>1.2.2 序列模型</h2><p>分类器只预测了单一类别的变量，但是图模型真正强大的地方在于它们可以为有内在依赖关系的多变量建模。这一节我们会讨论最简单的一种依赖形式，图模型的输出变量是以序列的形式排列的。我们用NLP领域中的实体识别（NER）来引入这个问题。实体识别就是识别文本中的名字，比如人名，地名，组织机构名等等并且标注出位置。这个问题的挑战在于就算是在一个很大的训练集中，大部分实体词出现的频率很低，这里不是说实体词频率低，而是它的具体的名字，比如人名，不同的人名字都不一样。所以系统必须通过上下文来判定他们。</p><p>一种处理实体词识别的方法是给每一个词独立的分类为Person,Location,Organization,or Other(非实体)，这种方法的问题在于它假设对于给定的输入，所有的命名实体词相对于相邻的词都是独立的然而实际上像“长江”是一个地名，而“长江大学”是一个机构名，“长江”被划分到哪个类别依赖于他后面的词，所以这个假设太粗糙。一个放宽这个假设的方法是把输出变量用线性链的方式排列起来，也就是隐马尔可夫模型(HMM)采用的方式。（这里注意一下如何从最简单的模型到马尔可夫模型到后面的条件随机场）。一个马尔可夫模型通过假设存在一个状态序列 <img src=\"https://www.zhihu.com/equation?tex=Y%3D%7By_t%7D%5ET_%7Bt%3D1%7D\" alt=\"Y={y_t}^T_{t=1}\" eeimg=\"1\"/> 为观察到的序列 <img src=\"https://www.zhihu.com/equation?tex=X%3D%7BX_t%7D%5ET_%7Bt%3D1%7D\" alt=\"X={X_t}^T_{t=1}\" eeimg=\"1\"/> 建模。使 <img src=\"https://www.zhihu.com/equation?tex=S\" alt=\"S\" eeimg=\"1\"/> 为一个有限的由所有可能的状态组成的集合， <img src=\"https://www.zhihu.com/equation?tex=O\" alt=\"O\" eeimg=\"1\"/> 为一个有限的由所有可能的观察组成的集合，也就是说对于所有的 <img src=\"https://www.zhihu.com/equation?tex=t%2Cx_t%5Cin+O%2Cy_t+%5Cin+S\" alt=\"t,x_t\\in O,y_t \\in S\" eeimg=\"1\"/> 。在实体识别的例子中，每一个观察 <img src=\"https://www.zhihu.com/equation?tex=x_t\" alt=\"x_t\" eeimg=\"1\"/> 就是位置 <img src=\"https://www.zhihu.com/equation?tex=t\" alt=\"t\" eeimg=\"1\"/> 的词，而每一个状态 <img src=\"https://www.zhihu.com/equation?tex=y_t\" alt=\"y_t\" eeimg=\"1\"/> 就是命名实体标签，即Person，Location,Organization,or Other中的一个。</p><p>为了给联合概率分布 <img src=\"https://www.zhihu.com/equation?tex=p%28%5Crm+%5Cvec+y%2C+%5Cvec+x%29\" alt=\"p(\\rm \\vec y, \\vec x)\" eeimg=\"1\"/> 合适的建模，隐马尔可夫模型做了两个独立性假设。首先，假设每一个状态只依赖于当前的前一个状态，也就是说在给定 <img src=\"https://www.zhihu.com/equation?tex=y_%7Bt-1%7D\" alt=\"y_{t-1}\" eeimg=\"1\"/> 之后， <img src=\"https://www.zhihu.com/equation?tex=y_t\" alt=\"y_t\" eeimg=\"1\"/> 相对于所有的 <img src=\"https://www.zhihu.com/equation?tex=y_1%2Cy_2%2C...%2Cy_%7Bt-2%7D\" alt=\"y_1,y_2,...,y_{t-2}\" eeimg=\"1\"/> 都是独立的。其次，假设每一个观察变量 <img src=\"https://www.zhihu.com/equation?tex=x_t\" alt=\"x_t\" eeimg=\"1\"/> 只依赖于当前的状态变量 <img src=\"https://www.zhihu.com/equation?tex=y_t\" alt=\"y_t\" eeimg=\"1\"/> 。有了这些假设我们就可以用三个概率分布来准确的描述隐马尔可夫模型：</p><p>1，概率分布 <img src=\"https://www.zhihu.com/equation?tex=p%28y_1%29\" alt=\"p(y_1)\" eeimg=\"1\"/> ，对应于初始状态；</p><p>2，转移概率分布 <img src=\"https://www.zhihu.com/equation?tex=p%28y_t%7Cy_%7Bt-1%7D%29\" alt=\"p(y_t|y_{t-1})\" eeimg=\"1\"/> ；</p><p>3，观察的概率分布 <img src=\"https://www.zhihu.com/equation?tex=p%28x_t%7Cy_t%29\" alt=\"p(x_t|y_t)\" eeimg=\"1\"/> 。</p><p>这样，状态序列 <img src=\"https://www.zhihu.com/equation?tex=%5Crm%5Cvec+y\" alt=\"\\rm\\vec y\" eeimg=\"1\"/> 和观察序列 <img src=\"https://www.zhihu.com/equation?tex=%5Crm+%5Cvec+x\" alt=\"\\rm \\vec x\" eeimg=\"1\"/> 的联合概率就可以分解为下式</p><p><img src=\"https://www.zhihu.com/equation?tex=+p%28%5Crm+%5Cvec+y%2C%5Cvec+x%29%3D%5Cit+%5Cprod+%7Bt%3D1%7D%5ETp%28y_t%7Cy%7Bt-1%7D%29p%28x_t%7Cy_t%29+%5Ctag%7B1.6%7D\" alt=\" p(\\rm \\vec y,\\vec x)=\\it \\prod {t=1}^Tp(y_t|y{t-1})p(x_t|y_t) \\tag{1.6}\" eeimg=\"1\"/> </p><p>上式的标记是一种简化了（就是让式子看起来更简洁好看一点）的形式，我们创造一个”假“的初始状态 <img src=\"https://www.zhihu.com/equation?tex=y_0\" alt=\"y_0\" eeimg=\"1\"/> ，它的大小接近0并且是每一个状态序列的开端。这样呢我们就可以把 <img src=\"https://www.zhihu.com/equation?tex=p%28y_1%29\" alt=\"p(y_1)\" eeimg=\"1\"/> 写成 <img src=\"https://www.zhihu.com/equation?tex=p%28y_1%7Cy_0%29\" alt=\"p(y_1|y_0)\" eeimg=\"1\"/> (想想看不做这个”假“上式会变成什么样子)。</p><p>隐马尔可夫模型已经被用在很多自然语言处理领域的序列标注的问题上，比如词性标注，命名实体识别和信息抽取。</p><h2>1.2.3 对比</h2><p>生成式模型和判别式模型都描述了 <img src=\"https://www.zhihu.com/equation?tex=%28%5Crm+%5Cvec+y%2C+%5Cvec+x%29\" alt=\"(\\rm \\vec y, \\vec x)\" eeimg=\"1\"/> 的分布，但是他们各自的方向不同。一个生成式模型例如朴素贝叶斯模型和隐马尔可夫模型，是一族联合概率分布的因式分解形式 <img src=\"https://www.zhihu.com/equation?tex=p%28%5Crm+%5Cvec+y%2C+%5Cvec+x%29%3D%5Cit+p%28%5Crm+%5Cvec+y%29%5Cit+p%28%5Crm+%5Cvec+x%7C+%5Cvec+y%29\" alt=\"p(\\rm \\vec y, \\vec x)=\\it p(\\rm \\vec y)\\it p(\\rm \\vec x| \\vec y)\" eeimg=\"1\"/> 。也就是说它描述了在给定标签以后如何采样，或者说是“生成”，特征值。而判别式模型，例如逻辑回归是一族条件概率分布 <img src=\"https://www.zhihu.com/equation?tex=p%28%5Crm+%5Cvec+y%7C%5Cvec+x%29\" alt=\"p(\\rm \\vec y|\\vec x)\" eeimg=\"1\"/> ，也就是说，对于判别式来讲，会直接为分类规则建模。理论上来讲，一个判别式也可以通过给输入提供一个边缘概率分 <img src=\"https://www.zhihu.com/equation?tex=p%28%5Crm+%5Cvec+x%29\" alt=\"p(\\rm \\vec x)\" eeimg=\"1\"/> 布来获得联合概率分布 <img src=\"https://www.zhihu.com/equation?tex=p%28%5Crm+%5Cvec+y%2C+%5Cvec+x%29\" alt=\"p(\\rm \\vec y, \\vec x)\" eeimg=\"1\"/> ，但没有人这么干。</p><p>判别式和生成式概念上最主要的区别是条件概率分布 <img src=\"https://www.zhihu.com/equation?tex=p%28%5Crm+%5Cvec+y%7C%5Cvec+x%29\" alt=\"p(\\rm \\vec y|\\vec x)\" eeimg=\"1\"/> 不包含 <img src=\"https://www.zhihu.com/equation?tex=p%28%5Crm+%5Cvec+x%29\" alt=\"p(\\rm \\vec x)\" eeimg=\"1\"/> （所以判别式不能通过标签生成样本数据），因为这对类别判定来说是不必要的。为 <img src=\"https://www.zhihu.com/equation?tex=p%28%5Crm+%5Cvec+x%29\" alt=\"p(\\rm \\vec x)\" eeimg=\"1\"/> 建模最大的困难在于它通常对包含很多具有高度依赖性的特征，这种特征通常是很难建模的。比如命名实体识别，隐马尔可夫模型就只依赖于一个特征，词本身。（注意前面讲到的隐马尔可夫模型，他实际上依赖于词和上一个状态，但是状态不是特征向量中来的，所以作者在此说只依赖于一个特征）。但是很多词，特别是人名，很可能不会出现在训练集中，所以词本身的特征是不包含任何信息的，（通常这些没有见过的词在程序中都是同一个词向量）。为了给这些词打标签，我们回去去发掘这些词的其他特征，比如首字母大小写，相邻词，和他的前缀后缀，它是否在之前预测过的实体词当中等等。</p><p>判别模型最主要的优势是它在高纬度且重合度高的特征上表现得更好，为了解释这一点，考虑一族朴素贝叶斯分布如式(1.4)所示。他们的条件概率的形式都可以用逻辑回归的形式表示如式(1.5)所示。但是实际上很多其他的联合概率分布模型，有些特征向量 <img src=\"https://www.zhihu.com/equation?tex=%5Crm+%5Cvec+x\" alt=\"\\rm \\vec x\" eeimg=\"1\"/> 之间的依赖关系很复杂，他们的条件形式也和(1.5)式相同。也就是说你们生成式建得了模的，我们判别式能建模，你们生成式建不了的，我们判别式也能建模（听着耳熟）。通过直接为条件概率建模，我们可以在 <img src=\"https://www.zhihu.com/equation?tex=p%28%5Crm+%5Cvec+x%29\" alt=\"p(\\rm \\vec x)\" eeimg=\"1\"/> 的形式未知的情况下建立一个有效的模型。判别模型比如CRF，假设 <img src=\"https://www.zhihu.com/equation?tex=%5Crm+%5Cvec+y\" alt=\"\\rm \\vec y\" eeimg=\"1\"/> 之间条件独立，并且假设了 <img src=\"https://www.zhihu.com/equation?tex=%5Crm+%5Cvec+y\" alt=\"\\rm \\vec y\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5Crm+%5Cvec+x\" alt=\"\\rm \\vec x\" eeimg=\"1\"/> 之间的依赖关系，但没有做 <img src=\"https://www.zhihu.com/equation?tex=%5Crm+%5Cvec+x\" alt=\"\\rm \\vec x\" eeimg=\"1\"/> 之间的条件独立性假设。这一点也可以通过图模型来理解，假如我们有一个表示联合概率分布 <img src=\"https://www.zhihu.com/equation?tex=p%28%5Crm+%5Cvec+y%7C+%5Cvec+x%29\" alt=\"p(\\rm \\vec y| \\vec x)\" eeimg=\"1\"/> 的因子图。如果我们继续构建关于它的条件概率分布 <img src=\"https://www.zhihu.com/equation?tex=p%28%5Crm+%5Cvec+y%7C+%5Cvec+x%29\" alt=\"p(\\rm \\vec y| \\vec x)\" eeimg=\"1\"/> ，那么任何只跟 <img src=\"https://www.zhihu.com/equation?tex=%5Crm+%5Cvec+x\" alt=\"\\rm \\vec x\" eeimg=\"1\"/> 相关的因子就会从因子图中消失，杜宇条件概率他们式无关紧要的，因为他们是相对于 <img src=\"https://www.zhihu.com/equation?tex=%5Crm+%5Cvec+y\" alt=\"\\rm \\vec y\" eeimg=\"1\"/> 的常量。(无向图因为式无向的，无向图中所有的节点都是相关的，如果它只跟 <img src=\"https://www.zhihu.com/equation?tex=%5Crm+%5Cvec+x\" alt=\"\\rm \\vec x\" eeimg=\"1\"/> 相关那么它就不会出现在无向图中，因为无向图中存在 <img src=\"https://www.zhihu.com/equation?tex=y%EF%BC%8Cy\" alt=\"y，y\" eeimg=\"1\"/> 跟 <img src=\"https://www.zhihu.com/equation?tex=x\" alt=\"x\" eeimg=\"1\"/> 相关)</p><p>为了在生成模型中考虑特征之间的内在联系，我们有两个选择。一个是强化这个模型让它可以表示输入之间的依赖关系，也就是说直接为 <img src=\"https://www.zhihu.com/equation?tex=x_t\" alt=\"x_t\" eeimg=\"1\"/> 添加边，但是通常这很难办到，比如很难想象如何去给单词的大小写和它的后缀之间的依赖关系建模。另一个是简化独立性假设，比如朴素贝叶斯模型。以隐马尔可夫模型为例，一个使用了朴素贝叶斯假设的隐马尔可夫模型有这样的形式： <img src=\"https://www.zhihu.com/equation?tex=p%28%5Crm+%5Cvec+x%2C%5Cvec+y%29%3D%5Cit+%5Cprod_%7Bt%3D1%7D%5ETp%28y_t%7Cy_%7Bt-1%7D%29%5Cprod_%7Bk%3D1%7D%5EKp%28x_%7Btk%7D%7Cy_t%29\" alt=\"p(\\rm \\vec x,\\vec y)=\\it \\prod_{t=1}^Tp(y_t|y_{t-1})\\prod_{k=1}^Kp(x_{tk}|y_t)\" eeimg=\"1\"/> 。这种想法有时候很有效，但是有时候也会有问题，因为这种独立性假设会降低准确率。比如虽然朴素贝叶斯分类器在文档分类上表现得很好，但是在很多应用中它不如逻辑回归。</p><p>除此之外，朴素贝叶斯模型可能会产生比较差的概率估计，举个例子，想象一下在一个二分类问题上训练一个朴素贝叶斯模型，这个二分类的样本中所有特征都是重复的，也就是说对于一个特征向量 <img src=\"https://www.zhihu.com/equation?tex=%5Crm+%5Cvec+x%3D%5Cit+%28x_1%2Cx_2%2C...%2Cx_K%29\" alt=\"\\rm \\vec x=\\it (x_1,x_2,...,x_K)\" eeimg=\"1\"/> ，我们把他变换成 <img src=\"https://www.zhihu.com/equation?tex=%5Crm+%5Cvec+x%5E%7B%27%7D%3D%5Cit+%28x_1%2Cx_1%2Cx_2%2Cx_2%2C...x_K%2Cx_K%29\" alt=\"\\rm \\vec x^{&#39;}=\\it (x_1,x_1,x_2,x_2,...x_K,x_K)\" eeimg=\"1\"/> 然后训练模型，虽然数据中没有加入新的信息，但是这种变化将会提升概率估计的置信度(confidence)，这就意味着模型对 <img src=\"https://www.zhihu.com/equation?tex=p%28y%7C%5Crm+%5Cvec+x%29\" alt=\"p(y|\\rm \\vec x)\" eeimg=\"1\"/> 的估计将会偏离 <img src=\"https://www.zhihu.com/equation?tex=p%28y%7C%5Crm+%5Cvec+x%29\" alt=\"p(y|\\rm \\vec x)\" eeimg=\"1\"/> 的实际值0.5。</p><p>像朴素贝叶斯这样的假设在用于生成序列模型的时候问题会变得更明显，因为预测应该结合置信度。如果序列模型的每个位置的概率估计置信度都偏高(overconfident)，那就很难合理的使用他们。朴素贝叶斯模型和逻辑回归的区别只跟一个因素相关，前者是生成式的，后者是判别式的；二者对于离散型的输入在其他方面是完全等价的。朴素贝叶斯和逻辑回归考虑了相同的假设空间，在这种情况下任何逻辑回归都可以以相同的预测边界转化为贝叶斯分类器，反之亦然。换种说法就是朴素贝叶斯模型(1.4)和逻辑回归(1.5)定义了相同的一族概率分布。如果我们用生成式的方式解释它就是：</p><p><img src=\"https://www.zhihu.com/equation?tex=p%28y%2C%5Crm+%5Cvec+x%29%3D%5Cfrac%7B%5Cexp+%7B%5Csum_k%5Ctheta_kf_k%28y%2C%5Crm+%5Cvec+x%29%7D%7D%7B%5Cit+%5Csum_%7B%5Ctilde%7By%7D%2C%5Ctilde%7Bx%7D%7D%5Cexp+%7B%5Csum_k+%5Ctheta_kf_k%28%5Ctilde%7By%7D%2C%5Ctilde%7Bx%7D%29%7D%7D%5Ctag%7B1.7%7D\" alt=\"p(y,\\rm \\vec x)=\\frac{\\exp {\\sum_k\\theta_kf_k(y,\\rm \\vec x)}}{\\it \\sum_{\\tilde{y},\\tilde{x}}\\exp {\\sum_k \\theta_kf_k(\\tilde{y},\\tilde{x})}}\\tag{1.7}\" eeimg=\"1\"/> </p><p> 这就意味着训练朴素贝叶斯模型是在对条件概率做最大似然，和我们可以从逻辑回归中得到的分类器相同，反过来，如果把逻辑回归模型给一个生成式的解释，如式(1.7)，并且通过对联合概率分布 <img src=\"https://www.zhihu.com/equation?tex=p%28y%2C%5Crm+%5Cvec+x%29\" alt=\"p(y,\\rm \\vec x)\" eeimg=\"1\"/> 做最大似然，那么我们可以得到和朴素贝叶斯模型相同的分类器。用吴恩达和乔丹的术语来讲，朴素贝叶斯模型和逻辑回归式来自于同一个<b>生成-判别对（generative-discriminative pair）</b>。</p><p>理论上这两种方法为什么会表现的如此不同其实并不是太清楚，因为我们总可以用贝叶斯定理去在这两种模型中做转换。比如在朴素贝叶斯模型中把 <img src=\"https://www.zhihu.com/equation?tex=p%28%5Crm+%5Cvec+y%29p%28%5Cvec+x+%7C%5Cvec+y%29\" alt=\"p(\\rm \\vec y)p(\\vec x |\\vec y)\" eeimg=\"1\"/> 转换为条件概率分布 <img src=\"https://www.zhihu.com/equation?tex=p%28%5Crm+%5Cvec+y%7C%5Cvec+x%29\" alt=\"p(\\rm \\vec y|\\vec x)\" eeimg=\"1\"/> 很容易，实际上这种条件模型和逻辑回归有相同的形式。并且如果我们设法去获得一个数据集的“真实”的生成模型，也就是说通过实实在在的从数据中采样获取分布 <img src=\"https://www.zhihu.com/equation?tex=p%5E%7B%2A%7D%28%5Crm+%5Cvec+y%7C%5Cvec+x%29%3D%5Cit+p%5E%7B%2A%7D+%5Crm+%28%5Crm+%5Cvec+y%29%5Cit+p%5E%2A%28%5Crm+%5Cvec+x%7C%5Cvec+y%29\" alt=\"p^{*}(\\rm \\vec y|\\vec x)=\\it p^{*} \\rm (\\rm \\vec y)\\it p^*(\\rm \\vec x|\\vec y)\" eeimg=\"1\"/> <i>，那么我们就能很简单的计算出真实的</i> <img src=\"https://www.zhihu.com/equation?tex=p%5E%7B%2A%7D%28%5Crm+%5Cvec+y%7C%5Cvec+x%29\" alt=\"p^{*}(\\rm \\vec y|\\vec x)\" eeimg=\"1\"/> ，也就是判别模型要求的目标。但是它更精准，因为在实际训练中，我们获取不到真实的分布，所以这两种方式在实际训练中会有不同。先估计 <img src=\"https://www.zhihu.com/equation?tex=p%28%5Crm+%5Cvec+y%29%5Cit+p%28%5Crm+%5Cvec+x%7C%5Cvec+y%29\" alt=\"p(\\rm \\vec y)\\it p(\\rm \\vec x|\\vec y)\" eeimg=\"1\"/> 然后计算 <img src=\"https://www.zhihu.com/equation?tex=p%28%5Crm+%5Cvec+y%7C%5Cvec+x%29\" alt=\"p(\\rm \\vec y|\\vec x)\" eeimg=\"1\"/> （也就是生成模型所做的）和直接估计 <img src=\"https://www.zhihu.com/equation?tex=p%28%5Crm+%5Cvec+y%7C%5Cvec+x%29\" alt=\"p(\\rm \\vec y|\\vec x)\" eeimg=\"1\"/> 的结果会有不同。换句话说生成式和判别式都是在估计 <img src=\"https://www.zhihu.com/equation?tex=p%28%5Crm+%5Cvec+y%7C%5Cvec+x%29\" alt=\"p(\\rm \\vec y|\\vec x)\" eeimg=\"1\"/> ，但是用了不同的方法。</p><p>一个可以深入透视生成式和判别式之间的不同的方法来自于Minka，假如我们有一个生成式模型 <img src=\"https://www.zhihu.com/equation?tex=p_g\" alt=\"p_g\" eeimg=\"1\"/> ，以 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 为参数，通过定义它有如下形式：</p><p><img src=\"https://www.zhihu.com/equation?tex=+p_g%28%5Crm+%5Cvec+y%2C+%5Cvec+x%3B%5Cit+%5Ctheta%29%3Dp_g%28%5Crm+%5Cvec+y+%3B+%5Cit+%5Ctheta%29p_g%28%5Crm+%5Cvec+x%7C%5Cvec+y%3B+%5Cit+%5Ctheta%29+%5Ctag%7B1.8%7D\" alt=\" p_g(\\rm \\vec y, \\vec x;\\it \\theta)=p_g(\\rm \\vec y ; \\it \\theta)p_g(\\rm \\vec x|\\vec y; \\it \\theta) \\tag{1.8}\" eeimg=\"1\"/> </p><p> 我们也可以用概率链式法则把上式写成：</p><p><img src=\"https://www.zhihu.com/equation?tex=+p_g%28%5Crm+%5Cvec+y%2C+%5Cvec+x%3B%5Cit+%5Ctheta%29%3Dp_g%28%5Crm+%5Cvec+x+%3B+%5Cit+%5Ctheta%29p_g%28%5Crm+%5Cvec+y%7C%5Cvec+x%3B+%5Cit+%5Ctheta%29+%5Ctag%7B1.9%7D\" alt=\" p_g(\\rm \\vec y, \\vec x;\\it \\theta)=p_g(\\rm \\vec x ; \\it \\theta)p_g(\\rm \\vec y|\\vec x; \\it \\theta) \\tag{1.9}\" eeimg=\"1\"/> </p><p>其中使用推论 <img src=\"https://www.zhihu.com/equation?tex=p_g%28%5Crm+%5Cvec+x%3B%5Cit+%5Ctheta%29%3D%5Csum_%7B%5Crm+%5Cvec+y%7D%5Cit+p_g%28%5Crm+%5Cvec+y%2C%5Cvec+x%3B%5Cit+%5Ctheta%29\" alt=\"p_g(\\rm \\vec x;\\it \\theta)=\\sum_{\\rm \\vec y}\\it p_g(\\rm \\vec y,\\vec x;\\it \\theta)\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=p_g%28%5Crm+%5Cvec+y%7C%5Cvec+x%3B%5Cit+%5Ctheta%29%3Dp_g%28%5Crm+%5Cvec+y%2C%5Cvec+x%3B%5Cit+%5Ctheta%29%2Fp_g%28%5Crm+%5Cvec+x%3B%5Cit+%5Ctheta%29\" alt=\"p_g(\\rm \\vec y|\\vec x;\\it \\theta)=p_g(\\rm \\vec y,\\vec x;\\it \\theta)/p_g(\\rm \\vec x;\\it \\theta)\" eeimg=\"1\"/> 计算的。</p><p>接下来比较一下这个生成模型和一个在同一族联合概率分布上的判别模型。为了获取这个判别模型我们定义一个输入上的先验概率 <img src=\"https://www.zhihu.com/equation?tex=p%28%5Crm+%5Cvec+x%29\" alt=\"p(\\rm \\vec x)\" eeimg=\"1\"/> ，这样 <img src=\"https://www.zhihu.com/equation?tex=p%28%5Crm+%5Cvec+x%29\" alt=\"p(\\rm \\vec x)\" eeimg=\"1\"/> 就可以通过设置一些参数来用 <img src=\"https://www.zhihu.com/equation?tex=p_g\" alt=\"p_g\" eeimg=\"1\"/> 表示。即 <img src=\"https://www.zhihu.com/equation?tex=p%28%5Crm+%5Cvec+x%29%3D%5Cit+p_c%28%5Crm+%5Cvec+x%3B%5Cit+%5Ctheta%5E%7B%27%7D%29%3D%5Csum_yp_g%28%5Crm+%5Cvec+y%2C+%5Cvec+x%7C%5Cit+%5Ctheta%5E%7B%27%7D%29\" alt=\"p(\\rm \\vec x)=\\it p_c(\\rm \\vec x;\\it \\theta^{&#39;})=\\sum_yp_g(\\rm \\vec y, \\vec x|\\it \\theta^{&#39;})\" eeimg=\"1\"/> (注：这里的 <img src=\"https://www.zhihu.com/equation?tex=p%28%5Crm+%5Cvec+x%29\" alt=\"p(\\rm \\vec x)\" eeimg=\"1\"/> 式先验概率，而生成式中的 <img src=\"https://www.zhihu.com/equation?tex=p%28%5Crm+%5Cvec+x%29\" alt=\"p(\\rm \\vec x)\" eeimg=\"1\"/> 式概率估计，所以这里边的参数不同），这里的参数 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%27%7D\" alt=\"\\theta^{&#39;}\" eeimg=\"1\"/> 是为了和(1.9)中的 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 区分开来。我们把这个它的条件概率分布用 <img src=\"https://www.zhihu.com/equation?tex=p_g\" alt=\"p_g\" eeimg=\"1\"/> 表示出来就是 <img src=\"https://www.zhihu.com/equation?tex=p_c%28%5Crm+%5Cvec+y%7C%5Cvec+x%3B%5Ctheta%29%3D%5Cit+p_g%28%5Crm+%5Cvec+y%7C%5Cvec+x%3B%5Cit+%5Ctheta%29\" alt=\"p_c(\\rm \\vec y|\\vec x;\\theta)=\\it p_g(\\rm \\vec y|\\vec x;\\it \\theta)\" eeimg=\"1\"/> ,然后即可求出联合概率分布：</p><p><img src=\"https://www.zhihu.com/equation?tex=p_c%28%5Crm+%5Cvec+y%7C%5Cvec+x%29%3D%5Cit+p_c%28%5Crm+%5Cvec+x%3B%5Cit+%5Ctheta%5E%7B%27%7D%29%5Cit+p_c%28%5Crm+%5Cvec+y%7C%5Cvec+x%3B%5Cit+%5Ctheta%29+%5Ctag%7B1.10%7D\" alt=\"p_c(\\rm \\vec y|\\vec x)=\\it p_c(\\rm \\vec x;\\it \\theta^{&#39;})\\it p_c(\\rm \\vec y|\\vec x;\\it \\theta) \\tag{1.10}\" eeimg=\"1\"/> </p><p>对比式(1.9)和式(1.10)可以发现，在拟合数据的时候条件式的方法拥有更高的自由度，因为没有必须要求 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta+%3D+%5Ctheta%5E%7B%27%7D\" alt=\"\\theta = \\theta^{&#39;}\" eeimg=\"1\"/> 。直觉上，因为(1.10)中的参数 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 在输入分布和条件表达式中都有用到，所以一组好的参数应该在两者上都会表现得比较好，潜在的代价是我们用 <img src=\"https://www.zhihu.com/equation?tex=p%28%5Crm+%5Cvec+x%29\" alt=\"p(\\rm \\vec x)\" eeimg=\"1\"/> 的准确率换取了 <img src=\"https://www.zhihu.com/equation?tex=p%28%5Crm+%5Cvec+y%7C%5Cvec+x%29\" alt=\"p(\\rm \\vec y|\\vec x)\" eeimg=\"1\"/> 的准确率，因为我们并不关心 <img src=\"https://www.zhihu.com/equation?tex=p%28%5Crm+%5Cvec+x%29\" alt=\"p(\\rm \\vec x)\" eeimg=\"1\"/> 的准确率。另一方面，因为增加了自由度，所以也会提高过拟合的风险，并且对于在测试集中未出现的数据的生成会更差。</p><p>虽然目前为止我们一直在批评生成模型，但他们还是有一些优点。生成模型更擅长处理因变量，部分标注数据和未标注数据。最极端的情况下，当所有的数据都未标注的时候，生成式可以无监督的方式应用，而无监督学习在判别模型中不太适合，而且是目前研究的一个很活跃的方向。</p><p>其次，有些情况下生成式会比判别式表现得更好，直觉上讲是因为输入模型 <img src=\"https://www.zhihu.com/equation?tex=p%28%5Crm+%5Cvec+x%29\" alt=\"p(\\rm \\vec x)\" eeimg=\"1\"/> 可能对条件表达式有一些影响，吴恩达和乔丹认为这种影响在数据集小的时候尤为显著。对于任何数据集，不可能提前预知到底是判别模型好还是生成模型好。最后对于很自然的会使用到生成式的模型，或者需要不仅能预测输出特征还能预测输入特征的应用，优先使用生成模型。</p><p>因为生成式有这样的形式 <img src=\"https://www.zhihu.com/equation?tex=p%28%5Crm+%5Cvec+y%2C%5Cvec+x%29%3D%5Cit+p%28%5Crm+%5Cvec+y%29%5Cit+p%28%5Crm+%5Cvec+x%7C%5Cvec+y%29\" alt=\"p(\\rm \\vec y,\\vec x)=\\it p(\\rm \\vec y)\\it p(\\rm \\vec x|\\vec y)\" eeimg=\"1\"/> ，所以很自然的会用有向图去表示生成模型，因为从拓扑关系讲，有向图中， <img src=\"https://www.zhihu.com/equation?tex=%5Crm+%5Cvec+y\" alt=\"\\rm \\vec y\" eeimg=\"1\"/> 先于 <img src=\"https://www.zhihu.com/equation?tex=%5Cvec+x\" alt=\"\\vec x\" eeimg=\"1\"/> 。同样的我们也会很自然的用无向图去表示判别式。但是这不是定理，比如无向生成模型有马尔科夫随机场(Markov random field)，有向判别模型有最大熵模型(MEMM)。当 <img src=\"https://www.zhihu.com/equation?tex=%5Crm+%5Cvec+x\" alt=\"\\rm \\vec x\" eeimg=\"1\"/> 先于 <img src=\"https://www.zhihu.com/equation?tex=%5Crm+%5Cvec+y\" alt=\"\\rm \\vec y\" eeimg=\"1\"/> 的时候，用有向图描述判别模型也是很有帮助的。</p><p>隐马尔可夫模型和线性链条件随机场(linear-chain CRF)之间的关系跟朴素贝叶斯模型和逻辑回归之间的关系一样，都是生成-判别对，隐马尔可夫模型也有相应的判别模型，就是条件随机场的一种特殊情况，我们将会在下一节解释。朴素贝叶斯、逻辑回归、生成式和条件随机场之间的关系如下如所示。 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-0f5037559ae8e75f26b2f74fd091520e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1674\" data-rawheight=\"708\" class=\"origin_image zh-lightbox-thumb\" width=\"1674\" data-original=\"https://pic3.zhimg.com/v2-0f5037559ae8e75f26b2f74fd091520e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1674&#39; height=&#39;708&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1674\" data-rawheight=\"708\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1674\" data-original=\"https://pic3.zhimg.com/v2-0f5037559ae8e75f26b2f74fd091520e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-0f5037559ae8e75f26b2f74fd091520e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>1.3 线性链条件随机场</h2><p>在介绍线性链条件随机场之前，我们先考虑一下一个隐马尔可夫模型的联合概率分布 <img src=\"https://www.zhihu.com/equation?tex=p%28%5Crm+%5Cvec+y%2C%5Cvec+x%29\" alt=\"p(\\rm \\vec y,\\vec x)\" eeimg=\"1\"/> 的条件概率分布 <img src=\"https://www.zhihu.com/equation?tex=p%28%5Crm+%5Cvec+y%7C%5Cvec+x%29\" alt=\"p(\\rm \\vec y|\\vec x)\" eeimg=\"1\"/> 作为引子。因为这个条件分布是条件随机场的特征函数的一个特殊形式。</p><p>首先，我们用更符合生成式的形式重写一些隐马尔可夫模型的联合概率分布(1.4)：</p><p><img src=\"https://www.zhihu.com/equation?tex=+p%28%5Crm+%5Cvec+y%2C%5Cvec+x%29%3D%5Cit+%5Cfrac%7B1%7D%7BZ%7D%5Cexp+%7B%5Csum_%7Bi%2Cj%5Cin+S%7D%5Ctheta_%7Bi%2Cj%7D%5Crm+1_%7B%5C%7B%5Cit+%7By_t%3Di%5C%7D%7D%7D%5Crm+1_%7B%5C%7B%5Cit+%7By_%7Bt-1%7D%3Dj%5C%7D%7D%7D%7D+%5Crm%2B%5Cit+%5Csum_%7Bi%5Cin+S%7D%5Csum_%7B+o%5Cin+O%7D%5Cmu_%7Boi%7D%5Crm+1_%7B%7B%5C%7B%5Cit+y_t%3Di%5C%7D%7D%7D%5Crm+1_%7B%5C%7B%5Cit+x_t%3Do%5C%7D%7D+%5Ctag%7B1.11%7D\" alt=\" p(\\rm \\vec y,\\vec x)=\\it \\frac{1}{Z}\\exp {\\sum_{i,j\\in S}\\theta_{i,j}\\rm 1_{\\{\\it {y_t=i\\}}}\\rm 1_{\\{\\it {y_{t-1}=j\\}}}} \\rm+\\it \\sum_{i\\in S}\\sum_{ o\\in O}\\mu_{oi}\\rm 1_{{\\{\\it y_t=i\\}}}\\rm 1_{\\{\\it x_t=o\\}} \\tag{1.11}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%5Ctheta+%3D+%7B%5Ctheta_%7Bij%7D%2Cu_%7Boi%7D%7D\" alt=\"\\theta = {\\theta_{ij},u_{oi}}\" eeimg=\"1\"/> 是关于分布的实数值的参数， <img src=\"https://www.zhihu.com/equation?tex=Z\" alt=\"Z\" eeimg=\"1\"/> 是正则常数保证分布的和为1，如果我们不添加这个参数那么有些 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 值可能不会产生一个正确的关于 <img src=\"https://www.zhihu.com/equation?tex=%28%5Crm+%5Cvec+y%2C%5Cvec+x%29\" alt=\"(\\rm \\vec y,\\vec x)\" eeimg=\"1\"/> 的分布，比如把所有的参数都设为1.</p><p>现在，有趣的地方在于(1.11)描述的刚好和(1.4)式描述的隐马尔可夫模型相同。每一个齐次隐马尔可夫模型都能通过如下参数设置写成式(1.11)的形式：</p><p><img src=\"https://www.zhihu.com/equation?tex=+%5Ctheta_%7Bi%2Cj%7D%3D%5Clog+p%28y%5E%7B%27%7D%3Di%7Cy%3Dj%29\" alt=\" \\theta_{i,j}=\\log p(y^{&#39;}=i|y=j)\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cmu_%7Bo%2Ci%7D%3D%5Clog+p%28x%3D0%2Cy%3Di%29\" alt=\"\\mu_{o,i}=\\log p(x=0,y=i)\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=+Z%3D1\" alt=\" Z=1\" eeimg=\"1\"/> </p><p>反过来也成立，也就是说，每一个可以因式分解成(1.11)的概率分布都是隐马尔可夫模型。（通过前后向算法可以实现这一点，前后向算法后面会讲到），所以除了添加了参数更灵活，我们没有向这一组分布中添加任何其他的分布。</p><p>通过<b>特征函数</b>概念的引入，我们可以把(1.11)式写的更简洁。就像我们之前在逻辑回归上干的一样。每一个特征函数有这样的形式 <img src=\"https://www.zhihu.com/equation?tex=f_k%28y_t%2Cy_%7Bt-1%7D%2Cx_t%29\" alt=\"f_k(y_t,y_{t-1},x_t)\" eeimg=\"1\"/> ,为了和式(1.11)相同，需要为每一次转移 <img src=\"https://www.zhihu.com/equation?tex=%28i%2Cj%29\" alt=\"(i,j)\" eeimg=\"1\"/> 设置这样的特征函数 <img src=\"https://www.zhihu.com/equation?tex=f_%7Bij%7D%28y%2Cy%5E%7B%27%7D%2Cx%29%3D%5Cbold+1_%7B%5C%7B%5Cit+y%3Di%5C%7D%7D%5Cbold+1_%7B%5C%7B%5Cit+y%5E%7B%27%7D%3Dj%5C%7D%7D\" alt=\"f_{ij}(y,y^{&#39;},x)=\\bold 1_{\\{\\it y=i\\}}\\bold 1_{\\{\\it y^{&#39;}=j\\}}\" eeimg=\"1\"/> ，对于每一个状态-观察对 <img src=\"https://www.zhihu.com/equation?tex=%28i%2Co%29\" alt=\"(i,o)\" eeimg=\"1\"/> 设置 <img src=\"https://www.zhihu.com/equation?tex=f_%7Bio%7D%28y%2Cy%5E%7B%27%7D%2Cx%29%3D%5Cbold+1_%7B%5C%7By%3Di%5C%7D%7D+%5Cbold+1_%7B%5C%7Bi%3Do%5C%7D%7D\" alt=\"f_{io}(y,y^{&#39;},x)=\\bold 1_{\\{y=i\\}} \\bold 1_{\\{i=o\\}}\" eeimg=\"1\"/> ，我们用一个一般化的特征方程 <img src=\"https://www.zhihu.com/equation?tex=f_k\" alt=\"f_k\" eeimg=\"1\"/> 来表示所有的 <img src=\"https://www.zhihu.com/equation?tex=f_%7Bij%7D\" alt=\"f_{ij}\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=f_%7Bio%7D\" alt=\"f_{io}\" eeimg=\"1\"/> ，然后隐马尔可夫模型就可以写成这样：</p><p><img src=\"https://www.zhihu.com/equation?tex=+p%28%5Crm+%5Cvec+y%2C%5Cvec+x%29%3D%5Cit+%5Cfrac%7B1%7D%7BZ%7D%5Cprod_%7Bt%3D1%7D%5E%7BT%7D%5Cexp%7B%5Csum_%7Bk%3D1%7D%5E%7BK%7D%5Ctheta_kf_k%28y_t%2Ct_%7Bt-1%7D%2Cx_i%29%7D+%5Ctag%7B1.12%7D\" alt=\" p(\\rm \\vec y,\\vec x)=\\it \\frac{1}{Z}\\prod_{t=1}^{T}\\exp{\\sum_{k=1}^{K}\\theta_kf_k(y_t,t_{t-1},x_i)} \\tag{1.12}\" eeimg=\"1\"/> </p><p>重申一遍，式(1.12)定义的一族概率分布和式(1.11)的完全相同，所以和隐马尔可夫模型的原生公式(1.4)也相同。</p><p>最后，我们把(1.12)标识的隐马尔可夫模型的条件分布 <img src=\"https://www.zhihu.com/equation?tex=p%28%5Crm+%5Cvec+y%7C%5Cvec+x%29\" alt=\"p(\\rm \\vec y|\\vec x)\" eeimg=\"1\"/> 写出来，即：</p><p><img src=\"https://www.zhihu.com/equation?tex=p%28%5Crm+%5Cvec+y%7C%5Cvec+x%29%3D%5Cit+%5Cfrac%7Bp%28%5Crm+%5Cvec+y%2C%5Cvec+x%29%7D%7B%5Cit+%5Csum_%7By%5E%7B%27%7D%7Dp%28%5Crm+%5Cvec+y%5E%7B%27%7D%2C%5Cvec+x%29%7D%3D%5Cit+%5Cfrac%7B%5Cprod%5E%7BT%7D%7Bt%3D1%7D%5Cexp%7B%7B%5Csum%7Bk%3D1%7D%5EK%5Ctheta_kf_k%28y_t%2Cy_%7Bt-1%7D%2Cx_t%29%7D%7D%7D%7B%5Csum_%7By%5E%7B%27%7D%7D%5Cprod_%7Bt%3D1%7D%5ET%5Cexp+%7B%7B%5Csum_%7Bk%3D1%7D%5EK%5Ctheta_kf_k%28y%5E%7B%27%7Dt%2Cy%5E%7B%27%7D%7Bt-1%7D%2Cx_t%29%7D%7D%7D+%5Ctag%7B1.13%7D\" alt=\"p(\\rm \\vec y|\\vec x)=\\it \\frac{p(\\rm \\vec y,\\vec x)}{\\it \\sum_{y^{&#39;}}p(\\rm \\vec y^{&#39;},\\vec x)}=\\it \\frac{\\prod^{T}{t=1}\\exp{{\\sum{k=1}^K\\theta_kf_k(y_t,y_{t-1},x_t)}}}{\\sum_{y^{&#39;}}\\prod_{t=1}^T\\exp {{\\sum_{k=1}^K\\theta_kf_k(y^{&#39;}t,y^{&#39;}{t-1},x_t)}}} \\tag{1.13}\" eeimg=\"1\"/> </p><p> 式(1.13)表示的条件分布式线性链条件随机场的一种特殊形式，特殊的地方在于，这种线性链条件随机场仅仅包含了当前词的特征，但是其他的线性链条件随机场有关于输入的更丰富的特征，比如前缀后缀，前后词等等，幸运的式这些拓展只需要在当前的形式上做一点微笑的改变即可，我们让特征方程更有普适性，这样就引出了线性链条件随机场的定义：</p><hr/><p><b>定义1.2.</b>  <img src=\"https://www.zhihu.com/equation?tex=Y%2CX\" alt=\"Y,X\" eeimg=\"1\"/> 式随机变量， <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta%3D%7B%5Ctheta_k%7D+%5Cin+%5Cmathbb%7BR%7D%5EK\" alt=\"\\theta={\\theta_k} \\in \\mathbb{R}^K\" eeimg=\"1\"/> 是参数向量， <img src=\"https://www.zhihu.com/equation?tex=%5Cmathcal%7BF%7D%3D%5C%7Bf_k%28y%2Cy%5E%7B%27%7D%2C%5Crm+%5Cvec+x%29%5C%7D_%7B%5Cit+k%3D1%7D%5EK\" alt=\"\\mathcal{F}=\\{f_k(y,y^{&#39;},\\rm \\vec x)\\}_{\\it k=1}^K\" eeimg=\"1\"/> <i>是一个实数值的特征方程集合。线性链条件随机场是一个有如下形式的概率分布</i> <img src=\"https://www.zhihu.com/equation?tex=p%28%5Crm+%5Cvec+y%7C%5Cvec+x%29\" alt=\"p(\\rm \\vec y|\\vec x)\" eeimg=\"1\"/> <i>： </i><img src=\"https://www.zhihu.com/equation?tex=p%28%5Crm+%5Cvec+y%7C%5Cvec+x%29%3D%5Cfrac%7B1%7D%7B%5Cit+Z%7D+%28%5Crm+%5Cvec+x%29%5Cprod%7B%5Cit+t%3D1%7D%5ET%5Cexp%7B%7B%5Csum_%7Bk%3D1%7D%5EK%5Ctheta_kf_k%28y_t%2Cy_t-1%2C%5Crm+%5Cvec+X_t%29%7D%7D+%5Ctag%7B1.14%7D\" alt=\"p(\\rm \\vec y|\\vec x)=\\frac{1}{\\it Z} (\\rm \\vec x)\\prod{\\it t=1}^T\\exp{{\\sum_{k=1}^K\\theta_kf_k(y_t,y_t-1,\\rm \\vec X_t)}} \\tag{1.14}\" eeimg=\"1\"/> </p><p>其中 <img src=\"https://www.zhihu.com/equation?tex=Z%28%5Crm+%5Cvec+x%29\" alt=\"Z(\\rm \\vec x)\" eeimg=\"1\"/> 是一个输入相互依赖的正则函数</p><p><img src=\"https://www.zhihu.com/equation?tex=+Z%28%5Crm+%5Cvec+x%29%3D%5Cit+%5Csum_y%5Cprod_%7Bt%3D1%7D%5ET%5Cexp%7B%7B%5Csum_%7Bk%3D1%7D%5EK%5Ctheta_kf_k%28y_t%2Cy_%7Bt-1%7D%2C%5Crm+%5Cvec+x_%7B%5Cit+t%7D%29%7D%7D+%5Ctag%7B1.15%7D\" alt=\" Z(\\rm \\vec x)=\\it \\sum_y\\prod_{t=1}^T\\exp{{\\sum_{k=1}^K\\theta_kf_k(y_t,y_{t-1},\\rm \\vec x_{\\it t})}} \\tag{1.15}\" eeimg=\"1\"/> </p><hr/><p>线性链条件随机场可以描述为关于 <img src=\"https://www.zhihu.com/equation?tex=%5Crm+%5Cvec+x\" alt=\"\\rm \\vec x\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5Crm+%5Cvec+y\" alt=\"\\rm \\vec y\" eeimg=\"1\"/> 的因子图，即：</p><p><img src=\"https://www.zhihu.com/equation?tex=+p%28%5Crm+%5Cvec+y%7C+%5Cvec+x%29%3D%5Cit+%5Cfrac%7B1%7D%7BZ%7D%7B%28%5Crm+%5Cvec+x%29%5Cit+%5Cprod_%7Bt%3D1%7D%5ET+%5CPsi_t%28y_t%2Cy_%7Bt-1%7D%2C%5Crm+%5Cvec+x%29%7D+%5Ctag%7B1.16%7D\" alt=\" p(\\rm \\vec y| \\vec x)=\\it \\frac{1}{Z}{(\\rm \\vec x)\\it \\prod_{t=1}^T \\Psi_t(y_t,y_{t-1},\\rm \\vec x)} \\tag{1.16}\" eeimg=\"1\"/> </p><p> 局部函数（local function） <img src=\"https://www.zhihu.com/equation?tex=%5CPsi+%28t%29\" alt=\"\\Psi (t)\" eeimg=\"1\"/> 有如下的线性链形式：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5CPsi+%28y_t%2Cy_%7Bt-1%7D%2C%5Crm+%5Cvec+x%29%3D%5Cexp%7B%7B%5Csum_%7Bk%3D1%7D%5EK%5Ctheta_kf_k%28y_t%2Cy_%7Bt-1%7D%2C%5Crm+%5Cvec+x%29%7D%7D+%5Ctag%7B1.17%7D\" alt=\"\\Psi (y_t,y_{t-1},\\rm \\vec x)=\\exp{{\\sum_{k=1}^K\\theta_kf_k(y_t,y_{t-1},\\rm \\vec x)}} \\tag{1.17}\" eeimg=\"1\"/> </p><p> 在后面介绍到通用型的条件随机场的时候这个式子会很有用。参数向量 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 的学习将会在后面讲到。</p><p>前面我们已经看到如果联合概率分布 <img src=\"https://www.zhihu.com/equation?tex=p%28%5Crm+%5Cvec+y%2C%5Cvec+x%29\" alt=\"p(\\rm \\vec y,\\vec x)\" eeimg=\"1\"/> 因式分解为一个隐马尔可夫模型，那么相关联的条件分布 <img src=\"https://www.zhihu.com/equation?tex=p%28%5Crm+%5Cvec+y%7C%5Cvec+x%29\" alt=\"p(\\rm \\vec y|\\vec x)\" eeimg=\"1\"/> 就是一个线性链条件随机场如下图所示。（译者注：这种对应的词标注模型应该是char级别的词标注） </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3b52d7b90008f50b2095d0c19a251d85_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1073\" data-rawheight=\"243\" class=\"origin_image zh-lightbox-thumb\" width=\"1073\" data-original=\"https://pic2.zhimg.com/v2-3b52d7b90008f50b2095d0c19a251d85_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1073&#39; height=&#39;243&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1073\" data-rawheight=\"243\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1073\" data-original=\"https://pic2.zhimg.com/v2-3b52d7b90008f50b2095d0c19a251d85_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-3b52d7b90008f50b2095d0c19a251d85_b.jpg\"/></figure><p> 另一种线性链条件随机场同样很有用，比如，在隐马尔可夫，模型中，对于所有的输入，状态 <img src=\"https://www.zhihu.com/equation?tex=i\" alt=\"i\" eeimg=\"1\"/> 到 <img src=\"https://www.zhihu.com/equation?tex=j\" alt=\"j\" eeimg=\"1\"/> 之间的转移的分数 <img src=\"https://www.zhihu.com/equation?tex=%5Clog%28p%28y_t%3Dj%7Cy_%7Bt-1%7D%3Di%29%29\" alt=\"\\log(p(y_t=j|y_{t-1}=i))\" eeimg=\"1\"/> 是相同的，在条件随机场中，我们允许转移 <img src=\"https://www.zhihu.com/equation?tex=%28i%2Cj%29\" alt=\"(i,j)\" eeimg=\"1\"/> 的分数依赖于当前的观察向量，只需要添加一个特征 <img src=\"https://www.zhihu.com/equation?tex=%5Cbold+1_%7B%5C%7B%5Cit+y_t%3Dj%5C%7D%7D+%5Cbold+1_%7B%5C%7B%5Cit+y_%7Bt-1%7D%3D1%5C%7D%7D%5Cbold+1_%7B%5C%7Bx_t%3Do%5C%7D%7D\" alt=\"\\bold 1_{\\{\\it y_t=j\\}} \\bold 1_{\\{\\it y_{t-1}=1\\}}\\bold 1_{\\{x_t=o\\}}\" eeimg=\"1\"/> ，拥有这种特征的条件随机场广泛的应用于文本类的应用，如下图所示 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-d64ceb9ef56d1a7bee81edd15b17f737_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1127\" data-rawheight=\"275\" class=\"origin_image zh-lightbox-thumb\" width=\"1127\" data-original=\"https://pic4.zhimg.com/v2-d64ceb9ef56d1a7bee81edd15b17f737_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1127&#39; height=&#39;275&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1127\" data-rawheight=\"275\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1127\" data-original=\"https://pic4.zhimg.com/v2-d64ceb9ef56d1a7bee81edd15b17f737_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-d64ceb9ef56d1a7bee81edd15b17f737_b.jpg\"/></figure><p> 实际上，因为条件随机场不能表示变量 <img src=\"https://www.zhihu.com/equation?tex=%5Crm+%5Cvec+x_1%2C...%2C%5Cvec+x_T\" alt=\"\\rm \\vec x_1,...,\\vec x_T\" eeimg=\"1\"/> 之间的依赖关系（上述两种条件随机场一种不包含变量之间的，另一种只包含了和前一个变量相关的依赖关系），我们可以让因子 <img src=\"https://www.zhihu.com/equation?tex=%5CPsi_t\" alt=\"\\Psi_t\" eeimg=\"1\"/> 依赖于整个观察向量 <img src=\"https://www.zhihu.com/equation?tex=%5Crm+%5Cvec+x\" alt=\"\\rm \\vec x\" eeimg=\"1\"/> 并且不破坏线型图的结构——这就允许我们把 <img src=\"https://www.zhihu.com/equation?tex=%5Crm+%5Cvec+x\" alt=\"\\rm \\vec x\" eeimg=\"1\"/> 视为一个整体作为一个变量。这样，特征函数就可写为 <img src=\"https://www.zhihu.com/equation?tex=f_k%28y_t%2Cy_%7Bt-1%7D%2C%5Crm+%5Cvec+x%29\" alt=\"f_k(y_t,y_{t-1},\\rm \\vec x)\" eeimg=\"1\"/> 并且可以对所有的输入变量 <img src=\"https://www.zhihu.com/equation?tex=%5Crm+%5Cvec+x\" alt=\"\\rm \\vec x\" eeimg=\"1\"/> 一起检验。这种方式可以广泛应用到条件随机场中而不是仅仅针对于线性链，一个拥有这种结构的线性链条件随机场如下图所示。 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-bc7e4532dc90eb0607f3ad02efd6d6d2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1119\" data-rawheight=\"284\" class=\"origin_image zh-lightbox-thumb\" width=\"1119\" data-original=\"https://pic3.zhimg.com/v2-bc7e4532dc90eb0607f3ad02efd6d6d2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1119&#39; height=&#39;284&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1119\" data-rawheight=\"284\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1119\" data-original=\"https://pic3.zhimg.com/v2-bc7e4532dc90eb0607f3ad02efd6d6d2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-bc7e4532dc90eb0607f3ad02efd6d6d2_b.jpg\"/></figure><p> 在这张图中，我们把 <img src=\"https://www.zhihu.com/equation?tex=%5Crm+%5Cvec+x%3D%28%5Cvec+x_1%2C...%2C+%5Cvec+x_T%29\" alt=\"\\rm \\vec x=(\\vec x_1,..., \\vec x_T)\" eeimg=\"1\"/> 作为一个大的观察节点，所有的因素都是相互关联的，而不是让每一个 <img src=\"https://www.zhihu.com/equation?tex=%5Crm+%5Cvec+x_1%2C...%5Cvec+x_T\" alt=\"\\rm \\vec x_1,...\\vec x_T\" eeimg=\"1\"/> 作为单独的节点。</p><p>为了说明线性链条件随机场的定义中每一个特征方程都可以依赖于任意时间步的观察，我们把观察 <img src=\"https://www.zhihu.com/equation?tex=%5Crm+%5Cvec+x\" alt=\"\\rm \\vec x\" eeimg=\"1\"/> 的的参数写成了 <img src=\"https://www.zhihu.com/equation?tex=f_k\" alt=\"f_k\" eeimg=\"1\"/> ，这意味着它包含了需要在计算时间 <img src=\"https://www.zhihu.com/equation?tex=t\" alt=\"t\" eeimg=\"1\"/> 的特征时关于全部的观察 <img src=\"https://www.zhihu.com/equation?tex=%5Crm+%5Cvec+x\" alt=\"\\rm \\vec x\" eeimg=\"1\"/> 的所有的成分（译者注：这个从句很长，比较难翻译，我的理解就是使用 <img src=\"https://www.zhihu.com/equation?tex=f_k\" alt=\"f_k\" eeimg=\"1\"/> 之后，观察序列 <img src=\"https://www.zhihu.com/equation?tex=%5Crm+%5Cvec+x\" alt=\"\\rm \\vec x\" eeimg=\"1\"/> 的所有的和计算时间 <img src=\"https://www.zhihu.com/equation?tex=t\" alt=\"t\" eeimg=\"1\"/> 的特征相关的因素都可以被包含进去，主要还是阐述了上一句的内容）。比如，如果条件随机场利用下一个词 <img src=\"https://www.zhihu.com/equation?tex=x_%7Bt%2B1%7D\" alt=\"x_{t+1}\" eeimg=\"1\"/> 作为特征，那么特征向量 <img src=\"https://www.zhihu.com/equation?tex=%5Crm+%5Cvec+x_t\" alt=\"\\rm \\vec x_t\" eeimg=\"1\"/> 被认为是包含了下一个词 <img src=\"https://www.zhihu.com/equation?tex=x_%7Bt%2B1%7D\" alt=\"x_{t+1}\" eeimg=\"1\"/> .</p><p>最后，注意到正则常数项 <img src=\"https://www.zhihu.com/equation?tex=Z%28%5Crm+%5Cvec+x%29\" alt=\"Z(\\rm \\vec x)\" eeimg=\"1\"/> 对所有的可能的状态序列（是状态项数的指数倍）做了求和。不过这可以通过后面将会介绍的前后向算法高效的计算。</p><h2>2.4 通用条件随机场</h2><p>现在我们来通过前面讨论过的线性链来生成一个普适性的图模型，和下面给出的条件随机场的定义相匹配。</p><hr/><p><b>定义1.3：</b> <img src=\"https://www.zhihu.com/equation?tex=G\" alt=\"G\" eeimg=\"1\"/> 是一个关于 <img src=\"https://www.zhihu.com/equation?tex=X\" alt=\"X\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=Y\" alt=\"Y\" eeimg=\"1\"/> 的因子图，如果对 <img src=\"https://www.zhihu.com/equation?tex=X\" alt=\"X\" eeimg=\"1\"/> 的任意值 <img src=\"https://www.zhihu.com/equation?tex=%5Crm+%5Cvec+x\" alt=\"\\rm \\vec x\" eeimg=\"1\"/> ，分布 <img src=\"https://www.zhihu.com/equation?tex=p%28%5Crm+%5Cvec+y%7C%5Cvec+x%29\" alt=\"p(\\rm \\vec y|\\vec x)\" eeimg=\"1\"/> 可以因式分解为 <img src=\"https://www.zhihu.com/equation?tex=G\" alt=\"G\" eeimg=\"1\"/> ，那么 <img src=\"https://www.zhihu.com/equation?tex=%28X%2CY%29\" alt=\"(X,Y)\" eeimg=\"1\"/> 就是一个条件随机场。</p><hr/><p>因此，每一个条件分布 <img src=\"https://www.zhihu.com/equation?tex=p%28%5Crm+%5Cvec+y%7C%5Cvec+x%29\" alt=\"p(\\rm \\vec y|\\vec x)\" eeimg=\"1\"/> 都是一个关于一些因子图的条件随机场。如果 <img src=\"https://www.zhihu.com/equation?tex=F%3D%7B%5CPsi_a%7D\" alt=\"F={\\Psi_a}\" eeimg=\"1\"/> 是 <img src=\"https://www.zhihu.com/equation?tex=G\" alt=\"G\" eeimg=\"1\"/> 中的一组因子，那么条件随机场的条件分布就是：</p><p><img src=\"https://www.zhihu.com/equation?tex=+p%28%5Crm+%5Cvec+y%7C%5Cvec+x%29%3D%5Cfrac%7B1%7D%7BZ%28%5Crm+%5Cvec+x%29%7D%5Cprod_%7B%5Cit+a%3D1%7D%5E%7B%5Cit+A%7D%5CPsi_%7B%5Cit+a%7D%28%5Cvec+y_%7B%5Cit+a%7D%2C%5Cvec+x_%7B%5Cit+a%7D+%5Ctag%7B1.18%7D%29\" alt=\" p(\\rm \\vec y|\\vec x)=\\frac{1}{Z(\\rm \\vec x)}\\prod_{\\it a=1}^{\\it A}\\Psi_{\\it a}(\\vec y_{\\it a},\\vec x_{\\it a} \\tag{1.18})\" eeimg=\"1\"/> </p><p> 这个式子和定义(1.1)中的无向图模型之间的区别是现在的正则项常数 <img src=\"https://www.zhihu.com/equation?tex=Z%28%5Crm+%5Cvec+x%29\" alt=\"Z(\\rm \\vec x)\" eeimg=\"1\"/> 是关于输入 <img src=\"https://www.zhihu.com/equation?tex=%5Crm+%5Cvec+x\" alt=\"\\rm \\vec x\" eeimg=\"1\"/> 的函数。因为条件会简化图模型，所以之前可能不可计算的 <img src=\"https://www.zhihu.com/equation?tex=Z%28%5Crm+%5Cvec+x%29\" alt=\"Z(\\rm \\vec x)\" eeimg=\"1\"/> 可能会变得可计算。</p><p>就像我们在隐马尔可夫模型和线性链条件随机场中做的一样，让势函数的对数 <img src=\"https://www.zhihu.com/equation?tex=%5Clog+%5CPsi_%7B%28a%29%7D\" alt=\"\\log \\Psi_{(a)}\" eeimg=\"1\"/> 是关于事先预定好的一组特征方程的线性函数很有用，即：</p><p><img src=\"https://www.zhihu.com/equation?tex=+%5CPsi_a%28%5Crm+%5Cvec+y%2C%5Cvec+x%29%3D%5Cexp%7B%7B%5Csum_%7Bk%3D1%7D%5E%7BK%28A%29%7D%5Ctheta_%7Bak%7Df_%7Bak%7D%28%5Crm+%5Cvec+y_%7B%5Cit+a%7D%2C%5Cvec+x_%7B%5Cit+a%7D%7D%7D+%5Ctag%7B1.19%7D\" alt=\" \\Psi_a(\\rm \\vec y,\\vec x)=\\exp{{\\sum_{k=1}^{K(A)}\\theta_{ak}f_{ak}(\\rm \\vec y_{\\it a},\\vec x_{\\it a}}} \\tag{1.19}\" eeimg=\"1\"/> </p><p>特征函数 <img src=\"https://www.zhihu.com/equation?tex=f_%7Bak%7D\" alt=\"f_{ak}\" eeimg=\"1\"/> 和权重 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_%7Bak%7D\" alt=\"\\theta_{ak}\" eeimg=\"1\"/> 使用因子的索引 <img src=\"https://www.zhihu.com/equation?tex=a\" alt=\"a\" eeimg=\"1\"/> 来索引是为了强调每一个因子都有自己的一组权重。一般来说，每一个因子也都允许有不同的特征函数。注意到如果 <img src=\"https://www.zhihu.com/equation?tex=%5Crm+%5Cvec+x\" alt=\"\\rm \\vec x\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5Crm+%5Cvec+y\" alt=\"\\rm \\vec y\" eeimg=\"1\"/> 是离散的话，那么对数-线性假设就不是一个额外的约束了，因为我们可以为每一个被 <img src=\"https://www.zhihu.com/equation?tex=%28%5Crm+%5Cvec+y_%7B%5Cit+a%7D%2C%5Cvec+x_%7B%5Cit+a%7D%29\" alt=\"(\\rm \\vec y_{\\it a},\\vec x_{\\it a})\" eeimg=\"1\"/> 选择 <img src=\"https://www.zhihu.com/equation?tex=f_%7Bak%7D\" alt=\"f_{ak}\" eeimg=\"1\"/> 作为指示函数，和我们在把隐马尔可夫模型转化为线性链条件随机场时所做的一样。</p><p>把式(1.19)和式(1.18)结合在一起，那么条件随机场的条件分布的对数-线性因子可以写成： </p><p><img src=\"https://www.zhihu.com/equation?tex=p%28%5Crm+%5Cvec+y%7C%5Cvec+x%29%3D%5Cfrac%7B1%7D%7B%5Cit+Z%28%5Crm+%5Cvec+x%29%7D%5Cprod_%7B%5Cit+%5CPsi_%7BA%7D+%5Cin+F%7D%5Cexp%7B%7B%5Cit+%5Csum_%7Bk%3D1%7D%5E%7BK%28A%29%7D%5Ctheta_%7Bak%7Df_%7Bak%7D%28%5Crm+%5Cvec+y_%7B%5Cit+a%7D%2C%5Cvec+x_%7B%5Cit+a%7D%29%7D%7D+%5Ctag%7B1.20%7D\" alt=\"p(\\rm \\vec y|\\vec x)=\\frac{1}{\\it Z(\\rm \\vec x)}\\prod_{\\it \\Psi_{A} \\in F}\\exp{{\\it \\sum_{k=1}^{K(A)}\\theta_{ak}f_{ak}(\\rm \\vec y_{\\it a},\\vec x_{\\it a})}} \\tag{1.20}\" eeimg=\"1\"/> </p><p> 除此之外，大部分应用中的模型都需要大量的参数绑定。比如在线性链的情况下，在每一个时间步都会对因子 <img src=\"https://www.zhihu.com/equation?tex=%5CPsi_t%28y_t%2Cy_%7Bt-1%7D%2C%5Crm+%5Cvec+x%29\" alt=\"\\Psi_t(y_t,y_{t-1},\\rm \\vec x)\" eeimg=\"1\"/> 使用同样的权重。为了说明这一点，我们会把图 <img src=\"https://www.zhihu.com/equation?tex=G\" alt=\"G\" eeimg=\"1\"/> 的分割成 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathcal+C%3D%7BC_1%2CC_2%2C...%2CC_p%7D\" alt=\"\\mathcal C={C_1,C_2,...,C_p}\" eeimg=\"1\"/> ，每一个 <img src=\"https://www.zhihu.com/equation?tex=C_p\" alt=\"C_p\" eeimg=\"1\"/> 是一个最大团，一个最大团是一组共享特征方程 <img src=\"https://www.zhihu.com/equation?tex=%7Bf_%7Bpk%7D%28%5Crm+%5Cvec+x_%7B%5Cit+c%7D%2C+%5Cvec+y_%7B%5Cit+c%7D%29%7D_%7B%5Cit+k%3D1%7D%5E%7B%5Cit+K%28p%29%7D\" alt=\"{f_{pk}(\\rm \\vec x_{\\it c}, \\vec y_{\\it c})}_{\\it k=1}^{\\it K(p)}\" eeimg=\"1\"/> <i>和对应的参数集合</i> <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_p+%5Cin+%5Cmathbb%7BR%7D%5E%7BK%28p%29%7D\" alt=\"\\theta_p \\in \\mathbb{R}^{K(p)}\" eeimg=\"1\"/> <i>的因子的集合。一个使用最大团的条件随机场可以写为：</i></p><p><img src=\"https://www.zhihu.com/equation?tex=p%28%5Crm+%5Cvec+y%7C%5Cvec+x%29%3D%5Cfrac%7B1%7D%7B%5Cit+Z%28%5Crm+%5Cvec+x%29%7D%5Cprod_%7B%5Cit+C_p+%5Cin+%5Cmathcal+c%7D%5Cprod_%7B%5Cit+%5CPsi_p+%5Cin+%5Cmathcal+c%7D%5Cit+%5CPsi_c%28%5Crm+%5Cvec+x_%7B%5Cmathcal+c%7D%2C%5Cvec+y_%7B%5Cmathcal+c%7D%3B%5Cit+%5Ctheta_p%29+%5Ctag%7B1.21%7D\" alt=\"p(\\rm \\vec y|\\vec x)=\\frac{1}{\\it Z(\\rm \\vec x)}\\prod_{\\it C_p \\in \\mathcal c}\\prod_{\\it \\Psi_p \\in \\mathcal c}\\it \\Psi_c(\\rm \\vec x_{\\mathcal c},\\vec y_{\\mathcal c};\\it \\theta_p) \\tag{1.21}\" eeimg=\"1\"/> </p><p> 每一个因子团被参数化为：</p><p><img src=\"https://www.zhihu.com/equation?tex=+%5CPsi_%7B%5Cmathcal+c%7D%28%5Crm+%5Cvec+x_%7B%5Cmathcal+c%7D%2C+%5Cvec+y_%7B%5Cmathcal+c%7D%3B%5Cit+%5Ctheta_p%29%3D%5Cexp%7B%5Csum_%7Bk%3D1%7D%5E%7BK%28p%29%7D%5Ctheta_%7Bpk%7Df_%7Bpk%7D%28%5Crm+%5Cvec+x_%7B%5Cmathcal+c%7D%2C%5Cvec+y_%7B%5Cmathcal+c%7D%29%7D+%5Ctag%7B1.22%7D\" alt=\" \\Psi_{\\mathcal c}(\\rm \\vec x_{\\mathcal c}, \\vec y_{\\mathcal c};\\it \\theta_p)=\\exp{\\sum_{k=1}^{K(p)}\\theta_{pk}f_{pk}(\\rm \\vec x_{\\mathcal c},\\vec y_{\\mathcal c})} \\tag{1.22}\" eeimg=\"1\"/> </p><p>正则函数： </p><p><img src=\"https://www.zhihu.com/equation?tex=Z%28%5Crm+%5Cvec+x%29%3D%5Csum_%7B%5Crm+%5Cvec+y%7D%5Cprod_%7BC_p+%5Cin+%5Cmathcal+c%7D%5Cprod_%7B%5Cit+%5CPsi_p+%5Cin+%5Cmathcal+c%7D%5Cit+%5CPsi_%7B%5Cmathcal+c%7D%28%5Crm+%5Cvec+x_%7B%5Cmathcal+c%7D%2C+%5Cvec+y_%7B%5Cmathcal+c%7D%3B%5Cit+%5Ctheta_p%29+%5Ctag%7B1.23%7D\" alt=\"Z(\\rm \\vec x)=\\sum_{\\rm \\vec y}\\prod_{C_p \\in \\mathcal c}\\prod_{\\it \\Psi_p \\in \\mathcal c}\\it \\Psi_{\\mathcal c}(\\rm \\vec x_{\\mathcal c}, \\vec y_{\\mathcal c};\\it \\theta_p) \\tag{1.23}\" eeimg=\"1\"/> </p><p> 这种最大团的表示方法在一个模型中会有相同的结构和参数。比如，在线性链条件随机场中，往往一个最大团 <img src=\"https://www.zhihu.com/equation?tex=C_0%3D%7B%5CPsi_t%28y_t%2Cy_%7Bt-1%7D%2C%5Crm+%5Cvec+x_%7B%5Cit+t%7D%29%5ET_%7B%5Cit+t%3D1%7D%7D\" alt=\"C_0={\\Psi_t(y_t,y_{t-1},\\rm \\vec x_{\\it t})^T_{\\it t=1}}\" eeimg=\"1\"/> 会用于一整个网络，所以 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathcal+C%3D%7BC_0%7D\" alt=\"\\mathcal C={C_0}\" eeimg=\"1\"/> 是一个单元素集合。如果我们想让每一个因子 <img src=\"https://www.zhihu.com/equation?tex=%5CPsi_t\" alt=\"\\Psi_t\" eeimg=\"1\"/> 在每一个时间 <img src=\"https://www.zhihu.com/equation?tex=t\" alt=\"t\" eeimg=\"1\"/> 都有不同的一组参数，例如非齐次隐马尔可夫模型，这可以通过指定 <img src=\"https://www.zhihu.com/equation?tex=T\" alt=\"T\" eeimg=\"1\"/> 个最大团， <img src=\"https://www.zhihu.com/equation?tex=%5Cmathcal+C%3D%7BC_t%7D%5ET_%7Bt%3D1%7D%EF%BC%8CC_t%3D%7B%5CPsi_t%28y_t%2Cy_%7Bt-1%7D%2C%5Crm+%5Cvec+x_%7B%5Cit+t%7D%29%7D\" alt=\"\\mathcal C={C_t}^T_{t=1}，C_t={\\Psi_t(y_t,y_{t-1},\\rm \\vec x_{\\it t})}\" eeimg=\"1\"/> 来实现。</p><p>（译者注，这一段包含大量术语，翻译的准确度应该比较低，想要更进一步了解的话可以直接搜括号中的英文）在定义条件随机场的时候最重要的地方在于指定相同的结构和参数。很多形式推荐指定最大团，我们将简短的提一下。比如<b>动态条件随机场</b>(dynamic conditional random field)是一个允许在每一个时间步有多个标签的模型，还有<b>关系马尔可夫模型</b>(renational Markov network)是一种图结构和参数绑定依赖于一个SQL-like语法的通用条件随机场。<b>马尔可夫逻辑网络</b>(Markov logic networks)使用了逻辑的形式去指定无向图中势函数的作用范围。本质上，对于知识库中的每一个一阶规则（first-order rule）都对应一组参数。一个MLN（马尔可夫逻辑网络）本质上可以被视为一个无向图中为指定相同的结构和参数绑定的编程约定。<b>命令式因子图</b>（impreatively defined factor graphs）使用了图灵完备(Turing-complete)方程的全部的表现取定义最大团，同时指定了模型的结构和足量统计的 <img src=\"https://www.zhihu.com/equation?tex=f_%7Bpk%7D\" alt=\"f_{pk}\" eeimg=\"1\"/> 。这些函数具有使用高级编程思想的灵活性，包括递归，任意搜索，惰性求值和记忆表。</p><p>原文比较长总共一百多页，这里把所有相关的理论都摘下来了，还有相关的特征工程和应用的例子可以自型参阅原论文，因为涉及到很多专业术语我翻译的可能也不是很准确很影响阅读，但是术语虽多，理解起来会比纯理论相对容易一点。还有相关的参数估计的算法限于篇幅放在下一节。</p><p>放一个开源的基于LSTM和CRF的词标注模型，这个模型在未分词的情况下处理阿里天池大赛的数据可以轻松达到70%的F1值，第一名是76%的样子，做下分词达到75%应该是没有问题的。</p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/guillaumegenthial/tf_ner\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-40e1db0ae540488cfb86f5a8a274eb70_ipico.jpg\" data-image-width=\"257\" data-image-height=\"257\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">guillaumegenthial/tf_ner</a><p></p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "自然语言处理", 
                    "tagLink": "https://api.zhihu.com/topics/19560026"
                }, 
                {
                    "tag": "条件随机场", 
                    "tagLink": "https://api.zhihu.com/topics/20682862"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/47084390", 
            "userName": "右左瓜子", 
            "userLink": "https://www.zhihu.com/people/6107fd0900473aae4e5ec6824b4fde4a", 
            "upvote": 4, 
            "title": "英文光学字符识别（OCR）", 
            "content": "<p>一开始是为了中文识别的需求来看的论文，但是看完发现在中文识别领域并不适用，所以做中文光学字符识别的同学可以直接走了，而且后面会提到因为诸多事务，没有完成代码编写工作，主要进来看代码的也可以走了，这篇文章封面有图之日，就是代码完成之时。</p><p>这一篇文章拖了很久了，因为最近事情又多了起来，另一方面不像前几篇，这篇的代码我之前没有实现过，所以比较费力一点。废话不多说了，先把论文放上来。</p><a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1707.03985\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">[1707.03985] Towards End-to-end Text Spotting with Convolutional Recurrent Neural Networks</a><p>在介绍这篇论文之前我们回忆一下之前的几篇文章谈到的目标检测与识别，以及文本分类。图像文字最直观的理解就是目标检测，但是不同于图片类型的目标检测，首先文字特别是汉字，字数繁多，普通的目标检测任务可能只需要识别特定的目标，但是文字识别就一定要识别全部常用文字，其次文字的组合大多数情况下是有规律可循的，猫可以呆在任何物体的边上，但是“猫”和“杯”在两个很常用的字却基本上不会在一起。所以文字种类繁多给识别带来难点，但是我们想能不能通过学习到文字之间的文法信息来降低识别难度。</p><p>带着上面的问题来看看这篇文章是怎么做的。这篇文章文字块检测的部分基本上使用了Faster RCNN的方法，只不过做了微小的修改，关键的部分在于对原本目标分类的部分做了大改，使用了两个LSTM网络。称为RFE和TRN，这是阅读的时候比较困难的部分。先把整个流程图贴上来</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-bc508eff447c6214a4667cb235bd793c_b.jpg\" data-size=\"normal\" data-rawwidth=\"1396\" data-rawheight=\"431\" class=\"origin_image zh-lightbox-thumb\" width=\"1396\" data-original=\"https://pic1.zhimg.com/v2-bc508eff447c6214a4667cb235bd793c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1396&#39; height=&#39;431&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1396\" data-rawheight=\"431\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1396\" data-original=\"https://pic1.zhimg.com/v2-bc508eff447c6214a4667cb235bd793c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-bc508eff447c6214a4667cb235bd793c_b.jpg\"/><figcaption>图片来自论文</figcaption></figure><p>前面的文字区域探测部分就不说了，原理同Faster RCNN，针对于文字大部分都是长宽比比较大的特性把anchor box的大小做了调整，接下来先讲一讲RFE</p><ul><li>Region Feature Encoder（RFE）</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-0bcb750ba45c1b43387406975d67e614_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1522\" data-rawheight=\"658\" class=\"origin_image zh-lightbox-thumb\" width=\"1522\" data-original=\"https://pic1.zhimg.com/v2-0bcb750ba45c1b43387406975d67e614_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1522&#39; height=&#39;658&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1522\" data-rawheight=\"658\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1522\" data-original=\"https://pic1.zhimg.com/v2-0bcb750ba45c1b43387406975d67e614_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-0bcb750ba45c1b43387406975d67e614_b.jpg\"/></figure><p>对比Faster RCNN的ROI pooling，作者采用了新的池化方式。对于一片 <img src=\"https://www.zhihu.com/equation?tex=h%5Ctimes+w\" alt=\"h\\times w\" eeimg=\"1\"/> 的ROI，新的池化方式选取如下区域做最大池化</p><p><img src=\"https://www.zhihu.com/equation?tex=H+%5Ctimes+%5Cmin%28W_%7Bmax%7D%2C+2Hw%2Fh%29\" alt=\"H \\times \\min(W_{max}, 2Hw/h)\" eeimg=\"1\"/> </p><p>其中 <img src=\"https://www.zhihu.com/equation?tex=H\" alt=\"H\" eeimg=\"1\"/> 大小固定， <img src=\"https://www.zhihu.com/equation?tex=W\" alt=\"W\" eeimg=\"1\"/> 的根据原高宽比扩大两倍，也就是说如果原高宽比是1：1，那么调整之后的高宽比是1：2，并且给出了一个最大值，超过最大值使用最大值。这么做主要hi针对狭长型的字符比如&#34;l&#34;, &#34;i&#34;，我的理解是对于这种字符如果使用原比例可能会导致感受野内信息太少。</p><p>池化之后的层会作为接下来LSTM网络的输入，问题在于LSTM的输入是序列，如何把特征图转化为序列，作者给出的方法是对于池化产生的特征图 <img src=\"https://www.zhihu.com/equation?tex=Q%5Cin+%5Cmathbb%7BR%7D%5E%7BC%5Ctimes+H+%5Ctimes+W%7D\" alt=\"Q\\in \\mathbb{R}^{C\\times H \\times W}\" eeimg=\"1\"/> 转化为 <img src=\"https://www.zhihu.com/equation?tex=q_1%2C...%2Cq_w%5Cin+%5Cmathbb%7BR%7D%5E%7BC+%5Ctimes+W%7D\" alt=\"q_1,...,q_w\\in \\mathbb{R}^{C \\times W}\" eeimg=\"1\"/>这样的序列，其中 <img src=\"https://www.zhihu.com/equation?tex=C\" alt=\"C\" eeimg=\"1\"/> 是通道数， <img src=\"https://www.zhihu.com/equation?tex=H%2CW\" alt=\"H,W\" eeimg=\"1\"/> 同上式。LSTM 的最后一层输出 <img src=\"https://www.zhihu.com/equation?tex=h_w\" alt=\"h_w\" eeimg=\"1\"/> 就捕获了 <img src=\"https://www.zhihu.com/equation?tex=Q\" alt=\"Q\" eeimg=\"1\"/> 中的有用信息，采用这种方式把池化结果转化为固定维度的表示。</p><ul><li>Text Recognition Network(TRN)</li></ul><p>TDN(Text Detecion Network)和Faster RCNN中类似不再赘述，重点讲一讲TRN，这应该是全篇论文的精华所在。TRN包含两个LTM网络，一个作为encoder一个作为decoder。先来看看encoder。encoder 的输入为RFE中的LSTM中每一个隐藏层的输出组成的序列 <img src=\"https://www.zhihu.com/equation?tex=h_1%2C...h_w\" alt=\"h_1,...h_w\" eeimg=\"1\"/>.encoder LSTM 每一个隐藏层的输出都会被记录下来作为decoder中注意力机制的上下文，记作<img src=\"https://www.zhihu.com/equation?tex=V%3D%5Bv_1%2C...%2Cv_w%5D+%5Cin+%5Cmathbb%7BR%7D%5E%7BR+%5Ctimes+W%7D\" alt=\"V=[v_1,...,v_w] \\in \\mathbb{R}^{R \\times W}\" eeimg=\"1\"/> 。</p><p>对于decoder LSTM，输入为长度为 <img src=\"https://www.zhihu.com/equation?tex=T%2B2+\" alt=\"T+2 \" eeimg=\"1\"/> 的向量： <img src=\"https://www.zhihu.com/equation?tex=X_0%2CX_1%2C...%2CX_%7BT%2B1%7D\" alt=\"X_0,X_1,...,X_{T+1}\" eeimg=\"1\"/> .其中：</p><p><img src=\"https://www.zhihu.com/equation?tex=X_0%3D%5BV_w%3B+Atten%28V%2C0%29%5D\" alt=\"X_0=[V_w; Atten(V,0)]\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=X_i%3D%5B%5Cpsi%28s_%7Bi-1%7D%29%3B+Atten%28V%2C+h%5E%7B%27%7D_%7Bi-1%7D%29%5D+%2C%5Cquad++for%5Cquad+i%3D1%2C...%2CT%2B1\" alt=\"X_i=[\\psi(s_{i-1}); Atten(V, h^{&#39;}_{i-1})] ,\\quad  for\\quad i=1,...,T+1\" eeimg=\"1\"/>  </p><p>解释一些变量的含义， <img src=\"https://www.zhihu.com/equation?tex=s_i\" alt=\"s_i\" eeimg=\"1\"/> 是样本中对应的输出，具体怎么对应作者没有提到，反正通过词嵌入就可以做到，输出 <img src=\"https://www.zhihu.com/equation?tex=s\" alt=\"s\" eeimg=\"1\"/> 被转化成这样的序列 <img src=\"https://www.zhihu.com/equation?tex=s%3D%5C%7B+s_0%2Cs_1%2C...s_%7BT%2B1%7D%5C%7D\" alt=\"s=\\{ s_0,s_1,...s_{T+1}\\}\" eeimg=\"1\"/> 其中 <img src=\"https://www.zhihu.com/equation?tex=s_0%2Cs_%7BT%2B1%7D\" alt=\"s_0,s_{T+1}\" eeimg=\"1\"/> 是标志文本开始和结束对应的符号，使用过google的word2vec的应该不陌生。 <img src=\"https://www.zhihu.com/equation?tex=V\" alt=\"V\" eeimg=\"1\"/> 就是上面encoder中的 <img src=\"https://www.zhihu.com/equation?tex=V\" alt=\"V\" eeimg=\"1\"/> ，重点在于 <img src=\"https://www.zhihu.com/equation?tex=%5Cpsi+%2C+Atten\" alt=\"\\psi , Atten\" eeimg=\"1\"/> 两个函数。 <img src=\"https://www.zhihu.com/equation?tex=%5Cpsi\" alt=\"\\psi\" eeimg=\"1\"/> 表示一个线性变换层加一个 <img src=\"https://www.zhihu.com/equation?tex=tanh\" alt=\"tanh\" eeimg=\"1\"/> 非线性变化层。 <img src=\"https://www.zhihu.com/equation?tex=Atten%28%29\" alt=\"Atten()\" eeimg=\"1\"/> 的定义由下式给出</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbf+g_j%3Dtanh%28%5Cbf+W_+%5Cit+v+%5Cbf+v_%5Cit+j%2B%5Cbf+W_%5Cit+h+%5Cit+h%5E%7B%27%7D_i%29%2C%5Cquad+j%3D1%2C...%2CW%2C\" alt=\"\\bf g_j=tanh(\\bf W_ \\it v \\bf v_\\it j+\\bf W_\\it h \\it h^{&#39;}_i),\\quad j=1,...,W,\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbf+%5Calpha+%3D+softmax%28%5Cbf+w%5E%5Ctop_%5Cit+g%5Ccdot+%5Cbf+%5Bg_1%2Cg_2%2C...%2Cg_%5Cit+W%5D%29%2C\" alt=\"\\bf \\alpha = softmax(\\bf w^\\top_\\it g\\cdot \\bf [g_1,g_2,...,g_\\it W]),\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbf+c_i%3D%5Csum+%5E%7BW%7D_%7Bj%3D1%7D%5Calpha_j+%5Cbf+v_+%5Cit+j%2C\" alt=\"\\bf c_i=\\sum ^{W}_{j=1}\\alpha_j \\bf v_ \\it j,\" eeimg=\"1\"/> </p><p>看看这个函数是如何实现注意力机制的。在此之前有必要先提一下注意力机制</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-3e2c05bd3788ffd6a05bc6f8ec421ba3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"929\" data-rawheight=\"413\" class=\"origin_image zh-lightbox-thumb\" width=\"929\" data-original=\"https://pic4.zhimg.com/v2-3e2c05bd3788ffd6a05bc6f8ec421ba3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;929&#39; height=&#39;413&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"929\" data-rawheight=\"413\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"929\" data-original=\"https://pic4.zhimg.com/v2-3e2c05bd3788ffd6a05bc6f8ec421ba3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-3e2c05bd3788ffd6a05bc6f8ec421ba3_b.jpg\"/></figure><p>注意力机制的核心在于给与针对想要与猜测的结果，对于已知的序列中的每个值给与不同的权重，然后获取一个Attention Value做进一步预测，比如在预测“<b>我有一只小毛驴，我从来也不____</b>”的时候，对于已知序列“<b>我 有 一只 小 毛驴，我 从来 也 不</b>”分别初始化不同的权重，然后通过定义合适的损失函数去更新权重。更详细的注意力机制相关内容可以阅读这一篇文章，他讲的更详细。</p><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/tg229dvt5i93mxaq5a6u/article/details/78422216\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-796c59c5388c94049f8e05c33eb94b7a_180x120.jpg\" data-image-width=\"720\" data-image-height=\"401\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">深度学习中的注意力机制 - CSDN大数据 - CSDN博客</a><p>回到前面decoder， <img src=\"https://www.zhihu.com/equation?tex=%5Cbf+Atten%28%29\" alt=\"\\bf Atten()\" eeimg=\"1\"/> 函数做的最重要的一件事就是针对 <img src=\"https://www.zhihu.com/equation?tex=%5Cbf+V\" alt=\"\\bf V\" eeimg=\"1\"/> 中不同的元素做了加权，具体的权重通过对encoder的输出 <img src=\"https://www.zhihu.com/equation?tex=%5Cbf+V\" alt=\"\\bf V\" eeimg=\"1\"/> 和RFE的输出 <img src=\"https://www.zhihu.com/equation?tex=%5Cbf+h\" alt=\"\\bf h\" eeimg=\"1\"/> 做线性变换和非线性变换获得。</p><p>最终decoder的输出是长度为38的向量，分别表示26个大小写不敏感的字母以及0到9十个数字和“?”,&#34;!&#34;，加上一个表示结束的标识。其实当时看到这里我是懵逼的，这种思路应该是处理不了中文的，因为中文字符的数量大概在三四千。就算是把中文拆解成笔画也是不可能的，因为不同于英文的顺序排列，中文的笔画可能会出现在一个方块内的任何位置，大概除了把汉字转化为拼音，不然的话基本上是不可能通过这种方式实现中文字符的识别的。但是自己挖的坑跪着也要填完，所以还是把损失函数介绍一下吧。</p><ul><li>Loss Function</li></ul><p>整个网络的损失函数包含三个部分</p><p><img src=\"https://www.zhihu.com/equation?tex=L%3D%5Cfrac%7B1%7D%7B%5Chat+N%7D%5Csum%5E%7B%5Chat+N%7D_%7Bi%3D1%7DL_%7Bcls%7D%28%5Chat+p_i%2C+%5Chat+p_i%5E%2A%29%2B%5Cfrac%7B1%7D%7B%5Chat+N_%2B%7D%5Csum%5E%7B%5Chat+N_%2B%7D_%7Bi%3D1%7DL_%7Breg%7D%28%5Chat+d_i%2C+%5Chat+d_i%5E%2A%29%2B%5Cfrac%7B1%7D%7B%5Chat+N_%2B%7D%5Csum%5E%7B%5Chat+N_%2B%7D_%7Bi%3D1%7DL_%7Brec%7D%28%5Cbf+Y%5E%5Cit+%7B%28i%29%7D%2C+%5Cbf+s%5E+%7B%28%5Cit+i%29%7D%29\" alt=\"L=\\frac{1}{\\hat N}\\sum^{\\hat N}_{i=1}L_{cls}(\\hat p_i, \\hat p_i^*)+\\frac{1}{\\hat N_+}\\sum^{\\hat N_+}_{i=1}L_{reg}(\\hat d_i, \\hat d_i^*)+\\frac{1}{\\hat N_+}\\sum^{\\hat N_+}_{i=1}L_{rec}(\\bf Y^\\it {(i)}, \\bf s^ {(\\it i)})\" eeimg=\"1\"/> </p><p>看下标应该可以看出来这个损失函数包含了一个是否是文字的分类损失以及一个坐标相关的回归损失，还有一个最终字符识别的损失， <img src=\"https://www.zhihu.com/equation?tex=%5Chat+N\" alt=\"\\hat N\" eeimg=\"1\"/> 是TPN的输出个数，代表的是TPN给出的文字区域正例和反例的总数量 <img src=\"https://www.zhihu.com/equation?tex=%5Chat+N%3D128\" alt=\"\\hat N=128\" eeimg=\"1\"/> ， <img src=\"https://www.zhihu.com/equation?tex=%5Chat+N_%2B\" alt=\"\\hat N_+\" eeimg=\"1\"/> 代表其中正例的个数, <img src=\"https://www.zhihu.com/equation?tex=%5Chat+N_%2B%3C64\" alt=\"\\hat N_+&lt;64\" eeimg=\"1\"/> 。其中前两个损失函数和Faster RCNN中的定义相同不在赘述，忘记了可以看看前两篇文章。</p><p><img src=\"https://www.zhihu.com/equation?tex=L_%7Bres%7D%28%5Cbf+Y%2C+s%29%3D-%5Csum_%7Bt%3D1%7D%5E%7BT%2B1%7D%5Clog+%5Cbf+y_%5Cit+t+%5Cbf%28%5Cit+s_t%5Cbf%29\" alt=\"L_{res}(\\bf Y, s)=-\\sum_{t=1}^{T+1}\\log \\bf y_\\it t \\bf(\\it s_t\\bf)\" eeimg=\"1\"/> </p><p>其中 <img src=\"https://www.zhihu.com/equation?tex=%5Cbf+Y%5E%7B%28i%29%7D+%3D+%5C%7B+y_0%5E%7B%28i%29%7D%2C...%2Cy_%7BT%2B1%7D%5E%7B%28i%29%7D+%5C%7D\" alt=\"\\bf Y^{(i)} = \\{ y_0^{(i)},...,y_{T+1}^{(i)} \\}\" eeimg=\"1\"/>表示decoder LSTM的输出。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>最后，很遗憾这篇论文没有给出相应的代码，而且因为理论上不能识别中文字符，加上最近诸多事务，下一个项目是做深度学习在网络安全上的应用，所以也没有进行进一步的代码编写工作，所以编程相关的就暂时撂下了。先立个flag，这段时间忙过会把代码核心部分贴上来，在这之前不给封面加图，大家什么时候看到这篇文章有封面图了，那就是代码OK了，可以点进来了。</p>", 
            "topic": [
                {
                    "tag": "OCR（光学字符识别）", 
                    "tagLink": "https://api.zhihu.com/topics/19574441"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "目标检测", 
                    "tagLink": "https://api.zhihu.com/topics/19596960"
                }
            ], 
            "comments": [
                {
                    "userName": "周杰", 
                    "userLink": "https://www.zhihu.com/people/f66d51b0ed37e9e7fe54738ee13efa53", 
                    "content": "<p>持续关注</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/45293568", 
            "userName": "右左瓜子", 
            "userLink": "https://www.zhihu.com/people/6107fd0900473aae4e5ec6824b4fde4a", 
            "upvote": 2, 
            "title": "Fast RCNN", 
            "content": "<p>上一篇详细的解读了Faster RCNN，忘了的话戳<b><a href=\"https://zhuanlan.zhihu.com/p/44784393\" class=\"internal\">这里</a>，</b>留下了个尾巴，就是Fast RCNN，这一篇就解读一下Fast RCNN，论文地址</p><a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1504.08083.pdf\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">fast rcnn</a><p>代码地址：</p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/rbgirshick/fast-rcnn\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic4.zhimg.com/v2-9f4e9c49a8e59e08abe70f8ba9b14fef_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">rbgirshick/fast-rcnn</a><p>废话就不多说了，我们先直接看论文。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-81fbd942a8a7e85efb9486b728ce7ce1_b.jpg\" data-size=\"normal\" data-rawwidth=\"616\" data-rawheight=\"241\" class=\"origin_image zh-lightbox-thumb\" width=\"616\" data-original=\"https://pic2.zhimg.com/v2-81fbd942a8a7e85efb9486b728ce7ce1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;616&#39; height=&#39;241&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"616\" data-rawheight=\"241\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"616\" data-original=\"https://pic2.zhimg.com/v2-81fbd942a8a7e85efb9486b728ce7ce1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-81fbd942a8a7e85efb9486b728ce7ce1_b.jpg\"/><figcaption>图来自论文</figcaption></figure><p>上面就是fast rcnn的结构图。fast rcnn的输入时一张图和一组目标的建议区域，论文里谈到图的时候用到了&#34;entire&#34;，也就是说一张完整的图和一些切割之后的图。模型先在原图上使用卷积和最大池化产生一个特征图(后面提到使用的是VGG16)，然后对于每一个目标建议区域使用ROI从特征图中提取固定长度的特征向量，ROI我们接下来会讲到。对于每一个特征向量使用全链接卷积然后产生两组输出，一组输出是使用softmax产生的概率估计评估包含目标的概率值的大小，另一组输出是对于建议区域坐标的偏置的输出，从上一篇文章过来的应该不会陌生，所以不多讲了。下面重点讲一讲ROI。</p><ul><li><b>ROI pooling Layer</b></li></ul><p>ROI pooling Layer就是使用最大池化把任何在感兴趣区域内的特征转化为一个小的，固定尺寸（ <img src=\"https://www.zhihu.com/equation?tex=H%5Ctimes+W\" alt=\"H\\times W\" eeimg=\"1\"/> ）的特征图。 其中 <img src=\"https://www.zhihu.com/equation?tex=H%2CW\" alt=\"H,W\" eeimg=\"1\"/> 是独立于任何ROI的超参数。ROI是在特征图上的矩形框，一个特征图由四个参数确定—— <img src=\"https://www.zhihu.com/equation?tex=%28r%2Cc%2Ch%2Cw%29\" alt=\"(r,c,h,w)\" eeimg=\"1\"/> .其中 <img src=\"https://www.zhihu.com/equation?tex=%28r%2Cc%29\" alt=\"(r,c)\" eeimg=\"1\"/> 是top-left值，也就是在特征图上从上往下数第 <img src=\"https://www.zhihu.com/equation?tex=r\" alt=\"r\" eeimg=\"1\"/> 个，从左往右数第 <img src=\"https://www.zhihu.com/equation?tex=c\" alt=\"c\" eeimg=\"1\"/> 个元素。 <img src=\"https://www.zhihu.com/equation?tex=%28w%2Ch%29\" alt=\"(w,h)\" eeimg=\"1\"/> 是矩形框的长宽值，通过这四个数字在特征图上定位一个矩形框，注意，我反复提到了特征图，因为我在看的时候会当成是在原图上取。</p><p>以上基本上是翻译的原文，我们需要注意的一点是ROI和ROI pooling Layer是不同的，ROI是一个矩形框，ROI pooling是一种池化方式，具体怎么池化的我们继续看论文。</p><p>ROI pooling是把ROI矩形框切割成 <img src=\"https://www.zhihu.com/equation?tex=H%5Ctimes+W\" alt=\"H\\times W\" eeimg=\"1\"/> 的网格，然后对每个网格进行最大池化。每个网格的大小也就是 <img src=\"https://www.zhihu.com/equation?tex=w%2FW+%5Ctimes+h%2FH\" alt=\"w/W \\times h/H\" eeimg=\"1\"/> ，如下图所示。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-1dc247339c7068664ec71a579759de63_b.jpg\" data-size=\"normal\" data-rawwidth=\"747\" data-rawheight=\"538\" class=\"origin_image zh-lightbox-thumb\" width=\"747\" data-original=\"https://pic4.zhimg.com/v2-1dc247339c7068664ec71a579759de63_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;747&#39; height=&#39;538&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"747\" data-rawheight=\"538\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"747\" data-original=\"https://pic4.zhimg.com/v2-1dc247339c7068664ec71a579759de63_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-1dc247339c7068664ec71a579759de63_b.jpg\"/><figcaption>作图不易，转贴注明来源</figcaption></figure><p>图中的 <img src=\"https://www.zhihu.com/equation?tex=W%2CH\" alt=\"W,H\" eeimg=\"1\"/> 取的是33，实际情况中一般比这个大，论文中取7*7。</p><p>以上就是ROI的内容，一个ROI实际上就是一个四元素元组。为了使用ROI，作者对VGG16做了微调。1.VGG16最后的最大池化层改为ROI池化；2.全链接层被之前提到的两个全连接层替代；3.输入由一组图片变为一组图片和一组图片对应的ROI。</p><p>其实第三点我是比较困惑的，输入的到底是ROI元组，还是ROI元组对应的区域，因为我不太了解两个输入是怎么卷积的。然后细看了后面的一些内容豁然开朗，如果你也有同样的困惑不要着急，下面会谈到。</p><ul><li><b>训练</b></li></ul><p>Fast RCNN采用随机梯度下降（SGD）来训练，使用分层抽样（herachical sampling）采样。具体抽样过程是这样的。首先随机抽样 <img src=\"https://www.zhihu.com/equation?tex=N\" alt=\"N\" eeimg=\"1\"/> 张图片，然后对每张图片采 <img src=\"https://www.zhihu.com/equation?tex=R%2FN\" alt=\"R/N\" eeimg=\"1\"/> 个ROI。文章中说 <img src=\"https://www.zhihu.com/equation?tex=R%3D128%2CN%3D2\" alt=\"R=128,N=2\" eeimg=\"1\"/> 的时候速度是 <img src=\"https://www.zhihu.com/equation?tex=N%3D128.R%3D128\" alt=\"N=128.R=128\" eeimg=\"1\"/> 的64倍。所以上一节提到的两个输入的问题解决了，是同一批样本中同时包含了图片和图片的ROI。</p><p>训练使用的损失函数是一种典型的多任务损失函数，先把公式写出来。</p><p><img src=\"https://www.zhihu.com/equation?tex=L%28p%2Cu%2Ct%5Eu%2Cv%29%3DL_%7Bcls%7D%28p%2Cu%29%2B%5Clambda%5Bu%5Cgeq1%5DL_%7Bloc%7D%28t%5Eu%2Cv%29\" alt=\"L(p,u,t^u,v)=L_{cls}(p,u)+\\lambda[u\\geq1]L_{loc}(t^u,v)\" eeimg=\"1\"/> </p><p>如果你是从上一篇文章过来的对这个公式应该比较熟悉，跟RPN的损失公式类似，但是有一些不同点。首先， <img src=\"https://www.zhihu.com/equation?tex=p%3D%28p_0%2C...%2Cp_K%29\" alt=\"p=(p_0,...,p_K)\" eeimg=\"1\"/> 是全链接卷积的结果经过softmax之后的K+1个输出，对应于K+1个categories,categories在本篇文章是首次出现有必要解释一下，目标检测的结果不仅是要知道有没有目标，而且要知道目标是什么，而这里的categories就是目标对应物体的标签， <img src=\"https://www.zhihu.com/equation?tex=p_k\" alt=\"p_k\" eeimg=\"1\"/> 就是目标是第 <img src=\"https://www.zhihu.com/equation?tex=k%2B1\" alt=\"k+1\" eeimg=\"1\"/> 个标签对应的物体的概率。回忆一下RPN，RPN这里对应的输出的是objectness score，也就是说包含目标的概率，不能探测出目标到底是什么，所以RPN后面还需要接一个Fast RCNN，如何接我们先不谈，继续谈损失函数。 <img src=\"https://www.zhihu.com/equation?tex=t\" alt=\"t\" eeimg=\"1\"/> 和上一篇的 <img src=\"https://www.zhihu.com/equation?tex=t\" alt=\"t\" eeimg=\"1\"/> 的意思一样不再展开，主要是公式太难打。 <img src=\"https://www.zhihu.com/equation?tex=L_%7Bcls%7D%28p%2Cu%29%3D-%5Clog+p_u\" alt=\"L_{cls}(p,u)=-\\log p_u\" eeimg=\"1\"/> ，是对于类别 <img src=\"https://www.zhihu.com/equation?tex=u\" alt=\"u\" eeimg=\"1\"/> 的对数损失。再看第二部分，首先解释一下Iverson bracket indicator function（说人话就是那个中括号，我不知道怎么翻译）</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Bu%5Cgeq1%5D%3D%5Cbegin%7Bequation%7D+%5Cleft%5C%7B++++++++++++++%5Cbegin%7Barray%7D%7Blr%7D++++++++++++++1%2C+%26++u%3E1%5C%5C++++++++++++++0%2C+%26+otherwise.%5C%5C+++++++++++++++++%5Cend%7Barray%7D+%5Cright.+%5Cend%7Bequation%7D+\" alt=\"[u\\geq1]=\\begin{equation} \\left\\{              \\begin{array}{lr}              1, &amp;  u&gt;1\\\\              0, &amp; otherwise.\\\\                 \\end{array} \\right. \\end{equation} \" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=u\" alt=\"u\" eeimg=\"1\"/> 是每一个目标类别对应的编号，一般的约定是背景，也就是没有目标的区域 <img src=\"https://www.zhihu.com/equation?tex=u%3D0\" alt=\"u=0\" eeimg=\"1\"/> ，其他的都按顺序取整数值，也就是说这个函数的意义就是丢弃不含目标的区域，仅此而已。搞清楚这一块继续看 <img src=\"https://www.zhihu.com/equation?tex=L_%7Bloc%7D\" alt=\"L_{loc}\" eeimg=\"1\"/> 。 <img src=\"https://www.zhihu.com/equation?tex=L_%7Bloc%7D\" alt=\"L_{loc}\" eeimg=\"1\"/> 是 <img src=\"https://www.zhihu.com/equation?tex=smooth_%7BL_1%7D\" alt=\"smooth_{L_1}\" eeimg=\"1\"/> 函数，其中 <img src=\"https://www.zhihu.com/equation?tex=t\" alt=\"t\" eeimg=\"1\"/> 的含义同上一篇，和RPN不同的地方在于， <img src=\"https://www.zhihu.com/equation?tex=L_%7Bloc%7D\" alt=\"L_{loc}\" eeimg=\"1\"/> 包含了所有目标类别的可能的位置偏移值。</p><p>是否是目标有天然的标签，所以样本的重点在于如何确定目标边框。类似于上一篇，在Fast RCNN中同样使用IoU，区别在于准则不同。Fast RCNN中取了IoU值大于0.5的25%的ROI。</p><p>正向传播和损失函数都有了，网络搭建就没有问题了，反向传播在ROI pooling层可能会有困惑，因为单独拿出来讲一下。因为ROI pooling是最大池化的一种变种，所以我们先回想一下最大池化中是怎么做反向传播的，最大池化中的反向传播只在卷积核对应的区域中的最大项中传播，所以ROI pooling层的反向传播应该也是只在每个网格起作用的那个元素上进行反向传播，有了这一步铺垫，下面我们把公式贴出来应该就比较好理解了。</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+L%7D%7B%5Cpartial+x_i%7D%3D%5Csum_r%5Csum_j%5Bi%3Di%5E%2A%28r%2Cj%29%5D%5Cfrac%7B%5Cpartial+L%7D%7B%5Cpartial%7By_%7Brj%7D%7D%7D\" alt=\"\\frac{\\partial L}{\\partial x_i}=\\sum_r\\sum_j[i=i^*(r,j)]\\frac{\\partial L}{\\partial{y_{rj}}}\" eeimg=\"1\"/> </p><p>解释一下符号， <img src=\"https://www.zhihu.com/equation?tex=x_i\" alt=\"x_i\" eeimg=\"1\"/> 是ROI pooling层中第 <img src=\"https://www.zhihu.com/equation?tex=i\" alt=\"i\" eeimg=\"1\"/> 个输入， <img src=\"https://www.zhihu.com/equation?tex=y_%7Brj%7D\" alt=\"y_{rj}\" eeimg=\"1\"/> 是第 <img src=\"https://www.zhihu.com/equation?tex=r\" alt=\"r\" eeimg=\"1\"/> 个ROI的第 <img src=\"https://www.zhihu.com/equation?tex=j\" alt=\"j\" eeimg=\"1\"/> 个输出， <img src=\"https://www.zhihu.com/equation?tex=y_%7Brj%7D%3Dx_%7Bi%5E%2A%28r%2Cj%29%7D\" alt=\"y_{rj}=x_{i^*(r,j)}\" eeimg=\"1\"/> ，其中 <img src=\"https://www.zhihu.com/equation?tex=i%5E%2A%28r%2Cj%29%3Darg%5Cmax+_%7Bi%5E%7B%27%7D%5Cin%5Cmathcal%7BR%7D%28r%2Cj%29%7Dx_%7Bi%5E%7B%27%7D%7D\" alt=\"i^*(r,j)=arg\\max _{i^{&#39;}\\in\\mathcal{R}(r,j)}x_{i^{&#39;}}\" eeimg=\"1\"/> ， <img src=\"https://www.zhihu.com/equation?tex=%5Cmathcal%7BR%7D%28r%2Cj%29\" alt=\"\\mathcal{R}(r,j)\" eeimg=\"1\"/> 是产生 <img src=\"https://www.zhihu.com/equation?tex=y_%7Br%2Cj%7D\" alt=\"y_{r,j}\" eeimg=\"1\"/> 对应的区块，中括号函数在上面已经介绍过了。有两次求和过程是因为特征图被切割了两次，一次切成多个ROI，一次是每个ROI被切割成固定大小的小方块，总而言之，就是只对小方块中最大的元素做反向传播，其他的舍弃，这也是中括号函数的作用。</p><p>在文章最后，作者提到了使用原图矩阵太大，所以使用了奇异值分解（SVD）把图像压缩后再处理，来达到一定程度的加速。</p><p>一些基准测试的结果就不在这里赘述了，最后结合上一篇Faster RCNN谈一谈Faster RCNN在哪里加速了。以下观点仅代表作者个人观点，没有实际论证过，如果不对还请各位指正，Faster RCNN的加速效果存在于预测中，训练的时候因为Faster RCNN要训练两个网络，所以不一定比Fast RCNN快多少，Faster RCNN和Fast RCNN最主要的不同在于Faster RCNN用神经网络预测出来的候选框取代了之前每一张图都要取很多候选框的过程，所以预测的时候Faster RCNN可以剔除掉大部分的错误候选框来达到一定程度的加速效果。</p><p>好了，本篇到此为止，讲完前面的铺垫我们后面开始逐渐进入OCR。</p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "神经网络", 
                    "tagLink": "https://api.zhihu.com/topics/19607065"
                }, 
                {
                    "tag": "目标检测", 
                    "tagLink": "https://api.zhihu.com/topics/19596960"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/44784393", 
            "userName": "右左瓜子", 
            "userLink": "https://www.zhihu.com/people/6107fd0900473aae4e5ec6824b4fde4a", 
            "upvote": 9, 
            "title": "OCR之前Faster RCNN", 
            "content": "<p>OCR应该算是深度学习领域的交叉学科，同时涉及图像处理和自然语言处理。先做图像识别进行粗分类，然后结合自然语言处理，把语言学上的特征纳入到分类考虑当中进一步提高准确率。目前为止比较前沿的是一种端到端的同时做文字检测和文字识别的网络，论文在这里：<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1707.03985.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Towards End-to-end Text Spotting with Convolutional Recurrent Neural Networks</a> 里边涉及到文字检测的部分采用了一种“TPN”的方法，是对经典的目标检测方法“RPN”做了一点变化。因为之前并没有处理过图像相关的内容，所以本节先介绍一下Faster RCNN(&#34;RPN&#34;是他的一部分)</p><p>论文地址：<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1506.01497.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</a></p><p>因为我们的主要目的是做文字检测和识别，所以就不对整个目标检测（object detection）的历史做回顾了(实际上从它的名字可以大致的看看它的发展史，即CNN→RCNN→Fast RCNN→Faster RCNN)，直接了解Faster RCNN.首先我们来看一下目标检测是要做什么。在Faster RCNN中目标检测的目的就是给出一张图，算法有能力准确的把图中的物体用矩形框框出来。之所以前面加了Faster RCNN作为限定词是因为现在有些目标检测算法已经可以直接把物体轮廓描出来，而不是矩形框。</p><p>知道了它要做什么，现在来看看它是怎么做到的。Faster RCNN的主体分为两部分，一部分是深度全卷积网络来给出建议区域（proposes regions）（RPN），另一部分是使用Fast RCNN来探测上一步给出的建议区域是否是包含目标的。其中在给出建议区域的时候采用了注意力机制。通过RPN（Region Propersal Network）来告诉网络注意力的位置。注意，RPN出来了，这应该是整篇论文最核心的部分，也是最绕的部分，下面会重点介绍一下RPN。</p><ul><li><b>Region Propersal Network</b></li></ul><blockquote>RPN是把一张图像作为输入，输出一系列可能包含有目标的矩形框，同时每个矩形框对应一个目标分数（衡量存在目标的可能性） 。在这个模型中使用了全链接卷积，因为我们想和Fast RCNN网络共享参数，所以想让RPN和Fast RCNN共享同一个卷积层。在具体实验中使用了ZF模型和VGG-16模型来产生这个卷积层。</blockquote><p>以上是我把第二篇论文的相关内容概括的翻译了一下。如果之前只做自然语言处理，那估计上面那段话会看的云里雾里。不过不要紧，抛开那些术语所包含的模型的细则，你只要知道整个流程是这样的，输入一张图，通过一个卷积模型得到一个卷积后的特征图（feature map），也就是上面说的卷积层。然后让RPN和Fast RCNN(注意这里是Fast不是Faster)在这个特征图上做计算。就好了，抛开VGG-16不讲（因为这又是一篇论文，但是实现方式比较简单，我们暂且认为它是一种可以很好的提取图像特征的卷积网络），接下来继续谈RPN，Fast RCNN后面会谈到不用着急。</p><p>前面我们提到RPN要给出建议区域，那建议区域怎么给出呢？是通过在特征图（在本文中特征图特指经过VGG-16或者ZF卷积产生的特征图）滑动一个nn的窗口，每个窗口映射成一个低维的向量（ZF对应256-d，VGG对应512-d）。好了，这里有疑问了，如何映射，因为文中没有细说，网上有人说“看作一个n*n的卷积”，但是“看作”两个字让人很不信服，后来在源码里面找到这么一段：</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"k\">def</span> <span class=\"nf\">_region_proposal</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">net_conv</span><span class=\"p\">,</span> <span class=\"n\">is_training</span><span class=\"p\">,</span> <span class=\"n\">initializer</span><span class=\"p\">):</span>\n    <span class=\"n\">rpn</span> <span class=\"o\">=</span> <span class=\"n\">slim</span><span class=\"o\">.</span><span class=\"n\">conv2d</span><span class=\"p\">(</span><span class=\"n\">net_conv</span><span class=\"p\">,</span> <span class=\"n\">cfg</span><span class=\"o\">.</span><span class=\"n\">RPN_CHANNELS</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">],</span> <span class=\"n\">trainable</span><span class=\"o\">=</span><span class=\"n\">is_training</span><span class=\"p\">,</span> <span class=\"n\">weights_initializer</span><span class=\"o\">=</span><span class=\"n\">initializer</span><span class=\"p\">,</span>\n                        <span class=\"n\">scope</span><span class=\"o\">=</span><span class=\"s2\">&#34;rpn_conv/3x3&#34;</span><span class=\"p\">)</span></code></pre></div><p>其中3*3是论文中取得窗口大小，通道数预先在配置文件中配置完毕。所以这一点有疑问的地方解决了，可以放心大胆的说这就是一个卷积（事实上在文章的后面确实提到了这就是一个卷积）。这里参考的是Faster RCNN的tensorflow实现版，地址：</p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/endernewton/tf-faster-rcnn\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/endernewton/</span><span class=\"invisible\">tf-faster-rcnn</span><span class=\"ellipsis\"></span></a><p>我在看这个的代码的时候当时我就想跪了，这么复杂的网络又梳理的这么好，而代码的作者又是论文的作者，第一次深切体会到什么叫高山仰止。代码做简单的配置就可以跑起来，简直是业界良心，好了，废话不多说了，继续谈RPN。</p><p>得到一维向量之后，把这个一维向量送给两个全链接层，一个是box-regression层，一个是box-classification层，都是1*1的全链接卷积层。整个RPN网络基本上就是这样，虽然我们做了一通计算，一顿操作猛如虎，可是好像什么都没得到，因为Anchor还没出来，RPN是Faster RCNN的核心，而Anchor是RPN的核心。</p><ul><li><b>Anchors</b></li></ul><p>在每个滑动窗口中会同时预测k个可能的区域，所以box-regression会有4k个输出，而box-classification会有2k个输出。在这里我解释一下，如果没做过图像处理（像我这样的）可能对2k和4k很疑惑，为什么，后面会有详细的说明，在这里你只要知道box-regression衡量了目标位置的坐标，而box-classification衡量了目标存在的可能性。坐标需要4个值来确定，可能性2个值。anchor就位于滑动窗口的中心，注意，窗口是在特征图上滑动的，并不是在输入图像上滑动的，因为特征图是经过多层卷积得到的，所以特征图上的一个窗口，对应与输入图像的一大块。实际上如果是用VGG得到的特征图，一个3*3的滑动窗口会包含228个像素点的信息。一个anchor对应于输入图像上一些固定尺度和固定长宽比的矩形框，被称为anchor box。作者在实验中使用了三个不同的尺度和3个长宽比，即k=9，因为下一篇要讲图像文字识别，所以这里多说一点，做图像文字识别相对于目标检测最大的区别就在于矩形框（anchor box）的选择，目标识别一般使用的1：1，1：2，2：1的矩形框，而文字识别用的是1：1，1：3甚至1：18这样的长方形矩形框。论文给出了一张图，可以很形象的展示anchor是什么。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-0006f7f09bbd7496e855fa84c4138f5c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1310\" data-rawheight=\"440\" class=\"origin_image zh-lightbox-thumb\" width=\"1310\" data-original=\"https://pic1.zhimg.com/v2-0006f7f09bbd7496e855fa84c4138f5c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1310&#39; height=&#39;440&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1310\" data-rawheight=\"440\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1310\" data-original=\"https://pic1.zhimg.com/v2-0006f7f09bbd7496e855fa84c4138f5c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-0006f7f09bbd7496e855fa84c4138f5c_b.jpg\"/></figure><p>到这里，论文介绍Anchor的内容就没有，不知道你们懵不懵逼，反正我看完是懵逼的，说的是anchor和anchor box一一对应但是是怎么对应的？网上很多解释也是猜测居多，没有给出实锤，为了了解作者本意还是去代码中找答案。</p><div class=\"highlight\"><pre><code class=\"language-text\">def generate_anchors_pre(height, width, feat_stride, anchor_scales=(8,16,32), anchor_ratios=(0.5,1,2)):\n  &#34;&#34;&#34; A wrapper function to generate anchors given different scales\n    Also return the number of anchors in variable &#39;length&#39;\n  &#34;&#34;&#34;\n  &#34;&#34;&#34;生成anchor的预处理方法，generate_anchors方法就是直接产生各种大小的anchor box，generate_anchors_pre方法\n     是把每一个anchor box对应到原图上\n      height = tf.to_int32(tf.ceil(self._im_info[0] / np.float32(self._feat_stride[0])))\n      width = tf.to_int32(tf.ceil(self._im_info[1] / np.float32(self._feat_stride[0])))\n      feat_stride: 经过VGG或者ZF后特征图相对于原图的在长或者宽上的缩放倍数，也就是说height和width对应于特征图长宽\n      anchor_scales：anchor尺寸\n      anchor_ratios: anchor长宽比\n  &#34;&#34;&#34;\n  anchors = generate_anchors(ratios=np.array(anchor_ratios), scales=np.array(anchor_scales)) # 产生各种大小的anchor box\n  A = anchors.shape[0] # anchor的种数\n  shift_x = np.arange(0, width) * feat_stride # 特征图相对于原图的偏移\n  shift_y = np.arange(0, height) * feat_stride # 特征图相对于原图的偏移\n  shift_x, shift_y = np.meshgrid(shift_x, shift_y) # 返回坐标矩阵\n  shifts = np.vstack((shift_x.ravel(), shift_y.ravel(), shift_x.ravel(), shift_y.ravel())).transpose()\n  K = shifts.shape[0]\n  # width changes faster, so here it is H, W, C\n  anchors = anchors.reshape((1, A, 4)) + shifts.reshape((1, K, 4)).transpose((1, 0, 2)) # anchor坐标加上anchor box大小\n  anchors = anchors.reshape((K * A, 4)).astype(np.float32, copy=False)\n  length = np.int32(anchors.shape[0]) \n  return anchors, length</code></pre></div><p>以上就是tensorflow实现版的anchor产生的部分代码，关键的地方我已经加了注释，对于shift_x和shift_y我们可以用代码段测试一下，更为直观。</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">shift_x</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">9</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"mi\">16</span> <span class=\"c1\"># 假设特征图为9*9， VGG的feat_stride为16</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">shift_y</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"mi\">9</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"mi\">16</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">shift_x</span><span class=\"p\">,</span> <span class=\"n\">shift_y</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">meshgrid</span><span class=\"p\">(</span><span class=\"n\">shift_x</span><span class=\"p\">,</span> <span class=\"n\">shift_y</span><span class=\"p\">)</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">fig</span> <span class=\"o\">=</span> <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">figure</span><span class=\"p\">()</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">scatter</span><span class=\"p\">(</span><span class=\"n\">shift_x</span><span class=\"o\">.</span><span class=\"n\">ravel</span><span class=\"p\">(),</span> <span class=\"n\">shift_y</span><span class=\"o\">.</span><span class=\"n\">ravel</span><span class=\"p\">())</span>\n<span class=\"o\">&gt;&gt;&gt;</span> <span class=\"n\">fig</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span></code></pre></div><p>结果如下</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-f20f7b35ec348f22d10b16ed896f09d0_b.jpg\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"473\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic1.zhimg.com/v2-f20f7b35ec348f22d10b16ed896f09d0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;640&#39; height=&#39;473&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"473\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https://pic1.zhimg.com/v2-f20f7b35ec348f22d10b16ed896f09d0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-f20f7b35ec348f22d10b16ed896f09d0_b.jpg\"/><figcaption>特征图每个点对应于原图一个点，在原图对应的点上放anchor box</figcaption></figure><p>因为VGG的网络层数太多，所以下面用一个简单的卷积来展示一下他的计算方式，如果想要了解同样的方法一层一层推下去就好了</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-0c0b6b13563d5534226b4336b88b87ca_b.jpg\" data-size=\"normal\" data-rawwidth=\"1018\" data-rawheight=\"919\" class=\"origin_image zh-lightbox-thumb\" width=\"1018\" data-original=\"https://pic3.zhimg.com/v2-0c0b6b13563d5534226b4336b88b87ca_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1018&#39; height=&#39;919&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1018\" data-rawheight=\"919\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1018\" data-original=\"https://pic3.zhimg.com/v2-0c0b6b13563d5534226b4336b88b87ca_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-0c0b6b13563d5534226b4336b88b87ca_b.jpg\"/><figcaption>作图不易，使用注明出处</figcaption></figure><p>上面两张图基本上把anchor讲的很明白了，作者在代码中还把超出图片的anchor box删除了，有兴趣的话可以自己看源码。好了整个RPN除了损失函数外其他部分已经完全打通了，下面就讲讲损失函数（原文中还有对anchor的平移不变性以及高效性的论述在此不展开）。</p><ul><li><b>Loss Function</b></li></ul><p>前面我们谈到RPN网络的reg层和cls层分别有4k和2k个输出，在探讨损失函数之前先来说明一下这两个层的输出到底代表了什么。在训练RPN的时候，对于每一个anchor box我们有一个二分类标签，表明是否有目标。这个二分类标签通过IoU(Intersction-over-Union)方法获得。IoU是衡量同一个图像上预测区和ground_truth给出的实际区域的重合度的一个标准，介于0到1，0表示无重合，1表示完全重合</p><p><img src=\"https://www.zhihu.com/equation?tex=IoU%3D%5Cfrac%7BTP%7D%7BFP%2BTP%2BFN%7D\" alt=\"IoU=\\frac{TP}{FP+TP+FN}\" eeimg=\"1\"/> </p><p>这个公式对于做过自然语言处理的同学来说应该很熟悉了，两个区域重叠区域大小比上覆盖区域的大小就是了。有了这个衡量标准，我们通过两个条件获取正例：</p><ol><li>一个anchor对应的k个anchor box中IoU值最大的</li><li>IoU值大于0.7的</li></ol><p>通常条件2就能获取大多数正例，条件一作为某些特殊情况的补充。反例的获取只有一个条件：IoU值小于0.3。其他的结果丢弃。通过这种方式得到cls层的标签，衡量了anchor box存在目标的可能性。对于reg层，我们给出这样的标签，训练时：</p><p><img src=\"https://www.zhihu.com/equation?tex=t_x%5E%2A%3D%28x%5E%2A-x_a%29%2Fw_a\" alt=\"t_x^*=(x^*-x_a)/w_a\" eeimg=\"1\"/> <img src=\"https://www.zhihu.com/equation?tex=%2Ct_y%5E%2A%3D%28y%5E%2A-y_a%29%2Fh_a\" alt=\",t_y^*=(y^*-y_a)/h_a\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=t_w%5E%2A%3D%5Clog%28w%5E%2A%2Fw_a%29\" alt=\"t_w^*=\\log(w^*/w_a)\" eeimg=\"1\"/> <img src=\"https://www.zhihu.com/equation?tex=%2Ct_h%5E%2A%3D%5Clog%28h%5E%2A%2Fh_a%29\" alt=\",t_h^*=\\log(h^*/h_a)\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=x%2Cy%2Cw%2Ch\" alt=\"x,y,w,h\" eeimg=\"1\"/> 分别代表横纵坐标和宽高，<img src=\"https://www.zhihu.com/equation?tex=x%5E%2A+\" alt=\"x^* \" eeimg=\"1\"/> 用来代表ground-truth的值， <img src=\"https://www.zhihu.com/equation?tex=x_a\" alt=\"x_a\" eeimg=\"1\"/> 代表anchor给的值的。预测时：</p><p><img src=\"https://www.zhihu.com/equation?tex=t_x%3D%28x-x_a%29%2Fw_a\" alt=\"t_x=(x-x_a)/w_a\" eeimg=\"1\"/> <img src=\"https://www.zhihu.com/equation?tex=%2Ct_y%3D%28y-y_a%29%2Fh_a\" alt=\",t_y=(y-y_a)/h_a\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=t_w%3D%5Clog%28w%2Fw_a%29\" alt=\"t_w=\\log(w/w_a)\" eeimg=\"1\"/> <img src=\"https://www.zhihu.com/equation?tex=%2Ct_h%3D%5Clog%28h%2Fh_a%29\" alt=\",t_h=\\log(h/h_a)\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=x\" alt=\"x\" eeimg=\"1\"/> 代表预测值。这样，训练时我们可以通过最小化 <img src=\"https://www.zhihu.com/equation?tex=t\" alt=\"t\" eeimg=\"1\"/> 来训练网络，预测是可以通过 <img src=\"https://www.zhihu.com/equation?tex=t\" alt=\"t\" eeimg=\"1\"/> 获取预测框的实际位置。</p><p>最终损失函数为：                      </p><p><img src=\"https://www.zhihu.com/equation?tex=L%28%5C%7Bp_i%5C%7D%2C%5C%7Bt_i%5C%7D%29%3D%5Cfrac%7B1%7D%7BN_%7Bcls%7D%7D%5Csum_iL_%7Bcls%7D%28p_i%2Cp_i%5E%2A%29%2B%5Clambda%5Cfrac%7B1%7D%7BN_%7Breg%7D%7D%5Csum_ip%5E%2AL_%7Breg%7D%28t_i%2Ct_i%5E%2A%29\" alt=\"L(\\{p_i\\},\\{t_i\\})=\\frac{1}{N_{cls}}\\sum_iL_{cls}(p_i,p_i^*)+\\lambda\\frac{1}{N_{reg}}\\sum_ip^*L_{reg}(t_i,t_i^*)\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=p_i\" alt=\"p_i\" eeimg=\"1\"/> 为anchor box是一个目标的概率， <img src=\"https://www.zhihu.com/equation?tex=p_i%5E%2A\" alt=\"p_i^*\" eeimg=\"1\"/> 通过前面的IoU算法获取正反例后正例为1反例为0。 <img src=\"https://www.zhihu.com/equation?tex=L_%7Breg%7D%28t_i%2Ct_i%5E%2A%29%3DR%28t_i%2Ct_i%5E%2A%29\" alt=\"L_{reg}(t_i,t_i^*)=R(t_i,t_i^*)\" eeimg=\"1\"/> , <img src=\"https://www.zhihu.com/equation?tex=R\" alt=\"R\" eeimg=\"1\"/> 是Fast RCNN中提出来的损失函数 <img src=\"https://www.zhihu.com/equation?tex=smooth+\" alt=\"smooth \" eeimg=\"1\"/> <img src=\"https://www.zhihu.com/equation?tex=L_1+\" alt=\"L_1 \" eeimg=\"1\"/> .</p><p><img src=\"https://www.zhihu.com/equation?tex=smooth_%7BL_1%7D%28x%29%3D%5Cleft%5C%7B++++++++++++++%5Cbegin%7Barray%7D%7Blr%7D++++++++++++++0.5x%5E2%2C+%26+if%3A%7Ct%7C%5Cleq1%5C%5C++++++++++++++%7Cx%7C-0.5+%26+otherwise%2C+++++++++++++++%5Cend%7Barray%7D+%5Cright.\" alt=\"smooth_{L_1}(x)=\\left\\{              \\begin{array}{lr}              0.5x^2, &amp; if:|t|\\leq1\\\\              |x|-0.5 &amp; otherwise,               \\end{array} \\right.\" eeimg=\"1\"/> </p><p>最后： <img src=\"https://www.zhihu.com/equation?tex=t_i%3D%5Csum_%7Bj%5Cin%5C%7Bx%2Cy%2Cw%2Ch%5C%7D%7Dt_j\" alt=\"t_i=\\sum_{j\\in\\{x,y,w,h\\}}t_j\" eeimg=\"1\"/> </p><p class=\"ztext-empty-paragraph\"><br/></p><p>通过训练RPN提出建议区域，一般然后再用Fast RCNN在同样的特征图上对建议区域做目标探测获得最终结果，文中提到了三种RPN和Fast RCNN的训练方式。我们下一篇谈完Fast RCNN之后继续谈。（主要是这一篇有点长了，我自己画图找源码前前后后也写了三天，已经明显感觉到有些公式已经懒得解释了，继续怼下去怕影响质量，所以Fast RCNN单独开一篇）</p>", 
            "topic": [
                {
                    "tag": "OCR（光学字符识别）", 
                    "tagLink": "https://api.zhihu.com/topics/19574441"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "目标检测", 
                    "tagLink": "https://api.zhihu.com/topics/19596960"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/44672558", 
            "userName": "右左瓜子", 
            "userLink": "https://www.zhihu.com/people/6107fd0900473aae4e5ec6824b4fde4a", 
            "upvote": 0, 
            "title": "学习的核心方法——梯度下降", 
            "content": "<p>不知道是不是“山竹”的影响，杭州这边也下起了大雨，可以有时间整理一下梯度下降的内容。关于梯度下降，需要搞清楚三个内容。什么是梯度下降，为什么要用梯度下降，以及怎么用梯度下降，不同于前两节涉及的多数是处理方法和网络结构的思考，这一节的内容数学的东西会多一点，但是如果耐心看下去都很简单，除了求偏导都是很基本的运算，而求偏导只不过是在求导的基础上加了一些规则。</p><p>在介绍梯度下降法之前我们通过一个简单的例子引入，回顾一下高中求二次函数极值的过程，在确定二次函数之后，通过对二次函数求导获得一个一次函数，然后获取导函数为零的解析解，得到的变量的值就是极值点。考虑到大部分函数解析解比较难求，我们采用数值的方式。第一种数值的方式是采用固定步长，理论上是可行的，但是如果采用了过大的步长则很容易越过极值点，而采用太小的步长则会增加迭代时间。</p><p>考虑到导数可以衡量函数的陡峭成都所以我们想通过导数来动态的确定步长：</p><p><img src=\"https://www.zhihu.com/equation?tex=x%5E%7B%27%7D+%3D+x-%5Cepsilon+f%5E%7B%27%7D%28x%29\" alt=\"x^{&#39;} = x-\\epsilon f^{&#39;}(x)\" eeimg=\"1\"/> </p><p>这样，在陡峭的地方拥有较大的步幅以快速接近极值点，较平坦的地方拥有较小的步幅防止越过极值点。其中 <img src=\"https://www.zhihu.com/equation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"/> 是一个可以进一步控制步长的常数，也就是学习率。实际效果如图所示：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-7ce771b2d66259825354b4295866954d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1920\" data-rawheight=\"924\" class=\"origin_image zh-lightbox-thumb\" width=\"1920\" data-original=\"https://pic2.zhimg.com/v2-7ce771b2d66259825354b4295866954d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1920&#39; height=&#39;924&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1920\" data-rawheight=\"924\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1920\" data-original=\"https://pic2.zhimg.com/v2-7ce771b2d66259825354b4295866954d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-7ce771b2d66259825354b4295866954d_b.jpg\"/></figure><p>本来打算用echart画的可是用js直接计算数据总会卡死浏览器，所以用了matplotlib，图片上展示的已经比较直观了，附上代码</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n<span class=\"kn\">import</span> <span class=\"nn\">matplotlib.pyplot</span> <span class=\"k\">as</span> <span class=\"nn\">plt</span>\n\n\n<span class=\"c1\"># 获取指定点切线坐标</span>\n<span class=\"k\">def</span> <span class=\"nf\">get_deriv</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">):</span>\n    <span class=\"n\">k</span> <span class=\"o\">=</span> <span class=\"mi\">2</span><span class=\"o\">*</span><span class=\"n\">x</span> <span class=\"o\">-</span> <span class=\"mi\">100</span>\n    <span class=\"n\">b</span> <span class=\"o\">=</span> <span class=\"n\">y</span> <span class=\"o\">-</span> <span class=\"n\">k</span><span class=\"o\">*</span><span class=\"n\">x</span>\n    <span class=\"n\">x_data</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"n\">i</span> <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">101</span><span class=\"p\">)])</span>\n    <span class=\"n\">y_data</span> <span class=\"o\">=</span> <span class=\"n\">k</span><span class=\"o\">*</span><span class=\"n\">x_data</span><span class=\"o\">+</span><span class=\"n\">b</span>\n    <span class=\"k\">return</span> <span class=\"n\">y_data</span>\n\n<span class=\"n\">x_data</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"n\">i</span> <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">101</span><span class=\"p\">)])</span>\n<span class=\"n\">y_data</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">power</span><span class=\"p\">((</span><span class=\"n\">x_data</span><span class=\"o\">-</span><span class=\"mi\">50</span><span class=\"p\">),</span> <span class=\"mi\">2</span><span class=\"p\">)</span>\n<span class=\"n\">x_n</span> <span class=\"o\">=</span> <span class=\"mi\">1</span> <span class=\"c1\"># 初始值</span>\n<span class=\"n\">lr</span> <span class=\"o\">=</span> <span class=\"mf\">0.3</span> <span class=\"c1\"># 学习率</span>\n<span class=\"n\">x_n_arr</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n<span class=\"n\">x_n_arr</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">x_n</span><span class=\"p\">)</span>\n<span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">10</span><span class=\"p\">):</span>\n    <span class=\"n\">x_n</span> <span class=\"o\">=</span> <span class=\"n\">x_n</span> <span class=\"o\">-</span> <span class=\"n\">lr</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"o\">*</span><span class=\"n\">x_n</span><span class=\"o\">-</span><span class=\"mi\">100</span><span class=\"p\">)</span>\n    <span class=\"k\">if</span> <span class=\"n\">x_n</span> <span class=\"o\">&gt;</span> <span class=\"mi\">60</span><span class=\"p\">:</span>\n        <span class=\"k\">break</span>\n    <span class=\"k\">else</span><span class=\"p\">:</span>\n        <span class=\"n\">x_n_arr</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">x_n</span><span class=\"p\">)</span>\n\n<span class=\"k\">for</span> <span class=\"n\">x_i</span> <span class=\"ow\">in</span> <span class=\"n\">x_n_arr</span><span class=\"p\">:</span>\n    <span class=\"n\">yi_data</span> <span class=\"o\">=</span> <span class=\"n\">get_deriv</span><span class=\"p\">(</span><span class=\"n\">x_i</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">x_i</span><span class=\"o\">-</span><span class=\"mi\">50</span><span class=\"p\">)</span><span class=\"o\">*</span><span class=\"p\">(</span><span class=\"n\">x_i</span><span class=\"o\">-</span><span class=\"mi\">50</span><span class=\"p\">))</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">scatter</span><span class=\"p\">(</span><span class=\"n\">x_i</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">x_i</span><span class=\"o\">-</span><span class=\"mi\">50</span><span class=\"p\">)</span><span class=\"o\">*</span><span class=\"p\">(</span><span class=\"n\">x_i</span><span class=\"o\">-</span><span class=\"mi\">50</span><span class=\"p\">))</span>\n    <span class=\"c1\"># 为了方便展示只取大于-1的值</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">x_data</span><span class=\"p\">[</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">where</span><span class=\"p\">(</span><span class=\"n\">yi_data</span><span class=\"o\">&gt;-</span><span class=\"mi\">50</span><span class=\"p\">)],</span> <span class=\"n\">yi_data</span><span class=\"p\">[</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">where</span><span class=\"p\">(</span><span class=\"n\">yi_data</span><span class=\"o\">&gt;-</span><span class=\"mi\">50</span><span class=\"p\">)],</span> <span class=\"s1\">&#39;--&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">x_data</span><span class=\"p\">,</span> <span class=\"n\">y_data</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">grid</span><span class=\"p\">()</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span></code></pre></div><p>好了，上面的整个过程其实就是一个用梯度下降计算极值的过程，现在把二次函数的变量从标量换成向量，二次函数换成凸函数，求导换成求.......问题来了，在输入是标量的情况下我们求导得到函数的导数，那输入是向量的情况下对函数求导的形式怎么写，叫什么呢？梯度就出来了，我们来看看梯度的正式的定义。</p><blockquote><b>梯度（gradient）</b>是相对于一个向量求导的导数，具有多维输入的函数 <img src=\"https://www.zhihu.com/equation?tex=f\" alt=\"f\" eeimg=\"1\"/> 的梯度是包含所有偏导数的向量，记为 <img src=\"https://www.zhihu.com/equation?tex=+%5Cnabla_xf%28x%29\" alt=\" \\nabla_xf(x)\" eeimg=\"1\"/> 。梯度的第 <img src=\"https://www.zhihu.com/equation?tex=i\" alt=\"i\" eeimg=\"1\"/> 个元素是 <img src=\"https://www.zhihu.com/equation?tex=f\" alt=\"f\" eeimg=\"1\"/> 关于 <img src=\"https://www.zhihu.com/equation?tex=x_i\" alt=\"x_i\" eeimg=\"1\"/> 的偏导数。</blockquote><p>上面我们通过感性的判断使用动态步长有利于，下面简单证明一下。在 <img src=\"https://www.zhihu.com/equation?tex=%5Ctextbf+u\" alt=\"\\textbf u\" eeimg=\"1\"/> (单位向量)方向的方向导数是函数 <img src=\"https://www.zhihu.com/equation?tex=f\" alt=\"f\" eeimg=\"1\"/> 在 <img src=\"https://www.zhihu.com/equation?tex=%5Ctextbf+u\" alt=\"\\textbf u\" eeimg=\"1\"/> 方向的斜率，即函数 <img src=\"https://www.zhihu.com/equation?tex=f%28x%2B%5Calpha+u%29\" alt=\"f(x+\\alpha u)\" eeimg=\"1\"/> (注意此处 <img src=\"https://www.zhihu.com/equation?tex=x%2Cu+\" alt=\"x,u \" eeimg=\"1\"/> 是向量)在 <img src=\"https://www.zhihu.com/equation?tex=%5Calpha%3D0\" alt=\"\\alpha=0\" eeimg=\"1\"/> 时关于 <img src=\"https://www.zhihu.com/equation?tex=%5Calpha\" alt=\"\\alpha\" eeimg=\"1\"/> 的导数. <img src=\"https://www.zhihu.com/equation?tex=%5Calpha+%3D+0+\" alt=\"\\alpha = 0 \" eeimg=\"1\"/> 时， <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+%5Calpha%7Df%28x+%2B+%5Calpha+u%29+%3D+u%5Ctop%5Cnabla_xf%28x%29\" alt=\"\\frac{\\partial}{\\partial \\alpha}f(x + \\alpha u) = u\\top\\nabla_xf(x)\" eeimg=\"1\"/> .</p><p>在求极小值的问题中（极大值问题可以转化为求极小值问题），为了最小化 <img src=\"https://www.zhihu.com/equation?tex=f\" alt=\"f\" eeimg=\"1\"/> ，我们希望找到使 <img src=\"https://www.zhihu.com/equation?tex=f\" alt=\"f\" eeimg=\"1\"/> 下降的最快的方向，最小化方向倒数（下降导数为负，下降最快求导数极小值）：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cmin%5Climits_%7Bu%2C+u%5Ctop+u%7Du%5Ctop+%5Cnabla_xf%28x%29%3D%5Cmin%5Climits_%7Bu%2C+u%5Ctop+u%7D%7C%7Cu%7C%7C_2%7C%7C%5Cnabla_xf%28x%29%7C%7C_2%5Ccos%5Ctheta\" alt=\"\\min\\limits_{u, u\\top u}u\\top \\nabla_xf(x)=\\min\\limits_{u, u\\top u}||u||_2||\\nabla_xf(x)||_2\\cos\\theta\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%7C%7Cx%7C%7C_2\" alt=\"||x||_2\" eeimg=\"1\"/> 即L2范数，这里不做展开。因为 <img src=\"https://www.zhihu.com/equation?tex=u%5Ctop+u+%3D+1\" alt=\"u\\top u = 1\" eeimg=\"1\"/> (单位向量乘积)，忽略与 <img src=\"https://www.zhihu.com/equation?tex=u\" alt=\"u\" eeimg=\"1\"/> 无关项上式简化为 <img src=\"https://www.zhihu.com/equation?tex=%5Cmin+%5Climits_%7Bu%7D%5Ccos+%5Ctheta\" alt=\"\\min \\limits_{u}\\cos \\theta\" eeimg=\"1\"/> 。即 <img src=\"https://www.zhihu.com/equation?tex=u\" alt=\"u\" eeimg=\"1\"/> 与梯度方向相反时取得最小，梯度向量指向上坡，负梯度向量指向下坡，所以负梯度方向上移动可以减小 <img src=\"https://www.zhihu.com/equation?tex=f\" alt=\"f\" eeimg=\"1\"/> ，这被称为最速下降法或梯度下降。梯度下降建议 <img src=\"https://www.zhihu.com/equation?tex=x\" alt=\"x\" eeimg=\"1\"/> 的更新公式为:</p><p><img src=\"https://www.zhihu.com/equation?tex=x%5E%7B%27%7D+%3D+x+-+%5Cepsilon%5Cnabla_xf%28x%29\" alt=\"x^{&#39;} = x - \\epsilon\\nabla_xf(x)\" eeimg=\"1\"/> </p><p>其中 <img src=\"https://www.zhihu.com/equation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"/> 为学习率。</p><p>目前为止我们解决了什么是梯度下降，为什么要用梯度下降，最后再来看看怎么更好的使用梯度下降。</p><ul><li><b>随机梯度下降(SGD)</b></li></ul><p>先看看直接梯度下降的问题在哪里。</p><p>机器学习算法中损失函数通常可以分解为每个样本损失函数的总和。所以对于求损失函数的极小值，每一次参数更新需要把每个样本的值代入梯度下降公式做一次计算。当训练样本大的时候，时间成本就会很高。所以随机梯度下降法就出来。</p><blockquote>随机梯度下降的核心是，梯度是期望，期望可使用小规模的样本近似估计。具体而言，在算法的每一步，我们从训练集中均匀抽出一minibatch (minibatch)样本。minibatch的数目 m′ 通常是一个相对较小的数，从一到几百。 重要的是，当训练集大小 m 增长时，m′ 通常是固定的。我们可能在拟合几十亿的样 本时，每次更新计算只用到几百个样本。</blockquote><p>关于随机梯度下降为什么可用，除了期望的解释外，找了很多资料并没有找到比较严谨的理论证明，我也证明不了，既然从期望的角度可以感性的解释一下，而事实证明他又是很可行的，我们就直接用吧。算法过程：</p><blockquote><b>随机梯度下降在第k个训练迭代的更新</b><br/>Require: 学习速率 <img src=\"https://www.zhihu.com/equation?tex=%5Cepsilon_k\" alt=\"\\epsilon_k\" eeimg=\"1\"/> <br/>Require: 初始参数 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> <br/>    while 没有达到停止准则 do<br/>           从训练集中采包含 <img src=\"https://www.zhihu.com/equation?tex=m\" alt=\"m\" eeimg=\"1\"/> 个样本 <img src=\"https://www.zhihu.com/equation?tex=%5C%7Bx%5E%7B%281%29%7D%2Cx%5E%7B%282%29%7D%2C...%2Cx%5E%7B%28m%29%7D%5C%7D\" alt=\"\\{x^{(1)},x^{(2)},...,x^{(m)}\\}\" eeimg=\"1\"/> 的minibatch，对应目标为      <br/> <img src=\"https://www.zhihu.com/equation?tex=y%5E%7B%28i%29%7D\" alt=\"y^{(i)}\" eeimg=\"1\"/> 。<br/>           计算梯度估计： <img src=\"https://www.zhihu.com/equation?tex=%5Chat%7Bg%7D+%5Cleftarrow+%2B%5Cfrac%7B1%7D%7Bm%7D%5Cnabla_%5Ctheta%5Csum_iL%28f%28x%5E%7B%28i%29%7D%3B%5Ctheta%29%2Cy%5E%7B%28i%29%7D%29\" alt=\"\\hat{g} \\leftarrow +\\frac{1}{m}\\nabla_\\theta\\sum_iL(f(x^{(i)};\\theta),y^{(i)})\" eeimg=\"1\"/> <br/>           应用更新： <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta%5Cleftarrow%5Ctheta-%5Cepsilon_k%5Chat+g\" alt=\"\\theta\\leftarrow\\theta-\\epsilon_k\\hat g\" eeimg=\"1\"/> <br/>     endwhile</blockquote><p>注意 <img src=\"https://www.zhihu.com/equation?tex=%5Cepsilon_k\" alt=\"\\epsilon_k\" eeimg=\"1\"/> ，下标表明他在每一步的训练都不是固定值。在实际的运用中，是否固定是可选的。我们可以看看tensorflow中SGD类的init方法签名</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"k\">class</span> <span class=\"nc\">SGD</span><span class=\"p\">(</span><span class=\"n\">Optimizer</span><span class=\"p\">):</span>\n  <span class=\"s2\">&#34;&#34;&#34;Stochastic gradient descent optimizer.\n</span><span class=\"s2\">  Includes support for momentum,\n</span><span class=\"s2\">  learning rate decay, and Nesterov momentum.\n</span><span class=\"s2\">  Arguments:\n</span><span class=\"s2\">      lr: float &gt;= 0. Learning rate.\n</span><span class=\"s2\">      momentum: float &gt;= 0. Parameter that accelerates SGD\n</span><span class=\"s2\">          in the relevant direction and dampens oscillations.\n</span><span class=\"s2\">      decay: float &gt;= 0. Learning rate decay over each update.\n</span><span class=\"s2\">      nesterov: boolean. Whether to apply Nesterov momentum.\n</span><span class=\"s2\">  &#34;&#34;&#34;</span>\n\n  <span class=\"k\">def</span> <span class=\"nf\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">lr</span><span class=\"o\">=</span><span class=\"mf\">0.01</span><span class=\"p\">,</span> <span class=\"n\">momentum</span><span class=\"o\">=</span><span class=\"mf\">0.</span><span class=\"p\">,</span> <span class=\"n\">decay</span><span class=\"o\">=</span><span class=\"mf\">0.</span><span class=\"p\">,</span> <span class=\"n\">nesterov</span><span class=\"o\">=</span><span class=\"kc\">False</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">):</span>\n    <span class=\"nb\">super</span><span class=\"p\">(</span><span class=\"n\">SGD</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">)</span></code></pre></div><p>momentum和nesterov两个参数后面会介绍，lr就是 <img src=\"https://www.zhihu.com/equation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"/> ，decay参数即为 <img src=\"https://www.zhihu.com/equation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"/> 的衰减。衰减参数是一个常数，确定了衰减值后 <img src=\"https://www.zhihu.com/equation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"/> 以如下方式衰减</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cepsilon_k+%3D+%281-%5Calpha%29%5Cepsilon_o%2B%5Calpha%5Cepsilon_%5Ctau\" alt=\"\\epsilon_k = (1-\\alpha)\\epsilon_o+\\alpha\\epsilon_\\tau\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%5Calpha+%3D+%5Cfrac%7Bk%7D%7B%5Ctau%7D\" alt=\"\\alpha = \\frac{k}{\\tau}\" eeimg=\"1\"/> 即衰减系数，也就是说衰减是线性的而且在 <img src=\"https://www.zhihu.com/equation?tex=%5Ctau\" alt=\"\\tau\" eeimg=\"1\"/> 步后停止衰减。之所以引入衰减是因为保证SGD收敛的一个充分条件是：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Csum_%7Bk%3D1%7D%5E%7B%5Cinfty%7D%5Cepsilon_k+%3D+%5Cinfty+%E4%B8%94%5Csum_%7Bk%3D1%7D%5E%7B%5Cinfty%7D%5Cepsilon_k%5E2+%3C+%5Cinfty\" alt=\"\\sum_{k=1}^{\\infty}\\epsilon_k = \\infty 且\\sum_{k=1}^{\\infty}\\epsilon_k^2 &lt; \\infty\" eeimg=\"1\"/> </p><p>实际上引入衰减后并不能保证第二个条件，因为即使在引入衰减后再 <img src=\"https://www.zhihu.com/equation?tex=%5Ctau\" alt=\"\\tau\" eeimg=\"1\"/> 步后也会停止衰减，使 <img src=\"https://www.zhihu.com/equation?tex=%5Cepsilon_k\" alt=\"\\epsilon_k\" eeimg=\"1\"/> 为常数。所以随机梯度下降的收敛率比batch梯度下降的收敛率要差。关于而这孰优孰劣的对比经常是一个权衡后的结果，在此不再展开。接下来来看看SGD方法的另外两个参数momentum、nesterov。对于这两种方法在此只介绍方法，具体收敛性能的论证涉及到矩阵分析的内容不再展开，想要做更深入的了解可以参考张贤达教授的《矩阵分析》中的具体章节。</p><ul><li><b>动量（momentum）法</b></li></ul><p>动量法的产生是为了加快梯度下降法的收敛速率，跟普通梯度下降法最大的不同在于动量法的参数有两步更新。动量算法引入了变量 <img src=\"https://www.zhihu.com/equation?tex=v\" alt=\"v\" eeimg=\"1\"/> 充当速度的角色——它代表参数在参数空 间移动的方向和速度。速度被设为负梯度的指数衰减平均。在动量学习算法中，我们假设是单位质量，因此速度向量 <img src=\"https://www.zhihu.com/equation?tex=v\" alt=\"v\" eeimg=\"1\"/> 也可以看作是粒子的动量，这一概念来自物理学中动量等于质量和速度的乘积。引入动量后，新的更新公式分为两步，第一步更新动量，第二步更新参数。</p><p><img src=\"https://www.zhihu.com/equation?tex=v%5Cleftarrow%5Calpha+v+-+%5Cepsilon%5Cnabla_%5Ctheta+%28%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5E%7Bm%7DL%28f%28x%5E%7B%28i%29%7D%3B%5Ctheta%29%2Cy%5E%7B%28i%29%7D%29%29%2C\" alt=\"v\\leftarrow\\alpha v - \\epsilon\\nabla_\\theta (\\frac{1}{m}\\sum_{i=1}^{m}L(f(x^{(i)};\\theta),y^{(i)})),\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%5Ctheta%5Cleftarrow%5Ctheta%2Bv\" alt=\"\\theta\\leftarrow\\theta+v\" eeimg=\"1\"/> </p><p>对比之前的梯度下降法， <img src=\"https://www.zhihu.com/equation?tex=v\" alt=\"v\" eeimg=\"1\"/> 可以记录之前所有的梯度信息，然后通过超参数 <img src=\"https://www.zhihu.com/equation?tex=%5Calpha%5Cin%5B0%2C1%29\" alt=\"\\alpha\\in[0,1)\" eeimg=\"1\"/> 来决定之前的梯度的贡献衰减的有多快。这个超参数也就是我们上面看到的tensorflow中的“momentum”参数。和普通梯度下降法的对比如下图（图来自Goodfellow的《深度学习》）：</p><figure data-size=\"small\"><noscript><img src=\"https://pic2.zhimg.com/v2-b394c7ffa7de7817b56af7cd61d5e539_b.jpg\" data-size=\"small\" data-rawwidth=\"624\" data-rawheight=\"547\" class=\"origin_image zh-lightbox-thumb\" width=\"624\" data-original=\"https://pic2.zhimg.com/v2-b394c7ffa7de7817b56af7cd61d5e539_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;624&#39; height=&#39;547&#39;&gt;&lt;/svg&gt;\" data-size=\"small\" data-rawwidth=\"624\" data-rawheight=\"547\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"624\" data-original=\"https://pic2.zhimg.com/v2-b394c7ffa7de7817b56af7cd61d5e539_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-b394c7ffa7de7817b56af7cd61d5e539_b.jpg\"/><figcaption>普通梯度下降法</figcaption></figure><figure data-size=\"small\"><noscript><img src=\"https://pic2.zhimg.com/v2-ae12e27d7cb8bac7e4646918c333e4f5_b.jpg\" data-size=\"small\" data-rawwidth=\"715\" data-rawheight=\"644\" class=\"origin_image zh-lightbox-thumb\" width=\"715\" data-original=\"https://pic2.zhimg.com/v2-ae12e27d7cb8bac7e4646918c333e4f5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;715&#39; height=&#39;644&#39;&gt;&lt;/svg&gt;\" data-size=\"small\" data-rawwidth=\"715\" data-rawheight=\"644\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"715\" data-original=\"https://pic2.zhimg.com/v2-ae12e27d7cb8bac7e4646918c333e4f5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-ae12e27d7cb8bac7e4646918c333e4f5_b.jpg\"/><figcaption>带有动量的梯度下降法</figcaption></figure><p>具体的算法如下</p><blockquote><b>使用动量的随机梯度下降</b><br/>Require：学习速率 <img src=\"https://www.zhihu.com/equation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"/> ，动量参数 <img src=\"https://www.zhihu.com/equation?tex=%5Calpha\" alt=\"\\alpha\" eeimg=\"1\"/> <br/>Require：初始参数 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta+\" alt=\"\\theta \" eeimg=\"1\"/> ，初始速度 <img src=\"https://www.zhihu.com/equation?tex=v\" alt=\"v\" eeimg=\"1\"/> <br/>    while 没达到停止准则 do<br/>        从训练集中采包含m个样本 <img src=\"https://www.zhihu.com/equation?tex=%5C%7Bx%5E%7B%281%29%7D%2Cx%5E%7B%282%29%7D%2C...%2Cx%5E%7B%28m%29%7D%5C%7D\" alt=\"\\{x^{(1)},x^{(2)},...,x^{(m)}\\}\" eeimg=\"1\"/> 的minibatch，对应目标为 <br/> <img src=\"https://www.zhihu.com/equation?tex=y%5E%7B%28i%29%7D\" alt=\"y^{(i)}\" eeimg=\"1\"/> <br/>        计算梯度估计： <img src=\"https://www.zhihu.com/equation?tex=g%5Cleftarrow+%5Cfrac%7B1%7D%7Bm%7D%5Cnabla_%7B%5Ctheta%7D%5Csum_%7Bi%3D1%7D%5E%7Bm%7DL%28f%28x%5E%7B%28i%29%7D%3B%5Ctheta%29%2Cy%5E%7B%28i%29%7D%29\" alt=\"g\\leftarrow \\frac{1}{m}\\nabla_{\\theta}\\sum_{i=1}^{m}L(f(x^{(i)};\\theta),y^{(i)})\" eeimg=\"1\"/> <br/>        计算速度更新： <img src=\"https://www.zhihu.com/equation?tex=v+%5Cleftarrow+%5Calpha+v-%5Cepsilon+g\" alt=\"v \\leftarrow \\alpha v-\\epsilon g\" eeimg=\"1\"/> <br/>        应用更新： <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta+%5Cleftarrow+%5Ctheta+%2B+v\" alt=\"\\theta \\leftarrow \\theta + v\" eeimg=\"1\"/> <br/>    endwhile</blockquote><p>假设初始速度 <img src=\"https://www.zhihu.com/equation?tex=v%3D0\" alt=\"v=0\" eeimg=\"1\"/> ，动量法观测到的梯度总为 <img src=\"https://www.zhihu.com/equation?tex=g\" alt=\"g\" eeimg=\"1\"/> ，那么最终的速度：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Clim_%7Bn+%5Crightarrow+%5Cinfty%7D%7Bv_n%7D+%3D%5Clim_%7Bn+%5Crightarrow+%5Cinfty%7D+-%5Calpha%5En%5Cepsilon+g-%5Calpha%5E%7Bn-1%7D%5Cepsilon+g-...-%5Cepsilon+g\" alt=\"\\lim_{n \\rightarrow \\infty}{v_n} =\\lim_{n \\rightarrow \\infty} -\\alpha^n\\epsilon g-\\alpha^{n-1}\\epsilon g-...-\\epsilon g\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%3D%5Clim_%7Bn+%5Crightarrow+%5Cinfty%7D-%5Cepsilon+g%28%5Calpha%5En%2B%5Calpha%5E%7Bn-1%7D%2B...%2B1%29\" alt=\"=\\lim_{n \\rightarrow \\infty}-\\epsilon g(\\alpha^n+\\alpha^{n-1}+...+1)\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%3D%5Cfrac%7B%5Cepsilon%7C%7Cg%7C%7C%7D%7B1-%5Calpha%7D+%2C%28%5Calpha%5Cin%5B0%2C1%29%29\" alt=\"=\\frac{\\epsilon||g||}{1-\\alpha} ,(\\alpha\\in[0,1))\" eeimg=\"1\"/> </p><p>通常 <img src=\"https://www.zhihu.com/equation?tex=%5Calpha\" alt=\"\\alpha\" eeimg=\"1\"/> 取0.5， 0.9， 0.99也就是说对应于最大20倍、10倍、100倍的加速</p><ul><li><b>Nesterov 动量</b></li></ul><p>顾名思义，Nesterov 动量是动量法的一个变种，所以废话就不多说了，先把算法摆上来</p><blockquote>Require：学习速率 <img src=\"https://www.zhihu.com/equation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"/> ，动量参数 <img src=\"https://www.zhihu.com/equation?tex=%5Calpha\" alt=\"\\alpha\" eeimg=\"1\"/> <br/>Require：初始参数 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta+\" alt=\"\\theta \" eeimg=\"1\"/> ，初始速度 <img src=\"https://www.zhihu.com/equation?tex=v\" alt=\"v\" eeimg=\"1\"/> <br/>    while 没达到停止准则 do<br/>        从训练集中采包含m个样本 <img src=\"https://www.zhihu.com/equation?tex=%5C%7Bx%5E%7B%281%29%7D%2Cx%5E%7B%282%29%7D%2C...%2Cx%5E%7B%28m%29%7D%5C%7D\" alt=\"\\{x^{(1)},x^{(2)},...,x^{(m)}\\}\" eeimg=\"1\"/> 的minibatch，对应目标为 <br/> <img src=\"https://www.zhihu.com/equation?tex=y%5E%7B%28i%29%7D\" alt=\"y^{(i)}\" eeimg=\"1\"/> <br/>        应用临时更新： <img src=\"https://www.zhihu.com/equation?tex=%5Ctilde%7B%5Ctheta%7D%5Cleftarrow+%5Ctheta%2B%5Calpha+v\" alt=\"\\tilde{\\theta}\\leftarrow \\theta+\\alpha v\" eeimg=\"1\"/> <br/>        计算梯度（在临时点）： <img src=\"https://www.zhihu.com/equation?tex=g%5Cleftarrow+%5Cfrac%7B1%7D%7Bm%7D%5Cnabla_%7B%5Ctilde%7B%5Ctheta%7D%7D%5Csum_%7Bi%3D1%7D%5E%7Bm%7DL%28f%28x%5E%7B%28i%29%7D%3B%5Ctilde%7B%5Ctheta%7D%29%2Cy%5E%7B%28i%29%7D%29\" alt=\"g\\leftarrow \\frac{1}{m}\\nabla_{\\tilde{\\theta}}\\sum_{i=1}^{m}L(f(x^{(i)};\\tilde{\\theta}),y^{(i)})\" eeimg=\"1\"/> <br/>        计算速度更新： <img src=\"https://www.zhihu.com/equation?tex=v+%5Cleftarrow+%5Calpha+v-%5Cepsilon+g\" alt=\"v \\leftarrow \\alpha v-\\epsilon g\" eeimg=\"1\"/> <br/>        应用更新： <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta+%5Cleftarrow+%5Ctheta+%2B+v\" alt=\"\\theta \\leftarrow \\theta + v\" eeimg=\"1\"/> <br/>    endwhile</blockquote><p>多了一步， <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 做了一次更新，加上了动量的因素。Nesterov 动量在batch梯度下降的时候可以显著提高收敛率，但是在随机梯度下降的时候并没有显著的提升，这是在运用的时候需要注意的。</p><p>梯度下降方法介绍到此，除此之外，对于学习率也有根据梯度自适应的方法，比如tensorflow中的AdamOptimizer，AdagradOptimizer，RMSPropOptimizer等等，他们的核心思想都是建立学习率与梯度的函数来根据梯度的变化动态的改变学习率，涉及的一些超参数tensorflow已经取了默认值，一般不需要改动，除非对收敛速度有很高的要求。本节的引用部分觉来自Goodfellow的《深度学习》一书。</p>", 
            "topic": [
                {
                    "tag": "梯度下降", 
                    "tagLink": "https://api.zhihu.com/topics/19650497"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/44585530", 
            "userName": "右左瓜子", 
            "userLink": "https://www.zhihu.com/people/6107fd0900473aae4e5ec6824b4fde4a", 
            "upvote": 11, 
            "title": "LSTM与文本分类", 
            "content": "<p>前面已经谈了词向量，词向量做完就可以做一些高级任务比如文本分类，问答系统，textsum等等。这一篇讲一讲文本分类的任务，主要是用在舆情分析和新闻分类上，当然任何可以归结为文本分类的任务都可以使用这种模型。</p><p>大部分时候文本分类都会用到RNN，因为不同于图像处理，经常只需要块状信息，文本相关的问题经常要用到全文的信息，也有机构主张文本问题用CNN但是并没有表现出明显好于RNN的效果。前面提到过LSTM，这一节会谈的更细一点。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-fa5317a98f87664e6ac02cf964df02fa_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"474\" data-rawheight=\"190\" class=\"origin_image zh-lightbox-thumb\" width=\"474\" data-original=\"https://pic3.zhimg.com/v2-fa5317a98f87664e6ac02cf964df02fa_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;474&#39; height=&#39;190&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"474\" data-rawheight=\"190\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"474\" data-original=\"https://pic3.zhimg.com/v2-fa5317a98f87664e6ac02cf964df02fa_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-fa5317a98f87664e6ac02cf964df02fa_b.jpg\"/></figure><p>上图即RNN的计算图，右边为展开后的图。和多层感知机不同的是RNN网络多了一个输入，即前一个网络隐藏层的输出会当作后一个网络的输入，注意一点，在这里我们说的是隐藏层的输出，实际使用的有所不同，后面会提到。这样网络再做反向传播的时候会保留从第一个输入开始所有的对分类有帮助的信息，但是有一个缺点是不能并行化处理，时间和内存成本很高。</p><p>实际处理的时候使用的是导师驱动过程，如下图所示。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-080513fbcaa000c1ef0cc506bf6c5966_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1439\" data-rawheight=\"1080\" class=\"origin_image zh-lightbox-thumb\" width=\"1439\" data-original=\"https://pic3.zhimg.com/v2-080513fbcaa000c1ef0cc506bf6c5966_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1439&#39; height=&#39;1080&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1439\" data-rawheight=\"1080\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1439\" data-original=\"https://pic3.zhimg.com/v2-080513fbcaa000c1ef0cc506bf6c5966_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-080513fbcaa000c1ef0cc506bf6c5966_b.jpg\"/></figure><p>原图来自Goodfellow的《深度学习》。关键点在于导师模式在训练时直接把前一次正确的输出直接给了后一次作为输入。这样做可以解决并行化计算的问题，因为训练的时候直接使用前一次的样本给的输出结果作为后一次计算的输入，不必等待前一次计算，也不用存储前一次的结算结果。但是缺点在于只保存了前一次输入的有限的信息，特别的在文本二分类任务中，对于高维度的输入矩阵，你只能使用他们的标签信息，而分类中相当于一个标量，所以在做测试和预测的时候，最后的预测结果只相当于前几次预测结果加权后加上最后一次预测结果。</p><p>网络是由一系列线性变化加上激活层构成，激活层一般也是使用常用的激活函数如sigmoid或者ReLu，所以求梯度上不存在什么问题，就不专门展开了。</p><p>再谈一谈LSTM（长短期记忆门控RNN），找了半天还是GoodFellow的图比较明晰，网上没有，就只好手画一个贴上来。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ac70cbc8f0a3282716fe9f5d0524d2b7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"873\" data-rawheight=\"654\" class=\"origin_image zh-lightbox-thumb\" width=\"873\" data-original=\"https://pic4.zhimg.com/v2-ac70cbc8f0a3282716fe9f5d0524d2b7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;873&#39; height=&#39;654&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"873\" data-rawheight=\"654\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"873\" data-original=\"https://pic4.zhimg.com/v2-ac70cbc8f0a3282716fe9f5d0524d2b7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-ac70cbc8f0a3282716fe9f5d0524d2b7_b.jpg\"/></figure><p>从此图可以很容易的看到lstm和rnn最大的不同在于，上一步的输出会在下一步的三个地方起作用，而普通的RNN只在输入的位置起作用，其次，每一次作用都由一个sigmoid单元控制来决定是抑制还是增强。而普通RNN中只是加权后相加。最下面右边三个sigmoid单元就是LSTM的核心，根据作用于主网络的位置的不同分为输入门，遗忘门，输出门，注意到每个门有三个对应的输入，后面我们展开每个门的公式的时候每个门也是对应三大块。</p><p>其他结构相同不再赘述，详细谈谈三个门。遗忘门，遗忘门作用在线性自环的位置，注意一下，普通的rnn是没有线性自环的（即右边的黑框），线性自环一开始是在渗漏单元（比LSTM稍早的一个想法）中提出的，用于处理长期依赖问题。现在我们只需要知道加入这个自环可以在一定程度上解决长期依赖的问题就好了。遗忘门的前向传播：</p><p><img src=\"https://www.zhihu.com/equation?tex=f_%7Bi%7D%5E%7B%28t%29%7D+%3D+%5Csigma%28b_%7Bi%7D%5E%7Bf%7D%2B%5Csum_%7Bj%7D%5E%7B%7D%7BU_%7Bi%2Cj%7D%5E%7Bf%7Dx_%7Bj%7D%5E%7B%28t%29%7D%7D%2B%5Csum_%7Bj%7D%5E%7B%7D%7BW_%7Bi%2Cj%7D%5E%7Bf%7Dh_%7Bj%7D%5E%7Bi-1%7D%7D%29\" alt=\"f_{i}^{(t)} = \\sigma(b_{i}^{f}+\\sum_{j}^{}{U_{i,j}^{f}x_{j}^{(t)}}+\\sum_{j}^{}{W_{i,j}^{f}h_{j}^{i-1}})\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=x_%7B%7D%5E%7B%28t%29%7D\" alt=\"x_{}^{(t)}\" eeimg=\"1\"/> 是当前输入， <img src=\"https://www.zhihu.com/equation?tex=h_%7B%7D%5E%7Bt%7D\" alt=\"h_{}^{t}\" eeimg=\"1\"/> 是当前隐藏层向量， <img src=\"https://www.zhihu.com/equation?tex=b_%7B%7D%5E%7Bf%7D%2CU_%7B%7D%5E%7Bf%7D%2CW_%7B%7D%5E%7Bf%7D\" alt=\"b_{}^{f},U_{}^{f},W_{}^{f}\" eeimg=\"1\"/> 分别是偏置、输入权重和遗忘门循环权重。LSTM细胞内部状态更新如下（图中标示“状态”处）</p><p><img src=\"https://www.zhihu.com/equation?tex=s_%7Bi%7D%5E%7B%28t%29%7D+%3D+f_%7Bi%7D%5E%7B%28t%29%7Ds_%7Bi%7D%5E%7B%28t-1%29%7D%2Bg_%7Bi%7D%5E%7B%28t%29%7D%5Csigma%28b_i%2B%5Csum_%7Bj%7D%5E%7B%7D%7BU_%7Bi.j%7Dx_%7Bj%7D%5E%7Bt%7D%2B%5Csum_%7Bj%7DW_%7Bi%2Cj%7Dh_%7Bj%7D%5E%7B%28t-1%29%7D%7D%29\" alt=\"s_{i}^{(t)} = f_{i}^{(t)}s_{i}^{(t-1)}+g_{i}^{(t)}\\sigma(b_i+\\sum_{j}^{}{U_{i.j}x_{j}^{t}+\\sum_{j}W_{i,j}h_{j}^{(t-1)}})\" eeimg=\"1\"/> </p><p>b,U,W分别是LSTM细胞中的偏置、输入权重和循环权重，Goodfellow的《深度学习》图中并没有上图中的虚线，但是他的更新公式里边的输入会把上一次的输出加进来（即上式中的 <img src=\"https://www.zhihu.com/equation?tex=h_j%5E%7B%28t-1%29%7D\" alt=\"h_j^{(t-1)}\" eeimg=\"1\"/> ）,而在其他的lstm图中也确实会把上一次的输出连接到输入上，所以我用虚线补上。 <img src=\"https://www.zhihu.com/equation?tex=g_i%5E%7B%28t%29%7D\" alt=\"g_i^{(t)}\" eeimg=\"1\"/> 是外部输入门，即上图中第二个sigmoid单元，它的更新方式类似与遗忘门，有多相似呢？相似到我在打公式的时候直接把f替换成g就好了。</p><p><img src=\"https://www.zhihu.com/equation?tex=g_%7Bi%7D%5E%7B%28t%29%7D+%3D+%5Csigma%28b_%7Bi%7D%5E%7Bg%7D%2B%5Csum_%7Bj%7D%5E%7B%7D%7BU_%7Bi%2Cj%7D%5E%7Bg%7Dx_%7Bj%7D%5E%7B%28t%29%7D%7D%2B%5Csum_%7Bj%7D%5E%7B%7D%7BW_%7Bi%2Cj%7D%5E%7Bg%7Dh_%7Bj%7D%5E%7Bi-1%7D%7D%29\" alt=\"g_{i}^{(t)} = \\sigma(b_{i}^{g}+\\sum_{j}^{}{U_{i,j}^{g}x_{j}^{(t)}}+\\sum_{j}^{}{W_{i,j}^{g}h_{j}^{i-1}})\" eeimg=\"1\"/> </p><p>得到 <img src=\"https://www.zhihu.com/equation?tex=s_%7B%7D%5E%7B%28t%29%7D\" alt=\"s_{}^{(t)}\" eeimg=\"1\"/>以后和输出门相乘后经由反双曲函数得到最终输出。输出门即上图最右边的门，同样的，更新方式与输入门遗忘门类似：</p><p><img src=\"https://www.zhihu.com/equation?tex=q_%7Bi%7D%5E%7B%28t%29%7D+%3D+%5Csigma%28b_%7Bi%7D%5E%7Bo%7D%2B%5Csum_%7Bj%7D%5E%7B%7D%7BU_%7Bi%2Cj%7D%5E%7Bo%7Dx_%7Bj%7D%5E%7B%28t%29%7D%7D%2B%5Csum_%7Bj%7D%5E%7B%7D%7BW_%7Bi%2Cj%7D%5E%7Bo%7Dh_%7Bj%7D%5E%7Bi-1%7D%7D%29\" alt=\"q_{i}^{(t)} = \\sigma(b_{i}^{o}+\\sum_{j}^{}{U_{i,j}^{o}x_{j}^{(t)}}+\\sum_{j}^{}{W_{i,j}^{o}h_{j}^{i-1}})\" eeimg=\"1\"/> </p><p>最后的输出</p><p><img src=\"https://www.zhihu.com/equation?tex=h_i%5E%7B%28t%29%7D+%3D+tanh%28s_i%5E%7B%28t%29%7D%29q_i%5E%7B%28t%29%7D\" alt=\"h_i^{(t)} = tanh(s_i^{(t)})q_i^{(t)}\" eeimg=\"1\"/> </p><p>至此LSTM前向传播就结束，反向传播很容易用链式反则得到。</p><p>我个人认为，三个门的形式完全一样，都有抑制或增强的效果，之所以起不同的名字，跟所在的位置相关，不必太纠结类似于输入门有没有遗忘效果的问题。</p><p>谈完LSTM，继续谈文本分类。既然专栏叫谈点儿实在的，所以一定会有相关的可以直接使用的代码，首推Adit Deshpande的基于tensorflow的在jupyter中运行的代码，注释详尽图文并茂真业界良心，而且还有其他各种类似方式的cnn和GAN网络，地址：</p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/adeshpande3/LSTM-Sentiment-Analysis\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/adeshpande3/</span><span class=\"invisible\">LSTM-Sentiment-Analysis</span><span class=\"ellipsis\"></span></a><p>基本上就是用双向的LSTM去给文本分类，在实战中可能需要加入正则项和提前终止。</p><p>其次是百度的一个情感分析的开源代码：</p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/baidu/Senta\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-2d1c4b71f84093693711b4f12cbe90d4_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/baidu/Senta</span><span class=\"invisible\"></span></a><p>是基于百度自己的paddlepaddle框架的，但是paddlepaddle目前为止不支持python3，优点是实现了基于各种网络的文本分类方法诸如CNN, LSTM, BILSTM, GRU等等。虽然开源代码是免费的并不能要求别人什么但是我还是觉得百度的代码诚意不够，第一没有做词嵌入，第二注释几乎没有，第三方法非常简单，与其说是开源了文本分类的方法不如说是开源了用paddlepaddle搭建各种网络的教程。</p><p>文本分类就谈到这里，后续将继续谈谈OCR，因为我的代码也还没撸完，不给代码不符合专栏“谈点儿实在的”的主体思想所以在这之前会介绍一下神经网络的基本知识。</p>", 
            "topic": [
                {
                    "tag": "LSTM", 
                    "tagLink": "https://api.zhihu.com/topics/20023220"
                }, 
                {
                    "tag": "RNN", 
                    "tagLink": "https://api.zhihu.com/topics/20086967"
                }, 
                {
                    "tag": "文本分类", 
                    "tagLink": "https://api.zhihu.com/topics/19576060"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/44308448", 
            "userName": "右左瓜子", 
            "userLink": "https://www.zhihu.com/people/6107fd0900473aae4e5ec6824b4fde4a", 
            "upvote": 19, 
            "title": "词向量进阶：ELMo", 
            "content": "<p>封面图来自Allennlp官网，侵删。</p><p>上一篇我们谈到了google的word2vec，word2vec的效果应该是很好了，我曾经再一堆金融语料中夹杂了水浒传的内容，最终输入鲁智深，输出的相似的前50个内容全是水浒传中的名字。但是word2vec有一个很大的缺陷，就是他能把词语的相似度表示的很好，但是却不能去区分他们的区别，比如“好”和“坏”，他们在一定程度上是非常相似的，但是意思又是相反的，有点像概率论中条件相关但是互斥的那种直觉上的矛盾。ELMo应运而生。ELMo的思路其实很简单，再word2vec中我们取出了最后一步的输出作为词向量，ELMo方法认为应该把所有隐藏层的输出取出来，然后整体作为词向量在接下来的文本处理任务中继续训练，相当于给不同的输出层不同的权重。很明显google的word2vec实际上的词向量输出只有一个，不适合，但是google在tensorflow框架中有一个基于biLstm的词向量训练文件，通过biLstm来训练词向量，实际情况是BiLstm训练出来的结果并不如word2vec（特指2013年Tomas Mikolov发表的论文中提到的词嵌入技术），但是BiLstm是有很多层的可以用来做ELMo(Embedding from Language Model)。</p><p>专栏名字叫谈点儿实在的，所以就直接先给出方法吧。<a href=\"https://link.zhihu.com/?target=https%3A//github.com/allenai/bilm-tf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">ELMo大法</a></p><p>直接分词之后把文本内容用Bilstm训练，然后用ELMo中自带的方法把每个词每一层的输出保存，然后再和上一节得到的gensim词向量连接（concate）起来作为实际使用的词向量，然后保存方便后续高级文本处理任务使用。双1080Ti的显卡跑了两天，文本大小600Mb。Bilstm语言模型的perplexity大致在30上下浮动，跟第二篇论文提到的相吻合。</p><p>再上论文地址吧，第一手的信息永远是最宝贵的：<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1802.05365.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Deep contextualized word representations</a></p><p>这篇论文阐述了ELMo的方法，但是具体的lstm并没有太多提及，如果想了解lstm语言模型的可以看看这篇 <a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1602.02410.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Exploring the Limits of Language Modeling</a> </p><p>虽然你不用了解Bilstm直接跑代码按照上面说的方式就能获取效果不错的词向量，但是本着刨根问底的精神我们还是来看看如何用lstm来构建语言模型获取词向量。</p><p>BiLstm来自于循环神经网路Lstm的一种拓展，双向的Lstm，之所以做单向的只能通过前文信息推断下一个词语，双向的可以把前向和后向的词语同时考虑，比如“孙悟空是一只_____，又叫美猴王”，通过前面的语境可能“鸟”、“鸡”、“狗”、“猴”有大致相同的概率，但是考虑到后文，可以让“猴”有更大的概率被选中。代码上的实现很简单，甚至不必改造网络，只需要把输入反转就可以了。</p><p>跟词袋(cbow)模型不同，循环神经网络依次输入的是一个序列而不是一个固定大小的窗口（虽然序列最终也会被截断或者补齐成固定大小），然后把序列中的每一个词语作为中心词最大化中心词与其他词组成句子的概率。</p><p><img src=\"https://www.zhihu.com/equation?tex=p%28t_1%2Ct_2%2C...%2Ct_N%29+%3D+%5Cprod_%7Bk%3D1%7D%5E%7BN%7Dp%28t_k%7Ct_1%2Ct_2%2C...%2Ct_n%29\" alt=\"p(t_1,t_2,...,t_N) = \\prod_{k=1}^{N}p(t_k|t_1,t_2,...,t_n)\" eeimg=\"1\"/> </p><p>在循环神经网络中对语言建模的过程中，每一次都是把序列中前k个值作为输入第k+1个值作为输出，如果使用了双向的rnn则把序列翻转后做同样的操作。所以我们实际上输入词语后做出的预测是预测这个词语的上一个词和下一个词。这有点反直觉，因为我们求得的东西并不是我们想要的二是和他相关但是正如马克思所说“人是一切社会关系的总和”，在非象形文字的世界，单个字几乎不包含任何信息，只有当他处于上下文中的时候他才有意义。就算是象形文字，经过几千年的抽象简化，计算机也很难把这些文字和对应的图像联系起来了。</p><p>下面介绍一下lstm获取词向量的基本的三种结构图。图片来自第二篇论文。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-458d2f261f20dc537d460a44d8cd8e01_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"582\" data-rawheight=\"474\" class=\"origin_image zh-lightbox-thumb\" width=\"582\" data-original=\"https://pic2.zhimg.com/v2-458d2f261f20dc537d460a44d8cd8e01_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;582&#39; height=&#39;474&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"582\" data-rawheight=\"474\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"582\" data-original=\"https://pic2.zhimg.com/v2-458d2f261f20dc537d460a44d8cd8e01_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-458d2f261f20dc537d460a44d8cd8e01_b.jpg\"/></figure><p>图片表示的比较直观了简单解释一下，第一种就是标准的lstm，词嵌入直接用外部的嵌入方式，比如gensim得来的词向量，第二种用cnn做词嵌入，经过lstm后再用相同的cnn解码，第三种在解码的时候又添加了一层lstm网络，来预测下一个词的每个字符，每个模型都有论文来专门做介绍，这里就不展开来了。</p><p>ELMo中的Bilm文件夹包含了训练词向量的程序，如果是以字符级别作为输入，会使用卷积，对应于中文就是使用单个汉字作为输入，如果是词级别的输入就直接用一个均匀分布来随机初始化词向量，对中文来讲使用分词后的文本以词作为输入比较好一点，因为中文词所包含的汉字一般就2到4个，而且大部分时候是两个，做卷积的话基本卷积一次就没有了，相当于一次矩阵相乘。</p><p>所以在Bilstm中，计算概率的公式变成了如下形式：</p><p><img src=\"https://www.zhihu.com/equation?tex=p%28t_1%2Ct_2%2C...%2Ct_N%29+%3D+%5Csum_%7Bk%3D1%7D%5E%7BN%7D%7B%28logp%28t_k%7Ct_1%2C....%2Ct_%7Bk-1%7D%3B%5CTheta_%7BLSTM%7D%5E%7B%5Crightarrow%7D%2C%5CTheta_s%29%2Blogp%28t_k%7Ct_%7Bk%2B1%7D%2C....%2Ct_%7BN%7D%3B%5CTheta_%7BLSTM%7D%5E%7B%5Cleftarrow%7D%2C%5CTheta_s%29%29%7D+\" alt=\"p(t_1,t_2,...,t_N) = \\sum_{k=1}^{N}{(logp(t_k|t_1,....,t_{k-1};\\Theta_{LSTM}^{\\rightarrow},\\Theta_s)+logp(t_k|t_{k+1},....,t_{N};\\Theta_{LSTM}^{\\leftarrow},\\Theta_s))} \" eeimg=\"1\"/> </p><p>三个地方需要注意，之前的相乘经过求对数变成相加，主要是方便反向传播，左右箭头分别代表两个方向的LSTM， <img src=\"https://www.zhihu.com/equation?tex=%5CTheta+\" alt=\"\\Theta \" eeimg=\"1\"/> 代表参数，其中 <img src=\"https://www.zhihu.com/equation?tex=%5CTheta_s\" alt=\"\\Theta_s\" eeimg=\"1\"/> 代表最后的softmax层的参数，所以不管前向还是后向的LSTM，都用的这个参数。</p><p>知道了输入输出以及概率计算表达式，然后通过最大化概率值来做方向传播更新参数即可。</p><p>最后一点，这个模型的评估参数是perplexity，解释一下perplexity，先列出表达式再解释合理性：</p><p><img src=\"https://www.zhihu.com/equation?tex=Pp%28S%29+%3D+p%28t_1t_2...t_N%29%5E%7B-%5Cfrac%7B1%7D%7BN%7D%7D%3D%5Csqrt%5BN%5D%7B%5Cfrac%7B1%7D%7B%5Cprod_%7Bi%3D1%7D%5E%7BN%7Dp%28t_i%7Ct_1t_2...t_%7Bi-1%7D%29%7D%7D\" alt=\"Pp(S) = p(t_1t_2...t_N)^{-\\frac{1}{N}}=\\sqrt[N]{\\frac{1}{\\prod_{i=1}^{N}p(t_i|t_1t_2...t_{i-1})}}\" eeimg=\"1\"/> </p><p>S表示句子，N为句子长度，t_i对应于第i个词。此式的标记符号与上两式保持一致。最直观的Pp(S)随着p(S)增大而减小，我们的训练目的就是最大化p(S)所以这一点很好了解。至于 <img src=\"https://www.zhihu.com/equation?tex=-%5Cfrac%7B1%7D%7BN%7D\" alt=\"-\\frac{1}{N}\" eeimg=\"1\"/> 次方主要是因为因为概率值总小于一，p(s)又是乘法运算，所以对于长句子有失公平性（越乘越小），所以有开N次方的运算，相当于计算每一个词的perplexity（有些翻译为困惑度）。</p><p>一个好的训练结果是perplexity在30到50之间，如果你用ELMo的程序跑出来的结果达不到40以下的话，就要看看自己的训练样本是不是太小了，个人亲测300m的文本是可以达到这个结果。此外EMLo的代码中实现了把每一层输出存储的方法，所以可以很方便的把每一层结果保存下来，论文中还提到在一些NLP任务中在神经网络最后一层，也就是最后softmax的前一个输出包含ELMo的话效果还能提升。</p><p>ELMo的想法很简单，但是实际提升效果确实很明显，首先一点因为词向量的维度增加了，使用ELMo中默认的参数最终得到的词向量的长度是1024加上gensim词向量的维度，而这个增加和单纯的在gensim里规定词向量的长度是不一样的，首先这是两个系统的，其次它保留了Bilstm中每一层的信息，而单纯的在gensim里规定很长很长的词向量，真正有意义的其实还是那么100个左右的维度，再增加其实等同于无意义的随机值填充。其次，把每一层的结果拿出来，在具体的任务如分类中重新加权，相当于保留了上下文信息之后再通过分类的损失函数帮忙把之前在位置、上下文上类似的词语，比如“好”和“坏”他们出现的位置和上下文应该是很接近的，通过分类任务重新对每一层加权来让他们有褒贬之分。</p><p>ELMo的基本使用大致如此，具体使用的时候还有一些数据清洗上的细则官网都给出来了不再赘述，如果你之前没用ELMo，墙裂建议你用一下，不仅是提高准确率，更重要的是在小样本上也能做得很好。</p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "自然语言处理", 
                    "tagLink": "https://api.zhihu.com/topics/19560026"
                }, 
                {
                    "tag": "词向量", 
                    "tagLink": "https://api.zhihu.com/topics/20142325"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/44212494", 
            "userName": "右左瓜子", 
            "userLink": "https://www.zhihu.com/people/6107fd0900473aae4e5ec6824b4fde4a", 
            "upvote": 3, 
            "title": "直接入正题吧，词向量初级", 
            "content": "<p>做自然语言处理一年多了，总想记下什么，所以就来几下点什么。</p><p>废话不多说，目前用的最多的word2vec的方式有两种，一种是谷歌的基于循环神经网络的，另一种，还是谷歌的2013年提出来的词嵌入方法，后来被gensim集成。两种都尝试过，gensim的方法结果更合理，所以此篇着重记录一下gensim的word2vec的方法。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>代码就不贴了，gensim的word2vec代码基本上是傻瓜式的，给个链接<a href=\"https://link.zhihu.com/?target=https%3A//radimrehurek.com/gensim/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">gensim大法</a>。训练300m大小的文本的在我的八代i7笔记本上大概要二十分钟。主要记一下他的相关原理。考虑到专栏名字叫来点实际的，如果不想看原理想直接用的直接去gensim的页面调用相关方法即可。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>论文地址：<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1310.4546.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Distributed Representations of Words and Phrases and their Compositionality</a></p><p class=\"ztext-empty-paragraph\"><br/></p><ol><li>概率语言模型</li></ol><p>先说说概率语言模型，概率语言模型目前为止就是在做一件事，以给出的文本为样本，去计算生产实践中遇到的词语组成一个句子的概率，如果给出一段文字，概率语言模型给出概率结果很高，而这段文字恰好是我们生产实践中经常遇到的句子，同时，给出乱七八糟的文字，模型给出很低的概率，那么我们就认为这个模型是好的。具体的数学公式</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7BT%7D%5Csum_%7Bt%3D1%7D%5E%7BT%7D%7B%5Csum_%7B%E2%88%92c%E2%89%A4j%E2%89%A4c%2Cj%5Cne0%7D%5E%7B%7D%7Blog+p%28w_%7Bt%2Bj%7D%7Cw_%7Bt%7D%29%7D%7D\" alt=\"\\frac{1}{T}\\sum_{t=1}^{T}{\\sum_{−c≤j≤c,j\\ne0}^{}{log p(w_{t+j}|w_{t})}}\" eeimg=\"1\"/> </p><p>T是句子长度，概率计算会做两次求和，即对句子中每一个词语 <img src=\"https://www.zhihu.com/equation?tex=w_%7Bt%7D\" alt=\"w_{t}\" eeimg=\"1\"/> 求<img src=\"https://www.zhihu.com/equation?tex=w_%7Bt%7D\" alt=\"w_{t}\" eeimg=\"1\"/> 和窗口大小为c的范围内的上下文根据概率语言模型计算得出的概率值。这一部分不多数，很好理解。问题的关键在于我们知道要建立概率语言模型，但是如何建立，也就是p的计算公式是什么。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>最直观的方法是使用softmax回归。在贴公式之前先思考一下我们需要一个什么样的方程。首先最明显的，对于 <img src=\"https://www.zhihu.com/equation?tex=w_%7Bt%2Bj%7D\" alt=\"w_{t+j}\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=w_%7Bt%7D\" alt=\"w_{t}\" eeimg=\"1\"/> 两个词来讲，一个好的概率公式应当是当他们同时在很近的位置出现在文档中时，两个词的向量经过概率公式计算后，概率值应该很大，反之亦然。但是不能单凭这一点就让这个概率值去趋近于1，因为还要考虑w_(t+j)（因为打公式比较麻烦，所以后面不影响阅读的去情况下我都用&#34;_&#34;去代表下标）跟文档中其他词的联系，最简单的例子“马云”和“阿里巴巴”关系很大，但是“淘宝”和“阿里巴巴”的关系也很大，所以我们要综合全文档词语之间的关系来构建这个概率公式。可以继续去思考他们的正反比关系来更进一步的自己构建出一个大致的公式，这里就不细说了，直接把公式贴出来。</p><p><img src=\"https://www.zhihu.com/equation?tex=p%28w_O%7Cw_I%29+%3D+%5Cfrac%7Bexp%28v_%7Bw_O%7D%5E%7B%27%7D%5Ctop+v_%7Bw_I%7D%29%7D%7B%5Csum_%7Bw%3D1%7D%5E%7BW%7D%7Bexp%28v_%7Bw%7D%5E%7B%27%7D%5Ctop+v_%7Bw_I%7D%29%7D%7D\" alt=\"p(w_O|w_I) = \\frac{exp(v_{w_O}^{&#39;}\\top v_{w_I})}{\\sum_{w=1}^{W}{exp(v_{w}^{&#39;}\\top v_{w_I})}}\" eeimg=\"1\"/> </p><p>有了前面的思考，这个公式应该比较好理解了，大致的说一下v代表词向量，v&#39;表示输出词向量，这个公式可以保证同时出现的时候概率大，但是又综合考虑了和文档中其他词与组合的情况。并且还能抑制不常用组合词语的概率值。公式在理论上是可行的，但是在实际计算中每一次做反向传播的时候都要遍历整个文档，文档的大小经常又会很大（小文档的训练效果也不好），所以0计算开销是很大的。所以文中提出了改良方法，即 Hierarchical Softmax。</p><ul><li><b>Hierarchical Softmax</b></li></ul><p>Hierarchical Softmax用文档构建了一个词语二叉树。这是一个比较常见的思路，就是把穷举变成递归来节省内存和时间开销。所以现在的问题时如何选择和构建二叉树，这篇论文选用了霍夫曼二叉树（a binary Huffman tree），旨在进一步节省时间开销。在介绍霍夫曼二叉树的之前我们还是做一下思考，我们需要一个什么样的树。理论上来讲，当我们把一个序列从遍历变为二叉树的之后，计算量会缩减到 <img src=\"https://www.zhihu.com/equation?tex=log_%7B2%7DN\" alt=\"log_{2}N\" eeimg=\"1\"/> （N为序列长度）,这是一个理论值，但是实际上我们能不能通过优化二叉树的结构来进一步缩短时间呢？这就是霍夫曼二叉树要做的。如何缩短时间，比如在《西游记》中，“孙悟空”这个词应该是会被经常检索到的，而“马云”这个词被检索到的次数应该会比“孙悟空”小很多，所以相比较而言，我们更希望“孙悟空”在更靠近根节点的位置，因为我们是从树根开始检索的，而“马云”这个词应该在更靠近叶节点的位置，这是我们想要的一颗，可以有效节省时间成本的二叉树。感性回归理性，为什么我们会认为“孙悟空”会比“马云“的检索次数多，词频。这样霍夫曼二叉树的结果就出来，贴一段维基的内容</p><blockquote>        霍夫曼树又称最优二叉树，是一种带权路径长度最短的<a href=\"https://link.zhihu.com/?target=https%3A//zh.wikipedia.org/wiki/%25E4%25BA%258C%25E5%258F%2589%25E6%25A0%2591\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">二叉树</a>。所谓树的带权路径长度，就是树中所有的叶结点的权值乘上其到根结点的路径长度（若根结点为0层，叶结点到根结点的路径长度为叶结点的层数）。树的路径长度是从树根到每一结点的路径长度之和，记为WPL=（W1*L1+W2*L2+W3*L3+...+Wn*Ln），N个权值Wi（i=1,2,...n）构成一棵有N个叶结点的二叉树，相应的叶结点的路径长度为Li（i=1,2,...n）。可以证明霍夫曼树的WPL是最小的。</blockquote><p>构建霍夫曼树</p><blockquote>⒈将每个英文字母依照出现频率由小排到大，最小在左。<br/>⒉每个字母都代表一个终端节点（叶节点），比较<b>F.O.R.G.E.T</b>六个字母中每个字母的出现频率，将最小的两个字母频率相加合成一个新的节点。如<i>Fig.2</i>所示，发现<b>F</b>与<b>O</b>的频率最小，故相加2+3=5。<br/>⒊比较<b>5.R.G.E.T</b>，发现<b>R</b>与<b>G</b>的频率最小，故相加4+4=8。<br/>⒋比较<b>5.8.E.T</b>，发现<b>5</b>与<b>E</b>的频率最小，故相加5+5=10。<br/>⒌比较<b>8.10.T</b>，发现<b>8</b>与<b>T</b>的频率最小，故相加8+7=15。<br/>⒍最后剩<b>10.15</b>，没有可以比较的对象，相加10+15=25。<br/>最后产生的树状图就是霍夫曼树</blockquote><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-576a5a6ede1a52422876de1a615bb6e5_b.gif\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"358\" data-rawheight=\"421\" data-thumbnail=\"https://pic2.zhimg.com/v2-576a5a6ede1a52422876de1a615bb6e5_b.jpg\" class=\"content_image\" width=\"358\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;358&#39; height=&#39;421&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"358\" data-rawheight=\"421\" data-thumbnail=\"https://pic2.zhimg.com/v2-576a5a6ede1a52422876de1a615bb6e5_b.jpg\" class=\"content_image lazy\" width=\"358\" data-actualsrc=\"https://pic2.zhimg.com/v2-576a5a6ede1a52422876de1a615bb6e5_b.gif\"/></figure><p>英文字母对应到我们的实际情况就是词语，结合图看应该很明显了，霍夫曼树不再赘述。我们有了树，可是我们依据什么来在这个树里边走呢？再看看这个树，二叉树，每一步都是要做一个二分类决策，等等，二分类决策，是不是就想起来我们的老朋友softmax了？是的，因为是二叉树所以每一步都可以用一个softmax来训练，这样就能再整个树上做反向传播。softmax不在赘述，至此，前向传播和反向传播都打通了。最终的公式是这样的：                         <img src=\"https://www.zhihu.com/equation?tex=p%28w%7Cw_%7BI%7D%29+%3D+%5Cprod_%7Bj%3D1%7D%5E%7BL%28w%29-1%7D%5Csigma%28%5B%5Bn%28w%2Cj%2B1%29+%3D+ch%28n%28w%2Cj%29%29%5D%5D%5Ccdot+v_%7Bn%28w%2C+j%29%7D%5E%7B%27%7D%5Ctop+v_%7Bw_I%7D%29\" alt=\"p(w|w_{I}) = \\prod_{j=1}^{L(w)-1}\\sigma([[n(w,j+1) = ch(n(w,j))]]\\cdot v_{n(w, j)}^{&#39;}\\top v_{w_I})\" eeimg=\"1\"/> </p><p>解释一下这个公式，[[ ]]在文中定义为一个运算符，实际上是双线括号，因为打不出来所以我用双中括号代替。里边的条件成立则值为1，否则-1。 <img src=\"https://www.zhihu.com/equation?tex=%5Cdelta\" alt=\"\\delta\" eeimg=\"1\"/> 即sigmoid函数。n(w, j) 代表从根节点到w节点路径上的第j个节点，这样根节点就是n(w,1), w所在节点就是n(w, L(w))，L(w)就是这个路径的长度了，ch(n)就是n的一个选定的叶子节点，如何选定是随机的，可以人为设置为左边的子节点，无论怎么选择，只需要注意一点，训练过程和计算过程要用同一规则，为了方便描述，本文中把左子节点作为正例也就是1，v即词向量，这些在训练之初即被初始化为指定长度的向量，然后根据反向传播更新，至于如何初始化的贴两段gensim的代码。</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"k\">def</span> <span class=\"nf\">reset_weights</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">hs</span><span class=\"p\">,</span> <span class=\"n\">negative</span><span class=\"p\">,</span> <span class=\"n\">wv</span><span class=\"p\">):</span>\n        <span class=\"s2\">&#34;&#34;&#34;Reset all projection weights to an initial (untrained) state, but keep the existing vocabulary.&#34;&#34;&#34;</span>\n        <span class=\"n\">logger</span><span class=\"o\">.</span><span class=\"n\">info</span><span class=\"p\">(</span><span class=\"s2\">&#34;resetting layer weights&#34;</span><span class=\"p\">)</span>\n        <span class=\"c1\"># 根据词表长度和初始的词向量维度参数构建词向量</span>\n        <span class=\"n\">wv</span><span class=\"o\">.</span><span class=\"n\">vectors</span> <span class=\"o\">=</span> <span class=\"n\">empty</span><span class=\"p\">((</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">wv</span><span class=\"o\">.</span><span class=\"n\">vocab</span><span class=\"p\">),</span> <span class=\"n\">wv</span><span class=\"o\">.</span><span class=\"n\">vector_size</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">REAL</span><span class=\"p\">)</span>\n        <span class=\"c1\"># randomize weights vector by vector, rather than materializing a huge random matrix in RAM at once</span>\n        <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"n\">xrange</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">wv</span><span class=\"o\">.</span><span class=\"n\">vocab</span><span class=\"p\">)):</span>\n            <span class=\"c1\"># construct deterministic seed from word AND seed argument</span>\n            <span class=\"c1\"># 为每个词随机生成向量，除了唯一性没有其他要求</span>\n            <span class=\"n\">wv</span><span class=\"o\">.</span><span class=\"n\">vectors</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">seeded_vector</span><span class=\"p\">(</span><span class=\"n\">wv</span><span class=\"o\">.</span><span class=\"n\">index2word</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">seed</span><span class=\"p\">),</span> <span class=\"n\">wv</span><span class=\"o\">.</span><span class=\"n\">vector_size</span><span class=\"p\">)</span>\n        <span class=\"k\">if</span> <span class=\"n\">hs</span><span class=\"p\">:</span>\n            <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">syn1</span> <span class=\"o\">=</span> <span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">wv</span><span class=\"o\">.</span><span class=\"n\">vocab</span><span class=\"p\">),</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">layer1_size</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">REAL</span><span class=\"p\">)</span>\n        <span class=\"k\">if</span> <span class=\"n\">negative</span><span class=\"p\">:</span>\n            <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">syn1neg</span> <span class=\"o\">=</span> <span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">wv</span><span class=\"o\">.</span><span class=\"n\">vocab</span><span class=\"p\">),</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">layer1_size</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">REAL</span><span class=\"p\">)</span>\n        <span class=\"n\">wv</span><span class=\"o\">.</span><span class=\"n\">vectors_norm</span> <span class=\"o\">=</span> <span class=\"kc\">None</span>\n\n        <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">vectors_lockf</span> <span class=\"o\">=</span> <span class=\"n\">ones</span><span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">wv</span><span class=\"o\">.</span><span class=\"n\">vocab</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">REAL</span><span class=\"p\">)</span>  <span class=\"c1\"># zeros suppress learning</span></code></pre></div><p>需要注意一下 <img src=\"https://www.zhihu.com/equation?tex=%5B%5Bn%28w%2Cj%2B1%29+%3D+ch%28n%28w%2Cj%29%29%5D%5D\" alt=\"[[n(w,j+1) = ch(n(w,j))]]\" eeimg=\"1\"/>，这个式子是这么来的，左边为正例，对应于sigmoid函数也就是说如果走的是左边，那么概率值就是 <img src=\"https://www.zhihu.com/equation?tex=%5Csigma%28+v_%7Bn%28w%2C+j%29%7D%5E%7B%27%7D%5Ctop+v_%7Bw_I%7D%29\" alt=\"\\sigma( v_{n(w, j)}^{&#39;}\\top v_{w_I})\" eeimg=\"1\"/> ,走右边即为 <img src=\"https://www.zhihu.com/equation?tex=1-%5Csigma%28+v_%7Bn%28w%2C+j%29%7D%5E%7B%27%7D%5Ctop+v_%7Bw_I%7D%29\" alt=\"1-\\sigma( v_{n(w, j)}^{&#39;}\\top v_{w_I})\" eeimg=\"1\"/> = <img src=\"https://www.zhihu.com/equation?tex=%5Csigma%28-+v_%7Bn%28w%2C+j%29%7D%5E%7B%27%7D%5Ctop+v_%7Bw_I%7D%29\" alt=\"\\sigma(- v_{n(w, j)}^{&#39;}\\top v_{w_I})\" eeimg=\"1\"/> ，用一个式子表示即为可得到前面的式子。之前我比较疑惑这样的式子很容易引起梯度消失，导致不能反向传播，后来想明白了，因为符号在每一条路径上都是固定的，所以知道了反向传播的路径，每一个式子的形式就固定了，就不存在梯度消失的问题了。</p><p>好了，最后总结一下，Hierarchical Softmax的核心是霍夫曼二叉树，霍夫曼二叉树解决了两个问题，一个是把对整个文档的遍历转换成对树的某一个分支的遍历，一个是通过霍夫曼树减少了树的遍历次数，霍夫曼树想了解更多的可以去单独了解一下霍夫曼编码，《硅谷》里边儿男主的压缩算法好像就用到了这种技术。霍夫曼树有一个问题在于永远不能达到比上下文词语中词频更低的词语（当低频词存在于上下文中时是可以到达的），从实际训练结果看，这种损失是微乎其微的。</p><p class=\"ztext-empty-paragraph\"><br/></p><ul><li><b>Negative Sampling</b></li></ul><p>就好像工程师们对跑车速度永远都不满足，大牛们对程序的运算速度也是永远都不满足，总是在追求比快更快，所以词向量界的Negative Sampling诞生了，原因很简单，上一节提到的霍夫曼树在遇到低频词的时候要走很久才能走完整个路径。</p><p>相比于Hierarchical Softmax，Negative Sampling不管在理论上还是实现上都要简单很多，没有繁琐的树结构。他的理论很简单，在获取窗口和中心词后（实际上这是cbow模型的一部分，cbow模型）将中心词和上下文作为正例，然后通过采样替换中心词，上下文和新的中心词组成负例，然后去最大化正例的概率的同时最小化负例的概率。公式如下</p><p><img src=\"https://www.zhihu.com/equation?tex=log%5Csigma%28v_%7Bw_%7BO%7D%7D%5E%7B%27%7D%5Ctop+v_%7Bw_%7BI%7D%7D%29%2B%5Csum_%7Bi%3D1%7D%5E%7Bk%7D%7BE_%7Bw_i%5Csim+P_%7Bn%7D%28w%29%7D%5Blog%5Csigma%28-v_%7Bw_%7Bi%7D%7D%5E%7B%27%7D%5Ctop+v_%7Bw_%7BI%7D%7D%29%5D%7D\" alt=\"log\\sigma(v_{w_{O}}^{&#39;}\\top v_{w_{I}})+\\sum_{i=1}^{k}{E_{w_i\\sim P_{n}(w)}[log\\sigma(-v_{w_{i}}^{&#39;}\\top v_{w_{I}})]}\" eeimg=\"1\"/> </p><p>公式比较好理解，重点在于后半部分，应该可以很容易的看出来公式是一个正例和k个负例对数似然的和，w_i来自于负采样，所以关键点在于如何负采样。</p><p>文章中并没有提到具体如何采样，只是提到了一个调试出来最好的参数值是3/4，刚好在阅读gensim的源码的时候也看到了一个3/4的超参数，然后顺藤摸瓜莫出来了一些。在具体的实现上，每一个词都赋给了一个被负采样采样的概率，具体的概率公式是</p><p><img src=\"https://www.zhihu.com/equation?tex=P%28w_i%29%3D%5Cfrac%7Bf%28w_i%29%5E%7B%5Cfrac%7B3%7D%7B4%7D%7D%7D%7B%5Csum_%7Bj%3D1%7D%5E%7Bn%7D%7Bf%28w_j%29%5E%7B%5Cfrac%7B3%7D%7B4%7D%7D%7D%7D\" alt=\"P(w_i)=\\frac{f(w_i)^{\\frac{3}{4}}}{\\sum_{j=1}^{n}{f(w_j)^{\\frac{3}{4}}}}\" eeimg=\"1\"/> </p><p>f(w_i)为w_i在文档中出现的频率，3/4即作者调出来的认为最佳的参数。每个w_i以P(w_i)的频率被选中。</p><p>至此算法主体已经介绍完了，gensim对于两种方法均有实现，通过传入不同的参数来选择不同的方法，最后就是关于停用词的处理。</p><ul><li><b>Subsampling of Frequent Words</b></li></ul><p>处理方式也比较简单，每个词以一定的概率被丢弃，具体的概率公式</p><p><img src=\"https://www.zhihu.com/equation?tex=P%28w_i%29+%3D+1+-+%5Csqrt%7B%5Cfrac%7Bt%7D%7Bf%28w_i%29%7D%7D\" alt=\"P(w_i) = 1 - \\sqrt{\\frac{t}{f(w_i)}}\" eeimg=\"1\"/> </p><p>f(w_i)依旧是词频，t是一个预设的阈值，作者提到他们使用的参数是 <img src=\"https://www.zhihu.com/equation?tex=10%5E%7B-5%7D\" alt=\"10^{-5}\" eeimg=\"1\"/> ,这个参数是gensim中的默认值，也是可调的。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>这篇论文已经过去五年了，直到2017年之前词向量技术一直没有突破性的进展，直到去年allennlp实验室推出了ELMo，下一篇将会讲到</p>", 
            "topic": [
                {
                    "tag": "word2vec", 
                    "tagLink": "https://api.zhihu.com/topics/19886836"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "词向量", 
                    "tagLink": "https://api.zhihu.com/topics/20142325"
                }
            ], 
            "comments": [
                {
                    "userName": "李雪峰", 
                    "userLink": "https://www.zhihu.com/people/81e78425809aa7b4713e08b22d0ede6a", 
                    "content": "<p>楼主写的不错，已经工作了嘛</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "右左瓜子", 
                            "userLink": "https://www.zhihu.com/people/6107fd0900473aae4e5ec6824b4fde4a", 
                            "content": "是的", 
                            "likes": 0, 
                            "replyToAuthor": "李雪峰"
                        }
                    ]
                }
            ]
        }
    ], 
    "url": "https://zhuanlan.zhihu.com/c_1022420809108713472"
}
