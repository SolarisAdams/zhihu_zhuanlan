{
    "title": "我的机器学习笔记", 
    "description": "分享一些最新在看的论文以及正在学习的算法。文章均为本人原创，严禁抄袭，如需转载请联系本人。", 
    "followers": [
        "https://www.zhihu.com/people/edward-77-20", 
        "https://www.zhihu.com/people/chen-wei-94-11", 
        "https://www.zhihu.com/people/liu-yang-guang-zi", 
        "https://www.zhihu.com/people/hanbei-wei", 
        "https://www.zhihu.com/people/guo-yuan-tao-22", 
        "https://www.zhihu.com/people/cao-ji-49-42", 
        "https://www.zhihu.com/people/xi-huo-69-9", 
        "https://www.zhihu.com/people/wei-yuan-88-25", 
        "https://www.zhihu.com/people/gu-yu-71-65", 
        "https://www.zhihu.com/people/ren-hui-kang", 
        "https://www.zhihu.com/people/ke-eric", 
        "https://www.zhihu.com/people/ding-lao-guai-96", 
        "https://www.zhihu.com/people/tang-bao-59-49", 
        "https://www.zhihu.com/people/alahlll", 
        "https://www.zhihu.com/people/hu-tao-58", 
        "https://www.zhihu.com/people/liao-xuan-79-79", 
        "https://www.zhihu.com/people/li-lin-da-4-62", 
        "https://www.zhihu.com/people/liu-wei-ming-37-41", 
        "https://www.zhihu.com/people/shawn-young-4", 
        "https://www.zhihu.com/people/ma-chen-14-42", 
        "https://www.zhihu.com/people/da-hai-de-tong-kong-51", 
        "https://www.zhihu.com/people/litn2008", 
        "https://www.zhihu.com/people/Certified-Stimulater", 
        "https://www.zhihu.com/people/abrams-25", 
        "https://www.zhihu.com/people/miao-dong-long", 
        "https://www.zhihu.com/people/li-qian-yong-60", 
        "https://www.zhihu.com/people/duan-ya-jun", 
        "https://www.zhihu.com/people/lingche-24", 
        "https://www.zhihu.com/people/xiao-fang-yao-jia-you", 
        "https://www.zhihu.com/people/xin-ling-ji-shi-49", 
        "https://www.zhihu.com/people/xiao-sa-di-fei", 
        "https://www.zhihu.com/people/quxiaofeng", 
        "https://www.zhihu.com/people/mel-tor", 
        "https://www.zhihu.com/people/mao-lan-mao-lu", 
        "https://www.zhihu.com/people/tian-cai-jian-sheng", 
        "https://www.zhihu.com/people/rachel-yeung", 
        "https://www.zhihu.com/people/txa0515", 
        "https://www.zhihu.com/people/wu-li-xi-78", 
        "https://www.zhihu.com/people/zhang-hang-70", 
        "https://www.zhihu.com/people/bai-yan-xiao-56", 
        "https://www.zhihu.com/people/pei-hao-lei", 
        "https://www.zhihu.com/people/ming-zhi-4-69", 
        "https://www.zhihu.com/people/liu-yu-xuan-3-67", 
        "https://www.zhihu.com/people/playmaker-64", 
        "https://www.zhihu.com/people/cao-guo-wei-69", 
        "https://www.zhihu.com/people/shu-yu-mu-ge-31", 
        "https://www.zhihu.com/people/liu-lei-83-67-15", 
        "https://www.zhihu.com/people/chi-xian-sheng-25", 
        "https://www.zhihu.com/people/zzx-47-7", 
        "https://www.zhihu.com/people/bian-ze-ying-38", 
        "https://www.zhihu.com/people/niu-zhuang-qun", 
        "https://www.zhihu.com/people/sheng-lai-bo-wei-ci", 
        "https://www.zhihu.com/people/fang-gao-97", 
        "https://www.zhihu.com/people/xiao-jj-4", 
        "https://www.zhihu.com/people/wang-ruo-cun", 
        "https://www.zhihu.com/people/bang-di-70", 
        "https://www.zhihu.com/people/sushupro", 
        "https://www.zhihu.com/people/li-peng-41-64", 
        "https://www.zhihu.com/people/dong-sheng-99-63", 
        "https://www.zhihu.com/people/yunzhongke", 
        "https://www.zhihu.com/people/luo-wei-bin-35", 
        "https://www.zhihu.com/people/mrtian-11-90", 
        "https://www.zhihu.com/people/augustus-74", 
        "https://www.zhihu.com/people/rainbow_jjh", 
        "https://www.zhihu.com/people/long-yixiang", 
        "https://www.zhihu.com/people/liu-bo-11-42-89", 
        "https://www.zhihu.com/people/myluo", 
        "https://www.zhihu.com/people/ding-ding-14-48-13", 
        "https://www.zhihu.com/people/an-du-65", 
        "https://www.zhihu.com/people/mou-shan-lei-51", 
        "https://www.zhihu.com/people/bian-shu-qing", 
        "https://www.zhihu.com/people/lihongyuers", 
        "https://www.zhihu.com/people/yooyoo2004", 
        "https://www.zhihu.com/people/zhang-bai-zheng", 
        "https://www.zhihu.com/people/jason-ding-28", 
        "https://www.zhihu.com/people/hu-lin-2", 
        "https://www.zhihu.com/people/li-xiang-73-34", 
        "https://www.zhihu.com/people/li-yang-zhong-86", 
        "https://www.zhihu.com/people/liu-lin-qi-78", 
        "https://www.zhihu.com/people/begeekmyfriend", 
        "https://www.zhihu.com/people/hong-alex", 
        "https://www.zhihu.com/people/go-ro-87", 
        "https://www.zhihu.com/people/duan-xing-99", 
        "https://www.zhihu.com/people/nan-yang-wu-hao", 
        "https://www.zhihu.com/people/ai-meng-meng-de-guai-qiao-ge", 
        "https://www.zhihu.com/people/mr-lin-82-68", 
        "https://www.zhihu.com/people/pang-yu-long", 
        "https://www.zhihu.com/people/kyle-chen-15", 
        "https://www.zhihu.com/people/xi-ze-wen", 
        "https://www.zhihu.com/people/fan-fan-26-41", 
        "https://www.zhihu.com/people/qian-qing-he", 
        "https://www.zhihu.com/people/alex-48-90", 
        "https://www.zhihu.com/people/xue-gao-24-65", 
        "https://www.zhihu.com/people/qiu-xian-sheng-69-64", 
        "https://www.zhihu.com/people/chevson", 
        "https://www.zhihu.com/people/zou-xiao-dong-59", 
        "https://www.zhihu.com/people/dong-dong-46-64", 
        "https://www.zhihu.com/people/1111112222", 
        "https://www.zhihu.com/people/zhong-er-cheng-xu-yuan", 
        "https://www.zhihu.com/people/chen-bo-86-42", 
        "https://www.zhihu.com/people/wei-zhang-52-25", 
        "https://www.zhihu.com/people/zhilingth", 
        "https://www.zhihu.com/people/gogo-30-43", 
        "https://www.zhihu.com/people/ykp-41", 
        "https://www.zhihu.com/people/frcskoh", 
        "https://www.zhihu.com/people/yang-ye-79", 
        "https://www.zhihu.com/people/liu-fei-94-95", 
        "https://www.zhihu.com/people/shuai-qi-de-da-ge-ge-5", 
        "https://www.zhihu.com/people/alex-45-44-1", 
        "https://www.zhihu.com/people/zhan-xun-lin", 
        "https://www.zhihu.com/people/cheng-guang-lei-40", 
        "https://www.zhihu.com/people/meng-bi-you-si-53", 
        "https://www.zhihu.com/people/sirice", 
        "https://www.zhihu.com/people/vincent-chong-40", 
        "https://www.zhihu.com/people/guang1991", 
        "https://www.zhihu.com/people/yue-liang-yu-liu-bian-shi-29-61", 
        "https://www.zhihu.com/people/windisrising", 
        "https://www.zhihu.com/people/qinlibo_nlp", 
        "https://www.zhihu.com/people/hwanji", 
        "https://www.zhihu.com/people/4recommend", 
        "https://www.zhihu.com/people/dgjk1010", 
        "https://www.zhihu.com/people/da-qi-85-21", 
        "https://www.zhihu.com/people/tian-ma-72xiao-shi", 
        "https://www.zhihu.com/people/wht-buaa", 
        "https://www.zhihu.com/people/niceworld-49-6", 
        "https://www.zhihu.com/people/zhang-jia-yi-6-82", 
        "https://www.zhihu.com/people/wang-lang-96-64", 
        "https://www.zhihu.com/people/zhairui", 
        "https://www.zhihu.com/people/li-sheng-38-23", 
        "https://www.zhihu.com/people/ni-wei-tai-yang", 
        "https://www.zhihu.com/people/k_liu007", 
        "https://www.zhihu.com/people/xpxpx", 
        "https://www.zhihu.com/people/boney-62", 
        "https://www.zhihu.com/people/yu-ji-shi-17", 
        "https://www.zhihu.com/people/an-ran-57", 
        "https://www.zhihu.com/people/wx31eac9e634f9147b", 
        "https://www.zhihu.com/people/sun-peng-fei-45", 
        "https://www.zhihu.com/people/summer-82-63-57", 
        "https://www.zhihu.com/people/tc-yi", 
        "https://www.zhihu.com/people/ni-hui-51-23", 
        "https://www.zhihu.com/people/wu-ye-nan-30", 
        "https://www.zhihu.com/people/she-liang", 
        "https://www.zhihu.com/people/ltt-60-73", 
        "https://www.zhihu.com/people/constantine-6-2", 
        "https://www.zhihu.com/people/zipoki", 
        "https://www.zhihu.com/people/lzwujun-78", 
        "https://www.zhihu.com/people/tkmeng-pie-huan-zai"
    ], 
    "article": [
        {
            "url": "https://zhuanlan.zhihu.com/p/85768094", 
            "userName": "Ivan Yan", 
            "userLink": "https://www.zhihu.com/people/3677a61f41a4d1cd630d63e270524c07", 
            "upvote": 196, 
            "title": "图神经网络：The Graph Neural Network Model", 
            "content": "<p>图神经网络（GNN）最近很火，很多自然语言的论文都用上了GNN网络。正好凑空学习一下关于GNN网络的知识。对于不熟悉GNN的朋友，可以先看[1]，沿着图神经网络的历史脉络，介绍了经典的图神经网络模型。这篇文章的主要目的是结合python代码来讲解Graph Neural Network Model如何实现，代码主要参考[2]。</p><h2>1、论文内容简介</h2><p>图神经网络最早的概念应该起源于以下两篇论文。</p><a href=\"https://link.zhihu.com/?target=https%3A//link.springer.com/chapter/10.1007/978-3-540-27868-9_4\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-dad5c7906f642df4a7352bf2939bdc38_120x160.jpg\" data-image-width=\"153\" data-image-height=\"232\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Graphical-Based Learning Environments for Pattern Recognition</a><a href=\"https://link.zhihu.com/?target=https%3A//ieeexplore.ieee.org/document/4700287\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">The Graph Neural Network Model</a><p>09年这篇论文对04年这篇进行了补充，内容大致差不多。如果要阅读原文的朋友，直接读第二篇就可以了。</p><p>神经网络最常见的应用领域就是图片，而图神经网络主要用于处理图论中的图，如下图所示，图片引用自[3]：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-fabefb9beb674fd7bc3241481487d566_b.jpg\" data-size=\"normal\" data-rawwidth=\"2782\" data-rawheight=\"1426\" class=\"origin_image zh-lightbox-thumb\" width=\"2782\" data-original=\"https://pic3.zhimg.com/v2-fabefb9beb674fd7bc3241481487d566_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2782&#39; height=&#39;1426&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"2782\" data-rawheight=\"1426\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2782\" data-original=\"https://pic3.zhimg.com/v2-fabefb9beb674fd7bc3241481487d566_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-fabefb9beb674fd7bc3241481487d566_b.jpg\"/><figcaption>图1 左图是传统神经网络可以处理的图片类型，符合欧式空间的要求。右图是图论中的图，非欧式空间，无法直接使用传统的神经网络进行处理。</figcaption></figure><p>之前的方法都是先将图预处理成能被神经网络处理的格式，例如将图表示成结点序列，但这会丢失很多拓扑信息。GNN可以直接处理图，将图或者图中的结点映射为向量。文章的具体内容不细讲了，可以参考原文，大致算法流程如下[5]：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-172afe24dc85337cb100132dd89f230e_b.jpg\" data-size=\"normal\" data-rawwidth=\"1652\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb\" width=\"1652\" data-original=\"https://pic3.zhimg.com/v2-172afe24dc85337cb100132dd89f230e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1652&#39; height=&#39;606&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1652\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1652\" data-original=\"https://pic3.zhimg.com/v2-172afe24dc85337cb100132dd89f230e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-172afe24dc85337cb100132dd89f230e_b.jpg\"/><figcaption>图2 GNN算法流程</figcaption></figure><p>从图中可以看出，算法主要分成两个步骤：forward, backward。跟传统神经网络的forward、backward类似。forward计算结果，backward求梯度，用于网络训练。</p><p>Forward过程，其实是做了这样一件事情：如下图[5]所示，这是一个环烃化合物的分子结构，图中的结点表示原子，边表示原子键，需要判断其是否有害。图中每个结点都包含了一系列特征（论文中写成label，我感觉翻译成特征更合适），例如每个原子的类型、原子自身的属性（Atom properties）、化合物的一些特征（Global properties）。这里将supervised node的表示作为整个图的表示。但是，supervised node并不包含整个图的信息。最简单的方式就是将所有的结点的特征相加，作为整个图的表示，但是这样并没有考虑图的拓扑信息。我们可以假设结点的信息会沿着边传递给相邻的结点。这样，在迭代t次之后，到达稳定状态之后，supervised node就包含了整个图的信息。这里，只要构建的 <img src=\"https://www.zhihu.com/equation?tex=F_w\" alt=\"F_w\" eeimg=\"1\"/> 满足固定点理论的要求，结点的状态最终会达到一个稳定状态。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-c5bc9dc8f7f18bd359d85f23ce122dc9_b.jpg\" data-size=\"normal\" data-rawwidth=\"565\" data-rawheight=\"374\" class=\"origin_image zh-lightbox-thumb\" width=\"565\" data-original=\"https://pic2.zhimg.com/v2-c5bc9dc8f7f18bd359d85f23ce122dc9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;565&#39; height=&#39;374&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"565\" data-rawheight=\"374\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"565\" data-original=\"https://pic2.zhimg.com/v2-c5bc9dc8f7f18bd359d85f23ce122dc9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-c5bc9dc8f7f18bd359d85f23ce122dc9_b.jpg\"/><figcaption>图3 GNN举例：判断环烃化合物是否有害</figcaption></figure><p>Forward的具体计算过程如下图[4]所示， <img src=\"https://www.zhihu.com/equation?tex=F_w\" alt=\"F_w\" eeimg=\"1\"/> 根据当前父结点，相邻子结点，相连的边的特征及状态对当前父结点的状态进行更新，然后迭代t次，达到稳定状态之后（这里的判断条件是结点状态的变化小于 <img src=\"https://www.zhihu.com/equation?tex=%5Cvarepsilon_f\" alt=\"\\varepsilon_f\" eeimg=\"1\"/> ）， <img src=\"https://www.zhihu.com/equation?tex=G_w\" alt=\"G_w\" eeimg=\"1\"/> 根据当前结点的状态，输出结点的output。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-90945baf5e09e78c26df1916c71ed772_b.jpg\" data-size=\"normal\" data-rawwidth=\"1151\" data-rawheight=\"500\" class=\"origin_image zh-lightbox-thumb\" width=\"1151\" data-original=\"https://pic3.zhimg.com/v2-90945baf5e09e78c26df1916c71ed772_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1151&#39; height=&#39;500&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1151\" data-rawheight=\"500\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1151\" data-original=\"https://pic3.zhimg.com/v2-90945baf5e09e78c26df1916c71ed772_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-90945baf5e09e78c26df1916c71ed772_b.jpg\"/><figcaption>图4 图以及它对应的编码网络</figcaption></figure><p>从上述的描述中，我们可以看出这个过程和RNN的过程非常相似，因此这个过程也可以表示成RNN的形式，如下图所示[4]：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-7030b07d89aeaeda2429bdf04fba31fc_b.jpg\" data-size=\"normal\" data-rawwidth=\"1109\" data-rawheight=\"419\" class=\"origin_image zh-lightbox-thumb\" width=\"1109\" data-original=\"https://pic1.zhimg.com/v2-7030b07d89aeaeda2429bdf04fba31fc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1109&#39; height=&#39;419&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1109\" data-rawheight=\"419\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1109\" data-original=\"https://pic1.zhimg.com/v2-7030b07d89aeaeda2429bdf04fba31fc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-7030b07d89aeaeda2429bdf04fba31fc_b.jpg\"/><figcaption>图5 图编码网络展开成RNN的形式</figcaption></figure><p>Backward过程的目的在于求解 <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+e_w%7D%7B%5Cpartial+w%7D\" alt=\"\\frac{\\partial e_w}{\\partial w}\" eeimg=\"1\"/> ，这里 <img src=\"https://www.zhihu.com/equation?tex=e_w\" alt=\"e_w\" eeimg=\"1\"/> 是损失函数。跟forward过程类型，也是找到t时刻使得梯度处于一个稳定状态。然后求解t时刻的梯度 <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+e_w%7D%7B%5Cpartial+w%7D\" alt=\"\\frac{\\partial e_w}{\\partial w}\" eeimg=\"1\"/> 。</p><h2>2、基于矩阵形式的GNN模型实现</h2><p>Forward的矩阵实现参考了[6]，为了方便后面的代码介绍，我在这里简单描述一下，这里用的图片也是来者[6]。</p><p>对于每个图，转化成图6中间的矩阵形式，第一列表示父结点id，第二列表示子结点id，第三列表示父结点标签，第四列表示子结点标签，第五列表示边的标签，第六列表示子结点 <img src=\"https://www.zhihu.com/equation?tex=t-1\" alt=\"t-1\" eeimg=\"1\"/> 时刻的状态。这里的标签理解成特征更加合适，不然容易和后面需要预测的结点标签弄混。因此，每一行对应一条边。通过函数f得到每条边在 <img src=\"https://www.zhihu.com/equation?tex=t\" alt=\"t\" eeimg=\"1\"/> 时刻对父结点状态的贡献，即父结点相连的子结点对父结点状态的贡献，如图6右侧的矩阵所示。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-e336ea0c8508deb9f263928048043841_b.jpg\" data-size=\"normal\" data-rawwidth=\"1255\" data-rawheight=\"385\" class=\"origin_image zh-lightbox-thumb\" width=\"1255\" data-original=\"https://pic2.zhimg.com/v2-e336ea0c8508deb9f263928048043841_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1255&#39; height=&#39;385&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1255\" data-rawheight=\"385\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1255\" data-original=\"https://pic2.zhimg.com/v2-e336ea0c8508deb9f263928048043841_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-e336ea0c8508deb9f263928048043841_b.jpg\"/><figcaption>图6 计算每条边在 t 时刻对父结点状态的贡献</figcaption></figure><p>同时，为了聚合子结点对父结点的状态，需要构造图7所示的一个矩阵，行表示边，列表示结点，数值1表示是否以该结点作为边的父结点。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-c092292466238aeef3dc8092ce647647_b.jpg\" data-size=\"normal\" data-rawwidth=\"903\" data-rawheight=\"502\" class=\"origin_image zh-lightbox-thumb\" width=\"903\" data-original=\"https://pic4.zhimg.com/v2-c092292466238aeef3dc8092ce647647_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;903&#39; height=&#39;502&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"903\" data-rawheight=\"502\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"903\" data-original=\"https://pic4.zhimg.com/v2-c092292466238aeef3dc8092ce647647_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-c092292466238aeef3dc8092ce647647_b.jpg\"/><figcaption>图7 边和父结点的关系矩阵</figcaption></figure><p>因此，我们可以通过上面得到的两个矩阵，做矩阵乘法得到聚合的父结点状态表示，如图8所示。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-3c797ca74fcfd0e1203a97b1cbcb70ee_b.jpg\" data-size=\"normal\" data-rawwidth=\"1365\" data-rawheight=\"514\" class=\"origin_image zh-lightbox-thumb\" width=\"1365\" data-original=\"https://pic3.zhimg.com/v2-3c797ca74fcfd0e1203a97b1cbcb70ee_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1365&#39; height=&#39;514&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1365\" data-rawheight=\"514\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1365\" data-original=\"https://pic3.zhimg.com/v2-3c797ca74fcfd0e1203a97b1cbcb70ee_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-3c797ca74fcfd0e1203a97b1cbcb70ee_b.jpg\"/><figcaption>图8 聚合每条边在 t 时刻对父结点状态的贡献，得到父结点的状态</figcaption></figure><h2>3、GNN模型的python代码实现</h2><p>下面，通过一个例子讲解如何使用python，tensorflow实现GNN。任务是将图分割成2部分，即将图中的结点分成两类。代码来自[2]，为了易读性，我简化了代码，完整的代码请见<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Ivan0131/gnn_demo\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/Ivan0131/gnn</span><span class=\"invisible\">_demo</span><span class=\"ellipsis\"></span></a>。</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"c1\"># reading train, validation dataset</span>\n<span class=\"n\">data_path</span> <span class=\"o\">=</span> <span class=\"s2\">&#34;./data&#34;</span>\n<span class=\"n\">set_name</span> <span class=\"o\">=</span> <span class=\"s2\">&#34;sub_15_7_200&#34;</span>\n<span class=\"c1\">############# training set ################</span>\n<span class=\"n\">inp</span><span class=\"p\">,</span> <span class=\"n\">arcnode</span><span class=\"p\">,</span> <span class=\"n\">nodegraph</span><span class=\"p\">,</span> <span class=\"n\">nodein</span><span class=\"p\">,</span> <span class=\"n\">labels</span><span class=\"p\">,</span> <span class=\"n\">_</span> <span class=\"o\">=</span> <span class=\"n\">gnn_utils</span><span class=\"o\">.</span><span class=\"n\">set_load_general</span><span class=\"p\">(</span><span class=\"n\">data_path</span><span class=\"p\">,</span> <span class=\"s2\">&#34;train&#34;</span><span class=\"p\">,</span> <span class=\"n\">set_name</span><span class=\"o\">=</span><span class=\"n\">set_name</span><span class=\"p\">)</span>\n<span class=\"n\">inp</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">a</span><span class=\"p\">[:,</span> <span class=\"mi\">1</span><span class=\"p\">:]</span> <span class=\"k\">for</span> <span class=\"n\">a</span> <span class=\"ow\">in</span> <span class=\"n\">inp</span><span class=\"p\">]</span>\n<span class=\"c1\">############ validation set #############</span>\n<span class=\"n\">inp_val</span><span class=\"p\">,</span> <span class=\"n\">arcnode_val</span><span class=\"p\">,</span> <span class=\"n\">nodegraph_val</span><span class=\"p\">,</span> <span class=\"n\">nodein_val</span><span class=\"p\">,</span> <span class=\"n\">labels_val</span><span class=\"p\">,</span> <span class=\"n\">_</span> <span class=\"o\">=</span> <span class=\"n\">gnn_utils</span><span class=\"o\">.</span><span class=\"n\">set_load_general</span><span class=\"p\">(</span><span class=\"n\">data_path</span><span class=\"p\">,</span> <span class=\"s2\">&#34;validation&#34;</span><span class=\"p\">,</span> <span class=\"n\">set_name</span><span class=\"o\">=</span><span class=\"n\">set_name</span><span class=\"p\">)</span>\n<span class=\"n\">inp_val</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">a</span><span class=\"p\">[:,</span> <span class=\"mi\">1</span><span class=\"p\">:]</span> <span class=\"k\">for</span> <span class=\"n\">a</span> <span class=\"ow\">in</span> <span class=\"n\">inp_val</span><span class=\"p\">]</span></code></pre></div><p>为了方便起见，直接使用了[3]的数据读取，重点讲解GNN的实现。其中inp是前面提到的图6中的矩阵，arcnode是图7中矩阵。inp和图6中的矩阵略有不同，前2列还是父结点和子结点的id，后面4列是父结点和子结点的四个特征，这里我们去除第一列（父结点id)，因为后面计算用不到。nodegraph表示各个结点属于哪个图，对整个图做预测的时候用的到，这里我们用不到，nodein是图结点数量，labels是我们要预测的每个结点的标签。</p><div class=\"highlight\"><pre><code class=\"language-text\">def f_w(inp):\n    with tf.variable_scope(&#39;State_net&#39;):\n        layer1 = tf.layers.dense(inp, 5, activation=tf.nn.sigmoid)\n        layer2 = tf.layers.dense(layer1, state_dim, activation=tf.nn.sigmoid)\n        return layer2\ndef g_w(inp):\n    with tf.variable_scope(&#39;Output_net&#39;):\n        layer1 = tf.layers.dense(inp, 5, activation=tf.nn.sigmoid)\n        layer2 = tf.layers.dense(layer1, output_dim, activation=None)\n        return layer2</code></pre></div><p>定义f<i>w和 g_w</i>函数，都由两层全连接组成。这里f_w函数实现的是计算每条边对父结点状态的贡献，见图6，还需要加上图8聚合的过程才是 <img src=\"https://www.zhihu.com/equation?tex=f_w\" alt=\"f_w\" eeimg=\"1\"/> 。</p><div class=\"highlight\"><pre><code class=\"language-text\">#init input placeholder\ncomp_inp = tf.placeholder(tf.float32, shape=(None, input_dim), name=&#34;input&#34;)\ny = tf.placeholder(tf.float32, shape=(None, output_dim), name=&#34;target&#34;)\n\n# state(t) &amp; state(t-1)\nstate = tf.placeholder(tf.float32, shape=(None, state_dim), name=&#34;state&#34;)\nstate_old = tf.placeholder(tf.float32, shape=(None, state_dim), name=&#34;old_state&#34;)\n\n# arch-node conversion matrix\nArcNode = tf.sparse_placeholder(tf.float32, name=&#34;ArcNode&#34;)</code></pre></div><p>定义tensorflow的palceholder用于数据输入。</p><div class=\"highlight\"><pre><code class=\"language-text\">def convergence(a, state, old_state, k):\n    with tf.variable_scope(&#39;Convergence&#39;):\n        # assign current state to old state\n        old_state = state\n        \n        # 获取子结点上一个时刻的状态\n        # grub states of neighboring node \n        gat = tf.gather(old_state, tf.cast(a[:, 0], tf.int32))\n        \n        # 去除第一列，即子结点的id\n        # slice to consider only label of the node and that of it&#39;s neighbor \n        # sl = tf.slice(a, [0, 1], [tf.shape(a)[0], tf.shape(a)[1] - 1])\n        # equivalent code\n        sl = a[:, 1:]\n        \n        # 将子结点上一个时刻的状态放到最后一列\n        # concat with retrieved state\n        inp = tf.concat([sl, gat], axis=1)\n\n        # evaluate next state and multiply by the arch-node conversion matrix to obtain per-node states\n        #计算子结点对父结点状态的贡献\n        layer1 = f_w(inp)\n        #聚合子结点对父结点状态的贡献，得到当前时刻的父结点的状态\n        state = tf.sparse_tensor_dense_matmul(ArcNode, layer1)\n\n        # update the iteration counter\n        k = k + 1\n    return a, state, old_state, k</code></pre></div><p>这里实现的是完整的 <img src=\"https://www.zhihu.com/equation?tex=f_w\" alt=\"f_w\" eeimg=\"1\"/> 函数，可以分成两个部分：构建图6中的矩阵（获取子结点 <img src=\"https://www.zhihu.com/equation?tex=t-1\" alt=\"t-1\" eeimg=\"1\"/> 时刻的状态，然后加入到最后一列，并去掉第一列，子结点id）；根据父结点相连的子结点对父结点状态的贡献，做累加，聚合得到父结点 <img src=\"https://www.zhihu.com/equation?tex=t\" alt=\"t\" eeimg=\"1\"/> 时刻的状态，这里使用矩阵乘法计算。</p><div class=\"highlight\"><pre><code class=\"language-text\">def condition(a, state, old_state, k):\n    # evaluate condition on the convergence of the state\n    with tf.variable_scope(&#39;condition&#39;):\n        # 检查当前状态和上一个时刻的状态的欧式距离是否小于阈值\n        # evaluate distance by state(t) and state(t-1)\n        outDistance = tf.sqrt(tf.reduce_sum(tf.square(tf.subtract(state, old_state)), 1) + 1e-10)\n        # vector showing item converged or not (given a certain threshold)\n        checkDistanceVec = tf.greater(outDistance, state_threshold)\n        \n        c1 = tf.reduce_any(checkDistanceVec)\n        \n        # 是否达到最大迭代次数\n        c2 = tf.less(k, max_iter)\n\n    return tf.logical_and(c1, c2)\n\n# 迭代计算，直到状态达到稳定状态\n# compute state\nwith tf.variable_scope(&#39;Loop&#39;):\n    k = tf.constant(0)\n    res, st, old_st, num = tf.while_loop(condition, convergence,\n                                         [comp_inp, state, state_old, k])\n    # 计算结点的output\n    out = g_w(st)\n</code></pre></div><p>前面讲过，GNN中结点的state需要迭代，直到达到一个稳定状态。上述代码就是这样一个过程，condition函数是判断状态是否达到稳定状态或者达到最大迭代次数，condition函数下面一段代码是对结点的state进行循环迭代，直至满足condition条件。到达稳定状态之后，使用g_w函数计算结点的output。</p><div class=\"highlight\"><pre><code class=\"language-text\">loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=out))\naccuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(out, 1), tf.argmax(y, 1)), dtype=tf.float32))\noptimizer = tf.train.AdamOptimizer(0.001)\ngrads = optimizer.compute_gradients(loss)\ntrain_op = optimizer.apply_gradients(grads, name=&#39;train_op&#39;)</code></pre></div><p>这里使用交叉熵作为模型的损失函数，准确率作为评测结果的指标。这里直接根据loss计算梯度，对变量进行更新，没有像论文中那样计算梯度的稳定状态t，然后求t时刻的梯度。</p><div class=\"highlight\"><pre><code class=\"language-text\">###train model####\nnum_epoch = 5000\n# 训练集placeholder输入\narcnode_train = tf.SparseTensorValue(indices=arcnode[0].indices, values=arcnode[0].values,\n                                    dense_shape=arcnode[0].dense_shape)\nfd_train = {comp_inp: inp[0], state: np.zeros((arcnode[0].dense_shape[0], state_dim)),\n          state_old: np.ones((arcnode[0].dense_shape[0], state_dim)),\n          ArcNode: arcnode_train, y: labels}\n#验证集placeholder输入\narcnode_valid = tf.SparseTensorValue(indices=arcnode_val[0].indices, values=arcnode_val[0].values,\n                                dense_shape=arcnode_val[0].dense_shape)\nfd_valid = {comp_inp: inp_val[0], state: np.zeros((arcnode_val[0].dense_shape[0], state_dim)),\n      state_old: np.ones((arcnode_val[0].dense_shape[0], state_dim)),\n      ArcNode: arcnode_valid, y: labels_val}\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    sess.run(tf.local_variables_initializer())\n    for i in range(0, num_epoch):\n        _, loss_val, accuracy_val = sess.run(\n                    [train_op, loss, accuracy],\n                    feed_dict=fd_train)\n        if i % 100 == 0:\n\n            loss_valid_val, accuracy_valid_val = sess.run(\n                    [loss, accuracy],\n                    feed_dict=fd_valid)\n            print(&#34;iter %s\\t training loss: %s,\\t training accuracy: %s,\\t validation loss: %s,\\t validation accuracy: %s&#34; % \n                  (i, loss_val, accuracy_val, loss_valid_val, accuracy_valid_val))</code></pre></div><p>这是模型的训练过程，迭代训练5000次。可以看出模型每次迭代的训练集输入和验证集输入都是一样的，因为这里用到的数据集只有一个图。模型训练的输出结果如下图所示：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-b5a5193bc5f1e1ace2e8bdb1b8c4c1f7_b.jpg\" data-size=\"normal\" data-rawwidth=\"999\" data-rawheight=\"348\" class=\"origin_image zh-lightbox-thumb\" width=\"999\" data-original=\"https://pic4.zhimg.com/v2-b5a5193bc5f1e1ace2e8bdb1b8c4c1f7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;999&#39; height=&#39;348&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"999\" data-rawheight=\"348\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"999\" data-original=\"https://pic4.zhimg.com/v2-b5a5193bc5f1e1ace2e8bdb1b8c4c1f7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-b5a5193bc5f1e1ace2e8bdb1b8c4c1f7_b.jpg\"/><figcaption>图9 模型训练的结果</figcaption></figure><h2>4、总结</h2><p>GNN模型可以使用热扩散的原理去理解它。例如，在 <img src=\"https://www.zhihu.com/equation?tex=t_0\" alt=\"t_0\" eeimg=\"1\"/> 时刻，传递给图中某些结点一些热量，使得这些结点的温度升高，这些结点会通过相连的边向相邻的结点传导热量。因此，下一个时刻，某个结点的热量的流入为相邻结点传递热量的累加和。随着时间的推移，每个结点的热量流入会达到一个稳定状态。GNN最大的特点是存在一个稳定状态，因此 <img src=\"https://www.zhihu.com/equation?tex=f_w\" alt=\"f_w\" eeimg=\"1\"/> 需要满足固定点的理论。后面一些图网络，不需要满足这一条件，例如GCN，GGNN。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>[1] 从图(Graph)到图卷积(Graph Convolution)：漫谈图神经网络模型 <a href=\"https://link.zhihu.com/?target=https%3A//www.cnblogs.com/SivilTaram/p/graph_neural_network_1.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">cnblogs.com/SivilTaram/</span><span class=\"invisible\">p/graph_neural_network_1.html</span><span class=\"ellipsis\"></span></a></p><p>[2] Graph Neural Network Model <a href=\"https://link.zhihu.com/?target=https%3A//github.com/mtiezzi/gnn\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/mtiezzi/gnn</span><span class=\"invisible\"></span></a></p><p>[3] Graph Neural Networks: A Review of Methods and Applications <a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1812.08434\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/abs/1812.0843</span><span class=\"invisible\">4</span><span class=\"ellipsis\"></span></a></p><p>[4] Graphical-Based Learning Environments for Pattern Recognition <a href=\"https://link.zhihu.com/?target=https%3A//link.springer.com/chapter/10.1007/978-3-540-27868-9_4\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">link.springer.com/chapt</span><span class=\"invisible\">er/10.1007/978-3-540-27868-9_4</span><span class=\"ellipsis\"></span></a></p><p>[5] The Graph Neural Network Model <a href=\"https://link.zhihu.com/?target=https%3A//ieeexplore.ieee.org/document/4700287\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">ieeexplore.ieee.org/doc</span><span class=\"invisible\">ument/4700287</span><span class=\"ellipsis\"></span></a></p><p>[6] Graph Neural Network Mode <a href=\"https://link.zhihu.com/?target=https%3A//mtiezzi.github.io/gnn_site/GNN/software.html%23matrix-based-implementation\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">mtiezzi.github.io/gnn_s</span><span class=\"invisible\">ite/GNN/software.html#matrix-based-implementation</span><span class=\"ellipsis\"></span></a></p><p></p>", 
            "topic": [
                {
                    "tag": "图神经网络（GNN）", 
                    "tagLink": "https://api.zhihu.com/topics/20747184"
                }
            ], 
            "comments": [
                {
                    "userName": "gogo", 
                    "userLink": "https://www.zhihu.com/people/2e580259486f34a71db05c15c3d2859c", 
                    "content": "讲得很透彻，大赞👍👍", 
                    "likes": 1, 
                    "childComments": []
                }, 
                {
                    "userName": "小崔", 
                    "userLink": "https://www.zhihu.com/people/5cac8da02977d33435493f819d2dedf6", 
                    "content": "<p>固定点，数学里一般说不动点！</p>", 
                    "likes": 1, 
                    "childComments": [
                        {
                            "userName": "Ivan Yan", 
                            "userLink": "https://www.zhihu.com/people/3677a61f41a4d1cd630d63e270524c07", 
                            "content": "这样呀，学到了。", 
                            "likes": 0, 
                            "replyToAuthor": "小崔"
                        }, 
                        {
                            "userName": "无人的城市", 
                            "userLink": "https://www.zhihu.com/people/4e07657da81fe75bd1ef63df6d44329c", 
                            "content": "Fixed point 翻译问题而已", 
                            "likes": 1, 
                            "replyToAuthor": "小崔"
                        }
                    ]
                }, 
                {
                    "userName": "zhuangAnjun", 
                    "userLink": "https://www.zhihu.com/people/2532ab585f590ccbb4ad1dcb0c744728", 
                    "content": "这个初始化的0/1矩阵在图很大的时候就不适用了。在很大的一个拓扑图中，我觉得可以将子图抽出，用w2v的方法来训练，这样应该也可以达到类似的效果", 
                    "likes": 1, 
                    "childComments": []
                }, 
                {
                    "userName": "花开无声", 
                    "userLink": "https://www.zhihu.com/people/9270f33b727d5d947ea4bde2d4ee90cf", 
                    "content": "讲得很棒，大佬带我学AI", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/82845443", 
            "userName": "Ivan Yan", 
            "userLink": "https://www.zhihu.com/people/3677a61f41a4d1cd630d63e270524c07", 
            "upvote": 5, 
            "title": "Query推荐—RIN: Reformulation Inference Network", 
            "content": "<p>最近在看Query推荐的相关论文，前几年的文章基本都是基于图的，例如query flow graph, query url bigraph。最近几年，这一领域也基本被深度学习占领了。先简单介绍一下Query推荐：用户去搜一个东西的时候，可能搜索结果不大满意，他会去不断修改这个Query，直到找到满意的结果。因此，现在的搜索引擎往往会有Query推荐功能，去预测用户想要的query，然后推荐给用户。接下来介绍一下“RIN: Reformulation Inference Network for Context-AwareQuery Suggestion”这篇文章，发表在CIKM 2018上。</p><a href=\"https://link.zhihu.com/?target=https%3A//jyunyu.csie.org/docs/pubs/cikm2018paper.pdf\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">RIN: Reformulation Inference Network for Context-AwareQuery Suggestion</a><h2>动机</h2><p>用户的搜索意图往往比较复杂，而查询的query往往比较短且可能具有歧义。因此，为了找到需要的信息，用户会不断去修改query，直到找到满意的内容。这严重影响了用户体验。因此，现在搜索引擎会推荐下一条query以及query补全。</p><p>为了捕获用户的搜索意图，直观的想法是直接使用上下文信息，包括同一个search session中的之前的queries和点击信息。之前的基于图的方法，往往基于query的相似性和共现关系。但是，这样构建的图会很稀疏，而影响最终的推荐结果。</p><p>根据之前的研究结果，作者把query的修改分成：句法修改和语义修改。句法修改包括增加单词、删除单词、缩写词拓展。语义修改，指修改query的含义，包括泛化和特色化。之前的基于query reformulation的方法，往往受限于预先定义好的reformulation策略或者知识图谱的帮助，无法解决范围外的reformulation策略和歧义的queries。</p><p>随着深度学习的兴起，特别是RNN网络，提供了一些新的思路来学习总结query reformulation的规律。但是，之前的基于RNN的方法有一下几个缺点：缺乏对之前query reformulation的理解；对于一些复杂的query reformulation，很难学习得到；query直接的转换很稀疏，因此RNN很难学到query reformulation的规律，现有的方法往往依赖额外的特征。</p><p>针对以上问题，作者提出了Reformulation Inference Network(RIN)，贡献度如下：</p><ol><li>提出了使用homomorphic编码，可以同时保持句法和语义属性。更通用和灵活的表示，可以帮助网络学习query reformulation。</li><li>提出了RIN网络来解决数据稀疏的问题。</li><li>通过实验来证明RIN网络的有用性。</li></ol><h2>问题描述</h2><p>一个search session可以写成query的序列 <img src=\"https://www.zhihu.com/equation?tex=%3Cq_1%2C+q_2%2C...%2Cq_L%3E\" alt=\"&lt;q_1, q_2,...,q_L&gt;\" eeimg=\"1\"/> ，每一条query <img src=\"https://www.zhihu.com/equation?tex=q\" alt=\"q\" eeimg=\"1\"/> 都由一组单词组成，记为 <img src=\"https://www.zhihu.com/equation?tex=T%28q%29\" alt=\"T(q)\" eeimg=\"1\"/> ，并对应相应的点击信息，记为 <img src=\"https://www.zhihu.com/equation?tex=u\" alt=\"u\" eeimg=\"1\"/> （一组点击的urls）。因此，文章的目标定义如下：</p><ul><li>给定一组候选的queries，给出相应的评分，使得 <img src=\"https://www.zhihu.com/equation?tex=q_%7BL%2B1%7D\" alt=\"q_{L+1}\" eeimg=\"1\"/> 的分值尽可能高。</li><li>生成一条 <img src=\"https://www.zhihu.com/equation?tex=q_%7BL%2B1%7D%5E%60\" alt=\"q_{L+1}^`\" eeimg=\"1\"/> ，使得 <img src=\"https://www.zhihu.com/equation?tex=q_%7BL%2B1%7D%5E%60\" alt=\"q_{L+1}^`\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=q_%7BL%2B1%7D\" alt=\"q_{L+1}\" eeimg=\"1\"/> 尽可能相似。</li></ul><h2>RIN网络</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-7cd4033d043776b906382169fa9ed667_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1187\" data-rawheight=\"397\" class=\"origin_image zh-lightbox-thumb\" width=\"1187\" data-original=\"https://pic4.zhimg.com/v2-7cd4033d043776b906382169fa9ed667_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1187&#39; height=&#39;397&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1187\" data-rawheight=\"397\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1187\" data-original=\"https://pic4.zhimg.com/v2-7cd4033d043776b906382169fa9ed667_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-7cd4033d043776b906382169fa9ed667_b.jpg\"/></figure><p>上图显示了RIN网络的架构，主要由4部分组成：query session encoder，reformulation inferencer，query discriminator和query generator。这里query session encoder的输入是作者提出的Homomorphic Query Embedding，定义如下：</p><p><img src=\"https://www.zhihu.com/equation?tex=v_q%3D+%5Csum_%7Bt+%5Cin+T%28q%29%7D+v_t\" alt=\"v_q= \\sum_{t \\in T(q)} v_t\" eeimg=\"1\"/> </p><p>这里 <img src=\"https://www.zhihu.com/equation?tex=T%28q%29\" alt=\"T(q)\" eeimg=\"1\"/> 是query包含的单词，而 <img src=\"https://www.zhihu.com/equation?tex=v_t\" alt=\"v_t\" eeimg=\"1\"/> 是每个单词的向量，后面解释如何得到， <img src=\"https://www.zhihu.com/equation?tex=v_q\" alt=\"v_q\" eeimg=\"1\"/> 就是query对应的Homomorphic Query Embedding。作者认为任何query reformulation都可以认为是单词的添加和删除，因此直接对query包含的单词向量进行求和，来表示query。这样做的好处有：句法关系（添加和删除单词）被保留了；embedding的潜在空间可以表示query的语义信息；query的线性结构可以帮助理解query reformulation的语义信息。例如，”Japan travel&#34;-&gt;&#34;Tokyo travel“可以被表示成 <img src=\"https://www.zhihu.com/equation?tex=v_%7BTokyo%7D-v_%7Bjapan%7D\" alt=\"v_{Tokyo}-v_{japan}\" eeimg=\"1\"/> ，和&#34;Italy hotel&#34;-&gt;&#34;Rome hotel&#34;的 <img src=\"https://www.zhihu.com/equation?tex=v_%7BRome%7D-v_%7BItaly%7D\" alt=\"v_{Rome}-v_{Italy}\" eeimg=\"1\"/> 类似，都是国家修改成首都。</p><p>这里，每个单词的向量 <img src=\"https://www.zhihu.com/equation?tex=v_t\" alt=\"v_t\" eeimg=\"1\"/> 跟之前的文章有所不同。之前的网络的词向量一般都是来自于word2vec或者glove，根据文章中单词的共现关系计算得到。这里，作者构造了如下所示的一个图，然后通过graph embedding算法，得到每个单词的向量。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-f52ff1cd7e1e5557688668c7bc1dccf2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"558\" data-rawheight=\"339\" class=\"origin_image zh-lightbox-thumb\" width=\"558\" data-original=\"https://pic3.zhimg.com/v2-f52ff1cd7e1e5557688668c7bc1dccf2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;558&#39; height=&#39;339&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"558\" data-rawheight=\"339\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"558\" data-original=\"https://pic3.zhimg.com/v2-f52ff1cd7e1e5557688668c7bc1dccf2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-f52ff1cd7e1e5557688668c7bc1dccf2_b.jpg\"/></figure><p>上图中有三种节点：term, query, website。每条query指向第一个单词，然后每个单词指向后续单词。同时，每条query还指向用户点击的website，和reformulatedquery。如果只看term节点，它表达的是单词的共现关系，等价于word2vec。此外还加入了query reformulation和点击这两个额外信息。</p><p><b>query session encoder</b>是一个带attention的双向RNN。RNN的输入为 <img src=\"https://www.zhihu.com/equation?tex=x_i%3D%5Bv_%7Bq_i%7D%3Br_%7Bi-1%7D%5D\" alt=\"x_i=[v_{q_i};r_{i-1}]\" eeimg=\"1\"/> ，这里 <img src=\"https://www.zhihu.com/equation?tex=r_%7Bi-1%7D%3Dv_%7Bq_i%7D-v_%7Bq_%7Bi-1%7D%7D\" alt=\"r_{i-1}=v_{q_i}-v_{q_{i-1}}\" eeimg=\"1\"/> ， <img src=\"https://www.zhihu.com/equation?tex=r_0\" alt=\"r_0\" eeimg=\"1\"/> 为零向量。RNN这里作者采用双向GRU。然后经过一层attention pooling得到context vector <img src=\"https://www.zhihu.com/equation?tex=c\" alt=\"c\" eeimg=\"1\"/> 。</p><p><b>Reformulation Inferencer：</b>要预测 <img src=\"https://www.zhihu.com/equation?tex=q_%7BL%2B1%7D\" alt=\"q_{L+1}\" eeimg=\"1\"/> 等价于预测 <img src=\"https://www.zhihu.com/equation?tex=r_L\" alt=\"r_L\" eeimg=\"1\"/> ，并且 <img src=\"https://www.zhihu.com/equation?tex=r_L\" alt=\"r_L\" eeimg=\"1\"/> 更容易预测一些，因为不同的query转换之间，他们的reformulation可能是相同的。这里直接通过2层的MLP预测 <img src=\"https://www.zhihu.com/equation?tex=r_L\" alt=\"r_L\" eeimg=\"1\"/> ，公式如下所示：</p><p><img src=\"https://www.zhihu.com/equation?tex=u_r%3DReLU%28F_%7Bh_r%7D%28c%29%29+%5C%5C+%5Cbar%7Br_L%7D+%3D+W_ru_r%2Bb_r\" alt=\"u_r=ReLU(F_{h_r}(c)) \\\\ \\bar{r_L} = W_ru_r+b_r\" eeimg=\"1\"/> </p><p> 这里 <img src=\"https://www.zhihu.com/equation?tex=F_%7Bh_r%7D\" alt=\"F_{h_r}\" eeimg=\"1\"/> 是全连接层， <img src=\"https://www.zhihu.com/equation?tex=ReLU\" alt=\"ReLU\" eeimg=\"1\"/> 是线性整流函数，<img src=\"https://www.zhihu.com/equation?tex=%5Cbar%7Br_L%7D\" alt=\"\\bar{r_L}\" eeimg=\"1\"/> 是预测的reformulation。</p><p>由于最终的结果是对候选query进行打分或者生成推荐的query。因此还需要query discriminator和query generator这两个模块。</p><p><b>query discriminator</b>的做法类似于文本匹配，将候选query <img src=\"https://www.zhihu.com/equation?tex=q_%7Bcan%7D\" alt=\"q_{can}\" eeimg=\"1\"/> 和context vector <img src=\"https://www.zhihu.com/equation?tex=c\" alt=\"c\" eeimg=\"1\"/> 拼在一起得到 <img src=\"https://www.zhihu.com/equation?tex=x_d+%3D+%5Bv_%7Bq_%7Bcan%7D%7D%3Bc%5D\" alt=\"x_d = [v_{q_{can}};c]\" eeimg=\"1\"/> ，然后经过双层MLP，具体如下:</p><p><img src=\"https://www.zhihu.com/equation?tex=u_d%3DReLU%28F_%7Bh_d%7D%28x_d%29%29%2C++%5C%5C+%5Cbar%7By%7D%3D%5Csigma%28F_%7Bh_d%7D%28u_d%29%29\" alt=\"u_d=ReLU(F_{h_d}(x_d)),  \\\\ \\bar{y}=\\sigma(F_{h_d}(u_d))\" eeimg=\"1\"/> </p><p>这里 <img src=\"https://www.zhihu.com/equation?tex=%5Csigma\" alt=\"\\sigma\" eeimg=\"1\"/> 是sigmoid函数， <img src=\"https://www.zhihu.com/equation?tex=%5Cbar%7By%7D\" alt=\"\\bar{y}\" eeimg=\"1\"/> 是对候选query的评分。</p><p><b>query generator </b>就是再接一个RNN做decoder，生成相应的query，类似于机器翻译。要生成第 <img src=\"https://www.zhihu.com/equation?tex=t\" alt=\"t\" eeimg=\"1\"/> 个单词 <img src=\"https://www.zhihu.com/equation?tex=w_t\" alt=\"w_t\" eeimg=\"1\"/> ，RNN的hidden state计算如下：</p><p><img src=\"https://www.zhihu.com/equation?tex=s_t%3DRNN%28s_%7Bt-1%7D%2C%5Bw_%7Bt-1%7D%3Bc_t%5D%29\" alt=\"s_t=RNN(s_{t-1},[w_{t-1};c_t])\" eeimg=\"1\"/> </p><p>这里 <img src=\"https://www.zhihu.com/equation?tex=c_t\" alt=\"c_t\" eeimg=\"1\"/> 是第 <img src=\"https://www.zhihu.com/equation?tex=t\" alt=\"t\" eeimg=\"1\"/> 个单词的context vecotr，由于每个单词需要的上下文信息是不同的，因此重新对query session context中的RNN hidden state做attention pooling得到 <img src=\"https://www.zhihu.com/equation?tex=c_t\" alt=\"c_t\" eeimg=\"1\"/> ，具体如下：</p><p><img src=\"https://www.zhihu.com/equation?tex=u_%7Bt%2Ci%7D%3Dtanh%28F_g%28%5Bs_%7Bt-1%7D%3Bh_i%5D%29%29%2C+%5C%5C+a_%7Bt%2Ci%7D%3D%5Cfrac%7Bexp%28u_%7Bt%2Ci%7D%5ETu_g%29%7D%7B%5Csum_%7Bi%5E%60%7D%7Bexp%28u_%7Bt%2Ci%5E%60%7D%5ETu_g%29%7D%7D+%5C%5C+c_t%3D%5Csum_%7Bi%7D%7Ba_%7Bt%2Ci%7D%7Dh_i\" alt=\"u_{t,i}=tanh(F_g([s_{t-1};h_i])), \\\\ a_{t,i}=\\frac{exp(u_{t,i}^Tu_g)}{\\sum_{i^`}{exp(u_{t,i^`}^Tu_g)}} \\\\ c_t=\\sum_{i}{a_{t,i}}h_i\" eeimg=\"1\"/> </p><p>然后根据 <img src=\"https://www.zhihu.com/equation?tex=s_t\" alt=\"s_t\" eeimg=\"1\"/> 计算字典中每个单词的条件概率：</p><p><img src=\"https://www.zhihu.com/equation?tex=P%28w_t%7Cw_1%2C...%2Cw_%7Bt-1%7D%2Cc%29%3Df%28s_t%29\" alt=\"P(w_t|w_1,...,w_{t-1},c)=f(s_t)\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=f%28%5Ccdot%29\" alt=\"f(\\cdot)\" eeimg=\"1\"/> 是projection layer。</p><h2>模型训练</h2><p>reformulation inferencer部分的损失函数就是真实的 <img src=\"https://www.zhihu.com/equation?tex=r_L\" alt=\"r_L\" eeimg=\"1\"/> 与预测的 <img src=\"https://www.zhihu.com/equation?tex=%5Cbar%7Br_L%7D\" alt=\"\\bar{r_L}\" eeimg=\"1\"/> 之间的距离：</p><p><img src=\"https://www.zhihu.com/equation?tex=loss_R%3D%5Cfrac%7B1%7D%7B2%7D%5C%7C+r_L+-+%5Cbar%7Br_L%7D%5C%7C_F%5E2\" alt=\"loss_R=\\frac{1}{2}\\| r_L - \\bar{r_L}\\|_F^2\" eeimg=\"1\"/> </p><p>这里 <img src=\"https://www.zhihu.com/equation?tex=%5C%7C%5Ccdot%5C%7C\" alt=\"\\|\\cdot\\|\" eeimg=\"1\"/> 是Frobenius norm。</p><p>对于discriminative任务，query discriminator的损失函数就是真实标签 <img src=\"https://www.zhihu.com/equation?tex=y\" alt=\"y\" eeimg=\"1\"/> 和预测标签 <img src=\"https://www.zhihu.com/equation?tex=%5Cbar%7By%7D\" alt=\"\\bar{y}\" eeimg=\"1\"/> 之间的交叉熵：</p><p><img src=\"https://www.zhihu.com/equation?tex=loss_D%3D-%28ylog%28%5Cbar%7By%7D%29%2B%281-y%29log%281-%5Cbar%7By%7D%29%29\" alt=\"loss_D=-(ylog(\\bar{y})+(1-y)log(1-\\bar{y}))\" eeimg=\"1\"/> </p><p>对于generative任务，query generator的损失函数是实际的query和生成的query之间的交叉熵：</p><p><img src=\"https://www.zhihu.com/equation?tex=loss_G%3D-%5Csum_%7Bw_t%7D%7Blog+P%28w_t%7CS_t%29%7D\" alt=\"loss_G=-\\sum_{w_t}{log P(w_t|S_t)}\" eeimg=\"1\"/> </p><p>这里 <img src=\"https://www.zhihu.com/equation?tex=w_t\" alt=\"w_t\" eeimg=\"1\"/> 是实际的query的第t个单词， <img src=\"https://www.zhihu.com/equation?tex=S_t\" alt=\"S_t\" eeimg=\"1\"/> 是 <img src=\"https://www.zhihu.com/equation?tex=w_t\" alt=\"w_t\" eeimg=\"1\"/> 之前的单词。</p><p>最终的损失函数为：</p><p><img src=\"https://www.zhihu.com/equation?tex=loss%3Dloss_R%2Bloss_%7Btask%7D\" alt=\"loss=loss_R+loss_{task}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=loss_%7Btask%7D\" alt=\"loss_{task}\" eeimg=\"1\"/> 为 <img src=\"https://www.zhihu.com/equation?tex=loss_D\" alt=\"loss_D\" eeimg=\"1\"/> 或者 <img src=\"https://www.zhihu.com/equation?tex=loss_G\" alt=\"loss_G\" eeimg=\"1\"/>  。</p><h2>实验结果</h2><p>这里简单写一下，具体的结果请参考原始论文。对于discriminative任务，使用<a href=\"https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Mean_reciprocal_rank\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">MRR（mean reciprocal rank）</a>作为评价指标，对于generative任务，使用<a href=\"https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Word_error_rate\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">PER（position independent word error rate）</a>作为评价指标。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-69315469182e0532ffd78989143f0f3c_b.png\" data-size=\"normal\" data-rawwidth=\"1497\" data-rawheight=\"217\" class=\"origin_image zh-lightbox-thumb\" width=\"1497\" data-original=\"https://pic1.zhimg.com/v2-69315469182e0532ffd78989143f0f3c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1497&#39; height=&#39;217&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1497\" data-rawheight=\"217\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1497\" data-original=\"https://pic1.zhimg.com/v2-69315469182e0532ffd78989143f0f3c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-69315469182e0532ffd78989143f0f3c_b.png\"/><figcaption>不同模型MRR的结果。可以看出RIN在rerank效果上明显好于其他几个模型。</figcaption></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-0f39770c557658a0d78214d036b01c81_b.jpg\" data-size=\"normal\" data-rawwidth=\"704\" data-rawheight=\"217\" class=\"origin_image zh-lightbox-thumb\" width=\"704\" data-original=\"https://pic2.zhimg.com/v2-0f39770c557658a0d78214d036b01c81_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;704&#39; height=&#39;217&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"704\" data-rawheight=\"217\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"704\" data-original=\"https://pic2.zhimg.com/v2-0f39770c557658a0d78214d036b01c81_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-0f39770c557658a0d78214d036b01c81_b.jpg\"/><figcaption>由于generate任务只有深度模型，因此只对比了两个深度模型。结果略好于最新的模型ACG，明显好于HRED。</figcaption></figure><p>此外，作者分析了Homomorphic Embedding的作用。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-1a4b8bda22603faa4d36399f205ce292_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"754\" data-rawheight=\"541\" class=\"origin_image zh-lightbox-thumb\" width=\"754\" data-original=\"https://pic3.zhimg.com/v2-1a4b8bda22603faa4d36399f205ce292_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;754&#39; height=&#39;541&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"754\" data-rawheight=\"541\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"754\" data-original=\"https://pic3.zhimg.com/v2-1a4b8bda22603faa4d36399f205ce292_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-1a4b8bda22603faa4d36399f205ce292_b.jpg\"/></figure><p>Q表示输入homomorphic query embedding，R表示输入embeddings of reformulations。可以看出，只输入Q效果比ACG还差，输入R模型的结果接近于Q+R。可以看出embeddings of reformulations对于模型预测推荐query更加有用。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-32ce1588be8258bb8810e3983e9a497b_b.jpg\" data-size=\"normal\" data-rawwidth=\"751\" data-rawheight=\"527\" class=\"origin_image zh-lightbox-thumb\" width=\"751\" data-original=\"https://pic4.zhimg.com/v2-32ce1588be8258bb8810e3983e9a497b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;751&#39; height=&#39;527&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"751\" data-rawheight=\"527\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"751\" data-original=\"https://pic4.zhimg.com/v2-32ce1588be8258bb8810e3983e9a497b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-32ce1588be8258bb8810e3983e9a497b_b.jpg\"/><figcaption>加和不加reformulation inference的区别。RIN(-I）表示不加reformulation inference。可以看出加入reformulation inference对模型结果略微的提升。</figcaption></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>总结</h2><p>这篇文章的主要贡献可以分成两个吧。</p><ol><li>提出了homomorphic embedding，使用这种方式相当于将用户改写query和点击信息也编码进了单词向量。对于实际应用，这个图会非常大，需要分布式平台技术。</li><li>加入了embeddings of reformulations，以及reformulation inference。由于用户输入的query的数量非常多，而且长尾非常严重。如果从改写规则的角度去看query之间的转换，在一定程度上确实可以解决数据稀疏的问题。</li></ol>", 
            "topic": [
                {
                    "tag": "query分析", 
                    "tagLink": "https://api.zhihu.com/topics/19611289"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/82129871", 
            "userName": "Ivan Yan", 
            "userLink": "https://www.zhihu.com/people/3677a61f41a4d1cd630d63e270524c07", 
            "upvote": 3, 
            "title": "知识蒸馏（Knowledge Distillation）简述（二）", 
            "content": "<h2>简介</h2><p>这两年BERT太火了，但是BERT模型是真的大，计算起来太慢了。知识蒸馏可以对模型进行压缩，迁移到小的模型。找了下是否有关于BERT的知识蒸馏的论文，然后看到了下面这篇论文，“Distilling Task-Specific Knowledge from BERT into Simple Neural Networks”。做法很简单，就是用知识蒸馏的方法，把BERT模型迁移到BiLSTM模型上。</p><a data-draft-node=\"block\" data-draft-type=\"link-card\" href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1903.12136\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Distilling Task-Specific Knowledge from BERT into Simple Neural Networks</a><h2>模型</h2><p>这篇文章的teacher网络是BERT模型。BERT模型的输入可以是一句话，做分类；或者两句话，做匹配；因此，针对这两种情况文章提出了2个模型，作为student网络。如下图所示：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-8df2b7f5e7418205a62c2c21b035d171_b.jpg\" data-rawwidth=\"461\" data-rawheight=\"437\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"461\" data-original=\"https://pic2.zhimg.com/v2-8df2b7f5e7418205a62c2c21b035d171_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;461&#39; height=&#39;437&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"461\" data-rawheight=\"437\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"461\" data-original=\"https://pic2.zhimg.com/v2-8df2b7f5e7418205a62c2c21b035d171_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-8df2b7f5e7418205a62c2c21b035d171_b.jpg\"/></figure><p>这个模型就是传统的biLSTM模型，输入词向量到biLSTM层，然后取最后一步的hidden states喂给全连接层，接softmax做分类。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-9e8f68d0e076cd878eb801440e2c5da8_b.jpg\" data-rawwidth=\"532\" data-rawheight=\"587\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"532\" data-original=\"https://pic1.zhimg.com/v2-9e8f68d0e076cd878eb801440e2c5da8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;532&#39; height=&#39;587&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"532\" data-rawheight=\"587\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"532\" data-original=\"https://pic1.zhimg.com/v2-9e8f68d0e076cd878eb801440e2c5da8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-9e8f68d0e076cd878eb801440e2c5da8_b.jpg\"/></figure><p> 对于文本匹配，文章作者用的模型也很简单，并没有使用特别复杂的文本匹配模型作为student网络。取两个句子最后一步的biLSTM的hidden states <img src=\"https://www.zhihu.com/equation?tex=h_%7Bs1%7D%2C+h_%7Bs2%7D\" alt=\"h_{s1}, h_{s2}\" eeimg=\"1\"/> ，然后拼接成 <img src=\"https://www.zhihu.com/equation?tex=%5Bh_%7Bs1%7D%2C+h_%7Bs2%7D%2C+h_%7Bs1%7D%5Codot+h_%7Bs2%7D%2C+%5Cleft%7C+h_%7Bs1%7D+-+h_%7Bs2%7D+%5Cright%7C%5D\" alt=\"[h_{s1}, h_{s2}, h_{s1}\\odot h_{s2}, \\left| h_{s1} - h_{s2} \\right|]\" eeimg=\"1\"/> 作为全连接层的输入，然后经过softmax得到匹配结果。 <img src=\"https://www.zhihu.com/equation?tex=%5Codot\" alt=\"\\odot\" eeimg=\"1\"/> 表示点乘。 </p><h2>Distillation目标函数</h2><p>模型已经介绍完了，下面介绍如何将BERT模型的知识迁移到biLSTM中。这里直接使用了teacher网络logits的输出，来作为student网络的distillation objective，而不是使用论文 “Distilling the Knowledge in a Neural Network”中的soft targets。具体公式如下所示：</p><p><img src=\"https://www.zhihu.com/equation?tex=y%3Dsoftmax%28z%29+%5C%5C+L_%7Bdistill%7D%3D%5Cleft%7C%7C+z%5E%7B%28B%29%7D+-+z%5E%7B%28S%29%7D+%5Cright%7C%7C_2%5E2+%5C%5C+L+%3D+%5Calpha+%5Ccdot+L_%7BCE%7D+%2B+%281-%5Calpha%29%5Ccdot+L_%7Bdistill%7D+%5C%5C+%3D+-%5Calpha%5Csum_%7Bi%7D%7Bt_i+log+y_i%5E%7B%28S%29%7D%7D+-+%281-%5Calpha%29%5Cleft%7C%7C+z%5E%7B%28B%29%7D+-+z%5E%7B%28S%29%7D+%5Cright%7C%7C_2%5E2\" alt=\"y=softmax(z) \\\\ L_{distill}=\\left|| z^{(B)} - z^{(S)} \\right||_2^2 \\\\ L = \\alpha \\cdot L_{CE} + (1-\\alpha)\\cdot L_{distill} \\\\ = -\\alpha\\sum_{i}{t_i log y_i^{(S)}} - (1-\\alpha)\\left|| z^{(B)} - z^{(S)} \\right||_2^2\" eeimg=\"1\"/> </p><p>这里 <img src=\"https://www.zhihu.com/equation?tex=z%5E%7B%28B%29%7D\" alt=\"z^{(B)}\" eeimg=\"1\"/>和 <img src=\"https://www.zhihu.com/equation?tex=z%5E%7B%28S%29%7D\" alt=\"z^{(S)}\" eeimg=\"1\"/>是teacher和student网络的logits，使用MSE（mean-squared-error）来作为 <img src=\"https://www.zhihu.com/equation?tex=L_%7Bdistill%7D\" alt=\"L_{distill}\" eeimg=\"1\"/> 的损失函数。t是真实标签。</p><p>在distillation过程中，小数据集可能无法完全的表达teacher网络的知识。因此，文中提出了三种数据增强的方法来人为的扩充数据集，防止过拟合。</p><ol><li><b>Masking </b>使用[mask]标签来随机替换一个单词，例如“I love the comedy&#34;替换为” I [mask] the comedy&#34;。</li><li><b>POS-guided word replacement</b> 将一个单词替换为相同POS标签的随机单词。例如，“What do pigs eat?&#34;替换为&#34;How do pigs eat?&#34;。</li><li><b>n-gram sampling</b> 随机采用n-gram，n从1到5，并丢弃其它单词。</li></ol><h2>实验结果</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-c636f71818a1a36f47764a69de708a6a_b.jpg\" data-rawwidth=\"1092\" data-rawheight=\"528\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"1092\" data-original=\"https://pic3.zhimg.com/v2-c636f71818a1a36f47764a69de708a6a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1092&#39; height=&#39;528&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1092\" data-rawheight=\"528\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1092\" data-original=\"https://pic3.zhimg.com/v2-c636f71818a1a36f47764a69de708a6a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-c636f71818a1a36f47764a69de708a6a_b.jpg\"/></figure><p>上表显示了模型的结果，可以看出加入知识蒸馏之后，模型的结果确实比单纯地使用BiLSTM的结果好，在SST-2和QQP上居然比ELMo要好。可以确定的是这样做确实可以将BERT中的知识迁移到BiLSTM中。但是BiLSTM模型的表达能力比BERT弱很多。显然有些知识是无法从BERT中迁移到BiLSTM中的，例如两个句子之间的co-attention信息和长距离依赖信息。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-e26aa8f051e4c45acee3c9f5cf101f05_b.jpg\" data-rawwidth=\"970\" data-rawheight=\"346\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"970\" data-original=\"https://pic2.zhimg.com/v2-e26aa8f051e4c45acee3c9f5cf101f05_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;970&#39; height=&#39;346&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"970\" data-rawheight=\"346\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"970\" data-original=\"https://pic2.zhimg.com/v2-e26aa8f051e4c45acee3c9f5cf101f05_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-e26aa8f051e4c45acee3c9f5cf101f05_b.jpg\"/></figure><p>上图显示了网络的inference时间，可以看到相比于BERT参数减少了98倍，速度加快了434倍。</p><p>这篇文章展示了如何将BERT的知识迁移到其它网络中（BiLSTM）。对与离线的数据处理，由于时间要求不高，现在一般会采用BERT模型，或者BERT的改进模型，因为效果确实比其它模型有很大的提升，基本可以无脑上BERT。对于线上的任务，BERT的inference确实太慢了。这篇文章的思路可以借鉴一下，实现也很简单，就是加一个 <img src=\"https://www.zhihu.com/equation?tex=L_%7Bdistill%7D\" alt=\"L_{distill}\" eeimg=\"1\"/> 损失。这篇文章的效果，我自己还没试过，具体效果是否有文章这么好，还不知道。但对没有预训练的网络，应该会有一定的效果提升。</p>", 
            "topic": [], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/81467832", 
            "userName": "Ivan Yan", 
            "userLink": "https://www.zhihu.com/people/3677a61f41a4d1cd630d63e270524c07", 
            "upvote": 87, 
            "title": "知识蒸馏（Knowledge Distillation）简述（一）", 
            "content": "<p>最新在看知识蒸馏的文章，主要是现在的深度学习模型越来越大，例如BERT。在线下处理数据，对时间要求不高的话，还能接受，能跑完就好。但是线上运行，对延迟要求高的话，像BERT这样的大模型，就很难满足要求。因此，就找了找模型压缩的方法。</p><p>知识蒸馏被广泛的用于模型压缩和迁移学习当中。开山之作应该是<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1503.02531\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">”Distilling the Knowledge in a Neural Network“</a>。这篇文章中，作者的motivation是找到一种方法，把多个模型的知识提炼给单个模型。</p><p>文章的标题是Distilling the Knowledge in a Neural Network，那么说明是神经网络的知识呢？一般认为模型的参数保留了模型学到的知识，因此最常见的迁移学习的方式就是在一个大的数据集上先做预训练，然后使用预训练得到的参数在一个小的数据集上做微调（两个数据集往往领域不同或者任务不同）。例如先在Imagenet上做预训练，然后在COCO数据集上做检测。在这篇论文中，作者认为可以将模型看成是黑盒子，针对特定模型的输入，得到的输出结果就是知识。因此，我们可以先训练好一个teacher网络，然后将teacher的网络的输出结果 <img src=\"https://www.zhihu.com/equation?tex=q\" alt=\"q\" eeimg=\"1\"/> 作为student网络的目标，训练student网络，使得student网络的结果 <img src=\"https://www.zhihu.com/equation?tex=p\" alt=\"p\" eeimg=\"1\"/> 接近 <img src=\"https://www.zhihu.com/equation?tex=q\" alt=\"q\" eeimg=\"1\"/> ，因此，我们可以将损失函数写成 <img src=\"https://www.zhihu.com/equation?tex=L%3DCE%28y%2C+p%29%2B%5Calpha+CE%28q%2C+p%29\" alt=\"L=CE(y, p)+\\alpha CE(q, p)\" eeimg=\"1\"/> 。这里CE是交叉熵（Cross Entropy），y是真实标签的onehot编码，q是teacher网络的输出结果，p是student网络的输出结果。</p><p>但是，直接使用teacher网络的softmax的输出结果 <img src=\"https://www.zhihu.com/equation?tex=q\" alt=\"q\" eeimg=\"1\"/> ，可能不大合适。因此，一个网络训练好之后，对于正确的答案会有一个很高的置信度。例如，在MNIST数据中，对于某个2的输入，对于2的预测概率会很高，而对于2类似的数字，例如3和7的预测概率为 <img src=\"https://www.zhihu.com/equation?tex=10%5E%7B-6%7D\" alt=\"10^{-6}\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=10%5E%7B-9%7D\" alt=\"10^{-9}\" eeimg=\"1\"/> 。这样的话，teacher网络学到数据的相似信息（例如数字2和3，7很类似）很难传达给student网络。由于它们的概率值接近0。因此，文章提出了soft target，公式如下所示：</p><p><img src=\"https://www.zhihu.com/equation?tex=q_i%3D%5Cfrac%7Bexp%28z_i%2FT%29%7D%7B%5Csum_j+exp%28z_j%2FT%29%7D\" alt=\"q_i=\\frac{exp(z_i/T)}{\\sum_j exp(z_j/T)}\" eeimg=\"1\"/> </p><p>这里 <img src=\"https://www.zhihu.com/equation?tex=z_i\" alt=\"z_i\" eeimg=\"1\"/> 是神经网络softmax前的输出logit。如果将T取1，这个公式就是softmax，根据logit输出各个类别的概率。如果T接近于0，则最大的值会越近1，其它值会接近0，近似于onehot编码。如果T越大，则输出的结果的分布越平缓，相当于平滑的一个作用，起到保留相似信息的作用。如果T等于无穷，就是一个均匀分布。</p><p>最终文章根据上述的损失函数对网络进行训练。</p><ol><li>在MNIST这个数据集上，先使用大的网络进行训练，测试集错误67个，小网络训练，测试集错误146个。加入soft targets到目标函数中，相当于正则项，测试集的错误降低到了74个。这证明了teacher网络确实把知识转移到了student网络，使得结果变好了。</li><li>第二个实验是在speech recognition领域，使用不同的参数训练了10个DNN，对这10个模型的预测结果求平均作为emsemble的结果，相比于单个模型有一定的提升。然后将这10个模型作为teacher网络，训练student网络。得到的Distilled Single model相比于直接的单个网络，也有一定的提升，结果见下表：</li></ol><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ff90976f717e2ad6378ba92f881f8604_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"515\" data-rawheight=\"123\" class=\"origin_image zh-lightbox-thumb\" width=\"515\" data-original=\"https://pic1.zhimg.com/v2-ff90976f717e2ad6378ba92f881f8604_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;515&#39; height=&#39;123&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"515\" data-rawheight=\"123\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"515\" data-original=\"https://pic1.zhimg.com/v2-ff90976f717e2ad6378ba92f881f8604_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-ff90976f717e2ad6378ba92f881f8604_b.jpg\"/></figure><p><b>结论</b></p><p>知识蒸馏，可以将一个网络的知识转移到另一个网络，两个网络可以是同构或者异构。做法是先训练一个teacher网络，然后使用这个teacher网络的输出和数据的真实标签去训练student网络。知识蒸馏，可以用来将网络从大网络转化成一个小网络，并保留接近于大网络的性能；也可以将多个网络的学到的知识转移到一个网络中，使得单个网络的性能接近emsemble的结果。</p><p>参考文献：</p><ol><li>Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. &#34;Distilling the knowledge in a neural network.&#34;<i>arXiv preprint arXiv:1503.02531</i>(2015).</li><li>Prakhar Ganesh. &#34;Knowledge Distillation : Simplified&#34;. <a href=\"https://link.zhihu.com/?target=https%3A//towardsdatascience.com/knowledge-distillation-simplified-dd4973dbc764\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">towardsdatascience.com/</span><span class=\"invisible\">knowledge-distillation-simplified-dd4973dbc764</span><span class=\"ellipsis\"></span></a>, 2019.</li></ol><p></p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "迁移学习 (Transfer Learning)", 
                    "tagLink": "https://api.zhihu.com/topics/20079475"
                }
            ], 
            "comments": []
        }
    ], 
    "url": "https://zhuanlan.zhihu.com/c_1155064304431783936"
}
