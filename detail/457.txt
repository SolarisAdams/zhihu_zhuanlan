{
    "followers": [
        "https://www.zhihu.com/people/Micro-Kun", 
        "https://www.zhihu.com/people/liu-e-shao", 
        "https://www.zhihu.com/people/wang-wang-20-73", 
        "https://www.zhihu.com/people/YangfanLiang", 
        "https://www.zhihu.com/people/ji-fo-74", 
        "https://www.zhihu.com/people/gogo-30-43", 
        "https://www.zhihu.com/people/lingyv", 
        "https://www.zhihu.com/people/zhang-xiao-hui-66-53", 
        "https://www.zhihu.com/people/michael-wu-25", 
        "https://www.zhihu.com/people/fan-tao-rong-66", 
        "https://www.zhihu.com/people/li-jun-16-72-29", 
        "https://www.zhihu.com/people/gao-fei-3-98", 
        "https://www.zhihu.com/people/li-yi-heng-60-80", 
        "https://www.zhihu.com/people/scottdc-31", 
        "https://www.zhihu.com/people/zhang-kun-58-72", 
        "https://www.zhihu.com/people/niceworld-49-6", 
        "https://www.zhihu.com/people/lan-hua-cao-6-22", 
        "https://www.zhihu.com/people/huang-teng-jiu-47", 
        "https://www.zhihu.com/people/feng-chui-wu-heng", 
        "https://www.zhihu.com/people/xie-shu-yi", 
        "https://www.zhihu.com/people/lucky-45-9-77", 
        "https://www.zhihu.com/people/yyuqing6161", 
        "https://www.zhihu.com/people/joyce-58-73-53", 
        "https://www.zhihu.com/people/sunhongwen", 
        "https://www.zhihu.com/people/jian-xiong", 
        "https://www.zhihu.com/people/zhang-jia-xin-1203", 
        "https://www.zhihu.com/people/liu-lin-lin-81-63", 
        "https://www.zhihu.com/people/la-geek", 
        "https://www.zhihu.com/people/wei-meng-de-xiao-lao-hu-80", 
        "https://www.zhihu.com/people/yan-yi-6-4", 
        "https://www.zhihu.com/people/yang-rui-34-92", 
        "https://www.zhihu.com/people/sun-yan-90-29"
    ], 
    "article": [
        {
            "url": "https://zhuanlan.zhihu.com/p/52154254", 
            "userName": "BlockheadLS", 
            "userLink": "https://www.zhihu.com/people/1c46b8d33e1f99c7c8a4f237d27751f9", 
            "upvote": 12, 
            "title": "FastText代码详解(一)", 
            "content": "<p>FastText是Tomas继word2vec的又一力作，它其实是两篇论文。一篇是<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1607.01759\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Bag of Tricks for Efficient Text Classification</a>，作用的是分类，速度快，效果也不差，在工业界中备受青睐；另一篇是<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1607.04606\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Enriching Word Vectors with Subword Information</a>，可以说是在word2vec基础上的发展。</p><h2><b>FastText原理介绍</b></h2><p>FastText原理和word2vec的原理差不多，除了输入不太一样，FastText可以做分类任务，以及一些代码细节外，其他都是一样的。有一篇介绍word2vec的文章讲的非常棒，<a href=\"https://link.zhihu.com/?target=https%3A//github.com/renpengcheng-github/nlp/blob/master/3.word2vec/word2vec_%25E4%25B8%25AD%25E7%259A%2584%25E6%2595%25B0%25E5%25AD%25A6%25E5%258E%259F%25E7%2590%2586%25E8%25AF%25A6%25E8%25A7%25A3.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">word2vec中的数学原理详解</a>，如果是word2vec的初学者，强烈建议去看。</p><p>下面以CBOW为例简单地介绍FastText的原理。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-cf9c5a934e1ff9b9410d912f5144828c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2050\" data-rawheight=\"1060\" class=\"origin_image zh-lightbox-thumb\" width=\"2050\" data-original=\"https://pic1.zhimg.com/v2-cf9c5a934e1ff9b9410d912f5144828c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2050&#39; height=&#39;1060&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2050\" data-rawheight=\"1060\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2050\" data-original=\"https://pic1.zhimg.com/v2-cf9c5a934e1ff9b9410d912f5144828c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-cf9c5a934e1ff9b9410d912f5144828c_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-194bf16794cb5752959cd31cd0c5b32d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"618\" data-rawheight=\"244\" class=\"origin_image zh-lightbox-thumb\" width=\"618\" data-original=\"https://pic2.zhimg.com/v2-194bf16794cb5752959cd31cd0c5b32d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;618&#39; height=&#39;244&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"618\" data-rawheight=\"244\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"618\" data-original=\"https://pic2.zhimg.com/v2-194bf16794cb5752959cd31cd0c5b32d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-194bf16794cb5752959cd31cd0c5b32d_b.jpg\"/></figure><p><b>输入层：</b>黑色框代表的是词向量，也是word2vec的输入；黑色框+蓝色框是FastText训练embedding时的输入，蓝色框代表的是一个word的一个subword，比如当minn=1,maxn=2时，单词“喜欢”首先包装成“&lt;喜欢&gt;”,所以subwords是[&lt;，喜，欢&gt;，&lt;喜，喜欢，欢&gt;] ；黑色框+蓝色框+红色框是FastText分类时的输入，红色框代表的是单词间word ngrams，比如当wordngrams=2时，“&lt; 我 爱 北京 天安门 &gt;”的wordngrams就是[&lt;我，我爱，爱北京，北京天安门，天安门&gt;]。</p><p><b>映射层：</b>在word2vec中，映射层是各个word embedding的相加；FastText中是加和平均，加和平均后的映射层向量为 <img src=\"https://www.zhihu.com/equation?tex=X_w\" alt=\"X_w\" eeimg=\"1\"/> 。</p><p><b>输出层：</b>输出层可以是softmax，层次softmax或者负采样。当训练embedding时，或者label数量太多时，必须要算所有单词或label得分，这个过程是非常耗时的，为了缩短训练时间，Toams提出了层次softmax和负采样。 层次softmax的核心是根据全部语料的词频建立Huffman树，词频大的越靠近根节点，利用上下文只预测中心词，从而调整树节点的参数，这样就预测的实际次数就变成了 <img src=\"https://www.zhihu.com/equation?tex=log_2N\" alt=\"log_2N\" eeimg=\"1\"/> , <img src=\"https://www.zhihu.com/equation?tex=N\" alt=\"N\" eeimg=\"1\"/> 是单词数。负采样是根据值定的采样算法在诸多的负样本中选取质量高的作为实际的负样本，有效减少预测label的同时也提高了word embedding的质量。</p><p><b>以CBOW模型、层次softmax为例推导：</b></p><p>上图中设一个单词为 <img src=\"https://www.zhihu.com/equation?tex=w\" alt=\"w\" eeimg=\"1\"/> ,上下文为 <img src=\"https://www.zhihu.com/equation?tex=context%28w%29\" alt=\"context(w)\" eeimg=\"1\"/> ,在word2vec中上下文只是指左右窗口中的单词，在FastText中还包含subwords和wordNgrams。</p><p>上下文预测中心词的概率表示为：</p><p><img src=\"https://www.zhihu.com/equation?tex=p%28w%7Ccontext%28w%29%29%3D%5Cprod_%7Bj%3D2%7D%5E%7Bl%5Ew%7Dp%28d_j%5Ew%7CX_w%2C%5Ctheta_%7Bj-1%7D%5Ew%29\" alt=\"p(w|context(w))=\\prod_{j=2}^{l^w}p(d_j^w|X_w,\\theta_{j-1}^w)\" eeimg=\"1\"/> </p><p>其中，</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D+++p%28d_j%5Ew%7CX_w%2C%5Ctheta_%7Bj-1%7D%5Ew%29%3D+%5Cleft%5C%7B++++++++++++++++%5Cbegin%7Barray%7D%7B%2A%2Alr%2A%2A%7D++++++++++++++++%5Csigma%28X_w%5E%5Ctop%5Ctheta_%7Bj-1%7D%5Ew%29%2C+%26+d_j%5Ew%3D1%5C%5C++++++++++++++++1-%5Csigma%28X_w%5E%5Ctop%5Ctheta_%7Bj-1%7D%5Ew%29%2C+%26++d_j%5Ew%3D0++++++++++++++%5Cend%7Barray%7D+++%5Cright.+++%5Cend%7Bequation%7D++\" alt=\"\\begin{equation}   p(d_j^w|X_w,\\theta_{j-1}^w)= \\left\\{                \\begin{array}{**lr**}                \\sigma(X_w^\\top\\theta_{j-1}^w), &amp; d_j^w=1\\\\                1-\\sigma(X_w^\\top\\theta_{j-1}^w), &amp;  d_j^w=0              \\end{array}   \\right.   \\end{equation}  \" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%3D%5Cleft%5B+%5Csigma%28X_w%5E%5Ctop%5Ctheta_%7Bj-1%7D%5Ew%29+%5Cright%5D%5E%7Bd_j%5Ew%7D%5Cleft%5B+1-%5Csigma%28X_w%5E%5Ctop%5Ctheta_%7Bj-1%7D%5Ew%29+%5Cright%5D%5E%7B1-d_j%5Ew%7D\" alt=\"\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =\\left[ \\sigma(X_w^\\top\\theta_{j-1}^w) \\right]^{d_j^w}\\left[ 1-\\sigma(X_w^\\top\\theta_{j-1}^w) \\right]^{1-d_j^w}\" eeimg=\"1\"/> </p><p>损失函数为：</p><p><img src=\"https://www.zhihu.com/equation?tex=L%3D%5Csum_%7Bw%5Cin+C%7D%5E%7B%7D%7Blog%5C+p%28w%7Ccontext%28w%29%29%7D\" alt=\"L=\\sum_{w\\in C}^{}{log\\ p(w|context(w))}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%3D%5Csum_%7Bw%5Cin+C%7D%5E%7B%7D%7Blog%5C+%5Cprod_%7Bj%3D2%7D%5E%7Bl%5Ew%7D%7B%5Cleft%5B+%5Csigma%28X_w%5E%5Ctop%5Ctheta_%7Bj-1%7D%5Ew%29+%5Cright%5D%5E%7Bd_j%5Ew%7D%5Cleft%5B+1-%5Csigma%28X_w%5E%5Ctop%5Ctheta_%7Bj-1%7D%5Ew%29+%5Cright%5D%5E%7B1-d_j%5Ew%7D%7D%7D\" alt=\"=\\sum_{w\\in C}^{}{log\\ \\prod_{j=2}^{l^w}{\\left[ \\sigma(X_w^\\top\\theta_{j-1}^w) \\right]^{d_j^w}\\left[ 1-\\sigma(X_w^\\top\\theta_{j-1}^w) \\right]^{1-d_j^w}}}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%3D%5Csum_%7Bw%5Cin+C%7D%5E%7B%7D%7B%5Csum_%7Bj%3D2%7D%5E%7Bl%5Ew%7D%7B%5C%7Bd_j%5Ewlog%5Csigma%28X_w%5E%5Ctop%5Ctheta_%7Bj-1%7D%5Ew%29%2B%281-d_j%5Ew%29log%281-%5Csigma%28X_w%5E%5Ctop%5Ctheta_%7Bj-1%7D%5Ew%29%29%5C%7D%7D%7D\" alt=\"=\\sum_{w\\in C}^{}{\\sum_{j=2}^{l^w}{\\{d_j^wlog\\sigma(X_w^\\top\\theta_{j-1}^w)+(1-d_j^w)log(1-\\sigma(X_w^\\top\\theta_{j-1}^w))\\}}}\" eeimg=\"1\"/> </p><p>其中 <img src=\"https://www.zhihu.com/equation?tex=C\" alt=\"C\" eeimg=\"1\"/> 是整个语料库。</p><p>取出上面最里面的公式来看，</p><p><img src=\"https://www.zhihu.com/equation?tex=L%28w%2Cj%29%3Dd_j%5Ewlog%5Csigma%28X_w%5E%5Ctop%5Ctheta_%7Bj-1%7D%5Ew%29%2B%281-d_j%5Ew%29log%281-%5Csigma%28X_w%5E%5Ctop%5Ctheta_%7Bj-1%7D%5Ew%29%29\" alt=\"L(w,j)=d_j^wlog\\sigma(X_w^\\top\\theta_{j-1}^w)+(1-d_j^w)log(1-\\sigma(X_w^\\top\\theta_{j-1}^w))\" eeimg=\"1\"/> </p><p>左右两边同时对 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_%7Bj-1%7D%5Ew\" alt=\"\\theta_{j-1}^w\" eeimg=\"1\"/> 求导有，</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+L%28w%2Cj%29%7D%7B%5Cpartial+%5Ctheta_%7Bj-1%7D%5Ew%7D+%3D+%28d_j%5Ew-%5Csigma%28X_w%5E%5Ctop%5Ctheta_%7Bj-1%7D%5Ew%29%29X_w%5E%5Ctop\" alt=\"\\frac{\\partial L(w,j)}{\\partial \\theta_{j-1}^w} = (d_j^w-\\sigma(X_w^\\top\\theta_{j-1}^w))X_w^\\top\" eeimg=\"1\"/> </p><p>根据对称性有，</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+L%28w%2Cj%29%7D%7B%5Cpartial+X_w%7D+%3D+%28d_j%5Ew-%5Csigma%28X_w%5E%5Ctop%5Ctheta_%7Bj-1%7D%5Ew%29%29%5Ctheta_%7Bj-1%7D%5Ew\" alt=\"\\frac{\\partial L(w,j)}{\\partial X_w} = (d_j^w-\\sigma(X_w^\\top\\theta_{j-1}^w))\\theta_{j-1}^w\" eeimg=\"1\"/> </p><p>先对词向量进行更新，一个同事说的好，前向传播和反向传播的时候的确应该是同一套参数：</p><p><img src=\"https://www.zhihu.com/equation?tex=v%28%5Ctilde%7Bw%7D%29%3Dv%28%5Ctilde%7Bw%7D%29%2B%5Ceta%5Csum_%7Bj%3D2%7D%5E%7Bl%5Ew%7D%7B%5Cfrac%7B%5Cpartial+L%28w%2Cj%29%7D%7B%5Cpartial+X_w%7D%7D\" alt=\"v(\\tilde{w})=v(\\tilde{w})+\\eta\\sum_{j=2}^{l^w}{\\frac{\\partial L(w,j)}{\\partial X_w}}\" eeimg=\"1\"/> ,</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Ctilde%7Bw%7D\" alt=\"\\tilde{w}\" eeimg=\"1\"/> 是context(w)中的一个词， <img src=\"https://www.zhihu.com/equation?tex=v%28%5Ctilde%7Bw%7D%29\" alt=\"v(\\tilde{w})\" eeimg=\"1\"/> 表示 <img src=\"https://www.zhihu.com/equation?tex=%5Ctilde%7Bw%7D\" alt=\"\\tilde{w}\" eeimg=\"1\"/> 的词向量。</p><p>再对对参数 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_%7Bj-1%7D%5Ew\" alt=\"\\theta_{j-1}^w\" eeimg=\"1\"/> 更新，顺序不能变：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_%7Bj-1%7D%5Ew+%3D+%5Ctheta_%7Bj-1%7D%5Ew+%2B%5Ceta%5Cfrac%7B%5Cpartial+L%28w%2Cj%29%7D%7B%5Cpartial+%5Ctheta_%7Bj-1%7D%5Ew%7D+\" alt=\"\\theta_{j-1}^w = \\theta_{j-1}^w +\\eta\\frac{\\partial L(w,j)}{\\partial \\theta_{j-1}^w} \" eeimg=\"1\"/> </p><p><b>以CBOW模型、Negsampling为例：</b></p><p>对于一个给定的正样本 <img src=\"https://www.zhihu.com/equation?tex=%28context%28w%29%2C+w%29\" alt=\"(context(w), w)\" eeimg=\"1\"/> ，我们想要最大化</p><p><img src=\"https://www.zhihu.com/equation?tex=g%28w%29%3D%5Cprod_%7Bu%5Cin%7B%5C%7Bw%5C%7D%7D%5Ccup+NEG%28w%29%7D%5E%7B%7Dp%28u%7Ccontext%28w%29%29\" alt=\"g(w)=\\prod_{u\\in{\\{w\\}}\\cup NEG(w)}^{}p(u|context(w))\" eeimg=\"1\"/> </p><p>其中， <img src=\"https://www.zhihu.com/equation?tex=NEG%28w%29\" alt=\"NEG(w)\" eeimg=\"1\"/> 是使用负采样算法得到的负样本集合，</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D+++p%28u%7Ccontext%28w%29%29%3D+%5Cleft%5C%7B++++++++++++++++%5Cbegin%7Barray%7D%7B%2A%2Alr%2A%2A%7D++++++++++++++++%5Csigma%28X_w%5E%5Ctop%5Ctheta_%7B%7D%5Eu%29%2C+%26+L%5Ew%28u%29%3D1%5C%5C++++++++++++++++1-%5Csigma%28X_w%5E%5Ctop%5Ctheta%5Eu%29%2C+%26++L%5Ew%28u%29%3D0++++++++++++++%5Cend%7Barray%7D+++%5Cright.+++%5Cend%7Bequation%7D++\" alt=\"\\begin{equation}   p(u|context(w))= \\left\\{                \\begin{array}{**lr**}                \\sigma(X_w^\\top\\theta_{}^u), &amp; L^w(u)=1\\\\                1-\\sigma(X_w^\\top\\theta^u), &amp;  L^w(u)=0              \\end{array}   \\right.   \\end{equation}  \" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%5C+%3D%5Cleft%5B+%5Csigma%28X_w%5E%5Ctop%5Ctheta%5Eu%29+%5Cright%5D%5E%7BL%5Ew%28u%29%7D%5Cleft%5B+1-%5Csigma%28X_w%5E%5Ctop%5Ctheta_%7B%7D%5Eu%29+%5Cright%5D%5E%7B1-L%5Ew%28u%29%7D\" alt=\"\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =\\left[ \\sigma(X_w^\\top\\theta^u) \\right]^{L^w(u)}\\left[ 1-\\sigma(X_w^\\top\\theta_{}^u) \\right]^{1-L^w(u)}\" eeimg=\"1\"/> </p><p>最终的损失函数就是：</p><p><img src=\"https://www.zhihu.com/equation?tex=L%3Dlog%5Cprod_%7Bw%5Cin+C%7D%5E%7B%7Dg%28w%29%3D%5Csum_%7Bw%5Cin+C%7D%5E%7B%7D%7Blog%5C+g%28w%29%7D\" alt=\"L=log\\prod_{w\\in C}^{}g(w)=\\sum_{w\\in C}^{}{log\\ g(w)}\" eeimg=\"1\"/> </p><p>那负采样算法是什么呢？一个词被选作负样本的概率为：</p><p><img src=\"https://www.zhihu.com/equation?tex=P%28w%29%3D%5Cfrac%7B%5Bcounter%28u%29%5D%5E%7B0.5%7D%7D%7B%5Csum_%7Bu%5Cin+D%7D%5E%7B%7D%7B%5Bcounter%28u%29%5D%5E%7B0.5%7D%7D%7D\" alt=\"P(w)=\\frac{[counter(u)]^{0.5}}{\\sum_{u\\in D}^{}{[counter(u)]^{0.5}}}\" eeimg=\"1\"/> </p><p>幂次是0.5，比幂次是1更能选中低频词。</p><h2><b>FastText数据格式</b></h2><p><b>分类时的输入：</b></p><p><img src=\"https://www.zhihu.com/equation?tex=%5C+%5C+%5C+%5C+%5C+word_1%5C+word_2%5C+word_3%5C+%E2%80%A6%5C+word_n%5C+%5C_%5C_label%5C_%5C_1\" alt=\"\\ \\ \\ \\ \\ word_1\\ word_2\\ word_3\\ …\\ word_n\\ \\_\\_label\\_\\_1\" eeimg=\"1\"/> </p><p> 空格，tab都可以分割，&#34;__label__&#34;分隔符可以由参数-label指定，以行为单位，最大数目不超过1024，否则强行截断。</p><p><b>训练embedding时的输入：</b></p><p><img src=\"https://www.zhihu.com/equation?tex=%5C+%5C+%5C+%5C+%5C+word_1%5C+word_2%5C+word_3%5C+%E2%80%A6%5C+word_n%5C\" alt=\"\\ \\ \\ \\ \\ word_1\\ word_2\\ word_3\\ …\\ word_n\\\" eeimg=\"1\"/> </p><p>同样空格，tab都可以分割。</p><p>subwords和wordNgrams都是程序自动计算的，不需要人工输入。</p><h2><b>FastText代码架构</b></h2><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/facebookresearch/fastText\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">FastText源码</a>的结构如下图所示。左边是代码文件，右边是代码架构。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-011bb5acdf663ea87df49a636e1165c6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"845\" data-rawheight=\"446\" class=\"origin_image zh-lightbox-thumb\" width=\"845\" data-original=\"https://pic3.zhimg.com/v2-011bb5acdf663ea87df49a636e1165c6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;845&#39; height=&#39;446&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"845\" data-rawheight=\"446\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"845\" data-original=\"https://pic3.zhimg.com/v2-011bb5acdf663ea87df49a636e1165c6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-011bb5acdf663ea87df49a636e1165c6_b.jpg\"/></figure><p>main文件是入口，会根据用户参数调用fasttext文件不同的函数。</p><p>fasttext文件可以用CBOW或者Skip-gram的方式训练word embedding，也可以做分类的训练和预测。</p><p>model文件负责模型具体逻辑的实现，比如CBOW或者Sg的具体训练过程，前向反馈后向传播等。</p><p>dictionary文件主要是前期对语料的处理，比如建立字典，统计词频，过滤低频词等。</p><p>args文件主要是涉及到用户可操作的参数。</p><p>matrix, real, vector文件主要是向量计算的辅助。</p><p>utils就两个函数，功能是文件大小和指针定位。</p><p>productquantizer, qmatrix文件主要是参数压缩的相关操作，不会涉及这块。</p><h2><b>FastText参数</b></h2><p>下面的参数都是用户可以指定的，简单介绍一下这些参数，具体意义可以看下图中的注释。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-a667af43ca9eb43c6037b750470084a4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"524\" data-rawheight=\"499\" class=\"origin_image zh-lightbox-thumb\" width=\"524\" data-original=\"https://pic1.zhimg.com/v2-a667af43ca9eb43c6037b750470084a4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;524&#39; height=&#39;499&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"524\" data-rawheight=\"499\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"524\" data-original=\"https://pic1.zhimg.com/v2-a667af43ca9eb43c6037b750470084a4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-a667af43ca9eb43c6037b750470084a4_b.jpg\"/></figure><p>loss可以是softmax，h-softmax，negsampling。</p><p>model可以是skipgram(sg)和cbow。</p><p>neg代表负采样的个数。</p><p>wordNgrams代表将一个单词和其后面的词组合在一起，如FastText原理介绍中输入层使用的例子。</p><p>minn和maxn分别代表subwords的最小长度和最大长度，如FastText原理介绍中输入层使用的&#34;喜欢&#34;的例子。</p><p>bucket表示可容纳的subwords和wordNgrams的数量，可以理解成是它们存放的表，与word存放的表是分开的。</p><p>t表示过滤高频词的阈值，像&#34;the&#34;，&#34;a&#34;这种高频但语义很少的词应该过滤掉。</p><p></p>", 
            "topic": [
                {
                    "tag": "word embedding", 
                    "tagLink": "https://api.zhihu.com/topics/20032226"
                }, 
                {
                    "tag": "词向量", 
                    "tagLink": "https://api.zhihu.com/topics/20142325"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/52070686", 
            "userName": "BlockheadLS", 
            "userLink": "https://www.zhihu.com/people/1c46b8d33e1f99c7c8a4f237d27751f9", 
            "upvote": 3, 
            "title": "[论文]Glove这个老古董", 
            "content": "<p>Glove是基于词与词的共现矩阵构造的word embedding模型，可以说是半统计半机器学习。</p><p><b>重点：读Glove一定要说服自己，很多的推导都是intuitive的。</b></p><h2><b>构造损失函数</b></h2><p><img src=\"https://www.zhihu.com/equation?tex=X\" alt=\"X\" eeimg=\"1\"/> 是词共现矩阵，是一个方阵。 <img src=\"https://www.zhihu.com/equation?tex=X_%7Bij%7D\" alt=\"X_{ij}\" eeimg=\"1\"/> 表示词语 <img src=\"https://www.zhihu.com/equation?tex=j\" alt=\"j\" eeimg=\"1\"/> 出现在词语 <img src=\"https://www.zhihu.com/equation?tex=i\" alt=\"i\" eeimg=\"1\"/> 上下文的次数。 <img src=\"https://www.zhihu.com/equation?tex=X_i\" alt=\"X_i\" eeimg=\"1\"/> 表示所有单词出现在词语 <img src=\"https://www.zhihu.com/equation?tex=i\" alt=\"i\" eeimg=\"1\"/> 上下文的次数之和，即</p><p><img src=\"https://www.zhihu.com/equation?tex=X_i%3D%5Csum_%7Bk%7D%5E%7B%7D%7BX_%7Bik%7D%7D\" alt=\"X_i=\\sum_{k}^{}{X_{ik}}\" eeimg=\"1\"/> </p><p>让 <img src=\"https://www.zhihu.com/equation?tex=P_%7Bij%7D\" alt=\"P_{ij}\" eeimg=\"1\"/> 表示词语 <img src=\"https://www.zhihu.com/equation?tex=j\" alt=\"j\" eeimg=\"1\"/> 出现在词语 <img src=\"https://www.zhihu.com/equation?tex=i\" alt=\"i\" eeimg=\"1\"/> 上下文的概率，即</p><p><img src=\"https://www.zhihu.com/equation?tex=P_%7Bij%7D%3DP%28j%7Ci%29%3DX_%7Bij%7D%2FX_i\" alt=\"P_{ij}=P(j|i)=X_{ij}/X_i\" eeimg=\"1\"/> 。</p><p>以下面的例子所示，设 <img src=\"https://www.zhihu.com/equation?tex=i%3Dice\" alt=\"i=ice\" eeimg=\"1\"/> , <img src=\"https://www.zhihu.com/equation?tex=j%3Dsteam\" alt=\"j=steam\" eeimg=\"1\"/> </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-867648522f5c8fd3b2dda59425eef667_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"600\" data-rawheight=\"128\" class=\"origin_image zh-lightbox-thumb\" width=\"600\" data-original=\"https://pic4.zhimg.com/v2-867648522f5c8fd3b2dda59425eef667_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;600&#39; height=&#39;128&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"600\" data-rawheight=\"128\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"600\" data-original=\"https://pic4.zhimg.com/v2-867648522f5c8fd3b2dda59425eef667_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-867648522f5c8fd3b2dda59425eef667_b.jpg\"/></figure><p>（1）如果k和i相关，k和j不相关， <img src=\"https://www.zhihu.com/equation?tex=P_%7Bik%7D%2FP_%7Bjk%7D\" alt=\"P_{ik}/P_{jk}\" eeimg=\"1\"/> 很大；</p><p>（2）如果k和i不相关，k和j相关， <img src=\"https://www.zhihu.com/equation?tex=P_%7Bik%7D%2FP_%7Bjk%7D\" alt=\"P_{ik}/P_{jk}\" eeimg=\"1\"/>很小；</p><p>（3）如果k和i，j都相关，或者都不相关， <img src=\"https://www.zhihu.com/equation?tex=P_%7Bik%7D%2FP_%7Bjk%7D\" alt=\"P_{ik}/P_{jk}\" eeimg=\"1\"/> 接近1。</p><p>从上面的小例子可以看出，相比于原始的概率，概率之比更能区分相关的词(solid and gas)和无关的词(water and fashion)。</p><p>现在我们可以构造一个向量函数，使得：<br/> <img src=\"https://www.zhihu.com/equation?tex=F%28w_i%2Cw_j%2C%5Ctilde%7Bw%7D_k%29%3D%5Cfrac%7BP_%7Bik%7D%7D%7BP_%7Bjk%7D%7D\" alt=\"F(w_i,w_j,\\tilde{w}_k)=\\frac{P_{ik}}{P_{jk}}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=w\" alt=\"w\" eeimg=\"1\"/> 是单词词向量， <img src=\"https://www.zhihu.com/equation?tex=%5Ctilde%7Bw%7D\" alt=\"\\tilde{w}\" eeimg=\"1\"/> 是上下文的词向量，这个意思可以理解为我们要构造的函数应该逼近于数据的统计分布。</p><p>上面等式右边表示了差异性，左边是向量，自然想到向量相减也代表了差异性，于是有：<br/> <img src=\"https://www.zhihu.com/equation?tex=F%28w_i-w_j%2C%5Ctilde%7Bw%7D_k%29%3D%5Cfrac%7BP_%7Bik%7D%7D%7BP_%7Bjk%7D%7D\" alt=\"F(w_i-w_j,\\tilde{w}_k)=\\frac{P_{ik}}{P_{jk}}\" eeimg=\"1\"/> </p><p>，注意到等式右边是一个标量。简单起见，左边也可以构造为：<br/> <img src=\"https://www.zhihu.com/equation?tex=F%28%28w_i-w_j%29%5E%5Ctop%5Ctilde%7Bw%7D_k%29%3D%5Cfrac%7BP_%7Bik%7D%7D%7BP_%7Bjk%7D%7D\" alt=\"F((w_i-w_j)^\\top\\tilde{w}_k)=\\frac{P_{ik}}{P_{jk}}\" eeimg=\"1\"/> </p><p>我们注意到， <img src=\"https://www.zhihu.com/equation?tex=w_i\" alt=\"w_i\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5Ctilde%7Bw%7D_k\" alt=\"\\tilde{w}_k\" eeimg=\"1\"/> 应该是对称的，同时，整个共现矩阵也应该是对称的， <img src=\"https://www.zhihu.com/equation?tex=X%3DX%5E%5Ctop\" alt=\"X=X^\\top\" eeimg=\"1\"/> 。接下来就要构造对称性。</p><p><img src=\"https://www.zhihu.com/equation?tex=F%28%28w_i-w_j%29%5E%5Ctop%5Ctilde%7Bw%7D_k%29%3D%5Cfrac%7BF%28w_i%5E%5Ctop%5Ctilde%7Bw%7D_k%29%7D%7BF%28w_j%5E%5Ctop%5Ctilde%7Bw%7D_k%29%7D\" alt=\"F((w_i-w_j)^\\top\\tilde{w}_k)=\\frac{F(w_i^\\top\\tilde{w}_k)}{F(w_j^\\top\\tilde{w}_k)}\" eeimg=\"1\"/> ，</p><p><img src=\"https://www.zhihu.com/equation?tex=F%28w_i%5E%5Ctop%5Ctilde%7Bw%7D_k%29%3DP_%7Bik%7D%3D%5Cfrac%7BX_%7Bik%7D%7D%7BX_i%7D\" alt=\"F(w_i^\\top\\tilde{w}_k)=P_{ik}=\\frac{X_{ik}}{X_i}\" eeimg=\"1\"/> </p><p>指定满足上式的函数 <img src=\"https://www.zhihu.com/equation?tex=F%3Dexp\" alt=\"F=exp\" eeimg=\"1\"/> ,</p><p><img src=\"https://www.zhihu.com/equation?tex=w_i%5E%5Ctop%5Ctilde%7Bw%7D_k%3Dlog%28P_%7Bik%7D%29%3Dlog%28X_%7Bik%7D%29-log%28X_i%29\" alt=\"w_i^\\top\\tilde{w}_k=log(P_{ik})=log(X_{ik})-log(X_i)\" eeimg=\"1\"/> </p><p>等式右边 <img src=\"https://www.zhihu.com/equation?tex=log%28X_i%29\" alt=\"log(X_i)\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=k\" alt=\"k\" eeimg=\"1\"/> 无关，可以看成是 <img src=\"https://www.zhihu.com/equation?tex=w_i\" alt=\"w_i\" eeimg=\"1\"/> 的偏置项。但是现在还不对称，所以再添加一个 <img src=\"https://www.zhihu.com/equation?tex=%5Ctilde%7Bw%7D_k\" alt=\"\\tilde{w}_k\" eeimg=\"1\"/> 的偏置项，得到，</p><p><img src=\"https://www.zhihu.com/equation?tex=w_i%5E%5Ctop%5Ctilde%7Bw%7D_k%2Bb_i%2B%5Ctilde%7Bb%7D_k%3Dlog%28X_%7Bik%7D%29\" alt=\"w_i^\\top\\tilde{w}_k+b_i+\\tilde{b}_k=log(X_{ik})\" eeimg=\"1\"/> ,</p><p><img src=\"https://www.zhihu.com/equation?tex=X_%7Bik%7D\" alt=\"X_{ik}\" eeimg=\"1\"/> 有很大可能为0，所以一种解决方案是 <img src=\"https://www.zhihu.com/equation?tex=log%28X_%7Bik%7D%29%5Crightarrow+log%281%2BX_%7Bik%7D%29\" alt=\"log(X_{ik})\\rightarrow log(1+X_{ik})\" eeimg=\"1\"/> 。</p><p>但是现在依然有一个问题，就是出现次数多的词对和出现次数少的词对是一样的权重。为此，引入一个权重函数 <img src=\"https://www.zhihu.com/equation?tex=f%28X_%7Bij%7D%29\" alt=\"f(X_{ij})\" eeimg=\"1\"/> 。</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D+++f%28x%29%3D+%5Cleft%5C%7B++++++++++++++++%5Cbegin%7Barray%7D%7B%2A%2Alr%2A%2A%7D++++++++++++++++%28x%2Fx_%7Bmax%7D%29%5E%5Calpha%2C+%26++if%5C+%5C+x+%3C+x_%7Bmax%7D%5C%5C++++++++++++++++1%2C+%26+otherwise%5C%5C++++++++++++++++++++%5Cend%7Barray%7D+++%5Cright.+++%5Cend%7Bequation%7D++\" alt=\"\\begin{equation}   f(x)= \\left\\{                \\begin{array}{**lr**}                (x/x_{max})^\\alpha, &amp;  if\\ \\ x &lt; x_{max}\\\\                1, &amp; otherwise\\\\                    \\end{array}   \\right.   \\end{equation}  \" eeimg=\"1\"/> </p><p>可以看到，权重是有上限的，不会随着频次无限增高，这一点和word2vec丢弃高频词的做法异曲同工。</p><p>最终得到了我们的损失函数,</p><p><img src=\"https://www.zhihu.com/equation?tex=J%3D%5Csum_%7Bi%2Cj%3D1%7D%5E%7BV%7D%7Bf%28X_%7Bij%7D%29%28w_i%5E%5Ctop%5Ctilde%7Bw%7D_j%2Bb_i%2B%5Ctilde%7Bb%7D_j-log%28X_%7Bij%7D%29%29%5E2%7D\" alt=\"J=\\sum_{i,j=1}^{V}{f(X_{ij})(w_i^\\top\\tilde{w}_j+b_i+\\tilde{b}_j-log(X_{ij}))^2}\" eeimg=\"1\"/> </p><p>其中 <img src=\"https://www.zhihu.com/equation?tex=V\" alt=\"V\" eeimg=\"1\"/> 是总的词汇表。然后，使用优化算法更新变量和词向量就可以了。</p><h2><b>Glove和Word2vec殊途同归</b></h2><p>我们试着从word2vec推到glove。这一部分有一些是自己的理解，和论文中也有一些不一致的地方，但不影响对这个topic的理解，可查阅glove论文。</p><p>令 <img src=\"https://www.zhihu.com/equation?tex=Q_%7Bij%7D\" alt=\"Q_{ij}\" eeimg=\"1\"/> 表示词语 <img src=\"https://www.zhihu.com/equation?tex=j\" alt=\"j\" eeimg=\"1\"/> 出现在词语 <img src=\"https://www.zhihu.com/equation?tex=i\" alt=\"i\" eeimg=\"1\"/> 上下文中的概率，也就是</p><p><img src=\"https://www.zhihu.com/equation?tex=Q_%7Bij%7D%3DP%28j%7Ci%29%2C+j%5Cin+context%28i%29\" alt=\"Q_{ij}=P(j|i), j\\in context(i)\" eeimg=\"1\"/> ,</p><p>而word2vec的损失函数也可以表示为</p><p><img src=\"https://www.zhihu.com/equation?tex=J%3D-%5Csum_%7Bi%5Cin+corpus%5C%5Cj%5Cin+context%28i%29%7D%5E%7B%7D%7BlogQ_%7Bij%7D%7D\" alt=\"J=-\\sum_{i\\in corpus\\\\j\\in context(i)}^{}{logQ_{ij}}\" eeimg=\"1\"/> ,</p><p>我们先不管word2vec中对 <img src=\"https://www.zhihu.com/equation?tex=Q_%7Bij%7D\" alt=\"Q_{ij}\" eeimg=\"1\"/> 做的层次softmax和负采样的近似计算。设词语 <img src=\"https://www.zhihu.com/equation?tex=j\" alt=\"j\" eeimg=\"1\"/> 出现在词语 <img src=\"https://www.zhihu.com/equation?tex=i\" alt=\"i\" eeimg=\"1\"/> 语境中的次数为 <img src=\"https://www.zhihu.com/equation?tex=X_%7Bij%7D\" alt=\"X_{ij}\" eeimg=\"1\"/> ，那么上式可以转化为，</p><p><img src=\"https://www.zhihu.com/equation?tex=J%3D-%5Csum_%7Bi%3D1%7D%5E%7BV%7D%7B%5Csum_%7Bj%3D1%7D%5E%7BV%7D%7BX_%7Bij%7DlogQ_%7Bij%7D%7D%7D\" alt=\"J=-\\sum_{i=1}^{V}{\\sum_{j=1}^{V}{X_{ij}logQ_{ij}}}\" eeimg=\"1\"/> ,</p><p>在Glove推导的时候提过， <img src=\"https://www.zhihu.com/equation?tex=X_i%3D%5Csum_%7Bk%7D%5E%7B%7D%7BX_%7Bik%7D%7D%2C+%5C+%5C+%5C+P_%7Bij%7D%3DX_%7Bij%7D%2FX_i\" alt=\"X_i=\\sum_{k}^{}{X_{ik}}, \\ \\ \\ P_{ij}=X_{ij}/X_i\" eeimg=\"1\"/> ， <img src=\"https://www.zhihu.com/equation?tex=V\" alt=\"V\" eeimg=\"1\"/> 是词表。重写上式有，</p><p><img src=\"https://www.zhihu.com/equation?tex=J%3D-%5Csum_%7Bi%3D1%7D%5E%7BV%7D%7BX_%7Bi%7D%5Csum_%7Bj%3D1%7D%5E%7BV%7D%7BP_%7Bij%7DlogQ_%7Bij%7D%7D%7D%3D%5Csum_%7Bi%3D1%7D%5E%7BV%7DX_iH%28P_i%2CQ_i%29\" alt=\"J=-\\sum_{i=1}^{V}{X_{i}\\sum_{j=1}^{V}{P_{ij}logQ_{ij}}}=\\sum_{i=1}^{V}X_iH(P_i,Q_i)\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=H%28P_i%2CQ_%7Bi%7D%29\" alt=\"H(P_i,Q_{i})\" eeimg=\"1\"/> 表示 <img src=\"https://www.zhihu.com/equation?tex=P_i%2CQ_%7Bi%7D\" alt=\"P_i,Q_{i}\" eeimg=\"1\"/> 的交叉熵，到这一步可以看到和Glove损失函数已经有一些相似了。</p><p>我们把 <img src=\"https://www.zhihu.com/equation?tex=H\" alt=\"H\" eeimg=\"1\"/> 替换为一个平方误差就有：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Chat%7BJ%7D%3D%5Csum_%7Bi%2Cj%7D%5E%7B%7D%7BX_i%28%5Chat%7BP%7D_%7Bij%7D-%5Chat%7BQ_%7Bij%7D%7D%29%5E2%7D\" alt=\"\\hat{J}=\\sum_{i,j}^{}{X_i(\\hat{P}_{ij}-\\hat{Q_{ij}})^2}\" eeimg=\"1\"/> ,</p><p>令 <img src=\"https://www.zhihu.com/equation?tex=%5Chat%7BP%7D_%7Bij%7D%3DX_%7Bij%7D\" alt=\"\\hat{P}_{ij}=X_{ij}\" eeimg=\"1\"/>， <img src=\"https://www.zhihu.com/equation?tex=%5Chat%7BQ%7D_%7Bij%7D%3Dexp%28w_i%5E%5Ctop%5Ctilde%7Bw%7D_j%29\" alt=\"\\hat{Q}_{ij}=exp(w_i^\\top\\tilde{w}_j)\" eeimg=\"1\"/> ,并辅以 <img src=\"https://www.zhihu.com/equation?tex=log\" alt=\"log\" eeimg=\"1\"/> ,最终有，</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Chat%7BJ%7D%3D%5Csum_%7Bi%2Cj%7D%5E%7B%7D%7BX_i%28%5Chat%7BP%7D_%7Bij%7D-%5Chat%7BQ_%7Bij%7D%7D%29%5E2%7D%3D%5Csum_%7Bi%2Cj%7D%5E%7B%7D%7BX_i%28w_i%5E%5Ctop%5Ctilde%7Bw%7D_j-logX_%7Bij%7D%29%5E2%7D\" alt=\"\\hat{J}=\\sum_{i,j}^{}{X_i(\\hat{P}_{ij}-\\hat{Q_{ij}})^2}=\\sum_{i,j}^{}{X_i(w_i^\\top\\tilde{w}_j-logX_{ij})^2}\" eeimg=\"1\"/> </p><p>在word2vec中，会对高频词一定概率丢弃，以减少像“the”, “a”这类高频词语义很少的词的影响。基于这个思想，将 <img src=\"https://www.zhihu.com/equation?tex=X_i\" alt=\"X_i\" eeimg=\"1\"/> 改造成 <img src=\"https://www.zhihu.com/equation?tex=f%28X_%7Bij%7D%29\" alt=\"f(X_{ij})\" eeimg=\"1\"/> ,</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Bequation%7D+++f%28x%29%3D+%5Cleft%5C%7B++++++++++++++++%5Cbegin%7Barray%7D%7B%2A%2Alr%2A%2A%7D++++++++++++++++%28x%2Fx_%7Bmax%7D%29%5E%5Calpha%2C+%26++if%5C+%5C+x+%3C+x_%7Bmax%7D%5C%5C++++++++++++++++1%2C+%26+otherwise%5C%5C++++++++++++++++++++%5Cend%7Barray%7D+++%5Cright.+++%5Cend%7Bequation%7D++\" alt=\"\\begin{equation}   f(x)= \\left\\{                \\begin{array}{**lr**}                (x/x_{max})^\\alpha, &amp;  if\\ \\ x &lt; x_{max}\\\\                1, &amp; otherwise\\\\                    \\end{array}   \\right.   \\end{equation}  \" eeimg=\"1\"/> ,</p><figure data-size=\"small\"><noscript><img src=\"https://pic2.zhimg.com/v2-ead575efc19aa2329760c5fb30b2918d_b.jpg\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"838\" data-rawheight=\"516\" class=\"origin_image zh-lightbox-thumb\" width=\"838\" data-original=\"https://pic2.zhimg.com/v2-ead575efc19aa2329760c5fb30b2918d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;838&#39; height=&#39;516&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"small\" data-rawwidth=\"838\" data-rawheight=\"516\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"838\" data-original=\"https://pic2.zhimg.com/v2-ead575efc19aa2329760c5fb30b2918d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-ead575efc19aa2329760c5fb30b2918d_b.jpg\"/></figure><p>最终的损失函数为，</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Chat%7BJ%7D%3D%5Csum_%7Bi%2Cj%7D%5E%7B%7D%7Bf%28X_%7Bij%7D%29%28w_i%5E%5Ctop%5Ctilde%7Bw%7D_j-logX_%7Bij%7D%29%5E2%7D\" alt=\"\\hat{J}=\\sum_{i,j}^{}{f(X_{ij})(w_i^\\top\\tilde{w}_j-logX_{ij})^2}\" eeimg=\"1\"/> </p><p>这就得到了glove的损失函数，所以word2vec和glove其实是在做一件事。</p><p>一些工作也证明了word2vec和glove有相似的效果，如<a href=\"https://link.zhihu.com/?target=http%3A//dsnotes.com/post/glove-enwiki/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">GloVe vs word2vec revisited</a>。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3c0171f44280a9f2fa673769349aa009_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1466\" data-rawheight=\"744\" class=\"origin_image zh-lightbox-thumb\" width=\"1466\" data-original=\"https://pic2.zhimg.com/v2-3c0171f44280a9f2fa673769349aa009_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1466&#39; height=&#39;744&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1466\" data-rawheight=\"744\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1466\" data-original=\"https://pic2.zhimg.com/v2-3c0171f44280a9f2fa673769349aa009_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-3c0171f44280a9f2fa673769349aa009_b.jpg\"/></figure><p>Glove的训练时间比较短，但是对内存要求会很高。</p><h2><b>Glove的致命缺点</b></h2><p>本部分来自于问题 <a href=\"https://www.zhihu.com/question/292482891/answer/492247284\" class=\"internal\">霍华德：word2vec、glove、cove、fastext以及elmo对于知识表达有什么优劣？</a></p><blockquote>glove模型的损失函数：<br/><img src=\"https://www.zhihu.com/equation?tex=loss+%3D+%5Csum_%7Bw_i%2Cw_j%7D%5Cleft%28%5Clangle+%5Cboldsymbol%7Bv%7D_i%2C+%5Cboldsymbol%7B%5Chat%7Bv%7D%7D_j%5Crangle%2Bb_i%2B%5Chat%7Bb%7D_j-%5Clog+X_%7Bij%7D%5Cright%29%5E2\" alt=\"loss = \\sum_{w_i,w_j}\\left(\\langle \\boldsymbol{v}_i, \\boldsymbol{\\hat{v}}_j\\rangle+b_i+\\hat{b}_j-\\log X_{ij}\\right)^2\" eeimg=\"1\"/><br/>在glove模型中，对目标词向量和上下文向量做了区分，并且最后将两组词向量求和，得到最终的词向量。模型中最大的问题在于参数 <img src=\"https://www.zhihu.com/equation?tex=b_i%2Cb_j\" alt=\"b_i,b_j\" eeimg=\"1\"/>也是可训练的参数，这会带来什么问题呢？<br/>看下面的证明：<br/><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Baligned%7D%26%5Csum_%7Bw_i%2Cw_j%7D%5Cleft%28%5Clangle+%5Cboldsymbol%7Bv%7D_i%2C+%5Cboldsymbol%7B%5Chat%7Bv%7D%7D_j%5Crangle%2Bb_i%2B%5Chat%7Bb%7D_j-%5Clog+X_%7Bij%7D%5Cright%29%5E2%5C%5C++%3D%26%5Csum_%7Bw_i%2Cw_j%7D%5Cleft%5B%5Clangle+%5Cboldsymbol%7Bv%7D_i%2B%5Cboldsymbol%7Bc%7D%2C+%5Cboldsymbol%7B%5Chat%7Bv%7D%7D_j%2B%5Cboldsymbol%7Bc%7D%5Crangle%2B%5CBig%28b_i-%5Clangle+%5Cboldsymbol%7Bv%7D_i%2C+%5Cboldsymbol%7Bc%7D%5Crangle+-+%5Cfrac%7B%7C%5Cboldsymbol%7Bc%7D%7C%5E2%7D%7B2%7D%5CBig%29%5Cright.%5C%5C++%26%5Cqquad%5Cqquad%5Cqquad%5Cqquad%5Cleft.%2B%5CBig%28%5Chat%7Bb%7D_j-%5Clangle+%5Cboldsymbol%7B%5Chat%7Bv%7D%7D_j%2C+%5Cboldsymbol%7Bc%7D%5Crangle+-+%5Cfrac%7B%7C%5Cboldsymbol%7Bc%7D%7C%5E2%7D%7B2%7D%5CBig%29-%5Clog+X_%7Bij%7D%5Cright%5D%5E2%5Cend%7Baligned%7D\" alt=\"\\begin{aligned}&amp;\\sum_{w_i,w_j}\\left(\\langle \\boldsymbol{v}_i, \\boldsymbol{\\hat{v}}_j\\rangle+b_i+\\hat{b}_j-\\log X_{ij}\\right)^2\\\\  =&amp;\\sum_{w_i,w_j}\\left[\\langle \\boldsymbol{v}_i+\\boldsymbol{c}, \\boldsymbol{\\hat{v}}_j+\\boldsymbol{c}\\rangle+\\Big(b_i-\\langle \\boldsymbol{v}_i, \\boldsymbol{c}\\rangle - \\frac{|\\boldsymbol{c}|^2}{2}\\Big)\\right.\\\\  &amp;\\qquad\\qquad\\qquad\\qquad\\left.+\\Big(\\hat{b}_j-\\langle \\boldsymbol{\\hat{v}}_j, \\boldsymbol{c}\\rangle - \\frac{|\\boldsymbol{c}|^2}{2}\\Big)-\\log X_{ij}\\right]^2\\end{aligned}\" eeimg=\"1\"/><br/>也就是说，对于glove训练处的词向量加上任意一个常数向量后，它还是这个损失函数的解！这就是很大的问题了，一旦在词向量上加上一个很大的常数向量，那么所有词向量之间就会非常接近，从而失去了词向量的意义。实践中可以发现，对于glove生成的词向量，停用词的模长远大于word2vec词向量的模长。如果下一步，你不过滤停用词，直接把几个词向量加起来求和用作其他任务时（如文本分类），停用词的词向量在求和词向量中占得比重还比较大，这明显很不合理。<br/>可以看出glove这个模型，有借鉴推荐系统中的FM（Factorization Machines）的思想，在推荐系统中，参数 <img src=\"https://www.zhihu.com/equation?tex=b_i%2Cb_j\" alt=\"b_i,b_j\" eeimg=\"1\"/>代表用户对特定商品的偏好，用一个偏移量来描述是合理的，但直接应用到词向量中，就不是很合理了。word2vec里是没有参数 <img src=\"https://www.zhihu.com/equation?tex=b_i%2Cb_j\" alt=\"b_i,b_j\" eeimg=\"1\"/>，所以大家会觉得word2vec效果好些，应用更加普及。</blockquote>", 
            "topic": [
                {
                    "tag": "自然语言处理", 
                    "tagLink": "https://api.zhihu.com/topics/19560026"
                }, 
                {
                    "tag": "词向量", 
                    "tagLink": "https://api.zhihu.com/topics/20142325"
                }, 
                {
                    "tag": "word embedding", 
                    "tagLink": "https://api.zhihu.com/topics/20032226"
                }
            ], 
            "comments": [
                {
                    "userName": "知乎用户", 
                    "userLink": "https://www.zhihu.com/people/0", 
                    "content": "<p>构造对称性那块很迷，原论文也看不懂</p><a class=\"comment_sticker\" href=\"https://pic2.zhimg.com/v2-5cc3f8dc72f94dc7e30391d104ec5319.gif\" data-width=\"\" data-height=\"\">[疑惑]</a>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/51940451", 
            "userName": "BlockheadLS", 
            "userLink": "https://www.zhihu.com/people/1c46b8d33e1f99c7c8a4f237d27751f9", 
            "upvote": 10, 
            "title": "Pearson和Spearman", 
            "content": "<p>我们评价word embedding和sentence embedding时，一个经常用到的方法是相似度评价。对于word embedding来说就是单词对的相似度，对于sentence embedding来说就是计算句子对的相似度。</p><h2><b>数据是什么样子</b></h2><p>一般的形式如下所示：</p><p>a1     b1     human-score1</p><p>a2    b2     human-score2</p><p>a3    b3     human-score3</p><p>....</p><p>a和b可以是单词，也可以是句子，human-score是人为给出的相似得分。human-score是从高到低排列或者从低到高排列，对于Pearson而言human-score有序排列不是必须的，但对于Spearman必须是有序的。</p><p>我们使用word embedding模型和sentence embedding模型得到a和b的embedding表示，之后通过余弦相似度计算它们之间的相似度，得到的形式如下：<br/>a1     b1     human-score1       model-score1</p><p>a2    b2     human-score2      model-score2</p><p>a3    b3     human-score3      model-score3</p><p>....</p><p>现在我们设最后两列序列分别是 <img src=\"https://www.zhihu.com/equation?tex=X\" alt=\"X\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=Y\" alt=\"Y\" eeimg=\"1\"/> 。</p><h2><b>Pearson相关系数</b></h2><p>Pearson系数和Spearman系数都是判断两个序列是否相似的统计工具。但它们的不同点也很明显。</p><p>Pearson主要是把两个序列看成两个变量，计算两个变量是否线性相关。Pearson公式如下：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Crho_%7BXY%7D%3D%5Cfrac%7Bcov%28X%2CY%29%7D%7B%5Csigma_X%5Csigma_Y%7D%3D%5Cfrac%7BE%28XY%29-EXEY%7D%7B%5Csigma_X%5Csigma_Y%7D\" alt=\"\\rho_{XY}=\\frac{cov(X,Y)}{\\sigma_X\\sigma_Y}=\\frac{E(XY)-EXEY}{\\sigma_X\\sigma_Y}\" eeimg=\"1\"/> </p><p>分子是协方差的计算公式，协方差我们知道，是计算两个变量的相关性，大于0是正相关，小于是负相关，相等不相关。除以分母相当于归一化到 <img src=\"https://www.zhihu.com/equation?tex=%5B-1%2C1%5D\" alt=\"[-1,1]\" eeimg=\"1\"/> 之间。</p><h2><b>Spearman等级系数</b></h2><p>Spearman是把<img src=\"https://www.zhihu.com/equation?tex=X\" alt=\"X\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=Y\" alt=\"Y\" eeimg=\"1\"/>就看成两个序列，关注点在于两个序列的单调性是否一致。其实Spearman是把human-score看成一个等级序列，而不管具体的值是什么，如下：</p><p>a1     b1     1</p><p>a2    b2     2</p><p>a3    b3     3</p><p>.....</p><p>Spearman的计算公式如下：<br/> <img src=\"https://www.zhihu.com/equation?tex=%5Crho_s%3D1-%5Cfrac%7B6%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%7Bd_i%5E2%7D%7D%7Bn%28n%5E2-1%29%7D\" alt=\"\\rho_s=1-\\frac{6\\sum_{i=1}^{n}{d_i^2}}{n(n^2-1)}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=d_i%5E2\" alt=\"d_i^2\" eeimg=\"1\"/> 表示两个变量等级的差。Spearman的取值范围也是 <img src=\"https://www.zhihu.com/equation?tex=%28-1%2C1%5D\" alt=\"(-1,1]\" eeimg=\"1\"/> ,它们的顺序完全一样时(每个<img src=\"https://www.zhihu.com/equation?tex=d_i%5E2\" alt=\"d_i^2\" eeimg=\"1\"/> 都为0)就是1，顺序完全相反时就接近于-1。</p><p>这个的证明比较有意思，所以记录一下：</p><p>假设两个等级序列完全相反，那么就是</p><p>序列 <img src=\"https://www.zhihu.com/equation?tex=p%3D1%2C2%2C...%2Cn\" alt=\"p=1,2,...,n\" eeimg=\"1\"/> ,序列 <img src=\"https://www.zhihu.com/equation?tex=q%3Dn%2Cn-1%2C...%2C1\" alt=\"q=n,n-1,...,1\" eeimg=\"1\"/> ,那么序列(假设 <img src=\"https://www.zhihu.com/equation?tex=n\" alt=\"n\" eeimg=\"1\"/> 为奇数)</p><p><img src=\"https://www.zhihu.com/equation?tex=d_i%5E2%3D%281-n%29%5E2%2C%282-%28n-1%29%29%5E2%2C%283-%28n-2%29%29%5E2%2C...%2C2%5E2%2C0%5E2%2C2%5E2...%2C%28%28n-1%29-2%29%5E2%2C%28n-1%29%5E2\" alt=\"d_i^2=(1-n)^2,(2-(n-1))^2,(3-(n-2))^2,...,2^2,0^2,2^2...,((n-1)-2)^2,(n-1)^2\" eeimg=\"1\"/> </p><p>那么</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Csum_%7B%7D%5E%7B%7D%7Bd_i%5E2%7D%3D2%2A%280%5E2%2B2%5E2%2B4%5E2%2B6%5E2%2B...%2B%28n-1%29%5E2%29%3D8%2A%280%5E2%2B1%5E2%2B2%5E2%2B...%2B%28%5Cfrac%7Bn-1%7D%7B2%7D-1%29%5E2%29\" alt=\"\\sum_{}^{}{d_i^2}=2*(0^2+2^2+4^2+6^2+...+(n-1)^2)=8*(0^2+1^2+2^2+...+(\\frac{n-1}{2}-1)^2)\" eeimg=\"1\"/>  ,</p><p>我们先来看看 <img src=\"https://www.zhihu.com/equation?tex=1%5E2%2B2%5E2%2B..%2Bn%5E2\" alt=\"1^2+2^2+..+n^2\" eeimg=\"1\"/> 怎么算，假设n=4，如下图所示，摆成一个正三角形，然后每次60度角翻转两次，最终得到三个正三角形。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-813b9ba035bf0b51f3a7dc8cba07818a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1150\" data-rawheight=\"310\" class=\"origin_image zh-lightbox-thumb\" width=\"1150\" data-original=\"https://pic3.zhimg.com/v2-813b9ba035bf0b51f3a7dc8cba07818a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1150&#39; height=&#39;310&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1150\" data-rawheight=\"310\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1150\" data-original=\"https://pic3.zhimg.com/v2-813b9ba035bf0b51f3a7dc8cba07818a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-813b9ba035bf0b51f3a7dc8cba07818a_b.jpg\"/></figure><p>将三个正三角形每个位置对应相加，每个位置都是9，也就是2n+1，一共有 <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7Bn%28n-1%29%7D%7B2%7D\" alt=\"\\frac{n(n-1)}{2}\" eeimg=\"1\"/> 个，最终 <img src=\"https://www.zhihu.com/equation?tex=1%5E2%2B2%5E2%2B..%2Bn%5E2%3D%5Cfrac%7B1%7D%7B3%7D%2A%5Cfrac%7Bn%28n-1%29%7D%7B2%7D%2A%282n%2B1%29\" alt=\"1^2+2^2+..+n^2=\\frac{1}{3}*\\frac{n(n-1)}{2}*(2n+1)\" eeimg=\"1\"/> 。</p><p>套用到我们的场景中，</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Csum_%7B%7D%5E%7B%7D%7Bd_i%5E2%7D%3D8%2A%280%5E2%2B1%5E2%2B2%5E2%2B...%2B%28%5Cfrac%7Bn-1%7D%7B2%7D-1%29%5E2%29%3D%5Cfrac%7B%28n-1%29%28n-2%29%28n-3%29%7D%7B3%7D\" alt=\"\\sum_{}^{}{d_i^2}=8*(0^2+1^2+2^2+...+(\\frac{n-1}{2}-1)^2)=\\frac{(n-1)(n-2)(n-3)}{3}\" eeimg=\"1\"/> ›</p><p>最终， <img src=\"https://www.zhihu.com/equation?tex=%5Crho_s%3D1-%5Cfrac%7B6%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%7Bd_i%5E2%7D%7D%7Bn%28n%5E2-1%29%7D%3D1-%5Cfrac%7B2%28n-1%29%28n-2%29%28n-3%29%7D%7Bn%28n%5E2-1%29%7D\" alt=\"\\rho_s=1-\\frac{6\\sum_{i=1}^{n}{d_i^2}}{n(n^2-1)}=1-\\frac{2(n-1)(n-2)(n-3)}{n(n^2-1)}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%5Clim_%7Bn+%5Crightarrow+%5Cinfty%7D%7B%5Crho_s%7D%3D-1\" alt=\"\\lim_{n \\rightarrow \\infty}{\\rho_s}=-1\" eeimg=\"1\"/> </p><p>所以Spearman永远都不会取到-1，以上证明对于n为偶数时同样成立。</p><h2><b>举个例子说明Spearman和Pearson的不同</b></h2><p>假设human-score是上升的，model-score可以有如下四种情况：<br/>（1）完美匹配正相关：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-45e6f3d9fb3b3138821d2bb36be4feb6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"460\" data-rawheight=\"322\" class=\"origin_image zh-lightbox-thumb\" width=\"460\" data-original=\"https://pic3.zhimg.com/v2-45e6f3d9fb3b3138821d2bb36be4feb6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;460&#39; height=&#39;322&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"460\" data-rawheight=\"322\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"460\" data-original=\"https://pic3.zhimg.com/v2-45e6f3d9fb3b3138821d2bb36be4feb6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-45e6f3d9fb3b3138821d2bb36be4feb6_b.jpg\"/></figure><p>（2）不完美匹配，但是是正相关。Spearman只关心单调性，所以依然是+1。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-31dc978f9a44452fff85683c5600f0a2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"528\" data-rawheight=\"330\" class=\"origin_image zh-lightbox-thumb\" width=\"528\" data-original=\"https://pic3.zhimg.com/v2-31dc978f9a44452fff85683c5600f0a2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;528&#39; height=&#39;330&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"528\" data-rawheight=\"330\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"528\" data-original=\"https://pic3.zhimg.com/v2-31dc978f9a44452fff85683c5600f0a2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-31dc978f9a44452fff85683c5600f0a2_b.jpg\"/></figure><p>（3）随机，都接近0。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-107a74df07cffe7e030280a916fdc738_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"598\" data-rawheight=\"336\" class=\"origin_image zh-lightbox-thumb\" width=\"598\" data-original=\"https://pic1.zhimg.com/v2-107a74df07cffe7e030280a916fdc738_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;598&#39; height=&#39;336&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"598\" data-rawheight=\"336\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"598\" data-original=\"https://pic1.zhimg.com/v2-107a74df07cffe7e030280a916fdc738_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-107a74df07cffe7e030280a916fdc738_b.jpg\"/></figure><p>（4）完美匹配负相关。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-626eae47978237c74ffff5e31cec65ec_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"484\" data-rawheight=\"324\" class=\"origin_image zh-lightbox-thumb\" width=\"484\" data-original=\"https://pic1.zhimg.com/v2-626eae47978237c74ffff5e31cec65ec_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;484&#39; height=&#39;324&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"484\" data-rawheight=\"324\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"484\" data-original=\"https://pic1.zhimg.com/v2-626eae47978237c74ffff5e31cec65ec_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-626eae47978237c74ffff5e31cec65ec_b.jpg\"/></figure><p>（5）不相关并不等同于独立，Pearson和Spearman都是刻画的是线性相关性，下面 <img src=\"https://www.zhihu.com/equation?tex=%5Crho_%7BXY%7D%3D0%2C%5Crho_s%3D0\" alt=\"\\rho_{XY}=0,\\rho_s=0\" eeimg=\"1\"/> ,但是二次强相关。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-bc5781d50164de5dc9bf16d66a92b165_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"406\" data-rawheight=\"262\" class=\"content_image\" width=\"406\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;406&#39; height=&#39;262&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"406\" data-rawheight=\"262\" class=\"content_image lazy\" width=\"406\" data-actualsrc=\"https://pic2.zhimg.com/v2-bc5781d50164de5dc9bf16d66a92b165_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "自然语言处理", 
                    "tagLink": "https://api.zhihu.com/topics/19560026"
                }, 
                {
                    "tag": "word embedding", 
                    "tagLink": "https://api.zhihu.com/topics/20032226"
                }, 
                {
                    "tag": "词向量", 
                    "tagLink": "https://api.zhihu.com/topics/20142325"
                }
            ], 
            "comments": [
                {
                    "userName": "Freya", 
                    "userLink": "https://www.zhihu.com/people/4be8fbda4f27f19914e4b0adf2e877a8", 
                    "content": "大佬这个是不是只能这么证明啊😭", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "基佛", 
                    "userLink": "https://www.zhihu.com/people/17f35440153ade02125156cf7a669e5d", 
                    "content": "<p>赞！</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "知乎用户", 
                    "userLink": "https://www.zhihu.com/people/0", 
                    "content": "<p>举例有点意思</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/51879600", 
            "userName": "BlockheadLS", 
            "userLink": "https://www.zhihu.com/people/1c46b8d33e1f99c7c8a4f237d27751f9", 
            "upvote": 16, 
            "title": "[论文] ELMo", 
            "content": "<p>     ELMo是context-dependent词向量生成的方法，一发表就成了the-state-of-the-art，论文<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1802.05365.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Deep contextualized word representations</a> 已经被NAACL收录。</p><h2><b>1. 怎样算是一个好的Embedding</b></h2><p>     ELMo的作者认为一个好的embedding需要满足两个条件，(1)能够捕捉到语法和语义信息，(2)能够区分一词多义的情况。</p><h2><b>2. ELMo: Embedding from Language Model</b></h2><p>ELMo的各个词向量表达依赖于输入句子，主要是基于双向语言模型来学习词向量表达。有一个 <img src=\"https://www.zhihu.com/equation?tex=N\" alt=\"N\" eeimg=\"1\"/> 个单词的序列 <img src=\"https://www.zhihu.com/equation?tex=%28t_1%2Ct_2%2C...%2Ct_N%29\" alt=\"(t_1,t_2,...,t_N)\" eeimg=\"1\"/> ，对于某一个单词 <img src=\"https://www.zhihu.com/equation?tex=t_k\" alt=\"t_k\" eeimg=\"1\"/> 用前向语言模型和后向语言模型分别表达就是 <img src=\"https://www.zhihu.com/equation?tex=p%28t_k%7Ct_1%2Ct_2%2C..%2Ct_%7Bk-1%7D%29%2C+p%28t_k%7Ct_%7Bk%2B1%7D%2C...%2Ct_N%29\" alt=\"p(t_k|t_1,t_2,..,t_{k-1}), p(t_k|t_{k+1},...,t_N)\" eeimg=\"1\"/> 。</p><p>      一个句子的前向表示是，</p><p><img src=\"https://www.zhihu.com/equation?tex=p%28t_1%2C...%2Ct_N%29+%3D+%5Cprod_%7Bk%3D1%7D%5E%7BN%7Dp%28t_k%7Ct_1%2C...%2Ct_%7Bk-1%7D%29\" alt=\"p(t_1,...,t_N) = \\prod_{k=1}^{N}p(t_k|t_1,...,t_{k-1})\" eeimg=\"1\"/> </p><p>      一个句子的后向表示是，</p><p><img src=\"https://www.zhihu.com/equation?tex=p%28t_1%2C...%2Ct_N%29%3D%5Cprod_%7Bk%3D1%7D%5E%7BN%7Dp%28t_k%7Ct_%7Bk%2B1%7D%2C...%2Ct_N%29\" alt=\"p(t_1,...,t_N)=\\prod_{k=1}^{N}p(t_k|t_{k+1},...,t_N)\" eeimg=\"1\"/> </p><p>      对于句子 <img src=\"https://www.zhihu.com/equation?tex=s\" alt=\"s\" eeimg=\"1\"/> 我们需要最大化每个单词的前后向表示的概率，如下所示：</p><p><img src=\"https://www.zhihu.com/equation?tex=max%5C+%5Csum_%7Bk%3D1%7D%5E%7Bs_N%7D%7Blog%5C+p%28t_k%7Ct_1%2C...%2Ct_%7Bk-1%7D%3B%5Ctheta_x%3B%5Coverrightarrow%7B%5Ctheta_%7BLSTM%7D%7D%3B%5Ctheta_s%29+%2B+log%5C+p%28t_k%7Ct_%7Bk%2B1%7D%2C...%2Ct_%7Bs_N%7D%3B%5Ctheta_x%3B%5Coverleftarrow%7B%5Ctheta_%7BLSTM%7D%7D%3B%5Ctheta_s%29%7D\" alt=\"max\\ \\sum_{k=1}^{s_N}{log\\ p(t_k|t_1,...,t_{k-1};\\theta_x;\\overrightarrow{\\theta_{LSTM}};\\theta_s) + log\\ p(t_k|t_{k+1},...,t_{s_N};\\theta_x;\\overleftarrow{\\theta_{LSTM}};\\theta_s)}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=s_N\" alt=\"s_N\" eeimg=\"1\"/> 表示句子单词个数, <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_x\" alt=\"\\theta_x\" eeimg=\"1\"/> 表示所有的word embedding表示， <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_s\" alt=\"\\theta_s\" eeimg=\"1\"/> 表示softmax层的参数，这两种参数是共享的。从左向右方向的LSTM <img src=\"https://www.zhihu.com/equation?tex=%5Coverrightarrow%7B%5Ctheta_%7BLSTM%7D%7D\" alt=\"\\overrightarrow{\\theta_{LSTM}}\" eeimg=\"1\"/> 和从右向左方向的 <img src=\"https://www.zhihu.com/equation?tex=%5Coverleftarrow%7B%5Ctheta_%7BLSTM%7D%7D\" alt=\"\\overleftarrow{\\theta_{LSTM}}\" eeimg=\"1\"/> 是两套参数，不共享。</p><p>      对于整个语料库而言，最终需要优化的函数就是</p><p><img src=\"https://www.zhihu.com/equation?tex=max%5C++%5Csum_%7Bs%3D1%7D%5E%7BD%7D%7B%5Csum_%7Bk%3D1%7D%5E%7Bs_N%7D%7Blog%5C+p%28t_k%7Ct_1%2C...%2Ct_%7Bk-1%7D%3B%5Ctheta_x%3B%5Coverrightarrow%7B%5Ctheta_%7BLSTM%7D%7D%3B%5Ctheta_s%29+%2B+log%5C+p%28t_k%7Ct_%7Bk%2B1%7D%2C...%2Ct_%7Bs_N%7D%3B%5Ctheta_x%3B%5Coverleftarrow%7B%5Ctheta_%7BLSTM%7D%7D%3B%5Ctheta_s%29%7D%7D\" alt=\"max\\  \\sum_{s=1}^{D}{\\sum_{k=1}^{s_N}{log\\ p(t_k|t_1,...,t_{k-1};\\theta_x;\\overrightarrow{\\theta_{LSTM}};\\theta_s) + log\\ p(t_k|t_{k+1},...,t_{s_N};\\theta_x;\\overleftarrow{\\theta_{LSTM}};\\theta_s)}}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=D\" alt=\"D\" eeimg=\"1\"/> 是语料库中的句子数， <img src=\"https://www.zhihu.com/equation?tex=s\" alt=\"s\" eeimg=\"1\"/> 表示一个句子，在有些任务中需要加入正则项 <img src=\"https://www.zhihu.com/equation?tex=%5Clambda%7C%7C%5Comega%7C%7C_2%5E2\" alt=\"\\lambda||\\omega||_2^2\" eeimg=\"1\"/> 。</p><p>      前向和后向语言模型可以使用LSTM来实现，也可以用transformer来做，transformer有更强的抽取特征的能力。ELMo采用的是LSTM。</p><h2><b>3. ELMo架构</b></h2><p>ELMo完整的架构如下图所示，主要是由char-CNN和双层Bi-LSTM组成。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-8f8faa16d45338263c6153f86f854917_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2840\" data-rawheight=\"1506\" class=\"origin_image zh-lightbox-thumb\" width=\"2840\" data-original=\"https://pic4.zhimg.com/v2-8f8faa16d45338263c6153f86f854917_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2840&#39; height=&#39;1506&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2840\" data-rawheight=\"1506\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2840\" data-original=\"https://pic4.zhimg.com/v2-8f8faa16d45338263c6153f86f854917_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-8f8faa16d45338263c6153f86f854917_b.jpg\"/></figure><p><b>(1) 为每个单词生成一个context-independent的向量</b></p><p>      这一部分借用了<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1508.06615.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">char-CNN论文</a>中的部分图，并修改了max pooling处错误的地方，并且对highway layers部分左边的箭头表示怀疑多余，如有不对欢迎讨论。</p><p><b>--</b> 最左下矩形是单词‘absurdity’的向量矩阵表示，图中展示了3个宽度为2的卷积核，4个宽度为3的卷积核，5个宽度为4的卷积核，共12个卷积，也就生成12个feature map。论文中使用的2048个卷积核，没有说明每个卷积核的宽度。</p><p><b>--</b> 然后最大池化每个feature map，拼接在一起，生成了一个12维的向量。论文是2048维。</p><p><b>--</b> 之后又加了两层highway layers，highway networks是为了解决神经网络训练时的衰退问题提出来的。highway networks借鉴了LSTM的思想，类似cell，可以让输入直接传到下一层，highway有两个门transform gate和carry gate。 <img src=\"https://www.zhihu.com/equation?tex=T\" alt=\"T\" eeimg=\"1\"/> 是transform gate, <img src=\"https://www.zhihu.com/equation?tex=1-T\" alt=\"1-T\" eeimg=\"1\"/> 是carry gate。 <img src=\"https://www.zhihu.com/equation?tex=H\" alt=\"H\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=T\" alt=\"T\" eeimg=\"1\"/> 都是非线性变换。</p><p><img src=\"https://www.zhihu.com/equation?tex=Y%3DH%28X%2CW_H%29%5Ccdot+T%28X%2CW_T%29+%2B+X%5Ccdot+%281-T%28X%2CW_T%29%29\" alt=\"Y=H(X,W_H)\\cdot T(X,W_T) + X\\cdot (1-T(X,W_T))\" eeimg=\"1\"/> </p><p><b>--</b> 经过两层的非线性变换之后，维度还是2048维，论文中使用了线性映射对其降维，最终每个单词的维度是512维。</p><p>       经过这一步的处理，每一个单词都会生成一个context-independent的词向量，并且由于卷积核有类似于ngram的作用，不同的单词也会产生关系，比如&#34;work&#34;和&#34;works&#34;有相同的部分，char-CNN可以捕捉。之前也有工作做这样的处理，比如word2vec作者Tomas2016年的论文<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1607.04606\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">FastText</a>，利用了subwords的信息。</p><p><b>(2) 使用双向语言模型生成context-dependent词向量</b></p><p><b>-- </b>输入是上一步生成的句子各个单词的context-independent词向量</p><p><b>--</b> 双层的BI-LSTM，每一层都是双向的LSTM，每一层都会生成词向量。黑绿黄代表三层的词向量。</p><p><b>-- </b>ELMO还用到了残差连接，残差连接出现在<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1512.03385\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">论文ResNet</a>，和highway network一样是为了解决神经网络训练深度不能太深的问题。ResNet主要的思想也是让输入无损失的传到更深层的网络，网络主要聚焦于学习残差，</p><p><img src=\"https://www.zhihu.com/equation?tex=y%3DF%28x%2C%5C%7BW_i%5C%7D%29%2Bx\" alt=\"y=F(x,\\{W_i\\})+x\" eeimg=\"1\"/> </p><p>              比如有一个函数F(5.1)=5.2, 相对改变量是很小的，在ResNet论文中也有提到，随着深度增加，梯度的改变越趋近于随机运动，在这个例子中，如果变成H(5.1) = 5 + F(0.1) = 5 + 0.2，那么相对该变量就会是200%，网络对细微的变化更敏感。</p><p>              论文中用ResNet是连接第一层和第二层biLSTM，一直在考虑为什么不是不同的时间步之间加残差连接呢，这一部分需要看看源码是怎样实现的。</p><blockquote>The final model uses L = 2 biLSTM layers with 4096 units and 512 dimension projections and a residual connection from the first to second layer. The final model uses L = 2 biLSTM layers with 4096 units and 512 dimension projections and a residual connection from the first to second layer. </blockquote><p>经过学习之后，就得到了各层次的word embedding表示，底层的word embedding会学到语法信息，上层的word embedding会学习到语义信息。</p><h2><b>4. 怎么用</b></h2><p>          现在ELMo已经学到了单词在各层的表示，一般情况下会根据具体的任务学习一个线性组合，把各层的word embedding组合使用。简单的使用的话可以只用最上面一层。</p><p>          一个 <img src=\"https://www.zhihu.com/equation?tex=L\" alt=\"L\" eeimg=\"1\"/> 层的ELMo其实是有 <img src=\"https://www.zhihu.com/equation?tex=2L%2B1\" alt=\"2L+1\" eeimg=\"1\"/> 个表示，其中包含输入层(也就是char-CNN的输出向量) ， <img src=\"https://www.zhihu.com/equation?tex=2L\" alt=\"2L\" eeimg=\"1\"/> 个前后向LSTM表示，即</p><p><img src=\"https://www.zhihu.com/equation?tex=R_k%3D%5C%7Bx_k%5E%7BLM%7D%2C%5Coverrightarrow%7Bh%7D_%7Bk%2Cj%7D%5E%7BLM%7D%2C%5Coverleftarrow%7Bh%7D_%7Bk%2Cj%7D%5E%7BLM%7D%7Cj%3D1%2C...%2CL%5C%7D%3D%5C%7Bh_%7Bk%2Cj%7D%5E%7BLM%7D%7Cj%3D0%2C...%2CL%5C%7D\" alt=\"R_k=\\{x_k^{LM},\\overrightarrow{h}_{k,j}^{LM},\\overleftarrow{h}_{k,j}^{LM}|j=1,...,L\\}=\\{h_{k,j}^{LM}|j=0,...,L\\}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=R_k\" alt=\"R_k\" eeimg=\"1\"/> 表示单词 <img src=\"https://www.zhihu.com/equation?tex=t_k\" alt=\"t_k\" eeimg=\"1\"/> 的词向量表示集合，<img src=\"https://www.zhihu.com/equation?tex=h_%7Bk%2C0%7D%5E%7BLM%7D\" alt=\"h_{k,0}^{LM}\" eeimg=\"1\"/> 表示输入层词向量，每一个biLSTM有 <img src=\"https://www.zhihu.com/equation?tex=h_%7Bk%2Cj%7D%5E%7BLM%7D%3D%5B%5Coverrightarrow%7Bh%7D_%7Bk%2Cj%7D%5E%7BLM%7D%3B%5Coverleftarrow%7Bh%7D_%7Bk%2Cj%7D%5E%7BLM%7D%5D\" alt=\"h_{k,j}^{LM}=[\\overrightarrow{h}_{k,j}^{LM};\\overleftarrow{h}_{k,j}^{LM}]\" eeimg=\"1\"/> ，对这个地方有些怀疑，为什么拼接呢，不是相加吗，有点讲不通啊。欢迎讨论。</p><p>          对于某一个具体的任务，对这些embedding计算一个线性加权和，</p><p><img src=\"https://www.zhihu.com/equation?tex=ELMo_k%5E%7Btask%7D%3D%5Cgamma%5E%7Btask%7D%5Csum_%7Bj%3D0%7D%5E%7BL%7D%7Bs_j%5E%7Btask%7Dh_%7Bk%2Cj%7D%5E%7BLM%7D%7D\" alt=\"ELMo_k^{task}=\\gamma^{task}\\sum_{j=0}^{L}{s_j^{task}h_{k,j}^{LM}}\" eeimg=\"1\"/> </p><p>其中， <img src=\"https://www.zhihu.com/equation?tex=s_j%5E%7Btask%7D\" alt=\"s_j^{task}\" eeimg=\"1\"/> 表示各层权重，<img src=\"https://www.zhihu.com/equation?tex=%5Cgamma%5E%7Btask%7D\" alt=\"\\gamma^{task}\" eeimg=\"1\"/> 表示缩放因子。 <img src=\"https://www.zhihu.com/equation?tex=s_j%5E%7Btask%7D\" alt=\"s_j^{task}\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5Cgamma%5E%7Btask%7D\" alt=\"\\gamma^{task}\" eeimg=\"1\"/> 对于某个任务应该是固定的，也就是所有的单词会共享这两种参数。论文中说<img src=\"https://www.zhihu.com/equation?tex=%5Cgamma%5E%7Btask%7D\" alt=\"\\gamma^{task}\" eeimg=\"1\"/>的作用比较明显，现在还没有想通为什么。</p><p></p>", 
            "topic": [
                {
                    "tag": "自然语言处理", 
                    "tagLink": "https://api.zhihu.com/topics/19560026"
                }, 
                {
                    "tag": "word embedding", 
                    "tagLink": "https://api.zhihu.com/topics/20032226"
                }
            ], 
            "comments": [
                {
                    "userName": "简枫", 
                    "userLink": "https://www.zhihu.com/people/147fc19c3801b056e6437864322c8836", 
                    "content": "<p>大佬！</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "WALLE爱劳动", 
                    "userLink": "https://www.zhihu.com/people/5587fe9dd1c65a31b93bffee3c03f8c0", 
                    "content": "学到了，跪谢大佬[惊喜]", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "WALLE爱劳动", 
                    "userLink": "https://www.zhihu.com/people/5587fe9dd1c65a31b93bffee3c03f8c0", 
                    "content": "关于文中提到的每一层bilstm两个方向输出，在输入下一层时为什么是拼接而不是加和的问题我做了一些思考。简单来说，这个跟引入cnn提取特征有很大关系，经典RNN中词向量每个维度的特征在处理过程中其特征含义是不变的，他主要是链接上下文信息更新词向量的特征信息。而cnn不同的是，它基于感知不同特征之间变化的纹理信息(联想在图像处理中的场景，可以这么理解)进行特征的高阶重构。在这个过程中词向量的特征属性由低阶属性更新成了更高阶的属性，每个位置的特征含义自然也发生了变化。类比“我喜欢苹果”，\"我\"的词向量从(是人类，吃东西，爱吃东西，不可食用，食用无毒副)已经更新成了\"吃货\"，而\"苹果\"则由(不是人类，不能吃东西，可食用，食用有益健康)更新成了(“健康食品”)，(吃货)和(健康食品)两个词向量自然是不能简单得相加。这是我的个人理解。", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "kaiyuan", 
                    "userLink": "https://www.zhihu.com/people/043cbfa23e392c322cdcae163b8673d5", 
                    "content": "<p>这里双向lstm相加和拼接都是可以的，只不过大家都用的拼接多一些。可以参考tf.keras.layers.Bidirectional</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "overfiter", 
                    "userLink": "https://www.zhihu.com/people/921e3d8a20e6f04d8a6eb7e4816e63e6", 
                    "content": "<p>在2L+1个向量中，其中2L个可以拼接，比如拼接之后是2*m维的向量，那剩下那个Xk(也就是char-CNN的输出向量)的维度，不一定是2*m吧。如果维数不同，后面怎么加权相加呢？</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "BlockheadLS", 
                            "userLink": "https://www.zhihu.com/people/1c46b8d33e1f99c7c8a4f237d27751f9", 
                            "content": "<p>LSTM的隐状态维度应该是可以和输入维度不一样的</p>", 
                            "likes": 1, 
                            "replyToAuthor": "overfiter"
                        }
                    ]
                }, 
                {
                    "userName": "知乎用户", 
                    "userLink": "https://www.zhihu.com/people/0", 
                    "content": "<p>lstm的残差连接，github上HIT-SCIR/ELMoForManyLangs的elmo.py源码中，是第一层lstm的输出加到第二层lstm的输出，相当于图中从绿点到黄点，而不是从黑点到黄点</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "BlockheadLS", 
                            "userLink": "https://www.zhihu.com/people/1c46b8d33e1f99c7c8a4f237d27751f9", 
                            "content": "<p>赞，我看一下</p>", 
                            "likes": 0, 
                            "replyToAuthor": "知乎用户"
                        }
                    ]
                }
            ]
        }
    ], 
    "url": "https://zhuanlan.zhihu.com/c_1045250051953680384"
}
