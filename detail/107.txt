{
    "title": "人工智能算法实践", 
    "description": "", 
    "followers": [
        "https://www.zhihu.com/people/duan-shui-liu-da-shi-xiong-24-62", 
        "https://www.zhihu.com/people/qiao-hai-jun", 
        "https://www.zhihu.com/people/xingzhou92", 
        "https://www.zhihu.com/people/lishuai8", 
        "https://www.zhihu.com/people/tfeima", 
        "https://www.zhihu.com/people/xie-xiao-wen-44-89", 
        "https://www.zhihu.com/people/xing-kong-46-58-8", 
        "https://www.zhihu.com/people/ji-an-86-27", 
        "https://www.zhihu.com/people/lu-xiao-bu-82-4", 
        "https://www.zhihu.com/people/debuluoyi_AI", 
        "https://www.zhihu.com/people/stackbox", 
        "https://www.zhihu.com/people/wang-aaron-90", 
        "https://www.zhihu.com/people/wang-zhi-xiang-25", 
        "https://www.zhihu.com/people/hui-yi-da-hai", 
        "https://www.zhihu.com/people/nlp-12-57", 
        "https://www.zhihu.com/people/foreveryoungsha", 
        "https://www.zhihu.com/people/zhu-chen-71-40", 
        "https://www.zhihu.com/people/yun-55-61", 
        "https://www.zhihu.com/people/xiao-xue-sheng-40-11", 
        "https://www.zhihu.com/people/xiaoyueai", 
        "https://www.zhihu.com/people/fan-zhi-fang-25", 
        "https://www.zhihu.com/people/xie-wen-cai-35", 
        "https://www.zhihu.com/people/yun-lang-22-49", 
        "https://www.zhihu.com/people/liudongze-50", 
        "https://www.zhihu.com/people/yang-troy-89", 
        "https://www.zhihu.com/people/williamwhe", 
        "https://www.zhihu.com/people/fan-jiang-ke-14", 
        "https://www.zhihu.com/people/chen-xiao-er-72-56", 
        "https://www.zhihu.com/people/huang-sheng-sheng", 
        "https://www.zhihu.com/people/jayden-li", 
        "https://www.zhihu.com/people/spirit-soul", 
        "https://www.zhihu.com/people/wei-yuan-88-25", 
        "https://www.zhihu.com/people/li-peng-41-64", 
        "https://www.zhihu.com/people/qing-dao-3", 
        "https://www.zhihu.com/people/mlxue-xi", 
        "https://www.zhihu.com/people/bai-ji-19", 
        "https://www.zhihu.com/people/4recommend", 
        "https://www.zhihu.com/people/xu-xiao-xian-67", 
        "https://www.zhihu.com/people/posong", 
        "https://www.zhihu.com/people/urika-bupt", 
        "https://www.zhihu.com/people/Tu-FF", 
        "https://www.zhihu.com/people/qiu-qiu-10-63-51", 
        "https://www.zhihu.com/people/shishi-75-12", 
        "https://www.zhihu.com/people/wang-zhe-68-63", 
        "https://www.zhihu.com/people/boojum", 
        "https://www.zhihu.com/people/cr777-37", 
        "https://www.zhihu.com/people/jiayue-chen", 
        "https://www.zhihu.com/people/gu-tian-94", 
        "https://www.zhihu.com/people/zhe-bie-55", 
        "https://www.zhihu.com/people/lei-xiao-yu-49", 
        "https://www.zhihu.com/people/damon-data", 
        "https://www.zhihu.com/people/yifdu", 
        "https://www.zhihu.com/people/wang-feng-13-68", 
        "https://www.zhihu.com/people/shiyi_gao", 
        "https://www.zhihu.com/people/emily-wang-22", 
        "https://www.zhihu.com/people/hen-ji-97", 
        "https://www.zhihu.com/people/roy_sung", 
        "https://www.zhihu.com/people/ren-ni-cai", 
        "https://www.zhihu.com/people/wo-ying-wei-wang", 
        "https://www.zhihu.com/people/mine-zhang", 
        "https://www.zhihu.com/people/mo-xiao-shan", 
        "https://www.zhihu.com/people/zhu-dao-lu-32", 
        "https://www.zhihu.com/people/liu-fei-94-95", 
        "https://www.zhihu.com/people/zbxiao-zhai-nan", 
        "https://www.zhihu.com/people/yao-yuan-17-92", 
        "https://www.zhihu.com/people/chen-chun-mei-45-53", 
        "https://www.zhihu.com/people/monkeydeking", 
        "https://www.zhihu.com/people/greg-51-42", 
        "https://www.zhihu.com/people/martinson", 
        "https://www.zhihu.com/people/aaron-alexand", 
        "https://www.zhihu.com/people/hobo-30-10", 
        "https://www.zhihu.com/people/li-yi-83-91", 
        "https://www.zhihu.com/people/henry-chen-2", 
        "https://www.zhihu.com/people/ji-dong-qi-22", 
        "https://www.zhihu.com/people/luchi007", 
        "https://www.zhihu.com/people/chen-chen-86-65", 
        "https://www.zhihu.com/people/johndream", 
        "https://www.zhihu.com/people/motuo", 
        "https://www.zhihu.com/people/lyu-xin-wei", 
        "https://www.zhihu.com/people/drinkiit", 
        "https://www.zhihu.com/people/zhang-chun-hui-95-92", 
        "https://www.zhihu.com/people/da-shui-fa-52", 
        "https://www.zhihu.com/people/wang-lang-96-64", 
        "https://www.zhihu.com/people/jkl-29", 
        "https://www.zhihu.com/people/sharpen-chen", 
        "https://www.zhihu.com/people/ni-wei-tai-yang", 
        "https://www.zhihu.com/people/shuhe-5-56", 
        "https://www.zhihu.com/people/yc-cq", 
        "https://www.zhihu.com/people/xiao-tao-47-89", 
        "https://www.zhihu.com/people/niceworld-49-6", 
        "https://www.zhihu.com/people/zhang-jia-yi-6-82", 
        "https://www.zhihu.com/people/HYDXT", 
        "https://www.zhihu.com/people/a-wei-jeffrey", 
        "https://www.zhihu.com/people/lijial", 
        "https://www.zhihu.com/people/IFYoung", 
        "https://www.zhihu.com/people/xiao-xiao-de-sha-zi-43", 
        "https://www.zhihu.com/people/chen-long-xi-6", 
        "https://www.zhihu.com/people/shi-kai-lin-33", 
        "https://www.zhihu.com/people/vsionchiou", 
        "https://www.zhihu.com/people/duan-xing-99", 
        "https://www.zhihu.com/people/liu-ze-ping-233", 
        "https://www.zhihu.com/people/willwinworld", 
        "https://www.zhihu.com/people/shipmaster-33", 
        "https://www.zhihu.com/people/zhong-er-cheng-xu-yuan", 
        "https://www.zhihu.com/people/oliver-zhang-40", 
        "https://www.zhihu.com/people/zhi-mu-8", 
        "https://www.zhihu.com/people/qing-feng-zhi-yu", 
        "https://www.zhihu.com/people/dtsci", 
        "https://www.zhihu.com/people/mr-lin-82-68", 
        "https://www.zhihu.com/people/xi-hong-shi-ji-dan-mian-46", 
        "https://www.zhihu.com/people/yan-ya-chen", 
        "https://www.zhihu.com/people/ceng-zhao-43", 
        "https://www.zhihu.com/people/gu-hai-shuo-81", 
        "https://www.zhihu.com/people/hs-chen-40", 
        "https://www.zhihu.com/people/kktong-67", 
        "https://www.zhihu.com/people/kim-74-51", 
        "https://www.zhihu.com/people/su-xiao-run", 
        "https://www.zhihu.com/people/lain01", 
        "https://www.zhihu.com/people/michael-wu-25", 
        "https://www.zhihu.com/people/liu-da-wei-40-78", 
        "https://www.zhihu.com/people/tao-yu-16-44", 
        "https://www.zhihu.com/people/yanghuaizhi", 
        "https://www.zhihu.com/people/cctv-87", 
        "https://www.zhihu.com/people/POLYDOGE", 
        "https://www.zhihu.com/people/tao-wei-73-55", 
        "https://www.zhihu.com/people/iverson-allen-75", 
        "https://www.zhihu.com/people/zhang-ming-feng-91", 
        "https://www.zhihu.com/people/pray-90-47", 
        "https://www.zhihu.com/people/liu-wen-yi-81", 
        "https://www.zhihu.com/people/SecondaryMarquis", 
        "https://www.zhihu.com/people/zhang-yue-29-56"
    ], 
    "article": [
        {
            "url": "https://zhuanlan.zhihu.com/p/88397924", 
            "userName": "DataFunTalk", 
            "userLink": "https://www.zhihu.com/org/09843313d8c5eff1b7d8bcfa65dc8b68", 
            "upvote": 6, 
            "title": "360展示广告召回系统的演进", 
            "content": "<p><b>导读：</b>随着展示广告业务数据量的日益增长，360展示广告召回系统也随之也进行不断升级改进。本次介绍主要从召回系统演进的角度详细阐述工程实践中的算法应用、技术难点以及解决方案。主要分成三块：第一个是360展示广告的大致介绍，第二个是整体架构介绍，第三个就是今天的主题召回模块的演进脉络。</p><p><b>▌1. 展示广告介绍</b></p><p><b>1.1 展示广告业务介绍</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-0734c3e287b47e0f5908a610a3dfedb8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"471\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-0734c3e287b47e0f5908a610a3dfedb8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;471&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"471\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-0734c3e287b47e0f5908a610a3dfedb8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-0734c3e287b47e0f5908a610a3dfedb8_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>展示广告主要是在RTB程序化购买框架下运行的，首先有媒体方，比如新浪、搜狐等市面上的各大媒体，我们都已经接入。媒体在一次曝光产生之前会把这次曝光发送给Ad Exchange（ADX）模块进行拍卖，360的ADX平台叫360 Max。ADX平台将该曝光发送给多个竞价平台DSP，DSP来决定是否对该次曝光进行竞价，360的DSP平台叫360点睛DSP。广告主们会针对自己的需求设置广告投放策略，DSP平台会从广告主设置的广告投放库中根据该次曝光的特征匹配出合适的创意（广告）候选集返回给ADX。一般要求DSP的响应时间为100ms左右，除去网络传输耗时，留给DSP模型的时间只有几十毫秒，其中间各个模块的时间就更少。ADX从接收到的所有候选集中选择出价最高的广告进行曝光。</p><p><b>1.2 常见展示广告</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3e267395a15af539cbbdbe10389debb1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"494\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-3e267395a15af539cbbdbe10389debb1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;494&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"494\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-3e267395a15af539cbbdbe10389debb1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-3e267395a15af539cbbdbe10389debb1_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>目前在360场景下主要有这几种广告类型，第一种类型是广告侧边栏，包含具体投放广告的详情页。第二种类型是整体的开屏广告，主要是品牌广告。第三种类型是在信息流场景下，将广告嵌入新闻上下文中且与文章内容形式保持一致。</p><p><b>▌2. 展示广告整体架构介绍</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-5417f3885734b6176280915541948ccf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"604\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-5417f3885734b6176280915541948ccf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;604&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"604\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-5417f3885734b6176280915541948ccf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-5417f3885734b6176280915541948ccf_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>整体架构流程：</p><p>流量从ADX发送给DSP端，DSP拿到流量之后交给检索召回模块，我们叫Ad Search模块，Ad Search模块分为两个部分，一个是Ad Search Root模块，对流量进行识别，判断其为哪种类型的流量，比如keyword流量、信息流流量、banner流量，然后交给不同级别的Ad Search Leaf模块对广告进行召回，选出相匹配的广告初步候选集，该模块就是我们今天的主题。然后将选出的广告初步候选集交给Ad Selector模块进行精排，即对广告的CTR或者CVR预估，进行打分，选出top K个广告返回给DSP Server。同时DSP Server会将点击、曝光、后续的日志写入kafka，并进行日志落地，日志落地后会进入后续的ETL离线流程，用于后续粗排、点击率等模型生成训练样本使用，另一部实时数据日志数据流会经过反作弊模块后过一遍ctr预估的online learning和实时曝光反馈的online feedback。 </p><p><b>▌3. 召回模块的演进脉络</b></p><p><b>3.1 检索召回模块</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-76a7224390913a5cd652d512870d384d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"501\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-76a7224390913a5cd652d512870d384d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;501&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"501\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-76a7224390913a5cd652d512870d384d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-76a7224390913a5cd652d512870d384d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>Ad Search Leaf召回模块展开来主要分三个阶段，召回、过滤和粗排。召回模块接收到DSP给流量请求中包含了用户信息和上下文信息。广告主设置的广告投放存在Ad Meta db中，然后建成相关的广告索引（Ad index），投放更新后才会实时更新这个索引。RTDB主要存储用户标签数据，标签数据一部分是通过请求中带的用户信息和上下文信息进行用户标签的实时更新，另一部分是通过线下离线全量更新。召回模块就是结合这两端选出初步的广告候选集，然后进入过滤模块（正排模块），过滤方法主要包括基于规则、黑白名单、广告主预算pacing过滤。最后进入粗排模块，对初选的广告候选集按评价函数模型进行打分，但没有精排模块那么复杂，相对比较简单，比如最开始基于核心特征的LR模型，后续升级过基于特征交叉的FFM模型。</p><p><b>3.2 召回通路</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-1e2bc3d2477a37e16b5375165cb6532d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"524\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-1e2bc3d2477a37e16b5375165cb6532d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;524&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"524\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-1e2bc3d2477a37e16b5375165cb6532d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-1e2bc3d2477a37e16b5375165cb6532d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>接下来讲一下在我们检索召回中一些通路，我们是进行多路召回的，其中有三种类型，第一种是上下文、第二种是用户行为、第三种是当前正在做的深度召回。</p><p>上下文召回有这几种类型，第一种是基于图片的，在内容页场景下，一个明星穿了哪件衣服，根据这件衣服投放哪件商品，做法是将图片向量化，计算广告商品与图片向量的相似度进行召回。第二种是基于标题的，主要是基于文本NLP相关模型进行召回。第三种是基于lbs的，广告主自身设定某个标签区域进行投放，在该区域内进行标签匹配召回，属于布尔召回。</p><p>用户行为召回有这几种类型，第一种是基于兴趣的，基于用户历史行为建立用户画像，打上兴趣标签，进行布尔召回。第二种是基于Query的，利用用户的query历史行为，进行NLP相关模型进行召回，与基于标题的方式类似。第三种是基于访问行为，利用广告主回传的用户商品行为，采用Item CF、ALS、Neural MF等模型进行召回。</p><p>最后一种召回通路是深度召回，主要是把user profile、媒体特性、上下文特性等特征结合起来进入深度模型中进行召回，接下来会具体介绍下这个是如何做的。</p><p><b>3.3 基于文本的召回</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-80cec4759c70bbe09abe3e1c5e5f03a2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"442\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-80cec4759c70bbe09abe3e1c5e5f03a2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;442&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"442\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-80cec4759c70bbe09abe3e1c5e5f03a2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-80cec4759c70bbe09abe3e1c5e5f03a2_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>上面讲的标题和query召回主要是文本召回，包括几种，第一种是精准匹配，有完全匹配和基于ngram的TF-IDF提取核心词匹配，属于比较简单的方式。第二种是模糊匹配，有word2vec和DSSM语义化模型，将文本语义向量化，然后按向量化检索召回。第三种是广泛匹配，是把语义化向量聚类成多个标签，然后按标签召回。</p><p><b>3.4 召回模块演进</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-5f97a0b7459d38288e93cd5ebe8dab61_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"444\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-5f97a0b7459d38288e93cd5ebe8dab61_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;444&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"444\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-5f97a0b7459d38288e93cd5ebe8dab61_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-5f97a0b7459d38288e93cd5ebe8dab61_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>我们的召回模块演进主要是分成三个阶段，第一种是布尔召回，刚才介绍的信息标签类型和lbs都是基于这种。第二种是向量检索召回，利用深度学习模型将广告、用户等信息都映射为向量，然后进行向量检索召回。第三种是我们现在在做的基于深度树的匹配。三个阶段是由浅入深的过程，接下来我们依次介绍这三个阶段。</p><p><b>3.5 布尔召回</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-5fc85e3ac0ee92adf309fbfcd77a586b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"517\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-5fc85e3ac0ee92adf309fbfcd77a586b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;517&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"517\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-5fc85e3ac0ee92adf309fbfcd77a586b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-5fc85e3ac0ee92adf309fbfcd77a586b_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>布尔召回在早期检索中都会用到的，我们的方式是基于树+维度bitMap分组+哈希表。广告主设置定向组合，比如访问某些网站的人群、有特定兴趣的人群等其他各种定向组合。而请求中会带有用户标签，问题就变成了怎么找到与用户标签匹配的定向组合广告？布尔召回本质就是基于倒排索引的布尔运算，所以关键在于倒排索引怎么构建？构建的方式有两个层级，第一层索引是将广告主的投放配置进行分解分组，每一个组为一个conjunction，一个广告投放会对应多个conjunction，这样就可以建立conjunction到广告投放的倒排索引。第二层索引在于根据用户标签找到对应的conjunction，我们的标签是一个int64类型的整数，高八位的某些bit位会做一些标识，来区分这个标签属于哪种类型，拿到用户的标签做位运算，找到bitMap分组的conjunction list。然后到基于每个conjunction到第一层取出对应的广告主集合，最后计算每个集合的交并，得到最终召回的广告候选集。</p><p>在实际使用中布尔召回有一些问题，比如倒排表有部分conjunction对应的广告集合很长（&gt;1w），检索性能很差，我们的实际解决方案是在布尔表达式中做一个转换，比如将先并后交的布尔运算改为先交后并的布尔运算，能极大优化检索的性能。</p><p><b>3.6 向量化召回</b></p><p>我们在实际向量化召回中有两种类型，第一种是类似于早期YouTube DNN，把用户变成一个向量，把item也变成一个向量，最后做一个向量相似度检索。第二种是用户相关历史内容变成一个向量，再进行向量相似度检索。两种类型都是基于两个向量的相似度进行检索，得到item候选集。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-85d210b21ada8411eb815b97190fe41f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"460\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-85d210b21ada8411eb815b97190fe41f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;460&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"460\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-85d210b21ada8411eb815b97190fe41f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-85d210b21ada8411eb815b97190fe41f_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>实际中我们选择的是微软提出的基于深度语义检索模型（DSSM），该模型的输入是一组相关的Query和Document，以及两个与Query不相关的Documents，然后分别dense为embedding向量，再经过NN网络，再做softmax打分，得到三个分数，训练目标就是是相关的Document分数尽可能高，不相关的Documents尽可能低。中间的NN层可以有多个选择，可以为多层FC，也可以类似textcnn那样为CNN+FC，或者为RNN。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-420a3863c936859739d18e9dd5c038b9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"604\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-420a3863c936859739d18e9dd5c038b9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;604&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"604\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-420a3863c936859739d18e9dd5c038b9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-420a3863c936859739d18e9dd5c038b9_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>在通过模型向量化之后就涉及到向量索引该怎么选择？下面介绍下常见的几种。第一种为LSH（局部敏感哈希）索引，方法就是基于多个哈希函数进行分桶，比较耗内存。第二种为faiss提出的IVF Flat，方法是构建由聚类中心向量构成的倒排表，针对输入向量，先找到聚类中心向量，再基于KNN方式进行检索，这种类型是保留精度的，效果跟直接暴力检索非常接近，但性能有很大的提升。第三种为IVF PQ，也是基于倒排，与IVF Flat区别在于在向量上做了有损压缩或PC降维，好处在于省内存，但是带来了实际检索精度的丢失。这三种可以根据实际业务场景进行选择。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-e711eb5f97163ba542b384ad9def63e4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-e711eb5f97163ba542b384ad9def63e4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;606&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-e711eb5f97163ba542b384ad9def63e4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-e711eb5f97163ba542b384ad9def63e4_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>3.7 深度树召回</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-95b4c8b7fa71eab6a7f99199a08d30b9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1031\" data-rawheight=\"407\" class=\"origin_image zh-lightbox-thumb\" width=\"1031\" data-original=\"https://pic2.zhimg.com/v2-95b4c8b7fa71eab6a7f99199a08d30b9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1031&#39; height=&#39;407&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1031\" data-rawheight=\"407\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1031\" data-original=\"https://pic2.zhimg.com/v2-95b4c8b7fa71eab6a7f99199a08d30b9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-95b4c8b7fa71eab6a7f99199a08d30b9_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>上面讲的布尔召回和向量化召回都有一定的缺点，比如布尔召回的有些conjunction的候选列表非常长，容易遇到性能瓶颈。向量检索会局限于模型的向量表达是否准确，而且向量空间有限。这里参考了阿里妈妈提出的深度树匹配（TDM）模型，深度树匹配能解决全量检索的性能问题和模型精度问题。如何构建一个深度检索树呢？在广告场景下选取的是基于ecpm（千次曝光预期收益）最大堆方式，最大堆树下当前层最优Top K孩子节点的父亲必然属于上层父节点的最优Top K，这样就可以从根节点逐层递归向下挑选Top K直至叶子层，同时极大提高检索速度。为了简便起见，我们构建树的形式是二叉树，则检索Top K的时间复杂度是2KlogN，N为item个数，且随着N的增长，相比暴力全量检索的性能提升更为明显。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-e0a5f843ea20c586204da88193b63514_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-e0a5f843ea20c586204da88193b63514_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;606&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-e0a5f843ea20c586204da88193b63514_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-e0a5f843ea20c586204da88193b63514_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>深度树模型的大致结构如上图所示，分为两部分，左边是流量端，包括用户profile特征（如标签、cookie历史行为、频次等）和上下文特征（如媒体广告位属性、黑白名单、时间特性等）。首先把这两种特征dense化变成embedding，再做特征交叉组合，在我们场景下特征交叉采用的是IPNN方式，然后再经过FC+BN的多层NN网络，最终转化为Search端的embedding，线上处理时，左边这个流程每次请求都会计算。右边是候选集端，是一颗检索树，叶节点就包含了广告投放信息，包括尺寸、类目、行业等广告主设置的定向信息，叶节点和父节点本质都是同一维度的embedding，都会进入FC+BN层，形成与Search端embedding相同维度的Tree Leaf和Parent embedding。左右两端的embedding做交叉+concat操作进入FC+BN层，最终得到一个分数，与样本label按交叉熵损失进行训练。整体的模型架构就是这样。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-50c307a761dc24104b73a0a072cb041d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"608\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-50c307a761dc24104b73a0a072cb041d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;608&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"608\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-50c307a761dc24104b73a0a072cb041d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-50c307a761dc24104b73a0a072cb041d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>那我们要怎么准备这颗树模型的样本呢？有两部分，分别为leaf样本和parent样本，对于leaf节点，选取精排模型输出的虚拟曝光Top3作为正例。负例包括两部分，第一部分是在正例叶子节点同层，随机选取其他叶子节点作为负例，第二部分是选择粗排阶段的一些负分item标识为负例。对于父节点，基于ecpm最大堆原理，将正例向上回溯的所有父节点都标记为正例，同层随机标记为负例。整体的样本规模是用了7天的采样曝光样本，大概在5000千万左右。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-430f7b72a67818d66c08e127cbfc3ef1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"604\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-430f7b72a67818d66c08e127cbfc3ef1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;604&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"604\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-430f7b72a67818d66c08e127cbfc3ef1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-430f7b72a67818d66c08e127cbfc3ef1_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>接下来我们该如何生成这颗树呢？我们这里有两种方式，第一种方式就是随机生成，将item随机放到叶结点上，当然也可以根据标签体系手动把相近的item放在同一个父节点下，基于随机深度树训练一轮，将叶子结点的embedding导出来，然后聚类，这样就可以根据聚类的结果重新生成一颗树，然后再去训练，一直迭代到指标变化不大。第二种方式就是自下而上合成树，先当这颗树不存在，只训练每个叶子结点的embedding，然后根据叶子结点的embedding向上聚类回溯成一颗树（kmeans或者根据曝光频次来聚类），接着就可以与第一种方式一样一直迭代直到指标变化不大。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-dd9381cd80ea0b06ad806d87a80f681c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"603\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-dd9381cd80ea0b06ad806d87a80f681c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;603&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"603\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-dd9381cd80ea0b06ad806d87a80f681c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-dd9381cd80ea0b06ad806d87a80f681c_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>我们最后选择方式是第二种方式，第一步就是先训练叶子，把叶子embedding导出来，然后聚类生成一颗树，基于这颗树结构重新训练叶子结点和父节点的embedding，得到最终上线的模型。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-0bd508295f5c78f7858a0a7b41173a2a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"604\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-0bd508295f5c78f7858a0a7b41173a2a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;604&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"604\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-0bd508295f5c78f7858a0a7b41173a2a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-0bd508295f5c78f7858a0a7b41173a2a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>接着我们来讲一下这个模型的loss，我们的目标是去拟合曝光，所以loss由两部分组成，第一部分是交叉熵损失，去拟合对应的样本是否曝光。第二部分是triplet loss，含义是叶子节点中正样本之间距离尽可能相近，而与负样本之间的距离尽可能远，来约束叶子距离。第一部分loss考虑的是拟合曝光，并没有考虑点击，所以可以把实际用户点击的样本进行加权，把即曝光又点击的样本着重考虑，体现重要性。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-1f9b3396c4a48ad6e9eac48efdc15360_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"605\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-1f9b3396c4a48ad6e9eac48efdc15360_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;605&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"605\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-1f9b3396c4a48ad6e9eac48efdc15360_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-1f9b3396c4a48ad6e9eac48efdc15360_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>线上如何检索呢？因为我们是基于最大堆的，所以采用的基于beam search的方式，就是逐层往下的检索，每一层都保留K个节点并往下扩展。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ea767720e5596c1f00beb504b020b933_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"603\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-ea767720e5596c1f00beb504b020b933_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;603&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"603\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-ea767720e5596c1f00beb504b020b933_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-ea767720e5596c1f00beb504b020b933_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>实际线上投放是有变化的，广告主会新增一些投放，对于这种新增投放怎么添加到这颗树中呢？比如新增一个item，我们会根据训练好的模型把item对应特征转换为embedding，然后把之前训练的叶子节点的embedding set拿出来，然后做一个相似度检索，得到item embedding最接近的叶子节点37，将该新item加入这个叶子节点37的list中去（线上训练时，叶子节点实际是一个cluster，一个列表，并不是一个单一的节点）。</p><p><b>▌4. 性能优化</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-8eb4e2e4ed64cbab33b51ebf4edc1084_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-8eb4e2e4ed64cbab33b51ebf4edc1084_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;606&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-8eb4e2e4ed64cbab33b51ebf4edc1084_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-8eb4e2e4ed64cbab33b51ebf4edc1084_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>最后提一下我们在实际中做的一些性能优化工作，主要介绍两方面：</p><p>第一方面是在检索阶段，针对树的最上几层，会根据Top K的大小，进行跳层（父节点个数&lt;K），针对最后几层可以返回cluster，交给后续精排业务去做排序。同时可以进行多路并行，提高性能，也可以将训练的double数据裁剪为uint8类型，针对相对排序指标影响不是很大。</p><p>第二方面是训练阶段，因为我们是基于tensorflow构建模型的，所以有常见的几种优化方式，比如避免使用feed dict，使用dataset这种高阶可以并行化的api。同时可以使用Timeline Profile等工具分析性能瓶颈，进行针对性的优化。针对python GIL的缺点，将高频调用的python function利用c++实现进行调用，并在调用前将GIL release掉，避免锁带来的性能影响。</p><p><b>▌参考资料：</b></p><p>1. Learning Tree-based Deep Model for Recommender Systems</p><p><a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1801.02294\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/abs/1801.0229</span><span class=\"invisible\">4</span><span class=\"ellipsis\"></span></a></p><p>2. 微软DSSM项目地址：</p><p><a href=\"https://link.zhihu.com/?target=https%3A//www.microsoft.com/en-us/research/project/dssm/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">microsoft.com/en-us/res</span><span class=\"invisible\">earch/project/dssm/</span><span class=\"ellipsis\"></span></a></p><p>文章作者：王华呈 360 资深算法工程师</p><p>编辑整理：杨辉之</p><p>内容来源：爱奇艺技术沙龙</p><p>出品社区：DataFun</p>", 
            "topic": [
                {
                    "tag": "奇虎 360", 
                    "tagLink": "https://api.zhihu.com/topics/19553284"
                }, 
                {
                    "tag": "360 搜索", 
                    "tagLink": "https://api.zhihu.com/topics/19743141"
                }, 
                {
                    "tag": "广告", 
                    "tagLink": "https://api.zhihu.com/topics/19553032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/87196328", 
            "userName": "DataFunTalk", 
            "userLink": "https://www.zhihu.com/org/09843313d8c5eff1b7d8bcfa65dc8b68", 
            "upvote": 10, 
            "title": "深度学习技术在美图个性化推荐的应用实践", 
            "content": "<p><b>导读：</b>美图秀秀社交化的推进过程中，沉淀了海量的优质内容和丰富的用户行为。推荐算法连接内容消费者和生产者，在促进平台的繁荣方面有着非常大的价值 。本次分享探讨美图在内容社区推荐场景下应用深度学习技术提升点击率、关注转化率和人均时长等多目标的实践经验。</p><p>1. 美图社区个性化推荐场景概况与挑战</p><p>2. embedding 技术在召回阶段的应用实践</p><ul><li>基于 Item2vec 的 item embedding</li><li>YouTubeNet 和双塔 DNN 在个性化深度召回模型应用实践</li></ul><p>3. 美图排序模型的研发落地 </p><ul><li>NFwFM 模型研发迭代历程和经验 </li><li>多任务学习 ( Multi-task NFwFM ) 在多目标预估场景的探索与实践</li></ul><p><b>▌美图社区个性化推荐场景与挑战</b></p><p><b>1. 业务场景</b></p><p>美图社区个性化推荐场景大大小小有十多个，其中流量比较大的场景是美图秀秀 app 的社区内容推荐 tab ( 图1 )，这个场景以双列瀑布流的形态给用户推荐他最感兴趣的内容。</p><p>当用户点击感兴趣的图片后会进入图1-2的相似推荐 feeds 流场景。在这个场景下, 用户消费的图片和视频，都是和用户刚刚点击进来图片是具有多种相似性的，如视觉、文本、topic 等。而如果用户是从双列瀑布流里点击视频，则会进入到图1-3的视频 feeds 流场景。这个场景主打让用户有沉浸式的消费体验。以上是美图社区内容推荐的主要业务场景。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-db813b094c4cf5c7269edc528da45c47_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-db813b094c4cf5c7269edc528da45c47_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;606&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-db813b094c4cf5c7269edc528da45c47_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-db813b094c4cf5c7269edc528da45c47_b.jpg\"/></figure><p>图1 美图个性化推荐业务场景</p><p><b>2. 工作目标</b></p><p>个性化推荐的首要目标是理解内容。从内容本身的视觉、文本以及特定场景下用户的行为来理解社区里可用于推荐的内容。接下来是理解用户，通过用户的基础画像 ( 年龄，性别等 )、设备画像 ( OS，机型等 )，以及用户的历史行为来挖掘其兴趣偏好。</p><p>再理解了社区的内容和用户之后，才是通过大规模的机器学习算法进行精准推荐，千人千面地连接用户与内容，从而持续提升用户体验，促进社区繁荣。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-95103f03279c01d5485ec7c3c12d03e2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"563\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-95103f03279c01d5485ec7c3c12d03e2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;563&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"563\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-95103f03279c01d5485ec7c3c12d03e2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-95103f03279c01d5485ec7c3c12d03e2_b.jpg\"/></figure><p>图2 美图个性化推荐业务目标</p><p><b>3. 挑战</b></p><p>在实际生产实践过程当中，主要遇到挑战如下：</p><ul><li><b>海量</b></li></ul><p>美图内容社区有月活超过1亿的用户，每天有100多万的候选图片和视频。在这种数据规模下，只在小数据规模下验证有效的复杂算法模型无法直接应用到工业界线上系统。</p><ul><li><b>实时</b></li></ul><p>算法需要在端到端小于 300ms 的时延里，每天处理超过3亿的个性化排序请求。这要求召回和排序算法不能过于复杂，要能够进行高效的计算。</p><ul><li><b>长尾</b></li></ul><p>在实际场景中，用户分布以及 item 行为分布都是长尾的：</p><ul><li>用户分布的长尾性：新用户占比超过27%；</li><li>曝光分布的长尾性：关注&lt;&lt; 点击&lt;&lt;曝光。</li></ul><p>在这样长尾数据上进行预估要求我们的模型具备稳定的泛化能力。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-6ad905e266f7aa6f0a61dc8d25a735a8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"599\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-6ad905e266f7aa6f0a61dc8d25a735a8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;599&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"599\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-6ad905e266f7aa6f0a61dc8d25a735a8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-6ad905e266f7aa6f0a61dc8d25a735a8_b.jpg\"/></figure><p>图3 美图个性化推荐的挑战</p><p><b>▌美图深度学习技术栈——召回端</b></p><p>在上述的应用场景和技术挑战下，美图是如何将深度学习应用到个性化推荐中的召回端和排序端的呢？我将会在下面为大家一一介绍。</p><p>召回端的 Item embedding 技术和召回模型，用于从百万级别的候选集里挑选 TOP 500个用户最感兴趣的候选集。相对于召回端，排序端的深度排序模型能够融合多种召回来源并进行统一排序，排序模型能包容规模更大的细粒度特征，相对召回模型，排序模型能够实现更加精准的推荐。</p><p>美图目前部署在线上系统的召回技术主要包括 Item2vec，YouTubeNet，以及双塔 DNN。</p><p><b>1. Item2vec</b></p><p>Item2vec，是一种通过用户行为来理解内容的方式。</p><p>传统的理解内容方式是基于用户行为构造 item 侧的统计类特征，例如 item 的点击率，收藏率等。这些特征是非常有效的，但是对内容的理解维度比较单一。</p><p>另一种方式是从图片的本身的视觉来提取比如图片质量、清晰度、图片物体等等。还可以通过内容本身的文本特征，比如关键词，实体词等等，来帮助理解内容。这些维度的特征在内容冷启动中是很有效的。但是他们无法表达内容的某些潜在特性，比如某个内容是否给用户呈现出清新有趣的感觉。这种潜在的特征借助用户的行为来理解比较合适。Item2vec 正是这样一种技术，它基于短时间内被浏览的 item 具有内在相似性的假设来学习 item 的 embedding。</p><p>在图4中可以看到 item2vec 在美图社区图片上的部分效果。可以看到和查询图在训练数据中高频共现和中频共现的 item，在背景和主体人物上和查询词是高度相似的。而低频共现的部分和我们的查询图片有些差异，不过主体内容总体上还是比较相似的。</p><p>总体而言，item2vec 是一种学习 item embedding 的成熟方案。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-f26662d591024e66b7e62597b7a87706_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1000\" data-rawheight=\"986\" class=\"origin_image zh-lightbox-thumb\" width=\"1000\" data-original=\"https://pic3.zhimg.com/v2-f26662d591024e66b7e62597b7a87706_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1000&#39; height=&#39;986&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1000\" data-rawheight=\"986\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1000\" data-original=\"https://pic3.zhimg.com/v2-f26662d591024e66b7e62597b7a87706_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-f26662d591024e66b7e62597b7a87706_b.jpg\"/></figure><p>图4 item2vec 可视化效果</p><p>Item2vec 学习出来的 item 向量是美图多种向量检索式召回策略的底层数据，包括实时兴趣，短期兴趣等等。比如当用户点击了某个 item，系统会实时地通过向量内积运算查询相似的 item 并插入到召回源头部，用于下一轮的排序。</p><p>使用 item2vec 学习出来的 item 向量作为底层数据的召回策略，在美图应用非常广泛，它们在整体曝光中占了10%以上。</p><p>实际应用时，我们是基于 skip-gram+negative sampling 来做 item2vec 的。</p><p>它是一个只有一个隐层的深度学习模型。输入端是用户的点击序列, 输出端是与输入端的 target item 邻近的64个 item。64相对于 NLP 里取的5-6个是比较大的，这是因为用户的点击序列不像自然语言那样具有严格的局部空间句法结构。在比较大上下文窗口中，更容易找到和目标 item 相似的上下文 item，模型更容易学习。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-5a88e865f74a3fc8cc2885976a0fe1c1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"586\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-5a88e865f74a3fc8cc2885976a0fe1c1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;586&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"586\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-5a88e865f74a3fc8cc2885976a0fe1c1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-5a88e865f74a3fc8cc2885976a0fe1c1_b.jpg\"/></figure><p>图5 item2vec 应用实践</p><p>负采样的个数是正样本数的两倍，这是一个效果和性能折中，在我们的应用中正负样本数1:2，能够在天级别模型更新情况下，取得比较好效果。隐层的 embedding size 取128可以得到比较好的效果, 这个取值同样是效果和性能的折中。另外，我们过滤了点击序列长度小于5的样本，这样过滤之后，我们的点击序列能覆盖99%候选图片和视频。</p><p>Item2vec 是学习 item embedding 向量的一种非常好的方案，它也覆盖了美图多个推荐业务。但是它不直接考虑用户的个性化行为，只考虑了训练样本中 item 与 item 之间的局部共现关系。如果要利用上丰富的用户侧特征，实现个性化的话，那么我们需要借鉴其他方案，而 YouTubeNet 正是这样一种业界成熟的方案。</p><p><b>2. YouTubeNet</b></p><p>YouTubeNet 是 Google 于2016年提出的。与 item2vec 不同，YouTubeNet 在学习 item 向量的时候考虑了用户向量。从模型的优化目标上可以看出，是在给定用户向量的情况下，从候选池中筛选出该用户最感兴趣的 item 列表。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-03de563ec713b0f4982d2d079360a37a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"599\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-03de563ec713b0f4982d2d079360a37a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;599&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"599\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-03de563ec713b0f4982d2d079360a37a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-03de563ec713b0f4982d2d079360a37a_b.jpg\"/></figure><p>图6-1 YouTubeNet 应用实践</p><p>从上图右侧模型架构图可以看到，模型训练用的label是用户最近点击过的 item 列表，特征包括两部分，一部分是用户更早之前点击过的列表 ( clicked_item_list )，另一部分是用户的 demographic 统计特征，如年龄、性别等。引入上述用户的个性化信息之后，模型通过学习 user embedding 和 item embedding，并离线导出用户向量和 item 向量。线上使用时利用 FAISS 工具离线计算好每个用户的 top N 候选 item 集，提供给排序算法使用。不过这种离线存储候选集的方式，不能实时应对用户不断变化的兴趣，要捕获这种变化，需要实时采集用户不断变化的点击 item 数据，实时计算用户侧向量。</p><p>实时计算用户侧向量的工作，一共分了两部分：第一部分是离线部分，为下图右侧的虚线部分，这里模型一天一更新。离线部分基本流程和上一段所述相同，模型训练完之后导出 item 向量并在 FAISS 中构建好索引。第二部分是实时部分，这一部分借助 kafka，实时采集用户点击行为数据并构建 clicked_item_list 特征，接着请求离线训练好的模型，计算出用户侧向量，最后从 FAISS 中查询的候选集，输出给排序服务。</p><p>使用 YouTubeNet 模型实现实时计算用户侧向量之后，曝光占比22%的 YouTubeNet 给整体带来了点击率3.67%的提升，人均时长提升2.22%。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-eb7a87c3670f88217b9797fde7fa888e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"629\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-eb7a87c3670f88217b9797fde7fa888e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;629&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"629\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-eb7a87c3670f88217b9797fde7fa888e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-eb7a87c3670f88217b9797fde7fa888e_b.jpg\"/></figure><p>图6-2 YouTubeNet 应用实践</p><p><b>3. 双塔 DNN</b></p><p>双塔 DNN 模型，联合用户行为和 item 侧特征进行召回。双塔 DNN 模型构建用户侧 embedding 的方式和 YouTubeNet 是一样的：先给用户的点击行为序列，年龄性别等稀疏的特征做 embedding，再经过几个简单的全连接层，得到用户侧向量。对于 item 侧特征，双塔 DNN 引入另一个子网络来学习，学习方式和用户侧特征是一样的。</p><p>离线训练完了之后和 YouTubeNet 还是一样，把 item 向量提前导出并加载到 FAISS。在线上环境使用的时候，实时计算用户侧向量，来快速捕获用户兴趣。引入 item 侧特征，使得线上用户点击率提升1.05%，人均时长提升0.76%。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-cf1c3a40dbeccf440d3f8b33faf6e040_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"567\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-cf1c3a40dbeccf440d3f8b33faf6e040_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;567&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"567\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-cf1c3a40dbeccf440d3f8b33faf6e040_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-cf1c3a40dbeccf440d3f8b33faf6e040_b.jpg\"/></figure><p>图7 双塔 DNN</p><p><b>4. 总结</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-3a110a78e3507a72b061aa402979e2fc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"875\" data-rawheight=\"191\" class=\"origin_image zh-lightbox-thumb\" width=\"875\" data-original=\"https://pic1.zhimg.com/v2-3a110a78e3507a72b061aa402979e2fc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;875&#39; height=&#39;191&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"875\" data-rawheight=\"191\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"875\" data-original=\"https://pic1.zhimg.com/v2-3a110a78e3507a72b061aa402979e2fc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-3a110a78e3507a72b061aa402979e2fc_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>Item2vec 学习出来的 item 向量具有很好的相似性，作为底层数据，服务了多个召回策略, 在美图具有非常广泛的应用。包括实时兴趣，短期兴趣等等，覆盖了10%以上的曝光占比。YouTubeNet 和双塔 DNN 则分别引入用户侧和 item 侧特征，有监督地学习用户最感兴趣的 item 候选集，在美图个性化推荐召回层，累计点击率提升了4.72%，时长提升了2.98%。</p><p><b>▌美图深度学习技术栈——排序端</b></p><p><b>1. 重新审视 NFM 模型</b></p><p>美图的第一代模型主打 LR 为主+人工特征组合。随着业务发展，大大小小的推荐场景越来越多，做特征的人力越来越紧张。恰逢深度学习在工业级推荐系统有大规模应用落地实践，因而逐渐将算法模型转向深度学习。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-cca69f692f5f6d2bb06c06cae270efc0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"622\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-cca69f692f5f6d2bb06c06cae270efc0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;622&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"622\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-cca69f692f5f6d2bb06c06cae270efc0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-cca69f692f5f6d2bb06c06cae270efc0_b.jpg\"/></figure><p>图8 NFM 模型</p><p>2018年年初，美图用 NFM 模型首次打败 LR 并取得稳定效果，NFM 模型创造性的将 FM 和深度模型端到端的联合训练，在底层就进行显示的特征交叉，NFM 论文的实验和美图数据集上的实验都表明了模型能够收敛更快也更加稳定。在美图的实践中，引入右侧多层感知机学习隐式的高阶特征交叉之后，效果进一步提升，好于原始的 NFM 模型。改进后的 NFM 模型在我们的推荐流场景中取得了5.5%的点击率，以及将近7%的时长提升。</p><p>在 NFM 模型取得稳定的正向效果之后，美图推荐团队继续探索了业界更多的模型。不过都未能落地，主要有以下两个原因。</p><p>第一个是像 Wide&amp;Deep， DeepFM， DCN 等从模型的复杂度上看没有比 NFM 拥有更强的预估能力，计算效率也没有明显优势。离线评估和线上实验上都没有得到正向效果。</p><p>第二种情况是，xDeepFM 和 NFFM 离线指标提升了，但是计算复杂度很高。此外 NFFM 模型参数量大，内存是个瓶颈。导致它俩无法大规模落地。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-31724a97a91436fa1e1e111f57ba0f6c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"857\" data-rawheight=\"331\" class=\"origin_image zh-lightbox-thumb\" width=\"857\" data-original=\"https://pic1.zhimg.com/v2-31724a97a91436fa1e1e111f57ba0f6c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;857&#39; height=&#39;331&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"857\" data-rawheight=\"331\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"857\" data-original=\"https://pic1.zhimg.com/v2-31724a97a91436fa1e1e111f57ba0f6c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-31724a97a91436fa1e1e111f57ba0f6c_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>经过一年多的探索，在业界的众多模型中我们没有找到合适美图推荐场景的排序算法。另外，在我们引入行为序列特征之后，NFM 的计算复杂度已经不能很好的支持线上流量。在这样的背景下，美图算法团队决定自行设计算法。</p><p>NFM 模型的优点在于，通过 Bi-Interaction，将 FM 模型端到端引入到深度模型，显式构建特征的乘性关系，加强模型预估能力，同时没有增加时间复杂度。但是在实际生产实践中，存在2个不足：</p><p>(1) NFM 需要足够的 Embedding layer 宽度来学习特征。在实际场景下，其宽度取200左右，效果最好。但是随着百万级别用户行为序列特征的加入，NFM 模型的计算量越来越大，越来越不能满足线上小于 300ms 的时延要求。</p><p>(2) 另一个不足是，NFM 模型本身存在 co-training 的问题，即：一个特征的学习，会不可避免地受到其他特征的影响。例如，用户的性别特征，与用户的网络环境特征是不相关的；但是 NFM 模型无法构建这种情况。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-7b62977ca9f3d2ea12bd07a7f419481b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"557\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-7b62977ca9f3d2ea12bd07a7f419481b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;557&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"557\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-7b62977ca9f3d2ea12bd07a7f419481b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-7b62977ca9f3d2ea12bd07a7f419481b_b.jpg\"/></figure><p>图9 NFM 模型的不足</p><p>基于以上不足，我们先来看下业界相关经验：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-23ca3fe97cd2965d2cdcb665349585e6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"584\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-23ca3fe97cd2965d2cdcb665349585e6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;584&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"584\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-23ca3fe97cd2965d2cdcb665349585e6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-23ca3fe97cd2965d2cdcb665349585e6_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>不管是在传统浅层模型时代还是在深度学习时代，引入特征的 field 信息之后，模型几乎是总能带来提升的。比如 FFM 仍然一直活跃在 Kaggle 等 CTR 预估比赛中，稳定的取得比不能建模 field 信息的 FM 更好的效果。而深度学习时代，业界很多公司比如2018年, 微软离线验证 xDeepFM 引入特征的 field 信息之后，相对不能建模 field 信息的 DCN 同样取得了很明显的提升，即便在现在，xDeepFM 仍然是很优秀的模型。但是他们或者计算量太高或者参数量太大，导致无法大规模应用到线上系统。基于上面对 NFM 模型的优点的实验和分析，美图算法团队开始尝试 NFwFM 模型。</p><p><b>2. NFwFM 模型</b></p><p><b>2.1 模型整体架构</b></p><p>NFwFM 模型是在 FwFM 模型的基础上演化出来的：通过 Field-wise Bi-Interaction 组件，将 FwFM 引入到深度模型里面。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-ec5f191d84b90892680d8e22671c8169_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"591\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-ec5f191d84b90892680d8e22671c8169_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;591&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"591\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-ec5f191d84b90892680d8e22671c8169_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-ec5f191d84b90892680d8e22671c8169_b.jpg\"/></figure><p>图10-1 NFwFM 模型整体架构</p><p>上图是 NFwFM 模型的整体架构，首先，把特征按照逻辑分为3个大模块：用户侧特征 ( 包括年龄、性别等 )，item 侧特征 ( 包括 item id，item 标签等 ) 以及上下文侧特征。</p><p>接下来将 FwFM 分解成了3个子模块：第一个模块是线性加和模块 ( 上图中 S 表示 )，不区别 field 学习的特征；第二个模块是矩阵分解模块 ( 上图中的 MF 部分 )，用来学习 field 粒度下的特征组合，比如 user field 和 item field 的二阶交叉；第三个模块是 FM 模块，用来学习 field 内部 feature 粒度的特征组合。</p><p><b>2.2 FwFM 和 FFM 相比</b></p><p>(1) FwFM 模型尺寸相对 FFM 少 M-1 倍。在美图实际应用中，特征量大约可减少30倍。</p><p>(2) FwFM 模型引入了 Field 相关的权重 ri,j ，解决了 FM 存在的不相关特征在学习过程中互相干扰的问题。</p><p>(3) 离线评估显示，FwFM 模型的预测性能 ( 例如 AUC 指标 ) 和 FFM 基本一致，而参数规模大大降低。</p><p>FwFM 模型由于要建模 field 信息，导致它无法像 FM 那样具备良好的线性时间复杂度。因此，需要将 FwFM 做矩阵分解 ( 上面架构图中的 MF 模块 )。</p><p><b>2.3 MF 模块</b></p><p>如下图所示，模型分别从用户侧和 item 侧提取特征向量 vi 和 vj，在这两个向量上进行矩阵分解，用来学习 field 粒度的特征组合。实际应用中，需要分别对用户侧、item 侧、context 侧进行两两矩阵分解，因此共有3个矩阵分解子模块。</p><p>通过离线评估显示，MF 分解前后的 FwFM 模型，其 AUC 等指标持平，但是相同参数规模下计算量降低 M*M 倍，计算效率大大提升。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-acf64ed92ef71434d4669f1959c2a752_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"310\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-acf64ed92ef71434d4669f1959c2a752_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;310&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"310\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-acf64ed92ef71434d4669f1959c2a752_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-acf64ed92ef71434d4669f1959c2a752_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>2.4 FM 模块</b></p><p>但是，MF 分解也会存在不足，例如：对于用户侧存在的丰富多样的特征，没有办法使用矩阵分解进行两两二阶交叉。</p><p>因此，在 field 内部引入 FM，用来显式学习用户侧 feature 粒度的所有特征的二阶交叉组合。具体来讲，给 user field 引入一个 FM 模型，对用户的年龄、性别等特征的二阶交叉，同样的算法也用于 item field 等。这样，FwFM 模型就演化成了下图这样一个 Field-wise Bi-Interaction 组件。引入 FM 模型后，模型的 AUC 指标提升了约0.002。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-2b4d1ce715e9092f2aa314e8cd798be1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"528\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-2b4d1ce715e9092f2aa314e8cd798be1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;528&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"528\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-2b4d1ce715e9092f2aa314e8cd798be1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-2b4d1ce715e9092f2aa314e8cd798be1_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>2.5 解决特征间干扰问题</b></p><p>但是这样并没有解决最一开始提到的问题：FM 模型在学习过程中，特征存在互相干扰的情况。</p><p>回顾一下前文所述的特征间干扰问题，即 FM 的 co-training 问题：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-71259132dccf6b282ba31163a17f5f51_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"663\" data-rawheight=\"80\" class=\"origin_image zh-lightbox-thumb\" width=\"663\" data-original=\"https://pic2.zhimg.com/v2-71259132dccf6b282ba31163a17f5f51_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;663&#39; height=&#39;80&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"663\" data-rawheight=\"80\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"663\" data-original=\"https://pic2.zhimg.com/v2-71259132dccf6b282ba31163a17f5f51_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-71259132dccf6b282ba31163a17f5f51_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>模型在对每一个特征进行学习的时候，都需要和其他特征进行交叉。例如，用户性别特征和网络环境特征应该是不相关的，但是模型在学习性别特征的时候不可避免地受到网络环境的影响。</p><p>为解决这一问题，借鉴 dropout 思路：模型训练完成 Bi-Interaction 后，按照伯努利分布 ( 期望为 β ) 随机丢弃部分二阶交叉项，以解决部分 co-training 问题。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-2ca6f16334a53518969f7befcf04d510_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"926\" data-rawheight=\"256\" class=\"origin_image zh-lightbox-thumb\" width=\"926\" data-original=\"https://pic1.zhimg.com/v2-2ca6f16334a53518969f7befcf04d510_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;926&#39; height=&#39;256&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"926\" data-rawheight=\"256\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"926\" data-original=\"https://pic1.zhimg.com/v2-2ca6f16334a53518969f7befcf04d510_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-2ca6f16334a53518969f7befcf04d510_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>具体来讲，先从伯努利分布中采样出由{0,1}组成的向量，再用该向量和 FM 模型计算得到的表示二阶特征交叉组合的向量进行相乘，这样可以随机丢弃部分二阶交叉项。在预估的时候是将 FM 做了 Bi-Interaction 后得到的向量，乘以伯努利分布的期望 β，用来对齐计算过程中丢失的数据的大小。引入如上思路之后，AUC 提升约0.001。</p><p><b>2.6 总结</b></p><p>总体来讲，我们从 FwFM 演化出 Field-wise Bi-Interaction 组件，包含线性加和模块用来学习一阶特征，还包括矩阵分解 ( MF ) 模块和 FM 模块，用来学习特征 field 粒度和 feature 粒度的特征交叉。相比于上一代 NFM 模型，使用这样的模型，在计算量和参数量都减少了6倍的情况下，点击率得到了5.19%的提升。</p><p><b>3. Multi-task NFwFM</b></p><p><b>3.1 多任务基本架构</b></p><p>接下来是多任务方面的工作。在深度学习时代，深度模型能够包含多种不同分布的样本，释放了多任务学习的最大价值。从实践角度考量，为使离线训练和在线预估效率较高，目标个数具有可拓展性。业界通常会选择下图中这种底层硬共享 ( hard-sharing ) 隐层的多任务架构，在这种架构下，因为点击率和关注转化率任务是强相关的工作，能增加共享隐层的学习速度，从而增加模型的收敛效率，而这两个任务中不相关的部分可以认为是相互任务的噪声，可以增强模型的泛化能力。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-cc66cce2991e79a3354ac2aa2fd8ceb3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"610\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-cc66cce2991e79a3354ac2aa2fd8ceb3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;610&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"610\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-cc66cce2991e79a3354ac2aa2fd8ceb3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-cc66cce2991e79a3354ac2aa2fd8ceb3_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>在学习的过程中，是利用两个任务简单加和的方式来学习多目标的。这个方式的离线 AUC 和单独的点击用户模型的 AUC 基本持平；线上点击率提升1.93%，关注转化率提升2.90%。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-e1fd1a6b42cd297c378f251e9ae91810_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"278\" data-rawheight=\"44\" class=\"content_image\" width=\"278\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;278&#39; height=&#39;44&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"278\" data-rawheight=\"44\" class=\"content_image lazy\" width=\"278\" data-actualsrc=\"https://pic1.zhimg.com/v2-e1fd1a6b42cd297c378f251e9ae91810_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>在实际情况中，点击和关注的样本比例大概为100:1；在这样很少的关注样本的情况下，使用上述的多任务架构就可以得到稳定的提升；这驱使我们引入更多的关注数据，来压榨多任务模型学习更多更高质量数据的能力。</p><p><b>3.2 样本 reweight</b></p><p>具体来说，我们引入一个样本 reweight 的概念，主要目的是为了引入更多更高质量的关注行为数据。因为无数的经验告诉我们这是非常有效的做法。</p><p>以下图为例，假设有A、B、C、D这4个 item，图中实线部分表示 item 的真实 CTR ( 由大到小分别是 C &gt; D &gt; B &gt; A )；而实际的关注转化率的关系是 A &gt; C &gt; B &gt; D = 0。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-35fb530e30d519e55df6e4205daf6913_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"548\" data-rawheight=\"414\" class=\"origin_image zh-lightbox-thumb\" width=\"548\" data-original=\"https://pic4.zhimg.com/v2-35fb530e30d519e55df6e4205daf6913_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;548&#39; height=&#39;414&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"548\" data-rawheight=\"414\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"548\" data-original=\"https://pic4.zhimg.com/v2-35fb530e30d519e55df6e4205daf6913_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-35fb530e30d519e55df6e4205daf6913_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>我们用实际的关注转化率取 reweight 这个样本之后，得到下图虚线部分的结果：C &gt; D &gt; A &gt; B，即原本点击率最低的 item A，在 reweight 之后由于关注转化率高而变得点击率比 B 更高，这样 item A 更容易被模型推荐出来, 这样就能够提升整体的关注转化率，并且因为 C、D 等因为本身点击率较高或者没有关注行为，它们不受分布改变的影响，因此他们的 CTR 大小关系不受影响。换一种理解，我们是在仅仅改变了有关注 item 的点击率分布的基础中引入了更多更高质量的关注行为数据。对原来的点击率预估模型的侵入很少，整体点击率不会下跌太多。该工作提高关注转化率14.93%，但是点击率提升很少 ( 约0.84% )。</p><p><b>3.3 Homoscedastic Uncertainty 学习方式</b></p><p>上述工作点击率提升很少的原因是 reweight 模型过于简单。如前文所述，多任务的缺点是在参数共享的情况下，如果两个任务有不相关的部分，两个任务就会互相干扰，从而影响效果。学术界将这种现象称为共享冲突。共享冲突这一问题分析和解决起来较为复杂。针对美图的具体场景，减少共享冲突的一种方法是加大点击率预估任务的重要性，让点击率预估任务主导底部共享参数学习，进而让整体模型优先正确预估点击率模型，再去预估点击转化率任务。</p><p>在实践中，我们用同方差不确定性来学习每个任务对整体的主导能力。具体来说，分别给点击率任务和关注转化率任务各自一个参数 ( θclick 和 θfollow ) 用来表示各自的不确定性；不确定性越小的任务对模型整体的主导性越强。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-f9fa36c36c173e984f3a0f4bb689ba8c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"521\" data-rawheight=\"58\" class=\"origin_image zh-lightbox-thumb\" width=\"521\" data-original=\"https://pic1.zhimg.com/v2-f9fa36c36c173e984f3a0f4bb689ba8c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;521&#39; height=&#39;58&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"521\" data-rawheight=\"58\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"521\" data-original=\"https://pic1.zhimg.com/v2-f9fa36c36c173e984f3a0f4bb689ba8c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-f9fa36c36c173e984f3a0f4bb689ba8c_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>从下图可以看出，左图的关注转化的不确定性达到0.76，确实比右图的点击率的不确定性 ( 约0.42 ) 更高；因此，让点击率预估任务主导整个模型的学习。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-5c5c3439ab84316f270ce42d7b3f1690_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"900\" data-rawheight=\"306\" class=\"origin_image zh-lightbox-thumb\" width=\"900\" data-original=\"https://pic1.zhimg.com/v2-5c5c3439ab84316f270ce42d7b3f1690_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;900&#39; height=&#39;306&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"900\" data-rawheight=\"306\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"900\" data-original=\"https://pic1.zhimg.com/v2-5c5c3439ab84316f270ce42d7b3f1690_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-5c5c3439ab84316f270ce42d7b3f1690_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>这样可以避免点击率下跌的风险 ( 实际上，点击率提升了1.57% )，而关注转化率的提升达到了15.65%。</p><p><b>4. 总结</b></p><p>排序端的工作，美图经历了从 LR 模型到深度学习模型的引进：</p><p>(1) 引入 NFM 模型，点击率提升了5.53%，人均时长提升6.97%</p><p>(2) NFwFM 模型在引入了特征 Filed 信息后，在模型尺寸和计算复杂度可控的情况下，点击率提升了5.19%，人均时长提升了2.93%；</p><p>多目标 NFwFM 模型，在引入更多更高质量数据之后，不仅关注转化率提升了15.65%，点击率也提升了1.57%。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-0d221c763b62624d60c095a25c7dd87c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"762\" data-rawheight=\"188\" class=\"origin_image zh-lightbox-thumb\" width=\"762\" data-original=\"https://pic1.zhimg.com/v2-0d221c763b62624d60c095a25c7dd87c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;762&#39; height=&#39;188&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"762\" data-rawheight=\"188\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"762\" data-original=\"https://pic1.zhimg.com/v2-0d221c763b62624d60c095a25c7dd87c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-0d221c763b62624d60c095a25c7dd87c_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>▌参考文献</b></p><p>1. Covington P, Adams J, Sargin E. Deep neural networks for youtube recommendations</p><p>2. Ma J, Zhao Z, Yi X, et al. Modeling task relationships in multi-task learning with multi-gate mixture-of-experts</p><p>3. Rich Caruana. 1998. Multitask learning. In Learning to learn</p><p>4. Lin T Y, Goyal P, Girshick R, et al. Focal loss for dense object detection</p><p>5. Kendall A, Gal Y, Cipolla R. Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</p><p>6. [白杨-2018]基于用户行为的视频聚类方案</p><p><a href=\"https://link.zhihu.com/?target=https%3A//cloud.tencent.com/developer/article/1193177\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">cloud.tencent.com/devel</span><span class=\"invisible\">oper/article/1193177</span><span class=\"ellipsis\"></span></a></p><p>7. [蒋文瑞 2018]. 深度模型 DNN 在个性化推荐场景中的应用</p><p><a href=\"https://link.zhihu.com/?target=https%3A//cloud.tencent.com/developer/article/1193180\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">cloud.tencent.com/devel</span><span class=\"invisible\">oper/article/1193180</span><span class=\"ellipsis\"></span></a></p><p>8. [陈文强 2019]. 多任务学习在美图推荐排序的近期实践</p><p><a href=\"https://link.zhihu.com/?target=https%3A//cloud.tencent.com/developer/article/1475686\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">多任务学习在美图推荐排序的近期实践 - 云+社区 - 腾讯云</a></p><p>文章作者：陈文强、白杨、黄海勇 </p><p>编辑整理：王吉东</p><p>内容来源：2019 DataFun Live 09</p><p>出品社区：DataFun</p><p>注：欢迎转载，转载请在留言区内留言。</p>", 
            "topic": [
                {
                    "tag": "美图", 
                    "tagLink": "https://api.zhihu.com/topics/19868374"
                }, 
                {
                    "tag": "个性化推荐", 
                    "tagLink": "https://api.zhihu.com/topics/19569242"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/86607378", 
            "userName": "DataFunTalk", 
            "userLink": "https://www.zhihu.com/org/09843313d8c5eff1b7d8bcfa65dc8b68", 
            "upvote": 6, 
            "title": "UC 信息流推荐模型在多目标和模型优化方面的进展", 
            "content": "<p><b>导读：</b>短视频已经成为信息流行业的风口，成为拉动规模增长的主要驱动力。短视频天然具有信息能量高、用户粘性大、内容丰富等优点，也有视频帧内容难以分析提取、结构化的缺点。如何提高短视频的分发效率和推荐精准度，做到千人千面的个性化推荐，是一个推荐系统的核心能力。基于深度学习的推荐模型，是业界前沿的研究课题；在短视频推荐的业务中，如何利用深度学习算法有效的提升消费点击和消费时长，是推荐模型的核心命题。此次演讲主要讲解视频推荐模型在多目标和模型优化方面的进展。包括以下几个模块：</p><ul><li>业务和系统</li><li>基于 Graph Embedding 的多目标</li><li>基于 WnD 的 Boosting 算法</li><li>未来规划</li></ul><p><b>▌业务和系统</b></p><p><b>1. 短视频业务背景、系统结构</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-d95ef8a693bdfb6cf29b760877f894e0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"543\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-d95ef8a693bdfb6cf29b760877f894e0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;543&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"543\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-d95ef8a693bdfb6cf29b760877f894e0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-d95ef8a693bdfb6cf29b760877f894e0_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>我们的业务主要是做视频推荐，嵌入到 UC 浏览器中做国内信息流：</p><ul><li>UC 从工具型产品转型至内容分发平台，信息流业务成为 UC 的第二大引擎；</li><li>短视频已经成为信息流行业的风口，成为拉动规模增长的主要驱动力；</li><li>短视频播放时长已经超过图文，极大的增强了信息流的用户粘性；</li><li>算法的持续优化与迭代提高了短视频流量分发的效率和准确度。</li></ul><p>右图是视频推荐的界面，其背后的视频推荐系统分为三个结构，第一个是召回模块，第二个是粗排的模型，第三个是精排的模型。从召回 -&gt; 粗排 -&gt; 精排，Item 的数目从多到少，推荐的准确性从低到高。</p><p><b>2. 技术演进史</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-e171568458bf62f68182aaec1c3d4cdc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"521\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-e171568458bf62f68182aaec1c3d4cdc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;521&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"521\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-e171568458bf62f68182aaec1c3d4cdc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-e171568458bf62f68182aaec1c3d4cdc_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>我们在视频推荐上，尝试过多个推荐方案：</p><ul><li>最早用的是 LR ( 如图，左下角 )，LR 的缺点主要是在特征工程上会耗费很大的人力；</li><li>后来我们又尝试了 GBDT 和 FM ( 如图，中间部分 )，但是这些模型在泛化性上相对较弱；</li><li>最后我们采用了 Wide &amp; Deep model ( 如图，右下角 )。</li></ul><p>这里主要介绍的工作是：从多目标的角度优化 Loss Function。与粗排和精排使用的 WnD 对比，线上收益增幅相当，增量超过。</p><p><b>3. 多目标在视频推荐模型中的应用</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-d24c43e765a6a71885c5f8ddd1113689_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"560\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-d24c43e765a6a71885c5f8ddd1113689_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;560&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"560\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-d24c43e765a6a71885c5f8ddd1113689_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-d24c43e765a6a71885c5f8ddd1113689_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>常规的多目标是共享隐层的 soft 多目标模型，我们使用的是基于正样本加权的模型。</p><p>共享隐层多目标模型的缺点：</p><p>① 优化目标差异：推荐模型强调 rank，关注正样本排序；传统的多目标模型强调 classify 分类，正负样本一视同仁；</p><p>② 样本不均衡：在信息流中正负样本数量、信息量存在差异，用户感知的信息更多来自正样本；</p><p>③ 场景原因：信息流有很多自动刷新的场景，负样本的可信度降低</p><p>④ 多目标的 Loss Function 很难设计，市面上的多目标方案大多是一个主目标和一个辅助的目标</p><p>我们使用的正样本加权的多目标模型的优点：</p><p>① 关注正样本 ranking</p><p>② 方便融合多种信息</p><p>我们的方案最终在线上取得了不错的效果。</p><p><b>▌基于 Graph Embedding 多目标融合的 WnD 模型优化</b></p><p><b>1. 优化 WnD 模型 logit 配比</b></p><ul><li>信号正向传播</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-34763eaa8ff04fa583de67a068de1a30_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"553\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-34763eaa8ff04fa583de67a068de1a30_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;553&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"553\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-34763eaa8ff04fa583de67a068de1a30_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-34763eaa8ff04fa583de67a068de1a30_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>首先是 WnD 模型的一个小的优化，我们知道 WnD 分为 LR 侧和 DNN 侧，LR 侧使用的优化器是 FTRL，它的优化速度非常快，在数千万 DAU 的场景下，只需采用一次流式训练，在这样的场景下 DNN 对样本的训练很难迭代多次，这时 WnD 模型更倾向于使用 LR 侧的 logit。基于这样的原因，我们想能不能增加 DNN 侧的 logit，提高系统的准确度，大体就是这样的 motivation。</p><p>具体实施的时候，见图中红框部分，d 指 DNN，w 指 LR，zdL 指 DNN 最后一个隐层的输入，zw 指 LR 最后一个隐层的输入，我们在 DNN 侧乘以了一个系数 m，这就是大体的一个优化方向：</p><p>① 对于已经能很好识别的样本：概率增幅不明显（如右图最上部分的箭头，已经进入饱和区，所以增加不明显）</p><p>② 对于可以识别，但可信度低的样本：概率增幅明显（如右图中间部分的箭头）</p><p>③ 识别错误：错误概率变化幅度明显（如右图最下部分的箭头）</p><p>④ 对于 ③，概率增加，可以增加 Loss，从上述3点，可以对错误样本有针对性的训练</p><ul><li>梯度反向传播、Adagrad 优化器</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-bb144ba39bec4c555950479b807c276f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"523\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-bb144ba39bec4c555950479b807c276f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;523&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"523\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-bb144ba39bec4c555950479b807c276f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-bb144ba39bec4c555950479b807c276f_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>这是一个反向传播公式的推导，上面讲过增加 DNN 侧的 logit 会增加 Loss，图中标红的表示 Loss 会增大，同时，m 可以提出来作用到 Adagrad 优化器时：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-2731c467f9a51c6949fb6cfca34e4f42_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"342\" data-rawheight=\"103\" class=\"content_image\" width=\"342\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;342&#39; height=&#39;103&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"342\" data-rawheight=\"103\" class=\"content_image lazy\" width=\"342\" data-actualsrc=\"https://pic3.zhimg.com/v2-2731c467f9a51c6949fb6cfca34e4f42_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>也是相应增加的。</p><p><b>2. 基于时长加权的多目标融合的 WnD 模型优化</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-ae259ede6ccb2cd01b637dee84c0002a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"553\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-ae259ede6ccb2cd01b637dee84c0002a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;553&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"553\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-ae259ede6ccb2cd01b637dee84c0002a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-ae259ede6ccb2cd01b637dee84c0002a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>这部分的 motivation 是我们在做模型训练时，做的还是二分类的一个分类模型，这时点击观看的时长没有用到。因此，我们利用用户观看时长，来提升推荐效果。</p><p>这里设计了一个 weigh 计算方式：对于某个视频用 u_play_len 这个视频的历史观看总时长 / avg_item_history_view_len 这个视频历史的平均观看时长，用 max 做了一个规范化，求得 weigh，且 weigh＞1。图中统计了右边视频随着时长增加，用户观看数量的一个变化情况。有了 weigh 我们可以对 WnD loss function reweight：</p><p>① 如果某用户观看 Item 时长低于历史平均时长，权重为1，那么原来的既得收益 ( ctr ) 不会变化。</p><p>② 如果某用户观看 Item 时长高于历史平均时长，权重为 1.x，那么此类视频加权充分训练，会被 Ranking 比较高的分数，增加时长收益。</p><p><b>3. 基于 Graph Embedding 多目标融合的 WnD 模型优化</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-8361fe2b0013a4e352c781c2b53a4a28_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"553\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-8361fe2b0013a4e352c781c2b53a4a28_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;553&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"553\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-8361fe2b0013a4e352c781c2b53a4a28_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-8361fe2b0013a4e352c781c2b53a4a28_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>基于 Graph Embedding 多目标融合的方案中，多目标包括：提升点击、提升时长、提升点击率、提升点击渗透。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-0118e29e52b9ce742e29a5a0b5b43b28_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"555\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-0118e29e52b9ce742e29a5a0b5b43b28_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;555&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"555\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-0118e29e52b9ce742e29a5a0b5b43b28_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-0118e29e52b9ce742e29a5a0b5b43b28_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>首先如何取到一个 Item Embedding？我们采用的是 Deep Walk 算法，在拓扑网络中，通过随机游走和 w2v 算法，学习网络中顶点的低维向量表示，向量中蕴含着顶点之间的相关性等其他特征。真正训练的时候采用的是 CBOW 训练策略，其 Loss Function 如下：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-66199a05a3c3a7c4c41aa002e5d2dc40_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"564\" data-rawheight=\"117\" class=\"origin_image zh-lightbox-thumb\" width=\"564\" data-original=\"https://pic1.zhimg.com/v2-66199a05a3c3a7c4c41aa002e5d2dc40_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;564&#39; height=&#39;117&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"564\" data-rawheight=\"117\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"564\" data-original=\"https://pic1.zhimg.com/v2-66199a05a3c3a7c4c41aa002e5d2dc40_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-66199a05a3c3a7c4c41aa002e5d2dc40_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-45b4bb27f044a9e2a2637fcc30393bec_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"554\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-45b4bb27f044a9e2a2637fcc30393bec_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;554&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"554\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-45b4bb27f044a9e2a2637fcc30393bec_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-45b4bb27f044a9e2a2637fcc30393bec_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>我们的方案是，从用户日志中挖掘出每个 User 的点击序列，根据点击序列建立无向图，然后在每个节点随机游走形成句子，最后经过 w2v 的训练策略，得到每个 Item 的 Embedding 表示。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-c8e1fdaab7128d2491d0ee70e0948e51_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"548\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-c8e1fdaab7128d2491d0ee70e0948e51_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;548&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"548\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-c8e1fdaab7128d2491d0ee70e0948e51_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-c8e1fdaab7128d2491d0ee70e0948e51_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>具体方案：</p><p>① 根据用户浏览历史，生成每个用户的点击序列；</p><p>② 建图，以 ItemId 作为顶点，点击序列中相邻顶点作为边，形成无向图；</p><p>③ 随机游走，随机选择图中的某个顶点，随机的进行游走，形成句子；</p><p>④ 利用 w2v 中的 CBOW 策略，进行训练。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-c709f6c6ac644b8cb00c7a78a0988a30_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"554\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-c709f6c6ac644b8cb00c7a78a0988a30_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;554&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"554\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-c709f6c6ac644b8cb00c7a78a0988a30_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-c709f6c6ac644b8cb00c7a78a0988a30_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>有了 Item Embedding 之后，做 Loss Function 的 reweight，reweight 时加入了位置、时长、连续性 ( 通过 Graph Embedding 计算得出 ) 等信息。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-94e6957fccbf86142456e7f2f1478cdc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"552\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-94e6957fccbf86142456e7f2f1478cdc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;552&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"552\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-94e6957fccbf86142456e7f2f1478cdc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-94e6957fccbf86142456e7f2f1478cdc_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>Loss Function reweight 方式是在正样本前面乘一个系数 di,j，然后负样本保持不变，如果拿掉系数，是一个标准的交叉熵公式，di,j 考虑了观看时长、观看连续性、Item 的位置等综合信息来得到的，它们之间用的是 α、β、γ 线性加权得到的：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-cc258288723260c38969e8a6a45c23cc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"59\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-cc258288723260c38969e8a6a45c23cc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;59&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"59\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-cc258288723260c38969e8a6a45c23cc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-cc258288723260c38969e8a6a45c23cc_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>f（play_len）是关于时长的函数，由播放时长的分布估计得出：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-c8f984cd8ac7c43a04b7069b120bb4e4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"224\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-c8f984cd8ac7c43a04b7069b120bb4e4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;224&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"224\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-c8f984cd8ac7c43a04b7069b120bb4e4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-c8f984cd8ac7c43a04b7069b120bb4e4_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>左下角为用户播放数的累加，它的上面是观看到某一时长 n 的用户播放数的累加，右边侧延续了时长加权公式的模板。</p><p><b>4. 连续性</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ce6e632bc3e3f23efe8725fd48caaa43_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"561\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-ce6e632bc3e3f23efe8725fd48caaa43_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;561&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"561\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-ce6e632bc3e3f23efe8725fd48caaa43_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-ce6e632bc3e3f23efe8725fd48caaa43_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3957ab85656cbe3ad2d3e8943615d301_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"555\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-3957ab85656cbe3ad2d3e8943615d301_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;555&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"555\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-3957ab85656cbe3ad2d3e8943615d301_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-3957ab85656cbe3ad2d3e8943615d301_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>接下来讲一个连续性的实际案例：</p><p>①②③④⑤⑥⑦⑧这是用户的点击顺序，对应的分数是基于连续性 w2v 算法计算出来的分数，我们认为分数较高的视频很容易带领用户继续看下去，把它称作一个连续性的信息，我们对此类信息进行加权，希望能够带动用户整体的消费。</p><p><b>▌基于 WnD 的 Boosting 算法</b></p><p><b>1. 模型</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-36f73b00740df76072172348d63fe6f8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"609\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-36f73b00740df76072172348d63fe6f8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;609&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"609\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-36f73b00740df76072172348d63fe6f8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-36f73b00740df76072172348d63fe6f8_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>由于 Wide&amp;Deep 建模方案是一个二分类，理论上只有一个分类超平面，很容造成样本错分。我们理想的方案是，虽然是二分类，但是我们可以有多个决策超平面，所以我们用了 Boosting 算法，综合5个 Wide&amp;Deep，进行二分类。其应用场景是训练的实时流，每次训练都是一个 Batch ( 相当于在线训练 )，中间融合了5个 Wide&amp;Deep 模型 ( 5个模型的特征是不变的，共享 Embedding 隐层，采用的是并行计算 )，最终生成一个强分类器。</p><p><b>Adaboost：</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-da037b0344da18153778412c87a4cac2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"555\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-da037b0344da18153778412c87a4cac2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;555&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"555\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-da037b0344da18153778412c87a4cac2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-da037b0344da18153778412c87a4cac2_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>这是 Adaboost 算法的标准模式：</p><p>① 初始化训练数据的权值分布</p><p>② 使用权值分布 Dm 的训练数据集学习，得到子分类器 Gm(x)</p><p>③ 计算 Gm(x) 在训练数据集上的分类误差率</p><p>④ 利用分类误差率计算基本分类器在最终分类器中所占的权重</p><p>⑤ 更新每个样本的权重，继续训练下一个模型，如果有5个子模型就训练5个子模型</p><p>⑥ 最终综合每个子模型的权重和预测值，得到一个强分类器</p><p><b>2. 算法</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-b6d250d7e12810c0321ebdd530520ebb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"545\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-b6d250d7e12810c0321ebdd530520ebb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;545&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"545\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-b6d250d7e12810c0321ebdd530520ebb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-b6d250d7e12810c0321ebdd530520ebb_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>具体的算法有几个不同点：</p><p>第一个是误差项的计算方式，原先标准 Adaboost 算法误差项计算用的是加权指标函数：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-8d853c7d3dc20f7a870a18d7b743e41a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"801\" data-rawheight=\"186\" class=\"origin_image zh-lightbox-thumb\" width=\"801\" data-original=\"https://pic3.zhimg.com/v2-8d853c7d3dc20f7a870a18d7b743e41a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;801&#39; height=&#39;186&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"801\" data-rawheight=\"186\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"801\" data-original=\"https://pic3.zhimg.com/v2-8d853c7d3dc20f7a870a18d7b743e41a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-8d853c7d3dc20f7a870a18d7b743e41a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>我们的方案采用的是 AUC 加权的误差项：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-e28b24e8f127c8b5efd9e570919d14ef_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"86\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-e28b24e8f127c8b5efd9e570919d14ef_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;86&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"86\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-e28b24e8f127c8b5efd9e570919d14ef_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-e28b24e8f127c8b5efd9e570919d14ef_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>每个子模型的权重，由于是在线训练，所以采用的是迭代的方式：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-49707d39c0227f17e3152f70f9e1a90d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"230\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-49707d39c0227f17e3152f70f9e1a90d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;230&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"230\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-49707d39c0227f17e3152f70f9e1a90d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-49707d39c0227f17e3152f70f9e1a90d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>每次用上一个 Batch 1/2 权重加上此次误差项的计算。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-0785dcb82db48c2ce1c1d870e47afe59_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"554\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-0785dcb82db48c2ce1c1d870e47afe59_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;554&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"554\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-0785dcb82db48c2ce1c1d870e47afe59_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-0785dcb82db48c2ce1c1d870e47afe59_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>第二个是样本权重的更新，样本权重更新时只对正样本进行更新，而标准的 Adaboost 是对正负样本都进行更新的：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-6d17eaa0b2b491cb15339e1081b74334_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"876\" data-rawheight=\"248\" class=\"origin_image zh-lightbox-thumb\" width=\"876\" data-original=\"https://pic1.zhimg.com/v2-6d17eaa0b2b491cb15339e1081b74334_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;876&#39; height=&#39;248&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"876\" data-rawheight=\"248\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"876\" data-original=\"https://pic1.zhimg.com/v2-6d17eaa0b2b491cb15339e1081b74334_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-6d17eaa0b2b491cb15339e1081b74334_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>di,j+ 指的是这个样本上一个子模型的权重，εi 是误差项。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b55bf1a241b2220d544d873a93bab255_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"324\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-b55bf1a241b2220d544d873a93bab255_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;324&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"324\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-b55bf1a241b2220d544d873a93bab255_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-b55bf1a241b2220d544d873a93bab255_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>这是样本的更新方式。</p><p>最终强分类器的预测和 Adaboost 标准模型差不多：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-bf5b635fb0509286a692ac9ec60dd913_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"825\" data-rawheight=\"283\" class=\"origin_image zh-lightbox-thumb\" width=\"825\" data-original=\"https://pic4.zhimg.com/v2-bf5b635fb0509286a692ac9ec60dd913_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;825&#39; height=&#39;283&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"825\" data-rawheight=\"283\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"825\" data-original=\"https://pic4.zhimg.com/v2-bf5b635fb0509286a692ac9ec60dd913_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-bf5b635fb0509286a692ac9ec60dd913_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>也是线性加权然后除以总的权重。</p><p>实际应用中 Adaboost.Rank 的 WnD 子模型有5个，每个 WnD 都有相同的网络结构且它们共享 Embedding 层，Adaboost.Rec 模型的磁盘大小只比单个 WnD 模型增加了6%。</p><p><b>3. 在短视频推荐中的应用</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-2ad1fcd866704c3fee5a74043d4fbd9c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"557\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-2ad1fcd866704c3fee5a74043d4fbd9c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;557&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"557\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-2ad1fcd866704c3fee5a74043d4fbd9c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-2ad1fcd866704c3fee5a74043d4fbd9c_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>方案的实验效果和 WnD 模型对比如上，提升了模型推荐的精准性。</p><p><b>▌未来规划</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-adf64f80d62a2580e3e587616ec4592c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"611\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-adf64f80d62a2580e3e587616ec4592c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;611&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"611\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-adf64f80d62a2580e3e587616ec4592c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-adf64f80d62a2580e3e587616ec4592c_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>未来的规划：</p><ul><li>现在的推荐都是基于 point wise 的推荐模型，每次都是预测单个 Item 的分数，未来希望可以一次给用户推荐一刷的结果。</li><li>因为每天都有新用户进来，所以我们希望建立一个留存新用户模型，来留住新用户。</li><li>UC 信息流目前的主要消费群体为男性用户，在推荐的偏好上模型和策略都偏向男性用户，我们希望可以留住更多的女性用户。</li></ul><p>这就是未来的一个方向，谢谢大家。</p><p>分享嘉宾：语露 阿里 高级算法工程师 </p><p>编辑整理：Hoh Xil</p><p>内容来源：大鱼技术沙龙</p><p>出品社区：DataFun</p>", 
            "topic": [
                {
                    "tag": "信息流", 
                    "tagLink": "https://api.zhihu.com/topics/19620202"
                }, 
                {
                    "tag": "优化", 
                    "tagLink": "https://api.zhihu.com/topics/19570512"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": [
                {
                    "userName": "全全", 
                    "userLink": "https://www.zhihu.com/people/54953b088c3a146f2114ee0e3330ef9a", 
                    "content": "请问  5个WnD 如何并行训练的", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/84672111", 
            "userName": "DataFunTalk", 
            "userLink": "https://www.zhihu.com/org/09843313d8c5eff1b7d8bcfa65dc8b68", 
            "upvote": 5, 
            "title": "UC 信息流视频标签识别技术", 
            "content": "<p><b>导读：</b>本次分享的主题为 UC 信息流视频标签识别技术，主要介绍标签识别整体架构以及基于多模态信息的视频标签识别方法，实现让机器理解海量的视频中的关键信息。</p><p>包括以下几部分：</p><ul><li>标签使用场景</li><li>标签识别系统架构</li><li>标签识别算法</li><li>未来工作</li></ul><p><b>▌标签使用场景</b></p><p>首先介绍下为什么使用标签以及标签的使用场景：</p><p><b>1. 什么是视频标签</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-d6d44bd374fe90cf43a0296b47adcc42_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"609\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-d6d44bd374fe90cf43a0296b47adcc42_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;609&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"609\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-d6d44bd374fe90cf43a0296b47adcc42_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-d6d44bd374fe90cf43a0296b47adcc42_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>UC 信息流推荐比较多的内容都是新闻资讯类的，每天新的内容都非常多，如果没有内容理解结构化的信息，是比较难做内容冷启动的。视频标签可以理解成描述一个视频的几个关键词，当我们看到视频标签的时候就可以大概知道这个视频的内容。这几张图片是我们信息流小视频的几张截图，大家可以看下可以给这几个小视频打上什么样的标签，比如第一个视频会打上时尚、穿搭、小姐姐等标签，第二个视频会打上美食、烧烤等标签。当我们提取这样的标签之后，该如何使用呢？</p><p><b>2. 标签在推荐中的使用</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-169efa260eaf2ff2e275e92efa7e9d4f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"609\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-169efa260eaf2ff2e275e92efa7e9d4f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;609&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"609\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-169efa260eaf2ff2e275e92efa7e9d4f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-169efa260eaf2ff2e275e92efa7e9d4f_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>标签在推荐中的使用大概有三个场景：</p><p><b>① 构建用户画像</b>：当用户点击前面时尚穿搭的视频，我们可以大概推测用户可能是对时尚比较感兴趣的女性用户，然后我们会通过对用户一系列的历史行为更精准的刻画用户的画像。</p><p><b>② 辅助内容推荐</b>：当用户触发一次刷新时，可以参考左边的图片，会调用推荐的接口拉取用户画像中用户的特征表示，然后从用户特征中匹配对应的标签作为 Key 来拉取候选的视频或资讯，放入一系列的排序模型中排序（这样的排序模型也会使用标签作为特征进行训练），再对排序之后的候选列表使用标签来控制多样性，避免推荐单一内容使用户产生烦感（我们会使用标签训练一些相似性的模型来判断两个资讯的相似度做一些过滤或者直接限制每一刷同样标签出现的次数），最后对经过排序和过滤之后的候选列表通过阈值截断，把最终的结果返回给用户。</p><p><b>③ 生成垂直频道</b>：比如有娱乐、搞笑、美女等垂直频道，可以方便用户直接获取感兴趣的内容。</p><p><b>▌标签识别系统架构</b></p><p>接下来介绍下标签识别的系统架构：</p><p><b>1. 标签体系</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-28d3964ed8ce89dbf401ebdf41d01b37_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"609\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-28d3964ed8ce89dbf401ebdf41d01b37_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;609&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"609\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-28d3964ed8ce89dbf401ebdf41d01b37_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-28d3964ed8ce89dbf401ebdf41d01b37_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>我们需要知道什么样的词适合当做标签来使用，因此，需要维护一套标签体系：</p><p><b>① 标签粒度</b>：在信息流的场景下，标签分为实体标签和语义标签，以一部影视剧为例，具体的某一部电影就可以是一个实体标签，某一类电影，比如喜剧片或者动作片，就是一个语义标签。</p><p><b>② 标签更新</b>：由于不断的有新的词出现，需要不断的更新标签库，标签来源（图中红框部分）：搜索日志、爬虫标签、竞品标签、热点事件等。我们会从来源中挖掘出新的候选标签，然后从大搜的知识图谱中拉取标签对应的属性，根据标签属性和标签库已经存在的标签挖掘它的同义词以及关联关系，再经过人工审核把标签和标签对应的属性入到标签库，供后续的使用。</p><p><b>③ 标签关系</b>：同义关系、关联关系（上一步已经介绍，不再复述）</p><p><b>2. 标签系统架构</b></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-bd61b6f12efcd6eb09a305643147e409_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"610\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-bd61b6f12efcd6eb09a305643147e409_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;610&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"610\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-bd61b6f12efcd6eb09a305643147e409_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-bd61b6f12efcd6eb09a305643147e409_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>下面介绍下标签的整体架构：</p><p>我们会用标签预测的 daemon 不断的消费 kafka 的资讯流，kafka 的资讯流包含新入库和属性发生变化的资讯。标签预测的 daemon 会去筛选新入库的资讯或者包含有模型依赖属性发生变化的资讯去请求标签预测的服务，通过把请求写入 kafka request queue。标签的预测服务会按照右边的流程做预测。首先把视频下载下来，进行抽帧或者文本的预处理，然后把预处理之后的数据拿到特征模块中抽取图像、视频、文本等特征，然后把抽取的特征输入到一系列的标签预测的模型中，把得到的每个模型预测出的结果放入后处理的模块中做融合、去重或者扩展，得到最终返回的标签预测结果，同样把结果写到 kafka response queue 中，然后返回到标签预测的 daemon，标签预测的结果直接写入 HBase 或者推到审核平台，经过人工审核之后再写入 HBase。由于有一些领域的标签变化是比较快的，比如影视剧或者综艺的标签，对于这样的标签预测模型，我们会定期重新拉取数据做例行化的训练。</p><p><b>▌标签识别算法</b></p><p>接下来主要介绍下标签识别的算法：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-98bd1fe0b9be94d30ef0e1ee9f7642f2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"611\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-98bd1fe0b9be94d30ef0e1ee9f7642f2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;611&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"611\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-98bd1fe0b9be94d30ef0e1ee9f7642f2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-98bd1fe0b9be94d30ef0e1ee9f7642f2_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>我们会使用 NextVlad 进行全局视频标签识别，在整体的标签识别上得到不错的效果，但是对一些长尾的和专业领域的标签处理的还不够好，对于一些专业领域的标签会再用更适合的模型做处理。比如使用 3D 识别行为标签和使用 mtcbb+InsightFace 识别人物标签。</p><p><b>1. NextVlad</b></p><ul><li><b>最初方案：</b></li></ul><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-03eec1f799b9bd7151f51b919ef0f3ad_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"609\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-03eec1f799b9bd7151f51b919ef0f3ad_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;609&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"609\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-03eec1f799b9bd7151f51b919ef0f3ad_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-03eec1f799b9bd7151f51b919ef0f3ad_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>NetVlad 是2017年 YouTube 8M 视频理解比赛（主要给 YouTube 的视频打标）中提出来的一个网络结构，主要分为三个部分：左边是特征的输入，中间是视频的特征提取，右边是模型的分类阶段。</p><p><b>①</b> <b>视频特征</b>：通过 Inception V3+pca 降维得到的一个1024维的视频特征；音频特征：经过音频的预处理，使用 VGGish+pca 降维得到的128维音频特征，会每秒进行一次采样，相当于每秒会得到一个128维的音频特征，VGGish 模型训练使用的是 Audio set 数据集，类似于在图片领域的 ImageNet 这样的模型，主要是做的是声音的分类，包括动物的声音、机械的声音或者音乐的风格等。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-a9e682ac0b8c2851b1d7e27abe2193be_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"610\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-a9e682ac0b8c2851b1d7e27abe2193be_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;610&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"610\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-a9e682ac0b8c2851b1d7e27abe2193be_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-a9e682ac0b8c2851b1d7e27abe2193be_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>②</b> <b>特征提取：</b>中间的特征提取层使用了双流的 NetVlad 模块，分别提取视频和音频的特征，之后拼接一个全连接层，然后通过 Context Gating 的机制来重新计算特征的相关性。NetVlad 的主要作用是（由于视频的长短可能是不一致的，输入的特征维度是不一样的。）通过把局部的特征聚合为整体特征的方式，得到一个固定长度的向量表示。</p><p>我们看下右边的公式：</p><ul><li>NetVlad：</li></ul><p>多帧特征聚合：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-d0202cc973cf738fdc46077d15fff537_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"220\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-d0202cc973cf738fdc46077d15fff537_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;220&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"220\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-d0202cc973cf738fdc46077d15fff537_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-d0202cc973cf738fdc46077d15fff537_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>i 表示第 M 帧，j 表示第 N 维特征，k 表示聚类的个数 K。αk(xi) 类似 Softmax 函数计算得到的是第 i 帧属于第 k 个聚类的概率，再乘上 j 维特征与 k 个聚类第 j 维的距离得到每一帧的 N x K 维的向量，通过对所有的 M 帧的 N 维向量进行累加，得到 N x K 维的视频级别向量：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-8d22d25e508738eeb846e2f01e7eaa45_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"694\" data-rawheight=\"200\" class=\"origin_image zh-lightbox-thumb\" width=\"694\" data-original=\"https://pic2.zhimg.com/v2-8d22d25e508738eeb846e2f01e7eaa45_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;694&#39; height=&#39;200&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"694\" data-rawheight=\"200\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"694\" data-original=\"https://pic2.zhimg.com/v2-8d22d25e508738eeb846e2f01e7eaa45_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-8d22d25e508738eeb846e2f01e7eaa45_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>Context Gating 作用：</li></ul><p>一方面，计算特征的相关性，使得比较重要的权重变的更大，不相关的权重变得比较小；</p><p>另一方面，引入一些非线性，使其效果更好。</p><ul><li>Moe：</li></ul><p><b>③</b> <b>分类阶段：</b>使用的是混合专家网络（如左下图），把输入 X 分别放入 K 个专家网络，每个专家的单元可以看做一个简单的分类器，再用 gating 网络训练每个分类器的权重，通过线性的加和得到最终预测的效果。这样做的目的是，我们认为每一个专家在他自己的专业领域里会做的比较好，最终的结果是相信了每个专家在他专业领域中的意见，对其进行整合，得到更好的效果。</p><ul><li><b>NextVlad：</b></li></ul><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-74f18a311dfa1596bc9f019cf2621568_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"610\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-74f18a311dfa1596bc9f019cf2621568_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;610&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"610\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-74f18a311dfa1596bc9f019cf2621568_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-74f18a311dfa1596bc9f019cf2621568_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>我们使用的 NextVlad 模型是在 NetVlad 的基础上进行改进的。NextVlad 是2018年 YouTube 8M 比赛中提出来的网络结构，在保持模型性能的同时，大幅降低了参数的数量。上面是原始的 NetVlad 单元的网络结构，下面是改进之后 NextVlad 的网络结构，标红的部分表示需要训练的参数量。NextVlad 首先是把输入 x 的 N 维特征扩张到 λN 维特征，得到 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-5b44069af57fee60a1348a860505777e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"65\" data-rawheight=\"60\" class=\"content_image\" width=\"65\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;65&#39; height=&#39;60&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"65\" data-rawheight=\"60\" class=\"content_image lazy\" width=\"65\" data-actualsrc=\"https://pic3.zhimg.com/v2-5b44069af57fee60a1348a860505777e_b.jpg\"/></figure><p>，再对 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-5b44069af57fee60a1348a860505777e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"65\" data-rawheight=\"60\" class=\"content_image\" width=\"65\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;65&#39; height=&#39;60&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"65\" data-rawheight=\"60\" class=\"content_image lazy\" width=\"65\" data-actualsrc=\"https://pic3.zhimg.com/v2-5b44069af57fee60a1348a860505777e_b.jpg\"/></figure><p> 做 reshape，得到 ( M，G，λN/G ) 的 x 波，这样在我们训练分类的时候，降到了 λN/G x K 的维度， 然后使用 Softmax 计算 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-08d5da63412135c585ef7ff1c2b58b18_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"65\" data-rawheight=\"60\" class=\"content_image\" width=\"65\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;65&#39; height=&#39;60&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"65\" data-rawheight=\"60\" class=\"content_image lazy\" width=\"65\" data-actualsrc=\"https://pic1.zhimg.com/v2-08d5da63412135c585ef7ff1c2b58b18_b.png\"/></figure><p> 属于第 K 个聚类的概率，使用 sigmoid 的门函数计算 group 权重，再通过一个连乘得到 i x G 个的聚类维度乘以 K 的向量，然后通过累加得到了一个 λN/G x K 的 y 维向量，这样在进入到全连接层，全连接层的维度就从 N x K x H 降到了 λN/G x K x H 的维度，这样做的话就使我们在方框内的特征 encoding 阶段整体的特征参数是增加的，可能更好的做特征表达，而在特征全连接层的特征量是下降的，达到了整体的参数量有所下降，而模型性能仍然能保持的效果。</p><ul><li>Squeeze-and-Excite-gating：</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-208fa6259ae3ebc1254d1176bdeeb301_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"609\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-208fa6259ae3ebc1254d1176bdeeb301_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;609&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"609\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-208fa6259ae3ebc1254d1176bdeeb301_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-208fa6259ae3ebc1254d1176bdeeb301_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>提出 NextVlad 的那篇 paper 同时还改进了 Context Gating 结构，通过把 squeeze 阶段的输 入B x F，即 batch size 乘以F维的向量特征，通过全连接压缩到 F/r 维，然后再加入一个全连接层，把 F/r 维重新放大到F维，再加上一个 sigmoid 得到一个 gate 权重，通过 x 乘上 gate 的权重得到最终的输出 Y，这样一方面减少了参数量，另一方面通过两层的全连接层，增加了它的非线性。</p><ul><li>Chaining Moe：</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-d3f28604a47e5d34bec25e30f50b5dcb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"609\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-d3f28604a47e5d34bec25e30f50b5dcb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;609&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"609\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-d3f28604a47e5d34bec25e30f50b5dcb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-d3f28604a47e5d34bec25e30f50b5dcb_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>在分类阶段，使用 Chaining Moe 替代原始的 Moe，其网络结构如上图，每个 Chaining Unit 是把原始输入拼接上前面 Chaining Unit 的输出结果，再输入到 Moe 中。这样做相当于每次我们都选取了一批专家，每次在专家讨论结果之后，再重新选择一个专家团。一方面进一步提升了非线性表达能力，另一方面可以减少过拟合。</p><ul><li>Knowledge Distillation with On-the-fly Navie Ensemble：</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-52a315542b1ea8f2cdb0baadfab6c1a7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"609\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-52a315542b1ea8f2cdb0baadfab6c1a7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;609&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"609\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-52a315542b1ea8f2cdb0baadfab6c1a7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-52a315542b1ea8f2cdb0baadfab6c1a7_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>NextVlad 的作者还提出了使用知识蒸馏的方式来做 On-the-fly 的 Ensemble，Mixture Prediction 相当于一个老师模型，上面的3个 Prediction 相当于3个学生模型，老师模型是通过原始输入求平均之后，训练一个门函数得到每一个学生模型的权重，然后再进行线性加和，得到最终老师模型的输出。老师模型训练的 loss 是原始的视频分类的交叉熵损失函数。每个学生模型是在原始的交叉熵损失函数的基础上加上了老师预测模型的结果，得到的一个 KL 散度 loss 来作为每个学生模型的损失函数。这样做是为了让简单的模型通过学习复杂老师模型的预测结果，使用比较少的参数量也能训练得到比较好的效果。</p><ul><li><b>最终方案：</b></li></ul><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-71ad02b6d2359a62bf9c15af889e9ec8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"610\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-71ad02b6d2359a62bf9c15af889e9ec8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;610&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"610\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-71ad02b6d2359a62bf9c15af889e9ec8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-71ad02b6d2359a62bf9c15af889e9ec8_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>我们最终的线上方案使用了知识蒸馏，用了3个 NextVlad 的子模型，每个子模型的结构如上图：对视频和音频的特征分别使用 NextVlad 进行提取，然后对于文本的特征使用 CNN 提取，再把提取到的多模态特征拼接起来输入到一个全连接层，然后输入到 SE Context Gating 中，最后输入到 C-Moe 中训练分类。</p><p><b>2. 行为标签</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-d4d0d7ddaa8d87fc6de35ca19c9e92e8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"611\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-d4d0d7ddaa8d87fc6de35ca19c9e92e8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;611&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"611\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-d4d0d7ddaa8d87fc6de35ca19c9e92e8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-d4d0d7ddaa8d87fc6de35ca19c9e92e8_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>我们可以看到前面的 NextVlad 模型没有用老视频的前后帧的顺序，一个时空的特征，很多行为是跟时间发生的先后顺序是比较相关的，比如跳起和落下，根据出现的顺序，会是两个完全不同的动作。我们在行为识别上引入了光流特征（可以理解为把 3D 的运动投影到 2D 的图片上），图中的 (a) (b) 是视频中的连续的两帧，(c) 是计算出来的光流信息，为了使用光流信息，我们会把它储存为一个水平的图片和一个垂直的图片，分别包含水平和垂直维度的向量，这样得到的两张图片可以用传统的图片模型进行训练。</p><ul><li>3D ConvNet：</li></ul><p>另外一个方案，使用图片的时空特征，通过 3D 卷积（在 2D 卷积的基础上，feature 又增加了一个维度，除了水平和垂直外又增加一个时空维度，它的窗口除了在水平和垂直上进行滑动，也会在时空上滑动）进行训练。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-6711c53439d01aed0849c6bdfa85845b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"610\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-6711c53439d01aed0849c6bdfa85845b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;610&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"610\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-6711c53439d01aed0849c6bdfa85845b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-6711c53439d01aed0849c6bdfa85845b_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>上图为目前比较主流的行为识别模型，有用到 3D 卷积网络，另外一个比较主流的是把每张图片对应的光流信息拼接起来，然后输入到卷积网络，再合并起来预测动作，我们使用的是 (e) I3D 模型，把每张图片对应的光流信息依次排列分别输入到 3D 卷积网络中，然后把 3D 卷积网络训练的特征拼接在一起预测行为。I3D 模型的主要贡献为：</p><ul><li>提出了 Kinetic 大规模数据集及预训练模型，可以在视频数据比较少的情况下，训练得到一个不错效果；</li><li>通过把 2D ConvNet 扩充成 3D 的 ConvNet，可以在参数初始化的时候用原来在 ImageNet 上预训练的参数做初始化；</li><li>支持 rgb 和 optical flow 分开预测，再合并分数。</li></ul><p>图中还对比了几种行为识别方法以及最终得到的效果，可以看到 I3D 模型在几个公开的数据集上都有比较好的表现。</p><p><b>3. 人物标签</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-f204b93d962954209cfb3c34028f24bb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"610\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-f204b93d962954209cfb3c34028f24bb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;610&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"610\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-f204b93d962954209cfb3c34028f24bb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-f204b93d962954209cfb3c34028f24bb_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>人物标签比较不同的地方，根据人物标签不同的粒度，需要采取不同的方法进行识别。人物标签的识别，可以理解成人脸或者人体的识别问题，我们在现有的模型中，选择了 InsightFace 模型：</p><ul><li>在大规模人脸识别上有较好表现，将 MegaFace 的精度提升到98%；</li><li>创新提出 ArcFace 损失函数，在 Softmax 以及之前在人脸识别领域对 loss 做优化的 SphereFace 和 CosFace 基础上，进一步优化了分类平面，具有更好的几何解释性，使类间的距离是更大的，类内距离是更小的。</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-5cc757ef34b84a1e8fd994dedb45a9aa_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"609\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-5cc757ef34b84a1e8fd994dedb45a9aa_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;609&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"609\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-5cc757ef34b84a1e8fd994dedb45a9aa_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-5cc757ef34b84a1e8fd994dedb45a9aa_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>人物标签生成的流程：</p><p>① 首先会用 mtcnn 检测视频帧中是否存在人脸，并且通过 mtcnn 把人脸区域剪裁出来，然后把检测出来的人脸输入到 Insightface 训练的人脸识别模型和 Insightface 训练的年龄性别模型中分别提取全连接层的向量。</p><p>② 对于实体人物标签我们会对需要识别的具体某一个人使用一批他的人脸图片得到一批他的人脸向量，通过计算得到这个人平均脸的向量，再把这个向量入库到 pangu 中。当有一个新的视频过来的时候，我们对单帧图片识别人脸并计算得到人脸向量，召回 pangu 中最相似的人脸向量，如果召回的人脸向量相似度满足阈值，我们认为图片中出现了对应的人物。</p><p>③ 我们对视频会采取多帧，对多帧分别预测，再通过融合来得到最终这个视频里出现的人物。</p><p>④ 对于年龄和性别这样的粗粒度的标签，我们会把通过 InsightFace 训练的人脸向量和年龄性别的向量拼接起来，然后把他们输入到一个分类模型中分类，得到他具体是某一个类型的人物标签。</p><p><b>▌未来的工作</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-da74d27ea149844daefa84c107ff6a91_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"610\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-da74d27ea149844daefa84c107ff6a91_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;610&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"610\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-da74d27ea149844daefa84c107ff6a91_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-da74d27ea149844daefa84c107ff6a91_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>我们目前还是有很多特征没有使用到，比如视频的一些场景特征和物体检测的特征，包括有些视频它的核心内容体现在视频中的 ocr 或语音上，这些是未来我们打算去尝试的特征。</li><li>利用更多的标签的关联关系，优化我们目前的打标结果。</li></ul><p><b>▌参考资料</b></p><p>这是我们工作中，参考的一些论文，大家感兴趣的话，可以去看一下，今天的分享就到这里，谢谢大家。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-7df04d51fe57436cee34a478d29e52f3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"609\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-7df04d51fe57436cee34a478d29e52f3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;609&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"609\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-7df04d51fe57436cee34a478d29e52f3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-7df04d51fe57436cee34a478d29e52f3_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-8d1f523a6d8c74e4e31717703a2d62f9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"661\" data-rawheight=\"27\" class=\"origin_image zh-lightbox-thumb\" width=\"661\" data-original=\"https://pic2.zhimg.com/v2-8d1f523a6d8c74e4e31717703a2d62f9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;661&#39; height=&#39;27&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"661\" data-rawheight=\"27\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"661\" data-original=\"https://pic2.zhimg.com/v2-8d1f523a6d8c74e4e31717703a2d62f9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-8d1f523a6d8c74e4e31717703a2d62f9_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>分享嘉宾</b></p><p><b>▬</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-45bf77786bb0296653a849059b5d5a30_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"500\" data-rawheight=\"500\" class=\"origin_image zh-lightbox-thumb\" width=\"500\" data-original=\"https://pic1.zhimg.com/v2-45bf77786bb0296653a849059b5d5a30_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;500&#39; height=&#39;500&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"500\" data-rawheight=\"500\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"500\" data-original=\"https://pic1.zhimg.com/v2-45bf77786bb0296653a849059b5d5a30_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-45bf77786bb0296653a849059b5d5a30_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>雪萌</b>阿里巴巴 | 算法专家</p><p><b>——END——</b></p>", 
            "topic": [
                {
                    "tag": "信息流", 
                    "tagLink": "https://api.zhihu.com/topics/19620202"
                }, 
                {
                    "tag": "识别技术", 
                    "tagLink": "https://api.zhihu.com/topics/19573024"
                }, 
                {
                    "tag": "标签（Tag）", 
                    "tagLink": "https://api.zhihu.com/topics/19557891"
                }
            ], 
            "comments": [
                {
                    "userName": "阿水", 
                    "userLink": "https://www.zhihu.com/people/8ab1433684696e60d746d4df1066daf7", 
                    "content": "干货有点多", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/78955633", 
            "userName": "DataFunTalk", 
            "userLink": "https://www.zhihu.com/org/09843313d8c5eff1b7d8bcfa65dc8b68", 
            "upvote": 106, 
            "title": "Hulu：视频广告系统中的算法实践", 
            "content": "<p><b>导读：</b>Hulu 是一家美国领先的互联网专业视频服务平台，商业广告是 Hulu 的重要变现手段之一。视频网站中的广告以担保合约式品牌广告为主，本次演讲分享了机器学习、人工智能技术如何在 Hulu 的广告系统中实践落地，如何帮助广告业务更加高效的运转，介绍了包括精准广告定向、广告流量匹配、广告库存预估等项目中机器学习算法的应用实践。</p><p><b>▌概述</b></p><p>首先介绍一下 Hulu 以及 Hulu 的广告产品形态。</p><p>Hulu 是一家美国的提供专业视频点播与直播的服务平台，成立于2006年，目前由迪士尼控股，拥有近3000万付费订阅用户，是全美用户数量增长最快的流媒体平台。</p><p>① 产品形态：点播，直播，回看等；</p><p>② 广告形态：以视频流中的15秒、30秒的视频广告短片为主；</p><p>③ 广告特点：</p><ul><li>以品牌类广告为主，品牌类广告看重长期效果，品牌的曝光度，用户认知等等；</li><li>采用 CPM ( Cost Per Mille，千次展示 ) 方式计费，即无论用户是否点击，按照展示次数计价；</li><li>以担保式广告订单为主，广告主希望其品牌曝光有一定量的保障，例如，某品牌希望在指定地区给指定用户群投放指定的次数；</li><li>质量要求非常高，包括清晰度、创意、制作的精良程度等等。</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-650ecb4ed84760a8fbf67f124447a393_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"608\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-650ecb4ed84760a8fbf67f124447a393_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;608&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"608\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-650ecb4ed84760a8fbf67f124447a393_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-650ecb4ed84760a8fbf67f124447a393_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>上面简单介绍了 Hulu 和 Hulu 的广告产品形态，接下来介绍下广告系统中我们主要面对的核心算法问题有哪些，以及算法发挥的主要作用。</p><p>广告生态系统有三个核心要素：广告主，用户，媒体方 ( Hulu ) 。每个要素都有自己的优化目标。</p><p><b>广告主：</b></p><p>广告主的核心优化目标是广告的投放效率、有效性，以及 ROAS ( Return-On-Ad-Spend，广告支出回报率 )，算法的主要应用为：</p><p>① 广告定向：使用机器学习算法进行受众定向、基于上下文的定向、lookalike 定向等；</p><p>② 订单及价格优化：给广告主一些订单定向条件及价格的建议，使其在 Hulu 上更好的进行订单的购买和投放；</p><p>③ 转化率优化：越来越多的品牌类的广告主也开始在意短期或者长期的转化率，使用机器学习方法提高转化率，把正确的广告投给正确的用户；</p><p><b>用户：</b></p><p>对于用户的核心优化目标是优化用户体验，尽可能减少广告对用户的干扰，给用户更有连贯性的观看体验，算法的主要应用为：</p><p>① 个性化广告：给用户推荐更感兴趣，更相关的广告</p><p>② 情景式广告：使用 AI 技术进行图像音频的识别、广告创意的生成，为用户带来浸入式广告体验</p><p>③ 个性化频次控制：为用户提供个性化频次控制，避免重复及过量广告为用户带来的反感</p><p><b>媒体方（Hulu）：</b></p><p>最终的优化目标：广告收入最大化，吸引更多的广告商和用户；与此同时我们也会进行生产环境的工作效率优化，算法的主要应用为：</p><p>① 库存预估：这是在担保式广告里面是一个必不可少的环节，在售卖广告以及做广告预算时发挥重要作用</p><p>② 广告流量匹配：对广告订单与用户流量进行匹配，保证担保式广告能够按时按量且均匀的进行投放</p><p>③ 定价策略优化：根据流量的热门程度及售卖压力，进行动态的定价，让整个系统得到一个收入的最大化。</p><p>下图总结了在 Hulu 的视频广告系统中的核心算法问题，也是我们研究员每天在做的事情：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-d173d6eb150073207e15798c670ec17b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"609\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-d173d6eb150073207e15798c670ec17b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;609&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"609\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-d173d6eb150073207e15798c670ec17b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-d173d6eb150073207e15798c670ec17b_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>① 广告定向，涉及到：</p><ul><li>预估模型：例如用户标签的补全，构建 lookalike 模型等等</li><li>主题模型：使用无监督方式挖掘用户属性</li><li>图像识别技术：识别视频内容中物体、场景、氛围等等，投放上下文相关视频广告</li></ul><p>② 库存预估，涉及到：</p><ul><li>时序预测算法：库存预估一般被建模成时序预测问题</li></ul><p>③ 流量匹配，涉及到：</p><ul><li>凸优化：使用凸优化进行离线流量匹配</li><li>自动化控制理论 ( PID Controller )：使用自动化控制理论进行线上动态调整</li></ul><p>④ 转化率优化，涉及到：</p><ul><li>CVR 预估模型</li><li>Casual Inference ( 因果推断 )：使用因果推断，得到一个更加公正、有效的指标来评估转化是否有效。</li><li>Bandit/增强学习：使用 Bandit 以及增强学习的方式来解决广告冷启动的问题</li></ul><p>⑤ 程序化交易广告，涉及到：</p><ul><li>RTB ( Real time bidding ) 策略研究</li><li>流量预估/CVR 预估/投放节奏控制</li></ul><p>⑥ 用户/广告体验分析，涉及到：</p><ul><li>统计理论</li><li>Casual Inference ( 因果推断 )</li></ul><p>⑦ 价格机制设计，涉及到：</p><ul><li>博弈问题</li><li>增强学习</li></ul><p><b>▌核心业务场景中的算法实践</b></p><p>由于篇幅有限，这里着重讲一下在三个比较核心的业务场景下，算法在 Hulu 广告系统中的实践与落地。</p><p><b>1. Ad Targeting 广告定向</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-65ecb2b0967006ab0b44ceebbd7089f6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-65ecb2b0967006ab0b44ceebbd7089f6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;606&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-65ecb2b0967006ab0b44ceebbd7089f6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-65ecb2b0967006ab0b44ceebbd7089f6_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>广告定向在品牌类广告中，是最最重要的一个环节。</p><p>在 Hulu 的场景下，主要有三种类型的广告定向：</p><p>① 上下文相关：例如用户用的设备，在什么位置，在哪个频道，看什么样的视频内容，插播广告点前后的视频内容是什么情景等等。</p><p>② 用户相关：例如用户年龄性别、用户基本属性以及根据用户历史行为挖掘出来的属性等。</p><p>③ 用户广告交互相关：</p><ul><li>Remarketing ( 再营销 )：例如投放广告给最近访问过广告商网站的用户，为他加深品牌印象；</li><li>Look-alike Targeting ( 相似访客定向 )：投放广告给与种子用户相似的受众，扩大受众人数，以及挖掘具有潜能的用户。</li></ul><p>下面分别介绍一下针对这三种类型的广告定向使用到的核心算法：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-4654c1dc1945232e326b3fd3ecd2ebce_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-4654c1dc1945232e326b3fd3ecd2ebce_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;606&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-4654c1dc1945232e326b3fd3ecd2ebce_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-4654c1dc1945232e326b3fd3ecd2ebce_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>a. 用户相关定向：</p><p>我们构建了用户画像系统作为用户定向的基础。底层是一个核心用户平台，负责用户数据的管理，版本控制，每天的 ETL 等等。注入的数据来自三种数据源：</p><p>① 来自第三方数据平台的用户标签、属性，</p><p>② Hulu 自己本身的一些标签，这里有两类工作：</p><ul><li>第一类是工作是标签的补全，我们会用一些例如 XGBoost、DNN 的预测模型来对这些缺失标签的用户进行一个预估，得到用户具有哪些标签，以及概率是多少。</li><li>第二类是无监督模式，我们从用户的历史行为以及其他属性中进行挖掘，挖掘出一些新的属性出来，会涉及聚类、主题模型、user2vec 的方式来生成用户的标签。</li></ul><p>③ 广告商提供的用户标签，针对这种标签最常用的就是进行 Lookalike 定向，我们会用机器学习的方式来进行预测，找到与种子用户相似的用户群体。</p><p>b. 上下文相关定向：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-89d9f869796024bdba0d1cbda88ab048_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"607\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-89d9f869796024bdba0d1cbda88ab048_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;607&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"607\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-89d9f869796024bdba0d1cbda88ab048_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-89d9f869796024bdba0d1cbda88ab048_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>这里主要介绍一下 AI 在视频内容上下文情景广告中的应用。视频广告会插播在视频内容播放前以及播放中， 投放与视频上下文情节比较相关的广告，会减少广告对用户的干扰，举个例子：用户当前所看的是婚礼视频，如果此时投放的广告是珠宝广告，那就毫无违和感，甚至会激起用户的购买欲望。另外，我们需要探测视频中的的一些暴力血腥画面，在这些地方不适合投放广告。</p><p>这个问题的核心是使用图像识别技术检测出视频每帧都出现了什么样的实体、情景以及氛围等。Hulu 结合业界的标准构建了自己的 Taxonomy 体系，分为情绪、物体、场景、位置以及声音几个大的分类。</p><p>简要介绍一下图像检测大致流程：考虑到获取大量数据样本的人力物力成本较大，我们最开始会借助于公开数据集 ( 比如 Open Iamge、Places 365等 )，使用 Inception V3、VGG 等模型去生成一些标签，接下来会根据 Hulu 的场景进行少量的人工标注，对模型进行 Finetune 以及使用多模型融合决定最终的标签。</p><p>c. 转化率优化：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-12acde02138abf1c8d6d097a47dcedb0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-12acde02138abf1c8d6d097a47dcedb0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;606&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-12acde02138abf1c8d6d097a47dcedb0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-12acde02138abf1c8d6d097a47dcedb0_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>虽然品牌类的广告点击率没有那么重要，但是越来越多的广告商开始和 Hulu 谈，我把我的转化数据给你，你帮我计算下我这次投放的转化率是多少。这就带来了另外一个问题，广告商最终还是希望优化自己广告的转化率，所以我们最近开始了这方面的研究。目前开展的研究是基于 Hulu 自己的广告的转化率优化，Hulu 在自己的视频里面也会放一些推广新剧的广告内容，例如一个15秒的预告，希望这些广告能投放给正确的用户，得到比较高的转化率。这个问题被建模成为 CVR 预估的问题，根据投放数据以及转化数据，通过机器学习来建模用户看到广告后的7天内、30天内的转化率，再投放的时候把正确的广告投放给正确的用户。</p><ul><li>转化率优化中的特征 &amp; 模型</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-0af515242fe4709a5e054e967864a037_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"605\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-0af515242fe4709a5e054e967864a037_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;605&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"605\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-0af515242fe4709a5e054e967864a037_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-0af515242fe4709a5e054e967864a037_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>特征主要分为三块：</p><ul><li>User related：基本的用户属性，如位置，活跃度，留存时间等，以及一些重要的历史行为，比如曾经看过哪些内容，搜索过哪些内容，收藏过哪些内容。</li><li>Ad related：广告本身的属性，如行业、剧目本身的信息等等。</li><li>User-Ad：用户这个广告看过多少次了，通过多次投放使用户加深印象。</li></ul><p>模型方面我们目前使用了 DIN + FM 模型进行建模，同时也在探索更好的模型优化。</p><p>d. 广告定向中不可忽视的问题：因果推断和共享账号</p><ul><li>因果推断</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-cdfed64fc447803ab8baa4afa1499e9a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-cdfed64fc447803ab8baa4afa1499e9a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;606&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-cdfed64fc447803ab8baa4afa1499e9a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-cdfed64fc447803ab8baa4afa1499e9a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>我们在评估广告转化率的时候，会考虑评估是否正确，广告是否有效，举个例子：假如有一群用户，不论你对不对他投放广告，他都会去买这辆车，如果把广告投放给这样的用户，最后评估广告的转化率的时候会评估的非常高，其实广告效果一般，因为你投不投他都会去买，其实是白白消耗了这次投放。这就需要引入 Casual Inference 作为评估指标，其核心思想是：评估这个用户看这个广告和不看这个广告的收益分别是怎么样，最后将两者之间的差作为评估指标，使用 Doubly Robust Estimator 进行建模。除此，更加重要的问题是怎么根据 Casual Inference 作为目标去进行转化率的优化，怎么找到这些广告增益最大的用户，然后把广告投放给这些用户。</p><ul><li>共享账号</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-5f34ce12f674077e510653ec4f9f4a33_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"607\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-5f34ce12f674077e510653ec4f9f4a33_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;607&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"607\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-5f34ce12f674077e510653ec4f9f4a33_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-5f34ce12f674077e510653ec4f9f4a33_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>接下来是用户共享账号的问题，例如：一个账号，孩子晚上8点左右看动画片，妈妈晚上10点左右看综艺节目，爸爸周末看体育视频，妈妈使用该账号的时候，发现推荐的都是动画片，找不到自己喜欢看的内容。这个在推荐以及广告定向中都是非常重要的问题，例如有的广告商，希望这个体育类广告只推荐给家庭中的爸爸，如果是小孩和妈妈看是没有效果的。</p><p>这个问题规划到三个方面来解决：</p><p>① 探测这个账号背后有多少个虚拟用户</p><p>② 给每个虚拟用户打上标签，预测他的行为模式</p><p>③ 用户来到 app 的时候预估是哪个虚拟用户，推荐对应的广告和内容</p><p>这个问题核心是一些聚类的算法和预估模型的问题，这也是我们目前正在解决的问题。</p><p><b>2. Inventory Prediction 流量预估</b></p><p>流量预估在担保式广告中是非常重要的环节，广告在售卖之前都要检查是否有足够多的库存余量卖给广告主，以保证售出的广告都能够保量完成投放。</p><p>问题简单描述为：给定未来的一段时间，一些定向条件的组合，预估有多少广告流量库存。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-6b5850882ca2a2d4dce1bc359e382dfe_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"607\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-6b5850882ca2a2d4dce1bc359e382dfe_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;607&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"607\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-6b5850882ca2a2d4dce1bc359e382dfe_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-6b5850882ca2a2d4dce1bc359e382dfe_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>这个问题可以建模成时序预估的问题，通常有以下两个步骤：</p><p>第一步：用时序预估模型来预估总的流量是多少；</p><p>第二步：根据历史分布把流量分配到不同的排列组合，不同的维度上。</p><ul><li><b>Time Series Model</b></li></ul><p>我们目前尝试过以下三类模型：</p><p>第一个是ARIMA ( Autoregressive Integrated Moving Average model )，稳定时序预估中最常用的模型，第二个是来自 Facebook 的 Prophet 模型，第三个是 LSTM。</p><p>ARIMA 作为最为直接简单的稳定时序预估模型，在我们的实验中，在短期时序的预估准确率很不错，但是对长期的预估相对来说就不那么稳定了。原因在于，以下一些因素都会影响到长期的广告流量时序数据：</p><ul><li>用户增长的因素</li><li>季节性的因素</li><li>每周的波动</li><li>节假日和特殊事件</li></ul><p>Prophe 模型是专门为具有这几种特性的时序数据而设计的时序预估模型。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-6568a657400813d8afb7cb15e50283de_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"605\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-6568a657400813d8afb7cb15e50283de_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;605&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"605\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-6568a657400813d8afb7cb15e50283de_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-6568a657400813d8afb7cb15e50283de_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>Prophet 把时序数据拆成了三部分：趋势部分，周期性和季节性部分，节假日部分，每一部分会单独做预测模型，最后合到一起得到最终的预测结果。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-31b3e97a90223b175691b965d6390552_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"607\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-31b3e97a90223b175691b965d6390552_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;607&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"607\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-31b3e97a90223b175691b965d6390552_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-31b3e97a90223b175691b965d6390552_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>例如，上图为 Prophet 模型产生的几种因素的分解：左上角为趋势，左下角为节假日，右边为季度性，分别为每个月和每周的变化，可以观察到淡季和旺季的变化。目前，我们采用的是 Prophet 模型，在 Hulu 的广告流量预估问题上表现稳健。</p><p><b>3. Inventory Allocation 流量匹配</b></p><p>流量匹配在担保式广告的投放环节发挥着重要的作用，保证所有广告订单都可以按时按量并且节奏均匀的投放完成。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-67836566592d2ba944492d4d9a941c97_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-67836566592d2ba944492d4d9a941c97_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;606&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-67836566592d2ba944492d4d9a941c97_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-67836566592d2ba944492d4d9a941c97_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>流量匹配算法会分为以下几个步骤：</p><p>第一步，把广告订单的投放目标拆分到每个小时，这个拆分会根据我们预估的流量和紧俏程度来进行分配；</p><p>第二步，进行 Offline Solving，把问题建模成一个二步图匹配算法，然后使用优化算法进行压缩解的求解；</p><p>第三步，进行线上调控，采用 PID ( Proportion Integration Differentiation ) 控制，根据真实的流量实时进行调控。</p><p>第一个步骤比较简单，下面简单介绍一下第二、三步中具体用到的算法。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-2407723ea14b00d3a9866dfb15fcca96_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-2407723ea14b00d3a9866dfb15fcca96_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;606&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-2407723ea14b00d3a9866dfb15fcca96_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-2407723ea14b00d3a9866dfb15fcca96_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>在离线匹配中，我们把问题建模成一个二部图匹配问题。</p><p>在广告中最主要有 SUPPLY 与 DEMAND 端的两个限制条件：上图中左边  SUPPLY 表示用户带来的流量，流量上会有一些标签，表示用户及广告位的属性，右边 DEMAND 表示广告商的订单，订单上也会有些属性，比如投给男性，或者某个地区的；SUPPLY 与 DEMAND 之间的连线表示这个广告位可以投放的广告。SUPPLY 端的限制条件为最多只有这么多的库存可以使用，DEMAND 端的限制条件为需要将每个广告订单都保量投放完成。</p><p>之后可以根据二部图建模优化问题，求解目标是每个边的权重，表示该流量以多大的比例分配给某个广告订单。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-6f2bfcf72090fa70f899a2183dade515_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-6f2bfcf72090fa70f899a2183dade515_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;606&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-6f2bfcf72090fa70f899a2183dade515_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-6f2bfcf72090fa70f899a2183dade515_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>首先会将原始优化问题转化为拉格朗日对偶问题，求解得到对偶变量 αj 和 βi，表示的分别是广告的对偶变量和流量的对偶变量。根据最优解满足 KKT 条件，我们可以只将相对数量级较低的广告对偶变量 αj 记下来，提高存储效率，线上再根据 KKT 条件计算 βi 和 xij。αj 可以被看做各个订单重要程度影响因子，作为线上选择订单的权重。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-c872b6363c7a8c55267ad2ce2baf376e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"607\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-c872b6363c7a8c55267ad2ce2baf376e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;607&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"607\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-c872b6363c7a8c55267ad2ce2baf376e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-c872b6363c7a8c55267ad2ce2baf376e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>但是这里还是存在一个问题，这里全部都是用的预估的流量和订单来进行匹配，但实际上会发生一些变化。所以需要第三步，线上实时调控。第一可以保证适应真实的线上流量， 第二也可以进一步保证投放节奏的稳定性。一般使用 PID ( Proportion Integration Differentiation ) 控制技术，这是常用于机械、电气系统中的一种应用非常广泛的自动控制技术，分为比例调节、积分调节、微分调节这3项的调节，根据线上广告真实投放的快慢来调控接下来应该投放的速度。</p><p>----------</p><p>以上就是今天的所有内容了，由于篇幅有限，计算广告以及视频广告系统中很多涉及到的算法没有办法一一覆盖，也有很多更有挑战的算法问题亟待研究员和算法工程师们来一一解决。本篇文章作为抛砖引玉，欢迎同行进行探讨指正。另外，对广告算法感兴趣、想要了解算法如何在商业化中落地、想要见证算法如何变现、想要解决具有挑战性问题的同学，欢迎联系：</p><p><a href=\"mailto:chunyang.wei@hulu.com\">chunyang.wei@hulu.com</a></p><p>分享嘉宾：韦春阳 Hulu  </p><p>编辑整理：Hoh Xil</p><p>内容来源：AI 科学前沿大会</p><p>出品社区：DataFun</p><p>注：欢迎转载，转载请注明出处</p>", 
            "topic": [
                {
                    "tag": "人工智能", 
                    "tagLink": "https://api.zhihu.com/topics/19551275"
                }, 
                {
                    "tag": "算法", 
                    "tagLink": "https://api.zhihu.com/topics/19553510"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": [
                {
                    "userName": "Rand Xie", 
                    "userLink": "https://www.zhihu.com/people/106d7aa89b8dfd7528960c8f677335d6", 
                    "content": "请问一下slides可以分享一下么？尝试Google一下但没找到.", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "Onefless", 
                    "userLink": "https://www.zhihu.com/people/ce5794751241bd033fe342b609e41c87", 
                    "content": "这个模型的框架非常不错", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "知乎用户", 
                    "userLink": "https://www.zhihu.com/people/0", 
                    "content": "同求slides", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/77261615", 
            "userName": "DataFunTalk", 
            "userLink": "https://www.zhihu.com/org/09843313d8c5eff1b7d8bcfa65dc8b68", 
            "upvote": 57, 
            "title": "京东电商推荐系统实践", 
            "content": "<figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-209077c8d0e97530cdbcb63a25b1d396_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"605\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-209077c8d0e97530cdbcb63a25b1d396_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;605&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"605\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-209077c8d0e97530cdbcb63a25b1d396_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-209077c8d0e97530cdbcb63a25b1d396_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>今天为大家分享下京东电商推荐系统实践方面的经验，主要包括：</p><ul><li>简介</li><li>排序模块</li><li>实时更新</li><li>召回和首轮排序</li><li>实验平台</li></ul><p><b>▌简介</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-cb8d7a6a7733d40dd711a810dd12556d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-cb8d7a6a7733d40dd711a810dd12556d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;600&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-cb8d7a6a7733d40dd711a810dd12556d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-cb8d7a6a7733d40dd711a810dd12556d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>说到推荐系统，最经典的就是协同过滤，上图是一个协同过滤的例子。协同过滤主要分为俩种：user-based 基于用户的协同过滤和 item-based 基于商品的协调过滤。</p><p>但是，现在绝大多数推荐系统都不会直接使用协同过滤来做推荐。目前主要用的是 learning to rank 框架。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-1c11c6b4872e942b7500be38e5cb4f40_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"532\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-1c11c6b4872e942b7500be38e5cb4f40_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;532&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"532\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-1c11c6b4872e942b7500be38e5cb4f40_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-1c11c6b4872e942b7500be38e5cb4f40_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>这里，是推荐系统的框架，整个推荐系统可以分为两部分，在线部分和离线部分。</p><ul><li>在线部分主要负责当用户访问时，如何把结果拼装好，然后返回给用户。主要模块有召回、排序和对结果的调整。</li><li>离线部分主要是对用户日志的数据分析，应用于线上。</li></ul><p>整个推荐系统大概就是这样的一个框架。</p><p>和新闻、视频这类的内容推荐相比，电商推荐系统又有一些特殊的地方，比如：</p><p>优化方向（点击、销售额、时长、用户留存等）。另外，电商中推荐的内容也会有很多种，尤其像是活动类的内容，很多推荐都是算法和人工运营共同完成的。这就是电商推荐和新闻推荐等的区别之处。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-9087a614bc13d409fd97db0c86d31fd2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"519\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-9087a614bc13d409fd97db0c86d31fd2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;519&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"519\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-9087a614bc13d409fd97db0c86d31fd2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-9087a614bc13d409fd97db0c86d31fd2_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>我们展开看下在线推荐系统：</p><p>除了刚才说的召回和排序以及最终的调整之外，还有实践过程中的一些细节。</p><ul><li>召回：这里召回会有很多种方法，如协同过滤，热门商品、实时促销等和应用场景相关的召回，还有一些基于 KNN 的召回。</li><li>过滤：召回之后，会进行过滤，主要是和应用场景相关，如已购商品过滤掉、没有库存的过滤掉，或者敏感的商品过滤掉等等这些逻辑。</li><li>排序：排序目前主要用到的是 DNN 模型，某些流量比较小的地方会用到 GBDT。</li><li>过滤：排序之后还会有些分页、同商品过滤等逻辑。</li></ul><p>调整：最终调整过程中，主要有两部分逻辑，多样性和探索逻辑。</p><p><b>▌排序模块</b></p><p><b>1. 模型结构</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-9f7c8bc96fdef8765dc27c368dd1edb2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"369\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-9f7c8bc96fdef8765dc27c368dd1edb2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;369&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"369\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-9f7c8bc96fdef8765dc27c368dd1edb2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-9f7c8bc96fdef8765dc27c368dd1edb2_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>深度学习 ranking 模型结构我们不作为重点讨论，这里列举了一种最经典的模型，它们都用到了很多 id 的 Embedding，然后这些 Embedding 规模都很大，这样训练和上线都比较耗时。因此，我们做了一些优化，比如做分布式的训练，并且会有一套 Pipeline 来完成模型的上线。另外，虽然模型很复杂，并且能带来很好的效果，但是特征工程还是必不可少的，很多指标的提升还是依赖于特征工程，当然也包括一些模型调整的工作。</p><p><b>2. 实践</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-00042de129c4d7ea20867d56fa412357_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-00042de129c4d7ea20867d56fa412357_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;600&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-00042de129c4d7ea20867d56fa412357_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-00042de129c4d7ea20867d56fa412357_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>那么如何把这些模型落地呢？我们看下整个模型的上线过程：</p><p>首先最重要的部分是模型训练平台和排序服务，因为很多深度模型计算量要求很高，为了能达到比较快的效果，需要部署单独的排序服务。目前比较流行的是 TensorFlow serving，可以很快速的来上线一个深度模型，并充分利用对分片、单机并行，达到很高的计算效率。</p><p>模型线上线下一致性问题对于模型效果非常重要，我们使用特征日志来实时记录特征，保证特征的一致性。这样离线处理的时候会把实时的用户反馈，和特征日志做一个结合生成训练样本，然后更新到模型训练平台上，平台更新之后在推送到线上，这样整个排序形成了一个闭环。</p><p><b>3. 实时更新</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-439be96dc19cd851d59bb6a2dbf4367f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"530\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-439be96dc19cd851d59bb6a2dbf4367f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;530&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"530\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-439be96dc19cd851d59bb6a2dbf4367f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-439be96dc19cd851d59bb6a2dbf4367f_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>我们的特征和模型都需要做实时的更新。因为我们经常需要很快的 catch 一些实时的信号，比如需要实时的用户画像来抓住实时的用户兴趣的变化，还比如需要抓住实时的商品画像，因为经常会有一些活动或者爆品，我们需要快速的捕捉这些信号，并应用到推荐中。另外还有一些实时的召回和特征，比如一些交叉的特征，实时的点击率，实时订单等特征。</p><p>除了特征外，模型也需要实时更新，对于电商场景来说这是有一定困难的，因为订单是有延时的，延时可能是十几分钟到十几小时不等，这样实时模型更新上就会采取一些保守的策略，比如用点击率对模型做些微调，然后订单数据再通过离线来获得，这属于业务场景的限制。</p><p><b>▌思考</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-2f2e198ba5acbb339e36cdd1da223522_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"525\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-2f2e198ba5acbb339e36cdd1da223522_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;525&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"525\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-2f2e198ba5acbb339e36cdd1da223522_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-2f2e198ba5acbb339e36cdd1da223522_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>排序可以算是推荐系统中比较重要的一个环节，但是只有排序肯定是不够的，事实上，有一些问题是目前的排序框架无法解决的：</p><ul><li>排序得到的结果非常相似，影响体验。</li><li>有多个优化目标，需要一个平衡（点击率、订单金额、用户交互时长等）。</li><li>计算能力有限，如果有无限的计算力，可以直接对全部候选集进行排序。</li></ul><p><b>1. 多样性</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-0d6f1e763d6da6e8a68fa413fc6df502_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"393\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-0d6f1e763d6da6e8a68fa413fc6df502_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;393&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"393\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-0d6f1e763d6da6e8a68fa413fc6df502_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-0d6f1e763d6da6e8a68fa413fc6df502_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>使用模型输出的结果一般都会非常相似，如果直接给用户看体验会很差，因此在模型之后我们需要加入多样性的逻辑。</p><p>比较通用的解决办法是多样性的 ranking，这是一个贪心算法，从第一个商品开始选，当选第二个商品的时候，会重新计算下候选集中每个商品的 score，然后选择一个 score 最高的。我们的方法是看 novelty score 候选商品的产品词分布和之前 N 个商品的产品词分布的 KL 距离。这样做的思路，就是选一个和已有商品最不像的商品，来更好的保证商品推荐结果的多样性。</p><p>由于纯基于算法的多样性可能会出现 badcase，因此还需要一个规则来进行兜底，确保在极端情况下结果也能接受。</p><p>最后，我们思考一个问题，有没有更好的方法实现多样性的逻辑呢？当然有，比如是否可以考虑使用 list wise ranking。这里只是为大家分享一个比较容易的，并且效果比较好的方法。</p><p><b>2. 多目标</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-da5572955f36bc39f3361c09a8761451_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"506\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-da5572955f36bc39f3361c09a8761451_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;506&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"506\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-da5572955f36bc39f3361c09a8761451_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-da5572955f36bc39f3361c09a8761451_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>我们的优化目标有很多，比如点击、转化、时长等，问题会变得比较复杂，单一的模型训练很难覆盖到所有指标。另外，经常我们需要在各个指标之间进行权衡，因此可调试性也非常重要。</p><p>一种很有用的方式是多模型 ranking，然后用某种方式把所有模型的结果 combine。</p><p>这也体现了一个思想，在算法的实际应用中，其实需要在算法的先进性和系统可维护性、可调试性之间做一个平衡。往往 paper 里很有创意的算法落地的时候是有些困难的。</p><p><b>3. 多轮排序</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-ec5b519b90f83dd093fa7dbc2ee6938e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"498\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-ec5b519b90f83dd093fa7dbc2ee6938e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;498&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"498\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-ec5b519b90f83dd093fa7dbc2ee6938e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-ec5b519b90f83dd093fa7dbc2ee6938e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>下面我们讨论一下多轮排序的问题。多轮排序是 learning to rank 实践中很重要的一个思想。使用多轮排序主要是因为计算资源的限制，无法使用复杂的模型进行大规模的候选集排序。右图描述了一个多轮排序的框架。这像是一个漏斗模型，从上往下模型的复杂度是递增的，同时候选集是逐渐减少的，就是越到后面用越复杂的模型来保证效果更好，越到前面可能只需要简单的模型来保证能拿到一些商品就可以了。</p><p>这样会存在一个问题，由于训练样本可能有偏，导致只有被用户看到的样本才有 label，但是一般不会有太大的影响。</p><p><b>▌基于索引的首轮排序</b></p><p><b>1. 索引召回</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-fc908c2ebde835e64d5609acb3e042bf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"505\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-fc908c2ebde835e64d5609acb3e042bf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;505&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"505\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-fc908c2ebde835e64d5609acb3e042bf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-fc908c2ebde835e64d5609acb3e042bf_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>下面我们重点介绍一下第一轮排序。倒排索引很常见，是信息检索里常用的工具。它通过把 doc 的内容索引到 doc id 的方式，快速通过内容来查找 doc。我们很多召回都是通过索引实现的。这里我列举了一些基于索引的召回方式，如 item cf 的 key、产品词、热门类目、促销产品词等。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-01ee672746d0448b39197633f1fb7fda_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"413\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-01ee672746d0448b39197633f1fb7fda_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;413&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"413\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-01ee672746d0448b39197633f1fb7fda_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-01ee672746d0448b39197633f1fb7fda_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>虽然索引能够很大程度上的缩小候选集的范围，但是经常情况下，第一轮排序的 doc 数量仍然可能会很大。为了保证性能，截断逻辑是必不可少的。通过情况下可以通过 quality score 截断，保留质量好的 doc。经过线性的 LR 或者 GBDT 模型就可以有结果了。另外截断之后需要有些多样性的逻辑，因为只有在召回的时候保持多样性，最终结果才会有多样性。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-a9a2dcbe8575ac3530745a62f46112af_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"506\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-a9a2dcbe8575ac3530745a62f46112af_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;506&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"506\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-a9a2dcbe8575ac3530745a62f46112af_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-a9a2dcbe8575ac3530745a62f46112af_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>基于 quality score 截断是一种 naive 的算法，这里我们讨论另一种业界也较常用的算法，wand。wand 其实是 weak and，它的重点是 wand 操作符。wand 操作符是一个布尔操作符，当 Xi wi 比 θ 大时，它的值是1，否则是0。之所以叫做 weak-and，是因为当 w 都取1， θ 取 K 时，wand 操作符就变成了 and，当 w 取1，θ 取1时，wand 操作符就变成了 or。可以看出 wand 是介于 and 和 or 之间的操作。对 Xi wi 求和的操作其实和我们线性模型很相似。通过 wand 操作符，我们可以定义一些上界，因为是倒排索引，可以给每个索引链赋予一个估计值，这样就可以拿到权重上界 UBt，这样通过和 wand 操作符对比，就可以快速的判断 UBt 是否满足条件，如果满足条件就可以快速的把一些 doc 扔掉，这样就可以快速的使用线性模型对全户做 ranking。可以看到，基于线性模型的分数做截断，比完全基于 quality score 截断的策略要稍微好一点。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-fe8928d4d4f054cfea2a71b31866b925_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"592\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-fe8928d4d4f054cfea2a71b31866b925_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;592&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"592\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-fe8928d4d4f054cfea2a71b31866b925_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-fe8928d4d4f054cfea2a71b31866b925_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>这里我列了 paper 中 wand 算法的伪代码。出于时间关系，我们不会过算法逻辑的细节。我认为它的主要的思路是通过快速使用 upper bound 做截断和跳转，可以略过很多明显不符合的候选 doc，从而减少计算 score 的次数。当然这种方法对于线性模型来说，有一个缺点，当我们需要多样性的时候，没办法很好的实现在模型中增加多样性的。</p><p>wand 算法目前已经应用非常广泛了，在很多开源的索引如 lucene 中，也会用到这种方法快速计算文本相关分。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-a73fbb40065f54878b48e6e84e8781b0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1031\" data-rawheight=\"570\" class=\"origin_image zh-lightbox-thumb\" width=\"1031\" data-original=\"https://pic1.zhimg.com/v2-a73fbb40065f54878b48e6e84e8781b0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1031&#39; height=&#39;570&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1031\" data-rawheight=\"570\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1031\" data-original=\"https://pic1.zhimg.com/v2-a73fbb40065f54878b48e6e84e8781b0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-a73fbb40065f54878b48e6e84e8781b0_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>刚刚我们介绍了使用倒排索引做第一轮排序，以及一个常见的排序加速算法，回过来我们思考一下倒排索引本身，它适用于什么场景，不适用于什么场景。</p><p>首先它适用于 kv 查找这种场景，并且 kv 查找也属于实际应用很多的情况。但是对于更复杂的方式，类似 graph 的召回方式不友好，比如找用户看过的商品中相似商品的相关商品，这时实现起来会比较麻烦，这是它的一些限制。再一个，我们需要有较好的截断策略，例如底层使用 relevence score 截断，排序使用 GBDT。</p><p>当然，索引还会受到机器本身的内存限制，限于机器的大小，很多时候我们需要多机分片部署索引，这样会带来一定的复杂性。虽然有些限制，但是索引是目前应用很广泛、有效的方式，包括在推荐、搜索等领域都会使用到。</p><p><b>2. KNN 召回</b><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-d4f5c9a567c3d8ff3fc8ffe8a82c675d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"955\" data-rawheight=\"583\" class=\"origin_image zh-lightbox-thumb\" width=\"955\" data-original=\"https://pic2.zhimg.com/v2-d4f5c9a567c3d8ff3fc8ffe8a82c675d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;955&#39; height=&#39;583&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"955\" data-rawheight=\"583\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"955\" data-original=\"https://pic2.zhimg.com/v2-d4f5c9a567c3d8ff3fc8ffe8a82c675d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-d4f5c9a567c3d8ff3fc8ffe8a82c675d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>除了索引召回，KNN 也是现在较常用的一种召回方式。首先，我们把所有的候选集转换成 embedding，我们把用户兴趣也可以转换成 embedding，通过定义 embedding 之间距离计算公式，我们可以定义 KNN 召回问题，也就是在全部候选池中，找到与用户最接近的 k 个结果。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-64f68231d55319a2ba5878a718f4efe0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"605\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-64f68231d55319a2ba5878a718f4efe0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;605&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"605\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-64f68231d55319a2ba5878a718f4efe0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-64f68231d55319a2ba5878a718f4efe0_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>定义好 KNN 召回的问题，下一步就是如何找到最近的 K 个候选集。由于整个候选集非常大，每次都使用用户的 embedding 去全量计算距离是不现实的，只能使用一种近似算法。我们今天分享其中的一种近似算法。是 facebook 开源的 KNN 计算库 faiss 使用的。其原理：</p><p>首先需要对全部候选集进行分块，每一块都会有自己的质心。paper 中使用 Lloyd 算法，将整个空间划分开。分块后，就需要对每一块构建索引，进而通过索引实现快速检索的功能。</p><p>右图是索引构建和检索的方法。</p><p>上半部分是如何构建索引（这里的优化点是使用了二级索引）：首先拿到 y 候选集之后，做一个 quantizer 分类得到一个一级索引，把它放到索引表中，另外还得到残差 compute residual，可以对残差再进行一次 quantizer，得到一个二级索引，通过两级索引来加快检索的速度，同理，在真正的 quary 的时候，拿到的是用户的向量 x，先做一个 quantizer，得到 k 近邻的一级索引，然后查找 k 个一级索引，同时拿到 k 个二级索引，然后在二级索引中查找，然后这里还有很多加速的算法（这里就不展开了），通过这样一种多层的查询方式来做到加速 K 近邻的算法。</p><p>PS：关于 KNN 的一些思考，KNN 是一种有效的方式，但是不是唯一有效的方式。比如之后分享的 TDM，能够比 KNN 更加灵活。</p><p><b>▌实验平台</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-e527286258d04da10c1ff3a3c7276edf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"517\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-e527286258d04da10c1ff3a3c7276edf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;517&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"517\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-e527286258d04da10c1ff3a3c7276edf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-e527286258d04da10c1ff3a3c7276edf_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>最后简单介绍下分层实验平台，因为大家想快速迭代特征和模型，离不开实验，经常会遇到的情况是实验流量不够用了，这时就需要对实验做分层。分层的逻辑见右图，通过在不同的 Layer 使用不同的哈希函数，保证每个 Layer 之间流量是正交的，这样就可以在不同的 Layer 上做不同的实验。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-c6a6e16d70616e097ac2b3c28a5bedd1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"481\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-c6a6e16d70616e097ac2b3c28a5bedd1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;481&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"481\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-c6a6e16d70616e097ac2b3c28a5bedd1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-c6a6e16d70616e097ac2b3c28a5bedd1_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>分层实验的具体做法：召回-&gt;排序-&gt;后处理-&gt;业务，另外还有一部分对齐流量，用来做全量的验证。</p><p>分层的优点，可以用于做实验的流量多，适合快速迭代；缺点，需要严格控制层与层之间的关系，防止相互干扰。本次分享就到这里，谢谢大家。</p><p>分享嘉宾：孟崇 京东 推荐架构负责人</p><p>编辑整理：Hoh Xil</p><p>内容来源：DataFun AI Talk</p><p>出品社区：DataFun</p><p>注：欢迎转载，转载请注明出处</p>", 
            "topic": [
                {
                    "tag": "人工智能", 
                    "tagLink": "https://api.zhihu.com/topics/19551275"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "推荐算法", 
                    "tagLink": "https://api.zhihu.com/topics/19580544"
                }
            ], 
            "comments": [
                {
                    "userName": "知乎用户", 
                    "userLink": "https://www.zhihu.com/people/0", 
                    "content": "<p>相关性是relevance不是relevence</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "知乎用户", 
                    "userLink": "https://www.zhihu.com/people/0", 
                    "content": "<p>“应用较广泛，而且效果也不错“ 也不应该是索引结构的限制。这个PPT有点不严谨。</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/84206297", 
            "userName": "DataFunTalk", 
            "userLink": "https://www.zhihu.com/org/09843313d8c5eff1b7d8bcfa65dc8b68", 
            "upvote": 88, 
            "title": "阿里妈妈品牌广告中的 NLP 算法实践", 
            "content": "<p><b>导读：</b>本次分享的主题为阿里妈妈品牌广告中的 NLP 算法实践，主要内容包括：</p><p>1. 品牌广告业务模式与技术架构的简要介绍</p><p>2. NLP 算法在品牌搜索广告中的实践，以两个具体的算法问题展开：品牌意图识别和短文本相关性</p><p><b>▌品牌广告业务模式与技术架构</b></p><p><b>1. 阿里妈妈品牌广告业务概况</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-fc76dcfe8e0d2977a8720fd0088eec12_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"670\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-fc76dcfe8e0d2977a8720fd0088eec12_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;670&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"670\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-fc76dcfe8e0d2977a8720fd0088eec12_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-fc76dcfe8e0d2977a8720fd0088eec12_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>阿里妈妈品牌广告构建了丰富的产品矩阵，有强大的生态资源支撑，其中手淘是最大的资源方，阿里系的多个 APP 也是我们重要的资源提供方。品牌广告产品分为两个重要方向——品牌展示广告、品牌搜索广告。其中品牌展示包括超级风暴，整合了各个 APP 开屏页的广告；品牌特秀，主要指手淘内具有稀缺性的 banner 展示广告；以及优酷和 OTT 等视频广告。售卖模式上，品牌展示广告基本以定价保量模式售卖。品牌搜索广告包括两个重要产品，一是明星店铺，以 CPM 模式售卖，二是品牌专区，以 CPT 模式进行售卖。</p><p><b>2. 确定性诉求决定品牌广告业务模式与技术方案</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-6e64e6a85acd71d4c71d5116f4548e81_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"667\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-6e64e6a85acd71d4c71d5116f4548e81_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;667&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"667\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-6e64e6a85acd71d4c71d5116f4548e81_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-6e64e6a85acd71d4c71d5116f4548e81_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>大家可能比较了解效果广告的业务模式与技术架构，那品牌广告如何运作呢？它与效果广告差异非常大，核心差异来自于品牌广告要满足品牌广告主的最大诉求，即确定性。确定性体现在品牌广告全链路强烈的可控性——下单前，对明确的资源进行询量，事先锁定资源；下单后，签订合约，并以固定广告预算进行投放；最后，按合约拿到确定性的投放结果，比如触达多少 UV、PV，N+ Reach、TA% 分布如何。</p><p>基于此，品牌广告业务和技术可以分为询量、下单、投放与评估四个阶段。询量阶段，和效果广告类似，需要实现定向体系。不同点是，需要对定向做流量和可用库存的预估。下单阶段，需要根据广告主确定要购买的定向、时段、预算，进行库存管理，比如库存的扣除或者释放，以及对订单进行报价。投放阶段，会面临多订单流量重叠的情况，要保证广告合约的确定性，技术上会涉及流量分配。在评估阶段，一般需要进行结案报告，如果是纯人工的方式效率会很低，因此我们需要有自动化的洞察、结案系统。最后，对投放的数据资产需要有沉淀的能力，实现持续化的消费者运营。</p><p><b>3. 阿里妈妈品牌广告技术架构</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-f9f040f9da00acf7889311b4a95f0e34_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"675\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-f9f040f9da00acf7889311b4a95f0e34_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;675&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"675\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-f9f040f9da00acf7889311b4a95f0e34_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-f9f040f9da00acf7889311b4a95f0e34_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>简要看一下阿里妈妈的品牌广告的技术框架。首先，从广告主侧视角看，广告主（客户）到达时，首先通过询量平台对要采购的流量、人群、时段和地域等各种定向进行询量询价，背后提供支撑的是库存管理、流量预估和定向技术。订单下单后，将构建广告索引，通过广告引擎实现在线投放，这里面涉及到的关键模块是在线投放、智能创意/频控。另外，从流量侧视角看，一个流量到来，会由统一的流量接入与分发层处理，根据流量类型分发到不同的引擎，效果广告或者品牌广告引擎。</p><p>这里要说的是，品牌广告内部的展示广告与搜索广告在重点技术模块上存在差异。品牌展示广告在整个链路上的重要模块是库存管理与流量预估，以及在线阶段的流量分配。这些是业内研究十分深入的方向，有很多技术资料，这里不做展开。品牌搜索广告，除了以上常规问题，其特殊问题还包括定向技术、智能创意模块，今天会重点分享。</p><p><b>4. 品牌搜索广告产品形态</b><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ccc149bd1b1ca428b94bab81ec941b57_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"670\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-ccc149bd1b1ca428b94bab81ec941b57_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;670&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"670\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-ccc149bd1b1ca428b94bab81ec941b57_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-ccc149bd1b1ca428b94bab81ec941b57_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>我们认识一下品牌搜索广告，左图是业内一种产品形态，右图是阿里品牌专区的一种形态。从技术视角看一下，品牌搜索广告需要解决什么样的问题。首先，需要判别触发词是有品牌意图的 query。上面的两个例子比如“奔驰”、“BALLY 男包”，我们去理解它的品牌意图是非常容易的。对比来看，我们去判断“鲨鱼女包”的品牌意图可能就有点不确定了，甚至部分女生也不太了解鲨鱼是一个包包品牌。再比如“苹果8手机膜”，苹果是手机品牌，但是这样的流量不是品牌广告想要的，因为这个 query 的背后的意图不是买苹果品牌的商品，而是要买某品牌的苹果手机膜。</p><p>另外一个问题是广告匹配约束。品牌广告比效果广告有更强的广告相关性要求，它体现在两个层面。第一，流量与广告主需要品牌匹配，广告主可以将品专作为品牌专属的营销阵地。第二个层面是品牌效果层面的约束，广告主希望广告互动率要尽量的高，对于搜索广告需要解决 query 与广告创意内容的语义匹配。</p><p><b>▌NLP 算法在品牌搜索广告中的实践</b></p><p><b>1. 品牌搜索广告中的 NLP 任务</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-49929cecf776a895be50ca04bbdb2b5f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"674\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-49929cecf776a895be50ca04bbdb2b5f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;674&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"674\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-49929cecf776a895be50ca04bbdb2b5f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-49929cecf776a895be50ca04bbdb2b5f_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>我们对上述问题进行提炼，包含两个技术模块，定向技术与智能创意技术。背后支撑三个重要业务能力：</p><p>第一是品牌流量的挖掘能力，扩充广告流量库存；</p><p>第二是品牌流量分配，解决广告主与流量的品牌匹配问题；</p><p>第三是智能创意匹配，解决的是广告内容与流量的匹配问题。</p><p>进一步，算法上对应两个 NLP 任务：</p><p>1. 品牌意图识别；</p><p>2. item 到 query 的短文本相关性。</p><p>下面重点介绍这两个任务的算法实践。</p><p><b>2. 品牌意图识别</b></p><p>① 问题定义<br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-73f5a98560b442e4d13bf703324dec7a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"671\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-73f5a98560b442e4d13bf703324dec7a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;671&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"671\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-73f5a98560b442e4d13bf703324dec7a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-73f5a98560b442e4d13bf703324dec7a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>品牌意图识别问题描述为，给定一个 query 去识别其是否具有搜索某 brand 下商品的意图。首先，该问题看起来与 entity linking 相似，但实质不同。如刚刚已经看到了一个例子“苹果8手机膜”，entity linking 要识别苹果是一个品牌 mention。但这并不是该任务要解决的问题，而是要识别这个 query 是否有搜索“苹果”品牌的强意图。可以将其看作 classification 问题。我们将解决方案分两个步骤：第一步是 blocking，对给定 query Q 圈选候选品牌 brand B，形成一对一的候选 &lt; Q , B &gt;；第二步是 matching，识别 &lt; Q , B &gt; 是否有相关意图。其中，blocking 可以采用一些启发式的规则，产生 Q 和 B 的候选集。我们将重点介绍 matching 的模型演进。</p><p>② 模型选型</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-1d7c8089f5889b9da97a4e5fa6f77bd2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"675\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-1d7c8089f5889b9da97a4e5fa6f77bd2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;675&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"675\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-1d7c8089f5889b9da97a4e5fa6f77bd2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-1d7c8089f5889b9da97a4e5fa6f77bd2_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>我们对品牌意图识别 matching 模型的演进分为三个阶段，已经完成的是前两个阶段，正在进行第三个阶段。前两个阶段主体思想是基于 Bert 进行 fine-tune。Bert 针对不同的 NLP 任务都有典型的建模方式，在该任务中，可以应用的有两个——Entailment task 与 Multi-Choice task。Entailment 任务中，Q 侧信息作为 Premise，B 侧信息则作为 Hypothesis。Multi-Choice 任务中，B 和 Q 分别作为 Context 和 Answer。</p><p>③ 模型演进</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-d1b2b5c0a5e2ab4cc661102ad65d3241_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"676\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-d1b2b5c0a5e2ab4cc661102ad65d3241_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;676&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"676\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-d1b2b5c0a5e2ab4cc661102ad65d3241_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-d1b2b5c0a5e2ab4cc661102ad65d3241_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>我们详细看一下 Entailment 模型实践，思路是对候选 &lt; Q , B &gt; 经过 Entailment Model 做一个预测，输出 match or no-match。其中，我们重点做了样本特征和模型结构上的一些设计。首先，样本特征上为了引入结构化的外部信息，我们将 Q 和 B 两个 segment 进行了三个 small segment 切分。在 Q 侧，small segment 分别是 query 本身、query click 的商品标题、query 的类目预测信息。B 侧也是一样的，分为候选 B 的标准品牌名称、品牌所属真实商品标题、品牌经营的类目信息。这是模型样本输入。对应地，我们在模型结构上针对 small segment 加入了一层 embedding，以更好地捕捉样本设计中的结构化信息。后续则正常输入到 Bert 进行 encoding，经过 n 层 dense layer 输出预测结果。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-bb12cf18c619929a355aa4ead7495529_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"675\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-bb12cf18c619929a355aa4ead7495529_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;675&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"675\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-bb12cf18c619929a355aa4ead7495529_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-bb12cf18c619929a355aa4ead7495529_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>模型演进的第二阶段是 Multi-Choice 模型。Motivation 是 blocking 步骤可能会产出单 query 命中多个 brand term 的情况，Entailment 任务属于 pointwise 形式的建模，对多 brand term 的打分可比性未直接的做约束和学习，可能导致品牌意图识别错误。例如，“华为荣耀 9X”匹配到两个候选品牌“华为”和“荣耀”，在具体业务场景中需要精确的识别其品牌意图为“荣耀”，而不能是华为。通过上一个部分介绍，我们可以了解到基于 Bert 进行 Multi-Choice 模型结构；在特征层面，Q 和 B 仍然采用一样的 small segment 特征。但在样本构造上，我们进行了一些工作。生成 &lt; Q , B &gt; 时，B 可能只有一个候选 brand term，这时需要提取多个候选的 &lt; Q , B &gt;，构成 Multi-Choice 的大样本。如果 Q 只有一个候选正样例 B，则在 query 的类目下 resample 出 N 个候选负样例 B；反过来，对于 Q 只有候选负样例 B 的情况，我们则对 B resample 一些正样例 Q。如上，从样本到模型构建了完整的 Multi-Choice 训练和预测流程。</p><p>④ 模型评价与技术展望</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-5fa46866a5330286066df4b213d8e080_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"675\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-5fa46866a5330286066df4b213d8e080_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;675&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"675\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-5fa46866a5330286066df4b213d8e080_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-5fa46866a5330286066df4b213d8e080_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>模型评价结果反映了 Multi-Choice 相对 Entailment 有一定效果提升。但是基于 Bert 预训练的语言模型无法解决语义鸿沟问题，尤其是当我们面临一些 knowledge-driven 的问题时，如果不引入 knowledge，模型效果很难有本质的提升。我们再看几个例子：</p><p>1. 刚刚已经提到了“华为荣耀 9X”；</p><p>2. “阿迪运动鞋”，人很容易感知阿迪是一个品牌，但是机器无法识别，这时最好建立“阿迪”与“阿迪达斯”标准品牌的同义关系，同时构建“阿迪达斯”可以经营“运动鞋”类目的知识；</p><p>3. “小棕瓶眼霜”没有直接的品牌 mention，但要理解“小棕瓶”是“雅诗兰黛”强专属度的产品别称，同时“雅诗兰黛”在经营“眼霜”类目。</p><p>以上知识的需求，驱动我们构建营销领域的品牌知识图谱，然后基于 knowledge graph 进行 NLP 建模。目前业内关于 KG 与 NLP 任务结合的研究已经变得非常火热，值得大家关注。</p><p><b>3. 短文本相关性</b><br/></p><p>① 问题定义</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-75ac2acfe95c7446d11800266a84540b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"638\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-75ac2acfe95c7446d11800266a84540b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;638&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"638\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-75ac2acfe95c7446d11800266a84540b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-75ac2acfe95c7446d11800266a84540b_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>第二个算法任务是短文本相关性。问题描述是，给定一个品牌意图词 query 与广告商品 item，计算 &lt; query , item &gt; 短文本的相关性。可以把它理解为语义匹配的问题，采用 learning to rank 解决。进一步的形式化，给定查询的短文本 Q 和对应短文本 D，判别相关性档位，相关性从0到5相关性由弱到强。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-8091077ec542427d1136e28d8b4458db_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"579\" data-rawheight=\"57\" class=\"origin_image zh-lightbox-thumb\" width=\"579\" data-original=\"https://pic4.zhimg.com/v2-8091077ec542427d1136e28d8b4458db_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;579&#39; height=&#39;57&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"579\" data-rawheight=\"57\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"579\" data-original=\"https://pic4.zhimg.com/v2-8091077ec542427d1136e28d8b4458db_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-8091077ec542427d1136e28d8b4458db_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>对于这类问题，我们的评估指标有 AUC 和 NDCG 等。</p><p>② 样本数据</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-8aea799065888a6e9be1697454591be7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"672\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-8aea799065888a6e9be1697454591be7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;672&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"672\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-8aea799065888a6e9be1697454591be7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-8aea799065888a6e9be1697454591be7_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>NLP 监督学习有一个很大的难点，即需要大规模有监督样本，但是其标注工作量巨大，将耗费大量人力和时间，一般公司难以承受。我们的解决思路是，先对测试样本进行人工标注，得到6万个样本，标注为0-5档层级。但是对于模型训练，采用6万规模的样本远远不够。我们的想法是把 learning to rank 的问题简化为一个二分类问题，判断相关还是不相关，而训练样本生成采用弱监督+数据增强的方法。弱监督的一种形式是，采用的数据标注不一定完全正确，但能一定程度上反应数据的相关性。淘宝用户的搜索点击行为就可以构建这样的弱监督样本。正样本非常明确，即 query 点击的商品 item 作为正样本。但未点击的 item 并不等价于不相关，我们需要人为的构建负样本。我们在负样本选取上做了大量工作，例如 query 无点击且 VSM 低的 item，再如与当前 query 低相似度的 query 点击的 item。另外，我们也做了大量的数据增强的工作，不在此展开。</p><p>③ 模型演进</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-6e3943c22ea162bae6a1d80f4e199347_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"674\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-6e3943c22ea162bae6a1d80f4e199347_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;674&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"674\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-6e3943c22ea162bae6a1d80f4e199347_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-6e3943c22ea162bae6a1d80f4e199347_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>构建完样本，考虑如何建模。我们最初的建模思路是采用经典的 DSSM 模型：对于 query Q，其正负样本 item title 用 D 代表，可以构建样本集合 &lt; Q , D+ , D- &gt;，作为 DSSM 的输入样本。我们发现对于 DSSM 模型结果，其算法评测指标比较理想。但在应用层面出现了问题，体现在候选 &lt; Q , D &gt; 的预测得分分布是偏正太分布。前面讲到，品牌广告业务需要满足广告主的确定性，广告主对流量规模、广告效果的稳定性高度关注。业务中需要决策相关性阈值进行 query 分配截断，正太分布下的阈值选择会导致不同天的 query 集合变动非常大，进而带来流量规模与广告效果的不确定性。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-a52ea352f7052e36760de625f70798f8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"677\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-a52ea352f7052e36760de625f70798f8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;677&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"677\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-a52ea352f7052e36760de625f70798f8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-a52ea352f7052e36760de625f70798f8_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>为了解决这个问题，我们做了模型升级。我们注意到 DSSM 拟合的是相似度偏序关系，即对任意 x+, x-: P(x+)&gt;P(x-)；那么对于样本 x，从最大熵的角度看 distribution P(x) 可能更趋向正太分布。我们希望模型直接拟合伯努利分布，即 x+:p(x+)=1, x-:p(x-)=0，这将使得 P(x) 的分布呈现哑铃状。我们考虑从 DSSM 的 pairwise 模型转到 pointwise 的二分类模型，以此来降低阈值选择的影响。另外，希望在新模型上进一步优化效果 ( DSSM 模型基于高层语义表征计算相关性，缺少底层的信息交互 ) ：新模型采用 Compare-Aggregate 架构，在 word-level 就引入了 attention 信息交互。结果表明，新模型使得样本预测得分呈现了哑铃状的理想状态，阈值截断影响大大降低。</p><p>④ 算法评价</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-c0889536dd31023586e86b0491c86455_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"674\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-c0889536dd31023586e86b0491c86455_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;674&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"674\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-c0889536dd31023586e86b0491c86455_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-c0889536dd31023586e86b0491c86455_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>从模型评价上看，Compare-Aggregate 在各个指标上全面超越了 DSSM。这里有两点 Insights：</p><p>1. 面对阈值决策问题时，为了大大降低阈值对结果稳定性的影响，模型选择上建议考虑拟合伯努利分布，避免拟合正太分布。</p><p>2. DSSM 这样的 representation-based model 存在两点局限性：</p><ul><li>模型假设了高层语义表征能够线性可分，但该假设无法验证。</li><li>模型结构上只在最终的 comparison 阶段才有 Q 和 D 的第一次交互，隐层表示缺少足够的信息交互；而 Compare-Aggregation 模型本质是 interaction-based model，从 word level 就开始进行信息交互，获得了更好的向量表示。</li></ul><p>我们认为基于 Bert 上做 fine-tuning 的部分任务也是同理。</p><p><b>▌总结</b></p><p>最后总结和分享一下我们在具体的业务场景进行 NLP 实践的感受：</p><ul><li>NLP 监督学习任务中样本数据获取是难点，可以考虑弱监督与数据增强。在模型方面，深度学习越来越工具化，可以针对实际问题在模型结构上做些思考和创新。</li><li>针对业务场景发现问题、定义问题，是应用算法工程师的核心竞争力；解决问题是应用算法工程师的基础门槛。</li><li>建议算法实践以履带式推进，这点与学术界有所不同，工业落地需要小步快跑，低成本试错，稳步迭代。</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-8d1f523a6d8c74e4e31717703a2d62f9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"661\" data-rawheight=\"27\" class=\"origin_image zh-lightbox-thumb\" width=\"661\" data-original=\"https://pic2.zhimg.com/v2-8d1f523a6d8c74e4e31717703a2d62f9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;661&#39; height=&#39;27&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"661\" data-rawheight=\"27\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"661\" data-original=\"https://pic2.zhimg.com/v2-8d1f523a6d8c74e4e31717703a2d62f9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-8d1f523a6d8c74e4e31717703a2d62f9_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>分享嘉宾</b></p><p><b>▬</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-cbf782ce207149088c90745f9c9b0e19_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"500\" data-rawheight=\"500\" class=\"origin_image zh-lightbox-thumb\" width=\"500\" data-original=\"https://pic2.zhimg.com/v2-cbf782ce207149088c90745f9c9b0e19_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;500&#39; height=&#39;500&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"500\" data-rawheight=\"500\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"500\" data-original=\"https://pic2.zhimg.com/v2-cbf782ce207149088c90745f9c9b0e19_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-cbf782ce207149088c90745f9c9b0e19_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>肖国锐</b>阿里 | 高级算法专家</p><p><b>——END——</b></p>", 
            "topic": [
                {
                    "tag": "品牌广告", 
                    "tagLink": "https://api.zhihu.com/topics/19572225"
                }, 
                {
                    "tag": "算法", 
                    "tagLink": "https://api.zhihu.com/topics/19553510"
                }, 
                {
                    "tag": "自然语言处理", 
                    "tagLink": "https://api.zhihu.com/topics/19560026"
                }
            ], 
            "comments": [
                {
                    "userName": "PaladinTyrion", 
                    "userLink": "https://www.zhihu.com/people/ab60b582204ef24e1ca758b6c069af7a", 
                    "content": "<p>写得不错的~~</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/84034717", 
            "userName": "DataFunTalk", 
            "userLink": "https://www.zhihu.com/org/09843313d8c5eff1b7d8bcfa65dc8b68", 
            "upvote": 3, 
            "title": "OCPC 广告算法在凤凰新媒体的实践", 
            "content": "<p><b>导读：</b>广告算法在优化点击率和竞价机制等提升平台收益的过程中，如果不能保证广告主转化的质和量就会影响到平台生态的稳定发展，因此在 CPC 的基础上，我们探索了 OCPC 模式，并在实际中成功应用。</p><p>本次分享的题目为 OCPC 广告算法在凤凰新媒体的实践探索，主要内容包括：</p><ul><li>背景介绍<br/></li><li>CVR 预估</li><li>二价机制和智能出价</li><li>OCPC 算法</li><li>技术架构</li></ul><p><b>▌背景介绍</b></p><p><b>1. 凤羽简介</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-8a56768756a72006de8cdcb059b136e2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"839\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-8a56768756a72006de8cdcb059b136e2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;839&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"839\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-8a56768756a72006de8cdcb059b136e2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-8a56768756a72006de8cdcb059b136e2_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>凤羽是凤凰卫视程序化广告变现优质品质曝光平台，汇聚了凤凰网、手机凤凰网、凤凰新闻客户端、凤凰视频客户端等多项业务，每天为凤凰网提供20亿次优质流量的曝光。</p><p><b>2. 什么是 OCPC</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-235f75da0f3ddff800e189ba46aaae33_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"604\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-235f75da0f3ddff800e189ba46aaae33_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;604&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"604\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-235f75da0f3ddff800e189ba46aaae33_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-235f75da0f3ddff800e189ba46aaae33_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>OCPC 是效果广告最近两年比较好的业务模式，广告主关心的还是成本 ROI，同时能跑量。 </p><p>CPC 按点击付费，广告主基本都接受这种付费方式。本质上，广告主会根据凤羽的 CPC 消耗，核算他的转化成本。所以广告主为了优化这个成本，他可能调低 CPC，但是抢不到量；选择做人群、时间定向，基于经验论或者数据反馈；或者跑人群包等等。</p><p>相比传统的 CPC 业务效果广告，OCPC 本质上，是通过算法把优化工作替广告主做了。而且由于有更好的手段，所以效果更好。OCPC 还是按照点击付费，但是 OCPC 一般情况下，客户不再出点击价，出的是目标成本价 CPA。OCPC 一般分第一阶段和第二阶段。第一阶段用于数据的冷启动，第二阶段用于积累数据。</p><p><b>3. 定义问题 </b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-700ba9acb40aec6c493781a0d5b44a8f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"649\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-700ba9acb40aec6c493781a0d5b44a8f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;649&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"649\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-700ba9acb40aec6c493781a0d5b44a8f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-700ba9acb40aec6c493781a0d5b44a8f_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>CPA 有两个目标：</p><p>目标1：转化率上升，转化成本下降， ECPM 上升，从而提高客户转化率。</p><p>目标2：成本稳定，消耗稳定，从竞价机制上提出优化点。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-39551db6a8f0a4df90591f992a835b94_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"603\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-39551db6a8f0a4df90591f992a835b94_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;603&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"603\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-39551db6a8f0a4df90591f992a835b94_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-39551db6a8f0a4df90591f992a835b94_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>相对应的，我们的目标是在客户指定成本的情况下，每天相对稳定的跑量。</p><p>我们提出了三种优化手段：</p><ul><li>二价机制，二价机制指的是出价格抢这次的流量，只需支付第二名的价格。</li><li>智能出价，相比二价机制的自然分配，智能出价是一种流量的更积极分配。会根据目标，进行智能的出价操作。</li><li>CVR ( 转化率 ) 预估，CVR 预估是整个阶段的基础，问题是 CVR 预估的样本非常稀少，需要做到多准确？</li></ul><p><b>▌CVR 预估</b><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-94ee9d8d79fdada6dc22a4fde13bd8f9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"603\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-94ee9d8d79fdada6dc22a4fde13bd8f9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;603&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"603\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-94ee9d8d79fdada6dc22a4fde13bd8f9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-94ee9d8d79fdada6dc22a4fde13bd8f9_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>由于在数据非常稀少的时候，很难做出准确和泛化的 CVR 预估，而 OCPC 可以完成。因为我们用到了二价机制，抢到的流量是第二名的价格，不过只要 CVR 的偏差可控，比如小于二价的 Gap，也能获得不错的成本控制。第二层面是 CVR 预估虽然有不准的成分，但我们可以通过智能出价来做经验控制，比如拿转化率高的部分，也可以把成本降下，从这个角度分析，CVR 预估不用特别准确。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-e4e9f04197d83fd42aa992e871e4d695_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"575\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-e4e9f04197d83fd42aa992e871e4d695_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;575&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"575\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-e4e9f04197d83fd42aa992e871e4d695_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-e4e9f04197d83fd42aa992e871e4d695_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>关于 CVR 数据稀疏的问题。新的客户在冷启动阶段的转化不到100个，使用不到100个的转化建立用户的转化率的预估，这个问题是比较困难的。其次，点击转化率与曝光转化率其实不一致，这是一个漏斗序列，先曝光再点击再转化。</p><p>如果用点击行为样本训练模型行预估点击转化率，在线上会出现数据不一致的情况，在数据样本比较小的时候，会有很大偏差。曝光转化率解决了样本不一致的问题，但是考虑到数据稀疏，建模后偏差过大，这种方法被弃用。</p><p>做过数据分析，发现点击率与转化率存在相关性，可以考虑将 CTR 与 CVR 一起联合训练联合建模，它们肯定有很多特征信息可以共用的。比如通过 weightedLR 做 loss 改造，在推荐等其它项目中有过不错的效果，后续可以一试。</p><p>最后，是构建一个模型还是多个模型？是分一个客户建模还是考虑同行业同目标一起建模？关键还是在于业务目标和数据情况：</p><p>OCPC 追求单个客户的完成率，要为单个客户做 ROI 达成和算法归因，同时各个客户之间的差异很大，这和全局最优化不一样。从这个角度出发，在考虑很多方法之后，决定为每个客户单独建模。所以会有很多模型，有很多工程上的版本控制，会考虑泛化与工程准确度的需求，我们会为相似的用户做一些先验的平滑，来提升准确度。后续 OCPC 的客户非常多的时候，可以考虑同一个行业，同一转化目标的类型一起建模，数据的稀少情况也能得到一定缓解。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-b8ae7d94ac3b2a2bf2577f11837e4da2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"602\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-b8ae7d94ac3b2a2bf2577f11837e4da2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;602&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"602\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-b8ae7d94ac3b2a2bf2577f11837e4da2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-b8ae7d94ac3b2a2bf2577f11837e4da2_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>我们目前的 CVR 预估的方法，借鉴 yahoo 的论文，采用一种在线学习的基于特征子空间的 bayes 平滑算法，并融合试探、分裂策略。步骤分为以下五步：</p><p>① GBDT 训练得到若干颗树，形成多种特征子空间 ( tree )；</p><p>② 不同广告采用自己样本驱动每个子空间 ( tree )，计算 CVR ( beta 分布 )，判断置信度；</p><p>③ 在线学习：进行 CPC 模式的退化；</p><p>④ 在线学习：分裂子空间和试探子空间；</p><p>⑤ 相似广告会做贝叶斯平滑，提升准确度提升泛化性能。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-565579ad2ac0a63d978ede57f3664278_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"604\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-565579ad2ac0a63d978ede57f3664278_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;604&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"604\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-565579ad2ac0a63d978ede57f3664278_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-565579ad2ac0a63d978ede57f3664278_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>下面是关于算法的一些细节，简单的说用 GDBT 对样本和特征做随机采样，然后用随机采样出来的多对异构的多棵树，从里面提炼出各个特征子空间，然后对这些特征子空间，每一个广告、每一个客户，我会单独用自己样本探索这些特征子空间，在里面做置信度判断、CVR 预估、试探、分裂策略，来保证 CVR 的准确性和泛化性能。整个算法在实现的过程中还是存在一些问题的，大概列出了以下几个问题：</p><p>① 为什么不采用 GBDT 的预估？</p><p>希望通过在线学习和置信度控制，来得到一个可以业务进化的有效模型和策略；但是，GBDT 仍然是一个有效的预估，可以考虑结合；</p><p>② GDBT 如何构建树？选多少颗树？</p><p>原生办法利用 LightGBM 特征的采样，第二种方法是人工做交叉，去选取特征，在实践中应该选多少棵树，也是看具体业务情况。我们的经验是一般用16颗树。</p><p>③ 如何多颗树的多个子空间给出的 CVR，组合出新的 CVR？有以下的几种做法：</p><p>-&gt; 取平均值。</p><p>-&gt; 参考每个子空间 CVR 的方差，给出加权平均 ( yahoo )。</p><p>-&gt; 参考每个子空间 CVR 的方差，子空间颗粒度做加权平均，子颗粒太大，方差就大，偏差也会大。</p><p>-&gt; 取 CVR 的最大和最小，最大值不推荐，训练模型是右偏的，本来是高估的，用最大值高估现象更严重，会高估用户成本，最小值想对安全，但是最小值可能跑不出量。</p><p>在后续的实践中，我们发现做 LR 集成还是一个更通用的模式。精度会高一些，同时可以将多个相似客户一起建模，提升一定的泛化能力，也减少了模型数量。</p><p>④ 多颗树中，有的子空间置信，有的不置信，如何选择试探、退化等策略？</p><p>-&gt; 冷启动阶段数据太少，要退化到 CPC 模型，平滑了 OCPC 的第一阶段和第二阶段；</p><p>-&gt; 相似广告的子空间数据要做贝叶斯平滑，做 MLE 求解；</p><p>-&gt; 试探策略，计算置信上界，乐观估计，促使拿量；用置信上界。系统中有一个试探系数来控制。实践来看，当采用适当的试探系数时候，客户的成本和平台的 ECPM 能有一个双赢。所以这个试探系数可以作为一个策略学习部分后续优化。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-55db0cb63ed97d5890500279e5da8fc2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-55db0cb63ed97d5890500279e5da8fc2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;600&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"600\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-55db0cb63ed97d5890500279e5da8fc2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-55db0cb63ed97d5890500279e5da8fc2_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>可以通过在线学习，进行快速试探。在线学习的一个很大的优点是可以提升准确度：</p><p>一开始样本特别稀少，CVR 预估 bias 很大，需要快速学习分裂，找到更置信更细化的子空间。另外，当竞价环境出现变化的时候，CVR 预估的偏差会大大增加，导致 OCPC 失败，比如在节假日。</p><p>比如，在正常情况下，可以拿到左边完整的特征分布的流量。但是当竞价变得激烈之后，同样的模型和策略，我只能拿到转化率很低的那部分流量，实际 CVR 的偏差就会很大。这是很失败的一个案例，在线学习能大大提高 CVR 的准确度。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-a08f39270fd0fe2c955b5356d72d722e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"597\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-a08f39270fd0fe2c955b5356d72d722e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;597&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"597\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-a08f39270fd0fe2c955b5356d72d722e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-a08f39270fd0fe2c955b5356d72d722e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>试探策略可以加快业务进化和 CVR 收敛。基于试探策略，将流量分配给不置信的子空间。它有两点好处，如下：</p><p>① 节省 OCPC 的试探弹药。将预算更多投放给可能有高转化率的子空间；能加快 CVR 预估的收敛速度，业务快速进化到第二阶段；</p><p>② 试探有一个刷新人群的效果；偏向于给新人群。</p><p>同时，实践中发现，试探能做为一个有效的策略，来影响最终的多目标达成，所以还是一个很好的智能调价 strategy。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-60f14f6e086e1d7a2e96bef6ad42a109_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"573\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-60f14f6e086e1d7a2e96bef6ad42a109_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;573&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"573\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-60f14f6e086e1d7a2e96bef6ad42a109_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-60f14f6e086e1d7a2e96bef6ad42a109_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>试探策略我们采用的是推荐常用的 UCB，为了避免样本过少，所有做了平滑，取威尔逊的置信上界。这样如果特征空间不置信，我们倾向于给它足够多的试探，就可以让客户第一阶段能更快的收敛。实际中，如果直接用置信上界去试探，由于太高的 CVR 值，可能出现跑量太快的异常情况。所以 CVR 预估值也不能太高，需要做一个上界的约束。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-b0d21911c207033956e357ac4f7e7fba_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"603\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-b0d21911c207033956e357ac4f7e7fba_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;603&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"603\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-b0d21911c207033956e357ac4f7e7fba_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-b0d21911c207033956e357ac4f7e7fba_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>在线学习有一个很明显的问题，转化其实是有很强的样本的延迟，不同的转化目标，不同商品，他的转化周期不一样。比如说 app 的下载转化是很快的，app 的激活可能要延迟好几天，我们在做在线学习的时候需要快速收敛，我们会以小时为单位对数据进行校验，然后开始做实时的返回学习。但是我们会对历史数据做指数平滑，同时我们会每一天、每两天、每三天为窗口的这段时间的数据重新，再重新做下全天的求证，样本延迟带来的偏差。</p><p>同时可以求解客户转化的概率的延迟分布，针对一个小时内的样本做权重上的求证。样本延迟还会带来一个问题，就是转化归因的问题，把转化归因到某一次点击上。比如三天某一个用户点击了广告并下载了 app，但是今天才实行了激活，这样的激活的转化是应该归因于三天前的点击，还是现在这一次的点击，这其实也需要自己取舍。</p><p><b>▌二价机制和智能出价</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-acf448a6ff4c948232a826421747b833_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"609\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-acf448a6ff4c948232a826421747b833_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;609&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"609\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-acf448a6ff4c948232a826421747b833_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-acf448a6ff4c948232a826421747b833_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>二价机制：在实际的过程中，OCPC 客户成本的波动性相当大，同时消耗非常不稳定，根本原因是 CVR 预估做不了太准确（如果可以做的准确就有更简单的解决方案了）。我们会采用智能出价，在二价基础上会更积极，能帮客户在高 CVR 上抢量，低 CVR 上丢量。智能出价带来了更稳定的成本ROI和客户消耗，并平衡平台和广告主收益。</p><p>智能出价：在多个客户情况下，智能出价变成一个组合优化的问题。可以参考阿里论文中贪婪方法求解，其中论文中的时间复杂度还可以优化，可以使时间复杂度降低 log(N) 倍。</p><p>智能出价是一种很有效的一种手段，但是核心还是 CVR 预估要准确，并需要找到一个合理的 CVR 期望。</p><p>关于 E(cvr) 的求解，阿里和新浪据说会采取用户的多个竞品对应极限的转化率。最好有多个竞品，去掉多个竞品中的最大值与最低值，然后取平均比较准确。但是在实际业务开展的过程中，尤其是 OCPC 没有足够竞品的时候，我们采用相对低效的手段，我们取客户本身已经近似的广告最近若干次的 CVR 预估，去除最高值与最低值的10%并取平均，这是一个低效的策略。</p><p>我们还在探索基于强化学习的动态调价。但是这个需要有很大的流量来做支撑，所以不太好建模。</p><p><b>▌OCPC 算法及架构</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-5501b07f559efcd3f32174256861b9c1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"604\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-5501b07f559efcd3f32174256861b9c1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;604&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"604\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-5501b07f559efcd3f32174256861b9c1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-5501b07f559efcd3f32174256861b9c1_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>总结下 OCPC 算法策略的特点：</p><p>① 业务进化，包含试探和分裂的策略，我们并没有明显的 OCPC 第一阶段；</p><p>② 在线学习，可以有一个在线的校准 ( Calibration ) 效果，可以一定程度上降低偏差；</p><p>③ 在小样本的情况下，通过评估置信度，在准确和泛化中找到一个平衡，人工干预整个算法的进程；</p><p>④ 整个算法有试探、分裂等策略，在实践中有一个很快速、很明显的优化链路。</p><p>这是我们截取的客户的案例，经过短短的一周，用户的 CPC 的消耗和成本就能快速稳定下来，蓝色是 CPC 跑出来的曲线，黄色是 OCPC 跑出的曲线，可以发现 OCPC 比 CPC 的成本要低30%左右，同时 OCPC 的波动性也大大降低了，整个算法收敛的还是很快的 。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-2eba6341f2a96b72768e0a8f3c82db4f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"602\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-2eba6341f2a96b72768e0a8f3c82db4f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;602&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"602\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-2eba6341f2a96b72768e0a8f3c82db4f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-2eba6341f2a96b72768e0a8f3c82db4f_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>接下来谈谈 OCPC 其他的细节，除了对算法进行常规 AUC 评测、OE 评测和偏差评测之外，OCPC 还有很多的策略、方法和参数。如果都在线上开启 AB 测试，成本太高；另外，线上测试样本不足，用户的转化可能只有10几个转化，用10几个转化做 AB 测试，在统计上难以置信。</p><p>因此，我们采用虚拟测试的方法进行测试。我们会对用户一段时间内历史的竞价数据进行随机采样，把数据完全复原。然后，基于虚拟的竞价数据，针对我们新的方法和策略进行虚拟测试，观察采用新的算法，成本能否得到优化。但是会有一个问题，这种方法无法完全对等。比如客户的部分流量竞价没有成功，所以没有曝光，这部分流量的后续点击、转化概率其实都是未知的。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-785a8c7d0c40bdd900d8a1e32ebfee97_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"511\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-785a8c7d0c40bdd900d8a1e32ebfee97_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;511&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"511\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-785a8c7d0c40bdd900d8a1e32ebfee97_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-785a8c7d0c40bdd900d8a1e32ebfee97_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>完成上述所有的手段后， 我们必须承认算法的不足，此时需要加入人工干预。干预手段包括：</p><p>① 对于已经转化的用户应该去掉，作弊的用户也去掉；</p><p>② 如果客户效果不好，可以采用刷新，促使用户流转起来；</p><p>③ 适当补量，对有问题的客户，可以通过人群补包补量，其他APP的流量引入；</p><p>④ 运营根据经验来做一些临时策略。</p><p>根据算法的表现，比如结算的 CPC 变低，可能是 CVR 预估偏低，这样的情况下可以做一定的修改。所以我们是用进化的算法+人工干预来保证每个客户的 RI。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-8cbeb6cf7d8459dc647a005377cda1e2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"716\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-8cbeb6cf7d8459dc647a005377cda1e2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;716&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"716\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-8cbeb6cf7d8459dc647a005377cda1e2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-8cbeb6cf7d8459dc647a005377cda1e2_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>最后是我们 OCPC 的技术架构落地。整个过程的关键是在线学习，融合了试探、分裂、净化的策略，在线追求好的工作链路，需要做些监控保证工作正常，可以构建虚拟侧环境，在线上引入 AB 测试，整个架构相对简单，预估部分与 CTR 工程相似，包括特征工程、模型。</p><p><b>▌未来工作</b></p><p>关于下一步的改善方案：</p><p>① 数据稀少，通过落地页的停留时长，来改善数据的稀疏性，停留时长与转化率有很重要的关系；</p><p>② 提升现有算法精度和泛化能力。考虑加入 GBDT 的预估值；采用特征和样本，提高 tree 的异构性；</p><p>③ 尝试采用强化学习的算法。对状态、动作空间都做一些限制，并引入确定的概率。这样在流量不大的场景下，是不是也可以求解。</p><p>分享嘉宾：凤凰新媒体广告算法团队</p><p>编辑整理：陈道昌</p><p>内容来源：2019 DataFun Live 01</p><p>出品社区：DataFun</p>", 
            "topic": [
                {
                    "tag": "CPC（广告模式）", 
                    "tagLink": "https://api.zhihu.com/topics/19623501"
                }, 
                {
                    "tag": "广告", 
                    "tagLink": "https://api.zhihu.com/topics/19553032"
                }, 
                {
                    "tag": "人工智能", 
                    "tagLink": "https://api.zhihu.com/topics/19551275"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/83487924", 
            "userName": "DataFunTalk", 
            "userLink": "https://www.zhihu.com/org/09843313d8c5eff1b7d8bcfa65dc8b68", 
            "upvote": 14, 
            "title": "阿里云小蜜对话机器人背后的核心算法", 
            "content": "<p><b>导读：</b>阿里小蜜智能对话开发平台是智能服务事业部推出的面向各行各业的对话构建平台，此次分享将结合平台，对小样本下的语言理解、用户模拟器和基于模型的对话管理的算法研究和落地进行介绍。</p><p><b>▌0. 对话系统简介</b></p><p>对话系统的一般架构如图：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-1aa1ee3d856bdcf99ac9b333ed38a304_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"642\" data-rawheight=\"480\" class=\"origin_image zh-lightbox-thumb\" width=\"642\" data-original=\"https://pic1.zhimg.com/v2-1aa1ee3d856bdcf99ac9b333ed38a304_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;642&#39; height=&#39;480&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"642\" data-rawheight=\"480\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"642\" data-original=\"https://pic1.zhimg.com/v2-1aa1ee3d856bdcf99ac9b333ed38a304_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-1aa1ee3d856bdcf99ac9b333ed38a304_b.jpg\"/></figure><p>图1：对话系统一般架构</p><p>这是我们所熟知的对话系统框架，这里面主要有：NLU 自然语言理解，DM 对话管理，NLG 自然语言生成3个主要模块，DM 里面有 dialog state tracking 用于对话状态追踪，policy 用于对话策略管理。</p><p>当我们在执行一个对话任务时，例如“开发票”，系统不仅要识别用户的需求，还需要与外部系统对接，进行订单号的合法性校验，调用开发票接口等，这时候 DM 不仅要完成与用户的交互、管理槽位信息，还需要访问外部接口，管理调用的结果。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-192aad6c8fa1752f8286c4b3313defee_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"427\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-192aad6c8fa1752f8286c4b3313defee_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;427&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"427\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-192aad6c8fa1752f8286c4b3313defee_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-192aad6c8fa1752f8286c4b3313defee_b.jpg\"/></figure><p>图2：完整的对话系统架构</p><p>当构建好一个机器人后，还需要进行对话的诊断，效果评测，才能发布上线，如果这些工作全都让人来完成，整个过程会非常费力，于是我们引入了用户模拟器来提高整体交付效率。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-87fe89790b95fa662087105d46a4d913_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"398\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-87fe89790b95fa662087105d46a4d913_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;398&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"398\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-87fe89790b95fa662087105d46a4d913_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-87fe89790b95fa662087105d46a4d913_b.jpg\"/></figure><p>图3：用户模拟器应用到对话系统</p><p>云小蜜对话机器人核心算法主要包括三部分：1. 自然语言理解；2. 对话管理；3. 用户模拟器。</p><p><b>▌1. 自然语言理解</b></p><p>由于云小蜜对话机器人需要满足各行各业各种场景下的对话服务需求，所以我们的自然语言理解是平台视角下的自然语言理解。根据训练样本的多少，我们把它分为3种不同的情况：<b>无样本、小样本、多样本</b>。在没有样本的情况下，我们提供了一套简单易懂的规则表示语法，帮助用户实现快速冷启动。以查天气为例，用户只需写1条规则，就能表示100多个句子。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-7a94cc28b60eecb1fdf27d6b441d6e04_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"440\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-7a94cc28b60eecb1fdf27d6b441d6e04_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;440&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"440\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-7a94cc28b60eecb1fdf27d6b441d6e04_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-7a94cc28b60eecb1fdf27d6b441d6e04_b.jpg\"/></figure><p>图4：无样本自然语言处理解决方案</p><p>在小样本的情况下，比如共10个类别的意图，每个意图下有十多个样本，这种情况下还不足以训练一个有监督模型，但我们可以借助平台数据积累的优势，当只有少量样本的情况下，也可以做出比较好的结果。</p><p>实现思路：我们先整理出一个大数量级的数据（十万级别），每一个类目几十条数据，为它建立 meta-learning 任务。对于一个具体任务来说：构建支撑集和预测集，通过 few-shot learning 的方法训练出 model，同时与预测集的 query 进行比较，计算 loss 并更新参数，然后不断迭代让其收敛。这只是一个 meta-learning 任务，我们可以反复抽样获得一系列这样的任务，不断优化同一个模型。在线预测阶段，用户标注的少量样本就是支撑集，将 query 输入模型获得分类结果。实验表明，few-shot learning 的效果优于无监督相似度匹配的方法。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-6355659df609a092a670d69e66c58ea5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"440\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-6355659df609a092a670d69e66c58ea5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;440&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"440\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-6355659df609a092a670d69e66c58ea5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-6355659df609a092a670d69e66c58ea5_b.jpg\"/></figure><p>图5：小样本自然语言处理方案</p><p>具体是怎么实现的呢？我们借鉴了图像领域的工作，图像领域大多数工作都只考虑了样本的信息，但是在 NLP 领域样本的信息可能会存在噪声或冗余，比如说“开发票”场景，用户在表述开发票这个事情，除了常见的“我要开发票“、”帮忙开一张发票“以外，他还可能会说：“你好，我前两天在你们店里买了一条裙子，请问现在能帮我开下发票吗？”而这样的句子是普遍存在的，我们需要对这些句子进行归纳，得到类别的信息，然后再与要预测的 query 比较语义相似度。它的神经网络结构分为3部分，首先是 Encoder 将句子变成句子向量，然后再通过 Induction Network 变成类向量，最后通过 Relation Network 计算向量距离，输出最终的结果。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-75b690de3f3bf8551fda4a6b3cb63be5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"435\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-75b690de3f3bf8551fda4a6b3cb63be5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;435&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"435\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-75b690de3f3bf8551fda4a6b3cb63be5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-75b690de3f3bf8551fda4a6b3cb63be5_b.jpg\"/></figure><p>图6：小样本训练算法</p><p>Memory-based Induction Network 是我们在 Induction Network 的基础上引入了 memory 机制，目的是模仿人类的记忆和类比能力，在效果上又有进一步提升。</p><p>Induction Network 很关键的一部分就是怎么把样本向量抽象到类向量，我们采用的是 matrix transformation 的方法，下图显示的是1个 5-way 10-shot 的数据，转换前，几个类很难区分，类中心不够内聚，转换后，类边界更清晰，更利于下游 relation 的计算。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ecce979def397658b4d9ec8d315f4254_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"432\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-ecce979def397658b4d9ec8d315f4254_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;432&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"432\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-ecce979def397658b4d9ec8d315f4254_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-ecce979def397658b4d9ec8d315f4254_b.jpg\"/></figure><p>图7：样本空间转换</p><p>当业务方有一定标注数据的情况下，我们就考虑上监督模型了。在云小蜜实际业务场景中，企业相关的标注数据的获取成本是比较高的，因此有监督模型的目标是希望让业务方能够在标注数据量不是很大的情况下达到很好的效果，因此我们构建了一个三层的模型，最底层是具有较强迁移能力的通用模型 BERT，在此基础上构建不同行业的模型，最后用相对较少的企业数据来训练模型。这样构建出来的企业的 NLU 分类模型，F1 基本都在90%+。</p><p>这种模型也有缺点，就是它的结构比较复杂，在线预测的时候延时会比较长，在真实生成环境中应用落地有困难，所以我们通过知识蒸馏的方法来进行模型压缩，在效果相当的同时预测效率更快了。</p><p>在实际业务场景的多数情况下，任务型对话和 FAQ 型问答一般都是同时存在的，我们也引入了多任务学习（multi-task learning），能让任务共享底层的信息并互相增强，使得模型具有更强的泛化能力。在政务场景里，我们通过多任务学习， acc 提升两个点以上。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-c8f3cab9afff98cfe1d4d1bed2c8e5ec_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"436\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-c8f3cab9afff98cfe1d4d1bed2c8e5ec_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;436&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"436\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-c8f3cab9afff98cfe1d4d1bed2c8e5ec_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-c8f3cab9afff98cfe1d4d1bed2c8e5ec_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>图8 ：大规模标注集上的多任务学习</p><p>简单小结一下，这是 NLU 的整体能力输出的能力版图：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-8c1a367cef02c5ee4edf9aa528c123e1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1068\" data-rawheight=\"480\" class=\"origin_image zh-lightbox-thumb\" width=\"1068\" data-original=\"https://pic2.zhimg.com/v2-8c1a367cef02c5ee4edf9aa528c123e1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1068&#39; height=&#39;480&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1068\" data-rawheight=\"480\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1068\" data-original=\"https://pic2.zhimg.com/v2-8c1a367cef02c5ee4edf9aa528c123e1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-8c1a367cef02c5ee4edf9aa528c123e1_b.jpg\"/></figure><p>图9：NLU 的整体能力输出。</p><p><b>▌2. 平台视角下的对话管理</b></p><p><b>对话管理成功的三要素：</b></p><p>1. 业务建模：能够对不同行业不同场景的业务进行抽象，能够用一套统一的表示体系建模，保证业务逻辑的正常运行；</p><p>2. 具备鲁棒性：能够很好的处理业务未定义的通用对话需求和各种异常情况；</p><p>3. 持续学习的能力：能够在与用户交互的过程中，不断的学习，不断适应新场景，根据用户的反馈调整系统的对话策略。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-ed7c25691d9dcaf96c3a2f88b57c5e56_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1025\" data-rawheight=\"327\" class=\"origin_image zh-lightbox-thumb\" width=\"1025\" data-original=\"https://pic3.zhimg.com/v2-ed7c25691d9dcaf96c3a2f88b57c5e56_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1025&#39; height=&#39;327&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1025\" data-rawheight=\"327\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1025\" data-original=\"https://pic3.zhimg.com/v2-ed7c25691d9dcaf96c3a2f88b57c5e56_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-ed7c25691d9dcaf96c3a2f88b57c5e56_b.jpg\"/></figure><p>图10：平台级对话管理的应用需求</p><p><b>2.1 基于 TaskFlow 的业务建模</b></p><p>我们以“火车票”场景为例，一个有经验的卖火车票的售票员在指导一个新售票员的时候，他会把常见的对话样例描述出来，让新售票员知道用户怎么问，我该怎么答，同时他还会告诉新售票员需要查询哪些系统才能获得票务信息，以及有票和无票的情况下分别怎么回复用户。</p><p>基于对上述真实场景的观察，我们对整个交互过程进行抽象，我们认为对话的基本单元是一个 turn ( 一轮 )，它可以拆解为3部分：用户说、机器人思考和机器人回复，分别对应三个基础节点：触发节点、函数节点和回复节点。上面说的是单轮的情形，如果把所有的后一轮的触发节点接到前一轮的回复节点后面，就构成了一个多轮交互 ( multi-turn ) 的对话。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-9da8d724dabbbf4c24b6e8b71232cb97_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"468\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-9da8d724dabbbf4c24b6e8b71232cb97_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;468&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"468\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-9da8d724dabbbf4c24b6e8b71232cb97_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-9da8d724dabbbf4c24b6e8b71232cb97_b.jpg\"/></figure><p>图11：基于 TaskFlow 的建模框架</p><p>为了让 TaskFlow 在平台上执行，我们设计了一种双层状态机的方案，上层是对话逻辑，底层是一套通用的对话引擎，通过这种解耦的设计，不论上层的业务逻辑如何变化，下层都用一套统一的引擎在支撑，如果想赋能上层业务，只需要不断升级底层的能力，上层的所有业务都会受益。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-430ec93527745e774dee8aa342b80250_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"424\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-430ec93527745e774dee8aa342b80250_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;424&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"424\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-430ec93527745e774dee8aa342b80250_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-430ec93527745e774dee8aa342b80250_b.jpg\"/></figure><p>图12：基于双层状态机的方案</p><p><b>2.2 具备鲁棒性</b></p><p>如果对话管理只能保证用户已定义的业务逻辑正确运行，它的鲁棒性还不够，还需要考虑更多情形，包括：通用对话能力和异常处理能力，如图：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-684ca4432627ee99c01c3426a323616f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"302\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-684ca4432627ee99c01c3426a323616f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;302&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"302\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-684ca4432627ee99c01c3426a323616f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-684ca4432627ee99c01c3426a323616f_b.jpg\"/></figure><p>图13：对话系统的鲁棒性</p><p><b>总体的实现思路是，通过系统内置 TaskFlow 实现对话鲁棒性。</b>以“流量包业务”的对话中模糊澄清为例，我们采用插件化的方式实现，在用户定义的 TaskFlow 基础上，增加澄清 TaskFlow，它的功能包括：</p><p>1. 判定是否需要触发澄清；</p><p>2. 选择澄清的策略：是隐式澄清还是显示澄清，是单意图澄清还是多意图澄清；</p><p>3. 澄清话术的生成。</p><p>在上线之前，系统会把这两部分的 Taskflow 进行编译和链接，变成一个可执行的 Taskflow，然后放进执行引擎提供在线服务。其他的功能，比如重听，个性化拒识等都是采用同样的方法实现。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-c174e131305e826b76fc016454d1f3f8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"939\" data-rawheight=\"528\" class=\"origin_image zh-lightbox-thumb\" width=\"939\" data-original=\"https://pic1.zhimg.com/v2-c174e131305e826b76fc016454d1f3f8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;939&#39; height=&#39;528&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"939\" data-rawheight=\"528\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"939\" data-original=\"https://pic1.zhimg.com/v2-c174e131305e826b76fc016454d1f3f8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-c174e131305e826b76fc016454d1f3f8_b.jpg\"/></figure><p>图 14：通过系统内置 TaskFlow 引擎增强系统鲁棒性</p><p><b>2.3 可持续学习</b></p><p>以上的两部分能力都是解决对话中高频、确定的部分问题。如果希望用户在实际对话中越聊越好，仅有以上部分是不够的，需要利用好对话数据，建立对话模型，去 cover 中长尾对话行为，并且基于反馈快速调整对话策略，从而获得更好高价值的智能。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-5b077e791db1ef934b8e1dd145ddb9d5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"977\" data-rawheight=\"478\" class=\"origin_image zh-lightbox-thumb\" width=\"977\" data-original=\"https://pic2.zhimg.com/v2-5b077e791db1ef934b8e1dd145ddb9d5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;977&#39; height=&#39;478&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"977\" data-rawheight=\"478\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"977\" data-original=\"https://pic2.zhimg.com/v2-5b077e791db1ef934b8e1dd145ddb9d5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-5b077e791db1ef934b8e1dd145ddb9d5_b.jpg\"/></figure><p>图15：可持续学习的对话系统</p><p><b>持续学习：</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ef60572a5d7f301a0041ca0c260719a8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"538\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-ef60572a5d7f301a0041ca0c260719a8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;538&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"538\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-ef60572a5d7f301a0041ca0c260719a8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-ef60572a5d7f301a0041ca0c260719a8_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>图16：对话系统增强学习技术架构</p><p>总体分为3步：</p><p>1. 构建 DM 模型，保证它是可学习的；</p><p>2. 让模型可交互学习；</p><p>3. 支持在线学习。</p><p>训练模型得先有数据，我们构建了一个用户模拟器，让它与机器人对话，从而获得大量的带标注的数据，然后分别训练 DST 模型和 Policy 模型，这一步完成了机器人知识的蒸馏，可以获得一个与规则系统效果上等价的 DM 模型。接下来，对 User Goals 进行采样，通过用户模拟器对 DM 模型进行交互，利用 Reward Evaluator 模型进行 Reward 打分，从而获得大量的 Transition 四元组：</p><p> &lt;state(t),act(t),state(t+1),reward(t)&gt;</p><p>利用增强学习 A2C 算法训练，直至收敛。然后发布到线上，进行在线学习。</p><p><b>2.4 Dialog State Tracking（DST）</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-4836f91e8dca501e56d1ecfc659c6735_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"411\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-4836f91e8dca501e56d1ecfc659c6735_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;411&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"411\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-4836f91e8dca501e56d1ecfc659c6735_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-4836f91e8dca501e56d1ecfc659c6735_b.jpg\"/></figure><p>图17：阿里与学术界在 DST 的差异</p><p>真实场景下，我们的 DST 与学术界相比，有以下几点主要差异：</p><p>1. 多智能体建模：DM 不仅跟用户交互，还跟多个外部服务交互，这些服务都可以看做是一个个智能体；</p><p>2. 追踪变量：外部返回的结果会存储在变量中，我们的 DST 扩展了数据形式；</p><p>3. slot-value 假设：学术界假设 slot value 都是离散可枚举的，哪怕是“时间”类型的值，会通过静态化处理进行简化，这显然和实际情况不符，我们不做这样的假设；</p><p>4. 追踪次数：我们对每轮对话中 tracking 的次数不做约束。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-efb9d1035dd2bf31daa831f7574ad5ed_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"699\" data-rawheight=\"556\" class=\"origin_image zh-lightbox-thumb\" width=\"699\" data-original=\"https://pic2.zhimg.com/v2-efb9d1035dd2bf31daa831f7574ad5ed_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;699&#39; height=&#39;556&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"699\" data-rawheight=\"556\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"699\" data-original=\"https://pic2.zhimg.com/v2-efb9d1035dd2bf31daa831f7574ad5ed_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-efb9d1035dd2bf31daa831f7574ad5ed_b.jpg\"/></figure><p>图18：约会议的追踪过程</p><p>如上图所示，一共有两轮对话，每一轮有两次对话追踪。</p><p>DST 模型的输入是上一轮对话的状态、上一轮系统行为、当前轮用户 utterance、API 返回，输出是当前轮的对话状态。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-231b08a6aa55aca488368036f12503d8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1044\" data-rawheight=\"432\" class=\"origin_image zh-lightbox-thumb\" width=\"1044\" data-original=\"https://pic1.zhimg.com/v2-231b08a6aa55aca488368036f12503d8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1044&#39; height=&#39;432&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1044\" data-rawheight=\"432\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1044\" data-original=\"https://pic1.zhimg.com/v2-231b08a6aa55aca488368036f12503d8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-231b08a6aa55aca488368036f12503d8_b.jpg\"/></figure><p>图19：DST 的状态跟踪流程</p><p>这个模型的核心是 operator。在“约会议”场景中，工号的 slot value 一直在变化，开会时间也是不可枚举，如果把 slot value 加入到模型中训练，在预测的时候就会遇到未知的 slot value，此时模型效果就变差。operator 的操作对象是 slot 本身，它只关心信息的流转，而不关心具体 value 是什么，它摆脱了对 value 具体值的依赖，因此具备更强的适应能力。</p><p><b>2.5 Policy: A2C-ER with TaskFlow bootstrapping</b></p><p>下图表示的是增强学习的数据收集和训练的过程。首先，用户输入 user act(t)，进入一个对话状态 state(t)，经过 feature generator 模块，得到 b(t)，接着把它输入到 policy 网络获得 act(t)，这个 act 会和对话状态一起输入到一个 Reward Evaluator 模型进行打分，得到 reward(t)，然后进入下一个对话状态 state(t+1)，这时候会得到一个 experience，它包括：</p><p>&lt;state(t),act(t),state(t+1),reward(t)&gt;</p><p>四元组，然后通过 off-policy 的方式进行训练，而这里收集到的样本分布和 policy 真正输出的样本分布存在偏差，可以利用重要性采样 ( Importance Sampling ) 的方法进行修正，这样能训出一个比较好的 policy 模型。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3ae0b00b83b51792b33fe6599bec7f6d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"911\" data-rawheight=\"576\" class=\"origin_image zh-lightbox-thumb\" width=\"911\" data-original=\"https://pic2.zhimg.com/v2-3ae0b00b83b51792b33fe6599bec7f6d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;911&#39; height=&#39;576&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"911\" data-rawheight=\"576\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"911\" data-original=\"https://pic2.zhimg.com/v2-3ae0b00b83b51792b33fe6599bec7f6d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-3ae0b00b83b51792b33fe6599bec7f6d_b.jpg\"/></figure><p>图20：Policy 的学习过程</p><p><b>▌3. 用户模拟器</b></p><p><b>3.1 Simulation System</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-a350743fce988c79fcb21d6450a48da7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"524\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-a350743fce988c79fcb21d6450a48da7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;524&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"524\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-a350743fce988c79fcb21d6450a48da7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-a350743fce988c79fcb21d6450a48da7_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>图21：Simulation System 的实现原理</p><p>从上面所说的 DM 模型的训练过程，我们可以看到用户模拟器起到非常重要的作用，这里我们介绍一下用户模拟器的实现原理。</p><p>我们的用户模拟器是以 Taskflow 为根基构建的，触发节点展开后形成的抽样树对应用户策略，函数节点+回复节点的串联对应用户状态管理，这是实现用户模拟器的基础。</p><p>它由3部分组成：User State Tracker、User Policy 和 User Model。其中 User Model 可以针对不同的任务设定不同的参数，比如：生成对话数据，对话评测，它们的 Goal 和 Profile 都可以不一样，这样既保证是一套统一的建模框架，同时又保证了系统的灵活性。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-5696eafbcf18ff96ff034096affae47b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"558\" data-rawheight=\"382\" class=\"origin_image zh-lightbox-thumb\" width=\"558\" data-original=\"https://pic4.zhimg.com/v2-5696eafbcf18ff96ff034096affae47b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;558&#39; height=&#39;382&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"558\" data-rawheight=\"382\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"558\" data-original=\"https://pic4.zhimg.com/v2-5696eafbcf18ff96ff034096affae47b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-5696eafbcf18ff96ff034096affae47b_b.jpg\"/></figure><p>图22：User Simulator - 实现框架</p><p>具体的应用场景如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b2d00f515e5e277d68adfd4a1335a005_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"610\" data-rawheight=\"337\" class=\"origin_image zh-lightbox-thumb\" width=\"610\" data-original=\"https://pic2.zhimg.com/v2-b2d00f515e5e277d68adfd4a1335a005_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;610&#39; height=&#39;337&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"610\" data-rawheight=\"337\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"610\" data-original=\"https://pic2.zhimg.com/v2-b2d00f515e5e277d68adfd4a1335a005_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-b2d00f515e5e277d68adfd4a1335a005_b.jpg\"/></figure><p>图23：User Simulator - 应用</p><p><b>3.2 User Simulator - 对话诊断</b></p><p>这是一个公积金查询的对话流，它一共有175条路径，如果要去覆盖需要输入1000次以上，这个过程耗时耗力。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-4d01f7465bac71362084780823d3f4f3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"359\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-4d01f7465bac71362084780823d3f4f3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;359&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"359\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-4d01f7465bac71362084780823d3f4f3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-4d01f7465bac71362084780823d3f4f3_b.jpg\"/></figure><p>图24：对话系统的对话诊断难度</p><p>为了解决这个问题，我们提出了利用一个机器人诊断另外一个机器人的想法，具体的实现架构如下图所示：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-1b97a6711f4e55caa85fe217f46721e5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"463\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-1b97a6711f4e55caa85fe217f46721e5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;463&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"463\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-1b97a6711f4e55caa85fe217f46721e5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-1b97a6711f4e55caa85fe217f46721e5_b.jpg\"/></figure><p>图 25：对话诊断机器人架构</p><p>它的基本逻辑是，首先有一个对话系统，它加载的是 Taskflow，对话系统与用户模拟器交互，产生对话日志，这里面有成功的 session 也有失败的 session，如果是失败的，会把它送到一个诊断分析的模块，进行错误分析和路径统计，得到错误详情及问题产生原因，反馈给业务人员，业务人员根据提示修正 Taskflow，然后重新诊断，以此往复直到所有的问题都解决。</p><p><b>▌4. 总结</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b67b3b99fbeec57b9f5a5eaa8c19f8bd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"949\" data-rawheight=\"406\" class=\"origin_image zh-lightbox-thumb\" width=\"949\" data-original=\"https://pic2.zhimg.com/v2-b67b3b99fbeec57b9f5a5eaa8c19f8bd_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;949&#39; height=&#39;406&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"949\" data-rawheight=\"406\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"949\" data-original=\"https://pic2.zhimg.com/v2-b67b3b99fbeec57b9f5a5eaa8c19f8bd_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-b67b3b99fbeec57b9f5a5eaa8c19f8bd_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>总结如上，这是本次分享的主要内容，谢谢大家！</p><p><b>嘉宾介绍</b></p><p>唐呈光 ( 蒽竹 )，阿里巴巴算法专家。2017年初加入阿里，对话工厂对话引擎负责人，主导人机对话相关技术在真实商业场景的应用，主要研究方向为用户模拟器和对话系统。此前曾在百度从事搜索推荐系统的算法研发工作，发表国家发明专利8项，国际发明专利2项。</p><p><b>——END——</b></p>", 
            "topic": [
                {
                    "tag": "人工智能", 
                    "tagLink": "https://api.zhihu.com/topics/19551275"
                }, 
                {
                    "tag": "算法", 
                    "tagLink": "https://api.zhihu.com/topics/19553510"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": [
                {
                    "userName": "于人明", 
                    "userLink": "https://www.zhihu.com/people/58c999c5869210be5a450162c4ea4fbe", 
                    "content": "<p>最可怕的是一定要先检查知产产权协议，不要最后折腾半天，和百度云一样，无论是开发者还是最终用户产生的所有资料版权全部归属百度，吃相太难看了~~~~~~~~~~</p><a class=\"comment_sticker\" href=\"https://pic2.zhimg.com/v2-32a02829cce575e2a28b03d4e4c5b121.gif\" data-width=\"\" data-height=\"\">[冷静一下]</a>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/82688637", 
            "userName": "DataFunTalk", 
            "userLink": "https://www.zhihu.com/org/09843313d8c5eff1b7d8bcfa65dc8b68", 
            "upvote": 3, 
            "title": "微博广告策略工程架构体系演进", 
            "content": "<p><b>导读：</b>本次分享的主题为微博广告策略工程架构体系演进，将介绍微博广告在从0到1，从1到 N 的过程中，微博广告架构是如何支持策略、算法、模型迭代的，包括以下几部分：</p><ul><li>概述<br/></li><li>微博广告策略工程架构体系演进<br/></li><li>精益驱动思想工具：“两翼计划”</li></ul><p><b>▌概述</b><br/></p><p><b>1. 广告样式与场景</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-2e83a409d9dbcd02068d889821d19abd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"994\" data-rawheight=\"550\" class=\"origin_image zh-lightbox-thumb\" width=\"994\" data-original=\"https://pic2.zhimg.com/v2-2e83a409d9dbcd02068d889821d19abd_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;994&#39; height=&#39;550&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"994\" data-rawheight=\"550\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"994\" data-original=\"https://pic2.zhimg.com/v2-2e83a409d9dbcd02068d889821d19abd_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-2e83a409d9dbcd02068d889821d19abd_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>上图是微博广告目前商业场景流，“一屏四大流”。“一屏”指打开微博的 Fashion，“四大流”指占据微博商业化的主体，包括关系信息流、热门流、评论流和热搜流。右图为广告投放的后台。</p><p><b>2. 广告参与方</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-f7af2bca3823ee142a14525f05f8b3ad_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1015\" data-rawheight=\"534\" class=\"origin_image zh-lightbox-thumb\" width=\"1015\" data-original=\"https://pic2.zhimg.com/v2-f7af2bca3823ee142a14525f05f8b3ad_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1015&#39; height=&#39;534&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1015\" data-rawheight=\"534\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1015\" data-original=\"https://pic2.zhimg.com/v2-f7af2bca3823ee142a14525f05f8b3ad_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-f7af2bca3823ee142a14525f05f8b3ad_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>如上图，做计算广告首先面临这些概念，根据不同的广告主的规模和对公司的重要程度分为 KA 类和中小类。KA 类往往是进行广量式的购买。中小类，常规的客户会进行竞价。</p><p>常见的计费方式有：CPE，CPM，CPD。</p><p>目前在互联网市场在大规模推广 OCPX，OCPX 作为一种需要很高的技术含量，也是一种很好的降低广告主投放风险的售卖方式。</p><p><b>3. 计算广告核心问题</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-e3c6417f4cf6d3a17659a6eb5e34b282_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1004\" data-rawheight=\"509\" class=\"origin_image zh-lightbox-thumb\" width=\"1004\" data-original=\"https://pic3.zhimg.com/v2-e3c6417f4cf6d3a17659a6eb5e34b282_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1004&#39; height=&#39;509&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1004\" data-rawheight=\"509\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1004\" data-original=\"https://pic3.zhimg.com/v2-e3c6417f4cf6d3a17659a6eb5e34b282_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-e3c6417f4cf6d3a17659a6eb5e34b282_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>这是广告涉及到的三方：平台（站方）、用户、广告主，在计算广告设计时的核心问题是如何追求三方之间均衡的、整体的利益最大化。</p><p><b>4. 广告投放流程</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-605cb53441848a1c22012286d4ec46c2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"993\" data-rawheight=\"560\" class=\"origin_image zh-lightbox-thumb\" width=\"993\" data-original=\"https://pic3.zhimg.com/v2-605cb53441848a1c22012286d4ec46c2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;993&#39; height=&#39;560&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"993\" data-rawheight=\"560\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"993\" data-original=\"https://pic3.zhimg.com/v2-605cb53441848a1c22012286d4ec46c2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-605cb53441848a1c22012286d4ec46c2_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>以上是一般的广告的投放流程。这是从广告主的视角、平台的视角以及从用户的视角来公共完成三方“相向而行”的活动：</p><p>广告营销策划的流程：创建推广计划 -&gt; 选择人群受众定向 -&gt; 设置广告预算 -&gt; 设定广告创意 -&gt; 启动广告投放 -&gt; 查看广告效果 -&gt; 下一步营销决策。</p><p>精准广告投放：针对广告库存请求，会对用户进行精准用户画像刻画，然后做广告的召回，对广告进行粗排和精排，挑选广告，根据不同平台进行广告创意渲染工作，最后展示给用户。</p><p>用户内容消费，比较简单请看图中流程。</p><p><b>▌微博广告策略工程架构体系演进</b></p><p><b>1. 微博广告工程架构发展史</b><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-dbe4ec041f48a658cf759ca32fe7ad59_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"998\" data-rawheight=\"563\" class=\"origin_image zh-lightbox-thumb\" width=\"998\" data-original=\"https://pic2.zhimg.com/v2-dbe4ec041f48a658cf759ca32fe7ad59_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;998&#39; height=&#39;563&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"998\" data-rawheight=\"563\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"998\" data-original=\"https://pic2.zhimg.com/v2-dbe4ec041f48a658cf759ca32fe7ad59_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-dbe4ec041f48a658cf759ca32fe7ad59_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>微博商业化进程不断的发展，支撑商业化的工程架构会随着具体的业务需求做改变。</p><p>刚开始做的是非信息流的广告，按照传统的方式会试投一些 banner 在微博，目前 banner 在微博的移动端已经没有了。从1.0版本简单弹窗式的广告系统，到2.0版本以粉丝通为代表的产品线陆续的孵化出来，这时候微博开始进行信息流广告的研发。微博是国内信息流广告的第一家，在探索中孵化出一系列的广告产品矩阵，为了产品的快速上线，复制了大量的广告系统，为了改变这一状况，随着17年底超级粉丝通的上线，对广告系统进行了整体重构，从此微博广告工程架构进入了4.0时代。</p><p><b>2. 投放系统架构4.0</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-03403d37905ac80e25dc8823a663a419_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"985\" data-rawheight=\"556\" class=\"origin_image zh-lightbox-thumb\" width=\"985\" data-original=\"https://pic2.zhimg.com/v2-03403d37905ac80e25dc8823a663a419_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;985&#39; height=&#39;556&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"985\" data-rawheight=\"556\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"985\" data-original=\"https://pic2.zhimg.com/v2-03403d37905ac80e25dc8823a663a419_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-03403d37905ac80e25dc8823a663a419_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>上图是17年的工程架构图，是随着微博广告产品线的探索演化出的工程架构4.0版本，当时处于流量的蓝海阶段，所以通过配合广告客户和广告预算的不断增加，不断提升广告系统的稳定性、高可用性、高并发来不断的实现广告收入的增长。因此我们也对系统进行了分层化的梳理，图中蓝色的区域是线上广告投放系统最核心的链路。通过广告请求，流量统一接入，接入包括微博多个产品矩阵，广告的请求会对多个产品矩阵进行请求分发，通过统一流量价格评估，对用户的请求进行响应。整体上这样一个引擎结构是为了不断的满足产品提出的面对客户的需求来进行的架构设定。</p><p>其基本流程是广告请求，广告库存接入，到总体流量的分发，请求用户的画像，经过竞价服务，会进行广告触发，会请求在线索引服务，形成比较完善的行为定向体系，包括：</p><p>1. 用户的行为定向。在微博上的行为定向，比如，对话题的互动，热门微博的互动，热搜人群的互动等。</p><p>2. 社交关系定向。比如关注的大 V 信息，如果在某个大 V 信息下用户的群体能理解为自然形成的社会群体，那么这个大 V 信息下面的粉丝群是可以进行选择投放的。</p><p>3. 精准人群定向。是由平台方或第三方的数据加工，或广告主根据一次投放效果进行的召回，或已有的客户信息形成的用户粒度聚合后的数据包，这个就是精准用户的集合。</p><p>4. 用户属性定向。包括用户画像，年龄、地域等。</p><p>以上是整体的在线投放流程，但是投放流程仅仅有以上这些是不够的，还包括个性化库存策略，广告的负反馈的策略，智能频次控制策略等，以及配套的 A/B Test 系统，这样就形成了广告投放的在线服务群。</p><p>由于流量来自于微博站方，所以微博广告请求是无需流量反作弊的。存在的反作弊主要是针对互动回传，也就是说广告投后的后链路数据的回传会有大规模的反作弊策略，当然也包括社交互动。然后会有一个实时的结算中心—结算系统，提供给广告主需要的报表，以及与广告主密切相关的账户系统，总体上形成了投后链路在线服务群。</p><p>之下是属于近线的数据访问，按照数据分类：用户基础数据、广告定向数据、广告实时流数据、算法模型训练数据、广告创意库数据来制定线上这种实时访问的需求。</p><p>最下面属于线下的数据仓库。线上投放完的数据会进入数据仓库落地。</p><p>这就是广告数据总线，实现数据流的方式一般通过 kafka 机制等实现，然后汇聚到数据仓库里，对数据进行分门别类。</p><p>图中最左边广告的监控系统，会从系统的各个层面对系统进行业务运行状况的监控以及服务稳定性的监控、可用性的监控。这些就是业务层面完整的一个工具链。原来多个产品线就逐渐聚合到这样的一个系统当中。</p><p>4.0时代的架构在整体上是为了“粗放式增长”而设定的工程架构体系。这种粗放增长的客观现实是，广告预算供应的持续提升和微博不断供给的流量变现规模，以及不断增加广告主的数量、预算和规模，来实现广告数的增长。这时对系统最大的考验是系统的高可用性，以及做业务需求时，对研发效率的保证，这样的架构就是“粗放增长之下的产物”。</p><p>这个架构体系是存在一些问题的（即红框中的）：相对来说对策略模型是相当淡化的，也就是从功能架构层面对算法模型的迭代是比较简单的。比如 A/B Test 使用的是非常原始的 A/B Test，在这种在人口红利的情况下能够快速支撑广告业务的增长，但是随着人口红利的消失，已经不能很好的支撑广告业务的增长，这时系统对策略模型的支持显得异常重要。</p><p><b>3. 系统如何支持广告增长的转型</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-f16ba2b43a5082a44efa1aafe13daca0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1005\" data-rawheight=\"553\" class=\"origin_image zh-lightbox-thumb\" width=\"1005\" data-original=\"https://pic1.zhimg.com/v2-f16ba2b43a5082a44efa1aafe13daca0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1005&#39; height=&#39;553&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1005\" data-rawheight=\"553\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1005\" data-original=\"https://pic1.zhimg.com/v2-f16ba2b43a5082a44efa1aafe13daca0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-f16ba2b43a5082a44efa1aafe13daca0_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>如何支撑广告增长的转型，会通过粗放型增长（扩流、扩广告主、扩预算）转型到精细化的增长，不断的提升投放效果来促进收入的增长。这时对应的系统需要进行转型，在系统不断完善的基础上，实现策略模型的良好驱动。在这种情况下随着算法不断引入新的深度学习模型，整体的工程架构也不断的深耕细作，从原来的业务划分方式（Target、Filter、Rank）转型成面向算法策略（召回、模型、机制、排序）的划分方式。</p><p><b>4. 流量漏斗模型+</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-e475d540d11e28e4da5d0b64664fc735_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"938\" data-rawheight=\"462\" class=\"origin_image zh-lightbox-thumb\" width=\"938\" data-original=\"https://pic2.zhimg.com/v2-e475d540d11e28e4da5d0b64664fc735_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;938&#39; height=&#39;462&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"938\" data-rawheight=\"462\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"938\" data-original=\"https://pic2.zhimg.com/v2-e475d540d11e28e4da5d0b64664fc735_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-e475d540d11e28e4da5d0b64664fc735_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>广告系统常用的模型：流量漏斗模型。对其重新进行思考和定义：开始是对广告进行召回，完成基于精准的最大概率展现，到相关性的挑选，再到以模型为核心的竞价排序机制。</p><p>本文不会从算法角度去讲解如何召回和相关性的机制等，主要是介绍工程是如何支持算法模型迭代的。</p><p><b>5. 面向策略的下一代投放架构</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-810bd982b6745183fd1ccaf468c0f956_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1021\" data-rawheight=\"576\" class=\"origin_image zh-lightbox-thumb\" width=\"1021\" data-original=\"https://pic3.zhimg.com/v2-810bd982b6745183fd1ccaf468c0f956_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1021&#39; height=&#39;576&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1021\" data-rawheight=\"576\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1021\" data-original=\"https://pic3.zhimg.com/v2-810bd982b6745183fd1ccaf468c0f956_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-810bd982b6745183fd1ccaf468c0f956_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>在架构系统4.0中的基础上对在线投放引擎进行业务分级，以满足新的流量漏斗模型。有以下关键点：</p><p>① 触发、模型、策略机制向独立纵深发展</p><p>系统在支持触发、模型、策略迭代上能够实现很好的各自的独立纵深发展，能够做到各自的快速迭代，互不影响。</p><p>② 引入精益驱动思想，系统双核驱动，释放算法迭代效率</p><p>在做整体的精细化转型的时候，系统需要不断的进行尝试，而尝试要有一个很好的尝试平台，所以引入了精益驱动的思想。在线精益平台包括：法拉第实验平台和法拉第精益洞察，这是一个比较好的促进业务模型迭代的工具链，更加注重数据的实时性和数据的密度。</p><p>系统架构总体上分为：在线精益工具平台、在线投放系统、近线数据访问、数据模型加工（曝光机器学习平台和在线实时流的机制）和离线数据平台</p><p>③ 特征数据的实时性和密度，模型独立化发展</p><p>重点说下在线投放服务，在服务中，会有流量接入，会有流量的触发，会有触发机制，包括多路触发，通过多路触发体制后，会有机制策略，包括模型预估服务，模型预估服务是聚合服务，会进行粗排、对数据进行裁剪，会在大规模分布式预估服务中完成，Ranker 也会基于预估进行精排。</p><p>特别说明一下，为什么进行粗排和精排，粗排我的理解是为了精排的性能考虑的，因为精排会涉及到大规模的精细计算，性能有可能会扛不住，所以需要粗排，而且在保证效果的情况下，为了性能的保证会有多级粗排。</p><p>最上面是法拉第实验平台和法拉第精益洞察。整体上会形成双核引擎：一个良好的工程架构体系和精益驱动的工具平台。</p><p><b>6. 如何支持广告物料的精准召回</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-55386a77d4c2f86c76d72ac8e01166cf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1011\" data-rawheight=\"573\" class=\"origin_image zh-lightbox-thumb\" width=\"1011\" data-original=\"https://pic4.zhimg.com/v2-55386a77d4c2f86c76d72ac8e01166cf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1011&#39; height=&#39;573&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1011\" data-rawheight=\"573\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1011\" data-original=\"https://pic4.zhimg.com/v2-55386a77d4c2f86c76d72ac8e01166cf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-55386a77d4c2f86c76d72ac8e01166cf_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>从微博广告的召回机制来看会有用户标签触发、社交传播触发、精准人群触发、内容触发、DNN 向量式触发，经过5路触发，会进行 MIXER 广告召回级的汇总，汇总后会有粗排策略、精排策略。</p><p>这里用到的信息，包括流量侧和广告侧。</p><ul><li>流量策：</li></ul><p>用户画像、请求上下文、历史互动行为</p><ul><li>广告侧：<br/></li></ul><p>广告主信息、计划信息、创意信息<br/></p><p><b>7. DNN 向量触发模型</b><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-1d8816d5487d767b0eae0dd186f6a8ff_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"977\" data-rawheight=\"549\" class=\"origin_image zh-lightbox-thumb\" width=\"977\" data-original=\"https://pic4.zhimg.com/v2-1d8816d5487d767b0eae0dd186f6a8ff_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;977&#39; height=&#39;549&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"977\" data-rawheight=\"549\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"977\" data-original=\"https://pic4.zhimg.com/v2-1d8816d5487d767b0eae0dd186f6a8ff_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-1d8816d5487d767b0eae0dd186f6a8ff_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>这里介绍下深度向量触发模型，用到的是双塔模型，包括用户侧和广告侧。用户侧根据用户信息进行训练生成用户侧的向量，用到了三层的神经网络，广告侧也是一样，整体来说训练都是采用离线的方式完成的，接下来会做实时的向量预估。进而在双塔汇合点进行相关性的判定，使用了简单的 cos 和 sigmoid 进行相关性的判定。</p><p><b>8. 触发工程架构</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-9e91991cae4ee051589c0a36992e8640_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1007\" data-rawheight=\"563\" class=\"origin_image zh-lightbox-thumb\" width=\"1007\" data-original=\"https://pic1.zhimg.com/v2-9e91991cae4ee051589c0a36992e8640_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1007&#39; height=&#39;563&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1007\" data-rawheight=\"563\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1007\" data-original=\"https://pic1.zhimg.com/v2-9e91991cae4ee051589c0a36992e8640_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-9e91991cae4ee051589c0a36992e8640_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>触发工程体系在如何更好的支持触发角度来说，研发了对应的服务体系。请求在请求召回时会触发 Agent 进行五路召回，包括双塔的召回、内容定向的召回、用户画像的召回、精准人群的召回、微博社交关系的召回，召回之后进行 Mixer，会结合质量预估服务进行裁剪。然后，线上会有实时的广告计划库数据和离线数据会对线上的五路触发分别根据需求进行访问，这样会形成一个整体的广告触发工程架构。</p><p><b>▌精益渠道思想工具：“两翼计划”</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-8873393e745bea4eced34dca43fd6ff5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1006\" data-rawheight=\"569\" class=\"origin_image zh-lightbox-thumb\" width=\"1006\" data-original=\"https://pic2.zhimg.com/v2-8873393e745bea4eced34dca43fd6ff5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1006&#39; height=&#39;569&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1006\" data-rawheight=\"569\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1006\" data-original=\"https://pic2.zhimg.com/v2-8873393e745bea4eced34dca43fd6ff5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-8873393e745bea4eced34dca43fd6ff5_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>随着超级粉丝通在17年上线，考虑到微博广告从粗放式增长到精细化增长的转型，需要有一个比较好的实验平台和一种线上运行洞察的方式和思想，这样就构成了精益驱动思想的来源。包括两部分：一是进行线上策略的实验和调控，一个是用于线上策略运行的精益洞察，与在线投放系统一起，构成一体两翼策略工程架构体系。</p><p><b>1. 法拉第实验平台</b><br/></p><p>① 法拉第分层实验模型</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-5204b7e2645edb145c662448d08b076c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1014\" data-rawheight=\"566\" class=\"origin_image zh-lightbox-thumb\" width=\"1014\" data-original=\"https://pic1.zhimg.com/v2-5204b7e2645edb145c662448d08b076c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1014&#39; height=&#39;566&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1014\" data-rawheight=\"566\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1014\" data-original=\"https://pic1.zhimg.com/v2-5204b7e2645edb145c662448d08b076c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-5204b7e2645edb145c662448d08b076c_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>法拉第实验采用正交分层模型，这种模型在互联网行业一般都会使用，包括百度的爱迪生、阿里的特斯拉等。微博法拉第模型思想来自于 Google 流量正交分解模型的经典论文，为多层的独立实验提供了理论基础，当然实验也结合了微博广告的实际情况，包括刚开始解决从无到有的问题，对论文中的模型进行了简化。最开始是没有域的概念，整体上每一层都会使用 hash 函数。</p><p>② 实验分桶</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-09e558369d5a61cf348dde86aa7bfc90_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1011\" data-rawheight=\"571\" class=\"origin_image zh-lightbox-thumb\" width=\"1011\" data-original=\"https://pic1.zhimg.com/v2-09e558369d5a61cf348dde86aa7bfc90_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1011&#39; height=&#39;571&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1011\" data-rawheight=\"571\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1011\" data-original=\"https://pic1.zhimg.com/v2-09e558369d5a61cf348dde86aa7bfc90_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-09e558369d5a61cf348dde86aa7bfc90_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>虽然每个实验层共享 hash 函数，但是 hash 函数参数不同，参数包括流量标识和实验 id 标识，呈现出的是不同层的流量分桶划分是正交的。另外，也引入 Google 论文中提到的分配条件，场景应用很经典，例如做一个实验，会考虑流量的重用或者实验的一些特征，包括实验的地域、性别等，这样的话，使用圈定或限定下的流量而不是使用全部的流量；如果使用全部流量的话会造成实验效果不显著，实验效果被稀释，容易引起不置信。这样可以使流量重复复用，并且能很好的观测到策略所产生的实验效果。这就是流量分桶类型和进行流量圈定的分配条件。</p><p>③ 法拉第实验平台</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-0f8113d6feab853c8d97f13ad48601d9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1014\" data-rawheight=\"569\" class=\"origin_image zh-lightbox-thumb\" width=\"1014\" data-original=\"https://pic2.zhimg.com/v2-0f8113d6feab853c8d97f13ad48601d9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1014&#39; height=&#39;569&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1014\" data-rawheight=\"569\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1014\" data-original=\"https://pic2.zhimg.com/v2-0f8113d6feab853c8d97f13ad48601d9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-0f8113d6feab853c8d97f13ad48601d9_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>这个是法拉第实验平台的整体架构图。采用全程自动化的机制。通过法拉第的 Web 入口，进行实验信息的记录，在线上流量入口处进行实验的下发和解析，根据实验的数据信息，进行线上策略的调控，会对实验命中数据进行解析，进行实验的埋点，然后进入实时的分析引擎，统计实验效果。</p><p>实验的下发和解析有两种方式：</p><p>一种是在流量的总入口处统一进行实验的下发和解析，一步到位，然后请求信息一块下发，随着请求链路走，最后返回，命中那些策略会有对应的标识。如果实验数比较少时，解析无压力，实验相关性解析比较小，实验消耗带宽不大，此时是合适的。但是随着实验规模的增大，由于现在的广告系统是分布式的系统，如果完整的实验信息一直随着请求下发的话带宽消耗会非常严重，造成返回结果超时，可用性下降，实验时间就会变得很长。因此出现了另一种方式，由对应的服务分别对对应策略的实验情况进行解析，而其他策略的实验情况则不需要解析，这样就只获取自己感兴趣的信息，避免了信息的冗余。</p><p>为什么刚开始没有进行这样的设计，因为系统刚开始是解决从无到有的问题而不是一到多的问题。第一步是实验平台的创建，采用第一种方式比较简单，可以很快的将实验下发。</p><p>④ 广告侧 A/B 实验</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-2b1907620fff008a5f62cb3e865ebac4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"998\" data-rawheight=\"569\" class=\"origin_image zh-lightbox-thumb\" width=\"998\" data-original=\"https://pic1.zhimg.com/v2-2b1907620fff008a5f62cb3e865ebac4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;998&#39; height=&#39;569&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"998\" data-rawheight=\"569\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"998\" data-original=\"https://pic1.zhimg.com/v2-2b1907620fff008a5f62cb3e865ebac4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-2b1907620fff008a5f62cb3e865ebac4_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>常用的 A/B 实验是关于流量侧的分桶实验，根据流量的不同配比进行不同实验的对比。但是这种流量侧的分桶实验不能满足广告的一些需求。在实际的广告系统中，会出现针对某些广告行业或者广告主，进行尝试性的策略，关注的实验效果也是被关注的广告行业或者广告主的效果。如果用流量侧实验的话也不是不行，但是在进行实验效果分析时候，数据分析需要具体到广告主粒度，会给统计分析引擎带来极大的挑战；如果不这么做，仍然只看整体的效果，实验分析效果必定是被稀释的，无法准确的反映出策略的意义。这样就需要进行广告侧实验的机制设计：</p><p>首先会进行实验对象的圈定，包括：广告主 、广告类别、广告计划、广告创意等。</p><p>然后进行同质或非同质实验：</p><p>同质实验（即对一个推广计划的受众进行随机的划分，一分为二，在一分为二的基础上作用不同的策略进行对比），一个计划，可能会创建两个类似于“伪计划”的概念，然后对伪计划同时进行系统投放。进入两个人群的划分当中，然后进行效果对比。</p><p>非同质实验，这个是尝试性的圈定一波人去进行实验，这种往往是没有对照的是根据实验效果进行纵向的对比判定，也就是当前的策略实验效果与之前没有进行的实验的策略运行效果进行对比判断。</p><p>⑤ 广告预算独立 A/B 实验</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-0f5f7ce547e4ad1bcbc042ebb3d86f5e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1005\" data-rawheight=\"565\" class=\"origin_image zh-lightbox-thumb\" width=\"1005\" data-original=\"https://pic3.zhimg.com/v2-0f5f7ce547e4ad1bcbc042ebb3d86f5e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1005&#39; height=&#39;565&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1005\" data-rawheight=\"565\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1005\" data-original=\"https://pic3.zhimg.com/v2-0f5f7ce547e4ad1bcbc042ebb3d86f5e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-0f5f7ce547e4ad1bcbc042ebb3d86f5e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>广告实验平台在具体的广告实验中会遇到的问题，这种问题在非广告业务中是没有的。因为在广告领域涉及到 A、U、C 这三方，这三方是个有效的完整回路，一个简单的回路就考虑到计划的投放-&gt;花钱-&gt;广告下线，这里会涉及预算，预算没有了就会下线。这里会涉及个问题，如果在进行这种简单的流量分桶实验的话，一分为二，上不同的策略，这两个策略的结果对广告主预算的消耗速度是不一样的，带来的结果是，假如某个分桶（五五分），一段时间会发现 A 桶消耗的快，如果没有进行预算的独立，会把 B 桶的预算拉过去，具体表现就是广告主的钱，在 A 桶花费的更多，如果流量小的时候效果可能非常好，但是随着流量的不断的放大会发现策略的实验效果会变的很小。所以广告预算的独立性会在广告主侧实验的时候就考虑到，比如说计划的一分为二，预算平均分配，各自独立，互不影响。最后会考虑将预算独立实验应用在流量侧分桶，目前没有做但是经过论证是可行的。</p><p>⑥ 实验检验与效果评估</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ad5a5189c25a8d98fce234c99ee4c8df_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1000\" data-rawheight=\"569\" class=\"origin_image zh-lightbox-thumb\" width=\"1000\" data-original=\"https://pic4.zhimg.com/v2-ad5a5189c25a8d98fce234c99ee4c8df_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1000&#39; height=&#39;569&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1000\" data-rawheight=\"569\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1000\" data-original=\"https://pic4.zhimg.com/v2-ad5a5189c25a8d98fce234c99ee4c8df_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-ad5a5189c25a8d98fce234c99ee4c8df_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>实验检验与效果评估，是实时的效果评估，能够做到5分钟（保守的）内的数据延迟，如果1分钟也可以，但是问题点在于涉及广告效果衡量时，比如 ctr=互动/曝光，互动可能会有延迟，会造成计算出的结果飘忽不定，所以为了让实验效果趋于稳定会5分钟计算一次。</p><p>对于跨多天的实验效果分析采取的机制是离线跨天+实时流分析引擎。由于实时流机制的效果不如离线的，这里又进行了优化，比如几个小时之前的，如果离线进行这种任务的调度都能跑出来的话，也会用离线的方式进行批量的处理来替换今天早些时候的实时流分析数据，实时流会倾向于用最近比如一个小时或两个小时的数据。在实验决策角度来说，实时流主要是看实验效果会不会带来严重的缺陷，以便进行实验的及时终止，离线的数据处理带来的实时报表主要作为实验效果的评定。</p><p>整体的实验效果会有可用性的监控（例如对流量分桶的监控，不同策略对流量层面的可用性是不一样的），保证两个实验分桶的可用性在同一个水平上的。</p><p>实验平台支持的业务指标将近4000万～1亿的规模，而且支持自定义指标的方式。同时实验支持版本的跟踪，也支持 nodiff 的对照实验等。</p><p><b>2. 广告精益洞察</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-19d4d62aecab7c35b9604420dda5a692_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1018\" data-rawheight=\"568\" class=\"origin_image zh-lightbox-thumb\" width=\"1018\" data-original=\"https://pic3.zhimg.com/v2-19d4d62aecab7c35b9604420dda5a692_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1018&#39; height=&#39;568&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1018\" data-rawheight=\"568\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1018\" data-original=\"https://pic3.zhimg.com/v2-19d4d62aecab7c35b9604420dda5a692_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-19d4d62aecab7c35b9604420dda5a692_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>精益洞察想解决的问题是：由于线上的策略很多，怎样让策略运行的时候，在系统之外能很好的看到系统的运行状态，使得策略运行的像水一样透明，后来提出了精益矩阵的描述方法，整体上的业务分为多层次的业务阶段，每个阶段都会有完整的洞察。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-98b0861739ddb3c2233d32c39815806a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1005\" data-rawheight=\"566\" class=\"origin_image zh-lightbox-thumb\" width=\"1005\" data-original=\"https://pic3.zhimg.com/v2-98b0861739ddb3c2233d32c39815806a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1005&#39; height=&#39;566&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1005\" data-rawheight=\"566\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1005\" data-original=\"https://pic3.zhimg.com/v2-98b0861739ddb3c2233d32c39815806a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-98b0861739ddb3c2233d32c39815806a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>上图是系统的架构图。数据落地，进入到实时流机制，实时流机制中的数据类别是分类的，之间是通过 key 进行关联，将日志构建组件化，包括线上的调试日志，用户的信息日志、线上跑策略跑模型的日志、广告日志等等，再往上会进行日志处理存储，分为离线存储和在线存储，在线存储会用到 PG 和 clickHouse 这种列式存储，以便在线即时访问。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-c95a8f1e4ba80a45c1a2d2106890e374_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"993\" data-rawheight=\"556\" class=\"origin_image zh-lightbox-thumb\" width=\"993\" data-original=\"https://pic1.zhimg.com/v2-c95a8f1e4ba80a45c1a2d2106890e374_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;993&#39; height=&#39;556&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"993\" data-rawheight=\"556\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"993\" data-original=\"https://pic1.zhimg.com/v2-c95a8f1e4ba80a45c1a2d2106890e374_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-c95a8f1e4ba80a45c1a2d2106890e374_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>整体的呈现效果是：每一层是一个业务策略阶段；请求访问或者广告召回集的处理，整体看是一个漏斗，从广告数量来看是二维的，一个广告召回集的数量，随着策略的运行，数量在不断的减少。每个层面上都可以更细粒度看，例如不同的竞价类型占比的分布情况以及整体的出价水平都可以细粒度的查看，分辨出在不同策略层之间的表现差异，洞察策略的具体影响效果。</p><p>精益洞察提供了不同的视角：</p><p>流量侧：看总体的流量的漏斗运行情况；</p><p>用户粒度：实现 debug 功能，对单个用户的请求在策略漏斗中的表现情况；</p><p>计划粒度：商业侧，比如某广告主计划的推广运行情况都可以洞察到。</p><p>本次的分享就到这里，谢谢大家。</p><p>------</p><p><b>PS：</b>微博广告研发中心，招募广告引擎高级工程师/技术专家（广告策略方向、分布式架构方向）中，团队技术氛围浓厚，技术至上，技术偏执狂，开放，自由，优秀者有股票，有意者请发送简历至：</p><p>tieniu@staff.weibo.com</p><p>广告策略方向：参与广告投放体系的实现，基于大规模用户行为数据，优化在线广告策略与模型，提高在线广告的相关度，优化广告投放效果。</p><p>分布式架构方向：负责微博广告引擎设计、重构、优化，支持大数据、高并发和高性能要求； 负责广告数据服务的研发，支持亿级用户的广告动态实时计算和匹配，以及数据分布式存储和访问； 对现有系统的不足进行分析，找到目前系统的瓶颈，改进广告系统的架构，提高系统性能以及效果。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-e6ec56e545dcbe4408f520a4aa692470_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"720\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-e6ec56e545dcbe4408f520a4aa692470_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;720&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"720\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-e6ec56e545dcbe4408f520a4aa692470_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-e6ec56e545dcbe4408f520a4aa692470_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>嘉宾介绍</b></p><p>李铁牛，微博广告引擎技术专家，微博广告投放系统负责人。2012年加入微博广告团队，微博广告创始团队核心成员，微博广告法拉第 A/B 分层实验系统设计者。对广告触发机制，广告智能频控，市场机制设计，精准受众定向技术， 广告策略实验框架等计算广告技术有深入研究，对广告服务模型建模和百亿级高并发广告系统有丰富的设计经验。</p><p><b>——END——</b></p>", 
            "topic": [
                {
                    "tag": "系统架构", 
                    "tagLink": "https://api.zhihu.com/topics/19578413"
                }, 
                {
                    "tag": "互联网广告", 
                    "tagLink": "https://api.zhihu.com/topics/19554609"
                }, 
                {
                    "tag": "人工智能", 
                    "tagLink": "https://api.zhihu.com/topics/19551275"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/82332581", 
            "userName": "DataFunTalk", 
            "userLink": "https://www.zhihu.com/org/09843313d8c5eff1b7d8bcfa65dc8b68", 
            "upvote": 21, 
            "title": "快看漫画个性化推荐探索与实践", 
            "content": "<p>本次分享的主题是快看漫画个性化推荐探索与实践，主要包括：</p><ul><li>业务介绍</li><li>技术挑战</li><li>技术探索</li><li>总结与未来规划</li></ul><p><b>▌业务介绍</b></p><p><b>1. 关于快看漫画</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-abd414eb51a408f7e94ca8440154cd2b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"605\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-abd414eb51a408f7e94ca8440154cd2b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;605&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"605\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-abd414eb51a408f7e94ca8440154cd2b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-abd414eb51a408f7e94ca8440154cd2b_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>快看世界创立于2014年，旗下快看漫画 app 是中国新生代内容社区和原创 IP 平台，截止2019年7月总用户量已经突破2亿，注册用户量突破1亿，月活突破4000万，绝大多数用户属于高活跃、高粘性的95后、00后，快看漫画今年被 QuestMobile 等机构评为“最受00后欢迎的产品”。</p><p>很多人来到快看漫画，可能更多的是想看漫画，实际我们的内容不只是漫画，还有社区的 UGC 内容，从产品属性来讲虽然现在更倾向于漫画，但是我们在逐渐强化社区属性，也是未来重点的战略方向。所以，对于推荐来讲我们是长内容和短内容结合的。</p><p><b>2. 快看漫画推荐业务</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-85d6567b066231773f37f859f4d905b2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-85d6567b066231773f37f859f4d905b2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;606&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-85d6567b066231773f37f859f4d905b2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-85d6567b066231773f37f859f4d905b2_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>上图为快看漫画的主要推荐业务场景，包括：首页个性推荐 tab，发现页推荐 tab，世界页推荐 tab，贴底相关推荐。画面会比之前好一些，对于推荐系统来讲，不光是技术、数据、算法，还和 UI/UE、领域知识相关。</p><p>内容形式包含：长漫画、短漫画、图文帖子、视频帖子等。</p><p>我们在做的事情就是如何为4000万月活用户很好的分发长内容和短内容。</p><p><b>▌技术挑战</b></p><p><b>1. 内容形式多样</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-03798c962ed1a80f41a889aaa5d3e3d6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"605\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-03798c962ed1a80f41a889aaa5d3e3d6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;605&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"605\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-03798c962ed1a80f41a889aaa5d3e3d6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-03798c962ed1a80f41a889aaa5d3e3d6_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>短内容（短视频、新闻资讯、用户帖子等）特点：</p><ul><li>占用用户碎片化时间，阅读时间短</li><li>兴趣点通常单一</li></ul><p>长内容（漫画、小说等）特点：</p><ul><li>占用用户大块的时间，阅读周期长</li><li>连续性、周期性、多章节多兴趣点</li></ul><p>针对多样的内容形式，我们面临的技术挑战：</p><ul><li>技术上如何捕捉长内容的连续性、周期性、多兴趣点等特征？</li><li>快看漫画既有长内容又有短内容，如何较好的融合两类内容？</li></ul><p><b>2. 内容风格独特</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-7e9def42098dbe428219cc62d5a207b8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"605\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-7e9def42098dbe428219cc62d5a207b8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;605&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"605\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-7e9def42098dbe428219cc62d5a207b8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-7e9def42098dbe428219cc62d5a207b8_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>快看漫画有大量的文本信息（帖子内容、弹幕、评论）和海量的图像信息（漫画图像、帖子图片），其带来的挑战为：</p><ul><li>如何进行漫画类图像内容理解？图中古风的图片可能比较好理解，但是如何分辨校园和都市，通过图像是很难判别的。</li><li>独特的社区文化（比如二次元），新生代文化“暗语”（如上图帖子中的内容，对这方面不了解的人都很难理解，对于机器来说更难理解），给文本内容理解带来挑战。</li></ul><p><b>▌技术探索：算法</b></p><p><b>1. 推荐算法演进</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ce40a0acaaedbd867b3cdfebab1e9abf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"463\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-ce40a0acaaedbd867b3cdfebab1e9abf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;463&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"463\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-ce40a0acaaedbd867b3cdfebab1e9abf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-ce40a0acaaedbd867b3cdfebab1e9abf_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>如果现在界定为深度学习时代，各大公司的产品都已经上了深度学习模型，深度学习的效果是非常好的，但是它的平台搭建周期是非常长的，并且很难被解释，是一个黑盒的东西，看不到摸不到，很难干预。对于前深度学习时代，也就是传统机器学习模型来说，它的可解释性强，训练起来比较容易，并且容易部署。</p><p><b>2. 快看推荐算法迭代</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-5071eabb1db42facb186cb02bf6073d9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"291\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-5071eabb1db42facb186cb02bf6073d9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;291&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"291\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-5071eabb1db42facb186cb02bf6073d9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-5071eabb1db42facb186cb02bf6073d9_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>快看推荐算法起步相对于推荐领域是比较晚的，但是相对于漫画垂直领域还是比较早的，我们在2019年以前更多的是基于内容的推荐，今年的上半年我们引入了协同过滤，同时19年到现在排序这块主要用到的是 XGBoost，未来我们会考虑深度学习。</p><p><b>3. 基于内容的推荐</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-11dd092f3ede9c8536f2173472003e5b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"456\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-11dd092f3ede9c8536f2173472003e5b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;456&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"456\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-11dd092f3ede9c8536f2173472003e5b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-11dd092f3ede9c8536f2173472003e5b_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>基于内容的推荐，最大的难点在于对内容的理解，我们有比较专业的运营和内容团队，在做推荐之前已经有了一些比较基础和简单的标签，可以快速的应用起来，所以我们最早做的是基于内容的推荐。做内容推荐，我们需要有很好的内容理解，构造好物品的画像，另外，需要很好的理解用户的兴趣偏好，构建用户的用户画像，我们把两者很好的结合就可以得到推荐的结果。对于内容推荐来讲，它的可解释性也是比较强的，对于在内容方面有很深积累的公司，可以很快的构建起来。</p><p>① 快看漫画标签体系</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-730edfacfa9f691857253203c8b64287_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"471\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-730edfacfa9f691857253203c8b64287_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;471&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"471\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-730edfacfa9f691857253203c8b64287_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-730edfacfa9f691857253203c8b64287_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>快看漫画的标签体系，分为三个维度：</p><ul><li>作品基础维度：搞笑、青春、治愈等</li><li>用户分发维度：男性、女性、青少年等</li><li>内容创作维度：青春成长、兄妹、学生等</li></ul><p>即使有专业的标签团队来打标签，建立很好的标签体系也需要很长的周期过程，因为人和人之间的感受和认知是有差距的，如何把这些标准制定好，保证每个作品打的标签是无差别的，这是一个专业性很强的问题。（上图为我们去年比较火的作品，被拍成了电影）</p><p>② 用户兴趣模型</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-6b532cf14b177e4778b5ead2f09b5199_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"431\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-6b532cf14b177e4778b5ead2f09b5199_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;431&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"431\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-6b532cf14b177e4778b5ead2f09b5199_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-6b532cf14b177e4778b5ead2f09b5199_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>做用户兴趣模型，需考虑：</p><ul><li>相关行为：关注、点赞、评论、分享等</li><li>行为粒度：会精确到关注的作品或具体某个章节</li><li>章节数量：章节数量不等，有的作品很长，有的作品很短，如何判断用户对一个感兴趣，对另一个作品不感兴趣</li><li>兴趣衰减：用户的兴趣是周期性的，会存在兴趣衰减的情况</li><li>作品热度：需考虑热门作品，大家都在看的内容</li></ul><p>③ 基于内容推荐总结</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-22faa63e385d1b0c731123b5d6ed3330_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"987\" data-rawheight=\"330\" class=\"origin_image zh-lightbox-thumb\" width=\"987\" data-original=\"https://pic1.zhimg.com/v2-22faa63e385d1b0c731123b5d6ed3330_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;987&#39; height=&#39;330&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"987\" data-rawheight=\"330\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"987\" data-original=\"https://pic1.zhimg.com/v2-22faa63e385d1b0c731123b5d6ed3330_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-22faa63e385d1b0c731123b5d6ed3330_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>基于内容推荐的总结，存在以下缺点：</p><ul><li>非常依赖标签</li><li>推荐粒度较粗，如果用户兴趣单一的话，召回会不足</li><li>缺乏新颖性</li></ul><p>但是，这是我们第一次上线基于内容推荐的模型，DAU 人均阅读次数率提升35%，效果还是很不错的。</p><p><b>4. 基于协同过滤</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-504941e82dcd4d9a1e21960570731d26_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"467\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-504941e82dcd4d9a1e21960570731d26_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;467&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"467\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-504941e82dcd4d9a1e21960570731d26_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-504941e82dcd4d9a1e21960570731d26_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>之后，我们引入了协同过滤，下面为我们实现了的三种算法：</p><ul><li>基于物品的协同过滤 ( Item-Based )，对于漫画来讲，作品数量不是特别大，可以很快的离线计算完成。</li><li>基于用户的协同过滤 ( User-Based )，由于我们有4000万的月活用户，做起来还是比较痛苦的，下面将重点介绍。</li><li>基于模型的协同过滤 ( Model-Based )</li></ul><p>由于协同过滤都是基于矩阵来完成的，我们采用的是业界常用的 KNN 近邻算法。</p><p>① KNN 召回</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-52cdf1fc8b1b497391e68e30f34a4432_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"466\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-52cdf1fc8b1b497391e68e30f34a4432_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;466&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"466\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-52cdf1fc8b1b497391e68e30f34a4432_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-52cdf1fc8b1b497391e68e30f34a4432_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>因为，基于用户的协同算法用户相似度计算量巨大，所以，针对 KNN 近邻算法，我们做了调研，对 Nmslib 和 Faiss 库做了对比：</p><p>它们都是开源的，可能 Faiss 会比较知名一点，因为是 Facebook 开源的，它们的实现语言都是 C++，都实现了 Python 绑定，但是 Faiss 会支持 GPU，都实现了目前最快的 HNSW 分层索引算法，右边为网上找的两个算法在单机 CPU 上的 benchmark，训练集大概 100+W，维度是200，查找的是100个近邻。大家可以看到，最外层绿色的线就是 Nmslib 实现的 HNSW 算法，紧接着深绿色的就是 Faiss 实现的 HNSW 算法，对比 Nmslib 会慢一点，再往下一条线是 Faiss 实现的 IVF 算法，它会稍微差一些，但是它可以支持 GPU 并行计算，所以按照 GPU 去考量，那么这个明显是胜出的，所以我们综合考虑，选择了 Faiss 作为近邻计算的基础库。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-346dafe783dd394d1043d862e7bea9fd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"435\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-346dafe783dd394d1043d862e7bea9fd_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;435&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"435\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-346dafe783dd394d1043d862e7bea9fd_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-346dafe783dd394d1043d862e7bea9fd_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>这里简单介绍下 Faiss 实现的算法。</p><p>Faiss IndexIVFFlat 实现过程：</p><ul><li>训练 &amp; 建索引</li></ul><p>① 聚类（找到聚类中心存储在量化器 quantizer 中）</p><p>② 找到每个向量最近的聚类中心点</p><p>③ 建立倒排id ( id 编号 ) list</p><p>④ 建立倒排 code ( 真实向量的倒排索引 ) list</p><ul><li>搜索 topK</li></ul><p>① 搜出查询向量最近的 n 个聚类中心点 id 及对应的距离</p><p>② 构建 k 个元素最大堆</p><p>③ Id 对应的倒排 list 每个向量计算距离后放入最大堆</p><p>④ 堆排序，最后做堆排序就可以得到 TopK</p><p>下面的 Faiss IndexIVFPQ，相当于一个升级优化版本，实现更复杂些，会计算残差，通过构建二级索引实现计算的加速。整体来说，我们实现了 User-Based CF 的实时在线召回。</p><p>② 基于协同过滤总结</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-140144a0f20092c30553b6fed75e18ef_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1022\" data-rawheight=\"360\" class=\"origin_image zh-lightbox-thumb\" width=\"1022\" data-original=\"https://pic4.zhimg.com/v2-140144a0f20092c30553b6fed75e18ef_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1022&#39; height=&#39;360&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1022\" data-rawheight=\"360\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1022\" data-original=\"https://pic4.zhimg.com/v2-140144a0f20092c30553b6fed75e18ef_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-140144a0f20092c30553b6fed75e18ef_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>协同过滤上线后，DAU 人均阅读次数提升了31%，同时协同过滤存在的缺点为：</p><ul><li>倾向于推荐热门内容 ( 当然可以通过一些方法对热门内容进行打压 )</li><li>对新用户和新内容不友好</li><li>相似矩阵的计算量大 ( 可以通过 ANN 的方式来解决 )</li></ul><p><b>5. 召回排序模型</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-04f6845d372229af9b86e68474e33513_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"555\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-04f6845d372229af9b86e68474e33513_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;555&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"555\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-04f6845d372229af9b86e68474e33513_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-04f6845d372229af9b86e68474e33513_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>我们有了基于内容的召回，基于协同过滤的召回，每个召回都有自己的排序结果，我们会考虑如何把这些结果合并起来，前期是基于规则的，后期我们采用 CTR 预估的方式，使用传统的召回+排序的结构。</p><p>① 常用 CTR 预估算法</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-eb66fa07e27af491ac9a61e1e1fa77b4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"465\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-eb66fa07e27af491ac9a61e1e1fa77b4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;465&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"465\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-eb66fa07e27af491ac9a61e1e1fa77b4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-eb66fa07e27af491ac9a61e1e1fa77b4_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>常用的 CTR 预估算法有：LR、FM &amp; FFM、GBDT ( 树模型 )、DNN ( 深度神经网络 )。</p><p>LR 模型</p><p>优势：</p><ul><li>模型简单，善于处理离散化特征 ( 包括 id 类特征 )</li><li>容易实现分布式，可处理大规模特征和样本集</li></ul><p>劣势：</p><ul><li>特征需要离散化</li><li>特征之间在模型中是孤立的，需要做大量特征工程来做特征交叉</li></ul><p>XGBoost 模型</p><p>优势：</p><ul><li>树模型具有一定的组合特征能力</li><li>善于处理联系特征，可进行特征筛选，人工特征工程量少</li></ul><p>劣势：</p><ul><li>具有很强的记忆行为，不利于挖掘长尾特征</li><li>组合特征的能力有限</li></ul><p>FM &amp; FFM 模型</p><p>优势：</p><ul><li>可以自动进行特征间的组合</li><li>通过引入特征隐向量，加速了训练的复杂度，善于处理稀疏数据</li></ul><p>劣势：</p><ul><li>工作量接近深度学习，效果不如深度学习</li><li>FFM 计算复杂度高</li></ul><p>DNN 模型</p><p>优势：</p><ul><li>可直接输入原始特征，减少交叉特征选择</li><li>效果好</li></ul><p>劣势：</p><ul><li>可解释性差</li><li>模型可能较大，调参复杂，需要较大的工程支持</li></ul><p>综上，我们最终选择人工特征工程量较少的 XGBoost 方案。</p><p>② 召回排序模型总结</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-f64b78c4c30f2f18043c6654dccffaad_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"975\" data-rawheight=\"370\" class=\"origin_image zh-lightbox-thumb\" width=\"975\" data-original=\"https://pic2.zhimg.com/v2-f64b78c4c30f2f18043c6654dccffaad_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;975&#39; height=&#39;370&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"975\" data-rawheight=\"370\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"975\" data-original=\"https://pic2.zhimg.com/v2-f64b78c4c30f2f18043c6654dccffaad_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-f64b78c4c30f2f18043c6654dccffaad_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>上线召回排序模型之后，DAU 人均阅读次数提升36.6%，目前的现状和问题：</p><ul><li>特征还需要进一步挖掘</li><li>模型的训练效果有待提升，需要工程上的提升</li><li>探索尝试新模型提升效果</li></ul><p><b>▌技术探索：系统架构</b></p><p><b>1. 架构的重要性</b></p><p>架构的重要性：算法是大脑，架构是骨架，如果没有好的推荐系统架构，算法很难落地。</p><p>好的推荐系统需要具备的特质：</p><ul><li>实时响应请求</li><li>及时、准确、全面的记录用户反馈</li><li>优雅降级，即使在服务出现问题的时候，也能推荐出个性化的结果</li><li>快速迭代推荐策略、算法</li></ul><p><b>2. 经典 Netflix 推荐系统架构</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-119e788f56c2556ae1fa27f8fcad13bb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-119e788f56c2556ae1fa27f8fcad13bb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;606&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-119e788f56c2556ae1fa27f8fcad13bb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-119e788f56c2556ae1fa27f8fcad13bb_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>这是 Netflix 在2013年公布的推荐系统架构，把推荐系统分为了三层：</p><ul><li>离线层：一个用户产生行为，通过事件分发，分发到离线层和近线层，离线部分是通过 hive 和 pag 这种通过离线的任务把数据分发到模型训练和一些离线计算上。</li><li>进线层（准实时层）：近线层有个组件叫 Manhattan，相当于今天常用的 Flink 和 Strom，把实时计算结果存储到 Cassandra 相当于 HBase，然后还有 EVcache 相当于 redis。</li><li>在线层：在线层会用离线计算的模型和近线计算的结果，得出在线的排序结果。</li></ul><p>这就是当时 Netflix 的推荐系统架构。</p><p><b>3. 快看推荐系统架构</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-fee5cfca447f0dbf4b420f30bac4c0c9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"603\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-fee5cfca447f0dbf4b420f30bac4c0c9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;603&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"603\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-fee5cfca447f0dbf4b420f30bac4c0c9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-fee5cfca447f0dbf4b420f30bac4c0c9_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>我们在做快看推荐系统架构的时候，实际上是没有参考 Netflix 的架构，但是，当我们完成之后发现，各个层也可以按照这个方式去划分：</p><ul><li>近线层（橙色，实时数据流过程）：客户端采集的日志数据，通过 Kafka、Flink 传递到实时用户画像和动态文档。</li><li>离线层（红色）：业务库数据通过 sqoop 导到 HDFS 后在 Spark 上计算，然后是离线模型，包括特征工程，模型训练，算法模型，向量索引，用户画像等等。</li><li>在线层（绿色）：包括在线的召回、排序、推荐、服务端、ios/android 等等。</li><li>工具（紫色）：标签权重模型、推荐结果追踪、数据指标监控和服务监控。</li></ul><p><b>4. AB 实验平台</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-7a83a71224978aba3cf5c011cb7bec6d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"607\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-7a83a71224978aba3cf5c011cb7bec6d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;607&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"607\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-7a83a71224978aba3cf5c011cb7bec6d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-7a83a71224978aba3cf5c011cb7bec6d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>快看的 AB 实验平台在功能上是非常完善的，是从产品各层级自上而下统一的实验标识，方便联动；实现了设备随机、用户随机、流量随机的随机分组方式；通过实验分层支持正交实验，可以在一个层做多组实验；同时支持互斥实验，确保流量调整时用户稳定落在某一分组。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-d812b18455266bf052fe6409facbc95b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"603\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-d812b18455266bf052fe6409facbc95b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;603&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"603\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-d812b18455266bf052fe6409facbc95b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-d812b18455266bf052fe6409facbc95b_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>对于指标计算，进行了显著性的总结和功效的总结，并且指标可配置，在做实验的时候想关注哪些指标可以进行配置，方便查看算法实验的效果。</p><p><b>5. 推荐结果追踪工具</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-9e4c93867282139d75bd38d00b306a8c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"604\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-9e4c93867282139d75bd38d00b306a8c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;604&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"604\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-9e4c93867282139d75bd38d00b306a8c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-9e4c93867282139d75bd38d00b306a8c_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>推荐往往会有一些 Bad case 暴露出来，如果没有做追踪，就很难查找那块儿出了问题，因此我们做了个性化推荐全链路的跟踪系统，保证了推荐的结果是因为什么推荐的，或者为什么没有被推荐，这样就保证了一个可解释性。如何解决的？我们会把当时的历史画像 Snapshot 和上下文，通过 HBase 记录下来。</p><p><b>▌总结与未来规划</b></p><p><b>1. 总结</b></p><p>本次分享主要介绍了快看和快看的推荐业务，从算法和系统两方面介绍了快看推荐技术在起步阶段的一些探索，并且介绍了大规模k近邻计算方法、AB 实验平台搭建等常用技术的落地方案。</p><p><b>2. 未来规划</b></p><ul><li>内容理解是推荐业务的基石，目前这块儿还比较欠缺，未来将探索漫画领域的图像和文本内容理解技术。</li><li>传统机器学习方法探索充分之后将尝试深度学习推荐算法，以期更好的推荐效果。</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-a5521b28d039fce7b75510da81e1a1e8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"720\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-a5521b28d039fce7b75510da81e1a1e8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;720&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"720\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-a5521b28d039fce7b75510da81e1a1e8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-a5521b28d039fce7b75510da81e1a1e8_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>嘉宾介绍</b></p><p>夏博，快看世界推荐研发负责人。清华大学软件学院硕士毕业，从业8年，先后就职于微策略 ( MicroStrategy )、万维思源 ( EverString )、一点资讯、快看世界；前期主要从事后端开发的工作，目前主要从事推荐系统的开发工作，现任快看世界产品研发-推荐研发负责人。</p><p><b>——END—— </b></p>", 
            "topic": [
                {
                    "tag": "推荐", 
                    "tagLink": "https://api.zhihu.com/topics/19551728"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "人工智能", 
                    "tagLink": "https://api.zhihu.com/topics/19551275"
                }
            ], 
            "comments": [
                {
                    "userName": "智多多", 
                    "userLink": "https://www.zhihu.com/people/7d0040e64ccb7cc1b02f09bb0bb87ded", 
                    "content": "简单清晰，实践经验丰富[赞]，为什么这样做前因后果说的很详细，但评估指标是不是太单一，只有一个业务指标？提高30%", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "DataFunTalk", 
                            "userLink": "https://www.zhihu.com/people/09843313d8c5eff1b7d8bcfa65dc8b68", 
                            "content": "<p>只是简单展示哈，毕竟更细粒度的东西也不好都拿出来给大家分享。</p>", 
                            "likes": 0, 
                            "replyToAuthor": "智多多"
                        }
                    ]
                }, 
                {
                    "userName": "全全", 
                    "userLink": "https://www.zhihu.com/people/54953b088c3a146f2114ee0e3330ef9a", 
                    "content": "具体问下，协同过滤关于faiss实现时，用户向量怎么得到的，用户向量维度多大?", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "DataFunTalk", 
                            "userLink": "https://www.zhihu.com/people/09843313d8c5eff1b7d8bcfa65dc8b68", 
                            "content": "<p>用户向量可以根据用户对不同分类或标签的兴趣倾向获取，如果物品数量小也可以根据直接有行为的物品的得到</p>", 
                            "likes": 0, 
                            "replyToAuthor": "全全"
                        }, 
                        {
                            "userName": "全全", 
                            "userLink": "https://www.zhihu.com/people/54953b088c3a146f2114ee0e3330ef9a", 
                            "content": "好的，我大概懂了，用户向量映射到类别和用模型学的“潜变量”有对比过效果吗", 
                            "likes": 0, 
                            "replyToAuthor": "DataFunTalk"
                        }
                    ]
                }, 
                {
                    "userName": "知乎用户", 
                    "userLink": "https://www.zhihu.com/people/0", 
                    "content": "<p>竟有大厂2019用协同过滤，还以为都深度学习了。看来我司现在还在用的协同过滤还能坚持一会。。</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "DataFunTalk", 
                            "userLink": "https://www.zhihu.com/people/09843313d8c5eff1b7d8bcfa65dc8b68", 
                            "content": "<p>哈哈，人家下一步也要上深度学习了。</p>", 
                            "likes": 0, 
                            "replyToAuthor": "知乎用户"
                        }, 
                        {
                            "userName": "Azul", 
                            "userLink": "https://www.zhihu.com/people/00c3e9d2b6b2ec05dafca35c02ca3989", 
                            "content": "<p>其实i2i还是很生猛的</p>", 
                            "likes": 0, 
                            "replyToAuthor": "知乎用户"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/78488485", 
            "userName": "DataFunTalk", 
            "userLink": "https://www.zhihu.com/org/09843313d8c5eff1b7d8bcfa65dc8b68", 
            "upvote": 23, 
            "title": "阿里妈妈深度树检索技术（TDM）及应用框架的探索实践", 
            "content": "<p><b>导读：</b>阿里妈妈是阿里巴巴集团旗下数字营销的大中台，2018年广告营收超过1500亿，近乎占据中国广告市场收入的半壁江山。如何驱动这艘商业航母不断前行，阿里妈妈技术团队始终坚持技术创新驱动业务增长的战略，而 TDM 正是在这一战略指导下，由阿里妈妈精准定向广告算法团队自主研究、设计、应用从而创造巨大商业价值的创新算法典型代表。</p><p>今天的主要内容分为四个部分：</p><p>1. 从互联网推荐业务看检索技术发展现状</p><p>2. 深度树检索 ( TDM ) 的设计原理和具体实现 </p><p>3. 深度树检索技术对于线上业务场景的适配应用</p><p>4. 我们对下一代检索技术未来发展的思考</p><p><b>▌从互联网推荐业务看检索技术发展现状</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-ea6478904a452b34f2ac244abd94972a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"607\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-ea6478904a452b34f2ac244abd94972a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;607&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"607\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-ea6478904a452b34f2ac244abd94972a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-ea6478904a452b34f2ac244abd94972a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>检索技术，是互联网推荐、搜索、广告的通用底层技术组成。</p><p>一提到检索技术，大家首先会想到搜索引擎，但是从另外一个视角来看，搜索、推荐、广告实际上是一脉相承的，他们的检索可以<b>统一定义为：从全量的物品里面，挑选出来用户感兴趣的物品。</b>不同的是，搜索中 Query 词显式表达了用户意图，而在推荐中往往利用的是用户隐式表达如历史行为、用户属性等，而广告则是在搜索/推荐上进一步施加了广告出价的影响。</p><p>我们认为检索技术的发展是数据、算力、算法相辅相成，协同作用的结果，具体表现为：</p><p>最初，数据少所以候选 Items 规模比较小，用很小的算力和简单的算法，就可以一步到位（如全量过一遍候选）计算出来用户感兴趣的 Items 是什么；</p><p>然而，数据在不断膨胀，尤其是随着近些年移动互联网的发展和移动设备的普及，使得我们的数据开始呈现爆炸式的增长，从而导致我们的候选 Items 规模也大幅增大，一步到位或者说原来的检索方式已经无法支撑这种量级的数据。</p><p>怎么办？在如此大规模数据下，算力和算法如何适应性设计呢？我们首先来看一下算力的发展，在十几年前我们的算力主要还是以 CPU 通用计算为主的算力体系，算力相对比较有限。大数据+有限算力，就要求我们对算法进行适应性设计，于是 Match+Rank 分段漏斗的算法架构被引入：从超大规模候选集中，先 Match 出小规模候选集，再通过 Rank 的方式排序出最终用户最感兴趣的 Items。逐渐地，Match+Rank 的模式成为了如今检索体系的主流架构。</p><p>数据在发展，算力体系也在发展，尤其是随着近十年来 GPU 大规模应用的兴起，形成了 CPU+GPU 异构计算的算力大融合。在大数据+大算力的现状下，我们开始思考，检索算法架构是不是可以再升级？类似于历史中的“合久必分，分久必合”：我们是不是可以突破 Match+Rank 的分段漏斗模式，将检索问题作为一个端到端整体去设计和联合优化？这就是我们今天要探讨和思考的问题，也是 TDM 的由来。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-67d25b7a52908ebbfabeebdb9dc26eae_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"605\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-67d25b7a52908ebbfabeebdb9dc26eae_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;605&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"605\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-67d25b7a52908ebbfabeebdb9dc26eae_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-67d25b7a52908ebbfabeebdb9dc26eae_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>我们怎么来思考检索架构升级呢？首先需要对检索本身进行深入分析，而作为检索的第一阶段，我们先从 Match 入手。Match 的核心任务是从全量的超大规模的候选集中高效的检索相关 TopK，然后交给后链路比如 Rank 做进一步处理。比如在电商场景，我们有大规模商品库，大概是十亿量级，我们会做 Match 召回千到万级别的 TopK，送给后链路做 Rank。</p><p>类似于受算力局限下检索 Match+Rank 两阶段的设计，Match 本身也受到算力局限的影响，所以它的经典实现也是两段式的，比如：</p><ul><li>搜索场景下，两段式的表现，就是 User 到 Query-rewrite，Query-rewrite 到 Doc；</li><li>推荐场景下，两段式的表现，就是 User 到 Interest-tag（如品牌），Interest-tag 到 Item。</li></ul><p>这里存在的问题是两段式相互隔离，比如我们对 User 到 Query-rewrite 进行优化形成各种匹配算法，然后对 Query-rewrite 到 Doc 优化形成倒排索引。这种两段式缺少联合优化，并且受限于线上性能影响，两段式的各个阶段的标签数量是有截断的，效果（如全量发现能力）上就存在局限。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-59c3a6c95e4a7b76401a2747382adb58_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"605\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-59c3a6c95e4a7b76401a2747382adb58_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;605&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"605\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-59c3a6c95e4a7b76401a2747382adb58_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-59c3a6c95e4a7b76401a2747382adb58_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>Match 两段式的经典实现如图中 Item-CF 所示，这里不做过多论述。如前述说的，它的优势是模型比较简单，实现成本较低，劣势是两段式无法联合优化，各阶段的标签截断导致效果受限。由此，我们自然会想到一个问题：两段式有缺陷，那我们是否可以改为一段式？</p><p>答案是肯定的，那么一段式要怎么做？</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-a07fc2e4065a1fc5737091e5c540f482_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"607\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-a07fc2e4065a1fc5737091e5c540f482_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;607&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"607\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-a07fc2e4065a1fc5737091e5c540f482_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-a07fc2e4065a1fc5737091e5c540f482_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>在推荐和相关领域，简单的一段式全库检索的经典代表，其实借鉴了图像检索的算法，即：内积模型的向量检索。它通过各种方式学到了 Item 的 Embedding，然后通过 PQ 方式构建分库索引，检索时实时计算 User Embedding，做最近邻 TopK Item 检索。内积模型向量检索在 Facebook 17年开源了 FAISS 库后，得到了广泛的应用。这种一段式全库检索，在发现能力上具有一定优势，但缺点是模型比较简单，能力有限。这里以 PCTR 预估为例，内积模式的 DQM、Attention 的 DIN、Attention+GRU 的 DIEN 做比较来看，内积模式的 AUC 是最低的，间接证明了内积模型是存在能力局限的。</p><p>另外，这种模式下索引构建和模型优化的目标存在不一致。积量化索引的优化目标是最小化近似误差，而向量检索（召回问题）的优化目标是最大化 TopK 召回率。所以由于两者优化的目标不一致，这会导致最终效果并不是最优的。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-d35fbdddc8c238523fbdb22552ecaeb9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"603\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-d35fbdddc8c238523fbdb22552ecaeb9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;603&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"603\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-d35fbdddc8c238523fbdb22552ecaeb9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-d35fbdddc8c238523fbdb22552ecaeb9_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>我们总结一下 Match 中检索技术的发展历程，不难发现，<b>检索技术是模型能力和索引效能的融合：</b></p><ul><li>基于商品的协同过滤（Item-CF），模型能力：启发式统计规则，没有学习成分；索引效能：分离两段式；问题：非学习模型，非面向全库索引；</li><li>内积模型向量检索，模型能力：内积模型；索引效能：全库一段式；问题：内积模型相对简单（问题：为什么是内积而不能是其他更复杂的模型如 DNN？留给读者思考）；索引和目标之间没有联合优化。</li></ul><p>那么面对这样的现状，下一代模型，我们要如何发展？答案是：更先进的模型、更先进（高效）的索引。</p><p><b>▌深度树检索 ( TDM ) 的设计原理和具体实现 </b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-581c46dbb34db70d50a3765b2fa0d052_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"605\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-581c46dbb34db70d50a3765b2fa0d052_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;605&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"605\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-581c46dbb34db70d50a3765b2fa0d052_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-581c46dbb34db70d50a3765b2fa0d052_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>应用先进模型，我们自然想到了深度学习模型，但它的引入会带来额外的问题：</p><p>① 深度学习的单点计算消耗 T 较大，在性能有界的情况下，我们的计算次数 N 不能过大；</p><p>② 要实现面向全库的检索，N 越大越好（最大时即为候选集 size），在性能有界的情况下，与上述 ① 产生了矛盾；</p><p>如何解决这一矛盾？还是“<b>检索技术=模型能力+索引效能</b>”，也即模型的升级需要索引结构的升级来适配，于是我们的核心任务变为：建立一个高效的索引结构来承载先进的模型能力。高效索引有很多，散列表、图等都有一些弊端，最终我们选择了树这一层次结构（如图所示）。</p><p>以十亿商品为例我们可以构建一棵30层的树，检索 Top1 我们只需要计算30次。</p><p>基于以上这些考量，我们提出了 Tree-based Deep Match（TDM）这个算法，具体我们需要进一步解决以下四个问题：</p><p>① 如何基于树实现高效检索?</p><p>② 如何做兴趣建模保证树检索有效性?</p><p>③ 如何学习兴趣模型?</p><p>④ 如何构建和优化树索引结构?</p><p>接下来，我们会以淘宝的商品推荐为例，做一个关于 TDM 设计和实现的详细介绍，来帮助大家理解。<br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-8ac57974b5676799ac30d961cac8d6e8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-8ac57974b5676799ac30d961cac8d6e8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;606&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-8ac57974b5676799ac30d961cac8d6e8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-8ac57974b5676799ac30d961cac8d6e8_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>如何做高效检索？不失一般性，假定我们的树是一棵完全二叉树，全部叶子节点代表全部商品，中间节点代表某种意义上的粗粒度聚合（例如“iPhone”是商品，作为叶子，“手机”是该商品的类别，作为父节点；但是这里需要说明，在通常的 TDM 树中，我们并不要求中间节点一定存在具体物理意义）。</p><p>树上如何做高效检索呢？我们采用了 BeamSearch 这种方式，自顶向下，做快速剪枝。</p><p>BeamSearch 检索方式如图中右侧流程所示：假设要寻找最优 Top2 个 Item，第一层有两个子节点，打分排序选出 Top2（只有2个，全取）；扩展出第二层四个子节点，打分排序选出 Top2；扩展出第三层四个子节点，打分排序选出 Top2，达到叶子层，最终返回 Top2 个 Items，查找复杂度为 O(2*K*log2N)，K 为返回个数，N 为叶子个数。</p><p>衍生问题：为什么该检索策略检索出来的 Top2，一定会是用户感兴趣的 Top2呢？即 BeamSearch 的有效性要如何保证？</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-a067ca3b09da3b52d56201dfaec93850_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"605\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-a067ca3b09da3b52d56201dfaec93850_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;605&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"605\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-a067ca3b09da3b52d56201dfaec93850_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-a067ca3b09da3b52d56201dfaec93850_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>有效的检索背后蕴藏了有效的兴趣建模，也即，要如何建模，使得 BeamSearch 检索有效。</p><p>为此我们提出了兴趣最大堆树的概念：用户对 n 节点兴趣的偏好，是用户对 n 节点的孩子节点们的兴趣偏好，取 max 后，在 n 所在的节点层做归一化。如图中右侧树所示，SN4 的兴趣偏好：</p><p>PSN4=normalization(LayerSN, max(PItem7, PItem8)) </p><p>在这样的假设下，有个很好的性质：最大堆树下，当前层最优 TopK 节点的父亲，一定属于上一层的最优 TopK。</p><p>以右图举例：如果 Item8 和 Item6 是叶子层最优 Top2，那么根据我们之前的描述，SN4 和 SN3 是 SN 层的最优 Top2。</p><p>因此，最大堆树的定义是 BeamSearch 有效的充分条件。所以我们可以从根节点递归向下，逐层挑选 TopK，然后扩展至叶子层。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-6409125684f15c3217c348f9f870fa21_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"604\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-6409125684f15c3217c348f9f870fa21_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;604&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"604\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-6409125684f15c3217c348f9f870fa21_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-6409125684f15c3217c348f9f870fa21_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>我们如何学习最大堆树的模型？BeamSearch 检索本质上要求具备对每一层进行 TopK 排序的能力。我们的做法是：构建符合这样性质的样本，让样本牵引模型学习，去逼近最大堆。</p><ul><li>具体思路：</li></ul><p>叶子层的节点兴趣比较容易构建，因为用户对叶子层的节点是有行为产生的，有行为/没有行为对应感兴趣/不感兴趣，以此来构建序标签；中间节点，用最大堆递归上述的方式去推导每一层的序标签；当我们有了每一层的序标签，就可以用深度学习去拟合序标签的样本</p><p>在实际操作中，我们没有用序样本去构建序模型，而是把序估计还原成点估计，此处可以理解为把排序模型转换成分类模型。</p><p>有了分类样本之后，就可以让模型去拟合，牵引模型学习逼近最大堆性质。</p><ul><li>采样方案：</li></ul><p>我们认为绿色代表正样本，红色代表负样本，如果用户对 Item6 节点有访问，那我们认为用户对 Item6 节点是一个正兴趣，我们在当前的叶子层做随机的负采样，那么上一层怎么做呢？正样本上溯路径的祖先仍然为正样本，然后每一层再做负采样，所以这样就固定出来每一层的正负样本，这样整个树的样本都已经固定完毕。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-50bfba30fcd522d6d85bcbfa501dd087_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"607\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-50bfba30fcd522d6d85bcbfa501dd087_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;607&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"607\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-50bfba30fcd522d6d85bcbfa501dd087_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-50bfba30fcd522d6d85bcbfa501dd087_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>有了正负样本，我们就可以用任意复杂模型去拟合 label。可以看到这样模式的优点：</p><p>我们刚刚一直在提要用先进模型或者深度模型，这里发现最大堆树的训练模式和检索阶段的效率保证了可以基于在线检索的性能约束（上界）来决定可以用多复杂的模型达到最高的检索效果。</p><p>上图为具体的模型结构，可以看到：</p><ul><li>先进模型网络结构：采用以 Attention 为核心的多层深度神经网络，实现对多峰兴趣的强大判别，F1（Precision 和 Recall 的调和平均数）提升了16%。</li><li>时空兼顾特征表征：以序列化建模用户特征的时序表达，对时序特征进行树上溯（Hierarchical Representation）实现用户特征的层上空间归约，F1 又提升了18%。</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-818d2485c6cab6f6b9cacff00b3e6006_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-818d2485c6cab6f6b9cacff00b3e6006_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;606&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-818d2485c6cab6f6b9cacff00b3e6006_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-818d2485c6cab6f6b9cacff00b3e6006_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>树结构在 TDM 算法中所扮演的关键角色：树不仅决定了检索的索引结构，更是决定了用于模型学习的样本分布，所以树结构的“好坏”直接决定了最大堆树模型学习的效果上界。</p><p><b>不失一般性，</b>假定我们的树仍为完全二叉树，叶子节点为 n 个，那树的本质其实就是如何将 n 个商品一一投射到 n 个叶子上去，也即树的形式化定义是找个一个 π 映射实现 n 对 n 的二部完美映射。那怎么样的树是好的树呢？也即何种 π 映射是最优的呢？从左下图可以看出来，右侧的树结构更优，因为它隐含了性别分类，可以使得 TDM 模型学习得更好（上限更高）。</p><p>为了达到模型和树的联合最优化，我们建立了统一的全局经验损失函数（如右上公式），并通过交替迭代的方式进行模型参数 θ 和映射函数 π 的联合优化。其中 π 的优化是一个带权二部图的最大匹配问题，复杂度比较高（O(n^3)），为此我们建立了右下的贪心近似算法（Algorithm2）来优化 π。最终的效果是这种联合优化让 F1 提升近10%。</p><p>PS：这里有现场同学提问</p><p>Q：前几页的最大堆树学习里有正负采样，为什么“统一的全局经验损失函数”里面没有负样本项了？</p><p>A：为了实现统一 Loss，我们这里对 TDM 模型进行了建模变换，从之前的二分类转换到了多分类，即将层上用户兴趣建模成用户对当前层上的全部树节点做多分类问题。上述统一 Loss 即为多分类建模下的标准交叉熵 Loss，其中的 p 为归一化概率。在实际实现方式上，我们采用了负采样 + NCE 的方法来近似多分类 Softmax。所以上述统一 Loss 里面确实没有了负样本项，“负样本”是以 NCE 方法中的负采样方式表现了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-a672a5f7209eaea4745eb6d6511072d6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-a672a5f7209eaea4745eb6d6511072d6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;606&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-a672a5f7209eaea4745eb6d6511072d6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-a672a5f7209eaea4745eb6d6511072d6_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>TDM 在 AmazonBooks 和 TaobaoUserBehavior 两个公开数据集上进行了测试。相对于基线的 ItemCF 和 Youtube 内积模型，TDM 的提升非常显著。我们也对 TDM 各种优化做了单项拆分分析，Attention、HierarchicalRep. 和联合学习的效果如之前所说，其中三者的联合效果达到了1+1+1&gt;3。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-d5ec4cdae3adfc27a1d9ee8ab88983da_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"607\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-d5ec4cdae3adfc27a1d9ee8ab88983da_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;607&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"607\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-d5ec4cdae3adfc27a1d9ee8ab88983da_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-d5ec4cdae3adfc27a1d9ee8ab88983da_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>回到最初的图中，我们再次发现：检索技术是模型能力和索引效能的融合。</p><p>从这个角度，大家思考一下，TDM 为什么可以解决这个融合呢？我们实现了对更先进模型，即深度学习模型的融合；其次，我们实现了更先进索引，采用了树结构来做索引，有别于常用的倒排索引，并且我们的树结构是一个与模型适配且联合学习的树。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-f06de0eba171367c5824cf291f73a40a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"605\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-f06de0eba171367c5824cf291f73a40a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;605&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"605\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-f06de0eba171367c5824cf291f73a40a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-f06de0eba171367c5824cf291f73a40a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>总结一下 TDM 提出、设计和实现的脉络：</p><ul><li>目标：解决从超大规模全量商品库中高效检索 TopK 商品的问题</li><li>思考：希望由先进模型带来性能/效果的提升；为赋能先进模型，需要高效索引；由此产生最大堆树的理论建模</li><li>探索：基于最大堆树，我们链接了 BeamSearch 检索下深度学习模型和高效树索引联合学习的方式，形成了基于学习树的全库索引的检索框架。</li></ul><p><b>▌深度树检索技术对于线上业务场景的适配应用</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-e593887197c510c2ec570d50144bd835_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"604\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-e593887197c510c2ec570d50144bd835_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;604&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"604\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-e593887197c510c2ec570d50144bd835_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-e593887197c510c2ec570d50144bd835_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>原生 TDM 是兴趣最优的检索方案，但是在实际业务中，有些业务目标不一定是兴趣最优。例如在广告业务中，我们要考虑 ECPM 最优。那么，我们要如何改造 TDM，使得其可以满足不同的业务目标？</p><p>这里面有如下挑战：</p><ul><li>算法要从理论上进行升级适配，使得最大堆树的建模适配业务指标；检索也需要改造，在整体链路的设计上要考虑多目标融合，使得检索体系可以支持多个业务目标</li><li>此外，克服性能瓶颈，实现检索功能的真正在线应用，也需要一段路要走。</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-707feb0a4ad4e3f2f738d4b7a63f2cda_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"603\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-707feb0a4ad4e3f2f738d4b7a63f2cda_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;603&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"603\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-707feb0a4ad4e3f2f738d4b7a63f2cda_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-707feb0a4ad4e3f2f738d4b7a63f2cda_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>“兴趣最大堆”改造成“任意指标最大堆”的过程中，也存在一些问题，比如：“兴趣”是二值问题，即感兴趣/不感兴趣，可以抽象成分类问题；而在广告中，“ECPM”是一个连续问题，即 ECPM 是一个连续的数字，更适合抽象成回归问题。基于这个现状，我们需要思考，如何构建这样的 TDM 模型。</p><p>仍然以广告为例：由于叶子层是每一个具体的 Item，那么这些 Item 的 ECPM 是可以计算的（由于 ECPM=PCTR*BID*1000，我们可以通过其他的模型计算 Item 的 PCTR，BID 通过查找/统计的方式可以得到），所以我们有叶子层的所有 Item 的 ECPM 值。所以我们可以通过 max 上溯的方式，将其引入到我们的最大堆树中来，也即：</p><p>PParent1= max(PItem1, PItem2)</p><p>上一层的节点值为该节点的孩子节点的最大值，此处不需要归一化。并以此方式递归上溯生成整棵树，并在检索的时候递归往下完成 TopK 检索。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-7a4fd8dd3acdcb75fbd79001c914d1fc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"605\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-7a4fd8dd3acdcb75fbd79001c914d1fc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;605&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"605\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-7a4fd8dd3acdcb75fbd79001c914d1fc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-7a4fd8dd3acdcb75fbd79001c914d1fc_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>由于一个模型并不能适配所有的场景，满足所有目标（例如：广告业务考虑广告收入的最大化；电商业务考虑电商成交最大化等），因此，我们对检索模型做了另外一个改造，也即：分模型联合检索框架，上层是公共部分，对通用的功能做抽象，下层是业务部分，根据业务现状做设计，两部分实现并行调用。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-212a587f93cb3568beefea6cc3d927bf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"607\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-212a587f93cb3568beefea6cc3d927bf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;607&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"607\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-212a587f93cb3568beefea6cc3d927bf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-212a587f93cb3568beefea6cc3d927bf_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>虽然 TDM 理论上可以支持任意深度模型，但实际应用中，我们仍然需要考虑在线检索 Bound 的约束。在一开始上线之初，TDM 的 RT 增量达到了 60ms，对于阿里妈妈的在线广告链路完全不可用。但通过后续的包括通信、计算、链路等优化，最终 TDM 实现了上线后 RT 增加接近于 0ms 的良好性能。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-aa7980c5a9397b298f615e4c597ac984_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-aa7980c5a9397b298f615e4c597ac984_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;606&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-aa7980c5a9397b298f615e4c597ac984_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-aa7980c5a9397b298f615e4c597ac984_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>现阶段 TDM 主要接入了阿里妈妈定向广告的 Match 阶段，包括 Shop/Node/Item 已经全量上线，覆盖了阿里妈妈定向广告主要场景的大部分流量，CTR 和 RPM 的效果提升都达到了两位数。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-42c1acb280c836c21501034deb322185_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"605\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-42c1acb280c836c21501034deb322185_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;605&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"605\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-42c1acb280c836c21501034deb322185_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-42c1acb280c836c21501034deb322185_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>最后，从学习树全库索引的检索，我们进一步把 TDM 扩展到了通用的业务应用框架。左边纵向为基于学习树全库索引的超大规模检索是如何设计的，右边纵向为如何应用 TDM 到线上业务，包括改造成业务最大堆树，然后面向业务的高性能优化和多层多模型的设计，最后真正应用于实际。</p><p><b>▌我们对下一代检索技术未来发展的思考</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-9d1999bcddd997e37f2230a3cc26d0db_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"607\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-9d1999bcddd997e37f2230a3cc26d0db_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;607&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"607\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-9d1999bcddd997e37f2230a3cc26d0db_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-9d1999bcddd997e37f2230a3cc26d0db_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>对于未来，我们希望将创新推向一个新的高度，当然这只靠我们一个团队是很难做到的。结合当前 TDM 在阿里妈妈的应用，我们希望能在下一个阶段，可以将 TDM 与现有技术做更深层的融合，比如在图检索、搜索业务等领域上面做一些优化和探索。</p><p>我们也一直坚持开放、开源的态度。TDM 的离线训练、在线预测的代码已经在 github 开源：</p><ul><li>离线训练： </li></ul><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/alibaba/x-deeplearning/wiki/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/alibaba/x-de</span><span class=\"invisible\">eplearning/wiki/</span><span class=\"ellipsis\"></span></a>深度树匹配模型(TDM) </p><ul><li>在线Serving：</li></ul><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/alibaba/x-deeplearning/wiki/TDMServing\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/alibaba/x-de</span><span class=\"invisible\">eplearning/wiki/TDMServing</span><span class=\"ellipsis\"></span></a> </p><p>我们也正在集团内部推动 TDM 一站式服务化平台的建设，第一阶段实现 DIMO（Data In, Model Out），第二阶段实现 DISO（Data In, Service Out），以期未来可以实现：算法工程师提供场景内行为数据，我们可以自动化构建 TDM 模型；算法工程师将 TDM 模型提交至云端，在线服务就可以自动加载运行，实现 TopK 检索。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-12af7f9b3f8620c59a6e248ea017bb82_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"604\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-12af7f9b3f8620c59a6e248ea017bb82_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;604&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"604\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-12af7f9b3f8620c59a6e248ea017bb82_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-12af7f9b3f8620c59a6e248ea017bb82_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>TDM 也还在继续创新过程中，包括：</p><ul><li>技术方面：实现千人千树、图空间增强、检索最优化；</li><li>业务方面：可解释性，基于树结构的广告白盒透出等；</li><li>生态建设：学术上正在承办 CIKM2019 比赛，工业上会持续开放、开源，加强云端建设能力。</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-5f149a200e42e9d4d11bc14087a13819_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-5f149a200e42e9d4d11bc14087a13819_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;606&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-5f149a200e42e9d4d11bc14087a13819_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-5f149a200e42e9d4d11bc14087a13819_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>“合久必分，分久必合”，受限于数据发展的爆炸式增长，和计算性能的限制，两段式 Match+Rank 统治了检索架构的主流，但在大数据+大算力的大环境下，我们需要对算法做出改变，为此 TDM 突破了原有的架构约束，做出了部分创新工作，实现了端到端联合优化的一体化检索框架。</p><p>当前的 TDM 不管是技术创新还是业务应用还处在发展初期，我们相信随着技术和业务的持续创新，TDM 未来有望发展成为一个面向搜索、推荐、广告业务领域的通用的底层框架。我们也非常欢迎各位可以一起来探讨、应用和改造 TDM 技术，实现更优的检索效果，取得更大的业务成果！今天的分享就到这里，谢谢大家。</p><p>关于 TDM 大家有问题可以通过 DataFun 联系到本次 TDM 技术分享人 何杰（花名：壶源），也可以直接发邮件到 <b>jay.hj@alibaba-inc.com</b>。同时，阿里妈妈精准定向广告算法团队也欢迎各位志士仁人加盟（校招/社招持续进行和开放中），简历发送，同上述邮箱<b>^^</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-912972b3483b75703c675562f4076757_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"1080\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-912972b3483b75703c675562f4076757_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;1080&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"1080\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-912972b3483b75703c675562f4076757_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-912972b3483b75703c675562f4076757_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>嘉宾介绍</b></p><p>何杰，花名壶源，阿里妈妈定向广告算法核心成员，商品/广告推荐算法匹配 ( Matching ) 方向负责人，阿里原创的基于树全库索引的全新一代大规模检索技术 ( Tree-based Deep Match，TDM ) 项目 PM，推动阿里巴巴定向广告新一代广告推荐匹配技术体系演进，在定向广告业务的实际应用中取得了显著的效果提升。</p><p>分享嘉宾：何杰 阿里妈妈 高级算法专家</p><p>编辑整理：孙锴</p><p>内容来源：DataFun AI Talk</p><p>出品社区：DataFun</p><p>注：欢迎转载，转载请注明出处</p><p><b>——END——</b></p>", 
            "topic": [
                {
                    "tag": "人工智能", 
                    "tagLink": "https://api.zhihu.com/topics/19551275"
                }, 
                {
                    "tag": "阿里巴巴集团", 
                    "tagLink": "https://api.zhihu.com/topics/19551577"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/82155975", 
            "userName": "DataFunTalk", 
            "userLink": "https://www.zhihu.com/org/09843313d8c5eff1b7d8bcfa65dc8b68", 
            "upvote": 5, 
            "title": "AutoML 在表数据中的研究与应用", 
            "content": "<p><b>导读：</b>大家好，今天分享的题目是 AutoML 在表数据中的研究与应用。对于 AutoML，大家听到比较多的可能是神经网络结构搜索 ( NAS，Neural Architecture Search )，NAS 主要应用于图像，而我们的工作主要应用于解决表数据 ( Tabular Data ) 中的建模问题。目前 NAS 在表数据中的研究较少，有兴趣的小伙伴可以尝试。</p><p><b>第四范式 AutoML</b> <b>Tables</b> <b>的效果</b></p><p>我们选取了10个 Kaggle 比赛数据，分别通过第四范式 AutoML Tables 和 Google Cloud AutoML Tables 产生结果并提交，然后分别计算每种方法的在整个排行榜中的相对排名，如图所示，第四范式的 AutoML 在表数据上的效果大部分要优于 Google Cloud AutoML，其中图中青色代表第四范式，蓝色代表 Google，柱状图越高表示效果越好。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-2ab78fa1568f9e49f6d2231158101773_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"604\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-2ab78fa1568f9e49f6d2231158101773_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;604&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"604\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-2ab78fa1568f9e49f6d2231158101773_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-2ab78fa1568f9e49f6d2231158101773_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>下面我主要从五个方面进行介绍：</p><ul><li>AutoML Tables 的背景</li><li>AutoML Tables 的自动特征工程</li><li>AutoML Tables 的自动参数寻优</li><li>第四范式 AutoML Tables 的落地案例</li><li>对未来工作的展望</li></ul><p><b>▌AutoML Tables 的背景</b></p><p><b>1. 什么是机器学习？</b></p><p><b>Tom Mitchell </b>在1997年《机器学习》的教材中讲到，定义如下：A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-3a4dec6cba3f4299ba3f5e478e5d6752_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"607\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-3a4dec6cba3f4299ba3f5e478e5d6752_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;607&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"607\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-3a4dec6cba3f4299ba3f5e478e5d6752_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-3a4dec6cba3f4299ba3f5e478e5d6752_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>2. 什么是 AutoML？</b></p><ul><li>机器学习步骤</li></ul><p>以现实中的场景“反欺诈”为例，介绍下机器学习的步骤。</p><p>① 形式化的定义问题</p><p>② 收集数据</p><p>③ 特征工程</p><p>④ 模型的训练</p><p>⑤ 模型的评估</p><p>⑥ 部署和应用（如果效果达到目标）</p><p>上述所有的工作基本都是由机器学习的专家来完成的。该过程可能是一个迭代的过程，需要根据模型的效果，多次选择数据、特征和调整模型等。</p><ul><li>AutoML 做什么？</li></ul><p>AutoML ( Automated Machine Learning ) 是利用机器来完成机器学习步骤中的某部分工作，而不是全部由专家来完成。目前，在相当多的领域缺乏有机器学习背景的开发人员。因此，利用 AutoML 可以使得机器学习更好的应用到更多的行业、更快地造福于社会。</p><p>现在 AutoML 主要侧重于特征工程、模型训练和模型评估，对于问题的形式化研究较少。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-41efe6d45d8f2634ef58fec5054e7bce_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"607\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-41efe6d45d8f2634ef58fec5054e7bce_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;607&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"607\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-41efe6d45d8f2634ef58fec5054e7bce_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-41efe6d45d8f2634ef58fec5054e7bce_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>3. 什么是 AutoML</b> <b>for</b> <b>Tables？</b></p><p>通常大家对于 AutoML 的印象更多来自于 Google 的 NAS 方面的文章，例如自动在 CIFAR10 或者 ImageNet 上搜索网络结构。这些通常是应用于图像数据的。</p><p>AutoML for tables 的工作主要是面对通过业务逻辑拼接成的宽表，不涉及图像数据或者 NLP。图中示例的数据集是来自 UCI 的数据，主要预测是否给客户贷款，是否有风险，用到的特征有年龄、工作、教育、资产等。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-e3313374b882d961a8b3e116d00bd12f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-e3313374b882d961a8b3e116d00bd12f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;606&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-e3313374b882d961a8b3e116d00bd12f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-e3313374b882d961a8b3e116d00bd12f_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>▌AutoML Tables 自动特征工程</b></p><p>根据前面提到的机器学习的 pipeline，我们先介绍 AutoML Tables 的自动特征工程。在参加竞赛中，大家会提到一个说法“特征决定了效果的上限，模型只是决定趋近于这个上限的程度”。在我们的实际工作中，特征也是一个提升空间更大的地方。</p><p>自动特征工程主要是由下面几个模块组成：</p><ul><li>自动拼表</li><li>自动特征生成</li><li>自动特征选择</li><li>自动特征增强</li></ul><p><b>1. 自动拼表</b></p><p>现实中完成一个业务场景的任务，是需要很多张表的。例如一张表描述用户信息，一张表描述商品信息，还有一张表描述其他的补充信息（例如上下文，浏览记录等）。</p><p><b>2. 自动特征生成</b></p><p>在自动特征生成中，我们主要是有下面四类的算子：</p><p>① 一元算子</p><p>基于特征做线性和非线性的变换，例如归一化、log 变换等。n 个特征，复杂度是 O(n)。</p><p>② 二元操作算子</p><p>例如加减乘除，笛卡尔积等。n 个特征，进行二元操作，则复杂度为 O(n2)。</p><p>③ group-by 算子</p><p>Group-by operator 是比较特殊的一种算子，它需要先做 partition，然后做 aggregation。此外，如果牵扯到时序性 ( 时序可能放在 group by，或者放在 window function )，算子的复杂度会比较高，同时需要小心穿越。</p><p>④ 高阶的算子 ( high-order )</p><p>例如有 k 阶，则从 n 个特征中选择 k 个进行操作，数量为 Cnk，再从这些特征中进行选择的话，就是指数的指数级，指数空间的搜索问题很难。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-0c9be72374e8671009b5a7ec21e1ebed_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-0c9be72374e8671009b5a7ec21e1ebed_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;606&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-0c9be72374e8671009b5a7ec21e1ebed_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-0c9be72374e8671009b5a7ec21e1ebed_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>3. 自动特征选择</b></p><p>特征是不是越多越好？答案是否定的。一方面有的特征是有害的，另一方面特征较多对于系统性能都会增加要求。所以，在自动生成了很多特征后，我们需要考虑如何从自动生成的特征中选取有效的特征。</p><p>首先，最直观的做法是将特征放入模型中，如果模型效果好就保留，效果不好就舍弃。这个方法有两个问题，一是如果应用到全量数据，进行计算会比较贵；另一个是一个特征本身没有用，和其他的特征组合有用。</p><p>经典的特征选择主要有 Filter、Wrapper、Embedded 三种方式，可以去查看相关的资料。我们做的工作的目标是快速地，低代价的进行特征选择。这次主要介绍一个 PFI ( permutation feature importance ) 特征重要性的方法和 ( field-wise logistic regression ) 的方法。</p><ul><li>PFI ( permutation feature importance ) 方法</li></ul><p>下面举例来介绍 PFI 方法，假设我们有10个特征待评估，首先进行一个模型训练，得到了模型的效果评估值 ( performance ) AUC 为0.8。然后，固定9个特征不变， shuffle 第一个特征，再次进行模型训练，得到新的评估值 AUC 为0.7。判定特征的重要性为两次的差值0.8-0.7，为0.1。重复上面的过程，如针对第二个特征，固定除这个特征外的9个特征，只是对第二个特征进行 shuffle，这个时候模型为0.75，则第二个特征的重要性为0.8-0.75，为0.05。该方法既可以做特征重要性，亦可做特征可解释性。该方法的动机是，特征越重要，对其扰动后，模型的效果抖动越大。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-c44dc3420829b1ed31d25a4d65197969_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-c44dc3420829b1ed31d25a4d65197969_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;606&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-c44dc3420829b1ed31d25a4d65197969_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-c44dc3420829b1ed31d25a4d65197969_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>逐域对数几率回归 ( field-wise logistic regression，FLR ) 方法</li></ul><p>当我们有10个特征，可分别用每个特征单独训练 LR 模型，但是这样代价比较高。我们主要进行了两方面的改进。</p><p>① 并行化改进</p><p>我们的改进之一是借鉴 boosting 的思想，并行来完成对于特征的建模。部分特征建模后，固定其权重，再进行剩余部分特征的建模。例如，当原始表有100个特征，又增加10个特征时，我们先利用100个特征训练的模型得到的特征权重，固定好这100个特征权重后，也即让 LR 有一个起始点，然后在这个起始点再去训练其他10个特征的权重。请注意：这10个特征的学习过程互相独立，即权重互不影响。</p><p>② 工程化改进</p><p>如果每次读取数据，只针对一个特征训练相应的 LR 模型，频繁 IO 导致性能下降。所以，我们结合参数服务器，实现扫描一次数据，训练出所有特征对应的 LR 模型，也即扫描一次数据，得到所有的特征重要性。</p><p>我们采用这两种方法得到特征重要性，然后进行迭代，最后得到有效的特征组合。</p><p><b>4. 自动特征增强</b></p><p>我们做的另一个工作是自动特征增强，由于里面仍然有很多问题较为困难，仍需要进一步研究。表数据中可能有各种数据，如 NLP 类型 ( 一个 user 的 profile 为文本 )，image 类型 ( user 的头像 )，audio 类型。另外还有 graph 类型 ( 例如考察团伙作案 )，以及 user 的住址等 Knowledge graph 的信息等。处理的方法一方面是进行直接处理 ( nlp 直接分词 )，另一种是 embedding 的方法，可以进行微调 ( fine tune )，也可以不做。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-e0ba2fea509c821c091fcecb98e0220f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"604\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-e0ba2fea509c821c091fcecb98e0220f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;604&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"604\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-e0ba2fea509c821c091fcecb98e0220f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-e0ba2fea509c821c091fcecb98e0220f_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>我们最后将其应用到不同的数据集中，如下图。有的数据集上的效果很好，有的数据集上的提升效果较小，但也可以提升建模效果。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-28cdf2791f6c08a0ae4ddfdecc22f1ca_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-28cdf2791f6c08a0ae4ddfdecc22f1ca_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;606&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-28cdf2791f6c08a0ae4ddfdecc22f1ca_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-28cdf2791f6c08a0ae4ddfdecc22f1ca_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>5. AutoCross 介绍</b></p><p>下面介绍下我们在 KDD 2019 上面的一个工作：自动特征组合 ( AutoCross )，详细可以参看文献 AutoCross: Automatic Feature Crossing for Tabular Data in Real-World Applications，地址：</p><p><u><a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1904.12857\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">arxiv.org/abs/1904.1285</span><span class=\"invisible\">7</span><span class=\"ellipsis\"></span></a></u> </p><p>5.1 AutoCross 的系统设计<br/></p><p>该系统包含从右到左三个部分：</p><p>① 基础设施</p><p>我们基于已有的大规模分布式的机器学习平台，搭建了一个并行处理机器学习算法。主要有参数服务器，cache 的优化， feature 的优化管理等。</p><p>② 算法集合</p><ul><li>Beam Search 方法</li></ul><p>基于 Beam search 解决了如何从原始特征选出2阶、5阶乃至10阶的高阶特征生成与选择方法。主要是采用贪婪算法，首先进行2阶选择，然后将重要的特征放入下一步，逐次迭代生成多阶特征组合。形象地解释，beam 就是一束光，光照到的就会进行特征验证，没有照到的就不会被衍生下去。</p><ul><li>逐域对数几率回归方法 ( field-wise logistic regression，FLR )</li></ul><p>如前文所述，它可以快速地选择特征。</p><ul><li>连续批训练梯度下降 ( successive mini-batch gradient descent )</li></ul><p>即使有50个原始特征 ( 现实较为常见 ) 两两组合后特征就上千，在全部数据上扫一遍会代价很高。为了加速迭代，我们借用调参算法的思想，将评估特征逐步砍半，降低评估代价。</p><ul><li>多粒度离散化 ( multi-granularity discretization ) 方法</li></ul><p>当组合离散特征和连续特征进行建模的时候，需要进行连续特征变换。传统的方法是 log，或者分桶。我们在实际过程中发现，在一些数据集合上，连续特征离散化对于桶的个数很敏感 ( 效果有5%的差异 )。为此，我们采用多粒度的离散化方法，对数值特征，根据不同粒度做离散化，同时生成多个不同粒度离散化对应的离散特征，然后采用逐域对数几率回归挑选出最优的离散特征。多个划分粒度既可以由用户指定，也可以由 AutoCross 根据数据大小和计算环境来自适应地选择，使得用户不需要反复调整离散化的粒度。</p><p>③ 工作流实现</p><p>对数据进行基本的预处理后，我们将特征组合和特征选择迭代，最终选出来 Top-K 个的特征。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-f66b77d23c22fcc1103f123e344f8c03_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-f66b77d23c22fcc1103f123e344f8c03_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;606&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-f66b77d23c22fcc1103f123e344f8c03_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-f66b77d23c22fcc1103f123e344f8c03_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>5.2 AutoCross 的效果</p><p>基于5个公开的数据集和5个实际业务数据集，我们验证 AutoCross 的效果。数据集范围从几万到几千万。</p><p>① 线性模型</p><p>首先对比利用生成特征和无该特征的建模效果的差异性，具体如下面的 LR ( base ) 和 AC+LR 的对比，可知 AutoCross 可以提升线性模型的建模效果。</p><p>② 非线性模型</p><p>此外，可以发现 AC+W&amp;D 结合后，模型效果不比 xDeepFM，因此 AutoCross 对非线性模型 ( DNN ) 也可以提升效果。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-21ecffe3d579445bfcd418fb1026b558_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-21ecffe3d579445bfcd418fb1026b558_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;606&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-21ecffe3d579445bfcd418fb1026b558_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-21ecffe3d579445bfcd418fb1026b558_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>▌AutoML Tables 自动参数寻优</b></p><p>关于超参数优化，常用方法有 Random search ( 方法较为简单，而且效果够好 )，论文中常用的 Grid search 方法，以及贝叶斯优化方法。我们这次不再讲解，主要介绍随机坐标收缩、连续减半算法、基于种群的优化和双层优化四种方法。</p><p>超参数寻优过程主要面临下面两个问题：</p><ul><li>用什么样的模型刻画超参数空间和效果，例如贝叶斯模型为高斯过程的，random search 是 Model free 的。</li><li>当有了模型效果评估后，如何选取采样策略，生成下一个组采样点和下一组超参数配置。</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-ce7148b5e95f2a46ae6f4be6e30e5a66_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"605\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-ce7148b5e95f2a46ae6f4be6e30e5a66_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;605&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"605\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-ce7148b5e95f2a46ae6f4be6e30e5a66_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-ce7148b5e95f2a46ae6f4be6e30e5a66_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>1. 优化算法</b></p><p>① 随机坐标收缩 ( RAndom COordinate Shrinking, RACOS )</p><p>该方法基于分类的思想，利用分类器将一个较大的超参数空间分成好坏两部分。其将一个超参数的组合定为一个坐标，操作过程中，每一维是独立的，操作显示为图中的矩形方式。其中，离散值是要或者不要该点，连续值为左移或者右移。在划分为好的超参数区间以一定的概率进行采样，同时兼顾探索和开发，随后逐渐将这些坐标收缩到某一点。具体为，在最小化的过程中，逐渐缩小分类器的阈值，逐渐缩小好的参数空间的范围。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-a8719341826c60b16a00cace7e60d410_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-a8719341826c60b16a00cace7e60d410_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;606&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-a8719341826c60b16a00cace7e60d410_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-a8719341826c60b16a00cace7e60d410_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>② 连续减半算法 ( SHA )</p><p>该方法思想朴素，然而非常有效。例如有30组参数，先在十分之一的数据上运行，选择出最好的十组，相当于砍掉了三分之二。然后利用这十组的数据，再跑一定的数据。如图，后面的数据只跑了一轮，所以它的代价一般较小。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-f7e3f7d94e0f4e2b53930e682239e829_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"605\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-f7e3f7d94e0f4e2b53930e682239e829_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;605&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"605\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-f7e3f7d94e0f4e2b53930e682239e829_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-f7e3f7d94e0f4e2b53930e682239e829_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>③ 基于种群的优化</p><p>PBT 主要有提前停止、热启动和分支限界三个特征。右边的图中，红色的点为起始点，组成了初始的种群，好的点会进行交叉变异等操作，进行生长直到生成下面蓝色的点 ( 效果比较好 )。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-8394555af9afa4cf256b2f213b60bcd0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"605\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-8394555af9afa4cf256b2f213b60bcd0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;605&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"605\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-8394555af9afa4cf256b2f213b60bcd0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-8394555af9afa4cf256b2f213b60bcd0_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>④ 双层优化</p><p>该方法属于层次优化的范畴。具体来说，机器学习会在训练集 ( training set ) 求得模型的参数，在验证集 ( validation set ) 上优化超参数。可以将训练集中对模型本身参数的选取作为一层，超参数在验证集上的优化作为一层，两层可以进行交互迭代。但该方法一个问题显著问题是：两层优化需要求二阶导数，会使得计算代价较高，对此也有些近似的方法可以替代。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-4fca68090761004f577d93e44c99c09f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-4fca68090761004f577d93e44c99c09f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;606&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-4fca68090761004f577d93e44c99c09f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-4fca68090761004f577d93e44c99c09f_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>2. 算法实例-AutoDSN</b></p><p>我们针对推荐等大规模稀疏数据，构建深度稀疏神经网络 ( DSN，Deep Sparse Network ) 方法。由于神经网络对于参数敏感，我们针对深度稀疏网络利用前面提到的方法进行自动超参数寻优。自动调整神经网络 ( AutoDSN ) 在五个数据集上面的效果都较好。在3-9倍代价内，可以达到专家调参效果的99%以上。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-ed1d3c92dab2ff79a24a6da228e2797a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-ed1d3c92dab2ff79a24a6da228e2797a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;606&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-ed1d3c92dab2ff79a24a6da228e2797a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-ed1d3c92dab2ff79a24a6da228e2797a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>▌AutoML Tables 落地案例</b></p><p><b>1. 学习圈理论</b></p><p>公司基于库伯经典学习圈提炼了机器学习的闭环流程。库伯的学习圈指的是人有理论后可以指导行动，行动有反馈，也即生成了经验，不断反思可以补充完善理论。这个对应机器学习就是收集行为数据，得到反馈数据，不断进行模型训练，然后进行模型的应用，模型应用后又可以收集数据。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-16c1daed5cd04e1e9cb51348450d4a6a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"608\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-16c1daed5cd04e1e9cb51348450d4a6a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;608&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"608\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-16c1daed5cd04e1e9cb51348450d4a6a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-16c1daed5cd04e1e9cb51348450d4a6a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>我们基于学习圈理论做了一个 AutoML 产品，如左边为一个学习圈的全流程闭环设计图。它可以支持一键建模，也支持 AI 模型的可解释和特征可解释性，并可以进行数据自动回流和指标自动计算等。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-f088adb02227d337c856471e706625c6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-f088adb02227d337c856471e706625c6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;606&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"606\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-f088adb02227d337c856471e706625c6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-f088adb02227d337c856471e706625c6_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>2. 实际应用场景</b></p><p>我们利用闭环的机器学习的产品给一个大型的互联网公司做了一个项目，目前看项目的效果不错，具体见图中描述。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-f471a80d9e05249981a103065d476466_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"605\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-f471a80d9e05249981a103065d476466_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;605&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"605\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-f471a80d9e05249981a103065d476466_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-f471a80d9e05249981a103065d476466_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>▌AutoML Tables 展望</b></p><p>下面主要考虑未来我们还可以做什么，主要包括效率、效果和交互式三个方面。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-c5ccf92e301de37525eb41a19c86af48_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"605\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-c5ccf92e301de37525eb41a19c86af48_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;605&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"605\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-c5ccf92e301de37525eb41a19c86af48_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-c5ccf92e301de37525eb41a19c86af48_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>1. 效果和效率</b></p><p>AutoML 是需要同时追求效果和效率的，一要降低成本，二要提高效果。如果不达到一定建模效果，那就不可以用。但如果为了达到一定的效果代价太高，也是不可行的。效果方面，从问题定义到模型上线都有可以优化的点。效率方面，一是可以应用更好的硬件，软硬一体，另一方面可以优化算法，例如通过特征空间划分等提高效率。</p><p><b>2. 可交互</b></p><p>当前的 AutoML 比较封闭，人的参与度不高。我们希望可以达到人需要的时候 AutoML 帮助我们，不需要 AutoML 时用户自定义，来提高建模效率。当人在建模流程中，不想要做某方面的选择，可以给定目标函数、输入，让机器来优化 ( 机器较为擅长 )。在优化的过程中，人可以随时介入到学习过程中，提前终止或修改搜索空间和方向。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-f7e9450e32ce280e022cc2256752b751_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"800\" data-rawheight=\"533\" class=\"origin_image zh-lightbox-thumb\" width=\"800\" data-original=\"https://pic2.zhimg.com/v2-f7e9450e32ce280e022cc2256752b751_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;800&#39; height=&#39;533&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"800\" data-rawheight=\"533\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"800\" data-original=\"https://pic2.zhimg.com/v2-f7e9450e32ce280e022cc2256752b751_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-f7e9450e32ce280e022cc2256752b751_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>嘉宾介绍</b></p><p>罗远飞，2015年10月加入第四范式，任职算法科学家。研发多个针对表数据的自动机器学习算法并产品化，显著提升了自动机器学习建模效果；参与设计开发了第四范式独有的大规模分布式机器学习框架。曾获得国内首届迁移学习算法大赛冠军，在 KDD/ACL/EMNLP 上发表文章，并申请十几项国内外专利。</p><p><b>——END—— </b></p>", 
            "topic": [
                {
                    "tag": "大规模机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/20047090"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "人工智能", 
                    "tagLink": "https://api.zhihu.com/topics/19551275"
                }
            ], 
            "comments": []
        }
    ], 
    "url": "https://zhuanlan.zhihu.com/c_1146735754985267200"
}
