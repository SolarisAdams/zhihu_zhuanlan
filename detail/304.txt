{
    "title": "Spark，从入门到精通", 
    "description": "", 
    "followers": [
        "https://www.zhihu.com/people/ren-xiang-91", 
        "https://www.zhihu.com/people/feng-lei-80", 
        "https://www.zhihu.com/people/jeff-kit", 
        "https://www.zhihu.com/people/sonriku", 
        "https://www.zhihu.com/people/ding-chao-44-77", 
        "https://www.zhihu.com/people/superxiong", 
        "https://www.zhihu.com/people/wanggy0225", 
        "https://www.zhihu.com/people/ledinpak", 
        "https://www.zhihu.com/people/nai-men-mo-er-77", 
        "https://www.zhihu.com/people/xiao-wei-ba-52-37-29", 
        "https://www.zhihu.com/people/hu-zi-xian-sheng-95-75", 
        "https://www.zhihu.com/people/phenixsdad", 
        "https://www.zhihu.com/people/flymetothemars-58", 
        "https://www.zhihu.com/people/du-shan-qiao-ke", 
        "https://www.zhihu.com/people/liu-liu-96-83-32", 
        "https://www.zhihu.com/people/blackorwhite", 
        "https://www.zhihu.com/people/aktutuni", 
        "https://www.zhihu.com/people/windybruce", 
        "https://www.zhihu.com/people/jimmy-85-97-50", 
        "https://www.zhihu.com/people/yang-xue-ya", 
        "https://www.zhihu.com/people/da-mu-lang", 
        "https://www.zhihu.com/people/lenged2016", 
        "https://www.zhihu.com/people/wish-64-79", 
        "https://www.zhihu.com/people/deng-zi-wei-33", 
        "https://www.zhihu.com/people/yan-jian-76-38", 
        "https://www.zhihu.com/people/yang-huan-cheng-38", 
        "https://www.zhihu.com/people/luo-yi-28-86", 
        "https://www.zhihu.com/people/liu-yi-25-9", 
        "https://www.zhihu.com/people/Zhiwei1996", 
        "https://www.zhihu.com/people/luo-chen-74-42", 
        "https://www.zhihu.com/people/wook9615", 
        "https://www.zhihu.com/people/sadhen", 
        "https://www.zhihu.com/people/little-29", 
        "https://www.zhihu.com/people/hangpan", 
        "https://www.zhihu.com/people/jason-48-47-64", 
        "https://www.zhihu.com/people/zhao-bing-xin-42", 
        "https://www.zhihu.com/people/lin-zhong-yuan-60", 
        "https://www.zhihu.com/people/ke-xue-xia", 
        "https://www.zhihu.com/people/666677-13", 
        "https://www.zhihu.com/people/wu-yun-cheng-14", 
        "https://www.zhihu.com/people/hnyk", 
        "https://www.zhihu.com/people/rochestor", 
        "https://www.zhihu.com/people/zheng-chao-chao", 
        "https://www.zhihu.com/people/dang-chu-xiang", 
        "https://www.zhihu.com/people/liang-wu-92", 
        "https://www.zhihu.com/people/wht-buaa", 
        "https://www.zhihu.com/people/an-ye-71", 
        "https://www.zhihu.com/people/you-zhi-yu-san", 
        "https://www.zhihu.com/people/wu-xiao-bai-92", 
        "https://www.zhihu.com/people/YangGuoDong", 
        "https://www.zhihu.com/people/njuptcyd", 
        "https://www.zhihu.com/people/yang-troy-89", 
        "https://www.zhihu.com/people/anjun", 
        "https://www.zhihu.com/people/ke-xiao-le-94", 
        "https://www.zhihu.com/people/luxun-huang", 
        "https://www.zhihu.com/people/luther-47-2", 
        "https://www.zhihu.com/people/cai-ru-xin-10", 
        "https://www.zhihu.com/people/hopesfish", 
        "https://www.zhihu.com/people/roger-roger-36", 
        "https://www.zhihu.com/people/wen-rui-94-8", 
        "https://www.zhihu.com/people/uou123", 
        "https://www.zhihu.com/people/lwys-94", 
        "https://www.zhihu.com/people/rain-zou-80", 
        "https://www.zhihu.com/people/proser", 
        "https://www.zhihu.com/people/lifeissocool", 
        "https://www.zhihu.com/people/decli"
    ], 
    "article": [
        {
            "url": "https://zhuanlan.zhihu.com/p/47045762", 
            "userName": "MT酱", 
            "userLink": "https://www.zhihu.com/people/28629950b8d6033df6b6f33fd0280e2a", 
            "upvote": 0, 
            "title": "从Spark MLlib到美图机器学习框架实践", 
            "content": "<p>MLlib 是 Apache Spark 的可扩展机器学习库，旨在简化机器学习的工程实践工作，并方便扩展到更大规模的数据集。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b> / 机器学习简介</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p>在深入介绍 Spark MLlib 之前先了解机器学习，根据维基百科的介绍，机器学习有下面几种定义：</p><ul><li>机器学习是一门人工智能的科学，该领域的主要研究对象是人工智能，特别是如何在<b>经验</b>学习中改善具体<b>算法</b>的<b>性能</b>；</li><li>机器学习是对能通过<b>经验</b>自动改进的计算机<b>算法</b>的研究；</li><li>机器学习是用数据或以往的<b>经验</b>，以此优化计算机程序的<b>性能</b>标准；</li><li>一种经常引用的英文定义是「A computer program is said to learn from <b>experience</b> E with respect to some class of tasks T and <b>performance measure</b> P, if its performance at tasks in T, as measured by P, improves with experience E.」。</li></ul><p><i>*加粗的是重点/加粗的是重点/加粗的是重点</i></p><p class=\"ztext-empty-paragraph\"><br/></p><p>其实在「美图数据技术团队」之前的科普文章<b><u><a href=\"https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU5ODU5MjM2Mw%3D%3D%26mid%3D2247484069%26idx%3D1%26sn%3D7676785c65bb99e17346908e6ec2b485%26chksm%3Dfe409ca1c93715b785168d1f94f57bc159f1cb9fa2d63caf94f31650eb2e8234feecee661c55%26scene%3D21%23wechat_redirect\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">贝叶斯概率模型一览</a></u></b>曾介绍过，机器学习狭义上是指代统计机器学习，统计学习根据任务类型可以分为监督学习、半监督学习、无监督学习、增强学习等。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-6faebec7b8b12c1761edc62ee1d48167_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"567\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-6faebec7b8b12c1761edc62ee1d48167_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;567&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"567\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-6faebec7b8b12c1761edc62ee1d48167_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-6faebec7b8b12c1761edc62ee1d48167_b.jpg\"/></figure><p>机器学习常用的算法可以分为以下种类：</p><blockquote><b>1.构造间隔理论分布：</b>人工神经网络、决策树、感知器、支持向量机、集成学习 AdaBoost、降维与度量学习、聚类、贝叶斯分类器；<br/><b>2.构造条件概率：</b>高斯过程回归、线性判别分析、最近邻居法、径向基函数核；<br/><b>3.通过再生模型构造概率密度函数：</b>最大期望算法、概率图模型（贝叶斯网和 Markov 随机场）、Generative Topographic Mapping；<br/><b>4.近似推断技术：</b>马尔可夫链、蒙特卡罗方法、变分法；<br/><b>5.最优化算法。</b></blockquote><p class=\"ztext-empty-paragraph\"><br/></p><h2><b> / Spark MLlib</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p>在上文我们曾提到机器学习的重点之一是「经验」，而对于计算机而言经验往往需要经过多轮迭代计算才能得到，而 Spark 擅长迭代计算，正好符合机器学习这一特性。在 Spark 官网上展示了逻辑回归算法在 Spark 和 Hadoop 上运行性能比较，从下图可以看出 MLlib 比 MapReduce 快了 100 倍。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-67a8fa8908a8d1c2d0abd4ed0721a57a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"250\" data-rawheight=\"129\" class=\"content_image\" width=\"250\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;250&#39; height=&#39;129&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"250\" data-rawheight=\"129\" class=\"content_image lazy\" width=\"250\" data-actualsrc=\"https://pic3.zhimg.com/v2-67a8fa8908a8d1c2d0abd4ed0721a57a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>Spark MLlib 主要包括以下几方面的内容：</p><ul><li><b>学习算法：</b>分类、回归、聚类和协同过滤；</li><li><b>特征处理：</b>特征提取、变换、降维和选择；</li><li><b>管道(Pipeline)：</b>用于构建、评估和调整机器学习管道的工具；</li><li><b>持久性：</b>保存和加载算法，模型和管道；</li><li><b>实用工具：</b>线性代数，统计，最优化，调参等工具。</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-4e40fe81738b173855dfe04a7f510fbf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"469\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-4e40fe81738b173855dfe04a7f510fbf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;469&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"469\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-4e40fe81738b173855dfe04a7f510fbf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-4e40fe81738b173855dfe04a7f510fbf_b.jpg\"/></figure><p>上表总结了 Spark MLlib 支持的功能结构，可以看出它所提供的算法丰富，但算法种类较少并且老旧，因此 Spark MLlib 在算法上支持与 kylin 项目有些脱节，它的主要功能更多是与特征相关的。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>ML Pipelines</b></p><p>从 Spark 2.0 开始基于 RDD 的 API 进入维护模式，Spark 的主要机器学习 API 现在是基于 DataFrame 的 API spark.ml，借鉴 Scikit-Learn 的设计提供了 Pipeline 套件，以构建机器学习工作流。 ML Pipelines 提供了一套基于 DataFrame 构建的统一的高级 API ，可帮助用户创建和调整实用的机器学习流程。</p><p><i>*「Spark ML」不是官方名称，偶尔用于指代基于 MLlib DataFrame 的 API</i></p><p class=\"ztext-empty-paragraph\"><br/></p><p>首先了解 ML Pipelines 内几个重要组件。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>DataFrame</b></p><p>DataFrame 让 Spark 具备了处理大规模结构化数据的能力。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-76bb3c2a93152f2582f0492c5fca77eb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"651\" data-rawheight=\"328\" class=\"origin_image zh-lightbox-thumb\" width=\"651\" data-original=\"https://pic4.zhimg.com/v2-76bb3c2a93152f2582f0492c5fca77eb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;651&#39; height=&#39;328&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"651\" data-rawheight=\"328\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"651\" data-original=\"https://pic4.zhimg.com/v2-76bb3c2a93152f2582f0492c5fca77eb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-76bb3c2a93152f2582f0492c5fca77eb_b.jpg\"/></figure><p>RDD 是分布式  Java 对象的集合，对象的内部数据结构对于 RDD 而言不可知。DataFrame 是一种以 RDD 为基础的分布式数据集，RDD 中存储了 Row 对象，Row 对象提供了详细的结构信息，即模式（schema），使得 DataFrame 具备了结构化数据的能力。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>Transforme</b></p><p>Transformer 通常是一个数据/特征变换的类，或一个训练好的模型。</p><p>每个 Transformer 都有 transform 函数，用于将一个 DataFrame 转换为另一个 DataFrame 。一般 transform 的过程是在输入的 DataFrame 上添加一列或者多列 ，Transformer.transform也是惰性执行，只会生成新的 DataFrame 变量，而不会去提交 job 计算 DataFrame 中的内容。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-cfcd4cf45e783ab25a00907cd93c8600_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"484\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-cfcd4cf45e783ab25a00907cd93c8600_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;484&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"484\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-cfcd4cf45e783ab25a00907cd93c8600_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-cfcd4cf45e783ab25a00907cd93c8600_b.jpg\"/></figure><p><b>Estimator</b></p><p>Estimator 抽象了从输入数据学习模型的过程，每个 Estimator 都实现了 fit 方法，用于给定 DataFrame 和 Params 后，生成一个 Transformer（即训练好的模型），每当调用 Estimator.fit() 后，都会产生 job 去训练模型，得到模型参数。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>Param</b></p><p>可以通过设置 Transformer 或 Estimator 实例的参数来设置模型参数，也可以通过传入 ParamMap 对象来设置模型参数。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-8289d6f0c4264f42d7a92058eb5111c1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"701\" data-rawheight=\"121\" class=\"origin_image zh-lightbox-thumb\" width=\"701\" data-original=\"https://pic2.zhimg.com/v2-8289d6f0c4264f42d7a92058eb5111c1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;701&#39; height=&#39;121&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"701\" data-rawheight=\"121\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"701\" data-original=\"https://pic2.zhimg.com/v2-8289d6f0c4264f42d7a92058eb5111c1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-8289d6f0c4264f42d7a92058eb5111c1_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>Pipeline</b></p><p>Pipeline 定义了一组数据处理流程，可以在 Pipeline 中加入 Transformer、Estimator 或另一个 Pipeline。Pipeline 继承自 Estimator，调用 Pipeline.fit 方法后返回一个 Transformer——PipelineModel；PipelineModel 继承自 Transformer，用于将输入经过 Pipeline 的各个 Transformer 的变换后，得到最终输出。</p><p>Spark MLlib 典型流程如下：</p><ul><li>构造训练数据集</li><li>构建各个 Stage</li><li>Stage 组成 Pipeline</li><li>启动模型训练</li><li>评估模型效果</li><li>计算预测结果</li></ul><p>通过一个 Pipeline 的文本分类示例来加深理解：</p><div class=\"highlight\"><pre><code class=\"language-text\">import org.apache.spark.ml.{Pipeline, PipelineModel}\nimport org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.ml.feature.{HashingTF, Tokenizer}\nimport org.apache.spark.ml.linalg.Vector\nimport org.apache.spark.sql.Row\n\n// Prepare training documents from a list of (id, text, label) tuples.\nval training = spark.createDataFrame(Seq(\n (0L, &#34;a b c d e spark&#34;, 1.0),\n (1L, &#34;b d&#34;, 0.0),\n (2L, &#34;spark f g h&#34;, 1.0),\n (3L, &#34;hadoop mapreduce&#34;, 0.0)\n)).toDF(&#34;id&#34;, &#34;text&#34;, &#34;label&#34;)\n\n// Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\nval tokenizer = new Tokenizer()\n .setInputCol(&#34;text&#34;)\n .setOutputCol(&#34;words&#34;)\nval hashingTF = new HashingTF()\n .setNumFeatures(1000)\n .setInputCol(tokenizer.getOutputCol)\n .setOutputCol(&#34;features&#34;)\nval lr = new LogisticRegression()\n .setMaxIter(10)\n .setRegParam(0.001)\nval pipeline = new Pipeline()\n .setStages(Array(tokenizer, hashingTF, lr))\n\n// Fit the pipeline to training documents.\nval model = pipeline.fit(training)\n\n// Now we can optionally save the fitted pipeline to disk\nmodel.write.overwrite().save(&#34;/tmp/spark-logistic-regression-model&#34;)\n\n// We can also save this unfit pipeline to disk\npipeline.write.overwrite().save(&#34;/tmp/unfit-lr-model&#34;)\n\n// And load it back in during production\nval sameModel = PipelineModel.load(&#34;/tmp/spark-logistic-regression-model&#34;)\n\n// Prepare test documents, which are unlabeled (id, text) tuples.\nval test = spark.createDataFrame(Seq(\n (4L, &#34;spark i j k&#34;),\n (5L, &#34;l m n&#34;),\n (6L, &#34;spark hadoop spark&#34;),\n (7L, &#34;apache hadoop&#34;)\n)).toDF(&#34;id&#34;, &#34;text&#34;)\n\n// Make predictions on test documents.\nmodel.transform(test)\n .select(&#34;id&#34;, &#34;text&#34;, &#34;probability&#34;, &#34;prediction&#34;)\n .collect()\n .foreach { case Row(id: Long, text: String, prob: Vector, prediction: Double) =&gt;\n   println(s&#34;($id, $text) --&gt; prob=$prob, prediction=$prediction&#34;)\n }</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><b>模型选择与调参</b></p><p>Spark MLlib 提供了 CrossValidator 和 TrainValidationSplit 两个模型选择和调参工具。模型选择与调参的三个基本组件分别是 Estimator、ParamGrid 和 Evaluator，其中 Estimator 包括算法或者 Pipeline；ParamGrid 即 ParamMap 集合，提供参数搜索空间；Evaluator 即评价指标。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>CrossValidator</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-3b83d0320df049bafd28231211d94c9c_b.jpg\" data-size=\"normal\" data-rawwidth=\"742\" data-rawheight=\"905\" class=\"origin_image zh-lightbox-thumb\" width=\"742\" data-original=\"https://pic1.zhimg.com/v2-3b83d0320df049bafd28231211d94c9c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;742&#39; height=&#39;905&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"742\" data-rawheight=\"905\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"742\" data-original=\"https://pic1.zhimg.com/v2-3b83d0320df049bafd28231211d94c9c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-3b83d0320df049bafd28231211d94c9c_b.jpg\"/><figcaption>via https://github.com/JerryLead/blogs/blob/master/BigDataSystems/Spark/ML/Introduction%20to%20MLlib%20Pipeline.md</figcaption></figure><p>CrossValidator 将数据集按照交叉验证数切分成 n 份，每次用 n-1 份作为训练集，剩余的作为测试集，训练并评估模型，重复 n 次，得到 n 个评估结果，求 n 次的平均值作为这次交叉验证的结果。接着对每个候选 ParamMap 重复上面的过程，选择最优的 ParamMap 并重新训练模型，得到最优参数的模型输出。</p><p><b>🌰举个例子：</b></p><div class=\"highlight\"><pre><code class=\"language-text\">// We use a ParamGridBuilder to construct a grid of parameters to search over.\n// With 3 values for hashingTF.numFeatures and 2 values for lr.regParam,\n// this grid will have 3 x 2 = 6 parameter settings for CrossValidator to choose from.\nval paramGrid = new ParamGridBuilder()\n .addGrid(hashingTF.numFeatures, Array(10, 100, 1000))\n .addGrid(lr.regParam, Array(0.1, 0.01))\n .build()\n\n// We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n// This will allow us to jointly choose parameters for all Pipeline stages.\n// A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n// Note that the evaluator here is a BinaryClassificationEvaluator and its default metric\n// is areaUnderROC.\nval cv = new CrossValidator()\n .setEstimator(pipeline)\n .setEvaluator(new BinaryClassificationEvaluator)\n .setEstimatorParamMaps(paramGrid)\n .setNumFolds(2)  // Use 3+ in practice\n .setParallelism(2)  // Evaluate up to 2 parameter settings in parallel\n\n// Run cross-validation, and choose the best set of parameters.\nval cvModel = cv.fit(training)\n\n// Prepare test documents, which are unlabeled (id, text) tuples.\nval test = spark.createDataFrame(Seq(\n (4L, &#34;spark i j k&#34;),\n (5L, &#34;l m n&#34;),\n (6L, &#34;mapreduce spark&#34;),\n (7L, &#34;apache hadoop&#34;)\n)).toDF(&#34;id&#34;, &#34;text&#34;)\n\n// Make predictions on test documents. cvModel uses the best model found (lrModel).\ncvModel.transform(test)\n .select(&#34;id&#34;, &#34;text&#34;, &#34;probability&#34;, &#34;prediction&#34;)\n .collect()\n .foreach { case Row(id: Long, text: String, prob: Vector, prediction: Double) =&gt;\n   println(s&#34;($id, $text) --&gt; prob=$prob, prediction=$prediction&#34;)\n }</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><b>TrainValidationSplit</b></p><p>TrainValidationSplit 使用 trainRatio 参数将训练集按照比例切分成训练和验证集，其中 trainRatio 比例的样本用于训练，剩余样本用于验证。</p><p>与 CrossValidator 不同的是，TrainValidationSplit 只有一次验证过程，可以简单看成是 CrossValidator 的 n 为 2 时的特殊版本。</p><p><b>🌰举个例子：</b></p><div class=\"highlight\"><pre><code class=\"language-text\">import org.apache.spark.ml.evaluation.RegressionEvaluator\nimport org.apache.spark.ml.regression.LinearRegression\nimport org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\n\n// Prepare training and test data.\nval data = spark.read.format(&#34;libsvm&#34;).load(&#34;data/mllib/sample_linear_regression_data.txt&#34;)\nval Array(training, test) = data.randomSplit(Array(0.9, 0.1), seed = 12345)\n\nval lr = new LinearRegression()\n   .setMaxIter(10)\n\n// We use a ParamGridBuilder to construct a grid of parameters to search over.\n// TrainValidationSplit will try all combinations of values and determine best model using\n// the evaluator.\nval paramGrid = new ParamGridBuilder()\n .addGrid(lr.regParam, Array(0.1, 0.01))\n .addGrid(lr.fitIntercept)\n .addGrid(lr.elasticNetParam, Array(0.0, 0.5, 1.0))\n .build()\n\n// In this case the estimator is simply the linear regression.\n// A TrainValidationSplit requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\nval trainValidationSplit = new TrainValidationSplit()\n .setEstimator(lr)\n .setEvaluator(new RegressionEvaluator)\n .setEstimatorParamMaps(paramGrid)\n // 80% of the data will be used for training and the remaining 20% for validation.\n .setTrainRatio(0.8)\n // Evaluate up to 2 parameter settings in parallel\n .setParallelism(2)\n\n// Run train validation split, and choose the best set of parameters.\nval model = trainValidationSplit.fit(training)\n\n// Make predictions on test data. model is the model with combination of parameters\n// that performed best.\nmodel.transform(test)\n .select(&#34;features&#34;, &#34;label&#34;, &#34;prediction&#34;)\n .show()</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><b>实现自定义 Transformer</b></p><p>继承自 Transformer 类，实现 transform 方法，通常是在输入的 DataFrame 上添加一列或多列。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-a8e3e083e269af62c988044f9084977e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"759\" data-rawheight=\"502\" class=\"origin_image zh-lightbox-thumb\" width=\"759\" data-original=\"https://pic3.zhimg.com/v2-a8e3e083e269af62c988044f9084977e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;759&#39; height=&#39;502&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"759\" data-rawheight=\"502\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"759\" data-original=\"https://pic3.zhimg.com/v2-a8e3e083e269af62c988044f9084977e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-a8e3e083e269af62c988044f9084977e_b.jpg\"/></figure><p>对于单输入列，单输出列的 Transformer 可以继承自 UnaryTransformer 类，并实现其中的 createTransformFunc 方法，实现对输入列每一行的处理，并返回相应的输出。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-1a3aa30342883bd52ee19299b2c66c86_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"810\" data-rawheight=\"333\" class=\"origin_image zh-lightbox-thumb\" width=\"810\" data-original=\"https://pic3.zhimg.com/v2-1a3aa30342883bd52ee19299b2c66c86_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;810&#39; height=&#39;333&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"810\" data-rawheight=\"333\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"810\" data-original=\"https://pic3.zhimg.com/v2-1a3aa30342883bd52ee19299b2c66c86_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-1a3aa30342883bd52ee19299b2c66c86_b.jpg\"/></figure><h2><b>/ 自研机器学习框架</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p>机器学习技术日新月异，却缺少高效灵活的框架降低新技术的调研成本，而经验与技术往往需要通过框架和工具来沉淀，并且算法人员常常受限于算力，导致离线证明有效的模型，因为预估时间复杂度过高而无法上线。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-163edaaacb126f51dca04c543795c8c5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"818\" data-rawheight=\"452\" class=\"origin_image zh-lightbox-thumb\" width=\"818\" data-original=\"https://pic2.zhimg.com/v2-163edaaacb126f51dca04c543795c8c5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;818&#39; height=&#39;452&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"818\" data-rawheight=\"452\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"818\" data-original=\"https://pic2.zhimg.com/v2-163edaaacb126f51dca04c543795c8c5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-163edaaacb126f51dca04c543795c8c5_b.jpg\"/></figure><p>据此美图数据技术团队以「开发简单灵活的机器学习工作流，降低算法人员的新算法调研成本及工程人员的维护成本，并且提供常用的领域内解决方案，将经验沉淀」的目标搭建了一套量身定制的机器学习框架用以解决上述问题，尤其是解决在推荐算法相关任务上遇到的问题。该框架总共包括 3 个组件：Spark Feature、Bamboo 与 Online Scorer。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>Spark Feature：训练样本生产</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-516deb998df60e7da075471d8178117f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"770\" data-rawheight=\"134\" class=\"origin_image zh-lightbox-thumb\" width=\"770\" data-original=\"https://pic4.zhimg.com/v2-516deb998df60e7da075471d8178117f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;770&#39; height=&#39;134&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"770\" data-rawheight=\"134\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"770\" data-original=\"https://pic4.zhimg.com/v2-516deb998df60e7da075471d8178117f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-516deb998df60e7da075471d8178117f_b.jpg\"/></figure><p>该组件主要用于训练样本的生产，实现了灵活高效的样本特征编码，可以实现将任意特征集合放在同一个空间进行编码，不同特征集合共享编码空间；为此我们提出了两个概念：第一个是「域」，用于定义共享相同建模过程的一组特征；第二个是「空间」，用于定义共享相同编码空间的一组域。</p><p>上图示例中的「Old」展示了在没有“域”和“空间”概念下的样本特征编码，所有特征从 1 开始编号；「New」展示了将 age 和 gender 分别放到 age 域和 gender 域后，两个域分别从 1 开始编码，互不影响。</p><p>Spark Feature 最终采用 TFRecords 作为训练样本的存储格式。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>Bamboo：模型定义与训练</b></p><p>该组件主要为了实现可扩展、高效、简单快速的模型定义与训练。为此，在设计 Bamboo 时我们遵循以下原则：</p><p><b>1.</b>layer 之间通过 tensor 进行交互，layer 的输入是 tensor，输出也是 tensor；</p><p><b>2.</b>为了最大限度地提高离线与在线效率，没有采用太多高级 api，如 keras，大多数模型与组件基于 Tensorflow 底层 api 开发，并且根据 Tensorflow 官方的性能优化指南对代码进行优化；</p><p><b>3.</b>提供 online-offline 的建模框架，复杂计算放到离线，在线只进行轻量计算，使得复杂模型更易上线；</p><p><b>4.</b>封装数据加载、模型训练与导出、效果评估以及提供了各种辅助工具，用户只需要定义前向推理网络，同时封装了大量的常用 layer，模型定义更快捷。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-cb8577b3e4b15b2a60ca97fbfbc607f3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"322\" data-rawheight=\"987\" class=\"content_image\" width=\"322\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;322&#39; height=&#39;987&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"322\" data-rawheight=\"987\" class=\"content_image lazy\" width=\"322\" data-actualsrc=\"https://pic4.zhimg.com/v2-cb8577b3e4b15b2a60ca97fbfbc607f3_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>Online Scorer：在线预测服务</b></p><p>Online Scorer的目标是提供一个统一，高效的在线推理服务，可以同时支持tensorflow，pytorch，xgboost等各种主流建模框架导出的模型。目前这块工作还在进行中，具体实现方案细节，我们放到后面的专题文章介绍。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-3290ab0a423df06dfe406d9e69915c73_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"404\" data-rawheight=\"559\" class=\"content_image\" width=\"404\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;404&#39; height=&#39;559&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"404\" data-rawheight=\"559\" class=\"content_image lazy\" width=\"404\" data-actualsrc=\"https://pic4.zhimg.com/v2-3290ab0a423df06dfe406d9e69915c73_b.jpg\"/></figure><p>以上就是美图自研机器学习框架的简要介绍，欢迎持续关注「美图数据技术团队」，后续将带来该平台的详细介绍。</p><p></p>", 
            "topic": [
                {
                    "tag": "大数据", 
                    "tagLink": "https://api.zhihu.com/topics/19740929"
                }, 
                {
                    "tag": "Spark", 
                    "tagLink": "https://api.zhihu.com/topics/19942170"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/42196360", 
            "userName": "MT酱", 
            "userLink": "https://www.zhihu.com/people/28629950b8d6033df6b6f33fd0280e2a", 
            "upvote": 8, 
            "title": "RDD原理与基本操作 | Spark，从入门到精通", 
            "content": "<blockquote>欢迎阅读美图数据技术团队的「Spark，从入门到精通」系列文章，本系列文章将由浅入深为大家介绍 Spark，从框架入门到底层架构的实现，相信总有一种姿势适合你，欢迎大家持续关注：）</blockquote><p class=\"ztext-empty-paragraph\"><br/></p><p>往期直通车：</p><p><a href=\"https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU5ODU5MjM2Mw%3D%3D%26mid%3D2247484147%26idx%3D2%26sn%3Dcca8dc960db221fb920bfb545d357ad9%26chksm%3Dfe409cf7c93715e1a2f76400f33e7b1e43b74d104bb73e2df5c2d66334708f2ab44affb32348%26scene%3D21%23wechat_redirect\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Hello Spark! </a></p><p><a href=\"https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU5ODU5MjM2Mw%3D%3D%26mid%3D2247484270%26idx%3D2%26sn%3Df287173c5d676625f11c7ef415b81cf4%26chksm%3Dfe409d6ac937147c2bd197104a3e4070c9d12786c9c37c17d699039a5d96858cafcc8eee53e5%26scene%3D21%23wechat_redirect\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Spark on Yarn</a></p><h2><b>什么是 RDD？</b></h2><p>传统的 MapReduce 虽然具有自动容错、平衡负载和可拓展性的优点，但是其最大缺点是在迭代计算式的时候，要进行大量的磁盘 IO 操作，而 RDD 正是解决这一缺点的抽象方法。RDD（Resilient Distributed Datasets）即弹性分布式数据集，从名字说起：</p><p><b>弹性</b></p><p>当计算过程中内存不足时可刷写到磁盘等外存上，可与外存做灵活的数据交换；</p><p>RDD 使用了一种“血统”的容错机制，在结构更新和丢失后可随时根据血统进行数据模型的重建；</p><p><b>分布式</b></p><p>就是可以分布在多台机器上进行并行计算；</p><p><b>数据集</b></p><p>一组只读的、可分区的分布式数据集合，集合内包含了多个分区。分区依照特定规则将具有相同属性的数据记录放在一起，每个分区相当于一个数据集片段。</p><p><b>RDD 内部结构</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-67adb3c3c97414b92323aaa959b9cf39_b.jpg\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"775\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-67adb3c3c97414b92323aaa959b9cf39_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;775&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"775\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic2.zhimg.com/v2-67adb3c3c97414b92323aaa959b9cf39_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-67adb3c3c97414b92323aaa959b9cf39_b.jpg\"/><figcaption>图 1</figcaption></figure><p>图 1 所示是 RDD 的内部结构图，它是一个只读、有属性的数据集。它的属性用来描述当前数据集的状态，数据集由数据的分区（partition）组成，并由（block）映射成真实数据。RDD 的主要属性可以分为 3 类：与其他 RDD 的关系（parents、dependencies）；数据(partitioner、checkpoint、storage level、iterator 等)；RDD 自身属性(sparkcontext、sparkconf)，接下来我们根据属性分类来深入介绍各个组件。</p><p><b>RDD 自身属性</b></p><p>从自身属性说起，<b>SparkContext</b> 是 Spark job 的入口，由 Driver 创建在 client 端，包括集群连接、RDD ID、累加器、广播变量等信息。<b>SparkConf</b> 是参数配置信息，包括：</p><ul><li><b>Spark api</b>，控制大部分的应用程序参数；</li><li><b>环境变量</b>，配置IP地址、端口等信息；</li><li><b>日志配置</b>，通过 log4j.properties 配置。</li></ul><p><b>数据</b></p><p>RDD 内部的数据集合在逻辑上和物理上被划分成多个小子集合，这样的每一个子集合我们将其称为<b>分区（Partitions）</b>，分区的个数会决定并行计算的粒度，而每一个分区数值的计算都是在一个单独的任务中进行的，因此并行任务的个数也是由 RDD分区的个数决定的。但事实上 RDD 只是数据集的抽象，分区内部并不会存储具体的数据。Partition 类内包含一个 index 成员，表示该分区在 RDD 内的编号，通过 RDD  编号+分区编号可以确定该分区对应的唯一块编号，再利用底层数据存储层提供的接口就能从存储介质（如：HDFS、Memory）中提取出分区对应的数据。</p><p>RDD 的分区方式主要包含两种：Hash Partitioner 和 Range Partitioner，这两种分区类型都是针对 Key-Value 类型的数据，如是非 Key-Value 类型则分区函数为 None。Hash 是以 Key 作为分区条件的散列分布，分区数据不连续，极端情况也可能散列到少数几个分区上导致数据不均等；Range 按 Key 的排序平衡分布，分区内数据连续，大小也相对均等。</p><p><b>Preferred Location</b> 是一个列表，用于存储每个 Partition 的优先位置。对于每个 HDFS 文件来说，这个列表保存的是每个 Partition 所在的块的位置，也就是该文件的「划分点」。</p><p><b>Storage Level </b>是 RDD 持久化的存储级别，RDD 持久化可以调用两种方法：cache 和 persist：persist 方法可以自由的设置存储级别，默认是持久化到内存；cache 方法是将 RDD 持久化到内存，cache 的内部实际上是调用了persist 方法，由于没有开放存储级别的参数设置，所以是直接持久化到内存。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-4811ef4ac9dc7e33b49990b9f9da105e_b.jpg\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"806\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-4811ef4ac9dc7e33b49990b9f9da105e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;806&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"806\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-4811ef4ac9dc7e33b49990b9f9da105e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-4811ef4ac9dc7e33b49990b9f9da105e_b.jpg\"/><figcaption>图 2</figcaption></figure><p>如图 2 所示是 Storage Level 各级别分布，那么如何选择一种最合适的持久化策略呢？默认情况下，性能最高的当然是 MEMORY_ONLY，但前提是你的内存必须足够大到可以绰绰有余地存放下整个 RDD 的所有数据。因为不进行序列化与反序列化操作，就避免了这部分的性能开销；对这个RDD的后续算子操作，都是基于纯内存中的数据的操作，不需要从磁盘文件中读取数据，性能也很高；而且不需要复制一份数据副本，并远程传送到其他节点上。但是这里必须要注意的是，在实际的生产环境中，恐怕能够直接用这种策略的场景还是有限的，如果 RDD 中数据比较多时（比如几十亿），直接用这种持久化级别，会导致 JVM 的 OOM 内存溢出异常。</p><p>如果使用 MEMORY_ONLY 级别时发生了内存溢出，那么建议尝试使用 MEMORY_ONLY_SER 级别。该级别会将 RDD 数据序列化后再保存在内存中，此时每个 partition 仅仅是一个字节数组而已，大大减少了对象数量，并降低了内存占用。这种级别比 MEMORY_ONLY 多出来的性能开销主要就是序列化与反序列化的开销，但是后续算子可以基于纯内存进行操作，因此性能总体还是比较高的。但可能发生 OOM  内存溢出的异常。</p><p>如果纯内存的级别都无法使用，那么建议使用 MEMORY_AND_DISK_SER 策略，而不是 MEMORY_AND_DISK 策略。因为既然到了这一步，就说明 RDD 的数据量很大，内存无法完全放下，序列化后的数据比较少，可以节省内存和磁盘的空间开销。同时该策略会优先尽量尝试将数据缓存在内存中，内存缓存不下才会写入磁盘。</p><p>通常不建议使用 DISK_ONLY 和后缀为_2 的级别：因为完全基于磁盘文件进行数据的读写，会导致性能急剧降低。后缀为_2的级别，必须将所有数据都复制一份副本，并发送到其他节点上，数据复制以及网络传输会导致较大的性能开销。</p><p><b>Checkpoint </b>是 Spark 提供的一种缓存机制，当需要计算依赖链非常长又想避免重新计算之前的 RDD 时，可以对 RDD 做 Checkpoint 处理，检查 RDD 是否被物化或计算，并将结果持久化到磁盘或 HDFS 内。Checkpoint 会把当前 RDD 保存到一个目录，要触发 action 操作的时候它才会执行。在 Checkpoint 应该先做持久化（persist 或者 cache）操作，否则就要重新计算一遍。若某个 RDD 成功执行 checkpoint，它前面的所有依赖链会被销毁。</p><p>与 Spark 提供的另一种缓存机制 cache 相比：cache 缓存数据由 executor 管理，若 executor 消失，它的数据将被清除，RDD 需要重新计算；而 checkpoint 将数据保存到磁盘或 HDFS 内，job 可以从 checkpoint 点继续计算。Spark 提供了 rdd.persist(StorageLevel.DISK_ONLY) 这样的方法，相当于 cache 到磁盘上，这样可以使 RDD 第一次被计算得到时就存储到磁盘上，它们之间的区别在于：persist 虽然可以将 RDD 的 partition 持久化到磁盘，但一旦作业执行结束，被 cache 到磁盘上的 RDD 会被清空；而 checkpoint 将 RDD 持久化到 HDFS 或本地文件夹，如果不被手动 remove 掉，是一直存在的。</p><p><b>Compute</b> 函数实现方式就是向上递归「获取父 RDD 分区数据进行计算」，直到遇到检查点 RDD 获取有缓存的 RDD。</p><p><b>Iterator</b> 用来查找当前 RDD Partition 与父 RDD 中 Partition 的血缘关系，并通过 Storage Level 确定迭代位置，直到确定真实数据的位置。它的实现流程如下：</p><ul><li>若标记了有缓存，则取缓存，取不到则进行 computeOrReadCheckpoint(计算或读检查点)。完了再存入缓存，以备后续使用。 </li><li>若未标记有缓存，则直接进行 computeOrReadCheckpoint。 </li><li>computeOrReadCheckpoint 这个过程也做两个判断：有做过 checkpoint 和没有做过 checkpoint，做过 checkpoint 则可以读取到检查点数据返回，没做过则调该 RDD 的实现类的 compute 函数计算。</li></ul><p><b>血统关系</b></p><p>一个作业从开始到结束的计算过程中产生了多个 RDD，RDD 之间是彼此相互依赖的，我们把这种父子依赖的关系称之为「血统」。</p><p>RDD 只支持粗颗粒变换，即只记录单个块（分区）上执行的单个操作，然后创建某个 RDD 的变换序列（血统 lineage）存储下来。</p><p>*变换序列指每个 RDD 都包含了它是如何由其他 RDD 变换过来的以及如何重建某一块数据的信息。</p><p>因此 RDD 的容错机制又称「血统」容错。 要实现这种「血统」容错机制，最大的难题就是如何表达父 RDD 和子 RDD 之间的依赖关系。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-c77f3d4c8009d06c103a9a6518355d43_b.jpg\" data-size=\"normal\" data-rawwidth=\"768\" data-rawheight=\"546\" class=\"origin_image zh-lightbox-thumb\" width=\"768\" data-original=\"https://pic4.zhimg.com/v2-c77f3d4c8009d06c103a9a6518355d43_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;768&#39; height=&#39;546&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"768\" data-rawheight=\"546\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"768\" data-original=\"https://pic4.zhimg.com/v2-c77f3d4c8009d06c103a9a6518355d43_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-c77f3d4c8009d06c103a9a6518355d43_b.jpg\"/><figcaption>图 3</figcaption></figure><p>如图 3 所示，父 RDD 的每个分区最多只能被子 RDD 的一个分区使用，称为窄依赖（narrow dependency）；若父 RDD 的每个分区可以被子 RDD 的多个分区使用，称为宽依赖（wide dependency）。简单来讲，窄依赖就是父子RDD分区间「一对一」的关系，而宽依赖就是「一对多」关系。从失败恢复来看，窄依赖的失败恢复起来更高效，因为它只需找到父 RDD 的一个对应分区即可，而且可以在不同节点上并行计算做恢复；宽依赖牵涉到父 RDD 的多个分区，需要得到所有依赖的父 RDD 分区的 shuffle 结果，恢复起来相对复杂些。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-b74c2d7a9e35f76d68fb677b0c8ce907_b.jpg\" data-size=\"normal\" data-rawwidth=\"700\" data-rawheight=\"546\" class=\"origin_image zh-lightbox-thumb\" width=\"700\" data-original=\"https://pic4.zhimg.com/v2-b74c2d7a9e35f76d68fb677b0c8ce907_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;700&#39; height=&#39;546&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"700\" data-rawheight=\"546\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"700\" data-original=\"https://pic4.zhimg.com/v2-b74c2d7a9e35f76d68fb677b0c8ce907_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-b74c2d7a9e35f76d68fb677b0c8ce907_b.jpg\"/><figcaption>图 4</figcaption></figure><p>根据 RDD 之间的宽窄依赖关系引申出 Stage 的概念，Stage 是由一组 RDD 组成的执行计划。如果 RDD 的衍生关系都是窄依赖，则可放在同一个 Stage 中运行，若 RDD 的依赖关系为宽依赖，则要划分到不同的 Stage。这样 Spark 在执行作业时，会按照 Stage 的划分, 生成一个最优、完整的执行计划。<br/></p><h2><b>RDD 的创建方式与分区机制</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p><b>RDD 的创建方式</b></p><p>RDD 的创建方式有四种：</p><p><b>1.使用程序中的集合创建 RDD</b>，RDD 的数据源是程序中的集合，通过 parallelize 或者 makeRDD 将集合转化为 RDD；</p><p>*例</p><div class=\"highlight\"><pre><code class=\"language-text\">val num = Array(1,2,3,4,5)\n\nval rdd = sc.parallelize(num)</code></pre></div><p><b>2.使用本地文件或 HDFS 创建 RD</b>D，RDD 的数据源是本地文件系统或 HDFS 的数据，使用 textFile 方法创建RDD。</p><p>*例</p><div class=\"highlight\"><pre><code class=\"language-text\">val rdd = sc.textFile(“hdfs://master:9000/rec/data”)</code></pre></div><p><b>3.使用数据流创建 RDD</b>，使用 Spark Streaming 的相关类，接收实时的输入数据流创建 RDD（数据流来源可以是 kafka、flume 等）。</p><p>*例</p><div class=\"highlight\"><pre><code class=\"language-text\">val ssc = new StreamingContext(conf, Seconds(1))\n\nval lines = ssc.socketTextStream(“localhost”, 9999)\n\nval words = lines.flatMap(_.split(“ ”))</code></pre></div><p><b>4.使用其他方式创建 RDD</b>，从其他数据库上创建 RDD，例如 Hbase、MySQL 等。</p><p>*例</p><div class=\"highlight\"><pre><code class=\"language-text\">val sqlContext = new SQLContext(sc)\n\nval url = &#34;jdbc:mysql://ip:port/xxxx&#34;\n\nval prop = new Properties()\n\nval df = sqlContext.read.jdbc(url, “play_time”, prop)</code></pre></div><p><b>RDD 的分区机制</b></p><p>RDD 的分区机制有两个关键点：一个是关键参数，即 Spark 的默认并发数 spark.default.parallelism；另一个是关键原则，RDD 分区尽可能使得分区的个数等于集群核心数目。</p><p>当配置文件 spark-default.conf 中显式配置了 spark.default.parallelism，那么 spark.default.parallelism=配置的值，否则按照如下规则进行取值：</p><p><b>1.本地模式</b>（不会启动 executor，由 SparkSubmit 进程生成指定数量的线程数来并发）</p><blockquote>spark-shell       spark.default.parallelism = 1<br/>spark-shell --master local[N] spark.default.parallelism = N （使用 N 个核）<br/>spark-shell --master local       spark.default.parallelism = 1</blockquote><p><b>2.伪集群模式</b>（x 为本机上启动的 executor 数，y 为每个 executor 使用的 core 数，z 为每个 executor 使用的内存）</p><blockquote>spark-shell --master local-cluster[x,y,z] spark.default.parallelism = x * y</blockquote><p><b>3.Yarn、standalone 等模式</b><br/></p><blockquote>spark.default.parallelism = max(所有 executor 使用的 core 总数，2)</blockquote><p><b>4.Mesos</b><br/></p><blockquote>spark.default.parallelism = 8</blockquote><p>spark.context 会生成两个参数，由 spark.default.parallelism 推导出这两个参数的值：</p><div class=\"highlight\"><pre><code class=\"language-text\">sc.defaultParallelism     = spark.default.parallelism\n\nsc.defaultMinPartitions  = min(spark.default.parallelism, 2)</code></pre></div><p>当 sc.defaultParallelism 和 sc.defaultMinPartitions 确认后，就可以推算 RDD 的分区数了。</p><ul><li>以 parallelize 方法为例</li></ul><div class=\"highlight\"><pre><code class=\"language-text\">val rdd = sc.parallelize(1 to 10)</code></pre></div><p>如果使用 parallelize 方法时没指定分区数， RDD 的分区数 = sc.defaultParallelism</p><ul><li>以 textFile 方法为例</li></ul><div class=\"highlight\"><pre><code class=\"language-text\">val rdd = sc.textFile(“path/file”)</code></pre></div><p>分区机制分两种情况：</p><p>1.从本地文件生成的 RDD，如果没有指定分区数，则默认分区数规则为</p><blockquote>rdd 的分区数 = max（本地 file 的分片数， sc.defaultMinPartitions）</blockquote><p>2.从 HDFS 生成的 RDD，如果没有指定分区数，则默认分区数规则为：</p><blockquote>rdd 的分区数 = max（hdfs 文件的 block 数目， sc.defaultMinPartitions）</blockquote><h2><br/><b> RDD 的常用操作 </b></h2><p>RDD 支持两种类型的操作：转换（Transformation）和动作（Action），转换操作是从已经存在的数据集中创建一个新的数据集，而动作操作是在数据集上进行计算后返回结果到 Driver，既触发 SparkContext 提交 Job 作业。转换操作都具有 Lazy 特性，即 Spark 不会立刻进行实际的计算，只会记录执行的轨迹，只有触发行动操作的时候，它才会根据 DAG 图真正执行。</p><p>转换与动作具体包含的操作种类如下图所示：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-0080758bf372c68bbc7cc9ff0c107578_b.jpg\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"653\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-0080758bf372c68bbc7cc9ff0c107578_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;653&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"653\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-0080758bf372c68bbc7cc9ff0c107578_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-0080758bf372c68bbc7cc9ff0c107578_b.jpg\"/><figcaption>图 5:转换操作</figcaption></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-e8a0367cc4af176d676bbe676906ca0f_b.jpg\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"544\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-e8a0367cc4af176d676bbe676906ca0f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;544&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"544\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-e8a0367cc4af176d676bbe676906ca0f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-e8a0367cc4af176d676bbe676906ca0f_b.jpg\"/><figcaption>图 6：动作操作</figcaption></figure><p>最后我们通过一段代码来看看它具体的操作：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-4e9995a395bcadba3e0d2d5ae37b3966_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"561\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-4e9995a395bcadba3e0d2d5ae37b3966_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;561&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"561\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-4e9995a395bcadba3e0d2d5ae37b3966_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-4e9995a395bcadba3e0d2d5ae37b3966_b.jpg\"/></figure><p>这段代码是用来计算某个视频被男性或女性用户的播放次数，其中 rdd_attr 用来记录用户性别，rdd_src 是用户对某个视频进行播放的记录，这两个 RDD 会进行一个 join 操作，比如这是某个男性用户对某个视频进行了播放，进行 map 操作之后得到视频 id 和性别作为 key，根据这个 key 做 reduceByKey 的操作，最终得到一个视频被男性/女性用户总共播放了多少次的 RDD，然后使用 combineByKey 合并同一个视频 id 的多个结果，最后保存到 HDFS 上。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>附：参考文章</b></p><blockquote>《Spark之深入理解RDD结构》<br/><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/u011094454/article/details/78992293\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">blog.csdn.net/u01109445</span><span class=\"invisible\">4/article/details/78992293</span><span class=\"ellipsis\"></span></a><br/>《RDD的数据结构模型》<br/><a href=\"https://link.zhihu.com/?target=https%3A//www.jianshu.com/p/dd7c7243e7f9%3Ffrom%3Dsinglemessage\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">jianshu.com/p/dd7c7243e</span><span class=\"invisible\">7f9?from=singlemessage</span><span class=\"ellipsis\"></span></a><br/>《Spark RDD详解》<br/><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/wangxiaotongfan/article/details/51395769\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">blog.csdn.net/wangxiaot</span><span class=\"invisible\">ongfan/article/details/51395769</span><span class=\"ellipsis\"></span></a><br/>《Spark RDD的默认分区数：（spark 2.1.0）》<br/><a href=\"https://link.zhihu.com/?target=https%3A//www.jianshu.com/p/4b7d07e754fa\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">jianshu.com/p/4b7d07e75</span><span class=\"invisible\">4fa</span><span class=\"ellipsis\"></span></a><br/>《Spark性能优化指南——基础篇》<br/><a href=\"https://link.zhihu.com/?target=https%3A//tech.meituan.com/spark_tuning_basic.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">tech.meituan.com/spark_</span><span class=\"invisible\">tuning_basic.html</span><span class=\"ellipsis\"></span></a></blockquote>", 
            "topic": [
                {
                    "tag": "MapReduce", 
                    "tagLink": "https://api.zhihu.com/topics/19621083"
                }, 
                {
                    "tag": "Spark", 
                    "tagLink": "https://api.zhihu.com/topics/19942170"
                }, 
                {
                    "tag": "Hadoop", 
                    "tagLink": "https://api.zhihu.com/topics/19563390"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/41151457", 
            "userName": "MT酱", 
            "userLink": "https://www.zhihu.com/people/28629950b8d6033df6b6f33fd0280e2a", 
            "upvote": 11, 
            "title": "Spark on Yarn", 
            "content": "<p>欢迎阅读美图数据技术团队的「Spark，从入门到精通」系列文章，本系列文章将由浅入深为大家介绍 Spark，从框架入门到底层架构的实现，相信总有一种姿势适合你，欢迎大家持续关注：）</p><p>往期直通车：<a href=\"https://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU5ODU5MjM2Mw%3D%3D%26mid%3D2247484147%26idx%3D2%26sn%3Dcca8dc960db221fb920bfb545d357ad9%26chksm%3Dfe409cf7c93715e1a2f76400f33e7b1e43b74d104bb73e2df5c2d66334708f2ab44affb32348%26scene%3D21%23wechat_redirect\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Hello Spark! </a></p><h2><b>为什么需要 Yarn？</b></h2><p>Yarn 的全称是 Yet Anther Resource Negotiator（另一种资源协商者）。它作为 Hadoop 的一个组件，官方对它的定义是<b>一个工作调度和集群资源管理的框架</b>。</p><p>Yarn 最早出现于 Hadoop 0.23 分支中，0.23 分支是一个实验性分支，之后经过了几次迭代，最后发布于 2014 年 6 月的 0.23.11 版本(该分支的最后一个版本)。在 0.23.0 发布后不久的 2011 年 12 月，Hadoop 的 0.20 分支发展成了 Hadoop1.0，一直到 1.0 的最后一个版本 1.2.1-stable 都没有出现 Yarn 的身影，而在 Hadoop2.0 的第一个版本 2.0.0-alpha，Yarn 已经作为一个正式组件加入。在 2.0.2-alpha 版本，它已经支持了 2k 台机器的集群，接着在 2.0.3-alpha 版本中已经可以支持 30k 台机器的集群。在 2.0.3-alpha 版本中同时还支持了多种资源，如 cpu&amp;memory 的调度和 ResourceManager restart。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-9a05f0d8293b6fdb8fd26c121bbabafb_b.jpg\" data-size=\"normal\" data-rawwidth=\"648\" data-rawheight=\"452\" class=\"origin_image zh-lightbox-thumb\" width=\"648\" data-original=\"https://pic4.zhimg.com/v2-9a05f0d8293b6fdb8fd26c121bbabafb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;648&#39; height=&#39;452&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"648\" data-rawheight=\"452\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"648\" data-original=\"https://pic4.zhimg.com/v2-9a05f0d8293b6fdb8fd26c121bbabafb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-9a05f0d8293b6fdb8fd26c121bbabafb_b.jpg\"/><figcaption>图 1，via https://blog.csdn.net/suifeng3051/article/details/49364677</figcaption></figure><p>如图 1 所示， Hadoop1.0 的运作流程如下：</p><blockquote>1.客户端提交任务给集群；<br/>2.JobTracker 接收 Job 请求；<br/>3.JobTracker 根据 Job 的输入参数向 NameNode 请求包含这些文件数据块的 DataNode 节点列表；<br/>4.JobTracker 确定 Job 的执行计划：确认 Map、Reduce 的 Task 数量，并分配 Task 到离数据块最近的节点上执行。</blockquote><p>最初，Hadoop1.0 能够很好地支撑大数据计算，但是随着计算规模的扩大和计算模型的多样化，它逐渐力不从心。众所周知当集群性能不足的时候可以简单粗暴地加机器，但 JobTracker 同时部署多个时只有一个是处于 active 状态，因此受限于这个 active JobTracker 的负载上限，整个集群能够容纳的机器也有限，有数据显示整个集群的管理上限约为 4k 台机器。同时应用程序相关和资源管理相关的逻辑全部放在 JobTracker中，当集群规模扩大的时候，会存在一个瓶颈。除此之外，Map-Reduce 计算模型与 JobTracker 的耦合过高，其他计算模型难以在 Hadoop1.0 上运行。</p><p>Yarn 是 Hadoop 基于这些问题的一个解决方案，接下来通过了解 Yarn 的组件、架构以及运作机制来分析 Yarn 是如何解决这些问题的。</p><h2><b>Yarn 是什么？</b></h2><p><b>Yarn 的组件&amp;基本架构</b></p><p>如图 2 所示 Yarn 采用 Master/Slave 结构，整体采用双层调度架构。在第一层的调度是 ResourceManager 和 NodeManager：ResourceManager 是 Master 节点，相当于 JobTracker，包含 Scheduler 和App Manager 两个组件，分管资源调度和应用管理；NodeManager 是 Slave 节点，可以部署在独立的机器上，用于管理机器上的资源。NodeManager 会向 ResourceManager 报告它的资源数量、使用情况，并接受 ResourceManager 的资源调度。</p><p>*ResourceManager 同 JobTracker 一样可以多机部署，并且只有一台处于 active 状态。但是在 ResourceManager 中将调度管理和应用管理作了拆分，两个组件的功能更专一。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-fe86a3a29c60321d7a2df62c5748a666_b.jpg\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"667\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-fe86a3a29c60321d7a2df62c5748a666_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;667&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"667\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-fe86a3a29c60321d7a2df62c5748a666_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-fe86a3a29c60321d7a2df62c5748a666_b.jpg\"/><figcaption>图 2</figcaption></figure><p>第二层的调度指的是 NodeManager 和 Container。NodeManager 会将 Cpu&amp;内存等资源抽象成一个个的 Container，并管理它们的生命周期。</p><p>通过采用双层调度结构将 Scheduler 管理的资源由细粒度的 Cpu&amp;内存变成了粗粒度的 Container，降低了负载。在 App Manager 组件中也只需要管理 App Master，不需要管理任务调度执行的完整信息，同样降低了负载。通过降低 ResourceManager 的负载，变相地提高了集群的扩展性。</p><p><b>Yarn 运作流程</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-d0e4660fc4042cfc25a09b16c022bdf2_b.jpg\" data-size=\"normal\" data-rawwidth=\"648\" data-rawheight=\"356\" class=\"origin_image zh-lightbox-thumb\" width=\"648\" data-original=\"https://pic3.zhimg.com/v2-d0e4660fc4042cfc25a09b16c022bdf2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;648&#39; height=&#39;356&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"648\" data-rawheight=\"356\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"648\" data-original=\"https://pic3.zhimg.com/v2-d0e4660fc4042cfc25a09b16c022bdf2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-d0e4660fc4042cfc25a09b16c022bdf2_b.jpg\"/><figcaption>图 3，via https://blog.csdn.net/suifeng3051/article/details/49486927</figcaption></figure><p>如图 3 所示 Yarn 的运作流程如下：</p><blockquote>1.客户端向 ResourceManager 的 App Manager 提交应用并请求一个 AppMaster 实例；<br/>2.ResourceManager 找到可以运行一个 Container 的 NodeManager，并在这个 Container 中启动 AppMaster 实例；<br/>3.App Master 向 ResourceManager 注册，注册之后，客户端就可以查询 ResourceManager 获得自己 App Master 的详情以及直接和 App Master 交互；<br/>4.接着 App Master 向 Resource Manager 请求资源，即 Container；<br/>5.获得 Container 后，App Master 启动 Container，并执行 Task；<br/>6.Container 执行过程中会把运行进度和状态等信息发送给 AppMaster；<br/>7.客户端主动和 App Master 交流应用的运行状态、进度更新等信息；<br/>8.所有工作完成 App Master 向 RM 取消注册然后关闭，同时所有的 Container 也归还给系统。</blockquote><p>通过这个 Job 的处理过程可以看到 App Master 是作为 Job 的驱动角色，它驱动了 Job 任务的调度执行。在这个运作流程中，App Manager 只需要管理 App Master 的生命周期以及保存它的内部状态，而 App Master 这个角色的抽象使得每种类型的应用都可以定制自己的 App Master，这样其他的计算模型就可以相对容易地运行在 Yarn 集群上。</p><p><b>Yarn HA（容灾备援）</b></p><p>接下来介绍的是 Yarn 集群高可用中关于容错备援的设计。根据图 3 所示的 Yarn 架构图，假如 Container 故障 Resource Manager 可以分配其他的 Container 继续执行，当运行 App Master 的 Container 故障后也将分配新的 Container，App Master 可以从 App Manager 获取信息恢复。当 NodeManager 故障的时候系统可以先把这个节点移除，在其他 NodeManager 重启再继续任务。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-baa93cbd45b42782014c88c4b9e3d488_b.jpg\" data-size=\"normal\" data-rawwidth=\"648\" data-rawheight=\"356\" class=\"origin_image zh-lightbox-thumb\" width=\"648\" data-original=\"https://pic1.zhimg.com/v2-baa93cbd45b42782014c88c4b9e3d488_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;648&#39; height=&#39;356&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"648\" data-rawheight=\"356\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"648\" data-original=\"https://pic1.zhimg.com/v2-baa93cbd45b42782014c88c4b9e3d488_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-baa93cbd45b42782014c88c4b9e3d488_b.jpg\"/><figcaption>图 4，via https://www.cnblogs.com/sodawoods-blogs/p/8715231.html</figcaption></figure><p>那么当 ResourceManager 故障的时候呢？如上文所说的在 Yarn 集群中，ResourceManager 可以启动多台，只有其中一台是 active 状态的，其他都处于待命状态。这台 active 状态的 ResourceManager 执行的时候会向 ZooKeeper 集群写入它的状态，当它故障的时候这些 RM 首先选举出另外一台 leader 变为 active 状态，然后从 ZooKeeper 集群加载 ResourceManager 的状态。在转移的过程中它不接收新的 Job，转移完成后才接收新 Job。</p><h2><b>Spark on Yarn </b></h2><p>首先介绍 Spark 的资源管理架构。Spark 集群考虑到了未来对接一些更强大的资源管理系统（如 Yarn、Mesos 等）没有在资源管理的设计上对外封闭，所以Spark 架构设计时将资源管理抽象出了一层，通过这种抽象能够构建一种插件式的资源管理模块。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-74cf18e2c2e27e657e2afcb73574cd0a_b.jpg\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"721\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-74cf18e2c2e27e657e2afcb73574cd0a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;721&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"721\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-74cf18e2c2e27e657e2afcb73574cd0a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-74cf18e2c2e27e657e2afcb73574cd0a_b.jpg\"/><figcaption>图 5，via http://shiyanjun.cn/archives/1545.html</figcaption></figure><p>如图 5 所示是 Spark 的资源管理架构图。Master 是 Spark 的 主控节点，在实际的生产环境中会有多个 Master，只有一个 Master 处于 active 状态。Worker 是 Spark 的工作节点，向 Master 汇报自身的资源、Executeor 执行状态的改变，并接受 Master 的命令启动 Executor 或 Driver。Driver 是应用程序的驱动程序，每个应用包括许多小任务，Driver 负责推动这些小任务的有序执行。Executor 是 Spark 的工作进程，由 Worker 监管，负责具体任务的执行。</p><p>以上就是 Spark 在资源管理上的抽象出来的架构，这个架构跟 Yarn 的架构十分相似，因此 Spark 很容易地构建于 Yarn 之上。在 Spark 和 Yarn 两边的角色对比中：Master 和 ResourceManager 对应，Worker 和 NodeManager 对应，Driver 和 App Master 对应，Executor 和 Container 对应。</p><p>根据 Spark 部署模式的不同资源管理架构也会有不同的形态。Spark 大致包括四种部署模式：</p><ul><li><b>Local 模式</b>：部署在同一个进程上，只有 Driver 角色。接受任务后创建 Driver 负责应用的调度执行，不涉及 Master 和 Worker；</li><li><b>Local-Cluster 模式</b>：部署在同一个进程上，存在 Master 和 Worker 角色，它们作为独立线程存在于这个进程内；</li><li><b>Standalone 模式</b>：Spark 真正的集群模式，在这个模式下 Master 和 Worker 是独立的进程；</li><li><b>第三方部署模式</b>：构建于 Yarn 或 Mesos 之上，由它们提供资源管理。</li></ul><p>接着看看 Spark on Yarn 对 Job 的处理过程。客户端提交一个任务给 Yarn ResourceManager 后，App Manager 接受任务并找到一个 Container 创建App Master，此时 App Master 上运行的是 Spark Driver。之后 App Master 申请 Container 并启动，Spark Driver 在 Container 上启动 Spark Executor，并调度 Spark Task 在 Spark Executor 上运行，等到所有的任务执行完毕后，向 App Manager 取消注册并释放资源。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-5f62784bb2720742f616bf9c6e372b52_b.jpg\" data-size=\"normal\" data-rawwidth=\"620\" data-rawheight=\"493\" class=\"origin_image zh-lightbox-thumb\" width=\"620\" data-original=\"https://pic3.zhimg.com/v2-5f62784bb2720742f616bf9c6e372b52_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;620&#39; height=&#39;493&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"620\" data-rawheight=\"493\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"620\" data-original=\"https://pic3.zhimg.com/v2-5f62784bb2720742f616bf9c6e372b52_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-5f62784bb2720742f616bf9c6e372b52_b.jpg\"/><figcaption>图 6，via https://www.iteblog.com/archives/1223.html</figcaption></figure><p>可以看出这个执行流程和 Yarn 对一个任务的处理过程几乎一致，不同的是在 Spark on Yarn 的 Job 处理过程中 App Master、Container 是交由 Spark 相对应的角色去处理的。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-bbe0f2c46ec8b51e2c28eafa07325edf_b.jpg\" data-size=\"normal\" data-rawwidth=\"620\" data-rawheight=\"493\" class=\"origin_image zh-lightbox-thumb\" width=\"620\" data-original=\"https://pic4.zhimg.com/v2-bbe0f2c46ec8b51e2c28eafa07325edf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;620&#39; height=&#39;493&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"620\" data-rawheight=\"493\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"620\" data-original=\"https://pic4.zhimg.com/v2-bbe0f2c46ec8b51e2c28eafa07325edf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-bbe0f2c46ec8b51e2c28eafa07325edf_b.jpg\"/><figcaption>图 7，via https://www.iteblog.com/archives/1223.html</figcaption></figure><p>Spark on Yarn 还有另外一种运行模式：Spark on Yarn-Client。不同于上述的 Spark on Yarn-Cluster，Spark on Yarn-Client 的客户端在提交完任务之后不会将 Spark Driver 托管给 Yarn，而是在客户端运行。App Master 申请完 Container 之后同样也是由 Spark Driver 去启动 Spark Executor，执行任务。</p><p>那为什么使用 Yarn 作为 Spark 的资源管理呢？我们来对比 Spark 集群模式 Standalone 和 Spark on Yarn 在资源调度能力上的区别：Spark 的 Standalone 模式只支持 FIFO 调度器，单用户串行，默认所有节点的所有资源对应用都是可用的；而 Yarn 不止支持 FIFO 的资源调度，还提供了弹性和公平的资源分配方式。</p><p>Yarn 是通过将资源分配给 queue 来进行资源分配的，每个 queue 可以设置它的资源分配方式，接着展开介绍 Yarn 的三种资源分配方式。</p><p><b>FIFO Scheduler</b></p><p>如果没有配置策略的话，所有的任务都提交到一个 default 队列，根据它们的提交顺序执行。富裕资源就执行任务，若资源不富裕就等待前面的任务执行完毕后释放资源，这就是 FIFO Scheduler 先入先出的分配方式。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-856db3c5d434f0d2f44c3bb9f205e1c4_b.jpg\" data-size=\"normal\" data-rawwidth=\"648\" data-rawheight=\"356\" class=\"origin_image zh-lightbox-thumb\" width=\"648\" data-original=\"https://pic1.zhimg.com/v2-856db3c5d434f0d2f44c3bb9f205e1c4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;648&#39; height=&#39;356&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"648\" data-rawheight=\"356\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"648\" data-original=\"https://pic1.zhimg.com/v2-856db3c5d434f0d2f44c3bb9f205e1c4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-856db3c5d434f0d2f44c3bb9f205e1c4_b.jpg\"/><figcaption>图 8，via https://blog.csdn.net/suifeng3051/article/details/49508261</figcaption></figure><p>如图 8 所示，在 Job1 提交时占用了所有的资源，不久后 Job2提交了，但是此时系统中已经没有资源可以分配给它了。加入 Job1 是一个大任务，那么 Job2 就只能等待一段很长的时间才能获得执行的资源。所以先入先出的分配方式存在一个问题就是大任务会占用很多资源，造成后面的小任务等待时间太长而饿死，因此一般不使用这个默认配置。</p><p><b>Capacity Scheduler</b></p><p>Capacity Scheduler 是一种多租户、弹性的分配方式。每个租户一个队列，每个队列可以配置能使用的资源上限与下限（譬如 50%，达到这个上限后即使其他的资源空置着，也不可使用），通过配置可以令队列至少有资源下限配置的资源可使用。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-16e61b7b9a38b83d81f1eff0b57eb4be_b.jpg\" data-size=\"normal\" data-rawwidth=\"648\" data-rawheight=\"356\" class=\"origin_image zh-lightbox-thumb\" width=\"648\" data-original=\"https://pic3.zhimg.com/v2-16e61b7b9a38b83d81f1eff0b57eb4be_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;648&#39; height=&#39;356&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"648\" data-rawheight=\"356\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"648\" data-original=\"https://pic3.zhimg.com/v2-16e61b7b9a38b83d81f1eff0b57eb4be_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-16e61b7b9a38b83d81f1eff0b57eb4be_b.jpg\"/><figcaption>图 9，via https://blog.csdn.net/suifeng3051/article/details/49508261</figcaption></figure><p>图 9 中队列 A 和队列 B 分配了相互独立的资源。Job1 提交给队列 A 执行，它只能使用队列 A 的资源。接着 Job2 提交给了队列B 就不必等待 Job1 释放资源了。这样就可以将大任务和小任务分配在两个队列中，这两个队列的资源相互独立，就不会造成小任务饿死的情况了。</p><p><b>Fair Scheduler</b></p><p>Fair Scheduler 是一种公平的分配方式，所谓的公平就是集群会<b>尽可能地按配置的比例分配</b>资源给队列。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-5e115005916822ec8c25d945ec6cb693_b.jpg\" data-size=\"normal\" data-rawwidth=\"648\" data-rawheight=\"356\" class=\"origin_image zh-lightbox-thumb\" width=\"648\" data-original=\"https://pic4.zhimg.com/v2-5e115005916822ec8c25d945ec6cb693_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;648&#39; height=&#39;356&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"648\" data-rawheight=\"356\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"648\" data-original=\"https://pic4.zhimg.com/v2-5e115005916822ec8c25d945ec6cb693_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-5e115005916822ec8c25d945ec6cb693_b.jpg\"/><figcaption>图 10，via https://blog.csdn.net/suifeng3051/article/details/49508261</figcaption></figure><p>图 10 中 Job1 提交给队列 A，它占用了集群的所有资源。接着 Job2 提交给了队列 B，这时 Job1 就需要释放它的一半的资源给队列 A 中的 Job2 使用。接着 Job3 也提交给了队列 B，这个时候 Job2 如果还未执行完毕的话也必须释放一半的资源给 Job3。这就是公平的分配方式，在队列范围内所有任务享用到的资源都是均分的。</p><h2><b>the Future of  Spark </b></h2><p>Mesos 的资源调度和 Yarn 类似，但是它提供了粗粒度和细粒度的两种模式。所谓的粗粒度和细粒度的差别在于：Executor 申请的资源是在执行前申请，还是在执行过程中按需申请。集群资源紧张时可能有一个 Executor 申请的资源在当时处于闲置状态，如果处于粗粒度模式下，这些资源在当时就浪费了。但是在细粒度模式下，Executor 执行时所需的资源是按照它的需求分配的，这样就不存在资源闲置的情况了。</p><p>因为 Mesos 的 Executor 是可以动态调整，而 Yarn 使用的 Container 是不可以动态调整的，所以目前 Yarn 是不支持细粒度的调度模式的，但 Yarn 已经有计划支持细粒度的资源管理方式。</p><p>除此之外在 Hadoop3.1.0 中 Yarn 提供了对 gpu 资源的支持，目前只支持 Nvidia gpu。期待 Spark 在其他方面的更多探索，下一篇我们将具体介绍 RDD，欢迎持续关注。</p><p></p>", 
            "topic": [
                {
                    "tag": "Hadoop", 
                    "tagLink": "https://api.zhihu.com/topics/19563390"
                }, 
                {
                    "tag": "MapReduce", 
                    "tagLink": "https://api.zhihu.com/topics/19621083"
                }, 
                {
                    "tag": "Spark", 
                    "tagLink": "https://api.zhihu.com/topics/19942170"
                }
            ], 
            "comments": [
                {
                    "userName": "Ethan楚翔", 
                    "userLink": "https://www.zhihu.com/people/d75b754b2ab16e3d90b87bb53f5899e4", 
                    "content": "<p>yarn-cluster下master和driver是一样吗?</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "大菜菜", 
                            "userLink": "https://www.zhihu.com/people/9d332a19dff96a82bb4bb9d44f612969", 
                            "content": "<p>不一样,master是一级资源调度的抽象概念,对应着yarn的RM,driver是二级调度的抽象,在yarn中更像是ApplicationManager</p>", 
                            "likes": 0, 
                            "replyToAuthor": "Ethan楚翔"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/40698254", 
            "userName": "MT酱", 
            "userLink": "https://www.zhihu.com/people/28629950b8d6033df6b6f33fd0280e2a", 
            "upvote": 11, 
            "title": "Spark Streaming VS Flink", 
            "content": "<blockquote><i>本文从编程模型、任务调度、时间机制、Kafka 动态分区的感知、容错及处理语义、背压等几个方面对比 Spark Streaming 与 Flink，希望对有实时处理需求业务的企业端用户在框架选型有所启发。本文篇幅较长，建议先收藏～微信link：</i><a href=\"https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s/jllAegJMYh_by95FhHt0jA\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Spark Streaming VS Flink</a></blockquote><h2><b>#编程模型对比</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p><b>运行角色</b></p><p>Spark Streaming 运行时的角色(standalone 模式)主要有：</p><ul><li><b>Master</b>:主要负责整体集群资源的管理和应用程序调度；</li><li><b>Worker</b>:负责单个节点的资源管理，driver 和 executor 的启动等；</li><li><b>Driver</b>:用户入口程序执行的地方，即 SparkContext 执行的地方，主要是 DAG 生成、stage 划分、task 生成及调度；</li><li><b>Executor</b>:负责执行 task，反馈执行状态和执行结果。</li></ul><p>Flink 运行时的角色(standalone 模式)主要有:</p><ul><li><b>Jobmanager</b>: 协调分布式执行，他们调度任务、协调 checkpoints、协调故障恢复等。至少有一个 JobManager。高可用情况下可以启动多个 JobManager，其中一个选举为 leader，其余为 standby；</li><li><b>Taskmanager</b>: 负责执行具体的 tasks、缓存、交换数据流，至少有一个 TaskManager；</li><li><b>Slot</b>: 每个 task slot 代表 TaskManager 的一个固定部分资源，Slot 的个数代表着 taskmanager 可并行执行的 task 数。</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p><b>生态</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-ae294c803fcf4b107783bfd14ef0f919_b.jpg\" data-size=\"normal\" data-rawwidth=\"865\" data-rawheight=\"267\" class=\"origin_image zh-lightbox-thumb\" width=\"865\" data-original=\"https://pic2.zhimg.com/v2-ae294c803fcf4b107783bfd14ef0f919_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;865&#39; height=&#39;267&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"865\" data-rawheight=\"267\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"865\" data-original=\"https://pic2.zhimg.com/v2-ae294c803fcf4b107783bfd14ef0f919_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-ae294c803fcf4b107783bfd14ef0f919_b.jpg\"/><figcaption>图 1：Spark Streaming 生态，via Spark 官网</figcaption></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-7e34c912b2ebce4674ce5f4f4064f3e7_b.jpg\" data-size=\"normal\" data-rawwidth=\"852\" data-rawheight=\"279\" class=\"origin_image zh-lightbox-thumb\" width=\"852\" data-original=\"https://pic4.zhimg.com/v2-7e34c912b2ebce4674ce5f4f4064f3e7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;852&#39; height=&#39;279&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"852\" data-rawheight=\"279\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"852\" data-original=\"https://pic4.zhimg.com/v2-7e34c912b2ebce4674ce5f4f4064f3e7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-7e34c912b2ebce4674ce5f4f4064f3e7_b.jpg\"/><figcaption>图 2：Flink 生态，via Flink官网</figcaption></figure><p><b>运行模型</b></p><p>Spark Streaming 是<b>微批处理</b>，运行的时候需要指定批处理的时间，每次运行 job 时处理一个批次的数据，流程如图 3 所示：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-39962f682cdd755f61295229b1bbdfca_b.jpg\" data-size=\"normal\" data-rawwidth=\"865\" data-rawheight=\"182\" class=\"origin_image zh-lightbox-thumb\" width=\"865\" data-original=\"https://pic3.zhimg.com/v2-39962f682cdd755f61295229b1bbdfca_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;865&#39; height=&#39;182&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"865\" data-rawheight=\"182\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"865\" data-original=\"https://pic3.zhimg.com/v2-39962f682cdd755f61295229b1bbdfca_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-39962f682cdd755f61295229b1bbdfca_b.jpg\"/><figcaption>图 3，via Spark 官网</figcaption></figure><p>Flink 是基于<b>事件</b>驱动的，事件可以理解为消息。事件驱动的应用程序是一种状态应用程序，它会从一个或者多个流中注入事件，通过触发计算更新状态，或外部动作对注入的事件作出反应。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-35bce29d91c6405c1e496dd88e836e0f_b.jpg\" data-size=\"normal\" data-rawwidth=\"827\" data-rawheight=\"313\" class=\"origin_image zh-lightbox-thumb\" width=\"827\" data-original=\"https://pic4.zhimg.com/v2-35bce29d91c6405c1e496dd88e836e0f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;827&#39; height=&#39;313&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"827\" data-rawheight=\"313\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"827\" data-original=\"https://pic4.zhimg.com/v2-35bce29d91c6405c1e496dd88e836e0f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-35bce29d91c6405c1e496dd88e836e0f_b.jpg\"/><figcaption>图 4，via Fink 官网</figcaption></figure><h2><b>#编程模型对比</b></h2><p>编程模型对比，主要是对比 flink 和 Spark Streaming 两者在代码编写上的区别。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>Spark Streaming</b></p><p>Spark Streaming 与 kafka 的结合主要是两种模型：</p><ul><li>基于 receiver dstream；</li><li>基于 direct dstream。</li></ul><p>以上两种模型编程机构近似，只是在 api 和内部数据获取有些区别，新版本的已经取消了基于 receiver 这种模式，企业中通常采用基于 direct Dstream 的模式。</p><div class=\"highlight\"><pre><code class=\"language-text\">val Array(brokers, topics) = args//    创建一个批处理时间是2s的context    \n   val sparkConf = new SparkConf().setAppName(&#34;DirectKafkaWordCount&#34;)    \n   val ssc = new StreamingContext(sparkConf, Seconds(2))    \n   //    使用broker和topic创建DirectStream    \n   val topicsSet = topics.split(&#34;,&#34;).toSet    \n   val kafkaParams = Map[String, String](&#34;metadata.broker.list&#34; -&gt; brokers)    \n   val messages = KafkaUtils.createDirectStream[String, String]( ssc, LocationStrategies.PreferConsistent,    ConsumerStrategies.Subscribe[String, String](topicsSet, kafkaParams))  \n     // Get the lines, split them into words, count the words and print    \n   val lines = messages.map(_.value)    \n   val words = lines.flatMap(_.split(&#34; &#34;))    \n   val wordCounts = words.map(x =&gt; (x, 1L)).reduceByKey(_ + _)   \n    wordCounts.print()     //    启动流    \n   ssc.start()    \n   ssc.awaitTermination()</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>通过以上代码我们可以 get 到：</p><ul><li>设置批处理时间</li><li>创建数据流</li><li>编写transform</li><li>编写action</li><li>启动执行</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p><b>Flink</b></p><p>接下来看 flink 与 kafka 结合是如何编写代码的。Flink 与 kafka 结合是事件驱动，大家可能对此会有疑问，消费 kafka 的数据调用 poll 的时候是批量获取数据的(可以设置批处理大小和超时时间)，这就不能叫做事件触发了。而实际上，flink 内部对 poll 出来的数据进行了整理，然后逐条 emit，形成了事件触发的机制。 下面的代码是 flink 整合 kafka 作为 data source 和 data sink：</p><div class=\"highlight\"><pre><code class=\"language-scala\"><span class=\"nc\">StreamExecutionEnvironment</span> <span class=\"n\">env</span> <span class=\"k\">=</span> <span class=\"nc\">StreamExecutionEnvironment</span><span class=\"o\">.</span><span class=\"n\">getExecutionEnvironment</span><span class=\"o\">();</span>\n   <span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">getConfig</span><span class=\"o\">().</span><span class=\"n\">disableSysoutLogging</span><span class=\"o\">();</span>\n   <span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">getConfig</span><span class=\"o\">().</span><span class=\"n\">setRestartStrategy</span><span class=\"o\">(</span><span class=\"nc\">RestartStrategies</span><span class=\"o\">.</span><span class=\"n\">fixedDelayRestart</span><span class=\"o\">(</span><span class=\"mi\">4</span><span class=\"o\">,</span> <span class=\"mi\">10000</span><span class=\"o\">));</span>\n   <span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">enableCheckpointing</span><span class=\"o\">(</span><span class=\"mi\">5000</span><span class=\"o\">);</span> <span class=\"c1\">// create a checkpoint every 5 seconds\n</span><span class=\"c1\"></span>   <span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">getConfig</span><span class=\"o\">().</span><span class=\"n\">setGlobalJobParameters</span><span class=\"o\">(</span><span class=\"n\">parameterTool</span><span class=\"o\">);</span> <span class=\"c1\">// make parameters available in the web interface\n</span><span class=\"c1\"></span>   <span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">setStreamTimeCharacteristic</span><span class=\"o\">(</span><span class=\"nc\">TimeCharacteristic</span><span class=\"o\">.</span><span class=\"nc\">EventTime</span><span class=\"o\">);</span>    \n   <span class=\"c1\">//    ExecutionConfig.GlobalJobParameters\n</span><span class=\"c1\"></span>   <span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">getConfig</span><span class=\"o\">().</span><span class=\"n\">setGlobalJobParameters</span><span class=\"o\">(</span><span class=\"kc\">null</span><span class=\"o\">);</span>    <span class=\"nc\">DataStream</span><span class=\"o\">&lt;</span><span class=\"nc\">KafkaEvent</span><span class=\"o\">&gt;</span> <span class=\"n\">input</span> <span class=\"k\">=</span> <span class=\"n\">env</span>\n           <span class=\"o\">.</span><span class=\"n\">addSource</span><span class=\"o\">(</span>                <span class=\"k\">new</span> <span class=\"nc\">FlinkKafkaConsumer010</span><span class=\"o\">&lt;&gt;(</span>\n                   <span class=\"n\">parameterTool</span><span class=\"o\">.</span><span class=\"n\">getRequired</span><span class=\"o\">(</span><span class=\"s\">&#34;input-topic&#34;</span><span class=\"o\">),</span>                    <span class=\"k\">new</span> <span class=\"nc\">KafkaEventSchema</span><span class=\"o\">(),</span>\n                   <span class=\"n\">parameterTool</span><span class=\"o\">.</span><span class=\"n\">getProperties</span><span class=\"o\">())</span>\n               <span class=\"o\">.</span><span class=\"n\">assignTimestampsAndWatermarks</span><span class=\"o\">(</span><span class=\"k\">new</span> <span class=\"nc\">CustomWatermarkExtractor</span><span class=\"o\">())).</span><span class=\"n\">setParallelism</span><span class=\"o\">(</span><span class=\"mi\">1</span><span class=\"o\">).</span><span class=\"n\">rebalance</span><span class=\"o\">()</span>\n           <span class=\"o\">.</span><span class=\"n\">keyBy</span><span class=\"o\">(</span><span class=\"s\">&#34;word&#34;</span><span class=\"o\">)</span>\n           <span class=\"o\">.</span><span class=\"n\">map</span><span class=\"o\">(</span><span class=\"k\">new</span> <span class=\"nc\">RollingAdditionMapper</span><span class=\"o\">()).</span><span class=\"n\">setParallelism</span><span class=\"o\">(</span><span class=\"mi\">0</span><span class=\"o\">);</span>\n   \n   <span class=\"n\">input</span><span class=\"o\">.</span><span class=\"n\">addSink</span><span class=\"o\">(</span>            <span class=\"k\">new</span> <span class=\"nc\">FlinkKafkaProducer010</span><span class=\"o\">&lt;&gt;(</span>\n                   <span class=\"n\">parameterTool</span><span class=\"o\">.</span><span class=\"n\">getRequired</span><span class=\"o\">(</span><span class=\"s\">&#34;output-topic&#34;</span><span class=\"o\">),</span>                    <span class=\"k\">new</span> <span class=\"nc\">KafkaEventSchema</span><span class=\"o\">(),</span>\n                   <span class=\"n\">parameterTool</span><span class=\"o\">.</span><span class=\"n\">getProperties</span><span class=\"o\">()));</span>\n   \n   <span class=\"n\">env</span><span class=\"o\">.</span><span class=\"n\">execute</span><span class=\"o\">(</span><span class=\"s\">&#34;Kafka 0.10 Example&#34;</span><span class=\"o\">);</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>从 Flink 与 kafka 结合的代码可以 get 到：</p><ul><li>注册数据 source</li><li>编写运行逻辑</li><li>注册数据 sink</li><li>调用 env.execute 相比于 Spark Streaming 少了设置批处理时间，还有一个显著的区别是 flink 的所有算子都是 lazy 形式的，调用 env.execute 会构建 jobgraph。client 端负责 Jobgraph 生成并提交它到集群运行；而 Spark Streaming的操作算子分 action 和 transform，其中仅有 transform 是 lazy 形式，而且 DAG 生成、stage 划分、任务调度是在 driver 端进行的，在 client 模式下 driver 运行于客户端处。</li></ul><h2><b>#任务调度原理</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p><b>Spark 任务调度</b></p><p>Spark Streaming 任务如上文提到的是基于微批处理的，实际上每个批次都是一个 Spark Core 的任务。对于编码完成的 Spark Core 任务在生成到最终执行结束主要包括以下几个部分：</p><ul><li>构建 DAG 图；</li><li>划分 stage；</li><li>生成 taskset；</li><li>调度 task。</li></ul><p>具体可参考图 5：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-c1896e93fd44cc86d748d16a667d2cf9_b.jpg\" data-size=\"normal\" data-rawwidth=\"599\" data-rawheight=\"321\" class=\"origin_image zh-lightbox-thumb\" width=\"599\" data-original=\"https://pic2.zhimg.com/v2-c1896e93fd44cc86d748d16a667d2cf9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;599&#39; height=&#39;321&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"599\" data-rawheight=\"321\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"599\" data-original=\"https://pic2.zhimg.com/v2-c1896e93fd44cc86d748d16a667d2cf9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-c1896e93fd44cc86d748d16a667d2cf9_b.jpg\"/><figcaption>图 5：Spark 任务调度</figcaption></figure><p>对于 job 的调度执行有 fifo 和 fair 两种模式，Task 是根据<b>数据本地性</b>调度执行的。 假设每个 Spark Streaming 任务消费的 kafka topic 有四个分区，中间有一个 transform操作（如 map）和一个 reduce 操作，如图 6 所示：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-c7ba6fba5a0b842377d1048ec31c440f_b.jpg\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"467\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-c7ba6fba5a0b842377d1048ec31c440f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;467&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"467\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-c7ba6fba5a0b842377d1048ec31c440f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-c7ba6fba5a0b842377d1048ec31c440f_b.jpg\"/><figcaption>图 6 </figcaption></figure><p>假设有两个 executor，其中每个 executor 三个核，那么每个批次相应的 task 运行位置是固定的吗？是否能预测？ 由于数据本地性和调度不确定性，每个批次对应 kafka 分区生成的 task 运行位置并不是固定的。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>Flink 任务调度</b></p><p>对于 flink 的流任务客户端首先会生成 StreamGraph，接着生成 JobGraph，然后将 jobGraph 提交给 Jobmanager 由它完成 jobGraph 到 ExecutionGraph 的转变，最后由 jobManager 调度执行。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-905a0152ecec0d0ee47a5f5885aa2e12_b.jpg\" data-size=\"normal\" data-rawwidth=\"865\" data-rawheight=\"524\" class=\"origin_image zh-lightbox-thumb\" width=\"865\" data-original=\"https://pic3.zhimg.com/v2-905a0152ecec0d0ee47a5f5885aa2e12_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;865&#39; height=&#39;524&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"865\" data-rawheight=\"524\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"865\" data-original=\"https://pic3.zhimg.com/v2-905a0152ecec0d0ee47a5f5885aa2e12_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-905a0152ecec0d0ee47a5f5885aa2e12_b.jpg\"/><figcaption>图 7</figcaption></figure><p>如图 7 所示有一个由 data source、MapFunction和 ReduceFunction 组成的程序，data source 和 MapFunction 的并发度都为 4，而 ReduceFunction 的并发度为 3。一个数据流由 Source-Map-Reduce 的顺序组成，在具有 2 个TaskManager、每个 TaskManager 都有 3 个 Task Slot 的集群上运行。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>可以看出 flink 的拓扑生成提交执行之后，除非故障，否则拓扑部件执行位置不变，并行度由每一个算子并行度决定，类似于 storm。而 spark Streaming 是每个批次都会根据数据本地性和资源情况进行调度，无固定的执行拓扑结构。 flink 是数据在拓扑结构里流动执行，而 Spark Streaming 则是对数据缓存批次并行处理。</p><h2><b>#时间机制对比</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p><b>流处理的时间</b></p><p>流处理程序在时间概念上总共有三个时间概念：</p><ul><li><b>处理时间</b></li></ul><p>处理时间是指<b>每台机器的系统时间</b>，当流程序采用处理时间时将使用运行各个运算符实例的机器时间。处理时间是最简单的时间概念，不需要流和机器之间的协调，它能提供最好的性能和最低延迟。然而在分布式和异步环境中，处理时间不能提供消息事件的时序性保证，因为它受到消息传输延迟，消息在算子之间流动的速度等方面制约。</p><ul><li><b>事件时间</b></li></ul><p>事件时间是指事件在其<b>设备上发生的时间</b>，这个时间在事件进入 flink 之前已经嵌入事件，然后 flink 可以提取该时间。基于事件时间进行处理的流程序可以保证事件在处理的时候的顺序性，但是基于事件时间的应用程序必须要结合 watermark 机制。基于事件时间的处理往往有一定的滞后性，因为它需要等待后续事件和处理无序事件，对于时间敏感的应用使用的时候要慎重考虑。</p><ul><li><b>注入时间</b></li></ul><p>注入时间是<b>事件注入到 flink 的时间</b>。事件在 source 算子处获取 source 的当前时间作为事件注入时间，后续的基于时间的处理算子会使用该时间处理数据。</p><p>相比于事件时间，注入时间不能够处理无序事件或者滞后事件，但是应用程序无序指定如何生成 watermark。在内部注入时间程序的处理和事件时间类似，但是时间戳分配和 watermark 生成都是自动的。</p><p>图 8 可以清晰地看出三种时间的区别：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-f66b5e339e2eb3ff506724f27b2a3f19_b.jpg\" data-size=\"normal\" data-rawwidth=\"865\" data-rawheight=\"390\" class=\"origin_image zh-lightbox-thumb\" width=\"865\" data-original=\"https://pic2.zhimg.com/v2-f66b5e339e2eb3ff506724f27b2a3f19_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;865&#39; height=&#39;390&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"865\" data-rawheight=\"390\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"865\" data-original=\"https://pic2.zhimg.com/v2-f66b5e339e2eb3ff506724f27b2a3f19_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-f66b5e339e2eb3ff506724f27b2a3f19_b.jpg\"/><figcaption>图 8</figcaption></figure><p><b>Spark 时间机制</b></p><p>Spark Streaming 只支持处理时间，Structured streaming 支持处理时间和事件时间，同时支持 watermark 机制处理滞后数据。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>Flink 时间机制</b></p><p>flink 支持三种时间机制：事件时间，注入时间，处理时间，同时支持 watermark 机制处理滞后数据。</p><h2><b>#kafka 动态分区检测 </b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p><b>Spark Streaming</b></p><p>对于有实时处理业务需求的企业，随着业务增长数据量也会同步增长，将导致原有的 kafka 分区数不满足数据写入所需的并发度，需要扩展 kafka 的分区或者增加 kafka 的 topic，这时就要求实时处理程序，如 SparkStreaming、flink 能检测到 kafka 新增的 topic 、分区及消费新增分区的数据。</p><p>接下来结合源码分析，Spark Streaming 和 flink 在 kafka 新增 topic 或 partition 时能否动态发现新增分区并消费处理新增分区的数据。 Spark Streaming 与 kafka 结合有两个区别比较大的版本，如图 9 所示是官网给出的对比数据：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-b2f982a551e247f4bcab023bbf96441f_b.jpg\" data-size=\"normal\" data-rawwidth=\"949\" data-rawheight=\"401\" class=\"origin_image zh-lightbox-thumb\" width=\"949\" data-original=\"https://pic4.zhimg.com/v2-b2f982a551e247f4bcab023bbf96441f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;949&#39; height=&#39;401&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"949\" data-rawheight=\"401\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"949\" data-original=\"https://pic4.zhimg.com/v2-b2f982a551e247f4bcab023bbf96441f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-b2f982a551e247f4bcab023bbf96441f_b.jpg\"/><figcaption>图 9</figcaption></figure><p>其中确认的是 Spark Streaming 与 kafka 0.8 版本结合不支持动态分区检测，与 0.10 版本结合支持，接着通过源码分析。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>Spark Streaming 与 kafka 0.8 版本结合</b></p><p>*源码分析只针对分区检测</p><p>入口是 DirectKafkaInputDStream 的 compute：</p><div class=\"highlight\"><pre><code class=\"language-text\">override def compute(validTime: Time): Option[KafkaRDD[K, V, U, T, R]] = {//    改行代码会计算这个job，要消费的每个kafka分区的最大偏移\n   val untilOffsets = clamp(latestLeaderOffsets(maxRetries))//    构建KafkaRDD，用指定的分区数和要消费的offset范围\n   val rdd = KafkaRDD[K, V, U, T, R](\n     context.sparkContext, kafkaParams, currentOffsets, untilOffsets, messageHandler)    // Report the record number and metadata of this batch interval to InputInfoTracker.\n   val offsetRanges = currentOffsets.map { case (tp, fo) =&gt;\n     val uo = untilOffsets(tp)      OffsetRange(tp.topic, tp.partition, fo, uo.offset)\n   }    val description = offsetRanges.filter { offsetRange =&gt;\n     // Don&#39;t display empty ranges.\n     offsetRange.fromOffset != offsetRange.untilOffset\n   }.map { offsetRange =&gt;\n     s&#34;topic: ${offsetRange.topic}\\tpartition: ${offsetRange.partition}\\t&#34; +\n       s&#34;offsets: ${offsetRange.fromOffset} to ${offsetRange.untilOffset}&#34;\n   }.mkString(&#34;\\n&#34;)    // Copy offsetRanges to immutable.List to prevent from being modified by the user\n   val metadata = Map(      &#34;offsets&#34; -&gt; offsetRanges.toList,      StreamInputInfo.METADATA_KEY_DESCRIPTION -&gt; description)    val inputInfo = StreamInputInfo(id, rdd.count, metadata)\n   ssc.scheduler.inputInfoTracker.reportInfo(validTime, inputInfo)\n\n   currentOffsets = untilOffsets.map(kv =&gt; kv._1 -&gt; kv._2.offset)    Some(rdd)\n }</code></pre></div><p>第一行就是计算得到该批次生成 KafkaRDD 每个分区要消费的最大 offset。 接着看 latestLeaderOffsets(maxRetries)</p><div class=\"highlight\"><pre><code class=\"language-text\">@tailrec  protected final def latestLeaderOffsets(retries: Int): Map[TopicAndPartition, LeaderOffset] = {//    可以看到的是用来指定获取最大偏移分区的列表还是只有currentOffsets，没有发现关于新增的分区的内容。\n   val o = kc.getLatestLeaderOffsets(currentOffsets.keySet)    // Either.fold would confuse @tailrec, do it manually\n   if (o.isLeft) {      val err = o.left.get.toString      if (retries &lt;= 0) {        throw new SparkException(err)\n     } else {\n       logError(err)        Thread.sleep(kc.config.refreshLeaderBackoffMs)\n       latestLeaderOffsets(retries - 1)\n     }\n   } else {\n     o.right.get\n   }\n }</code></pre></div><p>其中 <code>protected var currentOffsets = fromOffsets</code>，这个仅仅是在构建 DirectKafkaInputDStream 的时候初始化，并在 compute 里面更新：</p><div class=\"highlight\"><pre><code class=\"language-text\">currentOffsets = untilOffsets.map(kv =&gt; kv._1 -&gt; kv._2.offset)</code></pre></div><p>中间没有检测 kafka 新增 topic 或者分区的代码，所以可以确认 Spark Streaming 与 kafka 0.8 的版本结合不支持动态分区检测。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>Spark Streaming 与 kafka 0.10 版本结合</b></p><p>入口同样是 DirectKafkaInputDStream 的 compute 方法，捡主要的部分说，Compute 里第一行也是计算当前 job 生成 kafkardd 要消费的每个分区的最大 offset：</p><div class=\"highlight\"><pre><code class=\"language-text\">//    获取当前生成job，要用到的KafkaRDD每个分区最大消费偏移值\n   val untilOffsets = clamp(latestOffsets())</code></pre></div><p>具体检测 kafka 新增 topic 或者分区的代码在 <code>latestOffsets()</code></p><div class=\"highlight\"><pre><code class=\"language-text\">/**   \n* Returns the latest (highest) available offsets, taking new partitions into account.   */\n protected def latestOffsets(): Map[TopicPartition, Long] = {    val c = consumer\n   paranoidPoll(c)    // 获取所有的分区信息\n   val parts = c.assignment().asScala    // make sure new partitions are reflected in currentOffsets\n   // 做差获取新增的分区信息\n   val newPartitions = parts.diff(currentOffsets.keySet)    // position for new partitions determined by auto.offset.reset if no commit\n   // 新分区消费位置，没有记录的化是由auto.offset.reset决定\n   currentOffsets = currentOffsets ++ newPartitions.map(tp =&gt; tp -&gt; c.position(tp)).toMap    // don&#39;t want to consume messages, so pause\n   c.pause(newPartitions.asJava)    // find latest available offsets\n   c.seekToEnd(currentOffsets.keySet.asJava)\n   parts.map(tp =&gt; tp -&gt; c.position(tp)).toMap\n }</code></pre></div><p>该方法内有获取 kafka 新增分区，并将其更新到 currentOffsets 的过程，所以可以验证 Spark Streaming 与 kafka 0.10 版本结合支持动态分区检测。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>Flink</b></p><p>入口类是 FlinkKafkaConsumerBase，该类是所有 flink 的 kafka 消费者的父类。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-464dc72e929ddb03f056950515c6fd9f_b.jpg\" data-size=\"normal\" data-rawwidth=\"865\" data-rawheight=\"357\" class=\"origin_image zh-lightbox-thumb\" width=\"865\" data-original=\"https://pic4.zhimg.com/v2-464dc72e929ddb03f056950515c6fd9f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;865&#39; height=&#39;357&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"865\" data-rawheight=\"357\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"865\" data-original=\"https://pic4.zhimg.com/v2-464dc72e929ddb03f056950515c6fd9f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-464dc72e929ddb03f056950515c6fd9f_b.jpg\"/><figcaption>图 10</figcaption></figure><p>在 FlinkKafkaConsumerBase 的 run 方法中，创建了 kafkaFetcher，实际上就是消费者：</p><div class=\"highlight\"><pre><code class=\"language-text\">this.kafkaFetcher = createFetcher(\n       sourceContext,\n       subscribedPartitionsToStartOffsets,\n       periodicWatermarkAssigner,\n       punctuatedWatermarkAssigner,\n       (StreamingRuntimeContext) getRuntimeContext(),\n       offsetCommitMode,\n       getRuntimeContext().getMetricGroup().addGroup(KAFKA_CONSUMER_METRICS_GROUP),\n       useMetrics);</code></pre></div><p>接是创建了一个线程，该线程会定期检测 kafka 新增分区，然后将其添加到 kafkaFetcher 里。</p><div class=\"highlight\"><pre><code class=\"language-text\">if (discoveryIntervalMillis != PARTITION_DISCOVERY_DISABLED) {      final AtomicReference&lt;Exception&gt; discoveryLoopErrorRef = new AtomicReference&lt;&gt;();      this.discoveryLoopThread = new Thread(new Runnable() {        @Override\n       public void run() {          try {            // --------------------- partition discovery loop ---------------------\n\n           List&lt;KafkaTopicPartition&gt; discoveredPartitions;            // throughout the loop, we always eagerly check if we are still running before\n           // performing the next operation, so that we can escape the loop as soon as possible\n\n           while (running) {              if (LOG.isDebugEnabled()) {                LOG.debug(&#34;Consumer subtask {} is trying to discover new partitions ...&#34;, getRuntimeContext().getIndexOfThisSubtask());\n             }              try {\n               discoveredPartitions = partitionDiscoverer.discoverPartitions();\n             } catch (AbstractPartitionDiscoverer.WakeupException | AbstractPartitionDiscoverer.ClosedException e) {                // the partition discoverer may have been closed or woken up before or during the discovery;\n               // this would only happen if the consumer was canceled; simply escape the loop\n               break;\n             }              // no need to add the discovered partitions if we were closed during the meantime\n             if (running &amp;&amp; !discoveredPartitions.isEmpty()) {\n               kafkaFetcher.addDiscoveredPartitions(discoveredPartitions);\n             }              // do not waste any time sleeping if we&#39;re not running anymore\n             if (running &amp;&amp; discoveryIntervalMillis != 0) {                try {                  Thread.sleep(discoveryIntervalMillis);\n               } catch (InterruptedException iex) {                  // may be interrupted if the consumer was canceled midway; simply escape the loop\n                 break;\n               }\n             }\n           }\n         } catch (Exception e) {\n           discoveryLoopErrorRef.set(e);\n         } finally {            // calling cancel will also let the fetcher loop escape\n           // (if not running, cancel() was already called)\n           if (running) {\n             cancel();\n           }\n         }\n       }\n     }, &#34;Kafka Partition Discovery for &#34; + getRuntimeContext().getTaskNameWithSubtasks());\n\n     discoveryLoopThread.start();\n     kafkaFetcher.runFetchLoop();</code></pre></div><p>上面就是 flink 动态发现 kafka 新增分区的过程。不过与 Spark 无需做任何配置不同的是，flink 动态发现 kafka 新增分区，这个功能需要被使能的。也很简单，需要将 flink.partition-discovery.interval-millis 该属性设置为大于 0 即可。</p><h2><b>#容错机制及处理语义</b></h2><p>本节内容主要是想对比两者在故障恢复及如何保证仅一次的处理语义。这个时候适合抛出一个问题：实时处理的时候，如何保证数据仅一次处理语义？</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>Spark Streaming 保证仅一次处理</b></p><p>对于 Spark Streaming 任务，我们可以设置 checkpoint，然后假如发生故障并重启，我们可以从上次 checkpoint 之处恢复，但是这个行为只能使得数据不丢失，可能会重复处理，不能做到恰一次处理语义。</p><p>对于 Spark Streaming 与 kafka 结合的 direct Stream 可以自己维护 offset 到 zookeeper、kafka 或任何其它外部系统，每次提交完结果之后再提交 offset，这样故障恢复重启可以利用上次提交的 offset 恢复，保证数据不丢失。但是假如故障发生在提交结果之后、提交 offset 之前会导致数据多次处理，这个时候我们需要保证处理结果多次输出不影响正常的业务。</p><p>由此可以分析，假设要保证数据恰一次处理语义，那么结果输出和 offset 提交必须在一个事务内完成。在这里有以下两种做法：</p><ul><li>repartition(1) Spark Streaming 输出的 action 变成仅一个 partition，这样可以利用事务去做：</li></ul><div class=\"highlight\"><pre><code class=\"language-text\">Dstream.foreachRDD(rdd=&gt;{\n   rdd.repartition(1).foreachPartition(partition=&gt;{    //    开启事务\n       partition.foreach(each=&gt;{//        提交数据\n       })    //  提交事务\n   })\n })</code></pre></div><ul><li>将结果和 offset 一起提交</li></ul><p>也就是结果数据包含 offset。这样提交结果和提交 offset 就是一个操作完成，不会数据丢失，也不会重复处理。故障恢复的时候可以利用上次提交结果带的 offset。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>Flink 与 kafka 0.11 保证仅一次处理</b></p><p>若要 sink 支持仅一次语义，必须以事务的方式写数据到 Kafka，这样当提交事务时两次 checkpoint 间的所有写入操作作为一个事务被提交。这确保了出现故障或崩溃时这些写入操作能够被回滚。</p><p>在一个分布式且含有多个并发执行 sink 的应用中，仅仅执行单次提交或回滚是不够的，因为所有组件都必须对这些提交或回滚达成共识，这样才能保证得到一致性的结果。Flink 使用两阶段提交协议以及预提交(pre-commit)阶段来解决这个问题。</p><p>本例中的 Flink 应用如图 11 所示包含以下组件：</p><ul><li>一个source，从Kafka中读取数据（即KafkaConsumer）</li><li>一个时间窗口化的聚会操作</li><li>一个sink，将结果写回到Kafka（即KafkaProducer）</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-6ddb9500ecbfe0162750902478dec4d7_b.jpg\" data-size=\"normal\" data-rawwidth=\"865\" data-rawheight=\"447\" class=\"origin_image zh-lightbox-thumb\" width=\"865\" data-original=\"https://pic4.zhimg.com/v2-6ddb9500ecbfe0162750902478dec4d7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;865&#39; height=&#39;447&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"865\" data-rawheight=\"447\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"865\" data-original=\"https://pic4.zhimg.com/v2-6ddb9500ecbfe0162750902478dec4d7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-6ddb9500ecbfe0162750902478dec4d7_b.jpg\"/><figcaption>图 11</figcaption></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>下面详细讲解 flink 的两段提交思路：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-e1d8e4f58b402e81785682baae9df647_b.jpg\" data-size=\"normal\" data-rawwidth=\"865\" data-rawheight=\"416\" class=\"origin_image zh-lightbox-thumb\" width=\"865\" data-original=\"https://pic4.zhimg.com/v2-e1d8e4f58b402e81785682baae9df647_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;865&#39; height=&#39;416&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"865\" data-rawheight=\"416\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"865\" data-original=\"https://pic4.zhimg.com/v2-e1d8e4f58b402e81785682baae9df647_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-e1d8e4f58b402e81785682baae9df647_b.jpg\"/><figcaption>图 12</figcaption></figure><p>如图 12 所示，Flink checkpointing 开始时便进入到 pre-commit 阶段。具体来说，一旦 checkpoint 开始，Flink 的 JobManager 向输入流中写入一个 checkpoint barrier ，将流中所有消息分割成属于本次 checkpoint 的消息以及属于下次 checkpoint 的，barrier 也会在操作算子间流转。对于每个 operator 来说，该 barrier 会触发 operator 状态后端为该 operator 状态打快照。data source 保存了 Kafka 的 offset，之后把 checkpoint barrier 传递到后续的 operator。</p><p>这种方式仅适用于 operator 仅有它的内部状态。内部状态是指 Flink state backends 保存和管理的内容（如第二个 operator 中 window 聚合算出来的 sum）。</p><p>当一个进程仅有它的内部状态的时候，除了在 checkpoint 之前将需要将数据更改写入到 state backend，不需要在预提交阶段做其他的动作。在 checkpoint 成功的时候，Flink 会正确的提交这些写入，在 checkpoint 失败的时候会终止提交，过程可见图 13。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-5bdfea49d752c7d6fcf264f18c96dbb0_b.jpg\" data-size=\"normal\" data-rawwidth=\"865\" data-rawheight=\"468\" class=\"origin_image zh-lightbox-thumb\" width=\"865\" data-original=\"https://pic1.zhimg.com/v2-5bdfea49d752c7d6fcf264f18c96dbb0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;865&#39; height=&#39;468&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"865\" data-rawheight=\"468\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"865\" data-original=\"https://pic1.zhimg.com/v2-5bdfea49d752c7d6fcf264f18c96dbb0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-5bdfea49d752c7d6fcf264f18c96dbb0_b.jpg\"/><figcaption>图 13</figcaption></figure><p>当结合外部系统的时候，外部系统必须要支持可与两阶段提交协议捆绑使用的事务。显然本例中的 sink 由于引入了 kafka sink，因此在预提交阶段 data sink 必须预提交外部事务。如下图：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-21006c748b180b34883a87518074e946_b.jpg\" data-size=\"normal\" data-rawwidth=\"865\" data-rawheight=\"446\" class=\"origin_image zh-lightbox-thumb\" width=\"865\" data-original=\"https://pic3.zhimg.com/v2-21006c748b180b34883a87518074e946_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;865&#39; height=&#39;446&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"865\" data-rawheight=\"446\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"865\" data-original=\"https://pic3.zhimg.com/v2-21006c748b180b34883a87518074e946_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-21006c748b180b34883a87518074e946_b.jpg\"/><figcaption>图 14</figcaption></figure><p>当 barrier 在所有的算子中传递一遍，并且触发的快照写入完成，预提交阶段完成。所有的触发状态快照都被视为 checkpoint 的一部分，也可以说 checkpoint 是整个应用程序的状态快照，包括预提交外部状态。出现故障可以从 checkpoint 恢复。下一步就是通知所有的操作算子 checkpoint 成功。该阶段 jobmanager 会为每个 operator 发起 checkpoint 已完成的回调逻辑。</p><p>本例中 data source 和窗口操作无外部状态，因此该阶段，这两个算子无需执行任何逻辑，但是 data sink 是有外部状态的，因此，此时我们必须提交外部事务，如下图：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-c25efe242c955d1cd3d1a6560eb7fe97_b.jpg\" data-size=\"normal\" data-rawwidth=\"865\" data-rawheight=\"399\" class=\"origin_image zh-lightbox-thumb\" width=\"865\" data-original=\"https://pic4.zhimg.com/v2-c25efe242c955d1cd3d1a6560eb7fe97_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;865&#39; height=&#39;399&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"865\" data-rawheight=\"399\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"865\" data-original=\"https://pic4.zhimg.com/v2-c25efe242c955d1cd3d1a6560eb7fe97_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-c25efe242c955d1cd3d1a6560eb7fe97_b.jpg\"/><figcaption>图 15</figcaption></figure><p>以上就是 flink 实现恰一次处理的基本逻辑。</p><h2><b>#Back pressure </b></h2><p>消费者消费的速度低于生产者生产的速度，为了使应用正常，消费者会反馈给生产者来调节生产者生产的速度，以使得消费者需要多少，生产者生产多少。</p><blockquote>*back pressure 后面一律称为背压。</blockquote><p class=\"ztext-empty-paragraph\"><br/></p><p><b>Spark Streaming 的背压</b><br/>Spark Streaming 跟 kafka 结合是存在背压机制的，目标是根据当前 job 的处理情况来调节后续批次的获取 kafka 消息的条数。为了达到这个目的，Spark Streaming 在原有的架构上加入了一个 RateController，利用的算法是 PID，需要的反馈数据是任务处理的结束时间、调度时间、处理时间、消息条数，这些数据是通过 SparkListener 体系获得，然后通过 PIDRateEsimator 的 compute 计算得到一个速率，进而可以计算得到一个 offset，然后跟限速设置最大消费条数比较得到一个最终要消费的消息最大 offset。</p><p>PIDRateEsimator 的 compute 方法如下：</p><div class=\"highlight\"><pre><code class=\"language-text\">def compute(      time: Long, // in milliseconds\n     numElements: Long,      processingDelay: Long, // in milliseconds\n     schedulingDelay: Long // in milliseconds\n   ): Option[Double] = {\n   logTrace(s&#34;\\ntime = $time, # records = $numElements, &#34; +\n     s&#34;processing time = $processingDelay, scheduling delay = $schedulingDelay&#34;)    this.synchronized {      if (time &gt; latestTime &amp;&amp; numElements &gt; 0 &amp;&amp; processingDelay &gt; 0) {        val delaySinceUpdate = (time - latestTime).toDouble / 1000\n\n       val processingRate = numElements.toDouble / processingDelay * 1000\n\n       val error = latestRate - processingRate        val historicalError = schedulingDelay.toDouble * processingRate / batchIntervalMillis        // in elements/(second ^ 2)\n       val dError = (error - latestError) / delaySinceUpdate        val newRate = (latestRate - proportional * error -\n                                   integral * historicalError -\n                                   derivative * dError).max(minRate)\n       logTrace(s&#34;&#34;&#34;            | latestRate = $latestRate, error = $error            | latestError = $latestError, historicalError = $historicalError            | delaySinceUpdate = $delaySinceUpdate, dError = $dError            &#34;&#34;&#34;.stripMargin)\n\n       latestTime = time        if (firstRun) {\n         latestRate = processingRate\n         latestError = 0D\n         firstRun = false\n         logTrace(&#34;First run, rate estimation skipped&#34;)          None\n       } else {\n         latestRate = newRate\n         latestError = error\n         logTrace(s&#34;New rate = $newRate&#34;)          Some(newRate)\n       }\n     } else {\n       logTrace(&#34;Rate estimation skipped&#34;)        None\n     }\n   }\n }</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><b>Flink 的背压</b></p><p>与 Spark Streaming 的背压不同的是，Flink 背压是 jobmanager 针对每一个 task 每 50ms 触发 100 次 Thread.getStackTrace() 调用，求出阻塞的占比。过程如图 16 所示：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-14c45bea98727bd6358e3c89412d4098_b.jpg\" data-size=\"normal\" data-rawwidth=\"865\" data-rawheight=\"345\" class=\"origin_image zh-lightbox-thumb\" width=\"865\" data-original=\"https://pic1.zhimg.com/v2-14c45bea98727bd6358e3c89412d4098_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;865&#39; height=&#39;345&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"865\" data-rawheight=\"345\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"865\" data-original=\"https://pic1.zhimg.com/v2-14c45bea98727bd6358e3c89412d4098_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-14c45bea98727bd6358e3c89412d4098_b.jpg\"/><figcaption>图 16</figcaption></figure><p>阻塞占比在 web 上划分了三个等级：</p><ul><li><b>OK</b>: 0 &lt;= Ratio &lt;= 0.10，表示状态良好；</li><li><b>LOW</b>: 0.10 &lt; Ratio &lt;= 0.5，表示有待观察；</li><li><b>HIGH</b>: 0.5 &lt; Ratio &lt;= 1，表示要处理了。</li></ul><p>（完）</p><h2><b>深圳地区福利！</b></h2><p>美图的大数据团队在近几年的发展中，逐步演进和发展出美图的大数据体系，以及在美图业务场景下数据技术应用的最佳实践。8月11日我们将在深圳开设一场技术沙龙，我们邀请了来自美图公司的大数据负责人、架构师、魅族公司的数据技术专家、以及来自 Apache kylin 的 PMC。美图的大数据技术负责人和架构师会为大家分享美图在大数据技术上的探索、大数据的架构、以及数据技术的应用落地。魅族的数据技术专家会为大家介绍魅族的 DMP 系统的架构设计以及系统的演进历程。来自 Apache 顶级项目 kylin 的架构师会为大家分享 kylin 的技术原理与应用实践。四位技术专家会从多个角度不同层次，为大家分享各自在大数据技术上的实践经验。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>详情见报名链接：<a href=\"https://link.zhihu.com/?target=https%3A//www.bagevent.com/event/1106376%3Fpreview%3D1\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">大数据架构与数据技术应用实践 -百格活动</a></p>", 
            "topic": [
                {
                    "tag": "Spark", 
                    "tagLink": "https://api.zhihu.com/topics/19942170"
                }, 
                {
                    "tag": "Flink", 
                    "tagLink": "https://api.zhihu.com/topics/20043072"
                }, 
                {
                    "tag": "大数据", 
                    "tagLink": "https://api.zhihu.com/topics/19740929"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/40340710", 
            "userName": "MT酱", 
            "userLink": "https://www.zhihu.com/people/28629950b8d6033df6b6f33fd0280e2a", 
            "upvote": 8, 
            "title": "Hello Spark!", 
            "content": "<p>欢迎阅读美图数据技术团队的「Spark，从入门到精通」系列文章，本系列文章将由浅入深为大家介绍 Spark，从框架入门到底层架构的实现，相信总有一种姿势适合你，欢迎大家持续关注：）</p><h2><b>什么是Spark？</b></h2><p>Spark 是 UC Berkeley AMP lab 所开源的类 Hadoop MapReduce 的通用并行框架，是专为大规模数据处理而设计的快速通用的大数据处理引擎及轻量级的大数据处理统一平台。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>当我们在谈 Spark 的时候可能是指一个 Spark 应用程序，替代 MapReduce 运行在 Yarn上，存储在 HDFS 上的一个大数据批处理程序；也可能是指使用包含 Spark sql、Spark streaming 等子项目；甚至 Tachyon、Mesos 等大数据处理的统一平台，或者称为 Spark 生态。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-871ae18c969e8b93c7e1bd7a68b2bceb_b.jpg\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"590\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-871ae18c969e8b93c7e1bd7a68b2bceb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;590&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"590\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-871ae18c969e8b93c7e1bd7a68b2bceb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-871ae18c969e8b93c7e1bd7a68b2bceb_b.jpg\"/><figcaption>图 1</figcaption></figure><p>发展至今，Spark 已不仅仅是 MapReduce 的替换方案，它已经发出成了一个包含众多子项目的 Spark 生态。如图 1 所示，Spark 生态可分为四层：</p><ul><li>数据存储层，以 HDFS 、Tachyon 为代表的一些分布式文件存储系统或各种数据库；</li><li>资源管理层，Yarn、Mesos 等资源管理器；</li><li>数据处理引擎；</li><li>应用层，以 Spark 为基础产生的众多项目;<br/></li></ul><p>Spark SQL 提供 HiveQL（通过 Apache Hive 的 SQL 变体 Hive 查询语言）与Spark 进行交互的 API。每个数据库表被当做一个 RDD，Spark SQL 查询被转换为 Spark 操作。Spark Streaming 对实时数据流进行处理和控制，它允许程序能够像普通 RDD 一样处理实时数据。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>接下来的系列文章将会详细介绍 Spark 生态中的其他模块与各个子项目，接下来将通过与 MapReduce 的对比来介绍数据处理引擎Spark的特点及其原理。</p><h2><b>Spark的特点</b></h2><p>根据谷歌和百度的搜索结果显示，Spark 的搜索趋势已与 Hadoop 持平甚至赶超，标志着 Spark 已经成为计算部分的事实标准，也就是说大数据技术绕不开 Spark 了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在大数据的存储、计算、资源调度中，Spark 主要解决计算问题，即主要替代 Mapreduce 的功能，底层存储和资源调度很多公司仍然选择使用 HDFS、Yarn 来承载。为什么众多企业在 Hadoop 生态框架里都选择用 Spark 作为处理引擎？让我们仔细看看它有什么特点。</p><p><b>1.速度快</b>。Spark 基于内存进行计算（ 也有部分计算基于磁盘） ； </p><p><b>2.容易上手开发</b>。 Spark 基于 RDD 的计算模型， 比 Hadoop 基于 Map-Reduce 的计算模型要更易于理解、易于上手开发实现各种复杂功能，如二次排序、 topN 等复杂操作时更加便捷。；</p><p><b>3.超强的通用性</b>。 Spark 提供了 Spark RDD、 Spark SQL、 Spark Streaming、 Spark MLlib、 Spark GraphX 等技术组件， 可以一站式地完成大数据领域的离线批处理、 交互式查询、 流式计算、 机器学习、图计算等常见的任务；</p><p><b>4.集成 Hadoop</b>。 Spark 可以完美集成 Hadoop。 Hadoop 的 HDFS、 Hive、HBase 负责存储， Yarn 负责资源调度， Spark 负责大数据计算是比较流行的大数据解决方案。 </p><p><b>4.极高的活跃度</b>。 Spark 目前是 Apache 基金会的顶级项目， 全世界有大量的优秀工程师是 Spark 的 committer， 并且世界上很多顶级的 IT 公司都在大规模地使用Spark。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>看看同样是负责计算问题的 MapReduce，如图 2 所示是 MapReduce 计算 WordCount。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-db90c9403a510ddaf4717488aabaf827_b.jpg\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"788\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-db90c9403a510ddaf4717488aabaf827_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;788&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"788\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-db90c9403a510ddaf4717488aabaf827_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-db90c9403a510ddaf4717488aabaf827_b.jpg\"/><figcaption>图 2</figcaption></figure><p>MapReduce 解决了大数据处理中多种场景问题，但是它的局限性也很明显：</p><ul><li>MapReduce 只提供 Map 和 Reduce 两个操作，欠缺表达力，复杂的计算需要大量的 Job 才能完成。</li><li>中间结果也放在 HDFS 文件系统中，迭代计算的话效率很低。</li><li>适用 Batch 数据处理，对于交互式数据处理而言实时数据处理的支持不够。</li><li>需要写很多底层代码，难上手。如上所示的 WordCount 程序至少需要三个 java 类：Map 类、Reduce 类、Job 类，这里不详细列出。</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p>许多项目针对它的局限性进行了改进（如 Tez 等），接着看图 3 中 Spark 的具体操作流程：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-807678a422802cc77eb5a85442150ab4_b.jpg\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"535\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-807678a422802cc77eb5a85442150ab4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;535&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"535\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-807678a422802cc77eb5a85442150ab4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-807678a422802cc77eb5a85442150ab4_b.jpg\"/><figcaption>图 3</figcaption></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>首先我们可以看到 Spark 提供了丰富的算子（textFile、FlatMap、Map、ReduceByKey 等），在计算的中间结果也没有存储到 HDFS 的操作。然后，对于上图的 WordCount 程序，Spark 只需要如下一行代码：</p><div class=\"highlight\"><pre><code class=\"language-text\">sc.textFile(s&#34;${path}&#34;).flatMap(_.split(&#34; &#34;)).map(word =&gt; (word, 1)).reduceByKey(_ + _).saveAsTextFile(&#34;hdfs://xxx&#34;)</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>图 4 列举了 Spark 和 MapReduce 作为数据处理引擎的一些对比。值得一提的是关于数据处理的规模，Spark 在诞生后，社区里有很多质疑 Spark 处理数据规模的声音，随后官方给出了对于一 PB 数据排序的实验，并且处理时间打破了当时的记录。但我们也不能忽视，在实际生产过程中，我们面对的不是一个程序或者一个任务，在同一个集群，如果有很多的 Spark 程序没有得到很好的优化，会浪费大量的内存，从而让一些程序需要排队等待，在这种情况下，Spark 处理的数据规模可能会小于 MapReduce 处理的数据规模。（之后的系列文章也会介绍关于 Spark 内存调优的相关内容）</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-6bb8a2150d60ffe12a1a6df8d0b7abbc_b.jpg\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"727\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-6bb8a2150d60ffe12a1a6df8d0b7abbc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;727&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"727\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-6bb8a2150d60ffe12a1a6df8d0b7abbc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-6bb8a2150d60ffe12a1a6df8d0b7abbc_b.jpg\"/><figcaption>图 4</figcaption></figure><p>关于最后一点容错性，MapReduce 中每一步操作的结果都会被存入磁盘，在计算出现错误时可以很好的从磁盘进行恢复；Spark 则需要根据 RDD 中的信息进行数据的重新计算，会耗费一定的资源。Spark 提供两种方式进行故障恢复：通过数据的血缘关系再执行一遍前面的处理；Checkpoint 将数据集存储到持久存储中。理论上如果选择在每个完成的小步骤上加 CheckPoint，Spark 的容错性能可以和 MR 达到一样的稳健。当然，很少有人会这么做。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>我们通过 Spark 与 MapReduce 对比。看到了 Spark 对 MapReduce 局限性的改进，还有它快速、通用的特点。接下来将通过 Spark 的设计思想和执行过程来说明它为什么可以做到这些特点。</p><h2><b>Spark 的基本原理 </b></h2><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-3c14e44ee58ad5e39166a312a8989792_b.jpg\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"792\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-3c14e44ee58ad5e39166a312a8989792_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;792&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"792\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-3c14e44ee58ad5e39166a312a8989792_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-3c14e44ee58ad5e39166a312a8989792_b.jpg\"/><figcaption>图 5</figcaption></figure><p>如图 5 所示，在 Spark 集群中由一个节点作为 driver 端创建 SparkContext。Spark 应用程序的入口负责调度各个运算资源，协调各个 Worker Node上 的 Executor。根据用户输入的参数会产生若干个 workr，workr 节点运行若干个 executor，一个 executor 是一个进程，运行各自的 task，每个 task 执行相同的代码段处理不同的数据。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-db12a135bd9e4586773fcba892ec4a13_b.jpg\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"818\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-db12a135bd9e4586773fcba892ec4a13_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;818&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"818\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-db12a135bd9e4586773fcba892ec4a13_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-db12a135bd9e4586773fcba892ec4a13_b.jpg\"/><figcaption>图 6</figcaption></figure><p>如图 6 所示是 Spark 的具体执行过程，client 提交作业，通过反射 invoke 执行用户代码 main 函数，之后开始启动 CoarseGrainedExecutorBackend 和初始化 SparkContext。</p><blockquote>*SparkContext 初始化包括初始化监控页面 SparkUI、执行环境 SparkEnv、安全管理器 SecurityManager、stage 划分及调度器 DAGScheduler、task 作业调度器 TaskSchedulerImpl 、与 Executor 通信的调度端 CoarseGrainedSchedulerBackend。</blockquote><p class=\"ztext-empty-paragraph\"><br/></p><p>DAG Scheduler 将作业划分后，依次提交 stage 对应的 taskSet 给 TaskSchedulerImpl，TaskSchedulerImpl 会 submit taskset 给 driver 端的 CoarseGrainedSchedulerBackend 后端，接着 CoarseGrainedSchedulerBackend 会一个一个的 LaunchTask。在远端的 CoarseGrainedExecutorBackend 接收到 task 提交 event 后，会调用 Executor 执行 task，最终 task 是在 TaskRunner 的 run 方法内运行。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>那么在过程 4 中 DAG Scheduler 如何划分作业？如果产生 stage、task 等给 Executor 执行呢？接着我们看作业划分执行的示例。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-6f144c658ca181e926c20939386e7dc0_b.jpg\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"783\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-6f144c658ca181e926c20939386e7dc0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;783&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"783\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic1.zhimg.com/v2-6f144c658ca181e926c20939386e7dc0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-6f144c658ca181e926c20939386e7dc0_b.jpg\"/><figcaption>图 7</figcaption></figure><p>图 7 描述了一个 Spark 程序，从 HDFS 上读取数据产生 RDD-A 然后 flatmap 操作到 RDD-B，读取另一部分数据的到RDD-C，然后 map 操作的到 RDD-D，RDD-D 聚合操作 RDD-E，RDD-B 和 RDD-E 加入后得到 RDD-F，然后再将结果存储到 HDFS 上。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>Spark 根据 RDD 之间的不同点依赖关系切分成不同的阶段（Stage），途中有四个阶段，其中 Stage0 和 Stage2 由于没有依赖关系是可以并行执行的。但Stage2需要等待Stage1执行完毕。RDD-D 到 RDD- F 的聚合操作以及 Stage0 和 Stage2 得到的 RDD- B 和 RDD-E join在一起的到 RDD-F，这个过程会产生 shaffle。没有依赖关系的Stage是可以并行执行的，但是对于job，Spark是串行执行的，如果想要并行执行Job，可以在Spark程序中进行多线程编程。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在这个 DAG 图中，Spark 能够充分了解数据之间的血缘关系，这样某些任务失败后可以根据血缘关系重新执行计算获取失败了的 RDD。</p><blockquote>*<b>宽依赖和窄依赖</b><br/>窄依赖是指父RDD的每个分区只被子RDD的一个分区所使用，子RDD分区通常对应常数个父RDD分区；<br/>宽依赖是指父RDD的每个分区都可能被多个子RDD分区所使用，子RDD分区通常对应所有的父RDD分区。这个概念在下面的例子中会涉及。<br/><br/>Spark 提供了丰富的算子，操作也更加通用。那么这种划分作业、执行并行计算的方案如何使 Spark 产生基于内存计算的快速效果呢？都说 Spark 擅长迭代计算，那么我们通过一个经典的迭代问题 PageRank 算法来与 MapReduce 比较一下。</blockquote><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-94cab2f4054f5cfab5b5f371721656a3_b.jpg\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"527\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-94cab2f4054f5cfab5b5f371721656a3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;527&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"527\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-94cab2f4054f5cfab5b5f371721656a3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-94cab2f4054f5cfab5b5f371721656a3_b.jpg\"/><figcaption>图 8，via http://www.jos.org.cn/jos/ch/reader/create_pdf.aspx?file_no=5557&amp;amp;amp;amp;amp;amp;amp;journal_id=jos</figcaption></figure><p>图 8 是 MapReduce 进行 pagerank 算法的一次迭代过程，需要注意的是灰色的部分都是需要存储到磁盘的数据。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-218f82dff8245a8752f346d88e4a8f20_b.jpg\" data-size=\"normal\" data-rawwidth=\"792\" data-rawheight=\"487\" class=\"origin_image zh-lightbox-thumb\" width=\"792\" data-original=\"https://pic1.zhimg.com/v2-218f82dff8245a8752f346d88e4a8f20_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;792&#39; height=&#39;487&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"792\" data-rawheight=\"487\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"792\" data-original=\"https://pic1.zhimg.com/v2-218f82dff8245a8752f346d88e4a8f20_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-218f82dff8245a8752f346d88e4a8f20_b.jpg\"/><figcaption>图 9 ，via http://www.jos.org.cn/jos/ch/reader/create_pdf.aspx?file_no=5557&amp;amp;amp;amp;amp;amp;amp;journal_id=jos</figcaption></figure><p>图 9 所示是 Spark 执行 pageRank 算法的一次迭代过程，相较于 MapReduce 做了很多改进。首先在内存足够的情况下 Spark 允许用户将常用的数据缓存到内存中,加快了系统的运行速度；其次 Spark 对数据之间的依赖关系有了明确的划分，根据宽依赖与窄依赖关系进行任务的调度，可以实现管道化操作，使系统灵活性得以提高。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-fc81922055990879d69ed130e74e4814_b.jpg\" data-size=\"normal\" data-rawwidth=\"792\" data-rawheight=\"380\" class=\"origin_image zh-lightbox-thumb\" width=\"792\" data-original=\"https://pic1.zhimg.com/v2-fc81922055990879d69ed130e74e4814_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;792&#39; height=&#39;380&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"792\" data-rawheight=\"380\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"792\" data-original=\"https://pic1.zhimg.com/v2-fc81922055990879d69ed130e74e4814_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-fc81922055990879d69ed130e74e4814_b.jpg\"/><figcaption>图 10：MapReduce 进行 pagerank 算法的二次迭代，via http://www.jos.org.cn/jos/ch/reader/create_pdf.aspx?file_no=5557&amp;amp;amp;amp;amp;amp;amp;journal_id=jos</figcaption></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-1068b67a6e9d874b5a8bd10233e89b5c_b.jpg\" data-size=\"normal\" data-rawwidth=\"772\" data-rawheight=\"418\" class=\"origin_image zh-lightbox-thumb\" width=\"772\" data-original=\"https://pic1.zhimg.com/v2-1068b67a6e9d874b5a8bd10233e89b5c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;772&#39; height=&#39;418&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"772\" data-rawheight=\"418\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"772\" data-original=\"https://pic1.zhimg.com/v2-1068b67a6e9d874b5a8bd10233e89b5c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-1068b67a6e9d874b5a8bd10233e89b5c_b.jpg\"/><figcaption>图 11：Spark 进行 pagerank 算法的二次迭代，via http://www.jos.org.cn/jos/ch/reader/create_pdf.aspx?file_no=5557&amp;amp;amp;amp;amp;amp;amp;journal_id=jos</figcaption></figure><p>如图所示 Spark 可以将具有窄依赖关系的 RDD 分区分配到一个任务中,进行管道化操作，任务内部数据无需通过网络传输且任务之间互不干扰，因此 Spark 两次迭代只有三次 shuffle。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在一次迭代过程中，MapReduce 与 Spark 在性能上可能并没有很大的差别，但是随着迭代次数的增加，两者的差距逐渐显现出来。Spark 根据依赖关系采用的任务调度策略使得 shuffle 次数相较于 MapReduce 有了显著降低，因此 Spark 的设计十分适合进行迭代运算。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>回顾本篇文章，我们依次从概念、特点及原理三个角度初步介绍了 Spark，下一篇我们将具体介绍 Spark on Yarn 的运作流程与机制，敬请期待。</p><hr/><h2><b>号外！</b></h2><p>美图的大数据团队在近几年的发展中，逐步演进和发展出美图的大数据体系，以及在美图业务场景下数据技术应用的最佳实践。8月11日我们将在深圳开设一场技术沙龙，我们邀请了来自美图公司的大数据负责人、架构师、魅族公司的数据技术专家、以及来自 Apache kylin 的 PMC。美图的大数据技术负责人和架构师会为大家分享美图在大数据技术上的探索、大数据的架构、以及数据技术的应用落地。魅族的数据技术专家会为大家介绍魅族的 DMP 系统的架构设计以及系统的演进历程。来自 Apache 顶级项目 kylin 的架构师会为大家分享 kylin 的技术原理与应用实践。四位技术专家会从多个角度不同层次，为大家分享各自在大数据技术上的实践经验。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>详情见报名链接：<a href=\"https://link.zhihu.com/?target=https%3A//www.bagevent.com/event/1106376%3Fpreview%3D1\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">大数据架构与数据技术应用实践 -百格活动</a></p><hr/><p><b>附：Spark 相关术语表</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-03a8393cca783b579324997030f506a7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"812\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-03a8393cca783b579324997030f506a7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;812&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"812\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic4.zhimg.com/v2-03a8393cca783b579324997030f506a7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-03a8393cca783b579324997030f506a7_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "Spark", 
                    "tagLink": "https://api.zhihu.com/topics/19942170"
                }, 
                {
                    "tag": "大数据", 
                    "tagLink": "https://api.zhihu.com/topics/19740929"
                }, 
                {
                    "tag": "Hadoop", 
                    "tagLink": "https://api.zhihu.com/topics/19563390"
                }
            ], 
            "comments": [
                {
                    "userName": "Gordon", 
                    "userLink": "https://www.zhihu.com/people/9ee253ddd77e5be540155b84eeb00edd", 
                    "content": "<p>很好奇图表是用什么画的啊~</p><a class=\"comment_sticker\" href=\"https://pic4.zhimg.com/v2-db92f653a2ec17ea3ff309d6d56e8507.gif\" data-width=\"\" data-height=\"\">[吃瓜]</a>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "Gordon", 
                    "userLink": "https://www.zhihu.com/people/9ee253ddd77e5be540155b84eeb00edd", 
                    "content": "<p></p><figure data-size=\"normal\"><img src=\"https://pic1.zhimg.com/50/v2-bb280e2d6e7c2b57c924c9db9ab46400_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1005\" data-rawheight=\"239\" class=\"origin_image zh-lightbox-thumb\" width=\"1005\" data-original=\"https://pic1.zhimg.com/50/v2-bb280e2d6e7c2b57c924c9db9ab46400_r.jpg\"></figure><p>哇，非常喜欢这个图表，求教是怎么画的。有个小笔误呦~</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }
    ], 
    "url": "https://zhuanlan.zhihu.com/c_216887134"
}
