{
    "title": "时间序列数据库", 
    "description": "时间序列数据库 ( Time Series DataBase , 简称 TSDB) 是一种集时间序列数据高效读写，压缩存储，实时计算能力为一体的数据库服务，可广泛应用于物联网和互联网领域，实现对设备及业务服务的实时监控，实时预测告警。", 
    "followers": [
        "https://www.zhihu.com/people/cchaow", 
        "https://www.zhihu.com/people/fyls119", 
        "https://www.zhihu.com/people/alan7yg", 
        "https://www.zhihu.com/people/yi-peng-48-96", 
        "https://www.zhihu.com/people/huang-long-long", 
        "https://www.zhihu.com/people/imaemo", 
        "https://www.zhihu.com/people/tonydeng", 
        "https://www.zhihu.com/people/tkcb", 
        "https://www.zhihu.com/people/greeny-99-53", 
        "https://www.zhihu.com/people/wilhelm-82", 
        "https://www.zhihu.com/people/lipaul-51", 
        "https://www.zhihu.com/people/hui-tai-lang-da-wang-59", 
        "https://www.zhihu.com/people/mindgazer", 
        "https://www.zhihu.com/people/lenglang", 
        "https://www.zhihu.com/people/gao-shu-60", 
        "https://www.zhihu.com/people/xu-ran-3-59", 
        "https://www.zhihu.com/people/ye-ling-ye-83", 
        "https://www.zhihu.com/people/silence5197", 
        "https://www.zhihu.com/people/rock-rock-63", 
        "https://www.zhihu.com/people/chenzhepeter", 
        "https://www.zhihu.com/people/luo-bin-84-17", 
        "https://www.zhihu.com/people/eledata", 
        "https://www.zhihu.com/people/er-act", 
        "https://www.zhihu.com/people/xiao-xuan-xuan-24", 
        "https://www.zhihu.com/people/wydesign-15", 
        "https://www.zhihu.com/people/xuweiqing", 
        "https://www.zhihu.com/people/xie-bin-jie-48", 
        "https://www.zhihu.com/people/suemiyi", 
        "https://www.zhihu.com/people/hu-jia-xuan", 
        "https://www.zhihu.com/people/danny-qian", 
        "https://www.zhihu.com/people/li-jing-79", 
        "https://www.zhihu.com/people/chen-zheng-95", 
        "https://www.zhihu.com/people/leyi_q", 
        "https://www.zhihu.com/people/rongshen", 
        "https://www.zhihu.com/people/pin-yin-71", 
        "https://www.zhihu.com/people/kelvinji2009", 
        "https://www.zhihu.com/people/apppur", 
        "https://www.zhihu.com/people/tom-31-65", 
        "https://www.zhihu.com/people/wang-de-fu", 
        "https://www.zhihu.com/people/ekrab", 
        "https://www.zhihu.com/people/zhou-zhu-yi", 
        "https://www.zhihu.com/people/blastbao", 
        "https://www.zhihu.com/people/brucewu1990", 
        "https://www.zhihu.com/people/li-ke-23-83", 
        "https://www.zhihu.com/people/shui-qing-mu-hua-77-52", 
        "https://www.zhihu.com/people/wedgwood", 
        "https://www.zhihu.com/people/iyux", 
        "https://www.zhihu.com/people/vincent-shi-62", 
        "https://www.zhihu.com/people/snmple", 
        "https://www.zhihu.com/people/wen-zhe-95", 
        "https://www.zhihu.com/people/richard-71-95-83", 
        "https://www.zhihu.com/people/youchang-xu", 
        "https://www.zhihu.com/people/zhou-hao-1994", 
        "https://www.zhihu.com/people/chang-yong-gang-33", 
        "https://www.zhihu.com/people/bai-bing-45", 
        "https://www.zhihu.com/people/benjamin-67-50", 
        "https://www.zhihu.com/people/warren1119", 
        "https://www.zhihu.com/people/jinzuo", 
        "https://www.zhihu.com/people/junhwong", 
        "https://www.zhihu.com/people/peng-ying-jie-23-61", 
        "https://www.zhihu.com/people/ben-yang-10", 
        "https://www.zhihu.com/people/likelxl", 
        "https://www.zhihu.com/people/zhang-ke-jin", 
        "https://www.zhihu.com/people/n4mine", 
        "https://www.zhihu.com/people/wang-dong-pai-54", 
        "https://www.zhihu.com/people/huang-ke-ji-49", 
        "https://www.zhihu.com/people/wanghenshui", 
        "https://www.zhihu.com/people/xiang-ni-xi-de-meng-ku-fen", 
        "https://www.zhihu.com/people/liguanjun", 
        "https://www.zhihu.com/people/xiaosuo-39", 
        "https://www.zhihu.com/people/zhang-86-84", 
        "https://www.zhihu.com/people/xiang-wei-6-96", 
        "https://www.zhihu.com/people/ru-lai-65", 
        "https://www.zhihu.com/people/coder_wolf", 
        "https://www.zhihu.com/people/yangyu0", 
        "https://www.zhihu.com/people/chen-tian-yi-9", 
        "https://www.zhihu.com/people/mel-tor", 
        "https://www.zhihu.com/people/sinm-pan", 
        "https://www.zhihu.com/people/hellohell", 
        "https://www.zhihu.com/people/grakra", 
        "https://www.zhihu.com/people/yan-wen-97-59", 
        "https://www.zhihu.com/people/devonaha", 
        "https://www.zhihu.com/people/buzz-69", 
        "https://www.zhihu.com/people/chen-xin-lei-92", 
        "https://www.zhihu.com/people/nanpuyue", 
        "https://www.zhihu.com/people/bai-shen", 
        "https://www.zhihu.com/people/libin-sui", 
        "https://www.zhihu.com/people/scholar-32", 
        "https://www.zhihu.com/people/sun-38", 
        "https://www.zhihu.com/people/she-liang", 
        "https://www.zhihu.com/people/kkkkk-ppppp", 
        "https://www.zhihu.com/people/ye-zui-la-tu", 
        "https://www.zhihu.com/people/lixianmin", 
        "https://www.zhihu.com/people/dongdong-w", 
        "https://www.zhihu.com/people/sky-15", 
        "https://www.zhihu.com/people/cdp-71", 
        "https://www.zhihu.com/people/wang-liao-52", 
        "https://www.zhihu.com/people/bu-gei-xing-ming", 
        "https://www.zhihu.com/people/wolfer", 
        "https://www.zhihu.com/people/daydaychang", 
        "https://www.zhihu.com/people/guo-zhen-38", 
        "https://www.zhihu.com/people/huyujie-62", 
        "https://www.zhihu.com/people/lin-zheng-49", 
        "https://www.zhihu.com/people/sigmix", 
        "https://www.zhihu.com/people/yu-zhao-hui-92", 
        "https://www.zhihu.com/people/chi-cindy", 
        "https://www.zhihu.com/people/yang-tong-xue-16-78", 
        "https://www.zhihu.com/people/tan-zhen-hua-1", 
        "https://www.zhihu.com/people/liusihao", 
        "https://www.zhihu.com/people/craig-law", 
        "https://www.zhihu.com/people/yue-ying-55-64", 
        "https://www.zhihu.com/people/chris", 
        "https://www.zhihu.com/people/yan-jian-76-38", 
        "https://www.zhihu.com/people/zhang-da-mao", 
        "https://www.zhihu.com/people/guesswhathhh", 
        "https://www.zhihu.com/people/jian-jia-wei-yang", 
        "https://www.zhihu.com/people/wang-zhu-yi-65", 
        "https://www.zhihu.com/people/edward-15-1", 
        "https://www.zhihu.com/people/moonkey", 
        "https://www.zhihu.com/people/yi-nan-3-63", 
        "https://www.zhihu.com/people/tan_chao", 
        "https://www.zhihu.com/people/wang-zhe-26", 
        "https://www.zhihu.com/people/DeepNight", 
        "https://www.zhihu.com/people/gu-yu-71-65", 
        "https://www.zhihu.com/people/yang-yang-60-65-43", 
        "https://www.zhihu.com/people/zhao-ya-jun-11", 
        "https://www.zhihu.com/people/peng-he-sen", 
        "https://www.zhihu.com/people/sy-zheng", 
        "https://www.zhihu.com/people/zhao-zhen-hui-52", 
        "https://www.zhihu.com/people/yimeng.ch", 
        "https://www.zhihu.com/people/zhou-yi-jian-42", 
        "https://www.zhihu.com/people/mai-sui-95-14", 
        "https://www.zhihu.com/people/tao-hong-22", 
        "https://www.zhihu.com/people/zhaodi", 
        "https://www.zhihu.com/people/zhang-zi-jie-35", 
        "https://www.zhihu.com/people/wu-ming-9-92-45", 
        "https://www.zhihu.com/people/qiu-zhen-guang", 
        "https://www.zhihu.com/people/qian-qiu-yin-jun", 
        "https://www.zhihu.com/people/guo-feng-66-47", 
        "https://www.zhihu.com/people/louis-1-84", 
        "https://www.zhihu.com/people/uucode", 
        "https://www.zhihu.com/people/fang-zhi-44-68", 
        "https://www.zhihu.com/people/luoweisong", 
        "https://www.zhihu.com/people/palette", 
        "https://www.zhihu.com/people/xi-hong-shi-ji-dan-mian-46", 
        "https://www.zhihu.com/people/ariesdevil_lulu", 
        "https://www.zhihu.com/people/dwang-17", 
        "https://www.zhihu.com/people/tuan-chang", 
        "https://www.zhihu.com/people/huang-he-47-48", 
        "https://www.zhihu.com/people/pu-guan-ru", 
        "https://www.zhihu.com/people/tu-long-zhe", 
        "https://www.zhihu.com/people/kimmking", 
        "https://www.zhihu.com/people/sq892246139", 
        "https://www.zhihu.com/people/kidofwind", 
        "https://www.zhihu.com/people/hugbrain", 
        "https://www.zhihu.com/people/maplewish", 
        "https://www.zhihu.com/people/lao-guo-2-44", 
        "https://www.zhihu.com/people/juan-niao-wxy", 
        "https://www.zhihu.com/people/zhang-ce-28-72", 
        "https://www.zhihu.com/people/tang-yan-bo-59", 
        "https://www.zhihu.com/people/fourhu-68", 
        "https://www.zhihu.com/people/li-wei-ran-43", 
        "https://www.zhihu.com/people/li-fu-gui-87", 
        "https://www.zhihu.com/people/zhui-ru-ku-hai-xiao-chen-gou", 
        "https://www.zhihu.com/people/hu-xiao-miao-18", 
        "https://www.zhihu.com/people/mr.wing", 
        "https://www.zhihu.com/people/wang-lin-77-20", 
        "https://www.zhihu.com/people/keeplearning-71", 
        "https://www.zhihu.com/people/kang-rong-7", 
        "https://www.zhihu.com/people/lao-mao-qi", 
        "https://www.zhihu.com/people/wan-hong-2-94", 
        "https://www.zhihu.com/people/brianling-20", 
        "https://www.zhihu.com/people/kevinxhuang", 
        "https://www.zhihu.com/people/yinmin-li", 
        "https://www.zhihu.com/people/gen1cnt", 
        "https://www.zhihu.com/people/liu-ren-jie-61", 
        "https://www.zhihu.com/people/zxybazh", 
        "https://www.zhihu.com/people/zhong-jun-lin-33", 
        "https://www.zhihu.com/people/Namron", 
        "https://www.zhihu.com/people/hen-ji-97", 
        "https://www.zhihu.com/people/chen-ye-chao-7", 
        "https://www.zhihu.com/people/lei-xiao-yu-49", 
        "https://www.zhihu.com/people/he-zu-89", 
        "https://www.zhihu.com/people/chicao-ma", 
        "https://www.zhihu.com/people/zhang-ming-feng-91", 
        "https://www.zhihu.com/people/alice-69-26-21", 
        "https://www.zhihu.com/people/lian-zhi-wen", 
        "https://www.zhihu.com/people/snail-ren", 
        "https://www.zhihu.com/people/lu-ze-hu", 
        "https://www.zhihu.com/people/james0zan", 
        "https://www.zhihu.com/people/zrzhit", 
        "https://www.zhihu.com/people/li-ying-33-98-67", 
        "https://www.zhihu.com/people/roger-46-18", 
        "https://www.zhihu.com/people/zhang-yl-28", 
        "https://www.zhihu.com/people/htner", 
        "https://www.zhihu.com/people/eta-100a", 
        "https://www.zhihu.com/people/souhup", 
        "https://www.zhihu.com/people/cui-yang-22-19-31", 
        "https://www.zhihu.com/people/nian-nian-you-yu-45-15", 
        "https://www.zhihu.com/people/lvheyang", 
        "https://www.zhihu.com/people/wenlong-ma", 
        "https://www.zhihu.com/people/sofn-le", 
        "https://www.zhihu.com/people/veryzhun", 
        "https://www.zhihu.com/people/wang-xing-chi", 
        "https://www.zhihu.com/people/wang-qi-yuan-56", 
        "https://www.zhihu.com/people/lit10050528", 
        "https://www.zhihu.com/people/davin.liu", 
        "https://www.zhihu.com/people/yang-yi-mou-26", 
        "https://www.zhihu.com/people/micyng", 
        "https://www.zhihu.com/people/fanyu83", 
        "https://www.zhihu.com/people/sempr", 
        "https://www.zhihu.com/people/ghostkit", 
        "https://www.zhihu.com/people/GammaGo", 
        "https://www.zhihu.com/people/yingfeng", 
        "https://www.zhihu.com/people/bencheng0916", 
        "https://www.zhihu.com/people/kylin-chou", 
        "https://www.zhihu.com/people/guo-marvin", 
        "https://www.zhihu.com/people/sunisdown", 
        "https://www.zhihu.com/people/huang-zhi-hao-26", 
        "https://www.zhihu.com/people/yang-li-1", 
        "https://www.zhihu.com/people/zhu-xin-gan-29", 
        "https://www.zhihu.com/people/dahui", 
        "https://www.zhihu.com/people/davis-78-7", 
        "https://www.zhihu.com/people/zhu-jack-10", 
        "https://www.zhihu.com/people/michael-lee-18", 
        "https://www.zhihu.com/people/liu-yu-31-9", 
        "https://www.zhihu.com/people/qi-mo-dan", 
        "https://www.zhihu.com/people/ShiboChan", 
        "https://www.zhihu.com/people/feng-xiao-li", 
        "https://www.zhihu.com/people/andi", 
        "https://www.zhihu.com/people/wang-zhi-yang-44", 
        "https://www.zhihu.com/people/chen-zhou-zhou-50", 
        "https://www.zhihu.com/people/laixintao", 
        "https://www.zhihu.com/people/walterzhao", 
        "https://www.zhihu.com/people/xu-wan-jin-6", 
        "https://www.zhihu.com/people/minusli-79", 
        "https://www.zhihu.com/people/du-jie-59", 
        "https://www.zhihu.com/people/clearlove-41", 
        "https://www.zhihu.com/people/KKKIIO", 
        "https://www.zhihu.com/people/xlows1227", 
        "https://www.zhihu.com/people/yongge-89", 
        "https://www.zhihu.com/people/hu-ke-51-44", 
        "https://www.zhihu.com/people/alex23", 
        "https://www.zhihu.com/people/chen-lian-jie-6", 
        "https://www.zhihu.com/people/tu-dou-88-56", 
        "https://www.zhihu.com/people/zhang-wei-3-49-41", 
        "https://www.zhihu.com/people/powertyuui", 
        "https://www.zhihu.com/people/rei-hawking", 
        "https://www.zhihu.com/people/shenyindd", 
        "https://www.zhihu.com/people/chaomai", 
        "https://www.zhihu.com/people/liu-xiong-guang", 
        "https://www.zhihu.com/people/san-dao-27", 
        "https://www.zhihu.com/people/wang-chao-84-29", 
        "https://www.zhihu.com/people/huang-weilin", 
        "https://www.zhihu.com/people/shenli", 
        "https://www.zhihu.com/people/ping-pang-kuang-mo-29", 
        "https://www.zhihu.com/people/guo-ke-78-42", 
        "https://www.zhihu.com/people/cui-xiao-chen-82", 
        "https://www.zhihu.com/people/chen-hao-87-41-7", 
        "https://www.zhihu.com/people/yang-zhi-feng-79", 
        "https://www.zhihu.com/people/zhougong-cap", 
        "https://www.zhihu.com/people/chuan-tou-68-72", 
        "https://www.zhihu.com/people/shuweiqun", 
        "https://www.zhihu.com/people/wu-yao-3", 
        "https://www.zhihu.com/people/chaoqun", 
        "https://www.zhihu.com/people/zhao-xiao-qiang-84", 
        "https://www.zhihu.com/people/yjlfuture", 
        "https://www.zhihu.com/people/xiaowing", 
        "https://www.zhihu.com/people/benedictjin", 
        "https://www.zhihu.com/people/qingzhi-37", 
        "https://www.zhihu.com/people/yyalone", 
        "https://www.zhihu.com/people/tristansu", 
        "https://www.zhihu.com/people/sph-29"
    ], 
    "article": [
        {
            "url": "https://zhuanlan.zhihu.com/p/64933450", 
            "userName": "Jeff", 
            "userLink": "https://www.zhihu.com/people/c2e171b39183c7a115a48cfa68378f25", 
            "upvote": 4, 
            "title": "重磅！阿里云时空数据库正式免费公测", 
            "content": "<p>随着移动互联网和物联网的广泛普及，90%以上的数据是和时间+空间相关的，而越来越多的数据应用场景与时间和空间信息密不可分。时间 +空间维度的数据（我们称之为时空数据）是一种高维数据，需要更为高效的数据处理方式来处理，而普通的关系型数据库更适合于存储数值和字符类型数据，也缺少相关的时空算子。在实际应用场景上例如传感器网络、移动互联网、射频识别、全球定位系统等设备时刻输出时间和空间数据，数据量增长非常迅速，这对存储和管理时空数据带来了挑战，传统数据库很难应对以上场景。阿里云时空数据库能够存储、管理包括时间序列以及空间地理位置相关的数据，时空数据库具有时空数据模型、时空索引和时空算子，完全兼容SQL及SQL/MM标准，支持时空数据同业务数据一体化存储、无缝衔接，易于集成使用。</p><p>5月5日，阿里云时空数据库正式免费公测，感兴趣的同学可以免费申请试用：</p><p>公测免费购买页面：<a href=\"https://link.zhihu.com/?target=https%3A//common-buy.aliyun.com/%3FcommodityCode%3Dhitsdb_spatialpre%23/buy\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">common-buy.aliyun.com/?</span><span class=\"invisible\">commodityCode=hitsdb_spatialpre#/buy</span><span class=\"ellipsis\"></span></a></p><p>产品使用手册：<a href=\"https://link.zhihu.com/?target=https%3A//help.aliyun.com/document_detail/116088.html%3Fspm%3Da2c4g.11174283.6.727.1b22130eu4OBeh\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">help.aliyun.com/documen</span><span class=\"invisible\">t_detail/116088.html?spm=a2c4g.11174283.6.727.1b22130eu4OBeh</span><span class=\"ellipsis\"></span></a></p><h2><b>产品优势</b></h2><p><b>易用（标准SQL接口）</b></p><p>    SQL是目前最通用的数据库访问语言，时空数据库基于标准PostgreSQL，支持JDBC/ODBC驱动访问。时空数据同其它业务数据一体化处理，兼容OGC空间计算函数；支持符合OGC规范的WKT和WKB格式数据输入和输出。</p><p><b>写入性能强劲</b></p><p> 时空数据，插入是一个强需求，往往有大量设备上报轨迹、指标数据，因此对插入性能要求较高。阿里云时空数据库，单机不同规格下可以支持到数万到数十万的TPS写入。</p><p><b>高效分析能力</b></p><p> 时空数据，除单条的查询、POI查询，更多的是其他的分析类需求。这对时空数据库的分析能力也是一个挑战。阿里云时空支持查询条件自动选择分区，高效空间索引，并行的聚合操作等提升分析性能。</p><p><b>自动扩展分区</b></p><p> 业务对时空数据查询，往往都会对时间区间进行过滤，因此时空数据通常在分区时，会有一个时间或空间分区的概念。时空数据库支持自动扩展分区，减少用户的管理量，不需要人为的干预自动扩展分区。</p><p><b>功能丰富</b></p><p>具有丰富的时间和空间处理查询函数；支持点（POINT）、线（LINESTRING）、多边形（POLYGON）、多点（MULTIPOINT）、多线（MULTILINESTRING）、多多边形（MULTIPOLYGON）和几何对象集（GEOMETRYCOLLECTION）等几何类型存储。</p><p><b>自动保留策略</b></p><p> 根据用户配置，自动删除过旧数据，极大降低用户使用成本，减少用户管理工作。<br/> </p><p><b>自动Failover</b></p><p> 阿里云时空数据库提供全自动Failover机制，一旦所在硬件发生不可恢复的故障，会在非常短的时间内使用其他硬件替换故障硬件。这样可以减少因为不可控故障引发的服务中断时间。该Failover是全自动的，无需人工干预，用户也无需担心服务由于硬件故障造成的长时间不可用。</p><p><b>高可靠</b></p><p>时空数据库是一种高性能时空数据库，底层存储建立在阿里云高效云盘基础之上，高效云盘提供99.9999999%数据高可靠保障。可以保障时空数据库数据一旦写入，基本不会丢失。</p><h2><b>生态</b></h2><p>阿里云时空数据库在生态上非常易于同多种主流产品集成，比如地图引擎（如GeoServer和MapServer）、地图编辑系统（如QGIS和ArcGIS for Desktop）、数据分析与可视化产品（如Grafana、Zeppelin和Jupyter）、大数据分析平台（Spark），满足模块化集成需求，为时空数据管理提供有力支撑。</p><p><b>数据写入&amp;查询</b></p><p>时空数据库写入和查询非常便利，读写采用标准SQL，用户可以通过JDBC/ODBC驱动操作数据库，进行读写操作。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>用户也可以通过psql交互式终端向时空数据库写入数据，下面是几个简单的例子： </p><p>INSERT INTO tsdb_test VALUES (1001, &#39;2019-03-11 16:34:15&#39;, 1002.2, </p><p>ST_SetSRID(ST_MakePoint(10.3,20.1),4326)</p><p class=\"ztext-empty-paragraph\"><br/></p><p>用户通过交互终端查询数据，可以如下：</p><p>SELECT time,uid,speed,dev_type,ST_AsText(position) FROM metrics </p><p>WHERE time &gt;&#39;2017-01-01 01:02:00&#39; AND time &lt; &#39;2017-01-01 01:11:02&#39; AND </p><p>ST_Contains(ST_SetSRID(ST_MakeBox2D(ST_Point(12.4, 25.5),ST_Point(13.0,26.1)),4326),position);</p><p>关于时空数据库的具体用法，可以参考阿里云时空数据库文档：[开发指南](<a href=\"https://link.zhihu.com/?target=https%3A//help.aliyun.com/document_detail/115574.html%3Fspm%3Da2c4g.11186623.6.748.5ac13c70d9Q5mZ\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">help.aliyun.com/documen</span><span class=\"invisible\">t_detail/115574.html?spm=a2c4g.11186623.6.748.5ac13c70d9Q5mZ</span><span class=\"ellipsis\"></span></a>)</p><h2><b>场景</b></h2><p><b>地图服务</b></p><p>地图服务是一种非常广泛的应用，便于各类业务数据空间化、空间分析和可视化。这个场景介绍如何使用时空数据库搭建地图服务，并给出架构参考。</p><p><b>方案架构</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-b41a4570cd69a9d1bd9827e011b6fd6b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"479\" data-rawheight=\"585\" class=\"origin_image zh-lightbox-thumb\" width=\"479\" data-original=\"https://pic4.zhimg.com/v2-b41a4570cd69a9d1bd9827e011b6fd6b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;479&#39; height=&#39;585&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"479\" data-rawheight=\"585\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"479\" data-original=\"https://pic4.zhimg.com/v2-b41a4570cd69a9d1bd9827e011b6fd6b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-b41a4570cd69a9d1bd9827e011b6fd6b_b.jpg\"/></figure><p>时空数据库作为存储空间数据（如车辆定位数据）与空间查询引擎，提供后端支持。GeoServer（GeoServer是一款知名的开源地图服务引擎，支持OGC WFS、WMS、WPS等协议，易于部署，有大量的用户）作为地图服务引擎用于空间数据渲染和地图发布，前端客户端采用Leaflet或openlayers框架，同时支持PC/Android/iOS多种类型终端。地图发布的主要流程包括三步：第一步在时空数据库中导入业务数据后；第二步通过GeoServer关联数据库；第三步选择需要发布的图层，并对图层设定相应对式样。</p><p><b>人员监护</b></p><p>人员监护应用适用对儿童和老人监护，方便实时查看活动轨迹、健康指标（体温、血压、心跳等）；并设定电子围栏（特定区域，比如学校、小区、公园等），当活动人员离开特定区域时触发告警信息。</p><p><b>方案架构</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-a5baabec84776c644dac36c0d63ee39a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"794\" data-rawheight=\"581\" class=\"origin_image zh-lightbox-thumb\" width=\"794\" data-original=\"https://pic3.zhimg.com/v2-a5baabec84776c644dac36c0d63ee39a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;794&#39; height=&#39;581&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"794\" data-rawheight=\"581\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"794\" data-original=\"https://pic3.zhimg.com/v2-a5baabec84776c644dac36c0d63ee39a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-a5baabec84776c644dac36c0d63ee39a_b.jpg\"/></figure><p>时空数据库存储时空和指标数据，并提供空间查询功能，提供后端支持。GeoServer作为地图服务引擎用于空间数据渲染和地图发布，前端客户端采用Leaflet或openlayers框架。电子围栏服务用于判断移动目标同电子围栏的空间关系，并触发告警信息。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>车辆监控</b></p><p>车辆监控应用适用于查看车辆当前和历史轨迹，对车辆的行驶区域做限定，当脱离特定路线后能够报警；并对车辆传感器获取一些参数（比如车速、胎压、电池电压等）做实时监测。</p><p><b>方案架构</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-8f98ea3a077ee7ef26dee0919ba815a4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"850\" data-rawheight=\"581\" class=\"origin_image zh-lightbox-thumb\" width=\"850\" data-original=\"https://pic1.zhimg.com/v2-8f98ea3a077ee7ef26dee0919ba815a4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;850&#39; height=&#39;581&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"850\" data-rawheight=\"581\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"850\" data-original=\"https://pic1.zhimg.com/v2-8f98ea3a077ee7ef26dee0919ba815a4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-8f98ea3a077ee7ef26dee0919ba815a4_b.jpg\"/></figure><p>时空数据库作为存储轨迹及监测指标，提供空间及指标查询功能，提供后端支持。GeoServer作为地图服务引擎用于空间数据渲染、地图发布、时空数据入库，前端客户端采用Leaflet或openlayers框架。电子围栏服务用于判断移动目标同电子围栏的空间关系，并触发告警信息。电子围栏在这里起到过滤器，再地图服务器的WFS服务写入定位和传感器监测数据。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>物流配送</b></p><p>物流配送应用适合于物流行业，提供导航规划功能，并对物流过程做全程监控。</p><p><b>方案架构</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-15ad146a6b134d4cd0476b4f6bded0a6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1396\" data-rawheight=\"1160\" class=\"origin_image zh-lightbox-thumb\" width=\"1396\" data-original=\"https://pic3.zhimg.com/v2-15ad146a6b134d4cd0476b4f6bded0a6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1396&#39; height=&#39;1160&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1396\" data-rawheight=\"1160\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1396\" data-original=\"https://pic3.zhimg.com/v2-15ad146a6b134d4cd0476b4f6bded0a6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-15ad146a6b134d4cd0476b4f6bded0a6_b.jpg\"/></figure><p>时空数据库作为存储与路径规划引擎，提供后端支持。GeoServer作为地图服务引擎用于空间数据渲染、地图发布、时空数据入库，前端客户端采用Leaflet或openlayers框架。在时空数据库存储路网数据，路网数据是做导航规划的基础；在客户端选择起始点和目的地后，由时空数据库计算最佳导航路线，经客户端确认后把导航路线推送给物流终端。时空数据库充当两个角色：轨迹数据存储和导航路径计算。从物流终端获取的轨迹数据通过地图服务器WFS服务存入时空数据库。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>轨迹分析</b></p><p>轨迹分析用于计算轨迹之间的关系以及轨迹与专题地图之间的关系；轨迹分析可以用于分析道路拥堵时空特征、人员活动热点区域、异常行驶车辆等，适用业务场景非常广，比如可以用于商业选址、交通优化、公共安全等。</p><p><b>方案架构</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-de2d8de5b83a564aeaedba0b843b4eef_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"947\" data-rawheight=\"581\" class=\"origin_image zh-lightbox-thumb\" width=\"947\" data-original=\"https://pic4.zhimg.com/v2-de2d8de5b83a564aeaedba0b843b4eef_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;947&#39; height=&#39;581&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"947\" data-rawheight=\"581\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"947\" data-original=\"https://pic4.zhimg.com/v2-de2d8de5b83a564aeaedba0b843b4eef_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-de2d8de5b83a564aeaedba0b843b4eef_b.jpg\"/></figure><p>地图服务器（GeoServer）接收轨迹输入，轨迹和其它监测数据存入时空数据库；轨迹关联计算用于轨迹聚合计算，识别轨迹之间的关系（如轨迹聚类）和轨迹与地图之间的关系（如以道路作为专题图，车辆轨迹的密集程度反应道路的拥堵情况）。轨迹关联计算涉及大量的时空查询需要利用时空数据库做加速处理。 </p><h2><b>总结</b></h2><p> 时空数据库通过融合时序和空间数据模型，来满足不同时空数据场景的要求，更贴近业务；提供多元化索引（空间索引和时序索引等）来满足不同类型场景条件查询需求；提供自动分片及自动删除过旧数据策略，来降低用户管理成本，提升便利性。同时还在稳定性、可靠性、运维上提供优化服务，让用户能够在融合的PostgreSQL生态下，更专注于自己的业务。</p><p><b>欢迎加入阿里数据库产品钉钉群，一起交流。</b></p><p>阿里数据库技术交流群（600人+大群）入群方式：<br/>一：搜索钉群号即可入群：23124548<br/>二：扫描下方二维码进群：</p><p>阿里数据库技术交流群</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-4071982e3bd90b5f0eb5f453faf5d861_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"206\" data-rawheight=\"206\" class=\"content_image\" width=\"206\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;206&#39; height=&#39;206&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"206\" data-rawheight=\"206\" class=\"content_image lazy\" width=\"206\" data-actualsrc=\"https://pic2.zhimg.com/v2-4071982e3bd90b5f0eb5f453faf5d861_b.jpg\"/></figure><p></p><p></p>", 
            "topic": [
                {
                    "tag": "数据库", 
                    "tagLink": "https://api.zhihu.com/topics/19552067"
                }, 
                {
                    "tag": "时空", 
                    "tagLink": "https://api.zhihu.com/topics/19577718"
                }, 
                {
                    "tag": "GIS（地理信息系统）", 
                    "tagLink": "https://api.zhihu.com/topics/19563516"
                }
            ], 
            "comments": [
                {
                    "userName": "leopod", 
                    "userLink": "https://www.zhihu.com/people/74995d0d76f9055190b15baee53439a2", 
                    "content": "不就是用了开源gis那套吗？就是自己的了？可以卖钱了？", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "知乎用户", 
                            "userLink": "https://www.zhihu.com/people/0", 
                            "content": "<p>卖服务没啥问题吧</p>", 
                            "likes": 0, 
                            "replyToAuthor": "leopod"
                        }
                    ]
                }, 
                {
                    "userName": "Quant Learner", 
                    "userLink": "https://www.zhihu.com/people/e0ba932e1a6fb4074e13e93f8ae72cb3", 
                    "content": "<p>群满了，还有其他群可以加么</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/63902852", 
            "userName": "Jeff", 
            "userLink": "https://www.zhihu.com/people/c2e171b39183c7a115a48cfa68378f25", 
            "upvote": 12, 
            "title": "重磅 | 物联网数据分析利器 阿里云发布时序数据库InfluxDB版", 
            "content": "<p>近年来，由于IOT，APM等系统的需求，一种以时间戳为主键的数据模型，越来越流行，存储该数据模型的数据库被称为时序数据库。<br/>若干年中，市面上出现了很多种不同的时序数据库，他们或数据模型不同，或生态不同，或存储架构不同。经过数年的发展，InfluxDB一枝独秀，在DB-Engines中，遥遥领先其他的时序数据库，成为最受用户欢迎的数据库之一。<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-e8785bdee6861f49ca1991ec380d7c97_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1568\" data-rawheight=\"668\" class=\"origin_image zh-lightbox-thumb\" width=\"1568\" data-original=\"https://pic4.zhimg.com/v2-e8785bdee6861f49ca1991ec380d7c97_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1568&#39; height=&#39;668&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1568\" data-rawheight=\"668\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1568\" data-original=\"https://pic4.zhimg.com/v2-e8785bdee6861f49ca1991ec380d7c97_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-e8785bdee6861f49ca1991ec380d7c97_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>阿里云时序数据库InfluxDB®️版上线</b></p><p>为了满足广大物联网企业客户的对于InfluxDB的商业化需求， 阿里云时序数据库团队正式推出时序数据InfluxDB®️版。 时序数据InfluxDB®️版是基于开源InfluxDB提供的商业化时序数据库服务，免部署，零运维，高可靠，提供7*24小时专家答疑服务。</p><p>现在已经开始全面公测。<br/>公测购买页面：<br/><a href=\"https://link.zhihu.com/?target=https%3A//common-buy.aliyun.com/%3Fspm%3D5176.11451019.0.0.144575d16d7RE1%26commodityCode%3Dhitsdb_influxdb_pre%26accounttraceid%3D7545ae3e-f0e4-4df9-a0fc-b931328048fd%23/buy\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">common-buy.aliyun.com/?</span><span class=\"invisible\">spm=5176.11451019.0.0.144575d16d7RE1&amp;commodityCode=hitsdb_influxdb_pre&amp;accounttraceid=7545ae3e-f0e4-4df9-a0fc-b931328048fd#/buy</span><span class=\"ellipsis\"></span></a></p><p>复制链接到网页端 或扫描下方二维码，即可查看相关文档：<br/><a href=\"https://link.zhihu.com/?target=https%3A//help.aliyun.com/document_detail/113093.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">help.aliyun.com/documen</span><span class=\"invisible\">t_detail/113093.html</span><span class=\"ellipsis\"></span></a></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-b0d5c83205fce4c8ba0947ff14610a78_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"281\" data-rawheight=\"281\" class=\"content_image\" width=\"281\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;281&#39; height=&#39;281&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"281\" data-rawheight=\"281\" class=\"content_image lazy\" width=\"281\" data-actualsrc=\"https://pic1.zhimg.com/v2-b0d5c83205fce4c8ba0947ff14610a78_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>InfluxDB场景</b></p><p>InfluxDB是因为物联网而兴起的数据库，其天生具有IOT的特性。几乎所有的物联网数据都可以通过InfluxDB存储，分析与展示。</p><p>InfluxDB的具体使用场景包括：智慧物联网监控分析系统，传统石油化工、采矿以及制造企业设备数据采集与分析，医疗数据采集与分析，车联网，智慧交通等。InfluxDB同时还可以用于日志数据存储与分析，各种服务、软件以及系统监控数据采集、分析与报警，金融数据采集与分析等。</p><p>总之，只要符合写多读少、无事务要求、海量高并发持续写入、基于时间区间聚合分析以及基于时间区间快速查询的数据都可以使用InfluxDB。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ceafa1f6749ff40a970dfce4e59fe4bc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1504\" data-rawheight=\"486\" class=\"origin_image zh-lightbox-thumb\" width=\"1504\" data-original=\"https://pic1.zhimg.com/v2-ceafa1f6749ff40a970dfce4e59fe4bc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1504&#39; height=&#39;486&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1504\" data-rawheight=\"486\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1504\" data-original=\"https://pic1.zhimg.com/v2-ceafa1f6749ff40a970dfce4e59fe4bc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-ceafa1f6749ff40a970dfce4e59fe4bc_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>InfluxDB介绍</b></p><p>InfluxDB之所以能在众多时序数据库中成为DB-Engines中排名第一的时序数据库，来源它的几大优势：</p><ul><li>完整的生态</li><li>易用性</li></ul><p><b>完整的生态TICK</b><br/>InfluxDB不仅仅只提供存储服务，还提供了周边的工具，他们包括Telegraf, Chronograf以及Kapacitor。加上InfluxDB，他们的首字母恰好组成一个秒钟跳动一次的英文单词TICK。</p><p><b>Telegraf</b>: Telegraf是一个开源的时序数据收集器。它收集各种不同的时序数据，并把数据通过InfluxDB标准API发送给InfluxDB。Telegraf采用插件的方式，目前支持100多种不同服务的时序数据收集，用户可以开发自定义的插件收集数据。</p><p><b>Chronograf</b>: Chronograf是整个TICK生态的UI界面层。它让用户可以通过图形界面展现InfluxDB中的数据，同时它可以配置InfluxDB参数以及收集Kapacitor发送的报警信息</p><p><b>Kapacitor</b>: Kapacitor是一个事件处理及报警引擎，它能够根据建立的规则对异常时序数据进行报警，同时能够将这些警告发送给其他系统。</p><p>通过使用TICK生态，用户能轻松构建一个时序数据收集，存储，分析以及告警的完整系统。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-ddd206154c5da2131bffc8dc3aa369f5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"970\" data-rawheight=\"500\" class=\"origin_image zh-lightbox-thumb\" width=\"970\" data-original=\"https://pic2.zhimg.com/v2-ddd206154c5da2131bffc8dc3aa369f5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;970&#39; height=&#39;500&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"970\" data-rawheight=\"500\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"970\" data-original=\"https://pic2.zhimg.com/v2-ddd206154c5da2131bffc8dc3aa369f5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-ddd206154c5da2131bffc8dc3aa369f5_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>InfluxDB易用性</b></p><p>InfluxDB数据写入以及数据查询特别方便，其读写采用的是Restful API，用户可以通过HTTP/HTTPS方式直接读写数据。</p><p><b>数据写入</b></p><p>InfluxDB数据采用行协议方式写入。下面是一个行协议的示例数据：<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-1e8a00644de51807fbd86132b5406902_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"165\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-1e8a00644de51807fbd86132b5406902_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;165&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"165\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-1e8a00644de51807fbd86132b5406902_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-1e8a00644de51807fbd86132b5406902_b.png\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>InfluxDB支持通过curl直接将数据写入InfluxDB：</p><blockquote>curl -i -XPOST &#39;<a href=\"https://link.zhihu.com/?target=https%3A//localhost%3A8086/write%3Fdb%3Dmydb\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">localhost:8086/write?</span><span class=\"invisible\">db=mydb</span><span class=\"ellipsis\"></span></a>&#39; --data-binary &#39;temperature,machine=unit42,type=assembly &gt;external=25,internal=37 1434055562000000000&#39;</blockquote><p>​<br/>同时，用户也可以通过InfluxDB提供Command Line Interface（命令行界面）写入数据：</p><p>INSERT weather,location=us-midwest temperature=82 1465839830100400200<br/>​<br/>InfluxDB提供的多样化数据插入方式，可以让用户在不同平台快速的插入数据。</p><p><b>数据查询</b></p><p>InfluxDB提供SQL-like的查询语句：InfluxQL。InfluxSQL支持SQL风格的查询操作，关系型数据库的用户可以无缝切换到InfluxDB的使用。例如：从measurement h2o_feet查询5条记录：</p><blockquote>SELECT * FROM h2o_feet LIMIT 5<br/>name: h2o_feet<br/>time level description location water_level<br/>2015-08-18T00:00:00Z below 3 feet santa_monica 2.064<br/>2015-08-18T00:00:00Z between 6 and 9 feet coyote_creek 8.12<br/>2015-08-18T00:06:00Z between 6 and 9 feet coyote_creek 8.005<br/>2015-08-18T00:06:00Z below 3 feet santa_monica 2.116<br/>2015-08-18T00:12:00Z between 6 and 9 feet coyote_creek 7.887</blockquote><p>​<br/>关于InfluxQL的具体用法，可以参考阿里云时序数据库InfluxDB®️的文档：数据探索<br/>（<a href=\"https://link.zhihu.com/?target=https%3A//help.aliyun.com/document_detail/113131.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">help.aliyun.com/documen</span><span class=\"invisible\">t_detail/113131.html</span><span class=\"ellipsis\"></span></a>）</p><p>InfluxQL支持按照时间戳对数据分组的查询方式，只需要在influxQL后加上group by(分组时间间隔)即可：<br/>​</p><blockquote>SELECT MAX(&#34;water_level&#34;) FROM &#34;h2o_feet&#34; WHERE &#34;location&#34;=&#39;coyote_creek&#39; AND time &gt;= &#39;2015-09-18T16:00:00Z&#39; AND time &lt;= &#39;2015-09-18T16:42:00Z&#39; GROUP BY time(12m)<br/>name: h2o_feet<br/>time max<br/>2015-09-18T16:00:00Z 3.599<br/>2015-09-18T16:12:00Z 3.402<br/>2015-09-18T16:24:00Z 3.235<br/>2015-09-18T16:36:00Z</blockquote><p>group by的具体用法请参考：数据探索</p><p><b>先进的时序数据分析技术</b></p><p>InfluxQL除了支持SQL-like的查询语句，提供了大量的函数支持对时序数据进行分析。这些分析函数分为四大类：</p><p>Aggregation（聚合），Selector（选择），Transformation（转换）和预测（Prediction）。这些分析函数能够帮助用户轻松地时序数据转化为有用的信息。</p><p>除此之外，InfluxDB提供8种不同的分析技术，用户无需自己用InfluxQL实现这几种分析技术，可以直接使用这些分析技术进行金融以及投资方面的数据分析。</p><p>InfluxQL函数以及分析技术的具体用法请参考：InfluxQL函数<br/>（<a href=\"https://link.zhihu.com/?target=https%3A//help.aliyun.com/document_detail/113126.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">help.aliyun.com/documen</span><span class=\"invisible\">t_detail/113126.html</span><span class=\"ellipsis\"></span></a>）</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>阿里云时序数据库InfluxDB®️版优势</p><p>阿里云时序数据库InfluxDB®️版，在完全兼容InfluxDB行协议以及InfluxQL的基础上做了很多改进，给用户更多稳定、可靠、方便的服务。</p><p><b>数据高可靠</b><br/>阿里云时序数据库InfluxDB®️版的数据存储在阿里云的高效云盘上，高效云盘提供99.9999999%数据高可靠的保障。这样可以保障InfluxDB中数据一旦写入，就永远不会丢失。</p><p><b>高稳定性</b><br/>阿里云时序数据库InfluxDB®️版实现了对内存，硬盘等资源的有效管理，可以极大地减少由于硬件资源不够引起的InfluxDB不稳定的情况。</p><p><b>数据图形化展示</b><br/>阿里云时序数据库InfluxDB®️版与grafana，chronograf等图形展示平台无缝链接，用户购买阿里云时序数据库InfluxDB®️，阿里云自动为用户配置好图形展示平台的数据源，用户直接到图形展示平台完成自己需要的图形化dashboard。（公测结束后提供该功能）<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-b14597b2af3ee3cc1ae8d9ad5f66f416_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"253\" class=\"origin_image zh-lightbox-thumb\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-b14597b2af3ee3cc1ae8d9ad5f66f416_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1080&#39; height=&#39;253&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1080\" data-rawheight=\"253\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1080\" data-original=\"https://pic3.zhimg.com/v2-b14597b2af3ee3cc1ae8d9ad5f66f416_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-b14597b2af3ee3cc1ae8d9ad5f66f416_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>全自动化数据迁移工具</b><br/>阿里云时序数据库InfluxDB®️版提供全自动的数据迁移工具，用户“一键式”将自建的InfluxDB迁移到阿里云时序数据库InfluxDB®️版。</p><p><b>全自动Failover</b><br/>阿里云时序数据库InfluxDB®️版提供全自动Failover机制，一旦所在硬件发生不可恢复的故障，InfluxDB会在非常短的时间内使用其他硬件替换故障硬件，然后重启InfluxDB。这样可以减少因为不可控故障引发的服务中断时间。该Failover是全自动的，无需人工干预，即使是在节假日以及午夜，用户也无需担心服务由于硬件故障造成的长时间不可用。（全自动Failover公测期间暂时不可用）</p><p><b>高可用版InfluxDB®️</b><br/>阿里云时序数据库InfluxDB®️版将在公测结束以后提供高可用版本。高可用版将提供更加稳定的服务，对稳定性要求比较高的用户，将会从中感受到无限稳定的服务。</p><p><b>7*24小时阿里云专业维护</b><br/>阿里云为阿里云时序数据库InfluxDB®️版提供7*24小时，用户可以通过钉钉，微信随时得到专业的维护。咨询群（钉钉群）如下：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-026445bfdfb0a84b18ae59963334c26c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"372\" data-rawheight=\"556\" class=\"content_image\" width=\"372\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;372&#39; height=&#39;556&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"372\" data-rawheight=\"556\" class=\"content_image lazy\" width=\"372\" data-actualsrc=\"https://pic1.zhimg.com/v2-026445bfdfb0a84b18ae59963334c26c_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>总结</b></p><p>阿里云时序数据库InfluxDB®️版不但提供原生InfluxDB的全部优秀功能兼容TICK生态，同时还在稳定性、可靠性、维护上提供优化服务，让用户无限享受InfluxDB优点。</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>扫描下方二维码</b></p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>查看阿里云时序数据库InfluxDB®️版文档</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ff690a5226579262376c1dedef40aaa8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"300\" data-rawheight=\"300\" class=\"content_image\" width=\"300\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;300&#39; height=&#39;300&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"300\" data-rawheight=\"300\" class=\"content_image lazy\" width=\"300\" data-actualsrc=\"https://pic1.zhimg.com/v2-ff690a5226579262376c1dedef40aaa8_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "数据库", 
                    "tagLink": "https://api.zhihu.com/topics/19552067"
                }, 
                {
                    "tag": "时间序列分析", 
                    "tagLink": "https://api.zhihu.com/topics/19712111"
                }, 
                {
                    "tag": "InfluxDB", 
                    "tagLink": "https://api.zhihu.com/topics/20062289"
                }
            ], 
            "comments": [
                {
                    "userName": "知乎用户", 
                    "userLink": "https://www.zhihu.com/people/0", 
                    "content": "人家的开源版本允许商业化吗？", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "老滚的笼中鼠", 
                            "userLink": "https://www.zhihu.com/people/602e5da51a2d908f903fc6cda87b0a0d", 
                            "content": "MIT License的", 
                            "likes": 0, 
                            "replyToAuthor": "知乎用户"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/57121272", 
            "userName": "Jeff", 
            "userLink": "https://www.zhihu.com/people/c2e171b39183c7a115a48cfa68378f25", 
            "upvote": 11, 
            "title": "时序数据库连载系列：Berkeley 的黑科技 BTrDB", 
            "content": "<h2>本文是对面向 IoT 领域的开源时序数据库 BTrDB 内部实现细节的研究和介绍。</h2><h2>1. 场景介绍</h2><p>BTrDB 论文中介绍了一个实际的项目，能很好解释清楚 BTrDB 的设计初衷和适用场景：</p><p>在一个电网中大量部署了某类传感器，每个传感器会产生 12 条时间线，每条时间线频率为 120Hz（即每秒 120 个点），时间精度为 100ns；由于各种原因，传感器数据传输经常性出现延迟、（时间）乱序等。BTrDB 在该场景下单机能支撑 1000 个类似的传感器，写入速率约 1.44M points/s。</p><p>该项目有这样几个特点：<br/>1. 时间线在很长时间内有一定的不变性，其生命周期跟(IoT)设备周期一致<br/>2. 数据频率很高（120 Hz）且固定<br/>3. 数据的时间分辨率很高（100ns级别），一般如Druid，TimescaleDB 时间精度最多做到 ms 级别<br/>4. 数据传输经常性出现乱序<br/>5. 时间线数量有限</p><p>BTrDB 为了适应上述场景，设计并提出了 &#34;time-partitioning version-annotated copy-on-write tree&#34; 的数据结构，为每一条时间线构建了一棵树（可以参考B+Tree），数据在该树中按照时间戳排序，叶子节点有序得存放某个时间段内的时序数据。<br/>可以想见，这棵树的生命周期跟设备的生命周期直接挂钩，因此随着时间的发展，这棵树即使只包含一条时间线，也会占用很可观的存储空间（约 10M points/day）；另外由于是基于树结构，并且引入了版本（version-annotated) 的概念，BTrDB 可以很好的支持乱序数据和任意精度的时序数据。</p><p>由于数据结构不同于以往时序数据库基于LSM-Tree的变种，因此 BTrDB 还提供了一套新的时序数据查询接口，方便在 BTrDB 上层构建各种算法和应用。</p><h2>2. 数据结构</h2><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-ac6e044d455fd05c138ff909284280c9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"962\" data-rawheight=\"792\" class=\"origin_image zh-lightbox-thumb\" width=\"962\" data-original=\"https://pic2.zhimg.com/v2-ac6e044d455fd05c138ff909284280c9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;962&#39; height=&#39;792&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"962\" data-rawheight=\"792\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"962\" data-original=\"https://pic2.zhimg.com/v2-ac6e044d455fd05c138ff909284280c9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-ac6e044d455fd05c138ff909284280c9_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>2.1 version-annotated &amp; CoW 特性</h2><p>在数据写入时，BTrDB 会先修改在内存中的数据块（新建或者使用 CoW 机制修改已有块），当数据达到一定阈值时再写回底层块存储；由于 CoW 机制，并且底层块存储（默认使用Ceph）无法覆盖更新，因此只能创建一个新版本的数据块。</p><h2>2.2. 叶子节点</h2><p>对于 IoT 设备发来的数据，由于频率固定，这些叶子节点占用空间大小基本一致。</p><p>叶子节点还未持久化到底层存储时，在内存中通过数组的方式分别存放时间戳和（双精度浮点）值；在序列化到底层存储时，会通过delta-delta的方式压缩时间戳和值；在序列化双精度浮点数值之前，会将浮点数据数拆分为尾数和指数两部分，并分别进行delta-delta压缩。</p><h2>2.3. 中间节点:</h2><p>中间节点被划分为多个 bucket，每个 bucket 中存放着指向子节点的链接（带版本号）以及子节点的统计数据：</p><ul><li>子节点的时间范围</li><li>聚合数据，如 sum, max, min，count 等</li><li>子节点连接地址和版本号</li></ul><p>在处理查询时，如果中间节点的时间精度满足查询需求，查询操作就不再读取下层子节点了，这样就很自然的实现了将精度功能；这种实现方式，能够很好的处理乱序、重复数据以及删除操作，并且与其他现有的实现相比，能够很好的保证数据的一致性。</p><h2>2.4. 插入时树的分裂</h2><p>一棵新的树（对应一条新的时间线）只有一个根节点，在 BTrDB 的实现中，根节点时间跨度约为146.23年，这样每个 bucket 的时间跨度为 146.23/64 ~= 2.28 （年），根据默认配置，1970年在根节点的第16个 bucket。</p><p>可见，根节点在创建时就已经限制了数据的时间范围，后续数据的插入是自顶向下逐层分裂的，当数据因为丢失等原因造成时间线不完整时，部分节点的深度可能会不一样，因此并不是一颗严格的平衡树。数据插入过程如下：</p><ul><li>数据插入操作从根节点开始；</li><li>如果当前节点是中间节点，则遍历每个数据，为每个数据找到对应的 bucket；</li><li>如果对应的 bucket 不存在，则创建新的 bucket 和与该 bucket 关联的子节点：</li><ul><li>如果当前 bucket 待插入的点个数超过叶子节点最大点数（默认1024），则直接创建中间子节点；</li><li>否则，创建叶子节点；</li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>将数据插入到与所属 bucket 关联的子节点中；</li><li>如果当前节点是叶子节点，节点中数据个数和待插入数据个数总合超过 1024 个点，则分裂当前节点创建出新的中间节点，将数据插入新的中间节点；否则将待插入数据和当前节点已有数据合并，并按照时间戳排序；</li><li>当前节点插入成功后，自底向上更新父节点的统计信息；</li></ul><p>从上面过程可以看到，节点在插入时在两个地方可能出现分裂。一个是从根节点开始，自顶向下分裂；另一个是从叶子节点开始，向上分裂。</p><p>虽然这棵树并不是一颗平衡树，但是对于 IoT 类项目，设备的时间线生命周期、数据采集频率很稳定，在绝大多数场景下，节点中数据都是均衡分布的。</p><h2>2.5 节点占用的内存空间</h2><p>在默认实现中，叶子节点中最多存储1024个数据点；中间节点中最多存储64个子节点指针，因此：</p><ul><li>对于还未持久化的叶子节点，占用的内存空间为：1024*2*8 = 16K（见 2.2.1）</li><li>对于还未持久化的中间节点，占用的内存为：64*6*8 + 64*2*8 = 4K（见 2.2.2）</li></ul><h2>3. 数据存储相关</h2><h2>3.1 写 WAL</h2><ul><li>在数据插入时，会先将数据写入到 WAL(Write Ahead Log) 中；</li><li>每次写 WAL 都会返回一个check point，代表数据在WAL中的写入位置；</li><li>WAL 写入成功后，原始数据和 check point 会被写入时间线的缓冲区；</li><li>时间线的缓冲与时间线一一对应，最大容纳32768个数据点；</li><li>当缓冲区满时，数据会被插入到树结构中，并将该缓冲区对应的 check points 在 WAL 中标记为删除状态；</li><li>在 WAL 的 replay 过程中会根据已被删除的 check points 过滤原始数据。</li></ul><p>下面的示意图展示了WAL中 check points 与时间序列缓冲区的关联关系，在缓冲区清空后，BTrDB 会将已经删除的 check points 写入到当前 WAL 对应的块文件的元信息（block attributes）中：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-5237a0cac11390d82b92e4c45dc46ffe_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1514\" data-rawheight=\"686\" class=\"origin_image zh-lightbox-thumb\" width=\"1514\" data-original=\"https://pic3.zhimg.com/v2-5237a0cac11390d82b92e4c45dc46ffe_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1514&#39; height=&#39;686&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1514\" data-rawheight=\"686\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1514\" data-original=\"https://pic3.zhimg.com/v2-5237a0cac11390d82b92e4c45dc46ffe_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-5237a0cac11390d82b92e4c45dc46ffe_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>当一个 WAL 块文件中所有的 check points 都标记为已删除时，该文件就可以从 Ceph 中删除了。当前 WAL 文件大小超过16M时就会创建新的块文件，在理想情况下，块文件都能被及时删除；但是如果某些时间线出现异常，向前文提到的，其缓冲区在 8 小时后才能被回收，那么负责记录这些时间线的 WAL 文件也就只能在8小时后被回收。<br/>这些滞留的 WAL 文件大小只有16M，数量与出现异常的 IoT 设备数量成线性关系，因此需要更多 IoT 设备运行统计数据才能统计其影响。</p><h2>3.2 写 Block</h2><p>BTrDB 的树结构在持久化后会产生两类数据，一个称为 superblock，记录了当前树的最新版本、更新时间、根节点位置等基本信息；另外一个称为 segment，统一包含了树的叶子节点和中间节点的数据。</p><p>superblock 是带版本的，每个版本的 superblock 只占用16Byte，格式为：</p><div class=\"highlight\"><pre><code class=\"language-text\">{root: 根节点位置，8Byte, timestamp: 修改时间，8Byte}</code></pre></div><p>superblock 在 Ceph 中的寻址方式为：</p><div class=\"highlight\"><pre><code class=\"language-text\">块存储 id = uuid.toString() + (version &gt;&gt; 20)\n块存储中的 offset = (version &amp; 0xFFFFF)*16</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-59e116a5715eb1499ad8a3174daafb3e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1008\" data-rawheight=\"608\" class=\"origin_image zh-lightbox-thumb\" width=\"1008\" data-original=\"https://pic3.zhimg.com/v2-59e116a5715eb1499ad8a3174daafb3e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1008&#39; height=&#39;608&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1008\" data-rawheight=\"608\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1008\" data-original=\"https://pic3.zhimg.com/v2-59e116a5715eb1499ad8a3174daafb3e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-59e116a5715eb1499ad8a3174daafb3e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>在 BTrDB 中持久化树结构时叶子节点和中间节点会一并序列化到 segment 中，每个节点的寻址编码方式为：</p><div class=\"highlight\"><pre><code class=\"language-text\">块存储的 id = uuid.toString() + (address &gt;&gt; 24)\n节点在块存储中的偏移量 = (address &amp; 0xFFFFFF)</code></pre></div><p>可以看到， WAL 文件， superblock 块文件以及 segment 块文件大小都是 16M。另外，BTrDB 中没有 compaction，也没有对过期版本数据的清理，只有上文中介绍的对 WAL 的处理，写入放大会很明显。</p><h2>新的查询语义</h2><p>这里只是罗列、简单介绍下 BTrDB 提供的新的查询语义，这些查询语义的提出与 BTrDB 的数据结构有很大关系，或者是为了利用树结构某些特性，或者是为了规避树结构一些不足：</p><ul><li>GetRange(UUID, StartTime, EndTime, Version) → (Version, [(Time, Value)]) 查询时间线在某个时间范围内的详细（原始）数据；</li><li>GetLatestVersion(UUID) → Version 查询时间线最新版本；</li><li>GetStatisticalRange(UUID, StartTime, EndTime, Version, Resolution) → (Version, [(Time, Min, Mean, Max, Count)]) 获取给定时间范围内，满足一定时间精度的，大于等于给定版本的时间线的聚合数据；</li><li>GetNearestValue(UUID, Time, Version, Direction) → (Version, (Time, Value)) 向前、向后获取下一个点；</li><li>ComputeDiff(UUID, FromVersion, ToVersion, Resolution) → [(StartTime, EndTime)] 在满足给定时间精度条件下，获取两个版本号范围内，所有更新节点的起止时间；适合做增量计算。</li></ul><p>上面接口中的时间分辨率参数（Resolution），对于接口的性能有很大影响。前文提到根节点的时间分辨率是2.2年，从树的根节点到底层节点，节点中数据的时间分辨率越来越高；在查询时，低分辨率数据聚合程度高，扫面的数据块少；高分辨率的数据聚合程度低，但是需要扫面的数据块很多。</p><h2>总结</h2><p>BTrDB 中数据结构是针对单条时间线构建的，并且针对 IoT 设备数据稳定的特点，构建了一棵树来存储时序数据；树结构解决了传统 TSDB 在乱序插入、删除、预降精度等方面面临的问题。</p><p></p>", 
            "topic": [
                {
                    "tag": "互联网", 
                    "tagLink": "https://api.zhihu.com/topics/19550517"
                }, 
                {
                    "tag": "数据库", 
                    "tagLink": "https://api.zhihu.com/topics/19552067"
                }, 
                {
                    "tag": "大数据", 
                    "tagLink": "https://api.zhihu.com/topics/19740929"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/57120725", 
            "userName": "Jeff", 
            "userLink": "https://www.zhihu.com/people/c2e171b39183c7a115a48cfa68378f25", 
            "upvote": 3, 
            "title": "时序数据库连载系列: RISElab的大杀器Confluo", 
            "content": "<h2>挑战</h2><p>随着越来越多的应用达到每秒千万级的数据点采集能力，比如终端IoT网络监控，智能家居，数据中心等等。 并且这些数据被应用于在线查询展示，监控，离线根因分析和系统优化。 这些场景要求系统具备高速写入，低延迟的在线查询以及低开销的离线查询的能力。 然而已有的数据结构很难满足这些要求。有些数据结构侧重与高速的写入和简单的查询， 有些则侧重于复杂的查询，比如即席查询，离线查询，雾化视图等等，增加了维护开销，牺牲了写入的性能。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-0a9f50cae9da2f2d3eb9904366a430de_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"374\" data-rawheight=\"271\" class=\"content_image\" width=\"374\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;374&#39; height=&#39;271&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"374\" data-rawheight=\"271\" class=\"content_image lazy\" width=\"374\" data-actualsrc=\"https://pic3.zhimg.com/v2-0a9f50cae9da2f2d3eb9904366a430de_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>为了解决这些挑战，Confluo应运而生。</p><h2>前提和典型应用场景</h2><p>Confluo之所以可以同时实现几个挑战目标，是因为在一些场景上做了取舍。一个典型的场景是遥感数据</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-1a1837caf5636da7d6fd91270dc770b3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"854\" data-rawheight=\"202\" class=\"origin_image zh-lightbox-thumb\" width=\"854\" data-original=\"https://pic4.zhimg.com/v2-1a1837caf5636da7d6fd91270dc770b3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;854&#39; height=&#39;202&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"854\" data-rawheight=\"202\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"854\" data-original=\"https://pic4.zhimg.com/v2-1a1837caf5636da7d6fd91270dc770b3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-1a1837caf5636da7d6fd91270dc770b3_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>这些遥测数据有以下几个非常重要的特点：</p><ul><li>write-once: 数据追加写,无更新和删除</li><li>定长的数据类型</li><li>并发场景下没有事务，只保证原子性</li></ul><p>针对这些数据特点，Confluo实现了一个创新型数据结构来实现高吞吐，在线/离线查询。</p><h2>特性</h2><p>Confluo面向实时监控和数据流分析场景，比如网络监控和诊断框架，时序数据库，pub-sub的消息系统，主要特性包括：</p><ol><li>百万级数据点高并发写入</li><li>毫秒级在线查询</li><li>占用很少的的CPU资源实现即席查询</li></ol><h2>实现概要</h2><p>Confluo的基本存储抽象是新型的数据结构”Atomic MultiLog“，后面文章简称“AM”, AM依赖于2个关键性技术：</p><ul><li>AM是无锁并发日志集合，可以用来存储原始数据，聚合统计，雾化视图。每一个日志记录writeTail和readTail并发读写。</li><li>日志更新采用现代CPU硬件支持的原子指令集：AtomicLoad，AtomicStore，FetchAndAdd，CompareAndSwap</li></ul><p>AM在接口方面同数据库的表类似，所以应用在使用时首先创建一个固定schema的AM对象。然后按照这个schema写入数据流。并且创建索引(index)，过滤器(filter)，聚合器(aggregate)以及触发器(trigger)等等用于监控和诊断。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-7724c6a196ec04151889111ff35c03a7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"250\" data-rawheight=\"285\" class=\"content_image\" width=\"250\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;250&#39; height=&#39;285&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"250\" data-rawheight=\"285\" class=\"content_image lazy\" width=\"250\" data-actualsrc=\"https://pic4.zhimg.com/v2-7724c6a196ec04151889111ff35c03a7_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>Confluo 数据模型</h2><ul><li>Confluo的数据模型是强类型集合。</li><li>原生数据类型：BOOL, CHAR, SHORT, INT, LONG, FLOAT, DOUBLE, STRING.</li></ul><p> {<br/>  timestamp: ULONG,<br/>  op_latency_ms: DOUBLE,<br/>  cpu_util: DOUBLE,<br/>  mem_avail: DOUBLE,<br/>  log_msg: STRING(100)<br/>  }</p><ul><ul><li>时间戳8个字节，如果应用没有写入时间戳，Confluo会内置添加时间戳。</li><li>指标数据包含double类型以及string类型。</li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>自定义类型。自定义类型通过实现属性字段，注册接口，类型获取接口后，就可以作为schema的成员建立数据模型，追加数据以及执行filter，trigger等操作。</li></ul><h2>写入</h2><ul><li>创建存储数据的Store File</li><li>创建具有固定Schema的AM AM有3种存储模式：IN_MEMORY， DURABLE， DURABLE_RELAXED</li><ul><li>IN_MEMORY：所有的数据存储在内存中。</li><li>DURABLE：类似写穿的方式，数据持久化到磁盘</li><li>DURABLE_RELAXED：数据在内存中缓存，周期性持久化。</li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>执行AM基本操作。 AM定义了Index，Filter, Aggregate, trigger</li><ul><li>添加Index, 应用层可以为每一个指标建立K叉树索引.</li><li>添加Filter, filter 由关系和布尔运算符组成，应用于指标的过滤.</li></ul></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-bd9c20df0180439b2e8151f7a7ed4e9b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1896\" data-rawheight=\"550\" class=\"origin_image zh-lightbox-thumb\" width=\"1896\" data-original=\"https://pic4.zhimg.com/v2-bd9c20df0180439b2e8151f7a7ed4e9b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1896&#39; height=&#39;550&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1896\" data-rawheight=\"550\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1896\" data-original=\"https://pic4.zhimg.com/v2-bd9c20df0180439b2e8151f7a7ed4e9b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-bd9c20df0180439b2e8151f7a7ed4e9b_b.jpg\"/></figure><ul><li>添加Aggregate:适用于filter之后的记录聚合，比如： SUM,MIN,MAX,COUNT,AVG</li><li>添加Trigger:是一个布尔条件，适用于结果集上的操作 比如：MAX(latency_ms) &gt; 100</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-f562d14c5104150c35b815f2546290b4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"508\" data-rawheight=\"313\" class=\"origin_image zh-lightbox-thumb\" width=\"508\" data-original=\"https://pic1.zhimg.com/v2-f562d14c5104150c35b815f2546290b4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;508&#39; height=&#39;313&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"508\" data-rawheight=\"313\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"508\" data-original=\"https://pic1.zhimg.com/v2-f562d14c5104150c35b815f2546290b4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-f562d14c5104150c35b815f2546290b4_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>查询</h2><p>Confluo既可以离线查询也可以实时查询，区别在于是否要预定义规则。</p><ul><li>离线查询主要面向诊断分析，如果fExpressio已经定义，增直接查看FilterLog，否则通过IndexLog方式查询原始数据。</li><li>实时流式查询主要面向实时监控和报警，需要预定义规则。比如通过定义triggers实现报警能力，类似SUM(pktSize)&gt;1GB的报警规则定义。</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-e555ed9d6782369a0abcb3c153d2891c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"456\" data-rawheight=\"225\" class=\"origin_image zh-lightbox-thumb\" width=\"456\" data-original=\"https://pic1.zhimg.com/v2-e555ed9d6782369a0abcb3c153d2891c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;456&#39; height=&#39;225&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"456\" data-rawheight=\"225\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"456\" data-original=\"https://pic1.zhimg.com/v2-e555ed9d6782369a0abcb3c153d2891c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-e555ed9d6782369a0abcb3c153d2891c_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>解析器：语法解析的实现采用了spirit.对于这类小型符合EBNF语法规范的数据模型，spirit还是比较灵活。Confluo定义了exp,term,factor,predicate,identificate, value, quoted_string等几种语法规则.</li><li>执行计划：解析器生成表达式后，通过查询计划器生成执行计划为agg-&gt;filter-&gt;index。Confluo内置了一个简单的评估器，根据近似count计算cost判断走index或者full scan。</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-0f3d03bb332c94ad58bd6614aae863c9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"574\" data-rawheight=\"242\" class=\"origin_image zh-lightbox-thumb\" width=\"574\" data-original=\"https://pic2.zhimg.com/v2-0f3d03bb332c94ad58bd6614aae863c9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;574&#39; height=&#39;242&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"574\" data-rawheight=\"242\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"574\" data-original=\"https://pic2.zhimg.com/v2-0f3d03bb332c94ad58bd6614aae863c9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-0f3d03bb332c94ad58bd6614aae863c9_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>数据归档和压缩</h2><p>Confluo除了存储原始数据，同时需要存储索引，预定义的过滤，聚合等等，因此这些数据带来了存储开销的膨胀。通过引入归档方式把部分数据存储到冷设备，从而解决这一问题。目前支持3种数据归档方式：周期性的归档，强制性的归档，基于内存的归档。</p><ul><li>周期性的归档。 默认情况下，每5分钟数据会归档一次。后台归档管理任务周期性检测AM日志配置的大小，一旦超过限制DataLog，IndexLog，FilterLog会归档到冷设备存储。</li><li>强制性的归档 无论归档是否开启，用户都可以调用接口强制性归档。 接口上既支持全量归档，也支持基于偏移量的增量归档。</li><li>基于内存的归档 当周期性归档持续低于高速写入的数据量时内存会溢出，为避免这种情况，引入基于内存大小的归档机制。当系统内存达到自定义的阈值时，内存分配被阻塞，直到所有的AM归档到冷数据。</li><li>编码 归档时HeaderLog默认采用LZ4压缩，IndexLog和FilterLog采用Delta压缩。 解压在读取的时候由底层引擎完成，通过引用计数避免归档线程与读取线程之间的并发性访问。</li></ul><h2>核心技术点</h2><p><b>Atomic MultiLog</b></p><p>Atomic MultiLog是整个系统的核心技术点。主要包括DataLog，IndexLog, FilterLog, AggregateLog以及如何原子性的操作这些日志。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-04dfa3bc6d6eb5188d1d1b92a1dbf419_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"615\" data-rawheight=\"438\" class=\"origin_image zh-lightbox-thumb\" width=\"615\" data-original=\"https://pic2.zhimg.com/v2-04dfa3bc6d6eb5188d1d1b92a1dbf419_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;615&#39; height=&#39;438&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"615\" data-rawheight=\"438\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"615\" data-original=\"https://pic2.zhimg.com/v2-04dfa3bc6d6eb5188d1d1b92a1dbf419_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-04dfa3bc6d6eb5188d1d1b92a1dbf419_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>DataLog 分为2部分offset和原始数据点，offset是原始数据的唯一标识。</li><li>IndexLog 是datalog的索引部分，采用radix树组织索引，radix树是通用的字典类型数据结构，比如在监控场景中的IP地址，网络地址有大量的prefix是可以共享的。</li><li>FilterLog 存储了基于时间窗口切分的原始数据offset，按照radix树索引filter和窗口。</li><li>AggregateLog 同其它日志相似，也是基于时间分片的索引数据方式。由于聚合日志需要读后写，设计了thread-local的集合来保证安全访问。</li></ul><h2>集成方式：</h2><p>Confluo是一个开源C++项目。有2种集成模式：</p><ul><li>可以作为嵌入式的依赖库，支持在线和离线分析。</li><li>可以作为独立的服务，对外暴露RPC接口通信。</li></ul><p><b>总结</b></p><p>大名鼎鼎的riseLab新鲜出炉的Confluo，核心创新在于数据结构Atomic MultiLog，可以支持高速并发读写，单核可以运行1000个trigger，10个Filter。非常好的一个闪光点，找到某个特定业务场景，采用新硬件的原子操作和无锁日志做到了实时，离线，高速写入的统一。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>附录---源码分析</h2><p class=\"ztext-empty-paragraph\"><br/></p><h2>AM写入</h2><p>append data_log_.append //monolog原子性保证 monolog_linear&lt;-&gt;datalog atomic::faa(&amp;tail_, len) write schema_.apply_unsafe //filter更新 filters_.at(i)-&gt;update //index 记录原始日志的偏移量 indexes_.at(f.index_id())-&gt;insert(f.get_key(), offset) data_log_.flush</p><p>add_index add_index_task schema_.get_field_index col.set_indexing() atomic::strong::cas //index 日志。创建radix tree index_log.push_back(adix_index(col.type().size, 256)) //日志操作 monolog_exp2(array&lt;atomic_bucket_ref, NBUCKETS&gt; buckets_) atomic::faa monolog_exp2_base.set(idx, val) col.set_indexed metadata_.write_index_metadata(field_name, bucket_size); adix_index</p><p>add_filter parse_expression compile_expression //cexpr 表达式过滤 filters_.push_back(new filter(cexpr, default_filter)) monolog_exp2.push_back atomic::faa set(idx, val) write_filter_metadata //创建新的filter new filter</p><p>add_aggregate </p><p>add_aggregate_task</p><p>install_trigger add_trigger_task(name, expr, periodicity_ms) filters_.at(aggregate_id.filter_idx) -&gt;get_aggregate_info(aggregate_id.aggregate_idx)</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>AM查询接口：</h2><div class=\"highlight\"><pre><code class=\"language-text\">Offline:\nread\n  data_log_.cptr\n  rptr.decode\nexecute_filter\n  parse_expression\n  compile_expression\n  plan\n  plan.execute\n  execute_filter(&#34;b &gt; 4&#34;)\n\nexecute_aggregate\n  parse_aggregate\n  get_aggregator\n  parse_expression\n  compile_expression\n  planner_.plan\n  plan.execute\n  plan.aggregate\n\nOnline：//在线主要是预定义的name\nquery_filter(预定义filter name)\n  parse_expression\n  compile_expression\n  filter_map_.get filterid\n  lookup_range\n  query_filter(&#34;filter2&#34;, beg, end)\n\nget_aggregate\n  aggregate_map_.get\n  get_aggregate_info\n  get_aggregate\n  comb_op\n  get_aggregate(&#34;agg1&#34;, beg_ms, end_ms)\n\nget_alerts\n  idx_.range_lookup\n  get_alerts(beg, end, &#34;trigger1&#34;)</code></pre></div><p></p>", 
            "topic": [
                {
                    "tag": "互联网", 
                    "tagLink": "https://api.zhihu.com/topics/19550517"
                }, 
                {
                    "tag": "数据库", 
                    "tagLink": "https://api.zhihu.com/topics/19552067"
                }, 
                {
                    "tag": "大数据", 
                    "tagLink": "https://api.zhihu.com/topics/19740929"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/57120174", 
            "userName": "Jeff", 
            "userLink": "https://www.zhihu.com/people/c2e171b39183c7a115a48cfa68378f25", 
            "upvote": 8, 
            "title": "时序数据库连载系列：当SQL遇到时序 TimescaleDB", 
            "content": "<p>1.概述</p><p>TimescaleDB是<a href=\"https://link.zhihu.com/?target=https%3A//www.timescale.com/about\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Timescale Inc.</a>(成立于2015年)开发的一款号称兼容全SQL的 <b>时序数据库</b> 。它的本质是一个基于 <b>PostgreSQL</b>（以下简称 <b>PG</b> ）的扩展（ <b>Extension</b> ），主打的卖点如下：</p><ul><li>全SQL支持</li><li>背靠PostgreSQL的高可靠性</li><li>时序数据的高写入性能</li></ul><p>下文将对TimescaleDB这个产品进行解读。如无特殊说明，这里所说的TimescaleDB均是指<a href=\"https://link.zhihu.com/?target=https%3A//github.com/timescale/timescaledb\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Github上开源的单机版TimescleDB</a>的v1.1版本。</p><h2>2.数据模型</h2><p>由于TimescaleDB的根基还是PG，因此它的数据模型与NoSQL的时序数据库(如我们的阿里时序时空TSDB，InfluxDB等)截然不同。</p><p>在NoSQL的时序数据库中，数据模型通常如下所示，即一条数据中既包括了时间戳以及采集的数据，还包括设备的元数据（通常以 <b>Tagset</b> 体现）。数据模型如下所示：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-2b17c15cf892a2f56e1081e7b10fa87e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1269\" data-rawheight=\"347\" class=\"origin_image zh-lightbox-thumb\" width=\"1269\" data-original=\"https://pic3.zhimg.com/v2-2b17c15cf892a2f56e1081e7b10fa87e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1269&#39; height=&#39;347&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1269\" data-rawheight=\"347\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1269\" data-original=\"https://pic3.zhimg.com/v2-2b17c15cf892a2f56e1081e7b10fa87e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-2b17c15cf892a2f56e1081e7b10fa87e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>但是在TimescaleDB中，数据模型必须以一个二维表的形式呈现，这就需要用户结合自己使用时序数据的业务场景，自行设计定义二维表。</p><p>在TimescaleDB的官方文档中，对于如何设计时序数据的数据表，给出了两个范式：</p><ul><li>Narrow Table</li><li>Wide Table</li></ul><p>所谓的 <b>Narrow Table</b> 就是将metric分开记录，一行记录只包含一个 <b>metricValue - timestamp</b> 。举例如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-25d9815d7e73ee1babf601bb1d7cfe57_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1896\" data-rawheight=\"650\" class=\"origin_image zh-lightbox-thumb\" width=\"1896\" data-original=\"https://pic4.zhimg.com/v2-25d9815d7e73ee1babf601bb1d7cfe57_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1896&#39; height=&#39;650&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1896\" data-rawheight=\"650\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1896\" data-original=\"https://pic4.zhimg.com/v2-25d9815d7e73ee1babf601bb1d7cfe57_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-25d9815d7e73ee1babf601bb1d7cfe57_b.jpg\"/></figure><p>而所谓的 <b>Wide Table</b> 就是以时间戳为轴线，将同一设备的多个metric记录在同一行，至于设备一些属性（元数据）则只是作为记录的辅助数据，甚至可直接记录在别的表（之后需要时通过 <b>JOIN</b> 语句进行查询）</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-20ce9621a337b336fdf59ddf4eb5be4f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"620\" data-rawheight=\"133\" class=\"origin_image zh-lightbox-thumb\" width=\"620\" data-original=\"https://pic4.zhimg.com/v2-20ce9621a337b336fdf59ddf4eb5be4f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;620&#39; height=&#39;133&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"620\" data-rawheight=\"133\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"620\" data-original=\"https://pic4.zhimg.com/v2-20ce9621a337b336fdf59ddf4eb5be4f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-20ce9621a337b336fdf59ddf4eb5be4f_b.jpg\"/></figure><p>基本上可以认为： Narrow Table 对应的就是 <b>单值模型</b> ，而Wide Table对应的就是 <b>多值模型</b></p><p>由于采用的是传统数据库的关系表的模型，所以TimescaleDB的metric值必然是强类型的，它的类型可以是PostgreSQL中的 <b>数值类型</b> ， <b>字符串类型</b> 等。</p><h2>3.TimescaleDB的特性</h2><p>TimescaleDB在PostgreSQL的基础之上做了一系列扩展，主要涵盖以下方面：</p><ol><li>时序数据表的透明自动分区特性</li><li>提供了若干面向时序数据应用场景的特殊SQL接口</li><li>针对时序数据的写入和查询对PostgreSQL的 <b>Planner</b> 进行扩展</li><li>面向时序数据表的定制化并行查询</li></ol><p>其中 <b>3</b> 和 <b>4</b> 都是在PostgreSQL的现有机制上进行的面向时序数据场景的微创新。因此下文将主要对上述的 <b>1</b> 和 <b>2</b> 稍加展开说明</p><h2>透明自动分区特性</h2><p>在时序数据的应用场景下，其记录数往往是非常庞大的，很容易就达到 <b>数以billion计</b> 。而对于PG来说，由于大量的还是使用B+tree索引，所以当数据量到达一定量级后其写入性能就会出现明显的下降（这通常是由于索引本身变得非常庞大且复杂）。这样的性能下降对于时序数据的应用场景而言是不能忍受的，而TimescaleDB最核心的 <b>自动分区</b> 特性需要解决就是这个问题。这个特性希望达到的目标如下：</p><ul><li>随着数据写入的不断增加，将时序数据表的数据分区存放，保证每一个分区的索引维持在一个较小规模，从而维持住写入性能</li><li>基于时序数据的查询场景，自动分区时以时序数据的时间戳为分区键，从而确保查询时可以快速定位到所需的数据分区，保证查询性能</li><li>分区过程对用户透明，从而达到Auto-Scalability的效果</li></ul><p>TimescaleDB对于自动分区的实现，主要是基于PG的<a href=\"https://link.zhihu.com/?target=https%3A//www.postgresql.org/docs/11/tutorial-inheritance.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">表继承</a>机制进行的实现。TimescaleDB的自动分区机制概要可参见下图：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-631922553ce6b6e60843e6c226b08886_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1269\" data-rawheight=\"760\" class=\"origin_image zh-lightbox-thumb\" width=\"1269\" data-original=\"https://pic3.zhimg.com/v2-631922553ce6b6e60843e6c226b08886_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1269&#39; height=&#39;760&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1269\" data-rawheight=\"760\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1269\" data-original=\"https://pic3.zhimg.com/v2-631922553ce6b6e60843e6c226b08886_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-631922553ce6b6e60843e6c226b08886_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>在这个机制下, 用户创建了一张普通的时序表后，通过TimescaleDB的接口进行了hyper table注册后，后续的数据写入和查询操作事实上就由TimescaleDB接手了。上图中，用户创建的原始表一般被称为“主表”(main table), 而由TimescaleDB创建出的隐藏的子表一般被称为“chunk”</p><p>需要注意的是，chunk是伴随着数据写入而自动创建的，每次创建新的chunk时会计算这个chunk预计覆盖的时间戳范围（默认是 <b>一周</b> ）。且为了考虑到不同应用场景下，时序数据写入速度及密度都不相同，对于创建新分区时，新分区的时间戳范围会经过一个自适应算法进行计算，以便逐渐计算出某个应用场景下最适合的时间戳范围。与PG 10.0</p><p>自适应算法的详细实现位于TimescaleDB的chunk_adaptive.c的ts_calculate_chunk_interval()，其基本思路就是基于历史chunk的 <b>时间戳填充因子</b> 以及 <b>文件尺寸填充因子</b> 进行合理推算下一个chunk应该按什么时间戳范围来进行界定。</p><p>借助 <b>透明化自动分区</b> 的特性，根据官方的测试结果，在同样的数据量级下，TimescaleDB的写入性能与PG的 <b>传统单表</b> 写入场景相比，即使随着数量级的不断增大，性能也能维持在一个比较稳定的状态。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-75a4f041e2798f277682646b74f81252_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1725\" data-rawheight=\"1249\" class=\"origin_image zh-lightbox-thumb\" width=\"1725\" data-original=\"https://pic3.zhimg.com/v2-75a4f041e2798f277682646b74f81252_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1725&#39; height=&#39;1249&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1725\" data-rawheight=\"1249\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1725\" data-original=\"https://pic3.zhimg.com/v2-75a4f041e2798f277682646b74f81252_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-75a4f041e2798f277682646b74f81252_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>注： 上述Benchmark测试结果摘自<a href=\"https://link.zhihu.com/?target=https%3A//docs.timescale.com/v1.1/introduction/timescaledb-vs-postgres%23content-frame\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Timescale官网</a></p><h2>面向时序场景的定制功能</h2><p>TimescaleDB的对外接口就是SQL，它100%地继承了PG所支持的全部SQL特性。除此之外，面向时序数据库的使用场景，它也定制了一些接口供用户在应用中使用，而这些接口都是通过 SQL函数（标准名称为 <a href=\"https://link.zhihu.com/?target=https%3A//www.postgresql.org/docs/11/xfunc.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">User-defined Function</a>）予以呈现的。以下列举了一些这类接口的例子:</p><ul><li>time_bucket()函数</li></ul><p>该函数用于 <b>降采样</b> 查询时使用，通过该函数指定一个时间间隔，从而将时序数据按指定的间隔降采样，并辅以所需的聚合函数从而实现降采样查询。一个示例语句如下:</p><p>SELECT time_bucket(&#39;5 minutes&#39;, time)</p><p>  AS five_min, avg(cpu)</p><p>  FROM metrics</p><p>  GROUP BY five_min</p><p>  ORDER BY five_min DESC LIMIT 10;</p><p>将数据点按5分钟为单位做降采样求均值</p><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>新增的聚合函数</li></ul><p>为了提供对时序数据进行多样性地分析查询，TimescaleDB提供了下述新的聚合函数。</p><ul><ul><li>first() 求被聚合的一组数据中的第一个值</li><li>last() 求被聚合的一组数据中的最后一个值</li><li>histogram() 求被聚合的一组数据中值分布的直方图</li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><p>注: 新增的聚合函数在非时序场景也可以使用</p><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>drop_chunks()<br/>删除指定时间点之前/之后的数据chunk. 比如删除三个月时间前的所有chunk等等。这个接口可以用来类比 <b>InfluxDB</b> 的 <b>Retention Policies</b> 特性，但是目前TimescaleDB尚未实现自动执行的chunk删除。若需要完整的 <b>Retention Policies</b> 特性，需要使用系统级的定时任务（如 crontab）加上drop_chunks()语句来实现。</li></ul><p>drop_chunks()的示例语句如下。含义是删除conditions表中所有距今三个月之前以及四个月之后的数据分区:</p><p>SELECT drop_chunks(older_than =&gt; interval &#39;3 months&#39;, newer_than =&gt; interval &#39;4 months&#39;, table_name =&gt; &#39;conditions&#39;);</p><p>除此之外，TimescaleDB定制的一些接口基本都是方便数据库管理员对元数据进行管理的相关接口，在此就不赘述。包括以上接口在内的定义和示例可参见<a href=\"https://link.zhihu.com/?target=https%3A//docs.timescale.com/v1.1/api\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">官方的API文档</a></p><h2>4.TimescaleDB的存储机制</h2><p>TimescaleDB对PG的存储引擎未做任何变更，因此其索引数据和表数据的存储都是沿用的PG的存储。而且，TimescaleDB给chunk上索引时，都是使用的默认的B+tree索引，因此每一个chunk中数据的存储机制可以参见下图：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-41aba7e22aee2ff22d437778ea44fa7b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1269\" data-rawheight=\"709\" class=\"origin_image zh-lightbox-thumb\" width=\"1269\" data-original=\"https://pic4.zhimg.com/v2-41aba7e22aee2ff22d437778ea44fa7b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1269&#39; height=&#39;709&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1269\" data-rawheight=\"709\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1269\" data-original=\"https://pic4.zhimg.com/v2-41aba7e22aee2ff22d437778ea44fa7b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-41aba7e22aee2ff22d437778ea44fa7b_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>关于这套存储机制本身不用过多解释，毕竟TimescaleDB对其没有改动。不过考虑到时序数据库的使用场景，可以发现TimescaleDB的Chunk采用这套机制是比较合适的:</p><ul><li>PG存储的特征是 <b>只增不改</b> ，即无论是数据的插入还是变更。体现在Heap Tuple中都是Tuple的Append操作。因此这个存储引擎在用于OLTP场景下的普通数据表时，会存在<a href=\"https://link.zhihu.com/?target=http%3A//mysql.taobao.org/monthly/2015/12/07/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">表膨胀</a>问题；而在时序数据的应用场景中，时序数据正常情况下不会被更新或删除，因此可以避免表膨胀问题（当然，由于时序数据本身写入量很大，所以也可以认为海量数据被写入的情况下单表实际上仍然出现了膨胀，但这不是此处讨论的问题）</li><li>在原生的PG中，为了解决表膨胀问题，所以PG内存在 <b>AUTOVACUUM</b> 机制，即自动清理表中因更新/删除操作而产生的“Dead Tuple”，但是这将会引入一个新的问题，即AUTOVACUUM执行时会对表加共享锁从而对写入性能的影响。但是在时序数据的应用场景中，由于没有更新/删除的场景，也就不会存在“Dead Tuple“，因此这样的Chunk表就不会成为AUTOVACUUM的对象，因此INSERT性能便不会受到来自这方面的影响。</li></ul><p>至于对海量数据插入后表和索引增大的问题，这正好通过上述的 <b>自动分区</b> 特性进行了规避。</p><p>此外，由于TimescaleDB完全基于PG的存储引擎，对于 <b>WAL</b> 也未做任何修改。因此TimescaleDB的高可用集群方案也可基于PG的流复制技术进行搭建。<a href=\"https://link.zhihu.com/?target=https%3A//blog.timescale.com/high-availability-timescaledb-postgresql-patroni-a4572264a831\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">TimescaleDB官方也介绍了一些基于开源组件的HA方案</a></p><h2>5.小结</h2><p>综上所述，由于TimescaleDB完全基于PostgreSQL构建而成，因此它具有若干与生俱来的优势 ：</p><ul><li>100%继承PostgreSQL的生态。且由于完整支持SQL，对于未接触过时序数据的初学者反而更有吸引力</li><li>由于PostgreSQL的品质值得信赖，因此TimescaleDB在质量和稳定性上拥有品牌优势</li><li>强ACID支持</li></ul><p>当然，它的 <b>短板</b> 也是显而易见的</p><ul><li>由于只是PostgreSQL的一个Extension，因此它不能从内核/存储层面针对时序数据库的使用场景进行极致优化。</li><li>当前的产品架构来看仍然是一个单机库，不能发挥分布式技术的优势。而且数据虽然自动分区，但是由于时间戳决定分区，因此很容易形成I/O热点。</li><li>在功能层面，面向时序数据库场景的特性还比较有限。目前更像是一个 <b>传统OLTP数据库 + 部分时序特性</b> 。</li></ul><p>不管怎样，TimescaleDB也算是面向时序数据库从另一个角度发起的尝试。在当前时序数据库仍然处于新兴事物的阶段，它未来的发展方向也是值得我们关注并借鉴的。</p><p></p>", 
            "topic": [
                {
                    "tag": "互联网", 
                    "tagLink": "https://api.zhihu.com/topics/19550517"
                }, 
                {
                    "tag": "数据库", 
                    "tagLink": "https://api.zhihu.com/topics/19552067"
                }, 
                {
                    "tag": "大数据", 
                    "tagLink": "https://api.zhihu.com/topics/19740929"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/57119093", 
            "userName": "Jeff", 
            "userLink": "https://www.zhihu.com/people/c2e171b39183c7a115a48cfa68378f25", 
            "upvote": 8, 
            "title": "时序数据库连载系列: 时序数据库一哥InfluxDB之存储机制解析", 
            "content": "<p></p><p>InfluxDB 的存储机制解析</p><p>本文介绍了InfluxDB对于时序数据的存储/索引的设计。由于InfluxDB的集群版已在0.12版就不再开源，因此如无特殊说明，本文的介绍对象都是指 InfluxDB <b>单机版</b></p><h2>1. InfluxDB 的存储引擎演进</h2><p>尽管InfluxDB自发布以来历时三年多，其存储引擎的技术架构已经做过几次重大的改动, 以下将简要介绍一下InfluxDB的存储引擎演进的过程。</p><h2>1.1 演进简史</h2><ul><li>版本0.9.0之前</li><li><b>基于 LevelDB的LSMTree方案**</b></li><li>版本0.9.0～0.9.4</li><li><b>基于BoltDB的mmap COW B+tree方案</b></li><li>版本0.9.5～1.2</li><li>基于自研的 WAL + TSMFile 方案（TSMFile方案是0.9.6版本正式启用，0.9.5只是提供了原型）</li><li>版本1.3～至今</li><li><b>基于自研的 WAL + TSMFile + TSIFile 方案</b></li></ul><h2>1.2 演进的考量</h2><p>InfluxDB的存储引擎先后尝试过包括LevelDB, BoltDB在内的多种方案。但是对于InfluxDB的下述诉求终不能完美地支持：</p><ul><li>时序数据在降采样后会存在大批量的数据删除</li></ul><p>=&gt; <i>LevelDB的LSMTree删除代价过高</i></p><ul><li>单机环境存放大量数据时不能占用过多文件句柄</li></ul><p>=&gt; <i>LevelDB会随着时间增长产生大量小文件</i></p><ul><li>数据存储需要热备份</li></ul><p>=&gt; <i>LevelDB只能冷备</i></p><ul><li>大数据场景下写吞吐量要跟得上</li></ul><p>=&gt; <i>BoltDB的B+tree写操作吞吐量成瓶颈</i></p><ul><li>存储需具备良好的压缩性能</li><li>=&gt; <i>BoltDB不支持压缩</i></li></ul><p>此外，出于技术栈的一致性以及部署的简易性考虑（面向容器部署），InfluxDB团队希望存储引擎 与 其上层的TSDB引擎一样都是用GO编写，因此潜在的RocksDB选项被排除</p><p>基于上述痛点，InfluxDB团队决定自己做一个存储引擎的实现。</p><h2>2 InfluxDB的数据模型</h2><p>在解析InfluxDB的存储引擎之前，先回顾一下InfluxDB中的数据模型。</p><p>在InfluxDB中，时序数据支持多值模型，它的一条典型的时间点数据如下所示：</p><p>图 2-1<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-2b17c15cf892a2f56e1081e7b10fa87e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1269\" data-rawheight=\"347\" class=\"origin_image zh-lightbox-thumb\" width=\"1269\" data-original=\"https://pic3.zhimg.com/v2-2b17c15cf892a2f56e1081e7b10fa87e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1269&#39; height=&#39;347&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1269\" data-rawheight=\"347\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1269\" data-original=\"https://pic3.zhimg.com/v2-2b17c15cf892a2f56e1081e7b10fa87e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-2b17c15cf892a2f56e1081e7b10fa87e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>measurement:</li></ul><p>指标对象，也即一个数据源对象。每个measurement可以拥有一个或多个指标值，也即下文所述的<b>field</b>。在实际运用中，可以把一个现实中被检测的对象（如：“cpu”）定义为一个measurement</p><ul><li>tags:</li></ul><p>概念等同于大多数时序数据库中的tags, 通常通过tags可以唯一标示数据源。每个tag的key和value必须都是字符串。</p><ul><li>field:</li></ul><p>数据源记录的具体指标值。每一种指标被称作一个“field”，指标值就是 “field”对应的“value”</p><ul><li>timestamp:</li><li>数据的时间戳。在InfluxDB中，理论上时间戳可以精确到 <b>纳秒</b>（ns）级别</li></ul><p>此外，在InfluxDB中，measurement的概念之上还有一个对标传统DBMS的 <b>Database</b> 的概念，逻辑上每个Database下面可以有多个measurement。在单机版的InfluxDB实现中，每个Database实际对应了一个文件系统的 <b>目录</b>。</p><h2>2.1 Serieskey的概念</h2><p>InfluxDB中的SeriesKey的概念就是通常在时序数据库领域被称为 <b>时间线</b> 的概念, 一个SeriesKey在内存中的表示即为下述字符串(逗号和空格被转义)的 <b>字节数组</b>(<a href=\"https://link.zhihu.com/?target=http%3A//github.com/influxdata/influxdb/model%23MakeKey%28%29\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">github.com/influxdata/i</span><span class=\"invisible\">nfluxdb/model#MakeKey()</span><span class=\"ellipsis\"></span></a>)</p><p>{measurement名}{tagK1}={tagV1},{tagK2}={tagV2},...</p><p>其中，SeriesKey的长度不能超过 65535 字节</p><h2>2.2 支持的Field类型</h2><p>InfluxDB的Field值支持以下数据类型:</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-892fe13c08447313b91e0cb1bbcfdf81_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1880\" data-rawheight=\"692\" class=\"origin_image zh-lightbox-thumb\" width=\"1880\" data-original=\"https://pic2.zhimg.com/v2-892fe13c08447313b91e0cb1bbcfdf81_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1880&#39; height=&#39;692&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1880\" data-rawheight=\"692\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1880\" data-original=\"https://pic2.zhimg.com/v2-892fe13c08447313b91e0cb1bbcfdf81_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-892fe13c08447313b91e0cb1bbcfdf81_b.jpg\"/></figure><p>在InfluxDB中，Field的数据类型在以下范围内必须保持不变，否则写数据时会报错 <b>类型冲突</b>。</p><p><b>同一Serieskey + 同一field + 同一shard</b></p><h2>2.3 Shard的概念</h2><p>在InfluxDB中， <b>能且只能</b> 对一个Database指定一个 <b>Retention Policy</b> (简称:RP)。通过RP可以对指定的Database中保存的时序数据的留存时间(duration)进行设置。而 <b>Shard</b> 的概念就是由duration衍生而来。一旦一个Database的duration确定后, 那么在该Database的时序数据将会在这个duration范围内进一步按时间进行分片从而时数据分成以一个一个的shard为单位进行保存。</p><p>shard分片的时间 与 duration之间的关系如下</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-7a024700970eee758f161624d05b9692_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1886\" data-rawheight=\"384\" class=\"origin_image zh-lightbox-thumb\" width=\"1886\" data-original=\"https://pic3.zhimg.com/v2-7a024700970eee758f161624d05b9692_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1886&#39; height=&#39;384&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1886\" data-rawheight=\"384\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1886\" data-original=\"https://pic3.zhimg.com/v2-7a024700970eee758f161624d05b9692_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-7a024700970eee758f161624d05b9692_b.jpg\"/></figure><p>新建的Database在未显式指定RC的情况下，默认的RC为 <b>数据的Duration为永久，Shard分片时间为7天</b></p><p>注: 在闭源的集群版Influxdb中，用户可以通过RC规则指定数据在基于时间分片的基础上再按SeriesKey为单位进行进一步分片</p><h2>3. InfluxDB的存储引擎分析</h2><p>时序数据库的存储引擎主要需满足以下三个主要场景的性能需求</p><ol><li>大批量的时序数据写入的高性能</li><li>直接根据时间线(即Influxdb中的 <b>Serieskey</b> )在指定时间戳范围内扫描数据的高性能</li><li>间接通过measurement和部分tag查询指定时间戳范围内所有满足条件的时序数据的高性能</li></ol><p>InfluxDB在结合了1.2所述考量的基础上推出了他们的解决方案，即下面要介绍的 <b>WAL + TSMFile + TSIFile的方案</b></p><h2>3.1 WAL解析</h2><p>InfluxDB写入时序数据时为了确保数据完整性和可用性，与大部分数据库产品一样，都是会先写WAL,再写入缓存，最后刷盘。对于InfluxDB而言，写入时序数据的主要流程如同下图所示：</p><p>图 3-5<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-f77666f664c6872bfc2713bfe8e1fd83_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"970\" data-rawheight=\"1494\" class=\"origin_image zh-lightbox-thumb\" width=\"970\" data-original=\"https://pic4.zhimg.com/v2-f77666f664c6872bfc2713bfe8e1fd83_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;970&#39; height=&#39;1494&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"970\" data-rawheight=\"1494\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"970\" data-original=\"https://pic4.zhimg.com/v2-f77666f664c6872bfc2713bfe8e1fd83_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-f77666f664c6872bfc2713bfe8e1fd83_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>InfluxDB对于时间线数据和时序数据本身分开，分别写入不同的WAL中，其结构如下所示：</p><h2>索引数据的WAL</h2><p>由于InfluxDB支持对Measurement，TagKey，TagValue的删除操作，当然随着时序数据的不断写入，自然也包括 <b>增加新的时间线</b>，因此索引数据的WAL会区分当前所做的操作具体是什么，它的WAL的结构如下图所示</p><p>图 3-6<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-cb2ef3c1a48b2993e332cb68e78e99a5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1642\" data-rawheight=\"932\" class=\"origin_image zh-lightbox-thumb\" width=\"1642\" data-original=\"https://pic2.zhimg.com/v2-cb2ef3c1a48b2993e332cb68e78e99a5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1642&#39; height=&#39;932&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1642\" data-rawheight=\"932\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1642\" data-original=\"https://pic2.zhimg.com/v2-cb2ef3c1a48b2993e332cb68e78e99a5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-cb2ef3c1a48b2993e332cb68e78e99a5_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>时序数据的WAL</h2><p>由于InfluxDB对于时序数据的写操作永远只有单纯写入，因此它的Entry不需要区分操作种类，直接记录写入的数据即可</p><p>图 3-7<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-9774a55332d34212e17a89491a7bfe81_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1650\" data-rawheight=\"810\" class=\"origin_image zh-lightbox-thumb\" width=\"1650\" data-original=\"https://pic2.zhimg.com/v2-9774a55332d34212e17a89491a7bfe81_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1650&#39; height=&#39;810&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1650\" data-rawheight=\"810\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1650\" data-original=\"https://pic2.zhimg.com/v2-9774a55332d34212e17a89491a7bfe81_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-9774a55332d34212e17a89491a7bfe81_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>3.2 TSMFile解析</h2><p>TSMFile是InfluxDB对于时序数据的存储方案。在文件系统层面，每一个TSMFile对应了一个 <b>Shard</b>。</p><p>TSMFile的存储结构如下图所示:</p><p>图 3-1<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-d4767ea98fa1f3f81ec6801f77a28788_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1680\" data-rawheight=\"932\" class=\"origin_image zh-lightbox-thumb\" width=\"1680\" data-original=\"https://pic1.zhimg.com/v2-d4767ea98fa1f3f81ec6801f77a28788_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1680&#39; height=&#39;932&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1680\" data-rawheight=\"932\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1680\" data-original=\"https://pic1.zhimg.com/v2-d4767ea98fa1f3f81ec6801f77a28788_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-d4767ea98fa1f3f81ec6801f77a28788_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>其特点是在一个TSMFile中将 时序数据（i.e Timestamp + Field value）保存在数据区；将Serieskey 和 Field Name的信息保存在索引区，通过一个基于 Serieskey + Fieldkey构建的形似B+tree的文件内索引快速定位时序数据所在的 <b>数据块</b></p><p>注： 在当前版本中，单个TSMFile的最大长度为2GB，超过时即使是同一个Shard，也会继续新开一个TSMFile保存数据。本文的介绍出于简单化考虑，以下内容不考虑同一个Shard的TSMFile分裂的场景</p><ul><li>索引块的构成</li></ul><p>上文的索引块的构成，如下所示：</p><p>图 3-2<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-56d6fc6b460cc6855d8c241400dba670_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1484\" data-rawheight=\"840\" class=\"origin_image zh-lightbox-thumb\" width=\"1484\" data-original=\"https://pic1.zhimg.com/v2-56d6fc6b460cc6855d8c241400dba670_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1484&#39; height=&#39;840&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1484\" data-rawheight=\"840\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1484\" data-original=\"https://pic1.zhimg.com/v2-56d6fc6b460cc6855d8c241400dba670_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-56d6fc6b460cc6855d8c241400dba670_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>其中 <b>索引条目</b> 在InfluxDB的源码中被称为directIndex。在TSMFile中，索引块是按照 Serieskey + Fieldkey <b>排序</b> 后组织在一起的。</p><p>明白了TSMFile的索引区的构成，就可以很自然地理解InfluxDB如何高性能地在TSMFile扫描时序数据了：</p><ol><ol><li>根据用户指定的时间线（Serieskey）以及Field名 在 <b>索引区</b> 利用二分查找找到指定的Serieskey+FieldKey所处的 <b>索引数据块</b></li><li>根据用户指定的时间戳范围在 <b>索引数据块</b> 中查找数据落在哪个（或哪几个）<b>索引条目</b></li><li>将找到的 <b>索引条目</b> 对应的 <b>时序数据块</b> 加载到内存中进行进一步的Scan</li></ol></ol><p>注：上述的1，2，3只是简单化地介绍了查询机制，实际的实现中还有类似扫描的时间范围跨索引块等一系列复杂场景</p><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>时序数据的存储</li></ul><p>在图 3-1中介绍了时序数据块的结构：即同一个 Serieskey + Fieldkey 的 所有时间戳 - Field值对被拆分开，分成两个区：Timestamps区和Value区分别进行存储。它的目的是：<b>实际存储时可以分别对时间戳和Field值按不同的压缩算法进行存储以减少时序数据块的大小</b></p><p>采用的压缩算法如下所示：</p><ol><ol><li>Timestamp： <a href=\"https://link.zhihu.com/?target=http%3A//www.vldb.org/pvldb/vol8/p1816-teller.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Delta-of-delta encoding</a></li><li>Field Value：由于单个数据块的Field Value必然数据类型相同，因此可以集中按数据类型采用不同的压缩算法</li></ol></ol><ul><li>Float类: <a href=\"https://link.zhihu.com/?target=http%3A//www.vldb.org/pvldb/vol8/p1816-teller.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Gorrila&#39;s Float Commpression</a></li><li>Integer类型: Delta Encoding + Zigzag Conversion + RLE / Simple8b / None</li><li>String类型: <a href=\"https://link.zhihu.com/?target=https%3A//github.com/golang/snappy\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Snappy Compression</a></li><li>Boolean类型: Bit packing</li></ul><p>做查询时，当利用TSMFile的索引找到文件中的时序数据块时，将数据块载入内存并对Timestamp以及Field Value进行解压缩后以便继续后续的查询操作。</p><h2>3.3 TSIFile解析</h2><p>有了TSMFile，第3章开头所说的三个主要场景中的场景1和场景2都可以得到很好的解决。但是如果查询时用户并没有按预期按照Serieskey来指定查询条件，而是指定了更加复杂的条件，该如何确保它的查询性能？通常情况下，这个问题的解决方案是依赖倒排索引(<b>Inverted Index</b>)。</p><p>InfluxDB的倒排索引依赖于下述两个数据结构</p><ul><li>map&lt;SeriesID, SeriesKey&gt;</li><li>map&lt;tagkey, map&lt;tagvalue, List&lt;SeriesID&gt;&gt;&gt;</li></ul><p>它们在内存中展现如下：</p><p>图 3-3</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-c7c1d65e8df63ab6d950c8cd235acee4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1584\" data-rawheight=\"624\" class=\"origin_image zh-lightbox-thumb\" width=\"1584\" data-original=\"https://pic1.zhimg.com/v2-c7c1d65e8df63ab6d950c8cd235acee4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1584&#39; height=&#39;624&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1584\" data-rawheight=\"624\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1584\" data-original=\"https://pic1.zhimg.com/v2-c7c1d65e8df63ab6d950c8cd235acee4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-c7c1d65e8df63ab6d950c8cd235acee4_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>图 3-4</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-1c4bea1f840758520c32ed07c088018f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1267\" data-rawheight=\"528\" class=\"origin_image zh-lightbox-thumb\" width=\"1267\" data-original=\"https://pic4.zhimg.com/v2-1c4bea1f840758520c32ed07c088018f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1267&#39; height=&#39;528&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1267\" data-rawheight=\"528\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1267\" data-original=\"https://pic4.zhimg.com/v2-1c4bea1f840758520c32ed07c088018f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-1c4bea1f840758520c32ed07c088018f_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>但是在实际生产环境中，由于用户的时间线规模会变得很大，因此会造成倒排索引使用的内存过多，所以后来InfluxDB又引入了 <b>TSIFile</b></p><p>TSIFile的整体存储机制与TSMFile相似，也是以 <b>Shard</b> 为单位生成一个TSIFile。具体的存储格式就在此不赘述了。</p><h2>4. 总结</h2><p>以上就是对InfluxDB的存储机制的粗浅解析，由于目前所见的只有单机版的InfluxDB，所以尚不知道集群版的InfluxDB在存储方面有哪些不同。但是，即便是这单机版的存储机制，也对我们设计时序数据库有着重要的参考意义。</p><p></p>", 
            "topic": [
                {
                    "tag": "互联网", 
                    "tagLink": "https://api.zhihu.com/topics/19550517"
                }, 
                {
                    "tag": "数据库", 
                    "tagLink": "https://api.zhihu.com/topics/19552067"
                }, 
                {
                    "tag": "大数据", 
                    "tagLink": "https://api.zhihu.com/topics/19740929"
                }
            ], 
            "comments": [
                {
                    "userName": "渣渣昆", 
                    "userLink": "https://www.zhihu.com/people/d4b18030772788652c0284aa5673e6de", 
                    "content": "<p>想了解，tsm file的图形表示是使用哪种作图工作画的？是ppt吗？</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "老毛奇", 
                    "userLink": "https://www.zhihu.com/people/086fe2e6b457983c80911b6960d6d61c", 
                    "content": "<p>InfluxDB的倒排索引在大数据量的时候压缩率很低，貌似跟单纯的字符串空间差不多了。当然也许是我RP和shared没做优化的问题。</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "老毛奇", 
                    "userLink": "https://www.zhihu.com/people/086fe2e6b457983c80911b6960d6d61c", 
                    "content": "<p>而且只支持http协议这个太坑了，即使查询性能不错，查询结果输出占带宽这个问题也坑人不要不要的……</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "Flios Yan", 
                    "userLink": "https://www.zhihu.com/people/cb811365687a77428e52ec4a454538d0", 
                    "content": "<p>我也对这个画图工具很有兴趣，感觉非常清晰，请问是用的什么工具画的？</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "黄瓜瓜", 
                    "userLink": "https://www.zhihu.com/people/53854b6a3e64a17532f75e9906d72e0c", 
                    "content": "<p>对series data的wal有点错的，每个entry前还会有5个字节（1 + 4），其中1个字节是entry的属性（写入、删除、范围删除），4个字节是compressed后的entry长度。</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/57117813", 
            "userName": "Jeff", 
            "userLink": "https://www.zhihu.com/people/c2e171b39183c7a115a48cfa68378f25", 
            "upvote": 7, 
            "title": "时序数据库连载系列：指标届的独角兽Prometheus", 
            "content": "<p>简介</p><p>Prometheus是SoundCloud公司开发的一站式监控告警平台，依赖少，功能齐全。<br/>于2016年加入CNCF，广泛用于 Kubernetes集群的监控系统中，2018.8月成为继K8S之后第二个毕业的项目。Prometheus作为CNCF生态圈中的重要一员,其活跃度仅次于 Kubernetes。</p><h2>开局一张图：</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-f4f452db7618260c05808709598354e3_b.jpg\" data-size=\"normal\" data-rawwidth=\"741\" data-rawheight=\"480\" class=\"origin_image zh-lightbox-thumb\" width=\"741\" data-original=\"https://pic4.zhimg.com/v2-f4f452db7618260c05808709598354e3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;741&#39; height=&#39;480&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"741\" data-rawheight=\"480\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"741\" data-original=\"https://pic4.zhimg.com/v2-f4f452db7618260c05808709598354e3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-f4f452db7618260c05808709598354e3_b.jpg\"/><figcaption>   Prometheus的架构</figcaption></figure><h2>关键功能包括：</h2><ul><li>多维数据模型：metric，labels</li><li>灵活的查询语言：PromQL， 在同一个查询语句，可以对多个 metrics 进行乘法、加法、连接、取分数位等操作。</li><li>可独立部署，拆箱即用，不依赖分布式存储</li><li>通过Http pull的采集方式</li><li>通过push gateway来做push方式的兼容</li><li>通过静态配置或服务发现获取监控项</li><li>支持图表和dashboard等多种方式</li></ul><h2>核心组件：</h2><ul><li>Prometheus Server： 采集和存储时序数据</li><li>client库： 用于对接 Prometheus Server, 可以查询和上报数据</li><li>push gateway处理短暂任务：用于批量，短期的监控数据的汇总节点，主要用于业务数据汇报等</li><li>定制化的exporters,比如：HAProxy， StatsD，Graphite等等， 汇报机器数据的插件</li><li>告警管理：Prometheus 可以配置 rules，然后定时查询数据，当条件触发的时候，会将 alert 推送到配置的 Alertmanager</li><li>多种多样的支持工具</li></ul><h2>优势和劣势：</h2><ul><li>同InfluxDB相比, 在场景方面：PTSDB 适合数值型的时序数据。不适合日志型时序数据和用于计费的指标统计。InfluxDB面向的是通用时序平台，包括日志监控等场景。而Prometheus更侧重于指标方案。两个系统之间有非常多的相似之处，包括采集，存储，报警，展示等等</li><ul><li>Influxdata的组合有：telegraf+Influxdb+Kapacitor+Chronograf</li><li>Promethues的组合有：exporter+prometheus server+AlertManager+Grafana</li><li>采集端prometheus主推拉的模式，同时通过push gateway支持推的模式。influxdata的采集工具telegraf则主打推的方式。</li><li>存储方面二者在基本思想上相通，关键点上有差异包括：时间线的索引，乱序的处理等等。</li><li>数据模型上Influxdb支持多值模型，String类型等，更丰富一些。</li><li>Kapacitor 是一个混合了 prometheus 的数据处理,存储规则,报警规则以及告警通知功能的工具.而AlertManager进一步提供了分组,去重等等。</li><li>influxdb之前提供的cluster模式被移除了，现在只保留了基于relay的高可用，集群模式作为商业版本的特性发布。prometheus提供了一种很有特色的cluster模式，通过多层次的proxy来聚合多个prometheus节点实现扩展。<br/>同时开放了remote storage，因此二者又相互融合，Influxdb作为prometheus的远端存储。</li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>OpenTSDB 的数据模型与Prometheus几乎相同，查询语言上PromQL更简洁，OpenTSDB功能更丰富。OpenTSDB依赖的是Hadoop生态,Prometheus成长于Kubernetes生态。</li></ul><h2>数据模型</h2><ul><li>采用单值模型, 数据模型的核心概念是metric,labels和samples.</li><li>格式：&lt;metric name&gt;{&lt;label name&gt;=&lt;label value&gt;, …}</li><ul><li>例如：http_requests_total{method=&#34;POST&#34;,endpoint=&#34;/api/tracks&#34;}。</li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>metric的命名具有业务含义，比如http_request_total.</li><ul><li>指标的类型分为：Counter， Gauge，Historgram，Summary</li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>labels用于表示维度.Samples由时间戳和数值组成。</li><li>jobs and instances</li><ul><li>Prometheus 会自动生成target和instances作为标签</li><ul><li>job: api-server</li><ul><li>instance 1: 1.2.3.4:5670</li><li>instance 2: 1.2.3.4:5671</li></ul></ul></ul></ul><h2>整体设计思路​</h2><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-521611dfb4b80e84a3091660d1eab61d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"777\" data-rawheight=\"510\" class=\"origin_image zh-lightbox-thumb\" width=\"777\" data-original=\"https://pic2.zhimg.com/v2-521611dfb4b80e84a3091660d1eab61d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;777&#39; height=&#39;510&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"777\" data-rawheight=\"510\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"777\" data-original=\"https://pic2.zhimg.com/v2-521611dfb4b80e84a3091660d1eab61d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-521611dfb4b80e84a3091660d1eab61d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>Prometheus的整体技术架构可以分为几个重要模块：</p><ul><li>Main function：作为入口承担着各个组件的启动，连接，管理。以Actor-Like的模式协调组件的运行</li><li>Configuration：配置项的解析，验证，加载</li><li>Scrape discovery manager：服务发现管理器同抓取服务器通过同步channel通信，当配置改变时需要重启服务生效。</li><li>Scrape manager：抓取指标并发送到存储组件</li><li>Storage：</li><ul><li>Fanout Storage：存储的代理抽象层，屏蔽底层local storage和remote storage细节，samples向下双写，合并读取。</li><li>Remote Storage：Remote Storage创建了一个Queue管理器，基于负载轮流发送，读取客户端merge来自远端的数据。</li><li>Local Storage：基于本地磁盘的轻量级时序数据库。</li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>PromQL engine：查询表达式解析为抽象语法树和可执行查询，以Lazy Load的方式加载数据。</li><li>Rule manager：告警规则管理</li><li>Notifier：通知派发管理器</li><li>Notifier discovery：通知服务发现</li><li>Web UI and API：内嵌的管控界面，可运行查询表达式解析，结果展示。</li></ul><h2>PTSDB概述</h2><p>本文侧重于Local Storage PTSDB的解析. PTSDB的核心包括：倒排索引+窗口存储Block。<br/>数据的写入按照两个小时为一个时间窗口，将两小时内产生的数据存储在一个Head Block中，每一个块中包含该时间窗口内的所有样本数据(chunks)，元数据文件(meta.json)以及索引文件(index)。<br/>最新写入数据保存在内存block中， 2小时后写入磁盘。后台线程把2小时的数据最终合并成更大的数据块，一般的数据库在固定一个内存大小后，系统的写入和读取性能会受限于这个配置的内存大小。而PTSDB的内存大小是由最小时间周期，采集周期以及时间线数量来决定的。<br/>为防止内存数据丢失，实现wal机制。删除记录在独立的tombstone文件中。</p><h2>核心数据结构和存储格式</h2><p>PTSDB的核心数据结构是HeadAppender，Appender commit时wal日志编码落盘，同时写入head block中。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-ae80bb154bfc2871f5055f0612c2c812_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"856\" data-rawheight=\"363\" class=\"origin_image zh-lightbox-thumb\" width=\"856\" data-original=\"https://pic3.zhimg.com/v2-ae80bb154bfc2871f5055f0612c2c812_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;856&#39; height=&#39;363&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"856\" data-rawheight=\"363\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"856\" data-original=\"https://pic3.zhimg.com/v2-ae80bb154bfc2871f5055f0612c2c812_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-ae80bb154bfc2871f5055f0612c2c812_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>PTSDB本地存储使用自定义的文件结构。主要包含：WAL，元数据文件，索引，chunks，tombstones</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-a7175516ecedb7430a37511e983808bb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"327\" data-rawheight=\"301\" class=\"content_image\" width=\"327\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;327&#39; height=&#39;301&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"327\" data-rawheight=\"301\" class=\"content_image lazy\" width=\"327\" data-actualsrc=\"https://pic4.zhimg.com/v2-a7175516ecedb7430a37511e983808bb_b.jpg\"/></figure><h2>Write Ahead Log</h2><p>WAL 有3种编码格式：时间线，数据点，以及删除点。总体策略是基于文件大小滚动，并且根据最小内存时间执行清除。</p><ul><li>当日志写入时，以segment为单位存储，每个segment默认128M, 记录数大小达到32KB页时刷新一次。当剩余空间小于新的记录数大小时，创建新的Segment。</li><li>当compation时WAL基于时间执行清除策略，小于内存中block的最小时间的wal日志会被删除。</li><li>重启时，首先打开最新的Segment，从日志中恢复加载数据到内存。​</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-a61eceae851af4a8f4d374412f6e0cbd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"401\" data-rawheight=\"178\" class=\"content_image\" width=\"401\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;401&#39; height=&#39;178&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"401\" data-rawheight=\"178\" class=\"content_image lazy\" width=\"401\" data-actualsrc=\"https://pic2.zhimg.com/v2-a61eceae851af4a8f4d374412f6e0cbd_b.jpg\"/></figure><p>                            ​</p><h2>元数据文件</h2><p>meta.json文件记录了Chunks的具体信息, 比如新的compactin chunk来自哪几个小的chunk。 这个chunk的统计信息，比如：最小最大时间范围，时间线，数据点个数等等<br/>compaction线程根据统计信息判断该blocks是否可以做compact：（maxTime-minTime）占整体压缩时间范围的50%， 删除的时间线数量占总体数量的5%。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-7d0c9fb75c4e0c34137b9501849f2f38_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"371\" data-rawheight=\"325\" class=\"content_image\" width=\"371\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;371&#39; height=&#39;325&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"371\" data-rawheight=\"325\" class=\"content_image lazy\" width=\"371\" data-actualsrc=\"https://pic1.zhimg.com/v2-7d0c9fb75c4e0c34137b9501849f2f38_b.jpg\"/></figure><h2>索引</h2><p>索引一部分先写入Head Block中，随着compaction的触发落盘。<br/>索引采用的是倒排的方式，posting list里面的id是局部自增的，作为reference id表示时间线。索引compact时分为6步完成索引的落盘:Symbols-&gt;Series-&gt;LabelIndex-&gt;Posting-&gt;OffsetTable-&gt;TOC</p><ul><li>Symbols存储的是tagk, tagv按照字母序递增的字符串表。比如__name__,go_gc_duration_seconds, instance, localhost:9090等等。字符串按照utf8统一编码。</li><li>Series存储了两部分信息，一部分是标签键值对的符号表引用；另外一部分是时间线到数据文件的索引，按照时间窗口切割存储数据块记录的具体位置信息，因此在查询时可以快速跳过大量非查询窗口的记录数据，<br/>为了节省空间，时间戳范围和数据块的位置信息的存储采用差值编码。</li><li>LabelIndex存储标签键以及每一个标签键对应的所有标签值，当然具体存储的数据也是符号表里面的引用值。</li><li>Posting存储倒排的每个label对所对应的posting refid</li><li>OffsetTable加速查找做的一层映射，将这部分数据加载到内存。OffsetTable主要关联了LabelIndex和Posting数据块。TOC是各个数据块部分的位置偏移量，如果没有数据就可以跳过查找。​</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-3e7d0b5d2dbac0de680005f32de65863_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"869\" data-rawheight=\"703\" class=\"origin_image zh-lightbox-thumb\" width=\"869\" data-original=\"https://pic4.zhimg.com/v2-3e7d0b5d2dbac0de680005f32de65863_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;869&#39; height=&#39;703&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"869\" data-rawheight=\"703\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"869\" data-original=\"https://pic4.zhimg.com/v2-3e7d0b5d2dbac0de680005f32de65863_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-3e7d0b5d2dbac0de680005f32de65863_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>Chunks</h2><p>数据点存放在chunks目录下，每个data默认512M，数据的编码方式支持XOR，chunk按照refid来索引，refid由segmentid和文件内部偏移量两个部分组成。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-b101b394596063120d541136bd493968_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"333\" data-rawheight=\"30\" class=\"content_image\" width=\"333\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;333&#39; height=&#39;30&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"333\" data-rawheight=\"30\" class=\"content_image lazy\" width=\"333\" data-actualsrc=\"https://pic1.zhimg.com/v2-b101b394596063120d541136bd493968_b.jpg\"/></figure><h2>Tombstones</h2><p>记录删除通过mark的方式，数据的物理清除发生在compaction和reload的时候。以时间窗口为单位存储被删除记录的信息。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-53846216f3dd5e141081a66522c0a2fd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"333\" data-rawheight=\"88\" class=\"content_image\" width=\"333\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;333&#39; height=&#39;88&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"333\" data-rawheight=\"88\" class=\"content_image lazy\" width=\"333\" data-actualsrc=\"https://pic2.zhimg.com/v2-53846216f3dd5e141081a66522c0a2fd_b.jpg\"/></figure><h2>查询PromQL</h2><p>Promethues的查询语言是PromQL，语法解析AST，执行计划和数据聚合是由PromQL完成，fanout模块会向本地和远端同时下发查询数据，PTSDB负责本地数据的检索。<br/>PTSDB实现了定义的Adpator，包括Select, LabelNames, LabelValues和Querier.</p><p>PromQL定义了三类查询：<br/>瞬时数据 (Instant vector): 包含一组时序，每个时序只有一个点，例如：http_requests_total<br/>区间数据 (Range vector): 包含一组时序，每个时序有多个点，例：http_requests_total[5m]<br/>纯量数据 (Scalar): 纯量只有一个数字，没有时序，例如：count(http_requests_total)<br/>一些典型的查询包括：</p><ul><li>查询当前所有数据<br/>http_requests_total<br/>select * from http_requests_total where timestamp between xxxx and xxxx</li><li>条件查询<br/>http_requests_total{code=&#34;200&#34;, handler=&#34;query&#34;}<br/>select * from http_requests_total where code=&#34;200&#34; and handler=&#34;query&#34; and timestamp between xxxx and xxxx</li><li>模糊查询: code 为 2xx 的数据<br/>http_requests_total{code~=&#34;20&#34;}<br/>select * from http_requests_total where code like &#34;%20%&#34; and timestamp between xxxx and xxxx</li><li>值过滤： value大于100<br/>http_requests_total &gt; 100<br/>select * from http_requests_total where value &gt; 100 and timestamp between xxxx and xxxx</li><li>范围区间查询: 过去 5 分钟数据<br/>http_requests_total[5m]<br/>select * from http_requests_total where timestamp between xxxx-5m and xxxx</li><li>count 查询: 统计当前记录总数<br/>count(http_requests_total)<br/>select count(*) from http_requests_total where timestamp between xxxx and xxxx</li><li>sum 查询：统计当前数据总值<br/>sum(http_requests_total)<br/>select sum(value) from http_requests_total where timestamp between xxxx and xxxx</li><li>top 查询: 查询最靠前的 3 个值<br/>topk(3, http_requests_total)<br/>select * from http_requests_total where timestamp between xxxx and xxxx order by value desc limit 3</li><li>irate查询：速率查询<br/>irate(http_requests_total[5m])<br/>select code, handler, instance, job, method, sum(value)/300 AS value from http_requests_total where timestamp between xxxx and xxxx group by code, handler, instance, job, method;</li></ul><h2>PTSDB关键技术点</h2><h2>乱序处理</h2><p>PTSDB对于乱序的处理采用了最小时间窗口的方式，指定合法的最小时间戳，小于这一时间戳的数据会丢弃不再处理。<br/>合法最小时间戳取决于当前head block里面最早的时间戳和可存储的chunk范围。<br/>这种对于数据行为的限定极大的简化了设计的灵活性，对于compaction的高效处理以及数据完整性提供了基础。</p><h2>内存的管理</h2><p>使用mmap读取压缩合并后的大文件（不占用太多句柄），<br/>建立进程虚拟地址和文件偏移的映射关系，只有在查询读取对应的位置时才将数据真正读到物理内存。<br/>绕过文件系统page cache，减少了一次数据拷贝。<br/>查询结束后，对应内存由Linux系统根据内存压力情况自动进行回收，在回收之前可用于下一次查询命中。<br/>因此使用mmap自动管理查询所需的的内存缓存，具有管理简单，处理高效的优势。</p><h2>Compaction</h2><p>Compaction主要操作包括合并block、删除过期数据、重构chunk数据。</p><ul><li>合并多个block成为更大的block，可以有效减少block个，当查询覆盖的时间范围较长时，避免需要合并很多block的查询结果。</li><li>为提高删除效率，删除时序数据时，会记录删除的位置，只有block所有数据都需要删除时，才将block整个目录删除。</li><li>block合并的大小也需要进行限制，避免保留了过多已删除空间(额外的空间占用)。<br/>比较好的方法是根据数据保留时长，按百分比（如10%）计算block的最大时长, 当block的最小和最大时长超过2/3blok范围时，执行compaction</li></ul><h2>快照</h2><p>PTSDB提供了快照备份数据的功能，用户通过admin/snapshot协议可以生成快照，快照数据存储于data/snapshots/-目录。</p><h2>PTSDB最佳实践</h2><ul><li>在一般情况下，Prometheus中存储的每一个样本大概占用1-2字节大小。如果需要对Prometheus Server的本地磁盘空间做容量规划时，可以通过以下公式计算：<br/>needed_disk_space = retention_time_seconds * ingested_samples_per_second * bytes_per_sample</li><li>保留时间(retention_time_seconds)和样本大小(bytes_per_sample)不变的情况下，如果想减少本地磁盘的容量需求，<br/>只能通过减少每秒获取样本数(ingested_samples_per_second)的方式。<br/>因此有两种手段，一是减少时间序列的数量，二是增加采集样本的时间间隔。<br/>考虑到Prometheus会对时间序列进行压缩，因此减少时间序列的数量效果更明显。</li><li>PTSDB的限制在于集群和复制。因此当一个node宕机时，会导致一定窗口的数据丢失。<br/>当然，如果业务要求的数据可靠性不是特别苛刻，本地盘也可以存储几年的持久化数据。<br/>当PTSDB Corruption时，可以通过移除磁盘目录或者某个时间窗口的目录恢复。</li><li>PTSDB的高可用，集群和历史数据的保存可以借助于外部解决方案，不在本文讨论范围。</li><li>历史方案的局限性，PTSDB在早期采用的是单条时间线一个文件的存储方式。这中方案有非常多的弊端，比如：<br/>Snapshot的刷盘压力：定期清理文件的负担；低基数和长周期查询查询，需要打开大量文件；时间线膨胀可能导致inode耗尽。</li></ul><h2>PTSDB面临的挑战</h2><p>在使用过程中，PTSDB也在某些方面遇到了一些问题，比如；</p><ul><li>Compaction对于IO, CPU, 以及Memory的影响</li><li>冷启动后，预热阶段CPU和内存占用会上升</li><li>在高速写入时会出现CPU的Spike等等</li></ul><h2>总结</h2><p>PTSDB 作为K8S监控方案里面存储时序数据的实施标准，其在时序届影响力和热度都在逐步上升。Alibaba TSDB目前已经支持通过Adapter的方式作为其remote storage的方案。</p><p>​</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-144387ef7c8838ccd38202e4f3b33e73_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"741\" data-rawheight=\"480\" class=\"origin_image zh-lightbox-thumb\" width=\"741\" data-original=\"https://pic4.zhimg.com/v2-144387ef7c8838ccd38202e4f3b33e73_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;741&#39; height=&#39;480&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"741\" data-rawheight=\"480\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"741\" data-original=\"https://pic4.zhimg.com/v2-144387ef7c8838ccd38202e4f3b33e73_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-144387ef7c8838ccd38202e4f3b33e73_b.jpg\"/></figure><p></p><p></p>", 
            "topic": [
                {
                    "tag": "互联网", 
                    "tagLink": "https://api.zhihu.com/topics/19550517"
                }, 
                {
                    "tag": "数据库", 
                    "tagLink": "https://api.zhihu.com/topics/19552067"
                }, 
                {
                    "tag": "大数据", 
                    "tagLink": "https://api.zhihu.com/topics/19740929"
                }
            ], 
            "comments": [
                {
                    "userName": "知乎用户", 
                    "userLink": "https://www.zhihu.com/people/0", 
                    "content": "<p>试用过程中，发现pull模式的服务发现不能发现负载均衡后端的实例，也无法穿透到每个实例获取指标。只好改用influxdb， push模型了。</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/57117030", 
            "userName": "Jeff", 
            "userLink": "https://www.zhihu.com/people/c2e171b39183c7a115a48cfa68378f25", 
            "upvote": 10, 
            "title": "时序数据库连载系列：时序数据库那些事", 
            "content": "<p><b>时序数据库连载系列：时序数据库那些事</b></p><p>正如《银翼杀手》中那句在影史流传经典的台词：“I&#39;ve seen things you people wouldn&#39;t believe... All those ... moments will be lost in time, like tears...in rain.” 时间浩瀚的人类历史长河中总是一个耀眼的词汇，当科技的年轮划到数据时代，时间与数据库碰到一起，把数据库内建时间属性后，产生了时序数据库。时序数据库是一种带有时间戳业务属性的垂直型数据库。自从2014年开始，数据库热度排名网站DB-Engines就把时间序列数据库作为了独立的目录来分类统计，而且最近几年的增长率在全部数据库分类里排名第一（见下图）。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-8a38c48f485c9e5309f2cf53c2faa4c7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"881\" data-rawheight=\"445\" class=\"origin_image zh-lightbox-thumb\" width=\"881\" data-original=\"https://pic4.zhimg.com/v2-8a38c48f485c9e5309f2cf53c2faa4c7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;881&#39; height=&#39;445&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"881\" data-rawheight=\"445\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"881\" data-original=\"https://pic4.zhimg.com/v2-8a38c48f485c9e5309f2cf53c2faa4c7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-8a38c48f485c9e5309f2cf53c2faa4c7_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>时序数据库</b></p><p>A time series database (TSDB) is a software system that is optimized for handling time series data, arrays of numbers indexed by time (a datetime or a datetime range)</p><p>以上是维基百科对于时序数据库的定义。可以把它拆解成3个方面来看：时序特性，数据特性，数据库特性。</p><ul><li>时序特性：</li><ul><li>时间戳：通用的业务场景内以秒和毫秒精度为主，在一些遥感等高频采集领域，时间戳可以达到纳秒级别。时间戳种类包括unix系统时间戳和Calendar, 并且支持时区的自动适配。</li><li>采样频率：采集频率一般有2种，一种是周期性的时间采样频率，比如服务器性能相关的定期汇总指标。另外一种是离散型的采样，比如网站的访问等等</li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>数据特性：</li><ul><li>数据顺序追加</li><li>数据可多维关联</li><li>通常高频访问热数据</li><li>冷数据需要降维归档</li><li>数据主要覆盖数值，状态，事件</li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>数据库特性（CRUD）</li><ul><li>写入速率稳定并且远远大于读取</li><li>按照时间窗口访问数据</li><li>极少更新，存在一定窗口期的覆盖写</li><li>批量删除</li><li>具备通用数据库要求的高可用，高可靠，可伸缩特性</li><li>通常不需要具备事务的能力</li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><p><b>时序数据库发展简史</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-4134d6b294aa5d69eb4590bf0c226f97_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"888\" data-rawheight=\"326\" class=\"origin_image zh-lightbox-thumb\" width=\"888\" data-original=\"https://pic4.zhimg.com/v2-4134d6b294aa5d69eb4590bf0c226f97_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;888&#39; height=&#39;326&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"888\" data-rawheight=\"326\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"888\" data-original=\"https://pic4.zhimg.com/v2-4134d6b294aa5d69eb4590bf0c226f97_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-4134d6b294aa5d69eb4590bf0c226f97_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>第一代时序数据存储系统</b></p><p>虽然通用关系数据库可以存储时序数据，但是由于缺乏针对时间的特殊优化，比如按时间间隔存储和检索数据等等，因此在处理这些数据时效率相对不高。</p><p>第一代时序数据典型来源于监控领域，直接基于平板文件的简单存储工具成为这类数据的首先存储方式。</p><p>以RRDTool，Wishper为代表，通常这类系统处理的数据模型比较单一，单机容量受限，并且内嵌于监控告警方案。</p><p><b>基于通用存储的时序数据库</b></p><p>伴随着大数据和Hadoop的发展，时序数据量开始迅速增长，系统业务对于处理时序数据的扩展性等方面提出更多的要求。</p><p>基于通用存储而专门构建的时间序列数据库开始出现，它可以按时间间隔高效地存储和处理这些数据。像OpenTSDB，KairosDB等等。</p><p>这类时序数据库在继承通用存储优势的基础上，利用时序的特性规避部分通用存储的劣势，并且在数据模型，聚合分析方面做了贴合时序的大量创新。</p><p>比如OpenTSDB继承了HBase的宽表属性结合时序设计了偏移量的存储模型，利用salt缓解热点问题等等。</p><p>然而它也有诸多不足之处，比如低效的全局UID机制，聚合数据的加载不可控，无法处理高基数标签查询等等。</p><p><b>垂直型时序数据库的出现</b></p><p>随着docker，kubernetes, 微服务等技术的发展，以及对于IoT的发展预期越来越强烈。</p><p>在数据随着时间而增长的过程中，时间序列数据成为增长最快的数据类型之一。</p><p>高性能，低成本的垂直型时序数据库开始诞生，以InfluxDB为代表的具有时序特征的数据存储引擎逐步引领市场。</p><p>它们通常具备更加高级的数据处理能力，高效的压缩算法和符合时序特征的存储引擎。</p><p>比如InfluxDB的基于时间的TSMT存储，Gorilla压缩，面向时序的窗口计算函数p99，rate，自动rollup等等。</p><p>同时由于索引分离的架构，在膨胀型时间线，乱序等场景下依然面临着很大的挑战。</p><p><b>时序数据库发展现状</b></p><p>目前，DB-Engines把时间序列数据库作为独立的目录来分类统计，下图就是2018年业内流行的时序数据库的关注度排名和最近5年的变化趋势。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-98f59ad1e90731aa0f27192110f8b4f4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"579\" data-rawheight=\"327\" class=\"origin_image zh-lightbox-thumb\" width=\"579\" data-original=\"https://pic1.zhimg.com/v2-98f59ad1e90731aa0f27192110f8b4f4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;579&#39; height=&#39;327&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"579\" data-rawheight=\"327\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"579\" data-original=\"https://pic1.zhimg.com/v2-98f59ad1e90731aa0f27192110f8b4f4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-98f59ad1e90731aa0f27192110f8b4f4_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>公有云</li><ul><li>AWS Timestream</li><ul><li>2018.11 Amazon在AWS re Invent大会发布Timestream预览版。适用于 IoT 和运营应用程序等场景。提供自适应查询处理引擎快速地分析数据，自动对数据进行汇总、保留、分层和压缩处理。按照写入流量，存储空间，查询数据量的方式计费，以serverless的形式做到最低成本管理。</li></ul><li>Azure Series Insights</li><ul><li>2017.4 Microsoft发布时序见解预览版，提供的完全托管、端到端的存储和查询高度情景化loT时序数据解决方案。强大的可视化效果用于基于资产的数据见解和丰富的交互式临时数据分析。<br/>针对数据类型分为暖数据分析和原始数据分析，按照存储空间和查询量分别计费。</li></ul></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>开源</li><ul><li>OpenTSDB<br/>OpenTSDB是一个分布式的、可伸缩的时间序列数据库. 引入metric，tags等概念设计了一套针对时序场景的数据模型，底层采用HBase作为存储，利用时序场景的特性，采用特殊的rowkey方式，来提高时序的聚合和查询能力。</li><li>Prometheus<br/>Prometheus会将所有采集到的样本数据以时间序列（time-series）的方式保存在内存数据库中，并且定时保存到硬盘上。需要远端存储来保证可靠和扩展性。</li><li>InfluxDB<br/>InfluxDB是单机开源的时序数据库,由Go语言编写，无需特殊的环境依赖，简单方便。采用独有的TSMT结构实现高性能的读写。分布式需要商业化支持。</li><li>Timescale<br/>面向SQL生态的时序数据库，固定Schema，底层基于PG，按时间管理chunk table。</li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>学术</li><ul><li>BTrDB<br/>BtrDB面向高精度时序数据的存储应用，设计并提出了 &#34;time-partitioning version-annotated copy-on-write tree&#34; 的数据结构，为每一条时间线构建了一棵树，并且引入版本的概念处理数据的乱序场景</li><li>Confluo<br/>Confluo设计了新型的数据结构”Atomic MultiLog“，采用现代CPU硬件支持的原子指令集，支持百万级数据点高并发写入，毫秒级在线查询，占用很少的的CPU资源实现即席查询</li><li>Chronixdb<br/>ChronixDB基于Solr提供了时序存储，并且实现了特有的无损压缩算法，可以与Spark集成，提供丰富的时序分析能力。</li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>商业&amp;工业</li><ul><li>PI<br/>PI是OSI软件公司开发的大型实时数据库，广泛应用于电力，化工等行业，采用了旋转门压缩专利技术和独到的二次过滤技术，使进入到PI数据库的数据经过了最有效的压缩，极大地节省了硬盘空间</li><li>KDB<br/>KDB是Kx System开发的时间序列数据库，通常用于处理交易行情相关数据。支持流、内存计算和实时分析Billion级别的记录以及快速访问TB级别的历史数据。</li><li>Gorilla<br/>Gorilla是Facebook的一个基于内存的时序数据库，采用了一种新的时间序列压缩算法.</li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><p>可以将数据从16字节压缩到平均1.37字节，缩小12倍.并且设计了针对压缩算法的内存数据结构.在保持对单个时间序列进行时间段查找的同时也能快速和高效的进行全数据扫描。<br/>通过将时间序列数据写到不同地域的主机中，容忍单节点故障，网络切换，甚至是整个数据中心故障。</p><ul><li>投资市场</li><ul><li>2018年时序数据库创业公司在投资市场有2笔著名的投资。</li><li>Timescale获得了来自Benchmark Capital的\\$12.4M Series A轮融资。</li><li>InfluxDB获得了来自Sapphire Ventures的\\$35M C轮融资。</li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><p><b>业界典型时序数据库解析</b></p><p>近2年来时序数据库正处于高速发展的阶段。国内外云市场各大主流厂商已经从整个时序生态的不同角度切入，形成各自特色的解决方案完成布局，开始抢占流量。<br/>而以Facebook Gorilla为代表的优秀的时序数据库则是脱胎于满足自身业务发展的需要。学术上，在时序领域里面更是涌现了一大批黑科技，把时序数据的技术深度推向更高的台阶。<br/>阿里巴巴的TSDB团队自2016年第一版时序数据库落地后，逐步服务于DBPaaS，Sunfire等等集团业务，在2017年中旬公测后，于2018年3月底正式商业化。<br/>在此过程中，TSDB在技术方面不断吸纳时序领域各家之长，开启了自研的时序数据库发展之路。<br/>这个系列文章带领读者一起欣赏下当前时序领域的技术风景。</p><p></p>", 
            "topic": [
                {
                    "tag": "互联网", 
                    "tagLink": "https://api.zhihu.com/topics/19550517"
                }, 
                {
                    "tag": "数据库", 
                    "tagLink": "https://api.zhihu.com/topics/19552067"
                }, 
                {
                    "tag": "大数据", 
                    "tagLink": "https://api.zhihu.com/topics/19740929"
                }
            ], 
            "comments": []
        }
    ], 
    "url": "https://zhuanlan.zhihu.com/c_1081187679500120064"
}
