{
    "title": "贤鱼从零开始的异世界CV学习", 
    "description": "计算机视觉学习记录", 
    "followers": [
        "https://www.zhihu.com/people/zhuang-zhi-ling-yun-70", 
        "https://www.zhihu.com/people/tian-ma-72xiao-shi", 
        "https://www.zhihu.com/people/li-ling-12-18-63", 
        "https://www.zhihu.com/people/li-chuang-11-37", 
        "https://www.zhihu.com/people/lcx-85", 
        "https://www.zhihu.com/people/zi-shan-57-15", 
        "https://www.zhihu.com/people/takly-37", 
        "https://www.zhihu.com/people/zzydog1982", 
        "https://www.zhihu.com/people/leng-fei-86", 
        "https://www.zhihu.com/people/wallace-rice", 
        "https://www.zhihu.com/people/li-wang-wang-13", 
        "https://www.zhihu.com/people/crazyman-23", 
        "https://www.zhihu.com/people/yun-meng-81-26", 
        "https://www.zhihu.com/people/gu-xin-48", 
        "https://www.zhihu.com/people/ng-ng-80-61", 
        "https://www.zhihu.com/people/zhang-long-yu-61", 
        "https://www.zhihu.com/people/xiao-ming-3-98", 
        "https://www.zhihu.com/people/dufei-lee", 
        "https://www.zhihu.com/people/haqart", 
        "https://www.zhihu.com/people/bai-xiao-chuan", 
        "https://www.zhihu.com/people/he-jun-jie-14-80", 
        "https://www.zhihu.com/people/bamboo-24-73", 
        "https://www.zhihu.com/people/li-wei-56-79-17", 
        "https://www.zhihu.com/people/bu-zhi-fou-59-41", 
        "https://www.zhihu.com/people/binarytree-73", 
        "https://www.zhihu.com/people/liu-fei-94-95", 
        "https://www.zhihu.com/people/toeinriver", 
        "https://www.zhihu.com/people/chi-jiu-a", 
        "https://www.zhihu.com/people/rainbow-92-60", 
        "https://www.zhihu.com/people/no-no-17-93", 
        "https://www.zhihu.com/people/shuang-zi-xing-xing", 
        "https://www.zhihu.com/people/ez_ting", 
        "https://www.zhihu.com/people/zhmman-9", 
        "https://www.zhihu.com/people/racoon-99", 
        "https://www.zhihu.com/people/shi-san-pi-58", 
        "https://www.zhihu.com/people/aiolia723", 
        "https://www.zhihu.com/people/wei-lai-chen", 
        "https://www.zhihu.com/people/yang-gang-69-84", 
        "https://www.zhihu.com/people/nuo-wei-si-ji-kou-qiao-dan", 
        "https://www.zhihu.com/people/kan-qing-shi-jie-73", 
        "https://www.zhihu.com/people/wang-slam", 
        "https://www.zhihu.com/people/yang-jie", 
        "https://www.zhihu.com/people/xie-yong-jun-9", 
        "https://www.zhihu.com/people/ri-yue-dang-kong-zhao-37", 
        "https://www.zhihu.com/people/zhang-wen-42-3", 
        "https://www.zhihu.com/people/saunak", 
        "https://www.zhihu.com/people/geng-dong-dong-80", 
        "https://www.zhihu.com/people/yu-hong-sheng-91", 
        "https://www.zhihu.com/people/samuel-wang-65", 
        "https://www.zhihu.com/people/yun-zai-tian-shang-fei-17", 
        "https://www.zhihu.com/people/yang-wen-huan-31", 
        "https://www.zhihu.com/people/sstrugglee", 
        "https://www.zhihu.com/people/ruby-53-38", 
        "https://www.zhihu.com/people/chen-jia-hao-64-50", 
        "https://www.zhihu.com/people/jiang-jin-yue-14-4", 
        "https://www.zhihu.com/people/chen-1-54-49", 
        "https://www.zhihu.com/people/a-mao-84-87", 
        "https://www.zhihu.com/people/yan7", 
        "https://www.zhihu.com/people/zhao-wei-11594", 
        "https://www.zhihu.com/people/zhang-yin-song-44", 
        "https://www.zhihu.com/people/lu-yc-84", 
        "https://www.zhihu.com/people/qi-che-ren-82", 
        "https://www.zhihu.com/people/seantao", 
        "https://www.zhihu.com/people/deng-dai-hua-kai-53-75", 
        "https://www.zhihu.com/people/hahaha-92-99", 
        "https://www.zhihu.com/people/zhu-forrest", 
        "https://www.zhihu.com/people/nfreewind", 
        "https://www.zhihu.com/people/gu-nian-han-93", 
        "https://www.zhihu.com/people/jiao-luo-li-de-gui-hua-gao", 
        "https://www.zhihu.com/people/mao-xian-sheng-1-1", 
        "https://www.zhihu.com/people/zi-ran-xin-34", 
        "https://www.zhihu.com/people/mei-xi-31-99", 
        "https://www.zhihu.com/people/zhi-xing-lei-lei", 
        "https://www.zhihu.com/people/deng-zhi-wei-32-70", 
        "https://www.zhihu.com/people/madaoK-24", 
        "https://www.zhihu.com/people/zhu-xiao-hui-16", 
        "https://www.zhihu.com/people/lu-wei-3-41", 
        "https://www.zhihu.com/people/yang-hao-zi-24", 
        "https://www.zhihu.com/people/ceng-ying-tong-40", 
        "https://www.zhihu.com/people/wupeng_1993", 
        "https://www.zhihu.com/people/zhou-peng-21-71", 
        "https://www.zhihu.com/people/cici3072", 
        "https://www.zhihu.com/people/lingxi1996", 
        "https://www.zhihu.com/people/wen-wen-8-93", 
        "https://www.zhihu.com/people/jamin-60-60", 
        "https://www.zhihu.com/people/valeriedeng", 
        "https://www.zhihu.com/people/zhou-qiu-heng", 
        "https://www.zhihu.com/people/charles-meng-17", 
        "https://www.zhihu.com/people/gu-yu-che", 
        "https://www.zhihu.com/people/oliver-kahn-60", 
        "https://www.zhihu.com/people/stephen-wang-1", 
        "https://www.zhihu.com/people/zio-12", 
        "https://www.zhihu.com/people/liu-yu-72-1-83", 
        "https://www.zhihu.com/people/zhi-liao-12-22", 
        "https://www.zhihu.com/people/pei-ni-yiqi-chang-da", 
        "https://www.zhihu.com/people/grant-68", 
        "https://www.zhihu.com/people/yc-cq", 
        "https://www.zhihu.com/people/da-shi-sui-xiong-kou-39", 
        "https://www.zhihu.com/people/wang-kuan-57-47", 
        "https://www.zhihu.com/people/moni-gg", 
        "https://www.zhihu.com/people/li-xin-95-42-23", 
        "https://www.zhihu.com/people/hu-page", 
        "https://www.zhihu.com/people/jia-xue-feng", 
        "https://www.zhihu.com/people/kang-zhao-zi-xing-che-pao", 
        "https://www.zhihu.com/people/gu-yu-han-shuang-bai", 
        "https://www.zhihu.com/people/wan9009", 
        "https://www.zhihu.com/people/xianlong-chen", 
        "https://www.zhihu.com/people/wang-jia-zhu-43-31", 
        "https://www.zhihu.com/people/medivhc", 
        "https://www.zhihu.com/people/zhong-you-dong", 
        "https://www.zhihu.com/people/yanleirex", 
        "https://www.zhihu.com/people/xiao-wan-dou-52-94", 
        "https://www.zhihu.com/people/liu-lu-34-98", 
        "https://www.zhihu.com/people/ykp-41", 
        "https://www.zhihu.com/people/yiiwood", 
        "https://www.zhihu.com/people/su-feng-yu-5", 
        "https://www.zhihu.com/people/bliss-55", 
        "https://www.zhihu.com/people/guan-yue-60", 
        "https://www.zhihu.com/people/sx_AH", 
        "https://www.zhihu.com/people/liang-liang-25-85", 
        "https://www.zhihu.com/people/zhao-xing-ke-33", 
        "https://www.zhihu.com/people/yangliuyu", 
        "https://www.zhihu.com/people/jiang-rui-96-46", 
        "https://www.zhihu.com/people/zhao-hu-41-13", 
        "https://www.zhihu.com/people/shliu", 
        "https://www.zhihu.com/people/li-hong-12-65", 
        "https://www.zhihu.com/people/wang-ji-shi-jian-54", 
        "https://www.zhihu.com/people/fang-fa-64-69", 
        "https://www.zhihu.com/people/smmg", 
        "https://www.zhihu.com/people/Etisoppo", 
        "https://www.zhihu.com/people/feng-xing-long-5", 
        "https://www.zhihu.com/people/wang-peng-63-14", 
        "https://www.zhihu.com/people/jjww-71-58", 
        "https://www.zhihu.com/people/biaaib", 
        "https://www.zhihu.com/people/cyber007", 
        "https://www.zhihu.com/people/ye-cha-4", 
        "https://www.zhihu.com/people/kevinhayalitu", 
        "https://www.zhihu.com/people/qiao-hai-jun", 
        "https://www.zhihu.com/people/roachsinai", 
        "https://www.zhihu.com/people/phychology", 
        "https://www.zhihu.com/people/anArkitek", 
        "https://www.zhihu.com/people/wu-qi-72-92", 
        "https://www.zhihu.com/people/bu-ke-zhi-82", 
        "https://www.zhihu.com/people/zhao-rong-89-59", 
        "https://www.zhihu.com/people/william-zhu-82", 
        "https://www.zhihu.com/people/wang-xiao-hu-27-48", 
        "https://www.zhihu.com/people/li-hao-ran-92-97", 
        "https://www.zhihu.com/people/shi-yang-58-24", 
        "https://www.zhihu.com/people/wu-shi-93-35", 
        "https://www.zhihu.com/people/ikara-tasi", 
        "https://www.zhihu.com/people/superpermutation", 
        "https://www.zhihu.com/people/yuan-wei-5-84", 
        "https://www.zhihu.com/people/reccoon", 
        "https://www.zhihu.com/people/ronghua-zhang", 
        "https://www.zhihu.com/people/sheldon-wang-57", 
        "https://www.zhihu.com/people/huang-ming-89-21", 
        "https://www.zhihu.com/people/wang-chuan-27-27-48", 
        "https://www.zhihu.com/people/zihu92", 
        "https://www.zhihu.com/people/wang-jing-bo-27-88", 
        "https://www.zhihu.com/people/liu-han-ying-6", 
        "https://www.zhihu.com/people/libin-sui"
    ], 
    "article": [
        {
            "url": "https://zhuanlan.zhihu.com/p/66882733", 
            "userName": "贤鱼卓君", 
            "userLink": "https://www.zhihu.com/people/0be40edd972271a9c9afb446ee1a566d", 
            "upvote": 18, 
            "title": "双目ORB-SLAM2代码个人总结（一）", 
            "content": "<p>为了好好把ORB-SLAM2的代码搞懂，特别写文章作为总结。其代码本身就有许多的注释，我在学习以及总结时也参考了很多，特此说明。代码见：</p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/raulmur/ORB_SLAM2\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-7ea6511cf59172cacdf8030532749626_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">raulmur/ORB_SLAM2</a><p><b>注意：</b>目前我只关心双目的SLAM，因此只总结了双目的部分。总结时我也不会写很琐碎的东西，比如记录每一次处理时间这种比较无关紧要的；另外，不是最关键的函数我也不会分析（比如系统关闭类似的），我希望这个总结尽量高效。不过由于我才开始学，编程也不好，所以还是会写一些显而易见的东西，希望能对想我一样的初学者有所帮助。另外，本人由于水平所限也不可能全部理解或者理解的都正确，还请多多包涵。</p><p><b>此系列文章仅发于我的知乎专栏，转载请注明出处附上本文连接，不得用于商业用途，侵权必究。</b></p><hr/><p>首先说一下代码在命名变量时的规则：“p”表示指针数据类型， &#34;n&#34;表示int类型 ，“b”表示bool类型 &#34;s&#34;表示set类型 ，“v”表示vector数据类型，“I”表示list数据类型 ，“m”表示类的成员变量，“t”表示线程。</p><p>其中m、p、v、b我觉得是最常见，知道命名规则的话对代码理解会有一定帮助。</p><hr/><h2>1. <a href=\"https://link.zhihu.com/?target=http%3A//stereo_kitti.cc\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">stereo_kitti.cc</span><span class=\"invisible\"></span></a></h2><p>我是在KITTI数据跑的ORB-SLAM2，因此我以Example中的<a href=\"https://link.zhihu.com/?target=http%3A//stereo_kitti.cc\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">stereo_kitti.cc</span><span class=\"invisible\"></span></a>为入口学习代码。</p><p><b>1）读取图片的路径+名称、时间戳</b></p><p>这部分很简单，通过LoadImages函数完成，该函数把左右图片的路径+名称（string）、时间戳信息（double）分别读入三个vector容器：vstrImageLeft、vstrImageRight、vTimestamps。</p><p><b>2）创建SLAM系统</b></p><p>使用ORB_SLAM2::System类的构造函数创建SLAM系统，具体的在看System的代码时再说。这一步初始化了系统的各个线程，准备好处理输入的帧。</p><p><b>3）主循环：处理每一帧（左右图）输入</b></p><p>第一步把要处理的图像的路径+名称、时间戳都读到vector中，这一步仅需读取使用它们（图片使用cv::imread读取）。这一步中最关键的一步就是：</p><div class=\"highlight\"><pre><code class=\"language-cpp\"><span class=\"n\">SLAM</span><span class=\"p\">.</span><span class=\"n\">TrackStereo</span><span class=\"p\">(</span><span class=\"n\">imLeft</span><span class=\"p\">,</span><span class=\"n\">imRight</span><span class=\"p\">,</span><span class=\"n\">tframe</span><span class=\"p\">);</span>\n</code></pre></div><p>使用了ORB_SLAM2::System中的TrackStereo函数，输入就是：左图(cv::Mat)、右图(cv::Mat)、对应时间戳(double)。</p><p>如果处理一帧所用时间小于两帧之间实际拍摄所间隔时间，则用usleep函数停顿相应时间再进行下一轮循环。</p><p><b>4）主循环结束后系统关闭</b></p><p>使用System类中的Shutdown函数停止所有线程。最后计算了平均每帧的处理时间，并且保存相机轨迹到CameraTrajectory.txt文件。</p><hr/><h2>2. <a href=\"https://link.zhihu.com/?target=http%3A//System.cc\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">System.cc</span><span class=\"invisible\"></span></a></h2><p>前面用到了System类中的函数，这里进行学习<a href=\"https://link.zhihu.com/?target=http%3A//System.cc\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">System.cc</span><span class=\"invisible\"></span></a>代码。</p><p><b>1）ORB_SLAM2::System的构造函数</b></p><div class=\"highlight\"><pre><code class=\"language-cpp\"><span class=\"n\">System</span><span class=\"o\">::</span><span class=\"n\">System</span><span class=\"p\">(</span><span class=\"k\">const</span> <span class=\"n\">string</span> <span class=\"o\">&amp;</span><span class=\"n\">strVocFile</span><span class=\"p\">,</span> <span class=\"k\">const</span> <span class=\"n\">string</span> <span class=\"o\">&amp;</span><span class=\"n\">strSettingsFile</span><span class=\"p\">,</span> <span class=\"k\">const</span> <span class=\"n\">eSensor</span> <span class=\"n\">sensor</span><span class=\"p\">,</span>\n               <span class=\"k\">const</span> <span class=\"kt\">bool</span> <span class=\"n\">bUseViewer</span><span class=\"p\">)</span><span class=\"o\">:</span>\n        <span class=\"n\">mSensor</span><span class=\"p\">(</span><span class=\"n\">sensor</span><span class=\"p\">),</span> <span class=\"n\">mpViewer</span><span class=\"p\">(</span><span class=\"k\">static_cast</span><span class=\"o\">&lt;</span><span class=\"n\">Viewer</span><span class=\"o\">*&gt;</span><span class=\"p\">(</span><span class=\"nb\">NULL</span><span class=\"p\">)),</span> <span class=\"n\">mbReset</span><span class=\"p\">(</span><span class=\"nb\">false</span><span class=\"p\">),</span><span class=\"n\">mbActivateLocalizationMode</span><span class=\"p\">(</span><span class=\"nb\">false</span><span class=\"p\">),</span>\n        <span class=\"n\">mbDeactivateLocalizationMode</span><span class=\"p\">(</span><span class=\"nb\">false</span><span class=\"p\">)</span>\n</code></pre></div><p><b>ORB-SLAM2基于词袋方法实现闭环检测，同时利用词袋中的树结构加速了关键点之间的匹配。</b>构造函数的输入“strVocFile”是系统使用的视觉词典文件的名称，如：Vocabulary/ORBvoc.txt。System构造函数中使用DBoW2::TemplatedVocabulary（来自第三方库DBoW2的模板类）构造的ORBVocabulary类来初始化系统使用的视觉单词的词典，这个词典是System类的一个属性，名为<b>&#34;mpVocabulary&#34;</b>。mpVocabulary的初始化使用DBoW2::TemplatedVocabulary类的方法“loadFromTextFile”完成。</p><p>System构造函数的输入“strSettingsFile”是系统的配置文件，如：Examples/Stereo/KITTI00-02.yaml（该文件是用ORB-SLAM2跑KITTI的00-02序列所用的配置文件）。System的构造函数通过使用cv::FileStorage类的构造函数试图读配置文件来检查该文件是否有问题（构造函数并没有真正读取配置文件中的内容）。</p><p>System构造函数的输入“sensor”对应着枚举类型“eSensor”，这里双目应该是STEREO（=1）。</p><p>System构造函数最后一个输入“bUseViewer”表示是否使用Viewer线程，我们跑ORB-SLAM2时需要查看直观上的运行效果，因此这里构造System类对象时默认输入true。</p><p>根据最开始提到的命名规则，mpViewer是System的一个成员并且是一个指针，它指向系统的Viewer线程，在<a href=\"https://link.zhihu.com/?target=http%3A//Viewer.cc\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">Viewer.cc</span><span class=\"invisible\"></span></a>中定义，初始为NULL。</p><p>mbReset是一个布尔型的成员变量，系统重置的标志；mbActivateLocalizationMode是一个布尔型的成员变量，激活定位模式（应该是不建图只跟踪定位）的标志；mbDeactivateLocalizationMode则是关闭定位模式的标志。这三个系统的标志成员变量初始都为false。</p><p>由于ORB-SLAM2系统是“基于关键帧优化”的SLAM系统，因此System需要在构造函数中初始化一个关键的成员变量<b>“mpKeyFrameDatabase”</b>，它就是指向“关键帧数据库”的指针（类型为KeyFrameDatabase*）。<b>这个数据库主要用于重定位和闭环检测</b>，其实就是在“需要进行传感器所处位置的再次识别”时，系统会在这个关键帧数据库里搜索过去可能对应同一位置的关键帧。该数据库类的初始化通过KeyFrameDatabase（关键帧数据库的类）的构造函数完成（<a href=\"https://link.zhihu.com/?target=http%3A//KeyFrameDatabase.cc\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">KeyFrameDatabase.cc</span><span class=\"invisible\"></span></a>中），仅需输入系统的词典（*mpVocabulary）。这一步代码如下：</p><div class=\"highlight\"><pre><code class=\"language-cpp\"><span class=\"n\">mpKeyFrameDatabase</span> <span class=\"o\">=</span> <span class=\"k\">new</span> <span class=\"n\">KeyFrameDatabase</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">mpVocabulary</span><span class=\"p\">);</span>\n</code></pre></div><p>系统初始化了地图类（Map），同样是用一个成员指针指“<b>mpMap</b>”向它。Map的具体定义在<a href=\"https://link.zhihu.com/?target=http%3A//Map.cc\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">Map.cc</span><span class=\"invisible\"></span></a>中。<b>该类将保存所有关键帧和地图点的指针</b>。</p><p>系统还初始化了分别指向FrameDrawer和MapDrawer两个类的成员指针mpFrameDrawer、mpMapDrawer。这两个类分别定义在<a href=\"https://link.zhihu.com/?target=http%3A//FrameDrawer.cc\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">FrameDrawer.cc</span><span class=\"invisible\"></span></a>和<a href=\"https://link.zhihu.com/?target=http%3A//MapDrawer.cc\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">MapDrawer.cc</span><span class=\"invisible\"></span></a>中，是系统运行效果可视化的关键。</p><p>然后最关键的，System分别利用对应类的构造函数初始化了指向Tracking类(<a href=\"https://link.zhihu.com/?target=http%3A//Tracking.cc\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">Tracking.cc</span><span class=\"invisible\"></span></a>)、LocalMapping类(<a href=\"https://link.zhihu.com/?target=http%3A//LocalMapping.cc\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">LocalMapping.cc</span><span class=\"invisible\"></span></a>)、LoopClosing类(<a href=\"https://link.zhihu.com/?target=http%3A//LoopClosing.cc\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">LoopClosing.cc</span><span class=\"invisible\"></span></a>)的成员指针<b>mpTracker</b>(Tracking*)、<b>mpLocalMapper</b>(LocalMapping*)、<b>mpLoopCloser</b>(LoopClosing*)。说这三者是最关键的，原因在于这三者分别对应着ORB-SLAM2系统最基本的三个线程（可以参考一下ORB-SLAM2系统的结构图）。其中，System专门初始化了mptLocalMapping(std::thread*)、mptLoopClosing(std::thread*)两个变量来启动局部建图与闭环线程。为什么没有通过线程指针变量启动Tacking类对应的线程呢？因为还有一个主线程，Tacking类就是在主线程中被调用运行的。</p><p><b>Tracker（跟踪线程）：接受输入帧后计算对应的相机位姿（旋转与平移）</b>。它还要以较为宽松的条件决定创建一些新地图点、选出一些关键帧送到局部建图线程。另外，如果跟踪失败了就要进行重定位。</p><p><b>LocalMapper（局部建图线程）：管理着局部地图，还要进行局部集束优化。</b>另外，它也会创建新地图点，还会对地图点、关键帧进行较为严格的筛选剔除。</p><p><b>LoopCloser（闭环线程）：对每一个局部建图线程送过来的关键帧都利用词袋的方法在关键帧数据库里搜索有没有对应的闭环关键帧</b>（搜索到了跟当前关键帧长得很像的之前出现的关键帧，则说明传感器运动轨迹出现了闭环）。如果检测到了闭环，它就会进行一个图优化步骤，并且启动其后的“全局集束优化线程”。当然，检测到闭环后还要进行闭环融合（处理重叠的轨迹与重复的地图点）。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-1677b5595fb21cfb2e64ae0e2f93a568_b.jpg\" data-size=\"normal\" data-rawwidth=\"1149\" data-rawheight=\"793\" class=\"origin_image zh-lightbox-thumb\" width=\"1149\" data-original=\"https://pic1.zhimg.com/v2-1677b5595fb21cfb2e64ae0e2f93a568_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1149&#39; height=&#39;793&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1149\" data-rawheight=\"793\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1149\" data-original=\"https://pic1.zhimg.com/v2-1677b5595fb21cfb2e64ae0e2f93a568_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-1677b5595fb21cfb2e64ae0e2f93a568_b.jpg\"/><figcaption>ORB-SLAM2系统结构图，主要有跟踪、局部建图、闭环三个线程，外加一个独立出来的全局集束优化线程。</figcaption></figure><p>系统运行的可视化也是通过Viewer类(<a href=\"https://link.zhihu.com/?target=http%3A//Viewer.cc\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">Viewer.cc</span><span class=\"invisible\"></span></a>)对应的线程实现的。同前面的线程启动过程一样，先通过Viewer类的构造函数初始化System的成员指针mpViewer(Viewer*)，然后初始化对应的线程成员mptViewer(std::thread)。</p><p>最后，System的构造函数完成各个线程对应的类（mpTracker、mpLocalMapper、mpLoopCloser、mpViewer）之间的相互关联，其实就是保存一下别的类的指针（留个联系方式一样的hh）。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>2）System::TrackStereo</b></p><div class=\"highlight\"><pre><code class=\"language-cpp\"><span class=\"n\">cv</span><span class=\"o\">::</span><span class=\"n\">Mat</span> <span class=\"n\">System</span><span class=\"o\">::</span><span class=\"n\">TrackStereo</span><span class=\"p\">(</span><span class=\"k\">const</span> <span class=\"n\">cv</span><span class=\"o\">::</span><span class=\"n\">Mat</span> <span class=\"o\">&amp;</span><span class=\"n\">imLeft</span><span class=\"p\">,</span> <span class=\"k\">const</span> <span class=\"n\">cv</span><span class=\"o\">::</span><span class=\"n\">Mat</span> <span class=\"o\">&amp;</span><span class=\"n\">imRight</span><span class=\"p\">,</span> <span class=\"k\">const</span> <span class=\"kt\">double</span> <span class=\"o\">&amp;</span><span class=\"n\">timestamp</span><span class=\"p\">)</span>\n</code></pre></div><p>接下来分析的这个函数就是之前讲的整个系统主函数中主循环中的关键函数，其输入是左图(cv::Mat)、右图(cv::Mat)以及对应的时间戳(double)。</p><p>首先检查<b>系统模式是否发生变化</b>（也就是处理每一次输入帧之前都要检查模式是否变化）。<b>（A）</b>如果定位模式激活了（mbActivateLocalizationMode=true）那么就请求LocalMapper停止局部建图，直到其停止下来。紧接着Tracker需要把自己的成员标志“mbOnlyTracking”改为true，也就是跟踪线程将只进行跟踪。然后mbActivateLocalizationMode将被改回false。<b>（B）</b>如果关闭定位模式的标志被激活（mbDeactivateLocalizationMode=true），那么Tracker的“mbOnlyTracking”改为false。然后LocalMapper会release一下（mpLocalMapper-&gt;Release()）。最后mbDeactivateLocalizationMode改回false。</p><p>然后检查<b>系统是否被重置</b>（mbReset是否为true）。如果被重置，则Tracker被重置(mpTracker-&gt;Reset())，然后mbReset被改回false。</p><p>接下来是这个函数中最关键的一行代码：</p><div class=\"highlight\"><pre><code class=\"language-cpp\"><span class=\"n\">cv</span><span class=\"o\">::</span><span class=\"n\">Mat</span> <span class=\"n\">Tcw</span> <span class=\"o\">=</span> <span class=\"n\">mpTracker</span><span class=\"o\">-&gt;</span><span class=\"n\">GrabImageStereo</span><span class=\"p\">(</span><span class=\"n\">imLeft</span><span class=\"p\">,</span><span class=\"n\">imRight</span><span class=\"p\">,</span><span class=\"n\">timestamp</span><span class=\"p\">);</span>\n</code></pre></div><p><b>可以看到System::TrackStereo函数的输入起始被原封不动地输入到了Tracking类的GrabImageStereo方法，其得到的输出是当前帧对应的相机位姿</b>（从世界坐标系到当前帧坐标系的变换矩阵Tcw）。这个相机位姿也是本函数的返回值，也就是说本函数和Tracking::GrabImageStereo的输入输出完全一致。</p><p>最后本函数<b>更新了一下跟踪状态</b>：（1）mTrackingState：当前Tracker的状态，是一个枚举变量。（2）mTrackedMapPoints：当前帧跟踪到的地图点的指针的vector。（3）mTrackedKeyPointsUn：当前帧的无失真的关键点的vector。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>3）System::SaveTrajectoryKITTI</b></p><div class=\"highlight\"><pre><code class=\"language-cpp\"><span class=\"kt\">void</span> <span class=\"n\">System</span><span class=\"o\">::</span><span class=\"n\">SaveTrajectoryKITTI</span><span class=\"p\">(</span><span class=\"k\">const</span> <span class=\"n\">string</span> <span class=\"o\">&amp;</span><span class=\"n\">filename</span><span class=\"p\">)</span>\n</code></pre></div><p>这个函数会根据系统运行时得到相机在各个时刻的位姿来保存一个KITTI数据集官方要求规范的轨迹文件。它是在前面分析的主程序运行结束前被调用的（filename=&#34;CameraTrajectory.txt&#34;）。</p><p>该函数首先从System成员mpMap那里<b>获取到装着所有关键帧指针的 vpKFs</b>(vector)，并把这些指针按照关键帧ID从小到大<b>排序</b>（排序的原因是闭环检测之后第一个关键帧可能并不在第一位）。</p><p>通过“vpKFs[0]-&gt;GetPoseInverse()”得到第一个关键帧的“反位姿Twc”，其被作为<b>传感器的初始位姿Two</b>。</p><p>根据代码注释的说明：每一帧对应的相机位姿是相对于该帧的参考关键帧的位姿存储的（关键帧的位姿会被优化）。因此要获取某一帧的对应位姿必须先得到其参考关键帧的位姿，然后根据二者之间的变换再得到该帧的真正位姿。如果跟踪失败，则该帧的位姿不会保存。</p><p>接下来开始通过循环真正处理并保存轨迹。这里<b>用到的是一些Tracking的成员变量</b>：<b>（1）mlpReference</b>：保存每一帧的参考关键帧指针(KeyFrame*)的list；<b>（2）mlFrameTimes</b>：保存每一帧的时间戳(double)的list；<b>（3）mlRelativeFramePoses</b>：保存每一帧相对其参考关键帧的相对变换(cv::Mat)的list；<b>（4）mlbLost</b>：保存每一帧是否跟踪失败(bool)的list。</p><p>（其实我读了代码感觉作者并没有对跟踪失败的帧不保存轨迹...而且感觉时间戳信息也没用）</p><p><b>第一步，获取每一个参考关键帧相对于第一个关键帧的位姿 Trw</b>，方法为<b>Trw = Tcw * Two</b>。Tcw是参考关键帧自己的坐标系相对于世界坐标系的变换矩阵，Two是世界坐标系相对于前面提到的第一个关键帧的帧坐标系的变换矩阵，这二者相乘就会得到参考关键帧自己的坐标系相对于第一个关键帧的坐标系的变换矩阵Trw（即相对的位姿）。（需要说明一点，实际情况会稍复杂，因为参考关键帧有可能“不好”：pKF-&gt;isBad()=true。如果这样那么参考关键帧就被替换为这个不好的参考关键帧的父母关键帧，然后再计算前述的相对变换矩阵。）</p><p><b>第二步，得到每一个普通帧相对于其参考关键帧的位姿Tcw</b>，方法为<b>Tcw = Tcr * Trw</b>。Tcr是当前帧相对其参考关键帧的相对变换矩阵，保存在mlRelativeFramePoses中；Trw已经由第一步计算出来了。</p><p>得到了Tcw，其实任务已经基本结束了：通过Tcw可以直接得到 <b>Rwc（旋转矩阵）</b>与 <b>twc（平移向量）</b>。把这两者的数据按照KITTI要求存成一行即可。最后保存的轨迹文件CameraTrajectory.txt的每一行对应着一个输入帧的Rwc与twc。</p><hr/><h2>3.  <a href=\"https://link.zhihu.com/?target=http%3A//Tracking.cc\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">Tracking.cc</span><span class=\"invisible\"></span></a></h2><p>跟踪线程是系统的第一个线程，在System初始化时用到了Tracking类的构造函数，并且前文分析看到位姿的获取实际是调用了Tracking类的方法。</p><p><b>1）ORB_SLAM2::Tracking的构造函数</b></p><div class=\"highlight\"><pre><code class=\"language-cpp\"><span class=\"n\">Tracking</span><span class=\"o\">::</span><span class=\"n\">Tracking</span><span class=\"p\">(</span><span class=\"n\">System</span> <span class=\"o\">*</span><span class=\"n\">pSys</span><span class=\"p\">,</span> <span class=\"n\">ORBVocabulary</span><span class=\"o\">*</span> <span class=\"n\">pVoc</span><span class=\"p\">,</span> <span class=\"n\">FrameDrawer</span> <span class=\"o\">*</span><span class=\"n\">pFrameDrawer</span><span class=\"p\">,</span> <span class=\"n\">MapDrawer</span> <span class=\"o\">*</span><span class=\"n\">pMapDrawer</span><span class=\"p\">,</span> \n                   <span class=\"n\">Map</span> <span class=\"o\">*</span><span class=\"n\">pMap</span><span class=\"p\">,</span> <span class=\"n\">KeyFrameDatabase</span><span class=\"o\">*</span> <span class=\"n\">pKFDB</span><span class=\"p\">,</span> <span class=\"k\">const</span> <span class=\"n\">string</span> <span class=\"o\">&amp;</span><span class=\"n\">strSettingPath</span><span class=\"p\">,</span> <span class=\"k\">const</span> <span class=\"kt\">int</span> <span class=\"n\">sensor</span><span class=\"p\">)</span><span class=\"o\">:</span>\n    <span class=\"n\">mState</span><span class=\"p\">(</span><span class=\"n\">NO_IMAGES_YET</span><span class=\"p\">),</span> <span class=\"n\">mSensor</span><span class=\"p\">(</span><span class=\"n\">sensor</span><span class=\"p\">),</span> <span class=\"n\">mbOnlyTracking</span><span class=\"p\">(</span><span class=\"nb\">false</span><span class=\"p\">),</span> <span class=\"n\">mbVO</span><span class=\"p\">(</span><span class=\"nb\">false</span><span class=\"p\">),</span> <span class=\"n\">mpORBVocabulary</span><span class=\"p\">(</span><span class=\"n\">pVoc</span><span class=\"p\">),</span>\n    <span class=\"n\">mpKeyFrameDB</span><span class=\"p\">(</span><span class=\"n\">pKFDB</span><span class=\"p\">),</span> <span class=\"n\">mpInitializer</span><span class=\"p\">(</span><span class=\"k\">static_cast</span><span class=\"o\">&lt;</span><span class=\"n\">Initializer</span><span class=\"o\">*&gt;</span><span class=\"p\">(</span><span class=\"nb\">NULL</span><span class=\"p\">)),</span> <span class=\"n\">mpSystem</span><span class=\"p\">(</span><span class=\"n\">pSys</span><span class=\"p\">),</span> <span class=\"n\">mpViewer</span><span class=\"p\">(</span><span class=\"nb\">NULL</span><span class=\"p\">),</span>\n    <span class=\"n\">mpFrameDrawer</span><span class=\"p\">(</span><span class=\"n\">pFrameDrawer</span><span class=\"p\">),</span> <span class=\"n\">mpMapDrawer</span><span class=\"p\">(</span><span class=\"n\">pMapDrawer</span><span class=\"p\">),</span> <span class=\"n\">mpMap</span><span class=\"p\">(</span><span class=\"n\">pMap</span><span class=\"p\">),</span> <span class=\"n\">mnLastRelocFrameId</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n</code></pre></div><p>输入为系统指针pSys，词典指针pVoc，绘图类的指针pFrameDrawer、pMapDrawer，地图指针pMap，关键帧数据库指针pKFDB，系统配置文件名strSettingPath，传感器类型(int)。</p><p>接下来一些成员变量的初始化：前文提到过的Tracking线程状态mState为NO_IMAGES_YET(枚举变量)；传感器类型eSensor即为输入的sensor；系统默认跟踪且建图，因此mbOnlyTracking为false；mbVO是一个标志位，其仅当处于定位模式且当前帧关键点与前一帧地图点没有足够时为true，那时系统将只是一个能实现定位功能的视觉里程计；mpORBVocabulary初始化为输入的词典指针pVoc；mpKeyFrameDB初始化为输入的关键帧数据库指针pKFDB；mpInitializer是使用单目相机情况下进行初始化的，可以忽略；mpSystem初始化为输入的系统指针pSys；可视化类的指针mpViewer初始为NULL；mpFrameDrawer、mpMapDrawer、mpMap也都和前述变量类似初始化为对应的输入项；mnLastRelocFrameId记录了上一次进行重定位的帧的ID，初始化为0。</p><p><b>Tracking的构造函数的工作就是从系统配置文件读取相机参数</b>。以跑KITTI的00-02序列所用配置文件KITTI00-02.yaml为例，该文件部分内容截图如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-a3d6e3987410e173ba5c06b65e504328_b.jpg\" data-size=\"normal\" data-rawwidth=\"736\" data-rawheight=\"340\" class=\"origin_image zh-lightbox-thumb\" width=\"736\" data-original=\"https://pic1.zhimg.com/v2-a3d6e3987410e173ba5c06b65e504328_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;736&#39; height=&#39;340&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"736\" data-rawheight=\"340\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"736\" data-original=\"https://pic1.zhimg.com/v2-a3d6e3987410e173ba5c06b65e504328_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-a3d6e3987410e173ba5c06b65e504328_b.jpg\"/><figcaption>KITTI00-02.yaml文件部分内容</figcaption></figure><p>其读取与前文写的System检查它所用的方法相同，即构造OpenCV的cv::FileStorage类从而实现读取，部分代码如下：</p><div class=\"highlight\"><pre><code class=\"language-cpp\"><span class=\"n\">cv</span><span class=\"o\">::</span><span class=\"n\">FileStorage</span> <span class=\"n\">fSettings</span><span class=\"p\">(</span><span class=\"n\">strSettingPath</span><span class=\"p\">,</span> <span class=\"n\">cv</span><span class=\"o\">::</span><span class=\"n\">FileStorage</span><span class=\"o\">::</span><span class=\"n\">READ</span><span class=\"p\">);</span>\n    <span class=\"kt\">float</span> <span class=\"n\">fx</span> <span class=\"o\">=</span> <span class=\"n\">fSettings</span><span class=\"p\">[</span><span class=\"s\">&#34;Camera.fx&#34;</span><span class=\"p\">];</span>\n    <span class=\"kt\">float</span> <span class=\"n\">fy</span> <span class=\"o\">=</span> <span class=\"n\">fSettings</span><span class=\"p\">[</span><span class=\"s\">&#34;Camera.fy&#34;</span><span class=\"p\">];</span>\n</code></pre></div><p>（1）使用上述方法先读取到相机内参fx、fy、cx、cy，这样相机的<b>内参矩阵K</b>就被确定了。（2）读取相机的<b>径向畸变参数</b>k1、k2、k3以及<b>切向畸变参数</b>p1、p2。（3）读取相机的<b>参数bf</b>，该参数实际是“双目相机基线b × 相机内参fx”（根据双目相机模型，深度d=bf/视差）。（4）读取相机帧速率fps。（5）读取参数RGB（颜色存储顺序是RGB而不是BGR）。（6）每张图提取的特征数nFeatures，图像金字塔每层间的尺度因子scaleFactor，图像金字塔层数nLevels，提取FAST角点时所使用的初始响应值阈值iniThFAST，还有如果提取角点所用的响应值的初始阈值iniThFAST过高则还有一个最小阈值参数minThFAST。（7）对于采用双目相机的情况，ORB-SLAM2系统根据特征点对应的深度值大小区分了近点与远点，所用的阈值为<b>ThDepth</b>（近点的位移信息更准确，仅需要简单的三角化来初始化对应的地图点）。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>2）Tracking::GrabImageStereo</b></p><div class=\"highlight\"><pre><code class=\"language-cpp\"><span class=\"n\">cv</span><span class=\"o\">::</span><span class=\"n\">Mat</span> <span class=\"n\">Tracking</span><span class=\"o\">::</span><span class=\"n\">GrabImageStereo</span><span class=\"p\">(</span><span class=\"k\">const</span> <span class=\"n\">cv</span><span class=\"o\">::</span><span class=\"n\">Mat</span> <span class=\"o\">&amp;</span><span class=\"n\">imRectLeft</span><span class=\"p\">,</span> <span class=\"k\">const</span> <span class=\"n\">cv</span><span class=\"o\">::</span><span class=\"n\">Mat</span> <span class=\"o\">&amp;</span><span class=\"n\">imRectRight</span><span class=\"p\">,</span> <span class=\"k\">const</span> <span class=\"kt\">double</span> <span class=\"o\">&amp;</span><span class=\"n\">timestamp</span><span class=\"p\">)</span>\n</code></pre></div><p>根据前文，该函数是跟踪线程的核心，其被System::TrackStereo调用，输入的左右图与时间戳信息获取当前帧坐标系相对于世界坐标系的变换矩阵<b>Tcw</b>。</p><p>首先该函数把输入的左右图分别<b>转为灰度图</b>保存为Tracking的成员变量mImGray、mImGrayRight。</p><p>然后利用左右灰度图与时间戳信息<b>构造一个Frame类对象</b>（<a href=\"https://link.zhihu.com/?target=http%3A//Frame.cc\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">Frame.cc</span><span class=\"invisible\"></span></a>），其保存为Tracking的成员变量<b>mCurrentFrame</b>。</p><p>接下来<b>执行Tracking类的核心函数：Track()</b>。</p><p>Track()函数中将得到当前帧的Tcw，也就是mCurrentFrame的成员变量mTcw，其将被本函数返回。</p><p>综上，<b>本函数的核心其实是调用了Frame的构造函数以及Tracking::Track()</b>。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>3）Tracking::Track()</b></p><div class=\"highlight\"><pre><code class=\"language-cpp\"><span class=\"kt\">void</span> <span class=\"n\">Tracking</span><span class=\"o\">::</span><span class=\"n\">Track</span><span class=\"p\">()</span>\n</code></pre></div><p>上一个函数中说了，本函数是Tracking类的核心。</p><p>本函数主要工作：<b>初始化——初始跟踪估计相机位姿——跟踪局部地图——决定新的关键帧——当前帧相关信息保存</b>。运行有两种模式：只跟踪、跟踪且建图。</p><p>Tracking构造函数刚刚构造完的时候，Tracking的成员mState为NO_IMAGES_YET。这样的情况下，本函数首先把mState改为NOT_INITIALIZED，成员mLastProcessedState改为mState。接下来锁住地图的更新，进行后续的工作。</p><p><b>第一步：初始化</b>。</p><p>双目情况的初始化直接调用 Tracking::StereoInitialization() 函数进行。初始化之后通过“mpFrameDrawer-&gt;Update(this)”更新一下FrameDrawer。初始化的最后再检查一下mState：如果状态不是OK，那么本函数直接return结束运行；Tracking的状态OK则继续后续工作。</p><p><b>第二步：通过跟踪初始估计相机位姿</b>。</p><p>ORB-SLAM2系统并不是我们想的那样直接通过特征点与点云（地图点）的匹配计算出位姿。实际情况是：每一帧对应的<b>初始相机位姿估计是通过运动模型或者跟踪参考关键帧得到的</b>（如果跟踪失败，则位姿估计是通过重定位得到的）。</p><ul><li><b>跟踪且建图模式</b></li></ul><p>跟踪且建图模式下局部建图是被激活的。</p><p>如果Tracking的状态mState不是OK，直接调用 Tracking::Relocalization() 进行重定位，否则才进行一般的跟踪。跟踪之前，由于局部建图线程可能改变前一帧中被跟踪的地图点，因此先调用了 Tracking::CheckReplacedInLastFrame() 更新一下点云。一般的跟踪也有两种情况。跟踪情况①：没有对于速度的估计（mVelocity为空），或者距离上一次重定位才过去不超过1帧。跟踪情况②：情况①的否定。</p><p><b>跟踪情况①</b>：调用 Tracking::TrackReferenceKeyFrame() 跟踪当前帧的参考关键帧。</p><p><b>跟踪情况②：</b>调用 Tracking::TrackWithMotionModel() 通过运动模型跟踪。如果使用运动模型后匹配的点数不够多（之后具体解释），说明运动模型是失效的，那么还是跟情况①一样跟踪参考关键帧。</p><ul><li><b>只跟踪不建图模式</b></li></ul><p>该模式下如果mState是LOST就直接重定位，否则开始跟踪。跟踪根据Tracking的成员变量mbVO是否为true分为两种。mbVO为true说明在只跟踪模式下，当前帧与上一帧基本没匹配点对。</p><p><b>mbVO=true</b>。这种情况同时进行运动模型跟踪（如果mVelocity不为空）与重定位。如果不能用运动模型跟踪或者运动模型不可靠但是重定位结果好，那么就用重定位结果且mbVO改为false；否则就用运动模型的跟踪结果（这一步mbVO一直保持为true），并且对于采用运动模型跟踪时观察到的不是外点的地图点，这些地图点的成员变量mnFound要+1（说明其又被新的一帧观察到了）。当然，也有可能重定位和运动模型跟踪的结果都不好，这在后面会进行处理。</p><p><b>mbVO=false</b>。这种情况下有速度就用运动模型，否则跟踪参考关键帧。</p><p>以上就是跟踪来进行初始的相机位姿估计的步骤，其实就只有三种可能操作：<b>用运动模型跟踪</b>、<b>用参考关键帧跟踪</b>、<b>重定位</b>。需要说明的是，这三种操作都会返回一个bool值表示其结果是否可接受，记为<b>bOK</b>（实际采用的那个的返回值是本函数中的bOK）。</p><p><b>第三步：跟踪局部地图。</b></p><p>在跟踪且建图情况下，如果bOK为真，则调用 Tracking::TrackLocalMap() 跟踪局部地图；在只跟踪情况下，只有bOK为真且mbVO为假（mbVO为真则说明跟局部地图也没有什么匹配的点，无法跟踪局部地图）时才调用 Tracking::TrackLocalMap() 跟踪局部地图。因此，跟踪局部地图就是调用 Tracking::TrackLocalMap() 这个函数，该函数也会返回一个bool值表示跟踪结果是否令人满意，其将被作为新的<b>bOK</b>。</p><p>如果跟踪局部地图后bOK为真，则Tracking的状态mState为OK；否则mState状态为LOST。需要注意的是，有可能跟踪估计位姿那一步结果就不好因此没有进行后续的局部地图跟踪，该情况也会有bOK=false，因此mState也会是LOST。</p><p>接下来再次更新一下mpFrameDrawer。接着，如果bOK=true，本函数第四步开始决定要不要创建新的关键帧（前面说过ORB-SLAM2是基于关键帧优化的SLAM系统，这里开始将挑出关键帧）。</p><p><b>第四步：检查是否创建新的关键帧。</b>（前提是bOK=true，说明跟踪正常）</p><p>在真正决定是否创建新关键帧前先做一些其他的工作。</p><p>（1）更新运动模型的速度。Tacking的成员mLastFrame记录了上一个被处理的帧，如果mLastFrame的位姿(mTcw)为空则无法获得当前速度估计（mVelocity为空）；否则通过mLastFrame.mTcw计算出运动模型的速度mVelocity（<b>速度 = 当前帧的Tcw × 上一帧的Twc</b>）。这里我顺便来理解一下运<b>动模型的速度mVelocity</b>。上一帧和当前帧位姿知道后我们就能估计一个速度，这个速度可以在Tracking线程跟踪下一帧时使用。我的理解是这个速度实际是相同时间间隔（相机帧率是固定的）内相邻两帧坐标系之间的变换矩阵（<b>等式变为：速度 × 上一帧的Tcw = 当前帧的Tcw</b>），也就是说系统会试图用上一帧与当前帧之间的变换矩阵作为当前帧和下一帧之间的变换矩阵，这样等于认为相同时间间隔内的变换矩阵是相同的，也就被称为<b>匀速运动模型</b>。这一步结束后，mpMapDrawer把当前帧位姿mTcw设置为当前的相机位姿。</p><p>（2）首先需要说明一些新内容。ORB-SLAM2的关键是提取每一帧中的ORB特征点，这是系统运行的基础（我们真正利用的信息仅仅是特征点，因此系统的建图也是稀疏的）。每个Frame类会把其提取出的关键点的数量记为N（1~N同时也作为了这些关键点的索引）。每个<b>Frame类还有一个成员mvpMapPoints</b>，其容量也是N。如果对应索引（例：第 i 个）的关键点有匹配的地图点，则那个地图点的指针会被存到对应关键点索引的位置上（例：mvpMapPoints[ i ] ）；否则 mvpMapPoints[ i ] 就会是空的。<b>Frame类还有一个成员mvbOutlier</b>，它每一位表示对应索引的关键点的匹配到的地图点是否是外点（没有对应地图点则为false）。第（2）步中，检查当前帧的每个关键点对应的地图点：如果存在该对应地图点且没有关键帧观察到它（地图点的成员nObs&lt;1），则这个对应地图点从mvpMapPoints中删去（对应位置为空的MapPoint*），mvbOutlier对应位置为false。</p><p>（3）清理临时地图点。临时地图点保存在Tracking的成员 mlpTemporalPoints 之中，这里直接清空。所谓的临时地图点是指 Tracking::UpdateLastFrame 函数（TrackWithMotionModel函数中被调用）中人工补充的地图点。</p><p>然后<b>开始检查是否需要新关键帧</b>（Tracking::NeedNewKeyFrame()返回true，这一步条件比较宽松，因为局部建图线程还会剔除一些关键帧）。如果需要则调用Tracking::CreateNewKeyFrame() <b>创建新的关键帧</b>。</p><p>然后线程会<b>把当前帧关键对应的是外点的地图点清空</b>了。这个操作就是通过mCurrentFrame的mvpMapPoints和mvbOutlier完成的，可参考第四步（2）中说明。为什么在创建关键帧之后才清除是外点的地图点？根据代码的注释，关键帧将在后续进行集束优化，该优化将决定那些点是否真的是外点，因此不急着清除。而清除这些地图点的主要原因是不希望它们影响下一帧的位姿估计。</p><p><b>第五步：当前帧相关信息保存在Tracking的成员中</b>。</p><p>最后<b>保存一下当前帧的位姿等信息</b>（前面分析过的保存相机轨迹要用）。如果当前帧的mTcw已经得到了，则进行以下操作：（1）计算当前帧相对于参考关键帧的变换Tcr（Tcr = Tcw × Twr），Tcr被推入Tracking的成员 mlRelativeFramePoses 中保存；（2）当前帧的时间戳推入Tracking的成员 mlFrameTimes 中保存；（3）当前帧的参考关键帧推入Tracking的成员 mlpReferences 中保存；（4）当前帧是否跟踪失败的bool信息（mState==LOST）推入Tracking的成员 mlbLost 中保存。当然跟踪失败的时候当前帧的mTcw未知，则前面（1）（2）（3）步都只在尾部重复保存对应三个列表的尾部的元素，然后mlbLost尾部保存true。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>3）Tracking::StereoInitialization</b></p><div class=\"highlight\"><pre><code class=\"language-cpp\"><span class=\"kt\">void</span> <span class=\"n\">Tracking</span><span class=\"o\">::</span><span class=\"n\">StereoInitialization</span><span class=\"p\">()</span>\n</code></pre></div><p>前面Track()函数分析中刚开始的初始化就调用了本函数。本函数主要工作（条件是当前帧关键点足够多）：<b>当前帧设置为起始帧——当前帧构造为第一个关键帧——创建初始的点云地图——相关后处理</b>。</p><p><b>本函数要起作用的条件是：当前帧中关键点数量N&gt;500</b>。如果这个条件不满足，本函数不起作用；对于Track()函数来说，它将跳过当前帧，对之后的帧继续尝试初始化，直到初始化成功为止。本条件满足后，函数开始进行后续工作。</p><p><b>第一步：当前帧被设置为第一帧。</b></p><p>初始化就是在处理第一帧，该帧将被作为跟踪的起点。本步骤的实际操作是：<b>将当前帧的位姿Tcw设置为单位矩阵</b>。也就是说当前帧无旋转、无位移（毕竟它是起点嘛）。</p><p><b>第二步：当前帧构造为关键帧</b>。</p><p>直接使用KeyFrame的构造函数构造第一个关键帧，本函数将得到该关键帧的指针。由于地图类记录着所有关键帧的指针，因此把第一个关键帧加入到Tracking::mpMap中（代码为mpMap-&gt;AddKeyFrame(pKFini)）。</p><p><b>第三步：创建初始的地图点并建立必要的数据关联</b>。</p><p>该步骤将遍历当前帧的所有关键点，通过深度z&gt;0的关键点创建出对应地图点，具体步骤如下。（1）通过当前帧的 UnprojectStereo 方法将<b>第 i 个关键点反投影得到三维坐标 x3D</b> (cv::Mat)。（2）构造<b>新的MapPoint</b>（构造函数输入x3D、初始关键帧指针、地图指针mpMap），得到该地图点指针 pNewMP (MapPoint*)。（3）地图点记录下初始关键帧观察到了自己（pNewMP-&gt;AddObservation(pKFini,i)），初始关键帧记录下自己观察到了该地图点（pKFini-&gt;AddMapPoint(pNewMP,i)），地图点通过自己的 ComputeDistinctiveDescriptors() 方法计算最好的描述子（初始化中只有一个初始的关键帧观察到它，因此就使用地图点在该关键帧中的描述子）。（4）该地图点使用其 UpdateNormalAndDepth() 方法计算自己平均被观察的方向以及尺度不变距离。（5）该地图点加入到地图当中（mpMap-&gt;AddMapPoint(pNewMP)）。（6）当前帧也记录了该地图点，这条记录实际也通过索引与该地图点的对应关键点相关联（mCurrentFrame.mvpMapPoints[ i ]=pNewMP）。</p><p>一个初始的地图（很稀疏的点云）将通过第三步建立起来。</p><p><b>第四步：相关后处理</b>。</p><p>（1）首先<b>第一个关键帧得送到局部建图线程</b>中去。（2）<b>更新Tracking类的“上一…”信息</b>：当前帧通过一个Frame构造函数初始并设置为Tracking类的上一帧mLastFrame，当前帧的ID（mnID）设置为上一个关键帧的ID（mnLastKeyFrameId），构造的第一个关键帧记录为上一个关键帧（mpLastKeyFrame）。（3）<b>Tracking类的局部地图</b>（不是地图）<b>的相关数据记录</b>：初始关键帧推入mvpLocalKeyFrames 保存，当前已有的初始点云保存为 mvpLocalMapPoints，初始关键帧保存为当前Tracking的参考关键帧成员 mpReferenceKF。（4）<b>当前帧的参考关键帧设为根据自己创建的初始关键帧</b>，地图的参考地图点 mvpReferenceMapPoints 设置为前面保存的局部地图点，初始关键帧被推入地图的 mvpKeyFrameOrigins 保存。（5）当前帧的位姿Tcw作为当前相机位姿传递给MapDrawer。（6）<b>Tracking的状态eState设为OK</b>。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>4）Tracking::CheckReplacedInLastFrame()</b></p><div class=\"highlight\"><pre><code class=\"language-cpp\"><span class=\"kt\">void</span> <span class=\"n\">Tracking</span><span class=\"o\">::</span><span class=\"n\">CheckReplacedInLastFrame</span><span class=\"p\">()</span>\n</code></pre></div><p>本函数遍历<b>上一帧</b>所有关键点所对应的地图点（保存在 mLastFrame.mvpMapPoints 中），检查一下<b>地图点是否存在对应的替换点</b>（地图点的 GetReplaced() 方法是否可以返回一个地图点指针）。局部建图线程可能会使一些地图点存在对应的替换点，如果有替换点，就用其替换原本的上一帧地图点。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>5）Tracking::TrackReferenceKeyFrame()</b></p><div class=\"highlight\"><pre><code class=\"language-cpp\"><span class=\"kt\">bool</span> <span class=\"n\">Tracking</span><span class=\"o\">::</span><span class=\"n\">TrackReferenceKeyFrame</span><span class=\"p\">()</span>\n</code></pre></div><p>如前文所述，本函数完成的跟踪参考关键帧工作是跟踪估计帧位姿的方法之一。其大致步骤为：当前帧与<b>参考关键帧通过词袋快速匹配——当前帧记录成功匹配的地图点——PnP优化——处理地图点中的外点</b>。</p><p><b>第一步：首先在当前帧与参考关键帧之间进行ORB关键点与地图点的匹配</b>。</p><p>ORB-SLAM2系统中的ORB匹配都会通过词袋方法加速。（1）使用当前帧的 ComputeBoW() 方法计算当前帧的词袋向量。（2）使用对应构造函数初始化一个matcher（ORB_SLAM2::ORBmatcher ），使用其方法 SearchByBoW 在Tracking类的当前帧mCurrentFrame以及当前参考关键帧mpReferenceKF之间。对于匹配成功的当前帧的关键点，其对应参考关键帧的地图点的指针将被保存在向量 vpMapPointMatches中（与Frame的mvpMapPoints一样的形式，依次对应当前帧的关键点，若无对应则相应位置上元素为NULL）。（3）如果匹配点对少于15对，则本函数返回false（也就是前文提到的bOK为false，说明跟踪参考关键帧失败了）。</p><p>如果这一步的匹配成功，当前帧将把匹配的地图点 vpMapPointMatches 保存为自己的成员 mvpMapPoints。</p><p><b>第二步：使用PnP算法优化估计位姿。</b></p><p>首先将当前帧的位姿设置为上一帧的位姿Tcw，然后使用下面的代码对当前帧进行PnP位姿优化（具体的代码在<a href=\"https://link.zhihu.com/?target=http%3A//Optimizer.cc\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">Optimizer.cc</span><span class=\"invisible\"></span></a>中）。</p><div class=\"highlight\"><pre><code class=\"language-cpp\"><span class=\"n\">Optimizer</span><span class=\"o\">::</span><span class=\"n\">PoseOptimization</span><span class=\"p\">(</span><span class=\"o\">&amp;</span><span class=\"n\">mCurrentFrame</span><span class=\"p\">);</span>\n</code></pre></div><p><b>第三步：抛弃匹配地图点中的外点。</b></p><p>第二步的优化会决定匹配成功的那些地图点是否为外点（该bool信息保存在当前帧的mvbOutlier之中），因此跟踪的最后要抛弃掉外点。此过程与前文提到的 Track() 中处理外点的方法一样，是外点就把当前帧的mvpMapPoints对应位置改为空指针，mvbOutlier对应位置改回false。</p><p>另外，该地图点的成员mbTrackInView应改为false，表示这个地图点不再当前Tracking的视野之内；该地图点的成员 mnLastFrameSeen 改为当前帧的ID，表示最后跟丢这一个地图点时对应的帧是当前帧。</p><p>抛弃完外点之后最终可以确定匹配点数，如果其&gt;=10，则跟踪成功并返回true；否则跟踪当前参考关键帧失败，返回false（bOK=false）。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>6）Tracking::UpdateLastFrame</b></p><div class=\"highlight\"><pre><code class=\"language-cpp\"><span class=\"kt\">void</span> <span class=\"n\">Tracking</span><span class=\"o\">::</span><span class=\"n\">UpdateLastFrame</span><span class=\"p\">()</span>\n</code></pre></div><p><b>该函数将在使用运动模型跟踪时被调用，根据上一帧的参考关键帧更新上一帧的位姿</b>（处于只跟踪模式且上一帧没有对应关键帧，则补全上一帧的一些“近关键点”对应的地图点作为<b>临时地图点</b>）。</p><p><b>第一步：根据上一帧的参考关键帧更新上一帧的位姿。</b></p><p>（1）从Tracking的 mlRelativeFramePoses 尾部获得上一帧的参考关键帧到上一帧相对变换矩阵 <b>Tlr</b>。（2）上一帧的位姿 = Tlr × 上一帧的参考关键帧的位姿 Trw。</p><p>这一步完成后，如果系统满足“不是只跟踪模式、或是单目传感器情况、或是上一帧被创建了关键帧”，则直接结束本函数；否则进行后续步骤。</p><p><b>第二步：创建“视觉里程计”的地图点（临时地图点）。</b></p><p>首先将上一帧的关键点按照深度从小到大的顺序排序（深度必须为正）。如果上一帧的关键点对应的地图点不存在或者存在但没有被任何关键帧观察到，则创建对应的“视觉里程计”的地图点（临时地图点）。</p><p>接下来创建临时地图点，方法与Track()初始化时创建初始地图点方法一致：反投影获得三维坐标 x3D 后用构造函数生成新地图点。稍微有区别的是，这一步中创建新地图点时构造函数中输入了其对应关键点的索引 i ，另外该地图点仅保存进上一帧的 mvpMapPoints 以及Tracking的成员 <b>mlpTemporalPoints（本步骤仅是补充临时地图点）</b>之中。</p><p>该<b>创建临时地图点的步骤停止条件</b>：当前正在处理的关键点已经是远点（深度大于Tracking.mThDepth）并且有地图点（包括临时地图点）的关键点已经大于100个。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>7）Tracking::TrackWithMotionModel</b></p><div class=\"highlight\"><pre><code class=\"language-cpp\"><span class=\"kt\">bool</span> <span class=\"n\">Tracking</span><span class=\"o\">::</span><span class=\"n\">TrackWithMotionModel</span><span class=\"p\">()</span>\n</code></pre></div><p>本函数使用“匀速”运动模型估计当前帧位姿，主要工作为：更新上一帧位姿——当前帧位姿用运动模型的假设去计算更新——用当前帧去匹配上一帧的地图点——优化——处理外点。</p><p><b>第一步：调用UpdateLastFrame()更新上一帧位姿（可能创建上一帧的临时地图点）</b>。</p><p><b>第二步：使用运动模型计算当前帧位姿（当前帧Tcw = Tracking.mVelocity * 上一帧位姿Tcw）</b>。</p><p><b>第三步：将上一帧的地图点（可能包括第一步添加的临时地图点）投影至当前帧并与关键点匹配</b>。（1）清空当前帧的地图点（即mvpMapPoints中全部为空指针）。（2）创建ORBmatcher并使用其 <b>SearchByProjection</b> 方法进行上一帧地图点与当前帧关键点之间的匹配。</p><p>这一步的匹配需要设置阈值th，该阈值控制上一帧地图点投影到当前帧并在附近搜索关键点的范围大小（th越小肯定越难匹配到对应关键点）。第一次匹配i使用的th=15，若匹配点对少于20个则调整th=30并再次匹配；若第二次匹配点对还少于20则本函数返回false，通过运动模型跟踪失败。</p><p><b>第三步：根据匹配点对进行当前帧位姿优化</b>。前面使用运动模型估计的位姿肯定是不准确的，这一步使用匹配点对进行位姿优化，方法同跟踪参考关键帧（调用 PoseOptimization ）。不同的是：跟踪参考关键帧时不会首先估计一个位姿再优化。</p><p><b>第四步：抛弃外点。</b></p><p>这一步与 Tracking::TrackReferenceKeyFrame() 中抛弃外点的方法一致，可以参考前文。</p><p><b>最后判断一下本次跟踪状态</b>。</p><p><b>如果Tracking处于只跟踪模式</b>，那么如果当前帧有效的地图点（不是外点且有关键帧观察到它）少于10个，则Tracking的 mbVO 被设置为true；如果当前帧有效的地图点多于20个，则本次跟踪成功。<b>如果Tracking处于跟踪且建图模式</b>，那么只要当前帧有效地图点多于9个即跟踪成功，本函数返回true。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>8）Tracking::TrackLocalMap()</b></p><div class=\"highlight\"><pre><code class=\"language-cpp\"><span class=\"kt\">bool</span> <span class=\"n\">Tracking</span><span class=\"o\">::</span><span class=\"n\">TrackLocalMap</span><span class=\"p\">()</span>\n</code></pre></div><p>Track()通过跟踪前一帧或参考关键帧（或重定位）获取到当前帧位姿的估计之后，会调用本函数来跟踪局部地图。本函数主要工作流程为：<b>更新局部地图——当前帧与局部地图匹配——优化当前帧位姿——更新当前帧匹配到的地图点的信息——根据匹配内点数决定匹配是否成功（刚刚重定位的话条件更严格）。</b></p><p><b>第一步：获取局部点云地图</b>。</p><p>这一步实际使用 Tracking::UpdateLocalMap() 函数更新Tracking维护的局部地图（mvpLocalMapPoints（同时设置为mpMap的参考地图点mvpReferenceMapPoints）、mpReferenceKF（同时也是当前帧的参考关键帧））。</p><p><b>第二步：当前帧关键点与局部地图中地图点匹配。</b></p><p>这一步使用 Tracking::SearchLocalPoints() 在当前帧与局部地图之间进行点的匹配。</p><p><b>第三步：当前帧位姿优化。</b></p><p>与运动模型估计与跟踪参考帧时一样，采用 Optimizer::PoseOptimization(&amp;mCurrentFrame) 优化当前帧位姿。</p><p><b>第四步：更新地图点的数据，决定本次跟踪局部地图是否成功。</b></p><p>遍历当前帧跟踪到的地图点 mvpMapPoints，如果是外点则抛弃；如果不是外点则该地图点的 mnFound +1。另外，对于不是外点的地图点：如果只跟踪模式，则匹配内点数（ Tracking.mnMatchesInliers ）+1；否则如果该点被关键帧观察到，则匹配内点数+1。</p><p>（1）当前帧ID&lt;mnLastRelocFrameId+帧率mMaxFrames（该条件表示刚刚重定位过）的情况下：如果匹配点对数量不足50则本次跟踪局部地图失败，返回false。</p><p>（2）一般情况下或（1）没有直接返回，那么匹配点对数小于30返回false，否则跟踪成功返回true。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>9）Tracking::NeedNewKeyFrame()</b></p><div class=\"highlight\"><pre><code class=\"language-cpp\"><span class=\"kt\">bool</span> <span class=\"n\">Tracking</span><span class=\"o\">::</span><span class=\"n\">NeedNewKeyFrame</span><span class=\"p\">()</span>\n</code></pre></div><p>Tracking线程需要在尾部决定是否创建新的关键帧，本函数就完成了此判断。<b>本函数的判断结果主要取决于当前跟踪效果是否理想、当前帧的跟踪质量好坏、当前局部建图线程是否有空</b>。</p><p>如果Tracking处于<b>只跟踪模式</b>，则本函数直接返回false，不需要新关键帧；如果<b>局部建图线程因为检测到了闭环而被终止或正在终止</b>，则本函数直接返回false；如果<b>距离上一次重定位不久</b>，直接返回false。</p><p>接下来的判断较为复杂。首先建立了几个自条件是否满足的bool标志：<b>c1a：</b>距离上一次插入关键帧过去了mMaxFrames以上；<b>c1b：</b>距离上一次插入关键帧仅仅过去mMinFrames以上并且局部建图线程空闲；<b>c1c：</b>当前跟踪<b>很弱</b>(当前帧匹配的内点<b>远少于</b>参考关键帧中被观察数较多的地图点的数量) ，或Tracking现在需要插入更多的近地图点（当前帧有匹配地图点的近关键点很少并且还有很多近关键点没有匹配到地图点）；<b>c2a</b>：当前跟踪<b>较弱</b>(当前帧匹配的内点<b>较少于</b>参考关键帧中被观察数较多的地图点的数量) ，或Tracking现在需要插入更多的近地图点（当前帧有匹配地图点的近关键点很少并且还有很多近关键点没有匹配到地图点）；<b>c2b</b>：当前帧匹配的是内点的地图点数量大于15个。</p><p>后续判断必须满足条件： (c1a || c1b || c1c) &amp;&amp; c2a &amp;&amp; c2b。注意，c2b必须满足，因为只有满足了c2b才能说明<b>当前帧有价值作为新关键帧</b>，而其他的条件是说明我们<b>可以插入新关键帧</b>或者<b>跟踪效果不好必须要插入关键帧</b>。该条件不满足则直接返回false，满足则进行后续判断。</p><p><b>①局部建图线程空闲</b>：直接返回true，决定创建新关键帧。</p><p><b>②局部建图线程不空闲</b>：首先让局部建图线程去<b>打断集束优化操作</b>（mpLocalMapper-&gt;InterruptBA()）。最后，如果<b>局部建图线程的关键帧队列中关键帧数量小于3</b>则返回true，否则返回false。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>10）Tracking::CreateNewKeyFrame()</b></p><div class=\"highlight\"><pre><code class=\"language-cpp\"><span class=\"kt\">void</span> <span class=\"n\">Tracking</span><span class=\"o\">::</span><span class=\"n\">CreateNewKeyFrame</span><span class=\"p\">()</span>\n</code></pre></div><p>该函数主要工作流程：<b>停止局部地图——当前帧创建新KF——补全当前帧近关键点对应地图点——新关键帧插入局部建图——局部建图继续</b>。</p><p><b>第一步：停止局部建图线程。</b></p><p>首先通过“mpLocalMapper-&gt;SetNotStop(true)”停止局部建图，如果不成功则本函数直接返回；否则继续后面的步骤。</p><p><b>第二步：将当前帧构造为关键帧。</b></p><p>使用关键帧的构造函数将当前帧构造为关键帧，并将其设置为Tracking类和当前帧的“参考关键帧”。然后更新一下当前帧的旋转矩阵、位移向量、相机光心的世界坐标（Frame::UpdatePoseMatrices）。</p><p><b>第三步：补充创建当前帧近关键点对应的地图点（当前帧和新关键帧都将拥有该地图点）。</b></p><p>首先检查是否需要创建新的地图点（按照关键点<b>深度从小到大</b>）：对于深度大于0的当前帧关键点，如果其没有对应地图点或者地图点没有被观察到（Observation&lt;1），则创建新的地图点（与前面创建方法一样Frame::UnprojectStereo + MapPoint构造函数）并更新信息。</p><p>以上补充创建地图点的过程在满足下述条件时<b>停止</b>：关键点深度大于mThDepth且当前帧有100个以上地图点。</p><p><b>第四步：新关键帧送入局部建图线程。</b></p><p>该步骤通过“mpLocalMapper-&gt;InsertKeyFrame(pKF)”完成。随后局部建图线程继续运行，并将Tracking的mnLastKeyFrameId设置为当前帧ID、mpLastKeyFrame设置为新创建的关键帧。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>11）Tracking::SearchLocalPoints</b></p><div class=\"highlight\"><pre><code class=\"language-cpp\"><span class=\"kt\">void</span> <span class=\"n\">Tracking</span><span class=\"o\">::</span><span class=\"n\">SearchLocalPoints</span><span class=\"p\">()</span>\n</code></pre></div><p>该函数在 Tracking::TrackLocalMap() 函数中被调用，用来完成当前帧关键点与局部地图点之间的匹配。<b>该函数先统计有没有局部地图点存在被匹配的可能，存在这种点的话再调用 SearchByProjection 进行地图点的投影以及匹配。</b></p><p><b>第一步：标记已经被当前帧匹配好的地图点（之后不需要再搜索匹配）。</b></p><p>这一步通过遍历当前帧匹配的地图点mvpMapPoints来标记每个被当前帧匹配好的地图点（地图点的成员 mnLastFrameSeen 改为当前帧ID），这些地图点已经匹配成功之后不需要再匹配。</p><p><b>第二步：统计可能可以进行匹配的局部地图点的数量。</b></p><p>遍历Tracking线程跟踪的局部地图点 mvpLocalMapPoints ，跳过那些坏点（pMP-&gt;isBad()）以及第一步被标记的已经匹配的点，将<b>在当前帧视锥范围内（ mCurrentFrame.isInFrustum(pMP,0.5) ）的地图点</b>统计作为想要匹配的局部地图点。如果这些可以匹配的局部地图点数量大于零则进行后续步骤，否则本函数结束。</p><p><b>第三步：通过投影匹配当前帧关键点与局部地图点</b>。</p><p>首先创建ORBmatcher，然后设置搜索范围的阈值为1（如果距离上一次重定位不到两帧则阈值为5），最后调用ORBmatcher的 <b>SearchByProjection</b> 方法（输入就是当前帧、局部地图点、搜索范围阈值）进行匹配。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>12）Tracking::UpdateLocalPoints()</b></p><div class=\"highlight\"><pre><code class=\"language-cpp\"><span class=\"kt\">void</span> <span class=\"n\">Tracking</span><span class=\"o\">::</span><span class=\"n\">UpdateLocalPoints</span><span class=\"p\">()</span>\n</code></pre></div><p>该函数被 Tracking::UpdateLocalMap() 调用，后者又被 Tracking::TrackLocalMap() 在函数的开头调用。这个函数的<b>本质是通过局部关键帧得到最新的局部地图点</b>。</p><p><b>第一步：清空当前Tracking的局部地图点（mvpLocalMapPoints）。</b></p><p><b>第二步：当前局部关键帧对应的地图点作为局部地图点保存。</b></p><p>遍历当前的局部关键帧 mvpLocalKeyFrames ，对于每一关键帧都要遍历其地图点并将其推入 mvpLocalMapPoints 保存（注意通过标记 mnTrackReferenceForFrame 改为当前帧ID避免重复添加）。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>13）Tracking::UpdateLocalKeyFrames()</b></p><div class=\"highlight\"><pre><code class=\"language-cpp\"><span class=\"kt\">void</span> <span class=\"n\">Tracking</span><span class=\"o\">::</span><span class=\"n\">UpdateLocalKeyFrames</span><span class=\"p\">()</span>\n</code></pre></div><p>该函数同样被 Tracking::UpdateLocalMap() 调用，后者又被 Tracking::TrackLocalMap() 在函数的开头调用。</p><p>本函数流程：<b>先得到最初的局部关键帧（与当前帧共视地图点的关键帧）——加入最初局部帧的连接权重高的相邻关键帧——加入最初局部帧的子女、父母。观察到当前帧的地图点最多的初始的局部关键帧成为：当前帧和Tracking对象的参考关键帧</b>。</p><p>注意函数的关键点在于：<b>局部关键帧一定是和当前帧有关联</b>。</p><p><b>第一步：每个当前帧关键点对应的地图点给观察到他们的关键帧“投票”。</b></p><p>这一步先遍历当前帧关键点对应的地图点完成该投票，这样可以知道哪些关键帧和当前帧有关联。投票信息被保存在 keyframeCounter （map&lt;KeyFrame*,int&gt;）当中，每个元素对应（某一帧，相应票数）。如果该投票计数器是空的，则本函数直接返回。说明一下，<b>观察到某一地图点的关键帧可以通过地图点的 GetObservations() 方法获得</b>。</p><p><b>第二步：保存所有与当前帧看到共同地图点的关键帧，将共视点最多的关键帧的指针同时保存为pKFmax。</b></p><p>这一步根据第一步的投票结果进行，与当前帧有共视地图点的关键帧都被作为局部关键帧保存到 mvpLocalKeyFrames 。保存过的关键帧都把自己的成员 mnTrackReferenceForFrame 改为当前帧ID。另外，与当前帧共视程度最高的关键帧被单另记了下来。</p><p><b>第三步：第二步得到的局部关键帧在系统共视图中的相邻关键帧也作为局部关键帧保存。</b></p><p>这里终于第一次直接碰到了<b>“共视图（ Covisibility Graph ）”</b>这个ORB-SLAM1/2系统中十分重要的概念。简单说明一下，顾名思义，该图的节点是关键帧，该图的边的权重是其连接的两关键帧之间共同观察到的地图点的数量（边连接着有共视地图点的两关键帧）。详细内容可以参考ORB-SLAM（1不是2）的论文。</p><p><b>后续步骤主要在对第二步得到的局部关键帧的大循环中进行</b>，每一次循环前需要<b>限制局部关键帧数量</b>，如果已经够80张的话本函数将直接返回。</p><p>遍历第二步得到的局部关键帧，通过关键帧的 GetBestCovisibilityKeyFrames() 方法获取与正在遍历的关键帧共视点最多的相邻关键帧，这里向该方法输入10指定获取共视程度最高的前10张关键帧（也可能不够10张）。遍历当前正在遍历的局部关键帧的这几张相邻关键帧，一旦碰到好关键帧并且之前没碰到的（其mnTrackReferenceForFrame不是当前帧ID）就把它当做局部关键帧保存并将 mnTrackReferenceForFrame 改为当前帧ID，随后跳出对这些相邻关键帧的遍历（也就是说<b>之前的一张局部关键帧最多只能再带进来一张相邻关键帧</b>）。</p><p><b>第四步：第二步得到的局部关键帧的子女关键帧、父母关键帧加入局部关键帧。</b></p><p>对于所有的关键帧而言，其背后实际上有一棵生成树，也就是说：<b>关键帧有对应的父母关键帧与子女关键帧</b>。这一步还在对最初的局部关键帧的遍历中，首先得到正在遍历的关键帧的子女关键帧（）。</p><div class=\"highlight\"><pre><code class=\"language-cpp\"><span class=\"k\">const</span> <span class=\"n\">set</span><span class=\"o\">&lt;</span><span class=\"n\">KeyFrame</span><span class=\"o\">*&gt;</span> <span class=\"n\">spChilds</span> <span class=\"o\">=</span> <span class=\"n\">pKF</span><span class=\"o\">-&gt;</span><span class=\"n\">GetChilds</span><span class=\"p\">();</span>\n</code></pre></div><p>遍历这个set，如果有子女关键帧是好的并且还没加入到局部关键帧，那么将其加入局部关键帧并且标记（mnTrackReferenceForFrame 改为当前帧ID）然后退出遍历。也就是说，<b>每个初始关键帧同样只能带一个娃（也可能没娃）</b>。同理也将正在遍历的关键帧的父母帧保存下来，这里不再赘述。</p><p><b>最后一步：与当前帧共视地图点最多的初始的局部关键帧（pKFmax指着的）设置为Tracking与当前帧的参考关键帧</b>。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>14）Tracking::Relocalization()</b></p><div class=\"highlight\"><pre><code class=\"language-cpp\"><span class=\"kt\">bool</span> <span class=\"n\">Tracking</span><span class=\"o\">::</span><span class=\"n\">Relocalization</span><span class=\"p\">()</span>\n</code></pre></div><p>本函数用于跟踪丢失时进行重定位。</p><p><b>第一步：获取重定位的候选关键帧</b>。</p><p>之前提到词袋方法的时候说起过，该方法就是被用于闭环检测和重定位时的位置识别方法。因此首先通过当前帧的 ComputeBoW() 方法计算<b>当前帧的词袋向量表示</b>。</p><p>说是位置的识别，具体而言，在基于关键帧优化的SLAM系统中，位置的识别实际是找到对应同一位置的关键帧。基于图像的词袋向量表示，可能对应同一位置的候选关键帧（多个候选，指针存在向量中）将从前文提到过的关键帧数据库中查询得到，代码如下。如果没有候选，则函数直接返回。</p><div class=\"highlight\"><pre><code class=\"language-cpp\"><span class=\"n\">vector</span><span class=\"o\">&lt;</span><span class=\"n\">KeyFrame</span><span class=\"o\">*&gt;</span> <span class=\"n\">vpCandidateKFs</span> <span class=\"o\">=</span> <span class=\"n\">mpKeyFrameDB</span><span class=\"o\">-&gt;</span><span class=\"n\">DetectRelocalizationCandidates</span><span class=\"p\">(</span><span class=\"o\">&amp;</span><span class=\"n\">mCurrentFrame</span><span class=\"p\">);</span>\n</code></pre></div><p><b>第二步：通过ORB匹配确定是否为可靠的候选关键帧，对可靠的候选关键帧建立 PnPsolver 。</b></p><p>这一步我们筛选一下前面得到的可能对应着当前帧位置的候选关键帧，基本思想为：对应同一位置的帧之间应该有很多匹配的点对（毕竟观察着同一位置）。首先创建了一些要用的变量：一个ORBmatcher用来进行匹配；一个向量的向量 vvpMapPointMatches 用来保存每个候选关键帧的与当前帧匹配成功的地图点的指针；一个bool的向量 vbDiscarded 用来保存对应索引的候选关键帧是否被抛弃。</p><p>然后真正开始遍历第一步得到的候选关键帧，对于第 i 张关键帧，使用ORBmatcher的 SearchByBoW 方法进行其与当前帧之间的匹配，并将匹配成功的地图点的指针存入 vvpMapPointMatches[ i ] 。对于匹配点数少于15的候选关键帧，其对应的 vbDiscarded 中元素设为true，其将被抛弃；其他的关键帧即为合格，将为他们建立对应的PnPsolver（输入当前帧与匹配地图点的向量）并将PnPsolver对应指针存入 vpPnPsolvers中。本函数中使用一个变量 nCandidates 记录了合格的候选关键帧的数量。</p><p><b>第三步：通过匹配地图点优化当前帧位姿，确定最终的重定位关键帧。</b></p><p>如果 nCandidates 大于0并且还没有确定最终的重定位关键帧（一个bool变量 bMatch 仍为false），那么将一直进行一个大循环。在这个大循环中遍历之前的候选关键帧，步骤如下。</p><p>（1）跳过被标记为抛弃的关键帧。</p><p>（2）对合格的关键帧利用<b>PnPsolver进行5轮Ransac</b>迭代计算出一个位姿Tcw，该步骤代码为：</p><div class=\"highlight\"><pre><code class=\"language-cpp\"><span class=\"n\">cv</span><span class=\"o\">::</span><span class=\"n\">Mat</span> <span class=\"n\">Tcw</span> <span class=\"o\">=</span> <span class=\"n\">pSolver</span><span class=\"o\">-&gt;</span><span class=\"n\">iterate</span><span class=\"p\">(</span><span class=\"mi\">5</span><span class=\"p\">,</span><span class=\"n\">bNoMore</span><span class=\"p\">,</span><span class=\"n\">vbInliers</span><span class=\"p\">,</span><span class=\"n\">nInliers</span><span class=\"p\">);</span>\n</code></pre></div><p>输入的 bNoMore 表示 Ransac 是否达到最大，如果其为true则该关键帧也将被标记抛弃并且 nCandidates 减1；输入的 vbInliers 标记之前候选关键帧匹配到的地图点是否为内点、nInliers 统计了内点数量。对于是内点的地图点，存入了一个循环中的变量sFound（std::set）。</p><p>（3）<b>如果通过上面这行代码我们真的得到了一个位姿</b>，那么将其设置为<b>当前帧位姿</b>。对于匹配的地图点，将其全部设置为当前帧的地图点，也就是 mvpMapPoints（当然不是内点的地图点指针修改为NULL）。也就是说在<b>重定位的情况下地图点不是像正常情况那样反投影创建并得到地图点的</b>。</p><p>（4）继续得到了位姿的条件下，当前帧已经有了地图点，这样我们进行一次<b>当前帧的位姿优化</b>，代码如下：</p><div class=\"highlight\"><pre><code class=\"language-cpp\"><span class=\"kt\">int</span> <span class=\"n\">nGood</span> <span class=\"o\">=</span> <span class=\"n\">Optimizer</span><span class=\"o\">::</span><span class=\"n\">PoseOptimization</span><span class=\"p\">(</span><span class=\"o\">&amp;</span><span class=\"n\">mCurrentFrame</span><span class=\"p\">);</span>\n</code></pre></div><p>nGood表示优化结束后是内点的当前帧匹配的地图点数量，如果其小于10直接进行下一次循环（检验下一张候选关键帧）。需要注意的是：本次包括之后的位姿优化后有些当前帧的地图点会成为外点，因此优化完需要清除是外点的地图点的指针。</p><p>（5）如果nGood小于50大于10，通过<b>在一个窗口内投影并搜索匹配，再次进行优化</b>。这一步的匹配用一个条件较为宽松些的ORBmathcer进行，使用其 SearchByProjection 方法，代码如下。</p><div class=\"highlight\"><pre><code class=\"language-cpp\"><span class=\"kt\">int</span> <span class=\"n\">nadditional</span> <span class=\"o\">=</span> <span class=\"n\">matcher2</span><span class=\"p\">.</span><span class=\"n\">SearchByProjection</span><span class=\"p\">(</span><span class=\"n\">mCurrentFrame</span><span class=\"p\">,</span><span class=\"n\">vpCandidateKFs</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">],</span><span class=\"n\">sFound</span><span class=\"p\">,</span><span class=\"mi\">10</span><span class=\"p\">,</span><span class=\"mi\">100</span><span class=\"p\">);</span>\n</code></pre></div><p>输入了当前帧、候选关键帧、已经匹配的是内点的地图点。其中 10 是控制搜索范围大小的变量，100是控制描述子匹配阈值的变量。返回的 nadditional 表示这次额外加入的内点地图点的数量，如果其加上nGood还是不够50则再次重复这一步（不过用跟小的阈值3,64），然后重复之前进行的优化步骤并得到新的nGood。</p><p>（6）步骤（4）或（5）中得到nGood一旦够50则认为优化是成功的，bMatch为true（确定了重定位的对应关键帧），跳出循环。</p><p><b>最后检查 bMatch 是否为false</b>，如果是则重定位失败返回false；否则重定位成功，Tracking的 mnLastRelocFrameId 改为当前帧ID，返回true。</p><hr/><p></p>", 
            "topic": [
                {
                    "tag": "同时定位和地图构建（SLAM）", 
                    "tagLink": "https://api.zhihu.com/topics/20033502"
                }, 
                {
                    "tag": "视觉SLAM十四讲（书籍）", 
                    "tagLink": "https://api.zhihu.com/topics/20127173"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/59499965", 
            "userName": "贤鱼卓君", 
            "userLink": "https://www.zhihu.com/people/0be40edd972271a9c9afb446ee1a566d", 
            "upvote": 12, 
            "title": "Ubuntu14.04下编译运行ORB-SLAM2（示例：双目KITTI）", 
            "content": "<p></p><p>代码终于读的还算差不多了，现在要跑通ORB-SLAM2。之前想在Windows下跑通结果失败了...果然像高博说的搞SLAM的话还是老老实实用ubuntu吧，不过在Windows下建立好工程后倒是帮助我用VS2013方便地阅读了代码，也算很有收获。Ubuntu下跑通大概花了我一个晚上，作为阶段性胜利的纪念特记录于此。</p><p>参考了很多大佬们的文章，将主要的博文等链接罗列如下：</p><p><a href=\"https://link.zhihu.com/?target=https%3A//www.cnblogs.com/ilym/p/9080141.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">ORB_SLAM2编译与测试（一） - woft王 - 博客园</a></p><p><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/youngpan1101/article/details/58027049\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">blog.csdn.net/youngpan1</span><span class=\"invisible\">101/article/details/58027049</span><span class=\"ellipsis\"></span></a></p><p>ORB-SLAM2：</p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/raulmur/ORB_SLAM2\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-7ea6511cf59172cacdf8030532749626_ipico.jpg\" data-image-width=\"400\" data-image-height=\"400\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">raulmur/ORB_SLAM2</a><p class=\"ztext-empty-paragraph\"><br/></p><p>可能有些朋友网络不好，除了KITTI这种过大的文件，我会尽量提供百度云的链接。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>KITTI数据集下载链接，用迅雷下载比较好（20个g左右）：</p><p><a href=\"https://link.zhihu.com/?target=https%3A//s3.eu-central-1.amazonaws.com/avg-kitti/data_odometry_gray.zip\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">s3.eu-central-1.amazonaws.com</span><span class=\"invisible\">/avg-kitti/data_odometry_gray.zip</span><span class=\"ellipsis\"></span></a></p><hr/><h2>1.安装各种依赖</h2><ul><li><b>Eigen</b></li></ul><p>我第一次先安装Pangolin结果出错了，后来发现原因是没有在安装Pangolin之前先安装Eigen。保险起见建议先安装Eigen。</p><div class=\"highlight\"><pre><code class=\"language-bash\">sudo apt-get install libeigen3-dev</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><ul><li><b>Pangolin</b></li></ul><p>先安装依赖</p><div class=\"highlight\"><pre><code class=\"language-bash\">sudo apt-get install libglew-dev   <span class=\"c1\">#安装Glew</span>  \nsudo apt-get install cmake         <span class=\"c1\">#安装CMake 3</span> \nsudo apt-get install libboost-dev libboost-thread-dev libboost-filesystem-dev  <span class=\"c1\">#安装Boost</span>  \nsudo apt-get install libpython2.7-dev  <span class=\"c1\">#安装Python2</span></code></pre></div><p>然后下载源码编译安装</p><div class=\"highlight\"><pre><code class=\"language-bash\">git clone https://github.com/stevenlovegrove/Pangolin.git\n<span class=\"nb\">cd</span> Pangolin\nmkdir build\n<span class=\"nb\">cd</span> build\ncmake ..\nmake\nsudo make install</code></pre></div><p>我参考高博《视觉slam十四讲》提供的代码文件，先从github上下载Pangolin代码压缩包，解压后在其文件夹下打开终端，编译安装。（下载地址：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/stevenlovegrove/Pangolin\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">stevenlovegrove/Pangolin</a>）</p><p class=\"ztext-empty-paragraph\"><br/></p><p><i><b>Pangolin下载链接：<a href=\"https://link.zhihu.com/?target=https%3A//pan.baidu.com/s/15kdtdkM4evxteorH54X8vw\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">pan.baidu.com/s/15kdtdk</span><span class=\"invisible\">M4evxteorH54X8vw</span><span class=\"ellipsis\"></span></a></b></i> </p><p><i><b>提取码：0z75 </b></i></p><p class=\"ztext-empty-paragraph\"><br/></p><p>解压代码的压缩包后：</p><div class=\"highlight\"><pre><code class=\"language-bash\"><span class=\"nb\">cd</span> <span class=\"o\">[</span>path-to-pangolin<span class=\"o\">]</span> <span class=\"c1\">#进入Pangolin代码的文件夹</span>\nmkdir build\n<span class=\"nb\">cd</span> build\ncmake ..\nmake \nsudo make install </code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><ul><li><b>OpenCV</b></li></ul><p>接下来安装的是让我没想到的最头疼的OpenCV。因为ORB-SLAM2作者说他们是用OpenCV 2.4.11 和 OpenCV 3.2测试的，然后我比较倾向于选择新的版本，因此我配置了OpenCV3.2版本。据说3.0之后把一些开发中的代码分到了contrib之中，所以要下载两个源码的压缩包：opencv-3.2.0.zip/opencv_contrib-3.2.0.zip。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><i><b>以上两文件下载</b></i></p><p><i><b>链接：<a href=\"https://link.zhihu.com/?target=https%3A//pan.baidu.com/s/1kvokG_FzAxXsITVHCqpZjQ\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">pan.baidu.com/s/1kvokG_</span><span class=\"invisible\">FzAxXsITVHCqpZjQ</span><span class=\"ellipsis\"></span></a></b></i> </p><p><i><b>提取码：llql </b></i></p><p><i><b>链接：<a href=\"https://link.zhihu.com/?target=https%3A//pan.baidu.com/s/1FY92p4RG1f_ElplU7TD8XQ\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">pan.baidu.com/s/1FY92p4</span><span class=\"invisible\">RG1f_ElplU7TD8XQ</span><span class=\"ellipsis\"></span></a></b></i> </p><p><i><b>提取码：gzrs </b></i></p><p class=\"ztext-empty-paragraph\"><br/></p><p>依赖的安装（不放心的话可以多搜一些文章把各种所谓的依赖都安装一下）</p><div class=\"highlight\"><pre><code class=\"language-bash\">sudo apt-get install build-essential \nsudo apt-get install cmake git libgtk2.0-dev pkg-config libavcodec-dev libavformat-dev libswscale-dev \nsudo apt-get install python-dev python-numpy libtbb2 libtbb-dev libjpeg-dev libpng-dev libtiff-dev libjasper-dev libdc1394-22-dev</code></pre></div><p>解压前面说的opencv-3.2.0.zip / opencv_contrib-3.2.0.zip两个文件。我把它们放在了Ubuntu主文件夹之下，即两个路径分别为：~/opencv-3.2.0 和 ~/opencv_contrib-3.2.0。然后进入 ~/opencv-3.2.0：</p><div class=\"highlight\"><pre><code class=\"language-bash\"><span class=\"nb\">cd</span> opencv-3.2.0 \nsudo mkdir build \n<span class=\"nb\">cd</span> build \nsudo cmake -D <span class=\"nv\">CMAKE_BUILD_TYPE</span><span class=\"o\">=</span>RELEASE -D <span class=\"nv\">CMAKE_INSTALL_PREFIX</span><span class=\"o\">=</span>/usr/local -D <span class=\"nv\">OPENCV_EXTRA_MODULES_PATH</span><span class=\"o\">=</span>&lt;path to opencv_contrib-3.2.0/modules/&gt; ..</code></pre></div><p>注意最后cmake的一行，&lt;path to opencv_contrib-3.2.0/modules/&gt; 在我的路径下就是 ~/opencv_contrib-3.2.0/modules</p><p>到这时，稍等一会儿，不出意外的话会在终端中看到它在试图下载东西，过一会看似结束了，仔细一看会发现其实有错误，cmake没有成功。这时需要我们下载好两个文件放到对应文件夹下，若果文件夹中已有我们准备的文件那么就替换它。</p><p>文件1：<b><i>ippicv_linux_20151201.tgz </i></b></p><p>路径1：<i><b>opencv-3.2.0/3rdparty/ippicv/downloads/linux-808b791a6eac9ed78d32a7666804320e/ </b></i></p><p>文件2：<i><b>protobuf-cpp-3.1.0.tar.gz </b></i></p><p>路径2：<b><i>opencv_contrib-3.2.0/modules/dnn/.download/bd5e3eed635a8d32e2b99658633815ef/v3.1.0/ </i></b></p><p class=\"ztext-empty-paragraph\"><br/></p><p><b><i>以上两文件下载（由于百度云的错误和谐导致我不得不压缩起来）</i></b></p><p><i><b>链接: <a href=\"https://link.zhihu.com/?target=https%3A//pan.baidu.com/s/1VorzKC685uwZKsjml3Tu3w\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">pan.baidu.com/s/1VorzKC</span><span class=\"invisible\">685uwZKsjml3Tu3w</span><span class=\"ellipsis\"></span></a></b></i> </p><p><i><b>提取码: 2k1k </b></i></p><p class=\"ztext-empty-paragraph\"><br/></p><p>替换好之后，我们重新执行上面的最后一条指令进行cmake。最后编译安装：</p><div class=\"highlight\"><pre><code class=\"language-bash\"><span class=\"c1\">#同时启用四个processing unit编译，不保险的话可以用 nproc 查看数量，不够四个就直接sudo make</span>\nsudo make -j4 \nsudo make install</code></pre></div><p>编译安装完成后可以查看一下opencv版本：</p><div class=\"highlight\"><pre><code class=\"language-bash\">pkg-config --modversion opencv</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><ul><li><b>DBoW2 和 g2o</b></li></ul><p>这两个库ORB-SLAM2中作者是用的是他们修改过的，已经放在了其源码的3rdparty文件夹下，之后利用 build.sh 编译时会自动安装好，所以不需要我们自己配置。</p><hr/><h2>2.ORB-SLAM2编译运行</h2><p>根据大佬们的提醒，应该下载好源码后进入文件夹，把其中 <i>build.sh</i> 的最后一行 <i>make -j </i>改为 <i>make</i>，否则容易死机。我没有尝试会不会死机，感觉还是保险一点比较好..</p><div class=\"highlight\"><pre><code class=\"language-bash\">git clone https://github.com/raulmur/ORB_SLAM2.git ORB_SLAM2 \n<span class=\"nb\">cd</span> ORB_SLAM2\nchmod +x build.sh\n ./build.sh</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><i><b>ORB-SLAM2代码下载链接：<a href=\"https://link.zhihu.com/?target=https%3A//pan.baidu.com/s/18AyhtdGpXGyqSklVSDWfvw\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">pan.baidu.com/s/18Ayhtd</span><span class=\"invisible\">GpXGyqSklVSDWfvw</span><span class=\"ellipsis\"></span></a></b></i> </p><p><i><b>提取码：zg7y </b></i></p><p class=\"ztext-empty-paragraph\"><br/></p><p>或者可以像我一样在作者的项目主页上直接下载代码压缩包，解压后在终端中进入文件夹，然后：</p><div class=\"highlight\"><pre><code class=\"language-bash\"><span class=\"nb\">cd</span> ORB_SLAM2\nchmod +x build.sh\n ./build.sh</code></pre></div><p>编译成功后生成的可执行程序mono_tum, mono_kitti, rgbd_tum, stereo_kitti, mono_euroc and stereo_euroc，位于Examples目录下。之后的运行大家应该去参考ORB-SLAM2的主页，上面说的很清楚。以下我以自己测试的立体视觉（双目）+KITTI数据集为例说明使用方法。</p><p>通过作者在项目主页上提供的链接可以下载好KITTI数据集并解压，得到一个<i> dataset </i>文件夹。我把它直接放在桌面上，因此路径为：<i>~/Desktop/dataset</i></p><p>首先我们应当在ORB-SLAM2项目的主文件夹下，以我的路径为例：</p><p><i>~/Desktop/ORB_SLAM2-master</i></p><p>然后假如我要跑 KITTI 的 00 文件夹下的数据集，就输入：</p><div class=\"highlight\"><pre><code class=\"language-bash\">./Examples/Stereo/stereo_kitti Vocabulary/ORBvoc.txt Examples/Stereo/KITTI00-02.yaml ~/Desktop/dataset/sequences/00</code></pre></div><p>这条指令第一个参数：./Examples/Stereo/stereo_kitti。表示运行的程序是立体视觉+KITTI数据集的情景。</p><p>第二个参数：Vocabulary/ORBvoc.txt。这是词袋的相关文件所在路径。</p><p>第三个参数：Examples/Stereo/KITTI00-02.yaml。这是配置文件所在路径，KITTI数据集下，00～02对应KITTI00-02.yaml，03对应KITTI03.yaml，04～12对应KITTI04-12.yaml。</p><p>最后一个参数：<i>~/Desktop</i>/dataset/sequences/00。这是我电脑上KITTI的00序列数据集所在路径，注意要和第三个参数相对应。</p><p>经过我的测试，KITTI的00～12一共13个子数据集都可以跑。其他的传感器+数据集大家可以参考作者的项目主页说明进行测试。</p><p>最后放上我的运行截图～</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-89a4fc76eb58ad0b70a0175fcd5c3fcc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1715\" data-rawheight=\"940\" class=\"origin_image zh-lightbox-thumb\" width=\"1715\" data-original=\"https://pic1.zhimg.com/v2-89a4fc76eb58ad0b70a0175fcd5c3fcc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1715&#39; height=&#39;940&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1715\" data-rawheight=\"940\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1715\" data-original=\"https://pic1.zhimg.com/v2-89a4fc76eb58ad0b70a0175fcd5c3fcc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-89a4fc76eb58ad0b70a0175fcd5c3fcc_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "同时定位和地图构建（SLAM）", 
                    "tagLink": "https://api.zhihu.com/topics/20033502"
                }, 
                {
                    "tag": "视觉SLAM十四讲（书籍）", 
                    "tagLink": "https://api.zhihu.com/topics/20127173"
                }, 
                {
                    "tag": "立体视觉", 
                    "tagLink": "https://api.zhihu.com/topics/19902749"
                }
            ], 
            "comments": [
                {
                    "userName": "碣石路人", 
                    "userLink": "https://www.zhihu.com/people/f57f02c2c254f7264fe97689212252e5", 
                    "content": "是不是不支持opencv3.4.2，那本slambook好像默认就给的是这个版本，orb始终报错", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "贤鱼卓君", 
                            "userLink": "https://www.zhihu.com/people/0be40edd972271a9c9afb446ee1a566d", 
                            "content": "我只试过3.4.0，你确定是OpenCV的问题吗", 
                            "likes": 0, 
                            "replyToAuthor": "碣石路人"
                        }
                    ]
                }, 
                {
                    "userName": "哞哞哞哞哞咩", 
                    "userLink": "https://www.zhihu.com/people/2d8add2bef135c673bae71efddd36c87", 
                    "content": "作者男神啊", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "贤鱼卓君", 
                            "userLink": "https://www.zhihu.com/people/0be40edd972271a9c9afb446ee1a566d", 
                            "content": "。。。。", 
                            "likes": 0, 
                            "replyToAuthor": "哞哞哞哞哞咩"
                        }
                    ]
                }, 
                {
                    "userName": "legal high", 
                    "userLink": "https://www.zhihu.com/people/c9cf20e5e66d59ff2ded56fb1b2e9654", 
                    "content": "<p>其实把高博的依赖装完，这些不都有了么</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "头像很好认吗", 
                    "userLink": "https://www.zhihu.com/people/e939865a8c0cd4fe580ccb708f8c8cf9", 
                    "content": "是不是学SLAM的人里一半都是靠高博的十四讲[飙泪笑][飙泪笑]", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "贤鱼卓君", 
                            "userLink": "https://www.zhihu.com/people/0be40edd972271a9c9afb446ee1a566d", 
                            "content": "初学最好看一遍能大体有个概念，基础也能学个差不多", 
                            "likes": 0, 
                            "replyToAuthor": "头像很好认吗"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/53067853", 
            "userName": "贤鱼卓君", 
            "userLink": "https://www.zhihu.com/people/0be40edd972271a9c9afb446ee1a566d", 
            "upvote": 11, 
            "title": "DL+SLAM论文阅读：《Efficient DL for Stereo Matching》", 
            "content": "<p>还是像翻译一样记录一下，引用什么的不再标注。注意是<b>2016年发表</b>的文章。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>题目：Efﬁcient Deep Learning for Stereo Matching</h2><p><b>作者</b>：Wenjie Luo Alexander G. Schwing Raquel Urtasun </p><p>作者单位：Department of Computer Science, University of Toronto </p><p>作者联系方式：{wenjie, aschwing, urtasun}@<a href=\"https://link.zhihu.com/?target=http%3A//cs.toronto.edu\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">cs.toronto.edu</span><span class=\"invisible\"></span></a></p><p class=\"ztext-empty-paragraph\"><br/></p><h2>摘要：</h2><p>在过去的一年里，卷积神经网络被证明在立体估计方面表现得非常好。然而，目前的架构依赖于siamese网络，这种网络利用连接，然后是进一步的处理层，每对图像需要一分钟的GPU计算。与此相反，本文提出的匹配网络能够在不到一秒钟的GPU计算中产生非常准确的结果。为了实现这一目标，作者开发了一个<b>product layer</b>，它简单地计算siamese体系结构的两个表示之间的<b>内积</b>。作者通过将问题作为<b>多分类问题</b>来训练网络，在多类分类中，类别是所有可能的<b>视差（disparity）</b>。这使能够得到校准的分数，与现有的方法相比，可以得到更好的匹配性能。</p><hr/><h2>1.引入</h2><p>三维重建场景是机器人和自动驾驶汽车等许多应用的关键（我的毕设啊啊）。为了简化这一过程，通常使用激光雷达等三维传感器。利用相机是一个有吸引力的选择，因为它通常是一个更经济的解决方案。然而，尽管经过了几十年的研究，从立体声对中估计深度仍然是一个开放的问题。处理闭塞( cclusion )，大的饱和区域( large saturated areas )和重复模式( repetitive patterns )是仍然存在的挑战。</p><p>许多方法都试图从<b>局部匹配</b>中聚合信息。例如，cost aggregation 是对局部近邻的平均差异估计。类似地，半全局块匹配和基于马尔可夫随机场的方法将像素预测和局部平滑结合成一个能量函数。然而，所有这些方法都使用手工制作的成本函数，或者只从数据中学习线性组合的特征。</p><p>在过去的几年里，我们目睹了一场<b>高层视觉</b>的革命，即<b>从像素直接学习深层表现</b>，以前所未有的性能解决许多场景理解任务。这些方法在一些任务，如检测，分割和分类中是最先进的。</p><p>最近，卷积网络也被用来学习如何匹配立体估计的任务。目前的方法是通过将问题视作<b>二分类</b>来学习匹配网络的参数：给定左边图像中的一个patch，任务是预测右边图像中的一个patch是否正确匹配。虽然有文章表示在<b> KITTI </b>等具有挑战性的 benchmark 中表现出了出色的性能，但它在计算上非常昂贵，需要在GPU中进行一分钟的计算。这是因为他们利用了siamese体系结构，然后通过更多的层进行连接和进一步处理来计算最终的分数。</p><p>与此相反，本文提出的匹配网络能够在不到一秒钟的GPU计算中产生非常准确的结果。为了实现这一目标，作者开发了一个<b>product layer</b>，它简单地计算siamese体系结构的两个表示之间的<b>内积</b>。作者通过将问题作为<b>多分类问题</b>来训练网络，在多类分类中，类别是所有可能的<b>视差</b>。这使能够得到校准的分数，与现有的方法相比【29】，可以得到更好的匹配性能。</p><p>【29】<i> J. Zbontar and Y. LeCun. Computing the stereo matching cost with a convolutional neural network. In CVPR, 2015. 1, 2, 4, 6 </i></p><p>作者建议读者参考图1来了解他们的方法。作者他们演示了其方法在具有挑战性的 KITTI 基准测试上的有效性，并展示了使用平滑技术时的富有竞争性的结果。他们的代码和数据可以在网上找到:</p><a href=\"https://link.zhihu.com/?target=http%3A//www.cs.toronto.edu/deeplowlevelvision%25E3%2580%2582\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">http://www.cs.toronto.edu/deeplowlevelvision。</a><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-e32a0395eb01bce3c3ec2cc812ad6036_b.jpg\" data-size=\"normal\" data-rawwidth=\"789\" data-rawheight=\"554\" class=\"origin_image zh-lightbox-thumb\" width=\"789\" data-original=\"https://pic3.zhimg.com/v2-e32a0395eb01bce3c3ec2cc812ad6036_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;789&#39; height=&#39;554&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"789\" data-rawheight=\"554\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"789\" data-original=\"https://pic3.zhimg.com/v2-e32a0395eb01bce3c3ec2cc812ad6036_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-e32a0395eb01bce3c3ec2cc812ad6036_b.jpg\"/><figcaption>图1 为了学习信息丰富的图像patch表示，使用siamese网络，提取每个像素的所有可能差异的边缘分布。</figcaption></figure><hr/><h2>2. 相关工作</h2><p>在过去的几十年里，许多立体算法被开发出来。由于对所有现有方法的讨论都超出了本文的范围，因此作者主要将工作限制在利用学习的近期方法的一个子集中，这些方法大多可以表述为<b>能量最小化</b>。</p><p>早期学习方法侧重基于<b>修正</b>初始计算的<b>匹配成本</b>[16,17]。学习方法也被用来<b>调整能量最小化任务的超参数</b>。最早训练这些超参数的是[31,21,19]，它们研究了不同形式的概率图形模型。</p><p><b>倾斜平面模型</b>用倾斜的3D平面建模像素组。它们在自动驾驶场景中具有很强的竞争力，在这些场景中，鲁棒性是关键。他们有着悠久的历史，可以追溯到[2]，并且在<b>Middleburry</b>基准[22、15、3、24]和<b>KITTI</b>[25、26、27]上都非常成功。</p><p>研究了<b>多任务联合求解的整体模型</b>。这样做的好处是，许多低层次和高层次的任务是相互关联的，因此可以从联合求解中获益。例如[5,6,4,18,13]联合求解了立体分割和语义分割。Guney和Geiger[12]研究了诸如目标识别和语义分割等高级视觉任务对于立体匹配的实用性。</p><p>当使用立体估计作为流水线的一部分时，评估每个<b>匹配的信心</b>是关键。学习方法成功地应用于这一任务中，例如，通过一个随机森林分类器[14]结合几个置信测度，或者将随机森林预测合并到一个马尔科夫随机场[23]中。</p><p><b>卷积神经网络</b>在图像分类、目标检测和语义分割等高级视觉任务中表现良好。近年来，神经网络已被应用于低层次的视觉任务，如光流预测[10]。在立体估计的背景下，【28】利用CNN计算两个图像块之间的匹配成本。他们特别使用了siamese网络，该网络采用大小相同的左右图像补丁，顶部有几个完全连接的图层，以预测匹配成本。他们对模型进行了训练，使<b>二元交叉熵损失最小化</b>。【28】本着相似的想法，研究了不同的基于CNN的架构来比较图像patch的方法。他们发现，将左右图像作为不同的通道拼接在一起效果最好，但代价是速度非常慢。</p><p>【28】<i>S. Zagoruyko and N. Komodakis. Learning to compare image patches via convolutional neural networks. In CVPR, 2015. 1, 2</i></p><p>我们的工作与[29,28]最为相似，但有<b>两个主要区别</b>。首先，我们建议<b>①使用平滑的目标分布来学习所有视差值的概率分布</b>。因此，我们能够隐式地捕捉不同视差之间的相关性。这与对图像patch执行独立二元预测的[29]形成对比。其次，在卷积层顶部，我们<b>②使用一个简单的点积层来连接网络的两个分支</b>。这使我们能够更快地进行数量级的计算。我们注意到，在提交论文时未发表的并行工作中[30,7]也引入了一个点积层。</p><hr/><h2>3.Deep Learning for Stereo Matching</h2><p>我们对计算给定立体对的<b>视差图像</b>感兴趣。在本文中，作者<b>假设图像对是经过校正的，因此极线与水平图像轴对齐</b>。(注意这里我不会打出原文的很花哨的小写y，只能用大写y代替) 让 <img src=\"https://www.zhihu.com/equation?tex=y_i%5Cin+Y_i\" alt=\"y_i\\in Y_i\" eeimg=\"1\"/> 代表与第i个像素相关的视差，并让 <img src=\"https://www.zhihu.com/equation?tex=%5Cleft%7C+Y_i+%5Cright%7C\" alt=\"\\left| Y_i \\right|\" eeimg=\"1\"/> 是集合的基数( 通常是128或256 )。<b>立体算法通过计算左边图像中的每个像素的任何可能的视差值打分来估计一个三维成本量</b>。这通常是通过利用给定像素周围的一个小patch和每个patch的简单手工表示来实现的。与此相反，本文利用卷积神经网络来学习如何匹配。</p><p>为了实现这个目标，作者<b>使用了siamese体系结构，其中每个分支分别处理左边或右边的图像</b>。特别是,每个分支需要：</p><ul><li><b>一个图像作为输入</b>，</li><li>将前者传递通过<b>一组层</b>，每个层都由<b>小滤波器尺寸(例如,5×5或3×3)的空间卷积</b>组成</li><li>紧随其后的<b>一个空间批量（batch）标准化</b></li><li>最后<b>一个修正的线性单元(ReLU)</b>。</li></ul><p>注意，作者将ReLU从最后一层删除，以便不丢失在负值中编码的信息。在实验中，我们利用<b>每层不同数量的滤波器</b>，32或64，并在两个分支之间共享参数。</p><p>与现有的利用连接然后进行进一步处理的方法不同，作者使用一个product层，它简单地计算<b>两分支最终表示之间的内积来计算匹配得分</b>。这个简单的操作大大加快了计算速度。作者建议读者参考图2描述的一个4层网络的例子：滤波器尺寸3×3层，可得到大小9×9的感受域（receptive field）。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>训练：</b></p><ul><li>左侧分支</li></ul><p>作者使用从<b>一组像素中随机抽取的小的左侧图像块</b>，这些像素点的ground truth可用来训练网络。这种策略为我们提供了一系列不同的example，并且具有存储效率。特别是，每个<b>左图的patch的大小相当于网络的感受域的大小</b>。设 <img src=\"https://www.zhihu.com/equation?tex=%28x_i%2Cy_i%29\" alt=\"(x_i,y_i)\" eeimg=\"1\"/> 为从左侧图像中随机抽取的patch中心的图像坐标，设 <img src=\"https://www.zhihu.com/equation?tex=d_%7Bx_i%2Cy_i%7D\" alt=\"d_{x_i,y_i}\" eeimg=\"1\"/> 为对应的ground truth视差（ground truth就是正确的标签信息）。</p><ul><li>右侧分支</li></ul><p>对于右边分支的图像，使用一个<b>更大的patch</b>，这样就扩大了感受域的大小以及所有可能的视差（例如位移）。</p><ul><li>输出</li></ul><p>因此 siamese 网络的两个分支的输出为：一个64维的左分支表示，和一个<img src=\"https://www.zhihu.com/equation?tex=%5Cleft%7C+Y_i+%5Cright%7C\" alt=\"\\left| Y_i \\right|\" eeimg=\"1\"/>×64的右分支表示。然后将这两个向量作为输入传递到一个内积层，该层计算<img src=\"https://www.zhihu.com/equation?tex=%5Cleft%7C+Y_i+%5Cright%7C\" alt=\"\\left| Y_i \\right|\" eeimg=\"1\"/>视差的得分。这允许我们<b>计算所有可能视差上每个像素的 softmax</b>。</p><p>在训练过程中，作者通过网络参数权值 <b>w </b>最小化交叉熵损失：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-2704cc0786cc2c494bf9e061e63b958c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"409\" data-rawheight=\"93\" class=\"content_image\" width=\"409\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;409&#39; height=&#39;93&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"409\" data-rawheight=\"93\" class=\"content_image lazy\" width=\"409\" data-actualsrc=\"https://pic1.zhimg.com/v2-2704cc0786cc2c494bf9e061e63b958c_b.jpg\"/></figure><p>由于对<b>3像素的误差度量</b>感兴趣，作者使用平滑的目标分布 <img src=\"https://www.zhihu.com/equation?tex=p_%7Bgt%7D%28y_i%29\" alt=\"p_{gt}(y_i)\" eeimg=\"1\"/> ，以ground-truth <img src=\"https://www.zhihu.com/equation?tex=y_i%5E%7BGT%7D\" alt=\"y_i^{GT}\" eeimg=\"1\"/> 为中心，即：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-ffc61c1b47c83613b5381c1ce6fc00e2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"508\" data-rawheight=\"180\" class=\"origin_image zh-lightbox-thumb\" width=\"508\" data-original=\"https://pic3.zhimg.com/v2-ffc61c1b47c83613b5381c1ce6fc00e2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;508&#39; height=&#39;180&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"508\" data-rawheight=\"180\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"508\" data-original=\"https://pic3.zhimg.com/v2-ffc61c1b47c83613b5381c1ce6fc00e2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-ffc61c1b47c83613b5381c1ce6fc00e2_b.jpg\"/></figure><p>本论文中，作者设置 <img src=\"https://www.zhihu.com/equation?tex=%5Clambda_1%3D0.5%2C%5C+%5Clambda_2%3D0.2%2C%5C+%5Clambda_3%3D0.05\" alt=\"\\lambda_1=0.5,\\ \\lambda_2=0.2,\\ \\lambda_3=0.05\" eeimg=\"1\"/>。注意，这与用于分类的交叉熵形成了对比，其中 <img src=\"https://www.zhihu.com/equation?tex=p_%7Bgt%7D%28y_i%29\" alt=\"p_{gt}(y_i)\" eeimg=\"1\"/> 是一个delta函数，它将所有的质量都放在带注释的 groundtruth 配置上。</p><p>作者使用AdaGrad【8】的<b>随机梯度下降反向传播</b>来训练网络。与 moment-based 的随机梯度下降类似，AdaGrad根据历史信息调整梯度。相较于moment-based方法，该方法强调了罕见但信息丰富的特征。我们每隔几千次迭代就会调整学习速率，详见实验部分。</p><p>【8】<i> J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient methods for online learning and stochastic optimization. JMLR, 12:2121–2159, 2011. 3, 4 </i></p><p><b>测试：</b></p><p>通过从不同的训练图像中随机抽取位置组成一个 mini-batch 的训练过程进行对比，可以提高测试时的速度性能。作者的 siamese 网络为每个像素 i 计算64维的特征表示。为了有效地获得成本量（cost volume），我们对每个像素 i 只计算一次64维的表示，在计算成本量时，我们对涉及该位置的所有视差重复使用该值。</p><hr/><h2>4.平滑深层网络的输出（Smoothing Deep Net Outputs）</h2><p>给定CNN得到的一元数（unaries），作者计算出每个图像位置的所有视差的预测。请注意，简单地为每个像素输出最可能的配置并不能与现代立体算法竞争，后者利用不同形式的成本聚合、后处理和平滑。这对于处理具有遮挡、饱和或重复模式的复杂区域特别重要。</p><p>在过去的十年中，人们提出了许多不同的 MRF 来解决立体估计问题。大多数方法将每个随机变量定义为像素的视差，并对连续或邻近像素之间的平滑进行编码。另一种方法是将图像分割成区域，并为每个区域估计一个倾斜的三维平面。本文研究了不同平滑技术的效果。为了实现这一目标，作者<b>将立体匹配作为几种不同的马尔可夫随机场(MRFs)的推论，目的是平滑卷积神经网络产生的匹配结果</b>。特别地，我们研究了一些平滑方法：成本聚合、半全局块匹配以及【27】的倾斜平面方法。</p><p>【27】 <i>K. Yamaguchi, D. McAllester, and R. Urtasun. Efﬁcient joint segmentation, occlusion labeling, stereo and ﬂow estimation. In ECCV. 2014. 2, 3, 4, 5, 6, 7</i></p><p>下面作者将简要回顾这些技术。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>成本聚合（cost aggregation）</b>：作者用了非常简单的成本聚合方法，即通过5*5的窗口进行简单的平均池化。</p><p><b>半全局块匹配（Semi global block matching）</b>：通过引入额外的两两之间的势（pairwise potentials）来增加卷积神经网络获得的一元能量项，从而促进平滑的差异。特别的：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-1099920b621287622be1c04484185b0a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"507\" data-rawheight=\"126\" class=\"origin_image zh-lightbox-thumb\" width=\"507\" data-original=\"https://pic3.zhimg.com/v2-1099920b621287622be1c04484185b0a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;507&#39; height=&#39;126&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"507\" data-rawheight=\"126\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"507\" data-original=\"https://pic3.zhimg.com/v2-1099920b621287622be1c04484185b0a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-1099920b621287622be1c04484185b0a_b.jpg\"/></figure><p>这里 <img src=\"https://www.zhihu.com/equation?tex=%5Cvarepsilon\" alt=\"\\varepsilon\" eeimg=\"1\"/> 指的是4连接的网格。一元能量 <img src=\"https://www.zhihu.com/equation?tex=E_i%28y_i%29\" alt=\"E_i(y_i)\" eeimg=\"1\"/> 是神经网络的输出。作者定义两两之间的势如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-d2c6e2f5b61f8cf3a10172ba906b57e9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"632\" data-rawheight=\"158\" class=\"origin_image zh-lightbox-thumb\" width=\"632\" data-original=\"https://pic2.zhimg.com/v2-d2c6e2f5b61f8cf3a10172ba906b57e9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;632&#39; height=&#39;158&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"632\" data-rawheight=\"158\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"632\" data-original=\"https://pic2.zhimg.com/v2-d2c6e2f5b61f8cf3a10172ba906b57e9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-d2c6e2f5b61f8cf3a10172ba906b57e9_b.jpg\"/></figure><p>其中常数满足 <img src=\"https://www.zhihu.com/equation?tex=c_1%3Cc_2\" alt=\"c_1&lt;c_2\" eeimg=\"1\"/> 。作者遵循【29】的方法，如果在左边图像或右边图像的相应位置有强有力的证据表明存在边缘，则c1和c2会减小。请读者参阅他们的论文以了解更多详情。</p><p><b>倾斜平面（Slanted plane）</b>：为了构造深度图，这种方法对包含外观、位置、视差、平滑度和边界能量的能量进行块坐标下降。更具体地说，我们首先使用SLIC能量[1]的扩展对图像进行过分割。然后，对于每个超像素，我们计算倾斜平面估计[27]，它应该遵循从卷积神经网络获得的深度图证据。然后我们迭代这两个步骤，以最小化一个考虑外观、位置、视差、平滑度和边界能量的能量函数。建议感兴趣的读者参阅[27]了解详细信息。</p><p><b>复杂的后处理（Sophisticated post-processing）</b>：在[30]中，设计了一个三步后处理来执行插值、亚像素增强和细化。插值步骤通过对左右图像进行一致性检查，解决了左右图像视差图计算的冲突。亚像素增强将二次函数拟合到相邻点，得到增强深度图。为了在不模糊边缘的情况下平滑视差图，最后的细化步骤使用中值滤波器和双边滤波器。作者只使用插值步骤，因为发现在他们的情况下，其他两个并不总是能进一步提高性能。</p><hr/><h2>5.实验评估</h2><p>作者评估了<b>不同卷积神经网络结构</b>和<b>不同平滑技术</b>在KITTI 2012[11]和2015[20]数据集上的性能。在训练之前，将每个图像归一化使其均值为零、标准差为1。作者使用<b>均匀分布初始化网络参数</b>。我们利用AdaGrad算法[8]且设置 <img src=\"https://www.zhihu.com/equation?tex=1e%5E%7B-2%7D\" alt=\"1e^{-2}\" eeimg=\"1\"/> 的学习速率。经过24000次迭代，学习率下降了5倍，然后每8000次迭代学习率又进一步下降了5倍。作者使用的批处理大小是128。作者对网络进行了40000轮迭代的训练，在NVIDIA Titan-X 上大约需要6.5小时。</p><p>【11】<i>A. Geiger, P. Lenz, and R. Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In CVPR, 2012. 1, 4 </i></p><p>【20】 <i>M. Menze and A. Geiger. Object Scene Flow for Autonomous Vehicles. In CVPR, 2015. 4 </i></p><p class=\"ztext-empty-paragraph\"><br/></p><p>5.1 KITTI 2012 结果</p><p>KITTI 2012数据集包含194个训练图像和195个测试图像。为了比较下面描述的不同网络架构，作者使用随机选择的160对图像作为训练集，其余的34对图像作为验证集。</p><ul><li><b>匹配网络的比较：</b></li></ul><p>首先展示作者的网络的匹配能力，并与现有的匹配网络进行比较[29,30]。在这个实验中，作者不使用平滑或后处理，只利用网络的原始输出。根据KITTI，我们使用<b>视差误差大于固定阈值的像素的百分比</b>以及<b>端点误差</b>作为度量。作者把他们的架构称为“Ours(19)”，它由9层的3×3的卷积构成（导致接受域19×19像素的大小）。如表1所示，经过仅0.14秒的计算，作者的9层网络实现了3像素的非遮挡立体视觉误差8.61%。相比之下，[29]在较长时间20.13秒后获得12.99%。[30]更快的版本需要0.20秒，这导致性能下降到15.53%。如表1所示，我们的网络在各项指标上都大大优于之前设计的卷积神经网络。</p><ul><li><b>平滑的比较：</b></li></ul><p>接下来，作者评估了在使用不同网络大小时用于平滑和后处理的不同算法。特别地，作者评估了成本聚合、半全局块匹配和倾斜平面平滑，这些在前一节中已经描述。我们还为我们的网络试验了不同的接收场大小，这对应于改变我们架构的深度。和之前一样使用Ours(n)表示作者他们的接受域大小为n×n像素的网络架构。作者研究了n = 9，19，29，37的情况。作者使用内核的大小3×3对应n = 9, n = 19；内核的大小5×5对应n = 39。为实现接受域大小为29作者使用5层的5×5和4层的3×3核，这使得层的数量限制在9。</p><p>如表2所示，接受域大小不同的网络误差范围为6.61% (n = 37) ~ 16.69% (n = 9)。【30】的慢模型误差为12.99%，快模型误差为15.53%。<b>平滑后，网络的立体误差差异不再显著。</b>所有的错误都稍小于4%。由于<b>深度图往往是非常平滑的</b>，作者认为一个积极的平滑有助于平复噪声一元势（unary potential）。此外，作者还观察到，利用简单的成本聚合来进一步促进局部平滑有助于略微改善结果。这是由于这种技术消除了小的孤立的噪声区域。[30]中提出的后处理主要关注遮挡和亚像素增强方面，而[27]通过对倾斜平面的拟合对非纹理区域增加了额外的鲁棒性。这两种方法都略微改进了半全局块匹配输出。作者最好的模型组合实现了3像素的立体误差3.64%。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-668c52c6bf655a2ee3af395136762d1b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1313\" data-rawheight=\"584\" class=\"origin_image zh-lightbox-thumb\" width=\"1313\" data-original=\"https://pic4.zhimg.com/v2-668c52c6bf655a2ee3af395136762d1b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1313&#39; height=&#39;584&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1313\" data-rawheight=\"584\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1313\" data-original=\"https://pic4.zhimg.com/v2-668c52c6bf655a2ee3af395136762d1b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-668c52c6bf655a2ee3af395136762d1b_b.jpg\"/></figure><p><b>与最先进技术的比较：</b>为了评估测试集的性能，作者在整个训练集上训练接受域大小为19像素的模型，即“ours(19)”，得到的测试集性能如表3所示。由于他们并没有特别注重于寻找平滑和unaries的良好组合，所以性能略低于当前的最先进水平。（但是明显作者他们的方法速度极快）</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-d53ba4a8ed7b555d0db2d25bde106c64_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1211\" data-rawheight=\"296\" class=\"origin_image zh-lightbox-thumb\" width=\"1211\" data-original=\"https://pic1.zhimg.com/v2-d53ba4a8ed7b555d0db2d25bde106c64_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1211&#39; height=&#39;296&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1211\" data-rawheight=\"296\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1211\" data-original=\"https://pic1.zhimg.com/v2-d53ba4a8ed7b555d0db2d25bde106c64_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-d53ba4a8ed7b555d0db2d25bde106c64_b.jpg\"/></figure><p><b>定性分析：</b>用作者方法进行立体估计的例子如图3所示。我们观察到其方法受到无纹理区域以及具有重复模式的区域(如栅栏)的影响。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-74d3f4006bec0dd938b2e91bcfeff563_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1231\" data-rawheight=\"802\" class=\"origin_image zh-lightbox-thumb\" width=\"1231\" data-original=\"https://pic4.zhimg.com/v2-74d3f4006bec0dd938b2e91bcfeff563_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1231&#39; height=&#39;802&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1231\" data-rawheight=\"802\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1231\" data-original=\"https://pic4.zhimg.com/v2-74d3f4006bec0dd938b2e91bcfeff563_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-74d3f4006bec0dd938b2e91bcfeff563_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>5.2 KITTI 2015 结果</p><p>KITTI 2015数据集由200张训练图像和200张测试图像组成。我们直接处理RGB数据，而不是像KITTI 2012数据集那样的灰度图像。为了比较不同的网络架构，随机选择了160对图像对作为训练集，并使用剩下的40对图像对进行验证。</p><p><b>匹配网络的比较：</b>首先展示网络的匹配能力，并与现有的匹配网络进行比较[29,30]。在这个实验中，不使用平滑或后处理，而是使用网络的原始输出。作者通过“ours(37)”来来使用架构。它由9层的5×5卷积导致接受域的大小为37×37像素。如表4所示，9层网络在仅0.34秒的处理时间后，实现了3像素的立体误差7.23%，而[29]在明显更长的22.76秒的处理时间后，获得了12.45%。[29]更快的版本[30]需要0.21秒，但是与我们的方法相比，性能要低得多，只有14.96%。同样，作者的网络在所有标准上都比以前设计的卷积神经网络有很大的优势。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ffca1a2882ea947d1426d8a6a56bf764_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1145\" data-rawheight=\"195\" class=\"origin_image zh-lightbox-thumb\" width=\"1145\" data-original=\"https://pic1.zhimg.com/v2-ffca1a2882ea947d1426d8a6a56bf764_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1145&#39; height=&#39;195&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1145\" data-rawheight=\"195\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1145\" data-original=\"https://pic1.zhimg.com/v2-ffca1a2882ea947d1426d8a6a56bf764_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-ffca1a2882ea947d1426d8a6a56bf764_b.jpg\"/></figure><p><b>平滑比较：</b>表5显示了在不同的网络架构中应用不同的后处理技术的结果。正如在处理KITTI 2012图像时作者观察到的：使用平滑技术后，网络性能的差异消失。作者的最佳组合在验证集上实现了4.14%的3像素错误。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ca8c6a4381bcac05b72a48d327a21424_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1187\" data-rawheight=\"333\" class=\"origin_image zh-lightbox-thumb\" width=\"1187\" data-original=\"https://pic1.zhimg.com/v2-ca8c6a4381bcac05b72a48d327a21424_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1187&#39; height=&#39;333&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1187\" data-rawheight=\"333\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1187\" data-original=\"https://pic1.zhimg.com/v2-ca8c6a4381bcac05b72a48d327a21424_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-ca8c6a4381bcac05b72a48d327a21424_b.jpg\"/></figure><p><b>深度和滤波器尺寸的影响</b></p><p>接下来，作者从<b>匹配性能</b>和<b>运行时间</b>两方面评估CNN的<b>深度</b>和<b>接受域大小</b>的影响。图4a为匹配性能关于网络接受域大小的函数。我们观察到，不断增大的接受域大小可以获得更好的性能。然而，当接受域非常大时，改善是微妙的，因为网络开始忽略小对象的细节和深度不连续性。我们的发现对于非遮挡情况和所有像素都是一致的。如图4b所示，运行时间与参数个数高度相关。注意，接受域较大的模型不一定有更多的参数，因为可训练权值的多少也取决于滤波器的数量和每个卷积层的通道数量。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-25b6ed535998a6300207dfa70f1ee3a8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1058\" data-rawheight=\"465\" class=\"origin_image zh-lightbox-thumb\" width=\"1058\" data-original=\"https://pic1.zhimg.com/v2-25b6ed535998a6300207dfa70f1ee3a8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1058&#39; height=&#39;465&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1058\" data-rawheight=\"465\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1058\" data-original=\"https://pic1.zhimg.com/v2-25b6ed535998a6300207dfa70f1ee3a8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-25b6ed535998a6300207dfa70f1ee3a8_b.jpg\"/></figure><p><b>与最先进的对比：</b>为了评估测试集的性能，我们选择了拥有目前平滑技术的最佳模型，接受域为37像素，即“ours(37)。得到的测试集性能如表6所示。作者在更短的时间内达到了与其他模型相当的效果。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-c145a7c58bd4b1b76dd4b252d5ff1376_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1138\" data-rawheight=\"256\" class=\"origin_image zh-lightbox-thumb\" width=\"1138\" data-original=\"https://pic3.zhimg.com/v2-c145a7c58bd4b1b76dd4b252d5ff1376_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1138&#39; height=&#39;256&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1138\" data-rawheight=\"256\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1138\" data-original=\"https://pic3.zhimg.com/v2-c145a7c58bd4b1b76dd4b252d5ff1376_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-c145a7c58bd4b1b76dd4b252d5ff1376_b.jpg\"/></figure><p><b>定性结果：</b>我们提供了图5中测试集的结果。同样，注意到作者的算法受到无纹理区域和重复模式区域的影响。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-90e5f974245e1667d32546a51f44b6f9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1165\" data-rawheight=\"769\" class=\"origin_image zh-lightbox-thumb\" width=\"1165\" data-original=\"https://pic2.zhimg.com/v2-90e5f974245e1667d32546a51f44b6f9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1165&#39; height=&#39;769&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1165\" data-rawheight=\"769\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1165\" data-original=\"https://pic2.zhimg.com/v2-90e5f974245e1667d32546a51f44b6f9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-90e5f974245e1667d32546a51f44b6f9_b.jpg\"/></figure><hr/><h2>6. 总结</h2><p>卷积神经网络最近被证明在立体估计方面表现得非常好。目前的体系结构依赖于siamese网络，这种网络利用连接，然后是进一步的处理层，需要在GPU上花数分钟来处理立体对。与此相反，本文提出的匹配网络能够在不到一秒的GPU计算时间内产生非常准确的结果。我们的主要贡献是将连接层和后续处理层替换为<b>单个内积层</b>，该层计算匹配分数。作者使用交叉熵对所有可能的视差进行训练，这使能够得到校准的分数，与现有方法相比匹配性能更好。作者还研究了不同平滑技术对进一步提高性能的影响。在未来，作者计划将该方法用于其他低层次的视觉任务，如光流。他们还计划构建适合他们方法的平滑技术。</p><p></p>", 
            "topic": [
                {
                    "tag": "计算机视觉", 
                    "tagLink": "https://api.zhihu.com/topics/19590195"
                }, 
                {
                    "tag": "同时定位和地图构建（SLAM）", 
                    "tagLink": "https://api.zhihu.com/topics/20033502"
                }, 
                {
                    "tag": "立体匹配", 
                    "tagLink": "https://api.zhihu.com/topics/20083757"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/51968762", 
            "userName": "贤鱼卓君", 
            "userLink": "https://www.zhihu.com/people/0be40edd972271a9c9afb446ee1a566d", 
            "upvote": 29, 
            "title": "SLAM论文阅读：MonoSLAM", 
            "content": "<p> 要开始阅读SLAM文献了，感觉纯基础学习还是空荡荡的，赶紧单刀直入学学前辈们的V-SLAM成果。就当我是在大致翻译论文吧，基本都是我个人通过机器或者自己脑子进行的，引用什么的就不再啰嗦了，内容仅供学习参考。</p><p><b>注意：</b>由于论文来自不同年份，我直接翻译的地位评价之类的都局限于当时的发展情况，不一定适用于现在。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>由于我自己也是小白，所以一般不会概括省略很多东西，所以——长篇幅警告！</p><hr/><h2>MonoSLAM: Real-Time Single Camera SLAM（2007年）</h2><p>Andrew J. Davison, Ian D. Reid, Member, IEEE, Nicholas D. Molton, and Olivier Stasse, Member, IEEE</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2>摘要：</h2><p>作者们实现了能够实时恢复在未知场景中快速运动单目相机的3D轨迹的算法，给这个相应系统命名为“MonoSLAM”（SLAM方法在移动单目相机条件下第一次成功应用，实时且无漂移现象）。 </p><p><b>方法核心：</b>概率框架内在线建立自然地标的稀疏且持续的地图。</p><p><b>关键创新贡献：</b></p><ul><li>地图构建和测量的动态方法</li><li>使用了平滑相机运动的一般运动模型</li><li>单目特征初始化、特征方向估计的解决方法</li></ul><p><b>结果：</b></p><p>高效鲁棒的算法，可以以30Hz运行在标准PC、相机硬件上。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>1 介绍</h2><p>相机体积小、精度高、无侵入性强、易于理解、便宜，但2007年之前SLAM使用的传感器主要是激光雷达等等。原因是相机捕捉的是间接的光度影响，图像稀疏的特征集转到实时地图上比较困难，并且相机传送数据的速率远大于其他传感器。</p><p>因此之前的研究集中在从小图集恢复（SFM领域）。<b>SFM算法</b>工作于较长的图像序列并且是离线的重建运动轨迹和场景结构。为了得到全局一致的序列估计，通过逐帧特征匹配的局部运动估计会在整个序列上前向后向进行全局优化（<b>集束优化 bundle adjustment</b>）。这些方法适用于短的图像序列，但是无法应用于实时任意长序列的一致性定位。</p><p>作者指出在很多闭环系统中<b>实时性</b>是必须的（因为需要及时反馈），不过最关键的需求是“知道自己位置”而不是“构建出详细的场景最终地图”。尽管这两个问题是一对的，但是作者关注的是<b>以定位结果作为输出</b>。当然地图也会构建，不过它是一个为了定位而优化的地标的稀疏地图。</p><p>作者进一步指出：实时相机跟踪时常涉及在<b>受限环境中扩展和循环运动</b>（比如家里的一个扫地机器人就环境受限）。<b>可重复定位</b>（即不发生从地面真实逐渐漂移的情况）在这里将是必不可少的（区别于不断探索新地区）。这就是全概率SLAM方法发挥作用的地方：它会构建一个持久的场景地标地图，在基于状态的框架中被无限引用，并允许<b>循环闭包纠正长期漂移</b>。形成一个持久的世界地图意味着，如果摄像机的运动受到限制，那么算法的处理要求是有界的，并且可以保持连续的实时操作，这与通过匹配不断积累的历史位姿的跟踪方法明显不同。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>1.1 该论文贡献</p><p>主要贡献是前面说的第一次通过一个自由运动的相机实现实时定位和建图，关键方法是：主动的<b>特征映射和测量</b>、<b>平滑</b>的相机<b>一般3D运动模型</b>（从视频流捕获动态先验信息）、一种自顶向下解决单目<b>特征初始化</b>问题的方法。</p><p>相比于SFM，SLAM实现了<b>相机运动状态和地图的实时概率估计</b>，通过实时估计来实现高效处理。序列SLAM只需使用简单的映射启发式就能非常自然地<b>选择一组高度突出的、可跟踪的、但有效间隔的特征</b>，并将其放入其可视化地图中。合理的<b>置信度约束假设</b>可以避免除最重要的图像处理外的所有图像处理，并且在高帧速率下，除了极小的搜索区域外其他所有输入图像的搜索区域都被作者的算法完全忽略。（我们可以感受到作者的方法化繁为简提高了效率）</p><p>文章作者的映射方法可以概括为<b>“高质量特征的稀疏映射”</b>。</p><p>需要说明的是作者实现的MonoSLAM作为先驱者尤其<b>局限性</b>：应用场景是室内大小的尺寸、运动没有很强烈、实验平台的假设（限制）较多。不过作者的工作已经为SLAM研究开辟了新道路，未来SLAM努力的方向就是更大的场景、更剧烈的运动、更加精准的持续对看到的一切实时建图。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     </p><p class=\"ztext-empty-paragraph\"><br/></p><h2>2 相关工作</h2><p><b>这里简单写写（请注意文章是2007年的）。</b></p><p>文章作者工作的鼻祖：Harris和Pike的机器人系统使用单个摄像头的输入，按顺序构建了可视地图。Ayache和后来Beardsley等人在未校准的几何框架中提出了密切相关的方法，但这些方法无法闭合循环和纠正漂移。                                                    </p><p>Smith等人，还有同时期Moutarlier和Chatila提出，在单个状态向量和<b>扩展卡尔曼滤波器(EKF)</b>更新的协方差矩阵中，考虑所有相关的机器人定位和映射问题。直到20世纪90年代中后期，计算能力达到可以实际测试的水平时他们的才受到广泛关注。一些成功应用逐渐使EKF作为SLAM的核心估计技术得到了广泛的应用，并且其作为一种贝叶斯解的通用性也被理解逐渐理解（在多种平台和传感器下）。</p><p>但<b>EKF</b>存在<b>计算复杂性</b>和由于线性化而<b>精度不高</b>的问题。</p><p>在测绘精度和比例尺方面，重要成果则来自使用激光测距传感器的机器人。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>2.1 基于视觉的SLAM</p><p>作者的算法以视觉作为唯一的外向感知。第一节提到原来很少有人成功做到仅用视觉的SLAM系统，原因是：数据<b>输入速率高</b>、视觉数据固有的<b>3D性质（难提取）</b>、<b>缺深度信息</b>（很关键）、<b>提取长期特征进行映射存在困难</b>（当然作者实现了一个通过闭环纠正漂移的实时建图的SLAM系统）。</p><p>这一节开始作者回顾了很多视觉SLAM的工作，但是现在看来有点远古了，个人感觉并不重要就不多说了我自己也就大概瞅了几眼。（还是本文工作更有开创性，不然我不会第一个读这篇</p><p>作者提到他们的一个<b>工作重点</b>是将SLAM所需的<b>硬件简化</b>到最简单的情况（一台相机连一台电脑），并要求对该相机的<b>自由3D运动做出最低限度的假设</b>。也有人做过和作者他们类似的工作，但是没有做到长期跟踪或者闭环或者是成本等方面有所不足。</p><p>作者在接下来的章节将一步步向不熟悉之前SLAM方法的读者介绍他们的实现方法。</p><p>（希望作者真的是面向比较小白的人写的吧...）</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>3 方法（Method）</h2><p>3.1 基于概率统计的3D建图（Probabilistic 3D Map）</p><p>方法的关键概念是：<b>概率的基于特征的地图</b>。</p><p>这个地图表示了任何拍照时刻相机与特征的<b>状态估计</b>，更重要的是这些<b>估计的不确定性</b>。该地图通过<b>被EKF更新</b>而实现持续动态的演进（概率的状态估计在相机运动和特征观察过程中更新，同时特征可以不断添加也可以删除）。</p><p>该<b>地图的概率特征</b>不仅在于随着时间的推移传播的摄像机状态和特征的<b>平均“最佳”估计值</b>，而且在于描述与这些值可能<b>偏离大小</b>的一阶不确定性分布。（个人觉得可以理解为均值和方差）数学上，这个地图通过状态向量 <img src=\"https://www.zhihu.com/equation?tex=%5Chat%7B%5Cmathbf+x%7D\" alt=\"\\hat{\\mathbf x}\" eeimg=\"1\"/> 和协方差矩阵 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathrm+P\" alt=\"\\mathrm P\" eeimg=\"1\"/> 表示如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-d2a9cc5cb1a33343e254a39adddec472_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"779\" data-rawheight=\"213\" class=\"origin_image zh-lightbox-thumb\" width=\"779\" data-original=\"https://pic3.zhimg.com/v2-d2a9cc5cb1a33343e254a39adddec472_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;779&#39; height=&#39;213&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"779\" data-rawheight=\"213\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"779\" data-original=\"https://pic3.zhimg.com/v2-d2a9cc5cb1a33343e254a39adddec472_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-d2a9cc5cb1a33343e254a39adddec472_b.jpg\"/></figure><p>其中 <img src=\"https://www.zhihu.com/equation?tex=%5Chat%7B%5Cmathbf+x%7D\" alt=\"\\hat{\\mathbf x}\" eeimg=\"1\"/> 由堆积起来的状态估计组成，<img src=\"https://www.zhihu.com/equation?tex=%5Cmathrm+P\" alt=\"\\mathrm P\" eeimg=\"1\"/> 是一个等维度的可以拆分成子矩阵元素的方阵。这样一来所有地图参数的概率分布近似为<b>一维多变量高斯分布</b>，其维数等于状态向量的总大小。</p><p>其中，<b>相机的状态向量</b> <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf+x_v\" alt=\"\\mathbf x_v\" eeimg=\"1\"/> 包括<b>相对于 固定世界坐标系W</b>或<b>相机携带的“机器人”坐标系R</b>的米制的3D位置向量 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf+r%5EW\" alt=\"\\mathbf r^W\" eeimg=\"1\"/> 、方向四元数 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf+q%5E%7BRW%7D\" alt=\"\\mathbf q^{RW}\" eeimg=\"1\"/> 、速度向量 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf+v%5EW\" alt=\"\\mathbf v^W\" eeimg=\"1\"/> 、角速度向量 <img src=\"https://www.zhihu.com/equation?tex=%5Comega+%5ER\" alt=\"\\omega ^R\" eeimg=\"1\"/> (13个参数):</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-1e23ce40967fe3a54e602771836c5412_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"530\" data-rawheight=\"164\" class=\"origin_image zh-lightbox-thumb\" width=\"530\" data-original=\"https://pic3.zhimg.com/v2-1e23ce40967fe3a54e602771836c5412_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;530&#39; height=&#39;164&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"530\" data-rawheight=\"164\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"530\" data-original=\"https://pic3.zhimg.com/v2-1e23ce40967fe3a54e602771836c5412_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-1e23ce40967fe3a54e602771836c5412_b.jpg\"/></figure><p>另外，<b>特征状态</b> <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf+y_i\" alt=\"\\mathbf y_i\" eeimg=\"1\"/> 是特征点位置的3D位置向量。相机和特征几何、坐标框架定义在下图（世界坐标系与机器人坐标系）。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-fa1f0a172a351d6cb121e736fbdb78c6_b.jpg\" data-size=\"normal\" data-rawwidth=\"892\" data-rawheight=\"540\" class=\"origin_image zh-lightbox-thumb\" width=\"892\" data-original=\"https://pic3.zhimg.com/v2-fa1f0a172a351d6cb121e736fbdb78c6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;892&#39; height=&#39;540&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"892\" data-rawheight=\"540\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"892\" data-original=\"https://pic3.zhimg.com/v2-fa1f0a172a351d6cb121e736fbdb78c6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-fa1f0a172a351d6cb121e736fbdb78c6_b.jpg\"/><figcaption>图3a</figcaption></figure><p>之前也讲过作者他们建立地图的主要作用是允许实时的定位而不是要真的去完全描述场景，因此他们的目标是<b>获取高质量“地标”的稀疏集合</b>。作者假设场景是<b>刚性的</b>，则每个<b>地标都是静止的世界特征</b>。特别地，他们工作中假定每个地标都对应一个3D空间分布的<b>点特征</b>。同时，相机也被建模为一个要用<b>平移、旋转参数描述位置的刚体</b>，并且要维持对其线速度和角速度的估计。</p><p>以上描述地标和相机的方式很重要，因为之后会应用将在3.4节解释的运动力学。</p><p>上述的地图可以如下图方式画出：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-2f137259d7088beb6b46ee7c4eb02991_b.jpg\" data-size=\"normal\" data-rawwidth=\"803\" data-rawheight=\"665\" class=\"origin_image zh-lightbox-thumb\" width=\"803\" data-original=\"https://pic2.zhimg.com/v2-2f137259d7088beb6b46ee7c4eb02991_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;803&#39; height=&#39;665&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"803\" data-rawheight=\"665\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"803\" data-original=\"https://pic2.zhimg.com/v2-2f137259d7088beb6b46ee7c4eb02991_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-2f137259d7088beb6b46ee7c4eb02991_b.jpg\"/><figcaption>图1a 一个概率三维地图的快照，显示相机的位置估计和特征位置不确定性的椭球。在这张图和其他图中，特征颜色代码如下:红色=测量成功，蓝色=尝试测量失败，黄色=在这一步没有选择测量。</figcaption></figure><p>所有的几何估计都可以被看作是由表示不确定性边界的椭球形区域所包围(此处对应于三个标准差)。图1a所不能说明的是，不同的椭球体可能具有不同程度的相关性。在时序建图中，经常出现的一种情况：相机同时观察到的空间上距离较近的特征，它们位置的估计却有着明显的差异。（这种情况由地图的协方差矩阵P中由非对角矩阵块中的非零项表示，通过算法的操作自然产生。）</p><p>作者他们的SLAM算法的<b>复杂度</b>是 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathrm+O%28N%5E2%29\" alt=\"\\mathrm O(N^2)\" eeimg=\"1\"/> ，其中N是特征数量。这意味着特征数量需要限制，作者他们的实验条件下限制为100以内，在30Hz下实现。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>接下来作者说明为何使用“标准”的单、全协方差EKF方法来实现SLAM，而不是使用不同概率表示的变量。</p><p>正如前面所说的，作者的目标是<b>受限空间内持久的、可重复的定位</b>。作者他们构建的地图中，观察到的特征的模式和之前一些人的成果不同。</p><p>其他方法中机器人基本上是通过类似于走廊的拓扑结构，沿着探索的道路前行，直到它们回到以前见过的地方（很不频繁），才会纠正围绕循环的漂移。可以采用相对特殊的方法来将校正分布在定义良好的环上，无论是通过一系列不确定的位姿变换，子地图，还是通过从由有限数量的粒子表示的可能贫乏的离散轨迹假设集合中进行选择。 </p><p>作者算法的情况下，当一个自由相机在一个有限的空间内以3D方式移动和旋转时，单个特征将以不同的顺序进出视场，不同深度的不同特征集合在相机旋转时将会变的covisible（查不到这个词），并且不同大小不同互连模式的回路将会被例行闭合。   </p><p>作者等人认为：准确表示<b>地图不同部分之间</b>的详细的灵活的<b>相关性</b>十分重要。在作者那时已知方法中，只有特征的稀疏地图维持在单个状态向量和协方差矩阵内时才前述的相关性才有计算的可能。经过仔细的地图管理，前面说的有限的100个特征足以遍及一个房间大小的空间。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>3.2  自然的视觉地标（Natural Visual Landmarks）</p><p>之前就有工作证明：<b>相对较大的(11*11 pixels)图像块更适合作为持久的地标特征</b>，原因是大的模板相比标准的角点特征有更加独特的签名。然而，作者等人通过使用可用的相机位置信息来改进大的相机位移和旋转情况下的匹配，从而显著提高了该特征的效用。</p><p><b>突出图像区域</b>最初是通过Shi和Tomasi的检测算子从相机获取的单色图像中自动检测的(3.7节会介绍一些策略；注意作者当时使用单色图像主要是出于效率的考量)。SLAM目标是在可能的相机<b>剧烈运动下</b>能够<b>重复识别</b>一样的视觉地标，而由于在很小程度的相机运动后地标的外观很可能发生明显变化，所以直接的2D模板匹配作用有限。相应的进行改进，作者等人做出了<b>各个地标分布在一个局部平面上的近似</b>——很多情况下这个近似很合适，也有一些情况下这个近似表现很糟，但总比假设地标的图像块外表根本不变化要强。此外，由于我们不知道这个表面的方向，所以在<b>初始化时指定表面法向量平行于从特征到相机的向量</b>(在3.8节中，作者将提出一种方法来更新这个法向量方向的估计)。一旦一个特征的<b>三维位置及深度被完全初始化</b>，使用3.6节的方法，每个<b>特征被存储为一个有方向的平面的纹理</b>(下图 图1b所示)。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-d3db46182ab79bbeb3bc5f3b8608acb7_b.jpg\" data-size=\"normal\" data-rawwidth=\"1181\" data-rawheight=\"469\" class=\"origin_image zh-lightbox-thumb\" width=\"1181\" data-original=\"https://pic4.zhimg.com/v2-d3db46182ab79bbeb3bc5f3b8608acb7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1181&#39; height=&#39;469&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1181\" data-rawheight=\"469\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1181\" data-original=\"https://pic4.zhimg.com/v2-d3db46182ab79bbeb3bc5f3b8608acb7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-d3db46182ab79bbeb3bc5f3b8608acb7_b.jpg\"/><figcaption>图1b 检测到的视觉显著特征斑块作为视觉地标。通过反向投影推导出三维平面区域到其估计的世界位置。将这些平面区域投影到未来估计的摄像机位置，就可以预测新视角下patch的外观。</figcaption></figure><p>当从<b>新的相机位置</b>对特征进行测量时，<b>patch可以从3D投影到图像平面来为真实图像匹配创建一个模板</b>。此模板将是首次检测到该特性时捕获的原始正方形模板的扭曲版本。在一般情况下，这将是一个完全的<b>投影扭曲</b>，具有剪切和透视畸变，因为我们只是让模板反向、前向的通过相机模型。</p><p>即使特征所在的表面的方向不正确，扭曲仍然能够在绕扭转轴旋转、缩放等情况下成立。注意，作者的算法<b>不会随着时间的推移而更新保存的特性特征</b>，因为目标是可重复的定位，所以我们需要能够在任意长时间内精确地重新度量特征的位置。随着时间的推移而更新的模板往往会逐渐偏离最初的位置。</p><p class=\"ztext-empty-paragraph\"><br/></p><p> 3.3 系统初始化（System Initialization）</p><p>多数SLAM系统，刚刚开机时机器人对于周围世界的结构并没有什么确切的知识。建议一个可以在其中估计运动并建图的坐标系是一件很自由的事情，一般都会选择把坐标系固定在机器人的起点位置。</p><p>在作者他们的单目视觉SLAM算法中，选择使用小量的场景<b>先验信息</b>帮助系统启动，这些信息蕴含在一个<b>放在相机前的已知目标的形状</b>里。这样做提供了几个（一般是四个）已知位置和外观的特征。两点主要原因如下：</p><p>       ①单目SLAM中，没有直接的方法来测量特征深度或里程，然而若从已知尺寸的目标开始，我们就可以给估计的地图和运动设置一个<b>精确的scale</b>，这样运行时scale（比例）就不是一个完全未知的自由度。</p><p>       ②从一开始就在地图中拥有一些特征，这意味着我们可以立即进入正常的<b>预测-测量-更新</b>跟踪序列，而无需任何特殊的第一步。只使用一台相机时，由于深度未知，仅一次测量后特征并不能被完全初始化到地图中去，从而在我们的标准框架中将陷入没有特征匹配的困境，无法估计相机从第一帧到第二帧的运动。（当然，标准的立体算法提供了一个单独的方法，可以用来引导运动和结构估计）</p><p>如下，图2a 展示了有着典型初始化目标的跟踪的第一步。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-a145bd5f7d629588da07e6fa62cce2cd_b.jpg\" data-size=\"normal\" data-rawwidth=\"514\" data-rawheight=\"385\" class=\"origin_image zh-lightbox-thumb\" width=\"514\" data-original=\"https://pic2.zhimg.com/v2-a145bd5f7d629588da07e6fa62cce2cd_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;514&#39; height=&#39;385&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"514\" data-rawheight=\"385\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"514\" data-original=\"https://pic2.zhimg.com/v2-a145bd5f7d629588da07e6fa62cce2cd_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-a145bd5f7d629588da07e6fa62cce2cd_b.jpg\"/><figcaption>图2a 跟踪的第一帧，初始化目标四个已知特征的匹配。大的圆形搜索区域反映了初始相机位置估计的高不确定性。</figcaption></figure><p>在图示情况下，已知特征是黑矩形的四角——他们的位置在系统启动时就被置入地图中，且无任何不确定。接下来就是这些特征定义了SLAM的世界坐标。在第一跟踪帧上，摄像机被保持在相对于目标的某一近似已知的位置，以便跟踪开始。在状态向量中，初始相机位置被给出了一个初始不确定度，对应于几个度和厘米。这允许跟踪在第一帧非常健壮地“锁定”，只需启动标准跟踪周期。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>3.4 运动建模与预测（ Motion Modeling and Prediction ）</p><p>启动后，<b>状态向量可以用两种方式去更新</b>：<b>①</b>预测步骤，当摄像机在捕获图像之间的“盲”间隔内移动时<b>②</b>完成特征测量后的更新步骤。在本节中考虑<b>预测步骤</b>。</p><p>为一个由未知的人、机器人或其他运动物体携带的敏捷相机构建的运动模型，乍一看，似乎与为一个在平面上移动的轮式机器人建模有着根本的不同：关键区别在于后者的运动是被明确的我们知道的“控制输入”所驱动的，而前者的情况下对于相机运动我们<b>没有掌握任何先验信息</b>。然而最重要的，我们要知道两种情况都是表示物理系统的连续模型上的点。<b>任何一个模型都必须停留在一定程度的细节以及对模型与现实之间差异做出的概率假设之上</b>：这就是所谓的<b>过程不确定性</b>(process uncertainty 或噪声)。在轮式机器人情况下，<b>不定项</b>（uncertainty term）考虑了一些因素，如潜在的车轮滑移、表面不规则性和其他尚未明确建模的主要非系统影响。在之前敏捷相机中，它考虑了人类或机器人载体的未知动态和意图，但这些也可以被概率建模。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>作者目前使用的是“<b>恒定速度、恒定角速度模型</b>”。注意，这并不是说假设相机一直以恒定的速度运动，而是<b>统计模型在一个时间步长内的运动，平均来说，期望其未确定的加速度以高斯分布出现</b>。</p><p>模型如图2b所示。这个模型的含义是，我们在预期的相机运动中强加了一定的<b>平滑度</b>：很大的加速度是不太可能的。该模型具有微妙的有效性，即使在视觉测量稀疏的情况下也能给整个系统带来鲁棒性。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-4b11264b9497a24b58eb72b7994c8e6b_b.jpg\" data-size=\"normal\" data-rawwidth=\"542\" data-rawheight=\"247\" class=\"origin_image zh-lightbox-thumb\" width=\"542\" data-original=\"https://pic4.zhimg.com/v2-4b11264b9497a24b58eb72b7994c8e6b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;542&#39; height=&#39;247&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"542\" data-rawheight=\"247\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"542\" data-original=\"https://pic4.zhimg.com/v2-4b11264b9497a24b58eb72b7994c8e6b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-4b11264b9497a24b58eb72b7994c8e6b_b.jpg\"/><figcaption>图2b “平滑”运动模型的可视化：在每个相机位置，我们预测最有可能的路径，以及偏差较小的备选路径。</figcaption></figure><p>作者假设，在每一个时间步长，<b>0均值</b>和<b>高斯分布</b>的未知加速度 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf+a%5EW\" alt=\"\\mathbf a^W\" eeimg=\"1\"/> 和角加速度 <img src=\"https://www.zhihu.com/equation?tex=%5Calpha%5EW\" alt=\"\\alpha^W\" eeimg=\"1\"/> 过程引起速度和角速度的一个脉冲（<b>这种未知的加速就是噪声</b>）:</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-2104b550142f27b1a9f7deb0cda0dddf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"573\" data-rawheight=\"102\" class=\"origin_image zh-lightbox-thumb\" width=\"573\" data-original=\"https://pic4.zhimg.com/v2-2104b550142f27b1a9f7deb0cda0dddf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;573&#39; height=&#39;102&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"573\" data-rawheight=\"102\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"573\" data-original=\"https://pic4.zhimg.com/v2-2104b550142f27b1a9f7deb0cda0dddf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-2104b550142f27b1a9f7deb0cda0dddf_b.jpg\"/></figure><p>根据具体情况， <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf+V%5EW\" alt=\"\\mathbf V^W\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5COmega%5ER\" alt=\"\\Omega^R\" eeimg=\"1\"/> 可能是耦合在一起的(例如，假设每一时间步长都有一个力脉冲作用于携带着相机的物体的刚体形状，导致其线速度和角速度的同时相关变化)。然而，目前我们假设<b>噪声向量</b> <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf+n\" alt=\"\\mathbf n\" eeimg=\"1\"/> 的<b>协方差矩阵是对角的</b>，表示所有线性和旋转分量中不相关的噪声。产生的状态更新为：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-51121e97d696e3ef1b5abf387c947d36_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"695\" data-rawheight=\"178\" class=\"origin_image zh-lightbox-thumb\" width=\"695\" data-original=\"https://pic3.zhimg.com/v2-51121e97d696e3ef1b5abf387c947d36_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;695&#39; height=&#39;178&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"695\" data-rawheight=\"178\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"695\" data-original=\"https://pic3.zhimg.com/v2-51121e97d696e3ef1b5abf387c947d36_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-51121e97d696e3ef1b5abf387c947d36_b.jpg\"/></figure><p>这里 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf+q+%28%28%5Comega%5ER%2B%5COmega%5ER%29%5CDelta+t%29\" alt=\"\\mathbf q ((\\omega^R+\\Omega^R)\\Delta t)\" eeimg=\"1\"/> 表示了被角轴旋转矢量 <img src=\"https://www.zhihu.com/equation?tex=%28%5Comega%5ER%2B%5COmega%5ER%29%5CDelta+t\" alt=\"(\\omega^R+\\Omega^R)\\Delta t\" eeimg=\"1\"/> 定义的<b>四元数</b>。在扩展的卡尔曼滤波器中，新的状态估计 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf+f_v%28%5Cmathbf+x_v%2C%5Cmathbf+u%29\" alt=\"\\mathbf f_v(\\mathbf x_v,\\mathbf u)\" eeimg=\"1\"/> 一定伴随着相机运动后状态（过程噪声协方差） <img src=\"https://www.zhihu.com/equation?tex=%5Cmathrm+Q_v\" alt=\"\\mathrm Q_v\" eeimg=\"1\"/> 不确定性的增加。我们通过雅可比计算得到 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathrm+Q_v\" alt=\"\\mathrm Q_v\" eeimg=\"1\"/> ：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-0604045944dae6e808899f24db0ff4e3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"566\" data-rawheight=\"109\" class=\"origin_image zh-lightbox-thumb\" width=\"566\" data-original=\"https://pic4.zhimg.com/v2-0604045944dae6e808899f24db0ff4e3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;566&#39; height=&#39;109&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"566\" data-rawheight=\"109\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"566\" data-original=\"https://pic4.zhimg.com/v2-0604045944dae6e808899f24db0ff4e3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-0604045944dae6e808899f24db0ff4e3_b.jpg\"/></figure><p>其中 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathrm+P_n\" alt=\"\\mathrm P_n\" eeimg=\"1\"/> 是噪声向量 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf+n\" alt=\"\\mathbf n\" eeimg=\"1\"/> 的协方差。EKF的实现还需要计算雅可比 <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial%5Cmathbf+f_v%7D%7B%5Cpartial%5Cmathbf+x_v%7D\" alt=\"\\frac{\\partial\\mathbf f_v}{\\partial\\mathbf x_v}\" eeimg=\"1\"/> 。这些雅可比计算是复杂的，但是是一种容易处理的微分。</p><p><b>噪声协方差的讨论</b>：该运动模型中不确定性的增长速度由<img src=\"https://www.zhihu.com/equation?tex=%5Cmathrm+P_n\" alt=\"\\mathrm P_n\" eeimg=\"1\"/>的大小决定，将其中参数设置为小或大的值定义了我们期望的运动的平滑度。在<img src=\"https://www.zhihu.com/equation?tex=%5Cmathrm+P_n\" alt=\"\\mathrm P_n\" eeimg=\"1\"/>很小的情况下，我们期望有很小加速度的平滑运动，能够很好地跟踪这种类型的运动，但是不能处理突然的快速运动。很大的<img src=\"https://www.zhihu.com/equation?tex=%5Cmathrm+P_n\" alt=\"\\mathrm P_n\" eeimg=\"1\"/>意味着系统中的不确定性在每一个时间步长上显著增加，而这使得系统能够处理快速的加速度，非常大的不确定性意味着在每一个时间步长上必须进行许多更精确的测量来约束估计。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>3.5 活动特征测量与地图更新（Active Feature Measurement and Map Update ）</p><p>在本节中，我们将考虑SLAM地图中已经存在的<b>特征的测量过程</b>（我们将在下一节中讨论初始化）。</p><p>我们的方法的一个关键部分是在<b>决定测量哪个特征之前预测每个特征的图像位置</b>。特性匹配本身是为模板patch（使用3.2节的方法投射到当前相机估计的patch）使用一个直接的<b>归一化的互相关搜索</b>，模板的匹配方法则是扫描全图每一个位置并测试，直到找到一个峰值。<b>搜索匹配在计算上代价太大；预测是一种主动的方法，通过缩小搜索范围来实现效率的最大化</b>。</p><p>首先，利用相机位置的估计值 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf+x_v\" alt=\"\\mathbf x_v\" eeimg=\"1\"/> 和特征位置估计值的 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf+y_i\" alt=\"\\mathbf y_i\" eeimg=\"1\"/> ，则期望<b>点特征相对于相机的位置</b>为：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ba5c3277609c2604ba8092c41105f0b7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"515\" data-rawheight=\"56\" class=\"origin_image zh-lightbox-thumb\" width=\"515\" data-original=\"https://pic4.zhimg.com/v2-ba5c3277609c2604ba8092c41105f0b7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;515&#39; height=&#39;56&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"515\" data-rawheight=\"56\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"515\" data-original=\"https://pic4.zhimg.com/v2-ba5c3277609c2604ba8092c41105f0b7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-ba5c3277609c2604ba8092c41105f0b7_b.jpg\"/></figure><p>使用透视相机，那么<b>特征期望出现在图中的位置</b>(u,v)可以通过标准的针孔模型得到：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-1936f057d787304500a35d270b354efa_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"582\" data-rawheight=\"145\" class=\"origin_image zh-lightbox-thumb\" width=\"582\" data-original=\"https://pic3.zhimg.com/v2-1936f057d787304500a35d270b354efa_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;582&#39; height=&#39;145&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"582\" data-rawheight=\"145\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"582\" data-original=\"https://pic3.zhimg.com/v2-1936f057d787304500a35d270b354efa_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-1936f057d787304500a35d270b354efa_b.jpg\"/></figure><p>其中 <img src=\"https://www.zhihu.com/equation?tex=fk_u%2Cfk_v%2Cu_0%2Cv_0\" alt=\"fk_u,fk_v,u_0,v_0\" eeimg=\"1\"/> 都是标准的相机校准参数。</p><p>在作者工作中，他们使用了有着几乎100度角视野的<b>广角</b>相机，因为之前有研究表明SLAM的<b>准确性可以通过牺牲每像素的角分辨率换取增大的视野来提高</b>——当观察角度显著不同的多个特征能被同时观察到时，相机和地图估计将被更好地约束。但是，这类相机的照片特性不能像透视那样可以被很好地近似——如图3b所示，照片会表现出严重的<b>非透视畸变</b>（三维世界的直线在图片中并不会投影为直线）。然而，我们对这些原始图像<b>先进行特征匹配</b>，而不是先对它们进行逆失真恢复(注意，为了将这些图像用于增强现实，之后必须将其转换为透视投影以便显示，因为OpenGL只支持透视相机模型)。</p><p>我们因此用<b>径向畸变</b>扭曲透视投影的坐标 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf+u%3D%28u%2Cv%29\" alt=\"\\mathbf u=(u,v)\" eeimg=\"1\"/> 来得到最终预测的图像位置 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf+u_d%3D%28u_d%2Cv_d%29\" alt=\"\\mathbf u_d=(u_d,v_d)\" eeimg=\"1\"/> .选择以下径向<b>畸变模型</b>，是因为它具有很好的近似性，是<b>可逆</b>的:</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-5312beb04e7c6ded7a1740fce1971131_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"851\" data-rawheight=\"362\" class=\"origin_image zh-lightbox-thumb\" width=\"851\" data-original=\"https://pic2.zhimg.com/v2-5312beb04e7c6ded7a1740fce1971131_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;851&#39; height=&#39;362&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"851\" data-rawheight=\"362\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"851\" data-original=\"https://pic2.zhimg.com/v2-5312beb04e7c6ded7a1740fce1971131_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-5312beb04e7c6ded7a1740fce1971131_b.jpg\"/></figure><p>使用标准校准软件和网格来校准的相机的<b>典型校准值</b>为<img src=\"https://www.zhihu.com/equation?tex=fk_u%3Dfk_v%3D195\" alt=\"fk_u=fk_v=195\" eeimg=\"1\"/>pixels， <img src=\"https://www.zhihu.com/equation?tex=%28u_0%2Cv_0%29%3D%28162%2C125%29\" alt=\"(u_0,v_0)=(162,125)\" eeimg=\"1\"/> , <img src=\"https://www.zhihu.com/equation?tex=K_1%3D6%5Ctimes10%5E%7B-6%7D\" alt=\"K_1=6\\times10^{-6}\" eeimg=\"1\"/> （捕获图片为320*240的分辨率）。</p><p>还计算了这个两步投影函数相对于摄像机和特征位置的雅可比矩阵(这是一个简单的微分问题，很容易在纸上或软件中执行)。这些允许计算<b>预测的特征的图像位置中存在的不确定性</b>，可以由<b>对称的2*2  innovation covariance matrix</b>（不知道标准的译名）<b> Si</b>表示:</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-29eca7ac9b906886d9755354fef8f8d4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"834\" data-rawheight=\"184\" class=\"origin_image zh-lightbox-thumb\" width=\"834\" data-original=\"https://pic1.zhimg.com/v2-29eca7ac9b906886d9755354fef8f8d4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;834&#39; height=&#39;184&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"834\" data-rawheight=\"184\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"834\" data-original=\"https://pic1.zhimg.com/v2-29eca7ac9b906886d9755354fef8f8d4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-29eca7ac9b906886d9755354fef8f8d4_b.jpg\"/></figure><p>测量中的<b>恒定噪声协方差R</b>是对角矩阵，大小由图像分辨率决定。</p><p><b>Si</b>的已知知识允许了进行<b>图片搜索的主动方法</b>：Si表示了图像坐标下2D高斯概率密度函数的形状；选择一定数量的标准差(门控，通常是在 <img src=\"https://www.zhihu.com/equation?tex=3%5Csigma\" alt=\"3\\sigma\" eeimg=\"1\"/> 处)定义了一个<b>椭圆搜索窗口</b>，在这个窗口内特征出现的概率应该很高。在作者的系统中，<b>相关搜索总是在门控搜索区域内进行</b>，最大限度地提高了效率，最大限度地减少了错误匹配的可能。见图3b：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-1bf5d4c0c5ac0e24ceb599761fd9e3a0_b.jpg\" data-size=\"normal\" data-rawwidth=\"677\" data-rawheight=\"520\" class=\"origin_image zh-lightbox-thumb\" width=\"677\" data-original=\"https://pic1.zhimg.com/v2-1bf5d4c0c5ac0e24ceb599761fd9e3a0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;677&#39; height=&#39;520&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"677\" data-rawheight=\"520\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"677\" data-original=\"https://pic1.zhimg.com/v2-1bf5d4c0c5ac0e24ceb599761fd9e3a0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-1bf5d4c0c5ac0e24ceb599761fd9e3a0_b.jpg\"/><figcaption>图3b 在广角相机的原始图像中主动搜索特征。椭圆表示摄像机与特征相对位置的不确定性所产生的特征搜索区域，只搜索这些区域。</figcaption></figure><p>Si在主动搜索中有进一步的作用：它是对测量中所期望的信息内容的度量。<b>大的Si（结果难以预测）的特征搜索将提供更多关于相机估计和特征位置的信息</b>。之前第一作者的一些工作中，利用这点可以主动控制让机器人的视角朝最有利测量的方向变化。</p><p>在这里，我们无法控制相机的运动，但在有许多候选测量值可用的情况下，我们选择那些具有大的<b>Si</b>的测量值，<b>将每帧特征搜索的最大数量限制在最有信息量的10或12个</b>。选择这样的测量是为了沿着可用的最长轴<b>压缩系统中的不确定性</b>，并帮助确保在估计状态下不确定性的任何特定部分不会失控。</p><p>作者他们的主动搜索技术的比较明显的一点是：这是一种非常快速的<b>自底向上特征检测算法</b>，这种算法不加选择地处理图像，但可以在几毫秒内提取出图像中的所有特征。有了<b>主动搜索</b>算法，我们总是能够减少图像处理的工作量，但却要付出额外计算的巨大代价才能计算出<b>在哪里进行搜索</b>。</p><p>然而如果失去了相机的踪迹，作者他们并不认为主动搜索是有意义的——在不确定性很高的情况下，需要一个不同的过程来重新定位。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>3.6 特征初始化（Feature Initialization ）</p><p>在我们的单眼相机中，由于特征深度未知，无法直接将特征测量模型翻转来给出给定图像测量的新特征的位置和相机的位置。估计一个特征的深度需要摄像机的运动和从不同视角进行的多次测量。然而，作者他们避免了对图像中的新特征进行多帧跟踪而不尝试估计其三维位置的方法，然后执行一个小批估计步骤通过<b>多视角三角分析</b>来初始化其深度。这将违反作者自顶向下的方法并浪费可用的信息：当<b>相机快速移动</b>时，2D跟踪可能非常困难。此外，我们通常需要<b>非常快速地初始化特征</b>，因为视野狭窄的相机可能很快就会经过这些特征。</p><p>在识别和第一次测量新特征之后，我们使用的方法是初始化一条<b>过特征</b>的3D<b>线</b>到地图中。这是一条<b>半无限</b>的线，从估计的摄像机位置开始，沿特征观测方向趋于无穷，与其他地图成员一样，其<b>参数具有高斯不确定性</b>。该线在SLAM地图中表示为：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-41f677386503682b7e09f1ffc35dd2ff_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"240\" data-rawheight=\"99\" class=\"content_image\" width=\"240\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;240&#39; height=&#39;99&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"240\" data-rawheight=\"99\" class=\"content_image lazy\" width=\"240\" data-actualsrc=\"https://pic4.zhimg.com/v2-41f677386503682b7e09f1ffc35dd2ff_b.jpg\"/></figure><p>其中 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf+r_i\" alt=\"\\mathbf r_i\" eeimg=\"1\"/> 是该线一端点的位置， <img src=\"https://www.zhihu.com/equation?tex=%5Chat%7B%5Cmathbf%7Bh%7D%7D_i%5EW\" alt=\"\\hat{\\mathbf{h}}_i^W\" eeimg=\"1\"/> 是描述线方向的单位向量。</p><p>特征点的所有可能的三维位置都在这条直线上，但是我们还没有确定其位置的剩下一个自由度——特征点延这条线离端点的深度（距离）。<b>一组离散的深度假设沿这条线均匀分布</b>，可以认为是一维粒子分布或直方图表示的<b>深度的一维概率密度</b>。现在，我们做<b>一个近似</b>，在接下来的几个时间步骤中，当这个新特征被重新观察到，它的图像位置的测量只提供关于这个深度坐标的信息，它们对线的参数的影响是可以忽略的。这是一个很好的近似，因为深度的不确定性与直线方向的不确定性相比大很多。当<b>特征用一条线和一组深度假设</b>表示时，我们将其称为<b>部分初始化</b>。一旦我们<b>通过有峰值的深度的PDF（概率密度函数）的形式获得了良好的深度估计</b>，我们就可以<b>使用标准的3D高斯表示将该特征转换为“完全初始化”</b>。</p><p>在随后的每一个时间步长中，这些<b>假设都通过将它们投影到图像中来进行进行测试</b>，在图像中<b>每一个假设都被实例化为一个椭圆搜索区域</b>。每个椭圆的大小和形状都取决于该线的不确定参数：<b>每个距离 <img src=\"https://www.zhihu.com/equation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"/> 处的离散假设都有3D的世界坐标</b> <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf+y_%7B%5Clambda+i%7D%3D%5Cmathbf+r_i%5EW%2B%5Clambda+%5Chat%7B%5Cmathbf+h%7D_i%5EW\" alt=\"\\mathbf y_{\\lambda i}=\\mathbf r_i^W+\\lambda \\hat{\\mathbf h}_i^W\" eeimg=\"1\"/> （这个特征点坐标很好懂，方向配合离端点距离的表示方法）。这个坐标位置通过标准度量函数和3.5节的相对雅可比投影到图像中去，以此来得到每个深度的搜索椭圆。注意对于非投影相机的情况（例如用广角相机），椭圆中心将落在一条曲线上（而不是直线）——这并不存在问题，因为我们分别独立对待每一个假设。</p><p>作者使用一种有效的算法<b>在这组椭圆上对相同的特征模板进行相关搜索</b>，这些椭圆通常是显著重叠的（该算法建立了相关分值的查找表，避免了重叠区域的图像处理工作重复）。每个椭圆内的特征匹配为每个椭圆产生一个概率，其概率通过贝叶斯规则重新加权：<b>似然分值</b>就是椭圆搜索区域所隐含的二维高斯PDF在图像空间中所表示的<b>概率</b>。注意，在许多具有相<b>对较小重叠的小椭圆的情况下（当相机的定位估计很好的时候）</b>，我们在不同深度假设间的分辨能力（resolving power）要强很多，这也影响了深度分布走向（崩溃 collapse）峰值的速度。</p><p>图4为多帧搜索的进展情况，图5为典型的分布随时间的演化，从均匀到尖峰。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-14a1b0b33ab3f3bfe03a6d2b6c304393_b.jpg\" data-size=\"normal\" data-rawwidth=\"1344\" data-rawheight=\"279\" class=\"origin_image zh-lightbox-thumb\" width=\"1344\" data-original=\"https://pic4.zhimg.com/v2-14a1b0b33ab3f3bfe03a6d2b6c304393_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1344&#39; height=&#39;279&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1344\" data-rawheight=\"279\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1344\" data-original=\"https://pic4.zhimg.com/v2-14a1b0b33ab3f3bfe03a6d2b6c304393_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-14a1b0b33ab3f3bfe03a6d2b6c304393_b.jpg\"/><figcaption>图4 特征初始化中连续帧图像搜索的特写。在第一帧中，在搜索区域内识别候选特征图像patch。一条3D射线被添加到SLAM地图中，这条射线被投影到后续的图像中。深度假设从0.5 m到5 m的分布通过新的相机位置相对于射线的不确定性转化为一组椭圆，这些椭圆都被搜索以产生深度分布的贝叶斯重加权的可能性。</figcaption></figure><p>（图4 少量的时间步长通常足以减少深度，有可能足够近似为高斯分布，并使特征可以转换为一个完全初始化的点表示。）</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-9accc88d3cb98377a70f891c14fb192c_b.jpg\" data-size=\"normal\" data-rawwidth=\"550\" data-rawheight=\"427\" class=\"origin_image zh-lightbox-thumb\" width=\"550\" data-original=\"https://pic1.zhimg.com/v2-9accc88d3cb98377a70f891c14fb192c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;550&#39; height=&#39;427&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"550\" data-rawheight=\"427\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"550\" data-original=\"https://pic1.zhimg.com/v2-9accc88d3cb98377a70f891c14fb192c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-9accc88d3cb98377a70f891c14fb192c_b.jpg\"/><figcaption>图5 粒子集合表示的特征深度上的概率密度逐帧演化。100个等权重粒子最初均匀分布在0.5 m至5.0 m范围内;随着每次后续的图像测量，分布变得更接近高斯分布。</figcaption></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>当<b>深度与深度估计值的标准差之比</b>低于<b>阈值</b>(目前为0.3)时，可以安全地将分布近似为高斯分布，并将特征初始化为地图中的一个点。刚越过这个阈值的特征通常会保留较大的深度不确定性(见图1a，图中显示了几个沿相机大致观察方向拉长的不确定性椭球)，但随着相机的移动和进一步的标准测量，这种不确定性会迅速缩小。</p><p>影响这个初始化的重要因素是由<b>重叠椭圆生成的搜索区域的形状</b>。先验深度消除了沿整个极线搜索的需要，提高了初始化的鲁棒性和速度。在实时实现中，通过对每一步最弱的假设进行确定性<b>剪枝</b>，辅助粒子分布崩溃的速度(并节省相关搜索工作)，在典型的2-4帧左右的运动中，剪枝就足够了。需要注意的是，我们所做的大多数实验都涉及到相机的<b>侧向运动</b>，这种初始化方法在视差较小的光轴上的运动效果会更差。</p><p>自本节的初始化算法首次发表以来，对基本思想的一些有趣的发展已经发表。（最早初始化的算法发表：<i>A.J. Davison, “Real-Time Simultaneous Localisation and Mapping with a Single Camera,”Proc. Ninth Int’l Conf. Computer Vision,2003. </i>）特别是，Sola &#39;等人提出了一种算法，该算法通过一组沿三维初始化线重叠分布的三维高斯分布表示刚刚初始化特征的不确定性。这种方法吸引人的地方首先是高斯分布，它在逆深度上是均匀的，而不像我们的技术中均匀的是深度——这似乎是对多个样本的更有效的使用。此外，他们的技术允许对新特征的测量立即在改进相机的定位估计方面产生影响，从而在作者他们算法需要等待特征完全初始化的方面做出了改进。（后面还有一点我不再赘述）</p><p class=\"ztext-empty-paragraph\"><br/></p><p>3.7 地图管理（Map Management ）</p><p>整个算法的一个重要部分是对地图中<b>特征数量的管理</b>，需要动态地决定何时应该识别和初始化新特征，以及何时可能需要删除某个特征。作者的<b>地图维护标准</b>旨在保持<b>从任何相机位置可见的可靠特征的数量</b>接近由测量过程的细节、所需的定位精度和可用的计算能力决定的<b>预定值</b>：作者他们发现使用鱼眼相机，a number in the region of 12 可以在不让处理器过载的条件下准确定位。（这是未来理论研究的一个重要部分）</p><p>特征“可见性”(更准确地说，是预测的可测量性)是根据相机与特征的相对位置以及所保存的初始化特征的相机位置来计算的。特征的预测位置必须在图片里，并且相机必须距离看特征的初始视角没有平移太远，否则我们期望的相关性就会失败（注意可以处理任意旋转的情况）。只有当摄像机经过的区域中可见的数字小于这个阈值时，才会将特征添加到地图中——没有充分的理由，不希望增加特征的数量并增加滤波的计算复杂性。通过运行Shi和Tomasi的感兴趣图像运算符来检测特征，从而在放置于图像中的有限大小(约80*60像素)的框中找到最佳候选。<b>搜索框</b>的位置目前是<b>随机选择的</b>，其<b>约束条件</b>是不与任何现有的特征重叠，并且根据当前对摄像机速度和角速度的估计，任何被检测到的特征都不会立即从视野中消失。</p><p>如果在特征应该可见的情况下，经过预定次数的检测和匹配尝试，<b>超过固定比例(在作者的工作中，是50%)失败</b>，则从地图中删除一个特征。这个标准删除了“不好”的特性，原因有很多：它们不是真正的三维点(位于遮挡边界，如t型接头)，它们位于移动的物体上，它们是由曲面上的高光引起的，或者它们经常被遮挡。</p><p>在一段时间内，特征的“自然选择”通过这些地图管理标准进行，从而产生稳定的、静态的、可广泛观察的点特征地图。场景中的<b>杂乱部分（clutter）</b>可以处理，即使它有时会遮挡这些地标，因为试图测量被遮挡的地标只会失败，不会导致过滤器更新。只有当杂乱部分和地标之间的外观相似导致不匹配时，才会出现问题，这可能会导致灾难性的失败。然而，请注意，<b>任何类型的不匹配在良好跟踪期间都是极为罕见的</b>，因为大型特征模板提供了高度的唯一性，而主动搜索方法意味着匹配通常只尝试在非常小的图像区域(通常是横跨15-20像素)内进行。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>3.8 特征方向估计（Feature Orientation Estimation ）</p><p>在3.2节中，我们描述了从图像流中提取的视觉patch特征作为有方向的、局部平面的表面如何插入地图中。但解释说,这些表面的取向是最初只是假设，这证明在合理视角变化下计算特征外表是足够的。这是在第4节和第5节中介绍的应用中使用的方法。</p><p>本节作者展示，可以更进一步，在实时SLAM中使用<b>可视化测量</b>，实际<b>改善每个特征的任意指定方向</b>，<b>恢复特征位置局部表面法线的真实信息</b>。这提高了每个特征的可测量范围，但也使我们朝着实时恢复详细的三维地形图(而不是稀疏地标集)这一可能的未来目标迈进了一步。</p><p>由于我们<b>假设一个特征对应于三维空间中的一个局部平面区域</b>，当摄像机移动时，它的图像外观将通过扭曲为该特征捕获的初始模板，通过改变视点来转换。扭曲的确切性质将取决于相机的初始和当前位置，三维位置的特征，以及其局部表面的方向。SLAM系统提供了实时的相机位姿和3D特征位置的估计。我们现在额外地维护相机初始位置和每点局部表面方向的估计，这允许从当前的角度预测该特性的翘曲外观。在图像中，我们对当前的扭曲进行测量，并利用<b>预测与测量的差值</b>来更新表面方向估计。</p><p>图6a显示了摄像机在两个位置上观察一个有方向的平面patch的几何模型。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-5e3d96ba3efd354f29a752b5c50d347f_b.jpg\" data-size=\"normal\" data-rawwidth=\"604\" data-rawheight=\"281\" class=\"origin_image zh-lightbox-thumb\" width=\"604\" data-original=\"https://pic4.zhimg.com/v2-5e3d96ba3efd354f29a752b5c50d347f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;604&#39; height=&#39;281&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"604\" data-rawheight=\"281\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"604\" data-original=\"https://pic4.zhimg.com/v2-5e3d96ba3efd354f29a752b5c50d347f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-5e3d96ba3efd354f29a752b5c50d347f_b.jpg\"/><figcaption>图6a 相机在两个位置观察一个法向量是n的平面</figcaption></figure><p>将patch在一个视图中的外观与另一个视图中的外观联系起来的<b>扭曲（warp）</b>是由<b>单应性（homography）</b>描述的：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-ffd591fe00d1b7494761ca3fc200bc61_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"606\" data-rawheight=\"67\" class=\"origin_image zh-lightbox-thumb\" width=\"606\" data-original=\"https://pic2.zhimg.com/v2-ffd591fe00d1b7494761ca3fc200bc61_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;606&#39; height=&#39;67&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"606\" data-rawheight=\"67\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"606\" data-original=\"https://pic2.zhimg.com/v2-ffd591fe00d1b7494761ca3fc200bc61_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-ffd591fe00d1b7494761ca3fc200bc61_b.jpg\"/></figure><p><img src=\"https://www.zhihu.com/equation?tex=%5Cmathrm+C\" alt=\"\\mathrm C\" eeimg=\"1\"/> 是相机的校准矩阵，描述具有径向畸变的图像中的透视投影或局部近似透视投影； <img src=\"https://www.zhihu.com/equation?tex=%5Cmathrm+R\" alt=\"\\mathrm R\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf+t\" alt=\"\\mathbf t\" eeimg=\"1\"/> 描述相机的运动； <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf+n\" alt=\"\\mathbf n\" eeimg=\"1\"/> 是平面法向量； <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf+x_p\" alt=\"\\mathbf x_p\" eeimg=\"1\"/> 是patch中心的图像投影； <img src=\"https://www.zhihu.com/equation?tex=%5Cmathrm+I\" alt=\"\\mathrm I\" eeimg=\"1\"/> 是3*3的单位矩阵。</p><p>假设这种外观预测足以使特征的当前图像位置被发现（在由SLAM滤波器得到的椭圆不确定区域内对两个图像坐标使用一个<b>标准穷举相关搜索</b>）。下一步是测量预测模板和当前图像之间的<b>扭曲变化</b>。</p><p>假设扭曲的变化会很小并且搜索会发现全局最优解，我们进行一个更高效的“<b>概率的 inverse-compositional 梯度下降图像对齐</b>”步骤来在额外参数中搜索（而不是将穷举搜索扩展到所有可能扭曲的自由度并锁定模板的2D图像位置）。图6b 展示了特征方向估计的处理步骤。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-f2b1ba6d550ccc45388e9a57d7c16fad_b.jpg\" data-size=\"normal\" data-rawwidth=\"730\" data-rawheight=\"402\" class=\"origin_image zh-lightbox-thumb\" width=\"730\" data-original=\"https://pic2.zhimg.com/v2-f2b1ba6d550ccc45388e9a57d7c16fad_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;730&#39; height=&#39;402&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"730\" data-rawheight=\"402\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"730\" data-original=\"https://pic2.zhimg.com/v2-f2b1ba6d550ccc45388e9a57d7c16fad_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-f2b1ba6d550ccc45388e9a57d7c16fad_b.jpg\"/><figcaption>图6b 平面特征曲面三维方向估计的处理流程</figcaption></figure><p>当一个<b>新的特征被添加</b>到地图上时，我们<b>初始化</b>了一个与当前视图方向平行但不确定度很大的<b>表面法线估计</b>。目前我们做一种简化的近似：<b>特征法线的估计只与相机和特征位置的估计有微弱的相关性</b>。因此，法线估计不会存储在主SLAM状态向量中，而是保存在每个特征的单独的两个参数的EKF中。</p><p>图7为两种不同场景下的patch定位算法结果：一个室外场景，包含一个主平面；一个室内场景，其中几个盒子表现了不同方向的平面。在这两种情况下，经过几秒钟的跟踪，大多数地图中特征patch的方向都能很好地恢复。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-107c5d1d3c1a4d97465da49a557b8603_b.jpg\" data-size=\"normal\" data-rawwidth=\"1736\" data-rawheight=\"324\" class=\"origin_image zh-lightbox-thumb\" width=\"1736\" data-original=\"https://pic4.zhimg.com/v2-107c5d1d3c1a4d97465da49a557b8603_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1736&#39; height=&#39;324&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1736\" data-rawheight=\"324\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1736\" data-original=\"https://pic4.zhimg.com/v2-107c5d1d3c1a4d97465da49a557b8603_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-107c5d1d3c1a4d97465da49a557b8603_b.jpg\"/><figcaption>图7 这些视图是在几秒钟的运动后从实时运行的系统中捕获的，并显示了最初的线框假设方向和当前纹理patch的估计。</figcaption></figure><p>一般来说，很明显，<b>方向估计只适用于面积大且拥有重要的感兴趣纹理的patch</b>。因为在这种情况下，图像对齐操作可以准确地估计扭曲。这是在每个特征点估计准确法向量的一个<b>限制</b>，因为<b>许多特征具有非常简单的纹理模式</b>，如白色角拐角上的黑色，在这种地方完全的扭曲估计大受限制。在作者他们的例子中，场景某种程度上是人为的，它们同时包含了具有<b>显著扁平纹理</b>的<b>大平面区域</b>。</p><p>然而，应该记住的是，在作者的工作中，<b>估计特征方向的目前动机</b>是提高相机运动的范围，在这个范围内每一个长期地标将是可测量的。</p><p>那些很难得到准确的法线估计的特征正是那些准确估计法向量也不是很有必要的特征，因为它们在外观上表现出一种自然程度上的<b>视点不变性</b>。这些特性的正常估计值是否错误并不重要，因为仍然有可能匹配它们。我们将这项估计特征表面方向的工作视为从实时移动的摄像机中恢复更完整的场景几何的研究方向的一部分。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>4 结果：交互式增强现实（Results:Interactive Augmented Reality）</h2><p>在第5节介绍 MonoSLAM 的机器人应用之前，在本节中，作者给出了在增强现实场景中使用他们算法的结果（虚拟对象是交互式地插入到实时视频中的）。我们展示了如何稳定地将虚拟家具添加到一个30Hz的图像流中，该图像流是在一个房间内移动手持摄像机时捕捉到的。图8给出了本次演示的故事板，在本文提交的视频中有介绍。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-e72493686b724ba1e82e80432e4d6b50_b.jpg\" data-size=\"normal\" data-rawwidth=\"951\" data-rawheight=\"753\" class=\"origin_image zh-lightbox-thumb\" width=\"951\" data-original=\"https://pic1.zhimg.com/v2-e72493686b724ba1e82e80432e4d6b50_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;951&#39; height=&#39;753&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"951\" data-rawheight=\"753\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"951\" data-original=\"https://pic1.zhimg.com/v2-e72493686b724ba1e82e80432e4d6b50_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-e72493686b724ba1e82e80432e4d6b50_b.jpg\"/><figcaption>图8 通过单击实时图形显示的SLAM地图中的特性，可以定义虚拟对象所依附的3D平面。然后，当摄像机继续移动时，这些物体被固定在图像视图中的场景中。我们展示了跟踪对于摄像机快速运动，极端的旋转，和显着阻挡是如何的鲁棒。</figcaption></figure><p><b>在增强现实技术(AR)中，计算机图形被添加到现实世界的图像中，从摄像机生成一个复合场景</b>。如果图形在图像中移动，就像它们被固定在摄像机观察到的3D场景中一样，就会产生令人信服的效果。要做到这一点，相机的运动必须准确知道——然后它的位置可以输入一个标准的3D图形引擎，如OpenGL，然后将图形正确地呈现在真实图像之上。在这里，我们使用MonoSLAM从实时图像流实时估计手持摄像机的运动，并将其直接输入渲染引擎（rendering engine）。有很多种方法可以恢复运动的相机的运动，这已经被用于增强现实，通常具有额外的硬件，如附加在相机上的磁性或超声波传感器。只使用移动相机拍摄的图片来为运动估计实现摄像机跟踪是很有吸引力的一件事，但是以前的方法是离线操作：如用于电影后期制作；或需要先验知识的结构观察场景；或通过基准目标的放置、先验地图学习等等。作者他们在这里的方法是第一个可以在相机通过一个从未见过的场景的情况下，实现令人信服的<b>实时</b>和<b>无漂移</b>的AR。</p><p>在实现中，线性加速度噪声部分标准差被设置为 <img src=\"https://www.zhihu.com/equation?tex=10ms%5E%7B-2%7D\" alt=\"10ms^{-2}\" eeimg=\"1\"/> (重力加速度为1），角加速度噪声部分标准差被设置为 <img src=\"https://www.zhihu.com/equation?tex=6rads%5E%7B-2%7D\" alt=\"6rads^{-2}\" eeimg=\"1\"/> 。这些加速度的大小是经验的相机在手中快速而平稳地移动的近似动力学描述（<b>该算法不能处理非常突然、突然的移动</b>）。作者使用的摄像头是一个低成本的IEEE 1394网络摄像头，带有广角镜头，可在30赫兹下进行拍摄。他们<b>设置了软件控制的快门和增益控制可以消除运动模糊的大部分效果</b>，但<b>保留了高对比度的图像</b>——这在普通明亮的房间里是可以实现的。</p><p>按照图2a所示的简单目标开始初始化后，将摄像机移动到小房间的大部分地方进行实验，在几秒钟内动态地对一组有代表性的特征进行建图。跟踪持续了几分钟(数千帧)，需要SLAM初始化新特性——当然，之前访问的区域被重新访问时几乎不需要新的初始化。</p><p>毫无疑问，系统会运行更长的时间而没有问题，因为一旦图像特征中的不确定性变得很小，它们就非常稳定，并将地图锁定为无漂移状态。注意，<b>一旦对足够多的附近特征进行了建图，就有可能从墙上完全移除初始化目标</b>。</p><p>增强现实技术是通过人类用户的交互实现的，用户可以看到图像流和突出显示的跟踪特征，还可以看到一个可以操纵视点的3D显示下的摄像机和特征的估计位置。通过使用鼠标选择三个已经建图的特征，通过单击两个显示器中的任何一个，用户定义了一个可以附加虚拟对象的平面。在这个演示中，对象是要添加到室内场景中的虚拟家具——也许像在一个虚拟的“厨房配件”应用程序中一样——四个对象被添加到场景中三个不同的平面中，对应于两个垂直墙壁和一个工作台表面。</p><p>一般情况下，该算法在较小的空间内，通过<b>对摄像机的运动相对较少的约束</b>实现鲁棒的实时性能，并且可以<b>实现任意长时间的定位</b>。显然，算法无法应对没有找到有用的特征区域的情况(当相机面临一个空白的墙壁或天花板)。尽管只有两个或三个特征可见时跟踪也能“存活”,但是在这些时间内定位的不确定性越来越大，不过当更多特征回到视野的时候好的跟踪效果也会再次恢复。</p><p>小的和大的循环还是会被系统无缝闭合。相机的旋转运动点到一个新的未被注意的场景的一部分然后返回将导致一个小的闭合回路，将未被观察的区域新初始化特征绑定到地图准确对应的部分，然而一个平移运动在房间里需要较大的循环关闭。<b>主动特征选择机制</b>（3.5节）在闭环时得到了特别令人满意的表现：<b>它希望通过最大限度地减少不确定性来为地图增加最多的信息，这就需要重新观察经过一段时间的忽视后再次出现的特征</b>。这些特征的<b>Si</b> ——innovation covariance scores 得分远远高于最近测量的邻近特征，因为它们相对于摄像机的位置的不确定性增加了（没被测量期间导致之后不确定它的位置）。特别的是，<b>小的循环只要可能就立刻实现闭合</b>，从而减少了由大的闭环带来的可能引发问题的不确定性的较大增长。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>5 结果：类人类机器人SLAM（Results:Humanoid Robot SLAM）</h2><p>在这一节中，作者将介绍如何使用MonoSLAM为一个领先的类人机器人平台（HRP-2）在杂乱的室内工作空间中移动时提供实时SLAM。 </p><p>大多数先进的研究类人机器人都有视觉系统，但在基于视觉的建图和定位方面的尝试有限。Takaoka等人在对机器人附近杂乱的地板进行密集的三维重建时，提出了利用立体视觉和视觉测程方法来估计类人机器人运动的有趣结果。还有研究者利用利用占位网格建图和平面立体检测技术，对微型人形前方的自由空间区域进行了检测，但由于依靠里程表(或其他工作中的人工地板标记)进行定位，所以这也不是真正的SLAM。</p><p>利用MonoSLAM算法，作者他们在线构建的<b>只是一个稀疏的点地标地图</b>，而不是稠密表示，并且表明，尽管高加速度的3D运动带来了挑战，系统可以建立一个持久的地图，<b>在一个小区域内实现无漂移的实时定位</b>。在短暂的未来时间里，典型的类人活动 (例如在某种处理或服务任务期间) 将涉及到在一个小区域 (如一个房间) 内敏捷但重复的移动。一个重要的要求是定位和建图应该是可重复的，这样在<b>重复运动期间机器人的不确定性就不会随着时间的推移而增加</b>。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>5.1 视觉（Vision）</p><p>HRP-2标准配置高性能前视<b>三目</b>摄像机，能够在机器人前方的聚焦观测区域内进行精确的三维测量，适用于抓取或交互任务。由于已经表明，相比之下，一个广阔的视野有利于定位和建图，因此决定在HRP-2上增加一个广角相机 (视场约90度) ，仅使用该相机的输出进行SLAM。作者使用单参数径向畸变模型来校准广角相机，如3.5节所示。</p><p>由于机器人是从观察远壁的位置开始运动的，所以将自然和人为的特征混合放置在测量的位置（主要是在这面墙上）用于SLAM初始化（而不是使用标准目标初始化）。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>5.2 陀螺仪（Gyro）</p><p>与其他本体感受传感器一起，HRP-2在胸部装有一个三轴陀螺仪，用来以200Hz频率报告机体角速度测量结果。在类人SLAM应用中，虽然很有可能发展只使用视觉传感的SLAM，但这一额外信息的现成可用性促使作者在SLAM估计中应用它，并且它在降低绕圈运动不确定性的增长速度方面发挥了作用。</p><p>我们以30hz的视觉速率采样陀螺，以便在SLAM滤波器中使用。我们估计角速度测量的每个元素的标准偏差为 <img src=\"https://www.zhihu.com/equation?tex=0.01rads%5E%7B-1%7D\" alt=\"0.01rads^{-1}\" eeimg=\"1\"/> 。由于单摄像机SLAM状态向量包含了机器人在参考坐标系中表示的角速度，我们可以将这些测量值直接合并到EKF中，作为机器人自身状态的“内部测量”，这是视觉处理之前额外的卡尔曼更新步骤。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>5.3 结果（Results）</p><p>我们进行了一个真实的SLAM测试实验，在这个实验中，机器人被设定在半径0.75 m的圆内行走(图9)。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-fbf6ef621eb319278bbaebf3f2f2bc1a_b.jpg\" data-size=\"normal\" data-rawwidth=\"496\" data-rawheight=\"342\" class=\"origin_image zh-lightbox-thumb\" width=\"496\" data-original=\"https://pic3.zhimg.com/v2-fbf6ef621eb319278bbaebf3f2f2bc1a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;496&#39; height=&#39;342&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"496\" data-rawheight=\"342\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"496\" data-original=\"https://pic3.zhimg.com/v2-fbf6ef621eb319278bbaebf3f2f2bc1a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-fbf6ef621eb319278bbaebf3f2f2bc1a_b.jpg\"/><figcaption>图9 HRP-2绕圈走。该机器人是自主行走，不受束缚，与SLAM处理板和无线以太网连接到一个控制工作站。所见的支撑架仅用于安全目的。</figcaption></figure><p>这是一个完全探索性的动作，包括在结束一个大的循环之前观察新的区域。出于安全和监控的原因，该运动分为五个部分，中间有短暂的静止停顿：首先，机器人在没有旋转的情况下向右做一个向前的对角线运动，在这个运动中，机器人将自己放置在开始圆的位置上，然后向左转4个90度的弧线，机器人沿着一个圆的路径走，总是以切线方向行走。行走速度为HRP-2的标准速度，总行走时间约为30秒(即使机器人暂停，SLAM系统在30 Hz时仍在持续跟踪)。</p><p>图10为实验结果。</p><p>（图10：当人形机器人沿着半径0.75米的圆形轨迹行走时，从MonoSLAM拍摄的快照。黄色轨迹为估计的机器人轨迹，椭圆为特征位置不确定度，颜色编码如图1a所示。在环路闭合和漂移修正之前，可以看到图中的不确定性在增加。(a)早期探索和第一次转弯。(b)映射所有的数据和更大的不确定性。(c)就在闭环之前，最大不确定性。(d)闭环结束，漂移修正。）</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-fc6eb2383cf3b698d0c7900edd342bf7_b.jpg\" data-size=\"normal\" data-rawwidth=\"1015\" data-rawheight=\"504\" class=\"origin_image zh-lightbox-thumb\" width=\"1015\" data-original=\"https://pic4.zhimg.com/v2-fc6eb2383cf3b698d0c7900edd342bf7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1015&#39; height=&#39;504&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1015\" data-rawheight=\"504\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1015\" data-original=\"https://pic4.zhimg.com/v2-fc6eb2383cf3b698d0c7900edd342bf7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-fc6eb2383cf3b698d0c7900edd342bf7_b.jpg\"/><figcaption>图10</figcaption></figure><p>经典的SLAM行为被观察到，新建图的特征的不确定性在不断增长，直到一个先前的特征可以被重新观察到，闭环和漂移被修正。由于存储在协方差矩阵中的相关性，大量的特征被认为同时转向更好的估计位置。这个特征图现在适合长期使用，并且可以在不漂移定位精度的情况下完成任意数量的环路。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>6 系统细节（System Details）</h2><p>作者对<b>非典型环境下</b>的摄像机定位估计<b>精度</b>进行了<b>实验评估</b>。实验的摄像机、运动模型参数、30Hz帧速率与第四节的交互式增强现实实现相同。用精确测量的矩形轨迹和位于一个角落的3.3节的标准初始化目标标记出一个有很多物品的杂乱的水平桌面，定义了世界坐标系的原点和方向。这个场景的其他情况都是事先不知道的。</p><p>然后，移动一个装有已知长度铅垂线的手持摄像机，使垂直悬挂的重物紧贴轨道(图11)。这样，当相机按顺序到达四个角“路点”时，就可以准确地知道相机的地面真三维坐标(估计精度为1cm)。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-f0eb011d3e6a3789fe9ccdeb6bdb9fbc_b.jpg\" data-size=\"normal\" data-rawwidth=\"1238\" data-rawheight=\"236\" class=\"origin_image zh-lightbox-thumb\" width=\"1238\" data-original=\"https://pic1.zhimg.com/v2-f0eb011d3e6a3789fe9ccdeb6bdb9fbc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1238&#39; height=&#39;236&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1238\" data-rawheight=\"236\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1238\" data-original=\"https://pic1.zhimg.com/v2-f0eb011d3e6a3789fe9ccdeb6bdb9fbc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-f0eb011d3e6a3789fe9ccdeb6bdb9fbc_b.jpg\"/><figcaption>图11 地面真值表征实验。(a)摄像头飞过桌面场景，看到初始化目标和矩形轨迹。(b)轨迹在转角处跟随，外景实时相机跟踪，坐标轴增强图像视图。在这两幅图像中，可以看到悬挂的铅垂线，但并没有明显遮挡相机的视野。</figcaption></figure><p>在创建初始地图的几秒钟的短暂初始运动之后，摄像机被移动到并在矩形四角以上的位置一个接一个地暂停。下表给出了摄像机在四个角落的地面真值坐标，然后给出了MonoSLAM重复几次后得到的的平均估计值。 <img src=\"https://www.zhihu.com/equation?tex=%5Cpm\" alt=\"\\pm\" eeimg=\"1\"/> 号表示抽样估计值的标准差。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-0f39606ae81a39904d947fd4d0a016ca_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"641\" data-rawheight=\"274\" class=\"origin_image zh-lightbox-thumb\" width=\"641\" data-original=\"https://pic3.zhimg.com/v2-0f39606ae81a39904d947fd4d0a016ca_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;641&#39; height=&#39;274&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"641\" data-rawheight=\"274\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"641\" data-original=\"https://pic3.zhimg.com/v2-0f39606ae81a39904d947fd4d0a016ca_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-0f39606ae81a39904d947fd4d0a016ca_b.jpg\"/></figure><p>这些数据表明，在这个“桌面”的运动尺度上，MonoSLAM通常会给出精确到几厘米的定位结果，而“抖动”水平在1-2厘米左右。一些坐标，例如第二种路径点报告的0.93均值x值，显示的<b>一致性错误大于抖动级别</b>，这些错误在循环期间持续存在，但是在每次重访时会缓慢减小(在我们的实验中，每个循环大约减少1 cm)，因为概率SLAM逐渐将整个地图拉入一致性。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>6.2 处理要求（Processing Requirements）</p><p>在1.6 GHz奔腾M处理器上，30Hz (即每幅图像可处理33 ms) 条件下，每帧所需的处理时间的典型细分如下:</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-a4bfe904af8f5a398059935cd9aa463e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"585\" data-rawheight=\"234\" class=\"origin_image zh-lightbox-thumb\" width=\"585\" data-original=\"https://pic3.zhimg.com/v2-a4bfe904af8f5a398059935cd9aa463e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;585&#39; height=&#39;234&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"585\" data-rawheight=\"234\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"585\" data-original=\"https://pic3.zhimg.com/v2-a4bfe904af8f5a398059935cd9aa463e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-a4bfe904af8f5a398059935cd9aa463e_b.jpg\"/></figure><p>这说明30Hz的性能很容易实现——事实上，这种安全裕度是可取的，因为处理时间在帧与帧之间波动，另外应该尽可能避免帧丢失。不过，作者认为，如果图形化呈现得到简化，或者更新速度降低，那么在不久的将来，将运算频率加倍到60赫兹应该是可能的。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>6.3 软件（Software）</p><p>本文描述的系统构建在c++库<b>SceneLib</b>中，其中包括示例实时MonoSLAM应用程序，该库是一个开源项目，获得LGPL许可，可以从Scene主页获取使用：</p><a href=\"https://link.zhihu.com/?target=http%3A//www.doc.ic.ac.uk/~ajd/Movies/\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Index of /~ajd/Movies</a><p>1. kitchen.mp4.avi (basic method and augmented reality), </p><p>2. CircleHRP2.mpg (humanoid external view), and </p><p>3. hrploopclose.mpg (humanoid MonoSLAM output).</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>7 总结（Conclusions）</h2><p>在本文中，作者解释了MonoSLAM，一种使用单个自由移动摄像机同时进行定位和映射的实时算法。该方法的主要原理是<b>概率建图</b>、<b>运动建模</b>、<b>主动测量</b>和<b>高质量特征的稀疏图的建立</b>。主动特征搜索提供了效率，确保没有图像处理工作被浪费——这是一种真正的贝叶斯“自顶向下”的方法。作者提出了实验实现，展示了该算法的广泛适用性，希望在低成本和先进机器人、可穿戴计算、工业和娱乐增强现实以及用户界面等应用领域产生影响。</p><p>在未来的工作中，作者计划继续改进算法的性能，以应对更大的环境(室内和室外)，更动态的运动，更复杂的场景与更严重的遮挡，复杂的对象和变化的照明条件，创造真正实用的系统。我们将继续专注于硬实时操作、商用摄像机和最小假设。</p><p>这项工作将涉及几个方面。为了提高算法的动态性能，并能够处理比目前更快的运动，一个很有前途的可能性是研究捕捉速率大于30hz的相机。作者他们方法中主动搜索图像处理的一个有趣的方面是，帧速率加倍并不意味着图像处理工作加倍，就像自底向上的特征检测方案一样(参见[45])，因为由于减少了运动不确定性，搜索区域会相应地变小。目前有CMOS IEEE 1394相机，在全分辨率下提供100Hz的捕捉，在可编程的子窗口中提供更高的速率——作者的主动图像搜索技术将非常适合从中受益。作者他们希望在不久的将来用这种相机。</p><p>将当前生成的稀疏映射开发成更密集的表示形式肯定会有回报，从这些表示形式可以更全面地推断环境的几何形状（最初是通过尝试检测表面等高阶实体）。他们在<b>特征patch方向估计</b>方面的工作给出了一个强有力的暗示：这是可以实现的。因此，我们应该能够构建更完整、更有效的场景表示，同时保持实时操作。这些高效的高阶地图可能会给该SLAM系统一种类似人类的能力，能够快速捕捉房间的基本形状。</p><p>最后，为了<b>将算法扩展到非常大规模的场景</b>中，某种类型的<b>建立子地图策略</b>显然是合适的，尽管正如前面所讨论的仍然不清楚如何将可视化特性的地图清晰地划分为有意义的子块。正如在子地图相关的其他工作所显示的，只要能够在后台“地图匹配”子地图，由精确的小比例图组成的网络就可以通过一组相对松散的估计转换非常成功地连接起来。这与能够解决“丢失的机器人”的问题，定位在已知地图上，只有一个很弱的位置先验信息。并且这个问题已证明使用二维激光数据相对简单。仅凭视觉感知，这种匹配可以通过SIFT等不变的视觉特征类型来实现，或者在我们的环境中通过匹配更高层次的场景特征(如粗糙的3D表面)来实现。</p>", 
            "topic": [
                {
                    "tag": "计算机视觉", 
                    "tagLink": "https://api.zhihu.com/topics/19590195"
                }, 
                {
                    "tag": "同时定位和地图构建（SLAM）", 
                    "tagLink": "https://api.zhihu.com/topics/20033502"
                }
            ], 
            "comments": [
                {
                    "userName": "canny", 
                    "userLink": "https://www.zhihu.com/people/e0de7a675a54a197fa0512a598563766", 
                    "content": "很棒的一篇文章", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "王劭靖", 
                    "userLink": "https://www.zhihu.com/people/16e8bc966dfbf941c5e0ec551c5a74af", 
                    "content": "前辈，有个问题想请教，monoslam检测到环后，是怎么闭环的？[拜托][拜托]", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "贤鱼卓君", 
                            "userLink": "https://www.zhihu.com/people/0be40edd972271a9c9afb446ee1a566d", 
                            "content": "算不上前辈啦。monoslam没有闭环的功能哦", 
                            "likes": 0, 
                            "replyToAuthor": "王劭靖"
                        }, 
                        {
                            "userName": "王劭靖", 
                            "userLink": "https://www.zhihu.com/people/16e8bc966dfbf941c5e0ec551c5a74af", 
                            "content": "感谢[拜托]", 
                            "likes": 0, 
                            "replyToAuthor": "贤鱼卓君"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/51413433", 
            "userName": "贤鱼卓君", 
            "userLink": "https://www.zhihu.com/people/0be40edd972271a9c9afb446ee1a566d", 
            "upvote": 7, 
            "title": "图像理解(第3版) 第1章学习记录", 
            "content": "<p>之前在学那本Multiple View Geometry，看到有一些朋友的关注，我在这里表示感谢，这也是我继续记录自己学习情况的动力之一。但就实际而言那本书十分难啃，而且我觉得初学阶段看的话效用并不大，这导致这个入门过程的效率很低。或许我是有些焦躁但是就我个人的情况而言只能先暂缓那本书的学习了。我的导师让我先学习章毓晋老师的《图像工程》系列书籍。我个人学习过图像低层处理的课程，所以这次为了更快地入门SLAM我选择直接学习该系列的下册《图像理解》，这个看完我会再考虑中册的《图像分析》。之后的学习记录我也不会面面俱到，主要记录下我觉得重要的（譬如会跳过繁琐的工程意义不大的理论概念），希望看到的朋友们理解。另外文字资料、图片等都来自《图像工程（第3版）》特此声明。还是那句话，我的笔记顶多做一个参考，如果有兴趣的话大家还是去看原书。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>啰嗦完了那就开始挖新坑吧！</p><hr/><p>《图像理解》这本书涉及高层图像理解的基本原理和技术，包括对3D客观场景信息的获取和表达、景物重建、场景解释以及完成这些工作所需控制策略等。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>绪论这一章有四个部分：图像工程的发展、图像理解及相关学科、图像理解理论框架、内容框架和特点。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>1.1图像工程的发展</h2><p>首先我们得清楚我们的研究对象“图像”：图像是用各种观测系统以不同形式和手段观测客观世界而获得的，可以直接或间接作用于人眼 并进而产生视知觉的实体。而图像技术是各种图像加工技术的总称，图像工程则是一个对各种图象技术进行综合集成的研究和应用的整体框架。我觉得在这一节我们的任务就是理解图像工程的整体框架。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>首先是图像工程三层次的框图（对应章老师图像工程系列的三本书）。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-e6c5f31edb3f4ad482ceda6599047e87_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"481\" data-rawheight=\"225\" class=\"origin_image zh-lightbox-thumb\" width=\"481\" data-original=\"https://pic4.zhimg.com/v2-e6c5f31edb3f4ad482ceda6599047e87_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;481&#39; height=&#39;225&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"481\" data-rawheight=\"225\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"481\" data-original=\"https://pic4.zhimg.com/v2-e6c5f31edb3f4ad482ceda6599047e87_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-e6c5f31edb3f4ad482ceda6599047e87_b.jpg\"/></figure><p>这张图我们主要明白高层是指高的抽象程度即可。举个例子：一张图电脑读进去就是一个个像素拼成的矩阵，低层的图像处理都是对于这些像素的操作；到中层之后我们的处理“格局”会大一些，针对目标可能有例如图像分割等操作；最高层的则是让电脑像我们人一样能明白一幅图的含义。这最终目标“图像的含义”可能就是几个字的表达（数据量很小），但其中包含的可以说是“人工智能”，所以这是一个相当高的目标（因为是在提取图片精华）。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>然后我们理解一下下图图像工程的整体框架。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-ed26c002a8369aa498698aa141ef8a51_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"598\" data-rawheight=\"296\" class=\"origin_image zh-lightbox-thumb\" width=\"598\" data-original=\"https://pic2.zhimg.com/v2-ed26c002a8369aa498698aa141ef8a51_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;598&#39; height=&#39;296&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"598\" data-rawheight=\"296\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"598\" data-original=\"https://pic2.zhimg.com/v2-ed26c002a8369aa498698aa141ef8a51_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-ed26c002a8369aa498698aa141ef8a51_b.jpg\"/></figure><p>虚线框住的是前面说的图像工程三大部分，它们是基本模块，用各种技术帮人们从场景获得不同层次的信息。</p><p><b>低层处理</b>主要改善图像的视觉效果或者在保持视觉效果的基础上减小数据量（结果主要给用户看）。<b>中层分析</b>主要对感兴趣目标进行检测、提取和测量（结果主要提供描述图像目标特点和性质的数据）。<b>高层理解</b>主要了解把握图像内容并且解释原来的客观场景（结果提供客观世界信息，指导和规划行动）。这三层用到的技术包括人工智能、神经网络、遗传算法、模糊逻辑、图像代数等等。</p><p>同时为了完成一项工作我们需要采取合适的策略控制我们的各种操作。</p><p>（本节最后介绍了一些图像工程的系列综述，这里略过）</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>1.2图像理解及相关学科</h2><p><b>1.2.1 图象理解</b></p><p>图像理解是图像工程的高层。在图象分析的基础上，结合人工智能和认知理论，进一步研究图象中各目标的性质和它们之间的相互联系，并理解图象内容的含义以及解释原来的客观场景，从而指导和规划行动。简单来说，图像理解是在一定程度上以客观世界为中心，借助知识经验把握原本的客观世界。 </p><p>限于目前计算机系统的能力和图象理解技术的水平，“系统”完成较低层的工作，而人需要接着系统完成剩下的较高层的工作。所以说图像理解是目前主要的研究发展瓶颈。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>1.2.2 计算机视觉</b></p><p>我们很多研究都是有些仿生的想法，所以要先了解人类视觉。人类视觉过程：一个复杂的从感觉到知觉的过程。</p><p>感觉：感受到的是3-D世界之2-D投影得到的图象</p><p>知觉：由2-D图象认知3-D世界内容和含义</p><p>计算机视觉是指用计算机实现以上人的视觉功能。其中心问题是：从单目或夺目的、移动或静止的观察者获取到的一个运动或静止的目标或场景的一幅或序列图像中，理解目标场景以及其3D性质（感觉就是SLAM的关键）。</p><p>主要两种方法：仿生和工程。实际应用的都是工程方法，我们不去在意人类视觉系统的内部构造而是仅考虑输入输出，采用任何可行手段实现功能（黑匣子）。另外工程的实现方法也有两种：自底向上重建（从图像重构3D形状），自上向下识别（基于先验知识构建模型）。</p><p>最后再说一下图像理解和计算机视觉的关系。计算机视觉更强调用计算机实现人的视觉功能，虽然目前研究内容主要强调其与图像理解的结合，但是实际上要用到三个层次的多种技术。这本书中不会去可以区分这两个概念。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>1.2.3 其他相关学科</b></p><p>机器视觉：计算机视觉注重图像工程的高层，而机器视觉更关注图像获取、系统构造和算法实现。</p><p>模式识别：图像即是一种模式，识别是指从客观事实中自动建立符号描述或进行逻辑推理。模式识别就是对客观物体或者过程进行分类、描述的学科。图像理解中要用到模式识别的相关知识我认为是不需要多做解释的。</p><p>人工智能：指由人类用计算机模拟、执行或再生某些与人类智能有关功能的能力和技术。视觉功能是人类智能的一种体现，因此图像理解、计算机视觉和人工智能息息相关。</p><p>计算机图形学：其被称为计算机视觉的“逆问题”，因为视觉从2D图像提取3D信息，而图形学使用3D模型来生成2D场景。（大多时候和图像分析联系更多，用于可视化等）</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>1.2.4 图像理解应用领域</b></p><p>这里就简单列举一下应用领域：工业视觉 、人机交互、视觉导航、虚拟现实、图像自动解释等等。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>1.3 图像理解理论框架</h2><p>这一节主要是比较宏观的方法论，只简单写一些要点吧。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>1.3.1 马尔视觉计算理论</b></p><p>马尔于1982年提出视觉计算理论，勾画了一个理解视觉信息的框架。他的理论指出：要先理解视觉目的再去理解其中细节（个人感觉这句话能应用于各个方面）。其理论的要点如下：</p><ul><li><i>视觉是一个复杂的信息加工过程。</i></li></ul><p>要理解这个复杂过程首先要解决的问题是：如何<b>表达</b>并且<b>加工</b>视觉信息。</p><ul><li><i>视觉信息加工三要素：计算理论、算法实现、硬件实现。</i></li></ul><p>    ①计算理论首先要求视觉问题是可以用计算机计算的（对一问题能在给定输入和有限步内给出输出），然后研究计算的概念以及目的、提出使问题可计算的约束条件；</p><p>    ②算法实现首先要选取一种对加工对象实体的合适表达（输入输出），然后确定响应算法；</p><p>    ③硬件实现通常适应更高的实时性要求，注意不同的硬件一般会有各自相应的算法。</p><ul><li><i>视觉信息的三级内部表达。</i></li></ul><p>    ①<i>基素表达（primal sketch）</i></p><p>一种2-D表达，它是图象特征的集合，描述了物体上属性发生变化的轮廓部分（类似于人观察时往往先注意到变化剧烈的部分，可以简单理解为“轮廓”）。需要注意的是只用基素表达不能保证得到对场景的唯一解释，例如我们对下图最左边的图形可以有右边的多种理解。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-b89fd8b8bc60c2abe62c42513a8baaa8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"504\" data-rawheight=\"137\" class=\"origin_image zh-lightbox-thumb\" width=\"504\" data-original=\"https://pic1.zhimg.com/v2-b89fd8b8bc60c2abe62c42513a8baaa8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;504&#39; height=&#39;137&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"504\" data-rawheight=\"137\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"504\" data-original=\"https://pic1.zhimg.com/v2-b89fd8b8bc60c2abe62c42513a8baaa8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-b89fd8b8bc60c2abe62c42513a8baaa8_b.jpg\"/></figure><p>    ②<i>2.5D表达（2.5-D sketch）</i></p><p>根据一定的采样密度把要表达的目标按照正交投影的原则分解成很多面元，每个面元有一根法线向量表示其取向，构成2.5D图（针图）。它是一种本征图像（同时表达部分物体轮廓信息和以观察者为中心的物体表面取向信息）。下图是一个例子。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-76954b574045b4289043c15fa64f4814_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"227\" data-rawheight=\"218\" class=\"content_image\" width=\"227\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;227&#39; height=&#39;218&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"227\" data-rawheight=\"218\" class=\"content_image lazy\" width=\"227\" data-actualsrc=\"https://pic1.zhimg.com/v2-76954b574045b4289043c15fa64f4814_b.jpg\"/></figure><p>    ③<i>3D表达（3-D representation）</i></p><p>是以物体为中心（即也包括了物体不可见部分）的表达形式 ，在以物体为中心的坐标系中描述3-D物体的形状及其空间组织 。之后在第4章我们会学到一些3D表达的方式。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>以上视觉信息的三级内部表达是一个递进的过程，可以如下所示逐步进行。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-b89c0b98329a9864224fb76fbd27fe43_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"471\" data-rawheight=\"98\" class=\"origin_image zh-lightbox-thumb\" width=\"471\" data-original=\"https://pic4.zhimg.com/v2-b89c0b98329a9864224fb76fbd27fe43_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;471&#39; height=&#39;98&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"471\" data-rawheight=\"98\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"471\" data-original=\"https://pic4.zhimg.com/v2-b89c0b98329a9864224fb76fbd27fe43_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-b89c0b98329a9864224fb76fbd27fe43_b.jpg\"/></figure><p>各个层次的表达方式我们可以总结如下。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-0ffe1b5b957255387fd8f960f25aed5d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"755\" data-rawheight=\"242\" class=\"origin_image zh-lightbox-thumb\" width=\"755\" data-original=\"https://pic2.zhimg.com/v2-0ffe1b5b957255387fd8f960f25aed5d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;755&#39; height=&#39;242&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"755\" data-rawheight=\"242\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"755\" data-original=\"https://pic2.zhimg.com/v2-0ffe1b5b957255387fd8f960f25aed5d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-0ffe1b5b957255387fd8f960f25aed5d_b.jpg\"/></figure><ul><li>视觉信息的处理按照功能模块的形式组织。</li></ul><p>模块化地组织形式有着理论和实验的支持，同时也是对于人类视觉系统的仿生（各模块获取一定的本征视觉信息最终结合）。</p><ul><li>计算理论形式化表示必须考虑约束条件。</li></ul><p>一个“<b>适定问题</b>”其解满足：①存在的； ②唯一的；③连续地依赖于初始数据。</p><p>我们应当明白实际图像采集等过程中原始信息会发生变化：2D图像丢失深度信息，不同视角信息不同，物体遮挡会丢失信息，实际场景中大量复杂因素被综合成了单一的图像像素值，成像等过程会引入畸变和噪声……</p><p>由于以上信息变化的原因，视觉问题作为光学成像过程的逆问题是不适定（欠定、病态）的，基本不可能求解。实际解决视觉问题要根据客观世界的一般特性找出约束条件形成精密假设从而解决适定问题。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>1.3.2 对马尔理论框架的改进</b></p><p>马尔视觉计算理论框架（前面讲的三级表达）的不足：</p><p>    ①框架中输入是被动的 </p><p>    ②框架中加工目的不变，总是恢复场景中物体的位置和形状等 </p><p>    ③框架未足够重视高层知识的指导作用 </p><p>    ④整个框架中信息加工过程基本自下而上单向流动，没有反馈</p><p>相应改进之后的框架如下。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-acaaefe67e37f636c5043172d0d51972_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"536\" data-rawheight=\"163\" class=\"origin_image zh-lightbox-thumb\" width=\"536\" data-original=\"https://pic3.zhimg.com/v2-acaaefe67e37f636c5043172d0d51972_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;536&#39; height=&#39;163&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"536\" data-rawheight=\"163\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"536\" data-original=\"https://pic3.zhimg.com/v2-acaaefe67e37f636c5043172d0d51972_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-acaaefe67e37f636c5043172d0d51972_b.jpg\"/></figure><p>改进后框架增加了图像获取模块、根据目的进行加工过程的决策、应用高层知识指导加工、增加反馈控制。</p><p>（本章后边一些内容对于工程的意义不大，如果有同学对一些理论感兴趣可以自己看书）</p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p><p></p>", 
            "topic": [
                {
                    "tag": "计算机视觉", 
                    "tagLink": "https://api.zhihu.com/topics/19590195"
                }, 
                {
                    "tag": "同时定位和地图构建（SLAM）", 
                    "tagLink": "https://api.zhihu.com/topics/20033502"
                }, 
                {
                    "tag": "图像处理", 
                    "tagLink": "https://api.zhihu.com/topics/19556376"
                }
            ], 
            "comments": [
                {
                    "userName": "小明", 
                    "userLink": "https://www.zhihu.com/people/f9df45f835b010f97d2cb5abce86b441", 
                    "content": "推荐录制视频老哥", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/51099650", 
            "userName": "贤鱼卓君", 
            "userLink": "https://www.zhihu.com/people/0be40edd972271a9c9afb446ee1a566d", 
            "upvote": 8, 
            "title": "Multiple View Geometry in computer vision 学习记录03", 
            "content": "<p>（我发现在知乎里还是有人看我写的东西的，所以我还是好好打一下公式吧）</p><hr/><h2><b>2.4 A hierarchy of transformations</b></h2><p>标题中的关键字是“层级”，因此我们会先见识到多种变换，然后再去了解它们之间的层级关系。书中介绍了多种变换，每种变换都以“群（group）”的方式来定义。</p><ol><li><img src=\"https://www.zhihu.com/equation?tex=GL%28n%29\" alt=\"GL(n)\" eeimg=\"1\"/> ：the real general linear group on n dimensions（实数的、线性的、n*n、可逆）</li><li><img src=\"https://www.zhihu.com/equation?tex=PL%28n%29\" alt=\"PL(n)\" eeimg=\"1\"/> ：projective linear group（书中将这是GL(n)的一个“商群”，个人理解就是除以一个实数的缩放因子）</li><li><img src=\"https://www.zhihu.com/equation?tex=PL%283%29\" alt=\"PL(3)\" eeimg=\"1\"/> ：projective transformations of the plane n =3（我们前面一直讨论的H）</li><li>Affine group：最后一行是 (0,0,1) 的 PL(3)</li><li>Euclidean group：左上角2*2矩阵是正交矩阵的 Affine group</li><li>Oriented Euclidean group：左上角2*2矩阵行列式为1的Euclidean group</li></ol><p>以上我们由一般到特殊感受了一下变换的层级关系，书中讲解的时候则是由特殊到一般进行。书中要我们注意的是：有一些变换并不能构成群！（小白可以百度一下群的定义）</p><p>这里的反例就是透视变换。群要求其上的运算满足封闭性，而多次透视变换后结果并不能由一次透视变换得到，而是变成了投影变换。这点需要注意。</p><ul><li><b>不变量（Invariants）</b></li></ul><p>这里书中提了一下一些变换可以从一些“不变量”的角度描述。例如“欧几里得变换”就是旋转和平移，而“相似”则是旋转平移和各向同性缩放。相较之下，欧几里得变换不会使图像缩放，所以两点之间的距离是不变的，距离则是欧几里得变换中的不变量。另外，角度则是这两种变换中共同的不变量。</p><hr/><h2>2.4.1 Class I: Isometries </h2><p>Isometry是 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7BIR%5E2%7D\" alt=\"\\mathbf{IR^2}\" eeimg=\"1\"/> 平面上的变换，这种变换保持了欧氏距离。这里这个单词就意为“等距”，但我觉的这样直译不是很好我就还是用这个单词来表示这样的变换。Isometry的形式如下：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Bpmatrix%7D+x%27%5C%5C+y%27%5C%5C+1+%5Cend%7Bpmatrix%7D%3D%5Cbegin%7Bbmatrix%7D+%5Cepsilon+cos%5Ctheta+%26+-sin%5Ctheta%26t_x+%5C%5C+%5Cepsilon+sin%5Ctheta+%26+cos%5Ctheta+%26t_y+%5C%5C+0%26+0+%26+1+%5Cend%7Bbmatrix%7D%5Cbegin%7Bpmatrix%7D+x%5C%5C+y%5C%5C+1+%5Cend%7Bpmatrix%7D\" alt=\"\\begin{pmatrix} x&#39;\\\\ y&#39;\\\\ 1 \\end{pmatrix}=\\begin{bmatrix} \\epsilon cos\\theta &amp; -sin\\theta&amp;t_x \\\\ \\epsilon sin\\theta &amp; cos\\theta &amp;t_y \\\\ 0&amp; 0 &amp; 1 \\end{bmatrix}\\begin{pmatrix} x\\\\ y\\\\ 1 \\end{pmatrix}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"/> 取值为+1或者-1.① <img src=\"https://www.zhihu.com/equation?tex=%5Cepsilon%3D1\" alt=\"\\epsilon=1\" eeimg=\"1\"/> 时，Isometry是“保持方向的(orientation-preserving)”，并且是<b>欧式变换（平移+旋转）</b>。② <img src=\"https://www.zhihu.com/equation?tex=%5Cepsilon%3D-1\" alt=\"\\epsilon=-1\" eeimg=\"1\"/> 时，isometry不再是保持方向的，而是翻转方向的（反射变换diag(-1,1,1)就是一个例子）。</p><p>Isometry十分十分重要的原因是它可以描述刚体的运动，这对SLAM而言是十分关键的基础知识所以我会特别关注。上面的代数表达可以改的简洁一些：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7Bx%27%3DH_Ex%3D%7D%5Cbegin%7Bbmatrix%7D+%5Cmathbf+R+%26+%5Cmathbf+t%5C%5C+%5Cmathbf+0%5ET%261+%5Cend%7Bbmatrix%7D%5Cmathbf+x\" alt=\"\\mathbf{x&#39;=H_Ex=}\\begin{bmatrix} \\mathbf R &amp; \\mathbf t\\\\ \\mathbf 0^T&amp;1 \\end{bmatrix}\\mathbf x\" eeimg=\"1\"/> </p><p>矩阵里只有“1”是一个标量。 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7BR%7D\" alt=\"\\mathbf{R}\" eeimg=\"1\"/> 是2*2的旋转矩阵，并且是正交的（ <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7BR%7D%5ET%5Cmathbf%7BR%3DRR%7D%5ET%3D%5Cmathbf+I\" alt=\"\\mathbf{R}^T\\mathbf{R=RR}^T=\\mathbf I\" eeimg=\"1\"/> ） 。<img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf+t\" alt=\"\\mathbf t\" eeimg=\"1\"/> 是平移向量（2维）， <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7B0%7D\" alt=\"\\mathbf{0}\" eeimg=\"1\"/> 就是二维的零向量。值得注意的特殊情况是：t是零向量时只有纯旋转，R是单位矩阵时只有纯平移。</p><p>关于自由度，我觉得大家能看出来这里其主要作用的只有三个量，因此自由度是3.关于这个自由度可以很直观地理解：我们在平面上摆弄一个东西（除了翻转之外）只能决定移动的下一个位置还有旋转多少度。</p><p>还需要提一句，欧式变换也会被成为“位移（displacement）”。</p><ul><li><b>不变量</b></li></ul><p>在Isometry变换之下不变量有三个：两点间<b>长度</b>、两直线间<b>角度</b>、<b>面积</b>。这个很直观也就不多加解释了。</p><ul><li><b>群与方向</b></li></ul><p>之前说过 <img src=\"https://www.zhihu.com/equation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"/> 取值为1时（R的行列式为1），isometry是保持方向的。保持方向的isometry可以构成群，而翻转方向的（ <img src=\"https://www.zhihu.com/equation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"/> 为-1）isometry不能构成群。这点区别在之后的相似和仿射变换中也存在。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>2.4.2 Class II: Similarity transformations (similarity) </h2><p><b><i>注：这就是ORB-SLAM中的Sim3变换</i></b></p><p>相似变化我们可以很快理解，因为它只是isometry的小小改进：<u>相似变换是isometry与isotropic scaling的组合</u>。简单来讲就是可以放大缩小的isometry变换。这一点点小改变是通过添加一个标量“s”完成的，其矩阵表示如下：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Bpmatrix%7D+x%27%5C%5C+y%27%5C%5C+1+%5Cend%7Bpmatrix%7D%3D%5Cbegin%7Bbmatrix%7D+s%2Acos%5Ctheta+%26+-s%2Asin%5Ctheta%26t_x+%5C%5C+s%2A+sin%5Ctheta+%26+s%2Acos%5Ctheta+%26t_y+%5C%5C+0%26+0+%26+1+%5Cend%7Bbmatrix%7D%5Cbegin%7Bpmatrix%7D+x%5C%5C+y%5C%5C+1+%5Cend%7Bpmatrix%7D\" alt=\"\\begin{pmatrix} x&#39;\\\\ y&#39;\\\\ 1 \\end{pmatrix}=\\begin{bmatrix} s*cos\\theta &amp; -s*sin\\theta&amp;t_x \\\\ s* sin\\theta &amp; s*cos\\theta &amp;t_y \\\\ 0&amp; 0 &amp; 1 \\end{bmatrix}\\begin{pmatrix} x\\\\ y\\\\ 1 \\end{pmatrix}\" eeimg=\"1\"/> </p><p>写成更简洁的形式：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7Bx%27%3DH_Sx%3D%7D%5Cbegin%7Bbmatrix%7D+s%5Cmathbf+R+%26+%5Cmathbf+t%5C%5C+%5Cmathbf+0%5ET%261+%5Cend%7Bbmatrix%7D%5Cmathbf+x\" alt=\"\\mathbf{x&#39;=H_Sx=}\\begin{bmatrix} s\\mathbf R &amp; \\mathbf t\\\\ \\mathbf 0^T&amp;1 \\end{bmatrix}\\mathbf x\" eeimg=\"1\"/> </p><p>相似变换的自由度大家应该都能想到是4，因为相较于isometry多加了一个缩放因子。计算这个矩阵和计算isometry一样只需要两个点（算上对应的点一共其实是四个点），原因是similarity和isometry自由度都小于等于4，而一个点就能提供两个等式，所以两个点就足够计算出四个参数了。</p><ul><li><b>不变量</b></li></ul><p>同样我们类比着isometry来学。因为添加了缩放，所以相似变换的不变量会少一点：长度和面积都不再是不变量了，只有角度不变。这点我觉得大家学过相似三角形肯定都懂(笑</p><p>我们可以无耻的强行再找出一点不变量：比如长度和面积虽然会变，但是它们的比例可以在相似中得到保持。</p><ul><li><b>Metric Structure</b></li></ul><p>由于英语和专业知识水平所限我不是很清楚这个小标题是什么意思，直译的话就叫“度量结构”，但我觉得还是记一下英文名好了。书中说“Metric”这个术语会在第十章的重建部分经常提到，所以我们对于这个陌生的概念不用太焦虑。另外，书中说metric structure这个描述意味着这种结构被定义为相似的。我也没有很明白这短短的一个小节，所以对这部分知识我只能说：有缘再见吧~<br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2>2.4.3 Class III: Affine transformations (affinity)<br/></h2><p>仿射变换在相似的基础上更近一步：仿射变换是非奇异线性变换+平移。其矩阵表示如下：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Bpmatrix%7D+x%27%5C%5C+y%27%5C%5C+1+%5Cend%7Bpmatrix%7D%3D%5Cbegin%7Bbmatrix%7D+a_%7B11%7D+%26+a_%7B12%7D%26t_x+%5C%5C+a_%7B21%7D+%26a_%7B22%7D+%26t_y+%5C%5C+0%26+0+%26+1+%5Cend%7Bbmatrix%7D%5Cbegin%7Bpmatrix%7D+x%5C%5C+y%5C%5C+1+%5Cend%7Bpmatrix%7D\" alt=\"\\begin{pmatrix} x&#39;\\\\ y&#39;\\\\ 1 \\end{pmatrix}=\\begin{bmatrix} a_{11} &amp; a_{12}&amp;t_x \\\\ a_{21} &amp;a_{22} &amp;t_y \\\\ 0&amp; 0 &amp; 1 \\end{bmatrix}\\begin{pmatrix} x\\\\ y\\\\ 1 \\end{pmatrix}\" eeimg=\"1\"/> </p><p>写成块形式：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7Bx%27%3DH_Ax%3D%7D%5Cbegin%7Bbmatrix%7D+%5Cmathbf+A+%26+%5Cmathbf+t%5C%5C+%5Cmathbf+0%5ET%261+%5Cend%7Bbmatrix%7D%5Cmathbf+x\" alt=\"\\mathbf{x&#39;=H_Ax=}\\begin{bmatrix} \\mathbf A &amp; \\mathbf t\\\\ \\mathbf 0^T&amp;1 \\end{bmatrix}\\mathbf x\" eeimg=\"1\"/> </p><p>其中A是一个2*2的非奇异矩阵。大家应该能看出来仿射变换的自由度是6，因为放眼望去矩阵里就有六个未知数。为了避免一点误区得说明一下：这里不像原来说点和线的齐次坐标那样只是比例起作用实际自由度要减一，这里不能约！矩阵随便缩放会导致平移向量改变，那这个变化肯定就变了！所以请搞清楚仿射变换的dof是6.明白这点之后我们也自然知道计算这个矩阵只需要三个点（算上变换后的对应点一共六个）。</p><p>以上这种表示并不能帮助我们很好地理解仿射变换。为了理解其核心“A”，我们其实可以做一些分解进而明白小矩阵A其实是“旋转+各向异性缩放”：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7BA%3DR%28%5Ctheta+%29R%28-%5Cphi+%29DR%28%5Cphi+%29%7D\" alt=\"\\mathbf{A=R(\\theta )R(-\\phi )DR(\\phi )}\" eeimg=\"1\"/> </p><p>其中R都表示相应角度的旋转矩阵。D是一个简单的对角矩阵：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7BD%3D%5Cbegin%7Bbmatrix%7D+%5Clambda+_1+%260+%5C%5C+0%26+%5Clambda_2+%5Cend%7Bbmatrix%7D%7D\" alt=\"\\mathbf{D=\\begin{bmatrix} \\lambda _1 &amp;0 \\\\ 0&amp; \\lambda_2 \\end{bmatrix}}\" eeimg=\"1\"/> </p><p>至于这个变化怎么来的可以参考这本书的附录，一般情况我觉得这个不需要深究先记下就好了。根据上面这种分解，我们可以做出如下对A的解读（矩阵一般从右往左看）：<u>①旋转 φ ；②分别在x，y方向缩放λ1 λ2；③旋转回来(-φ)；④再旋转θ。</u><br/></p><p>前面说了A就是“旋转+各向异性缩放”，这里的“各向异性缩放”就是靠前三步来完成的！还是怕和我一样的小白犯一些低级错误，再说一下我个人对各向异性缩放的理解：<br/></p><ol><li>它缩放之前会旋转（<u>虽然之后会转回来但是在转回来之前进行了xy方向不同比例的缩放，所以会有很大影响</u>）</li><li>它缩放的时候xy方向缩放程度可以不同</li></ol><p>这两个骚操作结合起来就产生了各向异性缩放，导致图像变形，而不像相似变换那样形状不变只是整体的放大或者缩小。</p><ul><li><b>不变量</b></li></ul><p>已经解释过由于各向异性缩放，仿射变换不会像相似那样能保持形状不变。个人认为标志性的直观解释就是：一条很长的水平直线可以变成很短的斜着的直线。所以不变量肯定更少了....但是仍然可以强行找出来一些还是不变的东西：</p><ol><li>平行线（大家都是xy方向同步缩放的，转的角度也一样，平行的线怎么搞也不会相交）</li><li>平行线的长度比例（由于“各向异性”，不同方向的线的长度比保持不了，但平行线的长度比还是不可能变的）</li><li>面积比（不同方向的正方形在各向异性的变化下形状可能不一样了，但是由于大家xy方向拉伸的比例一样所以总的面积上的比例是不会变的，实际上大家的面积都缩放了 <img src=\"https://www.zhihu.com/equation?tex=%5Clambda+_1%5Clambda+_2\" alt=\"\\lambda _1\\lambda _2\" eeimg=\"1\"/> （就是A的行列式大小））</li></ol><p>还有需要说明的是仿射变换和isometry一样有保持方向和翻转方向之分，取决于A行列式的正负。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>2.4.4 Class IV: Projective transformations </h2><p>投影变换已经在2.3中定义过了，它是齐次坐标下的非奇异线性变换。这里我们再把它写成类似的块形式：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7Bx%27%3DH_Px%3D%7D%5Cbegin%7Bbmatrix%7D+%5Cmathbf+A+%26+%5Cmathbf+t%5C%5C+%5Cmathbf+v%5ET%26v+%5Cend%7Bbmatrix%7D%5Cmathbf+x\" alt=\"\\mathbf{x&#39;=H_Px=}\\begin{bmatrix} \\mathbf A &amp; \\mathbf t\\\\ \\mathbf v^T&amp;v \\end{bmatrix}\\mathbf x\" eeimg=\"1\"/> </p><p>这里的向量 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7Bv%7D%3D%28v_1%2Cv_2%29%5ET\" alt=\"\\mathbf{v}=(v_1,v_2)^T\" eeimg=\"1\"/> 。其他一些内容我觉得看2.3就够了。然后要注意一点就是在投影空间中投影是否是方向保持的是不可能区分的，这点会在2.6讲。</p><ul><li><b>不变量</b></li></ul><p>仿射中一条线上（或者说平行线间）的长度比是不变的，在投影变换中这一点也不成立了。书中提到的不变量是一条线上长度的交叉比（cross ratio，ratio of ratios），这点会在之后2.5讲。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>2.4.5 Summary and comparison </h2><p>前面我们学了isometry、相似、仿射、投影由易到难四个层次的变换。递进表现为：旋转+平移——缩放——各向异性缩放——与位置相关的缩放。</p><p>这里我再根据书中讲的强调一下后两个。仿射中的各向异性缩放表现为：缩放与位置无关，之和方向有关。投影中的缩放表现为同时和位置、方向有关。对于平面上两个正方形，只要它们的姿态（方向）是一致的，那么仿射变换结果一定相同；但是只要如果不在同一个位置或者不是同一个方向，那么两个正方形的投影变换一定不同。这两者区别的关键在于向量v是否为零，书中通过理想点 <img src=\"https://www.zhihu.com/equation?tex=%28x_1%2Cx_2%2C0%29%5ET\" alt=\"(x_1,x_2,0)^T\" eeimg=\"1\"/> 帮助我们看清这一点区别：</p><p>首先是理想点的仿射变换：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-aa7d8fbcdf81e6b0fc86bf23c70a3a1a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"421\" data-rawheight=\"117\" class=\"origin_image zh-lightbox-thumb\" width=\"421\" data-original=\"https://pic3.zhimg.com/v2-aa7d8fbcdf81e6b0fc86bf23c70a3a1a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;421&#39; height=&#39;117&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"421\" data-rawheight=\"117\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"421\" data-original=\"https://pic3.zhimg.com/v2-aa7d8fbcdf81e6b0fc86bf23c70a3a1a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-aa7d8fbcdf81e6b0fc86bf23c70a3a1a_b.jpg\"/></figure><p>其次是理想点的投影变换：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-0f3b9b539a56529cbc3ca74e41c3b316_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"442\" data-rawheight=\"103\" class=\"origin_image zh-lightbox-thumb\" width=\"442\" data-original=\"https://pic3.zhimg.com/v2-0f3b9b539a56529cbc3ca74e41c3b316_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;442&#39; height=&#39;103&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"442\" data-rawheight=\"103\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"442\" data-original=\"https://pic3.zhimg.com/v2-0f3b9b539a56529cbc3ca74e41c3b316_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-0f3b9b539a56529cbc3ca74e41c3b316_b.jpg\"/></figure><p>​可以看到，投影变换能把理想点映射到一个有限点，而这时仿射变换做不到的。这一特点使得投影变换可以对“灭点”建模。（和我之前提的素描透视中的灭点是同一个东西，之后遇到再讨论）</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>2.4.6 Decomposition of a projective transformation </h2><p>很有趣的一点是，投影变换可以被拆解成一些列前面提到的变换，具体如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-5e76ad9a6d509731c5280c44029bce4b_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"648\" data-rawheight=\"90\" class=\"origin_image zh-lightbox-thumb\" width=\"648\" data-original=\"https://pic4.zhimg.com/v2-5e76ad9a6d509731c5280c44029bce4b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;648&#39; height=&#39;90&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"648\" data-rawheight=\"90\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"648\" data-original=\"https://pic4.zhimg.com/v2-5e76ad9a6d509731c5280c44029bce4b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-5e76ad9a6d509731c5280c44029bce4b_b.png\"/></figure><p>​大家计算一下就能得到 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbf%7BA%3D%7Ds%5Cmathbf%7BRK%2Btv%7D%5ET\" alt=\"\\mathbf{A=}s\\mathbf{RK+tv}^T\" eeimg=\"1\"/>  ，K是上三角矩阵并且行列式归一为1.注意当v不为0是这样分解才是有效的，且这种分解在s大于0时唯一。</p><p>其实这个分解的顺序还能反过来如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-40ca65ae5ed48bee5a0d4ac94602f847_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"541\" data-rawheight=\"88\" class=\"origin_image zh-lightbox-thumb\" width=\"541\" data-original=\"https://pic4.zhimg.com/v2-40ca65ae5ed48bee5a0d4ac94602f847_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;541&#39; height=&#39;88&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"541\" data-rawheight=\"88\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"541\" data-original=\"https://pic4.zhimg.com/v2-40ca65ae5ed48bee5a0d4ac94602f847_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-40ca65ae5ed48bee5a0d4ac94602f847_b.png\"/></figure><p>​需要注意的是大家可以把这个结果写出了，和前一种分解的结果是不一样的，所以这里符号一样但实际值不同。</p><p>讲这个分解方法的原因是，这样分解可以在只需要确定部分变换时使用。这部分会在2.7讨论。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>2.4.7 The number of invariants </h2><p>这一节简单总结一下各变换的不变量，这一节有一点“玄”，大家主要看书，我在这里说一下自己的理解。说到不变量我们自然会想每种变换有多少个不变量。相信大家也有从前面学到这里也有体会，一个不变量的函数肯定也是不变量。比如欧式变换是最特殊的变换，只有旋转和平移所以长度是不变量，那么它肯定能像“更高等级的变换”一样保持住长度比或者交叉比什么的，因为后者这些都是长度的函数而已。所以我们讨论不变量数量的话应该很严格地讨论那些最本质、在函数上独立的不变量。</p><p>书中通过一句话就简洁地引出了一条很本质的结论：</p><p><u>函数独立的不变量的数量大于等于结构的自由度减去变换的自由度。</u></p><p>我只能说因为水平和精力所限没有搞懂这句话。不过这句话的思考角度大概就是一个结构提供的已知变量数去除变换需要确定的变量数之后剩下的差不多就是不变量的数。至于为什么是大于等于的且这个结构怎么去定义我确实无法回答，书中也没有论证，不过我斗胆觉得从工程角度来说这个问题应该问题不大所以我们不用太过关心。书中的举了线段的例子也就套了一下这个结论我就不赘述了，感觉意义不大。</p><p>最后就放上各种变换不变的几何性质总结，注意从上到下是由一般到特殊，随着变换的dof减小，其不变量是越来越多的。<br/>​</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-17467e88cf34f65b5a99202f7ea61534_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"916\" data-rawheight=\"680\" class=\"origin_image zh-lightbox-thumb\" width=\"916\" data-original=\"https://pic1.zhimg.com/v2-17467e88cf34f65b5a99202f7ea61534_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;916&#39; height=&#39;680&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"916\" data-rawheight=\"680\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"916\" data-original=\"https://pic1.zhimg.com/v2-17467e88cf34f65b5a99202f7ea61534_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-17467e88cf34f65b5a99202f7ea61534_b.jpg\"/></figure><p>​</p><hr/><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3c40829a650f96ab57be6975d78f8601_b.jpg\" data-caption=\"\" data-size=\"normal\" class=\"content_image\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;0&#39; height=&#39;0&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" class=\"content_image lazy\" data-actualsrc=\"https://pic2.zhimg.com/v2-3c40829a650f96ab57be6975d78f8601_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>​</p><p class=\"ztext-empty-paragraph\"><br/></p><p>​<br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>​<br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>​            </p><p></p>", 
            "topic": [
                {
                    "tag": "计算机视觉", 
                    "tagLink": "https://api.zhihu.com/topics/19590195"
                }, 
                {
                    "tag": "几何变换", 
                    "tagLink": "https://api.zhihu.com/topics/20205629"
                }, 
                {
                    "tag": "同时定位和地图构建（SLAM）", 
                    "tagLink": "https://api.zhihu.com/topics/20033502"
                }
            ], 
            "comments": [
                {
                    "userName": "hh慧", 
                    "userLink": "https://www.zhihu.com/people/0a5328103aba71353b6918e1a463ab9c", 
                    "content": "还没看完就给你赞了 好好写 加油", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "贤鱼卓君", 
                            "userLink": "https://www.zhihu.com/people/0be40edd972271a9c9afb446ee1a566d", 
                            "content": "不过老师让我换书了😂之后先换另一本书整理", 
                            "likes": 0, 
                            "replyToAuthor": "hh慧"
                        }
                    ]
                }, 
                {
                    "userName": "王宇森", 
                    "userLink": "https://www.zhihu.com/people/02332a0c35aaaa42d8d5cfcb0e51c350", 
                    "content": "老哥看完了吗", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "贤鱼卓君", 
                            "userLink": "https://www.zhihu.com/people/0be40edd972271a9c9afb446ee1a566d", 
                            "content": "没，感觉没必要看完，还是先专注项目需要了再学习。这样效果比较好[微笑]", 
                            "likes": 0, 
                            "replyToAuthor": "王宇森"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/50943177", 
            "userName": "贤鱼卓君", 
            "userLink": "https://www.zhihu.com/people/0be40edd972271a9c9afb446ee1a566d", 
            "upvote": 1, 
            "title": "Multiple View Geometry in computer vision 学习记录01", 
            "content": "<p> （原文在我的csdn博客中，我以后都会搬运一下hh。公式我懒得再重新打一遍了，默认成图片导致阅读有些不便还请谅解）</p><p>       我开始了本科毕业设计，要做V-SLAM方面内容。目前可以说几乎没有基础，除了图像处理和机器学习方面还有印象以外，线性代数都已经忘光了，计算机视觉也没有学过。在看《Multiple View Geometry in computer vision》这本书之前我在B站快速看了高博士的《视觉SLAM十四讲》视频，对于这方面内容有了基本了解，但是细节方面还没有深入。因此我找到了这本书想认真打好基础，在博客里记录一下自己的学习情况，和能看到我博客的小白们一同进步，开始CV的打怪升级之旅hhh</p><p>       需要说明的是：由于本人知识水平所限只能提供个人对本书的粗浅理解，其中肯定有不严谨甚至错误的地方，还请看到的朋友们指正。如果有幸被其他同学觉得我写的这些文字有用，我也建议只把我写的内容当做读这本书的一个辅助。之后我可能会在别的地方同步发出自己的学习记录，希望能和更多的同学一同进步~</p><p>       那就从这里正式开始吧！</p><p class=\"ztext-empty-paragraph\"><br/></p><hr/><h2>2.1 Planar Geometry</h2><p>这一节大致讲了平面几何的研究方法。书中采用的方法是“geometrics + algebraic methods”。作为研究来说还是代数方法更加重要且实际，代数方法中<u>几何实体通过坐标以及代数式题进行描述</u>。本书主要研究计算以及算法方面的问题，因此代数知识显得尤为重要，不过我个人不打算专门补线性代数什么的，为了提高效率还是在看这本书的过程中按需补补代数吧。</p><p class=\"ztext-empty-paragraph\"><br/></p><hr/><h2>2.2 The 2D projective plane</h2><p>我们都知道平面中的点可以由坐标对(x,y)表示，其属于IR2这个向量空间。我们用 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-2b052ea33bd55c30423af9d5e938d618_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"87\" data-rawheight=\"20\" class=\"content_image\" width=\"87\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;87&#39; height=&#39;20&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"87\" data-rawheight=\"20\" class=\"content_image lazy\" width=\"87\" data-actualsrc=\"https://pic1.zhimg.com/v2-2b052ea33bd55c30423af9d5e938d618_b.jpg\"/></figure><p> 表示平面上一点，这里粗体的x代表着它是向量，而上标T指明了这是列向量而不是行向量，这种表示方法我们要明白其含义。我们应该知道的是，几何实体在代数方法中我们都用<b>列向量</b>表示！</p><p>这一节我们将学习到平面中几何实体的齐次表示方法（<b>homogeneous notation</b>）。</p><h2>2.2.1 Points and Lines</h2><ul><li><b>直线的齐次表示</b></li></ul><p>我们都学习过直线可以用ax+by+c=0来表示，因此我们显然可以用向量 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-b7da115293eecabe83d4c44a2499b91f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"64\" data-rawheight=\"20\" class=\"content_image\" width=\"64\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;64&#39; height=&#39;20&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"64\" data-rawheight=\"20\" class=\"content_image lazy\" width=\"64\" data-actualsrc=\"https://pic4.zhimg.com/v2-b7da115293eecabe83d4c44a2499b91f_b.jpg\"/></figure><p> 来表示一条直线。应该注意到这并不是“一对一”的映射关系，因为用一个不为零的常数k可以进行缩放，即 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-5ab8937d509051c73eb17ce35e8f7a56_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"94\" data-rawheight=\"20\" class=\"content_image\" width=\"94\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;94&#39; height=&#39;20&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"94\" data-rawheight=\"20\" class=\"content_image lazy\" width=\"94\" data-actualsrc=\"https://pic3.zhimg.com/v2-5ab8937d509051c73eb17ce35e8f7a56_b.jpg\"/></figure><p> 是同一条直线。我们把同一条线的不同表示打包在一起构成一类，称其为“<b>齐次向量</b>”。</p><p>还有一点要注意： </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-41c2aba41c1defe692dda3a7558f9db5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"66\" data-rawheight=\"20\" class=\"content_image\" width=\"66\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;66&#39; height=&#39;20&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"66\" data-rawheight=\"20\" class=\"content_image lazy\" width=\"66\" data-actualsrc=\"https://pic2.zhimg.com/v2-41c2aba41c1defe692dda3a7558f9db5_b.jpg\"/></figure><p>  不对应任何一条直线！！！</p><ul><li><b>点的齐次表示</b></li></ul><p>点(x,y)在线ax+by+c=0上的条件就是 ax+by+c=0，代数的方法使用内积来表达这一关系。为了让点和线可以做内积我们把点也写成三维向量： </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-68bff4fddbf458bbeb89718aa8a960ec_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"68\" data-rawheight=\"20\" class=\"content_image\" width=\"68\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;68&#39; height=&#39;20&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"68\" data-rawheight=\"20\" class=\"content_image lazy\" width=\"68\" data-actualsrc=\"https://pic1.zhimg.com/v2-68bff4fddbf458bbeb89718aa8a960ec_b.jpg\"/></figure><p> ，这就是点的齐次表示。 要说明的是这也不是一对一的，可以用非零k去缩放。</p><p>从平面点的齐次表示的导出过程可以明白，第三个元素不可能是0！！！</p><ul><li><b>表示点x在线l上</b></li></ul><p>如前面解释的那样，表示为：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-d0de503aa7c2129e11bbc97de964fb6a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"60\" data-rawheight=\"17\" class=\"content_image\" width=\"60\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;60&#39; height=&#39;17&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"60\" data-rawheight=\"17\" class=\"content_image lazy\" width=\"60\" data-actualsrc=\"https://pic3.zhimg.com/v2-d0de503aa7c2129e11bbc97de964fb6a_b.jpg\"/></figure><p> 或者 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-cd125ebb6236abd06af2717a1a5d2a79_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"60\" data-rawheight=\"17\" class=\"content_image\" width=\"60\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;60&#39; height=&#39;17&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"60\" data-rawheight=\"17\" class=\"content_image lazy\" width=\"60\" data-actualsrc=\"https://pic2.zhimg.com/v2-cd125ebb6236abd06af2717a1a5d2a79_b.jpg\"/></figure><p> ，为了方便可以用点来表示点乘 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-27452bf0862b1eeca802a2171b68ca37_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"55\" data-rawheight=\"13\" class=\"content_image\" width=\"55\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;55&#39; height=&#39;13&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"55\" data-rawheight=\"13\" class=\"content_image lazy\" width=\"55\" data-actualsrc=\"https://pic4.zhimg.com/v2-27452bf0862b1eeca802a2171b68ca37_b.jpg\"/></figure><ul><li><b>自由度 Degrees of freedom(dof)</b></li></ul><p>自由度最简单的理解就是确定一个东西而真实需要的参数的个数。例如确定一条平面中直线我们虽然写成ax+by+c=0，但是其实可以用abc中非零的一个作为分母去归一化，实际只要两个参数，因此一条平面中直线的自由度是2不是3.</p><ul><li><b>表示线的交点</b></li></ul><p>有一个公式可以导出这一条和下一条。 The triple scalar product identity： </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-615ec0782b02847d30c77bd3121d54d0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"199\" data-rawheight=\"18\" class=\"content_image\" width=\"199\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;199&#39; height=&#39;18&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"199\" data-rawheight=\"18\" class=\"content_image lazy\" width=\"199\" data-actualsrc=\"https://pic1.zhimg.com/v2-615ec0782b02847d30c77bd3121d54d0_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>这个等式暗示着 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-fd386a2f9144f1634183f93c8c034c66_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"45\" data-rawheight=\"14\" class=\"content_image\" width=\"45\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;45&#39; height=&#39;14&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"45\" data-rawheight=\"14\" class=\"content_image lazy\" width=\"45\" data-actualsrc=\"https://pic3.zhimg.com/v2-fd386a2f9144f1634183f93c8c034c66_b.jpg\"/></figure><p> 这个东西和这两条线点乘一定是0，那不就和一个在这两条线上的点的效果一样吗？所以 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-fd386a2f9144f1634183f93c8c034c66_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"45\" data-rawheight=\"14\" class=\"content_image\" width=\"45\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;45&#39; height=&#39;14&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"45\" data-rawheight=\"14\" class=\"content_image lazy\" width=\"45\" data-actualsrc=\"https://pic3.zhimg.com/v2-fd386a2f9144f1634183f93c8c034c66_b.jpg\"/></figure><p> 就表示这两条线的交点。</p><ul><li><b>表示过两点的线</b> </li></ul><p>还是上面那个公式但是点和线角色互换：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-558fa08bf84df419336ad310fc6d9dee_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"219\" data-rawheight=\"18\" class=\"content_image\" width=\"219\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;219&#39; height=&#39;18&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"219\" data-rawheight=\"18\" class=\"content_image lazy\" width=\"219\" data-actualsrc=\"https://pic3.zhimg.com/v2-558fa08bf84df419336ad310fc6d9dee_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>同理 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b13657bd3062c418fcf87fcd08e5fd99_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"48\" data-rawheight=\"15\" class=\"content_image\" width=\"48\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;48&#39; height=&#39;15&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"48\" data-rawheight=\"15\" class=\"content_image lazy\" width=\"48\" data-actualsrc=\"https://pic2.zhimg.com/v2-b13657bd3062c418fcf87fcd08e5fd99_b.jpg\"/></figure><p> 显然就表示过这两点的直线。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>2.2.2 Ideal Points &amp; The Line at Infinity</h2><ul><li><b>平行线交点</b></li></ul><p>这个小标题有点不可思议，但是有其合理之处：我们上一节中学习了用叉乘表示两直线交点，那两条平行直线的向量叉乘得到的不就是其交点吗？两条直线 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-bc2e3b501de3766c873f0d9c1159d0cf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"95\" data-rawheight=\"20\" class=\"content_image\" width=\"95\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;95&#39; height=&#39;20&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"95\" data-rawheight=\"20\" class=\"content_image lazy\" width=\"95\" data-actualsrc=\"https://pic4.zhimg.com/v2-bc2e3b501de3766c873f0d9c1159d0cf_b.jpg\"/></figure><p> 和 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-06ce167179bd0124895255cd8319ec9d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"103\" data-rawheight=\"20\" class=\"content_image\" width=\"103\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;103&#39; height=&#39;20&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"103\" data-rawheight=\"20\" class=\"content_image lazy\" width=\"103\" data-actualsrc=\"https://pic2.zhimg.com/v2-06ce167179bd0124895255cd8319ec9d_b.jpg\"/></figure><p> 叉乘结果是 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-69373886886b60c52586b6f5c15b6a5c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"330\" data-rawheight=\"18\" class=\"content_image\" width=\"330\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;330&#39; height=&#39;18&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"330\" data-rawheight=\"18\" class=\"content_image lazy\" width=\"330\" data-actualsrc=\"https://pic1.zhimg.com/v2-69373886886b60c52586b6f5c15b6a5c_b.jpg\"/></figure><p>。看到这就放心了，平行线相交这种 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-f93c33d4de12c2c6b26810ba4514db54_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"25\" data-rawheight=\"17\" class=\"content_image\" width=\"25\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;25&#39; height=&#39;17&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"25\" data-rawheight=\"17\" class=\"content_image lazy\" width=\"25\" data-actualsrc=\"https://pic1.zhimg.com/v2-f93c33d4de12c2c6b26810ba4514db54_b.jpg\"/></figure><p> 中无意义的东西得到的果然也是“无意义”的点（因为第三个元素为0）。</p><p>我们也可以强行解读一下。为了得到 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-f93c33d4de12c2c6b26810ba4514db54_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"25\" data-rawheight=\"17\" class=\"content_image\" width=\"25\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;25&#39; height=&#39;17&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"25\" data-rawheight=\"17\" class=\"content_image lazy\" width=\"25\" data-actualsrc=\"https://pic1.zhimg.com/v2-f93c33d4de12c2c6b26810ba4514db54_b.jpg\"/></figure><p> 中点的坐标，我们会把点的齐次表示都去除以第三个元素来归一：(b/0, -a/0)。这表示这个平行线交点的坐标是无限大的，契合了那句话：<u>两平行线在无限远处相交</u>。</p><ul><li><b>理想点 &amp; 无限远直线</b></li></ul><p>上面的平行线交点（向量第三元素是0的点）就是<b>理想点</b>。我们定义一个无限远直线 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-31a6d6eaa8ccebed639acbc00c55c7c2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"111\" data-rawheight=\"20\" class=\"content_image\" width=\"111\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;111&#39; height=&#39;20&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"111\" data-rawheight=\"20\" class=\"content_image lazy\" width=\"111\" data-actualsrc=\"https://pic3.zhimg.com/v2-31a6d6eaa8ccebed639acbc00c55c7c2_b.jpg\"/></figure><p> ，可以验证任何一个理想点都在这条线上。</p><p>我们中学应该就学过 (b,-a) 是和直线 ax+by+c=0 的法向量相互垂直的向量。正巧可以看到  </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-bc2e3b501de3766c873f0d9c1159d0cf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"95\" data-rawheight=\"20\" class=\"content_image\" width=\"95\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;95&#39; height=&#39;20&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"95\" data-rawheight=\"20\" class=\"content_image lazy\" width=\"95\" data-actualsrc=\"https://pic4.zhimg.com/v2-bc2e3b501de3766c873f0d9c1159d0cf_b.jpg\"/></figure><p> 和 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-06ce167179bd0124895255cd8319ec9d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"103\" data-rawheight=\"20\" class=\"content_image\" width=\"103\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;103&#39; height=&#39;20&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"103\" data-rawheight=\"20\" class=\"content_image lazy\" width=\"103\" data-actualsrc=\"https://pic2.zhimg.com/v2-06ce167179bd0124895255cd8319ec9d_b.jpg\"/></figure><p> 这两平行线交点（也就是这两条线各自和 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-1ac8c9f12ce20cfed15c54a6be1a982c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"18\" data-rawheight=\"15\" class=\"content_image\" width=\"18\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;18&#39; height=&#39;15&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"18\" data-rawheight=\"15\" class=\"content_image lazy\" width=\"18\" data-actualsrc=\"https://pic1.zhimg.com/v2-1ac8c9f12ce20cfed15c54a6be1a982c_b.jpg\"/></figure><p> 的交点，也就是一个理想点）的前两个元素恰好构成 (b, -a)，所以：<u>直线和无限远直线的交点可以表示该直线的方向</u>。<u>无限远直线也就可以理解成所有 </u></p><u><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-f93c33d4de12c2c6b26810ba4514db54_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"25\" data-rawheight=\"17\" class=\"content_image\" width=\"25\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;25&#39; height=&#39;17&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"25\" data-rawheight=\"17\" class=\"content_image lazy\" width=\"25\" data-actualsrc=\"https://pic1.zhimg.com/v2-f93c33d4de12c2c6b26810ba4514db54_b.jpg\"/></figure></u><p><u> 中直线的方向的集合</u>。</p><p>还有一点概念化的东西：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-f93c33d4de12c2c6b26810ba4514db54_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"25\" data-rawheight=\"17\" class=\"content_image\" width=\"25\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;25&#39; height=&#39;17&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"25\" data-rawheight=\"17\" class=\"content_image lazy\" width=\"25\" data-actualsrc=\"https://pic1.zhimg.com/v2-f93c33d4de12c2c6b26810ba4514db54_b.jpg\"/></figure><p> 中添加了理想点之后就变成了 <b>投影空间</b> </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-26df7ca3a3a006f4c8b8cc7ef2671e70_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"24\" data-rawheight=\"17\" class=\"content_image\" width=\"24\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;24&#39; height=&#39;17&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"24\" data-rawheight=\"17\" class=\"content_image lazy\" width=\"24\" data-actualsrc=\"https://pic1.zhimg.com/v2-26df7ca3a3a006f4c8b8cc7ef2671e70_b.jpg\"/></figure><p>。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-26df7ca3a3a006f4c8b8cc7ef2671e70_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"24\" data-rawheight=\"17\" class=\"content_image\" width=\"24\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;24&#39; height=&#39;17&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"24\" data-rawheight=\"17\" class=\"content_image lazy\" width=\"24\" data-actualsrc=\"https://pic1.zhimg.com/v2-26df7ca3a3a006f4c8b8cc7ef2671e70_b.jpg\"/></figure><p>很厉害的是：其中任意两直线都能相交，任意两点都能过一条直线。</p><ul><li><b>投影平面模型</b></li></ul><p>这一部分我没有看很明白，可以慢慢理解下图（在三维的空间中去解读投影空间）。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-cffb5dc402ea20b0df96f64ee45f5a25_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1006\" data-rawheight=\"464\" class=\"origin_image zh-lightbox-thumb\" width=\"1006\" data-original=\"https://pic2.zhimg.com/v2-cffb5dc402ea20b0df96f64ee45f5a25_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1006&#39; height=&#39;464&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1006\" data-rawheight=\"464\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1006\" data-original=\"https://pic2.zhimg.com/v2-cffb5dc402ea20b0df96f64ee45f5a25_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-cffb5dc402ea20b0df96f64ee45f5a25_b.jpg\"/></figure><p>​<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3c40829a650f96ab57be6975d78f8601_b.jpg\" data-caption=\"\" data-size=\"normal\" class=\"content_image\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;0&#39; height=&#39;0&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" class=\"content_image lazy\" data-actualsrc=\"https://pic2.zhimg.com/v2-3c40829a650f96ab57be6975d78f8601_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><ul><li><b>对偶原则</b></li></ul><p>细心的同学能发现前面导出两直线交点和过两点直线的代数表达时其实利用的同一个等式，只是把线的向量换成了点的向量。这就是对偶原则：<u>任何二维投影几何的定理可以通过互换点、线的角色而得到其对偶定理</u>（并且这对定理只需要证明一次即可）。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>2.2.3 Conics &amp; Dual Conics</h2><p>中学学习的一般圆锥曲线代数定义成：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-fc78309e33bc155f3335b464a1aee160_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"270\" data-rawheight=\"20\" class=\"content_image\" width=\"270\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;270&#39; height=&#39;20&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"270\" data-rawheight=\"20\" class=\"content_image lazy\" width=\"270\" data-actualsrc=\"https://pic1.zhimg.com/v2-fc78309e33bc155f3335b464a1aee160_b.jpg\"/></figure><p> 。通过 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-4946d3a09c94adfbf74265f245fe3f80_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"171\" data-rawheight=\"18\" class=\"content_image\" width=\"171\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;171&#39; height=&#39;18&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"171\" data-rawheight=\"18\" class=\"content_image lazy\" width=\"171\" data-actualsrc=\"https://pic1.zhimg.com/v2-4946d3a09c94adfbf74265f245fe3f80_b.jpg\"/></figure><p> 得到齐次表达式：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-315f65c84e8712733dd4b03d068fdf0f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"375\" data-rawheight=\"20\" class=\"content_image\" width=\"375\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;375&#39; height=&#39;20&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"375\" data-rawheight=\"20\" class=\"content_image lazy\" width=\"375\" data-actualsrc=\"https://pic4.zhimg.com/v2-315f65c84e8712733dd4b03d068fdf0f_b.jpg\"/></figure><p> ，写成矩阵形式：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-2bd61bea5c4e7ef15c01b36b30eb3bff_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"80\" data-rawheight=\"17\" class=\"content_image\" width=\"80\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;80&#39; height=&#39;17&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"80\" data-rawheight=\"17\" class=\"content_image lazy\" width=\"80\" data-actualsrc=\"https://pic4.zhimg.com/v2-2bd61bea5c4e7ef15c01b36b30eb3bff_b.jpg\"/></figure><p> 。圆锥曲线的系数矩阵如下：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-6ab782df29c0a12fb0b357cefb20f927_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"168\" data-rawheight=\"67\" class=\"content_image\" width=\"168\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;168&#39; height=&#39;67&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"168\" data-rawheight=\"67\" class=\"content_image lazy\" width=\"168\" data-actualsrc=\"https://pic4.zhimg.com/v2-6ab782df29c0a12fb0b357cefb20f927_b.jpg\"/></figure><p> （显然这是对称矩阵 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-afbeaf6b57510db58b9e9a550f278ad4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"62\" data-rawheight=\"16\" class=\"content_image\" width=\"62\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;62&#39; height=&#39;16&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"62\" data-rawheight=\"16\" class=\"content_image lazy\" width=\"62\" data-actualsrc=\"https://pic1.zhimg.com/v2-afbeaf6b57510db58b9e9a550f278ad4_b.jpg\"/></figure><p>）</p><p>值得注意的是：类似的，决定圆锥曲线的是六个参数的比例，所以<u>圆锥曲线的dof是5</u>.</p><ul><li><b>五点决定一圆锥曲线</b></li></ul><p>由于圆锥曲线自由度是5，所以用五个点决定它十分自然。当然这个方程组可以写成矩阵的形式来解。</p><ul><li><b>圆锥曲线的切线</b></li></ul><p>直线l与圆锥曲线C相切于点x代数上满足：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-e1c3f644928be813b0030fafa834b1d9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"56\" data-rawheight=\"14\" class=\"content_image\" width=\"56\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;56&#39; height=&#39;14&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"56\" data-rawheight=\"14\" class=\"content_image lazy\" width=\"56\" data-actualsrc=\"https://pic2.zhimg.com/v2-e1c3f644928be813b0030fafa834b1d9_b.jpg\"/></figure><p> 。</p><p>很简单的可以看出如果曲线上一点x满足上式，那么这点一定也在直线l上。接下来我们只需要证明直线l和C只有这一个交点x即可。我们用反证法。假设有另一个点y也是直线l和曲线C的交点（</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-cf6c880f5d7607a43b0a4a89c0f0b3d6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"148\" data-rawheight=\"20\" class=\"content_image\" width=\"148\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;148&#39; height=&#39;20&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"148\" data-rawheight=\"20\" class=\"content_image lazy\" width=\"148\" data-actualsrc=\"https://pic3.zhimg.com/v2-cf6c880f5d7607a43b0a4a89c0f0b3d6_b.jpg\"/></figure><p>）。把 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-e1c3f644928be813b0030fafa834b1d9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"56\" data-rawheight=\"14\" class=\"content_image\" width=\"56\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;56&#39; height=&#39;14&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"56\" data-rawheight=\"14\" class=\"content_image lazy\" width=\"56\" data-actualsrc=\"https://pic2.zhimg.com/v2-e1c3f644928be813b0030fafa834b1d9_b.jpg\"/></figure><p> 带入得到 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-0d59fb20ba7fc4a7eedb3d1380ec7118_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"80\" data-rawheight=\"19\" class=\"content_image\" width=\"80\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;80&#39; height=&#39;19&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"80\" data-rawheight=\"19\" class=\"content_image lazy\" width=\"80\" data-actualsrc=\"https://pic1.zhimg.com/v2-0d59fb20ba7fc4a7eedb3d1380ec7118_b.jpg\"/></figure><p> （注意C是对称矩阵）。其实已经证明完了，因为通过上面几个等式可以很容易证明 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-b5f2096830bdbb9079bbd60b1edb7d04_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"196\" data-rawheight=\"21\" class=\"content_image\" width=\"196\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;196&#39; height=&#39;21&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"196\" data-rawheight=\"21\" class=\"content_image lazy\" width=\"196\" data-actualsrc=\"https://pic1.zhimg.com/v2-b5f2096830bdbb9079bbd60b1edb7d04_b.jpg\"/></figure><p> （展开就行）。最后得到这个式子意味着整个直线都在曲线上，显然不成立，这说明假设错误，所以证明得到只有一个切点x是直线和曲线交点。</p><ul><li><b>对偶圆锥曲线</b></li></ul><p>上面使用点定义的圆锥曲线，根据对偶定理可得用线确定的圆锥曲线，也就是对偶圆锥曲线。其系数矩阵记为</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-644b6d7627203aa38b70f20580a170a1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"20\" data-rawheight=\"13\" class=\"content_image\" width=\"20\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;20&#39; height=&#39;13&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"20\" data-rawheight=\"13\" class=\"content_image lazy\" width=\"20\" data-actualsrc=\"https://pic2.zhimg.com/v2-644b6d7627203aa38b70f20580a170a1_b.jpg\"/></figure><p>，也是对称的（如果它是非奇异的那么它其实就是C的逆）。对偶圆锥曲线满足 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-17f180fe3d6f04679b60059ea166227c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"77\" data-rawheight=\"17\" class=\"content_image\" width=\"77\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;77&#39; height=&#39;17&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"77\" data-rawheight=\"17\" class=\"content_image lazy\" width=\"77\" data-actualsrc=\"https://pic1.zhimg.com/v2-17f180fe3d6f04679b60059ea166227c_b.jpg\"/></figure><p> ，满足这一等式的直线l都和圆锥曲线相切。</p><p>同样根据对偶，切线切点满足 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-17cc1348dd2eb8f89eb4ceff8e693c27_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"63\" data-rawheight=\"14\" class=\"content_image\" width=\"63\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;63&#39; height=&#39;14&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"63\" data-rawheight=\"14\" class=\"content_image lazy\" width=\"63\" data-actualsrc=\"https://pic4.zhimg.com/v2-17cc1348dd2eb8f89eb4ceff8e693c27_b.jpg\"/></figure><p>。需要说明的是，这次对偶定理只有在C是满秩的时候可以直接得到这个结果，在这个条件下C可逆，</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-e5d210161ed2e969d43c373fe9d933d6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"77\" data-rawheight=\"16\" class=\"content_image\" width=\"77\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;77&#39; height=&#39;16&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"77\" data-rawheight=\"16\" class=\"content_image lazy\" width=\"77\" data-actualsrc=\"https://pic3.zhimg.com/v2-e5d210161ed2e969d43c373fe9d933d6_b.jpg\"/></figure><p> 。</p><ul><li><b>退化的圆锥曲线</b></li></ul><p>只要系数矩阵C不是满秩，那圆锥曲线就是退化的。下面一个例子，其中曲线仅由两条直线得到，所以秩是2。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ad8f40f69d860bf15fde6277fad85d9f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"126\" data-rawheight=\"18\" class=\"content_image\" width=\"126\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;126&#39; height=&#39;18&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"126\" data-rawheight=\"18\" class=\"content_image lazy\" width=\"126\" data-actualsrc=\"https://pic4.zhimg.com/v2-ad8f40f69d860bf15fde6277fad85d9f_b.jpg\"/></figure><p>，其中l和m是两直线。可以验证l和m上的点都满足</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-2bd61bea5c4e7ef15c01b36b30eb3bff_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"80\" data-rawheight=\"17\" class=\"content_image\" width=\"80\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;80&#39; height=&#39;17&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"80\" data-rawheight=\"17\" class=\"content_image lazy\" width=\"80\" data-actualsrc=\"https://pic4.zhimg.com/v2-2bd61bea5c4e7ef15c01b36b30eb3bff_b.jpg\"/></figure><p>。另外，零向量就是l和m的交点。</p><p>这个C同样有对偶的形式：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-371800091fae667d8fafe36114876e7a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"131\" data-rawheight=\"20\" class=\"content_image\" width=\"131\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;131&#39; height=&#39;20&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"131\" data-rawheight=\"20\" class=\"content_image lazy\" width=\"131\" data-actualsrc=\"https://pic3.zhimg.com/v2-371800091fae667d8fafe36114876e7a_b.jpg\"/></figure><p>。需要注意的是这里矩阵不再是可逆的：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-d08e167b8cd37aec325ed4517ebb632b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"77\" data-rawheight=\"20\" class=\"content_image\" width=\"77\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;77&#39; height=&#39;20&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"77\" data-rawheight=\"20\" class=\"content_image lazy\" width=\"77\" data-actualsrc=\"https://pic4.zhimg.com/v2-d08e167b8cd37aec325ed4517ebb632b_b.jpg\"/></figure><p>。</p>", 
            "topic": [
                {
                    "tag": "计算机视觉", 
                    "tagLink": "https://api.zhihu.com/topics/19590195"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/51021652", 
            "userName": "贤鱼卓君", 
            "userLink": "https://www.zhihu.com/people/0be40edd972271a9c9afb446ee1a566d", 
            "upvote": 0, 
            "title": "Multiple View Geometry in computer vision 学习记录02", 
            "content": "<h2>2.3 Projective transformations </h2><p>我姑且把这个翻译为投影变换，大致来说<u>投影变换是投影平面 </u></p><u><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-26df7ca3a3a006f4c8b8cc7ef2671e70_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"24\" data-rawheight=\"17\" class=\"content_image\" width=\"24\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;24&#39; height=&#39;17&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"24\" data-rawheight=\"17\" class=\"content_image lazy\" width=\"24\" data-actualsrc=\"https://pic1.zhimg.com/v2-26df7ca3a3a006f4c8b8cc7ef2671e70_b.jpg\"/></figure></u><u><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3c40829a650f96ab57be6975d78f8601_b.jpg\" data-caption=\"\" data-size=\"normal\" class=\"content_image\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;0&#39; height=&#39;0&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" class=\"content_image lazy\" data-actualsrc=\"https://pic2.zhimg.com/v2-3c40829a650f96ab57be6975d78f8601_b.jpg\"/></figure></u><p><u>​ 中一种可逆的点到点、线到线的映射。</u>我觉得要注意的就是两点：<b>可逆的</b>、<b>线到线的</b>（直线总不可能投影成弯的吧）。投影也被称为直射变换（Collineation，共线，这个名字很形象）、投影变换（Projective transformation）、单应性变换（Homography）。</p><p>           投影的代数定义为：一个映射 </p><u><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-84fba7e6a70861e9965e2048389905ea_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"103\" data-rawheight=\"17\" class=\"content_image\" width=\"103\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;103&#39; height=&#39;17&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"103\" data-rawheight=\"17\" class=\"content_image lazy\" width=\"103\" data-actualsrc=\"https://pic3.zhimg.com/v2-84fba7e6a70861e9965e2048389905ea_b.jpg\"/></figure></u><u><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3c40829a650f96ab57be6975d78f8601_b.jpg\" data-caption=\"\" data-size=\"normal\" class=\"content_image\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;0&#39; height=&#39;0&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" class=\"content_image lazy\" data-actualsrc=\"https://pic2.zhimg.com/v2-3c40829a650f96ab57be6975d78f8601_b.jpg\"/></figure></u><p><u>​</u> ，存在一个3*3的非奇异矩阵 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-ab032a147d2d4bfe48e5daa2bce5d18a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"16\" data-rawheight=\"12\" class=\"content_image\" width=\"16\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;16&#39; height=&#39;12&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"16\" data-rawheight=\"12\" class=\"content_image lazy\" width=\"16\" data-actualsrc=\"https://pic3.zhimg.com/v2-ab032a147d2d4bfe48e5daa2bce5d18a_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3c40829a650f96ab57be6975d78f8601_b.jpg\" data-caption=\"\" data-size=\"normal\" class=\"content_image\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;0&#39; height=&#39;0&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" class=\"content_image lazy\" data-actualsrc=\"https://pic2.zhimg.com/v2-3c40829a650f96ab57be6975d78f8601_b.jpg\"/></figure><p>​ 使得任何 </p><u><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-26df7ca3a3a006f4c8b8cc7ef2671e70_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"24\" data-rawheight=\"17\" class=\"content_image\" width=\"24\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;24&#39; height=&#39;17&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"24\" data-rawheight=\"17\" class=\"content_image lazy\" width=\"24\" data-actualsrc=\"https://pic1.zhimg.com/v2-26df7ca3a3a006f4c8b8cc7ef2671e70_b.jpg\"/></figure></u><u><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3c40829a650f96ab57be6975d78f8601_b.jpg\" data-caption=\"\" data-size=\"normal\" class=\"content_image\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;0&#39; height=&#39;0&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" class=\"content_image lazy\" data-actualsrc=\"https://pic2.zhimg.com/v2-3c40829a650f96ab57be6975d78f8601_b.jpg\"/></figure></u><p><u>​</u> 中一点 </p><u><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-65bb8e60f55b4751b57bc8cc54cc0bdd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"10\" data-rawheight=\"10\" class=\"content_image\" width=\"10\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;10&#39; height=&#39;10&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"10\" data-rawheight=\"10\" class=\"content_image lazy\" width=\"10\" data-actualsrc=\"https://pic2.zhimg.com/v2-65bb8e60f55b4751b57bc8cc54cc0bdd_b.jpg\"/></figure></u><u><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3c40829a650f96ab57be6975d78f8601_b.jpg\" data-caption=\"\" data-size=\"normal\" class=\"content_image\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;0&#39; height=&#39;0&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" class=\"content_image lazy\" data-actualsrc=\"https://pic2.zhimg.com/v2-3c40829a650f96ab57be6975d78f8601_b.jpg\"/></figure></u><p><u>​</u> 都有 </p><u><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-bb284465388d2dc9eed46434e53280e7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"81\" data-rawheight=\"18\" class=\"content_image\" width=\"81\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;81&#39; height=&#39;18&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"81\" data-rawheight=\"18\" class=\"content_image lazy\" width=\"81\" data-actualsrc=\"https://pic4.zhimg.com/v2-bb284465388d2dc9eed46434e53280e7_b.jpg\"/></figure></u><u><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3c40829a650f96ab57be6975d78f8601_b.jpg\" data-caption=\"\" data-size=\"normal\" class=\"content_image\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;0&#39; height=&#39;0&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" class=\"content_image lazy\" data-actualsrc=\"https://pic2.zhimg.com/v2-3c40829a650f96ab57be6975d78f8601_b.jpg\"/></figure></u><p><u>​</u>。</p><p>           上面的代数定义告诉我们任何一个投影变换都是齐次坐标下的线性变换，反之亦然。之前说重要的两点是：可逆、线到线。非奇异矩阵就是可逆的，所以简单看一下线到线的映射。假设有点x在直线l上，则有 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-cd125ebb6236abd06af2717a1a5d2a79_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"60\" data-rawheight=\"17\" class=\"content_image\" width=\"60\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;60&#39; height=&#39;17&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"60\" data-rawheight=\"17\" class=\"content_image lazy\" width=\"60\" data-actualsrc=\"https://pic2.zhimg.com/v2-cd125ebb6236abd06af2717a1a5d2a79_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3c40829a650f96ab57be6975d78f8601_b.jpg\" data-caption=\"\" data-size=\"normal\" class=\"content_image\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;0&#39; height=&#39;0&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" class=\"content_image lazy\" data-actualsrc=\"https://pic2.zhimg.com/v2-3c40829a650f96ab57be6975d78f8601_b.jpg\"/></figure><p>​ 。进一步由H可逆可以写成 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-17fedb11072d7305310caf4d3c103ff9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"110\" data-rawheight=\"17\" class=\"content_image\" width=\"110\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;110&#39; height=&#39;17&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"110\" data-rawheight=\"17\" class=\"content_image lazy\" width=\"110\" data-actualsrc=\"https://pic2.zhimg.com/v2-17fedb11072d7305310caf4d3c103ff9_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3c40829a650f96ab57be6975d78f8601_b.jpg\" data-caption=\"\" data-size=\"normal\" class=\"content_image\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;0&#39; height=&#39;0&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" class=\"content_image lazy\" data-actualsrc=\"https://pic2.zhimg.com/v2-3c40829a650f96ab57be6975d78f8601_b.jpg\"/></figure><p>​。我们可以机智地把这个等式前后结合，解读成直线l上点投影后得到的点 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-4ffcdcd8b246d32d3bfba1ca52a19f99_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"27\" data-rawheight=\"13\" class=\"content_image\" width=\"27\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;27&#39; height=&#39;13&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"27\" data-rawheight=\"13\" class=\"content_image lazy\" width=\"27\" data-actualsrc=\"https://pic2.zhimg.com/v2-4ffcdcd8b246d32d3bfba1ca52a19f99_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3c40829a650f96ab57be6975d78f8601_b.jpg\" data-caption=\"\" data-size=\"normal\" class=\"content_image\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;0&#39; height=&#39;0&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" class=\"content_image lazy\" data-actualsrc=\"https://pic2.zhimg.com/v2-3c40829a650f96ab57be6975d78f8601_b.jpg\"/></figure><p>​ 同样在一条直线上，这条直线是 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-b44661368b6347eb9acdb7e40a38aec8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"42\" data-rawheight=\"17\" class=\"content_image\" width=\"42\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;42&#39; height=&#39;17&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"42\" data-rawheight=\"17\" class=\"content_image lazy\" width=\"42\" data-actualsrc=\"https://pic1.zhimg.com/v2-b44661368b6347eb9acdb7e40a38aec8_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3c40829a650f96ab57be6975d78f8601_b.jpg\" data-caption=\"\" data-size=\"normal\" class=\"content_image\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;0&#39; height=&#39;0&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" class=\"content_image lazy\" data-actualsrc=\"https://pic2.zhimg.com/v2-3c40829a650f96ab57be6975d78f8601_b.jpg\"/></figure><p>​。</p><ul><li><b>投影变换</b></li></ul><p>           投影变换的代数表达 </p><u><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-e30cb15bca919451f6dfa56084fda0b7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"62\" data-rawheight=\"15\" class=\"content_image\" width=\"62\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;62&#39; height=&#39;15&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"62\" data-rawheight=\"15\" class=\"content_image lazy\" width=\"62\" data-actualsrc=\"https://pic4.zhimg.com/v2-e30cb15bca919451f6dfa56084fda0b7_b.jpg\"/></figure></u><u><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3c40829a650f96ab57be6975d78f8601_b.jpg\" data-caption=\"\" data-size=\"normal\" class=\"content_image\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;0&#39; height=&#39;0&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" class=\"content_image lazy\" data-actualsrc=\"https://pic2.zhimg.com/v2-3c40829a650f96ab57be6975d78f8601_b.jpg\"/></figure></u><p><u>​</u> 展开如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-02a789a79e81ba5d46e45afc8caf474b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"429\" data-rawheight=\"122\" class=\"origin_image zh-lightbox-thumb\" width=\"429\" data-original=\"https://pic4.zhimg.com/v2-02a789a79e81ba5d46e45afc8caf474b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;429&#39; height=&#39;122&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"429\" data-rawheight=\"122\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"429\" data-original=\"https://pic4.zhimg.com/v2-02a789a79e81ba5d46e45afc8caf474b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-02a789a79e81ba5d46e45afc8caf474b_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3c40829a650f96ab57be6975d78f8601_b.jpg\" data-caption=\"\" data-size=\"normal\" class=\"content_image\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;0&#39; height=&#39;0&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" class=\"content_image lazy\" data-actualsrc=\"https://pic2.zhimg.com/v2-3c40829a650f96ab57be6975d78f8601_b.jpg\"/></figure><p>​</p><p>          同样值得注意的是决定H的是9个系数间的比例，所以其dof是8.</p><ul><li><b>平面间映射</b></li></ul><p>          这一小部分讲的也比较难读懂。个人认为主要讲了“中心投影”。中心投影更确切地应该叫做透视，学过素描的人应该理解这个，我觉得这里的中心应该就是透视中的灭点。这部分没有给详细定义就不多说了。主要感性地理解一下下图吧：中心投影把一个平面变换成另一个平面，两者间对应的直线都可以看成是一个过投影中心的平面与这两个平面的交线。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-2fb721efefacd0bdb414ea8a8c06851a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"763\" data-rawheight=\"366\" class=\"origin_image zh-lightbox-thumb\" width=\"763\" data-original=\"https://pic3.zhimg.com/v2-2fb721efefacd0bdb414ea8a8c06851a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;763&#39; height=&#39;366&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"763\" data-rawheight=\"366\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"763\" data-original=\"https://pic3.zhimg.com/v2-2fb721efefacd0bdb414ea8a8c06851a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-2fb721efefacd0bdb414ea8a8c06851a_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3c40829a650f96ab57be6975d78f8601_b.jpg\" data-caption=\"\" data-size=\"normal\" class=\"content_image\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;0&#39; height=&#39;0&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" class=\"content_image lazy\" data-actualsrc=\"https://pic2.zhimg.com/v2-3c40829a650f96ab57be6975d78f8601_b.jpg\"/></figure><p>​</p><ul><li><b>例：从一个平面的透视图像来消除投影畸变</b></li></ul><p>            作为一名绘画爱好者我必须说明一下透视是产生立体感的关键，不过这里我们还是考虑怎么消除这种投影畸变，我们想看到一个平面内的平行直线而不是在透视效果下会相交的“平行线”。</p><p>            道理很简单，因为投影就是一个线性变换H，我们只要把这个H求出来就行，用它的逆就可以消除投影畸变。</p><p>            我们假设点(x,y)通过H映射到(x&#39;,y&#39;)，比较机智地用一下投影变换的代数定义：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-8a8c4e986fe86b31019439900b050d9f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"694\" data-rawheight=\"76\" class=\"origin_image zh-lightbox-thumb\" width=\"694\" data-original=\"https://pic4.zhimg.com/v2-8a8c4e986fe86b31019439900b050d9f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;694&#39; height=&#39;76&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"694\" data-rawheight=\"76\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"694\" data-original=\"https://pic4.zhimg.com/v2-8a8c4e986fe86b31019439900b050d9f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-8a8c4e986fe86b31019439900b050d9f_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3c40829a650f96ab57be6975d78f8601_b.jpg\" data-caption=\"\" data-size=\"normal\" class=\"content_image\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;0&#39; height=&#39;0&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" class=\"content_image lazy\" data-actualsrc=\"https://pic2.zhimg.com/v2-3c40829a650f96ab57be6975d78f8601_b.jpg\"/></figure><p>​</p><p>确定两个等式很自然，因为一个点的自由度只有2而不是3.这里我们用了点的齐次坐标的两个比例去套公式。整理一下写成如下的样子：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-bbfd4f80fe31e953f2c14a467f1f040a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"471\" data-rawheight=\"80\" class=\"origin_image zh-lightbox-thumb\" width=\"471\" data-original=\"https://pic3.zhimg.com/v2-bbfd4f80fe31e953f2c14a467f1f040a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;471&#39; height=&#39;80&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"471\" data-rawheight=\"80\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"471\" data-original=\"https://pic3.zhimg.com/v2-bbfd4f80fe31e953f2c14a467f1f040a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-bbfd4f80fe31e953f2c14a467f1f040a_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3c40829a650f96ab57be6975d78f8601_b.jpg\" data-caption=\"\" data-size=\"normal\" class=\"content_image\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;0&#39; height=&#39;0&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" class=\"content_image lazy\" data-actualsrc=\"https://pic2.zhimg.com/v2-3c40829a650f96ab57be6975d78f8601_b.jpg\"/></figure><p>​</p><p>这已经有两个等式了。前面说过H的自由度是8，所以我们用四个点就能得到8个等式进而解出H。需要特别注意的是，这四个点中任意三点都不能共线！</p><p>最后要说明一下：投影种类很多。除了透视还有“多次投影的串联”“打在另一平面的阴影”等。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>2.3.1 Transformations of lines and conics</h2><ul><li>线的变换</li></ul><p>线的变换其实在这一部分开始时已经间接证明了。我们为了说明投影变换满足线到线映射利用了</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-17fedb11072d7305310caf4d3c103ff9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"110\" data-rawheight=\"17\" class=\"content_image\" width=\"110\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;110&#39; height=&#39;17&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"110\" data-rawheight=\"17\" class=\"content_image lazy\" width=\"110\" data-actualsrc=\"https://pic2.zhimg.com/v2-17fedb11072d7305310caf4d3c103ff9_b.jpg\"/></figure><p>​<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3c40829a650f96ab57be6975d78f8601_b.jpg\" data-caption=\"\" data-size=\"normal\" class=\"content_image\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;0&#39; height=&#39;0&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" class=\"content_image lazy\" data-actualsrc=\"https://pic2.zhimg.com/v2-3c40829a650f96ab57be6975d78f8601_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>我们机智地把前两项后两项分别结合在一起解读这个等式，那么上面这个式子就可以看成 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-b02a96559cf9d774dc677399ec32e5af_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"68\" data-rawheight=\"19\" class=\"content_image\" width=\"68\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;68&#39; height=&#39;19&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"68\" data-rawheight=\"19\" class=\"content_image lazy\" width=\"68\" data-actualsrc=\"https://pic4.zhimg.com/v2-b02a96559cf9d774dc677399ec32e5af_b.jpg\"/></figure><p> 。其中 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-b9f93ba3765a0a00900af44f14d5db24_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"168\" data-rawheight=\"22\" class=\"content_image\" width=\"168\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;168&#39; height=&#39;22&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"168\" data-rawheight=\"22\" class=\"content_image lazy\" width=\"168\" data-actualsrc=\"https://pic1.zhimg.com/v2-b9f93ba3765a0a00900af44f14d5db24_b.jpg\"/></figure><p>。稍微变得好看一点就得到了直线的投影变换如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-268d496cc472725f782cc2a14ae131c1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"76\" data-rawheight=\"19\" class=\"content_image\" width=\"76\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;76&#39; height=&#39;19&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"76\" data-rawheight=\"19\" class=\"content_image lazy\" width=\"76\" data-actualsrc=\"https://pic2.zhimg.com/v2-268d496cc472725f782cc2a14ae131c1_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>圆锥曲线的变换</li></ul><p>导出圆锥曲线投影变换的讨论跟上面过程类似。先有圆锥曲线满足的式子 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-2bd61bea5c4e7ef15c01b36b30eb3bff_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"80\" data-rawheight=\"17\" class=\"content_image\" width=\"80\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;80&#39; height=&#39;17&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"80\" data-rawheight=\"17\" class=\"content_image lazy\" width=\"80\" data-actualsrc=\"https://pic4.zhimg.com/v2-2bd61bea5c4e7ef15c01b36b30eb3bff_b.jpg\"/></figure><p> 。然后把变换关系 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-f79a6ab519ffd4b82072215288757af0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"85\" data-rawheight=\"17\" class=\"content_image\" width=\"85\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;85&#39; height=&#39;17&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"85\" data-rawheight=\"17\" class=\"content_image lazy\" width=\"85\" data-actualsrc=\"https://pic1.zhimg.com/v2-f79a6ab519ffd4b82072215288757af0_b.jpg\"/></figure><p> 带入得到：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-0d6f5a90682b276a1f2a24108e4f35eb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"230\" data-rawheight=\"19\" class=\"content_image\" width=\"230\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;230&#39; height=&#39;19&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"230\" data-rawheight=\"19\" class=\"content_image lazy\" width=\"230\" data-actualsrc=\"https://pic4.zhimg.com/v2-0d6f5a90682b276a1f2a24108e4f35eb_b.jpg\"/></figure><p> 。显然我们可以看出来变换后的x&#39;所在的圆锥曲线是：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-1629af2d3a462e84e6190f0c046d0924_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"127\" data-rawheight=\"18\" class=\"content_image\" width=\"127\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;127&#39; height=&#39;18&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"127\" data-rawheight=\"18\" class=\"content_image lazy\" width=\"127\" data-actualsrc=\"https://pic1.zhimg.com/v2-1629af2d3a462e84e6190f0c046d0924_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>对于对偶圆锥曲线的投影变换，完全同理的步骤可以得到结果：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-93295fbe2e44e3fb40e2f43b3cf6e40a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"114\" data-rawheight=\"17\" class=\"content_image\" width=\"114\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;114&#39; height=&#39;17&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"114\" data-rawheight=\"17\" class=\"content_image lazy\" width=\"114\" data-actualsrc=\"https://pic3.zhimg.com/v2-93295fbe2e44e3fb40e2f43b3cf6e40a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p>下一章节将会讲变换的层级。作为热身我先放上下图，我们可以直观感受一下相似、仿射、投影之间的区别，体会他们之间的层级。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-2f302342da8af6cee9ac09471d1f8870_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"928\" data-rawheight=\"438\" class=\"origin_image zh-lightbox-thumb\" width=\"928\" data-original=\"https://pic1.zhimg.com/v2-2f302342da8af6cee9ac09471d1f8870_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;928&#39; height=&#39;438&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"928\" data-rawheight=\"438\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"928\" data-original=\"https://pic1.zhimg.com/v2-2f302342da8af6cee9ac09471d1f8870_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-2f302342da8af6cee9ac09471d1f8870_b.jpg\"/></figure><p>​<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3c40829a650f96ab57be6975d78f8601_b.jpg\" data-caption=\"\" data-size=\"normal\" class=\"content_image\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;0&#39; height=&#39;0&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" class=\"content_image lazy\" data-actualsrc=\"https://pic2.zhimg.com/v2-3c40829a650f96ab57be6975d78f8601_b.jpg\"/></figure><p></p><p></p>", 
            "topic": [
                {
                    "tag": "计算机视觉", 
                    "tagLink": "https://api.zhihu.com/topics/19590195"
                }, 
                {
                    "tag": "同时定位和地图构建（SLAM）", 
                    "tagLink": "https://api.zhihu.com/topics/20033502"
                }, 
                {
                    "tag": "代数几何", 
                    "tagLink": "https://api.zhihu.com/topics/19878547"
                }
            ], 
            "comments": []
        }
    ], 
    "url": "https://zhuanlan.zhihu.com/c_1050756493959327744"
}
