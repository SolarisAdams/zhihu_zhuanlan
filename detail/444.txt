{
    "title": "Fangzhçš„è¿›å‡»ä¹‹è·¯", 
    "description": "åˆ†äº«å­¦ä¹ ç»éªŒã€äººç”Ÿæ„Ÿæ‚Ÿ", 
    "followers": [
        "https://www.zhihu.com/people/li-xian-hui-57-80", 
        "https://www.zhihu.com/people/rou-qiu-68-63", 
        "https://www.zhihu.com/people/zhang-rui-han-28", 
        "https://www.zhihu.com/people/silencesi-shi", 
        "https://www.zhihu.com/people/cha-de-wei-ke-25", 
        "https://www.zhihu.com/people/wang-da-pao-55-38", 
        "https://www.zhihu.com/people/simon-meng", 
        "https://www.zhihu.com/people/zhao-yun-long-24-70", 
        "https://www.zhihu.com/people/hong-kou-gang-ting-dian-62", 
        "https://www.zhihu.com/people/path-54", 
        "https://www.zhihu.com/people/msn-54", 
        "https://www.zhihu.com/people/skyline-20-91", 
        "https://www.zhihu.com/people/leadingsci", 
        "https://www.zhihu.com/people/yang-ke-82", 
        "https://www.zhihu.com/people/wan-an-an-an-78", 
        "https://www.zhihu.com/people/guai-guai-44-30", 
        "https://www.zhihu.com/people/de-duo-90", 
        "https://www.zhihu.com/people/642009845", 
        "https://www.zhihu.com/people/nian-hua-er-19", 
        "https://www.zhihu.com/people/qian-mian-liu-zhuan", 
        "https://www.zhihu.com/people/luozhongbin", 
        "https://www.zhihu.com/people/xjz-76", 
        "https://www.zhihu.com/people/loser_1", 
        "https://www.zhihu.com/people/jiu-shi-ya", 
        "https://www.zhihu.com/people/liu-jiang-87-82", 
        "https://www.zhihu.com/people/wei-zi-ang-66", 
        "https://www.zhihu.com/people/li-wen-long-68-16", 
        "https://www.zhihu.com/people/shi-guang-wu-sheng-65-46", 
        "https://www.zhihu.com/people/tu-dou-si-chao-ren", 
        "https://www.zhihu.com/people/bai-jun-qi-52", 
        "https://www.zhihu.com/people/suchao-79", 
        "https://www.zhihu.com/people/sakulaha-na-mi", 
        "https://www.zhihu.com/people/wu-ze-qing-14", 
        "https://www.zhihu.com/people/deathxi-ren-ko", 
        "https://www.zhihu.com/people/fang-yue-37", 
        "https://www.zhihu.com/people/zzx-47-7", 
        "https://www.zhihu.com/people/li-qing-30-94-65-41", 
        "https://www.zhihu.com/people/ffffffff-36-84", 
        "https://www.zhihu.com/people/lucky-45-9-77", 
        "https://www.zhihu.com/people/lin-lin-87-38", 
        "https://www.zhihu.com/people/xiao-xiao-88-98-74", 
        "https://www.zhihu.com/people/doctorli-71", 
        "https://www.zhihu.com/people/sun-rui-xiang-3", 
        "https://www.zhihu.com/people/longines-53", 
        "https://www.zhihu.com/people/qing-xiao-xiang-xiao", 
        "https://www.zhihu.com/people/three-e-e-Point", 
        "https://www.zhihu.com/people/wang-jia-95-94", 
        "https://www.zhihu.com/people/fa-hao-tu-cao-10", 
        "https://www.zhihu.com/people/fuzhengwei", 
        "https://www.zhihu.com/people/wang-tian-ye-14", 
        "https://www.zhihu.com/people/he-mei-36-96", 
        "https://www.zhihu.com/people/bailingnan", 
        "https://www.zhihu.com/people/sophist-20", 
        "https://www.zhihu.com/people/yin-xing-shu-90-5", 
        "https://www.zhihu.com/people/guang-chen-99-1", 
        "https://www.zhihu.com/people/tshogx-1", 
        "https://www.zhihu.com/people/codeyangjun", 
        "https://www.zhihu.com/people/rong-kai-hua-93", 
        "https://www.zhihu.com/people/artb-sir", 
        "https://www.zhihu.com/people/harrytsz", 
        "https://www.zhihu.com/people/lu-shi-sheng-29", 
        "https://www.zhihu.com/people/guo-yan-jing-45", 
        "https://www.zhihu.com/people/guomuguomunuo", 
        "https://www.zhihu.com/people/codeman-lol", 
        "https://www.zhihu.com/people/yang-troy-89", 
        "https://www.zhihu.com/people/ren-xuan-chi", 
        "https://www.zhihu.com/people/jimmyhua-37", 
        "https://www.zhihu.com/people/fu-hao-37-24", 
        "https://www.zhihu.com/people/li-jia-hong-20", 
        "https://www.zhihu.com/people/jiang-hao-wen-91-36", 
        "https://www.zhihu.com/people/ke-le-teng-zi", 
        "https://www.zhihu.com/people/liu-ev-74", 
        "https://www.zhihu.com/people/lao-meng-zi-65", 
        "https://www.zhihu.com/people/cheng-ming-zhe-78", 
        "https://www.zhihu.com/people/jax-59", 
        "https://www.zhihu.com/people/ni-yong-hu-97-91", 
        "https://www.zhihu.com/people/chao-ge-74-26", 
        "https://www.zhihu.com/people/dou-yi-geng", 
        "https://www.zhihu.com/people/hanxumyself", 
        "https://www.zhihu.com/people/pigg-94", 
        "https://www.zhihu.com/people/huang-da-xian-57-37-18", 
        "https://www.zhihu.com/people/liu-zhao-jun-7-76", 
        "https://www.zhihu.com/people/francis-52-23", 
        "https://www.zhihu.com/people/zhang-xin-10-47-90", 
        "https://www.zhihu.com/people/xiao-yun-43-27-67", 
        "https://www.zhihu.com/people/SecondaryMarquis", 
        "https://www.zhihu.com/people/li-xin-wei-91-72", 
        "https://www.zhihu.com/people/gui-yu-chao", 
        "https://www.zhihu.com/people/zhaopeng-2", 
        "https://www.zhihu.com/people/li-xin-95-42-23", 
        "https://www.zhihu.com/people/zhanghuidream", 
        "https://www.zhihu.com/people/yang-song-yang", 
        "https://www.zhihu.com/people/spirit-22-32"
    ], 
    "article": [
        {
            "url": "https://zhuanlan.zhihu.com/p/47108882", 
            "userName": "ç›´ä¸Šäº‘éœ„", 
            "userLink": "https://www.zhihu.com/people/1033165ce4ad9c3fce69a0793dfab8ad", 
            "upvote": 27, 
            "title": "å´æ©è¾¾Coursera(DeepLearning.ai)ç¬”è®°å’Œä½œä¸šæ±‡æ€»å¸–", 
            "content": "<div class=\"highlight\"><pre><code class=\"language-text\">title: å´æ©è¾¾Coursera(DeepLearning.ai)ç¬”è®°å’Œä½œä¸šæ±‡æ€»å¸–\ndate: 2018-10-18 20:01:05\nid: dl-ai-summary\ntags: \n - dl.ai\ncategories:\n - æ±‡æ€»å¸–\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-fc1613b1e00dd44a89b5f550abd4321a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1616\" data-rawheight=\"856\" class=\"origin_image zh-lightbox-thumb\" width=\"1616\" data-original=\"https://pic3.zhimg.com/v2-fc1613b1e00dd44a89b5f550abd4321a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1616&#39; height=&#39;856&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1616\" data-rawheight=\"856\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1616\" data-original=\"https://pic3.zhimg.com/v2-fc1613b1e00dd44a89b5f550abd4321a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-fc1613b1e00dd44a89b5f550abd4321a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>å´æ©è¾¾Coursera(DeepLearning.ai)ç¬”è®°å’Œä½œä¸šæ±‡æ€»ã€‚</p><p>&lt;!--more--&gt;</p><p>å†æ—¶ä¸€ä¸ªå¤šæœˆç»ˆäºæŠŠNGçš„äº”é—¨è¯¾å…¨éƒ¨å­¦å®Œå¹¶ä¸”åšäº†ä½œä¸šå’Œç¬”è®°äº†ã€‚è¿™é‡Œæ±‡æ€»ä¸€ä¸‹ï¼š</p><h2><b>ç¬¬ä¸€é—¨è¯¾ï¼šç¥ç»ç½‘ç»œå’Œæ·±åº¦å­¦ä¹  </b></h2><p>ä¸»è¦è®²äº†ç¥ç»ç½‘ç»œçš„åŸºæœ¬æ¦‚å¿µï¼Œä»¥åŠæœºå™¨å­¦ä¹ çš„æ¢¯åº¦ä¸‹é™æ³•ï¼Œå‘é‡åŒ–ï¼Œè€Œåè¿›å…¥äº†æµ…å±‚å’Œæ·±å±‚ç¥ç»ç½‘ç»œçš„å®ç°ã€‚</p><ul><li>å‰ä¸¤å‘¨å¤ªç®€å•äº†ï¼Œåœ¨ä¹‹å‰çš„æœºå™¨å­¦ä¹ è¯¾ä¸ŠNGå…¨éƒ¨éƒ½è®²è¿‡äº†ï¼Œè¿™é‡Œå°±ä¸åšäº†ã€‚</li><li>ç¬¬ä¸‰å‘¨ï¼šä¸»è¦æ˜¯æµ…å±‚ç¥ç»ç½‘ç»œçš„å®ç°<br/></li><ul><li>ç¬”è®°ï¼š<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/2018091215/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">æµ…å±‚ç¥ç»ç½‘ç»œ</a></li><li>ä½œä¸šï¼š<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/2018091216/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">æµ…å±‚ç¥ç»ç½‘ç»œ</a></li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>ç¬¬å››å‘¨ï¼šæ·±å±‚ç¥ç»ç½‘ç»œçš„å®ç°<br/></li><ul><li>ç¬”è®°ï¼š<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/2018091316/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">æ·±å±‚ç¥ç»ç½‘ç»œ</a></li><li>ä½œä¸šï¼š<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/2018091318/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">æ·±å±‚ç¥ç»ç½‘ç»œ</a></li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>ç¬¬äºŒé—¨è¯¾ï¼šæ”¹å–„ç¥ç»ç½‘ç»œ</b></h2><p>ä»‹ç»äº†æ”¹å–„ç¥ç»ç½‘ç»œçš„æ–¹æ³•ï¼Œå¦‚æ­£åˆ™åŒ–ï¼Œè¶…å‚æ•°è°ƒèŠ‚ï¼Œä¼˜åŒ–ç®—æ³•ç­‰ã€‚</p><ul><li>ç¬¬ä¸€å‘¨ï¼šè®­ç»ƒé›†çš„åˆ’åˆ†ã€æ­£åˆ™åŒ–ã€dropout<br/></li><ul><li>ç¬”è®°ï¼š<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/20180901513/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">æ·±åº¦å­¦ä¹ çš„å®è·µå±‚é¢</a></li><li>ä½œä¸šï¼š<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/2018091515/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">æ·±åº¦å­¦ä¹ çš„å®è·µå±‚é¢</a></li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>ç¬¬äºŒå‘¨ï¼šMini-batchã€Momentumã€RMSã€Adamã€å­¦ä¹ ç‡è¡°å‡<br/></li><ul><li>ç¬”è®°ï¼š<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/2018091621/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">ä¼˜åŒ–ç®—æ³•</a></li><li>ä½œä¸šï¼š<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/2018091711/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">ä¼˜åŒ–ç®—æ³•</a></li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>ç¬¬ä¸‰å‘¨ï¼šè¶…å‚æ•°çš„è°ƒè¯•ã€BatchNormã€softmax<br/></li><ul><li>ç¬”è®°ï¼š<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/2018091720/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">è¶…å‚æ•°è°ƒè¯•</a></li><li>ä½œä¸šï¼š<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/2018091810/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">è¶…å‚æ•°è°ƒè¯•</a></li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>ç¬¬ä¸‰é—¨è¯¾ï¼šç»“æ„åŒ–æœºå™¨å­¦ä¹ é¡¹ç›®</b></h2><p>ä¸»è¦è®²äº†æœºå™¨å­¦ä¹ ä¸­çš„ä¸€äº›ç­–ç•¥ã€‚</p><ul><li>ç¬¬ä¸€å‘¨ï¼šMLç­–ç•¥ã€æ­£äº¤åŒ–ã€ä¼˜åŒ–æŒ‡æ ‡ã€æ•°æ®é›†çš„åˆ’åˆ†ã€åå·®<br/></li><ul><li>ç¬”è®°ï¼š<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/2018092016/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">æœºå™¨å­¦ä¹ ç­–ç•¥(1)</a></li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>ç¬¬äºŒå‘¨ï¼šè¯¯å·®åˆ†æã€æ•°æ®ä¸åŒåˆ†å¸ƒã€è¿ç§»å­¦ä¹ ã€å¤šä»»åŠ¡ã€ç«¯åˆ°ç«¯<br/></li><ul><li>ç¬”è®°ï¼š<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/2018092017/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">æœºå™¨å­¦ä¹ ç­–ç•¥(2)</a></li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>ç¬¬å››é—¨è¯¾ï¼šå·ç§¯ç¥ç»ç½‘ç»œ</b></h2><p>ä¸»è¦è®²äº†ç¥ç»ç½‘ç»œçš„åœ¨å›¾åƒä¸Šçš„éå¸¸é‡è¦çš„åº”ç”¨ï¼Œå·ç§¯ç¥ç»ç½‘ç»œã€‚</p><ul><li>ç¬¬ä¸€å‘¨ï¼špaddingã€æ­¥é•¿ã€æ± åŒ–ã€å·ç§¯<br/></li><ul><li>ç¬”è®°ï¼š<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/dl-ai-4-1/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">å·ç§¯ç¥ç»ç½‘ç»œ</a></li><li>ä½œä¸šï¼š<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/dl-ai-4-1h/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">å·ç§¯ç¥ç»ç½‘ç»œ</a></li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>ç¬¬äºŒå‘¨ï¼šä¸€äº›é‡è¦çš„ç¥ç»ç½‘ç»œç»“æ„ï¼ŒVGGã€ResNetã€Inceptionç­‰<br/></li><ul><li>ç¬”è®°ï¼š<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/dl-ai-4-2/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">æ·±åº¦å·ç§¯ç½‘ç»œå®ä¾‹æ¢ç©¶</a></li><li>ä½œä¸šï¼š<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/dl-ai-4-2h/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">æ·±åº¦å·ç§¯ç½‘ç»œå®ä¾‹æ¢ç©¶</a></li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>ç¬¬ä¸‰å‘¨ï¼šç›®æ ‡æ£€æµ‹ã€Bounding Boxã€IOUã€NMS<br/></li><ul><li>ç¬”è®°ï¼š<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/dl-ai-4-3/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">ç›®æ ‡æ£€æµ‹</a></li><li>ä½œä¸šï¼š<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/dl-ai-4-3h/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">ç›®æ ‡æ£€æµ‹</a></li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>ç¬¬å››å‘¨ï¼šäººè„¸è¯†åˆ«å’Œç¥ç»é£æ ¼è½¬æ¢<br/></li><ul><li>ç¬”è®°ï¼š<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/dl-ai-4-4/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">äººè„¸è¯†åˆ«å’Œç¥ç»é£æ ¼è½¬æ¢</a></li><li>ä½œä¸šï¼š<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/dl-ai-4-4h/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">äººè„¸è¯†åˆ«å’Œç¥ç»é£æ ¼è½¬æ¢</a></li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>ç¬¬äº”é—¨è¯¾ï¼šåºåˆ—æ¨¡å‹</b></h2><p>ä¸»è¦è®²äº†ç¥ç»ç½‘ç»œåœ¨è¯­è¨€é¢†åŸŸçš„åº”ç”¨ï¼Œç”¨RNNæ¨¡å‹</p><ul><li>ç¬¬ä¸€å‘¨ï¼šä»‹ç»äº†åŸºæœ¬çš„RNNã€GRUã€LSTM<br/></li><ul><li>ç¬”è®°ï¼š<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/dl-ai-5-1/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">å¾ªç¯ç¥ç»ç½‘ç»œ</a></li><li>ä½œä¸šï¼š<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/dl-ai-5-1h1/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">æ„å»ºRNN</a>ã€<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/dl-ai-5-1h2/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">å­—ç¬¦çº§ç”Ÿæˆæé¾™åå­—</a>ã€<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/dl-ai-5-1h3/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">LSTMç”Ÿæˆçˆµå£«ä¹</a></li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>ç¬¬äºŒå‘¨ï¼šè‡ªç„¶è¯­è¨€å¤„ç†ä¸è¯åµŒå…¥<br/></li><ul><li>ç¬”è®°ï¼š<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/dl-ai-5-2/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">è‡ªç„¶è¯­è¨€å¤„ç†ä¸è¯åµŒå…¥</a></li><li>ä½œä¸šï¼š<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/dl-ai-5-2h/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">è¯å‘é‡è¿ç®—å’Œemojiè¡¨æƒ…åŒ…</a></li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>ç¬¬ä¸‰å‘¨ï¼šåºåˆ—æ¨¡å‹å’Œæ³¨æ„åŠ›æœºåˆ¶<br/></li><ul><li>ç¬”è®°ï¼š<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/dl-ai-5-3/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">åºåˆ—æ¨¡å‹å’Œæ³¨æ„åŠ›æœºåˆ¶</a></li><li>ä½œä¸šï¼š<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/dl-ai-5-3h/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">æœºå™¨ç¿»è¯‘å’Œè§¦å‘å…³é”®å­—</a></li></ul></ul><p></p><p></p>", 
            "topic": [
                {
                    "tag": "å´æ©è¾¾ï¼ˆAndrew Ngï¼‰", 
                    "tagLink": "https://api.zhihu.com/topics/20003150"
                }, 
                {
                    "tag": "æœºå™¨å­¦ä¹ ", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "Coursera", 
                    "tagLink": "https://api.zhihu.com/topics/19742204"
                }
            ], 
            "comments": [
                {
                    "userName": "çŸ¥ä¹ç”¨æˆ·", 
                    "userLink": "https://www.zhihu.com/people/0", 
                    "content": "ä½ å¥½ï¼Œé—®ä¸€ä¸‹ï¼Œä½ ä¸€ä¸ªæœˆä¹‹é—´ï¼Œå¤§æ¦‚æ¯å¤©å­¦ä¹ å¤šä¹…ï¼ŸğŸ˜¯", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "ç›´ä¸Šäº‘éœ„", 
                            "userLink": "https://www.zhihu.com/people/1033165ce4ad9c3fce69a0793dfab8ad", 
                            "content": "æˆ‘å¿˜è®°äº†ï¼Œæœ‰æ•ˆå­¦ä¹ æ—¶é—´ä¸‰å››ä¸ªå°æ—¶å§ã€‚å¤§éƒ¨åˆ†æ—¶é—´æ˜¯æ²¡åŠæ³•é›†ä¸­ç²¾åŠ›çš„", 
                            "likes": 0, 
                            "replyToAuthor": "çŸ¥ä¹ç”¨æˆ·"
                        }, 
                        {
                            "userName": "çŸ¥ä¹ç”¨æˆ·", 
                            "userLink": "https://www.zhihu.com/people/0", 
                            "content": "å¥½çš„ï¼Œè°¢è°¢ã€‚", 
                            "likes": 0, 
                            "replyToAuthor": "ç›´ä¸Šäº‘éœ„"
                        }
                    ]
                }, 
                {
                    "userName": "Luna", 
                    "userLink": "https://www.zhihu.com/people/532397b06ab942beacc28e98d6b904e5", 
                    "content": "è°¢è°¢æ¥¼ä¸»ï¼Œæœ€è¿‘åˆšå¥½åœ¨çœ‹cnnï¼Œåˆ°æ—¶å¤šå¤šåƒä½ å­¦ä¹ å•Š", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "ç‹—å¨ƒå­", 
                    "userLink": "https://www.zhihu.com/people/6edc022455eb041f73e1c1575ace9e5c", 
                    "content": "æ¥¼ä¸»çœŸæ˜¯ä¸ªå¥½äºº", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/45753821", 
            "userName": "ç›´ä¸Šäº‘éœ„", 
            "userLink": "https://www.zhihu.com/people/1033165ce4ad9c3fce69a0793dfab8ad", 
            "upvote": 4, 
            "title": "cs231nä½œä¸šï¼šassignment1 - softmax", 
            "content": "<div class=\"highlight\"><pre><code class=\"language-text\">title: cs231nä½œä¸šï¼šassignment1 - softmax\nid: cs231n-1h-3\ntags:\n  - cs231n\n  - homework\ncategories:\n  - AI\n  - Deep Learning\ndate: 2018-09-27 16:02:57\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>GitHubåœ°å€ï¼š<a href=\"https://link.zhihu.com/?target=https%3A//github.com/ZJUFangzh/cs231n\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/ZJUFangzh/cs</span><span class=\"invisible\">231n</span><span class=\"ellipsis\"></span></a></p><p>é¦–å‘äºä¸ªäººåšå®¢: <a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Fangzhçš„ä¸ªäººåšå®¢ | äººå·¥æ™ºèƒ½æ‹¯æ•‘ä¸–ç•Œ</a> æ¬¢è¿æ¥è®¿</p><p>softmaxæ˜¯æœ€å¸¸ç”¨çš„åˆ†ç±»å™¨ä¹‹ä¸€ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><p>softmaxå’Œsvméƒ½æ˜¯å¸¸ç”¨çš„åˆ†ç±»å™¨ï¼Œè€Œsoftmaxæ›´ä¸ºå¸¸ç”¨ã€‚</p><p>å…·ä½“å¯ä»¥å‚è€ƒæˆ‘è¿™ç¯‡çš„æœ€åï¼Œngè€å¸ˆæœ‰è®²ï¼Œ<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/2018091720/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">softmax</a></p><p class=\"ztext-empty-paragraph\"><br/></p><p>å‰é¢æ•°æ®é›†çš„éƒ½è·ŸSVMçš„ä¸€æ ·ã€‚</p><p>ç›´æ¥è¿›å…¥losså’Œgradsæ¨å¯¼ç¯èŠ‚ã€‚</p><p><img src=\"https://www.zhihu.com/equation?tex=L_i+%3D+-log%28%5Cfrac%7Be%5E%7Bf_%7By_i%7D%7D%7D%7B%5Csum_j+e%5E%7Bf_j%7D%7D%29\" alt=\"L_i = -log(\\frac{e^{f_{y_i}}}{\\sum_j e^{f_j}})\" eeimg=\"1\"/> </p><p>å¯ä»¥çœ‹åˆ°ï¼Œè®¡ç®—çš„å…¬å¼ä¹Ÿå°±æ˜¯cross-entropyï¼Œå³</p><p><img src=\"https://www.zhihu.com/equation?tex=H%28p%2Cq%29+%3D+-+%5Csum_i+y_i+log%28y_%7Bi%7D%5E%7Bhat%7D%29\" alt=\"H(p,q) = - \\sum_i y_i log(y_{i}^{hat})\" eeimg=\"1\"/> </p><p>ä½†æ˜¯ï¼Œè¿™æ ·æœ‰ä¸€ä¸ªç¼ºç‚¹ï¼Œå°±æ˜¯æŒ‡æ•°$e^{f_{y_i}}$å¯èƒ½ä¼šç‰¹åˆ«å¤§ï¼Œè¿™æ ·å¯èƒ½å¯¼è‡´å†…å­˜ä¸è¶³ï¼Œè®¡ç®—ä¸ç¨³å®šç­‰é—®é¢˜ã€‚é‚£ä¹ˆå¯ä»¥åœ¨åˆ†å­åˆ†æ¯åŒä¹˜ä¸€ä¸ªå¸¸æ•°Cï¼Œä¸€èˆ¬Cå–ä¸º <img src=\"https://www.zhihu.com/equation?tex=logC+%3D+-max+f_j+\" alt=\"logC = -max f_j \" eeimg=\"1\"/> </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-cae4975702798a9de19495eb641e1de7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"389\" data-rawheight=\"100\" class=\"content_image\" width=\"389\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;389&#39; height=&#39;100&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"389\" data-rawheight=\"100\" class=\"content_image lazy\" width=\"389\" data-actualsrc=\"https://pic4.zhimg.com/v2-cae4975702798a9de19495eb641e1de7_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">f</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"mi\">123</span><span class=\"p\">,</span> <span class=\"mi\">456</span><span class=\"p\">,</span> <span class=\"mi\">789</span><span class=\"p\">])</span> <span class=\"c1\"># ä¾‹å­ä¸­æœ‰3ä¸ªåˆ†ç±»ï¼Œæ¯ä¸ªè¯„åˆ†çš„æ•°å€¼éƒ½å¾ˆå¤§</span>\n<span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">))</span> <span class=\"c1\"># ä¸å¦™ï¼šæ•°å€¼é—®é¢˜ï¼Œå¯èƒ½å¯¼è‡´æ•°å€¼çˆ†ç‚¸</span>\n<span class=\"err\">â€‹</span>\n<span class=\"c1\"># é‚£ä¹ˆå°†fä¸­çš„å€¼å¹³ç§»åˆ°æœ€å¤§å€¼ä¸º0ï¼š</span>\n<span class=\"n\">f</span> <span class=\"o\">-=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">max</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">)</span> <span class=\"c1\"># f becomes [-666, -333, 0]</span>\n<span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">))</span> <span class=\"c1\"># ç°åœ¨OKäº†ï¼Œå°†ç»™å‡ºæ­£ç¡®ç»“æœ</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>ç²¾ç¡®åœ°è¯´ï¼ŒSVMåˆ†ç±»å™¨ä½¿ç”¨çš„æ˜¯<i>æŠ˜å¶æŸå¤±ï¼ˆhinge lossï¼‰</i>ï¼Œæœ‰æ—¶å€™åˆè¢«ç§°ä¸º<i>æœ€å¤§è¾¹ç•ŒæŸå¤±ï¼ˆmax-margin lossï¼‰</i>ã€‚Softmaxåˆ†ç±»å™¨ä½¿ç”¨çš„æ˜¯<i>äº¤å‰ç†µæŸå¤±ï¼ˆcorss-entropy lossï¼‰</i>ã€‚Softmaxåˆ†ç±»å™¨çš„å‘½åæ˜¯ä»<i>softmaxå‡½æ•°</i>é‚£é‡Œå¾—æ¥çš„ï¼Œsoftmaxå‡½æ•°å°†åŸå§‹åˆ†ç±»è¯„åˆ†å˜æˆæ­£çš„å½’ä¸€åŒ–æ•°å€¼ï¼Œæ‰€æœ‰æ•°å€¼å’Œä¸º1ï¼Œè¿™æ ·å¤„ç†åäº¤å‰ç†µæŸå¤±æ‰èƒ½åº”ç”¨ã€‚æ³¨æ„ä»æŠ€æœ¯ä¸Šè¯´â€œsoftmaxæŸå¤±ï¼ˆsoftmax lossï¼‰â€æ˜¯æ²¡æœ‰æ„ä¹‰çš„ï¼Œå› ä¸ºsoftmaxåªæ˜¯ä¸€ä¸ªå‹ç¼©æ•°å€¼çš„å‡½æ•°ã€‚ä½†æ˜¯åœ¨è¿™ä¸ªè¯´æ³•å¸¸å¸¸è¢«ç”¨æ¥åšç®€ç§°ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><p>æ±‚å¯¼è¿‡ç¨‹å‚è€ƒï¼š<a href=\"https://zhuanlan.zhihu.com/p/37416115\" class=\"internal\">cs231n softmaxæ±‚å¯¼</a></p><p>æœ€ç»ˆå¾—åˆ°çš„å…¬å¼æ˜¯ï¼š</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-a2a53c650f8e78c8034a671acb561b56_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"381\" data-rawheight=\"272\" class=\"content_image\" width=\"381\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;381&#39; height=&#39;272&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"381\" data-rawheight=\"272\" class=\"content_image lazy\" width=\"381\" data-actualsrc=\"https://pic3.zhimg.com/v2-a2a53c650f8e78c8034a671acb561b56_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>softmaxä»£ç å®ç°</b></p><p>ç¼–è¾‘<code>cs231n/classifiers/softmax.py</code>,å…ˆå†™ä¸€ä¸‹<code>softmax_loss_naive</code>å‡½æ•°ï¼Œä¾æ—§æ˜¯å¾ªç¯ï¼š</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">softmax_loss_naive</span><span class=\"p\">(</span><span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">reg</span><span class=\"p\">):</span>\n  <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">  Softmax loss function, naive implementation (with loops)\n</span><span class=\"s2\">â€‹\n</span><span class=\"s2\">  Inputs have dimension D, there are C classes, and we operate on minibatches\n</span><span class=\"s2\">  of N examples.\n</span><span class=\"s2\">â€‹\n</span><span class=\"s2\">  Inputs:\n</span><span class=\"s2\">  - W: A numpy array of shape (D, C) containing weights.\n</span><span class=\"s2\">  - X: A numpy array of shape (N, D) containing a minibatch of data.\n</span><span class=\"s2\">  - y: A numpy array of shape (N,) containing training labels; y[i] = c means\n</span><span class=\"s2\">    that X[i] has label c, where 0 &lt;= c &lt; C.\n</span><span class=\"s2\">  - reg: (float) regularization strength\n</span><span class=\"s2\">â€‹\n</span><span class=\"s2\">  Returns a tuple of:\n</span><span class=\"s2\">  - loss as single float\n</span><span class=\"s2\">  - gradient with respect to weights W; an array of same shape as W\n</span><span class=\"s2\">  &#34;&#34;&#34;</span>\n  <span class=\"c1\"># Initialize the loss and gradient to zero.</span>\n  <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span>\n  <span class=\"n\">dW</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros_like</span><span class=\"p\">(</span><span class=\"n\">W</span><span class=\"p\">)</span>\n<span class=\"err\">â€‹</span>\n  <span class=\"c1\">#############################################################################</span>\n  <span class=\"c1\"># TODO: Compute the softmax loss and its gradient using explicit loops.     #</span>\n  <span class=\"c1\"># Store the loss in loss and the gradient in dW. If you are not careful     #</span>\n  <span class=\"c1\"># here, it is easy to run into numeric instability. Don&#39;t forget the        #</span>\n  <span class=\"c1\"># regularization!                                                           #</span>\n  <span class=\"c1\">#############################################################################</span>\n  <span class=\"p\">(</span><span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">D</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n  <span class=\"n\">C</span> <span class=\"o\">=</span> <span class=\"n\">W</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n  <span class=\"c1\">#éå†æ¯ä¸ªæ ·æœ¬</span>\n  <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">N</span><span class=\"p\">):</span>\n    <span class=\"n\">f_i</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">W</span><span class=\"p\">)</span>\n    <span class=\"c1\">#è¿›è¡Œå…¬å¼çš„æŒ‡æ•°ä¿®æ­£</span>\n    <span class=\"n\">f_i</span> <span class=\"o\">-=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">max</span><span class=\"p\">(</span><span class=\"n\">f_i</span><span class=\"p\">)</span>\n    <span class=\"n\">sum_j</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"n\">f_i</span><span class=\"p\">))</span>\n    <span class=\"c1\">#å¾—åˆ°æ ·æœ¬ä¸­æ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡</span>\n    <span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span> <span class=\"n\">k</span> <span class=\"p\">:</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"n\">f_i</span><span class=\"p\">[</span><span class=\"n\">k</span><span class=\"p\">])</span> <span class=\"o\">/</span> <span class=\"n\">sum_j</span>\n    <span class=\"n\">loss</span> <span class=\"o\">+=</span> <span class=\"o\">-</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">log</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"p\">(</span><span class=\"n\">y</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]))</span>\n    <span class=\"c1\">#æ ¹æ®softmaxæ±‚å¯¼å…¬å¼</span>\n    <span class=\"k\">for</span> <span class=\"n\">k</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">C</span><span class=\"p\">):</span>\n      <span class=\"n\">p_k</span> <span class=\"o\">=</span> <span class=\"n\">p</span><span class=\"p\">(</span><span class=\"n\">k</span><span class=\"p\">)</span>\n      <span class=\"n\">dW</span><span class=\"p\">[:,</span> <span class=\"n\">k</span><span class=\"p\">]</span> <span class=\"o\">+=</span> <span class=\"p\">(</span><span class=\"n\">p_k</span> <span class=\"o\">-</span> <span class=\"p\">(</span><span class=\"n\">k</span> <span class=\"o\">==</span> <span class=\"n\">y</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]))</span> <span class=\"o\">*</span> <span class=\"n\">X</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span>\n  \n  <span class=\"n\">loss</span> <span class=\"o\">/=</span> <span class=\"n\">N</span>\n  <span class=\"n\">loss</span> <span class=\"o\">+=</span> <span class=\"mf\">0.5</span> <span class=\"o\">*</span> <span class=\"n\">reg</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">W</span> <span class=\"o\">*</span> <span class=\"n\">W</span><span class=\"p\">)</span>\n  <span class=\"n\">dW</span> <span class=\"o\">/=</span> <span class=\"n\">N</span>\n  <span class=\"n\">dW</span> <span class=\"o\">+=</span> <span class=\"n\">reg</span><span class=\"o\">*</span><span class=\"n\">W</span>\n  <span class=\"c1\">#############################################################################</span>\n  <span class=\"c1\">#                          END OF YOUR CODE                                 #</span>\n  <span class=\"c1\">#############################################################################</span>\n<span class=\"err\">â€‹</span>\n  <span class=\"k\">return</span> <span class=\"n\">loss</span><span class=\"p\">,</span> <span class=\"n\">dW</span>\n<span class=\"err\">â€‹</span></code></pre></div><p>éªŒè¯ä¸€ä¸‹losså’Œgradå¾—åˆ°ï¼š</p><div class=\"highlight\"><pre><code class=\"language-text\">numerical: -0.621593 analytic: -0.621593, relative error: 7.693773e-09\nnumerical: -2.576505 analytic: -2.576505, relative error: 4.492083e-09\nnumerical: -1.527801 analytic: -1.527801, relative error: 4.264914e-08\nnumerical: 1.101379 analytic: 1.101379, relative error: 9.735173e-09\nnumerical: 2.375620 analytic: 2.375620, relative error: 3.791861e-08\nnumerical: 3.166961 analytic: 3.166960, relative error: 8.526285e-09\nnumerical: -1.440997 analytic: -1.440998, relative error: 4.728898e-08\nnumerical: 0.563304 analytic: 0.563304, relative error: 2.409996e-08\nnumerical: -2.057292 analytic: -2.057292, relative error: 1.820335e-08\nnumerical: -0.450338 analytic: -0.450338, relative error: 8.075985e-08\nnumerical: -0.233090 analytic: -0.233090, relative error: 4.136546e-08\nnumerical: 0.251391 analytic: 0.251391, relative error: 4.552523e-08\nnumerical: 0.787031 analytic: 0.787031, relative error: 5.036469e-08\nnumerical: -1.801593 analytic: -1.801594, relative error: 3.159903e-08\nnumerical: -0.294108 analytic: -0.294109, relative error: 1.792497e-07\nnumerical: -1.974307 analytic: -1.974307, relative error: 1.160708e-08\nnumerical: 2.986921 analytic: 2.986921, relative error: 2.788065e-08\nnumerical: -0.247281 analytic: -0.247281, relative error: 8.957573e-08\nnumerical: 0.569337 analytic: 0.569337, relative error: 2.384912e-08\nnumerical: -1.579298 analytic: -1.579298, relative error: 1.728733e-08</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><b>å‘é‡åŒ–softmax</b></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">softmax_loss_vectorized</span><span class=\"p\">(</span><span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">reg</span><span class=\"p\">):</span>\n  <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">  Softmax loss function, vectorized version.\n</span><span class=\"s2\">â€‹\n</span><span class=\"s2\">  Inputs and outputs are the same as softmax_loss_naive.\n</span><span class=\"s2\">  &#34;&#34;&#34;</span>\n  <span class=\"c1\"># Initialize the loss and gradient to zero.</span>\n  <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span>\n  <span class=\"n\">dW</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros_like</span><span class=\"p\">(</span><span class=\"n\">W</span><span class=\"p\">)</span>\n<span class=\"err\">â€‹</span>\n  <span class=\"c1\">#############################################################################</span>\n  <span class=\"c1\"># TODO: Compute the softmax loss and its gradient using no explicit loops.  #</span>\n  <span class=\"c1\"># Store the loss in loss and the gradient in dW. If you are not careful     #</span>\n  <span class=\"c1\"># here, it is easy to run into numeric instability. Don&#39;t forget the        #</span>\n  <span class=\"c1\"># regularization!                                                           #</span>\n  <span class=\"c1\">#############################################################################</span>\n  <span class=\"p\">(</span><span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">D</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n  <span class=\"n\">C</span> <span class=\"o\">=</span> <span class=\"n\">W</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n  <span class=\"n\">f</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">W</span><span class=\"p\">)</span>\n  <span class=\"c1\">#åœ¨åˆ—æ–¹å‘è¿›è¡ŒæŒ‡æ•°ä¿®æ­£</span>\n  <span class=\"n\">f</span> <span class=\"o\">-=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">max</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">,</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"n\">keepdims</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n  <span class=\"c1\">#æ±‚å¾—softmaxå„ä¸ªç±»çš„æ¦‚ç‡</span>\n  <span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">),</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"n\">keepdims</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n  <span class=\"n\">y_lable</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">N</span><span class=\"p\">,</span><span class=\"n\">C</span><span class=\"p\">))</span>\n  <span class=\"c1\">#y_lableå°±æ˜¯(N,C)ç»´çš„çŸ©é˜µï¼Œæ¯ä¸€è¡Œä¸­åªæœ‰å¯¹åº”çš„é‚£ä¸ªæ­£ç¡®ç±»åˆ« = 1ï¼Œå…¶ä»–éƒ½æ˜¯0</span>\n  <span class=\"n\">y_lable</span><span class=\"p\">[</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"n\">N</span><span class=\"p\">),</span><span class=\"n\">y</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>\n  <span class=\"c1\">#cross entropy</span>\n  <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"o\">-</span><span class=\"mi\">1</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">multiply</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">log</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"p\">),</span><span class=\"n\">y_lable</span><span class=\"p\">))</span> <span class=\"o\">/</span> <span class=\"n\">N</span>\n  <span class=\"n\">loss</span> <span class=\"o\">+=</span> <span class=\"mf\">0.5</span> <span class=\"o\">*</span> <span class=\"n\">reg</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span> <span class=\"n\">W</span> <span class=\"o\">*</span> <span class=\"n\">W</span><span class=\"p\">)</span>\n  <span class=\"c1\">#æ±‚å¯¼å…¬å¼ï¼Œå¾ˆæ¸…æ™°</span>\n  <span class=\"n\">dW</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"o\">-</span><span class=\"n\">y_lable</span><span class=\"p\">)</span>\n  <span class=\"n\">dW</span> <span class=\"o\">/=</span> <span class=\"n\">N</span>\n  <span class=\"n\">dW</span> <span class=\"o\">+=</span> <span class=\"n\">reg</span><span class=\"o\">*</span><span class=\"n\">W</span>\n<span class=\"err\">â€‹</span>\n<span class=\"err\">â€‹</span>\n  <span class=\"c1\">#############################################################################</span>\n  <span class=\"c1\">#                          END OF YOUR CODE                                 #</span>\n  <span class=\"c1\">#############################################################################</span>\n<span class=\"err\">â€‹</span>\n  <span class=\"k\">return</span> <span class=\"n\">loss</span><span class=\"p\">,</span> <span class=\"n\">dW</span>\n<span class=\"err\">â€‹</span>\n<span class=\"err\">â€‹</span></code></pre></div><p>æ£€éªŒä¸€ä¸‹å‘é‡åŒ–å’Œéå‘é‡åŒ–çš„æ—¶é—´ï¼š</p><div class=\"highlight\"><pre><code class=\"language-text\">naive loss: 2.357905e+00 computed in 0.091724s\nvectorized loss: 2.357905e+00 computed in 0.002995s\nLoss difference: 0.000000\nGradient difference: 0.000000</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>softmaxçš„å‡½æ•°å·²ç»ç¼–å†™å®Œæˆäº†ï¼Œæ¥ä¸‹æ¥è°ƒä¸€ä¸‹å­¦ä¹ ç‡å’Œæ­£åˆ™åŒ–ä¸¤ä¸ªè¶…å‚æ•°ï¼š</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># rates and regularization strengths; if you are careful you should be able to</span>\n<span class=\"c1\"># get a classification accuracy of over 0.35 on the validation set.</span>\n<span class=\"kn\">from</span> <span class=\"nn\">cs231n.classifiers</span> <span class=\"kn\">import</span> <span class=\"n\">Softmax</span>\n<span class=\"n\">results</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>\n<span class=\"n\">best_val</span> <span class=\"o\">=</span> <span class=\"o\">-</span><span class=\"mi\">1</span>\n<span class=\"n\">best_softmax</span> <span class=\"o\">=</span> <span class=\"bp\">None</span>\n<span class=\"n\">learning_rates</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mf\">1e-7</span><span class=\"p\">,</span> <span class=\"mf\">5e-7</span><span class=\"p\">]</span>\n<span class=\"n\">regularization_strengths</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mf\">2.5e4</span><span class=\"p\">,</span> <span class=\"mf\">5e4</span><span class=\"p\">]</span>\n<span class=\"err\">â€‹</span>\n<span class=\"c1\">################################################################################</span>\n<span class=\"c1\"># TODO:                                                                        #</span>\n<span class=\"c1\"># Use the validation set to set the learning rate and regularization strength. #</span>\n<span class=\"c1\"># This should be identical to the validation that you did for the SVM; save    #</span>\n<span class=\"c1\"># the best trained softmax classifer in best_softmax.                          #</span>\n<span class=\"c1\">################################################################################</span>\n<span class=\"k\">for</span> <span class=\"n\">lr</span> <span class=\"ow\">in</span> <span class=\"n\">learning_rates</span><span class=\"p\">:</span>\n    <span class=\"k\">for</span> <span class=\"n\">reg</span> <span class=\"ow\">in</span> <span class=\"n\">regularization_strengths</span><span class=\"p\">:</span>\n        <span class=\"n\">softmax</span> <span class=\"o\">=</span> <span class=\"n\">Softmax</span><span class=\"p\">()</span>\n        <span class=\"n\">loss_hist</span> <span class=\"o\">=</span> <span class=\"n\">softmax</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">,</span> <span class=\"n\">y_train</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span><span class=\"o\">=</span><span class=\"n\">lr</span><span class=\"p\">,</span> <span class=\"n\">reg</span><span class=\"o\">=</span><span class=\"n\">reg</span><span class=\"p\">,</span>\n                      <span class=\"n\">num_iters</span><span class=\"o\">=</span><span class=\"mi\">1500</span><span class=\"p\">,</span> <span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n        <span class=\"n\">y_train_pred</span> <span class=\"o\">=</span> <span class=\"n\">softmax</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">)</span>\n        <span class=\"n\">y_val_pred</span> <span class=\"o\">=</span> <span class=\"n\">softmax</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">X_val</span><span class=\"p\">)</span>\n        <span class=\"n\">y_train_acc</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">y_train_pred</span><span class=\"o\">==</span><span class=\"n\">y_train</span><span class=\"p\">)</span>\n        <span class=\"n\">y_val_acc</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">y_val_pred</span><span class=\"o\">==</span><span class=\"n\">y_val</span><span class=\"p\">)</span>\n        <span class=\"n\">results</span><span class=\"p\">[(</span><span class=\"n\">lr</span><span class=\"p\">,</span><span class=\"n\">reg</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">y_train_acc</span><span class=\"p\">,</span> <span class=\"n\">y_val_acc</span><span class=\"p\">]</span>\n        <span class=\"k\">if</span> <span class=\"n\">y_val_acc</span> <span class=\"o\">&gt;</span> <span class=\"n\">best_val</span><span class=\"p\">:</span>\n            <span class=\"n\">best_val</span> <span class=\"o\">=</span> <span class=\"n\">y_val_acc</span>\n            <span class=\"n\">best_softmax</span> <span class=\"o\">=</span> <span class=\"n\">softmax</span>\n<span class=\"c1\">################################################################################</span>\n<span class=\"c1\">#                              END OF YOUR CODE                                #</span>\n<span class=\"c1\">################################################################################</span>\n    \n<span class=\"c1\"># Print out results.</span>\n<span class=\"k\">for</span> <span class=\"n\">lr</span><span class=\"p\">,</span> <span class=\"n\">reg</span> <span class=\"ow\">in</span> <span class=\"nb\">sorted</span><span class=\"p\">(</span><span class=\"n\">results</span><span class=\"p\">):</span>\n    <span class=\"n\">train_accuracy</span><span class=\"p\">,</span> <span class=\"n\">val_accuracy</span> <span class=\"o\">=</span> <span class=\"n\">results</span><span class=\"p\">[(</span><span class=\"n\">lr</span><span class=\"p\">,</span> <span class=\"n\">reg</span><span class=\"p\">)]</span>\n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;lr </span><span class=\"si\">%e</span><span class=\"s1\"> reg </span><span class=\"si\">%e</span><span class=\"s1\"> train accuracy: </span><span class=\"si\">%f</span><span class=\"s1\"> val accuracy: </span><span class=\"si\">%f</span><span class=\"s1\">&#39;</span> <span class=\"o\">%</span> <span class=\"p\">(</span>\n                <span class=\"n\">lr</span><span class=\"p\">,</span> <span class=\"n\">reg</span><span class=\"p\">,</span> <span class=\"n\">train_accuracy</span><span class=\"p\">,</span> <span class=\"n\">val_accuracy</span><span class=\"p\">))</span>\n    \n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;best validation accuracy achieved during cross-validation: </span><span class=\"si\">%f</span><span class=\"s1\">&#39;</span> <span class=\"o\">%</span> <span class=\"n\">best_val</span><span class=\"p\">)</span>\n<span class=\"n\">lr</span> <span class=\"mf\">1.000000e-07</span> <span class=\"n\">reg</span> <span class=\"mf\">2.500000e+04</span> <span class=\"n\">train</span> <span class=\"n\">accuracy</span><span class=\"p\">:</span> <span class=\"mf\">0.350592</span> <span class=\"n\">val</span> <span class=\"n\">accuracy</span><span class=\"p\">:</span> <span class=\"mf\">0.354000</span>\n<span class=\"n\">lr</span> <span class=\"mf\">1.000000e-07</span> <span class=\"n\">reg</span> <span class=\"mf\">5.000000e+04</span> <span class=\"n\">train</span> <span class=\"n\">accuracy</span><span class=\"p\">:</span> <span class=\"mf\">0.329551</span> <span class=\"n\">val</span> <span class=\"n\">accuracy</span><span class=\"p\">:</span> <span class=\"mf\">0.342000</span>\n<span class=\"n\">lr</span> <span class=\"mf\">5.000000e-07</span> <span class=\"n\">reg</span> <span class=\"mf\">2.500000e+04</span> <span class=\"n\">train</span> <span class=\"n\">accuracy</span><span class=\"p\">:</span> <span class=\"mf\">0.347286</span> <span class=\"n\">val</span> <span class=\"n\">accuracy</span><span class=\"p\">:</span> <span class=\"mf\">0.359000</span>\n<span class=\"n\">lr</span> <span class=\"mf\">5.000000e-07</span> <span class=\"n\">reg</span> <span class=\"mf\">5.000000e+04</span> <span class=\"n\">train</span> <span class=\"n\">accuracy</span><span class=\"p\">:</span> <span class=\"mf\">0.328551</span> <span class=\"n\">val</span> <span class=\"n\">accuracy</span><span class=\"p\">:</span> <span class=\"mf\">0.337000</span>\n<span class=\"n\">best</span> <span class=\"n\">validation</span> <span class=\"n\">accuracy</span> <span class=\"n\">achieved</span> <span class=\"n\">during</span> <span class=\"n\">cross</span><span class=\"o\">-</span><span class=\"n\">validation</span><span class=\"p\">:</span> <span class=\"mf\">0.359000</span></code></pre></div><p></p>", 
            "topic": [
                {
                    "tag": "ç§‘æŠ€", 
                    "tagLink": "https://api.zhihu.com/topics/19556664"
                }, 
                {
                    "tag": "ä½œä¸š", 
                    "tagLink": "https://api.zhihu.com/topics/19586372"
                }
            ], 
            "comments": [
                {
                    "userName": "è’‹æµ©æ–‡", 
                    "userLink": "https://www.zhihu.com/people/82cfd3c86ef8ef5aeb384c1805b5e771", 
                    "content": "å¤§ä½¬ï¼Œå»ºè®®åšå®¢ä¸Šé¢çš„å¯ä»¥ä¼ åˆ°çŸ¥ä¹ä¸Šã€‚ã€‚ã€‚ã€‚è™½ç„¶å¥½åƒæ²¡ä»€ä¹ˆäººçœ‹ï¼Œä½†æ„Ÿè§‰è¿˜æ˜¯æŒºæœ‰å¸®åŠ©çš„ã€‚ã€‚ã€‚ã€‚", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "ç›´ä¸Šäº‘éœ„", 
                            "userLink": "https://www.zhihu.com/people/1033165ce4ad9c3fce69a0793dfab8ad", 
                            "content": "è°¢è°¢ï¼çŸ¥ä¹åªè½¬äº†ä¸€éƒ¨åˆ†ï¼Œå› ä¸ºå…¬å¼éƒ½è¦é‡æ–°æ•²ï¼Œè€Œä¸”æœ‰å­—æ•°é™åˆ¶ã€‚æœ€è¿‘å¿™ç€å®ä¹ éƒ½æ²¡ç©ºå†™äº†", 
                            "likes": 0, 
                            "replyToAuthor": "è’‹æµ©æ–‡"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/45753542", 
            "userName": "ç›´ä¸Šäº‘éœ„", 
            "userLink": "https://www.zhihu.com/people/1033165ce4ad9c3fce69a0793dfab8ad", 
            "upvote": 14, 
            "title": "cs231nä½œä¸šï¼šassignment1 - svm", 
            "content": "<div class=\"highlight\"><pre><code class=\"language-text\">title: &#39;cs231nä½œä¸šï¼šassignment1 - svm&#39;\nid: cs231n-1h-2\ntags:\n  - cs231n\n  - homework\ncategories:\n  - AI\n  - Deep Learning\ndate: 2018-09-27 14:17:45\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>GitHubåœ°å€ï¼š<a href=\"https://link.zhihu.com/?target=https%3A//github.com/ZJUFangzh/cs231n\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/ZJUFangzh/cs</span><span class=\"invisible\">231n</span><span class=\"ellipsis\"></span></a></p><p class=\"ztext-empty-paragraph\"><br/></p><p>é¦–å‘äºä¸ªäººåšå®¢: <a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Fangzhçš„ä¸ªäººåšå®¢ | äººå·¥æ™ºèƒ½æ‹¯æ•‘ä¸–ç•Œ</a> æ¬¢è¿æ¥è®¿</p><p>å®Œæˆäº†ä¸€ä¸ªåŸºäºSVMçš„æŸå¤±å‡½æ•°ã€‚</p><p>&lt;!--more--&gt;</p><h2><b>æ•°æ®é›†</b></h2><p>è½½å…¥çš„æ•°æ®é›†ä¾æ—§æ˜¯:</p><div class=\"highlight\"><pre><code class=\"language-text\">Train data shape:  (49000, 32, 32, 3)\nTrain labels shape:  (49000,)\nValidation data shape:  (1000, 32, 32, 3)\nValidation labels shape:  (1000,)\nTest data shape:  (1000, 32, 32, 3)\nTest labels shape:  (1000,)</code></pre></div><p>è€Œåè¿›è¡Œ32 * 32 * 3çš„å›¾åƒæ‹‰ä¼¸ï¼Œå¾—åˆ°ï¼š</p><div class=\"highlight\"><pre><code class=\"language-text\">Training data shape:  (49000, 3072)\nValidation data shape:  (1000, 3072)\nTest data shape:  (1000, 3072)\ndev data shape:  (500, 3072)</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>è¿›è¡Œä¸€ä¸‹ç®€å•çš„é¢„å¤„ç†ï¼Œå‡å»å›¾åƒçš„å¹³å‡å€¼</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># Preprocessing: subtract the mean image</span>\n<span class=\"c1\"># first: compute the image mean based on the training data</span>\n<span class=\"n\">mean_image</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">mean_image</span><span class=\"p\">[:</span><span class=\"mi\">10</span><span class=\"p\">])</span> <span class=\"c1\"># print a few of the elements</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">figure</span><span class=\"p\">(</span><span class=\"n\">figsize</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">4</span><span class=\"p\">,</span><span class=\"mi\">4</span><span class=\"p\">))</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">imshow</span><span class=\"p\">(</span><span class=\"n\">mean_image</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">((</span><span class=\"mi\">32</span><span class=\"p\">,</span><span class=\"mi\">32</span><span class=\"p\">,</span><span class=\"mi\">3</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">astype</span><span class=\"p\">(</span><span class=\"s1\">&#39;uint8&#39;</span><span class=\"p\">))</span> <span class=\"c1\"># visualize the mean image</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>\n<span class=\"c1\"># second: subtract the mean image from train and test data</span>\n<span class=\"n\">X_train</span> <span class=\"o\">-=</span> <span class=\"n\">mean_image</span>\n<span class=\"n\">X_val</span> <span class=\"o\">-=</span> <span class=\"n\">mean_image</span>\n<span class=\"n\">X_test</span> <span class=\"o\">-=</span> <span class=\"n\">mean_image</span>\n<span class=\"n\">X_dev</span> <span class=\"o\">-=</span> <span class=\"n\">mean_image</span>\n<span class=\"c1\"># third: append the bias dimension of ones (i.e. bias trick) so that our SVM</span>\n<span class=\"c1\"># only has to worry about optimizing a single weight matrix W.</span>\n<span class=\"n\">X_train</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">hstack</span><span class=\"p\">([</span><span class=\"n\">X_train</span><span class=\"p\">,</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">ones</span><span class=\"p\">((</span><span class=\"n\">X_train</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"mi\">1</span><span class=\"p\">))])</span>\n<span class=\"n\">X_val</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">hstack</span><span class=\"p\">([</span><span class=\"n\">X_val</span><span class=\"p\">,</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">ones</span><span class=\"p\">((</span><span class=\"n\">X_val</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"mi\">1</span><span class=\"p\">))])</span>\n<span class=\"n\">X_test</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">hstack</span><span class=\"p\">([</span><span class=\"n\">X_test</span><span class=\"p\">,</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">ones</span><span class=\"p\">((</span><span class=\"n\">X_test</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"mi\">1</span><span class=\"p\">))])</span>\n<span class=\"n\">X_dev</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">hstack</span><span class=\"p\">([</span><span class=\"n\">X_dev</span><span class=\"p\">,</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">ones</span><span class=\"p\">((</span><span class=\"n\">X_dev</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"mi\">1</span><span class=\"p\">))])</span>\n<span class=\"err\">â€‹</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">,</span> <span class=\"n\">X_val</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">,</span> <span class=\"n\">X_test</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">,</span> <span class=\"n\">X_dev</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span>\n<span class=\"p\">(</span><span class=\"mi\">49000</span><span class=\"p\">,</span> <span class=\"mi\">3073</span><span class=\"p\">)</span> <span class=\"p\">(</span><span class=\"mi\">1000</span><span class=\"p\">,</span> <span class=\"mi\">3073</span><span class=\"p\">)</span> <span class=\"p\">(</span><span class=\"mi\">1000</span><span class=\"p\">,</span> <span class=\"mi\">3073</span><span class=\"p\">)</span> <span class=\"p\">(</span><span class=\"mi\">500</span><span class=\"p\">,</span> <span class=\"mi\">3073</span><span class=\"p\">)</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>SVMåˆ†ç±»å™¨</b></h2><p>ç„¶åå°±å¯ä»¥å¼€å§‹æ¥ç¼–å†™<code>cs231n/classifiers/linear_svm.py</code>çš„SVMåˆ†ç±»å™¨äº†ã€‚åœ¨è¿™é‡Œå…ˆä»‹ç»ä¸€ä¸‹SVMçš„åŸºæœ¬å…¬å¼å’ŒåŸç†ã€‚</p><p>å‚è€ƒ<a href=\"https://link.zhihu.com/?target=https%3A//www.tinymind.cn/articles/404\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">CS231n:çº¿æ€§åˆ†ç±»</a></p><p>SVMæŸå¤±å‡½æ•°æƒ³è¦SVMåœ¨æ­£ç¡®åˆ†ç±»ä¸Šçš„æ¯”åˆ†å§‹ç»ˆæ¯”ä¸æ­£ç¡®çš„æ¯”åˆ†é«˜å‡ºä¸€ä¸ªè¾¹ç•Œå€¼$\\triangle$</p><p>ç¬¬iä¸ªæ•°æ®å›¾åƒä¸º <img src=\"https://www.zhihu.com/equation?tex=x_i\" alt=\"x_i\" eeimg=\"1\"/> ï¼Œæ­£ç¡®åˆ†ç±»ä¸º <img src=\"https://www.zhihu.com/equation?tex=y_i\" alt=\"y_i\" eeimg=\"1\"/> ï¼Œç„¶åæ ¹æ® <img src=\"https://www.zhihu.com/equation?tex=f%28x_i%2CW%29\" alt=\"f(x_i,W)\" eeimg=\"1\"/> æ¥è®¡ç®—ä¸åŒåˆ†ç±»çš„å€¼ï¼Œå°†åˆ†ç±»ç®€å†™ä¸º <img src=\"https://www.zhihu.com/equation?tex=s\" alt=\"s\" eeimg=\"1\"/> ï¼Œé‚£ä¹ˆç¬¬jç±»çš„å¾—åˆ†å°±æ˜¯ <img src=\"https://www.zhihu.com/equation?tex=s_j+%3D+f%28x_i%2CW%29_j\" alt=\"s_j = f(x_i,W)_j\" eeimg=\"1\"/> ï¼Œé’ˆå¯¹ç¬¬iä¸ªæ•°æ®çš„å¤šç±»SVMçš„æŸå¤±å‡½æ•°å®šä¹‰ä¸ºï¼š</p><p><img src=\"https://www.zhihu.com/equation?tex=L_i+%3D+%5Csum_%7Bj+%5Cneq+y_i%7D+max%280%2C+s_j+-+s_%7By_i%7D+%2B+%5Ctriangle%29\" alt=\"L_i = \\sum_{j \\neq y_i} max(0, s_j - s_{y_i} + \\triangle)\" eeimg=\"1\"/> </p><p>å¦‚ï¼šå‡è®¾æœ‰3ä¸ªåˆ†ç±»ï¼Œ <img src=\"https://www.zhihu.com/equation?tex=s+%3D+%5B+13%2C-7%2C11%5D\" alt=\"s = [ 13,-7,11]\" eeimg=\"1\"/> ï¼Œç¬¬ä¸€ä¸ªåˆ†ç±»æ˜¯æ­£ç¡®çš„ï¼Œä¹Ÿå°±æ˜¯ <img src=\"https://www.zhihu.com/equation?tex=y_i+%3D+0\" alt=\"y_i = 0\" eeimg=\"1\"/> ï¼Œå‡è®¾ <img src=\"https://www.zhihu.com/equation?tex=%5Ctriangle%3D10\" alt=\"\\triangle=10\" eeimg=\"1\"/> ï¼Œé‚£ä¹ˆæŠŠæ‰€æœ‰ä¸æ­£ç¡®çš„åˆ†ç±»åŠ èµ·æ¥( <img src=\"https://www.zhihu.com/equation?tex=j+%5Cneq+y_i\" alt=\"j \\neq y_i\" eeimg=\"1\"/> )ï¼Œ</p><p><img src=\"https://www.zhihu.com/equation?tex=L_i+%3D+max%280%2C-7-13%2B10%29%2Bmax%280%2C11-13%2B10%29\" alt=\"L_i = max(0,-7-13+10)+max(0,11-13+10)\" eeimg=\"1\"/> </p><p>å› ä¸ºSVMåªå…³å¿ƒå·®è·è‡³å°‘è¦å¤§äº10ï¼Œæ‰€ä»¥ <img src=\"https://www.zhihu.com/equation?tex=L_i+%3D+8\" alt=\"L_i = 8\" eeimg=\"1\"/> </p><p>é‚£ä¹ˆæŠŠå…¬å¼å¥—å…¥ï¼š</p><p><img src=\"https://www.zhihu.com/equation?tex=L_i+%3D+%5Csum_%7Bj+%5Cneq+y_i%7D+max%280%2C+w_j+x_i+-+w_%7By_i%7D+x_i+%2B+%5Ctriangle%29\" alt=\"L_i = \\sum_{j \\neq y_i} max(0, w_j x_i - w_{y_i} x_i + \\triangle)\" eeimg=\"1\"/> </p><p>åŠ å…¥æ­£åˆ™åï¼š</p><p><img src=\"https://www.zhihu.com/equation?tex=L+%3D+%5Cfrac%7B1%7D%7BN%7D+%5Csum_i+%5Csum_%7Bj+%5Cneq+y_i%7Dmax%280%2C+f%28x_i+%3BW%29_%7Bj%7D+-+f%28x_i+%3B+W%29%7By_i%7D+%2B+%5Ctriangle%29+%2B+%5Clambda+%5Csum_k+%5Csum_l+W%5E%7B2%7D_%7Bk%2Cl%7D+\" alt=\"L = \\frac{1}{N} \\sum_i \\sum_{j \\neq y_i}max(0, f(x_i ;W)_{j} - f(x_i ; W){y_i} + \\triangle) + \\lambda \\sum_k \\sum_l W^{2}_{k,l} \" eeimg=\"1\"/> </p><p>åˆ°ç›®å‰ä¸ºæ­¢è®¡ç®—äº†lossï¼Œç„¶åè¿˜éœ€è¦è®¡ç®—æ¢¯åº¦ä¸‹é™çš„gradsï¼Œ</p><p>å®˜æ–¹å¹¶æ²¡æœ‰ç»™æ¨å¯¼è¿‡ç¨‹ï¼Œè¿™æ‰æ˜¯cs231nä½œä¸šéš¾çš„åœ°æ–¹æ‰€åœ¨ã€‚ã€‚ã€‚</p><p>è¯¦ç»†å¯ä»¥çœ‹è¿™ä¸€ç¯‡æ–‡ç« <a href=\"https://zhuanlan.zhihu.com/p/37068455\" class=\"internal\">CS 231 SVM æ±‚å¯¼</a></p><p>æ€»ä¹‹å°±æ˜¯ä¸¤ä¸ªå…¬å¼ï¼š</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-48edb635d8dcccc1df161dbcba60c9da_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"731\" data-rawheight=\"730\" class=\"origin_image zh-lightbox-thumb\" width=\"731\" data-original=\"https://pic3.zhimg.com/v2-48edb635d8dcccc1df161dbcba60c9da_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;731&#39; height=&#39;730&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"731\" data-rawheight=\"730\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"731\" data-original=\"https://pic3.zhimg.com/v2-48edb635d8dcccc1df161dbcba60c9da_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-48edb635d8dcccc1df161dbcba60c9da_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>è€Œåå¼€å§‹ç¼–å†™<code>compute_loss_naive</code> å‡½æ•°ï¼Œå…ˆç”¨å¾ªç¯æ¥æ„Ÿå—ä¸€ä¸‹ï¼š</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">svm_loss_naive</span><span class=\"p\">(</span><span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">reg</span><span class=\"p\">):</span>\n  <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">  Structured SVM loss function, naive implementation (with loops).\n</span><span class=\"s2\">â€‹\n</span><span class=\"s2\">  Inputs have dimension D, there are C classes, and we operate on minibatches\n</span><span class=\"s2\">  of N examples.\n</span><span class=\"s2\">â€‹\n</span><span class=\"s2\">  Inputs:\n</span><span class=\"s2\">  - W: A numpy array of shape (D, C) containing weights.\n</span><span class=\"s2\">  - X: A numpy array of shape (N, D) containing a minibatch of data.\n</span><span class=\"s2\">  - y: A numpy array of shape (N,) containing training labels; y[i] = c means\n</span><span class=\"s2\">    that X[i] has label c, where 0 &lt;= c &lt; C.\n</span><span class=\"s2\">  - reg: (float) regularization strength\n</span><span class=\"s2\">â€‹\n</span><span class=\"s2\">  Returns a tuple of:\n</span><span class=\"s2\">  - loss as single float\n</span><span class=\"s2\">  - gradient with respect to weights W; an array of same shape as W\n</span><span class=\"s2\">  &#34;&#34;&#34;</span>\n  <span class=\"n\">dW</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"n\">W</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span> <span class=\"c1\"># initialize the gradient as zero</span>\n<span class=\"err\">â€‹</span>\n  <span class=\"c1\"># compute the loss and the gradient</span>\n  <span class=\"n\">num_classes</span> <span class=\"o\">=</span> <span class=\"n\">W</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n  <span class=\"n\">num_train</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n  <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span>\n  <span class=\"c1\">#é€ä¸ªè®¡ç®—æ¯ä¸ªæ ·æœ¬çš„loss</span>\n  <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">xrange</span><span class=\"p\">(</span><span class=\"n\">num_train</span><span class=\"p\">):</span>\n    <span class=\"c1\">#è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„å„ä¸ªåˆ†ç±»å¾—åˆ†</span>\n    <span class=\"n\">scores</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">W</span><span class=\"p\">)</span>\n    <span class=\"n\">correct_class_score</span> <span class=\"o\">=</span> <span class=\"n\">scores</span><span class=\"p\">[</span><span class=\"n\">y</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]]</span>\n    <span class=\"c1\">#è®¡ç®—æ¯ä¸ªåˆ†ç±»çš„å¾—åˆ†ï¼Œè®¡å…¥lossä¸­</span>\n    <span class=\"k\">for</span> <span class=\"n\">j</span> <span class=\"ow\">in</span> <span class=\"nb\">xrange</span><span class=\"p\">(</span><span class=\"n\">num_classes</span><span class=\"p\">):</span>\n      <span class=\"c1\"># æ ¹æ®å…¬å¼ï¼Œj==y[i]çš„å°±æ˜¯æœ¬èº«çš„åˆ†ç±»ï¼Œä¸ç”¨ç®—äº†</span>\n      <span class=\"k\">if</span> <span class=\"n\">j</span> <span class=\"o\">==</span> <span class=\"n\">y</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]:</span>\n        <span class=\"k\">continue</span>\n      <span class=\"n\">margin</span> <span class=\"o\">=</span> <span class=\"n\">scores</span><span class=\"p\">[</span><span class=\"n\">j</span><span class=\"p\">]</span> <span class=\"o\">-</span> <span class=\"n\">correct_class_score</span> <span class=\"o\">+</span> <span class=\"mi\">1</span> <span class=\"c1\"># note delta = 1</span>\n      <span class=\"c1\">#å¦‚æœè®¡ç®—çš„margin &gt; 0ï¼Œé‚£ä¹ˆå°±è¦ç®—å…¥lossï¼Œ</span>\n      <span class=\"k\">if</span> <span class=\"n\">margin</span> <span class=\"o\">&gt;</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n        <span class=\"n\">loss</span> <span class=\"o\">+=</span> <span class=\"n\">margin</span>\n        <span class=\"c1\">#å…¬å¼2</span>\n        <span class=\"n\">dW</span><span class=\"p\">[:,</span><span class=\"n\">y</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]]</span> <span class=\"o\">+=</span> <span class=\"o\">-</span><span class=\"n\">X</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">,:]</span><span class=\"o\">.</span><span class=\"n\">T</span>\n        <span class=\"c1\">#å…¬å¼1</span>\n        <span class=\"n\">dW</span><span class=\"p\">[:,</span><span class=\"n\">j</span><span class=\"p\">]</span> <span class=\"o\">+=</span> <span class=\"n\">X</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"p\">:]</span><span class=\"o\">.</span><span class=\"n\">T</span>\n  <span class=\"c1\"># Right now the loss is a sum over all training examples, but we want it</span>\n  <span class=\"c1\"># to be an average instead so we divide by num_train.</span>\n  <span class=\"n\">loss</span> <span class=\"o\">/=</span> <span class=\"n\">num_train</span>\n  <span class=\"n\">dW</span> <span class=\"o\">/=</span> <span class=\"n\">num_train</span>\n  <span class=\"c1\"># Add regularization to the loss.</span>\n  <span class=\"n\">loss</span> <span class=\"o\">+=</span> <span class=\"n\">reg</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">W</span> <span class=\"o\">*</span> <span class=\"n\">W</span><span class=\"p\">)</span>\n  <span class=\"n\">dW</span> <span class=\"o\">+=</span> <span class=\"n\">reg</span> <span class=\"o\">*</span> <span class=\"n\">W</span>\n  <span class=\"c1\">#############################################################################</span>\n  <span class=\"c1\"># TODO:                                                                     #</span>\n  <span class=\"c1\"># Compute the gradient of the loss function and store it dW.                #</span>\n  <span class=\"c1\"># Rather that first computing the loss and then computing the derivative,   #</span>\n  <span class=\"c1\"># it may be simpler to compute the derivative at the same time that the     #</span>\n  <span class=\"c1\"># loss is being computed. As a result you may need to modify some of the    #</span>\n  <span class=\"c1\"># code above to compute the gradient.                                       #</span>\n  <span class=\"c1\">#############################################################################</span>\n<span class=\"err\">â€‹</span>\n<span class=\"err\">â€‹</span>\n  <span class=\"k\">return</span> <span class=\"n\">loss</span><span class=\"p\">,</span> <span class=\"n\">dW</span>\n<span class=\"err\">â€‹</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>å†™å®Œåï¼Œç”¨æ¢¯åº¦æ£€éªŒæ£€æŸ¥ä¸€ä¸‹:</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># Once you&#39;ve implemented the gradient, recompute it with the code below</span>\n<span class=\"c1\"># and gradient check it with the function we provided for you</span>\n<span class=\"err\">â€‹</span>\n<span class=\"c1\"># Compute the loss and its gradient at W.</span>\n<span class=\"n\">loss</span><span class=\"p\">,</span> <span class=\"n\">grad</span> <span class=\"o\">=</span> <span class=\"n\">svm_loss_naive</span><span class=\"p\">(</span><span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">X_dev</span><span class=\"p\">,</span> <span class=\"n\">y_dev</span><span class=\"p\">,</span> <span class=\"mf\">0.0</span><span class=\"p\">)</span>\n<span class=\"err\">â€‹</span>\n<span class=\"c1\"># Numerically compute the gradient along several randomly chosen dimensions, and</span>\n<span class=\"c1\"># compare them with your analytically computed gradient. The numbers should match</span>\n<span class=\"c1\"># almost exactly along all dimensions.</span>\n<span class=\"kn\">from</span> <span class=\"nn\">cs231n.gradient_check</span> <span class=\"kn\">import</span> <span class=\"n\">grad_check_sparse</span>\n<span class=\"n\">f</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span> <span class=\"n\">w</span><span class=\"p\">:</span> <span class=\"n\">svm_loss_naive</span><span class=\"p\">(</span><span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">X_dev</span><span class=\"p\">,</span> <span class=\"n\">y_dev</span><span class=\"p\">,</span> <span class=\"mf\">0.0</span><span class=\"p\">)[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n<span class=\"n\">grad_numerical</span> <span class=\"o\">=</span> <span class=\"n\">grad_check_sparse</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">,</span> <span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">grad</span><span class=\"p\">)</span>\n<span class=\"err\">â€‹</span>\n<span class=\"c1\"># do the gradient check once again with regularization turned on</span>\n<span class=\"c1\"># you didn&#39;t forget the regularization gradient did you?</span>\n<span class=\"n\">loss</span><span class=\"p\">,</span> <span class=\"n\">grad</span> <span class=\"o\">=</span> <span class=\"n\">svm_loss_naive</span><span class=\"p\">(</span><span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">X_dev</span><span class=\"p\">,</span> <span class=\"n\">y_dev</span><span class=\"p\">,</span> <span class=\"mf\">5e1</span><span class=\"p\">)</span>\n<span class=\"n\">f</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span> <span class=\"n\">w</span><span class=\"p\">:</span> <span class=\"n\">svm_loss_naive</span><span class=\"p\">(</span><span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">X_dev</span><span class=\"p\">,</span> <span class=\"n\">y_dev</span><span class=\"p\">,</span> <span class=\"mf\">5e1</span><span class=\"p\">)[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n<span class=\"n\">grad_numerical</span> <span class=\"o\">=</span> <span class=\"n\">grad_check_sparse</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">,</span> <span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">grad</span><span class=\"p\">)</span>\n<span class=\"n\">numerical</span><span class=\"p\">:</span> <span class=\"mf\">34.663598</span> <span class=\"n\">analytic</span><span class=\"p\">:</span> <span class=\"mf\">34.663598</span><span class=\"p\">,</span> <span class=\"n\">relative</span> <span class=\"n\">error</span><span class=\"p\">:</span> <span class=\"mf\">6.995024e-13</span>\n<span class=\"n\">numerical</span><span class=\"p\">:</span> <span class=\"mf\">21.043334</span> <span class=\"n\">analytic</span><span class=\"p\">:</span> <span class=\"mf\">21.043334</span><span class=\"p\">,</span> <span class=\"n\">relative</span> <span class=\"n\">error</span><span class=\"p\">:</span> <span class=\"mf\">5.147242e-12</span>\n<span class=\"n\">numerical</span><span class=\"p\">:</span> <span class=\"mf\">1.334055</span> <span class=\"n\">analytic</span><span class=\"p\">:</span> <span class=\"mf\">1.334055</span><span class=\"p\">,</span> <span class=\"n\">relative</span> <span class=\"n\">error</span><span class=\"p\">:</span> <span class=\"mf\">5.315420e-11</span>\n<span class=\"n\">numerical</span><span class=\"p\">:</span> <span class=\"mf\">16.611704</span> <span class=\"n\">analytic</span><span class=\"p\">:</span> <span class=\"mf\">16.611704</span><span class=\"p\">,</span> <span class=\"n\">relative</span> <span class=\"n\">error</span><span class=\"p\">:</span> <span class=\"mf\">6.908581e-12</span>\n<span class=\"n\">numerical</span><span class=\"p\">:</span> <span class=\"mf\">25.327188</span> <span class=\"n\">analytic</span><span class=\"p\">:</span> <span class=\"mf\">25.327188</span><span class=\"p\">,</span> <span class=\"n\">relative</span> <span class=\"n\">error</span><span class=\"p\">:</span> <span class=\"mf\">1.552987e-11</span>\n<span class=\"n\">numerical</span><span class=\"p\">:</span> <span class=\"o\">-</span><span class=\"mf\">12.867717</span> <span class=\"n\">analytic</span><span class=\"p\">:</span> <span class=\"o\">-</span><span class=\"mf\">12.867717</span><span class=\"p\">,</span> <span class=\"n\">relative</span> <span class=\"n\">error</span><span class=\"p\">:</span> <span class=\"mf\">1.966004e-11</span>\n<span class=\"n\">numerical</span><span class=\"p\">:</span> <span class=\"mf\">15.066285</span> <span class=\"n\">analytic</span><span class=\"p\">:</span> <span class=\"mf\">15.066285</span><span class=\"p\">,</span> <span class=\"n\">relative</span> <span class=\"n\">error</span><span class=\"p\">:</span> <span class=\"mf\">7.012975e-12</span>\n<span class=\"n\">numerical</span><span class=\"p\">:</span> <span class=\"o\">-</span><span class=\"mf\">3.752014</span> <span class=\"n\">analytic</span><span class=\"p\">:</span> <span class=\"o\">-</span><span class=\"mf\">3.752014</span><span class=\"p\">,</span> <span class=\"n\">relative</span> <span class=\"n\">error</span><span class=\"p\">:</span> <span class=\"mf\">7.502607e-11</span>\n<span class=\"n\">numerical</span><span class=\"p\">:</span> <span class=\"mf\">9.927043</span> <span class=\"n\">analytic</span><span class=\"p\">:</span> <span class=\"mf\">9.927043</span><span class=\"p\">,</span> <span class=\"n\">relative</span> <span class=\"n\">error</span><span class=\"p\">:</span> <span class=\"mf\">9.010584e-13</span>\n<span class=\"n\">numerical</span><span class=\"p\">:</span> <span class=\"mf\">33.071345</span> <span class=\"n\">analytic</span><span class=\"p\">:</span> <span class=\"mf\">33.071345</span><span class=\"p\">,</span> <span class=\"n\">relative</span> <span class=\"n\">error</span><span class=\"p\">:</span> <span class=\"mf\">1.305438e-12</span>\n<span class=\"n\">numerical</span><span class=\"p\">:</span> <span class=\"o\">-</span><span class=\"mf\">19.227144</span> <span class=\"n\">analytic</span><span class=\"p\">:</span> <span class=\"o\">-</span><span class=\"mf\">19.227851</span><span class=\"p\">,</span> <span class=\"n\">relative</span> <span class=\"n\">error</span><span class=\"p\">:</span> <span class=\"mf\">1.836495e-05</span>\n<span class=\"n\">numerical</span><span class=\"p\">:</span> <span class=\"mf\">31.392728</span> <span class=\"n\">analytic</span><span class=\"p\">:</span> <span class=\"mf\">31.391611</span><span class=\"p\">,</span> <span class=\"n\">relative</span> <span class=\"n\">error</span><span class=\"p\">:</span> <span class=\"mf\">1.778034e-05</span>\n<span class=\"n\">numerical</span><span class=\"p\">:</span> <span class=\"o\">-</span><span class=\"mf\">10.450509</span> <span class=\"n\">analytic</span><span class=\"p\">:</span> <span class=\"o\">-</span><span class=\"mf\">10.456860</span><span class=\"p\">,</span> <span class=\"n\">relative</span> <span class=\"n\">error</span><span class=\"p\">:</span> <span class=\"mf\">3.037629e-04</span>\n<span class=\"n\">numerical</span><span class=\"p\">:</span> <span class=\"o\">-</span><span class=\"mf\">1.346690</span> <span class=\"n\">analytic</span><span class=\"p\">:</span> <span class=\"o\">-</span><span class=\"mf\">1.345625</span><span class=\"p\">,</span> <span class=\"n\">relative</span> <span class=\"n\">error</span><span class=\"p\">:</span> <span class=\"mf\">3.953276e-04</span>\n<span class=\"n\">numerical</span><span class=\"p\">:</span> <span class=\"mf\">7.843501</span> <span class=\"n\">analytic</span><span class=\"p\">:</span> <span class=\"mf\">7.846486</span><span class=\"p\">,</span> <span class=\"n\">relative</span> <span class=\"n\">error</span><span class=\"p\">:</span> <span class=\"mf\">1.902216e-04</span>\n<span class=\"n\">numerical</span><span class=\"p\">:</span> <span class=\"mf\">20.635011</span> <span class=\"n\">analytic</span><span class=\"p\">:</span> <span class=\"mf\">20.628368</span><span class=\"p\">,</span> <span class=\"n\">relative</span> <span class=\"n\">error</span><span class=\"p\">:</span> <span class=\"mf\">1.609761e-04</span>\n<span class=\"n\">numerical</span><span class=\"p\">:</span> <span class=\"mf\">23.654254</span> <span class=\"n\">analytic</span><span class=\"p\">:</span> <span class=\"mf\">23.652696</span><span class=\"p\">,</span> <span class=\"n\">relative</span> <span class=\"n\">error</span><span class=\"p\">:</span> <span class=\"mf\">3.294745e-05</span>\n<span class=\"n\">numerical</span><span class=\"p\">:</span> <span class=\"mf\">37.706709</span> <span class=\"n\">analytic</span><span class=\"p\">:</span> <span class=\"mf\">37.703260</span><span class=\"p\">,</span> <span class=\"n\">relative</span> <span class=\"n\">error</span><span class=\"p\">:</span> <span class=\"mf\">4.573495e-05</span>\n<span class=\"n\">numerical</span><span class=\"p\">:</span> <span class=\"mf\">9.558804</span> <span class=\"n\">analytic</span><span class=\"p\">:</span> <span class=\"mf\">9.566079</span><span class=\"p\">,</span> <span class=\"n\">relative</span> <span class=\"n\">error</span><span class=\"p\">:</span> <span class=\"mf\">3.804143e-04</span>\n<span class=\"n\">numerical</span><span class=\"p\">:</span> <span class=\"mf\">20.450011</span> <span class=\"n\">analytic</span><span class=\"p\">:</span> <span class=\"mf\">20.451451</span><span class=\"p\">,</span> <span class=\"n\">relative</span> <span class=\"n\">error</span><span class=\"p\">:</span> <span class=\"mf\">3.521650e-05</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><b>å‘é‡åŒ–SVM</b></p><p>å¥—å¾ªç¯è‚¯å®šæ˜¯æœ€èœçš„åšæ³•ï¼Œæˆ‘ä»¬åœ¨å¤„ç†å›¾åƒçš„æ—¶å€™è‚¯å®šéƒ½è¦ç”¨çŸ©é˜µç®—çš„ï¼š</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">svm_loss_vectorized</span><span class=\"p\">(</span><span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">reg</span><span class=\"p\">):</span>\n  <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">  Structured SVM loss function, vectorized implementation.\n</span><span class=\"s2\">â€‹\n</span><span class=\"s2\">  Inputs and outputs are the same as svm_loss_naive.\n</span><span class=\"s2\">  &#34;&#34;&#34;</span>\n  <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span>\n  <span class=\"n\">dW</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"n\">W</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span> <span class=\"c1\"># initialize the gradient as zero</span>\n<span class=\"err\">â€‹</span>\n  <span class=\"c1\">#############################################################################</span>\n  <span class=\"c1\"># TODO:                                                                     #</span>\n  <span class=\"c1\"># Implement a vectorized version of the structured SVM loss, storing the    #</span>\n  <span class=\"c1\"># result in loss.                                                           #</span>\n  <span class=\"c1\">#############################################################################</span>\n  <span class=\"c1\">#scores (N,C)</span>\n  <span class=\"n\">scores</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">W</span><span class=\"p\">)</span>\n  <span class=\"c1\">#num_classes = W.shape[1]</span>\n  <span class=\"n\">num_train</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n  <span class=\"c1\">#åˆ©ç”¨np.arange(),correct_class_scoreå˜æˆäº† (num_train,y)çš„çŸ©é˜µ</span>\n  <span class=\"n\">correct_class_score</span> <span class=\"o\">=</span> <span class=\"n\">scores</span><span class=\"p\">[</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"n\">num_train</span><span class=\"p\">),</span><span class=\"n\">y</span><span class=\"p\">]</span>\n  <span class=\"n\">correct_class_score</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"n\">correct_class_score</span><span class=\"p\">,(</span><span class=\"n\">num_train</span><span class=\"p\">,</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">))</span>\n  <span class=\"n\">margins</span> <span class=\"o\">=</span> <span class=\"n\">scores</span> <span class=\"o\">-</span> <span class=\"n\">correct_class_score</span> <span class=\"o\">+</span> <span class=\"mi\">1</span>\n  <span class=\"n\">margins</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">maximum</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">margins</span><span class=\"p\">)</span>\n  <span class=\"c1\">#ç„¶åè¿™é‡Œè®¡ç®—äº†j=y[i]çš„æƒ…å½¢ï¼Œæ‰€ä»¥æŠŠä»–ä»¬ç½®ä¸º0</span>\n  <span class=\"n\">margins</span><span class=\"p\">[</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"n\">num_train</span><span class=\"p\">),</span><span class=\"n\">y</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n  <span class=\"n\">loss</span> <span class=\"o\">+=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">margins</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">num_train</span>\n  <span class=\"n\">loss</span> <span class=\"o\">+=</span> <span class=\"n\">reg</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span> <span class=\"n\">W</span> <span class=\"o\">*</span> <span class=\"n\">W</span><span class=\"p\">)</span>\n  <span class=\"c1\">#############################################################################</span>\n  <span class=\"c1\">#                             END OF YOUR CODE                              #</span>\n  <span class=\"c1\">#############################################################################</span>\n<span class=\"err\">â€‹</span>\n<span class=\"err\">â€‹</span>\n  <span class=\"c1\">#############################################################################</span>\n  <span class=\"c1\"># TODO:                                                                     #</span>\n  <span class=\"c1\"># Implement a vectorized version of the gradient for the structured SVM     #</span>\n  <span class=\"c1\"># loss, storing the result in dW.                                           #</span>\n  <span class=\"c1\">#                                                                           #</span>\n  <span class=\"c1\"># Hint: Instead of computing the gradient from scratch, it may be easier    #</span>\n  <span class=\"c1\"># to reuse some of the intermediate values that you used to compute the     #</span>\n  <span class=\"c1\"># loss.                                                                     #</span>\n  <span class=\"c1\">#############################################################################</span>\n  <span class=\"n\">margins</span><span class=\"p\">[</span><span class=\"n\">margins</span> <span class=\"o\">&gt;</span> <span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>\n  <span class=\"c1\">#å› ä¸ºj=y[i]çš„é‚£ä¸€ä¸ªå…ƒç´ çš„gradè¦è®¡ç®— &gt;0 çš„é‚£äº›æ¬¡æ•°æ¬¡</span>\n  <span class=\"n\">row_sum</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">margins</span><span class=\"p\">,</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n  <span class=\"n\">margins</span><span class=\"p\">[</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"n\">num_train</span><span class=\"p\">),</span><span class=\"n\">y</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"o\">-</span><span class=\"n\">row_sum</span><span class=\"o\">.</span><span class=\"n\">T</span>\n  <span class=\"c1\">#æŠŠå…¬å¼1å’Œ2åˆåˆ°ä¸€èµ·è®¡ç®—äº†</span>\n  <span class=\"n\">dW</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">,</span><span class=\"n\">margins</span><span class=\"p\">)</span>\n  <span class=\"n\">dW</span> <span class=\"o\">/=</span> <span class=\"n\">num_train</span>\n  <span class=\"n\">dW</span> <span class=\"o\">+=</span> <span class=\"n\">reg</span> <span class=\"o\">*</span> <span class=\"n\">W</span>\n  <span class=\"c1\">#############################################################################</span>\n  <span class=\"c1\">#                             END OF YOUR CODE                              #</span>\n  <span class=\"c1\">#############################################################################</span>\n<span class=\"err\">â€‹</span>\n  <span class=\"k\">return</span> <span class=\"n\">loss</span><span class=\"p\">,</span> <span class=\"n\">dW</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>è®¡ç®—ä¸€ä¸‹ä¸¤è€…çš„æ—¶é—´å·®ï¼š</p><div class=\"highlight\"><pre><code class=\"language-text\">Naive loss: 8.577034e+00 computed in 0.084761s\nVectorized loss: 8.577034e+00 computed in 0.001029s\ndifference: -0.000000\nNaive loss and gradient: computed in 0.082744s\nVectorized loss and gradient: computed in 0.002027s\ndifference: 0.000000</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><b>Stochastic Gradient Descent</b></p><p>ç¼–è¾‘ä¸€ä¸‹<code>classifiers/linear_classifier/LinearClassifier.train()</code></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">class</span> <span class=\"nc\">LinearClassifier</span><span class=\"p\">(</span><span class=\"nb\">object</span><span class=\"p\">):</span>\n<span class=\"err\">â€‹</span>\n  <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n    <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">W</span> <span class=\"o\">=</span> <span class=\"bp\">None</span>\n<span class=\"err\">â€‹</span>\n  <span class=\"k\">def</span> <span class=\"nf\">train</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span><span class=\"o\">=</span><span class=\"mf\">1e-3</span><span class=\"p\">,</span> <span class=\"n\">reg</span><span class=\"o\">=</span><span class=\"mf\">1e-5</span><span class=\"p\">,</span> <span class=\"n\">num_iters</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">,</span>\n            <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"mi\">200</span><span class=\"p\">,</span> <span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Train this linear classifier using stochastic gradient descent.\n</span><span class=\"s2\">â€‹\n</span><span class=\"s2\">    Inputs:\n</span><span class=\"s2\">    - X: A numpy array of shape (N, D) containing training data; there are N\n</span><span class=\"s2\">      training samples each of dimension D.\n</span><span class=\"s2\">    - y: A numpy array of shape (N,) containing training labels; y[i] = c\n</span><span class=\"s2\">      means that X[i] has label 0 &lt;= c &lt; C for C classes.\n</span><span class=\"s2\">    - learning_rate: (float) learning rate for optimization.\n</span><span class=\"s2\">    - reg: (float) regularization strength.\n</span><span class=\"s2\">    - num_iters: (integer) number of steps to take when optimizing\n</span><span class=\"s2\">    - batch_size: (integer) number of training examples to use at each step.\n</span><span class=\"s2\">    - verbose: (boolean) If true, print progress during optimization.\n</span><span class=\"s2\">â€‹\n</span><span class=\"s2\">    Outputs:\n</span><span class=\"s2\">    A list containing the value of the loss function at each training iteration.\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    <span class=\"n\">num_train</span><span class=\"p\">,</span> <span class=\"n\">dim</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n    <span class=\"n\">num_classes</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">max</span><span class=\"p\">(</span><span class=\"n\">y</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"mi\">1</span> <span class=\"c1\"># assume y takes values 0...K-1 where K is number of classes</span>\n    <span class=\"k\">if</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">W</span> <span class=\"ow\">is</span> <span class=\"bp\">None</span><span class=\"p\">:</span>\n      <span class=\"c1\"># lazily initialize W</span>\n      <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">W</span> <span class=\"o\">=</span> <span class=\"mf\">0.001</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">dim</span><span class=\"p\">,</span> <span class=\"n\">num_classes</span><span class=\"p\">)</span>\n<span class=\"err\">â€‹</span>\n    <span class=\"c1\"># Run stochastic gradient descent to optimize W</span>\n    <span class=\"n\">loss_history</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n    <span class=\"k\">for</span> <span class=\"n\">it</span> <span class=\"ow\">in</span> <span class=\"nb\">xrange</span><span class=\"p\">(</span><span class=\"n\">num_iters</span><span class=\"p\">):</span>\n      <span class=\"n\">X_batch</span> <span class=\"o\">=</span> <span class=\"bp\">None</span>\n      <span class=\"n\">y_batch</span> <span class=\"o\">=</span> <span class=\"bp\">None</span>\n<span class=\"err\">â€‹</span>\n      <span class=\"c1\">#########################################################################</span>\n      <span class=\"c1\"># TODO:                                                                 #</span>\n      <span class=\"c1\"># Sample batch_size elements from the training data and their           #</span>\n      <span class=\"c1\"># corresponding labels to use in this round of gradient descent.        #</span>\n      <span class=\"c1\"># Store the data in X_batch and their corresponding labels in           #</span>\n      <span class=\"c1\"># y_batch; after sampling X_batch should have shape (dim, batch_size)   #</span>\n      <span class=\"c1\"># and y_batch should have shape (batch_size,)                           #</span>\n      <span class=\"c1\">#                                                                       #</span>\n      <span class=\"c1\"># Hint: Use np.random.choice to generate indices. Sampling with         #</span>\n      <span class=\"c1\"># replacement is faster than sampling without replacement.              #</span>\n      <span class=\"c1\">#########################################################################</span>\n      <span class=\"n\">batch_inx</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">choice</span><span class=\"p\">(</span><span class=\"n\">num_train</span><span class=\"p\">,</span><span class=\"n\">batch_size</span><span class=\"p\">)</span>\n      <span class=\"n\">X_batch</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"p\">[</span><span class=\"n\">batch_inx</span><span class=\"p\">,:]</span>\n      <span class=\"n\">y_batch</span> <span class=\"o\">=</span> <span class=\"n\">y</span><span class=\"p\">[</span><span class=\"n\">batch_inx</span><span class=\"p\">]</span>\n      <span class=\"c1\">#########################################################################</span>\n      <span class=\"c1\">#                       END OF YOUR CODE                                #</span>\n      <span class=\"c1\">#########################################################################</span>\n<span class=\"err\">â€‹</span>\n      <span class=\"c1\"># evaluate loss and gradient</span>\n      <span class=\"n\">loss</span><span class=\"p\">,</span> <span class=\"n\">grad</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">loss</span><span class=\"p\">(</span><span class=\"n\">X_batch</span><span class=\"p\">,</span> <span class=\"n\">y_batch</span><span class=\"p\">,</span> <span class=\"n\">reg</span><span class=\"p\">)</span>\n      <span class=\"n\">loss_history</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">loss</span><span class=\"p\">)</span>\n<span class=\"err\">â€‹</span>\n      <span class=\"c1\"># perform parameter update</span>\n      <span class=\"c1\">#########################################################################</span>\n      <span class=\"c1\"># TODO:                                                                 #</span>\n      <span class=\"c1\"># Update the weights using the gradient and the learning rate.          #</span>\n      <span class=\"c1\">#########################################################################</span>\n      <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">W</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">W</span> <span class=\"o\">-</span> <span class=\"n\">learning_rate</span> <span class=\"o\">*</span> <span class=\"n\">grad</span>\n      <span class=\"c1\">#########################################################################</span>\n      <span class=\"c1\">#                       END OF YOUR CODE                                #</span>\n      <span class=\"c1\">#########################################################################</span>\n<span class=\"err\">â€‹</span>\n      <span class=\"k\">if</span> <span class=\"n\">verbose</span> <span class=\"ow\">and</span> <span class=\"n\">it</span> <span class=\"o\">%</span> <span class=\"mi\">100</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n        <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;iteration </span><span class=\"si\">%d</span><span class=\"s1\"> / </span><span class=\"si\">%d</span><span class=\"s1\">: loss </span><span class=\"si\">%f</span><span class=\"s1\">&#39;</span> <span class=\"o\">%</span> <span class=\"p\">(</span><span class=\"n\">it</span><span class=\"p\">,</span> <span class=\"n\">num_iters</span><span class=\"p\">,</span> <span class=\"n\">loss</span><span class=\"p\">))</span>\n<span class=\"err\">â€‹</span>\n    <span class=\"k\">return</span> <span class=\"n\">loss_history</span></code></pre></div><p>å†ç¼–è¾‘ä¸€ä¸‹<code>predict</code>å‡½æ•°</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">predict</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Use the trained weights of this linear classifier to predict labels for\n</span><span class=\"s2\">    data points.\n</span><span class=\"s2\">â€‹\n</span><span class=\"s2\">    Inputs:\n</span><span class=\"s2\">    - X: A numpy array of shape (N, D) containing training data; there are N\n</span><span class=\"s2\">      training samples each of dimension D.\n</span><span class=\"s2\">â€‹\n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional\n</span><span class=\"s2\">      array of length N, and each element is an integer giving the predicted\n</span><span class=\"s2\">      class.\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    <span class=\"n\">y_pred</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">])</span>\n    <span class=\"c1\">###########################################################################</span>\n    <span class=\"c1\"># TODO:                                                                   #</span>\n    <span class=\"c1\"># Implement this method. Store the predicted labels in y_pred.            #</span>\n    <span class=\"c1\">###########################################################################</span>\n    <span class=\"n\">score</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">W</span><span class=\"p\">)</span>\n    <span class=\"n\">y_pred</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">argmax</span><span class=\"p\">(</span><span class=\"n\">score</span><span class=\"p\">,</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n    <span class=\"c1\">###########################################################################</span>\n    <span class=\"c1\">#                           END OF YOUR CODE                              #</span>\n    <span class=\"c1\">###########################################################################</span>\n    <span class=\"k\">return</span> <span class=\"n\">y_pred</span></code></pre></div><p>å¾—åˆ°é¢„æµ‹å€¼</p><div class=\"highlight\"><pre><code class=\"language-text\">training accuracy: 0.376633\nvalidation accuracy: 0.384000</code></pre></div><p>ç„¶åè°ƒä¸€è°ƒlearning_rateå’Œregularization:</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># Use the validation set to tune hyperparameters (regularization strength and</span>\n<span class=\"c1\"># learning rate). You should experiment with different ranges for the learning</span>\n<span class=\"c1\"># rates and regularization strengths; if you are careful you should be able to</span>\n<span class=\"c1\"># get a classification accuracy of about 0.4 on the validation set.</span>\n<span class=\"n\">learning_rates</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mf\">1e-7</span><span class=\"p\">,</span> <span class=\"mf\">3e-7</span><span class=\"p\">,</span><span class=\"mf\">5e-7</span><span class=\"p\">,</span><span class=\"mf\">9e-7</span><span class=\"p\">]</span>\n<span class=\"n\">regularization_strengths</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mf\">2.5e4</span><span class=\"p\">,</span> <span class=\"mf\">1e4</span><span class=\"p\">,</span><span class=\"mf\">3e4</span><span class=\"p\">,</span><span class=\"mf\">2e4</span><span class=\"p\">]</span>\n<span class=\"err\">â€‹</span>\n<span class=\"c1\"># results is dictionary mapping tuples of the form</span>\n<span class=\"c1\"># (learning_rate, regularization_strength) to tuples of the form</span>\n<span class=\"c1\"># (training_accuracy, validation_accuracy). The accuracy is simply the fraction</span>\n<span class=\"c1\"># of data points that are correctly classified.</span>\n<span class=\"n\">results</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>\n<span class=\"n\">best_val</span> <span class=\"o\">=</span> <span class=\"o\">-</span><span class=\"mi\">1</span>   <span class=\"c1\"># The highest validation accuracy that we have seen so far.</span>\n<span class=\"n\">best_svm</span> <span class=\"o\">=</span> <span class=\"bp\">None</span> <span class=\"c1\"># The LinearSVM object that achieved the highest validation rate.</span>\n<span class=\"err\">â€‹</span>\n<span class=\"c1\">################################################################################</span>\n<span class=\"c1\"># TODO:                                                                        #</span>\n<span class=\"c1\"># Write code that chooses the best hyperparameters by tuning on the validation #</span>\n<span class=\"c1\"># set. For each combination of hyperparameters, train a linear SVM on the      #</span>\n<span class=\"c1\"># training set, compute its accuracy on the training and validation sets, and  #</span>\n<span class=\"c1\"># store these numbers in the results dictionary. In addition, store the best   #</span>\n<span class=\"c1\"># validation accuracy in best_val and the LinearSVM object that achieves this  #</span>\n<span class=\"c1\"># accuracy in best_svm.                                                        #</span>\n<span class=\"c1\">#                                                                              #</span>\n<span class=\"c1\"># Hint: You should use a small value for num_iters as you develop your         #</span>\n<span class=\"c1\"># validation code so that the SVMs don&#39;t take much time to train; once you are #</span>\n<span class=\"c1\"># confident that your validation code works, you should rerun the validation   #</span>\n<span class=\"c1\"># code with a larger value for num_iters.                                      #</span>\n<span class=\"c1\">################################################################################</span>\n<span class=\"k\">for</span> <span class=\"n\">learning_rate</span> <span class=\"ow\">in</span> <span class=\"n\">learning_rates</span><span class=\"p\">:</span>\n    <span class=\"k\">for</span> <span class=\"n\">regularization_strength</span> <span class=\"ow\">in</span> <span class=\"n\">regularization_strengths</span><span class=\"p\">:</span>\n        <span class=\"n\">svm</span> <span class=\"o\">=</span> <span class=\"n\">LinearSVM</span><span class=\"p\">()</span>\n        <span class=\"n\">loss_hist</span> <span class=\"o\">=</span> <span class=\"n\">svm</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">,</span> <span class=\"n\">y_train</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span><span class=\"o\">=</span><span class=\"n\">learning_rate</span><span class=\"p\">,</span> <span class=\"n\">reg</span><span class=\"o\">=</span><span class=\"n\">regularization_strength</span><span class=\"p\">,</span>\n                      <span class=\"n\">num_iters</span><span class=\"o\">=</span><span class=\"mi\">1500</span><span class=\"p\">,</span> <span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n        <span class=\"n\">y_train_pred</span> <span class=\"o\">=</span> <span class=\"n\">svm</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">)</span>\n        <span class=\"n\">y_val_pred</span> <span class=\"o\">=</span> <span class=\"n\">svm</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">X_val</span><span class=\"p\">)</span>\n        <span class=\"n\">y_train_acc</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">y_train_pred</span><span class=\"o\">==</span><span class=\"n\">y_train</span><span class=\"p\">)</span>\n        <span class=\"n\">y_val_acc</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">y_val_pred</span><span class=\"o\">==</span><span class=\"n\">y_val</span><span class=\"p\">)</span>\n        <span class=\"n\">results</span><span class=\"p\">[(</span><span class=\"n\">learning_rate</span><span class=\"p\">,</span><span class=\"n\">regularization_strength</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">y_train_acc</span><span class=\"p\">,</span> <span class=\"n\">y_val_acc</span><span class=\"p\">]</span>\n        <span class=\"k\">if</span> <span class=\"n\">y_val_acc</span> <span class=\"o\">&gt;</span> <span class=\"n\">best_val</span><span class=\"p\">:</span>\n            <span class=\"n\">best_val</span> <span class=\"o\">=</span> <span class=\"n\">y_val_acc</span>\n            <span class=\"n\">best_svm</span> <span class=\"o\">=</span> <span class=\"n\">svm</span>\n<span class=\"err\">â€‹</span>\n<span class=\"c1\">################################################################################</span>\n<span class=\"c1\">#                              END OF YOUR CODE                                #</span>\n<span class=\"c1\">################################################################################</span>\n    \n<span class=\"c1\"># Print out results.</span>\n<span class=\"k\">for</span> <span class=\"n\">lr</span><span class=\"p\">,</span> <span class=\"n\">reg</span> <span class=\"ow\">in</span> <span class=\"nb\">sorted</span><span class=\"p\">(</span><span class=\"n\">results</span><span class=\"p\">):</span>\n    <span class=\"n\">train_accuracy</span><span class=\"p\">,</span> <span class=\"n\">val_accuracy</span> <span class=\"o\">=</span> <span class=\"n\">results</span><span class=\"p\">[(</span><span class=\"n\">lr</span><span class=\"p\">,</span> <span class=\"n\">reg</span><span class=\"p\">)]</span>\n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;lr </span><span class=\"si\">%e</span><span class=\"s1\"> reg </span><span class=\"si\">%e</span><span class=\"s1\"> train accuracy: </span><span class=\"si\">%f</span><span class=\"s1\"> val accuracy: </span><span class=\"si\">%f</span><span class=\"s1\">&#39;</span> <span class=\"o\">%</span> <span class=\"p\">(</span>\n                <span class=\"n\">lr</span><span class=\"p\">,</span> <span class=\"n\">reg</span><span class=\"p\">,</span> <span class=\"n\">train_accuracy</span><span class=\"p\">,</span> <span class=\"n\">val_accuracy</span><span class=\"p\">))</span>\n    \n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;best validation accuracy achieved during cross-validation: </span><span class=\"si\">%f</span><span class=\"s1\">&#39;</span> <span class=\"o\">%</span> <span class=\"n\">best_val</span><span class=\"p\">)</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><b>å°ç»“</b></p><ul><li>å¤šçœ‹çœ‹cs231nçš„noteæ–‡æ¡£</li><li>å¤šå­¦ä¹ å­¦ä¹ gradçš„æ¨å€’</li></ul>", 
            "topic": [
                {
                    "tag": "SVM", 
                    "tagLink": "https://api.zhihu.com/topics/19583524"
                }, 
                {
                    "tag": "ä½œä¸š", 
                    "tagLink": "https://api.zhihu.com/topics/19586372"
                }
            ], 
            "comments": [
                {
                    "userName": "æè‰ºæµ·", 
                    "userLink": "https://www.zhihu.com/people/3c4b27eef62a1a6c0217e19d7fb21deb", 
                    "content": "<p>é‚£ä¸ªæ¨SVMçš„æ¢¯åº¦é“¾æ¥æ²¡äº†ã€‚ã€‚ã€‚ã€‚è‡ªå·±æ…¢æ…¢æ</p>", 
                    "likes": 1, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/45753372", 
            "userName": "ç›´ä¸Šäº‘éœ„", 
            "userLink": "https://www.zhihu.com/people/1033165ce4ad9c3fce69a0793dfab8ad", 
            "upvote": 5, 
            "title": "cs231nä½œä¸šï¼šassignment1 - knn", 
            "content": "<div class=\"highlight\"><pre><code class=\"language-text\">title: cs231nä½œä¸šï¼šassignment1 - knn\nid: cs231n-1h-1\ntags:\n  - cs231n\n  - homework\ncategories:\n  - AI\n  - Deep Learning\ndate: 2018-09-26 12:41:15\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>GitHubåœ°å€ï¼š<a href=\"https://link.zhihu.com/?target=https%3A//github.com/ZJUFangzh/cs231n\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/ZJUFangzh/cs</span><span class=\"invisible\">231n</span><span class=\"ellipsis\"></span></a></p><p class=\"ztext-empty-paragraph\"><br/></p><p>é¦–å‘äºä¸ªäººåšå®¢: <a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Fangzhçš„ä¸ªäººåšå®¢ | äººå·¥æ™ºèƒ½æ‹¯æ•‘ä¸–ç•Œ</a> æ¬¢è¿æ¥è®¿</p><p class=\"ztext-empty-paragraph\"><br/></p><p>ä½¿ç”¨KNNç®—æ³•æ¥å®Œæˆå›¾åƒè¯†åˆ«ï¼Œæ•°æ®é›†ç”¨çš„æ˜¯cifar10ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><p>é¦–å…ˆçœ‹ä¸€ä¸‹æ•°æ®é›†çš„ç»´åº¦</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># Load the raw CIFAR-10 data.</span>\n<span class=\"n\">cifar10_dir</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;cs231n/datasets/cifar-10-batches-py&#39;</span>\n<span class=\"n\">X_train</span><span class=\"p\">,</span> <span class=\"n\">y_train</span><span class=\"p\">,</span> <span class=\"n\">X_test</span><span class=\"p\">,</span> <span class=\"n\">y_test</span> <span class=\"o\">=</span> <span class=\"n\">load_CIFAR10</span><span class=\"p\">(</span><span class=\"n\">cifar10_dir</span><span class=\"p\">)</span>\n<span class=\"err\">â€‹</span>\n<span class=\"c1\"># As a sanity check, we print out the size of the training and test data.</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;Training data shape: &#39;</span><span class=\"p\">,</span> <span class=\"n\">X_train</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;Training labels shape: &#39;</span><span class=\"p\">,</span> <span class=\"n\">y_train</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;Test data shape: &#39;</span><span class=\"p\">,</span> <span class=\"n\">X_test</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;Test labels shape: &#39;</span><span class=\"p\">,</span> <span class=\"n\">y_test</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span></code></pre></div><p>å¯ä»¥çœ‹åˆ°ï¼Œæ¯ä¸€å¼ å›¾ç‰‡æ˜¯ <img src=\"https://www.zhihu.com/equation?tex=32%C3%9732%C3%973\" alt=\"32Ã—32Ã—3\" eeimg=\"1\"/> ï¼Œè®­ç»ƒé›†æœ‰50000å¼ ï¼Œæµ‹è¯•é›†æœ‰10000å¼ </p><div class=\"highlight\"><pre><code class=\"language-text\">Training data shape:  (50000, 32, 32, 3)\nTraining labels shape:  (50000,)\nTest data shape:  (10000, 32, 32, 3)\nTest labels shape:  (10000,)</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>ä¸ºäº†æ›´å¤Ÿæ›´å¿«çš„è®¡ç®—ï¼Œå°±é€‰5000å¼ åšè®­ç»ƒï¼Œ500å¼ åšæµ‹è¯•å°±å¥½äº†</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># Subsample the data for more efficient code execution in this exercise</span>\n<span class=\"n\">num_training</span> <span class=\"o\">=</span> <span class=\"mi\">5000</span>\n<span class=\"n\">mask</span> <span class=\"o\">=</span> <span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">num_training</span><span class=\"p\">))</span>\n<span class=\"n\">X_train</span> <span class=\"o\">=</span> <span class=\"n\">X_train</span><span class=\"p\">[</span><span class=\"n\">mask</span><span class=\"p\">]</span>\n<span class=\"n\">y_train</span> <span class=\"o\">=</span> <span class=\"n\">y_train</span><span class=\"p\">[</span><span class=\"n\">mask</span><span class=\"p\">]</span>\n<span class=\"err\">â€‹</span>\n<span class=\"n\">num_test</span> <span class=\"o\">=</span> <span class=\"mi\">500</span>\n<span class=\"n\">mask</span> <span class=\"o\">=</span> <span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">num_test</span><span class=\"p\">))</span>\n<span class=\"n\">X_test</span> <span class=\"o\">=</span> <span class=\"n\">X_test</span><span class=\"p\">[</span><span class=\"n\">mask</span><span class=\"p\">]</span>\n<span class=\"n\">y_test</span> <span class=\"o\">=</span> <span class=\"n\">y_test</span><span class=\"p\">[</span><span class=\"n\">mask</span><span class=\"p\">]</span></code></pre></div><p>è€ŒåæŠŠåƒç´ æ‹‰æˆ3072çš„è¡Œå‘é‡</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># Reshape the image data into rows</span>\n<span class=\"n\">X_train</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">))</span>\n<span class=\"n\">X_test</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"n\">X_test</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">X_test</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">))</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">,</span> <span class=\"n\">X_test</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>å› ä¸ºknnä¸éœ€è¦è®­ç»ƒï¼Œæ‰€ä»¥å…ˆå­˜å…¥æ•°æ®ï¼š</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">from</span> <span class=\"nn\">cs231n.classifiers</span> <span class=\"kn\">import</span> <span class=\"n\">KNearestNeighbor</span>\n<span class=\"err\">â€‹</span>\n<span class=\"c1\"># Create a kNN classifier instance. </span>\n<span class=\"c1\"># Remember that training a kNN classifier is a noop: </span>\n<span class=\"c1\"># the Classifier simply remembers the data and does no further processing </span>\n<span class=\"n\">classifier</span> <span class=\"o\">=</span> <span class=\"n\">KNearestNeighbor</span><span class=\"p\">()</span>\n<span class=\"n\">classifier</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">,</span> <span class=\"n\">y_train</span><span class=\"p\">)</span></code></pre></div><p>ç„¶åè¦ä¿®æ”¹<code>k_nearest_neighbor.py</code>ä¸­çš„<code>compute_distances_two_loops</code></p><p>è¿™é‡Œå¥—äº†ä¸¤å±‚å¾ªç¯ï¼Œä¹Ÿå°±æ˜¯æ¯”è¾ƒè®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„æ¯ä¸€å¼ å›¾ç‰‡çš„é—´è·ï¼š</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">compute_distances_two_loops</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Compute the distance between each test point in X and each training point\n</span><span class=\"s2\">    in self.X_train using a nested loop over both the training data and the \n</span><span class=\"s2\">    test data.\n</span><span class=\"s2\">â€‹\n</span><span class=\"s2\">    Inputs:\n</span><span class=\"s2\">    - X: A numpy array of shape (num_test, D) containing test data.\n</span><span class=\"s2\">â€‹\n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    - dists: A numpy array of shape (num_test, num_train) where dists[i, j]\n</span><span class=\"s2\">      is the Euclidean distance between the ith test point and the jth training\n</span><span class=\"s2\">      point.\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    <span class=\"n\">num_test</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n    <span class=\"n\">num_train</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">X_train</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n    <span class=\"n\">dists</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">num_test</span><span class=\"p\">,</span> <span class=\"n\">num_train</span><span class=\"p\">))</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">xrange</span><span class=\"p\">(</span><span class=\"n\">num_test</span><span class=\"p\">):</span>\n      <span class=\"k\">for</span> <span class=\"n\">j</span> <span class=\"ow\">in</span> <span class=\"nb\">xrange</span><span class=\"p\">(</span><span class=\"n\">num_train</span><span class=\"p\">):</span>\n        <span class=\"c1\">#####################################################################</span>\n        <span class=\"c1\"># TODO:                                                             #</span>\n        <span class=\"c1\"># Compute the l2 distance between the ith test point and the jth    #</span>\n        <span class=\"c1\"># training point, and store the result in dists[i, j]. You should   #</span>\n        <span class=\"c1\"># not use a loop over dimension.                                    #</span>\n        <span class=\"c1\">#####################################################################</span>\n        <span class=\"n\">dists</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">][</span><span class=\"n\">j</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sqrt</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">square</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">,:]</span> <span class=\"o\">-</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">X_train</span><span class=\"p\">[</span><span class=\"n\">j</span><span class=\"p\">,:])))</span>\n        <span class=\"c1\">#####################################################################</span>\n        <span class=\"c1\">#                       END OF YOUR CODE                            #</span>\n        <span class=\"c1\">#####################################################################</span>\n    <span class=\"k\">return</span> <span class=\"n\">dists</span></code></pre></div><p>å¾—åˆ°äº†ä¸€ä¸ª$(500,5000)$çš„distsçŸ©é˜µã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><p>ç„¶åä¿®æ”¹<code>predict_labels</code>å‡½æ•°</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">predict_labels</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">dists</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Given a matrix of distances between test points and training points,\n</span><span class=\"s2\">    predict a label for each test point.\n</span><span class=\"s2\">â€‹\n</span><span class=\"s2\">    Inputs:\n</span><span class=\"s2\">    - dists: A numpy array of shape (num_test, num_train) where dists[i, j]\n</span><span class=\"s2\">      gives the distance betwen the ith test point and the jth training point.\n</span><span class=\"s2\">â€‹\n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    - y: A numpy array of shape (num_test,) containing predicted labels for the\n</span><span class=\"s2\">      test data, where y[i] is the predicted label for the test point X[i].  \n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    <span class=\"n\">num_test</span> <span class=\"o\">=</span> <span class=\"n\">dists</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n    <span class=\"n\">y_pred</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"n\">num_test</span><span class=\"p\">)</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">xrange</span><span class=\"p\">(</span><span class=\"n\">num_test</span><span class=\"p\">):</span>\n      <span class=\"c1\"># A list of length k storing the labels of the k nearest neighbors to</span>\n      <span class=\"c1\"># the ith test point.</span>\n      <span class=\"n\">closest_y</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n      <span class=\"c1\">#########################################################################</span>\n      <span class=\"c1\"># TODO:                                                                 #</span>\n      <span class=\"c1\"># Use the distance matrix to find the k nearest neighbors of the ith    #</span>\n      <span class=\"c1\"># testing point, and use self.y_train to find the labels of these       #</span>\n      <span class=\"c1\"># neighbors. Store these labels in closest_y.                           #</span>\n      <span class=\"c1\"># Hint: Look up the function numpy.argsort.                             #</span>\n      <span class=\"c1\">#########################################################################</span>\n      <span class=\"c1\">#æ‰¾åˆ°æ¯ä¸€ä¸ªæµ‹è¯•å›¾ç‰‡ä¸­å¯¹åº”çš„5000å¼ è®­ç»ƒé›†å›¾ç‰‡ï¼Œè·ç¦»æœ€è¿‘çš„å‰kä¸ª</span>\n      <span class=\"n\">closest_y</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">y_train</span><span class=\"p\">[</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">argsort</span><span class=\"p\">(</span><span class=\"n\">dists</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">])[:</span><span class=\"n\">k</span><span class=\"p\">]]</span>\n      <span class=\"c1\">#########################################################################</span>\n      <span class=\"c1\"># TODO:                                                                 #</span>\n      <span class=\"c1\"># Now that you have found the labels of the k nearest neighbors, you    #</span>\n      <span class=\"c1\"># need to find the most common label in the list closest_y of labels.   #</span>\n      <span class=\"c1\"># Store this label in y_pred[i]. Break ties by choosing the smaller     #</span>\n      <span class=\"c1\"># label.                                                                #</span>\n      <span class=\"c1\">#########################################################################</span>\n      <span class=\"c1\">#ç„¶åå°†è¿™Kä¸ªå›¾ç‰‡è¿›è¡ŒæŠ•ç¥¨ï¼Œå¾—ç¥¨æ•°æœ€å¤šçš„å°±æ˜¯é¢„æµ‹å€¼</span>\n      <span class=\"n\">y_pred</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">argmax</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">bincount</span><span class=\"p\">(</span><span class=\"n\">closest_y</span><span class=\"p\">))</span>\n      <span class=\"c1\">#########################################################################</span>\n      <span class=\"c1\">#                           END OF YOUR CODE                            # </span>\n      <span class=\"c1\">#########################################################################</span>\n<span class=\"err\">â€‹</span>\n    <span class=\"k\">return</span> <span class=\"n\">y_pred</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>é¢„æµ‹ä¸€ä¸‹ï¼š</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># Now implement the function predict_labels and run the code below:</span>\n<span class=\"c1\"># We use k = 1 (which is Nearest Neighbor).</span>\n<span class=\"n\">y_test_pred</span> <span class=\"o\">=</span> <span class=\"n\">classifier</span><span class=\"o\">.</span><span class=\"n\">predict_labels</span><span class=\"p\">(</span><span class=\"n\">dists</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"err\">â€‹</span>\n<span class=\"c1\"># Compute and print the fraction of correctly predicted examples</span>\n<span class=\"n\">num_correct</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">y_test_pred</span> <span class=\"o\">==</span> <span class=\"n\">y_test</span><span class=\"p\">)</span>\n<span class=\"n\">accuracy</span> <span class=\"o\">=</span> <span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"n\">num_correct</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">num_test</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;Got </span><span class=\"si\">%d</span><span class=\"s1\"> / </span><span class=\"si\">%d</span><span class=\"s1\"> correct =&gt; accuracy: </span><span class=\"si\">%f</span><span class=\"s1\">&#39;</span> <span class=\"o\">%</span> <span class=\"p\">(</span><span class=\"n\">num_correct</span><span class=\"p\">,</span> <span class=\"n\">num_test</span><span class=\"p\">,</span> <span class=\"n\">accuracy</span><span class=\"p\">))</span></code></pre></div><p>ç»“æœæ˜¯0.274</p><p>å†è¯•è¯•k=5çš„ç»“æœï¼Œæ˜¯0.278</p><p>ç„¶åå†ä¿®æ”¹<code>compute_distances_one_loop</code>å‡½æ•°ï¼Œè¿™æ¬¡äº‰å–åªç”¨ä¸€ä¸ªå¾ªç¯</p><div class=\"highlight\"><pre><code class=\"language-python\">  <span class=\"k\">def</span> <span class=\"nf\">compute_distances_one_loop</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Compute the distance between each test point in X and each training point\n</span><span class=\"s2\">    in self.X_train using a single loop over the test data.\n</span><span class=\"s2\">â€‹\n</span><span class=\"s2\">    Input / Output: Same as compute_distances_two_loops\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    <span class=\"n\">num_test</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n    <span class=\"n\">num_train</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">X_train</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n    <span class=\"n\">dists</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">num_test</span><span class=\"p\">,</span> <span class=\"n\">num_train</span><span class=\"p\">))</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">xrange</span><span class=\"p\">(</span><span class=\"n\">num_test</span><span class=\"p\">):</span>\n      <span class=\"c1\">#######################################################################</span>\n      <span class=\"c1\"># TODO:                                                               #</span>\n      <span class=\"c1\"># Compute the l2 distance between the ith test point and all training #</span>\n      <span class=\"c1\"># points, and store the result in dists[i, :].                        #</span>\n      <span class=\"c1\">#######################################################################</span>\n      <span class=\"c1\">#åˆ©ç”¨pythonçš„å¹¿æ’­ï¼Œä¸€æ¬¡æ€§ç®—å‡ºæ¯ä¸€å¼ å›¾ç‰‡ä¸5000å¼ å›¾ç‰‡çš„è·ç¦»</span>\n      <span class=\"n\">dists</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"p\">:]</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sqrt</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">square</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">X_train</span> <span class=\"o\">-</span> <span class=\"n\">X</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"p\">:]),</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">))</span>\n      <span class=\"c1\">#######################################################################</span>\n      <span class=\"c1\">#                         END OF YOUR CODE                            #</span>\n      <span class=\"c1\">#######################################################################</span>\n    <span class=\"k\">return</span> <span class=\"n\">dists</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>éªŒè¯ä¸€ä¸‹é—´è·æ˜¯</p><div class=\"highlight\"><pre><code class=\"language-text\">Difference was: 0.000000\nGood! The distance matrices are the same</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>ç„¶åäº‰å–ä¸ç”¨å¾ªç¯<code>compute_distances_no_loops</code>ï¼Œè¿™ä¸€æ­¥æ¯”è¾ƒéš¾ï¼Œæƒ³æ³•æ˜¯åˆ©ç”¨å¹³æ–¹å·®å…¬å¼$(x-y)^2 = x^2 + y^2 - 2xy$ï¼Œä½¿ç”¨çŸ©é˜µä¹˜æ³•å’ŒäºŒæ¬¡å¹¿æ’­ï¼Œç›´æ¥ç®—å‡ºè·ç¦»ï¼Œæ³¨æ„çŸ©é˜µçš„ç»´åº¦</p><div class=\"highlight\"><pre><code class=\"language-python\">  <span class=\"k\">def</span> <span class=\"nf\">compute_distances_no_loops</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Compute the distance between each test point in X and each training point\n</span><span class=\"s2\">    in self.X_train using no explicit loops.\n</span><span class=\"s2\">â€‹\n</span><span class=\"s2\">    Input / Output: Same as compute_distances_two_loops\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    <span class=\"n\">num_test</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n    <span class=\"n\">num_train</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">X_train</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n    <span class=\"n\">dists</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">num_test</span><span class=\"p\">,</span> <span class=\"n\">num_train</span><span class=\"p\">))</span> \n    <span class=\"c1\">#########################################################################</span>\n    <span class=\"c1\"># TODO:                                                                 #</span>\n    <span class=\"c1\"># Compute the l2 distance between all test points and all training      #</span>\n    <span class=\"c1\"># points without using any explicit loops, and store the result in      #</span>\n    <span class=\"c1\"># dists.                                                                #</span>\n    <span class=\"c1\">#                                                                       #</span>\n    <span class=\"c1\"># You should implement this function using only basic array operations; #</span>\n    <span class=\"c1\"># in particular you should not use functions from scipy.                #</span>\n    <span class=\"c1\">#                                                                       #</span>\n    <span class=\"c1\"># HINT: Try to formulate the l2 distance using matrix multiplication    #</span>\n    <span class=\"c1\">#       and two broadcast sums.                                         #</span>\n    <span class=\"c1\">#########################################################################</span>\n    <span class=\"n\">temp_2xy</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">X_train</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n    <span class=\"n\">temp_x2</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">square</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">),</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"n\">keepdims</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n    <span class=\"n\">temp_y2</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">square</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">X_train</span><span class=\"p\">),</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n    <span class=\"n\">dists</span> <span class=\"o\">=</span> <span class=\"n\">temp_x2</span> <span class=\"o\">+</span> <span class=\"n\">temp_2xy</span> <span class=\"o\">+</span> <span class=\"n\">temp_y2</span>\n    <span class=\"n\">dists</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sqrt</span><span class=\"p\">(</span><span class=\"n\">dists</span><span class=\"p\">)</span>\n    <span class=\"c1\">#########################################################################</span>\n    <span class=\"c1\">#                         END OF YOUR CODE                              #</span>\n    <span class=\"c1\">#########################################################################</span>\n    <span class=\"k\">return</span> <span class=\"n\">dists</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>å¯¹æ¯”ä¸€ä¸‹ä¸‰ç§æ–¹æ³•çš„æ—¶é—´ï¼Œæˆ‘è¿™é‡Œä¸çŸ¥é“ä¸ºä»€ä¹ˆtwoæ¯”oneçŸ­ï¼Œç†è®ºä¸Šæ˜¯å¾ªç¯è¶Šå°‘æ—¶é—´è¶ŠçŸ­ï¼š</p><div class=\"highlight\"><pre><code class=\"language-text\">Two loop version took 24.510484 seconds\nOne loop version took 56.412211 seconds\nNo loop version took 0.183508 seconds</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><b>äº¤å‰éªŒè¯</b></p><p>ç”¨äº¤å‰éªŒè¯æ¥æ‰¾åˆ°æœ€å¥½çš„k</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">num_folds</span> <span class=\"o\">=</span> <span class=\"mi\">5</span>\n<span class=\"n\">k_choices</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">12</span><span class=\"p\">,</span> <span class=\"mi\">15</span><span class=\"p\">,</span> <span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"mi\">50</span><span class=\"p\">,</span> <span class=\"mi\">100</span><span class=\"p\">]</span>\n<span class=\"err\">â€‹</span>\n<span class=\"n\">X_train_folds</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n<span class=\"n\">y_train_folds</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n<span class=\"c1\">################################################################################</span>\n<span class=\"c1\"># TODO:                                                                        #</span>\n<span class=\"c1\"># Split up the training data into folds. After splitting, X_train_folds and    #</span>\n<span class=\"c1\"># y_train_folds should each be lists of length num_folds, where                #</span>\n<span class=\"c1\"># y_train_folds[i] is the label vector for the points in X_train_folds[i].     #</span>\n<span class=\"c1\"># Hint: Look up the numpy array_split function.                                #</span>\n<span class=\"c1\">################################################################################</span>\n<span class=\"n\">X_train_folds</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array_split</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">,</span> <span class=\"n\">num_folds</span><span class=\"p\">)</span>\n<span class=\"n\">y_train_folds</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array_split</span><span class=\"p\">(</span><span class=\"n\">y_train</span><span class=\"p\">,</span> <span class=\"n\">num_folds</span><span class=\"p\">)</span>\n<span class=\"err\">â€‹</span>\n<span class=\"err\">â€‹</span>\n<span class=\"c1\">################################################################################</span>\n<span class=\"c1\">#                                 END OF YOUR CODE                             #</span>\n<span class=\"c1\">################################################################################</span>\n<span class=\"err\">â€‹</span>\n<span class=\"c1\"># A dictionary holding the accuracies for different values of k that we find</span>\n<span class=\"c1\"># when running cross-validation. After running cross-validation,</span>\n<span class=\"c1\"># k_to_accuracies[k] should be a list of length num_folds giving the different</span>\n<span class=\"c1\"># accuracy values that we found when using that value of k.</span>\n<span class=\"n\">k_to_accuracies</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>\n<span class=\"err\">â€‹</span>\n<span class=\"err\">â€‹</span>\n<span class=\"c1\">################################################################################</span>\n<span class=\"c1\"># TODO:                                                                        #</span>\n<span class=\"c1\"># Perform k-fold cross validation to find the best value of k. For each        #</span>\n<span class=\"c1\"># possible value of k, run the k-nearest-neighbor algorithm num_folds times,   #</span>\n<span class=\"c1\"># where in each case you use all but one of the folds as training data and the #</span>\n<span class=\"c1\"># last fold as a validation set. Store the accuracies for all fold and all     #</span>\n<span class=\"c1\"># values of k in the k_to_accuracies dictionary.                               #</span>\n<span class=\"c1\">################################################################################</span>\n<span class=\"n\">classifier</span> <span class=\"o\">=</span> <span class=\"n\">KNearestNeighbor</span><span class=\"p\">()</span>\n<span class=\"k\">for</span> <span class=\"n\">k</span> <span class=\"ow\">in</span> <span class=\"n\">k_choices</span><span class=\"p\">:</span>\n    <span class=\"n\">accuracies</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n    <span class=\"k\">for</span> <span class=\"n\">fold</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">num_folds</span><span class=\"p\">):</span>\n        <span class=\"n\">temp_X</span> <span class=\"o\">=</span> <span class=\"n\">X_train_folds</span><span class=\"p\">[:]</span>\n        <span class=\"n\">temp_y</span> <span class=\"o\">=</span> <span class=\"n\">y_train_folds</span><span class=\"p\">[:]</span>\n        <span class=\"n\">X_val_fold</span> <span class=\"o\">=</span> <span class=\"n\">temp_X</span><span class=\"o\">.</span><span class=\"n\">pop</span><span class=\"p\">(</span><span class=\"n\">fold</span><span class=\"p\">)</span>\n        <span class=\"n\">y_val_fold</span> <span class=\"o\">=</span> <span class=\"n\">temp_y</span><span class=\"o\">.</span><span class=\"n\">pop</span><span class=\"p\">(</span><span class=\"n\">fold</span><span class=\"p\">)</span>\n        <span class=\"n\">temp_X</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"n\">y</span> <span class=\"k\">for</span> <span class=\"n\">x</span> <span class=\"ow\">in</span> <span class=\"n\">temp_X</span> <span class=\"k\">for</span> <span class=\"n\">y</span> <span class=\"ow\">in</span> <span class=\"n\">x</span><span class=\"p\">])</span>\n        <span class=\"n\">temp_y</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"n\">y</span> <span class=\"k\">for</span> <span class=\"n\">x</span> <span class=\"ow\">in</span> <span class=\"n\">temp_y</span> <span class=\"k\">for</span> <span class=\"n\">y</span> <span class=\"ow\">in</span> <span class=\"n\">x</span><span class=\"p\">])</span>\n        <span class=\"n\">classifier</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"p\">(</span><span class=\"n\">temp_X</span><span class=\"p\">,</span><span class=\"n\">temp_y</span><span class=\"p\">)</span>\n        <span class=\"n\">y_val_pred</span> <span class=\"o\">=</span> <span class=\"n\">classifier</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">X_val_fold</span><span class=\"p\">,</span><span class=\"n\">k</span><span class=\"o\">=</span><span class=\"n\">k</span><span class=\"p\">)</span>\n        <span class=\"n\">num_correct</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">y_val_fold</span> <span class=\"o\">==</span> <span class=\"n\">y_val_pred</span><span class=\"p\">)</span>\n        <span class=\"n\">accuracies</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">num_correct</span> <span class=\"o\">/</span> <span class=\"n\">y_val_fold</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">])</span>\n    <span class=\"n\">k_to_accuracies</span><span class=\"p\">[</span><span class=\"n\">k</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">accuracies</span>\n    \n<span class=\"c1\">################################################################################</span>\n<span class=\"c1\">#                                 END OF YOUR CODE                             #</span>\n<span class=\"c1\">################################################################################</span>\n<span class=\"err\">â€‹</span>\n<span class=\"c1\"># Print out the computed accuracies</span>\n<span class=\"k\">for</span> <span class=\"n\">k</span> <span class=\"ow\">in</span> <span class=\"nb\">sorted</span><span class=\"p\">(</span><span class=\"n\">k_to_accuracies</span><span class=\"p\">):</span>\n    <span class=\"k\">for</span> <span class=\"n\">accuracy</span> <span class=\"ow\">in</span> <span class=\"n\">k_to_accuracies</span><span class=\"p\">[</span><span class=\"n\">k</span><span class=\"p\">]:</span>\n        <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;k = </span><span class=\"si\">%d</span><span class=\"s1\">, accuracy = </span><span class=\"si\">%f</span><span class=\"s1\">&#39;</span> <span class=\"o\">%</span> <span class=\"p\">(</span><span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">accuracy</span><span class=\"p\">))</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>ç”»ä¸ªå›¾ï¼š</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># plot the raw observations</span>\n<span class=\"k\">for</span> <span class=\"n\">k</span> <span class=\"ow\">in</span> <span class=\"n\">k_choices</span><span class=\"p\">:</span>\n    <span class=\"n\">accuracies</span> <span class=\"o\">=</span> <span class=\"n\">k_to_accuracies</span><span class=\"p\">[</span><span class=\"n\">k</span><span class=\"p\">]</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">scatter</span><span class=\"p\">([</span><span class=\"n\">k</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">accuracies</span><span class=\"p\">),</span> <span class=\"n\">accuracies</span><span class=\"p\">)</span>\n<span class=\"err\">â€‹</span>\n<span class=\"c1\"># plot the trend line with error bars that correspond to standard deviation</span>\n<span class=\"n\">accuracies_mean</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">v</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">k</span><span class=\"p\">,</span><span class=\"n\">v</span> <span class=\"ow\">in</span> <span class=\"nb\">sorted</span><span class=\"p\">(</span><span class=\"n\">k_to_accuracies</span><span class=\"o\">.</span><span class=\"n\">items</span><span class=\"p\">())])</span>\n<span class=\"n\">accuracies_std</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">std</span><span class=\"p\">(</span><span class=\"n\">v</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">k</span><span class=\"p\">,</span><span class=\"n\">v</span> <span class=\"ow\">in</span> <span class=\"nb\">sorted</span><span class=\"p\">(</span><span class=\"n\">k_to_accuracies</span><span class=\"o\">.</span><span class=\"n\">items</span><span class=\"p\">())])</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">errorbar</span><span class=\"p\">(</span><span class=\"n\">k_choices</span><span class=\"p\">,</span> <span class=\"n\">accuracies_mean</span><span class=\"p\">,</span> <span class=\"n\">yerr</span><span class=\"o\">=</span><span class=\"n\">accuracies_std</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">title</span><span class=\"p\">(</span><span class=\"s1\">&#39;Cross-validation on k&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">xlabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;k&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">ylabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;Cross-validation accuracy&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-bc0c3cf3949efec1defab4cad29a88e2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"395\" data-rawheight=\"278\" class=\"content_image\" width=\"395\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;395&#39; height=&#39;278&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"395\" data-rawheight=\"278\" class=\"content_image lazy\" width=\"395\" data-actualsrc=\"https://pic3.zhimg.com/v2-bc0c3cf3949efec1defab4cad29a88e2_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># Based on the cross-validation results above, choose the best value for k,   </span>\n<span class=\"c1\"># retrain the classifier using all the training data, and test it on the test</span>\n<span class=\"c1\"># data. You should be able to get above 28% accuracy on the test data.</span>\n<span class=\"n\">best_k</span> <span class=\"o\">=</span> <span class=\"mi\">10</span>\n<span class=\"err\">â€‹</span>\n<span class=\"n\">classifier</span> <span class=\"o\">=</span> <span class=\"n\">KNearestNeighbor</span><span class=\"p\">()</span>\n<span class=\"n\">classifier</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">,</span> <span class=\"n\">y_train</span><span class=\"p\">)</span>\n<span class=\"n\">y_test_pred</span> <span class=\"o\">=</span> <span class=\"n\">classifier</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">X_test</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"o\">=</span><span class=\"n\">best_k</span><span class=\"p\">)</span>\n<span class=\"err\">â€‹</span>\n<span class=\"c1\"># Compute and display the accuracy</span>\n<span class=\"n\">num_correct</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">y_test_pred</span> <span class=\"o\">==</span> <span class=\"n\">y_test</span><span class=\"p\">)</span>\n<span class=\"n\">accuracy</span> <span class=\"o\">=</span> <span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"n\">num_correct</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">num_test</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;Got </span><span class=\"si\">%d</span><span class=\"s1\"> / </span><span class=\"si\">%d</span><span class=\"s1\"> correct =&gt; accuracy: </span><span class=\"si\">%f</span><span class=\"s1\">&#39;</span> <span class=\"o\">%</span> <span class=\"p\">(</span><span class=\"n\">num_correct</span><span class=\"p\">,</span> <span class=\"n\">num_test</span><span class=\"p\">,</span> <span class=\"n\">accuracy</span><span class=\"p\">))</span></code></pre></div><p>å¾—åˆ°æœ€å¥½çš„k=10ï¼Œå‡†ç¡®ç‡æ˜¯0.282</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>å°ç»“</b></p><ul><li>cs231nçš„ä½œä¸šæ¯”DeepLearning.aiçš„éš¾å¤šäº†ï¼Œä¸æ˜¯ä¸€ä¸ªæ¡£æ¬¡çš„ï¼Œå…³é”®æ˜¯æç¤ºæ¯”è¾ƒå°‘ï¼Œæ‰€ä»¥è‡ªå·±åšèµ·æ¥æ¯”è¾ƒè´¹åŠ²</li><li>ä¸»è¦è¦å­¦ä¼šå‘é‡åŒ–çš„è¿ç®—ï¼Œå°‘ç”¨loopå¾ªç¯</li><li>knnå·²ç»è¢«æ·˜æ±°äº†ï¼Œè¿™ä¸ªä½œä¸šåªæ˜¯è®©æˆ‘ä»¬å…¥é—¨çœ‹çœ‹å›¾åƒè¯†åˆ«å¤§æ¦‚æ€ä¹ˆåš</li></ul><p></p>", 
            "topic": [
                {
                    "tag": "ä½œä¸š", 
                    "tagLink": "https://api.zhihu.com/topics/19586372"
                }
            ], 
            "comments": [
                {
                    "userName": "Jimmythebigmouth", 
                    "userLink": "https://www.zhihu.com/people/d309a3867f34bce1dd18250e3f25294c", 
                    "content": "<p>æ„Ÿè°¢</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "byfate", 
                    "userLink": "https://www.zhihu.com/people/fc62aa4ac3c34f88500f093495a83537", 
                    "content": "<p>ä½ å¥½ è¯·é—®ä¸ºä»€ä¹ˆæœ€å°çš„æ˜¯10</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "zephyr", 
                    "userLink": "https://www.zhihu.com/people/28a6efc5087751aa8fc2504484451162", 
                    "content": "nice[é…·]", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "ç™¾é‡Œå± è‹", 
                    "userLink": "https://www.zhihu.com/people/ea74a3b9adafcabba2f4af0d3667a4b4", 
                    "content": "<p>temp_X = np.array([y for x in temp_X for y in x])</p><p>temp_y = np.array([y for x in temp_y for y in x])</p><p>æ‚¨å¥½ï¼Œè¯·é—®è¿™é‡Œarrayé‡Œé¢çš„è¯­å¥[y for x in temp_X for y in x]æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ</p>", 
                    "likes": 1, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/45752897", 
            "userName": "ç›´ä¸Šäº‘éœ„", 
            "userLink": "https://www.zhihu.com/people/1033165ce4ad9c3fce69a0793dfab8ad", 
            "upvote": 0, 
            "title": "DeepLearning.aiä½œä¸š:(4-1)-- å·ç§¯ç¥ç»ç½‘ç»œ(part2)", 
            "content": "<h2><b>Part2ï¼šConvolutional Neural Networks: Application</b></h2><p>ç”¨TensorFlowæ¥æ­å»ºå·ç§¯ç¥ç»ç½‘ç»œã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>1.Create placeholders</b></h2><p>å…ˆåˆ›å»ºplaceholdersï¼Œç”¨æ¥è®­ç»ƒä¸­ä¼ é€’X,Y</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"err\">â€‹</span>\n<span class=\"k\">def</span> <span class=\"nf\">create_placeholders</span><span class=\"p\">(</span><span class=\"n\">n_H0</span><span class=\"p\">,</span> <span class=\"n\">n_W0</span><span class=\"p\">,</span> <span class=\"n\">n_C0</span><span class=\"p\">,</span> <span class=\"n\">n_y</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Creates the placeholders for the tensorflow session.\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    n_H0 -- scalar, height of an input image\n</span><span class=\"s2\">    n_W0 -- scalar, width of an input image\n</span><span class=\"s2\">    n_C0 -- scalar, number of channels of the input\n</span><span class=\"s2\">    n_y -- scalar, number of classes\n</span><span class=\"s2\">        \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    X -- placeholder for the data input, of shape [None, n_H0, n_W0, n_C0] and dtype &#34;float&#34;\n</span><span class=\"s2\">    Y -- placeholder for the input labels, of shape [None, n_y] and dtype &#34;float&#34;\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n<span class=\"err\">â€‹</span>\n    <span class=\"c1\">### START CODE HERE ### (â‰ˆ2 lines)</span>\n    <span class=\"n\">X</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">placeholder</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">,</span> <span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"bp\">None</span><span class=\"p\">,</span><span class=\"n\">n_H0</span><span class=\"p\">,</span> <span class=\"n\">n_W0</span><span class=\"p\">,</span> <span class=\"n\">n_C0</span><span class=\"p\">))</span>\n    <span class=\"n\">Y</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">placeholder</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">,</span> <span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"bp\">None</span><span class=\"p\">,</span><span class=\"n\">n_y</span><span class=\"p\">))</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>2.Initialize parameters</b></h2><p>ç”¨æ¥åˆå§‹åŒ–å‚æ•°ï¼Œä¸»è¦æ˜¯W1,W2,åœ¨è¿™é‡Œå°±æ²¡æœ‰ç”¨bäº†</p><p>ç”¨<code>W = tf.get_variable(&#34;W&#34;, [1,2,3,4], initializer = ...)</code></p><p>initializer ç”¨<code>tf.contrib.layers.xavier_initializer</code></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: initialize_parameters</span>\n<span class=\"err\">â€‹</span>\n<span class=\"k\">def</span> <span class=\"nf\">initialize_parameters</span><span class=\"p\">():</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Initializes weight parameters to build a neural network with tensorflow. The shapes are:\n</span><span class=\"s2\">                        W1 : [4, 4, 3, 8]\n</span><span class=\"s2\">                        W2 : [2, 2, 8, 16]\n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    parameters -- a dictionary of tensors containing W1, W2\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">set_random_seed</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>                              <span class=\"c1\"># so that your &#34;random&#34; numbers match ours</span>\n        \n    <span class=\"c1\">### START CODE HERE ### (approx. 2 lines of code)</span>\n    <span class=\"n\">W1</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">get_variable</span><span class=\"p\">(</span><span class=\"s1\">&#39;W1&#39;</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">8</span><span class=\"p\">],</span><span class=\"n\">initializer</span><span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">contrib</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">xavier_initializer</span><span class=\"p\">(</span><span class=\"n\">seed</span> <span class=\"o\">=</span> <span class=\"mi\">0</span> <span class=\"p\">))</span>\n    <span class=\"n\">W2</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">get_variable</span><span class=\"p\">(</span><span class=\"s1\">&#39;W2&#39;</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"mi\">16</span><span class=\"p\">],</span><span class=\"n\">initializer</span><span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">contrib</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">xavier_initializer</span><span class=\"p\">(</span><span class=\"n\">seed</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">))</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n<span class=\"err\">â€‹</span>\n    <span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">&#34;W1&#34;</span><span class=\"p\">:</span> <span class=\"n\">W1</span><span class=\"p\">,</span>\n                  <span class=\"s2\">&#34;W2&#34;</span><span class=\"p\">:</span> <span class=\"n\">W2</span><span class=\"p\">}</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">parameters</span></code></pre></div><p>è®°å¾—è¿™åªæ˜¯åˆ›å»ºäº†å›¾è€Œå·²ï¼Œå¹¶æ²¡æœ‰çœŸæ­£çš„åˆå§‹åŒ–å‚æ•°ï¼Œåœ¨æ‰§è¡Œä¸­è¿˜éœ€è¦</p><p><code>init = tf.global_variables_initializer()</code> </p><p><code>sess_test.run(init)</code></p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>3. Forward propagation</b></h2><p>æ¨¡å‹ä¸ºï¼šCONV2D -&gt; RELU -&gt; MAXPOOL -&gt; CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; FLATTEN -&gt; FULLYCONNECTED</p><div class=\"highlight\"><pre><code class=\"language-text\"> - Conv2D: stride 1, padding is &#34;SAME&#34;\n - ReLU\n - Max pool: Use an 8 by 8 filter size and an 8 by 8 stride, padding is &#34;SAME&#34;\n - Conv2D: stride 1, padding is &#34;SAME&#34;\n - ReLU\n - Max pool: Use a 4 by 4 filter size and a 4 by 4 stride, padding is &#34;SAME&#34;\n - Flatten the previous output.\n - FULLYCONNECTED (FC) layerï¼šè¿™é‡Œå…¨è¿æ¥å±‚ä¸éœ€è¦æœ‰æ¿€æ´»å‡½æ•°ï¼Œå› ä¸ºåé¢è®¡ç®—costçš„æ—¶å€™ä¼šåŠ ä¸Šsoftmaxï¼Œå› æ­¤è¿™é‡Œä¸éœ€è¦åŠ </code></pre></div><p>ç”¨åˆ°çš„å‡½æ•°ï¼š</p><ul><li><b>tf.nn.conv2d(X,W1, strides = [1,s,s,1], padding = &#39;SAME&#39;):</b> given an input $X$ and a group of filters $W1$, this function convolves $W1$&#39;s filters on X. The third input ([1,f,f,1]) represents the strides for each dimension of the input (m, n_H_prev, n_W_prev, n_C_prev). You can read the full documentation <a href=\"https://link.zhihu.com/?target=https%3A//www.tensorflow.org/api_docs/python/tf/nn/conv2d\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">here</a></li><li><b>tf.nn.max_pool(A, ksize = [1,f,f,1], strides = [1,s,s,1], padding = &#39;SAME&#39;):</b> given an input A, this function uses a window of size (f, f) and strides of size (s, s) to carry out max pooling over each window. You can read the full documentation <a href=\"https://link.zhihu.com/?target=https%3A//www.tensorflow.org/api_docs/python/tf/nn/max_pool\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">here</a></li><li><b>tf.nn.relu(Z1):</b> computes the elementwise ReLU of Z1 (which can be any shape). You can read the full documentation <a href=\"https://link.zhihu.com/?target=https%3A//www.tensorflow.org/api_docs/python/tf/nn/relu\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">here.</a></li><li><b>tf.contrib.layers.flatten(P)</b>: given an input P, this function flattens each example into a 1D vector it while maintaining the batch-size. It returns a flattened tensor with shape [batch_size, k]. You can read the full documentation <a href=\"https://link.zhihu.com/?target=https%3A//www.tensorflow.org/api_docs/python/tf/contrib/layers/flatten\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">here.</a></li><li><b>tf.contrib.layers.fully_connected(F, num_outputs):</b> given a the flattened input F, it returns the output computed using a fully connected layer. You can read the full documentation <a href=\"https://link.zhihu.com/?target=https%3A//www.tensorflow.org/api_docs/python/tf/contrib/layers/fully_connected\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">here.</a></li></ul><p>In the last function above (<code>tf.contrib.layers.fully_connected</code>), the fully connected layer automatically initializes weights in the graph and keeps on training them as you train the model. Hence, you did not need to initialize those weights when initializing the parameters. </p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: forward_propagation</span>\n<span class=\"err\">â€‹</span>\n<span class=\"k\">def</span> <span class=\"nf\">forward_propagation</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">parameters</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Implements the forward propagation for the model:\n</span><span class=\"s2\">    CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; FLATTEN -&gt; FULLYCONNECTED\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    X -- input dataset placeholder, of shape (input size, number of examples)\n</span><span class=\"s2\">    parameters -- python dictionary containing your parameters &#34;W1&#34;, &#34;W2&#34;\n</span><span class=\"s2\">                  the shapes are given in initialize_parameters\n</span><span class=\"s2\">â€‹\n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    Z3 -- the output of the last LINEAR unit\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"c1\"># Retrieve the parameters from the dictionary &#34;parameters&#34; </span>\n    <span class=\"n\">W1</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;W1&#39;</span><span class=\"p\">]</span>\n    <span class=\"n\">W2</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;W2&#39;</span><span class=\"p\">]</span>\n    \n    <span class=\"c1\">### START CODE HERE ###</span>\n    <span class=\"c1\"># CONV2D: stride of 1, padding &#39;SAME&#39;</span>\n    <span class=\"n\">Z1</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">conv2d</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"nb\">filter</span><span class=\"o\">=</span><span class=\"n\">W1</span><span class=\"p\">,</span> <span class=\"n\">strides</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">],</span><span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"s1\">&#39;SAME&#39;</span><span class=\"p\">)</span>\n    <span class=\"c1\"># RELU</span>\n    <span class=\"n\">A1</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">relu</span><span class=\"p\">(</span><span class=\"n\">Z1</span><span class=\"p\">)</span>\n    <span class=\"c1\"># MAXPOOL: window 8x8, sride 8, padding &#39;SAME&#39;</span>\n    <span class=\"n\">P1</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">max_pool</span><span class=\"p\">(</span><span class=\"n\">A1</span><span class=\"p\">,</span><span class=\"n\">ksize</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">strides</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span><span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"s1\">&#39;SAME&#39;</span><span class=\"p\">)</span>\n    <span class=\"c1\"># CONV2D: filters W2, stride 1, padding &#39;SAME&#39;</span>\n    <span class=\"n\">Z2</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">conv2d</span><span class=\"p\">(</span><span class=\"n\">P1</span><span class=\"p\">,</span> <span class=\"nb\">filter</span><span class=\"o\">=</span><span class=\"n\">W2</span><span class=\"p\">,</span> <span class=\"n\">strides</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span><span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"s1\">&#39;SAME&#39;</span><span class=\"p\">)</span>\n    <span class=\"c1\"># RELU</span>\n    <span class=\"n\">A2</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">relu</span><span class=\"p\">(</span><span class=\"n\">Z2</span><span class=\"p\">)</span>\n    <span class=\"c1\"># MAXPOOL: window 4x4, stride 4, padding &#39;SAME&#39;</span>\n    <span class=\"n\">P2</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">max_pool</span><span class=\"p\">(</span><span class=\"n\">A2</span><span class=\"p\">,</span><span class=\"n\">ksize</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">strides</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span><span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"s1\">&#39;SAME&#39;</span><span class=\"p\">)</span>\n    <span class=\"c1\"># FLATTEN</span>\n    <span class=\"n\">P2</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">contrib</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">flatten</span><span class=\"p\">(</span><span class=\"n\">P2</span><span class=\"p\">)</span>\n    <span class=\"c1\"># FULLY-CONNECTED without non-linear activation function (not not call softmax).</span>\n    <span class=\"c1\"># 6 neurons in output layer. Hint: one of the arguments should be &#34;activation_fn=None&#34; </span>\n    <span class=\"n\">Z3</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">contrib</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">fully_connected</span><span class=\"p\">(</span><span class=\"n\">P2</span><span class=\"p\">,</span> <span class=\"mi\">6</span><span class=\"p\">,</span><span class=\"n\">activation_fn</span><span class=\"o\">=</span><span class=\"bp\">None</span><span class=\"p\">)</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n<span class=\"err\">â€‹</span>\n    <span class=\"k\">return</span> <span class=\"n\">Z3</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>4. Compute cost</b></h2><ul><li><b>tf.nn.softmax_cross_entropy_with_logits(logits = Z3, labels = Y):</b> computes the softmax entropy loss. This function both computes the softmax activation function as well as the resulting loss. You can check the full documentation  <a href=\"https://link.zhihu.com/?target=https%3A//www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">here.</a>è¿™ä¸ªå‡½æ•°å·²ç»åŒ…å«äº†è®¡ç®—softmaxï¼Œè¿˜æœ‰æ±‚cross-entropyä¸¤ä»¶äº‹äº†ã€‚</li><li><b>tf.reduce_mean:</b> computes the mean of elements across dimensions of a tensor. Use this to sum the losses over all the examples to get the overall cost. You can check the full documentation <a href=\"https://link.zhihu.com/?target=https%3A//www.tensorflow.org/api_docs/python/tf/reduce_mean\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">here.</a></li></ul><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: compute_cost </span>\n<span class=\"err\">â€‹</span>\n<span class=\"k\">def</span> <span class=\"nf\">compute_cost</span><span class=\"p\">(</span><span class=\"n\">Z3</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Computes the cost\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n</span><span class=\"s2\">    Y -- &#34;true&#34; labels vector placeholder, same shape as Z3\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    cost - Tensor of the cost function\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"c1\">### START CODE HERE ### (1 line of code)</span>\n    <span class=\"n\">cost</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">reduce_mean</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">softmax_cross_entropy_with_logits</span><span class=\"p\">(</span><span class=\"n\">logits</span><span class=\"o\">=</span><span class=\"n\">Z3</span><span class=\"p\">,</span><span class=\"n\">labels</span><span class=\"o\">=</span><span class=\"n\">Y</span><span class=\"p\">))</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">cost</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>5. Model</b></h2><p>æŠŠå‰é¢çš„å‡½æ•°éƒ½ç»“åˆèµ·æ¥ï¼Œåˆ›å»ºä¸€ä¸ªå®Œæ•´çš„æ¨¡å‹ã€‚</p><p>å…¶ä¸­<code>random_mini_batches()</code>å·²ç»ç»™æˆ‘ä»¬äº†ï¼Œä¼˜åŒ–å™¨ä½¿ç”¨äº†</p><p><code>optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)</code></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: model</span>\n<span class=\"err\">â€‹</span>\n<span class=\"k\">def</span> <span class=\"nf\">model</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">,</span> <span class=\"n\">Y_train</span><span class=\"p\">,</span> <span class=\"n\">X_test</span><span class=\"p\">,</span> <span class=\"n\">Y_test</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span> <span class=\"o\">=</span> <span class=\"mf\">0.009</span><span class=\"p\">,</span>\n          <span class=\"n\">num_epochs</span> <span class=\"o\">=</span> <span class=\"mi\">100</span><span class=\"p\">,</span> <span class=\"n\">minibatch_size</span> <span class=\"o\">=</span> <span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"n\">print_cost</span> <span class=\"o\">=</span> <span class=\"bp\">True</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Implements a three-layer ConvNet in Tensorflow:\n</span><span class=\"s2\">    CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; FLATTEN -&gt; FULLYCONNECTED\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    X_train -- training set, of shape (None, 64, 64, 3)\n</span><span class=\"s2\">    Y_train -- test set, of shape (None, n_y = 6)\n</span><span class=\"s2\">    X_test -- training set, of shape (None, 64, 64, 3)\n</span><span class=\"s2\">    Y_test -- test set, of shape (None, n_y = 6)\n</span><span class=\"s2\">    learning_rate -- learning rate of the optimization\n</span><span class=\"s2\">    num_epochs -- number of epochs of the optimization loop\n</span><span class=\"s2\">    minibatch_size -- size of a minibatch\n</span><span class=\"s2\">    print_cost -- True to print the cost every 100 epochs\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    train_accuracy -- real number, accuracy on the train set (X_train)\n</span><span class=\"s2\">    test_accuracy -- real number, testing accuracy on the test set (X_test)\n</span><span class=\"s2\">    parameters -- parameters learnt by the model. They can then be used to predict.\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"n\">ops</span><span class=\"o\">.</span><span class=\"n\">reset_default_graph</span><span class=\"p\">()</span>                         <span class=\"c1\"># to be able to rerun the model without overwriting tf variables</span>\n    <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">set_random_seed</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>                             <span class=\"c1\"># to keep results consistent (tensorflow seed)</span>\n    <span class=\"n\">seed</span> <span class=\"o\">=</span> <span class=\"mi\">3</span>                                          <span class=\"c1\"># to keep results consistent (numpy seed)</span>\n    <span class=\"p\">(</span><span class=\"n\">m</span><span class=\"p\">,</span> <span class=\"n\">n_H0</span><span class=\"p\">,</span> <span class=\"n\">n_W0</span><span class=\"p\">,</span> <span class=\"n\">n_C0</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">X_train</span><span class=\"o\">.</span><span class=\"n\">shape</span>             \n    <span class=\"n\">n_y</span> <span class=\"o\">=</span> <span class=\"n\">Y_train</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span>                            \n    <span class=\"n\">costs</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>                                        <span class=\"c1\"># To keep track of the cost</span>\n    \n    <span class=\"c1\"># Create Placeholders of the correct shape</span>\n    <span class=\"c1\">### START CODE HERE ### (1 line)</span>\n    <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span> <span class=\"o\">=</span> <span class=\"n\">create_placeholders</span><span class=\"p\">(</span><span class=\"n\">n_H0</span><span class=\"p\">,</span> <span class=\"n\">n_W0</span><span class=\"p\">,</span><span class=\"n\">n_C0</span><span class=\"p\">,</span><span class=\"n\">n_y</span><span class=\"p\">)</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n<span class=\"err\">â€‹</span>\n    <span class=\"c1\"># Initialize parameters</span>\n    <span class=\"c1\">### START CODE HERE ### (1 line)</span>\n    <span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"n\">initialize_parameters</span><span class=\"p\">()</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"c1\"># Forward propagation: Build the forward propagation in the tensorflow graph</span>\n    <span class=\"c1\">### START CODE HERE ### (1 line)</span>\n    <span class=\"n\">Z3</span> <span class=\"o\">=</span> <span class=\"n\">forward_propagation</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span><span class=\"n\">parameters</span><span class=\"p\">)</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"c1\"># Cost function: Add cost function to tensorflow graph</span>\n    <span class=\"c1\">### START CODE HERE ### (1 line)</span>\n    <span class=\"n\">cost</span> <span class=\"o\">=</span> <span class=\"n\">compute_cost</span><span class=\"p\">(</span><span class=\"n\">Z3</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">)</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"c1\"># Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer that minimizes the cost.</span>\n    <span class=\"c1\">### START CODE HERE ### (1 line)</span>\n    <span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"o\">.</span><span class=\"n\">AdamOptimizer</span><span class=\"p\">(</span><span class=\"n\">learning_rate</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">minimize</span><span class=\"p\">(</span><span class=\"n\">cost</span><span class=\"p\">)</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"c1\"># Initialize all the variables globally</span>\n    <span class=\"n\">init</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">global_variables_initializer</span><span class=\"p\">()</span>\n     \n    <span class=\"c1\"># Start the session to compute the tensorflow graph</span>\n    <span class=\"k\">with</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">Session</span><span class=\"p\">()</span> <span class=\"k\">as</span> <span class=\"n\">sess</span><span class=\"p\">:</span>\n        \n        <span class=\"c1\"># Run the initialization</span>\n        <span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">init</span><span class=\"p\">)</span>\n        \n        <span class=\"c1\"># Do the training loop</span>\n        <span class=\"k\">for</span> <span class=\"n\">epoch</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">num_epochs</span><span class=\"p\">):</span>\n<span class=\"err\">â€‹</span>\n            <span class=\"n\">minibatch_cost</span> <span class=\"o\">=</span> <span class=\"mf\">0.</span>\n            <span class=\"n\">num_minibatches</span> <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">m</span> <span class=\"o\">/</span> <span class=\"n\">minibatch_size</span><span class=\"p\">)</span> <span class=\"c1\"># number of minibatches of size minibatch_size in the train set</span>\n            <span class=\"n\">seed</span> <span class=\"o\">=</span> <span class=\"n\">seed</span> <span class=\"o\">+</span> <span class=\"mi\">1</span>\n            <span class=\"n\">minibatches</span> <span class=\"o\">=</span> <span class=\"n\">random_mini_batches</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">,</span> <span class=\"n\">Y_train</span><span class=\"p\">,</span> <span class=\"n\">minibatch_size</span><span class=\"p\">,</span> <span class=\"n\">seed</span><span class=\"p\">)</span>\n<span class=\"err\">â€‹</span>\n            <span class=\"k\">for</span> <span class=\"n\">minibatch</span> <span class=\"ow\">in</span> <span class=\"n\">minibatches</span><span class=\"p\">:</span>\n<span class=\"err\">â€‹</span>\n                <span class=\"c1\"># Select a minibatch</span>\n                <span class=\"p\">(</span><span class=\"n\">minibatch_X</span><span class=\"p\">,</span> <span class=\"n\">minibatch_Y</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">minibatch</span>\n                <span class=\"c1\"># IMPORTANT: The line that runs the graph on a minibatch.</span>\n                <span class=\"c1\"># Run the session to execute the optimizer and the cost, the feedict should contain a minibatch for (X,Y).</span>\n                <span class=\"c1\">### START CODE HERE ### (1 line)</span>\n                <span class=\"n\">_</span> <span class=\"p\">,</span> <span class=\"n\">temp_cost</span> <span class=\"o\">=</span> <span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">([</span><span class=\"n\">optimizer</span><span class=\"p\">,</span><span class=\"n\">cost</span><span class=\"p\">],</span><span class=\"n\">feed_dict</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"n\">X</span><span class=\"p\">:</span><span class=\"n\">minibatch_X</span><span class=\"p\">,</span><span class=\"n\">Y</span><span class=\"p\">:</span><span class=\"n\">minibatch_Y</span><span class=\"p\">})</span>\n                <span class=\"c1\">### END CODE HERE ###</span>\n                \n                <span class=\"n\">minibatch_cost</span> <span class=\"o\">+=</span> <span class=\"n\">temp_cost</span> <span class=\"o\">/</span> <span class=\"n\">num_minibatches</span>\n                \n<span class=\"err\">â€‹</span>\n            <span class=\"c1\"># Print the cost every epoch</span>\n            <span class=\"k\">if</span> <span class=\"n\">print_cost</span> <span class=\"o\">==</span> <span class=\"bp\">True</span> <span class=\"ow\">and</span> <span class=\"n\">epoch</span> <span class=\"o\">%</span> <span class=\"mi\">5</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n                <span class=\"k\">print</span> <span class=\"p\">(</span><span class=\"s2\">&#34;Cost after epoch </span><span class=\"si\">%i</span><span class=\"s2\">: </span><span class=\"si\">%f</span><span class=\"s2\">&#34;</span> <span class=\"o\">%</span> <span class=\"p\">(</span><span class=\"n\">epoch</span><span class=\"p\">,</span> <span class=\"n\">minibatch_cost</span><span class=\"p\">))</span>\n            <span class=\"k\">if</span> <span class=\"n\">print_cost</span> <span class=\"o\">==</span> <span class=\"bp\">True</span> <span class=\"ow\">and</span> <span class=\"n\">epoch</span> <span class=\"o\">%</span> <span class=\"mi\">1</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n                <span class=\"n\">costs</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">minibatch_cost</span><span class=\"p\">)</span>\n        \n        \n        <span class=\"c1\"># plot the cost</span>\n        <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">squeeze</span><span class=\"p\">(</span><span class=\"n\">costs</span><span class=\"p\">))</span>\n        <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">ylabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;cost&#39;</span><span class=\"p\">)</span>\n        <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">xlabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;iterations (per tens)&#39;</span><span class=\"p\">)</span>\n        <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">title</span><span class=\"p\">(</span><span class=\"s2\">&#34;Learning rate =&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">learning_rate</span><span class=\"p\">))</span>\n        <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>\n<span class=\"err\">â€‹</span>\n        <span class=\"c1\"># Calculate the correct predictions</span>\n        <span class=\"n\">predict_op</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">argmax</span><span class=\"p\">(</span><span class=\"n\">Z3</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n        <span class=\"n\">correct_prediction</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">equal</span><span class=\"p\">(</span><span class=\"n\">predict_op</span><span class=\"p\">,</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">argmax</span><span class=\"p\">(</span><span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n        \n        <span class=\"c1\"># Calculate accuracy on the test set</span>\n        <span class=\"n\">accuracy</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">reduce_mean</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">cast</span><span class=\"p\">(</span><span class=\"n\">correct_prediction</span><span class=\"p\">,</span> <span class=\"s2\">&#34;float&#34;</span><span class=\"p\">))</span>\n        <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">accuracy</span><span class=\"p\">)</span>\n        <span class=\"n\">train_accuracy</span> <span class=\"o\">=</span> <span class=\"n\">accuracy</span><span class=\"o\">.</span><span class=\"nb\">eval</span><span class=\"p\">({</span><span class=\"n\">X</span><span class=\"p\">:</span> <span class=\"n\">X_train</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">:</span> <span class=\"n\">Y_train</span><span class=\"p\">})</span>\n        <span class=\"n\">test_accuracy</span> <span class=\"o\">=</span> <span class=\"n\">accuracy</span><span class=\"o\">.</span><span class=\"nb\">eval</span><span class=\"p\">({</span><span class=\"n\">X</span><span class=\"p\">:</span> <span class=\"n\">X_test</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">:</span> <span class=\"n\">Y_test</span><span class=\"p\">})</span>\n        <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s2\">&#34;Train Accuracy:&#34;</span><span class=\"p\">,</span> <span class=\"n\">train_accuracy</span><span class=\"p\">)</span>\n        <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s2\">&#34;Test Accuracy:&#34;</span><span class=\"p\">,</span> <span class=\"n\">test_accuracy</span><span class=\"p\">)</span>\n                \n        <span class=\"k\">return</span> <span class=\"n\">train_accuracy</span><span class=\"p\">,</span> <span class=\"n\">test_accuracy</span><span class=\"p\">,</span> <span class=\"n\">parameters</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>å¾—åˆ°æ•ˆæœå¦‚å›¾ï¼š</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-849c15a954deb072c1396fedaf1f5b13_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"396\" data-rawheight=\"278\" class=\"content_image\" width=\"396\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;396&#39; height=&#39;278&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"396\" data-rawheight=\"278\" class=\"content_image lazy\" width=\"396\" data-actualsrc=\"https://pic4.zhimg.com/v2-849c15a954deb072c1396fedaf1f5b13_b.jpg\"/></figure><p></p><p></p>", 
            "topic": [
                {
                    "tag": "å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰", 
                    "tagLink": "https://api.zhihu.com/topics/20043586"
                }, 
                {
                    "tag": "ç¥ç»ç½‘ç»œ", 
                    "tagLink": "https://api.zhihu.com/topics/19607065"
                }, 
                {
                    "tag": "æ·±åº¦å­¦ä¹ ï¼ˆDeep Learningï¼‰", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/45752499", 
            "userName": "ç›´ä¸Šäº‘éœ„", 
            "userLink": "https://www.zhihu.com/people/1033165ce4ad9c3fce69a0793dfab8ad", 
            "upvote": 0, 
            "title": "DeepLearning.aiä½œä¸š:(4-1)--å·ç§¯ç¥ç»ç½‘ç»œï¼ˆpart1ï¼‰", 
            "content": "<div class=\"highlight\"><pre><code class=\"language-text\">title: &#39;DeepLearning.aiä½œä¸š:(4-1)-- å·ç§¯ç¥ç»ç½‘ç»œï¼ˆFoundations of CNNï¼‰&#39;\nid: dl-ai-4-1h\ntags:\n  - dl.ai\n  - homework\ncategories:\n  - AI\n  - Deep Learning\ndate: 2018-09-30 16:07:23\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><b>é¦–å‘äºä¸ªäººåšå®¢: </b></p><b><a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Fangzhçš„ä¸ªäººåšå®¢ | äººå·¥æ™ºèƒ½æ‹¯æ•‘ä¸–ç•Œ</a></b><p><b> æ¬¢è¿æ¥è®¿</b></p><p>æœ¬å‘¨çš„ä½œä¸šåˆ†ä¸ºäº†ä¸¤éƒ¨åˆ†ï¼š</p><ul><li>å·ç§¯ç¥ç»ç½‘ç»œçš„æ¨¡å‹æ­å»º</li><li>ç”¨TensorFlowæ¥è®­ç»ƒå·ç§¯ç¥ç»ç½‘ç»œ</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>Part1ï¼šConvolutional Neural Networks: Step by Step</b></h2><p>ä¸»è¦å†…å®¹ï¼š</p><ul><li>convolution funtions:<br/></li><ul><li>Zero Padding</li><li>Convolve window</li><li>Convolution forward</li><li>Convolution backward (optional)</li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>Pooling functionsï¼š<br/></li><ul><li>Pooling forward</li><li>Create mask</li><li>Distribute value</li><li>Pooling backward (optional)</li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>Convolutional Neural Networks</b></h2><p>åˆ›å»ºCNNçš„ä¸»è¦å‡½æ•°</p><p><b>1. Zero Padding</b></p><p>å…ˆåˆ›å»ºä¸€ä¸ªpaddingå‡½æ•°ï¼Œç”¨æ¥è¾“å…¥å›¾åƒXï¼Œè¾“å‡ºpaddingåçš„å›¾åƒï¼Œè¿™é‡Œä½¿ç”¨çš„æ˜¯<code>np.pad()</code>å‡½æ•°ï¼Œ</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">a</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">pad</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"p\">((</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span><span class=\"mi\">3</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">0</span><span class=\"p\">)),</span> <span class=\"s1\">&#39;constant&#39;</span><span class=\"p\">,</span> <span class=\"n\">constant_values</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"o\">..</span><span class=\"p\">,</span><span class=\"o\">..</span><span class=\"p\">))</span>\n<span class=\"err\">è¡¨ç¤º</span><span class=\"n\">aæœ‰5ä¸ªç»´åº¦</span><span class=\"err\">ï¼Œåœ¨ç¬¬</span><span class=\"mi\">1</span><span class=\"err\">ç»´çš„ä¸¤è¾¹éƒ½å¡«ä¸Š</span><span class=\"mi\">1</span><span class=\"err\">ä¸ª</span><span class=\"n\">pad</span><span class=\"err\">ï¼Œå’Œç¬¬</span><span class=\"mi\">3</span><span class=\"err\">ç»´çš„ä¸¤è¾¹éƒ½å¡«ä¸Š</span><span class=\"mi\">3</span><span class=\"err\">ä¸ª</span><span class=\"n\">pad</span><span class=\"err\">ï¼Œ</span><span class=\"n\">constant_valuesè¡¨ç¤ºä¸¤è¾¹è¦å¡«å……çš„å€¼</span>\n<span class=\"err\">â€‹</span>\n<span class=\"k\">def</span> <span class=\"nf\">zero_pad</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">pad</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Pad with zeros all images of the dataset X. The padding is applied to the height and width of an image, \n</span><span class=\"s2\">    as illustrated in Figure 1.\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Argument:\n</span><span class=\"s2\">    X -- python numpy array of shape (m, n_H, n_W, n_C) representing a batch of m images\n</span><span class=\"s2\">    pad -- integer, amount of padding around each image on vertical and horizontal dimensions\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    X_pad -- padded image of shape (m, n_H + 2*pad, n_W + 2*pad, n_C)\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"c1\">### START CODE HERE ### (â‰ˆ 1 line)</span>\n    <span class=\"n\">X_pad</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">pad</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"p\">((</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">0</span><span class=\"p\">),(</span><span class=\"n\">pad</span><span class=\"p\">,</span><span class=\"n\">pad</span><span class=\"p\">),(</span><span class=\"n\">pad</span><span class=\"p\">,</span><span class=\"n\">pad</span><span class=\"p\">),(</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">0</span><span class=\"p\">)),</span> <span class=\"s1\">&#39;constant&#39;</span><span class=\"p\">,</span> <span class=\"n\">constant_values</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">0</span><span class=\"p\">))</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">X_pad</span></code></pre></div><p><b>2.Single step of convolution</b></p><p>åˆ›å»ºä¸€ä¸ªå•æ­¥çš„å·ç§¯è¿ç®—ï¼Œä¹Ÿå°±æ˜¯ä¸€æ¬¡è¾“å…¥ä¸€ä¸ªåˆ‡ç‰‡ï¼Œå¤§å°å’Œå·ç§¯æ ¸ç›¸åŒï¼Œå¯¹åº”å…ƒç´ ç›¸ä¹˜å†æ±‚å’Œï¼Œæœ€åå†åŠ ä¸ªbiasé¡¹ã€‚</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: conv_single_step</span>\n<span class=\"err\">â€‹</span>\n<span class=\"k\">def</span> <span class=\"nf\">conv_single_step</span><span class=\"p\">(</span><span class=\"n\">a_slice_prev</span><span class=\"p\">,</span> <span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Apply one filter defined by parameters W on a single slice (a_slice_prev) of the output activation \n</span><span class=\"s2\">    of the previous layer.\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    a_slice_prev -- slice of input data of shape (f, f, n_C_prev)\n</span><span class=\"s2\">    W -- Weight parameters contained in a window - matrix of shape (f, f, n_C_prev)\n</span><span class=\"s2\">    b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    Z -- a scalar value, result of convolving the sliding window (W, b) on a slice x of the input data\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n<span class=\"err\">â€‹</span>\n    <span class=\"c1\">### START CODE HERE ### (â‰ˆ 2 lines of code)</span>\n    <span class=\"c1\"># Element-wise product between a_slice and W. Do not add the bias yet.</span>\n    <span class=\"n\">s</span> <span class=\"o\">=</span> <span class=\"n\">a_slice_prev</span> <span class=\"o\">*</span> <span class=\"n\">W</span>\n    <span class=\"c1\"># Sum over all entries of the volume s.</span>\n    <span class=\"n\">Z</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">)</span>\n    <span class=\"c1\"># Add bias b to Z. Cast b to a float() so that Z results in a scalar value.</span>\n    <span class=\"n\">Z</span> <span class=\"o\">=</span> <span class=\"n\">Z</span> <span class=\"o\">+</span> <span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"n\">b</span><span class=\"p\">)</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n<span class=\"err\">â€‹</span>\n    <span class=\"k\">return</span> <span class=\"n\">Z</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><b>3.Convolutional Neural Networks - Forward pass</b></p><p>åˆ›å»ºä¸€æ¬¡å®Œæ•´çš„å·ç§¯è¿‡ç¨‹ï¼Œä¹Ÿå°±æ˜¯åˆ©ç”¨ä¸Šé¢çš„ä¸€æ¬¡å·ç§¯ï¼Œè¿›è¡Œforå¾ªç¯ã€‚è¿›è¡Œåˆ‡ç‰‡çš„æ—¶å€™ï¼Œæ³¨æ„è¾¹ç•Œ<code>vert_start, vert_end, horiz_start and horiz_end</code></p><p>è¿™ä¸€æ­¥åº”è¯¥å…ˆå¼„æ¸…æ¥šA_prevï¼ŒAï¼ŒWï¼Œbçš„ç»´åº¦ï¼Œè¶…å‚æ•°é¡¹åŒ…æ‹¬äº†strideå’Œpad</p><p><img src=\"https://www.zhihu.com/equation?tex=+n_H+%3D+%5Clfloor+%5Cfrac%7Bn%7BH%7Bprev%7D%7D+-+f+%2B+2+%5Ctimes+pad%7D%7Bstride%7D+%5Crfloor+%2B1+\" alt=\" n_H = \\lfloor \\frac{n{H{prev}} - f + 2 \\times pad}{stride} \\rfloor +1 \" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=+n_W+%3D+%5Clfloor+%5Cfrac%7Bn%7BW%7Bprev%7D%7D+-+f+%2B+2+%5Ctimes+pad%7D%7Bstride%7D+%5Crfloor+%2B1+\" alt=\" n_W = \\lfloor \\frac{n{W{prev}} - f + 2 \\times pad}{stride} \\rfloor +1 \" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=+n_C+%3D+%5Ctext%7Bnumber+of+filters+used+in+the+convolution%7D\" alt=\" n_C = \\text{number of filters used in the convolution}\" eeimg=\"1\"/> </p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: conv_forward</span>\n<span class=\"err\">â€‹</span>\n<span class=\"k\">def</span> <span class=\"nf\">conv_forward</span><span class=\"p\">(</span><span class=\"n\">A_prev</span><span class=\"p\">,</span> <span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">hparameters</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Implements the forward propagation for a convolution function\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    A_prev -- output activations of the previous layer, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n</span><span class=\"s2\">    W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)\n</span><span class=\"s2\">    b -- Biases, numpy array of shape (1, 1, 1, n_C)\n</span><span class=\"s2\">    hparameters -- python dictionary containing &#34;stride&#34; and &#34;pad&#34;\n</span><span class=\"s2\">        \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\n</span><span class=\"s2\">    cache -- cache of values needed for the conv_backward() function\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"c1\">### START CODE HERE ###</span>\n    <span class=\"c1\"># Retrieve dimensions from A_prev&#39;s shape (â‰ˆ1 line)  </span>\n    <span class=\"p\">(</span><span class=\"n\">m</span><span class=\"p\">,</span> <span class=\"n\">n_H_prev</span><span class=\"p\">,</span> <span class=\"n\">n_W_prev</span><span class=\"p\">,</span> <span class=\"n\">n_C_prev</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">A_prev</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n    \n    <span class=\"c1\"># Retrieve dimensions from W&#39;s shape (â‰ˆ1 line)</span>\n    <span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">,</span> <span class=\"n\">f</span><span class=\"p\">,</span> <span class=\"n\">n_C_prev</span><span class=\"p\">,</span> <span class=\"n\">n_C</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">W</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n    \n    <span class=\"c1\"># Retrieve information from &#34;hparameters&#34; (â‰ˆ2 lines)</span>\n    <span class=\"n\">stride</span> <span class=\"o\">=</span> <span class=\"n\">hparameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;stride&#39;</span><span class=\"p\">]</span>\n    <span class=\"n\">pad</span> <span class=\"o\">=</span> <span class=\"n\">hparameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;pad&#39;</span><span class=\"p\">]</span>\n    \n    <span class=\"c1\"># Compute the dimensions of the CONV output volume using the formula given above. Hint: use int() to floor. (â‰ˆ2 lines)</span>\n    <span class=\"n\">n_H</span> <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">((</span><span class=\"n\">n_H_prev</span> <span class=\"o\">+</span> <span class=\"mi\">2</span> <span class=\"o\">*</span> <span class=\"n\">pad</span> <span class=\"o\">-</span> <span class=\"n\">f</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">stride</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n    <span class=\"n\">n_W</span> <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">((</span><span class=\"n\">n_W_prev</span> <span class=\"o\">+</span> <span class=\"mi\">2</span> <span class=\"o\">*</span> <span class=\"n\">pad</span> <span class=\"o\">-</span> <span class=\"n\">f</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">stride</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"err\">â€‹</span>\n    <span class=\"c1\"># Initialize the output volume Z with zeros. (â‰ˆ1 line)</span>\n    <span class=\"n\">Z</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">m</span><span class=\"p\">,</span> <span class=\"n\">n_H</span><span class=\"p\">,</span> <span class=\"n\">n_W</span><span class=\"p\">,</span> <span class=\"n\">n_C</span><span class=\"p\">))</span>\n    \n    <span class=\"c1\"># Create A_prev_pad by padding A_prev</span>\n    <span class=\"n\">A_prev_pad</span> <span class=\"o\">=</span> <span class=\"n\">zero_pad</span><span class=\"p\">(</span><span class=\"n\">A_prev</span><span class=\"p\">,</span> <span class=\"n\">pad</span><span class=\"p\">)</span>\n    \n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">m</span><span class=\"p\">):</span>                               <span class=\"c1\"># loop over the batch of training examples</span>\n        <span class=\"n\">a_prev_pad</span> <span class=\"o\">=</span> <span class=\"n\">A_prev_pad</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span>                               <span class=\"c1\"># Select ith training example&#39;s padded activation</span>\n        <span class=\"k\">for</span> <span class=\"n\">h</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">n_H</span><span class=\"p\">):</span>                           <span class=\"c1\"># loop over vertical axis of the output volume</span>\n            <span class=\"k\">for</span> <span class=\"n\">w</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">n_W</span><span class=\"p\">):</span>                       <span class=\"c1\"># loop over horizontal axis of the output volume</span>\n                <span class=\"k\">for</span> <span class=\"n\">c</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">n_C</span><span class=\"p\">):</span>                   <span class=\"c1\"># loop over channels (= #filters) of the output volume</span>\n                    \n                    <span class=\"c1\"># Find the corners of the current &#34;slice&#34; (â‰ˆ4 lines)</span>\n                    <span class=\"n\">vert_start</span> <span class=\"o\">=</span> <span class=\"n\">h</span> <span class=\"o\">*</span> <span class=\"n\">stride</span>\n                    <span class=\"n\">vert_end</span> <span class=\"o\">=</span> <span class=\"n\">h</span> <span class=\"o\">*</span> <span class=\"n\">stride</span> <span class=\"o\">+</span> <span class=\"n\">f</span>\n                    <span class=\"n\">horiz_start</span> <span class=\"o\">=</span> <span class=\"n\">w</span> <span class=\"o\">*</span> <span class=\"n\">stride</span>\n                    <span class=\"n\">horiz_end</span> <span class=\"o\">=</span> <span class=\"n\">w</span> <span class=\"o\">*</span> <span class=\"n\">stride</span> <span class=\"o\">+</span> <span class=\"n\">f</span>\n                    \n                    <span class=\"c1\"># Use the corners to define the (3D) slice of a_prev_pad (See Hint above the cell). (â‰ˆ1 line)</span>\n                    <span class=\"n\">a_slice_prev</span> <span class=\"o\">=</span> <span class=\"n\">a_prev_pad</span><span class=\"p\">[</span><span class=\"n\">vert_start</span> <span class=\"p\">:</span> <span class=\"n\">vert_end</span><span class=\"p\">,</span> <span class=\"n\">horiz_start</span> <span class=\"p\">:</span> <span class=\"n\">horiz_end</span><span class=\"p\">]</span>\n                    \n                    <span class=\"c1\"># Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron. (â‰ˆ1 line)</span>\n                    <span class=\"n\">Z</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">h</span><span class=\"p\">,</span> <span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">c</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">conv_single_step</span><span class=\"p\">(</span><span class=\"n\">a_slice_prev</span><span class=\"p\">,</span><span class=\"n\">W</span><span class=\"p\">[:,:,:,</span><span class=\"n\">c</span><span class=\"p\">],</span><span class=\"n\">b</span><span class=\"p\">[:,:,:,</span><span class=\"n\">c</span><span class=\"p\">])</span>\n                                        \n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"c1\"># Making sure your output shape is correct</span>\n    <span class=\"k\">assert</span><span class=\"p\">(</span><span class=\"n\">Z</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"p\">(</span><span class=\"n\">m</span><span class=\"p\">,</span> <span class=\"n\">n_H</span><span class=\"p\">,</span> <span class=\"n\">n_W</span><span class=\"p\">,</span> <span class=\"n\">n_C</span><span class=\"p\">))</span>\n    \n    <span class=\"c1\"># Save information in &#34;cache&#34; for the backprop</span>\n    <span class=\"n\">cache</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">A_prev</span><span class=\"p\">,</span> <span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">hparameters</span><span class=\"p\">)</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">Z</span><span class=\"p\">,</span> <span class=\"n\">cache</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>Pooling layer</b></h2><p>åˆ›å»ºæ± åŒ–å±‚ï¼Œæ³¨æ„å¾—åˆ°çš„ç»´åº¦éœ€è¦å‘ä¸‹å–æ•´ï¼Œç”¨int()å¯¹float()è¿›è¡Œè½¬æ¢</p><p><img src=\"https://www.zhihu.com/equation?tex=n_H+%3D+%5Clfloor+%5Cfrac%7Bn%7BH%7Bprev%7D%7D+-+f%7D%7Bstride%7D+%5Crfloor+%2B1\" alt=\"n_H = \\lfloor \\frac{n{H{prev}} - f}{stride} \\rfloor +1\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=n_W+%3D+%5Clfloor+%5Cfrac%7Bn%7BW%7Bprev%7D%7D+-+f%7D%7Bstride%7D+%5Crfloor+%2B1\" alt=\"n_W = \\lfloor \\frac{n{W{prev}} - f}{stride} \\rfloor +1\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=n_C+%3D+n_%7BC_%7Bprev%7D%7D\" alt=\"n_C = n_{C_{prev}}\" eeimg=\"1\"/> </p><p>åŒæ ·éœ€è¦å…ˆè¿›è¡Œåˆ‡è¾¹ï¼Œè€Œååˆ†ä¸ºmaxå’Œaverageä¸¤ç§ï¼Œåˆ†åˆ«ç”¨np.maxå’Œnp.mean</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">pool_forward</span><span class=\"p\">(</span><span class=\"n\">A_prev</span><span class=\"p\">,</span> <span class=\"n\">hparameters</span><span class=\"p\">,</span> <span class=\"n\">mode</span> <span class=\"o\">=</span> <span class=\"s2\">&#34;max&#34;</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Implements the forward pass of the pooling layer\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n</span><span class=\"s2\">    hparameters -- python dictionary containing &#34;f&#34; and &#34;stride&#34;\n</span><span class=\"s2\">    mode -- the pooling mode you would like to use, defined as a string (&#34;max&#34; or &#34;average&#34;)\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C)\n</span><span class=\"s2\">    cache -- cache used in the backward pass of the pooling layer, contains the input and hparameters \n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"c1\"># Retrieve dimensions from the input shape</span>\n    <span class=\"p\">(</span><span class=\"n\">m</span><span class=\"p\">,</span> <span class=\"n\">n_H_prev</span><span class=\"p\">,</span> <span class=\"n\">n_W_prev</span><span class=\"p\">,</span> <span class=\"n\">n_C_prev</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">A_prev</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n    \n    <span class=\"c1\"># Retrieve hyperparameters from &#34;hparameters&#34;</span>\n    <span class=\"n\">f</span> <span class=\"o\">=</span> <span class=\"n\">hparameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;f&#34;</span><span class=\"p\">]</span>\n    <span class=\"n\">stride</span> <span class=\"o\">=</span> <span class=\"n\">hparameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;stride&#34;</span><span class=\"p\">]</span>\n    \n    <span class=\"c1\"># Define the dimensions of the output</span>\n    <span class=\"n\">n_H</span> <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"mi\">1</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">n_H_prev</span> <span class=\"o\">-</span> <span class=\"n\">f</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">stride</span><span class=\"p\">)</span>\n    <span class=\"n\">n_W</span> <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"mi\">1</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">n_W_prev</span> <span class=\"o\">-</span> <span class=\"n\">f</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">stride</span><span class=\"p\">)</span>\n    <span class=\"n\">n_C</span> <span class=\"o\">=</span> <span class=\"n\">n_C_prev</span>\n    \n    <span class=\"c1\"># Initialize output matrix A</span>\n    <span class=\"n\">A</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">m</span><span class=\"p\">,</span> <span class=\"n\">n_H</span><span class=\"p\">,</span> <span class=\"n\">n_W</span><span class=\"p\">,</span> <span class=\"n\">n_C</span><span class=\"p\">))</span>              \n    \n    <span class=\"c1\">### START CODE HERE ###</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">m</span><span class=\"p\">):</span>                         <span class=\"c1\"># loop over the training examples</span>\n        <span class=\"k\">for</span> <span class=\"n\">h</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">n_H</span><span class=\"p\">):</span>                     <span class=\"c1\"># loop on the vertical axis of the output volume</span>\n            <span class=\"k\">for</span> <span class=\"n\">w</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">n_W</span><span class=\"p\">):</span>                 <span class=\"c1\"># loop on the horizontal axis of the output volume</span>\n                <span class=\"k\">for</span> <span class=\"n\">c</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span> <span class=\"p\">(</span><span class=\"n\">n_C</span><span class=\"p\">):</span>            <span class=\"c1\"># loop over the channels of the output volume</span>\n                    \n                    <span class=\"c1\"># Find the corners of the current &#34;slice&#34; (â‰ˆ4 lines)</span>\n                    <span class=\"n\">vert_start</span> <span class=\"o\">=</span> <span class=\"n\">h</span> <span class=\"o\">*</span> <span class=\"n\">stride</span>\n                    <span class=\"n\">vert_end</span> <span class=\"o\">=</span> <span class=\"n\">vert_start</span> <span class=\"o\">+</span> <span class=\"n\">f</span>\n                    <span class=\"n\">horiz_start</span> <span class=\"o\">=</span> <span class=\"n\">w</span> <span class=\"o\">*</span> <span class=\"n\">stride</span>\n                    <span class=\"n\">horiz_end</span> <span class=\"o\">=</span> <span class=\"n\">horiz_start</span> <span class=\"o\">+</span> <span class=\"n\">f</span>\n                    \n                    <span class=\"c1\"># Use the corners to define the current slice on the ith training example of A_prev, channel c. (â‰ˆ1 line)</span>\n                    <span class=\"n\">a_prev_slice</span> <span class=\"o\">=</span> <span class=\"n\">A_prev</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">vert_start</span> <span class=\"p\">:</span> <span class=\"n\">vert_end</span><span class=\"p\">,</span> <span class=\"n\">horiz_start</span> <span class=\"p\">:</span> <span class=\"n\">horiz_end</span><span class=\"p\">,</span> <span class=\"n\">c</span><span class=\"p\">]</span>\n                    \n                    <span class=\"c1\"># Compute the pooling operation on the slice. Use an if statment to differentiate the modes. Use np.max/np.mean.</span>\n                    <span class=\"k\">if</span> <span class=\"n\">mode</span> <span class=\"o\">==</span> <span class=\"s2\">&#34;max&#34;</span><span class=\"p\">:</span>\n                        <span class=\"n\">A</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">h</span><span class=\"p\">,</span> <span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">c</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">max</span><span class=\"p\">(</span><span class=\"n\">a_prev_slice</span><span class=\"p\">)</span>\n                    <span class=\"k\">elif</span> <span class=\"n\">mode</span> <span class=\"o\">==</span> <span class=\"s2\">&#34;average&#34;</span><span class=\"p\">:</span>\n                        <span class=\"n\">A</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">h</span><span class=\"p\">,</span> <span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">c</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">a_prev_slice</span><span class=\"p\">)</span>\n    \n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"c1\"># Store the input and hparameters in &#34;cache&#34; for pool_backward()</span>\n    <span class=\"n\">cache</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">A_prev</span><span class=\"p\">,</span> <span class=\"n\">hparameters</span><span class=\"p\">)</span>\n    \n    <span class=\"c1\"># Making sure your output shape is correct</span>\n    <span class=\"k\">assert</span><span class=\"p\">(</span><span class=\"n\">A</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"p\">(</span><span class=\"n\">m</span><span class=\"p\">,</span> <span class=\"n\">n_H</span><span class=\"p\">,</span> <span class=\"n\">n_W</span><span class=\"p\">,</span> <span class=\"n\">n_C</span><span class=\"p\">))</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">A</span><span class=\"p\">,</span> <span class=\"n\">cache</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>Backpropagation in convolutional neural networks </b></h2><p>å·ç§¯ç¥ç»ç½‘ç»œçš„æ±‚å¯¼æ˜¯æ¯”è¾ƒéš¾ä»¥ç†è§£çš„ï¼Œè¿™é‡Œæœ‰å·ç§¯å±‚çš„æ±‚å¯¼å’Œæ± åŒ–å±‚çš„æ±‚å¯¼ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>1.Convolutional layer backward pass</b></h2><p>å‡è®¾ç»è¿‡å·ç§¯å±‚åæˆ‘ä»¬çš„è¾“å‡º <img src=\"https://www.zhihu.com/equation?tex=Z+%3D+W+%5Ctimes+A+%2Bb\" alt=\"Z = W \\times A +b\" eeimg=\"1\"/> </p><p>é‚£ä¹ˆåå‘ä¼ æ’­è¿‡ç¨‹ä¸­éœ€è¦æ±‚çš„å°±æ˜¯ <img src=\"https://www.zhihu.com/equation?tex=dA%2CdW%2Cdb\" alt=\"dA,dW,db\" eeimg=\"1\"/> ï¼Œå…¶ä¸­ <img src=\"https://www.zhihu.com/equation?tex=dA\" alt=\"dA\" eeimg=\"1\"/> æ˜¯åŸè¾“å…¥çš„æ•°æ®ï¼ŒåŒ…å«äº†åŸå›¾åƒä¸­çš„æ¯ä¸€ä¸ªåƒç´ ï¼Œ</p><p>è€Œè¿™ä¸ªæ—¶å€™å‡è®¾ä»åé¢ä¼ è¿‡æ¥çš„ <img src=\"https://www.zhihu.com/equation?tex=dZ\" alt=\"dZ\" eeimg=\"1\"/> æ˜¯å·²ç»çŸ¥é“çš„ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>1.è®¡ç®—dA</b></p><p>ä»å…¬å¼å¯ä»¥çœ‹å‡ºï¼Œ <img src=\"https://www.zhihu.com/equation?tex=dA+%3D+W+%5Ctimes+dZ\" alt=\"dA = W \\times dZ\" eeimg=\"1\"/> ï¼Œå…·ä½“ä¸€ç‚¹ï¼Œ <img src=\"https://www.zhihu.com/equation?tex=dA\" alt=\"dA\" eeimg=\"1\"/> çš„æ¯ä¸€ä¸ªåˆ‡ç‰‡å°±æ˜¯ <img src=\"https://www.zhihu.com/equation?tex=W_c\" alt=\"W_c\" eeimg=\"1\"/> ä¹˜ä¸Š$dZ$åœ¨è¾“å‡ºå›¾ç‰‡çš„<b>æ¯ä¸€ä¸ªåƒç´ </b>çš„æ±‚å’Œç»“æœï¼Œä»çŸ©é˜µçš„è§’åº¦ï¼Œæ¯ä¸€æ¬¡ <img src=\"https://www.zhihu.com/equation?tex=W_c%5Ctimes+dZ_%7Bhw%7D\" alt=\"W_c\\times dZ_{hw}\" eeimg=\"1\"/> å¾—åˆ°çš„å°±æ˜¯ä»<b>å•ä¸ªè¾“å‡ºçš„å›¾ç‰‡åƒç´ åˆ°è¾“å…¥å›¾ç‰‡åˆ‡ç‰‡ï¼ˆå¤§å°ä¸ºWï¼‰</b>çš„æ˜ å°„ã€‚å› æ­¤å…¬å¼ä¸ºï¼š</p><p><img src=\"https://www.zhihu.com/equation?tex=dA+%2B%3D+%5Csum+_%7Bh%3D0%7D+%5E%7Bn_H%7D+%5Csum_%7Bw%3D0%7D+%5E%7Bn_W%7D+W_c+%5Ctimes+dZ_%7Bhw%7D+\" alt=\"dA += \\sum _{h=0} ^{n_H} \\sum_{w=0} ^{n_W} W_c \\times dZ_{hw} \" eeimg=\"1\"/> </p><div class=\"highlight\"><pre><code class=\"language-text\">da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><b>2.è®¡ç®—dW</b></p><p>$dW = A \\times dZ$ï¼Œè€Œæ›´å…·ä½“ä¸€ç‚¹ï¼Œå› ä¸º<b>Wå¯¹Zçš„æ¯ä¸€ä¸ªåƒç´ éƒ½æ˜¯æœ‰ä½œç”¨çš„</b>ï¼Œæ‰€ä»¥å°±ç­‰äºæ¯ä¸€ä¸ªè¾“å…¥å›¾ç‰‡çš„åˆ‡ç‰‡ä¹˜ä»¥å¯¹åº”è¾“å‡ºå›¾ç‰‡åƒç´ çš„å¯¼æ•°ï¼Œç„¶åå†æ±‚å’Œï¼</p><p><img src=\"https://www.zhihu.com/equation?tex=dW_c++%2B%3D+%5Csum+_%7Bh%3D0%7D+%5E%7Bn_H%7D+%5Csum_%7Bw%3D0%7D+%5E+%7Bn_W%7D+a_%7Bslice%7D+%5Ctimes+dZ_%7Bhw%7D\" alt=\"dW_c  += \\sum _{h=0} ^{n_H} \\sum_{w=0} ^ {n_W} a_{slice} \\times dZ_{hw}\" eeimg=\"1\"/> </p><div class=\"highlight\"><pre><code class=\"language-text\">dW[:,:,:,c] += a_slice * dZ[i, h, w, c]</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><b>3.è®¡ç®—db</b></p><p><img src=\"https://www.zhihu.com/equation?tex=db+%3D+%5Csum_h+%5Csum_w+dZ_%7Bhw%7D+\" alt=\"db = \\sum_h \\sum_w dZ_{hw} \" eeimg=\"1\"/> </p><div class=\"highlight\"><pre><code class=\"language-text\">db[:,:,:,c] += dZ[i, h, w, c]</code></pre></div><p>æ‰€ä»¥å¾—åˆ°ä»¥ä¸‹ï¼š</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">conv_backward</span><span class=\"p\">(</span><span class=\"n\">dZ</span><span class=\"p\">,</span> <span class=\"n\">cache</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Implement the backward propagation for a convolution function\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)\n</span><span class=\"s2\">    cache -- cache of values needed for the conv_backward(), output of conv_forward()\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n</span><span class=\"s2\">               numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n</span><span class=\"s2\">    dW -- gradient of the cost with respect to the weights of the conv layer (W)\n</span><span class=\"s2\">          numpy array of shape (f, f, n_C_prev, n_C)\n</span><span class=\"s2\">    db -- gradient of the cost with respect to the biases of the conv layer (b)\n</span><span class=\"s2\">          numpy array of shape (1, 1, 1, n_C)\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"c1\">### START CODE HERE ###</span>\n    <span class=\"c1\"># Retrieve information from &#34;cache&#34;</span>\n    <span class=\"p\">(</span><span class=\"n\">A_prev</span><span class=\"p\">,</span> <span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">hparameters</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">cache</span>\n    \n    <span class=\"c1\"># Retrieve dimensions from A_prev&#39;s shape</span>\n    <span class=\"p\">(</span><span class=\"n\">m</span><span class=\"p\">,</span> <span class=\"n\">n_H_prev</span><span class=\"p\">,</span> <span class=\"n\">n_W_prev</span><span class=\"p\">,</span> <span class=\"n\">n_C_prev</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">A_prev</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n    \n    <span class=\"c1\"># Retrieve dimensions from W&#39;s shape</span>\n    <span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">,</span> <span class=\"n\">f</span><span class=\"p\">,</span> <span class=\"n\">n_C_prev</span><span class=\"p\">,</span> <span class=\"n\">n_C</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">W</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n    \n    <span class=\"c1\"># Retrieve information from &#34;hparameters&#34;</span>\n    <span class=\"n\">stride</span> <span class=\"o\">=</span> <span class=\"n\">hparameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;stride&#39;</span><span class=\"p\">]</span>\n    <span class=\"n\">pad</span> <span class=\"o\">=</span> <span class=\"n\">hparameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;pad&#39;</span><span class=\"p\">]</span>\n    \n    <span class=\"c1\"># Retrieve dimensions from dZ&#39;s shape</span>\n    <span class=\"p\">(</span><span class=\"n\">m</span><span class=\"p\">,</span> <span class=\"n\">n_H</span><span class=\"p\">,</span> <span class=\"n\">n_W</span><span class=\"p\">,</span> <span class=\"n\">n_C</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">dZ</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n    \n    <span class=\"c1\"># Initialize dA_prev, dW, db with the correct shapes</span>\n    <span class=\"n\">dA_prev</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"n\">A_prev</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span>                           \n    <span class=\"n\">dW</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"n\">W</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span>\n    <span class=\"n\">db</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"n\">b</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span>\n<span class=\"err\">â€‹</span>\n    <span class=\"c1\"># Pad A_prev and dA_prev</span>\n    <span class=\"n\">A_prev_pad</span> <span class=\"o\">=</span> <span class=\"n\">zero_pad</span><span class=\"p\">(</span><span class=\"n\">A_prev</span><span class=\"p\">,</span> <span class=\"n\">pad</span><span class=\"p\">)</span>\n    <span class=\"n\">dA_prev_pad</span> <span class=\"o\">=</span> <span class=\"n\">zero_pad</span><span class=\"p\">(</span><span class=\"n\">dA_prev</span><span class=\"p\">,</span> <span class=\"n\">pad</span><span class=\"p\">)</span>\n    \n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">m</span><span class=\"p\">):</span>                       <span class=\"c1\"># loop over the training examples</span>\n        \n        <span class=\"c1\"># select ith training example from A_prev_pad and dA_prev_pad</span>\n        <span class=\"n\">a_prev_pad</span> <span class=\"o\">=</span> <span class=\"n\">A_prev_pad</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span>\n        <span class=\"n\">da_prev_pad</span> <span class=\"o\">=</span> <span class=\"n\">dA_prev_pad</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span>\n        \n        <span class=\"k\">for</span> <span class=\"n\">h</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">n_H</span><span class=\"p\">):</span>                   <span class=\"c1\"># loop over vertical axis of the output volume</span>\n            <span class=\"k\">for</span> <span class=\"n\">w</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">n_W</span><span class=\"p\">):</span>               <span class=\"c1\"># loop over horizontal axis of the output volume</span>\n                <span class=\"k\">for</span> <span class=\"n\">c</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">n_C</span><span class=\"p\">):</span>           <span class=\"c1\"># loop over the channels of the output volume</span>\n                    \n                    <span class=\"c1\"># Find the corners of the current &#34;slice&#34;</span>\n                    <span class=\"n\">vert_start</span> <span class=\"o\">=</span> <span class=\"n\">h</span> <span class=\"o\">*</span> <span class=\"n\">stride</span>\n                    <span class=\"n\">vert_end</span> <span class=\"o\">=</span> <span class=\"n\">h</span> <span class=\"o\">*</span> <span class=\"n\">stride</span> <span class=\"o\">+</span> <span class=\"n\">f</span>\n                    <span class=\"n\">horiz_start</span> <span class=\"o\">=</span> <span class=\"n\">w</span> <span class=\"o\">*</span> <span class=\"n\">stride</span>\n                    <span class=\"n\">horiz_end</span> <span class=\"o\">=</span> <span class=\"n\">w</span> <span class=\"o\">*</span> <span class=\"n\">stride</span> <span class=\"o\">+</span> <span class=\"n\">f</span>\n                    \n                    <span class=\"c1\"># Use the corners to define the slice from a_prev_pad</span>\n                    <span class=\"n\">a_slice</span> <span class=\"o\">=</span> <span class=\"n\">a_prev_pad</span><span class=\"p\">[</span><span class=\"n\">vert_start</span> <span class=\"p\">:</span> <span class=\"n\">vert_end</span><span class=\"p\">,</span> <span class=\"n\">horiz_start</span> <span class=\"p\">:</span> <span class=\"n\">horiz_end</span><span class=\"p\">,</span> <span class=\"p\">:</span> <span class=\"p\">]</span>\n<span class=\"err\">â€‹</span>\n                    <span class=\"c1\"># Update gradients for the window and the filter&#39;s parameters using the code formulas given above</span>\n                    <span class=\"n\">da_prev_pad</span><span class=\"p\">[</span><span class=\"n\">vert_start</span><span class=\"p\">:</span><span class=\"n\">vert_end</span><span class=\"p\">,</span> <span class=\"n\">horiz_start</span><span class=\"p\">:</span><span class=\"n\">horiz_end</span><span class=\"p\">,</span> <span class=\"p\">:]</span> <span class=\"o\">+=</span> <span class=\"n\">W</span><span class=\"p\">[:,:,:,</span><span class=\"n\">c</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">dZ</span><span class=\"p\">[</span> <span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">h</span><span class=\"p\">,</span> <span class=\"n\">w</span> <span class=\"p\">,</span><span class=\"n\">c</span><span class=\"p\">]</span>\n<span class=\"err\">â€‹</span>\n                    <span class=\"n\">dW</span><span class=\"p\">[:,:,:,</span><span class=\"n\">c</span><span class=\"p\">]</span> <span class=\"o\">+=</span> <span class=\"n\">a_slice</span> <span class=\"o\">*</span> <span class=\"n\">dZ</span><span class=\"p\">[</span> <span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">h</span><span class=\"p\">,</span> <span class=\"n\">w</span> <span class=\"p\">,</span><span class=\"n\">c</span><span class=\"p\">]</span>\n                    <span class=\"n\">db</span><span class=\"p\">[:,:,:,</span><span class=\"n\">c</span><span class=\"p\">]</span> <span class=\"o\">+=</span> <span class=\"n\">dZ</span><span class=\"p\">[</span> <span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">h</span><span class=\"p\">,</span> <span class=\"n\">w</span> <span class=\"p\">,</span><span class=\"n\">c</span><span class=\"p\">]</span>\n                    \n        <span class=\"c1\"># Set the ith training example&#39;s dA_prev to the unpaded da_prev_pad (Hint: use X[pad:-pad, pad:-pad, :])</span>\n        <span class=\"n\">dA_prev</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"p\">:,</span> <span class=\"p\">:,</span> <span class=\"p\">:]</span> <span class=\"o\">=</span> <span class=\"n\">da_prev_pad</span><span class=\"p\">[</span><span class=\"n\">pad</span><span class=\"p\">:</span><span class=\"o\">-</span><span class=\"n\">pad</span><span class=\"p\">,</span> <span class=\"n\">pad</span><span class=\"p\">:</span><span class=\"o\">-</span><span class=\"n\">pad</span><span class=\"p\">,</span> <span class=\"p\">:]</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"c1\"># Making sure your output shape is correct</span>\n    <span class=\"k\">assert</span><span class=\"p\">(</span><span class=\"n\">dA_prev</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"p\">(</span><span class=\"n\">m</span><span class=\"p\">,</span> <span class=\"n\">n_H_prev</span><span class=\"p\">,</span> <span class=\"n\">n_W_prev</span><span class=\"p\">,</span> <span class=\"n\">n_C_prev</span><span class=\"p\">))</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">dA_prev</span><span class=\"p\">,</span> <span class=\"n\">dW</span><span class=\"p\">,</span> <span class=\"n\">db</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>Pooling layer - backward pass</b></h2><p>è¿™é‡Œmax poolingå’Œaverage poollingè¦åˆ†å¼€å¤„ç†ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>1. Max pooling - backward pass</b></p><p>å‡è®¾pool sizeæ˜¯$2 \\times 2$çš„ï¼Œé‚£ä¹ˆï¼Œ4ä¸ªåƒç´ ä¸­åªæœ‰1ä¸ªç•™ä¸‹æ¥äº†ï¼Œå…¶ä½™çš„éƒ½æ²¡æœ‰æ•ˆæœäº†ï¼Œæ‰€ä»¥åœ¨max poolingä¸­ï¼Œä»åé¢ä¼ é€’è¿‡æ¥çš„å¯¼æ•°å€¼ï¼Œ<b>åªä½œç”¨åœ¨maxçš„é‚£ä¸ªå…ƒç´ ï¼Œè€Œä¸”ç»§ç»­å¾€å‰ä¼ é€’ï¼Œä¸åšä»»ä½•æ”¹åŠ¨ï¼Œåœ¨å…¶ä½™3ä¸ªå…ƒç´ çš„å¯¼æ•°éƒ½æ˜¯0</b>ã€‚</p><p>åˆ›å»ºä¸€ä¸ªmaskçŸ©é˜µï¼Œè®©æœ€å¤§å€¼ä¸º1ï¼Œå…¶ä½™çš„éƒ½ä¸º0ï¼Œè¿™æ ·å­å°±å¯ä»¥ä½œä¸ºä¸€ä¸ªæ˜ å°„çŸ©é˜µå‘å‰æ˜ å°„äº†ã€‚</p><p><img src=\"https://www.zhihu.com/equation?tex=X+%3D+%5Cbegin%7Bbmatrix%7D1+%26%26+3++%5C%5C+4+%26%26+2+%5Cend%7Bbmatrix%7D+%5Cquad+%5Crightarrow++%5Cquad+M+%3D%5Cbegin%7Bbmatrix%7D+0+%26%26+0+%5C%5C+1+%26%26+0+%5Cend%7Bbmatrix%7D+\" alt=\"X = \\begin{bmatrix}1 &amp;&amp; 3  \\\\ 4 &amp;&amp; 2 \\end{bmatrix} \\quad \\rightarrow  \\quad M =\\begin{bmatrix} 0 &amp;&amp; 0 \\\\ 1 &amp;&amp; 0 \\end{bmatrix} \" eeimg=\"1\"/> </p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">create_mask_from_window</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Creates a mask from an input matrix x, to identify the max entry of x.\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    x -- Array of shape (f, f)\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    mask -- Array of the same shape as window, contains a True at the position corresponding to the max entry of x.\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"c1\">### START CODE HERE ### (â‰ˆ1 line)</span>\n    <span class=\"n\">mask</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">x</span> <span class=\"o\">==</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">max</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">))</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">mask</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><b>2.  Average pooling - backward pass</b></p><p>å’Œmaxä¸åŒï¼Œaverage poolingç›¸å½“äºæŠŠbackwardä¼ è¿‡æ¥çš„å€¼åˆ†æˆäº†$n_H \\times n_W$ç­‰åˆ†ã€‚æ‰€ä»¥è¦è®¡ç®—çš„å‚æ•°å°±æ¯”max poolingå¤šå¾ˆå¤šäº†ï¼Œè¿™ä¹Ÿå°±æ˜¯ä¸ºä»€ä¹ˆä¸€èˆ¬éƒ½ç”¨max poolingï¼Œä¸ç”¨average pooling</p><p><img src=\"https://www.zhihu.com/equation?tex=dZ+%3D+1+%5Cquad+%5Crightarrow++%5Cquad+dZ+%3D%5Cbegin%7Bbmatrix%7D+1%2F4+%26%26+1%2F4+%5C%5C+1%2F4+%26%26+1%2F4+%5Cend%7Bbmatrix%7D\" alt=\"dZ = 1 \\quad \\rightarrow  \\quad dZ =\\begin{bmatrix} 1/4 &amp;&amp; 1/4 \\\\ 1/4 &amp;&amp; 1/4 \\end{bmatrix}\" eeimg=\"1\"/> </p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">distribute_value</span><span class=\"p\">(</span><span class=\"n\">dz</span><span class=\"p\">,</span> <span class=\"n\">shape</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Distributes the input value in the matrix of dimension shape\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    dz -- input scalar\n</span><span class=\"s2\">    shape -- the shape (n_H, n_W) of the output matrix for which we want to distribute the value of dz\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    a -- Array of size (n_H, n_W) for which we distributed the value of dz\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"c1\">### START CODE HERE ###</span>\n    <span class=\"c1\"># Retrieve dimensions from shape (â‰ˆ1 line)</span>\n    <span class=\"p\">(</span><span class=\"n\">n_H</span><span class=\"p\">,</span> <span class=\"n\">n_W</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">shape</span>\n    \n    <span class=\"c1\"># Compute the value to distribute on the matrix (â‰ˆ1 line)</span>\n    <span class=\"n\">average</span> <span class=\"o\">=</span> <span class=\"n\">n_H</span> <span class=\"o\">*</span> <span class=\"n\">n_W</span>\n    \n    <span class=\"c1\"># Create a matrix where every entry is the &#34;average&#34; value (â‰ˆ1 line)</span>\n    <span class=\"n\">a</span> <span class=\"o\">=</span> <span class=\"n\">dz</span> <span class=\"o\">/</span> <span class=\"n\">average</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">ones</span><span class=\"p\">((</span><span class=\"n\">n_H</span><span class=\"p\">,</span> <span class=\"n\">n_W</span><span class=\"p\">))</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">a</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>ç»“åˆä¸¤ç§æ–¹æ³•ï¼š</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">pool_backward</span><span class=\"p\">(</span><span class=\"n\">dA</span><span class=\"p\">,</span> <span class=\"n\">cache</span><span class=\"p\">,</span> <span class=\"n\">mode</span> <span class=\"o\">=</span> <span class=\"s2\">&#34;max&#34;</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Implements the backward pass of the pooling layer\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    dA -- gradient of cost with respect to the output of the pooling layer, same shape as A\n</span><span class=\"s2\">    cache -- cache output from the forward pass of the pooling layer, contains the layer&#39;s input and hparameters \n</span><span class=\"s2\">    mode -- the pooling mode you would like to use, defined as a string (&#34;max&#34; or &#34;average&#34;)\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    dA_prev -- gradient of cost with respect to the input of the pooling layer, same shape as A_prev\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"c1\">### START CODE HERE ###</span>\n    \n    <span class=\"c1\"># Retrieve information from cache (â‰ˆ1 line)</span>\n    <span class=\"p\">(</span><span class=\"n\">A_prev</span><span class=\"p\">,</span> <span class=\"n\">hparameters</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">cache</span>\n    \n    <span class=\"c1\"># Retrieve hyperparameters from &#34;hparameters&#34; (â‰ˆ2 lines)</span>\n    <span class=\"n\">stride</span> <span class=\"o\">=</span> <span class=\"n\">hparameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;stride&#39;</span><span class=\"p\">]</span>\n    <span class=\"n\">f</span> <span class=\"o\">=</span> <span class=\"n\">hparameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;f&#39;</span><span class=\"p\">]</span>\n    \n    <span class=\"c1\"># Retrieve dimensions from A_prev&#39;s shape and dA&#39;s shape (â‰ˆ2 lines)</span>\n    <span class=\"n\">m</span><span class=\"p\">,</span> <span class=\"n\">n_H_prev</span><span class=\"p\">,</span> <span class=\"n\">n_W_prev</span><span class=\"p\">,</span> <span class=\"n\">n_C_prev</span> <span class=\"o\">=</span> <span class=\"n\">A_prev</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n    <span class=\"n\">m</span><span class=\"p\">,</span> <span class=\"n\">n_H</span><span class=\"p\">,</span> <span class=\"n\">n_W</span><span class=\"p\">,</span> <span class=\"n\">n_C</span> <span class=\"o\">=</span> <span class=\"n\">dA</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n    \n    <span class=\"c1\"># Initialize dA_prev with zeros (â‰ˆ1 line)</span>\n    <span class=\"n\">dA_prev</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"n\">A_prev</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span>\n    \n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">m</span><span class=\"p\">):</span>                       <span class=\"c1\"># loop over the training examples</span>\n        \n        <span class=\"c1\"># select training example from A_prev (â‰ˆ1 line)</span>\n        <span class=\"n\">a_prev</span> <span class=\"o\">=</span> <span class=\"n\">A_prev</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span>\n        \n        <span class=\"k\">for</span> <span class=\"n\">h</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">n_H</span><span class=\"p\">):</span>                   <span class=\"c1\"># loop on the vertical axis</span>\n            <span class=\"k\">for</span> <span class=\"n\">w</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">n_W</span><span class=\"p\">):</span>               <span class=\"c1\"># loop on the horizontal axis</span>\n                <span class=\"k\">for</span> <span class=\"n\">c</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">n_C</span><span class=\"p\">):</span>           <span class=\"c1\"># loop over the channels (depth)</span>\n                    \n                    <span class=\"c1\"># Find the corners of the current &#34;slice&#34; (â‰ˆ4 lines)</span>\n                    <span class=\"n\">vert_start</span> <span class=\"o\">=</span> <span class=\"n\">h</span> <span class=\"o\">*</span> <span class=\"n\">stride</span>\n                    <span class=\"n\">vert_end</span> <span class=\"o\">=</span> <span class=\"n\">vert_start</span> <span class=\"o\">+</span> <span class=\"n\">f</span>\n                    <span class=\"n\">horiz_start</span> <span class=\"o\">=</span> <span class=\"n\">w</span> <span class=\"o\">*</span> <span class=\"n\">stride</span>\n                    <span class=\"n\">horiz_end</span> <span class=\"o\">=</span> <span class=\"n\">horiz_start</span> <span class=\"o\">+</span> <span class=\"n\">f</span>\n                    \n                    <span class=\"c1\"># Compute the backward propagation in both modes.</span>\n                    <span class=\"k\">if</span> <span class=\"n\">mode</span> <span class=\"o\">==</span> <span class=\"s2\">&#34;max&#34;</span><span class=\"p\">:</span>\n                        \n                        <span class=\"c1\"># Use the corners and &#34;c&#34; to define the current slice from a_prev (â‰ˆ1 line)</span>\n                        <span class=\"n\">a_prev_slice</span> <span class=\"o\">=</span> <span class=\"n\">a_prev</span><span class=\"p\">[</span><span class=\"n\">vert_start</span> <span class=\"p\">:</span> <span class=\"n\">vert_end</span><span class=\"p\">,</span> <span class=\"n\">horiz_start</span> <span class=\"p\">:</span> <span class=\"n\">horiz_end</span><span class=\"p\">,</span> <span class=\"n\">c</span><span class=\"p\">]</span>\n                        <span class=\"c1\"># Create the mask from a_prev_slice (â‰ˆ1 line)</span>\n                        <span class=\"n\">mask</span> <span class=\"o\">=</span> <span class=\"n\">create_mask_from_window</span><span class=\"p\">(</span><span class=\"n\">a_prev_slice</span><span class=\"p\">)</span>\n                        <span class=\"c1\"># Set dA_prev to be dA_prev + (the mask multiplied by the correct entry of dA) (â‰ˆ1 line)</span>\n                        <span class=\"n\">dA_prev</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">vert_start</span><span class=\"p\">:</span> <span class=\"n\">vert_end</span><span class=\"p\">,</span> <span class=\"n\">horiz_start</span><span class=\"p\">:</span> <span class=\"n\">horiz_end</span><span class=\"p\">,</span> <span class=\"n\">c</span><span class=\"p\">]</span> <span class=\"o\">+=</span> <span class=\"n\">mask</span> <span class=\"o\">*</span> <span class=\"n\">dA</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">h</span><span class=\"p\">,</span> <span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">c</span><span class=\"p\">]</span>\n                        \n                    <span class=\"k\">elif</span> <span class=\"n\">mode</span> <span class=\"o\">==</span> <span class=\"s2\">&#34;average&#34;</span><span class=\"p\">:</span>\n                        \n                        <span class=\"c1\"># Get the value a from dA (â‰ˆ1 line)</span>\n                        <span class=\"n\">da</span> <span class=\"o\">=</span> <span class=\"n\">dA</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">h</span><span class=\"p\">,</span> <span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">c</span><span class=\"p\">]</span>\n                        <span class=\"c1\"># Define the shape of the filter as fxf (â‰ˆ1 line)</span>\n                        <span class=\"n\">shape</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">,</span> <span class=\"n\">f</span><span class=\"p\">)</span>\n                        <span class=\"c1\"># Distribute it to get the correct slice of dA_prev. i.e. Add the distributed value of da. (â‰ˆ1 line)</span>\n                        <span class=\"n\">dA_prev</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">vert_start</span><span class=\"p\">:</span> <span class=\"n\">vert_end</span><span class=\"p\">,</span> <span class=\"n\">horiz_start</span><span class=\"p\">:</span> <span class=\"n\">horiz_end</span><span class=\"p\">,</span> <span class=\"n\">c</span><span class=\"p\">]</span> <span class=\"o\">+=</span> <span class=\"n\">distribute_value</span><span class=\"p\">(</span><span class=\"n\">da</span><span class=\"p\">,</span> <span class=\"n\">shape</span><span class=\"p\">)</span>\n                        \n    <span class=\"c1\">### END CODE ###</span>\n    \n    <span class=\"c1\"># Making sure your output shape is correct</span>\n    <span class=\"k\">assert</span><span class=\"p\">(</span><span class=\"n\">dA_prev</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"n\">A_prev</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">dA_prev</span></code></pre></div><p></p><p></p><p></p>", 
            "topic": [
                {
                    "tag": "ç¥ç»ç½‘ç»œ", 
                    "tagLink": "https://api.zhihu.com/topics/19607065"
                }, 
                {
                    "tag": "å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰", 
                    "tagLink": "https://api.zhihu.com/topics/20043586"
                }, 
                {
                    "tag": "æ·±åº¦å­¦ä¹ ï¼ˆDeep Learningï¼‰", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/45752410", 
            "userName": "ç›´ä¸Šäº‘éœ„", 
            "userLink": "https://www.zhihu.com/people/1033165ce4ad9c3fce69a0793dfab8ad", 
            "upvote": 0, 
            "title": "DeepLearning.aiç¬”è®°:(4-1)-å·ç§¯ç¥ç»ç½‘ç»œï¼ˆFoundations of CNNï¼‰", 
            "content": "<div class=\"highlight\"><pre><code class=\"language-text\">title: &#39;DeepLearning.aiç¬”è®°:(4-1)-- å·ç§¯ç¥ç»ç½‘ç»œï¼ˆFoundations of CNNï¼‰&#39;\nid: dl-ai-4-1\ntags:\n  - dl.ai\ncategories:\n  - AI\n  - Deep Learning\ndate: 2018-09-30 10:20:54\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><b>é¦–å‘äºä¸ªäººåšå®¢: </b></p><a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Fangzhçš„ä¸ªäººåšå®¢ | äººå·¥æ™ºèƒ½æ‹¯æ•‘ä¸–ç•Œ</a><p><b> æ¬¢è¿æ¥è®¿</b></p><p>ç¬¬å››é—¨è¯¾å¼€å§‹å°±å­¦ä¹ æ·±åº¦å­¦ä¹ å…³äºè®¡ç®—æœºè§†è§‰çš„é‡è¦åº”ç”¨---å·ç§¯ç¥ç»ç½‘ç»œã€‚</p><p>ç¬¬ä¸€å‘¨ä¸»è¦æ˜¯å¯¹å·ç§¯ç¥ç»ç½‘ç»œçš„åŸºæœ¬æ„é€ å’ŒåŸç†åšäº†ä»‹ç»ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>è®¡ç®—æœºè§†è§‰</b></h2><p>è®¡ç®—æœºè§†è§‰æ˜¯æ·±åº¦å­¦ä¹ çš„ä¸€ä¸ªéå¸¸é‡è¦çš„åº”ç”¨ã€‚æ¯”å¦‚å›¾åƒåˆ†ç±»ï¼Œç›®æ ‡æ£€æµ‹ï¼Œå›¾ç‰‡é£æ ¼è¿ç§»ç­‰ã€‚</p><p>ç”¨ä¼ ç»Ÿçš„æ·±åº¦å­¦ä¹ ç®—æ³•ï¼Œå‡è®¾ä½ æœ‰ä¸€å¼  <img src=\"https://www.zhihu.com/equation?tex=64%C3%9764\" alt=\"64Ã—64\" eeimg=\"1\"/> çš„çŒ«ç‰‡ï¼Œåˆæœ‰RGBä¸‰é€šé“ï¼Œé‚£ä¹ˆè¿™ä¸ªæ—¶å€™æ˜¯<img src=\"https://www.zhihu.com/equation?tex=64%C3%9764%C3%973%3D12288\" alt=\"64Ã—64Ã—3=12288\" eeimg=\"1\"/> ï¼Œinput layerçš„ç»´åº¦å°±æ˜¯12288ï¼Œè¿™æ ·å…¶å®ä¹Ÿè¿˜å¯ä»¥ï¼Œå› ä¸ºå›¾ç‰‡å¾ˆå°ã€‚é‚£ä¹ˆå¦‚æœä½ æœ‰1000Ã—1000çš„ç…§ç‰‡å‘¢ï¼Œä½ çš„å‘é‡å°±ä¼šæœ‰300ä¸‡ï¼å‡è®¾æœ‰1000ä¸ªéšè—ç¥ç»å…ƒï¼Œé‚£ä¹ˆå°±æ˜¯ç¬¬ä¸€å±‚çš„å‚æ•°çŸ©é˜µWæœ‰30äº¿ä¸ªå‚æ•°ï¼ç®—åˆ°åœ°è€å¤©è’ã€‚æ‰€ä»¥ç”¨ä¼ ç»Ÿçš„æ·±åº¦å­¦ä¹ ç®—æ³•æ˜¯ä¸ç°å®çš„ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>è¾¹ç¼˜æ£€æµ‹</b></h2><p>å¦‚å›¾ï¼Œè¿™äº›è¾¹ç¼˜æ£€æµ‹ä¸­ï¼Œç”¨æ°´å¹³æ£€æµ‹å’Œå‚ç›´æ£€æµ‹ä¼šå¾—åˆ°ä¸åŒçš„ç»“æœã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-1f56c747f128bf20150265cfa818269b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"628\" data-rawheight=\"289\" class=\"origin_image zh-lightbox-thumb\" width=\"628\" data-original=\"https://pic4.zhimg.com/v2-1f56c747f128bf20150265cfa818269b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;628&#39; height=&#39;289&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"628\" data-rawheight=\"289\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"628\" data-original=\"https://pic4.zhimg.com/v2-1f56c747f128bf20150265cfa818269b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-1f56c747f128bf20150265cfa818269b_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>å‚ç›´æ£€æµ‹å¦‚ä¸‹å›¾ï¼Œç”¨ä¸€ä¸ª3Ã—3çš„è¿‡æ»¤å™¨ï¼ˆfilterï¼‰ï¼Œä¹Ÿå«å·ç§¯æ ¸ï¼Œåœ¨åŸå›¾ç‰‡6Ã—6çš„å¯¹åº”åœ°æ–¹æŒ‰å…ƒç´ ç›¸ä¹˜ï¼Œå¾—åˆ°4Ã—4çš„å›¾ç‰‡ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-05e528df409e83049377db340ec03a3f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1192\" data-rawheight=\"669\" class=\"origin_image zh-lightbox-thumb\" width=\"1192\" data-original=\"https://pic4.zhimg.com/v2-05e528df409e83049377db340ec03a3f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1192&#39; height=&#39;669&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1192\" data-rawheight=\"669\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1192\" data-original=\"https://pic4.zhimg.com/v2-05e528df409e83049377db340ec03a3f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-05e528df409e83049377db340ec03a3f_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>å¯ä»¥çœ‹åˆ°ï¼Œç”¨å‚ç›´è¾¹ç¼˜çš„filterå¯ä»¥å°†åŸå›¾ç‰‡ä¸­é—´çš„è¾¹ç¼˜åŒºåˆ†å‡ºæ¥ï¼Œä¹Ÿå°±æ˜¯å¾—åˆ°äº†æœ€å³å›¾ä¸­æœ€äº®çš„éƒ¨åˆ†å³ä¸ºæ£€æµ‹åˆ°çš„è¾¹ç¼˜ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><p>å½“ç„¶ï¼Œå¦‚æœå·¦å›¾çš„äº®æš—åˆ†ç•Œçº¿åè¿‡æ¥ï¼Œåˆ™è¾“å‡ºå›¾ç‰‡ä¸­æœ€æš—çš„éƒ¨åˆ†è¡¨ç¤ºè¾¹ç¼˜ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-85f83b1777b4f7e015c2ff92ffb312ae_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1540\" data-rawheight=\"866\" class=\"origin_image zh-lightbox-thumb\" width=\"1540\" data-original=\"https://pic3.zhimg.com/v2-85f83b1777b4f7e015c2ff92ffb312ae_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1540&#39; height=&#39;866&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1540\" data-rawheight=\"866\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1540\" data-original=\"https://pic3.zhimg.com/v2-85f83b1777b4f7e015c2ff92ffb312ae_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-85f83b1777b4f7e015c2ff92ffb312ae_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>ä¹Ÿè‡ªç„¶æœ‰æ°´å¹³çš„è¾¹ç¼˜åˆ†ç±»å™¨ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-c2a9609fbc25131f1fa6632e905dc3ea_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1049\" data-rawheight=\"334\" class=\"origin_image zh-lightbox-thumb\" width=\"1049\" data-original=\"https://pic3.zhimg.com/v2-c2a9609fbc25131f1fa6632e905dc3ea_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1049&#39; height=&#39;334&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1049\" data-rawheight=\"334\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1049\" data-original=\"https://pic3.zhimg.com/v2-c2a9609fbc25131f1fa6632e905dc3ea_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-c2a9609fbc25131f1fa6632e905dc3ea_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>è¿˜æœ‰æ›´å¤æ‚çš„ï¼Œä½†æ˜¯æˆ‘ä»¬ä¸éœ€è¦è¿›è¡Œäººå·¥çš„å†³å®šè¿™äº›filteræ˜¯ä»€ä¹ˆï¼Œå› ä¸ºæˆ‘ä»¬å¯ä»¥é€šè¿‡è®­ç»ƒï¼Œè®©æœºå™¨è‡ªå·±å­¦åˆ°è¿™äº›å‚æ•°ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-d0f41417ff116c9ca1171c4294b195af_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1445\" data-rawheight=\"809\" class=\"origin_image zh-lightbox-thumb\" width=\"1445\" data-original=\"https://pic4.zhimg.com/v2-d0f41417ff116c9ca1171c4294b195af_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1445&#39; height=&#39;809&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1445\" data-rawheight=\"809\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1445\" data-original=\"https://pic4.zhimg.com/v2-d0f41417ff116c9ca1171c4294b195af_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-d0f41417ff116c9ca1171c4294b195af_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>padding</b></h2><p>paddingæ˜¯å¡«å……çš„æ„æ€ã€‚</p><ul><li>æˆ‘ä»¬å¯ä»¥ä»ä¹‹å‰çš„ä¾‹å­çœ‹åˆ°ï¼Œæ¯ç»è¿‡ä¸€æ¬¡å·ç§¯è¿ç®—ï¼Œå›¾ç‰‡çš„åƒç´ éƒ½ä¼šå˜å°ï¼Œä» <img src=\"https://www.zhihu.com/equation?tex=6%C3%976+---%3E+4%C3%974\" alt=\"6Ã—6 ---&gt; 4Ã—4\" eeimg=\"1\"/> ï¼Œè¿™æ ·å­å›¾ç‰‡å°±ä¼šè¶Šæ¥è¶Šå°ï¼Œåé¢å°±æ¯›éƒ½ä¸å‰©äº†ã€‚</li><li>è¿˜æœ‰ä¸€ç‚¹å°±æ˜¯ï¼Œä»å·ç§¯çš„è¿ç®—æ–¹æ³•æ¥çœ‹ï¼Œè¾¹ç¼˜å’Œè§’è½çš„ä½ç½®å·ç§¯çš„æ¬¡æ•°å°‘ï¼Œä¼šä¸¢å¤±æœ‰ç”¨ä¿¡æ¯ã€‚</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p>æ‰€ä»¥å°±æœ‰paddingçš„æƒ³æ³•äº†ï¼Œä¹Ÿå°±æ˜¯åœ¨å›¾ç‰‡å››å‘¨å¡«è¡¥ä¸Šåƒç´ ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3fa96325eb0edc023deff99f438e9edd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"439\" data-rawheight=\"391\" class=\"origin_image zh-lightbox-thumb\" width=\"439\" data-original=\"https://pic2.zhimg.com/v2-3fa96325eb0edc023deff99f438e9edd_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;439&#39; height=&#39;391&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"439\" data-rawheight=\"391\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"439\" data-original=\"https://pic2.zhimg.com/v2-3fa96325eb0edc023deff99f438e9edd_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-3fa96325eb0edc023deff99f438e9edd_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>è®¡ç®—æ–¹æ³•å¦‚ä¸‹ï¼Œ</p><p>åŸæ•°æ®æ˜¯ <img src=\"https://www.zhihu.com/equation?tex=n+%5Ctimes+n\" alt=\"n \\times n\" eeimg=\"1\"/> ï¼Œfilterä¸º <img src=\"https://www.zhihu.com/equation?tex=f+%5Ctimes+f\" alt=\"f \\times f\" eeimg=\"1\"/> ,paddingä¸º <img src=\"https://www.zhihu.com/equation?tex=p+%5Ctimes+p\" alt=\"p \\times p\" eeimg=\"1\"/> ï¼Œ</p><p>é‚£ä¹ˆå¾—åˆ°çš„çŸ©é˜µå¤§å°æ˜¯ <img src=\"https://www.zhihu.com/equation?tex=%28n+%2B+2p+-f+%2B1%29%5Ctimes%28n+%2B+2p+-f+%2B1%29\" alt=\"(n + 2p -f +1)\\times(n + 2p -f +1)\" eeimg=\"1\"/> </p><p class=\"ztext-empty-paragraph\"><br/></p><p>paddingæœ‰ä¸¤ç§ï¼š</p><ul><li>validï¼šä¹Ÿå°±æ˜¯ä¸å¡«å……</li><li>sameï¼šè¾“å…¥ä¸è¾“å‡ºå¤§å°ç›¸åŒçš„å›¾ç‰‡, <img src=\"https://www.zhihu.com/equation?tex=p%3D%28f+-+1%29+%2F+2\" alt=\"p=(f - 1) / 2\" eeimg=\"1\"/> ï¼Œä¸€èˆ¬paddingä¸ºå¥‡æ•°ï¼Œå› ä¸ºfilteræ˜¯å¥‡æ•°</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>strideï¼ˆæ­¥é•¿ï¼‰</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p>å·ç§¯çš„æ­¥é•¿ä¹Ÿå°±æ˜¯æ¯ä¸€æ¬¡è¿ç®—åå¹³ç§»çš„è·ç¦»ï¼Œä¹‹å‰ä½¿ç”¨éƒ½æ˜¯stride=1ã€‚</p><p>å‡è®¾stride=2ï¼Œå°±ä¼šå¾—åˆ°ï¼š</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-931dcc80ec6b4c01795326386b4c3144_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1194\" data-rawheight=\"663\" class=\"origin_image zh-lightbox-thumb\" width=\"1194\" data-original=\"https://pic1.zhimg.com/v2-931dcc80ec6b4c01795326386b4c3144_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1194&#39; height=&#39;663&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1194\" data-rawheight=\"663\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1194\" data-original=\"https://pic1.zhimg.com/v2-931dcc80ec6b4c01795326386b4c3144_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-931dcc80ec6b4c01795326386b4c3144_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>å¾—åˆ°çš„çŸ©é˜µå¤§å°æ˜¯</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Clfloor+%5Cfrac%7Bn%2B2p-f%7D%7Bs%7D%2B1%5Crfloor+%5Ctimes+%5Clfloor+%5Cfrac%7Bn%2B2p-f%7D%7Bs%7D%2B1%5Crfloor\" alt=\"\\lfloor \\frac{n+2p-f}{s}+1\\rfloor \\times \\lfloor \\frac{n+2p-f}{s}+1\\rfloor\" eeimg=\"1\"/> </p><p>å‘ä¸‹å–æ•´: 59/60 = 0</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>ç«‹ä½“å·ç§¯</b></h2><p>ä¹‹å‰éƒ½æ˜¯å•é€šé“çš„å›¾ç‰‡è¿›è¡Œå·ç§¯ï¼Œå¦‚æœæœ‰RGBä¸‰ç§é¢œè‰²çš„è¯ï¼Œå°±è¦ä½¿ç”¨ç«‹ä½“å·ç§¯äº†ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-03c2e414ecd8d927befc461f9b981858_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1352\" data-rawheight=\"785\" class=\"origin_image zh-lightbox-thumb\" width=\"1352\" data-original=\"https://pic1.zhimg.com/v2-03c2e414ecd8d927befc461f9b981858_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1352&#39; height=&#39;785&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1352\" data-rawheight=\"785\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1352\" data-original=\"https://pic1.zhimg.com/v2-03c2e414ecd8d927befc461f9b981858_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-03c2e414ecd8d927befc461f9b981858_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>è¿™ä¸ªæ—¶å€™çš„å·ç§¯æ ¸å°±å˜æˆäº† <img src=\"https://www.zhihu.com/equation?tex=3+%5Ctimes+3+%5Ctimes+3\" alt=\"3 \\times 3 \\times 3\" eeimg=\"1\"/> çš„ä¸‰ç»´å·ç§¯æ ¸ï¼Œä¸€å…±27ä¸ªå‚æ•°ï¼Œæ¯æ¬¡å¯¹åº”ç€åŸå›¾ç‰‡ä¸Šçš„RGBä¸€å…±27ä¸ªåƒç´ è¿ç®—ï¼Œç„¶åæ±‚å’Œå¾—åˆ°è¾“å‡ºå›¾ç‰‡çš„ä¸€ä¸ªåƒç´ ã€‚å› ä¸ºåªæœ‰ä¸€ä¸ªå·ç§¯æ ¸ï¼Œè¿™ä¸ªæ—¶å€™è¾“å‡ºçš„è¿˜æ˜¯ <img src=\"https://www.zhihu.com/equation?tex=4+%5Ctimes+4+%5Ctimes+1\" alt=\"4 \\times 4 \\times 1\" eeimg=\"1\"/> çš„å›¾ç‰‡ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>å¤šä¸ªå·ç§¯æ ¸</b></p><p>å› ä¸ºä¸åŒçš„å·ç§¯æ ¸å¯ä»¥æå–ä¸åŒçš„å›¾ç‰‡ç‰¹å¾ï¼Œæ‰€ä»¥å¯ä»¥æœ‰å¾ˆå¤šä¸ªå·ç§¯æ ¸ï¼ŒåŒæ—¶æå–å›¾ç‰‡çš„ç‰¹å¾ï¼Œå¦‚åˆ†åˆ«æå–å›¾ç‰‡çš„æ°´å¹³å’Œå‚ç›´è¾¹ç¼˜ç‰¹å¾ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-319b1bc448a41fa25cb6dac56aaccf91_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1433\" data-rawheight=\"798\" class=\"origin_image zh-lightbox-thumb\" width=\"1433\" data-original=\"https://pic2.zhimg.com/v2-319b1bc448a41fa25cb6dac56aaccf91_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1433&#39; height=&#39;798&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1433\" data-rawheight=\"798\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1433\" data-original=\"https://pic2.zhimg.com/v2-319b1bc448a41fa25cb6dac56aaccf91_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-319b1bc448a41fa25cb6dac56aaccf91_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>å› ä¸ºæœ‰äº†ä¸¤ä¸ªå·ç§¯æ ¸ï¼Œè¿™æ—¶å€™è¾“å‡ºçš„å›¾ç‰‡å°±æ˜¯æœ‰ä¸¤é€šé“çš„å›¾ç‰‡ <img src=\"https://www.zhihu.com/equation?tex=4%5Ctimes+4+%5Ctimes+2\" alt=\"4\\times 4 \\times 2\" eeimg=\"1\"/> ã€‚</p><p>è¿™é‡Œè¦ææ¸…ä¸¤ä¸ªæ¦‚å¿µï¼Œå·ç§¯æ ¸çš„é€šé“æ•°å’Œä¸ªæ•°ï¼š</p><ul><li>é€šé“æ•°channelï¼šå³å·ç§¯æ ¸è¦ä½œç”¨åœ¨åŸå›¾ç‰‡ä¸Šï¼ŒåŸå›¾ç‰‡çš„é€šé“å¤„ <img src=\"https://www.zhihu.com/equation?tex=n_c\" alt=\"n_c\" eeimg=\"1\"/> ï¼Œå·ç§¯æ ¸çš„é€šé“æ•°å¿…é¡»å’ŒåŸå›¾ç‰‡é€šé“æ•°ç›¸åŒ</li><li>ä¸ªæ•°ï¼šå³è¦ä½¿ç”¨å¤šå°‘ä¸ªè¿™æ ·çš„å·ç§¯æ ¸ï¼Œä½¿ç”¨ <img src=\"https://www.zhihu.com/equation?tex=n_%7Bc%7D%5E%7B%5Cprime%7D\" alt=\"n_{c}^{\\prime}\" eeimg=\"1\"/> è¡¨ç¤ºï¼Œå·ç§¯æ ¸çš„ä¸ªæ•°ä¹Ÿå°±æ˜¯è¾“å‡ºå›¾ç‰‡çš„é€šé“æ•°ï¼Œå¦‚æœ‰ä¸¤ä¸ªå·ç§¯æ ¸ï¼Œé‚£ä¹ˆç”Ÿæˆäº† <img src=\"https://www.zhihu.com/equation?tex=4%5Ctimes+4+%5Ctimes+2\" alt=\"4\\times 4 \\times 2\" eeimg=\"1\"/> çš„å›¾ç‰‡ï¼Œ2  å°±æ˜¯å·ç§¯æ ¸çš„ä¸ªæ•°</li><li>å³ <img src=\"https://www.zhihu.com/equation?tex=n+%5Ctimes+n+%5Ctimes+n_c\" alt=\"n \\times n \\times n_c\" eeimg=\"1\"/> ï¼Œä¹˜ä¸Šçš„ <img src=\"https://www.zhihu.com/equation?tex=n_%7Bc%7D%5E%7B%5Cprime%7D\" alt=\"n_{c}^{\\prime}\" eeimg=\"1\"/> ä¸ªå·ç§¯æ ¸ <img src=\"https://www.zhihu.com/equation?tex=+f+%5Ctimes+f+%5Ctimes+n_c\" alt=\" f \\times f \\times n_c\" eeimg=\"1\"/> ï¼Œå¾—åˆ° <img src=\"https://www.zhihu.com/equation?tex=%28n+-f+%2B1%29%5Ctimes+%28n+-+f+%2B1+%29+%5Ctimes+n_%7Bc%7D%5E%7B%5Cprime%7D\" alt=\"(n -f +1)\\times (n - f +1 ) \\times n_{c}^{\\prime}\" eeimg=\"1\"/> çš„æ–°å›¾ç‰‡</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>å·ç§¯ç¥ç»ç½‘ç»œ</b></h2><p><b>å•å±‚å·ç§¯ç½‘ç»œ</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-d7bc50593f9224d28a7c523821220189_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1702\" data-rawheight=\"949\" class=\"origin_image zh-lightbox-thumb\" width=\"1702\" data-original=\"https://pic2.zhimg.com/v2-d7bc50593f9224d28a7c523821220189_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1702&#39; height=&#39;949&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1702\" data-rawheight=\"949\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1702\" data-original=\"https://pic2.zhimg.com/v2-d7bc50593f9224d28a7c523821220189_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-d7bc50593f9224d28a7c523821220189_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>å¦‚å›¾æ˜¯å•å±‚å·ç§¯çš„åŸºæœ¬è¿‡ç¨‹ï¼Œå…ˆç»è¿‡ä¸¤ä¸ªå·ç§¯æ ¸ï¼Œç„¶åå†åŠ ä¸Šbiasè¿›è¡Œreluæ¿€æ´»å‡½æ•°ã€‚</p><p>é‚£ä¹ˆå‡è®¾æŸå±‚å·ç§¯å±‚æœ‰10ä¸ª <img src=\"https://www.zhihu.com/equation?tex=3+%5Ctimes+3+%5Ctimes+3\" alt=\"3 \\times 3 \\times 3\" eeimg=\"1\"/> çš„å·ç§¯æ ¸ï¼Œé‚£ä¹ˆä¸€å…±æœ‰ <img src=\"https://www.zhihu.com/equation?tex=%283%5Ctimes3%5Ctimes3%2B1%29+%5Ctimes10%3D280\" alt=\"(3\\times3\\times3+1) \\times10=280\" eeimg=\"1\"/> ä¸ªå‚æ•°ï¼ŒåŠ 1æ˜¯åŠ ä¸Šäº†bias</p><p>åœ¨è¿™é‡Œæ€»ç»“äº†å„ä¸ªå‚æ•°çš„è¡¨ç¤ºæ–¹æ³•ï¼š</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-ad76c7c5adcf9b302070d9a7f105a772_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"797\" data-rawheight=\"429\" class=\"origin_image zh-lightbox-thumb\" width=\"797\" data-original=\"https://pic3.zhimg.com/v2-ad76c7c5adcf9b302070d9a7f105a772_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;797&#39; height=&#39;429&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"797\" data-rawheight=\"429\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"797\" data-original=\"https://pic3.zhimg.com/v2-ad76c7c5adcf9b302070d9a7f105a772_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-ad76c7c5adcf9b302070d9a7f105a772_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>ç®€å•ç¥ç»ç½‘ç»œ</b></p><p>ä¸€èˆ¬å·ç§¯ç¥ç»ç½‘ç»œå±‚çš„ç±»å‹æœ‰ï¼š</p><ul><li>convolutionå·ç§¯å±‚</li><li>poolæ± åŒ–å±‚</li><li>fully connectedå…¨è¿æ¥å±‚</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>æ± åŒ–å±‚</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p>pooling çš„ä½œç”¨å°±æ˜¯ç”¨æ¥å‹ç¼©æ•°æ®ï¼ŒåŠ é€Ÿè¿ç®—ï¼Œæé«˜æå–ç‰¹å¾çš„é²æ£’æ€§</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>Max pooling</b></p><p>åœ¨èŒƒå›´å†…å–æœ€å¤§å€¼</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-f609d24faa430ce85ba8a3dded2c0d33_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"982\" data-rawheight=\"579\" class=\"origin_image zh-lightbox-thumb\" width=\"982\" data-original=\"https://pic4.zhimg.com/v2-f609d24faa430ce85ba8a3dded2c0d33_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;982&#39; height=&#39;579&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"982\" data-rawheight=\"579\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"982\" data-original=\"https://pic4.zhimg.com/v2-f609d24faa430ce85ba8a3dded2c0d33_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-f609d24faa430ce85ba8a3dded2c0d33_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>Average Pooling</b></p><p>å–å¹³å‡å€¼</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-498068d322a83fb8ccb54179e5ee9e45_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"915\" data-rawheight=\"443\" class=\"origin_image zh-lightbox-thumb\" width=\"915\" data-original=\"https://pic2.zhimg.com/v2-498068d322a83fb8ccb54179e5ee9e45_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;915&#39; height=&#39;443&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"915\" data-rawheight=\"443\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"915\" data-original=\"https://pic2.zhimg.com/v2-498068d322a83fb8ccb54179e5ee9e45_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-498068d322a83fb8ccb54179e5ee9e45_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>å·ç§¯ç¥ç»ç½‘ç»œç¤ºä¾‹</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b7351b0a940b4def5d05d0513cdf0645_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1260\" data-rawheight=\"705\" class=\"origin_image zh-lightbox-thumb\" width=\"1260\" data-original=\"https://pic2.zhimg.com/v2-b7351b0a940b4def5d05d0513cdf0645_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1260&#39; height=&#39;705&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1260\" data-rawheight=\"705\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1260\" data-original=\"https://pic2.zhimg.com/v2-b7351b0a940b4def5d05d0513cdf0645_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-b7351b0a940b4def5d05d0513cdf0645_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>ä¸€èˆ¬convåéƒ½ä¼šè¿›è¡Œpoolingï¼Œæ‰€ä»¥å¯ä»¥æŠŠconvå’Œpoolingå½“åšä¸€å±‚ã€‚</p><p>å¦‚ä¸Šå›¾å°±æ˜¯ <img src=\"https://www.zhihu.com/equation?tex=conv-pool-conv-pool-fc-fc-fc-softmax\" alt=\"conv-pool-conv-pool-fc-fc-fc-softmax\" eeimg=\"1\"/> çš„å·ç§¯ç¥ç»ç½‘ç»œç»“æ„ã€‚</p><p>å„ä¸ªå±‚çš„å‚æ•°æ˜¯è¿™æ ·çš„ï¼š</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-54e8db85ed5217e6419214e70a91ff0f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"787\" data-rawheight=\"408\" class=\"origin_image zh-lightbox-thumb\" width=\"787\" data-original=\"https://pic4.zhimg.com/v2-54e8db85ed5217e6419214e70a91ff0f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;787&#39; height=&#39;408&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"787\" data-rawheight=\"408\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"787\" data-original=\"https://pic4.zhimg.com/v2-54e8db85ed5217e6419214e70a91ff0f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-54e8db85ed5217e6419214e70a91ff0f_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>å¯ä»¥çœ‹åˆ°ï¼Œåœ¨å·ç§¯å±‚çš„å‚æ•°éå¸¸å°‘ï¼Œæ± åŒ–å±‚æ²¡æœ‰å‚æ•°ï¼Œå¤§é‡çš„å‚æ•°åœ¨å…¨è¿æ¥å±‚ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>ä¸ºä½•ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼Ÿ</b></h2><p>è¿™é‡Œç»™å‡ºäº†ä¸¤ç‚¹ä¸»è¦åŸå› ï¼š</p><ul><li>å‚æ•°å…±äº«ï¼šå·ç§¯æ ¸çš„å‚æ•°æ˜¯åŸå›¾ç‰‡ä¸­å„ä¸ªåƒç´ ä¹‹é—´å…±äº«çš„ï¼Œæ‰€ä»¥å¤§å¤§å‡å°‘äº†å‚æ•°</li><li>è¿æ¥çš„ç¨€ç–æ€§ï¼šæ¯ä¸ªè¾“å‡ºå€¼ï¼Œå®é™…ä¸Šåªå–å†³äºå¾ˆå°‘é‡çš„è¾“å…¥è€Œå·²ã€‚</li></ul><p></p>", 
            "topic": [
                {
                    "tag": "å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰", 
                    "tagLink": "https://api.zhihu.com/topics/20043586"
                }, 
                {
                    "tag": "äººå·¥æ™ºèƒ½", 
                    "tagLink": "https://api.zhihu.com/topics/19551275"
                }, 
                {
                    "tag": "æœºå™¨å­¦ä¹ ", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/45752363", 
            "userName": "ç›´ä¸Šäº‘éœ„", 
            "userLink": "https://www.zhihu.com/people/1033165ce4ad9c3fce69a0793dfab8ad", 
            "upvote": 0, 
            "title": "DeepLearning.aiç¬”è®°:(3-2)-- æœºå™¨å­¦ä¹ ç­–ç•¥(2)(ML strategy)", 
            "content": "<div class=\"highlight\"><pre><code class=\"language-text\">title: &#39;DeepLearning.aiç¬”è®°:(3-2)-- æœºå™¨å­¦ä¹ ç­–ç•¥(2)(ML strategy)&#39;\nid: 2018092017\ntags:\n  - dl.ai\ncategories:\n  - AI\n  - Deep Learning\ndate: 2018-09-20 17:59:04\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>é¦–å‘äºä¸ªäººåšå®¢: </b></p><b><a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Fangzhçš„ä¸ªäººåšå®¢ | äººå·¥æ™ºèƒ½æ‹¯æ•‘ä¸–ç•Œ</a></b><p><b> æ¬¢è¿æ¥è®¿</b></p><p>è¿™å‘¨ç»§ç»­è®²äº†æœºå™¨å­¦ä¹ ç­–ç•¥,åŒ…æ‹¬è¯¯å·®åˆ†æã€é”™è¯¯æ ·æœ¬æ¸…æ¥šã€æ•°æ®åˆ†å¸ƒä¸åŒã€è¿ç§»å­¦ä¹ ã€å¤šä»»åŠ¡å­¦ä¹ ç­‰ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>è¯¯å·®åˆ†æ</b></h2><p>å¯¹äºè®­ç»ƒåçš„æ¨¡å‹ï¼Œå¦‚æœä¸è¿›è¡Œè¯¯å·®åˆ†æï¼Œé‚£ä¹ˆå¾ˆéš¾æå‡ç²¾åº¦ã€‚æ‰€ä»¥åº”è¯¥åœ¨éªŒè¯é›†ä¸­ï¼Œæ‰¾åˆ°æ ‡è®°é”™è¯¯çš„é‚£äº›æ ·æœ¬ï¼Œç»Ÿè®¡ä¸€ä¸‹éƒ½æ˜¯å› ä¸ºä»€ä¹ˆåŸå› å‡ºç°çš„é”™è¯¯ï¼Œå¦‚æ˜¯ä¸æ˜¯ç…§ç‰‡æ¨¡ç³Šï¼Œè¿˜æ˜¯æœ¬æ¥æ˜¯çŒ«æŠŠå®ƒæ ‡è®°æˆç‹—äº†ç­‰ç­‰ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>æ¸…é™¤é”™è¯¯æ ‡è®°æ ·æœ¬</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p>å¦‚æœæ˜¯éšæœºçš„è¯¯å·®ï¼Œä¹Ÿå°±æ˜¯äººä¸ºæ ‡è®°æ ·æœ¬å‡ºç°äº†éšæœºé”™è¯¯ï¼Œé‚£ä¹ˆæ²¡æœ‰å…³ç³»ï¼Œå› ä¸ºç®—æ³•å¯¹éšå³è¯¯å·®è¿˜æ˜¯å¾ˆæœ‰é²æ£’æ€§çš„ã€‚</p><p>å¦‚æœæ˜¯ç³»ç»Ÿè¯¯å·®ï¼Œé‚£æ²¡åŠæ³•äº†ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><p>æ¯”å¦‚è¯´æ€»ä½“è¯¯å·®æ˜¯10%ï¼Œç„¶åå‘ç°å› ä¸ºäººå·¥é”™è¯¯æ ‡è®°å¼•èµ·çš„è¯¯å·®æ˜¯0.6%ï¼Œé‚£ä¹ˆå…¶ä»–åŸå› é€ æˆçš„è¯¯å·®å°±æ˜¯9.4%ï¼Œè¿™ä¸ªæ—¶å€™åº”è¯¥é›†ä¸­ç²¾åŠ›å»æ‰¾é‚£9.4%çš„è¯¯å·®åŸå› ï¼Œå¹¶è¿›è¡Œä¿®æ­£ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>å¿«é€Ÿæ­å»ºç³»ç»Ÿ</b></h2><p>å¯¹äºä¸€ä¸ªé¡¹ç›®æ¥è¯´ï¼Œæˆ‘ä»¬ä¸€å¼€å§‹ä¸è¦æƒ³å¾—å¤ªå¤æ‚ï¼Œå…ˆå¿«é€Ÿæ­å»ºä¸€ä¸ªåŸºæœ¬çš„ç³»ç»Ÿï¼Œè¿›è¡Œè¿­ä»£ï¼Œç„¶ååœ¨æ…¢æ…¢åˆ†æï¼Œé€æ­¥æé«˜ï¼Œä¸è¦æƒ³ç€ä¸€æ­¥åˆ°ä½ï¼Œè¿™æ ·å­å¾€å¾€ä¼šéš¾ä»¥å…¥æ‰‹ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>ä¸åŒåˆ†å¸ƒçš„è®­ç»ƒå’Œæµ‹è¯•</b></h2><p>å‡è®¾ä½ åœ¨ç½‘ä¸Šæ‰¾åˆ°äº†20ä¸‡å¼ ç…§ç‰‡å»åˆ†æï¼Œä½†æ˜¯æˆ‘ä»¬å®é™…ä¸Šè¦æµ‹è¯•çš„æ˜¯ç”¨æˆ·åœ¨æ‰‹æœºæ‹æ‘„æƒ…å†µä¸‹çš„å‡†ç¡®åº¦ã€‚ä½†æ˜¯é—®é¢˜æ˜¯æ‰‹æœºä¸Šæ‹æ‘„çš„æ•°æ®ä¸è¶³ï¼Œå‡è®¾åªæœ‰1ä¸‡å¼ ã€‚ä¹Ÿå°±æ˜¯è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸æ˜¯åœ¨åŒä¸€åˆ†å¸ƒï¼Œé‚£ä¹ˆæ€ä¹ˆåŠå‘¢ï¼Ÿ</p><p>æ˜¾ç„¶ï¼Œå¦‚æœæŠŠ21ä¸‡å¼ ç…§ç‰‡åŠ åœ¨ä¸€èµ·ï¼Œé‡æ–°åˆ†é…ï¼Œæ˜¯ä¸åˆç†çš„ï¼Œå› ä¸ºè¿™æ ·å­ä½ éªŒè¯é›†å’Œæµ‹è¯•é›†ä¸Šçš„æ•°æ®æ˜¾ç„¶å¾ˆå°‘æ˜¯æ‰‹æœºæ‹æ‘„çš„ã€‚</p><p>æ‰€ä»¥ï¼Œåº”è¯¥ç”¨20ä¸‡å¼ ç…§ç‰‡ï¼Œå†åŠ ä¸Š5000å¼ ç…§ç‰‡ä½œä¸ºè®­ç»ƒé›†ï¼Œç„¶åæŠŠå‰©ä¸‹æ¥çš„5000å¼ ç…§ç‰‡å¯¹åŠåˆ†ä¸ºéªŒè¯é›†å’Œæµ‹è¯•é›†ï¼Œé‚£æ ·å­æ‰æ›´ä¸ºç¬¦åˆå®é™…æƒ…å†µã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>ä¸åŒåˆ†å¸ƒçš„åå·®å’Œæ–¹å·®</b></h2><p>å¦‚ä¸Šè¿°æƒ…å†µï¼Œä½ çš„è®­ç»ƒé›†å’ŒéªŒè¯æµ‹è¯•é›†ä¸åŒä¸€åˆ†å¸ƒçš„ï¼Œå‡è®¾training errorï¼š1%ï¼Œdev errorï¼š10%ï¼Œé‚£ä¹ˆè¿™ä¸ªæ—¶å€™èƒ½è¯´æ˜¯æ–¹å·®å¤ªå¤§å—ï¼Œæ˜¾ç„¶æ˜¯ä¸åˆç†çš„ï¼Œå› ä¸ºä¸æ˜¯åŒä¸€åˆ†å¸ƒçš„ã€‚</p><p>é‚£ä¹ˆè¿™ä¸ªæ—¶å€™åº”è¯¥é‡æ–°å®šä¹‰ä¸€ä¸ªé›†åˆï¼Œå«åšè®­ç»ƒéªŒè¯é›†ï¼štrain-dev</p><p>ä¹Ÿå°±æ˜¯åœ¨è®­ç»ƒé›†ä¸­æ‹¿å‡ºä¸€éƒ¨åˆ†æ•°æ®ï¼Œè·ŸéªŒè¯é›†åˆåœ¨ä¸€èµ·ï¼Œä¸å‚ä¸è®­ç»ƒï¼Œè¿™æ ·æˆ‘ä»¬å°±å¾—åˆ°äº†ï¼štraining errorï¼š1%ï¼Œtraining-dev errorï¼š9%ï¼Œdev errorï¼š10%ï¼Œå¦‚æœæ˜¯è¿™ç§æƒ…å†µï¼Œè¿™æ ·æ‰èƒ½è¯´æ˜¯æ–¹å·®é—®é¢˜ã€‚</p><p>å¦‚æœæ˜¯training errorï¼š1%ï¼Œtraining-dev errorï¼š1.5%ï¼Œdev errorï¼š10%ï¼Œé‚£ä¹ˆï¼Œæ˜¾ç„¶ä¸æ˜¯å› ä¸ºæ–¹å·®é—®é¢˜ï¼Œè€Œæ˜¯å› ä¸ºåˆ†å¸ƒä¸åŒè€Œå¯¼è‡´çš„ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><p>å¦‚ä½•è§£å†³å‘¢ï¼Ÿ</p><ul><li>è¿›è¡Œäººå·¥è¯¯å·®åˆ†æï¼Œçœ‹ä¸€çœ‹è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„å·®åˆ«åˆ°åº•åœ¨å“ªé‡Œï¼Œæ¯”å¦‚æ˜¯ä¸æ˜¯æœ‰å™ªéŸ³ã€ç…§ç‰‡æ¨¡ç³Šç­‰ç­‰</li><li>ç„¶åæŠŠè®­ç»ƒé›†æå¾—æ›´åƒæµ‹è¯•é›†ï¼Œä¹Ÿå°±æ˜¯å¤šæ”¶é›†ç‚¹ç±»ä¼¼äºæµ‹è¯•é›†çš„æ•°æ®ï¼Œæˆ–è€…é€šè¿‡äººå·¥åˆæˆæŠ€æœ¯ï¼ŒæŠŠå™ªå£°åŠ ä¸Šå»ã€‚</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>è¿ç§»å­¦ä¹ </b></h2><p>å¦‚æœæˆ‘ä»¬ç°åœ¨è®­ç»ƒäº†ä¸€ä¸ªçŒ«çš„åˆ†ç±»å™¨ï¼Œç„¶åè¿™ä¸ªæ—¶å€™æœ‰äº†æ–°ä»»åŠ¡ï¼Œè¦è¯†åˆ«çº¢ç»¿ç¯ï¼Œé—®é¢˜æ˜¯ï¼Œæˆ‘ä»¬æ²¡æœ‰é‚£ä¹ˆå¤šçº¢ç»¿ç¯çš„ç…§ç‰‡ï¼Œæ²¡æœ‰é‚£ä¹ˆå¤šçš„æ•°æ®ï¼Œé‚£æ€ä¹ˆåŠï¼Ÿè¿™æ—¶å€™å°±å¯ä»¥æŠŠè¿™ä¸ªçŒ«åˆ†ç±»å™¨å­¦ä¹ çš„å‚æ•°è¿ç§»åˆ°çº¢ç»¿ç¯åˆ†ç±»å™¨ä¸­ï¼Œåªè¦è¾“å‡ºå±‚çš„å¾®è°ƒå°±è¡Œäº†ã€‚å› ä¸ºå›¾åƒè¯†åˆ«çš„ç¥ç»ç½‘ç»œï¼Œåœ¨å‰é¢çš„ç½‘ç»œå¤§å¤šæ˜¯è¿›è¡Œä¸€äº›ç‰¹å¾æå–ï¼Œæ‰€ä»¥å¦‚æœè¿›è¡Œå›¾åƒè¯†åˆ«çš„è¿ç§»ï¼Œè¿˜æ˜¯å¾ˆæœ‰å¸®åŠ©çš„ï¼</p><p class=\"ztext-empty-paragraph\"><br/></p><p>ä½†æ˜¯è¿ç§»å­¦ä¹ æœ‰é™åˆ¶ï¼š</p><ul><li>å¿…é¡»æ˜¯ç›¸å…³çš„ç±»å‹ï¼Œæ¯”å¦‚éƒ½æ˜¯å›¾åƒè¯†åˆ«ï¼Œéƒ½æ˜¯è¯­éŸ³è¯†åˆ«</li><li>Açš„æ•°æ®è¿œå¤§äºBï¼Œå¦‚æœBçš„æ•°æ®å¤Ÿå¤šï¼Œé‚£è‡ªå·±ä»å¤´å¼€å§‹å­¦ä¸å°±å¥½äº†</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>Muti-taskå¤šä»»åŠ¡å­¦ä¹ </b></h2><p>å‡è®¾åœ¨è‡ªåŠ¨é©¾é©¶ä¸­ï¼Œéœ€è¦åŒæ—¶æ£€æµ‹å¾ˆå¤šç‰©ä½“ï¼Œæ¯”å¦‚äººã€çº¢ç»¿ç¯ï¼Œæ±½è½¦ç­‰ç­‰ã€‚</p><p>é‚£ä¹ˆå°±å¯ä»¥æŠŠè¿™äº›éƒ½å†™åˆ°ä¸€ä¸ªå‘é‡ä¸­ï¼š</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-cea1214f03abc3aad62c68d5ec3e3e1b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1350\" data-rawheight=\"754\" class=\"origin_image zh-lightbox-thumb\" width=\"1350\" data-original=\"https://pic4.zhimg.com/v2-cea1214f03abc3aad62c68d5ec3e3e1b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1350&#39; height=&#39;754&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1350\" data-rawheight=\"754\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1350\" data-original=\"https://pic4.zhimg.com/v2-cea1214f03abc3aad62c68d5ec3e3e1b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-cea1214f03abc3aad62c68d5ec3e3e1b_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>å¦‚å›¾ï¼Œ$y = [0 1 1 0]$å³è¡¨ç¤ºåŒæ—¶<b>æœ‰è½¦å’Œåœè½¦æ ‡å¿—</b>ã€‚</p><p>è¿™ä¸ªåˆå’Œsoftmaxä¸åŒï¼Œsoftmaxä¸€æ¬¡åªè¯†åˆ«ä¸€ç§ç‰©ä½“ï¼Œè€Œå¤šä»»åŠ¡å­¦ä¹ ä¸€æ¬¡å¯ä»¥è¯†åˆ«å¤šç§ç‰©ä½“ã€‚</p><p>è¿™ä¸ªæ—¶å€™çš„loss funtion å’Œlogisticæ˜¯ä¸€æ ·çš„ï¼š</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-41024721469c7e4ea0999ec2873f3e25_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"646\" data-rawheight=\"77\" class=\"origin_image zh-lightbox-thumb\" width=\"646\" data-original=\"https://pic2.zhimg.com/v2-41024721469c7e4ea0999ec2873f3e25_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;646&#39; height=&#39;77&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"646\" data-rawheight=\"77\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"646\" data-original=\"https://pic2.zhimg.com/v2-41024721469c7e4ea0999ec2873f3e25_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-41024721469c7e4ea0999ec2873f3e25_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>å¦‚æœåœ¨æ ‡æ³¨æ ·æœ¬ä¸­ï¼Œåªæ ‡æ³¨äº†æ¯å¼ å›¾ç‰‡çš„ä¸€éƒ¨åˆ†ï¼Œæ¯”å¦‚è¯´å›¾ç‰‡ä¸­æœ‰è¡Œäººå’Œè½¦ï¼Œåªæ ‡æ³¨çš„è¡Œäººï¼Œæœ‰æ²¡æœ‰è½¦æ˜¯ä¸çŸ¥é“çš„ï¼Œé‚£ä¹ˆå¯ä»¥è®¾ä¸ºé—®å·$y = [1 0 ? 0]$ï¼Œè¿™æ ·ä¹Ÿæ˜¯å¯ä»¥è®­ç»ƒçš„ï¼Œä½†æ˜¯åœ¨è®¡ç®—lossçš„æ—¶å€™ï¼Œè¦æŠŠè¿™ä¸ªæœªæ ‡è®°çš„éƒ¨åˆ†æ‰£é™¤ï¼Œä¸è¦è®¡ç®—åœ¨å†…ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>ç«¯åˆ°ç«¯å­¦ä¹ </b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p>å‡å¦‚æˆ‘ä»¬è¿›è¡Œå…¬å¸é—¨ç¦ï¼Œéœ€è¦åˆ·è„¸è¿›å…¥ï¼Œé‚£ä¹ˆè¿™æ—¶å€™ç®—æ³•éœ€è¦åˆ†æˆä¸¤æ­¥ï¼Œ</p><ul><li>é¦–å…ˆæ£€æµ‹åˆ°ä½ è¿™ä¸ªäººï¼Œç„¶åæ‰¾åˆ°äººè„¸çš„ä½ç½®</li><li>æŠŠäººè„¸å›¾åƒæ–¹æ³•ï¼Œç„¶ååœ¨æ”¾å…¥æ¨¡å‹ä¸­è®¡ç®—æ˜¯å¦åŒ¹é…</li></ul><p>è€Œç«¯åˆ°ç«¯å­¦ä¹ åˆ™ç›´æ¥å¿½ç•¥çš„è¿™ä¸ªè¿‡ç¨‹ï¼Œç›´æ¥æ‹ä¸€å¼ ç…§ç‰‡æ”¾å…¥æ¨¡å‹ï¼Œè¾“å‡ºç»“æœã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><p>å†æ¯”å¦‚è¯´è¯­éŸ³è¯†åˆ«çš„æ—¶å€™ï¼Œåœ¨æ•°æ®å°‘çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯èƒ½éœ€è¦</p><ul><li>æå–å£°éŸ³</li><li>åˆ†æè¯­æ³•</li><li>åˆ‡åˆ†æˆä¸€ä¸ªä¸ªå‘å£°å­—æ¯</li><li>ç»„æˆå¥å­</li><li>ç¿»è¯‘</li></ul><p>è€Œç«¯åˆ°ç«¯å­¦ä¹ ç›´æ¥æ˜¯ï¼šæå–å£°éŸ³---&gt;ç¿»è¯‘</p><p>å°±ä¸éœ€è¦äººä¸ºçš„è¿‡å¤šå¹²é¢„äº†ï¼Œå› ä¸ºæœºå™¨å¯ä»¥å­¦åˆ°çš„æ¯”äººä¸ºè§„å®šçš„è¿˜è¦å¥½ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><p>ä½†æ˜¯æ³¨æ„ä¸€ç‚¹æ˜¯ï¼Œéœ€è¦å¾ˆå¤§é‡çš„æ•°æ®çš„æ—¶å€™æ‰èƒ½è¿›è¡Œç«¯åˆ°ç«¯å­¦ä¹ ï¼›å¦‚æœæ•°æ®å¾ˆå°‘ï¼Œé‚£ä¹ˆè¿˜æ˜¯æ‰‹åŠ¨å¹²é¢„ï¼Œè®¾è®¡ä¸€äº›ç»„ä»¶æ•ˆæœä¼šå¥½ä¸€ç‚¹ã€‚</p><p></p>", 
            "topic": [
                {
                    "tag": "ç¬”è®°", 
                    "tagLink": "https://api.zhihu.com/topics/19554982"
                }, 
                {
                    "tag": "æœºå™¨å­¦ä¹ ", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "æ·±åº¦å­¦ä¹ ï¼ˆDeep Learningï¼‰", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/45752269", 
            "userName": "ç›´ä¸Šäº‘éœ„", 
            "userLink": "https://www.zhihu.com/people/1033165ce4ad9c3fce69a0793dfab8ad", 
            "upvote": 0, 
            "title": "DeepLearning.aiç¬”è®°:(3-1)-- æœºå™¨å­¦ä¹ ç­–ç•¥(1)(ML strategy)", 
            "content": "<div class=\"highlight\"><pre><code class=\"language-text\">title: &#39;DeepLearning.aiç¬”è®°:(3-1)-- æœºå™¨å­¦ä¹ ç­–ç•¥(1)(ML strategy)&#39;\nid: 2018092016\ntags:\n  - dl.ai\ncategories:\n  - AI\n  - Deep Learning\ndate: 2018-09-20 16:48:41\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><b>é¦–å‘äºä¸ªäººåšå®¢: </b></p><b><a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Fangzhçš„ä¸ªäººåšå®¢ | äººå·¥æ™ºèƒ½æ‹¯æ•‘ä¸–ç•Œ</a></b><p><b> æ¬¢è¿æ¥è®¿</b></p><p>ç¬¬ä¸‰é—¨è¯¾ä¸»è¦è®²äº†æœºå™¨å­¦ä¹ çš„ä¸€äº›ç­–ç•¥ï¼Œä¹Ÿå°±æ˜¯åœ¨ä½ åšé¡¹ç›®çš„æ—¶å€™ï¼Œåº”è¯¥è¦å…·ä½“æ ¹æ®ä»€ä¹ˆæ¥æ”¹è¿›ä½ çš„æ¨¡å‹ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>æ­£äº¤åŒ–</b></h2><p>åœ¨è®¾è®¡è¿‡ç¨‹ä¸­ï¼Œæœ€å¥½æ˜¯ä¿è¯å‡ ä¸ªå˜é‡ç›¸äº’ç‹¬ç«‹ï¼Œä¹Ÿå°±æ˜¯æ­£äº¤ã€‚å°±å¥½æ¯”ä½ åœ¨å¼€è½¦çš„æ—¶å€™ï¼Œæ²¹é—¨å’Œæ–¹å‘ç›˜æ˜¯ç›¸äº’ç‹¬ç«‹çš„ã€‚å¦‚æœæ–¹å‘ç›˜å’Œæ²¹é—¨ä¸ç‹¬ç«‹ï¼Œå½“ä½ è°ƒæ•´æ–¹å‘ç›˜çš„æ—¶å€™é€Ÿåº¦ä¹Ÿåœ¨å˜åŒ–ï¼Œå°±å¾ˆéš¾å—äº†ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><p>æ‰€ä»¥åœ¨ç›‘ç£å­¦ä¹ ä¸­ï¼Œä»¥ä¸‹å‡ ä¸ªåº”è¯¥æ­£äº¤ï¼š</p><ul><li>æŸå¤±å‡½æ•°åº”è¯¥åœ¨è®­ç»ƒé›†ä¸Šè¡¨ç°å¾ˆå¥½<br/></li><ul><li>å¦åˆ™ï¼Œå°±ä½¿ç”¨<b>æ›´å¤§çš„ç¥ç»ç½‘ç»œ</b>ï¼Œæˆ–è€…ä½¿ç”¨<b>æ›´å¥½çš„ä¼˜åŒ–ç®—æ³•</b></li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>åœ¨éªŒè¯é›†ä¸Šè¡¨ç°å¾ˆå¥½<br/></li><ul><li>å¦åˆ™ï¼Œå°±ç”¨<b>æ­£åˆ™åŒ–</b>æˆ–è€…<b>è®­ç»ƒé›†ä¸Šè¦æ›´å¤šçš„æ•°æ®</b></li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>åœ¨æµ‹è¯•æœºä¸Šè¡¨ç°å¾ˆå¥½<br/></li><ul><li>å¦åˆ™ï¼Œå°±ä½¿ç”¨<b>æ›´å¤§çš„éªŒè¯é›†</b></li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>ç°å®ä¸­è¡¨ç°å¾ˆå¥½<br/></li><ul><li>å¦åˆ™ï¼Œå°±æ£€æŸ¥ä¸€ä¸‹<b>éªŒè¯é›†</b>æ˜¯ä¸æ˜¯å¯¹çš„ï¼Œ<b>æŸå¤±å‡½æ•°æ˜¯ä¸æ˜¯å¥½çš„</b></li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>å•ä¸€æ•°å­—è¯„ä¼°æŒ‡æ ‡</b></h2><p>åœ¨è®­ç»ƒæ¨¡å‹ä¸­ï¼Œå½“ç„¶éœ€è¦ä¸€ç§æŒ‡æ ‡æ¥è¯„ä¼°ä¸€ä¸‹æ¨¡å‹æ˜¯ä¸æ˜¯å¥½çš„ã€‚</p><p>ä¸€èˆ¬ä½¿ç”¨ä¸¤ä¸ªå‚æ•°ï¼š</p><ul><li>å‡†ç¡®ç‡pï¼šåœ¨é¢„æµ‹çš„æ•°æ®ä¸­ï¼Œæ˜¯æ­£ç¡®çš„æ¦‚ç‡</li><li>å¬å›ç‡rï¼šåœ¨çœŸå®æ•°æ®ä¸­ï¼Œé¢„æµ‹æ˜¯æ­£ç¡®çš„æ¦‚ç‡</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p>ä¸€èˆ¬ç”¨F1 ScoreæŠŠä¸¤ä¸ªæŒ‡æ ‡ç»™ç»Ÿä¸€èµ·æ¥ï¼š</p><p><img src=\"https://www.zhihu.com/equation?tex=F1-Score+%3D+%5Cfrac%7B2%7D%7B%5Cfrac%7B1%7D%7Bp%7D+%2B+%5Cfrac%7B1%7D%7Br%7D%7D\" alt=\"F1-Score = \\frac{2}{\\frac{1}{p} + \\frac{1}{r}}\" eeimg=\"1\"/> </p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>æ»¡è¶³å’Œä¼˜åŒ–æŒ‡æ ‡</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p>ä¸€èˆ¬ï¼Œæ»¡è¶³æŒ‡æ ‡éƒ½æ˜¯ä¸€ä¸ªåŒºé—´èŒƒå›´ï¼Œæ¯”å¦‚æ—¶é—´ä¸Šåªè¦å°äº100mså°±å¯ä»¥ï¼Œè¿™æ ·å­ï¼Œå°±åœ¨æ»¡è¶³æ»¡è¶³æŒ‡æ ‡çš„æƒ…å†µä¸‹ï¼Œé€‰æ‹©æœ€ä¼˜æŒ‡æ ‡ï¼ˆå¦‚ç²¾ç¡®åº¦æœ€é«˜ï¼‰æœ€å¥½çš„é‚£ä¸ªæ¨¡å‹ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†çš„åˆ’åˆ†</b></h2><p>åº”è¯¥ä½¿éªŒè¯é›†å’Œæµ‹è¯•é›†çš„æ•°æ®æ»¡è¶³ç»Ÿä¸€åˆ†å¸ƒã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>ä¸äººç±»è¡¨ç°æ¯”è¾ƒ</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p><b>å¯é¿å…çš„åå·®</b></p><p>æˆ‘ä»¬è®­ç»ƒå‡ºæ¥çš„ç»“æœï¼Œåº”è¯¥å’Œäººç±»è¡¨ç°ä½œæ¯”è¾ƒï¼Œå¦‚æœå·®è·æ¯”è¾ƒå°ï¼Œé‚£ä¹ˆè¯´æ˜å¾ˆæ¥è¿‘äº†ï¼Œå¦‚æœå·®è·æ¯”è¾ƒå¤§ï¼Œåº”è¯¥ç€é‡ä¼˜åŒ–ç¼©å°è¿™ä¸ªå¯é¿å…çš„åå·®ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-28b4f146c163b619b5f5929ec0970345_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1074\" data-rawheight=\"482\" class=\"origin_image zh-lightbox-thumb\" width=\"1074\" data-original=\"https://pic2.zhimg.com/v2-28b4f146c163b619b5f5929ec0970345_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1074&#39; height=&#39;482&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1074\" data-rawheight=\"482\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1074\" data-original=\"https://pic2.zhimg.com/v2-28b4f146c163b619b5f5929ec0970345_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-28b4f146c163b619b5f5929ec0970345_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>å¦‚å›¾ï¼Œå·¦è¾¹è¯´æ˜åº”è¯¥ç€é‡å‡å°biasï¼Œå³è¾¹åº”è¯¥ç€é‡å‡å°variance</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>æ”¹å–„æ¨¡å‹çš„è¡¨ç°</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p>å‡å°‘biasï¼š</p><ul><li>è®­ç»ƒæ›´å¤§çš„æ¨¡å‹</li><li>æ›´é•¿çš„æ—¶é—´ï¼Œæ›´ä¼˜åŒ–çš„ç®—æ³•ï¼ˆMomentumï¼ŒRMSpropï¼ŒAdamï¼‰</li><li>å¯»æ‰¾æ›´å¥½çš„ç½‘ç»œæ¶æ„ã€æ›´å¥½çš„å‚æ•°</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p>å‡å°‘varianceï¼š</p><ul><li>æ”¶é›†æ›´å¤šçš„æ•°æ®</li><li>æ­£åˆ™åŒ–</li><li>æ›´å¥½çš„æ¶æ„å’Œå‚æ•°</li></ul>", 
            "topic": [
                {
                    "tag": "æœºå™¨å­¦ä¹ ", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "ç¬”è®°", 
                    "tagLink": "https://api.zhihu.com/topics/19554982"
                }, 
                {
                    "tag": "æ·±åº¦å­¦ä¹ ï¼ˆDeep Learningï¼‰", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/45752207", 
            "userName": "ç›´ä¸Šäº‘éœ„", 
            "userLink": "https://www.zhihu.com/people/1033165ce4ad9c3fce69a0793dfab8ad", 
            "upvote": 0, 
            "title": "DeepLearning.aiä½œä¸š:(2-3)-- è¶…å‚æ•°è°ƒè¯•", 
            "content": "<div class=\"highlight\"><pre><code class=\"language-text\">title: &#39;DeepLearning.aiä½œä¸š:(2-3)-- è¶…å‚æ•°è°ƒè¯•ï¼ˆHyperparameter tuningï¼‰&#39;\nid: 2018091810\ntags:\n  - dl.ai\n  - homework\ncategories:\n  - AI\n  - Deep Learning\ndate: 2018-09-18 10:35:32\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><b>é¦–å‘äºä¸ªäººåšå®¢:[<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Fangzhçš„ä¸ªäººåšå®¢ | äººå·¥æ™ºèƒ½æ‹¯æ•‘ä¸–ç•Œ</a>](<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Fangzhçš„ä¸ªäººåšå®¢ | äººå·¥æ™ºèƒ½æ‹¯æ•‘ä¸–ç•Œ</a>)ï¼Œæ¬¢è¿æ¥è®¿</b></p><ol><li>ä¸è¦æŠ„ä½œä¸šï¼</li><li>æˆ‘åªæ˜¯æŠŠæ€è·¯æ•´ç†äº†ï¼Œä¾›ä¸ªäººå­¦ä¹ ã€‚</li><li>ä¸è¦æŠ„ä½œä¸šï¼</li></ol><p>æœ¬å‘¨ä¸»è¦æ˜¯TensorFlowçš„ç®€å•æ•™ç¨‹ï¼Œæ²¡ä»€ä¹ˆå¥½è¯´çš„ï¼Œå¯ä»¥å»çœ‹çœ‹æ›´è¯¦ç»†ä¸€ç‚¹çš„æ•™ç¨‹ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: linear_function</span>\n<span class=\"err\">â€‹</span>\n<span class=\"k\">def</span> <span class=\"nf\">linear_function</span><span class=\"p\">():</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Implements a linear function: \n</span><span class=\"s2\">            Initializes W to be a random tensor of shape (4,3)\n</span><span class=\"s2\">            Initializes X to be a random tensor of shape (3,1)\n</span><span class=\"s2\">            Initializes b to be a random tensor of shape (4,1)\n</span><span class=\"s2\">    Returns: \n</span><span class=\"s2\">    result -- runs the session for Y = WX + b \n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">seed</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n    \n    <span class=\"c1\">### START CODE HERE ### (4 lines of code)</span>\n    <span class=\"n\">X</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">constant</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">name</span> <span class=\"o\">=</span> <span class=\"s2\">&#34;X&#34;</span><span class=\"p\">)</span>\n    <span class=\"n\">W</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">constant</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">4</span><span class=\"p\">,</span><span class=\"mi\">3</span><span class=\"p\">),</span> <span class=\"n\">name</span> <span class=\"o\">=</span> <span class=\"s2\">&#34;W&#34;</span><span class=\"p\">)</span>\n    <span class=\"n\">b</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">constant</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">4</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">name</span> <span class=\"o\">=</span> <span class=\"s2\">&#34;b&#34;</span><span class=\"p\">)</span>\n    <span class=\"n\">Y</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">W</span><span class=\"p\">,</span><span class=\"n\">X</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">b</span>\n    <span class=\"c1\">### END CODE HERE ### </span>\n    \n    <span class=\"c1\"># Create the session using tf.Session() and run it with sess.run(...) on the variable you want to calculate</span>\n    \n    <span class=\"c1\">### START CODE HERE ###</span>\n    <span class=\"n\">sess</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">Session</span><span class=\"p\">()</span>\n    <span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">Y</span><span class=\"p\">)</span>\n    <span class=\"c1\">### END CODE HERE ### </span>\n    \n    <span class=\"c1\"># close the session </span>\n    <span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">close</span><span class=\"p\">()</span>\n<span class=\"err\">â€‹</span>\n    <span class=\"k\">return</span> <span class=\"n\">result</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: sigmoid</span>\n<span class=\"err\">â€‹</span>\n<span class=\"k\">def</span> <span class=\"nf\">sigmoid</span><span class=\"p\">(</span><span class=\"n\">z</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Computes the sigmoid of z\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    z -- input value, scalar or vector\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns: \n</span><span class=\"s2\">    results -- the sigmoid of z\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"c1\">### START CODE HERE ### ( approx. 4 lines of code)</span>\n    <span class=\"c1\"># Create a placeholder for x. Name it &#39;x&#39;.</span>\n    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">placeholder</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">,</span><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s2\">&#34;x&#34;</span><span class=\"p\">)</span>\n<span class=\"err\">â€‹</span>\n    <span class=\"c1\"># compute sigmoid(x)</span>\n    <span class=\"n\">sigmoid</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">sigmoid</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n<span class=\"err\">â€‹</span>\n    <span class=\"c1\"># Create a session, and run it. Please use the method 2 explained above. </span>\n    <span class=\"c1\"># You should use a feed_dict to pass z&#39;s value to x. </span>\n    <span class=\"k\">with</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">Session</span><span class=\"p\">()</span> <span class=\"k\">as</span> <span class=\"n\">sess</span><span class=\"p\">:</span>\n        <span class=\"c1\"># Run session and call the output &#34;result&#34;</span>\n        <span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">sigmoid</span><span class=\"p\">,</span><span class=\"n\">feed_dict</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"n\">x</span><span class=\"p\">:</span><span class=\"n\">z</span><span class=\"p\">})</span>\n    \n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">result</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: cost</span>\n<span class=\"err\">â€‹</span>\n<span class=\"k\">def</span> <span class=\"nf\">cost</span><span class=\"p\">(</span><span class=\"n\">logits</span><span class=\"p\">,</span> <span class=\"n\">labels</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Computes the cost using the sigmoid cross entropy\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    logits -- vector containing z, output of the last linear unit (before the final sigmoid activation)\n</span><span class=\"s2\">    labels -- vector of labels y (1 or 0) \n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Note: What we&#39;ve been calling &#34;z&#34; and &#34;y&#34; in this class are respectively called &#34;logits&#34; and &#34;labels&#34; \n</span><span class=\"s2\">    in the TensorFlow documentation. So logits will feed into z, and labels into y. \n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    cost -- runs the session of the cost (formula (2))\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"c1\">### START CODE HERE ### </span>\n    \n    <span class=\"c1\"># Create the placeholders for &#34;logits&#34; (z) and &#34;labels&#34; (y) (approx. 2 lines)</span>\n    <span class=\"n\">z</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">placeholder</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">,</span><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s2\">&#34;z&#34;</span><span class=\"p\">)</span>\n    <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">placeholder</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">,</span><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s2\">&#34;y&#34;</span><span class=\"p\">)</span>\n    \n    <span class=\"c1\"># Use the loss function (approx. 1 line)</span>\n    <span class=\"n\">cost</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">sigmoid_cross_entropy_with_logits</span><span class=\"p\">(</span><span class=\"n\">logits</span><span class=\"o\">=</span><span class=\"n\">z</span><span class=\"p\">,</span><span class=\"n\">labels</span><span class=\"o\">=</span><span class=\"n\">y</span><span class=\"p\">)</span>\n    \n    <span class=\"c1\"># Create a session (approx. 1 line). See method 1 above.</span>\n    <span class=\"n\">sess</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">Session</span><span class=\"p\">()</span>\n    \n    <span class=\"c1\"># Run the session (approx. 1 line).</span>\n    <span class=\"n\">cost</span> <span class=\"o\">=</span> <span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">cost</span><span class=\"p\">,</span><span class=\"n\">feed_dict</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"n\">z</span><span class=\"p\">:</span><span class=\"n\">logits</span><span class=\"p\">,</span><span class=\"n\">y</span><span class=\"p\">:</span><span class=\"n\">labels</span><span class=\"p\">})</span>\n    \n    <span class=\"c1\"># Close the session (approx. 1 line). See method 1 above.</span>\n    <span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">close</span><span class=\"p\">()</span>\n    \n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">cost</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: one_hot_matrix</span>\n<span class=\"err\">â€‹</span>\n<span class=\"k\">def</span> <span class=\"nf\">one_hot_matrix</span><span class=\"p\">(</span><span class=\"n\">labels</span><span class=\"p\">,</span> <span class=\"n\">C</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Creates a matrix where the i-th row corresponds to the ith class number and the jth column\n</span><span class=\"s2\">                     corresponds to the jth training example. So if example j had a label i. Then entry (i,j) \n</span><span class=\"s2\">                     will be 1. \n</span><span class=\"s2\">                     \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    labels -- vector containing the labels \n</span><span class=\"s2\">    C -- number of classes, the depth of the one hot dimension\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns: \n</span><span class=\"s2\">    one_hot -- one hot matrix\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"c1\">### START CODE HERE ###</span>\n    \n    <span class=\"c1\"># Create a tf.constant equal to C (depth), name it &#39;C&#39;. (approx. 1 line)</span>\n    <span class=\"n\">C</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">constant</span><span class=\"p\">(</span><span class=\"n\">C</span><span class=\"p\">)</span>\n    \n    <span class=\"c1\"># Use tf.one_hot, be careful with the axis (approx. 1 line)</span>\n    <span class=\"n\">one_hot_matrix</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">one_hot</span><span class=\"p\">(</span><span class=\"n\">labels</span><span class=\"p\">,</span> <span class=\"n\">C</span><span class=\"p\">,</span> <span class=\"n\">axis</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">)</span>\n    \n    <span class=\"c1\"># Create the session (approx. 1 line)</span>\n    <span class=\"n\">sess</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">Session</span><span class=\"p\">()</span>\n    \n    <span class=\"c1\"># Run the session (approx. 1 line)</span>\n    <span class=\"n\">one_hot</span> <span class=\"o\">=</span> <span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">one_hot_matrix</span><span class=\"p\">)</span>\n    \n    <span class=\"c1\"># Close the session (approx. 1 line). See method 1 above.</span>\n    <span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">close</span><span class=\"p\">()</span>\n    \n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">one_hot</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: ones</span>\n<span class=\"err\">â€‹</span>\n<span class=\"k\">def</span> <span class=\"nf\">ones</span><span class=\"p\">(</span><span class=\"n\">shape</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Creates an array of ones of dimension shape\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    shape -- shape of the array you want to create\n</span><span class=\"s2\">        \n</span><span class=\"s2\">    Returns: \n</span><span class=\"s2\">    ones -- array containing only ones\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"c1\">### START CODE HERE ###</span>\n    \n    <span class=\"c1\"># Create &#34;ones&#34; tensor using tf.ones(...). (approx. 1 line)</span>\n    <span class=\"n\">ones</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">ones</span><span class=\"p\">(</span><span class=\"n\">shape</span><span class=\"p\">)</span>\n    \n    <span class=\"c1\"># Create the session (approx. 1 line)</span>\n    <span class=\"n\">sess</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">Session</span><span class=\"p\">()</span>\n    \n    <span class=\"c1\"># Run the session to compute &#39;ones&#39; (approx. 1 line)</span>\n    <span class=\"n\">ones</span> <span class=\"o\">=</span> <span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">ones</span><span class=\"p\">)</span>\n    \n    <span class=\"c1\"># Close the session (approx. 1 line). See method 1 above.</span>\n    <span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">close</span><span class=\"p\">()</span>\n    \n    <span class=\"c1\">### END CODE HERE ###</span>\n    <span class=\"k\">return</span> <span class=\"n\">ones</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>Building neural network</b></h2><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: create_placeholders</span>\n<span class=\"err\">â€‹</span>\n<span class=\"k\">def</span> <span class=\"nf\">create_placeholders</span><span class=\"p\">(</span><span class=\"n\">n_x</span><span class=\"p\">,</span> <span class=\"n\">n_y</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Creates the placeholders for the tensorflow session.\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    n_x -- scalar, size of an image vector (num_px * num_px = 64 * 64 * 3 = 12288)\n</span><span class=\"s2\">    n_y -- scalar, number of classes (from 0 to 5, so -&gt; 6)\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    X -- placeholder for the data input, of shape [n_x, None] and dtype &#34;float&#34;\n</span><span class=\"s2\">    Y -- placeholder for the input labels, of shape [n_y, None] and dtype &#34;float&#34;\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Tips:\n</span><span class=\"s2\">    - You will use None because it let&#39;s us be flexible on the number of examples you will for the placeholders.\n</span><span class=\"s2\">      In fact, the number of examples during test/train is different.\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n<span class=\"err\">â€‹</span>\n    <span class=\"c1\">### START CODE HERE ### (approx. 2 lines)</span>\n    <span class=\"n\">X</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">placeholder</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">,[</span><span class=\"n\">n_x</span><span class=\"p\">,</span><span class=\"bp\">None</span><span class=\"p\">])</span>\n    <span class=\"n\">Y</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">placeholder</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">,[</span><span class=\"n\">n_y</span><span class=\"p\">,</span><span class=\"bp\">None</span><span class=\"p\">])</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: initialize_parameters</span>\n<span class=\"err\">â€‹</span>\n<span class=\"k\">def</span> <span class=\"nf\">initialize_parameters</span><span class=\"p\">():</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Initializes parameters to build a neural network with tensorflow. The shapes are:\n</span><span class=\"s2\">                        W1 : [25, 12288]\n</span><span class=\"s2\">                        b1 : [25, 1]\n</span><span class=\"s2\">                        W2 : [12, 25]\n</span><span class=\"s2\">                        b2 : [12, 1]\n</span><span class=\"s2\">                        W3 : [6, 12]\n</span><span class=\"s2\">                        b3 : [6, 1]\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    parameters -- a dictionary of tensors containing W1, b1, W2, b2, W3, b3\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">set_random_seed</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>                   <span class=\"c1\"># so that your &#34;random&#34; numbers match ours</span>\n        \n    <span class=\"c1\">### START CODE HERE ### (approx. 6 lines of code)</span>\n    <span class=\"n\">W1</span> <span class=\"o\">=</span>  <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">get_variable</span><span class=\"p\">(</span><span class=\"s2\">&#34;W1&#34;</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"mi\">25</span><span class=\"p\">,</span><span class=\"mi\">12288</span><span class=\"p\">],</span> <span class=\"n\">initializer</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">contrib</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">xavier_initializer</span><span class=\"p\">(</span><span class=\"n\">seed</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n    <span class=\"n\">b1</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">get_variable</span><span class=\"p\">(</span><span class=\"s2\">&#34;b1&#34;</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"mi\">25</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">initializer</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">zeros_initializer</span><span class=\"p\">())</span>\n    <span class=\"n\">W2</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">get_variable</span><span class=\"p\">(</span><span class=\"s2\">&#34;W2&#34;</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"mi\">12</span><span class=\"p\">,</span><span class=\"mi\">25</span><span class=\"p\">],</span> <span class=\"n\">initializer</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">contrib</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">xavier_initializer</span><span class=\"p\">(</span><span class=\"n\">seed</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n    <span class=\"n\">b2</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">get_variable</span><span class=\"p\">(</span><span class=\"s2\">&#34;b2&#34;</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"mi\">12</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">initializer</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">zeros_initializer</span><span class=\"p\">())</span>\n    <span class=\"n\">W3</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">get_variable</span><span class=\"p\">(</span><span class=\"s2\">&#34;W3&#34;</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"mi\">6</span><span class=\"p\">,</span><span class=\"mi\">12</span><span class=\"p\">],</span> <span class=\"n\">initializer</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">contrib</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">xavier_initializer</span><span class=\"p\">(</span><span class=\"n\">seed</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n    <span class=\"n\">b3</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">get_variable</span><span class=\"p\">(</span><span class=\"s2\">&#34;b3&#34;</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"mi\">6</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">initializer</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">zeros_initializer</span><span class=\"p\">())</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n<span class=\"err\">â€‹</span>\n    <span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">&#34;W1&#34;</span><span class=\"p\">:</span> <span class=\"n\">W1</span><span class=\"p\">,</span>\n                  <span class=\"s2\">&#34;b1&#34;</span><span class=\"p\">:</span> <span class=\"n\">b1</span><span class=\"p\">,</span>\n                  <span class=\"s2\">&#34;W2&#34;</span><span class=\"p\">:</span> <span class=\"n\">W2</span><span class=\"p\">,</span>\n                  <span class=\"s2\">&#34;b2&#34;</span><span class=\"p\">:</span> <span class=\"n\">b2</span><span class=\"p\">,</span>\n                  <span class=\"s2\">&#34;W3&#34;</span><span class=\"p\">:</span> <span class=\"n\">W3</span><span class=\"p\">,</span>\n                  <span class=\"s2\">&#34;b3&#34;</span><span class=\"p\">:</span> <span class=\"n\">b3</span><span class=\"p\">}</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">parameters</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: forward_propagation</span>\n<span class=\"err\">â€‹</span>\n<span class=\"k\">def</span> <span class=\"nf\">forward_propagation</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">parameters</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Implements the forward propagation for the model: LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SOFTMAX\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    X -- input dataset placeholder, of shape (input size, number of examples)\n</span><span class=\"s2\">    parameters -- python dictionary containing your parameters &#34;W1&#34;, &#34;b1&#34;, &#34;W2&#34;, &#34;b2&#34;, &#34;W3&#34;, &#34;b3&#34;\n</span><span class=\"s2\">                  the shapes are given in initialize_parameters\n</span><span class=\"s2\">â€‹\n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    Z3 -- the output of the last LINEAR unit\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"c1\"># Retrieve the parameters from the dictionary &#34;parameters&#34; </span>\n    <span class=\"n\">W1</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;W1&#39;</span><span class=\"p\">]</span>\n    <span class=\"n\">b1</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;b1&#39;</span><span class=\"p\">]</span>\n    <span class=\"n\">W2</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;W2&#39;</span><span class=\"p\">]</span>\n    <span class=\"n\">b2</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;b2&#39;</span><span class=\"p\">]</span>\n    <span class=\"n\">W3</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;W3&#39;</span><span class=\"p\">]</span>\n    <span class=\"n\">b3</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;b3&#39;</span><span class=\"p\">]</span>\n    \n    <span class=\"c1\">### START CODE HERE ### (approx. 5 lines)              # Numpy Equivalents:</span>\n    <span class=\"n\">Z1</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">W1</span><span class=\"p\">,</span><span class=\"n\">X</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">b1</span>                                              <span class=\"c1\"># Z1 = np.dot(W1, X) + b1</span>\n    <span class=\"n\">A1</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">relu</span><span class=\"p\">(</span><span class=\"n\">Z1</span><span class=\"p\">)</span>                                              <span class=\"c1\"># A1 = relu(Z1)</span>\n    <span class=\"n\">Z2</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">W2</span><span class=\"p\">,</span><span class=\"n\">A1</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">b2</span>                                              <span class=\"c1\"># Z2 = np.dot(W2, a1) + b2</span>\n    <span class=\"n\">A2</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">relu</span><span class=\"p\">(</span><span class=\"n\">Z2</span><span class=\"p\">)</span>                                              <span class=\"c1\"># A2 = relu(Z2)</span>\n    <span class=\"n\">Z3</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">W3</span><span class=\"p\">,</span><span class=\"n\">A2</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">b3</span>                                              <span class=\"c1\"># Z3 = np.dot(W3,Z2) + b3</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">Z3</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: compute_cost </span>\n<span class=\"err\">â€‹</span>\n<span class=\"k\">def</span> <span class=\"nf\">compute_cost</span><span class=\"p\">(</span><span class=\"n\">Z3</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Computes the cost\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n</span><span class=\"s2\">    Y -- &#34;true&#34; labels vector placeholder, same shape as Z3\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    cost - Tensor of the cost function\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"c1\"># to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)</span>\n    <span class=\"n\">logits</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">transpose</span><span class=\"p\">(</span><span class=\"n\">Z3</span><span class=\"p\">)</span>\n    <span class=\"n\">labels</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">transpose</span><span class=\"p\">(</span><span class=\"n\">Y</span><span class=\"p\">)</span>\n    \n    <span class=\"c1\">### START CODE HERE ### (1 line of code)</span>\n    <span class=\"n\">cost</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">reduce_mean</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">softmax_cross_entropy_with_logits</span><span class=\"p\">(</span><span class=\"n\">logits</span> <span class=\"o\">=</span><span class=\"n\">logits</span><span class=\"p\">,</span> <span class=\"n\">labels</span> <span class=\"o\">=</span> <span class=\"n\">labels</span><span class=\"p\">))</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">cost</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">model</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">,</span> <span class=\"n\">Y_train</span><span class=\"p\">,</span> <span class=\"n\">X_test</span><span class=\"p\">,</span> <span class=\"n\">Y_test</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span> <span class=\"o\">=</span> <span class=\"mf\">0.0001</span><span class=\"p\">,</span>\n          <span class=\"n\">num_epochs</span> <span class=\"o\">=</span> <span class=\"mi\">1500</span><span class=\"p\">,</span> <span class=\"n\">minibatch_size</span> <span class=\"o\">=</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"n\">print_cost</span> <span class=\"o\">=</span> <span class=\"bp\">True</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Implements a three-layer tensorflow neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;RELU-&gt;LINEAR-&gt;SOFTMAX.\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    X_train -- training set, of shape (input size = 12288, number of training examples = 1080)\n</span><span class=\"s2\">    Y_train -- test set, of shape (output size = 6, number of training examples = 1080)\n</span><span class=\"s2\">    X_test -- training set, of shape (input size = 12288, number of training examples = 120)\n</span><span class=\"s2\">    Y_test -- test set, of shape (output size = 6, number of test examples = 120)\n</span><span class=\"s2\">    learning_rate -- learning rate of the optimization\n</span><span class=\"s2\">    num_epochs -- number of epochs of the optimization loop\n</span><span class=\"s2\">    minibatch_size -- size of a minibatch\n</span><span class=\"s2\">    print_cost -- True to print the cost every 100 epochs\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    parameters -- parameters learnt by the model. They can then be used to predict.\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"n\">ops</span><span class=\"o\">.</span><span class=\"n\">reset_default_graph</span><span class=\"p\">()</span>                         <span class=\"c1\"># to be able to rerun the model without overwriting tf variables</span>\n    <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">set_random_seed</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>                             <span class=\"c1\"># to keep consistent results</span>\n    <span class=\"n\">seed</span> <span class=\"o\">=</span> <span class=\"mi\">3</span>                                          <span class=\"c1\"># to keep consistent results</span>\n    <span class=\"p\">(</span><span class=\"n\">n_x</span><span class=\"p\">,</span> <span class=\"n\">m</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">X_train</span><span class=\"o\">.</span><span class=\"n\">shape</span>                          <span class=\"c1\"># (n_x: input size, m : number of examples in the train set)</span>\n    <span class=\"n\">n_y</span> <span class=\"o\">=</span> <span class=\"n\">Y_train</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>                            <span class=\"c1\"># n_y : output size</span>\n    <span class=\"n\">costs</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>                                        <span class=\"c1\"># To keep track of the cost</span>\n    \n    <span class=\"c1\"># Create Placeholders of shape (n_x, n_y)</span>\n    <span class=\"c1\">### START CODE HERE ### (1 line)</span>\n    <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span> <span class=\"o\">=</span> <span class=\"n\">create_placeholders</span><span class=\"p\">(</span><span class=\"n\">n_x</span><span class=\"p\">,</span><span class=\"n\">n_y</span><span class=\"p\">)</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n<span class=\"err\">â€‹</span>\n    <span class=\"c1\"># Initialize parameters</span>\n    <span class=\"c1\">### START CODE HERE ### (1 line)</span>\n    <span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"n\">initialize_parameters</span><span class=\"p\">()</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"c1\"># Forward propagation: Build the forward propagation in the tensorflow graph</span>\n    <span class=\"c1\">### START CODE HERE ### (1 line)</span>\n    <span class=\"n\">Z3</span> <span class=\"o\">=</span> <span class=\"n\">forward_propagation</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">parameters</span><span class=\"p\">)</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"c1\"># Cost function: Add cost function to tensorflow graph</span>\n    <span class=\"c1\">### START CODE HERE ### (1 line)</span>\n    <span class=\"n\">cost</span> <span class=\"o\">=</span> <span class=\"n\">compute_cost</span><span class=\"p\">(</span><span class=\"n\">Z3</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">)</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"c1\"># Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.</span>\n    <span class=\"c1\">### START CODE HERE ### (1 line)</span>\n    <span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"o\">.</span><span class=\"n\">GradientDescentOptimizer</span><span class=\"p\">(</span><span class=\"n\">learning_rate</span> <span class=\"o\">=</span> <span class=\"n\">learning_rate</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">minimize</span><span class=\"p\">(</span><span class=\"n\">cost</span><span class=\"p\">)</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"c1\"># Initialize all the variables</span>\n    <span class=\"n\">init</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">global_variables_initializer</span><span class=\"p\">()</span>\n<span class=\"err\">â€‹</span>\n    <span class=\"c1\"># Start the session to compute the tensorflow graph</span>\n    <span class=\"k\">with</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">Session</span><span class=\"p\">()</span> <span class=\"k\">as</span> <span class=\"n\">sess</span><span class=\"p\">:</span>\n        \n        <span class=\"c1\"># Run the initialization</span>\n        <span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">init</span><span class=\"p\">)</span>\n        \n        <span class=\"c1\"># Do the training loop</span>\n        <span class=\"k\">for</span> <span class=\"n\">epoch</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">num_epochs</span><span class=\"p\">):</span>\n<span class=\"err\">â€‹</span>\n            <span class=\"n\">epoch_cost</span> <span class=\"o\">=</span> <span class=\"mf\">0.</span>                       <span class=\"c1\"># Defines a cost related to an epoch</span>\n            <span class=\"n\">num_minibatches</span> <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">m</span> <span class=\"o\">/</span> <span class=\"n\">minibatch_size</span><span class=\"p\">)</span> <span class=\"c1\"># number of minibatches of size minibatch_size in the train set</span>\n            <span class=\"n\">seed</span> <span class=\"o\">=</span> <span class=\"n\">seed</span> <span class=\"o\">+</span> <span class=\"mi\">1</span>\n            <span class=\"n\">minibatches</span> <span class=\"o\">=</span> <span class=\"n\">random_mini_batches</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">,</span> <span class=\"n\">Y_train</span><span class=\"p\">,</span> <span class=\"n\">minibatch_size</span><span class=\"p\">,</span> <span class=\"n\">seed</span><span class=\"p\">)</span>\n<span class=\"err\">â€‹</span>\n            <span class=\"k\">for</span> <span class=\"n\">minibatch</span> <span class=\"ow\">in</span> <span class=\"n\">minibatches</span><span class=\"p\">:</span>\n<span class=\"err\">â€‹</span>\n                <span class=\"c1\"># Select a minibatch</span>\n                <span class=\"p\">(</span><span class=\"n\">minibatch_X</span><span class=\"p\">,</span> <span class=\"n\">minibatch_Y</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">minibatch</span>\n                \n                <span class=\"c1\"># IMPORTANT: The line that runs the graph on a minibatch.</span>\n                <span class=\"c1\"># Run the session to execute the &#34;optimizer&#34; and the &#34;cost&#34;, the feedict should contain a minibatch for (X,Y).</span>\n                <span class=\"c1\">### START CODE HERE ### (1 line)</span>\n                <span class=\"n\">_</span> <span class=\"p\">,</span> <span class=\"n\">minibatch_cost</span> <span class=\"o\">=</span> <span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">([</span><span class=\"n\">optimizer</span><span class=\"p\">,</span> <span class=\"n\">cost</span><span class=\"p\">],</span> <span class=\"n\">feed_dict</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"n\">X</span><span class=\"p\">:</span> <span class=\"n\">minibatch_X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">:</span> <span class=\"n\">minibatch_Y</span><span class=\"p\">})</span>\n                <span class=\"c1\">### END CODE HERE ###</span>\n                \n                <span class=\"n\">epoch_cost</span> <span class=\"o\">+=</span> <span class=\"n\">minibatch_cost</span> <span class=\"o\">/</span> <span class=\"n\">num_minibatches</span>\n<span class=\"err\">â€‹</span>\n            <span class=\"c1\"># Print the cost every epoch</span>\n            <span class=\"k\">if</span> <span class=\"n\">print_cost</span> <span class=\"o\">==</span> <span class=\"bp\">True</span> <span class=\"ow\">and</span> <span class=\"n\">epoch</span> <span class=\"o\">%</span> <span class=\"mi\">100</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n                <span class=\"k\">print</span> <span class=\"p\">(</span><span class=\"s2\">&#34;Cost after epoch </span><span class=\"si\">%i</span><span class=\"s2\">: </span><span class=\"si\">%f</span><span class=\"s2\">&#34;</span> <span class=\"o\">%</span> <span class=\"p\">(</span><span class=\"n\">epoch</span><span class=\"p\">,</span> <span class=\"n\">epoch_cost</span><span class=\"p\">))</span>\n            <span class=\"k\">if</span> <span class=\"n\">print_cost</span> <span class=\"o\">==</span> <span class=\"bp\">True</span> <span class=\"ow\">and</span> <span class=\"n\">epoch</span> <span class=\"o\">%</span> <span class=\"mi\">5</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n                <span class=\"n\">costs</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">epoch_cost</span><span class=\"p\">)</span>\n                \n        <span class=\"c1\"># plot the cost</span>\n        <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">squeeze</span><span class=\"p\">(</span><span class=\"n\">costs</span><span class=\"p\">))</span>\n        <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">ylabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;cost&#39;</span><span class=\"p\">)</span>\n        <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">xlabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;iterations (per tens)&#39;</span><span class=\"p\">)</span>\n        <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">title</span><span class=\"p\">(</span><span class=\"s2\">&#34;Learning rate =&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">learning_rate</span><span class=\"p\">))</span>\n        <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>\n<span class=\"err\">â€‹</span>\n        <span class=\"c1\"># lets save the parameters in a variable</span>\n        <span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">)</span>\n        <span class=\"k\">print</span> <span class=\"p\">(</span><span class=\"s2\">&#34;Parameters have been trained!&#34;</span><span class=\"p\">)</span>\n<span class=\"err\">â€‹</span>\n        <span class=\"c1\"># Calculate the correct predictions</span>\n        <span class=\"n\">correct_prediction</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">equal</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">argmax</span><span class=\"p\">(</span><span class=\"n\">Z3</span><span class=\"p\">),</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">argmax</span><span class=\"p\">(</span><span class=\"n\">Y</span><span class=\"p\">))</span>\n<span class=\"err\">â€‹</span>\n        <span class=\"c1\"># Calculate accuracy on the test set</span>\n        <span class=\"n\">accuracy</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">reduce_mean</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">cast</span><span class=\"p\">(</span><span class=\"n\">correct_prediction</span><span class=\"p\">,</span> <span class=\"s2\">&#34;float&#34;</span><span class=\"p\">))</span>\n<span class=\"err\">â€‹</span>\n        <span class=\"k\">print</span> <span class=\"p\">(</span><span class=\"s2\">&#34;Train Accuracy:&#34;</span><span class=\"p\">,</span> <span class=\"n\">accuracy</span><span class=\"o\">.</span><span class=\"nb\">eval</span><span class=\"p\">({</span><span class=\"n\">X</span><span class=\"p\">:</span> <span class=\"n\">X_train</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">:</span> <span class=\"n\">Y_train</span><span class=\"p\">}))</span>\n        <span class=\"k\">print</span> <span class=\"p\">(</span><span class=\"s2\">&#34;Test Accuracy:&#34;</span><span class=\"p\">,</span> <span class=\"n\">accuracy</span><span class=\"o\">.</span><span class=\"nb\">eval</span><span class=\"p\">({</span><span class=\"n\">X</span><span class=\"p\">:</span> <span class=\"n\">X_test</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">:</span> <span class=\"n\">Y_test</span><span class=\"p\">}))</span>\n        \n        <span class=\"k\">return</span> <span class=\"n\">parameters</span></code></pre></div><p></p>", 
            "topic": [
                {
                    "tag": "ç¥ç»ç½‘ç»œ", 
                    "tagLink": "https://api.zhihu.com/topics/19607065"
                }, 
                {
                    "tag": "è½¯ä»¶è°ƒè¯•", 
                    "tagLink": "https://api.zhihu.com/topics/19660363"
                }, 
                {
                    "tag": "æœºå™¨å­¦ä¹ ", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/45751849", 
            "userName": "ç›´ä¸Šäº‘éœ„", 
            "userLink": "https://www.zhihu.com/people/1033165ce4ad9c3fce69a0793dfab8ad", 
            "upvote": 0, 
            "title": "DeepLearning.aiç¬”è®°:(2-3)-- è¶…å‚æ•°è°ƒè¯•", 
            "content": "<p><b>é¦–å‘äºä¸ªäººåšå®¢:</b></p><a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Fangzhçš„ä¸ªäººåšå®¢ | äººå·¥æ™ºèƒ½æ‹¯æ•‘ä¸–ç•Œ</a><p><b>æ¬¢è¿æ¥è®¿</b></p><p>è¿™å‘¨ä¸»è¦è®²äº†è¿™äº›è¶…å‚æ•°è°ƒè¯•çš„æ–¹æ³•ä»¥åŠbatch normï¼Œè¿˜æœ‰softmaxå¤šåˆ†ç±»å‡½æ•°çš„ä½¿ç”¨ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>è°ƒè¯•å¤„ç†</b></h2><p>ä¹‹å‰æåˆ°çš„è¶…å‚æ•°æœ‰ï¼š</p><ul><li><img src=\"https://www.zhihu.com/equation?tex=%5Calpha\" alt=\"\\alpha\" eeimg=\"1\"/> </li><li>hidden units</li><li>minibatch size</li><li><img src=\"https://www.zhihu.com/equation?tex=%5Cbeta\" alt=\"\\beta\" eeimg=\"1\"/> (Momentum)</li><li>layers</li><li>learning rate decay</li><li><img src=\"https://www.zhihu.com/equation?tex=%5Cbeta_1%2C%5Cbeta_2%2C%5Cepsilon\" alt=\"\\beta_1,\\beta_2,\\epsilon\" eeimg=\"1\"/> </li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p>åœ¨è°ƒå‚ä¸­ï¼Œå¸¸ç”¨çš„æ–¹å¼æ˜¯åœ¨ç½‘æ ¼ä¸­å–ä¸åŒçš„ç‚¹ï¼Œç„¶åè®¡ç®—è¿™äº›ç‚¹ä¸­çš„æœ€ä½³å€¼ï¼Œ</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-44e696aef2ec587049175b65e1e000d9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1095\" data-rawheight=\"430\" class=\"origin_image zh-lightbox-thumb\" width=\"1095\" data-original=\"https://pic2.zhimg.com/v2-44e696aef2ec587049175b65e1e000d9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1095&#39; height=&#39;430&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1095\" data-rawheight=\"430\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1095\" data-original=\"https://pic2.zhimg.com/v2-44e696aef2ec587049175b65e1e000d9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-44e696aef2ec587049175b65e1e000d9_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>ä½†æ˜¯å·¦è¾¹æ˜¯å‡åŒ€çš„é€‰ç‚¹ï¼Œè¿™æ ·æœ‰å¯èƒ½å¯¼è‡´åœ¨æŸä¸€ä¸ªå‚æ•°ä¸Šå˜åŒ–å¾ˆå°ï¼Œæµªè´¹è®¡ç®—æ—¶é—´ï¼Œæ‰€ä»¥åº”è¯¥æ›´æ¨èå³è¾¹çš„é€‰ç‚¹æ–¹æ³•ï¼Œå³éšæœºé€‰ç‚¹ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><p>è€Œåï¼Œå½“éšæœºé€‰ç‚¹é€‰åˆ°å‡ ä¸ªç»“æœæ¯”è¾ƒå¥½çš„ç‚¹æ—¶ï¼Œé€æ­¥ç¼©å°èŒƒå›´ï¼Œè¿›è¡Œæ›´ç²¾ç»†çš„é€‰å–ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-89e4a7e88cbe89467062b718e356c1c5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"820\" data-rawheight=\"607\" class=\"origin_image zh-lightbox-thumb\" width=\"820\" data-original=\"https://pic2.zhimg.com/v2-89e4a7e88cbe89467062b718e356c1c5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;820&#39; height=&#39;607&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"820\" data-rawheight=\"607\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"820\" data-original=\"https://pic2.zhimg.com/v2-89e4a7e88cbe89467062b718e356c1c5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-89e4a7e88cbe89467062b718e356c1c5_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>è¶…å‚æ•°çš„åˆé€‚èŒƒå›´</b></h2><p>å½“ç„¶ï¼Œéšæœºé‡‡æ ·å¹¶ä¸æ˜¯åœ¨è½´ä¸Šå‡åŒ€çš„é‡‡æ ·ã€‚</p><p>æ¯”å¦‚è¯´ <img src=\"https://www.zhihu.com/equation?tex=%5Calpha+%3D+0.001++-1\" alt=\"\\alpha = 0.001  -1\" eeimg=\"1\"/> ï¼Œè¿™æ ·å­ï¼Œé‚£ä¹ˆåœ¨ <img src=\"https://www.zhihu.com/equation?tex=0.1-1\" alt=\"0.1-1\" eeimg=\"1\"/> çš„éƒ¨åˆ†å äº†90%çš„æ¦‚ç‡ï¼Œæ˜¾ç„¶æ˜¯ä¸åˆç†çš„ï¼Œæ‰€ä»¥åº”è¯¥å°†åŒºé—´å¯¹æ•°åŒ–ï¼Œè½¬åŒ–æˆ <img src=\"https://www.zhihu.com/equation?tex=%5B0.001%2C0.01%5D%2C%5B0.01%2C0.1%5D%2C%5B0.1%2C1%5D\" alt=\"[0.001,0.01],[0.01,0.1],[0.1,1]\" eeimg=\"1\"/> çš„åŒºé—´ï¼Œè¿™æ ·æ›´ä¸ºåˆç†ã€‚æ€è·¯æ˜¯ï¼š <img src=\"https://www.zhihu.com/equation?tex=10%5E%7B-3%7D+%3D+0.001\" alt=\"10^{-3} = 0.001\" eeimg=\"1\"/> ï¼Œæ‰€ä»¥å–å€¼ä» <img src=\"https://www.zhihu.com/equation?tex=%5B10%5E%7B-3%7D%2C10%5E%7B0%7D%5D\" alt=\"[10^{-3},10^{0}]\" eeimg=\"1\"/> ï¼Œæˆ‘ä»¬åªè¦å°†æŒ‡æ•°éšæœºå°±å¯ä»¥äº†ã€‚</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">r</span> <span class=\"o\">=</span> <span class=\"o\">-</span><span class=\"mi\">3</span><span class=\"o\">*</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">()</span> <span class=\"c1\"># rand()è¡¨ç¤ºåœ¨ [0ï¼Œ1]éšæœºå–æ ·ï¼Œå†ä¹˜ä»¥ç³»æ•°ï¼Œå°±å¯ä»¥å¾—åˆ°[-3,0]</span>\n<span class=\"n\">a</span> <span class=\"o\">=</span> <span class=\"mi\">10</span><span class=\"o\">**</span><span class=\"n\">r</span></code></pre></div><p>åŒç†, <img src=\"https://www.zhihu.com/equation?tex=%5Cbeta+%3D+0.9+%2C.....%2C0.999\" alt=\"\\beta = 0.9 ,.....,0.999\" eeimg=\"1\"/> </p><p>é€šè¿‡ <img src=\"https://www.zhihu.com/equation?tex=1-%5Cbeta+%3D+0.1%2C....%2C0.001\" alt=\"1-\\beta = 0.1,....,0.001\" eeimg=\"1\"/> ï¼Œæ‰€ä»¥ <img src=\"https://www.zhihu.com/equation?tex=1-%5Cbeta+%3D+10%5E%7Br%7D%EF%BC%8C%5Cbeta+%3D+1-10%5E%7Br%7D\" alt=\"1-\\beta = 10^{r}ï¼Œ\\beta = 1-10^{r}\" eeimg=\"1\"/> </p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>å½’ä¸€åŒ–ç½‘ç»œçš„æ¿€æ´»å‡½æ•°</b></h2><p>æˆ‘ä»¬ä¹‹å‰æ˜¯å°†è¾“å…¥çš„æ•°æ®Xå½’ä¸€åŒ–ï¼Œå¯ä»¥åŠ é€Ÿè®­ç»ƒï¼Œå…¶å®åœ¨ç¥ç»ç½‘ç»œä¸­ï¼Œä¹Ÿå¯ä»¥åŒæ ·å½’ä¸€åŒ–ï¼Œä¸€èˆ¬æ˜¯å¯¹ <img src=\"https://www.zhihu.com/equation?tex=z%5E%7B%5Bl%5D%7D\" alt=\"z^{[l]}\" eeimg=\"1\"/> å½’ä¸€åŒ–ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><p>è¿™ä¸ªæ–¹æ³•å«åš batch norm</p><p>å…¬å¼æ˜¯ï¼š</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-6ad8dc32758a53e5f147e62299553573_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"268\" data-rawheight=\"166\" class=\"content_image\" width=\"268\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;268&#39; height=&#39;166&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"268\" data-rawheight=\"166\" class=\"content_image lazy\" width=\"268\" data-actualsrc=\"https://pic4.zhimg.com/v2-6ad8dc32758a53e5f147e62299553573_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>åŠ ä¸Š <img src=\"https://www.zhihu.com/equation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"/> æ˜¯ä¸ºäº†ä¸è‡³äºé™¤ä»¥0</p><p class=\"ztext-empty-paragraph\"><br/></p><p>è€Œä¸€èˆ¬æ ‡å‡†åŒ–åè¿˜ä¼šåŠ ä¸Šä¸¤ä¸ªå‚æ•°ï¼Œæ¥è¡¨ç¤ºæ–°çš„æ–¹å·® <img src=\"https://www.zhihu.com/equation?tex=%5Cgamma\" alt=\"\\gamma\" eeimg=\"1\"/> å’Œå‡å€¼ <img src=\"https://www.zhihu.com/equation?tex=%5Cbeta\" alt=\"\\beta\" eeimg=\"1\"/> ï¼š</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-dea9fddb9cdab6406658af5376005434_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"288\" data-rawheight=\"83\" class=\"content_image\" width=\"288\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;288&#39; height=&#39;83&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"288\" data-rawheight=\"83\" class=\"content_image lazy\" width=\"288\" data-actualsrc=\"https://pic1.zhimg.com/v2-dea9fddb9cdab6406658af5376005434_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cgamma\" alt=\"\\gamma\" eeimg=\"1\"/> å’Œ <img src=\"https://www.zhihu.com/equation?tex=%5Cbeta\" alt=\"\\beta\" eeimg=\"1\"/> ä¹Ÿæ˜¯å‚æ•°ï¼Œå’Œ <img src=\"https://www.zhihu.com/equation?tex=w%2Cb\" alt=\"w,b\" eeimg=\"1\"/> ä¸€æ ·ï¼Œå¯ä»¥åœ¨å­¦ä¹ ä¸­è¿›è¡Œæ›´æ–°ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>å°†batch norm æ”¾å…¥ç¥ç»ç½‘ç»œ</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-5acf763fd0fc6ad3dba36bfcadcd5f01_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"709\" data-rawheight=\"390\" class=\"origin_image zh-lightbox-thumb\" width=\"709\" data-original=\"https://pic2.zhimg.com/v2-5acf763fd0fc6ad3dba36bfcadcd5f01_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;709&#39; height=&#39;390&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"709\" data-rawheight=\"390\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"709\" data-original=\"https://pic2.zhimg.com/v2-5acf763fd0fc6ad3dba36bfcadcd5f01_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-5acf763fd0fc6ad3dba36bfcadcd5f01_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>å¯ä»¥çœ‹åˆ°ï¼Œ</p><p>å…ˆæ±‚çš„ <img src=\"https://www.zhihu.com/equation?tex=z%5E%7B%5B1%5D%7D\" alt=\"z^{[1]}\" eeimg=\"1\"/> ï¼Œå†è¿›è¡Œbatch normï¼ŒåŠ ä¸Šå‚æ•° <img src=\"https://www.zhihu.com/equation?tex=%5Cbeta%5E%7B%5B1%5D%7D%2C%5Cgamma%5E%7B%5B1%5D%7D\" alt=\"\\beta^{[1]},\\gamma^{[1]}\" eeimg=\"1\"/> ï¼Œå¾—åˆ° <img src=\"https://www.zhihu.com/equation?tex=%7B%5Ctilde%7Bz%7D%7D%5E%7B%5B1%5D%7D\" alt=\"{\\tilde{z}}^{[1]}\" eeimg=\"1\"/> ,å†æ ¹æ®activation functionå¾—åˆ° <img src=\"https://www.zhihu.com/equation?tex=a%5E%7B%5B1%5D%7D\" alt=\"a^{[1]}\" eeimg=\"1\"/> </p><p>batch normåŒæ ·é€‚ç”¨äºMomentumã€RMSprop ã€Adamçš„æ¢¯åº¦ä¸‹é™æ³•æ¥è¿›è¡Œæ›´æ–°ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>Batch Normä¸ºä»€ä¹ˆæœ‰ç”¨ï¼Ÿ</b></h2><p>å¦‚æœæˆ‘ä»¬çš„å›¾ç‰‡ä¸­è®­ç»ƒçš„éƒ½æ˜¯é»‘çŒ«ï¼Œè¿™ä¸ªæ—¶å€™ç»™ä½ ä¸€äº›æ©˜çŒ«çš„å›¾ç‰‡ï¼Œé‚£ä¹ˆå¤§æ¦‚ç‡æ˜¯è®­ç»ƒä¸å¥½çš„ã€‚å› ä¸ºç›¸å½“äºæ ·æœ¬é›†åˆçš„åˆ†å¸ƒæ”¹å˜äº†ï¼Œbatch normå°±å¯ä»¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-d01efe746cae6c55b85da5888940d3d1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1186\" data-rawheight=\"388\" class=\"origin_image zh-lightbox-thumb\" width=\"1186\" data-original=\"https://pic2.zhimg.com/v2-d01efe746cae6c55b85da5888940d3d1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1186&#39; height=&#39;388&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1186\" data-rawheight=\"388\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1186\" data-original=\"https://pic2.zhimg.com/v2-d01efe746cae6c55b85da5888940d3d1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-d01efe746cae6c55b85da5888940d3d1_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>å¦‚æœè¿™ä¸ªæ—¶å€™è¦è®¡ç®—ç¬¬ä¸‰å±‚ï¼Œé‚£ä¹ˆå¾ˆæ˜¾ç„¶è®¡ç®—ç»“æœæ˜¯ä¾èµ–ç¬¬äºŒå±‚çš„æ•°æ®çš„ã€‚ä½†æ˜¯å¦‚æœæˆ‘ä»¬å¯¹ç¬¬äºŒå±‚çš„æ•°æ®è¿›è¡Œäº†å½’ä¸€åŒ–ï¼Œé‚£ä¹ˆå°±å¯ä»¥å°†ç¬¬äºŒå±‚çš„å‡å€¼å’Œæ–¹å·®éƒ½é™åˆ¶åœ¨åŒä¸€åˆ†å¸ƒï¼Œè€Œä¸”è¿™ä¸¤ä¸ªå‚æ•°æ˜¯è‡ªåŠ¨å­¦ä¹ çš„ã€‚ä¹Ÿå°±æ˜¯å½’ä¸€åŒ–åçš„æ•°æ®å¯ä»¥å‡å¼±å‰å±‚å‚æ•°çš„ä½œç”¨ä¸åå±‚å‚æ•°çš„ä½œç”¨ä¹‹é—´çš„è”ç³»ï¼Œå®ƒä½¿å¾—ç½‘ç»œæ¯å±‚éƒ½å¯ä»¥è‡ªå·±å­¦ä¹ ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><p>è¿˜æœ‰å°±æ˜¯batch normåœ¨æŸç§ç¨‹åº¦ä¸Šæœ‰æ­£åˆ™åŒ–çš„æ•ˆæœï¼Œå› ä¸ºå½’ä¸€åŒ–ä¼šä½¿å„ä¸ªå±‚ä¹‹é—´çš„ä¾èµ–æ€§é™ä½ï¼Œè€Œä¸”å½’ä¸€åŒ–æœ‰å¸¦æ¥ä¸€å®šçš„å™ªå£°ï¼Œæœ‰ç‚¹åƒdropoutã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>æµ‹è¯•é›†çš„batch norm</b></h2><p>batch normæ˜¯åœ¨è®­ç»ƒé›†ä¸Šå¾—åˆ°çš„ï¼Œé‚£ä¹ˆæ€ä¹ˆæŠŠå®ƒåº”ç”¨åœ¨æµ‹è¯•é›†å‘¢ï¼Ÿ</p><p>è¿™ä¸ªæ—¶å€™å¯ä»¥ç›´æ¥ä»è®­ç»ƒé›†ä¸­æ‹¿åˆ° <img src=\"https://www.zhihu.com/equation?tex=%5Cmu\" alt=\"\\mu\" eeimg=\"1\"/> å’Œ <img src=\"https://www.zhihu.com/equation?tex=%5Csigma%5E%7B2%7D\" alt=\"\\sigma^{2}\" eeimg=\"1\"/> </p><p>ä½¿ç”¨æŒ‡æ•°åŠ æƒå¹³å‡ï¼Œåœ¨æ¯ä¸€æ­¥ä¸­ä¿ç•™ <img src=\"https://www.zhihu.com/equation?tex=%5Cmu\" alt=\"\\mu\" eeimg=\"1\"/> å’Œ <img src=\"https://www.zhihu.com/equation?tex=%5Csigma%5E%7B2%7D\" alt=\"\\sigma^{2}\" eeimg=\"1\"/> ï¼Œå°±å¯ä»¥å¾—åˆ°è®­ç»ƒåçš„ <img src=\"https://www.zhihu.com/equation?tex=%5Cmu\" alt=\"\\mu\" eeimg=\"1\"/> å’Œ <img src=\"https://www.zhihu.com/equation?tex=%5Csigma%5E%7B2%7D\" alt=\"\\sigma^{2}\" eeimg=\"1\"/> </p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>softmax</b></h2><p>ä¹‹å‰è¯´çš„éƒ½æ˜¯äºŒåˆ†ç±»é—®é¢˜ï¼Œå¦‚ä½•è§£å†³å¤šåˆ†ç±»é—®é¢˜å‘¢ï¼Ÿ</p><p>å¯ä»¥ç”¨softmaxç®—æ³•æ¥è§£å†³ã€‚</p><p>å‰é¢çš„æ­¥éª¤éƒ½ä¸€æ ·ï¼Œè€Œåˆ°äº†æœ€åä¸€å±‚output layerï¼Œä½ æƒ³è¦åˆ†ä¸ºå¤šå°‘ç±»ï¼Œå°±ç”¨å¤šå°‘ä¸ªç¥ç»å…ƒã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><p>è¿™ä¸ªæ—¶å€™ï¼Œæœ€åä¸€å±‚çš„activation functionå°±å˜æˆäº†ï¼š</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-effa1cd1e4eac864b040ec40338ff06e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"409\" data-rawheight=\"160\" class=\"content_image\" width=\"409\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;409&#39; height=&#39;160&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"409\" data-rawheight=\"160\" class=\"content_image lazy\" width=\"409\" data-actualsrc=\"https://pic3.zhimg.com/v2-effa1cd1e4eac864b040ec40338ff06e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><img src=\"https://www.zhihu.com/equation?tex=a%5E%7B%5Bl%5D%7D_i\" alt=\"a^{[l]}_i\" eeimg=\"1\"/> å°±è¡¨ç¤ºäº†æ¯ä¸€ä¸ªåˆ†ç±»çš„æ¦‚ç‡ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><p>è®¡ç®—ä¾‹å­å¦‚å›¾ï¼š</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-8e024a11adff99f08dd36dc43a354a2e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"491\" data-rawheight=\"280\" class=\"origin_image zh-lightbox-thumb\" width=\"491\" data-original=\"https://pic3.zhimg.com/v2-8e024a11adff99f08dd36dc43a354a2e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;491&#39; height=&#39;280&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"491\" data-rawheight=\"280\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"491\" data-original=\"https://pic3.zhimg.com/v2-8e024a11adff99f08dd36dc43a354a2e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-8e024a11adff99f08dd36dc43a354a2e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>è€Œå®ƒçš„æŸå¤±å‡½æ•°ç”¨çš„ä¹Ÿæ˜¯cross-entropyï¼š</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-4c2e090c513660dbe9d9bad1cbe94c70_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"801\" data-rawheight=\"273\" class=\"origin_image zh-lightbox-thumb\" width=\"801\" data-original=\"https://pic1.zhimg.com/v2-4c2e090c513660dbe9d9bad1cbe94c70_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;801&#39; height=&#39;273&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"801\" data-rawheight=\"273\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"801\" data-original=\"https://pic1.zhimg.com/v2-4c2e090c513660dbe9d9bad1cbe94c70_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-4c2e090c513660dbe9d9bad1cbe94c70_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>æœ€ç»ˆå¾—åˆ°ä¸€ä¸ªå…³äºYçš„çŸ©é˜µï¼š</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-8ddc6dc74a27ea3b521965674eed0968_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"652\" data-rawheight=\"165\" class=\"origin_image zh-lightbox-thumb\" width=\"652\" data-original=\"https://pic1.zhimg.com/v2-8ddc6dc74a27ea3b521965674eed0968_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;652&#39; height=&#39;165&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"652\" data-rawheight=\"165\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"652\" data-original=\"https://pic1.zhimg.com/v2-8ddc6dc74a27ea3b521965674eed0968_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-8ddc6dc74a27ea3b521965674eed0968_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>å…¶å®æ˜¯å¯ä»¥è¯æ˜ï¼Œå½“åˆ†ç±»ä¸º2æ—¶ï¼Œsoftmaxå°±æ˜¯logistic regression</p><p></p><p></p><p></p><p></p><p></p><p></p>", 
            "topic": [
                {
                    "tag": "ç¬”è®°", 
                    "tagLink": "https://api.zhihu.com/topics/19554982"
                }, 
                {
                    "tag": "ç¥ç»ç½‘ç»œ", 
                    "tagLink": "https://api.zhihu.com/topics/19607065"
                }, 
                {
                    "tag": "æœºå™¨å­¦ä¹ ", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/44718731", 
            "userName": "ç›´ä¸Šäº‘éœ„", 
            "userLink": "https://www.zhihu.com/people/1033165ce4ad9c3fce69a0793dfab8ad", 
            "upvote": 0, 
            "title": "DeepLearning.aiä½œä¸š:(2-2)-ä¼˜åŒ–ç®—æ³•", 
            "content": "<h2>é¦–å‘äºä¸ªäººåšå®¢ï¼š<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">fangzh.top</span><span class=\"invisible\"></span></a>ï¼Œæ¬¢è¿æ¥è®¿</h2><ol><li>ä¸è¦æŠ„ä½œä¸šï¼</li><li>æˆ‘åªæ˜¯æŠŠæ€è·¯æ•´ç†äº†ï¼Œä¾›ä¸ªäººå­¦ä¹ ã€‚</li><li>ä¸è¦æŠ„ä½œä¸šï¼</li></ol><p>æœ¬å‘¨ä½œä¸šå®è·µäº†è¯¾ä¸Šçš„å„ç§ä¼˜åŒ–ç®—æ³•ï¼š</p><ul><li>mini-batch</li><li>momentum</li><li>Adam</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p>é¦–å…ˆæ˜¯æ ‡å‡†çš„gradient descentï¼š</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">update_parameters_with_gd</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">grads</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Update parameters using one step of gradient descent\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    parameters -- python dictionary containing your parameters to be updated:\n</span><span class=\"s2\">                    parameters[&#39;W&#39; + str(l)] = Wl\n</span><span class=\"s2\">                    parameters[&#39;b&#39; + str(l)] = bl\n</span><span class=\"s2\">    grads -- python dictionary containing your gradients to update each parameters:\n</span><span class=\"s2\">                    grads[&#39;dW&#39; + str(l)] = dWl\n</span><span class=\"s2\">                    grads[&#39;db&#39; + str(l)] = dbl\n</span><span class=\"s2\">    learning_rate -- the learning rate, scalar.\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    parameters -- python dictionary containing your updated parameters \n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n<span class=\"err\">â€‹</span>\n    <span class=\"n\">L</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">)</span> <span class=\"o\">//</span> <span class=\"mi\">2</span> <span class=\"c1\"># number of layers in the neural networks</span>\n<span class=\"err\">â€‹</span>\n    <span class=\"c1\"># Update rule for each parameter</span>\n    <span class=\"k\">for</span> <span class=\"n\">l</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">L</span><span class=\"p\">):</span>\n        <span class=\"c1\">### START CODE HERE ### (approx. 2 lines)</span>\n        <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;W&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span>  <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;W&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">-</span> <span class=\"n\">learning_rate</span> <span class=\"o\">*</span> <span class=\"n\">grads</span><span class=\"p\">[</span><span class=\"s1\">&#39;dW&#39;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span>\n        <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;b&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;b&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">-</span> <span class=\"n\">learning_rate</span> <span class=\"o\">*</span> <span class=\"n\">grads</span><span class=\"p\">[</span><span class=\"s1\">&#39;db&#39;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n        \n    <span class=\"k\">return</span> <span class=\"n\">parameters</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>mini-batch</b></h2><p>æ­¥éª¤æ˜¯ï¼š</p><ul><li>shuffleï¼šå°†æ•°æ®éšæœºæ‰“ä¹±ï¼Œä½¿ç”¨<code>np.random.permutation(m)</code>å‡½æ•°å¯ä»¥æŠŠmä¸ªæ ·æœ¬çš„é¡ºåºé‡æ–°æ˜ å°„ï¼Œå˜æˆä¸€ä¸ªlenä¸ºmçš„åˆ—è¡¨ï¼Œé‡Œé¢çš„å€¼å°±æ˜¯æ˜ å°„åŸæœ¬çš„é¡ºåºã€‚</li><li>å†æ ¹æ®sizeå¤§å°è¿›è¡Œåˆ†åŒºï¼Œéœ€è¦æ³¨æ„çš„æ˜¯æœ€åçš„æ•°æ®æœ‰å¯èƒ½å°äºsizeå¤§å°çš„ï¼Œå› ä¸ºå¯èƒ½æ— æ³•æ•´é™¤ï¼Œè¦å•ç‹¬è€ƒè™‘</li></ul><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: random_mini_batches</span>\n<span class=\"err\">â€‹</span>\n<span class=\"k\">def</span> <span class=\"nf\">random_mini_batches</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">mini_batch_size</span> <span class=\"o\">=</span> <span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"n\">seed</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Creates a list of random minibatches from (X, Y)\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    X -- input data, of shape (input size, number of examples)\n</span><span class=\"s2\">    Y -- true &#34;label&#34; vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n</span><span class=\"s2\">    mini_batch_size -- size of the mini-batches, integer\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">seed</span><span class=\"p\">(</span><span class=\"n\">seed</span><span class=\"p\">)</span>            <span class=\"c1\"># To make your &#34;random&#34; minibatches the same as ours</span>\n    <span class=\"n\">m</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span>                  <span class=\"c1\"># number of training examples</span>\n    <span class=\"n\">mini_batches</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n        \n    <span class=\"c1\"># Step 1: Shuffle (X, Y)</span>\n    <span class=\"n\">permutation</span> <span class=\"o\">=</span> <span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">permutation</span><span class=\"p\">(</span><span class=\"n\">m</span><span class=\"p\">))</span>\n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">permutation</span><span class=\"p\">)</span>\n    <span class=\"n\">shuffled_X</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"p\">[:,</span> <span class=\"n\">permutation</span><span class=\"p\">]</span>\n    <span class=\"n\">shuffled_Y</span> <span class=\"o\">=</span> <span class=\"n\">Y</span><span class=\"p\">[:,</span> <span class=\"n\">permutation</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">((</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"n\">m</span><span class=\"p\">))</span>\n<span class=\"err\">â€‹</span>\n    <span class=\"c1\"># Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.</span>\n    <span class=\"n\">num_complete_minibatches</span> <span class=\"o\">=</span> <span class=\"n\">math</span><span class=\"o\">.</span><span class=\"n\">floor</span><span class=\"p\">(</span><span class=\"n\">m</span><span class=\"o\">/</span><span class=\"n\">mini_batch_size</span><span class=\"p\">)</span> <span class=\"c1\"># number of mini batches of size mini_batch_size in your partitionning</span>\n    <span class=\"k\">for</span> <span class=\"n\">k</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">num_complete_minibatches</span><span class=\"p\">):</span>\n        <span class=\"c1\">### START CODE HERE ### (approx. 2 lines)</span>\n        <span class=\"n\">mini_batch_X</span> <span class=\"o\">=</span> <span class=\"n\">shuffled_X</span><span class=\"p\">[:,</span><span class=\"n\">k</span> <span class=\"o\">*</span> <span class=\"n\">mini_batch_size</span><span class=\"p\">:(</span><span class=\"n\">k</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)</span><span class=\"o\">*</span> <span class=\"n\">mini_batch_size</span><span class=\"p\">]</span>\n        <span class=\"n\">mini_batch_Y</span> <span class=\"o\">=</span> <span class=\"n\">shuffled_Y</span><span class=\"p\">[:,</span><span class=\"n\">k</span> <span class=\"o\">*</span> <span class=\"n\">mini_batch_size</span><span class=\"p\">:(</span><span class=\"n\">k</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)</span><span class=\"o\">*</span> <span class=\"n\">mini_batch_size</span><span class=\"p\">]</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n        <span class=\"n\">mini_batch</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">mini_batch_X</span><span class=\"p\">,</span> <span class=\"n\">mini_batch_Y</span><span class=\"p\">)</span>\n        <span class=\"n\">mini_batches</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">mini_batch</span><span class=\"p\">)</span>\n    \n    <span class=\"c1\"># Handling the end case (last mini-batch &lt; mini_batch_size)</span>\n    <span class=\"k\">if</span> <span class=\"n\">m</span> <span class=\"o\">%</span> <span class=\"n\">mini_batch_size</span> <span class=\"o\">!=</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n        <span class=\"c1\">### START CODE HERE ### (approx. 2 lines)</span>\n        <span class=\"n\">mini_batch_X</span> <span class=\"o\">=</span> <span class=\"n\">shuffled_X</span><span class=\"p\">[:,</span><span class=\"n\">num_complete_minibatches</span> <span class=\"o\">*</span> <span class=\"n\">mini_batch_size</span><span class=\"p\">:]</span>\n        <span class=\"n\">mini_batch_Y</span> <span class=\"o\">=</span> <span class=\"n\">shuffled_Y</span><span class=\"p\">[:,</span><span class=\"n\">num_complete_minibatches</span> <span class=\"o\">*</span> <span class=\"n\">mini_batch_size</span><span class=\"p\">:]</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n        <span class=\"n\">mini_batch</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">mini_batch_X</span><span class=\"p\">,</span> <span class=\"n\">mini_batch_Y</span><span class=\"p\">)</span>\n        <span class=\"n\">mini_batches</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">mini_batch</span><span class=\"p\">)</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">mini_batches</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>Momentum</b></h2><p>å…ˆåˆå§‹åŒ–ä¸º0ï¼Œ</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: initialize_velocity</span>\n<span class=\"err\">â€‹</span>\n<span class=\"k\">def</span> <span class=\"nf\">initialize_velocity</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Initializes the velocity as a python dictionary with:\n</span><span class=\"s2\">                - keys: &#34;dW1&#34;, &#34;db1&#34;, ..., &#34;dWL&#34;, &#34;dbL&#34; \n</span><span class=\"s2\">                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    parameters -- python dictionary containing your parameters.\n</span><span class=\"s2\">                    parameters[&#39;W&#39; + str(l)] = Wl\n</span><span class=\"s2\">                    parameters[&#39;b&#39; + str(l)] = bl\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    v -- python dictionary containing the current velocity.\n</span><span class=\"s2\">                    v[&#39;dW&#39; + str(l)] = velocity of dWl\n</span><span class=\"s2\">                    v[&#39;db&#39; + str(l)] = velocity of dbl\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"n\">L</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">)</span> <span class=\"o\">//</span> <span class=\"mi\">2</span> <span class=\"c1\"># number of layers in the neural networks</span>\n    <span class=\"n\">v</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>\n    \n    <span class=\"c1\"># Initialize velocity</span>\n    <span class=\"k\">for</span> <span class=\"n\">l</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">L</span><span class=\"p\">):</span>\n        <span class=\"c1\">### START CODE HERE ### (approx. 2 lines)</span>\n        <span class=\"n\">v</span><span class=\"p\">[</span><span class=\"s2\">&#34;dW&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;W&#39;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span><span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;W&#39;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]))</span>\n        <span class=\"n\">v</span><span class=\"p\">[</span><span class=\"s2\">&#34;db&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;b&#39;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span><span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;b&#39;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]))</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n        \n    <span class=\"k\">return</span> <span class=\"n\">v</span></code></pre></div><p>å†æŒ‰å…¬å¼è¿›è¡Œè¿­ä»£ï¼Œå› ä¸ºæŒ‡æ•°åŠ æƒå¹³å‡ä¸éœ€è¦çŸ¥é“å‰é¢nä¸ªæ•°æ®ï¼Œåªè¦ä¸€æ­¥ä¸€æ­¥è¿›è¡Œè¿­ä»£ï¼ŒçŸ¥é“å½“å‰çš„æ•°æ®å°±è¡Œï¼ŒèŠ‚çœç©ºé—´ã€‚</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: update_parameters_with_momentum</span>\n<span class=\"err\">â€‹</span>\n<span class=\"k\">def</span> <span class=\"nf\">update_parameters_with_momentum</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">grads</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">,</span> <span class=\"n\">beta</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Update parameters using Momentum\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    parameters -- python dictionary containing your parameters:\n</span><span class=\"s2\">                    parameters[&#39;W&#39; + str(l)] = Wl\n</span><span class=\"s2\">                    parameters[&#39;b&#39; + str(l)] = bl\n</span><span class=\"s2\">    grads -- python dictionary containing your gradients for each parameters:\n</span><span class=\"s2\">                    grads[&#39;dW&#39; + str(l)] = dWl\n</span><span class=\"s2\">                    grads[&#39;db&#39; + str(l)] = dbl\n</span><span class=\"s2\">    v -- python dictionary containing the current velocity:\n</span><span class=\"s2\">                    v[&#39;dW&#39; + str(l)] = ...\n</span><span class=\"s2\">                    v[&#39;db&#39; + str(l)] = ...\n</span><span class=\"s2\">    beta -- the momentum hyperparameter, scalar\n</span><span class=\"s2\">    learning_rate -- the learning rate, scalar\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    parameters -- python dictionary containing your updated parameters \n</span><span class=\"s2\">    v -- python dictionary containing your updated velocities\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n<span class=\"err\">â€‹</span>\n    <span class=\"n\">L</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">)</span> <span class=\"o\">//</span> <span class=\"mi\">2</span> <span class=\"c1\"># number of layers in the neural networks</span>\n    \n    <span class=\"c1\"># Momentum update for each parameter</span>\n    <span class=\"k\">for</span> <span class=\"n\">l</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">L</span><span class=\"p\">):</span>\n        \n        <span class=\"c1\">### START CODE HERE ### (approx. 4 lines)</span>\n        <span class=\"c1\"># compute velocities</span>\n        <span class=\"n\">v</span><span class=\"p\">[</span><span class=\"s2\">&#34;dW&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">beta</span> <span class=\"o\">*</span> <span class=\"n\">v</span><span class=\"p\">[</span><span class=\"s2\">&#34;dW&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"mi\">1</span> <span class=\"o\">-</span> <span class=\"n\">beta</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">grads</span><span class=\"p\">[</span><span class=\"s2\">&#34;dW&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span>\n        <span class=\"n\">v</span><span class=\"p\">[</span><span class=\"s2\">&#34;db&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">beta</span> <span class=\"o\">*</span> <span class=\"n\">v</span><span class=\"p\">[</span><span class=\"s2\">&#34;db&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"mi\">1</span> <span class=\"o\">-</span> <span class=\"n\">beta</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">grads</span><span class=\"p\">[</span><span class=\"s2\">&#34;db&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span>\n        <span class=\"c1\"># update parameters</span>\n        <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;W&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;W&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">-</span> <span class=\"n\">learning_rate</span> <span class=\"o\">*</span> <span class=\"n\">v</span><span class=\"p\">[</span><span class=\"s2\">&#34;dW&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span>\n        <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;b&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;b&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">-</span> <span class=\"n\">learning_rate</span> <span class=\"o\">*</span> <span class=\"n\">v</span><span class=\"p\">[</span><span class=\"s2\">&#34;dW&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n        \n    <span class=\"k\">return</span> <span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">v</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>Adam</b></h2><p>æ²¡ä»€ä¹ˆå¥½è¯´çš„ï¼Œå…ˆåˆå§‹åŒ–ï¼Œæ ¹æ®å…¬å¼æ¥å°±è¡Œäº†ã€‚</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">initialize_adam</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">)</span> <span class=\"p\">:</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Initializes v and s as two python dictionaries with:\n</span><span class=\"s2\">                - keys: &#34;dW1&#34;, &#34;db1&#34;, ..., &#34;dWL&#34;, &#34;dbL&#34; \n</span><span class=\"s2\">                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    parameters -- python dictionary containing your parameters.\n</span><span class=\"s2\">                    parameters[&#34;W&#34; + str(l)] = Wl\n</span><span class=\"s2\">                    parameters[&#34;b&#34; + str(l)] = bl\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns: \n</span><span class=\"s2\">    v -- python dictionary that will contain the exponentially weighted average of the gradient.\n</span><span class=\"s2\">                    v[&#34;dW&#34; + str(l)] = ...\n</span><span class=\"s2\">                    v[&#34;db&#34; + str(l)] = ...\n</span><span class=\"s2\">    s -- python dictionary that will contain the exponentially weighted average of the squared gradient.\n</span><span class=\"s2\">                    s[&#34;dW&#34; + str(l)] = ...\n</span><span class=\"s2\">                    s[&#34;db&#34; + str(l)] = ...\n</span><span class=\"s2\">â€‹\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"n\">L</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">)</span> <span class=\"o\">//</span> <span class=\"mi\">2</span> <span class=\"c1\"># number of layers in the neural networks</span>\n    <span class=\"n\">v</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>\n    <span class=\"n\">s</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>\n    \n    <span class=\"c1\"># Initialize v, s. Input: &#34;parameters&#34;. Outputs: &#34;v, s&#34;.</span>\n    <span class=\"k\">for</span> <span class=\"n\">l</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">L</span><span class=\"p\">):</span>\n    <span class=\"c1\">### START CODE HERE ### (approx. 4 lines)</span>\n        <span class=\"n\">v</span><span class=\"p\">[</span><span class=\"s2\">&#34;dW&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;W&#39;</span><span class=\"o\">+</span><span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span><span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;W&#39;</span><span class=\"o\">+</span><span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]))</span>\n        <span class=\"n\">v</span><span class=\"p\">[</span><span class=\"s2\">&#34;db&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;b&#39;</span><span class=\"o\">+</span><span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span><span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;b&#39;</span><span class=\"o\">+</span><span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]))</span>\n        <span class=\"n\">s</span><span class=\"p\">[</span><span class=\"s2\">&#34;dW&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;W&#39;</span><span class=\"o\">+</span><span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span><span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;W&#39;</span><span class=\"o\">+</span><span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]))</span>\n        <span class=\"n\">s</span><span class=\"p\">[</span><span class=\"s2\">&#34;db&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;b&#39;</span><span class=\"o\">+</span><span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span><span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;b&#39;</span><span class=\"o\">+</span><span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]))</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">v</span><span class=\"p\">,</span> <span class=\"n\">s</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">update_parameters_with_adam</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">grads</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">,</span> <span class=\"n\">s</span><span class=\"p\">,</span> <span class=\"n\">t</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span> <span class=\"o\">=</span> <span class=\"mf\">0.01</span><span class=\"p\">,</span>\n                                <span class=\"n\">beta1</span> <span class=\"o\">=</span> <span class=\"mf\">0.9</span><span class=\"p\">,</span> <span class=\"n\">beta2</span> <span class=\"o\">=</span> <span class=\"mf\">0.999</span><span class=\"p\">,</span>  <span class=\"n\">epsilon</span> <span class=\"o\">=</span> <span class=\"mf\">1e-8</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Update parameters using Adam\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    parameters -- python dictionary containing your parameters:\n</span><span class=\"s2\">                    parameters[&#39;W&#39; + str(l)] = Wl\n</span><span class=\"s2\">                    parameters[&#39;b&#39; + str(l)] = bl\n</span><span class=\"s2\">    grads -- python dictionary containing your gradients for each parameters:\n</span><span class=\"s2\">                    grads[&#39;dW&#39; + str(l)] = dWl\n</span><span class=\"s2\">                    grads[&#39;db&#39; + str(l)] = dbl\n</span><span class=\"s2\">    v -- Adam variable, moving average of the first gradient, python dictionary\n</span><span class=\"s2\">    s -- Adam variable, moving average of the squared gradient, python dictionary\n</span><span class=\"s2\">    learning_rate -- the learning rate, scalar.\n</span><span class=\"s2\">    beta1 -- Exponential decay hyperparameter for the first moment estimates \n</span><span class=\"s2\">    beta2 -- Exponential decay hyperparameter for the second moment estimates \n</span><span class=\"s2\">    epsilon -- hyperparameter preventing division by zero in Adam updates\n</span><span class=\"s2\">â€‹\n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    parameters -- python dictionary containing your updated parameters \n</span><span class=\"s2\">    v -- Adam variable, moving average of the first gradient, python dictionary\n</span><span class=\"s2\">    s -- Adam variable, moving average of the squared gradient, python dictionary\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"n\">L</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">)</span> <span class=\"o\">//</span> <span class=\"mi\">2</span>                 <span class=\"c1\"># number of layers in the neural networks</span>\n    <span class=\"n\">v_corrected</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>                         <span class=\"c1\"># Initializing first moment estimate, python dictionary</span>\n    <span class=\"n\">s_corrected</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>                         <span class=\"c1\"># Initializing second moment estimate, python dictionary</span>\n    \n    <span class=\"c1\"># Perform Adam update on all parameters</span>\n    <span class=\"k\">for</span> <span class=\"n\">l</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">L</span><span class=\"p\">):</span>\n        <span class=\"c1\"># Moving average of the gradients. Inputs: &#34;v, grads, beta1&#34;. Output: &#34;v&#34;.</span>\n        <span class=\"c1\">### START CODE HERE ### (approx. 2 lines)</span>\n        <span class=\"n\">v</span><span class=\"p\">[</span><span class=\"s2\">&#34;dW&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">beta1</span> <span class=\"o\">*</span> <span class=\"n\">v</span><span class=\"p\">[</span><span class=\"s2\">&#34;dW&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"o\">-</span><span class=\"n\">beta1</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">grads</span><span class=\"p\">[</span><span class=\"s1\">&#39;dW&#39;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span>\n        <span class=\"n\">v</span><span class=\"p\">[</span><span class=\"s2\">&#34;db&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">beta1</span> <span class=\"o\">*</span> <span class=\"n\">v</span><span class=\"p\">[</span><span class=\"s2\">&#34;db&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"o\">-</span><span class=\"n\">beta1</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">grads</span><span class=\"p\">[</span><span class=\"s1\">&#39;db&#39;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n<span class=\"err\">â€‹</span>\n        <span class=\"c1\"># Compute bias-corrected first moment estimate. Inputs: &#34;v, beta1, t&#34;. Output: &#34;v_corrected&#34;.</span>\n        <span class=\"c1\">### START CODE HERE ### (approx. 2 lines)</span>\n        <span class=\"n\">v_corrected</span><span class=\"p\">[</span><span class=\"s2\">&#34;dW&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">v</span><span class=\"p\">[</span><span class=\"s2\">&#34;dW&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"mi\">1</span> <span class=\"o\">-</span> <span class=\"n\">beta1</span> <span class=\"o\">**</span> <span class=\"n\">t</span><span class=\"p\">)</span>\n        <span class=\"n\">v_corrected</span><span class=\"p\">[</span><span class=\"s2\">&#34;db&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">v</span><span class=\"p\">[</span><span class=\"s2\">&#34;db&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"mi\">1</span> <span class=\"o\">-</span> <span class=\"n\">beta1</span> <span class=\"o\">**</span> <span class=\"n\">t</span><span class=\"p\">)</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n<span class=\"err\">â€‹</span>\n        <span class=\"c1\"># Moving average of the squared gradients. Inputs: &#34;s, grads, beta2&#34;. Output: &#34;s&#34;.</span>\n        <span class=\"c1\">### START CODE HERE ### (approx. 2 lines)</span>\n        <span class=\"n\">s</span><span class=\"p\">[</span><span class=\"s2\">&#34;dW&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">beta2</span> <span class=\"o\">*</span> <span class=\"n\">s</span><span class=\"p\">[</span><span class=\"s2\">&#34;dW&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"o\">-</span><span class=\"n\">beta2</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"n\">grads</span><span class=\"p\">[</span><span class=\"s1\">&#39;dW&#39;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span><span class=\"o\">**</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n        <span class=\"n\">s</span><span class=\"p\">[</span><span class=\"s2\">&#34;db&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">beta2</span> <span class=\"o\">*</span> <span class=\"n\">s</span><span class=\"p\">[</span><span class=\"s2\">&#34;db&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"o\">-</span><span class=\"n\">beta2</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"n\">grads</span><span class=\"p\">[</span><span class=\"s1\">&#39;db&#39;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span><span class=\"o\">**</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n<span class=\"err\">â€‹</span>\n        <span class=\"c1\"># Compute bias-corrected second raw moment estimate. Inputs: &#34;s, beta2, t&#34;. Output: &#34;s_corrected&#34;.</span>\n        <span class=\"c1\">### START CODE HERE ### (approx. 2 lines)</span>\n        <span class=\"n\">s_corrected</span><span class=\"p\">[</span><span class=\"s2\">&#34;dW&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">s</span><span class=\"p\">[</span><span class=\"s2\">&#34;dW&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"mi\">1</span> <span class=\"o\">-</span> <span class=\"n\">beta2</span> <span class=\"o\">**</span> <span class=\"n\">t</span><span class=\"p\">)</span>\n        <span class=\"n\">s_corrected</span><span class=\"p\">[</span><span class=\"s2\">&#34;db&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">s</span><span class=\"p\">[</span><span class=\"s2\">&#34;db&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"mi\">1</span> <span class=\"o\">-</span> <span class=\"n\">beta2</span> <span class=\"o\">**</span> <span class=\"n\">t</span><span class=\"p\">)</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n<span class=\"err\">â€‹</span>\n        <span class=\"c1\"># Update parameters. Inputs: &#34;parameters, learning_rate, v_corrected, s_corrected, epsilon&#34;. Output: &#34;parameters&#34;.</span>\n        <span class=\"c1\">### START CODE HERE ### (approx. 2 lines)</span>\n        <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;W&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;W&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">-</span> <span class=\"n\">learning_rate</span> <span class=\"o\">*</span> <span class=\"n\">v_corrected</span><span class=\"p\">[</span><span class=\"s2\">&#34;dW&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"n\">s_corrected</span><span class=\"p\">[</span><span class=\"s2\">&#34;dW&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span><span class=\"o\">**</span><span class=\"mf\">0.5</span> <span class=\"o\">+</span> <span class=\"n\">epsilon</span><span class=\"p\">)</span>\n        <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;b&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;W&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">-</span> <span class=\"n\">learning_rate</span> <span class=\"o\">*</span> <span class=\"n\">v_corrected</span><span class=\"p\">[</span><span class=\"s2\">&#34;db&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"n\">s_corrected</span><span class=\"p\">[</span><span class=\"s2\">&#34;db&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span><span class=\"o\">**</span><span class=\"mf\">0.5</span> <span class=\"o\">+</span> <span class=\"n\">epsilon</span><span class=\"p\">)</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n<span class=\"err\">â€‹</span>\n    <span class=\"k\">return</span> <span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">,</span> <span class=\"n\">s</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>æœ€åä»£å…¥æ¨¡å‹å‡½æ•°ï¼Œæ ¹æ®å…³é”®å­—é€‰æ‹©éœ€è¦çš„ä¼˜åŒ–ç®—æ³•å°±è¡Œäº†ã€‚</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">model</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">layers_dims</span><span class=\"p\">,</span> <span class=\"n\">optimizer</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span> <span class=\"o\">=</span> <span class=\"mf\">0.0007</span><span class=\"p\">,</span> <span class=\"n\">mini_batch_size</span> <span class=\"o\">=</span> <span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"n\">beta</span> <span class=\"o\">=</span> <span class=\"mf\">0.9</span><span class=\"p\">,</span>\n          <span class=\"n\">beta1</span> <span class=\"o\">=</span> <span class=\"mf\">0.9</span><span class=\"p\">,</span> <span class=\"n\">beta2</span> <span class=\"o\">=</span> <span class=\"mf\">0.999</span><span class=\"p\">,</span>  <span class=\"n\">epsilon</span> <span class=\"o\">=</span> <span class=\"mf\">1e-8</span><span class=\"p\">,</span> <span class=\"n\">num_epochs</span> <span class=\"o\">=</span> <span class=\"mi\">10000</span><span class=\"p\">,</span> <span class=\"n\">print_cost</span> <span class=\"o\">=</span> <span class=\"bp\">True</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    3-layer neural network model which can be run in different optimizer modes.\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    X -- input data, of shape (2, number of examples)\n</span><span class=\"s2\">    Y -- true &#34;label&#34; vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n</span><span class=\"s2\">    layers_dims -- python list, containing the size of each layer\n</span><span class=\"s2\">    learning_rate -- the learning rate, scalar.\n</span><span class=\"s2\">    mini_batch_size -- the size of a mini batch\n</span><span class=\"s2\">    beta -- Momentum hyperparameter\n</span><span class=\"s2\">    beta1 -- Exponential decay hyperparameter for the past gradients estimates \n</span><span class=\"s2\">    beta2 -- Exponential decay hyperparameter for the past squared gradients estimates \n</span><span class=\"s2\">    epsilon -- hyperparameter preventing division by zero in Adam updates\n</span><span class=\"s2\">    num_epochs -- number of epochs\n</span><span class=\"s2\">    print_cost -- True to print the cost every 1000 epochs\n</span><span class=\"s2\">â€‹\n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    parameters -- python dictionary containing your updated parameters \n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n<span class=\"err\">â€‹</span>\n    <span class=\"n\">L</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">layers_dims</span><span class=\"p\">)</span>             <span class=\"c1\"># number of layers in the neural networks</span>\n    <span class=\"n\">costs</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>                       <span class=\"c1\"># to keep track of the cost</span>\n    <span class=\"n\">t</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>                            <span class=\"c1\"># initializing the counter required for Adam update</span>\n    <span class=\"n\">seed</span> <span class=\"o\">=</span> <span class=\"mi\">10</span>                        <span class=\"c1\"># For grading purposes, so that your &#34;random&#34; minibatches are the same as ours</span>\n    \n    <span class=\"c1\"># Initialize parameters</span>\n    <span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"n\">initialize_parameters</span><span class=\"p\">(</span><span class=\"n\">layers_dims</span><span class=\"p\">)</span>\n<span class=\"err\">â€‹</span>\n    <span class=\"c1\"># Initialize the optimizer</span>\n    <span class=\"k\">if</span> <span class=\"n\">optimizer</span> <span class=\"o\">==</span> <span class=\"s2\">&#34;gd&#34;</span><span class=\"p\">:</span>\n        <span class=\"k\">pass</span> <span class=\"c1\"># no initialization required for gradient descent</span>\n    <span class=\"k\">elif</span> <span class=\"n\">optimizer</span> <span class=\"o\">==</span> <span class=\"s2\">&#34;momentum&#34;</span><span class=\"p\">:</span>\n        <span class=\"n\">v</span> <span class=\"o\">=</span> <span class=\"n\">initialize_velocity</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">)</span>\n    <span class=\"k\">elif</span> <span class=\"n\">optimizer</span> <span class=\"o\">==</span> <span class=\"s2\">&#34;adam&#34;</span><span class=\"p\">:</span>\n        <span class=\"n\">v</span><span class=\"p\">,</span> <span class=\"n\">s</span> <span class=\"o\">=</span> <span class=\"n\">initialize_adam</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">)</span>\n    \n    <span class=\"c1\"># Optimization loop</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">num_epochs</span><span class=\"p\">):</span>\n        \n        <span class=\"c1\"># Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch</span>\n        <span class=\"n\">seed</span> <span class=\"o\">=</span> <span class=\"n\">seed</span> <span class=\"o\">+</span> <span class=\"mi\">1</span>\n        <span class=\"n\">minibatches</span> <span class=\"o\">=</span> <span class=\"n\">random_mini_batches</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">mini_batch_size</span><span class=\"p\">,</span> <span class=\"n\">seed</span><span class=\"p\">)</span>\n<span class=\"err\">â€‹</span>\n        <span class=\"k\">for</span> <span class=\"n\">minibatch</span> <span class=\"ow\">in</span> <span class=\"n\">minibatches</span><span class=\"p\">:</span>\n<span class=\"err\">â€‹</span>\n            <span class=\"c1\"># Select a minibatch</span>\n            <span class=\"p\">(</span><span class=\"n\">minibatch_X</span><span class=\"p\">,</span> <span class=\"n\">minibatch_Y</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">minibatch</span>\n<span class=\"err\">â€‹</span>\n            <span class=\"c1\"># Forward propagation</span>\n            <span class=\"n\">a3</span><span class=\"p\">,</span> <span class=\"n\">caches</span> <span class=\"o\">=</span> <span class=\"n\">forward_propagation</span><span class=\"p\">(</span><span class=\"n\">minibatch_X</span><span class=\"p\">,</span> <span class=\"n\">parameters</span><span class=\"p\">)</span>\n<span class=\"err\">â€‹</span>\n            <span class=\"c1\"># Compute cost</span>\n            <span class=\"n\">cost</span> <span class=\"o\">=</span> <span class=\"n\">compute_cost</span><span class=\"p\">(</span><span class=\"n\">a3</span><span class=\"p\">,</span> <span class=\"n\">minibatch_Y</span><span class=\"p\">)</span>\n<span class=\"err\">â€‹</span>\n            <span class=\"c1\"># Backward propagation</span>\n            <span class=\"n\">grads</span> <span class=\"o\">=</span> <span class=\"n\">backward_propagation</span><span class=\"p\">(</span><span class=\"n\">minibatch_X</span><span class=\"p\">,</span> <span class=\"n\">minibatch_Y</span><span class=\"p\">,</span> <span class=\"n\">caches</span><span class=\"p\">)</span>\n<span class=\"err\">â€‹</span>\n            <span class=\"c1\"># Update parameters</span>\n            <span class=\"k\">if</span> <span class=\"n\">optimizer</span> <span class=\"o\">==</span> <span class=\"s2\">&#34;gd&#34;</span><span class=\"p\">:</span>\n                <span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"n\">update_parameters_with_gd</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">grads</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span><span class=\"p\">)</span>\n            <span class=\"k\">elif</span> <span class=\"n\">optimizer</span> <span class=\"o\">==</span> <span class=\"s2\">&#34;momentum&#34;</span><span class=\"p\">:</span>\n                <span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">v</span> <span class=\"o\">=</span> <span class=\"n\">update_parameters_with_momentum</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">grads</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">,</span> <span class=\"n\">beta</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span><span class=\"p\">)</span>\n            <span class=\"k\">elif</span> <span class=\"n\">optimizer</span> <span class=\"o\">==</span> <span class=\"s2\">&#34;adam&#34;</span><span class=\"p\">:</span>\n                <span class=\"n\">t</span> <span class=\"o\">=</span> <span class=\"n\">t</span> <span class=\"o\">+</span> <span class=\"mi\">1</span> <span class=\"c1\"># Adam counter</span>\n                <span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">,</span> <span class=\"n\">s</span> <span class=\"o\">=</span> <span class=\"n\">update_parameters_with_adam</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">grads</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">,</span> <span class=\"n\">s</span><span class=\"p\">,</span>\n                                                               <span class=\"n\">t</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span><span class=\"p\">,</span> <span class=\"n\">beta1</span><span class=\"p\">,</span> <span class=\"n\">beta2</span><span class=\"p\">,</span>  <span class=\"n\">epsilon</span><span class=\"p\">)</span>\n        \n        <span class=\"c1\"># Print the cost every 1000 epoch</span>\n        <span class=\"k\">if</span> <span class=\"n\">print_cost</span> <span class=\"ow\">and</span> <span class=\"n\">i</span> <span class=\"o\">%</span> <span class=\"mi\">1000</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n            <span class=\"k\">print</span> <span class=\"p\">(</span><span class=\"s2\">&#34;Cost after epoch </span><span class=\"si\">%i</span><span class=\"s2\">: </span><span class=\"si\">%f</span><span class=\"s2\">&#34;</span> <span class=\"o\">%</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">cost</span><span class=\"p\">))</span>\n        <span class=\"k\">if</span> <span class=\"n\">print_cost</span> <span class=\"ow\">and</span> <span class=\"n\">i</span> <span class=\"o\">%</span> <span class=\"mi\">100</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n            <span class=\"n\">costs</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">cost</span><span class=\"p\">)</span>\n                \n    <span class=\"c1\"># plot the cost</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">costs</span><span class=\"p\">)</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">ylabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;cost&#39;</span><span class=\"p\">)</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">xlabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;epochs (per 100)&#39;</span><span class=\"p\">)</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">title</span><span class=\"p\">(</span><span class=\"s2\">&#34;Learning rate = &#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">learning_rate</span><span class=\"p\">))</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>\n<span class=\"err\">â€‹</span>\n    <span class=\"k\">return</span> <span class=\"n\">parameters</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>æ•ˆæœ</b></h2><p><b>gradient descent</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-37522333a34f0a4a83b79f4b37cdfb33_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"451\" data-rawheight=\"278\" class=\"origin_image zh-lightbox-thumb\" width=\"451\" data-original=\"https://pic4.zhimg.com/v2-37522333a34f0a4a83b79f4b37cdfb33_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;451&#39; height=&#39;278&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"451\" data-rawheight=\"278\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"451\" data-original=\"https://pic4.zhimg.com/v2-37522333a34f0a4a83b79f4b37cdfb33_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-37522333a34f0a4a83b79f4b37cdfb33_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-bac9146c2d6679e8dff5b4bb98232309_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"458\" data-rawheight=\"278\" class=\"origin_image zh-lightbox-thumb\" width=\"458\" data-original=\"https://pic2.zhimg.com/v2-bac9146c2d6679e8dff5b4bb98232309_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;458&#39; height=&#39;278&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"458\" data-rawheight=\"278\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"458\" data-original=\"https://pic2.zhimg.com/v2-bac9146c2d6679e8dff5b4bb98232309_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-bac9146c2d6679e8dff5b4bb98232309_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>gradient descent with momentum</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-4be6330e00bf92624bddbdcd779bc9eb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"451\" data-rawheight=\"278\" class=\"origin_image zh-lightbox-thumb\" width=\"451\" data-original=\"https://pic4.zhimg.com/v2-4be6330e00bf92624bddbdcd779bc9eb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;451&#39; height=&#39;278&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"451\" data-rawheight=\"278\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"451\" data-original=\"https://pic4.zhimg.com/v2-4be6330e00bf92624bddbdcd779bc9eb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-4be6330e00bf92624bddbdcd779bc9eb_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-b567c999d13db08cba6189359d2e25ae_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"458\" data-rawheight=\"278\" class=\"origin_image zh-lightbox-thumb\" width=\"458\" data-original=\"https://pic3.zhimg.com/v2-b567c999d13db08cba6189359d2e25ae_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;458&#39; height=&#39;278&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"458\" data-rawheight=\"278\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"458\" data-original=\"https://pic3.zhimg.com/v2-b567c999d13db08cba6189359d2e25ae_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-b567c999d13db08cba6189359d2e25ae_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>Adam mode</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-806be2bf86839e8694b86c932370de61_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"445\" data-rawheight=\"278\" class=\"origin_image zh-lightbox-thumb\" width=\"445\" data-original=\"https://pic2.zhimg.com/v2-806be2bf86839e8694b86c932370de61_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;445&#39; height=&#39;278&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"445\" data-rawheight=\"278\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"445\" data-original=\"https://pic2.zhimg.com/v2-806be2bf86839e8694b86c932370de61_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-806be2bf86839e8694b86c932370de61_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-436b0f5d97a8d886d7fe952c3216a628_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"458\" data-rawheight=\"278\" class=\"origin_image zh-lightbox-thumb\" width=\"458\" data-original=\"https://pic1.zhimg.com/v2-436b0f5d97a8d886d7fe952c3216a628_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;458&#39; height=&#39;278&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"458\" data-rawheight=\"278\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"458\" data-original=\"https://pic1.zhimg.com/v2-436b0f5d97a8d886d7fe952c3216a628_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-436b0f5d97a8d886d7fe952c3216a628_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>æ•ˆæœè¿˜æ˜¯å¾ˆæ˜æ˜¾çš„ï¼š</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-d1b43e773c0b32688e1e530269278583_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"354\" data-rawheight=\"129\" class=\"content_image\" width=\"354\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;354&#39; height=&#39;129&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"354\" data-rawheight=\"129\" class=\"content_image lazy\" width=\"354\" data-actualsrc=\"https://pic4.zhimg.com/v2-d1b43e773c0b32688e1e530269278583_b.jpg\"/></figure><p></p><p></p>", 
            "topic": [
                {
                    "tag": "æœºå™¨å­¦ä¹ ", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "ç¥ç»ç½‘ç»œ", 
                    "tagLink": "https://api.zhihu.com/topics/19607065"
                }, 
                {
                    "tag": "æ·±åº¦å­¦ä¹ ï¼ˆDeep Learningï¼‰", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/44718107", 
            "userName": "ç›´ä¸Šäº‘éœ„", 
            "userLink": "https://www.zhihu.com/people/1033165ce4ad9c3fce69a0793dfab8ad", 
            "upvote": 0, 
            "title": "DeepLearning.aiç¬”è®°:(2-2)-ä¼˜åŒ–ç®—æ³•", 
            "content": "<h2>é¦–å‘äºä¸ªäººåšå®¢ï¼š<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">fangzh.top</span><span class=\"invisible\"></span></a>ï¼Œæ¬¢è¿æ¥è®¿</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>è¿™å‘¨å­¦ä¹ äº†ä¼˜åŒ–ç®—æ³•ï¼Œå¯ä»¥è®©ç¥ç»ç½‘ç»œè¿è¡Œçš„æ›´å¿«ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><p>ä¸»è¦æœ‰:</p><ul><li>mini-batch</li><li>åŠ¨é‡æ¢¯åº¦ä¸‹é™(momentum)</li><li>RMSprop</li><li>Adamä¼˜åŒ–ç®—æ³•</li><li>å­¦ä¹ ç‡è¡°å‡</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>mini-batch(å°æ‰¹é‡)</b></h2><p>åŸæœ¬çš„æ¢¯åº¦ä¸‹é™ç®—æ³•ï¼Œåœ¨æ¯ä¸€æ¬¡çš„è¿­ä»£ä¸­ï¼Œè¦æŠŠæ‰€æœ‰çš„æ•°æ®éƒ½è¿›è¡Œè®¡ç®—å†å–å¹³å‡ï¼Œé‚£å¦‚æœä½ çš„æ•°æ®é‡ç‰¹åˆ«å¤§çš„è¯ï¼Œæ¯è¿›è¡Œä¸€æ¬¡è¿­ä»£å°±ä¼šè€—è´¹å¤§é‡çš„æ—¶é—´ã€‚</p><p>æ‰€ä»¥å°±æœ‰äº†mini-batchï¼Œåšå°æ‰¹é‡çš„è®¡ç®—è¿­ä»£ã€‚ä¹Ÿå°±æ˜¯æŠŠè®­ç»ƒé›†åˆ’åˆ†æˆnç­‰åˆ†ï¼Œæ¯”å¦‚æ•°æ®é‡æœ‰500ä¸‡ä¸ªçš„æ—¶å€™ï¼Œä»¥1000ä¸ºå•ä½ï¼Œå°†æ•°æ®é›†åˆ’åˆ†ä¸º5000ä»½ï¼Œ <img src=\"https://www.zhihu.com/equation?tex=+x+%3D++%7Bx%5E%7B%5Clbrace+1+%5Crbrace%7D%2Cx%5E%7B%5Clbrace+2+%5Crbrace%7D%2Cx%5E%7B%5Clbrace+3+%5Crbrace%7D%2C.....%2Cx%5E%7B%5Clbrace+5000+%5Crbrace%7D%7D\" alt=\" x =  {x^{\\lbrace 1 \\rbrace},x^{\\lbrace 2 \\rbrace},x^{\\lbrace 3 \\rbrace},.....,x^{\\lbrace 5000 \\rbrace}}\" eeimg=\"1\"/> </p><p>ç”¨å¤§æ‹¬å¼§è¡¨ç¤ºæ¯ä¸€ä»½çš„mini-batchï¼Œå…¶ä¸­æ¯ä¸€ä»½ <img src=\"https://www.zhihu.com/equation?tex=x%5E%7B%5Clbrace+t+%5Crbrace%7D\" alt=\"x^{\\lbrace t \\rbrace}\" eeimg=\"1\"/> éƒ½æ˜¯1000ä¸ªæ ·æœ¬ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-06800c573474f6c5effafbed3caa52ab_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1469\" data-rawheight=\"451\" class=\"origin_image zh-lightbox-thumb\" width=\"1469\" data-original=\"https://pic4.zhimg.com/v2-06800c573474f6c5effafbed3caa52ab_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1469&#39; height=&#39;451&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1469\" data-rawheight=\"451\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1469\" data-original=\"https://pic4.zhimg.com/v2-06800c573474f6c5effafbed3caa52ab_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-06800c573474f6c5effafbed3caa52ab_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>è¿™ä¸ªæ—¶å€™å¼•å…¥epochçš„æ¦‚å¿µï¼Œ1ä¸ªepochç›¸å½“äºæ˜¯éå†äº†ä¸€æ¬¡æ•°æ®é›†ï¼Œæ¯”å¦‚ç”¨mini-batchï¼Œ1ä¸ªepochå°±å¯ä»¥è¿›è¡Œ5000æ¬¡è¿­ä»£ï¼Œè€Œä¼ ç»Ÿçš„batchæŠŠæ•°æ®é›†éƒ½ä¸€èµ·è®¡ç®—ï¼Œç›¸å½“äº1ä¸ªepochåªè¿›è¡Œäº†1æ¬¡è¿­ä»£ã€‚</p><p>å…·ä½“è®¡ç®—æ­¥éª¤æ˜¯ï¼š</p><ul><li>å…ˆåˆ’åˆ†å¥½æ¯ä¸€ä¸ªmini-batch</li><li><code>for t in range(5000)</code>ï¼Œå¾ªç¯æ¯æ¬¡è¿­ä»£<br/></li><ul><li>å¾ªç¯é‡Œé¢å’Œä¹‹å‰çš„è®¡ç®—è¿‡ç¨‹ä¸€æ ·ï¼Œå‰å‘ä¼ æ’­ï¼Œä½†æ¯æ¬¡è®¡ç®—é‡æ˜¯1000ä¸ªæ ·æœ¬</li><li>è®¡ç®—æŸå¤±å‡½æ•°</li><li>åå‘ä¼ æ’­</li><li>æ›´æ–°å‚æ•°</li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>batchå’Œmini-batchçš„å¯¹æ¯”å¦‚å›¾ï¼š</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-9005b6652ea8829154d3e22dcedc5fc5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1469\" data-rawheight=\"816\" class=\"origin_image zh-lightbox-thumb\" width=\"1469\" data-original=\"https://pic2.zhimg.com/v2-9005b6652ea8829154d3e22dcedc5fc5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1469&#39; height=&#39;816&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1469\" data-rawheight=\"816\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1469\" data-original=\"https://pic2.zhimg.com/v2-9005b6652ea8829154d3e22dcedc5fc5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-9005b6652ea8829154d3e22dcedc5fc5_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>å¦‚æœmini-batchçš„æ ·æœ¬ä¸ºmçš„è¯ï¼Œå…¶å®å°±æ˜¯<b>batch gradient descent</b>ï¼Œç¼ºç‚¹æ˜¯å¦‚æœæ ·æœ¬é‡å¤ªå¤§çš„è¯ï¼Œæ¯ä¸€æ¬¡è¿­ä»£çš„æ—¶é—´ä¼šæ¯”è¾ƒé•¿ï¼Œä½†æ˜¯ä¼˜ç‚¹æ˜¯æ¯ä¸€æ¬¡è¿­ä»£çš„æŸå¤±å‡½æ•°éƒ½æ˜¯ä¸‹é™çš„ï¼Œæ¯”è¾ƒå¹³ç¨³ã€‚</li><li>mini-batchæ ·æœ¬ä¸º1çš„è¯ï¼Œé‚£å°±æ˜¯<b>éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆStochastic gradient descentï¼‰</b>,ä¹Ÿå°±æ˜¯æ¯æ¬¡è¿­ä»£åªé€‰æ‹©å…¶ä¸­ä¸€ä¸ªæ ·æœ¬è¿›è¡Œè¿­ä»£ï¼Œä½†æ˜¯è¿™æ ·ä¼šå¤±å»äº†æ ·æœ¬å‘é‡åŒ–å¸¦æ¥çš„è®¡ç®—åŠ é€Ÿæ•ˆæœï¼ŒæŸå¤±å‡½æ•°æ€»ä½“æ˜¯ä¸‹é™çš„ï¼Œä½†æ˜¯å±€éƒ¨ä¼šå¾ˆæŠ–åŠ¨ï¼Œå¾ˆå¯èƒ½æ— æ³•è¾¾åˆ°å…¨å±€æœ€å°ç‚¹ã€‚</li><li>æ‰€ä»¥é€‰æ‹©ä¸€ä¸ªåˆé€‚çš„sizeå¾ˆé‡è¦ï¼Œ <img src=\"https://www.zhihu.com/equation?tex=1+%3C+size+%3C+m\" alt=\"1 &lt; size &lt; m\" eeimg=\"1\"/> ï¼Œå¯ä»¥å®ç°å¿«é€Ÿçš„è®¡ç®—æ•ˆæœï¼Œä¹Ÿèƒ½å¤Ÿäº«å—å‘é‡åŒ–å¸¦æ¥çš„åŠ é€Ÿã€‚</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-5972813cc0cdae43c4e9bdfe97d0b215_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"497\" data-rawheight=\"426\" class=\"origin_image zh-lightbox-thumb\" width=\"497\" data-original=\"https://pic2.zhimg.com/v2-5972813cc0cdae43c4e9bdfe97d0b215_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;497&#39; height=&#39;426&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"497\" data-rawheight=\"426\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"497\" data-original=\"https://pic2.zhimg.com/v2-5972813cc0cdae43c4e9bdfe97d0b215_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-5972813cc0cdae43c4e9bdfe97d0b215_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>mini-batch sizeçš„é€‰æ‹©</b></p><p>å› ä¸ºç”µè„‘çš„å†…å­˜å’Œä½¿ç”¨æ–¹å¼éƒ½æ˜¯äºŒè¿›åˆ¶çš„ï¼Œè€Œä¸”æ˜¯2çš„næ¬¡æ–¹ï¼Œæ‰€ä»¥ä¹‹å‰é€‰1000ä¹Ÿä¸å¤ªåˆç†ï¼Œå¯ä»¥é€‰1024ï¼Œä½†æ˜¯1024ä¹Ÿæ¯”è¾ƒå°‘è§ï¼Œä¸€èˆ¬æ˜¯ä»64åˆ°512ã€‚ä¹Ÿå°±æ˜¯$64ã€128ã€256ã€512$</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>æŒ‡æ•°åŠ æƒå¹³å‡(Exponentially weighted averages )</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-00c157a2c8b43d398f24216e4a310798_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"704\" data-rawheight=\"391\" class=\"origin_image zh-lightbox-thumb\" width=\"704\" data-original=\"https://pic1.zhimg.com/v2-00c157a2c8b43d398f24216e4a310798_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;704&#39; height=&#39;391&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"704\" data-rawheight=\"391\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"704\" data-original=\"https://pic1.zhimg.com/v2-00c157a2c8b43d398f24216e4a310798_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-00c157a2c8b43d398f24216e4a310798_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>è“è‰²çš„ç‚¹æ˜¯æ¯ä¸€å¤©çš„æ°”æ¸©ï¼Œå¯ä»¥çœ‹åˆ°æ˜¯éå¸¸æŠ–åŠ¨çš„ï¼Œé‚£å¦‚æœå¯ä»¥æŠŠå®ƒå¹³å‡ä¸€ä¸‹ï¼Œæ¯”å¦‚æŠŠ10å¤©å†…çš„æ°”æ¸©å¹³å‡ä¸€ä¸‹ï¼Œå°±å¯ä»¥å¾—åˆ°å¦‚çº¢è‰²çš„æ›²çº¿ã€‚</p><p>ä½†æ˜¯å¦‚æœæ˜¯å•çº¯çš„æŠŠå‰é¢çš„10å¤©æ°”æ¸©ä¸€èµ·å¹³å‡çš„è¯ï¼Œé‚£ä¹ˆè¿™æ ·ä½ å°±éœ€è¦æŠŠå‰10å¤©çš„æ°”æ¸©å…¨éƒ¨å‚¨å­˜è®°å½•ä¸‹æ¥ï¼Œè¿™æ ·å­è™½ç„¶ä¼šæ›´å‡†ä¸€ç‚¹ï¼Œä½†æ˜¯å¾ˆæµªè´¹å‚¨å­˜ç©ºé—´ï¼Œæ‰€ä»¥å°±æœ‰äº†<b>æŒ‡æ•°åŠ æƒå¹³å‡</b>è¿™æ ·çš„æ¦‚å¿µã€‚æ–¹æ³•å¦‚ä¸‹ï¼š</p><p><img src=\"https://www.zhihu.com/equation?tex=V_0+%3D+0\" alt=\"V_0 = 0\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=V_1+%3D+%5Cbeta+%2A+V_0+%2B+%281+-+%5Cbeta%29+%5Ctheta_1\" alt=\"V_1 = \\beta * V_0 + (1 - \\beta) \\theta_1\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%E2%80%A6%E2%80%A6\" alt=\"â€¦â€¦\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=V_t+%3D+%5Cbeta+%2A+V_%7Bt-1%7D+%2B+%281+-+%5Cbeta%29+%5Ctheta_t\" alt=\"V_t = \\beta * V_{t-1} + (1 - \\beta) \\theta_t\" eeimg=\"1\"/> </p><p>å…¶ä¸­ï¼Œ <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_t\" alt=\"\\theta_t\" eeimg=\"1\"/> è¡¨ç¤ºç¬¬tå¤©çš„æ¸©åº¦ï¼Œè€Œ <img src=\"https://www.zhihu.com/equation?tex=V_t\" alt=\"V_t\" eeimg=\"1\"/> è¡¨ç¤ºæŒ‡æ•°åŠ æƒå¹³å‡åçš„ç¬¬tå¤©æ¸©åº¦ï¼Œ <img src=\"https://www.zhihu.com/equation?tex=%5Cbeta\" alt=\"\\beta\" eeimg=\"1\"/> è¿™ä¸ªå‚æ•°è¡¨ç¤º <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7B1-%5Cbeta%7D\" alt=\"\\frac{1}{1-\\beta}\" eeimg=\"1\"/> å¤©çš„å¹³å‡ï¼Œä¹Ÿå°±æ˜¯ï¼Œ <img src=\"https://www.zhihu.com/equation?tex=%5Cbeta+%3D+0.9\" alt=\"\\beta = 0.9\" eeimg=\"1\"/> ï¼Œè¡¨ç¤º10å¤©å†…çš„å¹³å‡ï¼Œ <img src=\"https://www.zhihu.com/equation?tex=%5Cbeta+%3D+0.98\" alt=\"\\beta = 0.98\" eeimg=\"1\"/> ï¼Œè¡¨ç¤º50å¤©å†…çš„å¹³å‡ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-d1b2d7a4aa8ebba490950f72c4db22e1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"816\" data-rawheight=\"402\" class=\"origin_image zh-lightbox-thumb\" width=\"816\" data-original=\"https://pic2.zhimg.com/v2-d1b2d7a4aa8ebba490950f72c4db22e1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;816&#39; height=&#39;402&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"816\" data-rawheight=\"402\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"816\" data-original=\"https://pic2.zhimg.com/v2-d1b2d7a4aa8ebba490950f72c4db22e1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-d1b2d7a4aa8ebba490950f72c4db22e1_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>ç†è§£æŒ‡æ•°åŠ æƒå¹³å‡</b></h2><p>æˆ‘ä»¬å†æ¥çœ‹ä¸€ä¸‹å…¬å¼ï¼š</p><p><img src=\"https://www.zhihu.com/equation?tex=v_t+%3D+%5Cbeta+v_%7Bt-1%7D+%2B+%281+-+%5Cbeta%29+%5Ctheta_t\" alt=\"v_t = \\beta v_{t-1} + (1 - \\beta) \\theta_t\" eeimg=\"1\"/> </p><p>å‡è®¾$\\beta = 0.9$ï¼Œé‚£ä¹ˆ</p><p><img src=\"https://www.zhihu.com/equation?tex=v_%7B100%7D+%3D+0.9v_%7B99%7D+%2B+0.1%5Ctheta_%7B100%7D\" alt=\"v_{100} = 0.9v_{99} + 0.1\\theta_{100}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=v_%7B99%7D+%3D+0.9v_%7B98%7D+%2B+0.1%5Ctheta_%7B99%7D\" alt=\"v_{99} = 0.9v_{98} + 0.1\\theta_{99}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=v_%7B98%7D+%3D+0.9v_%7B97%7D+%2B+0.1%5Ctheta_%7B98%7D\" alt=\"v_{98} = 0.9v_{97} + 0.1\\theta_{98}\" eeimg=\"1\"/> </p><p>å±•å¼€ä¸€ä¸‹ï¼Œå¾—åˆ°ï¼š</p><p><img src=\"https://www.zhihu.com/equation?tex=+v_%7B100%7D+%3D+0.1+%5Ctheta_%7B100%7D+%2B+0.1+%5Ctimes+0.9+%5Ctimes+%5Ctheta_%7B99%7D+%2B++0.1+%5Ctimes+0.9%5E2++%5Ctimes+%5Ctheta_%7B98%7D+%2B+......\" alt=\" v_{100} = 0.1 \\theta_{100} + 0.1 \\times 0.9 \\times \\theta_{99} +  0.1 \\times 0.9^2  \\times \\theta_{98} + ......\" eeimg=\"1\"/> </p><p>çœ‹åˆ°æ²¡æœ‰ï¼Œæ¯ä¸€é¡¹éƒ½ä¼šä¹˜ä»¥0.9ï¼Œè¿™æ ·å°±æ˜¯æŒ‡æ•°åŠ æƒçš„æ„æ€äº†ï¼Œé‚£ä¹ˆä¸ºä»€ä¹ˆè¡¨ç¤ºçš„æ˜¯10å¤©å†…çš„å¹³å‡å€¼å‘¢ï¼Ÿæ˜æ˜æ˜¯10å¤©ä»¥å‰çš„æ•°æ®éƒ½æœ‰åŠ è¿›å»çš„æ‰å¯¹ï¼Œå…¶å®æ˜¯å› ä¸º <img src=\"https://www.zhihu.com/equation?tex=0.9%5E%7B10%7D+%5Capprox+0.35+%5Capprox+%5Cfrac%7B1%7D%7Be%7D\" alt=\"0.9^{10} \\approx 0.35 \\approx \\frac{1}{e}\" eeimg=\"1\"/> ï¼Œä¹Ÿå°±æ˜¯10å¤©ä»¥å‰çš„æƒé‡åªå äº†ä¸‰åˆ†ä¹‹ä¸€å·¦å³ï¼Œå·²ç»å¾ˆå°äº†ï¼Œæ‰€ä»¥æˆ‘ä»¬å°±å¯ä»¥è®¤ä¸ºè¿™ä¸ªæƒé‡å°±æ˜¯10å¤©å†…çš„æ¸©åº¦å¹³å‡ï¼Œå…¶å®æœ‰è¯¦ç»†çš„æ•°å­¦è¯æ˜çš„ï¼Œè¿™é‡Œå°±ä¸è¦è¯æ˜äº†ï¼Œåæ­£ç†è§£äº† <img src=\"https://www.zhihu.com/equation?tex=%281-%5Cepsilon%29%5E%7B%5Cfrac%7B1%7D%7B%5Cepsilon%7D%7D+%5Capprox+%5Cfrac%7B1%7D%7Be%7D%EF%BC%8C%5Cepsilon\" alt=\"(1-\\epsilon)^{\\frac{1}{\\epsilon}} \\approx \\frac{1}{e}ï¼Œ\\epsilon\" eeimg=\"1\"/> ä¸º0.02çš„æ—¶å€™ï¼Œå°±ä»£è¡¨äº†50å¤©å†…çš„æ•°æ®ã€‚</p><p>å› ä¸ºæŒ‡æ•°åŠ æƒå¹³å‡ä¸éœ€è¦çŸ¥é“å‰é¢nä¸ªæ•°æ®ï¼Œåªè¦ä¸€æ­¥ä¸€æ­¥è¿›è¡Œè¿­ä»£ï¼ŒçŸ¥é“å½“å‰çš„æ•°æ®å°±è¡Œï¼Œæ‰€ä»¥éå¸¸èŠ‚çœç©ºé—´ã€‚</p><h2><b>æŒ‡æ•°åŠ æƒå¹³å‡çš„åå·®ä¿®æ­£</b></h2><p>å¦‚æœä½ ç»†å¿ƒä¸€ç‚¹ï¼Œä½ å°±ä¼šå‘ç°å…¶å®è¿™ä¸ªå…¬å¼æœ‰é—®é¢˜ï¼Œ</p><p><img src=\"https://www.zhihu.com/equation?tex=V_0+%3D+0\" alt=\"V_0 = 0\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=V_1+%3D+%5Cbeta+%2A+V_0+%2B+%281+-+%5Cbeta%29+%5Ctheta_1\" alt=\"V_1 = \\beta * V_0 + (1 - \\beta) \\theta_1\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%E2%80%A6%E2%80%A6\" alt=\"â€¦â€¦\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=V_t+%3D+%5Cbeta+%2A+V_%7Bt-1%7D+%2B+%281+-+%5Cbeta%29+%5Ctheta_t\" alt=\"V_t = \\beta * V_{t-1} + (1 - \\beta) \\theta_t\" eeimg=\"1\"/> </p><p>å¦‚æœç¬¬ä¸€å¤©çš„æ¸©åº¦æ˜¯40æ‘„æ°åº¦ï¼Œé‚£ä¹ˆ <img src=\"https://www.zhihu.com/equation?tex=V_1+%3D+0.1+%2A+40+%3D+4\" alt=\"V_1 = 0.1 * 40 = 4\" eeimg=\"1\"/> ï¼Œæ˜¾ç„¶æ˜¯ä¸åˆç†çš„ã€‚å› ä¸ºåˆå§‹å€¼ <img src=\"https://www.zhihu.com/equation?tex=V_0+%3D+0\" alt=\"V_0 = 0\" eeimg=\"1\"/> ï¼Œä¹Ÿå°±æ˜¯å‰é¢å‡ å¤©çš„æ•°æ®éƒ½ä¼šæ™®éåä½ã€‚æ‰€ä»¥ç‰¹åˆ«æ˜¯åœ¨ä¼°æµ‹åˆæœŸï¼Œéœ€è¦è¿›è¡Œä¸€äº›ä¿®æ­£ï¼Œè¿™ä¸ªæ—¶å€™å°±ä¸è¦ç”¨ <img src=\"https://www.zhihu.com/equation?tex=v_t\" alt=\"v_t\" eeimg=\"1\"/> äº†ï¼Œè€Œæ˜¯ç”¨ <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7Bv_t%7D%7B1-%5Cbeta%5Et%7D\" alt=\"\\frac{v_t}{1-\\beta^t}\" eeimg=\"1\"/> æ¥ä»£è¡¨ç¬¬tå¤©çš„æ¸©åº¦å¹³å‡ï¼Œä½ ä¼šå‘ç°éšç€tçš„å¢åŠ ï¼Œ <img src=\"https://www.zhihu.com/equation?tex=%5Cbeta%5Et\" alt=\"\\beta^t\" eeimg=\"1\"/> æ¥è¿‘äº0ï¼Œæ‰€ä»¥åå·®ä¿®æ­£å‡ ä¹å°±æ²¡æœ‰ç”¨äº†ï¼Œè€Œtæ¯”è¾ƒå°çš„æ—¶å€™ï¼Œå°±éå¸¸æœ‰æ•ˆæœã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-d304dbf1380aa255138b954fbef332fd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"833\" data-rawheight=\"346\" class=\"origin_image zh-lightbox-thumb\" width=\"833\" data-original=\"https://pic2.zhimg.com/v2-d304dbf1380aa255138b954fbef332fd_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;833&#39; height=&#39;346&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"833\" data-rawheight=\"346\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"833\" data-original=\"https://pic2.zhimg.com/v2-d304dbf1380aa255138b954fbef332fd_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-d304dbf1380aa255138b954fbef332fd_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>ä¸è¿‡åœ¨å¤§éƒ¨åˆ†æœºå™¨å­¦ä¹ ä¸­ï¼Œä¸€èˆ¬ä¹Ÿä¸éœ€è¦ä¿®æ­£ï¼Œå› ä¸ºåªæ˜¯å‰é¢çš„åˆå§‹æ—¶æœŸæ¯”è¾ƒæœ‰åå·®è€Œå·²ï¼Œåˆ°åé¢å°±åŸºæœ¬ä¸ä¼šæœ‰åå·®äº†ï¼Œæ‰€ä»¥ä¹Ÿä¸å¤ªç”¨ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>åŠ¨é‡æ¢¯åº¦ä¸‹é™æ³• (Gradient descent with Momentum )</b></h2><p>ç”¨åŠ¨é‡æ¢¯åº¦ä¸‹é™æ³•è¿è¡Œé€Ÿåº¦æ€»æ˜¯æ¯”æ ‡å‡†çš„æ¢¯åº¦ä¸‹é™æ³•è¦æ¥çš„å¿«ã€‚å®ƒçš„åŸºæœ¬æ€æƒ³æ˜¯è®¡ç®—æ¢¯åº¦çš„æŒ‡æ•°åŠ æƒå¹³å‡æ•°ï¼Œç„¶åç”¨è¯¥æ¢¯åº¦æ¥æ›´æ–°æƒé‡ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><p>æ•ˆæœå¦‚å›¾ï¼š</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-5ed6d8f8ee5a2f5bc3347f29dd8c53a1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1411\" data-rawheight=\"383\" class=\"origin_image zh-lightbox-thumb\" width=\"1411\" data-original=\"https://pic2.zhimg.com/v2-5ed6d8f8ee5a2f5bc3347f29dd8c53a1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1411&#39; height=&#39;383&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1411\" data-rawheight=\"383\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1411\" data-original=\"https://pic2.zhimg.com/v2-5ed6d8f8ee5a2f5bc3347f29dd8c53a1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-5ed6d8f8ee5a2f5bc3347f29dd8c53a1_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>ä½¿ç”¨åŠ¨é‡æ¢¯åº¦ä¸‹é™æ³•åï¼Œåœ¨ç«–ç›´æ–¹å‘ä¸Šçš„æŠ–åŠ¨å‡å°‘äº†ï¼Œè€Œåœ¨æ°´å¹³æ–¹å‘ä¸Šçš„è¿åŠ¨åè€ŒåŠ é€Ÿäº†ã€‚</p><p>ç®—æ³•å…¬å¼ï¼š</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-60f2fa472b9a4de75a29bbd5ae54e9dd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1104\" data-rawheight=\"463\" class=\"origin_image zh-lightbox-thumb\" width=\"1104\" data-original=\"https://pic2.zhimg.com/v2-60f2fa472b9a4de75a29bbd5ae54e9dd_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1104&#39; height=&#39;463&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1104\" data-rawheight=\"463\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1104\" data-original=\"https://pic2.zhimg.com/v2-60f2fa472b9a4de75a29bbd5ae54e9dd_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-60f2fa472b9a4de75a29bbd5ae54e9dd_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>å¯ä»¥å‘ç°ï¼Œå°±æ˜¯æ ¹æ®æŒ‡æ•°å¹³å‡è®¡ç®—å‡ºäº† <img src=\"https://www.zhihu.com/equation?tex=v_%7BdW%7D\" alt=\"v_{dW}\" eeimg=\"1\"/> <i>ï¼Œç„¶åæ›´æ–°å‚æ•°æ—¶æŠŠ</i> <img src=\"https://www.zhihu.com/equation?tex=dW\" alt=\"dW\" eeimg=\"1\"/> <i>æ¢æˆäº†</i> <img src=\"https://www.zhihu.com/equation?tex=v_%7Bdw%7D\" alt=\"v_{dw}\" eeimg=\"1\"/> ï¼Œ <img src=\"https://www.zhihu.com/equation?tex=%5Cbeta\" alt=\"\\beta\" eeimg=\"1\"/> ä¸€èˆ¬çš„å–å€¼æ˜¯0.9ã€‚å¯ä»¥å‘ç°ï¼Œåœ¨çºµå‘çš„æ³¢åŠ¨ç»è¿‡å¹³å‡ä»¥åï¼Œå˜å¾—éå¸¸å°äº†ï¼Œè€Œå› ä¸ºåœ¨æ¨ªå‘ä¸Šï¼Œæ¯ä¸€æ¬¡çš„å¾®åˆ†åˆ†é‡éƒ½æ˜¯æŒ‡å‘ä½ç‚¹ï¼Œæ‰€ä»¥å¹³å‡åçš„å€¼ä¸€ç›´æœç€ä½ç‚¹å‰è¿›ã€‚</p><p>ç‰©ç†æ„ä¹‰ï¼š</p><ul><li>ä¸ªäººçš„ç†è§£æ˜¯å¤§æ¦‚è¿™ä¸ªå…¬å¼ä¹Ÿå¾ˆåƒåŠ¨é‡çš„å…¬å¼ <img src=\"https://www.zhihu.com/equation?tex=m+v+%3D+m_1+v_1+%2B+m_2+v_2\" alt=\"m v = m_1 v_1 + m_2 v_2\" eeimg=\"1\"/> ï¼Œä¹Ÿå°±æ˜¯æŠŠä¸¤ä¸ªç‰©ä½“åˆå¹¶äº†å¾—åˆ°æ–°ç‰©ä½“çš„è´¨é‡å’Œé€Ÿåº¦çš„æ„æ€</li><li>ç†è§£æˆé€Ÿåº¦å’ŒåŠ é€Ÿåº¦ï¼ŒæŠŠ <img src=\"https://www.zhihu.com/equation?tex=v_%7BdW%7D\" alt=\"v_{dW}\" eeimg=\"1\"/> çœ‹æˆé€Ÿåº¦ï¼Œ <img src=\"https://www.zhihu.com/equation?tex=dW\" alt=\"dW\" eeimg=\"1\"/> çœ‹æˆåŠ é€Ÿåº¦ï¼Œè¿™æ ·æ¯æ¬¡å› ä¸ºæœ‰é€Ÿåº¦çš„å­˜åœ¨ï¼ŒåŠ é€Ÿåº¦åªèƒ½å½±å“åˆ°é€Ÿåº¦çš„å¤§å°è€Œä¸èƒ½å¤Ÿç«‹åˆ»æ”¹å˜é€Ÿåº¦çš„æ–¹å‘ã€‚</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>RMSpropï¼ˆroot mean square propï¼‰</b></h2><p>å‡æ–¹æ ¹ä¼ æ’­ã€‚è¿™æ˜¯å¦ä¸€ç§æ¢¯åº¦ä¸‹é™çš„ä¼˜åŒ–ç®—æ³•ã€‚</p><p>é¡¾åæ€ä¹‰ï¼Œå…ˆå¹³æ–¹å†å¼€æ ¹å·ã€‚</p><p>å…¶å®å’ŒåŠ¨é‡æ¢¯åº¦ä¸‹é™æ³•å…¬å¼å·®ä¸å¤šï¼š</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-10dabf6dd378007f0660b9daa9cf0b04_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"958\" data-rawheight=\"454\" class=\"origin_image zh-lightbox-thumb\" width=\"958\" data-original=\"https://pic1.zhimg.com/v2-10dabf6dd378007f0660b9daa9cf0b04_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;958&#39; height=&#39;454&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"958\" data-rawheight=\"454\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"958\" data-original=\"https://pic1.zhimg.com/v2-10dabf6dd378007f0660b9daa9cf0b04_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-10dabf6dd378007f0660b9daa9cf0b04_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>åœ¨æ›´æ–°å‚æ•°çš„åˆ†æ¯é¡¹åŠ äº†ä¸€é¡¹ <img src=\"https://www.zhihu.com/equation?tex=%5Cepsilon+%3D+10%5E%7B-8%7D\" alt=\"\\epsilon = 10^{-8}\" eeimg=\"1\"/> ,æ¥ç¡®ä¿ç®—æ³•ä¸ä¼šé™¤ä»¥0</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>Adamç®—æ³•</b></h2><p>Adamç®—æ³•å…¶å®å°±æ˜¯ç»“åˆäº†Momentumå’ŒRMSprop ï¼Œæ³¨æ„è¿™ä¸ªæ—¶å€™è¦åŠ ä¸Šåå·®ä¿®æ­£ï¼š</p><ul><li>åˆå§‹åŒ–å‚æ•°ï¼š <img src=\"https://www.zhihu.com/equation?tex=v_%7BdW%7D+%3D+0%EF%BC%8CS_%7BdW%7D+%3D0%EF%BC%8Cv_%7Bdb%7D+%3D+0%EF%BC%8CS_%7Bdb%7D+%3D0\" alt=\"v_{dW} = 0ï¼ŒS_{dW} =0ï¼Œv_{db} = 0ï¼ŒS_{db} =0\" eeimg=\"1\"/> </li><li>åœ¨ç¬¬ <img src=\"https://www.zhihu.com/equation?tex=t\" alt=\"t\" eeimg=\"1\"/> æ¬¡è¿­ä»£ä¸­ï¼Œ<br/></li><ul><li>è®¡ç®—mini-batchçš„dW,db</li><li>Momentum: <img src=\"https://www.zhihu.com/equation?tex=v_%7BdW%7D%3D+%5Cbeta_%7B1%7Dv_%7BdW%7D+%2B+%28+1+-+%5Cbeta_%7B1%7D%29dW%EF%BC%8Cv_%7Bdb%7D%3D+%5Cbeta_%7B1%7Dv_%7Bdb%7D+%2B+%28+1+-%5Cbeta_%7B1%7D+%29%7Bdb%7D\" alt=\"v_{dW}= \\beta_{1}v_{dW} + ( 1 - \\beta_{1})dWï¼Œv_{db}= \\beta_{1}v_{db} + ( 1 -\\beta_{1} ){db}\" eeimg=\"1\"/> </li><li>RMSprop: <img src=\"https://www.zhihu.com/equation?tex=S_%7BdW%7D%3D%5Cbeta_%7B2%7DS_%7BdW%7D+%2B+%28+1+-+%5Cbeta_%7B2%7D%29%7B%28dW%29%7D%5E%7B2%7D%24%EF%BC%8C%24S_%7Bdb%7D+%3D%5Cbeta_%7B2%7DS_%7Bdb%7D+%2B+%5Cleft%28+1+-+%5Cbeta_%7B2%7D+%5Cright%29%7B%28db%29%7D%5E%7B2%7D\" alt=\"S_{dW}=\\beta_{2}S_{dW} + ( 1 - \\beta_{2}){(dW)}^{2}$ï¼Œ$S_{db} =\\beta_{2}S_{db} + \\left( 1 - \\beta_{2} \\right){(db)}^{2}\" eeimg=\"1\"/> </li><li><img src=\"https://www.zhihu.com/equation?tex=v_%7BdW%7D%5E%7B%5Ctext%7Bcorrected%7D%7D%3D+%5Cfrac%7Bv_%7BdW%7D%7D%7B1+-+%5Cbeta_%7B1%7D%5E%7Bt%7D%7D%EF%BC%8Cv_%7Bdb%7D%5E%7B%5Ctext%7Bcorrected%7D%7D+%3D%5Cfrac%7Bv_%7Bdb%7D%7D%7B1+-%5Cbeta_%7B1%7D%5E%7Bt%7D%7D\" alt=\"v_{dW}^{\\text{corrected}}= \\frac{v_{dW}}{1 - \\beta_{1}^{t}}ï¼Œv_{db}^{\\text{corrected}} =\\frac{v_{db}}{1 -\\beta_{1}^{t}}\" eeimg=\"1\"/> </li><li><img src=\"https://www.zhihu.com/equation?tex=S_%7BdW%7D%5E%7B%5Ctext%7Bcorrected%7D%7D+%3D%5Cfrac%7BS_%7BdW%7D%7D%7B1+-+%5Cbeta_%7B2%7D%5E%7Bt%7D%7D%EF%BC%8CS_%7Bdb%7D%5E%7B%5Ctext%7Bcorrected%7D%7D+%3D%5Cfrac%7BS_%7Bdb%7D%7D%7B1+-+%5Cbeta_%7B2%7D%5E%7Bt%7D%7D\" alt=\"S_{dW}^{\\text{corrected}} =\\frac{S_{dW}}{1 - \\beta_{2}^{t}}ï¼ŒS_{db}^{\\text{corrected}} =\\frac{S_{db}}{1 - \\beta_{2}^{t}}\" eeimg=\"1\"/> </li><li><img src=\"https://www.zhihu.com/equation?tex=W%3A%3D+W+-+%5Cfrac%7Ba+v_%7BdW%7D%5E%7B%5Ctext%7Bcorrected%7D%7D%7D%7B%5Csqrt%7BS_%7BdW%7D%5E%7B%5Ctext%7Bcorrected%7D%7D%7D+%2B%5Cvarepsilon%7D\" alt=\"W:= W - \\frac{a v_{dW}^{\\text{corrected}}}{\\sqrt{S_{dW}^{\\text{corrected}}} +\\varepsilon}\" eeimg=\"1\"/> </li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>è¶…å‚æ•°æœ‰ <img src=\"https://www.zhihu.com/equation?tex=%5Calpha%2C%5Cbeta_1%2C%5Cbeta_2%2C%5Cepsilon\" alt=\"\\alpha,\\beta_1,\\beta_2,\\epsilon\" eeimg=\"1\"/> ï¼Œä¸€èˆ¬ <img src=\"https://www.zhihu.com/equation?tex=%5Cbeta_1+%3D+0.9%2C%5Cbeta_2+%3D+0.999%2C%5Cepsilon+%3D+10%5E%7B-8%7D\" alt=\"\\beta_1 = 0.9,\\beta_2 = 0.999,\\epsilon = 10^{-8}\" eeimg=\"1\"/> </p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>å­¦ä¹ ç‡è¡°å‡</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p>åœ¨æ¢¯åº¦ä¸‹é™æ—¶ï¼Œå¦‚æœæ˜¯å›ºå®šçš„å­¦ä¹ ç‡ <img src=\"https://www.zhihu.com/equation?tex=%5Calpha\" alt=\"\\alpha\" eeimg=\"1\"/> ï¼Œåœ¨åˆ°è¾¾æœ€å°å€¼é™„è¿‘çš„æ—¶å€™ï¼Œå¯èƒ½ä¸ä¼šç²¾ç¡®æ”¶æ•›ï¼Œä¼šå¾ˆæŠ–åŠ¨ï¼Œå› æ­¤å¾ˆéš¾è¾¾åˆ°æœ€å°å€¼ï¼Œæ‰€ä»¥å¯ä»¥è€ƒè™‘å­¦ä¹ ç‡è¡°å‡ï¼Œåœ¨è¿­ä»£è¿‡ç¨‹ä¸­ï¼Œé€æ¸å‡å°$\\alpha$ï¼Œè¿™æ ·ä¸€å¼€å§‹æ¯”è¾ƒå¿«ï¼Œåæ¥æ…¢æ…¢çš„å˜æ…¢ã€‚</p><p>å¸¸ç”¨çš„æ˜¯ï¼š</p><p><img src=\"https://www.zhihu.com/equation?tex=a%3D+%5Cfrac%7B1%7D%7B1+%2B+decayrate+%2A+%5Ctext%7Bepoch_num%7D%7D+a_%7B0%7D\" alt=\"a= \\frac{1}{1 + decayrate * \\text{epoch_num}} a_{0}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=a+%3D%5Cfrac%7Bk%7D%7B%5Csqrt%7B%5Ctext%7Bepoch_num%7D%7D%7Da_%7B0%7D\" alt=\"a =\\frac{k}{\\sqrt{\\text{epoch_num}}}a_{0}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=a+%3D%5Cfrac%7Bk%7D%7B%5Csqrt%7Bt%7D%7Da_%7B0%7D\" alt=\"a =\\frac{k}{\\sqrt{t}}a_{0}\" eeimg=\"1\"/> </p><p></p><p></p>", 
            "topic": [
                {
                    "tag": "æœºå™¨å­¦ä¹ ", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "æ·±åº¦å­¦ä¹ ï¼ˆDeep Learningï¼‰", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "ç¥ç»ç½‘ç»œ", 
                    "tagLink": "https://api.zhihu.com/topics/19607065"
                }
            ], 
            "comments": [
                {
                    "userName": "çŸ¥ä¹ç”¨æˆ·", 
                    "userLink": "https://www.zhihu.com/people/0", 
                    "content": "<p>â€œä¸ªäººç†è§£â€è¿™ä¸ªåœ°æ–¹ä¸å¯¹ã€‚</p><p>è¡¥å……ä¸‹å§,dWå…¶å®æ˜¯æ ¹æ®äºŒé˜¶å·®åˆ†æ–¹ç¨‹æ¨å¯¼å¾—å‡ºçš„â€œåŠ¨é‡ç›¸å…³çš„é¡¹â€</p><p>åŠ é€Ÿåº¦è¿™ä¸ªè¯´æ³•æ˜¯æ­£ç¡®çš„ã€‚</p><p>æ‰€ä»¥ä¼ªä»£ç ä¸­éè¦è¯´åŠ¨é‡çš„è¯ï¼Œå…¶å®æ˜¯dW.</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/44675417", 
            "userName": "ç›´ä¸Šäº‘éœ„", 
            "userLink": "https://www.zhihu.com/people/1033165ce4ad9c3fce69a0793dfab8ad", 
            "upvote": 1, 
            "title": "DeepLearning.aiä½œä¸š:(2-1)-æ·±åº¦å­¦ä¹ å®è·µ", 
            "content": "<h2>æœ¬æ–‡æ‰‹æ³•äºä¸ªäººåšå®¢ï¼š<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">fangzh.top</span><span class=\"invisible\"></span></a>ï¼Œæ¬¢è¿æ¥è®¿</h2><ol><li>ä¸è¦æŠ„ä½œä¸šï¼</li><li>æˆ‘åªæ˜¯æŠŠæ€è·¯æ•´ç†äº†ï¼Œä¾›ä¸ªäººå­¦ä¹ ã€‚</li><li>ä¸è¦æŠ„ä½œä¸šï¼</li></ol><p class=\"ztext-empty-paragraph\"><br/></p><p>æœ¬å‘¨çš„ä½œä¸šåˆ†äº†3éƒ¨åˆ†ï¼š</p><ul><li>åˆå§‹åŒ–å‚æ•°</li><li>æ­£åˆ™åŒ–ï¼ˆL2ã€dropoutï¼‰</li><li>æ¢¯åº¦æ£€éªŒ</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>part1ï¼šInitialization</b></h2><p>ä¸»è¦è¯´æ˜çš„ä¸åŒçš„åˆå§‹åŒ–å¯¹è¿­ä»£çš„å½±å“ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><p>é¦–å…ˆï¼Œæ¨¡å‹å‡½æ•°æ˜¯è¿™æ ·çš„ï¼š</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">model</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span> <span class=\"o\">=</span> <span class=\"mf\">0.01</span><span class=\"p\">,</span> <span class=\"n\">num_iterations</span> <span class=\"o\">=</span> <span class=\"mi\">15000</span><span class=\"p\">,</span> <span class=\"n\">print_cost</span> <span class=\"o\">=</span> <span class=\"bp\">True</span><span class=\"p\">,</span> <span class=\"n\">initialization</span> <span class=\"o\">=</span> <span class=\"s2\">&#34;he&#34;</span><span class=\"p\">):</span>\n\n    <span class=\"n\">grads</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>\n    <span class=\"n\">costs</span> <span class=\"o\">=</span> <span class=\"p\">[]</span> <span class=\"c1\"># to keep track of the loss</span>\n    <span class=\"n\">m</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"c1\"># number of examples</span>\n    <span class=\"n\">layers_dims</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span>\n    \n    <span class=\"c1\"># Initialize parameters dictionary.</span>\n    <span class=\"k\">if</span> <span class=\"n\">initialization</span> <span class=\"o\">==</span> <span class=\"s2\">&#34;zeros&#34;</span><span class=\"p\">:</span>\n        <span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"n\">initialize_parameters_zeros</span><span class=\"p\">(</span><span class=\"n\">layers_dims</span><span class=\"p\">)</span>\n    <span class=\"k\">elif</span> <span class=\"n\">initialization</span> <span class=\"o\">==</span> <span class=\"s2\">&#34;random&#34;</span><span class=\"p\">:</span>\n        <span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"n\">initialize_parameters_random</span><span class=\"p\">(</span><span class=\"n\">layers_dims</span><span class=\"p\">)</span>\n    <span class=\"k\">elif</span> <span class=\"n\">initialization</span> <span class=\"o\">==</span> <span class=\"s2\">&#34;he&#34;</span><span class=\"p\">:</span>\n        <span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"n\">initialize_parameters_he</span><span class=\"p\">(</span><span class=\"n\">layers_dims</span><span class=\"p\">)</span>\n<span class=\"err\">â€‹</span>\n    <span class=\"c1\"># Loop (gradient descent)</span>\n<span class=\"err\">â€‹</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">num_iterations</span><span class=\"p\">):</span>\n<span class=\"err\">â€‹</span>\n        <span class=\"c1\"># Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID.</span>\n        <span class=\"n\">a3</span><span class=\"p\">,</span> <span class=\"n\">cache</span> <span class=\"o\">=</span> <span class=\"n\">forward_propagation</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">parameters</span><span class=\"p\">)</span>\n        \n        <span class=\"c1\"># Loss</span>\n        <span class=\"n\">cost</span> <span class=\"o\">=</span> <span class=\"n\">compute_loss</span><span class=\"p\">(</span><span class=\"n\">a3</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">)</span>\n<span class=\"err\">â€‹</span>\n        <span class=\"c1\"># Backward propagation.</span>\n        <span class=\"n\">grads</span> <span class=\"o\">=</span> <span class=\"n\">backward_propagation</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">cache</span><span class=\"p\">)</span>\n        \n        <span class=\"c1\"># Update parameters.</span>\n        <span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"n\">update_parameters</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">grads</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span><span class=\"p\">)</span>\n        \n        <span class=\"c1\"># Print the loss every 1000 iterations</span>\n        <span class=\"k\">if</span> <span class=\"n\">print_cost</span> <span class=\"ow\">and</span> <span class=\"n\">i</span> <span class=\"o\">%</span> <span class=\"mi\">1000</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n            <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s2\">&#34;Cost after iteration {}: {}&#34;</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">cost</span><span class=\"p\">))</span>\n            <span class=\"n\">costs</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">cost</span><span class=\"p\">)</span>\n            \n    <span class=\"c1\"># plot the loss</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">costs</span><span class=\"p\">)</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">ylabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;cost&#39;</span><span class=\"p\">)</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">xlabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;iterations (per hundreds)&#39;</span><span class=\"p\">)</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">title</span><span class=\"p\">(</span><span class=\"s2\">&#34;Learning rate =&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">learning_rate</span><span class=\"p\">))</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">parameters</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><b>1. Zero Initialization</b></p><p>æŠŠå‚æ•°å…¨éƒ½ç½®ä½0ï¼Œç»“æœæ˜¯æ˜¾è€Œæ˜“è§çš„ï¼Œå°±æ˜¯æ²¡æœ‰ä»»ä½•å˜åŒ–ã€‚</p><p><b>2. Random initialization</b></p><p>æŠŠWå‚æ•°éšæœºåŒ–äº†ï¼Œä½†æ˜¯ä¹˜ä»¥10å€ç³»æ•°ï¼Œæ‰€ä»¥å¯¼è‡´åˆå§‹åŒ–çš„å‚æ•°å¤ªå¤§ï¼Œæ”¶æ•›é€Ÿåº¦å¾ˆæ…¢</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">initialize_parameters_random</span><span class=\"p\">(</span><span class=\"n\">layers_dims</span><span class=\"p\">):</span>\n\n    <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">seed</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">)</span>               <span class=\"c1\"># This seed makes sure your &#34;random&#34; numbers will be the as ours</span>\n    <span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>\n    <span class=\"n\">L</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">layers_dims</span><span class=\"p\">)</span>            <span class=\"c1\"># integer representing the number of layers</span>\n    \n    <span class=\"k\">for</span> <span class=\"n\">l</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">L</span><span class=\"p\">):</span>\n        <span class=\"c1\">### START CODE HERE ### (â‰ˆ 2 lines of code)</span>\n        <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;W&#39;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">layers_dims</span><span class=\"p\">[</span><span class=\"n\">l</span><span class=\"p\">],</span> <span class=\"n\">layers_dims</span><span class=\"p\">[</span><span class=\"n\">l</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">])</span> <span class=\"o\">*</span> <span class=\"mi\">10</span>\n        <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;b&#39;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">layers_dims</span><span class=\"p\">[</span><span class=\"n\">l</span><span class=\"p\">],</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n<span class=\"err\">â€‹</span>\n    <span class=\"k\">return</span> <span class=\"n\">parameters</span></code></pre></div><p>ç»“æœä¸€èˆ¬èˆ¬</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-09f72f5f264d7bdcd5055c6f2ee8fae7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"451\" data-rawheight=\"278\" class=\"origin_image zh-lightbox-thumb\" width=\"451\" data-original=\"https://pic4.zhimg.com/v2-09f72f5f264d7bdcd5055c6f2ee8fae7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;451&#39; height=&#39;278&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"451\" data-rawheight=\"278\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"451\" data-original=\"https://pic4.zhimg.com/v2-09f72f5f264d7bdcd5055c6f2ee8fae7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-09f72f5f264d7bdcd5055c6f2ee8fae7_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>3. He initialization</b></p><p>æŠŠWå‚æ•°éšæœºåŒ–ï¼Œä½†æ˜¯ä¹˜ä¸Šç³»æ•° <code>sqrt(2./layers_dims[l-1])</code></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">initialize_parameters_he</span><span class=\"p\">(</span><span class=\"n\">layers_dims</span><span class=\"p\">):</span>\n    \n    <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">seed</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">)</span>\n    <span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>\n    <span class=\"n\">L</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">layers_dims</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"mi\">1</span> <span class=\"c1\"># integer representing the number of layers</span>\n     \n    <span class=\"k\">for</span> <span class=\"n\">l</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">L</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">):</span>\n        <span class=\"c1\">### START CODE HERE ### (â‰ˆ 2 lines of code)</span>\n        <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;W&#39;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">layers_dims</span><span class=\"p\">[</span><span class=\"n\">l</span><span class=\"p\">],</span><span class=\"n\">layers_dims</span><span class=\"p\">[</span><span class=\"n\">l</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">])</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sqrt</span><span class=\"p\">(</span><span class=\"mf\">2.</span><span class=\"o\">/</span><span class=\"n\">layers_dims</span><span class=\"p\">[</span><span class=\"n\">l</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">])</span>\n        <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;b&#39;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">layers_dims</span><span class=\"p\">[</span><span class=\"n\">l</span><span class=\"p\">],</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n        \n    <span class=\"k\">return</span> <span class=\"n\">parameters</span></code></pre></div><p>ç»“æœéå¸¸ç†æƒ³ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-e5a1cd4c69fb9a36cd5d628b2f20562d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"445\" data-rawheight=\"278\" class=\"origin_image zh-lightbox-thumb\" width=\"445\" data-original=\"https://pic2.zhimg.com/v2-e5a1cd4c69fb9a36cd5d628b2f20562d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;445&#39; height=&#39;278&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"445\" data-rawheight=\"278\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"445\" data-original=\"https://pic2.zhimg.com/v2-e5a1cd4c69fb9a36cd5d628b2f20562d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-e5a1cd4c69fb9a36cd5d628b2f20562d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>Part 2ï¼šRegularization</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p>æ•°æ®é›†ï¼š</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-e543fd04e7ea1d40a99727d33d180094_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"439\" data-rawheight=\"252\" class=\"origin_image zh-lightbox-thumb\" width=\"439\" data-original=\"https://pic1.zhimg.com/v2-e543fd04e7ea1d40a99727d33d180094_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;439&#39; height=&#39;252&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"439\" data-rawheight=\"252\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"439\" data-original=\"https://pic1.zhimg.com/v2-e543fd04e7ea1d40a99727d33d180094_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-e543fd04e7ea1d40a99727d33d180094_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>æ¨¡å‹å‡½æ•°ï¼š</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">model</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span> <span class=\"o\">=</span> <span class=\"mf\">0.3</span><span class=\"p\">,</span> <span class=\"n\">num_iterations</span> <span class=\"o\">=</span> <span class=\"mi\">30000</span><span class=\"p\">,</span> <span class=\"n\">print_cost</span> <span class=\"o\">=</span> <span class=\"bp\">True</span><span class=\"p\">,</span> <span class=\"n\">lambd</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">keep_prob</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">):</span>\n\n        \n    <span class=\"n\">grads</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>\n    <span class=\"n\">costs</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>                            <span class=\"c1\"># to keep track of the cost</span>\n    <span class=\"n\">m</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span>                        <span class=\"c1\"># number of examples</span>\n    <span class=\"n\">layers_dims</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span>\n    \n    <span class=\"c1\"># Initialize parameters dictionary.</span>\n    <span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"n\">initialize_parameters</span><span class=\"p\">(</span><span class=\"n\">layers_dims</span><span class=\"p\">)</span>\n<span class=\"err\">â€‹</span>\n    <span class=\"c1\"># Loop (gradient descent)</span>\n<span class=\"err\">â€‹</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">num_iterations</span><span class=\"p\">):</span>\n<span class=\"err\">â€‹</span>\n        <span class=\"c1\"># Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID.</span>\n        <span class=\"k\">if</span> <span class=\"n\">keep_prob</span> <span class=\"o\">==</span> <span class=\"mi\">1</span><span class=\"p\">:</span>\n            <span class=\"n\">a3</span><span class=\"p\">,</span> <span class=\"n\">cache</span> <span class=\"o\">=</span> <span class=\"n\">forward_propagation</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">parameters</span><span class=\"p\">)</span>\n        <span class=\"k\">elif</span> <span class=\"n\">keep_prob</span> <span class=\"o\">&lt;</span> <span class=\"mi\">1</span><span class=\"p\">:</span>\n            <span class=\"n\">a3</span><span class=\"p\">,</span> <span class=\"n\">cache</span> <span class=\"o\">=</span> <span class=\"n\">forward_propagation_with_dropout</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">keep_prob</span><span class=\"p\">)</span>\n        \n        <span class=\"c1\"># Cost function</span>\n        <span class=\"k\">if</span> <span class=\"n\">lambd</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n            <span class=\"n\">cost</span> <span class=\"o\">=</span> <span class=\"n\">compute_cost</span><span class=\"p\">(</span><span class=\"n\">a3</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">)</span>\n        <span class=\"k\">else</span><span class=\"p\">:</span>\n            <span class=\"n\">cost</span> <span class=\"o\">=</span> <span class=\"n\">compute_cost_with_regularization</span><span class=\"p\">(</span><span class=\"n\">a3</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">lambd</span><span class=\"p\">)</span>\n            \n        <span class=\"c1\"># Backward propagation.</span>\n        <span class=\"k\">assert</span><span class=\"p\">(</span><span class=\"n\">lambd</span><span class=\"o\">==</span><span class=\"mi\">0</span> <span class=\"ow\">or</span> <span class=\"n\">keep_prob</span><span class=\"o\">==</span><span class=\"mi\">1</span><span class=\"p\">)</span>    <span class=\"c1\"># it is possible to use both L2 regularization and dropout, </span>\n                                            <span class=\"c1\"># but this assignment will only explore one at a time</span>\n        <span class=\"k\">if</span> <span class=\"n\">lambd</span> <span class=\"o\">==</span> <span class=\"mi\">0</span> <span class=\"ow\">and</span> <span class=\"n\">keep_prob</span> <span class=\"o\">==</span> <span class=\"mi\">1</span><span class=\"p\">:</span>\n            <span class=\"n\">grads</span> <span class=\"o\">=</span> <span class=\"n\">backward_propagation</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">cache</span><span class=\"p\">)</span>\n        <span class=\"k\">elif</span> <span class=\"n\">lambd</span> <span class=\"o\">!=</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n            <span class=\"n\">grads</span> <span class=\"o\">=</span> <span class=\"n\">backward_propagation_with_regularization</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">cache</span><span class=\"p\">,</span> <span class=\"n\">lambd</span><span class=\"p\">)</span>\n        <span class=\"k\">elif</span> <span class=\"n\">keep_prob</span> <span class=\"o\">&lt;</span> <span class=\"mi\">1</span><span class=\"p\">:</span>\n            <span class=\"n\">grads</span> <span class=\"o\">=</span> <span class=\"n\">backward_propagation_with_dropout</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">cache</span><span class=\"p\">,</span> <span class=\"n\">keep_prob</span><span class=\"p\">)</span>\n        \n        <span class=\"c1\"># Update parameters.</span>\n        <span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"n\">update_parameters</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">grads</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span><span class=\"p\">)</span>\n        \n        <span class=\"c1\"># Print the loss every 10000 iterations</span>\n        <span class=\"k\">if</span> <span class=\"n\">print_cost</span> <span class=\"ow\">and</span> <span class=\"n\">i</span> <span class=\"o\">%</span> <span class=\"mi\">10000</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n            <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s2\">&#34;Cost after iteration {}: {}&#34;</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">cost</span><span class=\"p\">))</span>\n        <span class=\"k\">if</span> <span class=\"n\">print_cost</span> <span class=\"ow\">and</span> <span class=\"n\">i</span> <span class=\"o\">%</span> <span class=\"mi\">1000</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n            <span class=\"n\">costs</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">cost</span><span class=\"p\">)</span>\n    \n    <span class=\"c1\"># plot the cost</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">costs</span><span class=\"p\">)</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">ylabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;cost&#39;</span><span class=\"p\">)</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">xlabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;iterations (x1,000)&#39;</span><span class=\"p\">)</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">title</span><span class=\"p\">(</span><span class=\"s2\">&#34;Learning rate =&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">learning_rate</span><span class=\"p\">))</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">parameters</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>æ²¡æœ‰ä½¿ç”¨æ­£åˆ™åŒ–æ—¶ï¼Œæ•ˆæœï¼š</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-5da5b42cf43ea34651ee4d388491cf49_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"458\" data-rawheight=\"278\" class=\"origin_image zh-lightbox-thumb\" width=\"458\" data-original=\"https://pic2.zhimg.com/v2-5da5b42cf43ea34651ee4d388491cf49_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;458&#39; height=&#39;278&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"458\" data-rawheight=\"278\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"458\" data-original=\"https://pic2.zhimg.com/v2-5da5b42cf43ea34651ee4d388491cf49_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-5da5b42cf43ea34651ee4d388491cf49_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>L2 æ­£åˆ™</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p><b>è®¡ç®—ä»£ä»·å‡½æ•°</b></p><p><img src=\"https://www.zhihu.com/equation?tex=J_%7Bregularized%7D+%3D+%5Csmall+%5Cunderbrace%7B-%5Cfrac%7B1%7D%7Bm%7D+%5Csum%5Climits%7Bi+%3D+1%7D%5E%7Bm%7D+%5Clarge%7B%28%7D%5Csmall+y%5E%7B%28i%29%7D%5Clog%5Cleft%28a%5E%7B%5BL%5D+%28i%29%7D%5Cright%29+%2B+%281-y%5E%7B%28i%29%7D%29%5Clog%5Cleft%281-+a%5E%7B%5BL%5D+%28i%29%7D%5Cright%29+%5Clarge%7B%29%7D+%7D_%5Ctext%7Bcross-entropy+cost%7D+%2B+%5Cunderbrace%7B%5Cfrac%7B1%7D%7Bm%7D+%5Cfrac%7B%5Clambda%7D%7B2%7D+%5Csum%5Climits_l%5Csum%5Climits_k%5Csum%5Climits_j+W_%7Bk%2Cj%7D%5E%7B%5Bl%5D2%7D+%7D_%5Ctext%7BL2+regularization+cost%7D+\" alt=\"J_{regularized} = \\small \\underbrace{-\\frac{1}{m} \\sum\\limits{i = 1}^{m} \\large{(}\\small y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L] (i)}\\right) \\large{)} }_\\text{cross-entropy cost} + \\underbrace{\\frac{1}{m} \\frac{\\lambda}{2} \\sum\\limits_l\\sum\\limits_k\\sum\\limits_j W_{k,j}^{[l]2} }_\\text{L2 regularization cost} \" eeimg=\"1\"/> </p><p class=\"ztext-empty-paragraph\"><br/></p><p>å…¬å¼å·²ç»ç»™äº†ï¼Œåªè¦åŠ ä¸Šåé¢é‚£ä¸€é¡¹å°±å¯ä»¥äº†</p><p>ä½¿ç”¨<code>np.sum(np.square(Wl))</code>æ¥è®¡ç®— <img src=\"https://www.zhihu.com/equation?tex=%5Csum%5Climits_k%5Csum%5Climits_j+W_%7Bk%2Cj%7D%5E%7B%5Bl%5D2%7D\" alt=\"\\sum\\limits_k\\sum\\limits_j W_{k,j}^{[l]2}\" eeimg=\"1\"/> </p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: compute_cost_with_regularization</span>\n<span class=\"err\">â€‹</span>\n<span class=\"k\">def</span> <span class=\"nf\">compute_cost_with_regularization</span><span class=\"p\">(</span><span class=\"n\">A3</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">lambd</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Implement the cost function with L2 regularization. See formula (2) above.\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    A3 -- post-activation, output of forward propagation, of shape (output size, number of examples)\n</span><span class=\"s2\">    Y -- &#34;true&#34; labels vector, of shape (output size, number of examples)\n</span><span class=\"s2\">    parameters -- python dictionary containing parameters of the model\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    cost - value of the regularized loss function (formula (2))\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    <span class=\"n\">m</span> <span class=\"o\">=</span> <span class=\"n\">Y</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n    <span class=\"n\">W1</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;W1&#34;</span><span class=\"p\">]</span>\n    <span class=\"n\">W2</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;W2&#34;</span><span class=\"p\">]</span>\n    <span class=\"n\">W3</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;W3&#34;</span><span class=\"p\">]</span>\n    \n    <span class=\"n\">cross_entropy_cost</span> <span class=\"o\">=</span> <span class=\"n\">compute_cost</span><span class=\"p\">(</span><span class=\"n\">A3</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">)</span> <span class=\"c1\"># This gives you the cross-entropy part of the cost</span>\n    \n    <span class=\"c1\">### START CODE HERE ### (approx. 1 line)</span>\n    <span class=\"n\">L2_regularization_cost</span> <span class=\"o\">=</span> <span class=\"n\">lambd</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"mi\">2</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">square</span><span class=\"p\">(</span><span class=\"n\">W1</span><span class=\"p\">))</span> <span class=\"o\">+</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">square</span><span class=\"p\">(</span><span class=\"n\">W2</span><span class=\"p\">))</span> <span class=\"o\">+</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">square</span><span class=\"p\">(</span><span class=\"n\">W3</span><span class=\"p\">)))</span>\n    <span class=\"c1\">### END CODER HERE ###</span>\n    \n    <span class=\"n\">cost</span> <span class=\"o\">=</span> <span class=\"n\">cross_entropy_cost</span> <span class=\"o\">+</span> <span class=\"n\">L2_regularization_cost</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">cost</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><b>è®¡ç®—åå‘ä¼ æ’­å‡½æ•°</b></p><p>åœ¨ <img src=\"https://www.zhihu.com/equation?tex=dW\" alt=\"dW\" eeimg=\"1\"/> ä¸ŠåŠ ä¸Šäº†æ­£åˆ™é¡¹ <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Clambda%7D%7Bm%7D+W\" alt=\"\\frac{\\lambda}{m} W\" eeimg=\"1\"/> </p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: backward_propagation_with_regularization</span>\n<span class=\"err\">â€‹</span>\n<span class=\"k\">def</span> <span class=\"nf\">backward_propagation_with_regularization</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">cache</span><span class=\"p\">,</span> <span class=\"n\">lambd</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Implements the backward propagation of our baseline model to which we added an L2 regularization.\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    X -- input dataset, of shape (input size, number of examples)\n</span><span class=\"s2\">    Y -- &#34;true&#34; labels vector, of shape (output size, number of examples)\n</span><span class=\"s2\">    cache -- cache output from forward_propagation()\n</span><span class=\"s2\">    lambd -- regularization hyperparameter, scalar\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"n\">m</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n    <span class=\"p\">(</span><span class=\"n\">Z1</span><span class=\"p\">,</span> <span class=\"n\">A1</span><span class=\"p\">,</span> <span class=\"n\">W1</span><span class=\"p\">,</span> <span class=\"n\">b1</span><span class=\"p\">,</span> <span class=\"n\">Z2</span><span class=\"p\">,</span> <span class=\"n\">A2</span><span class=\"p\">,</span> <span class=\"n\">W2</span><span class=\"p\">,</span> <span class=\"n\">b2</span><span class=\"p\">,</span> <span class=\"n\">Z3</span><span class=\"p\">,</span> <span class=\"n\">A3</span><span class=\"p\">,</span> <span class=\"n\">W3</span><span class=\"p\">,</span> <span class=\"n\">b3</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">cache</span>\n    \n    <span class=\"n\">dZ3</span> <span class=\"o\">=</span> <span class=\"n\">A3</span> <span class=\"o\">-</span> <span class=\"n\">Y</span>\n    \n    <span class=\"c1\">### START CODE HERE ### (approx. 1 line)</span>\n    <span class=\"n\">dW3</span> <span class=\"o\">=</span> <span class=\"mf\">1.</span><span class=\"o\">/</span><span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">dZ3</span><span class=\"p\">,</span> <span class=\"n\">A2</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">lambd</span> <span class=\"o\">/</span> <span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">W3</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    <span class=\"n\">db3</span> <span class=\"o\">=</span> <span class=\"mf\">1.</span><span class=\"o\">/</span><span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">dZ3</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">keepdims</span> <span class=\"o\">=</span> <span class=\"bp\">True</span><span class=\"p\">)</span>\n    \n    <span class=\"n\">dA2</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">W3</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">,</span> <span class=\"n\">dZ3</span><span class=\"p\">)</span>\n    <span class=\"n\">dZ2</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">multiply</span><span class=\"p\">(</span><span class=\"n\">dA2</span><span class=\"p\">,</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">int64</span><span class=\"p\">(</span><span class=\"n\">A2</span> <span class=\"o\">&gt;</span> <span class=\"mi\">0</span><span class=\"p\">))</span>\n    <span class=\"c1\">### START CODE HERE ### (approx. 1 line)</span>\n    <span class=\"n\">dW2</span> <span class=\"o\">=</span> <span class=\"mf\">1.</span><span class=\"o\">/</span><span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">dZ2</span><span class=\"p\">,</span> <span class=\"n\">A1</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">lambd</span> <span class=\"o\">/</span> <span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">W2</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    <span class=\"n\">db2</span> <span class=\"o\">=</span> <span class=\"mf\">1.</span><span class=\"o\">/</span><span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">dZ2</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">keepdims</span> <span class=\"o\">=</span> <span class=\"bp\">True</span><span class=\"p\">)</span>\n    \n    <span class=\"n\">dA1</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">W2</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">,</span> <span class=\"n\">dZ2</span><span class=\"p\">)</span>\n    <span class=\"n\">dZ1</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">multiply</span><span class=\"p\">(</span><span class=\"n\">dA1</span><span class=\"p\">,</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">int64</span><span class=\"p\">(</span><span class=\"n\">A1</span> <span class=\"o\">&gt;</span> <span class=\"mi\">0</span><span class=\"p\">))</span>\n    <span class=\"c1\">### START CODE HERE ### (approx. 1 line)</span>\n    <span class=\"n\">dW1</span> <span class=\"o\">=</span> <span class=\"mf\">1.</span><span class=\"o\">/</span><span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">dZ1</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">lambd</span> <span class=\"o\">/</span> <span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">W1</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    <span class=\"n\">db1</span> <span class=\"o\">=</span> <span class=\"mf\">1.</span><span class=\"o\">/</span><span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">dZ1</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">keepdims</span> <span class=\"o\">=</span> <span class=\"bp\">True</span><span class=\"p\">)</span>\n    \n    <span class=\"n\">gradients</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">&#34;dZ3&#34;</span><span class=\"p\">:</span> <span class=\"n\">dZ3</span><span class=\"p\">,</span> <span class=\"s2\">&#34;dW3&#34;</span><span class=\"p\">:</span> <span class=\"n\">dW3</span><span class=\"p\">,</span> <span class=\"s2\">&#34;db3&#34;</span><span class=\"p\">:</span> <span class=\"n\">db3</span><span class=\"p\">,</span><span class=\"s2\">&#34;dA2&#34;</span><span class=\"p\">:</span> <span class=\"n\">dA2</span><span class=\"p\">,</span>\n                 <span class=\"s2\">&#34;dZ2&#34;</span><span class=\"p\">:</span> <span class=\"n\">dZ2</span><span class=\"p\">,</span> <span class=\"s2\">&#34;dW2&#34;</span><span class=\"p\">:</span> <span class=\"n\">dW2</span><span class=\"p\">,</span> <span class=\"s2\">&#34;db2&#34;</span><span class=\"p\">:</span> <span class=\"n\">db2</span><span class=\"p\">,</span> <span class=\"s2\">&#34;dA1&#34;</span><span class=\"p\">:</span> <span class=\"n\">dA1</span><span class=\"p\">,</span> \n                 <span class=\"s2\">&#34;dZ1&#34;</span><span class=\"p\">:</span> <span class=\"n\">dZ1</span><span class=\"p\">,</span> <span class=\"s2\">&#34;dW1&#34;</span><span class=\"p\">:</span> <span class=\"n\">dW1</span><span class=\"p\">,</span> <span class=\"s2\">&#34;db1&#34;</span><span class=\"p\">:</span> <span class=\"n\">db1</span><span class=\"p\">}</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">gradients</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>åŠ ä¸ŠL2æ­£åˆ™é¡¹åï¼Œæ•ˆæœå¾ˆæ˜æ˜¾ï¼š</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-f00ebbee0f00069740d67a2bc90518ca_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"458\" data-rawheight=\"278\" class=\"origin_image zh-lightbox-thumb\" width=\"458\" data-original=\"https://pic3.zhimg.com/v2-f00ebbee0f00069740d67a2bc90518ca_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;458&#39; height=&#39;278&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"458\" data-rawheight=\"278\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"458\" data-original=\"https://pic3.zhimg.com/v2-f00ebbee0f00069740d67a2bc90518ca_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-f00ebbee0f00069740d67a2bc90518ca_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>dropout</b></h2><p>åœ¨æ¯ä¸€æ¬¡è¿­ä»£ä¸­ï¼Œéƒ½éšæœºåˆ é™¤ä¸€å®šæ¦‚ç‡çš„neuronsã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>1. Forward propagation with dropout</b></p><p>åˆ†4æ­¥:</p><ol><li>æ¯ä¸€å±‚çš„ <img src=\"https://www.zhihu.com/equation?tex=d%5E%7B%5Bl%5D%7D\" alt=\"d^{[l]}\" eeimg=\"1\"/> å¯¹åº”æ¯ä¸€å±‚çš„ <img src=\"https://www.zhihu.com/equation?tex=a%5E%7B%5Bl%5D%7D\" alt=\"a^{[l]}\" eeimg=\"1\"/> ,å› ä¸ºæœ‰mä¸ªæ ·æœ¬ï¼Œæ‰€ä»¥å°±æœ‰ <img src=\"https://www.zhihu.com/equation?tex=D%5E%7B%5B1%5D%7D+%3D+%5Bd%5E%7B%5B1%5D%281%29%7D+d%5E%7B%5B1%5D%282%29%7D+...+d%5E%7B%5B1%5D%28m%29%7D%5D+\" alt=\"D^{[1]} = [d^{[1](1)} d^{[1](2)} ... d^{[1](m)}] \" eeimg=\"1\"/> of the same dimension as <img src=\"https://www.zhihu.com/equation?tex=A%5E%7B%5B1%5D%7D\" alt=\"A^{[1]}\" eeimg=\"1\"/> .ä½¿ç”¨np.random.rand(n,m)</li><li>å°† <img src=\"https://www.zhihu.com/equation?tex=D%5E%7B%5Bl%5D%7D\" alt=\"D^{[l]}\" eeimg=\"1\"/> å¸ƒå°”åŒ–ï¼Œ <img src=\"https://www.zhihu.com/equation?tex=+%3C+keepprob\" alt=\" &lt; keepprob\" eeimg=\"1\"/> åˆ†ä¸º 1å’Œ0</li><li>Set <img src=\"https://www.zhihu.com/equation?tex=A%5E%7B%5B1%5D%7D\" alt=\"A^{[1]}\" eeimg=\"1\"/> to <img src=\"https://www.zhihu.com/equation?tex=A%5E%7B%5B1%5D%7D+%2A+D%5E%7B%5B1%5D%7D\" alt=\"A^{[1]} * D^{[1]}\" eeimg=\"1\"/> .</li><li>Divide <img src=\"https://www.zhihu.com/equation?tex=A%5E%7B%5B1%5D%7D\" alt=\"A^{[1]}\" eeimg=\"1\"/> by <code>keep_prob</code>.</li></ol><p class=\"ztext-empty-paragraph\"><br/></p><p>è®°å¾—ç”¨cacheæŠŠæ¯ä¸€å±‚çš„Déƒ½è®°å½•ä¸‹æ¥</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: forward_propagation_with_dropout</span>\n<span class=\"err\">â€‹</span>\n<span class=\"k\">def</span> <span class=\"nf\">forward_propagation_with_dropout</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">keep_prob</span> <span class=\"o\">=</span> <span class=\"mf\">0.5</span><span class=\"p\">):</span>\n  \n    <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">seed</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n    \n    <span class=\"c1\"># retrieve parameters</span>\n    <span class=\"n\">W1</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;W1&#34;</span><span class=\"p\">]</span>\n    <span class=\"n\">b1</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;b1&#34;</span><span class=\"p\">]</span>\n    <span class=\"n\">W2</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;W2&#34;</span><span class=\"p\">]</span>\n    <span class=\"n\">b2</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;b2&#34;</span><span class=\"p\">]</span>\n    <span class=\"n\">W3</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;W3&#34;</span><span class=\"p\">]</span>\n    <span class=\"n\">b3</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;b3&#34;</span><span class=\"p\">]</span>\n    \n    <span class=\"c1\"># LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID</span>\n    <span class=\"n\">Z1</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">W1</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">b1</span>\n    <span class=\"n\">A1</span> <span class=\"o\">=</span> <span class=\"n\">relu</span><span class=\"p\">(</span><span class=\"n\">Z1</span><span class=\"p\">)</span>\n    <span class=\"c1\">### START CODE HERE ### (approx. 4 lines)         # Steps 1-4 below correspond to the Steps 1-4 described above. </span>\n    <span class=\"n\">D1</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"n\">A1</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">A1</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">])</span>                                         <span class=\"c1\"># Step 1: initialize matrix D1 = np.random.rand(..., ...)</span>\n    <span class=\"n\">D1</span> <span class=\"o\">=</span> <span class=\"n\">D1</span> <span class=\"o\">&lt;</span> <span class=\"n\">keep_prob</span>                                         <span class=\"c1\"># Step 2: convert entries of D1 to 0 or 1 (using keep_prob as the threshold)</span>\n    <span class=\"n\">A1</span> <span class=\"o\">=</span> <span class=\"n\">A1</span> <span class=\"o\">*</span> <span class=\"n\">D1</span>                                         <span class=\"c1\"># Step 3: shut down some neurons of A1</span>\n    <span class=\"n\">A1</span> <span class=\"o\">=</span> <span class=\"n\">A1</span> <span class=\"o\">/</span> <span class=\"n\">keep_prob</span>                                         <span class=\"c1\"># Step 4: scale the value of neurons that haven&#39;t been shut down</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    <span class=\"n\">Z2</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">W2</span><span class=\"p\">,</span> <span class=\"n\">A1</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">b2</span>\n    <span class=\"n\">A2</span> <span class=\"o\">=</span> <span class=\"n\">relu</span><span class=\"p\">(</span><span class=\"n\">Z2</span><span class=\"p\">)</span>\n    <span class=\"c1\">### START CODE HERE ### (approx. 4 lines)</span>\n    <span class=\"n\">D2</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"n\">A2</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">A2</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">])</span>                                              <span class=\"c1\"># Step 1: initialize matrix D2 = np.random.rand(..., ...)</span>\n    <span class=\"n\">D2</span> <span class=\"o\">=</span> <span class=\"n\">D2</span> <span class=\"o\">&lt;</span> <span class=\"n\">keep_prob</span>                                         <span class=\"c1\"># Step 2: convert entries of D2 to 0 or 1 (using keep_prob as the threshold)</span>\n    <span class=\"n\">A2</span> <span class=\"o\">=</span> <span class=\"n\">A2</span> <span class=\"o\">*</span> <span class=\"n\">D2</span>                                               <span class=\"c1\"># Step 3: shut down some neurons of A2</span>\n    <span class=\"n\">A2</span> <span class=\"o\">=</span> <span class=\"n\">A2</span> <span class=\"o\">/</span> <span class=\"n\">keep_prob</span>                                         <span class=\"c1\"># Step 4: scale the value of neurons that haven&#39;t been shut down</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    <span class=\"n\">Z3</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">W3</span><span class=\"p\">,</span> <span class=\"n\">A2</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">b3</span>\n    <span class=\"n\">A3</span> <span class=\"o\">=</span> <span class=\"n\">sigmoid</span><span class=\"p\">(</span><span class=\"n\">Z3</span><span class=\"p\">)</span>\n    \n    <span class=\"n\">cache</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">Z1</span><span class=\"p\">,</span> <span class=\"n\">D1</span><span class=\"p\">,</span> <span class=\"n\">A1</span><span class=\"p\">,</span> <span class=\"n\">W1</span><span class=\"p\">,</span> <span class=\"n\">b1</span><span class=\"p\">,</span> <span class=\"n\">Z2</span><span class=\"p\">,</span> <span class=\"n\">D2</span><span class=\"p\">,</span> <span class=\"n\">A2</span><span class=\"p\">,</span> <span class=\"n\">W2</span><span class=\"p\">,</span> <span class=\"n\">b2</span><span class=\"p\">,</span> <span class=\"n\">Z3</span><span class=\"p\">,</span> <span class=\"n\">A3</span><span class=\"p\">,</span> <span class=\"n\">W3</span><span class=\"p\">,</span> <span class=\"n\">b3</span><span class=\"p\">)</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">A3</span><span class=\"p\">,</span> <span class=\"n\">cache</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><b>2. Backward propagation with dropout</b></p><ol><li>reapplying the same mask <img src=\"https://www.zhihu.com/equation?tex=D%5E%7B%5B1%5D%7D\" alt=\"D^{[1]}\" eeimg=\"1\"/> to <code>dA1</code>. </li><li>divide <code>dA1</code> by <code>keep_prob</code></li></ol><p>åå‘ä¼ æ’­çš„æ—¶å€™ï¼Œè®©ä¹‹å‰çš„åˆ é™¤çš„neuronsä¾æ—§å½’0ï¼Œç„¶åä¹Ÿè¦é™¤ä»¥keepprobï¼Œå› ä¸º<code>dA = np.dot(W.T, dZ)</code>ï¼Œå¹¶æ²¡æœ‰é‡å¤é™¤ä»¥è¿‡ç³»æ•°ã€‚</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: backward_propagation_with_dropout</span>\n<span class=\"err\">â€‹</span>\n<span class=\"k\">def</span> <span class=\"nf\">backward_propagation_with_dropout</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">cache</span><span class=\"p\">,</span> <span class=\"n\">keep_prob</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Implements the backward propagation of our baseline model to which we added dropout.\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    X -- input dataset, of shape (2, number of examples)\n</span><span class=\"s2\">    Y -- &#34;true&#34; labels vector, of shape (output size, number of examples)\n</span><span class=\"s2\">    cache -- cache output from forward_propagation_with_dropout()\n</span><span class=\"s2\">    keep_prob - probability of keeping a neuron active during drop-out, scalar\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"n\">m</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n    <span class=\"p\">(</span><span class=\"n\">Z1</span><span class=\"p\">,</span> <span class=\"n\">D1</span><span class=\"p\">,</span> <span class=\"n\">A1</span><span class=\"p\">,</span> <span class=\"n\">W1</span><span class=\"p\">,</span> <span class=\"n\">b1</span><span class=\"p\">,</span> <span class=\"n\">Z2</span><span class=\"p\">,</span> <span class=\"n\">D2</span><span class=\"p\">,</span> <span class=\"n\">A2</span><span class=\"p\">,</span> <span class=\"n\">W2</span><span class=\"p\">,</span> <span class=\"n\">b2</span><span class=\"p\">,</span> <span class=\"n\">Z3</span><span class=\"p\">,</span> <span class=\"n\">A3</span><span class=\"p\">,</span> <span class=\"n\">W3</span><span class=\"p\">,</span> <span class=\"n\">b3</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">cache</span>\n    \n    <span class=\"n\">dZ3</span> <span class=\"o\">=</span> <span class=\"n\">A3</span> <span class=\"o\">-</span> <span class=\"n\">Y</span>\n    <span class=\"n\">dW3</span> <span class=\"o\">=</span> <span class=\"mf\">1.</span><span class=\"o\">/</span><span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">dZ3</span><span class=\"p\">,</span> <span class=\"n\">A2</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">)</span>\n    <span class=\"n\">db3</span> <span class=\"o\">=</span> <span class=\"mf\">1.</span><span class=\"o\">/</span><span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">dZ3</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">keepdims</span> <span class=\"o\">=</span> <span class=\"bp\">True</span><span class=\"p\">)</span>\n    <span class=\"n\">dA2</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">W3</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">,</span> <span class=\"n\">dZ3</span><span class=\"p\">)</span>\n    <span class=\"c1\">### START CODE HERE ### (â‰ˆ 2 lines of code)</span>\n    <span class=\"n\">dA2</span> <span class=\"o\">=</span> <span class=\"n\">dA2</span> <span class=\"o\">*</span> <span class=\"n\">D2</span>              <span class=\"c1\"># Step 1: Apply mask D2 to shut down the same neurons as during the forward propagation</span>\n    <span class=\"n\">dA2</span> <span class=\"o\">=</span> <span class=\"n\">dA2</span> <span class=\"o\">/</span> <span class=\"n\">keep_prob</span>             <span class=\"c1\"># Step 2: Scale the value of neurons that haven&#39;t been shut down</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    <span class=\"n\">dZ2</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">multiply</span><span class=\"p\">(</span><span class=\"n\">dA2</span><span class=\"p\">,</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">int64</span><span class=\"p\">(</span><span class=\"n\">A2</span> <span class=\"o\">&gt;</span> <span class=\"mi\">0</span><span class=\"p\">))</span>\n    <span class=\"n\">dW2</span> <span class=\"o\">=</span> <span class=\"mf\">1.</span><span class=\"o\">/</span><span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">dZ2</span><span class=\"p\">,</span> <span class=\"n\">A1</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">)</span>\n    <span class=\"n\">db2</span> <span class=\"o\">=</span> <span class=\"mf\">1.</span><span class=\"o\">/</span><span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">dZ2</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">keepdims</span> <span class=\"o\">=</span> <span class=\"bp\">True</span><span class=\"p\">)</span>\n    \n    <span class=\"n\">dA1</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">W2</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">,</span> <span class=\"n\">dZ2</span><span class=\"p\">)</span>\n    <span class=\"c1\">### START CODE HERE ### (â‰ˆ 2 lines of code)</span>\n    <span class=\"n\">dA1</span> <span class=\"o\">=</span> <span class=\"n\">dA1</span> <span class=\"o\">*</span> <span class=\"n\">D1</span>              <span class=\"c1\"># Step 1: Apply mask D1 to shut down the same neurons as during the forward propagation</span>\n    <span class=\"n\">dA1</span> <span class=\"o\">=</span> <span class=\"n\">dA1</span> <span class=\"o\">/</span> <span class=\"n\">keep_prob</span>              <span class=\"c1\"># Step 2: Scale the value of neurons that haven&#39;t been shut down</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    <span class=\"n\">dZ1</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">multiply</span><span class=\"p\">(</span><span class=\"n\">dA1</span><span class=\"p\">,</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">int64</span><span class=\"p\">(</span><span class=\"n\">A1</span> <span class=\"o\">&gt;</span> <span class=\"mi\">0</span><span class=\"p\">))</span>\n    <span class=\"n\">dW1</span> <span class=\"o\">=</span> <span class=\"mf\">1.</span><span class=\"o\">/</span><span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">dZ1</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">)</span>\n    <span class=\"n\">db1</span> <span class=\"o\">=</span> <span class=\"mf\">1.</span><span class=\"o\">/</span><span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">dZ1</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">keepdims</span> <span class=\"o\">=</span> <span class=\"bp\">True</span><span class=\"p\">)</span>\n    \n    <span class=\"n\">gradients</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">&#34;dZ3&#34;</span><span class=\"p\">:</span> <span class=\"n\">dZ3</span><span class=\"p\">,</span> <span class=\"s2\">&#34;dW3&#34;</span><span class=\"p\">:</span> <span class=\"n\">dW3</span><span class=\"p\">,</span> <span class=\"s2\">&#34;db3&#34;</span><span class=\"p\">:</span> <span class=\"n\">db3</span><span class=\"p\">,</span><span class=\"s2\">&#34;dA2&#34;</span><span class=\"p\">:</span> <span class=\"n\">dA2</span><span class=\"p\">,</span>\n                 <span class=\"s2\">&#34;dZ2&#34;</span><span class=\"p\">:</span> <span class=\"n\">dZ2</span><span class=\"p\">,</span> <span class=\"s2\">&#34;dW2&#34;</span><span class=\"p\">:</span> <span class=\"n\">dW2</span><span class=\"p\">,</span> <span class=\"s2\">&#34;db2&#34;</span><span class=\"p\">:</span> <span class=\"n\">db2</span><span class=\"p\">,</span> <span class=\"s2\">&#34;dA1&#34;</span><span class=\"p\">:</span> <span class=\"n\">dA1</span><span class=\"p\">,</span> \n                 <span class=\"s2\">&#34;dZ1&#34;</span><span class=\"p\">:</span> <span class=\"n\">dZ1</span><span class=\"p\">,</span> <span class=\"s2\">&#34;dW1&#34;</span><span class=\"p\">:</span> <span class=\"n\">dW1</span><span class=\"p\">,</span> <span class=\"s2\">&#34;db1&#34;</span><span class=\"p\">:</span> <span class=\"n\">db1</span><span class=\"p\">}</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">gradients</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>æœ€ç»ˆç»“æœ,ä¹Ÿè¿˜ä¸é”™ï¼š</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-979ebcf9376cc925909b582a4431d264_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"458\" data-rawheight=\"278\" class=\"origin_image zh-lightbox-thumb\" width=\"458\" data-original=\"https://pic1.zhimg.com/v2-979ebcf9376cc925909b582a4431d264_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;458&#39; height=&#39;278&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"458\" data-rawheight=\"278\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"458\" data-original=\"https://pic1.zhimg.com/v2-979ebcf9376cc925909b582a4431d264_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-979ebcf9376cc925909b582a4431d264_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>æ³¨æ„ï¼š</p><ul><li>dropoutä¹Ÿæ˜¯æ­£åˆ™åŒ–çš„ä¸€ç§</li><li>è®­ç»ƒçš„æ—¶å€™ç”¨ï¼Œæµ‹è¯•çš„æ—¶å€™ä¸è¦ç”¨</li><li>åœ¨æ­£å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ä¸­éƒ½è¦ç”¨</li></ul><h2><b>Part3:Gradient Checking</b></h2><p>é¦–å…ˆå†™äº†ä¸€ç»´çš„checking</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: forward_propagation</span>\n<span class=\"err\">â€‹</span>\n<span class=\"k\">def</span> <span class=\"nf\">forward_propagation</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">theta</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Implement the linear forward propagation (compute J) presented in Figure 1 (J(theta) = theta * x)\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    x -- a real-valued input\n</span><span class=\"s2\">    theta -- our parameter, a real number as well\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    J -- the value of function J, computed using the formula J(theta) = theta * x\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"c1\">### START CODE HERE ### (approx. 1 line)</span>\n    <span class=\"n\">J</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">theta</span><span class=\"p\">,</span><span class=\"n\">x</span><span class=\"p\">)</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">J</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: backward_propagation</span>\n<span class=\"err\">â€‹</span>\n<span class=\"k\">def</span> <span class=\"nf\">backward_propagation</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">theta</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Computes the derivative of J with respect to theta (see Figure 1).\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    x -- a real-valued input\n</span><span class=\"s2\">    theta -- our parameter, a real number as well\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    dtheta -- the gradient of the cost with respect to theta\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"c1\">### START CODE HERE ### (approx. 1 line)</span>\n    <span class=\"n\">dtheta</span> <span class=\"o\">=</span> <span class=\"n\">x</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">dtheta</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>æ ¹æ®å…¬å¼ï¼š</p><p><img src=\"https://www.zhihu.com/equation?tex=+difference+%3D+%5Cfrac+%7B%5Cmid%5Cmid+grad+-+gradapprox+%5Cmid%5Cmid_2%7D%7B%5Cmid%5Cmid+grad+%5Cmid%5Cmid_2+%2B+%5Cmid%5Cmid+gradapprox+%5Cmid%5Cmid_2%7D+\" alt=\" difference = \\frac {\\mid\\mid grad - gradapprox \\mid\\mid_2}{\\mid\\mid grad \\mid\\mid_2 + \\mid\\mid gradapprox \\mid\\mid_2} \" eeimg=\"1\"/> </p><p>æ­¥éª¤æ˜¯ï¼š</p><ol><li><img src=\"https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%2B%7D+%3D+%5Ctheta+%2B+%5Cvarepsilon\" alt=\"\\theta^{+} = \\theta + \\varepsilon\" eeimg=\"1\"/> </li><li><img src=\"https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B-%7D+%3D+%5Ctheta+-+%5Cvarepsilon\" alt=\"\\theta^{-} = \\theta - \\varepsilon\" eeimg=\"1\"/> </li><li><img src=\"https://www.zhihu.com/equation?tex=J%5E%7B%2B%7D+%3D+J%28%5Ctheta%5E%7B%2B%7D%29\" alt=\"J^{+} = J(\\theta^{+})\" eeimg=\"1\"/> </li><li><img src=\"https://www.zhihu.com/equation?tex=J%5E%7B-%7D+%3D+J%28%5Ctheta%5E%7B-%7D%29\" alt=\"J^{-} = J(\\theta^{-})\" eeimg=\"1\"/> </li><li><img src=\"https://www.zhihu.com/equation?tex=gradapprox+%3D+%5Cfrac%7BJ%5E%7B%2B%7D+-+J%5E%7B-%7D%7D%7B2++%5Cvarepsilon%7D\" alt=\"gradapprox = \\frac{J^{+} - J^{-}}{2  \\varepsilon}\" eeimg=\"1\"/> </li></ol><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">gradient_check</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">theta</span><span class=\"p\">,</span> <span class=\"n\">epsilon</span> <span class=\"o\">=</span> <span class=\"mf\">1e-7</span><span class=\"p\">):</span>\n\n    <span class=\"c1\"># Compute gradapprox using left side of formula (1). epsilon is small enough, you don&#39;t need to worry about the limit.</span>\n    <span class=\"c1\">### START CODE HERE ### (approx. 5 lines)</span>\n    <span class=\"n\">thetaplus</span> <span class=\"o\">=</span> <span class=\"n\">theta</span> <span class=\"o\">+</span> <span class=\"n\">epsilon</span>                               <span class=\"c1\"># Step 1</span>\n    <span class=\"n\">thetaminus</span> <span class=\"o\">=</span> <span class=\"n\">theta</span> <span class=\"o\">-</span> <span class=\"n\">epsilon</span>                              <span class=\"c1\"># Step 2</span>\n    <span class=\"n\">J_plus</span> <span class=\"o\">=</span> <span class=\"n\">forward_propagation</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">thetaplus</span><span class=\"p\">)</span>                              <span class=\"c1\"># Step 3</span>\n    <span class=\"n\">J_minus</span> <span class=\"o\">=</span> <span class=\"n\">forward_propagation</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">thetaminus</span><span class=\"p\">)</span>                                <span class=\"c1\"># Step 4</span>\n    <span class=\"n\">gradapprox</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">J_plus</span> <span class=\"o\">-</span> <span class=\"n\">J_minus</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"mi\">2</span> <span class=\"o\">*</span> <span class=\"n\">epsilon</span><span class=\"p\">)</span>                              <span class=\"c1\"># Step 5</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"c1\"># Check if gradapprox is close enough to the output of backward_propagation()</span>\n    <span class=\"c1\">### START CODE HERE ### (approx. 1 line)</span>\n    <span class=\"n\">grad</span> <span class=\"o\">=</span> <span class=\"n\">backward_propagation</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">theta</span><span class=\"p\">)</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"c1\">### START CODE HERE ### (approx. 1 line)</span>\n    <span class=\"n\">numerator</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linalg</span><span class=\"o\">.</span><span class=\"n\">norm</span><span class=\"p\">(</span><span class=\"n\">grad</span> <span class=\"o\">-</span> <span class=\"n\">gradapprox</span><span class=\"p\">)</span>                               <span class=\"c1\"># Step 1&#39;</span>\n    <span class=\"n\">denominator</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linalg</span><span class=\"o\">.</span><span class=\"n\">norm</span><span class=\"p\">(</span><span class=\"n\">grad</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linalg</span><span class=\"o\">.</span><span class=\"n\">norm</span><span class=\"p\">(</span><span class=\"n\">gradapprox</span><span class=\"p\">)</span>                             <span class=\"c1\"># Step 2&#39;</span>\n    <span class=\"n\">difference</span> <span class=\"o\">=</span> <span class=\"n\">numerator</span> <span class=\"o\">/</span> <span class=\"n\">denominator</span>                              <span class=\"c1\"># Step 3&#39;</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"k\">if</span> <span class=\"n\">difference</span> <span class=\"o\">&lt;</span> <span class=\"mf\">1e-7</span><span class=\"p\">:</span>\n        <span class=\"k\">print</span> <span class=\"p\">(</span><span class=\"s2\">&#34;The gradient is correct!&#34;</span><span class=\"p\">)</span>\n    <span class=\"k\">else</span><span class=\"p\">:</span>\n        <span class=\"k\">print</span> <span class=\"p\">(</span><span class=\"s2\">&#34;The gradient is wrong!&#34;</span><span class=\"p\">)</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">difference</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>åœ¨Nç»´çš„ç©ºé—´ä¸­ï¼Œ</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">forward_propagation_n</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">parameters</span><span class=\"p\">):</span>\n\n    <span class=\"c1\"># retrieve parameters</span>\n    <span class=\"n\">m</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n    <span class=\"n\">W1</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;W1&#34;</span><span class=\"p\">]</span>\n    <span class=\"n\">b1</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;b1&#34;</span><span class=\"p\">]</span>\n    <span class=\"n\">W2</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;W2&#34;</span><span class=\"p\">]</span>\n    <span class=\"n\">b2</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;b2&#34;</span><span class=\"p\">]</span>\n    <span class=\"n\">W3</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;W3&#34;</span><span class=\"p\">]</span>\n    <span class=\"n\">b3</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;b3&#34;</span><span class=\"p\">]</span>\n<span class=\"err\">â€‹</span>\n    <span class=\"c1\"># LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID</span>\n    <span class=\"n\">Z1</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">W1</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">b1</span>\n    <span class=\"n\">A1</span> <span class=\"o\">=</span> <span class=\"n\">relu</span><span class=\"p\">(</span><span class=\"n\">Z1</span><span class=\"p\">)</span>\n    <span class=\"n\">Z2</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">W2</span><span class=\"p\">,</span> <span class=\"n\">A1</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">b2</span>\n    <span class=\"n\">A2</span> <span class=\"o\">=</span> <span class=\"n\">relu</span><span class=\"p\">(</span><span class=\"n\">Z2</span><span class=\"p\">)</span>\n    <span class=\"n\">Z3</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">W3</span><span class=\"p\">,</span> <span class=\"n\">A2</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">b3</span>\n    <span class=\"n\">A3</span> <span class=\"o\">=</span> <span class=\"n\">sigmoid</span><span class=\"p\">(</span><span class=\"n\">Z3</span><span class=\"p\">)</span>\n<span class=\"err\">â€‹</span>\n    <span class=\"c1\"># Cost</span>\n    <span class=\"n\">logprobs</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">multiply</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">log</span><span class=\"p\">(</span><span class=\"n\">A3</span><span class=\"p\">),</span><span class=\"n\">Y</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">multiply</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">log</span><span class=\"p\">(</span><span class=\"mi\">1</span> <span class=\"o\">-</span> <span class=\"n\">A3</span><span class=\"p\">),</span> <span class=\"mi\">1</span> <span class=\"o\">-</span> <span class=\"n\">Y</span><span class=\"p\">)</span>\n    <span class=\"n\">cost</span> <span class=\"o\">=</span> <span class=\"mf\">1.</span><span class=\"o\">/</span><span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">logprobs</span><span class=\"p\">)</span>\n    \n    <span class=\"n\">cache</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">Z1</span><span class=\"p\">,</span> <span class=\"n\">A1</span><span class=\"p\">,</span> <span class=\"n\">W1</span><span class=\"p\">,</span> <span class=\"n\">b1</span><span class=\"p\">,</span> <span class=\"n\">Z2</span><span class=\"p\">,</span> <span class=\"n\">A2</span><span class=\"p\">,</span> <span class=\"n\">W2</span><span class=\"p\">,</span> <span class=\"n\">b2</span><span class=\"p\">,</span> <span class=\"n\">Z3</span><span class=\"p\">,</span> <span class=\"n\">A3</span><span class=\"p\">,</span> <span class=\"n\">W3</span><span class=\"p\">,</span> <span class=\"n\">b3</span><span class=\"p\">)</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">cost</span><span class=\"p\">,</span> <span class=\"n\">cache</span>\n<span class=\"k\">def</span> <span class=\"nf\">backward_propagation_n</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">cache</span><span class=\"p\">):</span>\n\n    \n    <span class=\"n\">m</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n    <span class=\"p\">(</span><span class=\"n\">Z1</span><span class=\"p\">,</span> <span class=\"n\">A1</span><span class=\"p\">,</span> <span class=\"n\">W1</span><span class=\"p\">,</span> <span class=\"n\">b1</span><span class=\"p\">,</span> <span class=\"n\">Z2</span><span class=\"p\">,</span> <span class=\"n\">A2</span><span class=\"p\">,</span> <span class=\"n\">W2</span><span class=\"p\">,</span> <span class=\"n\">b2</span><span class=\"p\">,</span> <span class=\"n\">Z3</span><span class=\"p\">,</span> <span class=\"n\">A3</span><span class=\"p\">,</span> <span class=\"n\">W3</span><span class=\"p\">,</span> <span class=\"n\">b3</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">cache</span>\n    \n    <span class=\"n\">dZ3</span> <span class=\"o\">=</span> <span class=\"n\">A3</span> <span class=\"o\">-</span> <span class=\"n\">Y</span>\n    <span class=\"n\">dW3</span> <span class=\"o\">=</span> <span class=\"mf\">1.</span><span class=\"o\">/</span><span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">dZ3</span><span class=\"p\">,</span> <span class=\"n\">A2</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">)</span>\n    <span class=\"n\">db3</span> <span class=\"o\">=</span> <span class=\"mf\">1.</span><span class=\"o\">/</span><span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">dZ3</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">keepdims</span> <span class=\"o\">=</span> <span class=\"bp\">True</span><span class=\"p\">)</span>\n    \n    <span class=\"n\">dA2</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">W3</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">,</span> <span class=\"n\">dZ3</span><span class=\"p\">)</span>\n    <span class=\"n\">dZ2</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">multiply</span><span class=\"p\">(</span><span class=\"n\">dA2</span><span class=\"p\">,</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">int64</span><span class=\"p\">(</span><span class=\"n\">A2</span> <span class=\"o\">&gt;</span> <span class=\"mi\">0</span><span class=\"p\">))</span>\n    <span class=\"n\">dW2</span> <span class=\"o\">=</span> <span class=\"mf\">1.</span><span class=\"o\">/</span><span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">dZ2</span><span class=\"p\">,</span> <span class=\"n\">A1</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"mi\">2</span>\n    <span class=\"n\">db2</span> <span class=\"o\">=</span> <span class=\"mf\">1.</span><span class=\"o\">/</span><span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">dZ2</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">keepdims</span> <span class=\"o\">=</span> <span class=\"bp\">True</span><span class=\"p\">)</span>\n    \n    <span class=\"n\">dA1</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">W2</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">,</span> <span class=\"n\">dZ2</span><span class=\"p\">)</span>\n    <span class=\"n\">dZ1</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">multiply</span><span class=\"p\">(</span><span class=\"n\">dA1</span><span class=\"p\">,</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">int64</span><span class=\"p\">(</span><span class=\"n\">A1</span> <span class=\"o\">&gt;</span> <span class=\"mi\">0</span><span class=\"p\">))</span>\n    <span class=\"n\">dW1</span> <span class=\"o\">=</span> <span class=\"mf\">1.</span><span class=\"o\">/</span><span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">dZ1</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">)</span>\n    <span class=\"n\">db1</span> <span class=\"o\">=</span> <span class=\"mf\">4.</span><span class=\"o\">/</span><span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">dZ1</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">keepdims</span> <span class=\"o\">=</span> <span class=\"bp\">True</span><span class=\"p\">)</span>\n    \n    <span class=\"n\">gradients</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">&#34;dZ3&#34;</span><span class=\"p\">:</span> <span class=\"n\">dZ3</span><span class=\"p\">,</span> <span class=\"s2\">&#34;dW3&#34;</span><span class=\"p\">:</span> <span class=\"n\">dW3</span><span class=\"p\">,</span> <span class=\"s2\">&#34;db3&#34;</span><span class=\"p\">:</span> <span class=\"n\">db3</span><span class=\"p\">,</span>\n                 <span class=\"s2\">&#34;dA2&#34;</span><span class=\"p\">:</span> <span class=\"n\">dA2</span><span class=\"p\">,</span> <span class=\"s2\">&#34;dZ2&#34;</span><span class=\"p\">:</span> <span class=\"n\">dZ2</span><span class=\"p\">,</span> <span class=\"s2\">&#34;dW2&#34;</span><span class=\"p\">:</span> <span class=\"n\">dW2</span><span class=\"p\">,</span> <span class=\"s2\">&#34;db2&#34;</span><span class=\"p\">:</span> <span class=\"n\">db2</span><span class=\"p\">,</span>\n                 <span class=\"s2\">&#34;dA1&#34;</span><span class=\"p\">:</span> <span class=\"n\">dA1</span><span class=\"p\">,</span> <span class=\"s2\">&#34;dZ1&#34;</span><span class=\"p\">:</span> <span class=\"n\">dZ1</span><span class=\"p\">,</span> <span class=\"s2\">&#34;dW1&#34;</span><span class=\"p\">:</span> <span class=\"n\">dW1</span><span class=\"p\">,</span> <span class=\"s2\">&#34;db1&#34;</span><span class=\"p\">:</span> <span class=\"n\">db1</span><span class=\"p\">}</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">gradients</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>è¿™ä¸ªæ—¶å€™ï¼Œç»™äº†ä¸¤ä¸ªå‡½æ•°ï¼Œå¯ä»¥åœ¨å­—å…¸å’Œå‘é‡ç»“æ„ç›¸äº’è½¬æ¢ï¼Œä¹Ÿå°±æ˜¯è¦è®¡ç®—$\\theta^{+}$æ—¶ï¼ŒæŠŠå­—å…¸è½¬ä¸ºå‘é‡ä¼šæ¯”è¾ƒå¥½è®¡ç®—ã€‚</p><div class=\"highlight\"><pre><code class=\"language-text\">dictionary_to_vector()\nvector_to_dictionary()</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-2226d0e9c32d65465ef44ae656e46aea_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1254\" data-rawheight=\"638\" class=\"origin_image zh-lightbox-thumb\" width=\"1254\" data-original=\"https://pic3.zhimg.com/v2-2226d0e9c32d65465ef44ae656e46aea_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1254&#39; height=&#39;638&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1254\" data-rawheight=\"638\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1254\" data-original=\"https://pic3.zhimg.com/v2-2226d0e9c32d65465ef44ae656e46aea_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-2226d0e9c32d65465ef44ae656e46aea_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>J_plus[i]å°±æ˜¯å‘é‡ä¸­çš„æ¯ä¸€ä¸ªå…ƒç´ ï¼Œä¹Ÿå°±æ˜¯W,bå±•å¼€ä¹‹åçš„æ¯ä¸€é¡¹å…ƒç´ </p><ul><li>To compute <code>J_plus[i]</code>:<br/></li></ul><ol><li>Set <img src=\"https://www.zhihu.com/equation?tex=+%5Ctheta%5E%7B%2B%7D\" alt=\" \\theta^{+}\" eeimg=\"1\"/> to <code>np.copy(parameters_values)</code></li><li>Set <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%2B%7D_i\" alt=\"\\theta^{+}_i\" eeimg=\"1\"/><i> to </i><img src=\"https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%2B%7D_i+%2B+%5Cvarepsilon\" alt=\"\\theta^{+}_i + \\varepsilon\" eeimg=\"1\"/> </li><li>Calculate <img src=\"https://www.zhihu.com/equation?tex=J%5E%7B%2B%7D_i\" alt=\"J^{+}_i\" eeimg=\"1\"/> using to <code>forward_propagation_n(x, y, vector_to_dictionary(</code> <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%2B%7D\" alt=\"\\theta^{+}\" eeimg=\"1\"/> <code>))</code>.     </li></ol><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>To compute <code>J_minus[i]</code>: do the same thing with <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B-%7D\" alt=\"\\theta^{-}\" eeimg=\"1\"/> </li><li>Compute <img src=\"https://www.zhihu.com/equation?tex=gradapprox%5Bi%5D+%3D+%5Cfrac%7BJ%5E%7B%2B%7D_i+-+J%5E%7B-%7D_i%7D%7B2+%5Cvarepsilon%7D\" alt=\"gradapprox[i] = \\frac{J^{+}_i - J^{-}_i}{2 \\varepsilon}\" eeimg=\"1\"/> </li></ul><p>ä»£ç å¦‚ä¸‹ï¼Œè®°ä½ thetaplusæ˜¯ä¸€ä¸ª(n,1)çš„å‘é‡ï¼Œå¾ªç¯è®¡ç®—æ¯ä¸€ä¸ªå‚æ•°çš„gradapproxï¼Œå†å’ŒåŸæœ¬çš„gradæ¯”è¾ƒï¼š</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: gradient_check_n</span>\n<span class=\"err\">â€‹</span>\n<span class=\"k\">def</span> <span class=\"nf\">gradient_check_n</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">gradients</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">epsilon</span> <span class=\"o\">=</span> <span class=\"mf\">1e-7</span><span class=\"p\">):</span>\n\n    <span class=\"c1\"># Set-up variables</span>\n    <span class=\"n\">parameters_values</span><span class=\"p\">,</span> <span class=\"n\">_</span> <span class=\"o\">=</span> <span class=\"n\">dictionary_to_vector</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">)</span>\n    <span class=\"n\">grad</span> <span class=\"o\">=</span> <span class=\"n\">gradients_to_vector</span><span class=\"p\">(</span><span class=\"n\">gradients</span><span class=\"p\">)</span>\n    <span class=\"n\">num_parameters</span> <span class=\"o\">=</span> <span class=\"n\">parameters_values</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n    <span class=\"n\">J_plus</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">num_parameters</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n    <span class=\"n\">J_minus</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">num_parameters</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n    <span class=\"n\">gradapprox</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">num_parameters</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n    \n    <span class=\"c1\"># Compute gradapprox</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">num_parameters</span><span class=\"p\">):</span>\n        \n        <span class=\"c1\"># Compute J_plus[i]. Inputs: &#34;parameters_values, epsilon&#34;. Output = &#34;J_plus[i]&#34;.</span>\n        <span class=\"c1\"># &#34;_&#34; is used because the function you have to outputs two parameters but we only care about the first one</span>\n        <span class=\"c1\">### START CODE HERE ### (approx. 3 lines)</span>\n        <span class=\"n\">thetaplus</span> <span class=\"o\">=</span>  <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">copy</span><span class=\"p\">(</span><span class=\"n\">parameters_values</span><span class=\"p\">)</span>                                      <span class=\"c1\"># Step 1</span>\n        <span class=\"n\">thetaplus</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">][</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">thetaplus</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">][</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"n\">epsilon</span>                                <span class=\"c1\"># Step 2</span>\n        <span class=\"n\">J_plus</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">],</span> <span class=\"n\">_</span> <span class=\"o\">=</span> <span class=\"n\">forward_propagation_n</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">vector_to_dictionary</span><span class=\"p\">(</span><span class=\"n\">thetaplus</span><span class=\"p\">))</span>                                   <span class=\"c1\"># Step 3</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n        \n        <span class=\"c1\"># Compute J_minus[i]. Inputs: &#34;parameters_values, epsilon&#34;. Output = &#34;J_minus[i]&#34;.</span>\n        <span class=\"c1\">### START CODE HERE ### (approx. 3 lines)</span>\n        <span class=\"n\">thetaminus</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">copy</span><span class=\"p\">(</span><span class=\"n\">parameters_values</span><span class=\"p\">)</span>                                        <span class=\"c1\"># Step 1</span>\n        <span class=\"n\">thetaminus</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">][</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">thetaminus</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">][</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">-</span> <span class=\"n\">epsilon</span>                               <span class=\"c1\"># Step 2        </span>\n        <span class=\"n\">J_minus</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">],</span> <span class=\"n\">_</span> <span class=\"o\">=</span> <span class=\"n\">forward_propagation_n</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">vector_to_dictionary</span><span class=\"p\">(</span><span class=\"n\">thetaminus</span><span class=\"p\">))</span>                                  <span class=\"c1\"># Step 3</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n        \n        <span class=\"c1\"># Compute gradapprox[i]</span>\n        <span class=\"c1\">### START CODE HERE ### (approx. 1 line)</span>\n        <span class=\"n\">gradapprox</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">J_plus</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span> <span class=\"o\">-</span> <span class=\"n\">J_minus</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">])</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"mi\">2</span> <span class=\"o\">*</span> <span class=\"n\">epsilon</span><span class=\"p\">)</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"c1\"># Compare gradapprox to backward propagation gradients by computing difference.</span>\n    <span class=\"c1\">### START CODE HERE ### (approx. 1 line)</span>\n    <span class=\"n\">numerator</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linalg</span><span class=\"o\">.</span><span class=\"n\">norm</span><span class=\"p\">(</span><span class=\"n\">grad</span> <span class=\"o\">-</span> <span class=\"n\">gradapprox</span><span class=\"p\">)</span>                                           <span class=\"c1\"># Step 1&#39;</span>\n    <span class=\"n\">denominator</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linalg</span><span class=\"o\">.</span><span class=\"n\">norm</span><span class=\"p\">(</span><span class=\"n\">grad</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linalg</span><span class=\"o\">.</span><span class=\"n\">norm</span><span class=\"p\">(</span><span class=\"n\">gradapprox</span><span class=\"p\">)</span>                                         <span class=\"c1\"># Step 2&#39;</span>\n    <span class=\"n\">difference</span> <span class=\"o\">=</span> <span class=\"n\">numerator</span> <span class=\"o\">/</span> <span class=\"n\">denominator</span>                                          <span class=\"c1\"># Step 3&#39;</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n<span class=\"err\">â€‹</span>\n    <span class=\"k\">if</span> <span class=\"n\">difference</span> <span class=\"o\">&gt;</span> <span class=\"mf\">2e-7</span><span class=\"p\">:</span>\n        <span class=\"k\">print</span> <span class=\"p\">(</span><span class=\"s2\">&#34;</span><span class=\"se\">\\033</span><span class=\"s2\">[93m&#34;</span> <span class=\"o\">+</span> <span class=\"s2\">&#34;There is a mistake in the backward propagation! difference = &#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">difference</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"s2\">&#34;</span><span class=\"se\">\\033</span><span class=\"s2\">[0m&#34;</span><span class=\"p\">)</span>\n    <span class=\"k\">else</span><span class=\"p\">:</span>\n        <span class=\"k\">print</span> <span class=\"p\">(</span><span class=\"s2\">&#34;</span><span class=\"se\">\\033</span><span class=\"s2\">[92m&#34;</span> <span class=\"o\">+</span> <span class=\"s2\">&#34;Your backward propagation works perfectly fine! difference = &#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">difference</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"s2\">&#34;</span><span class=\"se\">\\033</span><span class=\"s2\">[0m&#34;</span><span class=\"p\">)</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">difference</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>æ³¨æ„ï¼š</p><p>æ¢¯åº¦æ£€éªŒå¤ªæ…¢ï¼Œä¸è¦åœ¨è®­ç»ƒçš„æ—¶å€™è¿è¡Œï¼Œä½ è¿è¡Œåªæ˜¯ä¸ºäº†ä¿è¯ä½ çš„ç®—æ³•æ˜¯æ­£ç¡®çš„ã€‚</p>", 
            "topic": [
                {
                    "tag": "æ·±åº¦å­¦ä¹ ï¼ˆDeep Learningï¼‰", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "ç¥ç»ç½‘ç»œ", 
                    "tagLink": "https://api.zhihu.com/topics/19607065"
                }, 
                {
                    "tag": "æœºå™¨å­¦ä¹ ", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/44674876", 
            "userName": "ç›´ä¸Šäº‘éœ„", 
            "userLink": "https://www.zhihu.com/people/1033165ce4ad9c3fce69a0793dfab8ad", 
            "upvote": 0, 
            "title": "DeepLearning.aiç¬”è®°:(2-1) æ·±åº¦å­¦ä¹ å®è·µ", 
            "content": "<h2>æœ¬æ–‡é¦–å‘äºä¸ªäººåšå®¢ï¼š<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">fangzh.top</span><span class=\"invisible\"></span></a>ï¼Œæ¬¢è¿æ¥è®¿</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>ç¬¬äºŒé—¨è¯¾ä¸»è¦è®²çš„æ˜¯å¦‚ä½•æ”¹å–„ç¥ç»ç½‘ç»œï¼Œé€šè¿‡è¶…å‚æ•°çš„è°ƒè¯•ã€æ­£åˆ™åŒ–ä»¥åŠä¼˜åŒ–ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><p>ç¬¬ä¸€å‘¨ä¸»è¦æ˜¯è¯´äº†ä¸€äº›ä¹‹å‰æœºå™¨å­¦ä¹ é‡Œé¢æ¶‰åŠåˆ°çš„æ•°æ®é›†çš„åˆ’åˆ†ï¼Œä»¥åŠåˆå§‹åŒ–ï¼Œæ­£åˆ™åŒ–çš„æ–¹æ³•ï¼Œè¿˜æœ‰æ¢¯åº¦çš„éªŒè¯ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•é›†çš„åˆ’åˆ†</b></h2><p>è¿™äº›åœ¨ä¹‹å‰çš„æœºå™¨å­¦ä¹ è¯¾ç¨‹ä¸­éƒ½è®²è¿‡äº†ï¼Œè¿™é‡Œç®€å•è¯´ä¸€ä¸‹ã€‚</p><p>è®­ç»ƒé›†ä¹Ÿå°±æ˜¯ä½ è®­ç»ƒçš„æ ·æœ¬ï¼›éªŒè¯é›†æ˜¯ä½ è®­ç»ƒä¹‹åçš„å‚æ•°æ”¾åˆ°è¿™äº›æ•°æ®ä¸­åšéªŒè¯ï¼›è€Œæœ€ååšçš„æµ‹è¯•é›†åˆ™æ˜¯ç›¸å½“äºç”¨æ¥æœ€ç»ˆçš„æµ‹è¯•ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><p>ä¸€èˆ¬æ¥è¯´ï¼Œåˆ’åˆ†æ¯”ä¾‹ä¸º60%/20%/20%å°±å¯ä»¥äº†ï¼Œä½†æ˜¯å½“æ•°æ®è¶Šæ¥è¶Šå¤§ï¼Œå˜æˆä¸Šç™¾ä¸‡ï¼Œä¸Šåƒä¸‡çš„æ—¶å€™ï¼Œé‚£ä¹ˆéªŒè¯é›†å’Œæµ‹è¯•é›†å°±æ²¡å¿…è¦å é‚£ä¹ˆå¤§æ¯”é‡äº†ï¼Œå› ä¸ºå¤ªè¿‡æµªè´¹ï¼Œä¸€èˆ¬åœ¨0.5%-3%å·¦å³å°±å¯ä»¥ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><p>éœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒéªŒè¯é›†å’Œæµ‹è¯•é›†çš„æ•°æ®è¦æ¥æºç›¸åŒï¼ŒåŒåˆ†å¸ƒï¼Œä¹Ÿå°±æ˜¯åŒä¸€ç±»çš„æ•°æ®ï¼Œä¸èƒ½éªŒè¯é›†æ˜¯ç½‘ä¸Šçš„ï¼Œæµ‹è¯•é›†æ˜¯ä½ è‡ªå·±æ‹çš„ç…§ç‰‡ï¼Œè¿™æ ·è¯¯å·®ä¼šå¾ˆå¤§ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>bias and varianceï¼ˆåå·®å’Œæ–¹å·®ï¼‰</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-1db2dd95798106d95855fd9b1d3039f0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1444\" data-rawheight=\"402\" class=\"origin_image zh-lightbox-thumb\" width=\"1444\" data-original=\"https://pic1.zhimg.com/v2-1db2dd95798106d95855fd9b1d3039f0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1444&#39; height=&#39;402&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1444\" data-rawheight=\"402\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1444\" data-original=\"https://pic1.zhimg.com/v2-1db2dd95798106d95855fd9b1d3039f0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-1db2dd95798106d95855fd9b1d3039f0_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>high bias è¡¨ç¤ºçš„æ˜¯é«˜åå·®ï¼Œä¸€èˆ¬å‡ºç°åœ¨æ¬ æ‹Ÿåˆ(under fitting)çš„æƒ…å†µä¸‹ï¼Œ</p><p>high varianceè¡¨ç¤ºé«˜æ–¹å·®ï¼Œä¸€èˆ¬å‡ºç°åœ¨overfittingæƒ…å†µä¸‹ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><p>å¦‚ä½•è§£å†³å‘¢ï¼š</p><ul><li>high bias<br/></li><ul><li>æ›´å¤šçš„éšè—å±‚</li><li>æ¯ä¸€å±‚æ›´å¤šçš„ç¥ç»å…ƒ</li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>high variance<br/></li><ul><li>å¢åŠ æ•°æ®</li><li>æ­£åˆ™åŒ–</li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-984e1ac8d9676089bf44f169e0b9fff7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"831\" data-rawheight=\"250\" class=\"origin_image zh-lightbox-thumb\" width=\"831\" data-original=\"https://pic4.zhimg.com/v2-984e1ac8d9676089bf44f169e0b9fff7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;831&#39; height=&#39;250&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"831\" data-rawheight=\"250\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"831\" data-original=\"https://pic4.zhimg.com/v2-984e1ac8d9676089bf44f169e0b9fff7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-984e1ac8d9676089bf44f169e0b9fff7_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>ä»å·¦åˆ°å³4ç§æƒ…å†µå³æ˜¯ï¼š high variance ;  high bias ; high bias and high variance ; low bias and low variance</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>regularizationï¼ˆæ­£åˆ™åŒ–ï¼‰</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p>high varianceå¯ä»¥ä½¿ç”¨æ­£åˆ™åŒ–æ¥è§£å†³ã€‚</p><p>æˆ‘ä»¬çŸ¥é“ï¼Œåœ¨logistic regressionä¸­çš„æ­£åˆ™åŒ–é¡¹ï¼Œæ˜¯åœ¨æŸå¤±å‡½æ•°åé¢åŠ ä¸Šï¼š</p><p>L2 æ­£åˆ™ï¼š <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Clambda%7D%7B2m%7D%7C%7Cw%7C%7C%5E%7B2%7D_%7B2%7D+%3D+%5Cfrac%7B%5Clambda%7D%7B2m%7D%5Csum_%7Bj%3D1%7D%5E%7Bn_%7Bx%7D%7D%7B%7Cw%7C%7D+%3D++%5Cfrac%7B%5Clambda%7D%7B2m%7D+w%5ET+w\" alt=\"\\frac{\\lambda}{2m}||w||^{2}_{2} = \\frac{\\lambda}{2m}\\sum_{j=1}^{n_{x}}{|w|} =  \\frac{\\lambda}{2m} w^T w\" eeimg=\"1\"/> </p><p>L1æ­£åˆ™ï¼š <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Clambda%7D%7B2m%7D%7C%7Cw%7C%7C_%7B1%7D+%3D+%5Cfrac%7B%5Clambda%7D%7B2m%7D%5Csum_%7Bj%3D1%7D%5E%7Bn_%7Bx%7D%7D%7B%7Cw%7C%7D\" alt=\"\\frac{\\lambda}{2m}||w||_{1} = \\frac{\\lambda}{2m}\\sum_{j=1}^{n_{x}}{|w|}\" eeimg=\"1\"/> </p><p class=\"ztext-empty-paragraph\"><br/></p><p>ä¸€èˆ¬ç”¨L2æ­£åˆ™æ¥åšã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><p>åœ¨neural networkä¸­ï¼Œ</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-bc73b840a5eaac46c0d802c2f4beeec6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1027\" data-rawheight=\"270\" class=\"origin_image zh-lightbox-thumb\" width=\"1027\" data-original=\"https://pic3.zhimg.com/v2-bc73b840a5eaac46c0d802c2f4beeec6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1027&#39; height=&#39;270&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1027\" data-rawheight=\"270\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1027\" data-original=\"https://pic3.zhimg.com/v2-bc73b840a5eaac46c0d802c2f4beeec6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-bc73b840a5eaac46c0d802c2f4beeec6_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>å¯ä»¥çœ‹åˆ°åé¢çš„æ­£åˆ™å¼æ˜¯ä»ç¬¬1å±‚ç´¯åŠ åˆ°äº†ç¬¬Lå±‚çš„æ‰€æœ‰ç¥ç»ç½‘ç»œçš„æƒé‡ <img src=\"https://www.zhihu.com/equation?tex=%7C%7CW%5E%7B%5Bl%5D%7D%7C%7C_%7BF%7D\" alt=\"||W^{[l]}||_{F}\" eeimg=\"1\"/> çš„å¹³æ–¹ã€‚</p><p>è€Œæˆ‘ä»¬çŸ¥é“è¿™ä¸ªWæ˜¯ä¸€ä¸ª <img src=\"https://www.zhihu.com/equation?tex=n%5E%7B%5Bl%5D%7D+%5Ctimes+n%5E%7B%5Bl-1%5D%7D\" alt=\"n^{[l]} \\times n^{[l-1]}\" eeimg=\"1\"/> çš„çŸ©é˜µï¼Œé‚£ä¹ˆ</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-a40954b8ac725915422e9d3f44e0c688_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"845\" data-rawheight=\"103\" class=\"origin_image zh-lightbox-thumb\" width=\"845\" data-original=\"https://pic1.zhimg.com/v2-a40954b8ac725915422e9d3f44e0c688_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;845&#39; height=&#39;103&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"845\" data-rawheight=\"103\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"845\" data-original=\"https://pic1.zhimg.com/v2-a40954b8ac725915422e9d3f44e0c688_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-a40954b8ac725915422e9d3f44e0c688_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>å®ƒè¡¨ç¤ºçŸ©é˜µä¸­æ‰€æœ‰å…ƒç´ çš„å¹³æ–¹å’Œã€‚ä¹Ÿå°±è¿™ä¸€é¡¹åµŒå¥—äº†3å±‚çš„ <img src=\"https://www.zhihu.com/equation?tex=%5Csum\" alt=\"\\sum\" eeimg=\"1\"/> ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><p>é‚£ä¹ˆï¼Œå¦‚ä½•å®ç°è¿™ä¸ªèŒƒæ•°çš„æ¢¯åº¦ä¸‹é™å‘¢ï¼Ÿ</p><p class=\"ztext-empty-paragraph\"><br/></p><p>åœ¨åŸæœ¬çš„backpropä¸­,åŠ ä¸Šçš„æ­£åˆ™é¡¹çš„å¯¼æ•°ï¼Œ <img src=\"https://www.zhihu.com/equation?tex=dJ+%2F+dW\" alt=\"dJ / dW\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=dW%5E%7B%5Bl%5D%7D+%3D+%28form+backprop%29+%2B+%5Cfrac%7B%5Clambda%7D%7Bm%7DW%5E%7B%5Bl%5D%7D%24\" alt=\"dW^{[l]} = (form backprop) + \\frac{\\lambda}{m}W^{[l]}$\" eeimg=\"1\"/> </p><p>ä»£å…¥</p><p><img src=\"https://www.zhihu.com/equation?tex=W%5E%7B%5Bl%5D%7D+%3D+W%5E%7B%5Bl%5D%7D+-+%5Calpha+dW%5E%7B%5Bl%5D%7D\" alt=\"W^{[l]} = W^{[l]} - \\alpha dW^{[l]}\" eeimg=\"1\"/> </p><p>å¾—åˆ°ï¼š</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-d89a0c9eabf2a24af67adc918b08e6cb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"623\" data-rawheight=\"199\" class=\"origin_image zh-lightbox-thumb\" width=\"623\" data-original=\"https://pic4.zhimg.com/v2-d89a0c9eabf2a24af67adc918b08e6cb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;623&#39; height=&#39;199&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"623\" data-rawheight=\"199\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"623\" data-original=\"https://pic4.zhimg.com/v2-d89a0c9eabf2a24af67adc918b08e6cb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-d89a0c9eabf2a24af67adc918b08e6cb_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>å¯ä»¥çœ‹åˆ°ï¼Œ <img src=\"https://www.zhihu.com/equation?tex=%281+-+%5Cfrac%7B%5Calpha+%5Clambda%7D%7Bm%7D%29+%3C+1\" alt=\"(1 - \\frac{\\alpha \\lambda}{m}) &lt; 1\" eeimg=\"1\"/> ï¼Œæ‰€ä»¥æ¯ä¸€æ¬¡éƒ½ä¼šè®©Wå˜å°ï¼Œå› æ­¤L2èŒƒæ•°æ­£åˆ™åŒ–ä¹Ÿæˆä¸ºâ€œæƒé‡è¡°å‡â€</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>æ­£åˆ™åŒ–å¦‚ä½•é˜²æ­¢è¿‡æ‹Ÿåˆï¼Ÿ</b></h2><p>ç›´è§‚ç†è§£æ˜¯åœ¨ä»£ä»·å‡½æ•°åŠ å…¥æ­£åˆ™é¡¹åï¼Œå¦‚æœ <img src=\"https://www.zhihu.com/equation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"/> éå¸¸å¤§ï¼Œä¸ºäº†æ»¡è¶³ä»£ä»·å‡½æ•°æœ€å°åŒ–ï¼Œé‚£ä¹ˆ <img src=\"https://www.zhihu.com/equation?tex=w%5E%7B%5Bl%5D%7D\" alt=\"w^{[l]}\" eeimg=\"1\"/> è¿™ä¸€é¡¹å¿…é¡»éå¸¸æ¥è¿‘äº0ï¼Œæ‰€ä»¥å°±ç­‰ä»·äºå¾ˆå¤šç¥ç»å…ƒéƒ½æ²¡æœ‰ä½œç”¨äº†ï¼Œä»åŸæœ¬çš„éçº¿æ€§ç»“æ„å˜æˆäº†è¿‘ä¼¼çš„çº¿æ€§ç»“æ„ï¼Œè‡ªç„¶å°±ä¸ä¼šè¿‡æ‹Ÿåˆäº†ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-1e036e4ad60cc07f3e3def590ba68983_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1068\" data-rawheight=\"494\" class=\"origin_image zh-lightbox-thumb\" width=\"1068\" data-original=\"https://pic4.zhimg.com/v2-1e036e4ad60cc07f3e3def590ba68983_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1068&#39; height=&#39;494&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1068\" data-rawheight=\"494\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1068\" data-original=\"https://pic4.zhimg.com/v2-1e036e4ad60cc07f3e3def590ba68983_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-1e036e4ad60cc07f3e3def590ba68983_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>æˆ‘ä»¬å†æ¥ç›´è§‚æ„Ÿå—ä¸€ä¸‹ï¼Œ</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b427d2557912ed7fe1e99fe9adcb3399_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"484\" data-rawheight=\"231\" class=\"origin_image zh-lightbox-thumb\" width=\"484\" data-original=\"https://pic2.zhimg.com/v2-b427d2557912ed7fe1e99fe9adcb3399_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;484&#39; height=&#39;231&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"484\" data-rawheight=\"231\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"484\" data-original=\"https://pic2.zhimg.com/v2-b427d2557912ed7fe1e99fe9adcb3399_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-b427d2557912ed7fe1e99fe9adcb3399_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>å‡è®¾æ˜¯ä¸€ä¸ªtanh()å‡½æ•°ï¼Œé‚£ä¹ˆ <img src=\"https://www.zhihu.com/equation?tex=z+%3D+wx+%2B+b\" alt=\"z = wx + b\" eeimg=\"1\"/> ï¼Œå½“wéå¸¸æ¥è¿‘äº0æ—¶ï¼Œzä¹Ÿæ¥è¿‘äº0ï¼Œä¹Ÿå°±æ˜¯åœ¨åæ ‡è½´ä¸Š0é™„è¿‘èŒƒå›´å†…ï¼Œè¿™ä¸ªæ—¶å€™æ–œç‡æ¥è¿‘äºçº¿æ€§ï¼Œé‚£ä¹ˆæ•´ä¸ªç¥ç»ç½‘ç»œä¹Ÿéå¸¸æ¥è¿‘äºçº¿æ€§çš„ç½‘ç»œï¼Œé‚£ä¹ˆå°±ä¸ä¼šå‘ç”Ÿè¿‡æ‹Ÿåˆäº†ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>dropout æ­£åˆ™åŒ–</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p>dropout(éšæœºå¤±æ´»)ï¼Œä¹Ÿæ˜¯æ­£åˆ™åŒ–çš„ä¸€ç§ï¼Œé¡¾åæ€ä¹‰ï¼Œæ˜¯è®©ç¥ç»ç½‘ç»œä¸­çš„ç¥ç»å…ƒæŒ‰ç…§ä¸€å®šçš„æ¦‚ç‡éšæœºå¤±æ´»ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-109ec4d34be0e9742681e6fdc8626d15_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"964\" data-rawheight=\"374\" class=\"origin_image zh-lightbox-thumb\" width=\"964\" data-original=\"https://pic2.zhimg.com/v2-109ec4d34be0e9742681e6fdc8626d15_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;964&#39; height=&#39;374&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"964\" data-rawheight=\"374\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"964\" data-original=\"https://pic2.zhimg.com/v2-109ec4d34be0e9742681e6fdc8626d15_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-109ec4d34be0e9742681e6fdc8626d15_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>å®ç°dropoutï¼šinverted dropoutï¼ˆåå‘éšæœºå¤±æ´»ï¼‰</b></p><p>å®ç°dropoutæœ‰å¥½å‡ ç§ï¼Œä½†æ˜¯æœ€å¸¸ç”¨çš„è¿˜æ˜¯è¿™ä¸ªinverted dropout</p><p>å‡è®¾æ˜¯ä¸€ä¸ª3å±‚çš„ç¥ç»ç½‘ç»œï¼Œkeepprobè¡¨ç¤ºä¿ç•™èŠ‚ç‚¹çš„æ¦‚ç‡</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">keepprob</span> <span class=\"o\">=</span> <span class=\"mf\">0.8</span>\n<span class=\"c1\">#d3æ˜¯çŸ©é˜µï¼Œæ¯ä¸ªå…ƒç´ æœ‰true,false,åœ¨pythonä¸­ä»£è¡¨1å’Œ0</span>\n<span class=\"n\">d3</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"n\">a3</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span><span class=\"n\">a3</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">])</span> <span class=\"o\">&lt;</span> <span class=\"n\">keepprob</span>\n<span class=\"n\">a3</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">multiply</span><span class=\"p\">(</span><span class=\"n\">a3</span><span class=\"p\">,</span><span class=\"n\">d3</span><span class=\"p\">)</span>\n<span class=\"n\">a3</span> <span class=\"o\">/=</span> <span class=\"n\">keepprob</span></code></pre></div><p>å…¶ä¸­ç¬¬4å¼ <img src=\"https://www.zhihu.com/equation?tex=+a3+%2F%3D+keepprob\" alt=\" a3 /= keepprob\" eeimg=\"1\"/> </p><p>å‡è®¾ç¬¬ä¸‰å±‚æœ‰50ä¸ªç¥ç»å…ƒ a3.shape[0] = 50ï¼Œä¸€å…±æœ‰ <img src=\"https://www.zhihu.com/equation?tex=+50+%2A+m\" alt=\" 50 * m\" eeimg=\"1\"/> ç»´ï¼Œmæ˜¯æ ·æœ¬æ•°ï¼Œè¿™æ ·å­å°±ä¼šæœ‰å¹³å‡10ä¸ªç¥ç»å…ƒè¢«åˆ é™¤ï¼Œå› ä¸º <img src=\"https://www.zhihu.com/equation?tex=z%5E%7B%5B4%5D%7D+%3D+w%5E%7B%5B4%5D%7D+a%5E%7B%5B3%5D%7D+%2B+b%5E%7B%5B4%5D%7D\" alt=\"z^{[4]} = w^{[4]} a^{[3]} + b^{[4]}\" eeimg=\"1\"/> ï¼Œé‚£ä¹ˆè¿™ä¸ªæ—¶å€™ <img src=\"https://www.zhihu.com/equation?tex=z%5E%7B%5B4%5D%7D\" alt=\"z^{[4]}\" eeimg=\"1\"/> çš„æœŸæœ›å€¼å°±å°‘äº†20%,æ‰€ä»¥åœ¨æ¯ä¸ªç¥ç»å…ƒä¸Šéƒ½é™¤ä»¥keepprobçš„å€¼ï¼Œåˆšå¥½å¼¥è¡¥çš„ä¹‹å‰çš„æŸå¤±ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>æ³¨æ„</b></p><p>åœ¨testé˜¶æ®µï¼Œå°±ä¸éœ€è¦å†ä½¿ç”¨dropoutäº†ï¼Œè€Œæ˜¯åƒä¹‹å‰ä¸€æ ·ï¼Œç›´æ¥ä¹˜ä»¥å„ä¸ªå±‚çš„æƒé‡ï¼Œå¾—å‡ºé¢„æµ‹å€¼å°±å¯ä»¥ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>ç†è§£dropout</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p>ç›´è§‚ä¸Šï¼Œå› ä¸ºç¥ç»å…ƒæœ‰å¯èƒ½ä¼šè¢«éšæœºæ¸…é™¤ï¼Œè¿™æ ·å­åœ¨è®­ç»ƒä¸­ï¼Œå°±ä¸ä¼šè¿‡åˆ†ä¾èµ–æŸä¸€ä¸ªç¥ç»å…ƒæˆ–è€…ç‰¹å¾çš„æƒé‡ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><p>å½“ç„¶å¯ä»¥è®¾ç½®ä¸åŒå±‚æœ‰ä¸åŒçš„dropoutæ¦‚ç‡ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><p>è®¡ç®—æœºè§†è§‰é¢†åŸŸéå¸¸å–œæ¬¢ç”¨è¿™ä¸ªdropoutã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><p>ä½†æ˜¯è¿™ä¸ªä¸œè¥¿çš„ä¸€å¤§ç¼ºç‚¹å°±æ˜¯ä»£ä»·å‡½æ•°Jä¸èƒ½å†è¢«æ˜ç¡®å®šä¹‰ï¼Œæ¯æ¬¡éƒ½ä¼šéšæœºç§»é™¤ä¸€äº›èŠ‚ç‚¹ï¼Œæ‰€ä»¥å¾ˆéš¾è¿›è¡Œå¤æŸ¥ã€‚å¦‚æœéœ€è¦è°ƒè¯•çš„è¯ï¼Œé€šå¸¸ä¼šå…³é—­dropoutï¼Œè®¾ç½®ä¸º1ï¼Œè¿™æ ·å†æ¥debugã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>å½’ä¸€åŒ–</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p>å½’ä¸€åŒ–æ•°æ®å¯ä»¥åŠ é€Ÿç¥ç»ç½‘ç»œçš„è®­ç»ƒé€Ÿåº¦ã€‚</p><p>ä¸€èˆ¬æœ‰ä¸¤ä¸ªæ­¥éª¤ï¼š</p><ul><li>é›¶å‡å€¼</li><li>å½’ä¸€åŒ–æ–¹å·®</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-61cef71d0dd82c27f35cb74f8fc8531d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1085\" data-rawheight=\"552\" class=\"origin_image zh-lightbox-thumb\" width=\"1085\" data-original=\"https://pic2.zhimg.com/v2-61cef71d0dd82c27f35cb74f8fc8531d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1085&#39; height=&#39;552&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1085\" data-rawheight=\"552\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1085\" data-original=\"https://pic2.zhimg.com/v2-61cef71d0dd82c27f35cb74f8fc8531d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-61cef71d0dd82c27f35cb74f8fc8531d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>è¿™æ ·å­åœ¨gradientçš„æ—¶å€™å°±ä¼šèµ°çš„é¡ºç•…ä¸€ç‚¹ï¼š</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-5dccdcd69bdae5ce84499b85184c6235_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1074\" data-rawheight=\"601\" class=\"origin_image zh-lightbox-thumb\" width=\"1074\" data-original=\"https://pic2.zhimg.com/v2-5dccdcd69bdae5ce84499b85184c6235_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1074&#39; height=&#39;601&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1074\" data-rawheight=\"601\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1074\" data-original=\"https://pic2.zhimg.com/v2-5dccdcd69bdae5ce84499b85184c6235_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-5dccdcd69bdae5ce84499b85184c6235_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>å‚æ•°åˆå§‹åŒ–</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p>åˆç†çš„å‚æ•°åˆå§‹åŒ–å¯ä»¥æœ‰æ•ˆçš„åŠ å¿«ç¥ç»ç½‘ç»œçš„è®­ç»ƒé€Ÿåº¦ã€‚</p><p>ä¸€èˆ¬å‘¢ <img src=\"https://www.zhihu.com/equation?tex=z+%3D+w_1+x_1+%2B+w_2+x_2+%2B+...+%2B+w_n+x_n\" alt=\"z = w_1 x_1 + w_2 x_2 + ... + w_n x_n\" eeimg=\"1\"/> ï¼Œä¸€èˆ¬å¸Œæœ›zä¸è¦å¤ªå¤§ä¹Ÿä¸è¦å¤ªå°ã€‚æ‰€ä»¥å‘¢ï¼Œå¸Œæœ›nè¶Šå¤§ï¼Œwè¶Šå°æ‰å¥½ã€‚æœ€åˆç†çš„å°±æ˜¯æ–¹å·® <img src=\"https://www.zhihu.com/equation?tex=w+%3D+%5Cfrac%7B1%7D%7Bn%7D\" alt=\"w = \\frac{1}{n}\" eeimg=\"1\"/> ï¼Œæ‰€ä»¥ï¼š</p><div class=\"highlight\"><pre><code class=\"language-text\">WL = np.random.randn(WL.shape[0],WL.shape[1])* np.sqrt(1/n)</code></pre></div><p>è¿™ä¸ª <img src=\"https://www.zhihu.com/equation?tex=n\" alt=\"n\" eeimg=\"1\"/> å³ <img src=\"https://www.zhihu.com/equation?tex=n%5E%7B%5Bl-1%5D%7D\" alt=\"n^{[l-1]}\" eeimg=\"1\"/> </p><p class=\"ztext-empty-paragraph\"><br/></p><p>å¦‚æœæ˜¯reluå‡½æ•°ï¼Œ</p><p>é‚£ä¹ˆ <img src=\"https://www.zhihu.com/equation?tex=w+%3D+%5Cfrac%7B2%7D%7Bn%7D\" alt=\"w = \\frac{2}{n}\" eeimg=\"1\"/> æ¯”è¾ƒå¥½ï¼Œä¹Ÿå°±æ˜¯<code>np.sqrt(2/n)</code> </p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>æ¢¯åº¦çš„æ•°å€¼é€¼è¿‘</b></h2><p><img src=\"https://www.zhihu.com/equation?tex=+%5Cfrac%7B%5Cpartial+J%7D%7B%5Cpartial+%5Ctheta%7D+%3D+%5Clim_%7B%5Cvarepsilon+%5Cto+0%7D+%5Cfrac%7BJ%28%5Ctheta+%2B+%5Cvarepsilon%29+-+J%28%5Ctheta+-+%5Cvarepsilon%29%7D%7B2+%5Cvarepsilon%7D+\" alt=\" \\frac{\\partial J}{\\partial \\theta} = \\lim_{\\varepsilon \\to 0} \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon} \" eeimg=\"1\"/> </p><p>å¾®ç§¯åˆ†çš„å¸¸è¯†ï¼Œç”¨ <img src=\"https://www.zhihu.com/equation?tex=%5Cvarepsilon\" alt=\"\\varepsilon\" eeimg=\"1\"/> æ¥é€¼è¿‘æ¢¯åº¦ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>æ¢¯åº¦æ£€éªŒ</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p>ç”¨æ¢¯åº¦æ£€éªŒå¯ä»¥æ¥æ£€æŸ¥åœ¨åå‘ä¼ æ’­ä¸­çš„ç®—æ³•æœ‰æ²¡æœ‰é”™è¯¯ã€‚</p><p>è¿™ä¸ªæ—¶å€™ï¼Œå¯ä»¥æŠŠ <img src=\"https://www.zhihu.com/equation?tex=W%5E%7B%5B1%5D%7D%2Cb%5E%7B%5B1%5D%7D%2C......W%5E%7B%5Bl%5D%7D%2Cb%5E%7B%5Bl%5D%7D\" alt=\"W^{[1]},b^{[1]},......W^{[l]},b^{[l]}\" eeimg=\"1\"/> å˜æˆä¸€ä¸ªå‘é‡ï¼Œè¿™æ ·å¯ä»¥å¾—åˆ°ä¸€ä¸ªä»£ä»·å‡½æ•° <img src=\"https://www.zhihu.com/equation?tex=J%28%5Ctheta%29\" alt=\"J(\\theta)\" eeimg=\"1\"/> ï¼Œç„¶å <img src=\"https://www.zhihu.com/equation?tex=dW%2Cdb\" alt=\"dW,db\" eeimg=\"1\"/> ä¹Ÿå¯ä»¥è½¬æ¢æˆä¸€ä¸ªå‘é‡ï¼Œç”¨ <img src=\"https://www.zhihu.com/equation?tex=d%5Ctheta\" alt=\"d\\theta\" eeimg=\"1\"/> è¡¨ç¤ºï¼Œå’Œ <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> æœ‰ç›¸åŒçš„ç»´åº¦ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-52f7b041ea65aa9b935d40af8dd3a38c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1051\" data-rawheight=\"602\" class=\"origin_image zh-lightbox-thumb\" width=\"1051\" data-original=\"https://pic1.zhimg.com/v2-52f7b041ea65aa9b935d40af8dd3a38c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1051&#39; height=&#39;602&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1051\" data-rawheight=\"602\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1051\" data-original=\"https://pic1.zhimg.com/v2-52f7b041ea65aa9b935d40af8dd3a38c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-52f7b041ea65aa9b935d40af8dd3a38c_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>å†å¯¹æ¯ä¸€ä¸ª <img src=\"https://www.zhihu.com/equation?tex=d%5Ctheta_%7Bapprox%7D%5Bi%5D\" alt=\"d\\theta_{approx}[i]\" eeimg=\"1\"/> æ±‚ä¸Šé¢çš„åŒè¾¹æ¢¯åº¦é€¼è¿‘ï¼Œç„¶åä¹Ÿç”¨å¯¼æ•°æ±‚å¾—æ¯ä¸€ä¸ª <img src=\"https://www.zhihu.com/equation?tex=d%5Ctheta%5Bi%5D\" alt=\"d\\theta[i]\" eeimg=\"1\"/> ï¼Œç„¶åæ ¹æ®å›¾ä¸Šçš„cheakå…¬å¼ã€‚æ±‚æ¢¯åº¦é€¼è¿‘çš„æ—¶å€™ï¼Œè®¾ç½®ä¸¤è¾¹çš„ <img src=\"https://www.zhihu.com/equation?tex=%5Cvarepsilon+%3D+10%5E%7B-7%7D\" alt=\"\\varepsilon = 10^{-7}\" eeimg=\"1\"/> ï¼Œæœ€ç»ˆæ±‚å¾—çš„å€¼å¦‚æœæ˜¯ <img src=\"https://www.zhihu.com/equation?tex=10%5E%7B-7%7D\" alt=\"10^{-7}\" eeimg=\"1\"/> ï¼Œé‚£ä¹ˆå¾ˆæ­£å¸¸ï¼Œ <img src=\"https://www.zhihu.com/equation?tex=10%5E%7B-3%7D\" alt=\"10^{-3}\" eeimg=\"1\"/> å°±æ˜¯é”™äº†çš„ï¼Œå¦‚æœæ˜¯ <img src=\"https://www.zhihu.com/equation?tex=10%5E%7B-5%7D\" alt=\"10^{-5}\" eeimg=\"1\"/> ï¼Œé‚£ä¹ˆå°±éœ€è¦æ–Ÿé…Œä¸€ä¸‹äº†ã€‚</p><p><b>æ³¨æ„</b></p><ul><li>ä¸è¦åœ¨è®­ç»ƒä¸­ç”¨æ¢¯åº¦æ£€éªŒï¼Œå› ä¸ºå¾ˆæ…¢</li><li>å¦‚æœå‘ç°æœ‰é—®é¢˜ï¼Œé‚£ä¹ˆå®šä½åˆ°è¯¯å·®æ¯”è¾ƒå¤§çš„é‚£ä¸€å±‚æŸ¥çœ‹</li><li>å¦‚æœæœ‰æ­£åˆ™åŒ–ï¼Œè®°å¾—åŠ å…¥æ­£åˆ™é¡¹</li><li>ä¸è¦å’Œdropoutä¸€èµ·ä½¿ç”¨ï¼Œå› ä¸ºdropoutæœ¬æ¥å°±ä¸å®¹æ˜“è®¡ç®—æ¢¯åº¦ã€‚</li></ul><p></p><p></p><p></p>", 
            "topic": [
                {
                    "tag": "æ·±åº¦å­¦ä¹ ï¼ˆDeep Learningï¼‰", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "ç¥ç»ç½‘ç»œ", 
                    "tagLink": "https://api.zhihu.com/topics/19607065"
                }, 
                {
                    "tag": "æœºå™¨å­¦ä¹ ", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/44674476", 
            "userName": "ç›´ä¸Šäº‘éœ„", 
            "userLink": "https://www.zhihu.com/people/1033165ce4ad9c3fce69a0793dfab8ad", 
            "upvote": 0, 
            "title": "DeepLearning.aiä½œä¸š:(1-4)-æ·±å±‚ç¥ç»ç½‘ç»œ", 
            "content": "<h2>æœ¬æ–‡é¦–å‘äºä¸ªäººåšå®¢ï¼š<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">fangzh.top</span><span class=\"invisible\"></span></a>ï¼Œæ¬¢è¿æ¥è®¿</h2><ol><li>ä¸è¦æŠ„ä½œä¸šï¼</li><li>æˆ‘åªæ˜¯æŠŠæ€è·¯æ•´ç†äº†ï¼Œä¾›ä¸ªäººå­¦ä¹ ã€‚</li><li>ä¸è¦æŠ„ä½œä¸šï¼</li></ol><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>æœ¬å‘¨çš„ä½œä¸šåˆ†äº†ä¸¤ä¸ªéƒ¨åˆ†ï¼Œç¬¬ä¸€éƒ¨åˆ†å…ˆæ„å»ºç¥ç»ç½‘ç»œçš„åŸºæœ¬å‡½æ•°ï¼Œç¬¬äºŒéƒ¨åˆ†æ‰æ˜¯æ„å»ºå‡ºæ¨¡å‹å¹¶é¢„æµ‹ã€‚</p><h2><b>Part1</b></h2><p>æ„å»ºçš„å‡½æ•°æœ‰ï¼š</p><ul><li>Initialize the parameters<br/></li><ul><li>two-layer</li><li>L-layer</li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>forworad propagation<br/></li><ul><li>Linear part  å…ˆæ„å»ºä¸€ä¸ªçº¿æ€§çš„è®¡ç®—å‡½æ•°</li><li>linear-&gt;activation  åœ¨æ„å»ºæŸä¸€ä¸ªç¥ç»å…ƒçš„çº¿æ€§å’Œæ¿€æ´»å‡½æ•°</li><li>L_model_forward funciton  å†èåˆ L-1æ¬¡çš„Relu å’Œ   ä¸€æ¬¡ çš„ sigmoidæœ€åä¸€å±‚</li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>Compute loss</li><li>backward propagation<br/></li><ul><li>Linear part</li><li>linear-&gt;activation</li><li>L_model_backward funciton</li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>Initialization</b></h2><p>åˆå§‹åŒ–ä½¿ç”¨ï¼š</p><p>w :  <code>np.random.randn(shape)*0.01</code></p><p>b :  <code>np.zeros(shape)</code></p><p><b>1. two-layer</b></p><p>å…ˆå†™äº†ä¸ªä¸¤å±‚çš„åˆå§‹åŒ–å‡½æ•°ï¼Œä¸Šå‘¨å·²ç»å†™è¿‡äº†ã€‚</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">initialize_parameters</span><span class=\"p\">(</span><span class=\"n\">n_x</span><span class=\"p\">,</span> <span class=\"n\">n_h</span><span class=\"p\">,</span> <span class=\"n\">n_y</span><span class=\"p\">):</span>\n\n    <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">seed</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n    \n    <span class=\"c1\">### START CODE HERE ### (â‰ˆ 4 lines of code)</span>\n    <span class=\"n\">W1</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">n_h</span><span class=\"p\">,</span> <span class=\"n\">n_x</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"mf\">0.01</span>\n    <span class=\"n\">b1</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">n_h</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">))</span>\n    <span class=\"n\">W2</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">n_y</span><span class=\"p\">,</span> <span class=\"n\">n_h</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"mf\">0.01</span>\n    <span class=\"n\">b2</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">n_y</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">))</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"k\">assert</span><span class=\"p\">(</span><span class=\"n\">W1</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"p\">(</span><span class=\"n\">n_h</span><span class=\"p\">,</span> <span class=\"n\">n_x</span><span class=\"p\">))</span>\n    <span class=\"k\">assert</span><span class=\"p\">(</span><span class=\"n\">b1</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"p\">(</span><span class=\"n\">n_h</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n    <span class=\"k\">assert</span><span class=\"p\">(</span><span class=\"n\">W2</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"p\">(</span><span class=\"n\">n_y</span><span class=\"p\">,</span> <span class=\"n\">n_h</span><span class=\"p\">))</span>\n    <span class=\"k\">assert</span><span class=\"p\">(</span><span class=\"n\">b2</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"p\">(</span><span class=\"n\">n_y</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n    \n    <span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">&#34;W1&#34;</span><span class=\"p\">:</span> <span class=\"n\">W1</span><span class=\"p\">,</span>\n                  <span class=\"s2\">&#34;b1&#34;</span><span class=\"p\">:</span> <span class=\"n\">b1</span><span class=\"p\">,</span>\n                  <span class=\"s2\">&#34;W2&#34;</span><span class=\"p\">:</span> <span class=\"n\">W2</span><span class=\"p\">,</span>\n                  <span class=\"s2\">&#34;b2&#34;</span><span class=\"p\">:</span> <span class=\"n\">b2</span><span class=\"p\">}</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">parameters</span>    </code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>2. L-layer</b></p><p>ç„¶åå†™äº†ä¸ªLå±‚çš„åˆå§‹åŒ–å‡½æ•°ï¼Œå…¶ä¸­ï¼Œè¾“å…¥çš„å‚æ•°æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œå¦‚[12,4,3,1]ï¼Œè¡¨ç¤ºä¸€å…±4å±‚ï¼š</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">initialize_parameters_deep</span><span class=\"p\">(</span><span class=\"n\">layer_dims</span><span class=\"p\">):</span>\n\n    <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">seed</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">)</span>\n    <span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>\n    <span class=\"n\">L</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">layer_dims</span><span class=\"p\">)</span>            <span class=\"c1\"># number of layers in the network</span>\n<span class=\"err\">â€‹</span>\n    <span class=\"k\">for</span> <span class=\"n\">l</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">L</span><span class=\"p\">):</span>\n        <span class=\"c1\">### START CODE HERE ### (â‰ˆ 2 lines of code)</span>\n        <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;W&#39;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">layer_dims</span><span class=\"p\">[</span><span class=\"n\">l</span><span class=\"p\">],</span> <span class=\"n\">layer_dims</span><span class=\"p\">[</span><span class=\"n\">l</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">])</span> <span class=\"o\">*</span> <span class=\"mf\">0.01</span>\n        <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;b&#39;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">layer_dims</span><span class=\"p\">[</span><span class=\"n\">l</span><span class=\"p\">],</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n        \n        <span class=\"k\">assert</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;W&#39;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"p\">)]</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"p\">(</span><span class=\"n\">layer_dims</span><span class=\"p\">[</span><span class=\"n\">l</span><span class=\"p\">],</span> <span class=\"n\">layer_dims</span><span class=\"p\">[</span><span class=\"n\">l</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]))</span>\n        <span class=\"k\">assert</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;b&#39;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"p\">)]</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"p\">(</span><span class=\"n\">layer_dims</span><span class=\"p\">[</span><span class=\"n\">l</span><span class=\"p\">],</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n<span class=\"err\">â€‹</span>\n        \n    <span class=\"k\">return</span> <span class=\"n\">parameters</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>Forward propagation module</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p><b>1. Linear Forward</b></p><p>åˆ©ç”¨å…¬å¼ï¼š</p><p><img src=\"https://www.zhihu.com/equation?tex=Z%5E%7B%5Bl%5D%7D+%3D+W%5E%7B%5Bl%5D%7DA%5E%7B%5Bl-1%5D%7D+%2Bb%5E%7B%5Bl%5D%7D\" alt=\"Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\" eeimg=\"1\"/> </p><p>where <img src=\"https://www.zhihu.com/equation?tex=A%5E%7B%5B0%5D%7D+%3D+X\" alt=\"A^{[0]} = X\" eeimg=\"1\"/> </p><p>è¿™ä¸ªæ—¶å€™ï¼Œè¾“å…¥çš„å‚æ•°æ˜¯ A,W,b,è¾“å‡ºæ˜¯è®¡ç®—å¾—åˆ°çš„Zï¼Œä»¥åŠcache=ï¼ˆAï¼Œ Wï¼Œ bï¼‰ä¿å­˜èµ·æ¥</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">linear_forward</span><span class=\"p\">(</span><span class=\"n\">A</span><span class=\"p\">,</span> <span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">):</span>\n\n    <span class=\"k\">assert</span><span class=\"p\">(</span><span class=\"n\">Z</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"p\">(</span><span class=\"n\">W</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">A</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]))</span>\n    <span class=\"n\">cache</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">A</span><span class=\"p\">,</span> <span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">)</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">Z</span><span class=\"p\">,</span> <span class=\"n\">cache</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><b>2. Linear-Activation Forward</b></p><p>åœ¨è¿™é‡Œå°±æ˜¯æŠŠåˆšæ‰å¾—åˆ°çš„Zï¼Œé€šè¿‡ <img src=\"https://www.zhihu.com/equation?tex=A+%3D+g%28Z%29\" alt=\"A = g(Z)\" eeimg=\"1\"/> æ¿€æ´»å‡½æ•°ï¼Œåˆå¹¶æˆä¸€ä¸ª</p><p>è¿™ä¸ªæ—¶å€™ï¼Œnotebookå·²ç»ç»™äº†æˆ‘ä»¬ç°æˆçš„sigmoidå’Œreluå‡½æ•°äº†ï¼Œåªè¦è°ƒç”¨å°±è¡Œï¼Œä¸è¿‡åœ¨é‡Œé¢å¥½åƒæ²¡æœ‰è¯´æ˜æºä»£ç ï¼Œè¾“å‡ºéƒ½æ˜¯Aå’Œcache=Zï¼Œè¿™é‡Œè´´å‡ºæ¥ï¼š</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">sigmoid</span><span class=\"p\">(</span><span class=\"n\">Z</span><span class=\"p\">):</span>\n\n    <span class=\"n\">A</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"o\">/</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"o\">+</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"n\">Z</span><span class=\"p\">))</span>\n    <span class=\"n\">cache</span> <span class=\"o\">=</span> <span class=\"n\">Z</span>\n<span class=\"err\">â€‹</span>\n    <span class=\"k\">return</span> <span class=\"n\">A</span><span class=\"p\">,</span> <span class=\"n\">cache</span>\n<span class=\"k\">def</span> <span class=\"nf\">relu</span><span class=\"p\">(</span><span class=\"n\">Z</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Implement the RELU function.\n</span><span class=\"s2\">â€‹\n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    Z -- Output of the linear layer, of any shape\n</span><span class=\"s2\">â€‹\n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    A -- Post-activation parameter, of the same shape as Z\n</span><span class=\"s2\">    cache -- a python dictionary containing &#34;A&#34; ; stored for computing the backward pass efficiently\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n<span class=\"err\">â€‹</span>\n    <span class=\"n\">A</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">maximum</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"n\">Z</span><span class=\"p\">)</span>\n<span class=\"err\">â€‹</span>\n    <span class=\"k\">assert</span><span class=\"p\">(</span><span class=\"n\">A</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"n\">Z</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span>\n<span class=\"err\">â€‹</span>\n    <span class=\"n\">cache</span> <span class=\"o\">=</span> <span class=\"n\">Z</span> \n    <span class=\"k\">return</span> <span class=\"n\">A</span><span class=\"p\">,</span> <span class=\"n\">cache</span></code></pre></div><p>è€Œååˆ©ç”¨ä¹‹å‰çš„linear_forwardï¼Œå¯ä»¥å†™å‡ºæŸå±‚ç¥ç»å…ƒçš„å‰å‘å‡½æ•°äº†ï¼Œè¾“å…¥æ˜¯ <img src=\"https://www.zhihu.com/equation?tex=A%5E%7B%5Bl-1%5D%7D%2CW%2Cb\" alt=\"A^{[l-1]},W,b\" eeimg=\"1\"/> ï¼Œè¿˜æœ‰ä¸€ä¸ªæ˜¯è¯´æ˜sigmoidè¿˜æ˜¯reluçš„å­—ç¬¦ä¸²activationã€‚</p><p><b>è¾“å‡ºæ˜¯</b> <img src=\"https://www.zhihu.com/equation?tex=A%5E%7B%5Bl%5D%7D\" alt=\"A^{[l]}\" eeimg=\"1\"/> <b>å’Œcacheï¼Œè¿™é‡Œçš„cacheå·²ç»åŒ…å«çš„4ä¸ªå‚æ•°äº†ï¼Œåˆ†åˆ«æ˜¯</b> <img src=\"https://www.zhihu.com/equation?tex=A%5E%7B%5Bl-1%5D%7D%2CW%5E%7B%5Bl%5D%7D%2Cb%5E%7B%5Bl%5D%7D%2CZ%5E%7B%5Bl%5D%7D\" alt=\"A^{[l-1]},W^{[l]},b^{[l]},Z^{[l]}\" eeimg=\"1\"/> </p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"err\">â€‹</span>\n<span class=\"c1\"># GRADED FUNCTION: linear_activation_forward</span>\n<span class=\"err\">â€‹</span>\n<span class=\"k\">def</span> <span class=\"nf\">linear_activation_forward</span><span class=\"p\">(</span><span class=\"n\">A_prev</span><span class=\"p\">,</span> <span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"p\">):</span>\n\n    \n    <span class=\"k\">if</span> <span class=\"n\">activation</span> <span class=\"o\">==</span> <span class=\"s2\">&#34;sigmoid&#34;</span><span class=\"p\">:</span>\n        <span class=\"c1\"># Inputs: &#34;A_prev, W, b&#34;. Outputs: &#34;A, activation_cache&#34;.</span>\n        <span class=\"c1\">### START CODE HERE ### (â‰ˆ 2 lines of code)</span>\n        <span class=\"n\">Z</span><span class=\"p\">,</span> <span class=\"n\">linear_cache</span> <span class=\"o\">=</span> <span class=\"n\">linear_forward</span><span class=\"p\">(</span><span class=\"n\">A_prev</span><span class=\"p\">,</span> <span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">)</span>\n        <span class=\"n\">A</span><span class=\"p\">,</span> <span class=\"n\">activation_cache</span> <span class=\"o\">=</span> <span class=\"n\">sigmoid</span><span class=\"p\">(</span><span class=\"n\">Z</span><span class=\"p\">)</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"k\">elif</span> <span class=\"n\">activation</span> <span class=\"o\">==</span> <span class=\"s2\">&#34;relu&#34;</span><span class=\"p\">:</span>\n        <span class=\"c1\"># Inputs: &#34;A_prev, W, b&#34;. Outputs: &#34;A, activation_cache&#34;.</span>\n        <span class=\"c1\">### START CODE HERE ### (â‰ˆ 2 lines of code)</span>\n        <span class=\"n\">Z</span><span class=\"p\">,</span> <span class=\"n\">linear_cache</span> <span class=\"o\">=</span> <span class=\"n\">linear_forward</span><span class=\"p\">(</span><span class=\"n\">A_prev</span><span class=\"p\">,</span> <span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">)</span>\n        <span class=\"n\">A</span><span class=\"p\">,</span> <span class=\"n\">activation_cache</span> <span class=\"o\">=</span> <span class=\"n\">relu</span><span class=\"p\">(</span><span class=\"n\">Z</span><span class=\"p\">)</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"k\">assert</span> <span class=\"p\">(</span><span class=\"n\">A</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"p\">(</span><span class=\"n\">W</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">A_prev</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]))</span>\n    <span class=\"n\">cache</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">linear_cache</span><span class=\"p\">,</span> <span class=\"n\">activation_cache</span><span class=\"p\">)</span>\n   <span class=\"c1\"># print(cache)</span>\n    <span class=\"k\">return</span> <span class=\"n\">A</span><span class=\"p\">,</span> <span class=\"n\">cache</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><b>3. L-Layer Model</b></p><p>è¿™ä¸€æ­¥å°±æŠŠå¤šå±‚çš„ç¥ç»ç½‘ç»œä»å¤´åˆ°å°¾ä¸²èµ·æ¥äº†ã€‚å‰é¢æœ‰L-1å±‚çš„Reluï¼Œç¬¬Lå±‚æ˜¯sigmoidã€‚</p><p>è¾“å…¥æ˜¯Xï¼Œä¹Ÿå°±æ˜¯ <img src=\"https://www.zhihu.com/equation?tex=A%5E%7B%5B0%5D%7D\" alt=\"A^{[0]}\" eeimg=\"1\"/> ï¼Œå’Œ parametersåŒ…å«äº†å„ä¸ªå±‚çš„W,b</p><p>è¾“å‡ºæ˜¯æœ€åä¸€å±‚çš„ <img src=\"https://www.zhihu.com/equation?tex=A%5E%7B%5BL%5D%7D\" alt=\"A^{[L]}\" eeimg=\"1\"/> ï¼Œä¹Ÿå°±æ˜¯é¢„æµ‹ç»“æœ$Y_hat$ï¼Œä»¥åŠæ¯ä¸€å±‚çš„caches : <img src=\"https://www.zhihu.com/equation?tex=A%5E%7B%5Bl-1%5D%7D%2CW%5E%7B%5Bl%5D%7D%2Cb%5E%7B%5Bl%5D%7D%2CZ%5E%7B%5Bl%5D%7D\" alt=\"A^{[l-1]},W^{[l]},b^{[l]},Z^{[l]}\" eeimg=\"1\"/> </p><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">L_model_forward</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">parameters</span><span class=\"p\">):</span>\n\n    <span class=\"n\">caches</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n    <span class=\"n\">A</span> <span class=\"o\">=</span> <span class=\"n\">X</span>\n    <span class=\"n\">L</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">)</span> <span class=\"o\">//</span> <span class=\"mi\">2</span>                  <span class=\"c1\"># number of layers in the neural network</span>\n    \n    <span class=\"c1\"># Implement [LINEAR -&gt; RELU]*(L-1). Add &#34;cache&#34; to the &#34;caches&#34; list.</span>\n    <span class=\"k\">for</span> <span class=\"n\">l</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">L</span><span class=\"p\">):</span>\n        <span class=\"n\">A_prev</span> <span class=\"o\">=</span> <span class=\"n\">A</span> \n        <span class=\"c1\">### START CODE HERE ### (â‰ˆ 2 lines of code)</span>\n        <span class=\"n\">A</span><span class=\"p\">,</span> <span class=\"n\">cache</span> <span class=\"o\">=</span> <span class=\"n\">linear_activation_forward</span><span class=\"p\">(</span><span class=\"n\">A_prev</span><span class=\"p\">,</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;W&#39;</span><span class=\"o\">+</span><span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"p\">)],</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;b&#39;</span><span class=\"o\">+</span><span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"p\">)],</span> <span class=\"s1\">&#39;relu&#39;</span><span class=\"p\">)</span>\n        <span class=\"n\">caches</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">cache</span><span class=\"p\">)</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"c1\"># Implement LINEAR -&gt; SIGMOID. Add &#34;cache&#34; to the &#34;caches&#34; list.</span>\n    <span class=\"c1\">### START CODE HERE ### (â‰ˆ 2 lines of code)</span>\n    <span class=\"n\">AL</span><span class=\"p\">,</span> <span class=\"n\">cache</span> <span class=\"o\">=</span> <span class=\"n\">linear_activation_forward</span><span class=\"p\">(</span><span class=\"n\">A</span><span class=\"p\">,</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;W&#39;</span><span class=\"o\">+</span><span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">L</span><span class=\"p\">)],</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;b&#39;</span><span class=\"o\">+</span><span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">L</span><span class=\"p\">)],</span><span class=\"s1\">&#39;sigmoid&#39;</span><span class=\"p\">)</span>\n    <span class=\"n\">caches</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">cache</span><span class=\"p\">)</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n   <span class=\"c1\"># print(AL.shape)</span>\n    <span class=\"k\">assert</span><span class=\"p\">(</span><span class=\"n\">AL</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]))</span>\n            \n    <span class=\"k\">return</span> <span class=\"n\">AL</span><span class=\"p\">,</span> <span class=\"n\">caches</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>Cost function</b></h2><p><img src=\"https://www.zhihu.com/equation?tex=-%5Cfrac%7B1%7D%7Bm%7D+%5Csum%5Climits_%7Bi+%3D+1%7D%5E%7Bm%7D+%28y%5E%7B%28i%29%7D%5Clog%5Cleft%28a%5E%7B%5BL%5D+%28i%29%7D%5Cright%29+%2B+%281-y%5E%7B%28i%29%7D%29%5Clog%5Cleft%281-+a%5E%7BL%7D%5Cright%29%29+\" alt=\"-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{L}\\right)) \" eeimg=\"1\"/> </p><p>åˆ©ç”¨<code>np.multiply</code> and <code>np.sum</code>æ±‚å¾—äº¤å‰ç†µ</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"err\">â€‹</span>\n<span class=\"k\">def</span> <span class=\"nf\">compute_cost</span><span class=\"p\">(</span><span class=\"n\">AL</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">):</span>\n\n    <span class=\"n\">m</span> <span class=\"o\">=</span> <span class=\"n\">Y</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n<span class=\"err\">â€‹</span>\n    <span class=\"c1\"># Compute loss from aL and y.</span>\n    <span class=\"c1\">### START CODE HERE ### (â‰ˆ 1 lines of code)</span>\n    <span class=\"n\">cost</span> <span class=\"o\">=</span> <span class=\"o\">-</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">multiply</span><span class=\"p\">(</span><span class=\"n\">Y</span><span class=\"p\">,</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">log</span><span class=\"p\">(</span><span class=\"n\">AL</span><span class=\"p\">))</span> <span class=\"o\">+</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">multiply</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"o\">-</span><span class=\"n\">Y</span><span class=\"p\">,</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">log</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"o\">-</span><span class=\"n\">AL</span><span class=\"p\">)))</span> <span class=\"o\">/</span> <span class=\"n\">m</span>\n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">cost</span><span class=\"p\">)</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    <span class=\"n\">cost</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">squeeze</span><span class=\"p\">(</span><span class=\"n\">cost</span><span class=\"p\">)</span>      <span class=\"c1\"># To make sure your cost&#39;s shape is what we expect (e.g. this turns [[17]] into 17).</span>\n    <span class=\"k\">assert</span><span class=\"p\">(</span><span class=\"n\">cost</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"p\">())</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">cost</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>Backward propagation module</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p><b>1. Linear backward</b></p><p>é¦–å…ˆå‡è®¾çŸ¥é“ <img src=\"https://www.zhihu.com/equation?tex=dZ%5E%7B%5Bl%5D%7D+%3D+%5Cfrac%7B%5Cpartial+%5Cmathcal%7BL%7D+%7D%7B%5Cpartial+Z%5E%7B%5Bl%5D%7D%7D\" alt=\"dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}\" eeimg=\"1\"/> ï¼Œç„¶åæƒ³è¦æ±‚å¾—çš„æ˜¯ <img src=\"https://www.zhihu.com/equation?tex=%28dW%5E%7B%5Bl%5D%7D%2C+db%5E%7B%5Bl%5D%7D+dA%5E%7B%5Bl-1%5D%7D%29\" alt=\"(dW^{[l]}, db^{[l]} dA^{[l-1]})\" eeimg=\"1\"/> .</p><p>å…¬å¼å·²ç»ç»™ä½ äº†ï¼š</p><p><img src=\"https://www.zhihu.com/equation?tex=+dW%5E%7B%5Bl%5D%7D+%3D+%5Cfrac%7B%5Cpartial+%5Cmathcal%7BL%7D+%7D%7B%5Cpartial+W%5E%7B%5Bl%5D%7D%7D+%3D+%5Cfrac%7B1%7D%7Bm%7D+dZ%5E%7B%5Bl%5D%7D+A%5E%7B%5Bl-1%5D+T%7D++\" alt=\" dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T}  \" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=db%5E%7B%5Bl%5D%7D+%3D+%5Cfrac%7B%5Cpartial+%5Cmathcal%7BL%7D+%7D%7B%5Cpartial+b%5E%7B%5Bl%5D%7D%7D+%3D+%5Cfrac%7B1%7D%7Bm%7D+%5Csum_%7Bi+%3D+1%7D%5E%7Bm%7D+dZ%5E%7B%5Bl%5D+%28i%29%7D\" alt=\"db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l] (i)}\" eeimg=\"1\"/> </p><p class=\"ztext-empty-paragraph\"><br/></p><p><img src=\"https://www.zhihu.com/equation?tex=+dA%5E%7B%5Bl-1%5D%7D+%3D+%5Cfrac%7B%5Cpartial+%5Cmathcal%7BL%7D+%7D%7B%5Cpartial+A%5E%7B%5Bl-1%5D%7D%7D+%3D+W%5E%7B%5Bl%5D+T%7D+dZ%5E%7B%5Bl%5D%7D+\" alt=\" dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \" eeimg=\"1\"/> </p><p class=\"ztext-empty-paragraph\"><br/></p><p>cacheæ˜¯linear cache: A_prev,W,b</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">linear_backward</span><span class=\"p\">(</span><span class=\"n\">dZ</span><span class=\"p\">,</span> <span class=\"n\">cache</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Implement the linear portion of backward propagation for a single layer (layer l)\n</span><span class=\"s2\">â€‹\n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n</span><span class=\"s2\">    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n</span><span class=\"s2\">â€‹\n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n</span><span class=\"s2\">    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n</span><span class=\"s2\">    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    <span class=\"n\">A_prev</span><span class=\"p\">,</span> <span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">b</span> <span class=\"o\">=</span> <span class=\"n\">cache</span>\n    <span class=\"n\">m</span> <span class=\"o\">=</span> <span class=\"n\">A_prev</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n<span class=\"err\">â€‹</span>\n    <span class=\"c1\">### START CODE HERE ### (â‰ˆ 3 lines of code)</span>\n    <span class=\"n\">dW</span> <span class=\"o\">=</span> <span class=\"mi\">1</span> <span class=\"o\">/</span> <span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">dZ</span><span class=\"p\">,</span> <span class=\"n\">A_prev</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">)</span>\n    <span class=\"n\">db</span> <span class=\"o\">=</span> <span class=\"mi\">1</span> <span class=\"o\">/</span> <span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">dZ</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"n\">keepdims</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n    <span class=\"c1\">#print(db.shape)</span>\n    <span class=\"c1\">#print(b.shape)</span>\n    <span class=\"n\">dA_prev</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">W</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">,</span> <span class=\"n\">dZ</span><span class=\"p\">)</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"k\">assert</span> <span class=\"p\">(</span><span class=\"n\">dA_prev</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"n\">A_prev</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span>\n    <span class=\"k\">assert</span> <span class=\"p\">(</span><span class=\"n\">dW</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"n\">W</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span>\n    <span class=\"k\">assert</span> <span class=\"p\">(</span><span class=\"n\">db</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"n\">b</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">dA_prev</span><span class=\"p\">,</span> <span class=\"n\">dW</span><span class=\"p\">,</span> <span class=\"n\">db</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><b>2. Linear-Activation backward</b></p><p>dAé€šè¿‡æ¿€æ´»å‡½æ•°çš„å¯¼æ•°å¯ä»¥æ±‚å¾—dZï¼Œå†ç”±ä¸Šé¢çš„å‡½æ•°ï¼Œæœ€ç»ˆï¼š</p><p>è¾“å…¥ <img src=\"https://www.zhihu.com/equation?tex=dA%5E%7B%5Bl%5D%7D+%2C+cache\" alt=\"dA^{[l]} , cache\" eeimg=\"1\"/> </p><p>è¾“å‡º <img src=\"https://www.zhihu.com/equation?tex=dA%5E%7B%5Bl-1%5D%7D+%2CdW%2Cdb\" alt=\"dA^{[l-1]} ,dW,db\" eeimg=\"1\"/> </p><p>è¿™ä¸ªæ—¶å€™å®ƒæœ‰ç»™äº†ä¸¤ä¸ªç°æˆçš„å‡½æ•°<code>dZ = sigmoid_backward(dA, activation_cache)</code>ã€<code>dZ = relu_backward(dA, activation_cache)</code></p><p>æºä»£ç å¦‚ä¸‹,è¾“å…¥çš„éƒ½æ˜¯dAï¼Œå’Œ cache=Zï¼Œè¾“å‡ºæ˜¯dZï¼š</p><p><img src=\"https://www.zhihu.com/equation?tex=dZ%5E%7B%5Bl%5D%7D+%3D+dA%5E%7B%5Bl%5D%7D+%2A+g%27%28Z%5E%7B%5Bl%5D%7D%29\" alt=\"dZ^{[l]} = dA^{[l]} * g&#39;(Z^{[l]})\" eeimg=\"1\"/> </p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">sigmoid_backward</span><span class=\"p\">(</span><span class=\"n\">dA</span><span class=\"p\">,</span> <span class=\"n\">cache</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Implement the backward propagation for a single SIGMOID unit.\n</span><span class=\"s2\">â€‹\n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    dA -- post-activation gradient, of any shape\n</span><span class=\"s2\">    cache -- &#39;Z&#39; where we store for computing backward propagation efficiently\n</span><span class=\"s2\">â€‹\n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    dZ -- Gradient of the cost with respect to Z\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n<span class=\"err\">â€‹</span>\n    <span class=\"n\">Z</span> <span class=\"o\">=</span> <span class=\"n\">cache</span>\n<span class=\"err\">â€‹</span>\n    <span class=\"n\">s</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"o\">/</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"o\">+</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"n\">Z</span><span class=\"p\">))</span>\n    <span class=\"n\">dZ</span> <span class=\"o\">=</span> <span class=\"n\">dA</span> <span class=\"o\">*</span> <span class=\"n\">s</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"o\">-</span><span class=\"n\">s</span><span class=\"p\">)</span>\n<span class=\"err\">â€‹</span>\n    <span class=\"k\">assert</span> <span class=\"p\">(</span><span class=\"n\">dZ</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"n\">Z</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span>\n<span class=\"err\">â€‹</span>\n    <span class=\"k\">return</span> <span class=\"n\">dZ</span>\n<span class=\"k\">def</span> <span class=\"nf\">relu_backward</span><span class=\"p\">(</span><span class=\"n\">dA</span><span class=\"p\">,</span> <span class=\"n\">cache</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Implement the backward propagation for a single RELU unit.\n</span><span class=\"s2\">â€‹\n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    dA -- post-activation gradient, of any shape\n</span><span class=\"s2\">    cache -- &#39;Z&#39; where we store for computing backward propagation efficiently\n</span><span class=\"s2\">â€‹\n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    dZ -- Gradient of the cost with respect to Z\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n<span class=\"err\">â€‹</span>\n    <span class=\"n\">Z</span> <span class=\"o\">=</span> <span class=\"n\">cache</span>\n    <span class=\"n\">dZ</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">(</span><span class=\"n\">dA</span><span class=\"p\">,</span> <span class=\"n\">copy</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span> <span class=\"c1\"># just converting dz to a correct object.</span>\n<span class=\"err\">â€‹</span>\n    <span class=\"c1\"># When z &lt;= 0, you should set dz to 0 as well. </span>\n    <span class=\"n\">dZ</span><span class=\"p\">[</span><span class=\"n\">Z</span> <span class=\"o\">&lt;=</span> <span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n<span class=\"err\">â€‹</span>\n    <span class=\"k\">assert</span> <span class=\"p\">(</span><span class=\"n\">dZ</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"n\">Z</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span>\n<span class=\"err\">â€‹</span>\n    <span class=\"k\">return</span> <span class=\"n\">dZ</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>ç„¶åå¾—åˆ°äº†å‡½æ•°å¦‚ä¸‹,æ³¨æ„è¿™é‡Œé¢çš„cacheå·²ç»æ˜¯4ä¸ªå…ƒç´ äº†<code>linear_cache=A_prev,W,b</code>ã€<code>activation_cache=Z</code>ï¼š</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: linear_activation_backward</span>\n<span class=\"err\">â€‹</span>\n<span class=\"k\">def</span> <span class=\"nf\">linear_activation_backward</span><span class=\"p\">(</span><span class=\"n\">dA</span><span class=\"p\">,</span> <span class=\"n\">cache</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Implement the backward propagation for the LINEAR-&gt;ACTIVATION layer.\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    dA -- post-activation gradient for current layer l \n</span><span class=\"s2\">    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n</span><span class=\"s2\">    activation -- the activation to be used in this layer, stored as a text string: &#34;sigmoid&#34; or &#34;relu&#34;\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n</span><span class=\"s2\">    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n</span><span class=\"s2\">    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    <span class=\"n\">linear_cache</span><span class=\"p\">,</span> <span class=\"n\">activation_cache</span> <span class=\"o\">=</span> <span class=\"n\">cache</span>\n    \n    <span class=\"k\">if</span> <span class=\"n\">activation</span> <span class=\"o\">==</span> <span class=\"s2\">&#34;relu&#34;</span><span class=\"p\">:</span>\n        <span class=\"c1\">### START CODE HERE ### (â‰ˆ 2 lines of code)</span>\n        <span class=\"n\">dZ</span> <span class=\"o\">=</span> <span class=\"n\">relu_backward</span><span class=\"p\">(</span><span class=\"n\">dA</span><span class=\"p\">,</span> <span class=\"n\">activation_cache</span><span class=\"p\">)</span>\n        <span class=\"n\">dA_prev</span><span class=\"p\">,</span> <span class=\"n\">dW</span><span class=\"p\">,</span> <span class=\"n\">db</span> <span class=\"o\">=</span> <span class=\"n\">linear_backward</span><span class=\"p\">(</span><span class=\"n\">dZ</span><span class=\"p\">,</span> <span class=\"n\">linear_cache</span><span class=\"p\">)</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n        \n    <span class=\"k\">elif</span> <span class=\"n\">activation</span> <span class=\"o\">==</span> <span class=\"s2\">&#34;sigmoid&#34;</span><span class=\"p\">:</span>\n        <span class=\"c1\">### START CODE HERE ### (â‰ˆ 2 lines of code)</span>\n        <span class=\"n\">dZ</span> <span class=\"o\">=</span> <span class=\"n\">sigmoid_backward</span><span class=\"p\">(</span><span class=\"n\">dA</span><span class=\"p\">,</span> <span class=\"n\">activation_cache</span><span class=\"p\">)</span>\n        <span class=\"n\">dA_prev</span><span class=\"p\">,</span> <span class=\"n\">dW</span><span class=\"p\">,</span> <span class=\"n\">db</span> <span class=\"o\">=</span> <span class=\"n\">linear_backward</span><span class=\"p\">(</span><span class=\"n\">dZ</span><span class=\"p\">,</span> <span class=\"n\">linear_cache</span><span class=\"p\">)</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">dA_prev</span><span class=\"p\">,</span> <span class=\"n\">dW</span><span class=\"p\">,</span> <span class=\"n\">db</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><b>3.  L-Model Backward</b></p><p>å¯ä»¥æŠŠå‰é¢çš„å‡½æ•°ç©¿èµ·æ¥ï¼Œä»åé¢å¾€å‰é¢ä¼ æ’­äº†ï¼Œå…ˆç®—æœ€åä¸€å±‚çš„sigmoidï¼Œç„¶åå¾€å‰ç®—L-1çš„å¾ªç¯reluã€‚å…¶ä¸­ï¼ŒdALæ˜¯æŸå¤±å‡½æ•°çš„å¯¼æ•°ï¼Œè¿™ä¸ªæ˜¯é¢„å…ˆæ±‚å¾—çŸ¥é“çš„ï¼Œä¹Ÿå°±æ˜¯ </p><p><img src=\"https://www.zhihu.com/equation?tex=-%5Cfrac%7By%7D%7Ba%7D-%5Cfrac%7B1-y%7D%7B1-a%7D\" alt=\"-\\frac{y}{a}-\\frac{1-y}{1-a}\" eeimg=\"1\"/> </p><p>numpyè¡¨ç¤ºä¸ºï¼š</p><div class=\"highlight\"><pre><code class=\"language-text\">dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))</code></pre></div><p>æ•´ä¸ªbackwardä¸­ï¼Œæˆ‘ä»¬çš„è¾“å…¥åªæœ‰AL,Yå’Œcachesï¼Œ</p><p>è¾“å‡ºåˆ™æ˜¯æ¯ä¸€å±‚çš„gradsï¼ŒåŒ…æ‹¬äº† <img src=\"https://www.zhihu.com/equation?tex=dA%2CdW%2Cdb\" alt=\"dA,dW,db\" eeimg=\"1\"/> </p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: L_model_backward</span>\n<span class=\"err\">â€‹</span>\n<span class=\"k\">def</span> <span class=\"nf\">L_model_backward</span><span class=\"p\">(</span><span class=\"n\">AL</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">caches</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Implement the backward propagation for the [LINEAR-&gt;RELU] * (L-1) -&gt; LINEAR -&gt; SIGMOID group\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    AL -- probability vector, output of the forward propagation (L_model_forward())\n</span><span class=\"s2\">    Y -- true &#34;label&#34; vector (containing 0 if non-cat, 1 if cat)\n</span><span class=\"s2\">    caches -- list of caches containing:\n</span><span class=\"s2\">                every cache of linear_activation_forward() with &#34;relu&#34; (it&#39;s caches[l], for l in range(L-1) i.e l = 0...L-2)\n</span><span class=\"s2\">                the cache of linear_activation_forward() with &#34;sigmoid&#34; (it&#39;s caches[L-1])\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    grads -- A dictionary with the gradients\n</span><span class=\"s2\">             grads[&#34;dA&#34; + str(l)] = ... \n</span><span class=\"s2\">             grads[&#34;dW&#34; + str(l)] = ...\n</span><span class=\"s2\">             grads[&#34;db&#34; + str(l)] = ... \n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    <span class=\"n\">grads</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>\n    <span class=\"n\">L</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">caches</span><span class=\"p\">)</span> <span class=\"c1\"># the number of layers</span>\n    <span class=\"n\">m</span> <span class=\"o\">=</span> <span class=\"n\">AL</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n    <span class=\"n\">Y</span> <span class=\"o\">=</span> <span class=\"n\">Y</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"n\">AL</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span> <span class=\"c1\"># after this line, Y is the same shape as AL</span>\n    \n    <span class=\"c1\"># Initializing the backpropagation</span>\n    <span class=\"c1\">### START CODE HERE ### (1 line of code)</span>\n    <span class=\"n\">dAL</span> <span class=\"o\">=</span>  <span class=\"o\">-</span> <span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">divide</span><span class=\"p\">(</span><span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">AL</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">divide</span><span class=\"p\">(</span><span class=\"mi\">1</span> <span class=\"o\">-</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"mi\">1</span> <span class=\"o\">-</span> <span class=\"n\">AL</span><span class=\"p\">))</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"c1\"># Lth layer (SIGMOID -&gt; LINEAR) gradients. Inputs: &#34;dAL, current_cache&#34;. Outputs: &#34;grads[&#34;dAL-1&#34;], grads[&#34;dWL&#34;], grads[&#34;dbL&#34;]</span>\n    <span class=\"c1\">### START CODE HERE ### (approx. 2 lines)</span>\n    <span class=\"n\">current_cache</span> <span class=\"o\">=</span> <span class=\"n\">caches</span><span class=\"p\">[</span><span class=\"n\">L</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n    <span class=\"n\">grads</span><span class=\"p\">[</span><span class=\"s2\">&#34;dA&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">L</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">)],</span> <span class=\"n\">grads</span><span class=\"p\">[</span><span class=\"s2\">&#34;dW&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">L</span><span class=\"p\">)],</span> <span class=\"n\">grads</span><span class=\"p\">[</span><span class=\"s2\">&#34;db&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">L</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">linear_activation_backward</span><span class=\"p\">(</span><span class=\"n\">dAL</span><span class=\"p\">,</span> <span class=\"n\">current_cache</span><span class=\"p\">,</span> <span class=\"s1\">&#39;sigmoid&#39;</span><span class=\"p\">)</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"c1\"># Loop from l=L-2 to l=0</span>\n    <span class=\"k\">for</span> <span class=\"n\">l</span> <span class=\"ow\">in</span> <span class=\"nb\">reversed</span><span class=\"p\">(</span><span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">L</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">)):</span>\n        <span class=\"c1\"># lth layer: (RELU -&gt; LINEAR) gradients.</span>\n        <span class=\"c1\"># Inputs: &#34;grads[&#34;dA&#34; + str(l + 1)], current_cache&#34;. Outputs: &#34;grads[&#34;dA&#34; + str(l)] , grads[&#34;dW&#34; + str(l + 1)] , grads[&#34;db&#34; + str(l + 1)] </span>\n        <span class=\"c1\">### START CODE HERE ### (approx. 5 lines)</span>\n        <span class=\"n\">current_cache</span> <span class=\"o\">=</span> <span class=\"n\">caches</span><span class=\"p\">[</span><span class=\"n\">l</span><span class=\"p\">]</span>\n        <span class=\"n\">dA_prev_temp</span><span class=\"p\">,</span> <span class=\"n\">dW_temp</span><span class=\"p\">,</span> <span class=\"n\">db_temp</span> <span class=\"o\">=</span> <span class=\"n\">linear_activation_backward</span><span class=\"p\">(</span><span class=\"n\">grads</span><span class=\"p\">[</span><span class=\"s1\">&#39;dA&#39;</span><span class=\"o\">+</span><span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)],</span> <span class=\"n\">current_cache</span><span class=\"p\">,</span> <span class=\"s1\">&#39;relu&#39;</span><span class=\"p\">)</span>\n        <span class=\"n\">grads</span><span class=\"p\">[</span><span class=\"s2\">&#34;dA&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">dA_prev_temp</span>\n        <span class=\"n\">grads</span><span class=\"p\">[</span><span class=\"s2\">&#34;dW&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">dW_temp</span>\n        <span class=\"n\">grads</span><span class=\"p\">[</span><span class=\"s2\">&#34;db&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">db_temp</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n<span class=\"err\">â€‹</span>\n    <span class=\"k\">return</span> <span class=\"n\">grads</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>Update Parameters</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: update_parameters</span>\n<span class=\"err\">â€‹</span>\n<span class=\"k\">def</span> <span class=\"nf\">update_parameters</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">grads</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Update parameters using gradient descent\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    parameters -- python dictionary containing your parameters \n</span><span class=\"s2\">    grads -- python dictionary containing your gradients, output of L_model_backward\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    parameters -- python dictionary containing your updated parameters \n</span><span class=\"s2\">                  parameters[&#34;W&#34; + str(l)] = ... \n</span><span class=\"s2\">                  parameters[&#34;b&#34; + str(l)] = ...\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"n\">L</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">)</span> <span class=\"o\">//</span> <span class=\"mi\">2</span> <span class=\"c1\"># number of layers in the neural network</span>\n<span class=\"err\">â€‹</span>\n    <span class=\"c1\"># Update rule for each parameter. Use a for loop.</span>\n    <span class=\"c1\">### START CODE HERE ### (â‰ˆ 3 lines of code)</span>\n    <span class=\"k\">for</span> <span class=\"n\">l</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">L</span><span class=\"p\">):</span>\n        <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;W&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">-=</span> <span class=\"n\">learning_rate</span> <span class=\"o\">*</span> <span class=\"n\">grads</span><span class=\"p\">[</span><span class=\"s1\">&#39;dW&#39;</span><span class=\"o\">+</span><span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span>\n        <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;b&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">-=</span> <span class=\"n\">learning_rate</span> <span class=\"o\">*</span> <span class=\"n\">grads</span><span class=\"p\">[</span><span class=\"s1\">&#39;db&#39;</span><span class=\"o\">+</span><span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    <span class=\"k\">return</span> <span class=\"n\">parameters</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>Part2</b></h2><p>æœ‰äº†part1ä¸­çš„å‡½æ•°ï¼Œå°±å¾ˆå®¹æ˜“åœ¨part2ä¸­æ­å»ºæ¨¡å‹å’Œè®­ç»ƒäº†ã€‚</p><p>ä¾æ—§æ˜¯è¯†åˆ«çŒ«çŒ«çš„å›¾ç‰‡ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-9cbc30457780df7f97e20a22af6f02ac_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1480\" data-rawheight=\"810\" class=\"origin_image zh-lightbox-thumb\" width=\"1480\" data-original=\"https://pic1.zhimg.com/v2-9cbc30457780df7f97e20a22af6f02ac_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1480&#39; height=&#39;810&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1480\" data-rawheight=\"810\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1480\" data-original=\"https://pic1.zhimg.com/v2-9cbc30457780df7f97e20a22af6f02ac_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-9cbc30457780df7f97e20a22af6f02ac_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>å¼€å§‹å…ˆç”¨ä¸¤å±‚çš„layeråšè®­ç»ƒï¼Œå¾—åˆ°äº†ç²¾ç¡®åº¦æ˜¯72%ï¼Œè¿™é‡Œè´´ä»£ç å°±å¥½äº†ï¼ŒLå±‚å†è¯¦ç»†è¯´è¯´</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\">### CONSTANTS DEFINING THE MODEL ####</span>\n<span class=\"n\">n_x</span> <span class=\"o\">=</span> <span class=\"mi\">12288</span>     <span class=\"c1\"># num_px * num_px * 3</span>\n<span class=\"n\">n_h</span> <span class=\"o\">=</span> <span class=\"mi\">7</span>\n<span class=\"n\">n_y</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>\n<span class=\"n\">layers_dims</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">n_x</span><span class=\"p\">,</span> <span class=\"n\">n_h</span><span class=\"p\">,</span> <span class=\"n\">n_y</span><span class=\"p\">)</span>\n<span class=\"err\">â€‹</span>\n<span class=\"err\">â€‹</span>\n<span class=\"err\">â€‹</span>\n<span class=\"c1\"># GRADED FUNCTION: two_layer_model</span>\n<span class=\"err\">â€‹</span>\n<span class=\"k\">def</span> <span class=\"nf\">two_layer_model</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">layers_dims</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span> <span class=\"o\">=</span> <span class=\"mf\">0.0075</span><span class=\"p\">,</span> <span class=\"n\">num_iterations</span> <span class=\"o\">=</span> <span class=\"mi\">3000</span><span class=\"p\">,</span> <span class=\"n\">print_cost</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Implements a two-layer neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID.\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    X -- input data, of shape (n_x, number of examples)\n</span><span class=\"s2\">    Y -- true &#34;label&#34; vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n</span><span class=\"s2\">    layers_dims -- dimensions of the layers (n_x, n_h, n_y)\n</span><span class=\"s2\">    num_iterations -- number of iterations of the optimization loop\n</span><span class=\"s2\">    learning_rate -- learning rate of the gradient descent update rule\n</span><span class=\"s2\">    print_cost -- If set to True, this will print the cost every 100 iterations \n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    parameters -- a dictionary containing W1, W2, b1, and b2\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">seed</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n    <span class=\"n\">grads</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>\n    <span class=\"n\">costs</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>                              <span class=\"c1\"># to keep track of the cost</span>\n    <span class=\"n\">m</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span>                           <span class=\"c1\"># number of examples</span>\n    <span class=\"p\">(</span><span class=\"n\">n_x</span><span class=\"p\">,</span> <span class=\"n\">n_h</span><span class=\"p\">,</span> <span class=\"n\">n_y</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">layers_dims</span>\n    \n    <span class=\"c1\"># Initialize parameters dictionary, by calling one of the functions you&#39;d previously implemented</span>\n    <span class=\"c1\">### START CODE HERE ### (â‰ˆ 1 line of code)</span>\n    <span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"n\">initialize_parameters</span><span class=\"p\">(</span><span class=\"n\">n_x</span><span class=\"p\">,</span> <span class=\"n\">n_h</span><span class=\"p\">,</span> <span class=\"n\">n_y</span><span class=\"p\">)</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"c1\"># Get W1, b1, W2 and b2 from the dictionary parameters.</span>\n    <span class=\"n\">W1</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;W1&#34;</span><span class=\"p\">]</span>\n    <span class=\"n\">b1</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;b1&#34;</span><span class=\"p\">]</span>\n    <span class=\"n\">W2</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;W2&#34;</span><span class=\"p\">]</span>\n    <span class=\"n\">b2</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;b2&#34;</span><span class=\"p\">]</span>\n    \n    <span class=\"c1\"># Loop (gradient descent)</span>\n<span class=\"err\">â€‹</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">num_iterations</span><span class=\"p\">):</span>\n<span class=\"err\">â€‹</span>\n        <span class=\"c1\"># Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID. Inputs: &#34;X, W1, b1, W2, b2&#34;. Output: &#34;A1, cache1, A2, cache2&#34;.</span>\n        <span class=\"c1\">### START CODE HERE ### (â‰ˆ 2 lines of code)</span>\n        <span class=\"n\">A1</span><span class=\"p\">,</span> <span class=\"n\">cache1</span> <span class=\"o\">=</span> <span class=\"n\">linear_activation_forward</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">W1</span><span class=\"p\">,</span> <span class=\"n\">b1</span><span class=\"p\">,</span> <span class=\"s1\">&#39;relu&#39;</span><span class=\"p\">)</span>\n        <span class=\"n\">A2</span><span class=\"p\">,</span> <span class=\"n\">cache2</span> <span class=\"o\">=</span> <span class=\"n\">linear_activation_forward</span><span class=\"p\">(</span><span class=\"n\">A1</span><span class=\"p\">,</span> <span class=\"n\">W2</span><span class=\"p\">,</span> <span class=\"n\">b2</span><span class=\"p\">,</span> <span class=\"s1\">&#39;sigmoid&#39;</span><span class=\"p\">)</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n        \n        <span class=\"c1\"># Compute cost</span>\n        <span class=\"c1\">### START CODE HERE ### (â‰ˆ 1 line of code)</span>\n        <span class=\"n\">cost</span> <span class=\"o\">=</span> <span class=\"n\">compute_cost</span><span class=\"p\">(</span><span class=\"n\">A2</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">)</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n        \n        <span class=\"c1\"># Initializing backward propagation</span>\n        <span class=\"n\">dA2</span> <span class=\"o\">=</span> <span class=\"o\">-</span> <span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">divide</span><span class=\"p\">(</span><span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">A2</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">divide</span><span class=\"p\">(</span><span class=\"mi\">1</span> <span class=\"o\">-</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"mi\">1</span> <span class=\"o\">-</span> <span class=\"n\">A2</span><span class=\"p\">))</span>\n        \n        <span class=\"c1\"># Backward propagation. Inputs: &#34;dA2, cache2, cache1&#34;. Outputs: &#34;dA1, dW2, db2; also dA0 (not used), dW1, db1&#34;.</span>\n        <span class=\"c1\">### START CODE HERE ### (â‰ˆ 2 lines of code)</span>\n        <span class=\"n\">dA1</span><span class=\"p\">,</span> <span class=\"n\">dW2</span><span class=\"p\">,</span> <span class=\"n\">db2</span> <span class=\"o\">=</span> <span class=\"n\">linear_activation_backward</span><span class=\"p\">(</span><span class=\"n\">dA2</span><span class=\"p\">,</span> <span class=\"n\">cache2</span><span class=\"p\">,</span> <span class=\"s1\">&#39;sigmoid&#39;</span><span class=\"p\">)</span>\n        <span class=\"n\">dA0</span><span class=\"p\">,</span> <span class=\"n\">dW1</span><span class=\"p\">,</span> <span class=\"n\">db1</span> <span class=\"o\">=</span> <span class=\"n\">linear_activation_backward</span><span class=\"p\">(</span><span class=\"n\">dA1</span><span class=\"p\">,</span> <span class=\"n\">cache1</span><span class=\"p\">,</span> <span class=\"s1\">&#39;relu&#39;</span><span class=\"p\">)</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n        \n        <span class=\"c1\"># Set grads[&#39;dWl&#39;] to dW1, grads[&#39;db1&#39;] to db1, grads[&#39;dW2&#39;] to dW2, grads[&#39;db2&#39;] to db2</span>\n        <span class=\"n\">grads</span><span class=\"p\">[</span><span class=\"s1\">&#39;dW1&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">dW1</span>\n        <span class=\"n\">grads</span><span class=\"p\">[</span><span class=\"s1\">&#39;db1&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">db1</span>\n        <span class=\"n\">grads</span><span class=\"p\">[</span><span class=\"s1\">&#39;dW2&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">dW2</span>\n        <span class=\"n\">grads</span><span class=\"p\">[</span><span class=\"s1\">&#39;db2&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">db2</span>\n        \n        <span class=\"c1\"># Update parameters.</span>\n        <span class=\"c1\">### START CODE HERE ### (approx. 1 line of code)</span>\n        <span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"n\">update_parameters</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">grads</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span><span class=\"p\">)</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n<span class=\"err\">â€‹</span>\n        <span class=\"c1\"># Retrieve W1, b1, W2, b2 from parameters</span>\n        <span class=\"n\">W1</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;W1&#34;</span><span class=\"p\">]</span>\n        <span class=\"n\">b1</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;b1&#34;</span><span class=\"p\">]</span>\n        <span class=\"n\">W2</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;W2&#34;</span><span class=\"p\">]</span>\n        <span class=\"n\">b2</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;b2&#34;</span><span class=\"p\">]</span>\n        \n        <span class=\"c1\"># Print the cost every 100 training example</span>\n        <span class=\"k\">if</span> <span class=\"n\">print_cost</span> <span class=\"ow\">and</span> <span class=\"n\">i</span> <span class=\"o\">%</span> <span class=\"mi\">100</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n            <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s2\">&#34;Cost after iteration {}: {}&#34;</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">squeeze</span><span class=\"p\">(</span><span class=\"n\">cost</span><span class=\"p\">)))</span>\n        <span class=\"k\">if</span> <span class=\"n\">print_cost</span> <span class=\"ow\">and</span> <span class=\"n\">i</span> <span class=\"o\">%</span> <span class=\"mi\">100</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n            <span class=\"n\">costs</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">cost</span><span class=\"p\">)</span>\n       \n    <span class=\"c1\"># plot the cost</span>\n<span class=\"err\">â€‹</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">squeeze</span><span class=\"p\">(</span><span class=\"n\">costs</span><span class=\"p\">))</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">ylabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;cost&#39;</span><span class=\"p\">)</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">xlabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;iterations (per tens)&#39;</span><span class=\"p\">)</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">title</span><span class=\"p\">(</span><span class=\"s2\">&#34;Learning rate =&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">learning_rate</span><span class=\"p\">))</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">parameters</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>L-layer Neural Network</b></h2><p>ä½¿ç”¨ä¹‹å‰çš„å‡½æ•°ï¼š</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">initialize_parameters_deep</span><span class=\"p\">(</span><span class=\"n\">layers_dims</span><span class=\"p\">):</span>\n    <span class=\"o\">...</span>\n    <span class=\"k\">return</span> <span class=\"n\">parameters</span> \n<span class=\"k\">def</span> <span class=\"nf\">L_model_forward</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">parameters</span><span class=\"p\">):</span>\n    <span class=\"o\">...</span>\n    <span class=\"k\">return</span> <span class=\"n\">AL</span><span class=\"p\">,</span> <span class=\"n\">caches</span>\n<span class=\"k\">def</span> <span class=\"nf\">compute_cost</span><span class=\"p\">(</span><span class=\"n\">AL</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">):</span>\n    <span class=\"o\">...</span>\n    <span class=\"k\">return</span> <span class=\"n\">cost</span>\n<span class=\"k\">def</span> <span class=\"nf\">L_model_backward</span><span class=\"p\">(</span><span class=\"n\">AL</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">caches</span><span class=\"p\">):</span>\n    <span class=\"o\">...</span>\n    <span class=\"k\">return</span> <span class=\"n\">grads</span>\n<span class=\"k\">def</span> <span class=\"nf\">update_parameters</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">grads</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span><span class=\"p\">):</span>\n    <span class=\"o\">...</span>\n    <span class=\"k\">return</span> <span class=\"n\">parameters</span></code></pre></div><p>è¿™é‡Œä¸€å…±4å±‚ï¼š</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">layers_dims</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mi\">12288</span><span class=\"p\">,</span> <span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"mi\">7</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"c1\">#  4-layer model</span></code></pre></div><p>æ€è·¯æ˜¯ï¼š</p><ol><li>åˆå§‹åŒ–å‚æ•°</li><li>è¿›å…¥forçš„næ¬¡è¿­ä»£å¾ªç¯ï¼š<br/></li><ol><li>L_model_forward(X, parameters) å¾—åˆ° AL,caches</li><li>è®¡ç®—cost</li><li>L_model_backward(AL, Y, caches)è®¡ç®—grads</li><li>update_parameters(parameters, grads, learning_rate)æ›´æ–°å‚æ•°</li><li>æ¯100å±‚è®°å½•ä¸€ä¸‹costçš„å€¼</li></ol></ol><p class=\"ztext-empty-paragraph\"><br/></p><ol><li>ç”»å‡ºcostæ¢¯åº¦ä¸‹é™å›¾</li></ol><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: L_layer_model</span>\n<span class=\"err\">â€‹</span>\n<span class=\"k\">def</span> <span class=\"nf\">L_layer_model</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">layers_dims</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span> <span class=\"o\">=</span> <span class=\"mf\">0.0075</span><span class=\"p\">,</span> <span class=\"n\">num_iterations</span> <span class=\"o\">=</span> <span class=\"mi\">3000</span><span class=\"p\">,</span> <span class=\"n\">print_cost</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">):</span><span class=\"c1\">#lr was 0.009</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Implements a L-layer neural network: [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID.\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n</span><span class=\"s2\">    Y -- true &#34;label&#34; vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n</span><span class=\"s2\">    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n</span><span class=\"s2\">    learning_rate -- learning rate of the gradient descent update rule\n</span><span class=\"s2\">    num_iterations -- number of iterations of the optimization loop\n</span><span class=\"s2\">    print_cost -- if True, it prints the cost every 100 steps\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    parameters -- parameters learnt by the model. They can then be used to predict.\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n<span class=\"err\">â€‹</span>\n    <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">seed</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n    <span class=\"n\">costs</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>                         <span class=\"c1\"># keep track of cost</span>\n    \n    <span class=\"c1\"># Parameters initialization. (â‰ˆ 1 line of code)</span>\n    <span class=\"c1\">### START CODE HERE ###</span>\n    <span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"n\">initialize_parameters_deep</span><span class=\"p\">(</span><span class=\"n\">layers_dims</span><span class=\"p\">)</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"c1\"># Loop (gradient descent)</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">num_iterations</span><span class=\"p\">):</span>\n<span class=\"err\">â€‹</span>\n        <span class=\"c1\"># Forward propagation: [LINEAR -&gt; RELU]*(L-1) -&gt; LINEAR -&gt; SIGMOID.</span>\n        <span class=\"c1\">### START CODE HERE ### (â‰ˆ 1 line of code)</span>\n        <span class=\"n\">AL</span><span class=\"p\">,</span> <span class=\"n\">caches</span> <span class=\"o\">=</span> <span class=\"n\">L_model_forward</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">parameters</span><span class=\"p\">)</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n        \n        <span class=\"c1\"># Compute cost.</span>\n        <span class=\"c1\">### START CODE HERE ### (â‰ˆ 1 line of code)</span>\n        <span class=\"n\">cost</span> <span class=\"o\">=</span> <span class=\"n\">compute_cost</span><span class=\"p\">(</span><span class=\"n\">AL</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">)</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n    \n        <span class=\"c1\"># Backward propagation.</span>\n        <span class=\"c1\">### START CODE HERE ### (â‰ˆ 1 line of code)</span>\n        <span class=\"n\">grads</span> <span class=\"o\">=</span> <span class=\"n\">L_model_backward</span><span class=\"p\">(</span><span class=\"n\">AL</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">caches</span><span class=\"p\">)</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n \n        <span class=\"c1\"># Update parameters.</span>\n        <span class=\"c1\">### START CODE HERE ### (â‰ˆ 1 line of code)</span>\n        <span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"n\">update_parameters</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">grads</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span><span class=\"p\">)</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n                \n        <span class=\"c1\"># Print the cost every 100 training example</span>\n        <span class=\"k\">if</span> <span class=\"n\">print_cost</span> <span class=\"ow\">and</span> <span class=\"n\">i</span> <span class=\"o\">%</span> <span class=\"mi\">100</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n            <span class=\"k\">print</span> <span class=\"p\">(</span><span class=\"s2\">&#34;Cost after iteration </span><span class=\"si\">%i</span><span class=\"s2\">: </span><span class=\"si\">%f</span><span class=\"s2\">&#34;</span> <span class=\"o\">%</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">cost</span><span class=\"p\">))</span>\n        <span class=\"k\">if</span> <span class=\"n\">print_cost</span> <span class=\"ow\">and</span> <span class=\"n\">i</span> <span class=\"o\">%</span> <span class=\"mi\">100</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n            <span class=\"n\">costs</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">cost</span><span class=\"p\">)</span>\n            \n    <span class=\"c1\"># plot the cost</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">squeeze</span><span class=\"p\">(</span><span class=\"n\">costs</span><span class=\"p\">))</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">ylabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;cost&#39;</span><span class=\"p\">)</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">xlabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;iterations (per tens)&#39;</span><span class=\"p\">)</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">title</span><span class=\"p\">(</span><span class=\"s2\">&#34;Learning rate =&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">learning_rate</span><span class=\"p\">))</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">parameters</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>2500çš„è¿­ä»£æ¬¡æ•°ï¼Œç²¾åº¦è¾¾åˆ°äº†80%ï¼</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>å°ç»“</b></p><p>è¿‡ç¨‹å…¶å®æ˜¯å¾ˆæ¸…æ™°çš„ï¼Œå°±æ˜¯å…ˆåˆå§‹åŒ–å‚æ•°ï¼›å†å¼€å§‹å¾ªç¯ï¼Œå¾ªç¯ä¸­å…ˆè®¡ç®—å‰å‘ä¼ æ’­ï¼Œå¾—åˆ°æœ€åä¸€å±‚çš„ALï¼Œä»¥åŠæ¯ä¸€å±‚çš„cacheï¼Œå…¶ä¸­cacheåŒ…æ‹¬äº† A_prevï¼ŒWï¼Œbï¼ŒZï¼›ç„¶åè®¡ç®—ä¸€ä¸‹æ¯ä¸€æ¬¡è¿­ä»£çš„costï¼›å†è¿›è¡Œåå‘ä¼ æ’­ï¼Œå¾—åˆ°æ¯ä¸€å±‚çš„æ¢¯åº¦dA,dW,db;è®°å¾—æ¯100æ¬¡è¿­ä»£è®°å½•ä¸€ä¸‹costå€¼ï¼Œè¿™æ ·å°±å¯ä»¥ç”»å‡ºcostæ˜¯å¦‚ä½•ä¸‹é™çš„äº†ã€‚</p><p>part1æ„å»ºçš„é‚£äº›å‡½æ•°ï¼Œä¸€æ­¥æ­¥æ¥æ˜¯æ¯”è¾ƒç®€å•çš„ï¼Œä½†æ˜¯å¦‚æœè‡ªå·±è¦ä¸€ä¸‹å­æƒ³å‡ºæ¥çš„è¯ï¼Œä¹Ÿå¾ˆéš¾æƒ³å¾—åˆ°ã€‚æ‰€ä»¥æ€è·¯è¦æ¸…æ™°ï¼Œä¸€æ­¥ä¸€æ­¥æ¥ï¼Œæ‰èƒ½æ„å»ºå¥½å‡½æ•°ï¼</p>", 
            "topic": [
                {
                    "tag": "æœºå™¨å­¦ä¹ ", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "ç¥ç»ç½‘ç»œ", 
                    "tagLink": "https://api.zhihu.com/topics/19607065"
                }, 
                {
                    "tag": "æ·±åº¦å­¦ä¹ ï¼ˆDeep Learningï¼‰", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/44674206", 
            "userName": "ç›´ä¸Šäº‘éœ„", 
            "userLink": "https://www.zhihu.com/people/1033165ce4ad9c3fce69a0793dfab8ad", 
            "upvote": 0, 
            "title": "DeepLearning.aiç¬”è®°:(1-4)-æ·±å±‚ç¥ç»ç½‘ç»œ", 
            "content": "<h2>æœ¬æ–‡é¦–å‘äºä¸ªäººåšå®¢ï¼š<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">fangzh.top</span><span class=\"invisible\"></span></a>ï¼Œæ¬¢è¿æ¥è®¿</h2><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>è¿™ä¸€å‘¨ä¸»è¦è®²äº†æ·±å±‚çš„ç¥ç»ç½‘ç»œæ­å»ºã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>æ·±å±‚ç¥ç»ç½‘ç»œçš„ç¬¦å·è¡¨ç¤º</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-b1c7c7c0a4d0ac817ff518034b5c7427_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"998\" data-rawheight=\"502\" class=\"origin_image zh-lightbox-thumb\" width=\"998\" data-original=\"https://pic4.zhimg.com/v2-b1c7c7c0a4d0ac817ff518034b5c7427_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;998&#39; height=&#39;502&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"998\" data-rawheight=\"502\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"998\" data-original=\"https://pic4.zhimg.com/v2-b1c7c7c0a4d0ac817ff518034b5c7427_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-b1c7c7c0a4d0ac817ff518034b5c7427_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>åœ¨æ·±å±‚çš„ç¥ç»ç½‘ç»œä¸­ï¼Œ</p><ul><li><img src=\"https://www.zhihu.com/equation?tex=L\" alt=\"L\" eeimg=\"1\"/> è¡¨ç¤ºç¥ç»ç½‘ç»œçš„å±‚æ•° <img src=\"https://www.zhihu.com/equation?tex=+L+%3D+4\" alt=\" L = 4\" eeimg=\"1\"/> </li><li><img src=\"https://www.zhihu.com/equation?tex=n%5E%7B%5Bl%5D%7D\" alt=\"n^{[l]}\" eeimg=\"1\"/> è¡¨ç¤ºç¬¬ <img src=\"https://www.zhihu.com/equation?tex=l\" alt=\"l\" eeimg=\"1\"/> å±‚çš„ç¥ç»ç½‘ç»œä¸ªæ•°</li><li><img src=\"https://www.zhihu.com/equation?tex=W%5E%7B%5Bl%5D%7D%3A+%28n%5E%7B%5Bl%5D%7D%2Cn%5E%7Bl-1%7D%29\" alt=\"W^{[l]}: (n^{[l]},n^{l-1})\" eeimg=\"1\"/> </li><li><img src=\"https://www.zhihu.com/equation?tex=dW%5E%7B%5Bl%5D%7D%3A+%28n%5E%7B%5Bl%5D%7D%2Cn%5E%7Bl-1%7D%29\" alt=\"dW^{[l]}: (n^{[l]},n^{l-1})\" eeimg=\"1\"/> </li><li><img src=\"https://www.zhihu.com/equation?tex=b%5E%7B%5Bl%5D%7D%3A+%28n%5E%7B%5Bl%5D%7D%2C1%29\" alt=\"b^{[l]}: (n^{[l]},1)\" eeimg=\"1\"/> </li><li><img src=\"https://www.zhihu.com/equation?tex=db%5E%7B%5Bl%5D%7D%3A+%28n%5E%7B%5Bl%5D%7D%2C1%29\" alt=\"db^{[l]}: (n^{[l]},1)\" eeimg=\"1\"/> </li><li><img src=\"https://www.zhihu.com/equation?tex=z%5E%7B%5Bl%5D%7D%3A%28n%5E%7B%5Bl%5D%7D%2C1%29\" alt=\"z^{[l]}:(n^{[l]},1)\" eeimg=\"1\"/> </li><li><img src=\"https://www.zhihu.com/equation?tex=a%5E%7B%5Bl%5D%7D%3A%28n%5E%7B%5Bl%5D%7D%2C1%29\" alt=\"a^{[l]}:(n^{[l]},1)\" eeimg=\"1\"/> </li></ul><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p><b>å‰å‘ä¼ æ’­</b></p><p>input <img src=\"https://www.zhihu.com/equation?tex=a%5E%7B%5Bl-1%5D%7D\" alt=\"a^{[l-1]}\" eeimg=\"1\"/> </p><p>output <img src=\"https://www.zhihu.com/equation?tex=+a%5E%7B%5Bl%5D%7D%2Ccache+%28z%5E%7B%5Bl%5D%7D%29\" alt=\" a^{[l]},cache (z^{[l]})\" eeimg=\"1\"/> ï¼Œå…¶ä¸­cacheä¹Ÿé¡ºä¾¿æŠŠ <img src=\"https://www.zhihu.com/equation?tex=W%5E%7B%5Bl%5D%7D%2C++b%5E%7B%5Bl%5D%7D\" alt=\"W^{[l]},  b^{[l]}\" eeimg=\"1\"/> ä¹Ÿä¿å­˜ä¸‹æ¥äº†</p><p>æ‰€ä»¥ï¼Œå‰å‘ä¼ æ’­çš„å…¬å¼å¯ä»¥å†™ä½œï¼š</p><p><img src=\"https://www.zhihu.com/equation?tex=Z%5E%7B%5Bl%5D%7D+%3D+W%5E%7B%5Bl%5D%7D+A%5E%7B%5Bl-1%5D%7D+%2B+b%5E%7B%5Bl%5D%7D\" alt=\"Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=A%5E%7B%5Bl%5D%7D+%3D+g%5E%7B%5Bl%5D%7D%28Z%5E%7B%5Bl%5D%7D%29\" alt=\"A^{[l]} = g^{[l]}(Z^{[l]})\" eeimg=\"1\"/> </p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>ç»´åº¦</b></p><p>å‡è®¾æœ‰mä¸ªæ ·æœ¬ï¼Œé‚£ä¹ˆ <img src=\"https://www.zhihu.com/equation?tex=Z%5E%7B%5Bl%5D%7D\" alt=\"Z^{[l]}\" eeimg=\"1\"/> ç»´åº¦å°±æ˜¯ <img src=\"https://www.zhihu.com/equation?tex=%28n%5E%7B%5Bl%5D%7D%2C+m%29\" alt=\"(n^{[l]}, m)\" eeimg=\"1\"/> ï¼Œ <img src=\"https://www.zhihu.com/equation?tex=A%5E%7B%5Bl%5D%7D\" alt=\"A^{[l]}\" eeimg=\"1\"/> çš„ç»´åº¦å’Œ <img src=\"https://www.zhihu.com/equation?tex=Z%5E%7B%5Bl%5D%7D\" alt=\"Z^{[l]}\" eeimg=\"1\"/> ä¸€æ ·ã€‚</p><p>é‚£ä¹ˆ <img src=\"https://www.zhihu.com/equation?tex=W%5E%7B%5Bl%5D%7D+A%5E%7B%5Bl-1%5D%7D\" alt=\"W^{[l]} A^{[l-1]}\" eeimg=\"1\"/> ç»´åº¦å°±æ˜¯ <img src=\"https://www.zhihu.com/equation?tex=%28n%5E%7B%5Bl%5D%7D%2Cn%5E%7Bl-1%7D%29++%5Ctimes++%28n%5E%7B%5Bl-1%5D%7D%2Cm%29\" alt=\"(n^{[l]},n^{l-1})  \\times  (n^{[l-1]},m)\" eeimg=\"1\"/>  ä¹Ÿå°±æ˜¯  <img src=\"https://www.zhihu.com/equation?tex=%28n%5E%7B%5Bl%5D%7D%2C+m%29\" alt=\"(n^{[l]}, m)\" eeimg=\"1\"/> ï¼Œè¿™ä¸ªæ—¶å€™ï¼Œè¿˜éœ€è¦åŠ ä¸Š <img src=\"https://www.zhihu.com/equation?tex=b%5E%7B%5Bl%5D%7D\" alt=\"b^{[l]}\" eeimg=\"1\"/> ï¼Œè€Œ <img src=\"https://www.zhihu.com/equation?tex=b%5E%7B%5Bl%5D%7D\" alt=\"b^{[l]}\" eeimg=\"1\"/> æœ¬èº«çš„ç»´åº¦æ˜¯ <img src=\"https://www.zhihu.com/equation?tex=%28n%5E%7B%5Bl%5D%7D%2C1%29\" alt=\"(n^{[l]},1)\" eeimg=\"1\"/> ï¼Œå€ŸåŠ©pythonçš„å¹¿æ’­ï¼Œæ‰©å……åˆ°äº†mä¸ªç»´åº¦ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>åå‘ä¼ æ’­</b></p><p>input <img src=\"https://www.zhihu.com/equation?tex=da%5E%7B%5Bl%5D%7D\" alt=\"da^{[l]}\" eeimg=\"1\"/> </p><p>output <img src=\"https://www.zhihu.com/equation?tex=da%5E%7B%5Bl-1%5D%7D+%2C+dW%5E%7B%5Bl%5D%7D+%2C+db%5E%7B%5Bl%5D%7D\" alt=\"da^{[l-1]} , dW^{[l]} , db^{[l]}\" eeimg=\"1\"/> </p><p>å…¬å¼ï¼š</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-1bfccc4955853c52ac803f3bd5984b01_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"261\" data-rawheight=\"131\" class=\"content_image\" width=\"261\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;261&#39; height=&#39;131&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"261\" data-rawheight=\"131\" class=\"content_image lazy\" width=\"261\" data-actualsrc=\"https://pic2.zhimg.com/v2-1bfccc4955853c52ac803f3bd5984b01_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>å‘é‡åŒ–ï¼š</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-5d0b14d815bb8b995782e5bc83f5205e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"514\" data-rawheight=\"165\" class=\"origin_image zh-lightbox-thumb\" width=\"514\" data-original=\"https://pic3.zhimg.com/v2-5d0b14d815bb8b995782e5bc83f5205e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;514&#39; height=&#39;165&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"514\" data-rawheight=\"165\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"514\" data-original=\"https://pic3.zhimg.com/v2-5d0b14d815bb8b995782e5bc83f5205e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-5d0b14d815bb8b995782e5bc83f5205e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>æ­£å‘ä¼ æ’­å’Œåå‘ä¼ æ’­å¦‚å›¾ï¼š</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-394d24a0c677c67b764e7e70a00df14e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1124\" data-rawheight=\"557\" class=\"origin_image zh-lightbox-thumb\" width=\"1124\" data-original=\"https://pic3.zhimg.com/v2-394d24a0c677c67b764e7e70a00df14e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1124&#39; height=&#39;557&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1124\" data-rawheight=\"557\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1124\" data-original=\"https://pic3.zhimg.com/v2-394d24a0c677c67b764e7e70a00df14e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-394d24a0c677c67b764e7e70a00df14e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>å…·ä½“è¿‡ç¨‹ä¸ºï¼Œç¬¬ä¸€å±‚å’Œç¬¬äºŒå±‚ç”¨Reluå‡½æ•°ï¼Œç¬¬ä¸‰å±‚è¾“å‡ºç”¨sigmoidï¼Œè¿™ä¸ªæ—¶å€™çš„è¾“å‡ºå€¼æ˜¯ <img src=\"https://www.zhihu.com/equation?tex=a%5E%7B%5B3%5D%7D\" alt=\"a^{[3]}\" eeimg=\"1\"/> </p><p>è€Œé¦–å…ˆè¿›è¡Œåå‘ä¼ æ’­çš„æ—¶å€™å…ˆæ±‚å¾— <img src=\"https://www.zhihu.com/equation?tex=da%5E%7B%5B3%5D%7D+%3D+-+%5Cfrac%7By%7D%7Ba%7D+-+%5Cfrac%7B1-y%7D%7B1-a%7D\" alt=\"da^{[3]} = - \\frac{y}{a} - \\frac{1-y}{1-a}\" eeimg=\"1\"/> ï¼Œç„¶åå†åŒ…æ‹¬ä¹‹å‰å­˜åœ¨cacheé‡Œé¢çš„ <img src=\"https://www.zhihu.com/equation?tex=z%5E%7B%5B3%5D%7D\" alt=\"z^{[3]}\" eeimg=\"1\"/> ,åå‘ä¼ æ’­å¯ä»¥å¾—åˆ° <img src=\"https://www.zhihu.com/equation?tex=dw%5E%7B%5B3%5D%7D%2C+db%5E%7B%5B3%5D%7D%2Cda%5E%7B%5B2%5D%7D\" alt=\"dw^{[3]}, db^{[3]},da^{[2]}\" eeimg=\"1\"/> ï¼Œç„¶åç»§ç»­åå‘ï¼Œç›´åˆ°å¾—åˆ°äº† <img src=\"https://www.zhihu.com/equation?tex=dw%5E%7B%5B1%5D%7D%2Cdb%5E%7B%5B1%5D%7D\" alt=\"dw^{[1]},db^{[1]}\" eeimg=\"1\"/> åï¼Œæ›´æ–°ä¸€ä¸‹wï¼Œbçš„å‚æ•°ï¼Œç„¶åç»§ç»­åšå‰å‘ä¼ æ’­ã€åå‘ä¼ æ’­ï¼Œä¸æ–­å¾ªç¯ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>Why Deepï¼Ÿ</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-7ce4ac52be3a657bce1ed18f38697bee_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1216\" data-rawheight=\"515\" class=\"origin_image zh-lightbox-thumb\" width=\"1216\" data-original=\"https://pic3.zhimg.com/v2-7ce4ac52be3a657bce1ed18f38697bee_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1216&#39; height=&#39;515&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1216\" data-rawheight=\"515\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1216\" data-original=\"https://pic3.zhimg.com/v2-7ce4ac52be3a657bce1ed18f38697bee_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-7ce4ac52be3a657bce1ed18f38697bee_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>å¦‚å›¾ç›´è§‚ä¸Šæ„Ÿè§‰ï¼Œæ¯”å¦‚ç¬¬ä¸€å±‚ï¼Œå®ƒä¼šå…ˆè¯†åˆ«å‡ºä¸€äº›è¾¹ç¼˜ä¿¡æ¯ï¼›ç¬¬äºŒå±‚åˆ™å°†è¿™äº›è¾¹ç¼˜è¿›è¡Œæ•´åˆï¼Œå¾—åˆ°ä¸€äº›äº”å®˜ä¿¡æ¯ï¼Œå¦‚çœ¼ç›ã€å˜´å·´ç­‰ï¼›åˆ°äº†ç¬¬ä¸‰å±‚ï¼Œå°±å¯ä»¥å°†è¿™äº›ä¿¡æ¯æ•´åˆèµ·æ¥ï¼Œè¾“å‡ºä¸€å¼ äººè„¸äº†ã€‚</p><p>å¦‚æœç½‘ç»œå±‚æ•°ä¸å¤Ÿæ·±çš„è¯ï¼Œå¯ä»¥ç»„åˆçš„æƒ…å†µå°±å¾ˆå°‘ï¼Œæˆ–è€…éœ€è¦ç±»ä¼¼é—¨ç”µè·¯é‚£æ ·ï¼Œç”¨å•å±‚å¾ˆå¤šä¸ªç‰¹å¾æ‰èƒ½å¾—åˆ°å’Œæ·±å±‚ç¥ç»ç½‘ç»œç±»ä¼¼çš„æ•ˆæœã€‚</p><h2><b>æ­å»ºæ·±å±‚ç¥ç»ç½‘ç»œå—</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-d4fb44293f189d94a830d737179e12d5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"585\" data-rawheight=\"640\" class=\"origin_image zh-lightbox-thumb\" width=\"585\" data-original=\"https://pic2.zhimg.com/v2-d4fb44293f189d94a830d737179e12d5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;585&#39; height=&#39;640&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"585\" data-rawheight=\"640\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"585\" data-original=\"https://pic2.zhimg.com/v2-d4fb44293f189d94a830d737179e12d5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-d4fb44293f189d94a830d737179e12d5_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>å’Œä¹‹å‰è¯´çš„ä¸€æ ·ï¼Œä¸€ä¸ªç½‘ç»œå—ä¸­åŒ…å«äº†å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­ã€‚</p><p>å‰å‘è¾“å…¥ <img src=\"https://www.zhihu.com/equation?tex=a%5E%7B%5Bl-1%5D%7D\" alt=\"a^{[l-1]}\" eeimg=\"1\"/> ï¼Œç»è¿‡ç¥ç»ç½‘ç»œçš„è®¡ç®—ï¼Œ <img src=\"https://www.zhihu.com/equation?tex=g%5E%7B%5Bl%5D%7D%28w%5E%7B%5Bl%5D%7Da%5E%7B%5Bl-1%5D%7D+%2B+b%5E%7B%5Bl%5D%7D%29\" alt=\"g^{[l]}(w^{[l]}a^{[l-1]} + b^{[l]})\" eeimg=\"1\"/> å¾—åˆ° <img src=\"https://www.zhihu.com/equation?tex=a%5E%7B%5Bl%5D%7D\" alt=\"a^{[l]}\" eeimg=\"1\"/> </p><p>åå‘ä¼ æ’­ï¼Œè¾“å…¥ <img src=\"https://www.zhihu.com/equation?tex=da%5E%7B%5Bl%5D%7D\" alt=\"da^{[l]}\" eeimg=\"1\"/> ï¼Œå†æœ‰ä¹‹å‰åœ¨cacheçš„ <img src=\"https://www.zhihu.com/equation?tex=z%5E%7B%5Bl%5D%7D\" alt=\"z^{[l]}\" eeimg=\"1\"/> ,å³å¯å¾—åˆ° <img src=\"https://www.zhihu.com/equation?tex=dw%5E%7B%5Bl%5D%7D%2Cdb%5E%7B%5Bl%5D%7D\" alt=\"dw^{[l]},db^{[l]}\" eeimg=\"1\"/> è¿˜æœ‰ä¸Šä¸€å±‚çš„ <img src=\"https://www.zhihu.com/equation?tex=da%5E%7B%5Bl-1%5D%7D\" alt=\"da^{[l-1]}\" eeimg=\"1\"/> </p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>å‚æ•°ä¸è¶…å‚æ•°</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p>è¶…å‚æ•°å°±æ˜¯ä½ è‡ªå·±è°ƒçš„ï¼Œç„å­¦å‚æ•°ï¼š</p><ul><li>learning_rate</li><li>iterations</li><li>L = len(hidden layer)</li><li><img src=\"https://www.zhihu.com/equation?tex=n%5E%7B%5Bl%5D%7D\" alt=\"n^{[l]}\" eeimg=\"1\"/> </li><li>activation function</li><li>mini batch sizeï¼ˆæœ€å°çš„è®¡ç®—æ‰¹ï¼‰</li><li>regularizationï¼ˆæ­£åˆ™ï¼‰</li><li>momentumï¼ˆåŠ¨é‡ï¼‰</li></ul>", 
            "topic": [
                {
                    "tag": "ç¥ç»ç½‘ç»œ", 
                    "tagLink": "https://api.zhihu.com/topics/19607065"
                }, 
                {
                    "tag": "æ·±åº¦å­¦ä¹ ï¼ˆDeep Learningï¼‰", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "æœºå™¨å­¦ä¹ ", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/44396545", 
            "userName": "ç›´ä¸Šäº‘éœ„", 
            "userLink": "https://www.zhihu.com/people/1033165ce4ad9c3fce69a0793dfab8ad", 
            "upvote": 0, 
            "title": "DeepLearning.aiä½œä¸š:(1-3)-æµ…å±‚ç¥ç»ç½‘ç»œ", 
            "content": "<h2>æœ¬æ–‡é¦–å‘äºä¸ªäººåšå®¢ï¼š<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">fangzh.top</span><span class=\"invisible\"></span></a></h2><h2>å‰è¨€ï¼š</h2><ol><li>ä¸è¦æŠ„ä½œä¸šï¼</li><li>æˆ‘åªæ˜¯æŠŠæ€è·¯æ•´ç†äº†ï¼Œä¾›ä¸ªäººå­¦ä¹ ã€‚</li><li>ä¸è¦æŠ„ä½œä¸šï¼</li></ol><h2>æ•°æ®é›†</h2><p>æ•°æ®é›†æ˜¯ä¸€ä¸ªç±»ä¼¼èŠ±çš„æ•°æ®é›†ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-6ce4b7b2aed0978f7c95f2a6a350a124_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"515\" data-rawheight=\"349\" class=\"origin_image zh-lightbox-thumb\" width=\"515\" data-original=\"https://pic1.zhimg.com/v2-6ce4b7b2aed0978f7c95f2a6a350a124_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;515&#39; height=&#39;349&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"515\" data-rawheight=\"349\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"515\" data-original=\"https://pic1.zhimg.com/v2-6ce4b7b2aed0978f7c95f2a6a350a124_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-6ce4b7b2aed0978f7c95f2a6a350a124_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>è€Œå¦‚æœç”¨ä¼ ç»Ÿçš„logistic regressionï¼Œåšå‡ºæ¥çš„å°±æ˜¯ä¸€ä¸ªäºŒåˆ†ç±»é—®é¢˜ï¼Œç®€å•ç²—æš´çš„åˆ’å‡ºäº†ä¸€æ¡çº¿ï¼Œ</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-c789940ce3bfc86580b8e3e697e2049b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"532\" data-rawheight=\"391\" class=\"origin_image zh-lightbox-thumb\" width=\"532\" data-original=\"https://pic4.zhimg.com/v2-c789940ce3bfc86580b8e3e697e2049b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;532&#39; height=&#39;391&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"532\" data-rawheight=\"391\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"532\" data-original=\"https://pic4.zhimg.com/v2-c789940ce3bfc86580b8e3e697e2049b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-c789940ce3bfc86580b8e3e697e2049b_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>å¯ä»¥çœ‹è§ï¼Œå‡†ç¡®ç‡åªæœ‰47%ã€‚</p><p>æ‰€ä»¥å°±éœ€è¦æ„å»ºç¥ç»ç½‘ç»œæ¨¡å‹äº†ã€‚</p><h2>ç¥ç»ç½‘ç»œæ¨¡å‹</h2><p><b>Reminder</b>: The general methodology to build a Neural Network is to:</p><div class=\"highlight\"><pre><code class=\"language-text\">1. Define the neural network structure ( # of input units,  # of hidden units, etc). \n2. Initialize the model&#39;s parameters\n3. Loop:\n    - Implement forward propagation\n    - Compute loss\n    - Implement backward propagation to get the gradients\n    - Update parameters (gradient descent)</code></pre></div><p>å·²ç»ç»™å‡ºæ€è·¯äº†ï¼š</p><ol><li>å®šä¹‰ç¥ç»ç½‘ç»œçš„ç»“æ„</li><li>åˆå§‹åŒ–æ¨¡å‹å‚æ•°</li><li>å¾ªç¯ï¼š</li><ol><li>è®¡ç®—æ­£å‘ä¼ æ’­</li><li>è®¡ç®—æŸå¤±å‡½æ•°</li><li>è®¡ç®—åå‘ä¼ æ’­æ¥å¾—åˆ°grad</li><li>æ›´æ–°å‚æ•°</li></ol></ol><h2>1. å®šä¹‰ç¥ç»ç½‘ç»œç»“æ„</h2><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: layer_sizes</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">layer_sizes</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    X -- input dataset of shape (input size, number of examples)\n</span><span class=\"s2\">    Y -- labels of shape (output size, number of examples)\n</span><span class=\"s2\">\n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    n_x -- the size of the input layer\n</span><span class=\"s2\">    n_h -- the size of the hidden layer\n</span><span class=\"s2\">    n_y -- the size of the output layer\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    <span class=\"c1\">### START CODE HERE ### (â‰ˆ 3 lines of code)</span>\n    <span class=\"n\">n_x</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"c1\"># size of input layer</span>\n    <span class=\"n\">n_h</span> <span class=\"o\">=</span> <span class=\"mi\">4</span>\n    <span class=\"n\">n_y</span> <span class=\"o\">=</span> <span class=\"n\">Y</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"c1\"># size of output layer</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    <span class=\"k\">return</span> <span class=\"p\">(</span><span class=\"n\">n_x</span><span class=\"p\">,</span> <span class=\"n\">n_h</span><span class=\"p\">,</span> <span class=\"n\">n_y</span><span class=\"p\">)</span></code></pre></div><h2>2. åˆå§‹åŒ–å‚æ•°</h2><p>æ¥åˆå§‹åŒ–wå’Œbçš„å‚æ•°</p><p>w: <code>np.random.rand(a,b) * 0.01</code></p><p>b: <code>np.zeros((a,b))</code></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: initialize_parameters</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">initialize_parameters</span><span class=\"p\">(</span><span class=\"n\">n_x</span><span class=\"p\">,</span> <span class=\"n\">n_h</span><span class=\"p\">,</span> <span class=\"n\">n_y</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Argument:\n</span><span class=\"s2\">    n_x -- size of the input layer\n</span><span class=\"s2\">    n_h -- size of the hidden layer\n</span><span class=\"s2\">    n_y -- size of the output layer\n</span><span class=\"s2\">\n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    params -- python dictionary containing your parameters:\n</span><span class=\"s2\">                    W1 -- weight matrix of shape (n_h, n_x)\n</span><span class=\"s2\">                    b1 -- bias vector of shape (n_h, 1)\n</span><span class=\"s2\">                    W2 -- weight matrix of shape (n_y, n_h)\n</span><span class=\"s2\">                    b2 -- bias vector of shape (n_y, 1)\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n\n    <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">seed</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">)</span> <span class=\"c1\"># we set up a seed so that your output matches ours although the initialization is random.</span>\n\n    <span class=\"c1\">### START CODE HERE ### (â‰ˆ 4 lines of code)</span>\n    <span class=\"n\">W1</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">n_h</span><span class=\"p\">,</span> <span class=\"n\">n_x</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"mf\">0.01</span>\n    <span class=\"n\">b1</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">n_h</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n    <span class=\"n\">W2</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">n_y</span><span class=\"p\">,</span> <span class=\"n\">n_h</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"mf\">0.01</span>\n    <span class=\"n\">b2</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">n_y</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n\n    <span class=\"k\">assert</span> <span class=\"p\">(</span><span class=\"n\">W1</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"p\">(</span><span class=\"n\">n_h</span><span class=\"p\">,</span> <span class=\"n\">n_x</span><span class=\"p\">))</span>\n    <span class=\"k\">assert</span> <span class=\"p\">(</span><span class=\"n\">b1</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"p\">(</span><span class=\"n\">n_h</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n    <span class=\"k\">assert</span> <span class=\"p\">(</span><span class=\"n\">W2</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"p\">(</span><span class=\"n\">n_y</span><span class=\"p\">,</span> <span class=\"n\">n_h</span><span class=\"p\">))</span>\n    <span class=\"k\">assert</span> <span class=\"p\">(</span><span class=\"n\">b2</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"p\">(</span><span class=\"n\">n_y</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n\n    <span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">&#34;W1&#34;</span><span class=\"p\">:</span> <span class=\"n\">W1</span><span class=\"p\">,</span>\n                  <span class=\"s2\">&#34;b1&#34;</span><span class=\"p\">:</span> <span class=\"n\">b1</span><span class=\"p\">,</span>\n                  <span class=\"s2\">&#34;W2&#34;</span><span class=\"p\">:</span> <span class=\"n\">W2</span><span class=\"p\">,</span>\n                  <span class=\"s2\">&#34;b2&#34;</span><span class=\"p\">:</span> <span class=\"n\">b2</span><span class=\"p\">}</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">parameters</span></code></pre></div><h2>3. loop</h2><p>åœ¨è¿™é‡Œå¯ä»¥ä½¿ç”¨sigmoid()æ¥åšè¾“å‡ºå±‚çš„å‡½æ•°ï¼Œnp.tanh()æ¥åšhidden layerçš„æ¿€æ´»å‡½æ•°ã€‚</p><h2>3.1 forward propagation</h2><p>åœ¨è¿™ä¸ªå‡½æ•°ä¸­ï¼Œè¾“å…¥çš„æ˜¯Xï¼Œå’Œparametersï¼Œç„¶åå°±å¯ä»¥æ ¹æ®</p><p><img src=\"https://www.zhihu.com/equation?tex=z%5E%7B%5B1%5D+%28i%29%7D+%3D++W%5E%7B%5B1%5D%7D+x%5E%7B%28i%29%7D+%2B+b%5E%7B%5B1%5D%7D%5Ctag%7B1%7D\" alt=\"z^{[1] (i)} =  W^{[1]} x^{(i)} + b^{[1]}\\tag{1}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=a%5E%7B%5B1%5D+%28i%29%7D+%3D+%5Ctanh%28z%5E%7B%5B1%5D+%28i%29%7D%29%5Ctag%7B2%7D\" alt=\"a^{[1] (i)} = \\tanh(z^{[1] (i)})\\tag{2}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=z%5E%7B%5B2%5D+%28i%29%7D+%3D+W%5E%7B%5B2%5D%7D+a%5E%7B%5B1%5D+%28i%29%7D+%2B+b%5E%7B%5B2%5D%7D%5Ctag%7B3%7D\" alt=\"z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2]}\\tag{3}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%5Chat%7By%7D%5E%7B%28i%29%7D+%3D+a%5E%7B%5B2%5D+%28i%29%7D+%3D+%5Csigma%28z%5E%7B+%5B2%5D+%28i%29%7D%29%5Ctag%7B4%7D\" alt=\"\\hat{y}^{(i)} = a^{[2] (i)} = \\sigma(z^{ [2] (i)})\\tag{4}\" eeimg=\"1\"/> </p><p>å¾—åˆ°æ¯ä¸€å±‚çš„Zå’ŒAäº†ã€‚</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: forward_propagation</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">forward_propagation</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">parameters</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Argument:\n</span><span class=\"s2\">    X -- input data of size (n_x, m)\n</span><span class=\"s2\">    parameters -- python dictionary containing your parameters (output of initialization function)\n</span><span class=\"s2\">\n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    A2 -- The sigmoid output of the second activation\n</span><span class=\"s2\">    cache -- a dictionary containing &#34;Z1&#34;, &#34;A1&#34;, &#34;Z2&#34; and &#34;A2&#34;\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    <span class=\"c1\"># Retrieve each parameter from the dictionary &#34;parameters&#34;</span>\n    <span class=\"c1\">### START CODE HERE ### (â‰ˆ 4 lines of code)</span>\n    <span class=\"n\">W1</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;W1&#39;</span><span class=\"p\">]</span>\n    <span class=\"n\">b1</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;b1&#39;</span><span class=\"p\">]</span>\n    <span class=\"n\">W2</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;W2&#39;</span><span class=\"p\">]</span>\n    <span class=\"n\">b2</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;b2&#39;</span><span class=\"p\">]</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n\n    <span class=\"c1\"># Implement Forward Propagation to calculate A2 (probabilities)</span>\n    <span class=\"c1\">### START CODE HERE ### (â‰ˆ 4 lines of code)</span>\n    <span class=\"n\">Z1</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">W1</span><span class=\"p\">,</span><span class=\"n\">X</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">b1</span>\n    <span class=\"n\">A1</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">tanh</span><span class=\"p\">(</span><span class=\"n\">Z1</span><span class=\"p\">)</span>\n    <span class=\"n\">Z2</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">W2</span><span class=\"p\">,</span><span class=\"n\">A1</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">b2</span>\n    <span class=\"n\">A2</span> <span class=\"o\">=</span> <span class=\"n\">sigmoid</span><span class=\"p\">(</span><span class=\"n\">Z2</span><span class=\"p\">)</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n\n    <span class=\"k\">assert</span><span class=\"p\">(</span><span class=\"n\">A2</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]))</span>\n\n    <span class=\"n\">cache</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">&#34;Z1&#34;</span><span class=\"p\">:</span> <span class=\"n\">Z1</span><span class=\"p\">,</span>\n             <span class=\"s2\">&#34;A1&#34;</span><span class=\"p\">:</span> <span class=\"n\">A1</span><span class=\"p\">,</span>\n             <span class=\"s2\">&#34;Z2&#34;</span><span class=\"p\">:</span> <span class=\"n\">Z2</span><span class=\"p\">,</span>\n             <span class=\"s2\">&#34;A2&#34;</span><span class=\"p\">:</span> <span class=\"n\">A2</span><span class=\"p\">}</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">A2</span><span class=\"p\">,</span> <span class=\"n\">cache</span></code></pre></div><h2>3.2 cost</h2><p>æ¥ä¸‹æ¥ï¼Œåœ¨å¾—åˆ°A2çš„å€¼åï¼Œå°±å¯ä»¥æ ¹æ®å…¬å¼æ¥è®¡ç®—æŸå¤±å‡½æ•°äº†ã€‚</p><p><img src=\"https://www.zhihu.com/equation?tex=J+%3D+-+%5Cfrac%7B1%7D%7Bm%7D+%5Csum%5Climits_%7Bi+%3D+0%7D%5E%7Bm%7D+%5Clarge%7B%28%7D+%5Csmall+y%5E%7B%28i%29%7D%5Clog%5Cleft%28a%5E%7B%5B2%5D+%28i%29%7D%5Cright%29+%2B+%281-y%5E%7B%28i%29%7D%29%5Clog%5Cleft%281-+a%5E%7B%5B2%5D+%28i%29%7D%5Cright%29+%5Clarge%7B%29%7D+%5Csmall\" alt=\"J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large{(} \\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right) \\large{)} \\small\" eeimg=\"1\"/> </p><p>åœ¨è¿™é‡Œéœ€è¦æ³¨æ„çš„æ˜¯äº¤å‰ç†µçš„è®¡ç®—ï¼Œäº¤å‰ç†µä½¿ç”¨np.multiply()æ¥è®¡ç®—ï¼Œç„¶åç”¨np.sum()ï¼Œæ±‚å’Œã€‚</p><p>è€Œå•å•è®¡ç®—<code>logprobs = np.multiply(np.log(A2),Y)</code>æ˜¯ä¸å¤Ÿçš„ï¼Œå› ä¸ºè¿™ä¸ªåªå¾—åˆ°äº†å…¬å¼çš„å‰ä¸€åŠçš„éƒ¨åˆ†ï¼ŒY=0çš„éƒ¨åˆ†åœ¨å…ƒç´ ç›¸ä¹˜ä¸­å°±ç›¸å½“äºæ²¡æœ‰äº†ï¼Œæ‰€ä»¥è¿˜è¦å†åé¢åŠ ä¸€é¡¹<code>np.multiply(np.log(1-A2),1-Y)</code></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: compute_cost</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">compute_cost</span><span class=\"p\">(</span><span class=\"n\">A2</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">parameters</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Computes the cross-entropy cost given in equation (13)\n</span><span class=\"s2\">\n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n</span><span class=\"s2\">    Y -- &#34;true&#34; labels vector of shape (1, number of examples)\n</span><span class=\"s2\">    parameters -- python dictionary containing your parameters W1, b1, W2 and b2\n</span><span class=\"s2\">\n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    cost -- cross-entropy cost given equation (13)\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n\n    <span class=\"n\">m</span> <span class=\"o\">=</span> <span class=\"n\">Y</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"c1\"># number of example</span>\n\n    <span class=\"c1\"># Compute the cross-entropy cost</span>\n    <span class=\"c1\">### START CODE HERE ### (â‰ˆ 2 lines of code)</span>\n    <span class=\"n\">logprobs</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">multiply</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">log</span><span class=\"p\">(</span><span class=\"n\">A2</span><span class=\"p\">),</span><span class=\"n\">Y</span><span class=\"p\">)</span>  <span class=\"o\">+</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">multiply</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">log</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"o\">-</span><span class=\"n\">A2</span><span class=\"p\">),</span><span class=\"mi\">1</span><span class=\"o\">-</span><span class=\"n\">Y</span><span class=\"p\">)</span>\n    <span class=\"n\">cost</span> <span class=\"o\">=</span>  <span class=\"o\">-</span><span class=\"mi\">1</span> <span class=\"o\">/</span> <span class=\"n\">m</span> <span class=\"o\">*</span>  <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">logprobs</span><span class=\"p\">)</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    <span class=\"n\">cost</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">squeeze</span><span class=\"p\">(</span><span class=\"n\">cost</span><span class=\"p\">)</span>     <span class=\"c1\"># makes sure cost is the dimension we expect. </span>\n                                <span class=\"c1\"># E.g., turns [[17]] into 17 </span>\n    <span class=\"k\">assert</span><span class=\"p\">(</span><span class=\"nb\">isinstance</span><span class=\"p\">(</span><span class=\"n\">cost</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">))</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">cost</span></code></pre></div><h2>3.3 backworad propagation</h2><p>NGè¯´ç¥ç»ç½‘ç»œä¸­æœ€éš¾ç†è§£çš„æ˜¯è¿™ä¸ªï¼Œä½†æ˜¯ç°åœ¨å…¬å¼å·²ç»å¸®æˆ‘ä»¬æ¨å€’å¥½äº†ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-76fd95df5ad1e79b96b150233d02c4a3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2364\" data-rawheight=\"1264\" class=\"origin_image zh-lightbox-thumb\" width=\"2364\" data-original=\"https://pic4.zhimg.com/v2-76fd95df5ad1e79b96b150233d02c4a3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2364&#39; height=&#39;1264&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2364\" data-rawheight=\"1264\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2364\" data-original=\"https://pic4.zhimg.com/v2-76fd95df5ad1e79b96b150233d02c4a3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-76fd95df5ad1e79b96b150233d02c4a3_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>å…¶ä¸­ï¼Œ <img src=\"https://www.zhihu.com/equation?tex=+g%5E%7B%5B1%5D%27%7D%28Z%5E%7B%5B1%5D%7D%29\" alt=\" g^{[1]&#39;}(Z^{[1]})\" eeimg=\"1\"/> using<code>(1 - np.power(A1, 2))</code></p><p>å¯ä»¥çœ‹åˆ°ï¼Œå…¬å¼ä¸­éœ€è¦çš„å˜é‡æœ‰X,Y,A,W,ç„¶åè¾“å‡ºä¸€ä¸ªå­—å…¸ç»“æ„çš„grads</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">backward_propagation</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">cache</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Implement the backward propagation using the instructions above.\n</span><span class=\"s2\">\n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    parameters -- python dictionary containing our parameters \n</span><span class=\"s2\">    cache -- a dictionary containing &#34;Z1&#34;, &#34;A1&#34;, &#34;Z2&#34; and &#34;A2&#34;.\n</span><span class=\"s2\">    X -- input data of shape (2, number of examples)\n</span><span class=\"s2\">    Y -- &#34;true&#34; labels vector of shape (1, number of examples)\n</span><span class=\"s2\">\n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    grads -- python dictionary containing your gradients with respect to different parameters\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    <span class=\"n\">m</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n\n    <span class=\"c1\"># First, retrieve W1 and W2 from the dictionary &#34;parameters&#34;.</span>\n    <span class=\"c1\">### START CODE HERE ### (â‰ˆ 2 lines of code)</span>\n    <span class=\"n\">W1</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;W1&#39;</span><span class=\"p\">]</span>\n    <span class=\"n\">W2</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;W2&#39;</span><span class=\"p\">]</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n\n    <span class=\"c1\"># Retrieve also A1 and A2 from dictionary &#34;cache&#34;.</span>\n    <span class=\"c1\">### START CODE HERE ### (â‰ˆ 2 lines of code)</span>\n    <span class=\"n\">A1</span> <span class=\"o\">=</span> <span class=\"n\">cache</span><span class=\"p\">[</span><span class=\"s1\">&#39;A1&#39;</span><span class=\"p\">]</span>\n    <span class=\"n\">A2</span> <span class=\"o\">=</span> <span class=\"n\">cache</span><span class=\"p\">[</span><span class=\"s1\">&#39;A2&#39;</span><span class=\"p\">]</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n\n    <span class=\"c1\"># Backward propagation: calculate dW1, db1, dW2, db2. </span>\n    <span class=\"c1\">### START CODE HERE ### (â‰ˆ 6 lines of code, corresponding to 6 equations on slide above)</span>\n    <span class=\"n\">dZ2</span> <span class=\"o\">=</span> <span class=\"n\">A2</span> <span class=\"o\">-</span> <span class=\"n\">Y</span>\n    <span class=\"n\">dW2</span> <span class=\"o\">=</span> <span class=\"mi\">1</span> <span class=\"o\">/</span> <span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">dZ2</span><span class=\"p\">,</span> <span class=\"n\">A1</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">)</span>\n    <span class=\"n\">db2</span> <span class=\"o\">=</span> <span class=\"mi\">1</span> <span class=\"o\">/</span> <span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">dZ2</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">keepdims</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n    <span class=\"n\">dZ1</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">W2</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">,</span> <span class=\"n\">dZ2</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"mi\">1</span> <span class=\"o\">-</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">power</span><span class=\"p\">(</span><span class=\"n\">A1</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">))</span>\n    <span class=\"n\">dW1</span> <span class=\"o\">=</span> <span class=\"mi\">1</span> <span class=\"o\">/</span> <span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">dZ1</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">)</span>\n    <span class=\"n\">db1</span> <span class=\"o\">=</span> <span class=\"mi\">1</span> <span class=\"o\">/</span> <span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">dZ1</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">keepdims</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n\n    <span class=\"n\">grads</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">&#34;dW1&#34;</span><span class=\"p\">:</span> <span class=\"n\">dW1</span><span class=\"p\">,</span>\n             <span class=\"s2\">&#34;db1&#34;</span><span class=\"p\">:</span> <span class=\"n\">db1</span><span class=\"p\">,</span>\n             <span class=\"s2\">&#34;dW2&#34;</span><span class=\"p\">:</span> <span class=\"n\">dW2</span><span class=\"p\">,</span>\n             <span class=\"s2\">&#34;db2&#34;</span><span class=\"p\">:</span> <span class=\"n\">db2</span><span class=\"p\">}</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">grads</span></code></pre></div><h2>3.4 update parameters</h2><p>æœ€åæ ¹æ®å¾—åˆ°çš„gradsï¼Œä¹˜ä¸Šå­¦ä¹ é€Ÿç‡ï¼Œå°±å¯ä»¥æ›´æ–°å‚æ•°äº†ã€‚</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: update_parameters</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">update_parameters</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">grads</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span> <span class=\"o\">=</span> <span class=\"mf\">1.2</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Updates parameters using the gradient descent update rule given above\n</span><span class=\"s2\">\n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    parameters -- python dictionary containing your parameters \n</span><span class=\"s2\">    grads -- python dictionary containing your gradients \n</span><span class=\"s2\">\n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    parameters -- python dictionary containing your updated parameters \n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    <span class=\"c1\"># Retrieve each parameter from the dictionary &#34;parameters&#34;</span>\n    <span class=\"c1\">### START CODE HERE ### (â‰ˆ 4 lines of code)</span>\n    <span class=\"n\">W1</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;W1&#39;</span><span class=\"p\">]</span>\n    <span class=\"n\">b1</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;b1&#39;</span><span class=\"p\">]</span>\n    <span class=\"n\">W2</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;W2&#39;</span><span class=\"p\">]</span>\n    <span class=\"n\">b2</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;b2&#39;</span><span class=\"p\">]</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n\n    <span class=\"c1\"># Retrieve each gradient from the dictionary &#34;grads&#34;</span>\n    <span class=\"c1\">### START CODE HERE ### (â‰ˆ 4 lines of code)</span>\n    <span class=\"n\">dW1</span> <span class=\"o\">=</span> <span class=\"n\">grads</span><span class=\"p\">[</span><span class=\"s1\">&#39;dW1&#39;</span><span class=\"p\">]</span>\n    <span class=\"n\">db1</span> <span class=\"o\">=</span> <span class=\"n\">grads</span><span class=\"p\">[</span><span class=\"s1\">&#39;db1&#39;</span><span class=\"p\">]</span>\n    <span class=\"n\">dW2</span> <span class=\"o\">=</span> <span class=\"n\">grads</span><span class=\"p\">[</span><span class=\"s1\">&#39;dW2&#39;</span><span class=\"p\">]</span>\n    <span class=\"n\">db2</span> <span class=\"o\">=</span> <span class=\"n\">grads</span><span class=\"p\">[</span><span class=\"s1\">&#39;db2&#39;</span><span class=\"p\">]</span>\n    <span class=\"c1\">## END CODE HERE ###</span>\n\n    <span class=\"c1\"># Update rule for each parameter</span>\n    <span class=\"c1\">### START CODE HERE ### (â‰ˆ 4 lines of code)</span>\n    <span class=\"n\">W1</span> <span class=\"o\">=</span> <span class=\"n\">W1</span> <span class=\"o\">-</span> <span class=\"n\">learning_rate</span> <span class=\"o\">*</span> <span class=\"n\">dW1</span>\n    <span class=\"n\">b1</span> <span class=\"o\">=</span> <span class=\"n\">b1</span> <span class=\"o\">-</span> <span class=\"n\">learning_rate</span> <span class=\"o\">*</span> <span class=\"n\">db1</span>\n    <span class=\"n\">W2</span> <span class=\"o\">=</span> <span class=\"n\">W2</span> <span class=\"o\">-</span> <span class=\"n\">learning_rate</span> <span class=\"o\">*</span> <span class=\"n\">dW2</span>\n    <span class=\"n\">b2</span> <span class=\"o\">=</span> <span class=\"n\">b2</span> <span class=\"o\">-</span> <span class=\"n\">learning_rate</span> <span class=\"o\">*</span> <span class=\"n\">db2</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n\n    <span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">&#34;W1&#34;</span><span class=\"p\">:</span> <span class=\"n\">W1</span><span class=\"p\">,</span>\n                  <span class=\"s2\">&#34;b1&#34;</span><span class=\"p\">:</span> <span class=\"n\">b1</span><span class=\"p\">,</span>\n                  <span class=\"s2\">&#34;W2&#34;</span><span class=\"p\">:</span> <span class=\"n\">W2</span><span class=\"p\">,</span>\n                  <span class=\"s2\">&#34;b2&#34;</span><span class=\"p\">:</span> <span class=\"n\">b2</span><span class=\"p\">}</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">parameters</span></code></pre></div><p>ç„¶åæŠŠæ›´æ–°å®Œçš„å‚æ•°å†ä¼ å…¥å‰é¢çš„å¾ªç¯ä¸­ï¼Œä¸æ–­å¾ªç¯ï¼Œç›´åˆ°è¾¾åˆ°å¾ªç¯çš„æ¬¡æ•°ã€‚</p><h2>nn_model</h2><p>æŠŠå‰é¢çš„å‡½æ•°éƒ½è°ƒç”¨è¿‡æ¥ã€‚</p><p>æ¨¡å‹ä¸­ä¼ å…¥çš„å‚æ•°æ˜¯ï¼ŒX,Yï¼Œå’Œè¿­ä»£æ¬¡æ•°</p><ol><li>é¦–å…ˆéœ€è¦å¾—åˆ°ä½ è¦è®¾è®¡çš„ç¥ç»ç½‘ç»œç»“æ„ï¼Œè°ƒç”¨<code>layer_sizes()</code>å¾—åˆ°äº†n_x,n_yï¼Œä¹Ÿå°±æ˜¯è¾“å…¥å±‚å’Œè¾“å‡ºå±‚ã€‚</li><li>åˆå§‹åŒ–å‚æ•°<code>initialize_parameters(n_x, n_h, n_y)</code>,å¾—åˆ°åˆå§‹åŒ–çš„ W1, b1, W2, b2</li><li>ç„¶åå¼€å§‹å¾ªç¯</li><ol><li>ä½¿ç”¨<code>forward_propagation(X, parameters)</code>,å…ˆå¾—åˆ°å„ä¸ªç¥ç»å…ƒçš„è®¡ç®—å€¼ã€‚</li><li>ç„¶å<code>compute_cost(A2, Y, parameters)</code>,å¾—åˆ°cost</li><li><code>backward_propagation(parameters, cache, X, Y)</code>è®¡ç®—å‡ºæ¯ä¸€æ­¥çš„æ¢¯åº¦</li><li><code>update_parameters(parameters, grads)</code>æ›´æ–°ä¸€ä¸‹å‚æ•°</li><li>è¿”å›è®­ç»ƒå®Œçš„parameters</li></ol></ol><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: nn_model</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">nn_model</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">n_h</span><span class=\"p\">,</span> <span class=\"n\">num_iterations</span> <span class=\"o\">=</span> <span class=\"mi\">10000</span><span class=\"p\">,</span> <span class=\"n\">print_cost</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    X -- dataset of shape (2, number of examples)\n</span><span class=\"s2\">    Y -- labels of shape (1, number of examples)\n</span><span class=\"s2\">    n_h -- size of the hidden layer\n</span><span class=\"s2\">    num_iterations -- Number of iterations in gradient descent loop\n</span><span class=\"s2\">    print_cost -- if True, print the cost every 1000 iterations\n</span><span class=\"s2\">\n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    parameters -- parameters learnt by the model. They can then be used to predict.\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n\n    <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">seed</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">)</span>\n    <span class=\"n\">n_x</span> <span class=\"o\">=</span> <span class=\"n\">layer_sizes</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">)[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n    <span class=\"n\">n_y</span> <span class=\"o\">=</span> <span class=\"n\">layer_sizes</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">)[</span><span class=\"mi\">2</span><span class=\"p\">]</span>\n\n    <span class=\"c1\"># Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: &#34;n_x, n_h, n_y&#34;. Outputs = &#34;W1, b1, W2, b2, parameters&#34;.</span>\n    <span class=\"c1\">### START CODE HERE ### (â‰ˆ 5 lines of code)</span>\n    <span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"n\">initialize_parameters</span><span class=\"p\">(</span><span class=\"n\">n_x</span><span class=\"p\">,</span> <span class=\"n\">n_h</span><span class=\"p\">,</span> <span class=\"n\">n_y</span><span class=\"p\">)</span>\n    <span class=\"n\">W1</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;W1&#39;</span><span class=\"p\">]</span>\n    <span class=\"n\">b1</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;b1&#39;</span><span class=\"p\">]</span>\n    <span class=\"n\">W2</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;W2&#39;</span><span class=\"p\">]</span>\n    <span class=\"n\">b2</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;b2&#39;</span><span class=\"p\">]</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n\n    <span class=\"c1\"># Loop (gradient descent)</span>\n\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">num_iterations</span><span class=\"p\">):</span>\n\n        <span class=\"c1\">### START CODE HERE ### (â‰ˆ 4 lines of code)</span>\n        <span class=\"c1\"># Forward propagation. Inputs: &#34;X, parameters&#34;. Outputs: &#34;A2, cache&#34;.</span>\n        <span class=\"n\">A2</span><span class=\"p\">,</span> <span class=\"n\">cache</span> <span class=\"o\">=</span> <span class=\"n\">forward_propagation</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">parameters</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># Cost function. Inputs: &#34;A2, Y, parameters&#34;. Outputs: &#34;cost&#34;.</span>\n        <span class=\"n\">cost</span> <span class=\"o\">=</span> <span class=\"n\">compute_cost</span><span class=\"p\">(</span><span class=\"n\">A2</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">parameters</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># Backpropagation. Inputs: &#34;parameters, cache, X, Y&#34;. Outputs: &#34;grads&#34;.</span>\n        <span class=\"n\">grads</span> <span class=\"o\">=</span> <span class=\"n\">backward_propagation</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">cache</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># Gradient descent parameter update. Inputs: &#34;parameters, grads&#34;. Outputs: &#34;parameters&#34;.</span>\n        <span class=\"n\">parameters</span> <span class=\"o\">=</span>  <span class=\"n\">update_parameters</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">grads</span><span class=\"p\">)</span>\n\n        <span class=\"c1\">### END CODE HERE ###</span>\n\n        <span class=\"c1\"># Print the cost every 1000 iterations</span>\n        <span class=\"k\">if</span> <span class=\"n\">print_cost</span> <span class=\"ow\">and</span> <span class=\"n\">i</span> <span class=\"o\">%</span> <span class=\"mi\">1000</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n            <span class=\"k\">print</span> <span class=\"p\">(</span><span class=\"s2\">&#34;Cost after iteration </span><span class=\"si\">%i</span><span class=\"s2\">: </span><span class=\"si\">%f</span><span class=\"s2\">&#34;</span> <span class=\"o\">%</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">cost</span><span class=\"p\">))</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">parameters</span></code></pre></div><h2>é¢„æµ‹</h2><p>å¾—åˆ°è®­ç»ƒåçš„parametersï¼Œå†ç”¨<code>forward_propagation(X, parameters)</code>è®¡ç®—å‡ºè¾“å‡ºå±‚æœ€ç»ˆçš„å€¼A2ï¼Œä»¥0.5ä¸ºåˆ†ç•Œï¼Œåˆ†ä¸º0å’Œ1ã€‚</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: predict</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">predict</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Using the learned parameters, predicts a class for each example in X\n</span><span class=\"s2\">\n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    parameters -- python dictionary containing your parameters \n</span><span class=\"s2\">    X -- input data of size (n_x, m)\n</span><span class=\"s2\">\n</span><span class=\"s2\">    Returns\n</span><span class=\"s2\">    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n\n    <span class=\"c1\"># Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.</span>\n    <span class=\"c1\">### START CODE HERE ### (â‰ˆ 2 lines of code)</span>\n    <span class=\"n\">A2</span><span class=\"p\">,</span> <span class=\"n\">cache</span> <span class=\"o\">=</span> <span class=\"n\">forward_propagation</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">parameters</span><span class=\"p\">)</span>\n    <span class=\"n\">predictions</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">A2</span> <span class=\"o\">&gt;</span> <span class=\"mf\">0.5</span><span class=\"p\">)</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">predictions</span>\n<span class=\"c1\"># Build a model with a n_h-dimensional hidden layer</span>\n<span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"n\">nn_model</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">n_h</span> <span class=\"o\">=</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">num_iterations</span> <span class=\"o\">=</span> <span class=\"mi\">10000</span><span class=\"p\">,</span> <span class=\"n\">print_cost</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Plot the decision boundary</span>\n<span class=\"n\">plot_decision_boundary</span><span class=\"p\">(</span><span class=\"k\">lambda</span> <span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">),</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">title</span><span class=\"p\">(</span><span class=\"s2\">&#34;Decision Boundary for hidden layer size &#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"mi\">4</span><span class=\"p\">))</span></code></pre></div><p>å¯ä»¥çœ‹åˆ°ï¼Œè®­ç»ƒåç¥ç»ç½‘ç»œå¾—åˆ°çš„åˆ†ç•Œçº¿æ›´ä¸ºåˆç†ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-63742111bee73fdc6efd95bc17ceb95c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"532\" data-rawheight=\"391\" class=\"origin_image zh-lightbox-thumb\" width=\"532\" data-original=\"https://pic1.zhimg.com/v2-63742111bee73fdc6efd95bc17ceb95c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;532&#39; height=&#39;391&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"532\" data-rawheight=\"391\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"532\" data-original=\"https://pic1.zhimg.com/v2-63742111bee73fdc6efd95bc17ceb95c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-63742111bee73fdc6efd95bc17ceb95c_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># Print accuracy</span>\n<span class=\"n\">predictions</span> <span class=\"o\">=</span> <span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">)</span>\n<span class=\"k\">print</span> <span class=\"p\">(</span><span class=\"s1\">&#39;Accuracy: </span><span class=\"si\">%d</span><span class=\"s1\">&#39;</span> <span class=\"o\">%</span> <span class=\"nb\">float</span><span class=\"p\">((</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">Y</span><span class=\"p\">,</span><span class=\"n\">predictions</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"o\">-</span><span class=\"n\">Y</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"o\">-</span><span class=\"n\">predictions</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">))</span><span class=\"o\">/</span><span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"n\">Y</span><span class=\"o\">.</span><span class=\"n\">size</span><span class=\"p\">)</span><span class=\"o\">*</span><span class=\"mi\">100</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"s1\">&#39;%&#39;</span><span class=\"p\">)</span></code></pre></div><p>å‡†ç¡®ç‡é«˜è¾¾90%</p><h2>ä¼˜åŒ–å‚æ•°</h2><p>è¿™ä¸ªæ—¶å€™å°±å¯ä»¥è®¾ç½®ä¸åŒçš„hidden_layerçš„ç»´åº¦å¤§å°[1, 2, 3, 4, 5, 20, 50]</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># This may take about 2 minutes to run</span>\n\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">figure</span><span class=\"p\">(</span><span class=\"n\">figsize</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">16</span><span class=\"p\">,</span> <span class=\"mi\">32</span><span class=\"p\">))</span>\n<span class=\"n\">hidden_layer_sizes</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"mi\">50</span><span class=\"p\">]</span>\n<span class=\"k\">for</span> <span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">n_h</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">hidden_layer_sizes</span><span class=\"p\">):</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplot</span><span class=\"p\">(</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">i</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">title</span><span class=\"p\">(</span><span class=\"s1\">&#39;Hidden Layer of size </span><span class=\"si\">%d</span><span class=\"s1\">&#39;</span> <span class=\"o\">%</span> <span class=\"n\">n_h</span><span class=\"p\">)</span>\n    <span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"n\">nn_model</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">n_h</span><span class=\"p\">,</span> <span class=\"n\">num_iterations</span> <span class=\"o\">=</span> <span class=\"mi\">5000</span><span class=\"p\">)</span>\n    <span class=\"n\">plot_decision_boundary</span><span class=\"p\">(</span><span class=\"k\">lambda</span> <span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">),</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">)</span>\n    <span class=\"n\">predictions</span> <span class=\"o\">=</span> <span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">)</span>\n    <span class=\"n\">accuracy</span> <span class=\"o\">=</span> <span class=\"nb\">float</span><span class=\"p\">((</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">Y</span><span class=\"p\">,</span><span class=\"n\">predictions</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"o\">-</span><span class=\"n\">Y</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"o\">-</span><span class=\"n\">predictions</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">))</span><span class=\"o\">/</span><span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"n\">Y</span><span class=\"o\">.</span><span class=\"n\">size</span><span class=\"p\">)</span><span class=\"o\">*</span><span class=\"mi\">100</span><span class=\"p\">)</span>\n    <span class=\"k\">print</span> <span class=\"p\">(</span><span class=\"s2\">&#34;Accuracy for {} hidden units: {} %&#34;</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span><span class=\"n\">n_h</span><span class=\"p\">,</span> <span class=\"n\">accuracy</span><span class=\"p\">))</span>\n<span class=\"n\">Accuracy</span> <span class=\"k\">for</span> <span class=\"mi\">1</span> <span class=\"n\">hidden</span> <span class=\"n\">units</span><span class=\"p\">:</span> <span class=\"mf\">67.5</span> <span class=\"o\">%</span>\n<span class=\"n\">Accuracy</span> <span class=\"k\">for</span> <span class=\"mi\">2</span> <span class=\"n\">hidden</span> <span class=\"n\">units</span><span class=\"p\">:</span> <span class=\"mf\">67.25</span> <span class=\"o\">%</span>\n<span class=\"n\">Accuracy</span> <span class=\"k\">for</span> <span class=\"mi\">3</span> <span class=\"n\">hidden</span> <span class=\"n\">units</span><span class=\"p\">:</span> <span class=\"mf\">90.75</span> <span class=\"o\">%</span>\n<span class=\"n\">Accuracy</span> <span class=\"k\">for</span> <span class=\"mi\">4</span> <span class=\"n\">hidden</span> <span class=\"n\">units</span><span class=\"p\">:</span> <span class=\"mf\">90.5</span> <span class=\"o\">%</span>\n<span class=\"n\">Accuracy</span> <span class=\"k\">for</span> <span class=\"mi\">5</span> <span class=\"n\">hidden</span> <span class=\"n\">units</span><span class=\"p\">:</span> <span class=\"mf\">91.25</span> <span class=\"o\">%</span>\n<span class=\"n\">Accuracy</span> <span class=\"k\">for</span> <span class=\"mi\">20</span> <span class=\"n\">hidden</span> <span class=\"n\">units</span><span class=\"p\">:</span> <span class=\"mf\">90.0</span> <span class=\"o\">%</span>\n<span class=\"n\">Accuracy</span> <span class=\"k\">for</span> <span class=\"mi\">50</span> <span class=\"n\">hidden</span> <span class=\"n\">units</span><span class=\"p\">:</span> <span class=\"mf\">90.25</span> <span class=\"o\">%</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-22444884cd6036631c4e2cd1ae268772_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1306\" data-rawheight=\"2048\" class=\"origin_image zh-lightbox-thumb\" width=\"1306\" data-original=\"https://pic3.zhimg.com/v2-22444884cd6036631c4e2cd1ae268772_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1306&#39; height=&#39;2048&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1306\" data-rawheight=\"2048\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1306\" data-original=\"https://pic3.zhimg.com/v2-22444884cd6036631c4e2cd1ae268772_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-22444884cd6036631c4e2cd1ae268772_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>å¾—åˆ°çš„ç»“æœåœ¨n_h = 5æ—¶æœ‰æœ€å¤§å€¼ã€‚</p>", 
            "topic": [
                {
                    "tag": "æœºå™¨å­¦ä¹ ", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "æ·±åº¦å­¦ä¹ ï¼ˆDeep Learningï¼‰", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "ç¥ç»ç½‘ç»œ", 
                    "tagLink": "https://api.zhihu.com/topics/19607065"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/44396045", 
            "userName": "ç›´ä¸Šäº‘éœ„", 
            "userLink": "https://www.zhihu.com/people/1033165ce4ad9c3fce69a0793dfab8ad", 
            "upvote": 1, 
            "title": "DeepLearning.aiç¬”è®°:(1-3)-æµ…å±‚ç¥ç»ç½‘ç»œ", 
            "content": "<h2>æœ¬æ–‡é¦–å‘äºä¸ªäººåšå®¢ï¼š<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">fangzh.top</span><span class=\"invisible\"></span></a></h2><p>å‰é¢ä¸¤å‘¨è®²çš„æ˜¯ä¸€äº›logisticå’Œå‘é‡åŒ–çš„å†…å®¹ï¼Œä»¥åŠnumpyçš„åŸºæœ¬ä½¿ç”¨ï¼Œåœ¨ä»–ä¹‹å‰çš„æœºå™¨å­¦ä¹ è¯¾ç¨‹ä¸­å·²ç»è®²è¿‡äº†ï¼Œè¿™é‡Œå°±ä¸å†èµ˜è¿°ã€‚Week3ä¸»è¦è®²äº†å¦‚ä½•æ­å»ºä¸¤å±‚çš„ç¥ç»ç½‘ç»œã€‚</p><h2>ç¥ç»ç½‘ç»œçš„è¡¨ç¤º</h2><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-8757b594ed20d404f388130c93f16738_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1290\" data-rawheight=\"782\" class=\"origin_image zh-lightbox-thumb\" width=\"1290\" data-original=\"https://pic1.zhimg.com/v2-8757b594ed20d404f388130c93f16738_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1290&#39; height=&#39;782&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1290\" data-rawheight=\"782\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1290\" data-original=\"https://pic1.zhimg.com/v2-8757b594ed20d404f388130c93f16738_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-8757b594ed20d404f388130c93f16738_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>è¿™å‘¨çš„å†…å®¹å°±å›´ç»•ç€è¿™ä¸€å¼ å›¾æ¥è®²ã€‚</p><p><img src=\"https://www.zhihu.com/equation?tex=a_%7Bj%7D%5E%7B%5Bi%5D%7D\" alt=\"a_{j}^{[i]}\" eeimg=\"1\"/> </p><p>è¿™å°±æ˜¯æ¯ä¸€å±‚ç¥ç»å…ƒçš„è¡¨è¾¾æ–¹å¼ï¼Œä¸Šæ ‡ä¸­æ‹¬å·[]ï¼Œè¡¨ç¤ºæ˜¯ç¬¬å‡ å±‚çš„ç¥ç»å…ƒï¼›ä¸‹æ ‡è¡¨ç¤ºè¿™ä¸ªæ˜¯æŸä¸€å±‚çš„ç¬¬å‡ ä¸ªç¥ç»å…ƒã€‚</p><p>Input Layerï¼šè¾“å…¥å±‚ï¼Œä¹Ÿç”¨ <img src=\"https://www.zhihu.com/equation?tex=a_%7Bj%7D%5E%7B%5B0%5D%7D\" alt=\"a_{j}^{[0]}\" eeimg=\"1\"/> ï¼Œè¡¨ç¤ºç¬¬0å±‚</p><p>Hidden Layerï¼šè¡¨ç¤ºé™¤äº†æœ€åä¸€å±‚è¾“å‡ºå±‚ä»¥å¤–çš„å†…éƒ¨éšè—å±‚</p><p>Output Layerï¼šè¾“å‡ºå±‚ï¼Œè¡¨ç¤ºæœ€åä¸€å±‚</p><p>è€Œé€šå¸¸ç¥ç»ç½‘ç»œçš„å±‚æ•°ä¸€èˆ¬ä¸åŒ…æ‹¬è¾“å…¥å±‚ã€‚</p><p><img src=\"https://www.zhihu.com/equation?tex=w%5E%7B%5Bi%5D%7D\" alt=\"w^{[i]}\" eeimg=\"1\"/> ï¼šæ¯ä¸€å±‚çš„å‚æ•°wçš„ç»´åº¦æ˜¯ï¼ˆè¯¥å±‚ç¥ç»å…ƒä¸ªæ•°ï¼Œå‰é¢ä¸€å±‚ç¥ç»å…ƒä¸ªæ•°ï¼‰</p><p><img src=\"https://www.zhihu.com/equation?tex=b%5E%7B%5Bi%5D%7D\" alt=\"b^{[i]}\" eeimg=\"1\"/> ï¼šä¸ºï¼ˆæ¯ä¸€å±‚çš„ç¥ç»å…ƒä¸ªæ•°ï¼Œ1ï¼‰</p><h2>è®¡ç®—å•ä¸ªæ•°æ®çš„ç¥ç»ç½‘ç»œ</h2><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-7d46c24b97cc923d84e1889de5f47fce_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"546\" data-rawheight=\"251\" class=\"origin_image zh-lightbox-thumb\" width=\"546\" data-original=\"https://pic3.zhimg.com/v2-7d46c24b97cc923d84e1889de5f47fce_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;546&#39; height=&#39;251&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"546\" data-rawheight=\"251\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"546\" data-original=\"https://pic3.zhimg.com/v2-7d46c24b97cc923d84e1889de5f47fce_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-7d46c24b97cc923d84e1889de5f47fce_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>ç”±æ­¤å¾—åˆ°ï¼Œè®¡ç®—å•ä¸ªæ•°æ®çš„ç¥ç»ç½‘ç»œåªéœ€è¦4æ­¥ï¼š</p><p><img src=\"https://www.zhihu.com/equation?tex=z%5E%7B%5B1%5D%7D+%3D+W%5E%7B%5B1%5D%7Da%5E%7B%5B0%5D%7D+%2B+b%5E%7B%5B1%5D%7D+\" alt=\"z^{[1]} = W^{[1]}a^{[0]} + b^{[1]} \" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=a%5E%7B%5B1%5D%7D+%3D+%5Csigma%28z%5E%7B%5B1%5D%7D%29+\" alt=\"a^{[1]} = \\sigma(z^{[1]}) \" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=z%5E%7B%5B2%5D%7D+%3D+W%5E%7B%5B2%5D%7Da%5E%7B%5B1%5D%7D+%2B+b%5E%7B%5B2%5D%7D+\" alt=\"z^{[2]} = W^{[2]}a^{[1]} + b^{[2]} \" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=a%5E%7B%5B2%5D%7D+%3D+%5Csigma%28z%5E%7B%5B2%5D%7D%29\" alt=\"a^{[2]} = \\sigma(z^{[2]})\" eeimg=\"1\"/> </p><h2>å¤šæ•°æ®çš„å‘é‡åŒ–è¡¨ç¤º</h2><p>æˆ‘ä»¬çŸ¥é“ï¼Œå¤šä¸ªæ•°æ®çš„è¡¨ç¤ºå°±æ˜¯ <img src=\"https://www.zhihu.com/equation?tex=x%5E%7B%28i%29%7D\" alt=\"x^{(i)}\" eeimg=\"1\"/> ï¼Œä½¿ç”¨å°æ‹¬å·çš„ä¸Šæ ‡ã€‚ç¥ç»å…ƒä¹Ÿæ˜¯ä¸€æ ·ã€‚</p><p>å¦‚ <img src=\"https://www.zhihu.com/equation?tex=a%5E%7B%5B1%5D+%28i%29%7D\" alt=\"a^{[1] (i)}\" eeimg=\"1\"/> è¡¨ç¤ºç¬¬1å±‚ç¥ç»å…ƒçš„ç¬¬iä¸ªæ ·æœ¬ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-870a03a5327f87846ec507d34e28d10b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1450\" data-rawheight=\"814\" class=\"origin_image zh-lightbox-thumb\" width=\"1450\" data-original=\"https://pic4.zhimg.com/v2-870a03a5327f87846ec507d34e28d10b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1450&#39; height=&#39;814&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1450\" data-rawheight=\"814\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1450\" data-original=\"https://pic4.zhimg.com/v2-870a03a5327f87846ec507d34e28d10b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-870a03a5327f87846ec507d34e28d10b_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>é‚£ä¹ˆå¦‚æœæœ‰mä¸ªæ ·æœ¬ï¼Œä¸€ç›´åšforå¾ªç¯æ¥è®¡ç®—å‡ºè¿™äº›ç¥ç»å…ƒçš„å€¼ï¼Œå®åœ¨æ˜¯å¤ªæ…¢äº†ï¼Œæ‰€ä»¥è·Ÿlogisticä¸€æ ·ï¼Œå¯ä»¥ç›´æ¥ç”¨å‘é‡åŒ–æ¥è¡¨ç¤ºï¼Œè¿™ä¸ªæ—¶å€™ç”¨å¤§å†™å­—æ¯æ¥è¡¨ç¤ºã€‚</p><p><img src=\"https://www.zhihu.com/equation?tex=Z%5E%7B%5B1%5D%7D+%3D+W%5E%7B%5B1%5D%7DA%5E%7B%5B0%5D%7D+%2B+b%5E%7B%5B1%5D%7D+\" alt=\"Z^{[1]} = W^{[1]}A^{[0]} + b^{[1]} \" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=A%5E%7B%5B1%5D%7D+%3D+%5Csigma%28Z%5E%7B%5B1%5D%7D%29+\" alt=\"A^{[1]} = \\sigma(Z^{[1]}) \" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=Z%5E%7B%5B2%5D%7D+%3D+W%5E%7B%5B2%5D%7DA%5E%7B%5B1%5D%7D+%2B+b%5E%7B%5B2%5D%7D+\" alt=\"Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]} \" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=A%5E%7B%5B2%5D%7D+%3D+%5Csigma%28Z%5E%7B%5B2%5D%7D%29\" alt=\"A^{[2]} = \\sigma(Z^{[2]})\" eeimg=\"1\"/> </p><p>è¿™ä¸ªæ—¶å€™ï¼Œä¾‹å¦‚ <img src=\"https://www.zhihu.com/equation?tex=A%5E%7B%5B1%5D%7D\" alt=\"A^{[1]}\" eeimg=\"1\"/> æ˜¯ä¸€ä¸ª <img src=\"https://www.zhihu.com/equation?tex=%28n%2Cm%29\" alt=\"(n,m)\" eeimg=\"1\"/> çš„çŸ©é˜µï¼Œmæ˜¯æ ·æœ¬æ•°ï¼Œæ¯ä¸€åˆ—è¡¨ç¤ºä¸€ä¸ªæ ·æœ¬ï¼Œnæ˜¯è¯¥å±‚çš„ç¥ç»å…ƒä¸ªæ•°ã€‚</p><p>ä»æ°´å¹³ä¸Šçœ‹ï¼ŒçŸ©é˜µ Aä»£è¡¨äº†å„ä¸ªè®­ç»ƒæ ·æœ¬ã€‚ç«–ç›´ä¸Šçœ‹ï¼ŒAçš„ä¸åŒç´¢å¼•å¯¹åº”ä¸ç”¨çš„éšè—å•å…ƒã€‚</p><p>å¯¹çŸ©é˜µZå’ŒXä¹Ÿæ˜¯ç±»ä¼¼ï¼Œæ°´å¹³æ–¹å‘å¯¹åº”ä¸åŒçš„æ ·æœ¬ï¼Œç«–ç›´æ–¹å‘ä¸Šå¯¹åº”ä¸åŒçš„è¾“å…¥ç‰¹å¾ï¼Œä¹Ÿå°±æ˜¯ç¥ç»ç½‘ç»œè¾“å…¥å±‚çš„å„ä¸ªèŠ‚ç‚¹ã€‚</p><h2>æ¿€æ´»å‡½æ•°</h2><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-aa93949c44778647901831add92f2160_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1388\" data-rawheight=\"766\" class=\"origin_image zh-lightbox-thumb\" width=\"1388\" data-original=\"https://pic1.zhimg.com/v2-aa93949c44778647901831add92f2160_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1388&#39; height=&#39;766&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1388\" data-rawheight=\"766\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1388\" data-original=\"https://pic1.zhimg.com/v2-aa93949c44778647901831add92f2160_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-aa93949c44778647901831add92f2160_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>åœ¨æ­¤å‰éƒ½æ˜¯ç”¨sigmoidä½œä¸ºæ¿€æ´»å‡½æ•°çš„ã€‚ä½†æ˜¯æ¿€æ´»å‡½æ•°ä¸åªæœ‰è¿™ä¸€ç§ï¼Œå¸¸ç”¨çš„æœ‰4ç§ï¼Œåˆ†åˆ«æ˜¯ï¼šsigmoid, tanh, ReLu, Leaky ReLuã€‚</p><ul><li>sigmoid: <img src=\"https://www.zhihu.com/equation?tex=a+%3D++%5Cfrac%7B1%7D%7B1+%2B+e%5E%7B-z%7D%7D\" alt=\"a =  \\frac{1}{1 + e^{-z}}\" eeimg=\"1\"/> </li><ul><li>å¯¼æ•°ï¼š <img src=\"https://www.zhihu.com/equation?tex=a%5E%7B%5Cprime%7D+%3D+a%281-a%29\" alt=\"a^{\\prime} = a(1-a)\" eeimg=\"1\"/> </li></ul><li>tanh: <img src=\"https://www.zhihu.com/equation?tex=a+%3D+%5Cfrac%7Be%5Ez+-+e%5E%7B-z%7D%7D%7Be%5Ez+%2B+e%5E%7B-z%7D%7D\" alt=\"a = \\frac{e^z - e^{-z}}{e^z + e^{-z}}\" eeimg=\"1\"/> </li><ul><li>å¯¼æ•°ï¼š <img src=\"https://www.zhihu.com/equation?tex=a%5E%7B%5Cprime%7D+%3D+1+-+a%5E2\" alt=\"a^{\\prime} = 1 - a^2\" eeimg=\"1\"/> </li></ul><li>ReLu(ä¿®æ­£çº¿æ€§å•å…ƒ): <img src=\"https://www.zhihu.com/equation?tex=a+%3D+max%280%2C+z%29\" alt=\"a = max(0, z)\" eeimg=\"1\"/> </li><li>Leaky ReLu: <img src=\"https://www.zhihu.com/equation?tex=a+%3D+max%280.01z%2C+z%29\" alt=\"a = max(0.01z, z)\" eeimg=\"1\"/> </li></ul><p>tips:</p><ul><li>tanhå‡½æ•°åœ¨å€¼åŸŸä¸Šå¤„äº-1å’Œ+1ä¹‹é—´ï¼Œæ‰€ä»¥å‡å€¼æ›´æ¥è¿‘0ï¼Œä½¿ç”¨tanhæ¯”sigmoidæ›´èƒ½å¤Ÿä¸­å¿ƒåŒ–æ•°æ®ï¼Œä½¿å¾—å¹³å‡å€¼æ¥è¿‘0ï¼Œè€Œä¸æ˜¯0.5ã€‚</li><li>tanhåœ¨å¤§å¤šæ•°åœºåˆéƒ½æ˜¯ä¼˜äºsigmoidçš„ã€‚</li><li>ä½†æ˜¯sigmoidå’Œtanhæœ‰å…±åŒçš„ç¼ºç‚¹å°±æ˜¯zåœ¨ç‰¹åˆ«å¤§æˆ–è€…ç‰¹åˆ«å°çš„æ—¶å€™ï¼Œæ¢¯åº¦å¾ˆå°ï¼Œæ”¶æ•›é€Ÿåº¦å¾ˆæ…¢ã€‚</li><li>è€ŒReLuå¼¥è¡¥äº†ä¸¤è€…çš„ä¸è¶³ï¼Œåœ¨ <img src=\"https://www.zhihu.com/equation?tex=z+%3E+0\" alt=\"z &gt; 0\" eeimg=\"1\"/> æ—¶ï¼Œæ¢¯åº¦å§‹ç»ˆä¸º1ï¼Œæé«˜äº†é€Ÿåº¦ã€‚</li><li>Leaky ReLuä¿è¯äº† <img src=\"https://www.zhihu.com/equation?tex=z+%3C+0\" alt=\"z &lt; 0\" eeimg=\"1\"/> æ—¶ï¼Œæ¢¯åº¦ä¸ä¸º0ï¼Œä½†æ˜¯å®é™…ä¸Šæ•ˆæœå·®ä¸å¤šã€‚</li></ul><p>ç»“è®ºï¼š</p><ul><li>sigmoidï¼šé™¤äº†è¾“å‡ºå±‚æ˜¯ä¸€ä¸ªäºŒåˆ†ç±»é—®é¢˜çš„æ—¶å€™ä½¿ç”¨ï¼Œä¸ç„¶åŸºæœ¬ä¸ç”¨</li><li>tanhï¼šå‡ ä¹é€‚ç”¨äºä»»ä½•åœºåˆ</li><li>ReLuï¼šé»˜è®¤ä½¿ç”¨è¿™ä¸ªï¼Œå¦‚æœä¸ç¡®å®šä½ è¦ç”¨å“ªä¸ªæ¿€æ´»å‡½æ•°ï¼Œé‚£å°±é€‰ReLuæˆ–è€…Leaky ReLu</li></ul><h2>ä¸ºä»€ä¹ˆè¦ä½¿ç”¨éçº¿æ€§çš„æ¿€æ´»å‡½æ•°</h2><p>å¦‚æœä¸ç”¨æ¿€åŠ±å‡½æ•°ï¼ˆå…¶å®ç›¸å½“äºæ¿€åŠ±å‡½æ•°æ˜¯f(x) = xï¼‰ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ä½ æ¯ä¸€å±‚è¾“å‡ºéƒ½æ˜¯ä¸Šå±‚è¾“å…¥çš„çº¿æ€§å‡½æ•°ï¼Œå¾ˆå®¹æ˜“éªŒè¯ï¼Œæ— è®ºä½ ç¥ç»ç½‘ç»œæœ‰å¤šå°‘å±‚ï¼Œè¾“å‡ºéƒ½æ˜¯è¾“å…¥çš„çº¿æ€§ç»„åˆï¼Œä¸åªæœ‰ä¸€ä¸ªéšè—å±‚æ•ˆæœç›¸å½“ï¼Œè¿™ç§æƒ…å†µå°±æ˜¯å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰äº†ã€‚ æ­£å› ä¸ºä¸Šé¢çš„åŸå› ï¼Œæˆ‘ä»¬å†³å®šå¼•å…¥éçº¿æ€§å‡½æ•°ä½œä¸ºæ¿€åŠ±å‡½æ•°ï¼Œè¿™æ ·æ·±å±‚ç¥ç»ç½‘ç»œå°±æœ‰æ„ä¹‰äº†ï¼ˆä¸å†æ˜¯è¾“å…¥çš„çº¿æ€§ç»„åˆï¼Œå¯ä»¥é€¼è¿‘ä»»æ„å‡½æ•°ï¼‰ã€‚</p><h2>æ¢¯åº¦ä¸‹é™æ³•å…¬å¼</h2><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-8b63fb43ed15c306fc2241bff25403a0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1374\" data-rawheight=\"762\" class=\"origin_image zh-lightbox-thumb\" width=\"1374\" data-original=\"https://pic1.zhimg.com/v2-8b63fb43ed15c306fc2241bff25403a0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1374&#39; height=&#39;762&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1374\" data-rawheight=\"762\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1374\" data-original=\"https://pic1.zhimg.com/v2-8b63fb43ed15c306fc2241bff25403a0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-8b63fb43ed15c306fc2241bff25403a0_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>è¿™é‡Œç»™å‡ºäº†æµ…å±‚ç¥ç»ç½‘ç»œçš„æ¢¯åº¦ä¸‹é™æ³•å…¬å¼ã€‚å…¶ä¸­ <img src=\"https://www.zhihu.com/equation?tex=g%5E%7B%5B1%5D%27%7D%28Z%5E%7B%5B1%5D%7D%29\" alt=\"g^{[1]&#39;}(Z^{[1]})\" eeimg=\"1\"/> è¡¨ç¤ºä½ çš„æ¿€æ´»å‡½æ•°çš„å¯¼æ•°ã€‚</p><h2>å‚æ•°éšæœºåˆå§‹åŒ–</h2><p>åœ¨ç¥ç»ç½‘ç»œä¸­,å¦‚æœå°†å‚æ•°å…¨éƒ¨åˆå§‹åŒ–ä¸º0 ä¼šå¯¼è‡´ä¸€ä¸ªé—®é¢˜ï¼Œä¾‹å¦‚å¯¹äºä¸Šé¢çš„ç¥ç»ç½‘ç»œçš„ä¾‹å­ï¼Œå¦‚æœå°†å‚æ•°å…¨éƒ¨åˆå§‹åŒ–ä¸º0ï¼Œåœ¨æ¯è½®å‚æ•°æ›´æ–°çš„æ—¶å€™ï¼Œä¸è¾“å…¥å•å…ƒç›¸å…³çš„ä¸¤ä¸ªéšè—å•å…ƒçš„ç»“æœå°†æ˜¯ç›¸åŒçš„ã€‚</p><p>æ‰€ä»¥åˆå§‹åŒ–æ—¶ï¼ŒWè¦éšæœºåˆå§‹åŒ–ï¼Œbä¸å­˜åœ¨å¯¹ç§°æ€§é—®é¢˜ï¼Œæ‰€ä»¥å¯ä»¥è®¾ç½®ä¸º0</p><div class=\"highlight\"><pre><code class=\"language-text\">W = np.random.rand((2,2))* 0.01\nb = np.zero((2,1))</code></pre></div><p>å°†Wä¹˜ä»¥0.01æ˜¯ä¸ºäº†è®©Wåˆå§‹åŒ–è¶³å¤Ÿå°ï¼Œå› ä¸ºå¦‚æœå¾ˆå¤§çš„è¯ï¼ŒZå°±å¾ˆå¤§ï¼Œç”¨sigmoidæˆ–è€…tanhæ—¶ï¼Œæ‰€å¾—åˆ°çš„æ¢¯åº¦å°±ä¼šå¾ˆå°ï¼Œè®­ç»ƒè¿‡ç¨‹ä¼šå˜æ…¢ã€‚</p><p>ReLUå’ŒLeaky ReLUä½œä¸ºæ¿€æ´»å‡½æ•°æ—¶ï¼Œä¸å­˜åœ¨è¿™ç§é—®é¢˜ï¼Œå› ä¸ºåœ¨å¤§äº0çš„æ—¶å€™ï¼Œæ¢¯åº¦å‡ä¸º1ã€‚</p><p>å¥½å¥½åšä½œä¸šï¼Œæ‰èƒ½æœ‰æ›´æ·±çš„ä½“ä¼šï¼</p>", 
            "topic": [
                {
                    "tag": "æ·±åº¦å­¦ä¹ ï¼ˆDeep Learningï¼‰", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "ç¥ç»ç½‘ç»œ", 
                    "tagLink": "https://api.zhihu.com/topics/19607065"
                }, 
                {
                    "tag": "æœºå™¨å­¦ä¹ ", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/44213627", 
            "userName": "ç›´ä¸Šäº‘éœ„", 
            "userLink": "https://www.zhihu.com/people/1033165ce4ad9c3fce69a0793dfab8ad", 
            "upvote": 115, 
            "title": "hexoè¶…å®Œæ•´çš„æ­å»ºæ•™ç¨‹ï¼Œè®©ä½ æ‹¥æœ‰ä¸€ä¸ªä¸“å±ä¸ªäººåšå®¢", 
            "content": "<p></p><p>èŠ±äº†å‡ å¤©æ­å»ºäº†ä¸ªç½‘ç«™ï¼Œå…ˆä¸Šé“¾æ¥ï¼Œæ¬¢è¿æ¥è®¿ï¼š<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">fangzhçš„ä¸ªäººåšå®¢</a></p><p>ç°åœ¨å¸‚é¢ä¸Šçš„åšå®¢å¾ˆå¤šï¼Œå¦‚CSDNï¼Œåšå®¢å›­ï¼Œç®€ä¹¦ç­‰å¹³å°ï¼Œå¯ä»¥ç›´æ¥åœ¨ä¸Šé¢å‘è¡¨ï¼Œç”¨æˆ·äº¤äº’åšçš„å¥½ï¼Œå†™çš„æ–‡ç« ç™¾åº¦ä¹Ÿèƒ½æœç´¢çš„åˆ°ã€‚ç¼ºç‚¹æ˜¯æ¯”è¾ƒä¸è‡ªç”±ï¼Œä¼šå—åˆ°å¹³å°çš„å„ç§é™åˆ¶å’Œæ¶å¿ƒçš„å¹¿å‘Šã€‚</p><p>è€Œè‡ªå·±è´­ä¹°åŸŸåå’ŒæœåŠ¡å™¨ï¼Œæ­å»ºåšå®¢çš„æˆæœ¬å®åœ¨æ˜¯å¤ªé«˜äº†ï¼Œä¸å…‰æ˜¯è¯´è¿™äº›è´­ä¹°æˆæœ¬ï¼Œå•å•æ˜¯èŠ±åŠ›æ°”å»è‡ªå·±æ­è¿™ä¹ˆä¸€ä¸ªç½‘ç«™ï¼Œè¿˜è¦å®šæœŸçš„ç»´æŠ¤å®ƒï¼Œå¯¹äºæˆ‘ä»¬å¤§å¤šæ•°äººæ¥è¯´ï¼Œå®åœ¨æ˜¯æ²¡æœ‰è¿™æ ·çš„ç²¾åŠ›å’Œæ—¶é—´ã€‚</p><p>é‚£ä¹ˆå°±æœ‰ç¬¬ä¸‰ç§é€‰æ‹©ï¼Œç›´æ¥åœ¨github pageå¹³å°ä¸Šæ‰˜ç®¡æˆ‘ä»¬çš„åšå®¢ã€‚è¿™æ ·å°±å¯ä»¥å®‰å¿ƒçš„æ¥å†™ä½œï¼Œåˆä¸éœ€è¦å®šæœŸç»´æŠ¤ï¼Œè€Œä¸”hexoä½œä¸ºä¸€ä¸ªå¿«é€Ÿç®€æ´çš„åšå®¢æ¡†æ¶ï¼Œç”¨å®ƒæ¥æ­å»ºåšå®¢çœŸçš„éå¸¸å®¹æ˜“ã€‚</p><h2><b>Hexoç®€ä»‹</b></h2><p>Hexoæ˜¯ä¸€æ¬¾åŸºäºNode.jsçš„é™æ€åšå®¢æ¡†æ¶ï¼Œä¾èµ–å°‘æ˜“äºå®‰è£…ä½¿ç”¨ï¼Œå¯ä»¥æ–¹ä¾¿çš„ç”Ÿæˆé™æ€ç½‘é¡µæ‰˜ç®¡åœ¨GitHubå’ŒCodingä¸Šï¼Œæ˜¯æ­å»ºåšå®¢çš„é¦–é€‰æ¡†æ¶ã€‚å¤§å®¶å¯ä»¥è¿›å…¥<a href=\"https://link.zhihu.com/?target=https%3A//hexo.io/zh-cn/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">hexoå®˜ç½‘</a>è¿›è¡Œè¯¦ç»†æŸ¥çœ‹ï¼Œå› ä¸ºHexoçš„åˆ›å»ºè€…æ˜¯å°æ¹¾äººï¼Œå¯¹ä¸­æ–‡çš„æ”¯æŒå¾ˆå‹å¥½ï¼Œå¯ä»¥é€‰æ‹©ä¸­æ–‡è¿›è¡ŒæŸ¥çœ‹ã€‚</p><p>æ•™ç¨‹åˆ†ä¸‰ä¸ªéƒ¨åˆ†ï¼Œ</p><ul><li>ç¬¬ä¸€éƒ¨åˆ†ï¼šhexoçš„åˆçº§æ­å»ºè¿˜æœ‰éƒ¨ç½²åˆ°github pageä¸Šï¼Œä»¥åŠä¸ªäººåŸŸåçš„ç»‘å®šã€‚</li><li>ç¬¬äºŒéƒ¨åˆ†ï¼šhexoçš„åŸºæœ¬é…ç½®ï¼Œæ›´æ¢ä¸»é¢˜ï¼Œå®ç°å¤šç»ˆç«¯å·¥ä½œï¼Œä»¥åŠåœ¨coding pageéƒ¨ç½²å®ç°å›½å†…å¤–åˆ†æµ</li><li>ç¬¬ä¸‰éƒ¨åˆ†ï¼šhexoæ·»åŠ å„ç§åŠŸèƒ½ï¼ŒåŒ…æ‹¬æœç´¢çš„SEOï¼Œé˜…è¯»é‡ç»Ÿè®¡ï¼Œè®¿é—®é‡ç»Ÿè®¡å’Œè¯„è®ºç³»ç»Ÿç­‰ã€‚</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>ç¬¬ä¸€éƒ¨åˆ†</b></h2><p>hexoçš„åˆçº§æ­å»ºè¿˜æœ‰éƒ¨ç½²åˆ°github pageä¸Šï¼Œä»¥åŠä¸ªäººåŸŸåçš„ç»‘å®šã€‚</p><h2><b>Hexoæ­å»ºæ­¥éª¤</b></h2><ol><li>å®‰è£…Git</li><li>å®‰è£…Node.js</li><li>å®‰è£…Hexo</li><li>GitHubåˆ›å»ºä¸ªäººä»“åº“</li><li>ç”ŸæˆSSHæ·»åŠ åˆ°GitHub</li><li>å°†hexoéƒ¨ç½²åˆ°GitHub</li><li>è®¾ç½®ä¸ªäººåŸŸå</li><li>å‘å¸ƒæ–‡ç« </li></ol><h2><b>1. å®‰è£…Git</b></h2><p>Gitæ˜¯ç›®å‰ä¸–ç•Œä¸Šæœ€å…ˆè¿›çš„åˆ†å¸ƒå¼ç‰ˆæœ¬æ§åˆ¶ç³»ç»Ÿï¼Œå¯ä»¥æœ‰æ•ˆã€é«˜é€Ÿçš„å¤„ç†ä»å¾ˆå°åˆ°éå¸¸å¤§çš„é¡¹ç›®ç‰ˆæœ¬ç®¡ç†ã€‚ä¹Ÿå°±æ˜¯ç”¨æ¥ç®¡ç†ä½ çš„hexoåšå®¢æ–‡ç« ï¼Œä¸Šä¼ åˆ°GitHubçš„å·¥å…·ã€‚Gitéå¸¸å¼ºå¤§ï¼Œæˆ‘è§‰å¾—å»ºè®®æ¯ä¸ªäººéƒ½å»äº†è§£ä¸€ä¸‹ã€‚å»–é›ªå³°è€å¸ˆçš„Gitæ•™ç¨‹å†™çš„éå¸¸å¥½ï¼Œå¤§å®¶å¯ä»¥äº†è§£ä¸€ä¸‹ã€‚<a href=\"https://link.zhihu.com/?target=https%3A//www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Gitæ•™ç¨‹</a></p><p>windowsï¼šåˆ°gitå®˜ç½‘ä¸Šä¸‹è½½,<a href=\"https://link.zhihu.com/?target=https%3A//gitforwindows.org/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Download git</a>,ä¸‹è½½åä¼šæœ‰ä¸€ä¸ªGit Bashçš„å‘½ä»¤è¡Œå·¥å…·ï¼Œä»¥åå°±ç”¨è¿™ä¸ªå·¥å…·æ¥ä½¿ç”¨gitã€‚</p><p>linuxï¼šå¯¹linuxæ¥è¯´å®åœ¨æ˜¯å¤ªç®€å•äº†ï¼Œå› ä¸ºæœ€æ—©çš„gitå°±æ˜¯åœ¨linuxä¸Šç¼–å†™çš„ï¼Œåªéœ€è¦ä¸€è¡Œä»£ç </p><div class=\"highlight\"><pre><code class=\"language-text\">sudo apt-get install git</code></pre></div><p>å®‰è£…å¥½åï¼Œç”¨<code>git --version</code> æ¥æŸ¥çœ‹ä¸€ä¸‹ç‰ˆæœ¬</p><h2><b>2. å®‰è£…nodejs</b></h2><p>Hexoæ˜¯åŸºäºnodeJSç¼–å†™çš„ï¼Œæ‰€ä»¥éœ€è¦å®‰è£…ä¸€ä¸‹nodeJså’Œé‡Œé¢çš„npmå·¥å…·ã€‚</p><p>windowsï¼š<a href=\"https://link.zhihu.com/?target=https%3A//nodejs.org/en/download/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">nodejs</a>é€‰æ‹©LTSç‰ˆæœ¬å°±è¡Œäº†ã€‚</p><p>linuxï¼š</p><div class=\"highlight\"><pre><code class=\"language-text\">sudo apt-get install nodejs\nsudo apt-get install npm</code></pre></div><p>å®‰è£…å®Œåï¼Œæ‰“å¼€å‘½ä»¤è¡Œ</p><div class=\"highlight\"><pre><code class=\"language-text\">node -v\nnpm -v</code></pre></div><p>æ£€æŸ¥ä¸€ä¸‹æœ‰æ²¡æœ‰å®‰è£…æˆåŠŸ </p><p>é¡ºä¾¿è¯´ä¸€ä¸‹ï¼Œwindowsåœ¨gitå®‰è£…å®Œåï¼Œå°±å¯ä»¥ç›´æ¥ä½¿ç”¨git bashæ¥æ•²å‘½ä»¤è¡Œäº†ï¼Œä¸ç”¨è‡ªå¸¦çš„cmdï¼Œcmdæœ‰ç‚¹éš¾ç”¨ã€‚</p><h2><b>3. å®‰è£…hexo</b></h2><p>å‰é¢gitå’Œnodejså®‰è£…å¥½åï¼Œå°±å¯ä»¥å®‰è£…hexoäº†ï¼Œä½ å¯ä»¥å…ˆåˆ›å»ºä¸€ä¸ªæ–‡ä»¶å¤¹blogï¼Œç„¶å<code>cd</code>åˆ°è¿™ä¸ªæ–‡ä»¶å¤¹ä¸‹ï¼ˆæˆ–è€…åœ¨è¿™ä¸ªæ–‡ä»¶å¤¹ä¸‹ç›´æ¥å³é”®git bashæ‰“å¼€ï¼‰ã€‚</p><p>è¾“å…¥å‘½ä»¤</p><div class=\"highlight\"><pre><code class=\"language-text\">npm install -g hexo-cli</code></pre></div><p>ä¾æ—§ç”¨<code>hexo -v</code>æŸ¥çœ‹ä¸€ä¸‹ç‰ˆæœ¬</p><p>è‡³æ­¤å°±å…¨éƒ¨å®‰è£…å®Œäº†ã€‚</p><p>æ¥ä¸‹æ¥åˆå§‹åŒ–ä¸€ä¸‹hexo</p><div class=\"highlight\"><pre><code class=\"language-text\">hexo init myblog</code></pre></div><p>è¿™ä¸ªmyblogå¯ä»¥è‡ªå·±å–ä»€ä¹ˆåå­—éƒ½è¡Œï¼Œç„¶å</p><div class=\"highlight\"><pre><code class=\"language-bash\"><span class=\"nb\">cd</span> myblog //è¿›å…¥è¿™ä¸ªmyblogæ–‡ä»¶å¤¹\nnpm install</code></pre></div><p>æ–°å»ºå®Œæˆåï¼ŒæŒ‡å®šæ–‡ä»¶å¤¹ç›®å½•ä¸‹æœ‰ï¼š</p><ul><li>node_modules: ä¾èµ–åŒ…</li><li>publicï¼šå­˜æ”¾ç”Ÿæˆçš„é¡µé¢</li><li>scaffoldsï¼šç”Ÿæˆæ–‡ç« çš„ä¸€äº›æ¨¡æ¿</li><li>sourceï¼šç”¨æ¥å­˜æ”¾ä½ çš„æ–‡ç« </li><li>themesï¼šä¸»é¢˜</li><li>** _config.yml: åšå®¢çš„é…ç½®æ–‡ä»¶**</li></ul><div class=\"highlight\"><pre><code class=\"language-text\">hexo g\nhexo server</code></pre></div><p>æ‰“å¼€hexoçš„æœåŠ¡ï¼Œåœ¨æµè§ˆå™¨è¾“å…¥localhost:4000å°±å¯ä»¥çœ‹åˆ°ä½ ç”Ÿæˆçš„åšå®¢äº†ã€‚</p><p>å¤§æ¦‚é•¿è¿™æ ·ï¼š </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-d301dfbac9165b21ab1ee0a860f44de4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1344\" data-rawheight=\"542\" class=\"origin_image zh-lightbox-thumb\" width=\"1344\" data-original=\"https://pic1.zhimg.com/v2-d301dfbac9165b21ab1ee0a860f44de4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1344&#39; height=&#39;542&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1344\" data-rawheight=\"542\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1344\" data-original=\"https://pic1.zhimg.com/v2-d301dfbac9165b21ab1ee0a860f44de4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-d301dfbac9165b21ab1ee0a860f44de4_b.jpg\"/></figure><p> ä½¿ç”¨ctrl+cå¯ä»¥æŠŠæœåŠ¡å…³æ‰ã€‚</p><h2><b>4. GitHubåˆ›å»ºä¸ªäººä»“åº“</b></h2><p>é¦–å…ˆï¼Œä½ å…ˆè¦æœ‰ä¸€ä¸ªGitHubè´¦æˆ·ï¼Œå»æ³¨å†Œä¸€ä¸ªå§ã€‚</p><p>æ³¨å†Œå®Œç™»å½•åï¼Œåœ¨<a href=\"https://link.zhihu.com/?target=http%3A//GitHub.com\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">GitHub.com</span><span class=\"invisible\"></span></a>ä¸­çœ‹åˆ°ä¸€ä¸ªNew repositoryï¼Œæ–°å»ºä»“åº“ </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-4387cb776ccc72189bc06fd511c1e19e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"522\" data-rawheight=\"433\" class=\"origin_image zh-lightbox-thumb\" width=\"522\" data-original=\"https://pic3.zhimg.com/v2-4387cb776ccc72189bc06fd511c1e19e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;522&#39; height=&#39;433&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"522\" data-rawheight=\"433\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"522\" data-original=\"https://pic3.zhimg.com/v2-4387cb776ccc72189bc06fd511c1e19e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-4387cb776ccc72189bc06fd511c1e19e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>åˆ›å»ºä¸€ä¸ªå’Œä½ ç”¨æˆ·åç›¸åŒçš„ä»“åº“ï¼Œåé¢åŠ .<a href=\"https://link.zhihu.com/?target=http%3A//github.io\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">github.io</span><span class=\"invisible\"></span></a>ï¼Œåªæœ‰è¿™æ ·ï¼Œå°†æ¥è¦éƒ¨ç½²åˆ°GitHub pageçš„æ—¶å€™ï¼Œæ‰ä¼šè¢«è¯†åˆ«ï¼Œä¹Ÿå°±æ˜¯<a href=\"https://link.zhihu.com/?target=http%3A//xxxx.github.io\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">xxxx.github.io</span><span class=\"invisible\"></span></a>ï¼Œå…¶ä¸­xxxå°±æ˜¯ä½ æ³¨å†ŒGitHubçš„ç”¨æˆ·åã€‚æˆ‘è¿™é‡Œæ˜¯å·²ç»å»ºè¿‡äº†ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-9d5692a0e62fd62f902f1085348f5e7e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"680\" data-rawheight=\"868\" class=\"origin_image zh-lightbox-thumb\" width=\"680\" data-original=\"https://pic3.zhimg.com/v2-9d5692a0e62fd62f902f1085348f5e7e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;680&#39; height=&#39;868&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"680\" data-rawheight=\"868\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"680\" data-original=\"https://pic3.zhimg.com/v2-9d5692a0e62fd62f902f1085348f5e7e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-9d5692a0e62fd62f902f1085348f5e7e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>ç‚¹å‡»create repositoryã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>5. ç”ŸæˆSSHæ·»åŠ åˆ°GitHub</b></h2><p>å›åˆ°ä½ çš„git bashä¸­ï¼Œ</p><div class=\"highlight\"><pre><code class=\"language-text\">git config --global user.name &#34;yourname&#34;\ngit config --global user.email &#34;youremail&#34;</code></pre></div><p>è¿™é‡Œçš„yournameè¾“å…¥ä½ çš„GitHubç”¨æˆ·åï¼Œyouremailè¾“å…¥ä½ GitHubçš„é‚®ç®±ã€‚è¿™æ ·GitHubæ‰èƒ½çŸ¥é“ä½ æ˜¯ä¸æ˜¯å¯¹åº”å®ƒçš„è´¦æˆ·ã€‚</p><p>å¯ä»¥ç”¨ä»¥ä¸‹ä¸¤æ¡ï¼Œæ£€æŸ¥ä¸€ä¸‹ä½ æœ‰æ²¡æœ‰è¾“å¯¹</p><div class=\"highlight\"><pre><code class=\"language-text\">git config user.name\ngit config user.email</code></pre></div><p>ç„¶ååˆ›å»ºSSH,ä¸€è·¯å›è½¦</p><div class=\"highlight\"><pre><code class=\"language-text\">ssh-keygen -t rsa -C &#34;youremail&#34;</code></pre></div><p>è¿™ä¸ªæ—¶å€™å®ƒä¼šå‘Šè¯‰ä½ å·²ç»ç”Ÿæˆäº†.sshçš„æ–‡ä»¶å¤¹ã€‚åœ¨ä½ çš„ç”µè„‘ä¸­æ‰¾åˆ°è¿™ä¸ªæ–‡ä»¶å¤¹ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-a9593147853b8f2485f05d1f04d45bb6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"731\" data-rawheight=\"255\" class=\"origin_image zh-lightbox-thumb\" width=\"731\" data-original=\"https://pic3.zhimg.com/v2-a9593147853b8f2485f05d1f04d45bb6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;731&#39; height=&#39;255&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"731\" data-rawheight=\"255\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"731\" data-original=\"https://pic3.zhimg.com/v2-a9593147853b8f2485f05d1f04d45bb6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-a9593147853b8f2485f05d1f04d45bb6_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>sshï¼Œç®€å•æ¥è®²ï¼Œå°±æ˜¯ä¸€ä¸ªç§˜é’¥ï¼Œå…¶ä¸­ï¼Œ<code>id_rsa</code>æ˜¯ä½ è¿™å°ç”µè„‘çš„ç§äººç§˜é’¥ï¼Œä¸èƒ½ç»™åˆ«äººçœ‹çš„ï¼Œ<code>id_rsa.pub</code>æ˜¯å…¬å…±ç§˜é’¥ï¼Œå¯ä»¥éšä¾¿ç»™åˆ«äººçœ‹ã€‚æŠŠè¿™ä¸ªå…¬é’¥æ”¾åœ¨GitHubä¸Šï¼Œè¿™æ ·å½“ä½ é“¾æ¥GitHubè‡ªå·±çš„è´¦æˆ·æ—¶ï¼Œå®ƒå°±ä¼šæ ¹æ®å…¬é’¥åŒ¹é…ä½ çš„ç§é’¥ï¼Œå½“èƒ½å¤Ÿç›¸äº’åŒ¹é…æ—¶ï¼Œæ‰èƒ½å¤Ÿé¡ºåˆ©çš„é€šè¿‡gitä¸Šä¼ ä½ çš„æ–‡ä»¶åˆ°GitHubä¸Šã€‚</p><p>è€Œååœ¨GitHubçš„settingä¸­ï¼Œæ‰¾åˆ°SSH keysçš„è®¾ç½®é€‰é¡¹ï¼Œç‚¹å‡»<code>New SSH key</code> æŠŠä½ çš„<code>id_rsa.pub</code>é‡Œé¢çš„ä¿¡æ¯å¤åˆ¶è¿›å»ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-aad16de6208ce8a7b2cd51b16a1b2aa2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1324\" data-rawheight=\"591\" class=\"origin_image zh-lightbox-thumb\" width=\"1324\" data-original=\"https://pic3.zhimg.com/v2-aad16de6208ce8a7b2cd51b16a1b2aa2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1324&#39; height=&#39;591&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1324\" data-rawheight=\"591\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1324\" data-original=\"https://pic3.zhimg.com/v2-aad16de6208ce8a7b2cd51b16a1b2aa2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-aad16de6208ce8a7b2cd51b16a1b2aa2_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>åœ¨gitbashä¸­ï¼ŒæŸ¥çœ‹æ˜¯å¦æˆåŠŸ</p><div class=\"highlight\"><pre><code class=\"language-text\">ssh -T git@github.com</code></pre></div><h2><b>6. å°†hexoéƒ¨ç½²åˆ°GitHub</b></h2><p>è¿™ä¸€æ­¥ï¼Œæˆ‘ä»¬å°±å¯ä»¥å°†hexoå’ŒGitHubå…³è”èµ·æ¥ï¼Œä¹Ÿå°±æ˜¯å°†hexoç”Ÿæˆçš„æ–‡ç« éƒ¨ç½²åˆ°GitHubä¸Šï¼Œæ‰“å¼€ç«™ç‚¹é…ç½®æ–‡ä»¶ <code>_config.yml</code>ï¼Œç¿»åˆ°æœ€åï¼Œä¿®æ”¹ä¸º YourgithubNameå°±æ˜¯ä½ çš„GitHubè´¦æˆ·</p><div class=\"highlight\"><pre><code class=\"language-text\">deploy:\n  type: git\n  repo: https://github.com/YourgithubName/YourgithubName.github.io.git\n  branch: master</code></pre></div><p>è¿™ä¸ªæ—¶å€™éœ€è¦å…ˆå®‰è£…deploy-git ï¼Œä¹Ÿå°±æ˜¯éƒ¨ç½²çš„å‘½ä»¤,è¿™æ ·ä½ æ‰èƒ½ç”¨å‘½ä»¤éƒ¨ç½²åˆ°GitHubã€‚</p><div class=\"highlight\"><pre><code class=\"language-text\">npm install hexo-deployer-git --save</code></pre></div><p>ç„¶å</p><div class=\"highlight\"><pre><code class=\"language-text\">hexo clean\nhexo generate\nhexo deploy</code></pre></div><p>å…¶ä¸­ <code>hexo clean</code>æ¸…é™¤äº†ä½ ä¹‹å‰ç”Ÿæˆçš„ä¸œè¥¿ï¼Œä¹Ÿå¯ä»¥ä¸åŠ ã€‚ <code>hexo generate</code> é¡¾åæ€ä¹‰ï¼Œç”Ÿæˆé™æ€æ–‡ç« ï¼Œå¯ä»¥ç”¨ <code>hexo g</code>ç¼©å†™ <code>hexo deploy</code> éƒ¨ç½²æ–‡ç« ï¼Œå¯ä»¥ç”¨<code>hexo d</code>ç¼©å†™</p><p>æ³¨æ„deployæ—¶å¯èƒ½è¦ä½ è¾“å…¥usernameå’Œpasswordã€‚</p><p>å¾—åˆ°ä¸‹å›¾å°±è¯´æ˜éƒ¨ç½²æˆåŠŸäº†ï¼Œè¿‡ä¸€ä¼šå„¿å°±å¯ä»¥åœ¨<code>http://yourname.github.io</code> è¿™ä¸ªç½‘ç«™çœ‹åˆ°ä½ çš„åšå®¢äº†ï¼ï¼ </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-8fb2b2026aa0a748594642bb0f53f72b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"782\" data-rawheight=\"66\" class=\"origin_image zh-lightbox-thumb\" width=\"782\" data-original=\"https://pic4.zhimg.com/v2-8fb2b2026aa0a748594642bb0f53f72b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;782&#39; height=&#39;66&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"782\" data-rawheight=\"66\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"782\" data-original=\"https://pic4.zhimg.com/v2-8fb2b2026aa0a748594642bb0f53f72b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-8fb2b2026aa0a748594642bb0f53f72b_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>7. è®¾ç½®ä¸ªäººåŸŸå</b></h2><p>ç°åœ¨ä½ çš„ä¸ªäººç½‘ç«™çš„åœ°å€æ˜¯ <code>yourname.github.io</code>ï¼Œå¦‚æœè§‰å¾—è¿™ä¸ªç½‘å€é€¼æ ¼ä¸å¤ªå¤Ÿï¼Œè¿™å°±éœ€è¦ä½ è®¾ç½®ä¸ªäººåŸŸåäº†ã€‚ä½†æ˜¯éœ€è¦èŠ±é’±ã€‚</p><p>æ³¨å†Œä¸€ä¸ªé˜¿é‡Œäº‘è´¦æˆ·,åœ¨<a href=\"https://link.zhihu.com/?target=https%3A//wanwang.aliyun.com/%3Fspm%3D5176.8142029.digitalization.2.e9396d3e46JCc5\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">é˜¿é‡Œäº‘</a>ä¸Šä¹°ä¸€ä¸ªåŸŸåï¼Œæˆ‘ä¹°çš„æ˜¯ <code>fangzh.top</code>ï¼Œå„ä¸ªåç¼€çš„ä»·æ ¼ä¸å¤ªä¸€æ ·ï¼Œæ¯”å¦‚æœ€å¹¿æ³›çš„.comå°±æ¯”è¾ƒè´µï¼Œçœ‹ä¸ªäººå–œå¥½å’¯ã€‚</p><p>ä½ éœ€è¦å…ˆå»è¿›è¡Œå®åè®¤è¯,ç„¶ååœ¨åŸŸåæ§åˆ¶å°ä¸­ï¼Œçœ‹åˆ°ä½ è´­ä¹°çš„åŸŸåã€‚</p><p>ç‚¹<b>è§£æ</b>è¿›å»ï¼Œæ·»åŠ è§£æã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-11ace56a64930bf2e997aad3618f548e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"474\" data-rawheight=\"284\" class=\"origin_image zh-lightbox-thumb\" width=\"474\" data-original=\"https://pic3.zhimg.com/v2-11ace56a64930bf2e997aad3618f548e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;474&#39; height=&#39;284&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"474\" data-rawheight=\"284\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"474\" data-original=\"https://pic3.zhimg.com/v2-11ace56a64930bf2e997aad3618f548e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-11ace56a64930bf2e997aad3618f548e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>å…¶ä¸­ï¼Œ192.30.252.153 å’Œ 192.30.252.154 æ˜¯GitHubçš„æœåŠ¡å™¨åœ°å€ã€‚ <b>æ³¨æ„ï¼Œè§£æçº¿è·¯é€‰æ‹©é»˜è®¤</b>ï¼Œä¸è¦åƒæˆ‘ä¸€æ ·é€‰å¢ƒå¤–ã€‚è¿™ä¸ªå¢ƒå¤–æ˜¯åé¢æ¥åšå›½å†…å¤–åˆ†æµç”¨çš„,åœ¨åé¢çš„åšå®¢ä¸­ä¼šè®²åˆ°ã€‚è®°å¾—ç°åœ¨é€‰æ‹©<b>é»˜è®¤</b>ï¼ï¼</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-e572fb0a5de7519d606e80c49829f40f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"875\" data-rawheight=\"191\" class=\"origin_image zh-lightbox-thumb\" width=\"875\" data-original=\"https://pic4.zhimg.com/v2-e572fb0a5de7519d606e80c49829f40f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;875&#39; height=&#39;191&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"875\" data-rawheight=\"191\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"875\" data-original=\"https://pic4.zhimg.com/v2-e572fb0a5de7519d606e80c49829f40f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-e572fb0a5de7519d606e80c49829f40f_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>ç™»å½•GitHubï¼Œè¿›å…¥ä¹‹å‰åˆ›å»ºçš„ä»“åº“ï¼Œç‚¹å‡»settingsï¼Œè®¾ç½®Custom domainï¼Œè¾“å…¥ä½ çš„åŸŸå<code>fangzh.top</code></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b75ea5d443b68a8600601e596553ad09_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"388\" data-rawheight=\"166\" class=\"content_image\" width=\"388\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;388&#39; height=&#39;166&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"388\" data-rawheight=\"166\" class=\"content_image lazy\" width=\"388\" data-actualsrc=\"https://pic2.zhimg.com/v2-b75ea5d443b68a8600601e596553ad09_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>ç„¶ååœ¨ä½ çš„åšå®¢æ–‡ä»¶sourceä¸­åˆ›å»ºä¸€ä¸ªåä¸ºCNAMEæ–‡ä»¶ï¼Œä¸è¦åç¼€ã€‚å†™ä¸Šä½ çš„åŸŸåã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-1b974b97c26a5a9ff27d973f31d804b3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"296\" data-rawheight=\"226\" class=\"content_image\" width=\"296\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;296&#39; height=&#39;226&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"296\" data-rawheight=\"226\" class=\"content_image lazy\" width=\"296\" data-actualsrc=\"https://pic4.zhimg.com/v2-1b974b97c26a5a9ff27d973f31d804b3_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>æœ€åï¼Œåœ¨gitbashä¸­ï¼Œè¾“å…¥</p><div class=\"highlight\"><pre><code class=\"language-text\">hexo clean\nhexo g\nhexo d</code></pre></div><p>è¿‡ä¸äº†å¤šä¹…ï¼Œå†æ‰“å¼€ä½ çš„æµè§ˆå™¨ï¼Œè¾“å…¥ä½ è‡ªå·±çš„åŸŸåï¼Œå°±å¯ä»¥çœ‹åˆ°æ­å»ºçš„ç½‘ç«™å•¦ï¼</p><p>æ¥ä¸‹æ¥ä½ å°±å¯ä»¥æ­£å¼å¼€å§‹å†™æ–‡ç« äº†ã€‚</p><div class=\"highlight\"><pre><code class=\"language-text\">hexo new newpapername</code></pre></div><p>ç„¶ååœ¨source/_postä¸­æ‰“å¼€markdownæ–‡ä»¶ï¼Œå°±å¯ä»¥å¼€å§‹ç¼–è¾‘äº†ã€‚å½“ä½ å†™å®Œçš„æ—¶å€™ï¼Œå†</p><div class=\"highlight\"><pre><code class=\"language-text\">hexo clean\nhexo g\nhexo d</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>ç¬¬äºŒéƒ¨åˆ†</b></h2><p>hexoçš„åŸºæœ¬é…ç½®ï¼Œæ›´æ¢ä¸»é¢˜ï¼Œå®ç°å¤šç»ˆç«¯å·¥ä½œï¼Œä»¥åŠåœ¨coding pageéƒ¨ç½²å®ç°å›½å†…å¤–åˆ†æµã€‚</p><h2><b>1. hexoåŸºæœ¬é…ç½®</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p>åœ¨æ–‡ä»¶æ ¹ç›®å½•ä¸‹çš„<code>_config.yml</code>ï¼Œå°±æ˜¯æ•´ä¸ªhexoæ¡†æ¶çš„é…ç½®æ–‡ä»¶äº†ã€‚å¯ä»¥åœ¨é‡Œé¢ä¿®æ”¹å¤§éƒ¨åˆ†çš„é…ç½®ã€‚è¯¦ç»†å¯å‚è€ƒ<a href=\"https://link.zhihu.com/?target=https%3A//hexo.io/zh-cn/docs/configuration\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">å®˜æ–¹çš„é…ç½®</a>æè¿°ã€‚</p><h2><b>ç½‘ç«™</b></h2><p><b>å‚æ•°æè¿°</b><code>title</code>ç½‘ç«™æ ‡é¢˜<code>subtitle</code>ç½‘ç«™å‰¯æ ‡é¢˜<code>description</code>ç½‘ç«™æè¿°<code>author</code>æ‚¨çš„åå­—<code>language</code>ç½‘ç«™ä½¿ç”¨çš„è¯­è¨€<code>timezone</code>ç½‘ç«™æ—¶åŒºã€‚Hexo é»˜è®¤ä½¿ç”¨æ‚¨ç”µè„‘çš„æ—¶åŒºã€‚<a href=\"https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/List_of_tz_database_time_zones\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">æ—¶åŒºåˆ—è¡¨</a>ã€‚æ¯”å¦‚è¯´ï¼š<code>America/New_York</code>, <code>Japan</code>, å’Œ <code>UTC</code> ã€‚</p><p>å…¶ä¸­ï¼Œ<code>description</code>ä¸»è¦ç”¨äºSEOï¼Œå‘Šè¯‰æœç´¢å¼•æ“ä¸€ä¸ªå…³äºæ‚¨ç«™ç‚¹çš„ç®€å•æè¿°ï¼Œé€šå¸¸å»ºè®®åœ¨å…¶ä¸­åŒ…å«æ‚¨ç½‘ç«™çš„å…³é”®è¯ã€‚<code>author</code>å‚æ•°ç”¨äºä¸»é¢˜æ˜¾ç¤ºæ–‡ç« çš„ä½œè€…ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>ç½‘å€</b></h2><p><b>å‚æ•°æè¿°</b><code>url</code>ç½‘å€<code>root</code>ç½‘ç«™æ ¹ç›®å½•<code>permalink</code>æ–‡ç« çš„ <a href=\"https://link.zhihu.com/?target=https%3A//hexo.io/zh-cn/docs/permalinks\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">æ°¸ä¹…é“¾æ¥</a> æ ¼å¼<code>permalink_defaults</code>æ°¸ä¹…é“¾æ¥ä¸­å„éƒ¨åˆ†çš„é»˜è®¤å€¼</p><p>åœ¨è¿™é‡Œï¼Œä½ éœ€è¦æŠŠ<code>url</code>æ”¹æˆä½ çš„ç½‘ç«™åŸŸåã€‚</p><p>permalinkï¼Œä¹Ÿå°±æ˜¯ä½ ç”ŸæˆæŸä¸ªæ–‡ç« æ—¶çš„é‚£ä¸ªé“¾æ¥æ ¼å¼ã€‚</p><p>æ¯”å¦‚æˆ‘æ–°å»ºä¸€ä¸ªæ–‡ç« å«<code>temp.md</code>ï¼Œé‚£ä¹ˆè¿™ä¸ªæ—¶å€™ä»–è‡ªåŠ¨ç”Ÿæˆçš„åœ°å€å°±æ˜¯<code>http://yoursite.com/2018/09/05/temp</code>ã€‚</p><p>ä»¥ä¸‹æ˜¯å®˜æ–¹ç»™å‡ºçš„ç¤ºä¾‹ï¼Œå…³äºé“¾æ¥çš„å˜é‡è¿˜æœ‰å¾ˆå¤šï¼Œéœ€è¦çš„å¯ä»¥å»å®˜ç½‘ä¸ŠæŸ¥æ‰¾ <a href=\"https://link.zhihu.com/?target=https%3A//hexo.io/zh-cn/docs/permalinks\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">æ°¸ä¹…é“¾æ¥</a> ã€‚</p><p><b>å‚æ•°ç»“æœ</b><code>:year/:month/:day/:title/</code>2013/07/14/hello-world<code>:year-:month-:day-:title.html</code>2013-07-14-hello-world.html<code>:category/:title</code>foo/bar/hello-world</p><p class=\"ztext-empty-paragraph\"><br/></p><p>å†å¾€ä¸‹ç¿»ï¼Œä¸­é—´è¿™äº›éƒ½é»˜è®¤å°±å¥½äº†ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-text\">theme: landscape\nâ€‹\n# Deployment\n## Docs: https://hexo.io/docs/deployment.html\ndeploy:\n  type: git\n  repo: &lt;repository url&gt;\n  branch: [branch]\nâ€‹</code></pre></div><p><code>theme</code>å°±æ˜¯é€‰æ‹©ä»€ä¹ˆä¸»é¢˜ï¼Œä¹Ÿå°±æ˜¯åœ¨<code>theme</code>è¿™ä¸ªæ–‡ä»¶å¤¹ä¸‹ï¼Œåœ¨å®˜ç½‘ä¸Šæœ‰å¾ˆå¤šä¸ªä¸»é¢˜ï¼Œé»˜è®¤ç»™ä½ å®‰è£…çš„æ˜¯<code>lanscape</code>è¿™ä¸ªä¸»é¢˜ã€‚å½“ä½ éœ€è¦æ›´æ¢ä¸»é¢˜æ—¶ï¼Œåœ¨å®˜ç½‘ä¸Šä¸‹è½½ï¼ŒæŠŠä¸»é¢˜çš„æ–‡ä»¶æ”¾åœ¨<code>theme</code>æ–‡ä»¶å¤¹ä¸‹ï¼Œå†ä¿®æ”¹è¿™ä¸ªå‚æ•°å°±å¯ä»¥äº†ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><p>æ¥ä¸‹æ¥è¿™ä¸ª<code>deploy</code>å°±æ˜¯ç½‘ç«™çš„éƒ¨ç½²çš„ï¼Œ<code>repo</code>å°±æ˜¯ä»“åº“(<code>Repository</code>)çš„ç®€å†™ã€‚<code>branch</code>é€‰æ‹©ä»“åº“çš„å“ªä¸ªåˆ†æ”¯ã€‚è¿™ä¸ªåœ¨ä¹‹å‰è¿›è¡Œgithub pageéƒ¨ç½²çš„æ—¶å€™å·²ç»ä¿®æ”¹è¿‡äº†ï¼Œä¸å†èµ˜è¿°ã€‚è€Œè¿™ä¸ªåœ¨åé¢è¿›è¡ŒåŒå¹³å°éƒ¨ç½²çš„æ—¶å€™ä¼šå†æ¬¡ç”¨åˆ°ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>Front-matter</b></h2><p>Front-matter æ˜¯æ–‡ä»¶æœ€ä¸Šæ–¹ä»¥ <code>---</code> åˆ†éš”çš„åŒºåŸŸï¼Œç”¨äºæŒ‡å®šä¸ªåˆ«æ–‡ä»¶çš„å˜é‡ï¼Œä¸¾ä¾‹æ¥è¯´ï¼š</p><div class=\"highlight\"><pre><code class=\"language-text\">title: Hello World\ndate: 2013/7/13 20:46:25\n---</code></pre></div><p>ä¸‹æ˜¯é¢„å…ˆå®šä¹‰çš„å‚æ•°ï¼Œæ‚¨å¯åœ¨æ¨¡æ¿ä¸­ä½¿ç”¨è¿™äº›å‚æ•°å€¼å¹¶åŠ ä»¥åˆ©ç”¨ã€‚</p><p><b>å‚æ•°æè¿°</b><code>layout</code>å¸ƒå±€<code>title</code>æ ‡é¢˜<code>date</code>å»ºç«‹æ—¥æœŸ<code>updated</code>æ›´æ–°æ—¥æœŸ<code>comments</code>å¼€å¯æ–‡ç« çš„è¯„è®ºåŠŸèƒ½<code>tags</code>æ ‡ç­¾ï¼ˆä¸é€‚ç”¨äºåˆ†é¡µï¼‰<code>categories</code>åˆ†ç±»ï¼ˆä¸é€‚ç”¨äºåˆ†é¡µï¼‰<code>permalink</code>è¦†ç›–æ–‡ç« ç½‘å€</p><p class=\"ztext-empty-paragraph\"><br/></p><p>å…¶ä¸­ï¼Œåˆ†ç±»å’Œæ ‡ç­¾éœ€è¦åŒºåˆ«ä¸€ä¸‹ï¼Œåˆ†ç±»å…·æœ‰é¡ºåºæ€§å’Œå±‚æ¬¡æ€§ï¼Œä¹Ÿå°±æ˜¯è¯´ <code>Foo, Bar</code> ä¸ç­‰äº <code>Bar, Foo</code>ï¼›è€Œæ ‡ç­¾æ²¡æœ‰é¡ºåºå’Œå±‚æ¬¡ã€‚</p><div class=\"highlight\"><pre><code class=\"language-text\">categories:\n- Diary\ntags:\n- PS3\n- Games</code></pre></div><h2><b>layoutï¼ˆå¸ƒå±€ï¼‰</b></h2><p>å½“ä½ æ¯ä¸€æ¬¡ä½¿ç”¨ä»£ç </p><div class=\"highlight\"><pre><code class=\"language-text\">hexo new paper</code></pre></div><p>å®ƒå…¶å®é»˜è®¤ä½¿ç”¨çš„æ˜¯<code>post</code>è¿™ä¸ªå¸ƒå±€ï¼Œä¹Ÿå°±æ˜¯åœ¨<code>source</code>æ–‡ä»¶å¤¹ä¸‹çš„<code>_post</code>é‡Œé¢ã€‚</p><p>Hexo æœ‰ä¸‰ç§é»˜è®¤å¸ƒå±€ï¼š<code>post</code>ã€<code>page</code> å’Œ <code>draft</code>ï¼Œå®ƒä»¬åˆ†åˆ«å¯¹åº”ä¸åŒçš„è·¯å¾„ï¼Œè€Œæ‚¨è‡ªå®šä¹‰çš„å…¶ä»–å¸ƒå±€å’Œ <code>post</code> ç›¸åŒï¼Œéƒ½å°†å‚¨å­˜åˆ° <code>source/_posts</code> æ–‡ä»¶å¤¹ã€‚</p><p><b>å¸ƒå±€è·¯å¾„</b><code>postsource/_postspagesourcedraftsource/_drafts</code></p><p>è€Œnewè¿™ä¸ªå‘½ä»¤å…¶å®æ˜¯ï¼š</p><div class=\"highlight\"><pre><code class=\"language-text\">hexo new [layout] &lt;title&gt;</code></pre></div><p>åªä¸è¿‡è¿™ä¸ªlayouté»˜è®¤æ˜¯postç½¢äº†ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>page</b></h2><p>å¦‚æœä½ æƒ³å¦èµ·ä¸€é¡µï¼Œé‚£ä¹ˆå¯ä»¥ä½¿ç”¨</p><div class=\"highlight\"><pre><code class=\"language-text\">hexo new page board</code></pre></div><p>ç³»ç»Ÿä¼šè‡ªåŠ¨ç»™ä½ åœ¨sourceæ–‡ä»¶å¤¹ä¸‹åˆ›å»ºä¸€ä¸ªboardæ–‡ä»¶å¤¹ï¼Œä»¥åŠboardæ–‡ä»¶å¤¹ä¸­çš„index.mdï¼Œè¿™æ ·ä½ è®¿é—®çš„boardå¯¹åº”çš„é“¾æ¥å°±æ˜¯<code>http://xxx.xxx/board</code></p><h2><b>draft</b></h2><p>draftæ˜¯è‰ç¨¿çš„æ„æ€ï¼Œä¹Ÿå°±æ˜¯ä½ å¦‚æœæƒ³å†™æ–‡ç« ï¼Œåˆä¸å¸Œæœ›è¢«çœ‹åˆ°ï¼Œé‚£ä¹ˆå¯ä»¥</p><div class=\"highlight\"><pre><code class=\"language-text\">hexo new draft newpage</code></pre></div><p>è¿™æ ·ä¼šåœ¨source/_draftä¸­æ–°å»ºä¸€ä¸ªnewpage.mdæ–‡ä»¶ï¼Œå¦‚æœä½ çš„è‰ç¨¿æ–‡ä»¶å†™çš„è¿‡ç¨‹ä¸­ï¼Œæƒ³è¦é¢„è§ˆä¸€ä¸‹ï¼Œé‚£ä¹ˆå¯ä»¥ä½¿ç”¨</p><div class=\"highlight\"><pre><code class=\"language-text\">hexo server --draft</code></pre></div><p>åœ¨æœ¬åœ°ç«¯å£ä¸­å¼€å¯æœåŠ¡é¢„è§ˆã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><p>å¦‚æœä½ çš„è‰ç¨¿æ–‡ä»¶å†™å®Œäº†ï¼Œæƒ³è¦å‘è¡¨åˆ°postä¸­ï¼Œ</p><div class=\"highlight\"><pre><code class=\"language-text\">hexo publish draft newpage</code></pre></div><p>å°±ä¼šè‡ªåŠ¨æŠŠnewpage.mdå‘é€åˆ°postä¸­ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>2. æ›´æ¢ä¸»é¢˜</b></h2><p>åˆ°è¿™ä¸€æ­¥ï¼Œå¦‚æœä½ è§‰å¾—é»˜è®¤çš„<code>landscape</code>ä¸»é¢˜ä¸å¥½çœ‹ï¼Œé‚£ä¹ˆå¯ä»¥åœ¨å®˜ç½‘çš„ä¸»é¢˜ä¸­ï¼Œé€‰æ‹©ä½ å–œæ¬¢çš„ä¸€ä¸ªä¸»é¢˜è¿›è¡Œä¿®æ”¹å°±å¯ä»¥å•¦ã€‚<a href=\"https://link.zhihu.com/?target=https%3A//hexo.io/themes/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">ç‚¹è¿™é‡Œ</a></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-37456c59d1378608228faf65aa58e836_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1866\" data-rawheight=\"989\" class=\"origin_image zh-lightbox-thumb\" width=\"1866\" data-original=\"https://pic3.zhimg.com/v2-37456c59d1378608228faf65aa58e836_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1866&#39; height=&#39;989&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1866\" data-rawheight=\"989\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1866\" data-original=\"https://pic3.zhimg.com/v2-37456c59d1378608228faf65aa58e836_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-37456c59d1378608228faf65aa58e836_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>è¿™é‡Œæœ‰200å¤šä¸ªä¸»é¢˜å¯ä»¥é€‰ã€‚ä¸è¿‡æœ€å—æ¬¢è¿çš„å°±æ˜¯é‚£ä¹ˆå‡ ä¸ªï¼Œæ¯”å¦‚<a href=\"https://link.zhihu.com/?target=https%3A//github.com/theme-next/hexo-theme-next\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">NexTä¸»é¢˜</a>ï¼Œéå¸¸çš„ç®€æ´å¥½çœ‹ï¼Œå¤§å¤šæ•°äººéƒ½é€‰æ‹©è¿™ä¸ªï¼Œå…³äºè¿™ä¸ªçš„æ•™ç¨‹ä¹Ÿæ¯”è¾ƒå¤šã€‚ä¸è¿‡æˆ‘é€‰æ‹©çš„æ˜¯<a href=\"https://link.zhihu.com/?target=https%3A//github.com/ppoffice/hexo-theme-hueman\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">hueman</a>è¿™ä¸ªä¸»é¢˜ï¼Œå¥½åƒæ˜¯ä»WordPressç§»æ¤è¿‡æ¥çš„ï¼Œå±•ç¤ºæ•ˆæœå¦‚ä¸‹ï¼š</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-4973f83e8d757d7126cc5091f03ef452_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1848\" data-rawheight=\"944\" class=\"origin_image zh-lightbox-thumb\" width=\"1848\" data-original=\"https://pic3.zhimg.com/v2-4973f83e8d757d7126cc5091f03ef452_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1848&#39; height=&#39;944&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1848\" data-rawheight=\"944\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1848\" data-original=\"https://pic3.zhimg.com/v2-4973f83e8d757d7126cc5091f03ef452_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-4973f83e8d757d7126cc5091f03ef452_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>ä¸ç®¡æ€ä¹ˆæ ·ï¼Œè‡³å°‘æ˜¯ç¬¦åˆæˆ‘ä¸ªäººçš„å®¡ç¾ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><p>ç›´æ¥åœ¨githubé“¾æ¥ä¸Šä¸‹è½½ä¸‹æ¥ï¼Œç„¶åæ”¾åˆ°<code>theme</code>æ–‡ä»¶å¤¹ä¸‹å°±è¡Œäº†ï¼Œç„¶åå†åœ¨åˆšæ‰è¯´çš„é…ç½®æ–‡ä»¶ä¸­æŠŠ<code>theme</code>æ¢æˆé‚£ä¸ªä¸»é¢˜æ–‡ä»¶å¤¹çš„åå­—ï¼Œå®ƒå°±ä¼šè‡ªåŠ¨åœ¨<code>theme</code>æ–‡ä»¶å¤¹ä¸­æœç´¢ä½ é…ç½®çš„ä¸»é¢˜ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><p>è€Œåè¿›å…¥<code>hueman</code>è¿™ä¸ªæ–‡ä»¶å¤¹ï¼Œå¯ä»¥çœ‹åˆ°é‡Œé¢ä¹Ÿæœ‰ä¸€ä¸ªé…ç½®æ–‡ä»¶<code>_config.xml</code>ï¼Œè²Œä¼¼å®ƒé»˜è®¤æ˜¯<code>_config.xml.example</code>ï¼ŒæŠŠå®ƒå¤åˆ¶ä¸€ä»½ï¼Œé‡å‘½åä¸º<code>_config.xml</code>å°±å¯ä»¥äº†ã€‚è¿™ä¸ªé…ç½®æ–‡ä»¶æ˜¯ä¿®æ”¹ä½ æ•´ä¸ªä¸»é¢˜çš„é…ç½®æ–‡ä»¶ã€‚</p><h2><b>menuï¼ˆèœå•æ ï¼‰</b></h2><p>ä¹Ÿå°±æ˜¯ä¸Šé¢èœå•æ ä¸Šçš„è¿™äº›ä¸œè¥¿ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-f7759c435dcc46d877312b0b2c8b8e7d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"697\" data-rawheight=\"75\" class=\"origin_image zh-lightbox-thumb\" width=\"697\" data-original=\"https://pic2.zhimg.com/v2-f7759c435dcc46d877312b0b2c8b8e7d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;697&#39; height=&#39;75&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"697\" data-rawheight=\"75\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"697\" data-original=\"https://pic2.zhimg.com/v2-f7759c435dcc46d877312b0b2c8b8e7d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-f7759c435dcc46d877312b0b2c8b8e7d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>å…¶ä¸­ï¼ŒAboutè¿™ä¸ªä½ æ˜¯æ‰¾ä¸åˆ°ç½‘é¡µçš„ï¼Œå› ä¸ºä½ çš„æ–‡ç« ä¸­æ²¡æœ‰aboutè¿™ä¸ªä¸œè¥¿ã€‚å¦‚æœä½ æƒ³è¦çš„è¯ï¼Œå¯ä»¥æ‰§è¡Œå‘½ä»¤</p><div class=\"highlight\"><pre><code class=\"language-text\">hexo new page about</code></pre></div><p>å®ƒå°±ä¼šåœ¨æ ¹ç›®å½•ä¸‹<code>source</code>æ–‡ä»¶å¤¹ä¸­æ–°å»ºäº†ä¸€ä¸ª<code>about</code>æ–‡ä»¶å¤¹ï¼Œä»¥åŠindex.mdï¼Œåœ¨index.mdä¸­å†™ä¸Šä½ æƒ³è¦å†™çš„ä¸œè¥¿ï¼Œå°±å¯ä»¥åœ¨ç½‘ç«™ä¸Šå±•ç¤ºå‡ºæ¥äº†ã€‚</p><p>å¦‚æœä½ æƒ³è¦è‡ªå·±å†è‡ªå®šä¹‰ä¸€ä¸ªèœå•æ çš„é€‰é¡¹ï¼Œé‚£ä¹ˆå°±</p><div class=\"highlight\"><pre><code class=\"language-text\">hexo new page yourdiy</code></pre></div><p>ç„¶ååœ¨ä¸»é¢˜é…ç½®æ–‡ä»¶çš„menuèœå•æ æ·»åŠ ä¸€ä¸ª <code>Yourdiy : /yourdiy</code>ï¼Œæ³¨æ„å†’å·åé¢è¦æœ‰ç©ºæ ¼ï¼Œä»¥åŠå‰é¢çš„ç©ºæ ¼è¦å’Œmenuä¸­é»˜è®¤çš„ä¿æŒæ•´é½ã€‚ç„¶ååœ¨<code>languages</code>æ–‡ä»¶å¤¹ä¸­ï¼Œæ‰¾åˆ°<code>zh-CN.yml</code>ï¼Œåœ¨indexä¸­æ·»åŠ <code>yourdiy: &#39;ä¸­æ–‡æ„æ€&#39;</code>å°±å¯ä»¥æ˜¾ç¤ºä¸­æ–‡äº†ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>customize(å®šåˆ¶)</b></h2><p>åœ¨è¿™é‡Œå¯ä»¥ä¿®æ”¹ä½ çš„ä¸ªäººlogoï¼Œé»˜è®¤æ˜¯é‚£ä¸ªhuemanï¼Œåœ¨<code>source/css/images</code>æ–‡ä»¶å¤¹ä¸­æ”¾å…¥è‡ªå·±è¦çš„logoï¼Œå†æ”¹ä¸€ä¸‹<code>url</code>çš„é“¾æ¥åå­—å°±å¯ä»¥äº†ã€‚</p><p><code>favicon</code>æ˜¯ç½‘ç«™ä¸­å‡ºç°çš„é‚£ä¸ªå°å›¾æ ‡çš„iconï¼Œæ‰¾ä¸€å¼ ä½ å–œæ¬¢çš„logoï¼Œç„¶åè½¬æ¢æˆicoæ ¼å¼ï¼Œæ”¾åœ¨imagesæ–‡ä»¶å¤¹ä¸‹ï¼Œé…ç½®ä¸€ä¸‹è·¯å¾„å°±è¡Œã€‚</p><p><code>social_links</code> ï¼Œå¯ä»¥æ˜¾ç¤ºä½ çš„ç¤¾äº¤é“¾æ¥ï¼Œè€Œä¸”æ˜¯æœ‰logoçš„ã€‚</p><p><b>tips:</b></p><p>åœ¨è¿™é‡Œå¯ä»¥æ·»åŠ ä¸€ä¸ªrssåŠŸèƒ½ï¼Œä¹Ÿå°±æ˜¯é‚£ä¸ªç¬¦å·åƒwifiä¸€æ ·çš„ä¸œè¥¿ã€‚</p><h2><b>æ·»åŠ RSS</b></h2><p><b>1. ä»€ä¹ˆæ˜¯RSSï¼Ÿ</b></p><p>RSSä¹Ÿå°±æ˜¯è®¢é˜…åŠŸèƒ½ï¼Œä½ å¯ä»¥ç†è§£ä¸ºç±»ä¼¼ä¸è®¢é˜…å…¬ä¼—å·çš„åŠŸèƒ½ï¼Œæ¥è®¢é˜…å„ç§åšå®¢ï¼Œæ‚å¿—ç­‰ç­‰ã€‚</p><p><b>2. ä¸ºä»€ä¹ˆè¦ç”¨RSSï¼Ÿ</b></p><p>å°±å¦‚åŒè®¢é˜…å…¬ä¼—å·ä¸€æ ·ï¼Œä½ å¯¹æŸä¸ªå…¬ä¼—å·æ„Ÿå…´è¶£ï¼Œä½ æ€»ä¸å¯èƒ½ä¸€ç›´æ—¶ä¸æ—¶æœç´¢è¿™ä¸ªå…¬ä¼—å·æ¥çœ‹å®ƒçš„æ–‡ç« å§ã€‚åšå®¢ä¹Ÿæ˜¯ä¸€æ ·ï¼Œå¦‚æœä½ å–œæ¬¢æŸä¸ªåšä¸»ï¼Œæˆ–è€…æŸä¸ªå¹³å°çš„å†…å®¹ï¼Œä½ å¯ä»¥é€šè¿‡RSSè®¢é˜…å®ƒä»¬ï¼Œç„¶ååœ¨RSSé˜…è¯»å™¨ä¸Šå¯ä»¥å®æ—¶æ¨é€è¿™äº›æ¶ˆæ¯ã€‚ç°åœ¨ç½‘ä¸Šçš„åƒåœ¾æ¶ˆæ¯å¤ªå¤šäº†ï¼Œå¦‚æœä½ æ¯ä¸€å¤©éƒ½åœ¨çœ‹è¿™äº›æ¶ˆæ¯ä¸­åº¦è¿‡ï¼Œæ¼«æ— ç›®çš„çš„æµè§ˆï¼Œåªä¼šè®©ä½ çš„æ—¶é—´ä¸€ç‚¹ä¸€ç‚¹çš„æµé€ï¼Œå¤ªä¸å€¼å¾—äº†ã€‚å¦‚æœä½ å…³æ³¨çš„åšä¸»æ¯æ¬¡éƒ½å‘çš„æ¶ˆæ¯éƒ½æ˜¯ç²¾åï¼Œè€Œä¸”ä¸æ˜¯æ¯ä¸€å¤©åå‡ æ¡å‡ åæ¡çš„è½°ç‚¸ä½ ï¼Œé‚£ä¹ˆè¿™ä¸ªåšä¸»å°±å€¼å¾—ä½ çš„å…³æ³¨ï¼Œä½ å°±å¯ä»¥é€šè¿‡RSSè®¢é˜…ä»–ã€‚</p><p>åœ¨æˆ‘çš„ç†è§£ä¸­ï¼Œå¦‚æœä½ ä¸æƒ³æ¯å¤©éƒ½è¢«é‚£äº›æ²¡æœ‰è´¨é‡çš„æ¶ˆæ¯è½°ç‚¸ï¼Œåªæƒ³å®‰å®‰é™é™çš„å…³æ³¨å‡ ä¸ªåšä¸»ï¼Œæ¯å¤©çœ‹ä¸€äº›æœ‰è´¨é‡çš„å†…å®¹ä¹Ÿä¸ç”¨å¤ªå¤šï¼Œé‚£ä¹ˆRSSè®¢é˜…å€¼å¾—ä½ çš„æ‹¥æœ‰ã€‚</p><p><b>3. æ·»åŠ RSSåŠŸèƒ½</b></p><p>å…ˆå®‰è£…RSSæ’ä»¶</p><div class=\"highlight\"><pre><code class=\"language-text\">npm i hexo-generator-feed</code></pre></div><p>è€Œååœ¨ä½ æ•´ä¸ªé¡¹ç›®çš„<code>_config.yml</code>ä¸­æ‰¾åˆ°Extensionsï¼Œæ·»åŠ ï¼š</p><div class=\"highlight\"><pre><code class=\"language-text\"># Extensions\n## Plugins: https://hexo.io/plugins/\n#RSSè®¢é˜…\nplugin:\n- hexo-generator-feed\n#Feed Atom\nfeed:\n  type: atom\n  path: atom.xml\n  limit: 20</code></pre></div><p>è¿™ä¸ªæ—¶å€™ä½ çš„RSSé“¾æ¥å°±æ˜¯  åŸŸå<code>/atom.xml</code>äº†ã€‚</p><p>æ‰€ä»¥ï¼Œåœ¨ä¸»é¢˜é…ç½®æ–‡ä»¶ä¸­çš„è¿™ä¸ª<code>social links</code>ï¼Œå¼€å¯RSSçš„é¡µé¢åŠŸèƒ½ï¼Œè¿™æ ·ä½ ç½‘ç«™ä¸Šå°±æœ‰é‚£ä¸ªåƒwifiä¸€æ ·ç¬¦å·çš„RSS logoäº†ï¼Œæ³¨æ„ç©ºæ ¼ã€‚</p><div class=\"highlight\"><pre><code class=\"language-text\">rss: /atom.xml\n</code></pre></div><p><b>4. å¦‚ä½•å…³æ³¨RSSï¼Ÿ</b></p><p>é¦–å…ˆï¼Œä½ éœ€è¦ä¸€ä¸ªRSSé˜…è¯»å™¨ï¼Œåœ¨è¿™é‡Œæˆ‘æ¨è<code>inoreader</code>ï¼Œå®‡å®™ç¬¬ä¸€RSSé˜…è¯»å™¨ï¼Œè€Œä¸”ä¸­æ–‡æ”¯æŒçš„æŒºå¥½ã€‚ä¸è¿‡å®ƒæ²¡æœ‰PCç«¯çš„ç¨‹åºï¼Œåªæœ‰ç½‘é¡µç‰ˆï¼Œchromeä¸Šæœ‰æ’ä»¶ã€‚åœ¨å®˜ç½‘ä¸Šç”¨googleè´¦å·æˆ–è€…è‡ªå·±æ³¨å†Œè´¦å·ç™»å½•ï¼Œå°±å¯ä»¥å¼€å§‹ä½ çš„å…³æ³¨ä¹‹æ—…äº†ã€‚</p><p>æ¯æ¬¡éœ€è¦å…³æ³¨æŸä¸ªåšä¸»æ—¶ï¼Œå°±ç‚¹å¼€ä»–çš„RSSé“¾æ¥ï¼ŒæŠŠé“¾æ¥å¤åˆ¶åˆ°<code>inoreader</code>ä¸Šï¼Œå°±èƒ½å…³æ³¨äº†ï¼Œå½“ç„¶ï¼Œå¦‚æœæ˜¯æ¯”è¾ƒå¤§ä¼—åŒ–çš„å¾ˆå‰å®³çš„åšä¸»ï¼Œä½ ç›´æ¥æœåå­—ä¹Ÿå¯ä»¥çš„ï¼Œæ¯”å¦‚æ¯ä¸ªäººéƒ½éå¸¸ä½©æœçš„é˜®ä¸€å³°å¤§å¸ˆï¼Œç›´æ¥åœ¨é˜…è¯»å™¨ä¸Šæœç´¢<code>é˜®ä¸€å³°</code>ï¼Œåº”è¯¥å°±èƒ½å‡ºæ¥äº†ã€‚</p><p>æˆ‘å…³æ³¨çš„æ¯”å¦‚ï¼Œé˜®ä¸€å³°çš„ç½‘ç»œæ—¥å¿—ï¼Œæœˆå…‰åšå®¢ï¼ŒçŸ¥ä¹ç²¾é€‰ç­‰ï¼Œéƒ½å¾ˆä¸é”™ã€‚å½“ç„¶ï¼Œè¿˜æœ‰æˆ‘ï¼ï¼èµ¶å¿«å…³æ³¨æˆ‘å§ï¼ä½ å€¼å¾—æ‹¥æœ‰ï¼š<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/atom.xml\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">fangzh.top/atom.xml</span><span class=\"invisible\"></span></a></p><p>åœ¨å®‰å“ç«¯ï¼Œinoreaderä¹Ÿæœ‰ä¸‹è½½ï¼Œä¸è¿‡å› ä¸ºå›½å†…googleæ˜¯ç™»å½•ä¸äº†çš„ï¼Œä½ éœ€è¦åœ¨inoreaderå®˜ç½‘ä¸ŠæŠŠä½ çš„å¯†ç ä¿®æ”¹äº†ï¼Œç„¶åå°±å¯ä»¥ç”¨è´¦æˆ·åå’Œå¯†ç ç™»å½•äº†ã€‚</p><p>åœ¨IOSç«¯ï¼Œæ²¡ç”¨è¿‡ï¼Œå¥½åƒæ˜¯reader 3å¯ä»¥æ”¯æŒinoreaderè´¦æˆ·ï¼Œè¿˜æœ‰ä¸ªreadonä¹Ÿä¸é”™ï¼Œå¯ä»¥å»è¯•è¯•ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>widgets(ä¾§è¾¹æ )</b></h2><p>ä¾§è¾¹æ çš„å°æ ‡ç­¾ï¼Œå¦‚æœä½ æƒ³è‡ªå·±å¢åŠ ä¸€ä¸ªï¼Œæ¯”å¦‚æˆ‘å¢åŠ äº†ä¸€ä¸ªè”ç³»æ–¹å¼ï¼Œé‚£ä¹ˆæˆ‘æŠŠ<code>communication</code>å†™åœ¨ä¸Šé¢ï¼Œåœ¨<code>zh-CN.yml</code>ä¸­çš„<code>sidebar</code>ï¼Œæ·»åŠ <code>communication: &#39;ä¸­æ–‡&#39;</code>ã€‚</p><p>ç„¶ååœ¨<code>hueman/layout/widget</code>ä¸­æ·»åŠ ä¸€ä¸ª<code>communicaiton.ejs</code>ï¼Œå¡«å…¥æ¨¡æ¿ï¼š</p><div class=\"highlight\"><pre><code class=\"language-js\"><span class=\"o\">&lt;%</span> <span class=\"k\">if</span> <span class=\"p\">(</span><span class=\"nx\">site</span><span class=\"p\">.</span><span class=\"nx\">posts</span><span class=\"p\">.</span><span class=\"nx\">length</span><span class=\"p\">)</span> <span class=\"p\">{</span> <span class=\"o\">%&gt;</span>\n    <span class=\"o\">&lt;</span><span class=\"nx\">div</span> <span class=\"k\">class</span><span class=\"o\">=</span><span class=\"s2\">&#34;widget-wrap widget-list&#34;</span><span class=\"o\">&gt;</span>\n        <span class=\"o\">&lt;</span><span class=\"nx\">h3</span> <span class=\"k\">class</span><span class=\"o\">=</span><span class=\"s2\">&#34;widget-title&#34;</span><span class=\"o\">&gt;&lt;%=</span> <span class=\"nx\">__</span><span class=\"p\">(</span><span class=\"s1\">&#39;sidebar.communiation&#39;</span><span class=\"p\">)</span> <span class=\"o\">%&gt;&lt;</span><span class=\"err\">/h3&gt;</span>\n        <span class=\"o\">&lt;</span><span class=\"nx\">div</span> <span class=\"k\">class</span><span class=\"o\">=</span><span class=\"s2\">&#34;widget&#34;</span><span class=\"o\">&gt;</span>\n            <span class=\"c\">&lt;!--</span><span class=\"nx\">è¿™é‡Œæ·»åŠ ä½ è¦å†™çš„å†…å®¹</span><span class=\"o\">--&gt;</span>\n        <span class=\"o\">&lt;</span><span class=\"err\">/div&gt;</span>\n    <span class=\"o\">&lt;</span><span class=\"err\">/div&gt;</span>\n<span class=\"o\">&lt;%</span> <span class=\"p\">}</span> <span class=\"o\">%&gt;</span>\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>search(æœç´¢æ¡†)</b></h2><p>é»˜è®¤æœç´¢æ¡†æ˜¯ä¸èƒ½å¤Ÿç”¨çš„ï¼Œ</p><blockquote>you need to install <code>hexo-generator-json-content</code> before using Insight Search</blockquote><p>å®ƒå·²ç»å‘Šè¯‰ä½ äº†ï¼Œå¦‚æœæƒ³è¦ä½¿ç”¨ï¼Œå°±å®‰è£…è¿™ä¸ªæ’ä»¶ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>comment(è¯„è®ºç³»ç»Ÿ)</b></h2><p>è¿™é‡Œçš„å¤šæ•°éƒ½æ˜¯å›½å¤–çš„ï¼ŒåŸºæœ¬ç”¨ä¸äº†ã€‚è¿™ä¸ª<code>valine</code>å¥½åƒä¸é”™ï¼Œè¿˜èƒ½ç»Ÿè®¡æ–‡ç« é˜…è¯»é‡ï¼Œå¯ä»¥è‡ªå·±è¯•ä¸€è¯•ï¼Œ<a href=\"https://link.zhihu.com/?target=https%3A//valine.js.org/quickstart.html%23npm\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">é“¾æ¥</a>ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>miscellaneous(å…¶ä»–)</b></h2><p>è¿™é‡Œæˆ‘å°±æ”¹äº†ä¸€ä¸ª<code>links</code>ï¼Œå¯ä»¥æ·»åŠ å‹é“¾ã€‚æ³¨æ„ç©ºæ ¼è¦å¯¹ï¼ä¸ç„¶ä¼šæŠ¥é”™ï¼</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>æ€»ç»“ï¼š</b></h2><p>æ•´ä¸ªä¸»é¢˜çœ‹èµ·æ¥å¥½åƒå¾ˆå¤æ‚çš„æ ·å­ï¼Œä½†æ˜¯ä»”ç»†æ‹ä¸€æ‹å…¶å®ä¹Ÿæ¯”è¾ƒæµç•…ï¼Œ</p><ul><li>languages: é¡¾åæ€ä¹‰</li><li>layoutï¼šå¸ƒå±€æ–‡ä»¶ï¼Œå…¶å®åæœŸæƒ³è¦ä¿®æ”¹è‡ªå®šä¹‰ç½‘ç«™ä¸Šçš„ä¸œè¥¿ï¼Œæ·»åŠ å„ç§å„æ ·çš„ä¿¡æ¯ï¼Œä¸»è¦æ˜¯åœ¨è¿™é‡Œä¿®æ”¹ï¼Œå…¶ä¸­<code>comment</code>æ˜¯è¯„è®ºç³»ç»Ÿï¼Œ<code>common</code>æ˜¯å¸¸è§„çš„å¸ƒå±€ï¼Œæœ€å¸¸ä¿®æ”¹çš„åœ¨è¿™é‡Œé¢ï¼Œæ¯”å¦‚ä¿®æ”¹é¡µé¢<code>head</code>å’Œ<code>footer</code>çš„å†…å®¹ã€‚</li><li>scriptsï¼šjsè„šæœ¬ï¼Œæš‚æ—¶æ²¡ä»€ä¹ˆç”¨</li><li>sourceï¼šé‡Œé¢æ”¾äº†ä¸€äº›cssçš„æ ·å¼ï¼Œä»¥åŠå›¾ç‰‡</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>3. gitåˆ†æ”¯è¿›è¡Œå¤šç»ˆç«¯å·¥ä½œ</b></h2><p>é—®é¢˜æ¥äº†ï¼Œå¦‚æœä½ ç°åœ¨åœ¨è‡ªå·±çš„ç¬”è®°æœ¬ä¸Šå†™çš„åšå®¢ï¼Œéƒ¨ç½²åœ¨äº†ç½‘ç«™ä¸Šï¼Œé‚£ä¹ˆä½ åœ¨å®¶é‡Œç”¨å°å¼æœºï¼Œæˆ–è€…å®éªŒå®¤çš„å°å¼æœºï¼Œå‘ç°ä½ ç”µè„‘é‡Œé¢æ²¡æœ‰åšå®¢çš„æ–‡ä»¶ï¼Œæˆ–è€…è¦æ¢ç”µè„‘äº†ï¼Œæœ€åä¸çŸ¥é“æ€ä¹ˆç§»åŠ¨æ–‡ä»¶ï¼Œæ€ä¹ˆåŠï¼Ÿ</p><p>åœ¨è¿™é‡Œæˆ‘ä»¬å°±å¯ä»¥åˆ©ç”¨gitçš„åˆ†æ”¯ç³»ç»Ÿè¿›è¡Œå¤šç»ˆç«¯å·¥ä½œäº†ï¼Œè¿™æ ·æ¯æ¬¡æ‰“å¼€ä¸ä¸€æ ·çš„ç”µè„‘ï¼Œåªéœ€è¦è¿›è¡Œç®€å•çš„é…ç½®å’Œåœ¨githubä¸ŠæŠŠæ–‡ä»¶åŒæ­¥ä¸‹æ¥ï¼Œå°±å¯ä»¥æ— ç¼æ“ä½œäº†ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>æœºåˆ¶</b></h2><p>æœºåˆ¶æ˜¯è¿™æ ·çš„ï¼Œç”±äº<code>hexo d</code>ä¸Šä¼ éƒ¨ç½²åˆ°githubçš„å…¶å®æ˜¯hexoç¼–è¯‘åçš„æ–‡ä»¶ï¼Œæ˜¯ç”¨æ¥ç”Ÿæˆç½‘é¡µçš„ï¼Œä¸åŒ…å«æºæ–‡ä»¶ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-32ab30d0d28b916a204c03de4ed0fc4f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"288\" data-rawheight=\"475\" class=\"content_image\" width=\"288\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;288&#39; height=&#39;475&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"288\" data-rawheight=\"475\" class=\"content_image lazy\" width=\"288\" data-actualsrc=\"https://pic4.zhimg.com/v2-32ab30d0d28b916a204c03de4ed0fc4f_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>ä¹Ÿå°±æ˜¯ä¸Šä¼ çš„æ˜¯åœ¨æœ¬åœ°ç›®å½•é‡Œè‡ªåŠ¨ç”Ÿæˆçš„<code>.deploy_git</code>é‡Œé¢ã€‚</p><p>å…¶ä»–æ–‡ä»¶ ï¼ŒåŒ…æ‹¬æˆ‘ä»¬å†™åœ¨source é‡Œé¢çš„ï¼Œå’Œé…ç½®æ–‡ä»¶ï¼Œä¸»é¢˜æ–‡ä»¶ï¼Œéƒ½æ²¡æœ‰ä¸Šä¼ åˆ°github</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-59bb330178a4e010d9818911f789082b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"515\" data-rawheight=\"280\" class=\"origin_image zh-lightbox-thumb\" width=\"515\" data-original=\"https://pic4.zhimg.com/v2-59bb330178a4e010d9818911f789082b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;515&#39; height=&#39;280&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"515\" data-rawheight=\"280\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"515\" data-original=\"https://pic4.zhimg.com/v2-59bb330178a4e010d9818911f789082b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-59bb330178a4e010d9818911f789082b_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>æ‰€ä»¥å¯ä»¥åˆ©ç”¨gitçš„åˆ†æ”¯ç®¡ç†ï¼Œå°†æºæ–‡ä»¶ä¸Šä¼ åˆ°githubçš„å¦ä¸€ä¸ªåˆ†æ”¯å³å¯ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>ä¸Šä¼ åˆ†æ”¯</b></h2><p>é¦–å…ˆï¼Œå…ˆåœ¨githubä¸Šæ–°å»ºä¸€ä¸ªhexoåˆ†æ”¯ï¼Œå¦‚å›¾ï¼š</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ebb3e05632e85ab036663390305caa1c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"457\" data-rawheight=\"412\" class=\"origin_image zh-lightbox-thumb\" width=\"457\" data-original=\"https://pic1.zhimg.com/v2-ebb3e05632e85ab036663390305caa1c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;457&#39; height=&#39;412&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"457\" data-rawheight=\"412\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"457\" data-original=\"https://pic1.zhimg.com/v2-ebb3e05632e85ab036663390305caa1c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-ebb3e05632e85ab036663390305caa1c_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>ç„¶ååœ¨è¿™ä¸ªä»“åº“çš„settingsä¸­ï¼Œé€‰æ‹©é»˜è®¤åˆ†æ”¯ä¸ºhexoåˆ†æ”¯ï¼ˆè¿™æ ·æ¯æ¬¡åŒæ­¥çš„æ—¶å€™å°±ä¸ç”¨æŒ‡å®šåˆ†æ”¯ï¼Œæ¯”è¾ƒæ–¹ä¾¿ï¼‰ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-1899b6219f3787832652813b958b9b3d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"957\" data-rawheight=\"496\" class=\"origin_image zh-lightbox-thumb\" width=\"957\" data-original=\"https://pic2.zhimg.com/v2-1899b6219f3787832652813b958b9b3d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;957&#39; height=&#39;496&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"957\" data-rawheight=\"496\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"957\" data-original=\"https://pic2.zhimg.com/v2-1899b6219f3787832652813b958b9b3d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-1899b6219f3787832652813b958b9b3d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>ç„¶ååœ¨æœ¬åœ°çš„ä»»æ„ç›®å½•ä¸‹ï¼Œæ‰“å¼€git bashï¼Œ</p><div class=\"highlight\"><pre><code class=\"language-text\">git clone git@github.com:ZJUFangzh/ZJUFangzh.github.io.git</code></pre></div><p>å°†å…¶å…‹éš†åˆ°æœ¬åœ°ï¼Œå› ä¸ºé»˜è®¤åˆ†æ”¯å·²ç»è®¾æˆäº†hexoï¼Œæ‰€ä»¥cloneæ—¶åªcloneäº†hexoã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><p>æ¥ä¸‹æ¥åœ¨å…‹éš†åˆ°æœ¬åœ°çš„<code>ZJUFangzh.github.io</code>ä¸­ï¼ŒæŠŠé™¤äº†.git æ–‡ä»¶å¤¹å¤–çš„æ‰€æœ‰æ–‡ä»¶éƒ½åˆ æ‰</p><p> æŠŠä¹‹å‰æˆ‘ä»¬å†™çš„åšå®¢æºæ–‡ä»¶å…¨éƒ¨å¤åˆ¶è¿‡æ¥ï¼Œé™¤äº†<code>.deploy_git</code>ã€‚è¿™é‡Œåº”è¯¥è¯´ä¸€å¥ï¼Œå¤åˆ¶è¿‡æ¥çš„æºæ–‡ä»¶åº”è¯¥æœ‰ä¸€ä¸ª<code>.gitignore</code>ï¼Œç”¨æ¥å¿½ç•¥ä¸€äº›ä¸éœ€è¦çš„æ–‡ä»¶ï¼Œå¦‚æœæ²¡æœ‰çš„è¯ï¼Œè‡ªå·±æ–°å»ºä¸€ä¸ªï¼Œåœ¨é‡Œé¢å†™ä¸Šå¦‚ä¸‹ï¼Œè¡¨ç¤ºè¿™äº›ç±»å‹æ–‡ä»¶ä¸éœ€è¦gitï¼š</p><div class=\"highlight\"><pre><code class=\"language-text\">.DS_Store\nThumbs.db\ndb.json\n*.log\nnode_modules/\npublic/\n.deploy*/\n</code></pre></div><p>æ³¨æ„ï¼Œå¦‚æœä½ ä¹‹å‰å…‹éš†è¿‡themeä¸­çš„ä¸»é¢˜æ–‡ä»¶ï¼Œé‚£ä¹ˆåº”è¯¥æŠŠä¸»é¢˜æ–‡ä»¶ä¸­çš„<code>.git</code>æ–‡ä»¶å¤¹åˆ æ‰ï¼Œå› ä¸ºgitä¸èƒ½åµŒå¥—ä¸Šä¼ ï¼Œæœ€å¥½æ˜¯æ˜¾ç¤ºéšè—æ–‡ä»¶ï¼Œæ£€æŸ¥ä¸€ä¸‹æœ‰æ²¡æœ‰ï¼Œå¦åˆ™ä¸Šä¼ çš„æ—¶å€™ä¼šå‡ºé”™ï¼Œå¯¼è‡´ä½ çš„ä¸»é¢˜æ–‡ä»¶æ— æ³•ä¸Šä¼ ï¼Œè¿™æ ·ä½ çš„é…ç½®åœ¨åˆ«çš„ç”µè„‘ä¸Šå°±ç”¨ä¸äº†äº†ã€‚</p><p>è€Œå</p><div class=\"highlight\"><pre><code class=\"language-text\">git add .\ngit commit â€“m &#34;add branch&#34;\ngit push </code></pre></div><p>è¿™æ ·å°±ä¸Šä¼ å®Œäº†ï¼Œå¯ä»¥å»ä½ çš„githubä¸Šçœ‹ä¸€çœ‹hexoåˆ†æ”¯æœ‰æ²¡æœ‰ä¸Šä¼ ä¸Šå»ï¼Œå…¶ä¸­<code>node_modules</code>ã€<code>public</code>ã€<code>db.json</code>å·²ç»è¢«å¿½ç•¥æ‰äº†ï¼Œæ²¡æœ‰å…³ç³»ï¼Œä¸éœ€è¦ä¸Šä¼ çš„ï¼Œå› ä¸ºåœ¨åˆ«çš„ç”µè„‘ä¸Šéœ€è¦é‡æ–°è¾“å…¥å‘½ä»¤å®‰è£… ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-a94330ca825f4debde8ce7ceeb8f8394_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1250\" data-rawheight=\"621\" class=\"origin_image zh-lightbox-thumb\" width=\"1250\" data-original=\"https://pic1.zhimg.com/v2-a94330ca825f4debde8ce7ceeb8f8394_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1250&#39; height=&#39;621&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1250\" data-rawheight=\"621\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1250\" data-original=\"https://pic1.zhimg.com/v2-a94330ca825f4debde8ce7ceeb8f8394_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-a94330ca825f4debde8ce7ceeb8f8394_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>è¿™æ ·å°±ä¸Šä¼ å®Œäº†ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>æ›´æ¢ç”µè„‘æ“ä½œ</b></h2><p>ä¸€æ ·çš„ï¼Œè·Ÿä¹‹å‰çš„ç¯å¢ƒæ­å»ºä¸€æ ·ï¼Œ</p><ul><li>å®‰è£…git</li></ul><div class=\"highlight\"><pre><code class=\"language-text\">sudo apt-get install git\n</code></pre></div><ul><li>è®¾ç½®gitå…¨å±€é‚®ç®±å’Œç”¨æˆ·å</li></ul><div class=\"highlight\"><pre><code class=\"language-text\">git config --global user.name &#34;yourgithubname&#34;\ngit config --global user.email &#34;yourgithubemail&#34;\n</code></pre></div><ul><li>è®¾ç½®ssh key</li></ul><div class=\"highlight\"><pre><code class=\"language-text\">ssh-keygen -t rsa -C &#34;youremail&#34;\n#ç”Ÿæˆåå¡«åˆ°githubå’Œcodingä¸Šï¼ˆæœ‰codingå¹³å°çš„è¯ï¼‰\n#éªŒè¯æ˜¯å¦æˆåŠŸ\nssh -T git@github.com\nssh -T git@git.coding.net #(æœ‰codingå¹³å°çš„è¯)\n</code></pre></div><ul><li>å®‰è£…nodejs</li></ul><div class=\"highlight\"><pre><code class=\"language-text\">sudo apt-get install nodejs\nsudo apt-get install npm\n</code></pre></div><ul><li>å®‰è£…hexo  </li></ul><div class=\"highlight\"><pre><code class=\"language-text\">sudo npm install hexo-cli -g\n</code></pre></div><p>ä½†æ˜¯å·²ç»ä¸éœ€è¦åˆå§‹åŒ–äº†ï¼Œ</p><p>ç›´æ¥åœ¨ä»»æ„æ–‡ä»¶å¤¹ä¸‹ï¼Œ</p><div class=\"highlight\"><pre><code class=\"language-text\">git clone git@â€¦â€¦â€¦â€¦â€¦â€¦\n</code></pre></div><p>ç„¶åè¿›å…¥å…‹éš†åˆ°çš„æ–‡ä»¶å¤¹ï¼š</p><div class=\"highlight\"><pre><code class=\"language-text\">cd xxx.github.io\nnpm install\nnpm install hexo-deployer-git --save\n</code></pre></div><p>ç”Ÿæˆï¼Œéƒ¨ç½²ï¼š</p><div class=\"highlight\"><pre><code class=\"language-text\">hexo g\nhexo d\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>ç„¶åå°±å¯ä»¥å¼€å§‹å†™ä½ çš„æ–°åšå®¢äº†</p><div class=\"highlight\"><pre><code class=\"language-text\">hexo new newpage\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><b>Tips:</b></p><ol><li>ä¸è¦å¿˜äº†ï¼Œæ¯æ¬¡å†™å®Œæœ€å¥½éƒ½æŠŠæºæ–‡ä»¶ä¸Šä¼ ä¸€ä¸‹</li></ol><div class=\"highlight\"><pre><code class=\"language-text\">git add .\ngit commit â€“m &#34;xxxx&#34;\ngit push \n</code></pre></div><ol><li>å¦‚æœæ˜¯åœ¨å·²ç»ç¼–è¾‘è¿‡çš„ç”µè„‘ä¸Šï¼Œå·²ç»æœ‰cloneæ–‡ä»¶å¤¹äº†ï¼Œé‚£ä¹ˆï¼Œæ¯æ¬¡åªè¦å’Œè¿œç«¯åŒæ­¥ä¸€ä¸‹å°±è¡Œäº†</li></ol><div class=\"highlight\"><pre><code class=\"language-text\">git pull\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>4. coding pageä¸Šéƒ¨ç½²å®ç°å›½å†…å¤–åˆ†æµ</b></h2><p>ä¹‹å‰æˆ‘ä»¬å·²ç»æŠŠhexoæ‰˜ç®¡åœ¨githubäº†ï¼Œä½†æ˜¯githubæ˜¯å›½å¤–çš„ï¼Œè€Œä¸”ç™¾åº¦çš„çˆ¬è™«æ˜¯ä¸èƒ½å¤Ÿçˆ¬å–githubçš„ï¼Œæ‰€ä»¥å¦‚æœä½ å¸Œæœ›ä½ åšçš„åšå®¢èƒ½å¤Ÿåœ¨ç™¾åº¦å¼•æ“ä¸Šè¢«æ”¶å½•ï¼Œè€Œä¸”æƒ³è¦æ›´å¿«çš„è®¿é—®ï¼Œé‚£ä¹ˆå¯ä»¥åœ¨å›½å†…çš„coding pageåšä¸€ä¸ªæ‰˜ç®¡ï¼Œè¿™æ ·åœ¨å›½å†…è®¿é—®å°±æ˜¯coding pageï¼Œå›½å¤–å°±èµ°github pageã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>1. ç”³è¯·codingè´¦æˆ·ï¼Œæ–°å»ºé¡¹ç›®</b></p><p>å…ˆç”³è¯·ä¸€ä¸ªè´¦æˆ·ï¼Œç„¶ååˆ›å»ºæ–°çš„é¡¹ç›®ï¼Œè¿™ä¸€æ­¥é¡¹ç›®åç§°åº”è¯¥æ˜¯éšæ„çš„ã€‚</p><p><b>2.  æ·»åŠ ssh key</b></p><p>è¿™ä¸€æ­¥è·Ÿgithubä¸€æ ·ã€‚</p><p>æ·»åŠ åï¼Œæ£€æŸ¥ä¸€ä¸‹æ˜¯ä¸æ˜¯æ·»åŠ æˆåŠŸ</p><div class=\"highlight\"><pre><code class=\"language-text\">ssh -T git@git.coding.net\n</code></pre></div><p><b>3. ä¿®æ”¹_config.yml</b></p><p>hexoå®˜æ–¹æ–‡æ¡£æ˜¯è¿™æ ·çš„ï¼š</p><div class=\"highlight\"><pre><code class=\"language-text\">deploy:\n  type: git\n  message: [message]\n  repo:\n    github: &lt;repository url&gt;,[branch]\n    coding: &lt;repository url&gt;,[branch] \n</code></pre></div><p>é‚£ä¹ˆï¼Œæˆ‘ä»¬åªéœ€è¦ï¼š</p><div class=\"highlight\"><pre><code class=\"language-text\">deploy:\n  type: git\n  repo: \n    coding: git@git.coding.net:ZJUFangzh/ZJUFangzh.git,master\n    github: git@github.com:ZJUFangzh/ZJUFangzh.github.io.git,master\n</code></pre></div><p><b>4. éƒ¨ç½²</b></p><p>ä¿å­˜ä¸€ä¸‹ï¼Œç›´æ¥</p><div class=\"highlight\"><pre><code class=\"language-text\">hexo g\nhexo d\n</code></pre></div><p>è¿™æ ·å°±å¯ä»¥åœ¨codingçš„é¡¹ç›®ä¸Šçœ‹åˆ°ä½ éƒ¨ç½²çš„æ–‡ä»¶äº†ã€‚</p><p><b>5. å¼€å¯coding pagesæœåŠ¡ï¼Œç»‘å®šåŸŸå</b></p><p>å¦‚å›¾ï¼š</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-1136f014eae5c088808a265ded2c6845_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1215\" data-rawheight=\"426\" class=\"origin_image zh-lightbox-thumb\" width=\"1215\" data-original=\"https://pic2.zhimg.com/v2-1136f014eae5c088808a265ded2c6845_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1215&#39; height=&#39;426&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1215\" data-rawheight=\"426\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1215\" data-original=\"https://pic2.zhimg.com/v2-1136f014eae5c088808a265ded2c6845_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-1136f014eae5c088808a265ded2c6845_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>6. é˜¿é‡Œäº‘æ·»åŠ è§£æ</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-9a9ffbf08b2ec0917027ce2f3d14cfaf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"871\" data-rawheight=\"335\" class=\"origin_image zh-lightbox-thumb\" width=\"871\" data-original=\"https://pic4.zhimg.com/v2-9a9ffbf08b2ec0917027ce2f3d14cfaf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;871&#39; height=&#39;335&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"871\" data-rawheight=\"335\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"871\" data-original=\"https://pic4.zhimg.com/v2-9a9ffbf08b2ec0917027ce2f3d14cfaf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-9a9ffbf08b2ec0917027ce2f3d14cfaf_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>è¿™ä¸ªæ—¶å€™å°±å¯ä»¥æŠŠä¹‹å‰githubçš„è§£ææ”¹æˆå¢ƒå¤–ï¼ŒæŠŠcodingçš„è§£æè®¾ä¸ºé»˜è®¤äº†ã€‚</p><p><b>7. å»é™¤coding pageçš„è·³è½¬å¹¿å‘Š</b></p><p>coding pageçš„ä¸€ä¸ªæ¯”è¾ƒæ¶å¿ƒäººçš„åœ°æ–¹å°±æ˜¯ï¼Œä½ åªæ˜¯é“¶ç‰Œä¼šå‘˜çš„è¯ï¼Œè®¿é—®ä¼šå…ˆè·³è½¬åˆ°ä¸€ä¸ªå¹¿å‘Šï¼Œå†åˆ°ä½ è‡ªå·±çš„åŸŸåã€‚é‚£ä¹ˆå®ƒä¹Ÿç»™å‡ºäº†æ¶ˆé™¤çš„åŠæ³•ã€‚å³ä¸Šè§’åˆ‡æ¢åˆ°codingçš„æ—§ç‰ˆç•Œé¢ï¼Œé»˜è®¤æ–°ç‰ˆæ˜¯ä¸è¡Œçš„ã€‚ç„¶åå†æ¥åˆ°<code>pagesæœåŠ¡</code>è¿™é‡Œã€‚</p><p>è¿™é‡Œï¼š</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-2b9a0f546007202e28bdf917edd559df_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"692\" data-rawheight=\"443\" class=\"origin_image zh-lightbox-thumb\" width=\"692\" data-original=\"https://pic4.zhimg.com/v2-2b9a0f546007202e28bdf917edd559df_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;692&#39; height=&#39;443&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"692\" data-rawheight=\"443\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"692\" data-original=\"https://pic4.zhimg.com/v2-2b9a0f546007202e28bdf917edd559df_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-2b9a0f546007202e28bdf917edd559df_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>åªè¦ä½ åœ¨é¡µé¢ä¸Šæ·»åŠ ä¸€è¡Œæ–‡å­—ï¼Œå†™<code>Hosted by Coding Pages</code>ï¼Œç„¶åç‚¹ä¸‹é¢çš„å°å‹¾å‹¾ï¼Œä¸¤ä¸ªå·¥ä½œæ—¥å†…å®ƒå°±ä¼šå®¡æ ¸é€šè¿‡äº†ã€‚</p><div class=\"highlight\"><pre><code class=\"language-text\">&lt;p&gt;Hosted by &lt;a href=&#34;https://pages.coding.me&#34; style=&#34;font-weight: bold&#34;&gt;Coding Pages&lt;/a&gt;&lt;/p&gt;\n</code></pre></div><p>æˆ‘çš„é€‰æ‹©æ˜¯æŠŠè¿™ä¸€è¡Œä»£ç æ”¾åœ¨ä¸»é¢˜æ–‡ä»¶å¤¹<code>/layout/common/footer.ejs</code>é‡Œé¢ï¼Œä¹Ÿå°±æ˜¯æœ¬æ¥åœ¨é¡µé¢ä¸­çœ‹åˆ°çš„é¡µè„šéƒ¨åˆ†ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-fdc3b5fe205f9214a127464563395489_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"773\" data-rawheight=\"219\" class=\"origin_image zh-lightbox-thumb\" width=\"773\" data-original=\"https://pic2.zhimg.com/v2-fdc3b5fe205f9214a127464563395489_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;773&#39; height=&#39;219&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"773\" data-rawheight=\"219\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"773\" data-original=\"https://pic2.zhimg.com/v2-fdc3b5fe205f9214a127464563395489_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-fdc3b5fe205f9214a127464563395489_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>å½“ç„¶ï¼Œä¸ºäº†ç»Ÿä¸€ï¼Œæˆ‘åˆåœ¨åé¢åŠ ä¸Šäº†and <b>Github</b>å“ˆå“ˆï¼Œå¯ä»¥ä¸åŠ ã€‚</p><div class=\"highlight\"><pre><code class=\"language-text\">&lt;p&gt;&lt;span&gt;Hosted by &lt;a href=&#34;https://pages.coding.me&#34; style=&#34;font-weight: bold&#34;&gt;Coding Pages&lt;/a&gt;&lt;/span&gt; and &lt;span&gt;&lt;a href=&#34;https://github.com&#34; style=&#34;font-weight: bold&#34;&gt;Github&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;\n</code></pre></div><p>è¿™æ˜¯æœ€ç»ˆåŠ ä¸Šå»çš„ä»£ç ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>ç¬¬ä¸‰éƒ¨åˆ†</b></h2><p>hexoæ·»åŠ å„ç§åŠŸèƒ½ï¼ŒåŒ…æ‹¬æœç´¢çš„SEOï¼Œé˜…è¯»é‡ç»Ÿè®¡ï¼Œè®¿é—®é‡ç»Ÿè®¡å’Œè¯„è®ºç³»ç»Ÿç­‰ã€‚</p><p>è¿™ä¸€éƒ¨åˆ†å‚è€ƒäº†: <a href=\"https://link.zhihu.com/?target=http%3A//visugar.com/2017/08/01/20170801HexoPlugins/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">visugar.com</a>è¿™é‡Œé¢è¯´çš„å¾ˆè¯¦ç»†äº†ã€‚</p><h2><b>1. SEOä¼˜åŒ– </b></h2><p>æ¨å¹¿æ˜¯å¾ˆéº»çƒ¦çš„äº‹æƒ…ï¼Œæ€ä¹ˆæ ·åˆ«äººæ‰èƒ½çŸ¥é“æˆ‘ä»¬å‘¢ï¼Œé¦–å…ˆéœ€è¦è®©æœç´¢å¼•æ“æ”¶å½•ä½ çš„è¿™ä¸ªç½‘ç«™ï¼Œåˆ«äººæ‰èƒ½æœç´¢çš„åˆ°ã€‚é‚£ä¹ˆè¿™å°±éœ€è¦SEOä¼˜åŒ–äº†ã€‚</p><blockquote>SEOæ˜¯ç”±è‹±æ–‡Search Engine Optimizationç¼©å†™è€Œæ¥ï¼Œ ä¸­æ–‡æ„è¯‘ä¸ºâ€œæœç´¢å¼•æ“ä¼˜åŒ–â€ã€‚SEOæ˜¯æŒ‡é€šè¿‡ç«™å†…ä¼˜åŒ–æ¯”å¦‚ç½‘ç«™ç»“æ„è°ƒæ•´ã€ç½‘ç«™å†…å®¹å»ºè®¾ã€ç½‘ç«™ä»£ç ä¼˜åŒ–ç­‰ä»¥åŠç«™å¤–ä¼˜åŒ–ã€‚</blockquote><h2><b>ç™¾åº¦seo</b></h2><p>åˆšå»ºç«™çš„æ—¶å€™æ˜¯æ²¡æœ‰æœç´¢å¼•æ“æ”¶å½•æˆ‘ä»¬çš„ç½‘ç«™çš„ã€‚å¯ä»¥åœ¨æœç´¢å¼•æ“ä¸­è¾“å…¥<code>site:&lt;åŸŸå&gt;</code></p><p>æ¥æŸ¥çœ‹ä¸€ä¸‹ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>1. ç™»å½•ç™¾åº¦ç«™é•¿å¹³å°æ·»åŠ ç½‘ç«™</b></p><p>ç™»å½•<a href=\"https://link.zhihu.com/?target=https%3A//ziyuan.baidu.com/linksubmit/index%3F\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">ç™¾åº¦ç«™é•¿å¹³å°</a>ï¼Œåœ¨ç«™ç‚¹ç®¡ç†ä¸­æ·»åŠ ä½ è‡ªå·±çš„ç½‘ç«™ã€‚</p><p>éªŒè¯ç½‘ç«™æœ‰ä¸‰ç§æ–¹å¼ï¼šæ–‡ä»¶éªŒè¯ã€HTMLæ ‡ç­¾éªŒè¯ã€CNAMEéªŒè¯ã€‚</p><p>ç¬¬ä¸‰ç§æ–¹å¼æœ€ç®€å•ï¼Œåªè¦å°†å®ƒæä¾›ç»™ä½ çš„é‚£ä¸ªxxxxxä½¿ç”¨CNAMEè§£æåˆ°<a href=\"https://link.zhihu.com/?target=http%3A//xxx.baidu.com\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">xxx.baidu.com</span><span class=\"invisible\"></span></a>å°±å¯ä»¥äº†ã€‚ä¹Ÿå°±æ˜¯ç™»å½•ä½ çš„é˜¿é‡Œäº‘ï¼ŒæŠŠè¿™ä¸ªè§£æå¡«è¿›å»å°±OKäº†ã€‚</p><p><b>2. æäº¤é“¾æ¥</b></p><p>æˆ‘ä»¬éœ€è¦ä½¿ç”¨npmè‡ªåŠ¨ç”Ÿæˆç½‘ç«™çš„sitemapï¼Œç„¶åå°†ç”Ÿæˆçš„sitemapæäº¤åˆ°ç™¾åº¦å’Œå…¶ä»–æœç´¢å¼•æ“</p><div class=\"highlight\"><pre><code class=\"language-text\">npm install hexo-generator-sitemap --save     \nnpm install hexo-generator-baidu-sitemap --save\n</code></pre></div><p>è¿™æ—¶å€™ä½ éœ€è¦åœ¨ä½ çš„æ ¹ç›®å½•ä¸‹<code>_config.xml</code>ä¸­çœ‹çœ‹urlæœ‰æ²¡æœ‰æ”¹æˆä½ è‡ªå·±çš„ï¼š</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-c7be3d7d0eb3e838e24de8095ac86338_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"387\" data-rawheight=\"119\" class=\"content_image\" width=\"387\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;387&#39; height=&#39;119&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"387\" data-rawheight=\"119\" class=\"content_image lazy\" width=\"387\" data-actualsrc=\"https://pic1.zhimg.com/v2-c7be3d7d0eb3e838e24de8095ac86338_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>é‡æ–°éƒ¨ç½²åï¼Œå°±å¯ä»¥åœ¨publicæ–‡ä»¶å¤¹ä¸‹çœ‹åˆ°ç”Ÿæˆçš„sitemap.xmlå’Œbaidusitemap.xmläº†ã€‚</p><p>ç„¶åå°±å¯ä»¥å‘ç™¾åº¦æäº¤ä½ çš„ç«™ç‚¹åœ°å›¾äº†ã€‚</p><p>è¿™é‡Œå»ºè®®ä½¿ç”¨è‡ªåŠ¨æäº¤ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-466acfdc8359d94dc8064ae6d4d66a2d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"552\" data-rawheight=\"754\" class=\"origin_image zh-lightbox-thumb\" width=\"552\" data-original=\"https://pic2.zhimg.com/v2-466acfdc8359d94dc8064ae6d4d66a2d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;552&#39; height=&#39;754&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"552\" data-rawheight=\"754\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"552\" data-original=\"https://pic2.zhimg.com/v2-466acfdc8359d94dc8064ae6d4d66a2d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-466acfdc8359d94dc8064ae6d4d66a2d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>è‡ªåŠ¨æäº¤åˆåˆ†ä¸ºä¸‰ç§ï¼šä¸»åŠ¨æ¨é€ã€è‡ªåŠ¨æ¨é€ã€sitemapã€‚</p><p>å¯ä»¥ä¸‰ä¸ªä¸€èµ·æäº¤ä¸è¦ç´§ï¼Œæˆ‘é€‰æ‹©çš„æ˜¯åä¸¤ç§ã€‚</p><ul><li>è‡ªåŠ¨æ¨é€ï¼šæŠŠç™¾åº¦ç”Ÿæˆçš„è‡ªåŠ¨æ¨é€ä»£ç ï¼Œæ”¾åœ¨ä¸»é¢˜æ–‡ä»¶<code>/layout/common/head.ejs</code>çš„é€‚å½“ä½ç½®ï¼Œç„¶åéªŒè¯ä¸€ä¸‹å°±å¯ä»¥äº†ã€‚</li><li>sitemapï¼šæŠŠä¸¤ä¸ªsitemapåœ°å€ï¼Œæäº¤ä¸Šå»ï¼Œçœ‹åˆ°çŠ¶æ€æ­£å¸¸å°±OKäº†ã€‚</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ce4d363f858c25de4ada3f5bb02ecdf3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1122\" data-rawheight=\"796\" class=\"origin_image zh-lightbox-thumb\" width=\"1122\" data-original=\"https://pic4.zhimg.com/v2-ce4d363f858c25de4ada3f5bb02ecdf3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1122&#39; height=&#39;796&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1122\" data-rawheight=\"796\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1122\" data-original=\"https://pic4.zhimg.com/v2-ce4d363f858c25de4ada3f5bb02ecdf3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-ce4d363f858c25de4ada3f5bb02ecdf3_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>ps:</b> ç™¾åº¦æ”¶å½•æ¯”è¾ƒæ…¢ï¼Œæ…¢æ…¢ç­‰ä¸ªåå¤©åŠä¸ªæœˆå†å»<code>site:&lt;åŸŸå&gt;</code>çœ‹çœ‹æœ‰æ²¡æœ‰è¢«æ”¶å½•ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>googleçš„SEO</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p>æµç¨‹ä¸€æ ·ï¼Œgoogleæ›´ç®€å•ï¼Œè€Œä¸”æ”¶å½•æ›´å¿«ï¼Œè¿›å…¥<a href=\"https://link.zhihu.com/?target=https%3A//search.google.com/search-console/sitemaps%3Fresource_id%3Dhttp%3A//fangzh.top/%26hl%3Dzh-CN\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">googleç«™ç‚¹åœ°å›¾</a>ï¼Œæäº¤ç½‘ç«™å’Œsitemap.xmlï¼Œå°±å¯ä»¥äº†ã€‚</p><p>å¦‚æœä½ è¿™ä¸ªåŸŸååœ¨googleè¿™é‡Œå‡ºäº†é—®é¢˜ï¼Œé‚£ä½ å°±æäº¤ <a href=\"https://link.zhihu.com/?target=http%3A//yourname.github.io\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">yourname.github.io</span><span class=\"invisible\"></span></a>ï¼Œè¿™ä¸ªé“¾æ¥ï¼Œæ•ˆæœæ˜¯ä¸€æ ·çš„ã€‚</p><p>ä¸å‡ºæ„å¤–çš„è¯ä¸€å¤©å†…googleå°±èƒ½æ”¶å½•ä½ çš„ç½‘ç«™äº†ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-ef73c3e069f4b73417a6068c0d100471_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"939\" data-rawheight=\"776\" class=\"origin_image zh-lightbox-thumb\" width=\"939\" data-original=\"https://pic2.zhimg.com/v2-ef73c3e069f4b73417a6068c0d100471_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;939&#39; height=&#39;776&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"939\" data-rawheight=\"776\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"939\" data-original=\"https://pic2.zhimg.com/v2-ef73c3e069f4b73417a6068c0d100471_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-ef73c3e069f4b73417a6068c0d100471_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>å…¶ä»–çš„æœç´¢ï¼Œå¦‚æœç‹—æœç´¢ï¼Œ360æœç´¢ï¼Œæµç¨‹æ˜¯ä¸€æ ·çš„ï¼Œè¿™é‡Œå°±ä¸å†èµ˜è¿°ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>2. è¯„è®ºç³»ç»Ÿ</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p>è¯„è®ºç³»ç»Ÿæœ‰å¾ˆå¤šï¼Œä½†æ˜¯å¾ˆå¤šéƒ½æ˜¯å¢™å¤–çš„ç”¨ä¸äº†ï¼Œä¹‹å‰è¯´è¿‡è¿™ä¸ªvalineå¥½åƒé›†æˆåœ¨huemanå’Œnextä¸»é¢˜é‡Œé¢äº†ï¼Œä½†æ˜¯æˆ‘è¿˜æ²¡æœ‰ç ”ç©¶è¿‡ï¼Œæˆ‘çœ‹çš„æ˜¯<a href=\"https://link.zhihu.com/?target=http%3A//visugar.com/2017/08/01/20170801HexoPlugins/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">visugar</a>è¿™ä¸ªåšä¸»ç”¨çš„æ¥æ¯”åŠ›è¯„è®ºç³»ç»Ÿï¼Œæ„Ÿè§‰ä¹Ÿè¿˜ä¸é”™ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><p><a href=\"https://link.zhihu.com/?target=https%3A//livere.com/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">æ¥æ¯”åŠ›å®˜ç½‘</a>ï¼Œæ³¨å†Œå¥½åï¼Œç‚¹å‡»ç®¡ç†é¡µé¢ï¼Œåœ¨<code>ä»£ç ç®¡ç†</code>ä¸­æ‰¾åˆ°å®‰è£…ä»£ç ï¼š</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-c263c8377b987a0a32895b463db24a83_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1064\" data-rawheight=\"796\" class=\"origin_image zh-lightbox-thumb\" width=\"1064\" data-original=\"https://pic4.zhimg.com/v2-c263c8377b987a0a32895b463db24a83_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1064&#39; height=&#39;796&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1064\" data-rawheight=\"796\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1064\" data-original=\"https://pic4.zhimg.com/v2-c263c8377b987a0a32895b463db24a83_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-c263c8377b987a0a32895b463db24a83_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>è·å–å®‰è£…ä»£ç åï¼Œåœ¨ä¸»é¢˜çš„commentä¸‹æ–°å»ºä¸€ä¸ªæ–‡ä»¶æ”¾å…¥åˆšåˆšé‚£æ®µä»£ç ï¼Œå†æ‰¾åˆ°articleæ–‡ä»¶ï¼Œæ‰¾åˆ°å¦‚ä¸‹ä»£ç ï¼Œè‹¥æ²¡æœ‰åˆ™ç›´æ¥åœ¨footeråé¢æ·»åŠ å³å¯ã€‚livebeå³ä¸ºåˆšåˆšæ‰€åˆ›æ–‡ä»¶åç§°ã€‚</p><div class=\"highlight\"><pre><code class=\"language-text\">&lt;%- partial(&#39;comment/livebe&#39;) %&gt;\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>ç„¶åå¯ä»¥è‡ªå·±è®¾ç½®ä¸€äº›ä¸œè¥¿ï¼š</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-67202029da93884e296d145ba65039cb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1123\" data-rawheight=\"873\" class=\"origin_image zh-lightbox-thumb\" width=\"1123\" data-original=\"https://pic4.zhimg.com/v2-67202029da93884e296d145ba65039cb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1123&#39; height=&#39;873&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1123\" data-rawheight=\"873\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1123\" data-original=\"https://pic4.zhimg.com/v2-67202029da93884e296d145ba65039cb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-67202029da93884e296d145ba65039cb_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>è¿˜å¯ä»¥è®¾ç½®è¯„è®ºæé†’ï¼Œè¿™æ ·åˆ«äººè¯„è®ºä½ çš„æ—¶å€™å°±å¯ä»¥åŠæ—¶çŸ¥é“äº†ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>3. æ·»åŠ ç™¾åº¦ç»Ÿè®¡</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p>ç™¾åº¦ç»Ÿè®¡å¯ä»¥åœ¨åå°ä¸Šçœ‹åˆ°ä½ ç½‘ç«™çš„è®¿é—®æ•°ï¼Œæµè§ˆé‡ï¼Œæµè§ˆé“¾æ¥åˆ†å¸ƒç­‰å¾ˆé‡è¦çš„ä¿¡æ¯ã€‚æ‰€ä»¥æ·»åŠ ç™¾åº¦ç»Ÿè®¡èƒ½æ›´æœ‰æ•ˆçš„è®©ä½ æŒæ¡ä½ çš„ç½‘ç«™æƒ…å†µã€‚</p><p><a href=\"https://link.zhihu.com/?target=https%3A//tongji.baidu.com/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">ç™¾åº¦ç»Ÿè®¡</a>ï¼Œæ³¨å†Œä¸€ä¸‹ï¼Œè¿™é‡Œçš„è´¦å·å¥½åƒå’Œç™¾åº¦è´¦å·ä¸æ˜¯ä¸€èµ·çš„ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-d881052d4962047f570c0109c05898c1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"771\" data-rawheight=\"753\" class=\"origin_image zh-lightbox-thumb\" width=\"771\" data-original=\"https://pic2.zhimg.com/v2-d881052d4962047f570c0109c05898c1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;771&#39; height=&#39;753&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"771\" data-rawheight=\"753\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"771\" data-original=\"https://pic2.zhimg.com/v2-d881052d4962047f570c0109c05898c1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-d881052d4962047f570c0109c05898c1_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>ç…§æ ·æŠŠä»£ç å¤åˆ¶åˆ°<code>head.ejs</code>æ–‡ä»¶ä¸­ï¼Œç„¶åå†è¿›è¡Œä¸€ä¸‹å®‰è£…æ£€æŸ¥ï¼ŒåŠå°æ—¶å·¦å³å°±å¯ä»¥åœ¨ç™¾åº¦ç»Ÿè®¡é‡Œé¢çœ‹åˆ°è‡ªå·±çš„ç½‘ç«™ä¿¡æ¯äº†ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>4. æ–‡ç« é˜…è¯»é‡ç»Ÿè®¡leanCloud</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p><a href=\"https://link.zhihu.com/?target=https%3A//leancloud.cn/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">leanCloud</a>ï¼Œè¿›å»åæ³¨å†Œä¸€ä¸‹ï¼Œè¿›å…¥ååˆ›å»ºä¸€ä¸ªåº”ç”¨ï¼š</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-f396ec826528cc56a0c2c7aa610f736e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"615\" data-rawheight=\"450\" class=\"origin_image zh-lightbox-thumb\" width=\"615\" data-original=\"https://pic3.zhimg.com/v2-f396ec826528cc56a0c2c7aa610f736e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;615&#39; height=&#39;450&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"615\" data-rawheight=\"450\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"615\" data-original=\"https://pic3.zhimg.com/v2-f396ec826528cc56a0c2c7aa610f736e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-f396ec826528cc56a0c2c7aa610f736e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>åœ¨<code>å­˜å‚¨</code>ä¸­åˆ›å»ºClassï¼Œå‘½åä¸ºCounter,</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-002babac7a525a7c321165228d9f23ae_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"602\" data-rawheight=\"720\" class=\"origin_image zh-lightbox-thumb\" width=\"602\" data-original=\"https://pic3.zhimg.com/v2-002babac7a525a7c321165228d9f23ae_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;602&#39; height=&#39;720&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"602\" data-rawheight=\"720\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"602\" data-original=\"https://pic3.zhimg.com/v2-002babac7a525a7c321165228d9f23ae_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-002babac7a525a7c321165228d9f23ae_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>ç„¶ååœ¨è®¾ç½®é¡µé¢çœ‹åˆ°ä½ çš„<code>åº”ç”¨Key</code>ï¼Œåœ¨ä¸»é¢˜çš„é…ç½®æ–‡ä»¶ä¸­ï¼š</p><div class=\"highlight\"><pre><code class=\"language-text\">leancloud_visitors:\n  enable: true\n  app_id: ä½ çš„id\n  app_key: ä½ çš„key\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>åœ¨<code>article.ejs</code>ä¸­é€‚å½“çš„ä½ç½®æ·»åŠ å¦‚ä¸‹ï¼Œè¿™è¦çœ‹ä½ è®©æ–‡ç« çš„é˜…è¯»é‡ç»Ÿè®¡æ˜¾ç¤ºåœ¨å“ªä¸ªåœ°æ–¹äº†ï¼Œ</p><div class=\"highlight\"><pre><code class=\"language-text\">é˜…è¯»æ•°é‡:&lt;span id=&#34;&lt;%= url_for(post.path) %&gt;&#34; class=&#34;leancloud_visitors&#34; data-flag-title=&#34;&lt;%- post.title %&gt;&#34;&gt;&lt;/span&gt;æ¬¡\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>ç„¶ååœ¨<code>footer.ejs</code>çš„æœ€åï¼Œæ·»åŠ ï¼š</p><div class=\"highlight\"><pre><code class=\"language-text\">&lt;script src=&#34;//cdn1.lncld.net/static/js/2.5.0/av-min.js&#34;&gt;&lt;/script&gt;\n&lt;script&gt;\n    var APP_ID = &#39;ä½ çš„app id&#39;;\n    var APP_KEY = &#39;ä½ çš„app key&#39;;\n    AV.init({\n        appId: APP_ID,\n        appKey: APP_KEY\n    });\n    // æ˜¾ç¤ºæ¬¡æ•°\n    function showTime(Counter) {\n        var query = new AV.Query(&#34;Counter&#34;);\n        if($(&#34;.leancloud_visitors&#34;).length &gt; 0){\n            var url = $(&#34;.leancloud_visitors&#34;).attr(&#39;id&#39;).trim();\n            // where field\n            query.equalTo(&#34;words&#34;, url);\n            // count\n            query.count().then(function (number) {\n                // There are number instances of MyClass where words equals url.\n                $(document.getElementById(url)).text(number?  number : &#39;--&#39;);\n            }, function (error) {\n                // error is an instance of AVError.\n            });\n        }\n    }\n    // è¿½åŠ pv\n    function addCount(Counter) {\n        var url = $(&#34;.leancloud_visitors&#34;).length &gt; 0 ? $(&#34;.leancloud_visitors&#34;).attr(&#39;id&#39;).trim() : &#39;icafebolger.com&#39;;\n        var Counter = AV.Object.extend(&#34;Counter&#34;);\n        var query = new Counter;\n        query.save({\n            words: url\n        }).then(function (object) {\n        })\n    }\n    $(function () {\n        var Counter = AV.Object.extend(&#34;Counter&#34;);\n        addCount(Counter);\n        showTime(Counter);\n    });\n&lt;/script&gt;\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>é‡æ–°éƒ¨ç½²åå°±å¯ä»¥äº†ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>5. å¼•å…¥ä¸è’œå­è®¿é—®é‡å’Œè®¿é—®äººæ¬¡ç»Ÿè®¡</b></h2><p>ä¸è’œå­çš„æ·»åŠ éå¸¸éå¸¸æ–¹ä¾¿ï¼Œ<a href=\"https://link.zhihu.com/?target=http%3A//busuanzi.ibruce.info/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">ä¸è’œå­</a></p><p>åœ¨<code>footer.ejs</code>ä¸­çš„åˆé€‚ä½ç½®ï¼Œçœ‹ä½ è¦æ˜¾ç¤ºåœ¨å“ªä¸ªåœ°æ–¹ï¼Œæ·»åŠ ï¼š</p><div class=\"highlight\"><pre><code class=\"language-text\">&lt;!--è¿™ä¸€æ®µæ˜¯ä¸è’œå­çš„è®¿é—®é‡ç»Ÿè®¡ä»£ç --&gt;\n&lt;script async src=&#34;//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js&#34;&gt;&lt;/script&gt;\n&lt;span id=&#34;busuanzi_container_site_pv&#34;&gt;æœ¬ç«™æ€»è®¿é—®é‡&lt;span id=&#34;busuanzi_value_site_pv&#34;&gt;&lt;/span&gt;æ¬¡ &amp;nbsp;   &lt;/span&gt;\n&lt;span id=&#34;busuanzi_container_site_uv&#34;&gt;è®¿å®¢æ•°&lt;span id=&#34;busuanzi_value_site_uv&#34;&gt;&lt;/span&gt;äººæ¬¡&lt;/span&gt;\n</code></pre></div><p>å°±å¯ä»¥äº†ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>æ€»ç»“</b></h2><p>åˆ°è¿™é‡Œå°±åŸºæœ¬åšå®Œäº†ã€‚å…¶å®éƒ½æ˜¯å‚è€ƒåˆ«çš„åšä¸»çš„è®¾ç½®çš„ï¼Œä¸ä¸€å®šä»…é™äºhuemanä¸»é¢˜ï¼Œå…¶ä»–ä¸»é¢˜çš„è®¾ç½®ä¹Ÿæ˜¯å¤§ä½“ç›¸åŒçš„ï¼Œæ‰€ä»¥å¦‚æœä½ å¸Œæœ›è®¾ç½®åˆ«çš„ä¸»é¢˜ï¼Œé‚£ä¹ˆä»”ç»†çœ‹ä¸€ä¸‹è¿™ä¸ªä¸»é¢˜çš„ä»£ç ç»“æ„ï¼Œä¹Ÿèƒ½å¤ŸæŠŠä¸Šè¾¹çš„åŠŸèƒ½æ·»åŠ è¿›å»ã€‚</p><p class=\"ztext-empty-paragraph\"><br/></p><p>å¤šçœ‹çœ‹åˆ«çš„åšä¸»çš„é‚£äº›åŠŸèƒ½ï¼Œå¦‚æœæœ‰ä½ èƒ½æ‰¾åˆ°è‡ªå·±å–œæ¬¢çš„åŠŸèƒ½ï¼Œé‚£ä¹ˆå¥½å¥½å‘åŠ¨æœç´¢æŠ€èƒ½ï¼Œå¾ˆå¿«å°±èƒ½æ‰¾åˆ°æ€ä¹ˆåšäº†ã€‚åŠ æ²¹å§ï¼</p>", 
            "topic": [
                {
                    "tag": "ä¸ªäººåšå®¢", 
                    "tagLink": "https://api.zhihu.com/topics/19593765"
                }, 
                {
                    "tag": "Hexo", 
                    "tagLink": "https://api.zhihu.com/topics/19851557"
                }, 
                {
                    "tag": "åšå®¢", 
                    "tagLink": "https://api.zhihu.com/topics/19550419"
                }
            ], 
            "comments": [
                {
                    "userName": "å¾å¤©ç››", 
                    "userLink": "https://www.zhihu.com/people/fea7718b7e595871c3475fde62a90901", 
                    "content": "<p>éå¸¸å¥½çš„æ•™ç¨‹ï¼Œæ„Ÿè°¢ï¼</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "ç›Šè€…ä¸‰å‹", 
                    "userLink": "https://www.zhihu.com/people/af570118bf3ecd69482b7291614ffa68", 
                    "content": "<p>æ„Ÿè°¢åˆ†äº«ï¼æ–°æ‰‹å…¥é—¨ï¼Œè¯·é—®ä¸‹å¦‚ä½•åŠ ä¸€ä¸ªè‡ªå·±çš„ä¸»é¡µè¿›å»å‘¢ï¼Œå°±æ˜¯ç‚¹å‡»ä¸»é¡µè¿™ä¸¤ä¸ªå­—è·³è½¬åˆ°ä¸ªäººä¸»é¡µçš„html...ç›®å‰çœ‹åˆ°çš„éƒ½æ˜¯å¦‚ä½•ä¸Šä¼ md</p>", 
                    "likes": 1, 
                    "childComments": []
                }, 
                {
                    "userName": "é»å…ˆç”Ÿ", 
                    "userLink": "https://www.zhihu.com/people/049afcac650a1847466c6b1e192de26b", 
                    "content": "ä½ æ˜¯æˆ‘çœ‹åˆ°è®²çš„æ¯”è¾ƒæ¸…æ™° é€»è¾‘å¥½çš„ä¸€ç¯‡æ–‡ç« ï¼Œè°¢è°¢ä½œè€…", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "fly bird", 
                    "userLink": "https://www.zhihu.com/people/f7ffbc4456fc5869beec43cbade753aa", 
                    "content": "<p>éå¸¸è¯¦ç»† æ„Ÿè°¢ï¼</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "é“¶ææ ‘", 
                    "userLink": "https://www.zhihu.com/people/d7d09ab4dc2bf0a4887ffa1471750ede", 
                    "content": "å†™çš„å¾ˆè¯¦ç»†ï¼Œéå¸¸æ„Ÿè°¢ã€‚å“ï¼å¤šå¸Œæœ›èƒ½æœ‰ä¸€ä¸ªäººèƒ½å¤Ÿç»™æˆ‘æŒ‡å¯¼", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "ä¸å‘å‘", 
                    "userLink": "https://www.zhihu.com/people/991b2c52b022efd4ce5a448382696913", 
                    "content": "æ„Ÿè°¢", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "çŸ¥ä¹ç”¨æˆ·", 
                    "userLink": "https://www.zhihu.com/people/0", 
                    "content": "<p>éå¸¸ç³»ç»Ÿçš„å»ºç«™æŒ‡å—ï¼Œèµ</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "ç‹å­", 
                    "userLink": "https://www.zhihu.com/people/a17b047ed380b6fcc681a9dd1b15e9b5", 
                    "content": "1.åœ¨å“ªä¸ªåœ°æ–¹ä¿®æ”¹name/author ï¼Ÿ<br>2.ä¿®æ”¹å®Œéœ€è¦deployå—ï¼Ÿ", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "thirteenthree", 
                            "userLink": "https://www.zhihu.com/people/5c2aded44b591ba45f85e8f1f1e60211", 
                            "content": "<p>ä¸»é…ç½®config.ymlæ–‡ä»¶,ä¸æ˜¯ä¸»é¢˜ç›®å½•ä¸‹é‚£ä¸ªconfig.yml,æ˜¯ä¸themesåŒä¸€ç›®å½•çš„é‚£ä¸ªconfig.yml</p>", 
                            "likes": 0, 
                            "replyToAuthor": "ç‹å­"
                        }, 
                        {
                            "userName": "ç‹å­", 
                            "userLink": "https://www.zhihu.com/people/a17b047ed380b6fcc681a9dd1b15e9b5", 
                            "content": "å¥½çš„ï¼Œå·²okã€‚éå¸¸æ„Ÿè°¢", 
                            "likes": 0, 
                            "replyToAuthor": "thirteenthree"
                        }
                    ]
                }, 
                {
                    "userName": "å°å‘†å‘†çš„çœ‹", 
                    "userLink": "https://www.zhihu.com/people/3ea4319cb91890d032d022ea031e906a", 
                    "content": "ç­”ä¸»å¯ä¸å¯ä»¥ç»™ä¸ªè”æƒ³æ–¹å¼ï¼Œæ­å»ºåšå®¢è¿‡ç¨‹ä¸­é‡åˆ°å‡ ä¸ªé—®é¢˜æƒ³è¯·æ•™ä¸€ä¸‹æ‚¨", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "Mr.chuan", 
                    "userLink": "https://www.zhihu.com/people/3a5cbe6d56d1d114c7de1929ede7917e", 
                    "content": "<p>ä¸»é¢˜æ–‡ä»¶å¤¹ä¸­æœ‰node_modulesæ–‡ä»¶å¤¹ï¼Œhexo så°±å¯åŠ¨ä¸èµ·æ¥ï¼Œåˆ é™¤è¿™ä¸ªæ–‡ä»¶å¤¹å°±å¯ä»¥å¯åŠ¨èµ·æ¥ï¼Œè¿™ä¸ªæœ‰æ²¡æœ‰è§è¿‡å•Šã€‚</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "thirteenthree", 
                    "userLink": "https://www.zhihu.com/people/5c2aded44b591ba45f85e8f1f1e60211", 
                    "content": "<p>å†™è¿™ç§æ•™ç¨‹çš„äººæ€»æœ‰ä¸ªå¼Šç«¯æ²¡æœ‰è¯´,èœå•å†…å®¹æ˜¯ä¸»é¢˜æ”¯æŒæ‰èƒ½ä½¿ç”¨,è€Œè¿™ç§æ”¯æŒåˆä¸ä¸»é¢˜å¼€å‘äººæœ‰å…³,æœ‰çš„äººæ²¡æœ‰å¼€å‘èœå•,å°±è·Ÿaboutä¸€æ ·,å¥½å¤šäººæ˜¯æ²¡æœ‰å¼€å‘about,è¿™ç§é¡µé¢åªæœ‰,è‡ªå·±æœ‰èƒ½åŠ›å¼€å‘æ‰èƒ½éšå¿ƒæ‰€æ¬²çš„ä½¿ç”¨.hexoé»˜è®¤ä¸»é¢˜åªæœ‰homeä¸archives,ä¸ç®¡ä½ åŠ å“ªä¸ªèœå•,éƒ½æ˜¯ç©ºç™½çš„</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "åŠä¸ªé˜¿å®…", 
                    "userLink": "https://www.zhihu.com/people/eb8fa58456549b12d484c101c6436f00", 
                    "content": "ä½ å¥½,æˆ‘æƒ³è¯·æ•™æ‚¨ä¸€ä¸ªé—®é¢˜,å°±æ˜¯hexoæ˜¯å¿…é¡»æŠŠæ‰€æœ‰æ–‡ç« æ”¾åˆ°_postæ–‡ä»¶å¤¹ä¸‹é¢å—?èƒ½ä¸èƒ½å¤ŸæŠŠæ–‡ç«  è¿›è¡Œåˆ†ç±»,æ”¾åˆ°ä¸åŒçš„æ–‡ä»¶å¤¹ä¸‹,åœ¨æŠŠåˆ†ç±»æ–‡ä»¶å¤¹æ”¾åˆ°_post,", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "Mirtio", 
                    "userLink": "https://www.zhihu.com/people/afdfb8463a71afc5bd98f4f174014fea", 
                    "content": "<p>éƒ¨ç½²æˆåŠŸäº†ç”µè„‘ä¸èƒ½è®¿é—®æ‰‹æœºå¯ä»¥æ˜¯ä»€ä¹ˆåŸå› ï¼Ÿ</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }
    ], 
    "url": "https://zhuanlan.zhihu.com/fangzh"
}
