{
    "title": "Fangzh的进击之路", 
    "description": "分享学习经验、人生感悟", 
    "followers": [
        "https://www.zhihu.com/people/li-xian-hui-57-80", 
        "https://www.zhihu.com/people/rou-qiu-68-63", 
        "https://www.zhihu.com/people/zhang-rui-han-28", 
        "https://www.zhihu.com/people/silencesi-shi", 
        "https://www.zhihu.com/people/cha-de-wei-ke-25", 
        "https://www.zhihu.com/people/wang-da-pao-55-38", 
        "https://www.zhihu.com/people/simon-meng", 
        "https://www.zhihu.com/people/zhao-yun-long-24-70", 
        "https://www.zhihu.com/people/hong-kou-gang-ting-dian-62", 
        "https://www.zhihu.com/people/path-54", 
        "https://www.zhihu.com/people/msn-54", 
        "https://www.zhihu.com/people/skyline-20-91", 
        "https://www.zhihu.com/people/leadingsci", 
        "https://www.zhihu.com/people/yang-ke-82", 
        "https://www.zhihu.com/people/wan-an-an-an-78", 
        "https://www.zhihu.com/people/guai-guai-44-30", 
        "https://www.zhihu.com/people/de-duo-90", 
        "https://www.zhihu.com/people/642009845", 
        "https://www.zhihu.com/people/nian-hua-er-19", 
        "https://www.zhihu.com/people/qian-mian-liu-zhuan", 
        "https://www.zhihu.com/people/luozhongbin", 
        "https://www.zhihu.com/people/xjz-76", 
        "https://www.zhihu.com/people/loser_1", 
        "https://www.zhihu.com/people/jiu-shi-ya", 
        "https://www.zhihu.com/people/liu-jiang-87-82", 
        "https://www.zhihu.com/people/wei-zi-ang-66", 
        "https://www.zhihu.com/people/li-wen-long-68-16", 
        "https://www.zhihu.com/people/shi-guang-wu-sheng-65-46", 
        "https://www.zhihu.com/people/tu-dou-si-chao-ren", 
        "https://www.zhihu.com/people/bai-jun-qi-52", 
        "https://www.zhihu.com/people/suchao-79", 
        "https://www.zhihu.com/people/sakulaha-na-mi", 
        "https://www.zhihu.com/people/wu-ze-qing-14", 
        "https://www.zhihu.com/people/deathxi-ren-ko", 
        "https://www.zhihu.com/people/fang-yue-37", 
        "https://www.zhihu.com/people/zzx-47-7", 
        "https://www.zhihu.com/people/li-qing-30-94-65-41", 
        "https://www.zhihu.com/people/ffffffff-36-84", 
        "https://www.zhihu.com/people/lucky-45-9-77", 
        "https://www.zhihu.com/people/lin-lin-87-38", 
        "https://www.zhihu.com/people/xiao-xiao-88-98-74", 
        "https://www.zhihu.com/people/doctorli-71", 
        "https://www.zhihu.com/people/sun-rui-xiang-3", 
        "https://www.zhihu.com/people/longines-53", 
        "https://www.zhihu.com/people/qing-xiao-xiang-xiao", 
        "https://www.zhihu.com/people/three-e-e-Point", 
        "https://www.zhihu.com/people/wang-jia-95-94", 
        "https://www.zhihu.com/people/fa-hao-tu-cao-10", 
        "https://www.zhihu.com/people/fuzhengwei", 
        "https://www.zhihu.com/people/wang-tian-ye-14", 
        "https://www.zhihu.com/people/he-mei-36-96", 
        "https://www.zhihu.com/people/bailingnan", 
        "https://www.zhihu.com/people/sophist-20", 
        "https://www.zhihu.com/people/yin-xing-shu-90-5", 
        "https://www.zhihu.com/people/guang-chen-99-1", 
        "https://www.zhihu.com/people/tshogx-1", 
        "https://www.zhihu.com/people/codeyangjun", 
        "https://www.zhihu.com/people/rong-kai-hua-93", 
        "https://www.zhihu.com/people/artb-sir", 
        "https://www.zhihu.com/people/harrytsz", 
        "https://www.zhihu.com/people/lu-shi-sheng-29", 
        "https://www.zhihu.com/people/guo-yan-jing-45", 
        "https://www.zhihu.com/people/guomuguomunuo", 
        "https://www.zhihu.com/people/codeman-lol", 
        "https://www.zhihu.com/people/yang-troy-89", 
        "https://www.zhihu.com/people/ren-xuan-chi", 
        "https://www.zhihu.com/people/jimmyhua-37", 
        "https://www.zhihu.com/people/fu-hao-37-24", 
        "https://www.zhihu.com/people/li-jia-hong-20", 
        "https://www.zhihu.com/people/jiang-hao-wen-91-36", 
        "https://www.zhihu.com/people/ke-le-teng-zi", 
        "https://www.zhihu.com/people/liu-ev-74", 
        "https://www.zhihu.com/people/lao-meng-zi-65", 
        "https://www.zhihu.com/people/cheng-ming-zhe-78", 
        "https://www.zhihu.com/people/jax-59", 
        "https://www.zhihu.com/people/ni-yong-hu-97-91", 
        "https://www.zhihu.com/people/chao-ge-74-26", 
        "https://www.zhihu.com/people/dou-yi-geng", 
        "https://www.zhihu.com/people/hanxumyself", 
        "https://www.zhihu.com/people/pigg-94", 
        "https://www.zhihu.com/people/huang-da-xian-57-37-18", 
        "https://www.zhihu.com/people/liu-zhao-jun-7-76", 
        "https://www.zhihu.com/people/francis-52-23", 
        "https://www.zhihu.com/people/zhang-xin-10-47-90", 
        "https://www.zhihu.com/people/xiao-yun-43-27-67", 
        "https://www.zhihu.com/people/SecondaryMarquis", 
        "https://www.zhihu.com/people/li-xin-wei-91-72", 
        "https://www.zhihu.com/people/gui-yu-chao", 
        "https://www.zhihu.com/people/zhaopeng-2", 
        "https://www.zhihu.com/people/li-xin-95-42-23", 
        "https://www.zhihu.com/people/zhanghuidream", 
        "https://www.zhihu.com/people/yang-song-yang", 
        "https://www.zhihu.com/people/spirit-22-32"
    ], 
    "article": [
        {
            "url": "https://zhuanlan.zhihu.com/p/47108882", 
            "userName": "直上云霄", 
            "userLink": "https://www.zhihu.com/people/1033165ce4ad9c3fce69a0793dfab8ad", 
            "upvote": 27, 
            "title": "吴恩达Coursera(DeepLearning.ai)笔记和作业汇总帖", 
            "content": "<div class=\"highlight\"><pre><code class=\"language-text\">title: 吴恩达Coursera(DeepLearning.ai)笔记和作业汇总帖\ndate: 2018-10-18 20:01:05\nid: dl-ai-summary\ntags: \n - dl.ai\ncategories:\n - 汇总帖\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-fc1613b1e00dd44a89b5f550abd4321a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1616\" data-rawheight=\"856\" class=\"origin_image zh-lightbox-thumb\" width=\"1616\" data-original=\"https://pic3.zhimg.com/v2-fc1613b1e00dd44a89b5f550abd4321a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1616&#39; height=&#39;856&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1616\" data-rawheight=\"856\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1616\" data-original=\"https://pic3.zhimg.com/v2-fc1613b1e00dd44a89b5f550abd4321a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-fc1613b1e00dd44a89b5f550abd4321a_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>吴恩达Coursera(DeepLearning.ai)笔记和作业汇总。</p><p>&lt;!--more--&gt;</p><p>历时一个多月终于把NG的五门课全部学完并且做了作业和笔记了。这里汇总一下：</p><h2><b>第一门课：神经网络和深度学习 </b></h2><p>主要讲了神经网络的基本概念，以及机器学习的梯度下降法，向量化，而后进入了浅层和深层神经网络的实现。</p><ul><li>前两周太简单了，在之前的机器学习课上NG全部都讲过了，这里就不做了。</li><li>第三周：主要是浅层神经网络的实现<br/></li><ul><li>笔记：<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/2018091215/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">浅层神经网络</a></li><li>作业：<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/2018091216/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">浅层神经网络</a></li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>第四周：深层神经网络的实现<br/></li><ul><li>笔记：<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/2018091316/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">深层神经网络</a></li><li>作业：<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/2018091318/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">深层神经网络</a></li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>第二门课：改善神经网络</b></h2><p>介绍了改善神经网络的方法，如正则化，超参数调节，优化算法等。</p><ul><li>第一周：训练集的划分、正则化、dropout<br/></li><ul><li>笔记：<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/20180901513/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">深度学习的实践层面</a></li><li>作业：<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/2018091515/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">深度学习的实践层面</a></li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>第二周：Mini-batch、Momentum、RMS、Adam、学习率衰减<br/></li><ul><li>笔记：<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/2018091621/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">优化算法</a></li><li>作业：<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/2018091711/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">优化算法</a></li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>第三周：超参数的调试、BatchNorm、softmax<br/></li><ul><li>笔记：<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/2018091720/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">超参数调试</a></li><li>作业：<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/2018091810/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">超参数调试</a></li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>第三门课：结构化机器学习项目</b></h2><p>主要讲了机器学习中的一些策略。</p><ul><li>第一周：ML策略、正交化、优化指标、数据集的划分、偏差<br/></li><ul><li>笔记：<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/2018092016/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">机器学习策略(1)</a></li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>第二周：误差分析、数据不同分布、迁移学习、多任务、端到端<br/></li><ul><li>笔记：<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/2018092017/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">机器学习策略(2)</a></li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>第四门课：卷积神经网络</b></h2><p>主要讲了神经网络的在图像上的非常重要的应用，卷积神经网络。</p><ul><li>第一周：padding、步长、池化、卷积<br/></li><ul><li>笔记：<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/dl-ai-4-1/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">卷积神经网络</a></li><li>作业：<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/dl-ai-4-1h/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">卷积神经网络</a></li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>第二周：一些重要的神经网络结构，VGG、ResNet、Inception等<br/></li><ul><li>笔记：<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/dl-ai-4-2/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">深度卷积网络实例探究</a></li><li>作业：<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/dl-ai-4-2h/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">深度卷积网络实例探究</a></li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>第三周：目标检测、Bounding Box、IOU、NMS<br/></li><ul><li>笔记：<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/dl-ai-4-3/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">目标检测</a></li><li>作业：<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/dl-ai-4-3h/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">目标检测</a></li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>第四周：人脸识别和神经风格转换<br/></li><ul><li>笔记：<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/dl-ai-4-4/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">人脸识别和神经风格转换</a></li><li>作业：<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/dl-ai-4-4h/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">人脸识别和神经风格转换</a></li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>第五门课：序列模型</b></h2><p>主要讲了神经网络在语言领域的应用，用RNN模型</p><ul><li>第一周：介绍了基本的RNN、GRU、LSTM<br/></li><ul><li>笔记：<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/dl-ai-5-1/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">循环神经网络</a></li><li>作业：<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/dl-ai-5-1h1/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">构建RNN</a>、<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/dl-ai-5-1h2/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">字符级生成恐龙名字</a>、<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/dl-ai-5-1h3/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">LSTM生成爵士乐</a></li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>第二周：自然语言处理与词嵌入<br/></li><ul><li>笔记：<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/dl-ai-5-2/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">自然语言处理与词嵌入</a></li><li>作业：<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/dl-ai-5-2h/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">词向量运算和emoji表情包</a></li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>第三周：序列模型和注意力机制<br/></li><ul><li>笔记：<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/dl-ai-5-3/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">序列模型和注意力机制</a></li><li>作业：<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/dl-ai-5-3h/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">机器翻译和触发关键字</a></li></ul></ul><p></p><p></p>", 
            "topic": [
                {
                    "tag": "吴恩达（Andrew Ng）", 
                    "tagLink": "https://api.zhihu.com/topics/20003150"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "Coursera", 
                    "tagLink": "https://api.zhihu.com/topics/19742204"
                }
            ], 
            "comments": [
                {
                    "userName": "知乎用户", 
                    "userLink": "https://www.zhihu.com/people/0", 
                    "content": "你好，问一下，你一个月之间，大概每天学习多久？😯", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "直上云霄", 
                            "userLink": "https://www.zhihu.com/people/1033165ce4ad9c3fce69a0793dfab8ad", 
                            "content": "我忘记了，有效学习时间三四个小时吧。大部分时间是没办法集中精力的", 
                            "likes": 0, 
                            "replyToAuthor": "知乎用户"
                        }, 
                        {
                            "userName": "知乎用户", 
                            "userLink": "https://www.zhihu.com/people/0", 
                            "content": "好的，谢谢。", 
                            "likes": 0, 
                            "replyToAuthor": "直上云霄"
                        }
                    ]
                }, 
                {
                    "userName": "Luna", 
                    "userLink": "https://www.zhihu.com/people/532397b06ab942beacc28e98d6b904e5", 
                    "content": "谢谢楼主，最近刚好在看cnn，到时多多像你学习啊", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "狗娃子", 
                    "userLink": "https://www.zhihu.com/people/6edc022455eb041f73e1c1575ace9e5c", 
                    "content": "楼主真是个好人", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/45753821", 
            "userName": "直上云霄", 
            "userLink": "https://www.zhihu.com/people/1033165ce4ad9c3fce69a0793dfab8ad", 
            "upvote": 4, 
            "title": "cs231n作业：assignment1 - softmax", 
            "content": "<div class=\"highlight\"><pre><code class=\"language-text\">title: cs231n作业：assignment1 - softmax\nid: cs231n-1h-3\ntags:\n  - cs231n\n  - homework\ncategories:\n  - AI\n  - Deep Learning\ndate: 2018-09-27 16:02:57\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>GitHub地址：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/ZJUFangzh/cs231n\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/ZJUFangzh/cs</span><span class=\"invisible\">231n</span><span class=\"ellipsis\"></span></a></p><p>首发于个人博客: <a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Fangzh的个人博客 | 人工智能拯救世界</a> 欢迎来访</p><p>softmax是最常用的分类器之一。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>softmax和svm都是常用的分类器，而softmax更为常用。</p><p>具体可以参考我这篇的最后，ng老师有讲，<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/2018/2018091720/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">softmax</a></p><p class=\"ztext-empty-paragraph\"><br/></p><p>前面数据集的都跟SVM的一样。</p><p>直接进入loss和grads推导环节。</p><p><img src=\"https://www.zhihu.com/equation?tex=L_i+%3D+-log%28%5Cfrac%7Be%5E%7Bf_%7By_i%7D%7D%7D%7B%5Csum_j+e%5E%7Bf_j%7D%7D%29\" alt=\"L_i = -log(\\frac{e^{f_{y_i}}}{\\sum_j e^{f_j}})\" eeimg=\"1\"/> </p><p>可以看到，计算的公式也就是cross-entropy，即</p><p><img src=\"https://www.zhihu.com/equation?tex=H%28p%2Cq%29+%3D+-+%5Csum_i+y_i+log%28y_%7Bi%7D%5E%7Bhat%7D%29\" alt=\"H(p,q) = - \\sum_i y_i log(y_{i}^{hat})\" eeimg=\"1\"/> </p><p>但是，这样有一个缺点，就是指数$e^{f_{y_i}}$可能会特别大，这样可能导致内存不足，计算不稳定等问题。那么可以在分子分母同乘一个常数C，一般C取为 <img src=\"https://www.zhihu.com/equation?tex=logC+%3D+-max+f_j+\" alt=\"logC = -max f_j \" eeimg=\"1\"/> </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-cae4975702798a9de19495eb641e1de7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"389\" data-rawheight=\"100\" class=\"content_image\" width=\"389\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;389&#39; height=&#39;100&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"389\" data-rawheight=\"100\" class=\"content_image lazy\" width=\"389\" data-actualsrc=\"https://pic4.zhimg.com/v2-cae4975702798a9de19495eb641e1de7_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">f</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"mi\">123</span><span class=\"p\">,</span> <span class=\"mi\">456</span><span class=\"p\">,</span> <span class=\"mi\">789</span><span class=\"p\">])</span> <span class=\"c1\"># 例子中有3个分类，每个评分的数值都很大</span>\n<span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">))</span> <span class=\"c1\"># 不妙：数值问题，可能导致数值爆炸</span>\n<span class=\"err\">​</span>\n<span class=\"c1\"># 那么将f中的值平移到最大值为0：</span>\n<span class=\"n\">f</span> <span class=\"o\">-=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">max</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">)</span> <span class=\"c1\"># f becomes [-666, -333, 0]</span>\n<span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">))</span> <span class=\"c1\"># 现在OK了，将给出正确结果</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>精确地说，SVM分类器使用的是<i>折叶损失（hinge loss）</i>，有时候又被称为<i>最大边界损失（max-margin loss）</i>。Softmax分类器使用的是<i>交叉熵损失（corss-entropy loss）</i>。Softmax分类器的命名是从<i>softmax函数</i>那里得来的，softmax函数将原始分类评分变成正的归一化数值，所有数值和为1，这样处理后交叉熵损失才能应用。注意从技术上说“softmax损失（softmax loss）”是没有意义的，因为softmax只是一个压缩数值的函数。但是在这个说法常常被用来做简称。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>求导过程参考：<a href=\"https://zhuanlan.zhihu.com/p/37416115\" class=\"internal\">cs231n softmax求导</a></p><p>最终得到的公式是：</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-a2a53c650f8e78c8034a671acb561b56_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"381\" data-rawheight=\"272\" class=\"content_image\" width=\"381\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;381&#39; height=&#39;272&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"381\" data-rawheight=\"272\" class=\"content_image lazy\" width=\"381\" data-actualsrc=\"https://pic3.zhimg.com/v2-a2a53c650f8e78c8034a671acb561b56_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>softmax代码实现</b></p><p>编辑<code>cs231n/classifiers/softmax.py</code>,先写一下<code>softmax_loss_naive</code>函数，依旧是循环：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">softmax_loss_naive</span><span class=\"p\">(</span><span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">reg</span><span class=\"p\">):</span>\n  <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">  Softmax loss function, naive implementation (with loops)\n</span><span class=\"s2\">​\n</span><span class=\"s2\">  Inputs have dimension D, there are C classes, and we operate on minibatches\n</span><span class=\"s2\">  of N examples.\n</span><span class=\"s2\">​\n</span><span class=\"s2\">  Inputs:\n</span><span class=\"s2\">  - W: A numpy array of shape (D, C) containing weights.\n</span><span class=\"s2\">  - X: A numpy array of shape (N, D) containing a minibatch of data.\n</span><span class=\"s2\">  - y: A numpy array of shape (N,) containing training labels; y[i] = c means\n</span><span class=\"s2\">    that X[i] has label c, where 0 &lt;= c &lt; C.\n</span><span class=\"s2\">  - reg: (float) regularization strength\n</span><span class=\"s2\">​\n</span><span class=\"s2\">  Returns a tuple of:\n</span><span class=\"s2\">  - loss as single float\n</span><span class=\"s2\">  - gradient with respect to weights W; an array of same shape as W\n</span><span class=\"s2\">  &#34;&#34;&#34;</span>\n  <span class=\"c1\"># Initialize the loss and gradient to zero.</span>\n  <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span>\n  <span class=\"n\">dW</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros_like</span><span class=\"p\">(</span><span class=\"n\">W</span><span class=\"p\">)</span>\n<span class=\"err\">​</span>\n  <span class=\"c1\">#############################################################################</span>\n  <span class=\"c1\"># TODO: Compute the softmax loss and its gradient using explicit loops.     #</span>\n  <span class=\"c1\"># Store the loss in loss and the gradient in dW. If you are not careful     #</span>\n  <span class=\"c1\"># here, it is easy to run into numeric instability. Don&#39;t forget the        #</span>\n  <span class=\"c1\"># regularization!                                                           #</span>\n  <span class=\"c1\">#############################################################################</span>\n  <span class=\"p\">(</span><span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">D</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n  <span class=\"n\">C</span> <span class=\"o\">=</span> <span class=\"n\">W</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n  <span class=\"c1\">#遍历每个样本</span>\n  <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">N</span><span class=\"p\">):</span>\n    <span class=\"n\">f_i</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">W</span><span class=\"p\">)</span>\n    <span class=\"c1\">#进行公式的指数修正</span>\n    <span class=\"n\">f_i</span> <span class=\"o\">-=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">max</span><span class=\"p\">(</span><span class=\"n\">f_i</span><span class=\"p\">)</span>\n    <span class=\"n\">sum_j</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"n\">f_i</span><span class=\"p\">))</span>\n    <span class=\"c1\">#得到样本中每个类别的概率</span>\n    <span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span> <span class=\"n\">k</span> <span class=\"p\">:</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"n\">f_i</span><span class=\"p\">[</span><span class=\"n\">k</span><span class=\"p\">])</span> <span class=\"o\">/</span> <span class=\"n\">sum_j</span>\n    <span class=\"n\">loss</span> <span class=\"o\">+=</span> <span class=\"o\">-</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">log</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"p\">(</span><span class=\"n\">y</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]))</span>\n    <span class=\"c1\">#根据softmax求导公式</span>\n    <span class=\"k\">for</span> <span class=\"n\">k</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">C</span><span class=\"p\">):</span>\n      <span class=\"n\">p_k</span> <span class=\"o\">=</span> <span class=\"n\">p</span><span class=\"p\">(</span><span class=\"n\">k</span><span class=\"p\">)</span>\n      <span class=\"n\">dW</span><span class=\"p\">[:,</span> <span class=\"n\">k</span><span class=\"p\">]</span> <span class=\"o\">+=</span> <span class=\"p\">(</span><span class=\"n\">p_k</span> <span class=\"o\">-</span> <span class=\"p\">(</span><span class=\"n\">k</span> <span class=\"o\">==</span> <span class=\"n\">y</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]))</span> <span class=\"o\">*</span> <span class=\"n\">X</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span>\n  \n  <span class=\"n\">loss</span> <span class=\"o\">/=</span> <span class=\"n\">N</span>\n  <span class=\"n\">loss</span> <span class=\"o\">+=</span> <span class=\"mf\">0.5</span> <span class=\"o\">*</span> <span class=\"n\">reg</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">W</span> <span class=\"o\">*</span> <span class=\"n\">W</span><span class=\"p\">)</span>\n  <span class=\"n\">dW</span> <span class=\"o\">/=</span> <span class=\"n\">N</span>\n  <span class=\"n\">dW</span> <span class=\"o\">+=</span> <span class=\"n\">reg</span><span class=\"o\">*</span><span class=\"n\">W</span>\n  <span class=\"c1\">#############################################################################</span>\n  <span class=\"c1\">#                          END OF YOUR CODE                                 #</span>\n  <span class=\"c1\">#############################################################################</span>\n<span class=\"err\">​</span>\n  <span class=\"k\">return</span> <span class=\"n\">loss</span><span class=\"p\">,</span> <span class=\"n\">dW</span>\n<span class=\"err\">​</span></code></pre></div><p>验证一下loss和grad得到：</p><div class=\"highlight\"><pre><code class=\"language-text\">numerical: -0.621593 analytic: -0.621593, relative error: 7.693773e-09\nnumerical: -2.576505 analytic: -2.576505, relative error: 4.492083e-09\nnumerical: -1.527801 analytic: -1.527801, relative error: 4.264914e-08\nnumerical: 1.101379 analytic: 1.101379, relative error: 9.735173e-09\nnumerical: 2.375620 analytic: 2.375620, relative error: 3.791861e-08\nnumerical: 3.166961 analytic: 3.166960, relative error: 8.526285e-09\nnumerical: -1.440997 analytic: -1.440998, relative error: 4.728898e-08\nnumerical: 0.563304 analytic: 0.563304, relative error: 2.409996e-08\nnumerical: -2.057292 analytic: -2.057292, relative error: 1.820335e-08\nnumerical: -0.450338 analytic: -0.450338, relative error: 8.075985e-08\nnumerical: -0.233090 analytic: -0.233090, relative error: 4.136546e-08\nnumerical: 0.251391 analytic: 0.251391, relative error: 4.552523e-08\nnumerical: 0.787031 analytic: 0.787031, relative error: 5.036469e-08\nnumerical: -1.801593 analytic: -1.801594, relative error: 3.159903e-08\nnumerical: -0.294108 analytic: -0.294109, relative error: 1.792497e-07\nnumerical: -1.974307 analytic: -1.974307, relative error: 1.160708e-08\nnumerical: 2.986921 analytic: 2.986921, relative error: 2.788065e-08\nnumerical: -0.247281 analytic: -0.247281, relative error: 8.957573e-08\nnumerical: 0.569337 analytic: 0.569337, relative error: 2.384912e-08\nnumerical: -1.579298 analytic: -1.579298, relative error: 1.728733e-08</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><b>向量化softmax</b></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">softmax_loss_vectorized</span><span class=\"p\">(</span><span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">reg</span><span class=\"p\">):</span>\n  <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">  Softmax loss function, vectorized version.\n</span><span class=\"s2\">​\n</span><span class=\"s2\">  Inputs and outputs are the same as softmax_loss_naive.\n</span><span class=\"s2\">  &#34;&#34;&#34;</span>\n  <span class=\"c1\"># Initialize the loss and gradient to zero.</span>\n  <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span>\n  <span class=\"n\">dW</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros_like</span><span class=\"p\">(</span><span class=\"n\">W</span><span class=\"p\">)</span>\n<span class=\"err\">​</span>\n  <span class=\"c1\">#############################################################################</span>\n  <span class=\"c1\"># TODO: Compute the softmax loss and its gradient using no explicit loops.  #</span>\n  <span class=\"c1\"># Store the loss in loss and the gradient in dW. If you are not careful     #</span>\n  <span class=\"c1\"># here, it is easy to run into numeric instability. Don&#39;t forget the        #</span>\n  <span class=\"c1\"># regularization!                                                           #</span>\n  <span class=\"c1\">#############################################################################</span>\n  <span class=\"p\">(</span><span class=\"n\">N</span><span class=\"p\">,</span> <span class=\"n\">D</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n  <span class=\"n\">C</span> <span class=\"o\">=</span> <span class=\"n\">W</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n  <span class=\"n\">f</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">W</span><span class=\"p\">)</span>\n  <span class=\"c1\">#在列方向进行指数修正</span>\n  <span class=\"n\">f</span> <span class=\"o\">-=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">max</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">,</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"n\">keepdims</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n  <span class=\"c1\">#求得softmax各个类的概率</span>\n  <span class=\"n\">p</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">),</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"n\">keepdims</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n  <span class=\"n\">y_lable</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">N</span><span class=\"p\">,</span><span class=\"n\">C</span><span class=\"p\">))</span>\n  <span class=\"c1\">#y_lable就是(N,C)维的矩阵，每一行中只有对应的那个正确类别 = 1，其他都是0</span>\n  <span class=\"n\">y_lable</span><span class=\"p\">[</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"n\">N</span><span class=\"p\">),</span><span class=\"n\">y</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>\n  <span class=\"c1\">#cross entropy</span>\n  <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"o\">-</span><span class=\"mi\">1</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">multiply</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">log</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"p\">),</span><span class=\"n\">y_lable</span><span class=\"p\">))</span> <span class=\"o\">/</span> <span class=\"n\">N</span>\n  <span class=\"n\">loss</span> <span class=\"o\">+=</span> <span class=\"mf\">0.5</span> <span class=\"o\">*</span> <span class=\"n\">reg</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span> <span class=\"n\">W</span> <span class=\"o\">*</span> <span class=\"n\">W</span><span class=\"p\">)</span>\n  <span class=\"c1\">#求导公式，很清晰</span>\n  <span class=\"n\">dW</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">p</span><span class=\"o\">-</span><span class=\"n\">y_lable</span><span class=\"p\">)</span>\n  <span class=\"n\">dW</span> <span class=\"o\">/=</span> <span class=\"n\">N</span>\n  <span class=\"n\">dW</span> <span class=\"o\">+=</span> <span class=\"n\">reg</span><span class=\"o\">*</span><span class=\"n\">W</span>\n<span class=\"err\">​</span>\n<span class=\"err\">​</span>\n  <span class=\"c1\">#############################################################################</span>\n  <span class=\"c1\">#                          END OF YOUR CODE                                 #</span>\n  <span class=\"c1\">#############################################################################</span>\n<span class=\"err\">​</span>\n  <span class=\"k\">return</span> <span class=\"n\">loss</span><span class=\"p\">,</span> <span class=\"n\">dW</span>\n<span class=\"err\">​</span>\n<span class=\"err\">​</span></code></pre></div><p>检验一下向量化和非向量化的时间：</p><div class=\"highlight\"><pre><code class=\"language-text\">naive loss: 2.357905e+00 computed in 0.091724s\nvectorized loss: 2.357905e+00 computed in 0.002995s\nLoss difference: 0.000000\nGradient difference: 0.000000</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>softmax的函数已经编写完成了，接下来调一下学习率和正则化两个超参数：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># rates and regularization strengths; if you are careful you should be able to</span>\n<span class=\"c1\"># get a classification accuracy of over 0.35 on the validation set.</span>\n<span class=\"kn\">from</span> <span class=\"nn\">cs231n.classifiers</span> <span class=\"kn\">import</span> <span class=\"n\">Softmax</span>\n<span class=\"n\">results</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>\n<span class=\"n\">best_val</span> <span class=\"o\">=</span> <span class=\"o\">-</span><span class=\"mi\">1</span>\n<span class=\"n\">best_softmax</span> <span class=\"o\">=</span> <span class=\"bp\">None</span>\n<span class=\"n\">learning_rates</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mf\">1e-7</span><span class=\"p\">,</span> <span class=\"mf\">5e-7</span><span class=\"p\">]</span>\n<span class=\"n\">regularization_strengths</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mf\">2.5e4</span><span class=\"p\">,</span> <span class=\"mf\">5e4</span><span class=\"p\">]</span>\n<span class=\"err\">​</span>\n<span class=\"c1\">################################################################################</span>\n<span class=\"c1\"># TODO:                                                                        #</span>\n<span class=\"c1\"># Use the validation set to set the learning rate and regularization strength. #</span>\n<span class=\"c1\"># This should be identical to the validation that you did for the SVM; save    #</span>\n<span class=\"c1\"># the best trained softmax classifer in best_softmax.                          #</span>\n<span class=\"c1\">################################################################################</span>\n<span class=\"k\">for</span> <span class=\"n\">lr</span> <span class=\"ow\">in</span> <span class=\"n\">learning_rates</span><span class=\"p\">:</span>\n    <span class=\"k\">for</span> <span class=\"n\">reg</span> <span class=\"ow\">in</span> <span class=\"n\">regularization_strengths</span><span class=\"p\">:</span>\n        <span class=\"n\">softmax</span> <span class=\"o\">=</span> <span class=\"n\">Softmax</span><span class=\"p\">()</span>\n        <span class=\"n\">loss_hist</span> <span class=\"o\">=</span> <span class=\"n\">softmax</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">,</span> <span class=\"n\">y_train</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span><span class=\"o\">=</span><span class=\"n\">lr</span><span class=\"p\">,</span> <span class=\"n\">reg</span><span class=\"o\">=</span><span class=\"n\">reg</span><span class=\"p\">,</span>\n                      <span class=\"n\">num_iters</span><span class=\"o\">=</span><span class=\"mi\">1500</span><span class=\"p\">,</span> <span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n        <span class=\"n\">y_train_pred</span> <span class=\"o\">=</span> <span class=\"n\">softmax</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">)</span>\n        <span class=\"n\">y_val_pred</span> <span class=\"o\">=</span> <span class=\"n\">softmax</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">X_val</span><span class=\"p\">)</span>\n        <span class=\"n\">y_train_acc</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">y_train_pred</span><span class=\"o\">==</span><span class=\"n\">y_train</span><span class=\"p\">)</span>\n        <span class=\"n\">y_val_acc</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">y_val_pred</span><span class=\"o\">==</span><span class=\"n\">y_val</span><span class=\"p\">)</span>\n        <span class=\"n\">results</span><span class=\"p\">[(</span><span class=\"n\">lr</span><span class=\"p\">,</span><span class=\"n\">reg</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">y_train_acc</span><span class=\"p\">,</span> <span class=\"n\">y_val_acc</span><span class=\"p\">]</span>\n        <span class=\"k\">if</span> <span class=\"n\">y_val_acc</span> <span class=\"o\">&gt;</span> <span class=\"n\">best_val</span><span class=\"p\">:</span>\n            <span class=\"n\">best_val</span> <span class=\"o\">=</span> <span class=\"n\">y_val_acc</span>\n            <span class=\"n\">best_softmax</span> <span class=\"o\">=</span> <span class=\"n\">softmax</span>\n<span class=\"c1\">################################################################################</span>\n<span class=\"c1\">#                              END OF YOUR CODE                                #</span>\n<span class=\"c1\">################################################################################</span>\n    \n<span class=\"c1\"># Print out results.</span>\n<span class=\"k\">for</span> <span class=\"n\">lr</span><span class=\"p\">,</span> <span class=\"n\">reg</span> <span class=\"ow\">in</span> <span class=\"nb\">sorted</span><span class=\"p\">(</span><span class=\"n\">results</span><span class=\"p\">):</span>\n    <span class=\"n\">train_accuracy</span><span class=\"p\">,</span> <span class=\"n\">val_accuracy</span> <span class=\"o\">=</span> <span class=\"n\">results</span><span class=\"p\">[(</span><span class=\"n\">lr</span><span class=\"p\">,</span> <span class=\"n\">reg</span><span class=\"p\">)]</span>\n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;lr </span><span class=\"si\">%e</span><span class=\"s1\"> reg </span><span class=\"si\">%e</span><span class=\"s1\"> train accuracy: </span><span class=\"si\">%f</span><span class=\"s1\"> val accuracy: </span><span class=\"si\">%f</span><span class=\"s1\">&#39;</span> <span class=\"o\">%</span> <span class=\"p\">(</span>\n                <span class=\"n\">lr</span><span class=\"p\">,</span> <span class=\"n\">reg</span><span class=\"p\">,</span> <span class=\"n\">train_accuracy</span><span class=\"p\">,</span> <span class=\"n\">val_accuracy</span><span class=\"p\">))</span>\n    \n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;best validation accuracy achieved during cross-validation: </span><span class=\"si\">%f</span><span class=\"s1\">&#39;</span> <span class=\"o\">%</span> <span class=\"n\">best_val</span><span class=\"p\">)</span>\n<span class=\"n\">lr</span> <span class=\"mf\">1.000000e-07</span> <span class=\"n\">reg</span> <span class=\"mf\">2.500000e+04</span> <span class=\"n\">train</span> <span class=\"n\">accuracy</span><span class=\"p\">:</span> <span class=\"mf\">0.350592</span> <span class=\"n\">val</span> <span class=\"n\">accuracy</span><span class=\"p\">:</span> <span class=\"mf\">0.354000</span>\n<span class=\"n\">lr</span> <span class=\"mf\">1.000000e-07</span> <span class=\"n\">reg</span> <span class=\"mf\">5.000000e+04</span> <span class=\"n\">train</span> <span class=\"n\">accuracy</span><span class=\"p\">:</span> <span class=\"mf\">0.329551</span> <span class=\"n\">val</span> <span class=\"n\">accuracy</span><span class=\"p\">:</span> <span class=\"mf\">0.342000</span>\n<span class=\"n\">lr</span> <span class=\"mf\">5.000000e-07</span> <span class=\"n\">reg</span> <span class=\"mf\">2.500000e+04</span> <span class=\"n\">train</span> <span class=\"n\">accuracy</span><span class=\"p\">:</span> <span class=\"mf\">0.347286</span> <span class=\"n\">val</span> <span class=\"n\">accuracy</span><span class=\"p\">:</span> <span class=\"mf\">0.359000</span>\n<span class=\"n\">lr</span> <span class=\"mf\">5.000000e-07</span> <span class=\"n\">reg</span> <span class=\"mf\">5.000000e+04</span> <span class=\"n\">train</span> <span class=\"n\">accuracy</span><span class=\"p\">:</span> <span class=\"mf\">0.328551</span> <span class=\"n\">val</span> <span class=\"n\">accuracy</span><span class=\"p\">:</span> <span class=\"mf\">0.337000</span>\n<span class=\"n\">best</span> <span class=\"n\">validation</span> <span class=\"n\">accuracy</span> <span class=\"n\">achieved</span> <span class=\"n\">during</span> <span class=\"n\">cross</span><span class=\"o\">-</span><span class=\"n\">validation</span><span class=\"p\">:</span> <span class=\"mf\">0.359000</span></code></pre></div><p></p>", 
            "topic": [
                {
                    "tag": "科技", 
                    "tagLink": "https://api.zhihu.com/topics/19556664"
                }, 
                {
                    "tag": "作业", 
                    "tagLink": "https://api.zhihu.com/topics/19586372"
                }
            ], 
            "comments": [
                {
                    "userName": "蒋浩文", 
                    "userLink": "https://www.zhihu.com/people/82cfd3c86ef8ef5aeb384c1805b5e771", 
                    "content": "大佬，建议博客上面的可以传到知乎上。。。。虽然好像没什么人看，但感觉还是挺有帮助的。。。。", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "直上云霄", 
                            "userLink": "https://www.zhihu.com/people/1033165ce4ad9c3fce69a0793dfab8ad", 
                            "content": "谢谢！知乎只转了一部分，因为公式都要重新敲，而且有字数限制。最近忙着实习都没空写了", 
                            "likes": 0, 
                            "replyToAuthor": "蒋浩文"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/45753542", 
            "userName": "直上云霄", 
            "userLink": "https://www.zhihu.com/people/1033165ce4ad9c3fce69a0793dfab8ad", 
            "upvote": 14, 
            "title": "cs231n作业：assignment1 - svm", 
            "content": "<div class=\"highlight\"><pre><code class=\"language-text\">title: &#39;cs231n作业：assignment1 - svm&#39;\nid: cs231n-1h-2\ntags:\n  - cs231n\n  - homework\ncategories:\n  - AI\n  - Deep Learning\ndate: 2018-09-27 14:17:45\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>GitHub地址：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/ZJUFangzh/cs231n\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/ZJUFangzh/cs</span><span class=\"invisible\">231n</span><span class=\"ellipsis\"></span></a></p><p class=\"ztext-empty-paragraph\"><br/></p><p>首发于个人博客: <a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Fangzh的个人博客 | 人工智能拯救世界</a> 欢迎来访</p><p>完成了一个基于SVM的损失函数。</p><p>&lt;!--more--&gt;</p><h2><b>数据集</b></h2><p>载入的数据集依旧是:</p><div class=\"highlight\"><pre><code class=\"language-text\">Train data shape:  (49000, 32, 32, 3)\nTrain labels shape:  (49000,)\nValidation data shape:  (1000, 32, 32, 3)\nValidation labels shape:  (1000,)\nTest data shape:  (1000, 32, 32, 3)\nTest labels shape:  (1000,)</code></pre></div><p>而后进行32 * 32 * 3的图像拉伸，得到：</p><div class=\"highlight\"><pre><code class=\"language-text\">Training data shape:  (49000, 3072)\nValidation data shape:  (1000, 3072)\nTest data shape:  (1000, 3072)\ndev data shape:  (500, 3072)</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>进行一下简单的预处理，减去图像的平均值</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># Preprocessing: subtract the mean image</span>\n<span class=\"c1\"># first: compute the image mean based on the training data</span>\n<span class=\"n\">mean_image</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">)</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">mean_image</span><span class=\"p\">[:</span><span class=\"mi\">10</span><span class=\"p\">])</span> <span class=\"c1\"># print a few of the elements</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">figure</span><span class=\"p\">(</span><span class=\"n\">figsize</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">4</span><span class=\"p\">,</span><span class=\"mi\">4</span><span class=\"p\">))</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">imshow</span><span class=\"p\">(</span><span class=\"n\">mean_image</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">((</span><span class=\"mi\">32</span><span class=\"p\">,</span><span class=\"mi\">32</span><span class=\"p\">,</span><span class=\"mi\">3</span><span class=\"p\">))</span><span class=\"o\">.</span><span class=\"n\">astype</span><span class=\"p\">(</span><span class=\"s1\">&#39;uint8&#39;</span><span class=\"p\">))</span> <span class=\"c1\"># visualize the mean image</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>\n<span class=\"c1\"># second: subtract the mean image from train and test data</span>\n<span class=\"n\">X_train</span> <span class=\"o\">-=</span> <span class=\"n\">mean_image</span>\n<span class=\"n\">X_val</span> <span class=\"o\">-=</span> <span class=\"n\">mean_image</span>\n<span class=\"n\">X_test</span> <span class=\"o\">-=</span> <span class=\"n\">mean_image</span>\n<span class=\"n\">X_dev</span> <span class=\"o\">-=</span> <span class=\"n\">mean_image</span>\n<span class=\"c1\"># third: append the bias dimension of ones (i.e. bias trick) so that our SVM</span>\n<span class=\"c1\"># only has to worry about optimizing a single weight matrix W.</span>\n<span class=\"n\">X_train</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">hstack</span><span class=\"p\">([</span><span class=\"n\">X_train</span><span class=\"p\">,</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">ones</span><span class=\"p\">((</span><span class=\"n\">X_train</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"mi\">1</span><span class=\"p\">))])</span>\n<span class=\"n\">X_val</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">hstack</span><span class=\"p\">([</span><span class=\"n\">X_val</span><span class=\"p\">,</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">ones</span><span class=\"p\">((</span><span class=\"n\">X_val</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"mi\">1</span><span class=\"p\">))])</span>\n<span class=\"n\">X_test</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">hstack</span><span class=\"p\">([</span><span class=\"n\">X_test</span><span class=\"p\">,</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">ones</span><span class=\"p\">((</span><span class=\"n\">X_test</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"mi\">1</span><span class=\"p\">))])</span>\n<span class=\"n\">X_dev</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">hstack</span><span class=\"p\">([</span><span class=\"n\">X_dev</span><span class=\"p\">,</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">ones</span><span class=\"p\">((</span><span class=\"n\">X_dev</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"mi\">1</span><span class=\"p\">))])</span>\n<span class=\"err\">​</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">,</span> <span class=\"n\">X_val</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">,</span> <span class=\"n\">X_test</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">,</span> <span class=\"n\">X_dev</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span>\n<span class=\"p\">(</span><span class=\"mi\">49000</span><span class=\"p\">,</span> <span class=\"mi\">3073</span><span class=\"p\">)</span> <span class=\"p\">(</span><span class=\"mi\">1000</span><span class=\"p\">,</span> <span class=\"mi\">3073</span><span class=\"p\">)</span> <span class=\"p\">(</span><span class=\"mi\">1000</span><span class=\"p\">,</span> <span class=\"mi\">3073</span><span class=\"p\">)</span> <span class=\"p\">(</span><span class=\"mi\">500</span><span class=\"p\">,</span> <span class=\"mi\">3073</span><span class=\"p\">)</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>SVM分类器</b></h2><p>然后就可以开始来编写<code>cs231n/classifiers/linear_svm.py</code>的SVM分类器了。在这里先介绍一下SVM的基本公式和原理。</p><p>参考<a href=\"https://link.zhihu.com/?target=https%3A//www.tinymind.cn/articles/404\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">CS231n:线性分类</a></p><p>SVM损失函数想要SVM在正确分类上的比分始终比不正确的比分高出一个边界值$\\triangle$</p><p>第i个数据图像为 <img src=\"https://www.zhihu.com/equation?tex=x_i\" alt=\"x_i\" eeimg=\"1\"/> ，正确分类为 <img src=\"https://www.zhihu.com/equation?tex=y_i\" alt=\"y_i\" eeimg=\"1\"/> ，然后根据 <img src=\"https://www.zhihu.com/equation?tex=f%28x_i%2CW%29\" alt=\"f(x_i,W)\" eeimg=\"1\"/> 来计算不同分类的值，将分类简写为 <img src=\"https://www.zhihu.com/equation?tex=s\" alt=\"s\" eeimg=\"1\"/> ，那么第j类的得分就是 <img src=\"https://www.zhihu.com/equation?tex=s_j+%3D+f%28x_i%2CW%29_j\" alt=\"s_j = f(x_i,W)_j\" eeimg=\"1\"/> ，针对第i个数据的多类SVM的损失函数定义为：</p><p><img src=\"https://www.zhihu.com/equation?tex=L_i+%3D+%5Csum_%7Bj+%5Cneq+y_i%7D+max%280%2C+s_j+-+s_%7By_i%7D+%2B+%5Ctriangle%29\" alt=\"L_i = \\sum_{j \\neq y_i} max(0, s_j - s_{y_i} + \\triangle)\" eeimg=\"1\"/> </p><p>如：假设有3个分类， <img src=\"https://www.zhihu.com/equation?tex=s+%3D+%5B+13%2C-7%2C11%5D\" alt=\"s = [ 13,-7,11]\" eeimg=\"1\"/> ，第一个分类是正确的，也就是 <img src=\"https://www.zhihu.com/equation?tex=y_i+%3D+0\" alt=\"y_i = 0\" eeimg=\"1\"/> ，假设 <img src=\"https://www.zhihu.com/equation?tex=%5Ctriangle%3D10\" alt=\"\\triangle=10\" eeimg=\"1\"/> ，那么把所有不正确的分类加起来( <img src=\"https://www.zhihu.com/equation?tex=j+%5Cneq+y_i\" alt=\"j \\neq y_i\" eeimg=\"1\"/> )，</p><p><img src=\"https://www.zhihu.com/equation?tex=L_i+%3D+max%280%2C-7-13%2B10%29%2Bmax%280%2C11-13%2B10%29\" alt=\"L_i = max(0,-7-13+10)+max(0,11-13+10)\" eeimg=\"1\"/> </p><p>因为SVM只关心差距至少要大于10，所以 <img src=\"https://www.zhihu.com/equation?tex=L_i+%3D+8\" alt=\"L_i = 8\" eeimg=\"1\"/> </p><p>那么把公式套入：</p><p><img src=\"https://www.zhihu.com/equation?tex=L_i+%3D+%5Csum_%7Bj+%5Cneq+y_i%7D+max%280%2C+w_j+x_i+-+w_%7By_i%7D+x_i+%2B+%5Ctriangle%29\" alt=\"L_i = \\sum_{j \\neq y_i} max(0, w_j x_i - w_{y_i} x_i + \\triangle)\" eeimg=\"1\"/> </p><p>加入正则后：</p><p><img src=\"https://www.zhihu.com/equation?tex=L+%3D+%5Cfrac%7B1%7D%7BN%7D+%5Csum_i+%5Csum_%7Bj+%5Cneq+y_i%7Dmax%280%2C+f%28x_i+%3BW%29_%7Bj%7D+-+f%28x_i+%3B+W%29%7By_i%7D+%2B+%5Ctriangle%29+%2B+%5Clambda+%5Csum_k+%5Csum_l+W%5E%7B2%7D_%7Bk%2Cl%7D+\" alt=\"L = \\frac{1}{N} \\sum_i \\sum_{j \\neq y_i}max(0, f(x_i ;W)_{j} - f(x_i ; W){y_i} + \\triangle) + \\lambda \\sum_k \\sum_l W^{2}_{k,l} \" eeimg=\"1\"/> </p><p>到目前为止计算了loss，然后还需要计算梯度下降的grads，</p><p>官方并没有给推导过程，这才是cs231n作业难的地方所在。。。</p><p>详细可以看这一篇文章<a href=\"https://zhuanlan.zhihu.com/p/37068455\" class=\"internal\">CS 231 SVM 求导</a></p><p>总之就是两个公式：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-48edb635d8dcccc1df161dbcba60c9da_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"731\" data-rawheight=\"730\" class=\"origin_image zh-lightbox-thumb\" width=\"731\" data-original=\"https://pic3.zhimg.com/v2-48edb635d8dcccc1df161dbcba60c9da_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;731&#39; height=&#39;730&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"731\" data-rawheight=\"730\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"731\" data-original=\"https://pic3.zhimg.com/v2-48edb635d8dcccc1df161dbcba60c9da_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-48edb635d8dcccc1df161dbcba60c9da_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>而后开始编写<code>compute_loss_naive</code> 函数，先用循环来感受一下：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">svm_loss_naive</span><span class=\"p\">(</span><span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">reg</span><span class=\"p\">):</span>\n  <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">  Structured SVM loss function, naive implementation (with loops).\n</span><span class=\"s2\">​\n</span><span class=\"s2\">  Inputs have dimension D, there are C classes, and we operate on minibatches\n</span><span class=\"s2\">  of N examples.\n</span><span class=\"s2\">​\n</span><span class=\"s2\">  Inputs:\n</span><span class=\"s2\">  - W: A numpy array of shape (D, C) containing weights.\n</span><span class=\"s2\">  - X: A numpy array of shape (N, D) containing a minibatch of data.\n</span><span class=\"s2\">  - y: A numpy array of shape (N,) containing training labels; y[i] = c means\n</span><span class=\"s2\">    that X[i] has label c, where 0 &lt;= c &lt; C.\n</span><span class=\"s2\">  - reg: (float) regularization strength\n</span><span class=\"s2\">​\n</span><span class=\"s2\">  Returns a tuple of:\n</span><span class=\"s2\">  - loss as single float\n</span><span class=\"s2\">  - gradient with respect to weights W; an array of same shape as W\n</span><span class=\"s2\">  &#34;&#34;&#34;</span>\n  <span class=\"n\">dW</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"n\">W</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span> <span class=\"c1\"># initialize the gradient as zero</span>\n<span class=\"err\">​</span>\n  <span class=\"c1\"># compute the loss and the gradient</span>\n  <span class=\"n\">num_classes</span> <span class=\"o\">=</span> <span class=\"n\">W</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n  <span class=\"n\">num_train</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n  <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span>\n  <span class=\"c1\">#逐个计算每个样本的loss</span>\n  <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">xrange</span><span class=\"p\">(</span><span class=\"n\">num_train</span><span class=\"p\">):</span>\n    <span class=\"c1\">#计算每个样本的各个分类得分</span>\n    <span class=\"n\">scores</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">W</span><span class=\"p\">)</span>\n    <span class=\"n\">correct_class_score</span> <span class=\"o\">=</span> <span class=\"n\">scores</span><span class=\"p\">[</span><span class=\"n\">y</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]]</span>\n    <span class=\"c1\">#计算每个分类的得分，计入loss中</span>\n    <span class=\"k\">for</span> <span class=\"n\">j</span> <span class=\"ow\">in</span> <span class=\"nb\">xrange</span><span class=\"p\">(</span><span class=\"n\">num_classes</span><span class=\"p\">):</span>\n      <span class=\"c1\"># 根据公式，j==y[i]的就是本身的分类，不用算了</span>\n      <span class=\"k\">if</span> <span class=\"n\">j</span> <span class=\"o\">==</span> <span class=\"n\">y</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]:</span>\n        <span class=\"k\">continue</span>\n      <span class=\"n\">margin</span> <span class=\"o\">=</span> <span class=\"n\">scores</span><span class=\"p\">[</span><span class=\"n\">j</span><span class=\"p\">]</span> <span class=\"o\">-</span> <span class=\"n\">correct_class_score</span> <span class=\"o\">+</span> <span class=\"mi\">1</span> <span class=\"c1\"># note delta = 1</span>\n      <span class=\"c1\">#如果计算的margin &gt; 0，那么就要算入loss，</span>\n      <span class=\"k\">if</span> <span class=\"n\">margin</span> <span class=\"o\">&gt;</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n        <span class=\"n\">loss</span> <span class=\"o\">+=</span> <span class=\"n\">margin</span>\n        <span class=\"c1\">#公式2</span>\n        <span class=\"n\">dW</span><span class=\"p\">[:,</span><span class=\"n\">y</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]]</span> <span class=\"o\">+=</span> <span class=\"o\">-</span><span class=\"n\">X</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">,:]</span><span class=\"o\">.</span><span class=\"n\">T</span>\n        <span class=\"c1\">#公式1</span>\n        <span class=\"n\">dW</span><span class=\"p\">[:,</span><span class=\"n\">j</span><span class=\"p\">]</span> <span class=\"o\">+=</span> <span class=\"n\">X</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"p\">:]</span><span class=\"o\">.</span><span class=\"n\">T</span>\n  <span class=\"c1\"># Right now the loss is a sum over all training examples, but we want it</span>\n  <span class=\"c1\"># to be an average instead so we divide by num_train.</span>\n  <span class=\"n\">loss</span> <span class=\"o\">/=</span> <span class=\"n\">num_train</span>\n  <span class=\"n\">dW</span> <span class=\"o\">/=</span> <span class=\"n\">num_train</span>\n  <span class=\"c1\"># Add regularization to the loss.</span>\n  <span class=\"n\">loss</span> <span class=\"o\">+=</span> <span class=\"n\">reg</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">W</span> <span class=\"o\">*</span> <span class=\"n\">W</span><span class=\"p\">)</span>\n  <span class=\"n\">dW</span> <span class=\"o\">+=</span> <span class=\"n\">reg</span> <span class=\"o\">*</span> <span class=\"n\">W</span>\n  <span class=\"c1\">#############################################################################</span>\n  <span class=\"c1\"># TODO:                                                                     #</span>\n  <span class=\"c1\"># Compute the gradient of the loss function and store it dW.                #</span>\n  <span class=\"c1\"># Rather that first computing the loss and then computing the derivative,   #</span>\n  <span class=\"c1\"># it may be simpler to compute the derivative at the same time that the     #</span>\n  <span class=\"c1\"># loss is being computed. As a result you may need to modify some of the    #</span>\n  <span class=\"c1\"># code above to compute the gradient.                                       #</span>\n  <span class=\"c1\">#############################################################################</span>\n<span class=\"err\">​</span>\n<span class=\"err\">​</span>\n  <span class=\"k\">return</span> <span class=\"n\">loss</span><span class=\"p\">,</span> <span class=\"n\">dW</span>\n<span class=\"err\">​</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>写完后，用梯度检验检查一下:</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># Once you&#39;ve implemented the gradient, recompute it with the code below</span>\n<span class=\"c1\"># and gradient check it with the function we provided for you</span>\n<span class=\"err\">​</span>\n<span class=\"c1\"># Compute the loss and its gradient at W.</span>\n<span class=\"n\">loss</span><span class=\"p\">,</span> <span class=\"n\">grad</span> <span class=\"o\">=</span> <span class=\"n\">svm_loss_naive</span><span class=\"p\">(</span><span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">X_dev</span><span class=\"p\">,</span> <span class=\"n\">y_dev</span><span class=\"p\">,</span> <span class=\"mf\">0.0</span><span class=\"p\">)</span>\n<span class=\"err\">​</span>\n<span class=\"c1\"># Numerically compute the gradient along several randomly chosen dimensions, and</span>\n<span class=\"c1\"># compare them with your analytically computed gradient. The numbers should match</span>\n<span class=\"c1\"># almost exactly along all dimensions.</span>\n<span class=\"kn\">from</span> <span class=\"nn\">cs231n.gradient_check</span> <span class=\"kn\">import</span> <span class=\"n\">grad_check_sparse</span>\n<span class=\"n\">f</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span> <span class=\"n\">w</span><span class=\"p\">:</span> <span class=\"n\">svm_loss_naive</span><span class=\"p\">(</span><span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">X_dev</span><span class=\"p\">,</span> <span class=\"n\">y_dev</span><span class=\"p\">,</span> <span class=\"mf\">0.0</span><span class=\"p\">)[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n<span class=\"n\">grad_numerical</span> <span class=\"o\">=</span> <span class=\"n\">grad_check_sparse</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">,</span> <span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">grad</span><span class=\"p\">)</span>\n<span class=\"err\">​</span>\n<span class=\"c1\"># do the gradient check once again with regularization turned on</span>\n<span class=\"c1\"># you didn&#39;t forget the regularization gradient did you?</span>\n<span class=\"n\">loss</span><span class=\"p\">,</span> <span class=\"n\">grad</span> <span class=\"o\">=</span> <span class=\"n\">svm_loss_naive</span><span class=\"p\">(</span><span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">X_dev</span><span class=\"p\">,</span> <span class=\"n\">y_dev</span><span class=\"p\">,</span> <span class=\"mf\">5e1</span><span class=\"p\">)</span>\n<span class=\"n\">f</span> <span class=\"o\">=</span> <span class=\"k\">lambda</span> <span class=\"n\">w</span><span class=\"p\">:</span> <span class=\"n\">svm_loss_naive</span><span class=\"p\">(</span><span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">X_dev</span><span class=\"p\">,</span> <span class=\"n\">y_dev</span><span class=\"p\">,</span> <span class=\"mf\">5e1</span><span class=\"p\">)[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n<span class=\"n\">grad_numerical</span> <span class=\"o\">=</span> <span class=\"n\">grad_check_sparse</span><span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">,</span> <span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">grad</span><span class=\"p\">)</span>\n<span class=\"n\">numerical</span><span class=\"p\">:</span> <span class=\"mf\">34.663598</span> <span class=\"n\">analytic</span><span class=\"p\">:</span> <span class=\"mf\">34.663598</span><span class=\"p\">,</span> <span class=\"n\">relative</span> <span class=\"n\">error</span><span class=\"p\">:</span> <span class=\"mf\">6.995024e-13</span>\n<span class=\"n\">numerical</span><span class=\"p\">:</span> <span class=\"mf\">21.043334</span> <span class=\"n\">analytic</span><span class=\"p\">:</span> <span class=\"mf\">21.043334</span><span class=\"p\">,</span> <span class=\"n\">relative</span> <span class=\"n\">error</span><span class=\"p\">:</span> <span class=\"mf\">5.147242e-12</span>\n<span class=\"n\">numerical</span><span class=\"p\">:</span> <span class=\"mf\">1.334055</span> <span class=\"n\">analytic</span><span class=\"p\">:</span> <span class=\"mf\">1.334055</span><span class=\"p\">,</span> <span class=\"n\">relative</span> <span class=\"n\">error</span><span class=\"p\">:</span> <span class=\"mf\">5.315420e-11</span>\n<span class=\"n\">numerical</span><span class=\"p\">:</span> <span class=\"mf\">16.611704</span> <span class=\"n\">analytic</span><span class=\"p\">:</span> <span class=\"mf\">16.611704</span><span class=\"p\">,</span> <span class=\"n\">relative</span> <span class=\"n\">error</span><span class=\"p\">:</span> <span class=\"mf\">6.908581e-12</span>\n<span class=\"n\">numerical</span><span class=\"p\">:</span> <span class=\"mf\">25.327188</span> <span class=\"n\">analytic</span><span class=\"p\">:</span> <span class=\"mf\">25.327188</span><span class=\"p\">,</span> <span class=\"n\">relative</span> <span class=\"n\">error</span><span class=\"p\">:</span> <span class=\"mf\">1.552987e-11</span>\n<span class=\"n\">numerical</span><span class=\"p\">:</span> <span class=\"o\">-</span><span class=\"mf\">12.867717</span> <span class=\"n\">analytic</span><span class=\"p\">:</span> <span class=\"o\">-</span><span class=\"mf\">12.867717</span><span class=\"p\">,</span> <span class=\"n\">relative</span> <span class=\"n\">error</span><span class=\"p\">:</span> <span class=\"mf\">1.966004e-11</span>\n<span class=\"n\">numerical</span><span class=\"p\">:</span> <span class=\"mf\">15.066285</span> <span class=\"n\">analytic</span><span class=\"p\">:</span> <span class=\"mf\">15.066285</span><span class=\"p\">,</span> <span class=\"n\">relative</span> <span class=\"n\">error</span><span class=\"p\">:</span> <span class=\"mf\">7.012975e-12</span>\n<span class=\"n\">numerical</span><span class=\"p\">:</span> <span class=\"o\">-</span><span class=\"mf\">3.752014</span> <span class=\"n\">analytic</span><span class=\"p\">:</span> <span class=\"o\">-</span><span class=\"mf\">3.752014</span><span class=\"p\">,</span> <span class=\"n\">relative</span> <span class=\"n\">error</span><span class=\"p\">:</span> <span class=\"mf\">7.502607e-11</span>\n<span class=\"n\">numerical</span><span class=\"p\">:</span> <span class=\"mf\">9.927043</span> <span class=\"n\">analytic</span><span class=\"p\">:</span> <span class=\"mf\">9.927043</span><span class=\"p\">,</span> <span class=\"n\">relative</span> <span class=\"n\">error</span><span class=\"p\">:</span> <span class=\"mf\">9.010584e-13</span>\n<span class=\"n\">numerical</span><span class=\"p\">:</span> <span class=\"mf\">33.071345</span> <span class=\"n\">analytic</span><span class=\"p\">:</span> <span class=\"mf\">33.071345</span><span class=\"p\">,</span> <span class=\"n\">relative</span> <span class=\"n\">error</span><span class=\"p\">:</span> <span class=\"mf\">1.305438e-12</span>\n<span class=\"n\">numerical</span><span class=\"p\">:</span> <span class=\"o\">-</span><span class=\"mf\">19.227144</span> <span class=\"n\">analytic</span><span class=\"p\">:</span> <span class=\"o\">-</span><span class=\"mf\">19.227851</span><span class=\"p\">,</span> <span class=\"n\">relative</span> <span class=\"n\">error</span><span class=\"p\">:</span> <span class=\"mf\">1.836495e-05</span>\n<span class=\"n\">numerical</span><span class=\"p\">:</span> <span class=\"mf\">31.392728</span> <span class=\"n\">analytic</span><span class=\"p\">:</span> <span class=\"mf\">31.391611</span><span class=\"p\">,</span> <span class=\"n\">relative</span> <span class=\"n\">error</span><span class=\"p\">:</span> <span class=\"mf\">1.778034e-05</span>\n<span class=\"n\">numerical</span><span class=\"p\">:</span> <span class=\"o\">-</span><span class=\"mf\">10.450509</span> <span class=\"n\">analytic</span><span class=\"p\">:</span> <span class=\"o\">-</span><span class=\"mf\">10.456860</span><span class=\"p\">,</span> <span class=\"n\">relative</span> <span class=\"n\">error</span><span class=\"p\">:</span> <span class=\"mf\">3.037629e-04</span>\n<span class=\"n\">numerical</span><span class=\"p\">:</span> <span class=\"o\">-</span><span class=\"mf\">1.346690</span> <span class=\"n\">analytic</span><span class=\"p\">:</span> <span class=\"o\">-</span><span class=\"mf\">1.345625</span><span class=\"p\">,</span> <span class=\"n\">relative</span> <span class=\"n\">error</span><span class=\"p\">:</span> <span class=\"mf\">3.953276e-04</span>\n<span class=\"n\">numerical</span><span class=\"p\">:</span> <span class=\"mf\">7.843501</span> <span class=\"n\">analytic</span><span class=\"p\">:</span> <span class=\"mf\">7.846486</span><span class=\"p\">,</span> <span class=\"n\">relative</span> <span class=\"n\">error</span><span class=\"p\">:</span> <span class=\"mf\">1.902216e-04</span>\n<span class=\"n\">numerical</span><span class=\"p\">:</span> <span class=\"mf\">20.635011</span> <span class=\"n\">analytic</span><span class=\"p\">:</span> <span class=\"mf\">20.628368</span><span class=\"p\">,</span> <span class=\"n\">relative</span> <span class=\"n\">error</span><span class=\"p\">:</span> <span class=\"mf\">1.609761e-04</span>\n<span class=\"n\">numerical</span><span class=\"p\">:</span> <span class=\"mf\">23.654254</span> <span class=\"n\">analytic</span><span class=\"p\">:</span> <span class=\"mf\">23.652696</span><span class=\"p\">,</span> <span class=\"n\">relative</span> <span class=\"n\">error</span><span class=\"p\">:</span> <span class=\"mf\">3.294745e-05</span>\n<span class=\"n\">numerical</span><span class=\"p\">:</span> <span class=\"mf\">37.706709</span> <span class=\"n\">analytic</span><span class=\"p\">:</span> <span class=\"mf\">37.703260</span><span class=\"p\">,</span> <span class=\"n\">relative</span> <span class=\"n\">error</span><span class=\"p\">:</span> <span class=\"mf\">4.573495e-05</span>\n<span class=\"n\">numerical</span><span class=\"p\">:</span> <span class=\"mf\">9.558804</span> <span class=\"n\">analytic</span><span class=\"p\">:</span> <span class=\"mf\">9.566079</span><span class=\"p\">,</span> <span class=\"n\">relative</span> <span class=\"n\">error</span><span class=\"p\">:</span> <span class=\"mf\">3.804143e-04</span>\n<span class=\"n\">numerical</span><span class=\"p\">:</span> <span class=\"mf\">20.450011</span> <span class=\"n\">analytic</span><span class=\"p\">:</span> <span class=\"mf\">20.451451</span><span class=\"p\">,</span> <span class=\"n\">relative</span> <span class=\"n\">error</span><span class=\"p\">:</span> <span class=\"mf\">3.521650e-05</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><b>向量化SVM</b></p><p>套循环肯定是最菜的做法，我们在处理图像的时候肯定都要用矩阵算的：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">svm_loss_vectorized</span><span class=\"p\">(</span><span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">reg</span><span class=\"p\">):</span>\n  <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">  Structured SVM loss function, vectorized implementation.\n</span><span class=\"s2\">​\n</span><span class=\"s2\">  Inputs and outputs are the same as svm_loss_naive.\n</span><span class=\"s2\">  &#34;&#34;&#34;</span>\n  <span class=\"n\">loss</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span>\n  <span class=\"n\">dW</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"n\">W</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span> <span class=\"c1\"># initialize the gradient as zero</span>\n<span class=\"err\">​</span>\n  <span class=\"c1\">#############################################################################</span>\n  <span class=\"c1\"># TODO:                                                                     #</span>\n  <span class=\"c1\"># Implement a vectorized version of the structured SVM loss, storing the    #</span>\n  <span class=\"c1\"># result in loss.                                                           #</span>\n  <span class=\"c1\">#############################################################################</span>\n  <span class=\"c1\">#scores (N,C)</span>\n  <span class=\"n\">scores</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">W</span><span class=\"p\">)</span>\n  <span class=\"c1\">#num_classes = W.shape[1]</span>\n  <span class=\"n\">num_train</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n  <span class=\"c1\">#利用np.arange(),correct_class_score变成了 (num_train,y)的矩阵</span>\n  <span class=\"n\">correct_class_score</span> <span class=\"o\">=</span> <span class=\"n\">scores</span><span class=\"p\">[</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"n\">num_train</span><span class=\"p\">),</span><span class=\"n\">y</span><span class=\"p\">]</span>\n  <span class=\"n\">correct_class_score</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"n\">correct_class_score</span><span class=\"p\">,(</span><span class=\"n\">num_train</span><span class=\"p\">,</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">))</span>\n  <span class=\"n\">margins</span> <span class=\"o\">=</span> <span class=\"n\">scores</span> <span class=\"o\">-</span> <span class=\"n\">correct_class_score</span> <span class=\"o\">+</span> <span class=\"mi\">1</span>\n  <span class=\"n\">margins</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">maximum</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">margins</span><span class=\"p\">)</span>\n  <span class=\"c1\">#然后这里计算了j=y[i]的情形，所以把他们置为0</span>\n  <span class=\"n\">margins</span><span class=\"p\">[</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"n\">num_train</span><span class=\"p\">),</span><span class=\"n\">y</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n  <span class=\"n\">loss</span> <span class=\"o\">+=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">margins</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">num_train</span>\n  <span class=\"n\">loss</span> <span class=\"o\">+=</span> <span class=\"n\">reg</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span> <span class=\"n\">W</span> <span class=\"o\">*</span> <span class=\"n\">W</span><span class=\"p\">)</span>\n  <span class=\"c1\">#############################################################################</span>\n  <span class=\"c1\">#                             END OF YOUR CODE                              #</span>\n  <span class=\"c1\">#############################################################################</span>\n<span class=\"err\">​</span>\n<span class=\"err\">​</span>\n  <span class=\"c1\">#############################################################################</span>\n  <span class=\"c1\"># TODO:                                                                     #</span>\n  <span class=\"c1\"># Implement a vectorized version of the gradient for the structured SVM     #</span>\n  <span class=\"c1\"># loss, storing the result in dW.                                           #</span>\n  <span class=\"c1\">#                                                                           #</span>\n  <span class=\"c1\"># Hint: Instead of computing the gradient from scratch, it may be easier    #</span>\n  <span class=\"c1\"># to reuse some of the intermediate values that you used to compute the     #</span>\n  <span class=\"c1\"># loss.                                                                     #</span>\n  <span class=\"c1\">#############################################################################</span>\n  <span class=\"n\">margins</span><span class=\"p\">[</span><span class=\"n\">margins</span> <span class=\"o\">&gt;</span> <span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>\n  <span class=\"c1\">#因为j=y[i]的那一个元素的grad要计算 &gt;0 的那些次数次</span>\n  <span class=\"n\">row_sum</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">margins</span><span class=\"p\">,</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n  <span class=\"n\">margins</span><span class=\"p\">[</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">arange</span><span class=\"p\">(</span><span class=\"n\">num_train</span><span class=\"p\">),</span><span class=\"n\">y</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"o\">-</span><span class=\"n\">row_sum</span><span class=\"o\">.</span><span class=\"n\">T</span>\n  <span class=\"c1\">#把公式1和2合到一起计算了</span>\n  <span class=\"n\">dW</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">,</span><span class=\"n\">margins</span><span class=\"p\">)</span>\n  <span class=\"n\">dW</span> <span class=\"o\">/=</span> <span class=\"n\">num_train</span>\n  <span class=\"n\">dW</span> <span class=\"o\">+=</span> <span class=\"n\">reg</span> <span class=\"o\">*</span> <span class=\"n\">W</span>\n  <span class=\"c1\">#############################################################################</span>\n  <span class=\"c1\">#                             END OF YOUR CODE                              #</span>\n  <span class=\"c1\">#############################################################################</span>\n<span class=\"err\">​</span>\n  <span class=\"k\">return</span> <span class=\"n\">loss</span><span class=\"p\">,</span> <span class=\"n\">dW</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>计算一下两者的时间差：</p><div class=\"highlight\"><pre><code class=\"language-text\">Naive loss: 8.577034e+00 computed in 0.084761s\nVectorized loss: 8.577034e+00 computed in 0.001029s\ndifference: -0.000000\nNaive loss and gradient: computed in 0.082744s\nVectorized loss and gradient: computed in 0.002027s\ndifference: 0.000000</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><b>Stochastic Gradient Descent</b></p><p>编辑一下<code>classifiers/linear_classifier/LinearClassifier.train()</code></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">class</span> <span class=\"nc\">LinearClassifier</span><span class=\"p\">(</span><span class=\"nb\">object</span><span class=\"p\">):</span>\n<span class=\"err\">​</span>\n  <span class=\"k\">def</span> <span class=\"fm\">__init__</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">):</span>\n    <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">W</span> <span class=\"o\">=</span> <span class=\"bp\">None</span>\n<span class=\"err\">​</span>\n  <span class=\"k\">def</span> <span class=\"nf\">train</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span><span class=\"o\">=</span><span class=\"mf\">1e-3</span><span class=\"p\">,</span> <span class=\"n\">reg</span><span class=\"o\">=</span><span class=\"mf\">1e-5</span><span class=\"p\">,</span> <span class=\"n\">num_iters</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">,</span>\n            <span class=\"n\">batch_size</span><span class=\"o\">=</span><span class=\"mi\">200</span><span class=\"p\">,</span> <span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Train this linear classifier using stochastic gradient descent.\n</span><span class=\"s2\">​\n</span><span class=\"s2\">    Inputs:\n</span><span class=\"s2\">    - X: A numpy array of shape (N, D) containing training data; there are N\n</span><span class=\"s2\">      training samples each of dimension D.\n</span><span class=\"s2\">    - y: A numpy array of shape (N,) containing training labels; y[i] = c\n</span><span class=\"s2\">      means that X[i] has label 0 &lt;= c &lt; C for C classes.\n</span><span class=\"s2\">    - learning_rate: (float) learning rate for optimization.\n</span><span class=\"s2\">    - reg: (float) regularization strength.\n</span><span class=\"s2\">    - num_iters: (integer) number of steps to take when optimizing\n</span><span class=\"s2\">    - batch_size: (integer) number of training examples to use at each step.\n</span><span class=\"s2\">    - verbose: (boolean) If true, print progress during optimization.\n</span><span class=\"s2\">​\n</span><span class=\"s2\">    Outputs:\n</span><span class=\"s2\">    A list containing the value of the loss function at each training iteration.\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    <span class=\"n\">num_train</span><span class=\"p\">,</span> <span class=\"n\">dim</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n    <span class=\"n\">num_classes</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">max</span><span class=\"p\">(</span><span class=\"n\">y</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"mi\">1</span> <span class=\"c1\"># assume y takes values 0...K-1 where K is number of classes</span>\n    <span class=\"k\">if</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">W</span> <span class=\"ow\">is</span> <span class=\"bp\">None</span><span class=\"p\">:</span>\n      <span class=\"c1\"># lazily initialize W</span>\n      <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">W</span> <span class=\"o\">=</span> <span class=\"mf\">0.001</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">dim</span><span class=\"p\">,</span> <span class=\"n\">num_classes</span><span class=\"p\">)</span>\n<span class=\"err\">​</span>\n    <span class=\"c1\"># Run stochastic gradient descent to optimize W</span>\n    <span class=\"n\">loss_history</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n    <span class=\"k\">for</span> <span class=\"n\">it</span> <span class=\"ow\">in</span> <span class=\"nb\">xrange</span><span class=\"p\">(</span><span class=\"n\">num_iters</span><span class=\"p\">):</span>\n      <span class=\"n\">X_batch</span> <span class=\"o\">=</span> <span class=\"bp\">None</span>\n      <span class=\"n\">y_batch</span> <span class=\"o\">=</span> <span class=\"bp\">None</span>\n<span class=\"err\">​</span>\n      <span class=\"c1\">#########################################################################</span>\n      <span class=\"c1\"># TODO:                                                                 #</span>\n      <span class=\"c1\"># Sample batch_size elements from the training data and their           #</span>\n      <span class=\"c1\"># corresponding labels to use in this round of gradient descent.        #</span>\n      <span class=\"c1\"># Store the data in X_batch and their corresponding labels in           #</span>\n      <span class=\"c1\"># y_batch; after sampling X_batch should have shape (dim, batch_size)   #</span>\n      <span class=\"c1\"># and y_batch should have shape (batch_size,)                           #</span>\n      <span class=\"c1\">#                                                                       #</span>\n      <span class=\"c1\"># Hint: Use np.random.choice to generate indices. Sampling with         #</span>\n      <span class=\"c1\"># replacement is faster than sampling without replacement.              #</span>\n      <span class=\"c1\">#########################################################################</span>\n      <span class=\"n\">batch_inx</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">choice</span><span class=\"p\">(</span><span class=\"n\">num_train</span><span class=\"p\">,</span><span class=\"n\">batch_size</span><span class=\"p\">)</span>\n      <span class=\"n\">X_batch</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"p\">[</span><span class=\"n\">batch_inx</span><span class=\"p\">,:]</span>\n      <span class=\"n\">y_batch</span> <span class=\"o\">=</span> <span class=\"n\">y</span><span class=\"p\">[</span><span class=\"n\">batch_inx</span><span class=\"p\">]</span>\n      <span class=\"c1\">#########################################################################</span>\n      <span class=\"c1\">#                       END OF YOUR CODE                                #</span>\n      <span class=\"c1\">#########################################################################</span>\n<span class=\"err\">​</span>\n      <span class=\"c1\"># evaluate loss and gradient</span>\n      <span class=\"n\">loss</span><span class=\"p\">,</span> <span class=\"n\">grad</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">loss</span><span class=\"p\">(</span><span class=\"n\">X_batch</span><span class=\"p\">,</span> <span class=\"n\">y_batch</span><span class=\"p\">,</span> <span class=\"n\">reg</span><span class=\"p\">)</span>\n      <span class=\"n\">loss_history</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">loss</span><span class=\"p\">)</span>\n<span class=\"err\">​</span>\n      <span class=\"c1\"># perform parameter update</span>\n      <span class=\"c1\">#########################################################################</span>\n      <span class=\"c1\"># TODO:                                                                 #</span>\n      <span class=\"c1\"># Update the weights using the gradient and the learning rate.          #</span>\n      <span class=\"c1\">#########################################################################</span>\n      <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">W</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">W</span> <span class=\"o\">-</span> <span class=\"n\">learning_rate</span> <span class=\"o\">*</span> <span class=\"n\">grad</span>\n      <span class=\"c1\">#########################################################################</span>\n      <span class=\"c1\">#                       END OF YOUR CODE                                #</span>\n      <span class=\"c1\">#########################################################################</span>\n<span class=\"err\">​</span>\n      <span class=\"k\">if</span> <span class=\"n\">verbose</span> <span class=\"ow\">and</span> <span class=\"n\">it</span> <span class=\"o\">%</span> <span class=\"mi\">100</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n        <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;iteration </span><span class=\"si\">%d</span><span class=\"s1\"> / </span><span class=\"si\">%d</span><span class=\"s1\">: loss </span><span class=\"si\">%f</span><span class=\"s1\">&#39;</span> <span class=\"o\">%</span> <span class=\"p\">(</span><span class=\"n\">it</span><span class=\"p\">,</span> <span class=\"n\">num_iters</span><span class=\"p\">,</span> <span class=\"n\">loss</span><span class=\"p\">))</span>\n<span class=\"err\">​</span>\n    <span class=\"k\">return</span> <span class=\"n\">loss_history</span></code></pre></div><p>再编辑一下<code>predict</code>函数</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">predict</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Use the trained weights of this linear classifier to predict labels for\n</span><span class=\"s2\">    data points.\n</span><span class=\"s2\">​\n</span><span class=\"s2\">    Inputs:\n</span><span class=\"s2\">    - X: A numpy array of shape (N, D) containing training data; there are N\n</span><span class=\"s2\">      training samples each of dimension D.\n</span><span class=\"s2\">​\n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional\n</span><span class=\"s2\">      array of length N, and each element is an integer giving the predicted\n</span><span class=\"s2\">      class.\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    <span class=\"n\">y_pred</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">])</span>\n    <span class=\"c1\">###########################################################################</span>\n    <span class=\"c1\"># TODO:                                                                   #</span>\n    <span class=\"c1\"># Implement this method. Store the predicted labels in y_pred.            #</span>\n    <span class=\"c1\">###########################################################################</span>\n    <span class=\"n\">score</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">W</span><span class=\"p\">)</span>\n    <span class=\"n\">y_pred</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">argmax</span><span class=\"p\">(</span><span class=\"n\">score</span><span class=\"p\">,</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n    <span class=\"c1\">###########################################################################</span>\n    <span class=\"c1\">#                           END OF YOUR CODE                              #</span>\n    <span class=\"c1\">###########################################################################</span>\n    <span class=\"k\">return</span> <span class=\"n\">y_pred</span></code></pre></div><p>得到预测值</p><div class=\"highlight\"><pre><code class=\"language-text\">training accuracy: 0.376633\nvalidation accuracy: 0.384000</code></pre></div><p>然后调一调learning_rate和regularization:</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># Use the validation set to tune hyperparameters (regularization strength and</span>\n<span class=\"c1\"># learning rate). You should experiment with different ranges for the learning</span>\n<span class=\"c1\"># rates and regularization strengths; if you are careful you should be able to</span>\n<span class=\"c1\"># get a classification accuracy of about 0.4 on the validation set.</span>\n<span class=\"n\">learning_rates</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mf\">1e-7</span><span class=\"p\">,</span> <span class=\"mf\">3e-7</span><span class=\"p\">,</span><span class=\"mf\">5e-7</span><span class=\"p\">,</span><span class=\"mf\">9e-7</span><span class=\"p\">]</span>\n<span class=\"n\">regularization_strengths</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mf\">2.5e4</span><span class=\"p\">,</span> <span class=\"mf\">1e4</span><span class=\"p\">,</span><span class=\"mf\">3e4</span><span class=\"p\">,</span><span class=\"mf\">2e4</span><span class=\"p\">]</span>\n<span class=\"err\">​</span>\n<span class=\"c1\"># results is dictionary mapping tuples of the form</span>\n<span class=\"c1\"># (learning_rate, regularization_strength) to tuples of the form</span>\n<span class=\"c1\"># (training_accuracy, validation_accuracy). The accuracy is simply the fraction</span>\n<span class=\"c1\"># of data points that are correctly classified.</span>\n<span class=\"n\">results</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>\n<span class=\"n\">best_val</span> <span class=\"o\">=</span> <span class=\"o\">-</span><span class=\"mi\">1</span>   <span class=\"c1\"># The highest validation accuracy that we have seen so far.</span>\n<span class=\"n\">best_svm</span> <span class=\"o\">=</span> <span class=\"bp\">None</span> <span class=\"c1\"># The LinearSVM object that achieved the highest validation rate.</span>\n<span class=\"err\">​</span>\n<span class=\"c1\">################################################################################</span>\n<span class=\"c1\"># TODO:                                                                        #</span>\n<span class=\"c1\"># Write code that chooses the best hyperparameters by tuning on the validation #</span>\n<span class=\"c1\"># set. For each combination of hyperparameters, train a linear SVM on the      #</span>\n<span class=\"c1\"># training set, compute its accuracy on the training and validation sets, and  #</span>\n<span class=\"c1\"># store these numbers in the results dictionary. In addition, store the best   #</span>\n<span class=\"c1\"># validation accuracy in best_val and the LinearSVM object that achieves this  #</span>\n<span class=\"c1\"># accuracy in best_svm.                                                        #</span>\n<span class=\"c1\">#                                                                              #</span>\n<span class=\"c1\"># Hint: You should use a small value for num_iters as you develop your         #</span>\n<span class=\"c1\"># validation code so that the SVMs don&#39;t take much time to train; once you are #</span>\n<span class=\"c1\"># confident that your validation code works, you should rerun the validation   #</span>\n<span class=\"c1\"># code with a larger value for num_iters.                                      #</span>\n<span class=\"c1\">################################################################################</span>\n<span class=\"k\">for</span> <span class=\"n\">learning_rate</span> <span class=\"ow\">in</span> <span class=\"n\">learning_rates</span><span class=\"p\">:</span>\n    <span class=\"k\">for</span> <span class=\"n\">regularization_strength</span> <span class=\"ow\">in</span> <span class=\"n\">regularization_strengths</span><span class=\"p\">:</span>\n        <span class=\"n\">svm</span> <span class=\"o\">=</span> <span class=\"n\">LinearSVM</span><span class=\"p\">()</span>\n        <span class=\"n\">loss_hist</span> <span class=\"o\">=</span> <span class=\"n\">svm</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">,</span> <span class=\"n\">y_train</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span><span class=\"o\">=</span><span class=\"n\">learning_rate</span><span class=\"p\">,</span> <span class=\"n\">reg</span><span class=\"o\">=</span><span class=\"n\">regularization_strength</span><span class=\"p\">,</span>\n                      <span class=\"n\">num_iters</span><span class=\"o\">=</span><span class=\"mi\">1500</span><span class=\"p\">,</span> <span class=\"n\">verbose</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n        <span class=\"n\">y_train_pred</span> <span class=\"o\">=</span> <span class=\"n\">svm</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">)</span>\n        <span class=\"n\">y_val_pred</span> <span class=\"o\">=</span> <span class=\"n\">svm</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">X_val</span><span class=\"p\">)</span>\n        <span class=\"n\">y_train_acc</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">y_train_pred</span><span class=\"o\">==</span><span class=\"n\">y_train</span><span class=\"p\">)</span>\n        <span class=\"n\">y_val_acc</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">y_val_pred</span><span class=\"o\">==</span><span class=\"n\">y_val</span><span class=\"p\">)</span>\n        <span class=\"n\">results</span><span class=\"p\">[(</span><span class=\"n\">learning_rate</span><span class=\"p\">,</span><span class=\"n\">regularization_strength</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">y_train_acc</span><span class=\"p\">,</span> <span class=\"n\">y_val_acc</span><span class=\"p\">]</span>\n        <span class=\"k\">if</span> <span class=\"n\">y_val_acc</span> <span class=\"o\">&gt;</span> <span class=\"n\">best_val</span><span class=\"p\">:</span>\n            <span class=\"n\">best_val</span> <span class=\"o\">=</span> <span class=\"n\">y_val_acc</span>\n            <span class=\"n\">best_svm</span> <span class=\"o\">=</span> <span class=\"n\">svm</span>\n<span class=\"err\">​</span>\n<span class=\"c1\">################################################################################</span>\n<span class=\"c1\">#                              END OF YOUR CODE                                #</span>\n<span class=\"c1\">################################################################################</span>\n    \n<span class=\"c1\"># Print out results.</span>\n<span class=\"k\">for</span> <span class=\"n\">lr</span><span class=\"p\">,</span> <span class=\"n\">reg</span> <span class=\"ow\">in</span> <span class=\"nb\">sorted</span><span class=\"p\">(</span><span class=\"n\">results</span><span class=\"p\">):</span>\n    <span class=\"n\">train_accuracy</span><span class=\"p\">,</span> <span class=\"n\">val_accuracy</span> <span class=\"o\">=</span> <span class=\"n\">results</span><span class=\"p\">[(</span><span class=\"n\">lr</span><span class=\"p\">,</span> <span class=\"n\">reg</span><span class=\"p\">)]</span>\n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;lr </span><span class=\"si\">%e</span><span class=\"s1\"> reg </span><span class=\"si\">%e</span><span class=\"s1\"> train accuracy: </span><span class=\"si\">%f</span><span class=\"s1\"> val accuracy: </span><span class=\"si\">%f</span><span class=\"s1\">&#39;</span> <span class=\"o\">%</span> <span class=\"p\">(</span>\n                <span class=\"n\">lr</span><span class=\"p\">,</span> <span class=\"n\">reg</span><span class=\"p\">,</span> <span class=\"n\">train_accuracy</span><span class=\"p\">,</span> <span class=\"n\">val_accuracy</span><span class=\"p\">))</span>\n    \n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;best validation accuracy achieved during cross-validation: </span><span class=\"si\">%f</span><span class=\"s1\">&#39;</span> <span class=\"o\">%</span> <span class=\"n\">best_val</span><span class=\"p\">)</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><b>小结</b></p><ul><li>多看看cs231n的note文档</li><li>多学习学习grad的推倒</li></ul>", 
            "topic": [
                {
                    "tag": "SVM", 
                    "tagLink": "https://api.zhihu.com/topics/19583524"
                }, 
                {
                    "tag": "作业", 
                    "tagLink": "https://api.zhihu.com/topics/19586372"
                }
            ], 
            "comments": [
                {
                    "userName": "李艺海", 
                    "userLink": "https://www.zhihu.com/people/3c4b27eef62a1a6c0217e19d7fb21deb", 
                    "content": "<p>那个推SVM的梯度链接没了。。。。自己慢慢搞</p>", 
                    "likes": 1, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/45753372", 
            "userName": "直上云霄", 
            "userLink": "https://www.zhihu.com/people/1033165ce4ad9c3fce69a0793dfab8ad", 
            "upvote": 5, 
            "title": "cs231n作业：assignment1 - knn", 
            "content": "<div class=\"highlight\"><pre><code class=\"language-text\">title: cs231n作业：assignment1 - knn\nid: cs231n-1h-1\ntags:\n  - cs231n\n  - homework\ncategories:\n  - AI\n  - Deep Learning\ndate: 2018-09-26 12:41:15\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>GitHub地址：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/ZJUFangzh/cs231n\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/ZJUFangzh/cs</span><span class=\"invisible\">231n</span><span class=\"ellipsis\"></span></a></p><p class=\"ztext-empty-paragraph\"><br/></p><p>首发于个人博客: <a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Fangzh的个人博客 | 人工智能拯救世界</a> 欢迎来访</p><p class=\"ztext-empty-paragraph\"><br/></p><p>使用KNN算法来完成图像识别，数据集用的是cifar10。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>首先看一下数据集的维度</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># Load the raw CIFAR-10 data.</span>\n<span class=\"n\">cifar10_dir</span> <span class=\"o\">=</span> <span class=\"s1\">&#39;cs231n/datasets/cifar-10-batches-py&#39;</span>\n<span class=\"n\">X_train</span><span class=\"p\">,</span> <span class=\"n\">y_train</span><span class=\"p\">,</span> <span class=\"n\">X_test</span><span class=\"p\">,</span> <span class=\"n\">y_test</span> <span class=\"o\">=</span> <span class=\"n\">load_CIFAR10</span><span class=\"p\">(</span><span class=\"n\">cifar10_dir</span><span class=\"p\">)</span>\n<span class=\"err\">​</span>\n<span class=\"c1\"># As a sanity check, we print out the size of the training and test data.</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;Training data shape: &#39;</span><span class=\"p\">,</span> <span class=\"n\">X_train</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;Training labels shape: &#39;</span><span class=\"p\">,</span> <span class=\"n\">y_train</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;Test data shape: &#39;</span><span class=\"p\">,</span> <span class=\"n\">X_test</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;Test labels shape: &#39;</span><span class=\"p\">,</span> <span class=\"n\">y_test</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span></code></pre></div><p>可以看到，每一张图片是 <img src=\"https://www.zhihu.com/equation?tex=32%C3%9732%C3%973\" alt=\"32×32×3\" eeimg=\"1\"/> ，训练集有50000张，测试集有10000张</p><div class=\"highlight\"><pre><code class=\"language-text\">Training data shape:  (50000, 32, 32, 3)\nTraining labels shape:  (50000,)\nTest data shape:  (10000, 32, 32, 3)\nTest labels shape:  (10000,)</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>为了更够更快的计算，就选5000张做训练，500张做测试就好了</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># Subsample the data for more efficient code execution in this exercise</span>\n<span class=\"n\">num_training</span> <span class=\"o\">=</span> <span class=\"mi\">5000</span>\n<span class=\"n\">mask</span> <span class=\"o\">=</span> <span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">num_training</span><span class=\"p\">))</span>\n<span class=\"n\">X_train</span> <span class=\"o\">=</span> <span class=\"n\">X_train</span><span class=\"p\">[</span><span class=\"n\">mask</span><span class=\"p\">]</span>\n<span class=\"n\">y_train</span> <span class=\"o\">=</span> <span class=\"n\">y_train</span><span class=\"p\">[</span><span class=\"n\">mask</span><span class=\"p\">]</span>\n<span class=\"err\">​</span>\n<span class=\"n\">num_test</span> <span class=\"o\">=</span> <span class=\"mi\">500</span>\n<span class=\"n\">mask</span> <span class=\"o\">=</span> <span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">num_test</span><span class=\"p\">))</span>\n<span class=\"n\">X_test</span> <span class=\"o\">=</span> <span class=\"n\">X_test</span><span class=\"p\">[</span><span class=\"n\">mask</span><span class=\"p\">]</span>\n<span class=\"n\">y_test</span> <span class=\"o\">=</span> <span class=\"n\">y_test</span><span class=\"p\">[</span><span class=\"n\">mask</span><span class=\"p\">]</span></code></pre></div><p>而后把像素拉成3072的行向量</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># Reshape the image data into rows</span>\n<span class=\"n\">X_train</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">))</span>\n<span class=\"n\">X_test</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"n\">X_test</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">X_test</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">))</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">,</span> <span class=\"n\">X_test</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>因为knn不需要训练，所以先存入数据：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"kn\">from</span> <span class=\"nn\">cs231n.classifiers</span> <span class=\"kn\">import</span> <span class=\"n\">KNearestNeighbor</span>\n<span class=\"err\">​</span>\n<span class=\"c1\"># Create a kNN classifier instance. </span>\n<span class=\"c1\"># Remember that training a kNN classifier is a noop: </span>\n<span class=\"c1\"># the Classifier simply remembers the data and does no further processing </span>\n<span class=\"n\">classifier</span> <span class=\"o\">=</span> <span class=\"n\">KNearestNeighbor</span><span class=\"p\">()</span>\n<span class=\"n\">classifier</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">,</span> <span class=\"n\">y_train</span><span class=\"p\">)</span></code></pre></div><p>然后要修改<code>k_nearest_neighbor.py</code>中的<code>compute_distances_two_loops</code></p><p>这里套了两层循环，也就是比较训练集和测试集的每一张图片的间距：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">compute_distances_two_loops</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Compute the distance between each test point in X and each training point\n</span><span class=\"s2\">    in self.X_train using a nested loop over both the training data and the \n</span><span class=\"s2\">    test data.\n</span><span class=\"s2\">​\n</span><span class=\"s2\">    Inputs:\n</span><span class=\"s2\">    - X: A numpy array of shape (num_test, D) containing test data.\n</span><span class=\"s2\">​\n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    - dists: A numpy array of shape (num_test, num_train) where dists[i, j]\n</span><span class=\"s2\">      is the Euclidean distance between the ith test point and the jth training\n</span><span class=\"s2\">      point.\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    <span class=\"n\">num_test</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n    <span class=\"n\">num_train</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">X_train</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n    <span class=\"n\">dists</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">num_test</span><span class=\"p\">,</span> <span class=\"n\">num_train</span><span class=\"p\">))</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">xrange</span><span class=\"p\">(</span><span class=\"n\">num_test</span><span class=\"p\">):</span>\n      <span class=\"k\">for</span> <span class=\"n\">j</span> <span class=\"ow\">in</span> <span class=\"nb\">xrange</span><span class=\"p\">(</span><span class=\"n\">num_train</span><span class=\"p\">):</span>\n        <span class=\"c1\">#####################################################################</span>\n        <span class=\"c1\"># TODO:                                                             #</span>\n        <span class=\"c1\"># Compute the l2 distance between the ith test point and the jth    #</span>\n        <span class=\"c1\"># training point, and store the result in dists[i, j]. You should   #</span>\n        <span class=\"c1\"># not use a loop over dimension.                                    #</span>\n        <span class=\"c1\">#####################################################################</span>\n        <span class=\"n\">dists</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">][</span><span class=\"n\">j</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sqrt</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">square</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">,:]</span> <span class=\"o\">-</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">X_train</span><span class=\"p\">[</span><span class=\"n\">j</span><span class=\"p\">,:])))</span>\n        <span class=\"c1\">#####################################################################</span>\n        <span class=\"c1\">#                       END OF YOUR CODE                            #</span>\n        <span class=\"c1\">#####################################################################</span>\n    <span class=\"k\">return</span> <span class=\"n\">dists</span></code></pre></div><p>得到了一个$(500,5000)$的dists矩阵。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>然后修改<code>predict_labels</code>函数</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">predict_labels</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">dists</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Given a matrix of distances between test points and training points,\n</span><span class=\"s2\">    predict a label for each test point.\n</span><span class=\"s2\">​\n</span><span class=\"s2\">    Inputs:\n</span><span class=\"s2\">    - dists: A numpy array of shape (num_test, num_train) where dists[i, j]\n</span><span class=\"s2\">      gives the distance betwen the ith test point and the jth training point.\n</span><span class=\"s2\">​\n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    - y: A numpy array of shape (num_test,) containing predicted labels for the\n</span><span class=\"s2\">      test data, where y[i] is the predicted label for the test point X[i].  \n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    <span class=\"n\">num_test</span> <span class=\"o\">=</span> <span class=\"n\">dists</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n    <span class=\"n\">y_pred</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"n\">num_test</span><span class=\"p\">)</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">xrange</span><span class=\"p\">(</span><span class=\"n\">num_test</span><span class=\"p\">):</span>\n      <span class=\"c1\"># A list of length k storing the labels of the k nearest neighbors to</span>\n      <span class=\"c1\"># the ith test point.</span>\n      <span class=\"n\">closest_y</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n      <span class=\"c1\">#########################################################################</span>\n      <span class=\"c1\"># TODO:                                                                 #</span>\n      <span class=\"c1\"># Use the distance matrix to find the k nearest neighbors of the ith    #</span>\n      <span class=\"c1\"># testing point, and use self.y_train to find the labels of these       #</span>\n      <span class=\"c1\"># neighbors. Store these labels in closest_y.                           #</span>\n      <span class=\"c1\"># Hint: Look up the function numpy.argsort.                             #</span>\n      <span class=\"c1\">#########################################################################</span>\n      <span class=\"c1\">#找到每一个测试图片中对应的5000张训练集图片，距离最近的前k个</span>\n      <span class=\"n\">closest_y</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">y_train</span><span class=\"p\">[</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">argsort</span><span class=\"p\">(</span><span class=\"n\">dists</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">])[:</span><span class=\"n\">k</span><span class=\"p\">]]</span>\n      <span class=\"c1\">#########################################################################</span>\n      <span class=\"c1\"># TODO:                                                                 #</span>\n      <span class=\"c1\"># Now that you have found the labels of the k nearest neighbors, you    #</span>\n      <span class=\"c1\"># need to find the most common label in the list closest_y of labels.   #</span>\n      <span class=\"c1\"># Store this label in y_pred[i]. Break ties by choosing the smaller     #</span>\n      <span class=\"c1\"># label.                                                                #</span>\n      <span class=\"c1\">#########################################################################</span>\n      <span class=\"c1\">#然后将这K个图片进行投票，得票数最多的就是预测值</span>\n      <span class=\"n\">y_pred</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">argmax</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">bincount</span><span class=\"p\">(</span><span class=\"n\">closest_y</span><span class=\"p\">))</span>\n      <span class=\"c1\">#########################################################################</span>\n      <span class=\"c1\">#                           END OF YOUR CODE                            # </span>\n      <span class=\"c1\">#########################################################################</span>\n<span class=\"err\">​</span>\n    <span class=\"k\">return</span> <span class=\"n\">y_pred</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>预测一下：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># Now implement the function predict_labels and run the code below:</span>\n<span class=\"c1\"># We use k = 1 (which is Nearest Neighbor).</span>\n<span class=\"n\">y_test_pred</span> <span class=\"o\">=</span> <span class=\"n\">classifier</span><span class=\"o\">.</span><span class=\"n\">predict_labels</span><span class=\"p\">(</span><span class=\"n\">dists</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"err\">​</span>\n<span class=\"c1\"># Compute and print the fraction of correctly predicted examples</span>\n<span class=\"n\">num_correct</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">y_test_pred</span> <span class=\"o\">==</span> <span class=\"n\">y_test</span><span class=\"p\">)</span>\n<span class=\"n\">accuracy</span> <span class=\"o\">=</span> <span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"n\">num_correct</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">num_test</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;Got </span><span class=\"si\">%d</span><span class=\"s1\"> / </span><span class=\"si\">%d</span><span class=\"s1\"> correct =&gt; accuracy: </span><span class=\"si\">%f</span><span class=\"s1\">&#39;</span> <span class=\"o\">%</span> <span class=\"p\">(</span><span class=\"n\">num_correct</span><span class=\"p\">,</span> <span class=\"n\">num_test</span><span class=\"p\">,</span> <span class=\"n\">accuracy</span><span class=\"p\">))</span></code></pre></div><p>结果是0.274</p><p>再试试k=5的结果，是0.278</p><p>然后再修改<code>compute_distances_one_loop</code>函数，这次争取只用一个循环</p><div class=\"highlight\"><pre><code class=\"language-python\">  <span class=\"k\">def</span> <span class=\"nf\">compute_distances_one_loop</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Compute the distance between each test point in X and each training point\n</span><span class=\"s2\">    in self.X_train using a single loop over the test data.\n</span><span class=\"s2\">​\n</span><span class=\"s2\">    Input / Output: Same as compute_distances_two_loops\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    <span class=\"n\">num_test</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n    <span class=\"n\">num_train</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">X_train</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n    <span class=\"n\">dists</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">num_test</span><span class=\"p\">,</span> <span class=\"n\">num_train</span><span class=\"p\">))</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">xrange</span><span class=\"p\">(</span><span class=\"n\">num_test</span><span class=\"p\">):</span>\n      <span class=\"c1\">#######################################################################</span>\n      <span class=\"c1\"># TODO:                                                               #</span>\n      <span class=\"c1\"># Compute the l2 distance between the ith test point and all training #</span>\n      <span class=\"c1\"># points, and store the result in dists[i, :].                        #</span>\n      <span class=\"c1\">#######################################################################</span>\n      <span class=\"c1\">#利用python的广播，一次性算出每一张图片与5000张图片的距离</span>\n      <span class=\"n\">dists</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"p\">:]</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sqrt</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">square</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">X_train</span> <span class=\"o\">-</span> <span class=\"n\">X</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"p\">:]),</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">))</span>\n      <span class=\"c1\">#######################################################################</span>\n      <span class=\"c1\">#                         END OF YOUR CODE                            #</span>\n      <span class=\"c1\">#######################################################################</span>\n    <span class=\"k\">return</span> <span class=\"n\">dists</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>验证一下间距是</p><div class=\"highlight\"><pre><code class=\"language-text\">Difference was: 0.000000\nGood! The distance matrices are the same</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>然后争取不用循环<code>compute_distances_no_loops</code>，这一步比较难，想法是利用平方差公式$(x-y)^2 = x^2 + y^2 - 2xy$，使用矩阵乘法和二次广播，直接算出距离，注意矩阵的维度</p><div class=\"highlight\"><pre><code class=\"language-python\">  <span class=\"k\">def</span> <span class=\"nf\">compute_distances_no_loops</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Compute the distance between each test point in X and each training point\n</span><span class=\"s2\">    in self.X_train using no explicit loops.\n</span><span class=\"s2\">​\n</span><span class=\"s2\">    Input / Output: Same as compute_distances_two_loops\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    <span class=\"n\">num_test</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n    <span class=\"n\">num_train</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">X_train</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n    <span class=\"n\">dists</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">num_test</span><span class=\"p\">,</span> <span class=\"n\">num_train</span><span class=\"p\">))</span> \n    <span class=\"c1\">#########################################################################</span>\n    <span class=\"c1\"># TODO:                                                                 #</span>\n    <span class=\"c1\"># Compute the l2 distance between all test points and all training      #</span>\n    <span class=\"c1\"># points without using any explicit loops, and store the result in      #</span>\n    <span class=\"c1\"># dists.                                                                #</span>\n    <span class=\"c1\">#                                                                       #</span>\n    <span class=\"c1\"># You should implement this function using only basic array operations; #</span>\n    <span class=\"c1\"># in particular you should not use functions from scipy.                #</span>\n    <span class=\"c1\">#                                                                       #</span>\n    <span class=\"c1\"># HINT: Try to formulate the l2 distance using matrix multiplication    #</span>\n    <span class=\"c1\">#       and two broadcast sums.                                         #</span>\n    <span class=\"c1\">#########################################################################</span>\n    <span class=\"n\">temp_2xy</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">X_train</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n    <span class=\"n\">temp_x2</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">square</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">),</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"n\">keepdims</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n    <span class=\"n\">temp_y2</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">square</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">X_train</span><span class=\"p\">),</span><span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n    <span class=\"n\">dists</span> <span class=\"o\">=</span> <span class=\"n\">temp_x2</span> <span class=\"o\">+</span> <span class=\"n\">temp_2xy</span> <span class=\"o\">+</span> <span class=\"n\">temp_y2</span>\n    <span class=\"n\">dists</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sqrt</span><span class=\"p\">(</span><span class=\"n\">dists</span><span class=\"p\">)</span>\n    <span class=\"c1\">#########################################################################</span>\n    <span class=\"c1\">#                         END OF YOUR CODE                              #</span>\n    <span class=\"c1\">#########################################################################</span>\n    <span class=\"k\">return</span> <span class=\"n\">dists</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>对比一下三种方法的时间，我这里不知道为什么two比one短，理论上是循环越少时间越短：</p><div class=\"highlight\"><pre><code class=\"language-text\">Two loop version took 24.510484 seconds\nOne loop version took 56.412211 seconds\nNo loop version took 0.183508 seconds</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><b>交叉验证</b></p><p>用交叉验证来找到最好的k</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">num_folds</span> <span class=\"o\">=</span> <span class=\"mi\">5</span>\n<span class=\"n\">k_choices</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">12</span><span class=\"p\">,</span> <span class=\"mi\">15</span><span class=\"p\">,</span> <span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"mi\">50</span><span class=\"p\">,</span> <span class=\"mi\">100</span><span class=\"p\">]</span>\n<span class=\"err\">​</span>\n<span class=\"n\">X_train_folds</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n<span class=\"n\">y_train_folds</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n<span class=\"c1\">################################################################################</span>\n<span class=\"c1\"># TODO:                                                                        #</span>\n<span class=\"c1\"># Split up the training data into folds. After splitting, X_train_folds and    #</span>\n<span class=\"c1\"># y_train_folds should each be lists of length num_folds, where                #</span>\n<span class=\"c1\"># y_train_folds[i] is the label vector for the points in X_train_folds[i].     #</span>\n<span class=\"c1\"># Hint: Look up the numpy array_split function.                                #</span>\n<span class=\"c1\">################################################################################</span>\n<span class=\"n\">X_train_folds</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array_split</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">,</span> <span class=\"n\">num_folds</span><span class=\"p\">)</span>\n<span class=\"n\">y_train_folds</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array_split</span><span class=\"p\">(</span><span class=\"n\">y_train</span><span class=\"p\">,</span> <span class=\"n\">num_folds</span><span class=\"p\">)</span>\n<span class=\"err\">​</span>\n<span class=\"err\">​</span>\n<span class=\"c1\">################################################################################</span>\n<span class=\"c1\">#                                 END OF YOUR CODE                             #</span>\n<span class=\"c1\">################################################################################</span>\n<span class=\"err\">​</span>\n<span class=\"c1\"># A dictionary holding the accuracies for different values of k that we find</span>\n<span class=\"c1\"># when running cross-validation. After running cross-validation,</span>\n<span class=\"c1\"># k_to_accuracies[k] should be a list of length num_folds giving the different</span>\n<span class=\"c1\"># accuracy values that we found when using that value of k.</span>\n<span class=\"n\">k_to_accuracies</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>\n<span class=\"err\">​</span>\n<span class=\"err\">​</span>\n<span class=\"c1\">################################################################################</span>\n<span class=\"c1\"># TODO:                                                                        #</span>\n<span class=\"c1\"># Perform k-fold cross validation to find the best value of k. For each        #</span>\n<span class=\"c1\"># possible value of k, run the k-nearest-neighbor algorithm num_folds times,   #</span>\n<span class=\"c1\"># where in each case you use all but one of the folds as training data and the #</span>\n<span class=\"c1\"># last fold as a validation set. Store the accuracies for all fold and all     #</span>\n<span class=\"c1\"># values of k in the k_to_accuracies dictionary.                               #</span>\n<span class=\"c1\">################################################################################</span>\n<span class=\"n\">classifier</span> <span class=\"o\">=</span> <span class=\"n\">KNearestNeighbor</span><span class=\"p\">()</span>\n<span class=\"k\">for</span> <span class=\"n\">k</span> <span class=\"ow\">in</span> <span class=\"n\">k_choices</span><span class=\"p\">:</span>\n    <span class=\"n\">accuracies</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n    <span class=\"k\">for</span> <span class=\"n\">fold</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">num_folds</span><span class=\"p\">):</span>\n        <span class=\"n\">temp_X</span> <span class=\"o\">=</span> <span class=\"n\">X_train_folds</span><span class=\"p\">[:]</span>\n        <span class=\"n\">temp_y</span> <span class=\"o\">=</span> <span class=\"n\">y_train_folds</span><span class=\"p\">[:]</span>\n        <span class=\"n\">X_val_fold</span> <span class=\"o\">=</span> <span class=\"n\">temp_X</span><span class=\"o\">.</span><span class=\"n\">pop</span><span class=\"p\">(</span><span class=\"n\">fold</span><span class=\"p\">)</span>\n        <span class=\"n\">y_val_fold</span> <span class=\"o\">=</span> <span class=\"n\">temp_y</span><span class=\"o\">.</span><span class=\"n\">pop</span><span class=\"p\">(</span><span class=\"n\">fold</span><span class=\"p\">)</span>\n        <span class=\"n\">temp_X</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"n\">y</span> <span class=\"k\">for</span> <span class=\"n\">x</span> <span class=\"ow\">in</span> <span class=\"n\">temp_X</span> <span class=\"k\">for</span> <span class=\"n\">y</span> <span class=\"ow\">in</span> <span class=\"n\">x</span><span class=\"p\">])</span>\n        <span class=\"n\">temp_y</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"n\">y</span> <span class=\"k\">for</span> <span class=\"n\">x</span> <span class=\"ow\">in</span> <span class=\"n\">temp_y</span> <span class=\"k\">for</span> <span class=\"n\">y</span> <span class=\"ow\">in</span> <span class=\"n\">x</span><span class=\"p\">])</span>\n        <span class=\"n\">classifier</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"p\">(</span><span class=\"n\">temp_X</span><span class=\"p\">,</span><span class=\"n\">temp_y</span><span class=\"p\">)</span>\n        <span class=\"n\">y_val_pred</span> <span class=\"o\">=</span> <span class=\"n\">classifier</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">X_val_fold</span><span class=\"p\">,</span><span class=\"n\">k</span><span class=\"o\">=</span><span class=\"n\">k</span><span class=\"p\">)</span>\n        <span class=\"n\">num_correct</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">y_val_fold</span> <span class=\"o\">==</span> <span class=\"n\">y_val_pred</span><span class=\"p\">)</span>\n        <span class=\"n\">accuracies</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">num_correct</span> <span class=\"o\">/</span> <span class=\"n\">y_val_fold</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">])</span>\n    <span class=\"n\">k_to_accuracies</span><span class=\"p\">[</span><span class=\"n\">k</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">accuracies</span>\n    \n<span class=\"c1\">################################################################################</span>\n<span class=\"c1\">#                                 END OF YOUR CODE                             #</span>\n<span class=\"c1\">################################################################################</span>\n<span class=\"err\">​</span>\n<span class=\"c1\"># Print out the computed accuracies</span>\n<span class=\"k\">for</span> <span class=\"n\">k</span> <span class=\"ow\">in</span> <span class=\"nb\">sorted</span><span class=\"p\">(</span><span class=\"n\">k_to_accuracies</span><span class=\"p\">):</span>\n    <span class=\"k\">for</span> <span class=\"n\">accuracy</span> <span class=\"ow\">in</span> <span class=\"n\">k_to_accuracies</span><span class=\"p\">[</span><span class=\"n\">k</span><span class=\"p\">]:</span>\n        <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;k = </span><span class=\"si\">%d</span><span class=\"s1\">, accuracy = </span><span class=\"si\">%f</span><span class=\"s1\">&#39;</span> <span class=\"o\">%</span> <span class=\"p\">(</span><span class=\"n\">k</span><span class=\"p\">,</span> <span class=\"n\">accuracy</span><span class=\"p\">))</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>画个图：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># plot the raw observations</span>\n<span class=\"k\">for</span> <span class=\"n\">k</span> <span class=\"ow\">in</span> <span class=\"n\">k_choices</span><span class=\"p\">:</span>\n    <span class=\"n\">accuracies</span> <span class=\"o\">=</span> <span class=\"n\">k_to_accuracies</span><span class=\"p\">[</span><span class=\"n\">k</span><span class=\"p\">]</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">scatter</span><span class=\"p\">([</span><span class=\"n\">k</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">accuracies</span><span class=\"p\">),</span> <span class=\"n\">accuracies</span><span class=\"p\">)</span>\n<span class=\"err\">​</span>\n<span class=\"c1\"># plot the trend line with error bars that correspond to standard deviation</span>\n<span class=\"n\">accuracies_mean</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">v</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">k</span><span class=\"p\">,</span><span class=\"n\">v</span> <span class=\"ow\">in</span> <span class=\"nb\">sorted</span><span class=\"p\">(</span><span class=\"n\">k_to_accuracies</span><span class=\"o\">.</span><span class=\"n\">items</span><span class=\"p\">())])</span>\n<span class=\"n\">accuracies_std</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">std</span><span class=\"p\">(</span><span class=\"n\">v</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">k</span><span class=\"p\">,</span><span class=\"n\">v</span> <span class=\"ow\">in</span> <span class=\"nb\">sorted</span><span class=\"p\">(</span><span class=\"n\">k_to_accuracies</span><span class=\"o\">.</span><span class=\"n\">items</span><span class=\"p\">())])</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">errorbar</span><span class=\"p\">(</span><span class=\"n\">k_choices</span><span class=\"p\">,</span> <span class=\"n\">accuracies_mean</span><span class=\"p\">,</span> <span class=\"n\">yerr</span><span class=\"o\">=</span><span class=\"n\">accuracies_std</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">title</span><span class=\"p\">(</span><span class=\"s1\">&#39;Cross-validation on k&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">xlabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;k&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">ylabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;Cross-validation accuracy&#39;</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-bc0c3cf3949efec1defab4cad29a88e2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"395\" data-rawheight=\"278\" class=\"content_image\" width=\"395\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;395&#39; height=&#39;278&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"395\" data-rawheight=\"278\" class=\"content_image lazy\" width=\"395\" data-actualsrc=\"https://pic3.zhimg.com/v2-bc0c3cf3949efec1defab4cad29a88e2_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># Based on the cross-validation results above, choose the best value for k,   </span>\n<span class=\"c1\"># retrain the classifier using all the training data, and test it on the test</span>\n<span class=\"c1\"># data. You should be able to get above 28% accuracy on the test data.</span>\n<span class=\"n\">best_k</span> <span class=\"o\">=</span> <span class=\"mi\">10</span>\n<span class=\"err\">​</span>\n<span class=\"n\">classifier</span> <span class=\"o\">=</span> <span class=\"n\">KNearestNeighbor</span><span class=\"p\">()</span>\n<span class=\"n\">classifier</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">,</span> <span class=\"n\">y_train</span><span class=\"p\">)</span>\n<span class=\"n\">y_test_pred</span> <span class=\"o\">=</span> <span class=\"n\">classifier</span><span class=\"o\">.</span><span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">X_test</span><span class=\"p\">,</span> <span class=\"n\">k</span><span class=\"o\">=</span><span class=\"n\">best_k</span><span class=\"p\">)</span>\n<span class=\"err\">​</span>\n<span class=\"c1\"># Compute and display the accuracy</span>\n<span class=\"n\">num_correct</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">y_test_pred</span> <span class=\"o\">==</span> <span class=\"n\">y_test</span><span class=\"p\">)</span>\n<span class=\"n\">accuracy</span> <span class=\"o\">=</span> <span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"n\">num_correct</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">num_test</span>\n<span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s1\">&#39;Got </span><span class=\"si\">%d</span><span class=\"s1\"> / </span><span class=\"si\">%d</span><span class=\"s1\"> correct =&gt; accuracy: </span><span class=\"si\">%f</span><span class=\"s1\">&#39;</span> <span class=\"o\">%</span> <span class=\"p\">(</span><span class=\"n\">num_correct</span><span class=\"p\">,</span> <span class=\"n\">num_test</span><span class=\"p\">,</span> <span class=\"n\">accuracy</span><span class=\"p\">))</span></code></pre></div><p>得到最好的k=10，准确率是0.282</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>小结</b></p><ul><li>cs231n的作业比DeepLearning.ai的难多了，不是一个档次的，关键是提示比较少，所以自己做起来比较费劲</li><li>主要要学会向量化的运算，少用loop循环</li><li>knn已经被淘汰了，这个作业只是让我们入门看看图像识别大概怎么做</li></ul><p></p>", 
            "topic": [
                {
                    "tag": "作业", 
                    "tagLink": "https://api.zhihu.com/topics/19586372"
                }
            ], 
            "comments": [
                {
                    "userName": "Jimmythebigmouth", 
                    "userLink": "https://www.zhihu.com/people/d309a3867f34bce1dd18250e3f25294c", 
                    "content": "<p>感谢</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "byfate", 
                    "userLink": "https://www.zhihu.com/people/fc62aa4ac3c34f88500f093495a83537", 
                    "content": "<p>你好 请问为什么最小的是10</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "zephyr", 
                    "userLink": "https://www.zhihu.com/people/28a6efc5087751aa8fc2504484451162", 
                    "content": "nice[酷]", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "百里屠苏", 
                    "userLink": "https://www.zhihu.com/people/ea74a3b9adafcabba2f4af0d3667a4b4", 
                    "content": "<p>temp_X = np.array([y for x in temp_X for y in x])</p><p>temp_y = np.array([y for x in temp_y for y in x])</p><p>您好，请问这里array里面的语句[y for x in temp_X for y in x]是什么意思？</p>", 
                    "likes": 1, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/45752897", 
            "userName": "直上云霄", 
            "userLink": "https://www.zhihu.com/people/1033165ce4ad9c3fce69a0793dfab8ad", 
            "upvote": 0, 
            "title": "DeepLearning.ai作业:(4-1)-- 卷积神经网络(part2)", 
            "content": "<h2><b>Part2：Convolutional Neural Networks: Application</b></h2><p>用TensorFlow来搭建卷积神经网络。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>1.Create placeholders</b></h2><p>先创建placeholders，用来训练中传递X,Y</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"err\">​</span>\n<span class=\"k\">def</span> <span class=\"nf\">create_placeholders</span><span class=\"p\">(</span><span class=\"n\">n_H0</span><span class=\"p\">,</span> <span class=\"n\">n_W0</span><span class=\"p\">,</span> <span class=\"n\">n_C0</span><span class=\"p\">,</span> <span class=\"n\">n_y</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Creates the placeholders for the tensorflow session.\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    n_H0 -- scalar, height of an input image\n</span><span class=\"s2\">    n_W0 -- scalar, width of an input image\n</span><span class=\"s2\">    n_C0 -- scalar, number of channels of the input\n</span><span class=\"s2\">    n_y -- scalar, number of classes\n</span><span class=\"s2\">        \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    X -- placeholder for the data input, of shape [None, n_H0, n_W0, n_C0] and dtype &#34;float&#34;\n</span><span class=\"s2\">    Y -- placeholder for the input labels, of shape [None, n_y] and dtype &#34;float&#34;\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n<span class=\"err\">​</span>\n    <span class=\"c1\">### START CODE HERE ### (≈2 lines)</span>\n    <span class=\"n\">X</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">placeholder</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">,</span> <span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"bp\">None</span><span class=\"p\">,</span><span class=\"n\">n_H0</span><span class=\"p\">,</span> <span class=\"n\">n_W0</span><span class=\"p\">,</span> <span class=\"n\">n_C0</span><span class=\"p\">))</span>\n    <span class=\"n\">Y</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">placeholder</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">,</span> <span class=\"n\">shape</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"bp\">None</span><span class=\"p\">,</span><span class=\"n\">n_y</span><span class=\"p\">))</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>2.Initialize parameters</b></h2><p>用来初始化参数，主要是W1,W2,在这里就没有用b了</p><p>用<code>W = tf.get_variable(&#34;W&#34;, [1,2,3,4], initializer = ...)</code></p><p>initializer 用<code>tf.contrib.layers.xavier_initializer</code></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: initialize_parameters</span>\n<span class=\"err\">​</span>\n<span class=\"k\">def</span> <span class=\"nf\">initialize_parameters</span><span class=\"p\">():</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Initializes weight parameters to build a neural network with tensorflow. The shapes are:\n</span><span class=\"s2\">                        W1 : [4, 4, 3, 8]\n</span><span class=\"s2\">                        W2 : [2, 2, 8, 16]\n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    parameters -- a dictionary of tensors containing W1, W2\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">set_random_seed</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>                              <span class=\"c1\"># so that your &#34;random&#34; numbers match ours</span>\n        \n    <span class=\"c1\">### START CODE HERE ### (approx. 2 lines of code)</span>\n    <span class=\"n\">W1</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">get_variable</span><span class=\"p\">(</span><span class=\"s1\">&#39;W1&#39;</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">8</span><span class=\"p\">],</span><span class=\"n\">initializer</span><span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">contrib</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">xavier_initializer</span><span class=\"p\">(</span><span class=\"n\">seed</span> <span class=\"o\">=</span> <span class=\"mi\">0</span> <span class=\"p\">))</span>\n    <span class=\"n\">W2</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">get_variable</span><span class=\"p\">(</span><span class=\"s1\">&#39;W2&#39;</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"mi\">16</span><span class=\"p\">],</span><span class=\"n\">initializer</span><span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">contrib</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">xavier_initializer</span><span class=\"p\">(</span><span class=\"n\">seed</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">))</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n<span class=\"err\">​</span>\n    <span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">&#34;W1&#34;</span><span class=\"p\">:</span> <span class=\"n\">W1</span><span class=\"p\">,</span>\n                  <span class=\"s2\">&#34;W2&#34;</span><span class=\"p\">:</span> <span class=\"n\">W2</span><span class=\"p\">}</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">parameters</span></code></pre></div><p>记得这只是创建了图而已，并没有真正的初始化参数，在执行中还需要</p><p><code>init = tf.global_variables_initializer()</code> </p><p><code>sess_test.run(init)</code></p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>3. Forward propagation</b></h2><p>模型为：CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; FLATTEN -&gt; FULLYCONNECTED</p><div class=\"highlight\"><pre><code class=\"language-text\"> - Conv2D: stride 1, padding is &#34;SAME&#34;\n - ReLU\n - Max pool: Use an 8 by 8 filter size and an 8 by 8 stride, padding is &#34;SAME&#34;\n - Conv2D: stride 1, padding is &#34;SAME&#34;\n - ReLU\n - Max pool: Use a 4 by 4 filter size and a 4 by 4 stride, padding is &#34;SAME&#34;\n - Flatten the previous output.\n - FULLYCONNECTED (FC) layer：这里全连接层不需要有激活函数，因为后面计算cost的时候会加上softmax，因此这里不需要加</code></pre></div><p>用到的函数：</p><ul><li><b>tf.nn.conv2d(X,W1, strides = [1,s,s,1], padding = &#39;SAME&#39;):</b> given an input $X$ and a group of filters $W1$, this function convolves $W1$&#39;s filters on X. The third input ([1,f,f,1]) represents the strides for each dimension of the input (m, n_H_prev, n_W_prev, n_C_prev). You can read the full documentation <a href=\"https://link.zhihu.com/?target=https%3A//www.tensorflow.org/api_docs/python/tf/nn/conv2d\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">here</a></li><li><b>tf.nn.max_pool(A, ksize = [1,f,f,1], strides = [1,s,s,1], padding = &#39;SAME&#39;):</b> given an input A, this function uses a window of size (f, f) and strides of size (s, s) to carry out max pooling over each window. You can read the full documentation <a href=\"https://link.zhihu.com/?target=https%3A//www.tensorflow.org/api_docs/python/tf/nn/max_pool\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">here</a></li><li><b>tf.nn.relu(Z1):</b> computes the elementwise ReLU of Z1 (which can be any shape). You can read the full documentation <a href=\"https://link.zhihu.com/?target=https%3A//www.tensorflow.org/api_docs/python/tf/nn/relu\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">here.</a></li><li><b>tf.contrib.layers.flatten(P)</b>: given an input P, this function flattens each example into a 1D vector it while maintaining the batch-size. It returns a flattened tensor with shape [batch_size, k]. You can read the full documentation <a href=\"https://link.zhihu.com/?target=https%3A//www.tensorflow.org/api_docs/python/tf/contrib/layers/flatten\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">here.</a></li><li><b>tf.contrib.layers.fully_connected(F, num_outputs):</b> given a the flattened input F, it returns the output computed using a fully connected layer. You can read the full documentation <a href=\"https://link.zhihu.com/?target=https%3A//www.tensorflow.org/api_docs/python/tf/contrib/layers/fully_connected\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">here.</a></li></ul><p>In the last function above (<code>tf.contrib.layers.fully_connected</code>), the fully connected layer automatically initializes weights in the graph and keeps on training them as you train the model. Hence, you did not need to initialize those weights when initializing the parameters. </p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: forward_propagation</span>\n<span class=\"err\">​</span>\n<span class=\"k\">def</span> <span class=\"nf\">forward_propagation</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">parameters</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Implements the forward propagation for the model:\n</span><span class=\"s2\">    CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; FLATTEN -&gt; FULLYCONNECTED\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    X -- input dataset placeholder, of shape (input size, number of examples)\n</span><span class=\"s2\">    parameters -- python dictionary containing your parameters &#34;W1&#34;, &#34;W2&#34;\n</span><span class=\"s2\">                  the shapes are given in initialize_parameters\n</span><span class=\"s2\">​\n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    Z3 -- the output of the last LINEAR unit\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"c1\"># Retrieve the parameters from the dictionary &#34;parameters&#34; </span>\n    <span class=\"n\">W1</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;W1&#39;</span><span class=\"p\">]</span>\n    <span class=\"n\">W2</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;W2&#39;</span><span class=\"p\">]</span>\n    \n    <span class=\"c1\">### START CODE HERE ###</span>\n    <span class=\"c1\"># CONV2D: stride of 1, padding &#39;SAME&#39;</span>\n    <span class=\"n\">Z1</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">conv2d</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"nb\">filter</span><span class=\"o\">=</span><span class=\"n\">W1</span><span class=\"p\">,</span> <span class=\"n\">strides</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">],</span><span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"s1\">&#39;SAME&#39;</span><span class=\"p\">)</span>\n    <span class=\"c1\"># RELU</span>\n    <span class=\"n\">A1</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">relu</span><span class=\"p\">(</span><span class=\"n\">Z1</span><span class=\"p\">)</span>\n    <span class=\"c1\"># MAXPOOL: window 8x8, sride 8, padding &#39;SAME&#39;</span>\n    <span class=\"n\">P1</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">max_pool</span><span class=\"p\">(</span><span class=\"n\">A1</span><span class=\"p\">,</span><span class=\"n\">ksize</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">strides</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"mi\">8</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span><span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"s1\">&#39;SAME&#39;</span><span class=\"p\">)</span>\n    <span class=\"c1\"># CONV2D: filters W2, stride 1, padding &#39;SAME&#39;</span>\n    <span class=\"n\">Z2</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">conv2d</span><span class=\"p\">(</span><span class=\"n\">P1</span><span class=\"p\">,</span> <span class=\"nb\">filter</span><span class=\"o\">=</span><span class=\"n\">W2</span><span class=\"p\">,</span> <span class=\"n\">strides</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span><span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"s1\">&#39;SAME&#39;</span><span class=\"p\">)</span>\n    <span class=\"c1\"># RELU</span>\n    <span class=\"n\">A2</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">relu</span><span class=\"p\">(</span><span class=\"n\">Z2</span><span class=\"p\">)</span>\n    <span class=\"c1\"># MAXPOOL: window 4x4, stride 4, padding &#39;SAME&#39;</span>\n    <span class=\"n\">P2</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">max_pool</span><span class=\"p\">(</span><span class=\"n\">A2</span><span class=\"p\">,</span><span class=\"n\">ksize</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">strides</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">],</span><span class=\"n\">padding</span><span class=\"o\">=</span><span class=\"s1\">&#39;SAME&#39;</span><span class=\"p\">)</span>\n    <span class=\"c1\"># FLATTEN</span>\n    <span class=\"n\">P2</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">contrib</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">flatten</span><span class=\"p\">(</span><span class=\"n\">P2</span><span class=\"p\">)</span>\n    <span class=\"c1\"># FULLY-CONNECTED without non-linear activation function (not not call softmax).</span>\n    <span class=\"c1\"># 6 neurons in output layer. Hint: one of the arguments should be &#34;activation_fn=None&#34; </span>\n    <span class=\"n\">Z3</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">contrib</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">fully_connected</span><span class=\"p\">(</span><span class=\"n\">P2</span><span class=\"p\">,</span> <span class=\"mi\">6</span><span class=\"p\">,</span><span class=\"n\">activation_fn</span><span class=\"o\">=</span><span class=\"bp\">None</span><span class=\"p\">)</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n<span class=\"err\">​</span>\n    <span class=\"k\">return</span> <span class=\"n\">Z3</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>4. Compute cost</b></h2><ul><li><b>tf.nn.softmax_cross_entropy_with_logits(logits = Z3, labels = Y):</b> computes the softmax entropy loss. This function both computes the softmax activation function as well as the resulting loss. You can check the full documentation  <a href=\"https://link.zhihu.com/?target=https%3A//www.tensorflow.org/api_docs/python/tf/nn/softmax_cross_entropy_with_logits\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">here.</a>这个函数已经包含了计算softmax，还有求cross-entropy两件事了。</li><li><b>tf.reduce_mean:</b> computes the mean of elements across dimensions of a tensor. Use this to sum the losses over all the examples to get the overall cost. You can check the full documentation <a href=\"https://link.zhihu.com/?target=https%3A//www.tensorflow.org/api_docs/python/tf/reduce_mean\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">here.</a></li></ul><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: compute_cost </span>\n<span class=\"err\">​</span>\n<span class=\"k\">def</span> <span class=\"nf\">compute_cost</span><span class=\"p\">(</span><span class=\"n\">Z3</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Computes the cost\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n</span><span class=\"s2\">    Y -- &#34;true&#34; labels vector placeholder, same shape as Z3\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    cost - Tensor of the cost function\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"c1\">### START CODE HERE ### (1 line of code)</span>\n    <span class=\"n\">cost</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">reduce_mean</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">softmax_cross_entropy_with_logits</span><span class=\"p\">(</span><span class=\"n\">logits</span><span class=\"o\">=</span><span class=\"n\">Z3</span><span class=\"p\">,</span><span class=\"n\">labels</span><span class=\"o\">=</span><span class=\"n\">Y</span><span class=\"p\">))</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">cost</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>5. Model</b></h2><p>把前面的函数都结合起来，创建一个完整的模型。</p><p>其中<code>random_mini_batches()</code>已经给我们了，优化器使用了</p><p><code>optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)</code></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: model</span>\n<span class=\"err\">​</span>\n<span class=\"k\">def</span> <span class=\"nf\">model</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">,</span> <span class=\"n\">Y_train</span><span class=\"p\">,</span> <span class=\"n\">X_test</span><span class=\"p\">,</span> <span class=\"n\">Y_test</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span> <span class=\"o\">=</span> <span class=\"mf\">0.009</span><span class=\"p\">,</span>\n          <span class=\"n\">num_epochs</span> <span class=\"o\">=</span> <span class=\"mi\">100</span><span class=\"p\">,</span> <span class=\"n\">minibatch_size</span> <span class=\"o\">=</span> <span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"n\">print_cost</span> <span class=\"o\">=</span> <span class=\"bp\">True</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Implements a three-layer ConvNet in Tensorflow:\n</span><span class=\"s2\">    CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; FLATTEN -&gt; FULLYCONNECTED\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    X_train -- training set, of shape (None, 64, 64, 3)\n</span><span class=\"s2\">    Y_train -- test set, of shape (None, n_y = 6)\n</span><span class=\"s2\">    X_test -- training set, of shape (None, 64, 64, 3)\n</span><span class=\"s2\">    Y_test -- test set, of shape (None, n_y = 6)\n</span><span class=\"s2\">    learning_rate -- learning rate of the optimization\n</span><span class=\"s2\">    num_epochs -- number of epochs of the optimization loop\n</span><span class=\"s2\">    minibatch_size -- size of a minibatch\n</span><span class=\"s2\">    print_cost -- True to print the cost every 100 epochs\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    train_accuracy -- real number, accuracy on the train set (X_train)\n</span><span class=\"s2\">    test_accuracy -- real number, testing accuracy on the test set (X_test)\n</span><span class=\"s2\">    parameters -- parameters learnt by the model. They can then be used to predict.\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"n\">ops</span><span class=\"o\">.</span><span class=\"n\">reset_default_graph</span><span class=\"p\">()</span>                         <span class=\"c1\"># to be able to rerun the model without overwriting tf variables</span>\n    <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">set_random_seed</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>                             <span class=\"c1\"># to keep results consistent (tensorflow seed)</span>\n    <span class=\"n\">seed</span> <span class=\"o\">=</span> <span class=\"mi\">3</span>                                          <span class=\"c1\"># to keep results consistent (numpy seed)</span>\n    <span class=\"p\">(</span><span class=\"n\">m</span><span class=\"p\">,</span> <span class=\"n\">n_H0</span><span class=\"p\">,</span> <span class=\"n\">n_W0</span><span class=\"p\">,</span> <span class=\"n\">n_C0</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">X_train</span><span class=\"o\">.</span><span class=\"n\">shape</span>             \n    <span class=\"n\">n_y</span> <span class=\"o\">=</span> <span class=\"n\">Y_train</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span>                            \n    <span class=\"n\">costs</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>                                        <span class=\"c1\"># To keep track of the cost</span>\n    \n    <span class=\"c1\"># Create Placeholders of the correct shape</span>\n    <span class=\"c1\">### START CODE HERE ### (1 line)</span>\n    <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span> <span class=\"o\">=</span> <span class=\"n\">create_placeholders</span><span class=\"p\">(</span><span class=\"n\">n_H0</span><span class=\"p\">,</span> <span class=\"n\">n_W0</span><span class=\"p\">,</span><span class=\"n\">n_C0</span><span class=\"p\">,</span><span class=\"n\">n_y</span><span class=\"p\">)</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n<span class=\"err\">​</span>\n    <span class=\"c1\"># Initialize parameters</span>\n    <span class=\"c1\">### START CODE HERE ### (1 line)</span>\n    <span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"n\">initialize_parameters</span><span class=\"p\">()</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"c1\"># Forward propagation: Build the forward propagation in the tensorflow graph</span>\n    <span class=\"c1\">### START CODE HERE ### (1 line)</span>\n    <span class=\"n\">Z3</span> <span class=\"o\">=</span> <span class=\"n\">forward_propagation</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span><span class=\"n\">parameters</span><span class=\"p\">)</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"c1\"># Cost function: Add cost function to tensorflow graph</span>\n    <span class=\"c1\">### START CODE HERE ### (1 line)</span>\n    <span class=\"n\">cost</span> <span class=\"o\">=</span> <span class=\"n\">compute_cost</span><span class=\"p\">(</span><span class=\"n\">Z3</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">)</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"c1\"># Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer that minimizes the cost.</span>\n    <span class=\"c1\">### START CODE HERE ### (1 line)</span>\n    <span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"o\">.</span><span class=\"n\">AdamOptimizer</span><span class=\"p\">(</span><span class=\"n\">learning_rate</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">minimize</span><span class=\"p\">(</span><span class=\"n\">cost</span><span class=\"p\">)</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"c1\"># Initialize all the variables globally</span>\n    <span class=\"n\">init</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">global_variables_initializer</span><span class=\"p\">()</span>\n     \n    <span class=\"c1\"># Start the session to compute the tensorflow graph</span>\n    <span class=\"k\">with</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">Session</span><span class=\"p\">()</span> <span class=\"k\">as</span> <span class=\"n\">sess</span><span class=\"p\">:</span>\n        \n        <span class=\"c1\"># Run the initialization</span>\n        <span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">init</span><span class=\"p\">)</span>\n        \n        <span class=\"c1\"># Do the training loop</span>\n        <span class=\"k\">for</span> <span class=\"n\">epoch</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">num_epochs</span><span class=\"p\">):</span>\n<span class=\"err\">​</span>\n            <span class=\"n\">minibatch_cost</span> <span class=\"o\">=</span> <span class=\"mf\">0.</span>\n            <span class=\"n\">num_minibatches</span> <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">m</span> <span class=\"o\">/</span> <span class=\"n\">minibatch_size</span><span class=\"p\">)</span> <span class=\"c1\"># number of minibatches of size minibatch_size in the train set</span>\n            <span class=\"n\">seed</span> <span class=\"o\">=</span> <span class=\"n\">seed</span> <span class=\"o\">+</span> <span class=\"mi\">1</span>\n            <span class=\"n\">minibatches</span> <span class=\"o\">=</span> <span class=\"n\">random_mini_batches</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">,</span> <span class=\"n\">Y_train</span><span class=\"p\">,</span> <span class=\"n\">minibatch_size</span><span class=\"p\">,</span> <span class=\"n\">seed</span><span class=\"p\">)</span>\n<span class=\"err\">​</span>\n            <span class=\"k\">for</span> <span class=\"n\">minibatch</span> <span class=\"ow\">in</span> <span class=\"n\">minibatches</span><span class=\"p\">:</span>\n<span class=\"err\">​</span>\n                <span class=\"c1\"># Select a minibatch</span>\n                <span class=\"p\">(</span><span class=\"n\">minibatch_X</span><span class=\"p\">,</span> <span class=\"n\">minibatch_Y</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">minibatch</span>\n                <span class=\"c1\"># IMPORTANT: The line that runs the graph on a minibatch.</span>\n                <span class=\"c1\"># Run the session to execute the optimizer and the cost, the feedict should contain a minibatch for (X,Y).</span>\n                <span class=\"c1\">### START CODE HERE ### (1 line)</span>\n                <span class=\"n\">_</span> <span class=\"p\">,</span> <span class=\"n\">temp_cost</span> <span class=\"o\">=</span> <span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">([</span><span class=\"n\">optimizer</span><span class=\"p\">,</span><span class=\"n\">cost</span><span class=\"p\">],</span><span class=\"n\">feed_dict</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"n\">X</span><span class=\"p\">:</span><span class=\"n\">minibatch_X</span><span class=\"p\">,</span><span class=\"n\">Y</span><span class=\"p\">:</span><span class=\"n\">minibatch_Y</span><span class=\"p\">})</span>\n                <span class=\"c1\">### END CODE HERE ###</span>\n                \n                <span class=\"n\">minibatch_cost</span> <span class=\"o\">+=</span> <span class=\"n\">temp_cost</span> <span class=\"o\">/</span> <span class=\"n\">num_minibatches</span>\n                \n<span class=\"err\">​</span>\n            <span class=\"c1\"># Print the cost every epoch</span>\n            <span class=\"k\">if</span> <span class=\"n\">print_cost</span> <span class=\"o\">==</span> <span class=\"bp\">True</span> <span class=\"ow\">and</span> <span class=\"n\">epoch</span> <span class=\"o\">%</span> <span class=\"mi\">5</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n                <span class=\"k\">print</span> <span class=\"p\">(</span><span class=\"s2\">&#34;Cost after epoch </span><span class=\"si\">%i</span><span class=\"s2\">: </span><span class=\"si\">%f</span><span class=\"s2\">&#34;</span> <span class=\"o\">%</span> <span class=\"p\">(</span><span class=\"n\">epoch</span><span class=\"p\">,</span> <span class=\"n\">minibatch_cost</span><span class=\"p\">))</span>\n            <span class=\"k\">if</span> <span class=\"n\">print_cost</span> <span class=\"o\">==</span> <span class=\"bp\">True</span> <span class=\"ow\">and</span> <span class=\"n\">epoch</span> <span class=\"o\">%</span> <span class=\"mi\">1</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n                <span class=\"n\">costs</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">minibatch_cost</span><span class=\"p\">)</span>\n        \n        \n        <span class=\"c1\"># plot the cost</span>\n        <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">squeeze</span><span class=\"p\">(</span><span class=\"n\">costs</span><span class=\"p\">))</span>\n        <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">ylabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;cost&#39;</span><span class=\"p\">)</span>\n        <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">xlabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;iterations (per tens)&#39;</span><span class=\"p\">)</span>\n        <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">title</span><span class=\"p\">(</span><span class=\"s2\">&#34;Learning rate =&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">learning_rate</span><span class=\"p\">))</span>\n        <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>\n<span class=\"err\">​</span>\n        <span class=\"c1\"># Calculate the correct predictions</span>\n        <span class=\"n\">predict_op</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">argmax</span><span class=\"p\">(</span><span class=\"n\">Z3</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n        <span class=\"n\">correct_prediction</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">equal</span><span class=\"p\">(</span><span class=\"n\">predict_op</span><span class=\"p\">,</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">argmax</span><span class=\"p\">(</span><span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n        \n        <span class=\"c1\"># Calculate accuracy on the test set</span>\n        <span class=\"n\">accuracy</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">reduce_mean</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">cast</span><span class=\"p\">(</span><span class=\"n\">correct_prediction</span><span class=\"p\">,</span> <span class=\"s2\">&#34;float&#34;</span><span class=\"p\">))</span>\n        <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">accuracy</span><span class=\"p\">)</span>\n        <span class=\"n\">train_accuracy</span> <span class=\"o\">=</span> <span class=\"n\">accuracy</span><span class=\"o\">.</span><span class=\"nb\">eval</span><span class=\"p\">({</span><span class=\"n\">X</span><span class=\"p\">:</span> <span class=\"n\">X_train</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">:</span> <span class=\"n\">Y_train</span><span class=\"p\">})</span>\n        <span class=\"n\">test_accuracy</span> <span class=\"o\">=</span> <span class=\"n\">accuracy</span><span class=\"o\">.</span><span class=\"nb\">eval</span><span class=\"p\">({</span><span class=\"n\">X</span><span class=\"p\">:</span> <span class=\"n\">X_test</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">:</span> <span class=\"n\">Y_test</span><span class=\"p\">})</span>\n        <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s2\">&#34;Train Accuracy:&#34;</span><span class=\"p\">,</span> <span class=\"n\">train_accuracy</span><span class=\"p\">)</span>\n        <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s2\">&#34;Test Accuracy:&#34;</span><span class=\"p\">,</span> <span class=\"n\">test_accuracy</span><span class=\"p\">)</span>\n                \n        <span class=\"k\">return</span> <span class=\"n\">train_accuracy</span><span class=\"p\">,</span> <span class=\"n\">test_accuracy</span><span class=\"p\">,</span> <span class=\"n\">parameters</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>得到效果如图：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-849c15a954deb072c1396fedaf1f5b13_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"396\" data-rawheight=\"278\" class=\"content_image\" width=\"396\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;396&#39; height=&#39;278&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"396\" data-rawheight=\"278\" class=\"content_image lazy\" width=\"396\" data-actualsrc=\"https://pic4.zhimg.com/v2-849c15a954deb072c1396fedaf1f5b13_b.jpg\"/></figure><p></p><p></p>", 
            "topic": [
                {
                    "tag": "卷积神经网络（CNN）", 
                    "tagLink": "https://api.zhihu.com/topics/20043586"
                }, 
                {
                    "tag": "神经网络", 
                    "tagLink": "https://api.zhihu.com/topics/19607065"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/45752499", 
            "userName": "直上云霄", 
            "userLink": "https://www.zhihu.com/people/1033165ce4ad9c3fce69a0793dfab8ad", 
            "upvote": 0, 
            "title": "DeepLearning.ai作业:(4-1)--卷积神经网络（part1）", 
            "content": "<div class=\"highlight\"><pre><code class=\"language-text\">title: &#39;DeepLearning.ai作业:(4-1)-- 卷积神经网络（Foundations of CNN）&#39;\nid: dl-ai-4-1h\ntags:\n  - dl.ai\n  - homework\ncategories:\n  - AI\n  - Deep Learning\ndate: 2018-09-30 16:07:23\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><b>首发于个人博客: </b></p><b><a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Fangzh的个人博客 | 人工智能拯救世界</a></b><p><b> 欢迎来访</b></p><p>本周的作业分为了两部分：</p><ul><li>卷积神经网络的模型搭建</li><li>用TensorFlow来训练卷积神经网络</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>Part1：Convolutional Neural Networks: Step by Step</b></h2><p>主要内容：</p><ul><li>convolution funtions:<br/></li><ul><li>Zero Padding</li><li>Convolve window</li><li>Convolution forward</li><li>Convolution backward (optional)</li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>Pooling functions：<br/></li><ul><li>Pooling forward</li><li>Create mask</li><li>Distribute value</li><li>Pooling backward (optional)</li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>Convolutional Neural Networks</b></h2><p>创建CNN的主要函数</p><p><b>1. Zero Padding</b></p><p>先创建一个padding函数，用来输入图像X，输出padding后的图像，这里使用的是<code>np.pad()</code>函数，</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">a</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">pad</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"p\">((</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">0</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span><span class=\"mi\">3</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">0</span><span class=\"p\">)),</span> <span class=\"s1\">&#39;constant&#39;</span><span class=\"p\">,</span> <span class=\"n\">constant_values</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"o\">..</span><span class=\"p\">,</span><span class=\"o\">..</span><span class=\"p\">))</span>\n<span class=\"err\">表示</span><span class=\"n\">a有5个维度</span><span class=\"err\">，在第</span><span class=\"mi\">1</span><span class=\"err\">维的两边都填上</span><span class=\"mi\">1</span><span class=\"err\">个</span><span class=\"n\">pad</span><span class=\"err\">，和第</span><span class=\"mi\">3</span><span class=\"err\">维的两边都填上</span><span class=\"mi\">3</span><span class=\"err\">个</span><span class=\"n\">pad</span><span class=\"err\">，</span><span class=\"n\">constant_values表示两边要填充的值</span>\n<span class=\"err\">​</span>\n<span class=\"k\">def</span> <span class=\"nf\">zero_pad</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">pad</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Pad with zeros all images of the dataset X. The padding is applied to the height and width of an image, \n</span><span class=\"s2\">    as illustrated in Figure 1.\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Argument:\n</span><span class=\"s2\">    X -- python numpy array of shape (m, n_H, n_W, n_C) representing a batch of m images\n</span><span class=\"s2\">    pad -- integer, amount of padding around each image on vertical and horizontal dimensions\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    X_pad -- padded image of shape (m, n_H + 2*pad, n_W + 2*pad, n_C)\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"c1\">### START CODE HERE ### (≈ 1 line)</span>\n    <span class=\"n\">X_pad</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">pad</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"p\">((</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">0</span><span class=\"p\">),(</span><span class=\"n\">pad</span><span class=\"p\">,</span><span class=\"n\">pad</span><span class=\"p\">),(</span><span class=\"n\">pad</span><span class=\"p\">,</span><span class=\"n\">pad</span><span class=\"p\">),(</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">0</span><span class=\"p\">)),</span> <span class=\"s1\">&#39;constant&#39;</span><span class=\"p\">,</span> <span class=\"n\">constant_values</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"mi\">0</span><span class=\"p\">))</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">X_pad</span></code></pre></div><p><b>2.Single step of convolution</b></p><p>创建一个单步的卷积运算，也就是一次输入一个切片，大小和卷积核相同，对应元素相乘再求和，最后再加个bias项。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: conv_single_step</span>\n<span class=\"err\">​</span>\n<span class=\"k\">def</span> <span class=\"nf\">conv_single_step</span><span class=\"p\">(</span><span class=\"n\">a_slice_prev</span><span class=\"p\">,</span> <span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Apply one filter defined by parameters W on a single slice (a_slice_prev) of the output activation \n</span><span class=\"s2\">    of the previous layer.\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    a_slice_prev -- slice of input data of shape (f, f, n_C_prev)\n</span><span class=\"s2\">    W -- Weight parameters contained in a window - matrix of shape (f, f, n_C_prev)\n</span><span class=\"s2\">    b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    Z -- a scalar value, result of convolving the sliding window (W, b) on a slice x of the input data\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n<span class=\"err\">​</span>\n    <span class=\"c1\">### START CODE HERE ### (≈ 2 lines of code)</span>\n    <span class=\"c1\"># Element-wise product between a_slice and W. Do not add the bias yet.</span>\n    <span class=\"n\">s</span> <span class=\"o\">=</span> <span class=\"n\">a_slice_prev</span> <span class=\"o\">*</span> <span class=\"n\">W</span>\n    <span class=\"c1\"># Sum over all entries of the volume s.</span>\n    <span class=\"n\">Z</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">s</span><span class=\"p\">)</span>\n    <span class=\"c1\"># Add bias b to Z. Cast b to a float() so that Z results in a scalar value.</span>\n    <span class=\"n\">Z</span> <span class=\"o\">=</span> <span class=\"n\">Z</span> <span class=\"o\">+</span> <span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"n\">b</span><span class=\"p\">)</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n<span class=\"err\">​</span>\n    <span class=\"k\">return</span> <span class=\"n\">Z</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><b>3.Convolutional Neural Networks - Forward pass</b></p><p>创建一次完整的卷积过程，也就是利用上面的一次卷积，进行for循环。进行切片的时候，注意边界<code>vert_start, vert_end, horiz_start and horiz_end</code></p><p>这一步应该先弄清楚A_prev，A，W，b的维度，超参数项包括了stride和pad</p><p><img src=\"https://www.zhihu.com/equation?tex=+n_H+%3D+%5Clfloor+%5Cfrac%7Bn%7BH%7Bprev%7D%7D+-+f+%2B+2+%5Ctimes+pad%7D%7Bstride%7D+%5Crfloor+%2B1+\" alt=\" n_H = \\lfloor \\frac{n{H{prev}} - f + 2 \\times pad}{stride} \\rfloor +1 \" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=+n_W+%3D+%5Clfloor+%5Cfrac%7Bn%7BW%7Bprev%7D%7D+-+f+%2B+2+%5Ctimes+pad%7D%7Bstride%7D+%5Crfloor+%2B1+\" alt=\" n_W = \\lfloor \\frac{n{W{prev}} - f + 2 \\times pad}{stride} \\rfloor +1 \" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=+n_C+%3D+%5Ctext%7Bnumber+of+filters+used+in+the+convolution%7D\" alt=\" n_C = \\text{number of filters used in the convolution}\" eeimg=\"1\"/> </p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: conv_forward</span>\n<span class=\"err\">​</span>\n<span class=\"k\">def</span> <span class=\"nf\">conv_forward</span><span class=\"p\">(</span><span class=\"n\">A_prev</span><span class=\"p\">,</span> <span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">hparameters</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Implements the forward propagation for a convolution function\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    A_prev -- output activations of the previous layer, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n</span><span class=\"s2\">    W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)\n</span><span class=\"s2\">    b -- Biases, numpy array of shape (1, 1, 1, n_C)\n</span><span class=\"s2\">    hparameters -- python dictionary containing &#34;stride&#34; and &#34;pad&#34;\n</span><span class=\"s2\">        \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\n</span><span class=\"s2\">    cache -- cache of values needed for the conv_backward() function\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"c1\">### START CODE HERE ###</span>\n    <span class=\"c1\"># Retrieve dimensions from A_prev&#39;s shape (≈1 line)  </span>\n    <span class=\"p\">(</span><span class=\"n\">m</span><span class=\"p\">,</span> <span class=\"n\">n_H_prev</span><span class=\"p\">,</span> <span class=\"n\">n_W_prev</span><span class=\"p\">,</span> <span class=\"n\">n_C_prev</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">A_prev</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n    \n    <span class=\"c1\"># Retrieve dimensions from W&#39;s shape (≈1 line)</span>\n    <span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">,</span> <span class=\"n\">f</span><span class=\"p\">,</span> <span class=\"n\">n_C_prev</span><span class=\"p\">,</span> <span class=\"n\">n_C</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">W</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n    \n    <span class=\"c1\"># Retrieve information from &#34;hparameters&#34; (≈2 lines)</span>\n    <span class=\"n\">stride</span> <span class=\"o\">=</span> <span class=\"n\">hparameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;stride&#39;</span><span class=\"p\">]</span>\n    <span class=\"n\">pad</span> <span class=\"o\">=</span> <span class=\"n\">hparameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;pad&#39;</span><span class=\"p\">]</span>\n    \n    <span class=\"c1\"># Compute the dimensions of the CONV output volume using the formula given above. Hint: use int() to floor. (≈2 lines)</span>\n    <span class=\"n\">n_H</span> <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">((</span><span class=\"n\">n_H_prev</span> <span class=\"o\">+</span> <span class=\"mi\">2</span> <span class=\"o\">*</span> <span class=\"n\">pad</span> <span class=\"o\">-</span> <span class=\"n\">f</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">stride</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n    <span class=\"n\">n_W</span> <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">((</span><span class=\"n\">n_W_prev</span> <span class=\"o\">+</span> <span class=\"mi\">2</span> <span class=\"o\">*</span> <span class=\"n\">pad</span> <span class=\"o\">-</span> <span class=\"n\">f</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">stride</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n<span class=\"err\">​</span>\n    <span class=\"c1\"># Initialize the output volume Z with zeros. (≈1 line)</span>\n    <span class=\"n\">Z</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">m</span><span class=\"p\">,</span> <span class=\"n\">n_H</span><span class=\"p\">,</span> <span class=\"n\">n_W</span><span class=\"p\">,</span> <span class=\"n\">n_C</span><span class=\"p\">))</span>\n    \n    <span class=\"c1\"># Create A_prev_pad by padding A_prev</span>\n    <span class=\"n\">A_prev_pad</span> <span class=\"o\">=</span> <span class=\"n\">zero_pad</span><span class=\"p\">(</span><span class=\"n\">A_prev</span><span class=\"p\">,</span> <span class=\"n\">pad</span><span class=\"p\">)</span>\n    \n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">m</span><span class=\"p\">):</span>                               <span class=\"c1\"># loop over the batch of training examples</span>\n        <span class=\"n\">a_prev_pad</span> <span class=\"o\">=</span> <span class=\"n\">A_prev_pad</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span>                               <span class=\"c1\"># Select ith training example&#39;s padded activation</span>\n        <span class=\"k\">for</span> <span class=\"n\">h</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">n_H</span><span class=\"p\">):</span>                           <span class=\"c1\"># loop over vertical axis of the output volume</span>\n            <span class=\"k\">for</span> <span class=\"n\">w</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">n_W</span><span class=\"p\">):</span>                       <span class=\"c1\"># loop over horizontal axis of the output volume</span>\n                <span class=\"k\">for</span> <span class=\"n\">c</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">n_C</span><span class=\"p\">):</span>                   <span class=\"c1\"># loop over channels (= #filters) of the output volume</span>\n                    \n                    <span class=\"c1\"># Find the corners of the current &#34;slice&#34; (≈4 lines)</span>\n                    <span class=\"n\">vert_start</span> <span class=\"o\">=</span> <span class=\"n\">h</span> <span class=\"o\">*</span> <span class=\"n\">stride</span>\n                    <span class=\"n\">vert_end</span> <span class=\"o\">=</span> <span class=\"n\">h</span> <span class=\"o\">*</span> <span class=\"n\">stride</span> <span class=\"o\">+</span> <span class=\"n\">f</span>\n                    <span class=\"n\">horiz_start</span> <span class=\"o\">=</span> <span class=\"n\">w</span> <span class=\"o\">*</span> <span class=\"n\">stride</span>\n                    <span class=\"n\">horiz_end</span> <span class=\"o\">=</span> <span class=\"n\">w</span> <span class=\"o\">*</span> <span class=\"n\">stride</span> <span class=\"o\">+</span> <span class=\"n\">f</span>\n                    \n                    <span class=\"c1\"># Use the corners to define the (3D) slice of a_prev_pad (See Hint above the cell). (≈1 line)</span>\n                    <span class=\"n\">a_slice_prev</span> <span class=\"o\">=</span> <span class=\"n\">a_prev_pad</span><span class=\"p\">[</span><span class=\"n\">vert_start</span> <span class=\"p\">:</span> <span class=\"n\">vert_end</span><span class=\"p\">,</span> <span class=\"n\">horiz_start</span> <span class=\"p\">:</span> <span class=\"n\">horiz_end</span><span class=\"p\">]</span>\n                    \n                    <span class=\"c1\"># Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron. (≈1 line)</span>\n                    <span class=\"n\">Z</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">h</span><span class=\"p\">,</span> <span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">c</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">conv_single_step</span><span class=\"p\">(</span><span class=\"n\">a_slice_prev</span><span class=\"p\">,</span><span class=\"n\">W</span><span class=\"p\">[:,:,:,</span><span class=\"n\">c</span><span class=\"p\">],</span><span class=\"n\">b</span><span class=\"p\">[:,:,:,</span><span class=\"n\">c</span><span class=\"p\">])</span>\n                                        \n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"c1\"># Making sure your output shape is correct</span>\n    <span class=\"k\">assert</span><span class=\"p\">(</span><span class=\"n\">Z</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"p\">(</span><span class=\"n\">m</span><span class=\"p\">,</span> <span class=\"n\">n_H</span><span class=\"p\">,</span> <span class=\"n\">n_W</span><span class=\"p\">,</span> <span class=\"n\">n_C</span><span class=\"p\">))</span>\n    \n    <span class=\"c1\"># Save information in &#34;cache&#34; for the backprop</span>\n    <span class=\"n\">cache</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">A_prev</span><span class=\"p\">,</span> <span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">hparameters</span><span class=\"p\">)</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">Z</span><span class=\"p\">,</span> <span class=\"n\">cache</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>Pooling layer</b></h2><p>创建池化层，注意得到的维度需要向下取整，用int()对float()进行转换</p><p><img src=\"https://www.zhihu.com/equation?tex=n_H+%3D+%5Clfloor+%5Cfrac%7Bn%7BH%7Bprev%7D%7D+-+f%7D%7Bstride%7D+%5Crfloor+%2B1\" alt=\"n_H = \\lfloor \\frac{n{H{prev}} - f}{stride} \\rfloor +1\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=n_W+%3D+%5Clfloor+%5Cfrac%7Bn%7BW%7Bprev%7D%7D+-+f%7D%7Bstride%7D+%5Crfloor+%2B1\" alt=\"n_W = \\lfloor \\frac{n{W{prev}} - f}{stride} \\rfloor +1\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=n_C+%3D+n_%7BC_%7Bprev%7D%7D\" alt=\"n_C = n_{C_{prev}}\" eeimg=\"1\"/> </p><p>同样需要先进行切边，而后分为max和average两种，分别用np.max和np.mean</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">pool_forward</span><span class=\"p\">(</span><span class=\"n\">A_prev</span><span class=\"p\">,</span> <span class=\"n\">hparameters</span><span class=\"p\">,</span> <span class=\"n\">mode</span> <span class=\"o\">=</span> <span class=\"s2\">&#34;max&#34;</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Implements the forward pass of the pooling layer\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n</span><span class=\"s2\">    hparameters -- python dictionary containing &#34;f&#34; and &#34;stride&#34;\n</span><span class=\"s2\">    mode -- the pooling mode you would like to use, defined as a string (&#34;max&#34; or &#34;average&#34;)\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C)\n</span><span class=\"s2\">    cache -- cache used in the backward pass of the pooling layer, contains the input and hparameters \n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"c1\"># Retrieve dimensions from the input shape</span>\n    <span class=\"p\">(</span><span class=\"n\">m</span><span class=\"p\">,</span> <span class=\"n\">n_H_prev</span><span class=\"p\">,</span> <span class=\"n\">n_W_prev</span><span class=\"p\">,</span> <span class=\"n\">n_C_prev</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">A_prev</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n    \n    <span class=\"c1\"># Retrieve hyperparameters from &#34;hparameters&#34;</span>\n    <span class=\"n\">f</span> <span class=\"o\">=</span> <span class=\"n\">hparameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;f&#34;</span><span class=\"p\">]</span>\n    <span class=\"n\">stride</span> <span class=\"o\">=</span> <span class=\"n\">hparameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;stride&#34;</span><span class=\"p\">]</span>\n    \n    <span class=\"c1\"># Define the dimensions of the output</span>\n    <span class=\"n\">n_H</span> <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"mi\">1</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">n_H_prev</span> <span class=\"o\">-</span> <span class=\"n\">f</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">stride</span><span class=\"p\">)</span>\n    <span class=\"n\">n_W</span> <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"mi\">1</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"n\">n_W_prev</span> <span class=\"o\">-</span> <span class=\"n\">f</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"n\">stride</span><span class=\"p\">)</span>\n    <span class=\"n\">n_C</span> <span class=\"o\">=</span> <span class=\"n\">n_C_prev</span>\n    \n    <span class=\"c1\"># Initialize output matrix A</span>\n    <span class=\"n\">A</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">m</span><span class=\"p\">,</span> <span class=\"n\">n_H</span><span class=\"p\">,</span> <span class=\"n\">n_W</span><span class=\"p\">,</span> <span class=\"n\">n_C</span><span class=\"p\">))</span>              \n    \n    <span class=\"c1\">### START CODE HERE ###</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">m</span><span class=\"p\">):</span>                         <span class=\"c1\"># loop over the training examples</span>\n        <span class=\"k\">for</span> <span class=\"n\">h</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">n_H</span><span class=\"p\">):</span>                     <span class=\"c1\"># loop on the vertical axis of the output volume</span>\n            <span class=\"k\">for</span> <span class=\"n\">w</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">n_W</span><span class=\"p\">):</span>                 <span class=\"c1\"># loop on the horizontal axis of the output volume</span>\n                <span class=\"k\">for</span> <span class=\"n\">c</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span> <span class=\"p\">(</span><span class=\"n\">n_C</span><span class=\"p\">):</span>            <span class=\"c1\"># loop over the channels of the output volume</span>\n                    \n                    <span class=\"c1\"># Find the corners of the current &#34;slice&#34; (≈4 lines)</span>\n                    <span class=\"n\">vert_start</span> <span class=\"o\">=</span> <span class=\"n\">h</span> <span class=\"o\">*</span> <span class=\"n\">stride</span>\n                    <span class=\"n\">vert_end</span> <span class=\"o\">=</span> <span class=\"n\">vert_start</span> <span class=\"o\">+</span> <span class=\"n\">f</span>\n                    <span class=\"n\">horiz_start</span> <span class=\"o\">=</span> <span class=\"n\">w</span> <span class=\"o\">*</span> <span class=\"n\">stride</span>\n                    <span class=\"n\">horiz_end</span> <span class=\"o\">=</span> <span class=\"n\">horiz_start</span> <span class=\"o\">+</span> <span class=\"n\">f</span>\n                    \n                    <span class=\"c1\"># Use the corners to define the current slice on the ith training example of A_prev, channel c. (≈1 line)</span>\n                    <span class=\"n\">a_prev_slice</span> <span class=\"o\">=</span> <span class=\"n\">A_prev</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">vert_start</span> <span class=\"p\">:</span> <span class=\"n\">vert_end</span><span class=\"p\">,</span> <span class=\"n\">horiz_start</span> <span class=\"p\">:</span> <span class=\"n\">horiz_end</span><span class=\"p\">,</span> <span class=\"n\">c</span><span class=\"p\">]</span>\n                    \n                    <span class=\"c1\"># Compute the pooling operation on the slice. Use an if statment to differentiate the modes. Use np.max/np.mean.</span>\n                    <span class=\"k\">if</span> <span class=\"n\">mode</span> <span class=\"o\">==</span> <span class=\"s2\">&#34;max&#34;</span><span class=\"p\">:</span>\n                        <span class=\"n\">A</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">h</span><span class=\"p\">,</span> <span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">c</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">max</span><span class=\"p\">(</span><span class=\"n\">a_prev_slice</span><span class=\"p\">)</span>\n                    <span class=\"k\">elif</span> <span class=\"n\">mode</span> <span class=\"o\">==</span> <span class=\"s2\">&#34;average&#34;</span><span class=\"p\">:</span>\n                        <span class=\"n\">A</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">h</span><span class=\"p\">,</span> <span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">c</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">mean</span><span class=\"p\">(</span><span class=\"n\">a_prev_slice</span><span class=\"p\">)</span>\n    \n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"c1\"># Store the input and hparameters in &#34;cache&#34; for pool_backward()</span>\n    <span class=\"n\">cache</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">A_prev</span><span class=\"p\">,</span> <span class=\"n\">hparameters</span><span class=\"p\">)</span>\n    \n    <span class=\"c1\"># Making sure your output shape is correct</span>\n    <span class=\"k\">assert</span><span class=\"p\">(</span><span class=\"n\">A</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"p\">(</span><span class=\"n\">m</span><span class=\"p\">,</span> <span class=\"n\">n_H</span><span class=\"p\">,</span> <span class=\"n\">n_W</span><span class=\"p\">,</span> <span class=\"n\">n_C</span><span class=\"p\">))</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">A</span><span class=\"p\">,</span> <span class=\"n\">cache</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>Backpropagation in convolutional neural networks </b></h2><p>卷积神经网络的求导是比较难以理解的，这里有卷积层的求导和池化层的求导。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>1.Convolutional layer backward pass</b></h2><p>假设经过卷积层后我们的输出 <img src=\"https://www.zhihu.com/equation?tex=Z+%3D+W+%5Ctimes+A+%2Bb\" alt=\"Z = W \\times A +b\" eeimg=\"1\"/> </p><p>那么反向传播过程中需要求的就是 <img src=\"https://www.zhihu.com/equation?tex=dA%2CdW%2Cdb\" alt=\"dA,dW,db\" eeimg=\"1\"/> ，其中 <img src=\"https://www.zhihu.com/equation?tex=dA\" alt=\"dA\" eeimg=\"1\"/> 是原输入的数据，包含了原图像中的每一个像素，</p><p>而这个时候假设从后面传过来的 <img src=\"https://www.zhihu.com/equation?tex=dZ\" alt=\"dZ\" eeimg=\"1\"/> 是已经知道的。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>1.计算dA</b></p><p>从公式可以看出， <img src=\"https://www.zhihu.com/equation?tex=dA+%3D+W+%5Ctimes+dZ\" alt=\"dA = W \\times dZ\" eeimg=\"1\"/> ，具体一点， <img src=\"https://www.zhihu.com/equation?tex=dA\" alt=\"dA\" eeimg=\"1\"/> 的每一个切片就是 <img src=\"https://www.zhihu.com/equation?tex=W_c\" alt=\"W_c\" eeimg=\"1\"/> 乘上$dZ$在输出图片的<b>每一个像素</b>的求和结果，从矩阵的角度，每一次 <img src=\"https://www.zhihu.com/equation?tex=W_c%5Ctimes+dZ_%7Bhw%7D\" alt=\"W_c\\times dZ_{hw}\" eeimg=\"1\"/> 得到的就是从<b>单个输出的图片像素到输入图片切片（大小为W）</b>的映射。因此公式为：</p><p><img src=\"https://www.zhihu.com/equation?tex=dA+%2B%3D+%5Csum+_%7Bh%3D0%7D+%5E%7Bn_H%7D+%5Csum_%7Bw%3D0%7D+%5E%7Bn_W%7D+W_c+%5Ctimes+dZ_%7Bhw%7D+\" alt=\"dA += \\sum _{h=0} ^{n_H} \\sum_{w=0} ^{n_W} W_c \\times dZ_{hw} \" eeimg=\"1\"/> </p><div class=\"highlight\"><pre><code class=\"language-text\">da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><b>2.计算dW</b></p><p>$dW = A \\times dZ$，而更具体一点，因为<b>W对Z的每一个像素都是有作用的</b>，所以就等于每一个输入图片的切片乘以对应输出图片像素的导数，然后再求和！</p><p><img src=\"https://www.zhihu.com/equation?tex=dW_c++%2B%3D+%5Csum+_%7Bh%3D0%7D+%5E%7Bn_H%7D+%5Csum_%7Bw%3D0%7D+%5E+%7Bn_W%7D+a_%7Bslice%7D+%5Ctimes+dZ_%7Bhw%7D\" alt=\"dW_c  += \\sum _{h=0} ^{n_H} \\sum_{w=0} ^ {n_W} a_{slice} \\times dZ_{hw}\" eeimg=\"1\"/> </p><div class=\"highlight\"><pre><code class=\"language-text\">dW[:,:,:,c] += a_slice * dZ[i, h, w, c]</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><b>3.计算db</b></p><p><img src=\"https://www.zhihu.com/equation?tex=db+%3D+%5Csum_h+%5Csum_w+dZ_%7Bhw%7D+\" alt=\"db = \\sum_h \\sum_w dZ_{hw} \" eeimg=\"1\"/> </p><div class=\"highlight\"><pre><code class=\"language-text\">db[:,:,:,c] += dZ[i, h, w, c]</code></pre></div><p>所以得到以下：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">conv_backward</span><span class=\"p\">(</span><span class=\"n\">dZ</span><span class=\"p\">,</span> <span class=\"n\">cache</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Implement the backward propagation for a convolution function\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)\n</span><span class=\"s2\">    cache -- cache of values needed for the conv_backward(), output of conv_forward()\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n</span><span class=\"s2\">               numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n</span><span class=\"s2\">    dW -- gradient of the cost with respect to the weights of the conv layer (W)\n</span><span class=\"s2\">          numpy array of shape (f, f, n_C_prev, n_C)\n</span><span class=\"s2\">    db -- gradient of the cost with respect to the biases of the conv layer (b)\n</span><span class=\"s2\">          numpy array of shape (1, 1, 1, n_C)\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"c1\">### START CODE HERE ###</span>\n    <span class=\"c1\"># Retrieve information from &#34;cache&#34;</span>\n    <span class=\"p\">(</span><span class=\"n\">A_prev</span><span class=\"p\">,</span> <span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">hparameters</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">cache</span>\n    \n    <span class=\"c1\"># Retrieve dimensions from A_prev&#39;s shape</span>\n    <span class=\"p\">(</span><span class=\"n\">m</span><span class=\"p\">,</span> <span class=\"n\">n_H_prev</span><span class=\"p\">,</span> <span class=\"n\">n_W_prev</span><span class=\"p\">,</span> <span class=\"n\">n_C_prev</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">A_prev</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n    \n    <span class=\"c1\"># Retrieve dimensions from W&#39;s shape</span>\n    <span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">,</span> <span class=\"n\">f</span><span class=\"p\">,</span> <span class=\"n\">n_C_prev</span><span class=\"p\">,</span> <span class=\"n\">n_C</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">W</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n    \n    <span class=\"c1\"># Retrieve information from &#34;hparameters&#34;</span>\n    <span class=\"n\">stride</span> <span class=\"o\">=</span> <span class=\"n\">hparameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;stride&#39;</span><span class=\"p\">]</span>\n    <span class=\"n\">pad</span> <span class=\"o\">=</span> <span class=\"n\">hparameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;pad&#39;</span><span class=\"p\">]</span>\n    \n    <span class=\"c1\"># Retrieve dimensions from dZ&#39;s shape</span>\n    <span class=\"p\">(</span><span class=\"n\">m</span><span class=\"p\">,</span> <span class=\"n\">n_H</span><span class=\"p\">,</span> <span class=\"n\">n_W</span><span class=\"p\">,</span> <span class=\"n\">n_C</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">dZ</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n    \n    <span class=\"c1\"># Initialize dA_prev, dW, db with the correct shapes</span>\n    <span class=\"n\">dA_prev</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"n\">A_prev</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span>                           \n    <span class=\"n\">dW</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"n\">W</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span>\n    <span class=\"n\">db</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"n\">b</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span>\n<span class=\"err\">​</span>\n    <span class=\"c1\"># Pad A_prev and dA_prev</span>\n    <span class=\"n\">A_prev_pad</span> <span class=\"o\">=</span> <span class=\"n\">zero_pad</span><span class=\"p\">(</span><span class=\"n\">A_prev</span><span class=\"p\">,</span> <span class=\"n\">pad</span><span class=\"p\">)</span>\n    <span class=\"n\">dA_prev_pad</span> <span class=\"o\">=</span> <span class=\"n\">zero_pad</span><span class=\"p\">(</span><span class=\"n\">dA_prev</span><span class=\"p\">,</span> <span class=\"n\">pad</span><span class=\"p\">)</span>\n    \n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">m</span><span class=\"p\">):</span>                       <span class=\"c1\"># loop over the training examples</span>\n        \n        <span class=\"c1\"># select ith training example from A_prev_pad and dA_prev_pad</span>\n        <span class=\"n\">a_prev_pad</span> <span class=\"o\">=</span> <span class=\"n\">A_prev_pad</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span>\n        <span class=\"n\">da_prev_pad</span> <span class=\"o\">=</span> <span class=\"n\">dA_prev_pad</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span>\n        \n        <span class=\"k\">for</span> <span class=\"n\">h</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">n_H</span><span class=\"p\">):</span>                   <span class=\"c1\"># loop over vertical axis of the output volume</span>\n            <span class=\"k\">for</span> <span class=\"n\">w</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">n_W</span><span class=\"p\">):</span>               <span class=\"c1\"># loop over horizontal axis of the output volume</span>\n                <span class=\"k\">for</span> <span class=\"n\">c</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">n_C</span><span class=\"p\">):</span>           <span class=\"c1\"># loop over the channels of the output volume</span>\n                    \n                    <span class=\"c1\"># Find the corners of the current &#34;slice&#34;</span>\n                    <span class=\"n\">vert_start</span> <span class=\"o\">=</span> <span class=\"n\">h</span> <span class=\"o\">*</span> <span class=\"n\">stride</span>\n                    <span class=\"n\">vert_end</span> <span class=\"o\">=</span> <span class=\"n\">h</span> <span class=\"o\">*</span> <span class=\"n\">stride</span> <span class=\"o\">+</span> <span class=\"n\">f</span>\n                    <span class=\"n\">horiz_start</span> <span class=\"o\">=</span> <span class=\"n\">w</span> <span class=\"o\">*</span> <span class=\"n\">stride</span>\n                    <span class=\"n\">horiz_end</span> <span class=\"o\">=</span> <span class=\"n\">w</span> <span class=\"o\">*</span> <span class=\"n\">stride</span> <span class=\"o\">+</span> <span class=\"n\">f</span>\n                    \n                    <span class=\"c1\"># Use the corners to define the slice from a_prev_pad</span>\n                    <span class=\"n\">a_slice</span> <span class=\"o\">=</span> <span class=\"n\">a_prev_pad</span><span class=\"p\">[</span><span class=\"n\">vert_start</span> <span class=\"p\">:</span> <span class=\"n\">vert_end</span><span class=\"p\">,</span> <span class=\"n\">horiz_start</span> <span class=\"p\">:</span> <span class=\"n\">horiz_end</span><span class=\"p\">,</span> <span class=\"p\">:</span> <span class=\"p\">]</span>\n<span class=\"err\">​</span>\n                    <span class=\"c1\"># Update gradients for the window and the filter&#39;s parameters using the code formulas given above</span>\n                    <span class=\"n\">da_prev_pad</span><span class=\"p\">[</span><span class=\"n\">vert_start</span><span class=\"p\">:</span><span class=\"n\">vert_end</span><span class=\"p\">,</span> <span class=\"n\">horiz_start</span><span class=\"p\">:</span><span class=\"n\">horiz_end</span><span class=\"p\">,</span> <span class=\"p\">:]</span> <span class=\"o\">+=</span> <span class=\"n\">W</span><span class=\"p\">[:,:,:,</span><span class=\"n\">c</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"n\">dZ</span><span class=\"p\">[</span> <span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">h</span><span class=\"p\">,</span> <span class=\"n\">w</span> <span class=\"p\">,</span><span class=\"n\">c</span><span class=\"p\">]</span>\n<span class=\"err\">​</span>\n                    <span class=\"n\">dW</span><span class=\"p\">[:,:,:,</span><span class=\"n\">c</span><span class=\"p\">]</span> <span class=\"o\">+=</span> <span class=\"n\">a_slice</span> <span class=\"o\">*</span> <span class=\"n\">dZ</span><span class=\"p\">[</span> <span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">h</span><span class=\"p\">,</span> <span class=\"n\">w</span> <span class=\"p\">,</span><span class=\"n\">c</span><span class=\"p\">]</span>\n                    <span class=\"n\">db</span><span class=\"p\">[:,:,:,</span><span class=\"n\">c</span><span class=\"p\">]</span> <span class=\"o\">+=</span> <span class=\"n\">dZ</span><span class=\"p\">[</span> <span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">h</span><span class=\"p\">,</span> <span class=\"n\">w</span> <span class=\"p\">,</span><span class=\"n\">c</span><span class=\"p\">]</span>\n                    \n        <span class=\"c1\"># Set the ith training example&#39;s dA_prev to the unpaded da_prev_pad (Hint: use X[pad:-pad, pad:-pad, :])</span>\n        <span class=\"n\">dA_prev</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"p\">:,</span> <span class=\"p\">:,</span> <span class=\"p\">:]</span> <span class=\"o\">=</span> <span class=\"n\">da_prev_pad</span><span class=\"p\">[</span><span class=\"n\">pad</span><span class=\"p\">:</span><span class=\"o\">-</span><span class=\"n\">pad</span><span class=\"p\">,</span> <span class=\"n\">pad</span><span class=\"p\">:</span><span class=\"o\">-</span><span class=\"n\">pad</span><span class=\"p\">,</span> <span class=\"p\">:]</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"c1\"># Making sure your output shape is correct</span>\n    <span class=\"k\">assert</span><span class=\"p\">(</span><span class=\"n\">dA_prev</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"p\">(</span><span class=\"n\">m</span><span class=\"p\">,</span> <span class=\"n\">n_H_prev</span><span class=\"p\">,</span> <span class=\"n\">n_W_prev</span><span class=\"p\">,</span> <span class=\"n\">n_C_prev</span><span class=\"p\">))</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">dA_prev</span><span class=\"p\">,</span> <span class=\"n\">dW</span><span class=\"p\">,</span> <span class=\"n\">db</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>Pooling layer - backward pass</b></h2><p>这里max pooling和average poolling要分开处理。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>1. Max pooling - backward pass</b></p><p>假设pool size是$2 \\times 2$的，那么，4个像素中只有1个留下来了，其余的都没有效果了，所以在max pooling中，从后面传递过来的导数值，<b>只作用在max的那个元素，而且继续往前传递，不做任何改动，在其余3个元素的导数都是0</b>。</p><p>创建一个mask矩阵，让最大值为1，其余的都为0，这样子就可以作为一个映射矩阵向前映射了。</p><p><img src=\"https://www.zhihu.com/equation?tex=X+%3D+%5Cbegin%7Bbmatrix%7D1+%26%26+3++%5C%5C+4+%26%26+2+%5Cend%7Bbmatrix%7D+%5Cquad+%5Crightarrow++%5Cquad+M+%3D%5Cbegin%7Bbmatrix%7D+0+%26%26+0+%5C%5C+1+%26%26+0+%5Cend%7Bbmatrix%7D+\" alt=\"X = \\begin{bmatrix}1 &amp;&amp; 3  \\\\ 4 &amp;&amp; 2 \\end{bmatrix} \\quad \\rightarrow  \\quad M =\\begin{bmatrix} 0 &amp;&amp; 0 \\\\ 1 &amp;&amp; 0 \\end{bmatrix} \" eeimg=\"1\"/> </p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">create_mask_from_window</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Creates a mask from an input matrix x, to identify the max entry of x.\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    x -- Array of shape (f, f)\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    mask -- Array of the same shape as window, contains a True at the position corresponding to the max entry of x.\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"c1\">### START CODE HERE ### (≈1 line)</span>\n    <span class=\"n\">mask</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">x</span> <span class=\"o\">==</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">max</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">))</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">mask</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><b>2.  Average pooling - backward pass</b></p><p>和max不同，average pooling相当于把backward传过来的值分成了$n_H \\times n_W$等分。所以要计算的参数就比max pooling多很多了，这也就是为什么一般都用max pooling，不用average pooling</p><p><img src=\"https://www.zhihu.com/equation?tex=dZ+%3D+1+%5Cquad+%5Crightarrow++%5Cquad+dZ+%3D%5Cbegin%7Bbmatrix%7D+1%2F4+%26%26+1%2F4+%5C%5C+1%2F4+%26%26+1%2F4+%5Cend%7Bbmatrix%7D\" alt=\"dZ = 1 \\quad \\rightarrow  \\quad dZ =\\begin{bmatrix} 1/4 &amp;&amp; 1/4 \\\\ 1/4 &amp;&amp; 1/4 \\end{bmatrix}\" eeimg=\"1\"/> </p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">distribute_value</span><span class=\"p\">(</span><span class=\"n\">dz</span><span class=\"p\">,</span> <span class=\"n\">shape</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Distributes the input value in the matrix of dimension shape\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    dz -- input scalar\n</span><span class=\"s2\">    shape -- the shape (n_H, n_W) of the output matrix for which we want to distribute the value of dz\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    a -- Array of size (n_H, n_W) for which we distributed the value of dz\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"c1\">### START CODE HERE ###</span>\n    <span class=\"c1\"># Retrieve dimensions from shape (≈1 line)</span>\n    <span class=\"p\">(</span><span class=\"n\">n_H</span><span class=\"p\">,</span> <span class=\"n\">n_W</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">shape</span>\n    \n    <span class=\"c1\"># Compute the value to distribute on the matrix (≈1 line)</span>\n    <span class=\"n\">average</span> <span class=\"o\">=</span> <span class=\"n\">n_H</span> <span class=\"o\">*</span> <span class=\"n\">n_W</span>\n    \n    <span class=\"c1\"># Create a matrix where every entry is the &#34;average&#34; value (≈1 line)</span>\n    <span class=\"n\">a</span> <span class=\"o\">=</span> <span class=\"n\">dz</span> <span class=\"o\">/</span> <span class=\"n\">average</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">ones</span><span class=\"p\">((</span><span class=\"n\">n_H</span><span class=\"p\">,</span> <span class=\"n\">n_W</span><span class=\"p\">))</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">a</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>结合两种方法：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">pool_backward</span><span class=\"p\">(</span><span class=\"n\">dA</span><span class=\"p\">,</span> <span class=\"n\">cache</span><span class=\"p\">,</span> <span class=\"n\">mode</span> <span class=\"o\">=</span> <span class=\"s2\">&#34;max&#34;</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Implements the backward pass of the pooling layer\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    dA -- gradient of cost with respect to the output of the pooling layer, same shape as A\n</span><span class=\"s2\">    cache -- cache output from the forward pass of the pooling layer, contains the layer&#39;s input and hparameters \n</span><span class=\"s2\">    mode -- the pooling mode you would like to use, defined as a string (&#34;max&#34; or &#34;average&#34;)\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    dA_prev -- gradient of cost with respect to the input of the pooling layer, same shape as A_prev\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"c1\">### START CODE HERE ###</span>\n    \n    <span class=\"c1\"># Retrieve information from cache (≈1 line)</span>\n    <span class=\"p\">(</span><span class=\"n\">A_prev</span><span class=\"p\">,</span> <span class=\"n\">hparameters</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">cache</span>\n    \n    <span class=\"c1\"># Retrieve hyperparameters from &#34;hparameters&#34; (≈2 lines)</span>\n    <span class=\"n\">stride</span> <span class=\"o\">=</span> <span class=\"n\">hparameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;stride&#39;</span><span class=\"p\">]</span>\n    <span class=\"n\">f</span> <span class=\"o\">=</span> <span class=\"n\">hparameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;f&#39;</span><span class=\"p\">]</span>\n    \n    <span class=\"c1\"># Retrieve dimensions from A_prev&#39;s shape and dA&#39;s shape (≈2 lines)</span>\n    <span class=\"n\">m</span><span class=\"p\">,</span> <span class=\"n\">n_H_prev</span><span class=\"p\">,</span> <span class=\"n\">n_W_prev</span><span class=\"p\">,</span> <span class=\"n\">n_C_prev</span> <span class=\"o\">=</span> <span class=\"n\">A_prev</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n    <span class=\"n\">m</span><span class=\"p\">,</span> <span class=\"n\">n_H</span><span class=\"p\">,</span> <span class=\"n\">n_W</span><span class=\"p\">,</span> <span class=\"n\">n_C</span> <span class=\"o\">=</span> <span class=\"n\">dA</span><span class=\"o\">.</span><span class=\"n\">shape</span>\n    \n    <span class=\"c1\"># Initialize dA_prev with zeros (≈1 line)</span>\n    <span class=\"n\">dA_prev</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">(</span><span class=\"n\">A_prev</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span>\n    \n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">m</span><span class=\"p\">):</span>                       <span class=\"c1\"># loop over the training examples</span>\n        \n        <span class=\"c1\"># select training example from A_prev (≈1 line)</span>\n        <span class=\"n\">a_prev</span> <span class=\"o\">=</span> <span class=\"n\">A_prev</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span>\n        \n        <span class=\"k\">for</span> <span class=\"n\">h</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">n_H</span><span class=\"p\">):</span>                   <span class=\"c1\"># loop on the vertical axis</span>\n            <span class=\"k\">for</span> <span class=\"n\">w</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">n_W</span><span class=\"p\">):</span>               <span class=\"c1\"># loop on the horizontal axis</span>\n                <span class=\"k\">for</span> <span class=\"n\">c</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">n_C</span><span class=\"p\">):</span>           <span class=\"c1\"># loop over the channels (depth)</span>\n                    \n                    <span class=\"c1\"># Find the corners of the current &#34;slice&#34; (≈4 lines)</span>\n                    <span class=\"n\">vert_start</span> <span class=\"o\">=</span> <span class=\"n\">h</span> <span class=\"o\">*</span> <span class=\"n\">stride</span>\n                    <span class=\"n\">vert_end</span> <span class=\"o\">=</span> <span class=\"n\">vert_start</span> <span class=\"o\">+</span> <span class=\"n\">f</span>\n                    <span class=\"n\">horiz_start</span> <span class=\"o\">=</span> <span class=\"n\">w</span> <span class=\"o\">*</span> <span class=\"n\">stride</span>\n                    <span class=\"n\">horiz_end</span> <span class=\"o\">=</span> <span class=\"n\">horiz_start</span> <span class=\"o\">+</span> <span class=\"n\">f</span>\n                    \n                    <span class=\"c1\"># Compute the backward propagation in both modes.</span>\n                    <span class=\"k\">if</span> <span class=\"n\">mode</span> <span class=\"o\">==</span> <span class=\"s2\">&#34;max&#34;</span><span class=\"p\">:</span>\n                        \n                        <span class=\"c1\"># Use the corners and &#34;c&#34; to define the current slice from a_prev (≈1 line)</span>\n                        <span class=\"n\">a_prev_slice</span> <span class=\"o\">=</span> <span class=\"n\">a_prev</span><span class=\"p\">[</span><span class=\"n\">vert_start</span> <span class=\"p\">:</span> <span class=\"n\">vert_end</span><span class=\"p\">,</span> <span class=\"n\">horiz_start</span> <span class=\"p\">:</span> <span class=\"n\">horiz_end</span><span class=\"p\">,</span> <span class=\"n\">c</span><span class=\"p\">]</span>\n                        <span class=\"c1\"># Create the mask from a_prev_slice (≈1 line)</span>\n                        <span class=\"n\">mask</span> <span class=\"o\">=</span> <span class=\"n\">create_mask_from_window</span><span class=\"p\">(</span><span class=\"n\">a_prev_slice</span><span class=\"p\">)</span>\n                        <span class=\"c1\"># Set dA_prev to be dA_prev + (the mask multiplied by the correct entry of dA) (≈1 line)</span>\n                        <span class=\"n\">dA_prev</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">vert_start</span><span class=\"p\">:</span> <span class=\"n\">vert_end</span><span class=\"p\">,</span> <span class=\"n\">horiz_start</span><span class=\"p\">:</span> <span class=\"n\">horiz_end</span><span class=\"p\">,</span> <span class=\"n\">c</span><span class=\"p\">]</span> <span class=\"o\">+=</span> <span class=\"n\">mask</span> <span class=\"o\">*</span> <span class=\"n\">dA</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">h</span><span class=\"p\">,</span> <span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">c</span><span class=\"p\">]</span>\n                        \n                    <span class=\"k\">elif</span> <span class=\"n\">mode</span> <span class=\"o\">==</span> <span class=\"s2\">&#34;average&#34;</span><span class=\"p\">:</span>\n                        \n                        <span class=\"c1\"># Get the value a from dA (≈1 line)</span>\n                        <span class=\"n\">da</span> <span class=\"o\">=</span> <span class=\"n\">dA</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">h</span><span class=\"p\">,</span> <span class=\"n\">w</span><span class=\"p\">,</span> <span class=\"n\">c</span><span class=\"p\">]</span>\n                        <span class=\"c1\"># Define the shape of the filter as fxf (≈1 line)</span>\n                        <span class=\"n\">shape</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">f</span><span class=\"p\">,</span> <span class=\"n\">f</span><span class=\"p\">)</span>\n                        <span class=\"c1\"># Distribute it to get the correct slice of dA_prev. i.e. Add the distributed value of da. (≈1 line)</span>\n                        <span class=\"n\">dA_prev</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">vert_start</span><span class=\"p\">:</span> <span class=\"n\">vert_end</span><span class=\"p\">,</span> <span class=\"n\">horiz_start</span><span class=\"p\">:</span> <span class=\"n\">horiz_end</span><span class=\"p\">,</span> <span class=\"n\">c</span><span class=\"p\">]</span> <span class=\"o\">+=</span> <span class=\"n\">distribute_value</span><span class=\"p\">(</span><span class=\"n\">da</span><span class=\"p\">,</span> <span class=\"n\">shape</span><span class=\"p\">)</span>\n                        \n    <span class=\"c1\">### END CODE ###</span>\n    \n    <span class=\"c1\"># Making sure your output shape is correct</span>\n    <span class=\"k\">assert</span><span class=\"p\">(</span><span class=\"n\">dA_prev</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"n\">A_prev</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">dA_prev</span></code></pre></div><p></p><p></p><p></p>", 
            "topic": [
                {
                    "tag": "神经网络", 
                    "tagLink": "https://api.zhihu.com/topics/19607065"
                }, 
                {
                    "tag": "卷积神经网络（CNN）", 
                    "tagLink": "https://api.zhihu.com/topics/20043586"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/45752410", 
            "userName": "直上云霄", 
            "userLink": "https://www.zhihu.com/people/1033165ce4ad9c3fce69a0793dfab8ad", 
            "upvote": 0, 
            "title": "DeepLearning.ai笔记:(4-1)-卷积神经网络（Foundations of CNN）", 
            "content": "<div class=\"highlight\"><pre><code class=\"language-text\">title: &#39;DeepLearning.ai笔记:(4-1)-- 卷积神经网络（Foundations of CNN）&#39;\nid: dl-ai-4-1\ntags:\n  - dl.ai\ncategories:\n  - AI\n  - Deep Learning\ndate: 2018-09-30 10:20:54\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><b>首发于个人博客: </b></p><a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Fangzh的个人博客 | 人工智能拯救世界</a><p><b> 欢迎来访</b></p><p>第四门课开始就学习深度学习关于计算机视觉的重要应用---卷积神经网络。</p><p>第一周主要是对卷积神经网络的基本构造和原理做了介绍。</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>计算机视觉</b></h2><p>计算机视觉是深度学习的一个非常重要的应用。比如图像分类，目标检测，图片风格迁移等。</p><p>用传统的深度学习算法，假设你有一张 <img src=\"https://www.zhihu.com/equation?tex=64%C3%9764\" alt=\"64×64\" eeimg=\"1\"/> 的猫片，又有RGB三通道，那么这个时候是<img src=\"https://www.zhihu.com/equation?tex=64%C3%9764%C3%973%3D12288\" alt=\"64×64×3=12288\" eeimg=\"1\"/> ，input layer的维度就是12288，这样其实也还可以，因为图片很小。那么如果你有1000×1000的照片呢，你的向量就会有300万！假设有1000个隐藏神经元，那么就是第一层的参数矩阵W有30亿个参数！算到地老天荒。所以用传统的深度学习算法是不现实的。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>边缘检测</b></h2><p>如图，这些边缘检测中，用水平检测和垂直检测会得到不同的结果。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-1f56c747f128bf20150265cfa818269b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"628\" data-rawheight=\"289\" class=\"origin_image zh-lightbox-thumb\" width=\"628\" data-original=\"https://pic4.zhimg.com/v2-1f56c747f128bf20150265cfa818269b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;628&#39; height=&#39;289&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"628\" data-rawheight=\"289\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"628\" data-original=\"https://pic4.zhimg.com/v2-1f56c747f128bf20150265cfa818269b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-1f56c747f128bf20150265cfa818269b_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>垂直检测如下图，用一个3×3的过滤器（filter），也叫卷积核，在原图片6×6的对应地方按元素相乘，得到4×4的图片。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-05e528df409e83049377db340ec03a3f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1192\" data-rawheight=\"669\" class=\"origin_image zh-lightbox-thumb\" width=\"1192\" data-original=\"https://pic4.zhimg.com/v2-05e528df409e83049377db340ec03a3f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1192&#39; height=&#39;669&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1192\" data-rawheight=\"669\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1192\" data-original=\"https://pic4.zhimg.com/v2-05e528df409e83049377db340ec03a3f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-05e528df409e83049377db340ec03a3f_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>可以看到，用垂直边缘的filter可以将原图片中间的边缘区分出来，也就是得到了最右图中最亮的部分即为检测到的边缘。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>当然，如果左图的亮暗分界线反过来，则输出图片中最暗的部分表示边缘。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-85f83b1777b4f7e015c2ff92ffb312ae_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1540\" data-rawheight=\"866\" class=\"origin_image zh-lightbox-thumb\" width=\"1540\" data-original=\"https://pic3.zhimg.com/v2-85f83b1777b4f7e015c2ff92ffb312ae_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1540&#39; height=&#39;866&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1540\" data-rawheight=\"866\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1540\" data-original=\"https://pic3.zhimg.com/v2-85f83b1777b4f7e015c2ff92ffb312ae_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-85f83b1777b4f7e015c2ff92ffb312ae_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>也自然有水平的边缘分类器。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-c2a9609fbc25131f1fa6632e905dc3ea_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1049\" data-rawheight=\"334\" class=\"origin_image zh-lightbox-thumb\" width=\"1049\" data-original=\"https://pic3.zhimg.com/v2-c2a9609fbc25131f1fa6632e905dc3ea_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1049&#39; height=&#39;334&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1049\" data-rawheight=\"334\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1049\" data-original=\"https://pic3.zhimg.com/v2-c2a9609fbc25131f1fa6632e905dc3ea_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-c2a9609fbc25131f1fa6632e905dc3ea_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>还有更复杂的，但是我们不需要进行人工的决定这些filter是什么，因为我们可以通过训练，让机器自己学到这些参数。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-d0f41417ff116c9ca1171c4294b195af_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1445\" data-rawheight=\"809\" class=\"origin_image zh-lightbox-thumb\" width=\"1445\" data-original=\"https://pic4.zhimg.com/v2-d0f41417ff116c9ca1171c4294b195af_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1445&#39; height=&#39;809&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1445\" data-rawheight=\"809\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1445\" data-original=\"https://pic4.zhimg.com/v2-d0f41417ff116c9ca1171c4294b195af_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-d0f41417ff116c9ca1171c4294b195af_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>padding</b></h2><p>padding是填充的意思。</p><ul><li>我们可以从之前的例子看到，每经过一次卷积运算，图片的像素都会变小，从 <img src=\"https://www.zhihu.com/equation?tex=6%C3%976+---%3E+4%C3%974\" alt=\"6×6 ---&gt; 4×4\" eeimg=\"1\"/> ，这样子图片就会越来越小，后面就毛都不剩了。</li><li>还有一点就是，从卷积的运算方法来看，边缘和角落的位置卷积的次数少，会丢失有用信息。</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p>所以就有padding的想法了，也就是在图片四周填补上像素。</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3fa96325eb0edc023deff99f438e9edd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"439\" data-rawheight=\"391\" class=\"origin_image zh-lightbox-thumb\" width=\"439\" data-original=\"https://pic2.zhimg.com/v2-3fa96325eb0edc023deff99f438e9edd_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;439&#39; height=&#39;391&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"439\" data-rawheight=\"391\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"439\" data-original=\"https://pic2.zhimg.com/v2-3fa96325eb0edc023deff99f438e9edd_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-3fa96325eb0edc023deff99f438e9edd_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>计算方法如下，</p><p>原数据是 <img src=\"https://www.zhihu.com/equation?tex=n+%5Ctimes+n\" alt=\"n \\times n\" eeimg=\"1\"/> ，filter为 <img src=\"https://www.zhihu.com/equation?tex=f+%5Ctimes+f\" alt=\"f \\times f\" eeimg=\"1\"/> ,padding为 <img src=\"https://www.zhihu.com/equation?tex=p+%5Ctimes+p\" alt=\"p \\times p\" eeimg=\"1\"/> ，</p><p>那么得到的矩阵大小是 <img src=\"https://www.zhihu.com/equation?tex=%28n+%2B+2p+-f+%2B1%29%5Ctimes%28n+%2B+2p+-f+%2B1%29\" alt=\"(n + 2p -f +1)\\times(n + 2p -f +1)\" eeimg=\"1\"/> </p><p class=\"ztext-empty-paragraph\"><br/></p><p>padding有两种：</p><ul><li>valid：也就是不填充</li><li>same：输入与输出大小相同的图片, <img src=\"https://www.zhihu.com/equation?tex=p%3D%28f+-+1%29+%2F+2\" alt=\"p=(f - 1) / 2\" eeimg=\"1\"/> ，一般padding为奇数，因为filter是奇数</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>stride（步长）</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p>卷积的步长也就是每一次运算后平移的距离，之前使用都是stride=1。</p><p>假设stride=2，就会得到：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-931dcc80ec6b4c01795326386b4c3144_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1194\" data-rawheight=\"663\" class=\"origin_image zh-lightbox-thumb\" width=\"1194\" data-original=\"https://pic1.zhimg.com/v2-931dcc80ec6b4c01795326386b4c3144_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1194&#39; height=&#39;663&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1194\" data-rawheight=\"663\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1194\" data-original=\"https://pic1.zhimg.com/v2-931dcc80ec6b4c01795326386b4c3144_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-931dcc80ec6b4c01795326386b4c3144_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>得到的矩阵大小是</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Clfloor+%5Cfrac%7Bn%2B2p-f%7D%7Bs%7D%2B1%5Crfloor+%5Ctimes+%5Clfloor+%5Cfrac%7Bn%2B2p-f%7D%7Bs%7D%2B1%5Crfloor\" alt=\"\\lfloor \\frac{n+2p-f}{s}+1\\rfloor \\times \\lfloor \\frac{n+2p-f}{s}+1\\rfloor\" eeimg=\"1\"/> </p><p>向下取整: 59/60 = 0</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>立体卷积</b></h2><p>之前都是单通道的图片进行卷积，如果有RGB三种颜色的话，就要使用立体卷积了。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-03c2e414ecd8d927befc461f9b981858_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1352\" data-rawheight=\"785\" class=\"origin_image zh-lightbox-thumb\" width=\"1352\" data-original=\"https://pic1.zhimg.com/v2-03c2e414ecd8d927befc461f9b981858_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1352&#39; height=&#39;785&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1352\" data-rawheight=\"785\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1352\" data-original=\"https://pic1.zhimg.com/v2-03c2e414ecd8d927befc461f9b981858_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-03c2e414ecd8d927befc461f9b981858_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>这个时候的卷积核就变成了 <img src=\"https://www.zhihu.com/equation?tex=3+%5Ctimes+3+%5Ctimes+3\" alt=\"3 \\times 3 \\times 3\" eeimg=\"1\"/> 的三维卷积核，一共27个参数，每次对应着原图片上的RGB一共27个像素运算，然后求和得到输出图片的一个像素。因为只有一个卷积核，这个时候输出的还是 <img src=\"https://www.zhihu.com/equation?tex=4+%5Ctimes+4+%5Ctimes+1\" alt=\"4 \\times 4 \\times 1\" eeimg=\"1\"/> 的图片。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>多个卷积核</b></p><p>因为不同的卷积核可以提取不同的图片特征，所以可以有很多个卷积核，同时提取图片的特征，如分别提取图片的水平和垂直边缘特征。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-319b1bc448a41fa25cb6dac56aaccf91_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1433\" data-rawheight=\"798\" class=\"origin_image zh-lightbox-thumb\" width=\"1433\" data-original=\"https://pic2.zhimg.com/v2-319b1bc448a41fa25cb6dac56aaccf91_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1433&#39; height=&#39;798&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1433\" data-rawheight=\"798\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1433\" data-original=\"https://pic2.zhimg.com/v2-319b1bc448a41fa25cb6dac56aaccf91_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-319b1bc448a41fa25cb6dac56aaccf91_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>因为有了两个卷积核，这时候输出的图片就是有两通道的图片 <img src=\"https://www.zhihu.com/equation?tex=4%5Ctimes+4+%5Ctimes+2\" alt=\"4\\times 4 \\times 2\" eeimg=\"1\"/> 。</p><p>这里要搞清两个概念，卷积核的通道数和个数：</p><ul><li>通道数channel：即卷积核要作用在原图片上，原图片的通道处 <img src=\"https://www.zhihu.com/equation?tex=n_c\" alt=\"n_c\" eeimg=\"1\"/> ，卷积核的通道数必须和原图片通道数相同</li><li>个数：即要使用多少个这样的卷积核，使用 <img src=\"https://www.zhihu.com/equation?tex=n_%7Bc%7D%5E%7B%5Cprime%7D\" alt=\"n_{c}^{\\prime}\" eeimg=\"1\"/> 表示，卷积核的个数也就是输出图片的通道数，如有两个卷积核，那么生成了 <img src=\"https://www.zhihu.com/equation?tex=4%5Ctimes+4+%5Ctimes+2\" alt=\"4\\times 4 \\times 2\" eeimg=\"1\"/> 的图片，2  就是卷积核的个数</li><li>即 <img src=\"https://www.zhihu.com/equation?tex=n+%5Ctimes+n+%5Ctimes+n_c\" alt=\"n \\times n \\times n_c\" eeimg=\"1\"/> ，乘上的 <img src=\"https://www.zhihu.com/equation?tex=n_%7Bc%7D%5E%7B%5Cprime%7D\" alt=\"n_{c}^{\\prime}\" eeimg=\"1\"/> 个卷积核 <img src=\"https://www.zhihu.com/equation?tex=+f+%5Ctimes+f+%5Ctimes+n_c\" alt=\" f \\times f \\times n_c\" eeimg=\"1\"/> ，得到 <img src=\"https://www.zhihu.com/equation?tex=%28n+-f+%2B1%29%5Ctimes+%28n+-+f+%2B1+%29+%5Ctimes+n_%7Bc%7D%5E%7B%5Cprime%7D\" alt=\"(n -f +1)\\times (n - f +1 ) \\times n_{c}^{\\prime}\" eeimg=\"1\"/> 的新图片</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>卷积神经网络</b></h2><p><b>单层卷积网络</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-d7bc50593f9224d28a7c523821220189_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1702\" data-rawheight=\"949\" class=\"origin_image zh-lightbox-thumb\" width=\"1702\" data-original=\"https://pic2.zhimg.com/v2-d7bc50593f9224d28a7c523821220189_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1702&#39; height=&#39;949&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1702\" data-rawheight=\"949\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1702\" data-original=\"https://pic2.zhimg.com/v2-d7bc50593f9224d28a7c523821220189_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-d7bc50593f9224d28a7c523821220189_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>如图是单层卷积的基本过程，先经过两个卷积核，然后再加上bias进行relu激活函数。</p><p>那么假设某层卷积层有10个 <img src=\"https://www.zhihu.com/equation?tex=3+%5Ctimes+3+%5Ctimes+3\" alt=\"3 \\times 3 \\times 3\" eeimg=\"1\"/> 的卷积核，那么一共有 <img src=\"https://www.zhihu.com/equation?tex=%283%5Ctimes3%5Ctimes3%2B1%29+%5Ctimes10%3D280\" alt=\"(3\\times3\\times3+1) \\times10=280\" eeimg=\"1\"/> 个参数，加1是加上了bias</p><p>在这里总结了各个参数的表示方法：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-ad76c7c5adcf9b302070d9a7f105a772_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"797\" data-rawheight=\"429\" class=\"origin_image zh-lightbox-thumb\" width=\"797\" data-original=\"https://pic3.zhimg.com/v2-ad76c7c5adcf9b302070d9a7f105a772_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;797&#39; height=&#39;429&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"797\" data-rawheight=\"429\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"797\" data-original=\"https://pic3.zhimg.com/v2-ad76c7c5adcf9b302070d9a7f105a772_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-ad76c7c5adcf9b302070d9a7f105a772_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>简单神经网络</b></p><p>一般卷积神经网络层的类型有：</p><ul><li>convolution卷积层</li><li>pool池化层</li><li>fully connected全连接层</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>池化层</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p>pooling 的作用就是用来压缩数据，加速运算，提高提取特征的鲁棒性</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>Max pooling</b></p><p>在范围内取最大值</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-f609d24faa430ce85ba8a3dded2c0d33_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"982\" data-rawheight=\"579\" class=\"origin_image zh-lightbox-thumb\" width=\"982\" data-original=\"https://pic4.zhimg.com/v2-f609d24faa430ce85ba8a3dded2c0d33_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;982&#39; height=&#39;579&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"982\" data-rawheight=\"579\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"982\" data-original=\"https://pic4.zhimg.com/v2-f609d24faa430ce85ba8a3dded2c0d33_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-f609d24faa430ce85ba8a3dded2c0d33_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>Average Pooling</b></p><p>取平均值</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-498068d322a83fb8ccb54179e5ee9e45_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"915\" data-rawheight=\"443\" class=\"origin_image zh-lightbox-thumb\" width=\"915\" data-original=\"https://pic2.zhimg.com/v2-498068d322a83fb8ccb54179e5ee9e45_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;915&#39; height=&#39;443&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"915\" data-rawheight=\"443\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"915\" data-original=\"https://pic2.zhimg.com/v2-498068d322a83fb8ccb54179e5ee9e45_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-498068d322a83fb8ccb54179e5ee9e45_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>卷积神经网络示例</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b7351b0a940b4def5d05d0513cdf0645_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1260\" data-rawheight=\"705\" class=\"origin_image zh-lightbox-thumb\" width=\"1260\" data-original=\"https://pic2.zhimg.com/v2-b7351b0a940b4def5d05d0513cdf0645_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1260&#39; height=&#39;705&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1260\" data-rawheight=\"705\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1260\" data-original=\"https://pic2.zhimg.com/v2-b7351b0a940b4def5d05d0513cdf0645_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-b7351b0a940b4def5d05d0513cdf0645_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>一般conv后都会进行pooling，所以可以把conv和pooling当做一层。</p><p>如上图就是 <img src=\"https://www.zhihu.com/equation?tex=conv-pool-conv-pool-fc-fc-fc-softmax\" alt=\"conv-pool-conv-pool-fc-fc-fc-softmax\" eeimg=\"1\"/> 的卷积神经网络结构。</p><p>各个层的参数是这样的：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-54e8db85ed5217e6419214e70a91ff0f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"787\" data-rawheight=\"408\" class=\"origin_image zh-lightbox-thumb\" width=\"787\" data-original=\"https://pic4.zhimg.com/v2-54e8db85ed5217e6419214e70a91ff0f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;787&#39; height=&#39;408&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"787\" data-rawheight=\"408\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"787\" data-original=\"https://pic4.zhimg.com/v2-54e8db85ed5217e6419214e70a91ff0f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-54e8db85ed5217e6419214e70a91ff0f_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>可以看到，在卷积层的参数非常少，池化层没有参数，大量的参数在全连接层。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>为何用卷积神经网络？</b></h2><p>这里给出了两点主要原因：</p><ul><li>参数共享：卷积核的参数是原图片中各个像素之间共享的，所以大大减少了参数</li><li>连接的稀疏性：每个输出值，实际上只取决于很少量的输入而已。</li></ul><p></p>", 
            "topic": [
                {
                    "tag": "卷积神经网络（CNN）", 
                    "tagLink": "https://api.zhihu.com/topics/20043586"
                }, 
                {
                    "tag": "人工智能", 
                    "tagLink": "https://api.zhihu.com/topics/19551275"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/45752363", 
            "userName": "直上云霄", 
            "userLink": "https://www.zhihu.com/people/1033165ce4ad9c3fce69a0793dfab8ad", 
            "upvote": 0, 
            "title": "DeepLearning.ai笔记:(3-2)-- 机器学习策略(2)(ML strategy)", 
            "content": "<div class=\"highlight\"><pre><code class=\"language-text\">title: &#39;DeepLearning.ai笔记:(3-2)-- 机器学习策略(2)(ML strategy)&#39;\nid: 2018092017\ntags:\n  - dl.ai\ncategories:\n  - AI\n  - Deep Learning\ndate: 2018-09-20 17:59:04\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>首发于个人博客: </b></p><b><a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Fangzh的个人博客 | 人工智能拯救世界</a></b><p><b> 欢迎来访</b></p><p>这周继续讲了机器学习策略,包括误差分析、错误样本清楚、数据分布不同、迁移学习、多任务学习等。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>误差分析</b></h2><p>对于训练后的模型，如果不进行误差分析，那么很难提升精度。所以应该在验证集中，找到标记错误的那些样本，统计一下都是因为什么原因出现的错误，如是不是照片模糊，还是本来是猫把它标记成狗了等等。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>清除错误标记样本</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p>如果是随机的误差，也就是人为标记样本出现了随机错误，那么没有关系，因为算法对随即误差还是很有鲁棒性的。</p><p>如果是系统误差，那没办法了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>比如说总体误差是10%，然后发现因为人工错误标记引起的误差是0.6%，那么其他原因造成的误差就是9.4%，这个时候应该集中精力去找那9.4%的误差原因，并进行修正。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>快速搭建系统</b></h2><p>对于一个项目来说，我们一开始不要想得太复杂，先快速搭建一个基本的系统，进行迭代，然后在慢慢分析，逐步提高，不要想着一步到位，这样子往往会难以入手。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>不同分布的训练和测试</b></h2><p>假设你在网上找到了20万张照片去分析，但是我们实际上要测试的是用户在手机拍摄情况下的准确度。但是问题是手机上拍摄的数据不足，假设只有1万张。也就是训练集和测试集不是在同一分布，那么怎么办呢？</p><p>显然，如果把21万张照片加在一起，重新分配，是不合理的，因为这样子你验证集和测试集上的数据显然很少是手机拍摄的。</p><p>所以，应该用20万张照片，再加上5000张照片作为训练集，然后把剩下来的5000张照片对半分为验证集和测试集，那样子才更为符合实际情况。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>不同分布的偏差和方差</b></h2><p>如上述情况，你的训练集和验证测试集不同一分布的，假设training error：1%，dev error：10%，那么这个时候能说是方差太大吗，显然是不合理的，因为不是同一分布的。</p><p>那么这个时候应该重新定义一个集合，叫做训练验证集：train-dev</p><p>也就是在训练集中拿出一部分数据，跟验证集合在一起，不参与训练，这样我们就得到了：training error：1%，training-dev error：9%，dev error：10%，如果是这种情况，这样才能说是方差问题。</p><p>如果是training error：1%，training-dev error：1.5%，dev error：10%，那么，显然不是因为方差问题，而是因为分布不同而导致的。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>如何解决呢？</p><ul><li>进行人工误差分析，看一看训练集和测试集的差别到底在哪里，比如是不是有噪音、照片模糊等等</li><li>然后把训练集搞得更像测试集，也就是多收集点类似于测试集的数据，或者通过人工合成技术，把噪声加上去。</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>迁移学习</b></h2><p>如果我们现在训练了一个猫的分类器，然后这个时候有了新任务，要识别红绿灯，问题是，我们没有那么多红绿灯的照片，没有那么多的数据，那怎么办？这时候就可以把这个猫分类器学习的参数迁移到红绿灯分类器中，只要输出层的微调就行了。因为图像识别的神经网络，在前面的网络大多是进行一些特征提取，所以如果进行图像识别的迁移，还是很有帮助的！</p><p class=\"ztext-empty-paragraph\"><br/></p><p>但是迁移学习有限制：</p><ul><li>必须是相关的类型，比如都是图像识别，都是语音识别</li><li>A的数据远大于B，如果B的数据够多，那自己从头开始学不就好了</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>Muti-task多任务学习</b></h2><p>假设在自动驾驶中，需要同时检测很多物体，比如人、红绿灯，汽车等等。</p><p>那么就可以把这些都写到一个向量中：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-cea1214f03abc3aad62c68d5ec3e3e1b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1350\" data-rawheight=\"754\" class=\"origin_image zh-lightbox-thumb\" width=\"1350\" data-original=\"https://pic4.zhimg.com/v2-cea1214f03abc3aad62c68d5ec3e3e1b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1350&#39; height=&#39;754&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1350\" data-rawheight=\"754\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1350\" data-original=\"https://pic4.zhimg.com/v2-cea1214f03abc3aad62c68d5ec3e3e1b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-cea1214f03abc3aad62c68d5ec3e3e1b_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>如图，$y = [0 1 1 0]$即表示同时<b>有车和停车标志</b>。</p><p>这个又和softmax不同，softmax一次只识别一种物体，而多任务学习一次可以识别多种物体。</p><p>这个时候的loss funtion 和logistic是一样的：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-41024721469c7e4ea0999ec2873f3e25_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"646\" data-rawheight=\"77\" class=\"origin_image zh-lightbox-thumb\" width=\"646\" data-original=\"https://pic2.zhimg.com/v2-41024721469c7e4ea0999ec2873f3e25_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;646&#39; height=&#39;77&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"646\" data-rawheight=\"77\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"646\" data-original=\"https://pic2.zhimg.com/v2-41024721469c7e4ea0999ec2873f3e25_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-41024721469c7e4ea0999ec2873f3e25_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>如果在标注样本中，只标注了每张图片的一部分，比如说图片中有行人和车，只标注的行人，有没有车是不知道的，那么可以设为问号$y = [1 0 ? 0]$，这样也是可以训练的，但是在计算loss的时候，要把这个未标记的部分扣除，不要计算在内。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>端到端学习</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p>假如我们进行公司门禁，需要刷脸进入，那么这时候算法需要分成两步，</p><ul><li>首先检测到你这个人，然后找到人脸的位置</li><li>把人脸图像方法，然后在放入模型中计算是否匹配</li></ul><p>而端到端学习则直接忽略的这个过程，直接拍一张照片放入模型，输出结果。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>再比如说语音识别的时候，在数据少的情况下，我们可能需要</p><ul><li>提取声音</li><li>分析语法</li><li>切分成一个个发声字母</li><li>组成句子</li><li>翻译</li></ul><p>而端到端学习直接是：提取声音---&gt;翻译</p><p>就不需要人为的过多干预了，因为机器可以学到的比人为规定的还要好。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>但是注意一点是，需要很大量的数据的时候才能进行端到端学习；如果数据很少，那么还是手动干预，设计一些组件效果会好一点。</p><p></p>", 
            "topic": [
                {
                    "tag": "笔记", 
                    "tagLink": "https://api.zhihu.com/topics/19554982"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/45752269", 
            "userName": "直上云霄", 
            "userLink": "https://www.zhihu.com/people/1033165ce4ad9c3fce69a0793dfab8ad", 
            "upvote": 0, 
            "title": "DeepLearning.ai笔记:(3-1)-- 机器学习策略(1)(ML strategy)", 
            "content": "<div class=\"highlight\"><pre><code class=\"language-text\">title: &#39;DeepLearning.ai笔记:(3-1)-- 机器学习策略(1)(ML strategy)&#39;\nid: 2018092016\ntags:\n  - dl.ai\ncategories:\n  - AI\n  - Deep Learning\ndate: 2018-09-20 16:48:41\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><b>首发于个人博客: </b></p><b><a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Fangzh的个人博客 | 人工智能拯救世界</a></b><p><b> 欢迎来访</b></p><p>第三门课主要讲了机器学习的一些策略，也就是在你做项目的时候，应该要具体根据什么来改进你的模型。</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>正交化</b></h2><p>在设计过程中，最好是保证几个变量相互独立，也就是正交。就好比你在开车的时候，油门和方向盘是相互独立的。如果方向盘和油门不独立，当你调整方向盘的时候速度也在变化，就很难受了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>所以在监督学习中，以下几个应该正交：</p><ul><li>损失函数应该在训练集上表现很好<br/></li><ul><li>否则，就使用<b>更大的神经网络</b>，或者使用<b>更好的优化算法</b></li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>在验证集上表现很好<br/></li><ul><li>否则，就用<b>正则化</b>或者<b>训练集上要更多的数据</b></li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>在测试机上表现很好<br/></li><ul><li>否则，就使用<b>更大的验证集</b></li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>现实中表现很好<br/></li><ul><li>否则，就检查一下<b>验证集</b>是不是对的，<b>损失函数是不是好的</b></li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>单一数字评估指标</b></h2><p>在训练模型中，当然需要一种指标来评估一下模型是不是好的。</p><p>一般使用两个参数：</p><ul><li>准确率p：在预测的数据中，是正确的概率</li><li>召回率r：在真实数据中，预测是正确的概率</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p>一般用F1 Score把两个指标给统一起来：</p><p><img src=\"https://www.zhihu.com/equation?tex=F1-Score+%3D+%5Cfrac%7B2%7D%7B%5Cfrac%7B1%7D%7Bp%7D+%2B+%5Cfrac%7B1%7D%7Br%7D%7D\" alt=\"F1-Score = \\frac{2}{\\frac{1}{p} + \\frac{1}{r}}\" eeimg=\"1\"/> </p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>满足和优化指标</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p>一般，满足指标都是一个区间范围，比如时间上只要小于100ms就可以，这样子，就在满足满足指标的情况下，选择最优指标（如精确度最高）最好的那个模型。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>训练/验证/测试集的划分</b></h2><p>应该使验证集和测试集的数据满足统一分布。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>与人类表现比较</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p><b>可避免的偏差</b></p><p>我们训练出来的结果，应该和人类表现作比较，如果差距比较小，那么说明很接近了，如果差距比较大，应该着重优化缩小这个可避免的偏差。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-28b4f146c163b619b5f5929ec0970345_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1074\" data-rawheight=\"482\" class=\"origin_image zh-lightbox-thumb\" width=\"1074\" data-original=\"https://pic2.zhimg.com/v2-28b4f146c163b619b5f5929ec0970345_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1074&#39; height=&#39;482&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1074\" data-rawheight=\"482\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1074\" data-original=\"https://pic2.zhimg.com/v2-28b4f146c163b619b5f5929ec0970345_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-28b4f146c163b619b5f5929ec0970345_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>如图，左边说明应该着重减小bias，右边应该着重减小variance</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>改善模型的表现</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p>减少bias：</p><ul><li>训练更大的模型</li><li>更长的时间，更优化的算法（Momentum，RMSprop，Adam）</li><li>寻找更好的网络架构、更好的参数</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p>减少variance：</p><ul><li>收集更多的数据</li><li>正则化</li><li>更好的架构和参数</li></ul>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "笔记", 
                    "tagLink": "https://api.zhihu.com/topics/19554982"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/45752207", 
            "userName": "直上云霄", 
            "userLink": "https://www.zhihu.com/people/1033165ce4ad9c3fce69a0793dfab8ad", 
            "upvote": 0, 
            "title": "DeepLearning.ai作业:(2-3)-- 超参数调试", 
            "content": "<div class=\"highlight\"><pre><code class=\"language-text\">title: &#39;DeepLearning.ai作业:(2-3)-- 超参数调试（Hyperparameter tuning）&#39;\nid: 2018091810\ntags:\n  - dl.ai\n  - homework\ncategories:\n  - AI\n  - Deep Learning\ndate: 2018-09-18 10:35:32\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><b>首发于个人博客:[<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Fangzh的个人博客 | 人工智能拯救世界</a>](<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Fangzh的个人博客 | 人工智能拯救世界</a>)，欢迎来访</b></p><ol><li>不要抄作业！</li><li>我只是把思路整理了，供个人学习。</li><li>不要抄作业！</li></ol><p>本周主要是TensorFlow的简单教程，没什么好说的，可以去看看更详细一点的教程。</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: linear_function</span>\n<span class=\"err\">​</span>\n<span class=\"k\">def</span> <span class=\"nf\">linear_function</span><span class=\"p\">():</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Implements a linear function: \n</span><span class=\"s2\">            Initializes W to be a random tensor of shape (4,3)\n</span><span class=\"s2\">            Initializes X to be a random tensor of shape (3,1)\n</span><span class=\"s2\">            Initializes b to be a random tensor of shape (4,1)\n</span><span class=\"s2\">    Returns: \n</span><span class=\"s2\">    result -- runs the session for Y = WX + b \n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">seed</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n    \n    <span class=\"c1\">### START CODE HERE ### (4 lines of code)</span>\n    <span class=\"n\">X</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">constant</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">name</span> <span class=\"o\">=</span> <span class=\"s2\">&#34;X&#34;</span><span class=\"p\">)</span>\n    <span class=\"n\">W</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">constant</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">4</span><span class=\"p\">,</span><span class=\"mi\">3</span><span class=\"p\">),</span> <span class=\"n\">name</span> <span class=\"o\">=</span> <span class=\"s2\">&#34;W&#34;</span><span class=\"p\">)</span>\n    <span class=\"n\">b</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">constant</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"mi\">4</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">),</span> <span class=\"n\">name</span> <span class=\"o\">=</span> <span class=\"s2\">&#34;b&#34;</span><span class=\"p\">)</span>\n    <span class=\"n\">Y</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">W</span><span class=\"p\">,</span><span class=\"n\">X</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">b</span>\n    <span class=\"c1\">### END CODE HERE ### </span>\n    \n    <span class=\"c1\"># Create the session using tf.Session() and run it with sess.run(...) on the variable you want to calculate</span>\n    \n    <span class=\"c1\">### START CODE HERE ###</span>\n    <span class=\"n\">sess</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">Session</span><span class=\"p\">()</span>\n    <span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">Y</span><span class=\"p\">)</span>\n    <span class=\"c1\">### END CODE HERE ### </span>\n    \n    <span class=\"c1\"># close the session </span>\n    <span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">close</span><span class=\"p\">()</span>\n<span class=\"err\">​</span>\n    <span class=\"k\">return</span> <span class=\"n\">result</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: sigmoid</span>\n<span class=\"err\">​</span>\n<span class=\"k\">def</span> <span class=\"nf\">sigmoid</span><span class=\"p\">(</span><span class=\"n\">z</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Computes the sigmoid of z\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    z -- input value, scalar or vector\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns: \n</span><span class=\"s2\">    results -- the sigmoid of z\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"c1\">### START CODE HERE ### ( approx. 4 lines of code)</span>\n    <span class=\"c1\"># Create a placeholder for x. Name it &#39;x&#39;.</span>\n    <span class=\"n\">x</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">placeholder</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">,</span><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s2\">&#34;x&#34;</span><span class=\"p\">)</span>\n<span class=\"err\">​</span>\n    <span class=\"c1\"># compute sigmoid(x)</span>\n    <span class=\"n\">sigmoid</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">sigmoid</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">)</span>\n<span class=\"err\">​</span>\n    <span class=\"c1\"># Create a session, and run it. Please use the method 2 explained above. </span>\n    <span class=\"c1\"># You should use a feed_dict to pass z&#39;s value to x. </span>\n    <span class=\"k\">with</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">Session</span><span class=\"p\">()</span> <span class=\"k\">as</span> <span class=\"n\">sess</span><span class=\"p\">:</span>\n        <span class=\"c1\"># Run session and call the output &#34;result&#34;</span>\n        <span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">sigmoid</span><span class=\"p\">,</span><span class=\"n\">feed_dict</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"n\">x</span><span class=\"p\">:</span><span class=\"n\">z</span><span class=\"p\">})</span>\n    \n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">result</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: cost</span>\n<span class=\"err\">​</span>\n<span class=\"k\">def</span> <span class=\"nf\">cost</span><span class=\"p\">(</span><span class=\"n\">logits</span><span class=\"p\">,</span> <span class=\"n\">labels</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Computes the cost using the sigmoid cross entropy\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    logits -- vector containing z, output of the last linear unit (before the final sigmoid activation)\n</span><span class=\"s2\">    labels -- vector of labels y (1 or 0) \n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Note: What we&#39;ve been calling &#34;z&#34; and &#34;y&#34; in this class are respectively called &#34;logits&#34; and &#34;labels&#34; \n</span><span class=\"s2\">    in the TensorFlow documentation. So logits will feed into z, and labels into y. \n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    cost -- runs the session of the cost (formula (2))\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"c1\">### START CODE HERE ### </span>\n    \n    <span class=\"c1\"># Create the placeholders for &#34;logits&#34; (z) and &#34;labels&#34; (y) (approx. 2 lines)</span>\n    <span class=\"n\">z</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">placeholder</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">,</span><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s2\">&#34;z&#34;</span><span class=\"p\">)</span>\n    <span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">placeholder</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">,</span><span class=\"n\">name</span><span class=\"o\">=</span><span class=\"s2\">&#34;y&#34;</span><span class=\"p\">)</span>\n    \n    <span class=\"c1\"># Use the loss function (approx. 1 line)</span>\n    <span class=\"n\">cost</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">sigmoid_cross_entropy_with_logits</span><span class=\"p\">(</span><span class=\"n\">logits</span><span class=\"o\">=</span><span class=\"n\">z</span><span class=\"p\">,</span><span class=\"n\">labels</span><span class=\"o\">=</span><span class=\"n\">y</span><span class=\"p\">)</span>\n    \n    <span class=\"c1\"># Create a session (approx. 1 line). See method 1 above.</span>\n    <span class=\"n\">sess</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">Session</span><span class=\"p\">()</span>\n    \n    <span class=\"c1\"># Run the session (approx. 1 line).</span>\n    <span class=\"n\">cost</span> <span class=\"o\">=</span> <span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">cost</span><span class=\"p\">,</span><span class=\"n\">feed_dict</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"n\">z</span><span class=\"p\">:</span><span class=\"n\">logits</span><span class=\"p\">,</span><span class=\"n\">y</span><span class=\"p\">:</span><span class=\"n\">labels</span><span class=\"p\">})</span>\n    \n    <span class=\"c1\"># Close the session (approx. 1 line). See method 1 above.</span>\n    <span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">close</span><span class=\"p\">()</span>\n    \n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">cost</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: one_hot_matrix</span>\n<span class=\"err\">​</span>\n<span class=\"k\">def</span> <span class=\"nf\">one_hot_matrix</span><span class=\"p\">(</span><span class=\"n\">labels</span><span class=\"p\">,</span> <span class=\"n\">C</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Creates a matrix where the i-th row corresponds to the ith class number and the jth column\n</span><span class=\"s2\">                     corresponds to the jth training example. So if example j had a label i. Then entry (i,j) \n</span><span class=\"s2\">                     will be 1. \n</span><span class=\"s2\">                     \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    labels -- vector containing the labels \n</span><span class=\"s2\">    C -- number of classes, the depth of the one hot dimension\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns: \n</span><span class=\"s2\">    one_hot -- one hot matrix\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"c1\">### START CODE HERE ###</span>\n    \n    <span class=\"c1\"># Create a tf.constant equal to C (depth), name it &#39;C&#39;. (approx. 1 line)</span>\n    <span class=\"n\">C</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">constant</span><span class=\"p\">(</span><span class=\"n\">C</span><span class=\"p\">)</span>\n    \n    <span class=\"c1\"># Use tf.one_hot, be careful with the axis (approx. 1 line)</span>\n    <span class=\"n\">one_hot_matrix</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">one_hot</span><span class=\"p\">(</span><span class=\"n\">labels</span><span class=\"p\">,</span> <span class=\"n\">C</span><span class=\"p\">,</span> <span class=\"n\">axis</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">)</span>\n    \n    <span class=\"c1\"># Create the session (approx. 1 line)</span>\n    <span class=\"n\">sess</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">Session</span><span class=\"p\">()</span>\n    \n    <span class=\"c1\"># Run the session (approx. 1 line)</span>\n    <span class=\"n\">one_hot</span> <span class=\"o\">=</span> <span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">one_hot_matrix</span><span class=\"p\">)</span>\n    \n    <span class=\"c1\"># Close the session (approx. 1 line). See method 1 above.</span>\n    <span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">close</span><span class=\"p\">()</span>\n    \n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">one_hot</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: ones</span>\n<span class=\"err\">​</span>\n<span class=\"k\">def</span> <span class=\"nf\">ones</span><span class=\"p\">(</span><span class=\"n\">shape</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Creates an array of ones of dimension shape\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    shape -- shape of the array you want to create\n</span><span class=\"s2\">        \n</span><span class=\"s2\">    Returns: \n</span><span class=\"s2\">    ones -- array containing only ones\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"c1\">### START CODE HERE ###</span>\n    \n    <span class=\"c1\"># Create &#34;ones&#34; tensor using tf.ones(...). (approx. 1 line)</span>\n    <span class=\"n\">ones</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">ones</span><span class=\"p\">(</span><span class=\"n\">shape</span><span class=\"p\">)</span>\n    \n    <span class=\"c1\"># Create the session (approx. 1 line)</span>\n    <span class=\"n\">sess</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">Session</span><span class=\"p\">()</span>\n    \n    <span class=\"c1\"># Run the session to compute &#39;ones&#39; (approx. 1 line)</span>\n    <span class=\"n\">ones</span> <span class=\"o\">=</span> <span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">ones</span><span class=\"p\">)</span>\n    \n    <span class=\"c1\"># Close the session (approx. 1 line). See method 1 above.</span>\n    <span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">close</span><span class=\"p\">()</span>\n    \n    <span class=\"c1\">### END CODE HERE ###</span>\n    <span class=\"k\">return</span> <span class=\"n\">ones</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>Building neural network</b></h2><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: create_placeholders</span>\n<span class=\"err\">​</span>\n<span class=\"k\">def</span> <span class=\"nf\">create_placeholders</span><span class=\"p\">(</span><span class=\"n\">n_x</span><span class=\"p\">,</span> <span class=\"n\">n_y</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Creates the placeholders for the tensorflow session.\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    n_x -- scalar, size of an image vector (num_px * num_px = 64 * 64 * 3 = 12288)\n</span><span class=\"s2\">    n_y -- scalar, number of classes (from 0 to 5, so -&gt; 6)\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    X -- placeholder for the data input, of shape [n_x, None] and dtype &#34;float&#34;\n</span><span class=\"s2\">    Y -- placeholder for the input labels, of shape [n_y, None] and dtype &#34;float&#34;\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Tips:\n</span><span class=\"s2\">    - You will use None because it let&#39;s us be flexible on the number of examples you will for the placeholders.\n</span><span class=\"s2\">      In fact, the number of examples during test/train is different.\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n<span class=\"err\">​</span>\n    <span class=\"c1\">### START CODE HERE ### (approx. 2 lines)</span>\n    <span class=\"n\">X</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">placeholder</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">,[</span><span class=\"n\">n_x</span><span class=\"p\">,</span><span class=\"bp\">None</span><span class=\"p\">])</span>\n    <span class=\"n\">Y</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">placeholder</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">,[</span><span class=\"n\">n_y</span><span class=\"p\">,</span><span class=\"bp\">None</span><span class=\"p\">])</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: initialize_parameters</span>\n<span class=\"err\">​</span>\n<span class=\"k\">def</span> <span class=\"nf\">initialize_parameters</span><span class=\"p\">():</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Initializes parameters to build a neural network with tensorflow. The shapes are:\n</span><span class=\"s2\">                        W1 : [25, 12288]\n</span><span class=\"s2\">                        b1 : [25, 1]\n</span><span class=\"s2\">                        W2 : [12, 25]\n</span><span class=\"s2\">                        b2 : [12, 1]\n</span><span class=\"s2\">                        W3 : [6, 12]\n</span><span class=\"s2\">                        b3 : [6, 1]\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    parameters -- a dictionary of tensors containing W1, b1, W2, b2, W3, b3\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">set_random_seed</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>                   <span class=\"c1\"># so that your &#34;random&#34; numbers match ours</span>\n        \n    <span class=\"c1\">### START CODE HERE ### (approx. 6 lines of code)</span>\n    <span class=\"n\">W1</span> <span class=\"o\">=</span>  <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">get_variable</span><span class=\"p\">(</span><span class=\"s2\">&#34;W1&#34;</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"mi\">25</span><span class=\"p\">,</span><span class=\"mi\">12288</span><span class=\"p\">],</span> <span class=\"n\">initializer</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">contrib</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">xavier_initializer</span><span class=\"p\">(</span><span class=\"n\">seed</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n    <span class=\"n\">b1</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">get_variable</span><span class=\"p\">(</span><span class=\"s2\">&#34;b1&#34;</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"mi\">25</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">initializer</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">zeros_initializer</span><span class=\"p\">())</span>\n    <span class=\"n\">W2</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">get_variable</span><span class=\"p\">(</span><span class=\"s2\">&#34;W2&#34;</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"mi\">12</span><span class=\"p\">,</span><span class=\"mi\">25</span><span class=\"p\">],</span> <span class=\"n\">initializer</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">contrib</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">xavier_initializer</span><span class=\"p\">(</span><span class=\"n\">seed</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n    <span class=\"n\">b2</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">get_variable</span><span class=\"p\">(</span><span class=\"s2\">&#34;b2&#34;</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"mi\">12</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">initializer</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">zeros_initializer</span><span class=\"p\">())</span>\n    <span class=\"n\">W3</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">get_variable</span><span class=\"p\">(</span><span class=\"s2\">&#34;W3&#34;</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"mi\">6</span><span class=\"p\">,</span><span class=\"mi\">12</span><span class=\"p\">],</span> <span class=\"n\">initializer</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">contrib</span><span class=\"o\">.</span><span class=\"n\">layers</span><span class=\"o\">.</span><span class=\"n\">xavier_initializer</span><span class=\"p\">(</span><span class=\"n\">seed</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n    <span class=\"n\">b3</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">get_variable</span><span class=\"p\">(</span><span class=\"s2\">&#34;b3&#34;</span><span class=\"p\">,</span> <span class=\"p\">[</span><span class=\"mi\">6</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">],</span> <span class=\"n\">initializer</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">zeros_initializer</span><span class=\"p\">())</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n<span class=\"err\">​</span>\n    <span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">&#34;W1&#34;</span><span class=\"p\">:</span> <span class=\"n\">W1</span><span class=\"p\">,</span>\n                  <span class=\"s2\">&#34;b1&#34;</span><span class=\"p\">:</span> <span class=\"n\">b1</span><span class=\"p\">,</span>\n                  <span class=\"s2\">&#34;W2&#34;</span><span class=\"p\">:</span> <span class=\"n\">W2</span><span class=\"p\">,</span>\n                  <span class=\"s2\">&#34;b2&#34;</span><span class=\"p\">:</span> <span class=\"n\">b2</span><span class=\"p\">,</span>\n                  <span class=\"s2\">&#34;W3&#34;</span><span class=\"p\">:</span> <span class=\"n\">W3</span><span class=\"p\">,</span>\n                  <span class=\"s2\">&#34;b3&#34;</span><span class=\"p\">:</span> <span class=\"n\">b3</span><span class=\"p\">}</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">parameters</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: forward_propagation</span>\n<span class=\"err\">​</span>\n<span class=\"k\">def</span> <span class=\"nf\">forward_propagation</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">parameters</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Implements the forward propagation for the model: LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SOFTMAX\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    X -- input dataset placeholder, of shape (input size, number of examples)\n</span><span class=\"s2\">    parameters -- python dictionary containing your parameters &#34;W1&#34;, &#34;b1&#34;, &#34;W2&#34;, &#34;b2&#34;, &#34;W3&#34;, &#34;b3&#34;\n</span><span class=\"s2\">                  the shapes are given in initialize_parameters\n</span><span class=\"s2\">​\n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    Z3 -- the output of the last LINEAR unit\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"c1\"># Retrieve the parameters from the dictionary &#34;parameters&#34; </span>\n    <span class=\"n\">W1</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;W1&#39;</span><span class=\"p\">]</span>\n    <span class=\"n\">b1</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;b1&#39;</span><span class=\"p\">]</span>\n    <span class=\"n\">W2</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;W2&#39;</span><span class=\"p\">]</span>\n    <span class=\"n\">b2</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;b2&#39;</span><span class=\"p\">]</span>\n    <span class=\"n\">W3</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;W3&#39;</span><span class=\"p\">]</span>\n    <span class=\"n\">b3</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;b3&#39;</span><span class=\"p\">]</span>\n    \n    <span class=\"c1\">### START CODE HERE ### (approx. 5 lines)              # Numpy Equivalents:</span>\n    <span class=\"n\">Z1</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">W1</span><span class=\"p\">,</span><span class=\"n\">X</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">b1</span>                                              <span class=\"c1\"># Z1 = np.dot(W1, X) + b1</span>\n    <span class=\"n\">A1</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">relu</span><span class=\"p\">(</span><span class=\"n\">Z1</span><span class=\"p\">)</span>                                              <span class=\"c1\"># A1 = relu(Z1)</span>\n    <span class=\"n\">Z2</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">W2</span><span class=\"p\">,</span><span class=\"n\">A1</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">b2</span>                                              <span class=\"c1\"># Z2 = np.dot(W2, a1) + b2</span>\n    <span class=\"n\">A2</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">relu</span><span class=\"p\">(</span><span class=\"n\">Z2</span><span class=\"p\">)</span>                                              <span class=\"c1\"># A2 = relu(Z2)</span>\n    <span class=\"n\">Z3</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">W3</span><span class=\"p\">,</span><span class=\"n\">A2</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">b3</span>                                              <span class=\"c1\"># Z3 = np.dot(W3,Z2) + b3</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">Z3</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: compute_cost </span>\n<span class=\"err\">​</span>\n<span class=\"k\">def</span> <span class=\"nf\">compute_cost</span><span class=\"p\">(</span><span class=\"n\">Z3</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Computes the cost\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (6, number of examples)\n</span><span class=\"s2\">    Y -- &#34;true&#34; labels vector placeholder, same shape as Z3\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    cost - Tensor of the cost function\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"c1\"># to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)</span>\n    <span class=\"n\">logits</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">transpose</span><span class=\"p\">(</span><span class=\"n\">Z3</span><span class=\"p\">)</span>\n    <span class=\"n\">labels</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">transpose</span><span class=\"p\">(</span><span class=\"n\">Y</span><span class=\"p\">)</span>\n    \n    <span class=\"c1\">### START CODE HERE ### (1 line of code)</span>\n    <span class=\"n\">cost</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">reduce_mean</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">softmax_cross_entropy_with_logits</span><span class=\"p\">(</span><span class=\"n\">logits</span> <span class=\"o\">=</span><span class=\"n\">logits</span><span class=\"p\">,</span> <span class=\"n\">labels</span> <span class=\"o\">=</span> <span class=\"n\">labels</span><span class=\"p\">))</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">cost</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">model</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">,</span> <span class=\"n\">Y_train</span><span class=\"p\">,</span> <span class=\"n\">X_test</span><span class=\"p\">,</span> <span class=\"n\">Y_test</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span> <span class=\"o\">=</span> <span class=\"mf\">0.0001</span><span class=\"p\">,</span>\n          <span class=\"n\">num_epochs</span> <span class=\"o\">=</span> <span class=\"mi\">1500</span><span class=\"p\">,</span> <span class=\"n\">minibatch_size</span> <span class=\"o\">=</span> <span class=\"mi\">32</span><span class=\"p\">,</span> <span class=\"n\">print_cost</span> <span class=\"o\">=</span> <span class=\"bp\">True</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Implements a three-layer tensorflow neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;RELU-&gt;LINEAR-&gt;SOFTMAX.\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    X_train -- training set, of shape (input size = 12288, number of training examples = 1080)\n</span><span class=\"s2\">    Y_train -- test set, of shape (output size = 6, number of training examples = 1080)\n</span><span class=\"s2\">    X_test -- training set, of shape (input size = 12288, number of training examples = 120)\n</span><span class=\"s2\">    Y_test -- test set, of shape (output size = 6, number of test examples = 120)\n</span><span class=\"s2\">    learning_rate -- learning rate of the optimization\n</span><span class=\"s2\">    num_epochs -- number of epochs of the optimization loop\n</span><span class=\"s2\">    minibatch_size -- size of a minibatch\n</span><span class=\"s2\">    print_cost -- True to print the cost every 100 epochs\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    parameters -- parameters learnt by the model. They can then be used to predict.\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"n\">ops</span><span class=\"o\">.</span><span class=\"n\">reset_default_graph</span><span class=\"p\">()</span>                         <span class=\"c1\"># to be able to rerun the model without overwriting tf variables</span>\n    <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">set_random_seed</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>                             <span class=\"c1\"># to keep consistent results</span>\n    <span class=\"n\">seed</span> <span class=\"o\">=</span> <span class=\"mi\">3</span>                                          <span class=\"c1\"># to keep consistent results</span>\n    <span class=\"p\">(</span><span class=\"n\">n_x</span><span class=\"p\">,</span> <span class=\"n\">m</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">X_train</span><span class=\"o\">.</span><span class=\"n\">shape</span>                          <span class=\"c1\"># (n_x: input size, m : number of examples in the train set)</span>\n    <span class=\"n\">n_y</span> <span class=\"o\">=</span> <span class=\"n\">Y_train</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>                            <span class=\"c1\"># n_y : output size</span>\n    <span class=\"n\">costs</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>                                        <span class=\"c1\"># To keep track of the cost</span>\n    \n    <span class=\"c1\"># Create Placeholders of shape (n_x, n_y)</span>\n    <span class=\"c1\">### START CODE HERE ### (1 line)</span>\n    <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span> <span class=\"o\">=</span> <span class=\"n\">create_placeholders</span><span class=\"p\">(</span><span class=\"n\">n_x</span><span class=\"p\">,</span><span class=\"n\">n_y</span><span class=\"p\">)</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n<span class=\"err\">​</span>\n    <span class=\"c1\"># Initialize parameters</span>\n    <span class=\"c1\">### START CODE HERE ### (1 line)</span>\n    <span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"n\">initialize_parameters</span><span class=\"p\">()</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"c1\"># Forward propagation: Build the forward propagation in the tensorflow graph</span>\n    <span class=\"c1\">### START CODE HERE ### (1 line)</span>\n    <span class=\"n\">Z3</span> <span class=\"o\">=</span> <span class=\"n\">forward_propagation</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">parameters</span><span class=\"p\">)</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"c1\"># Cost function: Add cost function to tensorflow graph</span>\n    <span class=\"c1\">### START CODE HERE ### (1 line)</span>\n    <span class=\"n\">cost</span> <span class=\"o\">=</span> <span class=\"n\">compute_cost</span><span class=\"p\">(</span><span class=\"n\">Z3</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">)</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"c1\"># Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.</span>\n    <span class=\"c1\">### START CODE HERE ### (1 line)</span>\n    <span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">optimizer</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">train</span><span class=\"o\">.</span><span class=\"n\">GradientDescentOptimizer</span><span class=\"p\">(</span><span class=\"n\">learning_rate</span> <span class=\"o\">=</span> <span class=\"n\">learning_rate</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">minimize</span><span class=\"p\">(</span><span class=\"n\">cost</span><span class=\"p\">)</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"c1\"># Initialize all the variables</span>\n    <span class=\"n\">init</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">global_variables_initializer</span><span class=\"p\">()</span>\n<span class=\"err\">​</span>\n    <span class=\"c1\"># Start the session to compute the tensorflow graph</span>\n    <span class=\"k\">with</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">Session</span><span class=\"p\">()</span> <span class=\"k\">as</span> <span class=\"n\">sess</span><span class=\"p\">:</span>\n        \n        <span class=\"c1\"># Run the initialization</span>\n        <span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">init</span><span class=\"p\">)</span>\n        \n        <span class=\"c1\"># Do the training loop</span>\n        <span class=\"k\">for</span> <span class=\"n\">epoch</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">num_epochs</span><span class=\"p\">):</span>\n<span class=\"err\">​</span>\n            <span class=\"n\">epoch_cost</span> <span class=\"o\">=</span> <span class=\"mf\">0.</span>                       <span class=\"c1\"># Defines a cost related to an epoch</span>\n            <span class=\"n\">num_minibatches</span> <span class=\"o\">=</span> <span class=\"nb\">int</span><span class=\"p\">(</span><span class=\"n\">m</span> <span class=\"o\">/</span> <span class=\"n\">minibatch_size</span><span class=\"p\">)</span> <span class=\"c1\"># number of minibatches of size minibatch_size in the train set</span>\n            <span class=\"n\">seed</span> <span class=\"o\">=</span> <span class=\"n\">seed</span> <span class=\"o\">+</span> <span class=\"mi\">1</span>\n            <span class=\"n\">minibatches</span> <span class=\"o\">=</span> <span class=\"n\">random_mini_batches</span><span class=\"p\">(</span><span class=\"n\">X_train</span><span class=\"p\">,</span> <span class=\"n\">Y_train</span><span class=\"p\">,</span> <span class=\"n\">minibatch_size</span><span class=\"p\">,</span> <span class=\"n\">seed</span><span class=\"p\">)</span>\n<span class=\"err\">​</span>\n            <span class=\"k\">for</span> <span class=\"n\">minibatch</span> <span class=\"ow\">in</span> <span class=\"n\">minibatches</span><span class=\"p\">:</span>\n<span class=\"err\">​</span>\n                <span class=\"c1\"># Select a minibatch</span>\n                <span class=\"p\">(</span><span class=\"n\">minibatch_X</span><span class=\"p\">,</span> <span class=\"n\">minibatch_Y</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">minibatch</span>\n                \n                <span class=\"c1\"># IMPORTANT: The line that runs the graph on a minibatch.</span>\n                <span class=\"c1\"># Run the session to execute the &#34;optimizer&#34; and the &#34;cost&#34;, the feedict should contain a minibatch for (X,Y).</span>\n                <span class=\"c1\">### START CODE HERE ### (1 line)</span>\n                <span class=\"n\">_</span> <span class=\"p\">,</span> <span class=\"n\">minibatch_cost</span> <span class=\"o\">=</span> <span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">([</span><span class=\"n\">optimizer</span><span class=\"p\">,</span> <span class=\"n\">cost</span><span class=\"p\">],</span> <span class=\"n\">feed_dict</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"n\">X</span><span class=\"p\">:</span> <span class=\"n\">minibatch_X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">:</span> <span class=\"n\">minibatch_Y</span><span class=\"p\">})</span>\n                <span class=\"c1\">### END CODE HERE ###</span>\n                \n                <span class=\"n\">epoch_cost</span> <span class=\"o\">+=</span> <span class=\"n\">minibatch_cost</span> <span class=\"o\">/</span> <span class=\"n\">num_minibatches</span>\n<span class=\"err\">​</span>\n            <span class=\"c1\"># Print the cost every epoch</span>\n            <span class=\"k\">if</span> <span class=\"n\">print_cost</span> <span class=\"o\">==</span> <span class=\"bp\">True</span> <span class=\"ow\">and</span> <span class=\"n\">epoch</span> <span class=\"o\">%</span> <span class=\"mi\">100</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n                <span class=\"k\">print</span> <span class=\"p\">(</span><span class=\"s2\">&#34;Cost after epoch </span><span class=\"si\">%i</span><span class=\"s2\">: </span><span class=\"si\">%f</span><span class=\"s2\">&#34;</span> <span class=\"o\">%</span> <span class=\"p\">(</span><span class=\"n\">epoch</span><span class=\"p\">,</span> <span class=\"n\">epoch_cost</span><span class=\"p\">))</span>\n            <span class=\"k\">if</span> <span class=\"n\">print_cost</span> <span class=\"o\">==</span> <span class=\"bp\">True</span> <span class=\"ow\">and</span> <span class=\"n\">epoch</span> <span class=\"o\">%</span> <span class=\"mi\">5</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n                <span class=\"n\">costs</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">epoch_cost</span><span class=\"p\">)</span>\n                \n        <span class=\"c1\"># plot the cost</span>\n        <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">squeeze</span><span class=\"p\">(</span><span class=\"n\">costs</span><span class=\"p\">))</span>\n        <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">ylabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;cost&#39;</span><span class=\"p\">)</span>\n        <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">xlabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;iterations (per tens)&#39;</span><span class=\"p\">)</span>\n        <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">title</span><span class=\"p\">(</span><span class=\"s2\">&#34;Learning rate =&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">learning_rate</span><span class=\"p\">))</span>\n        <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>\n<span class=\"err\">​</span>\n        <span class=\"c1\"># lets save the parameters in a variable</span>\n        <span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"n\">sess</span><span class=\"o\">.</span><span class=\"n\">run</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">)</span>\n        <span class=\"k\">print</span> <span class=\"p\">(</span><span class=\"s2\">&#34;Parameters have been trained!&#34;</span><span class=\"p\">)</span>\n<span class=\"err\">​</span>\n        <span class=\"c1\"># Calculate the correct predictions</span>\n        <span class=\"n\">correct_prediction</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">equal</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">argmax</span><span class=\"p\">(</span><span class=\"n\">Z3</span><span class=\"p\">),</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">argmax</span><span class=\"p\">(</span><span class=\"n\">Y</span><span class=\"p\">))</span>\n<span class=\"err\">​</span>\n        <span class=\"c1\"># Calculate accuracy on the test set</span>\n        <span class=\"n\">accuracy</span> <span class=\"o\">=</span> <span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">reduce_mean</span><span class=\"p\">(</span><span class=\"n\">tf</span><span class=\"o\">.</span><span class=\"n\">cast</span><span class=\"p\">(</span><span class=\"n\">correct_prediction</span><span class=\"p\">,</span> <span class=\"s2\">&#34;float&#34;</span><span class=\"p\">))</span>\n<span class=\"err\">​</span>\n        <span class=\"k\">print</span> <span class=\"p\">(</span><span class=\"s2\">&#34;Train Accuracy:&#34;</span><span class=\"p\">,</span> <span class=\"n\">accuracy</span><span class=\"o\">.</span><span class=\"nb\">eval</span><span class=\"p\">({</span><span class=\"n\">X</span><span class=\"p\">:</span> <span class=\"n\">X_train</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">:</span> <span class=\"n\">Y_train</span><span class=\"p\">}))</span>\n        <span class=\"k\">print</span> <span class=\"p\">(</span><span class=\"s2\">&#34;Test Accuracy:&#34;</span><span class=\"p\">,</span> <span class=\"n\">accuracy</span><span class=\"o\">.</span><span class=\"nb\">eval</span><span class=\"p\">({</span><span class=\"n\">X</span><span class=\"p\">:</span> <span class=\"n\">X_test</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">:</span> <span class=\"n\">Y_test</span><span class=\"p\">}))</span>\n        \n        <span class=\"k\">return</span> <span class=\"n\">parameters</span></code></pre></div><p></p>", 
            "topic": [
                {
                    "tag": "神经网络", 
                    "tagLink": "https://api.zhihu.com/topics/19607065"
                }, 
                {
                    "tag": "软件调试", 
                    "tagLink": "https://api.zhihu.com/topics/19660363"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/45751849", 
            "userName": "直上云霄", 
            "userLink": "https://www.zhihu.com/people/1033165ce4ad9c3fce69a0793dfab8ad", 
            "upvote": 0, 
            "title": "DeepLearning.ai笔记:(2-3)-- 超参数调试", 
            "content": "<p><b>首发于个人博客:</b></p><a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/\" data-draft-node=\"block\" data-draft-type=\"link-card\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Fangzh的个人博客 | 人工智能拯救世界</a><p><b>欢迎来访</b></p><p>这周主要讲了这些超参数调试的方法以及batch norm，还有softmax多分类函数的使用。</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>调试处理</b></h2><p>之前提到的超参数有：</p><ul><li><img src=\"https://www.zhihu.com/equation?tex=%5Calpha\" alt=\"\\alpha\" eeimg=\"1\"/> </li><li>hidden units</li><li>minibatch size</li><li><img src=\"https://www.zhihu.com/equation?tex=%5Cbeta\" alt=\"\\beta\" eeimg=\"1\"/> (Momentum)</li><li>layers</li><li>learning rate decay</li><li><img src=\"https://www.zhihu.com/equation?tex=%5Cbeta_1%2C%5Cbeta_2%2C%5Cepsilon\" alt=\"\\beta_1,\\beta_2,\\epsilon\" eeimg=\"1\"/> </li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p>在调参中，常用的方式是在网格中取不同的点，然后计算这些点中的最佳值，</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-44e696aef2ec587049175b65e1e000d9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1095\" data-rawheight=\"430\" class=\"origin_image zh-lightbox-thumb\" width=\"1095\" data-original=\"https://pic2.zhimg.com/v2-44e696aef2ec587049175b65e1e000d9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1095&#39; height=&#39;430&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1095\" data-rawheight=\"430\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1095\" data-original=\"https://pic2.zhimg.com/v2-44e696aef2ec587049175b65e1e000d9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-44e696aef2ec587049175b65e1e000d9_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>但是左边是均匀的选点，这样有可能导致在某一个参数上变化很小，浪费计算时间，所以应该更推荐右边的选点方法，即随机选点。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>而后，当随机选点选到几个结果比较好的点时，逐步缩小范围，进行更精细的选取。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-89e4a7e88cbe89467062b718e356c1c5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"820\" data-rawheight=\"607\" class=\"origin_image zh-lightbox-thumb\" width=\"820\" data-original=\"https://pic2.zhimg.com/v2-89e4a7e88cbe89467062b718e356c1c5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;820&#39; height=&#39;607&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"820\" data-rawheight=\"607\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"820\" data-original=\"https://pic2.zhimg.com/v2-89e4a7e88cbe89467062b718e356c1c5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-89e4a7e88cbe89467062b718e356c1c5_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>超参数的合适范围</b></h2><p>当然，随机采样并不是在轴上均匀的采样。</p><p>比如说 <img src=\"https://www.zhihu.com/equation?tex=%5Calpha+%3D+0.001++-1\" alt=\"\\alpha = 0.001  -1\" eeimg=\"1\"/> ，这样子，那么在 <img src=\"https://www.zhihu.com/equation?tex=0.1-1\" alt=\"0.1-1\" eeimg=\"1\"/> 的部分占了90%的概率，显然是不合理的，所以应该将区间对数化，转化成 <img src=\"https://www.zhihu.com/equation?tex=%5B0.001%2C0.01%5D%2C%5B0.01%2C0.1%5D%2C%5B0.1%2C1%5D\" alt=\"[0.001,0.01],[0.01,0.1],[0.1,1]\" eeimg=\"1\"/> 的区间，这样更为合理。思路是： <img src=\"https://www.zhihu.com/equation?tex=10%5E%7B-3%7D+%3D+0.001\" alt=\"10^{-3} = 0.001\" eeimg=\"1\"/> ，所以取值从 <img src=\"https://www.zhihu.com/equation?tex=%5B10%5E%7B-3%7D%2C10%5E%7B0%7D%5D\" alt=\"[10^{-3},10^{0}]\" eeimg=\"1\"/> ，我们只要将指数随机就可以了。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">r</span> <span class=\"o\">=</span> <span class=\"o\">-</span><span class=\"mi\">3</span><span class=\"o\">*</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">()</span> <span class=\"c1\"># rand()表示在 [0，1]随机取样，再乘以系数，就可以得到[-3,0]</span>\n<span class=\"n\">a</span> <span class=\"o\">=</span> <span class=\"mi\">10</span><span class=\"o\">**</span><span class=\"n\">r</span></code></pre></div><p>同理, <img src=\"https://www.zhihu.com/equation?tex=%5Cbeta+%3D+0.9+%2C.....%2C0.999\" alt=\"\\beta = 0.9 ,.....,0.999\" eeimg=\"1\"/> </p><p>通过 <img src=\"https://www.zhihu.com/equation?tex=1-%5Cbeta+%3D+0.1%2C....%2C0.001\" alt=\"1-\\beta = 0.1,....,0.001\" eeimg=\"1\"/> ，所以 <img src=\"https://www.zhihu.com/equation?tex=1-%5Cbeta+%3D+10%5E%7Br%7D%EF%BC%8C%5Cbeta+%3D+1-10%5E%7Br%7D\" alt=\"1-\\beta = 10^{r}，\\beta = 1-10^{r}\" eeimg=\"1\"/> </p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>归一化网络的激活函数</b></h2><p>我们之前是将输入的数据X归一化，可以加速训练，其实在神经网络中，也可以同样归一化，一般是对 <img src=\"https://www.zhihu.com/equation?tex=z%5E%7B%5Bl%5D%7D\" alt=\"z^{[l]}\" eeimg=\"1\"/> 归一化。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>这个方法叫做 batch norm</p><p>公式是：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-6ad8dc32758a53e5f147e62299553573_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"268\" data-rawheight=\"166\" class=\"content_image\" width=\"268\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;268&#39; height=&#39;166&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"268\" data-rawheight=\"166\" class=\"content_image lazy\" width=\"268\" data-actualsrc=\"https://pic4.zhimg.com/v2-6ad8dc32758a53e5f147e62299553573_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>加上 <img src=\"https://www.zhihu.com/equation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"/> 是为了不至于除以0</p><p class=\"ztext-empty-paragraph\"><br/></p><p>而一般标准化后还会加上两个参数，来表示新的方差 <img src=\"https://www.zhihu.com/equation?tex=%5Cgamma\" alt=\"\\gamma\" eeimg=\"1\"/> 和均值 <img src=\"https://www.zhihu.com/equation?tex=%5Cbeta\" alt=\"\\beta\" eeimg=\"1\"/> ：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-dea9fddb9cdab6406658af5376005434_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"288\" data-rawheight=\"83\" class=\"content_image\" width=\"288\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;288&#39; height=&#39;83&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"288\" data-rawheight=\"83\" class=\"content_image lazy\" width=\"288\" data-actualsrc=\"https://pic1.zhimg.com/v2-dea9fddb9cdab6406658af5376005434_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cgamma\" alt=\"\\gamma\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5Cbeta\" alt=\"\\beta\" eeimg=\"1\"/> 也是参数，和 <img src=\"https://www.zhihu.com/equation?tex=w%2Cb\" alt=\"w,b\" eeimg=\"1\"/> 一样，可以在学习中进行更新。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>将batch norm 放入神经网络</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-5acf763fd0fc6ad3dba36bfcadcd5f01_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"709\" data-rawheight=\"390\" class=\"origin_image zh-lightbox-thumb\" width=\"709\" data-original=\"https://pic2.zhimg.com/v2-5acf763fd0fc6ad3dba36bfcadcd5f01_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;709&#39; height=&#39;390&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"709\" data-rawheight=\"390\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"709\" data-original=\"https://pic2.zhimg.com/v2-5acf763fd0fc6ad3dba36bfcadcd5f01_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-5acf763fd0fc6ad3dba36bfcadcd5f01_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>可以看到，</p><p>先求的 <img src=\"https://www.zhihu.com/equation?tex=z%5E%7B%5B1%5D%7D\" alt=\"z^{[1]}\" eeimg=\"1\"/> ，再进行batch norm，加上参数 <img src=\"https://www.zhihu.com/equation?tex=%5Cbeta%5E%7B%5B1%5D%7D%2C%5Cgamma%5E%7B%5B1%5D%7D\" alt=\"\\beta^{[1]},\\gamma^{[1]}\" eeimg=\"1\"/> ，得到 <img src=\"https://www.zhihu.com/equation?tex=%7B%5Ctilde%7Bz%7D%7D%5E%7B%5B1%5D%7D\" alt=\"{\\tilde{z}}^{[1]}\" eeimg=\"1\"/> ,再根据activation function得到 <img src=\"https://www.zhihu.com/equation?tex=a%5E%7B%5B1%5D%7D\" alt=\"a^{[1]}\" eeimg=\"1\"/> </p><p>batch norm同样适用于Momentum、RMSprop 、Adam的梯度下降法来进行更新。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>Batch Norm为什么有用？</b></h2><p>如果我们的图片中训练的都是黑猫，这个时候给你一些橘猫的图片，那么大概率是训练不好的。因为相当于样本集合的分布改变了，batch norm就可以解决这个问题。</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-d01efe746cae6c55b85da5888940d3d1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1186\" data-rawheight=\"388\" class=\"origin_image zh-lightbox-thumb\" width=\"1186\" data-original=\"https://pic2.zhimg.com/v2-d01efe746cae6c55b85da5888940d3d1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1186&#39; height=&#39;388&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1186\" data-rawheight=\"388\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1186\" data-original=\"https://pic2.zhimg.com/v2-d01efe746cae6c55b85da5888940d3d1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-d01efe746cae6c55b85da5888940d3d1_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>如果这个时候要计算第三层，那么很显然计算结果是依赖第二层的数据的。但是如果我们对第二层的数据进行了归一化，那么就可以将第二层的均值和方差都限制在同一分布，而且这两个参数是自动学习的。也就是归一化后的数据可以减弱前层参数的作用与后层参数的作用之间的联系，它使得网络每层都可以自己学习。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>还有就是batch norm在某种程度上有正则化的效果，因为归一化会使各个层之间的依赖性降低，而且归一化有带来一定的噪声，有点像dropout。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>测试集的batch norm</b></h2><p>batch norm是在训练集上得到的，那么怎么把它应用在测试集呢？</p><p>这个时候可以直接从训练集中拿到 <img src=\"https://www.zhihu.com/equation?tex=%5Cmu\" alt=\"\\mu\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5Csigma%5E%7B2%7D\" alt=\"\\sigma^{2}\" eeimg=\"1\"/> </p><p>使用指数加权平均，在每一步中保留 <img src=\"https://www.zhihu.com/equation?tex=%5Cmu\" alt=\"\\mu\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5Csigma%5E%7B2%7D\" alt=\"\\sigma^{2}\" eeimg=\"1\"/> ，就可以得到训练后的 <img src=\"https://www.zhihu.com/equation?tex=%5Cmu\" alt=\"\\mu\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5Csigma%5E%7B2%7D\" alt=\"\\sigma^{2}\" eeimg=\"1\"/> </p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>softmax</b></h2><p>之前说的都是二分类问题，如何解决多分类问题呢？</p><p>可以用softmax算法来解决。</p><p>前面的步骤都一样，而到了最后一层output layer，你想要分为多少类，就用多少个神经元。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>这个时候，最后一层的activation function就变成了：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-effa1cd1e4eac864b040ec40338ff06e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"409\" data-rawheight=\"160\" class=\"content_image\" width=\"409\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;409&#39; height=&#39;160&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"409\" data-rawheight=\"160\" class=\"content_image lazy\" width=\"409\" data-actualsrc=\"https://pic3.zhimg.com/v2-effa1cd1e4eac864b040ec40338ff06e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><img src=\"https://www.zhihu.com/equation?tex=a%5E%7B%5Bl%5D%7D_i\" alt=\"a^{[l]}_i\" eeimg=\"1\"/> 就表示了每一个分类的概率。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>计算例子如图：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-8e024a11adff99f08dd36dc43a354a2e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"491\" data-rawheight=\"280\" class=\"origin_image zh-lightbox-thumb\" width=\"491\" data-original=\"https://pic3.zhimg.com/v2-8e024a11adff99f08dd36dc43a354a2e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;491&#39; height=&#39;280&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"491\" data-rawheight=\"280\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"491\" data-original=\"https://pic3.zhimg.com/v2-8e024a11adff99f08dd36dc43a354a2e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-8e024a11adff99f08dd36dc43a354a2e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>而它的损失函数用的也是cross-entropy：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-4c2e090c513660dbe9d9bad1cbe94c70_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"801\" data-rawheight=\"273\" class=\"origin_image zh-lightbox-thumb\" width=\"801\" data-original=\"https://pic1.zhimg.com/v2-4c2e090c513660dbe9d9bad1cbe94c70_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;801&#39; height=&#39;273&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"801\" data-rawheight=\"273\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"801\" data-original=\"https://pic1.zhimg.com/v2-4c2e090c513660dbe9d9bad1cbe94c70_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-4c2e090c513660dbe9d9bad1cbe94c70_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>最终得到一个关于Y的矩阵：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-8ddc6dc74a27ea3b521965674eed0968_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"652\" data-rawheight=\"165\" class=\"origin_image zh-lightbox-thumb\" width=\"652\" data-original=\"https://pic1.zhimg.com/v2-8ddc6dc74a27ea3b521965674eed0968_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;652&#39; height=&#39;165&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"652\" data-rawheight=\"165\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"652\" data-original=\"https://pic1.zhimg.com/v2-8ddc6dc74a27ea3b521965674eed0968_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-8ddc6dc74a27ea3b521965674eed0968_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>其实是可以证明，当分类为2时，softmax就是logistic regression</p><p></p><p></p><p></p><p></p><p></p><p></p>", 
            "topic": [
                {
                    "tag": "笔记", 
                    "tagLink": "https://api.zhihu.com/topics/19554982"
                }, 
                {
                    "tag": "神经网络", 
                    "tagLink": "https://api.zhihu.com/topics/19607065"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/44718731", 
            "userName": "直上云霄", 
            "userLink": "https://www.zhihu.com/people/1033165ce4ad9c3fce69a0793dfab8ad", 
            "upvote": 0, 
            "title": "DeepLearning.ai作业:(2-2)-优化算法", 
            "content": "<h2>首发于个人博客：<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">fangzh.top</span><span class=\"invisible\"></span></a>，欢迎来访</h2><ol><li>不要抄作业！</li><li>我只是把思路整理了，供个人学习。</li><li>不要抄作业！</li></ol><p>本周作业实践了课上的各种优化算法：</p><ul><li>mini-batch</li><li>momentum</li><li>Adam</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p>首先是标准的gradient descent：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">update_parameters_with_gd</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">grads</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Update parameters using one step of gradient descent\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    parameters -- python dictionary containing your parameters to be updated:\n</span><span class=\"s2\">                    parameters[&#39;W&#39; + str(l)] = Wl\n</span><span class=\"s2\">                    parameters[&#39;b&#39; + str(l)] = bl\n</span><span class=\"s2\">    grads -- python dictionary containing your gradients to update each parameters:\n</span><span class=\"s2\">                    grads[&#39;dW&#39; + str(l)] = dWl\n</span><span class=\"s2\">                    grads[&#39;db&#39; + str(l)] = dbl\n</span><span class=\"s2\">    learning_rate -- the learning rate, scalar.\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    parameters -- python dictionary containing your updated parameters \n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n<span class=\"err\">​</span>\n    <span class=\"n\">L</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">)</span> <span class=\"o\">//</span> <span class=\"mi\">2</span> <span class=\"c1\"># number of layers in the neural networks</span>\n<span class=\"err\">​</span>\n    <span class=\"c1\"># Update rule for each parameter</span>\n    <span class=\"k\">for</span> <span class=\"n\">l</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">L</span><span class=\"p\">):</span>\n        <span class=\"c1\">### START CODE HERE ### (approx. 2 lines)</span>\n        <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;W&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span>  <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;W&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">-</span> <span class=\"n\">learning_rate</span> <span class=\"o\">*</span> <span class=\"n\">grads</span><span class=\"p\">[</span><span class=\"s1\">&#39;dW&#39;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span>\n        <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;b&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;b&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">-</span> <span class=\"n\">learning_rate</span> <span class=\"o\">*</span> <span class=\"n\">grads</span><span class=\"p\">[</span><span class=\"s1\">&#39;db&#39;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n        \n    <span class=\"k\">return</span> <span class=\"n\">parameters</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>mini-batch</b></h2><p>步骤是：</p><ul><li>shuffle：将数据随机打乱，使用<code>np.random.permutation(m)</code>函数可以把m个样本的顺序重新映射，变成一个len为m的列表，里面的值就是映射原本的顺序。</li><li>再根据size大小进行分区，需要注意的是最后的数据有可能小于size大小的，因为可能无法整除，要单独考虑</li></ul><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: random_mini_batches</span>\n<span class=\"err\">​</span>\n<span class=\"k\">def</span> <span class=\"nf\">random_mini_batches</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">mini_batch_size</span> <span class=\"o\">=</span> <span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"n\">seed</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Creates a list of random minibatches from (X, Y)\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    X -- input data, of shape (input size, number of examples)\n</span><span class=\"s2\">    Y -- true &#34;label&#34; vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n</span><span class=\"s2\">    mini_batch_size -- size of the mini-batches, integer\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">seed</span><span class=\"p\">(</span><span class=\"n\">seed</span><span class=\"p\">)</span>            <span class=\"c1\"># To make your &#34;random&#34; minibatches the same as ours</span>\n    <span class=\"n\">m</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span>                  <span class=\"c1\"># number of training examples</span>\n    <span class=\"n\">mini_batches</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n        \n    <span class=\"c1\"># Step 1: Shuffle (X, Y)</span>\n    <span class=\"n\">permutation</span> <span class=\"o\">=</span> <span class=\"nb\">list</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">permutation</span><span class=\"p\">(</span><span class=\"n\">m</span><span class=\"p\">))</span>\n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">permutation</span><span class=\"p\">)</span>\n    <span class=\"n\">shuffled_X</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"p\">[:,</span> <span class=\"n\">permutation</span><span class=\"p\">]</span>\n    <span class=\"n\">shuffled_Y</span> <span class=\"o\">=</span> <span class=\"n\">Y</span><span class=\"p\">[:,</span> <span class=\"n\">permutation</span><span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">((</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"n\">m</span><span class=\"p\">))</span>\n<span class=\"err\">​</span>\n    <span class=\"c1\"># Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.</span>\n    <span class=\"n\">num_complete_minibatches</span> <span class=\"o\">=</span> <span class=\"n\">math</span><span class=\"o\">.</span><span class=\"n\">floor</span><span class=\"p\">(</span><span class=\"n\">m</span><span class=\"o\">/</span><span class=\"n\">mini_batch_size</span><span class=\"p\">)</span> <span class=\"c1\"># number of mini batches of size mini_batch_size in your partitionning</span>\n    <span class=\"k\">for</span> <span class=\"n\">k</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">num_complete_minibatches</span><span class=\"p\">):</span>\n        <span class=\"c1\">### START CODE HERE ### (approx. 2 lines)</span>\n        <span class=\"n\">mini_batch_X</span> <span class=\"o\">=</span> <span class=\"n\">shuffled_X</span><span class=\"p\">[:,</span><span class=\"n\">k</span> <span class=\"o\">*</span> <span class=\"n\">mini_batch_size</span><span class=\"p\">:(</span><span class=\"n\">k</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)</span><span class=\"o\">*</span> <span class=\"n\">mini_batch_size</span><span class=\"p\">]</span>\n        <span class=\"n\">mini_batch_Y</span> <span class=\"o\">=</span> <span class=\"n\">shuffled_Y</span><span class=\"p\">[:,</span><span class=\"n\">k</span> <span class=\"o\">*</span> <span class=\"n\">mini_batch_size</span><span class=\"p\">:(</span><span class=\"n\">k</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)</span><span class=\"o\">*</span> <span class=\"n\">mini_batch_size</span><span class=\"p\">]</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n        <span class=\"n\">mini_batch</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">mini_batch_X</span><span class=\"p\">,</span> <span class=\"n\">mini_batch_Y</span><span class=\"p\">)</span>\n        <span class=\"n\">mini_batches</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">mini_batch</span><span class=\"p\">)</span>\n    \n    <span class=\"c1\"># Handling the end case (last mini-batch &lt; mini_batch_size)</span>\n    <span class=\"k\">if</span> <span class=\"n\">m</span> <span class=\"o\">%</span> <span class=\"n\">mini_batch_size</span> <span class=\"o\">!=</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n        <span class=\"c1\">### START CODE HERE ### (approx. 2 lines)</span>\n        <span class=\"n\">mini_batch_X</span> <span class=\"o\">=</span> <span class=\"n\">shuffled_X</span><span class=\"p\">[:,</span><span class=\"n\">num_complete_minibatches</span> <span class=\"o\">*</span> <span class=\"n\">mini_batch_size</span><span class=\"p\">:]</span>\n        <span class=\"n\">mini_batch_Y</span> <span class=\"o\">=</span> <span class=\"n\">shuffled_Y</span><span class=\"p\">[:,</span><span class=\"n\">num_complete_minibatches</span> <span class=\"o\">*</span> <span class=\"n\">mini_batch_size</span><span class=\"p\">:]</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n        <span class=\"n\">mini_batch</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">mini_batch_X</span><span class=\"p\">,</span> <span class=\"n\">mini_batch_Y</span><span class=\"p\">)</span>\n        <span class=\"n\">mini_batches</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">mini_batch</span><span class=\"p\">)</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">mini_batches</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>Momentum</b></h2><p>先初始化为0，</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: initialize_velocity</span>\n<span class=\"err\">​</span>\n<span class=\"k\">def</span> <span class=\"nf\">initialize_velocity</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Initializes the velocity as a python dictionary with:\n</span><span class=\"s2\">                - keys: &#34;dW1&#34;, &#34;db1&#34;, ..., &#34;dWL&#34;, &#34;dbL&#34; \n</span><span class=\"s2\">                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    parameters -- python dictionary containing your parameters.\n</span><span class=\"s2\">                    parameters[&#39;W&#39; + str(l)] = Wl\n</span><span class=\"s2\">                    parameters[&#39;b&#39; + str(l)] = bl\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    v -- python dictionary containing the current velocity.\n</span><span class=\"s2\">                    v[&#39;dW&#39; + str(l)] = velocity of dWl\n</span><span class=\"s2\">                    v[&#39;db&#39; + str(l)] = velocity of dbl\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"n\">L</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">)</span> <span class=\"o\">//</span> <span class=\"mi\">2</span> <span class=\"c1\"># number of layers in the neural networks</span>\n    <span class=\"n\">v</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>\n    \n    <span class=\"c1\"># Initialize velocity</span>\n    <span class=\"k\">for</span> <span class=\"n\">l</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">L</span><span class=\"p\">):</span>\n        <span class=\"c1\">### START CODE HERE ### (approx. 2 lines)</span>\n        <span class=\"n\">v</span><span class=\"p\">[</span><span class=\"s2\">&#34;dW&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;W&#39;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span><span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;W&#39;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]))</span>\n        <span class=\"n\">v</span><span class=\"p\">[</span><span class=\"s2\">&#34;db&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;b&#39;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span><span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;b&#39;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"p\">]</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]))</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n        \n    <span class=\"k\">return</span> <span class=\"n\">v</span></code></pre></div><p>再按公式进行迭代，因为指数加权平均不需要知道前面n个数据，只要一步一步进行迭代，知道当前的数据就行，节省空间。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: update_parameters_with_momentum</span>\n<span class=\"err\">​</span>\n<span class=\"k\">def</span> <span class=\"nf\">update_parameters_with_momentum</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">grads</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">,</span> <span class=\"n\">beta</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Update parameters using Momentum\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    parameters -- python dictionary containing your parameters:\n</span><span class=\"s2\">                    parameters[&#39;W&#39; + str(l)] = Wl\n</span><span class=\"s2\">                    parameters[&#39;b&#39; + str(l)] = bl\n</span><span class=\"s2\">    grads -- python dictionary containing your gradients for each parameters:\n</span><span class=\"s2\">                    grads[&#39;dW&#39; + str(l)] = dWl\n</span><span class=\"s2\">                    grads[&#39;db&#39; + str(l)] = dbl\n</span><span class=\"s2\">    v -- python dictionary containing the current velocity:\n</span><span class=\"s2\">                    v[&#39;dW&#39; + str(l)] = ...\n</span><span class=\"s2\">                    v[&#39;db&#39; + str(l)] = ...\n</span><span class=\"s2\">    beta -- the momentum hyperparameter, scalar\n</span><span class=\"s2\">    learning_rate -- the learning rate, scalar\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    parameters -- python dictionary containing your updated parameters \n</span><span class=\"s2\">    v -- python dictionary containing your updated velocities\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n<span class=\"err\">​</span>\n    <span class=\"n\">L</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">)</span> <span class=\"o\">//</span> <span class=\"mi\">2</span> <span class=\"c1\"># number of layers in the neural networks</span>\n    \n    <span class=\"c1\"># Momentum update for each parameter</span>\n    <span class=\"k\">for</span> <span class=\"n\">l</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">L</span><span class=\"p\">):</span>\n        \n        <span class=\"c1\">### START CODE HERE ### (approx. 4 lines)</span>\n        <span class=\"c1\"># compute velocities</span>\n        <span class=\"n\">v</span><span class=\"p\">[</span><span class=\"s2\">&#34;dW&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">beta</span> <span class=\"o\">*</span> <span class=\"n\">v</span><span class=\"p\">[</span><span class=\"s2\">&#34;dW&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"mi\">1</span> <span class=\"o\">-</span> <span class=\"n\">beta</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">grads</span><span class=\"p\">[</span><span class=\"s2\">&#34;dW&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span>\n        <span class=\"n\">v</span><span class=\"p\">[</span><span class=\"s2\">&#34;db&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">beta</span> <span class=\"o\">*</span> <span class=\"n\">v</span><span class=\"p\">[</span><span class=\"s2\">&#34;db&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"mi\">1</span> <span class=\"o\">-</span> <span class=\"n\">beta</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">grads</span><span class=\"p\">[</span><span class=\"s2\">&#34;db&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span>\n        <span class=\"c1\"># update parameters</span>\n        <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;W&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;W&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">-</span> <span class=\"n\">learning_rate</span> <span class=\"o\">*</span> <span class=\"n\">v</span><span class=\"p\">[</span><span class=\"s2\">&#34;dW&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span>\n        <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;b&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;b&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">-</span> <span class=\"n\">learning_rate</span> <span class=\"o\">*</span> <span class=\"n\">v</span><span class=\"p\">[</span><span class=\"s2\">&#34;dW&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n        \n    <span class=\"k\">return</span> <span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">v</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>Adam</b></h2><p>没什么好说的，先初始化，根据公式来就行了。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">initialize_adam</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">)</span> <span class=\"p\">:</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Initializes v and s as two python dictionaries with:\n</span><span class=\"s2\">                - keys: &#34;dW1&#34;, &#34;db1&#34;, ..., &#34;dWL&#34;, &#34;dbL&#34; \n</span><span class=\"s2\">                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    parameters -- python dictionary containing your parameters.\n</span><span class=\"s2\">                    parameters[&#34;W&#34; + str(l)] = Wl\n</span><span class=\"s2\">                    parameters[&#34;b&#34; + str(l)] = bl\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns: \n</span><span class=\"s2\">    v -- python dictionary that will contain the exponentially weighted average of the gradient.\n</span><span class=\"s2\">                    v[&#34;dW&#34; + str(l)] = ...\n</span><span class=\"s2\">                    v[&#34;db&#34; + str(l)] = ...\n</span><span class=\"s2\">    s -- python dictionary that will contain the exponentially weighted average of the squared gradient.\n</span><span class=\"s2\">                    s[&#34;dW&#34; + str(l)] = ...\n</span><span class=\"s2\">                    s[&#34;db&#34; + str(l)] = ...\n</span><span class=\"s2\">​\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"n\">L</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">)</span> <span class=\"o\">//</span> <span class=\"mi\">2</span> <span class=\"c1\"># number of layers in the neural networks</span>\n    <span class=\"n\">v</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>\n    <span class=\"n\">s</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>\n    \n    <span class=\"c1\"># Initialize v, s. Input: &#34;parameters&#34;. Outputs: &#34;v, s&#34;.</span>\n    <span class=\"k\">for</span> <span class=\"n\">l</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">L</span><span class=\"p\">):</span>\n    <span class=\"c1\">### START CODE HERE ### (approx. 4 lines)</span>\n        <span class=\"n\">v</span><span class=\"p\">[</span><span class=\"s2\">&#34;dW&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;W&#39;</span><span class=\"o\">+</span><span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span><span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;W&#39;</span><span class=\"o\">+</span><span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]))</span>\n        <span class=\"n\">v</span><span class=\"p\">[</span><span class=\"s2\">&#34;db&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;b&#39;</span><span class=\"o\">+</span><span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span><span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;b&#39;</span><span class=\"o\">+</span><span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]))</span>\n        <span class=\"n\">s</span><span class=\"p\">[</span><span class=\"s2\">&#34;dW&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;W&#39;</span><span class=\"o\">+</span><span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span><span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;W&#39;</span><span class=\"o\">+</span><span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]))</span>\n        <span class=\"n\">s</span><span class=\"p\">[</span><span class=\"s2\">&#34;db&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;b&#39;</span><span class=\"o\">+</span><span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span><span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;b&#39;</span><span class=\"o\">+</span><span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]))</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">v</span><span class=\"p\">,</span> <span class=\"n\">s</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">update_parameters_with_adam</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">grads</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">,</span> <span class=\"n\">s</span><span class=\"p\">,</span> <span class=\"n\">t</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span> <span class=\"o\">=</span> <span class=\"mf\">0.01</span><span class=\"p\">,</span>\n                                <span class=\"n\">beta1</span> <span class=\"o\">=</span> <span class=\"mf\">0.9</span><span class=\"p\">,</span> <span class=\"n\">beta2</span> <span class=\"o\">=</span> <span class=\"mf\">0.999</span><span class=\"p\">,</span>  <span class=\"n\">epsilon</span> <span class=\"o\">=</span> <span class=\"mf\">1e-8</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Update parameters using Adam\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    parameters -- python dictionary containing your parameters:\n</span><span class=\"s2\">                    parameters[&#39;W&#39; + str(l)] = Wl\n</span><span class=\"s2\">                    parameters[&#39;b&#39; + str(l)] = bl\n</span><span class=\"s2\">    grads -- python dictionary containing your gradients for each parameters:\n</span><span class=\"s2\">                    grads[&#39;dW&#39; + str(l)] = dWl\n</span><span class=\"s2\">                    grads[&#39;db&#39; + str(l)] = dbl\n</span><span class=\"s2\">    v -- Adam variable, moving average of the first gradient, python dictionary\n</span><span class=\"s2\">    s -- Adam variable, moving average of the squared gradient, python dictionary\n</span><span class=\"s2\">    learning_rate -- the learning rate, scalar.\n</span><span class=\"s2\">    beta1 -- Exponential decay hyperparameter for the first moment estimates \n</span><span class=\"s2\">    beta2 -- Exponential decay hyperparameter for the second moment estimates \n</span><span class=\"s2\">    epsilon -- hyperparameter preventing division by zero in Adam updates\n</span><span class=\"s2\">​\n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    parameters -- python dictionary containing your updated parameters \n</span><span class=\"s2\">    v -- Adam variable, moving average of the first gradient, python dictionary\n</span><span class=\"s2\">    s -- Adam variable, moving average of the squared gradient, python dictionary\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"n\">L</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">)</span> <span class=\"o\">//</span> <span class=\"mi\">2</span>                 <span class=\"c1\"># number of layers in the neural networks</span>\n    <span class=\"n\">v_corrected</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>                         <span class=\"c1\"># Initializing first moment estimate, python dictionary</span>\n    <span class=\"n\">s_corrected</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>                         <span class=\"c1\"># Initializing second moment estimate, python dictionary</span>\n    \n    <span class=\"c1\"># Perform Adam update on all parameters</span>\n    <span class=\"k\">for</span> <span class=\"n\">l</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">L</span><span class=\"p\">):</span>\n        <span class=\"c1\"># Moving average of the gradients. Inputs: &#34;v, grads, beta1&#34;. Output: &#34;v&#34;.</span>\n        <span class=\"c1\">### START CODE HERE ### (approx. 2 lines)</span>\n        <span class=\"n\">v</span><span class=\"p\">[</span><span class=\"s2\">&#34;dW&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">beta1</span> <span class=\"o\">*</span> <span class=\"n\">v</span><span class=\"p\">[</span><span class=\"s2\">&#34;dW&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"o\">-</span><span class=\"n\">beta1</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">grads</span><span class=\"p\">[</span><span class=\"s1\">&#39;dW&#39;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span>\n        <span class=\"n\">v</span><span class=\"p\">[</span><span class=\"s2\">&#34;db&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">beta1</span> <span class=\"o\">*</span> <span class=\"n\">v</span><span class=\"p\">[</span><span class=\"s2\">&#34;db&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"o\">-</span><span class=\"n\">beta1</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">grads</span><span class=\"p\">[</span><span class=\"s1\">&#39;db&#39;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n<span class=\"err\">​</span>\n        <span class=\"c1\"># Compute bias-corrected first moment estimate. Inputs: &#34;v, beta1, t&#34;. Output: &#34;v_corrected&#34;.</span>\n        <span class=\"c1\">### START CODE HERE ### (approx. 2 lines)</span>\n        <span class=\"n\">v_corrected</span><span class=\"p\">[</span><span class=\"s2\">&#34;dW&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">v</span><span class=\"p\">[</span><span class=\"s2\">&#34;dW&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"mi\">1</span> <span class=\"o\">-</span> <span class=\"n\">beta1</span> <span class=\"o\">**</span> <span class=\"n\">t</span><span class=\"p\">)</span>\n        <span class=\"n\">v_corrected</span><span class=\"p\">[</span><span class=\"s2\">&#34;db&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">v</span><span class=\"p\">[</span><span class=\"s2\">&#34;db&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"mi\">1</span> <span class=\"o\">-</span> <span class=\"n\">beta1</span> <span class=\"o\">**</span> <span class=\"n\">t</span><span class=\"p\">)</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n<span class=\"err\">​</span>\n        <span class=\"c1\"># Moving average of the squared gradients. Inputs: &#34;s, grads, beta2&#34;. Output: &#34;s&#34;.</span>\n        <span class=\"c1\">### START CODE HERE ### (approx. 2 lines)</span>\n        <span class=\"n\">s</span><span class=\"p\">[</span><span class=\"s2\">&#34;dW&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">beta2</span> <span class=\"o\">*</span> <span class=\"n\">s</span><span class=\"p\">[</span><span class=\"s2\">&#34;dW&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"o\">-</span><span class=\"n\">beta2</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"n\">grads</span><span class=\"p\">[</span><span class=\"s1\">&#39;dW&#39;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span><span class=\"o\">**</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n        <span class=\"n\">s</span><span class=\"p\">[</span><span class=\"s2\">&#34;db&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">beta2</span> <span class=\"o\">*</span> <span class=\"n\">s</span><span class=\"p\">[</span><span class=\"s2\">&#34;db&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"o\">-</span><span class=\"n\">beta2</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"n\">grads</span><span class=\"p\">[</span><span class=\"s1\">&#39;db&#39;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span><span class=\"o\">**</span><span class=\"mi\">2</span><span class=\"p\">)</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n<span class=\"err\">​</span>\n        <span class=\"c1\"># Compute bias-corrected second raw moment estimate. Inputs: &#34;s, beta2, t&#34;. Output: &#34;s_corrected&#34;.</span>\n        <span class=\"c1\">### START CODE HERE ### (approx. 2 lines)</span>\n        <span class=\"n\">s_corrected</span><span class=\"p\">[</span><span class=\"s2\">&#34;dW&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">s</span><span class=\"p\">[</span><span class=\"s2\">&#34;dW&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"mi\">1</span> <span class=\"o\">-</span> <span class=\"n\">beta2</span> <span class=\"o\">**</span> <span class=\"n\">t</span><span class=\"p\">)</span>\n        <span class=\"n\">s_corrected</span><span class=\"p\">[</span><span class=\"s2\">&#34;db&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">s</span><span class=\"p\">[</span><span class=\"s2\">&#34;db&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"mi\">1</span> <span class=\"o\">-</span> <span class=\"n\">beta2</span> <span class=\"o\">**</span> <span class=\"n\">t</span><span class=\"p\">)</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n<span class=\"err\">​</span>\n        <span class=\"c1\"># Update parameters. Inputs: &#34;parameters, learning_rate, v_corrected, s_corrected, epsilon&#34;. Output: &#34;parameters&#34;.</span>\n        <span class=\"c1\">### START CODE HERE ### (approx. 2 lines)</span>\n        <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;W&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;W&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">-</span> <span class=\"n\">learning_rate</span> <span class=\"o\">*</span> <span class=\"n\">v_corrected</span><span class=\"p\">[</span><span class=\"s2\">&#34;dW&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"n\">s_corrected</span><span class=\"p\">[</span><span class=\"s2\">&#34;dW&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span><span class=\"o\">**</span><span class=\"mf\">0.5</span> <span class=\"o\">+</span> <span class=\"n\">epsilon</span><span class=\"p\">)</span>\n        <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;b&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;W&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">-</span> <span class=\"n\">learning_rate</span> <span class=\"o\">*</span> <span class=\"n\">v_corrected</span><span class=\"p\">[</span><span class=\"s2\">&#34;db&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"n\">s_corrected</span><span class=\"p\">[</span><span class=\"s2\">&#34;db&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span><span class=\"o\">**</span><span class=\"mf\">0.5</span> <span class=\"o\">+</span> <span class=\"n\">epsilon</span><span class=\"p\">)</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n<span class=\"err\">​</span>\n    <span class=\"k\">return</span> <span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">,</span> <span class=\"n\">s</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>最后代入模型函数，根据关键字选择需要的优化算法就行了。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">model</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">layers_dims</span><span class=\"p\">,</span> <span class=\"n\">optimizer</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span> <span class=\"o\">=</span> <span class=\"mf\">0.0007</span><span class=\"p\">,</span> <span class=\"n\">mini_batch_size</span> <span class=\"o\">=</span> <span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"n\">beta</span> <span class=\"o\">=</span> <span class=\"mf\">0.9</span><span class=\"p\">,</span>\n          <span class=\"n\">beta1</span> <span class=\"o\">=</span> <span class=\"mf\">0.9</span><span class=\"p\">,</span> <span class=\"n\">beta2</span> <span class=\"o\">=</span> <span class=\"mf\">0.999</span><span class=\"p\">,</span>  <span class=\"n\">epsilon</span> <span class=\"o\">=</span> <span class=\"mf\">1e-8</span><span class=\"p\">,</span> <span class=\"n\">num_epochs</span> <span class=\"o\">=</span> <span class=\"mi\">10000</span><span class=\"p\">,</span> <span class=\"n\">print_cost</span> <span class=\"o\">=</span> <span class=\"bp\">True</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    3-layer neural network model which can be run in different optimizer modes.\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    X -- input data, of shape (2, number of examples)\n</span><span class=\"s2\">    Y -- true &#34;label&#34; vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n</span><span class=\"s2\">    layers_dims -- python list, containing the size of each layer\n</span><span class=\"s2\">    learning_rate -- the learning rate, scalar.\n</span><span class=\"s2\">    mini_batch_size -- the size of a mini batch\n</span><span class=\"s2\">    beta -- Momentum hyperparameter\n</span><span class=\"s2\">    beta1 -- Exponential decay hyperparameter for the past gradients estimates \n</span><span class=\"s2\">    beta2 -- Exponential decay hyperparameter for the past squared gradients estimates \n</span><span class=\"s2\">    epsilon -- hyperparameter preventing division by zero in Adam updates\n</span><span class=\"s2\">    num_epochs -- number of epochs\n</span><span class=\"s2\">    print_cost -- True to print the cost every 1000 epochs\n</span><span class=\"s2\">​\n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    parameters -- python dictionary containing your updated parameters \n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n<span class=\"err\">​</span>\n    <span class=\"n\">L</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">layers_dims</span><span class=\"p\">)</span>             <span class=\"c1\"># number of layers in the neural networks</span>\n    <span class=\"n\">costs</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>                       <span class=\"c1\"># to keep track of the cost</span>\n    <span class=\"n\">t</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>                            <span class=\"c1\"># initializing the counter required for Adam update</span>\n    <span class=\"n\">seed</span> <span class=\"o\">=</span> <span class=\"mi\">10</span>                        <span class=\"c1\"># For grading purposes, so that your &#34;random&#34; minibatches are the same as ours</span>\n    \n    <span class=\"c1\"># Initialize parameters</span>\n    <span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"n\">initialize_parameters</span><span class=\"p\">(</span><span class=\"n\">layers_dims</span><span class=\"p\">)</span>\n<span class=\"err\">​</span>\n    <span class=\"c1\"># Initialize the optimizer</span>\n    <span class=\"k\">if</span> <span class=\"n\">optimizer</span> <span class=\"o\">==</span> <span class=\"s2\">&#34;gd&#34;</span><span class=\"p\">:</span>\n        <span class=\"k\">pass</span> <span class=\"c1\"># no initialization required for gradient descent</span>\n    <span class=\"k\">elif</span> <span class=\"n\">optimizer</span> <span class=\"o\">==</span> <span class=\"s2\">&#34;momentum&#34;</span><span class=\"p\">:</span>\n        <span class=\"n\">v</span> <span class=\"o\">=</span> <span class=\"n\">initialize_velocity</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">)</span>\n    <span class=\"k\">elif</span> <span class=\"n\">optimizer</span> <span class=\"o\">==</span> <span class=\"s2\">&#34;adam&#34;</span><span class=\"p\">:</span>\n        <span class=\"n\">v</span><span class=\"p\">,</span> <span class=\"n\">s</span> <span class=\"o\">=</span> <span class=\"n\">initialize_adam</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">)</span>\n    \n    <span class=\"c1\"># Optimization loop</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">num_epochs</span><span class=\"p\">):</span>\n        \n        <span class=\"c1\"># Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch</span>\n        <span class=\"n\">seed</span> <span class=\"o\">=</span> <span class=\"n\">seed</span> <span class=\"o\">+</span> <span class=\"mi\">1</span>\n        <span class=\"n\">minibatches</span> <span class=\"o\">=</span> <span class=\"n\">random_mini_batches</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">mini_batch_size</span><span class=\"p\">,</span> <span class=\"n\">seed</span><span class=\"p\">)</span>\n<span class=\"err\">​</span>\n        <span class=\"k\">for</span> <span class=\"n\">minibatch</span> <span class=\"ow\">in</span> <span class=\"n\">minibatches</span><span class=\"p\">:</span>\n<span class=\"err\">​</span>\n            <span class=\"c1\"># Select a minibatch</span>\n            <span class=\"p\">(</span><span class=\"n\">minibatch_X</span><span class=\"p\">,</span> <span class=\"n\">minibatch_Y</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">minibatch</span>\n<span class=\"err\">​</span>\n            <span class=\"c1\"># Forward propagation</span>\n            <span class=\"n\">a3</span><span class=\"p\">,</span> <span class=\"n\">caches</span> <span class=\"o\">=</span> <span class=\"n\">forward_propagation</span><span class=\"p\">(</span><span class=\"n\">minibatch_X</span><span class=\"p\">,</span> <span class=\"n\">parameters</span><span class=\"p\">)</span>\n<span class=\"err\">​</span>\n            <span class=\"c1\"># Compute cost</span>\n            <span class=\"n\">cost</span> <span class=\"o\">=</span> <span class=\"n\">compute_cost</span><span class=\"p\">(</span><span class=\"n\">a3</span><span class=\"p\">,</span> <span class=\"n\">minibatch_Y</span><span class=\"p\">)</span>\n<span class=\"err\">​</span>\n            <span class=\"c1\"># Backward propagation</span>\n            <span class=\"n\">grads</span> <span class=\"o\">=</span> <span class=\"n\">backward_propagation</span><span class=\"p\">(</span><span class=\"n\">minibatch_X</span><span class=\"p\">,</span> <span class=\"n\">minibatch_Y</span><span class=\"p\">,</span> <span class=\"n\">caches</span><span class=\"p\">)</span>\n<span class=\"err\">​</span>\n            <span class=\"c1\"># Update parameters</span>\n            <span class=\"k\">if</span> <span class=\"n\">optimizer</span> <span class=\"o\">==</span> <span class=\"s2\">&#34;gd&#34;</span><span class=\"p\">:</span>\n                <span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"n\">update_parameters_with_gd</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">grads</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span><span class=\"p\">)</span>\n            <span class=\"k\">elif</span> <span class=\"n\">optimizer</span> <span class=\"o\">==</span> <span class=\"s2\">&#34;momentum&#34;</span><span class=\"p\">:</span>\n                <span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">v</span> <span class=\"o\">=</span> <span class=\"n\">update_parameters_with_momentum</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">grads</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">,</span> <span class=\"n\">beta</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span><span class=\"p\">)</span>\n            <span class=\"k\">elif</span> <span class=\"n\">optimizer</span> <span class=\"o\">==</span> <span class=\"s2\">&#34;adam&#34;</span><span class=\"p\">:</span>\n                <span class=\"n\">t</span> <span class=\"o\">=</span> <span class=\"n\">t</span> <span class=\"o\">+</span> <span class=\"mi\">1</span> <span class=\"c1\"># Adam counter</span>\n                <span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">,</span> <span class=\"n\">s</span> <span class=\"o\">=</span> <span class=\"n\">update_parameters_with_adam</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">grads</span><span class=\"p\">,</span> <span class=\"n\">v</span><span class=\"p\">,</span> <span class=\"n\">s</span><span class=\"p\">,</span>\n                                                               <span class=\"n\">t</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span><span class=\"p\">,</span> <span class=\"n\">beta1</span><span class=\"p\">,</span> <span class=\"n\">beta2</span><span class=\"p\">,</span>  <span class=\"n\">epsilon</span><span class=\"p\">)</span>\n        \n        <span class=\"c1\"># Print the cost every 1000 epoch</span>\n        <span class=\"k\">if</span> <span class=\"n\">print_cost</span> <span class=\"ow\">and</span> <span class=\"n\">i</span> <span class=\"o\">%</span> <span class=\"mi\">1000</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n            <span class=\"k\">print</span> <span class=\"p\">(</span><span class=\"s2\">&#34;Cost after epoch </span><span class=\"si\">%i</span><span class=\"s2\">: </span><span class=\"si\">%f</span><span class=\"s2\">&#34;</span> <span class=\"o\">%</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">cost</span><span class=\"p\">))</span>\n        <span class=\"k\">if</span> <span class=\"n\">print_cost</span> <span class=\"ow\">and</span> <span class=\"n\">i</span> <span class=\"o\">%</span> <span class=\"mi\">100</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n            <span class=\"n\">costs</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">cost</span><span class=\"p\">)</span>\n                \n    <span class=\"c1\"># plot the cost</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">costs</span><span class=\"p\">)</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">ylabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;cost&#39;</span><span class=\"p\">)</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">xlabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;epochs (per 100)&#39;</span><span class=\"p\">)</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">title</span><span class=\"p\">(</span><span class=\"s2\">&#34;Learning rate = &#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">learning_rate</span><span class=\"p\">))</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>\n<span class=\"err\">​</span>\n    <span class=\"k\">return</span> <span class=\"n\">parameters</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>效果</b></h2><p><b>gradient descent</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-37522333a34f0a4a83b79f4b37cdfb33_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"451\" data-rawheight=\"278\" class=\"origin_image zh-lightbox-thumb\" width=\"451\" data-original=\"https://pic4.zhimg.com/v2-37522333a34f0a4a83b79f4b37cdfb33_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;451&#39; height=&#39;278&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"451\" data-rawheight=\"278\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"451\" data-original=\"https://pic4.zhimg.com/v2-37522333a34f0a4a83b79f4b37cdfb33_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-37522333a34f0a4a83b79f4b37cdfb33_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-bac9146c2d6679e8dff5b4bb98232309_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"458\" data-rawheight=\"278\" class=\"origin_image zh-lightbox-thumb\" width=\"458\" data-original=\"https://pic2.zhimg.com/v2-bac9146c2d6679e8dff5b4bb98232309_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;458&#39; height=&#39;278&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"458\" data-rawheight=\"278\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"458\" data-original=\"https://pic2.zhimg.com/v2-bac9146c2d6679e8dff5b4bb98232309_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-bac9146c2d6679e8dff5b4bb98232309_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>gradient descent with momentum</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-4be6330e00bf92624bddbdcd779bc9eb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"451\" data-rawheight=\"278\" class=\"origin_image zh-lightbox-thumb\" width=\"451\" data-original=\"https://pic4.zhimg.com/v2-4be6330e00bf92624bddbdcd779bc9eb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;451&#39; height=&#39;278&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"451\" data-rawheight=\"278\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"451\" data-original=\"https://pic4.zhimg.com/v2-4be6330e00bf92624bddbdcd779bc9eb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-4be6330e00bf92624bddbdcd779bc9eb_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-b567c999d13db08cba6189359d2e25ae_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"458\" data-rawheight=\"278\" class=\"origin_image zh-lightbox-thumb\" width=\"458\" data-original=\"https://pic3.zhimg.com/v2-b567c999d13db08cba6189359d2e25ae_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;458&#39; height=&#39;278&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"458\" data-rawheight=\"278\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"458\" data-original=\"https://pic3.zhimg.com/v2-b567c999d13db08cba6189359d2e25ae_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-b567c999d13db08cba6189359d2e25ae_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>Adam mode</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-806be2bf86839e8694b86c932370de61_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"445\" data-rawheight=\"278\" class=\"origin_image zh-lightbox-thumb\" width=\"445\" data-original=\"https://pic2.zhimg.com/v2-806be2bf86839e8694b86c932370de61_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;445&#39; height=&#39;278&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"445\" data-rawheight=\"278\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"445\" data-original=\"https://pic2.zhimg.com/v2-806be2bf86839e8694b86c932370de61_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-806be2bf86839e8694b86c932370de61_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-436b0f5d97a8d886d7fe952c3216a628_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"458\" data-rawheight=\"278\" class=\"origin_image zh-lightbox-thumb\" width=\"458\" data-original=\"https://pic1.zhimg.com/v2-436b0f5d97a8d886d7fe952c3216a628_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;458&#39; height=&#39;278&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"458\" data-rawheight=\"278\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"458\" data-original=\"https://pic1.zhimg.com/v2-436b0f5d97a8d886d7fe952c3216a628_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-436b0f5d97a8d886d7fe952c3216a628_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>效果还是很明显的：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-d1b43e773c0b32688e1e530269278583_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"354\" data-rawheight=\"129\" class=\"content_image\" width=\"354\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;354&#39; height=&#39;129&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"354\" data-rawheight=\"129\" class=\"content_image lazy\" width=\"354\" data-actualsrc=\"https://pic4.zhimg.com/v2-d1b43e773c0b32688e1e530269278583_b.jpg\"/></figure><p></p><p></p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "神经网络", 
                    "tagLink": "https://api.zhihu.com/topics/19607065"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/44718107", 
            "userName": "直上云霄", 
            "userLink": "https://www.zhihu.com/people/1033165ce4ad9c3fce69a0793dfab8ad", 
            "upvote": 0, 
            "title": "DeepLearning.ai笔记:(2-2)-优化算法", 
            "content": "<h2>首发于个人博客：<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">fangzh.top</span><span class=\"invisible\"></span></a>，欢迎来访</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>这周学习了优化算法，可以让神经网络运行的更快。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>主要有:</p><ul><li>mini-batch</li><li>动量梯度下降(momentum)</li><li>RMSprop</li><li>Adam优化算法</li><li>学习率衰减</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>mini-batch(小批量)</b></h2><p>原本的梯度下降算法，在每一次的迭代中，要把所有的数据都进行计算再取平均，那如果你的数据量特别大的话，每进行一次迭代就会耗费大量的时间。</p><p>所以就有了mini-batch，做小批量的计算迭代。也就是把训练集划分成n等分，比如数据量有500万个的时候，以1000为单位，将数据集划分为5000份， <img src=\"https://www.zhihu.com/equation?tex=+x+%3D++%7Bx%5E%7B%5Clbrace+1+%5Crbrace%7D%2Cx%5E%7B%5Clbrace+2+%5Crbrace%7D%2Cx%5E%7B%5Clbrace+3+%5Crbrace%7D%2C.....%2Cx%5E%7B%5Clbrace+5000+%5Crbrace%7D%7D\" alt=\" x =  {x^{\\lbrace 1 \\rbrace},x^{\\lbrace 2 \\rbrace},x^{\\lbrace 3 \\rbrace},.....,x^{\\lbrace 5000 \\rbrace}}\" eeimg=\"1\"/> </p><p>用大括弧表示每一份的mini-batch，其中每一份 <img src=\"https://www.zhihu.com/equation?tex=x%5E%7B%5Clbrace+t+%5Crbrace%7D\" alt=\"x^{\\lbrace t \\rbrace}\" eeimg=\"1\"/> 都是1000个样本。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-06800c573474f6c5effafbed3caa52ab_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1469\" data-rawheight=\"451\" class=\"origin_image zh-lightbox-thumb\" width=\"1469\" data-original=\"https://pic4.zhimg.com/v2-06800c573474f6c5effafbed3caa52ab_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1469&#39; height=&#39;451&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1469\" data-rawheight=\"451\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1469\" data-original=\"https://pic4.zhimg.com/v2-06800c573474f6c5effafbed3caa52ab_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-06800c573474f6c5effafbed3caa52ab_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>这个时候引入epoch的概念，1个epoch相当于是遍历了一次数据集，比如用mini-batch，1个epoch就可以进行5000次迭代，而传统的batch把数据集都一起计算，相当于1个epoch只进行了1次迭代。</p><p>具体计算步骤是：</p><ul><li>先划分好每一个mini-batch</li><li><code>for t in range(5000)</code>，循环每次迭代<br/></li><ul><li>循环里面和之前的计算过程一样，前向传播，但每次计算量是1000个样本</li><li>计算损失函数</li><li>反向传播</li><li>更新参数</li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>batch和mini-batch的对比如图：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-9005b6652ea8829154d3e22dcedc5fc5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1469\" data-rawheight=\"816\" class=\"origin_image zh-lightbox-thumb\" width=\"1469\" data-original=\"https://pic2.zhimg.com/v2-9005b6652ea8829154d3e22dcedc5fc5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1469&#39; height=&#39;816&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1469\" data-rawheight=\"816\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1469\" data-original=\"https://pic2.zhimg.com/v2-9005b6652ea8829154d3e22dcedc5fc5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-9005b6652ea8829154d3e22dcedc5fc5_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>如果mini-batch的样本为m的话，其实就是<b>batch gradient descent</b>，缺点是如果样本量太大的话，每一次迭代的时间会比较长，但是优点是每一次迭代的损失函数都是下降的，比较平稳。</li><li>mini-batch样本为1的话，那就是<b>随机梯度下降（Stochastic gradient descent）</b>,也就是每次迭代只选择其中一个样本进行迭代，但是这样会失去了样本向量化带来的计算加速效果，损失函数总体是下降的，但是局部会很抖动，很可能无法达到全局最小点。</li><li>所以选择一个合适的size很重要， <img src=\"https://www.zhihu.com/equation?tex=1+%3C+size+%3C+m\" alt=\"1 &lt; size &lt; m\" eeimg=\"1\"/> ，可以实现快速的计算效果，也能够享受向量化带来的加速。</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-5972813cc0cdae43c4e9bdfe97d0b215_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"497\" data-rawheight=\"426\" class=\"origin_image zh-lightbox-thumb\" width=\"497\" data-original=\"https://pic2.zhimg.com/v2-5972813cc0cdae43c4e9bdfe97d0b215_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;497&#39; height=&#39;426&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"497\" data-rawheight=\"426\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"497\" data-original=\"https://pic2.zhimg.com/v2-5972813cc0cdae43c4e9bdfe97d0b215_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-5972813cc0cdae43c4e9bdfe97d0b215_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>mini-batch size的选择</b></p><p>因为电脑的内存和使用方式都是二进制的，而且是2的n次方，所以之前选1000也不太合理，可以选1024，但是1024也比较少见，一般是从64到512。也就是$64、128、256、512$</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>指数加权平均(Exponentially weighted averages )</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-00c157a2c8b43d398f24216e4a310798_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"704\" data-rawheight=\"391\" class=\"origin_image zh-lightbox-thumb\" width=\"704\" data-original=\"https://pic1.zhimg.com/v2-00c157a2c8b43d398f24216e4a310798_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;704&#39; height=&#39;391&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"704\" data-rawheight=\"391\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"704\" data-original=\"https://pic1.zhimg.com/v2-00c157a2c8b43d398f24216e4a310798_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-00c157a2c8b43d398f24216e4a310798_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>蓝色的点是每一天的气温，可以看到是非常抖动的，那如果可以把它平均一下，比如把10天内的气温平均一下，就可以得到如红色的曲线。</p><p>但是如果是单纯的把前面的10天气温一起平均的话，那么这样你就需要把前10天的气温全部储存记录下来，这样子虽然会更准一点，但是很浪费储存空间，所以就有了<b>指数加权平均</b>这样的概念。方法如下：</p><p><img src=\"https://www.zhihu.com/equation?tex=V_0+%3D+0\" alt=\"V_0 = 0\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=V_1+%3D+%5Cbeta+%2A+V_0+%2B+%281+-+%5Cbeta%29+%5Ctheta_1\" alt=\"V_1 = \\beta * V_0 + (1 - \\beta) \\theta_1\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%E2%80%A6%E2%80%A6\" alt=\"……\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=V_t+%3D+%5Cbeta+%2A+V_%7Bt-1%7D+%2B+%281+-+%5Cbeta%29+%5Ctheta_t\" alt=\"V_t = \\beta * V_{t-1} + (1 - \\beta) \\theta_t\" eeimg=\"1\"/> </p><p>其中， <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_t\" alt=\"\\theta_t\" eeimg=\"1\"/> 表示第t天的温度，而 <img src=\"https://www.zhihu.com/equation?tex=V_t\" alt=\"V_t\" eeimg=\"1\"/> 表示指数加权平均后的第t天温度， <img src=\"https://www.zhihu.com/equation?tex=%5Cbeta\" alt=\"\\beta\" eeimg=\"1\"/> 这个参数表示 <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7B1-%5Cbeta%7D\" alt=\"\\frac{1}{1-\\beta}\" eeimg=\"1\"/> 天的平均，也就是， <img src=\"https://www.zhihu.com/equation?tex=%5Cbeta+%3D+0.9\" alt=\"\\beta = 0.9\" eeimg=\"1\"/> ，表示10天内的平均， <img src=\"https://www.zhihu.com/equation?tex=%5Cbeta+%3D+0.98\" alt=\"\\beta = 0.98\" eeimg=\"1\"/> ，表示50天内的平均。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-d1b2d7a4aa8ebba490950f72c4db22e1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"816\" data-rawheight=\"402\" class=\"origin_image zh-lightbox-thumb\" width=\"816\" data-original=\"https://pic2.zhimg.com/v2-d1b2d7a4aa8ebba490950f72c4db22e1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;816&#39; height=&#39;402&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"816\" data-rawheight=\"402\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"816\" data-original=\"https://pic2.zhimg.com/v2-d1b2d7a4aa8ebba490950f72c4db22e1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-d1b2d7a4aa8ebba490950f72c4db22e1_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>理解指数加权平均</b></h2><p>我们再来看一下公式：</p><p><img src=\"https://www.zhihu.com/equation?tex=v_t+%3D+%5Cbeta+v_%7Bt-1%7D+%2B+%281+-+%5Cbeta%29+%5Ctheta_t\" alt=\"v_t = \\beta v_{t-1} + (1 - \\beta) \\theta_t\" eeimg=\"1\"/> </p><p>假设$\\beta = 0.9$，那么</p><p><img src=\"https://www.zhihu.com/equation?tex=v_%7B100%7D+%3D+0.9v_%7B99%7D+%2B+0.1%5Ctheta_%7B100%7D\" alt=\"v_{100} = 0.9v_{99} + 0.1\\theta_{100}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=v_%7B99%7D+%3D+0.9v_%7B98%7D+%2B+0.1%5Ctheta_%7B99%7D\" alt=\"v_{99} = 0.9v_{98} + 0.1\\theta_{99}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=v_%7B98%7D+%3D+0.9v_%7B97%7D+%2B+0.1%5Ctheta_%7B98%7D\" alt=\"v_{98} = 0.9v_{97} + 0.1\\theta_{98}\" eeimg=\"1\"/> </p><p>展开一下，得到：</p><p><img src=\"https://www.zhihu.com/equation?tex=+v_%7B100%7D+%3D+0.1+%5Ctheta_%7B100%7D+%2B+0.1+%5Ctimes+0.9+%5Ctimes+%5Ctheta_%7B99%7D+%2B++0.1+%5Ctimes+0.9%5E2++%5Ctimes+%5Ctheta_%7B98%7D+%2B+......\" alt=\" v_{100} = 0.1 \\theta_{100} + 0.1 \\times 0.9 \\times \\theta_{99} +  0.1 \\times 0.9^2  \\times \\theta_{98} + ......\" eeimg=\"1\"/> </p><p>看到没有，每一项都会乘以0.9，这样就是指数加权的意思了，那么为什么表示的是10天内的平均值呢？明明是10天以前的数据都有加进去的才对，其实是因为 <img src=\"https://www.zhihu.com/equation?tex=0.9%5E%7B10%7D+%5Capprox+0.35+%5Capprox+%5Cfrac%7B1%7D%7Be%7D\" alt=\"0.9^{10} \\approx 0.35 \\approx \\frac{1}{e}\" eeimg=\"1\"/> ，也就是10天以前的权重只占了三分之一左右，已经很小了，所以我们就可以认为这个权重就是10天内的温度平均，其实有详细的数学证明的，这里就不要证明了，反正理解了 <img src=\"https://www.zhihu.com/equation?tex=%281-%5Cepsilon%29%5E%7B%5Cfrac%7B1%7D%7B%5Cepsilon%7D%7D+%5Capprox+%5Cfrac%7B1%7D%7Be%7D%EF%BC%8C%5Cepsilon\" alt=\"(1-\\epsilon)^{\\frac{1}{\\epsilon}} \\approx \\frac{1}{e}，\\epsilon\" eeimg=\"1\"/> 为0.02的时候，就代表了50天内的数据。</p><p>因为指数加权平均不需要知道前面n个数据，只要一步一步进行迭代，知道当前的数据就行，所以非常节省空间。</p><h2><b>指数加权平均的偏差修正</b></h2><p>如果你细心一点，你就会发现其实这个公式有问题，</p><p><img src=\"https://www.zhihu.com/equation?tex=V_0+%3D+0\" alt=\"V_0 = 0\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=V_1+%3D+%5Cbeta+%2A+V_0+%2B+%281+-+%5Cbeta%29+%5Ctheta_1\" alt=\"V_1 = \\beta * V_0 + (1 - \\beta) \\theta_1\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%E2%80%A6%E2%80%A6\" alt=\"……\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=V_t+%3D+%5Cbeta+%2A+V_%7Bt-1%7D+%2B+%281+-+%5Cbeta%29+%5Ctheta_t\" alt=\"V_t = \\beta * V_{t-1} + (1 - \\beta) \\theta_t\" eeimg=\"1\"/> </p><p>如果第一天的温度是40摄氏度，那么 <img src=\"https://www.zhihu.com/equation?tex=V_1+%3D+0.1+%2A+40+%3D+4\" alt=\"V_1 = 0.1 * 40 = 4\" eeimg=\"1\"/> ，显然是不合理的。因为初始值 <img src=\"https://www.zhihu.com/equation?tex=V_0+%3D+0\" alt=\"V_0 = 0\" eeimg=\"1\"/> ，也就是前面几天的数据都会普遍偏低。所以特别是在估测初期，需要进行一些修正，这个时候就不要用 <img src=\"https://www.zhihu.com/equation?tex=v_t\" alt=\"v_t\" eeimg=\"1\"/> 了，而是用 <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7Bv_t%7D%7B1-%5Cbeta%5Et%7D\" alt=\"\\frac{v_t}{1-\\beta^t}\" eeimg=\"1\"/> 来代表第t天的温度平均，你会发现随着t的增加， <img src=\"https://www.zhihu.com/equation?tex=%5Cbeta%5Et\" alt=\"\\beta^t\" eeimg=\"1\"/> 接近于0，所以偏差修正几乎就没有用了，而t比较小的时候，就非常有效果。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-d304dbf1380aa255138b954fbef332fd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"833\" data-rawheight=\"346\" class=\"origin_image zh-lightbox-thumb\" width=\"833\" data-original=\"https://pic2.zhimg.com/v2-d304dbf1380aa255138b954fbef332fd_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;833&#39; height=&#39;346&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"833\" data-rawheight=\"346\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"833\" data-original=\"https://pic2.zhimg.com/v2-d304dbf1380aa255138b954fbef332fd_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-d304dbf1380aa255138b954fbef332fd_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>不过在大部分机器学习中，一般也不需要修正，因为只是前面的初始时期比较有偏差而已，到后面就基本不会有偏差了，所以也不太用。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>动量梯度下降法 (Gradient descent with Momentum )</b></h2><p>用动量梯度下降法运行速度总是比标准的梯度下降法要来的快。它的基本思想是计算梯度的指数加权平均数，然后用该梯度来更新权重。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>效果如图：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-5ed6d8f8ee5a2f5bc3347f29dd8c53a1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1411\" data-rawheight=\"383\" class=\"origin_image zh-lightbox-thumb\" width=\"1411\" data-original=\"https://pic2.zhimg.com/v2-5ed6d8f8ee5a2f5bc3347f29dd8c53a1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1411&#39; height=&#39;383&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1411\" data-rawheight=\"383\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1411\" data-original=\"https://pic2.zhimg.com/v2-5ed6d8f8ee5a2f5bc3347f29dd8c53a1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-5ed6d8f8ee5a2f5bc3347f29dd8c53a1_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>使用动量梯度下降法后，在竖直方向上的抖动减少了，而在水平方向上的运动反而加速了。</p><p>算法公式：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-60f2fa472b9a4de75a29bbd5ae54e9dd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1104\" data-rawheight=\"463\" class=\"origin_image zh-lightbox-thumb\" width=\"1104\" data-original=\"https://pic2.zhimg.com/v2-60f2fa472b9a4de75a29bbd5ae54e9dd_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1104&#39; height=&#39;463&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1104\" data-rawheight=\"463\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1104\" data-original=\"https://pic2.zhimg.com/v2-60f2fa472b9a4de75a29bbd5ae54e9dd_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-60f2fa472b9a4de75a29bbd5ae54e9dd_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>可以发现，就是根据指数平均计算出了 <img src=\"https://www.zhihu.com/equation?tex=v_%7BdW%7D\" alt=\"v_{dW}\" eeimg=\"1\"/> <i>，然后更新参数时把</i> <img src=\"https://www.zhihu.com/equation?tex=dW\" alt=\"dW\" eeimg=\"1\"/> <i>换成了</i> <img src=\"https://www.zhihu.com/equation?tex=v_%7Bdw%7D\" alt=\"v_{dw}\" eeimg=\"1\"/> ， <img src=\"https://www.zhihu.com/equation?tex=%5Cbeta\" alt=\"\\beta\" eeimg=\"1\"/> 一般的取值是0.9。可以发现，在纵向的波动经过平均以后，变得非常小了，而因为在横向上，每一次的微分分量都是指向低点，所以平均后的值一直朝着低点前进。</p><p>物理意义：</p><ul><li>个人的理解是大概这个公式也很像动量的公式 <img src=\"https://www.zhihu.com/equation?tex=m+v+%3D+m_1+v_1+%2B+m_2+v_2\" alt=\"m v = m_1 v_1 + m_2 v_2\" eeimg=\"1\"/> ，也就是把两个物体合并了得到新物体的质量和速度的意思</li><li>理解成速度和加速度，把 <img src=\"https://www.zhihu.com/equation?tex=v_%7BdW%7D\" alt=\"v_{dW}\" eeimg=\"1\"/> 看成速度， <img src=\"https://www.zhihu.com/equation?tex=dW\" alt=\"dW\" eeimg=\"1\"/> 看成加速度，这样每次因为有速度的存在，加速度只能影响到速度的大小而不能够立刻改变速度的方向。</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>RMSprop（root mean square prop）</b></h2><p>均方根传播。这是另一种梯度下降的优化算法。</p><p>顾名思义，先平方再开根号。</p><p>其实和动量梯度下降法公式差不多：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-10dabf6dd378007f0660b9daa9cf0b04_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"958\" data-rawheight=\"454\" class=\"origin_image zh-lightbox-thumb\" width=\"958\" data-original=\"https://pic1.zhimg.com/v2-10dabf6dd378007f0660b9daa9cf0b04_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;958&#39; height=&#39;454&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"958\" data-rawheight=\"454\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"958\" data-original=\"https://pic1.zhimg.com/v2-10dabf6dd378007f0660b9daa9cf0b04_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-10dabf6dd378007f0660b9daa9cf0b04_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>在更新参数的分母项加了一项 <img src=\"https://www.zhihu.com/equation?tex=%5Cepsilon+%3D+10%5E%7B-8%7D\" alt=\"\\epsilon = 10^{-8}\" eeimg=\"1\"/> ,来确保算法不会除以0</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>Adam算法</b></h2><p>Adam算法其实就是结合了Momentum和RMSprop ，注意这个时候要加上偏差修正：</p><ul><li>初始化参数： <img src=\"https://www.zhihu.com/equation?tex=v_%7BdW%7D+%3D+0%EF%BC%8CS_%7BdW%7D+%3D0%EF%BC%8Cv_%7Bdb%7D+%3D+0%EF%BC%8CS_%7Bdb%7D+%3D0\" alt=\"v_{dW} = 0，S_{dW} =0，v_{db} = 0，S_{db} =0\" eeimg=\"1\"/> </li><li>在第 <img src=\"https://www.zhihu.com/equation?tex=t\" alt=\"t\" eeimg=\"1\"/> 次迭代中，<br/></li><ul><li>计算mini-batch的dW,db</li><li>Momentum: <img src=\"https://www.zhihu.com/equation?tex=v_%7BdW%7D%3D+%5Cbeta_%7B1%7Dv_%7BdW%7D+%2B+%28+1+-+%5Cbeta_%7B1%7D%29dW%EF%BC%8Cv_%7Bdb%7D%3D+%5Cbeta_%7B1%7Dv_%7Bdb%7D+%2B+%28+1+-%5Cbeta_%7B1%7D+%29%7Bdb%7D\" alt=\"v_{dW}= \\beta_{1}v_{dW} + ( 1 - \\beta_{1})dW，v_{db}= \\beta_{1}v_{db} + ( 1 -\\beta_{1} ){db}\" eeimg=\"1\"/> </li><li>RMSprop: <img src=\"https://www.zhihu.com/equation?tex=S_%7BdW%7D%3D%5Cbeta_%7B2%7DS_%7BdW%7D+%2B+%28+1+-+%5Cbeta_%7B2%7D%29%7B%28dW%29%7D%5E%7B2%7D%24%EF%BC%8C%24S_%7Bdb%7D+%3D%5Cbeta_%7B2%7DS_%7Bdb%7D+%2B+%5Cleft%28+1+-+%5Cbeta_%7B2%7D+%5Cright%29%7B%28db%29%7D%5E%7B2%7D\" alt=\"S_{dW}=\\beta_{2}S_{dW} + ( 1 - \\beta_{2}){(dW)}^{2}$，$S_{db} =\\beta_{2}S_{db} + \\left( 1 - \\beta_{2} \\right){(db)}^{2}\" eeimg=\"1\"/> </li><li><img src=\"https://www.zhihu.com/equation?tex=v_%7BdW%7D%5E%7B%5Ctext%7Bcorrected%7D%7D%3D+%5Cfrac%7Bv_%7BdW%7D%7D%7B1+-+%5Cbeta_%7B1%7D%5E%7Bt%7D%7D%EF%BC%8Cv_%7Bdb%7D%5E%7B%5Ctext%7Bcorrected%7D%7D+%3D%5Cfrac%7Bv_%7Bdb%7D%7D%7B1+-%5Cbeta_%7B1%7D%5E%7Bt%7D%7D\" alt=\"v_{dW}^{\\text{corrected}}= \\frac{v_{dW}}{1 - \\beta_{1}^{t}}，v_{db}^{\\text{corrected}} =\\frac{v_{db}}{1 -\\beta_{1}^{t}}\" eeimg=\"1\"/> </li><li><img src=\"https://www.zhihu.com/equation?tex=S_%7BdW%7D%5E%7B%5Ctext%7Bcorrected%7D%7D+%3D%5Cfrac%7BS_%7BdW%7D%7D%7B1+-+%5Cbeta_%7B2%7D%5E%7Bt%7D%7D%EF%BC%8CS_%7Bdb%7D%5E%7B%5Ctext%7Bcorrected%7D%7D+%3D%5Cfrac%7BS_%7Bdb%7D%7D%7B1+-+%5Cbeta_%7B2%7D%5E%7Bt%7D%7D\" alt=\"S_{dW}^{\\text{corrected}} =\\frac{S_{dW}}{1 - \\beta_{2}^{t}}，S_{db}^{\\text{corrected}} =\\frac{S_{db}}{1 - \\beta_{2}^{t}}\" eeimg=\"1\"/> </li><li><img src=\"https://www.zhihu.com/equation?tex=W%3A%3D+W+-+%5Cfrac%7Ba+v_%7BdW%7D%5E%7B%5Ctext%7Bcorrected%7D%7D%7D%7B%5Csqrt%7BS_%7BdW%7D%5E%7B%5Ctext%7Bcorrected%7D%7D%7D+%2B%5Cvarepsilon%7D\" alt=\"W:= W - \\frac{a v_{dW}^{\\text{corrected}}}{\\sqrt{S_{dW}^{\\text{corrected}}} +\\varepsilon}\" eeimg=\"1\"/> </li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>超参数有 <img src=\"https://www.zhihu.com/equation?tex=%5Calpha%2C%5Cbeta_1%2C%5Cbeta_2%2C%5Cepsilon\" alt=\"\\alpha,\\beta_1,\\beta_2,\\epsilon\" eeimg=\"1\"/> ，一般 <img src=\"https://www.zhihu.com/equation?tex=%5Cbeta_1+%3D+0.9%2C%5Cbeta_2+%3D+0.999%2C%5Cepsilon+%3D+10%5E%7B-8%7D\" alt=\"\\beta_1 = 0.9,\\beta_2 = 0.999,\\epsilon = 10^{-8}\" eeimg=\"1\"/> </p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>学习率衰减</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p>在梯度下降时，如果是固定的学习率 <img src=\"https://www.zhihu.com/equation?tex=%5Calpha\" alt=\"\\alpha\" eeimg=\"1\"/> ，在到达最小值附近的时候，可能不会精确收敛，会很抖动，因此很难达到最小值，所以可以考虑学习率衰减，在迭代过程中，逐渐减小$\\alpha$，这样一开始比较快，后来慢慢的变慢。</p><p>常用的是：</p><p><img src=\"https://www.zhihu.com/equation?tex=a%3D+%5Cfrac%7B1%7D%7B1+%2B+decayrate+%2A+%5Ctext%7Bepoch_num%7D%7D+a_%7B0%7D\" alt=\"a= \\frac{1}{1 + decayrate * \\text{epoch_num}} a_{0}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=a+%3D%5Cfrac%7Bk%7D%7B%5Csqrt%7B%5Ctext%7Bepoch_num%7D%7D%7Da_%7B0%7D\" alt=\"a =\\frac{k}{\\sqrt{\\text{epoch_num}}}a_{0}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=a+%3D%5Cfrac%7Bk%7D%7B%5Csqrt%7Bt%7D%7Da_%7B0%7D\" alt=\"a =\\frac{k}{\\sqrt{t}}a_{0}\" eeimg=\"1\"/> </p><p></p><p></p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "神经网络", 
                    "tagLink": "https://api.zhihu.com/topics/19607065"
                }
            ], 
            "comments": [
                {
                    "userName": "知乎用户", 
                    "userLink": "https://www.zhihu.com/people/0", 
                    "content": "<p>“个人理解”这个地方不对。</p><p>补充下吧,dW其实是根据二阶差分方程推导得出的“动量相关的项”</p><p>加速度这个说法是正确的。</p><p>所以伪代码中非要说动量的话，其实是dW.</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/44675417", 
            "userName": "直上云霄", 
            "userLink": "https://www.zhihu.com/people/1033165ce4ad9c3fce69a0793dfab8ad", 
            "upvote": 1, 
            "title": "DeepLearning.ai作业:(2-1)-深度学习实践", 
            "content": "<h2>本文手法于个人博客：<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">fangzh.top</span><span class=\"invisible\"></span></a>，欢迎来访</h2><ol><li>不要抄作业！</li><li>我只是把思路整理了，供个人学习。</li><li>不要抄作业！</li></ol><p class=\"ztext-empty-paragraph\"><br/></p><p>本周的作业分了3部分：</p><ul><li>初始化参数</li><li>正则化（L2、dropout）</li><li>梯度检验</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>part1：Initialization</b></h2><p>主要说明的不同的初始化对迭代的影响。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>首先，模型函数是这样的：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">model</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span> <span class=\"o\">=</span> <span class=\"mf\">0.01</span><span class=\"p\">,</span> <span class=\"n\">num_iterations</span> <span class=\"o\">=</span> <span class=\"mi\">15000</span><span class=\"p\">,</span> <span class=\"n\">print_cost</span> <span class=\"o\">=</span> <span class=\"bp\">True</span><span class=\"p\">,</span> <span class=\"n\">initialization</span> <span class=\"o\">=</span> <span class=\"s2\">&#34;he&#34;</span><span class=\"p\">):</span>\n\n    <span class=\"n\">grads</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>\n    <span class=\"n\">costs</span> <span class=\"o\">=</span> <span class=\"p\">[]</span> <span class=\"c1\"># to keep track of the loss</span>\n    <span class=\"n\">m</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"c1\"># number of examples</span>\n    <span class=\"n\">layers_dims</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"mi\">10</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span>\n    \n    <span class=\"c1\"># Initialize parameters dictionary.</span>\n    <span class=\"k\">if</span> <span class=\"n\">initialization</span> <span class=\"o\">==</span> <span class=\"s2\">&#34;zeros&#34;</span><span class=\"p\">:</span>\n        <span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"n\">initialize_parameters_zeros</span><span class=\"p\">(</span><span class=\"n\">layers_dims</span><span class=\"p\">)</span>\n    <span class=\"k\">elif</span> <span class=\"n\">initialization</span> <span class=\"o\">==</span> <span class=\"s2\">&#34;random&#34;</span><span class=\"p\">:</span>\n        <span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"n\">initialize_parameters_random</span><span class=\"p\">(</span><span class=\"n\">layers_dims</span><span class=\"p\">)</span>\n    <span class=\"k\">elif</span> <span class=\"n\">initialization</span> <span class=\"o\">==</span> <span class=\"s2\">&#34;he&#34;</span><span class=\"p\">:</span>\n        <span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"n\">initialize_parameters_he</span><span class=\"p\">(</span><span class=\"n\">layers_dims</span><span class=\"p\">)</span>\n<span class=\"err\">​</span>\n    <span class=\"c1\"># Loop (gradient descent)</span>\n<span class=\"err\">​</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">num_iterations</span><span class=\"p\">):</span>\n<span class=\"err\">​</span>\n        <span class=\"c1\"># Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID.</span>\n        <span class=\"n\">a3</span><span class=\"p\">,</span> <span class=\"n\">cache</span> <span class=\"o\">=</span> <span class=\"n\">forward_propagation</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">parameters</span><span class=\"p\">)</span>\n        \n        <span class=\"c1\"># Loss</span>\n        <span class=\"n\">cost</span> <span class=\"o\">=</span> <span class=\"n\">compute_loss</span><span class=\"p\">(</span><span class=\"n\">a3</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">)</span>\n<span class=\"err\">​</span>\n        <span class=\"c1\"># Backward propagation.</span>\n        <span class=\"n\">grads</span> <span class=\"o\">=</span> <span class=\"n\">backward_propagation</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">cache</span><span class=\"p\">)</span>\n        \n        <span class=\"c1\"># Update parameters.</span>\n        <span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"n\">update_parameters</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">grads</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span><span class=\"p\">)</span>\n        \n        <span class=\"c1\"># Print the loss every 1000 iterations</span>\n        <span class=\"k\">if</span> <span class=\"n\">print_cost</span> <span class=\"ow\">and</span> <span class=\"n\">i</span> <span class=\"o\">%</span> <span class=\"mi\">1000</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n            <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s2\">&#34;Cost after iteration {}: {}&#34;</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">cost</span><span class=\"p\">))</span>\n            <span class=\"n\">costs</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">cost</span><span class=\"p\">)</span>\n            \n    <span class=\"c1\"># plot the loss</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">costs</span><span class=\"p\">)</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">ylabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;cost&#39;</span><span class=\"p\">)</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">xlabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;iterations (per hundreds)&#39;</span><span class=\"p\">)</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">title</span><span class=\"p\">(</span><span class=\"s2\">&#34;Learning rate =&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">learning_rate</span><span class=\"p\">))</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">parameters</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><b>1. Zero Initialization</b></p><p>把参数全都置位0，结果是显而易见的，就是没有任何变化。</p><p><b>2. Random initialization</b></p><p>把W参数随机化了，但是乘以10倍系数，所以导致初始化的参数太大，收敛速度很慢</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">initialize_parameters_random</span><span class=\"p\">(</span><span class=\"n\">layers_dims</span><span class=\"p\">):</span>\n\n    <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">seed</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">)</span>               <span class=\"c1\"># This seed makes sure your &#34;random&#34; numbers will be the as ours</span>\n    <span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>\n    <span class=\"n\">L</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">layers_dims</span><span class=\"p\">)</span>            <span class=\"c1\"># integer representing the number of layers</span>\n    \n    <span class=\"k\">for</span> <span class=\"n\">l</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">L</span><span class=\"p\">):</span>\n        <span class=\"c1\">### START CODE HERE ### (≈ 2 lines of code)</span>\n        <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;W&#39;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">layers_dims</span><span class=\"p\">[</span><span class=\"n\">l</span><span class=\"p\">],</span> <span class=\"n\">layers_dims</span><span class=\"p\">[</span><span class=\"n\">l</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">])</span> <span class=\"o\">*</span> <span class=\"mi\">10</span>\n        <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;b&#39;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">layers_dims</span><span class=\"p\">[</span><span class=\"n\">l</span><span class=\"p\">],</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n<span class=\"err\">​</span>\n    <span class=\"k\">return</span> <span class=\"n\">parameters</span></code></pre></div><p>结果一般般</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-09f72f5f264d7bdcd5055c6f2ee8fae7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"451\" data-rawheight=\"278\" class=\"origin_image zh-lightbox-thumb\" width=\"451\" data-original=\"https://pic4.zhimg.com/v2-09f72f5f264d7bdcd5055c6f2ee8fae7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;451&#39; height=&#39;278&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"451\" data-rawheight=\"278\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"451\" data-original=\"https://pic4.zhimg.com/v2-09f72f5f264d7bdcd5055c6f2ee8fae7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-09f72f5f264d7bdcd5055c6f2ee8fae7_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>3. He initialization</b></p><p>把W参数随机化，但是乘上系数 <code>sqrt(2./layers_dims[l-1])</code></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">initialize_parameters_he</span><span class=\"p\">(</span><span class=\"n\">layers_dims</span><span class=\"p\">):</span>\n    \n    <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">seed</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">)</span>\n    <span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>\n    <span class=\"n\">L</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">layers_dims</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"mi\">1</span> <span class=\"c1\"># integer representing the number of layers</span>\n     \n    <span class=\"k\">for</span> <span class=\"n\">l</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">L</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">):</span>\n        <span class=\"c1\">### START CODE HERE ### (≈ 2 lines of code)</span>\n        <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;W&#39;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">layers_dims</span><span class=\"p\">[</span><span class=\"n\">l</span><span class=\"p\">],</span><span class=\"n\">layers_dims</span><span class=\"p\">[</span><span class=\"n\">l</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">])</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">sqrt</span><span class=\"p\">(</span><span class=\"mf\">2.</span><span class=\"o\">/</span><span class=\"n\">layers_dims</span><span class=\"p\">[</span><span class=\"n\">l</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">])</span>\n        <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;b&#39;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">layers_dims</span><span class=\"p\">[</span><span class=\"n\">l</span><span class=\"p\">],</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n        \n    <span class=\"k\">return</span> <span class=\"n\">parameters</span></code></pre></div><p>结果非常理想。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-e5a1cd4c69fb9a36cd5d628b2f20562d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"445\" data-rawheight=\"278\" class=\"origin_image zh-lightbox-thumb\" width=\"445\" data-original=\"https://pic2.zhimg.com/v2-e5a1cd4c69fb9a36cd5d628b2f20562d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;445&#39; height=&#39;278&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"445\" data-rawheight=\"278\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"445\" data-original=\"https://pic2.zhimg.com/v2-e5a1cd4c69fb9a36cd5d628b2f20562d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-e5a1cd4c69fb9a36cd5d628b2f20562d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>Part 2：Regularization</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p>数据集：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-e543fd04e7ea1d40a99727d33d180094_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"439\" data-rawheight=\"252\" class=\"origin_image zh-lightbox-thumb\" width=\"439\" data-original=\"https://pic1.zhimg.com/v2-e543fd04e7ea1d40a99727d33d180094_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;439&#39; height=&#39;252&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"439\" data-rawheight=\"252\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"439\" data-original=\"https://pic1.zhimg.com/v2-e543fd04e7ea1d40a99727d33d180094_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-e543fd04e7ea1d40a99727d33d180094_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>模型函数：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">model</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span> <span class=\"o\">=</span> <span class=\"mf\">0.3</span><span class=\"p\">,</span> <span class=\"n\">num_iterations</span> <span class=\"o\">=</span> <span class=\"mi\">30000</span><span class=\"p\">,</span> <span class=\"n\">print_cost</span> <span class=\"o\">=</span> <span class=\"bp\">True</span><span class=\"p\">,</span> <span class=\"n\">lambd</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">keep_prob</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">):</span>\n\n        \n    <span class=\"n\">grads</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>\n    <span class=\"n\">costs</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>                            <span class=\"c1\"># to keep track of the cost</span>\n    <span class=\"n\">m</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span>                        <span class=\"c1\"># number of examples</span>\n    <span class=\"n\">layers_dims</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span>\n    \n    <span class=\"c1\"># Initialize parameters dictionary.</span>\n    <span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"n\">initialize_parameters</span><span class=\"p\">(</span><span class=\"n\">layers_dims</span><span class=\"p\">)</span>\n<span class=\"err\">​</span>\n    <span class=\"c1\"># Loop (gradient descent)</span>\n<span class=\"err\">​</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">num_iterations</span><span class=\"p\">):</span>\n<span class=\"err\">​</span>\n        <span class=\"c1\"># Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID.</span>\n        <span class=\"k\">if</span> <span class=\"n\">keep_prob</span> <span class=\"o\">==</span> <span class=\"mi\">1</span><span class=\"p\">:</span>\n            <span class=\"n\">a3</span><span class=\"p\">,</span> <span class=\"n\">cache</span> <span class=\"o\">=</span> <span class=\"n\">forward_propagation</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">parameters</span><span class=\"p\">)</span>\n        <span class=\"k\">elif</span> <span class=\"n\">keep_prob</span> <span class=\"o\">&lt;</span> <span class=\"mi\">1</span><span class=\"p\">:</span>\n            <span class=\"n\">a3</span><span class=\"p\">,</span> <span class=\"n\">cache</span> <span class=\"o\">=</span> <span class=\"n\">forward_propagation_with_dropout</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">keep_prob</span><span class=\"p\">)</span>\n        \n        <span class=\"c1\"># Cost function</span>\n        <span class=\"k\">if</span> <span class=\"n\">lambd</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n            <span class=\"n\">cost</span> <span class=\"o\">=</span> <span class=\"n\">compute_cost</span><span class=\"p\">(</span><span class=\"n\">a3</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">)</span>\n        <span class=\"k\">else</span><span class=\"p\">:</span>\n            <span class=\"n\">cost</span> <span class=\"o\">=</span> <span class=\"n\">compute_cost_with_regularization</span><span class=\"p\">(</span><span class=\"n\">a3</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">lambd</span><span class=\"p\">)</span>\n            \n        <span class=\"c1\"># Backward propagation.</span>\n        <span class=\"k\">assert</span><span class=\"p\">(</span><span class=\"n\">lambd</span><span class=\"o\">==</span><span class=\"mi\">0</span> <span class=\"ow\">or</span> <span class=\"n\">keep_prob</span><span class=\"o\">==</span><span class=\"mi\">1</span><span class=\"p\">)</span>    <span class=\"c1\"># it is possible to use both L2 regularization and dropout, </span>\n                                            <span class=\"c1\"># but this assignment will only explore one at a time</span>\n        <span class=\"k\">if</span> <span class=\"n\">lambd</span> <span class=\"o\">==</span> <span class=\"mi\">0</span> <span class=\"ow\">and</span> <span class=\"n\">keep_prob</span> <span class=\"o\">==</span> <span class=\"mi\">1</span><span class=\"p\">:</span>\n            <span class=\"n\">grads</span> <span class=\"o\">=</span> <span class=\"n\">backward_propagation</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">cache</span><span class=\"p\">)</span>\n        <span class=\"k\">elif</span> <span class=\"n\">lambd</span> <span class=\"o\">!=</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n            <span class=\"n\">grads</span> <span class=\"o\">=</span> <span class=\"n\">backward_propagation_with_regularization</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">cache</span><span class=\"p\">,</span> <span class=\"n\">lambd</span><span class=\"p\">)</span>\n        <span class=\"k\">elif</span> <span class=\"n\">keep_prob</span> <span class=\"o\">&lt;</span> <span class=\"mi\">1</span><span class=\"p\">:</span>\n            <span class=\"n\">grads</span> <span class=\"o\">=</span> <span class=\"n\">backward_propagation_with_dropout</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">cache</span><span class=\"p\">,</span> <span class=\"n\">keep_prob</span><span class=\"p\">)</span>\n        \n        <span class=\"c1\"># Update parameters.</span>\n        <span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"n\">update_parameters</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">grads</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span><span class=\"p\">)</span>\n        \n        <span class=\"c1\"># Print the loss every 10000 iterations</span>\n        <span class=\"k\">if</span> <span class=\"n\">print_cost</span> <span class=\"ow\">and</span> <span class=\"n\">i</span> <span class=\"o\">%</span> <span class=\"mi\">10000</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n            <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s2\">&#34;Cost after iteration {}: {}&#34;</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">cost</span><span class=\"p\">))</span>\n        <span class=\"k\">if</span> <span class=\"n\">print_cost</span> <span class=\"ow\">and</span> <span class=\"n\">i</span> <span class=\"o\">%</span> <span class=\"mi\">1000</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n            <span class=\"n\">costs</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">cost</span><span class=\"p\">)</span>\n    \n    <span class=\"c1\"># plot the cost</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">costs</span><span class=\"p\">)</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">ylabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;cost&#39;</span><span class=\"p\">)</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">xlabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;iterations (x1,000)&#39;</span><span class=\"p\">)</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">title</span><span class=\"p\">(</span><span class=\"s2\">&#34;Learning rate =&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">learning_rate</span><span class=\"p\">))</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">parameters</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>没有使用正则化时，效果：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-5da5b42cf43ea34651ee4d388491cf49_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"458\" data-rawheight=\"278\" class=\"origin_image zh-lightbox-thumb\" width=\"458\" data-original=\"https://pic2.zhimg.com/v2-5da5b42cf43ea34651ee4d388491cf49_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;458&#39; height=&#39;278&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"458\" data-rawheight=\"278\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"458\" data-original=\"https://pic2.zhimg.com/v2-5da5b42cf43ea34651ee4d388491cf49_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-5da5b42cf43ea34651ee4d388491cf49_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>L2 正则</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p><b>计算代价函数</b></p><p><img src=\"https://www.zhihu.com/equation?tex=J_%7Bregularized%7D+%3D+%5Csmall+%5Cunderbrace%7B-%5Cfrac%7B1%7D%7Bm%7D+%5Csum%5Climits%7Bi+%3D+1%7D%5E%7Bm%7D+%5Clarge%7B%28%7D%5Csmall+y%5E%7B%28i%29%7D%5Clog%5Cleft%28a%5E%7B%5BL%5D+%28i%29%7D%5Cright%29+%2B+%281-y%5E%7B%28i%29%7D%29%5Clog%5Cleft%281-+a%5E%7B%5BL%5D+%28i%29%7D%5Cright%29+%5Clarge%7B%29%7D+%7D_%5Ctext%7Bcross-entropy+cost%7D+%2B+%5Cunderbrace%7B%5Cfrac%7B1%7D%7Bm%7D+%5Cfrac%7B%5Clambda%7D%7B2%7D+%5Csum%5Climits_l%5Csum%5Climits_k%5Csum%5Climits_j+W_%7Bk%2Cj%7D%5E%7B%5Bl%5D2%7D+%7D_%5Ctext%7BL2+regularization+cost%7D+\" alt=\"J_{regularized} = \\small \\underbrace{-\\frac{1}{m} \\sum\\limits{i = 1}^{m} \\large{(}\\small y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L] (i)}\\right) \\large{)} }_\\text{cross-entropy cost} + \\underbrace{\\frac{1}{m} \\frac{\\lambda}{2} \\sum\\limits_l\\sum\\limits_k\\sum\\limits_j W_{k,j}^{[l]2} }_\\text{L2 regularization cost} \" eeimg=\"1\"/> </p><p class=\"ztext-empty-paragraph\"><br/></p><p>公式已经给了，只要加上后面那一项就可以了</p><p>使用<code>np.sum(np.square(Wl))</code>来计算 <img src=\"https://www.zhihu.com/equation?tex=%5Csum%5Climits_k%5Csum%5Climits_j+W_%7Bk%2Cj%7D%5E%7B%5Bl%5D2%7D\" alt=\"\\sum\\limits_k\\sum\\limits_j W_{k,j}^{[l]2}\" eeimg=\"1\"/> </p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: compute_cost_with_regularization</span>\n<span class=\"err\">​</span>\n<span class=\"k\">def</span> <span class=\"nf\">compute_cost_with_regularization</span><span class=\"p\">(</span><span class=\"n\">A3</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">lambd</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Implement the cost function with L2 regularization. See formula (2) above.\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    A3 -- post-activation, output of forward propagation, of shape (output size, number of examples)\n</span><span class=\"s2\">    Y -- &#34;true&#34; labels vector, of shape (output size, number of examples)\n</span><span class=\"s2\">    parameters -- python dictionary containing parameters of the model\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    cost - value of the regularized loss function (formula (2))\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    <span class=\"n\">m</span> <span class=\"o\">=</span> <span class=\"n\">Y</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n    <span class=\"n\">W1</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;W1&#34;</span><span class=\"p\">]</span>\n    <span class=\"n\">W2</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;W2&#34;</span><span class=\"p\">]</span>\n    <span class=\"n\">W3</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;W3&#34;</span><span class=\"p\">]</span>\n    \n    <span class=\"n\">cross_entropy_cost</span> <span class=\"o\">=</span> <span class=\"n\">compute_cost</span><span class=\"p\">(</span><span class=\"n\">A3</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">)</span> <span class=\"c1\"># This gives you the cross-entropy part of the cost</span>\n    \n    <span class=\"c1\">### START CODE HERE ### (approx. 1 line)</span>\n    <span class=\"n\">L2_regularization_cost</span> <span class=\"o\">=</span> <span class=\"n\">lambd</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"mi\">2</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">square</span><span class=\"p\">(</span><span class=\"n\">W1</span><span class=\"p\">))</span> <span class=\"o\">+</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">square</span><span class=\"p\">(</span><span class=\"n\">W2</span><span class=\"p\">))</span> <span class=\"o\">+</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">square</span><span class=\"p\">(</span><span class=\"n\">W3</span><span class=\"p\">)))</span>\n    <span class=\"c1\">### END CODER HERE ###</span>\n    \n    <span class=\"n\">cost</span> <span class=\"o\">=</span> <span class=\"n\">cross_entropy_cost</span> <span class=\"o\">+</span> <span class=\"n\">L2_regularization_cost</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">cost</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><b>计算反向传播函数</b></p><p>在 <img src=\"https://www.zhihu.com/equation?tex=dW\" alt=\"dW\" eeimg=\"1\"/> 上加上了正则项 <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Clambda%7D%7Bm%7D+W\" alt=\"\\frac{\\lambda}{m} W\" eeimg=\"1\"/> </p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: backward_propagation_with_regularization</span>\n<span class=\"err\">​</span>\n<span class=\"k\">def</span> <span class=\"nf\">backward_propagation_with_regularization</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">cache</span><span class=\"p\">,</span> <span class=\"n\">lambd</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Implements the backward propagation of our baseline model to which we added an L2 regularization.\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    X -- input dataset, of shape (input size, number of examples)\n</span><span class=\"s2\">    Y -- &#34;true&#34; labels vector, of shape (output size, number of examples)\n</span><span class=\"s2\">    cache -- cache output from forward_propagation()\n</span><span class=\"s2\">    lambd -- regularization hyperparameter, scalar\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"n\">m</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n    <span class=\"p\">(</span><span class=\"n\">Z1</span><span class=\"p\">,</span> <span class=\"n\">A1</span><span class=\"p\">,</span> <span class=\"n\">W1</span><span class=\"p\">,</span> <span class=\"n\">b1</span><span class=\"p\">,</span> <span class=\"n\">Z2</span><span class=\"p\">,</span> <span class=\"n\">A2</span><span class=\"p\">,</span> <span class=\"n\">W2</span><span class=\"p\">,</span> <span class=\"n\">b2</span><span class=\"p\">,</span> <span class=\"n\">Z3</span><span class=\"p\">,</span> <span class=\"n\">A3</span><span class=\"p\">,</span> <span class=\"n\">W3</span><span class=\"p\">,</span> <span class=\"n\">b3</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">cache</span>\n    \n    <span class=\"n\">dZ3</span> <span class=\"o\">=</span> <span class=\"n\">A3</span> <span class=\"o\">-</span> <span class=\"n\">Y</span>\n    \n    <span class=\"c1\">### START CODE HERE ### (approx. 1 line)</span>\n    <span class=\"n\">dW3</span> <span class=\"o\">=</span> <span class=\"mf\">1.</span><span class=\"o\">/</span><span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">dZ3</span><span class=\"p\">,</span> <span class=\"n\">A2</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">lambd</span> <span class=\"o\">/</span> <span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">W3</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    <span class=\"n\">db3</span> <span class=\"o\">=</span> <span class=\"mf\">1.</span><span class=\"o\">/</span><span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">dZ3</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">keepdims</span> <span class=\"o\">=</span> <span class=\"bp\">True</span><span class=\"p\">)</span>\n    \n    <span class=\"n\">dA2</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">W3</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">,</span> <span class=\"n\">dZ3</span><span class=\"p\">)</span>\n    <span class=\"n\">dZ2</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">multiply</span><span class=\"p\">(</span><span class=\"n\">dA2</span><span class=\"p\">,</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">int64</span><span class=\"p\">(</span><span class=\"n\">A2</span> <span class=\"o\">&gt;</span> <span class=\"mi\">0</span><span class=\"p\">))</span>\n    <span class=\"c1\">### START CODE HERE ### (approx. 1 line)</span>\n    <span class=\"n\">dW2</span> <span class=\"o\">=</span> <span class=\"mf\">1.</span><span class=\"o\">/</span><span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">dZ2</span><span class=\"p\">,</span> <span class=\"n\">A1</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">lambd</span> <span class=\"o\">/</span> <span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">W2</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    <span class=\"n\">db2</span> <span class=\"o\">=</span> <span class=\"mf\">1.</span><span class=\"o\">/</span><span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">dZ2</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">keepdims</span> <span class=\"o\">=</span> <span class=\"bp\">True</span><span class=\"p\">)</span>\n    \n    <span class=\"n\">dA1</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">W2</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">,</span> <span class=\"n\">dZ2</span><span class=\"p\">)</span>\n    <span class=\"n\">dZ1</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">multiply</span><span class=\"p\">(</span><span class=\"n\">dA1</span><span class=\"p\">,</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">int64</span><span class=\"p\">(</span><span class=\"n\">A1</span> <span class=\"o\">&gt;</span> <span class=\"mi\">0</span><span class=\"p\">))</span>\n    <span class=\"c1\">### START CODE HERE ### (approx. 1 line)</span>\n    <span class=\"n\">dW1</span> <span class=\"o\">=</span> <span class=\"mf\">1.</span><span class=\"o\">/</span><span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">dZ1</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">lambd</span> <span class=\"o\">/</span> <span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">W1</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    <span class=\"n\">db1</span> <span class=\"o\">=</span> <span class=\"mf\">1.</span><span class=\"o\">/</span><span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">dZ1</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">keepdims</span> <span class=\"o\">=</span> <span class=\"bp\">True</span><span class=\"p\">)</span>\n    \n    <span class=\"n\">gradients</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">&#34;dZ3&#34;</span><span class=\"p\">:</span> <span class=\"n\">dZ3</span><span class=\"p\">,</span> <span class=\"s2\">&#34;dW3&#34;</span><span class=\"p\">:</span> <span class=\"n\">dW3</span><span class=\"p\">,</span> <span class=\"s2\">&#34;db3&#34;</span><span class=\"p\">:</span> <span class=\"n\">db3</span><span class=\"p\">,</span><span class=\"s2\">&#34;dA2&#34;</span><span class=\"p\">:</span> <span class=\"n\">dA2</span><span class=\"p\">,</span>\n                 <span class=\"s2\">&#34;dZ2&#34;</span><span class=\"p\">:</span> <span class=\"n\">dZ2</span><span class=\"p\">,</span> <span class=\"s2\">&#34;dW2&#34;</span><span class=\"p\">:</span> <span class=\"n\">dW2</span><span class=\"p\">,</span> <span class=\"s2\">&#34;db2&#34;</span><span class=\"p\">:</span> <span class=\"n\">db2</span><span class=\"p\">,</span> <span class=\"s2\">&#34;dA1&#34;</span><span class=\"p\">:</span> <span class=\"n\">dA1</span><span class=\"p\">,</span> \n                 <span class=\"s2\">&#34;dZ1&#34;</span><span class=\"p\">:</span> <span class=\"n\">dZ1</span><span class=\"p\">,</span> <span class=\"s2\">&#34;dW1&#34;</span><span class=\"p\">:</span> <span class=\"n\">dW1</span><span class=\"p\">,</span> <span class=\"s2\">&#34;db1&#34;</span><span class=\"p\">:</span> <span class=\"n\">db1</span><span class=\"p\">}</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">gradients</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>加上L2正则项后，效果很明显：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-f00ebbee0f00069740d67a2bc90518ca_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"458\" data-rawheight=\"278\" class=\"origin_image zh-lightbox-thumb\" width=\"458\" data-original=\"https://pic3.zhimg.com/v2-f00ebbee0f00069740d67a2bc90518ca_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;458&#39; height=&#39;278&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"458\" data-rawheight=\"278\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"458\" data-original=\"https://pic3.zhimg.com/v2-f00ebbee0f00069740d67a2bc90518ca_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-f00ebbee0f00069740d67a2bc90518ca_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>dropout</b></h2><p>在每一次迭代中，都随机删除一定概率的neurons。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>1. Forward propagation with dropout</b></p><p>分4步:</p><ol><li>每一层的 <img src=\"https://www.zhihu.com/equation?tex=d%5E%7B%5Bl%5D%7D\" alt=\"d^{[l]}\" eeimg=\"1\"/> 对应每一层的 <img src=\"https://www.zhihu.com/equation?tex=a%5E%7B%5Bl%5D%7D\" alt=\"a^{[l]}\" eeimg=\"1\"/> ,因为有m个样本，所以就有 <img src=\"https://www.zhihu.com/equation?tex=D%5E%7B%5B1%5D%7D+%3D+%5Bd%5E%7B%5B1%5D%281%29%7D+d%5E%7B%5B1%5D%282%29%7D+...+d%5E%7B%5B1%5D%28m%29%7D%5D+\" alt=\"D^{[1]} = [d^{[1](1)} d^{[1](2)} ... d^{[1](m)}] \" eeimg=\"1\"/> of the same dimension as <img src=\"https://www.zhihu.com/equation?tex=A%5E%7B%5B1%5D%7D\" alt=\"A^{[1]}\" eeimg=\"1\"/> .使用np.random.rand(n,m)</li><li>将 <img src=\"https://www.zhihu.com/equation?tex=D%5E%7B%5Bl%5D%7D\" alt=\"D^{[l]}\" eeimg=\"1\"/> 布尔化， <img src=\"https://www.zhihu.com/equation?tex=+%3C+keepprob\" alt=\" &lt; keepprob\" eeimg=\"1\"/> 分为 1和0</li><li>Set <img src=\"https://www.zhihu.com/equation?tex=A%5E%7B%5B1%5D%7D\" alt=\"A^{[1]}\" eeimg=\"1\"/> to <img src=\"https://www.zhihu.com/equation?tex=A%5E%7B%5B1%5D%7D+%2A+D%5E%7B%5B1%5D%7D\" alt=\"A^{[1]} * D^{[1]}\" eeimg=\"1\"/> .</li><li>Divide <img src=\"https://www.zhihu.com/equation?tex=A%5E%7B%5B1%5D%7D\" alt=\"A^{[1]}\" eeimg=\"1\"/> by <code>keep_prob</code>.</li></ol><p class=\"ztext-empty-paragraph\"><br/></p><p>记得用cache把每一层的D都记录下来</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: forward_propagation_with_dropout</span>\n<span class=\"err\">​</span>\n<span class=\"k\">def</span> <span class=\"nf\">forward_propagation_with_dropout</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">keep_prob</span> <span class=\"o\">=</span> <span class=\"mf\">0.5</span><span class=\"p\">):</span>\n  \n    <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">seed</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n    \n    <span class=\"c1\"># retrieve parameters</span>\n    <span class=\"n\">W1</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;W1&#34;</span><span class=\"p\">]</span>\n    <span class=\"n\">b1</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;b1&#34;</span><span class=\"p\">]</span>\n    <span class=\"n\">W2</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;W2&#34;</span><span class=\"p\">]</span>\n    <span class=\"n\">b2</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;b2&#34;</span><span class=\"p\">]</span>\n    <span class=\"n\">W3</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;W3&#34;</span><span class=\"p\">]</span>\n    <span class=\"n\">b3</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;b3&#34;</span><span class=\"p\">]</span>\n    \n    <span class=\"c1\"># LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID</span>\n    <span class=\"n\">Z1</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">W1</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">b1</span>\n    <span class=\"n\">A1</span> <span class=\"o\">=</span> <span class=\"n\">relu</span><span class=\"p\">(</span><span class=\"n\">Z1</span><span class=\"p\">)</span>\n    <span class=\"c1\">### START CODE HERE ### (approx. 4 lines)         # Steps 1-4 below correspond to the Steps 1-4 described above. </span>\n    <span class=\"n\">D1</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"n\">A1</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">A1</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">])</span>                                         <span class=\"c1\"># Step 1: initialize matrix D1 = np.random.rand(..., ...)</span>\n    <span class=\"n\">D1</span> <span class=\"o\">=</span> <span class=\"n\">D1</span> <span class=\"o\">&lt;</span> <span class=\"n\">keep_prob</span>                                         <span class=\"c1\"># Step 2: convert entries of D1 to 0 or 1 (using keep_prob as the threshold)</span>\n    <span class=\"n\">A1</span> <span class=\"o\">=</span> <span class=\"n\">A1</span> <span class=\"o\">*</span> <span class=\"n\">D1</span>                                         <span class=\"c1\"># Step 3: shut down some neurons of A1</span>\n    <span class=\"n\">A1</span> <span class=\"o\">=</span> <span class=\"n\">A1</span> <span class=\"o\">/</span> <span class=\"n\">keep_prob</span>                                         <span class=\"c1\"># Step 4: scale the value of neurons that haven&#39;t been shut down</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    <span class=\"n\">Z2</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">W2</span><span class=\"p\">,</span> <span class=\"n\">A1</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">b2</span>\n    <span class=\"n\">A2</span> <span class=\"o\">=</span> <span class=\"n\">relu</span><span class=\"p\">(</span><span class=\"n\">Z2</span><span class=\"p\">)</span>\n    <span class=\"c1\">### START CODE HERE ### (approx. 4 lines)</span>\n    <span class=\"n\">D2</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"n\">A2</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">A2</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">])</span>                                              <span class=\"c1\"># Step 1: initialize matrix D2 = np.random.rand(..., ...)</span>\n    <span class=\"n\">D2</span> <span class=\"o\">=</span> <span class=\"n\">D2</span> <span class=\"o\">&lt;</span> <span class=\"n\">keep_prob</span>                                         <span class=\"c1\"># Step 2: convert entries of D2 to 0 or 1 (using keep_prob as the threshold)</span>\n    <span class=\"n\">A2</span> <span class=\"o\">=</span> <span class=\"n\">A2</span> <span class=\"o\">*</span> <span class=\"n\">D2</span>                                               <span class=\"c1\"># Step 3: shut down some neurons of A2</span>\n    <span class=\"n\">A2</span> <span class=\"o\">=</span> <span class=\"n\">A2</span> <span class=\"o\">/</span> <span class=\"n\">keep_prob</span>                                         <span class=\"c1\"># Step 4: scale the value of neurons that haven&#39;t been shut down</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    <span class=\"n\">Z3</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">W3</span><span class=\"p\">,</span> <span class=\"n\">A2</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">b3</span>\n    <span class=\"n\">A3</span> <span class=\"o\">=</span> <span class=\"n\">sigmoid</span><span class=\"p\">(</span><span class=\"n\">Z3</span><span class=\"p\">)</span>\n    \n    <span class=\"n\">cache</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">Z1</span><span class=\"p\">,</span> <span class=\"n\">D1</span><span class=\"p\">,</span> <span class=\"n\">A1</span><span class=\"p\">,</span> <span class=\"n\">W1</span><span class=\"p\">,</span> <span class=\"n\">b1</span><span class=\"p\">,</span> <span class=\"n\">Z2</span><span class=\"p\">,</span> <span class=\"n\">D2</span><span class=\"p\">,</span> <span class=\"n\">A2</span><span class=\"p\">,</span> <span class=\"n\">W2</span><span class=\"p\">,</span> <span class=\"n\">b2</span><span class=\"p\">,</span> <span class=\"n\">Z3</span><span class=\"p\">,</span> <span class=\"n\">A3</span><span class=\"p\">,</span> <span class=\"n\">W3</span><span class=\"p\">,</span> <span class=\"n\">b3</span><span class=\"p\">)</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">A3</span><span class=\"p\">,</span> <span class=\"n\">cache</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><b>2. Backward propagation with dropout</b></p><ol><li>reapplying the same mask <img src=\"https://www.zhihu.com/equation?tex=D%5E%7B%5B1%5D%7D\" alt=\"D^{[1]}\" eeimg=\"1\"/> to <code>dA1</code>. </li><li>divide <code>dA1</code> by <code>keep_prob</code></li></ol><p>反向传播的时候，让之前的删除的neurons依旧归0，然后也要除以keepprob，因为<code>dA = np.dot(W.T, dZ)</code>，并没有重复除以过系数。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: backward_propagation_with_dropout</span>\n<span class=\"err\">​</span>\n<span class=\"k\">def</span> <span class=\"nf\">backward_propagation_with_dropout</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">cache</span><span class=\"p\">,</span> <span class=\"n\">keep_prob</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Implements the backward propagation of our baseline model to which we added dropout.\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    X -- input dataset, of shape (2, number of examples)\n</span><span class=\"s2\">    Y -- &#34;true&#34; labels vector, of shape (output size, number of examples)\n</span><span class=\"s2\">    cache -- cache output from forward_propagation_with_dropout()\n</span><span class=\"s2\">    keep_prob - probability of keeping a neuron active during drop-out, scalar\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    gradients -- A dictionary with the gradients with respect to each parameter, activation and pre-activation variables\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"n\">m</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n    <span class=\"p\">(</span><span class=\"n\">Z1</span><span class=\"p\">,</span> <span class=\"n\">D1</span><span class=\"p\">,</span> <span class=\"n\">A1</span><span class=\"p\">,</span> <span class=\"n\">W1</span><span class=\"p\">,</span> <span class=\"n\">b1</span><span class=\"p\">,</span> <span class=\"n\">Z2</span><span class=\"p\">,</span> <span class=\"n\">D2</span><span class=\"p\">,</span> <span class=\"n\">A2</span><span class=\"p\">,</span> <span class=\"n\">W2</span><span class=\"p\">,</span> <span class=\"n\">b2</span><span class=\"p\">,</span> <span class=\"n\">Z3</span><span class=\"p\">,</span> <span class=\"n\">A3</span><span class=\"p\">,</span> <span class=\"n\">W3</span><span class=\"p\">,</span> <span class=\"n\">b3</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">cache</span>\n    \n    <span class=\"n\">dZ3</span> <span class=\"o\">=</span> <span class=\"n\">A3</span> <span class=\"o\">-</span> <span class=\"n\">Y</span>\n    <span class=\"n\">dW3</span> <span class=\"o\">=</span> <span class=\"mf\">1.</span><span class=\"o\">/</span><span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">dZ3</span><span class=\"p\">,</span> <span class=\"n\">A2</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">)</span>\n    <span class=\"n\">db3</span> <span class=\"o\">=</span> <span class=\"mf\">1.</span><span class=\"o\">/</span><span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">dZ3</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">keepdims</span> <span class=\"o\">=</span> <span class=\"bp\">True</span><span class=\"p\">)</span>\n    <span class=\"n\">dA2</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">W3</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">,</span> <span class=\"n\">dZ3</span><span class=\"p\">)</span>\n    <span class=\"c1\">### START CODE HERE ### (≈ 2 lines of code)</span>\n    <span class=\"n\">dA2</span> <span class=\"o\">=</span> <span class=\"n\">dA2</span> <span class=\"o\">*</span> <span class=\"n\">D2</span>              <span class=\"c1\"># Step 1: Apply mask D2 to shut down the same neurons as during the forward propagation</span>\n    <span class=\"n\">dA2</span> <span class=\"o\">=</span> <span class=\"n\">dA2</span> <span class=\"o\">/</span> <span class=\"n\">keep_prob</span>             <span class=\"c1\"># Step 2: Scale the value of neurons that haven&#39;t been shut down</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    <span class=\"n\">dZ2</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">multiply</span><span class=\"p\">(</span><span class=\"n\">dA2</span><span class=\"p\">,</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">int64</span><span class=\"p\">(</span><span class=\"n\">A2</span> <span class=\"o\">&gt;</span> <span class=\"mi\">0</span><span class=\"p\">))</span>\n    <span class=\"n\">dW2</span> <span class=\"o\">=</span> <span class=\"mf\">1.</span><span class=\"o\">/</span><span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">dZ2</span><span class=\"p\">,</span> <span class=\"n\">A1</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">)</span>\n    <span class=\"n\">db2</span> <span class=\"o\">=</span> <span class=\"mf\">1.</span><span class=\"o\">/</span><span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">dZ2</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">keepdims</span> <span class=\"o\">=</span> <span class=\"bp\">True</span><span class=\"p\">)</span>\n    \n    <span class=\"n\">dA1</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">W2</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">,</span> <span class=\"n\">dZ2</span><span class=\"p\">)</span>\n    <span class=\"c1\">### START CODE HERE ### (≈ 2 lines of code)</span>\n    <span class=\"n\">dA1</span> <span class=\"o\">=</span> <span class=\"n\">dA1</span> <span class=\"o\">*</span> <span class=\"n\">D1</span>              <span class=\"c1\"># Step 1: Apply mask D1 to shut down the same neurons as during the forward propagation</span>\n    <span class=\"n\">dA1</span> <span class=\"o\">=</span> <span class=\"n\">dA1</span> <span class=\"o\">/</span> <span class=\"n\">keep_prob</span>              <span class=\"c1\"># Step 2: Scale the value of neurons that haven&#39;t been shut down</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    <span class=\"n\">dZ1</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">multiply</span><span class=\"p\">(</span><span class=\"n\">dA1</span><span class=\"p\">,</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">int64</span><span class=\"p\">(</span><span class=\"n\">A1</span> <span class=\"o\">&gt;</span> <span class=\"mi\">0</span><span class=\"p\">))</span>\n    <span class=\"n\">dW1</span> <span class=\"o\">=</span> <span class=\"mf\">1.</span><span class=\"o\">/</span><span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">dZ1</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">)</span>\n    <span class=\"n\">db1</span> <span class=\"o\">=</span> <span class=\"mf\">1.</span><span class=\"o\">/</span><span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">dZ1</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">keepdims</span> <span class=\"o\">=</span> <span class=\"bp\">True</span><span class=\"p\">)</span>\n    \n    <span class=\"n\">gradients</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">&#34;dZ3&#34;</span><span class=\"p\">:</span> <span class=\"n\">dZ3</span><span class=\"p\">,</span> <span class=\"s2\">&#34;dW3&#34;</span><span class=\"p\">:</span> <span class=\"n\">dW3</span><span class=\"p\">,</span> <span class=\"s2\">&#34;db3&#34;</span><span class=\"p\">:</span> <span class=\"n\">db3</span><span class=\"p\">,</span><span class=\"s2\">&#34;dA2&#34;</span><span class=\"p\">:</span> <span class=\"n\">dA2</span><span class=\"p\">,</span>\n                 <span class=\"s2\">&#34;dZ2&#34;</span><span class=\"p\">:</span> <span class=\"n\">dZ2</span><span class=\"p\">,</span> <span class=\"s2\">&#34;dW2&#34;</span><span class=\"p\">:</span> <span class=\"n\">dW2</span><span class=\"p\">,</span> <span class=\"s2\">&#34;db2&#34;</span><span class=\"p\">:</span> <span class=\"n\">db2</span><span class=\"p\">,</span> <span class=\"s2\">&#34;dA1&#34;</span><span class=\"p\">:</span> <span class=\"n\">dA1</span><span class=\"p\">,</span> \n                 <span class=\"s2\">&#34;dZ1&#34;</span><span class=\"p\">:</span> <span class=\"n\">dZ1</span><span class=\"p\">,</span> <span class=\"s2\">&#34;dW1&#34;</span><span class=\"p\">:</span> <span class=\"n\">dW1</span><span class=\"p\">,</span> <span class=\"s2\">&#34;db1&#34;</span><span class=\"p\">:</span> <span class=\"n\">db1</span><span class=\"p\">}</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">gradients</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>最终结果,也还不错：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-979ebcf9376cc925909b582a4431d264_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"458\" data-rawheight=\"278\" class=\"origin_image zh-lightbox-thumb\" width=\"458\" data-original=\"https://pic1.zhimg.com/v2-979ebcf9376cc925909b582a4431d264_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;458&#39; height=&#39;278&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"458\" data-rawheight=\"278\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"458\" data-original=\"https://pic1.zhimg.com/v2-979ebcf9376cc925909b582a4431d264_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-979ebcf9376cc925909b582a4431d264_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>注意：</p><ul><li>dropout也是正则化的一种</li><li>训练的时候用，测试的时候不要用</li><li>在正向传播和反向传播中都要用</li></ul><h2><b>Part3:Gradient Checking</b></h2><p>首先写了一维的checking</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: forward_propagation</span>\n<span class=\"err\">​</span>\n<span class=\"k\">def</span> <span class=\"nf\">forward_propagation</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">theta</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Implement the linear forward propagation (compute J) presented in Figure 1 (J(theta) = theta * x)\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    x -- a real-valued input\n</span><span class=\"s2\">    theta -- our parameter, a real number as well\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    J -- the value of function J, computed using the formula J(theta) = theta * x\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"c1\">### START CODE HERE ### (approx. 1 line)</span>\n    <span class=\"n\">J</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">theta</span><span class=\"p\">,</span><span class=\"n\">x</span><span class=\"p\">)</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">J</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: backward_propagation</span>\n<span class=\"err\">​</span>\n<span class=\"k\">def</span> <span class=\"nf\">backward_propagation</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">theta</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Computes the derivative of J with respect to theta (see Figure 1).\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    x -- a real-valued input\n</span><span class=\"s2\">    theta -- our parameter, a real number as well\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    dtheta -- the gradient of the cost with respect to theta\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"c1\">### START CODE HERE ### (approx. 1 line)</span>\n    <span class=\"n\">dtheta</span> <span class=\"o\">=</span> <span class=\"n\">x</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">dtheta</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>根据公式：</p><p><img src=\"https://www.zhihu.com/equation?tex=+difference+%3D+%5Cfrac+%7B%5Cmid%5Cmid+grad+-+gradapprox+%5Cmid%5Cmid_2%7D%7B%5Cmid%5Cmid+grad+%5Cmid%5Cmid_2+%2B+%5Cmid%5Cmid+gradapprox+%5Cmid%5Cmid_2%7D+\" alt=\" difference = \\frac {\\mid\\mid grad - gradapprox \\mid\\mid_2}{\\mid\\mid grad \\mid\\mid_2 + \\mid\\mid gradapprox \\mid\\mid_2} \" eeimg=\"1\"/> </p><p>步骤是：</p><ol><li><img src=\"https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%2B%7D+%3D+%5Ctheta+%2B+%5Cvarepsilon\" alt=\"\\theta^{+} = \\theta + \\varepsilon\" eeimg=\"1\"/> </li><li><img src=\"https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B-%7D+%3D+%5Ctheta+-+%5Cvarepsilon\" alt=\"\\theta^{-} = \\theta - \\varepsilon\" eeimg=\"1\"/> </li><li><img src=\"https://www.zhihu.com/equation?tex=J%5E%7B%2B%7D+%3D+J%28%5Ctheta%5E%7B%2B%7D%29\" alt=\"J^{+} = J(\\theta^{+})\" eeimg=\"1\"/> </li><li><img src=\"https://www.zhihu.com/equation?tex=J%5E%7B-%7D+%3D+J%28%5Ctheta%5E%7B-%7D%29\" alt=\"J^{-} = J(\\theta^{-})\" eeimg=\"1\"/> </li><li><img src=\"https://www.zhihu.com/equation?tex=gradapprox+%3D+%5Cfrac%7BJ%5E%7B%2B%7D+-+J%5E%7B-%7D%7D%7B2++%5Cvarepsilon%7D\" alt=\"gradapprox = \\frac{J^{+} - J^{-}}{2  \\varepsilon}\" eeimg=\"1\"/> </li></ol><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">gradient_check</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">theta</span><span class=\"p\">,</span> <span class=\"n\">epsilon</span> <span class=\"o\">=</span> <span class=\"mf\">1e-7</span><span class=\"p\">):</span>\n\n    <span class=\"c1\"># Compute gradapprox using left side of formula (1). epsilon is small enough, you don&#39;t need to worry about the limit.</span>\n    <span class=\"c1\">### START CODE HERE ### (approx. 5 lines)</span>\n    <span class=\"n\">thetaplus</span> <span class=\"o\">=</span> <span class=\"n\">theta</span> <span class=\"o\">+</span> <span class=\"n\">epsilon</span>                               <span class=\"c1\"># Step 1</span>\n    <span class=\"n\">thetaminus</span> <span class=\"o\">=</span> <span class=\"n\">theta</span> <span class=\"o\">-</span> <span class=\"n\">epsilon</span>                              <span class=\"c1\"># Step 2</span>\n    <span class=\"n\">J_plus</span> <span class=\"o\">=</span> <span class=\"n\">forward_propagation</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">thetaplus</span><span class=\"p\">)</span>                              <span class=\"c1\"># Step 3</span>\n    <span class=\"n\">J_minus</span> <span class=\"o\">=</span> <span class=\"n\">forward_propagation</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">thetaminus</span><span class=\"p\">)</span>                                <span class=\"c1\"># Step 4</span>\n    <span class=\"n\">gradapprox</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">J_plus</span> <span class=\"o\">-</span> <span class=\"n\">J_minus</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"mi\">2</span> <span class=\"o\">*</span> <span class=\"n\">epsilon</span><span class=\"p\">)</span>                              <span class=\"c1\"># Step 5</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"c1\"># Check if gradapprox is close enough to the output of backward_propagation()</span>\n    <span class=\"c1\">### START CODE HERE ### (approx. 1 line)</span>\n    <span class=\"n\">grad</span> <span class=\"o\">=</span> <span class=\"n\">backward_propagation</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">theta</span><span class=\"p\">)</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"c1\">### START CODE HERE ### (approx. 1 line)</span>\n    <span class=\"n\">numerator</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linalg</span><span class=\"o\">.</span><span class=\"n\">norm</span><span class=\"p\">(</span><span class=\"n\">grad</span> <span class=\"o\">-</span> <span class=\"n\">gradapprox</span><span class=\"p\">)</span>                               <span class=\"c1\"># Step 1&#39;</span>\n    <span class=\"n\">denominator</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linalg</span><span class=\"o\">.</span><span class=\"n\">norm</span><span class=\"p\">(</span><span class=\"n\">grad</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linalg</span><span class=\"o\">.</span><span class=\"n\">norm</span><span class=\"p\">(</span><span class=\"n\">gradapprox</span><span class=\"p\">)</span>                             <span class=\"c1\"># Step 2&#39;</span>\n    <span class=\"n\">difference</span> <span class=\"o\">=</span> <span class=\"n\">numerator</span> <span class=\"o\">/</span> <span class=\"n\">denominator</span>                              <span class=\"c1\"># Step 3&#39;</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"k\">if</span> <span class=\"n\">difference</span> <span class=\"o\">&lt;</span> <span class=\"mf\">1e-7</span><span class=\"p\">:</span>\n        <span class=\"k\">print</span> <span class=\"p\">(</span><span class=\"s2\">&#34;The gradient is correct!&#34;</span><span class=\"p\">)</span>\n    <span class=\"k\">else</span><span class=\"p\">:</span>\n        <span class=\"k\">print</span> <span class=\"p\">(</span><span class=\"s2\">&#34;The gradient is wrong!&#34;</span><span class=\"p\">)</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">difference</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>在N维的空间中，</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">forward_propagation_n</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">parameters</span><span class=\"p\">):</span>\n\n    <span class=\"c1\"># retrieve parameters</span>\n    <span class=\"n\">m</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n    <span class=\"n\">W1</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;W1&#34;</span><span class=\"p\">]</span>\n    <span class=\"n\">b1</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;b1&#34;</span><span class=\"p\">]</span>\n    <span class=\"n\">W2</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;W2&#34;</span><span class=\"p\">]</span>\n    <span class=\"n\">b2</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;b2&#34;</span><span class=\"p\">]</span>\n    <span class=\"n\">W3</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;W3&#34;</span><span class=\"p\">]</span>\n    <span class=\"n\">b3</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;b3&#34;</span><span class=\"p\">]</span>\n<span class=\"err\">​</span>\n    <span class=\"c1\"># LINEAR -&gt; RELU -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID</span>\n    <span class=\"n\">Z1</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">W1</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">b1</span>\n    <span class=\"n\">A1</span> <span class=\"o\">=</span> <span class=\"n\">relu</span><span class=\"p\">(</span><span class=\"n\">Z1</span><span class=\"p\">)</span>\n    <span class=\"n\">Z2</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">W2</span><span class=\"p\">,</span> <span class=\"n\">A1</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">b2</span>\n    <span class=\"n\">A2</span> <span class=\"o\">=</span> <span class=\"n\">relu</span><span class=\"p\">(</span><span class=\"n\">Z2</span><span class=\"p\">)</span>\n    <span class=\"n\">Z3</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">W3</span><span class=\"p\">,</span> <span class=\"n\">A2</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">b3</span>\n    <span class=\"n\">A3</span> <span class=\"o\">=</span> <span class=\"n\">sigmoid</span><span class=\"p\">(</span><span class=\"n\">Z3</span><span class=\"p\">)</span>\n<span class=\"err\">​</span>\n    <span class=\"c1\"># Cost</span>\n    <span class=\"n\">logprobs</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">multiply</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">log</span><span class=\"p\">(</span><span class=\"n\">A3</span><span class=\"p\">),</span><span class=\"n\">Y</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">multiply</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">log</span><span class=\"p\">(</span><span class=\"mi\">1</span> <span class=\"o\">-</span> <span class=\"n\">A3</span><span class=\"p\">),</span> <span class=\"mi\">1</span> <span class=\"o\">-</span> <span class=\"n\">Y</span><span class=\"p\">)</span>\n    <span class=\"n\">cost</span> <span class=\"o\">=</span> <span class=\"mf\">1.</span><span class=\"o\">/</span><span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">logprobs</span><span class=\"p\">)</span>\n    \n    <span class=\"n\">cache</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">Z1</span><span class=\"p\">,</span> <span class=\"n\">A1</span><span class=\"p\">,</span> <span class=\"n\">W1</span><span class=\"p\">,</span> <span class=\"n\">b1</span><span class=\"p\">,</span> <span class=\"n\">Z2</span><span class=\"p\">,</span> <span class=\"n\">A2</span><span class=\"p\">,</span> <span class=\"n\">W2</span><span class=\"p\">,</span> <span class=\"n\">b2</span><span class=\"p\">,</span> <span class=\"n\">Z3</span><span class=\"p\">,</span> <span class=\"n\">A3</span><span class=\"p\">,</span> <span class=\"n\">W3</span><span class=\"p\">,</span> <span class=\"n\">b3</span><span class=\"p\">)</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">cost</span><span class=\"p\">,</span> <span class=\"n\">cache</span>\n<span class=\"k\">def</span> <span class=\"nf\">backward_propagation_n</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">cache</span><span class=\"p\">):</span>\n\n    \n    <span class=\"n\">m</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n    <span class=\"p\">(</span><span class=\"n\">Z1</span><span class=\"p\">,</span> <span class=\"n\">A1</span><span class=\"p\">,</span> <span class=\"n\">W1</span><span class=\"p\">,</span> <span class=\"n\">b1</span><span class=\"p\">,</span> <span class=\"n\">Z2</span><span class=\"p\">,</span> <span class=\"n\">A2</span><span class=\"p\">,</span> <span class=\"n\">W2</span><span class=\"p\">,</span> <span class=\"n\">b2</span><span class=\"p\">,</span> <span class=\"n\">Z3</span><span class=\"p\">,</span> <span class=\"n\">A3</span><span class=\"p\">,</span> <span class=\"n\">W3</span><span class=\"p\">,</span> <span class=\"n\">b3</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">cache</span>\n    \n    <span class=\"n\">dZ3</span> <span class=\"o\">=</span> <span class=\"n\">A3</span> <span class=\"o\">-</span> <span class=\"n\">Y</span>\n    <span class=\"n\">dW3</span> <span class=\"o\">=</span> <span class=\"mf\">1.</span><span class=\"o\">/</span><span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">dZ3</span><span class=\"p\">,</span> <span class=\"n\">A2</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">)</span>\n    <span class=\"n\">db3</span> <span class=\"o\">=</span> <span class=\"mf\">1.</span><span class=\"o\">/</span><span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">dZ3</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">keepdims</span> <span class=\"o\">=</span> <span class=\"bp\">True</span><span class=\"p\">)</span>\n    \n    <span class=\"n\">dA2</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">W3</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">,</span> <span class=\"n\">dZ3</span><span class=\"p\">)</span>\n    <span class=\"n\">dZ2</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">multiply</span><span class=\"p\">(</span><span class=\"n\">dA2</span><span class=\"p\">,</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">int64</span><span class=\"p\">(</span><span class=\"n\">A2</span> <span class=\"o\">&gt;</span> <span class=\"mi\">0</span><span class=\"p\">))</span>\n    <span class=\"n\">dW2</span> <span class=\"o\">=</span> <span class=\"mf\">1.</span><span class=\"o\">/</span><span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">dZ2</span><span class=\"p\">,</span> <span class=\"n\">A1</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"mi\">2</span>\n    <span class=\"n\">db2</span> <span class=\"o\">=</span> <span class=\"mf\">1.</span><span class=\"o\">/</span><span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">dZ2</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">keepdims</span> <span class=\"o\">=</span> <span class=\"bp\">True</span><span class=\"p\">)</span>\n    \n    <span class=\"n\">dA1</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">W2</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">,</span> <span class=\"n\">dZ2</span><span class=\"p\">)</span>\n    <span class=\"n\">dZ1</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">multiply</span><span class=\"p\">(</span><span class=\"n\">dA1</span><span class=\"p\">,</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">int64</span><span class=\"p\">(</span><span class=\"n\">A1</span> <span class=\"o\">&gt;</span> <span class=\"mi\">0</span><span class=\"p\">))</span>\n    <span class=\"n\">dW1</span> <span class=\"o\">=</span> <span class=\"mf\">1.</span><span class=\"o\">/</span><span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">dZ1</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">)</span>\n    <span class=\"n\">db1</span> <span class=\"o\">=</span> <span class=\"mf\">4.</span><span class=\"o\">/</span><span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">dZ1</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">keepdims</span> <span class=\"o\">=</span> <span class=\"bp\">True</span><span class=\"p\">)</span>\n    \n    <span class=\"n\">gradients</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">&#34;dZ3&#34;</span><span class=\"p\">:</span> <span class=\"n\">dZ3</span><span class=\"p\">,</span> <span class=\"s2\">&#34;dW3&#34;</span><span class=\"p\">:</span> <span class=\"n\">dW3</span><span class=\"p\">,</span> <span class=\"s2\">&#34;db3&#34;</span><span class=\"p\">:</span> <span class=\"n\">db3</span><span class=\"p\">,</span>\n                 <span class=\"s2\">&#34;dA2&#34;</span><span class=\"p\">:</span> <span class=\"n\">dA2</span><span class=\"p\">,</span> <span class=\"s2\">&#34;dZ2&#34;</span><span class=\"p\">:</span> <span class=\"n\">dZ2</span><span class=\"p\">,</span> <span class=\"s2\">&#34;dW2&#34;</span><span class=\"p\">:</span> <span class=\"n\">dW2</span><span class=\"p\">,</span> <span class=\"s2\">&#34;db2&#34;</span><span class=\"p\">:</span> <span class=\"n\">db2</span><span class=\"p\">,</span>\n                 <span class=\"s2\">&#34;dA1&#34;</span><span class=\"p\">:</span> <span class=\"n\">dA1</span><span class=\"p\">,</span> <span class=\"s2\">&#34;dZ1&#34;</span><span class=\"p\">:</span> <span class=\"n\">dZ1</span><span class=\"p\">,</span> <span class=\"s2\">&#34;dW1&#34;</span><span class=\"p\">:</span> <span class=\"n\">dW1</span><span class=\"p\">,</span> <span class=\"s2\">&#34;db1&#34;</span><span class=\"p\">:</span> <span class=\"n\">db1</span><span class=\"p\">}</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">gradients</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>这个时候，给了两个函数，可以在字典和向量结构相互转换，也就是要计算$\\theta^{+}$时，把字典转为向量会比较好计算。</p><div class=\"highlight\"><pre><code class=\"language-text\">dictionary_to_vector()\nvector_to_dictionary()</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-2226d0e9c32d65465ef44ae656e46aea_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1254\" data-rawheight=\"638\" class=\"origin_image zh-lightbox-thumb\" width=\"1254\" data-original=\"https://pic3.zhimg.com/v2-2226d0e9c32d65465ef44ae656e46aea_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1254&#39; height=&#39;638&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1254\" data-rawheight=\"638\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1254\" data-original=\"https://pic3.zhimg.com/v2-2226d0e9c32d65465ef44ae656e46aea_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-2226d0e9c32d65465ef44ae656e46aea_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>J_plus[i]就是向量中的每一个元素，也就是W,b展开之后的每一项元素</p><ul><li>To compute <code>J_plus[i]</code>:<br/></li></ul><ol><li>Set <img src=\"https://www.zhihu.com/equation?tex=+%5Ctheta%5E%7B%2B%7D\" alt=\" \\theta^{+}\" eeimg=\"1\"/> to <code>np.copy(parameters_values)</code></li><li>Set <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%2B%7D_i\" alt=\"\\theta^{+}_i\" eeimg=\"1\"/><i> to </i><img src=\"https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%2B%7D_i+%2B+%5Cvarepsilon\" alt=\"\\theta^{+}_i + \\varepsilon\" eeimg=\"1\"/> </li><li>Calculate <img src=\"https://www.zhihu.com/equation?tex=J%5E%7B%2B%7D_i\" alt=\"J^{+}_i\" eeimg=\"1\"/> using to <code>forward_propagation_n(x, y, vector_to_dictionary(</code> <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%2B%7D\" alt=\"\\theta^{+}\" eeimg=\"1\"/> <code>))</code>.     </li></ol><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>To compute <code>J_minus[i]</code>: do the same thing with <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B-%7D\" alt=\"\\theta^{-}\" eeimg=\"1\"/> </li><li>Compute <img src=\"https://www.zhihu.com/equation?tex=gradapprox%5Bi%5D+%3D+%5Cfrac%7BJ%5E%7B%2B%7D_i+-+J%5E%7B-%7D_i%7D%7B2+%5Cvarepsilon%7D\" alt=\"gradapprox[i] = \\frac{J^{+}_i - J^{-}_i}{2 \\varepsilon}\" eeimg=\"1\"/> </li></ul><p>代码如下，记住 thetaplus是一个(n,1)的向量，循环计算每一个参数的gradapprox，再和原本的grad比较：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: gradient_check_n</span>\n<span class=\"err\">​</span>\n<span class=\"k\">def</span> <span class=\"nf\">gradient_check_n</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">gradients</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">epsilon</span> <span class=\"o\">=</span> <span class=\"mf\">1e-7</span><span class=\"p\">):</span>\n\n    <span class=\"c1\"># Set-up variables</span>\n    <span class=\"n\">parameters_values</span><span class=\"p\">,</span> <span class=\"n\">_</span> <span class=\"o\">=</span> <span class=\"n\">dictionary_to_vector</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">)</span>\n    <span class=\"n\">grad</span> <span class=\"o\">=</span> <span class=\"n\">gradients_to_vector</span><span class=\"p\">(</span><span class=\"n\">gradients</span><span class=\"p\">)</span>\n    <span class=\"n\">num_parameters</span> <span class=\"o\">=</span> <span class=\"n\">parameters_values</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n    <span class=\"n\">J_plus</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">num_parameters</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n    <span class=\"n\">J_minus</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">num_parameters</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n    <span class=\"n\">gradapprox</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">num_parameters</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n    \n    <span class=\"c1\"># Compute gradapprox</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">num_parameters</span><span class=\"p\">):</span>\n        \n        <span class=\"c1\"># Compute J_plus[i]. Inputs: &#34;parameters_values, epsilon&#34;. Output = &#34;J_plus[i]&#34;.</span>\n        <span class=\"c1\"># &#34;_&#34; is used because the function you have to outputs two parameters but we only care about the first one</span>\n        <span class=\"c1\">### START CODE HERE ### (approx. 3 lines)</span>\n        <span class=\"n\">thetaplus</span> <span class=\"o\">=</span>  <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">copy</span><span class=\"p\">(</span><span class=\"n\">parameters_values</span><span class=\"p\">)</span>                                      <span class=\"c1\"># Step 1</span>\n        <span class=\"n\">thetaplus</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">][</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">thetaplus</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">][</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"n\">epsilon</span>                                <span class=\"c1\"># Step 2</span>\n        <span class=\"n\">J_plus</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">],</span> <span class=\"n\">_</span> <span class=\"o\">=</span> <span class=\"n\">forward_propagation_n</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">vector_to_dictionary</span><span class=\"p\">(</span><span class=\"n\">thetaplus</span><span class=\"p\">))</span>                                   <span class=\"c1\"># Step 3</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n        \n        <span class=\"c1\"># Compute J_minus[i]. Inputs: &#34;parameters_values, epsilon&#34;. Output = &#34;J_minus[i]&#34;.</span>\n        <span class=\"c1\">### START CODE HERE ### (approx. 3 lines)</span>\n        <span class=\"n\">thetaminus</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">copy</span><span class=\"p\">(</span><span class=\"n\">parameters_values</span><span class=\"p\">)</span>                                        <span class=\"c1\"># Step 1</span>\n        <span class=\"n\">thetaminus</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">][</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">thetaminus</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">][</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">-</span> <span class=\"n\">epsilon</span>                               <span class=\"c1\"># Step 2        </span>\n        <span class=\"n\">J_minus</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">],</span> <span class=\"n\">_</span> <span class=\"o\">=</span> <span class=\"n\">forward_propagation_n</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">vector_to_dictionary</span><span class=\"p\">(</span><span class=\"n\">thetaminus</span><span class=\"p\">))</span>                                  <span class=\"c1\"># Step 3</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n        \n        <span class=\"c1\"># Compute gradapprox[i]</span>\n        <span class=\"c1\">### START CODE HERE ### (approx. 1 line)</span>\n        <span class=\"n\">gradapprox</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">J_plus</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">]</span> <span class=\"o\">-</span> <span class=\"n\">J_minus</span><span class=\"p\">[</span><span class=\"n\">i</span><span class=\"p\">])</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"mi\">2</span> <span class=\"o\">*</span> <span class=\"n\">epsilon</span><span class=\"p\">)</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"c1\"># Compare gradapprox to backward propagation gradients by computing difference.</span>\n    <span class=\"c1\">### START CODE HERE ### (approx. 1 line)</span>\n    <span class=\"n\">numerator</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linalg</span><span class=\"o\">.</span><span class=\"n\">norm</span><span class=\"p\">(</span><span class=\"n\">grad</span> <span class=\"o\">-</span> <span class=\"n\">gradapprox</span><span class=\"p\">)</span>                                           <span class=\"c1\"># Step 1&#39;</span>\n    <span class=\"n\">denominator</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linalg</span><span class=\"o\">.</span><span class=\"n\">norm</span><span class=\"p\">(</span><span class=\"n\">grad</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linalg</span><span class=\"o\">.</span><span class=\"n\">norm</span><span class=\"p\">(</span><span class=\"n\">gradapprox</span><span class=\"p\">)</span>                                         <span class=\"c1\"># Step 2&#39;</span>\n    <span class=\"n\">difference</span> <span class=\"o\">=</span> <span class=\"n\">numerator</span> <span class=\"o\">/</span> <span class=\"n\">denominator</span>                                          <span class=\"c1\"># Step 3&#39;</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n<span class=\"err\">​</span>\n    <span class=\"k\">if</span> <span class=\"n\">difference</span> <span class=\"o\">&gt;</span> <span class=\"mf\">2e-7</span><span class=\"p\">:</span>\n        <span class=\"k\">print</span> <span class=\"p\">(</span><span class=\"s2\">&#34;</span><span class=\"se\">\\033</span><span class=\"s2\">[93m&#34;</span> <span class=\"o\">+</span> <span class=\"s2\">&#34;There is a mistake in the backward propagation! difference = &#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">difference</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"s2\">&#34;</span><span class=\"se\">\\033</span><span class=\"s2\">[0m&#34;</span><span class=\"p\">)</span>\n    <span class=\"k\">else</span><span class=\"p\">:</span>\n        <span class=\"k\">print</span> <span class=\"p\">(</span><span class=\"s2\">&#34;</span><span class=\"se\">\\033</span><span class=\"s2\">[92m&#34;</span> <span class=\"o\">+</span> <span class=\"s2\">&#34;Your backward propagation works perfectly fine! difference = &#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">difference</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"s2\">&#34;</span><span class=\"se\">\\033</span><span class=\"s2\">[0m&#34;</span><span class=\"p\">)</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">difference</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>注意：</p><p>梯度检验太慢，不要在训练的时候运行，你运行只是为了保证你的算法是正确的。</p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "神经网络", 
                    "tagLink": "https://api.zhihu.com/topics/19607065"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/44674876", 
            "userName": "直上云霄", 
            "userLink": "https://www.zhihu.com/people/1033165ce4ad9c3fce69a0793dfab8ad", 
            "upvote": 0, 
            "title": "DeepLearning.ai笔记:(2-1) 深度学习实践", 
            "content": "<h2>本文首发于个人博客：<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">fangzh.top</span><span class=\"invisible\"></span></a>，欢迎来访</h2><p class=\"ztext-empty-paragraph\"><br/></p><p>第二门课主要讲的是如何改善神经网络，通过超参数的调试、正则化以及优化。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>第一周主要是说了一些之前机器学习里面涉及到的数据集的划分，以及初始化，正则化的方法，还有梯度的验证。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>训练、验证、测试集的划分</b></h2><p>这些在之前的机器学习课程中都讲过了，这里简单说一下。</p><p>训练集也就是你训练的样本；验证集是你训练之后的参数放到这些数据中做验证；而最后做的测试集则是相当于用来最终的测试。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>一般来说，划分比例为60%/20%/20%就可以了，但是当数据越来越大，变成上百万，上千万的时候，那么验证集和测试集就没必要占那么大比重了，因为太过浪费，一般在0.5%-3%左右就可以。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>需要注意的是，验证集和测试集的数据要来源相同，同分布，也就是同一类的数据，不能验证集是网上的，测试集是你自己拍的照片，这样误差会很大。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>bias and variance（偏差和方差）</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-1db2dd95798106d95855fd9b1d3039f0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1444\" data-rawheight=\"402\" class=\"origin_image zh-lightbox-thumb\" width=\"1444\" data-original=\"https://pic1.zhimg.com/v2-1db2dd95798106d95855fd9b1d3039f0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1444&#39; height=&#39;402&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1444\" data-rawheight=\"402\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1444\" data-original=\"https://pic1.zhimg.com/v2-1db2dd95798106d95855fd9b1d3039f0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-1db2dd95798106d95855fd9b1d3039f0_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>high bias 表示的是高偏差，一般出现在欠拟合(under fitting)的情况下，</p><p>high variance表示高方差，一般出现在overfitting情况下。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>如何解决呢：</p><ul><li>high bias<br/></li><ul><li>更多的隐藏层</li><li>每一层更多的神经元</li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>high variance<br/></li><ul><li>增加数据</li><li>正则化</li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-984e1ac8d9676089bf44f169e0b9fff7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"831\" data-rawheight=\"250\" class=\"origin_image zh-lightbox-thumb\" width=\"831\" data-original=\"https://pic4.zhimg.com/v2-984e1ac8d9676089bf44f169e0b9fff7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;831&#39; height=&#39;250&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"831\" data-rawheight=\"250\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"831\" data-original=\"https://pic4.zhimg.com/v2-984e1ac8d9676089bf44f169e0b9fff7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-984e1ac8d9676089bf44f169e0b9fff7_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>从左到右4种情况即是： high variance ;  high bias ; high bias and high variance ; low bias and low variance</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>regularization（正则化）</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p>high variance可以使用正则化来解决。</p><p>我们知道，在logistic regression中的正则化项，是在损失函数后面加上：</p><p>L2 正则： <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Clambda%7D%7B2m%7D%7C%7Cw%7C%7C%5E%7B2%7D_%7B2%7D+%3D+%5Cfrac%7B%5Clambda%7D%7B2m%7D%5Csum_%7Bj%3D1%7D%5E%7Bn_%7Bx%7D%7D%7B%7Cw%7C%7D+%3D++%5Cfrac%7B%5Clambda%7D%7B2m%7D+w%5ET+w\" alt=\"\\frac{\\lambda}{2m}||w||^{2}_{2} = \\frac{\\lambda}{2m}\\sum_{j=1}^{n_{x}}{|w|} =  \\frac{\\lambda}{2m} w^T w\" eeimg=\"1\"/> </p><p>L1正则： <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Clambda%7D%7B2m%7D%7C%7Cw%7C%7C_%7B1%7D+%3D+%5Cfrac%7B%5Clambda%7D%7B2m%7D%5Csum_%7Bj%3D1%7D%5E%7Bn_%7Bx%7D%7D%7B%7Cw%7C%7D\" alt=\"\\frac{\\lambda}{2m}||w||_{1} = \\frac{\\lambda}{2m}\\sum_{j=1}^{n_{x}}{|w|}\" eeimg=\"1\"/> </p><p class=\"ztext-empty-paragraph\"><br/></p><p>一般用L2正则来做。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在neural network中，</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-bc73b840a5eaac46c0d802c2f4beeec6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1027\" data-rawheight=\"270\" class=\"origin_image zh-lightbox-thumb\" width=\"1027\" data-original=\"https://pic3.zhimg.com/v2-bc73b840a5eaac46c0d802c2f4beeec6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1027&#39; height=&#39;270&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1027\" data-rawheight=\"270\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1027\" data-original=\"https://pic3.zhimg.com/v2-bc73b840a5eaac46c0d802c2f4beeec6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-bc73b840a5eaac46c0d802c2f4beeec6_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>可以看到后面的正则式是从第1层累加到了第L层的所有神经网络的权重 <img src=\"https://www.zhihu.com/equation?tex=%7C%7CW%5E%7B%5Bl%5D%7D%7C%7C_%7BF%7D\" alt=\"||W^{[l]}||_{F}\" eeimg=\"1\"/> 的平方。</p><p>而我们知道这个W是一个 <img src=\"https://www.zhihu.com/equation?tex=n%5E%7B%5Bl%5D%7D+%5Ctimes+n%5E%7B%5Bl-1%5D%7D\" alt=\"n^{[l]} \\times n^{[l-1]}\" eeimg=\"1\"/> 的矩阵，那么</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-a40954b8ac725915422e9d3f44e0c688_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"845\" data-rawheight=\"103\" class=\"origin_image zh-lightbox-thumb\" width=\"845\" data-original=\"https://pic1.zhimg.com/v2-a40954b8ac725915422e9d3f44e0c688_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;845&#39; height=&#39;103&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"845\" data-rawheight=\"103\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"845\" data-original=\"https://pic1.zhimg.com/v2-a40954b8ac725915422e9d3f44e0c688_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-a40954b8ac725915422e9d3f44e0c688_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>它表示矩阵中所有元素的平方和。也就这一项嵌套了3层的 <img src=\"https://www.zhihu.com/equation?tex=%5Csum\" alt=\"\\sum\" eeimg=\"1\"/> 。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>那么，如何实现这个范数的梯度下降呢？</p><p class=\"ztext-empty-paragraph\"><br/></p><p>在原本的backprop中,加上的正则项的导数， <img src=\"https://www.zhihu.com/equation?tex=dJ+%2F+dW\" alt=\"dJ / dW\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=dW%5E%7B%5Bl%5D%7D+%3D+%28form+backprop%29+%2B+%5Cfrac%7B%5Clambda%7D%7Bm%7DW%5E%7B%5Bl%5D%7D%24\" alt=\"dW^{[l]} = (form backprop) + \\frac{\\lambda}{m}W^{[l]}$\" eeimg=\"1\"/> </p><p>代入</p><p><img src=\"https://www.zhihu.com/equation?tex=W%5E%7B%5Bl%5D%7D+%3D+W%5E%7B%5Bl%5D%7D+-+%5Calpha+dW%5E%7B%5Bl%5D%7D\" alt=\"W^{[l]} = W^{[l]} - \\alpha dW^{[l]}\" eeimg=\"1\"/> </p><p>得到：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-d89a0c9eabf2a24af67adc918b08e6cb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"623\" data-rawheight=\"199\" class=\"origin_image zh-lightbox-thumb\" width=\"623\" data-original=\"https://pic4.zhimg.com/v2-d89a0c9eabf2a24af67adc918b08e6cb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;623&#39; height=&#39;199&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"623\" data-rawheight=\"199\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"623\" data-original=\"https://pic4.zhimg.com/v2-d89a0c9eabf2a24af67adc918b08e6cb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-d89a0c9eabf2a24af67adc918b08e6cb_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>可以看到， <img src=\"https://www.zhihu.com/equation?tex=%281+-+%5Cfrac%7B%5Calpha+%5Clambda%7D%7Bm%7D%29+%3C+1\" alt=\"(1 - \\frac{\\alpha \\lambda}{m}) &lt; 1\" eeimg=\"1\"/> ，所以每一次都会让W变小，因此L2范数正则化也成为“权重衰减”</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>正则化如何防止过拟合？</b></h2><p>直观理解是在代价函数加入正则项后，如果 <img src=\"https://www.zhihu.com/equation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"/> 非常大，为了满足代价函数最小化，那么 <img src=\"https://www.zhihu.com/equation?tex=w%5E%7B%5Bl%5D%7D\" alt=\"w^{[l]}\" eeimg=\"1\"/> 这一项必须非常接近于0，所以就等价于很多神经元都没有作用了，从原本的非线性结构变成了近似的线性结构，自然就不会过拟合了。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-1e036e4ad60cc07f3e3def590ba68983_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1068\" data-rawheight=\"494\" class=\"origin_image zh-lightbox-thumb\" width=\"1068\" data-original=\"https://pic4.zhimg.com/v2-1e036e4ad60cc07f3e3def590ba68983_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1068&#39; height=&#39;494&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1068\" data-rawheight=\"494\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1068\" data-original=\"https://pic4.zhimg.com/v2-1e036e4ad60cc07f3e3def590ba68983_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-1e036e4ad60cc07f3e3def590ba68983_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>我们再来直观感受一下，</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b427d2557912ed7fe1e99fe9adcb3399_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"484\" data-rawheight=\"231\" class=\"origin_image zh-lightbox-thumb\" width=\"484\" data-original=\"https://pic2.zhimg.com/v2-b427d2557912ed7fe1e99fe9adcb3399_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;484&#39; height=&#39;231&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"484\" data-rawheight=\"231\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"484\" data-original=\"https://pic2.zhimg.com/v2-b427d2557912ed7fe1e99fe9adcb3399_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-b427d2557912ed7fe1e99fe9adcb3399_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>假设是一个tanh()函数，那么 <img src=\"https://www.zhihu.com/equation?tex=z+%3D+wx+%2B+b\" alt=\"z = wx + b\" eeimg=\"1\"/> ，当w非常接近于0时，z也接近于0，也就是在坐标轴上0附近范围内，这个时候斜率接近于线性，那么整个神经网络也非常接近于线性的网络，那么就不会发生过拟合了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>dropout 正则化</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p>dropout(随机失活)，也是正则化的一种，顾名思义，是让神经网络中的神经元按照一定的概率随机失活。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-109ec4d34be0e9742681e6fdc8626d15_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"964\" data-rawheight=\"374\" class=\"origin_image zh-lightbox-thumb\" width=\"964\" data-original=\"https://pic2.zhimg.com/v2-109ec4d34be0e9742681e6fdc8626d15_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;964&#39; height=&#39;374&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"964\" data-rawheight=\"374\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"964\" data-original=\"https://pic2.zhimg.com/v2-109ec4d34be0e9742681e6fdc8626d15_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-109ec4d34be0e9742681e6fdc8626d15_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>实现dropout：inverted dropout（反向随机失活）</b></p><p>实现dropout有好几种，但是最常用的还是这个inverted dropout</p><p>假设是一个3层的神经网络，keepprob表示保留节点的概率</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">keepprob</span> <span class=\"o\">=</span> <span class=\"mf\">0.8</span>\n<span class=\"c1\">#d3是矩阵，每个元素有true,false,在python中代表1和0</span>\n<span class=\"n\">d3</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">rand</span><span class=\"p\">(</span><span class=\"n\">a3</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span><span class=\"n\">a3</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">])</span> <span class=\"o\">&lt;</span> <span class=\"n\">keepprob</span>\n<span class=\"n\">a3</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">multiply</span><span class=\"p\">(</span><span class=\"n\">a3</span><span class=\"p\">,</span><span class=\"n\">d3</span><span class=\"p\">)</span>\n<span class=\"n\">a3</span> <span class=\"o\">/=</span> <span class=\"n\">keepprob</span></code></pre></div><p>其中第4式 <img src=\"https://www.zhihu.com/equation?tex=+a3+%2F%3D+keepprob\" alt=\" a3 /= keepprob\" eeimg=\"1\"/> </p><p>假设第三层有50个神经元 a3.shape[0] = 50，一共有 <img src=\"https://www.zhihu.com/equation?tex=+50+%2A+m\" alt=\" 50 * m\" eeimg=\"1\"/> 维，m是样本数，这样子就会有平均10个神经元被删除，因为 <img src=\"https://www.zhihu.com/equation?tex=z%5E%7B%5B4%5D%7D+%3D+w%5E%7B%5B4%5D%7D+a%5E%7B%5B3%5D%7D+%2B+b%5E%7B%5B4%5D%7D\" alt=\"z^{[4]} = w^{[4]} a^{[3]} + b^{[4]}\" eeimg=\"1\"/> ，那么这个时候 <img src=\"https://www.zhihu.com/equation?tex=z%5E%7B%5B4%5D%7D\" alt=\"z^{[4]}\" eeimg=\"1\"/> 的期望值就少了20%,所以在每个神经元上都除以keepprob的值，刚好弥补的之前的损失。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>注意</b></p><p>在test阶段，就不需要再使用dropout了，而是像之前一样，直接乘以各个层的权重，得出预测值就可以。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>理解dropout</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p>直观上，因为神经元有可能会被随机清除，这样子在训练中，就不会过分依赖某一个神经元或者特征的权重。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>当然可以设置不同层有不同的dropout概率。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>计算机视觉领域非常喜欢用这个dropout。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>但是这个东西的一大缺点就是代价函数J不能再被明确定义，每次都会随机移除一些节点，所以很难进行复查。如果需要调试的话，通常会关闭dropout，设置为1，这样再来debug。</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>归一化</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p>归一化数据可以加速神经网络的训练速度。</p><p>一般有两个步骤：</p><ul><li>零均值</li><li>归一化方差</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-61cef71d0dd82c27f35cb74f8fc8531d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1085\" data-rawheight=\"552\" class=\"origin_image zh-lightbox-thumb\" width=\"1085\" data-original=\"https://pic2.zhimg.com/v2-61cef71d0dd82c27f35cb74f8fc8531d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1085&#39; height=&#39;552&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1085\" data-rawheight=\"552\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1085\" data-original=\"https://pic2.zhimg.com/v2-61cef71d0dd82c27f35cb74f8fc8531d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-61cef71d0dd82c27f35cb74f8fc8531d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>这样子在gradient的时候就会走的顺畅一点：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-5dccdcd69bdae5ce84499b85184c6235_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1074\" data-rawheight=\"601\" class=\"origin_image zh-lightbox-thumb\" width=\"1074\" data-original=\"https://pic2.zhimg.com/v2-5dccdcd69bdae5ce84499b85184c6235_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1074&#39; height=&#39;601&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1074\" data-rawheight=\"601\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1074\" data-original=\"https://pic2.zhimg.com/v2-5dccdcd69bdae5ce84499b85184c6235_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-5dccdcd69bdae5ce84499b85184c6235_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>参数初始化</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p>合理的参数初始化可以有效的加快神经网络的训练速度。</p><p>一般呢 <img src=\"https://www.zhihu.com/equation?tex=z+%3D+w_1+x_1+%2B+w_2+x_2+%2B+...+%2B+w_n+x_n\" alt=\"z = w_1 x_1 + w_2 x_2 + ... + w_n x_n\" eeimg=\"1\"/> ，一般希望z不要太大也不要太小。所以呢，希望n越大，w越小才好。最合理的就是方差 <img src=\"https://www.zhihu.com/equation?tex=w+%3D+%5Cfrac%7B1%7D%7Bn%7D\" alt=\"w = \\frac{1}{n}\" eeimg=\"1\"/> ，所以：</p><div class=\"highlight\"><pre><code class=\"language-text\">WL = np.random.randn(WL.shape[0],WL.shape[1])* np.sqrt(1/n)</code></pre></div><p>这个 <img src=\"https://www.zhihu.com/equation?tex=n\" alt=\"n\" eeimg=\"1\"/> 即 <img src=\"https://www.zhihu.com/equation?tex=n%5E%7B%5Bl-1%5D%7D\" alt=\"n^{[l-1]}\" eeimg=\"1\"/> </p><p class=\"ztext-empty-paragraph\"><br/></p><p>如果是relu函数，</p><p>那么 <img src=\"https://www.zhihu.com/equation?tex=w+%3D+%5Cfrac%7B2%7D%7Bn%7D\" alt=\"w = \\frac{2}{n}\" eeimg=\"1\"/> 比较好，也就是<code>np.sqrt(2/n)</code> </p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>梯度的数值逼近</b></h2><p><img src=\"https://www.zhihu.com/equation?tex=+%5Cfrac%7B%5Cpartial+J%7D%7B%5Cpartial+%5Ctheta%7D+%3D+%5Clim_%7B%5Cvarepsilon+%5Cto+0%7D+%5Cfrac%7BJ%28%5Ctheta+%2B+%5Cvarepsilon%29+-+J%28%5Ctheta+-+%5Cvarepsilon%29%7D%7B2+%5Cvarepsilon%7D+\" alt=\" \\frac{\\partial J}{\\partial \\theta} = \\lim_{\\varepsilon \\to 0} \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon} \" eeimg=\"1\"/> </p><p>微积分的常识，用 <img src=\"https://www.zhihu.com/equation?tex=%5Cvarepsilon\" alt=\"\\varepsilon\" eeimg=\"1\"/> 来逼近梯度。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>梯度检验</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p>用梯度检验可以来检查在反向传播中的算法有没有错误。</p><p>这个时候，可以把 <img src=\"https://www.zhihu.com/equation?tex=W%5E%7B%5B1%5D%7D%2Cb%5E%7B%5B1%5D%7D%2C......W%5E%7B%5Bl%5D%7D%2Cb%5E%7B%5Bl%5D%7D\" alt=\"W^{[1]},b^{[1]},......W^{[l]},b^{[l]}\" eeimg=\"1\"/> 变成一个向量，这样可以得到一个代价函数 <img src=\"https://www.zhihu.com/equation?tex=J%28%5Ctheta%29\" alt=\"J(\\theta)\" eeimg=\"1\"/> ，然后 <img src=\"https://www.zhihu.com/equation?tex=dW%2Cdb\" alt=\"dW,db\" eeimg=\"1\"/> 也可以转换成一个向量，用 <img src=\"https://www.zhihu.com/equation?tex=d%5Ctheta\" alt=\"d\\theta\" eeimg=\"1\"/> 表示，和 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 有相同的维度。</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-52f7b041ea65aa9b935d40af8dd3a38c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1051\" data-rawheight=\"602\" class=\"origin_image zh-lightbox-thumb\" width=\"1051\" data-original=\"https://pic1.zhimg.com/v2-52f7b041ea65aa9b935d40af8dd3a38c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1051&#39; height=&#39;602&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1051\" data-rawheight=\"602\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1051\" data-original=\"https://pic1.zhimg.com/v2-52f7b041ea65aa9b935d40af8dd3a38c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-52f7b041ea65aa9b935d40af8dd3a38c_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>再对每一个 <img src=\"https://www.zhihu.com/equation?tex=d%5Ctheta_%7Bapprox%7D%5Bi%5D\" alt=\"d\\theta_{approx}[i]\" eeimg=\"1\"/> 求上面的双边梯度逼近，然后也用导数求得每一个 <img src=\"https://www.zhihu.com/equation?tex=d%5Ctheta%5Bi%5D\" alt=\"d\\theta[i]\" eeimg=\"1\"/> ，然后根据图上的cheak公式。求梯度逼近的时候，设置两边的 <img src=\"https://www.zhihu.com/equation?tex=%5Cvarepsilon+%3D+10%5E%7B-7%7D\" alt=\"\\varepsilon = 10^{-7}\" eeimg=\"1\"/> ，最终求得的值如果是 <img src=\"https://www.zhihu.com/equation?tex=10%5E%7B-7%7D\" alt=\"10^{-7}\" eeimg=\"1\"/> ，那么很正常， <img src=\"https://www.zhihu.com/equation?tex=10%5E%7B-3%7D\" alt=\"10^{-3}\" eeimg=\"1\"/> 就是错了的，如果是 <img src=\"https://www.zhihu.com/equation?tex=10%5E%7B-5%7D\" alt=\"10^{-5}\" eeimg=\"1\"/> ，那么就需要斟酌一下了。</p><p><b>注意</b></p><ul><li>不要在训练中用梯度检验，因为很慢</li><li>如果发现有问题，那么定位到误差比较大的那一层查看</li><li>如果有正则化，记得加入正则项</li><li>不要和dropout一起使用，因为dropout本来就不容易计算梯度。</li></ul><p></p><p></p><p></p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "神经网络", 
                    "tagLink": "https://api.zhihu.com/topics/19607065"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/44674476", 
            "userName": "直上云霄", 
            "userLink": "https://www.zhihu.com/people/1033165ce4ad9c3fce69a0793dfab8ad", 
            "upvote": 0, 
            "title": "DeepLearning.ai作业:(1-4)-深层神经网络", 
            "content": "<h2>本文首发于个人博客：<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">fangzh.top</span><span class=\"invisible\"></span></a>，欢迎来访</h2><ol><li>不要抄作业！</li><li>我只是把思路整理了，供个人学习。</li><li>不要抄作业！</li></ol><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>本周的作业分了两个部分，第一部分先构建神经网络的基本函数，第二部分才是构建出模型并预测。</p><h2><b>Part1</b></h2><p>构建的函数有：</p><ul><li>Initialize the parameters<br/></li><ul><li>two-layer</li><li>L-layer</li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>forworad propagation<br/></li><ul><li>Linear part  先构建一个线性的计算函数</li><li>linear-&gt;activation  在构建某一个神经元的线性和激活函数</li><li>L_model_forward funciton  再融合 L-1次的Relu 和   一次 的 sigmoid最后一层</li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li>Compute loss</li><li>backward propagation<br/></li><ul><li>Linear part</li><li>linear-&gt;activation</li><li>L_model_backward funciton</li></ul></ul><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>Initialization</b></h2><p>初始化使用：</p><p>w :  <code>np.random.randn(shape)*0.01</code></p><p>b :  <code>np.zeros(shape)</code></p><p><b>1. two-layer</b></p><p>先写了个两层的初始化函数，上周已经写过了。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">initialize_parameters</span><span class=\"p\">(</span><span class=\"n\">n_x</span><span class=\"p\">,</span> <span class=\"n\">n_h</span><span class=\"p\">,</span> <span class=\"n\">n_y</span><span class=\"p\">):</span>\n\n    <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">seed</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n    \n    <span class=\"c1\">### START CODE HERE ### (≈ 4 lines of code)</span>\n    <span class=\"n\">W1</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">n_h</span><span class=\"p\">,</span> <span class=\"n\">n_x</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"mf\">0.01</span>\n    <span class=\"n\">b1</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">n_h</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">))</span>\n    <span class=\"n\">W2</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">n_y</span><span class=\"p\">,</span> <span class=\"n\">n_h</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"mf\">0.01</span>\n    <span class=\"n\">b2</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">n_y</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"p\">))</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"k\">assert</span><span class=\"p\">(</span><span class=\"n\">W1</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"p\">(</span><span class=\"n\">n_h</span><span class=\"p\">,</span> <span class=\"n\">n_x</span><span class=\"p\">))</span>\n    <span class=\"k\">assert</span><span class=\"p\">(</span><span class=\"n\">b1</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"p\">(</span><span class=\"n\">n_h</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n    <span class=\"k\">assert</span><span class=\"p\">(</span><span class=\"n\">W2</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"p\">(</span><span class=\"n\">n_y</span><span class=\"p\">,</span> <span class=\"n\">n_h</span><span class=\"p\">))</span>\n    <span class=\"k\">assert</span><span class=\"p\">(</span><span class=\"n\">b2</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"p\">(</span><span class=\"n\">n_y</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n    \n    <span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">&#34;W1&#34;</span><span class=\"p\">:</span> <span class=\"n\">W1</span><span class=\"p\">,</span>\n                  <span class=\"s2\">&#34;b1&#34;</span><span class=\"p\">:</span> <span class=\"n\">b1</span><span class=\"p\">,</span>\n                  <span class=\"s2\">&#34;W2&#34;</span><span class=\"p\">:</span> <span class=\"n\">W2</span><span class=\"p\">,</span>\n                  <span class=\"s2\">&#34;b2&#34;</span><span class=\"p\">:</span> <span class=\"n\">b2</span><span class=\"p\">}</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">parameters</span>    </code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>2. L-layer</b></p><p>然后写了个L层的初始化函数，其中，输入的参数是一个列表，如[12,4,3,1]，表示一共4层：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">initialize_parameters_deep</span><span class=\"p\">(</span><span class=\"n\">layer_dims</span><span class=\"p\">):</span>\n\n    <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">seed</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">)</span>\n    <span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>\n    <span class=\"n\">L</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">layer_dims</span><span class=\"p\">)</span>            <span class=\"c1\"># number of layers in the network</span>\n<span class=\"err\">​</span>\n    <span class=\"k\">for</span> <span class=\"n\">l</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">L</span><span class=\"p\">):</span>\n        <span class=\"c1\">### START CODE HERE ### (≈ 2 lines of code)</span>\n        <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;W&#39;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">layer_dims</span><span class=\"p\">[</span><span class=\"n\">l</span><span class=\"p\">],</span> <span class=\"n\">layer_dims</span><span class=\"p\">[</span><span class=\"n\">l</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">])</span> <span class=\"o\">*</span> <span class=\"mf\">0.01</span>\n        <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;b&#39;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">layer_dims</span><span class=\"p\">[</span><span class=\"n\">l</span><span class=\"p\">],</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n        \n        <span class=\"k\">assert</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;W&#39;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"p\">)]</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"p\">(</span><span class=\"n\">layer_dims</span><span class=\"p\">[</span><span class=\"n\">l</span><span class=\"p\">],</span> <span class=\"n\">layer_dims</span><span class=\"p\">[</span><span class=\"n\">l</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]))</span>\n        <span class=\"k\">assert</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;b&#39;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"p\">)]</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"p\">(</span><span class=\"n\">layer_dims</span><span class=\"p\">[</span><span class=\"n\">l</span><span class=\"p\">],</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n<span class=\"err\">​</span>\n        \n    <span class=\"k\">return</span> <span class=\"n\">parameters</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>Forward propagation module</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p><b>1. Linear Forward</b></p><p>利用公式：</p><p><img src=\"https://www.zhihu.com/equation?tex=Z%5E%7B%5Bl%5D%7D+%3D+W%5E%7B%5Bl%5D%7DA%5E%7B%5Bl-1%5D%7D+%2Bb%5E%7B%5Bl%5D%7D\" alt=\"Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\" eeimg=\"1\"/> </p><p>where <img src=\"https://www.zhihu.com/equation?tex=A%5E%7B%5B0%5D%7D+%3D+X\" alt=\"A^{[0]} = X\" eeimg=\"1\"/> </p><p>这个时候，输入的参数是 A,W,b,输出是计算得到的Z，以及cache=（A， W， b）保存起来</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">linear_forward</span><span class=\"p\">(</span><span class=\"n\">A</span><span class=\"p\">,</span> <span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">):</span>\n\n    <span class=\"k\">assert</span><span class=\"p\">(</span><span class=\"n\">Z</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"p\">(</span><span class=\"n\">W</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">A</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]))</span>\n    <span class=\"n\">cache</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">A</span><span class=\"p\">,</span> <span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">)</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">Z</span><span class=\"p\">,</span> <span class=\"n\">cache</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><b>2. Linear-Activation Forward</b></p><p>在这里就是把刚才得到的Z，通过 <img src=\"https://www.zhihu.com/equation?tex=A+%3D+g%28Z%29\" alt=\"A = g(Z)\" eeimg=\"1\"/> 激活函数，合并成一个</p><p>这个时候，notebook已经给了我们现成的sigmoid和relu函数了，只要调用就行，不过在里面好像没有说明源代码，输出都是A和cache=Z，这里贴出来：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">sigmoid</span><span class=\"p\">(</span><span class=\"n\">Z</span><span class=\"p\">):</span>\n\n    <span class=\"n\">A</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"o\">/</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"o\">+</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"n\">Z</span><span class=\"p\">))</span>\n    <span class=\"n\">cache</span> <span class=\"o\">=</span> <span class=\"n\">Z</span>\n<span class=\"err\">​</span>\n    <span class=\"k\">return</span> <span class=\"n\">A</span><span class=\"p\">,</span> <span class=\"n\">cache</span>\n<span class=\"k\">def</span> <span class=\"nf\">relu</span><span class=\"p\">(</span><span class=\"n\">Z</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Implement the RELU function.\n</span><span class=\"s2\">​\n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    Z -- Output of the linear layer, of any shape\n</span><span class=\"s2\">​\n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    A -- Post-activation parameter, of the same shape as Z\n</span><span class=\"s2\">    cache -- a python dictionary containing &#34;A&#34; ; stored for computing the backward pass efficiently\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n<span class=\"err\">​</span>\n    <span class=\"n\">A</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">maximum</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span><span class=\"n\">Z</span><span class=\"p\">)</span>\n<span class=\"err\">​</span>\n    <span class=\"k\">assert</span><span class=\"p\">(</span><span class=\"n\">A</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"n\">Z</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span>\n<span class=\"err\">​</span>\n    <span class=\"n\">cache</span> <span class=\"o\">=</span> <span class=\"n\">Z</span> \n    <span class=\"k\">return</span> <span class=\"n\">A</span><span class=\"p\">,</span> <span class=\"n\">cache</span></code></pre></div><p>而后利用之前的linear_forward，可以写出某层神经元的前向函数了，输入是 <img src=\"https://www.zhihu.com/equation?tex=A%5E%7B%5Bl-1%5D%7D%2CW%2Cb\" alt=\"A^{[l-1]},W,b\" eeimg=\"1\"/> ，还有一个是说明sigmoid还是relu的字符串activation。</p><p><b>输出是</b> <img src=\"https://www.zhihu.com/equation?tex=A%5E%7B%5Bl%5D%7D\" alt=\"A^{[l]}\" eeimg=\"1\"/> <b>和cache，这里的cache已经包含的4个参数了，分别是</b> <img src=\"https://www.zhihu.com/equation?tex=A%5E%7B%5Bl-1%5D%7D%2CW%5E%7B%5Bl%5D%7D%2Cb%5E%7B%5Bl%5D%7D%2CZ%5E%7B%5Bl%5D%7D\" alt=\"A^{[l-1]},W^{[l]},b^{[l]},Z^{[l]}\" eeimg=\"1\"/> </p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"err\">​</span>\n<span class=\"c1\"># GRADED FUNCTION: linear_activation_forward</span>\n<span class=\"err\">​</span>\n<span class=\"k\">def</span> <span class=\"nf\">linear_activation_forward</span><span class=\"p\">(</span><span class=\"n\">A_prev</span><span class=\"p\">,</span> <span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"p\">):</span>\n\n    \n    <span class=\"k\">if</span> <span class=\"n\">activation</span> <span class=\"o\">==</span> <span class=\"s2\">&#34;sigmoid&#34;</span><span class=\"p\">:</span>\n        <span class=\"c1\"># Inputs: &#34;A_prev, W, b&#34;. Outputs: &#34;A, activation_cache&#34;.</span>\n        <span class=\"c1\">### START CODE HERE ### (≈ 2 lines of code)</span>\n        <span class=\"n\">Z</span><span class=\"p\">,</span> <span class=\"n\">linear_cache</span> <span class=\"o\">=</span> <span class=\"n\">linear_forward</span><span class=\"p\">(</span><span class=\"n\">A_prev</span><span class=\"p\">,</span> <span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">)</span>\n        <span class=\"n\">A</span><span class=\"p\">,</span> <span class=\"n\">activation_cache</span> <span class=\"o\">=</span> <span class=\"n\">sigmoid</span><span class=\"p\">(</span><span class=\"n\">Z</span><span class=\"p\">)</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"k\">elif</span> <span class=\"n\">activation</span> <span class=\"o\">==</span> <span class=\"s2\">&#34;relu&#34;</span><span class=\"p\">:</span>\n        <span class=\"c1\"># Inputs: &#34;A_prev, W, b&#34;. Outputs: &#34;A, activation_cache&#34;.</span>\n        <span class=\"c1\">### START CODE HERE ### (≈ 2 lines of code)</span>\n        <span class=\"n\">Z</span><span class=\"p\">,</span> <span class=\"n\">linear_cache</span> <span class=\"o\">=</span> <span class=\"n\">linear_forward</span><span class=\"p\">(</span><span class=\"n\">A_prev</span><span class=\"p\">,</span> <span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">)</span>\n        <span class=\"n\">A</span><span class=\"p\">,</span> <span class=\"n\">activation_cache</span> <span class=\"o\">=</span> <span class=\"n\">relu</span><span class=\"p\">(</span><span class=\"n\">Z</span><span class=\"p\">)</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"k\">assert</span> <span class=\"p\">(</span><span class=\"n\">A</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"p\">(</span><span class=\"n\">W</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">],</span> <span class=\"n\">A_prev</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]))</span>\n    <span class=\"n\">cache</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">linear_cache</span><span class=\"p\">,</span> <span class=\"n\">activation_cache</span><span class=\"p\">)</span>\n   <span class=\"c1\"># print(cache)</span>\n    <span class=\"k\">return</span> <span class=\"n\">A</span><span class=\"p\">,</span> <span class=\"n\">cache</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><b>3. L-Layer Model</b></p><p>这一步就把多层的神经网络从头到尾串起来了。前面有L-1层的Relu，第L层是sigmoid。</p><p>输入是X，也就是 <img src=\"https://www.zhihu.com/equation?tex=A%5E%7B%5B0%5D%7D\" alt=\"A^{[0]}\" eeimg=\"1\"/> ，和 parameters包含了各个层的W,b</p><p>输出是最后一层的 <img src=\"https://www.zhihu.com/equation?tex=A%5E%7B%5BL%5D%7D\" alt=\"A^{[L]}\" eeimg=\"1\"/> ，也就是预测结果$Y_hat$，以及每一层的caches : <img src=\"https://www.zhihu.com/equation?tex=A%5E%7B%5Bl-1%5D%7D%2CW%5E%7B%5Bl%5D%7D%2Cb%5E%7B%5Bl%5D%7D%2CZ%5E%7B%5Bl%5D%7D\" alt=\"A^{[l-1]},W^{[l]},b^{[l]},Z^{[l]}\" eeimg=\"1\"/> </p><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">L_model_forward</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">parameters</span><span class=\"p\">):</span>\n\n    <span class=\"n\">caches</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>\n    <span class=\"n\">A</span> <span class=\"o\">=</span> <span class=\"n\">X</span>\n    <span class=\"n\">L</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">)</span> <span class=\"o\">//</span> <span class=\"mi\">2</span>                  <span class=\"c1\"># number of layers in the neural network</span>\n    \n    <span class=\"c1\"># Implement [LINEAR -&gt; RELU]*(L-1). Add &#34;cache&#34; to the &#34;caches&#34; list.</span>\n    <span class=\"k\">for</span> <span class=\"n\">l</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">L</span><span class=\"p\">):</span>\n        <span class=\"n\">A_prev</span> <span class=\"o\">=</span> <span class=\"n\">A</span> \n        <span class=\"c1\">### START CODE HERE ### (≈ 2 lines of code)</span>\n        <span class=\"n\">A</span><span class=\"p\">,</span> <span class=\"n\">cache</span> <span class=\"o\">=</span> <span class=\"n\">linear_activation_forward</span><span class=\"p\">(</span><span class=\"n\">A_prev</span><span class=\"p\">,</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;W&#39;</span><span class=\"o\">+</span><span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"p\">)],</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;b&#39;</span><span class=\"o\">+</span><span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"p\">)],</span> <span class=\"s1\">&#39;relu&#39;</span><span class=\"p\">)</span>\n        <span class=\"n\">caches</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">cache</span><span class=\"p\">)</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"c1\"># Implement LINEAR -&gt; SIGMOID. Add &#34;cache&#34; to the &#34;caches&#34; list.</span>\n    <span class=\"c1\">### START CODE HERE ### (≈ 2 lines of code)</span>\n    <span class=\"n\">AL</span><span class=\"p\">,</span> <span class=\"n\">cache</span> <span class=\"o\">=</span> <span class=\"n\">linear_activation_forward</span><span class=\"p\">(</span><span class=\"n\">A</span><span class=\"p\">,</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;W&#39;</span><span class=\"o\">+</span><span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">L</span><span class=\"p\">)],</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;b&#39;</span><span class=\"o\">+</span><span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">L</span><span class=\"p\">)],</span><span class=\"s1\">&#39;sigmoid&#39;</span><span class=\"p\">)</span>\n    <span class=\"n\">caches</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">cache</span><span class=\"p\">)</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n   <span class=\"c1\"># print(AL.shape)</span>\n    <span class=\"k\">assert</span><span class=\"p\">(</span><span class=\"n\">AL</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]))</span>\n            \n    <span class=\"k\">return</span> <span class=\"n\">AL</span><span class=\"p\">,</span> <span class=\"n\">caches</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>Cost function</b></h2><p><img src=\"https://www.zhihu.com/equation?tex=-%5Cfrac%7B1%7D%7Bm%7D+%5Csum%5Climits_%7Bi+%3D+1%7D%5E%7Bm%7D+%28y%5E%7B%28i%29%7D%5Clog%5Cleft%28a%5E%7B%5BL%5D+%28i%29%7D%5Cright%29+%2B+%281-y%5E%7B%28i%29%7D%29%5Clog%5Cleft%281-+a%5E%7BL%7D%5Cright%29%29+\" alt=\"-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{L}\\right)) \" eeimg=\"1\"/> </p><p>利用<code>np.multiply</code> and <code>np.sum</code>求得交叉熵</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"err\">​</span>\n<span class=\"k\">def</span> <span class=\"nf\">compute_cost</span><span class=\"p\">(</span><span class=\"n\">AL</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">):</span>\n\n    <span class=\"n\">m</span> <span class=\"o\">=</span> <span class=\"n\">Y</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n<span class=\"err\">​</span>\n    <span class=\"c1\"># Compute loss from aL and y.</span>\n    <span class=\"c1\">### START CODE HERE ### (≈ 1 lines of code)</span>\n    <span class=\"n\">cost</span> <span class=\"o\">=</span> <span class=\"o\">-</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">multiply</span><span class=\"p\">(</span><span class=\"n\">Y</span><span class=\"p\">,</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">log</span><span class=\"p\">(</span><span class=\"n\">AL</span><span class=\"p\">))</span> <span class=\"o\">+</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">multiply</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"o\">-</span><span class=\"n\">Y</span><span class=\"p\">,</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">log</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"o\">-</span><span class=\"n\">AL</span><span class=\"p\">)))</span> <span class=\"o\">/</span> <span class=\"n\">m</span>\n    <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"n\">cost</span><span class=\"p\">)</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    <span class=\"n\">cost</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">squeeze</span><span class=\"p\">(</span><span class=\"n\">cost</span><span class=\"p\">)</span>      <span class=\"c1\"># To make sure your cost&#39;s shape is what we expect (e.g. this turns [[17]] into 17).</span>\n    <span class=\"k\">assert</span><span class=\"p\">(</span><span class=\"n\">cost</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"p\">())</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">cost</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>Backward propagation module</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p><b>1. Linear backward</b></p><p>首先假设知道 <img src=\"https://www.zhihu.com/equation?tex=dZ%5E%7B%5Bl%5D%7D+%3D+%5Cfrac%7B%5Cpartial+%5Cmathcal%7BL%7D+%7D%7B%5Cpartial+Z%5E%7B%5Bl%5D%7D%7D\" alt=\"dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}\" eeimg=\"1\"/> ，然后想要求得的是 <img src=\"https://www.zhihu.com/equation?tex=%28dW%5E%7B%5Bl%5D%7D%2C+db%5E%7B%5Bl%5D%7D+dA%5E%7B%5Bl-1%5D%7D%29\" alt=\"(dW^{[l]}, db^{[l]} dA^{[l-1]})\" eeimg=\"1\"/> .</p><p>公式已经给你了：</p><p><img src=\"https://www.zhihu.com/equation?tex=+dW%5E%7B%5Bl%5D%7D+%3D+%5Cfrac%7B%5Cpartial+%5Cmathcal%7BL%7D+%7D%7B%5Cpartial+W%5E%7B%5Bl%5D%7D%7D+%3D+%5Cfrac%7B1%7D%7Bm%7D+dZ%5E%7B%5Bl%5D%7D+A%5E%7B%5Bl-1%5D+T%7D++\" alt=\" dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T}  \" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=db%5E%7B%5Bl%5D%7D+%3D+%5Cfrac%7B%5Cpartial+%5Cmathcal%7BL%7D+%7D%7B%5Cpartial+b%5E%7B%5Bl%5D%7D%7D+%3D+%5Cfrac%7B1%7D%7Bm%7D+%5Csum_%7Bi+%3D+1%7D%5E%7Bm%7D+dZ%5E%7B%5Bl%5D+%28i%29%7D\" alt=\"db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l] (i)}\" eeimg=\"1\"/> </p><p class=\"ztext-empty-paragraph\"><br/></p><p><img src=\"https://www.zhihu.com/equation?tex=+dA%5E%7B%5Bl-1%5D%7D+%3D+%5Cfrac%7B%5Cpartial+%5Cmathcal%7BL%7D+%7D%7B%5Cpartial+A%5E%7B%5Bl-1%5D%7D%7D+%3D+W%5E%7B%5Bl%5D+T%7D+dZ%5E%7B%5Bl%5D%7D+\" alt=\" dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \" eeimg=\"1\"/> </p><p class=\"ztext-empty-paragraph\"><br/></p><p>cache是linear cache: A_prev,W,b</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">linear_backward</span><span class=\"p\">(</span><span class=\"n\">dZ</span><span class=\"p\">,</span> <span class=\"n\">cache</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Implement the linear portion of backward propagation for a single layer (layer l)\n</span><span class=\"s2\">​\n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n</span><span class=\"s2\">    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n</span><span class=\"s2\">​\n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n</span><span class=\"s2\">    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n</span><span class=\"s2\">    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    <span class=\"n\">A_prev</span><span class=\"p\">,</span> <span class=\"n\">W</span><span class=\"p\">,</span> <span class=\"n\">b</span> <span class=\"o\">=</span> <span class=\"n\">cache</span>\n    <span class=\"n\">m</span> <span class=\"o\">=</span> <span class=\"n\">A_prev</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n<span class=\"err\">​</span>\n    <span class=\"c1\">### START CODE HERE ### (≈ 3 lines of code)</span>\n    <span class=\"n\">dW</span> <span class=\"o\">=</span> <span class=\"mi\">1</span> <span class=\"o\">/</span> <span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">dZ</span><span class=\"p\">,</span> <span class=\"n\">A_prev</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">)</span>\n    <span class=\"n\">db</span> <span class=\"o\">=</span> <span class=\"mi\">1</span> <span class=\"o\">/</span> <span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">dZ</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span><span class=\"n\">keepdims</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n    <span class=\"c1\">#print(db.shape)</span>\n    <span class=\"c1\">#print(b.shape)</span>\n    <span class=\"n\">dA_prev</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">W</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">,</span> <span class=\"n\">dZ</span><span class=\"p\">)</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"k\">assert</span> <span class=\"p\">(</span><span class=\"n\">dA_prev</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"n\">A_prev</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span>\n    <span class=\"k\">assert</span> <span class=\"p\">(</span><span class=\"n\">dW</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"n\">W</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span>\n    <span class=\"k\">assert</span> <span class=\"p\">(</span><span class=\"n\">db</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"n\">b</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">dA_prev</span><span class=\"p\">,</span> <span class=\"n\">dW</span><span class=\"p\">,</span> <span class=\"n\">db</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><b>2. Linear-Activation backward</b></p><p>dA通过激活函数的导数可以求得dZ，再由上面的函数，最终：</p><p>输入 <img src=\"https://www.zhihu.com/equation?tex=dA%5E%7B%5Bl%5D%7D+%2C+cache\" alt=\"dA^{[l]} , cache\" eeimg=\"1\"/> </p><p>输出 <img src=\"https://www.zhihu.com/equation?tex=dA%5E%7B%5Bl-1%5D%7D+%2CdW%2Cdb\" alt=\"dA^{[l-1]} ,dW,db\" eeimg=\"1\"/> </p><p>这个时候它有给了两个现成的函数<code>dZ = sigmoid_backward(dA, activation_cache)</code>、<code>dZ = relu_backward(dA, activation_cache)</code></p><p>源代码如下,输入的都是dA，和 cache=Z，输出是dZ：</p><p><img src=\"https://www.zhihu.com/equation?tex=dZ%5E%7B%5Bl%5D%7D+%3D+dA%5E%7B%5Bl%5D%7D+%2A+g%27%28Z%5E%7B%5Bl%5D%7D%29\" alt=\"dZ^{[l]} = dA^{[l]} * g&#39;(Z^{[l]})\" eeimg=\"1\"/> </p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">sigmoid_backward</span><span class=\"p\">(</span><span class=\"n\">dA</span><span class=\"p\">,</span> <span class=\"n\">cache</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Implement the backward propagation for a single SIGMOID unit.\n</span><span class=\"s2\">​\n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    dA -- post-activation gradient, of any shape\n</span><span class=\"s2\">    cache -- &#39;Z&#39; where we store for computing backward propagation efficiently\n</span><span class=\"s2\">​\n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    dZ -- Gradient of the cost with respect to Z\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n<span class=\"err\">​</span>\n    <span class=\"n\">Z</span> <span class=\"o\">=</span> <span class=\"n\">cache</span>\n<span class=\"err\">​</span>\n    <span class=\"n\">s</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"o\">/</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"o\">+</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">exp</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"n\">Z</span><span class=\"p\">))</span>\n    <span class=\"n\">dZ</span> <span class=\"o\">=</span> <span class=\"n\">dA</span> <span class=\"o\">*</span> <span class=\"n\">s</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"o\">-</span><span class=\"n\">s</span><span class=\"p\">)</span>\n<span class=\"err\">​</span>\n    <span class=\"k\">assert</span> <span class=\"p\">(</span><span class=\"n\">dZ</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"n\">Z</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span>\n<span class=\"err\">​</span>\n    <span class=\"k\">return</span> <span class=\"n\">dZ</span>\n<span class=\"k\">def</span> <span class=\"nf\">relu_backward</span><span class=\"p\">(</span><span class=\"n\">dA</span><span class=\"p\">,</span> <span class=\"n\">cache</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Implement the backward propagation for a single RELU unit.\n</span><span class=\"s2\">​\n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    dA -- post-activation gradient, of any shape\n</span><span class=\"s2\">    cache -- &#39;Z&#39; where we store for computing backward propagation efficiently\n</span><span class=\"s2\">​\n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    dZ -- Gradient of the cost with respect to Z\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n<span class=\"err\">​</span>\n    <span class=\"n\">Z</span> <span class=\"o\">=</span> <span class=\"n\">cache</span>\n    <span class=\"n\">dZ</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">(</span><span class=\"n\">dA</span><span class=\"p\">,</span> <span class=\"n\">copy</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span> <span class=\"c1\"># just converting dz to a correct object.</span>\n<span class=\"err\">​</span>\n    <span class=\"c1\"># When z &lt;= 0, you should set dz to 0 as well. </span>\n    <span class=\"n\">dZ</span><span class=\"p\">[</span><span class=\"n\">Z</span> <span class=\"o\">&lt;=</span> <span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span>\n<span class=\"err\">​</span>\n    <span class=\"k\">assert</span> <span class=\"p\">(</span><span class=\"n\">dZ</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"n\">Z</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span>\n<span class=\"err\">​</span>\n    <span class=\"k\">return</span> <span class=\"n\">dZ</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>然后得到了函数如下,注意这里面的cache已经是4个元素了<code>linear_cache=A_prev,W,b</code>、<code>activation_cache=Z</code>：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: linear_activation_backward</span>\n<span class=\"err\">​</span>\n<span class=\"k\">def</span> <span class=\"nf\">linear_activation_backward</span><span class=\"p\">(</span><span class=\"n\">dA</span><span class=\"p\">,</span> <span class=\"n\">cache</span><span class=\"p\">,</span> <span class=\"n\">activation</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Implement the backward propagation for the LINEAR-&gt;ACTIVATION layer.\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    dA -- post-activation gradient for current layer l \n</span><span class=\"s2\">    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n</span><span class=\"s2\">    activation -- the activation to be used in this layer, stored as a text string: &#34;sigmoid&#34; or &#34;relu&#34;\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n</span><span class=\"s2\">    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n</span><span class=\"s2\">    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    <span class=\"n\">linear_cache</span><span class=\"p\">,</span> <span class=\"n\">activation_cache</span> <span class=\"o\">=</span> <span class=\"n\">cache</span>\n    \n    <span class=\"k\">if</span> <span class=\"n\">activation</span> <span class=\"o\">==</span> <span class=\"s2\">&#34;relu&#34;</span><span class=\"p\">:</span>\n        <span class=\"c1\">### START CODE HERE ### (≈ 2 lines of code)</span>\n        <span class=\"n\">dZ</span> <span class=\"o\">=</span> <span class=\"n\">relu_backward</span><span class=\"p\">(</span><span class=\"n\">dA</span><span class=\"p\">,</span> <span class=\"n\">activation_cache</span><span class=\"p\">)</span>\n        <span class=\"n\">dA_prev</span><span class=\"p\">,</span> <span class=\"n\">dW</span><span class=\"p\">,</span> <span class=\"n\">db</span> <span class=\"o\">=</span> <span class=\"n\">linear_backward</span><span class=\"p\">(</span><span class=\"n\">dZ</span><span class=\"p\">,</span> <span class=\"n\">linear_cache</span><span class=\"p\">)</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n        \n    <span class=\"k\">elif</span> <span class=\"n\">activation</span> <span class=\"o\">==</span> <span class=\"s2\">&#34;sigmoid&#34;</span><span class=\"p\">:</span>\n        <span class=\"c1\">### START CODE HERE ### (≈ 2 lines of code)</span>\n        <span class=\"n\">dZ</span> <span class=\"o\">=</span> <span class=\"n\">sigmoid_backward</span><span class=\"p\">(</span><span class=\"n\">dA</span><span class=\"p\">,</span> <span class=\"n\">activation_cache</span><span class=\"p\">)</span>\n        <span class=\"n\">dA_prev</span><span class=\"p\">,</span> <span class=\"n\">dW</span><span class=\"p\">,</span> <span class=\"n\">db</span> <span class=\"o\">=</span> <span class=\"n\">linear_backward</span><span class=\"p\">(</span><span class=\"n\">dZ</span><span class=\"p\">,</span> <span class=\"n\">linear_cache</span><span class=\"p\">)</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">dA_prev</span><span class=\"p\">,</span> <span class=\"n\">dW</span><span class=\"p\">,</span> <span class=\"n\">db</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><b>3.  L-Model Backward</b></p><p>可以把前面的函数穿起来，从后面往前面传播了，先算最后一层的sigmoid，然后往前算L-1的循环relu。其中，dAL是损失函数的导数，这个是预先求得知道的，也就是 </p><p><img src=\"https://www.zhihu.com/equation?tex=-%5Cfrac%7By%7D%7Ba%7D-%5Cfrac%7B1-y%7D%7B1-a%7D\" alt=\"-\\frac{y}{a}-\\frac{1-y}{1-a}\" eeimg=\"1\"/> </p><p>numpy表示为：</p><div class=\"highlight\"><pre><code class=\"language-text\">dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))</code></pre></div><p>整个backward中，我们的输入只有AL,Y和caches，</p><p>输出则是每一层的grads，包括了 <img src=\"https://www.zhihu.com/equation?tex=dA%2CdW%2Cdb\" alt=\"dA,dW,db\" eeimg=\"1\"/> </p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: L_model_backward</span>\n<span class=\"err\">​</span>\n<span class=\"k\">def</span> <span class=\"nf\">L_model_backward</span><span class=\"p\">(</span><span class=\"n\">AL</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">caches</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Implement the backward propagation for the [LINEAR-&gt;RELU] * (L-1) -&gt; LINEAR -&gt; SIGMOID group\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    AL -- probability vector, output of the forward propagation (L_model_forward())\n</span><span class=\"s2\">    Y -- true &#34;label&#34; vector (containing 0 if non-cat, 1 if cat)\n</span><span class=\"s2\">    caches -- list of caches containing:\n</span><span class=\"s2\">                every cache of linear_activation_forward() with &#34;relu&#34; (it&#39;s caches[l], for l in range(L-1) i.e l = 0...L-2)\n</span><span class=\"s2\">                the cache of linear_activation_forward() with &#34;sigmoid&#34; (it&#39;s caches[L-1])\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    grads -- A dictionary with the gradients\n</span><span class=\"s2\">             grads[&#34;dA&#34; + str(l)] = ... \n</span><span class=\"s2\">             grads[&#34;dW&#34; + str(l)] = ...\n</span><span class=\"s2\">             grads[&#34;db&#34; + str(l)] = ... \n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    <span class=\"n\">grads</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>\n    <span class=\"n\">L</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">caches</span><span class=\"p\">)</span> <span class=\"c1\"># the number of layers</span>\n    <span class=\"n\">m</span> <span class=\"o\">=</span> <span class=\"n\">AL</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n    <span class=\"n\">Y</span> <span class=\"o\">=</span> <span class=\"n\">Y</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"n\">AL</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">)</span> <span class=\"c1\"># after this line, Y is the same shape as AL</span>\n    \n    <span class=\"c1\"># Initializing the backpropagation</span>\n    <span class=\"c1\">### START CODE HERE ### (1 line of code)</span>\n    <span class=\"n\">dAL</span> <span class=\"o\">=</span>  <span class=\"o\">-</span> <span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">divide</span><span class=\"p\">(</span><span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">AL</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">divide</span><span class=\"p\">(</span><span class=\"mi\">1</span> <span class=\"o\">-</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"mi\">1</span> <span class=\"o\">-</span> <span class=\"n\">AL</span><span class=\"p\">))</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"c1\"># Lth layer (SIGMOID -&gt; LINEAR) gradients. Inputs: &#34;dAL, current_cache&#34;. Outputs: &#34;grads[&#34;dAL-1&#34;], grads[&#34;dWL&#34;], grads[&#34;dbL&#34;]</span>\n    <span class=\"c1\">### START CODE HERE ### (approx. 2 lines)</span>\n    <span class=\"n\">current_cache</span> <span class=\"o\">=</span> <span class=\"n\">caches</span><span class=\"p\">[</span><span class=\"n\">L</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n    <span class=\"n\">grads</span><span class=\"p\">[</span><span class=\"s2\">&#34;dA&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">L</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">)],</span> <span class=\"n\">grads</span><span class=\"p\">[</span><span class=\"s2\">&#34;dW&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">L</span><span class=\"p\">)],</span> <span class=\"n\">grads</span><span class=\"p\">[</span><span class=\"s2\">&#34;db&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">L</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">linear_activation_backward</span><span class=\"p\">(</span><span class=\"n\">dAL</span><span class=\"p\">,</span> <span class=\"n\">current_cache</span><span class=\"p\">,</span> <span class=\"s1\">&#39;sigmoid&#39;</span><span class=\"p\">)</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"c1\"># Loop from l=L-2 to l=0</span>\n    <span class=\"k\">for</span> <span class=\"n\">l</span> <span class=\"ow\">in</span> <span class=\"nb\">reversed</span><span class=\"p\">(</span><span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">L</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">)):</span>\n        <span class=\"c1\"># lth layer: (RELU -&gt; LINEAR) gradients.</span>\n        <span class=\"c1\"># Inputs: &#34;grads[&#34;dA&#34; + str(l + 1)], current_cache&#34;. Outputs: &#34;grads[&#34;dA&#34; + str(l)] , grads[&#34;dW&#34; + str(l + 1)] , grads[&#34;db&#34; + str(l + 1)] </span>\n        <span class=\"c1\">### START CODE HERE ### (approx. 5 lines)</span>\n        <span class=\"n\">current_cache</span> <span class=\"o\">=</span> <span class=\"n\">caches</span><span class=\"p\">[</span><span class=\"n\">l</span><span class=\"p\">]</span>\n        <span class=\"n\">dA_prev_temp</span><span class=\"p\">,</span> <span class=\"n\">dW_temp</span><span class=\"p\">,</span> <span class=\"n\">db_temp</span> <span class=\"o\">=</span> <span class=\"n\">linear_activation_backward</span><span class=\"p\">(</span><span class=\"n\">grads</span><span class=\"p\">[</span><span class=\"s1\">&#39;dA&#39;</span><span class=\"o\">+</span><span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)],</span> <span class=\"n\">current_cache</span><span class=\"p\">,</span> <span class=\"s1\">&#39;relu&#39;</span><span class=\"p\">)</span>\n        <span class=\"n\">grads</span><span class=\"p\">[</span><span class=\"s2\">&#34;dA&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">dA_prev_temp</span>\n        <span class=\"n\">grads</span><span class=\"p\">[</span><span class=\"s2\">&#34;dW&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">dW_temp</span>\n        <span class=\"n\">grads</span><span class=\"p\">[</span><span class=\"s2\">&#34;db&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">=</span> <span class=\"n\">db_temp</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n<span class=\"err\">​</span>\n    <span class=\"k\">return</span> <span class=\"n\">grads</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>Update Parameters</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: update_parameters</span>\n<span class=\"err\">​</span>\n<span class=\"k\">def</span> <span class=\"nf\">update_parameters</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">grads</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Update parameters using gradient descent\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    parameters -- python dictionary containing your parameters \n</span><span class=\"s2\">    grads -- python dictionary containing your gradients, output of L_model_backward\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    parameters -- python dictionary containing your updated parameters \n</span><span class=\"s2\">                  parameters[&#34;W&#34; + str(l)] = ... \n</span><span class=\"s2\">                  parameters[&#34;b&#34; + str(l)] = ...\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"n\">L</span> <span class=\"o\">=</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">)</span> <span class=\"o\">//</span> <span class=\"mi\">2</span> <span class=\"c1\"># number of layers in the neural network</span>\n<span class=\"err\">​</span>\n    <span class=\"c1\"># Update rule for each parameter. Use a for loop.</span>\n    <span class=\"c1\">### START CODE HERE ### (≈ 3 lines of code)</span>\n    <span class=\"k\">for</span> <span class=\"n\">l</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"n\">L</span><span class=\"p\">):</span>\n        <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;W&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">-=</span> <span class=\"n\">learning_rate</span> <span class=\"o\">*</span> <span class=\"n\">grads</span><span class=\"p\">[</span><span class=\"s1\">&#39;dW&#39;</span><span class=\"o\">+</span><span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span>\n        <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;b&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span> <span class=\"o\">-=</span> <span class=\"n\">learning_rate</span> <span class=\"o\">*</span> <span class=\"n\">grads</span><span class=\"p\">[</span><span class=\"s1\">&#39;db&#39;</span><span class=\"o\">+</span><span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">l</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)]</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    <span class=\"k\">return</span> <span class=\"n\">parameters</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>Part2</b></h2><p>有了part1中的函数，就很容易在part2中搭建模型和训练了。</p><p>依旧是识别猫猫的图片。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-9cbc30457780df7f97e20a22af6f02ac_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1480\" data-rawheight=\"810\" class=\"origin_image zh-lightbox-thumb\" width=\"1480\" data-original=\"https://pic1.zhimg.com/v2-9cbc30457780df7f97e20a22af6f02ac_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1480&#39; height=&#39;810&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1480\" data-rawheight=\"810\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1480\" data-original=\"https://pic1.zhimg.com/v2-9cbc30457780df7f97e20a22af6f02ac_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-9cbc30457780df7f97e20a22af6f02ac_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>开始先用两层的layer做训练，得到了精确度是72%，这里贴代码就好了，L层再详细说说</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\">### CONSTANTS DEFINING THE MODEL ####</span>\n<span class=\"n\">n_x</span> <span class=\"o\">=</span> <span class=\"mi\">12288</span>     <span class=\"c1\"># num_px * num_px * 3</span>\n<span class=\"n\">n_h</span> <span class=\"o\">=</span> <span class=\"mi\">7</span>\n<span class=\"n\">n_y</span> <span class=\"o\">=</span> <span class=\"mi\">1</span>\n<span class=\"n\">layers_dims</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">n_x</span><span class=\"p\">,</span> <span class=\"n\">n_h</span><span class=\"p\">,</span> <span class=\"n\">n_y</span><span class=\"p\">)</span>\n<span class=\"err\">​</span>\n<span class=\"err\">​</span>\n<span class=\"err\">​</span>\n<span class=\"c1\"># GRADED FUNCTION: two_layer_model</span>\n<span class=\"err\">​</span>\n<span class=\"k\">def</span> <span class=\"nf\">two_layer_model</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">layers_dims</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span> <span class=\"o\">=</span> <span class=\"mf\">0.0075</span><span class=\"p\">,</span> <span class=\"n\">num_iterations</span> <span class=\"o\">=</span> <span class=\"mi\">3000</span><span class=\"p\">,</span> <span class=\"n\">print_cost</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Implements a two-layer neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID.\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    X -- input data, of shape (n_x, number of examples)\n</span><span class=\"s2\">    Y -- true &#34;label&#34; vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n</span><span class=\"s2\">    layers_dims -- dimensions of the layers (n_x, n_h, n_y)\n</span><span class=\"s2\">    num_iterations -- number of iterations of the optimization loop\n</span><span class=\"s2\">    learning_rate -- learning rate of the gradient descent update rule\n</span><span class=\"s2\">    print_cost -- If set to True, this will print the cost every 100 iterations \n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    parameters -- a dictionary containing W1, W2, b1, and b2\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    \n    <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">seed</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n    <span class=\"n\">grads</span> <span class=\"o\">=</span> <span class=\"p\">{}</span>\n    <span class=\"n\">costs</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>                              <span class=\"c1\"># to keep track of the cost</span>\n    <span class=\"n\">m</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span>                           <span class=\"c1\"># number of examples</span>\n    <span class=\"p\">(</span><span class=\"n\">n_x</span><span class=\"p\">,</span> <span class=\"n\">n_h</span><span class=\"p\">,</span> <span class=\"n\">n_y</span><span class=\"p\">)</span> <span class=\"o\">=</span> <span class=\"n\">layers_dims</span>\n    \n    <span class=\"c1\"># Initialize parameters dictionary, by calling one of the functions you&#39;d previously implemented</span>\n    <span class=\"c1\">### START CODE HERE ### (≈ 1 line of code)</span>\n    <span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"n\">initialize_parameters</span><span class=\"p\">(</span><span class=\"n\">n_x</span><span class=\"p\">,</span> <span class=\"n\">n_h</span><span class=\"p\">,</span> <span class=\"n\">n_y</span><span class=\"p\">)</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"c1\"># Get W1, b1, W2 and b2 from the dictionary parameters.</span>\n    <span class=\"n\">W1</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;W1&#34;</span><span class=\"p\">]</span>\n    <span class=\"n\">b1</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;b1&#34;</span><span class=\"p\">]</span>\n    <span class=\"n\">W2</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;W2&#34;</span><span class=\"p\">]</span>\n    <span class=\"n\">b2</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;b2&#34;</span><span class=\"p\">]</span>\n    \n    <span class=\"c1\"># Loop (gradient descent)</span>\n<span class=\"err\">​</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">num_iterations</span><span class=\"p\">):</span>\n<span class=\"err\">​</span>\n        <span class=\"c1\"># Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID. Inputs: &#34;X, W1, b1, W2, b2&#34;. Output: &#34;A1, cache1, A2, cache2&#34;.</span>\n        <span class=\"c1\">### START CODE HERE ### (≈ 2 lines of code)</span>\n        <span class=\"n\">A1</span><span class=\"p\">,</span> <span class=\"n\">cache1</span> <span class=\"o\">=</span> <span class=\"n\">linear_activation_forward</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">W1</span><span class=\"p\">,</span> <span class=\"n\">b1</span><span class=\"p\">,</span> <span class=\"s1\">&#39;relu&#39;</span><span class=\"p\">)</span>\n        <span class=\"n\">A2</span><span class=\"p\">,</span> <span class=\"n\">cache2</span> <span class=\"o\">=</span> <span class=\"n\">linear_activation_forward</span><span class=\"p\">(</span><span class=\"n\">A1</span><span class=\"p\">,</span> <span class=\"n\">W2</span><span class=\"p\">,</span> <span class=\"n\">b2</span><span class=\"p\">,</span> <span class=\"s1\">&#39;sigmoid&#39;</span><span class=\"p\">)</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n        \n        <span class=\"c1\"># Compute cost</span>\n        <span class=\"c1\">### START CODE HERE ### (≈ 1 line of code)</span>\n        <span class=\"n\">cost</span> <span class=\"o\">=</span> <span class=\"n\">compute_cost</span><span class=\"p\">(</span><span class=\"n\">A2</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">)</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n        \n        <span class=\"c1\"># Initializing backward propagation</span>\n        <span class=\"n\">dA2</span> <span class=\"o\">=</span> <span class=\"o\">-</span> <span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">divide</span><span class=\"p\">(</span><span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">A2</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">divide</span><span class=\"p\">(</span><span class=\"mi\">1</span> <span class=\"o\">-</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"mi\">1</span> <span class=\"o\">-</span> <span class=\"n\">A2</span><span class=\"p\">))</span>\n        \n        <span class=\"c1\"># Backward propagation. Inputs: &#34;dA2, cache2, cache1&#34;. Outputs: &#34;dA1, dW2, db2; also dA0 (not used), dW1, db1&#34;.</span>\n        <span class=\"c1\">### START CODE HERE ### (≈ 2 lines of code)</span>\n        <span class=\"n\">dA1</span><span class=\"p\">,</span> <span class=\"n\">dW2</span><span class=\"p\">,</span> <span class=\"n\">db2</span> <span class=\"o\">=</span> <span class=\"n\">linear_activation_backward</span><span class=\"p\">(</span><span class=\"n\">dA2</span><span class=\"p\">,</span> <span class=\"n\">cache2</span><span class=\"p\">,</span> <span class=\"s1\">&#39;sigmoid&#39;</span><span class=\"p\">)</span>\n        <span class=\"n\">dA0</span><span class=\"p\">,</span> <span class=\"n\">dW1</span><span class=\"p\">,</span> <span class=\"n\">db1</span> <span class=\"o\">=</span> <span class=\"n\">linear_activation_backward</span><span class=\"p\">(</span><span class=\"n\">dA1</span><span class=\"p\">,</span> <span class=\"n\">cache1</span><span class=\"p\">,</span> <span class=\"s1\">&#39;relu&#39;</span><span class=\"p\">)</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n        \n        <span class=\"c1\"># Set grads[&#39;dWl&#39;] to dW1, grads[&#39;db1&#39;] to db1, grads[&#39;dW2&#39;] to dW2, grads[&#39;db2&#39;] to db2</span>\n        <span class=\"n\">grads</span><span class=\"p\">[</span><span class=\"s1\">&#39;dW1&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">dW1</span>\n        <span class=\"n\">grads</span><span class=\"p\">[</span><span class=\"s1\">&#39;db1&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">db1</span>\n        <span class=\"n\">grads</span><span class=\"p\">[</span><span class=\"s1\">&#39;dW2&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">dW2</span>\n        <span class=\"n\">grads</span><span class=\"p\">[</span><span class=\"s1\">&#39;db2&#39;</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">db2</span>\n        \n        <span class=\"c1\"># Update parameters.</span>\n        <span class=\"c1\">### START CODE HERE ### (approx. 1 line of code)</span>\n        <span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"n\">update_parameters</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">grads</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span><span class=\"p\">)</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n<span class=\"err\">​</span>\n        <span class=\"c1\"># Retrieve W1, b1, W2, b2 from parameters</span>\n        <span class=\"n\">W1</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;W1&#34;</span><span class=\"p\">]</span>\n        <span class=\"n\">b1</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;b1&#34;</span><span class=\"p\">]</span>\n        <span class=\"n\">W2</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;W2&#34;</span><span class=\"p\">]</span>\n        <span class=\"n\">b2</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s2\">&#34;b2&#34;</span><span class=\"p\">]</span>\n        \n        <span class=\"c1\"># Print the cost every 100 training example</span>\n        <span class=\"k\">if</span> <span class=\"n\">print_cost</span> <span class=\"ow\">and</span> <span class=\"n\">i</span> <span class=\"o\">%</span> <span class=\"mi\">100</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n            <span class=\"k\">print</span><span class=\"p\">(</span><span class=\"s2\">&#34;Cost after iteration {}: {}&#34;</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">squeeze</span><span class=\"p\">(</span><span class=\"n\">cost</span><span class=\"p\">)))</span>\n        <span class=\"k\">if</span> <span class=\"n\">print_cost</span> <span class=\"ow\">and</span> <span class=\"n\">i</span> <span class=\"o\">%</span> <span class=\"mi\">100</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n            <span class=\"n\">costs</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">cost</span><span class=\"p\">)</span>\n       \n    <span class=\"c1\"># plot the cost</span>\n<span class=\"err\">​</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">squeeze</span><span class=\"p\">(</span><span class=\"n\">costs</span><span class=\"p\">))</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">ylabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;cost&#39;</span><span class=\"p\">)</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">xlabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;iterations (per tens)&#39;</span><span class=\"p\">)</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">title</span><span class=\"p\">(</span><span class=\"s2\">&#34;Learning rate =&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">learning_rate</span><span class=\"p\">))</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">parameters</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>L-layer Neural Network</b></h2><p>使用之前的函数：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">initialize_parameters_deep</span><span class=\"p\">(</span><span class=\"n\">layers_dims</span><span class=\"p\">):</span>\n    <span class=\"o\">...</span>\n    <span class=\"k\">return</span> <span class=\"n\">parameters</span> \n<span class=\"k\">def</span> <span class=\"nf\">L_model_forward</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">parameters</span><span class=\"p\">):</span>\n    <span class=\"o\">...</span>\n    <span class=\"k\">return</span> <span class=\"n\">AL</span><span class=\"p\">,</span> <span class=\"n\">caches</span>\n<span class=\"k\">def</span> <span class=\"nf\">compute_cost</span><span class=\"p\">(</span><span class=\"n\">AL</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">):</span>\n    <span class=\"o\">...</span>\n    <span class=\"k\">return</span> <span class=\"n\">cost</span>\n<span class=\"k\">def</span> <span class=\"nf\">L_model_backward</span><span class=\"p\">(</span><span class=\"n\">AL</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">caches</span><span class=\"p\">):</span>\n    <span class=\"o\">...</span>\n    <span class=\"k\">return</span> <span class=\"n\">grads</span>\n<span class=\"k\">def</span> <span class=\"nf\">update_parameters</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">grads</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span><span class=\"p\">):</span>\n    <span class=\"o\">...</span>\n    <span class=\"k\">return</span> <span class=\"n\">parameters</span></code></pre></div><p>这里一共4层：</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"n\">layers_dims</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mi\">12288</span><span class=\"p\">,</span> <span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"mi\">7</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"c1\">#  4-layer model</span></code></pre></div><p>思路是：</p><ol><li>初始化参数</li><li>进入for的n次迭代循环：<br/></li><ol><li>L_model_forward(X, parameters) 得到 AL,caches</li><li>计算cost</li><li>L_model_backward(AL, Y, caches)计算grads</li><li>update_parameters(parameters, grads, learning_rate)更新参数</li><li>每100层记录一下cost的值</li></ol></ol><p class=\"ztext-empty-paragraph\"><br/></p><ol><li>画出cost梯度下降图</li></ol><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: L_layer_model</span>\n<span class=\"err\">​</span>\n<span class=\"k\">def</span> <span class=\"nf\">L_layer_model</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">layers_dims</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span> <span class=\"o\">=</span> <span class=\"mf\">0.0075</span><span class=\"p\">,</span> <span class=\"n\">num_iterations</span> <span class=\"o\">=</span> <span class=\"mi\">3000</span><span class=\"p\">,</span> <span class=\"n\">print_cost</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">):</span><span class=\"c1\">#lr was 0.009</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Implements a L-layer neural network: [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID.\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n</span><span class=\"s2\">    Y -- true &#34;label&#34; vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n</span><span class=\"s2\">    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n</span><span class=\"s2\">    learning_rate -- learning rate of the gradient descent update rule\n</span><span class=\"s2\">    num_iterations -- number of iterations of the optimization loop\n</span><span class=\"s2\">    print_cost -- if True, it prints the cost every 100 steps\n</span><span class=\"s2\">    \n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    parameters -- parameters learnt by the model. They can then be used to predict.\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n<span class=\"err\">​</span>\n    <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">seed</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n    <span class=\"n\">costs</span> <span class=\"o\">=</span> <span class=\"p\">[]</span>                         <span class=\"c1\"># keep track of cost</span>\n    \n    <span class=\"c1\"># Parameters initialization. (≈ 1 line of code)</span>\n    <span class=\"c1\">### START CODE HERE ###</span>\n    <span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"n\">initialize_parameters_deep</span><span class=\"p\">(</span><span class=\"n\">layers_dims</span><span class=\"p\">)</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    \n    <span class=\"c1\"># Loop (gradient descent)</span>\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">num_iterations</span><span class=\"p\">):</span>\n<span class=\"err\">​</span>\n        <span class=\"c1\"># Forward propagation: [LINEAR -&gt; RELU]*(L-1) -&gt; LINEAR -&gt; SIGMOID.</span>\n        <span class=\"c1\">### START CODE HERE ### (≈ 1 line of code)</span>\n        <span class=\"n\">AL</span><span class=\"p\">,</span> <span class=\"n\">caches</span> <span class=\"o\">=</span> <span class=\"n\">L_model_forward</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">parameters</span><span class=\"p\">)</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n        \n        <span class=\"c1\"># Compute cost.</span>\n        <span class=\"c1\">### START CODE HERE ### (≈ 1 line of code)</span>\n        <span class=\"n\">cost</span> <span class=\"o\">=</span> <span class=\"n\">compute_cost</span><span class=\"p\">(</span><span class=\"n\">AL</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">)</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n    \n        <span class=\"c1\"># Backward propagation.</span>\n        <span class=\"c1\">### START CODE HERE ### (≈ 1 line of code)</span>\n        <span class=\"n\">grads</span> <span class=\"o\">=</span> <span class=\"n\">L_model_backward</span><span class=\"p\">(</span><span class=\"n\">AL</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">caches</span><span class=\"p\">)</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n \n        <span class=\"c1\"># Update parameters.</span>\n        <span class=\"c1\">### START CODE HERE ### (≈ 1 line of code)</span>\n        <span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"n\">update_parameters</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">grads</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span><span class=\"p\">)</span>\n        <span class=\"c1\">### END CODE HERE ###</span>\n                \n        <span class=\"c1\"># Print the cost every 100 training example</span>\n        <span class=\"k\">if</span> <span class=\"n\">print_cost</span> <span class=\"ow\">and</span> <span class=\"n\">i</span> <span class=\"o\">%</span> <span class=\"mi\">100</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n            <span class=\"k\">print</span> <span class=\"p\">(</span><span class=\"s2\">&#34;Cost after iteration </span><span class=\"si\">%i</span><span class=\"s2\">: </span><span class=\"si\">%f</span><span class=\"s2\">&#34;</span> <span class=\"o\">%</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">cost</span><span class=\"p\">))</span>\n        <span class=\"k\">if</span> <span class=\"n\">print_cost</span> <span class=\"ow\">and</span> <span class=\"n\">i</span> <span class=\"o\">%</span> <span class=\"mi\">100</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n            <span class=\"n\">costs</span><span class=\"o\">.</span><span class=\"n\">append</span><span class=\"p\">(</span><span class=\"n\">cost</span><span class=\"p\">)</span>\n            \n    <span class=\"c1\"># plot the cost</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">plot</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">squeeze</span><span class=\"p\">(</span><span class=\"n\">costs</span><span class=\"p\">))</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">ylabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;cost&#39;</span><span class=\"p\">)</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">xlabel</span><span class=\"p\">(</span><span class=\"s1\">&#39;iterations (per tens)&#39;</span><span class=\"p\">)</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">title</span><span class=\"p\">(</span><span class=\"s2\">&#34;Learning rate =&#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"n\">learning_rate</span><span class=\"p\">))</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">show</span><span class=\"p\">()</span>\n    \n    <span class=\"k\">return</span> <span class=\"n\">parameters</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>2500的迭代次数，精度达到了80%！</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>小结</b></p><p>过程其实是很清晰的，就是先初始化参数；再开始循环，循环中先计算前向传播，得到最后一层的AL，以及每一层的cache，其中cache包括了 A_prev，W，b，Z；然后计算一下每一次迭代的cost；再进行反向传播，得到每一层的梯度dA,dW,db;记得每100次迭代记录一下cost值，这样就可以画出cost是如何下降的了。</p><p>part1构建的那些函数，一步步来是比较简单的，但是如果自己要一下子想出来的话，也很难想得到。所以思路要清晰，一步一步来，才能构建好函数！</p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "神经网络", 
                    "tagLink": "https://api.zhihu.com/topics/19607065"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/44674206", 
            "userName": "直上云霄", 
            "userLink": "https://www.zhihu.com/people/1033165ce4ad9c3fce69a0793dfab8ad", 
            "upvote": 0, 
            "title": "DeepLearning.ai笔记:(1-4)-深层神经网络", 
            "content": "<h2>本文首发于个人博客：<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">fangzh.top</span><span class=\"invisible\"></span></a>，欢迎来访</h2><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>这一周主要讲了深层的神经网络搭建。</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>深层神经网络的符号表示</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-b1c7c7c0a4d0ac817ff518034b5c7427_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"998\" data-rawheight=\"502\" class=\"origin_image zh-lightbox-thumb\" width=\"998\" data-original=\"https://pic4.zhimg.com/v2-b1c7c7c0a4d0ac817ff518034b5c7427_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;998&#39; height=&#39;502&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"998\" data-rawheight=\"502\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"998\" data-original=\"https://pic4.zhimg.com/v2-b1c7c7c0a4d0ac817ff518034b5c7427_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-b1c7c7c0a4d0ac817ff518034b5c7427_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>在深层的神经网络中，</p><ul><li><img src=\"https://www.zhihu.com/equation?tex=L\" alt=\"L\" eeimg=\"1\"/> 表示神经网络的层数 <img src=\"https://www.zhihu.com/equation?tex=+L+%3D+4\" alt=\" L = 4\" eeimg=\"1\"/> </li><li><img src=\"https://www.zhihu.com/equation?tex=n%5E%7B%5Bl%5D%7D\" alt=\"n^{[l]}\" eeimg=\"1\"/> 表示第 <img src=\"https://www.zhihu.com/equation?tex=l\" alt=\"l\" eeimg=\"1\"/> 层的神经网络个数</li><li><img src=\"https://www.zhihu.com/equation?tex=W%5E%7B%5Bl%5D%7D%3A+%28n%5E%7B%5Bl%5D%7D%2Cn%5E%7Bl-1%7D%29\" alt=\"W^{[l]}: (n^{[l]},n^{l-1})\" eeimg=\"1\"/> </li><li><img src=\"https://www.zhihu.com/equation?tex=dW%5E%7B%5Bl%5D%7D%3A+%28n%5E%7B%5Bl%5D%7D%2Cn%5E%7Bl-1%7D%29\" alt=\"dW^{[l]}: (n^{[l]},n^{l-1})\" eeimg=\"1\"/> </li><li><img src=\"https://www.zhihu.com/equation?tex=b%5E%7B%5Bl%5D%7D%3A+%28n%5E%7B%5Bl%5D%7D%2C1%29\" alt=\"b^{[l]}: (n^{[l]},1)\" eeimg=\"1\"/> </li><li><img src=\"https://www.zhihu.com/equation?tex=db%5E%7B%5Bl%5D%7D%3A+%28n%5E%7B%5Bl%5D%7D%2C1%29\" alt=\"db^{[l]}: (n^{[l]},1)\" eeimg=\"1\"/> </li><li><img src=\"https://www.zhihu.com/equation?tex=z%5E%7B%5Bl%5D%7D%3A%28n%5E%7B%5Bl%5D%7D%2C1%29\" alt=\"z^{[l]}:(n^{[l]},1)\" eeimg=\"1\"/> </li><li><img src=\"https://www.zhihu.com/equation?tex=a%5E%7B%5Bl%5D%7D%3A%28n%5E%7B%5Bl%5D%7D%2C1%29\" alt=\"a^{[l]}:(n^{[l]},1)\" eeimg=\"1\"/> </li></ul><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>前向传播和反向传播</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p><b>前向传播</b></p><p>input <img src=\"https://www.zhihu.com/equation?tex=a%5E%7B%5Bl-1%5D%7D\" alt=\"a^{[l-1]}\" eeimg=\"1\"/> </p><p>output <img src=\"https://www.zhihu.com/equation?tex=+a%5E%7B%5Bl%5D%7D%2Ccache+%28z%5E%7B%5Bl%5D%7D%29\" alt=\" a^{[l]},cache (z^{[l]})\" eeimg=\"1\"/> ，其中cache也顺便把 <img src=\"https://www.zhihu.com/equation?tex=W%5E%7B%5Bl%5D%7D%2C++b%5E%7B%5Bl%5D%7D\" alt=\"W^{[l]},  b^{[l]}\" eeimg=\"1\"/> 也保存下来了</p><p>所以，前向传播的公式可以写作：</p><p><img src=\"https://www.zhihu.com/equation?tex=Z%5E%7B%5Bl%5D%7D+%3D+W%5E%7B%5Bl%5D%7D+A%5E%7B%5Bl-1%5D%7D+%2B+b%5E%7B%5Bl%5D%7D\" alt=\"Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=A%5E%7B%5Bl%5D%7D+%3D+g%5E%7B%5Bl%5D%7D%28Z%5E%7B%5Bl%5D%7D%29\" alt=\"A^{[l]} = g^{[l]}(Z^{[l]})\" eeimg=\"1\"/> </p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>维度</b></p><p>假设有m个样本，那么 <img src=\"https://www.zhihu.com/equation?tex=Z%5E%7B%5Bl%5D%7D\" alt=\"Z^{[l]}\" eeimg=\"1\"/> 维度就是 <img src=\"https://www.zhihu.com/equation?tex=%28n%5E%7B%5Bl%5D%7D%2C+m%29\" alt=\"(n^{[l]}, m)\" eeimg=\"1\"/> ， <img src=\"https://www.zhihu.com/equation?tex=A%5E%7B%5Bl%5D%7D\" alt=\"A^{[l]}\" eeimg=\"1\"/> 的维度和 <img src=\"https://www.zhihu.com/equation?tex=Z%5E%7B%5Bl%5D%7D\" alt=\"Z^{[l]}\" eeimg=\"1\"/> 一样。</p><p>那么 <img src=\"https://www.zhihu.com/equation?tex=W%5E%7B%5Bl%5D%7D+A%5E%7B%5Bl-1%5D%7D\" alt=\"W^{[l]} A^{[l-1]}\" eeimg=\"1\"/> 维度就是 <img src=\"https://www.zhihu.com/equation?tex=%28n%5E%7B%5Bl%5D%7D%2Cn%5E%7Bl-1%7D%29++%5Ctimes++%28n%5E%7B%5Bl-1%5D%7D%2Cm%29\" alt=\"(n^{[l]},n^{l-1})  \\times  (n^{[l-1]},m)\" eeimg=\"1\"/>  也就是  <img src=\"https://www.zhihu.com/equation?tex=%28n%5E%7B%5Bl%5D%7D%2C+m%29\" alt=\"(n^{[l]}, m)\" eeimg=\"1\"/> ，这个时候，还需要加上 <img src=\"https://www.zhihu.com/equation?tex=b%5E%7B%5Bl%5D%7D\" alt=\"b^{[l]}\" eeimg=\"1\"/> ，而 <img src=\"https://www.zhihu.com/equation?tex=b%5E%7B%5Bl%5D%7D\" alt=\"b^{[l]}\" eeimg=\"1\"/> 本身的维度是 <img src=\"https://www.zhihu.com/equation?tex=%28n%5E%7B%5Bl%5D%7D%2C1%29\" alt=\"(n^{[l]},1)\" eeimg=\"1\"/> ，借助python的广播，扩充到了m个维度。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>反向传播</b></p><p>input <img src=\"https://www.zhihu.com/equation?tex=da%5E%7B%5Bl%5D%7D\" alt=\"da^{[l]}\" eeimg=\"1\"/> </p><p>output <img src=\"https://www.zhihu.com/equation?tex=da%5E%7B%5Bl-1%5D%7D+%2C+dW%5E%7B%5Bl%5D%7D+%2C+db%5E%7B%5Bl%5D%7D\" alt=\"da^{[l-1]} , dW^{[l]} , db^{[l]}\" eeimg=\"1\"/> </p><p>公式：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-1bfccc4955853c52ac803f3bd5984b01_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"261\" data-rawheight=\"131\" class=\"content_image\" width=\"261\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;261&#39; height=&#39;131&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"261\" data-rawheight=\"131\" class=\"content_image lazy\" width=\"261\" data-actualsrc=\"https://pic2.zhimg.com/v2-1bfccc4955853c52ac803f3bd5984b01_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>向量化：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-5d0b14d815bb8b995782e5bc83f5205e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"514\" data-rawheight=\"165\" class=\"origin_image zh-lightbox-thumb\" width=\"514\" data-original=\"https://pic3.zhimg.com/v2-5d0b14d815bb8b995782e5bc83f5205e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;514&#39; height=&#39;165&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"514\" data-rawheight=\"165\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"514\" data-original=\"https://pic3.zhimg.com/v2-5d0b14d815bb8b995782e5bc83f5205e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-5d0b14d815bb8b995782e5bc83f5205e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>正向传播和反向传播如图：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-394d24a0c677c67b764e7e70a00df14e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1124\" data-rawheight=\"557\" class=\"origin_image zh-lightbox-thumb\" width=\"1124\" data-original=\"https://pic3.zhimg.com/v2-394d24a0c677c67b764e7e70a00df14e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1124&#39; height=&#39;557&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1124\" data-rawheight=\"557\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1124\" data-original=\"https://pic3.zhimg.com/v2-394d24a0c677c67b764e7e70a00df14e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-394d24a0c677c67b764e7e70a00df14e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>具体过程为，第一层和第二层用Relu函数，第三层输出用sigmoid，这个时候的输出值是 <img src=\"https://www.zhihu.com/equation?tex=a%5E%7B%5B3%5D%7D\" alt=\"a^{[3]}\" eeimg=\"1\"/> </p><p>而首先进行反向传播的时候先求得 <img src=\"https://www.zhihu.com/equation?tex=da%5E%7B%5B3%5D%7D+%3D+-+%5Cfrac%7By%7D%7Ba%7D+-+%5Cfrac%7B1-y%7D%7B1-a%7D\" alt=\"da^{[3]} = - \\frac{y}{a} - \\frac{1-y}{1-a}\" eeimg=\"1\"/> ，然后再包括之前存在cache里面的 <img src=\"https://www.zhihu.com/equation?tex=z%5E%7B%5B3%5D%7D\" alt=\"z^{[3]}\" eeimg=\"1\"/> ,反向传播可以得到 <img src=\"https://www.zhihu.com/equation?tex=dw%5E%7B%5B3%5D%7D%2C+db%5E%7B%5B3%5D%7D%2Cda%5E%7B%5B2%5D%7D\" alt=\"dw^{[3]}, db^{[3]},da^{[2]}\" eeimg=\"1\"/> ，然后继续反向，直到得到了 <img src=\"https://www.zhihu.com/equation?tex=dw%5E%7B%5B1%5D%7D%2Cdb%5E%7B%5B1%5D%7D\" alt=\"dw^{[1]},db^{[1]}\" eeimg=\"1\"/> 后，更新一下w，b的参数，然后继续做前向传播、反向传播，不断循环。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>Why Deep？</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-7ce4ac52be3a657bce1ed18f38697bee_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1216\" data-rawheight=\"515\" class=\"origin_image zh-lightbox-thumb\" width=\"1216\" data-original=\"https://pic3.zhimg.com/v2-7ce4ac52be3a657bce1ed18f38697bee_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1216&#39; height=&#39;515&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1216\" data-rawheight=\"515\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1216\" data-original=\"https://pic3.zhimg.com/v2-7ce4ac52be3a657bce1ed18f38697bee_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-7ce4ac52be3a657bce1ed18f38697bee_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>如图直观上感觉，比如第一层，它会先识别出一些边缘信息；第二层则将这些边缘进行整合，得到一些五官信息，如眼睛、嘴巴等；到了第三层，就可以将这些信息整合起来，输出一张人脸了。</p><p>如果网络层数不够深的话，可以组合的情况就很少，或者需要类似门电路那样，用单层很多个特征才能得到和深层神经网络类似的效果。</p><h2><b>搭建深层神经网络块</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-d4fb44293f189d94a830d737179e12d5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"585\" data-rawheight=\"640\" class=\"origin_image zh-lightbox-thumb\" width=\"585\" data-original=\"https://pic2.zhimg.com/v2-d4fb44293f189d94a830d737179e12d5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;585&#39; height=&#39;640&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"585\" data-rawheight=\"640\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"585\" data-original=\"https://pic2.zhimg.com/v2-d4fb44293f189d94a830d737179e12d5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-d4fb44293f189d94a830d737179e12d5_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>和之前说的一样，一个网络块中包含了前向传播和反向传播。</p><p>前向输入 <img src=\"https://www.zhihu.com/equation?tex=a%5E%7B%5Bl-1%5D%7D\" alt=\"a^{[l-1]}\" eeimg=\"1\"/> ，经过神经网络的计算， <img src=\"https://www.zhihu.com/equation?tex=g%5E%7B%5Bl%5D%7D%28w%5E%7B%5Bl%5D%7Da%5E%7B%5Bl-1%5D%7D+%2B+b%5E%7B%5Bl%5D%7D%29\" alt=\"g^{[l]}(w^{[l]}a^{[l-1]} + b^{[l]})\" eeimg=\"1\"/> 得到 <img src=\"https://www.zhihu.com/equation?tex=a%5E%7B%5Bl%5D%7D\" alt=\"a^{[l]}\" eeimg=\"1\"/> </p><p>反向传播，输入 <img src=\"https://www.zhihu.com/equation?tex=da%5E%7B%5Bl%5D%7D\" alt=\"da^{[l]}\" eeimg=\"1\"/> ，再有之前在cache的 <img src=\"https://www.zhihu.com/equation?tex=z%5E%7B%5Bl%5D%7D\" alt=\"z^{[l]}\" eeimg=\"1\"/> ,即可得到 <img src=\"https://www.zhihu.com/equation?tex=dw%5E%7B%5Bl%5D%7D%2Cdb%5E%7B%5Bl%5D%7D\" alt=\"dw^{[l]},db^{[l]}\" eeimg=\"1\"/> 还有上一层的 <img src=\"https://www.zhihu.com/equation?tex=da%5E%7B%5Bl-1%5D%7D\" alt=\"da^{[l-1]}\" eeimg=\"1\"/> </p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>参数与超参数</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p>超参数就是你自己调的，玄学参数：</p><ul><li>learning_rate</li><li>iterations</li><li>L = len(hidden layer)</li><li><img src=\"https://www.zhihu.com/equation?tex=n%5E%7B%5Bl%5D%7D\" alt=\"n^{[l]}\" eeimg=\"1\"/> </li><li>activation function</li><li>mini batch size（最小的计算批）</li><li>regularization（正则）</li><li>momentum（动量）</li></ul>", 
            "topic": [
                {
                    "tag": "神经网络", 
                    "tagLink": "https://api.zhihu.com/topics/19607065"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/44396545", 
            "userName": "直上云霄", 
            "userLink": "https://www.zhihu.com/people/1033165ce4ad9c3fce69a0793dfab8ad", 
            "upvote": 0, 
            "title": "DeepLearning.ai作业:(1-3)-浅层神经网络", 
            "content": "<h2>本文首发于个人博客：<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">fangzh.top</span><span class=\"invisible\"></span></a></h2><h2>前言：</h2><ol><li>不要抄作业！</li><li>我只是把思路整理了，供个人学习。</li><li>不要抄作业！</li></ol><h2>数据集</h2><p>数据集是一个类似花的数据集。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-6ce4b7b2aed0978f7c95f2a6a350a124_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"515\" data-rawheight=\"349\" class=\"origin_image zh-lightbox-thumb\" width=\"515\" data-original=\"https://pic1.zhimg.com/v2-6ce4b7b2aed0978f7c95f2a6a350a124_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;515&#39; height=&#39;349&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"515\" data-rawheight=\"349\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"515\" data-original=\"https://pic1.zhimg.com/v2-6ce4b7b2aed0978f7c95f2a6a350a124_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-6ce4b7b2aed0978f7c95f2a6a350a124_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>而如果用传统的logistic regression，做出来的就是一个二分类问题，简单粗暴的划出了一条线，</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-c789940ce3bfc86580b8e3e697e2049b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"532\" data-rawheight=\"391\" class=\"origin_image zh-lightbox-thumb\" width=\"532\" data-original=\"https://pic4.zhimg.com/v2-c789940ce3bfc86580b8e3e697e2049b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;532&#39; height=&#39;391&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"532\" data-rawheight=\"391\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"532\" data-original=\"https://pic4.zhimg.com/v2-c789940ce3bfc86580b8e3e697e2049b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-c789940ce3bfc86580b8e3e697e2049b_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>可以看见，准确率只有47%。</p><p>所以就需要构建神经网络模型了。</p><h2>神经网络模型</h2><p><b>Reminder</b>: The general methodology to build a Neural Network is to:</p><div class=\"highlight\"><pre><code class=\"language-text\">1. Define the neural network structure ( # of input units,  # of hidden units, etc). \n2. Initialize the model&#39;s parameters\n3. Loop:\n    - Implement forward propagation\n    - Compute loss\n    - Implement backward propagation to get the gradients\n    - Update parameters (gradient descent)</code></pre></div><p>已经给出思路了：</p><ol><li>定义神经网络的结构</li><li>初始化模型参数</li><li>循环：</li><ol><li>计算正向传播</li><li>计算损失函数</li><li>计算反向传播来得到grad</li><li>更新参数</li></ol></ol><h2>1. 定义神经网络结构</h2><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: layer_sizes</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">layer_sizes</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    X -- input dataset of shape (input size, number of examples)\n</span><span class=\"s2\">    Y -- labels of shape (output size, number of examples)\n</span><span class=\"s2\">\n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    n_x -- the size of the input layer\n</span><span class=\"s2\">    n_h -- the size of the hidden layer\n</span><span class=\"s2\">    n_y -- the size of the output layer\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    <span class=\"c1\">### START CODE HERE ### (≈ 3 lines of code)</span>\n    <span class=\"n\">n_x</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"c1\"># size of input layer</span>\n    <span class=\"n\">n_h</span> <span class=\"o\">=</span> <span class=\"mi\">4</span>\n    <span class=\"n\">n_y</span> <span class=\"o\">=</span> <span class=\"n\">Y</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">]</span> <span class=\"c1\"># size of output layer</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    <span class=\"k\">return</span> <span class=\"p\">(</span><span class=\"n\">n_x</span><span class=\"p\">,</span> <span class=\"n\">n_h</span><span class=\"p\">,</span> <span class=\"n\">n_y</span><span class=\"p\">)</span></code></pre></div><h2>2. 初始化参数</h2><p>来初始化w和b的参数</p><p>w: <code>np.random.rand(a,b) * 0.01</code></p><p>b: <code>np.zeros((a,b))</code></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: initialize_parameters</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">initialize_parameters</span><span class=\"p\">(</span><span class=\"n\">n_x</span><span class=\"p\">,</span> <span class=\"n\">n_h</span><span class=\"p\">,</span> <span class=\"n\">n_y</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Argument:\n</span><span class=\"s2\">    n_x -- size of the input layer\n</span><span class=\"s2\">    n_h -- size of the hidden layer\n</span><span class=\"s2\">    n_y -- size of the output layer\n</span><span class=\"s2\">\n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    params -- python dictionary containing your parameters:\n</span><span class=\"s2\">                    W1 -- weight matrix of shape (n_h, n_x)\n</span><span class=\"s2\">                    b1 -- bias vector of shape (n_h, 1)\n</span><span class=\"s2\">                    W2 -- weight matrix of shape (n_y, n_h)\n</span><span class=\"s2\">                    b2 -- bias vector of shape (n_y, 1)\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n\n    <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">seed</span><span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"p\">)</span> <span class=\"c1\"># we set up a seed so that your output matches ours although the initialization is random.</span>\n\n    <span class=\"c1\">### START CODE HERE ### (≈ 4 lines of code)</span>\n    <span class=\"n\">W1</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">n_h</span><span class=\"p\">,</span> <span class=\"n\">n_x</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"mf\">0.01</span>\n    <span class=\"n\">b1</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">n_h</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n    <span class=\"n\">W2</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">randn</span><span class=\"p\">(</span><span class=\"n\">n_y</span><span class=\"p\">,</span> <span class=\"n\">n_h</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"mf\">0.01</span>\n    <span class=\"n\">b2</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">zeros</span><span class=\"p\">((</span><span class=\"n\">n_y</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n\n    <span class=\"k\">assert</span> <span class=\"p\">(</span><span class=\"n\">W1</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"p\">(</span><span class=\"n\">n_h</span><span class=\"p\">,</span> <span class=\"n\">n_x</span><span class=\"p\">))</span>\n    <span class=\"k\">assert</span> <span class=\"p\">(</span><span class=\"n\">b1</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"p\">(</span><span class=\"n\">n_h</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n    <span class=\"k\">assert</span> <span class=\"p\">(</span><span class=\"n\">W2</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"p\">(</span><span class=\"n\">n_y</span><span class=\"p\">,</span> <span class=\"n\">n_h</span><span class=\"p\">))</span>\n    <span class=\"k\">assert</span> <span class=\"p\">(</span><span class=\"n\">b2</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"p\">(</span><span class=\"n\">n_y</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">))</span>\n\n    <span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">&#34;W1&#34;</span><span class=\"p\">:</span> <span class=\"n\">W1</span><span class=\"p\">,</span>\n                  <span class=\"s2\">&#34;b1&#34;</span><span class=\"p\">:</span> <span class=\"n\">b1</span><span class=\"p\">,</span>\n                  <span class=\"s2\">&#34;W2&#34;</span><span class=\"p\">:</span> <span class=\"n\">W2</span><span class=\"p\">,</span>\n                  <span class=\"s2\">&#34;b2&#34;</span><span class=\"p\">:</span> <span class=\"n\">b2</span><span class=\"p\">}</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">parameters</span></code></pre></div><h2>3. loop</h2><p>在这里可以使用sigmoid()来做输出层的函数，np.tanh()来做hidden layer的激活函数。</p><h2>3.1 forward propagation</h2><p>在这个函数中，输入的是X，和parameters，然后就可以根据</p><p><img src=\"https://www.zhihu.com/equation?tex=z%5E%7B%5B1%5D+%28i%29%7D+%3D++W%5E%7B%5B1%5D%7D+x%5E%7B%28i%29%7D+%2B+b%5E%7B%5B1%5D%7D%5Ctag%7B1%7D\" alt=\"z^{[1] (i)} =  W^{[1]} x^{(i)} + b^{[1]}\\tag{1}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=a%5E%7B%5B1%5D+%28i%29%7D+%3D+%5Ctanh%28z%5E%7B%5B1%5D+%28i%29%7D%29%5Ctag%7B2%7D\" alt=\"a^{[1] (i)} = \\tanh(z^{[1] (i)})\\tag{2}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=z%5E%7B%5B2%5D+%28i%29%7D+%3D+W%5E%7B%5B2%5D%7D+a%5E%7B%5B1%5D+%28i%29%7D+%2B+b%5E%7B%5B2%5D%7D%5Ctag%7B3%7D\" alt=\"z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2]}\\tag{3}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%5Chat%7By%7D%5E%7B%28i%29%7D+%3D+a%5E%7B%5B2%5D+%28i%29%7D+%3D+%5Csigma%28z%5E%7B+%5B2%5D+%28i%29%7D%29%5Ctag%7B4%7D\" alt=\"\\hat{y}^{(i)} = a^{[2] (i)} = \\sigma(z^{ [2] (i)})\\tag{4}\" eeimg=\"1\"/> </p><p>得到每一层的Z和A了。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: forward_propagation</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">forward_propagation</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">parameters</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Argument:\n</span><span class=\"s2\">    X -- input data of size (n_x, m)\n</span><span class=\"s2\">    parameters -- python dictionary containing your parameters (output of initialization function)\n</span><span class=\"s2\">\n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    A2 -- The sigmoid output of the second activation\n</span><span class=\"s2\">    cache -- a dictionary containing &#34;Z1&#34;, &#34;A1&#34;, &#34;Z2&#34; and &#34;A2&#34;\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    <span class=\"c1\"># Retrieve each parameter from the dictionary &#34;parameters&#34;</span>\n    <span class=\"c1\">### START CODE HERE ### (≈ 4 lines of code)</span>\n    <span class=\"n\">W1</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;W1&#39;</span><span class=\"p\">]</span>\n    <span class=\"n\">b1</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;b1&#39;</span><span class=\"p\">]</span>\n    <span class=\"n\">W2</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;W2&#39;</span><span class=\"p\">]</span>\n    <span class=\"n\">b2</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;b2&#39;</span><span class=\"p\">]</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n\n    <span class=\"c1\"># Implement Forward Propagation to calculate A2 (probabilities)</span>\n    <span class=\"c1\">### START CODE HERE ### (≈ 4 lines of code)</span>\n    <span class=\"n\">Z1</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">W1</span><span class=\"p\">,</span><span class=\"n\">X</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">b1</span>\n    <span class=\"n\">A1</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">tanh</span><span class=\"p\">(</span><span class=\"n\">Z1</span><span class=\"p\">)</span>\n    <span class=\"n\">Z2</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">W2</span><span class=\"p\">,</span><span class=\"n\">A1</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">b2</span>\n    <span class=\"n\">A2</span> <span class=\"o\">=</span> <span class=\"n\">sigmoid</span><span class=\"p\">(</span><span class=\"n\">Z2</span><span class=\"p\">)</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n\n    <span class=\"k\">assert</span><span class=\"p\">(</span><span class=\"n\">A2</span><span class=\"o\">.</span><span class=\"n\">shape</span> <span class=\"o\">==</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]))</span>\n\n    <span class=\"n\">cache</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">&#34;Z1&#34;</span><span class=\"p\">:</span> <span class=\"n\">Z1</span><span class=\"p\">,</span>\n             <span class=\"s2\">&#34;A1&#34;</span><span class=\"p\">:</span> <span class=\"n\">A1</span><span class=\"p\">,</span>\n             <span class=\"s2\">&#34;Z2&#34;</span><span class=\"p\">:</span> <span class=\"n\">Z2</span><span class=\"p\">,</span>\n             <span class=\"s2\">&#34;A2&#34;</span><span class=\"p\">:</span> <span class=\"n\">A2</span><span class=\"p\">}</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">A2</span><span class=\"p\">,</span> <span class=\"n\">cache</span></code></pre></div><h2>3.2 cost</h2><p>接下来，在得到A2的值后，就可以根据公式来计算损失函数了。</p><p><img src=\"https://www.zhihu.com/equation?tex=J+%3D+-+%5Cfrac%7B1%7D%7Bm%7D+%5Csum%5Climits_%7Bi+%3D+0%7D%5E%7Bm%7D+%5Clarge%7B%28%7D+%5Csmall+y%5E%7B%28i%29%7D%5Clog%5Cleft%28a%5E%7B%5B2%5D+%28i%29%7D%5Cright%29+%2B+%281-y%5E%7B%28i%29%7D%29%5Clog%5Cleft%281-+a%5E%7B%5B2%5D+%28i%29%7D%5Cright%29+%5Clarge%7B%29%7D+%5Csmall\" alt=\"J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large{(} \\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right) \\large{)} \\small\" eeimg=\"1\"/> </p><p>在这里需要注意的是交叉熵的计算，交叉熵使用np.multiply()来计算，然后用np.sum()，求和。</p><p>而单单计算<code>logprobs = np.multiply(np.log(A2),Y)</code>是不够的，因为这个只得到了公式的前一半的部分，Y=0的部分在元素相乘中就相当于没有了，所以还要再后面加一项<code>np.multiply(np.log(1-A2),1-Y)</code></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: compute_cost</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">compute_cost</span><span class=\"p\">(</span><span class=\"n\">A2</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">parameters</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Computes the cross-entropy cost given in equation (13)\n</span><span class=\"s2\">\n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n</span><span class=\"s2\">    Y -- &#34;true&#34; labels vector of shape (1, number of examples)\n</span><span class=\"s2\">    parameters -- python dictionary containing your parameters W1, b1, W2 and b2\n</span><span class=\"s2\">\n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    cost -- cross-entropy cost given equation (13)\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n\n    <span class=\"n\">m</span> <span class=\"o\">=</span> <span class=\"n\">Y</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span> <span class=\"c1\"># number of example</span>\n\n    <span class=\"c1\"># Compute the cross-entropy cost</span>\n    <span class=\"c1\">### START CODE HERE ### (≈ 2 lines of code)</span>\n    <span class=\"n\">logprobs</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">multiply</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">log</span><span class=\"p\">(</span><span class=\"n\">A2</span><span class=\"p\">),</span><span class=\"n\">Y</span><span class=\"p\">)</span>  <span class=\"o\">+</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">multiply</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">log</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"o\">-</span><span class=\"n\">A2</span><span class=\"p\">),</span><span class=\"mi\">1</span><span class=\"o\">-</span><span class=\"n\">Y</span><span class=\"p\">)</span>\n    <span class=\"n\">cost</span> <span class=\"o\">=</span>  <span class=\"o\">-</span><span class=\"mi\">1</span> <span class=\"o\">/</span> <span class=\"n\">m</span> <span class=\"o\">*</span>  <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">logprobs</span><span class=\"p\">)</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n    <span class=\"n\">cost</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">squeeze</span><span class=\"p\">(</span><span class=\"n\">cost</span><span class=\"p\">)</span>     <span class=\"c1\"># makes sure cost is the dimension we expect. </span>\n                                <span class=\"c1\"># E.g., turns [[17]] into 17 </span>\n    <span class=\"k\">assert</span><span class=\"p\">(</span><span class=\"nb\">isinstance</span><span class=\"p\">(</span><span class=\"n\">cost</span><span class=\"p\">,</span> <span class=\"nb\">float</span><span class=\"p\">))</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">cost</span></code></pre></div><h2>3.3 backworad propagation</h2><p>NG说神经网络中最难理解的是这个，但是现在公式已经帮我们推倒好了。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-76fd95df5ad1e79b96b150233d02c4a3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2364\" data-rawheight=\"1264\" class=\"origin_image zh-lightbox-thumb\" width=\"2364\" data-original=\"https://pic4.zhimg.com/v2-76fd95df5ad1e79b96b150233d02c4a3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;2364&#39; height=&#39;1264&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"2364\" data-rawheight=\"1264\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"2364\" data-original=\"https://pic4.zhimg.com/v2-76fd95df5ad1e79b96b150233d02c4a3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-76fd95df5ad1e79b96b150233d02c4a3_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>其中， <img src=\"https://www.zhihu.com/equation?tex=+g%5E%7B%5B1%5D%27%7D%28Z%5E%7B%5B1%5D%7D%29\" alt=\" g^{[1]&#39;}(Z^{[1]})\" eeimg=\"1\"/> using<code>(1 - np.power(A1, 2))</code></p><p>可以看到，公式中需要的变量有X,Y,A,W,然后输出一个字典结构的grads</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"k\">def</span> <span class=\"nf\">backward_propagation</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">cache</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Implement the backward propagation using the instructions above.\n</span><span class=\"s2\">\n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    parameters -- python dictionary containing our parameters \n</span><span class=\"s2\">    cache -- a dictionary containing &#34;Z1&#34;, &#34;A1&#34;, &#34;Z2&#34; and &#34;A2&#34;.\n</span><span class=\"s2\">    X -- input data of shape (2, number of examples)\n</span><span class=\"s2\">    Y -- &#34;true&#34; labels vector of shape (1, number of examples)\n</span><span class=\"s2\">\n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    grads -- python dictionary containing your gradients with respect to different parameters\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    <span class=\"n\">m</span> <span class=\"o\">=</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">shape</span><span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">]</span>\n\n    <span class=\"c1\"># First, retrieve W1 and W2 from the dictionary &#34;parameters&#34;.</span>\n    <span class=\"c1\">### START CODE HERE ### (≈ 2 lines of code)</span>\n    <span class=\"n\">W1</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;W1&#39;</span><span class=\"p\">]</span>\n    <span class=\"n\">W2</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;W2&#39;</span><span class=\"p\">]</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n\n    <span class=\"c1\"># Retrieve also A1 and A2 from dictionary &#34;cache&#34;.</span>\n    <span class=\"c1\">### START CODE HERE ### (≈ 2 lines of code)</span>\n    <span class=\"n\">A1</span> <span class=\"o\">=</span> <span class=\"n\">cache</span><span class=\"p\">[</span><span class=\"s1\">&#39;A1&#39;</span><span class=\"p\">]</span>\n    <span class=\"n\">A2</span> <span class=\"o\">=</span> <span class=\"n\">cache</span><span class=\"p\">[</span><span class=\"s1\">&#39;A2&#39;</span><span class=\"p\">]</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n\n    <span class=\"c1\"># Backward propagation: calculate dW1, db1, dW2, db2. </span>\n    <span class=\"c1\">### START CODE HERE ### (≈ 6 lines of code, corresponding to 6 equations on slide above)</span>\n    <span class=\"n\">dZ2</span> <span class=\"o\">=</span> <span class=\"n\">A2</span> <span class=\"o\">-</span> <span class=\"n\">Y</span>\n    <span class=\"n\">dW2</span> <span class=\"o\">=</span> <span class=\"mi\">1</span> <span class=\"o\">/</span> <span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">dZ2</span><span class=\"p\">,</span> <span class=\"n\">A1</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">)</span>\n    <span class=\"n\">db2</span> <span class=\"o\">=</span> <span class=\"mi\">1</span> <span class=\"o\">/</span> <span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">dZ2</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">keepdims</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n    <span class=\"n\">dZ1</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">W2</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">,</span> <span class=\"n\">dZ2</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"mi\">1</span> <span class=\"o\">-</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">power</span><span class=\"p\">(</span><span class=\"n\">A1</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">))</span>\n    <span class=\"n\">dW1</span> <span class=\"o\">=</span> <span class=\"mi\">1</span> <span class=\"o\">/</span> <span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">dZ1</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">)</span>\n    <span class=\"n\">db1</span> <span class=\"o\">=</span> <span class=\"mi\">1</span> <span class=\"o\">/</span> <span class=\"n\">m</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">dZ1</span><span class=\"p\">,</span> <span class=\"n\">axis</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">keepdims</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n\n    <span class=\"n\">grads</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">&#34;dW1&#34;</span><span class=\"p\">:</span> <span class=\"n\">dW1</span><span class=\"p\">,</span>\n             <span class=\"s2\">&#34;db1&#34;</span><span class=\"p\">:</span> <span class=\"n\">db1</span><span class=\"p\">,</span>\n             <span class=\"s2\">&#34;dW2&#34;</span><span class=\"p\">:</span> <span class=\"n\">dW2</span><span class=\"p\">,</span>\n             <span class=\"s2\">&#34;db2&#34;</span><span class=\"p\">:</span> <span class=\"n\">db2</span><span class=\"p\">}</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">grads</span></code></pre></div><h2>3.4 update parameters</h2><p>最后根据得到的grads，乘上学习速率，就可以更新参数了。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: update_parameters</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">update_parameters</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">grads</span><span class=\"p\">,</span> <span class=\"n\">learning_rate</span> <span class=\"o\">=</span> <span class=\"mf\">1.2</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Updates parameters using the gradient descent update rule given above\n</span><span class=\"s2\">\n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    parameters -- python dictionary containing your parameters \n</span><span class=\"s2\">    grads -- python dictionary containing your gradients \n</span><span class=\"s2\">\n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    parameters -- python dictionary containing your updated parameters \n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n    <span class=\"c1\"># Retrieve each parameter from the dictionary &#34;parameters&#34;</span>\n    <span class=\"c1\">### START CODE HERE ### (≈ 4 lines of code)</span>\n    <span class=\"n\">W1</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;W1&#39;</span><span class=\"p\">]</span>\n    <span class=\"n\">b1</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;b1&#39;</span><span class=\"p\">]</span>\n    <span class=\"n\">W2</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;W2&#39;</span><span class=\"p\">]</span>\n    <span class=\"n\">b2</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;b2&#39;</span><span class=\"p\">]</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n\n    <span class=\"c1\"># Retrieve each gradient from the dictionary &#34;grads&#34;</span>\n    <span class=\"c1\">### START CODE HERE ### (≈ 4 lines of code)</span>\n    <span class=\"n\">dW1</span> <span class=\"o\">=</span> <span class=\"n\">grads</span><span class=\"p\">[</span><span class=\"s1\">&#39;dW1&#39;</span><span class=\"p\">]</span>\n    <span class=\"n\">db1</span> <span class=\"o\">=</span> <span class=\"n\">grads</span><span class=\"p\">[</span><span class=\"s1\">&#39;db1&#39;</span><span class=\"p\">]</span>\n    <span class=\"n\">dW2</span> <span class=\"o\">=</span> <span class=\"n\">grads</span><span class=\"p\">[</span><span class=\"s1\">&#39;dW2&#39;</span><span class=\"p\">]</span>\n    <span class=\"n\">db2</span> <span class=\"o\">=</span> <span class=\"n\">grads</span><span class=\"p\">[</span><span class=\"s1\">&#39;db2&#39;</span><span class=\"p\">]</span>\n    <span class=\"c1\">## END CODE HERE ###</span>\n\n    <span class=\"c1\"># Update rule for each parameter</span>\n    <span class=\"c1\">### START CODE HERE ### (≈ 4 lines of code)</span>\n    <span class=\"n\">W1</span> <span class=\"o\">=</span> <span class=\"n\">W1</span> <span class=\"o\">-</span> <span class=\"n\">learning_rate</span> <span class=\"o\">*</span> <span class=\"n\">dW1</span>\n    <span class=\"n\">b1</span> <span class=\"o\">=</span> <span class=\"n\">b1</span> <span class=\"o\">-</span> <span class=\"n\">learning_rate</span> <span class=\"o\">*</span> <span class=\"n\">db1</span>\n    <span class=\"n\">W2</span> <span class=\"o\">=</span> <span class=\"n\">W2</span> <span class=\"o\">-</span> <span class=\"n\">learning_rate</span> <span class=\"o\">*</span> <span class=\"n\">dW2</span>\n    <span class=\"n\">b2</span> <span class=\"o\">=</span> <span class=\"n\">b2</span> <span class=\"o\">-</span> <span class=\"n\">learning_rate</span> <span class=\"o\">*</span> <span class=\"n\">db2</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n\n    <span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"s2\">&#34;W1&#34;</span><span class=\"p\">:</span> <span class=\"n\">W1</span><span class=\"p\">,</span>\n                  <span class=\"s2\">&#34;b1&#34;</span><span class=\"p\">:</span> <span class=\"n\">b1</span><span class=\"p\">,</span>\n                  <span class=\"s2\">&#34;W2&#34;</span><span class=\"p\">:</span> <span class=\"n\">W2</span><span class=\"p\">,</span>\n                  <span class=\"s2\">&#34;b2&#34;</span><span class=\"p\">:</span> <span class=\"n\">b2</span><span class=\"p\">}</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">parameters</span></code></pre></div><p>然后把更新完的参数再传入前面的循环中，不断循环，直到达到循环的次数。</p><h2>nn_model</h2><p>把前面的函数都调用过来。</p><p>模型中传入的参数是，X,Y，和迭代次数</p><ol><li>首先需要得到你要设计的神经网络结构，调用<code>layer_sizes()</code>得到了n_x,n_y，也就是输入层和输出层。</li><li>初始化参数<code>initialize_parameters(n_x, n_h, n_y)</code>,得到初始化的 W1, b1, W2, b2</li><li>然后开始循环</li><ol><li>使用<code>forward_propagation(X, parameters)</code>,先得到各个神经元的计算值。</li><li>然后<code>compute_cost(A2, Y, parameters)</code>,得到cost</li><li><code>backward_propagation(parameters, cache, X, Y)</code>计算出每一步的梯度</li><li><code>update_parameters(parameters, grads)</code>更新一下参数</li><li>返回训练完的parameters</li></ol></ol><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: nn_model</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">nn_model</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">n_h</span><span class=\"p\">,</span> <span class=\"n\">num_iterations</span> <span class=\"o\">=</span> <span class=\"mi\">10000</span><span class=\"p\">,</span> <span class=\"n\">print_cost</span><span class=\"o\">=</span><span class=\"bp\">False</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    X -- dataset of shape (2, number of examples)\n</span><span class=\"s2\">    Y -- labels of shape (1, number of examples)\n</span><span class=\"s2\">    n_h -- size of the hidden layer\n</span><span class=\"s2\">    num_iterations -- Number of iterations in gradient descent loop\n</span><span class=\"s2\">    print_cost -- if True, print the cost every 1000 iterations\n</span><span class=\"s2\">\n</span><span class=\"s2\">    Returns:\n</span><span class=\"s2\">    parameters -- parameters learnt by the model. They can then be used to predict.\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n\n    <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">random</span><span class=\"o\">.</span><span class=\"n\">seed</span><span class=\"p\">(</span><span class=\"mi\">3</span><span class=\"p\">)</span>\n    <span class=\"n\">n_x</span> <span class=\"o\">=</span> <span class=\"n\">layer_sizes</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">)[</span><span class=\"mi\">0</span><span class=\"p\">]</span>\n    <span class=\"n\">n_y</span> <span class=\"o\">=</span> <span class=\"n\">layer_sizes</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">)[</span><span class=\"mi\">2</span><span class=\"p\">]</span>\n\n    <span class=\"c1\"># Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: &#34;n_x, n_h, n_y&#34;. Outputs = &#34;W1, b1, W2, b2, parameters&#34;.</span>\n    <span class=\"c1\">### START CODE HERE ### (≈ 5 lines of code)</span>\n    <span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"n\">initialize_parameters</span><span class=\"p\">(</span><span class=\"n\">n_x</span><span class=\"p\">,</span> <span class=\"n\">n_h</span><span class=\"p\">,</span> <span class=\"n\">n_y</span><span class=\"p\">)</span>\n    <span class=\"n\">W1</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;W1&#39;</span><span class=\"p\">]</span>\n    <span class=\"n\">b1</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;b1&#39;</span><span class=\"p\">]</span>\n    <span class=\"n\">W2</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;W2&#39;</span><span class=\"p\">]</span>\n    <span class=\"n\">b2</span> <span class=\"o\">=</span> <span class=\"n\">parameters</span><span class=\"p\">[</span><span class=\"s1\">&#39;b2&#39;</span><span class=\"p\">]</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n\n    <span class=\"c1\"># Loop (gradient descent)</span>\n\n    <span class=\"k\">for</span> <span class=\"n\">i</span> <span class=\"ow\">in</span> <span class=\"nb\">range</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">num_iterations</span><span class=\"p\">):</span>\n\n        <span class=\"c1\">### START CODE HERE ### (≈ 4 lines of code)</span>\n        <span class=\"c1\"># Forward propagation. Inputs: &#34;X, parameters&#34;. Outputs: &#34;A2, cache&#34;.</span>\n        <span class=\"n\">A2</span><span class=\"p\">,</span> <span class=\"n\">cache</span> <span class=\"o\">=</span> <span class=\"n\">forward_propagation</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">parameters</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># Cost function. Inputs: &#34;A2, Y, parameters&#34;. Outputs: &#34;cost&#34;.</span>\n        <span class=\"n\">cost</span> <span class=\"o\">=</span> <span class=\"n\">compute_cost</span><span class=\"p\">(</span><span class=\"n\">A2</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">parameters</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># Backpropagation. Inputs: &#34;parameters, cache, X, Y&#34;. Outputs: &#34;grads&#34;.</span>\n        <span class=\"n\">grads</span> <span class=\"o\">=</span> <span class=\"n\">backward_propagation</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">cache</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">)</span>\n\n        <span class=\"c1\"># Gradient descent parameter update. Inputs: &#34;parameters, grads&#34;. Outputs: &#34;parameters&#34;.</span>\n        <span class=\"n\">parameters</span> <span class=\"o\">=</span>  <span class=\"n\">update_parameters</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">grads</span><span class=\"p\">)</span>\n\n        <span class=\"c1\">### END CODE HERE ###</span>\n\n        <span class=\"c1\"># Print the cost every 1000 iterations</span>\n        <span class=\"k\">if</span> <span class=\"n\">print_cost</span> <span class=\"ow\">and</span> <span class=\"n\">i</span> <span class=\"o\">%</span> <span class=\"mi\">1000</span> <span class=\"o\">==</span> <span class=\"mi\">0</span><span class=\"p\">:</span>\n            <span class=\"k\">print</span> <span class=\"p\">(</span><span class=\"s2\">&#34;Cost after iteration </span><span class=\"si\">%i</span><span class=\"s2\">: </span><span class=\"si\">%f</span><span class=\"s2\">&#34;</span> <span class=\"o\">%</span><span class=\"p\">(</span><span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">cost</span><span class=\"p\">))</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">parameters</span></code></pre></div><h2>预测</h2><p>得到训练后的parameters，再用<code>forward_propagation(X, parameters)</code>计算出输出层最终的值A2，以0.5为分界，分为0和1。</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># GRADED FUNCTION: predict</span>\n\n<span class=\"k\">def</span> <span class=\"nf\">predict</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;\n</span><span class=\"s2\">    Using the learned parameters, predicts a class for each example in X\n</span><span class=\"s2\">\n</span><span class=\"s2\">    Arguments:\n</span><span class=\"s2\">    parameters -- python dictionary containing your parameters \n</span><span class=\"s2\">    X -- input data of size (n_x, m)\n</span><span class=\"s2\">\n</span><span class=\"s2\">    Returns\n</span><span class=\"s2\">    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n</span><span class=\"s2\">    &#34;&#34;&#34;</span>\n\n    <span class=\"c1\"># Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.</span>\n    <span class=\"c1\">### START CODE HERE ### (≈ 2 lines of code)</span>\n    <span class=\"n\">A2</span><span class=\"p\">,</span> <span class=\"n\">cache</span> <span class=\"o\">=</span> <span class=\"n\">forward_propagation</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">parameters</span><span class=\"p\">)</span>\n    <span class=\"n\">predictions</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">A2</span> <span class=\"o\">&gt;</span> <span class=\"mf\">0.5</span><span class=\"p\">)</span>\n    <span class=\"c1\">### END CODE HERE ###</span>\n\n    <span class=\"k\">return</span> <span class=\"n\">predictions</span>\n<span class=\"c1\"># Build a model with a n_h-dimensional hidden layer</span>\n<span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"n\">nn_model</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">n_h</span> <span class=\"o\">=</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"n\">num_iterations</span> <span class=\"o\">=</span> <span class=\"mi\">10000</span><span class=\"p\">,</span> <span class=\"n\">print_cost</span><span class=\"o\">=</span><span class=\"bp\">True</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Plot the decision boundary</span>\n<span class=\"n\">plot_decision_boundary</span><span class=\"p\">(</span><span class=\"k\">lambda</span> <span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">),</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">)</span>\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">title</span><span class=\"p\">(</span><span class=\"s2\">&#34;Decision Boundary for hidden layer size &#34;</span> <span class=\"o\">+</span> <span class=\"nb\">str</span><span class=\"p\">(</span><span class=\"mi\">4</span><span class=\"p\">))</span></code></pre></div><p>可以看到，训练后神经网络得到的分界线更为合理。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-63742111bee73fdc6efd95bc17ceb95c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"532\" data-rawheight=\"391\" class=\"origin_image zh-lightbox-thumb\" width=\"532\" data-original=\"https://pic1.zhimg.com/v2-63742111bee73fdc6efd95bc17ceb95c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;532&#39; height=&#39;391&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"532\" data-rawheight=\"391\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"532\" data-original=\"https://pic1.zhimg.com/v2-63742111bee73fdc6efd95bc17ceb95c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-63742111bee73fdc6efd95bc17ceb95c_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># Print accuracy</span>\n<span class=\"n\">predictions</span> <span class=\"o\">=</span> <span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">)</span>\n<span class=\"k\">print</span> <span class=\"p\">(</span><span class=\"s1\">&#39;Accuracy: </span><span class=\"si\">%d</span><span class=\"s1\">&#39;</span> <span class=\"o\">%</span> <span class=\"nb\">float</span><span class=\"p\">((</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">Y</span><span class=\"p\">,</span><span class=\"n\">predictions</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"o\">-</span><span class=\"n\">Y</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"o\">-</span><span class=\"n\">predictions</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">))</span><span class=\"o\">/</span><span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"n\">Y</span><span class=\"o\">.</span><span class=\"n\">size</span><span class=\"p\">)</span><span class=\"o\">*</span><span class=\"mi\">100</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"s1\">&#39;%&#39;</span><span class=\"p\">)</span></code></pre></div><p>准确率高达90%</p><h2>优化参数</h2><p>这个时候就可以设置不同的hidden_layer的维度大小[1, 2, 3, 4, 5, 20, 50]</p><div class=\"highlight\"><pre><code class=\"language-python\"><span class=\"c1\"># This may take about 2 minutes to run</span>\n\n<span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">figure</span><span class=\"p\">(</span><span class=\"n\">figsize</span><span class=\"o\">=</span><span class=\"p\">(</span><span class=\"mi\">16</span><span class=\"p\">,</span> <span class=\"mi\">32</span><span class=\"p\">))</span>\n<span class=\"n\">hidden_layer_sizes</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"mi\">3</span><span class=\"p\">,</span> <span class=\"mi\">4</span><span class=\"p\">,</span> <span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">20</span><span class=\"p\">,</span> <span class=\"mi\">50</span><span class=\"p\">]</span>\n<span class=\"k\">for</span> <span class=\"n\">i</span><span class=\"p\">,</span> <span class=\"n\">n_h</span> <span class=\"ow\">in</span> <span class=\"nb\">enumerate</span><span class=\"p\">(</span><span class=\"n\">hidden_layer_sizes</span><span class=\"p\">):</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">subplot</span><span class=\"p\">(</span><span class=\"mi\">5</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"n\">i</span><span class=\"o\">+</span><span class=\"mi\">1</span><span class=\"p\">)</span>\n    <span class=\"n\">plt</span><span class=\"o\">.</span><span class=\"n\">title</span><span class=\"p\">(</span><span class=\"s1\">&#39;Hidden Layer of size </span><span class=\"si\">%d</span><span class=\"s1\">&#39;</span> <span class=\"o\">%</span> <span class=\"n\">n_h</span><span class=\"p\">)</span>\n    <span class=\"n\">parameters</span> <span class=\"o\">=</span> <span class=\"n\">nn_model</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">,</span> <span class=\"n\">n_h</span><span class=\"p\">,</span> <span class=\"n\">num_iterations</span> <span class=\"o\">=</span> <span class=\"mi\">5000</span><span class=\"p\">)</span>\n    <span class=\"n\">plot_decision_boundary</span><span class=\"p\">(</span><span class=\"k\">lambda</span> <span class=\"n\">x</span><span class=\"p\">:</span> <span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">),</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">Y</span><span class=\"p\">)</span>\n    <span class=\"n\">predictions</span> <span class=\"o\">=</span> <span class=\"n\">predict</span><span class=\"p\">(</span><span class=\"n\">parameters</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">)</span>\n    <span class=\"n\">accuracy</span> <span class=\"o\">=</span> <span class=\"nb\">float</span><span class=\"p\">((</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">Y</span><span class=\"p\">,</span><span class=\"n\">predictions</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"o\">-</span><span class=\"n\">Y</span><span class=\"p\">,</span><span class=\"mi\">1</span><span class=\"o\">-</span><span class=\"n\">predictions</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">))</span><span class=\"o\">/</span><span class=\"nb\">float</span><span class=\"p\">(</span><span class=\"n\">Y</span><span class=\"o\">.</span><span class=\"n\">size</span><span class=\"p\">)</span><span class=\"o\">*</span><span class=\"mi\">100</span><span class=\"p\">)</span>\n    <span class=\"k\">print</span> <span class=\"p\">(</span><span class=\"s2\">&#34;Accuracy for {} hidden units: {} %&#34;</span><span class=\"o\">.</span><span class=\"n\">format</span><span class=\"p\">(</span><span class=\"n\">n_h</span><span class=\"p\">,</span> <span class=\"n\">accuracy</span><span class=\"p\">))</span>\n<span class=\"n\">Accuracy</span> <span class=\"k\">for</span> <span class=\"mi\">1</span> <span class=\"n\">hidden</span> <span class=\"n\">units</span><span class=\"p\">:</span> <span class=\"mf\">67.5</span> <span class=\"o\">%</span>\n<span class=\"n\">Accuracy</span> <span class=\"k\">for</span> <span class=\"mi\">2</span> <span class=\"n\">hidden</span> <span class=\"n\">units</span><span class=\"p\">:</span> <span class=\"mf\">67.25</span> <span class=\"o\">%</span>\n<span class=\"n\">Accuracy</span> <span class=\"k\">for</span> <span class=\"mi\">3</span> <span class=\"n\">hidden</span> <span class=\"n\">units</span><span class=\"p\">:</span> <span class=\"mf\">90.75</span> <span class=\"o\">%</span>\n<span class=\"n\">Accuracy</span> <span class=\"k\">for</span> <span class=\"mi\">4</span> <span class=\"n\">hidden</span> <span class=\"n\">units</span><span class=\"p\">:</span> <span class=\"mf\">90.5</span> <span class=\"o\">%</span>\n<span class=\"n\">Accuracy</span> <span class=\"k\">for</span> <span class=\"mi\">5</span> <span class=\"n\">hidden</span> <span class=\"n\">units</span><span class=\"p\">:</span> <span class=\"mf\">91.25</span> <span class=\"o\">%</span>\n<span class=\"n\">Accuracy</span> <span class=\"k\">for</span> <span class=\"mi\">20</span> <span class=\"n\">hidden</span> <span class=\"n\">units</span><span class=\"p\">:</span> <span class=\"mf\">90.0</span> <span class=\"o\">%</span>\n<span class=\"n\">Accuracy</span> <span class=\"k\">for</span> <span class=\"mi\">50</span> <span class=\"n\">hidden</span> <span class=\"n\">units</span><span class=\"p\">:</span> <span class=\"mf\">90.25</span> <span class=\"o\">%</span></code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-22444884cd6036631c4e2cd1ae268772_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1306\" data-rawheight=\"2048\" class=\"origin_image zh-lightbox-thumb\" width=\"1306\" data-original=\"https://pic3.zhimg.com/v2-22444884cd6036631c4e2cd1ae268772_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1306&#39; height=&#39;2048&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1306\" data-rawheight=\"2048\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1306\" data-original=\"https://pic3.zhimg.com/v2-22444884cd6036631c4e2cd1ae268772_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-22444884cd6036631c4e2cd1ae268772_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>得到的结果在n_h = 5时有最大值。</p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "神经网络", 
                    "tagLink": "https://api.zhihu.com/topics/19607065"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/44396045", 
            "userName": "直上云霄", 
            "userLink": "https://www.zhihu.com/people/1033165ce4ad9c3fce69a0793dfab8ad", 
            "upvote": 1, 
            "title": "DeepLearning.ai笔记:(1-3)-浅层神经网络", 
            "content": "<h2>本文首发于个人博客：<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">fangzh.top</span><span class=\"invisible\"></span></a></h2><p>前面两周讲的是一些logistic和向量化的内容，以及numpy的基本使用，在他之前的机器学习课程中已经讲过了，这里就不再赘述。Week3主要讲了如何搭建两层的神经网络。</p><h2>神经网络的表示</h2><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-8757b594ed20d404f388130c93f16738_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1290\" data-rawheight=\"782\" class=\"origin_image zh-lightbox-thumb\" width=\"1290\" data-original=\"https://pic1.zhimg.com/v2-8757b594ed20d404f388130c93f16738_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1290&#39; height=&#39;782&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1290\" data-rawheight=\"782\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1290\" data-original=\"https://pic1.zhimg.com/v2-8757b594ed20d404f388130c93f16738_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-8757b594ed20d404f388130c93f16738_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>这周的内容就围绕着这一张图来讲。</p><p><img src=\"https://www.zhihu.com/equation?tex=a_%7Bj%7D%5E%7B%5Bi%5D%7D\" alt=\"a_{j}^{[i]}\" eeimg=\"1\"/> </p><p>这就是每一层神经元的表达方式，上标中括号[]，表示是第几层的神经元；下标表示这个是某一层的第几个神经元。</p><p>Input Layer：输入层，也用 <img src=\"https://www.zhihu.com/equation?tex=a_%7Bj%7D%5E%7B%5B0%5D%7D\" alt=\"a_{j}^{[0]}\" eeimg=\"1\"/> ，表示第0层</p><p>Hidden Layer：表示除了最后一层输出层以外的内部隐藏层</p><p>Output Layer：输出层，表示最后一层</p><p>而通常神经网络的层数一般不包括输入层。</p><p><img src=\"https://www.zhihu.com/equation?tex=w%5E%7B%5Bi%5D%7D\" alt=\"w^{[i]}\" eeimg=\"1\"/> ：每一层的参数w的维度是（该层神经元个数，前面一层神经元个数）</p><p><img src=\"https://www.zhihu.com/equation?tex=b%5E%7B%5Bi%5D%7D\" alt=\"b^{[i]}\" eeimg=\"1\"/> ：为（每一层的神经元个数，1）</p><h2>计算单个数据的神经网络</h2><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-7d46c24b97cc923d84e1889de5f47fce_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"546\" data-rawheight=\"251\" class=\"origin_image zh-lightbox-thumb\" width=\"546\" data-original=\"https://pic3.zhimg.com/v2-7d46c24b97cc923d84e1889de5f47fce_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;546&#39; height=&#39;251&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"546\" data-rawheight=\"251\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"546\" data-original=\"https://pic3.zhimg.com/v2-7d46c24b97cc923d84e1889de5f47fce_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-7d46c24b97cc923d84e1889de5f47fce_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>由此得到，计算单个数据的神经网络只需要4步：</p><p><img src=\"https://www.zhihu.com/equation?tex=z%5E%7B%5B1%5D%7D+%3D+W%5E%7B%5B1%5D%7Da%5E%7B%5B0%5D%7D+%2B+b%5E%7B%5B1%5D%7D+\" alt=\"z^{[1]} = W^{[1]}a^{[0]} + b^{[1]} \" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=a%5E%7B%5B1%5D%7D+%3D+%5Csigma%28z%5E%7B%5B1%5D%7D%29+\" alt=\"a^{[1]} = \\sigma(z^{[1]}) \" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=z%5E%7B%5B2%5D%7D+%3D+W%5E%7B%5B2%5D%7Da%5E%7B%5B1%5D%7D+%2B+b%5E%7B%5B2%5D%7D+\" alt=\"z^{[2]} = W^{[2]}a^{[1]} + b^{[2]} \" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=a%5E%7B%5B2%5D%7D+%3D+%5Csigma%28z%5E%7B%5B2%5D%7D%29\" alt=\"a^{[2]} = \\sigma(z^{[2]})\" eeimg=\"1\"/> </p><h2>多数据的向量化表示</h2><p>我们知道，多个数据的表示就是 <img src=\"https://www.zhihu.com/equation?tex=x%5E%7B%28i%29%7D\" alt=\"x^{(i)}\" eeimg=\"1\"/> ，使用小括号的上标。神经元也是一样。</p><p>如 <img src=\"https://www.zhihu.com/equation?tex=a%5E%7B%5B1%5D+%28i%29%7D\" alt=\"a^{[1] (i)}\" eeimg=\"1\"/> 表示第1层神经元的第i个样本。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-870a03a5327f87846ec507d34e28d10b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1450\" data-rawheight=\"814\" class=\"origin_image zh-lightbox-thumb\" width=\"1450\" data-original=\"https://pic4.zhimg.com/v2-870a03a5327f87846ec507d34e28d10b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1450&#39; height=&#39;814&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1450\" data-rawheight=\"814\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1450\" data-original=\"https://pic4.zhimg.com/v2-870a03a5327f87846ec507d34e28d10b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-870a03a5327f87846ec507d34e28d10b_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>那么如果有m个样本，一直做for循环来计算出这些神经元的值，实在是太慢了，所以跟logistic一样，可以直接用向量化来表示，这个时候用大写字母来表示。</p><p><img src=\"https://www.zhihu.com/equation?tex=Z%5E%7B%5B1%5D%7D+%3D+W%5E%7B%5B1%5D%7DA%5E%7B%5B0%5D%7D+%2B+b%5E%7B%5B1%5D%7D+\" alt=\"Z^{[1]} = W^{[1]}A^{[0]} + b^{[1]} \" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=A%5E%7B%5B1%5D%7D+%3D+%5Csigma%28Z%5E%7B%5B1%5D%7D%29+\" alt=\"A^{[1]} = \\sigma(Z^{[1]}) \" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=Z%5E%7B%5B2%5D%7D+%3D+W%5E%7B%5B2%5D%7DA%5E%7B%5B1%5D%7D+%2B+b%5E%7B%5B2%5D%7D+\" alt=\"Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]} \" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=A%5E%7B%5B2%5D%7D+%3D+%5Csigma%28Z%5E%7B%5B2%5D%7D%29\" alt=\"A^{[2]} = \\sigma(Z^{[2]})\" eeimg=\"1\"/> </p><p>这个时候，例如 <img src=\"https://www.zhihu.com/equation?tex=A%5E%7B%5B1%5D%7D\" alt=\"A^{[1]}\" eeimg=\"1\"/> 是一个 <img src=\"https://www.zhihu.com/equation?tex=%28n%2Cm%29\" alt=\"(n,m)\" eeimg=\"1\"/> 的矩阵，m是样本数，每一列表示一个样本，n是该层的神经元个数。</p><p>从水平上看，矩阵 A代表了各个训练样本。竖直上看，A的不同索引对应不用的隐藏单元。</p><p>对矩阵Z和X也是类似，水平方向对应不同的样本，竖直方向上对应不同的输入特征，也就是神经网络输入层的各个节点。</p><h2>激活函数</h2><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-aa93949c44778647901831add92f2160_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1388\" data-rawheight=\"766\" class=\"origin_image zh-lightbox-thumb\" width=\"1388\" data-original=\"https://pic1.zhimg.com/v2-aa93949c44778647901831add92f2160_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1388&#39; height=&#39;766&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1388\" data-rawheight=\"766\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1388\" data-original=\"https://pic1.zhimg.com/v2-aa93949c44778647901831add92f2160_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-aa93949c44778647901831add92f2160_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>在此前都是用sigmoid作为激活函数的。但是激活函数不只有这一种，常用的有4种，分别是：sigmoid, tanh, ReLu, Leaky ReLu。</p><ul><li>sigmoid: <img src=\"https://www.zhihu.com/equation?tex=a+%3D++%5Cfrac%7B1%7D%7B1+%2B+e%5E%7B-z%7D%7D\" alt=\"a =  \\frac{1}{1 + e^{-z}}\" eeimg=\"1\"/> </li><ul><li>导数： <img src=\"https://www.zhihu.com/equation?tex=a%5E%7B%5Cprime%7D+%3D+a%281-a%29\" alt=\"a^{\\prime} = a(1-a)\" eeimg=\"1\"/> </li></ul><li>tanh: <img src=\"https://www.zhihu.com/equation?tex=a+%3D+%5Cfrac%7Be%5Ez+-+e%5E%7B-z%7D%7D%7Be%5Ez+%2B+e%5E%7B-z%7D%7D\" alt=\"a = \\frac{e^z - e^{-z}}{e^z + e^{-z}}\" eeimg=\"1\"/> </li><ul><li>导数： <img src=\"https://www.zhihu.com/equation?tex=a%5E%7B%5Cprime%7D+%3D+1+-+a%5E2\" alt=\"a^{\\prime} = 1 - a^2\" eeimg=\"1\"/> </li></ul><li>ReLu(修正线性单元): <img src=\"https://www.zhihu.com/equation?tex=a+%3D+max%280%2C+z%29\" alt=\"a = max(0, z)\" eeimg=\"1\"/> </li><li>Leaky ReLu: <img src=\"https://www.zhihu.com/equation?tex=a+%3D+max%280.01z%2C+z%29\" alt=\"a = max(0.01z, z)\" eeimg=\"1\"/> </li></ul><p>tips:</p><ul><li>tanh函数在值域上处于-1和+1之间，所以均值更接近0，使用tanh比sigmoid更能够中心化数据，使得平均值接近0，而不是0.5。</li><li>tanh在大多数场合都是优于sigmoid的。</li><li>但是sigmoid和tanh有共同的缺点就是z在特别大或者特别小的时候，梯度很小，收敛速度很慢。</li><li>而ReLu弥补了两者的不足，在 <img src=\"https://www.zhihu.com/equation?tex=z+%3E+0\" alt=\"z &gt; 0\" eeimg=\"1\"/> 时，梯度始终为1，提高了速度。</li><li>Leaky ReLu保证了 <img src=\"https://www.zhihu.com/equation?tex=z+%3C+0\" alt=\"z &lt; 0\" eeimg=\"1\"/> 时，梯度不为0，但是实际上效果差不多。</li></ul><p>结论：</p><ul><li>sigmoid：除了输出层是一个二分类问题的时候使用，不然基本不用</li><li>tanh：几乎适用于任何场合</li><li>ReLu：默认使用这个，如果不确定你要用哪个激活函数，那就选ReLu或者Leaky ReLu</li></ul><h2>为什么要使用非线性的激活函数</h2><p>如果不用激励函数（其实相当于激励函数是f(x) = x），在这种情况下你每一层输出都是上层输入的线性函数，很容易验证，无论你神经网络有多少层，输出都是输入的线性组合，与只有一个隐藏层效果相当，这种情况就是多层感知机（MLP）了。 正因为上面的原因，我们决定引入非线性函数作为激励函数，这样深层神经网络就有意义了（不再是输入的线性组合，可以逼近任意函数）。</p><h2>梯度下降法公式</h2><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-8b63fb43ed15c306fc2241bff25403a0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1374\" data-rawheight=\"762\" class=\"origin_image zh-lightbox-thumb\" width=\"1374\" data-original=\"https://pic1.zhimg.com/v2-8b63fb43ed15c306fc2241bff25403a0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1374&#39; height=&#39;762&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1374\" data-rawheight=\"762\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1374\" data-original=\"https://pic1.zhimg.com/v2-8b63fb43ed15c306fc2241bff25403a0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-8b63fb43ed15c306fc2241bff25403a0_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>这里给出了浅层神经网络的梯度下降法公式。其中 <img src=\"https://www.zhihu.com/equation?tex=g%5E%7B%5B1%5D%27%7D%28Z%5E%7B%5B1%5D%7D%29\" alt=\"g^{[1]&#39;}(Z^{[1]})\" eeimg=\"1\"/> 表示你的激活函数的导数。</p><h2>参数随机初始化</h2><p>在神经网络中,如果将参数全部初始化为0 会导致一个问题，例如对于上面的神经网络的例子，如果将参数全部初始化为0，在每轮参数更新的时候，与输入单元相关的两个隐藏单元的结果将是相同的。</p><p>所以初始化时，W要随机初始化，b不存在对称性问题，所以可以设置为0</p><div class=\"highlight\"><pre><code class=\"language-text\">W = np.random.rand((2,2))* 0.01\nb = np.zero((2,1))</code></pre></div><p>将W乘以0.01是为了让W初始化足够小，因为如果很大的话，Z就很大，用sigmoid或者tanh时，所得到的梯度就会很小，训练过程会变慢。</p><p>ReLU和Leaky ReLU作为激活函数时，不存在这种问题，因为在大于0的时候，梯度均为1。</p><p>好好做作业，才能有更深的体会！</p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "神经网络", 
                    "tagLink": "https://api.zhihu.com/topics/19607065"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/44213627", 
            "userName": "直上云霄", 
            "userLink": "https://www.zhihu.com/people/1033165ce4ad9c3fce69a0793dfab8ad", 
            "upvote": 115, 
            "title": "hexo超完整的搭建教程，让你拥有一个专属个人博客", 
            "content": "<p></p><p>花了几天搭建了个网站，先上链接，欢迎来访：<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">fangzh的个人博客</a></p><p>现在市面上的博客很多，如CSDN，博客园，简书等平台，可以直接在上面发表，用户交互做的好，写的文章百度也能搜索的到。缺点是比较不自由，会受到平台的各种限制和恶心的广告。</p><p>而自己购买域名和服务器，搭建博客的成本实在是太高了，不光是说这些购买成本，单单是花力气去自己搭这么一个网站，还要定期的维护它，对于我们大多数人来说，实在是没有这样的精力和时间。</p><p>那么就有第三种选择，直接在github page平台上托管我们的博客。这样就可以安心的来写作，又不需要定期维护，而且hexo作为一个快速简洁的博客框架，用它来搭建博客真的非常容易。</p><h2><b>Hexo简介</b></h2><p>Hexo是一款基于Node.js的静态博客框架，依赖少易于安装使用，可以方便的生成静态网页托管在GitHub和Coding上，是搭建博客的首选框架。大家可以进入<a href=\"https://link.zhihu.com/?target=https%3A//hexo.io/zh-cn/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">hexo官网</a>进行详细查看，因为Hexo的创建者是台湾人，对中文的支持很友好，可以选择中文进行查看。</p><p>教程分三个部分，</p><ul><li>第一部分：hexo的初级搭建还有部署到github page上，以及个人域名的绑定。</li><li>第二部分：hexo的基本配置，更换主题，实现多终端工作，以及在coding page部署实现国内外分流</li><li>第三部分：hexo添加各种功能，包括搜索的SEO，阅读量统计，访问量统计和评论系统等。</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>第一部分</b></h2><p>hexo的初级搭建还有部署到github page上，以及个人域名的绑定。</p><h2><b>Hexo搭建步骤</b></h2><ol><li>安装Git</li><li>安装Node.js</li><li>安装Hexo</li><li>GitHub创建个人仓库</li><li>生成SSH添加到GitHub</li><li>将hexo部署到GitHub</li><li>设置个人域名</li><li>发布文章</li></ol><h2><b>1. 安装Git</b></h2><p>Git是目前世界上最先进的分布式版本控制系统，可以有效、高速的处理从很小到非常大的项目版本管理。也就是用来管理你的hexo博客文章，上传到GitHub的工具。Git非常强大，我觉得建议每个人都去了解一下。廖雪峰老师的Git教程写的非常好，大家可以了解一下。<a href=\"https://link.zhihu.com/?target=https%3A//www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Git教程</a></p><p>windows：到git官网上下载,<a href=\"https://link.zhihu.com/?target=https%3A//gitforwindows.org/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Download git</a>,下载后会有一个Git Bash的命令行工具，以后就用这个工具来使用git。</p><p>linux：对linux来说实在是太简单了，因为最早的git就是在linux上编写的，只需要一行代码</p><div class=\"highlight\"><pre><code class=\"language-text\">sudo apt-get install git</code></pre></div><p>安装好后，用<code>git --version</code> 来查看一下版本</p><h2><b>2. 安装nodejs</b></h2><p>Hexo是基于nodeJS编写的，所以需要安装一下nodeJs和里面的npm工具。</p><p>windows：<a href=\"https://link.zhihu.com/?target=https%3A//nodejs.org/en/download/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">nodejs</a>选择LTS版本就行了。</p><p>linux：</p><div class=\"highlight\"><pre><code class=\"language-text\">sudo apt-get install nodejs\nsudo apt-get install npm</code></pre></div><p>安装完后，打开命令行</p><div class=\"highlight\"><pre><code class=\"language-text\">node -v\nnpm -v</code></pre></div><p>检查一下有没有安装成功 </p><p>顺便说一下，windows在git安装完后，就可以直接使用git bash来敲命令行了，不用自带的cmd，cmd有点难用。</p><h2><b>3. 安装hexo</b></h2><p>前面git和nodejs安装好后，就可以安装hexo了，你可以先创建一个文件夹blog，然后<code>cd</code>到这个文件夹下（或者在这个文件夹下直接右键git bash打开）。</p><p>输入命令</p><div class=\"highlight\"><pre><code class=\"language-text\">npm install -g hexo-cli</code></pre></div><p>依旧用<code>hexo -v</code>查看一下版本</p><p>至此就全部安装完了。</p><p>接下来初始化一下hexo</p><div class=\"highlight\"><pre><code class=\"language-text\">hexo init myblog</code></pre></div><p>这个myblog可以自己取什么名字都行，然后</p><div class=\"highlight\"><pre><code class=\"language-bash\"><span class=\"nb\">cd</span> myblog //进入这个myblog文件夹\nnpm install</code></pre></div><p>新建完成后，指定文件夹目录下有：</p><ul><li>node_modules: 依赖包</li><li>public：存放生成的页面</li><li>scaffolds：生成文章的一些模板</li><li>source：用来存放你的文章</li><li>themes：主题</li><li>** _config.yml: 博客的配置文件**</li></ul><div class=\"highlight\"><pre><code class=\"language-text\">hexo g\nhexo server</code></pre></div><p>打开hexo的服务，在浏览器输入localhost:4000就可以看到你生成的博客了。</p><p>大概长这样： </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-d301dfbac9165b21ab1ee0a860f44de4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1344\" data-rawheight=\"542\" class=\"origin_image zh-lightbox-thumb\" width=\"1344\" data-original=\"https://pic1.zhimg.com/v2-d301dfbac9165b21ab1ee0a860f44de4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1344&#39; height=&#39;542&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1344\" data-rawheight=\"542\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1344\" data-original=\"https://pic1.zhimg.com/v2-d301dfbac9165b21ab1ee0a860f44de4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-d301dfbac9165b21ab1ee0a860f44de4_b.jpg\"/></figure><p> 使用ctrl+c可以把服务关掉。</p><h2><b>4. GitHub创建个人仓库</b></h2><p>首先，你先要有一个GitHub账户，去注册一个吧。</p><p>注册完登录后，在<a href=\"https://link.zhihu.com/?target=http%3A//GitHub.com\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">GitHub.com</span><span class=\"invisible\"></span></a>中看到一个New repository，新建仓库 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-4387cb776ccc72189bc06fd511c1e19e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"522\" data-rawheight=\"433\" class=\"origin_image zh-lightbox-thumb\" width=\"522\" data-original=\"https://pic3.zhimg.com/v2-4387cb776ccc72189bc06fd511c1e19e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;522&#39; height=&#39;433&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"522\" data-rawheight=\"433\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"522\" data-original=\"https://pic3.zhimg.com/v2-4387cb776ccc72189bc06fd511c1e19e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-4387cb776ccc72189bc06fd511c1e19e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>创建一个和你用户名相同的仓库，后面加.<a href=\"https://link.zhihu.com/?target=http%3A//github.io\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">github.io</span><span class=\"invisible\"></span></a>，只有这样，将来要部署到GitHub page的时候，才会被识别，也就是<a href=\"https://link.zhihu.com/?target=http%3A//xxxx.github.io\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">xxxx.github.io</span><span class=\"invisible\"></span></a>，其中xxx就是你注册GitHub的用户名。我这里是已经建过了。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-9d5692a0e62fd62f902f1085348f5e7e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"680\" data-rawheight=\"868\" class=\"origin_image zh-lightbox-thumb\" width=\"680\" data-original=\"https://pic3.zhimg.com/v2-9d5692a0e62fd62f902f1085348f5e7e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;680&#39; height=&#39;868&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"680\" data-rawheight=\"868\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"680\" data-original=\"https://pic3.zhimg.com/v2-9d5692a0e62fd62f902f1085348f5e7e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-9d5692a0e62fd62f902f1085348f5e7e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>点击create repository。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>5. 生成SSH添加到GitHub</b></h2><p>回到你的git bash中，</p><div class=\"highlight\"><pre><code class=\"language-text\">git config --global user.name &#34;yourname&#34;\ngit config --global user.email &#34;youremail&#34;</code></pre></div><p>这里的yourname输入你的GitHub用户名，youremail输入你GitHub的邮箱。这样GitHub才能知道你是不是对应它的账户。</p><p>可以用以下两条，检查一下你有没有输对</p><div class=\"highlight\"><pre><code class=\"language-text\">git config user.name\ngit config user.email</code></pre></div><p>然后创建SSH,一路回车</p><div class=\"highlight\"><pre><code class=\"language-text\">ssh-keygen -t rsa -C &#34;youremail&#34;</code></pre></div><p>这个时候它会告诉你已经生成了.ssh的文件夹。在你的电脑中找到这个文件夹。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-a9593147853b8f2485f05d1f04d45bb6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"731\" data-rawheight=\"255\" class=\"origin_image zh-lightbox-thumb\" width=\"731\" data-original=\"https://pic3.zhimg.com/v2-a9593147853b8f2485f05d1f04d45bb6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;731&#39; height=&#39;255&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"731\" data-rawheight=\"255\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"731\" data-original=\"https://pic3.zhimg.com/v2-a9593147853b8f2485f05d1f04d45bb6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-a9593147853b8f2485f05d1f04d45bb6_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>ssh，简单来讲，就是一个秘钥，其中，<code>id_rsa</code>是你这台电脑的私人秘钥，不能给别人看的，<code>id_rsa.pub</code>是公共秘钥，可以随便给别人看。把这个公钥放在GitHub上，这样当你链接GitHub自己的账户时，它就会根据公钥匹配你的私钥，当能够相互匹配时，才能够顺利的通过git上传你的文件到GitHub上。</p><p>而后在GitHub的setting中，找到SSH keys的设置选项，点击<code>New SSH key</code> 把你的<code>id_rsa.pub</code>里面的信息复制进去。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-aad16de6208ce8a7b2cd51b16a1b2aa2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1324\" data-rawheight=\"591\" class=\"origin_image zh-lightbox-thumb\" width=\"1324\" data-original=\"https://pic3.zhimg.com/v2-aad16de6208ce8a7b2cd51b16a1b2aa2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1324&#39; height=&#39;591&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1324\" data-rawheight=\"591\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1324\" data-original=\"https://pic3.zhimg.com/v2-aad16de6208ce8a7b2cd51b16a1b2aa2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-aad16de6208ce8a7b2cd51b16a1b2aa2_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>在gitbash中，查看是否成功</p><div class=\"highlight\"><pre><code class=\"language-text\">ssh -T git@github.com</code></pre></div><h2><b>6. 将hexo部署到GitHub</b></h2><p>这一步，我们就可以将hexo和GitHub关联起来，也就是将hexo生成的文章部署到GitHub上，打开站点配置文件 <code>_config.yml</code>，翻到最后，修改为 YourgithubName就是你的GitHub账户</p><div class=\"highlight\"><pre><code class=\"language-text\">deploy:\n  type: git\n  repo: https://github.com/YourgithubName/YourgithubName.github.io.git\n  branch: master</code></pre></div><p>这个时候需要先安装deploy-git ，也就是部署的命令,这样你才能用命令部署到GitHub。</p><div class=\"highlight\"><pre><code class=\"language-text\">npm install hexo-deployer-git --save</code></pre></div><p>然后</p><div class=\"highlight\"><pre><code class=\"language-text\">hexo clean\nhexo generate\nhexo deploy</code></pre></div><p>其中 <code>hexo clean</code>清除了你之前生成的东西，也可以不加。 <code>hexo generate</code> 顾名思义，生成静态文章，可以用 <code>hexo g</code>缩写 <code>hexo deploy</code> 部署文章，可以用<code>hexo d</code>缩写</p><p>注意deploy时可能要你输入username和password。</p><p>得到下图就说明部署成功了，过一会儿就可以在<code>http://yourname.github.io</code> 这个网站看到你的博客了！！ </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-8fb2b2026aa0a748594642bb0f53f72b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"782\" data-rawheight=\"66\" class=\"origin_image zh-lightbox-thumb\" width=\"782\" data-original=\"https://pic4.zhimg.com/v2-8fb2b2026aa0a748594642bb0f53f72b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;782&#39; height=&#39;66&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"782\" data-rawheight=\"66\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"782\" data-original=\"https://pic4.zhimg.com/v2-8fb2b2026aa0a748594642bb0f53f72b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-8fb2b2026aa0a748594642bb0f53f72b_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>7. 设置个人域名</b></h2><p>现在你的个人网站的地址是 <code>yourname.github.io</code>，如果觉得这个网址逼格不太够，这就需要你设置个人域名了。但是需要花钱。</p><p>注册一个阿里云账户,在<a href=\"https://link.zhihu.com/?target=https%3A//wanwang.aliyun.com/%3Fspm%3D5176.8142029.digitalization.2.e9396d3e46JCc5\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">阿里云</a>上买一个域名，我买的是 <code>fangzh.top</code>，各个后缀的价格不太一样，比如最广泛的.com就比较贵，看个人喜好咯。</p><p>你需要先去进行实名认证,然后在域名控制台中，看到你购买的域名。</p><p>点<b>解析</b>进去，添加解析。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-11ace56a64930bf2e997aad3618f548e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"474\" data-rawheight=\"284\" class=\"origin_image zh-lightbox-thumb\" width=\"474\" data-original=\"https://pic3.zhimg.com/v2-11ace56a64930bf2e997aad3618f548e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;474&#39; height=&#39;284&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"474\" data-rawheight=\"284\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"474\" data-original=\"https://pic3.zhimg.com/v2-11ace56a64930bf2e997aad3618f548e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-11ace56a64930bf2e997aad3618f548e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>其中，192.30.252.153 和 192.30.252.154 是GitHub的服务器地址。 <b>注意，解析线路选择默认</b>，不要像我一样选境外。这个境外是后面来做国内外分流用的,在后面的博客中会讲到。记得现在选择<b>默认</b>！！</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-e572fb0a5de7519d606e80c49829f40f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"875\" data-rawheight=\"191\" class=\"origin_image zh-lightbox-thumb\" width=\"875\" data-original=\"https://pic4.zhimg.com/v2-e572fb0a5de7519d606e80c49829f40f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;875&#39; height=&#39;191&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"875\" data-rawheight=\"191\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"875\" data-original=\"https://pic4.zhimg.com/v2-e572fb0a5de7519d606e80c49829f40f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-e572fb0a5de7519d606e80c49829f40f_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>登录GitHub，进入之前创建的仓库，点击settings，设置Custom domain，输入你的域名<code>fangzh.top</code></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b75ea5d443b68a8600601e596553ad09_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"388\" data-rawheight=\"166\" class=\"content_image\" width=\"388\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;388&#39; height=&#39;166&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"388\" data-rawheight=\"166\" class=\"content_image lazy\" width=\"388\" data-actualsrc=\"https://pic2.zhimg.com/v2-b75ea5d443b68a8600601e596553ad09_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>然后在你的博客文件source中创建一个名为CNAME文件，不要后缀。写上你的域名。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-1b974b97c26a5a9ff27d973f31d804b3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"296\" data-rawheight=\"226\" class=\"content_image\" width=\"296\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;296&#39; height=&#39;226&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"296\" data-rawheight=\"226\" class=\"content_image lazy\" width=\"296\" data-actualsrc=\"https://pic4.zhimg.com/v2-1b974b97c26a5a9ff27d973f31d804b3_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>最后，在gitbash中，输入</p><div class=\"highlight\"><pre><code class=\"language-text\">hexo clean\nhexo g\nhexo d</code></pre></div><p>过不了多久，再打开你的浏览器，输入你自己的域名，就可以看到搭建的网站啦！</p><p>接下来你就可以正式开始写文章了。</p><div class=\"highlight\"><pre><code class=\"language-text\">hexo new newpapername</code></pre></div><p>然后在source/_post中打开markdown文件，就可以开始编辑了。当你写完的时候，再</p><div class=\"highlight\"><pre><code class=\"language-text\">hexo clean\nhexo g\nhexo d</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>第二部分</b></h2><p>hexo的基本配置，更换主题，实现多终端工作，以及在coding page部署实现国内外分流。</p><h2><b>1. hexo基本配置</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p>在文件根目录下的<code>_config.yml</code>，就是整个hexo框架的配置文件了。可以在里面修改大部分的配置。详细可参考<a href=\"https://link.zhihu.com/?target=https%3A//hexo.io/zh-cn/docs/configuration\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">官方的配置</a>描述。</p><h2><b>网站</b></h2><p><b>参数描述</b><code>title</code>网站标题<code>subtitle</code>网站副标题<code>description</code>网站描述<code>author</code>您的名字<code>language</code>网站使用的语言<code>timezone</code>网站时区。Hexo 默认使用您电脑的时区。<a href=\"https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/List_of_tz_database_time_zones\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">时区列表</a>。比如说：<code>America/New_York</code>, <code>Japan</code>, 和 <code>UTC</code> 。</p><p>其中，<code>description</code>主要用于SEO，告诉搜索引擎一个关于您站点的简单描述，通常建议在其中包含您网站的关键词。<code>author</code>参数用于主题显示文章的作者。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>网址</b></h2><p><b>参数描述</b><code>url</code>网址<code>root</code>网站根目录<code>permalink</code>文章的 <a href=\"https://link.zhihu.com/?target=https%3A//hexo.io/zh-cn/docs/permalinks\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">永久链接</a> 格式<code>permalink_defaults</code>永久链接中各部分的默认值</p><p>在这里，你需要把<code>url</code>改成你的网站域名。</p><p>permalink，也就是你生成某个文章时的那个链接格式。</p><p>比如我新建一个文章叫<code>temp.md</code>，那么这个时候他自动生成的地址就是<code>http://yoursite.com/2018/09/05/temp</code>。</p><p>以下是官方给出的示例，关于链接的变量还有很多，需要的可以去官网上查找 <a href=\"https://link.zhihu.com/?target=https%3A//hexo.io/zh-cn/docs/permalinks\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">永久链接</a> 。</p><p><b>参数结果</b><code>:year/:month/:day/:title/</code>2013/07/14/hello-world<code>:year-:month-:day-:title.html</code>2013-07-14-hello-world.html<code>:category/:title</code>foo/bar/hello-world</p><p class=\"ztext-empty-paragraph\"><br/></p><p>再往下翻，中间这些都默认就好了。</p><p class=\"ztext-empty-paragraph\"><br/></p><div class=\"highlight\"><pre><code class=\"language-text\">theme: landscape\n​\n# Deployment\n## Docs: https://hexo.io/docs/deployment.html\ndeploy:\n  type: git\n  repo: &lt;repository url&gt;\n  branch: [branch]\n​</code></pre></div><p><code>theme</code>就是选择什么主题，也就是在<code>theme</code>这个文件夹下，在官网上有很多个主题，默认给你安装的是<code>lanscape</code>这个主题。当你需要更换主题时，在官网上下载，把主题的文件放在<code>theme</code>文件夹下，再修改这个参数就可以了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>接下来这个<code>deploy</code>就是网站的部署的，<code>repo</code>就是仓库(<code>Repository</code>)的简写。<code>branch</code>选择仓库的哪个分支。这个在之前进行github page部署的时候已经修改过了，不再赘述。而这个在后面进行双平台部署的时候会再次用到。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>Front-matter</b></h2><p>Front-matter 是文件最上方以 <code>---</code> 分隔的区域，用于指定个别文件的变量，举例来说：</p><div class=\"highlight\"><pre><code class=\"language-text\">title: Hello World\ndate: 2013/7/13 20:46:25\n---</code></pre></div><p>下是预先定义的参数，您可在模板中使用这些参数值并加以利用。</p><p><b>参数描述</b><code>layout</code>布局<code>title</code>标题<code>date</code>建立日期<code>updated</code>更新日期<code>comments</code>开启文章的评论功能<code>tags</code>标签（不适用于分页）<code>categories</code>分类（不适用于分页）<code>permalink</code>覆盖文章网址</p><p class=\"ztext-empty-paragraph\"><br/></p><p>其中，分类和标签需要区别一下，分类具有顺序性和层次性，也就是说 <code>Foo, Bar</code> 不等于 <code>Bar, Foo</code>；而标签没有顺序和层次。</p><div class=\"highlight\"><pre><code class=\"language-text\">categories:\n- Diary\ntags:\n- PS3\n- Games</code></pre></div><h2><b>layout（布局）</b></h2><p>当你每一次使用代码</p><div class=\"highlight\"><pre><code class=\"language-text\">hexo new paper</code></pre></div><p>它其实默认使用的是<code>post</code>这个布局，也就是在<code>source</code>文件夹下的<code>_post</code>里面。</p><p>Hexo 有三种默认布局：<code>post</code>、<code>page</code> 和 <code>draft</code>，它们分别对应不同的路径，而您自定义的其他布局和 <code>post</code> 相同，都将储存到 <code>source/_posts</code> 文件夹。</p><p><b>布局路径</b><code>postsource/_postspagesourcedraftsource/_drafts</code></p><p>而new这个命令其实是：</p><div class=\"highlight\"><pre><code class=\"language-text\">hexo new [layout] &lt;title&gt;</code></pre></div><p>只不过这个layout默认是post罢了。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>page</b></h2><p>如果你想另起一页，那么可以使用</p><div class=\"highlight\"><pre><code class=\"language-text\">hexo new page board</code></pre></div><p>系统会自动给你在source文件夹下创建一个board文件夹，以及board文件夹中的index.md，这样你访问的board对应的链接就是<code>http://xxx.xxx/board</code></p><h2><b>draft</b></h2><p>draft是草稿的意思，也就是你如果想写文章，又不希望被看到，那么可以</p><div class=\"highlight\"><pre><code class=\"language-text\">hexo new draft newpage</code></pre></div><p>这样会在source/_draft中新建一个newpage.md文件，如果你的草稿文件写的过程中，想要预览一下，那么可以使用</p><div class=\"highlight\"><pre><code class=\"language-text\">hexo server --draft</code></pre></div><p>在本地端口中开启服务预览。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>如果你的草稿文件写完了，想要发表到post中，</p><div class=\"highlight\"><pre><code class=\"language-text\">hexo publish draft newpage</code></pre></div><p>就会自动把newpage.md发送到post中。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>2. 更换主题</b></h2><p>到这一步，如果你觉得默认的<code>landscape</code>主题不好看，那么可以在官网的主题中，选择你喜欢的一个主题进行修改就可以啦。<a href=\"https://link.zhihu.com/?target=https%3A//hexo.io/themes/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">点这里</a></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-37456c59d1378608228faf65aa58e836_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1866\" data-rawheight=\"989\" class=\"origin_image zh-lightbox-thumb\" width=\"1866\" data-original=\"https://pic3.zhimg.com/v2-37456c59d1378608228faf65aa58e836_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1866&#39; height=&#39;989&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1866\" data-rawheight=\"989\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1866\" data-original=\"https://pic3.zhimg.com/v2-37456c59d1378608228faf65aa58e836_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-37456c59d1378608228faf65aa58e836_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>这里有200多个主题可以选。不过最受欢迎的就是那么几个，比如<a href=\"https://link.zhihu.com/?target=https%3A//github.com/theme-next/hexo-theme-next\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">NexT主题</a>，非常的简洁好看，大多数人都选择这个，关于这个的教程也比较多。不过我选择的是<a href=\"https://link.zhihu.com/?target=https%3A//github.com/ppoffice/hexo-theme-hueman\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">hueman</a>这个主题，好像是从WordPress移植过来的，展示效果如下：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-4973f83e8d757d7126cc5091f03ef452_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1848\" data-rawheight=\"944\" class=\"origin_image zh-lightbox-thumb\" width=\"1848\" data-original=\"https://pic3.zhimg.com/v2-4973f83e8d757d7126cc5091f03ef452_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1848&#39; height=&#39;944&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1848\" data-rawheight=\"944\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1848\" data-original=\"https://pic3.zhimg.com/v2-4973f83e8d757d7126cc5091f03ef452_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-4973f83e8d757d7126cc5091f03ef452_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>不管怎么样，至少是符合我个人的审美。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>直接在github链接上下载下来，然后放到<code>theme</code>文件夹下就行了，然后再在刚才说的配置文件中把<code>theme</code>换成那个主题文件夹的名字，它就会自动在<code>theme</code>文件夹中搜索你配置的主题。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>而后进入<code>hueman</code>这个文件夹，可以看到里面也有一个配置文件<code>_config.xml</code>，貌似它默认是<code>_config.xml.example</code>，把它复制一份，重命名为<code>_config.xml</code>就可以了。这个配置文件是修改你整个主题的配置文件。</p><h2><b>menu（菜单栏）</b></h2><p>也就是上面菜单栏上的这些东西。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-f7759c435dcc46d877312b0b2c8b8e7d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"697\" data-rawheight=\"75\" class=\"origin_image zh-lightbox-thumb\" width=\"697\" data-original=\"https://pic2.zhimg.com/v2-f7759c435dcc46d877312b0b2c8b8e7d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;697&#39; height=&#39;75&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"697\" data-rawheight=\"75\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"697\" data-original=\"https://pic2.zhimg.com/v2-f7759c435dcc46d877312b0b2c8b8e7d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-f7759c435dcc46d877312b0b2c8b8e7d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>其中，About这个你是找不到网页的，因为你的文章中没有about这个东西。如果你想要的话，可以执行命令</p><div class=\"highlight\"><pre><code class=\"language-text\">hexo new page about</code></pre></div><p>它就会在根目录下<code>source</code>文件夹中新建了一个<code>about</code>文件夹，以及index.md，在index.md中写上你想要写的东西，就可以在网站上展示出来了。</p><p>如果你想要自己再自定义一个菜单栏的选项，那么就</p><div class=\"highlight\"><pre><code class=\"language-text\">hexo new page yourdiy</code></pre></div><p>然后在主题配置文件的menu菜单栏添加一个 <code>Yourdiy : /yourdiy</code>，注意冒号后面要有空格，以及前面的空格要和menu中默认的保持整齐。然后在<code>languages</code>文件夹中，找到<code>zh-CN.yml</code>，在index中添加<code>yourdiy: &#39;中文意思&#39;</code>就可以显示中文了。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>customize(定制)</b></h2><p>在这里可以修改你的个人logo，默认是那个hueman，在<code>source/css/images</code>文件夹中放入自己要的logo，再改一下<code>url</code>的链接名字就可以了。</p><p><code>favicon</code>是网站中出现的那个小图标的icon，找一张你喜欢的logo，然后转换成ico格式，放在images文件夹下，配置一下路径就行。</p><p><code>social_links</code> ，可以显示你的社交链接，而且是有logo的。</p><p><b>tips:</b></p><p>在这里可以添加一个rss功能，也就是那个符号像wifi一样的东西。</p><h2><b>添加RSS</b></h2><p><b>1. 什么是RSS？</b></p><p>RSS也就是订阅功能，你可以理解为类似与订阅公众号的功能，来订阅各种博客，杂志等等。</p><p><b>2. 为什么要用RSS？</b></p><p>就如同订阅公众号一样，你对某个公众号感兴趣，你总不可能一直时不时搜索这个公众号来看它的文章吧。博客也是一样，如果你喜欢某个博主，或者某个平台的内容，你可以通过RSS订阅它们，然后在RSS阅读器上可以实时推送这些消息。现在网上的垃圾消息太多了，如果你每一天都在看这些消息中度过，漫无目的的浏览，只会让你的时间一点一点的流逝，太不值得了。如果你关注的博主每次都发的消息都是精华，而且不是每一天十几条几十条的轰炸你，那么这个博主就值得你的关注，你就可以通过RSS订阅他。</p><p>在我的理解中，如果你不想每天都被那些没有质量的消息轰炸，只想安安静静的关注几个博主，每天看一些有质量的内容也不用太多，那么RSS订阅值得你的拥有。</p><p><b>3. 添加RSS功能</b></p><p>先安装RSS插件</p><div class=\"highlight\"><pre><code class=\"language-text\">npm i hexo-generator-feed</code></pre></div><p>而后在你整个项目的<code>_config.yml</code>中找到Extensions，添加：</p><div class=\"highlight\"><pre><code class=\"language-text\"># Extensions\n## Plugins: https://hexo.io/plugins/\n#RSS订阅\nplugin:\n- hexo-generator-feed\n#Feed Atom\nfeed:\n  type: atom\n  path: atom.xml\n  limit: 20</code></pre></div><p>这个时候你的RSS链接就是  域名<code>/atom.xml</code>了。</p><p>所以，在主题配置文件中的这个<code>social links</code>，开启RSS的页面功能，这样你网站上就有那个像wifi一样符号的RSS logo了，注意空格。</p><div class=\"highlight\"><pre><code class=\"language-text\">rss: /atom.xml\n</code></pre></div><p><b>4. 如何关注RSS？</b></p><p>首先，你需要一个RSS阅读器，在这里我推荐<code>inoreader</code>，宇宙第一RSS阅读器，而且中文支持的挺好。不过它没有PC端的程序，只有网页版，chrome上有插件。在官网上用google账号或者自己注册账号登录，就可以开始你的关注之旅了。</p><p>每次需要关注某个博主时，就点开他的RSS链接，把链接复制到<code>inoreader</code>上，就能关注了，当然，如果是比较大众化的很厉害的博主，你直接搜名字也可以的，比如每个人都非常佩服的阮一峰大师，直接在阅读器上搜索<code>阮一峰</code>，应该就能出来了。</p><p>我关注的比如，阮一峰的网络日志，月光博客，知乎精选等，都很不错。当然，还有我！！赶快关注我吧！你值得拥有：<a href=\"https://link.zhihu.com/?target=http%3A//fangzh.top/atom.xml\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">fangzh.top/atom.xml</span><span class=\"invisible\"></span></a></p><p>在安卓端，inoreader也有下载，不过因为国内google是登录不了的，你需要在inoreader官网上把你的密码修改了，然后就可以用账户名和密码登录了。</p><p>在IOS端，没用过，好像是reader 3可以支持inoreader账户，还有个readon也不错，可以去试试。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>widgets(侧边栏)</b></h2><p>侧边栏的小标签，如果你想自己增加一个，比如我增加了一个联系方式，那么我把<code>communication</code>写在上面，在<code>zh-CN.yml</code>中的<code>sidebar</code>，添加<code>communication: &#39;中文&#39;</code>。</p><p>然后在<code>hueman/layout/widget</code>中添加一个<code>communicaiton.ejs</code>，填入模板：</p><div class=\"highlight\"><pre><code class=\"language-js\"><span class=\"o\">&lt;%</span> <span class=\"k\">if</span> <span class=\"p\">(</span><span class=\"nx\">site</span><span class=\"p\">.</span><span class=\"nx\">posts</span><span class=\"p\">.</span><span class=\"nx\">length</span><span class=\"p\">)</span> <span class=\"p\">{</span> <span class=\"o\">%&gt;</span>\n    <span class=\"o\">&lt;</span><span class=\"nx\">div</span> <span class=\"k\">class</span><span class=\"o\">=</span><span class=\"s2\">&#34;widget-wrap widget-list&#34;</span><span class=\"o\">&gt;</span>\n        <span class=\"o\">&lt;</span><span class=\"nx\">h3</span> <span class=\"k\">class</span><span class=\"o\">=</span><span class=\"s2\">&#34;widget-title&#34;</span><span class=\"o\">&gt;&lt;%=</span> <span class=\"nx\">__</span><span class=\"p\">(</span><span class=\"s1\">&#39;sidebar.communiation&#39;</span><span class=\"p\">)</span> <span class=\"o\">%&gt;&lt;</span><span class=\"err\">/h3&gt;</span>\n        <span class=\"o\">&lt;</span><span class=\"nx\">div</span> <span class=\"k\">class</span><span class=\"o\">=</span><span class=\"s2\">&#34;widget&#34;</span><span class=\"o\">&gt;</span>\n            <span class=\"c\">&lt;!--</span><span class=\"nx\">这里添加你要写的内容</span><span class=\"o\">--&gt;</span>\n        <span class=\"o\">&lt;</span><span class=\"err\">/div&gt;</span>\n    <span class=\"o\">&lt;</span><span class=\"err\">/div&gt;</span>\n<span class=\"o\">&lt;%</span> <span class=\"p\">}</span> <span class=\"o\">%&gt;</span>\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>search(搜索框)</b></h2><p>默认搜索框是不能够用的，</p><blockquote>you need to install <code>hexo-generator-json-content</code> before using Insight Search</blockquote><p>它已经告诉你了，如果想要使用，就安装这个插件。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>comment(评论系统)</b></h2><p>这里的多数都是国外的，基本用不了。这个<code>valine</code>好像不错，还能统计文章阅读量，可以自己试一试，<a href=\"https://link.zhihu.com/?target=https%3A//valine.js.org/quickstart.html%23npm\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">链接</a>。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>miscellaneous(其他)</b></h2><p>这里我就改了一个<code>links</code>，可以添加友链。注意空格要对！不然会报错！</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>总结：</b></h2><p>整个主题看起来好像很复杂的样子，但是仔细捋一捋其实也比较流畅，</p><ul><li>languages: 顾名思义</li><li>layout：布局文件，其实后期想要修改自定义网站上的东西，添加各种各样的信息，主要是在这里修改，其中<code>comment</code>是评论系统，<code>common</code>是常规的布局，最常修改的在这里面，比如修改页面<code>head</code>和<code>footer</code>的内容。</li><li>scripts：js脚本，暂时没什么用</li><li>source：里面放了一些css的样式，以及图片</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>3. git分支进行多终端工作</b></h2><p>问题来了，如果你现在在自己的笔记本上写的博客，部署在了网站上，那么你在家里用台式机，或者实验室的台式机，发现你电脑里面没有博客的文件，或者要换电脑了，最后不知道怎么移动文件，怎么办？</p><p>在这里我们就可以利用git的分支系统进行多终端工作了，这样每次打开不一样的电脑，只需要进行简单的配置和在github上把文件同步下来，就可以无缝操作了。</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>机制</b></h2><p>机制是这样的，由于<code>hexo d</code>上传部署到github的其实是hexo编译后的文件，是用来生成网页的，不包含源文件。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-32ab30d0d28b916a204c03de4ed0fc4f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"288\" data-rawheight=\"475\" class=\"content_image\" width=\"288\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;288&#39; height=&#39;475&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"288\" data-rawheight=\"475\" class=\"content_image lazy\" width=\"288\" data-actualsrc=\"https://pic4.zhimg.com/v2-32ab30d0d28b916a204c03de4ed0fc4f_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>也就是上传的是在本地目录里自动生成的<code>.deploy_git</code>里面。</p><p>其他文件 ，包括我们写在source 里面的，和配置文件，主题文件，都没有上传到github</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-59bb330178a4e010d9818911f789082b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"515\" data-rawheight=\"280\" class=\"origin_image zh-lightbox-thumb\" width=\"515\" data-original=\"https://pic4.zhimg.com/v2-59bb330178a4e010d9818911f789082b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;515&#39; height=&#39;280&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"515\" data-rawheight=\"280\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"515\" data-original=\"https://pic4.zhimg.com/v2-59bb330178a4e010d9818911f789082b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-59bb330178a4e010d9818911f789082b_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>所以可以利用git的分支管理，将源文件上传到github的另一个分支即可。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>上传分支</b></h2><p>首先，先在github上新建一个hexo分支，如图：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ebb3e05632e85ab036663390305caa1c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"457\" data-rawheight=\"412\" class=\"origin_image zh-lightbox-thumb\" width=\"457\" data-original=\"https://pic1.zhimg.com/v2-ebb3e05632e85ab036663390305caa1c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;457&#39; height=&#39;412&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"457\" data-rawheight=\"412\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"457\" data-original=\"https://pic1.zhimg.com/v2-ebb3e05632e85ab036663390305caa1c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-ebb3e05632e85ab036663390305caa1c_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>然后在这个仓库的settings中，选择默认分支为hexo分支（这样每次同步的时候就不用指定分支，比较方便）。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-1899b6219f3787832652813b958b9b3d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"957\" data-rawheight=\"496\" class=\"origin_image zh-lightbox-thumb\" width=\"957\" data-original=\"https://pic2.zhimg.com/v2-1899b6219f3787832652813b958b9b3d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;957&#39; height=&#39;496&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"957\" data-rawheight=\"496\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"957\" data-original=\"https://pic2.zhimg.com/v2-1899b6219f3787832652813b958b9b3d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-1899b6219f3787832652813b958b9b3d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>然后在本地的任意目录下，打开git bash，</p><div class=\"highlight\"><pre><code class=\"language-text\">git clone git@github.com:ZJUFangzh/ZJUFangzh.github.io.git</code></pre></div><p>将其克隆到本地，因为默认分支已经设成了hexo，所以clone时只clone了hexo。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>接下来在克隆到本地的<code>ZJUFangzh.github.io</code>中，把除了.git 文件夹外的所有文件都删掉</p><p> 把之前我们写的博客源文件全部复制过来，除了<code>.deploy_git</code>。这里应该说一句，复制过来的源文件应该有一个<code>.gitignore</code>，用来忽略一些不需要的文件，如果没有的话，自己新建一个，在里面写上如下，表示这些类型文件不需要git：</p><div class=\"highlight\"><pre><code class=\"language-text\">.DS_Store\nThumbs.db\ndb.json\n*.log\nnode_modules/\npublic/\n.deploy*/\n</code></pre></div><p>注意，如果你之前克隆过theme中的主题文件，那么应该把主题文件中的<code>.git</code>文件夹删掉，因为git不能嵌套上传，最好是显示隐藏文件，检查一下有没有，否则上传的时候会出错，导致你的主题文件无法上传，这样你的配置在别的电脑上就用不了了。</p><p>而后</p><div class=\"highlight\"><pre><code class=\"language-text\">git add .\ngit commit –m &#34;add branch&#34;\ngit push </code></pre></div><p>这样就上传完了，可以去你的github上看一看hexo分支有没有上传上去，其中<code>node_modules</code>、<code>public</code>、<code>db.json</code>已经被忽略掉了，没有关系，不需要上传的，因为在别的电脑上需要重新输入命令安装 。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-a94330ca825f4debde8ce7ceeb8f8394_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1250\" data-rawheight=\"621\" class=\"origin_image zh-lightbox-thumb\" width=\"1250\" data-original=\"https://pic1.zhimg.com/v2-a94330ca825f4debde8ce7ceeb8f8394_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1250&#39; height=&#39;621&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1250\" data-rawheight=\"621\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1250\" data-original=\"https://pic1.zhimg.com/v2-a94330ca825f4debde8ce7ceeb8f8394_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-a94330ca825f4debde8ce7ceeb8f8394_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>这样就上传完了。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>更换电脑操作</b></h2><p>一样的，跟之前的环境搭建一样，</p><ul><li>安装git</li></ul><div class=\"highlight\"><pre><code class=\"language-text\">sudo apt-get install git\n</code></pre></div><ul><li>设置git全局邮箱和用户名</li></ul><div class=\"highlight\"><pre><code class=\"language-text\">git config --global user.name &#34;yourgithubname&#34;\ngit config --global user.email &#34;yourgithubemail&#34;\n</code></pre></div><ul><li>设置ssh key</li></ul><div class=\"highlight\"><pre><code class=\"language-text\">ssh-keygen -t rsa -C &#34;youremail&#34;\n#生成后填到github和coding上（有coding平台的话）\n#验证是否成功\nssh -T git@github.com\nssh -T git@git.coding.net #(有coding平台的话)\n</code></pre></div><ul><li>安装nodejs</li></ul><div class=\"highlight\"><pre><code class=\"language-text\">sudo apt-get install nodejs\nsudo apt-get install npm\n</code></pre></div><ul><li>安装hexo  </li></ul><div class=\"highlight\"><pre><code class=\"language-text\">sudo npm install hexo-cli -g\n</code></pre></div><p>但是已经不需要初始化了，</p><p>直接在任意文件夹下，</p><div class=\"highlight\"><pre><code class=\"language-text\">git clone git@………………\n</code></pre></div><p>然后进入克隆到的文件夹：</p><div class=\"highlight\"><pre><code class=\"language-text\">cd xxx.github.io\nnpm install\nnpm install hexo-deployer-git --save\n</code></pre></div><p>生成，部署：</p><div class=\"highlight\"><pre><code class=\"language-text\">hexo g\nhexo d\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>然后就可以开始写你的新博客了</p><div class=\"highlight\"><pre><code class=\"language-text\">hexo new newpage\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p><b>Tips:</b></p><ol><li>不要忘了，每次写完最好都把源文件上传一下</li></ol><div class=\"highlight\"><pre><code class=\"language-text\">git add .\ngit commit –m &#34;xxxx&#34;\ngit push \n</code></pre></div><ol><li>如果是在已经编辑过的电脑上，已经有clone文件夹了，那么，每次只要和远端同步一下就行了</li></ol><div class=\"highlight\"><pre><code class=\"language-text\">git pull\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>4. coding page上部署实现国内外分流</b></h2><p>之前我们已经把hexo托管在github了，但是github是国外的，而且百度的爬虫是不能够爬取github的，所以如果你希望你做的博客能够在百度引擎上被收录，而且想要更快的访问，那么可以在国内的coding page做一个托管，这样在国内访问就是coding page，国外就走github page。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>1. 申请coding账户，新建项目</b></p><p>先申请一个账户，然后创建新的项目，这一步项目名称应该是随意的。</p><p><b>2.  添加ssh key</b></p><p>这一步跟github一样。</p><p>添加后，检查一下是不是添加成功</p><div class=\"highlight\"><pre><code class=\"language-text\">ssh -T git@git.coding.net\n</code></pre></div><p><b>3. 修改_config.yml</b></p><p>hexo官方文档是这样的：</p><div class=\"highlight\"><pre><code class=\"language-text\">deploy:\n  type: git\n  message: [message]\n  repo:\n    github: &lt;repository url&gt;,[branch]\n    coding: &lt;repository url&gt;,[branch] \n</code></pre></div><p>那么，我们只需要：</p><div class=\"highlight\"><pre><code class=\"language-text\">deploy:\n  type: git\n  repo: \n    coding: git@git.coding.net:ZJUFangzh/ZJUFangzh.git,master\n    github: git@github.com:ZJUFangzh/ZJUFangzh.github.io.git,master\n</code></pre></div><p><b>4. 部署</b></p><p>保存一下，直接</p><div class=\"highlight\"><pre><code class=\"language-text\">hexo g\nhexo d\n</code></pre></div><p>这样就可以在coding的项目上看到你部署的文件了。</p><p><b>5. 开启coding pages服务，绑定域名</b></p><p>如图：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-1136f014eae5c088808a265ded2c6845_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1215\" data-rawheight=\"426\" class=\"origin_image zh-lightbox-thumb\" width=\"1215\" data-original=\"https://pic2.zhimg.com/v2-1136f014eae5c088808a265ded2c6845_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1215&#39; height=&#39;426&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1215\" data-rawheight=\"426\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1215\" data-original=\"https://pic2.zhimg.com/v2-1136f014eae5c088808a265ded2c6845_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-1136f014eae5c088808a265ded2c6845_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>6. 阿里云添加解析</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-9a9ffbf08b2ec0917027ce2f3d14cfaf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"871\" data-rawheight=\"335\" class=\"origin_image zh-lightbox-thumb\" width=\"871\" data-original=\"https://pic4.zhimg.com/v2-9a9ffbf08b2ec0917027ce2f3d14cfaf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;871&#39; height=&#39;335&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"871\" data-rawheight=\"335\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"871\" data-original=\"https://pic4.zhimg.com/v2-9a9ffbf08b2ec0917027ce2f3d14cfaf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-9a9ffbf08b2ec0917027ce2f3d14cfaf_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>这个时候就可以把之前github的解析改成境外，把coding的解析设为默认了。</p><p><b>7. 去除coding page的跳转广告</b></p><p>coding page的一个比较恶心人的地方就是，你只是银牌会员的话，访问会先跳转到一个广告，再到你自己的域名。那么它也给出了消除的办法。右上角切换到coding的旧版界面，默认新版是不行的。然后再来到<code>pages服务</code>这里。</p><p>这里：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-2b9a0f546007202e28bdf917edd559df_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"692\" data-rawheight=\"443\" class=\"origin_image zh-lightbox-thumb\" width=\"692\" data-original=\"https://pic4.zhimg.com/v2-2b9a0f546007202e28bdf917edd559df_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;692&#39; height=&#39;443&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"692\" data-rawheight=\"443\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"692\" data-original=\"https://pic4.zhimg.com/v2-2b9a0f546007202e28bdf917edd559df_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-2b9a0f546007202e28bdf917edd559df_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>只要你在页面上添加一行文字，写<code>Hosted by Coding Pages</code>，然后点下面的小勾勾，两个工作日内它就会审核通过了。</p><div class=\"highlight\"><pre><code class=\"language-text\">&lt;p&gt;Hosted by &lt;a href=&#34;https://pages.coding.me&#34; style=&#34;font-weight: bold&#34;&gt;Coding Pages&lt;/a&gt;&lt;/p&gt;\n</code></pre></div><p>我的选择是把这一行代码放在主题文件夹<code>/layout/common/footer.ejs</code>里面，也就是本来在页面中看到的页脚部分。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-fdc3b5fe205f9214a127464563395489_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"773\" data-rawheight=\"219\" class=\"origin_image zh-lightbox-thumb\" width=\"773\" data-original=\"https://pic2.zhimg.com/v2-fdc3b5fe205f9214a127464563395489_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;773&#39; height=&#39;219&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"773\" data-rawheight=\"219\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"773\" data-original=\"https://pic2.zhimg.com/v2-fdc3b5fe205f9214a127464563395489_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-fdc3b5fe205f9214a127464563395489_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>当然，为了统一，我又在后面加上了and <b>Github</b>哈哈，可以不加。</p><div class=\"highlight\"><pre><code class=\"language-text\">&lt;p&gt;&lt;span&gt;Hosted by &lt;a href=&#34;https://pages.coding.me&#34; style=&#34;font-weight: bold&#34;&gt;Coding Pages&lt;/a&gt;&lt;/span&gt; and &lt;span&gt;&lt;a href=&#34;https://github.com&#34; style=&#34;font-weight: bold&#34;&gt;Github&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;\n</code></pre></div><p>这是最终加上去的代码。</p><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>第三部分</b></h2><p>hexo添加各种功能，包括搜索的SEO，阅读量统计，访问量统计和评论系统等。</p><p>这一部分参考了: <a href=\"https://link.zhihu.com/?target=http%3A//visugar.com/2017/08/01/20170801HexoPlugins/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">visugar.com</a>这里面说的很详细了。</p><h2><b>1. SEO优化 </b></h2><p>推广是很麻烦的事情，怎么样别人才能知道我们呢，首先需要让搜索引擎收录你的这个网站，别人才能搜索的到。那么这就需要SEO优化了。</p><blockquote>SEO是由英文Search Engine Optimization缩写而来， 中文意译为“搜索引擎优化”。SEO是指通过站内优化比如网站结构调整、网站内容建设、网站代码优化等以及站外优化。</blockquote><h2><b>百度seo</b></h2><p>刚建站的时候是没有搜索引擎收录我们的网站的。可以在搜索引擎中输入<code>site:&lt;域名&gt;</code></p><p>来查看一下。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>1. 登录百度站长平台添加网站</b></p><p>登录<a href=\"https://link.zhihu.com/?target=https%3A//ziyuan.baidu.com/linksubmit/index%3F\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">百度站长平台</a>，在站点管理中添加你自己的网站。</p><p>验证网站有三种方式：文件验证、HTML标签验证、CNAME验证。</p><p>第三种方式最简单，只要将它提供给你的那个xxxxx使用CNAME解析到<a href=\"https://link.zhihu.com/?target=http%3A//xxx.baidu.com\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">xxx.baidu.com</span><span class=\"invisible\"></span></a>就可以了。也就是登录你的阿里云，把这个解析填进去就OK了。</p><p><b>2. 提交链接</b></p><p>我们需要使用npm自动生成网站的sitemap，然后将生成的sitemap提交到百度和其他搜索引擎</p><div class=\"highlight\"><pre><code class=\"language-text\">npm install hexo-generator-sitemap --save     \nnpm install hexo-generator-baidu-sitemap --save\n</code></pre></div><p>这时候你需要在你的根目录下<code>_config.xml</code>中看看url有没有改成你自己的：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-c7be3d7d0eb3e838e24de8095ac86338_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"387\" data-rawheight=\"119\" class=\"content_image\" width=\"387\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;387&#39; height=&#39;119&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"387\" data-rawheight=\"119\" class=\"content_image lazy\" width=\"387\" data-actualsrc=\"https://pic1.zhimg.com/v2-c7be3d7d0eb3e838e24de8095ac86338_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>重新部署后，就可以在public文件夹下看到生成的sitemap.xml和baidusitemap.xml了。</p><p>然后就可以向百度提交你的站点地图了。</p><p>这里建议使用自动提交。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-466acfdc8359d94dc8064ae6d4d66a2d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"552\" data-rawheight=\"754\" class=\"origin_image zh-lightbox-thumb\" width=\"552\" data-original=\"https://pic2.zhimg.com/v2-466acfdc8359d94dc8064ae6d4d66a2d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;552&#39; height=&#39;754&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"552\" data-rawheight=\"754\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"552\" data-original=\"https://pic2.zhimg.com/v2-466acfdc8359d94dc8064ae6d4d66a2d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-466acfdc8359d94dc8064ae6d4d66a2d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>自动提交又分为三种：主动推送、自动推送、sitemap。</p><p>可以三个一起提交不要紧，我选择的是后两种。</p><ul><li>自动推送：把百度生成的自动推送代码，放在主题文件<code>/layout/common/head.ejs</code>的适当位置，然后验证一下就可以了。</li><li>sitemap：把两个sitemap地址，提交上去，看到状态正常就OK了。</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ce4d363f858c25de4ada3f5bb02ecdf3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1122\" data-rawheight=\"796\" class=\"origin_image zh-lightbox-thumb\" width=\"1122\" data-original=\"https://pic4.zhimg.com/v2-ce4d363f858c25de4ada3f5bb02ecdf3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1122&#39; height=&#39;796&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1122\" data-rawheight=\"796\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1122\" data-original=\"https://pic4.zhimg.com/v2-ce4d363f858c25de4ada3f5bb02ecdf3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-ce4d363f858c25de4ada3f5bb02ecdf3_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>ps:</b> 百度收录比较慢，慢慢等个十天半个月再去<code>site:&lt;域名&gt;</code>看看有没有被收录。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>google的SEO</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p>流程一样，google更简单，而且收录更快，进入<a href=\"https://link.zhihu.com/?target=https%3A//search.google.com/search-console/sitemaps%3Fresource_id%3Dhttp%3A//fangzh.top/%26hl%3Dzh-CN\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">google站点地图</a>，提交网站和sitemap.xml，就可以了。</p><p>如果你这个域名在google这里出了问题，那你就提交 <a href=\"https://link.zhihu.com/?target=http%3A//yourname.github.io\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://</span><span class=\"visible\">yourname.github.io</span><span class=\"invisible\"></span></a>，这个链接，效果是一样的。</p><p>不出意外的话一天内google就能收录你的网站了。</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-ef73c3e069f4b73417a6068c0d100471_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"939\" data-rawheight=\"776\" class=\"origin_image zh-lightbox-thumb\" width=\"939\" data-original=\"https://pic2.zhimg.com/v2-ef73c3e069f4b73417a6068c0d100471_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;939&#39; height=&#39;776&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"939\" data-rawheight=\"776\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"939\" data-original=\"https://pic2.zhimg.com/v2-ef73c3e069f4b73417a6068c0d100471_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-ef73c3e069f4b73417a6068c0d100471_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>其他的搜索，如搜狗搜索，360搜索，流程是一样的，这里就不再赘述。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>2. 评论系统</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p>评论系统有很多，但是很多都是墙外的用不了，之前说过这个valine好像集成在hueman和next主题里面了，但是我还没有研究过，我看的是<a href=\"https://link.zhihu.com/?target=http%3A//visugar.com/2017/08/01/20170801HexoPlugins/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">visugar</a>这个博主用的来比力评论系统，感觉也还不错。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><a href=\"https://link.zhihu.com/?target=https%3A//livere.com/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">来比力官网</a>，注册好后，点击管理页面，在<code>代码管理</code>中找到安装代码：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-c263c8377b987a0a32895b463db24a83_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1064\" data-rawheight=\"796\" class=\"origin_image zh-lightbox-thumb\" width=\"1064\" data-original=\"https://pic4.zhimg.com/v2-c263c8377b987a0a32895b463db24a83_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1064&#39; height=&#39;796&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1064\" data-rawheight=\"796\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1064\" data-original=\"https://pic4.zhimg.com/v2-c263c8377b987a0a32895b463db24a83_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-c263c8377b987a0a32895b463db24a83_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>获取安装代码后，在主题的comment下新建一个文件放入刚刚那段代码，再找到article文件，找到如下代码，若没有则直接在footer后面添加即可。livebe即为刚刚所创文件名称。</p><div class=\"highlight\"><pre><code class=\"language-text\">&lt;%- partial(&#39;comment/livebe&#39;) %&gt;\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>然后可以自己设置一些东西：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-67202029da93884e296d145ba65039cb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1123\" data-rawheight=\"873\" class=\"origin_image zh-lightbox-thumb\" width=\"1123\" data-original=\"https://pic4.zhimg.com/v2-67202029da93884e296d145ba65039cb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1123&#39; height=&#39;873&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1123\" data-rawheight=\"873\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1123\" data-original=\"https://pic4.zhimg.com/v2-67202029da93884e296d145ba65039cb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-67202029da93884e296d145ba65039cb_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>还可以设置评论提醒，这样别人评论你的时候就可以及时知道了。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>3. 添加百度统计</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p>百度统计可以在后台上看到你网站的访问数，浏览量，浏览链接分布等很重要的信息。所以添加百度统计能更有效的让你掌握你的网站情况。</p><p><a href=\"https://link.zhihu.com/?target=https%3A//tongji.baidu.com/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">百度统计</a>，注册一下，这里的账号好像和百度账号不是一起的。</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-d881052d4962047f570c0109c05898c1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"771\" data-rawheight=\"753\" class=\"origin_image zh-lightbox-thumb\" width=\"771\" data-original=\"https://pic2.zhimg.com/v2-d881052d4962047f570c0109c05898c1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;771&#39; height=&#39;753&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"771\" data-rawheight=\"753\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"771\" data-original=\"https://pic2.zhimg.com/v2-d881052d4962047f570c0109c05898c1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-d881052d4962047f570c0109c05898c1_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>照样把代码复制到<code>head.ejs</code>文件中，然后再进行一下安装检查，半小时左右就可以在百度统计里面看到自己的网站信息了。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>4. 文章阅读量统计leanCloud</b></h2><p class=\"ztext-empty-paragraph\"><br/></p><p><a href=\"https://link.zhihu.com/?target=https%3A//leancloud.cn/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">leanCloud</a>，进去后注册一下，进入后创建一个应用：</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-f396ec826528cc56a0c2c7aa610f736e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"615\" data-rawheight=\"450\" class=\"origin_image zh-lightbox-thumb\" width=\"615\" data-original=\"https://pic3.zhimg.com/v2-f396ec826528cc56a0c2c7aa610f736e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;615&#39; height=&#39;450&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"615\" data-rawheight=\"450\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"615\" data-original=\"https://pic3.zhimg.com/v2-f396ec826528cc56a0c2c7aa610f736e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-f396ec826528cc56a0c2c7aa610f736e_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>在<code>存储</code>中创建Class，命名为Counter,</p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-002babac7a525a7c321165228d9f23ae_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"602\" data-rawheight=\"720\" class=\"origin_image zh-lightbox-thumb\" width=\"602\" data-original=\"https://pic3.zhimg.com/v2-002babac7a525a7c321165228d9f23ae_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;602&#39; height=&#39;720&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"602\" data-rawheight=\"720\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"602\" data-original=\"https://pic3.zhimg.com/v2-002babac7a525a7c321165228d9f23ae_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-002babac7a525a7c321165228d9f23ae_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>然后在设置页面看到你的<code>应用Key</code>，在主题的配置文件中：</p><div class=\"highlight\"><pre><code class=\"language-text\">leancloud_visitors:\n  enable: true\n  app_id: 你的id\n  app_key: 你的key\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>在<code>article.ejs</code>中适当的位置添加如下，这要看你让文章的阅读量统计显示在哪个地方了，</p><div class=\"highlight\"><pre><code class=\"language-text\">阅读数量:&lt;span id=&#34;&lt;%= url_for(post.path) %&gt;&#34; class=&#34;leancloud_visitors&#34; data-flag-title=&#34;&lt;%- post.title %&gt;&#34;&gt;&lt;/span&gt;次\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>然后在<code>footer.ejs</code>的最后，添加：</p><div class=\"highlight\"><pre><code class=\"language-text\">&lt;script src=&#34;//cdn1.lncld.net/static/js/2.5.0/av-min.js&#34;&gt;&lt;/script&gt;\n&lt;script&gt;\n    var APP_ID = &#39;你的app id&#39;;\n    var APP_KEY = &#39;你的app key&#39;;\n    AV.init({\n        appId: APP_ID,\n        appKey: APP_KEY\n    });\n    // 显示次数\n    function showTime(Counter) {\n        var query = new AV.Query(&#34;Counter&#34;);\n        if($(&#34;.leancloud_visitors&#34;).length &gt; 0){\n            var url = $(&#34;.leancloud_visitors&#34;).attr(&#39;id&#39;).trim();\n            // where field\n            query.equalTo(&#34;words&#34;, url);\n            // count\n            query.count().then(function (number) {\n                // There are number instances of MyClass where words equals url.\n                $(document.getElementById(url)).text(number?  number : &#39;--&#39;);\n            }, function (error) {\n                // error is an instance of AVError.\n            });\n        }\n    }\n    // 追加pv\n    function addCount(Counter) {\n        var url = $(&#34;.leancloud_visitors&#34;).length &gt; 0 ? $(&#34;.leancloud_visitors&#34;).attr(&#39;id&#39;).trim() : &#39;icafebolger.com&#39;;\n        var Counter = AV.Object.extend(&#34;Counter&#34;);\n        var query = new Counter;\n        query.save({\n            words: url\n        }).then(function (object) {\n        })\n    }\n    $(function () {\n        var Counter = AV.Object.extend(&#34;Counter&#34;);\n        addCount(Counter);\n        showTime(Counter);\n    });\n&lt;/script&gt;\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>重新部署后就可以了。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>5. 引入不蒜子访问量和访问人次统计</b></h2><p>不蒜子的添加非常非常方便，<a href=\"https://link.zhihu.com/?target=http%3A//busuanzi.ibruce.info/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">不蒜子</a></p><p>在<code>footer.ejs</code>中的合适位置，看你要显示在哪个地方，添加：</p><div class=\"highlight\"><pre><code class=\"language-text\">&lt;!--这一段是不蒜子的访问量统计代码--&gt;\n&lt;script async src=&#34;//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js&#34;&gt;&lt;/script&gt;\n&lt;span id=&#34;busuanzi_container_site_pv&#34;&gt;本站总访问量&lt;span id=&#34;busuanzi_value_site_pv&#34;&gt;&lt;/span&gt;次 &amp;nbsp;   &lt;/span&gt;\n&lt;span id=&#34;busuanzi_container_site_uv&#34;&gt;访客数&lt;span id=&#34;busuanzi_value_site_uv&#34;&gt;&lt;/span&gt;人次&lt;/span&gt;\n</code></pre></div><p>就可以了。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>总结</b></h2><p>到这里就基本做完了。其实都是参考别的博主的设置的，不一定仅限于hueman主题，其他主题的设置也是大体相同的，所以如果你希望设置别的主题，那么仔细看一下这个主题的代码结构，也能够把上边的功能添加进去。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>多看看别的博主的那些功能，如果有你能找到自己喜欢的功能，那么好好发动搜索技能，很快就能找到怎么做了。加油吧！</p>", 
            "topic": [
                {
                    "tag": "个人博客", 
                    "tagLink": "https://api.zhihu.com/topics/19593765"
                }, 
                {
                    "tag": "Hexo", 
                    "tagLink": "https://api.zhihu.com/topics/19851557"
                }, 
                {
                    "tag": "博客", 
                    "tagLink": "https://api.zhihu.com/topics/19550419"
                }
            ], 
            "comments": [
                {
                    "userName": "徐天盛", 
                    "userLink": "https://www.zhihu.com/people/fea7718b7e595871c3475fde62a90901", 
                    "content": "<p>非常好的教程，感谢！</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "益者三友", 
                    "userLink": "https://www.zhihu.com/people/af570118bf3ecd69482b7291614ffa68", 
                    "content": "<p>感谢分享！新手入门，请问下如何加一个自己的主页进去呢，就是点击主页这两个字跳转到个人主页的html...目前看到的都是如何上传md</p>", 
                    "likes": 1, 
                    "childComments": []
                }, 
                {
                    "userName": "黎先生", 
                    "userLink": "https://www.zhihu.com/people/049afcac650a1847466c6b1e192de26b", 
                    "content": "你是我看到讲的比较清晰 逻辑好的一篇文章，谢谢作者", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "fly bird", 
                    "userLink": "https://www.zhihu.com/people/f7ffbc4456fc5869beec43cbade753aa", 
                    "content": "<p>非常详细 感谢！</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "银杏树", 
                    "userLink": "https://www.zhihu.com/people/d7d09ab4dc2bf0a4887ffa1471750ede", 
                    "content": "写的很详细，非常感谢。哎！多希望能有一个人能够给我指导", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "不呐呐", 
                    "userLink": "https://www.zhihu.com/people/991b2c52b022efd4ce5a448382696913", 
                    "content": "感谢", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "知乎用户", 
                    "userLink": "https://www.zhihu.com/people/0", 
                    "content": "<p>非常系统的建站指南，赞</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "王子", 
                    "userLink": "https://www.zhihu.com/people/a17b047ed380b6fcc681a9dd1b15e9b5", 
                    "content": "1.在哪个地方修改name/author ？<br>2.修改完需要deploy吗？", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "thirteenthree", 
                            "userLink": "https://www.zhihu.com/people/5c2aded44b591ba45f85e8f1f1e60211", 
                            "content": "<p>主配置config.yml文件,不是主题目录下那个config.yml,是与themes同一目录的那个config.yml</p>", 
                            "likes": 0, 
                            "replyToAuthor": "王子"
                        }, 
                        {
                            "userName": "王子", 
                            "userLink": "https://www.zhihu.com/people/a17b047ed380b6fcc681a9dd1b15e9b5", 
                            "content": "好的，已ok。非常感谢", 
                            "likes": 0, 
                            "replyToAuthor": "thirteenthree"
                        }
                    ]
                }, 
                {
                    "userName": "小呆呆的看", 
                    "userLink": "https://www.zhihu.com/people/3ea4319cb91890d032d022ea031e906a", 
                    "content": "答主可不可以给个联想方式，搭建博客过程中遇到几个问题想请教一下您", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "Mr.chuan", 
                    "userLink": "https://www.zhihu.com/people/3a5cbe6d56d1d114c7de1929ede7917e", 
                    "content": "<p>主题文件夹中有node_modules文件夹，hexo s就启动不起来，删除这个文件夹就可以启动起来，这个有没有见过啊。</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "thirteenthree", 
                    "userLink": "https://www.zhihu.com/people/5c2aded44b591ba45f85e8f1f1e60211", 
                    "content": "<p>写这种教程的人总有个弊端没有说,菜单内容是主题支持才能使用,而这种支持又与主题开发人有关,有的人没有开发菜单,就跟about一样,好多人是没有开发about,这种页面只有,自己有能力开发才能随心所欲的使用.hexo默认主题只有home与archives,不管你加哪个菜单,都是空白的</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "半个阿宅", 
                    "userLink": "https://www.zhihu.com/people/eb8fa58456549b12d484c101c6436f00", 
                    "content": "你好,我想请教您一个问题,就是hexo是必须把所有文章放到_post文件夹下面吗?能不能够把文章 进行分类,放到不同的文件夹下,在把分类文件夹放到_post,", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "Mirtio", 
                    "userLink": "https://www.zhihu.com/people/afdfb8463a71afc5bd98f4f174014fea", 
                    "content": "<p>部署成功了电脑不能访问手机可以是什么原因？</p>", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }
    ], 
    "url": "https://zhuanlan.zhihu.com/fangzh"
}
