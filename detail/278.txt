{
    "title": "通往AI之路", 
    "description": "", 
    "followers": [
        "https://www.zhihu.com/people/tian-shi-zhi-yi-88-82", 
        "https://www.zhihu.com/people/pan-jian-97-6", 
        "https://www.zhihu.com/people/san-shi-jie-9", 
        "https://www.zhihu.com/people/yinbingzhe", 
        "https://www.zhihu.com/people/shawn_xy", 
        "https://www.zhihu.com/people/18070496669", 
        "https://www.zhihu.com/people/blue-blue-83", 
        "https://www.zhihu.com/people/kay-1-26", 
        "https://www.zhihu.com/people/liluyi", 
        "https://www.zhihu.com/people/xgn911", 
        "https://www.zhihu.com/people/cen-bing-18", 
        "https://www.zhihu.com/people/hao-zhang-44-23", 
        "https://www.zhihu.com/people/yang-wan-li-20-19", 
        "https://www.zhihu.com/people/gao-sui-yun", 
        "https://www.zhihu.com/people/Steven_Jokes", 
        "https://www.zhihu.com/people/fei-hong-76-96", 
        "https://www.zhihu.com/people/dailin23", 
        "https://www.zhihu.com/people/victor-zhao", 
        "https://www.zhihu.com/people/pyb2468", 
        "https://www.zhihu.com/people/wo-de-shi-jie-17-22", 
        "https://www.zhihu.com/people/niutou-36", 
        "https://www.zhihu.com/people/zhao-yan-dong-30", 
        "https://www.zhihu.com/people/xiao-yu-59-21-91", 
        "https://www.zhihu.com/people/faker-18", 
        "https://www.zhihu.com/people/ni-hui-51-23", 
        "https://www.zhihu.com/people/shentg", 
        "https://www.zhihu.com/people/wei-jun-26-94", 
        "https://www.zhihu.com/people/zhenning-li-59", 
        "https://www.zhihu.com/people/ai-learning-25", 
        "https://www.zhihu.com/people/reatank", 
        "https://www.zhihu.com/people/fhackcidd", 
        "https://www.zhihu.com/people/0-12-Chen", 
        "https://www.zhihu.com/people/yang-kai-ge", 
        "https://www.zhihu.com/people/huang-chao-lin-6", 
        "https://www.zhihu.com/people/hao-tian-87", 
        "https://www.zhihu.com/people/kong-chui-shun", 
        "https://www.zhihu.com/people/she-liang", 
        "https://www.zhihu.com/people/wang-zhi-wen-15", 
        "https://www.zhihu.com/people/zhao-jun-83-21-53", 
        "https://www.zhihu.com/people/mr-zhu-x", 
        "https://www.zhihu.com/people/xiao-xiao-15-59-26", 
        "https://www.zhihu.com/people/fighting-11-67", 
        "https://www.zhihu.com/people/xiao-liu-41-24", 
        "https://www.zhihu.com/people/zhang-jia-peng-3-34", 
        "https://www.zhihu.com/people/zheng-zhi-hang", 
        "https://www.zhihu.com/people/wan-yi-53-91", 
        "https://www.zhihu.com/people/huang-shuai-4", 
        "https://www.zhihu.com/people/xu-jun-qing-63", 
        "https://www.zhihu.com/people/wst-dc", 
        "https://www.zhihu.com/people/lu-hong-liang-22", 
        "https://www.zhihu.com/people/ha-jdd", 
        "https://www.zhihu.com/people/myeclipse-47", 
        "https://www.zhihu.com/people/wen-tong-14-31", 
        "https://www.zhihu.com/people/yzy-98-77", 
        "https://www.zhihu.com/people/ggson1222", 
        "https://www.zhihu.com/people/Sing-F", 
        "https://www.zhihu.com/people/prince-62-20", 
        "https://www.zhihu.com/people/kou-men-de-jiu-kai-men", 
        "https://www.zhihu.com/people/gu-dian-pai-liu-mang", 
        "https://www.zhihu.com/people/hominine", 
        "https://www.zhihu.com/people/william-wei-73", 
        "https://www.zhihu.com/people/rao-zhe-heng", 
        "https://www.zhihu.com/people/qianghuazhnag", 
        "https://www.zhihu.com/people/future-67-92", 
        "https://www.zhihu.com/people/jiangfuture", 
        "https://www.zhihu.com/people/yang-jing-lin-16", 
        "https://www.zhihu.com/people/shjiang-23", 
        "https://www.zhihu.com/people/sunyong2012", 
        "https://www.zhihu.com/people/dd-lll", 
        "https://www.zhihu.com/people/pi-pi-3-74", 
        "https://www.zhihu.com/people/yan-yu-chuan-lin", 
        "https://www.zhihu.com/people/yong-ge-16-26", 
        "https://www.zhihu.com/people/mr.jeffrey28", 
        "https://www.zhihu.com/people/liu-lin-37-30", 
        "https://www.zhihu.com/people/szouc", 
        "https://www.zhihu.com/people/fandy-11-69", 
        "https://www.zhihu.com/people/King-billy-tyrant", 
        "https://www.zhihu.com/people/lan-lan-lan-lan-96-82", 
        "https://www.zhihu.com/people/zhangsan-75-77", 
        "https://www.zhihu.com/people/jia-wu-33", 
        "https://www.zhihu.com/people/yi-tian-xu", 
        "https://www.zhihu.com/people/wu-long-69", 
        "https://www.zhihu.com/people/yuan-shan-50-35", 
        "https://www.zhihu.com/people/xing-yun-36-25", 
        "https://www.zhihu.com/people/xiang-kao-bo", 
        "https://www.zhihu.com/people/huang-liao-liao-44", 
        "https://www.zhihu.com/people/wang-tian-yuan-77", 
        "https://www.zhihu.com/people/zhi-xing-zhe-4-8", 
        "https://www.zhihu.com/people/heng-tong-27-37", 
        "https://www.zhihu.com/people/zhang-da-ye-5-73", 
        "https://www.zhihu.com/people/tang-yi-fan-87-42", 
        "https://www.zhihu.com/people/superjay-56", 
        "https://www.zhihu.com/people/wang-hai-32-15", 
        "https://www.zhihu.com/people/chenlongzhen.tech", 
        "https://www.zhihu.com/people/chwhc", 
        "https://www.zhihu.com/people/zheng-cheng-gong-ming", 
        "https://www.zhihu.com/people/sha-kai-wen", 
        "https://www.zhihu.com/people/linkerlin", 
        "https://www.zhihu.com/people/peimeng-sui", 
        "https://www.zhihu.com/people/lao-mi-7-58", 
        "https://www.zhihu.com/people/sam-chen-79", 
        "https://www.zhihu.com/people/katsu-79", 
        "https://www.zhihu.com/people/hai-yang-42-97", 
        "https://www.zhihu.com/people/wu-zhi-91-62", 
        "https://www.zhihu.com/people/huang-dou-dou-63-70", 
        "https://www.zhihu.com/people/ljing-kang", 
        "https://www.zhihu.com/people/li-dong-lun-51", 
        "https://www.zhihu.com/people/zhang-lei-17-51", 
        "https://www.zhihu.com/people/cerena-8", 
        "https://www.zhihu.com/people/clyce", 
        "https://www.zhihu.com/people/xiang-fang-pin-93", 
        "https://www.zhihu.com/people/spearous", 
        "https://www.zhihu.com/people/heaven-38-1", 
        "https://www.zhihu.com/people/llozz", 
        "https://www.zhihu.com/people/ccqian-qian-79", 
        "https://www.zhihu.com/people/zhao-xiao-hu-31", 
        "https://www.zhihu.com/people/wang-jun-84-98-33", 
        "https://www.zhihu.com/people/luozhongbin", 
        "https://www.zhihu.com/people/xu-yan-84-38", 
        "https://www.zhihu.com/people/lin-63-43-54", 
        "https://www.zhihu.com/people/meng-hu-14-67", 
        "https://www.zhihu.com/people/imustli-qi", 
        "https://www.zhihu.com/people/yue-xing-64-49", 
        "https://www.zhihu.com/people/zheng-yi-49-69", 
        "https://www.zhihu.com/people/miao-zi-85-77", 
        "https://www.zhihu.com/people/littleeyes-59", 
        "https://www.zhihu.com/people/xiaoxuanzaizai", 
        "https://www.zhihu.com/people/vctzhou", 
        "https://www.zhihu.com/people/learn-furtherly", 
        "https://www.zhihu.com/people/cheng-bin-71-49", 
        "https://www.zhihu.com/people/ace-85-1", 
        "https://www.zhihu.com/people/xai-97-68", 
        "https://www.zhihu.com/people/wang-yi-de-18", 
        "https://www.zhihu.com/people/lou-min-hao", 
        "https://www.zhihu.com/people/jiu-zhi-65-68", 
        "https://www.zhihu.com/people/zeavan", 
        "https://www.zhihu.com/people/tangyan1988", 
        "https://www.zhihu.com/people/yimu-cheng-lin-96", 
        "https://www.zhihu.com/people/jiao-jian-89", 
        "https://www.zhihu.com/people/feng-lang-55-26", 
        "https://www.zhihu.com/people/jiang-yu-guang-9", 
        "https://www.zhihu.com/people/cbhhh-91", 
        "https://www.zhihu.com/people/he-zhi-qiang-52-74", 
        "https://www.zhihu.com/people/hh-zz-77-1", 
        "https://www.zhihu.com/people/Turiny", 
        "https://www.zhihu.com/people/kyolxs", 
        "https://www.zhihu.com/people/hu-pan-de-si-nian", 
        "https://www.zhihu.com/people/caifengsteven", 
        "https://www.zhihu.com/people/si-ye-88-10", 
        "https://www.zhihu.com/people/menscheniskind", 
        "https://www.zhihu.com/people/ptmagic-magic", 
        "https://www.zhihu.com/people/he-zu-dao-92-37", 
        "https://www.zhihu.com/people/he-he-67-27-85", 
        "https://www.zhihu.com/people/zzx-47-7", 
        "https://www.zhihu.com/people/ling-cheng-yu-24", 
        "https://www.zhihu.com/people/bin-li-44", 
        "https://www.zhihu.com/people/zhou-fan-99-47", 
        "https://www.zhihu.com/people/yue-nu-li-yue-xing-yun-83-48", 
        "https://www.zhihu.com/people/yi-tai-zhi-40", 
        "https://www.zhihu.com/people/xiao-xiao-shen-le", 
        "https://www.zhihu.com/people/jiong-jiong-de-li-da-jiong", 
        "https://www.zhihu.com/people/te-li-du-xing-1-28", 
        "https://www.zhihu.com/people/ma-jing-46-7", 
        "https://www.zhihu.com/people/ok1022", 
        "https://www.zhihu.com/people/ygmpkk", 
        "https://www.zhihu.com/people/qiu-bi-long-76-86", 
        "https://www.zhihu.com/people/even-xie", 
        "https://www.zhihu.com/people/shensdu", 
        "https://www.zhihu.com/people/oliver-kahn-60", 
        "https://www.zhihu.com/people/towan", 
        "https://www.zhihu.com/people/xiao-qing-xu-60-58", 
        "https://www.zhihu.com/people/han-beng-beng-26", 
        "https://www.zhihu.com/people/tankzhouNo1", 
        "https://www.zhihu.com/people/ban-ge-nong-min", 
        "https://www.zhihu.com/people/du-du-du-29-95", 
        "https://www.zhihu.com/people/yi-li-80", 
        "https://www.zhihu.com/people/sakerjin", 
        "https://www.zhihu.com/people/qiong-58-83", 
        "https://www.zhihu.com/people/zhao-jing-huan", 
        "https://www.zhihu.com/people/du-yu-feng-97-40", 
        "https://www.zhihu.com/people/chevson", 
        "https://www.zhihu.com/people/yang-yang-yang-45-40-51", 
        "https://www.zhihu.com/people/ni-fei-84", 
        "https://www.zhihu.com/people/chen-bo-47-82", 
        "https://www.zhihu.com/people/duan-xing-99", 
        "https://www.zhihu.com/people/blueman", 
        "https://www.zhihu.com/people/dai-wei-66-30", 
        "https://www.zhihu.com/people/yeu-yang", 
        "https://www.zhihu.com/people/damion_cmy", 
        "https://www.zhihu.com/people/tu-wen-zhe", 
        "https://www.zhihu.com/people/zhang-zi-qian-13-62", 
        "https://www.zhihu.com/people/tzxu", 
        "https://www.zhihu.com/people/ztc-90", 
        "https://www.zhihu.com/people/xuhao-62", 
        "https://www.zhihu.com/people/peng_ynu", 
        "https://www.zhihu.com/people/qin-shi-ming-yue-29", 
        "https://www.zhihu.com/people/Sostudio", 
        "https://www.zhihu.com/people/yu-chang-cun-54", 
        "https://www.zhihu.com/people/a-xi-1-74", 
        "https://www.zhihu.com/people/tan-zheng-86-36", 
        "https://www.zhihu.com/people/li-ying-ru", 
        "https://www.zhihu.com/people/mi-le-fa-shi-20", 
        "https://www.zhihu.com/people/wang-meng-77-57", 
        "https://www.zhihu.com/people/wang-jing-69-10", 
        "https://www.zhihu.com/people/reparo-26", 
        "https://www.zhihu.com/people/forrestbing", 
        "https://www.zhihu.com/people/qian-shan-xue-6", 
        "https://www.zhihu.com/people/AI_CONTROL", 
        "https://www.zhihu.com/people/chen-xiao-feng-14-77", 
        "https://www.zhihu.com/people/marvin-65-57", 
        "https://www.zhihu.com/people/zhihu-tg", 
        "https://www.zhihu.com/people/streamers", 
        "https://www.zhihu.com/people/sanschan", 
        "https://www.zhihu.com/people/xie-nai-er-de", 
        "https://www.zhihu.com/people/shi-feng-97-84", 
        "https://www.zhihu.com/people/wiliam-36", 
        "https://www.zhihu.com/people/zhang-qin-sheng-sheng", 
        "https://www.zhihu.com/people/leng-ruo-bing-21-29", 
        "https://www.zhihu.com/people/yi-quan-60-7", 
        "https://www.zhihu.com/people/maxiaobo", 
        "https://www.zhihu.com/people/chen-hao-dong-84-38", 
        "https://www.zhihu.com/people/joewang-38", 
        "https://www.zhihu.com/people/yi-yu-98-98", 
        "https://www.zhihu.com/people/runhua-zhao", 
        "https://www.zhihu.com/people/chen-hao-74-48", 
        "https://www.zhihu.com/people/wumoyany", 
        "https://www.zhihu.com/people/xbchen82-76", 
        "https://www.zhihu.com/people/mousehuu", 
        "https://www.zhihu.com/people/gushan", 
        "https://www.zhihu.com/people/zhang-luo-meng", 
        "https://www.zhihu.com/people/whjxnyzh", 
        "https://www.zhihu.com/people/miss_gavva", 
        "https://www.zhihu.com/people/zhan-yinan", 
        "https://www.zhihu.com/people/kevin-hill", 
        "https://www.zhihu.com/people/w33haa-36", 
        "https://www.zhihu.com/people/pei-xu-68", 
        "https://www.zhihu.com/people/CJay_Wang", 
        "https://www.zhihu.com/people/huanghaiping", 
        "https://www.zhihu.com/people/gao-lei-7", 
        "https://www.zhihu.com/people/edwin-yi-69", 
        "https://www.zhihu.com/people/moni-gg", 
        "https://www.zhihu.com/people/chen-ying-50-56-4", 
        "https://www.zhihu.com/people/tinker-34-37", 
        "https://www.zhihu.com/people/meng-yu-38-35", 
        "https://www.zhihu.com/people/xiao-de-83-54", 
        "https://www.zhihu.com/people/def-jam", 
        "https://www.zhihu.com/people/xiao-shan-94-8", 
        "https://www.zhihu.com/people/wu-yiping-20", 
        "https://www.zhihu.com/people/zhang-rui-71-28-78", 
        "https://www.zhihu.com/people/chen-qian-dr"
    ], 
    "article": [
        {
            "url": "https://zhuanlan.zhihu.com/p/71394061", 
            "userName": "黄伟亮", 
            "userLink": "https://www.zhihu.com/people/294306d6706f830e3cce35fb4acf46cb", 
            "upvote": 14, 
            "title": "阿里云中间件挑战赛初赛——实战分析", 
            "content": "<p>首次上榜</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-7ca0c873d0338dbba4127e5a7362085d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"800\" data-rawheight=\"448\" class=\"origin_image zh-lightbox-thumb\" width=\"800\" data-original=\"https://pic2.zhimg.com/v2-7ca0c873d0338dbba4127e5a7362085d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;800&#39; height=&#39;448&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"800\" data-rawheight=\"448\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"800\" data-original=\"https://pic2.zhimg.com/v2-7ca0c873d0338dbba4127e5a7362085d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-7ca0c873d0338dbba4127e5a7362085d_b.jpg\"/></figure><p>再次上榜</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-e3c627cce3eabdc816a84372d67dbe28_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1214\" data-rawheight=\"886\" class=\"origin_image zh-lightbox-thumb\" width=\"1214\" data-original=\"https://pic1.zhimg.com/v2-e3c627cce3eabdc816a84372d67dbe28_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1214&#39; height=&#39;886&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1214\" data-rawheight=\"886\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1214\" data-original=\"https://pic1.zhimg.com/v2-e3c627cce3eabdc816a84372d67dbe28_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-e3c627cce3eabdc816a84372d67dbe28_b.jpg\"/></figure><p>最后冲刺</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-856709614fe181852de2f141e21c53db_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1485\" data-rawheight=\"883\" class=\"origin_image zh-lightbox-thumb\" width=\"1485\" data-original=\"https://pic4.zhimg.com/v2-856709614fe181852de2f141e21c53db_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1485&#39; height=&#39;883&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1485\" data-rawheight=\"883\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1485\" data-original=\"https://pic4.zhimg.com/v2-856709614fe181852de2f141e21c53db_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-856709614fe181852de2f141e21c53db_b.jpg\"/></figure><p>第一次参赛这样的编程比赛，如果要用一句话来总结初赛这一个多月来的感受的话，就是：</p><p>累，并快乐着。</p><p>从6月22号开始，<br/>一共提交了正好200次测评；<br/>两百多次代码提交；<br/>进行了五次代码重构；<br/>先后迭代了六个算法方案。</p><p>这是一个非常难得的高并发编程的实战经历。<br/>除了实实在在地锻炼了一把并发编程外，笔者还摸索出一套通过JAVA来实现简单强化学习的模式。</p><p>以下便是比赛的详细总结。</p><hr/><h2>赛题内容</h2><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-5ddb0d21f34f0660ede0434b77128eb3_b.jpg\" data-size=\"normal\" data-rawwidth=\"975\" data-rawheight=\"768\" class=\"origin_image zh-lightbox-thumb\" width=\"975\" data-original=\"https://pic4.zhimg.com/v2-5ddb0d21f34f0660ede0434b77128eb3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;975&#39; height=&#39;768&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"975\" data-rawheight=\"768\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"975\" data-original=\"https://pic4.zhimg.com/v2-5ddb0d21f34f0660ede0434b77128eb3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-5ddb0d21f34f0660ede0434b77128eb3_b.jpg\"/><figcaption>来源：阿里巴巴2019中间件性能挑战赛</figcaption></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-feea51160f802e45cce78e9667a7587e_b.jpg\" data-size=\"normal\" data-rawwidth=\"786\" data-rawheight=\"246\" class=\"origin_image zh-lightbox-thumb\" width=\"786\" data-original=\"https://pic3.zhimg.com/v2-feea51160f802e45cce78e9667a7587e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;786&#39; height=&#39;246&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"786\" data-rawheight=\"246\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"786\" data-original=\"https://pic3.zhimg.com/v2-feea51160f802e45cce78e9667a7587e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-feea51160f802e45cce78e9667a7587e_b.jpg\"/><figcaption>来源：阿里巴巴2019中间件性能挑战赛</figcaption></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-6a24d0a1afb5b188989cc46e3116c68b_b.jpg\" data-size=\"normal\" data-rawwidth=\"1268\" data-rawheight=\"354\" class=\"origin_image zh-lightbox-thumb\" width=\"1268\" data-original=\"https://pic4.zhimg.com/v2-6a24d0a1afb5b188989cc46e3116c68b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1268&#39; height=&#39;354&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1268\" data-rawheight=\"354\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1268\" data-original=\"https://pic4.zhimg.com/v2-6a24d0a1afb5b188989cc46e3116c68b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-6a24d0a1afb5b188989cc46e3116c68b_b.jpg\"/><figcaption>来源：阿里巴巴2019中间件性能挑战赛</figcaption></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2>思路解释</h2><p>既然要实现负载均衡，本质上就是要解决“如何分配请求到哪个上游服务”的问题。</p><p>必须要说明的是，尽管笔者最后的成绩也算不错，但对Dubbo的认识仅仅处于入门阶段，若文中相关的认识有误，敬请指正。</p><p>为了更好地认识赛题中的线程协作模型，我画了一个比较丑的示意图：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-40c6487c830b70c41b939a4fe9c7f9e6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1636\" data-rawheight=\"544\" class=\"origin_image zh-lightbox-thumb\" width=\"1636\" data-original=\"https://pic3.zhimg.com/v2-40c6487c830b70c41b939a4fe9c7f9e6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1636&#39; height=&#39;544&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1636\" data-rawheight=\"544\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1636\" data-original=\"https://pic3.zhimg.com/v2-40c6487c830b70c41b939a4fe9c7f9e6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-40c6487c830b70c41b939a4fe9c7f9e6_b.jpg\"/></figure><p>示意图说明：</p><ul><li>小人代表压测工具的线程，一共1024个线程；</li><li>粉色箭头代表异步调用；</li><li>白色箭头代表同步调用；</li><li>左边最外层框框代表consumer进程；</li><li>右边最外层框框代表provider进程；</li><li>内层的小框代表该进程内部的工作线程</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p>搞清楚了程序大致的运作情况后，才开始下手思考，如何分配才能更高效。</p><p><b>方案一：按固定比例加权随机</b></p><p>由于题目已知条件是三个provider的最大线程数分别为200、450、650，因此很自然地会联想到使用这个比例来进行加权随机分配。</p><p>这个方案大概的成绩范围在113万左右。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-eec595c8c21b18edfbb5b1d4cec563db_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"350\" data-rawheight=\"158\" class=\"content_image\" width=\"350\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;350&#39; height=&#39;158&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"350\" data-rawheight=\"158\" class=\"content_image lazy\" width=\"350\" data-actualsrc=\"https://pic4.zhimg.com/v2-eec595c8c21b18edfbb5b1d4cec563db_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-c3aba624f3d1b1e47932e6f876d30388_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1863\" data-rawheight=\"721\" class=\"origin_image zh-lightbox-thumb\" width=\"1863\" data-original=\"https://pic1.zhimg.com/v2-c3aba624f3d1b1e47932e6f876d30388_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1863&#39; height=&#39;721&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1863\" data-rawheight=\"721\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1863\" data-original=\"https://pic1.zhimg.com/v2-c3aba624f3d1b1e47932e6f876d30388_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-c3aba624f3d1b1e47932e6f876d30388_b.jpg\"/></figure><p>从上图压测曲线可以看到，由于采用的是固定的权重进行随机分配，除此以外没有对provider的请求数量做任何限制，这就导致了请求数量有可能大于provider的可用线程，从而产生请求错误。</p><p>综上所述，方案一的缺陷在于：</p><ol><li>固定的权重没有根据服务端服务能力变化而变化；</li><li>没有计算服务端的可用线程数量，仅仅采用的是加权随机，会导致服务端过载。</li></ol><p class=\"ztext-empty-paragraph\"><br/></p><p><b>方案二：闭环队列</b></p><p>所谓闭环队列，就是在consumer节点上维护一个并发队列（灵感来自于Disruptor，但由于赛题禁止引用第三方类库，最终使用ConcurrentLinkedQueue来实现），在预热阶段，先初始化一个invoker下标队列，然后在调用UserLoadBalance的select方法时，直接从队列中获取对应的invoker下标，从而以O(1)的复杂度直接获取invoker。</p><p>该方案的初衷，本意是想减少获取invoker下标时的计算量（后来发现这个不是瓶颈，解不解决差别不大），本质上是解决了方案一中的缺陷2——通过在初始化时预定义了provider的可用线程数，当consumer需要调用invoker时，从队列中按顺序获取，当请求返回时，把相应的invoker下标重新加入队列，从而形成了一个服务闭环。</p><p>这个方案大概的最佳成绩在125万</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-b492656ececfbcd858a9b8b06baa0516_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"346\" data-rawheight=\"162\" class=\"content_image\" width=\"346\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;346&#39; height=&#39;162&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"346\" data-rawheight=\"162\" class=\"content_image lazy\" width=\"346\" data-actualsrc=\"https://pic3.zhimg.com/v2-b492656ececfbcd858a9b8b06baa0516_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-9984f069f0b709e1747c8a494fde5bba_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1878\" data-rawheight=\"721\" class=\"origin_image zh-lightbox-thumb\" width=\"1878\" data-original=\"https://pic3.zhimg.com/v2-9984f069f0b709e1747c8a494fde5bba_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1878&#39; height=&#39;721&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1878\" data-rawheight=\"721\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1878\" data-original=\"https://pic3.zhimg.com/v2-9984f069f0b709e1747c8a494fde5bba_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-9984f069f0b709e1747c8a494fde5bba_b.jpg\"/></figure><p>从上图压测曲线可以看到，使用这种服务闭环的方案，本质上就是在consumer端对上游服务进行了预计算，从而避免了采用加权随机时所产生的服务端过载问题。</p><p>因此，上图的请求成功率为100%。</p><p>尽管这个方案刚实现后，我的排名一度升至42名，后来就一直没有更好的解决方案，排名也很快掉出了200名以外。我曾一度认为提升的空间在于提高consumer的select方法的算法效率，以及provider端的异步化处理。<br/>直到后来重新阅读源码时才开始真正理解题目中的provider是如何运作的。</p><p>以provider-large为例，理论最大线程数为650，而实际上题目设定了，在计分阶段，最大可用线程数要小于这个数，一般在500多这个范围波动，每六秒变更一次。这个限制在源码中是通过信号量机制来实现的。也就是说，假设信号量在某个时刻为550，如果这时consumer往provider发出的请求数量大于550的部分，实际上是进入了等待队列，对吞吐量并没有起任何正面影响。</p><p>由于客户端总的线程数恒定为1024，假如多分了给provider-large，就必须导致其他的provider没有满载运行。因此总的吞吐量就上不去。</p><p>通过日志排查，可以验证该问题：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-940ddf1514a0af037977df9a37eb1706_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1748\" data-rawheight=\"258\" class=\"origin_image zh-lightbox-thumb\" width=\"1748\" data-original=\"https://pic3.zhimg.com/v2-940ddf1514a0af037977df9a37eb1706_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1748&#39; height=&#39;258&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1748\" data-rawheight=\"258\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1748\" data-original=\"https://pic3.zhimg.com/v2-940ddf1514a0af037977df9a37eb1706_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-940ddf1514a0af037977df9a37eb1706_b.png\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-19fa9ce6febb786c559e731cf7721341_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1426\" data-rawheight=\"219\" class=\"origin_image zh-lightbox-thumb\" width=\"1426\" data-original=\"https://pic2.zhimg.com/v2-19fa9ce6febb786c559e731cf7721341_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1426&#39; height=&#39;219&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1426\" data-rawheight=\"219\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1426\" data-original=\"https://pic2.zhimg.com/v2-19fa9ce6febb786c559e731cf7721341_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-19fa9ce6febb786c559e731cf7721341_b.png\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-f26cdc5caf8779b5e9dcb1770b1fcf5a_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1497\" data-rawheight=\"243\" class=\"origin_image zh-lightbox-thumb\" width=\"1497\" data-original=\"https://pic3.zhimg.com/v2-f26cdc5caf8779b5e9dcb1770b1fcf5a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1497&#39; height=&#39;243&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1497\" data-rawheight=\"243\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1497\" data-original=\"https://pic3.zhimg.com/v2-f26cdc5caf8779b5e9dcb1770b1fcf5a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-f26cdc5caf8779b5e9dcb1770b1fcf5a_b.png\"/></figure><p>从以上三图可以看到，在某个时刻，三个provider的服务能力变更，最大线程数限制分别为115、340、569（总和必然等于1024）。而我通过日志记录的相应线程分配情况如下图：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-8c173f5699997f21c5b6f71495b601d8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"703\" data-rawheight=\"797\" class=\"origin_image zh-lightbox-thumb\" width=\"703\" data-original=\"https://pic1.zhimg.com/v2-8c173f5699997f21c5b6f71495b601d8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;703&#39; height=&#39;797&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"703\" data-rawheight=\"797\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"703\" data-original=\"https://pic1.zhimg.com/v2-8c173f5699997f21c5b6f71495b601d8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-8c173f5699997f21c5b6f71495b601d8_b.jpg\"/></figure><p>可以看到与服务端的最大线程数并不匹配。</p><p>综上所述，方案二的缺陷在于：</p><ul><li>尽管很好地解决了服务端过载问题，但仍然没有能够跟进服务端服务能力的变化。</li></ul><p>之后的方案均继承了该方案。</p><h2>方案三——类强化学习</h2><p>在一开始接触这个赛题的时候，我就觉得有种可以使用强化学习的想法。但不知道从何入手。直到经历了前两个方案，才发现强化学习方案确实有实现的可能。</p><p>首先，强化学习的关键在于“生成策略 -&gt; 获取策略的环境反馈 ...（重复多次） -&gt; 最后得到最优策略”。在目前赛题的场景下，策略指的就是invoker的分配方案，而环境反馈就是总的吞吐量，吞吐量越大，证明分配得越合理。</p><p>使用强化学习最大的问题在于“策略从哪来”。笔者最朴素的方案是自己先生成一些“策略数据点”。比如我们已经知道了三个provider的最大线程数为200、450、650，那么我们就可以利用这些值来生成，比如我把最大线程数定为上界，把1/2位，即100、225、325定为下界，然后对每个维度都分十份。</p><p>数学敏感的朋友看到这里估计会质疑，三个维度每个维度是十个数据点，那么组合起来就是 <img src=\"https://www.zhihu.com/equation?tex=10%5E3+%3D+1000\" alt=\"10^3 = 1000\" eeimg=\"1\"/> 个数据点，即使把三个维度之和不在1024附近范围的数据点去掉，也还有几十个数据点组合，假设100毫秒尝试一个，全部遍历一遍也要好几秒的时间。</p><p>情况确实如此。该方案最要命的问题在于策略空间过大，搜索时间过长，而服务能力变化太快，从而不能直接采用该方案。</p><p>然而为了验证该方案的可能性，我还是采用了hardcode的方法来压缩上下界，从而使得策略空间在十个数据点组合以内。结果显示该方法可行。</p><p>实验结果最高得分超过了125万。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-acbe10b12d57995996afa772a603e51b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"436\" data-rawheight=\"206\" class=\"origin_image zh-lightbox-thumb\" width=\"436\" data-original=\"https://pic4.zhimg.com/v2-acbe10b12d57995996afa772a603e51b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;436&#39; height=&#39;206&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"436\" data-rawheight=\"206\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"436\" data-original=\"https://pic4.zhimg.com/v2-acbe10b12d57995996afa772a603e51b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-acbe10b12d57995996afa772a603e51b_b.jpg\"/></figure><p>这里说的强化学习仅仅是思路上的。听起来好像好厉害，实际上用JAVA来实现就是两个关键点，一个包装类记录策略的具体内容与相应的得分，一个是放到优先队列来自动排序。</p><p>综上所述，方案三的缺陷在于：</p><ul><li>在策略空间太大的情况下，由于搜索时间过长而导致无法应用。</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><h2>方案四——采用服务端信息估算服务端能力变化与最大线程分配比例</h2><p>经过前三个方案的努力无果后，笔者尝试了把目光从consumer端转向了provider端。</p><p>首先provider可以提供的信息有：单位时间间隔内的请求数、每个请求消耗的时间、当前活跃的线程数、以及相关的硬件资源信息，如CPU核心数、JVM内存状况、JVM的CPU使用率等等。</p><p>最初，我曾经想通过使用 <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B%E5%8D%95%E4%BD%8D%E6%97%B6%E9%97%B4%E5%86%85%E8%AF%B7%E6%B1%82%E6%95%B0%7D%7B%E5%8D%95%E4%BD%8D%E6%97%B6%E9%97%B4%E5%86%85%E6%AF%8F%E4%B8%AA%E7%BA%BF%E7%A8%8B%E7%9A%84%E5%90%9E%E5%90%90%E9%87%8F%7D\" alt=\"\\frac{单位时间内请求数}{单位时间内每个线程的吞吐量}\" eeimg=\"1\"/> 来直接估算最大线程数，但很快发现这个不可行。因为请求数是consumer发出的，如果这时consumer又以这个请求数为计算依据就显然不合理了。这就好比我们随意调整电脑的时间，然后又根据这个电脑的时间来决定上下班的时间，显然不可能得到准确的结果。</p><p>尽管单位时间内的请求数不能直接用来求线程数，但在这尝试的过程中发现，自己统计的平均请求时间是可以拿来使用的：那就是用来统计provider服务能力的变化。</p><p><b>方案四子任务A——如何估算服务能力的变化</b></p><p>这里的统计方法笔者是吸取了T检验的统计方法，收集最近30个时间间隔（每个间隔100毫秒）的统计样本，然后求出这些样本的平均请求时间及标准差，再收集最近几个时间间隔样本作为测试样本。如果测试样本的均值与统计样本的均值之差大于1.96个标准差，则判定为服务能力发生了改变。</p><p>需要注意的是，笔者在应用这些统计学理论时，并没有拘泥于理论本身（比如两个比较的样本大小是否一致），而是从实践效果出发，只要能够比较准确地判断服务能力的改变，就达到了笔者的使用初衷。</p><p>通过这个统计的方法，基本上对服务能力变更的准确率达到90%。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-f79cf9de54ae7560a1468d324c4b68ab_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1013\" data-rawheight=\"868\" class=\"origin_image zh-lightbox-thumb\" width=\"1013\" data-original=\"https://pic4.zhimg.com/v2-f79cf9de54ae7560a1468d324c4b68ab_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1013&#39; height=&#39;868&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1013\" data-rawheight=\"868\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1013\" data-original=\"https://pic4.zhimg.com/v2-f79cf9de54ae7560a1468d324c4b68ab_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-f79cf9de54ae7560a1468d324c4b68ab_b.jpg\"/></figure><p>上图便是笔者每100毫秒通过CallbackListener回调产生的日志，然后通过上述算法判定所得到的结果。【】中的数字代表服务开始的秒数，因此期望看到的应该是30、36、42、48、54、60、66、72、78、84这十个数字。从上图可见，该算法基本上能比较准确地捕获到。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>方案四子任务B——如何才能计算出合理的分配比例</b></p><p>尝到了通过provider信息计算得出服务能力变化时机的甜头，我就在想，怎么样才能计出更近似的分配比较呢（我不奢望百分百准确，近似也行啊）。</p><p>于是我理了一下思路发现：</p><ul><li>CPU负载与线程数存在着某种联系：[1,2,3]与[200,450,650]。即线程数与CPU核心数量的比例大致相同。</li><li>CPU负载与请求数存在一个正比关系。不管几核的CPU，只要没有请求，CPU负载都为零，这个很符合逻辑是不是；而对于处理相同的请求数，理论上讲CPU的核心越大，负载越小。但反过来讲，当CPU负载差不多的时候，双核的CPU所处理的请求数是不是就必然是单核CPU的两倍呢？直觉告诉我不一定。</li></ul><p>基于以上两点的认知，我尝试把它量化成数学公式，经过几番尝试，结合使用日志生成的真实数据来验证算法的准确性，最终发现了这样一条公式：</p><p><img src=\"https://www.zhihu.com/equation?tex=%E6%9F%90provider%E7%9A%84%E7%BA%BF%E7%A8%8B%E5%88%86%E9%85%8D%E7%B3%BB%E6%95%B0+%3D+%5Cfrac%7B%E5%8D%95%E4%BD%8D%E6%97%B6%E9%97%B4%E9%97%B4%E9%9A%94%E8%AF%B7%E6%B1%82%E6%95%B0%7D%7BJVM%E7%9A%84CPU%E8%B4%9F%E8%BD%BD%7D+%2A+log%28%E6%9C%80%E5%A4%A7%E7%BA%BF%E7%A8%8B%E6%95%B0%29+%2A+%E6%9C%80%E5%A4%A7%E7%BA%BF%E7%A8%8B%E6%95%B0\" alt=\"某provider的线程分配系数 = \\frac{单位时间间隔请求数}{JVM的CPU负载} * log(最大线程数) * 最大线程数\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%E6%9F%90provider%E7%9A%84%E7%BA%BF%E7%A8%8B%E6%95%B0+%3D+%5Cfrac%7B%E6%9F%90provider%E7%9A%84%E7%BA%BF%E7%A8%8B%E5%88%86%E9%85%8D%E7%B3%BB%E6%95%B0%7D%7B%5Csum%28%E6%89%80%E6%9C%89provider%E7%9A%84%E7%BA%BF%E7%A8%8B%E5%88%86%E9%85%8D%E7%B3%BB%E6%95%B0%E4%B9%8B%E5%92%8C%29%7D+%2A+1024\" alt=\"某provider的线程数 = \\frac{某provider的线程分配系数}{\\sum(所有provider的线程分配系数之和)} * 1024\" eeimg=\"1\"/> </p><p>公式二不难理解，解释一下为什么有公式一：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B%E5%8D%95%E4%BD%8D%E6%97%B6%E9%97%B4%E9%97%B4%E9%9A%94%E8%AF%B7%E6%B1%82%E6%95%B0%7D%7BJVM%E7%9A%84CPU%E8%B4%9F%E8%BD%BD%7D\" alt=\"\\frac{单位时间间隔请求数}{JVM的CPU负载}\" eeimg=\"1\"/> ，这个用请求数比CPU负载，我们无需要关心具体数值的数学意义，只需要知道它可以反映两个比较关系：</p><ol><li>CPU负载与请求数存在一个正比关系</li><li>把这个正比关系看作一个斜率的话，不同核心数量的CPU必然有着不一样的斜率。注意这里笔者仅仅是用斜率来比喻，从实践看这个正比关系显然不是一个常数。如果它是常数的话，那么分配比例系数就恒定不变了。</li></ol><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-0717545545f0aee61abd0e91e1f1180a_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"720\" data-rawheight=\"48\" class=\"origin_image zh-lightbox-thumb\" width=\"720\" data-original=\"https://pic3.zhimg.com/v2-0717545545f0aee61abd0e91e1f1180a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;720&#39; height=&#39;48&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"720\" data-rawheight=\"48\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"720\" data-original=\"https://pic3.zhimg.com/v2-0717545545f0aee61abd0e91e1f1180a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-0717545545f0aee61abd0e91e1f1180a_b.png\"/></figure><p>上图是一个真实的100毫秒统计数据，可以更直观地感受一下。</p><p><img src=\"https://www.zhihu.com/equation?tex=log%28%E6%9C%80%E5%A4%A7%E7%BA%BF%E7%A8%8B%E6%95%B0%29+\" alt=\"log(最大线程数) \" eeimg=\"1\"/> ，这个其实是CPU核心数的变种计法。由于log(1)=0，所以就借用了最大线程数来近似替代CPU核心数。</p><p><img src=\"https://www.zhihu.com/equation?tex=%E6%9C%80%E5%A4%A7%E7%BA%BF%E7%A8%8B%E6%95%B0\" alt=\"最大线程数\" eeimg=\"1\"/> ，还记得这个本质上就是最朴素的方案一的计法吧。</p><p>换句话说，该公式本质上是在方案一的基础上增加了CPU负载与CPU核心数量这两个指标，由于不同核心数量的CPU之间的负载不具有可比性，因而加入单位时间间隔请求数来起到统一量纲的作用。</p><p>当然，以上公式只是体现了逻辑上的量化，然后再通过实际数据去验证，经过多次尝试所得，并非绝对的理论。而且必须强调的是，这个公式有效的前提是，CPU的频率是一致的。同时需要记住的是，这个公式仅仅用于体现一个比较关系，其计得的数值究竟有没有实际意义不在探讨范围之内。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2>方案五——综合方案</h2><p>经过以上多个方案的探索，最终的方案就是把上述方案的优点全部融合起来。</p><p><b>Provider端：</b></p><p>关于Provider信息的统计：<br/>如下图</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-0717545545f0aee61abd0e91e1f1180a_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"720\" data-rawheight=\"48\" class=\"origin_image zh-lightbox-thumb\" width=\"720\" data-original=\"https://pic3.zhimg.com/v2-0717545545f0aee61abd0e91e1f1180a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;720&#39; height=&#39;48&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"720\" data-rawheight=\"48\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"720\" data-original=\"https://pic3.zhimg.com/v2-0717545545f0aee61abd0e91e1f1180a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-0717545545f0aee61abd0e91e1f1180a_b.png\"/></figure><ul><li>totalReq：单位时间间隔内的请求完成数；</li><li>cpuLoad：JVM的CPU负载信息；</li><li>cpuCores：CPU核心数量；</li><li>avgRTT：单位时间间隔内的平均请求时间；</li><li>其他：略</li></ul><p>以上两个信息均通过CallbackListener以每100毫秒的间隔进行统计并反馈。</p><p><b>Consumer端：</b></p><p>关于请求限制：<br/>使用原子类计数器统计每个invoker的请求数与返回数，确保不会发出过载请求；同时根据分配策略动态设置请求上限；</p><p>关于服务能力变化监控： <br/>使用方案四的基于RTT的统计办法来实现；</p><p>关于线程分配方案的计算： <br/>使用方案四的基于CPU负载、请求数、CPU核心数、最大线程数综合计得；</p><p>关于策略的搜索：<br/>由于此时的分配方案是实时计算，并且是基于一个动态变化的数据去计算，所以可以把每个方案看作是一个策略搜索点进行策略学习，以吞吐量为评分标准，进行十次的策略搜索然后利用最优队列选最优的继续执行。</p><p>按上述思路实现后，基本上成绩稳定在128万左右。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-ffc437db40eee5f2a5ec76d69650dd15_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"473\" data-rawheight=\"849\" class=\"origin_image zh-lightbox-thumb\" width=\"473\" data-original=\"https://pic2.zhimg.com/v2-ffc437db40eee5f2a5ec76d69650dd15_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;473&#39; height=&#39;849&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"473\" data-rawheight=\"849\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"473\" data-original=\"https://pic2.zhimg.com/v2-ffc437db40eee5f2a5ec76d69650dd15_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-ffc437db40eee5f2a5ec76d69650dd15_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>以上便是此是初赛赛题的设计思路。</p><hr/><p>最后安利一个课程。平常工作中，公司的项目基本没有涉及并发编程，多亏学习了该并发编程课，让我打下了坚实的并发编程理论基础。有兴趣研究并发编程的小伙伴，强烈推荐此课！</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-d883bc4b007dded36dca81540e92bfd4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1242\" data-rawheight=\"2209\" class=\"origin_image zh-lightbox-thumb\" width=\"1242\" data-original=\"https://pic1.zhimg.com/v2-d883bc4b007dded36dca81540e92bfd4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1242&#39; height=&#39;2209&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1242\" data-rawheight=\"2209\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1242\" data-original=\"https://pic1.zhimg.com/v2-d883bc4b007dded36dca81540e92bfd4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-d883bc4b007dded36dca81540e92bfd4_b.jpg\"/></figure><p></p>", 
            "topic": [
                {
                    "tag": "高并发", 
                    "tagLink": "https://api.zhihu.com/topics/19596218"
                }, 
                {
                    "tag": "中间件", 
                    "tagLink": "https://api.zhihu.com/topics/19599657"
                }, 
                {
                    "tag": "阿里天池", 
                    "tagLink": "https://api.zhihu.com/topics/20139645"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/35965070", 
            "userName": "黄伟亮", 
            "userLink": "https://www.zhihu.com/people/294306d6706f830e3cce35fb4acf46cb", 
            "upvote": 62, 
            "title": "从强化学习到深度强化学习（下）", 
            "content": "<p>强化学习的基础知识可参考：</p><a href=\"https://zhuanlan.zhihu.com/p/35688924\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic4.zhimg.com/v2-991ab4b45d8f1d6340adc9313c13e91b_180x120.jpg\" data-image-width=\"738\" data-image-height=\"274\" class=\"internal\">kevinbauer：从强化学习到深度强化学习（上）</a><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>从离散空间到连续空间</b></h2><p>在之前提到的强化学习任务中，都是有限的MDP框架，即动作空间及状态空间的个数都是有限个。然而，现实生活中的很多问题动作空间与状态空间并非离散的，而是连续的。那么如何用强化学习的理论基础去解决问题呢？主要有两种思路：离散化处理、函数逼近。</p><p><b>离散化处理：</b></p><p>指的是把连续空间用区域o化的方式划分成有限的个数。具体的处理手法有Tile coding及Coarse coding。</p><p><b>函数逼近：</b></p><p>指的是把输入与输出看作是经过某个函数的映射，然后用梯度更新的方法找到这个函数。具体的处理手法有线性函数逼近、核函数逼近、以及非线性函数逼近。</p><hr/><p>深度强化学习常用的算法思路有以下三种：深度Q学习、策略梯度、行动者与评论者方法。以下逐一总结这三种思路的特点。</p><h2><b>深度Q学习</b></h2><p>对于传统的强化学习，我们的解题思路是通过计算最优值函数（包括状态值函数 <img src=\"https://www.zhihu.com/equation?tex=v_%2A\" alt=\"v_*\" eeimg=\"1\"/> 以及动作值函数 <img src=\"https://www.zhihu.com/equation?tex=q_%2A\" alt=\"q_*\" eeimg=\"1\"/> ），从而求得最优策略 <img src=\"https://www.zhihu.com/equation?tex=%5Cpi_%2A\" alt=\"\\pi_*\" eeimg=\"1\"/> 。而状态值函数可以看作是状态 <img src=\"https://www.zhihu.com/equation?tex=s\" alt=\"s\" eeimg=\"1\"/> 到某实数的映射：<img src=\"https://www.zhihu.com/equation?tex=s%5Crightarrow+v_%7B%5Cpi%7D%5Crightarrow+v_%7B%5Cpi%7D%28s%29+%5Cin+%5Cmathbb%7BR%7D\" alt=\"s\\rightarrow v_{\\pi}\\rightarrow v_{\\pi}(s) \\in \\mathbb{R}\" eeimg=\"1\"/> 。而在连续空间的问题时，状态 <img src=\"https://www.zhihu.com/equation?tex=s\" alt=\"s\" eeimg=\"1\"/> 一般以向量 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbb%7Bx%7D%28s%29\" alt=\"\\mathbb{x}(s)\" eeimg=\"1\"/> 表示，因此可以看作是状态向量 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbb%7Bx%7D%28s%29\" alt=\"\\mathbb{x}(s)\" eeimg=\"1\"/> 到某实数的映射： <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbb%7Bx%7D%28s%29%5Crightarrow+v_%7B%5Cpi%7D%5Crightarrow+v_%7B%5Cpi%7D%28s%2C+w%29+%5Cin+%5Cmathbb%7BR%7D\" alt=\"\\mathbb{x}(s)\\rightarrow v_{\\pi}\\rightarrow v_{\\pi}(s, w) \\in \\mathbb{R}\" eeimg=\"1\"/> 。</p><p>于是我们就有了目标函数 <img src=\"https://www.zhihu.com/equation?tex=%5Bv_%7B%5Cpi%7D%28s%2Cw%29+-+%5Chat+v_%7B%5Cpi%7D%28s%2Cw%29%5D%5E2\" alt=\"[v_{\\pi}(s,w) - \\hat v_{\\pi}(s,w)]^2\" eeimg=\"1\"/> ，然后就可以通过梯度下降算法做函数逼近， <img src=\"https://www.zhihu.com/equation?tex=%5CDelta+w+%3D+%5Calpha%28v_%7B%5Cpi%7D%28s%29+-+%5Chat+v%28s%2Cw%29%29%5Ctriangledown+_w+%5Chat+v%28s%2Cw%29\" alt=\"\\Delta w = \\alpha(v_{\\pi}(s) - \\hat v(s,w))\\triangledown _w \\hat v(s,w)\" eeimg=\"1\"/> 。</p><p>同时，对于动作值函数，也有 <img src=\"https://www.zhihu.com/equation?tex=%5CDelta+w+%3D+%5Calpha+%28q_%7B%5Cpi%7D%28s%2C+a%29+-+%5Chat+q%28s%2Ca%2Cw%29%29+%5Cbigtriangledown+_w+%5Chat+q%28s%2Ca%2Cw%29\" alt=\"\\Delta w = \\alpha (q_{\\pi}(s, a) - \\hat q(s,a,w)) \\bigtriangledown _w \\hat q(s,a,w)\" eeimg=\"1\"/> 。</p><p>然而，问题是此时我们并不知道 <img src=\"https://www.zhihu.com/equation?tex=v_%7B%5Cpi%7D\" alt=\"v_{\\pi}\" eeimg=\"1\"/> 与 <img src=\"https://www.zhihu.com/equation?tex=q_%7B%5Cpi%7D\" alt=\"q_{\\pi}\" eeimg=\"1\"/> 的函数真值是多少。于是，需要用两种方法去估算函数真值：蒙特卡罗方法与时间差分方法。</p><p><b>蒙特卡罗方法：</b></p><p>我们从前面的经验可知，基于值函数（<img src=\"https://www.zhihu.com/equation?tex=v_%7B%5Cpi%7D\" alt=\"v_{\\pi}\" eeimg=\"1\"/> 或  <img src=\"https://www.zhihu.com/equation?tex=q_%7B%5Cpi%7D\" alt=\"q_{\\pi}\" eeimg=\"1\"/>）的方法，解题的思路都是 <img src=\"https://www.zhihu.com/equation?tex=%28+v_%7B%5Cpi%7D+%5Crightarrow+%29+%5C+q_%7B%5Cpi%7D+%5Crightarrow+%5Cpi%27\" alt=\"( v_{\\pi} \\rightarrow ) \\ q_{\\pi} \\rightarrow \\pi&#39;\" eeimg=\"1\"/> ，而 <img src=\"https://www.zhihu.com/equation?tex=v_%7B%5Cpi%7D\" alt=\"v_{\\pi}\" eeimg=\"1\"/> 或  <img src=\"https://www.zhihu.com/equation?tex=q_%7B%5Cpi%7D\" alt=\"q_{\\pi}\" eeimg=\"1\"/> 的更新公式为：</p><ul><li><img src=\"https://www.zhihu.com/equation?tex=V%28S_t%29+%5Cleftarrow+V%28S_t%29+%2B+%5Calpha+%28G_t+-+V%28S_t%29%29%29\" alt=\"V(S_t) \\leftarrow V(S_t) + \\alpha (G_t - V(S_t)))\" eeimg=\"1\"/> </li><li><img src=\"https://www.zhihu.com/equation?tex=Q%28S_t%2C+A_t%29+%5Cleftarrow+Q%28S_t%2C+A_t%29+%2B+%5Calpha+%28G_t+-+Q%28S_t%2C+A_t%29%29%29\" alt=\"Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha (G_t - Q(S_t, A_t)))\" eeimg=\"1\"/> </li></ul><p>在更新公式中可以看到每次的更新的效果为：</p><ul><li><img src=\"https://www.zhihu.com/equation?tex=%5CDelta+V+%3D+%5Calpha%28G_t+-+V%28S_t%29%29\" alt=\"\\Delta V = \\alpha(G_t - V(S_t))\" eeimg=\"1\"/> </li><li><img src=\"https://www.zhihu.com/equation?tex=%5CDelta+Q+%3D+%5Calpha%28G_t+-+Q%28S_t%2C+A_t%29%29\" alt=\"\\Delta Q = \\alpha(G_t - Q(S_t, A_t))\" eeimg=\"1\"/> </li></ul><p>值函数的迭代过程，本质上就是一个不精确的（误差很大）的值函数，逐渐更新收敛形成精确的值函数的过程。假如把这过程理解为梯度下降，我们便可以把 <img src=\"https://www.zhihu.com/equation?tex=G_t\" alt=\"G_t\" eeimg=\"1\"/> 看作是值函数 <img src=\"https://www.zhihu.com/equation?tex=v_%7B%5Cpi%7D%28S_t%29\" alt=\"v_{\\pi}(S_t)\" eeimg=\"1\"/> 与 <img src=\"https://www.zhihu.com/equation?tex=q_%7B%5Cpi%7D%28S_t%29\" alt=\"q_{\\pi}(S_t)\" eeimg=\"1\"/> 的真值，于是有：</p><ul><li><img src=\"https://www.zhihu.com/equation?tex=%5CDelta+w+%3D+%5Calpha+%28G_t+-+%5Chat+v%28S_t%2C+w%29%29%5Cbigtriangledown+_w+%5Chat+v%28S_t%2C+w%29\" alt=\"\\Delta w = \\alpha (G_t - \\hat v(S_t, w))\\bigtriangledown _w \\hat v(S_t, w)\" eeimg=\"1\"/> </li><li><img src=\"https://www.zhihu.com/equation?tex=%5CDelta+w+%3D+%5Calpha+%28G_t+-+%5Chat+q%28S_t%2CA_t%2C+w%29%29%5Cbigtriangledown+_w+%5Chat+q%28S_t%2C+A_t%2C+w%29\" alt=\"\\Delta w = \\alpha (G_t - \\hat q(S_t,A_t, w))\\bigtriangledown _w \\hat q(S_t, A_t, w)\" eeimg=\"1\"/> </li></ul><p>有了以上转换，便可以通过Tensorflow等运算框架去计算梯度，从而求得值函数的真值。写成伪代码如下图：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-0bd067911b18c5c5f0e337b8a3688aea_b.jpg\" data-size=\"normal\" data-rawwidth=\"1304\" data-rawheight=\"599\" class=\"origin_image zh-lightbox-thumb\" width=\"1304\" data-original=\"https://pic3.zhimg.com/v2-0bd067911b18c5c5f0e337b8a3688aea_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1304&#39; height=&#39;599&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1304\" data-rawheight=\"599\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1304\" data-original=\"https://pic3.zhimg.com/v2-0bd067911b18c5c5f0e337b8a3688aea_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-0bd067911b18c5c5f0e337b8a3688aea_b.jpg\"/><figcaption>资料来源：Udacity深度学习课程</figcaption></figure><p>代码说明：</p><ul><li>以上伪代码使用的背景是针对阶段性任务，并且使用的 <img src=\"https://www.zhihu.com/equation?tex=G_t\" alt=\"G_t\" eeimg=\"1\"/> 是全访问（every visit，MC算法专用定义）</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p><b>时间差分方法：</b></p><p>对于时间差分方法，我们有值函数：</p><ul><li><img src=\"https://www.zhihu.com/equation?tex=V%28S_t%29+%5Cleftarrow+V%28S_t%29+%2B+%5Calpha%28R_%7Bt%2B1%7D+%2B+%5Cgamma+V%28S_%7Bt%2B1%7D%29+-+V%28S_t%29%29+\" alt=\"V(S_t) \\leftarrow V(S_t) + \\alpha(R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)) \" eeimg=\"1\"/> </li><li><img src=\"https://www.zhihu.com/equation?tex=Q%28S_t%2C+A_t%29+%5Cleftarrow+Q%28S_t%2C+A_t%29+%2B+%5Calpha+%28R_%7Bt%2B1%7D+%2B+%5Cgamma+Q%28S_%7Bt%2B1%7D%2CA_%7Bt%2B1%7D%29+-+Q%28S_t%2C+A_t%29%29\" alt=\"Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha (R_{t+1} + \\gamma Q(S_{t+1},A_{t+1}) - Q(S_t, A_t))\" eeimg=\"1\"/> </li></ul><p>与MC方法同理，TD方法的梯度更新公式可以写成：</p><ul><li><img src=\"https://www.zhihu.com/equation?tex=%5CDelta+w+%3D+%5Calpha+%28R_%7Bt%2B1%7D+%2B+%5Cgamma+%5Chat+v%28S_%7Bt%2B1%7D%2Cw%29+-+%5Chat+v%28S_t%2Cw%29%29%5Cbigtriangledown+_w+%5Chat+v%28S_t%2Cw%29\" alt=\"\\Delta w = \\alpha (R_{t+1} + \\gamma \\hat v(S_{t+1},w) - \\hat v(S_t,w))\\bigtriangledown _w \\hat v(S_t,w)\" eeimg=\"1\"/> </li><li><img src=\"https://www.zhihu.com/equation?tex=%5CDelta+w+%3D+%5Calpha+%28R_%7Bt%2B1%7D+%2B+%5Cgamma+%5Chat+q%28S_%7Bt%2B1%7D%2CA_%7Bt%2B1%7D%2C+w%29+-+%5Chat+q%28S_t%2CA_t%2C+w%29%29%5Cbigtriangledown+_w+%5Chat+q%28S_t%2C+A_t%2C+w%29\" alt=\"\\Delta w = \\alpha (R_{t+1} + \\gamma \\hat q(S_{t+1},A_{t+1}, w) - \\hat q(S_t,A_t, w))\\bigtriangledown _w \\hat q(S_t, A_t, w)\" eeimg=\"1\"/> </li></ul><p>于是，写成伪代码如下图：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-dfa5929c00025cfdf55a1754d4fc4692_b.jpg\" data-size=\"normal\" data-rawwidth=\"1327\" data-rawheight=\"601\" class=\"origin_image zh-lightbox-thumb\" width=\"1327\" data-original=\"https://pic3.zhimg.com/v2-dfa5929c00025cfdf55a1754d4fc4692_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1327&#39; height=&#39;601&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1327\" data-rawheight=\"601\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1327\" data-original=\"https://pic3.zhimg.com/v2-dfa5929c00025cfdf55a1754d4fc4692_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-dfa5929c00025cfdf55a1754d4fc4692_b.jpg\"/><figcaption>资料来源：Udacity深度学习课程</figcaption></figure><p>代码说明：</p><ul><li>该代码是针对阶段性任务。若要针对连续任务，只需修改循环条件；</li><li>Sarsa算法属于同步策略（on-policy），即更新值函数与行动用的是同一套策略；</li><li>该算法适合在线学习，能够快速收敛；</li><li>但该算法的更新值函数与行动之间的关联过于紧密，导致可能无法得到更优的策略</li></ul><p>针对Sarsa的不足，Q-learning是另一种选择，伪代码如下图：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-bf671477d4f15650162e38948f89a6e4_b.jpg\" data-size=\"normal\" data-rawwidth=\"1174\" data-rawheight=\"508\" class=\"origin_image zh-lightbox-thumb\" width=\"1174\" data-original=\"https://pic1.zhimg.com/v2-bf671477d4f15650162e38948f89a6e4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1174&#39; height=&#39;508&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1174\" data-rawheight=\"508\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1174\" data-original=\"https://pic1.zhimg.com/v2-bf671477d4f15650162e38948f89a6e4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-bf671477d4f15650162e38948f89a6e4_b.jpg\"/><figcaption>资料来源：Udacity深度学习课程</figcaption></figure><p>代码说明：</p><ul><li>Q-learning与Sarsa算法最大的不同，在于更新值函数时的策略与行动策略可能不同；</li><li>Q-learning属于异步策略（off-policy），即更新值函数的策略与行动策略不同；</li><li>该算法适合离线学习，因为值函数的更新会延迟反应在行动策略的改变上；</li><li>该算法不单能学习环境反馈，还能学习人为经验；</li><li>更适合批量学习（batch learning）</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p>以上提到的算法，仅仅解决了深度学习中梯度更新的问题。除了梯度更新，如何让智能体实现通用学习？Deep Q Network， 又称DQN。</p><p>DQN指的是模型结构是DNN，CNN，RNN等常见的神经网络模型，如下图：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-0c398ef99ca89bc3387589cc5d422c82_b.jpg\" data-size=\"normal\" data-rawwidth=\"715\" data-rawheight=\"413\" class=\"origin_image zh-lightbox-thumb\" width=\"715\" data-original=\"https://pic3.zhimg.com/v2-0c398ef99ca89bc3387589cc5d422c82_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;715&#39; height=&#39;413&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"715\" data-rawheight=\"413\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"715\" data-original=\"https://pic3.zhimg.com/v2-0c398ef99ca89bc3387589cc5d422c82_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-0c398ef99ca89bc3387589cc5d422c82_b.jpg\"/><figcaption>DQN示例。资料来源：参考资料[7]</figcaption></figure><p>而梯度更新则是使用上述提及深度Q-learning的更新公式： <img src=\"https://www.zhihu.com/equation?tex=%5CDelta+w+%3D+%5Calpha+%28R_%7Bt%2B1%7D+%2B+%5Cgamma+max_a+%5Chat+q%28S_%7Bt%2B1%7D%2Ca%2C+w%29+-+%5Chat+q%28S_t%2CA_t%2C+w%29%29%5Cbigtriangledown+_w+%5Chat+q%28S_t%2C+A_t%2C+w%29\" alt=\"\\Delta w = \\alpha (R_{t+1} + \\gamma max_a \\hat q(S_{t+1},a, w) - \\hat q(S_t,A_t, w))\\bigtriangledown _w \\hat q(S_t, A_t, w)\" eeimg=\"1\"/> ，并以此进行函数逼近。</p><p>DQN的训练过程需要大量的训练数据，即便这样，由于状态输入向量与动作输出向量之间的紧密联系，导致无法保证函数能够收敛成为最优值函数，从而也导致策略不稳定或者效果不佳。为了解决这个问题，需要用到以下两种处理手法来优化训练过程。</p><p><b>经验回放</b></p><p>以动作值函数为例，我们知道梯度更新公式为 <img src=\"https://www.zhihu.com/equation?tex=%5CDelta+w+%3D+%5Calpha+%28R_%7Bt%2B1%7D+%2B+%5Cgamma+max_a+%5Chat+q%28S_%7Bt%2B1%7D%2Ca%2C+w%29+-+%5Chat+q%28S_t%2CA_t%2C+w%29%29%5Cbigtriangledown+_w+%5Chat+q%28S_t%2C+A_t%2C+w%29\" alt=\"\\Delta w = \\alpha (R_{t+1} + \\gamma max_a \\hat q(S_{t+1},a, w) - \\hat q(S_t,A_t, w))\\bigtriangledown _w \\hat q(S_t, A_t, w)\" eeimg=\"1\"/> ，因此对于每次更新计算，都需要用到一组经验元组 <img src=\"https://www.zhihu.com/equation?tex=%3CS_t%2C+A_t%2C+R_%7Bt%2B1%7D%2C+S_%7Bt%2B1%7D%2C+A_%7Bt%2B1%7D%3E\" alt=\"&lt;S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}&gt;\" eeimg=\"1\"/> 。在有限MDP问题中，我们更新值函数时，每组经验元组会且只会使用一次；而（在无限MDP）我们使用DQN时，我们可以使用一个暂存区，把一定数量的经验元组暂存起来，然后能暂存区积累一定数量的经验元组时，再从暂存区<b>随机抽取经验元组进行训练</b>。这样做有两个好处：一是通过随机抽取经验元组，人为地切断了元组之间的前后关联关系，这样可以使函数避免落入局部最优；二是使一组经验元组可能会被使用多次，使得函数能更好地收敛。</p><p>通过经验回放处理，相当于把一个强化学习问题转化成一个监督学习问题。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>固定Q目标</b></p><p>在梯度更新公式<img src=\"https://www.zhihu.com/equation?tex=%5CDelta+w+%3D+%5Calpha+%28R_%7Bt%2B1%7D+%2B+%5Cgamma+max_a+%5Chat+q%28S_%7Bt%2B1%7D%2Ca%2C+w%29+-+%5Chat+q%28S_t%2CA_t%2C+w%29%29%5Cbigtriangledown+_w+%5Chat+q%28S_t%2C+A_t%2C+w%29\" alt=\"\\Delta w = \\alpha (R_{t+1} + \\gamma max_a \\hat q(S_{t+1},a, w) - \\hat q(S_t,A_t, w))\\bigtriangledown _w \\hat q(S_t, A_t, w)\" eeimg=\"1\"/> 中， <img src=\"https://www.zhihu.com/equation?tex=R_%7Bt%2B1%7D+%2B+%5Cgamma+max_a+%5Chat+q%28S_%7Bt%2B1%7D%2Ca%2C+w%29+\" alt=\"R_{t+1} + \\gamma max_a \\hat q(S_{t+1},a, w) \" eeimg=\"1\"/> 实际上是梯度更新的目标值（TD target）。然而，权值 <img src=\"https://www.zhihu.com/equation?tex=w\" alt=\"w\" eeimg=\"1\"/> 是一个变化量，从而导致目标值随之改变。这就好比坐在驴背手持萝卜来教驴直线走路，如下图：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-6632de7ce25dede787f0fa915f6d4814_b.jpg\" data-size=\"normal\" data-rawwidth=\"903\" data-rawheight=\"635\" class=\"origin_image zh-lightbox-thumb\" width=\"903\" data-original=\"https://pic1.zhimg.com/v2-6632de7ce25dede787f0fa915f6d4814_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;903&#39; height=&#39;635&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"903\" data-rawheight=\"635\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"903\" data-original=\"https://pic1.zhimg.com/v2-6632de7ce25dede787f0fa915f6d4814_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-6632de7ce25dede787f0fa915f6d4814_b.jpg\"/><figcaption>资料来源：Udacity深度学习课程</figcaption></figure><p>由于驴在移动的过程中会同时移动萝卜的位置，因此这样是无法教会驴走直线的。正确的做法是，人站在地上固定不动，让驴靠近时，人再往后退使驴可以继续走，如下图：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-c155ab36a32410731f2c43b867a453a5_b.jpg\" data-size=\"normal\" data-rawwidth=\"1131\" data-rawheight=\"598\" class=\"origin_image zh-lightbox-thumb\" width=\"1131\" data-original=\"https://pic2.zhimg.com/v2-c155ab36a32410731f2c43b867a453a5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1131&#39; height=&#39;598&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1131\" data-rawheight=\"598\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1131\" data-original=\"https://pic2.zhimg.com/v2-c155ab36a32410731f2c43b867a453a5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-c155ab36a32410731f2c43b867a453a5_b.jpg\"/><figcaption>资料来源：Udacity深度学习课程</figcaption></figure><p>同理，梯度更新时，目标值的权值应该用一个固定权值代替，使权值更新若干次后，再用最新的权值替换一次固定权值，如此类推。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>深度Q学习的具体算法</b></p><p>以上关于深度Q学习已经作了足够的铺垫，以下来看看伪代码：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-35d33e1981056950531605f7cdd85b1b_b.jpg\" data-size=\"normal\" data-rawwidth=\"966\" data-rawheight=\"743\" class=\"origin_image zh-lightbox-thumb\" width=\"966\" data-original=\"https://pic4.zhimg.com/v2-35d33e1981056950531605f7cdd85b1b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;966&#39; height=&#39;743&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"966\" data-rawheight=\"743\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"966\" data-original=\"https://pic4.zhimg.com/v2-35d33e1981056950531605f7cdd85b1b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-35d33e1981056950531605f7cdd85b1b_b.jpg\"/><figcaption>资料来源：Udacity深度学习课程</figcaption></figure><p>代码说明：</p><ul><li>数组D用于暂存经验元组，容量为N个；</li><li>以随机数初始化动作值函数的权值 <img src=\"https://www.zhihu.com/equation?tex=w\" alt=\"w\" eeimg=\"1\"/> ；</li><li>并以 <img src=\"https://www.zhihu.com/equation?tex=w\" alt=\"w\" eeimg=\"1\"/> 初始化固定权值 <img src=\"https://www.zhihu.com/equation?tex=w%5E-\" alt=\"w^-\" eeimg=\"1\"/> ；</li><li>设置M代训练循环；</li><li>对于每一代都要重新初始化起始状态向量；</li><li>对于每一代的每个时间步，主要做两件事：生成经验元组与训练；</li><li>生成的经验元组存于数组D；</li><li>从数组D获取随机批量的经验，先计算TD目标值，再进行梯度更新计算；</li><li>每隔C步就更新一次固定权值 <img src=\"https://www.zhihu.com/equation?tex=w%5E-\" alt=\"w^-\" eeimg=\"1\"/> 。</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p><b>深度Q学习的优化思路</b></p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cdiamond\" alt=\"\\diamond\" eeimg=\"1\"/> <b>双DQN（Double DQN）</b></p><p>从DQN的梯度更新公式 <img src=\"https://www.zhihu.com/equation?tex=%5CDelta+w+%3D+%5Calpha+%28R_%7Bt%2B1%7D+%2B+%5Cgamma+max_a+%5Chat+q%28S_%7Bt%2B1%7D%2Ca%2C+w%29+-+%5Chat+q%28S_t%2CA_t%2C+w%29%29%5Cbigtriangledown+_w+%5Chat+q%28S_t%2C+A_t%2C+w%29\" alt=\"\\Delta w = \\alpha (R_{t+1} + \\gamma max_a \\hat q(S_{t+1},a, w) - \\hat q(S_t,A_t, w))\\bigtriangledown _w \\hat q(S_t, A_t, w)\" eeimg=\"1\"/> 可知，其中的 <img src=\"https://www.zhihu.com/equation?tex=max_a+%5Chat+q%28S_%7Bt%2B1%7D%2Ca%2C+w%29+%3D+%5Chat+q%28S_%7Bt%2B1%7D%2C+argmax_a+%5Chat+q%28S_%7Bt%2B1%7D%2Ca%2Cw%29%2C+w%29\" alt=\"max_a \\hat q(S_{t+1},a, w) = \\hat q(S_{t+1}, argmax_a \\hat q(S_{t+1},a,w), w)\" eeimg=\"1\"/> 。由于在训练的初始阶段， <img src=\"https://www.zhihu.com/equation?tex=%5Chat+q\" alt=\"\\hat q\" eeimg=\"1\"/> 函数的值并不准确，我们并不应该太过盲目地信任这些值，并以此作为行动依据。那么怎么才能使策略更可靠呢？</p><p>其中一个被验证的方法是双DQN（论文《<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1509.06461\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Deep Reinforcement Learning with Double Q-learning</a>》）。大概的思路如下图：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-234ecd4c43e69b4749023a07e2bcb60b_b.jpg\" data-size=\"normal\" data-rawwidth=\"631\" data-rawheight=\"287\" class=\"origin_image zh-lightbox-thumb\" width=\"631\" data-original=\"https://pic4.zhimg.com/v2-234ecd4c43e69b4749023a07e2bcb60b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;631&#39; height=&#39;287&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"631\" data-rawheight=\"287\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"631\" data-original=\"https://pic4.zhimg.com/v2-234ecd4c43e69b4749023a07e2bcb60b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-234ecd4c43e69b4749023a07e2bcb60b_b.jpg\"/><figcaption>资料来源：Udacity深度学习课程</figcaption></figure><p>对于公式中的两个权值 <img src=\"https://www.zhihu.com/equation?tex=w\" alt=\"w\" eeimg=\"1\"/> ，使用不同的权值。这样就相当于使用了两个DQN，一个用于生成策略，另一个用于评估策略。如果使用之前提到的固定Q目标的方法与双DQN方法结合的话，固定权值 <img src=\"https://www.zhihu.com/equation?tex=w%5E-\" alt=\"w^-\" eeimg=\"1\"/> 就可以作为评估器使用。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cdiamond\" alt=\"\\diamond\" eeimg=\"1\"/> <b>优先经验回放</b></p><p>前面提到经验回放，我们会在经验暂存区随机取样进行批量训练。然而，有些经验比较“宝贵”出现的概率不高，但“信息量”很大，如果能进行多次训练，能有效提升DQN的性能。那么如何定义哪些经验元组更“宝贵”呢？</p><p>答案是TD error，如下图：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-56db3ebc1027eb181cf35a659f885526_b.jpg\" data-size=\"normal\" data-rawwidth=\"600\" data-rawheight=\"450\" class=\"origin_image zh-lightbox-thumb\" width=\"600\" data-original=\"https://pic3.zhimg.com/v2-56db3ebc1027eb181cf35a659f885526_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;600&#39; height=&#39;450&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"600\" data-rawheight=\"450\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"600\" data-original=\"https://pic3.zhimg.com/v2-56db3ebc1027eb181cf35a659f885526_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-56db3ebc1027eb181cf35a659f885526_b.jpg\"/><figcaption>资料来源：Udacity深度学习课程</figcaption></figure><p>TD误差越大，代表实际与预测的差距越大，因此“信息量”越大，模型能从中学到的规律就越多。因此，在保存经验元组时，可以用优先队列进行保存，以TD误差的绝对值作为优先级。在经验抽样时，不再以均匀分布来抽取，而按照优先级的分值分配概率。</p><p>以上的处理经过证明，能减少值函数所需的批次更新数量（论文《<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1511.05952\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Prioritized Experience Replay</a>》）。</p><p>在上述处理的基础上，还能进行几处优化：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-d544a015598b3544f1814dd661efa48c_b.jpg\" data-size=\"normal\" data-rawwidth=\"858\" data-rawheight=\"546\" class=\"origin_image zh-lightbox-thumb\" width=\"858\" data-original=\"https://pic1.zhimg.com/v2-d544a015598b3544f1814dd661efa48c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;858&#39; height=&#39;546&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"858\" data-rawheight=\"546\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"858\" data-original=\"https://pic1.zhimg.com/v2-d544a015598b3544f1814dd661efa48c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-d544a015598b3544f1814dd661efa48c_b.jpg\"/><figcaption>资料来源：Udacity深度学习课程</figcaption></figure><ul><li>如果一个经验元组的TD误差很小甚至为零，那么它在抽样时被抽中的概率就几乎为零，但这并不代表这个经验就不值得学习，也许只是因为模型在之前所经历的经验样本有限，使估值比较接近真实值而已；因此在原来TD误差的基础上加上一个常量e作为优先级得分，从而让这些经验元组也有可能被选中。</li><li>另一个问题是，如果优先级得分较高的经验总是会大概率地被不断回放，这可能会导致模型过拟合于这部分经验子集；为了避免这个问题，对经验的优先级得分加上一个指数 <img src=\"https://www.zhihu.com/equation?tex=a\" alt=\"a\" eeimg=\"1\"/> ，这就相当于降低了 <img src=\"https://www.zhihu.com/equation?tex=p_i\" alt=\"p_i\" eeimg=\"1\"/> 的概率，因为当 <img src=\"https://www.zhihu.com/equation?tex=a%3D0\" alt=\"a=0\" eeimg=\"1\"/> 时，相当于以均匀的概率选择经验元组，而当 <img src=\"https://www.zhihu.com/equation?tex=a+%3D+1\" alt=\"a = 1\" eeimg=\"1\"/> 时就以优先级得分作为概率计算。</li><li>当我们使用优先级得分抽样时，我们需要微调更新规则，加上权重系数。具体原因可参考论文。</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cdiamond\" alt=\"\\diamond\" eeimg=\"1\"/> <b>对抗网络架构（Dueling Networks）</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-a0a2f3728f2bb74c2492f0e12d101747_b.jpg\" data-size=\"normal\" data-rawwidth=\"1237\" data-rawheight=\"575\" class=\"origin_image zh-lightbox-thumb\" width=\"1237\" data-original=\"https://pic4.zhimg.com/v2-a0a2f3728f2bb74c2492f0e12d101747_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1237&#39; height=&#39;575&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1237\" data-rawheight=\"575\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1237\" data-original=\"https://pic4.zhimg.com/v2-a0a2f3728f2bb74c2492f0e12d101747_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-a0a2f3728f2bb74c2492f0e12d101747_b.jpg\"/><figcaption>资料来源：Udacity深度学习课程</figcaption></figure><p>对抗网络与原始DQN的主要区别在于全连接层从一个变为两个，具体参考论文《<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1511.06581\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Dueling Network Architectures for Deep Reinforcement Learning</a>》。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>关于DQN的实践，可参考以下实战项目：</p><a href=\"https://zhuanlan.zhihu.com/p/36249826\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic3.zhimg.com/v2-d2c8a891c7d10543f87984d2a0769a5e_180x120.jpg\" data-image-width=\"762\" data-image-height=\"351\" class=\"internal\">黄伟亮：DQN实战：MIT强化学习实战—Deep Traffic（上）</a><hr/><p>深度Q学习是一种基于值函数的方法（value-based method），即根据状态值或动作值函数来获得最优策略。而有另一种方法，直接求最优策略而无需基于值函数（policy-based method），这就是策略梯度。</p><h2><b>策略梯度（policy gradient）</b></h2><p>既然已经有了基于值函数的方法，为何还需要有直接求最优策略的方法呢？</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cdiamond\" alt=\"\\diamond\" eeimg=\"1\"/> <b>因为更简单：</b><br/>最优策略 <img src=\"https://www.zhihu.com/equation?tex=%5Cpi_%2A\" alt=\"\\pi_*\" eeimg=\"1\"/> 本质上是一个策略函数。</p><ul><li>对于确定性策略而言，策略函数实际上是 <img src=\"https://www.zhihu.com/equation?tex=s%5Crightarrow+a\" alt=\"s\\rightarrow a\" eeimg=\"1\"/> 的映射；</li><li>对于随机性策略而言，策略函数实际上是 <img src=\"https://www.zhihu.com/equation?tex=s%5Crightarrow+%5Cmathbb+P%5Ba%7Cs%5D\" alt=\"s\\rightarrow \\mathbb P[a|s]\" eeimg=\"1\"/> 概率分布的映射；</li></ul><p>直接求策略函数的好处是可以减少对大量无关数据的储存。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cdiamond\" alt=\"\\diamond\" eeimg=\"1\"/> <b>因为能生成最优的随机策略：</b></p><p>基于值函数的方法，无法生成真正的随机策略，如下图：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-d262c9b355b02b43b503a2257e06630c_b.jpg\" data-size=\"normal\" data-rawwidth=\"612\" data-rawheight=\"294\" class=\"origin_image zh-lightbox-thumb\" width=\"612\" data-original=\"https://pic1.zhimg.com/v2-d262c9b355b02b43b503a2257e06630c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;612&#39; height=&#39;294&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"612\" data-rawheight=\"294\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"612\" data-original=\"https://pic1.zhimg.com/v2-d262c9b355b02b43b503a2257e06630c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-d262c9b355b02b43b503a2257e06630c_b.jpg\"/><figcaption>资料来源：Udacity深度学习课程</figcaption></figure><p>智能体的目标是要拿到香蕉。辣椒代表惩罚。智能体能感知当前状态下周围的环境，当智能体位于中间时，由于三边无墙，向下移动是最优策略；当智能体位于两边时，当左边有墙则向右移动为最优，而当右边有墙则向左移动为最优。但当智能体位于阴影区域时，两块阴影区域的状态完全一样，如下图：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-b2bba95f096c925ec789bf571a181a40_b.jpg\" data-size=\"normal\" data-rawwidth=\"614\" data-rawheight=\"329\" class=\"origin_image zh-lightbox-thumb\" width=\"614\" data-original=\"https://pic1.zhimg.com/v2-b2bba95f096c925ec789bf571a181a40_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;614&#39; height=&#39;329&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"614\" data-rawheight=\"329\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"614\" data-original=\"https://pic1.zhimg.com/v2-b2bba95f096c925ec789bf571a181a40_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-b2bba95f096c925ec789bf571a181a40_b.jpg\"/><figcaption>资料来源：Udacity深度学习课程</figcaption></figure><p>如果根据值函数求最优策略，会形成要么全部向左，要么全部向右的策略，这样会导致智能体陷入死循环。即便使用 <img src=\"https://www.zhihu.com/equation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"/> 系数来增加策略的随机性，也需要很长的时间才能让智能体“摸索”到出路，这样的模型效率相当低；如果增大 <img src=\"https://www.zhihu.com/equation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"/> 系数，则会严重影响模型性能。因此，最优的策略是在阴影区域使用随机策略，如下图：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-efbdef895c49258eaa8aefc69ffad14e_b.jpg\" data-size=\"normal\" data-rawwidth=\"610\" data-rawheight=\"264\" class=\"origin_image zh-lightbox-thumb\" width=\"610\" data-original=\"https://pic3.zhimg.com/v2-efbdef895c49258eaa8aefc69ffad14e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;610&#39; height=&#39;264&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"610\" data-rawheight=\"264\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"610\" data-original=\"https://pic3.zhimg.com/v2-efbdef895c49258eaa8aefc69ffad14e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-efbdef895c49258eaa8aefc69ffad14e_b.jpg\"/><figcaption>资料来源：Udacity深度学习课程</figcaption></figure><p>基于值函数的方法倾向于学习确定性策略或者近似确定性策略，而基于策略的方法能学习期望的随机性策略。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cdiamond\" alt=\"\\diamond\" eeimg=\"1\"/> <b>因为对于连续的动作空间，基于值函数的方法并不适用：</b></p><p>我们知道在基于值函数的方法中，最优策略是用 <img src=\"https://www.zhihu.com/equation?tex=argmax\" alt=\"argmax\" eeimg=\"1\"/> 函数求出，这仅仅适用于离散的动作空间；要在连续的动作空间中找最大值并不容易，尤其是当动作空间不是一维而是多维。因此，需要使用直接求策略函数的方法。</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>那么如何得到策略函数，使得从状态可以直接映射到动作或者动作分布呢？</b></p><p>首先，我们知道我们希望找到一个从某状态 <img src=\"https://www.zhihu.com/equation?tex=s\" alt=\"s\" eeimg=\"1\"/> 映射到动作 <img src=\"https://www.zhihu.com/equation?tex=a\" alt=\"a\" eeimg=\"1\"/> 或者动作分布 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbb%7BP%7D%5Ba%7Cs%5D\" alt=\"\\mathbb{P}[a|s]\" eeimg=\"1\"/> 的函数。从线性代数理论可知，把一个向量映射到另一个向量，可以直接用矩阵相乘可得（详见《程序员的数学——线性代数》一书有通俗易懂的讲解）。因此，我们把这映射函数命名为权重 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> （注意，矩阵等价于映射函数，同时这个矩阵又可称为权重）。于是我们可以把这个映射关系写成 <img src=\"https://www.zhihu.com/equation?tex=a%5Csim+%5Cpi%28s%2Ca%2C%5Ctheta%29+%3D+%5Cmathbb%7BP%7D%5Ba%7Cs%2C%5Ctheta%5D\" alt=\"a\\sim \\pi(s,a,\\theta) = \\mathbb{P}[a|s,\\theta]\" eeimg=\"1\"/> ，用中文翻译就是“策略 <img src=\"https://www.zhihu.com/equation?tex=%5Cpi\" alt=\"\\pi\" eeimg=\"1\"/> 就是在权值 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 下，状态 <img src=\"https://www.zhihu.com/equation?tex=s\" alt=\"s\" eeimg=\"1\"/> 到动作分布 <img src=\"https://www.zhihu.com/equation?tex=a\" alt=\"a\" eeimg=\"1\"/> 的映射”。只要我们找到最合适的映射权重 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 使得我们的期望收益最大化，那么这时的策略 <img src=\"https://www.zhihu.com/equation?tex=%5Cpi_%7B%5Ctheta%7D\" alt=\"\\pi_{\\theta}\" eeimg=\"1\"/> 便是最优策略。这就是我们的目标。把这个目标用函数的形式表达，可以写成公式： <img src=\"https://www.zhihu.com/equation?tex=J%28%5Ctheta%29+%3D+%5Cmathbb+E_%7B%5Cpi%7D%5BR%28%5Ctau%29%5D\" alt=\"J(\\theta) = \\mathbb E_{\\pi}[R(\\tau)]\" eeimg=\"1\"/> 。</p><p>当我第一次看到这公式时是相当困惑：怎么突然冒出个参数 <img src=\"https://www.zhihu.com/equation?tex=%5Ctau\" alt=\"\\tau\" eeimg=\"1\"/> ？对于这问题，如解释不当，还望高人指点：</p><ul><li><img src=\"https://www.zhihu.com/equation?tex=R%28%5Ctau%29\" alt=\"R(\\tau)\" eeimg=\"1\"/> 其实是一个抽象函数。也就是说这个函数可以在不同的情况下套用不同的具体函数；同样，参数 <img src=\"https://www.zhihu.com/equation?tex=%5Ctau\" alt=\"\\tau\" eeimg=\"1\"/> 也是一个抽象参数。</li><li>对于阶段性任务而言，具体函数表达为 <img src=\"https://www.zhihu.com/equation?tex=J_1%28%5Ctheta%29+%3D+%5Cmathbb+E_%7B%5Cpi%7D%5BG_1%5D+%3D+%5Cmathbb+E_%7B%5Cpi%7D%5BV%28s_1%29%5D\" alt=\"J_1(\\theta) = \\mathbb E_{\\pi}[G_1] = \\mathbb E_{\\pi}[V(s_1)]\" eeimg=\"1\"/> ，即从起始状态开始计算回合的期望状态值；</li><li>对于持续性任务而言，由于没有明确的起始状态，则可以考虑以下三种计算方法（阶段性任务也适用）：</li></ul><ol><li><img src=\"https://www.zhihu.com/equation?tex=J_%7B%5Cbar%7Bv%7D%7D%28%5Ctheta%29+%3D+%5Cmathbb+E_%7B%5Cpi%7D%5BV%28s%29%5D\" alt=\"J_{\\bar{v}}(\\theta) = \\mathbb E_{\\pi}[V(s)]\" eeimg=\"1\"/> ，则以期望状态值均值作为目标函数；</li><li><img src=\"https://www.zhihu.com/equation?tex=J_%7B%5Cbar%7Bq%7D%7D%28%5Ctheta%29+%3D+%5Cmathbb+E_%7B%5Cpi%7D%5BQ%28s%2Ca%29%5D\" alt=\"J_{\\bar{q}}(\\theta) = \\mathbb E_{\\pi}[Q(s,a)]\" eeimg=\"1\"/> ，则以期望动作值均值作为目标函数；</li><li><img src=\"https://www.zhihu.com/equation?tex=J_%7B%5Cbar%7Br%7D%7D%28%5Ctheta%29+%3D+%5Cmathbb+E_%7B%5Cpi%7D%5Br%5D\" alt=\"J_{\\bar{r}}(\\theta) = \\mathbb E_{\\pi}[r]\" eeimg=\"1\"/> ，则以期望奖励均值作为目标函数；</li></ol><ul><li>我们可以根据不同的情况，选用以上四种具体目标函数的任意一种。比如，在我们不清楚如何得到值函数的情况下，用一段时期内（每个阶段）的奖励均值作为计算依据，是很自然的思路——在统计期内，期望奖励越高，当然就代表策略越好啦。</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p>既然有了目标函数，如何求得最优策略呢？</p><p>我们知道当目标函数 <img src=\"https://www.zhihu.com/equation?tex=J%28%5Ctheta%29\" alt=\"J(\\theta)\" eeimg=\"1\"/> 越大，策略就越好。如何求得最优策略的 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 呢？有两种思路：</p><ul><li>随机搜索：直接估算权重 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> ，无需知道目标函数 <img src=\"https://www.zhihu.com/equation?tex=J%28%5Ctheta%29\" alt=\"J(\\theta)\" eeimg=\"1\"/> </li><li>策略梯度：根据目标函数 <img src=\"https://www.zhihu.com/equation?tex=J%28%5Ctheta%29\" alt=\"J(\\theta)\" eeimg=\"1\"/> 来求梯度更新</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p><b>随机搜索</b></p><p>随机搜索是指随机生成权重 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> ，然后评估策略 <img src=\"https://www.zhihu.com/equation?tex=%5Cpi_%7B%5Ctheta%7D\" alt=\"\\pi_{\\theta}\" eeimg=\"1\"/> 所得到的期望奖励；若期望奖励比之前高，则用 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 更新权重。如此不断迭代。由于该方法效率极低，因此一般情况下不采用，因此不作详细说明。大概的算法有：</p><ul><li>最陡上升爬山法：随机生成 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> ，然后以 <img src=\"https://www.zhihu.com/equation?tex=%5Csigma\" alt=\"\\sigma\" eeimg=\"1\"/> 为标准差随机生成多个 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta%27\" alt=\"\\theta&#39;\" eeimg=\"1\"/> ，若最大的 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta%27\" alt=\"\\theta&#39;\" eeimg=\"1\"/> 大于 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> ，则更新 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 。不断迭代这过程直到迭代停止。</li><li>模拟退火法：爬山法类似，但在更新权重的同时，会缩小 <img src=\"https://www.zhihu.com/equation?tex=%5Csigma\" alt=\"\\sigma\" eeimg=\"1\"/> 。</li><li>适应噪点法：与模拟退火法类似，但当找不到更好的 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta%27\" alt=\"\\theta&#39;\" eeimg=\"1\"/> 时，会扩大 <img src=\"https://www.zhihu.com/equation?tex=%5Csigma\" alt=\"\\sigma\" eeimg=\"1\"/> 。</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><p><b>策略梯度</b></p><p>策略梯度的基本思路是，利用目标函数 <img src=\"https://www.zhihu.com/equation?tex=J%28%5Ctheta%29\" alt=\"J(\\theta)\" eeimg=\"1\"/> 来求梯度更新。更新逻辑为： <img src=\"https://www.zhihu.com/equation?tex=%5CDelta+%5Ctheta+%3D+%5Calpha+%5Cbigtriangledown+_%5Ctheta+J%28%5Ctheta%29\" alt=\"\\Delta \\theta = \\alpha \\bigtriangledown _\\theta J(\\theta)\" eeimg=\"1\"/> ，其中 <img src=\"https://www.zhihu.com/equation?tex=%5Calpha\" alt=\"\\alpha\" eeimg=\"1\"/> 为学习率（更新步长）。</p><p>然而，目标函数并不总是可导（比如当目标函数很复杂时）。这时则需要用一个估算值来代替近似梯度，即 <img src=\"https://www.zhihu.com/equation?tex=%5Cbigtriangledown+_%5Ctheta+J%28%5Ctheta%29+%3D+%5Cfrac%7B%5Cpartial+J%28%5Ctheta%29%7D%7B%5Cpartial+%5Ctheta_k%7D+%5Capprox+%5Cfrac%7BJ%28%5Ctheta+%2B+%5Cepsilon+u_k%29+-+J%28%5Ctheta%29%7D%7B%5Cepsilon%7D%EF%BC%8C%28for%5C+k%5Cin+%5B1%2Cn%5D%29\" alt=\"\\bigtriangledown _\\theta J(\\theta) = \\frac{\\partial J(\\theta)}{\\partial \\theta_k} \\approx \\frac{J(\\theta + \\epsilon u_k) - J(\\theta)}{\\epsilon}，(for\\ k\\in [1,n])\" eeimg=\"1\"/> ，其中 <img src=\"https://www.zhihu.com/equation?tex=k\" alt=\"k\" eeimg=\"1\"/> 是权重 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 的维度数， <img src=\"https://www.zhihu.com/equation?tex=u_k\" alt=\"u_k\" eeimg=\"1\"/> 是第 <img src=\"https://www.zhihu.com/equation?tex=k\" alt=\"k\" eeimg=\"1\"/> 维的单位向量， <img src=\"https://www.zhihu.com/equation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"/> 是一个非常小的数。该近似法的本质是对每一个维度进行斜率估算，以得到整个目标函数的梯度 <img src=\"https://www.zhihu.com/equation?tex=%5Cbigtriangledown+_%5Ctheta+J%28%5Ctheta%29\" alt=\"\\bigtriangledown _\\theta J(\\theta)\" eeimg=\"1\"/> ，即 <img src=\"https://www.zhihu.com/equation?tex=%5Cbigtriangledown+_%5Ctheta+J%28%5Ctheta%29+%3D+%5B%5Cfrac%7B%5Cpartial+J%28%5Ctheta%29%7D%7B%5Cpartial+%5Ctheta_1%7D%2C%5Cfrac%7B%5Cpartial+J%28%5Ctheta%29%7D%7B%5Cpartial+%5Ctheta_2%7D%2C...%2C%5Cfrac%7B%5Cpartial+J%28%5Ctheta%29%7D%7B%5Cpartial+%5Ctheta_n%7D%5D\" alt=\"\\bigtriangledown _\\theta J(\\theta) = [\\frac{\\partial J(\\theta)}{\\partial \\theta_1},\\frac{\\partial J(\\theta)}{\\partial \\theta_2},...,\\frac{\\partial J(\\theta)}{\\partial \\theta_n}]\" eeimg=\"1\"/> 。</p><p>以上用斜率近似求梯度的方法，每次更新都要计算一次，因此计算量相当大。因此，我们可以通过变换目标函数，使其变得更容易求导。</p><p>已知 <img src=\"https://www.zhihu.com/equation?tex=J%28%5Ctheta%29+%3D+%5Cmathbb+E_%7B%5Cpi%7D%5BR%28%5Ctau%29%5D\" alt=\"J(\\theta) = \\mathbb E_{\\pi}[R(\\tau)]\" eeimg=\"1\"/> ，因此有 <img src=\"https://www.zhihu.com/equation?tex=%5Cbigtriangledown_%7B%5Ctheta%7D+J%28%5Ctheta%29+%3D%5Cbigtriangledown_%7B%5Ctheta%7D+%5Cmathbb+E_%7B%5Cpi%7D%5BR%28%5Ctau%29%5D\" alt=\"\\bigtriangledown_{\\theta} J(\\theta) =\\bigtriangledown_{\\theta} \\mathbb E_{\\pi}[R(\\tau)]\" eeimg=\"1\"/> 。直接求 <img src=\"https://www.zhihu.com/equation?tex=%5Cbigtriangledown_%7B%5Ctheta%7D+%5Cmathbb+E_%7B%5Cpi%7D%5BR%28%5Ctau%29%5D\" alt=\"\\bigtriangledown_{\\theta} \\mathbb E_{\\pi}[R(\\tau)]\" eeimg=\"1\"/> 不太好办，因此暂且把奖励函数 <img src=\"https://www.zhihu.com/equation?tex=R%28%5Ctau%29\" alt=\"R(\\tau)\" eeimg=\"1\"/> 用随机得分函数<img src=\"https://www.zhihu.com/equation?tex=f%28x%29\" alt=\"f(x)\" eeimg=\"1\"/> 代替，其中 <img src=\"https://www.zhihu.com/equation?tex=x%5Csim+%5Cmathbb+P%5Bx%7C%5Ctheta%5D\" alt=\"x\\sim \\mathbb P[x|\\theta]\" eeimg=\"1\"/> （x是基于 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 的概率分布）。于是便可以做以下一系列的变换：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-a2d7f2df1ca1e03ef7ab4c164798f815_b.jpg\" data-size=\"normal\" data-rawwidth=\"967\" data-rawheight=\"634\" class=\"origin_image zh-lightbox-thumb\" width=\"967\" data-original=\"https://pic2.zhimg.com/v2-a2d7f2df1ca1e03ef7ab4c164798f815_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;967&#39; height=&#39;634&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"967\" data-rawheight=\"634\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"967\" data-original=\"https://pic2.zhimg.com/v2-a2d7f2df1ca1e03ef7ab4c164798f815_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-a2d7f2df1ca1e03ef7ab4c164798f815_b.jpg\"/><figcaption>资料来源：Udacity深度学习课程</figcaption></figure><p>有了以上的变换启发，再把奖励函数替换回来，于是有：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-be36c99029fd63002fb435057a39b34b_b.jpg\" data-size=\"normal\" data-rawwidth=\"1044\" data-rawheight=\"586\" class=\"origin_image zh-lightbox-thumb\" width=\"1044\" data-original=\"https://pic4.zhimg.com/v2-be36c99029fd63002fb435057a39b34b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1044&#39; height=&#39;586&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1044\" data-rawheight=\"586\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1044\" data-original=\"https://pic4.zhimg.com/v2-be36c99029fd63002fb435057a39b34b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-be36c99029fd63002fb435057a39b34b_b.jpg\"/><figcaption>资料来源：Udacity深度学习课程</figcaption></figure><p>公式说明：</p><ul><li>根据我们前面的定义 <img src=\"https://www.zhihu.com/equation?tex=%5Cpi%28s%2Ca%2C%5Ctheta%29+%3D+%5Cmathbb%7BP%7D%5Ba%7Cs%2C%5Ctheta%5D\" alt=\"\\pi(s,a,\\theta) = \\mathbb{P}[a|s,\\theta]\" eeimg=\"1\"/> 。</li><li>假如某个动作 <img src=\"https://www.zhihu.com/equation?tex=a\" alt=\"a\" eeimg=\"1\"/> 的概率很低，那该动作的对数值的绝对值就很大，若这时的奖励值很高，那么就会使 <img src=\"https://www.zhihu.com/equation?tex=%5CDelta+%5Ctheta\" alt=\"\\Delta \\theta\" eeimg=\"1\"/> 更大；相反，如果 <img src=\"https://www.zhihu.com/equation?tex=a\" alt=\"a\" eeimg=\"1\"/> 的概率本来就很高，那即使奖励值很高， <img src=\"https://www.zhihu.com/equation?tex=%5CDelta+%5Ctheta\" alt=\"\\Delta \\theta\" eeimg=\"1\"/> 的变化也不大。因此，用对数几率乘以奖励函数，可以起到纠正权重 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 的作用。</li></ul><p>这就是策略梯度的基本更新公式。</p><p>有了基本更新公式，我们就可以用蒙特卡罗方法，通过不断地与环境互动来得到经验样本，从而估算更新值：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-0f495413385db408d039e8964306248d_b.jpg\" data-size=\"normal\" data-rawwidth=\"895\" data-rawheight=\"476\" class=\"origin_image zh-lightbox-thumb\" width=\"895\" data-original=\"https://pic2.zhimg.com/v2-0f495413385db408d039e8964306248d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;895&#39; height=&#39;476&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"895\" data-rawheight=\"476\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"895\" data-original=\"https://pic2.zhimg.com/v2-0f495413385db408d039e8964306248d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-0f495413385db408d039e8964306248d_b.jpg\"/><figcaption>资料来源：Udacity深度学习课程</figcaption></figure><p>代码说明：</p><ul><li><img src=\"https://www.zhihu.com/equation?tex=G_t+%3D+R_%7Bt%2B1%7D+%2B+%5Cgamma+R_%7Bt%2B2%7D+%2B+%5Cgamma%5E2+R_%7Bt%2B3%7D+%2B+...\" alt=\"G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ...\" eeimg=\"1\"/> </li><li>每个阶段（回合）更新一次策略函数，并用于下个回合生成经验元组</li></ul><hr/><p>到此为止，我们掌握了基于值函数（状态值函数与动作值函数）的方法，这些方法对于离散的动作空间是可行的；而对于连续的动作空间、以及要生成随机性策略，则需要应用基于策略函数的方法。然而基于策略函数的方法最大的问题在于无法衡量策略究竟是不是最优。这就需要借助值函数的方法了。结合值函数与策略函数的方法，称为行动者与评论者方法（Actor-Critic method）</p><h2><b>行动者与评论者方法</b></h2><p>前面提到，奖励函数 <img src=\"https://www.zhihu.com/equation?tex=R%28%5Ctau%29\" alt=\"R(\\tau)\" eeimg=\"1\"/> 是一个抽象函数。在蒙特卡罗方法中，用了 <img src=\"https://www.zhihu.com/equation?tex=R%28%5Ctau%29+%3D+G_t+%3D+R_%7Bt%2B1%7D+%2B+%5Cgamma+R_%7Bt%2B2%7D+%2B+%5Cgamma%5E2+R_%7Bt%2B3%7D+%2B+...\" alt=\"R(\\tau) = G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + ...\" eeimg=\"1\"/> 。由于蒙特卡罗方法只能用于阶段性任务，因此对于持续性任务，该方法并不适用。一是因为无法计算奖励折扣，二是因为无法定义策略更新时机。</p><p>因此，我们需要一个能在线计算的奖励函数。比如动作值函数 <img src=\"https://www.zhihu.com/equation?tex=Q%28s%2Ca%29\" alt=\"Q(s,a)\" eeimg=\"1\"/> ，即把更新公式写成： <img src=\"https://www.zhihu.com/equation?tex=%5CDelta+%5Ctheta+%3D+%5Calpha+%5Cbigtriangledown_%5Ctheta+%28log+%5Cpi%28S_t%2CA_t%2C%5Ctheta%29%29Q%28S_t%2CA_t%29\" alt=\"\\Delta \\theta = \\alpha \\bigtriangledown_\\theta (log \\pi(S_t,A_t,\\theta))Q(S_t,A_t)\" eeimg=\"1\"/> 。</p><p>那动作值函数 <img src=\"https://www.zhihu.com/equation?tex=Q%28s%2Ca%29\" alt=\"Q(s,a)\" eeimg=\"1\"/> 要怎么计算呢？可以参考之前的动作值更新思路：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5CDelta+Q+%3D+%5Calpha%28R_%7Bt%2B1%7D+%2B+%5Cgamma+Q%28S_%7Bt%2B1%7D%2CA_%7Bt%2B1%7D%29+-+Q%28S_t%2C+A_t%29%29\" alt=\"\\Delta Q = \\alpha(R_{t+1} + \\gamma Q(S_{t+1},A_{t+1}) - Q(S_t, A_t))\" eeimg=\"1\"/> 。</p><p>注意！以上公式是Sarsa0的更新公式，我们都知道这个公式只对离散的状态空间与动作空间适用。对于连续的状态空间与动作空间，动作值函数的更新公式要加上权重w来做映射：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5CDelta+w+%3D+%5Calpha%28R_%7Bt%2B1%7D+%2B+%5Cgamma+%5Chat+q%28S_%7Bt%2B1%7D%2CA_%7Bt%2B1%7D%2C+w%29+-+%5Chat+q%28S_t%2C+A_t%2C+w%29%29+%5Cbigtriangledown_w+%5Chat+q%28S_t%2C+A_t%2C+w%29\" alt=\"\\Delta w = \\alpha(R_{t+1} + \\gamma \\hat q(S_{t+1},A_{t+1}, w) - \\hat q(S_t, A_t, w)) \\bigtriangledown_w \\hat q(S_t, A_t, w)\" eeimg=\"1\"/> </p><p>注意！由于在前面已经用了 <img src=\"https://www.zhihu.com/equation?tex=%5Calpha\" alt=\"\\alpha\" eeimg=\"1\"/> 作为 <img src=\"https://www.zhihu.com/equation?tex=%5CDelta+%5Ctheta\" alt=\"\\Delta \\theta\" eeimg=\"1\"/> 的学习率，因为这里需要把 <img src=\"https://www.zhihu.com/equation?tex=%5CDelta+w\" alt=\"\\Delta w\" eeimg=\"1\"/> 的学习率换成 <img src=\"https://www.zhihu.com/equation?tex=%5Cbeta\" alt=\"\\beta\" eeimg=\"1\"/> ：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5CDelta+w+%3D+%5Cbeta%28R_%7Bt%2B1%7D+%2B+%5Cgamma+%5Chat+q%28S_%7Bt%2B1%7D%2CA_%7Bt%2B1%7D%2C+w%29+-+%5Chat+q%28S_t%2C+A_t%2C+w%29%29+%5Cbigtriangledown_w+%5Chat+q%28S_t%2C+A_t%2C+w%29\" alt=\"\\Delta w = \\beta(R_{t+1} + \\gamma \\hat q(S_{t+1},A_{t+1}, w) - \\hat q(S_t, A_t, w)) \\bigtriangledown_w \\hat q(S_t, A_t, w)\" eeimg=\"1\"/> </p><p class=\"ztext-empty-paragraph\"><br/></p><p>于是，到此为止我们有了两个逼近函数：</p><ul><li>策略函数 <img src=\"https://www.zhihu.com/equation?tex=%5Cpi+%28s%2Ca%2C%5Ctheta%29\" alt=\"\\pi (s,a,\\theta)\" eeimg=\"1\"/> ，它就好比是一个演员（Actor）专门负责做动作；</li><li>值函数 <img src=\"https://www.zhihu.com/equation?tex=%5Chat+q%28s%2Ca%2Cw%29\" alt=\"\\hat q(s,a,w)\" eeimg=\"1\"/> ，它就好比是一个评论者（Critic）专门负责给反馈；</li></ul><p>这两个函数可以由两个不同的神经网络去逼近，并互相作用。整个作用过程如下图：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-d58f39e34b71992788354010565a6af6_b.jpg\" data-size=\"normal\" data-rawwidth=\"1436\" data-rawheight=\"756\" class=\"origin_image zh-lightbox-thumb\" width=\"1436\" data-original=\"https://pic3.zhimg.com/v2-d58f39e34b71992788354010565a6af6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1436&#39; height=&#39;756&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1436\" data-rawheight=\"756\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1436\" data-original=\"https://pic3.zhimg.com/v2-d58f39e34b71992788354010565a6af6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-d58f39e34b71992788354010565a6af6_b.jpg\"/><figcaption>资料来源：Udacity深度学习课程</figcaption></figure><p>说明：</p><ol><li>根据环境状态 <img src=\"https://www.zhihu.com/equation?tex=S_t\" alt=\"S_t\" eeimg=\"1\"/> ，通过策略函数 <img src=\"https://www.zhihu.com/equation?tex=%5Cpi_%7B%5Ctheta%7D\" alt=\"\\pi_{\\theta}\" eeimg=\"1\"/> 映射得出相应的动作 <img src=\"https://www.zhihu.com/equation?tex=A_t\" alt=\"A_t\" eeimg=\"1\"/> </li><li>这时通过 <img src=\"https://www.zhihu.com/equation?tex=%5CDelta+%5Ctheta\" alt=\"\\Delta \\theta\" eeimg=\"1\"/> 的更新公式便可以更新策略函数</li><li><img src=\"https://www.zhihu.com/equation?tex=A_t\" alt=\"A_t\" eeimg=\"1\"/> 与环境互动，生成 <img src=\"https://www.zhihu.com/equation?tex=R_%7Bt%2B1%7D%2C+S_%7Bt%2B1%7D\" alt=\"R_{t+1}, S_{t+1}\" eeimg=\"1\"/> </li><li>这时通过 <img src=\"https://www.zhihu.com/equation?tex=%5CDelta+w\" alt=\"\\Delta w\" eeimg=\"1\"/> 的更新公式便可以更新值函数</li><li>有了 <img src=\"https://www.zhihu.com/equation?tex=S_%7Bt%2B1%7D\" alt=\"S_{t+1}\" eeimg=\"1\"/> 与新的策略函数，便可以重复步骤1进行迭代</li><li>注意：与蒙特卡罗的回合更新不同，行动者与评论者方法是单步更新</li></ol><p class=\"ztext-empty-paragraph\"><br/></p><p>仔细研究更新机制，我们可以有所改进：</p><p>由于 <img src=\"https://www.zhihu.com/equation?tex=%5CDelta+%5Ctheta+%3D+%5Calpha+%5Cbigtriangledown_%5Ctheta+J%28%5Ctheta%29\" alt=\"\\Delta \\theta = \\alpha \\bigtriangledown_\\theta J(\\theta)\" eeimg=\"1\"/> ，</p><p>又有 <img src=\"https://www.zhihu.com/equation?tex=%5Cbigtriangledown_%7B%5Ctheta%7DJ%28%5Ctheta%29+%3D+%5Cmathbb+E_%7B%5Cpi%7D%5B%5Cbigtriangledown_%7B%5Ctheta%7D+%28log+%5Cpi%28s%2Ca%2C%5Ctheta%29%29%5Chat+q%28s%2Ca%2Cw%29%5D\" alt=\"\\bigtriangledown_{\\theta}J(\\theta) = \\mathbb E_{\\pi}[\\bigtriangledown_{\\theta} (log \\pi(s,a,\\theta))\\hat q(s,a,w)]\" eeimg=\"1\"/> 。</p><p>我们之所以能计算期望值 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathbb+E_%7B%5Cpi%7D\" alt=\"\\mathbb E_{\\pi}\" eeimg=\"1\"/> 是因为我们用了小的学习率 <img src=\"https://www.zhihu.com/equation?tex=%5Calpha\" alt=\"\\alpha\" eeimg=\"1\"/> 来迭代计算得到。之所以需要保持足够小的学习率，是因为在最终随机抽样时，单个样本可能会变化很大。只要是在计算期望值，就会存在相关误差。如果我们尝试去估算这个期望值，则最好使样本之间的方差尽量小，这会使得整个过程更加稳定。该方程方差的大小主要是受奖励函数 <img src=\"https://www.zhihu.com/equation?tex=%5Chat+q\" alt=\"\\hat q\" eeimg=\"1\"/> 所影响。对于每个时间步， <img src=\"https://www.zhihu.com/equation?tex=%5Chat+q\" alt=\"\\hat q\" eeimg=\"1\"/> 的值可能会变化很大，因为它基于单个奖励。这样可能会导致策略更新的步长不一。那么，如何才能减少这个方差呢？</p><p>假设动作值函数 <img src=\"https://www.zhihu.com/equation?tex=Q%28s%2Ca%29\" alt=\"Q(s,a)\" eeimg=\"1\"/> 符合正态分布，那么该分布的均值 <img src=\"https://www.zhihu.com/equation?tex=%5Cmu+%3D+%5Cmathbb+E_%7B%5Cpi%7D%5BQ%28s%2Ca%29%5D\" alt=\"\\mu = \\mathbb E_{\\pi}[Q(s,a)]\" eeimg=\"1\"/> 。而对于状态 <img src=\"https://www.zhihu.com/equation?tex=s\" alt=\"s\" eeimg=\"1\"/> ， <img src=\"https://www.zhihu.com/equation?tex=Q%28s%2Ca%29\" alt=\"Q(s,a)\" eeimg=\"1\"/> 是在状态 <img src=\"https://www.zhihu.com/equation?tex=s\" alt=\"s\" eeimg=\"1\"/> 下整个动作空间下的分布，由于这个分布的期望值本质上就是状态值，于是有 <img src=\"https://www.zhihu.com/equation?tex=%5Cmu+%3D+%5Cmathbb+E_%7B%5Cpi%7D%5BQ%28s%2Ca%29%5D+%3D+V%28s%29\" alt=\"\\mu = \\mathbb E_{\\pi}[Q(s,a)] = V(s)\" eeimg=\"1\"/>。此时，如果把奖励函数表达为：</p><p><img src=\"https://www.zhihu.com/equation?tex=A%28s%2Ca%29+%3D+Q%28s%2Ca%29+-+V%28s%29\" alt=\"A(s,a) = Q(s,a) - V(s)\" eeimg=\"1\"/> ， <img src=\"https://www.zhihu.com/equation?tex=A\" alt=\"A\" eeimg=\"1\"/> 被称为优势函数。它可以理解为“对于状态 <img src=\"https://www.zhihu.com/equation?tex=s\" alt=\"s\" eeimg=\"1\"/> ，当我们采取特定动作 <img src=\"https://www.zhihu.com/equation?tex=a\" alt=\"a\" eeimg=\"1\"/> ，而非随机动作时，我们可以从中得到多少额外收益”。因此，优势函数不仅会稳定学习过程，而且可以更好地区分动作。</p><p>把优势函数代入原来的得分函数，有：</p><ul><li><img src=\"https://www.zhihu.com/equation?tex=%5CDelta+%5Ctheta+%3D+%5Calpha+%5Cbigtriangledown_%7B%5Ctheta%7D%28log%5Cpi%28s%2Ca%2C%5Ctheta%29%29%28%5Chat+q%28s%2Ca%2Cw%29+-+%5Chat+v%28s%2Cw%27%29%29\" alt=\"\\Delta \\theta = \\alpha \\bigtriangledown_{\\theta}(log\\pi(s,a,\\theta))(\\hat q(s,a,w) - \\hat v(s,w&#39;))\" eeimg=\"1\"/> </li></ul><p>为了进一步简化计算，避免计算两个值函数，可以用TD误差来估算优势函数，有：</p><ul><li><img src=\"https://www.zhihu.com/equation?tex=%5CDelta+%5Ctheta+%3D+%5Calpha+%5Cbigtriangledown_%7B%5Ctheta%7D%28log%5Cpi%28s%2Ca%2C%5Ctheta%29%29%28r+%2B+%5Cgamma+%5Chat+v%28s%27%2Cw%29+-+%5Chat+v%28s%2Cw%29%29\" alt=\"\\Delta \\theta = \\alpha \\bigtriangledown_{\\theta}(log\\pi(s,a,\\theta))(r + \\gamma \\hat v(s&#39;,w) - \\hat v(s,w))\" eeimg=\"1\"/> </li></ul><p>注意，这时的值函数权重的更新公式也会变为：</p><ul><li><img src=\"https://www.zhihu.com/equation?tex=%5CDelta+w+%3D+%5Cbeta%28r+%2B+%5Cgamma+%5Chat+v%28S_%7Bt%2B1%7D%2Cw%29+-+%5Chat+v%28S_t%2Cw%29%29%5Cbigtriangledown_w+%5Chat+v%28S_t%2Cw%29\" alt=\"\\Delta w = \\beta(r + \\gamma \\hat v(S_{t+1},w) - \\hat v(S_t,w))\\bigtriangledown_w \\hat v(S_t,w)\" eeimg=\"1\"/> </li></ul><hr/><p>现在我们梳理一下：</p><p>Q-learning是在离散状态空间与动作空间下，寻找最优策略 <img src=\"https://www.zhihu.com/equation?tex=%5Cpi_%2A\" alt=\"\\pi_*\" eeimg=\"1\"/> 的有效方法。但当状态空间维度过高或者在连续的状态空间下，Q-learning方法就不再适用，这时就需要用到DQN来解决高维状态空间的问题；然而，DQN只能解决高维状态空间，却不能解决高维动作空间，因为DQN是依赖最大化动作值来找到合适的策略。因此，为了解决高维动作空间的问题，就有了Policy Gradient方法。但原始Policy Gradient方法无法及时得知所得的动作值，使得性能相当不理想，于是有了Actor-Critic方法。Actor-Critic方法基本上解决了高维状态与动作空间的问题，并使性能有明显的提升。但原始的Actor-Critic方法对于复杂问题可能会不稳定。所以才有了以下的改进方法。</p><h2><b>DDPG（Deep Deterministic Policy Gradient，深度确定性策略梯度）</b></h2><p>DDPG是基于Actor-Critic方法框架，并引入DQN的两种高稳定性的训练思路：1, 经验回放；2, 固定Q目标。另外，在论文中提及DDPG在搭建神经网络时还用到了BN层来优化训练。</p><p>回顾Actor-Critic方法，我们知道：</p><ul><li>对于Actor函数的更新公式为（未考虑优势函数的优化）：<br/> <img src=\"https://www.zhihu.com/equation?tex=%5CDelta+%5Ctheta+%3D+%5Calpha+%5Cbigtriangledown_%5Ctheta+%28log+%5Cpi%28S_t%2CA_t%2C%5Ctheta%29%29%5Chat+q%28S_t%2CA_t%29\" alt=\"\\Delta \\theta = \\alpha \\bigtriangledown_\\theta (log \\pi(S_t,A_t,\\theta))\\hat q(S_t,A_t)\" eeimg=\"1\"/> </li><li>对于Critic函数的更新公式为：<br/><img src=\"https://www.zhihu.com/equation?tex=%5CDelta+w+%3D+%5Cbeta%28R_%7Bt%2B1%7D+%2B+%5Cgamma+%5Chat+q%28S_%7Bt%2B1%7D%2CA_%7Bt%2B1%7D%2C+w%29+-+%5Chat+q%28S_t%2C+A_t%2C+w%29%29+%5Cbigtriangledown_w+%5Chat+q%28S_t%2C+A_t%2C+w%29\" alt=\"\\Delta w = \\beta(R_{t+1} + \\gamma \\hat q(S_{t+1},A_{t+1}, w) - \\hat q(S_t, A_t, w)) \\bigtriangledown_w \\hat q(S_t, A_t, w)\" eeimg=\"1\"/> </li></ul><p>而在DDPG略有不同的是，对于Actor与Critic函数，各自都会有两个神经网络（原理参考DQN的固定Q目标优化方法）。具体参考以下伪代码（<b>注意符号表达与之前略有不同</b>）：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-8715c8a60e197aff4abcada34e35ed8b_b.jpg\" data-size=\"normal\" data-rawwidth=\"704\" data-rawheight=\"514\" class=\"origin_image zh-lightbox-thumb\" width=\"704\" data-original=\"https://pic4.zhimg.com/v2-8715c8a60e197aff4abcada34e35ed8b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;704&#39; height=&#39;514&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"704\" data-rawheight=\"514\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"704\" data-original=\"https://pic4.zhimg.com/v2-8715c8a60e197aff4abcada34e35ed8b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-8715c8a60e197aff4abcada34e35ed8b_b.jpg\"/><figcaption>资料来源：参考资料[8]</figcaption></figure><p>代码说明：</p><ul><li>首先随机初始化一个Critic神经网络 <img src=\"https://www.zhihu.com/equation?tex=Q%28s%2Ca%7C%5Ctheta%5E%7BQ%7D%29\" alt=\"Q(s,a|\\theta^{Q})\" eeimg=\"1\"/> ，其中 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta%5EQ\" alt=\"\\theta^Q\" eeimg=\"1\"/> 是网络权重；同理，随机初始化一个Actor神经网络 <img src=\"https://www.zhihu.com/equation?tex=%5Cmu%28s%7C%5Ctheta%5E%7B%5Cmu%7D%29\" alt=\"\\mu(s|\\theta^{\\mu})\" eeimg=\"1\"/> ，其中 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%5Cmu%7D\" alt=\"\\theta^{\\mu}\" eeimg=\"1\"/> 是网络权重；</li><li>再定义一个Critic目标网络 <img src=\"https://www.zhihu.com/equation?tex=Q%27\" alt=\"Q&#39;\" eeimg=\"1\"/> ，以及一个Actor的目标网络 <img src=\"https://www.zhihu.com/equation?tex=%5Cmu%27\" alt=\"\\mu&#39;\" eeimg=\"1\"/> ，并使 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta%5E%7BQ%27%7D+%3D+%5Ctheta%5EQ\" alt=\"\\theta^{Q&#39;} = \\theta^Q\" eeimg=\"1\"/> 、 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%5Cmu%27%7D+%3D+%5Ctheta%5E%7B%5Cmu%7D\" alt=\"\\theta^{\\mu&#39;} = \\theta^{\\mu}\" eeimg=\"1\"/> ；</li><li>初始化一个经验缓存 <img src=\"https://www.zhihu.com/equation?tex=R\" alt=\"R\" eeimg=\"1\"/> ；</li><li>对于每一个回合：</li><ul><li>初始化一个用于动作探索的随机噪点生成函数 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathcal+N\" alt=\"\\mathcal N\" eeimg=\"1\"/> ；</li><li>重置环境并获取一个初始状态向量 <img src=\"https://www.zhihu.com/equation?tex=s_1\" alt=\"s_1\" eeimg=\"1\"/> ；</li><li>对于回合中的每个时间步：</li><ul><li>依据当前的策略函数、Actor网络 <img src=\"https://www.zhihu.com/equation?tex=%5Cmu%28s_t%7C%5Ctheta%5E%7B%5Cmu%7D%29\" alt=\"\\mu(s_t|\\theta^{\\mu})\" eeimg=\"1\"/> 与探索噪点函数 <img src=\"https://www.zhihu.com/equation?tex=%5Cmathcal+N_t\" alt=\"\\mathcal N_t\" eeimg=\"1\"/>  生成当前的动作 <img src=\"https://www.zhihu.com/equation?tex=a_t\" alt=\"a_t\" eeimg=\"1\"/> </li><li>利用 <img src=\"https://www.zhihu.com/equation?tex=a_t\" alt=\"a_t\" eeimg=\"1\"/> 与环境互动，得到 <img src=\"https://www.zhihu.com/equation?tex=r_%7Bt%2B1%7D\" alt=\"r_{t+1}\" eeimg=\"1\"/> 与新的状态 <img src=\"https://www.zhihu.com/equation?tex=s_%7Bt%2B1%7D\" alt=\"s_{t+1}\" eeimg=\"1\"/> </li><li>把该经验元组 <img src=\"https://www.zhihu.com/equation?tex=%28s_t%2C+a_t%2C+r_%7Bt%2B1%7D%2C+s_%7Bt%2B1%7D%29\" alt=\"(s_t, a_t, r_{t+1}, s_{t+1})\" eeimg=\"1\"/> 保存到经验缓存 <img src=\"https://www.zhihu.com/equation?tex=R\" alt=\"R\" eeimg=\"1\"/> （注意：此处为了与之前的命名一致，没有按以上论文伪代码那样写成 <img src=\"https://www.zhihu.com/equation?tex=r_t\" alt=\"r_t\" eeimg=\"1\"/> ，而是写成 <img src=\"https://www.zhihu.com/equation?tex=r_%7Bt%2B1%7D\" alt=\"r_{t+1}\" eeimg=\"1\"/> 。实质上是同一个奖励）</li><li>当经验缓存的数量大于抽样批次数量N时，从经验缓存随机抽样N个经验元组</li><li>使用Critic目标网络与Actor目标网络来当“真实”的值函数输出： <img src=\"https://www.zhihu.com/equation?tex=y_i+%3D+r_i+%2B+%5Cgamma+Q%27%28s_%7Bi%7D%27%2C+%5Cmu%27%28s_i%27%7C%5Ctheta%5E%7B%5Cmu%27%7D%29%7C%5Ctheta%5E%7BQ%27%7D%29%29\" alt=\"y_i = r_i + \\gamma Q&#39;(s_{i}&#39;, \\mu&#39;(s_i&#39;|\\theta^{\\mu&#39;})|\\theta^{Q&#39;}))\" eeimg=\"1\"/> <br/>注意：此处的 <img src=\"https://www.zhihu.com/equation?tex=i\" alt=\"i\" eeimg=\"1\"/> 代表第 <img src=\"https://www.zhihu.com/equation?tex=i\" alt=\"i\" eeimg=\"1\"/> 个经验元组，因此为了不与其他经验元组混淆，笔者把论文的公式略为修改把 <img src=\"https://www.zhihu.com/equation?tex=i\" alt=\"i\" eeimg=\"1\"/> 个经验元组中的 <img src=\"https://www.zhihu.com/equation?tex=s_%7Bt%2B1%7D\" alt=\"s_{t+1}\" eeimg=\"1\"/> 表达为 <img src=\"https://www.zhihu.com/equation?tex=s_i%27\" alt=\"s_i&#39;\" eeimg=\"1\"/> 。</li><li>有了“真实值”与预估值，我们才有损失函数 <img src=\"https://www.zhihu.com/equation?tex=L+%3D+%5Cfrac%7B1%7D%7BN%7D+%5Csum_i+%28y_i+-+Q%28s_i%2C+a_i%7C%5Ctheta%5EQ%29%29%5E2\" alt=\"L = \\frac{1}{N} \\sum_i (y_i - Q(s_i, a_i|\\theta^Q))^2\" eeimg=\"1\"/> </li><li>有了损失函数，才能求梯度： <img src=\"https://www.zhihu.com/equation?tex=%5Cbigtriangledown+_%7B%5Ctheta%5E%7B%5Cmu%7D%7DJ+%3D+%5Cfrac%7B1%7D%7BN%7D+%5Cbigtriangledown+_a+Q%28s%2C+a%7C+%5Ctheta%5EQ%29%7C_%7Bs%3Ds_i%2C+a%3D%5Cmu%28s_i%29%7D+%5Cbigtriangledown+_%7B%5Ctheta%5E%7B%5Cmu%7D%7D%5Cmu%28s%7C%5Ctheta%5E%7B%5Cmu%7D%29%7C_%7Bs_i%7D\" alt=\"\\bigtriangledown _{\\theta^{\\mu}}J = \\frac{1}{N} \\bigtriangledown _a Q(s, a| \\theta^Q)|_{s=s_i, a=\\mu(s_i)} \\bigtriangledown _{\\theta^{\\mu}}\\mu(s|\\theta^{\\mu})|_{s_i}\" eeimg=\"1\"/> <br/>说明：坦白说，笔者第一眼看到这公式时有点眩晕的感觉，估计能一眼看懂这公式的应该是数学系毕业的。但当笔者看完代码再回头写一遍这公式时，就觉得非常理解了。稍后再作详细分析。</li><li>最后，以一个学习率 <img src=\"https://www.zhihu.com/equation?tex=%5Ctau\" alt=\"\\tau\" eeimg=\"1\"/> 来分别更新目标网络，因此有：<br/> <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta+%5E%7BQ%27%7D+%5Cleftarrow+%5Ctau+%5Ctheta%5EQ+%2B+%281-%5Ctau%29%5Ctheta%5E%7BQ%27%7D\" alt=\"\\theta ^{Q&#39;} \\leftarrow \\tau \\theta^Q + (1-\\tau)\\theta^{Q&#39;}\" eeimg=\"1\"/> ；<br/> <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%5Cmu%27%7D+%5Cleftarrow+%5Ctau+%5Ctheta%5E%7B%5Cmu%7D+%2B+%281-+%5Ctau%29+%5Ctheta%5E%7B%5Cmu%27%7D\" alt=\"\\theta^{\\mu&#39;} \\leftarrow \\tau \\theta^{\\mu} + (1- \\tau) \\theta^{\\mu&#39;}\" eeimg=\"1\"/> </li></ul></ul></ul><p>以下是DDPG的学习更新代码：</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"k\">def</span> <span class=\"nf\">learn</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"p\">,</span> <span class=\"n\">experiences</span><span class=\"p\">):</span>\n    <span class=\"s2\">&#34;&#34;&#34;Update policy and value parameters using given batch of experience tuples.&#34;&#34;&#34;</span>\n    <span class=\"c1\"># Convert experience tuples to separate arrays for each element (states, actions, rewards, etc.)</span>\n    <span class=\"n\">states</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">vstack</span><span class=\"p\">([</span><span class=\"n\">e</span><span class=\"o\">.</span><span class=\"n\">state</span> <span class=\"k\">for</span> <span class=\"n\">e</span> <span class=\"ow\">in</span> <span class=\"n\">experiences</span> <span class=\"k\">if</span> <span class=\"n\">e</span> <span class=\"ow\">is</span> <span class=\"ow\">not</span> <span class=\"kc\">None</span><span class=\"p\">])</span>\n    <span class=\"n\">actions</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"n\">e</span><span class=\"o\">.</span><span class=\"n\">action</span> <span class=\"k\">for</span> <span class=\"n\">e</span> <span class=\"ow\">in</span> <span class=\"n\">experiences</span> <span class=\"k\">if</span> <span class=\"n\">e</span> <span class=\"ow\">is</span> <span class=\"ow\">not</span> <span class=\"kc\">None</span><span class=\"p\">])</span><span class=\"o\">.</span><span class=\"n\">astype</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">action_size</span><span class=\"p\">)</span>\n    <span class=\"n\">rewards</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"n\">e</span><span class=\"o\">.</span><span class=\"n\">reward</span> <span class=\"k\">for</span> <span class=\"n\">e</span> <span class=\"ow\">in</span> <span class=\"n\">experiences</span> <span class=\"k\">if</span> <span class=\"n\">e</span> <span class=\"ow\">is</span> <span class=\"ow\">not</span> <span class=\"kc\">None</span><span class=\"p\">])</span><span class=\"o\">.</span><span class=\"n\">astype</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n    <span class=\"n\">dones</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">array</span><span class=\"p\">([</span><span class=\"n\">e</span><span class=\"o\">.</span><span class=\"n\">done</span> <span class=\"k\">for</span> <span class=\"n\">e</span> <span class=\"ow\">in</span> <span class=\"n\">experiences</span> <span class=\"k\">if</span> <span class=\"n\">e</span> <span class=\"ow\">is</span> <span class=\"ow\">not</span> <span class=\"kc\">None</span><span class=\"p\">])</span><span class=\"o\">.</span><span class=\"n\">astype</span><span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">uint8</span><span class=\"p\">)</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">)</span>\n    <span class=\"n\">next_states</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">vstack</span><span class=\"p\">([</span><span class=\"n\">e</span><span class=\"o\">.</span><span class=\"n\">next_state</span> <span class=\"k\">for</span> <span class=\"n\">e</span> <span class=\"ow\">in</span> <span class=\"n\">experiences</span> <span class=\"k\">if</span> <span class=\"n\">e</span> <span class=\"ow\">is</span> <span class=\"ow\">not</span> <span class=\"kc\">None</span><span class=\"p\">])</span>\n\n    <span class=\"c1\"># 通过Actor目标网络，根据next_state生成next_action</span>\n    <span class=\"n\">actions_next</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">actor_target</span><span class=\"o\">.</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">predict_on_batch</span><span class=\"p\">(</span><span class=\"n\">next_states</span><span class=\"p\">)</span>\n    <span class=\"c1\"># 通过Critic目标网络，根据next_state与next_action生成动作值Q_targets_next</span>\n    <span class=\"n\">Q_targets_next</span> <span class=\"o\">=</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">critic_target</span><span class=\"o\">.</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">predict_on_batch</span><span class=\"p\">([</span><span class=\"n\">next_states</span><span class=\"p\">,</span> <span class=\"n\">actions_next</span><span class=\"p\">])</span>\n\n    <span class=\"c1\"># 通过TD目标公式求Critic网络的“真实值 y_i”(当回合结束dones=1)</span>\n    <span class=\"n\">Q_targets</span> <span class=\"o\">=</span> <span class=\"n\">rewards</span> <span class=\"o\">+</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">gamma</span> <span class=\"o\">*</span> <span class=\"n\">Q_targets_next</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"mi\">1</span> <span class=\"o\">-</span> <span class=\"n\">dones</span><span class=\"p\">)</span>\n    <span class=\"c1\"># 以Q_targets为目标值，states与actions为输入训练Critic估值网络</span>\n    <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">critic_local</span><span class=\"o\">.</span><span class=\"n\">model</span><span class=\"o\">.</span><span class=\"n\">train_on_batch</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">states</span><span class=\"p\">,</span> <span class=\"n\">actions</span><span class=\"p\">],</span> <span class=\"n\">y</span><span class=\"o\">=</span><span class=\"n\">Q_targets</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># 借助Tensorflow的方法获取本轮Critic估值网络的训练梯度</span>\n    <span class=\"n\">action_gradients</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">critic_local</span><span class=\"o\">.</span><span class=\"n\">get_action_gradients</span><span class=\"p\">([</span><span class=\"n\">states</span><span class=\"p\">,</span> <span class=\"n\">actions</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">]),</span> <span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">action_size</span><span class=\"p\">))</span>\n    <span class=\"c1\"># 借助Tensorflow的方法训练Actor估值网络</span>\n    <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">actor_local</span><span class=\"o\">.</span><span class=\"n\">train_fn</span><span class=\"p\">([</span><span class=\"n\">states</span><span class=\"p\">,</span> <span class=\"n\">action_gradients</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">])</span>  <span class=\"c1\"># custom training function</span>\n\n    <span class=\"c1\"># 以学习率\\tau来更新目标网络</span>\n    <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">soft_update</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">critic_local</span><span class=\"o\">.</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">critic_target</span><span class=\"o\">.</span><span class=\"n\">model</span><span class=\"p\">)</span>\n    <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">soft_update</span><span class=\"p\">(</span><span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">actor_local</span><span class=\"o\">.</span><span class=\"n\">model</span><span class=\"p\">,</span> <span class=\"bp\">self</span><span class=\"o\">.</span><span class=\"n\">actor_target</span><span class=\"o\">.</span><span class=\"n\">model</span><span class=\"p\">)</span>\n</code></pre></div><p>以上代码中，最难的部分仍然是Actor估值网络与Critic估值网络的权重更新部分。伪代码的难点在于数学公式的理解，而代码的难点在于Tensorflow（或其他）框架的使用。</p><p>这就是DDPG理论部分的全部内容。</p><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p><b>以上便是强化学习相关课程的全部内容整理。笔者水平有限，若有勘误，欢迎指正。</b></p><p class=\"ztext-empty-paragraph\"><br/></p><p>延伸阅读：</p><p><a href=\"https://link.zhihu.com/?target=https%3A//www.sypopo.com/post/0zoDJBA2r2/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">为你分享73篇论文解决深度强化学习的18个关键问题 / 水缘泡泡</a></p><hr/><p>参考资料：</p><ol><li>Udacity深度学习进阶课程P5，课程优惠码：6E1CAAC8</li><li>《<a href=\"https://link.zhihu.com/?target=http%3A//go.udacity.com/rl-textbook\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Reinforcement Learning: An Introduction</a>》，Richard S. Sutton and Andrew G. Barto</li><li><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/saltriver/article/details/52194918\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">蒙特卡洛方法到底有什么用 - CSDN博客</a></li><li><a href=\"https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Monte_Carlo_method\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Monte Carlo method</a>, <a href=\"https://link.zhihu.com/?target=http%3A//en.wikipedia.org/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">en.wikipedia.org</a> </li><li><a href=\"https://link.zhihu.com/?target=https%3A//karpathy.github.io/2016/05/31/rl/%3F_utm_source%3D1-2-2\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Deep Reinforcement Learning: Pong from Pixels</a> </li><li>《<a href=\"https://link.zhihu.com/?target=http%3A//www.deeplearningbook.org/contents/ml.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Deep Learning</a>》, 5.5_Maximum Likelihood Estimation</li><li>《<a href=\"https://link.zhihu.com/?target=https%3A//storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Human-level control through deep reinforcement learning</a>》， <a href=\"https://link.zhihu.com/?target=http%3A//nature.com/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">nature.com</a></li><li><a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1509.02971\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">[1509.02971] Continuous control with deep reinforcement learning</a> </li><li><a href=\"https://link.zhihu.com/?target=https%3A//pemami4911.github.io/blog/2016/08/21/ddpg-rl.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Deep Deterministic Policy Gradients in TensorFlow</a></li></ol>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "人工智能", 
                    "tagLink": "https://api.zhihu.com/topics/19551275"
                }
            ], 
            "comments": [
                {
                    "userName": "milo chena", 
                    "userLink": "https://www.zhihu.com/people/f44caa4148fbdd76e3aac8e5258ece18", 
                    "content": "离线学习和异步策略有什么差别捏？是不是一个东西呀～我想不明白，谢谢你～", 
                    "likes": 1, 
                    "childComments": [
                        {
                            "userName": "黄伟亮", 
                            "userLink": "https://www.zhihu.com/people/294306d6706f830e3cce35fb4acf46cb", 
                            "content": "<p>我是这么理解的：online/offline指的是是否能一边在生产环境运行一边训练； on-policy（同步）/off-policy（异步）指的是生成动作时与更新值函数时，是否用的是同一个策略。</p>", 
                            "likes": 1, 
                            "replyToAuthor": "milo chena"
                        }, 
                        {
                            "userName": "陈浩", 
                            "userLink": "https://www.zhihu.com/people/05b0bbc8a6a1a92a1a1008a12a6c7acf", 
                            "content": "<p>完全不是一个东西。离线学习off-line是指一个episode结束后再更新值函数。异步策略off-policy是指用其他策略估计待估计策略的优劣。</p>", 
                            "likes": 2, 
                            "replyToAuthor": "milo chena"
                        }
                    ]
                }, 
                {
                    "userName": "milo chena", 
                    "userLink": "https://www.zhihu.com/people/f44caa4148fbdd76e3aac8e5258ece18", 
                    "content": "深度的部分写的非常好，请问你用什么编辑的公式呀，是markdown不？超厉害！", 
                    "likes": 1, 
                    "childComments": [
                        {
                            "userName": "黄伟亮", 
                            "userLink": "https://www.zhihu.com/people/294306d6706f830e3cce35fb4acf46cb", 
                            "content": "<p>知乎自带公式编辑器</p>", 
                            "likes": 1, 
                            "replyToAuthor": "milo chena"
                        }
                    ]
                }, 
                {
                    "userName": "玄玄", 
                    "userLink": "https://www.zhihu.com/people/1a5a196c1e910c16a27f46fbad997fae", 
                    "content": "<p>楼主研究的很深入，学习了</p>", 
                    "likes": 1, 
                    "childComments": []
                }, 
                {
                    "userName": "黄伟亮", 
                    "userLink": "https://www.zhihu.com/people/294306d6706f830e3cce35fb4acf46cb", 
                    "content": "<p>我觉得应该这样理解：在线学习与离线学习是个“适不适合”的问题；而同步策略与异步策略是个“是不是”的问题。也就是说，一般而言，同步策略比较适合在线学习，而异步策略比较适合离线学习。因为异步策略的策略更新会有延迟，所以不适合一边在生产环境运行一边进行学习，即不适合在线学习。</p>", 
                    "likes": 1, 
                    "childComments": []
                }, 
                {
                    "userName": "pyb2468", 
                    "userLink": "https://www.zhihu.com/people/ab9edfacd5bf3f9d25118c1bc6d11da4", 
                    "content": "哇  厉害厉害", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/35516173", 
            "userName": "黄伟亮", 
            "userLink": "https://www.zhihu.com/people/294306d6706f830e3cce35fb4acf46cb", 
            "upvote": 0, 
            "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition (VGG模型论文译文，下)", 
            "content": "<p>版权声明：</p><p>该论文原文出处地址为<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1409.1556\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">[1409.1556] Very Deep Convolutional Networks for Large-Scale Image Recognition</a>；</p><p>标题大图来源地址为<a href=\"https://link.zhihu.com/?target=https%3A//www.cs.toronto.edu/~frossard/post/vgg16/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">VGG in TensorFlow</a>；</p><p>本翻译仅供学习用途，并未经原作者授权，因此任何转载所造成的侵权行为均有可能受到追责。我们是一群机器学习爱好者，如果你也有兴趣与我们共同翻译原始论文，欢迎加入！详情请邮件联系kevinbauer@163.com。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>VGG模型论文探讨并证明了以下观点：</p><ol><li>用多层的卷积层组合配以小尺寸的滤波器，能实现大尺寸滤波器的感受野的同时，还能使参数数量更少；</li><li>表示层深度的增加，能有效提升网络的性能；</li><li>模型融合的性能优于单模型的性能；</li><li>训练期间分阶段降低学习率有助模型收敛；</li></ol><p class=\"ztext-empty-paragraph\"><br/></p><p>上半部分链接：</p><a href=\"https://zhuanlan.zhihu.com/p/34851133\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-dfe4eaaa4450e2b58b38c5fe82f918c0_180x120.jpg\" data-image-width=\"470\" data-image-height=\"276\" class=\"internal\">kevinbauer：Very Deep Convolutional Networks for Large-Scale Image Recognition (VGG模型论文译文，上)</a><p>正文（下）</p><p>4 CLASSIFICATION EXPERIMENTS</p><p>4 分类实验</p><p><b>Dataset.</b> In this section, we present the image classification results achieved by the described ConvNet architectures on the ILSVRC-2012 dataset (which was used for ILSVRC 2012–2014 challenges). The dataset includes images of 1000 classes, and is split into three sets: training (1.3M images), validation (50K images), and testing (100K images with held-out class labels). The classification performance is evaluated using two measures: the top-1 and top-5 error. The former is a multi-class classification error, i.e. the proportion of incorrectly classified images; the latter is the main evaluation criterion used in ILSVRC, and is computed as the proportion of images such that the ground-truth category is outside the top-5 predicted categories. For the majority of experiments, we used the validation set as the test set. Certain experiments were also carried out on the test set and submitted to the official ILSVRC server as a “VGG” team entry to the ILSVRC-2014 competition (Russakovsky et al., 2014).</p><p>数据集。在本节中，我们将介绍上述ConvNet体系结构在ILSVRC-2012数据集（用于ILSVRC 2012-2014挑战赛）上实现的图像分类结果。该数据集包括1000种分类的图像，并且被分成三组：训练集（1.3M张图像），验证集（50K张图像）和测试集（100K张标签被去除的图像）。分类性能使用两个标准来评估：Top-1和Top-5的误差。前者是多分类误差，即错误分类图像的比例；后者是ILSVRC比赛使用的主要评估标准，按照Top-5预测分类中不存在真实分类的图像所占比例计算。对于大多数实验，我们使用验证集作为测试集。还对测试集进行了一些实验，并将其作为ILGVRC-2014竞赛“VGG”团队的参赛作品（Russakovsky等，2014）提交给官方ILSVRC服务器。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>4.1 SINGLE SCALE EVALUATION</p><p>4.1 （图像）单尺度评估</p><p>We begin with evaluating the performance of individual ConvNet models at a single scale with the layer configurations described in Sect. 2.2. The test image size was set as follows: Q = S for fixed S, and Q = 0.5(S min + S max ) for jittered S ∈ [S min , S max ]. The results of are shown in Table 3.</p><p>我们从单尺度开始对各个ConvNet模型的性能进行评估，模型的配置如2.2节所述。 测试图像大小设置如下：对于固定的S，Q = S；对于变动的S，S∈[Smin，Smax]，Q = 0.5（Smin + Smax）。 结果如表3所示。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>First, we note that using local response normalisation (A-LRN network) does not improve on the model A without any normalisation layers. We thus do not employ normalisation in the deeper architectures (B–E).</p><p>首先，我们注意到在没有任何标准化层的情况下，使用局部响应标准化（A-LRN网络）在模型A上没有改进。 因此，我们在深层架构（B-E）中不采用标准化层。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>Second, we observe that the classification error decreases with the increased ConvNet depth: from 11 layers in A to 19 layers in E. Notably, in spite of the same depth, the configuration C (which contains three 1 × 1 conv. layers), performs worse than the configuration D, which uses 3 × 3 conv. layers throughout the network. This indicates that while the additional non-linearity does help (C is better than B), it is also important to capture spatial context by using conv. filters with non-trivial receptive fields (D is better than C). The error rate of our architecture saturates when the depth reaches 19 layers, but even deeper models might be beneficial for larger datasets. We also compared the net B with a shallow net with five 5 × 5 conv. layers, which was derived from B by replacing each pair of 3 × 3 conv. layers with a single 5 × 5 conv. layer (which has the same receptive field as explained in Sect. 2.3). The top-1 error of the shallow net was measured to be 7% higher than that of B (on a center crop), which confirms that a deep net with small filters outperforms a shallow net with larger filters.</p><p>其次，我们观察到分类错误随着ConvNet深度的增加而减少：从模型A的11层到模型E的19层。值得注意的是，尽管深度相同，配置C（其包含三个1×1的转换层），比整个网络全部使用3×3 卷积的配置D更差。这表明虽然额外的非线性确实有帮助（C比B好），但使用感受野范围不少的卷积核（D比C好）捕获空间上下文也很重要。当深度达到19层时，我们架构的错误率会饱和，但即使是更深的模型也可能对更大的数据集有所帮助。我们还特意以B网络为基准，把每两层3x3卷积替换为一层5×5 卷积核的浅网进行比较（其具有与B网络相同的感受野，见2.3节中解释）。该浅层网络的Top-1误差比B网络（用中心裁切图像）的误差高7％，这证实了带有小型卷积核的深网优于具有更大卷积核的浅网。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>Finally, scale jittering at training time (S ∈ [256; 512]) leads to significantly better results than training on images with fixed smallest side (S = 256 or S = 384), even though a single scale is used at test time. This confirms that training set augmentation by scale jittering is indeed helpful for capturing multi-scale image statistics.</p><p>最后，即使在测试时使用单尺度图像，训练时的图像缩放（S∈[256; 512]）比起图像固定最小边（S = 256或S = 384）得到明显更好的结果。 这证实了通过尺度抖动来增强训练集确实有助于捕获多尺度图像统计信息。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-0711ec0c299b383fc44651ef45c62a91_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"870\" data-rawheight=\"386\" class=\"origin_image zh-lightbox-thumb\" width=\"870\" data-original=\"https://pic2.zhimg.com/v2-0711ec0c299b383fc44651ef45c62a91_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;870&#39; height=&#39;386&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"870\" data-rawheight=\"386\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"870\" data-original=\"https://pic2.zhimg.com/v2-0711ec0c299b383fc44651ef45c62a91_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-0711ec0c299b383fc44651ef45c62a91_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>4.2 MULTI-SCALE EVALUATION</p><p>4.2 多尺度评估</p><p>Having evaluated the ConvNet models at a single scale, we now assess the effect of scale jittering at test time. It consists of running a model over several rescaled versions of a test image (corresponding to different values of Q), followed by averaging the resulting class posteriors. Considering that a large discrepancy between training and testing scales leads to a drop in performance, the models trained with fixed S were evaluated over three test image sizes, close to the training one: Q = {S − 32, S, S + 32}. At the same time, scale jittering at training time allows the network to be applied to a wider range of scales at test time, so the model trained with variable S ∈ [Smin ; Smax ] was evaluated over a larger range of sizes Q = {Smin , 0.5(Smin + Smax ), Smax }.</p><p>在评估了ConvNet模型的单一尺度后，我们现在评估测试时尺度抖动的影响。 它包括在一个测试图像的多个缩放版本上运行模型（对应于不同的Q值），然后对结果分类的后验概率进行平均。 考虑到训练和测试所用尺度的巨大差异会导致性能下降，用固定S训练的模型在三个图像尺寸上进行测试评估，测试尺寸接近训练尺寸：Q = {S - 32，S，S + 32}。 同时，训练时的尺度抖动可使网络在测试时使用更广泛的尺度范围，所以模型用变量S∈[Smin, Smax]在更大范围的尺寸Q = {Smin，0.5(Smin + Smax)，Smax}下评估。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>The results, presented in Table 4, indicate that scale jittering at test time leads to better performance (as compared to evaluating the same model at a single scale, shown in Table 3). As before, the deepest configurations (D and E) perform the best, and scale jittering is better than training with a fixed smallest side S. Our best single-network performance on the validation set is 24.8%/7.5% top-1/top-5 error (highlighted in bold in Table 4). On the test set, the configuration E achieves 7.3% top-5 error.</p><p>表4中所示的结果表明，在测试时的尺度抖动可得到更好的性能（与在单尺度上评估相同模型相比，如表3所示）。 与以前一样，最深的配置（D和E）表现最好，而使用缩放抖动的训练模型优于使用固定最小边S的训练模型。我们在验证集上的最佳单网络性能是Top-1 24.8％、Top-5 7.5％错误率（表4中以粗体突出显示）。 在测试集上，配置E达到7.3％ 的Top-5错误率。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-802e19c2c149e113b104e8400712995f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"888\" data-rawheight=\"330\" class=\"origin_image zh-lightbox-thumb\" width=\"888\" data-original=\"https://pic4.zhimg.com/v2-802e19c2c149e113b104e8400712995f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;888&#39; height=&#39;330&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"888\" data-rawheight=\"330\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"888\" data-original=\"https://pic4.zhimg.com/v2-802e19c2c149e113b104e8400712995f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-802e19c2c149e113b104e8400712995f_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>4.3 MULTI-CROP EVALUATION</p><p>4.3 多裁切图像评估</p><p>In Table 5 we compare dense ConvNet evaluation with mult-crop evaluation (see Sect. 3.2 for details). We also assess the complementarity of the two evaluation techniques by averaging their softmax outputs. As can be seen, using multiple crops performs slightly better than dense evaluation, and the two approaches are indeed complementary, as their combination outperforms each of them. As noted above, we hypothesize that this is due to a different treatment of convolution boundary conditions.</p><p>在表5中，我们将密集的ConvNet评估与多裁切图像评估进行比较（详情请参见3.2节）。 我们还通过softmax输出均值来评估两种评估技术的互补性。 可以看出，使用多裁切图像的表现略好于密集评估，而且这两种方法确实是互补的，因为它们的组合优于其中的每一种。 如上所述，我们假设这是由于对卷积边界条件的不同处理。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-3338fa4d55820dc02a9c4dc97144fdda_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"919\" data-rawheight=\"240\" class=\"origin_image zh-lightbox-thumb\" width=\"919\" data-original=\"https://pic3.zhimg.com/v2-3338fa4d55820dc02a9c4dc97144fdda_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;919&#39; height=&#39;240&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"919\" data-rawheight=\"240\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"919\" data-original=\"https://pic3.zhimg.com/v2-3338fa4d55820dc02a9c4dc97144fdda_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-3338fa4d55820dc02a9c4dc97144fdda_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>4.4 CONVNET FUSION</p><p>4.4 卷积网络融合</p><p>Up until now, we evaluated the performance of individual ConvNet models. In this part of the experiments, we combine the outputs of several models by averaging their soft-max class posteriors. This improves the performance due to complementarity of the models, and was used in the top ILSVRC submissions in 2012 (Krizhevsky et al., 2012) and 2013 (Zeiler &amp; Fergus, 2013; Sermanet et al., 2014).</p><p>到目前为止，我们评估了单个ConvNet模型的性能。 在这部分实验中，我们通过模型的softmax分类后验概率均值来合并几个模型的输出。 由于模型的互补性，这提高了性能，这种方法在2012年（Krizhevsky等，2012）和2013年（Zeiler＆Fergus，2013; Sermanet等，2014）的最优ILSVRC提交中使用过。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>The results are shown in Table 6. By the time of ILSVRC submission we had only trained the single-scale networks, as well as a multi-scale model D (by fine-tuning only the fully-connected layers rather than all layers). The resulting ensemble of 7 networks has 7.3% ILSVRC test error. After the submission, we considered an ensemble of only two best-performing multi-scale models (configurations D and E), which reduced the test error to 7.0% using dense evaluation and 6.8% using combined dense and multi-crop evaluation. For reference, our best-performing single model achieves 7.1% error (model E, Table 5).</p><p>结果如表6所示。到ILSVRC提交时，我们只训练单尺度网络以及多尺度模型D（通过仅对完全连接层而不是所有层进行微调）。 由此产生的7个网络的融合模型具有7.3％的ILSVRC测试错误率。 提交之后，我们考虑了只有两个表现最好的多尺度模型（配置D和E）的集合，它使用密集评估将测试错误率降低到7.0％，而使用组合密集和多裁切图像评估将测试错误率则降低到6.8％。 作为参考，我们表现最佳的单模型实现了7.1％的误差（模型E，表5）。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-f1dce194bdcc3695fa78334ed03f52ff_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"972\" data-rawheight=\"288\" class=\"origin_image zh-lightbox-thumb\" width=\"972\" data-original=\"https://pic4.zhimg.com/v2-f1dce194bdcc3695fa78334ed03f52ff_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;972&#39; height=&#39;288&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"972\" data-rawheight=\"288\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"972\" data-original=\"https://pic4.zhimg.com/v2-f1dce194bdcc3695fa78334ed03f52ff_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-f1dce194bdcc3695fa78334ed03f52ff_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>4.5 COMPARISON WITH THE STATE OF THE ART</p><p>4.5 与当前最先进的技术相比较</p><p>Finally, we compare our results with the state of the art in Table 7. In the classification task of ILSVRC-2014 challenge (Russakovsky et al., 2014), our “VGG” team secured the 2nd place with 7.3% test error using an ensemble of 7 models. After the submission, we decreased the error rate to 6.8% using an ensemble of 2 models.</p><p>最后，我们将我们的结果与表7中的最新技术进行比较。在ILSVRC-2014挑战赛（Russakovsky等，2014）的分类任务中，我们的“VGG”团队以7.3％的测试错误率获得第二名，使用 7个模型融合。 提交后，我们使用2个模型的集合将错误率降低到6.8％。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>As can be seen from Table 7, our very deep ConvNets significantly outperform the previous generation of models, which achieved the best results in the ILSVRC-2012 and ILSVRC-2013 competitions. Our result is also competitive with respect to the classification task winner (GoogLeNet with 6.7% error) and substantially outperforms the ILSVRC-2013 winning submission Clarifai, which achieved 11.2% with outside training data and 11.7% without it. This is remarkable, considering that our best result is achieved by combining just two models – significantly less than used in most ILSVRC submissions. In terms of the single-net performance, our architecture achieves the best result (7.0% test error), outperforming a single GoogLeNet by 0.9%. Notably, we did not depart from the classical ConvNet architecture of LeCun et al. (1989), but improved it by substantially increasing the depth.</p><p>从表7可以看出，我们的深层卷积网络显著地超过了在ILSVRC-2012和ILSVRC-2013比赛中取得了最好的成绩的上一代模型。 我们的结果也对分类任务获胜者（GoogLeNet有6.7％的错误率）具有竞争力，并且大大优于ILSVRC-2013的获奖提交Clarifai，其使用外部训练数据达到11.2％，而不使用外部数据则为11.7％。 考虑到我们的最佳结果仅仅是通过两个模型融合来实现的，并显著地低于大多数ILSVRC提交中的使用的模型，这相当不简单。 就单网性能而言，我们的架构实现了最好的结果（7.0％的测试错误），优于单个GoogLeNet 0.9％。 值得注意的是，我们并没有偏离LeCun等人的经典ConvNet架构（1989），但通过大幅增加深度来改善它。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-572a91288b0e56773e4080244d1e178c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"964\" data-rawheight=\"466\" class=\"origin_image zh-lightbox-thumb\" width=\"964\" data-original=\"https://pic1.zhimg.com/v2-572a91288b0e56773e4080244d1e178c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;964&#39; height=&#39;466&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"964\" data-rawheight=\"466\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"964\" data-original=\"https://pic1.zhimg.com/v2-572a91288b0e56773e4080244d1e178c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-572a91288b0e56773e4080244d1e178c_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>5 CONCLUSION</p><p>5 结论</p><p>In this work we evaluated very deep convolutional networks (up to 19 weight layers) for large-scale image classification. It was demonstrated that the representation depth is beneficial for the classification accuracy, and that state-of-the-art performance on the ImageNet challenge dataset can be achieved using a conventional ConvNet architecture (LeCun et al., 1989; Krizhevsky et al., 2012) with substantially increased depth. In the appendix, we also show that our models generalise well to a wide range of tasks and datasets, matching or outperforming more complex recognition pipelines built around less deep image representations. Our results yet again confirm the importance of depth in visual representations.</p><p>在这项工作中，我们评估了用于大规模图像分类的深层卷积网络（多达19个权值层）。 已经证明，表示层的深度有利于分类准确性，并且通过大幅增加网络深度便可以使用传统的ConvNet架构来实现ImageNet挑战数据集上的最新性能（LeCun等，1989; Krizhevsky等， 2012）。 在附录中，我们还展示了我们的模型能很好地泛化应用于其他的任务和数据集，不亚于甚至性能优于那些深度略浅、更复杂的识别流水线。 我们的结果再一次证实了视觉表示中深度的重要性。</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>参考文献（略）</p><p class=\"ztext-empty-paragraph\"><br/></p><p>附录（略）</p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "计算机视觉", 
                    "tagLink": "https://api.zhihu.com/topics/19590195"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/34851133", 
            "userName": "黄伟亮", 
            "userLink": "https://www.zhihu.com/people/294306d6706f830e3cce35fb4acf46cb", 
            "upvote": 7, 
            "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition (VGG模型论文译文，上)", 
            "content": "<p>版权声明：</p><p>该论文原文出处地址为<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1409.1556\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">[1409.1556] Very Deep Convolutional Networks for Large-Scale Image Recognition</a>；</p><p>标题大图来源地址为<a href=\"https://link.zhihu.com/?target=https%3A//www.cs.toronto.edu/~frossard/post/vgg16/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">VGG in TensorFlow</a>；</p><p>本翻译仅供学习用途，并未经原作者授权，因此任何转载所造成的侵权行为均有可能受到追责。我们是一群机器学习爱好者，如果你也有兴趣与我们共同翻译原始论文，欢迎加入！详情请邮件联系kevinbauer@163.com。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>VGG模型论文探讨并证明了以下观点：</p><ol><li>用多层的卷积层组合配以小尺寸的滤波器，能实现大尺寸滤波器的感受野的同时，还能使参数数量更少；</li><li>表示层深度的增加，能有效提升网络的性能；</li><li>模型融合的性能优于单模型的性能；</li><li>训练期间分阶段降低学习率有助模型收敛；</li></ol><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>正文（上）</p><p>ABSTRACT</p><p>摘要</p><p>In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16–19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.</p><p>在本文中，我们研究了大规模图像识别任务下卷积网络深度对其预测准确率的影响。 我们的主要贡献是使用具有非常小的（3×3）卷积滤波器的架构对深度不断递增的网络进行全面评估，结果表明通过将权重层深度推到16-19层可以在现有技术配置下（使准确率）实现显著提升。 这些发现是我们在ImageNet  2014挑战赛的提交基础，我们的团队分别获得了图像定位项目第一名和分类跟踪项目第二名。 我们还表明，我们的表示层能很好地泛化于其他数据集，并从中获得最新的成果。 我们已经公开发布了两款性能最佳的ConvNet模型，以便于在计算机视觉中、深度视觉表示的使用上作进一步研究。</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>1 INTRODUCTION</p><p>1 介绍</p><p>Convolutional networks (ConvNets) have recently enjoyed a great success in large-scale image and video recognition which has become possible due to the large public image repositories, such as ImageNet (Deng et al., 2009), and high-performance computing systems, such as GPUs or large-scale distributed clusters (Dean et al., 2012). In particular, an important role in the advance of deep visual recognition architectures has been played by the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) (Russakovsky et al., 2014), which has served as a testbed for a few generations of large-scale image classification systems, from high-dimensional shallow feature encodings (Perronnin et al., 2010) (the winner of ILSVRC-2011) to deep ConvNets (Krizhevsky et al.,2012) (the winner of ILSVRC-2012).</p><p>由于有了大型公共图像库（如ImageNet）和高性能计算系统（如GPU或大规模分布式集群），卷积网络（ConvNets）最近在大规模图像识别和视频识别方面取得了巨大成功。 特别是，ImageNet大规模视觉识别挑战赛（ILSVRC）已经在深度视觉识别架构的发展中发挥了重要作用，它已经成为几代大规模图像分类系统的测试平台，从高维浅层特征编码到深度卷积网络。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>With ConvNets becoming more of a commodity in the computer vision field, a number of attempts have been made to improve the original architecture of Krizhevsky et al. (2012) in a bid to achieve better accuracy. For instance, the best-performing submissions to the ILSVRC-2013 (Zeiler &amp; Fergus, 2013; Sermanet et al., 2014) utilised smaller receptive window size and smaller stride of the first convolutional layer. Another line of improvements dealt with training and testing the networks densely over the whole image and over multiple scales (Sermanet et al.,2014; Howard, 2014). In this paper, we address another important aspect of ConvNet architecture design – its depth. To this end, we fix other parameters of the architecture, and steadily increase the depth of the network by adding more convolutional layers, which is feasible due to the use of very small (3 × 3) convolution filters in all layers.</p><p>随着卷积网络在计算机视觉领域逐渐成为一种商品，为了实现更高的准确性，业界已经在改进Krizhevsky等人的所提出原始架构上做出了许多尝试。 例如，ILSVRC-2013的最优提交结果采用较小尺寸的感受野窗口，以及在第一卷积层使用较小的步幅。 另一系列改进涉及在整个图像和多个尺度上密集训练和测试网络。 在本文中，我们解决了ConvNet架构设计的另一个重要方面 - 它的深度。 为此，我们修复了架构的其他参数，并通过添加更多卷积层来稳定增加网络深度，由于在所有层中都使用了非常小的（3×3）卷积滤波器，这是可行的。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>As a result, we come up with significantly more accurate ConvNet architectures, which not only achieve the state-of-the-art accuracy on ILSVRC classification and localisation tasks, but are also applicable to other image recognition datasets, where they achieve excellent performance even when used as a part of a relatively simple pipelines (e.g. deep features classified by a linear SVM without fine-tuning). We have released our two best-performing models 1 to facilitate further research. </p><p>因此，我们提出了更精确的ConvNet体系结构，它不仅刷新了ILSVRC分类任务和图像定位任务的准确率记录，它还适用于其他图像识别数据集，即使在作为相对简单的管线的一部分使用时，它也能实现出色的性能（例如，不需要微调的线性SVM分类的深度特征）。 我们发布了两款性能最佳的模型1，以促进进一步的研究。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>The rest of the paper is organised as follows. In Sect. 2, we describe our ConvNet configurations. The details of the image classification training and evaluation are then presented in Sect. 3, and the configurations are compared on the ILSVRC classification task in Sect. 4. Sect. 5 concludes the paper. For completeness, we also describe and assess our ILSVRC-2014 object localisation system in Appendix A, and discuss the generalisation of very deep features to other datasets in Appendix B. Finally, Appendix C contains the list of major paper revisions.</p><p>本文的其余部分安排如下： 在第2部分，我们描述我们的ConvNet配置；图像分类训练和评估的细节将在第3部分中介绍；在第4部分，会基于ILSVRC分类任务对不同的模型配置进行比较；第5部分，会进行全面总结。 为了完整起见，我们还在附录A中描述和评估了ILSVRC-2014对象定位系统，在附录B中讨论将深度特征泛化到其他数据集的方法。最后，附录C包含主要论文修订版的列表。</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>2 ConvNet CONFIGURATIONS</p><p>2 卷积网络配置</p><p>To measure the improvement brought by the increased ConvNet depth in a fair setting, all our ConvNet layer configurations are designed using the same principles, inspired by Ciresan et al. (2011); Krizhevsky et al. (2012). In this section, we first describe a generic layout of our ConvNet configurations (Sect. 2.1) and then detail the specific configurations used in the evaluation (Sect. 2.2). Our design choices are then discussed and compared to the prior art in Sect. 2.3.</p><p>受Ciresan 与Krizhevsky 等人的启发，为了衡量ConvNet深度在公平环境中所带来的改进，我们所有的ConvNet层配置都采用相同的原则设计。在本部分中，我们首先描述我们的ConvNet配置的通用布局（第2.1节），然后详细说明评估中使用的具体配置（第2.2节）。 然后（第2.3节）讨论我们的设计选择，并与现有技术进行比较。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>2.1 ARCHITECTURE</p><p>2.1 架构</p><p>During training, the input to our ConvNets is a fixed-size 224 × 224 RGB image. The only preprocessing we do is subtracting the mean RGB value, computed on the training set, from each pixel. The image is passed through a stack of convolutional (conv.) layers, where we use filters with a very small receptive field: 3 × 3 (which is the smallest size to capture the notion of left/right, up/down, center). In one of the configurations we also utilise 1 × 1 convolution filters, which can be seen as a linear transformation of the input channels (followed by non-linearity). The convolution stride is fixed to 1 pixel; the spatial padding of conv. layer input is such that the spatial resolution is preserved after convolution, i.e. the padding is 1 pixel for 3 × 3 conv. layers. Spatial pooling is carried out by five max-pooling layers, which follow some of the conv. layers (not all the conv. layers are followed by max-pooling). Max-pooling is performed over a 2 × 2 pixel window, with stride 2.</p><p>在训练期间，我们ConvNets的输入是固定尺寸的224×224 RGB图像。我们所做的唯一预处理是从每个像素中减去在训练集上计算的RGB均值。图像通过一叠卷积层，我们使用了感受野非常小的卷积核：3×3（这是左/右，上/下，中心点概念可捕获的最小尺寸）。在其中一种配置中，我们还使用1×1卷积滤波器，这可以看作是输入通道的线性变换（随后是非线性）。卷积步长固定为1个像素；卷积层的空间填充是指使得在卷积操作后保留原空间的分辨率，比如使用3×3 卷积核，就填充1个像素。空间池化是由五个最大池化层完成的，每个池化层前面都会有若干个卷积层（并非所有的卷积层后都使用最大池化层）。 最大池化是以2×2像素窗口上执行，步幅为2。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>A stack of convolutional layers (which has a different depth in different architectures) is followed by three Fully-Connected (FC) layers: the first two have 4096 channels each, the third performs 1000-way ILSVRC classification and thus contains 1000 channels (one for each class). The final layer is the soft-max layer. The configuration of the fully connected layers is the same in all networks. </p><p>一堆卷积层（在不同的体系结构中具有不同的深度）之后是三个完全连接（FC）层：前两个具有4096个通道，第三个执行1000路ILSVRC分类，因此包含1000个通道（一个 为每个类）。 最后一层是soft-max层。 全连接层的配置在所有网络中都是相同的。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>All hidden layers are equipped with the rectification (ReLU (Krizhevsky et al., 2012)) non-linearity. We note that none of our networks (except for one) contain Local Response Normalisation (LRN) normalisation (Krizhevsky et al., 2012): as will be shown in Sect. 4, such normalisation does not improve the performance on the ILSVRC dataset, but leads to increased memory consumption and computation time. Where applicable, the parameters for the LRN layer are those of (Krizhevsky et al., 2012).</p><p>所有隐藏层都配备了ReLU激活函数。 我们注意到我们的网络（除了一个网络）都没有包含局部响应归一化层（LRN）标准化（Krizhevsky et al。，2012）。 如第4部分所示，这种标准化不会提升ILSVRC数据集的性能，但会导致内存消耗和计算时间的增加。 在使用的情况下，LRN层的参数是（Krizhevsky et al。，2012）的参数。</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>2.2 CONFIGURATIONS</p><p>2.2 配置</p><p>The ConvNet configurations, evaluated in this paper, are outlined in Table 1, one per column. In the following we will refer to the nets by their names (A–E). All configurations follow the generic design presented in Sect. 2.1, and differ only in the depth: from 11 weight layers in the network A (8 conv. and 3 FC layers) to 19 weight layers in the network E (16 conv. and 3 FC layers). The width of conv. layers (the number of channels) is rather small, starting from 64 in the first layer and then increasing by a factor of 2 after each max-pooling layer, until it reaches 512. In Table 2 we report the number of parameters for each configuration. In spite of a large depth, the number of weights in our nets is not greater than the number of weights in a more shallow net with larger conv. layer widths and receptive fields (144M weights in (Sermanet et al., 2014)).</p><p>本文中评估的ConvNet配置在表1中列出，每列一个。 下面我们将以他们的名字（A-E）来提及。 所有的配置都遵循2.1节中提到的通用设计，并且仅在深度上有所不同：从网络A中的11个权重层（8个卷积层和3个全连接层）到网络E中的19个权重层（16个卷积层和3个全连接层）。卷积层的宽度（通道数量）相当小，从第一层的64开始，然后在每个最大池层后增加1倍，直到达到512。在表2中，我们报告了每个配置的参数数目。 尽管深度很大，但我们网络的权重数量不会超过那些深度较浅、但卷积核和感受野宽度更大的网络。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-7a0f9391fbdab0f5c3e8c61693ebe205_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"635\" data-rawheight=\"637\" class=\"origin_image zh-lightbox-thumb\" width=\"635\" data-original=\"https://pic2.zhimg.com/v2-7a0f9391fbdab0f5c3e8c61693ebe205_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;635&#39; height=&#39;637&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"635\" data-rawheight=\"637\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"635\" data-original=\"https://pic2.zhimg.com/v2-7a0f9391fbdab0f5c3e8c61693ebe205_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-7a0f9391fbdab0f5c3e8c61693ebe205_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>2.3 DISCUSSION</p><p>2.3 讨论</p><p>Our ConvNet configurations are quite different from the ones used in the top-performing entries of the ILSVRC-2012 (Krizhevsky et al., 2012) and ILSVRC-2013 competitions (Zeiler &amp; Fergus, 2013; Sermanet et al., 2014). Rather than using relatively large receptive fields in the first conv. layers (e.g. 11 × 11 with stride 4 in (Krizhevsky et al., 2012), or 7 × 7 with stride 2 in (Zeiler &amp; Fergus, 2013; Sermanet et al., 2014)), we use very small 3 × 3 receptive fields throughout the whole net, which are convolved with the input at every pixel (with stride 1). It is easy to see that a stack of two 3 × 3 conv. layers (without spatial pooling in between) has an effective receptive field of 5 × 5; three such layers have a 7 × 7 effective receptive field. So what have we gained by using, for instance, a stack of three 3 × 3 conv. layers instead of a single 7 × 7 layer? First, we incorporate three non-linear rectification layers instead of a single one, which makes the decision function more discriminative. Second, we decrease the number of parameters: assuming that both the input and the output of a three-layer 3 × 3 convolution stack has C channels, the stack is parametrised by  <img src=\"https://www.zhihu.com/equation?tex=3%5Ctimes%283%5E2C%5E2%EF%BC%89%3D+27C%5E2\" alt=\"3\\times(3^2C^2）= 27C^2\" eeimg=\"1\"/> weights; at the same time, a single 7 × 7 conv. layer would require <img src=\"https://www.zhihu.com/equation?tex=7%5E2C%5E2+%3D+49C%5E2\" alt=\"7^2C^2 = 49C^2\" eeimg=\"1\"/> parameters, i.e. 81% more. This can be seen as imposing a regularisation on the 7 × 7 conv. filters, forcing them to have a decomposition through the 3 × 3 filters (with non-linearity injected in between).</p><p>我们的ConvNet配置与 ILSVRC-2012以及 ILSVRC-2013的最佳模型相比有着较大差别。区别于之前的模型在首层卷积层所使用的较大的感受野（比如11x11卷积核配合步幅4（Krizhevsky et al., 2012），7x7卷积核配合步幅2（Zeiler &amp; Fergus, 2013; Sermanet et al., 2014）），我们整体都使用了非常小的3x3卷积核配合步幅1。显而易见的是，用两层的3x3卷积层组合（中间不包含池化层）所得到的感受野相当于一层的5x5卷积层的感受野；而三层这样的卷积层组合所得到的感受野相当于一层的7x7卷积核的感受野。那么，如果我们用三层3x3的卷积层组合来代替一层7x7卷积层，我们会得到什么呢？首先，我们并入了三个ReLU激活函数，而不是一个，这使决策功能的分辨力更强。 其次，我们减少参数的数量：假设三层3×3卷积层相叠的输入和输出都具有C个通道，则该叠层参数化为 <img src=\"https://www.zhihu.com/equation?tex=3%5Ctimes%283%5E2C%5E2%EF%BC%89%3D+27C%5E2\" alt=\"3\\times(3^2C^2）= 27C^2\" eeimg=\"1\"/> 个权重; 同时，一个7×7 卷积层需要 <img src=\"https://www.zhihu.com/equation?tex=7%5E2C%5E2+%3D+49C%5E2\" alt=\"7^2C^2 = 49C^2\" eeimg=\"1\"/> 参数，参数增加81％。 这可以被看作是在7×7卷积中实施正规化， 迫使他们通过3×3卷积核进行分解（两者之间注入非线性）。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>The incorporation of 1 × 1 conv. layers (configuration C, Table 1) is a way to increase the non-linearity of the decision function without affecting the receptive fields of the conv. layers. Even though in our case the 1 × 1 convolution is essentially a linear projection onto the space of the same dimensionality (the number of input and output channels is the same), an additional non-linearity is introduced by the rectification function. It should be noted that 1 × 1 conv. layers have recently been utilised in the “Network in Network” architecture of Lin et al. (2014).</p><p>纳入1×1卷积核（配置C，表1）是一种增加决策函数的非线性而不影响卷积层感受野的方法。 尽管在我们的例子中，1×1卷积本质上是一个线性投影到相同维度的空间上（输入和输出通道的数目是相同的），但激活函数引入了一个额外的非线性。 应该指出，最近Lin等人在其“网络中的网络”体系结构中使用了这种1×1卷积层（2014）。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>Small-size convolution filters have been previously used by Ciresan et al. (2011), but their nets are significantly less deep than ours, and they did not evaluate on the large-scale ILSVRC dataset. Goodfellow et al. (2014) applied deep ConvNets (11 weight layers) to the task of street number recognition, and showed that the increased depth led to better performance. GoogLeNet (Szegedy et al., 2014), a top-performing entry of the ILSVRC-2014 classification task, was developed independently of our work, but is similar in that it is based on very deep ConvNets (22 weight layers) and small convolution filters (apart from 3 × 3, they also use 1 × 1 and 5 × 5 convolutions). Their network topology is, however, more complex than ours, and the spatial resolution of the feature maps is reduced more aggressively in the first layers to decrease the amount of computation. As will be shown in Sect. 4.5, our model is outperforming that of Szegedy et al. (2014) in terms of the single-network classification accuracy.</p><p>Ciresan等人以前曾使用过小尺寸的卷积滤波器 （2011年），但他们的网络明显不如我们的深，并且他们没有对大规模ILSVRC数据集进行评估。 Goodfellow等人（2014）将深度ConvNets（11个权重层）应用于街道号识别任务，并表明增加深度能获得更好的性能。 GoogLeNet（Szegedy et al.，2014）是ILSVRC-2014分类任务中性能最好的一个入门版本，它的开发与我们的工作无关，但它的基础是非常深的ConvNets（22个加权层）和小卷积滤波器（除3×3外，还使用1×1和5×5卷积）。但是，它们的网络拓扑结构比我们的要复杂，并且在第一层中，特征映射的空间分辨率会更加激进地降低以减少计算量。正如将在4.5节中所显示的那样，我们的模型超过了Szegedy等人的模型（2014年）的单网分类准确性。</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>3 CLASSIFICATION FRAMEWORK</p><p>3 分类框架</p><p>In the previous section we presented the details of our network configurations. In this section, we describe the details of classification ConvNet training and evaluation.</p><p>在上一节中，我们介绍了网络配置的细节。 在本节中，我们将介绍ConvNet培训和评估的分类细节。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>3.1 TRAINING</p><p>3.1 训练</p><p>The ConvNet training procedure generally follows Krizhevsky et al. (2012) (except for sampling the input crops from multi-scale training images, as explained later). Namely, the training is carried out by optimising the multinomial logistic regression objective using mini-batch gradient descent (based on back-propagation (LeCun et al., 1989)) with momentum. The batch size was set to 256, momentum to 0.9. The training was regularised by weight decay (the L2 penalty multiplier set to <img src=\"https://www.zhihu.com/equation?tex=5+%C2%B7+10%5E%7B%E2%88%924%7D\" alt=\"5 · 10^{−4}\" eeimg=\"1\"/> ) and dropout regularisation for the first two fully-connected layers (dropout ratio set to 0.5). The learning rate was initially set to  <img src=\"https://www.zhihu.com/equation?tex=10%5E%7B%E2%88%922%7D\" alt=\"10^{−2}\" eeimg=\"1\"/>  , and then decreased by a factor of 10 when the validation set accuracy stopped improving. In total, the learning rate was decreased 3 times, and the learning was stopped after 370K iterations (74 epochs). We conjecture that in spite of the larger number of parameters and the greater depth of our nets compared to (Krizhevsky et al., 2012), the nets required less epochs to converge due to (a) implicit regularisation imposed by greater depth and smaller conv. filter sizes; (b) pre-initialisation of certain layers.</p><p>ConvNet的训练过程基本上参照Krizhevsky等人（2012）（除了从多尺度训练图像中采集输入裁剪图像，如后文所述）。也就是说，训练是通过使用小批量梯度下降（基于反向传播（LeCun et al。，1989））的动量优化多项逻辑回归目标来实现的。 批量大小设置为256，动量为0.9。 训练通过权值衰减（L2惩罚系数设置为 <img src=\"https://www.zhihu.com/equation?tex=5+%C2%B7+10%5E%7B%E2%88%924%7D\" alt=\"5 · 10^{−4}\" eeimg=\"1\"/> ）和前两个完全连接层（dropout设置为0.5）的dropout正则化来调整。 学习率最初设置为 <img src=\"https://www.zhihu.com/equation?tex=10%5E%7B%E2%88%922%7D\" alt=\"10^{−2}\" eeimg=\"1\"/> ，然后在验证集精度停止改进时再降低10倍。 总的来说，学习率一共降低了3次，并且在370K个迭代（74代）后停止了学习。 我们推测，尽管与（Krizhevsky et al.，2012）相比，网络的参数数量更多，网络深度也更大，但能用更少的迭代次数来实现收敛，由于：（a）更大深度和更小卷积核所带来的隐式正则化；（b）某些图层的预初始化。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>The initialisation of the network weights is important, since bad initialisation can stall learning due to the instability of gradient in deep nets. To circumvent this problem, we began with training the configuration A (Table 1), shallow enough to be trained with random initialisation. Then, when training deeper architectures, we initialised the first four convolutional layers and the last three fully-connected layers with the layers of net A (the intermediate layers were initialised randomly). We did not decrease the learning rate for the pre-initialised layers, allowing them to change during learning. For random initialisation (where applicable), we sampled the weights from a normal distribution with the zero mean and  <img src=\"https://www.zhihu.com/equation?tex=10%5E%7B%E2%88%922%7D\" alt=\"10^{−2}\" eeimg=\"1\"/>  variance. The biases were initialised with zero. It is worth noting that after the paper submission we found that it is possible to initialise the weights without pre-training by using the random initialisation procedure of Glorot &amp; Bengio (2010).</p><p>网络权重的初始化很重要，因为由于深度网络中的梯度不稳定，初始化不好可能会导致学习停滞。 为了避免这个问题，我们从训练配置A（表1）开始，这个网络足够浅，可以随机初始化进行训练。 然后，当训练更深的体系结构时，我们使用了网络A的权值来初始化了前四个卷积层和最后三个完全连接的层，（中间层随机初始化）。 我们没有降低预初始化图层的学习速率，允许它们在学习期间改变。 对于随机初始化（如有），我们从具有零均值和 <img src=\"https://www.zhihu.com/equation?tex=10%5E%7B%E2%88%922%7D\" alt=\"10^{−2}\" eeimg=\"1\"/> 方差的正态分布采样权重。 偏差初始化为零。 值得注意的是，在提交论文后，我们发现可以使用Glorot＆Bengio（2010）的随机初始化程序在没有预先训练的情况下初始化权重。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>To obtain the fixed-size 224×224 ConvNet input images, they were randomly cropped from rescaled training images (one crop per image per SGD iteration). To further augment the training set, the crops underwent random horizontal flipping and random RGB colour shift (Krizhevsky et al., 2012). Training image rescaling is explained below.</p><p>为了获得224×224固定大小的 ConvNet输入图像，他们从重新缩放的训练图像中随机裁剪（每个SGD迭代每个图像裁剪一次）。 为了进一步增强训练集，被裁剪的图像经过随机水平翻转和随机RGB颜色偏移处理（Krizhevsky et al.，2012）。 下面将介绍训练图像缩放。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>Training image size.</b> Let S be the smallest side of an isotropically-rescaled training image, from which the ConvNet input is cropped (we also refer to S as the training scale). While the crop size is fixed to 224 × 224, in principle S can take on any value not less than 224: for S = 224 the crop will capture whole-image statistics, completely spanning the smallest side of a training image; for S ≫ 224 the crop will correspond to a small part of the image, containing a small object or an object part.</p><p><b>训练图像尺寸。</b> 设S是等比例缩放的训练图像的最小边，ConvNet基于这些图像的裁剪作为输入（我们也称S为训练尺度）。 虽然裁剪大小固定为224×224，但原则上S可以取不小于224的任何值：对于S = 224，裁剪图将捕获整幅图像统计数据，完全跨越训练图像的最小边; 对于S&gt;&gt;224，裁剪图将对应于图像的一小部分，包含一个小物体或一个物体部分。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>We consider two approaches for setting the training scale S. The first is to fix S, which corresponds to single-scale training (note that image content within the sampled crops can still represent multi-scale image statistics). In our experiments, we evaluated models trained at two fixed scales: S = 256 (which has been widely used in the prior art (Krizhevsky et al., 2012; Zeiler &amp; Fergus, 2013; Sermanet et al., 2014)) and S = 384. Given a ConvNet configuration, we first trained the network using S = 256. To speed-up training of the S = 384 network, it was initialised with the weights pretrained with S = 256, and we used a smaller initial learning rate of <img src=\"https://www.zhihu.com/equation?tex=10%5E%7B%E2%88%923%7D\" alt=\"10^{−3}\" eeimg=\"1\"/> .</p><p>我们考虑设定训练尺度S的两种方法。第一种方法是固定S，这对应于单尺度训练（注意采样作物中的图像内容仍然可以表示多尺度图像统计）。 在我们的实验中，我们评估了以两个固定尺度训练的模型：S = 256（已被广泛用于现有技术（Krizhevsky等，2012; Zeiler＆Fergus，2013; Sermanet等，2014））和S = 384。给定一个ConvNet配置，我们首先使用S = 256来训练网络。为了加速S = 384网络的训练，它被初始化为具有S = 256的预训练权重，并且我们使用较小的学习率初始值为 <img src=\"https://www.zhihu.com/equation?tex=10%5E%7B%E2%88%923%7D\" alt=\"10^{−3}\" eeimg=\"1\"/> 。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>The second approach to setting S is multi-scale training, where each training image is individually rescaled by randomly sampling S from a certain range [Smin , Smax ] (we used Smin = 256 and Smax = 512). Since objects in images can be of different size, it is beneficial to take this into account during training. This can also be seen as training set augmentation by scale jittering, where a single model is trained to recognise objects over a wide range of scales. For speed reasons, we trained multi-scale models by fine-tuning all layers of a single-scale model with the same configuration, pre-trained with fixed S = 384.</p><p>设定S的第二种方法是多尺度训练，其中通过从特定范围[Smin，Smax]（我们使用Smin = 256和Smax = 512）随机采样S来单独重新调整每个训练图像。 由于图像中的物体可能具有不同的大小，因此在训练时考虑到这一点是有益的。 这也可以看作是通过缩放抖动来增强训练集，其中单个模型被训练以识别多种类别的物体。 出于速度的原因，我们通过对具有相同配置的单尺度模型的所有层进行微调来训练多尺度模型，并使用固定的S = 384进行预训练。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>3.2 TESTING</p><p>3.2 测试</p><p>At test time, given a trained ConvNet and an input image, it is classified in the following way. First, it is isotropically rescaled to a pre-defined smallest image side, denoted as Q (we also refer to it as the test scale). We note that Q is not necessarily equal to the training scale S (as we will show in Sect. 4, using several values of Q for each S leads to improved performance). Then, the network is applied densely over the rescaled test image in a way similar to (Sermanet et al., 2014). Namely, the fully-connected layers are first converted to convolutional layers (the first FC layer to a 7 × 7 conv. layer, the last two FC layers to 1 × 1 conv. layers). The resulting fully-convolutional net is then applied to the whole (uncropped) image. The result is a class score map with the number of channels equal to the number of classes, and a variable spatial resolution, dependent on the input image size. Finally, to obtain a fixed-size vector of class scores for the image, the class score map is spatially averaged (sum-pooled). We also augment the test set by horizontal flipping of the images; the softmax class posteriors of the original and flipped images are averaged to obtain the final scores for the image.</p><p>在测试时，给定一个训练有素的ConvNet和一个输入图像，它按以下方式分类。首先，将其等比例缩放到预定义的最小边，表示为Q（我们也将其称为测试尺度）。我们注意到，Q不一定等于训练尺度S（如我们将在第4部分中所示，对每个S使用几个Q值可获得性能改进）。然后，网络以类似于（Sermanet等人，2014）的方式被密集地应用在重新缩放的测试图像上。也就是说，完全连接的层首先被转换成卷积层（第一个FC层转为7×7的卷积层，后两个FC层转为1×1 卷积层）。然后将所得的全卷积网络应用于整个（未裁剪的）图像。其结果是一个类别得分映射，其类别数等于任务的目标分类数，以及一个可变的空间分辨率，取决于输入图像的大小。最后，为了获得固定大小的图像类别分数的向量，类别得分映射会被空间平均（加总池化）。我们还通过水平翻转图像来增强测试集；对原始图像和翻转图像的softmax分类概率进行平均以获得图像的最终分数。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>Since the fully-convolutional network is applied over the whole image, there is no need to sample multiple crops at test time (Krizhevsky et al., 2012), which is less efficient as it requires network re-computation for each crop. At the same time, using a large set of crops, as done by Szegedy et al. (2014), can lead to improved accuracy, as it results in a finer sampling of the input image compared to the fully-convolutional net. Also, multi-crop evaluation is complementary to dense evaluation due to different convolution boundary conditions: when applying a ConvNet to a crop, the convolved feature maps are padded with zeros, while in the case of dense evaluation the padding for the same crop naturally comes from the neighbouring parts of an image (due to both the convolutions and spatial pooling), which substantially increases the overall network receptive field, so more context is captured. While we believe that in practice the increased computation time of multiple crops does not justify the potential gains in accuracy, for reference we also evaluate our networks using 50 crops per scale (5 × 5 regular grid with 2 flips), for a total of 150 crops over 3 scales, which is comparable to 144 crops over 4 scales used by Szegedy et al. (2014).</p><p>由于全卷积网络应用于整个图像，因此不需要在测试时间对多个裁剪图像进行采样（Krizhevsky et al。，2012），这样效率较低，因为它需要网络对每个裁剪图像进行重新计算。同时，使用大量的裁剪图像，如Szegedy等人（2014）所做的那样可以提高准确性，因为与全卷积网络相比，它可以更精细地对输入图像进行采样。此外，由于卷积边界条件不同，多裁剪图像评估与密集评估是互补的：将ConvNet应用于裁剪图像时，卷积后的特征映射用零填充，而在密集评估的情况下，同一裁切图像的填充天然地来自于图像的相邻部分（由于卷积和空间池化），这大大增加了整个网络的感受野，因此捕获更多的上下文信息。尽管我们认为在实践中增加多裁切图像的计算时间并不能证明潜在的准确度增加，但我们对于每种尺寸规模（5×5个常规栅格和2种翻转）都使用50个裁切图像来评估我们的网络，总共150个裁切图像、超过3个尺度，这与Szegedy等人使用的4种尺度、144个裁切图像相当 （2014）。</p><p class=\"ztext-empty-paragraph\"><br/></p><p class=\"ztext-empty-paragraph\"><br/></p><p>3.3 IMPLEMENTATION DETAILS</p><p>3.3 实现细节</p><p>Our implementation is derived from the publicly available C++ Caffe toolbox (Jia, 2013) (branched out in December 2013), but contains a number of significant modifications, allowing us to perform training and evaluation on multiple GPUs installed in a single system, as well as train and evaluate on full-size (uncropped) images at multiple scales (as described above). Multi-GPU training exploits data parallelism, and is carried out by splitting each batch of training images into several GPU batches, processed in parallel on each GPU. After the GPU batch gradients are computed, they are averaged to obtain the gradient of the full batch. Gradient computation is synchronous across the GPUs, so the result is exactly the same as when training on a single GPU. While more sophisticated methods of speeding up ConvNet training have been recently proposed (Krizhevsky, 2014), which employ model and data parallelism for different layers of the net, we have found that our conceptually much simpler scheme already provides a speedup of 3.75 times on an off-the-shelf 4-GPU system, as compared to using a single GPU. On a system equipped with four NVIDIA Titan Black GPUs, training a single net took 2–3 weeks depending on the architecture.</p><p>我们的实现源自公开发布的C ++ Caffe工具箱（Jia，2013）（2013年12月推出），但包含许多重大修改，使得我们能在装有多个GPU的单系统中执行训练和评估，以及能够对多种规模的全尺寸（未裁剪）图像（如上所述）进行训练和评估。多GPU训练利用数据并行性，并且通过将每批训练图像分成几个GPU批次并在每个GPU上并行处理来执行。计算GPU批梯度后，计算它们的均值以获得完整批次的梯度。梯度计算在GPU中是同步的，因此结果与在单个GPU上进行训练时完全相同。尽管最近提出了更加复杂的加速ConvNet训练的方法（Krizhevsky，2014），它们针对网络的不同层使用模型和数据并行性，但我们发现，与使用单个GPU相比，我们概念更简单的方案在现成的4 GPU系统上已经提供了3.75倍的加速。在配备四个NVIDIA Titan Black GPU的系统上，根据架构的不同，训练一个网络需要2-3周的时间。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>下半部分链接：</p><a href=\"https://zhuanlan.zhihu.com/p/35516173\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-dfe4eaaa4450e2b58b38c5fe82f918c0_180x120.jpg\" data-image-width=\"470\" data-image-height=\"276\" class=\"internal\">kevinbauer：Very Deep Convolutional Networks for Large-Scale Image Recognition (VGG模型论文译文，下)</a><p></p><p></p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "计算机视觉", 
                    "tagLink": "https://api.zhihu.com/topics/19590195"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/36453041", 
            "userName": "黄伟亮", 
            "userLink": "https://www.zhihu.com/people/294306d6706f830e3cce35fb4acf46cb", 
            "upvote": 9, 
            "title": "DQN实战：MIT强化学习实战—Deep Traffic（下）", 
            "content": "<p><a href=\"https://zhuanlan.zhihu.com/p/36249826\" class=\"internal\">前文</a>已经对DeepTraffic的基本环境进行了一番分析与研究，本文主要着重研究不同的DQN结构与超参数对模型性能的影响。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>前文提到，一个默认的DQN训练并测试出来的成绩大概是51MPH。从左侧的道路模拟效果可以发现，导致成绩不提高的主要原因是当前方有车阻塞时，智能车不懂变道。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-89917745bb5a53f22a22ba2b4b61dc8e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"316\" data-rawheight=\"322\" class=\"content_image\" width=\"316\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;316&#39; height=&#39;322&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"316\" data-rawheight=\"322\" class=\"content_image lazy\" width=\"316\" data-actualsrc=\"https://pic3.zhimg.com/v2-89917745bb5a53f22a22ba2b4b61dc8e_b.jpg\"/></figure><p>因此，我们第一个目标就是要重新构造一个“会判断阻塞时自动变道”的智能车。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>如何让智能车“自动变道超车”？</b></h2><p>首先，我们需要优化智能车的“视野”。</p><div class=\"highlight\"><pre><code class=\"language-js\"><span class=\"nx\">lanesSide</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">;</span>\n<span class=\"nx\">patchesAhead</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">;</span>\n<span class=\"nx\">patchesBehind</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">;</span>\n</code></pre></div><p>由于默认的“视野”只有当前车道的车前一格距离，这会导致所有的状态值全部是一模一样，因此小车不会有任何行动。</p><div class=\"highlight\"><pre><code class=\"language-js\"><span class=\"nx\">learn</span> <span class=\"o\">=</span> <span class=\"kd\">function</span> <span class=\"p\">(</span><span class=\"nx\">state</span><span class=\"p\">,</span> <span class=\"nx\">lastReward</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n<span class=\"nx\">brain</span><span class=\"p\">.</span><span class=\"nx\">backward</span><span class=\"p\">(</span><span class=\"nx\">lastReward</span><span class=\"p\">);</span>\n<span class=\"kd\">var</span> <span class=\"nx\">action</span> <span class=\"o\">=</span> <span class=\"nx\">brain</span><span class=\"p\">.</span><span class=\"nx\">forward</span><span class=\"p\">(</span><span class=\"nx\">state</span><span class=\"p\">);</span>\n\n<span class=\"nx\">draw_net</span><span class=\"p\">();</span>\n<span class=\"nx\">draw_stats</span><span class=\"p\">();</span>\n<span class=\"nx\">console</span><span class=\"p\">.</span><span class=\"nx\">log</span><span class=\"p\">(</span><span class=\"nx\">action</span><span class=\"p\">);</span> <span class=\"c1\">//增加输出，观察行动值：\n</span><span class=\"c1\"></span><span class=\"k\">return</span> <span class=\"nx\">action</span><span class=\"p\">;</span>\n<span class=\"p\">}</span>\n</code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-d37ddf8a5c8a34855828404f93a3c087_b.jpg\" data-size=\"normal\" data-rawwidth=\"301\" data-rawheight=\"86\" class=\"content_image\" width=\"301\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;301&#39; height=&#39;86&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"301\" data-rawheight=\"86\" class=\"content_image lazy\" width=\"301\" data-actualsrc=\"https://pic4.zhimg.com/v2-d37ddf8a5c8a34855828404f93a3c087_b.jpg\"/><figcaption>行动值含义</figcaption></figure><p>从浏览器F12控制台可以看到，此时的行动值全部为0。</p><p>为此，我们先把前方“视野”改为5格。</p><div class=\"highlight\"><pre><code class=\"language-js\"><span class=\"nx\">lanesSide</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">;</span>\n<span class=\"nx\">patchesAhead</span> <span class=\"o\">=</span> <span class=\"mi\">5</span><span class=\"p\">;</span> <span class=\"c1\">//视野范围增加为5\n</span><span class=\"c1\"></span><span class=\"nx\">patchesBehind</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">;</span>\n</code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b431da6acc4b3b963a987786e8b73dfd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"191\" data-rawheight=\"217\" class=\"content_image\" width=\"191\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;191&#39; height=&#39;217&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"191\" data-rawheight=\"217\" class=\"content_image lazy\" width=\"191\" data-actualsrc=\"https://pic2.zhimg.com/v2-b431da6acc4b3b963a987786e8b73dfd_b.jpg\"/></figure><p>可以看到，当前方有车阻塞时，智能车（由于有安全系统，这个不需要我们理会）会自动保持三格的距离（有两格被占）。</p><p>另外，由于神经网络的隐藏层默认只有一个神经元，显然导致表达能力不足，会影响预测的性能。因此按照一般的MLP设置，把隐藏层的神经元数量增加到输入层的两倍。</p><div class=\"highlight\"><pre><code class=\"language-js\"><span class=\"kd\">var</span> <span class=\"nx\">layer_defs</span> <span class=\"o\">=</span> <span class=\"p\">[];</span>\n    <span class=\"nx\">layer_defs</span><span class=\"p\">.</span><span class=\"nx\">push</span><span class=\"p\">({</span>\n    <span class=\"nx\">type</span><span class=\"o\">:</span> <span class=\"s1\">&#39;input&#39;</span><span class=\"p\">,</span>\n    <span class=\"nx\">out_sx</span><span class=\"o\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span>\n    <span class=\"nx\">out_sy</span><span class=\"o\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span>\n    <span class=\"nx\">out_depth</span><span class=\"o\">:</span> <span class=\"nx\">network_size</span>\n<span class=\"p\">});</span>\n<span class=\"nx\">layer_defs</span><span class=\"p\">.</span><span class=\"nx\">push</span><span class=\"p\">({</span>\n    <span class=\"nx\">type</span><span class=\"o\">:</span> <span class=\"s1\">&#39;fc&#39;</span><span class=\"p\">,</span>\n    <span class=\"nx\">num_neurons</span><span class=\"o\">:</span> <span class=\"nx\">network_size</span><span class=\"o\">*</span><span class=\"mi\">2</span><span class=\"p\">,</span> <span class=\"c1\">// 改为输入层大小的两倍\n</span><span class=\"c1\"></span>    <span class=\"nx\">activation</span><span class=\"o\">:</span> <span class=\"s1\">&#39;relu&#39;</span>\n<span class=\"p\">});</span>\n<span class=\"nx\">layer_defs</span><span class=\"p\">.</span><span class=\"nx\">push</span><span class=\"p\">({</span>\n    <span class=\"nx\">type</span><span class=\"o\">:</span> <span class=\"s1\">&#39;regression&#39;</span><span class=\"p\">,</span>\n    <span class=\"nx\">num_neurons</span><span class=\"o\">:</span> <span class=\"nx\">num_actions</span>\n<span class=\"p\">});</span>\n</code></pre></div><p class=\"ztext-empty-paragraph\"><br/></p><p>修改后，结果得到了显著的提高。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-6e121bbd05fcbebc2c63911a34ae7b95_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"582\" data-rawheight=\"257\" class=\"origin_image zh-lightbox-thumb\" width=\"582\" data-original=\"https://pic2.zhimg.com/v2-6e121bbd05fcbebc2c63911a34ae7b95_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;582&#39; height=&#39;257&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"582\" data-rawheight=\"257\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"582\" data-original=\"https://pic2.zhimg.com/v2-6e121bbd05fcbebc2c63911a34ae7b95_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-6e121bbd05fcbebc2c63911a34ae7b95_b.jpg\"/></figure><p>此时，从左侧的模拟效果看，智能车已经可以自动变道超车了。然而，会有几个存在问题：</p><ol><li>智能车遇到前方有车时，需要停下来等一两秒才能“反应过来”要变道；</li><li>当智能车被多辆车“围困”时，不懂提前预判避开困境，或者不懂通过先减速再变道来摆脱困境。</li></ol><hr/><h2><b>如何让小车增加预判能力？</b></h2><p>简单地增加“视野”让智能车再看远一点能否有所帮助呢？</p><div class=\"highlight\"><pre><code class=\"language-js\"><span class=\"nx\">lanesSide</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">;</span>\n<span class=\"nx\">patchesAhead</span> <span class=\"o\">=</span> <span class=\"mi\">10</span><span class=\"p\">;</span> <span class=\"c1\">//把视野范围增加到10格\n</span><span class=\"c1\"></span><span class=\"nx\">patchesBehind</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">;</span>\n</code></pre></div><p>结果发现，没有显著地提高车速。而且从左侧的模拟效果看，智能车的行为基本上与之前没有变化。为什么呢？</p><p>试想下，当智能车前第10格有小车阻挡时，智能车选择无行动（即行动值为0，维持原速前进），与选择变道所得到的奖励是一样的。既然奖励是一样的，必然导致行动与之前不会有变化。</p><p>我尝试了多种超参数组合，包括把lanesSide改为1、调整网络隐藏层节点数、调整epsilon的衰减函数、总的训练次数等等。尝试过十几种参数组合，仍然无法让成绩有显著的提升（后来反思，根本原因是思路没有质的改变）。于是，我带着困惑尝试求助于google，结果找到了一个曾经的排行榜top-1解决方案 （参考资料[6]），<a href=\"https://link.zhihu.com/?target=https%3A//raw.githubusercontent.com/parilo/DeepTraffic-solution/master/solution.js\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">raw.githubusercontent.com</span><span class=\"invisible\">/parilo/DeepTraffic-solution/master/solution.js</span><span class=\"ellipsis\"></span></a>。</p><p>该方案在思路最大的突破在于它把temporal_window设置为0，然后在“视野”上的设置如下：</p><div class=\"highlight\"><pre><code class=\"language-js\"><span class=\"nx\">lanesSide</span> <span class=\"o\">=</span> <span class=\"mi\">3</span><span class=\"p\">;</span>\n<span class=\"nx\">patchesAhead</span> <span class=\"o\">=</span> <span class=\"mi\">50</span><span class=\"p\">;</span> \n<span class=\"nx\">patchesBehind</span> <span class=\"o\">=</span> <span class=\"mi\">10</span><span class=\"p\">;</span> \n</code></pre></div><p>在方案的说明里，作者提到该方案可以达到75MPH左右的成绩，并且一度排行榜首位置。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-d29c4b1031716c86117c126d1c3ad8ca_b.jpg\" data-size=\"normal\" data-rawwidth=\"1030\" data-rawheight=\"357\" class=\"origin_image zh-lightbox-thumb\" width=\"1030\" data-original=\"https://pic3.zhimg.com/v2-d29c4b1031716c86117c126d1c3ad8ca_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1030&#39; height=&#39;357&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1030\" data-rawheight=\"357\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1030\" data-original=\"https://pic3.zhimg.com/v2-d29c4b1031716c86117c126d1c3ad8ca_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-d29c4b1031716c86117c126d1c3ad8ca_b.jpg\"/><figcaption>资料来源：参考资料[7]</figcaption></figure><p>因此，我抱着学习的态度按源码重现了一番，结果如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ae52e1ca5f06620cc8159abf1a64dd14_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"593\" data-rawheight=\"271\" class=\"origin_image zh-lightbox-thumb\" width=\"593\" data-original=\"https://pic1.zhimg.com/v2-ae52e1ca5f06620cc8159abf1a64dd14_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;593&#39; height=&#39;271&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"593\" data-rawheight=\"271\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"593\" data-original=\"https://pic1.zhimg.com/v2-ae52e1ca5f06620cc8159abf1a64dd14_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-ae52e1ca5f06620cc8159abf1a64dd14_b.jpg\"/></figure><p>在训练过程中，由于看到奖励曲线与作者发布的视频中所看到的曲线完全不一样，智能车的行为也是相当“智障”，让我一度怀疑是不是代码有问题。后来我硬着头皮等了两个小时左右，总算看到智能车开始从“智障”变得“智能”了。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-cddb2f14fd75192f3ea16da7b153b32d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"365\" data-rawheight=\"124\" class=\"content_image\" width=\"365\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;365&#39; height=&#39;124&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"365\" data-rawheight=\"124\" class=\"content_image lazy\" width=\"365\" data-actualsrc=\"https://pic2.zhimg.com/v2-cddb2f14fd75192f3ea16da7b153b32d_b.jpg\"/></figure><p>整个训练过程一共大概花了五个多小时左右（有兴趣的朋友也可以自己试试）。</p><p>需要说明的是，以上的solution.js的保存代码以及保存模型不知道是平台版本问题，还是作者有所保留，笔者多次测试自己保存的训练模型，只要加载代码就能百分百重现之前的结果；而solution.js的模型加载后马上运行测试只有65MPH，并不能即便是重新训练模型后也只有70MPH左右，离作者所说的75MPH有明显的差距。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>原理的探索</b></p><p>为了验证temporal_window=0以及增加“视野”的设置对模型提升有显著效果，我用以下代码自己再实践了一遍：</p><div class=\"highlight\"><pre><code class=\"language-js\"><span class=\"nx\">lanesSide</span> <span class=\"o\">=</span> <span class=\"mi\">3</span><span class=\"p\">;</span>\n<span class=\"nx\">patchesAhead</span> <span class=\"o\">=</span> <span class=\"mi\">50</span><span class=\"p\">;</span>\n<span class=\"nx\">patchesBehind</span> <span class=\"o\">=</span> <span class=\"mi\">10</span><span class=\"p\">;</span>\n<span class=\"nx\">trainIterations</span> <span class=\"o\">=</span> <span class=\"mi\">50000</span><span class=\"p\">;</span> <span class=\"c1\">//训练次数只用了刚才的十分一\n</span><span class=\"c1\"></span>\n<span class=\"c1\">// the number of other autonomous vehicles controlled by your network\n</span><span class=\"c1\"></span><span class=\"nx\">otherAgents</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">;</span> <span class=\"c1\">// max of 10\n</span><span class=\"c1\"></span>\n<span class=\"kd\">var</span> <span class=\"nx\">num_inputs</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"nx\">lanesSide</span> <span class=\"o\">*</span> <span class=\"mi\">2</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"nx\">patchesAhead</span> <span class=\"o\">+</span> <span class=\"nx\">patchesBehind</span><span class=\"p\">);</span>\n<span class=\"kd\">var</span> <span class=\"nx\">num_actions</span> <span class=\"o\">=</span> <span class=\"mi\">5</span><span class=\"p\">;</span>\n<span class=\"kd\">var</span> <span class=\"nx\">temporal_window</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">;</span>\n<span class=\"kd\">var</span> <span class=\"nx\">network_size</span> <span class=\"o\">=</span> <span class=\"nx\">num_inputs</span> <span class=\"o\">*</span> <span class=\"nx\">temporal_window</span> <span class=\"o\">+</span> <span class=\"nx\">num_actions</span> <span class=\"o\">*</span> <span class=\"nx\">temporal_window</span> <span class=\"o\">+</span> <span class=\"nx\">num_inputs</span><span class=\"p\">;</span>\n\n<span class=\"kd\">var</span> <span class=\"nx\">layer_defs</span> <span class=\"o\">=</span> <span class=\"p\">[];</span>\n    <span class=\"nx\">layer_defs</span><span class=\"p\">.</span><span class=\"nx\">push</span><span class=\"p\">({</span>\n    <span class=\"nx\">type</span><span class=\"o\">:</span> <span class=\"s1\">&#39;input&#39;</span><span class=\"p\">,</span>\n    <span class=\"nx\">out_sx</span><span class=\"o\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span>\n    <span class=\"nx\">out_sy</span><span class=\"o\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span>\n    <span class=\"nx\">out_depth</span><span class=\"o\">:</span> <span class=\"nx\">network_size</span>\n<span class=\"p\">});</span>\n<span class=\"nx\">layer_defs</span><span class=\"p\">.</span><span class=\"nx\">push</span><span class=\"p\">({</span>\n    <span class=\"nx\">type</span><span class=\"o\">:</span> <span class=\"s1\">&#39;fc&#39;</span><span class=\"p\">,</span>\n    <span class=\"c1\">//为了验证主要的影响在于temporal_window与“视野”，\n</span><span class=\"c1\"></span>    <span class=\"c1\">//此时网络架构部分是随意设的，主要考虑因素是不要让节点太多以免训练时间太长\n</span><span class=\"c1\"></span>    <span class=\"nx\">num_neurons</span><span class=\"o\">:</span> <span class=\"nx\">network_size</span><span class=\"p\">,</span> \n    <span class=\"nx\">activation</span><span class=\"o\">:</span> <span class=\"s1\">&#39;tanh&#39;</span>\n<span class=\"p\">});</span>\n<span class=\"nx\">layer_defs</span><span class=\"p\">.</span><span class=\"nx\">push</span><span class=\"p\">({</span>\n    <span class=\"nx\">type</span><span class=\"o\">:</span> <span class=\"s1\">&#39;regression&#39;</span><span class=\"p\">,</span>\n    <span class=\"nx\">num_neurons</span><span class=\"o\">:</span> <span class=\"nx\">num_actions</span>\n<span class=\"p\">});</span>\n\n<span class=\"kd\">var</span> <span class=\"nx\">tdtrainer_options</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"nx\">learning_rate</span><span class=\"o\">:</span> <span class=\"mf\">0.0005</span><span class=\"p\">,</span>\n    <span class=\"nx\">momentum</span><span class=\"o\">:</span> <span class=\"mf\">0.0</span><span class=\"p\">,</span>\n    <span class=\"nx\">batch_size</span><span class=\"o\">:</span> <span class=\"mi\">64</span><span class=\"p\">,</span>\n    <span class=\"nx\">l2_decay</span><span class=\"o\">:</span> <span class=\"mf\">0.01</span>\n<span class=\"p\">};</span>\n\n<span class=\"kd\">var</span> <span class=\"nx\">opt</span> <span class=\"o\">=</span> <span class=\"p\">{};</span>\n<span class=\"nx\">opt</span><span class=\"p\">.</span><span class=\"nx\">temporal_window</span> <span class=\"o\">=</span> <span class=\"nx\">temporal_window</span><span class=\"p\">;</span>\n<span class=\"nx\">opt</span><span class=\"p\">.</span><span class=\"nx\">experience_size</span> <span class=\"o\">=</span> <span class=\"mi\">30000</span><span class=\"p\">;</span>\n<span class=\"nx\">opt</span><span class=\"p\">.</span><span class=\"nx\">start_learn_threshold</span> <span class=\"o\">=</span> <span class=\"mi\">5000</span><span class=\"p\">;</span>\n<span class=\"nx\">opt</span><span class=\"p\">.</span><span class=\"nx\">gamma</span> <span class=\"o\">=</span> <span class=\"mf\">0.9</span><span class=\"p\">;</span>\n<span class=\"nx\">opt</span><span class=\"p\">.</span><span class=\"nx\">learning_steps_total</span> <span class=\"o\">=</span> <span class=\"mi\">30000</span><span class=\"p\">;</span>\n<span class=\"nx\">opt</span><span class=\"p\">.</span><span class=\"nx\">learning_steps_burnin</span> <span class=\"o\">=</span> <span class=\"mi\">5000</span><span class=\"p\">;</span>\n<span class=\"nx\">opt</span><span class=\"p\">.</span><span class=\"nx\">epsilon_min</span> <span class=\"o\">=</span> <span class=\"mf\">0.1</span><span class=\"p\">;</span>\n<span class=\"nx\">opt</span><span class=\"p\">.</span><span class=\"nx\">epsilon_test_time</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span><span class=\"p\">;</span>\n<span class=\"nx\">opt</span><span class=\"p\">.</span><span class=\"nx\">layer_defs</span> <span class=\"o\">=</span> <span class=\"nx\">layer_defs</span><span class=\"p\">;</span>\n<span class=\"nx\">opt</span><span class=\"p\">.</span><span class=\"nx\">tdtrainer_options</span> <span class=\"o\">=</span> <span class=\"nx\">tdtrainer_options</span><span class=\"p\">;</span>\n\n<span class=\"c1\">//后面略\n</span></code></pre></div><p>得到的结果如下图：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-a3777192c0e1812e8ab7e4dedd0e53fc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"591\" data-rawheight=\"272\" class=\"origin_image zh-lightbox-thumb\" width=\"591\" data-original=\"https://pic1.zhimg.com/v2-a3777192c0e1812e8ab7e4dedd0e53fc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;591&#39; height=&#39;272&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"591\" data-rawheight=\"272\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"591\" data-original=\"https://pic1.zhimg.com/v2-a3777192c0e1812e8ab7e4dedd0e53fc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-a3777192c0e1812e8ab7e4dedd0e53fc_b.jpg\"/></figure><p>由图可见，在没有使用很复杂的网络架构与仅用十分一的训练次数，就已经做到68.34MPH的水平，因此可以推断使用了使用更大的“视野”的同时只考虑一帧的状态，就能有效提高小车的预判能力。</p><p>但为什么会这样呢？</p><p>从DQN的论文中提到，状态 s 在定义时，是用了四帧图像作为输入向量。</p><blockquote>Because the agent only observes the current screen, the task is partially observed <br/>and many emulator states are perceptually aliased (that is, it is impossible to fully<br/>understand the current situation from only the current screen x t ). Therefore,<br/>sequences of actions and observations, s t ~x 1 ,a 1 ,x 2 ,:::,a t{1 ,x t , are input to the algorithm, which then learns game strategies depending upon these sequences. All sequences in the emulator are assumed to terminate in a finite number of time-steps. This formalism gives rise to a large but finite Markov decision process (MDP) in which each sequence is a distinct state. As a result, we can apply standard reinforcement learning methods for MDPs, simply by using the complete sequence s t as the state representation at time t.<br/>                 -- from 《Human-level control through deep reinforcement learning》</blockquote><p>仔细分析发现，DQN论文中提及的前几帧状态与动作元组都是以图像的形式融合，换句话说，与DeepTraffic中直接把动作向量以concate的形式硬接在一起不同。在DQN论文中，把四帧图像叠成一个三维张量，然后用了卷积网络提取图像特征，这样能有效降低参数数量。而本例中若用此思路去构建输入向量，会导致计算量暴增。</p><p>假设我们以一个3x5的视野为例：</p><ul><li>当temporal_window=0时，输入向量长度为15，假定网格只有三种状态（实际上不止三个状态值）：（有车时状态值为0到1之间）占用、（无车时状态值为1）可用、（靠边时状态值为0）不可用。那么所需要的理论经验元组为3^15个，即14,348,907个经验元组；</li><li>当temporal_window=1时，输入向量长度为35，假定行动状态只有两种（代码调试时发现），那么所需要的理论经验元组为3^30*2^5个，已经是一个15位的天文数字了。</li></ul><p>再换另一个角度看。当我们只有一帧图像时，我们是如何判断下一步的动作的呢？</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-fc3dedca658c07b4389dbf8dcc6c865f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"328\" data-rawheight=\"704\" class=\"content_image\" width=\"328\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;328&#39; height=&#39;704&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"328\" data-rawheight=\"704\" class=\"content_image lazy\" width=\"328\" data-actualsrc=\"https://pic4.zhimg.com/v2-fc3dedca658c07b4389dbf8dcc6c865f_b.jpg\"/></figure><p>如果用文字来描述的话，我认为可以总结为两点：</p><ol><li>假如前方有堵塞，则以旁边无堵塞的车道变道；</li><li>假如前方无堵塞，则选择“视野范围”内车最少、别人离我车距离最远的车道变道。</li></ol><p>尽管以上的文字并不一定全面、不一定准确，但至少在本案例中，人类能基于一帧图像作出合理的判断。这就代表图像信息不存在歧义，因此对于机器来说也是有可能基于一帧图像来找到合理的策略。试想一个反例：例如有一个飞来的球，智能体的目标是要避开它，那么一帧的图像显然有歧义，因为连人类都无法从一帧的图像去判断来球的飞行轨迹，智能体同样会面对这个困惑。</p><p>综合以上两点分析，可以推断：由于本例temporal_window导致的参数暴增是使用一帧图像的充分条件，而结合一帧图像本身不具有歧义能得出最优策略这个必要条件，最终使得“使用一帧图像”成为合理选择。</p><hr/><h2><b>如何调整神经网络与超参数组合让成绩进一步提升？</b></h2><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-8c163a31fbe4bdc6ea473faabee4a886_b.jpg\" data-size=\"normal\" data-rawwidth=\"1290\" data-rawheight=\"960\" class=\"origin_image zh-lightbox-thumb\" width=\"1290\" data-original=\"https://pic3.zhimg.com/v2-8c163a31fbe4bdc6ea473faabee4a886_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1290&#39; height=&#39;960&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1290\" data-rawheight=\"960\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1290\" data-original=\"https://pic3.zhimg.com/v2-8c163a31fbe4bdc6ea473faabee4a886_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-8c163a31fbe4bdc6ea473faabee4a886_b.jpg\"/><figcaption>图1a。资料来源：参考资料[8]</figcaption></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-9f4ab5ad6cffc7db5b20eb958cee6306_b.jpg\" data-size=\"normal\" data-rawwidth=\"1290\" data-rawheight=\"960\" class=\"origin_image zh-lightbox-thumb\" width=\"1290\" data-original=\"https://pic3.zhimg.com/v2-9f4ab5ad6cffc7db5b20eb958cee6306_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1290&#39; height=&#39;960&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1290\" data-rawheight=\"960\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1290\" data-original=\"https://pic3.zhimg.com/v2-9f4ab5ad6cffc7db5b20eb958cee6306_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-9f4ab5ad6cffc7db5b20eb958cee6306_b.jpg\"/><figcaption>图1b。资料来源：参考资料[8]</figcaption></figure><p>以上两图是来自于MIT Self-driving Car项目组对所有的提交结果的统计图（仅显示了速度70MPH以上的数据）。我们可以借此来对网络进行改造，虽然这样做貌似有点作弊嫌疑，但这主要是为了学习而不是为了争排行榜，所以笔者在此先声明立场。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>项目组通过对提交结果的统计，发现以下几个调参思路对结果有显著影响：</p><p><b>神经网络的大小很重要</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3be09dbf3543548cc6d4d94eecc25131_b.jpg\" data-size=\"normal\" data-rawwidth=\"1290\" data-rawheight=\"960\" class=\"origin_image zh-lightbox-thumb\" width=\"1290\" data-original=\"https://pic2.zhimg.com/v2-3be09dbf3543548cc6d4d94eecc25131_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1290&#39; height=&#39;960&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1290\" data-rawheight=\"960\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1290\" data-original=\"https://pic2.zhimg.com/v2-3be09dbf3543548cc6d4d94eecc25131_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-3be09dbf3543548cc6d4d94eecc25131_b.jpg\"/><figcaption>资料来源：参考资料[8]</figcaption></figure><p>由上图、以及前面的图1a与图1b三张统计图可以看出，网络层数越多、训练参数越多，模型的性能会越好。当然，这也意味着需要更长的训练时间。从以上三图可以推断，最优的输入向量长度在400至500的范围；最优的网络层数是五至七层；最优的参数数量在20000至30000个参数；最优的训练次数是50至100万次。</p><p>需要说明的是，网络层数的统计是以输出模型的layers数组长度为计算依据。以默认网络为例，层数就是五层，如下图：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-75765a2dd18734c74149a47dfef08002_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"429\" data-rawheight=\"250\" class=\"origin_image zh-lightbox-thumb\" width=\"429\" data-original=\"https://pic3.zhimg.com/v2-75765a2dd18734c74149a47dfef08002_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;429&#39; height=&#39;250&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"429\" data-rawheight=\"250\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"429\" data-original=\"https://pic3.zhimg.com/v2-75765a2dd18734c74149a47dfef08002_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-75765a2dd18734c74149a47dfef08002_b.jpg\"/></figure><p><b>“活在当下”</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-6e98ddf7f61cc116517a1deb416f7bf8_b.jpg\" data-size=\"normal\" data-rawwidth=\"1290\" data-rawheight=\"960\" class=\"origin_image zh-lightbox-thumb\" width=\"1290\" data-original=\"https://pic1.zhimg.com/v2-6e98ddf7f61cc116517a1deb416f7bf8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1290&#39; height=&#39;960&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1290\" data-rawheight=\"960\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1290\" data-original=\"https://pic1.zhimg.com/v2-6e98ddf7f61cc116517a1deb416f7bf8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-6e98ddf7f61cc116517a1deb416f7bf8_b.jpg\"/><figcaption>资料来源：参考资料[8]</figcaption></figure><p>正如笔者前面提到的temporal_window问题，项目组的成员也很惊讶地发现当temporal_window=0时成绩是最好的。但论文中没有分析具体的原因，至少从统计结果看，证实了笔者前面的逻辑推断。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>更大的“视野”范围</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-6630f30f75c84844870121e47601d85c_b.jpg\" data-size=\"normal\" data-rawwidth=\"1290\" data-rawheight=\"960\" class=\"origin_image zh-lightbox-thumb\" width=\"1290\" data-original=\"https://pic1.zhimg.com/v2-6630f30f75c84844870121e47601d85c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1290&#39; height=&#39;960&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1290\" data-rawheight=\"960\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1290\" data-original=\"https://pic1.zhimg.com/v2-6630f30f75c84844870121e47601d85c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-6630f30f75c84844870121e47601d85c_b.jpg\"/><figcaption>资料来源：参考资料[8]</figcaption></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-869fc591c14e014c41222b269d8879fa_b.jpg\" data-size=\"normal\" data-rawwidth=\"1290\" data-rawheight=\"960\" class=\"origin_image zh-lightbox-thumb\" width=\"1290\" data-original=\"https://pic3.zhimg.com/v2-869fc591c14e014c41222b269d8879fa_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1290&#39; height=&#39;960&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1290\" data-rawheight=\"960\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1290\" data-original=\"https://pic3.zhimg.com/v2-869fc591c14e014c41222b269d8879fa_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-869fc591c14e014c41222b269d8879fa_b.jpg\"/><figcaption>资料来源：参考资料[8]</figcaption></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3afd87d36e7d1d6cee37a04e7f8150f1_b.jpg\" data-size=\"normal\" data-rawwidth=\"1290\" data-rawheight=\"960\" class=\"origin_image zh-lightbox-thumb\" width=\"1290\" data-original=\"https://pic2.zhimg.com/v2-3afd87d36e7d1d6cee37a04e7f8150f1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1290&#39; height=&#39;960&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1290\" data-rawheight=\"960\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1290\" data-original=\"https://pic2.zhimg.com/v2-3afd87d36e7d1d6cee37a04e7f8150f1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-3afd87d36e7d1d6cee37a04e7f8150f1_b.jpg\"/><figcaption>资料来源：参考资料[8]</figcaption></figure><p>从“视野”方面的参数统计可见，最优的视野参数正是之前提到的方案中所采用的视野参数。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>考虑远期收益</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-4b5b2a35ea245d53a571c466d265b68f_b.jpg\" data-size=\"normal\" data-rawwidth=\"1290\" data-rawheight=\"960\" class=\"origin_image zh-lightbox-thumb\" width=\"1290\" data-original=\"https://pic4.zhimg.com/v2-4b5b2a35ea245d53a571c466d265b68f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1290&#39; height=&#39;960&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"1290\" data-rawheight=\"960\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1290\" data-original=\"https://pic4.zhimg.com/v2-4b5b2a35ea245d53a571c466d265b68f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-4b5b2a35ea245d53a571c466d265b68f_b.jpg\"/><figcaption>资料来源：参考资料[8]</figcaption></figure><p>从统计结果可知，最优的gamma值是0.9或0.98。</p><hr/><h2><b>实践出真知</b></h2><p>经过几周的摸索与训练（利用阿里云24小时不间断地训练），笔者使用了以下网络参数：</p><div class=\"highlight\"><pre><code class=\"language-js\"><span class=\"c1\">//&lt;![CDATA[\n</span><span class=\"c1\"></span>\n<span class=\"c1\">// a few things don&#39;t have var in front of them - they update already existing variables the game needs\n</span><span class=\"c1\"></span><span class=\"nx\">lanesSide</span> <span class=\"o\">=</span> <span class=\"mi\">3</span><span class=\"p\">;</span>\n<span class=\"nx\">patchesAhead</span> <span class=\"o\">=</span> <span class=\"mi\">50</span><span class=\"p\">;</span>\n<span class=\"nx\">patchesBehind</span> <span class=\"o\">=</span> <span class=\"mi\">5</span><span class=\"p\">;</span>\n<span class=\"nx\">trainIterations</span> <span class=\"o\">=</span> <span class=\"mi\">1000000</span><span class=\"p\">;</span>\n\n<span class=\"c1\">// the number of other autonomous vehicles controlled by your network\n</span><span class=\"c1\"></span><span class=\"nx\">otherAgents</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">;</span> <span class=\"c1\">// max of 10\n</span><span class=\"c1\"></span>\n<span class=\"kd\">var</span> <span class=\"nx\">num_inputs</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"nx\">lanesSide</span> <span class=\"o\">*</span> <span class=\"mi\">2</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"nx\">patchesAhead</span> <span class=\"o\">+</span> <span class=\"nx\">patchesBehind</span><span class=\"p\">);</span>\n<span class=\"kd\">var</span> <span class=\"nx\">num_actions</span> <span class=\"o\">=</span> <span class=\"mi\">5</span><span class=\"p\">;</span>\n<span class=\"kd\">var</span> <span class=\"nx\">temporal_window</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">;</span>\n<span class=\"kd\">var</span> <span class=\"nx\">network_size</span> <span class=\"o\">=</span> <span class=\"nx\">num_inputs</span> <span class=\"o\">*</span> <span class=\"nx\">temporal_window</span> <span class=\"o\">+</span> <span class=\"nx\">num_actions</span> <span class=\"o\">*</span> <span class=\"nx\">temporal_window</span> <span class=\"o\">+</span> <span class=\"nx\">num_inputs</span><span class=\"p\">;</span>\n\n<span class=\"kd\">var</span> <span class=\"nx\">layer_defs</span> <span class=\"o\">=</span> <span class=\"p\">[];</span>\n    <span class=\"nx\">layer_defs</span><span class=\"p\">.</span><span class=\"nx\">push</span><span class=\"p\">({</span>\n    <span class=\"nx\">type</span><span class=\"o\">:</span> <span class=\"s1\">&#39;input&#39;</span><span class=\"p\">,</span>\n    <span class=\"nx\">out_sx</span><span class=\"o\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span>\n    <span class=\"nx\">out_sy</span><span class=\"o\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span>\n    <span class=\"nx\">out_depth</span><span class=\"o\">:</span> <span class=\"nx\">network_size</span>\n<span class=\"p\">});</span>\n<span class=\"nx\">layer_defs</span><span class=\"p\">.</span><span class=\"nx\">push</span><span class=\"p\">({</span>\n    <span class=\"nx\">type</span><span class=\"o\">:</span> <span class=\"s1\">&#39;fc&#39;</span><span class=\"p\">,</span>\n    <span class=\"nx\">num_neurons</span><span class=\"o\">:</span> <span class=\"nx\">network_size</span><span class=\"o\">*</span><span class=\"mi\">2</span><span class=\"p\">,</span>\n    <span class=\"nx\">activation</span><span class=\"o\">:</span> <span class=\"s1\">&#39;tanh&#39;</span>\n<span class=\"p\">});</span>\n<span class=\"nx\">layer_defs</span><span class=\"p\">.</span><span class=\"nx\">push</span><span class=\"p\">({</span>\n    <span class=\"nx\">type</span><span class=\"o\">:</span> <span class=\"s1\">&#39;fc&#39;</span><span class=\"p\">,</span>\n    <span class=\"nx\">num_neurons</span><span class=\"o\">:</span> <span class=\"nx\">network_size</span><span class=\"p\">,</span>\n    <span class=\"nx\">activation</span><span class=\"o\">:</span> <span class=\"s1\">&#39;tanh&#39;</span>\n<span class=\"p\">});</span>\n<span class=\"nx\">layer_defs</span><span class=\"p\">.</span><span class=\"nx\">push</span><span class=\"p\">({</span>\n    <span class=\"nx\">type</span><span class=\"o\">:</span> <span class=\"s1\">&#39;regression&#39;</span><span class=\"p\">,</span>\n    <span class=\"nx\">num_neurons</span><span class=\"o\">:</span> <span class=\"nx\">num_actions</span>\n<span class=\"p\">});</span>\n\n<span class=\"kd\">var</span> <span class=\"nx\">tdtrainer_options</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"nx\">learning_rate</span><span class=\"o\">:</span> <span class=\"mf\">0.001</span><span class=\"p\">,</span>\n    <span class=\"nx\">momentum</span><span class=\"o\">:</span> <span class=\"mf\">0.0</span><span class=\"p\">,</span>\n    <span class=\"nx\">batch_size</span><span class=\"o\">:</span> <span class=\"mi\">64</span><span class=\"p\">,</span>\n    <span class=\"nx\">l2_decay</span><span class=\"o\">:</span> <span class=\"mf\">0.01</span>\n<span class=\"p\">};</span>\n\n<span class=\"kd\">var</span> <span class=\"nx\">opt</span> <span class=\"o\">=</span> <span class=\"p\">{};</span>\n<span class=\"nx\">opt</span><span class=\"p\">.</span><span class=\"nx\">temporal_window</span> <span class=\"o\">=</span> <span class=\"nx\">temporal_window</span><span class=\"p\">;</span>\n<span class=\"nx\">opt</span><span class=\"p\">.</span><span class=\"nx\">experience_size</span> <span class=\"o\">=</span> <span class=\"mi\">50000</span><span class=\"p\">;</span>\n<span class=\"nx\">opt</span><span class=\"p\">.</span><span class=\"nx\">start_learn_threshold</span> <span class=\"o\">=</span> <span class=\"mi\">20000</span><span class=\"p\">;</span>\n<span class=\"nx\">opt</span><span class=\"p\">.</span><span class=\"nx\">gamma</span> <span class=\"o\">=</span> <span class=\"mf\">0.98</span><span class=\"p\">;</span>\n<span class=\"nx\">opt</span><span class=\"p\">.</span><span class=\"nx\">learning_steps_total</span> <span class=\"o\">=</span> <span class=\"mi\">30000</span><span class=\"p\">;</span>\n<span class=\"nx\">opt</span><span class=\"p\">.</span><span class=\"nx\">learning_steps_burnin</span> <span class=\"o\">=</span> <span class=\"mi\">10000</span><span class=\"p\">;</span>\n<span class=\"nx\">opt</span><span class=\"p\">.</span><span class=\"nx\">epsilon_min</span> <span class=\"o\">=</span> <span class=\"mf\">0.1</span><span class=\"p\">;</span>\n<span class=\"nx\">opt</span><span class=\"p\">.</span><span class=\"nx\">epsilon_test_time</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span><span class=\"p\">;</span>\n<span class=\"nx\">opt</span><span class=\"p\">.</span><span class=\"nx\">layer_defs</span> <span class=\"o\">=</span> <span class=\"nx\">layer_defs</span><span class=\"p\">;</span>\n<span class=\"nx\">opt</span><span class=\"p\">.</span><span class=\"nx\">tdtrainer_options</span> <span class=\"o\">=</span> <span class=\"nx\">tdtrainer_options</span><span class=\"p\">;</span>\n\n<span class=\"c1\">//略\n</span><span class=\"c1\"></span>\n<span class=\"c1\">//]]&gt;\n</span></code></pre></div><p>代码说明：</p><ul><li>为了尽可能与Anton Pechenko的方案有所区别，笔者采用了不同的神经网络结构；</li><li>每次训练，优化器都是由代码封装好并随机选择的，因此每次训练的过程的平均奖励曲线都完全不同；</li><li>笔者发现在训练的过程当中，可以在任意时刻保存训练网络，此时保存的JS文件可用于加载并进行测试，这样有助于我们在合适的训练时机停止训练；</li><li>右边的平均奖励曲线可以用于作为停止训练的判断依据；</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-1a229a110cfb5769264b99e13a05748c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"456\" data-rawheight=\"192\" class=\"origin_image zh-lightbox-thumb\" width=\"456\" data-original=\"https://pic1.zhimg.com/v2-1a229a110cfb5769264b99e13a05748c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;456&#39; height=&#39;192&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"456\" data-rawheight=\"192\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"456\" data-original=\"https://pic1.zhimg.com/v2-1a229a110cfb5769264b99e13a05748c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-1a229a110cfb5769264b99e13a05748c_b.jpg\"/></figure><p>上图是训练进行到四分一左右时（注意：曲线图的横坐标并不等同于进度，该横坐标没有什么实质作用。以训练进度条为准），平均奖励曲线创了个小新高，于是我把网络保存下来。测试结果如下图（总算是能够进入70MPH以上了）：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-25f5f6805d79e72c8b00ea193ad83a75_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"611\" data-rawheight=\"273\" class=\"origin_image zh-lightbox-thumb\" width=\"611\" data-original=\"https://pic2.zhimg.com/v2-25f5f6805d79e72c8b00ea193ad83a75_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;611&#39; height=&#39;273&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"611\" data-rawheight=\"273\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"611\" data-original=\"https://pic2.zhimg.com/v2-25f5f6805d79e72c8b00ea193ad83a75_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-25f5f6805d79e72c8b00ea193ad83a75_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>既然在训练的过程中可以在任意时间点保存网络参数，那么这相当于是“早停”操作啊。问题是，这个不是Tensorflow，在API的设计上没有留这个方法设置。</p><p>还记得我之前曾经试过在learn函数加输出用来观察奖励函数吧：</p><div class=\"highlight\"><pre><code class=\"language-js\"><span class=\"nx\">learn</span> <span class=\"o\">=</span> <span class=\"kd\">function</span> <span class=\"p\">(</span><span class=\"nx\">state</span><span class=\"p\">,</span> <span class=\"nx\">lastReward</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n<span class=\"nx\">brain</span><span class=\"p\">.</span><span class=\"nx\">backward</span><span class=\"p\">(</span><span class=\"nx\">lastReward</span><span class=\"p\">);</span>\n<span class=\"kd\">var</span> <span class=\"nx\">action</span> <span class=\"o\">=</span> <span class=\"nx\">brain</span><span class=\"p\">.</span><span class=\"nx\">forward</span><span class=\"p\">(</span><span class=\"nx\">state</span><span class=\"p\">);</span>\n<span class=\"nx\">console</span><span class=\"p\">.</span><span class=\"nx\">log</span><span class=\"p\">(</span><span class=\"nx\">lastReward</span><span class=\"p\">);</span> <span class=\"c1\">//输出观察奖励值\n</span><span class=\"c1\"></span><span class=\"nx\">draw_net</span><span class=\"p\">();</span>\n<span class=\"nx\">draw_stats</span><span class=\"p\">();</span>\n\n<span class=\"k\">return</span> <span class=\"nx\">action</span><span class=\"p\">;</span>\n<span class=\"p\">}</span>\n</code></pre></div><p>既然从F12控制台能看到奖励值的不断刷新，证明这个learn回调会不断地被调用。于是，笔者突发奇想，能否在learn回调函数中来一波骚操作，使得当平均奖励达到某个水平时自动保存神经网络，以达到“早停”的效果呢？事实证明，完全可行。这可以说是笔者对该项目最大的贡献——“创新”的训练方法：</p><div class=\"highlight\"><pre><code class=\"language-js\"><span class=\"c1\">//前面的超参数设置，略\n</span><span class=\"c1\"></span>\n<span class=\"nx\">brain</span> <span class=\"o\">=</span> <span class=\"k\">new</span> <span class=\"nx\">deepqlearn</span><span class=\"p\">.</span><span class=\"nx\">Brain</span><span class=\"p\">(</span><span class=\"nx\">num_inputs</span><span class=\"p\">,</span> <span class=\"nx\">num_actions</span><span class=\"p\">,</span> <span class=\"nx\">opt</span><span class=\"p\">);</span>\n\n<span class=\"nx\">avg_score</span> <span class=\"o\">=</span> <span class=\"mf\">2.2</span><span class=\"p\">;</span> <span class=\"c1\">//在learn回调函数外，定义一个全局变量阀值\n</span><span class=\"c1\"></span>\n<span class=\"nx\">learn</span> <span class=\"o\">=</span> <span class=\"kd\">function</span> <span class=\"p\">(</span><span class=\"nx\">state</span><span class=\"p\">,</span> <span class=\"nx\">lastReward</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n<span class=\"nx\">brain</span><span class=\"p\">.</span><span class=\"nx\">backward</span><span class=\"p\">(</span><span class=\"nx\">lastReward</span><span class=\"p\">);</span>\n<span class=\"kd\">var</span> <span class=\"nx\">action</span> <span class=\"o\">=</span> <span class=\"nx\">brain</span><span class=\"p\">.</span><span class=\"nx\">forward</span><span class=\"p\">(</span><span class=\"nx\">state</span><span class=\"p\">);</span>\n\n<span class=\"nx\">draw_net</span><span class=\"p\">();</span>\n<span class=\"nx\">draw_stats</span><span class=\"p\">();</span>\n\n<span class=\"c1\">//当平均奖励大于阀值时就自动保存\n</span><span class=\"c1\"></span><span class=\"k\">if</span><span class=\"p\">(</span><span class=\"nx\">brain</span><span class=\"p\">.</span><span class=\"nx\">average_reward_window</span><span class=\"p\">.</span><span class=\"nx\">get_average</span><span class=\"p\">()</span><span class=\"o\">&gt;</span><span class=\"nx\">avg_score</span><span class=\"p\">){</span>\n    <span class=\"nx\">downloadCode</span><span class=\"p\">();</span> <span class=\"c1\">//自动保存\n</span><span class=\"c1\"></span>    <span class=\"nx\">avg_score</span> <span class=\"o\">+=</span> <span class=\"mf\">0.1</span><span class=\"p\">;</span> <span class=\"c1\">//让阀值自增长，这样保存的网络会越来越厉害\n</span><span class=\"c1\"></span><span class=\"p\">}</span>\n\n<span class=\"k\">return</span> <span class=\"nx\">action</span><span class=\"p\">;</span>\n<span class=\"p\">}</span>\n</code></pre></div><p>代码说明：</p><ul><li>为什么笔者可以取到平均奖励？看源码看出来的！</li><li>为什么笔者知道自动保存方法？也是看源码看出来的！</li></ul><p>需要说明的是，以上的代码写法经实践后有BUG，原因是训练与测评时，框架使用了一些复杂的多线程处理，而在线程加载的时候会省略了某个JS而导致控制台报错。一旦报错就无法正常训练与测评。因此，以上代码要手动加入缺失的那部分JS方法。具体代码如下：</p><div class=\"highlight\"><pre><code class=\"language-js\"><span class=\"c1\">//前面的超参数设置，略\n</span><span class=\"c1\"></span>\n<span class=\"nx\">brain</span> <span class=\"o\">=</span> <span class=\"k\">new</span> <span class=\"nx\">deepqlearn</span><span class=\"p\">.</span><span class=\"nx\">Brain</span><span class=\"p\">(</span><span class=\"nx\">num_inputs</span><span class=\"p\">,</span> <span class=\"nx\">num_actions</span><span class=\"p\">,</span> <span class=\"nx\">opt</span><span class=\"p\">);</span>\n\n<span class=\"nx\">avg_score</span> <span class=\"o\">=</span> <span class=\"mf\">2.2</span><span class=\"p\">;</span> <span class=\"c1\">//在learn回调函数外，定义一个全局变量阀值\n</span><span class=\"c1\"></span>\n<span class=\"c1\">///////////////////////////////////////////////////\n</span><span class=\"c1\">//从原来的site.js提取的函数\n</span><span class=\"c1\"></span><span class=\"kd\">function</span> <span class=\"nx\">getData</span><span class=\"p\">(</span><span class=\"nb\">document</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"kd\">var</span> <span class=\"nx\">data</span> <span class=\"o\">=</span> <span class=\"nb\">document</span><span class=\"p\">.</span><span class=\"nx\">getElementById</span><span class=\"p\">(</span><span class=\"s2\">&#34;sampleCode&#34;</span><span class=\"p\">).</span><span class=\"nx\">innerHTML</span><span class=\"p\">.</span><span class=\"nx\">split</span><span class=\"p\">(</span><span class=\"s2\">&#34;\\n&#34;</span><span class=\"p\">).</span><span class=\"nx\">join</span><span class=\"p\">(</span><span class=\"s2\">&#34;\\n&#34;</span><span class=\"p\">)</span> <span class=\"o\">+</span> <span class=\"s2\">&#34;\\n/*###########*/\\n&#34;</span><span class=\"p\">;</span>\n    <span class=\"k\">if</span> <span class=\"p\">(</span><span class=\"nx\">brain</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n        <span class=\"nx\">data</span> <span class=\"o\">+=</span> <span class=\"s2\">&#34;if (brain) {\\nbrain.value_net.fromJSON(&#34;</span> <span class=\"o\">+</span> <span class=\"nx\">JSON</span><span class=\"p\">.</span><span class=\"nx\">stringify</span><span class=\"p\">(</span><span class=\"nx\">brain</span><span class=\"p\">.</span><span class=\"nx\">value_net</span><span class=\"p\">.</span><span class=\"nx\">toJSON</span><span class=\"p\">())</span> <span class=\"o\">+</span> <span class=\"s2\">&#34;);\\n}&#34;</span><span class=\"p\">;</span>\n    <span class=\"p\">}</span>\n    <span class=\"k\">return</span> <span class=\"nx\">data</span><span class=\"p\">;</span>\n<span class=\"p\">}</span>\n\n<span class=\"nx\">downloadCode</span> <span class=\"o\">=</span> <span class=\"kd\">function</span> <span class=\"p\">(</span><span class=\"nb\">document</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"kd\">var</span> <span class=\"nx\">blob</span> <span class=\"o\">=</span> <span class=\"k\">new</span> <span class=\"nx\">Blob</span><span class=\"p\">([</span><span class=\"nx\">getData</span><span class=\"p\">(</span><span class=\"nb\">document</span><span class=\"p\">)],</span> <span class=\"p\">{</span><span class=\"nx\">type</span><span class=\"o\">:</span> <span class=\"s1\">&#39;text/plain&#39;</span><span class=\"p\">});</span>\n    <span class=\"kd\">var</span> <span class=\"nx\">url</span> <span class=\"o\">=</span> <span class=\"nx\">URL</span><span class=\"p\">.</span><span class=\"nx\">createObjectURL</span><span class=\"p\">(</span><span class=\"nx\">blob</span><span class=\"p\">);</span>\n    <span class=\"kd\">var</span> <span class=\"nx\">blobAnchor</span> <span class=\"o\">=</span> <span class=\"nb\">document</span><span class=\"p\">.</span><span class=\"nx\">getElementById</span><span class=\"p\">(</span><span class=\"s2\">&#34;blobDownload&#34;</span><span class=\"p\">);</span>\n    <span class=\"nx\">blobAnchor</span><span class=\"p\">.</span><span class=\"nx\">download</span> <span class=\"o\">=</span> <span class=\"s2\">&#34;net.js&#34;</span><span class=\"p\">;</span>\n    <span class=\"nx\">blobAnchor</span><span class=\"p\">.</span><span class=\"nx\">href</span> <span class=\"o\">=</span> <span class=\"nx\">url</span><span class=\"p\">;</span>\n    <span class=\"nx\">blobAnchor</span><span class=\"p\">.</span><span class=\"nx\">click</span><span class=\"p\">();</span>\n<span class=\"p\">}</span>\n<span class=\"c1\">////////////////////////////////////////////////////\n</span><span class=\"c1\"></span>\n<span class=\"nx\">learn</span> <span class=\"o\">=</span> <span class=\"kd\">function</span> <span class=\"p\">(</span><span class=\"nx\">state</span><span class=\"p\">,</span> <span class=\"nx\">lastReward</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n<span class=\"nx\">brain</span><span class=\"p\">.</span><span class=\"nx\">backward</span><span class=\"p\">(</span><span class=\"nx\">lastReward</span><span class=\"p\">);</span>\n<span class=\"kd\">var</span> <span class=\"nx\">action</span> <span class=\"o\">=</span> <span class=\"nx\">brain</span><span class=\"p\">.</span><span class=\"nx\">forward</span><span class=\"p\">(</span><span class=\"nx\">state</span><span class=\"p\">);</span>\n\n<span class=\"nx\">draw_net</span><span class=\"p\">();</span>\n<span class=\"nx\">draw_stats</span><span class=\"p\">();</span>\n\n<span class=\"k\">if</span><span class=\"p\">(</span><span class=\"nx\">brain</span><span class=\"o\">!=</span><span class=\"kc\">undefined</span> <span class=\"o\">&amp;&amp;</span> <span class=\"nx\">brain</span><span class=\"p\">.</span><span class=\"nx\">average_reward_window</span><span class=\"p\">.</span><span class=\"nx\">get_average</span><span class=\"p\">()</span><span class=\"o\">&gt;</span><span class=\"nx\">avg_score</span><span class=\"p\">){</span>\n    <span class=\"k\">if</span><span class=\"p\">(</span><span class=\"nb\">window</span><span class=\"p\">.</span><span class=\"nb\">document</span><span class=\"p\">){</span>\n        <span class=\"nx\">downloadCode</span><span class=\"p\">(</span><span class=\"nb\">window</span><span class=\"p\">.</span><span class=\"nb\">document</span><span class=\"p\">);</span>\n        <span class=\"nx\">avg_score</span> <span class=\"o\">+=</span> <span class=\"mf\">0.1</span><span class=\"p\">;</span>\n    <span class=\"p\">}</span>\n<span class=\"p\">}</span>\n\n<span class=\"k\">return</span> <span class=\"nx\">action</span><span class=\"p\">;</span>\n<span class=\"p\">}</span>\n</code></pre></div><p><b>需要特别强调的是：</b></p><p><b>这时自动保存的net.js文件不能直接加载。原因是该文件的网络超参数与网络结构的代码会输出成默认的代码。因此，js文件加载前要手动用文件编辑器把代码更改为训练设置的代码。否则训练模型不能生效。</b></p><p class=\"ztext-empty-paragraph\"><br/></p><p>最后，经过训练后的模型表现为：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-0de3318617e005eaf9b600259e6685f2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"599\" data-rawheight=\"263\" class=\"origin_image zh-lightbox-thumb\" width=\"599\" data-original=\"https://pic3.zhimg.com/v2-0de3318617e005eaf9b600259e6685f2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;599&#39; height=&#39;263&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"599\" data-rawheight=\"263\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"599\" data-original=\"https://pic3.zhimg.com/v2-0de3318617e005eaf9b600259e6685f2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-0de3318617e005eaf9b600259e6685f2_b.jpg\"/></figure><hr/><h2><b>DeepTraffic的总结与启发：</b></h2><p>该项目可以简单地理解为一个调参过程；然而仔细品味，却有许多值得深挖与研究的地方。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>环境状态的定义与网格化</b></p><p>把环境与状态进行网格化，使一个复杂的环境状态变得简单。在面对实际问题时，“如何定义状态”其实是个可以让人很困惑的问题。试想一下，假如我们从零开始定义这个项目的环境，为什么不把当前智能车自身的车速也定义成状态的一部分呢？是不是由于项目本来就有了“安全系统”，使我们无需考虑安全问题才不必把自身的车速也定义成状态呢？假如答案是肯定的话，那么给予我们的启发是：在面对复杂的实际问题时，把问题分为若干个单一的子问题，让若干个智能体共同作用，可以解决因状态维度过高、要考虑的问题过多而引起的困惑。如本项目中，实际上是由两个智能体共同作用而成：一个是控速系统、一个是安全系统。只是安全系统无需我们来实现而已。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>奖励的设定</b></p><p>与环境状态的定义同理，奖励的设定仅仅用了速度的线性函数来作为奖励函数，简单明了。在我刚开始接触这项目、还没去看源码时，我就在猜奖励函数是怎么定义的：我一开始的想法是奖励与车速及超车数有关。后来把奖励值输出控制台才发现原来奖励函数只考虑了速度变量。仔细想想超车数确实是个冗余且模棱两可的参数。因为只要前进的时候足够长且时速在50以上，超车数就会不断地增加；而且我们的智能体的目标是衡量平均车速，与超车数没有任何逻辑关系。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>神经网络的学习与输入向量形状的无关性</b></p><p>从本项目可见，状态网格本来是一个二维数组，压成一维数组后对神经网络的判断没有任何影响。更让人深刻的是，尽管增加temporal_window不会对结果有促进作用，但历史的状态与动作数组同样是以一维数组的方式直接拼接在当前的状态数组后，同样不会对神经网络的判断产生影响。</p><p><br/><b>这就是一个DQN实验室</b></p><p>本项目就相当于一个DQN实验室。该项目自2017年1月开始发布，时间还不是很长，目前也只能修改参数与神经网络。期待在日后的新版本中，可以开放更多自由度，比如可以在训练过程中动态设定网络权重，从而可以实现更多的可能性。<br/><br/></p><p><b>源码包含了自动调参方法可以手动</b></p><p>值得一提的是，在convnet.js中有个MagicNet的对象可以用作网络优化。不过笔者暂时还不是太懂，大概的思路在参考资料[9]有介绍。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>通过一个模拟环境观察智能车在道路上可能存在的问题以及得到相关的启示</b></p><p>项目组通过对所有的提交结果进行统计发现，当智能车总是“贪婪”地追求更高平均时速时，整个交通系统的平均车速也会有所提升。</p><p>而笔者也得到另一个启示，或者说发现了一个问题：在空旷的道路上，智能车时常会随意变道。如果在实际应用时，这对于车内的乘员而言简直是恶梦。“如何让智能车更加稳定地前行”则是该项目的后续思考课题。</p><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p>参考资料：</p><ol><li><a href=\"https://link.zhihu.com/?target=https%3A//selfdrivingcars.mit.edu/deeptraffic-about/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">DeepTraffic - About | MIT 6.S094: Deep Learning for Self-Driving Cars</a></li><li><a href=\"https://link.zhihu.com/?target=https%3A//cs.stanford.edu/people/karpathy/convnetjs/docs.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Deep Learning in your browser</a></li><li><a href=\"https://link.zhihu.com/?target=https%3A//cs.stanford.edu/people/karpathy/convnetjs/demo/rldemo.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">ConvNetJS Deep Q Learning Reinforcement Learning with Neural Network demo</a></li><li>程序网页中F12的源代码convnet.js, deepqlearn.js</li><li>《<a href=\"https://link.zhihu.com/?target=https%3A//storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Human-level control through deep reinforcement learning</a>》，nature.com</li><li><a href=\"https://link.zhihu.com/?target=https%3A//github.com/parilo/DeepTraffic-solution\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">网上搜索的其他解决方案 by Anton Pechenko</a></li><li><a href=\"https://link.zhihu.com/?target=https%3A//medium.com/%40jasonsalas_89883/my-attempt-at-beating-deeptraffic-de3b9d443629\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">网上搜索的其他解决方案 by Jason Salas</a></li><li><a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1801.02805\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Driving Fast through Dense Traffic with Deep Reinforcement Learning</a></li><li><a href=\"https://link.zhihu.com/?target=https%3A//research.googleblog.com/2017/05/using-machine-learning-to-explore.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Using Machine Learning to Explore Neural Network Architecture</a></li></ol>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "神经网络", 
                    "tagLink": "https://api.zhihu.com/topics/19607065"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": [
                {
                    "userName": "知乎用户", 
                    "userLink": "https://www.zhihu.com/people/0", 
                    "content": "<p>非常感谢楼主！</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "碣石", 
                    "userLink": "https://www.zhihu.com/people/b14e629fd7edc78b0fdb4a14410ba130", 
                    "content": "<p>我用了你的代码进行实验，结果进度条走到百分之十就不动了，而且出现了前面没车也不加速的情况。这种情况是必须的嘛</p><a class=\"comment_sticker\" href=\"https://pic2.zhimg.com/v2-f4bcc55c40efedc78401a3b6c59e50e5.gif\" data-width=\"\" data-height=\"\">[吃惊]</a>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "黄伟亮", 
                            "userLink": "https://www.zhihu.com/people/294306d6706f830e3cce35fb4acf46cb", 
                            "content": "是的", 
                            "likes": 0, 
                            "replyToAuthor": "碣石"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/36249826", 
            "userName": "黄伟亮", 
            "userLink": "https://www.zhihu.com/people/294306d6706f830e3cce35fb4acf46cb", 
            "upvote": 45, 
            "title": "DQN实战：MIT强化学习实战—Deep Traffic（上）", 
            "content": "<p>本项目是对DQN知识掌握程度的检验，需要对强化学习基础知识以及DQN有所了解，详细可参阅文章：</p><a href=\"https://zhuanlan.zhihu.com/p/35688924\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic4.zhimg.com/v2-991ab4b45d8f1d6340adc9313c13e91b_180x120.jpg\" data-image-width=\"738\" data-image-height=\"274\" class=\"internal\">kevinbauer：从强化学习到深度强化学习（上）</a><a href=\"https://zhuanlan.zhihu.com/p/35965070\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic4.zhimg.com/v2-991ab4b45d8f1d6340adc9313c13e91b_180x120.jpg\" data-image-width=\"738\" data-image-height=\"274\" class=\"internal\">kevinbauer：从强化学习到深度强化学习（下）</a><p>先来了解一下该项目的背景：</p><blockquote><a href=\"https://link.zhihu.com/?target=https%3A//selfdrivingcars.mit.edu/deeptraffic/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">DeepTraffic</a> is a deep reinforcement learning competition part of the MIT Deep Learning for Self-Driving Cars course. The goal is to create a neural network to drive a vehicle (or multiple vehicles) as fast as possible through dense highway traffic. An instance of your neural network gets to control one of the cars (displayed in red) and has to learn how to navigate efficiently to go as fast as possible. The car already comes with a safety system, so you don’t have to worry about the basic task of driving – the net only has to tell the car if it should accelerate/slow down or change lanes, and it will do so if that is possible without crashing into other cars.<br/>                                                               —— from 《DeepTraffic: About》</blockquote><p>DeepTraffic实际上就是一个DQN的实验平台。详细的可参阅<a href=\"https://link.zhihu.com/?target=https%3A//selfdrivingcars.mit.edu/deeptraffic/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">官方网站</a>。</p><hr/><p>首先，DeepTraffic的目标是让小车跑得越快越好。小车的限速是80MPH，因此可以理解为本项目的目标就是越接近这个极限越好。而这个项目的最低目标是让小车车速超过65MPH。我们先来看看排行榜： </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-7b112490d9b75e9c1d5090f6f7a0fa3f_b.jpg\" data-size=\"normal\" data-rawwidth=\"507\" data-rawheight=\"676\" class=\"origin_image zh-lightbox-thumb\" width=\"507\" data-original=\"https://pic4.zhimg.com/v2-7b112490d9b75e9c1d5090f6f7a0fa3f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;507&#39; height=&#39;676&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"507\" data-rawheight=\"676\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"507\" data-original=\"https://pic4.zhimg.com/v2-7b112490d9b75e9c1d5090f6f7a0fa3f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-7b112490d9b75e9c1d5090f6f7a0fa3f_b.jpg\"/><figcaption>资源来源：https://selfdrivingcars.mit.edu/deeptraffic-leaderboard/</figcaption></figure><p>可以看到，目前的排行榜第一名已经相当接近80的极限了。</p><p>我们再看看当我们什么都不改变的时候，基础模型的表现是多少：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-0afc432a8da7a6dc5bba989ea635815e_b.jpg\" data-size=\"normal\" data-rawwidth=\"628\" data-rawheight=\"316\" class=\"origin_image zh-lightbox-thumb\" width=\"628\" data-original=\"https://pic3.zhimg.com/v2-0afc432a8da7a6dc5bba989ea635815e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;628&#39; height=&#39;316&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"628\" data-rawheight=\"316\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"628\" data-original=\"https://pic3.zhimg.com/v2-0afc432a8da7a6dc5bba989ea635815e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-0afc432a8da7a6dc5bba989ea635815e_b.jpg\"/><figcaption>基础模型</figcaption></figure><p>试了几次，大概都是在51MPH左右。</p><p>好了，那我们从代码分析开始吧：</p><div class=\"highlight\"><pre><code class=\"language-js\"><span class=\"c1\">//&lt;![CDATA[\n</span><span class=\"c1\"></span>\n<span class=\"c1\">// a few things don&#39;t have var in front of them - they update already existing variables the game needs\n</span><span class=\"c1\"></span><span class=\"nx\">lanesSide</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">;</span><span class=\"c1\">//可观察多少条旁边车道\n</span><span class=\"c1\"></span><span class=\"nx\">patchesAhead</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">;</span><span class=\"c1\">//可往前观察多少格\n</span><span class=\"c1\"></span><span class=\"nx\">patchesBehind</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">;</span><span class=\"c1\">//可往后观察多少格\n</span><span class=\"c1\"></span><span class=\"nx\">trainIterations</span> <span class=\"o\">=</span> <span class=\"mi\">10000</span><span class=\"p\">;</span><span class=\"c1\">//训练的迭代数\n</span><span class=\"c1\"></span>\n<span class=\"c1\">// the number of other autonomous vehicles controlled by your network\n</span><span class=\"c1\"></span><span class=\"nx\">otherAgents</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">;</span> <span class=\"c1\">// 多辆自动驾驶车辆在路上行驶\n</span><span class=\"c1\"></span>\n<span class=\"c1\">// 计算状态向量（不用修改）\n</span><span class=\"c1\"></span><span class=\"kd\">var</span> <span class=\"nx\">num_inputs</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"nx\">lanesSide</span> <span class=\"o\">*</span> <span class=\"mi\">2</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"nx\">patchesAhead</span> <span class=\"o\">+</span> <span class=\"nx\">patchesBehind</span><span class=\"p\">);</span>\n<span class=\"c1\">// 动作数量（不用修改）\n</span><span class=\"c1\"></span><span class=\"kd\">var</span> <span class=\"nx\">num_actions</span> <span class=\"o\">=</span> <span class=\"mi\">5</span><span class=\"p\">;</span>\n<span class=\"c1\">// 参考的时间窗口\n</span><span class=\"c1\"></span><span class=\"kd\">var</span> <span class=\"nx\">temporal_window</span> <span class=\"o\">=</span> <span class=\"mi\">3</span><span class=\"p\">;</span>\n<span class=\"c1\">// 把状态向量转化为神经网络的输入向量（不用修改）\n</span><span class=\"c1\"></span><span class=\"kd\">var</span> <span class=\"nx\">network_size</span> <span class=\"o\">=</span> <span class=\"nx\">num_inputs</span> <span class=\"o\">*</span> <span class=\"nx\">temporal_window</span> <span class=\"o\">+</span> <span class=\"nx\">num_actions</span> <span class=\"o\">*</span> <span class=\"nx\">temporal_window</span> <span class=\"o\">+</span> <span class=\"nx\">num_inputs</span><span class=\"p\">;</span>\n\n<span class=\"c1\">//神经网络的架构\n</span><span class=\"c1\"></span><span class=\"kd\">var</span> <span class=\"nx\">layer_defs</span> <span class=\"o\">=</span> <span class=\"p\">[];</span>\n    <span class=\"nx\">layer_defs</span><span class=\"p\">.</span><span class=\"nx\">push</span><span class=\"p\">({</span>\n    <span class=\"nx\">type</span><span class=\"o\">:</span> <span class=\"s1\">&#39;input&#39;</span><span class=\"p\">,</span>\n    <span class=\"nx\">out_sx</span><span class=\"o\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span>\n    <span class=\"nx\">out_sy</span><span class=\"o\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span>\n    <span class=\"nx\">out_depth</span><span class=\"o\">:</span> <span class=\"nx\">network_size</span>\n<span class=\"p\">});</span>\n<span class=\"nx\">layer_defs</span><span class=\"p\">.</span><span class=\"nx\">push</span><span class=\"p\">({</span>\n    <span class=\"nx\">type</span><span class=\"o\">:</span> <span class=\"s1\">&#39;fc&#39;</span><span class=\"p\">,</span>\n    <span class=\"nx\">num_neurons</span><span class=\"o\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span>\n    <span class=\"nx\">activation</span><span class=\"o\">:</span> <span class=\"s1\">&#39;relu&#39;</span>\n<span class=\"p\">});</span>\n<span class=\"nx\">layer_defs</span><span class=\"p\">.</span><span class=\"nx\">push</span><span class=\"p\">({</span>\n    <span class=\"nx\">type</span><span class=\"o\">:</span> <span class=\"s1\">&#39;regression&#39;</span><span class=\"p\">,</span>\n    <span class=\"nx\">num_neurons</span><span class=\"o\">:</span> <span class=\"nx\">num_actions</span>\n<span class=\"p\">});</span>\n\n<span class=\"c1\">//训练的超参数\n</span><span class=\"c1\"></span><span class=\"kd\">var</span> <span class=\"nx\">tdtrainer_options</span> <span class=\"o\">=</span> <span class=\"p\">{</span>\n    <span class=\"nx\">learning_rate</span><span class=\"o\">:</span> <span class=\"mf\">0.001</span><span class=\"p\">,</span> <span class=\"c1\">//学习率\n</span><span class=\"c1\"></span>    <span class=\"nx\">momentum</span><span class=\"o\">:</span> <span class=\"mf\">0.0</span><span class=\"p\">,</span> <span class=\"c1\">//动量\n</span><span class=\"c1\"></span>    <span class=\"nx\">batch_size</span><span class=\"o\">:</span> <span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"c1\">//批大小\n</span><span class=\"c1\"></span>    <span class=\"nx\">l2_decay</span><span class=\"o\">:</span> <span class=\"mf\">0.01</span> <span class=\"c1\">//衰减\n</span><span class=\"c1\"></span><span class=\"p\">};</span>\n\n<span class=\"c1\">//其他超参数\n</span><span class=\"c1\"></span><span class=\"kd\">var</span> <span class=\"nx\">opt</span> <span class=\"o\">=</span> <span class=\"p\">{};</span>\n<span class=\"nx\">opt</span><span class=\"p\">.</span><span class=\"nx\">temporal_window</span> <span class=\"o\">=</span> <span class=\"nx\">temporal_window</span><span class=\"p\">;</span>\n<span class=\"nx\">opt</span><span class=\"p\">.</span><span class=\"nx\">experience_size</span> <span class=\"o\">=</span> <span class=\"mi\">3000</span><span class=\"p\">;</span> <span class=\"c1\">//经验暂存区大小\n</span><span class=\"c1\"></span><span class=\"nx\">opt</span><span class=\"p\">.</span><span class=\"nx\">start_learn_threshold</span> <span class=\"o\">=</span> <span class=\"mi\">500</span><span class=\"p\">;</span> <span class=\"c1\">//开始训练阀值\n</span><span class=\"c1\"></span><span class=\"nx\">opt</span><span class=\"p\">.</span><span class=\"nx\">gamma</span> <span class=\"o\">=</span> <span class=\"mf\">0.7</span><span class=\"p\">;</span> <span class=\"c1\">//远期奖励折扣率\n</span><span class=\"c1\">//this.epsilon = Math.min(1.0, Math.max(this.epsilon_min, 1.0-(this.age - this.learning_steps_burnin)/(this.learning_steps_total - this.learning_steps_burnin))); \n</span><span class=\"c1\"></span><span class=\"nx\">opt</span><span class=\"p\">.</span><span class=\"nx\">learning_steps_total</span> <span class=\"o\">=</span> <span class=\"mi\">10000</span><span class=\"p\">;</span> <span class=\"c1\">//用于计算epsilon\n</span><span class=\"c1\"></span><span class=\"nx\">opt</span><span class=\"p\">.</span><span class=\"nx\">learning_steps_burnin</span> <span class=\"o\">=</span> <span class=\"mi\">1000</span><span class=\"p\">;</span> <span class=\"c1\">//用于计算epsilon,开始训练时随机行动训练次数\n</span><span class=\"c1\"></span><span class=\"nx\">opt</span><span class=\"p\">.</span><span class=\"nx\">epsilon_min</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span><span class=\"p\">;</span> <span class=\"c1\">//训练时的最小epsilon\n</span><span class=\"c1\"></span><span class=\"nx\">opt</span><span class=\"p\">.</span><span class=\"nx\">epsilon_test_time</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span><span class=\"p\">;</span> <span class=\"c1\">//测试时的epsilon值\n</span><span class=\"c1\"></span><span class=\"nx\">opt</span><span class=\"p\">.</span><span class=\"nx\">layer_defs</span> <span class=\"o\">=</span> <span class=\"nx\">layer_defs</span><span class=\"p\">;</span>\n<span class=\"nx\">opt</span><span class=\"p\">.</span><span class=\"nx\">tdtrainer_options</span> <span class=\"o\">=</span> <span class=\"nx\">tdtrainer_options</span><span class=\"p\">;</span>\n\n<span class=\"c1\">//构建神经网络\n</span><span class=\"c1\"></span><span class=\"nx\">brain</span> <span class=\"o\">=</span> <span class=\"k\">new</span> <span class=\"nx\">deepqlearn</span><span class=\"p\">.</span><span class=\"nx\">Brain</span><span class=\"p\">(</span><span class=\"nx\">num_inputs</span><span class=\"p\">,</span> <span class=\"nx\">num_actions</span><span class=\"p\">,</span> <span class=\"nx\">opt</span><span class=\"p\">);</span>\n\n<span class=\"c1\">// 训练并生成动作\n</span><span class=\"c1\"></span><span class=\"nx\">learn</span> <span class=\"o\">=</span> <span class=\"kd\">function</span> <span class=\"p\">(</span><span class=\"nx\">state</span><span class=\"p\">,</span> <span class=\"nx\">lastReward</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n<span class=\"nx\">brain</span><span class=\"p\">.</span><span class=\"nx\">backward</span><span class=\"p\">(</span><span class=\"nx\">lastReward</span><span class=\"p\">);</span>\n<span class=\"kd\">var</span> <span class=\"nx\">action</span> <span class=\"o\">=</span> <span class=\"nx\">brain</span><span class=\"p\">.</span><span class=\"nx\">forward</span><span class=\"p\">(</span><span class=\"nx\">state</span><span class=\"p\">);</span>\n\n<span class=\"nx\">draw_net</span><span class=\"p\">();</span>\n<span class=\"nx\">draw_stats</span><span class=\"p\">();</span>\n\n<span class=\"k\">return</span> <span class=\"nx\">action</span><span class=\"p\">;</span>\n<span class=\"p\">}</span>\n\n<span class=\"c1\">//]]&gt;\n</span></code></pre></div><p>代码说明（大致的代码逻辑已经用中文注释，下面只说关键点）：</p><ul><li><b>什么是状态向量？</b></li></ul><p>由于小车的观察范围被网格化处理，如下图：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-e096bf0f51c07cf116001e84627649d1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"151\" data-rawheight=\"300\" class=\"content_image\" width=\"151\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;151&#39; height=&#39;300&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"151\" data-rawheight=\"300\" class=\"content_image lazy\" width=\"151\" data-actualsrc=\"https://pic2.zhimg.com/v2-e096bf0f51c07cf116001e84627649d1_b.jpg\"/></figure><p>因此网格的状态所组成的向量就是状态向量。我们验证一下：</p><p>首先，在代码加上输出</p><div class=\"highlight\"><pre><code class=\"language-js\"><span class=\"nx\">lanesSide</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">;</span>\n<span class=\"nx\">patchesAhead</span> <span class=\"o\">=</span> <span class=\"mi\">5</span><span class=\"p\">;</span> <span class=\"c1\">// 把前方观察范围设为5格\n</span><span class=\"c1\"></span><span class=\"nx\">patchesBehind</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">;</span>\n\n<span class=\"c1\">//中间代码略...\n</span><span class=\"c1\"></span>\n<span class=\"nx\">learn</span> <span class=\"o\">=</span> <span class=\"kd\">function</span> <span class=\"p\">(</span><span class=\"nx\">state</span><span class=\"p\">,</span> <span class=\"nx\">lastReward</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n<span class=\"nx\">console</span><span class=\"p\">.</span><span class=\"nx\">log</span><span class=\"p\">(</span><span class=\"nx\">state</span><span class=\"p\">);</span> <span class=\"c1\">// 加上输出，查看状态\n</span><span class=\"c1\"></span><span class=\"nx\">brain</span><span class=\"p\">.</span><span class=\"nx\">backward</span><span class=\"p\">(</span><span class=\"nx\">lastReward</span><span class=\"p\">);</span>\n<span class=\"kd\">var</span> <span class=\"nx\">action</span> <span class=\"o\">=</span> <span class=\"nx\">brain</span><span class=\"p\">.</span><span class=\"nx\">forward</span><span class=\"p\">(</span><span class=\"nx\">state</span><span class=\"p\">);</span>\n\n<span class=\"nx\">draw_net</span><span class=\"p\">();</span>\n<span class=\"nx\">draw_stats</span><span class=\"p\">();</span>\n\n<span class=\"k\">return</span> <span class=\"nx\">action</span><span class=\"p\">;</span>\n<span class=\"p\">}</span>\n</code></pre></div><p>然后在浏览器的F12控制台，可以看到不断有状态数组输出，如下图：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-5766d35efbc6c37fd12e062b31989568_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"406\" data-rawheight=\"128\" class=\"content_image\" width=\"406\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;406&#39; height=&#39;128&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"406\" data-rawheight=\"128\" class=\"content_image lazy\" width=\"406\" data-actualsrc=\"https://pic1.zhimg.com/v2-5766d35efbc6c37fd12e062b31989568_b.jpg\"/></figure><p>状态值会在[0,1]的范围，当网格没有其他车辆时状态值为1，当网格有其他车辆占位时状态值为(0,1)之间，当网格不可行驶（比如靠边的车道）时状态值为0。</p><p>我们再验证一下观察旁边一条车道的情况：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-c0218c1236c70b5f7bb39d5de8cdca73_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"205\" data-rawheight=\"168\" class=\"content_image\" width=\"205\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;205&#39; height=&#39;168&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"205\" data-rawheight=\"168\" class=\"content_image lazy\" width=\"205\" data-actualsrc=\"https://pic4.zhimg.com/v2-c0218c1236c70b5f7bb39d5de8cdca73_b.jpg\"/></figure><div class=\"highlight\"><pre><code class=\"language-js\"><span class=\"nx\">lanesSide</span> <span class=\"o\">=</span> <span class=\"mi\">1</span><span class=\"p\">;</span> <span class=\"c1\">// 观察旁边一条车道\n</span><span class=\"c1\"></span><span class=\"nx\">patchesAhead</span> <span class=\"o\">=</span> <span class=\"mi\">5</span><span class=\"p\">;</span> <span class=\"c1\">// 把前方观察范围设为5格\n</span><span class=\"c1\"></span><span class=\"nx\">patchesBehind</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">;</span>\n</code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-e10878eee079ed37a40a902b9a5bcd23_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"613\" data-rawheight=\"107\" class=\"origin_image zh-lightbox-thumb\" width=\"613\" data-original=\"https://pic4.zhimg.com/v2-e10878eee079ed37a40a902b9a5bcd23_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;613&#39; height=&#39;107&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"613\" data-rawheight=\"107\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"613\" data-original=\"https://pic4.zhimg.com/v2-e10878eee079ed37a40a902b9a5bcd23_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-e10878eee079ed37a40a902b9a5bcd23_b.jpg\"/></figure><p>可以看到，当状态为一个 <img src=\"https://www.zhihu.com/equation?tex=m%5Ctimes+n\" alt=\"m\\times n\" eeimg=\"1\"/> 的矩阵时，会被压成一个长度为 <img src=\"https://www.zhihu.com/equation?tex=m+%5Ctimes+n\" alt=\"m \\times n\" eeimg=\"1\"/> 的一维数组。</p><p class=\"ztext-empty-paragraph\"><br/></p><ul><li><b>如何定义时间步？</b></li></ul><p>在强化学习的理论框架里，有个很重要的概念：时间步。智能体在每个时间步，根据环境的状态采取相应的行动，从而得到相应的奖励反馈。那么，在DeepTraffic模型中，时间步则是每隔30帧就取一帧代表一个时间步。</p><blockquote>The simulation uses <i>frames</i> as an internal measure of time – so neither a slow computer, nor a slow net influences the result. The <i>Simulation Speed</i> setting lets you control how the simulation is displayed to you – using the <i>Normal </i>setting the simulation tries to draw the frames matching real time, so it waits if the actual calculation is going faster – <i>Fast</i> displays frames as soon as they are finished, which may be much faster.<br/>                                                                    —— from 《DeepTraffic: About》</blockquote><p class=\"ztext-empty-paragraph\"><br/></p><blockquote>Further, there is one car (displayed in red) that is not using these random actions. This is the car controlled by the deep reinforcement learning agent. It is able to choose an action every 30 frames (the time it takes to make a lane change)<br/>and gets a cutout of the state map as an input to compute its actions.<br/>—— from 《<a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1801.02805\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Driving Fast through Dense Traffic with Deep Reinforcement Learning</a>》</blockquote><p>同时需要说明的是，系统不会因为运行机器的快慢而影响计算结果以及模拟效果。</p><p class=\"ztext-empty-paragraph\"><br/></p><ul><li><b>状态向量如何转化为输入向量？</b></li></ul><p>从原来给定的代码可以看到，输入向量大小与状态向量大小并不相同：</p><div class=\"highlight\"><pre><code class=\"language-js\"><span class=\"c1\">//状态向量经过temporal_window的处理进行转换\n</span><span class=\"c1\"></span><span class=\"kd\">var</span> <span class=\"nx\">network_size</span> <span class=\"o\">=</span> <span class=\"nx\">num_inputs</span> <span class=\"o\">*</span> <span class=\"nx\">temporal_window</span> <span class=\"o\">+</span> <span class=\"nx\">num_actions</span> <span class=\"o\">*</span> <span class=\"nx\">temporal_window</span> <span class=\"o\">+</span> <span class=\"nx\">num_inputs</span><span class=\"p\">;</span>\n\n<span class=\"kd\">var</span> <span class=\"nx\">layer_defs</span> <span class=\"o\">=</span> <span class=\"p\">[];</span>\n    <span class=\"nx\">layer_defs</span><span class=\"p\">.</span><span class=\"nx\">push</span><span class=\"p\">({</span>\n    <span class=\"nx\">type</span><span class=\"o\">:</span> <span class=\"s1\">&#39;input&#39;</span><span class=\"p\">,</span>\n    <span class=\"nx\">out_sx</span><span class=\"o\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span>\n    <span class=\"nx\">out_sy</span><span class=\"o\">:</span> <span class=\"mi\">1</span><span class=\"p\">,</span>\n    <span class=\"nx\">out_depth</span><span class=\"o\">:</span> <span class=\"nx\">network_size</span>\n<span class=\"p\">});</span>\n</code></pre></div><p>temporal_window是什么？<br/>为了搞清这个问题，我把源码从头到尾翻了一遍，在deepqlearn.js找到以下这段代码：</p><div class=\"highlight\"><pre><code class=\"language-js\"><span class=\"c1\">//摘自deepqlearn.js\n</span><span class=\"c1\"></span><span class=\"nx\">getNetInput</span><span class=\"o\">:</span> <span class=\"kd\">function</span><span class=\"p\">(</span><span class=\"nx\">xt</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n      <span class=\"c1\">// return s = (x,a,x,a,x,a,xt) state vector. \n</span><span class=\"c1\"></span>      <span class=\"c1\">// It&#39;s a concatenation of last window_size (x,a) pairs and current state x\n</span><span class=\"c1\"></span>      <span class=\"kd\">var</span> <span class=\"nx\">w</span> <span class=\"o\">=</span> <span class=\"p\">[];</span>\n      <span class=\"nx\">w</span> <span class=\"o\">=</span> <span class=\"nx\">w</span><span class=\"p\">.</span><span class=\"nx\">concat</span><span class=\"p\">(</span><span class=\"nx\">xt</span><span class=\"p\">);</span> <span class=\"c1\">// start with current state\n</span><span class=\"c1\"></span>      <span class=\"c1\">// and now go backwards and append states and actions from history temporal_window times\n</span><span class=\"c1\"></span>      <span class=\"kd\">var</span> <span class=\"nx\">n</span> <span class=\"o\">=</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">window_size</span><span class=\"p\">;</span> \n      <span class=\"k\">for</span><span class=\"p\">(</span><span class=\"kd\">var</span> <span class=\"nx\">k</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">;</span><span class=\"nx\">k</span><span class=\"o\">&lt;</span><span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">temporal_window</span><span class=\"p\">;</span><span class=\"nx\">k</span><span class=\"o\">++</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n        <span class=\"c1\">// state\n</span><span class=\"c1\"></span>        <span class=\"nx\">w</span> <span class=\"o\">=</span> <span class=\"nx\">w</span><span class=\"p\">.</span><span class=\"nx\">concat</span><span class=\"p\">(</span><span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">state_window</span><span class=\"p\">[</span><span class=\"nx\">n</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"o\">-</span><span class=\"nx\">k</span><span class=\"p\">]);</span>\n        <span class=\"c1\">// action, encoded as 1-of-k indicator vector. We scale it up a bit because\n</span><span class=\"c1\"></span>        <span class=\"c1\">// we dont want weight regularization to undervalue this information, as it only exists once\n</span><span class=\"c1\"></span>        <span class=\"kd\">var</span> <span class=\"nx\">action1ofk</span> <span class=\"o\">=</span> <span class=\"k\">new</span> <span class=\"nb\">Array</span><span class=\"p\">(</span><span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">num_actions</span><span class=\"p\">);</span>\n        <span class=\"k\">for</span><span class=\"p\">(</span><span class=\"kd\">var</span> <span class=\"nx\">q</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">;</span><span class=\"nx\">q</span><span class=\"o\">&lt;</span><span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">num_actions</span><span class=\"p\">;</span><span class=\"nx\">q</span><span class=\"o\">++</span><span class=\"p\">)</span> <span class=\"nx\">action1ofk</span><span class=\"p\">[</span><span class=\"nx\">q</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span><span class=\"p\">;</span>\n        <span class=\"nx\">action1ofk</span><span class=\"p\">[</span><span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">action_window</span><span class=\"p\">[</span><span class=\"nx\">n</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"o\">-</span><span class=\"nx\">k</span><span class=\"p\">]]</span> <span class=\"o\">=</span> <span class=\"mf\">1.0</span><span class=\"o\">*</span><span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">num_states</span><span class=\"p\">;</span>\n        <span class=\"nx\">w</span> <span class=\"o\">=</span> <span class=\"nx\">w</span><span class=\"p\">.</span><span class=\"nx\">concat</span><span class=\"p\">(</span><span class=\"nx\">action1ofk</span><span class=\"p\">);</span>\n      <span class=\"p\">}</span>\n      <span class=\"k\">return</span> <span class=\"nx\">w</span><span class=\"p\">;</span>\n    <span class=\"p\">},</span>\n</code></pre></div><p>代码说明：</p><ul><li>英文注释为源代码的注释；</li><li>getNetInput函数会把一个当前时间步的状态向量转换成一个包含前若干个时间步的状态向量以及动作向量的输入向量；</li><li>temporal_window是往前追溯多个对 <img src=\"https://www.zhihu.com/equation?tex=%3Cs%2Ca%3E\" alt=\"&lt;s,a&gt;\" eeimg=\"1\"/> 状态动作组；</li><li>经过转换后，神经网络的输入向量就相当于让状态从原来的静态特征变为包含动态特征的向量。</li></ul><p class=\"ztext-empty-paragraph\"><br/></p><ul><li><b>如何理解神经网络的架构？</b></li></ul><p>以默认的网络为例，实际上就是一个MLP，一共三层：一个输入层、一个隐藏层、一个输出层。由于该网络最终要计算的是动作值，因此输出层没有使用激活函数。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-f9d99a853bb33097bb55dc24abd969f5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"334\" data-rawheight=\"206\" class=\"content_image\" width=\"334\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;334&#39; height=&#39;206&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"334\" data-rawheight=\"206\" class=\"content_image lazy\" width=\"334\" data-actualsrc=\"https://pic2.zhimg.com/v2-f9d99a853bb33097bb55dc24abd969f5_b.jpg\"/></figure><p>神经网络的详细生成过程可参考convnet.js的源代码。</p><p class=\"ztext-empty-paragraph\"><br/></p><ul><li><b>如何理解平均奖励曲线？</b></li></ul><div class=\"highlight\"><pre><code class=\"language-js\"><span class=\"nx\">learn</span> <span class=\"o\">=</span> <span class=\"kd\">function</span> <span class=\"p\">(</span><span class=\"nx\">state</span><span class=\"p\">,</span> <span class=\"nx\">lastReward</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n<span class=\"nx\">brain</span><span class=\"p\">.</span><span class=\"nx\">backward</span><span class=\"p\">(</span><span class=\"nx\">lastReward</span><span class=\"p\">);</span>\n<span class=\"kd\">var</span> <span class=\"nx\">action</span> <span class=\"o\">=</span> <span class=\"nx\">brain</span><span class=\"p\">.</span><span class=\"nx\">forward</span><span class=\"p\">(</span><span class=\"nx\">state</span><span class=\"p\">);</span>\n<span class=\"nx\">console</span><span class=\"p\">.</span><span class=\"nx\">log</span><span class=\"p\">(</span><span class=\"nx\">lastReward</span><span class=\"p\">);</span> <span class=\"c1\">//输出观察奖励值\n</span><span class=\"c1\"></span><span class=\"nx\">draw_net</span><span class=\"p\">();</span>\n<span class=\"nx\">draw_stats</span><span class=\"p\">();</span>\n\n<span class=\"k\">return</span> <span class=\"nx\">action</span><span class=\"p\">;</span>\n<span class=\"p\">}</span>\n</code></pre></div><p>通过浏览器控制台可以观察到，当时速达到80MPH时，奖励值为3；当时速为40MPH时，奖励值为0；奖励函数是一个与时速线性相关的函数。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-b7f6945e251e77fa44ede30a2178f7e2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"368\" data-rawheight=\"116\" class=\"content_image\" width=\"368\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;368&#39; height=&#39;116&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"368\" data-rawheight=\"116\" class=\"content_image lazy\" width=\"368\" data-actualsrc=\"https://pic3.zhimg.com/v2-b7f6945e251e77fa44ede30a2178f7e2_b.jpg\"/></figure><p>上图所示的曲线是平均奖励曲线。当我们修改了代码然后点击“apply code”时，曲线起点会从上一次的计算点开始计算（这点导致我一度很困惑，但似乎对结果没有影响，不明白的可以忽略）。这里只需要知道，一个策略的平均奖励是0分的话，那智能车的平均时速就是40MPH；若平均奖励是3分的话，那智能车的平均时速就是80MPH。</p><p class=\"ztext-empty-paragraph\"><br/></p><ul><li><b>策略的生成过程是怎样的？</b></li></ul><p>我们从DQN的理论可知，经验回放与固定Q目标、以及其他关于DQN的改进方案都会对最终的函数逼近结果有显著的影响。在默认的代码实现中，是否有考虑进行这些处理呢？我们又要翻源码了：</p><div class=\"highlight\"><pre><code class=\"language-js\"><span class=\"c1\">//摘自deepqlearn.js\n</span><span class=\"c1\"></span><span class=\"nx\">backward</span><span class=\"o\">:</span> <span class=\"kd\">function</span><span class=\"p\">(</span><span class=\"nx\">reward</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n      <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">latest_reward</span> <span class=\"o\">=</span> <span class=\"nx\">reward</span><span class=\"p\">;</span>\n      <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">average_reward_window</span><span class=\"p\">.</span><span class=\"nx\">add</span><span class=\"p\">(</span><span class=\"nx\">reward</span><span class=\"p\">);</span>\n      <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">reward_window</span><span class=\"p\">.</span><span class=\"nx\">shift</span><span class=\"p\">();</span>\n      <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">reward_window</span><span class=\"p\">.</span><span class=\"nx\">push</span><span class=\"p\">(</span><span class=\"nx\">reward</span><span class=\"p\">);</span>\n      \n      <span class=\"k\">if</span><span class=\"p\">(</span><span class=\"o\">!</span><span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">learning</span><span class=\"p\">)</span> <span class=\"p\">{</span> <span class=\"k\">return</span><span class=\"p\">;</span> <span class=\"p\">}</span> \n      \n      <span class=\"c1\">// various book-keeping\n</span><span class=\"c1\"></span>      <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">age</span> <span class=\"o\">+=</span> <span class=\"mi\">1</span><span class=\"p\">;</span>\n      \n      <span class=\"c1\">// it is time t+1 and we have to store (s_t, a_t, r_t, s_{t+1}) as new experience\n</span><span class=\"c1\"></span>      <span class=\"c1\">// (given that an appropriate number of state measurements already exist, of course)\n</span><span class=\"c1\"></span>      <span class=\"k\">if</span><span class=\"p\">(</span><span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">forward_passes</span> <span class=\"o\">&gt;</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">temporal_window</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n        <span class=\"kd\">var</span> <span class=\"nx\">e</span> <span class=\"o\">=</span> <span class=\"k\">new</span> <span class=\"nx\">Experience</span><span class=\"p\">();</span>\n        <span class=\"kd\">var</span> <span class=\"nx\">n</span> <span class=\"o\">=</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">window_size</span><span class=\"p\">;</span>\n        <span class=\"nx\">e</span><span class=\"p\">.</span><span class=\"nx\">state0</span> <span class=\"o\">=</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">net_window</span><span class=\"p\">[</span><span class=\"nx\">n</span><span class=\"o\">-</span><span class=\"mi\">2</span><span class=\"p\">];</span>\n        <span class=\"nx\">e</span><span class=\"p\">.</span><span class=\"nx\">action0</span> <span class=\"o\">=</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">action_window</span><span class=\"p\">[</span><span class=\"nx\">n</span><span class=\"o\">-</span><span class=\"mi\">2</span><span class=\"p\">];</span>\n        <span class=\"nx\">e</span><span class=\"p\">.</span><span class=\"nx\">reward0</span> <span class=\"o\">=</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">reward_window</span><span class=\"p\">[</span><span class=\"nx\">n</span><span class=\"o\">-</span><span class=\"mi\">2</span><span class=\"p\">];</span>\n        <span class=\"nx\">e</span><span class=\"p\">.</span><span class=\"nx\">state1</span> <span class=\"o\">=</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">net_window</span><span class=\"p\">[</span><span class=\"nx\">n</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">];</span>\n        <span class=\"k\">if</span><span class=\"p\">(</span><span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">experience</span><span class=\"p\">.</span><span class=\"nx\">length</span> <span class=\"o\">&lt;</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">experience_size</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n          <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">experience</span><span class=\"p\">.</span><span class=\"nx\">push</span><span class=\"p\">(</span><span class=\"nx\">e</span><span class=\"p\">);</span>\n        <span class=\"p\">}</span> <span class=\"k\">else</span> <span class=\"p\">{</span>\n          <span class=\"c1\">// replace. finite memory!\n</span><span class=\"c1\"></span>          <span class=\"kd\">var</span> <span class=\"nx\">ri</span> <span class=\"o\">=</span> <span class=\"nx\">convnetjs</span><span class=\"p\">.</span><span class=\"nx\">randi</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">experience_size</span><span class=\"p\">);</span>\n          <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">experience</span><span class=\"p\">[</span><span class=\"nx\">ri</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"nx\">e</span><span class=\"p\">;</span>\n        <span class=\"p\">}</span>\n      <span class=\"p\">}</span>\n      \n      <span class=\"c1\">// learn based on experience, once we have some samples to go on\n</span><span class=\"c1\"></span>      <span class=\"c1\">// this is where the magic happens...\n</span><span class=\"c1\"></span>      <span class=\"c1\">// 当经验元组的缓存经验大于开始阀值时才开始训练\n</span><span class=\"c1\"></span>      <span class=\"k\">if</span><span class=\"p\">(</span><span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">experience</span><span class=\"p\">.</span><span class=\"nx\">length</span> <span class=\"o\">&gt;</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">start_learn_threshold</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n        <span class=\"kd\">var</span> <span class=\"nx\">avcost</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span><span class=\"p\">;</span>\n        <span class=\"k\">for</span><span class=\"p\">(</span><span class=\"kd\">var</span> <span class=\"nx\">k</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">;</span><span class=\"nx\">k</span> <span class=\"o\">&lt;</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">tdtrainer</span><span class=\"p\">.</span><span class=\"nx\">batch_size</span><span class=\"p\">;</span><span class=\"nx\">k</span><span class=\"o\">++</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n          <span class=\"c1\">//re是随机数下标\n</span><span class=\"c1\"></span>          <span class=\"kd\">var</span> <span class=\"nx\">re</span> <span class=\"o\">=</span> <span class=\"nx\">convnetjs</span><span class=\"p\">.</span><span class=\"nx\">randi</span><span class=\"p\">(</span><span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">experience</span><span class=\"p\">.</span><span class=\"nx\">length</span><span class=\"p\">);</span>\n          <span class=\"c1\">//e是随机经验元组\n</span><span class=\"c1\"></span>          <span class=\"kd\">var</span> <span class=\"nx\">e</span> <span class=\"o\">=</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">experience</span><span class=\"p\">[</span><span class=\"nx\">re</span><span class=\"p\">];</span>\n          <span class=\"kd\">var</span> <span class=\"nx\">x</span> <span class=\"o\">=</span> <span class=\"k\">new</span> <span class=\"nx\">convnetjs</span><span class=\"p\">.</span><span class=\"nx\">Vol</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">net_inputs</span><span class=\"p\">);</span>\n          <span class=\"nx\">x</span><span class=\"p\">.</span><span class=\"nx\">w</span> <span class=\"o\">=</span> <span class=\"nx\">e</span><span class=\"p\">.</span><span class=\"nx\">state0</span><span class=\"p\">;</span>\n          <span class=\"c1\">//用policy函数生成最优策略\n</span><span class=\"c1\"></span>          <span class=\"kd\">var</span> <span class=\"nx\">maxact</span> <span class=\"o\">=</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">policy</span><span class=\"p\">(</span><span class=\"nx\">e</span><span class=\"p\">.</span><span class=\"nx\">state1</span><span class=\"p\">);</span>\n          <span class=\"c1\">//r就是TD目标\n</span><span class=\"c1\"></span>          <span class=\"kd\">var</span> <span class=\"nx\">r</span> <span class=\"o\">=</span> <span class=\"nx\">e</span><span class=\"p\">.</span><span class=\"nx\">reward0</span> <span class=\"o\">+</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">gamma</span> <span class=\"o\">*</span> <span class=\"nx\">maxact</span><span class=\"p\">.</span><span class=\"nx\">value</span><span class=\"p\">;</span>\n          <span class=\"kd\">var</span> <span class=\"nx\">ystruct</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"nx\">dim</span><span class=\"o\">:</span> <span class=\"nx\">e</span><span class=\"p\">.</span><span class=\"nx\">action0</span><span class=\"p\">,</span> <span class=\"nx\">val</span><span class=\"o\">:</span> <span class=\"nx\">r</span><span class=\"p\">};</span>\n          <span class=\"c1\">//\n</span><span class=\"c1\"></span>          <span class=\"kd\">var</span> <span class=\"nx\">loss</span> <span class=\"o\">=</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">tdtrainer</span><span class=\"p\">.</span><span class=\"nx\">train</span><span class=\"p\">(</span><span class=\"nx\">x</span><span class=\"p\">,</span> <span class=\"nx\">ystruct</span><span class=\"p\">);</span>\n          <span class=\"nx\">avcost</span> <span class=\"o\">+=</span> <span class=\"nx\">loss</span><span class=\"p\">.</span><span class=\"nx\">loss</span><span class=\"p\">;</span>\n        <span class=\"p\">}</span>\n        <span class=\"nx\">avcost</span> <span class=\"o\">=</span> <span class=\"nx\">avcost</span><span class=\"o\">/</span><span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">tdtrainer</span><span class=\"p\">.</span><span class=\"nx\">batch_size</span><span class=\"p\">;</span>\n        <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">average_loss_window</span><span class=\"p\">.</span><span class=\"nx\">add</span><span class=\"p\">(</span><span class=\"nx\">avcost</span><span class=\"p\">);</span>\n      <span class=\"p\">}</span>\n    <span class=\"p\">},</span>\n</code></pre></div><p>代码说明：</p><ul><li>英文注释为源码注释</li><li>中文注释为笔者的注释</li></ul><p>从backward函数可知，在训练时确实使用了经验回放。</p><p>而固定Q目标的本质是预测时所用的函数Q与实际计算的TD目标时所用的函数Q&#39;不是同一个函数。那源代码里是什么状况呢？从上述的backward函数可知，每次训练是先从policy函数生成一个maxact对象，然后再根据TD目标的计算公式计算目标函数值ystruct.val。查看policy函数有：</p><div class=\"highlight\"><pre><code class=\"language-js\"><span class=\"c1\">//摘自deepqlearn.js\n</span><span class=\"c1\"></span><span class=\"nx\">policy</span><span class=\"o\">:</span> <span class=\"kd\">function</span><span class=\"p\">(</span><span class=\"nx\">s</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n      <span class=\"c1\">// compute the value of doing any action in this state\n</span><span class=\"c1\"></span>      <span class=\"c1\">// and return the argmax action and its value\n</span><span class=\"c1\"></span>      <span class=\"kd\">var</span> <span class=\"nx\">svol</span> <span class=\"o\">=</span> <span class=\"k\">new</span> <span class=\"nx\">convnetjs</span><span class=\"p\">.</span><span class=\"nx\">Vol</span><span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">net_inputs</span><span class=\"p\">);</span>\n      <span class=\"nx\">svol</span><span class=\"p\">.</span><span class=\"nx\">w</span> <span class=\"o\">=</span> <span class=\"nx\">s</span><span class=\"p\">;</span>\n      <span class=\"c1\">//以当前的值函数进行计算\n</span><span class=\"c1\"></span>      <span class=\"kd\">var</span> <span class=\"nx\">action_values</span> <span class=\"o\">=</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">value_net</span><span class=\"p\">.</span><span class=\"nx\">forward</span><span class=\"p\">(</span><span class=\"nx\">svol</span><span class=\"p\">);</span>\n      <span class=\"kd\">var</span> <span class=\"nx\">maxk</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">;</span> \n      <span class=\"kd\">var</span> <span class=\"nx\">maxval</span> <span class=\"o\">=</span> <span class=\"nx\">action_values</span><span class=\"p\">.</span><span class=\"nx\">w</span><span class=\"p\">[</span><span class=\"mi\">0</span><span class=\"p\">];</span>\n      <span class=\"k\">for</span><span class=\"p\">(</span><span class=\"kd\">var</span> <span class=\"nx\">k</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">;</span><span class=\"nx\">k</span><span class=\"o\">&lt;</span><span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">num_actions</span><span class=\"p\">;</span><span class=\"nx\">k</span><span class=\"o\">++</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n        <span class=\"k\">if</span><span class=\"p\">(</span><span class=\"nx\">action_values</span><span class=\"p\">.</span><span class=\"nx\">w</span><span class=\"p\">[</span><span class=\"nx\">k</span><span class=\"p\">]</span> <span class=\"o\">&gt;</span> <span class=\"nx\">maxval</span><span class=\"p\">)</span> <span class=\"p\">{</span> <span class=\"nx\">maxk</span> <span class=\"o\">=</span> <span class=\"nx\">k</span><span class=\"p\">;</span> <span class=\"nx\">maxval</span> <span class=\"o\">=</span> <span class=\"nx\">action_values</span><span class=\"p\">.</span><span class=\"nx\">w</span><span class=\"p\">[</span><span class=\"nx\">k</span><span class=\"p\">];</span> <span class=\"p\">}</span>\n      <span class=\"p\">}</span>\n      <span class=\"k\">return</span> <span class=\"p\">{</span><span class=\"nx\">action</span><span class=\"o\">:</span><span class=\"nx\">maxk</span><span class=\"p\">,</span> <span class=\"nx\">value</span><span class=\"o\">:</span><span class=\"nx\">maxval</span><span class=\"p\">};</span>\n    <span class=\"p\">},</span>\n</code></pre></div><p>再来查查this.value_net是什么：</p><div class=\"highlight\"><pre><code class=\"language-js\"><span class=\"c1\">//摘自deepqlearn.js\n</span><span class=\"c1\"></span>    <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">value_net</span> <span class=\"o\">=</span> <span class=\"k\">new</span> <span class=\"nx\">convnetjs</span><span class=\"p\">.</span><span class=\"nx\">Net</span><span class=\"p\">();</span>\n    <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">value_net</span><span class=\"p\">.</span><span class=\"nx\">makeLayers</span><span class=\"p\">(</span><span class=\"nx\">layer_defs</span><span class=\"p\">);</span>\n    \n    <span class=\"c1\">// and finally we need a Temporal Difference Learning trainer!\n</span><span class=\"c1\"></span>    <span class=\"kd\">var</span> <span class=\"nx\">tdtrainer_options</span> <span class=\"o\">=</span> <span class=\"p\">{</span><span class=\"nx\">learning_rate</span><span class=\"o\">:</span><span class=\"mf\">0.01</span><span class=\"p\">,</span> <span class=\"nx\">momentum</span><span class=\"o\">:</span><span class=\"mf\">0.0</span><span class=\"p\">,</span> <span class=\"nx\">batch_size</span><span class=\"o\">:</span><span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"nx\">l2_decay</span><span class=\"o\">:</span><span class=\"mf\">0.01</span><span class=\"p\">};</span>\n    <span class=\"k\">if</span><span class=\"p\">(</span><span class=\"k\">typeof</span> <span class=\"nx\">opt</span><span class=\"p\">.</span><span class=\"nx\">tdtrainer_options</span> <span class=\"o\">!==</span> <span class=\"s1\">&#39;undefined&#39;</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n      <span class=\"nx\">tdtrainer_options</span> <span class=\"o\">=</span> <span class=\"nx\">opt</span><span class=\"p\">.</span><span class=\"nx\">tdtrainer_options</span><span class=\"p\">;</span> <span class=\"c1\">// allow user to overwrite this\n</span><span class=\"c1\"></span>    <span class=\"p\">}</span>\n    <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">tdtrainer</span> <span class=\"o\">=</span> <span class=\"k\">new</span> <span class=\"nx\">convnetjs</span><span class=\"p\">.</span><span class=\"nx\">SGDTrainer</span><span class=\"p\">(</span><span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">value_net</span><span class=\"p\">,</span> <span class=\"nx\">tdtrainer_options</span><span class=\"p\">);</span>\n</code></pre></div><p>可以看到this.value_net实际上就是this.tdtrainer封装的网络，那么还要再深挖tdtrainer内部研究下才能下定论。</p><div class=\"highlight\"><pre><code class=\"language-js\"><span class=\"c1\">//摘自convnet.js\n</span><span class=\"c1\"></span><span class=\"nx\">train</span><span class=\"o\">:</span> <span class=\"kd\">function</span><span class=\"p\">(</span><span class=\"nx\">x</span><span class=\"p\">,</span> <span class=\"nx\">y</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n\n      <span class=\"kd\">var</span> <span class=\"nx\">start</span> <span class=\"o\">=</span> <span class=\"k\">new</span> <span class=\"nb\">Date</span><span class=\"p\">().</span><span class=\"nx\">getTime</span><span class=\"p\">();</span>\n      <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">net</span><span class=\"p\">.</span><span class=\"nx\">forward</span><span class=\"p\">(</span><span class=\"nx\">x</span><span class=\"p\">,</span> <span class=\"kc\">true</span><span class=\"p\">);</span> <span class=\"c1\">// also set the flag that lets the net know we&#39;re just training\n</span><span class=\"c1\"></span>      <span class=\"kd\">var</span> <span class=\"nx\">end</span> <span class=\"o\">=</span> <span class=\"k\">new</span> <span class=\"nb\">Date</span><span class=\"p\">().</span><span class=\"nx\">getTime</span><span class=\"p\">();</span>\n      <span class=\"kd\">var</span> <span class=\"nx\">fwd_time</span> <span class=\"o\">=</span> <span class=\"nx\">end</span> <span class=\"o\">-</span> <span class=\"nx\">start</span><span class=\"p\">;</span>\n\n      <span class=\"kd\">var</span> <span class=\"nx\">start</span> <span class=\"o\">=</span> <span class=\"k\">new</span> <span class=\"nb\">Date</span><span class=\"p\">().</span><span class=\"nx\">getTime</span><span class=\"p\">();</span>\n      <span class=\"kd\">var</span> <span class=\"nx\">cost_loss</span> <span class=\"o\">=</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">net</span><span class=\"p\">.</span><span class=\"nx\">backward</span><span class=\"p\">(</span><span class=\"nx\">y</span><span class=\"p\">);</span>\n      <span class=\"kd\">var</span> <span class=\"nx\">l2_decay_loss</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span><span class=\"p\">;</span>\n      <span class=\"kd\">var</span> <span class=\"nx\">l1_decay_loss</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span><span class=\"p\">;</span>\n      <span class=\"kd\">var</span> <span class=\"nx\">end</span> <span class=\"o\">=</span> <span class=\"k\">new</span> <span class=\"nb\">Date</span><span class=\"p\">().</span><span class=\"nx\">getTime</span><span class=\"p\">();</span>\n      <span class=\"kd\">var</span> <span class=\"nx\">bwd_time</span> <span class=\"o\">=</span> <span class=\"nx\">end</span> <span class=\"o\">-</span> <span class=\"nx\">start</span><span class=\"p\">;</span>\n      \n      <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">k</span><span class=\"o\">++</span><span class=\"p\">;</span>\n      <span class=\"k\">if</span><span class=\"p\">(</span><span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">k</span> <span class=\"o\">%</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">batch_size</span> <span class=\"o\">===</span> <span class=\"mi\">0</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n\n        <span class=\"kd\">var</span> <span class=\"nx\">pglist</span> <span class=\"o\">=</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">net</span><span class=\"p\">.</span><span class=\"nx\">getParamsAndGrads</span><span class=\"p\">();</span>\n\n        <span class=\"c1\">// initialize lists for accumulators. Will only be done once on first iteration\n</span><span class=\"c1\"></span>        <span class=\"k\">if</span><span class=\"p\">(</span><span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">gsum</span><span class=\"p\">.</span><span class=\"nx\">length</span> <span class=\"o\">===</span> <span class=\"mi\">0</span> <span class=\"o\">&amp;&amp;</span> <span class=\"p\">(</span><span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">method</span> <span class=\"o\">!==</span> <span class=\"s1\">&#39;sgd&#39;</span> <span class=\"o\">||</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">momentum</span> <span class=\"o\">&gt;</span> <span class=\"mf\">0.0</span><span class=\"p\">))</span> <span class=\"p\">{</span>\n          <span class=\"c1\">// only vanilla sgd doesnt need either lists\n</span><span class=\"c1\"></span>          <span class=\"c1\">// momentum needs gsum\n</span><span class=\"c1\"></span>          <span class=\"c1\">// adagrad needs gsum\n</span><span class=\"c1\"></span>          <span class=\"c1\">// adadelta needs gsum and xsum\n</span><span class=\"c1\"></span>          <span class=\"k\">for</span><span class=\"p\">(</span><span class=\"kd\">var</span> <span class=\"nx\">i</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">;</span><span class=\"nx\">i</span><span class=\"o\">&lt;</span><span class=\"nx\">pglist</span><span class=\"p\">.</span><span class=\"nx\">length</span><span class=\"p\">;</span><span class=\"nx\">i</span><span class=\"o\">++</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n            <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">gsum</span><span class=\"p\">.</span><span class=\"nx\">push</span><span class=\"p\">(</span><span class=\"nx\">global</span><span class=\"p\">.</span><span class=\"nx\">zeros</span><span class=\"p\">(</span><span class=\"nx\">pglist</span><span class=\"p\">[</span><span class=\"nx\">i</span><span class=\"p\">].</span><span class=\"nx\">params</span><span class=\"p\">.</span><span class=\"nx\">length</span><span class=\"p\">));</span>\n            <span class=\"k\">if</span><span class=\"p\">(</span><span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">method</span> <span class=\"o\">===</span> <span class=\"s1\">&#39;adadelta&#39;</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n              <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">xsum</span><span class=\"p\">.</span><span class=\"nx\">push</span><span class=\"p\">(</span><span class=\"nx\">global</span><span class=\"p\">.</span><span class=\"nx\">zeros</span><span class=\"p\">(</span><span class=\"nx\">pglist</span><span class=\"p\">[</span><span class=\"nx\">i</span><span class=\"p\">].</span><span class=\"nx\">params</span><span class=\"p\">.</span><span class=\"nx\">length</span><span class=\"p\">));</span>\n            <span class=\"p\">}</span> <span class=\"k\">else</span> <span class=\"p\">{</span>\n              <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">xsum</span><span class=\"p\">.</span><span class=\"nx\">push</span><span class=\"p\">([]);</span> <span class=\"c1\">// conserve memory\n</span><span class=\"c1\"></span>            <span class=\"p\">}</span>\n          <span class=\"p\">}</span>\n        <span class=\"p\">}</span>\n\n        <span class=\"c1\">// perform an update for all sets of weights\n</span><span class=\"c1\"></span>        <span class=\"k\">for</span><span class=\"p\">(</span><span class=\"kd\">var</span> <span class=\"nx\">i</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">;</span><span class=\"nx\">i</span><span class=\"o\">&lt;</span><span class=\"nx\">pglist</span><span class=\"p\">.</span><span class=\"nx\">length</span><span class=\"p\">;</span><span class=\"nx\">i</span><span class=\"o\">++</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n          <span class=\"kd\">var</span> <span class=\"nx\">pg</span> <span class=\"o\">=</span> <span class=\"nx\">pglist</span><span class=\"p\">[</span><span class=\"nx\">i</span><span class=\"p\">];</span> <span class=\"c1\">// param, gradient, other options in future (custom learning rate etc)\n</span><span class=\"c1\"></span>          <span class=\"kd\">var</span> <span class=\"nx\">p</span> <span class=\"o\">=</span> <span class=\"nx\">pg</span><span class=\"p\">.</span><span class=\"nx\">params</span><span class=\"p\">;</span>\n          <span class=\"kd\">var</span> <span class=\"nx\">g</span> <span class=\"o\">=</span> <span class=\"nx\">pg</span><span class=\"p\">.</span><span class=\"nx\">grads</span><span class=\"p\">;</span>\n\n          <span class=\"c1\">// learning rate for some parameters.\n</span><span class=\"c1\"></span>          <span class=\"kd\">var</span> <span class=\"nx\">l2_decay_mul</span> <span class=\"o\">=</span> <span class=\"k\">typeof</span> <span class=\"nx\">pg</span><span class=\"p\">.</span><span class=\"nx\">l2_decay_mul</span> <span class=\"o\">!==</span> <span class=\"s1\">&#39;undefined&#39;</span> <span class=\"o\">?</span> <span class=\"nx\">pg</span><span class=\"p\">.</span><span class=\"nx\">l2_decay_mul</span> <span class=\"o\">:</span> <span class=\"mf\">1.0</span><span class=\"p\">;</span>\n          <span class=\"kd\">var</span> <span class=\"nx\">l1_decay_mul</span> <span class=\"o\">=</span> <span class=\"k\">typeof</span> <span class=\"nx\">pg</span><span class=\"p\">.</span><span class=\"nx\">l1_decay_mul</span> <span class=\"o\">!==</span> <span class=\"s1\">&#39;undefined&#39;</span> <span class=\"o\">?</span> <span class=\"nx\">pg</span><span class=\"p\">.</span><span class=\"nx\">l1_decay_mul</span> <span class=\"o\">:</span> <span class=\"mf\">1.0</span><span class=\"p\">;</span>\n          <span class=\"kd\">var</span> <span class=\"nx\">l2_decay</span> <span class=\"o\">=</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">l2_decay</span> <span class=\"o\">*</span> <span class=\"nx\">l2_decay_mul</span><span class=\"p\">;</span>\n          <span class=\"kd\">var</span> <span class=\"nx\">l1_decay</span> <span class=\"o\">=</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">l1_decay</span> <span class=\"o\">*</span> <span class=\"nx\">l1_decay_mul</span><span class=\"p\">;</span>\n\n          <span class=\"kd\">var</span> <span class=\"nx\">plen</span> <span class=\"o\">=</span> <span class=\"nx\">p</span><span class=\"p\">.</span><span class=\"nx\">length</span><span class=\"p\">;</span>\n          <span class=\"k\">for</span><span class=\"p\">(</span><span class=\"kd\">var</span> <span class=\"nx\">j</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">;</span><span class=\"nx\">j</span><span class=\"o\">&lt;</span><span class=\"nx\">plen</span><span class=\"p\">;</span><span class=\"nx\">j</span><span class=\"o\">++</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n            <span class=\"nx\">l2_decay_loss</span> <span class=\"o\">+=</span> <span class=\"nx\">l2_decay</span><span class=\"o\">*</span><span class=\"nx\">p</span><span class=\"p\">[</span><span class=\"nx\">j</span><span class=\"p\">]</span><span class=\"o\">*</span><span class=\"nx\">p</span><span class=\"p\">[</span><span class=\"nx\">j</span><span class=\"p\">]</span><span class=\"o\">/</span><span class=\"mi\">2</span><span class=\"p\">;</span> <span class=\"c1\">// accumulate weight decay loss\n</span><span class=\"c1\"></span>            <span class=\"nx\">l1_decay_loss</span> <span class=\"o\">+=</span> <span class=\"nx\">l1_decay</span><span class=\"o\">*</span><span class=\"nb\">Math</span><span class=\"p\">.</span><span class=\"nx\">abs</span><span class=\"p\">(</span><span class=\"nx\">p</span><span class=\"p\">[</span><span class=\"nx\">j</span><span class=\"p\">]);</span>\n            <span class=\"kd\">var</span> <span class=\"nx\">l1grad</span> <span class=\"o\">=</span> <span class=\"nx\">l1_decay</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"nx\">p</span><span class=\"p\">[</span><span class=\"nx\">j</span><span class=\"p\">]</span> <span class=\"o\">&gt;</span> <span class=\"mi\">0</span> <span class=\"o\">?</span> <span class=\"mi\">1</span> <span class=\"o\">:</span> <span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">);</span>\n            <span class=\"kd\">var</span> <span class=\"nx\">l2grad</span> <span class=\"o\">=</span> <span class=\"nx\">l2_decay</span> <span class=\"o\">*</span> <span class=\"p\">(</span><span class=\"nx\">p</span><span class=\"p\">[</span><span class=\"nx\">j</span><span class=\"p\">]);</span>\n\n            <span class=\"kd\">var</span> <span class=\"nx\">gij</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"nx\">l2grad</span> <span class=\"o\">+</span> <span class=\"nx\">l1grad</span> <span class=\"o\">+</span> <span class=\"nx\">g</span><span class=\"p\">[</span><span class=\"nx\">j</span><span class=\"p\">])</span> <span class=\"o\">/</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">batch_size</span><span class=\"p\">;</span> <span class=\"c1\">// raw batch gradient\n</span><span class=\"c1\"></span>\n            <span class=\"kd\">var</span> <span class=\"nx\">gsumi</span> <span class=\"o\">=</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">gsum</span><span class=\"p\">[</span><span class=\"nx\">i</span><span class=\"p\">];</span>\n            <span class=\"kd\">var</span> <span class=\"nx\">xsumi</span> <span class=\"o\">=</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">xsum</span><span class=\"p\">[</span><span class=\"nx\">i</span><span class=\"p\">];</span>\n            <span class=\"k\">if</span><span class=\"p\">(</span><span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">method</span> <span class=\"o\">===</span> <span class=\"s1\">&#39;adagrad&#39;</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n              <span class=\"c1\">// adagrad update\n</span><span class=\"c1\"></span>              <span class=\"nx\">gsumi</span><span class=\"p\">[</span><span class=\"nx\">j</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"nx\">gsumi</span><span class=\"p\">[</span><span class=\"nx\">j</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"nx\">gij</span> <span class=\"o\">*</span> <span class=\"nx\">gij</span><span class=\"p\">;</span>\n              <span class=\"kd\">var</span> <span class=\"nx\">dx</span> <span class=\"o\">=</span> <span class=\"o\">-</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">learning_rate</span> <span class=\"o\">/</span> <span class=\"nb\">Math</span><span class=\"p\">.</span><span class=\"nx\">sqrt</span><span class=\"p\">(</span><span class=\"nx\">gsumi</span><span class=\"p\">[</span><span class=\"nx\">j</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">eps</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"nx\">gij</span><span class=\"p\">;</span>\n              <span class=\"nx\">p</span><span class=\"p\">[</span><span class=\"nx\">j</span><span class=\"p\">]</span> <span class=\"o\">+=</span> <span class=\"nx\">dx</span><span class=\"p\">;</span>\n            <span class=\"p\">}</span> <span class=\"k\">else</span> <span class=\"k\">if</span><span class=\"p\">(</span><span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">method</span> <span class=\"o\">===</span> <span class=\"s1\">&#39;windowgrad&#39;</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n              <span class=\"c1\">// this is adagrad but with a moving window weighted average\n</span><span class=\"c1\"></span>              <span class=\"c1\">// so the gradient is not accumulated over the entire history of the run. \n</span><span class=\"c1\"></span>              <span class=\"c1\">// it&#39;s also referred to as Idea #1 in Zeiler paper on Adadelta. Seems reasonable to me!\n</span><span class=\"c1\"></span>              <span class=\"nx\">gsumi</span><span class=\"p\">[</span><span class=\"nx\">j</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">ro</span> <span class=\"o\">*</span> <span class=\"nx\">gsumi</span><span class=\"p\">[</span><span class=\"nx\">j</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"o\">-</span><span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">ro</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"nx\">gij</span> <span class=\"o\">*</span> <span class=\"nx\">gij</span><span class=\"p\">;</span>\n              <span class=\"kd\">var</span> <span class=\"nx\">dx</span> <span class=\"o\">=</span> <span class=\"o\">-</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">learning_rate</span> <span class=\"o\">/</span> <span class=\"nb\">Math</span><span class=\"p\">.</span><span class=\"nx\">sqrt</span><span class=\"p\">(</span><span class=\"nx\">gsumi</span><span class=\"p\">[</span><span class=\"nx\">j</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">eps</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"nx\">gij</span><span class=\"p\">;</span> <span class=\"c1\">// eps added for better conditioning\n</span><span class=\"c1\"></span>              <span class=\"nx\">p</span><span class=\"p\">[</span><span class=\"nx\">j</span><span class=\"p\">]</span> <span class=\"o\">+=</span> <span class=\"nx\">dx</span><span class=\"p\">;</span>\n            <span class=\"p\">}</span> <span class=\"k\">else</span> <span class=\"k\">if</span><span class=\"p\">(</span><span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">method</span> <span class=\"o\">===</span> <span class=\"s1\">&#39;adadelta&#39;</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n              <span class=\"c1\">// assume adadelta if not sgd or adagrad\n</span><span class=\"c1\"></span>              <span class=\"nx\">gsumi</span><span class=\"p\">[</span><span class=\"nx\">j</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">ro</span> <span class=\"o\">*</span> <span class=\"nx\">gsumi</span><span class=\"p\">[</span><span class=\"nx\">j</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"o\">-</span><span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">ro</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"nx\">gij</span> <span class=\"o\">*</span> <span class=\"nx\">gij</span><span class=\"p\">;</span>\n              <span class=\"kd\">var</span> <span class=\"nx\">dx</span> <span class=\"o\">=</span> <span class=\"o\">-</span> <span class=\"nb\">Math</span><span class=\"p\">.</span><span class=\"nx\">sqrt</span><span class=\"p\">((</span><span class=\"nx\">xsumi</span><span class=\"p\">[</span><span class=\"nx\">j</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">eps</span><span class=\"p\">)</span><span class=\"o\">/</span><span class=\"p\">(</span><span class=\"nx\">gsumi</span><span class=\"p\">[</span><span class=\"nx\">j</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">eps</span><span class=\"p\">))</span> <span class=\"o\">*</span> <span class=\"nx\">gij</span><span class=\"p\">;</span>\n              <span class=\"nx\">xsumi</span><span class=\"p\">[</span><span class=\"nx\">j</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">ro</span> <span class=\"o\">*</span> <span class=\"nx\">xsumi</span><span class=\"p\">[</span><span class=\"nx\">j</span><span class=\"p\">]</span> <span class=\"o\">+</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"o\">-</span><span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">ro</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"nx\">dx</span> <span class=\"o\">*</span> <span class=\"nx\">dx</span><span class=\"p\">;</span> <span class=\"c1\">// yes, xsum lags behind gsum by 1.\n</span><span class=\"c1\"></span>              <span class=\"nx\">p</span><span class=\"p\">[</span><span class=\"nx\">j</span><span class=\"p\">]</span> <span class=\"o\">+=</span> <span class=\"nx\">dx</span><span class=\"p\">;</span>\n            <span class=\"p\">}</span> <span class=\"k\">else</span> <span class=\"k\">if</span><span class=\"p\">(</span><span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">method</span> <span class=\"o\">===</span> <span class=\"s1\">&#39;nesterov&#39;</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n            \t<span class=\"kd\">var</span> <span class=\"nx\">dx</span> <span class=\"o\">=</span> <span class=\"nx\">gsumi</span><span class=\"p\">[</span><span class=\"nx\">j</span><span class=\"p\">];</span>\n            \t<span class=\"nx\">gsumi</span><span class=\"p\">[</span><span class=\"nx\">j</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"nx\">gsumi</span><span class=\"p\">[</span><span class=\"nx\">j</span><span class=\"p\">]</span> <span class=\"o\">*</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">momentum</span> <span class=\"o\">+</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">learning_rate</span> <span class=\"o\">*</span> <span class=\"nx\">gij</span><span class=\"p\">;</span>\n                <span class=\"nx\">dx</span> <span class=\"o\">=</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">momentum</span> <span class=\"o\">*</span> <span class=\"nx\">dx</span> <span class=\"o\">-</span> <span class=\"p\">(</span><span class=\"mf\">1.0</span> <span class=\"o\">+</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">momentum</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"nx\">gsumi</span><span class=\"p\">[</span><span class=\"nx\">j</span><span class=\"p\">];</span>\n                <span class=\"nx\">p</span><span class=\"p\">[</span><span class=\"nx\">j</span><span class=\"p\">]</span> <span class=\"o\">+=</span> <span class=\"nx\">dx</span><span class=\"p\">;</span>\n            <span class=\"p\">}</span> <span class=\"k\">else</span> <span class=\"p\">{</span>\n              <span class=\"c1\">// assume SGD\n</span><span class=\"c1\"></span>              <span class=\"k\">if</span><span class=\"p\">(</span><span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">momentum</span> <span class=\"o\">&gt;</span> <span class=\"mf\">0.0</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n                <span class=\"c1\">// momentum update\n</span><span class=\"c1\"></span>                <span class=\"kd\">var</span> <span class=\"nx\">dx</span> <span class=\"o\">=</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">momentum</span> <span class=\"o\">*</span> <span class=\"nx\">gsumi</span><span class=\"p\">[</span><span class=\"nx\">j</span><span class=\"p\">]</span> <span class=\"o\">-</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">learning_rate</span> <span class=\"o\">*</span> <span class=\"nx\">gij</span><span class=\"p\">;</span> <span class=\"c1\">// step\n</span><span class=\"c1\"></span>                <span class=\"nx\">gsumi</span><span class=\"p\">[</span><span class=\"nx\">j</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"nx\">dx</span><span class=\"p\">;</span> <span class=\"c1\">// back this up for next iteration of momentum\n</span><span class=\"c1\"></span>                <span class=\"nx\">p</span><span class=\"p\">[</span><span class=\"nx\">j</span><span class=\"p\">]</span> <span class=\"o\">+=</span> <span class=\"nx\">dx</span><span class=\"p\">;</span> <span class=\"c1\">// apply corrected gradient\n</span><span class=\"c1\"></span>              <span class=\"p\">}</span> <span class=\"k\">else</span> <span class=\"p\">{</span>\n                <span class=\"c1\">// vanilla sgd\n</span><span class=\"c1\"></span>                <span class=\"nx\">p</span><span class=\"p\">[</span><span class=\"nx\">j</span><span class=\"p\">]</span> <span class=\"o\">+=</span>  <span class=\"o\">-</span> <span class=\"k\">this</span><span class=\"p\">.</span><span class=\"nx\">learning_rate</span> <span class=\"o\">*</span> <span class=\"nx\">gij</span><span class=\"p\">;</span>\n              <span class=\"p\">}</span>\n            <span class=\"p\">}</span>\n            <span class=\"nx\">g</span><span class=\"p\">[</span><span class=\"nx\">j</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mf\">0.0</span><span class=\"p\">;</span> <span class=\"c1\">// zero out gradient so that we can begin accumulating anew\n</span><span class=\"c1\"></span>          <span class=\"p\">}</span>\n        <span class=\"p\">}</span>\n      <span class=\"p\">}</span>\n\n      <span class=\"c1\">// appending softmax_loss for backwards compatibility, but from now on we will always use cost_loss\n</span><span class=\"c1\"></span>      <span class=\"c1\">// in future, TODO: have to completely redo the way loss is done around the network as currently \n</span><span class=\"c1\"></span>      <span class=\"c1\">// loss is a bit of a hack. Ideally, user should specify arbitrary number of loss functions on any layer\n</span><span class=\"c1\"></span>      <span class=\"c1\">// and it should all be computed correctly and automatically. \n</span><span class=\"c1\"></span>      <span class=\"k\">return</span> <span class=\"p\">{</span><span class=\"nx\">fwd_time</span><span class=\"o\">:</span> <span class=\"nx\">fwd_time</span><span class=\"p\">,</span> <span class=\"nx\">bwd_time</span><span class=\"o\">:</span> <span class=\"nx\">bwd_time</span><span class=\"p\">,</span> \n              <span class=\"nx\">l2_decay_loss</span><span class=\"o\">:</span> <span class=\"nx\">l2_decay_loss</span><span class=\"p\">,</span> <span class=\"nx\">l1_decay_loss</span><span class=\"o\">:</span> <span class=\"nx\">l1_decay_loss</span><span class=\"p\">,</span>\n              <span class=\"nx\">cost_loss</span><span class=\"o\">:</span> <span class=\"nx\">cost_loss</span><span class=\"p\">,</span> <span class=\"nx\">softmax_loss</span><span class=\"o\">:</span> <span class=\"nx\">cost_loss</span><span class=\"p\">,</span> \n              <span class=\"nx\">loss</span><span class=\"o\">:</span> <span class=\"nx\">cost_loss</span> <span class=\"o\">+</span> <span class=\"nx\">l1_decay_loss</span> <span class=\"o\">+</span> <span class=\"nx\">l2_decay_loss</span><span class=\"p\">}</span>\n    <span class=\"p\">}</span>\n</code></pre></div><p>从源代码分析看，没有看到有相关的固定Q目标的处理。</p><p>虽然如此，但由于这对于所有人来讲都是一样的，也就是说，我们无法通过对网络训练过程进行优化来提高成绩，因此此问题可以忽略。</p><p class=\"ztext-empty-paragraph\"><br/></p><p>以上是对实战环境的分析与熟悉，基于以上的认识，我们再来构建我们自己的DQN，看看能让成绩提升多少。</p><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p>参考资料：</p><ol><li><a href=\"https://link.zhihu.com/?target=https%3A//selfdrivingcars.mit.edu/deeptraffic-about/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">DeepTraffic - About | MIT 6.S094: Deep Learning for Self-Driving Cars</a></li><li><a href=\"https://link.zhihu.com/?target=https%3A//cs.stanford.edu/people/karpathy/convnetjs/docs.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Deep Learning in your browser</a></li><li><a href=\"https://link.zhihu.com/?target=https%3A//cs.stanford.edu/people/karpathy/convnetjs/demo/rldemo.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">ConvNetJS Deep Q Learning Reinforcement Learning with Neural Network demo</a></li><li>程序网页中F12的源代码convnet.js, deepqlearn.js</li><li><a href=\"https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1801.02805\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Driving Fast through Dense Traffic with Deep Reinforcement Learning</a></li></ol><p></p><p></p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "强化学习 (Reinforcement Learning)", 
                    "tagLink": "https://api.zhihu.com/topics/20039099"
                }
            ], 
            "comments": [
                {
                    "userName": "wangjinxin", 
                    "userLink": "https://www.zhihu.com/people/d1815b96323bde3d0a1c67453beda950", 
                    "content": "<p>楼主好厉害呀</p>", 
                    "likes": 1, 
                    "childComments": []
                }, 
                {
                    "userName": "知乎用户", 
                    "userLink": "https://www.zhihu.com/people/0", 
                    "content": "<p>老司机开车咯～</p>", 
                    "likes": 0, 
                    "childComments": []
                }, 
                {
                    "userName": "张子豪", 
                    "userLink": "https://www.zhihu.com/people/2561332f7f81caba818db4b1150642ef", 
                    "content": "楼主，我要好好学习一下你这个，做一个视频出来给我的学生讲", 
                    "likes": 0, 
                    "childComments": []
                }
            ]
        }
    ], 
    "url": "https://zhuanlan.zhihu.com/c_185053605"
}
