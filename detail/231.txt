{
    "title": "DeepAI", 
    "description": "重点方向：深度学习、算法和模型、目标检测、图像处理；记录学习过程，分享和传播知识\n", 
    "followers": [
        "https://www.zhihu.com/people/cheng-jian-ming-80", 
        "https://www.zhihu.com/people/qi-yun-44-44", 
        "https://www.zhihu.com/people/shi-ying-jun-5", 
        "https://www.zhihu.com/people/satia", 
        "https://www.zhihu.com/people/zhong-shi-ang", 
        "https://www.zhihu.com/people/lyqqiang", 
        "https://www.zhihu.com/people/suixing", 
        "https://www.zhihu.com/people/mu-1-90", 
        "https://www.zhihu.com/people/jiang-jin-sheng-51", 
        "https://www.zhihu.com/people/yao-yan-xiao-37", 
        "https://www.zhihu.com/people/xu-pan-pan-38-13", 
        "https://www.zhihu.com/people/txa0515", 
        "https://www.zhihu.com/people/xiao-qing-61-37", 
        "https://www.zhihu.com/people/leo-lee-58-57", 
        "https://www.zhihu.com/people/cui-yue-63-33", 
        "https://www.zhihu.com/people/michael-wu-25", 
        "https://www.zhihu.com/people/tanfeng1990", 
        "https://www.zhihu.com/people/j_ping_96", 
        "https://www.zhihu.com/people/monkeyeric", 
        "https://www.zhihu.com/people/ding-bin-75-13", 
        "https://www.zhihu.com/people/pater", 
        "https://www.zhihu.com/people/shao-xian-sheng-85-59", 
        "https://www.zhihu.com/people/gao-xue-tong-33", 
        "https://www.zhihu.com/people/wang-chong-wen-92", 
        "https://www.zhihu.com/people/tu-zi-30-60-35", 
        "https://www.zhihu.com/people/hai-se-xiang-hao", 
        "https://www.zhihu.com/people/wei-xiao-xiang-qian-xing-19", 
        "https://www.zhihu.com/people/san-jun-sama", 
        "https://www.zhihu.com/people/ye-mo-liu-huo", 
        "https://www.zhihu.com/people/huang-xun-yi", 
        "https://www.zhihu.com/people/wu-ji-81", 
        "https://www.zhihu.com/people/joy586210", 
        "https://www.zhihu.com/people/mai-sui-73-64", 
        "https://www.zhihu.com/people/chai-bu-duo-21-48", 
        "https://www.zhihu.com/people/mu-mu-31-13-6", 
        "https://www.zhihu.com/people/zhangsan-97-39", 
        "https://www.zhihu.com/people/yly-29-71", 
        "https://www.zhihu.com/people/qiangwei-69", 
        "https://www.zhihu.com/people/han-li-feng-13", 
        "https://www.zhihu.com/people/Dr.Kaiser", 
        "https://www.zhihu.com/people/javales", 
        "https://www.zhihu.com/people/xuan-huang-80-32", 
        "https://www.zhihu.com/people/kacholy-cc", 
        "https://www.zhihu.com/people/zhong-xiao-li-86", 
        "https://www.zhihu.com/people/shi-yu-hao-Simon", 
        "https://www.zhihu.com/people/vina-19-62", 
        "https://www.zhihu.com/people/yu-hai-long-22", 
        "https://www.zhihu.com/people/xi-hong-shi-ji-dan-mian-46", 
        "https://www.zhihu.com/people/choiyeren", 
        "https://www.zhihu.com/people/xiaojidan", 
        "https://www.zhihu.com/people/liang-de-peng", 
        "https://www.zhihu.com/people/myuan", 
        "https://www.zhihu.com/people/bigpotatoc", 
        "https://www.zhihu.com/people/tao-sheng-yi-jiu-2-16", 
        "https://www.zhihu.com/people/sun-yuan-66-56", 
        "https://www.zhihu.com/people/si-kao-31-9", 
        "https://www.zhihu.com/people/wang-yichen-43-26", 
        "https://www.zhihu.com/people/zhang-gao-jian-89", 
        "https://www.zhihu.com/people/yu-zhou-60-88", 
        "https://www.zhihu.com/people/yu-yue-88-73", 
        "https://www.zhihu.com/people/wu-ze-wei-38", 
        "https://www.zhihu.com/people/han-yan-hua-13", 
        "https://www.zhihu.com/people/a-ben-14-33", 
        "https://www.zhihu.com/people/ba-li-dao-62", 
        "https://www.zhihu.com/people/ye-li-tiao-deng-kan-jian-99", 
        "https://www.zhihu.com/people/he-ying-68", 
        "https://www.zhihu.com/people/aixiao-cai-niao", 
        "https://www.zhihu.com/people/er-xiang-bo-11-95", 
        "https://www.zhihu.com/people/wx7b1c1640895e2638", 
        "https://www.zhihu.com/people/tian-ye-5-60", 
        "https://www.zhihu.com/people/zhao-xin-16-48", 
        "https://www.zhihu.com/people/falcon-29-66", 
        "https://www.zhihu.com/people/ji-jun-wen", 
        "https://www.zhihu.com/people/song-hui-11-93", 
        "https://www.zhihu.com/people/hum-75", 
        "https://www.zhihu.com/people/yut2kem", 
        "https://www.zhihu.com/people/xl-su", 
        "https://www.zhihu.com/people/chu-xiao-43", 
        "https://www.zhihu.com/people/wang-jiu-ke", 
        "https://www.zhihu.com/people/li-ze-qun-69", 
        "https://www.zhihu.com/people/hu-xi-ting-70", 
        "https://www.zhihu.com/people/tctom-32", 
        "https://www.zhihu.com/people/deneber", 
        "https://www.zhihu.com/people/yikerainbow", 
        "https://www.zhihu.com/people/zzq-14-59", 
        "https://www.zhihu.com/people/hal-47", 
        "https://www.zhihu.com/people/si-shu-zheng", 
        "https://www.zhihu.com/people/yang-zhao-48-10", 
        "https://www.zhihu.com/people/dexterchen-54", 
        "https://www.zhihu.com/people/lijiaying", 
        "https://www.zhihu.com/people/yin-shen-74", 
        "https://www.zhihu.com/people/gua-ji-gua-ji-26", 
        "https://www.zhihu.com/people/xstormli", 
        "https://www.zhihu.com/people/wu-xooox", 
        "https://www.zhihu.com/people/rootwang", 
        "https://www.zhihu.com/people/yi-ai-you-65", 
        "https://www.zhihu.com/people/hongxi-meng", 
        "https://www.zhihu.com/people/zi-ding-yi-41-19", 
        "https://www.zhihu.com/people/xiao-gu-gu-24", 
        "https://www.zhihu.com/people/PY_Wang", 
        "https://www.zhihu.com/people/aRic_Zheng", 
        "https://www.zhihu.com/people/1874-41-46", 
        "https://www.zhihu.com/people/chen-wei-47-99-79", 
        "https://www.zhihu.com/people/hu-luo-bu-ye-hao-chi", 
        "https://www.zhihu.com/people/li-zheng-cao-94", 
        "https://www.zhihu.com/people/zhou-yang-62-21", 
        "https://www.zhihu.com/people/tan-chi-si-nai-ke", 
        "https://www.zhihu.com/people/zhaolianrui", 
        "https://www.zhihu.com/people/liu-fei-94-95", 
        "https://www.zhihu.com/people/steven-21-7-25", 
        "https://www.zhihu.com/people/zzx-47-7", 
        "https://www.zhihu.com/people/su-yang-30-67", 
        "https://www.zhihu.com/people/yildhd-wang", 
        "https://www.zhihu.com/people/yyc-47-2", 
        "https://www.zhihu.com/people/wang-ming-66-71", 
        "https://www.zhihu.com/people/li-xin-yang-26-33", 
        "https://www.zhihu.com/people/li-chen-xi-49-5", 
        "https://www.zhihu.com/people/fyy-cloud", 
        "https://www.zhihu.com/people/ma-yu-xiang-4", 
        "https://www.zhihu.com/people/yang-wei-li-3-14", 
        "https://www.zhihu.com/people/qi-che-ren-82", 
        "https://www.zhihu.com/people/she-liang", 
        "https://www.zhihu.com/people/tang-zai-shi-zi-kan-shu-de-xiao-nu-hai", 
        "https://www.zhihu.com/people/johnny-63-54", 
        "https://www.zhihu.com/people/wang-laos", 
        "https://www.zhihu.com/people/qin-xiao-yu-34-69"
    ], 
    "article": [
        {
            "url": "https://zhuanlan.zhihu.com/p/90227781", 
            "userName": "Lyon", 
            "userLink": "https://www.zhihu.com/people/38495a9b1f83e4a612543ce6c523df7f", 
            "upvote": 1, 
            "title": "Ubuntu18.04必备的21款软件(安装详解)", 
            "content": "<h2>Ubuntu18.04必备的21款软件(安装详解)</h2><p>以下是9012年11月，本人Ubuntu18.04安装的21款软件，不一定最全，但是软件都是最新的，附带安装教程+安装包下载，权当记录 + 分享！如果，对你有用，别忘了点个赞哦：）</p><ul><li><b>1.搜狗输入法</b></li><li><b>2.网易云音乐</b></li><li><b>3.百度网盘</b></li><li><b>4.福昕PDF阅读器</b></li><li><b>5.Shutter截图</b></li><li><b>6.Flameshot截图</b></li><li><b>7.wGet</b></li><li><b>8.金山WPS</b></li><li><b>9.谷歌浏览器</b></li><li><b>10.VLC视频播放器</b></li><li><b>11.微信</b></li><li><b>12.Teamview</b></li><li><b>13.Vim</b></li><li><b>14.Sublime Text</b></li><li><b>15.JDK8</b></li><li><b>16.Maven</b></li><li><b>17.Postman</b></li><li><b>18.IntelliJ IDEA</b></li><li><b>19.Pycharm</b></li><li><b>20.Anaconda</b></li><li><b>21.MySQL8.0</b></li></ul><p><a href=\"https://link.zhihu.com/?target=https%3A//pan.baidu.com/s/1KAuUPdMUAxj16B0OgoU7Aw\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">安装包下载</a> 密码: qj1d</p><p>本文同步发布在：<a href=\"https://link.zhihu.com/?target=https%3A//www.yuque.com/docs/share/22d46937-4fbf-4800-8a97-480f70c47bf2%23\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">语雀文档</a></p><hr/><h2>常用软件</h2><h2>1.搜狗输入法</h2><p>装机必备的软件，直接装就行无！无需提前装好Fcitx环境，因为装搜狗时会自动安装这个环境。</p><h3>下载</h3><p><a href=\"https://link.zhihu.com/?target=https%3A//pinyin.sogou.com/linux/%3Fr%3Dpinyin\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">pinyin.sogou.com/linux/?</span><span class=\"invisible\">r=pinyin</span><span class=\"ellipsis\"></span></a></p><h3>安装</h3><p>安装相对来说比较容易，直接参考：<a href=\"https://link.zhihu.com/?target=https%3A//jingyan.baidu.com/article/0a52e3f4fa2ba8bf63ed724d.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">如何在Ubuntu系统中安装搜狗输入法</a></p><h3>卸载</h3><p>搜狗输入法刚开始安装有点问题，后来想卸载了重新用，结果没想到卸载带来了更大的问题，卸载完怎么装都显示不出来了......搜了半天发现是原来配置没有清理干净！注意，要加上参数-P或--purge,删除/净化程序及其配置文件</p><div class=\"highlight\"><pre><code class=\"language-bash\"><span class=\"c1\">#1.卸载搜狗</span>\nsudo dpkg -P sogoupinyin\n<span class=\"c1\">#2.卸载fcitx环境</span>\n可以sudo dpkg -P fcitx也可在Ubuntu软件中直接点卸载\n<span class=\"c1\">#3.删除所有带rc标记的包</span>\ndpkg -l <span class=\"p\">|</span> grep ^rc <span class=\"p\">|</span> cut -d<span class=\"s1\">&#39; &#39;</span> -f3 <span class=\"p\">|</span> sudo xargs dpkg --purge\n<span class=\"c1\">#4.用户～/.config/下删除所有和搜狗、fcitx相关的文件夹</span>\nSogouPY SogouPY.users sogou-qimpanel fcitx</code></pre></div><h2>2.网易云音乐</h2><p>虽然歌曲库少了点，不过支持Linux,不像QQ音乐没有Linux版的，差评</p><h3>下载</h3><p><a href=\"https://link.zhihu.com/?target=https%3A//music.163.com/%23/download\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">music.163.com/#</span><span class=\"invisible\">/download</span><span class=\"ellipsis\"></span></a> </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-1326f4710dc296b00bfd245283f04844_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1227\" data-rawheight=\"670\" class=\"origin_image zh-lightbox-thumb\" width=\"1227\" data-original=\"https://pic1.zhimg.com/v2-1326f4710dc296b00bfd245283f04844_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1227&#39; height=&#39;670&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1227\" data-rawheight=\"670\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1227\" data-original=\"https://pic1.zhimg.com/v2-1326f4710dc296b00bfd245283f04844_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-1326f4710dc296b00bfd245283f04844_b.jpg\"/></figure><h3>安装</h3><p>安装很简单，基本没踩坑</p><div class=\"highlight\"><pre><code class=\"language-bash\">sudo dpkg -i  /your_path_to/etease-cloud-music_1.2.1_amd64_ubuntu_20190428.deb\n<span class=\"c1\"># 如果没有安装成功，缺少依赖，则执行</span>\nsudo apt-get install -f</code></pre></div><h2>3.百度网盘</h2><p>百度网盘这个神器还有Linux版的，不错！</p><h3>下载</h3><p><a href=\"https://link.zhihu.com/?target=https%3A//pan.baidu.com/download/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">pan.baidu.com/download/</span><span class=\"invisible\"></span></a></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-50cf2002c54673c07535be1d38d98532_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1187\" data-rawheight=\"717\" class=\"origin_image zh-lightbox-thumb\" width=\"1187\" data-original=\"https://pic3.zhimg.com/v2-50cf2002c54673c07535be1d38d98532_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1187&#39; height=&#39;717&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1187\" data-rawheight=\"717\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1187\" data-original=\"https://pic3.zhimg.com/v2-50cf2002c54673c07535be1d38d98532_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-50cf2002c54673c07535be1d38d98532_b.jpg\"/></figure><h3>安装</h3><p>安装很简单，基本没踩坑</p><div class=\"highlight\"><pre><code class=\"language-bash\">sudo dpkg -i  /your_path_to/baidunetdisk_linux_2.0.2.deb\n<span class=\"c1\"># 如果没有安装成功，缺少依赖，则执行</span>\nsudo apt-get install -f</code></pre></div><h2>4.福昕PDF阅读器</h2><h3>下载</h3><p><a href=\"https://link.zhihu.com/?target=https%3A//www.foxitsoftware.cn/downloads/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">foxitsoftware.cn/downlo</span><span class=\"invisible\">ads/</span><span class=\"ellipsis\"></span></a></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-4f0ccaf00a4c151cb1de9925f1ab2ea4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1267\" data-rawheight=\"655\" class=\"origin_image zh-lightbox-thumb\" width=\"1267\" data-original=\"https://pic1.zhimg.com/v2-4f0ccaf00a4c151cb1de9925f1ab2ea4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1267&#39; height=&#39;655&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1267\" data-rawheight=\"655\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1267\" data-original=\"https://pic1.zhimg.com/v2-4f0ccaf00a4c151cb1de9925f1ab2ea4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-4f0ccaf00a4c151cb1de9925f1ab2ea4_b.jpg\"/></figure><h3>安装</h3><p>安装很简单，基本没踩坑,下载后直接解压缩，是个.run文件，可以直接双击运行安装</p><h2>5.Shutter(截图+编辑软件)</h2><p>搜了一下，大多推荐shutter这款截图软件，果断决定下一个</p><h3>下载+安装</h3><p>可以在Ubuntu自带的【Ubuntu软件】里搜索shutter下载，不过更推荐直接命令获取：</p><div class=\"highlight\"><pre><code class=\"language-bash\">sudo apt install shutter\n<span class=\"c1\"># 或sudo apt-get -i shutter</span></code></pre></div><p>安装完成即可使用，不过通常18.04版本的shutter只有截图功能，没开启“编辑”功能，需要编辑的需要额外下载以下三个工具包：</p><ul><li><a href=\"https://link.zhihu.com/?target=https%3A//launchpad.net/ubuntu/%2Barchive/primary/%2Bfiles/libgoocanvas-common_1.0.0-1_all.deb\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">libgoocanvas-common</a></li><li><a href=\"https://link.zhihu.com/?target=https%3A//launchpad.net/ubuntu/%2Barchive/primary/%2Bfiles/libgoocanvas3_1.0.0-1_amd64.deb\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">libgoocanvas3</a></li><li><a href=\"https://link.zhihu.com/?target=https%3A//launchpad.net/ubuntu/%2Barchive/primary/%2Bfiles/libgoo-canvas-perl_0.06-2ubuntu3_amd64.deb\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">libgoo-canvas-perl</a></li></ul><p>然后运行：</p><div class=\"highlight\"><pre><code class=\"language-bash\">dpkg -i /your_path_to/libgoocanvas-common_1.0.0-1_all.deb\ndpkg -i /your_path_to/libgoocanvas3_1.0.0-1_amd64.deb\ndpkg -i /your_path_to/libgoo-canvas-perl_0.06-2ubuntu3_amd64.deb\napt-get -f install</code></pre></div><blockquote> 参考：<a href=\"https://link.zhihu.com/?target=https%3A//ijuer.com/ubuntu-18-04-%25e4%25b8%258a%25e5%25ae%2589%25e8%25a3%2585-shutter-%25e5%25b9%25b6%25e5%2590%25af%25e7%2594%25a8-edit-%25e5%258a%259f%25e8%2583%25bd/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Ubuntu 18.04 上安装 Shutter 并启用 Edit 功能</a></blockquote><h2>6.Flameshot(截图+编辑软件)</h2><h3>下载+安装</h3><p>这是一款同样推荐的截图软件，类似QQ截图那样挺方便的，截图+框选/注释等实用的编辑功能，项目在github开源：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/lupoDharkael/flameshot\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/lupoDharkael</span><span class=\"invisible\">/flameshot</span><span class=\"ellipsis\"></span></a> 同样可以在Ubuntu自带的【Ubuntu软件】里搜索shutter下载，不过更推荐直接命令获取：</p><div class=\"highlight\"><pre><code class=\"language-bash\">sudo apt install flameshot\n<span class=\"c1\"># 或sudo apt-get -i flameshot</span></code></pre></div><h2>7.wGet（下载神器）</h2><p>在Ubuntu下想找迅雷，结果没找到，又不想安个虚拟机专门跑迅雷，于是推荐wGet，这是一个简化版的迅雷!</p><h3>下载</h3><p><a href=\"https://link.zhihu.com/?target=https%3A//ugetdm.com/downloads/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">ugetdm.com/downloads/</span><span class=\"invisible\"></span></a> </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-6edf27b6e41dd3b94b3660b6687d5da6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1301\" data-rawheight=\"652\" class=\"origin_image zh-lightbox-thumb\" width=\"1301\" data-original=\"https://pic3.zhimg.com/v2-6edf27b6e41dd3b94b3660b6687d5da6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1301&#39; height=&#39;652&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1301\" data-rawheight=\"652\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1301\" data-original=\"https://pic3.zhimg.com/v2-6edf27b6e41dd3b94b3660b6687d5da6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-6edf27b6e41dd3b94b3660b6687d5da6_b.jpg\"/></figure><p> 推荐官网直接下载，当然也可以在【Ubuntu软件】中直接下载，不过上面的版本有点老，而官网是2.2.1-stable最新版的。不过，先不要着急下载，<b>官网推荐用ppa方式安装↓</b></p><h3>安装</h3><p><a href=\"https://link.zhihu.com/?target=https%3A//ugetdm.com/downloads/ubuntu/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">ugetdm.com/downloads/ub</span><span class=\"invisible\">untu/</span><span class=\"ellipsis\"></span></a> </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3c9019c99fb0988b91e42e7a2594bacd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1343\" data-rawheight=\"828\" class=\"origin_image zh-lightbox-thumb\" width=\"1343\" data-original=\"https://pic2.zhimg.com/v2-3c9019c99fb0988b91e42e7a2594bacd_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1343&#39; height=&#39;828&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1343\" data-rawheight=\"828\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1343\" data-original=\"https://pic2.zhimg.com/v2-3c9019c99fb0988b91e42e7a2594bacd_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-3c9019c99fb0988b91e42e7a2594bacd_b.jpg\"/></figure><div class=\"highlight\"><pre><code class=\"language-bash\">sudo add-apt-repository ppa:plushuang-tw/uget-stable\nsudo apt update \nsudo apt install uget aria2</code></pre></div><h2>8.金山WPS</h2><p>Office有windows版、mac版本、唯独没有提供Linux版，于是WPS成为了主力，话说因为雷军的原因，个人对WPS还是挺有感情的，这么多年金山系列的软件都挺不错的！对了，WPS据说是当年求伯君一个人整出来的，太厉害了！！！莫名想到：鲁大师的第一代也是一位姓鲁的师傅开发出来的：）</p><h3>下载</h3><p><a href=\"https://link.zhihu.com/?target=https%3A//www.wps.cn/product/wpslinux\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">wps.cn/product/wpslinux</span><span class=\"invisible\"></span></a> </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-24e90c11bc7c954aec0f6cc28e072a4c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1327\" data-rawheight=\"821\" class=\"origin_image zh-lightbox-thumb\" width=\"1327\" data-original=\"https://pic1.zhimg.com/v2-24e90c11bc7c954aec0f6cc28e072a4c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1327&#39; height=&#39;821&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1327\" data-rawheight=\"821\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1327\" data-original=\"https://pic1.zhimg.com/v2-24e90c11bc7c954aec0f6cc28e072a4c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-24e90c11bc7c954aec0f6cc28e072a4c_b.jpg\"/></figure><h3>安装</h3><div class=\"highlight\"><pre><code class=\"language-bash\">sudo dpkg -i /your_path_to/wps-office_11.1.0.8865_amd64.deb</code></pre></div><p>这里需要注意，下载下来的WPS是需要字体支持的，需要手动安装，否则使用时会提示字体缺失，解决方法： 百度一下：ubuntu安装wps字体</p><h2>9.谷歌浏览器</h2><p>Ubuntu自带的火狐浏览器其实也不错了，不过谷歌用顺手了，还是下一个吧。</p><h3>下载</h3><p><a href=\"https://link.zhihu.com/?target=https%3A//www.google.cn/chrome/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">google.cn/chrome/</span><span class=\"invisible\"></span></a></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-23cc536000494ea46c562e0e2d197732_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1337\" data-rawheight=\"818\" class=\"origin_image zh-lightbox-thumb\" width=\"1337\" data-original=\"https://pic3.zhimg.com/v2-23cc536000494ea46c562e0e2d197732_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1337&#39; height=&#39;818&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1337\" data-rawheight=\"818\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1337\" data-original=\"https://pic3.zhimg.com/v2-23cc536000494ea46c562e0e2d197732_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-23cc536000494ea46c562e0e2d197732_b.jpg\"/></figure><h3>安装</h3><div class=\"highlight\"><pre><code class=\"language-bash\">sudo dpkg -i /your_path_to/google-chrome-stable_current_amd64.deb\nsudo apt-get -f install</code></pre></div><h2>10.VLC视频播放器</h2><p>搜了一圈，发现VLC推荐的人挺多，下载一个，看视频必备。</p><h3>下载+安装</h3><p><a href=\"https://link.zhihu.com/?target=https%3A//www.videolan.org/vlc/download-ubuntu.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">videolan.org/vlc/downlo</span><span class=\"invisible\">ad-ubuntu.html</span><span class=\"ellipsis\"></span></a> </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-bfcb74fa7eda2d4a29a7b1e63216145e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1288\" data-rawheight=\"768\" class=\"origin_image zh-lightbox-thumb\" width=\"1288\" data-original=\"https://pic3.zhimg.com/v2-bfcb74fa7eda2d4a29a7b1e63216145e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1288&#39; height=&#39;768&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1288\" data-rawheight=\"768\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1288\" data-original=\"https://pic3.zhimg.com/v2-bfcb74fa7eda2d4a29a7b1e63216145e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-bfcb74fa7eda2d4a29a7b1e63216145e_b.jpg\"/></figure><p> 官方给出了两种软件安装方式：</p><ul><li><b>在【Ubuntu软件】中搜索“vlc”并安装；</b></li><li><b>命令行执行 <code>% sudo snap install vlc</code></b></li></ul><p>我用的是第二种方式</p><h2>11.微信</h2><p>很可惜，腾讯官方并没有提供QQ/微信的Linux版下载，于是只能在虚拟机的Windows中装软件，或者利用开源项目</p><h3>下载+安装</h3><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/geeeeeeeeek/electronic-wechat/releases\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/geeeeeeeeek/</span><span class=\"invisible\">electronic-wechat/releases</span><span class=\"ellipsis\"></span></a> 不过作者很久没更新了，怕后期不好用，我这里直接用的是微信网页版，不过改造一下看上去和桌面版的没什么不同：）网页版的改造方式参考：<a href=\"https://link.zhihu.com/?target=https%3A//segmentfault.com/a/1190000019478007%3Futm_source%3Dtag-newest\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">在ubuntu中使用微信的三种方式</a></p><h2>12.Teamview</h2><p>谁用谁知道，远程控制电脑不要太舒服：）</p><h3>下载</h3><p><a href=\"https://link.zhihu.com/?target=https%3A//www.teamviewer.cn/cn/download/linux/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">teamviewer.cn/cn/downlo</span><span class=\"invisible\">ad/linux/</span><span class=\"ellipsis\"></span></a> </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-3cd05161e45ee3428ac304459db0dd68_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1270\" data-rawheight=\"775\" class=\"origin_image zh-lightbox-thumb\" width=\"1270\" data-original=\"https://pic1.zhimg.com/v2-3cd05161e45ee3428ac304459db0dd68_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1270&#39; height=&#39;775&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1270\" data-rawheight=\"775\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1270\" data-original=\"https://pic1.zhimg.com/v2-3cd05161e45ee3428ac304459db0dd68_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-3cd05161e45ee3428ac304459db0dd68_b.jpg\"/></figure><h3>安装</h3><p>将下载好的deb包放在合适位置，譬如：/usr/local/software/</p><div class=\"highlight\"><pre><code class=\"language-bash\"><span class=\"nb\">cd</span> /usr/local/software/\nsudo dpkg -i  teamviewer_14.7.1965_amd64.deb\n<span class=\"c1\"># 如果报错或缺少依赖</span>\nsudo apt-get install -f\nsudo dpkg -i  teamviewer_14.7.1965_amd64.deb</code></pre></div><hr/><h2>编程软件</h2><h2>13.Vim</h2><p>这个不解释了，只要你用到shell,必装的一款软件</p><h3>下载+安装</h3><div class=\"highlight\"><pre><code class=\"language-bash\">sudo apt-get install vim</code></pre></div><h2>14.Sublime Text</h2><p>轻量又高效的文本编辑器，暗黑色风格很高大上</p><h3>下载</h3><p><a href=\"https://link.zhihu.com/?target=http%3A//www.sublimetext.com/3\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">sublimetext.com/3</span><span class=\"invisible\"></span></a> </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-11465155bb62b10c7d7b430119ca93ee_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1273\" data-rawheight=\"726\" class=\"origin_image zh-lightbox-thumb\" width=\"1273\" data-original=\"https://pic3.zhimg.com/v2-11465155bb62b10c7d7b430119ca93ee_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1273&#39; height=&#39;726&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1273\" data-rawheight=\"726\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1273\" data-original=\"https://pic3.zhimg.com/v2-11465155bb62b10c7d7b430119ca93ee_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-11465155bb62b10c7d7b430119ca93ee_b.jpg\"/></figure><h3>安装</h3><p><b>方式一：snap安装</b></p><div class=\"highlight\"><pre><code class=\"language-bash\"><span class=\"c1\"># 安装Snap</span>\nsudo apt install snapd\n<span class=\"c1\"># 安装Sublime text</span>\nsudo snap install sublime-text</code></pre></div><p><b>方式二：官方源安装</b></p><div class=\"highlight\"><pre><code class=\"language-bash\">wget -qO - https://download.sublimetext.com/sublimehq-pub.gpg <span class=\"p\">|</span> sudo apt-key add -\n<span class=\"c1\"># 安装必要组件</span>\nsudo apt-get install apt-transport-https\n<span class=\"c1\"># 添加sublimetext的源</span>\n<span class=\"nb\">echo</span> <span class=\"s2\">&#34;deb https://download.sublimetext.com/ apt/stable/&#34;</span> <span class=\"p\">|</span> sudo tee /etc/apt/sources.list.d/sublime-text.list\n<span class=\"c1\"># 更新源</span>\nsudo apt-get update\n<span class=\"c1\"># 修复缺失包</span>\nsudo apt-get install sublime-text --fix-missing</code></pre></div><p>参考：<a href=\"https://link.zhihu.com/?target=https%3A//www.linuxidc.com/Linux/2019-03/157533.htm\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">linuxidc.com/Linux/2019</span><span class=\"invisible\">-03/157533.htm</span><span class=\"ellipsis\"></span></a></p><h2>15.JDK8</h2><p>搞Java装机必备的</p><h3>下载</h3><p><a href=\"https://link.zhihu.com/?target=https%3A//www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">oracle.com/technetwork/</span><span class=\"invisible\">java/javase/downloads/jdk8-downloads-2133151.html</span><span class=\"ellipsis\"></span></a> Ubuntu系统下选择X64的tar包，当然rpm包安装也可。 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-5d1b538ff588cccc1a6be9fd881b8afa_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1104\" data-rawheight=\"716\" class=\"origin_image zh-lightbox-thumb\" width=\"1104\" data-original=\"https://pic3.zhimg.com/v2-5d1b538ff588cccc1a6be9fd881b8afa_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1104&#39; height=&#39;716&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1104\" data-rawheight=\"716\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1104\" data-original=\"https://pic3.zhimg.com/v2-5d1b538ff588cccc1a6be9fd881b8afa_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-5d1b538ff588cccc1a6be9fd881b8afa_b.jpg\"/></figure><h3>安装</h3><p>主要就是解压缩包 + 配置环境变量,我习惯将tar包移动到/user/local/下</p><p><b>a.解压缩</b></p><div class=\"highlight\"><pre><code class=\"language-bash\">sudo tar -xzvf /user/local/software/jdk-8u191-linux-x64.tar.gz</code></pre></div><p>配置环境变量，根据自己需求配置用户/系统变量下面以用户变量为例： </p><p><b>b. 编辑环境变量</b></p><p><code>sudo vim ~/.bashrc</code> ~/的意思是在当前用户的主目录下，找.bashrc文件等价于/home/user_name/.bashrc</p><div class=\"highlight\"><pre><code class=\"language-bash\"><span class=\"nb\">export</span> <span class=\"nv\">JAVA_HOME</span><span class=\"o\">=</span>/usr/local/jdk1.8.0_191\n<span class=\"nb\">export</span> <span class=\"nv\">JRE_HOME</span><span class=\"o\">=</span><span class=\"si\">${</span><span class=\"nv\">JAVA_HOME</span><span class=\"si\">}</span>/jre\n<span class=\"nb\">export</span> <span class=\"nv\">CLASSPATH</span><span class=\"o\">=</span>.:<span class=\"si\">${</span><span class=\"nv\">JAVA_HOME</span><span class=\"si\">}</span>/lib:<span class=\"si\">${</span><span class=\"nv\">JRE_HOME</span><span class=\"si\">}</span>/lib\n<span class=\"nb\">export</span> <span class=\"nv\">PATH</span><span class=\"o\">=</span>.:<span class=\"si\">${</span><span class=\"nv\">JAVA_HOME</span><span class=\"si\">}</span>/bin:<span class=\"nv\">$PATH</span></code></pre></div><p><b>c.刷新变量</b> </p><p><code>source ~/.bashrc</code> 完成后，java -version看到java版本号，即表示安装成功！</p><h2>16.Maven</h2><p>搞Java当然少不了Maven,二者的关系就行python少不了pip,前端少不了npm：）</p><h3>下载</h3><p><a href=\"https://link.zhihu.com/?target=https%3A//maven.apache.org/download.cgi\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">maven.apache.org/downlo</span><span class=\"invisible\">ad.cgi</span><span class=\"ellipsis\"></span></a> </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-80851a9dcdb118c657d443e7c5b765c7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1328\" data-rawheight=\"829\" class=\"origin_image zh-lightbox-thumb\" width=\"1328\" data-original=\"https://pic4.zhimg.com/v2-80851a9dcdb118c657d443e7c5b765c7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1328&#39; height=&#39;829&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1328\" data-rawheight=\"829\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1328\" data-original=\"https://pic4.zhimg.com/v2-80851a9dcdb118c657d443e7c5b765c7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-80851a9dcdb118c657d443e7c5b765c7_b.jpg\"/></figure><h3>安装</h3><p>和安装Java一样，很简单，只不过多了一个配置镜像源的步骤。 </p><p><b>a.解压缩</b></p><div class=\"highlight\"><pre><code class=\"language-bash\">sudo tar -xzvf /user/local/apache-maven-3.6.2-bin.tar.gz</code></pre></div><p><b>b.编辑环境变量</b></p><p><code>sudo vim ~/.bashrc</code></p><div class=\"highlight\"><pre><code class=\"language-bash\"><span class=\"nb\">export</span> <span class=\"nv\">MAVEN_HOME</span><span class=\"o\">=</span>/usr/local/apache-maven-3.6.2\n<span class=\"nb\">export</span> <span class=\"nv\">PATH</span><span class=\"o\">=</span><span class=\"si\">${</span><span class=\"nv\">MAVEN_HOME</span><span class=\"si\">}</span>/bin:<span class=\"nv\">$PATH</span></code></pre></div><p><b>c.刷新变量</b> </p><p><code>source ~/.bashrc</code></p><h3>配置镜像源</h3><p>由于maven镜像在国外，由于大家都知道的原因，直接用默认源下载资源是很慢的，需要换成国内的镜像源头，可以直接配阿里源： 编辑maven的settings.xml文件（maven主目录下/conf/），在区块之间加入：</p><div class=\"highlight\"><pre><code class=\"language-xml\"><span class=\"nt\">&lt;mirror&gt;</span>\n    <span class=\"nt\">&lt;id&gt;</span>aliyunmaven<span class=\"nt\">&lt;/id&gt;</span>\n    <span class=\"nt\">&lt;mirrorOf&gt;</span>*<span class=\"nt\">&lt;/mirrorOf&gt;</span>\n    <span class=\"nt\">&lt;name&gt;</span>阿里云公共仓库<span class=\"nt\">&lt;/name&gt;</span>\n    <span class=\"nt\">&lt;url&gt;</span>https://maven.aliyun.com/repository/public<span class=\"nt\">&lt;/url&gt;</span>\n<span class=\"nt\">&lt;/mirror&gt;</span></code></pre></div><p>如果需要添加其他代理仓库，可参考：<a href=\"https://link.zhihu.com/?target=https%3A//help.aliyun.com/document_detail/102512.html%3Fspm%3Da2c40.aliyun_maven_repo.0.0.36183054D18M8i\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">官方指南</a></p><h2>17.Postman</h2><p>Web开发必备的神器</p><h3>下载+安装</h3><p>千万不要费劲，照着百度到的一系列的安装教程来安装，直接在Ubuntu自带的【Ubuntu软件】中搜索Postman，直接傻瓜式安装即可 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-c225f88a5ccfae26e62b3d53dd2eb0db_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1257\" data-rawheight=\"892\" class=\"origin_image zh-lightbox-thumb\" width=\"1257\" data-original=\"https://pic4.zhimg.com/v2-c225f88a5ccfae26e62b3d53dd2eb0db_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1257&#39; height=&#39;892&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1257\" data-rawheight=\"892\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1257\" data-original=\"https://pic4.zhimg.com/v2-c225f88a5ccfae26e62b3d53dd2eb0db_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-c225f88a5ccfae26e62b3d53dd2eb0db_b.jpg\"/></figure><h2>18.IntelliJ IDEA</h2><p>jetbrains公司出品的，宇宙第一好用的Java IDE，谁用谁知道</p><h3>下载</h3><p><a href=\"https://link.zhihu.com/?target=http%3A//www.jetbrains.com/idea/download/%23section%3Dlinux\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">jetbrains.com/idea/down</span><span class=\"invisible\">load/#section=linux</span><span class=\"ellipsis\"></span></a> </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b1466fe271bf9931a0cca218251772a9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1313\" data-rawheight=\"810\" class=\"origin_image zh-lightbox-thumb\" width=\"1313\" data-original=\"https://pic2.zhimg.com/v2-b1466fe271bf9931a0cca218251772a9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1313&#39; height=&#39;810&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1313\" data-rawheight=\"810\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1313\" data-original=\"https://pic2.zhimg.com/v2-b1466fe271bf9931a0cca218251772a9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-b1466fe271bf9931a0cca218251772a9_b.jpg\"/></figure><p> 直接下载Ultimate版，官网很温馨地提示了，可以使用支付宝付款，有实力的还是支持正版，实在不行淘宝上买一个激活码即可：）</p><h3>安装</h3><p>将下载好的安装包，放在你需要的位置，譬如：/user/local/software/</p><div class=\"highlight\"><pre><code class=\"language-bash\"><span class=\"nb\">cd</span> /user/local/software/\n<span class=\"c1\"># 解压缩</span>\nsudo tar -xzvf ideaIU-2019.2.4.tar.gz\n<span class=\"c1\"># 解压完的文件夹：idea-IU-192.7142.36</span>\n<span class=\"c1\"># 更改权限</span>\nsudo chmod <span class=\"m\">755</span>  idea-IU-192.7142.36\n<span class=\"c1\">#执行安装脚本</span>\nsh idea-IU-192.7142.36/bin/idea.sh</code></pre></div><h2>19.Pycharm</h2><p>和IDEA师出同门，是非常好用的一款Python IDE,有钱请支付宝支持一波，否则，还是用激活码吧，对了Jetbrains系列的软件可以公用一个激活码哦：）</p><h3>下载</h3><p><a href=\"https://link.zhihu.com/?target=http%3A//www.jetbrains.com/pycharm/download/%23section%3Dlinux\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">http://www.</span><span class=\"visible\">jetbrains.com/pycharm/d</span><span class=\"invisible\">ownload/#section=linux</span><span class=\"ellipsis\"></span></a> </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-0992c726c77b68a4431d19c03d984cb8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1328\" data-rawheight=\"820\" class=\"origin_image zh-lightbox-thumb\" width=\"1328\" data-original=\"https://pic1.zhimg.com/v2-0992c726c77b68a4431d19c03d984cb8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1328&#39; height=&#39;820&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1328\" data-rawheight=\"820\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1328\" data-original=\"https://pic1.zhimg.com/v2-0992c726c77b68a4431d19c03d984cb8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-0992c726c77b68a4431d19c03d984cb8_b.jpg\"/></figure><h3>安装</h3><p>同IDEA，也是直接解压缩，cd到主目录/bin,执行<code>sh ./pycharm.sh</code></p><h2>20.Anaconda</h2><p>Anaconda是用来管理各种虚拟环境和包的，搞AI必用的一款软件，官网直接找对应的系统下载即可。</p><h3>下载</h3><p><a href=\"https://link.zhihu.com/?target=https%3A//www.anaconda.com/distribution/%23download-section\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">anaconda.com/distributi</span><span class=\"invisible\">on/#download-section</span><span class=\"ellipsis\"></span></a></p><h3>安装</h3><p>安装比较简单，切换到root用户执行或者sudo执行： <code>bash /your_path_to/Anaconda3-2019.10-Linux-x86_64.sh</code> 根据提示输入Enter,yes即可，最后会确认路径，如果用默认的直接Enter否则输入自定义的安装路径再按Enter即可。安装完成后 <code>conda --version</code> 能看到版本号即表示安装成功 安装完成，根据自己需要配置环境变量：</p><div class=\"highlight\"><pre><code class=\"language-bash\"><span class=\"nb\">export</span> <span class=\"nv\">CONDA_PATH</span><span class=\"o\">=</span>/usr/local/software/anaconda3\n<span class=\"nb\">export</span> <span class=\"nv\">PATH</span><span class=\"o\">=</span><span class=\"si\">${</span><span class=\"nv\">CONDA_PATH</span><span class=\"si\">}</span>/bin:<span class=\"nv\">$PATH</span></code></pre></div><h3>卸载</h3><p>删除anaconda，直接删除文件夹+清理环境变量即可</p><p><b>a.删除主文件夹anaconda3</b> </p><p>直接找到安装时的anaconda3文件夹即可，可以用： <code>sudo find / -type d -name anaconda3</code>找到文件夹 然后删除文件夹<code>sudo rm -rf  /your_path_to/anaconda3</code> </p><p><b>b.删除文件夹</b> </p><p>删除anaconda的配置文件夹.condarc，可以用命令： <code>sudo find / -type f -name .condarc</code>找到其安装位置，删除之。 删除环境包文件夹.conda，命令同上。</p><p><b>c.删除conda初始化脚本</b> </p><p>通常conda会在.bashrc中创建一段脚本，如下： </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-4110d7944f3ccdc6281e47d7b90a1887_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"731\" data-rawheight=\"492\" class=\"origin_image zh-lightbox-thumb\" width=\"731\" data-original=\"https://pic4.zhimg.com/v2-4110d7944f3ccdc6281e47d7b90a1887_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;731&#39; height=&#39;492&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"731\" data-rawheight=\"492\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"731\" data-original=\"https://pic4.zhimg.com/v2-4110d7944f3ccdc6281e47d7b90a1887_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-4110d7944f3ccdc6281e47d7b90a1887_b.jpg\"/></figure><p> 如果是root管理员默认位置安装，则该脚本位于/root/.bashrc；如果是普通用户安装，则通常位于/home/your_user_name/.bashrc。譬如我的.bashrc位于/home/lyon/下，执行: vim /home/lyon/.bashrc，删除这段conda initialize初始化脚本 </p><p><b>d.清除环境变量</b> </p><p>需要注意的是，如果你配置了anaconda的环境变量，则需要在对应的bashrc或profile中删除掉。如果你配置的用户变量，通常在/home/your_user_name/下可以找到.bashrc和.profile，如果是系统变量，则通常是/etc/profile</p><h2>21.MySQL8.0</h2><p>据说mysql8.0相比于5.7有了不小的升级，于是决定安个新版8.0试试，传统的mysql安装还是比较麻烦的，这里推荐直接用官网给出的APT安装方式，适合Ubuntu、Debian系统</p><h3>下载</h3><p><a href=\"https://link.zhihu.com/?target=https%3A//dev.mysql.com/downloads/repo/apt/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">dev.mysql.com/downloads</span><span class=\"invisible\">/repo/apt/</span><span class=\"ellipsis\"></span></a> 首先下载mysql配置工具，后面的配置都通过它来完成 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ac643477d09117cd0051f786e210531c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1011\" data-rawheight=\"603\" class=\"origin_image zh-lightbox-thumb\" width=\"1011\" data-original=\"https://pic1.zhimg.com/v2-ac643477d09117cd0051f786e210531c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1011&#39; height=&#39;603&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1011\" data-rawheight=\"603\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1011\" data-original=\"https://pic1.zhimg.com/v2-ac643477d09117cd0051f786e210531c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-ac643477d09117cd0051f786e210531c_b.jpg\"/></figure><h3>安装</h3><p>同样，将下载好的文件放在适合的位置，譬如/user/local/</p><div class=\"highlight\"><pre><code class=\"language-bash\"><span class=\"nb\">cd</span> /usr/local\nsudo dpkg -i mysql-apt-config_0.8.14-1_all.deb\n<span class=\"c1\"># 安装mysql-apt-config时会让你选择需要安装的版本，之后继续：</span>\nsudo apt-get update\nsudo apt-get install mysql-server</code></pre></div><p>安装完成后，服务自动，可以用mysql --version查看版本号：</p><div class=\"highlight\"><pre><code class=\"language-bash\">&gt; mysql --version\nmysql  Ver <span class=\"m\">8</span>.0.18 <span class=\"k\">for</span> Linux on x86_64 <span class=\"o\">(</span>MySQL Community Server - GPL<span class=\"o\">)</span></code></pre></div><p>更多说明详见：<a href=\"https://link.zhihu.com/?target=https%3A//dev.mysql.com/doc/mysql-apt-repo-quick-guide/en/%23repo-qg-apt-select-series\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">MySQL APT存储库的快速指南</a> <a href=\"https://link.zhihu.com/?target=https%3A//www.yuque.com/attachments/yuque/0/2019/pdf/216914/1572936703086-b50821f8-0ecb-4639-9e75-23714a28847f.pdf%3F_lake_card%3D%257B%2522uid%2522%253A%25221572936699881-0%2522%252C%2522src%2522%253A%2522https%253A%252F%252Fwww.yuque.com%252Fattachments%252Fyuque%252F0%252F2019%252Fpdf%252F216914%252F1572936703086-b50821f8-0ecb-4639-9e75-23714a28847f.pdf%2522%252C%2522name%2522%253A%2522mysql-apt-repo-quick-guide-en.pdf%2522%252C%2522size%2522%253A66846%252C%2522type%2522%253A%2522application%252Fpdf%2522%252C%2522ext%2522%253A%2522pdf%2522%252C%2522progress%2522%253A%257B%2522percent%2522%253A0%257D%252C%2522status%2522%253A%2522done%2522%252C%2522percent%2522%253A0%252C%2522id%2522%253A%2522u6kyG%2522%252C%2522card%2522%253A%2522file%2522%257D\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">mysql-apt-repo-quick-guide-en.pdf</a></p><h3>命令</h3><p><b>查看状态：sudo service mysql status </b></p><p><b>启动服务：sudo service mysql start </b></p><p><b>停止服务：sudo service mysql stop</b></p><hr/><h2>如果，对你有用，别忘了点个赞哦：）</h2>", 
            "topic": [
                {
                    "tag": "Ubuntu 18.04", 
                    "tagLink": "https://api.zhihu.com/topics/20232627"
                }, 
                {
                    "tag": "Ubuntu", 
                    "tagLink": "https://api.zhihu.com/topics/19557067"
                }, 
                {
                    "tag": "软件安装", 
                    "tagLink": "https://api.zhihu.com/topics/19586076"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/80376257", 
            "userName": "Lyon", 
            "userLink": "https://www.zhihu.com/people/38495a9b1f83e4a612543ce6c523df7f", 
            "upvote": 5, 
            "title": "【吴恩达深度学习】—深度神经网络的反向传播推导", 
            "content": "<h2>前言：</h2><p>继吴恩达机器学习公开课之后，吴恩达大佬又和网易合作，免费开放了深度学习的课程，感谢大佬，感谢网易，同样感谢提供配套课程讲义的黄海广博士。</p><p>本文主要是个人的学习总结，原内容源于视频第一门课：神经网络和深度学习—第三周，文章内容同步发布在<a href=\"https://link.zhihu.com/?target=https%3A//www.yuque.com/docs/share/3bb85013-03d4-4dfe-8c42-8dc07eeddca7\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">语雀文档</a>上。</p><p><b>课程资源</b>： <a href=\"https://link.zhihu.com/?target=https%3A//mooc.study.163.com/smartSpec/detail/1001319001.htm\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">网易云课堂课程资源</a> <a href=\"https://link.zhihu.com/?target=https%3A//github.com/fengdu78/deeplearning_ai_books\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">课程配套讲义</a></p><hr/><h2><b>吴恩达深度学习1—深度神经网络的反向传播推导</b></h2><p>      神经网络的正向传播和反向传播梯度下降的推导过程其实在吴恩达机器学习公开课第五周：<a href=\"https://zhuanlan.zhihu.com/p/74167352%22%20%5Ct%20%22_blank\" class=\"internal\">https://zhuanlan.zhihu.com/p/74167352</a> 已经推导过了，不过之前的例子不够通用，重点在于解释反向传播的过程。这次我们进行更通用点的推导。</p><p><b>首先，我们总结一下单个神经网络和简单神经网络的向量化表示，然后对一个2层神经网络进行前向传播 + 反向传播的公式推导；最后总结出一个更一般的神经网络反向传播推导公式。</b></p><h2><b>1. 神经网络的向量化表示</b></h2><p><b>1.1 单个神经元：</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-57abdda168cff19006e765c6bf3fa442_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"425\" data-rawheight=\"326\" class=\"origin_image zh-lightbox-thumb\" width=\"425\" data-original=\"https://pic3.zhimg.com/v2-57abdda168cff19006e765c6bf3fa442_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;425&#39; height=&#39;326&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"425\" data-rawheight=\"326\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"425\" data-original=\"https://pic3.zhimg.com/v2-57abdda168cff19006e765c6bf3fa442_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-57abdda168cff19006e765c6bf3fa442_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><img src=\"https://www.zhihu.com/equation?tex=x\" alt=\"x\" eeimg=\"1\"/> 为输入层输出变量矩阵； 我们用 <img src=\"https://www.zhihu.com/equation?tex=a%5E%7B%5Bl%5D%7D\" alt=\"a^{[l]}\" eeimg=\"1\"/> 来表示神经网络第l层的所有神经元激活后的结果矩阵。根据定义， <img src=\"https://www.zhihu.com/equation?tex=a%5E%7B%5BL%5D%7D\" alt=\"a^{[L]}\" eeimg=\"1\"/> 即表示输出层的输出结果。</p><p>      这里： <img src=\"https://www.zhihu.com/equation?tex=a%5E%7B%5B0%5D%7D+%3D+x+%3D++%5Cleft%5B+%5Cbegin%7Bmatrix%7D+x_1+%5C%5C+x_2+%5C%5C+x_3+++%5Cend%7Bmatrix%7D+%5Cright%5D\" alt=\"a^{[0]} = x =  \\left[ \\begin{matrix} x_1 \\\\ x_2 \\\\ x_3   \\end{matrix} \\right]\" eeimg=\"1\"/> <b><i>，</i></b> <img src=\"https://www.zhihu.com/equation?tex=a%5E%7B%5B1%5D%7D+%3D+%5Cleft%5B++%5Cbegin%7Bmatrix%7D++a%5E%7B%5B1%5D%7D_1+%5C%5C+++%5Cend%7Bmatrix%7D+%5Cright%5D\" alt=\"a^{[1]} = \\left[  \\begin{matrix}  a^{[1]}_1 \\\\   \\end{matrix} \\right]\" eeimg=\"1\"/> ，<b><i>w </i></b>是权重矩阵，因为<b><i>w</i></b>为列向量，故在计算 <img src=\"https://www.zhihu.com/equation?tex=z+%3D+w%5ETx+%2B+b\" alt=\"z = w^Tx + b\" eeimg=\"1\"/> 时，用的是其转置。</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>1.2 简单神经网络</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-2b7880c996cbd65add34964504bd1235_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"919\" data-rawheight=\"289\" class=\"origin_image zh-lightbox-thumb\" width=\"919\" data-original=\"https://pic2.zhimg.com/v2-2b7880c996cbd65add34964504bd1235_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;919&#39; height=&#39;289&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"919\" data-rawheight=\"289\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"919\" data-original=\"https://pic2.zhimg.com/v2-2b7880c996cbd65add34964504bd1235_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-2b7880c996cbd65add34964504bd1235_b.jpg\"/></figure><p>我们用<b><i> L </i></b>表示层数，上图：<b><i>L </i></b>= 2表示神经网络层数为2，输入层 <img src=\"https://www.zhihu.com/equation?tex=n%5E%7B%5B0%5D%7D%3D+n_x+%3D+3\" alt=\"n^{[0]}= n_x = 3\" eeimg=\"1\"/> </p><p>有三个变量，同时 <img src=\"https://www.zhihu.com/equation?tex=a%5E%7B%5B0%5D%7D+%3D+x+%3D++%5Cleft%5B+%5Cbegin%7Bmatrix%7D+x_1+%5C%5C+x_2+%5C%5C+x_3+++%5Cend%7Bmatrix%7D+%5Cright%5D\" alt=\"a^{[0]} = x =  \\left[ \\begin{matrix} x_1 \\\\ x_2 \\\\ x_3   \\end{matrix} \\right]\" eeimg=\"1\"/> 中间的隐藏层有4个神经元 <img src=\"https://www.zhihu.com/equation?tex=n%5E%7B%5B1%5D%7D+%3D+4\" alt=\"n^{[1]} = 4\" eeimg=\"1\"/> ，隐藏层激活后的结果矩阵 <img src=\"https://www.zhihu.com/equation?tex=a%5E%7B%5B1%5D%7D+%3D+%5Cleft%5B++%5Cbegin%7Bmatrix%7D++a%5E%7B%5B1%5D%7D_1+%5C%5C++a%5E%7B%5B1%5D%7D_2+%5C%5C++a%5E%7B%5B1%5D%7D_3+%5C%5C++a%5E%7B%5B1%5D%7D_4+%5C%5C+++++%5Cend%7Bmatrix%7D+++%5Cright%5D\" alt=\"a^{[1]} = \\left[  \\begin{matrix}  a^{[1]}_1 \\\\  a^{[1]}_2 \\\\  a^{[1]}_3 \\\\  a^{[1]}_4 \\\\     \\end{matrix}   \\right]\" eeimg=\"1\"/> ；输出层有1个神经元 <img src=\"https://www.zhihu.com/equation?tex=n%5E%7B%5B2%5D%7D+%3D+1\" alt=\"n^{[2]} = 1\" eeimg=\"1\"/> </p><p><b>1.2.1单样本向量化表示</b></p><p>      用方程式表示隐藏层的z和激活结果a，如上图右边的方程式，看起来很直观，不过实际计算时，不可能一个个地根据上述方程来计算，而是用向量化地方式表示。</p><p><img src=\"https://www.zhihu.com/equation?tex=z%5E%7B%5B1%5D%7D+%3D+W%5E%7B%5B1%5D%7Dx+%2B+b%5E%7B%5B1%5D%7D\" alt=\"z^{[1]} = W^{[1]}x + b^{[1]}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=a%5E%7B%5B1%5D%7D+%3D+%5Csigma%28+z%5E%7B%5B1%5D%7D%29\" alt=\"a^{[1]} = \\sigma( z^{[1]})\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=z%5E%7B%5B2%5D%7D+%3D+W%5E%7B%5B2%5D%7Dx+%2B+b%5E%7B%5B2%5D%7D\" alt=\"z^{[2]} = W^{[2]}x + b^{[2]}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=a%5E%7B%5B2%5D%7D+%3D+%5Csigma%28+z%5E%7B%5B2%5D%7D%29\" alt=\"a^{[2]} = \\sigma( z^{[2]})\" eeimg=\"1\"/> </p><p><b>1.2.2 多样本向量化表示</b></p><p>      1.2.1中的公式适用于单个样本，当我们有多个样本(mini-batch)时，假设有m批样本，则我们就需要从i = 1~m，重复计算这四个过程。实际情况没那么复杂，我们可以直接用多样本向量化的公式表示如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-93a5aeb1ab6e9db18c2188619723fb7d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"532\" data-rawheight=\"214\" class=\"origin_image zh-lightbox-thumb\" width=\"532\" data-original=\"https://pic2.zhimg.com/v2-93a5aeb1ab6e9db18c2188619723fb7d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;532&#39; height=&#39;214&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"532\" data-rawheight=\"214\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"532\" data-original=\"https://pic2.zhimg.com/v2-93a5aeb1ab6e9db18c2188619723fb7d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-93a5aeb1ab6e9db18c2188619723fb7d_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-1af3fd821d07b6d7c90e6a5fd2b63e45_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"656\" data-rawheight=\"214\" class=\"origin_image zh-lightbox-thumb\" width=\"656\" data-original=\"https://pic2.zhimg.com/v2-1af3fd821d07b6d7c90e6a5fd2b63e45_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;656&#39; height=&#39;214&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"656\" data-rawheight=\"214\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"656\" data-original=\"https://pic2.zhimg.com/v2-1af3fd821d07b6d7c90e6a5fd2b63e45_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-1af3fd821d07b6d7c90e6a5fd2b63e45_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-d694074ad2ca032cf6bc2f61e11f1c41_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"664\" data-rawheight=\"214\" class=\"origin_image zh-lightbox-thumb\" width=\"664\" data-original=\"https://pic2.zhimg.com/v2-d694074ad2ca032cf6bc2f61e11f1c41_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;664&#39; height=&#39;214&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"664\" data-rawheight=\"214\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"664\" data-original=\"https://pic2.zhimg.com/v2-d694074ad2ca032cf6bc2f61e11f1c41_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-d694074ad2ca032cf6bc2f61e11f1c41_b.jpg\"/></figure><p><b>注：此处 <img src=\"https://www.zhihu.com/equation?tex=a%5E%7B%5B1%5D%28i%29%7D\" alt=\"a^{[1](i)}\" eeimg=\"1\"/> 中[1]表示神经网络第一层,<i>(i)</i>表示第<i>i </i>批样本</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-723c73cb3f0f9f1614fc9fc931f6850a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"428\" data-rawheight=\"106\" class=\"origin_image zh-lightbox-thumb\" width=\"428\" data-original=\"https://pic3.zhimg.com/v2-723c73cb3f0f9f1614fc9fc931f6850a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;428&#39; height=&#39;106&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"428\" data-rawheight=\"106\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"428\" data-original=\"https://pic3.zhimg.com/v2-723c73cb3f0f9f1614fc9fc931f6850a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-723c73cb3f0f9f1614fc9fc931f6850a_b.jpg\"/></figure><h2><b>2.简单神经网络的推导</b></h2><p><b>损失函数Cost function公式： </b><img src=\"https://www.zhihu.com/equation?tex=J%28W%5E%7B%5B1%5D%7D%2Cb%5E%7B%5B1%5D%7D%2CW%5E%7B%5B2%5D%7D%2Cb%5E%7B%5B2%5D%7D%29+%3D+%5Cfrac%7B1%7D%7Bm%7D+%5CSigma%5Em_%7Bi%3D1%7DL%28%5Cwidehat%7By%7D%2Cy%29\" alt=\"J(W^{[1]},b^{[1]},W^{[2]},b^{[2]}) = \\frac{1}{m} \\Sigma^m_{i=1}L(\\widehat{y},y)\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=dW%5E%7B%5B1%5D%7D+%3D+%5Cfrac%7B%5Cvartheta+J%7D%7B%5Cvartheta+W%5E%7B%5B1%5D%7D%7D%2C+db%5E%7B%5B1%5D%7D+%3D+%5Cfrac%7B%5Cvartheta+J%7D%7B%5Cvartheta+b%5E%7B%5B1%5D%7D%7D\" alt=\"dW^{[1]} = \\frac{\\vartheta J}{\\vartheta W^{[1]}}, db^{[1]} = \\frac{\\vartheta J}{\\vartheta b^{[1]}}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=W%5E%7B%5B1%5D%7D+%3A%3D+W%5E%7B%5B1%5D%7D+-%5Calpha+dW%5E%7B%5B1%5D%7D+%2C+b%5E%7B%5B1%5D%7D+%3A%3D+b%5E%7B%5B1%5D%7D+-%5Calpha+db%5E%7B%5B1%5D%7D\" alt=\"W^{[1]} := W^{[1]} -\\alpha dW^{[1]} , b^{[1]} := b^{[1]} -\\alpha db^{[1]}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=dW%5E%7B%5B2%5D%7D+%3D+%5Cfrac%7B%5Cvartheta+J%7D%7B%5Cvartheta+W%5E%7B%5B2%5D%7D%7D%2C+db%5E%7B%5B2%5D%7D+%3D+%5Cfrac%7B%5Cvartheta+J%7D%7B%5Cvartheta+b%5E%7B%5B2%5D%7D%7D\" alt=\"dW^{[2]} = \\frac{\\vartheta J}{\\vartheta W^{[2]}}, db^{[2]} = \\frac{\\vartheta J}{\\vartheta b^{[2]}}\" eeimg=\"1\"/> </p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>正向传播forward propagation：</b></p><p><img src=\"https://www.zhihu.com/equation?tex=Z%5E%7B%5B1%5D%7D+%3D+W%5E%7B%5B1%5D%7Dx+%2B+b%5E%7B%5B1%5D%7D\" alt=\"Z^{[1]} = W^{[1]}x + b^{[1]}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=A%5E%7B%5B1%5D%7D+%3D+%5Csigma%28+z%5E%7B%5B1%5D%7D%29\" alt=\"A^{[1]} = \\sigma( z^{[1]})\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=Z%5E%7B%5B2%5D%7D+%3D+W%5E%7B%5B2%5D%7DA%5E%7B%5B1%5D%7D+%2B+b%5E%7B%5B2%5D%7D\" alt=\"Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=A%5E%7B%5B2%5D%7D+%3D+%5Csigma%28+z%5E%7B%5B2%5D%7D%29\" alt=\"A^{[2]} = \\sigma( z^{[2]})\" eeimg=\"1\"/> </p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>反向传播back propagation:</b></p><p><img src=\"https://www.zhihu.com/equation?tex=dZ%5E%7B%5B2%5D%7D+%3D+A%5E%7B%5B2%5D%7D-Y%2CY%3D%5By%5E%7B%5B1%5D%7D+y%5E%7B%5B2%5D%7D...y%5E%7B%5Bm%5D%7D%5D\" alt=\"dZ^{[2]} = A^{[2]}-Y,Y=[y^{[1]} y^{[2]}...y^{[m]}]\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=+Y%E8%A1%A8%E7%A4%BA%E4%BA%86m%E4%B8%AA%E6%A0%B7%E6%9C%AC%EF%BC%8Cy%5E%7B%28i%29%7D%E8%A1%A8%E7%A4%BA%E7%AC%ACi%E4%B8%AA%E6%A0%B7%E6%9C%AC%E7%9A%84%E9%A2%84%E6%B5%8B%E5%80%BC%E7%9F%A9%E9%98%B5\" alt=\" Y表示了m个样本，y^{(i)}表示第i个样本的预测值矩阵\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=dW%5E%7B%5B2%5D%7D+%3D+%5Cfrac%7B1%7D%7Bm%7D+dZ%5E%7B%5B2%5D%7D+A%5E%7B%5B1%5DT%7D\" alt=\"dW^{[2]} = \\frac{1}{m} dZ^{[2]} A^{[1]T}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=db%5E%7B%5B2%5D%7D+%3D+%5Cfrac%7B1%7D%7Bm%7Dnp.sum%28dZ%5E%7B%5B2%5D%7D%2C+axis%3D1%2C+keepdims+%3D+True%29\" alt=\"db^{[2]} = \\frac{1}{m}np.sum(dZ^{[2]}, axis=1, keepdims = True)\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=dZ%5E%7B%5B1%5D%7D+%3D+W%5E%7B%5B2%5DT%7DdZ%5E%7B%5B2%5D%7D+%2A+g%27%5E%7B%5B1%5D%7D%28Z%5E%7B%5B1%5D%7D%29\" alt=\"dZ^{[1]} = W^{[2]T}dZ^{[2]} * g&#39;^{[1]}(Z^{[1]})\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=dW%5E%7B%5B1%5D%7D+%3D+%5Cfrac%7B1%7D%7Bm%7D+dZ%5E%7B%5B1%5D%7D+X%5E%7BT%7D\" alt=\"dW^{[1]} = \\frac{1}{m} dZ^{[1]} X^{T}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=db%5E%7B%5B1%5D%7D+%3D+%5Cfrac%7B1%7D%7Bm%7Dnp.sum%28dZ%5E%7B%5B1%5D%7D%2C+axis%3D1%2C+keepdims+%3D+True%29\" alt=\"db^{[1]} = \\frac{1}{m}np.sum(dZ^{[1]}, axis=1, keepdims = True)\" eeimg=\"1\"/> </p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>dW的公式推导：</b></p><p><img src=\"https://www.zhihu.com/equation?tex=dW%5E%7B%5B2%5D%7D+%3D+%5Cfrac%7B%5Cvartheta+J%7D%7B%5Cvartheta+W%5E%7B%5B2%5D%7D%7D+%3D++%5Cfrac%7B%5Cvartheta+J%7D%7B%5Cvartheta+Z%5E%7B%5B2%5D%7D%7D+%2A+%5Cfrac%7B%5Cvartheta+Z%5E%7B%5B2%5D%7D%7D%7B%5Cvartheta+W%5E%7B%5B2%5D%7D%7D+%3D+dZ%5E%7B%5B2%5D%7D%2A%5Cfrac%7B%5Cvartheta+%28W%5E%7B%5B2%5D%7DA%5E%7B%5B1%5D%7D%2Bb%29%7D%7B%5Cvartheta+W%5E%7B%5B2%5D%7D%7D+%3D++dZ%5E%7B%5B2%5D%7DA%5E%7B%5B1%5D%7D+%3D+%5Cfrac%7B1%7D%7Bm%7DdZ%5E%7B%5B2%5D%7DA%5E%7B%5B1%5DT%7D\" alt=\"dW^{[2]} = \\frac{\\vartheta J}{\\vartheta W^{[2]}} =  \\frac{\\vartheta J}{\\vartheta Z^{[2]}} * \\frac{\\vartheta Z^{[2]}}{\\vartheta W^{[2]}} = dZ^{[2]}*\\frac{\\vartheta (W^{[2]}A^{[1]}+b)}{\\vartheta W^{[2]}} =  dZ^{[2]}A^{[1]} = \\frac{1}{m}dZ^{[2]}A^{[1]T}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7Bm%7D%E5%92%8CA%5E%7B%5B1%5DT%7D\" alt=\"\\frac{1}{m}和A^{[1]T}\" eeimg=\"1\"/> </p><p>1.这里需要注意的是，我们会repeaat i次（i从1~m），每次都会计算dW，推导公式中的</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cvartheta+J\" alt=\"\\vartheta J\" eeimg=\"1\"/> 表示第i次过程汇总的损失。故 <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cvartheta+J%7D%7B%5Cvartheta+Z%5E%7B%5B2%5D%7D%7D+%3D+%5Cfrac%7B1%7D%7Bm%7DdZ%5E%7B%5B2%5D%7D\" alt=\"\\frac{\\vartheta J}{\\vartheta Z^{[2]}} = \\frac{1}{m}dZ^{[2]}\" eeimg=\"1\"/> </p><p>2. <img src=\"https://www.zhihu.com/equation?tex=dW%5E%7B%5B2%5D%7D\" alt=\"dW^{[2]}\" eeimg=\"1\"/> 是pxq矩阵，其中p表示第二层神经元数量、q表示第一层神经元数量； <img src=\"https://www.zhihu.com/equation?tex=dZ%5E%7B%5B2%5D%7D\" alt=\"dZ^{[2]}\" eeimg=\"1\"/> </p><p>是px1矩阵； <img src=\"https://www.zhihu.com/equation?tex=A%5E%7B%5B1%5D%7D\" alt=\"A^{[1]}\" eeimg=\"1\"/> 是qx1矩阵，为了保持队形，故将其转置。</p><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>3.神经网络反向传播推导</b></h2><p><b>3.1前向传播</b></p><p><img src=\"https://www.zhihu.com/equation?tex=z%5E%7B%5Bl%5D%7D+%3D+W%5E%7B%5Bl%5D%7D.a%5E%7B%5Bl-1%5D%7D+%2B+b%5E%7B%5Bl%5D%7D\" alt=\"z^{[l]} = W^{[l]}.a^{[l-1]} + b^{[l]}\" eeimg=\"1\"/>  ，b为偏差</p><p><b>第<i>l </i>层的结果矩阵:</b> </p><p><img src=\"https://www.zhihu.com/equation?tex=a%5E%7B%5Bl%5D%7D+%3D+g%5E%7B%5Bl%5D%7D%28z%5E%7B%5Bl%5D%7D%29\" alt=\"a^{[l]} = g^{[l]}(z^{[l]})\" eeimg=\"1\"/>                  ，g(z)为激活函数</p><p><b>向量化地表示：</b></p><p><img src=\"https://www.zhihu.com/equation?tex=Z%5E%7B%5Bl%5D%7D+%3D+W%5E%7B%5Bl%5D%7D.A%5E%7B%5Bl-1%5D%7D+%2B+b%5E%7B%5Bl%5D%7D\" alt=\"Z^{[l]} = W^{[l]}.A^{[l-1]} + b^{[l]}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=A%5E%7B%5Bl%5D%7D+%3D+g%5E%7B%5Bl%5D%7D%28Z%5E%7B%5Bl%5D%7D%29\" alt=\"A^{[l]} = g^{[l]}(Z^{[l]})\" eeimg=\"1\"/> </p><p>我们从<b><i>l = 0 </i></b>开始，即 <img src=\"https://www.zhihu.com/equation?tex=A%5E%7B%5B0%5D%7D+%3D+X\" alt=\"A^{[0]} = X\" eeimg=\"1\"/> 来表示输入层矩阵，开始计算 <img src=\"https://www.zhihu.com/equation?tex=z%5E%7B%5B1%5D%7D+%3D+W%5E%7B%5B1%5D%7D.A%5E%7B%5B0%5D%7D+%2B+b%5E%7B%5B1%5D%7D\" alt=\"z^{[1]} = W^{[1]}.A^{[0]} + b^{[1]}\" eeimg=\"1\"/> 重复此过程直到最后的输出层：<img src=\"https://www.zhihu.com/equation?tex=z%5E%7B%5BL%5D%7D+%3D+W%5E%7B%5BL%5D%7D.A%5E%7B%5BL-1%5D%7D+%2B+b%5E%7B%5BL%5D%7D%2CA%5E%7B%5BL%5D%7D+%3D+g%5E%7B%5BL%5D%7D%28Z%5E%7B%5BL%5D%7D%29\" alt=\"z^{[L]} = W^{[L]}.A^{[L-1]} + b^{[L]},A^{[L]} = g^{[L]}(Z^{[L]})\" eeimg=\"1\"/> ，便完成了整个前向传播的推导</p><p class=\"ztext-empty-paragraph\"><br/></p><p><b>3.2反向传播</b></p><p>      完成前向传播的推导后，我们知道了神经网络每一层的激活值，于是可以根据激活值从后往前逐层推导每一层的偏导。</p><p><b>向量化表示的步骤如下：</b></p><p><img src=\"https://www.zhihu.com/equation?tex=dZ%5E%7B%5Bl%5D%7D+%3D+dA%5E%7B%5Bl%5D%7D%2A%5Bg%5E%7B%5Bl%5D%7D%28Z%5E%7B%5Bl%5D%7D%29%5D%27\" alt=\"dZ^{[l]} = dA^{[l]}*[g^{[l]}(Z^{[l]})]&#39;\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=dW%5E%7B%5Bl%5D%7D+%3D+%5Cfrac%7B1%7D%7Bm%7DdZ%5E%7B%5Bl%5D%7D.A%5E%7B%5Bl-1%5DT%7D\" alt=\"dW^{[l]} = \\frac{1}{m}dZ^{[l]}.A^{[l-1]T}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=db%5E%7B%5Bl%5D%7D+%3D+%5Cfrac%7B1%7D%7Bm%7Dnp.sum%28dZ%5E%7B%5Bl%5D%7D%2C+axis+%3D+1+%2Ckeepdims+%3D+True%29\" alt=\"db^{[l]} = \\frac{1}{m}np.sum(dZ^{[l]}, axis = 1 ,keepdims = True)\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=dA%5E%7B%5Bl-1%5D%7D+%3D+W%5E%7B%5Bl%5DT%7D.dZ%5E%7B%5Bl%5D%7D\" alt=\"dA^{[l-1]} = W^{[l]T}.dZ^{[l]}\" eeimg=\"1\"/> </p><p></p><p></p><p></p>", 
            "topic": [
                {
                    "tag": "吴恩达（Andrew Ng）", 
                    "tagLink": "https://api.zhihu.com/topics/20003150"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "反向传播", 
                    "tagLink": "https://api.zhihu.com/topics/20682860"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/76066325", 
            "userName": "Lyon", 
            "userLink": "https://www.zhihu.com/people/38495a9b1f83e4a612543ce6c523df7f", 
            "upvote": 2, 
            "title": "【吴恩达深度学习】—梯度爆炸/消失、权重初始化和梯度检验", 
            "content": "<h2>前言：</h2><p>继吴恩达机器学习公开课之后，吴恩达大佬又和网易合作，免费开放了深度学习的课程，感谢大佬，感谢网易，同样感谢提供配套课程讲义的黄海广博士。</p><p>本文主要是个人的学习总结，原内容源于视频第一门课：神经网络和深度学习—第三周，文章内容同步发布在<a href=\"https://link.zhihu.com/?target=https%3A//www.yuque.com/docs/share/3bb85013-03d4-4dfe-8c42-8dc07eeddca7\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">语雀文档</a>上。</p><p><b>课程资源</b>： <a href=\"https://link.zhihu.com/?target=https%3A//mooc.study.163.com/smartSpec/detail/1001319001.htm\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">网易云课堂课程资源</a> <a href=\"https://link.zhihu.com/?target=https%3A//github.com/fengdu78/deeplearning_ai_books\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">课程配套讲义</a></p><hr/><p><b>吴恩达深度学习2—梯度爆炸、权重初始化、梯度检验</b></p><p><b>1.梯度消失/梯度爆炸</b></p><p>      训练神经网络，尤其是深度神经所面临的一个问题就是<b>梯度消失或梯度爆炸(Vanishing/Exploding gradients)</b>，也就是你训练神经网络的时候，导数或坡度有时会变得非常大，或者非常小，甚至于以指数方式变小，这加大了训练的难度。</p><p>举个例子，假设我们有如下神经网络层：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-75e3af7bda2eeda67fdd679689f0ca69_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"769\" data-rawheight=\"172\" class=\"origin_image zh-lightbox-thumb\" width=\"769\" data-original=\"https://pic2.zhimg.com/v2-75e3af7bda2eeda67fdd679689f0ca69_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;769&#39; height=&#39;172&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"769\" data-rawheight=\"172\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"769\" data-original=\"https://pic2.zhimg.com/v2-75e3af7bda2eeda67fdd679689f0ca69_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-75e3af7bda2eeda67fdd679689f0ca69_b.jpg\"/></figure><p>      网络层数 =<b><i> L</i></b>，每层包含2个激活单元，每一层的权重矩阵<b><i>W</i></b>都相同，且</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-b03f9d5e1b2692516f6fb2cca1331954_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"320\" data-rawheight=\"112\" class=\"content_image\" width=\"320\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;320&#39; height=&#39;112&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"320\" data-rawheight=\"112\" class=\"content_image lazy\" width=\"320\" data-actualsrc=\"https://pic1.zhimg.com/v2-b03f9d5e1b2692516f6fb2cca1331954_b.jpg\"/></figure><p>，且假设bias都为0为了简单起见，激活函数我们不用sigmoid/ReLu,而使用传统的线性激活函数 <img src=\"https://www.zhihu.com/equation?tex=g%28z%29+%3D+z\" alt=\"g(z) = z\" eeimg=\"1\"/> ，则假设函数可以写成：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-4473cf72bdd35cf20d488c5dc698f749_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1042\" data-rawheight=\"120\" class=\"origin_image zh-lightbox-thumb\" width=\"1042\" data-original=\"https://pic2.zhimg.com/v2-4473cf72bdd35cf20d488c5dc698f749_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1042&#39; height=&#39;120&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1042\" data-rawheight=\"120\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1042\" data-original=\"https://pic2.zhimg.com/v2-4473cf72bdd35cf20d488c5dc698f749_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-4473cf72bdd35cf20d488c5dc698f749_b.jpg\"/></figure><p>      假设x1 = x2 = 1，则经过神经网络模型得出的预测值 <img src=\"https://www.zhihu.com/equation?tex=y+%3D+1.5%5EL\" alt=\"y = 1.5^L\" eeimg=\"1\"/> ,如果对于一个深度神经网络来说如果<b><i>L</i></b>足够大，即网络足够深，可以看见其预测值<b><i>y</i></b>将呈现指数级增长，增长的比率是 <img src=\"https://www.zhihu.com/equation?tex=1.5%5EL\" alt=\"1.5^L\" eeimg=\"1\"/> ，同理，如果权重矩阵是</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-29006156bb24086033742dd574b3d172_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"320\" data-rawheight=\"112\" class=\"content_image\" width=\"320\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;320&#39; height=&#39;112&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"320\" data-rawheight=\"112\" class=\"content_image lazy\" width=\"320\" data-actualsrc=\"https://pic3.zhimg.com/v2-29006156bb24086033742dd574b3d172_b.jpg\"/></figure><p>，则将是指数下降。最近 Microsoft 对 152 层神经网络的研究取得了很大进展，在这样一个深度神经网络中，如果激活函数或梯度函数以与<b><i>L</i></b>相关的指数增长或递减，它们的值将会变得极大或极小，从而导致训练难度上升，尤其是梯度指数小于<b><i>L</i></b>时，梯度下降算法的步长会非常非常小，梯度下降算法将花费很长时间来学习。这便是梯度消失和梯度爆炸的含义。</p><p><b>2.神经网络的权重初始化</b></p><p>      上节课，我们学习了深度神经网络如何产生梯度消失和梯度爆炸问题，最终针对该问题，我们想出了一个不完整的解决方案，虽然不能彻底解决问题，却很有用，有助于我们为神经网络更谨慎地选择随机初始化参数，为了更好地理解它，我们先举一个神经单元初始化地例子，然后再演变到整个深度网络</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-c9b375428012456bea7b25f88e608ccf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"486\" data-rawheight=\"271\" class=\"origin_image zh-lightbox-thumb\" width=\"486\" data-original=\"https://pic4.zhimg.com/v2-c9b375428012456bea7b25f88e608ccf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;486&#39; height=&#39;271&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"486\" data-rawheight=\"271\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"486\" data-original=\"https://pic4.zhimg.com/v2-c9b375428012456bea7b25f88e608ccf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-c9b375428012456bea7b25f88e608ccf_b.jpg\"/></figure><p>      该单个神经元有4个输入特征x1到x4，经过a = g(z)处理，最终得到预测值y，暂时忽略偏差b，在计算z的时候，我们有方程：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-e6bb80cbe9b5580a4f087e581d5f150a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"528\" data-rawheight=\"42\" class=\"origin_image zh-lightbox-thumb\" width=\"528\" data-original=\"https://pic3.zhimg.com/v2-e6bb80cbe9b5580a4f087e581d5f150a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;528&#39; height=&#39;42&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"528\" data-rawheight=\"42\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"528\" data-original=\"https://pic3.zhimg.com/v2-e6bb80cbe9b5580a4f087e581d5f150a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-e6bb80cbe9b5580a4f087e581d5f150a_b.jpg\"/></figure><p> 为了预防z的值过大或过小，我们最优化的方式是取</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-14ad16d151929e0fc019e56e0a622898_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"246\" data-rawheight=\"94\" class=\"content_image\" width=\"246\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;246&#39; height=&#39;94&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"246\" data-rawheight=\"94\" class=\"content_image lazy\" width=\"246\" data-actualsrc=\"https://pic1.zhimg.com/v2-14ad16d151929e0fc019e56e0a622898_b.jpg\"/></figure><p><b><i>Var</i></b>表示求方差的意思，权重的方差 = 1/n,即权重分布符合~ <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7B%5Csqrt+n%7D\" alt=\"\\frac{1}{\\sqrt n}\" eeimg=\"1\"/> 。标准的权重初始化公式在Python中可以表示如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-d177a2bbe405092b8e75f0a810a51745_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"912\" data-rawheight=\"100\" class=\"origin_image zh-lightbox-thumb\" width=\"912\" data-original=\"https://pic2.zhimg.com/v2-d177a2bbe405092b8e75f0a810a51745_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;912&#39; height=&#39;100&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"912\" data-rawheight=\"100\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"912\" data-original=\"https://pic2.zhimg.com/v2-d177a2bbe405092b8e75f0a810a51745_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-d177a2bbe405092b8e75f0a810a51745_b.jpg\"/></figure><p>其中， <img src=\"https://www.zhihu.com/equation?tex=n%5E%7B%5Bl-1%5D%7D\" alt=\"n^{[l-1]}\" eeimg=\"1\"/> 就是喂给第l层神经单元的数量，即l-1层神经元数量。上面的公式通常适用于<b><i>tanh</i></b>激活函数，<b>如果激活函数用的是ReLu,则</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-ebde8c44d6eafcdce62ac02631e872e1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"246\" data-rawheight=\"94\" class=\"content_image\" width=\"246\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;246&#39; height=&#39;94&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"246\" data-rawheight=\"94\" class=\"content_image lazy\" width=\"246\" data-actualsrc=\"https://pic2.zhimg.com/v2-ebde8c44d6eafcdce62ac02631e872e1_b.jpg\"/></figure><p><b>更适合。具体证明和推导可以参考相关论文。</b></p><p>      希望你现在对梯度消失或爆炸问题以及如何为权重初始化合理值已经有了一个直观认识，希望你设置的权重矩阵既不会增长过快，也不会太快下降到 0，从而训练出一个权重或梯度不会增长或消失过快的深度网络。我们在训练深度网络时，这也是一个加快训练速度的技巧。</p><p><b>3.梯度的数值逼近</b></p><p>      在实施 backprop(反向传播)时，有一个测试叫做<b>梯度检验，它的作用是确保反向传播正确实施</b>。因为有时候，反向传播求梯度时会有些问题，譬如在代码经行矩阵计算中可能会有bug。为了逐渐实现梯度检验，我们需要先了解<b>梯度的数值逼近</b>，下面看一个例子：</p><p>假设函数</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-80e8652db0d2e43b2ac52c738eeedf3f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"170\" data-rawheight=\"58\" class=\"content_image\" width=\"170\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;170&#39; height=&#39;58&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"170\" data-rawheight=\"58\" class=\"content_image lazy\" width=\"170\" data-actualsrc=\"https://pic4.zhimg.com/v2-80e8652db0d2e43b2ac52c738eeedf3f_b.jpg\"/></figure><p>我们要在θ = 1的地方对其实行梯度逼近，逼近的策略其实可以从两个方向，</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-1b296b4320a07f68b9c9bff5cbd078b2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"214\" data-rawheight=\"60\" class=\"content_image\" width=\"214\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;214&#39; height=&#39;60&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"214\" data-rawheight=\"60\" class=\"content_image lazy\" width=\"214\" data-actualsrc=\"https://pic3.zhimg.com/v2-1b296b4320a07f68b9c9bff5cbd078b2_b.jpg\"/></figure><p>。这里取ε = 0.01，如下图我们可以得到两个小三角形：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-3aa81eb998690599b9974e77ee41df37_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"304\" data-rawheight=\"283\" class=\"content_image\" width=\"304\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;304&#39; height=&#39;283&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"304\" data-rawheight=\"283\" class=\"content_image lazy\" width=\"304\" data-actualsrc=\"https://pic4.zhimg.com/v2-3aa81eb998690599b9974e77ee41df37_b.jpg\"/></figure><p>我们用小三角形的高度/宽度之比可以分别模拟在θ = 1处的梯度(导数)，所谓梯度逼近，就是求在θ = 1附近，梯度最接近其真实值的坐标。根据导数的定义和数学证明，我们发现，用大三角型的高度/宽度更接近函数在θ = 1的导数，导数值 = 3θ = 3*1 = 3</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-341218c80b1590facfce4c23c64c02c2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"361\" data-rawheight=\"320\" class=\"content_image\" width=\"361\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;361&#39; height=&#39;320&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"361\" data-rawheight=\"320\" class=\"content_image lazy\" width=\"361\" data-actualsrc=\"https://pic3.zhimg.com/v2-341218c80b1590facfce4c23c64c02c2_b.jpg\"/></figure><p>此时</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-2cbdbd43f2b07150f2d827cecbcc6b49_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"524\" data-rawheight=\"102\" class=\"origin_image zh-lightbox-thumb\" width=\"524\" data-original=\"https://pic2.zhimg.com/v2-2cbdbd43f2b07150f2d827cecbcc6b49_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;524&#39; height=&#39;102&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"524\" data-rawheight=\"102\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"524\" data-original=\"https://pic2.zhimg.com/v2-2cbdbd43f2b07150f2d827cecbcc6b49_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-2cbdbd43f2b07150f2d827cecbcc6b49_b.jpg\"/></figure><p> 最接近其真实导数值3。这个表达式计算出的导数误差叫双边误差，而用</p><p>小三角形计算的</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-fbbe1c6e635ebd8b1aaf81ad0e3a212c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"284\" data-rawheight=\"102\" class=\"content_image\" width=\"284\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;284&#39; height=&#39;102&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"284\" data-rawheight=\"102\" class=\"content_image lazy\" width=\"284\" data-actualsrc=\"https://pic1.zhimg.com/v2-fbbe1c6e635ebd8b1aaf81ad0e3a212c_b.jpg\"/></figure><p>或者</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-f66bbf8d227534d3042cba99353dd2eb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"284\" data-rawheight=\"102\" class=\"content_image\" width=\"284\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;284&#39; height=&#39;102&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"284\" data-rawheight=\"102\" class=\"content_image lazy\" width=\"284\" data-actualsrc=\"https://pic4.zhimg.com/v2-f66bbf8d227534d3042cba99353dd2eb_b.jpg\"/></figure><p>则称为单边误差，可以证明双边误差计算出的误差最小，同时也符号导数公式的定义</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-bbb626bb83a1bebbd70d6e944c2bc413_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"496\" data-rawheight=\"102\" class=\"origin_image zh-lightbox-thumb\" width=\"496\" data-original=\"https://pic4.zhimg.com/v2-bbb626bb83a1bebbd70d6e944c2bc413_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;496&#39; height=&#39;102&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"496\" data-rawheight=\"102\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"496\" data-original=\"https://pic4.zhimg.com/v2-bbb626bb83a1bebbd70d6e944c2bc413_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-bbb626bb83a1bebbd70d6e944c2bc413_b.jpg\"/></figure><p><b>4.反向传播的梯度检验</b></p><p><b>梯度检验的目的就在于检测反向传播计算过程中的bug，检验 backprop 的实施是否正确。</b></p><p>      假设神经网络中有下列参数：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-6a337890bc5ee5b5cb5ca66980f536f1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"582\" data-rawheight=\"66\" class=\"origin_image zh-lightbox-thumb\" width=\"582\" data-original=\"https://pic2.zhimg.com/v2-6a337890bc5ee5b5cb5ca66980f536f1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;582&#39; height=&#39;66&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"582\" data-rawheight=\"66\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"582\" data-original=\"https://pic2.zhimg.com/v2-6a337890bc5ee5b5cb5ca66980f536f1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-6a337890bc5ee5b5cb5ca66980f536f1_b.jpg\"/></figure><p>，为了执行梯度检验，需要把所有的W矩阵转化为参数向量θ，针对参数向量θ中的每个组成元素，其梯度</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-1b28583afa8ec91453bef6282c05929e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"386\" data-rawheight=\"112\" class=\"content_image\" width=\"386\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;386&#39; height=&#39;112&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"386\" data-rawheight=\"112\" class=\"content_image lazy\" width=\"386\" data-actualsrc=\"https://pic3.zhimg.com/v2-1b28583afa8ec91453bef6282c05929e_b.jpg\"/></figure><p>,我们用双边误差来计算其逼近梯度，逼近梯度的表达式如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-c039fe79c0cbfd460c592e7df0d89128_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1070\" data-rawheight=\"102\" class=\"origin_image zh-lightbox-thumb\" width=\"1070\" data-original=\"https://pic1.zhimg.com/v2-c039fe79c0cbfd460c592e7df0d89128_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1070&#39; height=&#39;102&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1070\" data-rawheight=\"102\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1070\" data-original=\"https://pic1.zhimg.com/v2-c039fe79c0cbfd460c592e7df0d89128_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-c039fe79c0cbfd460c592e7df0d89128_b.jpg\"/></figure><p>      然后，我们用下面的式子计算梯度间的误差程度：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-90665cca0835dd33b4ca8c92c6277879_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"374\" data-rawheight=\"118\" class=\"content_image\" width=\"374\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;374&#39; height=&#39;118&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"374\" data-rawheight=\"118\" class=\"content_image lazy\" width=\"374\" data-actualsrc=\"https://pic2.zhimg.com/v2-90665cca0835dd33b4ca8c92c6277879_b.jpg\"/></figure><p>      譬如此时我用 <img src=\"https://www.zhihu.com/equation?tex=%5Cepsilon+%3D+10%5E%7B-7%7D\" alt=\"\\epsilon = 10^{-7}\" eeimg=\"1\"/> 来计算，通过计算得到次方程式的值&lt;= <img src=\"https://www.zhihu.com/equation?tex=10%5E%7B-7%7D\" alt=\"10^{-7}\" eeimg=\"1\"/> ，则梯度下降是正常的，如果得到的值 <img src=\"https://www.zhihu.com/equation?tex=10%5E%7B-5%7D\" alt=\"10^{-5}\" eeimg=\"1\"/> 或者更大如 <img src=\"https://www.zhihu.com/equation?tex=10%5E%7B-3%7D\" alt=\"10^{-3}\" eeimg=\"1\"/> ，则需要检测所有的θ项，看看是否存在一个i值，使得 <img src=\"https://www.zhihu.com/equation?tex=d%5Ctheta_%7Bapprox%7D%5Bi%5D%E5%92%8Cd%5Ctheta%5Bi%5D\" alt=\"d\\theta_{approx}[i]和d\\theta[i]\" eeimg=\"1\"/> 差距过大。</p><p><b>注意事项：</b></p><ul><li>首先，不要在训练中使用梯度检验，它只用于调试</li><li>第二点，如果算法的梯度检验失败，要检查所有项，检查每一项，并试着找出 bug</li><li>第三点，在实施梯度检验时，如果使用正则化，请注意正则项</li><li><b>梯度检验不能与 dropout 同时使用</b>，因为每次迭代过程中， dropout 会随机消除隐藏层单元的不同子集，难以计算 dropout 在梯度下降上的代价函数<b><i>J</i></b></li></ul><p></p>", 
            "topic": [
                {
                    "tag": "梯度爆炸问题", 
                    "tagLink": "https://api.zhihu.com/topics/20687248"
                }, 
                {
                    "tag": "梯度消失问题", 
                    "tagLink": "https://api.zhihu.com/topics/20687965"
                }, 
                {
                    "tag": "吴恩达（Andrew Ng）", 
                    "tagLink": "https://api.zhihu.com/topics/20003150"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/76065523", 
            "userName": "Lyon", 
            "userLink": "https://www.zhihu.com/people/38495a9b1f83e4a612543ce6c523df7f", 
            "upvote": 3, 
            "title": "【吴恩达深度学习】—参数、超参数、正则化", 
            "content": "<h2>前言：</h2><p>继吴恩达机器学习公开课之后，吴恩达大佬又和网易合作，免费开放了深度学习的课程，感谢大佬，感谢网易，同样感谢提供配套课程讲义的黄海广博士。</p><p>本文主要是个人的学习总结，原内容源于视频第二门课：改善深层神经网络—第一周，文章内容同步发布在<a href=\"https://link.zhihu.com/?target=https%3A//www.yuque.com/docs/share/16291c8e-9994-417d-b590-099b0c29db90\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">语雀文档</a>上。</p><p><b>课程资源</b>： <a href=\"https://link.zhihu.com/?target=https%3A//mooc.study.163.com/smartSpec/detail/1001319001.htm\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">网易云课堂课程资源</a> <a href=\"https://link.zhihu.com/?target=https%3A//github.com/fengdu78/deeplearning_ai_books\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">课程配套讲义</a></p><hr/><p><b>吴恩达深度学习2—参数、超参数、正则化</b></p><p><b>1.参数 VS 超参数</b></p><p><b>1.1 什么是超参数（Hyperparameters ）？</b></p><p>      比如算法中的 learning rate <img src=\"https://www.zhihu.com/equation?tex=%5Calpha\" alt=\"\\alpha\" eeimg=\"1\"/> （学习率）、 iterations(梯度下降法循环的数量)、 </p><p><img src=\"https://www.zhihu.com/equation?tex=L\" alt=\"L\" eeimg=\"1\"/> （隐藏层数目）、 <img src=\"https://www.zhihu.com/equation?tex=n%5E%7B%5Bl%5D%7D\" alt=\"n^{[l]}\" eeimg=\"1\"/> （隐藏层单元数目）、 choice of activation function（激活函数的选择）都需要你来设置，这些数字实际上控制了最后的参数<b><i>W</i></b>和<b><i>b</i></b>的值，所以它们被称作超参数。</p><p>      实际上深度学习有很多不同的超参数，之后我们也会介绍一些其他的超参数，如momentum、 mini batch size、 regularization parameters 等等。</p><p><b>1.2 如何寻找超参数的最优值？</b></p><p><b>今天的深度学习应用领域，还是很经验性的过程</b>，通常你有个想法，比如你可能大致知道一个最好的学习率值，可能说 <img src=\"https://www.zhihu.com/equation?tex=%5Calpha+%3D+0.01\" alt=\"\\alpha = 0.01\" eeimg=\"1\"/> 最好，我会想先试试看，然后你可以实际试一下，训练一下看看效果如何。然后基于尝试的结果你会发现，你觉得学习率设定再提高到 0.05 会比较好。如果你不确定什么值是最好的，你大可以先试试一个学习率 <img src=\"https://www.zhihu.com/equation?tex=%5Calpha\" alt=\"\\alpha\" eeimg=\"1\"/> ，再看看损失函数<b><i>J</i></b>的值有没有下降。</p><p><b>这可能的确是深度学习比较让人不满的一部分，也就是你必须尝试很多次不同可能性。</b></p><p><b>2.神经网络中的正则化</b></p><p>      在深度学习神经网络中，除了传统的L2正则化，还有Dropout正则化方式，正则化的目的在于降低模型的过拟合程度，除了常用的这两种方式，降低过拟合程度还可以采用数据扩增和early stopping的方式。</p><p><b>2.1 L2正则化</b></p><p>      在之前的机器学习第三周的文章中，我们介绍了线性回归和逻辑回归中的正则化，这种正则化方式称为：<b>L2正则化</b><a href=\"https://zhuanlan.zhihu.com/p/73404297\" class=\"internal\"><span class=\"invisible\">https://</span><span class=\"visible\">zhuanlan.zhihu.com/p/73</span><span class=\"invisible\">404297</span><span class=\"ellipsis\"></span></a></p><p><b>2.2 Dropout正则化</b></p><p>      在深度学习模型中，由于引入了神经元，故正则化的方式也有新的方式，我们可以采用Dropout正则化（Dropout Regularization），Dropout当然不能翻译为辍学，其意思为随机失活，即对神经网络中的神经元做随机失活处理。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-b801d2808a51513db317b713e7b44f7b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"439\" data-rawheight=\"232\" class=\"origin_image zh-lightbox-thumb\" width=\"439\" data-original=\"https://pic4.zhimg.com/v2-b801d2808a51513db317b713e7b44f7b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;439&#39; height=&#39;232&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"439\" data-rawheight=\"232\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"439\" data-original=\"https://pic4.zhimg.com/v2-b801d2808a51513db317b713e7b44f7b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-b801d2808a51513db317b713e7b44f7b_b.jpg\"/></figure><p>      假设你在训练上图这样的神经网络，它存在过拟合，这就是 dropout 所要处理的，我们复制这个神经网络， dropout 会遍历网络的每一层，并设置消除神经网络中节点的概率。假设网络中的每一层，每个节点都以抛硬币的方式设置概率，每个节点得以保留和消除的概率都是 0.5，设置完节点概率，我们会消除一些节点，然后删除掉从该节点进出的连线，最后得到一个节点更少，规模更小的网络，然后用 backprop 方法进行训练。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-98693551ceb866edd4ac30d7ee11190b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"430\" data-rawheight=\"284\" class=\"origin_image zh-lightbox-thumb\" width=\"430\" data-original=\"https://pic4.zhimg.com/v2-98693551ceb866edd4ac30d7ee11190b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;430&#39; height=&#39;284&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"430\" data-rawheight=\"284\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"430\" data-original=\"https://pic4.zhimg.com/v2-98693551ceb866edd4ac30d7ee11190b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-98693551ceb866edd4ac30d7ee11190b_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-0b38cee31c8853b70d8f8bf1332d93a1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"444\" data-rawheight=\"234\" class=\"origin_image zh-lightbox-thumb\" width=\"444\" data-original=\"https://pic2.zhimg.com/v2-0b38cee31c8853b70d8f8bf1332d93a1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;444&#39; height=&#39;234&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"444\" data-rawheight=\"234\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"444\" data-original=\"https://pic2.zhimg.com/v2-0b38cee31c8853b70d8f8bf1332d93a1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-0b38cee31c8853b70d8f8bf1332d93a1_b.jpg\"/></figure><p>      这是网络节点精简后的一个样本，对于其它样本，我们照旧以抛硬币的方式设置概率，保留一类节点集合，删除其它类型的节点集合。对于每个训练样本，我们都将采用一个精简后神经网络来训练它，这种方法似乎有点怪，单纯遍历节点，编码也是随机的，可它真的有效。</p><p><b>简单来说，dropout正则化不仅可以精简网络，加速训练，还可以防止过拟合。</b>dropout如何实施？方法有很多，常用的有<b>inverted dropout</b>，具体内容参考视频：<a href=\"https://link.zhihu.com/?target=https%3A//mooc.study.163.com/learn/2001281003%3Ftid%3D2001391036\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">mooc.study.163.com/lear</span><span class=\"invisible\">n/2001281003?tid=2001391036</span><span class=\"ellipsis\"></span></a></p><p><b>2.3 数据扩增</b></p><p>      假设你正在拟合猫咪图片分类器，如果你想通过扩增训练数据来解决过拟合，但扩增数据代价高，而且有时候我们无法扩增数据，但我们可以通过添加这类图片来增加训练集。例如，水平翻转图片，并把它添加到训练集。所以现在训练集中有原图，还有翻转后的这张图片，所以通过水平翻转图片，训练集则可以增大一倍，因为训练集有冗余，这虽然不如我们额外收集一组新图片那么好，但这样做节省了获取更多猫咪图片的花费。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-380fc7d70a6d59db6008316fa6c0f879_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"868\" data-rawheight=\"344\" class=\"origin_image zh-lightbox-thumb\" width=\"868\" data-original=\"https://pic2.zhimg.com/v2-380fc7d70a6d59db6008316fa6c0f879_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;868&#39; height=&#39;344&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"868\" data-rawheight=\"344\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"868\" data-original=\"https://pic2.zhimg.com/v2-380fc7d70a6d59db6008316fa6c0f879_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-380fc7d70a6d59db6008316fa6c0f879_b.jpg\"/></figure><p>      除了水平翻转图片，你也可以随意裁剪图片，这张图是把原图旋转并随意放大后裁剪的，仍能辨别出图片中的猫咪。通过随意翻转和裁剪图片，我们可以增大数据集，额外生成假训练数据。和全新的，独立的猫咪图片数据相比，这些额外的假的数据无法包含像全新数据那么多的信息，但我们这么做基本没有花费，代价几乎为零，除了一些对抗性代价。以这种方式扩增算法数据，进而正则化数据集，减少过拟合比较廉价。</p><p>      像这样人工合成数据的话，我们要通过算法验证，图片中的猫经过水平翻转之后依然是猫。大家注意，我并没有垂直翻转，因为我们不想上下颠倒图片，也可以随机选取放大后的部分图片，猫可能还在上面。<br/>      对于光学字符识别，我们还可以通过添加数字，随意旋转或扭曲数字来扩增数据，把这些数字添加到训练集，它们仍然是数字。为了方便说明，我对字符做了强变形处理，所以数字 4 看起来是波形的，其实不用对数字 4 做这么夸张的扭曲，只要轻微的变形就好，我做成这样是为了让大家看的更清楚。实际操作的时候，我们通常对字符做更轻微的变形处理。因为这几个 4 看起来有点扭曲。<b>所以，数据扩增可作为正则化方法使用，实际功能上也与正则化相似。</b></p><p><b>2.4 early stopping</b></p><p>      early stopping顾名思义，就是及早停止的意思，意味着在训练神经网络模型中及早的停止训练。那么在哪里停止？为什么要停止？</p><p>      因为在训练过程中，我们希望训练误差，损失函数越来越小，我们可以绘制训练误差或损失函数J和迭代次数的关系曲线，同时，我们也可以用验证集来绘制同样的曲线，通常验证集的误差却会经历先下降后上升的过程，于是这个最低点就是我们需要停止训练的点。</p><p>为什么 ？因为我们的目标是防止模型过拟合，为了在一个合适的地方停止模型，防止过拟合，我们用 <img src=\"https://www.zhihu.com/equation?tex=%7C%7Cw%7C%7C%5E2_F\" alt=\"||w||^2_F\" eeimg=\"1\"/> ，即弗罗贝尼乌斯范数来表示，w即参数矩阵，在模型训练开始时由于是随机初始化，故 <img src=\"https://www.zhihu.com/equation?tex=%7C%7Cw%7C%7C%5E2_F\" alt=\"||w||^2_F\" eeimg=\"1\"/> 最小，随着模型训练， <img src=\"https://www.zhihu.com/equation?tex=%7C%7Cw%7C%7C%5E2_F\" alt=\"||w||^2_F\" eeimg=\"1\"/> 越大，我们需要在取一个中间值，即在下图紫色线段上的最低点，此时我们停止模型训练，这个点的 <img src=\"https://www.zhihu.com/equation?tex=%7C%7Cw%7C%7C%5E2_F\" alt=\"||w||^2_F\" eeimg=\"1\"/> 处于中间值，即不过大也不过小，模型过拟合程度较低。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-23da02ba47056775d0924f2862c19dde_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"801\" data-rawheight=\"387\" class=\"origin_image zh-lightbox-thumb\" width=\"801\" data-original=\"https://pic3.zhimg.com/v2-23da02ba47056775d0924f2862c19dde_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;801&#39; height=&#39;387&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"801\" data-rawheight=\"387\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"801\" data-original=\"https://pic3.zhimg.com/v2-23da02ba47056775d0924f2862c19dde_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-23da02ba47056775d0924f2862c19dde_b.jpg\"/></figure><p><b>优点：</b></p><p>Early stopping 的优点是，只运行一次梯度下降，你可以找出w的较小值中间值和较大值，而无需尝试L2正则化超级参数λ的很多值。节约了模型训练的时间。</p><p><b>缺点：</b></p><p>及早停止了训练，可能过拟合程度降低了，但是模型的损失J并没有降低到最小，即模型并未达到全局最优。</p><p></p><p></p>", 
            "topic": [
                {
                    "tag": "超参数", 
                    "tagLink": "https://api.zhihu.com/topics/20687672"
                }, 
                {
                    "tag": "正则化", 
                    "tagLink": "https://api.zhihu.com/topics/20682937"
                }, 
                {
                    "tag": "神经网络", 
                    "tagLink": "https://api.zhihu.com/topics/19607065"
                }
            ], 
            "comments": [
                {
                    "userName": "知乎用户", 
                    "userLink": "https://www.zhihu.com/people/0", 
                    "content": "<p>L2怎么实现正则化？有了BN之后L2并不能约束范数，就算没有BN，激活函数用relu的时候L2也不能约束范数，这个问题一直没想明白。</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "Lyon", 
                            "userLink": "https://www.zhihu.com/people/38495a9b1f83e4a612543ce6c523df7f", 
                            "content": "<p>你好，BN和正则化的目的不太一样，BN主要用于规范每一层输入的分布，防止梯度消失加速收敛；正则化则是对参数起作用，降低过拟合程度(不过BN也能起到部分正则化的作用)，具体可以看下：<br><br><a href=\"https://zhuanlan.zhihu.com/p/34879333\" class=\"internal\"><span class=\"invisible\">https://</span><span class=\"visible\">zhuanlan.zhihu.com/p/34</span><span class=\"invisible\">879333</span><span class=\"ellipsis\"></span></a><br><br><a href=\"https://www.zhihu.com/question/288370837\" class=\"internal\"><span class=\"invisible\">https://www.</span><span class=\"visible\">zhihu.com/question/2883</span><span class=\"invisible\">70837</span><span class=\"ellipsis\"></span></a><br><br><br><br>L2正则化就是普通正则化，具体怎么起作用，推导过程之前的文章里有写的：<br><br><a href=\"https://zhuanlan.zhihu.com/p/73404297\" class=\"internal\"><span class=\"invisible\">https://</span><span class=\"visible\">zhuanlan.zhihu.com/p/73</span><span class=\"invisible\">404297</span><span class=\"ellipsis\"></span></a></p>", 
                            "likes": 0, 
                            "replyToAuthor": "知乎用户"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/76048966", 
            "userName": "Lyon", 
            "userLink": "https://www.zhihu.com/people/38495a9b1f83e4a612543ce6c523df7f", 
            "upvote": 10, 
            "title": "【吴恩达深度学习】—神经网络中的激活函数", 
            "content": "<h2>前言：</h2><p>      继吴恩达机器学习公开课之后，吴恩达大佬又和网易合作，免费开放了深度学习的课程，感谢大佬，感谢网易，同样感谢提供配套课程讲义的黄海广博士。</p><p>      本文主要是个人的学习总结，原内容源于视频第一门课：神经网络和深度学习—第三周，文章内容同步发布在<a href=\"https://link.zhihu.com/?target=https%3A//www.yuque.com/docs/share/8e441063-5bc2-46c8-b430-1d7a8851ea3e\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">语雀文档</a>上。</p><p><b>课程资源</b>： <a href=\"https://link.zhihu.com/?target=https%3A//mooc.study.163.com/smartSpec/detail/1001319001.htm\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">网易云课堂课程资源</a> <a href=\"https://link.zhihu.com/?target=https%3A//github.com/fengdu78/deeplearning_ai_books\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">课程配套讲义</a></p><hr/><p><b>吴恩达深度学习1—神经网络中的激活函数</b></p><p><b>1.激活函数</b></p><p>      使用一个神经网络时，需要决定使用哪种<b>激活函数（Activation functions）</b>用隐藏层上，哪种用在输出节点上。到目前为止，之前只用过 sigmoid 激活函数，但是，有时其他的激活函数效果会更好。</p><p>常用的四种激活函数：</p><ul><li><b>sigmoid     </b></li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-cfb704f79e391be09e70ea1c292aab51_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"364\" data-rawheight=\"102\" class=\"content_image\" width=\"364\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;364&#39; height=&#39;102&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"364\" data-rawheight=\"102\" class=\"content_image lazy\" width=\"364\" data-actualsrc=\"https://pic2.zhimg.com/v2-cfb704f79e391be09e70ea1c292aab51_b.jpg\"/></figure><ul><li><b>tanh           </b></li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-2abe5307eb838c565264644a55736cc6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"452\" data-rawheight=\"112\" class=\"origin_image zh-lightbox-thumb\" width=\"452\" data-original=\"https://pic3.zhimg.com/v2-2abe5307eb838c565264644a55736cc6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;452&#39; height=&#39;112&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"452\" data-rawheight=\"112\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"452\" data-original=\"https://pic3.zhimg.com/v2-2abe5307eb838c565264644a55736cc6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-2abe5307eb838c565264644a55736cc6_b.jpg\"/></figure><ul><li><b>ReLu           </b></li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-1000275e84a9684b507e9249d2c7f51f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"252\" data-rawheight=\"52\" class=\"content_image\" width=\"252\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;252&#39; height=&#39;52&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"252\" data-rawheight=\"52\" class=\"content_image lazy\" width=\"252\" data-actualsrc=\"https://pic4.zhimg.com/v2-1000275e84a9684b507e9249d2c7f51f_b.jpg\"/></figure><ul><li><b>Leaky ReLu </b></li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-2c32e2d07a7175ab098de111851ff1a3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"326\" data-rawheight=\"52\" class=\"content_image\" width=\"326\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;326&#39; height=&#39;52&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"326\" data-rawheight=\"52\" class=\"content_image lazy\" width=\"326\" data-actualsrc=\"https://pic4.zhimg.com/v2-2c32e2d07a7175ab098de111851ff1a3_b.jpg\"/></figure><p>图像如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-130cecb98670321609a8b3b90628b78e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1267\" data-rawheight=\"660\" class=\"origin_image zh-lightbox-thumb\" width=\"1267\" data-original=\"https://pic3.zhimg.com/v2-130cecb98670321609a8b3b90628b78e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1267&#39; height=&#39;660&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1267\" data-rawheight=\"660\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1267\" data-original=\"https://pic3.zhimg.com/v2-130cecb98670321609a8b3b90628b78e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-130cecb98670321609a8b3b90628b78e_b.jpg\"/></figure><p><b>1.1 sigmoid函数</b></p><p>公式：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-cfb704f79e391be09e70ea1c292aab51_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"364\" data-rawheight=\"102\" class=\"content_image\" width=\"364\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;364&#39; height=&#39;102&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"364\" data-rawheight=\"102\" class=\"content_image lazy\" width=\"364\" data-actualsrc=\"https://pic2.zhimg.com/v2-cfb704f79e391be09e70ea1c292aab51_b.jpg\"/></figure><p>      sigmoid函数的值域：(0,1)，现在基本上使用sigmoid函数的场合很少，<b>大多数情况下使用tanh 函数替代， tanh 函数在所有场合都优于 sigmoid 函数。</b>除非在二分类情况下，期待输出的y值为0或1（而不是-1和1）</p><p><b>1.2 tanh函数</b></p><p>公式：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-2abe5307eb838c565264644a55736cc6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"452\" data-rawheight=\"112\" class=\"origin_image zh-lightbox-thumb\" width=\"452\" data-original=\"https://pic3.zhimg.com/v2-2abe5307eb838c565264644a55736cc6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;452&#39; height=&#39;112&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"452\" data-rawheight=\"112\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"452\" data-original=\"https://pic3.zhimg.com/v2-2abe5307eb838c565264644a55736cc6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-2abe5307eb838c565264644a55736cc6_b.jpg\"/></figure><p>      tanh 函数是 sigmoid 的向下平移和伸缩后的结果。对它进行了变形后，穿过了(0,0)点，并且值域介于+1 和-1 之间。结果表明，如果在隐藏层上使用tanh 函数，效果总是优于 sigmoid 函数。且在训练一个算法模型时，如果使用 tanh 函数代替sigmoid 函数中心化数据，使得数据的平均值更接近 0 而不是sigmoid函数的0.5</p><p><b>sigmoid和tanh函数的缺点：</b></p><p><b>在z特别大或者特别小的情况下，导数的梯度或者函数的斜率会变得特别小，最后就会接近于 0，导致降低梯度下降的速度。</b></p><p><b>1.3 ReLu函数</b></p><p>公式：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-1000275e84a9684b507e9249d2c7f51f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"252\" data-rawheight=\"52\" class=\"content_image\" width=\"252\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;252&#39; height=&#39;52&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"252\" data-rawheight=\"52\" class=\"content_image lazy\" width=\"252\" data-actualsrc=\"https://pic4.zhimg.com/v2-1000275e84a9684b507e9249d2c7f51f_b.jpg\"/></figure><p>      修正线性单元（Rectified linear unit，ReLU）是在机器学习中很流行的一个函数，用其替代sigmoid函数可大幅加速梯度下降的过程，降低训练时间。</p><p>如图，函数分为两个部分，左半部分z&lt;0时，函数值恒 = 0，导数 = 0；右半部分当z &gt;= 0 时，函数是斜率为1的线性函数，导数恒 = 1.</p><p><b>这有一些选择激活函数的经验法则：</b> </p><p><b>如果输出是 0、 1 值（二分类问题），则输出层选择 sigmoid /tanh函数，然后其它的所有单元都选择 Relu 函数。</b></p><p><b>1.4 Leaky ReLu函数</b></p><p>公式：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-2c32e2d07a7175ab098de111851ff1a3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"326\" data-rawheight=\"52\" class=\"content_image\" width=\"326\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;326&#39; height=&#39;52&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"326\" data-rawheight=\"52\" class=\"content_image lazy\" width=\"326\" data-actualsrc=\"https://pic4.zhimg.com/v2-2c32e2d07a7175ab098de111851ff1a3_b.jpg\"/></figure><p>      Leaky ReLu是Relu的改装版，当z是负值时，这个函数的值不是等于 0，而是轻微的倾斜,为什么常数是 0.01？其实是个经验值，当然，也可以选择不同的参数。 这个函数通常比 Relu 激活函数效果要好，尽管在实际中 Leaky ReLu 使用的并不多。</p><p>      Relu由于在z &lt; 0时，梯度直接为0，神经元此时不会训练，即大大加速了模型训练，节约时间。但同时会产生所谓的稀疏性，为了平衡稀疏性和模型训练效率，才诞生了 Leaky ReLu。</p><p><b>2.激活函数的导数</b></p><ul><li><b>sigmoid         </b><img src=\"https://www.zhihu.com/equation?tex=g%28z%29+%3D+%5Cfrac%7B1%7D%7B1%2Be%5E%7B-Z%7D%7D\" alt=\"g(z) = \\frac{1}{1+e^{-Z}}\" eeimg=\"1\"/> </li><li><b>tanh               </b><img src=\"https://www.zhihu.com/equation?tex=g%28z%29+%3D+tanh%28z%29+%3D+%5Cfrac%7Be%5E%7BZ%7D-e%5E%7B-Z%7D%7D%7Be%5E%7BZ%7D%2Be%5E%7B-Z%7D%7D\" alt=\"g(z) = tanh(z) = \\frac{e^{Z}-e^{-Z}}{e^{Z}+e^{-Z}}\" eeimg=\"1\"/> </li><li><b>ReLu               </b><img src=\"https://www.zhihu.com/equation?tex=g%28z%29+%3D+max%280%2Cz%29\" alt=\"g(z) = max(0,z)\" eeimg=\"1\"/> </li><li><b>Leaky ReLu    </b><img src=\"https://www.zhihu.com/equation?tex=g%28z%29+%3D+max%280.01z%2Cz%29\" alt=\"g(z) = max(0.01z,z)\" eeimg=\"1\"/> </li></ul><p><b>2.1 sigmoid</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-e5c0344ebdd23d7e4ff34568676ffa56_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"284\" data-rawheight=\"102\" class=\"content_image\" width=\"284\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;284&#39; height=&#39;102&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"284\" data-rawheight=\"102\" class=\"content_image lazy\" width=\"284\" data-actualsrc=\"https://pic3.zhimg.com/v2-e5c0344ebdd23d7e4ff34568676ffa56_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-1ee9b4713441fac4dc4b4599b033af07_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"904\" data-rawheight=\"106\" class=\"origin_image zh-lightbox-thumb\" width=\"904\" data-original=\"https://pic4.zhimg.com/v2-1ee9b4713441fac4dc4b4599b033af07_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;904&#39; height=&#39;106&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"904\" data-rawheight=\"106\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"904\" data-original=\"https://pic4.zhimg.com/v2-1ee9b4713441fac4dc4b4599b033af07_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-1ee9b4713441fac4dc4b4599b033af07_b.jpg\"/></figure><p>在神经网络中：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-1315eceebca9e9942599462f04447191_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"638\" data-rawheight=\"100\" class=\"origin_image zh-lightbox-thumb\" width=\"638\" data-original=\"https://pic2.zhimg.com/v2-1315eceebca9e9942599462f04447191_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;638&#39; height=&#39;100&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"638\" data-rawheight=\"100\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"638\" data-original=\"https://pic2.zhimg.com/v2-1315eceebca9e9942599462f04447191_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-1315eceebca9e9942599462f04447191_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>2.2 tanh</b></p><p class=\"ztext-empty-paragraph\"><br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-262c5b72e2d3bdb4d867750926ed8a2e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"502\" data-rawheight=\"112\" class=\"origin_image zh-lightbox-thumb\" width=\"502\" data-original=\"https://pic3.zhimg.com/v2-262c5b72e2d3bdb4d867750926ed8a2e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;502&#39; height=&#39;112&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"502\" data-rawheight=\"112\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"502\" data-original=\"https://pic3.zhimg.com/v2-262c5b72e2d3bdb4d867750926ed8a2e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-262c5b72e2d3bdb4d867750926ed8a2e_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-cb7f0a1f95e6180dc9d39110613d1ba3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"446\" data-rawheight=\"100\" class=\"origin_image zh-lightbox-thumb\" width=\"446\" data-original=\"https://pic4.zhimg.com/v2-cb7f0a1f95e6180dc9d39110613d1ba3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;446&#39; height=&#39;100&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"446\" data-rawheight=\"100\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"446\" data-original=\"https://pic4.zhimg.com/v2-cb7f0a1f95e6180dc9d39110613d1ba3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-cb7f0a1f95e6180dc9d39110613d1ba3_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p><b>2.3 ReLu</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-12615a3b7ede43fe61556c53e31c16e1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"302\" data-rawheight=\"52\" class=\"content_image\" width=\"302\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;302&#39; height=&#39;52&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"302\" data-rawheight=\"52\" class=\"content_image lazy\" width=\"302\" data-actualsrc=\"https://pic2.zhimg.com/v2-12615a3b7ede43fe61556c53e31c16e1_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-2d1d18ff821d4d0c2163bea7f28696d1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"598\" data-rawheight=\"154\" class=\"origin_image zh-lightbox-thumb\" width=\"598\" data-original=\"https://pic2.zhimg.com/v2-2d1d18ff821d4d0c2163bea7f28696d1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;598&#39; height=&#39;154&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"598\" data-rawheight=\"154\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"598\" data-original=\"https://pic2.zhimg.com/v2-2d1d18ff821d4d0c2163bea7f28696d1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-2d1d18ff821d4d0c2163bea7f28696d1_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>通常在z =  0的时候给定其导数0或1，不过z = 0的情况很少</p><p><b>2.4 Leaky ReLu</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-011500cb9aae463d70006bef91f28555_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"376\" data-rawheight=\"52\" class=\"content_image\" width=\"376\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;376&#39; height=&#39;52&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"376\" data-rawheight=\"52\" class=\"content_image lazy\" width=\"376\" data-actualsrc=\"https://pic2.zhimg.com/v2-011500cb9aae463d70006bef91f28555_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-2258731413189e961291156fafbd141a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"608\" data-rawheight=\"154\" class=\"origin_image zh-lightbox-thumb\" width=\"608\" data-original=\"https://pic3.zhimg.com/v2-2258731413189e961291156fafbd141a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;608&#39; height=&#39;154&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"608\" data-rawheight=\"154\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"608\" data-original=\"https://pic3.zhimg.com/v2-2258731413189e961291156fafbd141a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-2258731413189e961291156fafbd141a_b.jpg\"/></figure><p>通常在z =  0的时候给定其导数0.01或1，不过z = 0的情况很少</p><p></p>", 
            "topic": [
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }, 
                {
                    "tag": "激活函数", 
                    "tagLink": "https://api.zhihu.com/topics/20682949"
                }, 
                {
                    "tag": "神经网络", 
                    "tagLink": "https://api.zhihu.com/topics/19607065"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/75326539", 
            "userName": "Lyon", 
            "userLink": "https://www.zhihu.com/people/38495a9b1f83e4a612543ce6c523df7f", 
            "upvote": 4, 
            "title": "【吴恩达机器学习】第六周—机器学习系统设计", 
            "content": "<p><b>前言：</b></p><p>最近在学习深度学习，看了不少教程，发现还是吴恩达的比较适用于我。吴恩达机器学习公开课视频最早是斯坦福大学的课程视频(那个画面有点老)，新版的视频在网易云课堂上可以随时学习。仅仅通过视频学习，可能会有点快，因为有的知识点需要反复推敲和回味。感谢github上一位朋友的精心整理，让我们可以配合讲义一起学习，讲义有HTML版、PDF版、Markdown版的：视频配合讲义看，事半功倍</p><p>本文包括接下来的系列文章，都会是吴恩达机器学习课程的个人学习总结，有很多文字和图片直接参考了PDF版的课程讲义，再次谢过提供讲义的朋友！</p><p>网易云课堂课程视频：</p><p><a href=\"https://link.zhihu.com/?target=https%3A//study.163.com/course/courseMain.htm%3FcourseId%3D1004570029\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">study.163.com/course/co</span><span class=\"invisible\">urseMain.htm?courseId=1004570029</span><span class=\"ellipsis\"></span></a></p><p>课程同步配套讲义：</p><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/fengdu78/Coursera-ML-AndrewNg-Notes\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/fengdu78/Cou</span><span class=\"invisible\">rsera-ML-AndrewNg-Notes</span><span class=\"ellipsis\"></span></a></p><p>原文发布在语雀文档上,效果更好看点：</p><a href=\"https://link.zhihu.com/?target=https%3A//www.yuque.com/docs/share/076b0aa5-a551-4da3-ba94-c05fce0e5b05\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-72bbb1e26eacf07192c32d1b6514dde0_ipico.jpg\" data-image-width=\"512\" data-image-height=\"512\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">第六周 · 语雀</a><hr/><p><b>第六周</b></p><p><b>1. 应用机器学习的建议</b></p><p><b>1.1 下一步做什么</b></p><p>      仍然使用预测房价的学习例子，假如你已经完成了正则化线性回归，也就是最小化代价函数<b><i>J </i></b>的值，假如，在你得到你的学习参数以后，如果你要将你的假设函数放到一组新的房屋样本上进行测试，假如说你发现在预测房价时产生了巨大的误差，现在你的问题是要想改进这个算法，接下来应该怎么办？</p><p>      获得更多的训练实例——通常是有效的， 但代价较大， 下面的方法也可能有效， 可考虑先采用下面的几种方法。</p><ul><li><b>1.尝试减少特征的数量</b></li><li><b>2.尝试获得更多的特征</b></li><li><b>3.尝试增加多项式特征</b></li><li><b>4.尝试减少正则化程度λ</b></li><li><b>5.尝试增加正则化程度λ</b></li></ul><p>      总结一下，我们可以采取的手段主要在这些方面：数据集、特征、多项式特征、正则化项。我们不应该随机选择上面的某种方法来改进我们的算法，而是运用一些<b>机器学习诊断法（Machine learning dignostic）</b>来帮助我们知道上面哪些方法对我们的算法是有效的。</p><p><b>诊断法的意义：</b>你通过执行这种测试，能够深入了解某种算法到底是否有用。这通常也能够告诉你，要想改进一种算法的效果，什么样的尝试，才是有意义的。</p><p><b>1.2 评估假设函数(Evaluating a Hypothesis)</b></p><p>我们不能仅仅通过模型训练时的损失来判断一个假设函数是好是坏，因为存在过拟合的可能，如图所示：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-7cd0fb2494562d258dfc7fc4b365c45b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"749\" data-rawheight=\"359\" class=\"origin_image zh-lightbox-thumb\" width=\"749\" data-original=\"https://pic4.zhimg.com/v2-7cd0fb2494562d258dfc7fc4b365c45b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;749&#39; height=&#39;359&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"749\" data-rawheight=\"359\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"749\" data-original=\"https://pic4.zhimg.com/v2-7cd0fb2494562d258dfc7fc4b365c45b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-7cd0fb2494562d258dfc7fc4b365c45b_b.jpg\"/></figure><p>      过拟合时，模型对训练数据能完美预测和拟合，但是对于新给定的数据则效果很差，怎么办？对于这个简单的例子，我们可以对假设函数ℎ(x)画图，观察图形趋势，但更一般地做法：将数据分为训练集和测试集，用测试集来评估假设函数的拟合程度和模型的效果。</p><p>      为了检验算法是否过拟合，我们将数据分成训练集和测试集，通常用 70%的数据作为训练集，用剩下 30%的数据作为测试集。很重要的一点是训练集和测试集均要含有各种类型的数据， 通常我们要对数据进行“洗牌”， 然后再分成训练集和测试集。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-e82dfb653c300bf3e6f4e73682e8e60b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"802\" data-rawheight=\"458\" class=\"origin_image zh-lightbox-thumb\" width=\"802\" data-original=\"https://pic4.zhimg.com/v2-e82dfb653c300bf3e6f4e73682e8e60b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;802&#39; height=&#39;458&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"802\" data-rawheight=\"458\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"802\" data-original=\"https://pic4.zhimg.com/v2-e82dfb653c300bf3e6f4e73682e8e60b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-e82dfb653c300bf3e6f4e73682e8e60b_b.jpg\"/></figure><p>      测试集评估在通过训练集让我们的模型学习得出其参数后，对测试集运用该模型，我们有两种方式计算误差：</p><p>1.对于线性回归模型，我们利用测试集数据计算损失函数</p><p>2.对于逻辑回归模型，我们不仅可以利用测试集数据计算其损失函数，我们还可以计算其错误划分的损失，即误分类损失。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-bfbc731c54a860ab1fd21704145d8d6d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"758\" data-rawheight=\"225\" class=\"origin_image zh-lightbox-thumb\" width=\"758\" data-original=\"https://pic2.zhimg.com/v2-bfbc731c54a860ab1fd21704145d8d6d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;758&#39; height=&#39;225&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"758\" data-rawheight=\"225\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"758\" data-original=\"https://pic2.zhimg.com/v2-bfbc731c54a860ab1fd21704145d8d6d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-bfbc731c54a860ab1fd21704145d8d6d_b.jpg\"/></figure><p><b>1.3 模型选择和交叉验证</b></p><p><b>1.3.1 交叉验证集的意义</b></p><p>      假设我们要在 10 个不同次数的二项式模型之间进行选择：</p><ol><li><img src=\"https://www.zhihu.com/equation?tex=h_%5Ctheta%28x%29+%3D+%5Ctheta_0+%2B+%5Ctheta_1x\" alt=\"h_\\theta(x) = \\theta_0 + \\theta_1x\" eeimg=\"1\"/> </li></ol><p>2. <img src=\"https://www.zhihu.com/equation?tex=h_%5Ctheta%28x%29+%3D+%5Ctheta_0+%2B+%5Ctheta_1x+%2B+%5Ctheta_2x%5E2\" alt=\"h_\\theta(x) = \\theta_0 + \\theta_1x + \\theta_2x^2\" eeimg=\"1\"/> </p><p>3. <img src=\"https://www.zhihu.com/equation?tex=h_%5Ctheta%28x%29+%3D+%5Ctheta_0+%2B+%5Ctheta_1x+%2B+%5Ctheta_2x%5E2+%2B+%5Ctheta_3x%5E3\" alt=\"h_\\theta(x) = \\theta_0 + \\theta_1x + \\theta_2x^2 + \\theta_3x^3\" eeimg=\"1\"/> </p><p>...</p><p>10. <img src=\"https://www.zhihu.com/equation?tex=h_%5Ctheta%28x%29+%3D+%5Ctheta_0+%2B+%5Ctheta_1x+%2B+...%2B+%5Ctheta_%7B10%7Dx%5E%7B10%7D\" alt=\"h_\\theta(x) = \\theta_0 + \\theta_1x + ...+ \\theta_{10}x^{10}\" eeimg=\"1\"/> </p><p>      显然越高次数的多项式模型越能够适应我们的训练数据集，但是适应训练数据集并不代表着能推广至一般情况。具体来说，我们先用训练集训练得到这10个模型(假设函数)的参数矩阵 <img src=\"https://www.zhihu.com/equation?tex=%5CTheta%5E%7B%281%29%7D%2C%5CTheta%5E%7B%282%29%7D...%5CTheta%5E%7B%2810%29%7D\" alt=\"\\Theta^{(1)},\\Theta^{(2)}...\\Theta^{(10)}\" eeimg=\"1\"/> ,然后用测试集来评估和测试这些参数矩阵的对测试集样本的损失函数。</p><p>      但，问题来了，即使我们用测试集取到了损失函数J(θ)最低的一个模型,譬如第10个，那么我们也不敢肯定，这个模型在新的数据上的泛化能力。因为，测试集在计算其损失函数J(θ)时，已经拟合了一个比较隐蔽的东西—即多项式的最高次幂d(dgree of  polynomial)。式子1拟合了d = 1...式子10拟合了d = 10</p><p>      怎么解决这个问题 ？ 我们需要引入交叉验证集。譬如：使用 60%的数据作为训练集，使用 20%的数据作为交叉验证集，使用 20%的数据作为测试集。引入了交叉验证集后，这三组数据可以分别计算出训练损失、交叉验证损失、测试损失：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-5cd443ff2828173d7d3a3b9001b79da7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"793\" data-rawheight=\"492\" class=\"origin_image zh-lightbox-thumb\" width=\"793\" data-original=\"https://pic4.zhimg.com/v2-5cd443ff2828173d7d3a3b9001b79da7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;793&#39; height=&#39;492&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"793\" data-rawheight=\"492\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"793\" data-original=\"https://pic4.zhimg.com/v2-5cd443ff2828173d7d3a3b9001b79da7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-5cd443ff2828173d7d3a3b9001b79da7_b.jpg\"/></figure><p>      引入了交叉验证集后，我们的流程也发生了一点改变：</p><ul><li><b>1.训练集训练模型得到参数矩阵 <img src=\"https://www.zhihu.com/equation?tex=%5CTheta%5E%7B%281%29%7D%2C%5CTheta%5E%7B%282%29%7D...%5CTheta%5E%7B%2810%29%7D\" alt=\"\\Theta^{(1)},\\Theta^{(2)}...\\Theta^{(10)}\" eeimg=\"1\"/></b> </li><li><b>      2.用交叉验证集计算交叉验证集的损失J(θ) </b></li><li><b>      3.挑选最小的模型来用测试集测试，评估其泛化能力，计算验证集的损失J(θ) </b></li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-8d636692b1affa954000fbd91f9c97a9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"801\" data-rawheight=\"392\" class=\"origin_image zh-lightbox-thumb\" width=\"801\" data-original=\"https://pic2.zhimg.com/v2-8d636692b1affa954000fbd91f9c97a9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;801&#39; height=&#39;392&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"801\" data-rawheight=\"392\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"801\" data-original=\"https://pic2.zhimg.com/v2-8d636692b1affa954000fbd91f9c97a9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-8d636692b1affa954000fbd91f9c97a9_b.jpg\"/></figure><p>      如上图，采用新方式后，我们用交叉验证集合测试得到损失最小的模型是式子4. 故我们决定采用4.来对其进行测试，用测试集的数据来计算其损失 <img src=\"https://www.zhihu.com/equation?tex=J_%7Btest%7D%28%5Ctheta%5E%7B%284%29%7D%29\" alt=\"J_{test}(\\theta^{(4)})\" eeimg=\"1\"/> </p><p><b>1.4 诊断和偏差、方差</b></p><p>      当你运行一个学习算法时，如果这个算法的表现不理想，那么多半是出现两种情况：要么是偏差比较大，要么是方差比较大。换句话说，出现的情况要么是欠拟合，要么是过拟合问题。那么这两种情况，哪个和偏差有关，哪个和方差有关，或者是不是和两个都有关？搞清楚这一点非常重要，因为能判断出现的情况是这两种情况中的哪一种。</p><p><b>下面看一个例子：</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-663ef816977f9e84235a1c5bcf77bf34_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1008\" data-rawheight=\"514\" class=\"origin_image zh-lightbox-thumb\" width=\"1008\" data-original=\"https://pic1.zhimg.com/v2-663ef816977f9e84235a1c5bcf77bf34_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1008&#39; height=&#39;514&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1008\" data-rawheight=\"514\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1008\" data-original=\"https://pic1.zhimg.com/v2-663ef816977f9e84235a1c5bcf77bf34_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-663ef816977f9e84235a1c5bcf77bf34_b.jpg\"/></figure><p>      图中左边的模型对数据拟合不够，导致高偏差；右边的对数据拟合过度，虽然偏差较低但对新数据的预测能力查，泛化能力不够，导致高方差。中间的模型是比较适中的。</p><p>       P.S.这里偏差的概念比较好懂，误差即指数据集中数据和模型计算出的估计值之间的差值，方差即描述样本和平均值之间的偏离程度，所以看图理解也很直观。</p><p><b>损失和多项式维度d之间的关系</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ee4212923db8505f646d13623c755bd4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1010\" data-rawheight=\"576\" class=\"origin_image zh-lightbox-thumb\" width=\"1010\" data-original=\"https://pic1.zhimg.com/v2-ee4212923db8505f646d13623c755bd4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1010&#39; height=&#39;576&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1010\" data-rawheight=\"576\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1010\" data-original=\"https://pic1.zhimg.com/v2-ee4212923db8505f646d13623c755bd4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-ee4212923db8505f646d13623c755bd4_b.jpg\"/></figure><p>      上图展示了模型训练集、交叉验证集/测试集跑出的损失和多项式维度d之间的关系，</p><p>粉线表明：在训练集中d越高，损失越小，拟合程度越高(但不代表模型足够好，反而其泛化能力差)；</p><p>红线表明：在测试集/交叉验证集跑出来的模型上，损失有个先下降后上升的过程，在d = 2处取到最优。最开始其损失高，是因为模型欠拟合，高偏差，最后其损失高是因为模型过拟合，高方差。</p><p><b>1.4 正则化和偏差、方差</b></p><p>      在我们在训练模型的过程中，一般会使用一些正则化方法来防止过拟合。但是我们可能会正则化的程度太高或太小了， 即我们在选择 λ 的值时也需要思考与刚才选择多项式模型次数类似的问题。</p><p><b>关于正则化的概念，请参考第三周的内容</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-9edb6108cc330a86a8f61dd235894e26_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"965\" data-rawheight=\"487\" class=\"origin_image zh-lightbox-thumb\" width=\"965\" data-original=\"https://pic3.zhimg.com/v2-9edb6108cc330a86a8f61dd235894e26_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;965&#39; height=&#39;487&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"965\" data-rawheight=\"487\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"965\" data-original=\"https://pic3.zhimg.com/v2-9edb6108cc330a86a8f61dd235894e26_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-9edb6108cc330a86a8f61dd235894e26_b.jpg\"/></figure><p>      我们选择一系列的想要测试的λ值，通常是 0-10 之间的呈现 2 倍关系的值（如：0,0.01,0.02,0.04,0.08,0.15,0.32,0.64,1.28,2.56,5.12,10共 12 个）。我们同样把数据分为训练集、交叉验证集和测试集。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-0e5d324c5d93a88b4ca5f77bb9503740_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"990\" data-rawheight=\"551\" class=\"origin_image zh-lightbox-thumb\" width=\"990\" data-original=\"https://pic1.zhimg.com/v2-0e5d324c5d93a88b4ca5f77bb9503740_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;990&#39; height=&#39;551&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"990\" data-rawheight=\"551\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"990\" data-original=\"https://pic1.zhimg.com/v2-0e5d324c5d93a88b4ca5f77bb9503740_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-0e5d324c5d93a88b4ca5f77bb9503740_b.jpg\"/></figure><p>      然后，针对这12组模型，我们得到他们的参数矩阵θ，通过交叉验证集来计算交叉验证损失，取最小的模型5来做最后的测试，测试集的损失即为： <img src=\"https://www.zhihu.com/equation?tex=J_%7Btest%7D%28%5Ctheta%5E%7B%285%29%7D%29\" alt=\"J_{test}(\\theta^{(5)})\" eeimg=\"1\"/> </p><p>      对于同一个模型，正则化参数λ和损失的关系图如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-27575a6b3663b244c47114ed8c8dc5de_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1067\" data-rawheight=\"560\" class=\"origin_image zh-lightbox-thumb\" width=\"1067\" data-original=\"https://pic3.zhimg.com/v2-27575a6b3663b244c47114ed8c8dc5de_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1067&#39; height=&#39;560&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1067\" data-rawheight=\"560\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1067\" data-original=\"https://pic3.zhimg.com/v2-27575a6b3663b244c47114ed8c8dc5de_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-27575a6b3663b244c47114ed8c8dc5de_b.jpg\"/></figure><p>      可见，对于训练集，λ = 0时的损失是最小的，随着λ增大，整个损失越来越大；对于交叉验证集/测试集，损失随着λ 增加，先减小后增加</p><p><b>1.5 学习曲线</b></p><p><b>学习曲线Learning Curves</b>是将训练集误差和交叉验证集误差作为训练集实例数量(m)的函数绘制的图表。可以用其判断模型是否处于高方差、高偏差状态，从而改进模型。</p><p><b>1.5.1 正常学习曲线</b></p><p>      这里假设m = 100，则学习曲线是从m = 1时的模型损失坐标开始绘制到m = 100时的模型损失曲线。我们看一下训练集和交叉验证集的损失函数，此时暂时不考虑正则化项：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-db688279f28a64cbf7e03259a164c984_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"497\" data-rawheight=\"219\" class=\"origin_image zh-lightbox-thumb\" width=\"497\" data-original=\"https://pic1.zhimg.com/v2-db688279f28a64cbf7e03259a164c984_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;497&#39; height=&#39;219&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"497\" data-rawheight=\"219\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"497\" data-original=\"https://pic1.zhimg.com/v2-db688279f28a64cbf7e03259a164c984_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-db688279f28a64cbf7e03259a164c984_b.jpg\"/></figure><p>然后，假设我们的假设函数如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-2c1ed94420eb7a9983432bf5dc97289d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"440\" data-rawheight=\"58\" class=\"origin_image zh-lightbox-thumb\" width=\"440\" data-original=\"https://pic2.zhimg.com/v2-2c1ed94420eb7a9983432bf5dc97289d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;440&#39; height=&#39;58&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"440\" data-rawheight=\"58\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"440\" data-original=\"https://pic2.zhimg.com/v2-2c1ed94420eb7a9983432bf5dc97289d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-2c1ed94420eb7a9983432bf5dc97289d_b.jpg\"/></figure><p>下面我们开始绘制训练集的学习曲线：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-87b457c7174149839790d1236a782e76_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"353\" data-rawheight=\"376\" class=\"content_image\" width=\"353\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;353&#39; height=&#39;376&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"353\" data-rawheight=\"376\" class=\"content_image lazy\" width=\"353\" data-actualsrc=\"https://pic3.zhimg.com/v2-87b457c7174149839790d1236a782e76_b.jpg\"/></figure><p>      可以看出，m = 1和2时，方程能完美拟合数据，即损失为0，m = 3以后随着数据量增大，出现不拟合数据的情况增大，即平均损失增加。（这里需要说明，即使假设函数不是二次项的，而是一次项的，对于m = 1，m = 2时一样可以做到完美拟合0误差）汇总起来，学习曲线如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-95d348a74d19ee3f8687b84564c74de4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"547\" data-rawheight=\"301\" class=\"origin_image zh-lightbox-thumb\" width=\"547\" data-original=\"https://pic1.zhimg.com/v2-95d348a74d19ee3f8687b84564c74de4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;547&#39; height=&#39;301&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"547\" data-rawheight=\"301\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"547\" data-original=\"https://pic1.zhimg.com/v2-95d348a74d19ee3f8687b84564c74de4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-95d348a74d19ee3f8687b84564c74de4_b.jpg\"/></figure><p>蓝色部分是训练集的学习曲线，粉色部分是交叉验证集的学习曲线。m很小时，往往假设函数能较好地模拟，</p><p>使得损失很低，随着m增加，损失会逐渐上升直到一定范围。交叉验证集m很小时，模型往往泛化能力较差，故对于新的数据，不能很好拟合，损失较大，随着m上升，模型趋于稳定，此时交叉验证集的数据往往也能较好预测了，损失降低，最终趋于收敛(最终往往大于训练集的损失)</p><p><b>1.5.2 高偏差学习曲线</b></p><p>上面的例子，是正常情况下的学习曲线，那么如果假设函数不当，高方差或高偏差下的拟合曲线是怎样的呢？</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-574d55cae7220ecb8702c9b38ec9cb82_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"913\" data-rawheight=\"548\" class=\"origin_image zh-lightbox-thumb\" width=\"913\" data-original=\"https://pic3.zhimg.com/v2-574d55cae7220ecb8702c9b38ec9cb82_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;913&#39; height=&#39;548&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"913\" data-rawheight=\"548\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"913\" data-original=\"https://pic3.zhimg.com/v2-574d55cae7220ecb8702c9b38ec9cb82_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-574d55cae7220ecb8702c9b38ec9cb82_b.jpg\"/></figure><p>      高偏差bias时，即模型欠拟合，对数据不能较好预测，故在一定范围内m越大误差越大，且最后收敛到一个较高的水平。</p><p><b>启发：</b>增大训练集m的尺寸对于降低模型在交叉验证集上的损失没有任何帮助。</p><p><b>1.5.3 高方差学习曲线</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-66f32785c87136ca407cdb2176cc4897_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"968\" data-rawheight=\"552\" class=\"origin_image zh-lightbox-thumb\" width=\"968\" data-original=\"https://pic4.zhimg.com/v2-66f32785c87136ca407cdb2176cc4897_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;968&#39; height=&#39;552&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"968\" data-rawheight=\"552\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"968\" data-original=\"https://pic4.zhimg.com/v2-66f32785c87136ca407cdb2176cc4897_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-66f32785c87136ca407cdb2176cc4897_b.jpg\"/></figure><p>      可以看到，交叉验证集和训练集的损失变化较为平缓，且中间有较大的距离，这意味着增加m对于模型在交叉验证集上降低损失有较好效果。</p><p><b>启发：</b>高方差时，即过拟合，增加m数量会减轻模型过拟合程度，从而使预测未知数据能力变好，反映在交叉验证集上的表现，即损失降低。</p><p><b>1.6 下一步做什么？</b></p><p>      我们已经介绍了怎样评价一个学习算法，我们讨论了模型选择问题，偏差和方差的问题。那么这些诊断法则怎样帮助我们判断，哪些方法可能有助于改进学习算法的效果，而哪些可能是徒劳的呢？<br/>      让我们再次回到最开始的例子，在那里寻找答案，这就是我们之前的例子。回顾 1.1中提出的六种可选的下一步，让我们来看一看我们在什么情况下应该怎样选择：</p><p><b>1. 获得更多的训练实例——解决高方差</b></p><p><b>2. 尝试减少特征的数量——解决高方差</b></p><p><b>3. 尝试获得更多的特征——解决高偏差</b></p><p><b>4. 尝试增加多项式特征——解决高偏差</b></p><p><b>5.尝试减少正则化程度 λ——解决高偏差</b></p><p><b>6.尝试增加正则化程度 λ——解决高方差</b></p><p><b>神经网络的方差和偏差：</b></p><p>使用较小的神经网络，类似于参数较少的情况，容易导致高偏差和欠拟合，但计算代价较小使用较大的神经网络，类似于参数较多的情况，容易导致高方差和过拟合，虽然计算代价比较大，但是可以通过正则化手段来调整而更加适应数据。通常选择较大的神经网络并采用正则化处理会比采用较小的神经网络效果要好。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-15af873b9e25b573bf15d9e7bce1b8ee_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"977\" data-rawheight=\"524\" class=\"origin_image zh-lightbox-thumb\" width=\"977\" data-original=\"https://pic3.zhimg.com/v2-15af873b9e25b573bf15d9e7bce1b8ee_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;977&#39; height=&#39;524&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"977\" data-rawheight=\"524\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"977\" data-original=\"https://pic3.zhimg.com/v2-15af873b9e25b573bf15d9e7bce1b8ee_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-15af873b9e25b573bf15d9e7bce1b8ee_b.jpg\"/></figure><p>      对于神经网络中的隐藏层的层数的选择，通常从一层开始逐渐增加层数，为了更好地作选择，可以把数据分为训练集、交叉验证集和测试集，针对不同隐藏层层数的神经网络训练神经网络， 然后选择交叉验证集代价最小的神经网络。</p><p><b>2. 机器学习系统的设计</b></p><p><b>2.1 首先做什么</b></p><p><b>系统设计是个需要实际动手和采坑累计经验的活，好的设计能节约时间，优化模型和算法。本节内容建议阅读视频，因为没有很多公式和理论，有大量的文字和需要理解的东西。</b></p><p><b>本周以一个垃圾邮件分类器算法为例进行讨论。</b></p><p>      为了解决这样一个问题，我们首先要做的决定是如何选择并表达特征向量x，我们可以选择一个由 100 个最常出现在垃圾邮件中的词所构成的列表，根据这些词是否有在邮件中出现，来获得我们的特征向量（出现为 1，不出现为 0），尺寸为 100×1。</p><p>为了构建这个分类器算法，我们可以做很多事，例如：</p><ul><li>1.收集更多的数据，让我们有更多的垃圾邮件和非垃圾邮件的样本</li><li>2. 基于邮件的路由信息开发一系列复杂的特征</li><li>3. 基于邮件的正文信息开发一系列复杂的特征，包括考虑截词的处理</li><li>4. 为探测刻意的拼写错误（把 watch 写成 w4tch）开发复杂的算法</li></ul><p>      在上面这些选项中，非常难决定应该在哪一项上花费时间和精力，作出明智的选择，比随着感觉走要更好。 当我们使用机器学习时， 总是可以“头脑风暴”一下， 想出一堆方法来试试。实际上，当你需要通过头脑风暴来想出不同方法来尝试去提高精度的时候，你可能已经超越了很多人了。</p><p><b>2.2 误差分析Error Analysis</b></p><p>      在本次课程中，我们将会讲到误差分析（Error Analysis）的概念。这会帮助你更系统地做出决定。如果你准备研究机器学习的东西，或者构造机器学习应用程序，最好的实践方法不是建立一个非常复杂的系统，拥有多么复杂的变量；而是构建一个简单的算法，这样你可以很快地实现它。<br/>      每当我研究机器学习的问题时，我最多只会花一天的时间，就是字面意义上的 24 小时，来试图很快的把结果搞出来，即便效果不好。坦白的说，就是根本没有用复杂的系统，但是只是很快的得到的结果。即便运行得不完美，但是也把它运行一遍，最后通过交叉验证来检验数据。一旦做完，你可以画出学习曲线，通过画出学习曲线，以及检验误差，来找出你的算法是否有高偏差和高方差的问题，或者别的问题。在这样分析之后，再来决定用更多的数据训练，或者加入更多的特征变量是否有用。这么做的原因是：这在你刚接触机器学习问题时是一个很好的方法，你并不能提前知道你是否需要复杂的特征变量，或者你是否需要更多的数据，还是别的什么。提前知道你应该做什么，是非常难的，因为你缺少证据，缺少学习曲线。因此，你很难知道你应该把时间花在什么地方来提高算法的表现。但是当你实践一个非常简单即便不完美的方法时，你可以通过画出学习曲线来做出进一步的选择。你可以用这种方式来避免一种电脑编程里的过早优化问题，这种理念是：我们必须用证据来领导我们的决策，怎样分配自己的时间来优化算法，而不是仅仅凭直觉，凭直觉得出的东西一般总是错误的。除了画出学习曲线之外，一件非常有用的事是误差分析，我的意思是说：当我们在构造垃圾邮件分类器时，我会看一看我的交叉验证数据集，然后亲自看一看哪些邮件被算法错误地分类。因此，通过这些被算法错误分类的垃圾邮件与非垃圾邮件，你可以发现某些系统性的规律：什么类型的邮件总是被错误分类。经常地这样做之后，这个过程能启发你构造新的特征变量，或者告诉你：现在这个系统的短处，然后启发你如何去提高它。</p><p><b>构建一个学习算法的推荐方法为：</b></p><p>1.<b> 从一个简单的能快速实现的算法开始，实现该算法并用交叉验证集数据测试这个算法</b></p><p>2. <b>绘制学习曲线，决定是增加更多数据，或者添加更多特征，还是其他选择</b></p><p>3. <b>进行误差分析：人工检查交叉验证集中我们算法中产生预测误差的实例，看看这些实例是否有某种系统化的趋势</b></p><p><b>以我们的垃圾邮件过滤器为例</b></p><p>      误差分析要做的既是检验交叉验证集中我们的算法产生错误预测的所有邮件，看：是否能将这些邮件按照类分组。例如医药品垃圾邮件，仿冒品垃圾邮件或者密码窃取邮件等。然后看分类器对哪一组邮件的预测误差最大，并着手优化。<br/>      思考怎样能改进分类器。例如，发现是否缺少某些特征，记下这些特征出现的次数。例如记录下错误拼写出现了多少次，异常的邮件路由情况出现了多少次等等，然后从出现次数最多的情况开始着手优化。误差分析并不总能帮助我们判断应该采取怎样的行动。有时我们需要尝试不同的模型，然后进行比较，在模型比较时，用数值来判断哪一个模型更好更有效，通常我们是看交叉验证集的误差。</p><p><b>在 我 们 的 垃 圾 邮 件 分 类 器 例 子 中 </b></p><p>      对 于 “我们是否应该将discount/discounts/discounted/discounting 处理成同一个词？ ”如果这样做可以改善我们算法，我们会采用一些截词软件。误差分析不能帮助我们做出这类判断，我们只能尝试采用和不采用截词软件这两种不同方案，然后根据数值检验的结果来判断哪一种更好。<br/>      因此，当你在构造学习算法的时候，你总是会去尝试很多新的想法，实现出很多版本的学习算法，如果每一次你实践新想法的时候，你都要手动地检测这些例子，去看看是表现差还是表现好，那么这很难让你做出决定。到底是否使用词干提取，是否区分大小写。但是通过一个量化的数值评估，你可以看看这个数字，误差是变大还是变小了。你可以通过它更快地实践你的新想法，它基本上非常直观地告诉你：你的想法是提高了算法表现，还是让它变得更坏，这会大大提高你实践算法时的速度。所以我强烈推荐在交叉验证集上来实施误差分析，而不是在测试集上。但是，还是有一些人会在测试集上来做误差分析。即使这从数学上讲是不合适的。所以我还是推荐你在交叉验证向量上来做误差分析。</p><p>      总结一下，当你在研究一个新的机器学习问题时，我总是推荐你实现一个较为简单快速、即便不是那么完美的算法。我几乎从未见过人们这样做。大家经常干的事情是：花费大量的时间在构造算法上，构造他们以为的简单的方法。因此，不要担心你的算法太简单，或者太不完美，而是尽可能快地实现你的算法。当你有了初始的实现之后，它会变成一个非常有力的工具，来帮助你决定下一步的做法。因为我们可以先看看算法造成的错误，通过误差分析，来看看他犯了什么错，然后来决定优化的方式。另一件事是：假设你有了一个快速而不完美的算法实现，又有一个数值的评估数据，这会帮助你尝试新的想法，快速地发现你尝试的这些想法是否能够提高算法的表现，从而你会更快地做出决定，在算法中放弃什么，吸收什么误差分析可以帮助我们系统化地选择该做什么。</p><p><b>2.3 不对称性分类的误差</b></p><p><b>2.3.1 偏斜类skewed classes</b></p><p>      通常情况下，我们在误差分析中，设定某个实数来评估学习算法，并衡量它的表现。但有时仅仅依靠数字并不能解决问题，尤其是遇到<b>偏斜类（skewed classes）</b>的情况看个例子：</p><p>还是以预测肿瘤良性/恶心为例，实际样本中，恶性肿瘤占比为0.5%，良性为99.5%，我们要用一个算法来预测肿瘤的良性/恶性情况。</p><p>此时有两个算法模型：A.神经网络模型 B.一个错误的模型</p><p>在B模型中，每次预测都判断为良性，则对于真实样本来看，其误差在0.5%;A模型经过正常训练的神经网络模型，误差在1%，那么我们就不能仅仅依靠数字来比较算法模型的准确性。因为明显地B是一个错误的模型。</p><p><b>2.3.2 查准率查全率</b></p><p>为了解决上面这个例子，我们需要引入<b>查准率Precision和查全率Recall</b></p><p>引入查准率查全率后，我们将算法的预测情况归为以下四类：</p><p><b>1. 正确肯定（True Positive,TP）：  预测为真，实际为真</b></p><p><b>2. 正确否定（True Negative,TN）：预测为假，实际为假</b></p><p><b>3. 错误肯定（False Positive,FP）：  预测为真，实际为假</b></p><p><b>4. 错误否定（False Negative,FN）：预测为假，实际为真</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-f9a74a6ff950b1b4798ed209dbfd7cf8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"428\" data-rawheight=\"96\" class=\"origin_image zh-lightbox-thumb\" width=\"428\" data-original=\"https://pic1.zhimg.com/v2-f9a74a6ff950b1b4798ed209dbfd7cf8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;428&#39; height=&#39;96&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"428\" data-rawheight=\"96\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"428\" data-original=\"https://pic1.zhimg.com/v2-f9a74a6ff950b1b4798ed209dbfd7cf8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-f9a74a6ff950b1b4798ed209dbfd7cf8_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-ec0d765a60f16434d078d38c2ce9e487_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"368\" data-rawheight=\"96\" class=\"content_image\" width=\"368\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;368&#39; height=&#39;96&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"368\" data-rawheight=\"96\" class=\"content_image lazy\" width=\"368\" data-actualsrc=\"https://pic4.zhimg.com/v2-ec0d765a60f16434d078d38c2ce9e487_b.jpg\"/></figure><p>查准率，即查出来的TP类是准确的和所有预测的类数量的比例（此处以TP，即预测为真的类计算）</p><p>查全率，即查出来的TP类占所有实际类的比例</p><p>      在本例中，查准率的含义：预测出的，且正确判断其为恶性肿瘤患者数量/预测总数；查全率的含义：查出的恶性肿瘤患者数量 / 实际恶性肿瘤患者数量。这样，我们即可通过查准率和查全率来评估模型的准确情况了，很明显的，我们希望查准率和查全率都是越大越好。当然极限值是查准率 = 查全率 = 1，即代表模型能做到0误差预测。</p><p><b>2.4 查准率VS查全率</b></p><p>      理想状态下，查准率和查全率当然都是越高越好，最好都 = 1，但是实际上不可能达到。这时候我们就需要考虑在实际的算法模型中，究竟对于查准率和查全率哪个更看重，分别的目标值是多少。</p><p>      譬如，一个手机生产商的产线判断屏幕缺陷的算法模型，我希望查全率尽可能高，<b>查全率Recall = 0.999，</b>即我希望在产线上经过总共有1000个缺陷屏幕，那么我只允许放掉1块，将其判断为OK，其余999块缺陷屏幕，则算法必须识别出来。而对于查准率，可能生产商的要求就没有那么严格，查准率可以设置为0.6...</p><p>在肿瘤预测的例子里，如果我们希望只在非常确信的情况下预测为真（肿瘤为恶性），即我们希望更高的查<br/>准率，我们可以使用比 0.5 更大的阀值，如 0.7， 0.9。这样做我们会减少错误预测病人为恶性肿瘤的情况，同时却会增加未能成功预测肿瘤为恶性的情况。如果我们希望提高查全率，尽可能地让所有有可能是恶性肿瘤的病人都得到进一步地检查、诊断，我们可以使用比 0.5 更小的阀值，如 0.3。</p><p><b>那么我们怎么衡量之间的关系？</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-f18cd0f8d01f86ea7c434fd2bcaa3d28_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"629\" data-rawheight=\"243\" class=\"origin_image zh-lightbox-thumb\" width=\"629\" data-original=\"https://pic1.zhimg.com/v2-f18cd0f8d01f86ea7c434fd2bcaa3d28_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;629&#39; height=&#39;243&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"629\" data-rawheight=\"243\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"629\" data-original=\"https://pic1.zhimg.com/v2-f18cd0f8d01f86ea7c434fd2bcaa3d28_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-f18cd0f8d01f86ea7c434fd2bcaa3d28_b.jpg\"/></figure><p>假设我有三个算法模型，对应的查准率查全率如图所示，则究竟哪个是我想要的？这里通常有两种方式：</p><p><b>1.求均值 </b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-125612cee2166379cf4a0fe6c6d8536a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"336\" data-rawheight=\"94\" class=\"content_image\" width=\"336\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;336&#39; height=&#39;94&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"336\" data-rawheight=\"94\" class=\"content_image lazy\" width=\"336\" data-actualsrc=\"https://pic3.zhimg.com/v2-125612cee2166379cf4a0fe6c6d8536a_b.jpg\"/></figure><p><b>2.求F1值 </b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-8e5022cb4fc0dbffbfb88f01f93060df_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"356\" data-rawheight=\"96\" class=\"content_image\" width=\"356\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;356&#39; height=&#39;96&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"356\" data-rawheight=\"96\" class=\"content_image lazy\" width=\"356\" data-actualsrc=\"https://pic4.zhimg.com/v2-8e5022cb4fc0dbffbfb88f01f93060df_b.jpg\"/></figure><p><b>      由于第一种方式通常比较粗糙，我们选用第二种方式，第二种方式中如果P或R某一项较大，则F值较小，P和R较为接近和平均，则F值较大。当然我们也可以根据业务需求自己拟定合适的P或R值，或者设定一下系数如P*0.2 + R*0.8</b></p><p><b>2.5 机器学习中的数据</b></p><p>      虽然算法很重要，但是大量数据面前，可能不同算法模型最后得到的模型都会达到较好的效果。</p><p>      很多很多年前，我认识的两位研究人员 Michele Banko 和 Eric Brill 进行了一项有趣的研究，他们尝试通过机器学习算法来区分常见的易混淆的单词，他们尝试了许多种不同的算法，并发现数据量非常大时，这些不同类型的算法效果都很好。</p><p>      比如， 在这样的句子中： 早餐我吃了__个鸡蛋(to,two,too)， 在这个例子中， “早餐我吃了 2 个鸡蛋”， 这是一个易混淆的单词的例子。 于是他们把诸如这样的机器学习问题， 当做一类监督学习问题，并尝试将其分类，什么样的词，在一个英文句子特定的位置，才是合适的。他们用了几种不同的学习算法，这些算法都是在他们 2001 年进行研究的时候，都已经被公认是比较领先的。因此他们使用了一个方差，用于逻辑回归上的一个方差，被称作&#34;感知器&#34;(perceptron)。他们也采取了一些过去常用，但是现在比较少用的算法，比如 Winnow算法，很类似于回归问题，在一些方面又有所不同，过去用得比较多，但现在用得不太多。还有一种基于内存的学习算法，现在也用得比较少了，但是我稍后会讨论一点，而且他们用了一个朴素算法。这些具体算法的细节不那么重要， 我们下面希望探讨，什么时候我们会希望获得更多数据，而非修改算法。他们所做的就是改变了训练数据集的大小，并尝试将这些<br/>学习算法用于不同大小的训练数据集中，这就是他们得到的结果：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-a79acc1ca9811b79e7244768e73d15a9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"432\" data-rawheight=\"343\" class=\"origin_image zh-lightbox-thumb\" width=\"432\" data-original=\"https://pic2.zhimg.com/v2-a79acc1ca9811b79e7244768e73d15a9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;432&#39; height=&#39;343&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"432\" data-rawheight=\"343\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"432\" data-original=\"https://pic2.zhimg.com/v2-a79acc1ca9811b79e7244768e73d15a9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-a79acc1ca9811b79e7244768e73d15a9_b.jpg\"/></figure><p>这些趋势非常明显， 首先大部分算法，都具有相似的性能，其次，随着训练数据集的增大，在横轴上代表以百万为单位的训练集大小，从 0.1 个百万到 1000 百万，也就是到了 10亿规模的训练集的样本，这些算法的性能也都对应地增强了。<br/>      事实上，如果你选择任意一个算法，可能是选择了一个&#34;劣等的&#34;算法，如果你给这个劣等算法更多的数据，那么从这些例子中看起来的话，它看上去很有可能会其他算法更好，甚至会比&#34;优等算法&#34;更好。由于这项原始的研究非常具有影响力，因此已经有一系列许多不同的研究显示了类似的结果。这些结果表明，许多不同的学习算法有时倾向于表现出非常相似的表现，这还取决于一些细节，但是真正能提高性能的，是你能够给一个算法大量的训练数据。像这样的结果，引起了一种在机器学习中的普遍共识： &#34;取得成功的人不是拥有最好算法的人，而是拥有最多数据的人&#34;。<br/> </p><p></p>", 
            "topic": [
                {
                    "tag": "吴恩达（Andrew Ng）", 
                    "tagLink": "https://api.zhihu.com/topics/20003150"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "人工智能", 
                    "tagLink": "https://api.zhihu.com/topics/19551275"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/75173557", 
            "userName": "Lyon", 
            "userLink": "https://www.zhihu.com/people/38495a9b1f83e4a612543ce6c523df7f", 
            "upvote": 13, 
            "title": "【目录导航】—吴恩达机器学习—笔记整理", 
            "content": "<p><b>前言:</b></p><p><b>      本文是对DeepAI专栏，吴恩达机器学习—笔记整理系列的目录索引。每周内容对应原课程视频一周的内容，课程内容总体来说层层递进，对于新手，大家最好按部就班地学习。</b></p><p><b>      对于老鸟，可以按需阅读。譬如第七周的SVM支持向量机和核函数如果用不到，完全可以不看。搞深度学习相关的，重点章节在第四周、第五周，介绍了神经元、前向传播和反向传播。</b></p><hr/><p><b>第一周</b></p><p><a href=\"https://zhuanlan.zhihu.com/p/73363177\" class=\"internal\">单变量线性回归和损失函数、梯度下降的概念</a> </p><p><b>第二周</b></p><p><a href=\"https://zhuanlan.zhihu.com/p/73403012\" class=\"internal\">多变量线性回归和特征缩放、学习率</a> </p><p><b>第三周</b></p><p><a href=\"https://zhuanlan.zhihu.com/p/73404297\" class=\"internal\">分类问题逻辑回归和过拟合、正则化</a> </p><p><b>第四周</b></p><p><a href=\"https://zhuanlan.zhihu.com/p/73665825\" class=\"internal\">神经元、神经网络和前向传播算法</a> </p><p><b>第五周</b></p><p><a href=\"https://zhuanlan.zhihu.com/p/74167352\" class=\"internal\">神经网络、反向传播算法和随机初始化</a> </p><p><b>第六周</b></p><p><a href=\"https://zhuanlan.zhihu.com/p/75326539\" class=\"internal\">应用机器学习的建议和系统设计</a></p><p><b>第七周</b></p><p><a href=\"https://zhuanlan.zhihu.com/p/74764135\" class=\"internal\">支持向量机SVM和核函数</a> </p><p><b>第八周</b></p><p><a href=\"https://zhuanlan.zhihu.com/p/74902766\" class=\"internal\">聚类K-Means算法、降维和主成分分析</a> </p><p><b>第九周</b></p><p><a href=\"https://zhuanlan.zhihu.com/p/75036754\" class=\"internal\">异常检测和高斯分布、推荐系统和协同过滤</a> </p><p><b>第十周</b></p><p><a href=\"https://zhuanlan.zhihu.com/p/75171589\" class=\"internal\">大规模机器学习和随机梯度下降算法</a></p>", 
            "topic": [
                {
                    "tag": "吴恩达（Andrew Ng）", 
                    "tagLink": "https://api.zhihu.com/topics/20003150"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": [
                {
                    "userName": "普罗米修C", 
                    "userLink": "https://www.zhihu.com/people/f35c9aeb698bb9fbc0e44e4f088034e2", 
                    "content": "<p>这个是目前在知乎看到的最好的笔记了，拿来复习一下真的很不错 </p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "Lyon", 
                            "userLink": "https://www.zhihu.com/people/38495a9b1f83e4a612543ce6c523df7f", 
                            "content": "承蒙夸奖[拜托]", 
                            "likes": 0, 
                            "replyToAuthor": "普罗米修C"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/75171589", 
            "userName": "Lyon", 
            "userLink": "https://www.zhihu.com/people/38495a9b1f83e4a612543ce6c523df7f", 
            "upvote": 3, 
            "title": "【吴恩达机器学习】第十周—大规模机器学习和随机梯度下降", 
            "content": "<p><b>前言：</b></p><p>最近在学习深度学习，看了不少教程，发现还是吴恩达的比较适用于我。吴恩达机器学习公开课视频最早是斯坦福大学的课程视频(那个画面有点老)，新版的视频在网易云课堂上可以随时学习。仅仅通过视频学习，可能会有点快，因为有的知识点需要反复推敲和回味。感谢github上一位朋友的精心整理，让我们可以配合讲义一起学习，讲义有HTML版、PDF版、Markdown版的：视频配合讲义看，事半功倍</p><p>本文包括接下来的系列文章，都会是吴恩达机器学习课程的个人学习总结，有很多文字和图片直接参考了PDF版的课程讲义，再次谢过提供讲义的朋友！</p><p>网易云课堂课程视频：</p><p><a href=\"https://link.zhihu.com/?target=https%3A//study.163.com/course/courseMain.htm%3FcourseId%3D1004570029\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">study.163.com/course/co</span><span class=\"invisible\">urseMain.htm?courseId=1004570029</span><span class=\"ellipsis\"></span></a></p><p>课程同步配套讲义：</p><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/fengdu78/Coursera-ML-AndrewNg-Notes\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/fengdu78/Cou</span><span class=\"invisible\">rsera-ML-AndrewNg-Notes</span><span class=\"ellipsis\"></span></a></p><p>原文发布在语雀文档上,效果更好看点：</p><a href=\"https://link.zhihu.com/?target=https%3A//www.yuque.com/docs/share/404fd05b-3069-4c4e-85cf-a7b2978d40f1\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-72bbb1e26eacf07192c32d1b6514dde0_ipico.jpg\" data-image-width=\"512\" data-image-height=\"512\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">第十周 · 语雀</a><hr/><p><b>第十周</b></p><p><b>1. 大规模机器学习</b></p><p><b>1.1 大型数据集</b></p><p>      现实世界中，往往数据集的规模很大，譬如人口普查数据、谷歌、阿里、亚马逊,....等这些互联网公司产生的海量数量。不论采用怎样的算法或优化，可能最后决定模型准确度的主要因素就是数据集的规模，于是，研究和优化大规模数据集的训练变成了很重要的内容。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-086dfc106b470ead676a6da9f3250bb0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"888\" data-rawheight=\"505\" class=\"origin_image zh-lightbox-thumb\" width=\"888\" data-original=\"https://pic1.zhimg.com/v2-086dfc106b470ead676a6da9f3250bb0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;888&#39; height=&#39;505&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"888\" data-rawheight=\"505\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"888\" data-original=\"https://pic1.zhimg.com/v2-086dfc106b470ead676a6da9f3250bb0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-086dfc106b470ead676a6da9f3250bb0_b.jpg\"/></figure><p>      针对大数据集，如果我们一上来就用传统的梯度下降算法，可能往往会训练很慢很慢，达不到预期要求。那么我们该投入多少数据量来训练模型？便成了一个很实际的问题。譬如，数据总量有1亿条，那么我是不是将数据集的尺寸设为m = 100000000，来投入模型训练 ？</p><p>这时，行之有效的方法是投入一个较小数量的样本经行检查式的预训练，并观察训练损失、验证损失和数据集数量m的关系曲线：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-a726bbb5c59446506ca3bf4e6b7e8be3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"974\" data-rawheight=\"532\" class=\"origin_image zh-lightbox-thumb\" width=\"974\" data-original=\"https://pic4.zhimg.com/v2-a726bbb5c59446506ca3bf4e6b7e8be3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;974&#39; height=&#39;532&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"974\" data-rawheight=\"532\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"974\" data-original=\"https://pic4.zhimg.com/v2-a726bbb5c59446506ca3bf4e6b7e8be3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-a726bbb5c59446506ca3bf4e6b7e8be3_b.jpg\"/></figure><p>      假设我们预训练数据集尺寸m = 1000, 训练和交叉验证数据集的损失和m的关系如上右图，则表示，即使我们增大数据集的尺寸m也不会对模型的训练产生好处，模型的损失不再会下降很多（即模型精度不会提高很多）；</p><p>假设关系图如上左，则表示我们还应该继续加大训练集的数量m。</p><p><b>1.2 随机梯度下降</b></p><p>      假设训练集有3亿数据，那么如果我们用传统的批量梯度下降算法，没迭代一个参数θ，我们就需要遍历这3亿条数据，这还没考虑计算机内存能否装下这些数据，然而这才仅仅是n个参数中的1个θ的迭代。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-034bb2c73cf629f74324fd85fa966126_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"918\" data-rawheight=\"481\" class=\"origin_image zh-lightbox-thumb\" width=\"918\" data-original=\"https://pic3.zhimg.com/v2-034bb2c73cf629f74324fd85fa966126_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;918&#39; height=&#39;481&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"918\" data-rawheight=\"481\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"918\" data-original=\"https://pic3.zhimg.com/v2-034bb2c73cf629f74324fd85fa966126_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-034bb2c73cf629f74324fd85fa966126_b.jpg\"/></figure><p>      那么什么是<b>随机梯度下降Stochastic Gradient Descent</b>呢？随机梯度下降SGD算法采用了一种方式，让我们再迭代参数θ时无需累加整个数据集的参数(3亿条)，而是直接对当前数据 <img src=\"https://www.zhihu.com/equation?tex=%28x%5E%7B%28i%29%7D%2C+y%5E%7B%28i%29%7D%29\" alt=\"(x^{(i)}, y^{(i)})\" eeimg=\"1\"/> 求其关于θ的偏导，来经行迭代，且为了保持随机性，在开始前需要对整个数据集进行随机处理，以保证 <img src=\"https://www.zhihu.com/equation?tex=%28x%5E%7B%28i%29%7D%2C+y%5E%7B%28i%29%7D%29\" alt=\"(x^{(i)}, y^{(i)})\" eeimg=\"1\"/> 的随机性。</p><p>具体如下图：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-24491c82ca2a694925c7abac4a99a36e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"939\" data-rawheight=\"490\" class=\"origin_image zh-lightbox-thumb\" width=\"939\" data-original=\"https://pic3.zhimg.com/v2-24491c82ca2a694925c7abac4a99a36e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;939&#39; height=&#39;490&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"939\" data-rawheight=\"490\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"939\" data-original=\"https://pic3.zhimg.com/v2-24491c82ca2a694925c7abac4a99a36e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-24491c82ca2a694925c7abac4a99a36e_b.jpg\"/></figure><p><b>SGD公式表示:</b></p><b><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-d8cbb41c67c6476f00502b6917628144_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"912\" data-rawheight=\"419\" class=\"origin_image zh-lightbox-thumb\" width=\"912\" data-original=\"https://pic1.zhimg.com/v2-d8cbb41c67c6476f00502b6917628144_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;912&#39; height=&#39;419&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"912\" data-rawheight=\"419\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"912\" data-original=\"https://pic1.zhimg.com/v2-d8cbb41c67c6476f00502b6917628144_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-d8cbb41c67c6476f00502b6917628144_b.jpg\"/></figure></b><p><b>利弊分析：</b></p><p>      随机梯度下降算法在每一次计算之后便更新参数θ，而不需要首先将所有的训练集求和，在梯度下降算法还没有完成一次迭代时，随机梯度下降算法便已经走出了很远。</p><p>      但是这样的算法存在的问题是， 不是每一步都是朝着”正确”的方向迈出的。 因此算法虽然会逐渐走向全局最小值的位置，但是可能无法站到那个最小值的那一点，而是在最小值点附近徘徊</p><p><b>1.3 小批量梯度下降</b></p><p><b>      小批量梯度下降Mini-Batch Gradient Descent </b>算法是介于传统梯度下降算法(批量梯度下降)和随机梯度下降算法之间的算法，每计算常数b次训练实例，便更新一次参数θ。</p><p>      传统梯度下降算法，每更新一次θ遍历整个数据集m个样本，SGD则遍历1个，小批量梯度下降遍历b个数据样本，b取值范围通常介于2~100之间。</p><p>      举个例子，数据集有数据1000个，我们可以采取每批10个数据进行梯度下降：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-5bcb51419a5fd76c4c3dc6044df1d449_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"603\" data-rawheight=\"429\" class=\"origin_image zh-lightbox-thumb\" width=\"603\" data-original=\"https://pic2.zhimg.com/v2-5bcb51419a5fd76c4c3dc6044df1d449_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;603&#39; height=&#39;429&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"603\" data-rawheight=\"429\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"603\" data-original=\"https://pic2.zhimg.com/v2-5bcb51419a5fd76c4c3dc6044df1d449_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-5bcb51419a5fd76c4c3dc6044df1d449_b.jpg\"/></figure><p>      这样做的好处在于，我们可以用向量化的方式来循环b个训练实例。如果我们用的线性代数函数库比较好，能够支持平行处理，那么算法的总体表现将不受影响（与随机梯度下降相同）。</p><p><b>1.4 随机梯度下降收敛</b></p><p>      本小节主要介绍在随机梯度下降模型运行过程中的图像和学习率α的选取</p><p><b>1.4.1平均损失-迭代次数</b></p><p>      在随机梯度下降中，我们在每一次更新θ前都先计算一下损失函数cost()，并绘制平均损失和迭代次数的图表，用图表我们可以评估模型学习情况，损失下降情况。</p><p>      譬如我们可以采用这种方式，每经过1000个数据样本，算一次平均损失（Σcost()/1000），将平均损失和当前迭代的轮数作为坐标绘图，我们绘制的图表可能如下图所示：从左至右分别命名为图1、图2、图3和图4</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-4689e6f65691222722b45720ac1a0a82_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"937\" data-rawheight=\"565\" class=\"origin_image zh-lightbox-thumb\" width=\"937\" data-original=\"https://pic3.zhimg.com/v2-4689e6f65691222722b45720ac1a0a82_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;937&#39; height=&#39;565&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"937\" data-rawheight=\"565\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"937\" data-original=\"https://pic3.zhimg.com/v2-4689e6f65691222722b45720ac1a0a82_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-4689e6f65691222722b45720ac1a0a82_b.jpg\"/></figure><p><b>图一:</b></p><p>      假设我们的数据集大小为1000，蓝色图像为SGD的损失曲线，可以看出学习效果不错，模型最终趋于收敛(即损失不再降低)，那此时如果我们改用更小的学习率α来训练模型，得出的可能是红色曲线，我们发现，红色曲线下降到了更低的损失点，表面模型更优(同时也表明，蓝色曲线只是收敛到了局部最优解而不是全局最优解)</p><p><b>图二：</b></p><p>      蓝色的曲线是正常SGD的损失曲线，此时我们不改变学习率，只是简单地将绘图间隔改为每5000个数据点一次，则我们可能会发现，绘制出的图(红色曲线)更为平滑。（因为拓宽了间隔，会导致cost变化幅度更服从整体，即趋于平滑）</p><p><b>图三：</b></p><p>      蓝色曲线是SGD曲线，不过从曲线来看，模型并没有收敛，此时我们可以同样增加间隔至5000，此时绘制出的曲线如红色，可以看出，虽然损失下降的比较慢，不过还是正常下降的，即模型是正常学习的。如果画出的曲线如粉色线，那么则表明模型训练出了问题。可能是数据集、学习率、特征等情况需要调整。</p><p><b>图四：</b></p><p>      蓝色曲线是SGD曲线，很明显是异常的，损失不降反升，我们可以尝试减小学习率α</p><p><b>1.4.2 学习率α选择</b></p><p>      我们也可以令学习率随着迭代次数的增加而减小，例如令：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-524f978c998eaa616d7d9dd9ad9d2a8f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"588\" data-rawheight=\"100\" class=\"origin_image zh-lightbox-thumb\" width=\"588\" data-original=\"https://pic4.zhimg.com/v2-524f978c998eaa616d7d9dd9ad9d2a8f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;588&#39; height=&#39;100&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"588\" data-rawheight=\"100\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"588\" data-original=\"https://pic4.zhimg.com/v2-524f978c998eaa616d7d9dd9ad9d2a8f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-524f978c998eaa616d7d9dd9ad9d2a8f_b.jpg\"/></figure><p>      我们可以用两个常量+迭代次数来构造学习率α的公式，使得随着迭代次数的增加，公式分母增加，学习率α逐渐减小。</p><p><b>为什么采用变化的学习率？且越来越低？</b></p><p>      理论上来说，学习率越小越好，学习率越小，则意味着每次迭代过程损失降低的较小，即向全局最优点前进的步伐越小，从而不容易导致陷入“局部最优解”的情况。但是不可能一开始就将α设置的很小，不然模型训练时间将大大增加，而且通常在模型训练的前期，损失降低的越快，此时利用较大的α可以迅速降低loss，加快训练速度。</p><p>于是乎我们便可以采用式子中的方式，逐渐降低α的方式来训练。</p><p><b>1.5 在线学习</b></p><p>      今天，许多大型网站或者许多大型网络公司，使用不同版本的在线学习机制算法，从大批的涌入又离开网站的用户身上进行学习。特别要提及的是，如果你有一个由连续的用户流引发的连续的数据流，进入你的网站，你能做的是使用一个在线学习机制，从数据流中学习用户的偏好，然后使用这些信息来优化一些关于网站的决策。</p><p>      假定你有一个提供运输服务的公司，用户们来向你询问把包裹从 A 地运到 B 地的服务，同时假定你有一个网站，让用户们可多次登陆，然后他们告诉你，他们想从哪里寄出包裹，以及包裹要寄到哪里去，也就是出发地与目的地，然后你的网站开出运输包裹的的服务价格。比如，我会收取$50 来运输你的包裹，我会收取$20 之类的，然后根据你开给用户的这个价格，用户有时会接受这个运输服务，那么这就是个正样本，有时他们会走掉，然后他们拒绝购买你的运输服务，所以，让我们假定我们想要一个学习算法来帮助我们，优化我们想给用户开出的价格。</p><p>      一个算法来从中学习的时候来模型化问题在线学习算法指的是对数据流而非离线的静态数据集的学习。许多在线网站都有持续不断的用户流，对于每一个用户，网站希望能在不将数据存储到数据库中便顺利地进行算法学习。<br/>      假使我们正在经营一家物流公司，每当一个用户询问从地点 A 至地点 B 的快递费用时，我们给用户一个报价，该用户可能选择接受(y=1)或不接受(y=0)</p><p>      现在，我们希望构建一个模型，来预测用户接受报价使用我们的物流服务的可能性。因此报价 是我们的一个特征，其他特征为距离，起始地点，目标地点以及特定的用户数据。模型的输出是p(y=1)。</p><p>在线学习的算法与随机梯度下降算法有些类似，我们对单一的实例进行学习，而非对一个提前定义的训练集进行循环</p><p>一旦对一个数据的学习完成了，我们便可以丢弃该数据，不需要再存储它了。这种方式的好处在于，我们的算法可以很好的适应用户的倾向性，算法可以针对用户的当前行为不断地更新模型以适应该用户。</p><p>每次交互事件并不只产生一个数据集，例如，我们一次给用户提供 3 个物流选项，用户选择 2 项，我们实际上可以获得 3 个新的训练实例，因而我们的算法可以一次从 3 个实例中学习并更新模型。</p><p>      这些问题中的任何一个都可以被归类到标准的，拥有一个固定的样本集的机器学习问题中。或许，你可以运行一个你自己的网站，尝试运行几天，然后保存一个数据集，一个固定的数据集，然后对其运行一个学习算法。但是这些是实际的问题，在这些问题里，你会看到大公司会获取如此多的数据，真的没有必要来保存一个固定的数据集，取而代之的是你可以使用一个在线学习算法来连续的学习，从这些用户不断产生的数据中来学习。这就是在线学习机制，然后就像我们所看到的，我们所使用的这个算法与随机梯度下降算法非常类似，唯一的区别的是，我们不会使用一个固定的数据集，我们会做的是获取一个用户样本，从那个样本中学习，然后丢弃那个样本并继续下去，而且如果你对某一种应用有一个连续的数据流，这样的算法可能会非常值得考虑。当然，在线学习的一个优点就是，如果你有一个变化的用户群，又或者你在尝试预测的事情，在缓慢变化，就像你的用户的品味在缓慢变化，这个在线学习算法，可以慢慢地调试你所学习到的假设，将其调节更新到最新的用户行为。</p><p><b>1.6 Map reduce和数据并行</b></p><p><b>Map reduce和数据并行(Data Parallelism)</b>是对于大规模机器学习问题而言是非常重要的概念。之前提到，如果我们用批量梯度下降算法来求解大规模数据集的最优解，我们需要对整个训练集进行循环，计算偏导数和代价，再求和，计算代价非常大。如果我们能够将我们的数据集分配给不多台计算机，让每一台计算机处理数据集的一个子集，然后我们将计所的结果汇总在求和。这样的方法叫做映射简化。</p><blockquote>MapReduce最早是由Google公司研究提出的一种面向大规模数据处理的并行计算模型和方法,搞大数据的同学一定很熟悉这个概念。MapReduce的推出给大数据并行处理带来了巨大的革命性影响，使其已经成为事实上的大数据处理的工业标准。</blockquote><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-2458d735dbf34aab1c5c0790adb88aa0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"790\" data-rawheight=\"467\" class=\"origin_image zh-lightbox-thumb\" width=\"790\" data-original=\"https://pic1.zhimg.com/v2-2458d735dbf34aab1c5c0790adb88aa0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;790&#39; height=&#39;467&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"790\" data-rawheight=\"467\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"790\" data-original=\"https://pic1.zhimg.com/v2-2458d735dbf34aab1c5c0790adb88aa0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-2458d735dbf34aab1c5c0790adb88aa0_b.jpg\"/></figure><p>      具体而言，如果任何学习算法能够表达为，对训练集的函数的求和，那么便能将这个任务分配给多台计算机（或者同一台计算机的不同 CPU 核心），以达到加速处理的目的。<br/>      例如，我们有 400 个训练实例，我们可以将批量梯度下降的求和任务分配给 4 台计算机进行处理：</p><p><b>Map-reduce</b></p><p><b>Batch gradient descent</b>: <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_j+%3A%3D%5Ctheta_j-+%5Calpha+%5Cfrac%7B1%7D%7B400%7D+%5CSigma_%7Bi%3D1%7D%5E%7B400%7D%28h_%5Ctheta%28x%5E%7B%28i%29%7D%29-y%5E%7B%28i%29%7D%29x_j%5E%7B%28i%29%7D\" alt=\"\\theta_j :=\\theta_j- \\alpha \\frac{1}{400} \\Sigma_{i=1}^{400}(h_\\theta(x^{(i)})-y^{(i)})x_j^{(i)}\" eeimg=\"1\"/> </p><p><b>Machine 1:</b> <img src=\"https://www.zhihu.com/equation?tex=Use%28x%5E%7B%281%29%7D%2Cy%5E%7B%281%29%7D%29%2C...%2C%28x%5E%7B%28100%29%7D%2Cy%5E%7B%28100%29%7D%29\" alt=\"Use(x^{(1)},y^{(1)}),...,(x^{(100)},y^{(100)})\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=temp_j%5E%7B%28i%29%7D+%3D+%5CSigma_%7Bi%3D1%7D%5E%7B100%7D%28h_%5Ctheta%28x%5E%7B%28i%29%7D%29-y%5E%7B%28i%29%7D%29+x_j%5E%7B%28i%29%7D\" alt=\"temp_j^{(i)} = \\Sigma_{i=1}^{100}(h_\\theta(x^{(i)})-y^{(i)}) x_j^{(i)}\" eeimg=\"1\"/> </p><p><b>Machine 2:</b> <img src=\"https://www.zhihu.com/equation?tex=Use%28x%5E%7B%28101%29%7D%2Cy%5E%7B%28101%29%7D%29%2C...%2C%28x%5E%7B%28200%29%7D%2Cy%5E%7B%28200%29%7D%29\" alt=\"Use(x^{(101)},y^{(101)}),...,(x^{(200)},y^{(200)})\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=temp_j%5E%7B%28i%29%7D+%3D+%5CSigma_%7Bi%3D101%7D%5E%7B200%7D%28h_%5Ctheta%28x%5E%7B%28i%29%7D%29-y%5E%7B%28i%29%7D%29+x_j%5E%7B%28i%29%7D\" alt=\"temp_j^{(i)} = \\Sigma_{i=101}^{200}(h_\\theta(x^{(i)})-y^{(i)}) x_j^{(i)}\" eeimg=\"1\"/> </p><p><b>Machine 3:</b> <img src=\"https://www.zhihu.com/equation?tex=Use%28x%5E%7B%28201%29%7D%2Cy%5E%7B%28201%29%7D%29%2C...%2C%28x%5E%7B%28300%29%7D%2Cy%5E%7B%28300%29%7D%29\" alt=\"Use(x^{(201)},y^{(201)}),...,(x^{(300)},y^{(300)})\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=temp_j%5E%7B%28i%29%7D+%3D+%5CSigma_%7Bi%3D201%7D%5E%7B300%7D%28h_%5Ctheta%28x%5E%7B%28i%29%7D%29-y%5E%7B%28i%29%7D%29+x_j%5E%7B%28i%29%7D\" alt=\"temp_j^{(i)} = \\Sigma_{i=201}^{300}(h_\\theta(x^{(i)})-y^{(i)}) x_j^{(i)}\" eeimg=\"1\"/> </p><p><b>Machine 4:</b> <img src=\"https://www.zhihu.com/equation?tex=Use%28x%5E%7B%28301%29%7D%2Cy%5E%7B%28301%29%7D%29%2C...%2C%28x%5E%7B%28400%29%7D%2Cy%5E%7B%28400%29%7D%29\" alt=\"Use(x^{(301)},y^{(301)}),...,(x^{(400)},y^{(400)})\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=temp_j%5E%7B%28i%29%7D+%3D+%5CSigma_%7Bi%3D301%7D%5E%7B400%7D%28h_%5Ctheta%28x%5E%7B%28i%29%7D%29-y%5E%7B%28i%29%7D%29+x_j%5E%7B%28i%29%7D\" alt=\"temp_j^{(i)} = \\Sigma_{i=301}^{400}(h_\\theta(x^{(i)})-y^{(i)}) x_j^{(i)}\" eeimg=\"1\"/> </p><p class=\"ztext-empty-paragraph\"><br/></p><p>将这四台机器运算的结果发送至另一台中心服务器，我们便可利用temp汇总得到梯度下降θ更新的表达式：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_j+%3A%3D%5Ctheta_j-+%5Calpha+%5Cfrac%7B1%7D%7B400%7D+%28temp%5E%7B%281%29%7D_j+%2B+temp%5E%7B%282%29%7D_j+%2B+temp%5E%7B%283%29%7D_j+%2B+temp%5E%7B%284%29%7D_j%29\" alt=\"\\theta_j :=\\theta_j- \\alpha \\frac{1}{400} (temp^{(1)}_j + temp^{(2)}_j + temp^{(3)}_j + temp^{(4)}_j)\" eeimg=\"1\"/> </p><p><b>2. 应用实例</b></p><p>略</p><p><b>3. 总结</b></p><p>      欢迎来到《机器学习》课的最后一段视频。我们已经一起学习很长一段时间了。在最后这段视频中，我想快速地回顾一下这门课的主要内容，然后简单说几句想说的话。作为这门课的结束时间，那么我们学到了些什么呢？在这门课中，我们花了大量的时间介绍了诸如线性回归、逻辑回归、神经网络、支持向量机等等一些监督学习算法，这类算法具有带标签的数据和样本，比如 <img src=\"https://www.zhihu.com/equation?tex=x%5E%7B%28i%29%7D%E3%80%81y%5E%7B%28i%29%7D\" alt=\"x^{(i)}、y^{(i)}\" eeimg=\"1\"/> <br/>      然后我们也花了很多时间介绍无监督学习。例如 K-均值聚类、用于降维的主成分分析，以及当你只有一系列无标签数据 <img src=\"https://www.zhihu.com/equation?tex=x%5E%7B%28i%29%7D\" alt=\"x^{(i)}\" eeimg=\"1\"/> 时的异常检测算法。</p><p>      当然，有时带标签的数据，也可以用于异常检测算法的评估。此外，我们也花时间讨论了一些特别的应用或者特别的话题，比如说推荐系统。以及大规模机器学习系统，包括并行系统和映射化简方法，还有其他一些特别的应用。比如，用于计算机视觉技术的滑动窗口分类算法。</p><p>      最后，我们还提到了很多关于构建机器学习系统的实用建议。这包括了怎样理解某个机器学习算法是否正常工作的原因，所以我们谈到了偏差和方差的问题，也谈到了解决方差问题的正则化，同时我们也讨论了怎样决定接下来怎么做的问题，也就是说当你在开发一个机器学习系统时，什么工作才是接下来应该优先考虑的问题。因此我们讨论了学习算法的评价法。介绍了评价矩阵，比如：查准率、召回率以及 F1 分数，还有评价学习算法比较实用的<br/>训练集、交叉验证集和测试集。我们也介绍了学习算法的调试，以及如何确保学习算法的正常运行，于是我们介绍了一些诊断法，比如学习曲线，同时也讨论了误差分析、上限分析等等内容。</p><p>      所有这些工具都能有效地指引你决定接下来应该怎样做，让你把宝贵的时间用在刀刃上。现在你已经掌握了很多机器学习的工具，包括监督学习算法和无监督学习算法等等。</p><p>      但除了这些以外，我更希望你现在不仅仅只是认识这些工具，更重要的是掌握怎样有效地利用这些工具来建立强大的机器学习系统。所以，以上就是这门课的全部内容。如果你跟着我们的课程一路走来，到现在，你应该已经感觉到自己已经成为机器学习方面的专家了吧？</p><p><b>      我们都知道，机器学习是一门对科技、工业产生深远影响的重要学科，而现在，你已经完全具备了应用这些机器学习工具来创造伟大成就的能力。我希望你们中的很多人都能在相应的领域，应用所学的机器学习工具，构建出完美的机器学习系统，开发出无与伦比的产品和应用。并且我也希望你们通过应用机器学习，不仅仅改变自己的生活，有朝一日，还要让更多的人生活得更加美好！</b></p><p>      我也想告诉大家，教这门课对我来讲是一种享受。所以，谢谢大家！<br/>      最后，在结束之前，我还想再多说一点：那就是，也许不久以前我也是一个学生，即使是现在，我也尽可能挤出时间听一些课，学一些新的东西。所以，我深知要坚持学完这门课是很需要花一些时间的，我知道，也许你是一个很忙的人，生活中有很多很多事情要处理。正因如此，你依然挤出时间来观看这些课程视频。我知道，很多视频的时间都长达数小时，你依然花了好多时间来做这些复习题。你们中好多人，还愿意花时间来研究那些编程练习，那些又长又复杂的编程练习。我对你们表示衷心的感谢！我知道你们很多人在这门课中都非常努力，很多人都在这门课上花了很多时间，很多人都为这门课贡献了自己的很多精力。所以，我衷心地希望你们能从这门课中有所收获！<br/>      最后我想说！再次感谢你们选修这门课程！</p><p><b><i>Andew Ng</i></b></p><hr/><p>最后感谢吴恩达老师，提供如此通俗易懂的课程让普通的学生都可以入门机器学习，感谢提供视频PDF讲义的黄海广博士</p><p>课程在网易云课堂上的链接：<a href=\"https://link.zhihu.com/?target=https%3A//study.163.com/course/courseMain.htm%3FcourseId%3D1004570029\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">study.163.com/course/co</span><span class=\"invisible\">urseMain.htm?courseId=1004570029</span><span class=\"ellipsis\"></span></a></p><p>PDF讲义链接：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/fengdu78/Coursera-ML-AndrewNg-Notes\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/fengdu78/Cou</span><span class=\"invisible\">rsera-ML-AndrewNg-Notes</span><span class=\"ellipsis\"></span></a></p><p></p>", 
            "topic": [
                {
                    "tag": "随机梯度下降算法", 
                    "tagLink": "https://api.zhihu.com/topics/20712231"
                }, 
                {
                    "tag": "大规模机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/20047090"
                }, 
                {
                    "tag": "吴恩达（Andrew Ng）", 
                    "tagLink": "https://api.zhihu.com/topics/20003150"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/75036754", 
            "userName": "Lyon", 
            "userLink": "https://www.zhihu.com/people/38495a9b1f83e4a612543ce6c523df7f", 
            "upvote": 18, 
            "title": "【吴恩达机器学习】第九周—异常检测和推荐系统", 
            "content": "<p><b>前言：</b></p><p>最近在学习深度学习，看了不少教程，发现还是吴恩达的比较适用于我。吴恩达机器学习公开课视频最早是斯坦福大学的课程视频(那个画面有点老)，新版的视频在网易云课堂上可以随时学习。仅仅通过视频学习，可能会有点快，因为有的知识点需要反复推敲和回味。感谢github上一位朋友的精心整理，让我们可以配合讲义一起学习，讲义有HTML版、PDF版、Markdown版的：视频配合讲义看，事半功倍</p><p>本文包括接下来的系列文章，都会是吴恩达机器学习课程的个人学习总结，有很多文字和图片直接参考了PDF版的课程讲义，再次谢过提供讲义的朋友！</p><p>网易云课堂课程视频：</p><p><a href=\"https://link.zhihu.com/?target=https%3A//study.163.com/course/courseMain.htm%3FcourseId%3D1004570029\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">study.163.com/course/co</span><span class=\"invisible\">urseMain.htm?courseId=1004570029</span><span class=\"ellipsis\"></span></a></p><p>课程同步配套讲义：</p><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/fengdu78/Coursera-ML-AndrewNg-Notes\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/fengdu78/Cou</span><span class=\"invisible\">rsera-ML-AndrewNg-Notes</span><span class=\"ellipsis\"></span></a></p><p>原文发布在语雀文档上,效果更好看点：</p><a href=\"https://link.zhihu.com/?target=https%3A//www.yuque.com/docs/share/5196406e-d251-4c34-9d9a-a2f3ef0ca886\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-72bbb1e26eacf07192c32d1b6514dde0_ipico.jpg\" data-image-width=\"512\" data-image-height=\"512\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">第九周 · 语雀</a><hr/><p><b>第九周</b></p><p><b>1. 异常检测</b></p><p><b>1.1 问题的动机</b></p><p><b>      异常检测，Anomaly detection，</b>常用于非监督学习，让我们用一个飞机引擎的异常检测例子来说明。</p><p>      假想你是一个飞机引擎制造商，当你生产的飞机引擎从生产线上流出时，你需要进行QA(质量控制测试)，而作为这个测试的一部分，你测量了飞机引擎的一些特征变量，比如引擎运转时产生的热量，或者引擎的振动等等。假设此处有2个特征x1,x2。m个数据样本从 <img src=\"https://www.zhihu.com/equation?tex=x%5E%7B%281%29%7D%2Cx%5E%7B%282%29%7D%2C..%2Cx%5E%7B%28m%29%7D\" alt=\"x^{(1)},x^{(2)},..,x^{(m)}\" eeimg=\"1\"/> ，将样本和特征的关系绘制成图表如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-376762a3ae3f64d14e30bf7577a92901_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"774\" data-rawheight=\"424\" class=\"origin_image zh-lightbox-thumb\" width=\"774\" data-original=\"https://pic2.zhimg.com/v2-376762a3ae3f64d14e30bf7577a92901_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;774&#39; height=&#39;424&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"774\" data-rawheight=\"424\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"774\" data-original=\"https://pic2.zhimg.com/v2-376762a3ae3f64d14e30bf7577a92901_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-376762a3ae3f64d14e30bf7577a92901_b.jpg\"/></figure><p>      这个图表有什么用途呢？大意上给定一个新的测试数据 <img src=\"https://www.zhihu.com/equation?tex=x_%7Btest%7D\" alt=\"x_{test}\" eeimg=\"1\"/> （上图中的绿点），如果此样本在红色样本的中间，则我们可以认为其是正常的，否则我们认为这个引擎室异常的(下面那个绿点)。具体点说，这种方法叫做密度估计，表达式如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-32fce9f326c39fb2ab3be28c70a72cfc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"277\" data-rawheight=\"76\" class=\"content_image\" width=\"277\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;277&#39; height=&#39;76&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"277\" data-rawheight=\"76\" class=\"content_image lazy\" width=\"277\" data-actualsrc=\"https://pic1.zhimg.com/v2-32fce9f326c39fb2ab3be28c70a72cfc_b.jpg\"/></figure><p>      其中p(x)为概率密度分布函数， <img src=\"https://www.zhihu.com/equation?tex=%5Cvarepsilon\" alt=\"\\varepsilon\" eeimg=\"1\"/> 为划定的概率值，且通常这个p(x)分布选用正态分布(高斯分布)。我们可以设定<img src=\"https://www.zhihu.com/equation?tex=%5Cvarepsilon\" alt=\"\\varepsilon\" eeimg=\"1\"/>= 0.001，则当新的引擎样本经过 <img src=\"https://www.zhihu.com/equation?tex=p%28x_%7Btest%7D%29%3C%5Cvarepsilon\" alt=\"p(x_{test})&lt;\\varepsilon\" eeimg=\"1\"/> 时，我们就可以判定此引擎为不合格。</p><p>其他应用：</p><p>      异常检测主要用来识别欺骗。例如在线采集而来的有关用户的数据，一个特征向量中可能会包含如：用户多久登录一次，访问过的页面，在论坛发布的帖子数量，甚至是打字速度<br/>等。尝试根据这些特征构建一个模型，可以用这个模型来识别那些不符合该模式的用户。<br/>再一个例子是检测一个数据中心，特征可能包含：内存使用情况，被访问的磁盘数量，CPU 的负载，网络的通信量等。根据这些特征可以构建一个模型，用来判断某些计算机是不<br/>是有可能出错了。</p><p><b>1.2 正态分布/高斯分布</b></p><p><b>异常检测假设特征符合正太分布/高斯分布，如果数据的分布不是高斯分布，异常检测算法也能够工作，但是最好还是将数据转换成高斯分布，所以我们需要掌握正太分布的知识。</b></p><p><b>      正态分布(Normal Distribution)</b>也叫高斯分布(Gaussian Distribution),我们先回归一下正太分布的基本知识：</p><p>如果，我们认为变量x服从正态分布，则其可以表示为： <img src=\"https://www.zhihu.com/equation?tex=x+%5Csim+N%28%5Cmu%2C%5Csigma%5E2%29\" alt=\"x \\sim N(\\mu,\\sigma^2)\" eeimg=\"1\"/> 服从正态分布的函数，其有两个重要指标： <img src=\"https://www.zhihu.com/equation?tex=%E6%9C%9F%E6%9C%9B%EF%BC%9A%5Cmu%2C%E6%96%B9%E5%B7%AE%EF%BC%9A%5Csigma%5E2\" alt=\"期望：\\mu,方差：\\sigma^2\" eeimg=\"1\"/> </p><p>其中：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cmu+%3D+%5Cfrac%7B1%7D%7Bm%7D%5CSigma%5Em_%7Bi%3D1%7D+x%5E%7B%28i%29%7D%2C+%5Csigma%5E2+%3D+%5Cfrac%7B1%7D%7Bm%7D%5CSigma%5Em_%7Bi%3D1%7D+%28x%5E%7B%28i%29%7D-%5Cmu%29%5E2\" alt=\"\\mu = \\frac{1}{m}\\Sigma^m_{i=1} x^{(i)}, \\sigma^2 = \\frac{1}{m}\\Sigma^m_{i=1} (x^{(i)}-\\mu)^2\" eeimg=\"1\"/> </p><p>整个分布的概率密度函数为：</p><p><img src=\"https://www.zhihu.com/equation?tex=p%28x%2C%5Cmu%2C%5Csigma%5E2%29+%3D+++%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Csigma%7D+exp%28-%5Cfrac%7B%28x-%5Cmu%29%5E2%7D%7B2%5Csigma%5E2%7D%29\" alt=\"p(x,\\mu,\\sigma^2) =   \\frac{1}{\\sqrt{2\\pi}\\sigma} exp(-\\frac{(x-\\mu)^2}{2\\sigma^2})\" eeimg=\"1\"/> </p><p>整个概率密度函数的累加和为1，即表示100%。不同期望和方差的高斯分布图像如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-12d09c72fb4b27b625ca466e0d1d2323_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"639\" data-rawheight=\"421\" class=\"origin_image zh-lightbox-thumb\" width=\"639\" data-original=\"https://pic4.zhimg.com/v2-12d09c72fb4b27b625ca466e0d1d2323_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;639&#39; height=&#39;421&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"639\" data-rawheight=\"421\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"639\" data-original=\"https://pic4.zhimg.com/v2-12d09c72fb4b27b625ca466e0d1d2323_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-12d09c72fb4b27b625ca466e0d1d2323_b.jpg\"/></figure><p>      注：机器学习中对于方差我们通常只除以m而非统计学中的(m − 1)。这里顺便提一下，在实际使用中，到底是选择使用1/m还是1/(m − 1)其实区别很小，只要你有一个还算大的训练集，在机器学习领域大部分人更习惯使用1/m这个版本的公式。这两个版本的公式在理论特性和数学特性上稍有不同，但是在实际使用中，他们的区别甚小，几乎可以忽略不计</p><p><b>1.3 异常检测算法</b></p><p>      假设，我们有一组无标签(没有y)的训练集 <img src=\"https://www.zhihu.com/equation?tex=x%5E%7B%281%29%7D%2Cx%5E%7B%282%29%7D%2C..%2Cx%5E%7B%28m%29%7D\" alt=\"x^{(1)},x^{(2)},..,x^{(m)}\" eeimg=\"1\"/> ，这些训练集有n个特征，我们将用这些数据利用正太分布，构造出异常检测算法。其实，很简单，无非是算出训练集所有样本在每个特征上的的期望和方差，然后所有的概率相乘，即可得到总体概率密度函数。我们根据得到的p(x)和设定的判断边界 <img src=\"https://www.zhihu.com/equation?tex=%5Cvarepsilon\" alt=\"\\varepsilon\" eeimg=\"1\"/> 即可对未知样本经行异常检测，这便是一个简单的异常检测算法。具体如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-f0d5108c80c7809a9d881ae64ace966f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"815\" data-rawheight=\"467\" class=\"origin_image zh-lightbox-thumb\" width=\"815\" data-original=\"https://pic4.zhimg.com/v2-f0d5108c80c7809a9d881ae64ace966f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;815&#39; height=&#39;467&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"815\" data-rawheight=\"467\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"815\" data-original=\"https://pic4.zhimg.com/v2-f0d5108c80c7809a9d881ae64ace966f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-f0d5108c80c7809a9d881ae64ace966f_b.jpg\"/></figure><p>这样，一个简单的异常检测算法便开发完成了。下面看一个例子的概率分布例子，以及如何用它来进行异常检测的。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-06429d09ecbafb314c836c26f1d220b0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"750\" data-rawheight=\"444\" class=\"origin_image zh-lightbox-thumb\" width=\"750\" data-original=\"https://pic1.zhimg.com/v2-06429d09ecbafb314c836c26f1d220b0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;750&#39; height=&#39;444&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"750\" data-rawheight=\"444\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"750\" data-original=\"https://pic1.zhimg.com/v2-06429d09ecbafb314c836c26f1d220b0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-06429d09ecbafb314c836c26f1d220b0_b.jpg\"/></figure><p>      在这里，我们有两个特征x1,x2，左上角是训练集中所有的样本点(红色)和特征关系图；右上角是单个特征的概率密度分布图，左下角是总体的分布图。</p><p>      这时，我们通过概率密度分布可以对新的样本点经行异常检测了，此处设定的 <img src=\"https://www.zhihu.com/equation?tex=%5Cvarepsilon\" alt=\"\\varepsilon\" eeimg=\"1\"/> 为0.02(判定边界)。经过计算，可以发现x_test1样本点的概率 &gt; 0.02,正常；x_test1概率 &lt; 0.02,故被判断为异常。</p><p><b>1.4 异常检测系统</b></p><p><b>异常检测系统是基于异常检测算法开发的，其不仅包含异常检测算法，还增加了开发和评价过程，主要是在真实环境下，对样本的划分(训练集、交叉验证集、测试集)、对系统的评价等。</b></p><p>      异常检测算法是一个非监督学习算法，意味着我们无法根据结果变量y的值来告诉我们数据是否真的是异常的。我们需要另一种方法来帮助检验算法是否有效。<b>当我们开发一个异常检测系统时</b>，我们从带标记（异常或正常）的数据着手，我们从其中选择<b>一部分正常数据用于构建训练集</b>，然后用<b>剩下的正常数据和异常数据混合的数据构成交叉检验集和测试集</b></p><p>例如：我们有 10000 台正常引擎的数据,有20 台异常引擎的数据。 我们这样分配数据：</p><p>6000 台正常引擎的数据作为训练集</p><p>2000 台正常引擎和 10 台异常引擎的数据作为交叉检验集</p><p>2000 台正常引擎和 10 台异常引擎的数据作为测试集</p><p><b>具体的评价方法如下：</b></p><p>1.根据测试集数据，我们估计特征的平均值和方差并构建p(x)函数</p><p>2.对交叉检验集，我们尝试使用不同的 <img src=\"https://www.zhihu.com/equation?tex=%5Cvarepsilon\" alt=\"\\varepsilon\" eeimg=\"1\"/> 值作为阀值，并预测数据是否异常，根据 F1 值或者查准率与查全率的比例来选择<img src=\"https://www.zhihu.com/equation?tex=%5Cvarepsilon\" alt=\"\\varepsilon\" eeimg=\"1\"/></p><p>3.选出<img src=\"https://www.zhihu.com/equation?tex=%5Cvarepsilon\" alt=\"\\varepsilon\" eeimg=\"1\"/>后，针对测试集进行预测，计算异常检验系统的F1值，或者查准率与查全率之比。</p><p><b>1.5 异常检测和监督学习对比</b></p><p>之前我们构建的异常检测系统也使用了带标记的数据，与监督学习有些相似，下面的对比有助于选择采用监督学习还是异常检测</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-c69529a99672b383d805006f78e95658_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1122\" data-rawheight=\"505\" class=\"origin_image zh-lightbox-thumb\" width=\"1122\" data-original=\"https://pic1.zhimg.com/v2-c69529a99672b383d805006f78e95658_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1122&#39; height=&#39;505&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1122\" data-rawheight=\"505\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1122\" data-original=\"https://pic1.zhimg.com/v2-c69529a99672b383d805006f78e95658_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-c69529a99672b383d805006f78e95658_b.jpg\"/></figure><p><b>1.6 特征选择</b></p><p><b>1.6.1 数据转换</b></p><p>      在选择特征之前，我们要尽量确保数据是基本符号高斯分布的，否则我们需要将其转化成近似高斯分布的形态。例如使用对数函数： <img src=\"https://www.zhihu.com/equation?tex=x+%3D+log%28x%2Bc%29\" alt=\"x = log(x+c)\" eeimg=\"1\"/> 其中c为非负数,范围在0-1之间。在 python 中，通常用 np.log1p()函数。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-2e8df6e2de4c218f76d2aaa944c7d7c3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"603\" data-rawheight=\"415\" class=\"origin_image zh-lightbox-thumb\" width=\"603\" data-original=\"https://pic4.zhimg.com/v2-2e8df6e2de4c218f76d2aaa944c7d7c3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;603&#39; height=&#39;415&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"603\" data-rawheight=\"415\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"603\" data-original=\"https://pic4.zhimg.com/v2-2e8df6e2de4c218f76d2aaa944c7d7c3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-2e8df6e2de4c218f76d2aaa944c7d7c3_b.jpg\"/></figure><p><b>1.6.1 误差分析</b></p><p>      误差分析的目的在于：</p><p>      从已有的模型和特征中开始跑样本，通过对预测结果中判断错误的数据(误差)经行分析，从而发现和挑选更适合的特征，从而改进模型。</p><p>      我们通过p(x)来判断一个样本是正常还是异常，且通常情况下，我们希望正常样本的p(x)尽量大，异常样本的p(x)足够小，但，这往往就是通常会出问题的地方，即当一个实际上是异常的样本点，经过异常检测系统判断后p(x)确足够大，即异常检测系统判断失效，导致判断错误。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-3178375a256c7a9ee5f87ea21479790f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"680\" data-rawheight=\"406\" class=\"origin_image zh-lightbox-thumb\" width=\"680\" data-original=\"https://pic4.zhimg.com/v2-3178375a256c7a9ee5f87ea21479790f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;680&#39; height=&#39;406&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"680\" data-rawheight=\"406\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"680\" data-original=\"https://pic4.zhimg.com/v2-3178375a256c7a9ee5f87ea21479790f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-3178375a256c7a9ee5f87ea21479790f_b.jpg\"/></figure><p>这里举个例子：原模型有一个特征x1,经过异常检测算法拟合出来的曲线如上左图，这时候有一个异常样本，用该模型检测时，确发现其x值在正常区间，如上左图中绿点所示，所以我们得到一个信息：该异常检测模型不够完善，可能是由于特征x1不够，不能覆盖样本的总体特征情况。</p><p>此时，我们即可构造出新特征，即采用x1和x2两个特征，再重新应用异常检测算法训练模型，新训练的样本特征分布图如上右图，通过这个新的异常检测系统，我们可以成功预测出异常点。</p><p>例2：</p><p>      一个数据中心的例子，数据中心通常是由n台服务器组成的集群，对这n台服务器的运行状态经行监控就显得非常重要。通常情况下，我们可以选出4个特征：</p><ul><li>x1 = 内存占用</li><li>x2 = 磁盘每秒访问次数</li><li>x3 = CPU负载</li><li>x4 = 网络流量</li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-41d8e391685cdcfd20f36f2d864ef5e4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"748\" data-rawheight=\"370\" class=\"origin_image zh-lightbox-thumb\" width=\"748\" data-original=\"https://pic1.zhimg.com/v2-41d8e391685cdcfd20f36f2d864ef5e4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;748&#39; height=&#39;370&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"748\" data-rawheight=\"370\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"748\" data-original=\"https://pic1.zhimg.com/v2-41d8e391685cdcfd20f36f2d864ef5e4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-41d8e391685cdcfd20f36f2d864ef5e4_b.jpg\"/></figure><p>     通常，这些特征能较好的满足异常检测的需求，但实际情况发现，这个模型不能捕获到一类的异常情况，譬如：</p><p>当x3和x4都很低时，我们认为某台服务器在经历较大规模的用户访问，计算和网络吞吐都很高，模型将其判为异常，但是当其中只有一个较高时，譬如x3较低，x4较高，模型可能判断为正常，那么就会忽略一种情况：</p><p>程序死锁，CPU飙升至很高，但是对外提供服务的网络流量几乎为0，那么为了捕捉这种异常，我们需要建立新的特征，譬如可以用x5，x6。</p><p><b>1.7 多元高斯分布 </b></p><p>      在1.3小节中，我们发现正常的高斯分布可以同时应对n个特征，那么多元高斯分布存在的意义在哪？正常情况下，普通的高斯分布，要求的n个特征必须特征明确，且互相独立，无相关性，这样才能较好地构建模型，如果特征间可能存在关系，则可能导致模型的不准确，此时我们可以考虑用多元高斯分布。</p><p>还是以1.6中数据中心，服务器集群的例子：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-0dcd855941f7cd15443c3989e2ed66b6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"754\" data-rawheight=\"420\" class=\"origin_image zh-lightbox-thumb\" width=\"754\" data-original=\"https://pic3.zhimg.com/v2-0dcd855941f7cd15443c3989e2ed66b6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;754&#39; height=&#39;420&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"754\" data-rawheight=\"420\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"754\" data-original=\"https://pic3.zhimg.com/v2-0dcd855941f7cd15443c3989e2ed66b6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-0dcd855941f7cd15443c3989e2ed66b6_b.jpg\"/></figure><p>      分别用x1 = CPU负载、x2 = 内存使用量作为两个特征，构建出的特征-样本分布图、特征密度函数图如上。根据这两个特征构建的异常检测模型，会认为所有在左图粉色圆圈中的样本都是正常的，但是实际情况下，我们可能会发现其实在左图蓝色小圈中的样本是正常的，而篮圈以外的样本都应该是异常的（如左图中的绿色点）。于是，这个模型便不够好。我们可以用多元高斯分布改造它。<br/>      普通高斯分布的概率密度，是各个特征的概率密度公式累乘的，公式如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-1c1458267861814d540c6b3e28da1940_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"511\" data-rawheight=\"100\" class=\"origin_image zh-lightbox-thumb\" width=\"511\" data-original=\"https://pic1.zhimg.com/v2-1c1458267861814d540c6b3e28da1940_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;511&#39; height=&#39;100&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"511\" data-rawheight=\"100\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"511\" data-original=\"https://pic1.zhimg.com/v2-1c1458267861814d540c6b3e28da1940_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-1c1458267861814d540c6b3e28da1940_b.jpg\"/></figure><p>      而多元高斯分布的概率密度公式，不需要分别计算各特征的概率密度再累乘，而是通过求协方差矩阵：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-bc69251788a9b51a727d20ae90ddbdd1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"504\" data-rawheight=\"107\" class=\"origin_image zh-lightbox-thumb\" width=\"504\" data-original=\"https://pic2.zhimg.com/v2-bc69251788a9b51a727d20ae90ddbdd1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;504&#39; height=&#39;107&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"504\" data-rawheight=\"107\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"504\" data-original=\"https://pic2.zhimg.com/v2-bc69251788a9b51a727d20ae90ddbdd1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-bc69251788a9b51a727d20ae90ddbdd1_b.jpg\"/></figure><p>多元高斯分布的概率密度公式如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-8b3d7688ecb03e8acc2c94c5bc104eb6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"423\" data-rawheight=\"61\" class=\"origin_image zh-lightbox-thumb\" width=\"423\" data-original=\"https://pic3.zhimg.com/v2-8b3d7688ecb03e8acc2c94c5bc104eb6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;423&#39; height=&#39;61&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"423\" data-rawheight=\"61\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"423\" data-original=\"https://pic3.zhimg.com/v2-8b3d7688ecb03e8acc2c94c5bc104eb6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-8b3d7688ecb03e8acc2c94c5bc104eb6_b.jpg\"/></figure><p><b>1.7.1 注意</b></p><ul><li>1.此处的 <img src=\"https://www.zhihu.com/equation?tex=%5Cmu\" alt=\"\\mu\" eeimg=\"1\"/> 是一个n维向量，其维度n = 特征总数。其中每个值代表了每个特征的期望。</li><li>2.训练样本数m必须 &gt; 特征维度n，<b>不然的话协方差矩阵不可逆的</b>，通常需要m&gt;10n 另外特征冗余也会导致协方差矩阵不可逆</li></ul><p><b>1.7.2 和原高斯分布的比较</b></p><p>原高斯分布模型被广泛使用着，如果特征之间在某种程度上存在相互关联的情况，我们可以通过构造新新特征的方法来捕捉这些相关性。如果训练集不是太大，并且没有太多的特征，我们可以使用多元高斯分布模型。</p><p><b>原高斯分布模型</b> </p><p><b>多元高斯分布模型</b></p><p>不能捕捉特征之间的相关性 但可以通过将<br/>特征进行组合的方法来解决</p><p>自动捕捉特征之间的相关性</p><p>计算代价低，能适应大规模的特征 </p><p>计算代价较高 训练集较小时也同样适用</p><p><b>2. 推荐系统</b></p><p><b>2.1 问题规划</b></p><p>      在接下来的视频中，我想讲一下推荐系统。我想讲推荐系统有两个原因：<br/>      第一、仅仅因为<b>它是机器学习中的一个重要的应用</b>。在过去几年，我偶尔访问硅谷不同的技术公司，我常和工作在这儿致力于机器学习应用的人们聊天，我常问他们，最重要的机器学习的应用是什么，或者，你最想改进的机器学习应用有哪些。我最常听到的答案是推荐系统。现在，在硅谷有很多团体试图建立很好的推荐系统。因此，如果你考虑网站像亚马逊，或网飞公司或易趣，或 iTunes Genius，有很多的网站或系统试图推荐新产品给用户。如，亚马逊推荐新书给你，网飞公司试图推荐新电影给你，等等。这些推荐系统，根据浏览你过去买过什么书，或过去评价过什么电影来判断。这些系统会带来很大一部分收入，比如为亚马逊和像网飞这样的公司。因此，对推荐系统性能的改善，将对这些企业的有实质性和直接的影响。<br/>      推荐系统是个有趣的问题，在学术机器学习中因此，我们可以去参加一个学术机器学习会议，推荐系统问题实际上受到很少的关注，或者，至少在学术界它占了很小的份额。但是，如果你看正在发生的事情，许多有能力构建这些系统的科技企业，他们似乎在很多企业中占据很高的优先级。这是我为什么在这节课讨论它的原因之一。<br/>      我想讨论推荐系统地第二个原因是：这个班视频的最后几集我想讨论机器学习中的一些大思想，并和大家分享。这节课我们也看到了，对机器学习来说，特征是很重要的，你所选择的特征，将对你学习算法的性能有很大的影响。因此，在机器学习中有一种大思想，它针对一些问题，可能并不是所有的问题，而是一些问题，有算法可以为你自动学习一套好的特征。因此，不要试图手动设计，而手写代码这是目前为止我们常干的。有一些设置，你可以有一个算法，仅仅学习其使用的特征，推荐系统就是此类的一个例子。还有很多其它的，但是通过推荐系统，我们将领略一小部分特征学习的思想，至少，你将能够了解到这方面的一个例子，我认为，机器学习中的大思想也是这样。因此，让我们开始讨论推荐系统问题形式化</p><p>      我们从一个例子开始定义推荐系统的问题。</p><p>      假使我们是一个电影供应商，我们有 5 部电影和 4 个用户，我们要求用户为电影打分。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-586e224706e3e2763159d92492bbc731_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"748\" data-rawheight=\"391\" class=\"origin_image zh-lightbox-thumb\" width=\"748\" data-original=\"https://pic2.zhimg.com/v2-586e224706e3e2763159d92492bbc731_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;748&#39; height=&#39;391&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"748\" data-rawheight=\"391\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"748\" data-original=\"https://pic2.zhimg.com/v2-586e224706e3e2763159d92492bbc731_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-586e224706e3e2763159d92492bbc731_b.jpg\"/></figure><p>      前三部电影是爱情片，后两部则是动作片，我们可以看出 Alice 和 Bob 似乎更倾向与爱情片， 而 Carol 和 Dave 似乎更倾向与动作片。并且没有一个用户给所有的电影都打过分。我们希望构建一个算法来预测他们每个人可能会给他们没看过的电影打多少分，并以此作为推荐的依据。</p><p>下面引入一些标记：</p><p><img src=\"https://www.zhihu.com/equation?tex=n_u\" alt=\"n_u\" eeimg=\"1\"/> 代表用户数量</p><p><img src=\"https://www.zhihu.com/equation?tex=n_m\" alt=\"n_m\" eeimg=\"1\"/> 代表电影数量</p><p><img src=\"https://www.zhihu.com/equation?tex=r%28i%2Cj%29\" alt=\"r(i,j)\" eeimg=\"1\"/> 如果用户j给电影i评过分，则记为1</p><p><img src=\"https://www.zhihu.com/equation?tex=y%5E%7B%28i%2Cj%29%7D\" alt=\"y^{(i,j)}\" eeimg=\"1\"/> 用户j给电影i评分的数值，从0~5</p><p><img src=\"https://www.zhihu.com/equation?tex=m%5Ej\" alt=\"m^j\" eeimg=\"1\"/> 代表用户评过分的电影总数</p><p><b>2.2 基于内容的推荐系统</b></p><p>      在一个基于内容的推荐系统算法中，我们假设对于我们希望推荐的东西有一些数据，这些数据是有关这些东西的特征。</p><p>在我们的例子中，我们可以假设每部电影都有两个特征，如x1代表电影的浪漫程度,x2代表电影的动作程度。这样，对于每部电影，我们都可以给这两个特征打分(0~1)，用于表现每个电影的特征。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-d503613d012fa76f6daec78b11f45a3d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"699\" data-rawheight=\"235\" class=\"origin_image zh-lightbox-thumb\" width=\"699\" data-original=\"https://pic2.zhimg.com/v2-d503613d012fa76f6daec78b11f45a3d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;699&#39; height=&#39;235&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"699\" data-rawheight=\"235\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"699\" data-original=\"https://pic2.zhimg.com/v2-d503613d012fa76f6daec78b11f45a3d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-d503613d012fa76f6daec78b11f45a3d_b.jpg\"/></figure><p>      则每部电影都有一个特征向量，如 <img src=\"https://www.zhihu.com/equation?tex=x%5E%7B%281%29%7D\" alt=\"x^{(1)}\" eeimg=\"1\"/> 表示第一部电影的特征向量，其爱情程度0.9，动作程度0，故其特征向量为：[0.9,0]</p><p>下面我们要基于以上特征构建一个推荐算法，顾名思义推荐算法就是用于给观众推荐电影的，简单的推荐算法大意如下：<b>给观众推荐电影之前，我们首先需要遍历电影库，用算法来评估观众可能打分的分值，最后取高打分的电影推荐给观众。那么问题就转化为了，我们需要一个模型，输入用户和电影，模型将输出观众可能打分的分值。</b></p><p>      假设我们采用线性回归模型，我们需要对每个用户建立线性回归模型，参数说明如下：<br/>      1. <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%28j%29%7D\" alt=\"\\theta^{(j)}\" eeimg=\"1\"/> 表示用户j的参数向量</p><p>      2. <img src=\"https://www.zhihu.com/equation?tex=x%5E%7B%28i%29%7D\" alt=\"x^{(i)}\" eeimg=\"1\"/> 表示电影i的特征向量</p><p>      3.  <img src=\"https://www.zhihu.com/equation?tex=%28%5Ctheta%5E%7B%28j%29%7D%29%5ET+x%5E%7B%28i%29%7D\" alt=\"(\\theta^{(j)})^T x^{(i)}\" eeimg=\"1\"/> 表示用户j对电影i的预估评分</p><p><b>针对用户j，模型的代价函数：</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ed745fed8a8faa0ff925fd521b0bf294_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"442\" data-rawheight=\"87\" class=\"origin_image zh-lightbox-thumb\" width=\"442\" data-original=\"https://pic1.zhimg.com/v2-ed745fed8a8faa0ff925fd521b0bf294_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;442&#39; height=&#39;87&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"442\" data-rawheight=\"87\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"442\" data-original=\"https://pic1.zhimg.com/v2-ed745fed8a8faa0ff925fd521b0bf294_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-ed745fed8a8faa0ff925fd521b0bf294_b.jpg\"/></figure><p>      Σ的下标表示我们只计算那些用户j评过分的电影。在一般的线性回归模型中，误差项和正则项应该都是乘以1/2m,在这里我们将m去掉，并且我们不对方差项 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_0\" alt=\"\\theta_0\" eeimg=\"1\"/> 进行正则化处理。上面的代价函数只是针对一个用户的，为了学习所有用户，我们将所有用户的代价函数求和：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-67990c3d89687125825964e2a8e53698_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"575\" data-rawheight=\"95\" class=\"origin_image zh-lightbox-thumb\" width=\"575\" data-original=\"https://pic1.zhimg.com/v2-67990c3d89687125825964e2a8e53698_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;575&#39; height=&#39;95&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"575\" data-rawheight=\"95\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"575\" data-original=\"https://pic1.zhimg.com/v2-67990c3d89687125825964e2a8e53698_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-67990c3d89687125825964e2a8e53698_b.jpg\"/></figure><p>如果我们要用梯度下降法来求解最优解，我们计算代价函数的偏导数后得到梯度下降的更新公式为：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-e16b3898262e9ffd6478efd274e373d3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"663\" data-rawheight=\"155\" class=\"origin_image zh-lightbox-thumb\" width=\"663\" data-original=\"https://pic4.zhimg.com/v2-e16b3898262e9ffd6478efd274e373d3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;663&#39; height=&#39;155&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"663\" data-rawheight=\"155\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"663\" data-original=\"https://pic4.zhimg.com/v2-e16b3898262e9ffd6478efd274e373d3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-e16b3898262e9ffd6478efd274e373d3_b.jpg\"/></figure><p><b>2.3 协同过滤</b></p><p>      在之前的基于内容的推荐系统中，对于每一部电影，我们都掌握了可用的特征，使用这些特征训练出了每一个用户的参数。相反地，如果我们拥有用户的参数，我们可以学习得出电影的特征。</p><p>      我们通过用户对电影的评论(θ)可以同样建立模型来评估电影的特征，模型的损失函数如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-f4e9393cf33ab5d6a68044ae5697af6f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"569\" data-rawheight=\"94\" class=\"origin_image zh-lightbox-thumb\" width=\"569\" data-original=\"https://pic4.zhimg.com/v2-f4e9393cf33ab5d6a68044ae5697af6f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;569&#39; height=&#39;94&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"569\" data-rawheight=\"94\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"569\" data-original=\"https://pic4.zhimg.com/v2-f4e9393cf33ab5d6a68044ae5697af6f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-f4e9393cf33ab5d6a68044ae5697af6f_b.jpg\"/></figure><p>      但是如果我们既没有用户的参数，也没有电影的特征，这两种方法都不可行了。协同过滤算法可以同时学习这两者。</p><p><b>      简单来说我们可以随机初始化一些θ然后学习出特征x，再由x学习改进新的θ,从而不断地学习和收敛，协同过滤算法有点像练武，左右手互博，从而学会一招一式.....(但实际应用时，其实可以二者同时学习和优化)</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-30ae874cf8473254011381b1de002c99_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"668\" data-rawheight=\"317\" class=\"origin_image zh-lightbox-thumb\" width=\"668\" data-original=\"https://pic2.zhimg.com/v2-30ae874cf8473254011381b1de002c99_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;668&#39; height=&#39;317&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"668\" data-rawheight=\"317\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"668\" data-original=\"https://pic2.zhimg.com/v2-30ae874cf8473254011381b1de002c99_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-30ae874cf8473254011381b1de002c99_b.jpg\"/></figure><p><b>2.4 协同过滤算法</b></p><p>       在2.3中介绍的概念，是方便理解的，实际应用过程中，我们其实没有必要先计算θ再计算x再计算θ....如此循环往复，我们可以同时对二者进行优化，所以这也是协同过滤算法名字的意义所在。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-b06328c8f1a6875e3d9536ad455818a3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"679\" data-rawheight=\"266\" class=\"origin_image zh-lightbox-thumb\" width=\"679\" data-original=\"https://pic4.zhimg.com/v2-b06328c8f1a6875e3d9536ad455818a3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;679&#39; height=&#39;266&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"679\" data-rawheight=\"266\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"679\" data-original=\"https://pic4.zhimg.com/v2-b06328c8f1a6875e3d9536ad455818a3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-b06328c8f1a6875e3d9536ad455818a3_b.jpg\"/></figure><p>那合在一起的代价函数 <img src=\"https://www.zhihu.com/equation?tex=J%28x%5E%7B%281%29%7D%2Cx%5E%7B%282%29%7D%2C..%2Cx%5E%7B%28n_m%29%7D%2C%5Ctheta%5E%7B%281%29%7D%2C%5Ctheta%5E%7B%282%29%7D%2C..%2C%5Ctheta%5E%7B%28n_u%29%7D%29\" alt=\"J(x^{(1)},x^{(2)},..,x^{(n_m)},\\theta^{(1)},\\theta^{(2)},..,\\theta^{(n_u)})\" eeimg=\"1\"/> </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-d28ed78e3ca0f9b405ac0f0bdcb4f253_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"697\" data-rawheight=\"93\" class=\"origin_image zh-lightbox-thumb\" width=\"697\" data-original=\"https://pic4.zhimg.com/v2-d28ed78e3ca0f9b405ac0f0bdcb4f253_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;697&#39; height=&#39;93&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"697\" data-rawheight=\"93\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"697\" data-original=\"https://pic4.zhimg.com/v2-d28ed78e3ca0f9b405ac0f0bdcb4f253_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-d28ed78e3ca0f9b405ac0f0bdcb4f253_b.jpg\"/></figure><p>此时，我们的优化目标，变成了：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b212d817df2280183b376b520845f4e9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"639\" data-rawheight=\"83\" class=\"origin_image zh-lightbox-thumb\" width=\"639\" data-original=\"https://pic2.zhimg.com/v2-b212d817df2280183b376b520845f4e9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;639&#39; height=&#39;83&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"639\" data-rawheight=\"83\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"639\" data-original=\"https://pic2.zhimg.com/v2-b212d817df2280183b376b520845f4e9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-b212d817df2280183b376b520845f4e9_b.jpg\"/></figure><p><b>2.5 向量化：低秩矩阵分解</b></p><p>      矩阵的秩r(A)，r(A)&lt;=min(m,n),A是m*n型矩阵，举个例子：</p><p><img src=\"https://www.zhihu.com/equation?tex=A+%3D++%5Cleft%5B++%5Cbegin%7Bmatrix%7D++++1+%5C%5C++++2+%5C%5C+++++++3+++%5Cend%7Bmatrix%7D+++%5Cright%5D\" alt=\"A =  \\left[  \\begin{matrix}    1 \\\\    2 \\\\       3   \\end{matrix}   \\right]\" eeimg=\"1\"/> <img src=\"https://www.zhihu.com/equation?tex=B+%3D++%5Cleft%5B++%5Cbegin%7Bmatrix%7D++++1+%261+%5C%5C++++2+%262+%5C%5C++++3+%263+++%5Cend%7Bmatrix%7D+++%5Cright%5D\" alt=\"B =  \\left[  \\begin{matrix}    1 &amp;1 \\\\    2 &amp;2 \\\\    3 &amp;3   \\end{matrix}   \\right]\" eeimg=\"1\"/> </p><p>r(A) = r(B) = 1,因为B矩阵中两列之间可以简化，故实际矩阵的秩为1。</p><p>      低秩矩阵即表示r(A)较小的矩阵。在前面的协同过滤例子中，我们通过模型可以预测出一个用户对所有电影的评分，这个评分矩阵即一个低秩矩阵。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-2521cfbd2d9ebb6667070cbabbb9d6ca_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"718\" data-rawheight=\"237\" class=\"origin_image zh-lightbox-thumb\" width=\"718\" data-original=\"https://pic3.zhimg.com/v2-2521cfbd2d9ebb6667070cbabbb9d6ca_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;718&#39; height=&#39;237&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"718\" data-rawheight=\"237\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"718\" data-original=\"https://pic3.zhimg.com/v2-2521cfbd2d9ebb6667070cbabbb9d6ca_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-2521cfbd2d9ebb6667070cbabbb9d6ca_b.jpg\"/></figure><p>      如图，给定所有用户对电影的评分矩阵Y，我们可以算出用户对电影的评分矩阵（上图右），这个矩阵中 <img src=\"https://www.zhihu.com/equation?tex=%28%5Ctheta%5E%7B%281%29%7D%29%5ET%28x%5E%7B%282%29%7D%29\" alt=\"(\\theta^{(1)})^T(x^{(2)})\" eeimg=\"1\"/> 表示第1个用户对第2部电影的评分。矩阵的size： <img src=\"https://www.zhihu.com/equation?tex=n_m%2An_u\" alt=\"n_m*n_u\" eeimg=\"1\"/>（行*列），但由于矩阵间行列存在关系，可以被分解，故实际上矩阵的秩为 <img src=\"https://www.zhihu.com/equation?tex=min%28n_m%2Cn_u%29\" alt=\"min(n_m,n_u)\" eeimg=\"1\"/> ,即此矩阵可以被看做是低秩矩阵。分解过程如下：</p><p>我们令：</p><p><img src=\"https://www.zhihu.com/equation?tex=X+%3D++%5Cleft%5B++%5Cbegin%7Bmatrix%7D++++-%28x%5E%7B%281%29%7D%29%5ET-+%5C%5C++++-%28x%5E%7B%282%29%7D%29%5ET-+%5C%5C++++..%5C%5C++++-%28x%5E%7B%28n_m%29%7D%29%5ET-+++%5Cend%7Bmatrix%7D+++%5Cright%5D\" alt=\"X =  \\left[  \\begin{matrix}    -(x^{(1)})^T- \\\\    -(x^{(2)})^T- \\\\    ..\\\\    -(x^{(n_m)})^T-   \\end{matrix}   \\right]\" eeimg=\"1\"/> <img src=\"https://www.zhihu.com/equation?tex=%5CTheta+%3D++%5Cleft%5B++%5Cbegin%7Bmatrix%7D++++-%28%5Ctheta%5E%7B%281%29%7D%29%5ET-+%5C%5C++++-%28%5Ctheta%5E%7B%282%29%7D%29%5ET-+%5C%5C++++..%5C%5C++++-%28%5Ctheta%5E%7B%28n_u%29%7D%29%5ET-+++%5Cend%7Bmatrix%7D+++%5Cright%5D\" alt=\"\\Theta =  \\left[  \\begin{matrix}    -(\\theta^{(1)})^T- \\\\    -(\\theta^{(2)})^T- \\\\    ..\\\\    -(\\theta^{(n_u)})^T-   \\end{matrix}   \\right]\" eeimg=\"1\"/> </p><p>则矩阵可以表示为 <img src=\"https://www.zhihu.com/equation?tex=X%5CTheta%5ET\" alt=\"X\\Theta^T\" eeimg=\"1\"/> </p><p>这种将低秩矩阵分开表示的行为，即可称为<b>低秩矩阵分解Low Rank Matrix Factorization</b></p><p><b>相似推荐</b><br/>      如果一个用户在看电影i，我们想要给他推荐和i相似的电影，那么怎么办？其实，很简单，如果我遍历电影库，找到了某部电影j，从而使电影i的特征和电影j的特征之间差异最小，那么j即是我需要推荐的电影。</p><p><img src=\"https://www.zhihu.com/equation?tex=%5CDelta+%3Dmin%28%7C%7Cx%5E%7B%28i%29%7D-x%5E%7B%28j%29%7D%7C%7C%29\" alt=\"\\Delta =min(||x^{(i)}-x^{(j)}||)\" eeimg=\"1\"/> </p><p>      同理，如果根据电影i推荐5部电影，则同样遍历电影库，找到Δ最小的前5部电影即可。</p><b><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-979c3e68fa03332e4e7eb1b1735e261a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"681\" data-rawheight=\"339\" class=\"origin_image zh-lightbox-thumb\" width=\"681\" data-original=\"https://pic3.zhimg.com/v2-979c3e68fa03332e4e7eb1b1735e261a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;681&#39; height=&#39;339&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"681\" data-rawheight=\"339\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"681\" data-original=\"https://pic3.zhimg.com/v2-979c3e68fa03332e4e7eb1b1735e261a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-979c3e68fa03332e4e7eb1b1735e261a_b.jpg\"/></figure></b><p><b>2.6 均值归一化</b></p><p><b>在协同过滤算法中，有时候均值归一化会让算法运行的更好</b></p><b><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-613da00de344abdd964290f5e80a913e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"934\" data-rawheight=\"528\" class=\"origin_image zh-lightbox-thumb\" width=\"934\" data-original=\"https://pic3.zhimg.com/v2-613da00de344abdd964290f5e80a913e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;934&#39; height=&#39;528&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"934\" data-rawheight=\"528\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"934\" data-original=\"https://pic3.zhimg.com/v2-613da00de344abdd964290f5e80a913e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-613da00de344abdd964290f5e80a913e_b.jpg\"/></figure></b><p>       还是使用之前电影推荐的例子，这次多加了一个观众Eve，但是他对这5部电影都没有看过，于是协同过滤算法预测他给这5部电影打分都为同样的0分，这样，就无法给其推荐电影了，显然这样是不太妥当的。这就是可以用均值归一化来处理的典型例子。</p><p>      我们可以计算出每部电影在每个用户评分下的均值，记为矩阵 <img src=\"https://www.zhihu.com/equation?tex=%5Cmu\" alt=\"\\mu\" eeimg=\"1\"/> ，然后用户评分矩阵Y = 原矩阵 - <img src=\"https://www.zhihu.com/equation?tex=%5Cmu\" alt=\"\\mu\" eeimg=\"1\"/> ，得到归一化的矩阵Y，这样做的结果是：即使Eve没有看过任何一部电影，推荐算法任然可以根据每部电影平均得分来为其推荐。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-8d8295603685b0eca356ea947da35ba8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"950\" data-rawheight=\"527\" class=\"origin_image zh-lightbox-thumb\" width=\"950\" data-original=\"https://pic1.zhimg.com/v2-8d8295603685b0eca356ea947da35ba8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;950&#39; height=&#39;527&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"950\" data-rawheight=\"527\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"950\" data-original=\"https://pic1.zhimg.com/v2-8d8295603685b0eca356ea947da35ba8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-8d8295603685b0eca356ea947da35ba8_b.jpg\"/></figure><p></p><p></p>", 
            "topic": [
                {
                    "tag": "异常检测", 
                    "tagLink": "https://api.zhihu.com/topics/20101124"
                }, 
                {
                    "tag": "推荐系统", 
                    "tagLink": "https://api.zhihu.com/topics/19563024"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/74902766", 
            "userName": "Lyon", 
            "userLink": "https://www.zhihu.com/people/38495a9b1f83e4a612543ce6c523df7f", 
            "upvote": 10, 
            "title": "【吴恩达机器学习】第八周—聚类降维Kmeans算法", 
            "content": "<p><b>前言：</b></p><p>最近在学习深度学习，看了不少教程，发现还是吴恩达的比较适用于我。吴恩达机器学习公开课视频最早是斯坦福大学的课程视频(那个画面有点老)，新版的视频在网易云课堂上可以随时学习。仅仅通过视频学习，可能会有点快，因为有的知识点需要反复推敲和回味。感谢github上一位朋友的精心整理，让我们可以配合讲义一起学习，讲义有HTML版、PDF版、Markdown版的：视频配合讲义看，事半功倍</p><p>本文包括接下来的系列文章，都会是吴恩达机器学习课程的个人学习总结，有很多文字和图片直接参考了PDF版的课程讲义，再次谢过提供讲义的朋友！</p><p>网易云课堂课程视频：</p><p><a href=\"https://link.zhihu.com/?target=https%3A//study.163.com/course/courseMain.htm%3FcourseId%3D1004570029\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">study.163.com/course/co</span><span class=\"invisible\">urseMain.htm?courseId=1004570029</span><span class=\"ellipsis\"></span></a></p><p>课程同步配套讲义：</p><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/fengdu78/Coursera-ML-AndrewNg-Notes\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/fengdu78/Cou</span><span class=\"invisible\">rsera-ML-AndrewNg-Notes</span><span class=\"ellipsis\"></span></a></p><p>原文发布在语雀文档上,效果更好看点：</p><a href=\"https://link.zhihu.com/?target=https%3A//www.yuque.com/docs/share/0302e6b7-8eb8-454b-8878-c549aafdce09\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-72bbb1e26eacf07192c32d1b6514dde0_ipico.jpg\" data-image-width=\"512\" data-image-height=\"512\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">第八周 · 语雀</a><hr/><p><b>第八周</b></p><p><b>1.聚类(Clustering)</b></p><p><b>1.1 介绍</b></p><p>      之前的课程介绍的都是监督学习、而<b>聚类属于非监督学习</b>，在一个典型的监督学习中，我们有一个有标签的训练集，我们的目标是找到能够区分正样本和负样本的决策边界，在这里的监督学习中，我们有一系列标签，我们需要据此拟合一个假设函数。与此不同的是，在非监督学习中，我们的数据没有附带任何标签，我们拿到的数据就是这样的：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-b88656e34a5f849155c11e5545b0bd07_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"615\" data-rawheight=\"466\" class=\"origin_image zh-lightbox-thumb\" width=\"615\" data-original=\"https://pic4.zhimg.com/v2-b88656e34a5f849155c11e5545b0bd07_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;615&#39; height=&#39;466&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"615\" data-rawheight=\"466\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"615\" data-original=\"https://pic4.zhimg.com/v2-b88656e34a5f849155c11e5545b0bd07_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-b88656e34a5f849155c11e5545b0bd07_b.jpg\"/></figure><p>      在这里我们有一系列点，却没有标签。因此，我们的训练集可以写成只有 <img src=\"https://www.zhihu.com/equation?tex=x%5E%7B%281%29%7D%2Cx%7B%282%29%7D...x%7B%28m%29%7D\" alt=\"x^{(1)},x{(2)}...x{(m)}\" eeimg=\"1\"/> ，我们没有任何标签y。因此，图上画的这些点没有标签信息。也就是说，在非监督学习中，我们需要将一系列无标签的训练数据，输入到一个算法中，然后我们告诉这个算法，快去为我们找找这个数据的内在结构给定数据。我们可能需要某种算法帮助我们寻找一种结构。图上的数据看起来可以分成两个分开的点集（称为簇），一个能够找到我圈出的这些点集的算法，就被称为<b>聚类算法</b>。</p><p>     聚类算法是无监督学习中的一类算法统称，包括多种具体的算法如<b>K-均值算法</b>、<b>均值偏移聚类算法、DBSCAN聚类算法、层次聚类算法等。</b></p><p><b>用途</b></p><p>聚类算法的常见用途和使用场景有哪些？</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b8a779d05e81d6945004d693c93897d1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"775\" data-rawheight=\"494\" class=\"origin_image zh-lightbox-thumb\" width=\"775\" data-original=\"https://pic2.zhimg.com/v2-b8a779d05e81d6945004d693c93897d1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;775&#39; height=&#39;494&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"775\" data-rawheight=\"494\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"775\" data-original=\"https://pic2.zhimg.com/v2-b8a779d05e81d6945004d693c93897d1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-b8a779d05e81d6945004d693c93897d1_b.jpg\"/></figure><p>      在这门课程的早些时候，我曾经列举过一些应用：比如市场分割。也许你在数据库中存储了许多客户的信息，而你希望将他们<b>分成不同的客户群</b>，这样你可以对不同类型的客户分别销售产品或者分别提供更适合的服务。社交网络分析：事实上有许多研究人员正在研究这样一些内容，他们关注一群人，关注社交网络，例如 Facebook， Google+，或者是其他的一些信息，比如说：你经常跟哪些人联系，而这些人又经常给哪些人发邮件，由此找到关系密切的人群。因此，这可能需要另一个聚类算法，你希望用它发现<b>社交网络</b>中关系密切的朋友。我有一个朋友正在研究这个问题，他希望使用聚类算法来更好的<b>组织计算机集群</b>，或者更好的管理数据中心。因为如果你知道数据中心中，那些计算机经常协作工作。那么，你可以重新分配资源，重新布局网络。由此优化数据中心，优化数据通信。<br/>      最后，我实际上还在研究如何利用<b>聚类算法了解星系的形成</b>。然后用这个知识，了解一些天文学上的细节问题。好的，这就是聚类算法。这将是我们介绍的第一个非监督学习算法。</p><p><b>1.2 K-均值算法(K-Means Algorithm)</b></p><p><b>1.2.1 介绍</b></p><p><b>K-均值是最普及的聚类算法，算法接受一个未标记的数据集，然后将数据聚类成不同的组。</b></p><p>      K-均值算法是一个迭代算法，原理也很简单易懂，为什么叫K？因为我们可以取K个随机点，也就是预先想要分的类别数。该算法的运作流程如下：</p><p>1.选择K个随机点作为<b>聚类中心(cluster centroids)</b>；</p><p>2.遍历数据集中的每个数据，算出每个数据到K个点的距离，将该点和距离最近的聚类中心聚成一类</p><p>3.计算出K个类别，每一类包含的数据点的均值，将该类的聚类中心点移动至均值所在的位置。这便是一次迭代过程</p><p>4.重复2~3，直至K个聚类中心都停止移动，完成迭代。</p><p>下面用图片的方式演示这一过程：</p><p>1.绿色为所有的数据点，这里取K = 2，即选出两个随机点作为聚类中心。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-d23e762e660548b01e82de2388c549c3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"679\" data-rawheight=\"483\" class=\"origin_image zh-lightbox-thumb\" width=\"679\" data-original=\"https://pic4.zhimg.com/v2-d23e762e660548b01e82de2388c549c3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;679&#39; height=&#39;483&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"679\" data-rawheight=\"483\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"679\" data-original=\"https://pic4.zhimg.com/v2-d23e762e660548b01e82de2388c549c3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-d23e762e660548b01e82de2388c549c3_b.jpg\"/></figure><p>2.遍历数据集中的每个数据点，算出其聚类红点和蓝点的距离，从而聚类到红色类或蓝色类</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-1e76822e296400c5348c1cbbb8e6897b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"693\" data-rawheight=\"512\" class=\"origin_image zh-lightbox-thumb\" width=\"693\" data-original=\"https://pic4.zhimg.com/v2-1e76822e296400c5348c1cbbb8e6897b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;693&#39; height=&#39;512&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"693\" data-rawheight=\"512\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"693\" data-original=\"https://pic4.zhimg.com/v2-1e76822e296400c5348c1cbbb8e6897b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-1e76822e296400c5348c1cbbb8e6897b_b.jpg\"/></figure><p>3.计算出红色和蓝色类别，每一类包含的数据点的均值，将该类的聚类中心点移动至均值所在的位置。 </p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-77455af301cb1e9f9242da5793b4844e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"687\" data-rawheight=\"508\" class=\"origin_image zh-lightbox-thumb\" width=\"687\" data-original=\"https://pic3.zhimg.com/v2-77455af301cb1e9f9242da5793b4844e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;687&#39; height=&#39;508&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"687\" data-rawheight=\"508\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"687\" data-original=\"https://pic3.zhimg.com/v2-77455af301cb1e9f9242da5793b4844e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-77455af301cb1e9f9242da5793b4844e_b.jpg\"/></figure><p>      4.重复迭代过程，直到<b>聚类中心点不再改变</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-4b1c176cfd8c4843d63ee759e217539f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"630\" data-rawheight=\"482\" class=\"origin_image zh-lightbox-thumb\" width=\"630\" data-original=\"https://pic4.zhimg.com/v2-4b1c176cfd8c4843d63ee759e217539f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;630&#39; height=&#39;482&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"630\" data-rawheight=\"482\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"630\" data-original=\"https://pic4.zhimg.com/v2-4b1c176cfd8c4843d63ee759e217539f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-4b1c176cfd8c4843d63ee759e217539f_b.jpg\"/></figure><p>      最后，这些散乱的样本点，便通过K-Means算法聚成了2类。</p><p><b>1.2.2 数学定义</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-83fc3b5f619349df492bf8daf27ca542_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"559\" data-rawheight=\"340\" class=\"origin_image zh-lightbox-thumb\" width=\"559\" data-original=\"https://pic3.zhimg.com/v2-83fc3b5f619349df492bf8daf27ca542_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;559&#39; height=&#39;340&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"559\" data-rawheight=\"340\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"559\" data-original=\"https://pic3.zhimg.com/v2-83fc3b5f619349df492bf8daf27ca542_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-83fc3b5f619349df492bf8daf27ca542_b.jpg\"/></figure><p>对于K-means算法，我们指定K个<b>聚类中心(cluster centroids)，</b>输入的样本点属于实数域，维度为m个。随机初始化K个聚类中心点 <img src=\"https://www.zhihu.com/equation?tex=%5Cmu_1%2C%5Cmu_2%2C...%5Cmu_K\" alt=\"\\mu_1,\\mu_2,...\\mu_K\" eeimg=\"1\"/> 然后重复迭代过程，如下图所示：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-1b137bb80d573c6a6281193cbeac1008_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"785\" data-rawheight=\"432\" class=\"origin_image zh-lightbox-thumb\" width=\"785\" data-original=\"https://pic1.zhimg.com/v2-1b137bb80d573c6a6281193cbeac1008_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;785&#39; height=&#39;432&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"785\" data-rawheight=\"432\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"785\" data-original=\"https://pic1.zhimg.com/v2-1b137bb80d573c6a6281193cbeac1008_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-1b137bb80d573c6a6281193cbeac1008_b.jpg\"/></figure><p>      式子里 <img src=\"https://www.zhihu.com/equation?tex=c%5E%7B%28i%29%7D\" alt=\"c^{(i)}\" eeimg=\"1\"/> 表示第i个样本点到第k个聚类中心的最小距离,不过按照惯例，用距离的平方来表示： <img src=\"https://www.zhihu.com/equation?tex=min_k%7C%7Cx%5E%7B%28i%29%7D-%5Cmu_k%7C%7C%5E2\" alt=\"min_k||x^{(i)}-\\mu_k||^2\" eeimg=\"1\"/> 。然后 <img src=\"https://www.zhihu.com/equation?tex=%5Cmu_k\" alt=\"\\mu_k\" eeimg=\"1\"/>的计算也很简单，即求所有 <img src=\"https://www.zhihu.com/equation?tex=c%5E%7B%28i%29%7D\" alt=\"c^{(i)}\" eeimg=\"1\"/> 的均值，就不写出来了。</p><p><b>1.3 优化目标</b></p><p>K-均值算法为何如此火热 ？因为其不仅对于有明显分类情况的数据有效，对于杂乱的堆在一起的数据同样有效，如下图：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-ece3dccf17565fb365b279c3e2410bf5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"815\" data-rawheight=\"444\" class=\"origin_image zh-lightbox-thumb\" width=\"815\" data-original=\"https://pic2.zhimg.com/v2-ece3dccf17565fb365b279c3e2410bf5_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;815&#39; height=&#39;444&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"815\" data-rawheight=\"444\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"815\" data-original=\"https://pic2.zhimg.com/v2-ece3dccf17565fb365b279c3e2410bf5_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-ece3dccf17565fb365b279c3e2410bf5_b.jpg\"/></figure><p>      左图中的数据，很明显是可以分为3类的，但右边的数据则堆在一起，不那么好分。假定数据的左边分别表示一个人的身高和体重，则我们T恤厂商想利用这些数据生产3类尺寸的衣服，即可利用K均值算法，舍得K = 3(S、M、L三类衣服)。将这些身高体重数据聚类成3类，从而指导生产。</p><p><b>1.3.1 畸变函数</b></p><p>      K-均值最小化问题，是要最小化所有的数据点与其所关联的聚类中心点之间的距离之和，因此 K-均值的损失函数（又称<b>畸变函数 Distortion function</b>）为：</p><p><img src=\"https://www.zhihu.com/equation?tex=J%28c%5E%7B%281%29%7D%2C..%2Cc%5E%7B%28m%29%7D%2C%5Cmu_1%2C..%2C%5Cmu_K%29+%3D++%5Cfrac%7B1%7D%7Bm%7D%5CSigma_%7B%28i%3D1%29%7D%5Em++%7C%7CX%5E%7B%28i%29%7D-%5Cmu_%7B%28c%5E%7B%28i%29%7D%29%7D%7C%7C%5E2\" alt=\"J(c^{(1)},..,c^{(m)},\\mu_1,..,\\mu_K) =  \\frac{1}{m}\\Sigma_{(i=1)}^m  ||X^{(i)}-\\mu_{(c^{(i)})}||^2\" eeimg=\"1\"/> </p><p>      其中 <img src=\"https://www.zhihu.com/equation?tex=%5Cmu_%7Bc%5E%7B%28i%29%7D%7D\" alt=\"\\mu_{c^{(i)}}\" eeimg=\"1\"/> 代表和 <img src=\"https://www.zhihu.com/equation?tex=x%5E%7B%28i%29%7D\" alt=\"x^{(i)}\" eeimg=\"1\"/> 最近的聚类中心点。我们的的优化目标便是找出使得代价函数最小的  <img src=\"https://www.zhihu.com/equation?tex=c%5E%7B%281%29%7D%2Cc%5E%7B%282%29%7D...c%5E%7B%28m%29%7D%E5%92%8C%5Cmu_1%2C%5Cmu_2%2C...%5Cmu_K\" alt=\"c^{(1)},c^{(2)}...c^{(m)}和\\mu_1,\\mu_2,...\\mu_K\" eeimg=\"1\"/> </p><p>      回顾刚才给出的: K-均值迭代算法， 我们知道，第一个循环是用于减小<b><i>c</i></b>引起的代价，而第二个循环则是用于减小<b><i>μ</i></b>引起的代价。迭代的过程一定会是每一次迭代都在减小代价函数，不然便是出现了错误。</p><p><b>1.4 随机初始化</b></p><p><b>1.4.1 初始化过程</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-dd6f9b065e0d515f3552c7d0b9b5f109_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"851\" data-rawheight=\"457\" class=\"origin_image zh-lightbox-thumb\" width=\"851\" data-original=\"https://pic2.zhimg.com/v2-dd6f9b065e0d515f3552c7d0b9b5f109_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;851&#39; height=&#39;457&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"851\" data-rawheight=\"457\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"851\" data-original=\"https://pic2.zhimg.com/v2-dd6f9b065e0d515f3552c7d0b9b5f109_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-dd6f9b065e0d515f3552c7d0b9b5f109_b.jpg\"/></figure><p>       在运行 K-均值算法的之前，我们首先要随机初始化所有的聚类中心点，即：1. 我们应该选择K &lt; m，这个很好理解，因为m为训练数据的个数，聚类的类别数永远不会超过样本个数。2.随机选择K个训练数据实例，并令这K个中心点 = K个数据实例</p><p><b>1.4.2 潜在的问题</b></p><p>      K-均值的一个问题在于，它有可能会停留在一个局部最小值处，而这取决于初始化的情况。即不同的随机初始化的聚类中心点导致的聚类结果可能不同，如下图：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-788f21dd835a38569f0119c44e29b4db_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"873\" data-rawheight=\"511\" class=\"origin_image zh-lightbox-thumb\" width=\"873\" data-original=\"https://pic4.zhimg.com/v2-788f21dd835a38569f0119c44e29b4db_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;873&#39; height=&#39;511&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"873\" data-rawheight=\"511\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"873\" data-original=\"https://pic4.zhimg.com/v2-788f21dd835a38569f0119c44e29b4db_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-788f21dd835a38569f0119c44e29b4db_b.jpg\"/></figure><p>      1.是全局最优的聚类、2.和3.则是比较失败的聚类，他们只收敛到了局部最优解，而没达到全局最优，因为1、2、3采用了不同的初始化聚类中心。</p><p><b>1.4.3 解决措施</b></p><p>      为了解决这个问题，我们通常需要多次运行 K-均值算法，每一次都重新进行随机初始化，最后再比较多次运行 K-均值的结果，选择代价函数最小的结果。这种方法在K较小的时候还是可行的，但是如果K较大，这么做也可能不会有明显地改善。</p><p><b>1.5 选择聚类数K</b></p><p><b>      没有所谓最好的选择聚类数的方法，通常是需要根据不同的问题，人工进行选择的。</b>选择的时候思考我们运用 K-均值算法聚类的动机是什么，然后选择能最好服务于该目的标聚类数。</p><p>      譬如上一个例子，服装厂为了生成SML三种型号的衣服，将聚类数K 设置为3</p><p>      当人们在讨论， 选择聚类数目的方法时， 有一个可能会谈及的方法叫作<b>“肘部法则”</b>。 关于“肘部法则”， 我们所需要做的是改变K值，也就是聚类类别数目的总数。我们用一个聚类来运行 K 均值聚类方法。这就意味着，所有的数据都会分到一个聚类里，然后计算成本函数或者计算畸变函数J 。 K代表聚类数字。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-910ae8295fb0386f0939f10ad8c019d3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"843\" data-rawheight=\"420\" class=\"origin_image zh-lightbox-thumb\" width=\"843\" data-original=\"https://pic4.zhimg.com/v2-910ae8295fb0386f0939f10ad8c019d3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;843&#39; height=&#39;420&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"843\" data-rawheight=\"420\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"843\" data-original=\"https://pic4.zhimg.com/v2-910ae8295fb0386f0939f10ad8c019d3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-910ae8295fb0386f0939f10ad8c019d3_b.jpg\"/></figure><p>      我们可能会得到一条类似于这样的曲线。 像一个人的肘部。 这就是“肘部法则”所做的，让我们来看这样一个图，看起来就好像有一个很清楚的肘在那儿。好像人的手臂，如果你伸出你的胳膊， 那么这就是你的肩关节、 肘关节、 手。 这就是“肘部法则”。 你会发现这种模式，它的畸变值会迅速下降，从 1 到 2，从 2 到 3 之后，你会在 3 的时候达到一个肘点。在此之后，畸变值就下降的非常慢，看起来就像使用 3 个聚类来进行聚类是正确的，这是因为那个点是曲线的肘点，畸变值下降得很快。</p><p><b>2. 降维(Dimensionality Reduction)</b></p><p>除了之前讨论的聚类，降维将是我们讨论的第二种非监督学习问题。使用降维的原因主要有：</p><ul><li><b>1.数据压缩</b></li><li><b>2.数据可视化</b></li></ul><p><b>2.1 数据压缩Data Compression</b></p><p>数据压缩不仅仅是压缩数据，节省计算机内存或磁盘空间，同时，也能加快我们的学习算法。</p><p><b>2.1.1 降维：2D -&gt; 1D</b></p><p>      本例中我们使用降维来展示一下，其在数据压缩方面的效果，假设数据集有2个特征：厘米、英寸，由于这两个特征维度都是表示长度的，即特征高度重合，故我们可以将其压缩一下，将维度从2维变为1维。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-71ce3d82708c0bde9cfc7b497d3432a8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"796\" data-rawheight=\"459\" class=\"origin_image zh-lightbox-thumb\" width=\"796\" data-original=\"https://pic1.zhimg.com/v2-71ce3d82708c0bde9cfc7b497d3432a8_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;796&#39; height=&#39;459&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"796\" data-rawheight=\"459\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"796\" data-original=\"https://pic1.zhimg.com/v2-71ce3d82708c0bde9cfc7b497d3432a8_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-71ce3d82708c0bde9cfc7b497d3432a8_b.jpg\"/></figure><p>      具体做法就是，画出一条如图中绿线所示的直线，将所有点投影至线上，这些点在新线上的特征我们可以用z表示，则2维的特征即转化为了1维的特征，很明显地，这次降维可以大幅压缩数据，理论上可以节约1半存储特征的磁盘空间。</p><p><b>2.1.2 降维：3D -&gt; 2D</b></p><p>      理论上，我们可以将降维拓展到任意维度，譬如将有1000个特征维降低到100个特征维度，即1000D -&gt; 100 D，不过，出于方便演示的效果，我们下面这个例子是从3维降维到2维：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-ac3f5da33b498c265953afd0db84062d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"903\" data-rawheight=\"500\" class=\"origin_image zh-lightbox-thumb\" width=\"903\" data-original=\"https://pic2.zhimg.com/v2-ac3f5da33b498c265953afd0db84062d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;903&#39; height=&#39;500&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"903\" data-rawheight=\"500\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"903\" data-original=\"https://pic2.zhimg.com/v2-ac3f5da33b498c265953afd0db84062d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-ac3f5da33b498c265953afd0db84062d_b.jpg\"/></figure><p>      图1是3维的特征点云，我们将其投影至一个由z1,z2表示的二维平面上，如图2所示。这时，3个维度消失了，我们可以用新的2维z1,z2来表示特征点。新的平面图如图三所示。</p><p><b>2.2 数据可视化 </b></p><p>      在许多及其学习问题中，如果我们能将数据可视化，我们便能寻找到一个更好的解决方案，降维可以帮助我们。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-0fefbb27bdd6023a9034f0ea855afb40_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"871\" data-rawheight=\"466\" class=\"origin_image zh-lightbox-thumb\" width=\"871\" data-original=\"https://pic1.zhimg.com/v2-0fefbb27bdd6023a9034f0ea855afb40_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;871&#39; height=&#39;466&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"871\" data-rawheight=\"466\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"871\" data-original=\"https://pic1.zhimg.com/v2-0fefbb27bdd6023a9034f0ea855afb40_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-0fefbb27bdd6023a9034f0ea855afb40_b.jpg\"/></figure><p>      假使我们有有关于许多不同国家的数据，每一个特征向量都有 50 个特征（如 GDP，人均 GDP，平均寿命等）。如果要将这个 50 维的数据可视化是不可能的。使用降维的方法将其降至 2 维，我们便可以将其可视化了。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-25786249c57fabc8ddb7a162e868c8cb_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"831\" data-rawheight=\"488\" class=\"origin_image zh-lightbox-thumb\" width=\"831\" data-original=\"https://pic4.zhimg.com/v2-25786249c57fabc8ddb7a162e868c8cb_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;831&#39; height=&#39;488&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"831\" data-rawheight=\"488\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"831\" data-original=\"https://pic4.zhimg.com/v2-25786249c57fabc8ddb7a162e868c8cb_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-25786249c57fabc8ddb7a162e868c8cb_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-9edc834c4e7d3b1cc044640256f87e76_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"838\" data-rawheight=\"459\" class=\"origin_image zh-lightbox-thumb\" width=\"838\" data-original=\"https://pic3.zhimg.com/v2-9edc834c4e7d3b1cc044640256f87e76_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;838&#39; height=&#39;459&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"838\" data-rawheight=\"459\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"838\" data-original=\"https://pic3.zhimg.com/v2-9edc834c4e7d3b1cc044640256f87e76_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-9edc834c4e7d3b1cc044640256f87e76_b.jpg\"/></figure><p>      这样做的问题在于，降维的算法只负责减少维数，新产生的特征的意义就必须由我们自己去发现了。</p><p><b>2.3 主成分分析算法</b></p><p>      降维和聚类一样，是无监督学习中的两类方法，<b>降维中最常用的算法—主成分分析Principal Component Analysis，简称PCA。</b>在 PCA 中，我们要做的是找到一个方向向量（Vector direction），当我们把所有的数据都投射到该向量上时，我们希望投射平均均方误差能尽可能地小。方向向量是一个经过原点的向量，而投射误差是从特征向量向该方向向量作垂线的长度。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b7be20050ad9f44ded1ef62534a7d475_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"815\" data-rawheight=\"470\" class=\"origin_image zh-lightbox-thumb\" width=\"815\" data-original=\"https://pic2.zhimg.com/v2-b7be20050ad9f44ded1ef62534a7d475_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;815&#39; height=&#39;470&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"815\" data-rawheight=\"470\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"815\" data-original=\"https://pic2.zhimg.com/v2-b7be20050ad9f44ded1ef62534a7d475_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-b7be20050ad9f44ded1ef62534a7d475_b.jpg\"/></figure><p><b>      下面给出主成分分析问题的描述：</b></p><p>      问题是要将n维数据降至k维，目标是找到向量 <img src=\"https://www.zhihu.com/equation?tex=u%5E%7B%281%29%7D%2Cu%5E%7B%282%29%7D...u%5E%7B%28k%29%7D\" alt=\"u^{(1)},u^{(2)}...u^{(k)}\" eeimg=\"1\"/> ,使得总的投射误差最小。</p><p><b>      主成分分析与线性回归的比较：</b></p><p>      主成分分析与线性回归看上去有些类似，实际上是两种不同的算法。主成分分析最小化的是<b>投射误差（Projected Error）</b>，而线性回归尝试的是最小化预测误差。线性回归的目的是预测结果，而主成分分析不作任何预测。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-dc3c9469d0e1127f2d64e885acad1dc2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"820\" data-rawheight=\"433\" class=\"origin_image zh-lightbox-thumb\" width=\"820\" data-original=\"https://pic3.zhimg.com/v2-dc3c9469d0e1127f2d64e885acad1dc2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;820&#39; height=&#39;433&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"820\" data-rawheight=\"433\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"820\" data-original=\"https://pic3.zhimg.com/v2-dc3c9469d0e1127f2d64e885acad1dc2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-dc3c9469d0e1127f2d64e885acad1dc2_b.jpg\"/></figure><p>      PCA 将n个特征降维到k个，可以用来进行数据压缩，如果 100 维的向量最后可以用 10维来表示，那么压缩率为 90%。同样图像处理领域的 KL 变换使用 PCA 做图像压缩。但 PCA要保证降维后，还要保证数据的特性损失最小。</p><p>      PCA 技术的一大好处是对数据进行降维的处理。 我们<b>可以对新求出的“主元”向量的重要性进行排序，根据需要取前面最重要的部分，将后面的维数省去，可以达到降维从而简化模型或是对数据进行压缩的效果。同时最大程度的保持了原有数据的信息。</b></p><p>      PCA 技术的一个很大的优点是，它是<b>完全无参数限制</b>的。在 PCA 的计算过程中完全不需要人为的设定参数或是根据任何经验模型对计算进行干预，最后的结果只与数据相关，与用户是独立的。但是，这一点同时也可以看作是缺点。如果用户对观测对象有一定的先验知识，掌握了数据的一些特征，却无法通过参数化等方法对处理过程进行干预，可能会得不到预期的效果，效率也不高。</p><p><b>利用PCA将特征矩阵从n维减少到k维的过程：</b></p><p>第一步:</p><p>      是均值归一化。我们需要计算出所有特征的均值，然后令 <img src=\"https://www.zhihu.com/equation?tex=x_j+%3D+x_j+-+%5Cmu_j\" alt=\"x_j = x_j - \\mu_j\" eeimg=\"1\"/> ，如果特征在不同数量级，任然需要进行特征缩放处理，即将其除以标准差 <img src=\"https://www.zhihu.com/equation?tex=%5Csigma%5E2\" alt=\"\\sigma^2\" eeimg=\"1\"/> </p><p>第二步：</p><p>      计算<b>协方差矩阵（covariance matrix）</b>协方差矩阵用符号<b>Σ</b>表示，</p><p><b>                                        Σ</b> = <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7Bm%7D%5CSigma%5En_%7Bi%3D1%7D%28x%5E%7B%28i%29%7D%29%28x%5E%7B%28i%29%7D%29%5ET\" alt=\"\\frac{1}{m}\\Sigma^n_{i=1}(x^{(i)})(x^{(i)})^T\" eeimg=\"1\"/> </p><p>第三步：</p><p>      是计算协方差矩阵<b>Σ</b>的特征向量（eigenvectors），在 Octave 里我们可以利用奇异值分解（singular value decomposition）来求解(过程略)</p><p><b>2.4 选择主成分数</b></p><p>      主成分分析算法中，我们需要将n纬特征转化为k纬新的特征，k即为主成分数量，这个数量怎么确定？这里我们需要了解两个概念：</p><p>      1. <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7Bm%7D%5CSigma_%7B%28i%3D1%29%7D%5Em++%7C%7Cx%5E%7B%28i%29%7D%7C%7C%5E2\" alt=\"\\frac{1}{m}\\Sigma_{(i=1)}^m  ||x^{(i)}||^2\" eeimg=\"1\"/> 代表训练集的方差</p><p><br/>      2. <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7Bm%7D%5CSigma_%7B%28i%3D1%29%7D%5Em++%7C%7Cx%5E%7B%28i%29%7D-x%5E%7B%28i%29%7D_%7B%28approx%29%7D%7C%7C%5E2\" alt=\"\\frac{1}{m}\\Sigma_{(i=1)}^m  ||x^{(i)}-x^{(i)}_{(approx)}||^2\" eeimg=\"1\"/> 代表投射的均方误差</p><p>      我们的目标是在满足均方误差/方差小于给定数如1%的情况下，选择尽可能小的k值，譬如从k = 1开始尝试，不满足再尝试k = 2....。这里比值&lt;1%,我们在PCA中可以用保留99%的方差性来表示，这个值用于描述新特征纬度对样本特征保留的度量。保留率越高越好。当然，有时候也不一定用1%，可能会用5%或者10%等。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-be42e2f37952babb2c03f989239e402f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"622\" data-rawheight=\"306\" class=\"origin_image zh-lightbox-thumb\" width=\"622\" data-original=\"https://pic4.zhimg.com/v2-be42e2f37952babb2c03f989239e402f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;622&#39; height=&#39;306&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"622\" data-rawheight=\"306\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"622\" data-original=\"https://pic4.zhimg.com/v2-be42e2f37952babb2c03f989239e402f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-be42e2f37952babb2c03f989239e402f_b.jpg\"/></figure><p><b>2.5 压缩重现</b></p><p>      什么是重建的压缩重现？<b>直观点说就是将已经压缩的很少的纬度还原，回到压缩前很多的纬度的近似。</b> </p><p><b>PCA </b>作为压缩算法。你可能需要把 1000 维的数据压缩 100 维特征，或具有三维数据压缩到一二维表示。所以，如果这是一个压缩算法，应该能回到这个压缩表示，回到你原有的高维数据的一种近似。所以，给定的𝑧(𝑖)，这可能100 维，怎么回到你原来的表示𝑥(𝑖)，这可能是 1000 维的数组? </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-bf3813c4ed2e0408f4c53501dc8d47ed_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"600\" data-rawheight=\"348\" class=\"origin_image zh-lightbox-thumb\" width=\"600\" data-original=\"https://pic2.zhimg.com/v2-bf3813c4ed2e0408f4c53501dc8d47ed_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;600&#39; height=&#39;348&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"600\" data-rawheight=\"348\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"600\" data-original=\"https://pic2.zhimg.com/v2-bf3813c4ed2e0408f4c53501dc8d47ed_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-bf3813c4ed2e0408f4c53501dc8d47ed_b.jpg\"/></figure><p>      如上左图，假设压缩算法将原先2纬特征的向量x1,x2压缩到了一纬z,那么怎么由z回到x1、x2的状态？此时，我们需要借助Ureduce。</p><p>      Xapprox = Ureduce . z,这里Ureduce是nxk的向量，乘以kx1的向量z，则可以近似得到n纬的向量X，n = 2是即可近似还原成二维的特征。如果平方投影误差不大的话，则这个二维特征就近似=x1和x2，如上右图。</p><p><b>2.6 应用建议</b></p><p>      假使我们正在针对一张 100×100 像素的图片进行某个计算机视觉的机器学习，即总共 有 10000 个特征。 </p><p>      1. 第一步是运用主要成分分析将数据压缩至 1000 个特征<br/>      2. 然后对训练集运行学习算法。<br/>      3. 在预测时，采用之前学习而来的𝑈𝑟𝑒𝑑𝑢𝑐𝑒将输入的特征𝑥转换成特征向量𝑧，然后再进行预测<br/>      注:如果我们有交叉验证集合测试集，也采用对训练集学习而来的𝑈𝑟𝑒𝑑𝑢𝑐𝑒。 错误的主要成分分析情况:一个常见错误使用主要成分分析的情况是，将其用于减少过 拟合(减少了特征的数量)。这样做非常不好，不如尝试正则化处理。原因在于主要成分分 析只是近似地丢弃掉一些特征，它并不考虑任何与结果变量有关的信息，因此可能会丢失非 常重要的特征。然而当我们进行正则化处理时，会考虑到结果变量，不会丢掉重要的数据。 </p><p>      另一个常见的错误是，默认地将主要成分分析作为学习过程中的一部分，这虽然很多时 候有效果，最好还是从所有原始特征开始，只在有必要的时候(算法运行太慢或者占用太多 内存)才考虑采用主要成分分析。 </p><p></p>", 
            "topic": [
                {
                    "tag": "吴恩达（Andrew Ng）", 
                    "tagLink": "https://api.zhihu.com/topics/20003150"
                }, 
                {
                    "tag": "聚类", 
                    "tagLink": "https://api.zhihu.com/topics/19590190"
                }, 
                {
                    "tag": "降维算法", 
                    "tagLink": "https://api.zhihu.com/topics/20687563"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/74764135", 
            "userName": "Lyon", 
            "userLink": "https://www.zhihu.com/people/38495a9b1f83e4a612543ce6c523df7f", 
            "upvote": 7, 
            "title": "【吴恩达机器学习】第七周—SVM支持向量机与核函数", 
            "content": "<p><b>前言：</b></p><p>最近在学习深度学习，看了不少教程，发现还是吴恩达的比较适用于我。吴恩达机器学习公开课视频最早是斯坦福大学的课程视频(那个画面有点老)，新版的视频在网易云课堂上可以随时学习。仅仅通过视频学习，可能会有点快，因为有的知识点需要反复推敲和回味。感谢github上一位朋友的精心整理，让我们可以配合讲义一起学习，讲义有HTML版、PDF版、Markdown版的：视频配合讲义看，事半功倍</p><p>本文包括接下来的系列文章，都会是吴恩达机器学习课程的个人学习总结，有很多文字和图片直接参考了PDF版的课程讲义，再次谢过提供讲义的朋友！</p><p>网易云课堂课程视频：</p><p><a href=\"https://link.zhihu.com/?target=https%3A//study.163.com/course/courseMain.htm%3FcourseId%3D1004570029\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">study.163.com/course/co</span><span class=\"invisible\">urseMain.htm?courseId=1004570029</span><span class=\"ellipsis\"></span></a></p><p>课程同步配套讲义：</p><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/fengdu78/Coursera-ML-AndrewNg-Notes\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/fengdu78/Cou</span><span class=\"invisible\">rsera-ML-AndrewNg-Notes</span><span class=\"ellipsis\"></span></a></p><p>原文发布在语雀文档上,效果更好看点：</p><a href=\"https://link.zhihu.com/?target=https%3A//www.yuque.com/docs/share/83cee9e2-6139-4bb2-a93c-060dec1d4711\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-72bbb1e26eacf07192c32d1b6514dde0_ipico.jpg\" data-image-width=\"512\" data-image-height=\"512\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">第七周 · 语雀</a><hr/><h2>1. 支持向量机Support Vector Machines</h2><h2>1.1 介绍</h2><p>      在分类问题中，除了线性的逻辑回归模型和非线性的深度神经网络外，我们还可以应用一种被广泛应用于工业界和学术界的模型—支持向量机，简称SVM，与逻辑回归和神经网络相比，支持向量机在学习复杂的非线性方程时提供了一种更为清晰，更加强大的方式。</p><p>      尽管现在深度学习十分流行，了解支持向量机的原理，对想法的形式化、简化、及一步步使模型更一般化的过程，及其具体实现仍然有其研究价值。另一方面，支持向量机仍有其一席之地。相比深度神经网络，支持向量机特别擅长于特征维数多于样本数的情况，而小样本学习至今仍是深度学习的一大难题。</p><p>关于支持向量机的简单概念和定义，请参考这篇文章：                                                                                         <a href=\"https://www.zhihu.com/question/21094489/answer/190046611\" class=\"internal\"><span class=\"invisible\">https://www.</span><span class=\"visible\">zhihu.com/question/2109</span><span class=\"invisible\">4489/answer/190046611</span><span class=\"ellipsis\"></span></a></p><p>更详细地了解和推导SVM，请参考以下几篇文章：</p><p><a href=\"https://zhuanlan.zhihu.com/p/40857202\" class=\"internal\"><span class=\"invisible\">https://</span><span class=\"visible\">zhuanlan.zhihu.com/p/40</span><span class=\"invisible\">857202</span><span class=\"ellipsis\"></span></a> </p><p><a href=\"https://zhuanlan.zhihu.com/p/31652569\" class=\"internal\"><span class=\"invisible\">https://</span><span class=\"visible\">zhuanlan.zhihu.com/p/31</span><span class=\"invisible\">652569</span><span class=\"ellipsis\"></span></a> </p><h2>1.2 从逻辑回归到SVM</h2><p>下面，我们利用逻辑回归模型，建立简单的支持向量机，来进行对比和讲解。</p><h3>1.2.1 假设函数</h3><p>逻辑回归假设函数： <img src=\"https://www.zhihu.com/equation?tex=h_%5Ctheta%28x%29+%3D+g%28%5Ctheta%5ETX%29+%3D+%5Cfrac%7B1%7D%7B1%2Be%5E%7B-%5Ctheta%5ETX%7D%7D\" alt=\"h_\\theta(x) = g(\\theta^TX) = \\frac{1}{1+e^{-\\theta^TX}}\" eeimg=\"1\"/> ，函数图像如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-427e76fc032dcc36ea46475253a32c71_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"356\" data-rawheight=\"293\" class=\"content_image\" width=\"356\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;356&#39; height=&#39;293&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"356\" data-rawheight=\"293\" class=\"content_image lazy\" width=\"356\" data-actualsrc=\"https://pic2.zhimg.com/v2-427e76fc032dcc36ea46475253a32c71_b.jpg\"/></figure><p>对于二元分类，目标值只能取2种：y = 1、y = 0；我们对假设函数的希望是，能够尽量准确地对样本进行分类，即：</p><p>y = 1时，我们希望z远大于0， <img src=\"https://www.zhihu.com/equation?tex=h_%5Ctheta%28x%29+%5Capprox+1%EF%BC%8C%5Ctheta%5ET+x+%5Cgg0\" alt=\"h_\\theta(x) \\approx 1，\\theta^T x \\gg0\" eeimg=\"1\"/> </p><p>y = 0时，我们希望z远小于0， <img src=\"https://www.zhihu.com/equation?tex=h_%5Ctheta%28x%29+%5Capprox+0%EF%BC%8C%5Ctheta%5ET+x+%5Cll+0\" alt=\"h_\\theta(x) \\approx 0，\\theta^T x \\ll 0\" eeimg=\"1\"/> </p><h3>1.2.2 损失函数</h3><p>逻辑回归中的总损失函数： <img src=\"https://www.zhihu.com/equation?tex=J%28%5Ctheta%29%3D+%5Cfrac%7B1%7D%7Bm%7D%5CSigma%5Em_%7Bi%3D1%7DCost%28h_%5Ctheta%28x%5E%7B%28i%29%7D%29+%2C+y%5E%7B%28i%29%7D%29+%3D+-%5Cfrac%7B1%7D%7Bm%7D%5CSigma%5Em_%7Bi%3D1%7D%5By%5E%7B%28i%29%7D%2Alog%28h_%5Ctheta%28x%5E%7B%28i%29%7D%29%29%2B%281-y%5E%7B%28i%29%7D%29%2Alog%281-h_%5Ctheta%28x%5E%7B%28i%29%7D%29%29%5D\" alt=\"J(\\theta)= \\frac{1}{m}\\Sigma^m_{i=1}Cost(h_\\theta(x^{(i)}) , y^{(i)}) = -\\frac{1}{m}\\Sigma^m_{i=1}[y^{(i)}*log(h_\\theta(x^{(i)}))+(1-y^{(i)})*log(1-h_\\theta(x^{(i)}))]\" eeimg=\"1\"/> </p><p>其中，每个训练集样本点的损失：</p><p><img src=\"https://www.zhihu.com/equation?tex=Cost%28h_%5Ctheta%28x%29+%2C+y%29+%3D+++-y%2Alog%28h_%5Ctheta%28x%29%29-%281-y%29%2Alog%281-h_%5Ctheta%28x%29%29\" alt=\"Cost(h_\\theta(x) , y) =   -y*log(h_\\theta(x))-(1-y)*log(1-h_\\theta(x))\" eeimg=\"1\"/> </p><p>当y = 1时，我们得到损失的表达式： <img src=\"https://www.zhihu.com/equation?tex=-y%2Alog%28h_%5Ctheta%28x%29%29+%3D+-1%2Alog%5Cfrac%7B1%7D%7B1%2Be%5E%7B%28-%5Ctheta%5ETx%29%7D%7D+%3D+-log%5Cfrac%7B1%7D%7B1%2Be%5E%7B%28-z%29%7D%7D\" alt=\"-y*log(h_\\theta(x)) = -1*log\\frac{1}{1+e^{(-\\theta^Tx)}} = -log\\frac{1}{1+e^{(-z)}}\" eeimg=\"1\"/> </p><p>当y = 0时，我们得到： <img src=\"https://www.zhihu.com/equation?tex=-%281-y%29%2Alog%281-h_%5Ctheta%28x%29%29+%3D+-log%281-%5Cfrac%7B1%7D%7B1%2Be%5E%7B%28-z%29%7D%7D%29\" alt=\"-(1-y)*log(1-h_\\theta(x)) = -log(1-\\frac{1}{1+e^{(-z)}})\" eeimg=\"1\"/> </p><p>我们可以画出损失Cost和变量z之间的关系，如下图:</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-ea2f0019529a08c2034ee05a13c17471_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"807\" data-rawheight=\"518\" class=\"origin_image zh-lightbox-thumb\" width=\"807\" data-original=\"https://pic2.zhimg.com/v2-ea2f0019529a08c2034ee05a13c17471_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;807&#39; height=&#39;518&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"807\" data-rawheight=\"518\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"807\" data-original=\"https://pic2.zhimg.com/v2-ea2f0019529a08c2034ee05a13c17471_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-ea2f0019529a08c2034ee05a13c17471_b.jpg\"/></figure><p>现在，我们将建立支持向量机，在此处，即画出线段，用于分割二维平面。</p><p>如图，对于左图，向量机建立如下：</p><p>以z = 1为端点，往左画出一条紧紧贴和函数曲线的直线，往右，画出一条平行z轴的直线，2条线交与z = 1这点这两条直线构成了一个新的界限，我们命名此函数为 <img src=\"https://www.zhihu.com/equation?tex=cost_1%28z%29\" alt=\"cost_1(z)\" eeimg=\"1\"/> ；</p><p>对于右图，类似地，画出两条线，不过这次以z  = -1为交点。命名新的函数为 <img src=\"https://www.zhihu.com/equation?tex=cost_0%28z%29\" alt=\"cost_0(z)\" eeimg=\"1\"/> </p><h3>1.2.3 SVM数学定义</h3><p>逻辑回归中，损失函数如下： <img src=\"https://www.zhihu.com/equation?tex=J%28%5Ctheta%29%3D+%5Cfrac%7B1%7D%7Bm%7D%5CSigma%5Em_%7Bi%3D1%7D%5B-y%5E%7B%28i%29%7D%2Alog%28h_%5Ctheta%28x%5E%7B%28i%29%7D%29%29-%281-y%5E%7B%28i%29%7D%29%2Alog%281-h_%5Ctheta%28x%5E%7B%28i%29%7D%29%29%5D+%2B+%5Cfrac%7B%5Clambda%7D%7B2m%7D%5CSigma%5E%7B%7Bn%7D%7D_%7Bi%3D1%7D%5Ctheta_j%5E2\" alt=\"J(\\theta)= \\frac{1}{m}\\Sigma^m_{i=1}[-y^{(i)}*log(h_\\theta(x^{(i)}))-(1-y^{(i)})*log(1-h_\\theta(x^{(i)}))] + \\frac{\\lambda}{2m}\\Sigma^{{n}}_{i=1}\\theta_j^2\" eeimg=\"1\"/> </p><p>我们在1.2.2中建立了支持向量机，用 <img src=\"https://www.zhihu.com/equation?tex=cost_1%28z%29\" alt=\"cost_1(z)\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=cost_0%28z%29\" alt=\"cost_0(z)\" eeimg=\"1\"/> 替换了原来的逻辑回归，故此时的损失函数如下：</p><p><img src=\"https://www.zhihu.com/equation?tex=J%28%5Ctheta%29%3D+%5Cfrac%7B1%7D%7Bm%7D%5CSigma%5Em_%7Bi%3D1%7D%5By%5E%7B%28i%29%7Dcost_1%28z%29%2B%281-y%5E%7B%28i%29%7D%29cost_0%28z%29%5D+%2B+%5Cfrac%7B%5Clambda%7D%7B2m%7D%5CSigma%5E%7B%7Bn%7D%7D_%7Bi%3D1%7D%5Ctheta_j%5E2\" alt=\"J(\\theta)= \\frac{1}{m}\\Sigma^m_{i=1}[y^{(i)}cost_1(z)+(1-y^{(i)})cost_0(z)] + \\frac{\\lambda}{2m}\\Sigma^{{n}}_{i=1}\\theta_j^2\" eeimg=\"1\"/> </p><p>经过进一步简化，我们可以得到SVM中的损失函数：</p><p><img src=\"https://www.zhihu.com/equation?tex=J%28%5Ctheta%29%3D+C%2A%5CSigma%5Em_%7Bi%3D1%7D%5By%5E%7B%28i%29%7Dcost_1%28%5Ctheta%5ETx%5E%7B%28i%29%7D%29%2B%281-y%5E%7B%28i%29%7D%29cost_0%28%5Ctheta%5ETx%5E%7B%28i%29%7D%29%5D+%2B+%5Cfrac%7B1%7D%7B2%7D%5CSigma%5E%7B%7Bn%7D%7D_%7Bi%3D1%7D%5Ctheta_j%5E2\" alt=\"J(\\theta)= C*\\Sigma^m_{i=1}[y^{(i)}cost_1(\\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\\theta^Tx^{(i)})] + \\frac{1}{2}\\Sigma^{{n}}_{i=1}\\theta_j^2\" eeimg=\"1\"/> </p><p>这里C = 1/λ，去除了1/m，并不影响min(J(θ))这个目标的达成。</p><p>支持向量机的数学定义：</p><p><img src=\"https://www.zhihu.com/equation?tex=min+C%2A%5CSigma%5Em_%7Bi%3D1%7D%5By%5E%7B%28i%29%7Dcost_1%28%5Ctheta%5ETx%5E%7B%28i%29%7D%29%2B%281-y%5E%7B%28i%29%7D%29cost_0%28%5Ctheta%5ETx%5E%7B%28i%29%7D%29%5D+%2B+%5Cfrac%7B1%7D%7B2%7D%5CSigma%5E%7B%7Bn%7D%7D_%7Bi%3D1%7D%5Ctheta_j%5E2\" alt=\"min C*\\Sigma^m_{i=1}[y^{(i)}cost_1(\\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\\theta^Tx^{(i)})] + \\frac{1}{2}\\Sigma^{{n}}_{i=1}\\theta_j^2\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=h_%5Ctheta%28x%29%3D%5Cbegin%7Bcases%7D+1++%28%5Ctheta%5ETx+%5Cge+0%29%5C%5C%5B2ex%5D+0%28Other%29+%5Cend%7Bcases%7D\" alt=\"h_\\theta(x)=\\begin{cases} 1  (\\theta^Tx \\ge 0)\\\\[2ex] 0(Other) \\end{cases}\" eeimg=\"1\"/> </p><p>在这个例子中，我们可以看到：</p><p>1.这里的假设函数直接输出的是值0或1，而不是逻辑回归中的概率值。</p><p>2.支持向量机的定义类似损失函数，不过比损失函数更进一步，因为其要求min将损失最小化。</p><h2>1.3 大间距的直观理解</h2><p>      有时候，人们会将把SVM叫做<b>大间距分类器</b>，为什么支持向量机被称为大间距分类器 ？这一小节将直观地介绍其中的含义。我们回顾一下支持向量机的模型定义：</p><p><img src=\"https://www.zhihu.com/equation?tex=min+C%2A%5CSigma%5Em_%7Bi%3D1%7D%5By%5E%7B%28i%29%7Dcost_1%28%5Ctheta%5ETx%5E%7B%28i%29%7D%29%2B%281-y%5E%7B%28i%29%7D%29cost_0%28%5Ctheta%5ETx%5E%7B%28i%29%7D%29%5D+%2B+%5Cfrac%7B1%7D%7B2%7D%5CSigma%5E%7B%7Bn%7D%7D_%7Bi%3D1%7D%5Ctheta_j%5E2\" alt=\"min C*\\Sigma^m_{i=1}[y^{(i)}cost_1(\\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\\theta^Tx^{(i)})] + \\frac{1}{2}\\Sigma^{{n}}_{i=1}\\theta_j^2\" eeimg=\"1\"/> </p><p>我们的目标是最小化这个方程，下面我们结合图像说明最小化的过程：   </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-5799e01372d2f827973a06d67e080170_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"809\" data-rawheight=\"429\" class=\"origin_image zh-lightbox-thumb\" width=\"809\" data-original=\"https://pic1.zhimg.com/v2-5799e01372d2f827973a06d67e080170_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;809&#39; height=&#39;429&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"809\" data-rawheight=\"429\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"809\" data-original=\"https://pic1.zhimg.com/v2-5799e01372d2f827973a06d67e080170_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-5799e01372d2f827973a06d67e080170_b.jpg\"/></figure><h3>1.3.1 参数C很大时</h3><p><b>这里我们假设参数C很大</b>，此时最小化方程式的重点放在左半部分，而可以&#34;忽略&#34;右边的 <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7B2%7D%5CSigma%5E%7B%7Bn%7D%7D_%7Bi%3D1%7D%5Ctheta_j%5E2\" alt=\"\\frac{1}{2}\\Sigma^{{n}}_{i=1}\\theta_j^2\" eeimg=\"1\"/> 。所以我们的优化目标主要放在让左边式子为0上：</p><p>1.当样本为正,y  = 1,则根据 <img src=\"https://www.zhihu.com/equation?tex=cost_1%28z%29\" alt=\"cost_1(z)\" eeimg=\"1\"/> 的函数图像，我们希望 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta%5ETx+%5Cge+1\" alt=\"\\theta^Tx \\ge 1\" eeimg=\"1\"/> ,因为此时： <img src=\"https://www.zhihu.com/equation?tex=y%5E%7B%28i%29%7Dcost_1%28%5Ctheta%5ETx%5E%7B%28i%29%7D%29+%3D+1%2A0+%3D+0\" alt=\"y^{(i)}cost_1(\\theta^Tx^{(i)}) = 1*0 = 0\" eeimg=\"1\"/> ，我们可以使得方程最小化。</p><p>2.当样本为负时，即y = 0,则根据 <img src=\"https://www.zhihu.com/equation?tex=cost_0%28z%29\" alt=\"cost_0(z)\" eeimg=\"1\"/> 函数图像，此时，我们希望 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta%5ETx+%5Cle+-1\" alt=\"\\theta^Tx \\le -1\" eeimg=\"1\"/> ,因为此时： <img src=\"https://www.zhihu.com/equation?tex=%281-y%5E%7B%28i%29%7D%29cost_0%28%5Ctheta%5ETx%5E%7B%28i%29%7D%29%5D+%3D+1%2A0+%3D+0\" alt=\"(1-y^{(i)})cost_0(\\theta^Tx^{(i)})] = 1*0 = 0\" eeimg=\"1\"/> </p><p>不过，我们注意到，如果一个样本满足y = 1时， <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta%5ETx+%5Cge+0\" alt=\"\\theta^Tx \\ge 0\" eeimg=\"1\"/> 即可使模型能将其准确分类，对于负样本y = 0同样只需要 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta%5ETx+%5Cle+0\" alt=\"\\theta^Tx \\le 0\" eeimg=\"1\"/> 即可。那么支持向量机里，<b>追求的最优化方程究竟会带来什么？</b></p><p>我们将样本点x在坐标系中向量化表示,即x是一条从原点开始，指向(x1,x2)的矢量，x的模长 = x1^2 + x2^2，则我们可以得到一系列样本点坐标图，如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-eb9835962692ca9177ae65d5d4f527c5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"419\" data-rawheight=\"336\" class=\"content_image\" width=\"419\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;419&#39; height=&#39;336&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"419\" data-rawheight=\"336\" class=\"content_image lazy\" width=\"419\" data-actualsrc=\"https://pic2.zhimg.com/v2-eb9835962692ca9177ae65d5d4f527c5_b.jpg\"/></figure><p><b>追求最优化方程究竟会带来什么？</b></p><p>答：会带来一条支持向量机的超平面，在二维方程中，超平面即一条直线，我们会得到一条直线，将样本点分割开来，且这条直线满足：<img src=\"https://www.zhihu.com/equation?tex=min+C%2A%5CSigma%5Em_%7Bi%3D1%7D%5By%5E%7B%28i%29%7Dcost_1%28%5Ctheta%5ETx%5E%7B%28i%29%7D%29%2B%281-y%5E%7B%28i%29%7D%29cost_0%28%5Ctheta%5ETx%5E%7B%28i%29%7D%29%5D+%2B+%5Cfrac%7B1%7D%7B2%7D%5CSigma%5E%7B%7Bn%7D%7D_%7Bi%3D1%7D%5Ctheta_j%5E2\" alt=\"min C*\\Sigma^m_{i=1}[y^{(i)}cost_1(\\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\\theta^Tx^{(i)})] + \\frac{1}{2}\\Sigma^{{n}}_{i=1}\\theta_j^2\" eeimg=\"1\"/> ，即将这个表达式的值取到最小：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-997acf42be566b428582b149a5a21256_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"423\" data-rawheight=\"344\" class=\"origin_image zh-lightbox-thumb\" width=\"423\" data-original=\"https://pic3.zhimg.com/v2-997acf42be566b428582b149a5a21256_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;423&#39; height=&#39;344&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"423\" data-rawheight=\"344\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"423\" data-original=\"https://pic3.zhimg.com/v2-997acf42be566b428582b149a5a21256_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-997acf42be566b428582b149a5a21256_b.jpg\"/></figure><p>      这里，在样本点中，我们可以划出一条满足支持向量机方程的直线：黑色线。这条划分样本点的线在SVM中也叫<b>决策边界(SVM Decision Boundary)</b>。这条黑线不仅满足数学表达式上的最小属性，从图像上看，黑色线也能满足和样本间的最大距离，这里的最大是指总体最大，这也是支持向量机被称为<b>大间距分类器(Large margin classifier)</b>的缘由。</p><blockquote>当然，当样本上升到3维、4维、N维时，支持向量机就表示一个平面、多维超平面，而不仅仅是一条线。但是同样会满足大间距分类器这样的含义。即保持到样本点间的最大距离。</blockquote><p>      这里需要说明的是：决策边界可以划出n条，如图中的粉色、绿色、黑色.....但满足最小化方程式的值的边界只有一条，这条边界被称为支持向量机。</p><h3>1.3.2 参数C较小时</h3><p>      当然，我们上面都是基于假设参数C很大时的情况，那如果C不是很大时，我们就不仅仅要考虑方程式中左边的项，还需要同步考虑右边 <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7B2%7D%5CSigma%5E%7B%7Bn%7D%7D_%7Bi%3D1%7D%5Ctheta_j%5E2\" alt=\"\\frac{1}{2}\\Sigma^{{n}}_{i=1}\\theta_j^2\" eeimg=\"1\"/> 项了。下面我们再看一个例子：</p><p>当C很大时，我们的支持向量机画出了一条决策边界：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-144fa10d0eb658ab00a597441adc620e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"458\" data-rawheight=\"325\" class=\"origin_image zh-lightbox-thumb\" width=\"458\" data-original=\"https://pic3.zhimg.com/v2-144fa10d0eb658ab00a597441adc620e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;458&#39; height=&#39;325&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"458\" data-rawheight=\"325\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"458\" data-original=\"https://pic3.zhimg.com/v2-144fa10d0eb658ab00a597441adc620e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-144fa10d0eb658ab00a597441adc620e_b.jpg\"/></figure><p>此时，又新增一个样本点(位于图中左下角靠近原点)，那么为了继续满足支持向量机方程式的定义，我们的决策边界变更了，如图中的粉线：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-424a93da3cd3242d2bbd912e3e450ce3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"425\" data-rawheight=\"332\" class=\"origin_image zh-lightbox-thumb\" width=\"425\" data-original=\"https://pic4.zhimg.com/v2-424a93da3cd3242d2bbd912e3e450ce3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;425&#39; height=&#39;332&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"425\" data-rawheight=\"332\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"425\" data-original=\"https://pic4.zhimg.com/v2-424a93da3cd3242d2bbd912e3e450ce3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-424a93da3cd3242d2bbd912e3e450ce3_b.jpg\"/></figure><p>      但是，需要注意，仅仅由一个样本点导致的决策边界发生大幅改变，是不明智的。此时我可以将原本很大的C变小，这样，我们考虑的就不仅仅是左边的 <img src=\"https://www.zhihu.com/equation?tex=C%2A%5CSigma%5Em_%7Bi%3D1%7D%5By%5E%7B%28i%29%7Dcost_1%28%5Ctheta%5ETx%5E%7B%28i%29%7D%29%2B%281-y%5E%7B%28i%29%7D%29cost_0%28%5Ctheta%5ETx%5E%7B%28i%29%7D%29%5D\" alt=\"C*\\Sigma^m_{i=1}[y^{(i)}cost_1(\\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\\theta^Tx^{(i)})]\" eeimg=\"1\"/> ，因为C很小的化，这个式子的乘积就会变得较小，对于整个方程式的最小化影响会降低，这时，支持向量机的决策边界就会忽略掉一些异常点影响，即决策界还是会保持在黑线上，而不会划出粉线。因为支持向量机要保证的是总体方程式最小化。</p><h3>1.3.3 关于参数C</h3><p>      回顾之前的表达式可知，参数<i><b>C = 1/λ </b></i>当C较大时，对应λ较小，即<b>正则化参数较小</b>，可能会导致<b>过拟合</b>，和支持向量机的<b>高方差；</b>当C较小时，对应λ较大，可能导致<b>欠拟合</b>(拟合不够)，和支持向量机的<b>高偏差</b></p><h2>1.4 大间距的数学原理</h2><h3>1.4.1 向量内积 </h3><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-255d797f722997f16fa2a42eec09fc95_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"888\" data-rawheight=\"539\" class=\"origin_image zh-lightbox-thumb\" width=\"888\" data-original=\"https://pic2.zhimg.com/v2-255d797f722997f16fa2a42eec09fc95_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;888&#39; height=&#39;539&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"888\" data-rawheight=\"539\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"888\" data-original=\"https://pic2.zhimg.com/v2-255d797f722997f16fa2a42eec09fc95_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-255d797f722997f16fa2a42eec09fc95_b.jpg\"/></figure><p>为了方便举例，此处以二维向量举例。u、v都是二维向量，它们的内积： <img src=\"https://www.zhihu.com/equation?tex=u%5ETv+%3D+u_1v_1+%2B+u_2v_2+%3D+v%5ETu\" alt=\"u^Tv = u_1v_1 + u_2v_2 = v^Tu\" eeimg=\"1\"/> </p><p>内积的含义在哪里 ？ 图中我们可以用投影和范数(在欧几里得范数中即 = 模长)来表示：</p><p><img src=\"https://www.zhihu.com/equation?tex=u%5ETv+%3D+p.%7C%7Cu%7C%7C+%28p%5Cin+R%29\" alt=\"u^Tv = p.||u|| (p\\in R)\" eeimg=\"1\"/> </p><p><b>用文字表示：u和v的内积 = 向量v在向量u上的投影乘以向量向量u的范数（或者反过来表示也一样） </b>这里需要注意，如图中的第二个图所展示的：当向量u和v角度&gt;90°时，p值为负。</p><h3>1.4.2 SVM的数学原理</h3><p>      之前支持向量机的方程，写作：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-32622dad86df86c87293167b7a512c78_b.jpg\" data-size=\"normal\" data-rawwidth=\"430\" data-rawheight=\"268\" class=\"origin_image zh-lightbox-thumb\" width=\"430\" data-original=\"https://pic1.zhimg.com/v2-32622dad86df86c87293167b7a512c78_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;430&#39; height=&#39;268&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"430\" data-rawheight=\"268\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"430\" data-original=\"https://pic1.zhimg.com/v2-32622dad86df86c87293167b7a512c78_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-32622dad86df86c87293167b7a512c78_b.jpg\"/><figcaption>在数学里面s.t.是subject to 的缩写，意为：使得...满足</figcaption></figure><p>这里，为了方便说吗，简化一下令 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_0+%3D+0%2Cn%3D2\" alt=\"\\theta_0 = 0,n=2\" eeimg=\"1\"/> 。此时， <img src=\"https://www.zhihu.com/equation?tex=min%5Cfrac%7B1%7D%7B2%7D%5CSigma%5Ctheta%5E2_j+%3D+%5Cfrac%7B1%7D%7B2%7D%28%5Ctheta%5E2_1%2B%5Ctheta%5E2_2%29\" alt=\"min\\frac{1}{2}\\Sigma\\theta^2_j = \\frac{1}{2}(\\theta^2_1+\\theta^2_2)\" eeimg=\"1\"/> ，此时 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_1%E5%92%8C%5Ctheta_2\" alt=\"\\theta_1和\\theta_2\" eeimg=\"1\"/> 可以看作是向量 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 的两个分量，则有 <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7B2%7D%28%5Ctheta%5E2_1%2B%5Ctheta%5E2_2%29+%3D+%5Cfrac%7B1%7D%7B2%7D+%5Csqrt%7B%28%5Ctheta%5E2_1%2B%5Ctheta%5E2_2%29%7D%5E2+%3D++%5Cfrac%7B1%7D%7B2%7D%7C%7C%5Ctheta%7C%7C%5E2\" alt=\"\\frac{1}{2}(\\theta^2_1+\\theta^2_2) = \\frac{1}{2} \\sqrt{(\\theta^2_1+\\theta^2_2)}^2 =  \\frac{1}{2}||\\theta||^2\" eeimg=\"1\"/> ；对于 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta%5ET+x%5E%7B%28i%29%7D\" alt=\"\\theta^T x^{(i)}\" eeimg=\"1\"/> ，可以将其看作是 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta%E5%92%8Cx%5E%7B%28i%29%7D\" alt=\"\\theta和x^{(i)}\" eeimg=\"1\"/> 的内积， <img src=\"https://www.zhihu.com/equation?tex=p\" alt=\"p\" eeimg=\"1\"/> 表示 <img src=\"https://www.zhihu.com/equation?tex=x%5E%7B%28i%29%7D%E5%9C%A8%5Ctheta\" alt=\"x^{(i)}在\\theta\" eeimg=\"1\"/> 上的投影，则向量关系示意图如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-8dd3b5f7d495e725b18c217294ee83d5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"347\" data-rawheight=\"241\" class=\"content_image\" width=\"347\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;347&#39; height=&#39;241&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"347\" data-rawheight=\"241\" class=\"content_image lazy\" width=\"347\" data-actualsrc=\"https://pic2.zhimg.com/v2-8dd3b5f7d495e725b18c217294ee83d5_b.jpg\"/></figure><p>整个表达式可以转化为如下形式：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-691a9c8ee9b603c122d5dec98127dd46_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"617\" data-rawheight=\"227\" class=\"origin_image zh-lightbox-thumb\" width=\"617\" data-original=\"https://pic3.zhimg.com/v2-691a9c8ee9b603c122d5dec98127dd46_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;617&#39; height=&#39;227&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"617\" data-rawheight=\"227\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"617\" data-original=\"https://pic3.zhimg.com/v2-691a9c8ee9b603c122d5dec98127dd46_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-691a9c8ee9b603c122d5dec98127dd46_b.jpg\"/></figure><p>这时再看一个例子：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-2abbf2f014f50aa6932d767828d4a845_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"869\" data-rawheight=\"307\" class=\"origin_image zh-lightbox-thumb\" width=\"869\" data-original=\"https://pic2.zhimg.com/v2-2abbf2f014f50aa6932d767828d4a845_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;869&#39; height=&#39;307&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"869\" data-rawheight=\"307\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"869\" data-original=\"https://pic2.zhimg.com/v2-2abbf2f014f50aa6932d767828d4a845_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-2abbf2f014f50aa6932d767828d4a845_b.jpg\"/></figure><p>      左图种的绿线是支持向量机的一种决策边界，我们称为A，右图的绿线是另一种决策边界B，支持向量机会更倾向于选择哪种决策边界呢？ 答案是B</p><p>       因为根据公式，支持向量机需要最小化θ的范数，即 <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7B2%7D%28%5Ctheta%5E2_1%2B%5Ctheta%5E2_2%29+%3D+%5Cfrac%7B1%7D%7B2%7D+%5Csqrt%7B%28%5Ctheta%5E2_1%2B%5Ctheta%5E2_2%29%7D%5E2+%3D++%5Cfrac%7B1%7D%7B2%7D%7C%7C%5Ctheta%7C%7C%5E2\" alt=\"\\frac{1}{2}(\\theta^2_1+\\theta^2_2) = \\frac{1}{2} \\sqrt{(\\theta^2_1+\\theta^2_2)}^2 =  \\frac{1}{2}||\\theta||^2\" eeimg=\"1\"/> 取到最小。</p><p>      为了满足 <img src=\"https://www.zhihu.com/equation?tex=p%5E%7B%28i%29%7D.%7C%7C%5Ctheta%7C%7C+%3E%3D1\" alt=\"p^{(i)}.||\\theta|| &gt;=1\" eeimg=\"1\"/> 的条件，当||θ||取值越小时，向量机希望 <img src=\"https://www.zhihu.com/equation?tex=p%5E%7B%28i%29%7D\" alt=\"p^{(i)}\" eeimg=\"1\"/> 越大，即 <img src=\"https://www.zhihu.com/equation?tex=x%5E%7B%28i%29%7D%E5%9C%A8%5Ctheta\" alt=\"x^{(i)}在\\theta\" eeimg=\"1\"/> 上的投影越大。且 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 和绿色的决策边界垂直，很明显可以看出，当决策边界为B时，投影越大，即 <img src=\"https://www.zhihu.com/equation?tex=p%5E%7B%28i%29%7D\" alt=\"p^{(i)}\" eeimg=\"1\"/> 足够大，我们可以取到更小的 <img src=\"https://www.zhihu.com/equation?tex=%7C%7C%5Ctheta%7C%7C\" alt=\"||\\theta||\" eeimg=\"1\"/> 。</p><h2>1.5 核函数</h2><p>回顾我们之前讨论过可以使用高级数的多项式模型来解决无法用直线进行分隔的分类问题: </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-29b3bd8dce69b45b8e8f98d2592105b1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"386\" data-rawheight=\"261\" class=\"content_image\" width=\"386\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;386&#39; height=&#39;261&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"386\" data-rawheight=\"261\" class=\"content_image lazy\" width=\"386\" data-actualsrc=\"https://pic2.zhimg.com/v2-29b3bd8dce69b45b8e8f98d2592105b1_b.jpg\"/></figure><p>为了获取上图中的决策边界，我们的假设函数可能是： <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_0+%2B+%5Ctheta_1+x_1+%2B+%5Ctheta_2+x_2+%2B++%5Ctheta_3+x_1x_2+%2B+%5Ctheta_4+x_1%5E2+%2B%5Ctheta_5+x_2%5E2+%2B...\" alt=\"\\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 +  \\theta_3 x_1x_2 + \\theta_4 x_1^2 +\\theta_5 x_2^2 +...\" eeimg=\"1\"/> </p><p>的形式，为了方便，我们可以用一系列新的特征值来替换模型中的每一项，譬如：</p><p><img src=\"https://www.zhihu.com/equation?tex=f_1+%3D+x_1%2Cf_2+%3D+x_2%2C+f_3+%3D+x_1x_2%2Cf_4%3Dx_1%5E2%2C+f_5%3Dx_2%5E2\" alt=\"f_1 = x_1,f_2 = x_2, f_3 = x_1x_2,f_4=x_1^2, f_5=x_2^2\" eeimg=\"1\"/> </p><p>则假设函数便可以转化为： <img src=\"https://www.zhihu.com/equation?tex=h_%5Ctheta%28x%29%3D%5Ctheta_0+%2B+%5Ctheta_1f_1+%2B+%5Ctheta_2+f_2+%2B++%5Ctheta_3f_3+%2B+%5Ctheta_4f_4+%2B%5Ctheta_5f_5+%2B...\" alt=\"h_\\theta(x)=\\theta_0 + \\theta_1f_1 + \\theta_2 f_2 +  \\theta_3f_3 + \\theta_4f_4 +\\theta_5f_5 +...\" eeimg=\"1\"/> </p><p>这种方法即通过多项式模型的方式构造新特征 <img src=\"https://www.zhihu.com/equation?tex=f_1%2Cf_2%2Cf_3...f_n\" alt=\"f_1,f_2,f_3...f_n\" eeimg=\"1\"/> ，那么有没有其他方式来构造新特征？有，通过核函数即可完成。</p><p>为了构造新的特征 <img src=\"https://www.zhihu.com/equation?tex=f_1%2Cf_2%2Cf_3\" alt=\"f_1,f_2,f_3\" eeimg=\"1\"/> 我们引入<b>地标（landmark)</b> <img src=\"https://www.zhihu.com/equation?tex=l%5E%7B%281%29%7D%2Cl%5E%7B%282%29%7D%2Cl%5E%7B%283%29%7D\" alt=\"l^{(1)},l^{(2)},l^{(3)}\" eeimg=\"1\"/> ,我们可以通过判断样本x和地标间的近似程度来选取新的特征 <img src=\"https://www.zhihu.com/equation?tex=f_1%2Cf_2%2Cf_3\" alt=\"f_1,f_2,f_3\" eeimg=\"1\"/> </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-0fd4ba14c8e1819d87439514dc9af547_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"683\" data-rawheight=\"389\" class=\"origin_image zh-lightbox-thumb\" width=\"683\" data-original=\"https://pic4.zhimg.com/v2-0fd4ba14c8e1819d87439514dc9af547_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;683&#39; height=&#39;389&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"683\" data-rawheight=\"389\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"683\" data-original=\"https://pic4.zhimg.com/v2-0fd4ba14c8e1819d87439514dc9af547_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-0fd4ba14c8e1819d87439514dc9af547_b.jpg\"/></figure><p>如上图所示，特征 <img src=\"https://www.zhihu.com/equation?tex=f_1%2Cf_2%2Cf_3\" alt=\"f_1,f_2,f_3\" eeimg=\"1\"/> 都可以用similarity(x,l)函数来获取，这里的similarity(x,l)函数即被称为—核函数（kernel function）,在本例中我们用核函数中的一种—高斯核函数来举例，即：<img src=\"https://www.zhihu.com/equation?tex=f_i+%3D+similarity%28x%2Cl%5E%7B%28i%29%7D%29+%3D+exp%28-%5Cfrac%7B%7C%7Cx-l%5E%7B%28i%29%7D%7C%7C%5E2%7D%7B2%5Csigma%5E2%7D%29\" alt=\"f_i = similarity(x,l^{(i)}) = exp(-\\frac{||x-l^{(i)}||^2}{2\\sigma^2})\" eeimg=\"1\"/> </p><p>地标的作用是什么 ？如果一个样本x距离地标距离接近/等于0，则 <img src=\"https://www.zhihu.com/equation?tex=f_i+%5Csimeq+exp%28-0%29+%3D+1\" alt=\"f_i \\simeq exp(-0) = 1\" eeimg=\"1\"/> ,否则 = 0，于是我们利用样本和地标间的关系来得出了特征f的值。</p><h3>1.5.1 地标<b>landmark</b>和𝜎</h3><p><img src=\"https://www.zhihu.com/equation?tex=l%E5%92%8C%5Csigma\" alt=\"l和\\sigma\" eeimg=\"1\"/> 会对模型和特征f有什么影响？ 我们看一个例子：</p><p>这里取地标为固定向量，三组不同的 <img src=\"https://www.zhihu.com/equation?tex=%5Csigma\" alt=\"\\sigma\" eeimg=\"1\"/> </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-6ee6ea88aa73d888c925817cf8f6381c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"686\" data-rawheight=\"412\" class=\"origin_image zh-lightbox-thumb\" width=\"686\" data-original=\"https://pic1.zhimg.com/v2-6ee6ea88aa73d888c925817cf8f6381c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;686&#39; height=&#39;412&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"686\" data-rawheight=\"412\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"686\" data-original=\"https://pic1.zhimg.com/v2-6ee6ea88aa73d888c925817cf8f6381c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-6ee6ea88aa73d888c925817cf8f6381c_b.jpg\"/></figure><p>看图，可以总结出几点：</p><p>1.红色顶点处，即向量 <img src=\"https://www.zhihu.com/equation?tex=x+%3D+%5Cbegin%7BBmatrix%7D+x_1%5C%5Cx_2+%5Cend%7BBmatrix%7D+%3D++%5Cbegin%7BBmatrix%7D+3%5C%5C5+%5Cend%7BBmatrix%7D\" alt=\"x = \\begin{Bmatrix} x_1\\\\x_2 \\end{Bmatrix} =  \\begin{Bmatrix} 3\\\\5 \\end{Bmatrix}\" eeimg=\"1\"/> 和向量地标向量 <img src=\"https://www.zhihu.com/equation?tex=l\" alt=\"l\" eeimg=\"1\"/> 重合处，即距离= 0，故此时 <img src=\"https://www.zhihu.com/equation?tex=f%3D1\" alt=\"f=1\" eeimg=\"1\"/> </p><p>2.可以看见 <img src=\"https://www.zhihu.com/equation?tex=%5Csigma\" alt=\"\\sigma\" eeimg=\"1\"/> 越大，图像越宽，同样的x样本向量，譬如 <img src=\"https://www.zhihu.com/equation?tex=x+%3D+%5Cbegin%7BBmatrix%7D+x_1%5C%5Cx_2+%5Cend%7BBmatrix%7D+%3D++%5Cbegin%7BBmatrix%7D+4%5C%5C4+%5Cend%7BBmatrix%7D\" alt=\"x = \\begin{Bmatrix} x_1\\\\x_2 \\end{Bmatrix} =  \\begin{Bmatrix} 4\\\\4 \\end{Bmatrix}\" eeimg=\"1\"/> ，这个样本在图1中就会被判定为 <img src=\"https://www.zhihu.com/equation?tex=f%3D0\" alt=\"f=0\" eeimg=\"1\"/> ，而在图3中则可能被判定为 <img src=\"https://www.zhihu.com/equation?tex=f%3D1\" alt=\"f=1\" eeimg=\"1\"/> ，即 <img src=\"https://www.zhihu.com/equation?tex=%5Csigma\" alt=\"\\sigma\" eeimg=\"1\"/> 会影响到最终特征值f的判断。即随着𝑥的改变𝑓值改变的速率受到𝜎的控制。 </p><h3>1.5.2 决策边界</h3><p>假定：假设函数值&gt;=0时预测y = 1,否则y = 0，则通过上面的高斯核函数我们可以算出每个样本点x距离地标l的距离，从而算出每个特征f，从而求出每个样本点的预测值y，即可以正确给每个样本分类，从而得到一条决策边界。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-0c624dc9761f8e27908849a35e0e85aa_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"726\" data-rawheight=\"364\" class=\"origin_image zh-lightbox-thumb\" width=\"726\" data-original=\"https://pic3.zhimg.com/v2-0c624dc9761f8e27908849a35e0e85aa_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;726&#39; height=&#39;364&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"726\" data-rawheight=\"364\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"726\" data-original=\"https://pic3.zhimg.com/v2-0c624dc9761f8e27908849a35e0e85aa_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-0c624dc9761f8e27908849a35e0e85aa_b.jpg\"/></figure><p>例如： <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_0%3D-0.5%2C%5Ctheta_1%3D1%2C%5Ctheta_2%3D1%2C%5Ctheta_3%3D0\" alt=\"\\theta_0=-0.5,\\theta_1=1,\\theta_2=1,\\theta_3=0\" eeimg=\"1\"/> </p><p>对于红色点x，由于其距离地标l1较近，故f1 = 1,同时其距离l2和l3较远，故f2 = f3 = 0，假设函数值= -0.5+1 = 0.5&gt;0故预测其y = 1；</p><p>对于绿色点x,f2 = 1 ,假设函数值  = -0.5+0+1+0 =0.5故其预测也为1</p><p>.....</p><p>可以看出此例中存在一条决策边界，如红线划出的范围，在边界以内的样本都是预测y = 1，边界外的都是y = 0。</p><h3>1.5.3 核函数2</h3><p>上一个例子，比较简单地说明了核函数的应用，但是实际情况下，核函数怎么使用呢？地标l又如何选取？</p><p>实际情况下，我们会选取和样本点数量同样多的且值相同的地标l</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-d698fb0755c8ca1acf73384690aa28e6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"698\" data-rawheight=\"410\" class=\"origin_image zh-lightbox-thumb\" width=\"698\" data-original=\"https://pic3.zhimg.com/v2-d698fb0755c8ca1acf73384690aa28e6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;698&#39; height=&#39;410&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"698\" data-rawheight=\"410\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"698\" data-original=\"https://pic3.zhimg.com/v2-d698fb0755c8ca1acf73384690aa28e6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-d698fb0755c8ca1acf73384690aa28e6_b.jpg\"/></figure><p>和之前的一样，如果我们有m个样本，就能得到m+1个特征矩阵f(加了一项f0作为bias)</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-167cfa4bb50e127f514ac6f972b59362_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"751\" data-rawheight=\"410\" class=\"origin_image zh-lightbox-thumb\" width=\"751\" data-original=\"https://pic3.zhimg.com/v2-167cfa4bb50e127f514ac6f972b59362_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;751&#39; height=&#39;410&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"751\" data-rawheight=\"410\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"751\" data-original=\"https://pic3.zhimg.com/v2-167cfa4bb50e127f514ac6f972b59362_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-167cfa4bb50e127f514ac6f972b59362_b.jpg\"/></figure><p>得到新的特征后，我们可以写出代价函数的表达式：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-ce61b9b5a4f9671822b64a45e51c7c88_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"668\" data-rawheight=\"225\" class=\"origin_image zh-lightbox-thumb\" width=\"668\" data-original=\"https://pic1.zhimg.com/v2-ce61b9b5a4f9671822b64a45e51c7c88_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;668&#39; height=&#39;225&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"668\" data-rawheight=\"225\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"668\" data-original=\"https://pic1.zhimg.com/v2-ce61b9b5a4f9671822b64a45e51c7c88_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-ce61b9b5a4f9671822b64a45e51c7c88_b.jpg\"/></figure><p>      这里可以看到 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta%5ETf%5E%7B%28i%29%7D\" alt=\"\\theta^Tf^{(i)}\" eeimg=\"1\"/> 替代了原来的 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta%5ETx%5E%7B%28i%29%7D\" alt=\"\\theta^Tx^{(i)}\" eeimg=\"1\"/> ,因为f是计算出来的用于模拟x的特征值。最后一项 <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7B2%7D%5CSigma%5En_%7B%28j%3D1%29%7D%5Ctheta%5E2_j\" alt=\"\\frac{1}{2}\\Sigma^n_{(j=1)}\\theta^2_j\" eeimg=\"1\"/> 实际上的n-可以替换成m，因为这里特征值只有m个。然后，在实际计算的时候， <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B1%7D%7B2%7D%5CSigma%5En_%7B%28j%3D1%29%7D%5Ctheta%5E2_j+%3D+%5Ctheta%5ET%5Ctheta\" alt=\"\\frac{1}{2}\\Sigma^n_{(j=1)}\\theta^2_j = \\theta^T\\theta\" eeimg=\"1\"/> 我们会在之间加一个矩阵M，不同的核函数，M不同，目的在于优化计算和迭代速度。所以最终，正则化项应该是： <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta%5ETM%5Ctheta\" alt=\"\\theta^TM\\theta\" eeimg=\"1\"/> </p><p>      在此，我们不介绍最小化支持向量机的代价函数的方法，你可以使用现有的软件包(如 <b>liblinear</b>,<b>libsvm </b>等)。在使用这些软件包最小化我们的代价函数之前，我们通常需要编写核函数，并且如果我们使用高斯核函数，那么在使用之前进行特征缩放是非常必要的。 </p><p>      另外，支持向量机也可以不使用核函数，不使用核函数又称为线性核函数(<b>linear kernel</b>)， 当我们不采用非常复杂的函数，或者我们的训练集特征非常多而实例非常少的时候，可以采用这种不带核函数的支持向量机。</p><h2>1.6 使用支持向量机 </h2><p>本节主要是对支持向量机、核函数等概念和使用的一个总结，我就直接copy了。</p><hr/><p>      目前为止，我们已经讨论了 <b>SVM </b>比较抽象的层面，在这个视频中我将要讨论到为了运行或者运用 <b>SVM</b>。你实际上所需要的一些东西:支持向量机算法，提出了一个特别优化的问 题。但是就如在之前的视频中我简单提到的，我真的<b>不建议你自己写代码来求解参数𝜃</b>，因此由于今天我们中的很少人，或者其实没有人考虑过自己写代码来转换矩阵，或求一个数的平方根等我们只是知道如何去调用库函数来实现这些功能。同样的，用以解决 <b>SVM </b>最优化 问题的软件很复杂，且已经有研究者做了很多年数值优化了。因此你提出好的软件库和好的软件包来做这样一些事儿。然后强烈建议使用高优化软件库中的一个，而不是尝试自己落实一些数据。有许多好的软件库，我正好用得最多的两个是 <b>liblinear </b>和 <b>libsvm</b>，但是真的有很 多软件库可以用来做这件事儿。你可以连接许多你可能会用来编写学习算法的主要编程语言。 </p><p>    在高斯核函数之外我们还有其他一些选择，如:<br/> <b>多项式核函数(Polynomial Kernel)</b><br/> <b>字符串核函数(String kernel)</b><br/> <b>卡方核函数( chi-square kernel)</b><br/> <b>直方图交集核函数(histogram intersection kernel)</b><br/>        等等... </p><p>这些核函数的目标也都是根据训练集和地标之间的距离来构建新特征，这些核函数需要 满足 <b>Mercer&#39;s </b>定理，才能被支持向量机的优化软件正确处理。 </p><h3>1.6.2多类分类问题 </h3><p>      假设我们利用之前介绍的一对多方法来解决一个多类分类问题。如果一共有𝑘个类，则 我们需要𝑘个模型，以及𝑘个参数向量𝜃。我们同样也可以训练𝑘个支持向量机来解决多类分 类问题。但是大多数支持向量机软件包都有内置的多类分类功能，我们只要直接使用即可。 </p><p>尽管你不去写你自己的 <b>SVM </b>的优化软件，但是你也需要做几件事: </p><p>1、是提出参数𝐶的选择。我们在之前的视频中讨论过误差/方差在这方面的性质。 </p><p>2、你也需要选择内核参数或你想要使用的相似函数，其中一个选择是:我们选择不需要任何内核参数，<b>没有内核参数的理念，也叫线性核函数。</b>因此，如果有人说他使用了线性核的 <b>SVM</b>(支持向量机)，这就意味这他使用了不带有核函数的 <b>SVM</b>(支持向量机)。 </p><h3>1.6.3逻辑回归or支持向量机 </h3><p><b>在两者之间，我们应该如何选择呢?</b></p><p><b>下面是一些普遍使用的准则:</b></p><p>      𝑛为特征数，𝑚为训练样本数。 </p><p>      (1)如果相较于𝑚而言，𝑛要大许多，即训练集数据量不够支持我们训练一个复杂的非线 性模型，我们选用逻辑回归模型或者不带核函数的支持向量机。 </p><p>      (2)如果𝑛较小，而且𝑚大小中等，例如𝑛在 1-1000 之间，而𝑚在 10-10000 之间，使用高斯核函数的支持向量机。</p><p>      (3)如果𝑛较小，而𝑚较大，例如𝑛在 1-1000 之间，而𝑚大于 50000，则使用支持向量机会 非常慢，解决方案是创造、增加更多的特征，然后使用逻辑回归或不带核函数的支持向量机。 值得一提的是，神经网络在以上三种情况下都可能会有较好的表现，但是训练神经网络可能非常慢，选择支持向量机的原因主要在于它的代价函数是凸函数，不存在局部最小值。 今天的 <b>SVM </b>包会工作得很好，但是它们仍然会有一些慢。当你有非常非常大的训练集，且用高斯核函数是在这种情况下，我经常会做的是尝试手动地创建，拥有更多的特征变量， 然后用逻辑回归或者不带核函数的支持向量机。如果你看到这个幻灯片，看到了逻辑回归， 或者不带核函数的支持向量机。在这个两个地方，我把它们放在一起是有原因的。原因是: 逻辑回归和不带核函数的支持向量机它们都是非常相似的算法，不管是逻辑回归还是不带核 函数的 <b>SVM</b>，通常都会做相似的事情，并给出相似的结果。但是根据你实现的情况，其中一 个可能会比另一个更加有效。但是在其中一个算法应用的地方，逻辑回归或不带核函数的 <b>SVM </b>另一个也很有可能很有效。但是随着 <b>SVM </b>的复杂度增加，当你使用不同的内核函数来 学习复杂的非线性函数时，这个体系，你知道的，当你有多达 1 万(10,000)的样本时，也 可能是 5 万(50,000)，你的特征变量的数量这是相当大的。那是一个非常常见的体系，也 许在这个体系里，不带核函数的支持向量机就会表现得相当突出。你可以做比这困难得多需 要逻辑回归的事情。 </p><p><b>最后，神经网络使用于什么时候呢?</b> </p><p>      对于所有的这些问题，对于所有的这些不同体系 一个设计得很好的神经网络也很有可能会非常有效。有一个缺点是，或者说是有时可能不会 使用神经网络的原因是:对于许多这样的问题，神经网络训练起来可能会特别慢，但是如果 你有一个非常好的 <b>SVM </b>实现包，它可能会运行得比较快比神经网络快很多，尽管我们在此之前没有展示，但是事实证明，<b>SVM 的优化问题，是一种凸优化问题。因此，好的 SVM 优化软件包总是会找到全局最小值，或者接近它的值。对于 SVM 你不需要担心局部最优。</b></p><p>      在实际应用中，局部最优不是神经网络所需要解决的一个重大问题，所以这是你在使用 <b>SVM </b>的时候不需要太去担心的一个问题。根据你的问题，神经网络可能会比 <b>SVM </b>慢，尤其是在这样一个体系中，至于这里给出的参考，看上去有些模糊，如果你在考虑一些问题，这些参 考会有一些模糊，但是我仍然不能完全确定，我是该用这个算法还是改用那个算法，这个没 有太大关系，当我遇到机器学习问题的时候，有时它确实不清楚这是否是最好的算法，但是 就如在之前的视频中看到的算法确实很重要。但是通常更加重要的是:你有多少数据，你有 多熟练是否擅长做误差分析和排除学习算法，指出如何设定新的特征变量和找出其他能决定 你学习算法的变量等方面，通常这些方面会比你使用逻辑回归还是 <b>SVM </b>这方面更加重要。 但是，已经说过了，<b>SVM </b>仍然被广泛认为是一种最强大的学习算法，这是一个体系，包含了 什么时候一个有效的方法去学习复杂的非线性函数。因此，实际上与逻辑回归、神经网络、<b>SVM </b>一起使用这些方法来提高学习算法，我认为你会很好地建立很有技术的状态。(编者 注:当时 <b>GPU </b>计算比较慢，神经网络还不流行。) </p><p>      机器学习系统对于一个宽泛的应用领域来说，这是另一个在你军械库里非常强大的工具， 你可以把它应用到很多地方，如硅谷、在工业、学术等领域建立许多高性能的机器学习系统。</p><p></p>", 
            "topic": [
                {
                    "tag": "SVM", 
                    "tagLink": "https://api.zhihu.com/topics/19583524"
                }, 
                {
                    "tag": "kernel（核函数）", 
                    "tagLink": "https://api.zhihu.com/topics/19619891"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": [
                {
                    "userName": "知乎用户", 
                    "userLink": "https://www.zhihu.com/people/0", 
                    "content": "<p>说真的，这种看懂了，但是他用Octave写的代码理解不了怎么破？</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "Lyon", 
                            "userLink": "https://www.zhihu.com/people/38495a9b1f83e4a612543ce6c523df7f", 
                            "content": "代码更好理解吧，哪里不会百度哪里[捂脸]", 
                            "likes": 0, 
                            "replyToAuthor": "知乎用户"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/74167352", 
            "userName": "Lyon", 
            "userLink": "https://www.zhihu.com/people/38495a9b1f83e4a612543ce6c523df7f", 
            "upvote": 11, 
            "title": "【吴恩达机器学习】第五周—神经网络反向传播算法", 
            "content": "<p><b>前言：</b></p><p>      最近在学习深度学习，看了不少教程，发现还是吴恩达的比较适用于我。吴恩达机器学习公开课视频最早是斯坦福大学的课程视频(那个画面有点老)，新版的视频在网易云课堂上可以随时学习。仅仅通过视频学习，可能会有点快，因为有的知识点需要反复推敲和回味。感谢github上一位朋友的精心整理，让我们可以配合讲义一起学习，讲义有HTML版、PDF版、Markdown版的：视频配合讲义看，事半功倍</p><p>      本文包括接下来的系列文章，都会是吴恩达机器学习课程的个人学习总结，有很多文字和图片直接参考了PDF版的课程讲义，再次谢过提供讲义的朋友！</p><p>网易云课堂课程视频：</p><p><a href=\"https://link.zhihu.com/?target=https%3A//study.163.com/course/courseMain.htm%3FcourseId%3D1004570029\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">study.163.com/course/co</span><span class=\"invisible\">urseMain.htm?courseId=1004570029</span><span class=\"ellipsis\"></span></a></p><p>课程同步配套讲义：</p><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/fengdu78/Coursera-ML-AndrewNg-Notes\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/fengdu78/Cou</span><span class=\"invisible\">rsera-ML-AndrewNg-Notes</span><span class=\"ellipsis\"></span></a></p><p>原文发布在语雀文档上,效果更好看点：</p><a href=\"https://link.zhihu.com/?target=https%3A//www.yuque.com/docs/share/c8164518-463f-4eb6-a0dc-9d618499fa63\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-72bbb1e26eacf07192c32d1b6514dde0_ipico.jpg\" data-image-width=\"512\" data-image-height=\"512\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">第五周 · 语雀</a><hr/><h2><b>第五周</b></h2><h2><b>1.神经网络的损失函数</b></h2><p>      神经网络模型中损失函数/代价函数和之前的逻辑回归模型中的代价函数有什么区别？先回顾下正则化的逻辑回归模型：<br/>      损失函数：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-a7486779b3c9d82abf5f75fec833097b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"594\" data-rawheight=\"92\" class=\"origin_image zh-lightbox-thumb\" width=\"594\" data-original=\"https://pic4.zhimg.com/v2-a7486779b3c9d82abf5f75fec833097b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;594&#39; height=&#39;92&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"594\" data-rawheight=\"92\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"594\" data-original=\"https://pic4.zhimg.com/v2-a7486779b3c9d82abf5f75fec833097b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-a7486779b3c9d82abf5f75fec833097b_b.jpg\"/></figure><p>      梯度下降算法：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-5e2e23a4774275dcde994d09fca9718f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"480\" data-rawheight=\"300\" class=\"origin_image zh-lightbox-thumb\" width=\"480\" data-original=\"https://pic4.zhimg.com/v2-5e2e23a4774275dcde994d09fca9718f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;480&#39; height=&#39;300&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"480\" data-rawheight=\"300\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"480\" data-original=\"https://pic4.zhimg.com/v2-5e2e23a4774275dcde994d09fca9718f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-5e2e23a4774275dcde994d09fca9718f_b.jpg\"/></figure><p>注意：这里样本点有m个，特征参数θ有n个</p><p>      损失函数的核心就是求m个样本点的总误差，然后除以m，得到平均误差，即平均损失。那么在神经网络中的损失函数公式是怎样？其实，思想都一样，都是求平均损失，只是表现方式看上去复杂了一些。前半部分很好理解，就是将每个样本点的k类的交叉熵损失相加求期望，关键在于后面的正则化项，看上去不太好理解。<br/><b>神经网络模型的损失函数公式如下：</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-7268f854dd23632fcb84aa961d3d28b3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"674\" data-rawheight=\"165\" class=\"origin_image zh-lightbox-thumb\" width=\"674\" data-original=\"https://pic4.zhimg.com/v2-7268f854dd23632fcb84aa961d3d28b3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;674&#39; height=&#39;165&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"674\" data-rawheight=\"165\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"674\" data-original=\"https://pic4.zhimg.com/v2-7268f854dd23632fcb84aa961d3d28b3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-7268f854dd23632fcb84aa961d3d28b3_b.jpg\"/></figure><p>      后半部分的正则化项是如何得来？请看下面1.1节公式推导。</p><h2><b>1.1公式推导</b></h2><p>      乍一看公式，一脸懵，下面，我们还是会结合一个例子推导一下。推导前，先明确下神经网络模型中的分类情况和一些参数的含义：</p><p> 这里定义：</p><p><img src=\"https://www.zhihu.com/equation?tex=L%3A%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%80%BB%E5%B1%82%E6%95%B0\" alt=\"L:神经网络模型的总层数\" eeimg=\"1\"/> <br/> <img src=\"https://www.zhihu.com/equation?tex=s_l%EF%BC%9Al%E6%BF%80%E6%B4%BB%E5%8D%95%E5%85%83%28%E7%A5%9E%E7%BB%8F%E5%85%83%29%E4%B8%AA%E6%95%B0\" alt=\"s_l：l激活单元(神经元)个数\" eeimg=\"1\"/> <img src=\"https://www.zhihu.com/equation?tex=K%EF%BC%9A%E5%88%86%E7%B1%BB%E7%B1%BB%E5%88%AB%E6%95%B0%5Cbegin%7Bcases%7D++k+%3D+2%EF%BC%8C%E8%BE%93%E5%87%BA%E5%B1%82%E5%8F%AA%E9%9C%80%E8%A6%81%E4%B8%80%E4%B8%AA%E6%BF%80%E6%B4%BB%E5%8D%95%E5%85%83%E5%8D%B3%E5%8F%AF%EF%BC%8Cs_l+%3D+1+%5C%5C%5B2ex%5D+k+%3E%3D3%2C%E8%BE%93%E5%87%BA%E5%B1%82%E9%9C%80%E8%A6%81k%E4%B8%AA%E6%BF%80%E6%B4%BB%E5%8D%95%E5%85%83%EF%BC%8Cs_l+%3D+k+%5Cend%7Bcases%7D\" alt=\"K：分类类别数\\begin{cases}  k = 2，输出层只需要一个激活单元即可，s_l = 1 \\\\[2ex] k &gt;=3,输出层需要k个激活单元，s_l = k \\end{cases}\" eeimg=\"1\"/> <br/>然后，看下面的神经网络模型：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-c41623328b38ee5ff6c4ad0522c8e266_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"889\" data-rawheight=\"253\" class=\"origin_image zh-lightbox-thumb\" width=\"889\" data-original=\"https://pic3.zhimg.com/v2-c41623328b38ee5ff6c4ad0522c8e266_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;889&#39; height=&#39;253&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"889\" data-rawheight=\"253\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"889\" data-original=\"https://pic3.zhimg.com/v2-c41623328b38ee5ff6c4ad0522c8e266_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-c41623328b38ee5ff6c4ad0522c8e266_b.jpg\"/></figure><p>这是一个包含m个样本点，L = 4,k = 4的神经网络分类模型<br/>第1~4层的激活单元数分别为：3，5，5，4  </p><p><img src=\"https://www.zhihu.com/equation?tex=s_l+%3D+3%2C5%2C5%2C4%28for+l+%3D+1%2C2%2C3%2C4%29\" alt=\"s_l = 3,5,5,4(for l = 1,2,3,4)\" eeimg=\"1\"/> </p><p><b>下面我们看一下每一layer的矩阵运算，重点看θ矩阵：</b><br/>      第一层：5x4矩阵 </p><p><img src=\"https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%281%29%7D+%3D+%5Cbegin%7BBmatrix%7D+%5Ctheta_%7B10%7D+%26+%5Ctheta_%7B11%7D+%26+%5Ctheta_%7B12%7D+%26%5Ctheta_%7B13%7D+%5C%5C+%5Ctheta_%7B20%7D+%26..%26..%26..+%5C%5C+%5Ctheta_%7B30%7D+%26..%26..%26..+%5C%5C+%5Ctheta_%7B40%7D+%26..%26..%26..+%5C%5C+%5Ctheta_%7B50%7D+%26..%26..%26+%5Ctheta_%7B53%7D+%5Cend%7BBmatrix%7D\" alt=\"\\theta^{(1)} = \\begin{Bmatrix} \\theta_{10} &amp; \\theta_{11} &amp; \\theta_{12} &amp;\\theta_{13} \\\\ \\theta_{20} &amp;..&amp;..&amp;.. \\\\ \\theta_{30} &amp;..&amp;..&amp;.. \\\\ \\theta_{40} &amp;..&amp;..&amp;.. \\\\ \\theta_{50} &amp;..&amp;..&amp; \\theta_{53} \\end{Bmatrix}\" eeimg=\"1\"/> ， <img src=\"https://www.zhihu.com/equation?tex=X%5E%7B%281%29%7D+%3D+%5Cbegin%7BBmatrix%7D+x_0%28bias%29%5C%5C+x_1%5C%5C++x_2%5C%5C+x_3%5C%5C+%5Cend%7BBmatrix%7D\" alt=\"X^{(1)} = \\begin{Bmatrix} x_0(bias)\\\\ x_1\\\\  x_2\\\\ x_3\\\\ \\end{Bmatrix}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%5CLongrightarrow+%5Ctheta%5E%7B%281%29%7DX%5E%7B%281%29%7D+%3D+%5Calpha%5E%7B%282%29%7D+%3D+%5Cbegin%7BBmatrix%7D++%5Calpha_1%5C%5C++%5Calpha_2%5C%5C+%5Calpha_3%5C%5C+%5Calpha_4%5C%5C+%5Calpha_5+%5Cend%7BBmatrix%7D\" alt=\"\\Longrightarrow \\theta^{(1)}X^{(1)} = \\alpha^{(2)} = \\begin{Bmatrix}  \\alpha_1\\\\  \\alpha_2\\\\ \\alpha_3\\\\ \\alpha_4\\\\ \\alpha_5 \\end{Bmatrix}\" eeimg=\"1\"/> <br/>      第二层：5x6矩阵 </p><p><img src=\"https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%282%29%7D+%3D+%5Cbegin%7BBmatrix%7D+%5Ctheta_%7B10%7D+%26+%5Ctheta_%7B11%7D+%26+%5Ctheta_%7B12%7D+%26%5Ctheta_%7B13%7D%26%5Ctheta_%7B14%7D%26%5Ctheta_%7B15%7D+%5C%5C+%5Ctheta_%7B20%7D+%26..%26..%26..%26..%26..+%5C%5C+%5Ctheta_%7B30%7D+%26..%26..%26..%26..%26..+%5C%5C+%5Ctheta_%7B40%7D+%26..%26..%26..%26..%26..+%5C%5C+%5Ctheta_%7B50%7D+%26..%26..%26..%26..%26+%5Ctheta_%7B55%7D+%5Cend%7BBmatrix%7D\" alt=\"\\theta^{(2)} = \\begin{Bmatrix} \\theta_{10} &amp; \\theta_{11} &amp; \\theta_{12} &amp;\\theta_{13}&amp;\\theta_{14}&amp;\\theta_{15} \\\\ \\theta_{20} &amp;..&amp;..&amp;..&amp;..&amp;.. \\\\ \\theta_{30} &amp;..&amp;..&amp;..&amp;..&amp;.. \\\\ \\theta_{40} &amp;..&amp;..&amp;..&amp;..&amp;.. \\\\ \\theta_{50} &amp;..&amp;..&amp;..&amp;..&amp; \\theta_{55} \\end{Bmatrix}\" eeimg=\"1\"/> ， <img src=\"https://www.zhihu.com/equation?tex=%5Calpha%5E%7B%281%29%7D+%3D+%5Cbegin%7BBmatrix%7D+%5Calpha_0%28bias%29%5C%5C+%5Calpha_1%5C%5C++%5Calpha_2%5C%5C+%5Calpha_3%5C%5C+%5Calpha_4%5C%5C+%5Calpha_5+%5Cend%7BBmatrix%7D\" alt=\"\\alpha^{(1)} = \\begin{Bmatrix} \\alpha_0(bias)\\\\ \\alpha_1\\\\  \\alpha_2\\\\ \\alpha_3\\\\ \\alpha_4\\\\ \\alpha_5 \\end{Bmatrix}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%5CLongrightarrow+%5Ctheta%5E%7B%282%29%7D%5Calpha%5E%7B%282%29%7D+%3D+%5Calpha%5E%7B%283%29%7D%3D+%5Cbegin%7BBmatrix%7D+%5Calpha_1%5C%5C++%5Calpha_2%5C%5C+%5Calpha_3%5C%5C+%5Calpha_4%5C%5C+%5Calpha_5+%5Cend%7BBmatrix%7D\" alt=\"\\Longrightarrow \\theta^{(2)}\\alpha^{(2)} = \\alpha^{(3)}= \\begin{Bmatrix} \\alpha_1\\\\  \\alpha_2\\\\ \\alpha_3\\\\ \\alpha_4\\\\ \\alpha_5 \\end{Bmatrix}\" eeimg=\"1\"/> </p><p>      第三层：4x6矩阵 </p><p><img src=\"https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%283%29%7D+%3D+%5Cbegin%7BBmatrix%7D+%5Ctheta_%7B10%7D+%26+%5Ctheta_%7B11%7D+%26+%5Ctheta_%7B12%7D+%26%5Ctheta_%7B13%7D%26+%5Ctheta_%7B14%7D+%26+%5Ctheta_%7B15%7D++%5C%5C+%5Ctheta_%7B20%7D+%26..%26..%26..%26..%26..+%5C%5C+%5Ctheta_%7B30%7D+%26..%26..%26..%26..%26..+%5C%5C+%5Ctheta_%7B40%7D+%26..%26..%26..%26..%26+%5Ctheta_%7B45%7D+%5Cend%7BBmatrix%7D\" alt=\"\\theta^{(3)} = \\begin{Bmatrix} \\theta_{10} &amp; \\theta_{11} &amp; \\theta_{12} &amp;\\theta_{13}&amp; \\theta_{14} &amp; \\theta_{15}  \\\\ \\theta_{20} &amp;..&amp;..&amp;..&amp;..&amp;.. \\\\ \\theta_{30} &amp;..&amp;..&amp;..&amp;..&amp;.. \\\\ \\theta_{40} &amp;..&amp;..&amp;..&amp;..&amp; \\theta_{45} \\end{Bmatrix}\" eeimg=\"1\"/> ， <img src=\"https://www.zhihu.com/equation?tex=%5Calpha%5E%7B%282%29%7D+%3D+%5Cbegin%7BBmatrix%7D+%5Calpha_0%28bias%29%5C%5C+%5Calpha_1%5C%5C++%5Calpha_2%5C%5C+%5Calpha_3%5C%5C+%5Calpha_4%5C%5C+%5Calpha_5+%5Cend%7BBmatrix%7D\" alt=\"\\alpha^{(2)} = \\begin{Bmatrix} \\alpha_0(bias)\\\\ \\alpha_1\\\\  \\alpha_2\\\\ \\alpha_3\\\\ \\alpha_4\\\\ \\alpha_5 \\end{Bmatrix}\" eeimg=\"1\"/></p><p><img src=\"https://www.zhihu.com/equation?tex=%5CLongrightarrow+%5Ctheta%5E%7B%283%29%7D%5Calpha%5E%7B%283%29%7D+%3D+%5Calpha%5E%7B%284%29%7D%3D%5Cbegin%7BBmatrix%7D+%5Calpha_1%5C%5C++%5Calpha_2%5C%5C+%5Calpha_3%5C%5C+%5Calpha_4+%5Cend%7BBmatrix%7D\" alt=\"\\Longrightarrow \\theta^{(3)}\\alpha^{(3)} = \\alpha^{(4)}=\\begin{Bmatrix} \\alpha_1\\\\  \\alpha_2\\\\ \\alpha_3\\\\ \\alpha_4 \\end{Bmatrix}\" eeimg=\"1\"/> <br/><br/>下面再看神经网络模型下的损失函数后半部分的正则化项</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Clambda%7D%7B2m%7D+%5Csum%5E%7BL-1%7D_%7Bl%3D1%7D+++%5Csum%5E%7Bs_l%7D_%7Bi%3D1%7D++%5Csum%5E%7Bs_l%2B1%7D_%7Bj%3D1%7D++%28%5CTheta%5E%7B%28l%29%7D_%7Bji%7D%29%5E2\" alt=\"\\frac{\\lambda}{2m} \\sum^{L-1}_{l=1}   \\sum^{s_l}_{i=1}  \\sum^{s_l+1}_{j=1}  (\\Theta^{(l)}_{ji})^2\" eeimg=\"1\"/> </p><p>在本例中 <img src=\"https://www.zhihu.com/equation?tex=L+%3D+4%2Cs_l+%3D+3%2C5%2C5%2C4+%28+l+%3D+1%2C2%2C3%2C4%29\" alt=\"L = 4,s_l = 3,5,5,4 ( l = 1,2,3,4)\" eeimg=\"1\"/> ，将正则化项展开：<br/><br/> <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Clambda%7D%7B2m%7D+%5Csum%5E%7BL-1%7D_%7Bl%3D1%7D+++%5Csum%5E%7Bs_l%7D_%7Bi%3D1%7D++%5Csum%5E%7Bs_%7Bl%2B1%7D%7D_%7Bj%3D1%7D++%28%5CTheta%5E%7B%28l%29%7D_%7Bji%7D%29%5E2+%5CLongleftrightarrow\" alt=\"\\frac{\\lambda}{2m} \\sum^{L-1}_{l=1}   \\sum^{s_l}_{i=1}  \\sum^{s_{l+1}}_{j=1}  (\\Theta^{(l)}_{ji})^2 \\Longleftrightarrow\" eeimg=\"1\"/></p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Clambda%7D%7B2m%7D++%5Csum%5E%7B3%7D_%7Bi%3D1%7D++%5Csum%5E%7B5%7D_%7Bj%3D1%7D++%28%5Ctheta%5E%7B%281%29%7D_%7Bji%7D%29%5E2\" alt=\"\\frac{\\lambda}{2m}  \\sum^{3}_{i=1}  \\sum^{5}_{j=1}  (\\theta^{(1)}_{ji})^2\" eeimg=\"1\"/> + <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Clambda%7D%7B2m%7D++%5Csum%5E%7B5%7D_%7Bi%3D1%7D++%5Csum%5E%7B5%7D_%7Bj%3D1%7D++%28%5Ctheta%5E%7B%282%29%7D_%7Bji%7D%29%5E2\" alt=\"\\frac{\\lambda}{2m}  \\sum^{5}_{i=1}  \\sum^{5}_{j=1}  (\\theta^{(2)}_{ji})^2\" eeimg=\"1\"/> + <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Clambda%7D%7B2m%7D++%5Csum%5E%7B5%7D_%7Bi%3D1%7D++%5Csum%5E%7B4%7D_%7Bj%3D1%7D++%28%5Ctheta%5E%7B%283%29%7D_%7Bji%7D%29%5E2\" alt=\"\\frac{\\lambda}{2m}  \\sum^{5}_{i=1}  \\sum^{4}_{j=1}  (\\theta^{(3)}_{ji})^2\" eeimg=\"1\"/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-d5e43cc67c6c8a6b8173d702f4282209_b.png\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"949\" data-rawheight=\"85\" class=\"origin_image zh-lightbox-thumb\" width=\"949\" data-original=\"https://pic2.zhimg.com/v2-d5e43cc67c6c8a6b8173d702f4282209_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;949&#39; height=&#39;85&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"949\" data-rawheight=\"85\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"949\" data-original=\"https://pic2.zhimg.com/v2-d5e43cc67c6c8a6b8173d702f4282209_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-d5e43cc67c6c8a6b8173d702f4282209_b.png\"/></figure><p><b>第一项</b> <img src=\"https://www.zhihu.com/equation?tex=%5Csum%5E%7Bs_%7Bl%2B1%7D%7D_%7Bj%3D1%7D\" alt=\"\\sum^{s_{l+1}}_{j=1}\" eeimg=\"1\"/> <b>遍历每一行，第二项</b> <img src=\"https://www.zhihu.com/equation?tex=%5Csum%5E%7Bs_%7Bl%7D%7D_%7Bi%3D1%7D\" alt=\"\\sum^{s_{l}}_{i=1}\" eeimg=\"1\"/> <b>遍历每一列，最后</b> <img src=\"https://www.zhihu.com/equation?tex=%5Csum%5E%7BL-1%7D_%7Bl%3D1%7D\" alt=\"\\sum^{L-1}_{l=1}\" eeimg=\"1\"/> <b>遍历每一层layer，为什么 ？ </b><br/><br/>      最外面 <img src=\"https://www.zhihu.com/equation?tex=%5Csum%5E%7Bs_%7Bl%7D%7D_%7Bi%3D1%7D\" alt=\"\\sum^{s_{l}}_{i=1}\" eeimg=\"1\"/> 遍历每一层layer，这个比较好理解，因为每一层的参数矩阵 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%28l%29%7D\" alt=\"\\theta^{(l)}\" eeimg=\"1\"/> 都是不同的，为了求代价函数，必需加权每一层的参数矩阵；然后里面两层遍历行和列，也很好理解。因为每一层的参数矩阵，实际上影响了每一个输出的类别，所以对于这层参数矩阵，我们需要加权其中的每一个参数项 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_%7Bij%7D\" alt=\"\\theta_{ij}\" eeimg=\"1\"/> ,至于最后是从行遍历到列，还是从列遍历到行，其实都一样。<br/>      还有一点需要注意：i 为什么从1开始 ?因为和逻辑回归中的一样，神经网络模型中的正则化项也一般是从 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_0\" alt=\"\\theta_0\" eeimg=\"1\"/> 开始。故从i = 1开始是为了去除<img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_0\" alt=\"\\theta_0\" eeimg=\"1\"/></p><h2><b>2.反向传播Backpropagation</b></h2><p>      我们之前对神经网络模型的计算用的都是从layer = 1到2到....L的正向逐级计算，即正向传播算法，现在我们先回顾一下正向/前向传播算法：</p><p>还是用1.中的例子，假设神经网络模型中： <img src=\"https://www.zhihu.com/equation?tex=K+%3D+4%2C+S_L+%3D+4%2C+L+%3D+4\" alt=\"K = 4, S_L = 4, L = 4\" eeimg=\"1\"/> ,训练集中只有一个数据实例 <img src=\"https://www.zhihu.com/equation?tex=%28x%5E%7B%281%29%7D%2Cy%5E%7B%281%29%7D%29\" alt=\"(x^{(1)},y^{(1)})\" eeimg=\"1\"/> 。神经网络模型和前向传播算法表示如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-2314dc9b332050f2ccd55b3dbb56ac08_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"819\" data-rawheight=\"473\" class=\"origin_image zh-lightbox-thumb\" width=\"819\" data-original=\"https://pic1.zhimg.com/v2-2314dc9b332050f2ccd55b3dbb56ac08_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;819&#39; height=&#39;473&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"819\" data-rawheight=\"473\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"819\" data-original=\"https://pic1.zhimg.com/v2-2314dc9b332050f2ccd55b3dbb56ac08_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-2314dc9b332050f2ccd55b3dbb56ac08_b.jpg\"/></figure><p><img src=\"https://www.zhihu.com/equation?tex=a%5E%7B%284%29%7D\" alt=\"a^{(4)}\" eeimg=\"1\"/> 即最终的假设函数<img src=\"https://www.zhihu.com/equation?tex=%3Dh_%5Ctheta%28x%29+%3D+g%28z%5E%7B%284%29%7D%29+%3D+g%28%5CTheta%5E%7B%283%29%7D%5Calpha%5E%7B%283%29%7D%29\" alt=\"=h_\\theta(x) = g(z^{(4)}) = g(\\Theta^{(3)}\\alpha^{(3)})\" eeimg=\"1\"/> </p><h2><b>2.1反向传播的意义？</b></h2><p><b>      反向传播就是和之前正向传播算法相对应的，从神经网络模型的最后一层误差开始，逐层往前推导的，反向传播求误差，其实是求模型代价函数偏导</b> <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cvartheta%7D%7B%5Cvartheta%5Ctheta%5E%7B%28l%29%7D_%7Bij%7D%7D+J%28%5CTheta%29\" alt=\"\\frac{\\vartheta}{\\vartheta\\theta^{(l)}_{ij}} J(\\Theta)\" eeimg=\"1\"/> <b>的一种手段，意义就在于比正向传播算法更快速更高效地求出代价函数偏导,在实际模型训练过程中，能大大降低计算时间，使模型更快收敛。关于反向传播算法，更详细的了解请看以下几篇文章：</b></p><p><a href=\"https://zhuanlan.zhihu.com/p/25081671\" class=\"internal\"><span class=\"invisible\">https://</span><span class=\"visible\">zhuanlan.zhihu.com/p/25</span><span class=\"invisible\">081671</span><span class=\"ellipsis\"></span></a><br/><a href=\"https://zhuanlan.zhihu.com/p/21407711\" class=\"internal\"><span class=\"invisible\">https://</span><span class=\"visible\">zhuanlan.zhihu.com/p/21</span><span class=\"invisible\">407711</span><span class=\"ellipsis\"></span></a></p><h2><b>2.2反向传播示例</b></h2><p>      沿用上图的神经网络模型，我们要计算代价函数偏导：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-7268f854dd23632fcb84aa961d3d28b3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"674\" data-rawheight=\"165\" class=\"origin_image zh-lightbox-thumb\" width=\"674\" data-original=\"https://pic4.zhimg.com/v2-7268f854dd23632fcb84aa961d3d28b3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;674&#39; height=&#39;165&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"674\" data-rawheight=\"165\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"674\" data-original=\"https://pic4.zhimg.com/v2-7268f854dd23632fcb84aa961d3d28b3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-7268f854dd23632fcb84aa961d3d28b3_b.jpg\"/></figure><p>      不考虑正则项，假设有m个样本，k项输出，我们从最后一层(输出层)的误差开始计算，假设误差用 <img src=\"https://www.zhihu.com/equation?tex=%5Cdelta\" alt=\"\\delta\" eeimg=\"1\"/> 表示。我们以最后一层第k项为例：</p><p>误差 = 预测值 - 实际值 = <img src=\"https://www.zhihu.com/equation?tex=a_k%5E%7B%284%29%7D+-+y_k%5E%7B%284%29%7D%28k+%3D+1%2C2%2C3%2C4%29\" alt=\"a_k^{(4)} - y_k^{(4)}(k = 1,2,3,4)\" eeimg=\"1\"/> <br/><br/>第4层整体误差用向量表示： <img src=\"https://www.zhihu.com/equation?tex=%5Cdelta%5E%7B%284%29%7D+%3D+a%5E%7B%284%29%7D+-+y\" alt=\"\\delta^{(4)} = a^{(4)} - y\" eeimg=\"1\"/> <br/>第3层的误差： <img src=\"https://www.zhihu.com/equation?tex=%5Cdelta%5E%7B%283%29%7D+%3D+%28%5CTheta%5E%7B%283%29%7D%29%5ET%5Cdelta%5E%7B%284%29%7D%2Ag%27%28z%5E%7B%283%29%7D%29\" alt=\"\\delta^{(3)} = (\\Theta^{(3)})^T\\delta^{(4)}*g&#39;(z^{(3)})\" eeimg=\"1\"/> <br/><b>其中左半部分</b> <img src=\"https://www.zhihu.com/equation?tex=%28%5CTheta%5E%7B%283%29%7D%29%5ET%5Cdelta%5E%7B%284%29%7D\" alt=\"(\\Theta^{(3)})^T\\delta^{(4)}\" eeimg=\"1\"/> <b>表示权重导致的误差的和；右半部分</b> <img src=\"https://www.zhihu.com/equation?tex=g%27%28z%5E%7B%283%29%7D%29+%3Da%5E%7B%283%29%7D+%2A+%281-a%5E%7B%283%29%7D%29\" alt=\"g&#39;(z^{(3)}) =a^{(3)} * (1-a^{(3)})\" eeimg=\"1\"/> <b>是激活函数Sigmoid函数的导数。</b></p><p>具体的解析和推导见下面。<br/>第2层的误差： <img src=\"https://www.zhihu.com/equation?tex=%5Cdelta%5E%7B%282%29%7D+%3D+%28%5CTheta%5E%7B%282%29%7D%29%5ET%5Cdelta%5E%7B%283%29%7D%2Ag%27%28z%5E%7B%282%29%7D%29\" alt=\"\\delta^{(2)} = (\\Theta^{(2)})^T\\delta^{(3)}*g&#39;(z^{(2)})\" eeimg=\"1\"/> <br/>第1层误差：由于第一层为输入层，输入层输入的是实际的训练集实例，不存在误差。</p><h3><b>2.2.1误差公式解析</b></h3><p><b>公式解析：</b><br/> <img src=\"https://www.zhihu.com/equation?tex=%5Cdelta%5E%7B%284%29%7D+%3D+a%5E%7B%284%29%7D+-+y\" alt=\"\\delta^{(4)} = a^{(4)} - y\" eeimg=\"1\"/> 表示第4层误差(4 x 1);</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cdelta%5E%7B%283%29%7D\" alt=\"\\delta^{(3)}\" eeimg=\"1\"/> 表示第3层的误差矩阵（5 x 1）；<br/> <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%283%29%7D\" alt=\"\\theta^{(3)}\" eeimg=\"1\"/> 表示第3层权重/参数，形态为：4 x 5(不考虑bias)；</p><p><br/>第3层的误差： <img src=\"https://www.zhihu.com/equation?tex=%5Cdelta%5E%7B%283%29%7D+%3D+%28%5CTheta%5E%7B%283%29%7D%29%5ET%5Cdelta%5E%7B%284%29%7D%2Ag%27%28z%5E%7B%283%29%7D%29\" alt=\"\\delta^{(3)} = (\\Theta^{(3)})^T\\delta^{(4)}*g&#39;(z^{(3)})\" eeimg=\"1\"/> 是怎么来的 ？第3层误差和第4层误差的关系是怎么样的？为了方便理解，举个不是很严谨的小例子：<br/><br/><br/>      我们现在思考下第3层误差和第4层误差的关系，这里假设 <img src=\"https://www.zhihu.com/equation?tex=%5Cdelta%5E%7B%284%29%7D%3D%5Cbegin%7BBmatrix%7D+1000%5C%5C++2000%5C%5C+3000%5C%5C+4000+%5Cend%7BBmatrix%7D\" alt=\"\\delta^{(4)}=\\begin{Bmatrix} 1000\\\\  2000\\\\ 3000\\\\ 4000 \\end{Bmatrix}\" eeimg=\"1\"/> ，求第3层第一个激活单元 <img src=\"https://www.zhihu.com/equation?tex=a%5E%7B%283%29%7D_1\" alt=\"a^{(3)}_1\" eeimg=\"1\"/> 的误差 ?  已知<img src=\"https://www.zhihu.com/equation?tex=a%5E%7B%283%29%7D_1\" alt=\"a^{(3)}_1\" eeimg=\"1\"/>到第4层4个激活单元的权重分别为：0.1，0.2，0.3，0.4则<img src=\"https://www.zhihu.com/equation?tex=a%5E%7B%283%29%7D_1\" alt=\"a^{(3)}_1\" eeimg=\"1\"/></p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cdelta%5E%7B%283%29%7D_1\" alt=\"\\delta^{(3)}_1\" eeimg=\"1\"/> = 0.1*1000 + 0.2*2000 + 0.3*3000 + 0.4*4000 = 100+400+900+1600 = 3000<br/><br/>      如果不考虑到误差的变化情况(即不考虑其导数项)， <img src=\"https://www.zhihu.com/equation?tex=%5Cdelta%5E%7B%283%29%7D+%3D+%28%5CTheta%5E%7B%283%29%7D%29%5ET%5Cdelta%5E%7B%284%29%7D\" alt=\"\\delta^{(3)} = (\\Theta^{(3)})^T\\delta^{(4)}\" eeimg=\"1\"/> 就可以表示第3层的损失了，为什么加上后面的 <img src=\"https://www.zhihu.com/equation?tex=g%27%28z%5E%7B%283%29%7D%29\" alt=\"g&#39;(z^{(3)})\" eeimg=\"1\"/> ？考虑到误差和预测值的变化率相同，故 <img src=\"https://www.zhihu.com/equation?tex=%5Cdelta%5E%7B%283%29%7D+%3D+%28%5CTheta%5E%7B%283%29%7D%29%5ET%5Cdelta%5E%7B%284%29%7D%2Ag%27%28z%5E%7B%283%29%7D%29\" alt=\"\\delta^{(3)} = (\\Theta^{(3)})^T\\delta^{(4)}*g&#39;(z^{(3)})\" eeimg=\"1\"/> 中误差的偏导项可以用预测值的导数项来表示，即式子里右半边表示 <img src=\"https://www.zhihu.com/equation?tex=g%27%28z%5E%7B%283%29%7D%29+%3D+%5Ba%5E%7B%283%29%7D%5D%27\" alt=\"g&#39;(z^{(3)}) = [a^{(3)}]&#39;\" eeimg=\"1\"/> </p><h3><b>2.2.2误差公式推导</b></h3><p><b>公式推导：</b><br/> <img src=\"https://www.zhihu.com/equation?tex=h_%5Ctheta%28z%29+%3D+g%28z%29+%3D+%5Cfrac%7B1%7D%7B1%2Be%5E%7B-z%7D%7D\" alt=\"h_\\theta(z) = g(z) = \\frac{1}{1+e^{-z}}\" eeimg=\"1\"/> ,令 <img src=\"https://www.zhihu.com/equation?tex=f%28z%29+%3D+%281%2Be%5E%7B-z%7D%29\" alt=\"f(z) = (1+e^{-z})\" eeimg=\"1\"/> ，则： <img src=\"https://www.zhihu.com/equation?tex=g%28z%29+%3D+%5Cfrac%7B1%7D%7Bf%28z%29%7D\" alt=\"g(z) = \\frac{1}{f(z)}\" eeimg=\"1\"/> </p><p class=\"ztext-empty-paragraph\"><br/></p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Bcases%7D+f%27%28z%29+%3D+%281%2Be%5E%7B-z%7D%29%27%3D-e%5E%7B-z%7D+%3D+1-f%28z%29++%5C%5C%5B2ex%5D+g%27%28z%29+%3D+%5B%5Cfrac%7B1%7D%7Bf%28z%29%7D%5D%27%3D%5Cfrac%7B-1%7D%7Bf%5E2%28z%29%7D%2Af%27%28z%29+%5Cend%7Bcases%7D\" alt=\"\\begin{cases} f&#39;(z) = (1+e^{-z})&#39;=-e^{-z} = 1-f(z)  \\\\[2ex] g&#39;(z) = [\\frac{1}{f(z)}]&#39;=\\frac{-1}{f^2(z)}*f&#39;(z) \\end{cases}\" eeimg=\"1\"/> </p><p><img src=\"https://www.zhihu.com/equation?tex=%5CLongrightarrow++++g%27%28z%29+%3D+%5Cfrac%7Bf%28z%29-1%7D%7Bf%5E2%28z%29%7D+%3D%5Cfrac%7Bf%28z%29-1%7D%7Bf%28z%29%7D+%2A%5Cfrac%7B1%7D%7Bf%28z%29%7D+%3D+%281-g%28z%29%29%2Ag%28z%29\" alt=\"\\Longrightarrow    g&#39;(z) = \\frac{f(z)-1}{f^2(z)} =\\frac{f(z)-1}{f(z)} *\\frac{1}{f(z)} = (1-g(z))*g(z)\" eeimg=\"1\"/> </p><p class=\"ztext-empty-paragraph\"><br/></p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cbegin%7Bcases%7D+g%27%28t%29+%3D+%281-g%28t%29%29%2Ag%28t%29++%5C%5C%5B2ex%5D+g%28z%5E%7B%283%29%7D%29+%3D+a%5E%7B%283%29%7D+%5Cend%7Bcases%7D\" alt=\"\\begin{cases} g&#39;(t) = (1-g(t))*g(t)  \\\\[2ex] g(z^{(3)}) = a^{(3)} \\end{cases}\" eeimg=\"1\"/> <img src=\"https://www.zhihu.com/equation?tex=%5CLongrightarrow++g%27%28z%5E%7B%283%29%7D%29+%3Da%5E%7B%283%29%7D+%2A+%281-a%5E%7B%283%29%7D%29\" alt=\"\\Longrightarrow  g&#39;(z^{(3)}) =a^{(3)} * (1-a^{(3)})\" eeimg=\"1\"/> </p><h2><b>2.3反向传播算法</b></h2><h3><b>2.3.1偏导公式</b></h3><p>      上面2.1小结，我们得出了每一层的误差，通过公式推导，我们可以将代价函数的偏导用这些误差表示。此处，不考虑正则化项，即 <img src=\"https://www.zhihu.com/equation?tex=%5Clambda+%3D0\" alt=\"\\lambda =0\" eeimg=\"1\"/> 我们可以得到代价函数的偏导：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cvartheta%7D%7B%5Cvartheta%5Ctheta%5E%7B%28l%29%7D_%7Bij%7D%7D+J%28%5CTheta%29+%3D+a_j%5E%7B%28l%29%7D%5Cdelta%5E%7B%28l%2B1%29%7D_i\" alt=\"\\frac{\\vartheta}{\\vartheta\\theta^{(l)}_{ij}} J(\\Theta) = a_j^{(l)}\\delta^{(l+1)}_i\" eeimg=\"1\"/> </p><p>式中:</p><p><img src=\"https://www.zhihu.com/equation?tex=l\" alt=\"l\" eeimg=\"1\"/> 表示的是神经网络层数;<br/><img src=\"https://www.zhihu.com/equation?tex=j\" alt=\"j\" eeimg=\"1\"/> 表示在当前层下，激活单元的下标;<br/><img src=\"https://www.zhihu.com/equation?tex=i\" alt=\"i\" eeimg=\"1\"/> 有点特殊，代表下一层中误差单元的下标，是受到权重矩阵中第 <img src=\"https://www.zhihu.com/equation?tex=i\" alt=\"i\" eeimg=\"1\"/> 行影响的下一层中的误差单元的下标</p><p>我们获取了所有误差以后，通过这个偏导公式，我们可以计算出每一个激活单元的偏导。</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cvartheta%7D%7B%5Cvartheta%5Ctheta%5E%7B%28l%29%7D_%7Bij%7D%7D+J%28%5CTheta%29+%3D+a_j%5E%7B%28l%29%7D%5Cdelta%5E%7B%28l%2B1%29%7D_i\" alt=\"\\frac{\\vartheta}{\\vartheta\\theta^{(l)}_{ij}} J(\\Theta) = a_j^{(l)}\\delta^{(l+1)}_i\" eeimg=\"1\"/> </p><p>具体推导过程可以参考知乎文章：<br/><a href=\"https://zhuanlan.zhihu.com/p/58068618\" class=\"internal\"><span class=\"invisible\">https://</span><span class=\"visible\">zhuanlan.zhihu.com/p/58</span><span class=\"invisible\">068618</span><span class=\"ellipsis\"></span></a> </p><h3><b>2.3.2算法公式</b></h3><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-39da7a025711c5d1a0f1676a9ead04e2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"852\" data-rawheight=\"411\" class=\"origin_image zh-lightbox-thumb\" width=\"852\" data-original=\"https://pic3.zhimg.com/v2-39da7a025711c5d1a0f1676a9ead04e2_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;852&#39; height=&#39;411&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"852\" data-rawheight=\"411\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"852\" data-original=\"https://pic3.zhimg.com/v2-39da7a025711c5d1a0f1676a9ead04e2_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-39da7a025711c5d1a0f1676a9ead04e2_b.jpg\"/></figure><p>当我们训练集有m个样本点时，我们会遍历每个样本<b>For i = 1 to m  </b>第一层激活单元的输入值 <img src=\"https://www.zhihu.com/equation?tex=a%5E%7B%281%29%7D+%3D+x%5E%7B%281%29%7D\" alt=\"a^{(1)} = x^{(1)}\" eeimg=\"1\"/> 我们先用正向传播算法，计算出每一层的 <img src=\"https://www.zhihu.com/equation?tex=a%5E%7B%28l%29%7D\" alt=\"a^{(l)}\" eeimg=\"1\"/> ，然后用反向传播计算出每一层（从 <img src=\"https://www.zhihu.com/equation?tex=%5Cdelta%5E%7B%28L-1%29%7D%2C%5Cdelta%5E%7B%28L-2%29%7D...%5Cdelta%5E%7B%282%29%7D\" alt=\"\\delta^{(L-1)},\\delta^{(L-2)}...\\delta^{(2)}\" eeimg=\"1\"/> ）的误差矩阵，最后我们会对每一个节点的误差做迭代：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5CDelta%5E%7B%28l%29%7D_%7B%28ij%29%7D+%3A%3D+%5CDelta%5E%7B%28l%29%7D_%7B%28ij%29%7D+%2B+a_j%5E%7B%28l%29%7D%5Cdelta%5E%7B%28l%2B1%29%7D_+i\" alt=\"\\Delta^{(l)}_{(ij)} := \\Delta^{(l)}_{(ij)} + a_j^{(l)}\\delta^{(l+1)}_ i\" eeimg=\"1\"/> </p><p>注：这里的 <img src=\"https://www.zhihu.com/equation?tex=%5CDelta\" alt=\"\\Delta\" eeimg=\"1\"/> 是 <img src=\"https://www.zhihu.com/equation?tex=%5Cdelta\" alt=\"\\delta\" eeimg=\"1\"/> 的大写形式。迭代求出 <img src=\"https://www.zhihu.com/equation?tex=%5CDelta%5E%7B%28l%29%7D_%7B%28ij%29%7D\" alt=\"\\Delta^{(l)}_{(ij)}\" eeimg=\"1\"/> 后，我们就可以计算代价函数的偏导数了，计算方法如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-5fa75d2942b2c52e09e9fac8bf8a1f33_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"265\" data-rawheight=\"121\" class=\"content_image\" width=\"265\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;265&#39; height=&#39;121&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"265\" data-rawheight=\"121\" class=\"content_image lazy\" width=\"265\" data-actualsrc=\"https://pic4.zhimg.com/v2-5fa75d2942b2c52e09e9fac8bf8a1f33_b.jpg\"/></figure><p>得到的 <img src=\"https://www.zhihu.com/equation?tex=D%5E%7B%28l%29%7D_%7Bij%7D\" alt=\"D^{(l)}_{ij}\" eeimg=\"1\"/> 即得到了偏导，即： <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cvartheta%7D%7B%5Cvartheta%5Ctheta%5E%7B%28l%29%7D_%7Bij%7D%7D+J%28%5CTheta%29+%3D+D%5E%7B%28l%29%7D_%7Bij%7D\" alt=\"\\frac{\\vartheta}{\\vartheta\\theta^{(l)}_{ij}} J(\\Theta) = D^{(l)}_{ij}\" eeimg=\"1\"/> </p><h3><b>2.3.3进一步理解</b></h3><p>先看一个例子：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-6a6658db78620eb6ccca6fd6129b7634_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"715\" data-rawheight=\"344\" class=\"origin_image zh-lightbox-thumb\" width=\"715\" data-original=\"https://pic1.zhimg.com/v2-6a6658db78620eb6ccca6fd6129b7634_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;715&#39; height=&#39;344&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"715\" data-rawheight=\"344\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"715\" data-original=\"https://pic1.zhimg.com/v2-6a6658db78620eb6ccca6fd6129b7634_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-6a6658db78620eb6ccca6fd6129b7634_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>假设 <img src=\"https://www.zhihu.com/equation?tex=%5Cdelta%5E%7B%283%29%7D\" alt=\"\\delta^{(3)}\" eeimg=\"1\"/> 已知,求 <img src=\"https://www.zhihu.com/equation?tex=%5Cdelta%5E%7B%282%29%7D_2\" alt=\"\\delta^{(2)}_2\" eeimg=\"1\"/> ?</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cdelta%5E%7B%282%29%7D_2\" alt=\"\\delta^{(2)}_2\" eeimg=\"1\"/> 是第二层中第二个激活单元的误差（+1项不看），可见这个激活单元向第3层传递有两条路径：蓝色线和红色线，蓝线权重用 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%282%29%7D_%7B12%7D\" alt=\"\\theta^{(2)}_{12}\" eeimg=\"1\"/> 表示，红线权重用 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta%5E%7B%282%29%7D_%7B22%7D\" alt=\"\\theta^{(2)}_{22}\" eeimg=\"1\"/> 表示。则：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cdelta%5E%7B%282%29%7D_2%3D%5Ctheta%5E%7B%282%29%7D_%7B12%7D%5Cdelta%5E%7B%283%29%7D_1%2B%5Ctheta%5E%7B%282%29%7D_%7B22%7D%5Cdelta%5E%7B%283%29%7D_2\" alt=\"\\delta^{(2)}_2=\\theta^{(2)}_{12}\\delta^{(3)}_1+\\theta^{(2)}_{22}\\delta^{(3)}_2\" eeimg=\"1\"/> <br/><br/>然后，我们思考一下 <img src=\"https://www.zhihu.com/equation?tex=%5Cdelta%5E%7B%28l%29%7D_j%E5%92%8CJ%28%5Ctheta%29\" alt=\"\\delta^{(l)}_j和J(\\theta)\" eeimg=\"1\"/> 的关系，这里先看一下代价函数：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-7268f854dd23632fcb84aa961d3d28b3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"674\" data-rawheight=\"165\" class=\"origin_image zh-lightbox-thumb\" width=\"674\" data-original=\"https://pic4.zhimg.com/v2-7268f854dd23632fcb84aa961d3d28b3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;674&#39; height=&#39;165&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"674\" data-rawheight=\"165\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"674\" data-original=\"https://pic4.zhimg.com/v2-7268f854dd23632fcb84aa961d3d28b3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-7268f854dd23632fcb84aa961d3d28b3_b.jpg\"/></figure><p><br/>我们简化一下，先不考虑正则化项，且数据集样本点只有一个(x,y)则函数变成:</p><p><img src=\"https://www.zhihu.com/equation?tex=J%28%5Ctheta%29%3D+Cost%28h_%5Ctheta%28x%29+%2C+y%29+%3D+y%2Alog%28h_%5Ctheta%28x%29%2B%281-y%29%2Alog%281-h_%5Ctheta%28x%29%29\" alt=\"J(\\theta)= Cost(h_\\theta(x) , y) = y*log(h_\\theta(x)+(1-y)*log(1-h_\\theta(x))\" eeimg=\"1\"/> </p><blockquote>更进一步，可以想象下代价函数描述的是假设和真实值之间的偏差（用方差表示）： <img src=\"https://www.zhihu.com/equation?tex=J%28%5Ctheta%29%3D+Cost%28h_%5Ctheta%28x%29+%2C+y%29+%5Capprox++%5Cfrac%7B%28h_%5Ctheta%28x%29-y%29%5E2%7D%7B2%7D\" alt=\"J(\\theta)= Cost(h_\\theta(x) , y) \\approx  \\frac{(h_\\theta(x)-y)^2}{2}\" eeimg=\"1\"/> </blockquote><p>在之前，我们对代价函数求权重的偏导可得出：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cvartheta%7D%7B%5Cvartheta%5Ctheta%5E%7B%28l%29%7D_%7Bij%7D%7D+J%28%5CTheta%29+%3D+a_j%5E%7B%28l%29%7D%5Cdelta%5E%7B%28l%2B1%29%7D_i\" alt=\"\\frac{\\vartheta}{\\vartheta\\theta^{(l)}_{ij}} J(\\Theta) = a_j^{(l)}\\delta^{(l+1)}_i\" eeimg=\"1\"/> </p><p>这里 <img src=\"https://www.zhihu.com/equation?tex=z+%3D+h_%5Ctheta%28x%29+%3D+%5CTheta+a\" alt=\"z = h_\\theta(x) = \\Theta a\" eeimg=\"1\"/> 故有：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cvartheta+J%28%5CTheta%29%7D%7B%5Cvartheta+z%7D++%3D+%5Cfrac%7B%5Cvartheta+J%28%5CTheta%29%7D%7B%5Cvartheta+%5Ctheta%7D%2A+%5Cfrac%7B%5Cvartheta+%5Ctheta%7D%7B%5Cvartheta+z%7D+%3D++a%5Cdelta%2A%5Cfrac%7B%5Cvartheta+%5Ctheta%7D%7B%5Cvartheta+z%7D+%3D+a%5Cdelta%2A%281%2Fa%29+%3D+%5Cdelta\" alt=\"\\frac{\\vartheta J(\\Theta)}{\\vartheta z}  = \\frac{\\vartheta J(\\Theta)}{\\vartheta \\theta}* \\frac{\\vartheta \\theta}{\\vartheta z} =  a\\delta*\\frac{\\vartheta \\theta}{\\vartheta z} = a\\delta*(1/a) = \\delta\" eeimg=\"1\"/> </p><p>从公式可知误差 = 损失函数对假设函数的偏导。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-6a12f1c1c57e324b3a53a23a3ea9a88f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"557\" data-rawheight=\"132\" class=\"origin_image zh-lightbox-thumb\" width=\"557\" data-original=\"https://pic4.zhimg.com/v2-6a12f1c1c57e324b3a53a23a3ea9a88f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;557&#39; height=&#39;132&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"557\" data-rawheight=\"132\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"557\" data-original=\"https://pic4.zhimg.com/v2-6a12f1c1c57e324b3a53a23a3ea9a88f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-6a12f1c1c57e324b3a53a23a3ea9a88f_b.jpg\"/></figure><h2><b>3.梯度检验</b></h2><p>      当我们对一个较为复杂的模型（例如神经网络）使用梯度下降算法时，可能会存在一些不容易察觉的错误，意味着，虽然代价看上去在不断减小，但最终的结果可能并不是最优解。<br/>      为了避免这样的问题，我们采取一种叫做<b>梯度的数值检验（Numerical Gradient Checking）</b>方法。这种方法的思想是通过估计梯度值来检验我们计算的导数值是否真的是我们要求的。对梯度的估计采用的<b>方法是在代价函数上沿着切线的方向选择离两个非常近的点然后计算两个点的平均值用以估计梯度。</b> </p><h2><b>4.随机初始化</b></h2><p>      任何优化算法都需要一些初始的参数。到目前为止我们都是初始所有参数为 0，这样的初始方法对于逻辑回归来说是可行的，但是对于神经网络来说是不可行的。如果我们令所有的初始参数都为 0，这将意味着我们第二层的所有激活单元都会有相同的值。同理，如果我们初始所有的参数都为一个非 0 的数，结果也是一样的。<br/> <b>我们通常初始参数为正负</b> <img src=\"https://www.zhihu.com/equation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"/> <b>之间的随机值。</b><img src=\"https://www.zhihu.com/equation?tex=%5Cepsilon\" alt=\"\\epsilon\" eeimg=\"1\"/> <b>通常是一个非常小的值，如0.001  </b></p><h2><b>5.总结概述</b></h2><p>总结一下使用神经网络时的步骤：</p><p><b>网络结构：</b><br/>第一件要做的事是选择网络结构，即决定选择多少层以及决定每层分别有多少个单元。<br/>第一层的单元数即我们训练集的特征数量。最后一层的单元数是我们训练集的结果的类的数量。<br/>如果隐藏层数大于 1，确保每个隐藏层的单元个数相同，通常情况下隐藏层单元的个数越多越好。</p><p><b>我们真正要决定的是隐藏层的层数和每个中间层的单元数。</b></p><p><b>训练神经网络：</b><br/>1. 参数的随机初始化<br/>2. 利用正向传播方法计算所有的 <img src=\"https://www.zhihu.com/equation?tex=h_%5Ctheta%28x%29\" alt=\"h_\\theta(x)\" eeimg=\"1\"/> <br/>3. 编写计算代价函数 <img src=\"https://www.zhihu.com/equation?tex=J%28%5Ctheta%29\" alt=\"J(\\theta)\" eeimg=\"1\"/> 的代码<br/>4. 利用反向传播方法计算所有偏导数<br/>5. 利用数值检验方法检验这些偏导数<br/>6. 使用优化算法来最小化代价函数</p>", 
            "topic": [
                {
                    "tag": "反向传播", 
                    "tagLink": "https://api.zhihu.com/topics/20682860"
                }, 
                {
                    "tag": "神经网络", 
                    "tagLink": "https://api.zhihu.com/topics/19607065"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }
            ], 
            "comments": [
                {
                    "userName": "也无风雨也无情", 
                    "userLink": "https://www.zhihu.com/people/39756da3f51a0b3c579588baca217f94", 
                    "content": "<p>西瓜书上的神经网络搞明白了，却怎么都看不懂吴恩达讲的神经网络</p><a class=\"comment_sticker\" href=\"https://pic2.zhimg.com/v2-e213ddb29e5a2adb54d6343d5dea27d1.gif\" data-width=\"\" data-height=\"\">[摊手]</a>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "Lyon", 
                            "userLink": "https://www.zhihu.com/people/38495a9b1f83e4a612543ce6c523df7f", 
                            "content": "<a href=\"https://pic4.zhimg.com/v2-fa3cb6bc9ec57da84ab53a60f48d0c6f.gif\" class=\"comment_sticker\" data-width=\"0\" data-height=\"0\" data-sticker-id=\"951517103955070976\">[棒]</a>", 
                            "likes": 0, 
                            "replyToAuthor": "也无风雨也无情"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/73665825", 
            "userName": "Lyon", 
            "userLink": "https://www.zhihu.com/people/38495a9b1f83e4a612543ce6c523df7f", 
            "upvote": 4, 
            "title": "【吴恩达机器学习】第四周—前馈神经网络", 
            "content": "<p><b>前言：</b></p><p>最近在学习深度学习，看了不少教程，发现还是吴恩达的比较适用于我。吴恩达机器学习公开课视频最早是斯坦福大学的课程视频(那个画面有点老)，新版的视频在网易云课堂上可以随时学习。仅仅通过视频学习，可能会有点快，因为有的知识点需要反复推敲和回味。感谢github上一位朋友的精心整理，让我们可以配合讲义一起学习，讲义有HTML版、PDF版、Markdown版的：视频配合讲义看，事半功倍</p><p>本文包括接下来的系列文章，都会是吴恩达机器学习课程的个人学习总结，有很多文字和图片直接参考了PDF版的课程讲义，再次谢过提供讲义的朋友！</p><p>网易云课堂课程视频：</p><p><a href=\"https://link.zhihu.com/?target=https%3A//study.163.com/course/courseMain.htm%3FcourseId%3D1004570029\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">study.163.com/course/co</span><span class=\"invisible\">urseMain.htm?courseId=1004570029</span><span class=\"ellipsis\"></span></a></p><p>课程同步配套讲义：</p><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/fengdu78/Coursera-ML-AndrewNg-Notes\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/fengdu78/Cou</span><span class=\"invisible\">rsera-ML-AndrewNg-Notes</span><span class=\"ellipsis\"></span></a></p><p>原文发布在语雀文档上,效果更好看点：</p><a href=\"https://link.zhihu.com/?target=https%3A//www.yuque.com/docs/share/7a788374-f5a2-4550-949c-8fad37c66c14\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-72bbb1e26eacf07192c32d1b6514dde0_ipico.jpg\" data-image-width=\"512\" data-image-height=\"512\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">第四周 · 语雀</a><hr/><h2><b>第四周</b></h2><h2><b>1. 神经元</b></h2><h2><b>1.1 神经元</b></h2><p>为了构建神经网络模型，我们需要首先思考大脑中的神经网络是怎样的？每一个神经元都可以被认为是一个处理单元/神经核（processing unit/Nucleus），它含有许多<b>输入/树突（input/Dendrite）</b>，并且有一个<b>输出/轴突（output/Axon）</b>。神经网络是大量神经元相互链接并通过电脉冲来交流的一个网络。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-bc5968e2a50a0de6c85c819f20421cfe_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"428\" data-rawheight=\"242\" class=\"origin_image zh-lightbox-thumb\" width=\"428\" data-original=\"https://pic3.zhimg.com/v2-bc5968e2a50a0de6c85c819f20421cfe_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;428&#39; height=&#39;242&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"428\" data-rawheight=\"242\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"428\" data-original=\"https://pic3.zhimg.com/v2-bc5968e2a50a0de6c85c819f20421cfe_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-bc5968e2a50a0de6c85c819f20421cfe_b.jpg\"/></figure><h2><b>1.2 神经元间的沟通</b></h2><p>      神经元之间通过电信号进行沟通(微弱的电流)。所以如果神经元想要传递一个消息，它就会就通过它的轴突，发送一段微弱电流给其他神经元的树突。<br/>      接收到信号的神经元会对消息进行处理，处理后可能会通过自己的轴突再将信息传递出去给其他神经元。这就是神经元之间信号传递的简要概述。</p><h2><b>2. 神经网络</b></h2><p>      神经网络是模仿大脑神经元，建立的模型。模型中的每个神经元都是一个单独的【学习模型】，这些神经元也叫做<b>激活单元(activation unit)</b></p><h2><b>2.1 Sigmoid神经元</b></h2><p>      以下示例为逻辑回归模型中常用的，sigmoid神经元的简单示例：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-c22c03a076530f0e3c944b3deb580eef_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"425\" data-rawheight=\"268\" class=\"origin_image zh-lightbox-thumb\" width=\"425\" data-original=\"https://pic4.zhimg.com/v2-c22c03a076530f0e3c944b3deb580eef_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;425&#39; height=&#39;268&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"425\" data-rawheight=\"268\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"425\" data-original=\"https://pic4.zhimg.com/v2-c22c03a076530f0e3c944b3deb580eef_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-c22c03a076530f0e3c944b3deb580eef_b.jpg\"/></figure><p>      中间的神经元(激活单元)是一个sigmoid函数，它将左边输入的 <img src=\"https://www.zhihu.com/equation?tex=x_1%2Cx_2%2Cx_3\" alt=\"x_1,x_2,x_3\" eeimg=\"1\"/> 和参数<i>θ</i>相乘后作为输入，经过自身的计算得到结果 <img src=\"https://www.zhihu.com/equation?tex=h_%5Ctheta%28x%29\" alt=\"h_\\theta(x)\" eeimg=\"1\"/> ，在神经网络中，参数又可被成为<b>权重（weight）</b>。</p><h2><b>2.2 三层神经网络</b></h2><p>      在Sigmod神经元的基础上，我们可以设计出一个简单的三层神经网络：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-b105e76fdee14effa1115ee4bb6b2992_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"395\" data-rawheight=\"190\" class=\"content_image\" width=\"395\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;395&#39; height=&#39;190&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"395\" data-rawheight=\"190\" class=\"content_image lazy\" width=\"395\" data-actualsrc=\"https://pic3.zhimg.com/v2-b105e76fdee14effa1115ee4bb6b2992_b.jpg\"/></figure><p><img src=\"https://www.zhihu.com/equation?tex=x_1%2Cx_2%2Cx_3\" alt=\"x_1,x_2,x_3\" eeimg=\"1\"/> 作为神经网络的第一层，被称为<b>输入层(Input Layer)</b>，输入层中的每个节点即输入单元，每个输入单元即包含着原始的输入值，<img src=\"https://www.zhihu.com/equation?tex=a_1%2Ca_2%2Ca_3\" alt=\"a_1,a_2,a_3\" eeimg=\"1\"/>作为中间单元，构成了神经网络的第二层，也称为中间层/<b>隐藏层(Hidden Layers)，</b>他们的作用是：负责将数据进行处理，然后呈递到下一层；最后一层被称为<b>输出层（Output Layer）</b><br/> <b>神经网络模型中，通常只有一个输入层，一个输出层，中间层/隐藏层可以有任意多个。所有层加起来构成了整个神经网络模型。每一层的输出变量都是下一层的输入变量。</b><br/>      对于上述模型，给每一层添加<b>偏差单位（bias unit）</b>后，图像如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-a91e9d5b23a3596522e86ad2542b6391_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"401\" data-rawheight=\"221\" class=\"content_image\" width=\"401\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;401&#39; height=&#39;221&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"401\" data-rawheight=\"221\" class=\"content_image lazy\" width=\"401\" data-actualsrc=\"https://pic2.zhimg.com/v2-a91e9d5b23a3596522e86ad2542b6391_b.jpg\"/></figure><p><b>中间层激活单元的表达式：</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-a1f79b050a38cc8dd6b0bc71906f0cfe_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"61\" data-rawheight=\"22\" class=\"content_image\" width=\"61\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;61&#39; height=&#39;22&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"61\" data-rawheight=\"22\" class=\"content_image lazy\" width=\"61\" data-actualsrc=\"https://pic3.zhimg.com/v2-a1f79b050a38cc8dd6b0bc71906f0cfe_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-aaf37c2b6ae45edb46274ec8b8c18f4a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"367\" data-rawheight=\"164\" class=\"content_image\" width=\"367\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;367&#39; height=&#39;164&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"367\" data-rawheight=\"164\" class=\"content_image lazy\" width=\"367\" data-actualsrc=\"https://pic3.zhimg.com/v2-aaf37c2b6ae45edb46274ec8b8c18f4a_b.jpg\"/></figure><p><img src=\"https://www.zhihu.com/equation?tex=a_i%5E%7B%28j%29%7D\" alt=\"a_i^{(j)}\" eeimg=\"1\"/> 代表第i层的第j个激活单元；</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_%7Bmn%7D%5E%7B%28j%29%7D\" alt=\"\\theta_{mn}^{(j)}\" eeimg=\"1\"/>右上角 j 代表是从第j层映射到第 j + 1层时权重的矩阵；右下角<b><i>mn</i></b>代表该参数在<b><i>θ</i></b>矩阵中的位置.<b><i>其中，θ </i></b>矩阵的形态m = 第 j + 1层激活单元数 ；n =  第 j 层激活单元数 + 1</p><p>      例如：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_%7Bmn%7D%5E%7B%281%29%7D\" alt=\"\\theta_{mn}^{(1)}\" eeimg=\"1\"/> 第一层<b><i>θ</i></b>参数矩阵的形态<b><i>3 x 4</i></b></p><p><b><i>            m </i></b>= 第二层激活单元数 = 3；<b><i>n =</i></b> 第一层激活单元数 3 + 1 = 4</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_%7Bmn%7D%5E%7B%282%29%7D\" alt=\"\\theta_{mn}^{(2)}\" eeimg=\"1\"/> 第二层<b><i>θ</i></b>参数矩阵的形态<b>1<i> x 4</i></b></p><p><b><i>            m </i></b>= 第三层激活单元数 = 1；<b><i>n =</i></b> 第二层激活单元数 3 + 1 = 4</p><p><b>输入层表达式：</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-f9f733e89bdb73097dbe42a81616aec7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"486\" data-rawheight=\"119\" class=\"origin_image zh-lightbox-thumb\" width=\"486\" data-original=\"https://pic4.zhimg.com/v2-f9f733e89bdb73097dbe42a81616aec7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;486&#39; height=&#39;119&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"486\" data-rawheight=\"119\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"486\" data-original=\"https://pic4.zhimg.com/v2-f9f733e89bdb73097dbe42a81616aec7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-f9f733e89bdb73097dbe42a81616aec7_b.jpg\"/></figure><p>把 <img src=\"https://www.zhihu.com/equation?tex=x%2C%5Ctheta%2C%5Calpha\" alt=\"x,\\theta,\\alpha\" eeimg=\"1\"/> 分别用矩阵表示，我们可以得到： <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta+X%3D+a\" alt=\"\\theta X= a\" eeimg=\"1\"/></p><p><b>输出层表达式：</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-b0d457385a640a7255a75a7139c1d7fc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"418\" data-rawheight=\"54\" class=\"content_image\" width=\"418\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;418&#39; height=&#39;54&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"418\" data-rawheight=\"54\" class=\"content_image lazy\" width=\"418\" data-actualsrc=\"https://pic1.zhimg.com/v2-b0d457385a640a7255a75a7139c1d7fc_b.jpg\"/></figure><p>      从式子里可以看出，对于每个 <img src=\"https://www.zhihu.com/equation?tex=a_i\" alt=\"a_i\" eeimg=\"1\"/> ，都依赖上层的x以及x对应的参数θ，这样从左至右逐级依赖的算法模型，称为：<b>前向传播算法( FORWARD PROPAGATION )）</b></p><h2><b>2.3 神经网络的优势</b></h2><p>      从2.2节我们可以看到，位于神经网络模型输出层的预测函数可以写作：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-b0d457385a640a7255a75a7139c1d7fc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"418\" data-rawheight=\"54\" class=\"content_image\" width=\"418\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;418&#39; height=&#39;54&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"418\" data-rawheight=\"54\" class=\"content_image lazy\" width=\"418\" data-actualsrc=\"https://pic1.zhimg.com/v2-b0d457385a640a7255a75a7139c1d7fc_b.jpg\"/></figure><p>     我们可以把 <img src=\"https://www.zhihu.com/equation?tex=a_0%2Ca_1%2Ca_2%2Ca_3\" alt=\"a_0,a_1,a_2,a_3\" eeimg=\"1\"/> 看成更为高级的特征值，也就是<img src=\"https://www.zhihu.com/equation?tex=x_0%2Cx_1%2Cx_2%2Cx_3\" alt=\"x_0,x_1,x_2,x_3\" eeimg=\"1\"/>的进化体，虽然他们是由x决定的，但由于是梯度下降的，所以a是变化的，并且变化的越来越厉害，所以这些高级的特征值比普通的逻辑回归/线性回归模型中仅仅将x次方厉害，也能更好地预测新数据。<br/> <b>从本质上来讲，神经网络能够通过学习得出其自身的一系列特征。</b>在普通的逻辑回归中，我们被限制为使用数据中的原始特征 <img src=\"https://www.zhihu.com/equation?tex=x_1%2Cx_2...x_n\" alt=\"x_1,x_2...x_n\" eeimg=\"1\"/> .我们虽然可以使用一些二项式项来组合这些特征，但是我们仍然受到这些原始特征的限制。在神经网络中，原始特征只是输入层，在我们上面三层的神经网络例子中，第三层也就是输出层做出的预测利用的是第二层的特征，而非输入层中的原始特征，我们可以认为第二层中的特征是神经网络通过学习后自己得出的一系列用于预测输出变量的新特征。<br/> <b>这就是神经网络模型相比于逻辑回归和线性回归的优势。</b></p><h2><b>3.样本和直观理解</b></h2><p>      我们可以用一个单一的激活单元 + 不同的权重即可描三种二元逻辑运算符（BINARY LOGICAL OPERATORS） ：<b>逻辑与（AND）、 逻辑或（OR）、逻辑非（NOT）</b></p><p>      我们只需要将假设函数 <img src=\"https://www.zhihu.com/equation?tex=h_%5Ctheta%28x%29\" alt=\"h_\\theta(x)\" eeimg=\"1\"/> 使用sigmoid函数即可:</p><p><img src=\"https://www.zhihu.com/equation?tex=h_%5Ctheta%28z%29+%3D+g%28z%29+%3D+%5Cfrac%7B1%7D%7B1%2Be%5E%7B-z%7D%7D\" alt=\"h_\\theta(z) = g(z) = \\frac{1}{1+e^{-z}}\" eeimg=\"1\"/> </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-90e74b646df38be423f542280c01e277_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"318\" data-rawheight=\"218\" class=\"content_image\" width=\"318\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;318&#39; height=&#39;218&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"318\" data-rawheight=\"218\" class=\"content_image lazy\" width=\"318\" data-actualsrc=\"https://pic4.zhimg.com/v2-90e74b646df38be423f542280c01e277_b.jpg\"/></figure><p>      令 <img src=\"https://www.zhihu.com/equation?tex=z+%3D+%5Ctheta_0+%2B+%5Ctheta_1x_1+%2B+%5Ctheta_2x_2+%2C%28x_1%2Cx_2%29+%5Cin%7B0%2C1%7D\" alt=\"z = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 ,(x_1,x_2) \\in{0,1}\" eeimg=\"1\"/> <br/>     从而： <img src=\"https://www.zhihu.com/equation?tex=h_%5Ctheta%28x%29++%3D+g%28%5Ctheta_0+%2B+%5Ctheta_1x_1+%2B+%5Ctheta_2x_2%29\" alt=\"h_\\theta(x)  = g(\\theta_0 + \\theta_1x_1 + \\theta_2x_2)\" eeimg=\"1\"/> </p><p><br/>      要想实现逻辑与AND，我们可以分别设置 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_0%2C%5Ctheta_1%2C%5Ctheta_2\" alt=\"\\theta_0,\\theta_1,\\theta_2\" eeimg=\"1\"/> = -30，20，20，表达式 <img src=\"https://www.zhihu.com/equation?tex=h_%5Ctheta%28x%29++%3D+g%28-30+%2B+20x_1+%2B+20x_2%29\" alt=\"h_\\theta(x)  = g(-30 + 20x_1 + 20x_2)\" eeimg=\"1\"/> <br/>      要想实现逻辑或OR，可以设置<img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_0%2C%5Ctheta_1%2C%5Ctheta_2\" alt=\"\\theta_0,\\theta_1,\\theta_2\" eeimg=\"1\"/>分别 = -10，20， 20， <img src=\"https://www.zhihu.com/equation?tex=h_%5Ctheta%28x%29++%3D+g%28-10+%2B+20x_1+%2B+20x_2%29\" alt=\"h_\\theta(x)  = g(-10 + 20x_1 + 20x_2)\" eeimg=\"1\"/> <br/>      要想实现逻辑非NOT，更简单，只需要一个输入即可。可以设置 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_0%2C%5Ctheta_1\" alt=\"\\theta_0,\\theta_1\" eeimg=\"1\"/>分别 = 10，-20， <img src=\"https://www.zhihu.com/equation?tex=h_%5Ctheta%28x%29++%3D+g%2810+-+20x_1%29\" alt=\"h_\\theta(x)  = g(10 - 20x_1)\" eeimg=\"1\"/> </p><h2><b>4.多类分类</b></h2><p>      当我们要用神经网络模型来实现多分类，譬如训练一个模型来识别路人、汽车、摩托车和卡车。那么神经网络模型改如何构成？</p><p>假设我们的输入有三个维度、输出根据上述可知有4个分类，并且是4个互斥的分类。那么我们的神经网络模型可以设计成如下的形状：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-b4be683f26c1537b983dbbcb8e68e95e_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"343\" data-rawheight=\"138\" class=\"content_image\" width=\"343\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;343&#39; height=&#39;138&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"343\" data-rawheight=\"138\" class=\"content_image lazy\" width=\"343\" data-actualsrc=\"https://pic3.zhimg.com/v2-b4be683f26c1537b983dbbcb8e68e95e_b.jpg\"/></figure><p>即总共4层、中间有两层隐藏层的网络结构。结果是4 x 1维矩阵。<br/>我们可以用</p><p><img src=\"https://www.zhihu.com/equation?tex=h_%5Ctheta%28x%29+%3D+%5Cbegin%7BBmatrix%7D+1+%5C%5C+0+%5C%5C+0+%5C%5C+0++%5Cend%7BBmatrix%7D\" alt=\"h_\\theta(x) = \\begin{Bmatrix} 1 \\\\ 0 \\\\ 0 \\\\ 0  \\end{Bmatrix}\" eeimg=\"1\"/> ,<img src=\"https://www.zhihu.com/equation?tex=h_%5Ctheta%28x%29+%3D+%5Cbegin%7BBmatrix%7D+0+%5C%5C+1+%5C%5C+0+%5C%5C+0++%5Cend%7BBmatrix%7D\" alt=\"h_\\theta(x) = \\begin{Bmatrix} 0 \\\\ 1 \\\\ 0 \\\\ 0  \\end{Bmatrix}\" eeimg=\"1\"/> ,<img src=\"https://www.zhihu.com/equation?tex=h_%5Ctheta%28x%29+%3D+%5Cbegin%7BBmatrix%7D+0+%5C%5C+0+%5C%5C+1+%5C%5C+0++%5Cend%7BBmatrix%7D\" alt=\"h_\\theta(x) = \\begin{Bmatrix} 0 \\\\ 0 \\\\ 1 \\\\ 0  \\end{Bmatrix}\" eeimg=\"1\"/> ,<img src=\"https://www.zhihu.com/equation?tex=h_%5Ctheta%28x%29+%3D+%5Cbegin%7BBmatrix%7D+0+%5C%5C+0+%5C%5C+0+%5C%5C+1+%5Cend%7BBmatrix%7D\" alt=\"h_\\theta(x) = \\begin{Bmatrix} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{Bmatrix}\" eeimg=\"1\"/> ,</p><p>来分别表示输出：<br/>路人、汽车、摩托、卡车。</p>", 
            "topic": [
                {
                    "tag": "神经网络", 
                    "tagLink": "https://api.zhihu.com/topics/19607065"
                }, 
                {
                    "tag": "前馈神经网络", 
                    "tagLink": "https://api.zhihu.com/topics/20682959"
                }, 
                {
                    "tag": "吴恩达（Andrew Ng）", 
                    "tagLink": "https://api.zhihu.com/topics/20003150"
                }
            ], 
            "comments": [
                {
                    "userName": "也无风雨也无情", 
                    "userLink": "https://www.zhihu.com/people/39756da3f51a0b3c579588baca217f94", 
                    "content": "<p>总表达式那里好像有错误，应该是θ x=α</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "Lyon", 
                            "userLink": "https://www.zhihu.com/people/38495a9b1f83e4a612543ce6c523df7f", 
                            "content": "你好，多谢指出错误，已改正！", 
                            "likes": 0, 
                            "replyToAuthor": "也无风雨也无情"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/73404297", 
            "userName": "Lyon", 
            "userLink": "https://www.zhihu.com/people/38495a9b1f83e4a612543ce6c523df7f", 
            "upvote": 5, 
            "title": "【吴恩达机器学习】第三周—逻辑回归、过拟合、正则化", 
            "content": "<p><b>前言：</b></p><p>最近在学习深度学习，看了不少教程，发现还是吴恩达的比较适用于我。吴恩达机器学习公开课视频最早是斯坦福大学的课程视频(那个画面有点老)，新版的视频在网易云课堂上可以随时学习。仅仅通过视频学习，可能会有点快，因为有的知识点需要反复推敲和回味。感谢github上一位朋友的精心整理，让我们可以配合讲义一起学习，讲义有HTML版、PDF版、Markdown版的：视频配合讲义看，事半功倍</p><p>本文包括接下来的系列文章，都会是吴恩达机器学习课程的个人学习总结，有很多文字和图片直接参考了PDF版的课程讲义，再次谢过提供讲义的朋友！</p><p>网易云课堂课程视频：</p><p><a href=\"https://link.zhihu.com/?target=https%3A//study.163.com/course/courseMain.htm%3FcourseId%3D1004570029\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">study.163.com/course/co</span><span class=\"invisible\">urseMain.htm?courseId=1004570029</span><span class=\"ellipsis\"></span></a></p><p>课程同步配套讲义：</p><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/fengdu78/Coursera-ML-AndrewNg-Notes\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/fengdu78/Cou</span><span class=\"invisible\">rsera-ML-AndrewNg-Notes</span><span class=\"ellipsis\"></span></a></p><p>原文发布在语雀文档上,效果更好看点：</p><a href=\"https://link.zhihu.com/?target=https%3A//www.yuque.com/docs/share/a218a684-ba75-4df2-add0-a1773f62a983\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-72bbb1e26eacf07192c32d1b6514dde0_ipico.jpg\" data-image-width=\"512\" data-image-height=\"512\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">第三周 · 语雀</a><hr/><h2><b>第三周</b></h2><h2><b>1.分类问题（Regression）</b></h2><p>在分类问题中，你要预测的变量 𝑦 是离散的值，我们将学习一种叫做<b>逻辑回归 (Logistic</b>br/&gt;Regression) 的算法，这是目前最流行使用最广泛的一种学习算法。顺便说一下，逻辑回归算法是分类算法，我们将它作为分类算法使用。有时候可能因为<br/>这个算法的名字中出现了“回归”使你感到困惑，但逻辑回归算法实际上是一种分类算法，它<br/>适用于标签 𝑦 取值离散的情况，如:1 0 0 1。<br/><br/>为什么不用线性回归来解决分类问题 ？<br/>简单来说，因为分类问题的y取值为固定的几个类别，譬如肿瘤分类为0 表示良性、1表示恶性，如果同样用线性回归y = k * x + b来表示，x为肿瘤尺寸，则得出的y范围可能远 &gt; 1,但结果总会落到0和1上，会显得很奇怪。</p><h2><b>2.逻辑回归(Logistic Regression)</b></h2><p>前面说了，逻辑回归是适用于分类问题的常见算法，这个算法的性质是:它的输出值永远在 0 到 1 之间。</p><h2><b>3.假设函数表达式Hypothesis Representation</b></h2><p>回到之前的乳腺癌分类上，我希望输出的预测值是介于0~1之间，此时用逻辑回归算法怎么实现呢？ <br/>其实，逻辑回归中，可以用<b>Sigmoid函数，来实现在R区间的输入，得到0~1之间的输出</b><br/><b>Sigmoid函数：</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-58954161311522cd8dd831ec1bc195e0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"92\" data-rawheight=\"33\" class=\"content_image\" width=\"92\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;92&#39; height=&#39;33&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"92\" data-rawheight=\"33\" class=\"content_image lazy\" width=\"92\" data-actualsrc=\"https://pic1.zhimg.com/v2-58954161311522cd8dd831ec1bc195e0_b.jpg\"/></figure><p>函数图像如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-bd3b2cae1db0fc59b997b1e6b30db512_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"496\" data-rawheight=\"293\" class=\"origin_image zh-lightbox-thumb\" width=\"496\" data-original=\"https://pic3.zhimg.com/v2-bd3b2cae1db0fc59b997b1e6b30db512_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;496&#39; height=&#39;293&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"496\" data-rawheight=\"293\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"496\" data-original=\"https://pic3.zhimg.com/v2-bd3b2cae1db0fc59b997b1e6b30db512_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-bd3b2cae1db0fc59b997b1e6b30db512_b.jpg\"/></figure><p>则，经过<b>Sigmoid改良后的假设函数如下：</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-50819d0c87a459e9c1b9534989104c15_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"179\" data-rawheight=\"36\" class=\"content_image\" width=\"179\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;179&#39; height=&#39;36&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"179\" data-rawheight=\"36\" class=\"content_image lazy\" width=\"179\" data-actualsrc=\"https://pic2.zhimg.com/v2-50819d0c87a459e9c1b9534989104c15_b.jpg\"/></figure><p>看图可知，对于任意参数z：<br/>如果 <img src=\"https://www.zhihu.com/equation?tex=h_%5Ctheta%28x%29\" alt=\"h_\\theta(x)\" eeimg=\"1\"/> = 0.5，那么其为恶性/良性的概率为50%<br/><img src=\"https://www.zhihu.com/equation?tex=h_%5Ctheta%28x%29\" alt=\"h_\\theta(x)\" eeimg=\"1\"/>&gt; 0.5,则我们判定预测值y = 1<br/><img src=\"https://www.zhihu.com/equation?tex=h_%5Ctheta%28x%29\" alt=\"h_\\theta(x)\" eeimg=\"1\"/>&lt; 0.5,则我们判定预测值y = 0<br/>例如，如果对于给定的𝑥，通过已经确定的参数计算得出h𝜃 (𝑥) = 0.7，则表示有 70%的<br/>几率𝑦为1(正向类)，相应地𝑦为0(负向类)的几率为 1-0.7=0.3。<br/>，根据图像可知：<br/>z &gt; 0时</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-b58ddda56505a15b793867e41bce90f6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"138\" data-rawheight=\"19\" class=\"content_image\" width=\"138\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;138&#39; height=&#39;19&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"138\" data-rawheight=\"19\" class=\"content_image lazy\" width=\"138\" data-actualsrc=\"https://pic3.zhimg.com/v2-b58ddda56505a15b793867e41bce90f6_b.jpg\"/></figure><p><img src=\"https://www.zhihu.com/equation?tex=h_%5Ctheta%28x%29\" alt=\"h_\\theta(x)\" eeimg=\"1\"/>&gt; 0.5 预测值y 为1<br/>z &lt; 0时</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-b58ddda56505a15b793867e41bce90f6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"138\" data-rawheight=\"19\" class=\"content_image\" width=\"138\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;138&#39; height=&#39;19&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"138\" data-rawheight=\"19\" class=\"content_image lazy\" width=\"138\" data-actualsrc=\"https://pic3.zhimg.com/v2-b58ddda56505a15b793867e41bce90f6_b.jpg\"/></figure><p><img src=\"https://www.zhihu.com/equation?tex=h_%5Ctheta%28x%29\" alt=\"h_\\theta(x)\" eeimg=\"1\"/>&lt; 0.5 预测值y 为0</p><h2><b>4.判定边界Decision Boundary</b></h2><p>现在假设我们有一个模型:</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-c9fad6b50f3ef6b30f12fe7d4b43eaea_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"547\" data-rawheight=\"185\" class=\"origin_image zh-lightbox-thumb\" width=\"547\" data-original=\"https://pic3.zhimg.com/v2-c9fad6b50f3ef6b30f12fe7d4b43eaea_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;547&#39; height=&#39;185&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"547\" data-rawheight=\"185\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"547\" data-original=\"https://pic3.zhimg.com/v2-c9fad6b50f3ef6b30f12fe7d4b43eaea_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-c9fad6b50f3ef6b30f12fe7d4b43eaea_b.jpg\"/></figure><p>      并且参数𝜃 是向量[-3 1 1]。 则当−3 + 𝑥1 + 𝑥2 ≥ 0，即𝑥1 + 𝑥2 ≥ 3时，模型将预测 𝑦 =<br/>1。 我们可以绘制直线: <b>𝑥1 + 𝑥2 = 3</b>，这条线便是我们模型的分界线，将预测为 1 的区域和预<br/>测为 0 的区域分隔开。这条线即被称为—<b>判定边界Decision Boundary</b></p><p>      假设我们的数据呈现这样的分布情况，怎样的模型才能适合呢? 其实，判定边界不一定是直线，还可能是曲线，如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-69c8b0e2ac6947587508aaf8bde09798_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"384\" data-rawheight=\"275\" class=\"content_image\" width=\"384\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;384&#39; height=&#39;275&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"384\" data-rawheight=\"275\" class=\"content_image lazy\" width=\"384\" data-actualsrc=\"https://pic1.zhimg.com/v2-69c8b0e2ac6947587508aaf8bde09798_b.jpg\"/></figure><p>      因为需要用曲线才能分隔 𝑦 = 0 的区域和 𝑦 = 1 的区域，我们需要二次方特征:<br/>h (𝑥)=𝑔(𝜃 +𝜃 𝑥 +𝜃 𝑥 +𝜃 𝑥2 +𝜃 𝑥2)是[-1 0 0 1 1]，则我们得到的判定边界恰好是圆点在原点且半径为 1 的圆形。</p><h2><b>5.损失函数Cost Function</b></h2><p>      在之前的房间-面积模型中(单变量线性回归问题),我们用到的是平方差损失函数，那么对此处的分类问题，我们可不可以用之前的平方差损失函数呢 ？ <b>先给结论，不行，此处我们应该用交叉熵损失函数</b><br/>为什么？</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-bd2ad56a17f241646995dfb8fa3b6356_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"214\" data-rawheight=\"31\" class=\"content_image\" width=\"214\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;214&#39; height=&#39;31&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"214\" data-rawheight=\"31\" class=\"content_image lazy\" width=\"214\" data-actualsrc=\"https://pic3.zhimg.com/v2-bd2ad56a17f241646995dfb8fa3b6356_b.jpg\"/></figure><p><br/>而此处</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-a384f37ddff32d38ef2fc1244232b5f2_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"118\" data-rawheight=\"36\" class=\"content_image\" width=\"118\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;118&#39; height=&#39;36&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"118\" data-rawheight=\"36\" class=\"content_image lazy\" width=\"118\" data-actualsrc=\"https://pic3.zhimg.com/v2-a384f37ddff32d38ef2fc1244232b5f2_b.jpg\"/></figure><p>,带入后得到的损失函数比较复杂，不过可以通过图像看出 <img src=\"https://www.zhihu.com/equation?tex=J%28%5Ctheta%29\" alt=\"J(\\theta)\" eeimg=\"1\"/> 和 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 的关系：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-4efae5763efc6f8d352b8e630f66b8af_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"307\" data-rawheight=\"214\" class=\"content_image\" width=\"307\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;307&#39; height=&#39;214&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"307\" data-rawheight=\"214\" class=\"content_image lazy\" width=\"307\" data-actualsrc=\"https://pic4.zhimg.com/v2-4efae5763efc6f8d352b8e630f66b8af_b.jpg\"/></figure><p>      可以看出，损失函数曲线比较【浪】，拥有多个局部最优解，曲线非凸，如果用这样的代价函数来让机器学习迭代，则容易陷入局部最优解中，而找不到全局最优解，即找不到使得损失函数值最小的loss，也就找不到最优化的模型。<br/>故，不可用采用平方损失函数，作为替代，我们可以采取【交叉熵损失函数】</p><p><b>      凸函数：</b><br/>对于实数集上的凸函数，一般的判别方法是求它的二阶导数，如果其二阶导数在区间上非负，就称为凸函数</p><p>简单的例子 y = x^2 二阶导为2 &gt; 0,故其为凸函数(形状上看上去是凹的，千万别弄反！)<br/><br/>在这里，我们定义损失函数 ：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-faf26acd33f25652e91a97a403c55e1a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"215\" data-rawheight=\"31\" class=\"content_image\" width=\"215\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;215&#39; height=&#39;31&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"215\" data-rawheight=\"31\" class=\"content_image lazy\" width=\"215\" data-actualsrc=\"https://pic3.zhimg.com/v2-faf26acd33f25652e91a97a403c55e1a_b.jpg\"/></figure><p>其中：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-17074d4139919babd5406e89056cfd6f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"278\" data-rawheight=\"47\" class=\"content_image\" width=\"278\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;278&#39; height=&#39;47&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"278\" data-rawheight=\"47\" class=\"content_image lazy\" width=\"278\" data-actualsrc=\"https://pic4.zhimg.com/v2-17074d4139919babd5406e89056cfd6f_b.jpg\"/></figure><p>复习一下对数函数，简单的</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b827c2be3e1278d4d98f8d7e060548a9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"57\" data-rawheight=\"15\" class=\"content_image\" width=\"57\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;57&#39; height=&#39;15&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"57\" data-rawheight=\"15\" class=\"content_image lazy\" width=\"57\" data-actualsrc=\"https://pic2.zhimg.com/v2-b827c2be3e1278d4d98f8d7e060548a9_b.jpg\"/></figure><p> 函数经过点（1，0）、（2，1）函数图像如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-39840dd16d1d964678d10d6f49eb011d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"908\" data-rawheight=\"900\" class=\"origin_image zh-lightbox-thumb\" width=\"908\" data-original=\"https://pic2.zhimg.com/v2-39840dd16d1d964678d10d6f49eb011d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;908&#39; height=&#39;900&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"908\" data-rawheight=\"900\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"908\" data-original=\"https://pic2.zhimg.com/v2-39840dd16d1d964678d10d6f49eb011d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-39840dd16d1d964678d10d6f49eb011d_b.jpg\"/></figure><p>下面，让我们推导一波公式：</p><p><img src=\"https://www.zhihu.com/equation?tex=h_%5Ctheta%28x%29\" alt=\"h_\\theta(x)\" eeimg=\"1\"/> 是 <img src=\"https://www.zhihu.com/equation?tex=Sigmod\" alt=\"Sigmod\" eeimg=\"1\"/> 函数(单调递增，导数&gt;0)， <img src=\"https://www.zhihu.com/equation?tex=log\" alt=\"log\" eeimg=\"1\"/> 函数隐藏了常数项底数，我们设其为a，简化一下上面的 <img src=\"https://www.zhihu.com/equation?tex=Cost%28%29\" alt=\"Cost()\" eeimg=\"1\"/> 函数</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-88bf8c8ba0f06aeab6b2ee420f2af527_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"301\" data-rawheight=\"47\" class=\"content_image\" width=\"301\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;301&#39; height=&#39;47&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"301\" data-rawheight=\"47\" class=\"content_image lazy\" width=\"301\" data-actualsrc=\"https://pic4.zhimg.com/v2-88bf8c8ba0f06aeab6b2ee420f2af527_b.jpg\"/></figure><p>当<b><i> y = 1</i></b>时，<b><i>t </i></b>取值范围：（0.5，1），</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-80f38ced1455648a99e8a01291b694ca_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"254\" data-rawheight=\"32\" class=\"content_image\" width=\"254\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;254&#39; height=&#39;32&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"254\" data-rawheight=\"32\" class=\"content_image lazy\" width=\"254\" data-actualsrc=\"https://pic3.zhimg.com/v2-80f38ced1455648a99e8a01291b694ca_b.jpg\"/></figure><p><br/>故函数单调递减，且<b><i>t </i></b>趋于1时， <img src=\"https://www.zhihu.com/equation?tex=Cost%28%29\" alt=\"Cost()\" eeimg=\"1\"/> 趋于 0。示意图左下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-907268f2966d5a3c8ed62d5c6b3224b5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"389\" data-rawheight=\"192\" class=\"content_image\" width=\"389\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;389&#39; height=&#39;192&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"389\" data-rawheight=\"192\" class=\"content_image lazy\" width=\"389\" data-actualsrc=\"https://pic2.zhimg.com/v2-907268f2966d5a3c8ed62d5c6b3224b5_b.jpg\"/></figure><p>当 <b><i>y = 0</i></b>时，<b><i>t </i></b>取值范围：（0，0.5），</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-161ebb2d2809bdee988c00f483ab7a1a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"278\" data-rawheight=\"33\" class=\"content_image\" width=\"278\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;278&#39; height=&#39;33&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"278\" data-rawheight=\"33\" class=\"content_image lazy\" width=\"278\" data-actualsrc=\"https://pic3.zhimg.com/v2-161ebb2d2809bdee988c00f483ab7a1a_b.jpg\"/></figure><p>故函数单调增，且<b><i> t </i></b>趋于0， <img src=\"https://www.zhihu.com/equation?tex=Cost%28%29\" alt=\"Cost()\" eeimg=\"1\"/>趋于 0，示意图右上。<br/><br/>最后，合并一下:</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-72397358faf98bc6e5b9c5325d92759b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"354\" data-rawheight=\"17\" class=\"content_image\" width=\"354\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;354&#39; height=&#39;17&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"354\" data-rawheight=\"17\" class=\"content_image lazy\" width=\"354\" data-actualsrc=\"https://pic4.zhimg.com/v2-72397358faf98bc6e5b9c5325d92759b_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-159e2494c4060899b1ae6c4bc57893f9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"571\" data-rawheight=\"31\" class=\"origin_image zh-lightbox-thumb\" width=\"571\" data-original=\"https://pic2.zhimg.com/v2-159e2494c4060899b1ae6c4bc57893f9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;571&#39; height=&#39;31&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"571\" data-rawheight=\"31\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"571\" data-original=\"https://pic2.zhimg.com/v2-159e2494c4060899b1ae6c4bc57893f9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-159e2494c4060899b1ae6c4bc57893f9_b.jpg\"/></figure><p>此损失/代价函数即为可以进行梯度下降求导的，没有局部最优解的凸函数：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-d09ef07d7cf7cb1152d87638c72393ba_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"257\" data-rawheight=\"188\" class=\"content_image\" width=\"257\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;257&#39; height=&#39;188&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"257\" data-rawheight=\"188\" class=\"content_image lazy\" width=\"257\" data-actualsrc=\"https://pic3.zhimg.com/v2-d09ef07d7cf7cb1152d87638c72393ba_b.jpg\"/></figure><p>证明过程见文章第5.点末尾</p><p>Python代码实现如下：</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"kn\">import</span> <span class=\"nn\">numpy</span> <span class=\"k\">as</span> <span class=\"nn\">np</span>\n<span class=\"k\">def</span> <span class=\"nf\">cost</span><span class=\"p\">(</span><span class=\"n\">theta</span><span class=\"p\">,</span> <span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">):</span>\n<span class=\"n\">theta</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">matrix</span><span class=\"p\">(</span><span class=\"n\">theta</span><span class=\"p\">)</span>\n<span class=\"n\">X</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">matrix</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">)</span>\n<span class=\"n\">y</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">matrix</span><span class=\"p\">(</span><span class=\"n\">y</span><span class=\"p\">)</span>\n<span class=\"n\">first</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">multiply</span><span class=\"p\">(</span><span class=\"o\">-</span><span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">log</span><span class=\"p\">(</span><span class=\"n\">sigmoid</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"o\">*</span> <span class=\"n\">theta</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">)))</span>\n<span class=\"n\">second</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">multiply</span><span class=\"p\">((</span><span class=\"mi\">1</span> <span class=\"o\">-</span> <span class=\"n\">y</span><span class=\"p\">),</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">log</span><span class=\"p\">(</span><span class=\"mi\">1</span> <span class=\"o\">-</span> <span class=\"n\">sigmoid</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"o\">*</span> <span class=\"n\">theta</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">)))</span>\n<span class=\"k\">return</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">first</span> <span class=\"o\">-</span> <span class=\"n\">second</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">))</span></code></pre></div><p>在得到这样一个代价函数以后，我们便可以用梯度下降算法来求得能使代价函数最小的<br/>参数了。算法为：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-cf9c81183881c4c66d4167dd58290e58_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"427\" data-rawheight=\"293\" class=\"origin_image zh-lightbox-thumb\" width=\"427\" data-original=\"https://pic1.zhimg.com/v2-cf9c81183881c4c66d4167dd58290e58_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;427&#39; height=&#39;293&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"427\" data-rawheight=\"293\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"427\" data-original=\"https://pic1.zhimg.com/v2-cf9c81183881c4c66d4167dd58290e58_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-cf9c81183881c4c66d4167dd58290e58_b.jpg\"/></figure><p><b>一些梯度下降算法之外的选择：</b><br/> 除了梯度下降算法以外，还有一些常被用来令代价函数最小的算法，这些算法更加复杂和优越，而且通常不需要人工选择学习率，通常比梯度下降算法要更加快速。这些算法有： </p><ul><li><b>共轭梯度（Conjugate Gradient）</b></li><li><b>局部优化法(Broyden fletcher goldfarb shann,BFGS)</b></li><li><b>有限内存局部优化法(LBFGS)</b></li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-745455acebe337168de54c37f6170e52_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"690\" data-rawheight=\"941\" class=\"origin_image zh-lightbox-thumb\" width=\"690\" data-original=\"https://pic3.zhimg.com/v2-745455acebe337168de54c37f6170e52_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;690&#39; height=&#39;941&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"690\" data-rawheight=\"941\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"690\" data-original=\"https://pic3.zhimg.com/v2-745455acebe337168de54c37f6170e52_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-745455acebe337168de54c37f6170e52_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>注：虽然得到的梯度下降算法表面上看上去与线性回归的梯度下降算法一样，但是这里的</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-50819d0c87a459e9c1b9534989104c15_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"179\" data-rawheight=\"36\" class=\"content_image\" width=\"179\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;179&#39; height=&#39;36&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"179\" data-rawheight=\"36\" class=\"content_image lazy\" width=\"179\" data-actualsrc=\"https://pic2.zhimg.com/v2-50819d0c87a459e9c1b9534989104c15_b.jpg\"/></figure><p>与线性回归中不同，所以实际上是不一样的。<br/>另外，在运行梯度下降算法之前，进行特征缩放依旧是非常必要的。</p><h2><b>6.简化的损失函数和梯度下降</b></h2><p>在上面我们得出损失函数公式如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-159e2494c4060899b1ae6c4bc57893f9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"571\" data-rawheight=\"31\" class=\"origin_image zh-lightbox-thumb\" width=\"571\" data-original=\"https://pic2.zhimg.com/v2-159e2494c4060899b1ae6c4bc57893f9_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;571&#39; height=&#39;31&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"571\" data-rawheight=\"31\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"571\" data-original=\"https://pic2.zhimg.com/v2-159e2494c4060899b1ae6c4bc57893f9_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-159e2494c4060899b1ae6c4bc57893f9_b.jpg\"/></figure><p>那么为了使损失函数尽量小，我们对其求梯度(求导过程见第5.点末尾)：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-cf9c81183881c4c66d4167dd58290e58_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"427\" data-rawheight=\"293\" class=\"origin_image zh-lightbox-thumb\" width=\"427\" data-original=\"https://pic1.zhimg.com/v2-cf9c81183881c4c66d4167dd58290e58_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;427&#39; height=&#39;293&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"427\" data-rawheight=\"293\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"427\" data-original=\"https://pic1.zhimg.com/v2-cf9c81183881c4c66d4167dd58290e58_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-cf9c81183881c4c66d4167dd58290e58_b.jpg\"/></figure><p><br/>      现在，如果你把这个更新规则和我们之前用在线性回归上的进行比较的话，你会惊讶地发现，这个式子正是我们用来做线性回归梯度下降的。那么，线性回归和逻辑回归是同一个算法吗？要回答这个问题，我们要观察逻辑回归看<br/>看发生了哪些变化。实际上，假设的定义发生了变化。<br/>对于线性回归假设函数：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-5c842c6ce799f0d79e5bb445c1931476_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"282\" data-rawheight=\"19\" class=\"content_image\" width=\"282\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;282&#39; height=&#39;19&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"282\" data-rawheight=\"19\" class=\"content_image lazy\" width=\"282\" data-actualsrc=\"https://pic3.zhimg.com/v2-5c842c6ce799f0d79e5bb445c1931476_b.jpg\"/></figure><p>而逻辑回归中假设函数：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-50819d0c87a459e9c1b9534989104c15_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"179\" data-rawheight=\"36\" class=\"content_image\" width=\"179\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;179&#39; height=&#39;36&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"179\" data-rawheight=\"36\" class=\"content_image lazy\" width=\"179\" data-actualsrc=\"https://pic2.zhimg.com/v2-50819d0c87a459e9c1b9534989104c15_b.jpg\"/></figure><p>      因此，即使更新参数的规则看起来基本相同，但由于假设的定义发生了变化，所以逻辑函数的梯度下降，跟线性回归的梯度下降实际上是两个完全不同的东西。</p><p>      最后还有一点，我们之前在谈线性回归时讲到的特征缩放，我们看到了特征缩放是如何提高梯度下降的收敛速度的，这个<b>特征缩放的方法，也适用于逻辑回归</b>。如果你的特征范围差距很大的话，那么应用特征缩放的方法，同样也可以让逻辑回归中，梯度下降收敛更快。<br/>就是这样，现在你知道如何实现<b>逻辑回归，这是一种非常强大，甚至可能世界上使用最广泛的一种分类算法。</b></p><h2><b>7.高级优化</b></h2><p>     现在我们换个角度来看什么是梯度下降，我们有个代价函数 <img src=\"https://www.zhihu.com/equation?tex=J%28%5Ctheta%29\" alt=\"J(\\theta)\" eeimg=\"1\"/> ，而我们想要使其最小化，那么我们需要做的是编写代码，当输入参数<img src=\"https://www.zhihu.com/equation?tex=J%28%5Ctheta%29\" alt=\"J(\\theta)\" eeimg=\"1\"/>时，它们会计算出两样东西：<img src=\"https://www.zhihu.com/equation?tex=J%28%5Ctheta%29\" alt=\"J(\\theta)\" eeimg=\"1\"/>以及j等于 0、 1 直到n时的偏导数项。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-0b9177937c727e864f2d905145651721_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"373\" data-rawheight=\"324\" class=\"content_image\" width=\"373\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;373&#39; height=&#39;324&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"373\" data-rawheight=\"324\" class=\"content_image lazy\" width=\"373\" data-actualsrc=\"https://pic2.zhimg.com/v2-0b9177937c727e864f2d905145651721_b.jpg\"/></figure><p>      假设我们已经完成了可以实现这两件事的代码，那么梯度下降所做的就是反复执行这些更新。对于梯度下降来说，我认为从技术上讲，你实际并不需要编写代码来计算代价函数<img src=\"https://www.zhihu.com/equation?tex=J%28%5Ctheta%29\" alt=\"J(\\theta)\" eeimg=\"1\"/>。你只需要编写代码来计算导数项，但是，如果你希望代码还要能够监控<img src=\"https://www.zhihu.com/equation?tex=J%28%5Ctheta%29\" alt=\"J(\\theta)\" eeimg=\"1\"/>的收敛性，那么我们就需要自己编写代码来计算代价函数<img src=\"https://www.zhihu.com/equation?tex=J%28%5Ctheta%29\" alt=\"J(\\theta)\" eeimg=\"1\"/>和偏导项</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-ea006100c973364d6319149c2e297ce6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"50\" data-rawheight=\"35\" class=\"content_image\" width=\"50\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;50&#39; height=&#39;35&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"50\" data-rawheight=\"35\" class=\"content_image lazy\" width=\"50\" data-actualsrc=\"https://pic3.zhimg.com/v2-ea006100c973364d6319149c2e297ce6_b.jpg\"/></figure><p>       而<b>梯度下降并不是我们可以使用的唯一算法</b>，还有其他一些算法，更高级、更复杂。如果我们能用这些方法来计算代价函数<img src=\"https://www.zhihu.com/equation?tex=J%28%5Ctheta%29\" alt=\"J(\\theta)\" eeimg=\"1\"/></p><p>和偏导数项</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-ea006100c973364d6319149c2e297ce6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"50\" data-rawheight=\"35\" class=\"content_image\" width=\"50\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;50&#39; height=&#39;35&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"50\" data-rawheight=\"35\" class=\"content_image lazy\" width=\"50\" data-actualsrc=\"https://pic3.zhimg.com/v2-ea006100c973364d6319149c2e297ce6_b.jpg\"/></figure><p>的话，那么这些算法就是为我们优化代价函数的不同方法， <b>共轭梯度法 BFGS (变尺度法) 和 L-BFGS (限制变尺度法) </b>就是其中一些更高级的优化算法。它们需要有一种方法来计算<img src=\"https://www.zhihu.com/equation?tex=J%28%5Ctheta%29\" alt=\"J(\\theta)\" eeimg=\"1\"/>以及需要一种方法计算导数项，然后使用比梯度下降更复杂的算法来最小化代价函数。<br/>      这三种算法有许多优点：</p><p>      一个是使用这其中任何一个算法，你<b>通常不需要手动选择学习率</b> <img src=\"https://www.zhihu.com/equation?tex=%5Calpha\" alt=\"\\alpha\" eeimg=\"1\"/> ，所以对于这些算法的一种思路是，给出计算导数项和代价函数的方法，你可以认为算法有一个智能的内部循环，而且，事实上，他们确实有一个智能的内部循环，称为<b>线性搜索(line search)算法</b>，它可以自动尝试不同的学习率<img src=\"https://www.zhihu.com/equation?tex=%5Calpha\" alt=\"\\alpha\" eeimg=\"1\"/>，并自动选择一个好的学习速率，因此它甚至可以为每次迭代选择不同的学习速率，那么你就不需要自己选择。这些算法实际上在做更复杂的事情，而不仅仅是选择一个好的学习率，所以它们往往最终收敛得远远快于梯度下降，<i>不过关于它们到底做什么的详细讨论，已经超过了本门课程的范围。</i></p><h2><b>8.多元/多类别分类Multiclass Classification</b></h2><p>这里插播一句，吴恩达老师的课实在是太好了，生动形象....点赞👍</p><p>      现实世界中，没有那么多的非黑即白的二元分类问题，更多的是多分类，譬如给你柑橘类水果？可能是橘子、柚子、芦柑、橙子...这么多分类。那么问题来了，如果你用AI模型来判断，模型会告诉你它是橙子，概率在50%，那么它究竟是怎么运作的？<br/>      模型会生成一系列置信度，譬如：水果是橘子、柚子、芦柑、橙子的概率分别是20%、35%、38%、50%，最终挑选一个概率最高的类别label，将这个水果归类到label中。<br/>比较一下二元分类问题和多元分类问题，他们的数据集看上去可能是这样：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-79c964707cc3216718915f9b138c8a83_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"427\" data-rawheight=\"197\" class=\"origin_image zh-lightbox-thumb\" width=\"427\" data-original=\"https://pic4.zhimg.com/v2-79c964707cc3216718915f9b138c8a83_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;427&#39; height=&#39;197&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"427\" data-rawheight=\"197\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"427\" data-original=\"https://pic4.zhimg.com/v2-79c964707cc3216718915f9b138c8a83_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-79c964707cc3216718915f9b138c8a83_b.jpg\"/></figure><p>      我们之前学习过了二元分类问题的模型和算法，用逻辑回归 + 梯度下降可以完美地解决，那么对于三元、多元分类问题呢 ？其实原理是类似的<br/>用白话过一遍流程：<br/>遍历每个类、譬如第一轮我只关注绿色三角，那么我可以建立模型</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-28fe392e5debce842e84574a1a8a8fdf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"41\" data-rawheight=\"22\" class=\"content_image\" width=\"41\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;41&#39; height=&#39;22&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"41\" data-rawheight=\"22\" class=\"content_image lazy\" width=\"41\" data-actualsrc=\"https://pic4.zhimg.com/v2-28fe392e5debce842e84574a1a8a8fdf_b.jpg\"/></figure><p>，将绿三角标记为正向类 y = 1、将红叉叉和蓝框框都标记为负向类；经过这一轮的模型，我就可以判断出一个数据是绿三角的概率了；然后再对红叉叉建立模型</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-6573dc1d6309a2c7e03fd26b3bce77db_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"41\" data-rawheight=\"22\" class=\"content_image\" width=\"41\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;41&#39; height=&#39;22&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"41\" data-rawheight=\"22\" class=\"content_image lazy\" width=\"41\" data-actualsrc=\"https://pic4.zhimg.com/v2-6573dc1d6309a2c7e03fd26b3bce77db_b.jpg\"/></figure><p>，将红叉叉标记为正相类 y = 2,绿三角和蓝框框都标为负向类，最后建立模型</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-3b79f789b960a7e51030780c4d4ddd4b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"41\" data-rawheight=\"22\" class=\"content_image\" width=\"41\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;41&#39; height=&#39;22&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"41\" data-rawheight=\"22\" class=\"content_image lazy\" width=\"41\" data-actualsrc=\"https://pic4.zhimg.com/v2-3b79f789b960a7e51030780c4d4ddd4b_b.jpg\"/></figure><p>来对蓝框框做同样的操作。<br/>总之就是有N个类别就建立N个模型，对于一个样本，我们需要用这N个模型依次检验其在该模型下属于正向类的概率。</p><h2><b>9.过拟合over-fitting</b></h2><h3><b>过拟合 </b></h3><p><b>过度拟合的意思，意味着模型训练过程中，对训练集的模拟和学习过度贴合；</b><br/>过拟合带来的影响：模型训练时的检测率很高效果很好，但是用于实际检验时，效果很差，模型不能很准确地预测，即<b>泛化能力</b>差。</p><h3><b>欠拟合 </b></h3><p><b>和过拟合相对，欠拟合是指模型和数据集间的拟合程度不够，学习不足。</b><br/>欠拟合的影响：和过拟合相对，欠拟合是指模型和数据集间的拟合程度不够，可能是学习轮数不够、数据集特征不规则、模型选择有问题等。欠拟合时，模型的泛化能力同样会很差。</p><h3><b>泛化能力（generalization ability）</b></h3><p><b>是指一个机器学习算法对于没有见过的样本的识别能力。泛化能力自然是越高越好。</b></p><p>分类问题和回归问题中都可能存在过拟合的问题，见下图：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-9c79fff76efc1b24cd842b7e0fad2212_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"703\" data-rawheight=\"648\" class=\"origin_image zh-lightbox-thumb\" width=\"703\" data-original=\"https://pic3.zhimg.com/v2-9c79fff76efc1b24cd842b7e0fad2212_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;703&#39; height=&#39;648&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"703\" data-rawheight=\"648\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"703\" data-original=\"https://pic3.zhimg.com/v2-9c79fff76efc1b24cd842b7e0fad2212_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-9c79fff76efc1b24cd842b7e0fad2212_b.jpg\"/></figure><h3><b>解决或改善？</b></h3><p>1.<b>丢弃一些不能帮助我们正确预测的特征。可以是手工选择保留哪些特征，或者使用一些模型选择的算法来帮忙（例如 PCA）</b><br/>2.<b>正则化。 保留所有的特征，但是减少参数的大小（magnitude）。</b></p><h2><b>10.正则化</b></h2><p>假设回归问题中，过拟合的模型如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-bc395237c32e160578f3e6c57a7c01fc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"287\" data-rawheight=\"20\" class=\"content_image\" width=\"287\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;287&#39; height=&#39;20&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"287\" data-rawheight=\"20\" class=\"content_image lazy\" width=\"287\" data-actualsrc=\"https://pic1.zhimg.com/v2-bc395237c32e160578f3e6c57a7c01fc_b.jpg\"/></figure><p>      从第9.点中图像上，我们可以看出，正是那些高次项导致了过拟合的产生，所以<b>降低高次项的系数</b><img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/><b>，我们就能降低过拟合程度。</b>但是，我们不能直接修改模型中的参数大小，而通过修改代价函数中参数大小来实现“曲线救国”<br/>正常回归问题的损失函数如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-bd2ad56a17f241646995dfb8fa3b6356_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"214\" data-rawheight=\"31\" class=\"content_image\" width=\"214\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;214&#39; height=&#39;31&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"214\" data-rawheight=\"31\" class=\"content_image lazy\" width=\"214\" data-actualsrc=\"https://pic3.zhimg.com/v2-bd2ad56a17f241646995dfb8fa3b6356_b.jpg\"/></figure><p>对损失函数做梯度下降算法，如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-cf9c81183881c4c66d4167dd58290e58_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"427\" data-rawheight=\"293\" class=\"origin_image zh-lightbox-thumb\" width=\"427\" data-original=\"https://pic1.zhimg.com/v2-cf9c81183881c4c66d4167dd58290e58_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;427&#39; height=&#39;293&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"427\" data-rawheight=\"293\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"427\" data-original=\"https://pic1.zhimg.com/v2-cf9c81183881c4c66d4167dd58290e58_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-cf9c81183881c4c66d4167dd58290e58_b.jpg\"/></figure><p>      可见，每次迭代中，为了使迭代后的参数 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta\" alt=\"\\theta\" eeimg=\"1\"/> 变更小，我们需要使</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-61e8ed1127fd7b3d4197246a73d84a34_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"70\" data-rawheight=\"35\" class=\"content_image\" width=\"70\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;70&#39; height=&#39;35&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"70\" data-rawheight=\"35\" class=\"content_image lazy\" width=\"70\" data-actualsrc=\"https://pic1.zhimg.com/v2-61e8ed1127fd7b3d4197246a73d84a34_b.jpg\"/></figure><p>尽可能大，而学习率固定，所以我们只能想办法让倒数项尽可能大。所以我们可以对损失/代价函数<img src=\"https://www.zhihu.com/equation?tex=J%28%5Ctheta%29\" alt=\"J(\\theta)\" eeimg=\"1\"/>做一波修改：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-0bb0ea64287bcaeaae62130131e6651f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"623\" data-rawheight=\"60\" class=\"origin_image zh-lightbox-thumb\" width=\"623\" data-original=\"https://pic4.zhimg.com/v2-0bb0ea64287bcaeaae62130131e6651f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;623&#39; height=&#39;60&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"623\" data-rawheight=\"60\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"623\" data-original=\"https://pic4.zhimg.com/v2-0bb0ea64287bcaeaae62130131e6651f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-0bb0ea64287bcaeaae62130131e6651f_b.jpg\"/></figure><p>      可见，我们给参数<img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_3\" alt=\"\\theta_3\" eeimg=\"1\"/>和<img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_4\" alt=\"\\theta_4\" eeimg=\"1\"/>加上了系数，这可以称为【惩罚】，对修改后的代价函数，做梯度下降算法.对<img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_3\" alt=\"\\theta_3\" eeimg=\"1\"/>的更新如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-d4fd9cded62f0b664b7c908ff9252f06_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"395\" data-rawheight=\"35\" class=\"content_image\" width=\"395\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;395&#39; height=&#39;35&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"395\" data-rawheight=\"35\" class=\"content_image lazy\" width=\"395\" data-actualsrc=\"https://pic3.zhimg.com/v2-d4fd9cded62f0b664b7c908ff9252f06_b.jpg\"/></figure><p>对<img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_4\" alt=\"\\theta_4\" eeimg=\"1\"/>的更新如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-df76a09b4a3031d276d5c579f21b4e8b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"402\" data-rawheight=\"35\" class=\"content_image\" width=\"402\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;402&#39; height=&#39;35&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"402\" data-rawheight=\"35\" class=\"content_image lazy\" width=\"402\" data-actualsrc=\"https://pic4.zhimg.com/v2-df76a09b4a3031d276d5c579f21b4e8b_b.jpg\"/></figure><p>可见，对于参数<img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_3\" alt=\"\\theta_3\" eeimg=\"1\"/>和<img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_4\" alt=\"\\theta_4\" eeimg=\"1\"/>，因为有系数的存在，每次求导都会让其降低的更多更快，而且，不会影响其他参数项的下降，从而达到比较理想的效果。</p><h2><b>1.一般表示</b></h2><p>      如果我们有</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-e5073958314502252bfab96edb521f79_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"71\" data-rawheight=\"15\" class=\"content_image\" width=\"71\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;71&#39; height=&#39;15&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"71\" data-rawheight=\"15\" class=\"content_image lazy\" width=\"71\" data-actualsrc=\"https://pic2.zhimg.com/v2-e5073958314502252bfab96edb521f79_b.jpg\"/></figure><p>      总计n个参数(通常数<img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_0\" alt=\"\\theta_0\" eeimg=\"1\"/>不用惩罚)，通常会对所有的参数进行惩罚，并设置系数<img src=\"https://www.zhihu.com/equation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"/>，这样的结果是得到了一个较为简单的能防止过拟合问题的假设函数：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-decb57a275483c080a7e1d3948af01b5_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"356\" data-rawheight=\"60\" class=\"content_image\" width=\"356\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;356&#39; height=&#39;60&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"356\" data-rawheight=\"60\" class=\"content_image lazy\" width=\"356\" data-actualsrc=\"https://pic2.zhimg.com/v2-decb57a275483c080a7e1d3948af01b5_b.jpg\"/></figure><p>其中 <img src=\"https://www.zhihu.com/equation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"/> ，被称为<b>正则化参数Regularization Parameter.</b>经过正则化处理后的模型与原模型对比图如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-a5a864578291dfdd58465959f14cbb4b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"356\" data-rawheight=\"216\" class=\"content_image\" width=\"356\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;356&#39; height=&#39;216&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"356\" data-rawheight=\"216\" class=\"content_image lazy\" width=\"356\" data-actualsrc=\"https://pic4.zhimg.com/v2-a5a864578291dfdd58465959f14cbb4b_b.jpg\"/></figure><p><br/>蓝色是处理前，过拟合的模型数据表现，粉红色的是经过正则化处理后的表现。<br/>如果正则化系数<img src=\"https://www.zhihu.com/equation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"/>过小，将会导致效果不好，模型的拟合度依旧很高；<br/>如果正则化系数<img src=\"https://www.zhihu.com/equation?tex=%5Clambda\" alt=\"\\lambda\" eeimg=\"1\"/>过大，则会将除了<img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_0\" alt=\"\\theta_0\" eeimg=\"1\"/>以外的所有系数都惩罚殆尽，导致假设函数近似：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-fce3dc13e12ddc974a3c6f329f50cc9b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"64\" data-rawheight=\"17\" class=\"content_image\" width=\"64\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;64&#39; height=&#39;17&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"64\" data-rawheight=\"17\" class=\"content_image lazy\" width=\"64\" data-actualsrc=\"https://pic4.zhimg.com/v2-fce3dc13e12ddc974a3c6f329f50cc9b_b.jpg\"/></figure><p> 即变成图中的红线。</p><h2><b>2.正则化线性回归</b></h2><p>此处比较简单，我就直接贴图了：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-2b9f9e2f750bd91d7fb5cace4ad87565_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"719\" data-rawheight=\"677\" class=\"origin_image zh-lightbox-thumb\" width=\"719\" data-original=\"https://pic2.zhimg.com/v2-2b9f9e2f750bd91d7fb5cace4ad87565_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;719&#39; height=&#39;677&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"719\" data-rawheight=\"677\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"719\" data-original=\"https://pic2.zhimg.com/v2-2b9f9e2f750bd91d7fb5cace4ad87565_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-2b9f9e2f750bd91d7fb5cace4ad87565_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>3.正则化逻辑回归</b></h2><p>此处比较简单，我就直接贴图了：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-a7486779b3c9d82abf5f75fec833097b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"594\" data-rawheight=\"92\" class=\"origin_image zh-lightbox-thumb\" width=\"594\" data-original=\"https://pic4.zhimg.com/v2-a7486779b3c9d82abf5f75fec833097b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;594&#39; height=&#39;92&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"594\" data-rawheight=\"92\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"594\" data-original=\"https://pic4.zhimg.com/v2-a7486779b3c9d82abf5f75fec833097b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-a7486779b3c9d82abf5f75fec833097b_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-5e2e23a4774275dcde994d09fca9718f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"480\" data-rawheight=\"300\" class=\"origin_image zh-lightbox-thumb\" width=\"480\" data-original=\"https://pic4.zhimg.com/v2-5e2e23a4774275dcde994d09fca9718f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;480&#39; height=&#39;300&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"480\" data-rawheight=\"300\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"480\" data-original=\"https://pic4.zhimg.com/v2-5e2e23a4774275dcde994d09fca9718f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-5e2e23a4774275dcde994d09fca9718f_b.jpg\"/></figure><p><b>需要说明的是：虽然加了正则化后、逻辑回归和线性回归的梯度下降公式看上去一样，但实际意义由于假设函数的不同，而完全不同。</b></p><p>Python代码：</p><div class=\"highlight\"><pre><code class=\"language-text\">import numpy as np\ndef costReg(theta, X, y, learningRate):\ntheta = np.matrix(theta)\nX = np.matrix(X)\ny = np.matrix(y)\nfirst = np.multiply(-y, np.log(sigmoid(X*theta.T)))\nsecond = np.multiply((1 - y), np.log(1 - sigmoid(X*theta.T)))\nreg = (learningRate / (2 * len(X))* np.sum(np.power(theta[:,1:the\nta.shape[1]],2))\nreturn np.sum(first - second) / (len(X)) + reg</code></pre></div><p></p><p></p>", 
            "topic": [
                {
                    "tag": "过拟合", 
                    "tagLink": "https://api.zhihu.com/topics/20683622"
                }, 
                {
                    "tag": "逻辑回归", 
                    "tagLink": "https://api.zhihu.com/topics/20178024"
                }, 
                {
                    "tag": "吴恩达（Andrew Ng）", 
                    "tagLink": "https://api.zhihu.com/topics/20003150"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/73403012", 
            "userName": "Lyon", 
            "userLink": "https://www.zhihu.com/people/38495a9b1f83e4a612543ce6c523df7f", 
            "upvote": 5, 
            "title": "【吴恩达机器学习】第二周—多变量线性回归", 
            "content": "<p><b>前言：</b></p><p>最近在学习深度学习，看了不少教程，发现还是吴恩达的比较适用于我。吴恩达机器学习公开课视频最早是斯坦福大学的课程视频(那个画面有点老)，新版的视频在网易云课堂上可以随时学习。仅仅通过视频学习，可能会有点快，因为有的知识点需要反复推敲和回味。感谢github上一位朋友的精心整理，让我们可以配合讲义一起学习，讲义有HTML版、PDF版、Markdown版的：视频配合讲义看，事半功倍</p><p>本文包括接下来的系列文章，都会是吴恩达机器学习课程的个人学习总结，有很多文字和图片直接参考了PDF版的课程讲义，再次谢过提供讲义的朋友！</p><p>网易云课堂课程视频：</p><p><a href=\"https://link.zhihu.com/?target=https%3A//study.163.com/course/courseMain.htm%3FcourseId%3D1004570029\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">study.163.com/course/co</span><span class=\"invisible\">urseMain.htm?courseId=1004570029</span><span class=\"ellipsis\"></span></a></p><p>课程同步配套讲义：</p><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/fengdu78/Coursera-ML-AndrewNg-Notes\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/fengdu78/Cou</span><span class=\"invisible\">rsera-ML-AndrewNg-Notes</span><span class=\"ellipsis\"></span></a> </p><p>原文发布在语雀文档上,效果更好看点：</p><a href=\"https://link.zhihu.com/?target=https%3A//www.yuque.com/docs/share/912c5e5f-acdd-4564-8929-9122f3f40b5b\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-72bbb1e26eacf07192c32d1b6514dde0_ipico.jpg\" data-image-width=\"512\" data-image-height=\"512\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">第二周 · 语雀</a><hr/><h2><b>第二周</b></h2><h2><b>1.多变量线性回归</b></h2><p>在第一周的房价和房屋面积的例子中，由于变量只有一个—面积，所以这类机器学习问题称为单变量线性回归，很明显，当变量数量&gt;1时，即为多变量线性回归</p><h2><b>2.多维特征</b></h2><p>现在，假设除了房屋面积外，又增加了房屋数量、楼层、房屋年龄等特征，则此模型即变为了多变量的模型，模型的特征为:</p><p><img src=\"https://www.zhihu.com/equation?tex=%28x_1%2Cx_2....x_n%29\" alt=\"(x_1,x_2....x_n)\" eeimg=\"1\"/> </p><p>则相应的多维的假设函数为：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-b524ad5d4c3e9ffefe7013a0030d1295_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"222\" data-rawheight=\"17\" class=\"content_image\" width=\"222\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;222&#39; height=&#39;17&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"222\" data-rawheight=\"17\" class=\"content_image lazy\" width=\"222\" data-actualsrc=\"https://pic2.zhimg.com/v2-b524ad5d4c3e9ffefe7013a0030d1295_b.jpg\"/></figure><p>为了方便，此时引入 <img src=\"https://www.zhihu.com/equation?tex=x_0%3D1\" alt=\"x_0=1\" eeimg=\"1\"/> :                          </p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-aeba164195dee630109268f6e968a16f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"237\" data-rawheight=\"17\" class=\"content_image\" width=\"237\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;237&#39; height=&#39;17&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"237\" data-rawheight=\"17\" class=\"content_image lazy\" width=\"237\" data-actualsrc=\"https://pic4.zhimg.com/v2-aeba164195dee630109268f6e968a16f_b.jpg\"/></figure><p>简化一下，假设函数可以简化成：    </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-d1bdf980a52ba677d6f2c5ad65598d55_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"78\" data-rawheight=\"19\" class=\"content_image\" width=\"78\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;78&#39; height=&#39;19&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"78\" data-rawheight=\"19\" class=\"content_image lazy\" width=\"78\" data-actualsrc=\"https://pic2.zhimg.com/v2-d1bdf980a52ba677d6f2c5ad65598d55_b.jpg\"/></figure><p><br/> <br/>其中，特征矩阵 <img src=\"https://www.zhihu.com/equation?tex=X\" alt=\"X\" eeimg=\"1\"/> 的维度是 <img src=\"https://www.zhihu.com/equation?tex=m%E8%A1%8C+%2A+%28n%2B1%29%E5%88%97\" alt=\"m行 * (n+1)列\" eeimg=\"1\"/> ， <img src=\"https://www.zhihu.com/equation?tex=T\" alt=\"T\" eeimg=\"1\"/> 代表特征矩阵的转置</p><h2><b>3.多变量梯度下降</b></h2><p>多变量线性回归的损失函数/代价函数和之前的单变量线性回归类似，用到的还是平均损失函数，只是变量维度多了</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-a6c0de98c6d0448538d5e0a795bed318_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"287\" data-rawheight=\"31\" class=\"content_image\" width=\"287\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;287&#39; height=&#39;31&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"287\" data-rawheight=\"31\" class=\"content_image lazy\" width=\"287\" data-actualsrc=\"https://pic1.zhimg.com/v2-a6c0de98c6d0448538d5e0a795bed318_b.jpg\"/></figure><p>其中：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-5c842c6ce799f0d79e5bb445c1931476_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"282\" data-rawheight=\"19\" class=\"content_image\" width=\"282\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;282&#39; height=&#39;19&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"282\" data-rawheight=\"19\" class=\"content_image lazy\" width=\"282\" data-actualsrc=\"https://pic3.zhimg.com/v2-5c842c6ce799f0d79e5bb445c1931476_b.jpg\"/></figure><p><br/>我们的目标和单变量线性回归问题中一样，是要找出使得代价函数最小的一系列参数。<br/>多变量线性回归的批量梯度下降算法为：<br/> <br/><b>Repeat {</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-0a9ee413df4d2252776d11f1a3ac73a9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"182\" data-rawheight=\"37\" class=\"content_image\" width=\"182\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;182&#39; height=&#39;37&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"182\" data-rawheight=\"37\" class=\"content_image lazy\" width=\"182\" data-actualsrc=\"https://pic2.zhimg.com/v2-0a9ee413df4d2252776d11f1a3ac73a9_b.jpg\"/></figure><p><b>}</b></p><p><b>即：</b><br/><b> Repeat {</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-73550e9111f7da5f00abf7e83b579e7b_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"267\" data-rawheight=\"37\" class=\"content_image\" width=\"267\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;267&#39; height=&#39;37&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"267\" data-rawheight=\"37\" class=\"content_image lazy\" width=\"267\" data-actualsrc=\"https://pic4.zhimg.com/v2-73550e9111f7da5f00abf7e83b579e7b_b.jpg\"/></figure><p><b>}</b></p><p><b>求导后得到：</b></p><p><b>Repeat {</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-3fc4905a3abda80bb6662d4eec5ed991_b.jpg\" data-size=\"normal\" data-rawwidth=\"230\" data-rawheight=\"31\" class=\"content_image\" width=\"230\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;230&#39; height=&#39;31&#39;&gt;&lt;/svg&gt;\" data-size=\"normal\" data-rawwidth=\"230\" data-rawheight=\"31\" class=\"content_image lazy\" width=\"230\" data-actualsrc=\"https://pic2.zhimg.com/v2-3fc4905a3abda80bb6662d4eec5ed991_b.jpg\"/><figcaption>           for (j = 0,1,2...n)</figcaption></figure><p><b>}</b></p><p>特征维度&gt;1时(n&gt;1)有： </p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-8c7b7b562dee783851e196ef179e03c8_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"232\" data-rawheight=\"31\" class=\"content_image\" width=\"232\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;232&#39; height=&#39;31&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"232\" data-rawheight=\"31\" class=\"content_image lazy\" width=\"232\" data-actualsrc=\"https://pic1.zhimg.com/v2-8c7b7b562dee783851e196ef179e03c8_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-25a893008fb5164f0a30176cf213de54_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"232\" data-rawheight=\"31\" class=\"content_image\" width=\"232\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;232&#39; height=&#39;31&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"232\" data-rawheight=\"31\" class=\"content_image lazy\" width=\"232\" data-actualsrc=\"https://pic1.zhimg.com/v2-25a893008fb5164f0a30176cf213de54_b.jpg\"/></figure><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-5370802589d2782316f6541578343b00_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"232\" data-rawheight=\"31\" class=\"content_image\" width=\"232\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;232&#39; height=&#39;31&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"232\" data-rawheight=\"31\" class=\"content_image lazy\" width=\"232\" data-actualsrc=\"https://pic1.zhimg.com/v2-5370802589d2782316f6541578343b00_b.jpg\"/></figure><p>开始随机选择一系列参数值，计算所有预测结果，再给所有参数一个新的值，如此循环直到收敛，即损失函数局部最小值。代码示例：</p><div class=\"highlight\"><pre><code class=\"language-python3\"><span class=\"k\">def</span> <span class=\"nf\">computeCost</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">,</span> <span class=\"n\">y</span><span class=\"p\">,</span> <span class=\"n\">theta</span><span class=\"p\">):</span>\n    <span class=\"n\">inner</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">power</span><span class=\"p\">(((</span><span class=\"n\">X</span> <span class=\"o\">*</span> <span class=\"n\">theta</span><span class=\"o\">.</span><span class=\"n\">T</span><span class=\"p\">)</span> <span class=\"o\">-</span> <span class=\"n\">y</span><span class=\"p\">),</span> <span class=\"mi\">2</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"nb\">sum</span><span class=\"p\">(</span><span class=\"n\">inner</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"mi\">2</span><span class=\"o\">*</span> <span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">X</span><span class=\"p\">))</span></code></pre></div><h2><b>4.特征缩放</b></h2><p>还是以房子为例，多变量线性回归模型中，房价不仅取决于房屋面积这个特征，还取决于房屋数量这个特征。这两个特征的范围分别为：<br/>面积：0~2000平方英尺<br/>房屋数量：0~5</p><p>以这两个特征绘制的等高线图如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-2e687a5a600c347510eb082a7737137a_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"258\" data-rawheight=\"267\" class=\"content_image\" width=\"258\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;258&#39; height=&#39;267&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"258\" data-rawheight=\"267\" class=\"content_image lazy\" width=\"258\" data-actualsrc=\"https://pic3.zhimg.com/v2-2e687a5a600c347510eb082a7737137a_b.jpg\"/></figure><p>从图中可以看出，图像比较扁，如果，梯度下降算法需要经过非常多次的迭代才能收敛，这时就需要通过一种手段来将这些特征的尺度都缩小到合适的范围，譬如-1~1之间，这种方式就叫做<b>特征缩放</b><br/>对x1特征，只需除以2000，对x2特征，除以5。即可使得特征值范围属于0~1</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-0d2df911400f2cdf62cc800d142a2b02_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"764\" data-rawheight=\"481\" class=\"origin_image zh-lightbox-thumb\" width=\"764\" data-original=\"https://pic3.zhimg.com/v2-0d2df911400f2cdf62cc800d142a2b02_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;764&#39; height=&#39;481&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"764\" data-rawheight=\"481\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"764\" data-original=\"https://pic3.zhimg.com/v2-0d2df911400f2cdf62cc800d142a2b02_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-0d2df911400f2cdf62cc800d142a2b02_b.jpg\"/></figure><h2><b>5.学习率</b></h2><p>      梯度下降算法收敛所需要的迭代次数根据模型的不同而不同，我们不能提前预知，我们<br/>可以绘制迭代次数和代价函数的图表来观测算法在何时趋于收敛。<br/>如下图所示：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-ee8bada1b81a67f002c44ff66d18858d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"480\" class=\"origin_image zh-lightbox-thumb\" width=\"640\" data-original=\"https://pic2.zhimg.com/v2-ee8bada1b81a67f002c44ff66d18858d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;640&#39; height=&#39;480&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"640\" data-rawheight=\"480\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"640\" data-original=\"https://pic2.zhimg.com/v2-ee8bada1b81a67f002c44ff66d18858d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-ee8bada1b81a67f002c44ff66d18858d_b.jpg\"/></figure><p><br/>横轴为迭代次数、纵轴为损失函数值—loss，可以看见，<b>通常在训练刚开始，单位迭代次数下，loss下降的最快，随着迭代次数增加、loss下降的越来越慢，直至近乎停止，趋于收敛。</b></p><p>      有一些自动测试是否收敛的方法，例如将代价函数的变化值与某个阀值(例如 0.001)<br/>进行比较，但通常看上面这样的图表更好。<br/> <b>梯度下降算法的每次迭代受到学习率的影响，如果学习率𝑎过小，则达到收敛所需的迭代次数会非常高;如果学习率𝑎过大，每次迭代可能不会减小代价函数，可能会越过局部最</b>br/&gt;小值导致无法收敛。</p><p>通常可以考虑尝试这些学习率:  𝛼 = 0.01，0.03，0.1，0.3，1，3，10  </p><p></p><p></p>", 
            "topic": [
                {
                    "tag": "多元线性回归", 
                    "tagLink": "https://api.zhihu.com/topics/19665399"
                }, 
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "吴恩达（Andrew Ng）", 
                    "tagLink": "https://api.zhihu.com/topics/20003150"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/73363177", 
            "userName": "Lyon", 
            "userLink": "https://www.zhihu.com/people/38495a9b1f83e4a612543ce6c523df7f", 
            "upvote": 7, 
            "title": "【吴恩达机器学习】第一周—单变量线性回归", 
            "content": "<p><b>前言：</b></p><p>      最近在学习深度学习，看了不少教程，发现还是吴恩达的比较适用于我。吴恩达机器学习公开课视频最早是斯坦福大学的课程视频(那个画面有点老)，新版的视频在网易云课堂上可以随时学习。仅仅通过视频学习，可能会有点快，因为有的知识点需要反复推敲和回味。感谢github上一位朋友的精心整理，让我们可以配合讲义一起学习，讲义有HTML版、PDF版、Markdown版的：视频配合讲义看，事半功倍</p><p>        本文包括接下来的系列文章，都会是吴恩达机器学习课程的个人学习总结，有很多文字和图片直接参考了PDF版的课程讲义，再次谢过提供讲义的朋友！</p><p>网易云课堂课程视频：</p><p><a href=\"https://link.zhihu.com/?target=https%3A//study.163.com/course/courseMain.htm%3FcourseId%3D1004570029\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">study.163.com/course/co</span><span class=\"invisible\">urseMain.htm?courseId=1004570029</span><span class=\"ellipsis\"></span></a> </p><p>课程同步配套讲义：</p><p><a href=\"https://link.zhihu.com/?target=https%3A//github.com/fengdu78/Coursera-ML-AndrewNg-Notes\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/fengdu78/Cou</span><span class=\"invisible\">rsera-ML-AndrewNg-Notes</span><span class=\"ellipsis\"></span></a> </p><hr/><p>原文发布在语雀文档上,效果更好看点：</p><a href=\"https://link.zhihu.com/?target=https%3A//www.yuque.com/docs/share/88a6a4e1-4ae4-42ed-b3fa-7b48c9512474\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-72bbb1e26eacf07192c32d1b6514dde0_ipico.jpg\" data-image-width=\"512\" data-image-height=\"512\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">第一周 · 语雀</a><hr/><h2><b>第一周</b></h2><h2><b>1.课程回顾</b></h2><h2><b>例1：房价和面积—预测</b></h2><p>给定一组房价和房屋面积的数据集，通过机器学习算法(监督学习)来拟合画出一条线，根据这条线来对未知的数据进行判断。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-cf843a78b7e15cb0fb3632de1e22b82c_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"692\" data-rawheight=\"507\" class=\"origin_image zh-lightbox-thumb\" width=\"692\" data-original=\"https://pic1.zhimg.com/v2-cf843a78b7e15cb0fb3632de1e22b82c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;692&#39; height=&#39;507&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"692\" data-rawheight=\"507\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"692\" data-original=\"https://pic1.zhimg.com/v2-cf843a78b7e15cb0fb3632de1e22b82c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-cf843a78b7e15cb0fb3632de1e22b82c_b.jpg\"/></figure><p>      假设机器通过这些房价数据学习得到一条房价—面积关系线，如上图中红线，那么如果你朋友的750英尺的房子，就可以通过这条红线来估算出房价，可以看出，大约是在150K美金左右。</p><h3><b>这是一个典型的回归问题(Regression)，因为结果(房价)可以是任意实数，且数据是可以连续的。</b></h3><p>更进一步，由于变量1个—房屋尺寸，且预测结果(房价)和变量间的表达式可以用线性方程描述y = k*x + b所有，此问题在机器学习中的术语叫做：</p><p><b>单变量线性回归 Linear Regression with One Variable</b></p><h2><b>例2：乳腺癌良性/恶性和尺寸关系</b></h2><p>给定一组数据：乳腺癌肿瘤尺寸大小 和 癌症恶性/良性关系，通过机器学习，来预测一个给定肿瘤尺寸大小的患者，疾病是恶性还是良性的概率。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-91519d48823d29cc726049feb815e07f_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"715\" data-rawheight=\"500\" class=\"origin_image zh-lightbox-thumb\" width=\"715\" data-original=\"https://pic4.zhimg.com/v2-91519d48823d29cc726049feb815e07f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;715&#39; height=&#39;500&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"715\" data-rawheight=\"500\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"715\" data-original=\"https://pic4.zhimg.com/v2-91519d48823d29cc726049feb815e07f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-91519d48823d29cc726049feb815e07f_b.jpg\"/></figure><h3><b>这个问题和之前的房价问题有些区别，虽然同样的，变量只有一个，即x轴上的尺寸大小，但是y轴的结果却只有两种：0和1，0表示肿瘤分类为良性；1表示分类为恶性。</b></h3><h3><b>所以，这是一个典型的分类问题（Classification）由于这里结果只有两种，所以此分类问题为二元分类问题</b></h3><h2><b>2.分类Classification</b></h2><p>这两个例子都比较通俗易懂，新接触到的名词虽然有点多但也无需过度理解，两个例子中涉及到了监督学习中的<b>分类问题</b>、<b>回归问题</b>。分类问题很好理解，机器预测值只有固定的几个类别,课程中为两种：0良性肿瘤  1 恶性肿瘤，所以称为二分类，除了二分类以外，其他的统称多分类。</p><h2><b>3.回归Regression</b></h2><p>与分类问题相对，回归问题的预测值范围比较自由，可以是连续的，且可取任意实数</p><h2><b>4.监督学习Supervised Learning</b></h2><p>课程中的两个问题：房价-房屋面积、肿瘤良恶—肿瘤尺寸都是属于监督学习。可以先简单理解，监督学习就是给定了数据集，且数据是规则明确的，有标签的。可以理解为结构化的、存储在数据库中的数据，喂给机器学习的情况，就叫做监督学习。</p><h2><b>5.无监督学习Unsupervised Learning</b></h2><p>与监督学习相对，如果给出的数据集、没有明确标签或者是非结构化的，那么这类数据的机器学习，就叫做无监督学习。除了监督学习、无监督学习、还有半监督学习的概念。具体可以看看知乎：<a href=\"https://www.zhihu.com/question/23194489/answer/25028661\" class=\"internal\"><span class=\"invisible\">https://www.</span><span class=\"visible\">zhihu.com/question/2319</span><span class=\"invisible\">4489/answer/25028661</span><span class=\"ellipsis\"></span></a></p><h2><b>6.假设hypothesis</b></h2><p>我们以例1为例说明hypothesis(假设)，这也涉及到后面的代价函数。<br/>ℎ—hypothesis(假设)，代表学习算法的解决方案或函数。<br/>ℎ表示一个函数，实际上即模型的预测函数，在例1中，输入是房屋尺寸大小，就像你朋友想出售的房屋，因此 ℎ 根据输入的房屋面积值x 来得到此防房屋估价y，因此，ℎ是一个从x 到 y的映射，由于是单变量线性回归，映射关系可以简单表示为：</p><p><img src=\"https://www.zhihu.com/equation?tex=y+%3D+k%2Ax+%2B+b\" alt=\"y = k*x + b\" eeimg=\"1\"/> </p><p>等价于 : <img src=\"https://www.zhihu.com/equation?tex=h_%5Ctheta%28x%29%3D+%5Ctheta_0+%2B+%5Ctheta_1x\" alt=\"h_\\theta(x)= \\theta_0 + \\theta_1x\" eeimg=\"1\"/> </p><h2><b>举个栗子</b></h2><p>很明显，在此例中，通过假设函数，我们可以对数据集以外的样本做预测，如果假设函数是： <img src=\"https://www.zhihu.com/equation?tex=h_%5Ctheta%28x%29+%3D+++1%2F4x\" alt=\"h_\\theta(x) =   1/4x\" eeimg=\"1\"/> ,那么如果你的朋友有一套面积为500(英尺)的房子，那么你就可以告诉他，你的房价预估在125K(12.5万)美金</p><h2><b>误差</b></h2><p>你那个朋友过来找到你了，说你的假设有误啊，我那套500英尺的房子，明明就是100K而已，你的数据集的数据还记录了我的房子呢......你这有误差啊？！<br/>预测值125和真实值之间差距为25K，这里25K就被称为建模误差。</p><h2><b>7.损失函数Cost Function</b></h2><p><b>代价函数/损失函数，就是用于评估误差水平的函数，常见的损失函数有平方损失函数、交叉熵损失函数，其中前者多用于回归问题，后者多用于分类问题。</b>理论上，给定一批房屋面积—价格数据点，我可以根据这批数据画出无数条假设函数 <img src=\"https://www.zhihu.com/equation?tex=h_%5Ctheta%28x%29\" alt=\"h_\\theta(x)\" eeimg=\"1\"/> 直线用于模拟房价和面积之间的关系，那么怎么找到最优的那条线？这时，我们就会用到代价函数，好的代价函数必然使得数据集总体的误差最小。                                     ‘</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-aa1bf6ca6ea2e96c2ebc7da879cccfc9_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"316\" data-rawheight=\"196\" class=\"content_image\" width=\"316\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;316&#39; height=&#39;196&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"316\" data-rawheight=\"196\" class=\"content_image lazy\" width=\"316\" data-actualsrc=\"https://pic2.zhimg.com/v2-aa1bf6ca6ea2e96c2ebc7da879cccfc9_b.jpg\"/></figure><p>举个例子，此时我有三个样本点 （1，1），（2，2），（3，3）,我怎么确定一个假设函数h，使得这条线能最优化地拟合所有数据，能更精确地预测下一个位置样本点的数据，譬如x = 5时 y的值？这里人眼很明显一看就能确定</p><p><img src=\"https://www.zhihu.com/equation?tex=h_%5Ctheta%28x%29+%3D+0+%2B+x\" alt=\"h_\\theta(x) = 0 + x\" eeimg=\"1\"/> </p><p>  即可，不过对于机器，怎么去确定这个方程？<br/>此时就需要用到代价函数，这里我们可以用回归问题通用的平方损失函数/平方代价函数，评估假设函数的误差水平。这里，例1的代价函数如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-48c54bd5545bbbe59aa0cf9856ac4bc4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"268\" data-rawheight=\"39\" class=\"content_image\" width=\"268\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;268&#39; height=&#39;39&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"268\" data-rawheight=\"39\" class=\"content_image lazy\" width=\"268\" data-actualsrc=\"https://pic1.zhimg.com/v2-48c54bd5545bbbe59aa0cf9856ac4bc4_b.jpg\"/></figure><p><br/>直白点意思就是：求每个样本点i 的误差的平方，累加求平均值，关于最后为什么是1/2m 而不是 1/m，这个其实主要是通用的约定，为了求导方便。</p><p><b>损失函数的意义就在于，通过求损失函数的值，我们可以评估预测函数的准确性，损失越小，则模型的预测越精准。所以，训练模型很多时候就是降低损失，找到损失函数的最小值，通过其最小值来产出最优的假设函数。 </b></p><h2><b>8.二元函数梯度下降Gradient Descent</b></h2><h2><b>图解</b></h2><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-198e22140452761a599f936364923ff6_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"752\" data-rawheight=\"774\" class=\"origin_image zh-lightbox-thumb\" width=\"752\" data-original=\"https://pic3.zhimg.com/v2-198e22140452761a599f936364923ff6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;752&#39; height=&#39;774&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"752\" data-rawheight=\"774\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"752\" data-original=\"https://pic3.zhimg.com/v2-198e22140452761a599f936364923ff6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-198e22140452761a599f936364923ff6_b.jpg\"/></figure><h2><b>概述</b></h2><h3><b>目标</b></h3><p>      找到损失函数的最小值，即 <img src=\"https://www.zhihu.com/equation?tex=J%28%5Ctheta_0%2C%5Ctheta_1%29\" alt=\"J(\\theta_0,\\theta_1)\" eeimg=\"1\"/> <b>最小的点，所以我们利用梯度下降算法(而不是梯度上升)</b>为了找到损失函数的最小值，我们采用梯度下降算法，图中为二元函数梯度下降图解，如果参数为更多元，则无法绘制出相应的梯度下降图，不过这幅图恰巧能生动地解释梯度下降的含义。<br/>      这里x、y轴分别为参数 <img src=\"https://www.zhihu.com/equation?tex=J%28%5Ctheta_0%2C%5Ctheta_1%29\" alt=\"J(\\theta_0,\\theta_1)\" eeimg=\"1\"/>所在维度，z轴代表代价函数J的大小，所有的 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_0%E5%92%8C%5Ctheta_1\" alt=\"\\theta_0和\\theta_1\" eeimg=\"1\"/>参数点和损失函数<img src=\"https://www.zhihu.com/equation?tex=J%28%5Ctheta_0%2C%5Ctheta_1%29\" alt=\"J(\\theta_0,\\theta_1)\" eeimg=\"1\"/>值构成了五彩的三维曲面，最终目标：即通过梯度下降算法，找到<img src=\"https://www.zhihu.com/equation?tex=J%28%5Ctheta_0%2C%5Ctheta_1%29\" alt=\"J(\\theta_0,\\theta_1)\" eeimg=\"1\"/>最小的点。肉眼可见右边红色箭头所指的点，为局部梯度最低点，即局部<img src=\"https://www.zhihu.com/equation?tex=J%28%5Ctheta_0%2C%5Ctheta_1%29\" alt=\"J(\\theta_0,\\theta_1)\" eeimg=\"1\"/>最小值点；左边红色箭头所指的点为整个三维曲面上的最小值点，即全局最低点。</p><h2><b>公式</b></h2><p>这里，<b>批量梯度下降（batch gradient descent）</b>算法的公式为：</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_j+%3A%3D+%5Ctheta_j+-+%5Calpha%5Cfrac%7B%5Cvartheta%7D%7B%5Cvartheta%5Ctheta_j%7DJ%28%5Ctheta_0%2C%5Ctheta_1%29v\" alt=\"\\theta_j := \\theta_j - \\alpha\\frac{\\vartheta}{\\vartheta\\theta_j}J(\\theta_0,\\theta_1)v\" eeimg=\"1\"/> <b>for( j = 0 and j = 1)</b></p><p><b>参数解释</b></p><p>1.公式中为什么是 <img src=\"https://www.zhihu.com/equation?tex=-%5Calpha\" alt=\"-\\alpha\" eeimg=\"1\"/> ，因为用于表示梯度下降，即逐渐降低，故用负号表示<br/>2.公式中用的是<b> :=</b> 符号，此含义表示，等式中的参数需要同时更新</p><p>如此处有两个参数<img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_0%E5%92%8C%5Ctheta_1\" alt=\"\\theta_0和\\theta_1\" eeimg=\"1\"/>，正确的更新方式是：<br/>第一步：  <img src=\"https://www.zhihu.com/equation?tex=temp0+%3D+%5Ctheta_0++-+%5Calpha%5Cfrac%7B%5Cvartheta%7D%7B%5Cvartheta%5Ctheta_0%7DJ%28%5Ctheta_0%2C%5Ctheta_1%29\" alt=\"temp0 = \\theta_0  - \\alpha\\frac{\\vartheta}{\\vartheta\\theta_0}J(\\theta_0,\\theta_1)\" eeimg=\"1\"/> <img src=\"https://www.zhihu.com/equation?tex=temp1+%3D+%5Ctheta_1++-+%5Calpha%5Cfrac%7B%5Cvartheta%7D%7B%5Cvartheta%5Ctheta_1%7DJ%28%5Ctheta_0%2C%5Ctheta_1%29\" alt=\"temp1 = \\theta_1  - \\alpha\\frac{\\vartheta}{\\vartheta\\theta_1}J(\\theta_0,\\theta_1)\" eeimg=\"1\"/> <br/>第二步：  <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_0+%3D+temp0\" alt=\"\\theta_0 = temp0\" eeimg=\"1\"/> <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_1+%3D+temp1\" alt=\"\\theta_1 = temp1\" eeimg=\"1\"/> </p><p><br/>其中第一步完成后才能进行第二步，且在每一步执行时，内部求temp1、temp2时也是并行的关系</p><h2><b>学习率</b></h2><p><b>批量梯度下降公式中，</b> <img src=\"https://www.zhihu.com/equation?tex=%5Calpha\" alt=\"\\alpha\" eeimg=\"1\"/> <b>为学习率（learning rate）</b>它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大，在批量梯度下降中，我们每一次都同时让所有的参数减去学习速率乘以代价函数的导数</p><h3><b>学习率过小</b></h3><p>如果 <img src=\"https://www.zhihu.com/equation?tex=%5Calpha\" alt=\"\\alpha\" eeimg=\"1\"/> 太小了，即我的学习速率太小，结果就是只能这样像小宝宝一样一点点地挪动，去努力接近最低点，这样就需要很多步才能到达最低点。所以，如果<img src=\"https://www.zhihu.com/equation?tex=%5Calpha\" alt=\"\\alpha\" eeimg=\"1\"/>太小的话可能会很慢，因为它会一点点挪动，它会需要很多步才能到达全局最低点。</p><h3><b>学习率过大</b></h3><p>如果<img src=\"https://www.zhihu.com/equation?tex=%5Calpha\" alt=\"\\alpha\" eeimg=\"1\"/>太大，那么梯度下降法可能会越过最低点，甚至可能无法收敛，下一次迭代又移动了一大步，越过一次，又越过一次，一次次越过最低点，直到你发现实际上离最低点越来越远。所以，如果<img src=\"https://www.zhihu.com/equation?tex=%5Calpha\" alt=\"\\alpha\" eeimg=\"1\"/>太大，它会导致无法收敛，甚至发散</p><h2><b>9.梯度下降的线性回归</b></h2><p>线性回归模型 : <img src=\"https://www.zhihu.com/equation?tex=h_%5Ctheta%28x%29%3D+%5Ctheta_0+%2B+%5Ctheta_1x\" alt=\"h_\\theta(x)= \\theta_0 + \\theta_1x\" eeimg=\"1\"/> </p><p>线性回归模型的损失函数</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-48c54bd5545bbbe59aa0cf9856ac4bc4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"268\" data-rawheight=\"39\" class=\"content_image\" width=\"268\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;268&#39; height=&#39;39&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"268\" data-rawheight=\"39\" class=\"content_image lazy\" width=\"268\" data-actualsrc=\"https://pic1.zhimg.com/v2-48c54bd5545bbbe59aa0cf9856ac4bc4_b.jpg\"/></figure><p> 梯度下降算法:</p><p><img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_j+%3A%3D+%5Ctheta_j+-+%5Calpha%5Cfrac%7B%5Cvartheta%7D%7B%5Cvartheta%5Ctheta_j%7DJ%28%5Ctheta_0%2C%5Ctheta_1%29\" alt=\"\\theta_j := \\theta_j - \\alpha\\frac{\\vartheta}{\\vartheta\\theta_j}J(\\theta_0,\\theta_1)\" eeimg=\"1\"/> <b>(for j = 0 and j = 1)</b></p><p>对损失函数/代价函数运用梯度下降方法，求导：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-8bab59bc5afab1a893b8cc6a4adffe7d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"522\" data-rawheight=\"409\" class=\"origin_image zh-lightbox-thumb\" width=\"522\" data-original=\"https://pic2.zhimg.com/v2-8bab59bc5afab1a893b8cc6a4adffe7d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;522&#39; height=&#39;409&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"522\" data-rawheight=\"409\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"522\" data-original=\"https://pic2.zhimg.com/v2-8bab59bc5afab1a893b8cc6a4adffe7d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-8bab59bc5afab1a893b8cc6a4adffe7d_b.jpg\"/></figure><p><b>Tips:</b> <br/>j = 0时即对 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_0\" alt=\"\\theta_0\" eeimg=\"1\"/> 求导， <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cvartheta+h_%5Ctheta+%28x%5Ei%29%7D%7B%5Cvartheta%5Ctheta_0%7D+%3D+1\" alt=\"\\frac{\\vartheta h_\\theta (x^i)}{\\vartheta\\theta_0} = 1\" eeimg=\"1\"/> </p><p><br/>j = 1时即对 <img src=\"https://www.zhihu.com/equation?tex=%5Ctheta_1\" alt=\"\\theta_1\" eeimg=\"1\"/> 求导， <img src=\"https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cvartheta+h_%5Ctheta+%28x%5Ei%29%7D%7B%5Cvartheta%5Ctheta_1%7D+%3D+x%5Ei\" alt=\"\\frac{\\vartheta h_\\theta (x^i)}{\\vartheta\\theta_1} = x^i\" eeimg=\"1\"/> </p><p></p><p></p>", 
            "topic": [
                {
                    "tag": "吴恩达（Andrew Ng）", 
                    "tagLink": "https://api.zhihu.com/topics/20003150"
                }, 
                {
                    "tag": "线性回归", 
                    "tagLink": "https://api.zhihu.com/topics/19650500"
                }, 
                {
                    "tag": "梯度下降", 
                    "tagLink": "https://api.zhihu.com/topics/19650497"
                }
            ], 
            "comments": [
                {
                    "userName": "火凤燎原", 
                    "userLink": "https://www.zhihu.com/people/7f124d467a5fed41bcd9f9607c4f1361", 
                    "content": "<p>答主，可以转载吗？</p>", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "Lyon", 
                            "userLink": "https://www.zhihu.com/people/38495a9b1f83e4a612543ce6c523df7f", 
                            "content": "可以的，注明来源就行", 
                            "likes": 0, 
                            "replyToAuthor": "火凤燎原"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/69680445", 
            "userName": "Lyon", 
            "userLink": "https://www.zhihu.com/people/38495a9b1f83e4a612543ce6c523df7f", 
            "upvote": 8, 
            "title": "Yolo-v3目标检测—Java调用C++(JNI)", 
            "content": "<h2>前言</h2><p>其实这篇文章重点在如何用Java的JNI调用C++的dll，记录一下，避免以后自己忘了.....</p><p>原文发表在<a href=\"https://link.zhihu.com/?target=https%3A//www.yuque.com/docs/share/c252c572-2868-4157-acdf-b622f6385eab\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">语雀文档</a>上，排版更美观</p><hr/><h2>简介</h2><p><a href=\"https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/JNI/9412164%3Ffr%3Daladdin\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">JNI—摘自百度百科</a><br/>JNI是Java Native Interface的缩写，它提供了若干的<a href=\"https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/API/10154\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">API</a>实现了Java和其他语言的通信（主要是<a href=\"https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/C/7252092\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">C</a>&amp;<a href=\"https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/C%252B%252B\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">C++</a>）。从Java1.1开始，JNI标准成为java平台的一部分，它允许Java代码和其他语言写的代码进行交互。JNI一开始是为了本地已<a href=\"https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%25E7%25BC%2596%25E8%25AF%2591/1258343\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">编译</a>语言，尤其是C和C++而设计的，但是它并不妨碍你使用其他编程语言，只要调用约定受支持就可以了。使用java与本地已编译的代码<a href=\"https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%25E4%25BA%25A4%25E4%25BA%2592/6964417\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">交互</a>，通常会丧失平台<a href=\"https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%25E5%258F%25AF%25E7%25A7%25BB%25E6%25A4%258D%25E6%2580%25A7/6931884\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">可移植性</a>。但是，有些情况下这样做是可以接受的，甚至是必须的。例如，使用一些旧的库，与硬件、操作系统进行交互，或者为了提高程序的性能。JNI标准至少要保证<a href=\"https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%25E6%259C%25AC%25E5%259C%25B0%25E4%25BB%25A3%25E7%25A0%2581\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">本地代码</a>能工作在任何Java <a href=\"https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%25E8%2599%259A%25E6%258B%259F%25E6%259C%25BA\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">虚拟机</a>环境。</p><p>由于近期在玩yolo、darknet，C++项目下的图像识别(目标检测)，想尝试下将生成的dll提供给Java服务端调用，于是就有了本篇文章~记个流水账怕以后自己忘了....</p><hr/><h2>流程</h2><p>整体来说，要直接将dll被java调用是不可能的，因为两种语言基本数据类型、方法定义这些是不同的，所以需要用VS新建一个dll项目，生成java项目中可调用的dll。 </p><h2>1.新建native接口方法类</h2><p>在Java项目中任意位置新建一个类，声名需要用到的native方法，凡是用native修饰的方法，都是后面调用的dll中的方法(C++实现),static块中System.load方法即可实现加载dll，在刚开始这部分可以忽略不写，等VS生成dll后再过来添加。<br/>DarknetJavaSDK.java</p><div class=\"highlight\"><pre><code class=\"language-java\"><span class=\"kn\">package</span> <span class=\"nn\">com.xxx.ai.image.detection.service.sdk</span><span class=\"o\">;</span>\n\n<span class=\"kn\">import</span> <span class=\"nn\">java.io.File</span><span class=\"o\">;</span>\n\n<span class=\"kd\">public</span> <span class=\"kd\">class</span> <span class=\"nc\">DarknetJavaSDK</span> <span class=\"o\">{</span>\n\n    <span class=\"kd\">public</span> <span class=\"kd\">native</span> <span class=\"n\">String</span> <span class=\"nf\">get_version</span><span class=\"o\">();</span>\n\n    <span class=\"kd\">public</span> <span class=\"kd\">native</span> <span class=\"kt\">boolean</span> <span class=\"nf\">set_logfile_path</span><span class=\"o\">(</span><span class=\"n\">String</span> <span class=\"n\">logPath</span><span class=\"o\">);</span>\n\n    <span class=\"kd\">public</span> <span class=\"kd\">native</span> <span class=\"kt\">boolean</span> <span class=\"nf\">load_model</span><span class=\"o\">(</span><span class=\"n\">String</span> <span class=\"n\">cfgPath</span><span class=\"o\">,</span> <span class=\"n\">String</span> <span class=\"n\">modelPath</span><span class=\"o\">);</span>\n\n    <span class=\"kd\">public</span> <span class=\"kd\">native</span> <span class=\"kt\">int</span> <span class=\"nf\">detect_image</span><span class=\"o\">(</span><span class=\"n\">String</span> <span class=\"n\">imagePath</span><span class=\"o\">,</span> <span class=\"n\">String</span> <span class=\"n\">outDirPath</span><span class=\"o\">);</span></code></pre></div><h2>2.生成.h头文件</h2><p>生成头文件时，因为DarknetJavaSDK.java文件从属于包：<br/>package com.xxx.ai.image.detection.service.sdk;所以，需要cd到.../src/main/java目录下(即com/xxx/ai的上一级目录)，运行：<br/><i><b>javah com.xxx.ai.image.detection.service.sdk.DarknetJavaSDK</b></i><br/>即可在当前目录下生成.h文件：com_xxx_ai_image_detection_service_sdk_DarknetJavaSDK.h</p><blockquote><b>特别注意：</b>如果当前类：DarknetJavaSDK.java 中有依赖其他你自定义的Java类，则可能报错，因为类加载的路径中找不到。解决方法：指定-classpath到.../src/main/java目录下，这样即可加载到此路径下com包下的所有依赖类<br/>例如：javah -classpath D:\\personalProject\\AI\\image\\detection\\src\\main\\java com.flowingbit.ai.image.detection.service.sdk.DarknetJavaSDK<br/> </blockquote><div class=\"highlight\"><pre><code class=\"language-cpp\"><span class=\"cm\">/* DO NOT EDIT THIS FILE - it is machine generated */</span>\n<span class=\"cp\">#include</span> <span class=\"cpf\">&lt;jni.h&gt;</span><span class=\"cp\">\n</span><span class=\"cp\"></span><span class=\"cm\">/* Header for class com_xxx_ai_image_detection_service_sdk_DarknetJavaSDK */</span>\n\n<span class=\"cp\">#ifndef _Included_com_xxx_ai_image_detection_service_sdk_DarknetJavaSDK\n</span><span class=\"cp\">#define _Included_com_xxx_ai_image_detection_service_sdk_DarknetJavaSDK\n</span><span class=\"cp\">#ifdef __cplusplus\n</span><span class=\"cp\"></span><span class=\"k\">extern</span> <span class=\"s\">&#34;C&#34;</span> <span class=\"p\">{</span>\n<span class=\"cp\">#endif\n</span><span class=\"cp\"></span><span class=\"cm\">/*\n</span><span class=\"cm\"> * Class:     com_xxx_ai_image_detection_service_sdk_DarknetJavaSDK\n</span><span class=\"cm\"> * Method:    get_version\n</span><span class=\"cm\"> * Signature: ()Ljava/lang/String;\n</span><span class=\"cm\"> */</span>\n<span class=\"n\">JNIEXPORT</span> <span class=\"n\">jstring</span> <span class=\"n\">JNICALL</span> <span class=\"n\">Java_com_xxx_ai_image_detection_service_sdk_DarknetJavaSDK_get_1version</span>\n  <span class=\"p\">(</span><span class=\"n\">JNIEnv</span> <span class=\"o\">*</span><span class=\"p\">,</span> <span class=\"n\">jobject</span><span class=\"p\">);</span>\n\n<span class=\"cm\">/*\n</span><span class=\"cm\"> * Class:     com_xxx_ai_image_detection_service_sdk_DarknetJavaSDK\n</span><span class=\"cm\"> * Method:    set_logfile_path\n</span><span class=\"cm\"> * Signature: (Ljava/lang/String;)Z\n</span><span class=\"cm\"> */</span>\n<span class=\"n\">JNIEXPORT</span> <span class=\"n\">jboolean</span> <span class=\"n\">JNICALL</span> <span class=\"nf\">Java_com_xxx_ai_image_detection_service_sdk_DarknetJavaSDK_set_1logfile_1path</span>\n  <span class=\"p\">(</span><span class=\"n\">JNIEnv</span> <span class=\"o\">*</span><span class=\"p\">,</span> <span class=\"n\">jobject</span><span class=\"p\">,</span> <span class=\"n\">jstring</span><span class=\"p\">);</span>\n\n<span class=\"cm\">/*\n</span><span class=\"cm\"> * Class:     com_xxx_ai_image_detection_service_sdk_DarknetJavaSDK\n</span><span class=\"cm\"> * Method:    load_model\n</span><span class=\"cm\"> * Signature: (Ljava/lang/String;Ljava/lang/String;)Z\n</span><span class=\"cm\"> */</span>\n<span class=\"n\">JNIEXPORT</span> <span class=\"n\">jboolean</span> <span class=\"n\">JNICALL</span> <span class=\"nf\">Java_com_xxx_ai_image_detection_service_sdk_DarknetJavaSDK_load_1model</span>\n  <span class=\"p\">(</span><span class=\"n\">JNIEnv</span> <span class=\"o\">*</span><span class=\"p\">,</span> <span class=\"n\">jobject</span><span class=\"p\">,</span> <span class=\"n\">jstring</span><span class=\"p\">,</span> <span class=\"n\">jstring</span><span class=\"p\">);</span>\n\n<span class=\"cm\">/*\n</span><span class=\"cm\"> * Class:     com_xxx_ai_image_detection_service_sdk_DarknetJavaSDK\n</span><span class=\"cm\"> * Method:    detect_image\n</span><span class=\"cm\"> * Signature: (Ljava/lang/String;Ljava/lang/String;)I\n</span><span class=\"cm\"> */</span>\n<span class=\"n\">JNIEXPORT</span> <span class=\"n\">jint</span> <span class=\"n\">JNICALL</span> <span class=\"nf\">Java_com_xxx_ai_image_detection_service_sdk_DarknetJavaSDK_detect_1image</span>\n  <span class=\"p\">(</span><span class=\"n\">JNIEnv</span> <span class=\"o\">*</span><span class=\"p\">,</span> <span class=\"n\">jobject</span><span class=\"p\">,</span> <span class=\"n\">jstring</span><span class=\"p\">,</span> <span class=\"n\">jstring</span><span class=\"p\">);</span>\n\n<span class=\"cp\">#ifdef __cplusplus\n</span><span class=\"cp\"></span><span class=\"p\">}</span>\n<span class=\"cp\">#endif\n</span><span class=\"cp\">#endif\n</span></code></pre></div><h2>4.VS新建dll项目</h2><p>我的项目：DarknetDllForJava</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-7251e3b2a5de12ec38c65d4e1874aebe_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1152\" data-rawheight=\"614\" class=\"origin_image zh-lightbox-thumb\" width=\"1152\" data-original=\"https://pic3.zhimg.com/v2-7251e3b2a5de12ec38c65d4e1874aebe_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1152&#39; height=&#39;614&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1152\" data-rawheight=\"614\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1152\" data-original=\"https://pic3.zhimg.com/v2-7251e3b2a5de12ec38c65d4e1874aebe_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-7251e3b2a5de12ec38c65d4e1874aebe_b.jpg\"/></figure><h3>添加必须的头文件：</h3><p><b>jni.h</b><br/><b>com_xxx_ai_image_detection_service_sdk_DarknetJavaSDK.h</b></p><blockquote> jni.h通常在jdk的include目录下，如我的：C:\\Program Files\\Java\\jdk1.8.0_151\\include<br/> </blockquote><h3>VC++目录下设置包含路径：</h3><p><b>jni.h依赖的路径：</b><br/>C:\\Program Files\\Java\\jdk1.8.0_151\\include<br/>C:\\Program Files\\Java\\jdk1.8.0_151\\include\\win32</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-a713d9e704cb53c39817a5a37f9137c4_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1123\" data-rawheight=\"560\" class=\"origin_image zh-lightbox-thumb\" width=\"1123\" data-original=\"https://pic1.zhimg.com/v2-a713d9e704cb53c39817a5a37f9137c4_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1123&#39; height=&#39;560&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1123\" data-rawheight=\"560\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1123\" data-original=\"https://pic1.zhimg.com/v2-a713d9e704cb53c39817a5a37f9137c4_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-a713d9e704cb53c39817a5a37f9137c4_b.jpg\"/></figure><h3>在DarknetDllForJava.cpp定义.h的导出函数</h3><div class=\"highlight\"><pre><code class=\"language-cpp\"><span class=\"c1\">// DarknetDllForJava.cpp : 定义 DLL 应用程序的导出函数。\n</span><span class=\"c1\">//\n</span><span class=\"c1\"></span>\n<span class=\"cp\">#include</span> <span class=\"cpf\">&#34;stdafx.h&#34;</span><span class=\"cp\">\n</span><span class=\"cp\">#include</span> <span class=\"cpf\">&#34;com_xxx_ai_image_detection_service_sdk_DarknetJavaSDK.h&#34;</span><span class=\"cp\">\n</span><span class=\"cp\">#include</span> <span class=\"cpf\">&#34;dll_api.h&#34;</span><span class=\"cp\">\n</span><span class=\"cp\"></span>\n<span class=\"n\">JNIEXPORT</span> <span class=\"n\">jstring</span> <span class=\"n\">JNICALL</span> <span class=\"nf\">Java_com_xxx_ai_image_detection_service_sdk_DarknetJavaSDK_get_1version</span><span class=\"p\">(</span><span class=\"n\">JNIEnv</span> <span class=\"o\">*</span><span class=\"n\">env</span><span class=\"p\">,</span> <span class=\"n\">jobject</span> <span class=\"n\">obj</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"k\">return</span> <span class=\"n\">get_version</span><span class=\"p\">(</span><span class=\"n\">env</span><span class=\"p\">);</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">JNIEXPORT</span> <span class=\"n\">jboolean</span> <span class=\"n\">JNICALL</span> <span class=\"nf\">Java_com_xxx_ai_image_detection_service_sdk_DarknetJavaSDK_set_1logfile_1path</span><span class=\"p\">(</span><span class=\"n\">JNIEnv</span> <span class=\"o\">*</span><span class=\"n\">env</span><span class=\"p\">,</span> <span class=\"n\">jobject</span> <span class=\"n\">obj</span><span class=\"p\">,</span> <span class=\"n\">jstring</span> <span class=\"n\">logPath</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"k\">return</span> <span class=\"n\">set_logfile_path</span><span class=\"p\">(</span><span class=\"n\">env</span><span class=\"p\">,</span> <span class=\"n\">logPath</span><span class=\"p\">);</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">JNIEXPORT</span> <span class=\"n\">jboolean</span> <span class=\"n\">JNICALL</span> <span class=\"nf\">Java_com_xxx_ai_image_detection_service_sdk_DarknetJavaSDK_load_1model</span><span class=\"p\">(</span><span class=\"n\">JNIEnv</span> <span class=\"o\">*</span><span class=\"n\">env</span><span class=\"p\">,</span> <span class=\"n\">jobject</span><span class=\"p\">,</span> <span class=\"n\">jstring</span> <span class=\"n\">cfgPath</span><span class=\"p\">,</span> <span class=\"n\">jstring</span> <span class=\"n\">modelPath</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"k\">return</span> <span class=\"n\">load_model</span><span class=\"p\">(</span><span class=\"n\">env</span><span class=\"p\">,</span> <span class=\"n\">cfgPath</span><span class=\"p\">,</span> <span class=\"n\">modelPath</span><span class=\"p\">);</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">JNIEXPORT</span> <span class=\"n\">jint</span> <span class=\"n\">JNICALL</span> <span class=\"nf\">Java_com_xxx_ai_image_detection_service_sdk_DarknetJavaSDK_detect_1image</span><span class=\"p\">(</span><span class=\"n\">JNIEnv</span> <span class=\"o\">*</span><span class=\"n\">env</span><span class=\"p\">,</span> <span class=\"n\">jobject</span><span class=\"p\">,</span> <span class=\"n\">jstring</span> <span class=\"n\">imagePath</span><span class=\"p\">,</span> <span class=\"n\">jstring</span> <span class=\"n\">outDirPath</span><span class=\"p\">)</span> <span class=\"p\">{</span>\n    <span class=\"k\">return</span> <span class=\"n\">detect_image</span><span class=\"p\">(</span><span class=\"n\">env</span><span class=\"p\">,</span> <span class=\"n\">imagePath</span><span class=\"p\">,</span> <span class=\"n\">outDirPath</span><span class=\"p\">);</span>\n<span class=\"p\">}</span>\n</code></pre></div><h3>新建导出函数头文件dll_api.h</h3><div class=\"highlight\"><pre><code class=\"language-cpp\"><span class=\"cp\">#pragma once\n</span><span class=\"cp\">#ifndef DLL_API_H\n</span><span class=\"cp\">#define DLL_API_H\n</span><span class=\"cp\"></span>\n<span class=\"n\">jstring</span> <span class=\"nf\">get_version</span><span class=\"p\">(</span><span class=\"n\">JNIEnv</span> <span class=\"o\">*</span><span class=\"n\">env</span><span class=\"p\">);</span>\n\n<span class=\"n\">jboolean</span> <span class=\"nf\">set_logfile_path</span><span class=\"p\">(</span><span class=\"n\">JNIEnv</span> <span class=\"o\">*</span><span class=\"n\">env</span><span class=\"p\">,</span> <span class=\"n\">jstring</span> <span class=\"n\">logPath</span><span class=\"p\">);</span>\n\n<span class=\"n\">jboolean</span> <span class=\"nf\">load_model</span><span class=\"p\">(</span><span class=\"n\">JNIEnv</span> <span class=\"o\">*</span><span class=\"n\">env</span><span class=\"p\">,</span> <span class=\"n\">jstring</span> <span class=\"n\">cfgPath</span><span class=\"p\">,</span> <span class=\"n\">jstring</span> <span class=\"n\">modelPath</span><span class=\"p\">);</span>\n\n<span class=\"kt\">int</span> <span class=\"nf\">detect_image</span><span class=\"p\">(</span><span class=\"n\">JNIEnv</span> <span class=\"o\">*</span><span class=\"n\">env</span><span class=\"p\">,</span> <span class=\"n\">jstring</span> <span class=\"n\">imagePath</span><span class=\"p\">,</span> <span class=\"n\">jstring</span> <span class=\"n\">outDirPath</span><span class=\"p\">);</span>\n\n<span class=\"cp\">#endif\n</span></code></pre></div><h3>新建dll_api.cpp，定义函数实现</h3><p>这里就是按照dll_api.h里的函数定义，编写其实现，需要注意的是，需要添加#include <br/>然后，Java中的基本数据类型和C++中的有些是需要相互转化的，如：<br/>jboolean表示java中的布尔值true和false，在c++中对应的是JNI_FALSE和JNI_TRUE；<br/>jstring表示java中的String类，jstring和c++中的string类的相互转化可以用以下函数：</p><div class=\"highlight\"><pre><code class=\"language-cpp\"><span class=\"n\">jstring</span> <span class=\"nf\">str2jstring</span><span class=\"p\">(</span><span class=\"n\">JNIEnv</span><span class=\"o\">*</span> <span class=\"n\">env</span><span class=\"p\">,</span> <span class=\"k\">const</span> <span class=\"kt\">char</span><span class=\"o\">*</span> <span class=\"n\">pat</span><span class=\"p\">)</span>\n<span class=\"p\">{</span>\n    <span class=\"c1\">//定义java String类 strClass\n</span><span class=\"c1\"></span>    <span class=\"n\">jclass</span> <span class=\"n\">strClass</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">env</span><span class=\"p\">)</span><span class=\"o\">-&gt;</span><span class=\"n\">FindClass</span><span class=\"p\">(</span><span class=\"s\">&#34;Ljava/lang/String;&#34;</span><span class=\"p\">);</span>\n    <span class=\"c1\">//获取String(byte[],String)的构造器,用于将本地byte[]数组转换为一个新String\n</span><span class=\"c1\"></span>    <span class=\"n\">jmethodID</span> <span class=\"n\">ctorID</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">env</span><span class=\"p\">)</span><span class=\"o\">-&gt;</span><span class=\"n\">GetMethodID</span><span class=\"p\">(</span><span class=\"n\">strClass</span><span class=\"p\">,</span> <span class=\"s\">&#34;&lt;init&gt;&#34;</span><span class=\"p\">,</span> <span class=\"s\">&#34;([BLjava/lang/String;)V&#34;</span><span class=\"p\">);</span>\n    <span class=\"c1\">//建立byte数组\n</span><span class=\"c1\"></span>    <span class=\"n\">jbyteArray</span> <span class=\"n\">bytes</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">env</span><span class=\"p\">)</span><span class=\"o\">-&gt;</span><span class=\"n\">NewByteArray</span><span class=\"p\">(</span><span class=\"n\">strlen</span><span class=\"p\">(</span><span class=\"n\">pat</span><span class=\"p\">));</span>\n    <span class=\"c1\">//将char* 转换为byte数组\n</span><span class=\"c1\"></span>    <span class=\"p\">(</span><span class=\"n\">env</span><span class=\"p\">)</span><span class=\"o\">-&gt;</span><span class=\"n\">SetByteArrayRegion</span><span class=\"p\">(</span><span class=\"n\">bytes</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">,</span> <span class=\"n\">strlen</span><span class=\"p\">(</span><span class=\"n\">pat</span><span class=\"p\">),</span> <span class=\"p\">(</span><span class=\"n\">jbyte</span><span class=\"o\">*</span><span class=\"p\">)</span><span class=\"n\">pat</span><span class=\"p\">);</span>\n    <span class=\"c1\">// 设置String, 保存语言类型,用于byte数组转换至String时的参数\n</span><span class=\"c1\"></span>    <span class=\"n\">jstring</span> <span class=\"n\">encoding</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">env</span><span class=\"p\">)</span><span class=\"o\">-&gt;</span><span class=\"n\">NewStringUTF</span><span class=\"p\">(</span><span class=\"s\">&#34;GB2312&#34;</span><span class=\"p\">);</span>\n    <span class=\"c1\">//将byte数组转换为java String,并输出\n</span><span class=\"c1\"></span>    <span class=\"k\">return</span> <span class=\"p\">(</span><span class=\"n\">jstring</span><span class=\"p\">)(</span><span class=\"n\">env</span><span class=\"p\">)</span><span class=\"o\">-&gt;</span><span class=\"n\">NewObject</span><span class=\"p\">(</span><span class=\"n\">strClass</span><span class=\"p\">,</span> <span class=\"n\">ctorID</span><span class=\"p\">,</span> <span class=\"n\">bytes</span><span class=\"p\">,</span> <span class=\"n\">encoding</span><span class=\"p\">);</span>\n<span class=\"p\">}</span>\n\n<span class=\"n\">string</span> <span class=\"nf\">jstring2str</span><span class=\"p\">(</span><span class=\"n\">JNIEnv</span><span class=\"o\">*</span> <span class=\"n\">env</span><span class=\"p\">,</span> <span class=\"n\">jstring</span> <span class=\"n\">jstr</span><span class=\"p\">)</span>\n<span class=\"p\">{</span>\n    <span class=\"kt\">char</span><span class=\"o\">*</span>   <span class=\"n\">rtn</span> <span class=\"o\">=</span> <span class=\"nb\">NULL</span><span class=\"p\">;</span>\n    <span class=\"n\">jclass</span>   <span class=\"n\">clsstring</span> <span class=\"o\">=</span> <span class=\"n\">env</span><span class=\"o\">-&gt;</span><span class=\"n\">FindClass</span><span class=\"p\">(</span><span class=\"s\">&#34;java/lang/String&#34;</span><span class=\"p\">);</span>\n    <span class=\"n\">jstring</span>   <span class=\"n\">strencode</span> <span class=\"o\">=</span> <span class=\"n\">env</span><span class=\"o\">-&gt;</span><span class=\"n\">NewStringUTF</span><span class=\"p\">(</span><span class=\"s\">&#34;GB2312&#34;</span><span class=\"p\">);</span>\n    <span class=\"n\">jmethodID</span>   <span class=\"n\">mid</span> <span class=\"o\">=</span> <span class=\"n\">env</span><span class=\"o\">-&gt;</span><span class=\"n\">GetMethodID</span><span class=\"p\">(</span><span class=\"n\">clsstring</span><span class=\"p\">,</span> <span class=\"s\">&#34;getBytes&#34;</span><span class=\"p\">,</span> <span class=\"s\">&#34;(Ljava/lang/String;)[B&#34;</span><span class=\"p\">);</span>\n    <span class=\"n\">jbyteArray</span>   <span class=\"n\">barr</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"n\">jbyteArray</span><span class=\"p\">)</span><span class=\"n\">env</span><span class=\"o\">-&gt;</span><span class=\"n\">CallObjectMethod</span><span class=\"p\">(</span><span class=\"n\">jstr</span><span class=\"p\">,</span> <span class=\"n\">mid</span><span class=\"p\">,</span> <span class=\"n\">strencode</span><span class=\"p\">);</span>\n    <span class=\"n\">jsize</span>   <span class=\"n\">alen</span> <span class=\"o\">=</span> <span class=\"n\">env</span><span class=\"o\">-&gt;</span><span class=\"n\">GetArrayLength</span><span class=\"p\">(</span><span class=\"n\">barr</span><span class=\"p\">);</span>\n    <span class=\"n\">jbyte</span><span class=\"o\">*</span>   <span class=\"n\">ba</span> <span class=\"o\">=</span> <span class=\"n\">env</span><span class=\"o\">-&gt;</span><span class=\"n\">GetByteArrayElements</span><span class=\"p\">(</span><span class=\"n\">barr</span><span class=\"p\">,</span> <span class=\"n\">JNI_FALSE</span><span class=\"p\">);</span>\n    <span class=\"k\">if</span> <span class=\"p\">(</span><span class=\"n\">alen</span> <span class=\"o\">&gt;</span> <span class=\"mi\">0</span><span class=\"p\">)</span>\n    <span class=\"p\">{</span>\n        <span class=\"n\">rtn</span> <span class=\"o\">=</span> <span class=\"p\">(</span><span class=\"kt\">char</span><span class=\"o\">*</span><span class=\"p\">)</span><span class=\"n\">malloc</span><span class=\"p\">(</span><span class=\"n\">alen</span> <span class=\"o\">+</span> <span class=\"mi\">1</span><span class=\"p\">);</span>\n        <span class=\"n\">memcpy</span><span class=\"p\">(</span><span class=\"n\">rtn</span><span class=\"p\">,</span> <span class=\"n\">ba</span><span class=\"p\">,</span> <span class=\"n\">alen</span><span class=\"p\">);</span>\n        <span class=\"n\">rtn</span><span class=\"p\">[</span><span class=\"n\">alen</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"mi\">0</span><span class=\"p\">;</span>\n    <span class=\"p\">}</span>\n    <span class=\"n\">env</span><span class=\"o\">-&gt;</span><span class=\"n\">ReleaseByteArrayElements</span><span class=\"p\">(</span><span class=\"n\">barr</span><span class=\"p\">,</span> <span class=\"n\">ba</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">);</span>\n    <span class=\"n\">std</span><span class=\"o\">::</span><span class=\"n\">string</span> <span class=\"n\">stemp</span><span class=\"p\">(</span><span class=\"n\">rtn</span><span class=\"p\">);</span>\n    <span class=\"n\">free</span><span class=\"p\">(</span><span class=\"n\">rtn</span><span class=\"p\">);</span>\n    <span class=\"k\">return</span>   <span class=\"n\">stemp</span><span class=\"p\">;</span>\n<span class=\"p\">}</span>\n</code></pre></div><h2>5.生成dll，并在Java中加载</h2><p>第4.步骤完成后，生成的dll时可以直接被java加载利用的，只需要在DarknetJavaSDK.java中用<br/>_<b>System.<i>*load(YOUR_</i>*DLL_PATH);</b>_即可完成dll加载工作，顺序不正常会报错。。。<br/>以我的为例：<br/>DarknetDllForJava.dll是第4.步新建的dll项目生成的dll，其运行依赖上面三个dll，所以其顺序放在最后。<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-9b6510768da280c2b34d90e240d0fdf0_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"641\" data-rawheight=\"80\" class=\"origin_image zh-lightbox-thumb\" width=\"641\" data-original=\"https://pic1.zhimg.com/v2-9b6510768da280c2b34d90e240d0fdf0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;641&#39; height=&#39;80&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"641\" data-rawheight=\"80\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"641\" data-original=\"https://pic1.zhimg.com/v2-9b6510768da280c2b34d90e240d0fdf0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-9b6510768da280c2b34d90e240d0fdf0_b.jpg\"/></figure><p><br/>然后再添加@Service注解，让其可以作为一个service被Autowired,改造后的DarknetJavaSDK.java：</p><div class=\"highlight\"><pre><code class=\"language-java\"><span class=\"kn\">package</span> <span class=\"nn\">com.xxx.ai.image.detection.service.sdk</span><span class=\"o\">;</span>\n<span class=\"kn\">import</span> <span class=\"nn\">org.springframework.stereotype.Service</span><span class=\"o\">;</span>\n<span class=\"kn\">import</span> <span class=\"nn\">java.io.File</span><span class=\"o\">;</span>\n<span class=\"nd\">@Service</span>\n<span class=\"kd\">public</span> <span class=\"kd\">class</span> <span class=\"nc\">DarknetJavaSDK</span> <span class=\"o\">{</span>\n\n    <span class=\"kd\">private</span> <span class=\"kd\">static</span> <span class=\"kd\">final</span> <span class=\"n\">String</span> <span class=\"n\">OPENCV_WORLD340_DLL</span> <span class=\"o\">=</span> <span class=\"s\">&#34;opencv_world340.dll&#34;</span><span class=\"o\">;</span>\n    <span class=\"kd\">private</span> <span class=\"kd\">static</span> <span class=\"kd\">final</span> <span class=\"n\">String</span> <span class=\"n\">PTHREADVC2_DLL</span> <span class=\"o\">=</span> <span class=\"s\">&#34;pthreadVC2.dll&#34;</span><span class=\"o\">;</span>\n    <span class=\"kd\">private</span> <span class=\"kd\">static</span> <span class=\"kd\">final</span> <span class=\"n\">String</span> <span class=\"n\">YOLO_DLL_CPU_REALEASE_DLL</span> <span class=\"o\">=</span> <span class=\"s\">&#34;yolo_dll_cpu_r.dll&#34;</span><span class=\"o\">;</span>\n    <span class=\"kd\">private</span> <span class=\"kd\">static</span> <span class=\"kd\">final</span> <span class=\"n\">String</span> <span class=\"n\">DARKNETDLL_FOR_JAVA_DLL</span> <span class=\"o\">=</span> <span class=\"s\">&#34;DarknetDllForJava.dll&#34;</span><span class=\"o\">;</span>\n\n    <span class=\"kd\">static</span><span class=\"o\">{</span>\n        <span class=\"n\">StringBuilder</span> <span class=\"n\">sb</span> <span class=\"o\">=</span> <span class=\"k\">new</span> <span class=\"n\">StringBuilder</span><span class=\"o\">(</span><span class=\"n\">System</span><span class=\"o\">.</span><span class=\"na\">getProperty</span><span class=\"o\">(</span><span class=\"s\">&#34;user.dir&#34;</span><span class=\"o\">)).</span><span class=\"na\">append</span><span class=\"o\">(</span><span class=\"n\">File</span><span class=\"o\">.</span><span class=\"na\">separator</span><span class=\"o\">).</span><span class=\"na\">append</span><span class=\"o\">(</span><span class=\"s\">&#34;dll&#34;</span><span class=\"o\">).</span><span class=\"na\">append</span><span class=\"o\">(</span><span class=\"n\">File</span><span class=\"o\">.</span><span class=\"na\">separator</span><span class=\"o\">);</span>\n        <span class=\"kd\">final</span> <span class=\"n\">String</span> <span class=\"n\">dirPath</span> <span class=\"o\">=</span> <span class=\"n\">sb</span><span class=\"o\">.</span><span class=\"na\">toString</span><span class=\"o\">();</span>\n        <span class=\"n\">System</span><span class=\"o\">.</span><span class=\"na\">load</span><span class=\"o\">(</span><span class=\"n\">dirPath</span> <span class=\"o\">+</span> <span class=\"n\">OPENCV_WORLD340_DLL</span><span class=\"o\">);</span>\n        <span class=\"n\">System</span><span class=\"o\">.</span><span class=\"na\">load</span><span class=\"o\">(</span><span class=\"n\">dirPath</span> <span class=\"o\">+</span> <span class=\"n\">PTHREADVC2_DLL</span><span class=\"o\">);</span>\n        <span class=\"n\">System</span><span class=\"o\">.</span><span class=\"na\">load</span><span class=\"o\">(</span><span class=\"n\">dirPath</span> <span class=\"o\">+</span> <span class=\"n\">YOLO_DLL_CPU_REALEASE_DLL</span><span class=\"o\">);</span>\n        <span class=\"n\">System</span><span class=\"o\">.</span><span class=\"na\">load</span><span class=\"o\">(</span><span class=\"n\">dirPath</span> <span class=\"o\">+</span> <span class=\"n\">DARKNETDLL_FOR_JAVA_DLL</span><span class=\"o\">);</span>\n    <span class=\"o\">}</span>\n\n    <span class=\"kd\">public</span> <span class=\"kd\">native</span> <span class=\"n\">String</span> <span class=\"nf\">get_version</span><span class=\"o\">();</span>\n\n    <span class=\"kd\">public</span> <span class=\"kd\">native</span> <span class=\"kt\">boolean</span> <span class=\"nf\">set_logfile_path</span><span class=\"o\">(</span><span class=\"n\">String</span> <span class=\"n\">logPath</span><span class=\"o\">);</span>\n\n    <span class=\"kd\">public</span> <span class=\"kd\">native</span> <span class=\"kt\">boolean</span> <span class=\"nf\">load_model</span><span class=\"o\">(</span><span class=\"n\">String</span> <span class=\"n\">cfgPath</span><span class=\"o\">,</span> <span class=\"n\">String</span> <span class=\"n\">modelPath</span><span class=\"o\">);</span>\n\n    <span class=\"kd\">public</span> <span class=\"kd\">native</span> <span class=\"kt\">int</span> <span class=\"nf\">detect_image</span><span class=\"o\">(</span><span class=\"n\">String</span> <span class=\"n\">imagePath</span><span class=\"o\">,</span> <span class=\"n\">String</span> <span class=\"n\">outDirPath</span><span class=\"o\">);</span></code></pre></div><hr/><h2>效果演示：</h2><p>Java接口调用本地方法：detect_image（）<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-aae632b32b6e36dc8707a4987414d818_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"559\" data-rawheight=\"315\" class=\"origin_image zh-lightbox-thumb\" width=\"559\" data-original=\"https://pic1.zhimg.com/v2-aae632b32b6e36dc8707a4987414d818_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;559&#39; height=&#39;315&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"559\" data-rawheight=\"315\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"559\" data-original=\"https://pic1.zhimg.com/v2-aae632b32b6e36dc8707a4987414d818_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-aae632b32b6e36dc8707a4987414d818_b.jpg\"/></figure><p><br/>返回检测出的目标数量20、在指定dirPath下生成检测图片：<br/></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-5e534ceff0142249880f35b12289d3f3_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"559\" data-rawheight=\"315\" class=\"origin_image zh-lightbox-thumb\" width=\"559\" data-original=\"https://pic4.zhimg.com/v2-5e534ceff0142249880f35b12289d3f3_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;559&#39; height=&#39;315&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"559\" data-rawheight=\"315\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"559\" data-original=\"https://pic4.zhimg.com/v2-5e534ceff0142249880f35b12289d3f3_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-5e534ceff0142249880f35b12289d3f3_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><blockquote> 整个过程参考过如下文章： <br/><a href=\"https://link.zhihu.com/?target=https%3A//blog.csdn.net/qq_38288172/article/details/82387946\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">blog.csdn.net/qq_382881</span><span class=\"invisible\">72/article/details/82387946</span><span class=\"ellipsis\"></span></a> <br/><a href=\"https://link.zhihu.com/?target=https%3A//www.jb51.net/article/132930.htm\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">jb51.net/article/132930</span><span class=\"invisible\">.htm</span><span class=\"ellipsis\"></span></a> <br/><a href=\"https://link.zhihu.com/?target=https%3A//www.cnblogs.com/haitaofeiyang/p/7698121.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://www.</span><span class=\"visible\">cnblogs.com/haitaofeiya</span><span class=\"invisible\">ng/p/7698121.html</span><span class=\"ellipsis\"></span></a><br/> </blockquote>", 
            "topic": [
                {
                    "tag": "yolov3", 
                    "tagLink": "https://api.zhihu.com/topics/20756884"
                }, 
                {
                    "tag": "图像识别", 
                    "tagLink": "https://api.zhihu.com/topics/19588774"
                }, 
                {
                    "tag": "Java", 
                    "tagLink": "https://api.zhihu.com/topics/19561132"
                }
            ], 
            "comments": [
                {
                    "userName": "小小飞贼", 
                    "userLink": "https://www.zhihu.com/people/b9f7119a2d10d5ba9c42fc773da33b58", 
                    "content": "最近刚好有用到，谢谢分享", 
                    "likes": 0, 
                    "childComments": [
                        {
                            "userName": "Lyon", 
                            "userLink": "https://www.zhihu.com/people/38495a9b1f83e4a612543ce6c523df7f", 
                            "content": "<a href=\"https://pic4.zhimg.com/v2-fa3cb6bc9ec57da84ab53a60f48d0c6f.gif\" class=\"comment_sticker\" data-width=\"0\" data-height=\"0\" data-sticker-id=\"951517103955070976\">[棒]</a>", 
                            "likes": 0, 
                            "replyToAuthor": "小小飞贼"
                        }
                    ]
                }
            ]
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/58243984", 
            "userName": "Lyon", 
            "userLink": "https://www.zhihu.com/people/38495a9b1f83e4a612543ce6c523df7f", 
            "upvote": 3, 
            "title": "darknet入门—训练自己的图像分类器", 
            "content": "<p>知乎编辑器效果有限，原文发布在语雀文档上，看上去效果更好~</p><a data-draft-node=\"block\" data-draft-type=\"link-card\" href=\"https://link.zhihu.com/?target=https%3A//www.yuque.com/docs/share/f3a79c64-047b-4cbe-9ca2-24a5682d345e\" data-image=\"https://pic1.zhimg.com/v2-72bbb1e26eacf07192c32d1b6514dde0_ipico.jpg\" data-image-width=\"512\" data-image-height=\"512\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">yolov3入门—训练自己的图像分类器 · 语雀</a><hr/><h2><b>前言</b></h2><p>官网提供的demo非常友好：<a href=\"https://link.zhihu.com/?target=https%3A//pjreddie.com/darknet/imagenet/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">pjreddie.com/darknet/im</span><span class=\"invisible\">agenet/</span><span class=\"ellipsis\"></span></a> 。你既可以下载别人的模型和配置文件用于验证图像分类，也可以自己从头训练一个图像分类器。下文分别介绍这两种方式。(Windows10系统上)</p><blockquote>前提：需要安装和编译darknet<br/>Linux/Unix系统下比较容易，用make命令即可，<br/>windows系统稍微复杂点，参考：<a href=\"https://link.zhihu.com/?target=https%3A//www.yuque.com/docs/share/ab7506ed-cdfd-4269-9af0-49801d8b25fe\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">yolov3入门—目标检测(安装、编译、实现)</a></blockquote><hr/><h2><b>1.使用现成的模型来分类图像</b></h2><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-fe27d8e7118dba7ff288639ad6e2a45e_b.jpg\" data-rawwidth=\"794\" data-rawheight=\"906\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"794\" data-original=\"https://pic3.zhimg.com/v2-fe27d8e7118dba7ff288639ad6e2a45e_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;794&#39; height=&#39;906&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"794\" data-rawheight=\"906\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"794\" data-original=\"https://pic3.zhimg.com/v2-fe27d8e7118dba7ff288639ad6e2a45e_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-fe27d8e7118dba7ff288639ad6e2a45e_b.jpg\"/></figure><p><b>1.1使用darknet19模型</b></p><p><b>Unix/Linux</b></p><p>官网给的流程和说明很详细，只需按照步骤来做即可，Linux/Unix系统下只需要简单的2行代码：</p><div class=\"highlight\"><pre><code class=\"language-bash\">wget https://pjreddie.com/media/files/darknet19.weights \n./darknet classifier predict cfg/imagenet1k.data cfg/darknet19.cfg darknet19.weights data/dog.jpg</code></pre></div><p><b>Windows</b></p><p>windows下可以直接用迅雷下载<a href=\"https://link.zhihu.com/?target=https%3A//www.yuque.com/docs/share/f3a79c64-047b-4cbe-9ca2-24a5682d345e\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">权重文件</a>，下载完成，</p><p>在darknet-master\\build\\darknet\\x64\\目录下新建文件夹weights，将darknet19.weights放入其中，然后在x64目录下打开cmd运行：</p><div class=\"highlight\"><pre><code class=\"language-text\">darknet.exe classifier predict cfg/imagenet1k.data cfg/darknet19.cfg weights/darknet19.weights data/dog.jpg </code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-60f85716464c08f6d58b975fb0fa0145_b.jpg\" data-rawwidth=\"1128\" data-rawheight=\"788\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"1128\" data-original=\"https://pic2.zhimg.com/v2-60f85716464c08f6d58b975fb0fa0145_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1128&#39; height=&#39;788&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1128\" data-rawheight=\"788\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1128\" data-original=\"https://pic2.zhimg.com/v2-60f85716464c08f6d58b975fb0fa0145_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-60f85716464c08f6d58b975fb0fa0145_b.jpg\"/></figure><p>可见，这只狗狗是爱斯基摩的概率：0.284，是malamute（雪橇犬?）的概率：0.157。我们可以尝试不同的图片来检测其分类的准确性，只要将dog.jpg换成其他图片即可。</p><hr/><h2><b>1.2使用其他模型</b></h2><p>官网贴心地给出了其他模型和配置文件的下载链接，下载下来，直接使用即可</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-4a906f0253463a924e1a7c72e564ce39_b.jpg\" data-rawwidth=\"782\" data-rawheight=\"970\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"782\" data-original=\"https://pic2.zhimg.com/v2-4a906f0253463a924e1a7c72e564ce39_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;782&#39; height=&#39;970&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"782\" data-rawheight=\"970\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"782\" data-original=\"https://pic2.zhimg.com/v2-4a906f0253463a924e1a7c72e564ce39_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-4a906f0253463a924e1a7c72e564ce39_b.jpg\"/></figure><p>各种模型的大小、准确率、检测时间对比</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-0777ba68ba6ece4709281ff9ce22aee1_b.jpg\" data-rawwidth=\"740\" data-rawheight=\"658\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"740\" data-original=\"https://pic2.zhimg.com/v2-0777ba68ba6ece4709281ff9ce22aee1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;740&#39; height=&#39;658&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"740\" data-rawheight=\"658\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"740\" data-original=\"https://pic2.zhimg.com/v2-0777ba68ba6ece4709281ff9ce22aee1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-0777ba68ba6ece4709281ff9ce22aee1_b.jpg\"/></figure><hr/><h2><b>2.使用自己训练的模型</b></h2><p>训练自己的图片分类模型，首先要准备好数据，即要分类的图像数据、分类的类别名称，训练配置cfg文件，官网提供了一个现成的<a href=\"https://link.zhihu.com/?target=https%3A//www.cs.toronto.edu/~kriz/cifar.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">CIFAR-10</a>数据集，供我们直接使用，我们直接根据<a href=\"https://link.zhihu.com/?target=https%3A//pjreddie.com/darknet/train-cifar/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">官网的步骤</a>，一步步来即可。</p><h2><b>2.1处理数据集文件</b></h2><p><b>1.获取数据</b></p><p><a href=\"https://link.zhihu.com/?target=https%3A//pjreddie.com/media/files/cifar.tgz\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">pjreddie.com/media/file</span><span class=\"invisible\">s/cifar.tgz</span><span class=\"ellipsis\"></span></a></p><p><b>2.解压.tgz数据集文件</b></p><p>解压至darknet-master\\build\\darknet\\x64\\data文件夹下</p><p>解压完，会多出一个cifar文件夹，文件夹里包含test、train和labels.txt三个文件</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-df2328c2502a20c36a9303cc8ad2bc7b_b.jpg\" data-rawwidth=\"712\" data-rawheight=\"269\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"712\" data-original=\"https://pic4.zhimg.com/v2-df2328c2502a20c36a9303cc8ad2bc7b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;712&#39; height=&#39;269&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"712\" data-rawheight=\"269\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"712\" data-original=\"https://pic4.zhimg.com/v2-df2328c2502a20c36a9303cc8ad2bc7b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-df2328c2502a20c36a9303cc8ad2bc7b_b.jpg\"/></figure><p><b>3.生成路径文件</b></p><p>根据官网步骤，生成路径文件即生成train.list和test.list文件，这些文件中包含了训练/测试图片的路径，用于后面训练/测试时加载到指定图片。</p><p>Unix/Linux下可以用命令方便地生成这两个.list文件，windows就稍微麻烦点。</p><p><b>Unix/Linux：</b></p><div class=\"highlight\"><pre><code class=\"language-text\">cd cifar find `pwd`/train -name \\*.png &gt; train.list \nfind `pwd`/test -name \\*.png &gt; test.list \ncd ../..</code></pre></div><p><b>Windows：</b></p><div class=\"highlight\"><pre><code class=\"language-text\">cd cifar(cmd中cd到cifar目录) \ndir /b /s train *.png &gt; train.list\ndir /b /s test *.png &gt; test.list</code></pre></div><p>生成后的.list应该如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-d852c28d8512e1c06bcacb76eedb0d5a_b.jpg\" data-rawwidth=\"892\" data-rawheight=\"329\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"892\" data-original=\"https://pic3.zhimg.com/v2-d852c28d8512e1c06bcacb76eedb0d5a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;892&#39; height=&#39;329&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"892\" data-rawheight=\"329\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"892\" data-original=\"https://pic3.zhimg.com/v2-d852c28d8512e1c06bcacb76eedb0d5a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-d852c28d8512e1c06bcacb76eedb0d5a_b.jpg\"/></figure><p><b>4.添加.cfg配置文件</b></p><p>在cfg目录下，已经存在cifar.cfg,我们可以拿来直接使用，如果没有GPU，我们可以新建一个cifar_small.cfg也能用于训练。</p><div class=\"highlight\"><pre><code class=\"language-text\">[net]\nbatch=128\nsubdivisions=1\nheight=28\nwidth=28\nchannels=3\nmax_crop=32\nmin_crop=32\nhue=.1\nsaturation=.75\nexposure=.75\nlearning_rate=0.1\npolicy=poly\npower=4\nmax_batches = 5000\nmomentum=0.9\ndecay=0.0005\n[convolutional]\nbatch_normalize=1\nfilters=32\nsize=3\nstride=1\npad=1\nactivation=leaky\n[maxpool]\nsize=2\nstride=2\n[convolutional]\nbatch_normalize=1\nfilters=16\nsize=1\nstride=1\npad=1\nactivation=leaky\n[convolutional]\nbatch_normalize=1\nfilters=64\nsize=3\nstride=1\npad=1\nactivation=leaky\n[maxpool]\nsize=2\nstride=2\n[convolutional]\nbatch_normalize=1\nfilters=32\nsize=1\nstride=1\npad=1\nactivation=leaky\n[convolutional]\nbatch_normalize=1\nfilters=128\nsize=3\nstride=1\npad=1\nactivation=leaky\n[convolutional]\nbatch_normalize=1\nfilters=64\nsize=1\nstride=1\npad=1\nactivation=leaky\n[convolutional]\nfilters=10\nsize=1\nstride=1\npad=1\nactivation=leaky\n[avgpool]\n[softmax]</code></pre></div><h2><b>2.2训练模型</b></h2><p><b>Unix/Linux</b></p><div class=\"highlight\"><pre><code class=\"language-text\">./darknet classifier train cfg/cifar.data cfg/cifar.cfg \n（或：./darknet classifier train cfg/cifar.data cfg/cifar_small.cfg）</code></pre></div><p><b>Windows</b></p><div class=\"highlight\"><pre><code class=\"language-text\">darknet.exe classifier train cfg/cifar.data cfg/cifar.cfg  \n（或：darknet.exe classifier train cfg/cifar.data cfg/cifar_small.cfg）</code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-d9a090e757fcca81b8993967bd3eee99_b.jpg\" data-rawwidth=\"1220\" data-rawheight=\"740\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"1220\" data-original=\"https://pic2.zhimg.com/v2-d9a090e757fcca81b8993967bd3eee99_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1220&#39; height=&#39;740&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1220\" data-rawheight=\"740\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1220\" data-original=\"https://pic2.zhimg.com/v2-d9a090e757fcca81b8993967bd3eee99_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-d9a090e757fcca81b8993967bd3eee99_b.jpg\"/></figure><h2><b>2.3验证模型</b></h2><p>模型训练时会在buckup文件夹下陆续生成.weight文件：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-1dfad77aeeda0d07ebf06a34ff481c11_b.jpg\" data-rawwidth=\"203\" data-rawheight=\"484\" data-size=\"normal\" data-caption=\"\" class=\"content_image\" width=\"203\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;203&#39; height=&#39;484&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"203\" data-rawheight=\"484\" data-size=\"normal\" data-caption=\"\" class=\"content_image lazy\" width=\"203\" data-actualsrc=\"https://pic2.zhimg.com/v2-1dfad77aeeda0d07ebf06a34ff481c11_b.jpg\"/></figure><p>我们可以任意选择一个.weights验证其准确性，这里我们用cifar_50000.weights做测试。</p><p><b>Unix/Linux</b></p><div class=\"highlight\"><pre><code class=\"language-text\">./darknet classifier valid cfg/cifar.data cfg/cifar.cfg backup/cifar_50000.weights</code></pre></div><p><b>Windows</b></p><div class=\"highlight\"><pre><code class=\"language-text\">darknet.exe classifier valid cfg/cifar.data cfg/cifar.cfg backup/cifar_50000.weights</code></pre></div><p><b>Top-1、Top-2准确率</b></p><p>Top-1准确率是什么意思？我们指知道此图像分类模型是多分类，即一张动物的图，可能是90%的概率是雪橇犬，50%的概率是爱斯基摩狗，10%的概率是猫.....Top1准确率，即预测概率最大的预测label的准确率。同理Top-2准确率，即label出现在预测概率排行前2位时的准确率.</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-37413304a42094fe0900910fcffe784c_b.jpg\" data-rawwidth=\"326\" data-rawheight=\"796\" data-size=\"normal\" data-caption=\"\" class=\"content_image\" width=\"326\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;326&#39; height=&#39;796&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"326\" data-rawheight=\"796\" data-size=\"normal\" data-caption=\"\" class=\"content_image lazy\" width=\"326\" data-actualsrc=\"https://pic1.zhimg.com/v2-37413304a42094fe0900910fcffe784c_b.jpg\"/></figure><p>可见，我们使用cifar_50000.weights权重时，Top-1准确率大约在90.4%，换个cifar_10000.weights试试呢？</p><div class=\"highlight\"><pre><code class=\"language-text\">darknet.exe classifier valid cfg/cifar.data cfg/cifar.cfg backup/cifar_10000.weights</code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-5b94ef5c27abae52d7bc4ce17141819a_b.jpg\" data-rawwidth=\"324\" data-rawheight=\"634\" data-size=\"normal\" data-caption=\"\" class=\"content_image\" width=\"324\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;324&#39; height=&#39;634&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"324\" data-rawheight=\"634\" data-size=\"normal\" data-caption=\"\" class=\"content_image lazy\" width=\"324\" data-actualsrc=\"https://pic3.zhimg.com/v2-5b94ef5c27abae52d7bc4ce17141819a_b.jpg\"/></figure><p>Top-1准确率在71.1%附近，明显不如cifar_50000.weights</p><h2><b>2.4单张图片分类预测</b></h2><div class=\"highlight\"><pre><code class=\"language-text\">darknet.exe classifier predict cfg/cifar.data cfg/cifar.cfg backup/cifar_50000.weights</code></pre></div><p>输入后CMD会弹出Enter Image Path，此时输入图片路径即可</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-f1ef1864bfb94309bf36e4f85cfc5e61_b.jpg\" data-rawwidth=\"1014\" data-rawheight=\"638\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"1014\" data-original=\"https://pic2.zhimg.com/v2-f1ef1864bfb94309bf36e4f85cfc5e61_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1014&#39; height=&#39;638&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1014\" data-rawheight=\"638\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1014\" data-original=\"https://pic2.zhimg.com/v2-f1ef1864bfb94309bf36e4f85cfc5e61_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-f1ef1864bfb94309bf36e4f85cfc5e61_b.jpg\"/></figure><p>上图用0_cat.png来检测了模型，模型预测的分类结果：</p><p>猫：0.957748</p><p>青蛙：0.0418</p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "图像识别", 
                    "tagLink": "https://api.zhihu.com/topics/19588774"
                }, 
                {
                    "tag": "深度学习（Deep Learning）", 
                    "tagLink": "https://api.zhihu.com/topics/19813032"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/58028543", 
            "userName": "Lyon", 
            "userLink": "https://www.zhihu.com/people/38495a9b1f83e4a612543ce6c523df7f", 
            "upvote": 8, 
            "title": "OpenCV—Hello world代码示例", 
            "content": "<h2><b>简介</b></h2><ul><li><b>OpenCV</b></li></ul><p>OpenCV是一个基于BSD许可（开源）发行的跨平台计算机视觉库，可以运行在Linux、Windows、Android和Mac OS操作系统上。它轻量级而且高效——由一系列 C 函数和少量 C++ 类<a href=\"https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%25E6%259E%2584%25E6%2588%2590/103686\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">构成</a>，同时提供了Python、Ruby、MATLAB等语言的接口，实现了<a href=\"https://link.zhihu.com/?target=https%3A//baike.baidu.com/item/%25E5%259B%25BE%25E5%2583%258F%25E5%25A4%2584%25E7%2590%2586/294902\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">图像处理</a>和计算机视觉方面的很多通用算法。</p><ul><li><b>示例代码说明</b></li></ul><p>官网的页面虽然很土，但是示例代码还是很良心的：<a href=\"https://link.zhihu.com/?target=https%3A//docs.opencv.org/master/d9/df8/tutorial_root.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">OpenCV Tutorials</a></p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-39d5d636db325377c020b89bcd4377fd_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1252\" data-rawheight=\"682\" class=\"origin_image zh-lightbox-thumb\" width=\"1252\" data-original=\"https://pic2.zhimg.com/v2-39d5d636db325377c020b89bcd4377fd_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1252&#39; height=&#39;682&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1252\" data-rawheight=\"682\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1252\" data-original=\"https://pic2.zhimg.com/v2-39d5d636db325377c020b89bcd4377fd_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-39d5d636db325377c020b89bcd4377fd_b.jpg\"/></figure><p>本文运行的代码全部取自于opencv官网，提供了代码和傻瓜式运行教程~，从下载VS2017，下载OpenCV到配置，运行。即使不懂c++，照样可以玩转OpenCV的hello world。</p><blockquote>知乎编辑器效果有限，原文包括下载资源链接，请移步语雀：</blockquote><a href=\"https://link.zhihu.com/?target=https%3A//www.yuque.com/docs/share/e211404d-9a3d-41cf-8dfb-c1c2c11bfef5\" data-draft-node=\"block\" data-draft-type=\"link-card\" data-image=\"https://pic1.zhimg.com/v2-72bbb1e26eacf07192c32d1b6514dde0_ipico.jpg\" data-image-width=\"512\" data-image-height=\"512\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">OpenCV—Hello world代码示例 · 语雀</a><p><b>主要的demo方法和简介如下：</b></p><div class=\"highlight\"><pre><code class=\"language-cpp\"><span class=\"c1\">//加载一张图片\n</span><span class=\"c1\"></span><span class=\"kt\">int</span> <span class=\"nf\">load_one_image</span><span class=\"p\">(</span><span class=\"n\">string</span> <span class=\"n\">path</span><span class=\"p\">);</span>\n<span class=\"c1\">//加载一张图、并转化为灰图保持至outpath\n</span><span class=\"c1\"></span><span class=\"kt\">int</span> <span class=\"nf\">modify_and_save_one_image</span><span class=\"p\">(</span><span class=\"n\">string</span> <span class=\"n\">inpath</span><span class=\"p\">,</span> <span class=\"n\">string</span> <span class=\"n\">outpath</span><span class=\"p\">);</span>\n<span class=\"c1\">//介绍Mat - 基本图像容器\n</span><span class=\"c1\"></span><span class=\"kt\">int</span> <span class=\"nf\">mat_the_basic_image_container</span><span class=\"p\">();</span>\n<span class=\"c1\">//改变图像对比度和亮度\n</span><span class=\"c1\"></span><span class=\"kt\">int</span> <span class=\"nf\">change_image_contrast_and_brightness</span><span class=\"p\">(</span><span class=\"n\">string</span> <span class=\"n\">path</span><span class=\"p\">);</span>\n<span class=\"c1\">//利用OpenCV绘制基础图像(线段、圆、椭圆..)\n</span><span class=\"c1\"></span><span class=\"kt\">int</span> <span class=\"nf\">basic_drawing</span><span class=\"p\">();</span>\n<span class=\"c1\">//线性滤波平滑处理图像\n</span><span class=\"c1\"></span><span class=\"kt\">int</span> <span class=\"nf\">smoothing_image</span><span class=\"p\">(</span><span class=\"n\">string</span> <span class=\"n\">path</span><span class=\"p\">);</span>\n<span class=\"c1\">//侵蚀和膨胀处理图像\n</span><span class=\"c1\"></span><span class=\"kt\">int</span> <span class=\"nf\">erode_and_dilate_image</span><span class=\"p\">(</span><span class=\"n\">string</span> <span class=\"n\">path</span><span class=\"p\">);</span>\n<span class=\"c1\">//更多形态学处理\n</span><span class=\"c1\"></span><span class=\"kt\">int</span> <span class=\"nf\">more_image_processing</span><span class=\"p\">(</span><span class=\"n\">string</span> <span class=\"n\">path</span><span class=\"p\">);</span>\n<span class=\"c1\">//利用形态学处理提取图片中水平和垂直的线条\n</span><span class=\"c1\"></span><span class=\"kt\">int</span> <span class=\"nf\">extract_horizontal_and_vertical_lines_form_image</span><span class=\"p\">(</span><span class=\"n\">string</span> <span class=\"n\">path</span><span class=\"p\">);</span>\n<span class=\"c1\">//图像金字塔(采样、缩放)\n</span><span class=\"c1\"></span><span class=\"kt\">int</span> <span class=\"nf\">image_pyramids</span><span class=\"p\">(</span><span class=\"n\">string</span> <span class=\"n\">path</span><span class=\"p\">);</span>\n<span class=\"c1\">//基本阈值处理\n</span><span class=\"c1\"></span><span class=\"kt\">int</span> <span class=\"nf\">basic_thresholding_operations</span><span class=\"p\">(</span><span class=\"n\">string</span> <span class=\"n\">path</span><span class=\"p\">);</span>\n<span class=\"c1\">//图像线性滤波器\n</span><span class=\"c1\"></span><span class=\"kt\">int</span> <span class=\"nf\">image_linear_filters</span><span class=\"p\">(</span><span class=\"n\">string</span> <span class=\"n\">path</span><span class=\"p\">);</span>\n<span class=\"c1\">//给图像加上边框\n</span><span class=\"c1\"></span><span class=\"kt\">int</span> <span class=\"nf\">add_border_to_image</span><span class=\"p\">(</span><span class=\"n\">string</span> <span class=\"n\">path</span><span class=\"p\">);</span>\n<span class=\"c1\">//霍夫圆变换(利用其检测图像中的圆)\n</span><span class=\"c1\"></span><span class=\"kt\">int</span> <span class=\"nf\">hough_circle_transform</span><span class=\"p\">(</span><span class=\"n\">string</span> <span class=\"n\">path</span><span class=\"p\">);</span>\n<span class=\"c1\">//图像重映射(改变方向)\n</span><span class=\"c1\"></span><span class=\"kt\">int</span> <span class=\"nf\">image_remapping</span><span class=\"p\">(</span><span class=\"n\">string</span> <span class=\"n\">path</span><span class=\"p\">);</span>\n<span class=\"c1\">//给图像中的轮廓画出椭圆和边框\n</span><span class=\"c1\"></span><span class=\"kt\">int</span> <span class=\"nf\">create_boxes_and_ellopses_for_contours</span><span class=\"p\">(</span><span class=\"n\">string</span> <span class=\"n\">path</span><span class=\"p\">);</span>\n<span class=\"c1\">//失去焦点(模糊)图像恢复\n</span><span class=\"c1\"></span><span class=\"kt\">int</span> <span class=\"nf\">out_of_focus_image_recovery</span><span class=\"p\">(</span><span class=\"kt\">int</span> <span class=\"n\">argc</span><span class=\"p\">,</span> <span class=\"n\">string</span> <span class=\"n\">imagepath</span><span class=\"p\">,</span> <span class=\"kt\">char</span> <span class=\"o\">*</span><span class=\"n\">argv</span><span class=\"p\">[]);</span>\n</code></pre></div><hr/><h2><b>运行环境</b></h2><p>首先，只需要win10的环境，配上VS2017，再下载OpenCV3.x的压缩包即可。</p><blockquote><a href=\"https://link.zhihu.com/?target=https%3A//visualstudio.microsoft.com/zh-hans/downloads/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">VS2017</a><br/><a href=\"https://link.zhihu.com/?target=https%3A//opencv.org/releases.html\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">OpenCV3.X</a><br/>示例图片下载 &gt;&gt;图片.rar(0.27 MB)<br/>代码下载：    &gt;&gt;代码.rar(18.66 kB)</blockquote><h2><b>效果展示</b></h2><p><code>1.extract_horizontal_and_vertical_lines_form_image()；</code></p><a class=\"video-box\" href=\"https://link.zhihu.com/?target=https%3A//www.zhihu.com/video/1084827728997556224\" target=\"_blank\" data-video-id=\"\" data-video-playable=\"true\" data-name=\"OpenCV官网demo展示\" data-poster=\"https://pic4.zhimg.com/v2-c1d2a664abbed31c45103554271206ab.jpg\" data-lens-id=\"1084827728997556224\"><img class=\"thumbnail\" src=\"https://pic4.zhimg.com/v2-c1d2a664abbed31c45103554271206ab.jpg\"/><span class=\"content\"><span class=\"title\">OpenCV官网demo展示<span class=\"z-ico-extern-gray\"></span><span class=\"z-ico-extern-blue\"></span></span><span class=\"url\"><span class=\"z-ico-video\"></span>https://www.zhihu.com/video/1084827728997556224</span></span></a><p><code>2.smoothing_image()</code> </p><a class=\"video-box\" href=\"https://link.zhihu.com/?target=https%3A//www.zhihu.com/video/1084827679739625472\" target=\"_blank\" data-video-id=\"\" data-video-playable=\"true\" data-name=\"OpenCV官网demo展示\" data-poster=\"https://pic3.zhimg.com/v2-db6afd1d0cd2ba9fabe2391b8faeef32.jpg\" data-lens-id=\"1084827679739625472\"><img class=\"thumbnail\" src=\"https://pic3.zhimg.com/v2-db6afd1d0cd2ba9fabe2391b8faeef32.jpg\"/><span class=\"content\"><span class=\"title\">OpenCV官网demo展示<span class=\"z-ico-extern-gray\"></span><span class=\"z-ico-extern-blue\"></span></span><span class=\"url\"><span class=\"z-ico-video\"></span>https://www.zhihu.com/video/1084827679739625472</span></span></a><h2><b>运行前准备</b></h2><p><b>新建项目</b></p><ul><li><b>打开VS，文件-&gt;新建项目</b></li></ul><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-94b81983e6c6bd999a5997af6223c151_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"856\" data-rawheight=\"535\" class=\"origin_image zh-lightbox-thumb\" width=\"856\" data-original=\"https://pic2.zhimg.com/v2-94b81983e6c6bd999a5997af6223c151_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;856&#39; height=&#39;535&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"856\" data-rawheight=\"535\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"856\" data-original=\"https://pic2.zhimg.com/v2-94b81983e6c6bd999a5997af6223c151_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-94b81983e6c6bd999a5997af6223c151_b.jpg\"/></figure><p>选第一个即可。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-bb18ccc98e5a89daf40c7f07b74b0f62_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1346\" data-rawheight=\"815\" class=\"origin_image zh-lightbox-thumb\" width=\"1346\" data-original=\"https://pic3.zhimg.com/v2-bb18ccc98e5a89daf40c7f07b74b0f62_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1346&#39; height=&#39;815&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1346\" data-rawheight=\"815\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1346\" data-original=\"https://pic3.zhimg.com/v2-bb18ccc98e5a89daf40c7f07b74b0f62_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-bb18ccc98e5a89daf40c7f07b74b0f62_b.jpg\"/></figure><p>我的项目名称为opencv_demo，即项目根目录。</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-04cf5412e233699a768701f2b574a6a7_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"748\" data-rawheight=\"444\" class=\"origin_image zh-lightbox-thumb\" width=\"748\" data-original=\"https://pic4.zhimg.com/v2-04cf5412e233699a768701f2b574a6a7_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;748&#39; height=&#39;444&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"748\" data-rawheight=\"444\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"748\" data-original=\"https://pic4.zhimg.com/v2-04cf5412e233699a768701f2b574a6a7_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-04cf5412e233699a768701f2b574a6a7_b.jpg\"/></figure><p>将解压后的opencv包，放到任意路径，我一般习惯放在项目同级目录下。进入项目文件夹，双击项目名称.sln即可再VS里启动项目，如图所示，可以准备一些图片放在这里，待会测试opencv会用到。</p><blockquote>示例图片下载 &gt;&gt;图片.rar(0.27 MB)</blockquote><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-1d48d188e15493dc341ead28a1950cdc_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"769\" data-rawheight=\"419\" class=\"origin_image zh-lightbox-thumb\" width=\"769\" data-original=\"https://pic1.zhimg.com/v2-1d48d188e15493dc341ead28a1950cdc_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;769&#39; height=&#39;419&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"769\" data-rawheight=\"419\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"769\" data-original=\"https://pic1.zhimg.com/v2-1d48d188e15493dc341ead28a1950cdc_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-1d48d188e15493dc341ead28a1950cdc_b.jpg\"/></figure><p>发现，还有个同名的opencv_demo文件夹，再点击进去，将代码压缩包里的代码一起解压出来</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-e7784f938200494f87a2c4e9b2508a22_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"825\" data-rawheight=\"573\" class=\"origin_image zh-lightbox-thumb\" width=\"825\" data-original=\"https://pic3.zhimg.com/v2-e7784f938200494f87a2c4e9b2508a22_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;825&#39; height=&#39;573&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"825\" data-rawheight=\"573\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"825\" data-original=\"https://pic3.zhimg.com/v2-e7784f938200494f87a2c4e9b2508a22_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-e7784f938200494f87a2c4e9b2508a22_b.jpg\"/></figure><p>然后选中这些文件，将.h开头的拖至VS右侧【头文件】下，其余的拖至【源文件】下</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-298cfe830b08c9e178a6fd0e13f6f918_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1337\" data-rawheight=\"823\" class=\"origin_image zh-lightbox-thumb\" width=\"1337\" data-original=\"https://pic1.zhimg.com/v2-298cfe830b08c9e178a6fd0e13f6f918_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1337&#39; height=&#39;823&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1337\" data-rawheight=\"823\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1337\" data-original=\"https://pic1.zhimg.com/v2-298cfe830b08c9e178a6fd0e13f6f918_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-298cfe830b08c9e178a6fd0e13f6f918_b.jpg\"/></figure><h2><b>代码结构</b></h2><p>代码压缩包里的代码分为两类：<b>1.头文件 2.源文件</b></p><p>1.头文件即.h结尾的，此处即demohub.h，在这里定义了示例代码的方法名称、变量，类似Java种的接口</p><p>2.源文件，即核心的demo示例片段代码，一个.cpp对应一个Opencv的demo</p><p>源文件种opencv_demo.cpp是核心，在其中的main()方法种，调用demohub.h中定义的各种demo方法。</p><blockquote>代码下载：代码.rar(18.66 kB)</blockquote><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-242e42f6a025c67ab7ebfc80a3b1f5cf_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"661\" data-rawheight=\"784\" class=\"origin_image zh-lightbox-thumb\" width=\"661\" data-original=\"https://pic4.zhimg.com/v2-242e42f6a025c67ab7ebfc80a3b1f5cf_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;661&#39; height=&#39;784&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"661\" data-rawheight=\"784\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"661\" data-original=\"https://pic4.zhimg.com/v2-242e42f6a025c67ab7ebfc80a3b1f5cf_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-242e42f6a025c67ab7ebfc80a3b1f5cf_b.jpg\"/></figure><h2><b>VS配置</b></h2><ul><li><b>VC++目录配置</b></li></ul><p>选择右侧，项目文件夹鼠标右键-&gt;属性，配置VC++目录中的【包含目录】和【库目录】</p><p>包含目录，要将opencv-&gt;build-&gt;include目录添加上去；库目录，即lib所在目录，这里需要opencv-&gt;build-&gt;x64-&gt;vc14-&gt;lib。</p><ul><li><b>包含目录：</b></li></ul><p><code>D:\\personalProject\\AI\\imageProcessing\\opencv\\build\\include</code></p><ul><li><b>库目录：</b></li></ul><p><code>D:\\personalProject\\AI\\imageProcessing\\opencv\\build\\x64\\vc14\\lib</code> </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-7a6d4094cd3840f0acb6c39007c4303d_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1196\" data-rawheight=\"723\" class=\"origin_image zh-lightbox-thumb\" width=\"1196\" data-original=\"https://pic2.zhimg.com/v2-7a6d4094cd3840f0acb6c39007c4303d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1196&#39; height=&#39;723&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"1196\" data-rawheight=\"723\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1196\" data-original=\"https://pic2.zhimg.com/v2-7a6d4094cd3840f0acb6c39007c4303d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-7a6d4094cd3840f0acb6c39007c4303d_b.jpg\"/></figure><ul><li><b>链接器配置</b></li></ul><p>附加依赖项，新增：<code>opencv_world340.lib</code> </p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-49fb005e87842c02570324e26c9221e1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"980\" data-rawheight=\"637\" class=\"origin_image zh-lightbox-thumb\" width=\"980\" data-original=\"https://pic2.zhimg.com/v2-49fb005e87842c02570324e26c9221e1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;980&#39; height=&#39;637&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"980\" data-rawheight=\"637\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"980\" data-original=\"https://pic2.zhimg.com/v2-49fb005e87842c02570324e26c9221e1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-49fb005e87842c02570324e26c9221e1_b.jpg\"/></figure><h2><b>运行demo</b></h2><p>在opencv_demo.cpp的main函数中，我们可以调用任意之前在demohub.h中声名的方法，每个方法对应一个.cpp示例代码</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-7243458a92500a15c341261ed3c51fc1_b.jpg\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"924\" data-rawheight=\"685\" class=\"origin_image zh-lightbox-thumb\" width=\"924\" data-original=\"https://pic2.zhimg.com/v2-7243458a92500a15c341261ed3c51fc1_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;924&#39; height=&#39;685&#39;&gt;&lt;/svg&gt;\" data-caption=\"\" data-size=\"normal\" data-rawwidth=\"924\" data-rawheight=\"685\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"924\" data-original=\"https://pic2.zhimg.com/v2-7243458a92500a15c341261ed3c51fc1_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-7243458a92500a15c341261ed3c51fc1_b.jpg\"/></figure><p>如上图所示：在main里运行smoothing_image(),这个demo方法的效果是：线性滤波平滑处理图像</p>", 
            "topic": [
                {
                    "tag": "OpenCV", 
                    "tagLink": "https://api.zhihu.com/topics/19587715"
                }, 
                {
                    "tag": "图像识别", 
                    "tagLink": "https://api.zhihu.com/topics/19588774"
                }, 
                {
                    "tag": "AI技术", 
                    "tagLink": "https://api.zhihu.com/topics/20106982"
                }
            ], 
            "comments": []
        }, 
        {
            "url": "https://zhuanlan.zhihu.com/p/57953764", 
            "userName": "Lyon", 
            "userLink": "https://www.zhihu.com/people/38495a9b1f83e4a612543ce6c523df7f", 
            "upvote": 14, 
            "title": "darknet入门—yolov3目标检测(安装、编译、实现)", 
            "content": "<p>知乎编辑器效果有限，原文发布在语雀文档上，看上去效果更好~</p><a data-draft-node=\"block\" data-draft-type=\"link-card\" href=\"https://link.zhihu.com/?target=https%3A//www.yuque.com/docs/share/ab7506ed-cdfd-4269-9af0-49801d8b25fe\" data-image=\"https://pic1.zhimg.com/v2-72bbb1e26eacf07192c32d1b6514dde0_ipico.jpg\" data-image-width=\"512\" data-image-height=\"512\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">yolo-v3入门—目标检测(安装、编译、实现) · 语雀</a><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>效果图</b></h2><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-8973e10e17cc436313fa2723a93d2dea_b.jpg\" data-rawwidth=\"935\" data-rawheight=\"647\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"935\" data-original=\"https://pic3.zhimg.com/v2-8973e10e17cc436313fa2723a93d2dea_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;935&#39; height=&#39;647&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"935\" data-rawheight=\"647\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"935\" data-original=\"https://pic3.zhimg.com/v2-8973e10e17cc436313fa2723a93d2dea_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-8973e10e17cc436313fa2723a93d2dea_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p class=\"ztext-empty-paragraph\"><br/></p><h2>​<b>简介</b></h2><p>Yolo，是实时物体检测的算法系统，基于Darknet—一个用C和CUDA编写的开源神经网络框架。它快速，易于安装，并支持CPU和GPU计算，也是yolo的底层。本文主要介绍在win10系统上配置darknet环境，编译，使用yolo实现开头展示的目标检测效果。</p><p><b>主要包括以下几个步骤：</b></p><p><b>编译流程 -&gt; 环境安装 -&gt; VS2017配置和编译 -&gt; 运行展示</b></p><blockquote><i>YOLO</i>，美语新词，是You Only Live Once的首字母缩略词，意为你只能活一次，应该活在当下，大胆去做。</blockquote><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-e481476ad8215265ffe2ac948bd4f988_b.jpg\" data-rawwidth=\"789\" data-rawheight=\"435\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"789\" data-original=\"https://pic1.zhimg.com/v2-e481476ad8215265ffe2ac948bd4f988_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;789&#39; height=&#39;435&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"789\" data-rawheight=\"435\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"789\" data-original=\"https://pic1.zhimg.com/v2-e481476ad8215265ffe2ac948bd4f988_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-e481476ad8215265ffe2ac948bd4f988_b.jpg\"/></figure><p>本文的YOLO，意为：<b>You Only Look Once</b>，再点进官网，一股暗黑色系地风格让人觉得很magic，编程真有意思~</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-cf9bf6c3d9c2bb1345c38baf48dc39b6_b.jpg\" data-rawwidth=\"781\" data-rawheight=\"601\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"781\" data-original=\"https://pic3.zhimg.com/v2-cf9bf6c3d9c2bb1345c38baf48dc39b6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;781&#39; height=&#39;601&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"781\" data-rawheight=\"601\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"781\" data-original=\"https://pic3.zhimg.com/v2-cf9bf6c3d9c2bb1345c38baf48dc39b6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-cf9bf6c3d9c2bb1345c38baf48dc39b6_b.jpg\"/></figure><p><b>yolo官网：</b><a href=\"https://link.zhihu.com/?target=https%3A//pjreddie.com/darknet/yolo/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">pjreddie.com/darknet/yo</span><span class=\"invisible\">lo/</span><span class=\"ellipsis\"></span></a></p><p><b>darknet官网：</b><a href=\"https://link.zhihu.com/?target=https%3A//pjreddie.com/darknet/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">pjreddie.com/darknet/</span><span class=\"invisible\"></span></a></p><p><b>github：</b></p><p>Linux：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/pjreddie/darknet\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/pjreddie/dar</span><span class=\"invisible\">knet</span><span class=\"ellipsis\"></span></a>      windows：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/AlexeyAB/darknet\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">https://github.com/AlexeyAB/darkne</a>t</p><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>编译流程</b></h2><p>要使用yolo，必须先安装darknet、darknet可以直接在githb上下载压缩包：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/AlexeyAB/darknet\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/AlexeyAB/dar</span><span class=\"invisible\">knet</span><span class=\"ellipsis\"></span></a></p><p>本文主要介绍<b>darknet在win10</b>上的编译流程。github上介绍了Linux和Windows下darknet的编译，我这里直接用谷歌浏览器翻译了：</p><p><b>1.如何在Linux上编译</b></p><p>只是make在darknet目录中。在make之前，您可以在Makefile：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/AlexeyAB/darknet/blob/9c1b9a2cf6363546c152251be578a21f3c3caec6/Makefile%23L1\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">link中</a>设置这些选项</p><ul><li>GPU=1用CUDA构建以使用GPU加速（CUDA应该在/usr/local/cuda）</li><li>CUDNN=1使用cuDNN v5-v7构建以使用GPU加速训练（cuDNN应该在/usr/local/cudnn）</li><li>CUDNN_HALF=1 构建Tensor核心（在Titan V / Tesla V100 / DGX-2及更高版本上）加速检测3x，训练2x</li><li>OPENCV=1 使用OpenCV 3.x / 2.4.x构建 - 允许检测来自网络摄像机或网络摄像头的视频文件和视频流</li><li>DEBUG=1 可以调试Yolo的版本</li><li>OPENMP=1 使用OpenMP支持构建，通过使用多核CPU来加速Yolo</li><li>LIBSO=1构建使用此库的库darknet.so和二进制可运行文件uselib。或者您可以尝试运行LD_LIBRARY_PATH=./:$LD_LIBRARY_PATH ./uselib test.mp4如何从您自己的代码中使用此SO库 - 您可以查看C ++示例：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">https</a>：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">//github.com/AlexeyAB/darknet/blob/master/src/yolo_console_dll.cpp</a> 或者使用这样的方式：LD_LIBRARY_PATH=./:$LD_LIBRARY_PATH ./uselib data/coco.names cfg/yolov3.cfg yolov3.weights test.mp4</li></ul><p>要在Linux上运行Darknet，请使用本文中的示例，./darknet而不是darknet.exe使用，即使用此命令：./darknet detector test ./cfg/coco.data ./cfg/yolov3.cfg ./yolov3.weights</p><p><b>2.如何在Windows上编译（使用vcpkg）</b></p><ol><li>将Visual Studio安装或更新到至少2017版，确保将其完全修补（如果不确定自动更新到最新版本，请再次运行安装程序）。如果您需要从头开始安装，请从此处下载VS：<a href=\"https://link.zhihu.com/?target=http%3A//visualstudio.com/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">Visual Studio 2017社区</a></li><li>安装CUDA和cuDNN</li><li>安装git和cmake。确保它们至少在当前帐户的路径上</li><li>例如，安装<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Microsoft/vcpkg\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">vcpkg</a>并尝试安装测试库以确保一切正常vcpkg install opengl</li><li>定义环境变量，VCPKG_ROOT指向安装路径vcpkg</li><li>使用名称VCPKG_DEFAULT_TRIPLET和值定义另一个环境变量x64-windows</li><li>打开Powershell（作为标准用户）并键入（最后一个命令需要确认并用于清理不必要的文件）</li></ol><div class=\"highlight\"><pre><code class=\"language-text\">PS \\ &gt;                   cd $ env： VCPKG_ROOT \nPS Code \\ vcpkg &gt;          。\\ vcpkg install pthreads opencv ＃ replace with opencv [cuda]如果你想使用cuda-accelerated openCV</code></pre></div><ol><li>[仅限CUDA]使用首选计算功能自定义CMakeLists.txt</li><li>使用Powershell脚本构建build.ps1或使用Visual Studio 2017的“打开文件夹”功能。在第一个选项中，如果要使用Visual Studio，您将找到包含所有相应配置的构建后由CMake为您创建的自定义解决方案系统的标志。</li></ol><p class=\"ztext-empty-paragraph\"><br/></p><hr/><p class=\"ztext-empty-paragraph\"><br/></p><h2><b>环境安装</b></h2><h2><b>1.安装Visual Studio2017</b></h2><p>官网下载安装社区版即可：<a href=\"https://link.zhihu.com/?target=https%3A//visualstudio.microsoft.com/zh-hans/downloads/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">visualstudio.microsoft.com</span><span class=\"invisible\">/zh-hans/downloads/</span><span class=\"ellipsis\"></span></a></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-94bfe5a70d6ce72da27fe0308ece0efa_b.jpg\" data-rawwidth=\"766\" data-rawheight=\"566\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"766\" data-original=\"https://pic3.zhimg.com/v2-94bfe5a70d6ce72da27fe0308ece0efa_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;766&#39; height=&#39;566&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"766\" data-rawheight=\"566\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"766\" data-original=\"https://pic3.zhimg.com/v2-94bfe5a70d6ce72da27fe0308ece0efa_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-94bfe5a70d6ce72da27fe0308ece0efa_b.jpg\"/></figure><p>安装时依照自己的开发需要，勾选相应的环境：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-09395a2ed3d5a6c2c73aac04ffb28b65_b.jpg\" data-rawwidth=\"1280\" data-rawheight=\"847\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"1280\" data-original=\"https://pic2.zhimg.com/v2-09395a2ed3d5a6c2c73aac04ffb28b65_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1280&#39; height=&#39;847&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1280\" data-rawheight=\"847\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1280\" data-original=\"https://pic2.zhimg.com/v2-09395a2ed3d5a6c2c73aac04ffb28b65_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-09395a2ed3d5a6c2c73aac04ffb28b65_b.jpg\"/></figure><p>一定要选下面这个V140工具集，不然用141的会报错.....</p><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-0d1797f3b9664395b6b36d04280c8575_b.jpg\" data-rawwidth=\"1181\" data-rawheight=\"804\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"1181\" data-original=\"https://pic2.zhimg.com/v2-0d1797f3b9664395b6b36d04280c8575_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1181&#39; height=&#39;804&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1181\" data-rawheight=\"804\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1181\" data-original=\"https://pic2.zhimg.com/v2-0d1797f3b9664395b6b36d04280c8575_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-0d1797f3b9664395b6b36d04280c8575_b.jpg\"/></figure><p>安装时没有选完整也没关系，可以在Visual Studio Installer修改</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-d1e72578b19cc7b0c34fee89f76e415c_b.jpg\" data-rawwidth=\"351\" data-rawheight=\"613\" data-size=\"normal\" data-caption=\"\" class=\"content_image\" width=\"351\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;351&#39; height=&#39;613&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"351\" data-rawheight=\"613\" data-size=\"normal\" data-caption=\"\" class=\"content_image lazy\" width=\"351\" data-actualsrc=\"https://pic1.zhimg.com/v2-d1e72578b19cc7b0c34fee89f76e415c_b.jpg\"/></figure><h2><b>2.安装CUDA和CUDNN</b></h2><p>官网下载即可，我的安装包如下：</p><p>cuda_9.0.176_win10.exe</p><p>cudnn-9.0-windows10-x64-v7.4.2.24</p><h2><b>3.安装git和cmake</b></h2><p>git直接在官网下载即可：<a href=\"https://link.zhihu.com/?target=https%3A//git-scm.com/download/win\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">git-scm.com/download/wi</span><span class=\"invisible\">n</span><span class=\"ellipsis\"></span></a> </p><p>cmake也是，直接下载：<a href=\"https://link.zhihu.com/?target=https%3A//cmake.org/download/\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">cmake.org/download/</span><span class=\"invisible\"></span></a></p><p>刚开始，我git版本过低，后来在cmd中用update升级了一下~现在的版本如下：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-70419013a5f44276a9f4309b10f67022_b.jpg\" data-rawwidth=\"564\" data-rawheight=\"184\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"564\" data-original=\"https://pic3.zhimg.com/v2-70419013a5f44276a9f4309b10f67022_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;564&#39; height=&#39;184&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"564\" data-rawheight=\"184\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"564\" data-original=\"https://pic3.zhimg.com/v2-70419013a5f44276a9f4309b10f67022_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-70419013a5f44276a9f4309b10f67022_b.jpg\"/></figure><p>下载完成，记得配置环境变量：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-bb19f1d3072c11b2443a08d224480c4c_b.jpg\" data-rawwidth=\"520\" data-rawheight=\"557\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"520\" data-original=\"https://pic1.zhimg.com/v2-bb19f1d3072c11b2443a08d224480c4c_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;520&#39; height=&#39;557&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"520\" data-rawheight=\"557\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"520\" data-original=\"https://pic1.zhimg.com/v2-bb19f1d3072c11b2443a08d224480c4c_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-bb19f1d3072c11b2443a08d224480c4c_b.jpg\"/></figure><h2><b>4.安装vcpkg</b></h2><p>详见：<a href=\"https://link.zhihu.com/?target=https%3A//github.com/Microsoft/vcpkg\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/Microsoft/vc</span><span class=\"invisible\">pkg</span><span class=\"ellipsis\"></span></a></p><p>安装完成后，用vcpkg安装pthreads和opencv。</p><p>cd到vcpkg的主目录，运行命令：<b>.\\vcpkg install pthreads opencv</b></p><p><b>此处我发现，pthreads可以安装成功，opencv总是安装失败.....</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-a1f1045818a1211d92c0440f18a18ec6_b.jpg\" data-rawwidth=\"963\" data-rawheight=\"517\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"963\" data-original=\"https://pic3.zhimg.com/v2-a1f1045818a1211d92c0440f18a18ec6_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;963&#39; height=&#39;517&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"963\" data-rawheight=\"517\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"963\" data-original=\"https://pic3.zhimg.com/v2-a1f1045818a1211d92c0440f18a18ec6_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-a1f1045818a1211d92c0440f18a18ec6_b.jpg\"/></figure><p><b>看报错信息，下载opencv：3.4.3.tar.gz时报错，于是手动</b><a href=\"https://link.zhihu.com/?target=https%3A//github.com/opencv/opencv/archive/3.4.3.tar.gz\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">下载tar包</a><b>，放入vcpkg的downloads目录下，再次尝试安装：</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-e13a05f5f96f077927cbae2c616c778a_b.jpg\" data-rawwidth=\"926\" data-rawheight=\"658\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"926\" data-original=\"https://pic3.zhimg.com/v2-e13a05f5f96f077927cbae2c616c778a_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;926&#39; height=&#39;658&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"926\" data-rawheight=\"658\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"926\" data-original=\"https://pic3.zhimg.com/v2-e13a05f5f96f077927cbae2c616c778a_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-e13a05f5f96f077927cbae2c616c778a_b.jpg\"/></figure><p>至此，准备工作基本做完，我们可以开始编译darknet了</p><h2><b>5.安装opencv</b></h2><p>单纯安装opencv对版本没有限制，但是为了和darknet匹配，必须安装特定的版本，darnnet官网要求的版本是opencv 3.x，明确要求小于4.0版。</p><blockquote>我一次下了多个版本的opencv，发现3.0.0，3.4.0的可用，4.0.1的不可用。实践证明，确实如此，4.0.1的在编译daeknet时VS会报错，说有些接口方法不存在....3.x的版本中有些能用，有些不能用。</blockquote><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-50d3402bcb44e158e921e3533e93fcaa_b.jpg\" data-rawwidth=\"558\" data-rawheight=\"198\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"558\" data-original=\"https://pic3.zhimg.com/v2-50d3402bcb44e158e921e3533e93fcaa_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;558&#39; height=&#39;198&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"558\" data-rawheight=\"198\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"558\" data-original=\"https://pic3.zhimg.com/v2-50d3402bcb44e158e921e3533e93fcaa_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-50d3402bcb44e158e921e3533e93fcaa_b.jpg\"/></figure><p>官网下载： <a href=\"https://link.zhihu.com/?target=https%3A//opencv.org/releases.html\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">opencv.org/releases.htm</span><span class=\"invisible\">l</span><span class=\"ellipsis\"></span></a></p><figure data-size=\"normal\"><noscript><img src=\"https://pic3.zhimg.com/v2-b539c05b131742d98e970beeb92b7cfa_b.jpg\" data-rawwidth=\"623\" data-rawheight=\"774\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"623\" data-original=\"https://pic3.zhimg.com/v2-b539c05b131742d98e970beeb92b7cfa_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;623&#39; height=&#39;774&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"623\" data-rawheight=\"774\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"623\" data-original=\"https://pic3.zhimg.com/v2-b539c05b131742d98e970beeb92b7cfa_r.jpg\" data-actualsrc=\"https://pic3.zhimg.com/v2-b539c05b131742d98e970beeb92b7cfa_b.jpg\"/></figure><p>下载后解压至任意目录，方便自己使用时能找到即可</p><h2><b>6.下载darnnet</b></h2><p>darknet直接在<a href=\"https://link.zhihu.com/?target=https%3A//github.com/AlexeyAB/darknet\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">github.com/AlexeyAB/dar</span><span class=\"invisible\">knet</span><span class=\"ellipsis\"></span></a> 下载zip包即可，同样可以解压至任意目录</p><p>​<a href=\"https://link.zhihu.com/?target=http%3A//darknet-master.zip/\" class=\" wrap external\" target=\"_blank\" rel=\"nofollow noreferrer\">darknet-master.zip(8.1 MB)</a></p><hr/><h2><b>VS2017配置和编译</b></h2><p>      首先找到之前下载的daeknet-master目录，进入-&gt;build-&gt;darknet文件夹下，可以看见有三个.sln文件，这三个文件是三个VS项目入口，其中</p><ul><li><b>darknet.sln主要用来编译darknet，生成darknet.exe可执行文件</b>；</li><li><b>yolo_cpp_dll.sln主要用来编译可供外部使用的GPU版yolo的dll和lib文件；</b></li><li><b>yolo_cpp_dll_no_gpu.sln主要用来编译可供外部使用的非GPU版(即CPU版)dll和lib。</b></li></ul><blockquote>dll动态链接库，lib静态链接库 前面提到过，yolo支持CPU和GPU计算，CPU和GPU运行yolo所依赖的库文件是不同的</blockquote><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-be49229c3f2b9e6039da02bd9939bd57_b.jpg\" data-rawwidth=\"698\" data-rawheight=\"429\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"698\" data-original=\"https://pic4.zhimg.com/v2-be49229c3f2b9e6039da02bd9939bd57_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;698&#39; height=&#39;429&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"698\" data-rawheight=\"429\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"698\" data-original=\"https://pic4.zhimg.com/v2-be49229c3f2b9e6039da02bd9939bd57_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-be49229c3f2b9e6039da02bd9939bd57_b.jpg\"/></figure><p>  双击darknet.sln即可在VS2017中打开这个项目。</p><h2><b>VS2017属性配置</b></h2><p>找到项目目录，右键点属性：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-fbf8753ed608286abc0f35e335aa13ef_b.jpg\" data-rawwidth=\"1467\" data-rawheight=\"842\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"1467\" data-original=\"https://pic4.zhimg.com/v2-fbf8753ed608286abc0f35e335aa13ef_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1467&#39; height=&#39;842&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1467\" data-rawheight=\"842\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1467\" data-original=\"https://pic4.zhimg.com/v2-fbf8753ed608286abc0f35e335aa13ef_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-fbf8753ed608286abc0f35e335aa13ef_b.jpg\"/></figure><p>在属性页面，我们需要经行一系列配置</p><h2><b>目标平台版本</b></h2><p>目标平台版本即要生成的exe目标运行环境，默认是8.1，可修改为10，不配置也暂时无影响</p><h2><b>平台工具集</b></h2><p>即VS2017开发平台的工具集，经过采坑发现v141不行、v140可用</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-0c2884292ddb74419f965cdd8c9047c0_b.jpg\" data-rawwidth=\"1162\" data-rawheight=\"692\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"1162\" data-original=\"https://pic1.zhimg.com/v2-0c2884292ddb74419f965cdd8c9047c0_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1162&#39; height=&#39;692&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1162\" data-rawheight=\"692\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1162\" data-original=\"https://pic1.zhimg.com/v2-0c2884292ddb74419f965cdd8c9047c0_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-0c2884292ddb74419f965cdd8c9047c0_b.jpg\"/></figure><h2><b>VC++目录</b></h2><p>主要配2处：包含目录和引用目录，根据自己opencv安装目录配置相应路径.下面以我的路径为例：</p><p><b>包含目录：</b></p><div class=\"highlight\"><pre><code class=\"language-text\">D:\\software\\opencv\\opencv3.4.0\\opencv\\build\\include</code></pre></div><p><b>库目录：</b></p><div class=\"highlight\"><pre><code class=\"language-text\">D:\\software\\opencv\\opencv3.4.0\\opencv\\build\\x64\\vc15\\lib\nD:\\software\\opencv\\opencv3.4.0\\opencv\\build\\x64\\vc14\\lib</code></pre></div><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-58d3b135ff35a054c0db97d575630b63_b.jpg\" data-rawwidth=\"1160\" data-rawheight=\"687\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"1160\" data-original=\"https://pic4.zhimg.com/v2-58d3b135ff35a054c0db97d575630b63_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1160&#39; height=&#39;687&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1160\" data-rawheight=\"687\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1160\" data-original=\"https://pic4.zhimg.com/v2-58d3b135ff35a054c0db97d575630b63_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-58d3b135ff35a054c0db97d575630b63_b.jpg\"/></figure><p><b>编译darknet.exe</b></p><p>现在，我们可以尝试编译了，点击生成​</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-6997847a036ca03b94c4c9e72588a047_b.jpg\" data-rawwidth=\"1326\" data-rawheight=\"759\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"1326\" data-original=\"https://pic4.zhimg.com/v2-6997847a036ca03b94c4c9e72588a047_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1326&#39; height=&#39;759&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1326\" data-rawheight=\"759\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1326\" data-original=\"https://pic4.zhimg.com/v2-6997847a036ca03b94c4c9e72588a047_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-6997847a036ca03b94c4c9e72588a047_b.jpg\"/></figure><h2><b>报异常</b></h2><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-05ce6590e4f85d9372fab0c8ab3f3135_b.jpg\" data-rawwidth=\"1309\" data-rawheight=\"331\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"1309\" data-original=\"https://pic2.zhimg.com/v2-05ce6590e4f85d9372fab0c8ab3f3135_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1309&#39; height=&#39;331&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1309\" data-rawheight=\"331\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1309\" data-original=\"https://pic2.zhimg.com/v2-05ce6590e4f85d9372fab0c8ab3f3135_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-05ce6590e4f85d9372fab0c8ab3f3135_b.jpg\"/></figure><p><b>异常信息</b></p><blockquote>1&gt;------ 已启动生成: 项目: darknet, 配置: Release x64 ------ 1&gt;D:\\software\\darknet\\darknet190226\\new\\darknet-master\\build\\darknet\\darknet.vcxproj(54,5): error MSB4019: 未找到导入的项目“C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Community\\Common7\\IDE\\VC\\VCTargets\\BuildCustomizations\\CUDA 10.0.props”。请确认 &lt;Import&gt; 声明中的路径正确，且磁盘上存在该文件。 1&gt;已完成生成项目“darknet.vcxproj”的操作 - 失败。 ========== 生成: 成功 0 个，失败 1 个，最新 0 个，跳过 0 个 ==========</blockquote><h2><b>解决异常</b></h2><p>我们顺着异常提示的文件夹路径点进去发现，确实没有CUDA 10.0.props这个文件，但是有CUDA 9.0.props，原来在当前版本的darknet中，默认使用的CUDA 10.0,我们需要在配置darknet.vcxproj中修改(和darknet.sln在同级目录)</p><p>用notepad++修改，将10.0改为9.0即可，总共改头尾两处。​</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-e48cdfd48e7a3a73a8bc3463fe80f170_b.jpg\" data-rawwidth=\"1028\" data-rawheight=\"482\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"1028\" data-original=\"https://pic1.zhimg.com/v2-e48cdfd48e7a3a73a8bc3463fe80f170_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1028&#39; height=&#39;482&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1028\" data-rawheight=\"482\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1028\" data-original=\"https://pic1.zhimg.com/v2-e48cdfd48e7a3a73a8bc3463fe80f170_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-e48cdfd48e7a3a73a8bc3463fe80f170_b.jpg\"/></figure><h2><b>再次编译</b></h2><figure data-size=\"normal\"><noscript><img src=\"https://pic2.zhimg.com/v2-5b54860847939b00fc7c1d4c633b505d_b.jpg\" data-rawwidth=\"1258\" data-rawheight=\"349\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"1258\" data-original=\"https://pic2.zhimg.com/v2-5b54860847939b00fc7c1d4c633b505d_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1258&#39; height=&#39;349&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1258\" data-rawheight=\"349\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1258\" data-original=\"https://pic2.zhimg.com/v2-5b54860847939b00fc7c1d4c633b505d_r.jpg\" data-actualsrc=\"https://pic2.zhimg.com/v2-5b54860847939b00fc7c1d4c633b505d_b.jpg\"/></figure><p>编译成功后，会在darknet-master\\build\\darknet\\x64下生成darknet.exe文件，这个就是可执行的程序。</p><h2><b>编译GPU版库</b></h2><p>编译GPU版库文件，主要即产生可以支持GPU运行的yolo的dll和lib文件，用于方便在别的C++项目中直接调用，编译前的VS2017配置和上面编译darknet.exe基本类似，此处还需配置一处：</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-2b3c7d9f6b4bd7f9c03ac6c6cf2a808b_b.jpg\" data-rawwidth=\"1183\" data-rawheight=\"702\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"1183\" data-original=\"https://pic4.zhimg.com/v2-2b3c7d9f6b4bd7f9c03ac6c6cf2a808b_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1183&#39; height=&#39;702&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1183\" data-rawheight=\"702\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1183\" data-original=\"https://pic4.zhimg.com/v2-2b3c7d9f6b4bd7f9c03ac6c6cf2a808b_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-2b3c7d9f6b4bd7f9c03ac6c6cf2a808b_b.jpg\"/></figure><p>CUDA C/C++ -&gt;Deviece -&gt; Code Generation处。原compute_75,sm_75改为compute_52,sm_52</p><p>然后就是正常的生成，生成成功后在darknet-master\\build\\darknet\\x64目录下会新生成：</p><p><b>yolo_cpp_dll.dll和yolo_cpp_dll.lib</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-35019b472c271645ef67a11f54e35864_b.jpg\" data-rawwidth=\"1234\" data-rawheight=\"753\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"1234\" data-original=\"https://pic1.zhimg.com/v2-35019b472c271645ef67a11f54e35864_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;1234&#39; height=&#39;753&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"1234\" data-rawheight=\"753\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"1234\" data-original=\"https://pic1.zhimg.com/v2-35019b472c271645ef67a11f54e35864_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-35019b472c271645ef67a11f54e35864_b.jpg\"/></figure><h2><b>编译CPU版库</b></h2><p>CPU版本库的编译和GPU类似，就不赘述了</p><hr/><h2><b>运行展示</b></h2><p>1.下载yolov3.weights并放到darknet.exe同级目录下，下载链接：</p><p><a href=\"https://link.zhihu.com/?target=https%3A//pjreddie.com/media/files/yolov3.weights\" class=\" external\" target=\"_blank\" rel=\"nofollow noreferrer\"><span class=\"invisible\">https://</span><span class=\"visible\">pjreddie.com/media/file</span><span class=\"invisible\">s/yolov3.weights</span><span class=\"ellipsis\"></span></a></p><p>2.运行</p><p>cd到darknet.exe所在目录，运行命令即可。</p><p><b>命令1：darknet detect cfg/yolov3.cfg yolov3.weights data/dog.jpg</b></p><p><b>效果1：</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-a56869748ea0de0165bfe5d50ec6380f_b.jpg\" data-rawwidth=\"941\" data-rawheight=\"1003\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"941\" data-original=\"https://pic4.zhimg.com/v2-a56869748ea0de0165bfe5d50ec6380f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;941&#39; height=&#39;1003&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"941\" data-rawheight=\"1003\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"941\" data-original=\"https://pic4.zhimg.com/v2-a56869748ea0de0165bfe5d50ec6380f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-a56869748ea0de0165bfe5d50ec6380f_b.jpg\"/></figure><p><b>命令2：</b></p><p><b>darknet detect cfg/yolov3.cfg yolov3.weights data/bird.jpg</b></p><p><b>效果2：</b></p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-226e426ac0758415dc986a5df8b8382f_b.jpg\" data-rawwidth=\"993\" data-rawheight=\"698\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"993\" data-original=\"https://pic4.zhimg.com/v2-226e426ac0758415dc986a5df8b8382f_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;993&#39; height=&#39;698&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"993\" data-rawheight=\"698\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"993\" data-original=\"https://pic4.zhimg.com/v2-226e426ac0758415dc986a5df8b8382f_r.jpg\" data-actualsrc=\"https://pic4.zhimg.com/v2-226e426ac0758415dc986a5df8b8382f_b.jpg\"/></figure><blockquote>第二张黑天鹅竟然识别成了bird，哈哈哈哈~</blockquote><p class=\"ztext-empty-paragraph\"><br/></p><hr/><h2><b>Tips:</b></h2><p>1.运行时如果提示找不到opencv_world340.dll，需要手动将opencv下的此dll放入darknet.exe运行同级目录下</p><figure data-size=\"normal\"><noscript><img src=\"https://pic1.zhimg.com/v2-80aa03698277f0a130aecefb963c1c08_b.jpg\" data-rawwidth=\"751\" data-rawheight=\"301\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb\" width=\"751\" data-original=\"https://pic1.zhimg.com/v2-80aa03698277f0a130aecefb963c1c08_r.jpg\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;751&#39; height=&#39;301&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"751\" data-rawheight=\"301\" data-size=\"normal\" data-caption=\"\" class=\"origin_image zh-lightbox-thumb lazy\" width=\"751\" data-original=\"https://pic1.zhimg.com/v2-80aa03698277f0a130aecefb963c1c08_r.jpg\" data-actualsrc=\"https://pic1.zhimg.com/v2-80aa03698277f0a130aecefb963c1c08_b.jpg\"/></figure><p>我的opencv_world340.dll所在目录：</p><div class=\"highlight\"><pre><code class=\"language-text\">D:\\software\\opencv\\opencv3.4.0\\opencv\\build\\x64\\vc14\\bin</code></pre></div><p>opencv_world340.dll对应Release版、opencv_world340d.dll对应debug版。</p><p>2.如果运行时报内存溢出的异常，需要手动修改yolov3-voc.cfg中的batch和subdivisions，可以都修改为1</p><figure data-size=\"normal\"><noscript><img src=\"https://pic4.zhimg.com/v2-fc40f9f096adb261129fcb8289bdcc0b_b.jpg\" data-rawwidth=\"272\" data-rawheight=\"285\" data-size=\"normal\" data-caption=\"\" class=\"content_image\" width=\"272\"/></noscript><img src=\"data:image/svg+xml;utf8,&lt;svg xmlns=&#39;http://www.w3.org/2000/svg&#39; width=&#39;272&#39; height=&#39;285&#39;&gt;&lt;/svg&gt;\" data-rawwidth=\"272\" data-rawheight=\"285\" data-size=\"normal\" data-caption=\"\" class=\"content_image lazy\" width=\"272\" data-actualsrc=\"https://pic4.zhimg.com/v2-fc40f9f096adb261129fcb8289bdcc0b_b.jpg\"/></figure><p class=\"ztext-empty-paragraph\"><br/></p><p>​</p><p class=\"ztext-empty-paragraph\"><br/></p><p><br/> </p>", 
            "topic": [
                {
                    "tag": "机器学习", 
                    "tagLink": "https://api.zhihu.com/topics/19559450"
                }, 
                {
                    "tag": "图像识别", 
                    "tagLink": "https://api.zhihu.com/topics/19588774"
                }, 
                {
                    "tag": "OpenCV", 
                    "tagLink": "https://api.zhihu.com/topics/19587715"
                }
            ], 
            "comments": []
        }
    ], 
    "url": "https://zhuanlan.zhihu.com/DeepAI"
}
